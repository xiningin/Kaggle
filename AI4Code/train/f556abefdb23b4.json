{"cell_type":{"4774bf43":"code","310f1bec":"code","6260916a":"code","7f4be38b":"code","db42e6f6":"code","23d7d7e5":"code","fa172fb1":"code","ca2c6691":"code","08611223":"code","08d33764":"code","5789c10a":"code","7165dad6":"code","5ed467c0":"code","f64702d9":"code","37264e6e":"code","e4333bde":"code","f5196a4b":"code","8e33ce97":"code","689dd239":"code","083aafd2":"code","5a0b91bb":"code","e80d63d2":"code","353ab21c":"code","2f211c4a":"code","29a2cf21":"code","a2a65fbf":"code","6a1b210f":"code","147791fd":"code","e9b3579b":"code","1debd087":"code","0723b8c9":"code","378c1ad7":"code","c6abf87e":"code","0fbffdc4":"code","37cf02b3":"code","c244e387":"code","ed120b34":"code","a9b229d0":"code","938f95da":"code","d4799c0e":"code","e267489a":"code","e8e86e50":"code","5f670f8a":"code","a967becc":"code","979a5642":"code","d2c18931":"code","1ce2cc3d":"code","e3a8ec04":"code","5c214698":"markdown","e56688ab":"markdown","3d913e7b":"markdown","41a507f7":"markdown","b0087c87":"markdown","dbcc892e":"markdown","09adea74":"markdown","df1e1590":"markdown","acfb8172":"markdown","c471e64c":"markdown","efe4b264":"markdown","d361c03e":"markdown","afaa24a0":"markdown","29bf5a69":"markdown","a30463f6":"markdown","b652928c":"markdown","35cb58c1":"markdown","348a5487":"markdown","2d0739c9":"markdown","be51db75":"markdown","520a7237":"markdown","7492d5e2":"markdown","39971a50":"markdown","2f9802df":"markdown","27f129a2":"markdown","ea27984f":"markdown"},"source":{"4774bf43":"import seaborn as sns\nimport matplotlib.pyplot as plt","310f1bec":"\nbin_x = 0\nbin_y = 0\n\nstarting_position_x = -5\nstarting_position_y = -5\n\nplt.scatter(bin_x, bin_y, label = \"Bin\")\nplt.scatter(starting_position_x, starting_position_y, label = \"A\")\nplt.ylim([-10,10])\nplt.xlim([-10,10])\nplt.grid()\nplt.legend()\nplt.show()","6260916a":"import pandas as pd\nimport numpy as np","7f4be38b":"\ndef probability(bin_x, bin_y, state_x, state_y, throw_deg):\n\n\n    #First throw exception rule if person is directly on top of bin:\n    if((state_x==bin_x) & (state_y==bin_y)):\n        probability = 1\n    else:\n        \n        \n        # To accomodate for going over the 0 degree line\n        if((throw_deg>270) & (state_x<=bin_x) & (state_y<=bin_y)):\n            throw_deg = throw_deg - 360\n        elif((throw_deg<90) & (state_x>bin_x) & (state_y<bin_y)):\n            throw_deg = 360 + throw_deg\n        else:\n            throw_deg = throw_deg\n            \n        # Calculate Euclidean distance\n        distance = ((bin_x - state_x)**2 + (bin_y - state_y)**2)**0.5\n\n        # max distance for bin will always be on of the 4 corner points:\n        corner_x = [-10,-10,10,10]\n        corner_y = [-10,10,-10,10]\n        dist_table = pd.DataFrame()\n        for corner in range(0,4):\n            dist = pd.DataFrame({'distance':((bin_x - corner_x[corner])**2 + (bin_y - corner_y[corner])**2)**0.5}, index = [corner])\n            dist_table = dist_table.append(dist)\n        dist_table = dist_table.reset_index()\n        dist_table = dist_table.sort_values('distance', ascending = False)\n        max_dist = dist_table['distance'][0]\n        \n        distance_score = 1 - (distance\/max_dist)\n\n\n        # First if person is directly horizontal or vertical of bin:\n        if((state_x==bin_x) & (state_y>bin_y)):\n            direction = 180\n        elif((state_x==bin_x) & (state_y<bin_y)):\n             direction = 0\n        \n        elif((state_x>bin_x) & (state_y==bin_y)):\n             direction = 270\n        elif((state_x<bin_x) & (state_y==bin_y)):\n             direction = 90\n              \n        # If person is north-east of bin:\n        elif((state_x>bin_x) & (state_y>bin_y)):\n            opp = abs(bin_x - state_x)\n            adj = abs(bin_y - state_y)\n            direction = 180 +  np.degrees(np.arctan(opp\/adj))\n\n        # If person is south-east of bin:\n        elif((state_x>bin_x) & (state_y<bin_y)):\n            opp = abs(bin_y - state_y)\n            adj = abs(bin_x - state_x)\n            direction = 270 +  np.degrees(np.arctan(opp\/adj))\n\n        # If person is south-west of bin:\n        elif((state_x<bin_x) & (state_y<bin_y)):\n            opp = abs(bin_x - state_x)\n            adj = abs(bin_y - state_y)\n            direction =  np.degrees(np.arctan(opp\/adj))\n\n        # If person is north-west of bin:\n        elif((state_x<bin_x) & (state_y>bin_y)):\n            opp = abs(bin_y - state_y)\n            adj = abs(bin_x - state_x)\n            direction = 90 +  np.degrees(np.arctan(opp\/adj))\n\n        direction_score = (45-abs(direction - throw_deg))\/45\n      \n        probability = distance_score*direction_score\n        if(probability>0):\n            probability = probability\n        else:\n            probability = 0\n        \n    return(probability)\n    \n    \n    \n","db42e6f6":"bin_x = 0\nbin_y = 0\n\nstarting_position_x = -5\nstarting_position_y = -5\n\ntest_1 = probability(bin_x, bin_y, starting_position_x, starting_position_y, 50)\ntest_2 = probability(bin_x, bin_y, starting_position_x, starting_position_y, 60)","23d7d7e5":"print(\"Probability of first throw at 50 degrees = \", np.round(test_1,4))\nprint(\"Probability of second throw at 60 degress = \", np.round(test_2,4))","fa172fb1":"bin_x = 0\nbin_y = 0\nthrow_direction = 180\n\nprob_table = pd.DataFrame()\nfor i in range(0,20):\n    state_x = -10 + i\n    for j in range(0,20):\n        state_y = -10 + j\n        prob = pd.DataFrame({'x':state_x,'y':state_y,'prob':probability(bin_x, bin_y, state_x, state_y, throw_direction)}, index = [0])\n        prob_table = prob_table.append(prob)\nprob_table = prob_table.reset_index()\n\nplt.scatter(prob_table['x'], prob_table['y'], s=prob_table['prob']*400, alpha=0.5)\nplt.ylim([-10,10])\nplt.xlim([-10,10])\nplt.grid()\nplt.title(\"Probability of Landing Shot for a given Thrown Direction: \\n \" + str(throw_direction)+\" degrees\")\nplt.show()\n\n        ","ca2c6691":"from plotly.offline import init_notebook_mode, iplot, plot\nfrom IPython.display import display, HTML\nimport plotly\nimport plotly.plotly as py\n\ninit_notebook_mode(connected=True)\n","08611223":"bin_x = 0\nbin_y = 0\n\nprob_table = pd.DataFrame()\nfor z in range(0,37):\n    throw_direction = z*10\n    for i in range(0,20):\n        state_x = -10 + i\n        for j in range(0,20):\n            state_y = -10 + j\n            prob = pd.DataFrame({'throw_dir':throw_direction,'x':state_x,'y':state_y,'prob':probability(bin_x, bin_y, state_x, state_y, throw_direction)}, index = [0])\n            prob_table = prob_table.append(prob)\nprob_table = prob_table.reset_index(drop=True)\n        ","08d33764":"prob_table.head()","5789c10a":"# Create interactive animation to show change of throwing direction to state probabilities (https:\/\/www.philiposbornedata.com\/2018\/03\/01\/creating-interactive-animation-for-parameter-optimisation-using-plot-ly\/)\n# This code is only used for visual and can be ignored entirely otherwise\nprob_table_2 = prob_table\nprob_table_2['continent'] = 'Test'\nprob_table_2['country'] = 'Test2'\n\nprob_table_2.columns = ['year', 'lifeExp', 'gdpPercap', 'pop', 'continent', 'country']\nprob_table_2 = prob_table_2[prob_table_2['pop']>=0]\nprob_table_2['pop'] = prob_table_2['pop']*100000000\nprob_table_2.head()\nalpha = list(set(prob_table_2['year']))\nalpha = np.round(alpha,1)\nalpha = np.sort(alpha)#[::-1]\nyears = np.round([(alpha) for alpha in alpha],1)\nyears\n\ndataset = prob_table_2\n\ncontinents = []\nfor continent in dataset['continent']:\n    if continent not in continents:\n        continents.append(continent)\n# make figure\nfigure = {\n    'data': [],\n    'layout': {},\n    'frames': []\n}\n\n# fill in most of layout\nfigure['layout']['title'] = \"Probability of Throwing Paper for each State as Thrown Direction Varies <br> PhilipOsborneData.com\"\nfigure['layout']['xaxis'] = {'range': [-10,10], 'title': 'x'}\nfigure['layout']['yaxis'] = {'range': [-10,10],'title': 'y', 'type': 'linear'}\nfigure['layout']['hovermode'] = 'closest'\nfigure['layout']['sliders'] = {\n    'args': [\n        'transition', {\n            'duration': 400,\n            'easing': 'cubic-in-out'\n        }\n    ],\n    'initialValue': '1952',\n    'plotlycommand': 'animate',\n    'values': years,\n    'visible': True\n}\n\nfigure['layout']['autosize'] = False\nfigure['layout']['width'] = 750\nfigure['layout']['height'] = 750\n\n\nfigure['layout']['updatemenus'] = [\n    {\n        'buttons': [\n            {\n                'args': [None, {'frame': {'duration': 500, 'redraw': False},\n                         'fromcurrent': True, 'transition': {'duration': 300, 'easing': 'quadratic-in-out'}}],\n                'label': 'Play',\n                'method': 'animate'\n            },\n            {\n                'args': [[None], {'frame': {'duration': 0, 'redraw': False}, 'mode': 'immediate',\n                'transition': {'duration': 0}}],\n                'label': 'Pause',\n                'method': 'animate'\n            }\n        ],\n        'direction': 'left',\n        'pad': {'r': 10, 't': 87},\n        'showactive': False,\n        'type': 'buttons',\n        'x': 0.1,\n        'xanchor': 'right',\n        'y': 0,\n        'yanchor': 'top'\n    }\n]\n\nsliders_dict = {\n    'active': 0,\n    'yanchor': 'top',\n    'xanchor': 'left',\n    'currentvalue': {\n        'font': {'size': 20},\n        'prefix': 'Thrown Direction: ',\n        'visible': True,\n        'xanchor': 'right'\n    },\n    'transition': {'duration': 300, 'easing': 'cubic-in-out'},\n    'pad': {'b': 10, 't': 50},\n    'len': 0.9,\n    'x': 0.1,\n    'y': 0,\n    'steps': []\n}\n\n# make data\nyear = 1.0\nfor continent in continents:\n    dataset_by_year = dataset[np.round(dataset['year'],1) == np.round(year,1)]\n    dataset_by_year_and_cont = dataset_by_year[dataset_by_year['continent'] == continent]\n\n    data_dict = {\n        'x': list(dataset_by_year_and_cont['lifeExp']),\n        'y': list(dataset_by_year_and_cont['gdpPercap']),\n        'mode': 'markers',\n        'text': list(dataset_by_year_and_cont['country']),\n        'marker': {\n            'sizemode': 'area',\n            'sizeref': 200000,\n            'size': list(dataset_by_year_and_cont['pop'])\n        },\n        'name': continent\n    }\n    figure['data'].append(data_dict)\n\n# make frames\nfor year in years:\n    frame = {'data': [], 'name': str(year)}\n    for continent in continents:\n        dataset_by_year = dataset[np.round(dataset['year'],1) == np.round(year,1)]\n        dataset_by_year_and_cont = dataset_by_year[dataset_by_year['continent'] == continent]\n\n        data_dict = {\n            'x': list(dataset_by_year_and_cont['lifeExp']),\n            'y': list(dataset_by_year_and_cont['gdpPercap']),\n            'mode': 'markers',\n            'text': list(dataset_by_year_and_cont['country']),\n            'marker': {\n                'sizemode': 'area',\n                'sizeref': 200000,\n                'size': list(dataset_by_year_and_cont['pop'])\n            },\n            'name': continent\n        }\n        frame['data'].append(data_dict)\n\n    figure['frames'].append(frame)\n    slider_step = {'args': [\n        [year],\n        {'frame': {'duration': 300, 'redraw': False},\n         'mode': 'immediate',\n       'transition': {'duration': 300}}\n     ],\n     'label': str(year),\n     'method': 'animate'}\n    sliders_dict['steps'].append(slider_step)\n\n\nfigure['layout']['sliders'] = [sliders_dict]\n\n# iplot to do in notebook or plot to open new tab\niplot(figure)\n#plot(figure)","7165dad6":"#Define Q(s,a) table by all possible states and THROW actions initialised to 0\nQ_table = pd.DataFrame()\nfor z in range(0,360):\n    throw_direction = int(z)\n    for i in range(0,21):\n        state_x = int(-10 + i)\n        for j in range(0,21):\n            state_y = int(-10 + j)\n            reward = 0\n            Q = pd.DataFrame({'throw_dir':throw_direction,'move_dir':\"none\",'state_x':state_x,'state_y':state_y,'Q':0, 'reward': reward}, index = [0])\n            Q_table = Q_table.append(Q)\nQ_table = Q_table.reset_index(drop=True)\nprint(\"Q table 1 initialised\")\nQ_table.head()","5ed467c0":"#Define Q(s,a) table by all possible states and MOVE actions initialised to 0\n\nfor x in range(0,21):\n    state_x = int(-10 + x)\n    for y in range(0,21):\n        state_y = int(-10 + y)\n        for m in range(0,8):\n            move_dir = int(m)\n            \n            # skip impossible moves starting with 4 corners then edges\n            if((state_x==10)&(state_y==10)&(move_dir==0)):\n                continue\n            elif((state_x==10)&(state_y==10)&(move_dir==2)):\n                continue\n                \n            elif((state_x==10)&(state_y==-10)&(move_dir==2)):\n                continue\n            elif((state_x==10)&(state_y==-10)&(move_dir==4)):\n                continue\n                \n            elif((state_x==-10)&(state_y==-10)&(move_dir==4)):\n                continue\n            elif((state_x==-10)&(state_y==-10)&(move_dir==6)):\n                continue\n                \n            elif((state_x==-10)&(state_y==10)&(move_dir==6)):\n                continue\n            elif((state_x==-10)&(state_y==10)&(move_dir==0)):\n                continue\n                \n            elif((state_x==10) & (move_dir == 1)):\n                continue\n            elif((state_x==10) & (move_dir == 2)):\n                continue\n            elif((state_x==10) & (move_dir == 3)):\n                continue\n                 \n            elif((state_x==-10) & (move_dir == 5)):\n                continue\n            elif((state_x==-10) & (move_dir == 6)):\n                continue\n            elif((state_x==-10) & (move_dir == 7)):\n                continue\n                 \n            elif((state_y==10) & (move_dir == 1)):\n                continue\n            elif((state_y==10) & (move_dir == 0)):\n                continue\n            elif((state_y==10) & (move_dir == 7)):\n                continue\n                 \n            elif((state_y==-10) & (move_dir == 3)):\n                continue\n            elif((state_y==-10) & (move_dir == 4)):\n                continue\n            elif((state_y==-10) & (move_dir == 5)):\n                continue\n                 \n            else:\n                reward = 0\n                Q = pd.DataFrame({'throw_dir':\"none\",'move_dir':move_dir,'state_x':state_x,'state_y':state_y,'Q':0, 'reward': reward}, index = [0])\n                Q_table = Q_table.append(Q)\nQ_table = Q_table.reset_index(drop=True)\nprint(\"Q table 2 initialised\")\nQ_table.tail()","f64702d9":"Q_table[(Q_table['state_x']==-10) &(Q_table['throw_dir']==\"none\")].head(5)","37264e6e":"Q_table_VI = Q_table.copy()","e4333bde":"Q_table_VI['V'] = 0","f5196a4b":"bin_x = 0\nbin_y = 0\n\nprob_list = pd.DataFrame()\nfor n,action in enumerate(Q_table_VI['throw_dir']):\n    # Guarantee 100% probability if movement\n    if(action == \"none\"):\n        prob = 1\n    # Calculate if thrown\n    else:\n        prob = probability(bin_x, bin_y, Q_table_VI['state_x'][n], Q_table_VI['state_y'][n], action)\n    prob_list = prob_list.append(pd.DataFrame({'prob':prob}, index = [n] ))\nprob_list = prob_list.reset_index(drop=True)\nQ_table_VI['prob'] = prob_list['prob']","8e33ce97":"Q_table_VI.head(5)","689dd239":"Q_table_VI[ (Q_table_VI['state_x']==-1) & (Q_table_VI['state_y']==-1) & (Q_table_VI['throw_dir']==45)]","083aafd2":"import time\nfrom IPython.display import clear_output","5a0b91bb":"input_table = Q_table_VI.copy()\ngamma = 0.8\nnum_repeats = 5\n\nstart_time = time.time()\n\noutput_metric_table = pd.DataFrame()\n# Repeat until converges\nfor repeats in range(0,num_repeats):\n    clear_output(wait=True)\n    state_sub_full = pd.DataFrame()\n    \n    \n    output_metric_table = output_metric_table.append(pd.DataFrame({'mean_Q':input_table['Q'].mean(), \n                                                                   'sum_Q': input_table['Q'].sum(),\n                                                                   'mean_V':input_table[['state_x', 'state_y','V']].drop_duplicates(['state_x', 'state_y', 'V'])['V'].mean(),\n                                                                   'sum_V': input_table[['state_x', 'state_y','V']].drop_duplicates(['state_x', 'state_y', 'V'])['V'].sum()}, index = [repeats]))\n    \n    \n    # Iterate over all states defined by max - min of x times by max - min of y\n    for x in range(0,21):\n        state_x = -10 + x\n        for y in range(0,21):\n            state_y = -10 + y\n\n            state_sub = input_table[ (input_table['state_x']==state_x) & (input_table['state_y']==state_y)]\n            Q_sub_list = pd.DataFrame()\n            for n, action in state_sub.iterrows():\n                # Move action update Q\n                if(action['throw_dir'] == \"none\"):\n                    move_direction = action['move_dir']\n                    #Map this to actual direction and find V(s) for next state\n                    if(move_direction == 0):\n                        move_x = 0\n                        move_y = 1\n                    elif(move_direction == 1):\n                        move_x = 1\n                        move_y = 1\n                    elif(move_direction == 2):\n                        move_x = 1\n                        move_y = 0\n                    elif(move_direction == 3):\n                        move_x = 1\n                        move_y = -1\n                    elif(move_direction == 4):\n                        move_x = 0\n                        move_y = -1\n                    elif(move_direction == 5):\n                        move_x = -1\n                        move_y = -1\n                    elif(move_direction == 6):\n                        move_x = -1\n                        move_y = 0\n                    elif(move_direction == 7):\n                        move_x = -1\n                        move_y = 1\n                    Q = 1*(action['reward'] + gamma*max(input_table[ (input_table['state_x']==int(state_x+move_x)) & (input_table['state_y']==int(state_y+move_y))]['V']) )\n                # Throw update Q +1 if sucessful throw or -1 if failed\n                else:\n                    Q = (action['prob']*(action['reward'] + gamma*1)) +  ((1-action['prob'])*(action['reward'] + gamma*-1))\n                Q_sub_list = Q_sub_list.append(pd.DataFrame({'Q':Q}, index = [n]))\n            state_sub['Q'] = Q_sub_list['Q']\n            state_sub['V'] = max(state_sub['Q'])\n            state_sub_full = state_sub_full.append(state_sub)\n            \n            \n\n    \n            \n    input_table = state_sub_full.copy()\n    print(\"Repeats completed: \", np.round((repeats+1)\/num_repeats,2)*100, \"%\")\n    \nend_time = time.time()\n\nprint(\"total time taken this loop: \", np.round((end_time - start_time)\/60,2), \" minutes\")","e80d63d2":"state_sub_full.head(3)","353ab21c":"state_sub_full[ (state_sub_full['state_x']==-4) & (state_sub_full['state_y']==-4) & (state_sub_full['Q']== max(state_sub_full[ (state_sub_full['state_x']==-4) & (state_sub_full['state_y']==-4)]['Q']))]","2f211c4a":"output_metric_table","29a2cf21":"plt.plot(range(0,len(output_metric_table)), output_metric_table['mean_V'])\nplt.title(\"Mean Q for all State-Action Pairs for each Update \")\nplt.show()","a2a65fbf":"Q_table_VI_3 = state_sub_full.copy()","6a1b210f":"optimal_action_list = pd.DataFrame()\nfor x in range(0,21):\n    state_x = int(-10 + x)\n    for y in range(0,21):\n        state_y = int(-10 + y)\n        \n        Q_table_VI_3\n        \n        optimal_action = pd.DataFrame({'state_x':state_x, 'state_y': state_y, \n                                      'move_dir': Q_table_VI_3[ (Q_table_VI_3['state_x']==state_x) & (Q_table_VI_3['state_y']==state_y) &  (Q_table_VI_3['Q'] == max(Q_table_VI_3[(Q_table_VI_3['state_x']==state_x) & \n                                                      (Q_table_VI_3['state_y']==state_y)]['Q']))].reset_index(drop=True)['move_dir'][0],\n                                      'throw_dir': Q_table_VI_3[ (Q_table_VI_3['state_x']==state_x) & (Q_table_VI_3['state_y']==state_y) &  (Q_table_VI_3['Q'] == max(Q_table_VI_3[(Q_table_VI_3['state_x']==state_x) & \n                                                      (Q_table_VI_3['state_y']==state_y)]['Q']))].reset_index(drop=True)['throw_dir'][0]},\n                                     index = [state_y])\n        optimal_action_list = optimal_action_list.append(optimal_action)\noptimal_action_list = optimal_action_list.reset_index(drop=True)","147791fd":"optimal_action_list.head(5)","e9b3579b":"optimal_action_list[(optimal_action_list['state_x']==-1)&(optimal_action_list['state_y']==-1)]","1debd087":"optimal_action_list['Action'] = np.where( optimal_action_list['move_dir'] == 'none', 'THROW', 'MOVE'  )","0723b8c9":"sns.scatterplot( x=\"state_x\", y=\"state_y\", data=optimal_action_list,  hue='Action')\nplt.title(\"Optimal Policy for Given Probabilities\")\nplt.ylim([-10,10])\nplt.xlim([-10,10])\nplt.show()","378c1ad7":"optimal_action_list['move_x'] = np.where(optimal_action_list['move_dir'] == 0, int(0),\n                                         np.where(optimal_action_list['move_dir'] == 1, int(1),\n                                         np.where(optimal_action_list['move_dir'] == 2, int(1),\n                                         np.where(optimal_action_list['move_dir'] == 3, int(1),\n                                         np.where(optimal_action_list['move_dir'] == 4, int(0),\n                                         np.where(optimal_action_list['move_dir'] == 5, int(-1),\n                                         np.where(optimal_action_list['move_dir'] == 6, int(-1),\n                                         np.where(optimal_action_list['move_dir'] == 7, int(-1),\n                                         int(-1000)\n                                        ))))))))\noptimal_action_list['move_y'] = np.where(optimal_action_list['move_dir'] == 0, int(1),\n                                         np.where(optimal_action_list['move_dir'] == 1, int(1),\n                                         np.where(optimal_action_list['move_dir'] == 2, int(0),\n                                         np.where(optimal_action_list['move_dir'] == 3, int(-1),\n                                         np.where(optimal_action_list['move_dir'] == 4, int(-1),\n                                         np.where(optimal_action_list['move_dir'] == 5, int(-1),\n                                         np.where(optimal_action_list['move_dir'] == 6, int(0),\n                                         np.where(optimal_action_list['move_dir'] == 7, int(1),\n                                         int(-1000)\n                                        ))))))))\noptimal_action_list['throw_dir_2'] = np.where(optimal_action_list['throw_dir']==\"none\",int(-1000), optimal_action_list['throw_dir'])\noptimal_action_list.head(10)","c6abf87e":"arrow_scale = 0.1","0fbffdc4":"# Define horizontal arrow component as 0.1*move direction or 0.1\/-0.1 depending on throw direction\noptimal_action_list['u'] = np.where(optimal_action_list['Action']==\"MOVE\", optimal_action_list['move_x']*arrow_scale,\n                                    np.where(optimal_action_list['throw_dir_2']==0, 0,np.where(optimal_action_list['throw_dir_2']==180, 0,\n                                    np.where(optimal_action_list['throw_dir_2']==90, arrow_scale ,np.where(optimal_action_list['throw_dir_2']==270, -arrow_scale,\n                                    np.where(optimal_action_list['throw_dir_2']<180, arrow_scale,-arrow_scale))))))\noptimal_action_list.head(5)","37cf02b3":"# Define vertical arrow component based 0.1*move direciton or +\/- u*tan(throw_dir) accordingly\noptimal_action_list['v'] = np.where(optimal_action_list['Action']==\"MOVE\", optimal_action_list['move_y']*arrow_scale, \n                                    np.where(optimal_action_list['throw_dir_2']==0, arrow_scale,np.where(optimal_action_list['throw_dir_2']==180, -arrow_scale,\n                                    np.where(optimal_action_list['throw_dir_2']==90, 0,np.where(optimal_action_list['throw_dir_2']==270, 0,\n                                    optimal_action_list['u']\/np.tan(np.deg2rad(optimal_action_list['throw_dir_2'].astype(np.float64))))))))\noptimal_action_list.head(5)","c244e387":"x = optimal_action_list['state_x']\ny = optimal_action_list['state_y']\nu = optimal_action_list['u'].values\nv = optimal_action_list['v'].values\nplt.figure(figsize=(10, 10))\nplt.quiver(x,y,u,v,scale=0.5,scale_units='inches')\nsns.scatterplot( x=\"state_x\", y=\"state_y\", data=optimal_action_list,  hue='Action')\nplt.title(\"Optimal Policy for Given Probabilities\")\nplt.show()\n","ed120b34":"# Create Quiver plot showing current optimal policy in one cell\narrow_scale = 0.1\n\nQ_table_VI_3 = state_sub_full.copy()\n\noptimal_action_list = pd.DataFrame()\nfor x in range(0,21):\n    state_x = int(-10 + x)\n    for y in range(0,21):\n        state_y = int(-10 + y)\n        \n        Q_table_VI_3\n        \n        optimal_action = pd.DataFrame({'state_x':state_x, 'state_y': state_y, \n                                      'move_dir': Q_table_VI_3[ (Q_table_VI_3['state_x']==state_x) & (Q_table_VI_3['state_y']==state_y) &  (Q_table_VI_3['Q'] == max(Q_table_VI_3[(Q_table_VI_3['state_x']==state_x) & \n                                                      (Q_table_VI_3['state_y']==state_y)]['Q']))].reset_index(drop=True)['move_dir'][0],\n                                      'throw_dir': Q_table_VI_3[ (Q_table_VI_3['state_x']==state_x) & (Q_table_VI_3['state_y']==state_y) &  (Q_table_VI_3['Q'] == max(Q_table_VI_3[(Q_table_VI_3['state_x']==state_x) & \n                                                      (Q_table_VI_3['state_y']==state_y)]['Q']))].reset_index(drop=True)['throw_dir'][0]},\n                                     index = [state_y])\n        optimal_action_list = optimal_action_list.append(optimal_action)\noptimal_action_list = optimal_action_list.reset_index(drop=True)\n\noptimal_action_list['Action'] = np.where( optimal_action_list['move_dir'] == 'none', 'THROW', 'MOVE'  )\n\n\noptimal_action_list['move_x'] = np.where(optimal_action_list['move_dir'] == 0, int(0),\n                                         np.where(optimal_action_list['move_dir'] == 1, int(1),\n                                         np.where(optimal_action_list['move_dir'] == 2, int(1),\n                                         np.where(optimal_action_list['move_dir'] == 3, int(1),\n                                         np.where(optimal_action_list['move_dir'] == 4, int(0),\n                                         np.where(optimal_action_list['move_dir'] == 5, int(-1),\n                                         np.where(optimal_action_list['move_dir'] == 6, int(-1),\n                                         np.where(optimal_action_list['move_dir'] == 7, int(-1),\n                                         int(-1000)\n                                        ))))))))\noptimal_action_list['move_y'] = np.where(optimal_action_list['move_dir'] == 0, int(1),\n                                         np.where(optimal_action_list['move_dir'] == 1, int(1),\n                                         np.where(optimal_action_list['move_dir'] == 2, int(0),\n                                         np.where(optimal_action_list['move_dir'] == 3, int(-1),\n                                         np.where(optimal_action_list['move_dir'] == 4, int(-1),\n                                         np.where(optimal_action_list['move_dir'] == 5, int(-1),\n                                         np.where(optimal_action_list['move_dir'] == 6, int(0),\n                                         np.where(optimal_action_list['move_dir'] == 7, int(1),\n                                         int(-1000)\n                                        ))))))))\noptimal_action_list['throw_dir_2'] = np.where(optimal_action_list['throw_dir']==\"none\",int(-1000), optimal_action_list['throw_dir'])\n\n# Define horizontal arrow component as 0.1*move direction or 0.1\/-0.1 depending on throw direction\noptimal_action_list['u'] = np.where(optimal_action_list['Action']==\"MOVE\", optimal_action_list['move_x']*arrow_scale,\n                                    np.where(optimal_action_list['throw_dir_2']==0, 0,np.where(optimal_action_list['throw_dir_2']==180, 0,\n                                    np.where(optimal_action_list['throw_dir_2']==90, arrow_scale ,np.where(optimal_action_list['throw_dir_2']==270, -arrow_scale,\n                                    np.where(optimal_action_list['throw_dir_2']<180, arrow_scale,-arrow_scale))))))\n                                             \n# Define vertical arrow component based 0.1*move direciton or +\/- u*tan(throw_dir) accordingly\noptimal_action_list['v'] = np.where(optimal_action_list['Action']==\"MOVE\", optimal_action_list['move_y']*arrow_scale, \n                                    np.where(optimal_action_list['throw_dir_2']==0, arrow_scale,np.where(optimal_action_list['throw_dir_2']==180, -arrow_scale,\n                                    np.where(optimal_action_list['throw_dir_2']==90, 0,np.where(optimal_action_list['throw_dir_2']==270, 0,\n                                    optimal_action_list['u']\/np.tan(np.deg2rad(optimal_action_list['throw_dir_2'].astype(np.float64))))))))\n                                             \nx = optimal_action_list['state_x']\ny = optimal_action_list['state_y']\nu = optimal_action_list['u'].values\nv = optimal_action_list['v'].values\n\n#plt.figure(figsize=(10, 10))\n#plt.quiver(x,y,u,v,scale=0.5,scale_units='inches')\n#sns.scatterplot( x=\"state_x\", y=\"state_y\", data=optimal_action_list,  hue='Action')\n#plt.title(\"Optimal Policy for Given Probabilities\")\n#plt.show()\n\n","a9b229d0":"input_table = Q_table_VI.copy()\ngamma = 0.8\nnum_repeats = 15\n\nstart_time = time.time()\n\noutput_metric_table = pd.DataFrame()\n# Repeat until converges\nfor repeats in range(0,num_repeats):\n    clear_output(wait=True)\n    state_sub_full = pd.DataFrame()\n    \n    \n    output_metric_table = output_metric_table.append(pd.DataFrame({'mean_Q':input_table['Q'].mean(), \n                                                                   'sum_Q': input_table['Q'].sum(),\n                                                                   'mean_V':input_table[['state_x', 'state_y','V']].drop_duplicates(['state_x', 'state_y', 'V'])['V'].mean(),\n                                                                   'sum_V': input_table[['state_x', 'state_y','V']].drop_duplicates(['state_x', 'state_y', 'V'])['V'].sum()}, index = [repeats]))\n    \n    \n    # Iterate over all states defined by max - min of x times by max - min of y\n    for x in range(0,21):\n        state_x = -10 + x\n        for y in range(0,21):\n            state_y = -10 + y\n\n            state_sub = input_table[ (input_table['state_x']==state_x) & (input_table['state_y']==state_y)]\n            Q_sub_list = pd.DataFrame()\n            for n, action in state_sub.iterrows():\n                # Move action update Q\n                if(action['throw_dir'] == \"none\"):\n                    move_direction = action['move_dir']\n                    #Map this to actual direction and find V(s) for next state\n                    if(move_direction == 0):\n                        move_x = 0\n                        move_y = 1\n                    elif(move_direction == 1):\n                        move_x = 1\n                        move_y = 1\n                    elif(move_direction == 2):\n                        move_x = 1\n                        move_y = 0\n                    elif(move_direction == 3):\n                        move_x = 1\n                        move_y = -1\n                    elif(move_direction == 4):\n                        move_x = 0\n                        move_y = -1\n                    elif(move_direction == 5):\n                        move_x = -1\n                        move_y = -1\n                    elif(move_direction == 6):\n                        move_x = -1\n                        move_y = 0\n                    elif(move_direction == 7):\n                        move_x = -1\n                        move_y = 1\n                    Q = 1*(action['reward'] + gamma*max(input_table[ (input_table['state_x']==int(state_x+move_x)) & (input_table['state_y']==int(state_y+move_y))]['V']) )\n                # Throw update Q +1 if sucessful throw or -1 if failed\n                else:\n                    Q = (action['prob']*(action['reward'] + gamma*1)) +  ((1-action['prob'])*(action['reward'] + gamma*-1))\n                Q_sub_list = Q_sub_list.append(pd.DataFrame({'Q':Q}, index = [n]))\n            state_sub['Q'] = Q_sub_list['Q']\n            state_sub['V'] = max(state_sub['Q'])\n            state_sub_full = state_sub_full.append(state_sub)\n            \n    \n    \n    ###\n    # Create Quiver plot showing current optimal policy in one cell\n    arrow_scale = 0.1\n\n    Q_table_VI_3 = state_sub_full.copy()\n\n    optimal_action_list = pd.DataFrame()\n    for x in range(0,21):\n        state_x = int(-10 + x)\n        for y in range(0,21):\n            state_y = int(-10 + y)\n\n            Q_table_VI_3\n\n            optimal_action = pd.DataFrame({'state_x':state_x, 'state_y': state_y, \n                                          'move_dir': Q_table_VI_3[ (Q_table_VI_3['state_x']==state_x) & (Q_table_VI_3['state_y']==state_y) &  (Q_table_VI_3['Q'] == max(Q_table_VI_3[(Q_table_VI_3['state_x']==state_x) & \n                                                          (Q_table_VI_3['state_y']==state_y)]['Q']))].reset_index(drop=True)['move_dir'][0],\n                                          'throw_dir': Q_table_VI_3[ (Q_table_VI_3['state_x']==state_x) & (Q_table_VI_3['state_y']==state_y) &  (Q_table_VI_3['Q'] == max(Q_table_VI_3[(Q_table_VI_3['state_x']==state_x) & \n                                                          (Q_table_VI_3['state_y']==state_y)]['Q']))].reset_index(drop=True)['throw_dir'][0]},\n                                         index = [state_y])\n            optimal_action_list = optimal_action_list.append(optimal_action)\n    optimal_action_list = optimal_action_list.reset_index(drop=True)\n\n    optimal_action_list['Action'] = np.where( optimal_action_list['move_dir'] == 'none', 'THROW', 'MOVE'  )\n\n\n    optimal_action_list['move_x'] = np.where(optimal_action_list['move_dir'] == 0, int(0),\n                                             np.where(optimal_action_list['move_dir'] == 1, int(1),\n                                             np.where(optimal_action_list['move_dir'] == 2, int(1),\n                                             np.where(optimal_action_list['move_dir'] == 3, int(1),\n                                             np.where(optimal_action_list['move_dir'] == 4, int(0),\n                                             np.where(optimal_action_list['move_dir'] == 5, int(-1),\n                                             np.where(optimal_action_list['move_dir'] == 6, int(-1),\n                                             np.where(optimal_action_list['move_dir'] == 7, int(-1),\n                                             int(-1000)\n                                            ))))))))\n    optimal_action_list['move_y'] = np.where(optimal_action_list['move_dir'] == 0, int(1),\n                                             np.where(optimal_action_list['move_dir'] == 1, int(1),\n                                             np.where(optimal_action_list['move_dir'] == 2, int(0),\n                                             np.where(optimal_action_list['move_dir'] == 3, int(-1),\n                                             np.where(optimal_action_list['move_dir'] == 4, int(-1),\n                                             np.where(optimal_action_list['move_dir'] == 5, int(-1),\n                                             np.where(optimal_action_list['move_dir'] == 6, int(0),\n                                             np.where(optimal_action_list['move_dir'] == 7, int(1),\n                                             int(-1000)\n                                            ))))))))\n    optimal_action_list['throw_dir_2'] = np.where(optimal_action_list['throw_dir']==\"none\",int(-1000), optimal_action_list['throw_dir'])\n\n    # Define horizontal arrow component as 0.1*move direction or 0.1\/-0.1 depending on throw direction\n    optimal_action_list['u'] = np.where(optimal_action_list['Action']==\"MOVE\", optimal_action_list['move_x']*arrow_scale,\n                                        np.where(optimal_action_list['throw_dir_2']==0, 0,np.where(optimal_action_list['throw_dir_2']==180, 0,\n                                        np.where(optimal_action_list['throw_dir_2']==90, arrow_scale ,np.where(optimal_action_list['throw_dir_2']==270, -arrow_scale,\n                                        np.where(optimal_action_list['throw_dir_2']<180, arrow_scale,-arrow_scale))))))\n\n    # Define vertical arrow component based 0.1*move direciton or +\/- u*tan(throw_dir) accordingly\n    optimal_action_list['v'] = np.where(optimal_action_list['Action']==\"MOVE\", optimal_action_list['move_y']*arrow_scale, \n                                        np.where(optimal_action_list['throw_dir_2']==0, arrow_scale,np.where(optimal_action_list['throw_dir_2']==180, -arrow_scale,\n                                        np.where(optimal_action_list['throw_dir_2']==90, 0,np.where(optimal_action_list['throw_dir_2']==270, 0,\n                                        optimal_action_list['u']\/np.tan(np.deg2rad(optimal_action_list['throw_dir_2'].astype(np.float64))))))))\n\n\n\n    x = optimal_action_list['state_x']\n    y = optimal_action_list['state_y']\n    u = optimal_action_list['u'].values\n    v = optimal_action_list['v'].values\n    plt.figure(figsize=(10, 10))\n    plt.quiver(x,y,u,v,scale=0.5,scale_units='inches')\n    sns.scatterplot( x=\"state_x\", y=\"state_y\", data=optimal_action_list,  hue='Action')\n    plt.title(\"Optimal Policy for Given Probabilities for iteration \" +str(repeats))\n    #plt.savefig('E:\\\\Documents\\\\RL\\\\RL from scratch v2\\\\QuiverPlots\\\\'+str(repeats)+'.png')   # save the figure to file\n    plt.close() \n\n    ###\n    input_table = state_sub_full.copy()\n    print(\"Repeats completed: \", np.round((repeats+1)\/num_repeats,2)*100, \"%\")\n    \nend_time = time.time()\n\nprint(\"total time taken this loop: \", np.round((end_time - start_time)\/60,2), \" minutes\")","938f95da":"plt.plot(range(0,len(output_metric_table)), output_metric_table['mean_V'])\nplt.title(\"Mean Q for all State-Action Pairs for each Update \")\nplt.show()","d4799c0e":"# Create Quiver plot showing current optimal policy in one cell\narrow_scale = 0.1\n\nQ_table_VI_3 = state_sub_full.copy()\n\noptimal_action_list = pd.DataFrame()\nfor x in range(0,21):\n    state_x = int(-10 + x)\n    for y in range(0,21):\n        state_y = int(-10 + y)\n        \n        Q_table_VI_3\n        \n        optimal_action = pd.DataFrame({'state_x':state_x, 'state_y': state_y, \n                                      'move_dir': Q_table_VI_3[ (Q_table_VI_3['state_x']==state_x) & (Q_table_VI_3['state_y']==state_y) &  (Q_table_VI_3['Q'] == max(Q_table_VI_3[(Q_table_VI_3['state_x']==state_x) & \n                                                      (Q_table_VI_3['state_y']==state_y)]['Q']))].reset_index(drop=True)['move_dir'][0],\n                                      'throw_dir': Q_table_VI_3[ (Q_table_VI_3['state_x']==state_x) & (Q_table_VI_3['state_y']==state_y) &  (Q_table_VI_3['Q'] == max(Q_table_VI_3[(Q_table_VI_3['state_x']==state_x) & \n                                                      (Q_table_VI_3['state_y']==state_y)]['Q']))].reset_index(drop=True)['throw_dir'][0]},\n                                     index = [state_y])\n        optimal_action_list = optimal_action_list.append(optimal_action)\noptimal_action_list = optimal_action_list.reset_index(drop=True)\n\noptimal_action_list['Action'] = np.where( optimal_action_list['move_dir'] == 'none', 'THROW', 'MOVE'  )\n\n\noptimal_action_list['move_x'] = np.where(optimal_action_list['move_dir'] == 0, int(0),\n                                         np.where(optimal_action_list['move_dir'] == 1, int(1),\n                                         np.where(optimal_action_list['move_dir'] == 2, int(1),\n                                         np.where(optimal_action_list['move_dir'] == 3, int(1),\n                                         np.where(optimal_action_list['move_dir'] == 4, int(0),\n                                         np.where(optimal_action_list['move_dir'] == 5, int(-1),\n                                         np.where(optimal_action_list['move_dir'] == 6, int(-1),\n                                         np.where(optimal_action_list['move_dir'] == 7, int(-1),\n                                         int(-1000)\n                                        ))))))))\noptimal_action_list['move_y'] = np.where(optimal_action_list['move_dir'] == 0, int(1),\n                                         np.where(optimal_action_list['move_dir'] == 1, int(1),\n                                         np.where(optimal_action_list['move_dir'] == 2, int(0),\n                                         np.where(optimal_action_list['move_dir'] == 3, int(-1),\n                                         np.where(optimal_action_list['move_dir'] == 4, int(-1),\n                                         np.where(optimal_action_list['move_dir'] == 5, int(-1),\n                                         np.where(optimal_action_list['move_dir'] == 6, int(0),\n                                         np.where(optimal_action_list['move_dir'] == 7, int(1),\n                                         int(-1000)\n                                        ))))))))\noptimal_action_list['throw_dir_2'] = np.where(optimal_action_list['throw_dir']==\"none\",int(-1000), optimal_action_list['throw_dir'])\n\n# Define horizontal arrow component as 0.1*move direction or 0.1\/-0.1 depending on throw direction\noptimal_action_list['u'] = np.where(optimal_action_list['Action']==\"MOVE\", optimal_action_list['move_x']*arrow_scale,\n                                    np.where(optimal_action_list['throw_dir_2']==0, 0,np.where(optimal_action_list['throw_dir_2']==180, 0,\n                                    np.where(optimal_action_list['throw_dir_2']==90, arrow_scale ,np.where(optimal_action_list['throw_dir_2']==270, -arrow_scale,\n                                    np.where(optimal_action_list['throw_dir_2']<180, arrow_scale,-arrow_scale))))))\n\n# Define vertical arrow component based 0.1*move direciton or +\/- u*tan(throw_dir) accordingly\noptimal_action_list['v'] = np.where(optimal_action_list['Action']==\"MOVE\", optimal_action_list['move_y']*arrow_scale, \n                                    np.where(optimal_action_list['throw_dir_2']==0, arrow_scale,np.where(optimal_action_list['throw_dir_2']==180, -arrow_scale,\n                                    np.where(optimal_action_list['throw_dir_2']==90, 0,np.where(optimal_action_list['throw_dir_2']==270, 0,\n                                    optimal_action_list['u']\/np.tan(np.deg2rad(optimal_action_list['throw_dir_2'].astype(np.float64))))))))\n\nx = optimal_action_list['state_x']\ny = optimal_action_list['state_y']\nu = optimal_action_list['u'].values\nv = optimal_action_list['v'].values\nplt.figure(figsize=(10, 10))\nplt.quiver(x,y,u,v,scale=0.5,scale_units='inches')\nsns.scatterplot( x=\"state_x\", y=\"state_y\", data=optimal_action_list,  hue='Action')\nplt.title(\"Optimal Policy for Given Probabilities\")\nplt.show()\n\n","e267489a":"optimal_action_list [ (optimal_action_list['state_x']==-10) & (optimal_action_list['state_y']==0)].head()","e8e86e50":"Q_table_VI_3 [ (Q_table_VI_3['state_x']==-5) & (Q_table_VI_3['state_y']==-5)].sort_values('Q', ascending=False).head()","5f670f8a":"# Create Quiver plot showing current optimal policy in one cell\narrow_scale = 0.1\n\nQ_table_VI_3 = state_sub_full.copy()\n\noptimal_action_list = pd.DataFrame()\nfor x in range(0,21):\n    state_x = int(-10 + x)\n    for y in range(0,21):\n        state_y = int(-10 + y)\n        \n        Q_table_VI_3\n        \n        for i in range(0,len(Q_table_VI_3[ (Q_table_VI_3['state_x']==state_x) & (Q_table_VI_3['state_y']==state_y) &  (Q_table_VI_3['Q'] == max(Q_table_VI_3[(Q_table_VI_3['state_x']==state_x) & \n                                                      (Q_table_VI_3['state_y']==state_y)]['Q']))].reset_index(drop=True)['move_dir'])):\n            optimal_action = pd.DataFrame({'state_x':state_x, 'state_y': state_y, \n                                          'move_dir': Q_table_VI_3[ (Q_table_VI_3['state_x']==state_x) & (Q_table_VI_3['state_y']==state_y) &  (Q_table_VI_3['Q'] == max(Q_table_VI_3[(Q_table_VI_3['state_x']==state_x) & \n                                                          (Q_table_VI_3['state_y']==state_y)]['Q']))].reset_index(drop=True)['move_dir'][i],\n                                          'throw_dir': Q_table_VI_3[ (Q_table_VI_3['state_x']==state_x) & (Q_table_VI_3['state_y']==state_y) &  (Q_table_VI_3['Q'] == max(Q_table_VI_3[(Q_table_VI_3['state_x']==state_x) & \n                                                          (Q_table_VI_3['state_y']==state_y)]['Q']))].reset_index(drop=True)['throw_dir'][0]},\n                                         index = [state_y])\n            optimal_action_list = optimal_action_list.append(optimal_action)\noptimal_action_list = optimal_action_list.reset_index(drop=True)\n\noptimal_action_list['Action'] = np.where( optimal_action_list['move_dir'] == 'none', 'THROW', 'MOVE'  )\n\n\noptimal_action_list['move_x'] = np.where(optimal_action_list['move_dir'] == 0, int(0),\n                                         np.where(optimal_action_list['move_dir'] == 1, int(1),\n                                         np.where(optimal_action_list['move_dir'] == 2, int(1),\n                                         np.where(optimal_action_list['move_dir'] == 3, int(1),\n                                         np.where(optimal_action_list['move_dir'] == 4, int(0),\n                                         np.where(optimal_action_list['move_dir'] == 5, int(-1),\n                                         np.where(optimal_action_list['move_dir'] == 6, int(-1),\n                                         np.where(optimal_action_list['move_dir'] == 7, int(-1),\n                                         int(-1000)\n                                        ))))))))\noptimal_action_list['move_y'] = np.where(optimal_action_list['move_dir'] == 0, int(1),\n                                         np.where(optimal_action_list['move_dir'] == 1, int(1),\n                                         np.where(optimal_action_list['move_dir'] == 2, int(0),\n                                         np.where(optimal_action_list['move_dir'] == 3, int(-1),\n                                         np.where(optimal_action_list['move_dir'] == 4, int(-1),\n                                         np.where(optimal_action_list['move_dir'] == 5, int(-1),\n                                         np.where(optimal_action_list['move_dir'] == 6, int(0),\n                                         np.where(optimal_action_list['move_dir'] == 7, int(1),\n                                         int(-1000)\n                                        ))))))))\noptimal_action_list['throw_dir_2'] = np.where(optimal_action_list['throw_dir']==\"none\",int(-1000), optimal_action_list['throw_dir'])\n\n# Define horizontal arrow component as 0.1*move direction or 0.1\/-0.1 depending on throw direction\noptimal_action_list['u'] = np.where(optimal_action_list['Action']==\"MOVE\", optimal_action_list['move_x']*arrow_scale,\n                                    np.where(optimal_action_list['throw_dir_2']==0, 0,np.where(optimal_action_list['throw_dir_2']==180, 0,\n                                    np.where(optimal_action_list['throw_dir_2']==90, arrow_scale ,np.where(optimal_action_list['throw_dir_2']==270, -arrow_scale,\n                                    np.where(optimal_action_list['throw_dir_2']<180, arrow_scale,-arrow_scale))))))\n\n# Define vertical arrow component based 0.1*move direciton or +\/- u*tan(throw_dir) accordingly\noptimal_action_list['v'] = np.where(optimal_action_list['Action']==\"MOVE\", optimal_action_list['move_y']*arrow_scale, \n                                    np.where(optimal_action_list['throw_dir_2']==0, arrow_scale,np.where(optimal_action_list['throw_dir_2']==180, -arrow_scale,\n                                    np.where(optimal_action_list['throw_dir_2']==90, 0,np.where(optimal_action_list['throw_dir_2']==270, 0,\n                                    optimal_action_list['u']\/np.tan(np.deg2rad(optimal_action_list['throw_dir_2'].astype(np.float64))))))))\n\nx = optimal_action_list['state_x']\ny = optimal_action_list['state_y']\nu = optimal_action_list['u'].values\nv = optimal_action_list['v'].values\nplt.figure(figsize=(10, 10))\nplt.quiver(x,y,u,v,scale=0.5,scale_units='inches')\nsns.scatterplot( x=\"state_x\", y=\"state_y\", data=optimal_action_list,  hue='Action', alpha = 0.3)\nplt.title(\"Optimal Policy for Given Probabilities\")\nplt.show()\n\n","a967becc":"input_table = Q_table_VI.copy()\ngamma = 0.8\nnum_repeats = 10\n\nstart_time = time.time()\n\noutput_metric_table = pd.DataFrame()\n# Repeat until converges\nfor repeats in range(0,num_repeats):\n    clear_output(wait=True)\n    state_sub_full = pd.DataFrame()\n    \n    \n    output_metric_table = output_metric_table.append(pd.DataFrame({'mean_Q':input_table['Q'].mean(), \n                                                                   'sum_Q': input_table['Q'].sum(),\n                                                                   'mean_V':input_table[['state_x', 'state_y','V']].drop_duplicates(['state_x', 'state_y', 'V'])['V'].mean(),\n                                                                   'sum_V': input_table[['state_x', 'state_y','V']].drop_duplicates(['state_x', 'state_y', 'V'])['V'].sum()}, index = [repeats]))\n    \n    \n    # Iterate over all states defined by max - min of x times by max - min of y\n    for x in range(0,21):\n        state_x = -10 + x\n        for y in range(0,21):\n            state_y = -10 + y\n\n            state_sub = input_table[ (input_table['state_x']==state_x) & (input_table['state_y']==state_y)]\n            Q_sub_list = pd.DataFrame()\n            for n, action in state_sub.iterrows():\n                # Move action update Q\n                if(action['throw_dir'] == \"none\"):\n                    move_direction = action['move_dir']\n                    #Map this to actual direction and find V(s) for next state\n                    if(move_direction == 0):\n                        move_x = 0\n                        move_y = 1\n                    elif(move_direction == 1):\n                        move_x = 1\n                        move_y = 1\n                    elif(move_direction == 2):\n                        move_x = 1\n                        move_y = 0\n                    elif(move_direction == 3):\n                        move_x = 1\n                        move_y = -1\n                    elif(move_direction == 4):\n                        move_x = 0\n                        move_y = -1\n                    elif(move_direction == 5):\n                        move_x = -1\n                        move_y = -1\n                    elif(move_direction == 6):\n                        move_x = -1\n                        move_y = 0\n                    elif(move_direction == 7):\n                        move_x = -1\n                        move_y = 1\n                    Q = 1*(action['reward'] + gamma*max(input_table[ (input_table['state_x']==int(state_x+move_x)) & (input_table['state_y']==int(state_y+move_y))]['V']) )\n                # Throw update Q +1 if sucessful throw or -1 if failed\n                else:\n                    Q = (action['prob']*(action['reward'] + gamma*1)) +  ((1-action['prob'])*(action['reward'] + gamma*-1))\n                Q_sub_list = Q_sub_list.append(pd.DataFrame({'Q':Q}, index = [n]))\n            state_sub['Q'] = Q_sub_list['Q']\n            state_sub['V'] = max(state_sub['Q'])\n            state_sub_full = state_sub_full.append(state_sub)\n            \n    \n    \n    ###\n    # Create Quiver plot showing current optimal policy in one cell\n    arrow_scale = 0.1\n\n    Q_table_VI_3 = state_sub_full.copy()\n\n    optimal_action_list = pd.DataFrame()\n    for x in range(0,21):\n        state_x = int(-10 + x)\n        for y in range(0,21):\n            state_y = int(-10 + y)\n\n            Q_table_VI_3\n\n            for i in range(0,len(Q_table_VI_3[ (Q_table_VI_3['state_x']==state_x) & (Q_table_VI_3['state_y']==state_y) &  (Q_table_VI_3['Q'] == max(Q_table_VI_3[(Q_table_VI_3['state_x']==state_x) & \n                                                      (Q_table_VI_3['state_y']==state_y)]['Q']))].reset_index(drop=True)['move_dir'])):\n                optimal_action = pd.DataFrame({'state_x':state_x, 'state_y': state_y, \n                                              'move_dir': Q_table_VI_3[ (Q_table_VI_3['state_x']==state_x) & (Q_table_VI_3['state_y']==state_y) &  (Q_table_VI_3['Q'] == max(Q_table_VI_3[(Q_table_VI_3['state_x']==state_x) & \n                                                              (Q_table_VI_3['state_y']==state_y)]['Q']))].reset_index(drop=True)['move_dir'][i],\n                                              'throw_dir': Q_table_VI_3[ (Q_table_VI_3['state_x']==state_x) & (Q_table_VI_3['state_y']==state_y) &  (Q_table_VI_3['Q'] == max(Q_table_VI_3[(Q_table_VI_3['state_x']==state_x) & \n                                                              (Q_table_VI_3['state_y']==state_y)]['Q']))].reset_index(drop=True)['throw_dir'][0]},\n                                             index = [state_y])\n                optimal_action_list = optimal_action_list.append(optimal_action)\n    optimal_action_list = optimal_action_list.reset_index(drop=True)\n\n    optimal_action_list['Action'] = np.where( optimal_action_list['move_dir'] == 'none', 'THROW', 'MOVE'  )\n\n\n    optimal_action_list['move_x'] = np.where(optimal_action_list['move_dir'] == 0, int(0),\n                                             np.where(optimal_action_list['move_dir'] == 1, int(1),\n                                             np.where(optimal_action_list['move_dir'] == 2, int(1),\n                                             np.where(optimal_action_list['move_dir'] == 3, int(1),\n                                             np.where(optimal_action_list['move_dir'] == 4, int(0),\n                                             np.where(optimal_action_list['move_dir'] == 5, int(-1),\n                                             np.where(optimal_action_list['move_dir'] == 6, int(-1),\n                                             np.where(optimal_action_list['move_dir'] == 7, int(-1),\n                                             int(-1000)\n                                            ))))))))\n    optimal_action_list['move_y'] = np.where(optimal_action_list['move_dir'] == 0, int(1),\n                                             np.where(optimal_action_list['move_dir'] == 1, int(1),\n                                             np.where(optimal_action_list['move_dir'] == 2, int(0),\n                                             np.where(optimal_action_list['move_dir'] == 3, int(-1),\n                                             np.where(optimal_action_list['move_dir'] == 4, int(-1),\n                                             np.where(optimal_action_list['move_dir'] == 5, int(-1),\n                                             np.where(optimal_action_list['move_dir'] == 6, int(0),\n                                             np.where(optimal_action_list['move_dir'] == 7, int(1),\n                                             int(-1000)\n                                            ))))))))\n    optimal_action_list['throw_dir_2'] = np.where(optimal_action_list['throw_dir']==\"none\",int(-1000), optimal_action_list['throw_dir'])\n\n    # Define horizontal arrow component as 0.1*move direction or 0.1\/-0.1 depending on throw direction\n    optimal_action_list['u'] = np.where(optimal_action_list['Action']==\"MOVE\", optimal_action_list['move_x']*arrow_scale,\n                                        np.where(optimal_action_list['throw_dir_2']==0, 0,np.where(optimal_action_list['throw_dir_2']==180, 0,\n                                        np.where(optimal_action_list['throw_dir_2']==90, arrow_scale ,np.where(optimal_action_list['throw_dir_2']==270, -arrow_scale,\n                                        np.where(optimal_action_list['throw_dir_2']<180, arrow_scale,-arrow_scale))))))\n\n    # Define vertical arrow component based 0.1*move direciton or +\/- u*tan(throw_dir) accordingly\n    optimal_action_list['v'] = np.where(optimal_action_list['Action']==\"MOVE\", optimal_action_list['move_y']*arrow_scale, \n                                        np.where(optimal_action_list['throw_dir_2']==0, arrow_scale,np.where(optimal_action_list['throw_dir_2']==180, -arrow_scale,\n                                        np.where(optimal_action_list['throw_dir_2']==90, 0,np.where(optimal_action_list['throw_dir_2']==270, 0,\n                                        optimal_action_list['u']\/np.tan(np.deg2rad(optimal_action_list['throw_dir_2'].astype(np.float64))))))))\n\n\n\n    x = optimal_action_list['state_x']\n    y = optimal_action_list['state_y']\n    u = optimal_action_list['u'].values\n    v = optimal_action_list['v'].values\n    plt.figure(figsize=(10, 10))\n    plt.quiver(x,y,u,v,scale=0.5,scale_units='inches')\n    sns.scatterplot( x=\"state_x\", y=\"state_y\", data=optimal_action_list,  hue='Action', alpha = 0.5)\n    plt.title(\"Optimal Policy for Given Probabilities for iteration \" +str(repeats))\n    #plt.savefig('E:\\\\Documents\\\\RL\\\\RL from scratch v2\\\\QuiverPlots\\\\'+str(repeats)+'.png')   # save the figure to file\n    plt.close() \n\n    ###\n    input_table = state_sub_full.copy()\n    print(\"Repeats completed: \", np.round((repeats+1)\/num_repeats,2)*100, \"%\")\n    \nend_time = time.time()\n\nprint(\"total time taken this loop: \", np.round((end_time - start_time)\/60,2), \" minutes\")","979a5642":"# SAVE OPTIMAL POLICY FOR LATER COMPARISON\n# This is the .csv saved as the dataset and will be used when we apply RL algorithms in later outputs for comparison\nOptimal_Policy_VI = optimal_action_list.copy()\n#Optimal_Policy_VI.to_csv('E:\\Documents\\RL\\RL from scratch v2\\OptimalPolicy_angletol=45.csv')","d2c18931":"input_table = Q_table_VI.copy()\ninput_table['reward'] = -0.05\ngamma = 0.8\nnum_repeats = 10\n\nstart_time = time.time()\n\noutput_metric_table = pd.DataFrame()\n# Repeat until converges\nfor repeats in range(0,num_repeats):\n    clear_output(wait=True)\n    state_sub_full = pd.DataFrame()\n    \n    \n    output_metric_table = output_metric_table.append(pd.DataFrame({'mean_Q':input_table['Q'].mean(), \n                                                                   'sum_Q': input_table['Q'].sum(),\n                                                                   'mean_V':input_table[['state_x', 'state_y','V']].drop_duplicates(['state_x', 'state_y', 'V'])['V'].mean(),\n                                                                   'sum_V': input_table[['state_x', 'state_y','V']].drop_duplicates(['state_x', 'state_y', 'V'])['V'].sum()}, index = [repeats]))\n    \n    \n    # Iterate over all states defined by max - min of x times by max - min of y\n    for x in range(0,21):\n        state_x = -10 + x\n        for y in range(0,21):\n            state_y = -10 + y\n\n            state_sub = input_table[ (input_table['state_x']==state_x) & (input_table['state_y']==state_y)]\n            Q_sub_list = pd.DataFrame()\n            for n, action in state_sub.iterrows():\n                # Move action update Q\n                if(action['throw_dir'] == \"none\"):\n                    move_direction = action['move_dir']\n                    #Map this to actual direction and find V(s) for next state\n                    if(move_direction == 0):\n                        move_x = 0\n                        move_y = 1\n                    elif(move_direction == 1):\n                        move_x = 1\n                        move_y = 1\n                    elif(move_direction == 2):\n                        move_x = 1\n                        move_y = 0\n                    elif(move_direction == 3):\n                        move_x = 1\n                        move_y = -1\n                    elif(move_direction == 4):\n                        move_x = 0\n                        move_y = -1\n                    elif(move_direction == 5):\n                        move_x = -1\n                        move_y = -1\n                    elif(move_direction == 6):\n                        move_x = -1\n                        move_y = 0\n                    elif(move_direction == 7):\n                        move_x = -1\n                        move_y = 1\n                    Q = 1*(action['reward'] + gamma*max(input_table[ (input_table['state_x']==int(state_x+move_x)) & (input_table['state_y']==int(state_y+move_y))]['V']) )\n                # Throw update Q +1 if sucessful throw or -1 if failed\n                else:\n                    Q = (action['prob']*(action['reward'] + gamma*1)) +  ((1-action['prob'])*(action['reward'] + gamma*-1))\n                Q_sub_list = Q_sub_list.append(pd.DataFrame({'Q':Q}, index = [n]))\n            state_sub['Q'] = Q_sub_list['Q']\n            state_sub['V'] = max(state_sub['Q'])\n            state_sub_full = state_sub_full.append(state_sub)\n            \n    input_table = state_sub_full.copy()\n    print(\"Repeats completed: \", np.round((repeats+1)\/num_repeats,2)*100, \"%\")\n    \nend_time = time.time()\n\nprint(\"total time taken this loop: \", np.round((end_time - start_time)\/60,2), \" minutes\")","1ce2cc3d":"plt.plot(range(0,len(output_metric_table)), output_metric_table['mean_V'])\nplt.title(\"Mean Q for all State-Action Pairs for each Update \")\nplt.show()","e3a8ec04":"# Create Quiver plot showing current optimal policy in one cell\narrow_scale = 0.1\n\nQ_table_VI_3 = state_sub_full.copy()\n\noptimal_action_list = pd.DataFrame()\nfor x in range(0,21):\n    state_x = int(-10 + x)\n    for y in range(0,21):\n        state_y = int(-10 + y)\n        \n        Q_table_VI_3\n        \n        for i in range(0,len(Q_table_VI_3[ (Q_table_VI_3['state_x']==state_x) & (Q_table_VI_3['state_y']==state_y) &  (Q_table_VI_3['Q'] == max(Q_table_VI_3[(Q_table_VI_3['state_x']==state_x) & \n                                                      (Q_table_VI_3['state_y']==state_y)]['Q']))].reset_index(drop=True)['move_dir'])):\n            optimal_action = pd.DataFrame({'state_x':state_x, 'state_y': state_y, \n                                          'move_dir': Q_table_VI_3[ (Q_table_VI_3['state_x']==state_x) & (Q_table_VI_3['state_y']==state_y) &  (Q_table_VI_3['Q'] == max(Q_table_VI_3[(Q_table_VI_3['state_x']==state_x) & \n                                                          (Q_table_VI_3['state_y']==state_y)]['Q']))].reset_index(drop=True)['move_dir'][i],\n                                          'throw_dir': Q_table_VI_3[ (Q_table_VI_3['state_x']==state_x) & (Q_table_VI_3['state_y']==state_y) &  (Q_table_VI_3['Q'] == max(Q_table_VI_3[(Q_table_VI_3['state_x']==state_x) & \n                                                          (Q_table_VI_3['state_y']==state_y)]['Q']))].reset_index(drop=True)['throw_dir'][0]},\n                                         index = [state_y])\n            optimal_action_list = optimal_action_list.append(optimal_action)\noptimal_action_list = optimal_action_list.reset_index(drop=True)\n\noptimal_action_list['Action'] = np.where( optimal_action_list['move_dir'] == 'none', 'THROW', 'MOVE'  )\n\n\noptimal_action_list['move_x'] = np.where(optimal_action_list['move_dir'] == 0, int(0),\n                                         np.where(optimal_action_list['move_dir'] == 1, int(1),\n                                         np.where(optimal_action_list['move_dir'] == 2, int(1),\n                                         np.where(optimal_action_list['move_dir'] == 3, int(1),\n                                         np.where(optimal_action_list['move_dir'] == 4, int(0),\n                                         np.where(optimal_action_list['move_dir'] == 5, int(-1),\n                                         np.where(optimal_action_list['move_dir'] == 6, int(-1),\n                                         np.where(optimal_action_list['move_dir'] == 7, int(-1),\n                                         int(-1000)\n                                        ))))))))\noptimal_action_list['move_y'] = np.where(optimal_action_list['move_dir'] == 0, int(1),\n                                         np.where(optimal_action_list['move_dir'] == 1, int(1),\n                                         np.where(optimal_action_list['move_dir'] == 2, int(0),\n                                         np.where(optimal_action_list['move_dir'] == 3, int(-1),\n                                         np.where(optimal_action_list['move_dir'] == 4, int(-1),\n                                         np.where(optimal_action_list['move_dir'] == 5, int(-1),\n                                         np.where(optimal_action_list['move_dir'] == 6, int(0),\n                                         np.where(optimal_action_list['move_dir'] == 7, int(1),\n                                         int(-1000)\n                                        ))))))))\noptimal_action_list['throw_dir_2'] = np.where(optimal_action_list['throw_dir']==\"none\",int(-1000), optimal_action_list['throw_dir'])\n\n# Define horizontal arrow component as 0.1*move direction or 0.1\/-0.1 depending on throw direction\noptimal_action_list['u'] = np.where(optimal_action_list['Action']==\"MOVE\", optimal_action_list['move_x']*arrow_scale,\n                                    np.where(optimal_action_list['throw_dir_2']==0, 0,np.where(optimal_action_list['throw_dir_2']==180, 0,\n                                    np.where(optimal_action_list['throw_dir_2']==90, arrow_scale ,np.where(optimal_action_list['throw_dir_2']==270, -arrow_scale,\n                                    np.where(optimal_action_list['throw_dir_2']<180, arrow_scale,-arrow_scale))))))\n\n# Define vertical arrow component based 0.1*move direciton or +\/- u*tan(throw_dir) accordingly\noptimal_action_list['v'] = np.where(optimal_action_list['Action']==\"MOVE\", optimal_action_list['move_y']*arrow_scale, \n                                    np.where(optimal_action_list['throw_dir_2']==0, arrow_scale,np.where(optimal_action_list['throw_dir_2']==180, -arrow_scale,\n                                    np.where(optimal_action_list['throw_dir_2']==90, 0,np.where(optimal_action_list['throw_dir_2']==270, 0,\n                                    optimal_action_list['u']\/np.tan(np.deg2rad(optimal_action_list['throw_dir_2'].astype(np.float64))))))))\n\nx = optimal_action_list['state_x']\ny = optimal_action_list['state_y']\nu = optimal_action_list['u'].values\nv = optimal_action_list['v'].values\nplt.figure(figsize=(10, 10))\nplt.quiver(x,y,u,v,scale=0.5,scale_units='inches')\nsns.scatterplot( x=\"state_x\", y=\"state_y\", data=optimal_action_list,  hue='Action', alpha = 0.5)\nplt.title(\"Optimal Policy for Given Probabilities\")\nplt.show()\n\n","5c214698":"![Enironment Demo](https:\/\/i.imgur.com\/3woVbKI.png)","e56688ab":"### 2.4 Increasing the number of iterations\n\nWe now re-apply the value-iteration algorithm and the subsequent evaluation but increase the number of repeats from 5 to 10 and find that our policy coverges to the optimal one within this. \n\n#### To demonstrate the change over each iteration further, I have included an export of the quiver plot analysis for each iteration and combined these into a small animation. The export is commented out in this published notebook but can be used in your own.\n\n","3d913e7b":"# Reinforcement Learning from Scratch Part 1: Finding the Optimal Policy of an Environment Fully Defined within a Python Notebook\n\n## Solving an Example Task of Throwing Paper into a Bin\n\n\nThis notebook attempts to solve a basic task of throwing paper into a bin using reinforcement learning. In this problem, we may throw from any position in the room but the probability of it is relative to the current distance from the bin and the direction in which the paper is thrown. Therefore the actions available are to throw the paper in any 360 degree direction or move to a new position to try and increase the probability that a throw made will go into the bin.\n\nWe first introduce the problem where the bin's location is known and can be solved directly with Value-Itearation methods before showing how RL can be used similarly to find the optimal policy if the probabilities are hidden in later notebooks.\n\nFurthermore, we introduce the option to add control to the environment where, for example, we can punish the algorithm less for missed throws so that the algorithm will take higher risks.\n\nLastly, we demonstrate how the envrionment can be changed and, for example, may have walls blocking throws from certain positions. \n\nI will also be publishing an accompanying article explaining the process without code that can be found on my website or Medium page:\n\nhttps:\/\/www.philiposbornedata.com\/\n\nhttps:\/\/medium.com\/@philiposbornedata\n\n","41a507f7":"#### Improving Visualisation of Optimal Policy\n\nAlthough the chart shows whether the optimal action is either a throw or move it doesn't show us which direction these are in. Therefore, we will map each optimal action to a vector of u and v and use these to create a quiver plot (https:\/\/matplotlib.org\/api\/_as_gen\/matplotlib.axes.Axes.quiver.html). \n\nFirst, we map the move direction to its x and y components and set the actions which are throwing (currently labelled as \"none\") in column to a very large negative integer so we do not have issues when we want to scale the values in the column by a factor. If we didnt do this we would recieve an error as we would be trying to divide a string element by a number and this isn't possible. We repeat this for the throw direction column as well.\n\nWe then define the scale of the arrows and use this to define the horizontal component labelled u. For movement actions, we simply multiply the movement in the x direction by this factor and for the throw direction we either move 1 unit left or right (accounting for no horizontal movement for 0 or 180 degrees and no vertical movement at 90 or 270 degrees). \n\nThe horizontal component is then used to calculate the vertical component with some basic trigonometry where we again account for certain angles that would cause errors in the calculations. ","b0087c87":"### 2.2 Initialise State-Action Pairs\nBefore applying the algorithm, we intialise each state-action value into a table. First we formthis for all throwing actions then all moving actions. \n\nWe can throw in any direction and therefore there are 360 actions for each degree starting from north as 0 clockwise to 359 degrees. \n\nAlthough movement may seem simpler in that there are 8 possible actions (north, north east, east, etc) there are complications in that unlike being able to throw in any direction from any position, there are some movements that aren't possible. For example, if we are at the edge of the room, we cannot move beyong the boundary and this needs to be accounted for. Although this could be coded nicer, I have done this manually with the if\/elif statements shown that skips the row if the position and movement is not possible. \n","dbcc892e":"#### Save Optimal Policy for Later Comparison\n\nThis final policy is the one we will consider optimal","09adea74":"#### We can combine the previous code for creating the quiver plot into one code cell","df1e1590":"If we use this to calculate the previous examples, we find the same results:","acfb8172":"#### Extra Code Features: Tracking loop progress and run-time\nTo improve our code, we introduce two useful tools for keep track of the run time. First, we import 'time' and then use this to calculate how long the Value Iteration algorithm thats to run for the given inputs.\n\nSecondly, which I have found extremely useful for algorithms that take more than a few minutes to run, is to introduce a simple way of tracking the current progress. In short, we print the current iteration and clear this output after each stage using the second import. More info can be found here: https:\/\/www.philiposbornedata.com\/2018\/06\/28\/the-simplest-cleanest-method-for-tracking-a-for-loops-progress-and-expected-run-time-in-python-notebooks\/","c471e64c":"## Part 2: Optimal Policy for Environment with Known Probabilities\n\n### 2.1 Model-based Methods\nThe aim is for us to find the optimal action in each state by either throwing or moving in a given direction. Because we have known probabilities, we can actually use model-based methods and will demonstrate this first and can use value iteration to achieve this via the following formula:\n\n\\begin{equation}\n    Q_{k+1}(s,a) =  \\sum^{s'}{P(s'|s,a) (R(s,a,s')+ \u03b3Vk(s'))} \\ for \\ k \u2265 0\n\\end{equation}\nwhere\n\\begin{equation}\n    V_k(s) = max_a \\ Qk(s,a) \\ for \\ k>0.\n\\end{equation}\n\n$P(s'|s,a)$ is the probability of reaching the next state given the current state and action. We have defined this as 1 for movement actions and calculated by the probability function otherwise.\n\n$R(s,a,s')$ corresponds to the reward for reaching the next state given the current state and action. \n\nGamma will effect what our algorithm values more important whether it be short of long term rewards and a value close to 1 will value long term rewards more.\n\nValue iteration starts with an arbitrary function V0 and uses the following equations to get the functions for k+1 stages to go from the functions for k stages to go (https:\/\/artint.info\/html\/ArtInt_227.html).\n\n","efe4b264":"### 1.4 Probability Function \n\nWith this diagram in mind, we create a function that calculates the probability of a throw's success from only given position relative to the bin.\n\nFirst, if the position is the same as the bin (i.e. the person is directly inside the bin already) then the probability is fixed to 100%. \n\nNext, we have to re-define the throwing direction in two cases to accomodate for the fact that 360 degrees is the same as 0 degrees. For example, if we are south-west of the bin and throw 350 degrees, this would be the same as -10 degrees and would then relate to a bearing from the person to the bin less than 90 correctly.\n\nThen the euclidean distance is calculated followed by the max distance a person could be from the bin.\n\nWe then calculate the bearing from the person to the bin following the previous figure and calcualte the score bounded within a +\/- 45 degree window. Throws that are closest to the true bearing score higher whilst those further away score less, anything more than 45 degrees (or less than -45 degrees) are negative and then set to a zero probability. \n\nLastly, the overall probability is related to both the distance and direction given the current position. ","d361c03e":"#### This figure shows the optimal policy defining the best action at each state. \n\nThis may seem unusual at first as, for example, state (-10,0) seems as though it should move directly east if it wants to reach the goal faster but if we count the number of steps, as shown below, this is equal.\n\nIn fact, there are a number of optimal actions for many of the move states. Any state nots in the diagonal streams can move in any direction that tends towards to bin and still be optimal. i.e. state (-10,0) can move north-east, east or south-east and still be an optimal action. We have currently simply chosen the first one that appears but if we correct this we have the complete optimal policy shown below.\n\nFurthermore, there may be some states where moving is just as optimal as throwing and we therefore reduce the alpha of each point's colour in an attempt to visualise those that have both.","afaa24a0":"![Imgur](https:\/\/i.imgur.com\/ThYVmiG.gif[](http:\/\/))","29bf5a69":"## Part 1: Defining the Environment\n\nThe environment's probabilities are calcualted based on the direction in which the paper is thrown and the current distance from the bin. \n\nFor example, in the image below we have three people labelled A, B and C. A and B both throw in the correct direction but person A is closer than B and so will have a higher probability of landing the shot. \n\nPerson C is closer than person B but throws in the completely wrong direction and so will have a very low probability of hitting the bin. This may seem illogical that person C would throw in this direction but, as we will show more later, an algorithm has to try a range of directions first to figure out where the successes are and will have no visual guide as to where the bin is. \n\n\n","a30463f6":"### Conclusion\n\nWe have defined the environment completely within Python and have the optimal policy calculated using value-iteration. The next step is to try and find the optimal policy using RL given we don't actually have the probabilities for each state-action pair. \n\nIn the next notebook, I will show how this can be computed using Q-learning or Monte Carlo methods. ","b652928c":"![Imgur](https:\/\/i.imgur.com\/lCNiiwf.png)","35cb58c1":"To create the environment in python, we convert the diagram into 2-d dimensions of x and y values and use bearing mathematics to calculate the angles thrown. We used normalised x and y values so that they must be between 0 and 10 (or 0 and -10) as whole numbers.\n\n\n### 1.1 Distance Measure\nWe can plot this and show person's A position in this definition to be (-0.5,-0.5). This is their current state and their distance from the bin can be calculated using the Euclidean distance measure:\n\n\\begin{equation}\n distance(State,Bin) = \\sqrt{(State_x-Bin_x)^2 + (State_y-Bin_y)^2}\n\\end{equation}\n\nand in our case, we have:\n\n\\begin{equation}\n distance(A, Bin) = \\sqrt{(-0.5-0)^2 + (-0.5-0)^2} = \\sqrt{0.5} \\approx 0.7071\n\\end{equation}\n\nFor the final calcualtions, we normalise this and reverse the value so that a high score indicates that the person is closer to the target bin:\n\n\n\\begin{equation}\n distance\\_score(State, Bin) = 1-\\frac{distance(State,Bin)}{Max\\_dist(State,Bin)} \n\\end{equation}\n\nBecause we have fixed our 2-d dimensions between (-1, 1), the max posssible distance the person could be is $ \\sqrt{(1) + (1)} = \\sqrt{2}$ from the bin. Therefore our distance score for person A is:\n\n\\begin{equation}\n distance\\_score(A, Bin) = 1-\\frac{\\sqrt{0.5}}{\\sqrt{2}} = 1 - 0.5 = 0.5\n\\end{equation}\n\n\n### 1.2 Direction Measure\n\nPerson A then has a decision to make, do they move or do they throw in a chosen direction. For now, let imagine they choose to throw the paper, their first throw is at 50 degrees and the second is 60 degrees from due north. The direction of the bin from person A can be calculated by simple trigonometry:\n\n\n\\begin{equation}\n direction(A, bin) = tan^{-1}(0.5\/0,5) = 45 \\ degrees\n\\end{equation}\n\nTherefore, the first throw is 5 degrees off the true direction and the second is 15 degrees.\n\nWhen we consider that good throws are bounded by 45 degrees either side of the actual direction (i.e. not throwing the wrong way) then we can use the following to calculate how good this chosen direction is. Any direction beyond the 90 degress will produce a negative value and be mapped to probability of 0:\n\n\\begin{equation}\n dir\\_score(State, Throw\\_action) =  \\frac{45 - |True\\_dir - Actual\\_dir|}{45}  \n\\end{equation}\n\n\\begin{equation}\n dir\\_score(A, Throw_1) = \\frac{ 45 - |45 - 50|}{45} \\approx 0.8889\n\\end{equation}\n\n\\begin{equation}\n dir\\_score(A, Throw_2) =  \\frac{45 - |45 - 60|}{45} \\approx 0.6667  \n\\end{equation}\n\n\nBoth are fairly close but their first throw is more likely to hit the bin. \n\n### 1.3 Probability Calculation\n\nWe therefore calculate our probability of a successful throw to be relative to both these measures:\n\n\n\\begin{equation}\n prob\\_success(State, Throw\\_action) = distance\\_score*dir\\_score  \n\\end{equation}\n\n\nWhich we have for person A's first throw to be:\n\n\n\\begin{equation}\n prob\\_success(A, Throw_1) = 0.5*0.9722 \\approx 0.4444  \n\\end{equation}\n\nand the second to be:\n\n\\begin{equation}\n prob\\_success(A, Throw_1) = 0.5*0.9167 \\approx 0.3333\n\\end{equation}\n","348a5487":"#### Analysing Value-Iteration Output\nWe therefore have our output table that shows the quality of each state-action pair and the corresponding V value. \n\nFirst, we need to conisder whether this has converged to the optimal value and can plot the mean Q values for each update. Clearly after just 10 iterations this has not converged and will need to increase this to a suitable value. It took 20 miinutes to run for 10 iterations and so we can assume that it take approximately 2 per iteration.","2d0739c9":"![Imgur](https:\/\/i.imgur.com\/n9aTN4b.png)","be51db75":"#### We repeat the process one last time for completion and save the quiver plot at each step for an animation","520a7237":"#### Finding the Optimal Policy for Given Results\n\nAlthough we know this hasn't fully converged yet, if we assume it has for now we can begin to analyse the results to find the optimal action in any given state. The optimal action is the one that has the highest Q value for th given state and is found for each state in the cell below. ","7492d5e2":"To make things easier for continuous runs, we combine these calculations into a single function. The distance score is calcualted as before and we make sure to account for a bin being placed is other locations.\n\nThe direction score calculations are a little harder to generalise. In our example person A is south-west from the bin and therefore the angle was a simple calculation but if we applied the same to say a person placed north-east then this would be incorrect. Furthermore, because the bin can be placed anywhere we need to first find where the person is relative to this, not just the origin, and then used to to establish to angle calcuation required. \n\nThis is summarised in the diagram below where we have generalised each of the trignomertric calculations based on the person's relative position to the bin:","39971a50":"#### Change reward for each step to be equal to a small negative value. i.e. want to reach goal faster\n\nAside from changing the two previously mentioned parameters, we can also introduce rewards for each state-action pair. We could use this to define some bias in favour or against specific actions or could more generally decide we want the optimal policy to be more risky and increase the value of throwing. \n\nIf, for example, we set the reward of each action equal to a small negative value, our calculations will value throwing slightly less than before but will consider moving even worse in some states. Therefore, more states will be incetivsed to throw instead of move.","2f9802df":"### 2.3 Value-Iteration Optimal Policy\n\nWe start by initialising V(s) for all states, calculate the Q(s,a) matrix from this then update V(s) accordingly. This is repeated back and forth until the results converge. \n\nNext, we calculate the probability of each state-action pair using the function introduced previously if a thrown action or simply 1 if a move action.\n\nWe are now ready to apply the Value-Iteration and introduce the two parameters gamma and the number of iterations\/repeats. Gamma will effect what our algorithm values more important whether it be short of long term rewards and a value close to 1 will value long term rewards more.\n\n*\"Different values of gamma may produce different policies. Lower gamma values will put more weight on short-term gains, whereas higher gamma values will put more weight towards long-term gains. Asymptotically, the closer gamma is to 1, the closer the policy will be to one that optimizes the gains over infinite time. On the other hand, value iteration will be slower to converge.*\n\n*The best gamma depends on your domain. Sometimes it makes sense to look for short term gains (e.g. money gained sooner is actually more valuable than the same amount earned later), other times you want to look as far ahead as you can. And i would say that for a given MDP, there is probably a point (for high values of gamma) where the optimal policies will stabilize (no longer change when you increase gamma even more).\"* https:\/\/stats.stackexchange.com\/questions\/137590\/mdp-value-iteration-choosing-gamma\n\nThe number of iterations required to converge depends entirely on the scale of the problem, we will simply try some reasonable values and then observe the results after to find the optimal value. \n","27f129a2":"#### Plotting Probabilities for Each State\nNow that we have this as a function, we can easily calculate and plot the probabilities of all points in our 2-d grid for a fixed throwing direction. \n\nThe probabilities are defined by the angle we set in the previous function, currently this is 45 degrees but this can reduced or increased if desired and the results will change accordingly. We may also want to scale the proability differently for distances.","ea27984f":"#### Animated Plot for All Throwing Directions\n\nTo demonstrate this further, we can iterate through a number of throwing directions and create an interactive animation. The code becomes a little complex and you can always simply use the prevous code chunk and change the throw_direction manually to explore different positions. However, if you are interested in understanding how the animation is created, you can follow a guide I wrote to do this here:\n\nhttps:\/\/www.philiposbornedata.com\/2018\/03\/01\/creating-interactive-animation-for-parameter-optimisation-using-plot-ly\/\n\n\nI will be using simimlar animations later to demonstrate the effect of varying parameters.\n"}}