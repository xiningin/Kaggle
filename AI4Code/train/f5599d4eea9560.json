{"cell_type":{"de17b8bb":"code","93c7af65":"code","ad83bb60":"code","e8405882":"code","9fab8df9":"code","f443318f":"code","22c761b1":"code","b0df6829":"code","ecb8a74d":"code","224d088b":"code","620fe2c8":"code","56a81b22":"code","76b1fd81":"code","0c0f8741":"code","cfc5b037":"code","f1b35606":"code","9dfab2c2":"code","1d460ddb":"code","9ce49dd1":"code","721b1e95":"code","4d4d6b70":"markdown","313a86f0":"markdown","4f2899a0":"markdown","df31e7c0":"markdown","2f526e89":"markdown","7c5b5b90":"markdown","c57709ef":"markdown","863d4c9f":"markdown","a7164e9b":"markdown","d59a6438":"markdown","d6152ce2":"markdown","29ae9f83":"markdown"},"source":{"de17b8bb":"import copy\nimport gc\nimport glob\nimport numpy as np\nimport pandas as pd\nimport time\n\nimport cupy as cp\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\nimport soundfile as sf\nfrom tqdm.notebook import tqdm\nfrom xgboost.sklearn import XGBClassifier\nfrom xgboost import plot_importance","93c7af65":"trainfiles = sorted(glob.glob( '..\/input\/rfcx-species-audio-detection\/train\/*.flac' ))\ntestfiles = sorted(glob.glob( '..\/input\/rfcx-species-audio-detection\/test\/*.flac' ))\nlen(trainfiles), len(testfiles), trainfiles[0]","ad83bb60":"traint = pd.read_csv( '..\/input\/rfcx-species-audio-detection\/train_tp.csv' )\ntrainf = pd.read_csv( '..\/input\/rfcx-species-audio-detection\/train_fp.csv' )\ntraint.shape, trainf.shape","e8405882":"traint.head()","9fab8df9":"trainf.head()","f443318f":"pyplot.rcParams[\"figure.figsize\"] = (20,10)\ndata, samplerate = sf.read('..\/input\/rfcx-species-audio-detection\/train\/a66546dfd.flac')\ndata = cp.array(data)\nvarfft = cp.abs( cp.fft.fft(data)[:(len(data)\/\/2)] )\nreshaped = cp.asnumpy( varfft.reshape( (1000,1440) ).mean(axis=1) )\n\nprint(f\"len(cp.fft.fft(data)): {len(cp.fft.fft(data))}\")\nprint(f\"len(data): {len(data)}\")\nprint(f\"len(data)\/\/2: {len(data)\/\/2}\")\nprint(f\"varfft.shape: {varfft.shape}\")\nprint(f\"reshaped.shape: {reshaped.shape}\")","22c761b1":"pyplot.plot(range(0, len(varfft)), cp.asnumpy(varfft))","b0df6829":"pyplot.plot(range(0, len(reshaped)), cp.asnumpy(reshaped))","ecb8a74d":"del data, varfft, reshaped","224d088b":"def extract_fft(fn):\n    data, samplerate = sf.read(fn)\n    data = cp.array(data)\n    varfft = cp.abs( cp.fft.fft(data)[:(len(data)\/\/2)] )\n    return cp.asnumpy( varfft.reshape( (1000,1440) ).mean(axis=1) )","620fe2c8":"X_train = []\nfor fn in tqdm(trainfiles):\n    X_train.append( extract_fft(fn) )\nX_train = np.stack(X_train)\ngc.collect()\n\nX_train.shape","56a81b22":"X_test = []\nfor fn in tqdm(testfiles):\n    X_test.append( extract_fft(fn) )\nX_test = np.stack(X_test)\ngc.collect()\n\nX_test.shape","76b1fd81":"tt = traint[['recording_id','species_id']].copy()\ntf = trainf[['recording_id','species_id']].copy()\ntt[\"tp_and_fp\"] = \"tp\"\ntf[\"tp_and_fp\"] = \"fp\"\n# The order is True Positive first, False Positive second\ny_train_all_classes = pd.concat( (tt, tf) )\n\nfor i in range(24):\n    y_train_all_classes['s'+str(i)] = 0\n    # Notice that the False Positive labels should be 0\n    # Will correct them in the helper function\n    y_train_all_classes.loc[y_train_all_classes.species_id==i,'s'+str(i)] = 1\n\ny_train_all_classes.head()","0c0f8741":"def get_unique_tp_and_tn(y_train_all_classes, target_col):\n    \"\"\"\n    Get the recording_id of True Positive and True Negative only\n    \"\"\"\n    df_tp_and_tn = (\n        y_train_all_classes[[\"recording_id\", \"tp_and_fp\", target_col]]\n        # Exclude False Negative ones as they are useless\n        .query(f'tp_and_fp == \"tp\" or {target_col} == 1')\n    )\n    # If they are False Positive, need to correct the label\n    df_tp_and_tn.loc[df_tp_and_tn.tp_and_fp==\"fp\", target_col] = 0\n    df_unique_tp_and_tn = (\n        df_tp_and_tn\n        .groupby(\"recording_id\")\n        .max(target_col)\n    )\n    return df_unique_tp_and_tn\n\n# Example\nget_unique_tp_and_tn(y_train_all_classes, \"s19\").sort_values(\"s19\", ascending=False)","cfc5b037":"X_train = pd.DataFrame(X_train)\nrecording_id = [path[44:53] for path in trainfiles]\nX_train = X_train.set_index(pd.Index(recording_id))\n\nX_test = pd.DataFrame(X_test)","f1b35606":"X_train.head()","9dfab2c2":"sub = pd.DataFrame({'recording_id': [f.split('\/')[-1].split('.')[0] for f in testfiles] })\n\nparams = {\n    \"n_estimators\": [40, 60, 80, 100, 120],\n    \"max_depth\": [1, 2, 3, 5, 8, 13],\n    \"learning_rate\": [0.02, 0.04, 0.08, 0.16, 0.32],\n    \"colsample_bytree\": [0.5, 0.6, 0.7, 0.8, 0.9, 1],\n    \"colsample_bylevel\": [0.5, 0.6, 0.7, 0.8, 0.9, 1],\n    \"colsample_bynode\": [0.5, 0.6, 0.7, 0.8, 0.9, 1],\n    \"subsample\": [0.5, 0.6, 0.7, 0.8, 0.9, 1],\n    \"gamma\": [0, 0.001, 0.005, 0.025, 0.125],\n    \"min_child_weight\": [0, 1, 2, 3, 4, 5],\n    \"max_delta_step\": [0, 1, 2, 3, 4, 5],\n    \"reg_alpha\": [0, 0.001, 0.005, 0.025, 0.125],\n    \"reg_lambda\": [0.9, 1, 1.1, 1.2]\n}\n\nmodel = XGBClassifier(\n    n_jobs=-1,\n    random_state=1,\n    tree_method=\"gpu_hist\",\n    predictor=\"gpu_predictor\"\n)\n\ncv = RandomizedSearchCV(\n    estimator = model, \n    param_distributions = params,\n    n_jobs = -1,\n    n_iter = 32,\n    cv = StratifiedKFold(n_splits=5, random_state=1),\n    return_train_score = False,\n    random_state = 1,\n    refit = True,\n    scoring = \"average_precision\"\n)\n\nfor tgt in range(0,24):\n    # updated each loop\n    starttime = time.time()\n    # extract the corresponding target col\n    y_train = get_unique_tp_and_tn(y_train_all_classes, 's'+str(tgt))\n    # part of the scale_pos_weight can only be specified within the loop\n    params_copy = copy.deepcopy(params)\n    params_copy[\"scale_pos_weight\"] = [4, 8, 16, np.sum(y_train.values==0) \/ np.sum(y_train.values==1)]\n    cv_copy = copy.deepcopy(cv)\n    cv_copy.param_distributions = params_copy\n    # fit\n    cv_copy.fit(\n        X_train.loc[y_train.index],\n        y_train.values.ravel()\n    )\n    print(\"==================================================\")\n    print(f\"best_score of {tgt}: {cv_copy.best_score_}\")\n    print(f\"best_params of {tgt}: {cv_copy.best_params_}\")\n    # plot_importance(cv_copy.best_estimator_, max_num_features = 20, title = \"Feature importance\" + str(tgt))\n    # pyplot.scatter(range(len(cv_copy.best_estimator_.feature_importances_)), cv_copy.best_estimator_.feature_importances_)\n\n    # predict\n    sub['s'+str(tgt)] = cv_copy.best_estimator_.predict_proba(X_test)[:,1]\n    print(f\"{tgt} time: {time.time()-starttime}\")","1d460ddb":"sub.head()","9ce49dd1":"sub.to_csv('submission.csv', index=False)","721b1e95":"!ls","4d4d6b70":"### Turn into dataframe","313a86f0":"After reshaping","4f2899a0":"A helper function","df31e7c0":"Before reshaping","2f526e89":"### y_train","7c5b5b90":"- Original notebook: https:\/\/www.kaggle.com\/titericz\/0-525-tabular-xgboost-gpu-fft-gpu-cuml-fast\/\n- Changes to the original notebook\n        - Training samples more carefully selected\n        - One model instead of multiple models\n        - Hyperparameter fine tuned\n- This model does not use data in `TFRecord` format. No data in the `tfrecords` folder is used.\n- This model does not use `t_min, f_min, t_max, f_max` to extract relevant sections of the audio.\n- Because the testing data does not have `songtype_id, t_min, f_min, t_max, f_max`, also will not use them as plain XGBoost features","c57709ef":"### Take a look at the True Positive species labels and False Positives species labels\n\n- This model does not use `t_min, f_min, t_max, f_max` to extract relevant sections of the audio.\n- Because the testing data does not have `songtype_id, t_min, f_min, t_max, f_max`, also will not use them as plain XGBoost features","863d4c9f":"### Train\n\n    specify a submission file skeleton\n    specify a hyperparameter dict template\n    specify a RandomizedSearchCV template\n    for each class\n        extract the corresponding target col\n        specify an edited hyperparameter dict, for the sake of scale_pos_weight\n        specify an edited RandomizedSearchCV, for the sake of scale_pos_weight\n        RandomizedSearchCV fit(X_train, y_train)\n        RandomizedSearchCV.best_estimator_.predict_proba(X_test)\n        append the prediction to the submission file as a new column\n  \nThe following 2 parameters make GPU work for XGBClassifier\n- `tree_method='gpu_hist'`\n- `predictor='gpu_predictor'`","a7164e9b":"### Check","d59a6438":"### X_train","d6152ce2":"### Fourier transform function\n\n- The reason to reshape to (1000, 1440) is to reduce the features. Instead of using 1440000 features, we average every 1440 features, decreasing the number of features to 1000.\n- In `varfft = cp.abs( cp.fft.fft(data)[:(len(data)\/\/2)] )` we only get half of the length, because the rest half is just mirroring.","29ae9f83":"### X_test"}}