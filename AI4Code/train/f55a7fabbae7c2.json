{"cell_type":{"5ffcc405":"code","c4fb82ac":"code","6fe57907":"code","67101403":"code","f345264a":"code","6b16b6fa":"code","e30f1bd7":"code","8df473bb":"code","3a1622ae":"code","c4482865":"code","d716e991":"code","36b139d6":"code","ce7b74b6":"code","774a829f":"code","6989aa70":"code","15e2a294":"code","a13e1f7e":"code","dde55be4":"code","a0e56e65":"code","db811d40":"code","81e83b95":"code","1e56d583":"code","aaedac87":"code","5fb8fb6a":"code","e912f997":"code","7c4ce4ef":"code","653e54cc":"code","9889d9a9":"code","7c3a3b4d":"code","b3fca133":"code","c18dc5bb":"code","ef736e3a":"code","2650c4c7":"code","d1328b1b":"code","f9d8486e":"code","908f3b0d":"code","4cb609e0":"code","d43c7159":"code","e5c2b65c":"code","67f019a0":"code","45574296":"code","3cf3312c":"code","80f2ebd0":"code","a83ca8eb":"code","9c50bb5c":"markdown","8a0537ef":"markdown","aa28e0e0":"markdown","18999455":"markdown","737e4e8b":"markdown","dfc40b79":"markdown","c8269a14":"markdown","8636706a":"markdown","c139ecf3":"markdown","a9091f6f":"markdown","d481681f":"markdown","1eef43a2":"markdown","edd65379":"markdown","6c48951d":"markdown","fcb3c6e9":"markdown","6cc7bfa6":"markdown","3154ef09":"markdown","a12b3342":"markdown","59eb9295":"markdown","858b420b":"markdown","df239b5d":"markdown","2a942cba":"markdown","b82375ff":"markdown"},"source":{"5ffcc405":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c4fb82ac":"import pandas as pd\n\nprint(pd.__version__)","6fe57907":"auto_data = pd.read_csv('\/kaggle\/input\/auto-mpg.data', delim_whitespace = True, header = None,\n                       names = [\n                                'mpg',\n                                'cylinders',\n                                'displacement',\n                                'horsepower',\n                                'weight',\n                                'aceeleration',\n                                'model',\n                                'origin',\n                                'car_name'\n    ])","67101403":"auto_data.head()","f345264a":"auto_data.info()","6b16b6fa":"auto_data.describe()","e30f1bd7":"auto_data['horsepower'] = pd.to_numeric(auto_data['horsepower'], errors='coerce')","8df473bb":"auto_data.info()","3a1622ae":"auto_data['car_name'].nunique()","c4482865":"auto_data = auto_data.drop(['car_name'], axis=1)","d716e991":"auto_data.head()","36b139d6":"auto_data_nan = auto_data[auto_data.isnull().any(axis=1)]\nauto_data_nan.head(10)","ce7b74b6":"auto_data_final = auto_data.dropna(axis=0)\nauto_data_final[auto_data_final.isnull().any(axis=1)]","774a829f":"from sklearn.model_selection import train_test_split\n\nX = auto_data_final.drop('mpg', axis=1)\ny = auto_data_final['mpg']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state =0)","6989aa70":"from sklearn.svm import SVR\n\nmodel = SVR(kernel='linear', C=1.0)\nmodel.fit(X_train, y_train)","15e2a294":"model.coef_","a13e1f7e":"y_predict = model.predict(X_test)","dde55be4":"from sklearn.metrics import mean_squared_error\n\nmodel_mse = mean_squared_error(y_predict, y_test)\nprint(model_mse)","a0e56e65":"# Check the correlation matrix to derive horsepower feature by help of other feature\ncorr = auto_data.corr()\ncorr.style.background_gradient(cmap='coolwarm').set_precision(4)","db811d40":"auto_data_4_cylinders = auto_data[auto_data['cylinders'] ==4]\nprint(len(auto_data_4_cylinders))\nauto_data_4_cylinders.head()","81e83b95":"%matplotlib inline\n\nauto_data_4_cylinders['horsepower'].plot.hist(bins=10, alpha=0.5)","1e56d583":"import numpy as np\nfrom sklearn.impute import SimpleImputer\n\nimp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')","aaedac87":"auto_data_4_cylinders['horsepower'] = imp_mean.fit_transform(auto_data_4_cylinders[['horsepower']])","5fb8fb6a":"auto_data_4_cylinders[auto_data_4_cylinders.isnull().any(axis=1)].head()","e912f997":"auto_data_6_cylinders = auto_data[auto_data['cylinders']==6]\nauto_data_6_cylinders.head()","7c4ce4ef":"%matplotlib inline\nauto_data_6_cylinders['horsepower'].plot.hist(bins=10, alpha=0.5)","653e54cc":"auto_data_6_cylinders[auto_data_6_cylinders['horsepower']< 160]['horsepower'].plot.hist(bins=10, alpha=0.5)","9889d9a9":"auto_data_6_cylinders[auto_data_6_cylinders.isnull().any(axis=1)].head()","7c3a3b4d":"import numpy as np\nfrom sklearn.impute import SimpleImputer\n\nmean_imp = SimpleImputer(missing_values=np.nan, strategy='mean')","b3fca133":"mean_imp.fit(auto_data_6_cylinders[auto_data_6_cylinders['horsepower'] < 160][['horsepower']])\n\nauto_data_6_cylinders['horsepower'] = mean_imp.transform(auto_data_6_cylinders[['horsepower']])","c18dc5bb":"auto_data_6_cylinders[auto_data_6_cylinders.isnull().any(axis=1)]","ef736e3a":"auto_data_others = auto_data[~auto_data['cylinders'].isin((4,6))]\nprint(len(auto_data_others))","2650c4c7":"auto_data_final = pd.concat([auto_data_others, auto_data_4_cylinders, auto_data_6_cylinders], axis=0)\nprint(len(auto_data_final))","d1328b1b":"# Uncomment below if you want to drop the rows rather than data imputation\n# auto_data_final = auto_data.dropna(axis=0)","f9d8486e":"auto_data_final[auto_data_final.isnull().any(axis=1)]","908f3b0d":"print(len(auto_data_final))\nauto_data_final.head()","4cb609e0":"from sklearn.model_selection import train_test_split\n\nX = auto_data_final.drop('mpg', axis=1)\ny = auto_data_final['mpg']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state =0)","d43c7159":"from sklearn.svm import SVR\n\nmodel = SVR(kernel='linear', C=1.0)\nmodel.fit(X_train, y_train)","e5c2b65c":"model.coef_","67f019a0":"model.score(X_train, y_train)","45574296":"y_predict = model.predict(X_test)","3cf3312c":"%pylab inline\npylab.rcParams['figure.figsize'] = (15, 6)\n\nplt.plot(y_predict, label='Predicted')\nplt.plot(y_test.values, label='Actual')\nplt.ylabel('MPG')\n\nplt.legend()\nplt.show()","80f2ebd0":"model.score(X_test, y_test)","a83ca8eb":"from sklearn.metrics import mean_squared_error\n\nmodel_mse = mean_squared_error(y_predict, y_test)\nprint(model_mse)","9c50bb5c":"#### Check if car_name feature add any value for modeling or not","8a0537ef":"#### Check if NaN has been removed or not","aa28e0e0":"#### You can either drop the feature or impute NaN with some meaningful value. I am going for data imputation.","18999455":"## Problem Statement: Given 8 pieces of information (features) about a vehicle, predict its mileage\n\n\n##### Note:\n\n* The file does not come with headers, so we specify them explicitly","737e4e8b":"#### Convert horsepower feature to numeric","dfc40b79":"#### Check the coefficients for each of feature","c8269a14":"#### Get R-squared value for training data","8636706a":"#### This looks like normal distribution, so we can go for data imputation with mean strategy\n\n#### Printing the target rows for imputation","c139ecf3":" #### Since we removed all NaN, so now it is time to merge back all dataset together","a9091f6f":"#### Compare between predicted and actual value of mpg","d481681f":"> #### Calculate Mean Sqaured Error","1eef43a2":"#### Draw histogram to understand the data distribution for feature horsepower","edd65379":"#### Check if NaN has been removed from dataset or not","6c48951d":"#### Start with model training\n\n#### split the data into train\/test","fcb3c6e9":"#### It seems 160 is an outlier, so plot the histogram excluding 160","6cc7bfa6":"#### Since histogram seems normal distribution, we can pick mean as our imputation stratergy","3154ef09":"#### Get Predictions on test data","a12b3342":"#### Since out of 398 rows, there are 305 distinct car names so there is no value of having the feature. Drop the feature from data set","59eb9295":"#### As you can see horsepower is strongly correleated with feature cylinders, displacement and weight.\n\n#### For simplicity sake, I am considering cylinder feature","858b420b":"#### Calculate Mean Square Error","df239b5d":"#### Get R-square score on test data","2a942cba":"#### Check if there is any NaN value or not","b82375ff":"#### Repeat the same process with 6 cylinder vehicles"}}