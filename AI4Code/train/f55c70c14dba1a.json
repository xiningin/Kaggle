{"cell_type":{"46d92448":"code","0c2ef4e1":"code","fe7618c5":"code","3029ea39":"code","a8a3726b":"code","59366196":"code","95c30d86":"code","b5784923":"code","38de205f":"code","ca2ab1ff":"code","ac84c30b":"code","fc3c037a":"code","c7a02713":"code","c3c5dc56":"code","9c1c4865":"code","c0564bb8":"code","88d399a6":"code","694187ba":"code","8b9fe79b":"code","7a31ba47":"code","77c6c8be":"code","c0738f26":"code","127c0fd2":"code","3e6db9ae":"code","b8cddc23":"code","da5a9ebf":"code","fe8a3a1f":"code","bdfe84b6":"code","34c258da":"code","c3ae626d":"code","bbd11d94":"code","d06d6db3":"code","7f231269":"code","7f74e70c":"code","ff07d57d":"code","18b2ca1c":"code","c79115c6":"code","753ba258":"code","f6a680a6":"code","f3ff8bca":"code","192322cd":"code","436f30b0":"code","8e10d678":"code","7080c5a4":"code","e20b7982":"code","c6d7ff97":"code","d3a3e275":"code","a4d1c3f3":"code","cb8da580":"code","b9233b10":"code","4ac185db":"code","34710f65":"code","6f19036d":"code","c9586334":"code","7245e80d":"code","e2ca8888":"code","2c8b36c7":"code","acd4a02e":"markdown","7d35403d":"markdown","04385b62":"markdown","4c2577d5":"markdown","99816efe":"markdown","46509d9f":"markdown","214347c9":"markdown","f4fc1af5":"markdown","3db65065":"markdown","e4de89a6":"markdown","7ca073a3":"markdown","a3de04a4":"markdown","a9a489c4":"markdown","12655083":"markdown","87dc8499":"markdown","5abad1ef":"markdown"},"source":{"46d92448":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU,SimpleRNN\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom plotly import graph_objs as go\nimport plotly.express as ex\nimport plotly.figure_factory as ff","0c2ef4e1":"test = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv.zip')\ntrain = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip')","fe7618c5":"train.head()","3029ea39":"test.head()","a8a3726b":"print(train.shape)\nprint(test.shape)","59366196":"import re,string\n\ndef strip_links(text):\n    link_regex    = re.compile('((https?):((\/\/)|(\\\\\\\\))+([\\w\\d:#@%\/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n    links         = re.findall(link_regex, text)\n    for link in links:\n        text = text.replace(link[0], ', ')    \n    return text","95c30d86":"train['comment_text']=train['comment_text'].apply(lambda x:strip_links(x))\ntest['comment_text']=test['comment_text'].apply(lambda x:strip_links(x))","b5784923":"### replace :\\n \ntrain['comment_text']=train['comment_text'].str.replace(\"\\n\",' ')","38de205f":"### replace :\\n \ntest['comment_text']=test['comment_text'].str.replace(\"\\n\",' ')","ca2ab1ff":"# Define the function to remove the punctuation\ndef remove_punctuations(text):\n    for punctuation in string.punctuation:\n        text = text.replace(punctuation, '')\n    return text\n# Apply to the DF series\ntrain['comment_text'] = train['comment_text'].apply(remove_punctuations) ","ac84c30b":"# Apply to the DF series\ntest['comment_text'] = test['comment_text'].apply(remove_punctuations) ","fc3c037a":"list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\nY = train[list_classes].values","c7a02713":"X_train, X_test, y_train, y_test = train_test_split(train.comment_text.values, Y,  \n                                                  random_state=42, \n                                                  test_size=0.2)","c3c5dc56":"## Check lenght of text in the data\ntrain['comment_text'].apply(lambda x:len(str(x).split())).max()","9c1c4865":"max_features = 5000\nmaxlen = 500","c0564bb8":"token=tf.keras.preprocessing.text.Tokenizer(num_words=max_features)\ntoken.fit_on_texts(train.comment_text)","88d399a6":"X_train_seq=token.texts_to_sequences(X_train)\nX_test_seq=token.texts_to_sequences(X_test)","694187ba":"#zero pad the sequences\nX_train_pad = sequence.pad_sequences(X_train_seq, maxlen=maxlen)\nX_test_pad = sequence.pad_sequences(X_test_seq, maxlen=maxlen)","8b9fe79b":"word_index = token.word_index","7a31ba47":"len(token.word_index)##251102","77c6c8be":"!wget -c \"https:\/\/s3.amazonaws.com\/dl4j-distribution\/GoogleNews-vectors-negative300.bin.gz\"\n!gzip -d GoogleNews-vectors-negative300.bin.gz\n!ls -l","c0738f26":"from gensim.models import Word2Vec, KeyedVectors\n# Load pretrained Glove model (in word2vec form)\nword2vec_model = KeyedVectors.load_word2vec_format(\".\/GoogleNews-vectors-negative300.bin\", binary=True)","127c0fd2":"#Embedding length based on selected model - we are using 50d here.\nembedding_vector_length = 300","3e6db9ae":"#Initialize embedding matrix\nembedding_matrix = np.zeros((max_features + 1, embedding_vector_length))\nprint(embedding_matrix.shape)","b8cddc23":"for word, i in sorted(token.word_index.items(),key=lambda x:x[1]):\n    if i > (max_features+1):\n        break\n    try:\n        embedding_vector = word2vec_model[word] #Reading word's embedding from Glove model for a given word\n        embedding_matrix[i] = embedding_vector\n    except:\n        pass","da5a9ebf":"embedding_matrix","fe8a3a1f":"#Initialize model\nimport tensorflow as tf\ntf.keras.backend.clear_session()\nmodel = tf.keras.Sequential()","bdfe84b6":"# A simpleRNN without any pretrained embeddings and one dense layer\nmodel = Sequential()\nmodel.add(tf.keras.layers.Embedding(max_features + 1, #Vocablury size\n                                    embedding_vector_length, #Embedding size\n                                    weights=[embedding_matrix], #Embeddings taken from pre-trained model\n                                    trainable=False, #As embeddings are already available, we will not train this layer. It will act as lookup layer.\n                                    input_length=maxlen) #Number of words in each review\n         )\nmodel.add(SimpleRNN(100))\nmodel.add(Dense(6, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.summary()","34c258da":"history = model.fit(X_train_pad,\n                    y_train,\n                    epochs=3,\n                    batch_size=32,          \n                    validation_data=(X_test_pad, y_test))","c3ae626d":"!wget http:\/\/nlp.stanford.edu\/data\/glove.6B.zip\n#unzip the file, we get multiple embedding files. We can use either one of them\n!unzip glove.6B.zip\n!ls -l","bbd11d94":"from gensim.scripts.glove2word2vec import glove2word2vec\n\n#Glove file - we are using model with 50 embedding size\nglove_input_file = 'glove.6B.50d.txt'\n\n#Name for word2vec file\nword2vec_output_file = 'glove.6B.50d.txt.word2vec'\n\n#Convert Glove embeddings to Word2Vec embeddings\nglove2word2vec(glove_input_file, word2vec_output_file)","d06d6db3":"### We will extract word embedding for which we are interested in; the pre trained has 400k words each with 50 embedding vector size.\nfrom gensim.models import Word2Vec, KeyedVectors\n\n# Load pretrained Glove model (in word2vec form)\nglove_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n\n#Embedding length based on selected model - we are using 50d here.\nembedding_vector_length = 50","7f231269":"#Initialize embedding matrix\nembedding_matrix = np.zeros((max_features + 1, embedding_vector_length))\nprint(embedding_matrix.shape)","7f74e70c":"for word, i in sorted(token.word_index.items(),key=lambda x:x[1]):\n    if i > (max_features+1):\n        break\n    try:\n        embedding_vector = glove_model[word] #Reading word's embedding from Glove model for a given word\n        embedding_matrix[i] = embedding_vector\n    except:\n        pass","ff07d57d":"#Initialize model\nimport tensorflow as tf\ntf.keras.backend.clear_session()\nmodel = tf.keras.Sequential()","18b2ca1c":"# A simpleRNN without any pretrained embeddings and one dense layer\nmodel = Sequential()\nmodel.add(tf.keras.layers.Embedding(max_features + 1, #Vocablury size\n                                    embedding_vector_length, #Embedding size\n                                    weights=[embedding_matrix], #Embeddings taken from pre-trained model\n                                    trainable=False, #As embeddings are already available, we will not train this layer. It will act as lookup layer.\n                                    input_length=maxlen) #Number of words in each review\n         )\nmodel.add(SimpleRNN(100))\nmodel.add(Dense(6, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.summary()","c79115c6":"history_glove=model.fit(X_train_pad,\n                        y_train,\n                        epochs=3,\n                        batch_size=32,          \n                        validation_data=(X_test_pad, y_test))","753ba258":"import fasttext.util\n### uncomment this when you nned to download pretrained fasttext model\n# fasttext.util.download_model('en', if_exists='ignore')  # English","f6a680a6":"### remove unnecessary files\n# !rm -rf  .\/cc.en.300.bin.gz\n# !rm -rf .\/GoogleNews-vectors-negative300.bin.gz\n# !rm -rf .\/glove.6B.300d.txt\n# !rm -rf .\/glove.6B.200d.txt\n# !rm -rf .\/glove.6B.100d.txt\n# !rm -rf .\/glove.6B.zip","f3ff8bca":"ft = fasttext.load_model('cc.en.300.bin')","192322cd":"### reduct the vector dimension to 50\nfasttext.util.reduce_model(ft, 50)","436f30b0":"#Initialize embedding matrix\nembedding_matrix_fasttext = np.zeros((max_features + 1, embedding_vector_length))\nprint(embedding_matrix_fasttext.shape)","8e10d678":"for word, i in sorted(token.word_index.items(),key=lambda x:x[1]):\n    if i > (max_features+1):\n        break\n    try:\n        embedding_vector = ft[word] #Reading word's embedding from Glove model for a given word\n        embedding_matrix_fasttext[i] = embedding_vector\n    except:\n        pass","7080c5a4":"# A simpleRNN without any pretrained embeddings and one dense layer\nmodel = Sequential()\nmodel.add(tf.keras.layers.Embedding(max_features + 1, #Vocablury size\n                                    embedding_vector_length, #Embedding size\n                                    weights=[embedding_matrix_fasttext], #Embeddings taken from pre-trained model\n                                    trainable=False, #As embeddings are already available, we will not train this layer. It will act as lookup layer.\n                                    input_length=maxlen) #Number of words in each review\n         )\nmodel.add(SimpleRNN(100))\nmodel.add(Dense(6, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.summary()","e20b7982":"history_fasttext=model.fit(X_train_pad,y_train,\n                           epochs=3,\n                           batch_size=32,          \n                           validation_data=(X_test_pad, y_test))","c6d7ff97":"history.history","d3a3e275":"history_glove.history","a4d1c3f3":"history_fasttext.history","cb8da580":"loss_list=[history.history,history_glove.history,history_fasttext.history]","b9233b10":"loss_list","4ac185db":"loss_dict={'w2v_loss':loss_list[0]['loss'],'w2v_val_loss':loss_list[0]['val_loss'],\n           'glove_loss':loss_list[1]['loss'],'glove_val_loss':loss_list[1]['val_loss'],\n           'fasttext_loss':loss_list[2]['loss'],'fasttext_val_loss':loss_list[2]['val_loss']}\nacc_dict={'w2v_acc':loss_list[0]['accuracy'],'w2v_val_acc':loss_list[0]['val_accuracy'],\n           'glove_acc':loss_list[1]['accuracy'],'glove_val_acc':loss_list[1]['val_accuracy'],\n           'fasttext_acc':loss_list[2]['accuracy'],'fasttext_val_acc':loss_list[2]['val_accuracy']}","34710f65":"loss_dict['w2v_loss']","6f19036d":"np.arange(1,3,1)","c9586334":"epochRange = np.arange(1,4,1)\nplt.plot(epochRange,loss_dict['w2v_loss'])\nplt.plot(epochRange,loss_dict['glove_loss'])\nplt.plot(epochRange,loss_dict['fasttext_loss'])\nplt.title('Training loss for different embeddings')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['Word2Vec', 'GLOVE','FastText'], loc='upper left')\nplt.show()","7245e80d":"epochRange = np.arange(1,4,1)\nplt.plot(epochRange,loss_dict['w2v_val_loss'])\nplt.plot(epochRange,loss_dict['glove_val_loss'])\nplt.plot(epochRange,loss_dict['fasttext_val_loss'])\nplt.title('Validation loss for different embeddings')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['Word2Vec', 'GLOVE','FastText'], loc='upper left')\nplt.show()","e2ca8888":"epochRange = np.arange(1,4,1)\nplt.plot(epochRange,acc_dict['w2v_val_acc'])\nplt.plot(epochRange,acc_dict['glove_val_acc'])\nplt.plot(epochRange,acc_dict['fasttext_val_acc'])\nplt.title('Validation accuracy for different embeddings')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['Word2Vec', 'GLOVE','FastText'], loc='upper left')\nplt.show()","2c8b36c7":"epochRange = np.arange(1,4,1)\nplt.plot(epochRange,acc_dict['w2v_acc'])\nplt.plot(epochRange,acc_dict['glove_acc'])\nplt.plot(epochRange,acc_dict['fasttext_acc'])\nplt.title('Training accuracy for different embeddings')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['Word2Vec', 'GLOVE','FastText'], loc='upper left')\nplt.show()","acd4a02e":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">Model Building Using Word2vec&nbsp;&nbsp;&nbsp;&nbsp;<\/h1> ","7d35403d":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">Do some cleaning on the data&nbsp;&nbsp;&nbsp;&nbsp;<\/h1> ","04385b62":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">Loss curve for 3 embeddings&nbsp;&nbsp;&nbsp;&nbsp;<\/h1> ","4c2577d5":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">Fasttext Embeddings&nbsp;&nbsp;&nbsp;&nbsp;<\/h1> ","99816efe":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">Train and Val split&nbsp;&nbsp;&nbsp;&nbsp;<\/h1> ","46509d9f":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">Validation Loss curve for 3 embeddings&nbsp;&nbsp;&nbsp;&nbsp;<\/h1>","214347c9":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">Training Accuracy curve for 3 embeddings&nbsp;&nbsp;&nbsp;&nbsp;<\/h1>","f4fc1af5":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">Read the data&nbsp;&nbsp;&nbsp;&nbsp;<\/h1> ","3db65065":"<br>\n<h1 style = \"font-size:40px; font-family:Garamond ; font-weight : normal; background-color: #f6f5f5 ; color : #fe346e; text-align: center; border-radius: 100px 100px;\">Understanding Embedding with Classification<\/h1>\n<br>\n    \n<center><img src=\"https:\/\/mlwhiz.com\/images\/word2vec.png\"><\/center>\n\n### <h3 style=\"color:#fe346e\">Word2Vec<\/h3>\nWhat are word embeddings exactly? Loosely speaking, they are vector representations of a particular word. Having said this, what follows is how do we generate them? More importantly, how do they capture the context?\nWord2Vec is one of the most popular technique to learn word embeddings using shallow neural network. It was developed by Tomas Mikolov in 2013 at Google.\n\n### <h3 style=\"color:#fe346e\">Why do we need them?<\/h3>\nConsider the following similar sentences: Have a good day and Have a great day. They hardly have different meaning. If we construct an exhaustive vocabulary (let\u2019s call it V), it would have V = {Have, a, good, great, day}.\n\nNow, let us create a one-hot encoded vector for each of these words in V. Length of our one-hot encoded vector would be equal to the size of V (=5). We would have a vector of zeros except for the element at the index representing the corresponding word in the vocabulary. That particular element would be one. The encodings below would explain this better.\nHave = `[1,0,0,0,0]`; a=`[0,1,0,0,0]` ; good=`[0,0,1,0,0]` ; great=`[0,0,0,1,0]` ; day=`[0,0,0,0,1]` (represents transpose)\n\nIf we try to visualize these encodings, we can think of a 5 dimensional space, where each word occupies one of the dimensions and has nothing to do with the rest (no projection along the other dimensions). This means \u2018good\u2019 and \u2018great\u2019 are as different as \u2018day\u2019 and \u2018have\u2019, which is not true.\nOur objective is to have words with similar context occupy close spatial positions. Mathematically, the cosine of the angle between such vectors should be close to 1, i.e. angle close to 0.\n\n<center><img src=\"https:\/\/miro.medium.com\/max\/1394\/0*XMW5mf81LSHodnTi.png\"><\/center>\n\n### <h3 style=\"color:#fe346e\">How does Word2Vec work?<\/h3>\nWord2Vec is a method to construct such an embedding. It can be obtained using two methods (both involving Neural Networks): Skip Gram and Common Bag Of Words (CBOW)\n\n### CBOW Model: \n\nThis method takes the context of each word as the input and tries to predict the word corresponding to the context. Consider our example: Have a great day.\nLet the input to the Neural Network be the word, great. Notice that here we are trying to predict a target word (day) using a single context input word great. More specifically, we use the one hot encoding of the input word and measure the output error compared to one hot encoding of the target word (day). In the process of predicting the target word, we learn the vector representation of the target word.\n\nThe architecture is below in Figure 1:\n<img src=\"https:\/\/miro.medium.com\/max\/1400\/0*3DFDpaXoglalyB4c.png\">\n\nThe input or the context word is a one hot encoded vector of size V. The hidden layer contains N neurons and the output is again a V length vector with the elements being the softmax values.\n\n### Skip-Gram Model:\n\n<img src=\"https:\/\/miro.medium.com\/max\/1400\/0*Ta3qx5CQsrJloyCA.png\">\n\nThis looks like multiple-context CBOW model just got flipped. To some extent that is true.\n\nWe input the target word into the network. The model outputs C probability distributions. What does this mean?\nFor each context position, we get C probability distributions of V probabilities, one for each word.\n\n### <h3 style=\"color:#fe346e\">Stanford\u2019s competing Approach \u2014 GloVe (2014)<\/h3>\n\nGloVe is a \"count-based\" model.Count-based models learn their vectors by essentially doing dimensionality reduction on the co-occurrence counts matrix. They first construct a large matrix of (words x context) co-occurrence information, i.e. for each \"word\" (the rows), you count how frequently we see this word in some \"context\" (the columns) in a large corpus.  The number of \"contexts\" is of course large, since it is essentially combinatorial in size. So then they factorize this matrix to yield a lower-dimensional (word x features) matrix, where each row now yields a vector representation for each word. In general, this is done by minimizing a \"reconstruction loss\" which tries to find the lowerdimensional representations which can explain most of the variance in the high-dimensional data. In the specific case of GloVe, the counts matrix is preprocessed by normalizing the counts and log-smoothing them. This turns out to be A Good Thing in terms of the quality of the learned representations.\n\nHowever, as pointed out, when we control for all the training hyper-parameters, the embeddings generated using the both Word2Vec and GLoVe methods tend to perform very similarly in downstream NLP tasks. The additional benefits of GloVe over word2vec is that it is easier to parallelize the implementation which means it's easier to train over more data, which, with these models, is always A Good Thing.\n\n### <h3 style=\"color:#fe346e\">fasttext<\/h3>\n\nfastText as a library for efficient learning of word representations and sentence classification. It is written in C++ and supports multiprocessing during training. FastText allows you to train supervised and unsupervised representations of words and sentences. These representations (embeddings) can be used for numerous applications from data compression, as features into additional models, for candidate selection, or as initializers for transfer learning.\nFastText supports training continuous bag of words (CBOW) or Skip-gram models using negative sampling, softmax or hierarchical softmax loss functions. I have primarily used fastText for training semantic embeddings for a corpus of size in the order of tens millions, and am happy with how it has performed and scaled for this task. I had a hard time finding documentation beyond the documentation for getting started, so in this post I am going to walk you through the internals of fastText and how it works. An understanding of how the word2vec models work is expected.\n\nFastText is able to achieve really good performance for word representations and sentence classification, specially in the case of rare words by making use of character level information.\nEach word is represented as a bag of character n-grams in addition to the word itself, so for example, for the word matter, with n = 3, the fastText representations for the character n-grams is <ma, mat, att, tte, ter, er>. < and > are added as boundary symbols to distinguish the ngram of a word from a word itself, so for example, if the word mat is part of the vocabulary, it is represented as <mat>. This helps preserve the meaning of shorter words that may show up as ngrams of other words. Inherently, this also allows you to capture meaning for suffixes\/prefixes.","e4de89a6":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">Glove Embeddings&nbsp;&nbsp;&nbsp;&nbsp;<\/h1> ","7ca073a3":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">Model Building Using GloVe&nbsp;&nbsp;&nbsp;&nbsp;<\/h1> ","a3de04a4":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">Training Loss curve for 3 embeddings&nbsp;&nbsp;&nbsp;&nbsp;<\/h1>","a9a489c4":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">Import Libraries&nbsp;&nbsp;&nbsp;&nbsp;<\/h1> ","12655083":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">Validation Accuracy curve for 3 embeddings&nbsp;&nbsp;&nbsp;&nbsp;<\/h1>","87dc8499":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">Define Vocab size and input string size&nbsp;&nbsp;&nbsp;&nbsp;<\/h1> ","5abad1ef":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">Word2Vec embeddings&nbsp;&nbsp;&nbsp;&nbsp;<\/h1> "}}