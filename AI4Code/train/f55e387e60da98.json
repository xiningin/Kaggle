{"cell_type":{"ce17c316":"code","b99a9592":"code","584cd6f2":"code","1be4331f":"code","57b74c27":"code","f96f0fbf":"code","8d8d9e5e":"code","af1842b4":"code","fd05c7f3":"code","4e053ec3":"code","5a66e550":"code","6a4c12e5":"code","6a498141":"code","073cebce":"code","966850bc":"code","45102971":"code","0c9d5a4d":"code","e5d4c64d":"code","fd00d68b":"markdown","655b372c":"markdown","36afebb1":"markdown","d399ac5e":"markdown","642dc074":"markdown","d37470d2":"markdown","1aa3738f":"markdown","958f61a8":"markdown","5c285c21":"markdown","e927a3c3":"markdown","3aad07d9":"markdown","805f7dda":"markdown","a8d9622b":"markdown","bded2d04":"markdown","2def2b80":"markdown"},"source":{"ce17c316":"# import data processing and visualisation libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# import image processing libraries\nimport cv2\nimport skimage\nfrom skimage.transform import resize\n\n# import tensorflow and keras\nimport tensorflow as tf\nfrom tensorflow import keras\nimport os\n\nprint(\"Packages imported...\")","b99a9592":"## Exploring files in folder\nfolder_path = '\/kaggle\/input\/asl-alphabet\/'\nfor path, directories, files in os.walk(folder_path):\n    print(path,'--> number of files : ', len(files))\n# I SEE TEST DATA IS MEANT TO BE TEST IN WILD. but it's kinda usless honestly it's taken from same dataset, same human hand","584cd6f2":"#Preparing data in dataframe for easier data handling\ntrain_folder = '\/kaggle\/input\/asl-alphabet\/asl_alphabet_train\/asl_alphabet_train'\nall_data = []\nfor folder in os.listdir(train_folder):\n    # GET ALL FILES IN FOLDER\n    label_folder = os.path.join(train_folder, folder)\n    onlyfiles = [{'label':folder,'path':os.path.join(label_folder, f)} for f in os.listdir(label_folder) if os.path.isfile(os.path.join(label_folder, f))]\n    all_data += onlyfiles\ndata_df = pd.DataFrame(all_data)\ndata_df","1be4331f":"# Distrubtion of the target\nfig, ax = plt.subplots(figsize =(20, 10))\nax.hist(data_df['label'], bins=100)\nax.set_title(f'Targets Histogram ')\nplt.show()\n# honeslty I didn't need the visuals as I saw the count frequency of the folder, but hey it won't bite","57b74c27":"#SHOWING SOME RANDOM IMAGES\nimport random\nimport matplotlib.image as mpimg\n\nsigns = data_df['label'].unique().tolist()\nimages = []\nfor sign in signs:\n    rows = data_df[data_df['label']==sign]['path']\n    random_pick = random.randint(a=0, b=len(rows))\n    filepath = rows.iloc[random_pick]\n    img = mpimg.imread(filepath)\n    plt.figure()\n    plt.title(sign)\n    plt.imshow(img)","f96f0fbf":"rnd_img =  mpimg.imread(data_df['path'].iloc[0])\nprint(f'image size : {rnd_img.shape}')","8d8d9e5e":"from sklearn.model_selection import train_test_split\n#ENCODING LABEL to 0,1,2,3,4,5,6, etc..\n# parameters\nx_col = 'path'\ny_col = 'label'\ntest_size = 0.2\n\n#splitting data ..................\ntrain_df, test_df = train_test_split(data_df, test_size= test_size, random_state=42,stratify=data_df[[y_col]])\nprint(f'train size : {len(train_df)}')\nprint(f'test size : {len(test_df)}')","af1842b4":"# CREATING DATA GENERATORS\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nimg_width, img_height = 64, 64\nbatch_size = 32\nno_of_classes = len(data_df[y_col].unique())\n\n# NO AUGMENTAION, JUST NRORMALIZING THE DATA\n# TRAINING GENERATOR\ntrain_datagen = ImageDataGenerator(rescale = 1\/255.0)\ntrain_generator = train_datagen.flow_from_dataframe(\n    dataframe=train_df,x_col=x_col, y_col=y_col,\n    target_size=(img_width, img_height),class_mode='categorical', batch_size=batch_size,\n    shuffle=False,\n)\n\n# TESTING GENERATOR\nvalidation_datagen = ImageDataGenerator(rescale = 1\/255.0)\nvalidation_generator = validation_datagen.flow_from_dataframe(\n    dataframe=test_df, x_col=x_col, y_col=y_col,\n    target_size=(img_width, img_height), class_mode='categorical', batch_size=batch_size,\n    shuffle=False\n)\n","fd05c7f3":"from keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Activation, Dense, Flatten\nmodel = Sequential()\n\nmodel.add(Conv2D(32, (5, 5), input_shape=(img_width, img_height, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Conv2D(64, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Conv2D(64, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(128, activation='relu'))\n\nmodel.add(Dense(no_of_classes, activation='softmax'))\n\nmodel.summary()","4e053ec3":"# OERFORMING EARLY STOPS \nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nearly_stop = EarlyStopping(monitor='val_loss',patience=2)\n# put model trackers\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])","5a66e550":"# TRAINNING\nepochs = 30\nhistory = model.fit(train_generator,\n                    epochs=epochs,\n                    verbose=1,\n                    validation_data=validation_generator,\n                    callbacks = [early_stop]\n                   )\n","6a4c12e5":"metrics = pd.DataFrame(history.history)\nprint(\"The model metrics are\")\nmetrics","6a498141":"acc=history.history['accuracy']\nval_acc=history.history['val_accuracy']\nloss=history.history['loss']\nval_loss=history.history['val_loss']\n\nepochs=range(len(acc))\n\nfig = plt.figure(figsize=(14,7))\nplt.plot(epochs, acc, 'r', label=\"Training Accuracy\")\nplt.plot(epochs, val_acc, 'b', label=\"Validation Accuracy\")\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Training and validation accuracy')\nplt.legend(loc='lower right')\nplt.show()\n","073cebce":"fig = plt.figure(figsize=(14,7))\nplt.plot(epochs, loss, 'r', label=\"Training Loss\")\nplt.plot(epochs, val_loss, 'b', label=\"Validation Loss\")\nplt.legend(loc='upper right')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and validation loss')","966850bc":"test_loss, test_acc = model.evaluate(validation_generator)\nprint('loss :' ,test_loss, 'acc :' ,test_acc)","45102971":"from sklearn import metrics\n\npredictions = model.predict(validation_generator, verbose=1)\n# Get most likely class\npredicted_classes = np.argmax(predictions, axis=-1)\npredictions = np.argmax(predictions, axis=-1) #multiple categories\n\ntrue_classes = validation_generator.classes\nclass_labels = list(validation_generator.class_indices.keys())  \nreport = metrics.classification_report(true_classes, predicted_classes, target_names=class_labels)\nprint(report)    ","0c9d5a4d":"plt.figure(figsize=(30, 20))\n\nax = sns.heatmap(metrics.confusion_matrix(true_classes,predicted_classes))\nax.set_title('Confusion Matrix with labels')\nax.set_xlabel('Predicted Values')\nax.set_ylabel('Actual Values ')\n\n\n## Ticket labels - List must be in alphabetical order\nax.xaxis.set_ticklabels(class_labels)\nax.yaxis.set_ticklabels(class_labels)\n\nplt.show()\n","e5d4c64d":"# from keras.models import load_model\nmodel.save('latest_model_12_11_2021.h5')\nprint(\"Model saved successfully...\")","fd00d68b":"## Question 1. \n- Explain briefly the intuition behind your choices regarding network\narchitecture and hyperparameters.  \n\nThe network I choosed is a CNN. since it's well known when dealing with images. a CNN do a good work on that. I started with simple one. trainnig from scratch and it showed good results in this dataset so I didn't need to do use transfer learning on this task.  \nthe hyperparametrs are :  \n- Learning rate : we are using adam optimizer to it's adabtive\n- batch size : 32 seems a good start with the size of the dataset.\n- epochs : 30 started with a lower number as it showed it was enough.\n- activtion function: Softmax beacause it maps output to range (0,1) and it can be used as probability distribution given the output sums upto 1 and it's used in multiclass classification methods","655b372c":"### Modeling","36afebb1":"## 5 Model metrics","d399ac5e":"<h2><center>American Sign Language Detection<\/center><\/h2>","642dc074":"# Part 2 \n","d37470d2":"## Question 4. \n- If, instead of the provided dataset, our dataset consisted of zoomed-out\nvideos containing single words such as in this gif. In an abstract way, how would your\nnetwork architecture change to be able to perform well on this classification task? (You\nmay use a block diagram to demonstrate your new network).\n\nI would cut the video into frames. and I will still use a CNN for extracting image features. after that I would used a RNN model to extract sequence features and add a classification layers at the end for the new words classes. I would read this survey to be more sure on my approach https:\/\/paperswithcode.com\/paper\/word-level-deep-sign-language-recognition","1aa3738f":"# Machine Learning Engineer Task  \n## Part 1   \nIn this task, you are asked to design, build and train your best classification model for\nthis Sign Language dataset. Our goal is to be able to have a model that can correctly\nclassify the entire alphabet represented as hand gestures for American Sign Language.\nYour implementation should be primarily using Python3 and Tensorflow\/Pytorch. You\ncan use other libraries for preprocessing if necessary.  \nFor hardware acceleration, if needed, you can use free tier GPU notebooks\/instances\nsuch as Google Colab, AWS or Google Cloud.  \nKindly demonstrate the following:  \n1. Graphs \/ Visualization for the chosen model performance metrics (Accuracy \/ Loss).\n2. The exported tensorboard graph for the model architecture.\n3. Confusion matrix of the model predictions on your testing set.\n4. Exported trained weights and graph of your model for testing.  \n  \nYou are encouraged to do the following, if necessary:  \n1. Shrink the dataset (in case of limited hardware resources)\n1. Re-split the Training & Testing sets.\n2. Perform cross-validation.\n3. Neglect unnecessary labels in the dataset\n4. Refer to research papers (But do cite any ideas inspired by them)  \n  \n<b> Bonus: <\/b>\n1. Build your best model in terms of performance while maintaining a relatively\nsmall model size.\n2. Deploy your model for inference through a Flask server.  ","958f61a8":"## Question 3. \n- Would this model, given the provided dataset, be able to generalize well in\nother scenes and backgrounds?  \nI don't believe it will with th eprovided dataset. reason because the dataset is same person with a only a few variations. so no it won't be able to generaize well. that's what I believe.","5c285c21":"### 3. Data prepocessing <a id=6><\/a>","e927a3c3":"## Question 2. \n- Comment on the positives \/ negatives of this dataset. (Quality, balance\netc.)  \nThe problem is mutli-class classification. 29 balanced class. so it gives a good data in the distrubtion. images is clear 200, 200. but here is the thing. it's the same person hand and same light conditions. no varity of enviroment. which can result in making the model biased by the data to these condition. even to the same person. so I believe the data doesn't have enough variation to be able to train a model to a real world problem.","3aad07d9":"[back to top](#19)","805f7dda":"### Inspired by\n\n1. https:\/\/www.kaggle.com\/namanmanchanda\/asl-detection-99-accuracy","a8d9622b":"#### Splitting data","bded2d04":"### 1. Importing packages","2def2b80":"### 2. Exploring data"}}