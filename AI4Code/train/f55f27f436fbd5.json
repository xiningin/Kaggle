{"cell_type":{"5be174f5":"code","b5d5dee1":"code","0fd521e2":"code","dcbdab64":"code","1a052944":"code","996b8b8f":"code","535e922b":"code","765a3508":"code","46b3ff8c":"code","d3cd547e":"code","b8f8710f":"code","586de6eb":"code","9b1d321c":"code","3b8c05ae":"code","06f46d54":"code","8e90122e":"code","3f42186c":"code","d1603e28":"code","4c92a24a":"code","82708ec5":"code","039a53f4":"code","d94d995f":"code","ba77e083":"code","a072c270":"code","cad84848":"markdown","94f37a08":"markdown","82e19cc4":"markdown","1b435250":"markdown","1552b8b8":"markdown","c041531d":"markdown","42cede4a":"markdown","46d1cd27":"markdown","e2f0c725":"markdown","94546844":"markdown","f9c02423":"markdown","bb1d9dd4":"markdown","709315c8":"markdown"},"source":{"5be174f5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b5d5dee1":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.preprocessing import StandardScaler","0fd521e2":"df = pd.read_csv('\/kaggle\/input\/equipfailstest\/equip_failures_training_set.csv')","dcbdab64":"df.describe()","1a052944":"cols = df.columns\nfor col in cols:\n    df[col] = pd.to_numeric(df[col], errors='coerce')","996b8b8f":"df.isna().sum().sum()","535e922b":"df.fillna(-1, inplace=True)","765a3508":"df.isna().sum().sum()","46b3ff8c":"df.target.plot(kind='hist')","d3cd547e":"split=0.2\ndf_fail = df[df.target == 1]\ndf_normal = df[df.target == 0].sample(n=int(df_fail.shape[0]\/split), random_state=42)\nbal = df_fail.append(df_normal)","b8f8710f":"bal.target.plot(kind='hist')","586de6eb":"X, y = bal.iloc[:,2:] , bal['target']\ntotal, test = df.iloc[:,2:] , df['target']","9b1d321c":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","3b8c05ae":"scaler = StandardScaler()\nscaler.fit(X)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\ntotal = scaler.transform(total)","06f46d54":"params_grid = {\n    'n_neighbors' : list(range(2,20,1)),\n    'weights' : ['uniform', 'distance']\n}","8e90122e":"clf = GridSearchCV(KNeighborsClassifier(), params_grid, scoring='f1', cv=5)","3f42186c":"clf.fit(X_train,y_train)","d1603e28":"clf.best_params_","4c92a24a":"y_pred = clf.predict(X_test)","82708ec5":"print(f'The accuracy score: {accuracy_score(y_test, y_pred)}')\nprint(f'The F1 Score is: {f1_score(y_test,y_pred)}')","039a53f4":"pred = clf.predict(total)","d94d995f":"print(f'The total accuracy score: {accuracy_score(test, pred)}')\nprint(f'The total F1 Score is: {f1_score(test,pred)}')","ba77e083":"from joblib import dump, load","a072c270":"dump(clf, 'gridsearch_knn.joblib')\ndump(scaler, 'scaler.joblib')","cad84848":"### Scaling the data\nWithout scaling the data, larger numeric columns can be more impactful.  Standard scaler normalizes the data under a normal distribution","94f37a08":"### Checking both the F-1 and accuracy scores","82e19cc4":"### Only 1 column is in numeric dtype\nThis next step goes through and turns the string columns into numeric.  Everytime the to_numeric fails, I force the function to input an NaN to take care of later","1b435250":"Checking the best parameters","1552b8b8":"## Exploratory Analysis And Preparing the Dataset","c041531d":"We need to hand the NaN in the dataset.  If I was using a timeseries type dataset I would use fillna with a forwardfill or backfill.  However, I don't believe this dataset was in order so I will input a placeholder number.  -1 is usuallly a good number for this.","42cede4a":"## Balancing the dataset\nA common problem with this type of dataset is having more 'normal' operating conditions than failures.  The models tend to bias guessing 'normal' therefore cheating the system.  This results in low F-1 Scores.  I chose to take all the failure data then randomly sampling only a portion of target == 0 data.  Through experimentation I landed on 1:5 ratio","46d1cd27":"## First Import Packages and Import the Data","e2f0c725":"### Setting up the gridsearchcv\nI only ran two separate algorithms for time constarints.  I ran the KNearestNeighbors and the RandomForestClassifier.  I will only show the KnearestNeighbors here. If I had more time I would run SVM and likely a full ANN with keras\n\nIt's important to run a gridsearch here to find the optimal hyperparameters.  Also the cross_validation (cv) helps generalize the model and can reduce the risk of overfitting.  Also changing the scoring parameter to F1 as stated in the rules to have the gridsearch utilize f1 and not accuracy score","94546844":"### Finally taking all the data (prior to balancing the data) to assess if the model fits the data generally.","f9c02423":"## Lastly Productionizing the Model\nJoblib is a good way to store the models and scalers for production.  If I had more time I would put the scaler and model into a sklearn pipeline for further automating the workflow","bb1d9dd4":"## The Machine Learning\nSplitting Test and Training Data.  Also bringing the full dataset through the pipeline as total to score the model against it","709315c8":"# Predictive Maintenance Workbook and Explanation\n### Doug Valentine - DS Petroleum Eng"}}