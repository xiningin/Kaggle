{"cell_type":{"76f91ead":"code","6229b1f6":"code","f70254d9":"code","437d8120":"code","b20eebf5":"code","23f168df":"code","cf95673b":"code","53ddbb6e":"code","e53801b4":"code","e000b5a0":"code","bdf1f323":"code","e8a4c30c":"code","32eac1b1":"code","2c06929d":"code","0005de13":"code","15e81c7d":"code","cdfb72be":"code","65de7ef5":"code","5fd84f53":"code","8ce400ba":"code","dc4313c1":"code","ce89c231":"code","b7cf1113":"code","993a266b":"code","d0629bd0":"code","68b8b604":"code","6cb53bf1":"code","5fb835cc":"code","612867a5":"code","1828d70d":"code","4d1016e9":"code","abb22015":"code","2e65e1f3":"code","ea2f4019":"code","57062900":"code","1674dc3e":"code","206056fa":"code","21b6ed24":"code","a2e5db40":"code","77417b66":"code","fab707be":"code","dec80eff":"code","573a0d31":"code","a3af40d2":"code","4d688757":"code","68f2a2a0":"code","edf6bc7f":"code","b47705d0":"code","b89a5b1c":"code","2d689c35":"code","a716d338":"code","9e01975a":"code","32668273":"code","15b92a53":"code","c0b18c8f":"code","9ed7e3a5":"code","4abd8c47":"code","c74e1f7f":"code","35b40871":"code","bfc6854b":"code","2b8b094f":"code","376d3382":"code","67c415d0":"code","bb143edc":"code","82466eaa":"code","88030e99":"code","9cdb42f8":"code","1017dc62":"code","b776c960":"code","b7f35230":"code","7a9cf7a9":"code","78fc3195":"code","01b3a2b7":"code","c1cea803":"code","d3eb47d5":"code","ba8da41f":"code","45d17e64":"code","06508235":"code","92c131dc":"code","c682d3f8":"code","4cd28012":"code","73359527":"code","613ec527":"code","6f7ba6bc":"code","27f8e2b8":"code","b5f6c1da":"code","0a96c13d":"code","ce5658ba":"code","5738c0d4":"code","198f200b":"code","35c6f1c2":"code","92486771":"code","12bffeae":"code","4f060357":"code","cfe0486a":"markdown","2d820951":"markdown","3e460238":"markdown","0f8eb130":"markdown","012ee59c":"markdown","9bc592de":"markdown"},"source":{"76f91ead":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6229b1f6":"MDU_df = pd.read_csv('..\/input\/emaildata\/test_MDU.csv')\nRet_df = pd.read_csv('..\/input\/emaildata\/test_Retirements.csv')\nTra_df = pd.read_csv('..\/input\/emaildata\/test_Transfers.csv')\ntest_df = pd.read_csv('..\/input\/testdata\/test_Test-Data.csv')","f70254d9":"Ret_df.head()","437d8120":"Tra_df.head()","b20eebf5":"MDU_df.isna().sum()\/len(MDU_df)\n","23f168df":"Tra_df.isna().sum()\/len(Tra_df)","cf95673b":"Ret_df.isna().sum()\/len(Ret_df)","53ddbb6e":"def drop_columns_threshold(df,threshold):    \n    for col in df.columns:\n        if(df.isna().sum()[col]\/len(df) > threshold):\n            df.drop(columns = col,inplace = True)\n            \ndrop_columns_threshold(MDU_df,0.95)\ndrop_columns_threshold(Tra_df,0.95)\ndrop_columns_threshold(Ret_df,0.95)\ndrop_columns_threshold(test_df,0.90)\n","e53801b4":"MDU_df['target'] = 0\nTra_df['target'] = 1\nRet_df['target'] = 2","e000b5a0":"total_df = pd.concat([MDU_df,Tra_df,Ret_df], axis=0, join='inner')\n","bdf1f323":"total_df.reset_index()\ntotal_df.head()","e8a4c30c":"test_df","32eac1b1":"total_df.isna().sum()","2c06929d":"englishWords","0005de13":"import nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nfrom nltk.stem import WordNetLemmatizer \nnltk.download('wordnet')\nfrom nltk.tokenize import word_tokenize \nfrom nltk.tokenize import sent_tokenize\nstop_words = set(stopwords.words('english')) \nwith open (\"..\/input\/common-nouns\/wordsEn.txt\", \"r\") as file:\n    englishWords=file.read().split()\n    \nnltk.download('punkt')\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport re , unicodedata , string\n\nclass Preprocessing_text:\n    '''\n    Preprocessing pipeline : sequence of indices representing the methods to be used while preprocessing each row in the df passed\n    The index to method mapping is as follows:\n    remove_URL:1, remove_non_ascii:2, remove_punctuation:3, remove_stopwords:4, to_lower:5, lemmatize_postags:6, \n    replace_nan:7, get_top_n_words:8, remove_common_words:9\n\n\n    All methods can be used in apply() method for dataframes [except #8 which requires the entire corpus] \n    '''\n    def __init__ (self,preprocessing_pipeline=[5,4,2,3,9],stopword_list=list(set(stopwords.words('english')))):\n        self.stopword_list = stopword_list\n        self.preprocess_methods = [self.blank,self.remove_URL,self.remove_non_ascii,self.remove_punctuation,self.remove_stopwords,\n                                    self.to_lower,self.lemmatize_postags,self.replace_nan,self.get_top_n_words,self.taking_common_nouns]\n        self.preprocessing_pipeline = preprocessing_pipeline\n        self.words_frequent = []\n        \n    def blank(self,sample_str):\n        return sample_str\n\n    def remove_URL(self,sample_str):\n        \"\"\"Remove URLs from a sample string\"\"\"\n        return re.sub(r\"http\\S+\", \"\", sample_str)\n\n    def remove_non_ascii(self,sample_str):\n        \"\"\"Remove non-ASCII characters from a sample string [sample_str]\"\"\"\n        words = word_tokenize(sample_str)\n        new_words = []\n        for word in words:\n            new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n            new_words.append(new_word)\n        return ' '.join(new_words)\n\n    def remove_punctuation(self,sample_str):\n        \"\"\"Remove punctuation from a sample string\"\"\"\n        words = word_tokenize(sample_str)\n        new_words = []\n        for word in words:\n            new_word = re.sub(r'[^\\w\\s]', '', word)\n            if new_word != '':\n                new_words.append(new_word)\n        return ' '.join(new_words)\n\n    def remove_stopwords(self,sample_str):\n        \"\"\"Remove stop words from a sample string\"\"\"\n        words = word_tokenize(sample_str)\n        new_words = []\n        for word in words:\n            if word.lower() not in self.stopword_list:\n                new_words.append(word)\n        return ' '.join(new_words)\n\n    def to_lower(self,sample_str):\n        \"\"\" Converting all words to lowercase in a sample string\"\"\"\n        return str(sample_str).lower()\n\n\n    def lemmatize_postags(self,sample_str):\n        \"\"\"Lemmatize verbs,adj and noun in a sample string\"\"\"\n        words = word_tokenize(sample_str)\n        lemmatizer = WordNetLemmatizer()\n        lemmas = []\n        for word in words:\n            word = lemmatizer.lemmatize(word, pos='v')\n            word = lemmatizer.lemmatize(word, pos='n')\n            word = lemmatizer.lemmatize(word, pos='a')\n            lemmas.append(lemma)\n        return ' '.join(lemmas)\n\n    def replace_nan(self,sample_str):\n        \"\"\"Replacing nan strings with empty strings - required for textrank\"\"\"        \n        sample_str_new = re.sub('nan' , '' , str(sample_str))\n        return sample_str_new\n\n    def get_top_n_words(self,corpus, n=10):\n        \"\"\"\n        List the top n words in a vocabulary according to occurrence in a text corpus and updates self.words_frequent to be used later\n\n        get_top_n_words([\"I love Python\", \"Python is a language programming\", \"Hello world\", \"I love the world\"]) -> \n        [('python', 2),\n        ('world', 2),\n        ('love', 2),\n        ('hello', 1),\n        ('is', 1),\n        ('programming', 1),\n        ('the', 1),\n        ('language', 1)]\n        Repetitive words may be removed because they might not hold enough specific information to be assigned as keywords\n        \"\"\"\n        vec = CountVectorizer().fit(corpus)\n        bag_of_words = vec.transform(corpus)\n        sum_words = bag_of_words.sum(axis=0) \n        self.words_frequent = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n        self.words_frequent =sorted(self.words_frequent, key = lambda x: x[1], reverse=True)\n        self.words_frequent = [word for word,count in self.words_frequent]\n        #print(self.words_frequent)\n        \n        print('Most {} common words in the given corpus are : \\n'.format(n))\n        for i in range(n):\n            print(self.words_frequent[i])\n        self.words_frequent = self.words_frequent[:n]\n\n    def taking_common_nouns(self,sample_str):\n        \"\"\"Removes the most frequent words from the sample string using words_frequent list provided by get_top_n_words\"\"\"\n        words = word_tokenize(sample_str)\n        #print(words)\n        filtered_words = []\n        for word in words:\n            #print(word)\n            if word in englishWords :\n                filtered_words.append(word)\n        return \" \".join(filtered_words)\n\n    def main_df_preprocess(self,df,column_list):\n        \"\"\"Main function to apply all listed methods as specified\"\"\"    \n        for col in column_list:\n            df[\"Preprocessed_\" + col] = df[col].apply(self.preprocess_methods[self.preprocessing_pipeline[0]])\n            for index in self.preprocessing_pipeline[1:]:\n                if index!=8:\n                    print(index,col)\n                    df[\"Preprocessed_\" + col] = df[\"Preprocessed_\" + col].apply(self.preprocess_methods[index])\n                else:\n                    self.preprocess_methods[index](df['Preprocessed_' + col])\n        return df","15e81c7d":"\nstopword_list=list(set(stopwords.words('english')))\nstopword_list.append('subject')\nstopword_list.append('mrs')\nstopword_list.append('mr')\nstopword_list\n\npreprocessing = Preprocessing_text(stopword_list = stopword_list)\npreprocessing.main_df_preprocess(total_df,['subject_msg','body'])\n# preprocessing.main_df_preprocess(test_df,['subject_msg','body'])\n","cdfb72be":"stopword_list","65de7ef5":"total_df","5fd84f53":"from nltk.tokenize import RegexpTokenizer\n\n\ntokenizer = RegexpTokenizer(r'\\w+')\n#Just change token type here to see the effect with different columns\ntotal_df[\"body_tokens\"] = total_df[\"Preprocessed_body\"].apply(tokenizer.tokenize)\ntotal_df[\"subject_tokens\"] = total_df[\"Preprocessed_subject_msg\"].apply(tokenizer.tokenize)\n\ntotal_df['body_tokens'].head()","8ce400ba":"def remove_values_from_list(the_list, val):\n   return [value for value in the_list if value != val]\n\nwith open (\"..\/input\/common-nouns\/wordsEn.txt\", \"r\") as file:\n    englishWords=file.read().split()","dc4313c1":"from gensim.models import Word2Vec\nfrom sklearn.decomposition import PCA\nnlp_col = 'subject_tokens' \nall_words = [word for tokens in total_df[nlp_col] for word in tokens]\nVOCAB = sorted(list(set(all_words)))\nstop_words = set(stopwords.words('english')) \nvocab = {}\nfor i in range(3):\n    sentences = total_df[total_df['target'] == i+1][nlp_col].values\n    # sentences\n    # train model\n    model_temp = Word2Vec(sentences, min_count=1)\n    # summarize vocabulary\n    vocab[i] = list(model_temp.wv.vocab)\n#     print(\"This is the vocabulary on which the model is trained: \", words)\n# print(vocab)\n\nfor i in range(3):\n    for j in range(len(vocab[i])): \n        if(vocab[i][j].lower() in vocab[0] and vocab[i][j].lower() in vocab[1] and vocab[i][j].lower() in vocab[2]):\n            stop_words.add(vocab[i][j])\n            remove_values_from_list(vocab[0],vocab[i][j])\n            remove_values_from_list(vocab[1],vocab[i][j])\n            remove_values_from_list(vocab[2],vocab[i][j])\n            print(vocab[i][j],end = \"-\")\n# print(\"These are the ststop_words)\n\n# print(VOCAB)\nfor word in VOCAB:\n    if(word.isnumeric() or word in stop_words or word.lower() not in englishWords ):\n        print()\n        print(word,end = \"X\")\n        total_df[nlp_col] = total_df[nlp_col].apply(lambda x: remove_values_from_list(x,word))\n        remove_values_from_list(vocab[0],word)\n        remove_values_from_list(vocab[1],word)\n        remove_values_from_list(vocab[2],word)\n            \nall_words = [word for tokens in total_df[nlp_col] for word in tokens]\nsentence_lengths = [len(tokens) for tokens in total_df[nlp_col]]\nVOCAB = sorted(list(set(all_words)))\nprint(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(VOCAB)))\nprint(\"Max sentence length is %s\" % max(sentence_lengths))","ce89c231":"vocab","b7cf1113":"for i in range(len(total_df[nlp_col].values)):\n    if('abc' in total_df[nlp_col].values[i]):\n        print('yes')","993a266b":"vocab.values()","d0629bd0":"\n# define training data\nsentences = vocab.values()\nmodel_tot = Word2Vec(sentences, min_count=1)\n\n\nX = []\ncolor = []\nfor i in range(3):\n#     print(\"This is the vocabulary on which the model is trained: \", vocab[i])\n    # access vector for one word\n    #     print(\"This is the generated output dimension: \" ,model_tot['Thank'].shape)\n\n    # print(new_model)\n    # fit a 2d PCA model to the vectors\n    for vec in model_tot[vocab[i]]:\n        X.append(vec)\n        color.append(['r','g','b'][i])\n    print(X)\npca = PCA(n_components=2)\n# X\n\nresult = pca.fit_transform(X)\nprint(result)\n\n# # create a scatter plot of the projection\n# plt.figure(figsize=(20, 20))\n# plt.scatter(result[:, 0], result[:, 1],color = color)\n# words = list(model_temp.wv.vocab)\n# for i, word in enumerate(words):\n#     plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n    \n# plt.show()","68b8b604":"import matplotlib.pyplot as plt\n\n# create a scatter plot of the projection\nplt.figure(figsize=(20, 20))\nplt.scatter(result[:, 0], result[:, 1],color = color)\nwords = list(model_temp.wv.vocab)\nfor i, word in enumerate(words):\n    plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n    \nplt.show()","6cb53bf1":"\n\nfig = plt.figure(figsize=(10, 10)) \nplt.xlabel('Sentence length')\nplt.ylabel('Number of sentences')\nplt.hist(sentence_lengths)\nplt.show()","5fb835cc":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\ndef cv(data):\n    count_vectorizer = CountVectorizer(stop_words='english')\n    # Initialize the \"CountVectorizer\" object, which is scikit-learn's\n    # bag of words tool\n    \n    emb = count_vectorizer.fit_transform(data)\n    # fit_transform() does two functions: First, it fits the model\n    # and learns the vocabulary; second, it transforms our training data\n    # into feature vectors. The input to fit_transform should be a list of \n    # strings.\n\n    return emb, count_vectorizer\n\n","612867a5":"from yellowbrick.text import FreqDistVisualizer\n\n\nX , count_vectorizer = cv(total_df['Preprocessed_subject_msg'])\nfeatures   = count_vectorizer.get_feature_names()\n\nplt\nvisualizer = FreqDistVisualizer(features=features, orient='h',n=20)\nvisualizer.fit(X)\nvisualizer.show()","1828d70d":"print(count_vectorizer.vocabulary_)","4d1016e9":"X\n","abb22015":"from gensim.models import word2vec\n\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ntsne_model = TSNE(perplexity=40, n_components=2, n_iter=2500, random_state=23)\nnew_values = tsne_model.fit_transform(X)","2e65e1f3":"x = []\ny = []\nfor value in new_values:\n    x.append(value[0])\n    y.append(value[1])\n        \nplt.figure(figsize=(16, 16))\ncategories = np.unique(total_df['target'])\ncolors = np.linspace(0, 1, len(categories))\ncolordict = dict(zip(categories, colors))  \ntotal_df[\"Color\"] = total_df['target'].apply(lambda x: colordict[x])\n\n\n","ea2f4019":"total_df.index = pd.RangeIndex(len(total_df.index))","57062900":"len(colors)","1674dc3e":"color = ['r','g','b']\ncolors = []\nfor i in total_df.index:\n#     print(total_df['target'][i])\n    colors.append(color[total_df['target'][i] - 1])\ntsne_df = pd.DataFrame({'x' : x , 'y' : y, 'color' : colors})","206056fa":"plt.scatter(tsne_df['x'],tsne_df['y'],color = tsne_df['color'])\nplt.show()","21b6ed24":"lda_col = 'subject_tokens'\ntotal_df[lda_col]","a2e5db40":"import gensim \ndictionary = gensim.corpora.Dictionary(total_df[lda_col])\ncount = 0\nfor k, v in dictionary.iteritems():\n    print(k, v)\n    count += 1\n    if count > 10:\n        break","77417b66":"dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)","fab707be":"bow_corpus = [dictionary.doc2bow(doc) for doc in total_df[lda_col]]\nbow_corpus","dec80eff":"from gensim import corpora, models\ntfidf = models.TfidfModel(bow_corpus)\ncorpus_tfidf = tfidf[bow_corpus]\nfrom pprint import pprint\nfor doc in corpus_tfidf:\n    pprint(doc)\n    break","573a0d31":"lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)","a3af40d2":"for idx, topic in lda_model.print_topics(-1):\n    print('Topic: {} \\nWords: {}'.format(idx, topic))","4d688757":"lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\nfor idx, topic in lda_model_tfidf.print_topics(-1):\n    print('Topic: {} Word: {}'.format(idx, topic))","68f2a2a0":"for index, score in sorted(lda_model_tfidf[bow_corpus[1]], key=lambda tup: -1*tup[1]):\n    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))","edf6bc7f":"\nX_train, X_test, y_train, y_test = train_test_split(total_df[train_cols],total_df['target'], test_size=0.33, random_state=42)","b47705d0":"import os\nimport io\nimport numpy\nfrom pandas import DataFrame\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB,GaussianNB\nfrom sklearn.model_selection import train_test_split\n\n# X = gensim.models.Word2Vec(total_df['Preprocessed_subject_msg'], min_count=1,size=1000)\n# X['test']\n\n# X_train, X_test, y_train, y_test = train_test_split(X,total_df['target'], test_size=0.33, random_state=42,stratify = total_df['target'])\n\nvectorizer = CountVectorizer()\nvectorizer = TfidfVectorizer()\n# # model_tot = Word2Vec(sentences, min_count=1)\nvectorizer.fit(X_train['Preprocessed_subject_msg'].values)\ncounts = vectorizer.transform(X_train['Preprocessed_subject_msg'].values)\n# print(vectorizer.vocabulary_)\n\nclassifier = MultinomialNB()\nclassifier.fit(counts.toarray(), y_train)","b89a5b1c":"example_counts = vectorizer.transform(X_test['Preprocessed_subject_msg'].values)\nexample_counts","2d689c35":"# examples = ['Free Viagra now!!!', \"Hi Bob, how about a game of golf tomorrow?\"]\npredictions = classifier.predict(example_counts.toarray())\npredictions","a716d338":"(y_test == predictions).sum()\/len(predictions)","9e01975a":"classifier.predict_proba(example_counts.toarray())","32668273":"predictions","15b92a53":"from sklearn.preprocessing import OneHotEncoder\n\none_hot = pd.get_dummies(total_df['target'],prefix = 'target')\n# Drop column B as it is now encoded\n# df = df.drop('',axis = 1)\n# Join the encoded df\ntotal_df_bert = total_df.join(one_hot)","c0b18c8f":"total_df_bert\n","9ed7e3a5":"X_train, X_test, y_train, y_test = train_test_split(total_df_bert[train_cols],total_df_bert['target_2'], test_size=0.33, random_state=42)","4abd8c47":"from transformers import BertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)","c74e1f7f":"\nvocabulary = tokenizer.get_vocab()\n\nprint(list(vocabulary.keys())[5000:5020])","35b40871":"max_length_test = 20\ntest_sentence = 'Test tokenization sentence. Followed by another sentence'\n\n# add special tokens\n\ntest_sentence_with_special_tokens = '[CLS]' + test_sentence + '[SEP]'\n\ntokenized = tokenizer.tokenize(test_sentence_with_special_tokens)\n\nprint('tokenized', tokenized)\n\n# convert tokens to ids in WordPiece\ninput_ids = tokenizer.convert_tokens_to_ids(tokenized)\n  \n# precalculation of pad length, so that we can reuse it later on\npadding_length = max_length_test - len(input_ids)\n\n# map tokens to WordPiece dictionary and add pad token for those text shorter than our max length\ninput_ids = input_ids + ([0] * padding_length)\n\n# attention should focus just on sequence with non padded tokens\nattention_mask = [1] * len(input_ids)\n\n# do not focus attention on padded tokens\nattention_mask = attention_mask + ([0] * padding_length)\n\n# token types, needed for example for question answering, for our purpose we will just set 0 as we have just one sequence\ntoken_type_ids = [0] * max_length_test\n\nbert_input = {\n    \"token_ids\": input_ids,\n    \"token_type_ids\": token_type_ids,\n    \"attention_mask\": attention_mask\n}\nprint(bert_input)","bfc6854b":"def convert_example_to_feature(review):\n  \n  # combine step for tokenization, WordPiece vector mapping, adding special tokens as well as truncating reviews longer than the max length\n  \n  return tokenizer.encode_plus(review, \n                add_special_tokens = True, # add [CLS], [SEP]\n                max_length = max_length, # max length of the text that can go to BERT\n                pad_to_max_length = True, # add [PAD] tokens\n                return_attention_mask = True, # add attention mask to not focus on pad tokens\n              )\n","2b8b094f":"max_length = 512\nbatch_size = 6","376d3382":"import tensorflow as tf\nimport tensorflow_datasets as tfds\n# map to the expected input to TFBertForSequenceClassification, see here \ndef map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n  return {\n      \"input_ids\": input_ids,\n      \"token_type_ids\": token_type_ids,\n      \"attention_mask\": attention_masks,\n  }, label\n\ndef encode_examples(ds, limit=-1):\n\n  # prepare list, so that we can build up final TensorFlow dataset from slices.\n  input_ids_list = []\n  token_type_ids_list = []\n  attention_mask_list = []\n  label_list = []\n\n  if (limit > 0):\n      ds = ds.take(limit)\n    \n  for review, label in ds:\n    print(review,label)\n    bert_input = convert_example_to_feature(review)\n    print(bert_input)\n    input_ids_list.append(bert_input['input_ids'])\n    token_type_ids_list.append(bert_input['token_type_ids'])\n    attention_mask_list.append(bert_input['attention_mask'])\n    label_list.append([label])\n\n  return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)","67c415d0":"# pd.concat(X_train,y_train)\nimport pandas as pd","bb143edc":"ds_train = pd.concat([X_train,y_train], axis=1, join='inner')\nds_test = pd.concat([X_test,y_test], axis=1, join='inner')\n","82466eaa":"total_df_bert","88030e99":"\n# train dataset\nds_train_encoded = encode_examples(ds_train[['Preprocessed_body','target_2']].to_numpy()).shuffle(10000).batch(batch_size)\n\n# test dataset\nds_test_encoded = encode_examples(ds_test[['Preprocessed_body','target_2']].to_numpy()).batch(batch_size)","9cdb42f8":"from transformers import TFBertForSequenceClassification\nimport tensorflow as tf\n\n# recommended learning rate for Adam 5e-5, 3e-5, 2e-5\nlearning_rate = 2e-5\n\n# we will do just 1 epoch for illustration, though multiple epochs might be better as long as we will not overfit the model\nnumber_of_epochs = 1\n\n\n# model initialization\nmodel = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n\n# choosing Adam optimizer\noptimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n\n# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmetric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n\nmodel.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n","1017dc62":"number_of_epochs = 2\n# model = tf.keras.Model(inputs=, outputs=y_train.toarray(), name='BERT_MultiLabel_MultiClass')\nbert_history = model.fit(ds_train_encoded, epochs=number_of_epochs, validation_data=ds_test_encoded,batch_size=64)","b776c960":"test_target = to_categorical(y_test).T[1:4].T\n# test_y_product = to_categorical(data_test['Product'])\ntest_x = tokenizer(\n    text=X_test['Preprocessed_body'].to_list(),\n    add_special_tokens=True,\n    max_length=max_length,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = False,\n    verbose = True)\n\nmodel.evaluate(\n    x={'input_ids': test_x['input_ids']},\n    y={'target': test_target}\n)","b7f35230":"from transformers import TFBertModel,  BertConfig, BertTokenizerFast\n# Then what you need from tensorflow.keras\nfrom tensorflow.keras.layers import Input, Dropout, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.initializers import TruncatedNormal\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.metrics import CategoricalAccuracy,Accuracy\nfrom tensorflow.keras.utils import to_categorical\n# And pandas for data import + sklearn because you allways need sklearn\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split","7a9cf7a9":"#######################################\n### --------- Setup BERT ---------- ###\n# Name of the BERT model to use\nmodel_name = 'bert-base-uncased'\n# Max length of tokens\nmax_length = 100\n# Load transformers config and set output_hidden_states to False\nconfig = BertConfig.from_pretrained(model_name)\nconfig.output_hidden_states = False\n# Load BERT tokenizer\ntokenizer = BertTokenizerFast.from_pretrained(pretrained_model_name_or_path = model_name, config = config)\n# Load the Transformers BERT model\ntransformer_model = TFBertModel.from_pretrained(model_name, config = config)","78fc3195":"train_cols = [x for x in total_df.columns if x != 'target']\ntrain_cols","01b3a2b7":"total_df.columns","c1cea803":"X_train, X_test, y_train, y_test = train_test_split(total_df[train_cols],total_df['target'], test_size=0.33, random_state=42)","d3eb47d5":"def convert_example_to_feature(review):\n  \n  # combine step for tokenization, WordPiece vector mapping, adding special tokens as well as truncating reviews longer than the max length\n  \n  return tokenizer.encode_plus(review, \n                add_special_tokens = True, # add [CLS], [SEP]\n                max_length = max_length, # max length of the text that can go to BERT\n                pad_to_max_length = True, # add [PAD] tokens\n                return_attention_mask = True, # add attention mask to not focus on pad tokens\n              )\n\ndef encode_examples(ds, limit=-1):\n\n  # prepare list, so that we can build up final TensorFlow dataset from slices.\n  input_ids_list = []\n  \n  if (limit > 0):\n      ds = ds.take(limit)\n    \n  for review, label in ds:\n    print(review,label)\n    bert_input = convert_example_to_feature(review)\n    print(bert_input)\n    input_ids_list.append(bert_input['input_ids'])\n#     token_type_ids_list.append(bert_input['token_type_ids'])\n#     attention_mask_list.append(bert_input['attention_mask'])\n#     label_list.append([label])\n\n  return input_ids_list","ba8da41f":"transformer_model.layers[0]","45d17e64":"bert = transformer_model.layers[0]\n# Build your model input\ninput_ids = Input(shape=(max_length,), name='input_ids', dtype='int32')\ninputs = {'input_ids': input_ids}\n# Load the Transformers BERT model as a layer in a Keras model\nbert_model = bert(inputs)[1]\ndropout = Dropout(config.hidden_dropout_prob, name='pooled_output')\npooled_output = dropout(bert_model, training=False)\n# Then build your model output\n# Dense(units=len(y_train.value_counts()), kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='target')(pooled_output)\n\ntarget_vars = Dense(units=len(y_train.value_counts()), kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='target',activation='softmax')(pooled_output)\n# product = Dense(units=len(data.Product_label.value_counts()), kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='product')(pooled_output)\noutputs = {'target': target_vars}\n# And combine it all in a model object\nmodel = Model(inputs=inputs, outputs=outputs, name='BERT_MultiLabel_MultiClass')\n# Take a look at the model\nmodel.summary()","06508235":"optimizer = Adam(\n    learning_rate=5e-05,\n    epsilon=1e-08,\n    decay=0.01,\n    clipnorm=1.0)\n# Set loss and metrics\nloss = {'target': CategoricalCrossentropy(from_logits = True)}\nmetric = {'target': CategoricalAccuracy('accuracy')}\n# Compile the model\nmodel.compile(\n    optimizer = optimizer,\n    loss = loss, \n    metrics = metric)\n# Ready output data for the model\ny_target = to_categorical(y_train)\n\n# y_product = to_categorical(data['Product'])\n# Tokenize the input (takes some time)\nx = tokenizer(\n    text=X_train['Preprocessed_body'].to_list(),\n    add_special_tokens=True,\n    max_length=max_length,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = False,\n    verbose = True)\n# Fit the model\nhistory = model.fit(\n    x={'input_ids': x['input_ids']},\n    y={'target': y_target},\n    batch_size=64,\n    epochs=15)","92c131dc":"total_df","c682d3f8":"# Ready test data\ntest_target = to_categorical(y_test)\n# test_y_product = to_categorical(data_test['Product'])\ntest_x = tokenizer(\n    text=X_test['Preprocessed_subject_msg'].to_list(),\n    add_special_tokens=True,\n    max_length=max_length,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = False,\n    verbose = True)\n# Run evaluation\nmodel_eval = model.evaluate(\n    x={'input_ids': test_x['input_ids']},\n    y={'target': test_target}\n)","4cd28012":"# Ready test data\ntest_target = to_categorical(y_test)\n# test_y_product = to_categorical(data_test['Product'])\ntest_x = tokenizer(\n    text=test_df['Preprocessed_subject_msg'].to_list(),\n    add_special_tokens=True,\n    max_length=max_length,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = False,\n    verbose = True)\n\npreds = model.predict(test_x['input_ids'])","73359527":"test_df['MDU_prob'] = preds['target'].T[0]\ntest_df['Tra_prob'] = preds['target'].T[1]\ntest_df['Ret_prob'] = preds['target'].T[2]\n","613ec527":"preds['target'].argmax()","6f7ba6bc":"test_df['Prediction'] = 0","27f8e2b8":"test_df['Prediction'] = np.where(preds['target']","b5f6c1da":"prediction = []\nfor i in test_df.index:\n    prediction.append(preds['target'][i].argmax())\nprediction,columns = ['Prediction']\n\n# test_df.join(pd.DataFrame()","0a96c13d":"def map(x):\n    if(x == 0):\n        return 'MDU'\n    if(x == 1):\n        return 'Transfer'\n    if(x == 2):\n        return 'Retirement'\n","ce5658ba":"test_df['Prediction'] = test_df['Prediction'].apply(lambda x: map(x))","5738c0d4":"test_df.to_csv('output.csv')","198f200b":"\ntest_df['Prediction'].apply(\n    if(preds['target'][i].argmax() == 0):\n        test_df[i]['Prediction'] = 'MDU'\n    if(preds['target'][i].argmax() == 1):\n        test_df[i]['Prediction'] = 'Transfer'\n    if(preds['target'][i].argmax() == 2):\n        test_df[i]['Prediction'] = 'Retirement'\n    )","35c6f1c2":"preds","92486771":"cols = total_df.columns\ntest_df = pd.read_csv('..\/input\/testdata\/test_Test-Data.csv')","12bffeae":"test_df.head(40)","4f060357":"model","cfe0486a":"## BERT try 2","2d820951":"## T-SNE on a category body or subject","3e460238":"## LDA","0f8eb130":"## BERT and Tensorflow\n","012ee59c":"## Naive Bayes","9bc592de":"## PCA on a category (subject or body) with Word2Vec"}}