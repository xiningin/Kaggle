{"cell_type":{"b58469e7":"code","1abccce3":"code","b15de70d":"code","a83021bc":"code","22b57056":"code","7c604f95":"code","09697e29":"code","4eb59fa4":"code","33897317":"code","23e03743":"code","f0ae66f0":"code","15574936":"code","aa8c5667":"code","20303e8d":"code","ba087e29":"code","ba9decbb":"code","bf743e63":"code","0b466afc":"code","35a4c59b":"code","9038f2a1":"code","53c0698a":"code","7651c24d":"code","dace2bf6":"code","b3513b39":"code","5e9cbd53":"code","3b1ab5d7":"code","606163f2":"code","5a0b4a33":"code","a10cace6":"code","ac56e86a":"code","5f3aff27":"code","a8ea4d7d":"code","90a65bf3":"code","bcab5911":"code","8639ae06":"code","ebfff591":"markdown","0dc67863":"markdown","9ec6dc2a":"markdown","2360fcb9":"markdown","9b241633":"markdown","9e62e6ac":"markdown"},"source":{"b58469e7":"!nvidia-smi","1abccce3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np\nimport pandas as pd\nimport os \nimport sys\nimport matplotlib.pyplot as plt\nfrom pandas import plotting\nfrom sklearn import preprocessing\nimport sklearn\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.impute import SimpleImputer\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import tree\nfrom sklearn.preprocessing import LabelEncoder\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import OneHotEncoder\nfrom tqdm import tqdm\nfrom sklearn.metrics import log_loss\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.preprocessing import RobustScaler\nimport pandas_profiling\nfrom catboost import CatBoostClassifier\nfrom catboost import cv\nfrom catboost import Pool\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b15de70d":"train = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\ntest = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\ntrainTargets = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\ntrainTargetsNonScored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_nonscored.csv')\ncombinedTrain = train.merge(trainTargets)","a83021bc":"featureCols = train.columns\nlabelCols = trainTargets.drop('sig_id',axis=1).columns","22b57056":"# shuffle the dataset\ncombinedTrain = combinedTrain.sample(frac=1, random_state= 2)\n# delete the index\ncombinedTrain.reset_index(drop=True, inplace=True)\n# show the head of combined df\ncombinedTrain.head()\n# reconstruct the dataframe \ntrain = combinedTrain[featureCols]\ntrainTargets = combinedTrain[labelCols]","7c604f95":"#check the cleanliness of the data \ntrain.isnull().sum().sum()","09697e29":"#step 1 - One hot Encoding\nforEncoding = ['cp_time', 'cp_type', 'cp_dose']\ntrain = pd.concat([train,(pd.get_dummies(train[forEncoding].astype(str),dummy_na=False, dtype=np.uint8,prefix=\"feature\"))],axis=1)\ntest = pd.concat([test,(pd.get_dummies(test[forEncoding].astype(str),dummy_na=False, dtype=np.uint8,prefix=\"feature\"))],axis=1)","4eb59fa4":"# Step 2 - Get columns of features and labels\nlabelCols = [col for col in trainTargets.columns if col != 'sig_id']\nfeatureCols = [col for col in train.columns if col not in ['sig_id', 'cp_time', 'cp_type', 'cp_dose']]\nprint('Number of different labels:', len(labelCols))\nprint('Number of features:', len(featureCols))","33897317":"# Step 3 - Separate the Dataset\nX = train[featureCols]\nX_test = test[featureCols]\ny = trainTargets","23e03743":"cat_features= ['feature_24',\n 'feature_48',\n 'feature_72',\n 'feature_ctl_vehicle',\n 'feature_trt_cp',\n 'feature_D1',\n 'feature_D2']\nnonCat = [col for col in X.columns if col not in cat_features]","f0ae66f0":"# Step 4 - Standardize the data using a robust scalar\nrsc = RobustScaler()\nX[nonCat] = rsc.fit_transform(X[nonCat])\nX_test[nonCat] = rsc.transform(X_test[nonCat])\nX = pd.DataFrame(X)\nX_test = pd.DataFrame(X_test)","15574936":"X","aa8c5667":"# Step 5 - Convert to numpy array\nX = X.iloc[:].to_numpy()\nX_test = X_test.iloc[:].to_numpy()\ny = y.iloc[:].to_numpy()","20303e8d":"print(X.shape)\nprint(X_test.shape)\nprint(y.shape)","ba087e29":"# very initial model\nxgb = XGBClassifier(n_estimators=500,seed=123,learning_rate=0.15,max_depth=5,colsample_bytree=1,subsample=1,tree_method='gpu_hist')\nclassifier = MultiOutputClassifier(xgb)","ba9decbb":"# Parameters from https:\/\/www.kaggle.com\/fchmiel\/xgboost-baseline-multilabel-classification\nparams = {'estimator__colsample_bytree': 0.6522,\n          'estimator__gamma': 3.6975,\n          'estimator__learning_rate': 0.0503,\n          'estimator__max_delta_step': 2.0706,\n          'estimator__max_depth': 10,\n          'estimator__min_child_weight': 31.5800,\n          'estimator__n_estimators': 166,\n          'estimator__subsample': 0.8639\n         }\n\nclassifier.set_params(**params)","bf743e63":"# solution inspired from := https:\/\/www.kaggle.com\/fchmiel\/xgboost-baseline-multilabel-classification\/log\ntest_preds = np.zeros((X_test.shape[0], y.shape[1]))\nloss1 = []\nkf=KFold(n_splits=5, random_state=100, shuffle=True)\nfor iteration, (train_index,validation_index) in enumerate(kf.split(X, y)):\n    print('ITERATION NUMBER - ', iteration)\n    X_train, X_val = X[train_index], X[validation_index]\n    y_train, y_val = y[train_index], y[validation_index]\n    \n    classifier.fit(X_train, y_train)\n    val_preds = classifier.predict_proba(X_val) \n    val_preds = np.array(val_preds)[:,:,1].T #(num_labels,num_samples,prob_0\/1)\n    \n    loss = log_loss(np.ravel(y_val), np.ravel(val_preds))\n    loss1.append(loss)\n    preds = classifier.predict_proba(X_test)\n    preds = np.array(preds)[:,:,1].T #(num_labels,num_samples,prob_0\/1)\n    test_preds += preds \/ 5 #take average of 10 models\n    \nprint(loss1)\nprint('Mean CV loss across folds', np.mean(loss1))","0b466afc":"train = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\ntest = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\ntrainTargets = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\ntrainTargetsNonScored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_nonscored.csv')\ncombinedTrain = train.merge(trainTargets)\nfeatureCols = train.columns\nlabelCols = trainTargets.drop('sig_id',axis=1).columns","35a4c59b":"# shuffle the dataset\ncombinedTrain = combinedTrain.sample(frac=1, random_state= 2)\n# delete the index\ncombinedTrain.reset_index(drop=True, inplace=True)\n# show the head of combined df\ncombinedTrain.head()\n# reconstruct the dataframe \ntrain = combinedTrain[featureCols]\ntrainTargets = combinedTrain[labelCols]","9038f2a1":"# Step 2 - Get columns of features and labels\nlabelCols = [col for col in trainTargets.columns if col != 'sig_id']\nfeatureCols = [col for col in train.columns if col !='sig_id']\nprint('Number of different labels:', len(labelCols))\nprint('Number of features:', len(featureCols))","53c0698a":"lEncode = preprocessing.LabelEncoder()\nforEncoding = ['cp_time','cp_type','cp_dose']\n# for train\n\nfor i in forEncoding:\n  lEncode.fit(train[i])\n  x = lEncode.transform(train[i])\n  train[i] = x\n\n# for test\nfor i in forEncoding:\n  lEncode.fit(test[i])\n  x = lEncode.transform(test[i])\n  test[i] = x","7651c24d":"# Step 3 - Separate the Dataset\nX = train[featureCols]\nX_test = test[featureCols]\ny = trainTargets","dace2bf6":"cat_features= ['cp_time', 'cp_type', 'cp_dose']\nnonCat = [col for col in X.columns if col not in cat_features]","b3513b39":"# Step 4 - Standardize the data using a robust scalar\nrsc = RobustScaler()\nX[nonCat] = rsc.fit_transform(X[nonCat])\nX_test[nonCat] = rsc.transform(X_test[nonCat])\nX = pd.DataFrame(X)\nX_test = pd.DataFrame(X_test)","5e9cbd53":"# Step 5 - Convert to numpy array\nX = X.iloc[:].to_numpy()\nX_test = X_test.iloc[:].to_numpy()\ny = y.iloc[:].to_numpy()","3b1ab5d7":"y.shape","606163f2":"cat_features= ['cp_time', 'cp_type', 'cp_dose']","5a0b4a33":"# second model - Catboost\nparams = {'loss_function':'MultiClass',\n          #'eval_metric':'los',\n          #'cat_features': cat_features,\n          'task_type': 'GPU',\n          'border_count': 32,\n          'verbose': 200,\n          'random_seed': 1,\n          #'learning_rate' : 0.1,\n          #'random_strength' : 0.1,\n          #'depth' : 8,\n          'early_stopping_rounds' : 200,\n          #'leaf_estimation_method' : 'Newton'\n         }\ncbc = CatBoostClassifier(**params)\nclassifier = MultiOutputClassifier(cbc)","a10cace6":"classifier.fit(X, y)\npreds = classifier.predict_proba(X_test)\npreds = np.array(preds)[:,:,1].T\ntest_preds = (test_preds\/2)  + (preds\/2)","ac56e86a":"test_preds","5f3aff27":"#classifier.fit(train_data, # instead of X_train, y_train\n#          eval_set=valid_data, # instead of (X_valid, y_valid)\n#          use_best_model=True, \n#          plot=True\n#         );","a8ea4d7d":"'''loss1 = []\nkf=KFold(n_splits=3, random_state=100, shuffle=True)\nfor iteration, (train_index,validation_index) in enumerate(kf.split(X, y)):\n    print('ITERATION NUMBER - ', iteration)\n    X_train, X_val = X[train_index], X[validation_index]\n    y_train, y_val = y[train_index], y[validation_index]\n    \n    classifier.fit(X_train, y_train)\n    val_preds = classifier.predict_proba(X_val) \n    val_preds = np.array(val_preds)[:,1].T #(num_labels,num_samples,prob_0\/1)\n    \n    loss = log_loss(np.ravel(y_val), np.ravel(val_preds))\n    loss1.append(loss)\n    preds = classifier.predict_proba(X_test)\n    print(preds)\n    preds = np.array(preds)[:,1].T #(num_labels,num_samples,prob_0\/1)\n    test_preds += preds \/ 3 #take average of 10 models\n    \nprint(loss1)\nprint('Mean CV loss across folds', np.mean(loss1))'''","90a65bf3":"## from discussion at https:\/\/www.kaggle.com\/c\/lish-moa\/discussion\/180304\n## and code from https:\/\/www.kaggle.com\/fchmiel\/xgboost-baseline-multilabel-classification\/comments\n\n# set control test preds to 0\nmask = test['cp_type']=='ctl_vehicle' # wherever the cp_type = 'ctl_vehicle' \n\ntest_preds[mask] = 0","bcab5911":"# read the sample submission and make a new DF\nsub = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')\n#sub = sub.iloc[:]\n#sub = sub.to_frame()\nsub.iloc[:,1:] = test_preds","8639ae06":"sub.to_csv('submission.csv', index=False)","ebfff591":"#  Basic Model Prototyping","0dc67863":"# Import Dataset #","9ec6dc2a":"# Submission","2360fcb9":"# Basic Dataset Operations #","9b241633":"# PreProcessing","9e62e6ac":"# Second Model"}}