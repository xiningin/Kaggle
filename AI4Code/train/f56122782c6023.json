{"cell_type":{"820d3ffe":"code","d52f18ef":"code","dafddb77":"code","9a9a189d":"code","8063a427":"code","a1ea1c23":"code","be8d8a24":"code","bf1025ff":"code","8bacd1f1":"code","dbc4c729":"code","c422af2f":"markdown","84373aa6":"markdown","e3f2302a":"markdown","ff543ed1":"markdown","7c9b3d49":"markdown","60490b1a":"markdown"},"source":{"820d3ffe":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d52f18ef":"\n!pip install tensorflow==1.14\n!pip install tensorflow-gpu==1.14\n!pip install pyvi\n!pip install keras-self-attention==0.35.0\n!pip install keras-multi-head==0.16.0\n!pip install keras-layer-normalization==0.10.0\n!pip install annoy==1.15.1\n\n","dafddb77":"import tensorflow as tf","9a9a189d":"x = tf.random.uniform([3, 3])\nprint(\"Is there a GPU available: \"),\nprint(tf.test.is_gpu_available())\n\nprint(\"Is the Tensor on GPU #0:  \"),\nprint(x.device.endswith('GPU:0'))\n\nprint(\"Device name: {}\".format((x.device)))","8063a427":"# From spacy english model\nEMOTICONS = set(\"\"\"\n:)\n:-)\n:))\n:-))\n:)))\n:-)))\n(:\n(-:\n=)\n(=\n\")\n:]\n:-]\n[:\n[-:\n:o)\n(o:\n:}\n:-}\n8)\n8-)\n(-8\n;)\n;-)\n(;\n(-;\n:(\n:-(\n:((\n:-((\n:(((\n:-(((\n):\n)-:\n=(\n>:(\n:')\n:'-)\n:'(\n:'-(\n:\/\n:-\/\n=\/\n=|\n:|\n:-|\n:1\n:P\n:-P\n:p\n:-p\n:O\n:-O\n:o\n:-o\n:0\n:-0\n:()\n>:o\n:*\n:-*\n:3\n:-3\n=3\n:>\n:->\n:X\n:-X\n:x\n:-x\n:D\n:-D\n;D\n;-D\n=D\nxD\nXD\nxDD\nXDD\n8D\n8-D\n^_^\n^__^\n^___^\n>.<\n>.>\n<.<\n._.\n;_;\n-_-\n-__-\nv.v\nV.V\nv_v\nV_V\no_o\no_O\nO_o\nO_O\n0_o\no_0\n0_0\no.O\nO.o\nO.O\no.o\n0.0\no.0\n0.o\n@_@\n<3\n<33\n<333\n<\/3\n(^_^)\n(-_-)\n(._.)\n(>_<)\n(*_*)\n(\u00ac_\u00ac)\n\u0ca0_\u0ca0\n\u0ca0\ufe35\u0ca0\n(\u0ca0_\u0ca0)\n\u00af\\(\u30c4)\/\u00af\n(\u256f\u00b0\u25a1\u00b0\uff09\u256f\ufe35\u253b\u2501\u253b\n><(((*>\n\"\"\".split())\n\nDEFAULT_MAX_FEATURES = 200000\nDEFAULT_MAX_LENGTH = 300\n","a1ea1c23":"from keras.layers import Layer\nimport keras.backend as K\n\nclass AttLayer(Layer):\n    def __init__(self, context_size):\n        self._context_size = context_size\n        self.supports_masking = True\n        # self._linear = Dense(context_size, activation = \"tanh\")\n        super(AttLayer, self).__init__()\n\n    def build(self, input_shape):\n        self._W = self.add_weight(\n            name = \"W\",\n            shape = (input_shape[-1], self._context_size),\n            initializer=\"he_normal\",\n            trainable=True\n        )\n        self._b = self.add_weight(\n            name = \"b\",\n            shape = (1, self._context_size),\n            initializer=\"constant\",\n            trainable=True\n        )\n        self._context = self.add_weight(\n            name = \"context\",\n            shape = (self._context_size, 1),\n            initializer = \"he_normal\",\n            trainable = True\n        )\n        super(AttLayer, self).build(input_shape)\n\n\n    def compute_mask(self, input, input_mask=None):\n        return input_mask\n\n\n    def call(self, input, mask = None):\n        # input: (N, T, M)\n        rep = K.tanh(K.dot(input, self._W) + self._b) # (N, T, C)\n        score = K.squeeze(K.dot(rep, self._context), axis = -1) # (N, T)\n\n        weight = K.exp(score)\n        if mask is not None:\n            weight *= K.cast(mask, K.floatx())\n\n        weight \/= K.cast(K.sum(weight, axis = 1, keepdims = True) + K.epsilon(), K.floatx())\n\n\n        # weight = softmax(score, axis = -1) # (N, T)\n        op = K.batch_dot(input, weight, axes = (1, 1)) # (N, M)\n\n        return op\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], input_shape[-1])\n\n\n\nclass AdditiveLayer(Layer):\n    def __init__(self):\n        super(AdditiveLayer, self).__init__()\n\n    def build(self, input_shape):\n        self._w = self.add_weight(\n            name = \"w\",\n            shape = (1, input_shape[-1]),\n            initializer=\"constant\",\n            trainable=True\n        )\n        super(AdditiveLayer, self).build(input_shape)\n\n\n\n    def call(self, input):\n        return input + self._w\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n","be8d8a24":"#!\/usr\/bin\/env python\n# -*- coding: utf-8 -*-\n\nimport pandas as pd\nimport copy\nimport os\nimport numpy as np\nimport re\nimport pickle\nimport keras.backend as K\n\nfrom tqdm import tqdm\nfrom collections import defaultdict\nfrom os.path import abspath\nfrom spacy.lang.vi import Vietnamese\nfrom gensim.models.keyedvectors import KeyedVectors\nfrom sklearn.metrics import f1_score\nimport string\n\n\ndef read_csv(path):\n    df = pd.read_csv(path)\n    category = df['Label'].unique()\n    category_to_id = {cate: idx for idx, cate in enumerate(category)}\n    y_ = df['Label'].map(category_to_id).values\n    labels = np.zeros((len(y_), y_.max()+1))\n    labels[np.arange(len(y_)), y_] = 1\n    content = df['Content'].values\n    with open('..\/input\/kpdl-data\/token.pkl', 'rb') as f:\n        unpickler = pickle.Unpickler(f)\n        docs = unpickler.load()\n    return [labels, content, docs]\n\n\ndef split_array(arr, condition):\n    if len(arr) == 0:\n        return []\n    result = []\n    accumulated = [arr[0]]\n    for ele in arr[1:]:\n        if condition(ele):\n            result.append(copy.deepcopy(accumulated))\n            accumulated = [copy.deepcopy(ele)]\n        else:\n            accumulated.append(copy.deepcopy(ele))\n    result.append(copy.deepcopy(accumulated))\n    return result\n\n\ndef read_file(file_path, is_train=True):\n    file_path = abspath(file_path)\n    data_lines = list(\n        filter(lambda x: x != '', open(file_path).read().split('\\n')))\n    pattern = ('train' if is_train else 'test') + '_[0-9]{5}'\n    datas = split_array(data_lines, lambda x: bool(re.match(pattern, x)))\n    if is_train:\n        result_array = list(map(\n            lambda x: [x[0], ' '.join(x[1:-1]), int(x[-1])], datas))\n    else:\n        result_array = list(map(lambda x: [x[0], ' '.join(x[1:])], datas))\n    columns = ['name', 'text', 'label'] if is_train else ['name', 'text']\n    return pd.DataFrame(result_array, columns=columns)\n\n\ndef tokenize(texts):\n    nlp = Vietnamese()\n    docs = []\n    for text in texts:\n        tokens = np.array([postprocess_token(token.text) for token in nlp(text.lower())[1:-1]])\n        docs.append(tokens)\n\n    return docs\n\n\ndef postprocess_token(token):\n    if token in string.punctuation:\n        return '<punct>'\n    elif token.isdigit():\n        return '<number>'\n    else:\n        return token\n\n\n\ndef make_embedding(texts, embedding_path, max_features):\n    embedding_path = abspath(embedding_path)\n\n    def get_coefs(word, *arr):\n        return word, np.asarray(arr, dtype='float32')\n\n    if embedding_path.endswith('.vec'):\n        embedding_index = dict(get_coefs(*o.strip().split(\" \"))\n                               for o in open(embedding_path))\n        mean_embedding = np.mean(np.array(list(embedding_index.values())))\n    elif embedding_path.endswith('bin'):\n        embedding_index = KeyedVectors.load_word2vec_format(\n            embedding_path, binary=True)\n        mean_embedding = np.mean(embedding_index.vectors, axis=0)\n    embed_size = mean_embedding.shape[0]\n    word_index = sorted(list({word.lower() for sentence in texts for word in sentence}))\n    \n    nb_words = min(max_features, len(word_index))\n    embedding_matrix = np.zeros((nb_words + 1, embed_size))\n    i = 1\n    word_map = defaultdict(lambda: nb_words)\n    for word in word_index:\n        if i >= max_features:\n            continue\n        if word in embedding_index:\n            embedding_matrix[i] = embedding_index[word]\n        else:\n            embedding_matrix[i] = mean_embedding\n        word_map[word] = i\n        i += 1\n    \n    embedding_matrix[-1] = mean_embedding\n    return embed_size, word_map, embedding_matrix\n\ndef text_to_sequences(texts, word_map, max_len=DEFAULT_MAX_LENGTH):\n    texts_id = []\n    for sentence in texts:\n        sentence = [word_map[word.lower()] for word in sentence][:max_len]\n        padded_setence = np.pad(\n            sentence, (0, max(0, max_len - len(sentence))), 'constant', constant_values=0)\n        texts_id.append(padded_setence)\n    return np.array(texts_id)\n\n\n# def find_threshold(pred_proba, y_true, metric = f1_score):\n#     cur_acc = 0\n#     cur_thres = 0\n#     for ind in range(len(pred_proba) - 1):\n#         threshold = (pred_proba[ind][0] + pred_proba[ind + 1][0]) \/ 2\n#         pred = (pred_proba > threshold).astype(np.int8)\n#         acc = metric(pred, y_true, average='weighted')\n#         if acc > cur_acc:\n#             cur_thres = threshold\n#             cur_acc = acc\n\n#     return cur_thres\n\n\ndef f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n\n        Only computes a batch-wise average of precision.\n\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))\n\n\ndef predictions_to_submission(test_data, predictor):\n    tqdm.pandas()\n    submission = test_data[['id']]\n    submission['label'] = test_data['text'].progress_apply(predictor)\n    return submission\n\n\n# HELPERS FOR HIERARCHICAL MODEL:\ndef sent_tokenize(texts):\n    nlp = Vietnamese()\n    nlp.add_pipe(nlp.create_pipe('sentencizer'))\n    docs = []\n    for text in texts:\n        text_tokenized = []\n        if (len(text) > 3):\n            for sentence in nlp(text.lower()[1:-1]).sents:\n                sent_tokens = np.array([postprocess_token(token.text) for token in sentence])\n                text_tokenized.append(sent_tokens)\n        else:\n            text_tokenized.append([])\n        docs.append(text_tokenized)\n\n    return docs\n\n\ndef sent_embedding(tokenized_texts, embedding_path, max_features):\n    embedding_path = abspath(embedding_path)\n\n    def get_coefs(word, *arr):\n        return word, np.asarray(arr, dtype='float32')\n\n    if embedding_path.endswith('.vec'):\n        embedding_index = dict(get_coefs(*o.strip().split(\" \"))\n                               for o in open(embedding_path))\n        mean_embedding = np.mean(np.array(list(embedding_index.values())))\n    elif embedding_path.endswith('bin'):\n        embedding_index = KeyedVectors.load_word2vec_format(\n            embedding_path, binary=True)\n        mean_embedding = np.mean(embedding_index.vectors, axis=0)\n    embed_size = mean_embedding.shape[0]\n    word_index = {word.lower() for text in tokenized_texts for sentence in text for word in sentence}\n    nb_words = min(max_features, len(word_index))\n    embedding_matrix = np.zeros((nb_words + 1, embed_size))\n\n    i = 1\n    word_map = defaultdict(lambda: nb_words)\n    for word in word_index:\n        if i >= max_features:\n            continue\n        if word in embedding_index:\n            embedding_matrix[i] = embedding_index[word]\n        else:\n            embedding_matrix[i] = mean_embedding\n        word_map[word] = i\n        i += 1\n    embedding_matrix[-1] = mean_embedding\n    return embed_size, word_map, embedding_matrix\n\n\ndef text_sents_to_sequences(texts, word_map, max_nb_sent, max_sent_len):\n    ret = []\n    for i in range(len(texts)):\n        text_vecs = []\n        for j in range(len(texts[i])):\n            if (j < max_nb_sent):\n                sent_vecs = []\n                for k in range(len(texts[i][j])):\n                    if (k < max_sent_len):\n                        sent_vecs.append(word_map[texts[i][j][k]])\n                if (len(sent_vecs) < max_sent_len):\n                    sent_vecs = np.pad(\n                        sent_vecs,\n                        (0, max(0, max_sent_len - len(sent_vecs))),\n                        'constant',\n                        constant_values=0\n                    )\n                text_vecs.append(sent_vecs)\n\n\n        if (len(text_vecs) < max_nb_sent):\n            text_vecs = np.pad(\n                text_vecs,\n                ((0, max_nb_sent - len(text_vecs)), (0, 0)),\n                'constant',\n                constant_values=0\n            )\n\n        ret.append(text_vecs)\n\n    return np.array(ret)\n\n","bf1025ff":"from keras.models import Model\nfrom keras.layers import \\\n    Dense, Embedding, Input, \\\n    Conv1D, MaxPool1D, \\\n    Dropout, BatchNormalization, \\\n    Bidirectional, \\\n    Concatenate, Flatten, Add, CuDNNLSTM\n\nfrom keras import optimizers\n# import tensorflow as tf\n# from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n\ndef TextCNN(embeddingMatrix = None, embed_size = 400, max_features = 20000, output_feat = 23, maxlen = DEFAULT_MAX_LENGTH, filter_sizes = {2, 3, 4, 5}, use_fasttext = False, trainable = True, use_additive_emb = False):\n    if use_fasttext:\n        inp = Input(shape=(maxlen, embed_size))\n        x = inp\n    else:\n        inp = Input(shape = (maxlen, ))\n        x = Embedding(input_dim = max_features, output_dim = embed_size, weights = [embeddingMatrix], trainable = trainable)(inp)\n\n\n    conv_ops = []\n    for filter_size in filter_sizes:\n        conv = Conv1D(128, filter_size, activation = 'relu')(x)\n        pool = MaxPool1D(5)(conv)\n        conv_ops.append(pool)\n\n    concat = Concatenate(axis = 1)(conv_ops)\n    # concat = Dropout(0.1)(concat)\n    concat = BatchNormalization()(concat)\n\n\n    conv_2 = Conv1D(128, 5, activation = 'relu')(concat)\n    conv_2 = MaxPool1D(5)(conv_2)\n    conv_2 = BatchNormalization()(conv_2)\n    # conv_2 = Dropout(0.1)(conv_2)\n\n    conv_3 = Conv1D(128, 5, activation = 'relu')(conv_2)\n    conv_3 = MaxPool1D(5)(conv_3)\n    conv_3 = BatchNormalization()(conv_3)\n    # conv_3 = Dropout(0.1)(conv_3)\n\n\n    flat = Flatten()(conv_3)\n\n    op = Dense(64, activation = \"relu\")(flat)\n    # op = Dropout(0.5)(op)\n    op = BatchNormalization()(op)\n    op = Dense(output_feat, activation = \"softmax\")(op)\n\n    model = Model(inputs = inp, outputs = op)\n\n    optim = optimizers.Adam(lr=0.001)\n    model.compile(loss = 'binary_crossentropy', optimizer = optim, metrics = ['accuracy', f1])\n    return model\n\n\ndef VDCNN(embeddingMatrix = None, embed_size = 400, max_features = 20000, output_feat = 23, maxlen = DEFAULT_MAX_LENGTH, filter_sizes = {2, 3, 4, 5}, use_fasttext = False, trainable = True, use_additive_emb = False):\n    if use_fasttext:\n        inp = Input(shape=(maxlen, embed_size))\n        x = inp\n    else:\n        inp = Input(shape = (maxlen, ))\n        x = Embedding(input_dim = max_features, output_dim = embed_size, weights = [embeddingMatrix], trainable = trainable)(inp)\n\n\n    conv_ops = []\n    for filter_size in filter_sizes:\n        conv = Conv1D(128, filter_size, activation = 'relu')(x)\n        pool = MaxPool1D(5)(conv)\n        conv_ops.append(pool)\n\n    concat = Concatenate(axis = 1)(conv_ops)\n    # concat = Dropout(0.1)(concat)\n    concat = BatchNormalization()(concat)\n\n\n    conv_2_main = Conv1D(128, 5, activation = 'relu', padding='same')(concat)\n    conv_2_main = BatchNormalization()(conv_2_main)\n    conv_2_main = Conv1D(128, 5, activation = 'relu', padding='same')(conv_2_main)\n    conv_2_main = BatchNormalization()(conv_2_main)\n    conv_2 = Add()([concat, conv_2_main])\n    conv_2 = MaxPool1D(pool_size = 2, strides = 2)(conv_2)\n    # conv_2 = BatchNormalization()(conv_2)\n    # conv_2 = Dropout(0.1)(conv_2)\n\n    conv_3_main = Conv1D(128, 5, activation = 'relu', padding='same')(conv_2)\n    conv_3_main = BatchNormalization()(conv_3_main)\n    conv_3_main = Conv1D(128, 5, activation = 'relu', padding='same')(conv_3_main)\n    conv_3_main = BatchNormalization()(conv_3_main)\n    conv_3 = Add()([conv_2, conv_3_main])\n    conv_3 = MaxPool1D(pool_size = 2, strides = 2)(conv_3)\n    # conv_3 = BatchNormalization()(conv_3)\n    # conv_3 = Dropout(0.1)(conv_3)\n\n\n    flat = Flatten()(conv_3)\n\n    op = Dense(64, activation = \"relu\")(flat)\n    # op = Dropout(0.5)(op)\n    op = BatchNormalization()(op)\n    op = Dense(output_feat, activation = \"softmax\")(op)\n\n    model = Model(inputs = inp, outputs = op)\n\n    optim = optimizers.Adam(lr=0.001)\n\n    model.compile(loss = 'binary_crossentropy', optimizer = optim, metrics = ['accuracy', f1])\n    return model\n\n\n\n# Based on http:\/\/konukoii.com\/blog\/2018\/02\/19\/twitter-sentiment-analysis-using-combined-lstm-cnn-models\/\ndef LSTMCNN(embeddingMatrix = None, embed_size = 400, output_feat = 23, max_features = 20000, maxlen = DEFAULT_MAX_LENGTH, filter_sizes = {2, 3, 4, 5}, use_fasttext = False, trainable = True, use_additive_emb = False):\n    if use_fasttext:\n        inp = Input(shape=(maxlen, embed_size))\n        x = inp\n    else:\n        inp = Input(shape = (maxlen, ))\n        x = Embedding(input_dim = max_features, output_dim = embed_size, weights = [embeddingMatrix], trainable = trainable)(inp)\n\n\n    x = Bidirectional(CuDNNLSTM(128, return_sequences = True))(x)\n\n\n    conv_ops = []\n    for filter_size in filter_sizes:\n        conv = Conv1D(128, filter_size, activation = 'relu')(x)\n        pool = MaxPool1D(5)(conv)\n        conv_ops.append(pool)\n\n    concat = Concatenate(axis = 1)(conv_ops)\n    concat = Dropout(0.5)(concat)\n    # concat = BatchNormalization()(concat)\n\n\n    conv_2 = Conv1D(128, 5, activation = 'relu')(concat)\n    conv_2 = MaxPool1D(5)(conv_2)\n    # conv_2 = BatchNormalization()(conv_2)\n    conv_2 = Dropout(0.5)(conv_2)\n\n    # conv_3 = Conv1D(128, 5, activation = 'relu')(conv_2)\n    # conv_3 = MaxPool1D(5)(conv_3)\n    # conv_3 = BatchNormalization()(conv_3)\n    # conv_3 = Dropout(0.1)(conv_3)\n\n\n    flat = Flatten()(conv_2)\n\n    op = Dense(64, activation = \"relu\")(flat)\n    op = Dropout(0.5)(op)\n    # op = BatchNormalization()(op)\n    op = Dense(output_feat, activation = \"sigmoid\")(op)\n\n    model = Model(inputs = inp, outputs = op)\n    optim = optimizers.Adam(lr=0.001)\n\n    model.compile(loss = 'binary_crossentropy', optimizer = optim, metrics = ['accuracy', f1])\n    return model\n","8bacd1f1":"from keras.models import Model\nfrom keras.layers import \\\n    Dense, Embedding, Input, \\\n    GRU, LSTM, Bidirectional, \\\n    GlobalMaxPool1D, GlobalAveragePooling1D, Dropout, \\\n    Lambda, Concatenate, TimeDistributed, CuDNNLSTM, CuDNNGRU\nfrom keras_self_attention import SeqSelfAttention, SeqWeightedAttention\nfrom keras.activations import softmax\nfrom keras_layer_normalization import LayerNormalization\nfrom keras.utils.vis_utils import plot_model\n# import tensorflow as tf\n# from tensorflow.compat.v1.keras.layers import CuDNNLSTM, CuDNNGRU\n\n\n\n\ndef RNNKeras(embeddingMatrix = None,output_feats=23, embed_size = 400, max_features = DEFAULT_MAX_FEATURES, maxlen = DEFAULT_MAX_LENGTH, use_fasttext = False, trainable = True, use_additive_emb = False):\n    if use_fasttext:\n        inp = Input(shape=(maxlen, embed_size))\n        x = inp\n    else:\n        inp = Input(shape = (maxlen, ))\n        x = Embedding(input_dim = max_features, output_dim = embed_size, weights = [embeddingMatrix], trainable = trainable)(inp)\n\n\n    x = Bidirectional(CuDNNGRU(128, return_sequences = True))(x)\n    x = Dropout(0.5)(x)\n    x = Bidirectional(CuDNNGRU(128, return_sequences = True))(x)\n    x = Dropout(0.5)(x)\n\n    max_pool = GlobalMaxPool1D()(x)\n    avg_pool = GlobalAveragePooling1D()(x)\n    last = Lambda(lambda x: x[:, 0, :])(x)\n    concat_pool = Concatenate(axis = -1)([last, max_pool, avg_pool])\n\n    op = Dense(64, activation = \"relu\")(concat_pool)\n    op = Dropout(0.5)(op)\n    op = Dense(output_feats, activation = \"sigmoid\")(op)\n\n    model = Model(inputs = inp, outputs = op)\n    optim = optimizers.Adam(lr=0.001)\n    model.compile(loss = 'binary_crossentropy', optimizer = optim, metrics = ['accuracy', f1])\n    return model\n\ndef RNNKerasCPU(embeddingMatrix = None, embed_size = 400, output_feats=23, max_features = DEFAULT_MAX_FEATURES, maxlen = DEFAULT_MAX_LENGTH, use_fasttext = False, trainable = True, use_additive_emb = False):\n    if use_fasttext:\n        inp = Input(shape=(maxlen, embed_size))\n        x = inp\n    else:\n        inp = Input(shape = (maxlen, ))\n        x = Embedding(input_dim = max_features, output_dim = embed_size, weights = [embeddingMatrix], trainable = trainable)(inp)\n\n\n    x = Bidirectional(GRU(128, return_sequences = True, recurrent_dropout = 0.5, dropout = 0.5))(x)\n    # x = Dropout(0.5)(x)\n    x = Bidirectional(GRU(128, return_sequences = True, recurrent_dropout = 0.5, dropout = 0.5))(x)\n    # x = Dropout(0.5)(x)\n\n    max_pool = GlobalMaxPool1D()(x)\n    avg_pool = GlobalAveragePooling1D()(x)\n    last = Lambda(lambda x: x[:, 0, :])(x)\n    concat_pool = Concatenate(axis = -1)([last, max_pool, avg_pool])\n\n    op = Dense(64, activation = \"relu\")(concat_pool)\n    op = Dropout(0.5)(op)\n    op = Dense(output_feats, activation = \"sigmoid\")(op)\n\n    model = Model(inputs = inp, outputs = op)\n    optim = optimizers.Adam(lr=0.001)\n    model.compile(loss = 'binary_crossentropy', optimizer = optim, metrics = ['accuracy', f1])\n    return model\n\ndef LSTMKeras(embeddingMatrix = None, embed_size = 400,output_feats=23, trainable = True, max_features = DEFAULT_MAX_FEATURES, maxlen = DEFAULT_MAX_LENGTH):\n    inp = Input(shape = (maxlen, ))\n    x = Embedding(input_dim = max_features, output_dim = embed_size, weights = [embeddingMatrix], trainable = trainable)(inp)\n    x = Bidirectional(CuDNNLSTM(50, return_sequences = True))(x)\n    # x = Dropout(0.1)(x)\n    x = Bidirectional(CuDNNLSTM(50, return_sequences = True))(x)\n    x = Dropout(0.1)(x)\n    x = GlobalMaxPool1D()(x)\n    x = Dense(50, activation = \"relu\")(x)\n    x = Dropout(0.1)(x)\n    x = Dense(output_feats, activation = \"sigmoid\")(x)\n    model = Model(inputs = inp, outputs = x)\n    optim = optimizers.Adam(lr=0.001)\n    model.compile(loss = 'binary_crossentropy', optimizer = optim, metrics = ['accuracy', f1])\n    return model\n\n\ndef SARNNKerasCPU(embeddingMatrix = None, output_feats=23, embed_size = 400, max_features = DEFAULT_MAX_FEATURES, maxlen = DEFAULT_MAX_LENGTH, use_fasttext = False, trainable = True, use_additive_emb = False):\n    if use_fasttext:\n        inp = Input(shape=(maxlen, embed_size))\n        x = inp\n    else:\n        inp = Input(shape = (maxlen, ))\n        x = Embedding(input_dim = max_features, output_dim = embed_size, weights = [embeddingMatrix], trainable = trainable)(inp)\n\n\n    x = Bidirectional(LSTM(128, return_sequences = True))(x)\n    x = SeqSelfAttention(\n        # attention_type = SeqSelfAttention.ATTENTION_TYPE_MUL,\n        attention_regularizer_weight=1e-4,\n    )(x)\n    # x = LayerNormalization()(x)\n    x = Dropout(0.5)(x)\n\n    x = Bidirectional(LSTM(128, return_sequences = True))(x)\n    x = SeqWeightedAttention()(x)\n    # x = LayerNormalization()(x)\n    x = Dropout(0.5)(x)\n\n    x = Dense(64, activation = \"relu\")(x)\n    x = Dropout(0.5)(x)\n    x = Dense(output_feats, activation = \"sigmoid\")(x)\n    model = Model(inputs = inp, outputs = x)\n    optim = optimizers.Adam(lr=0.001)\n    model.compile(loss = 'binary_crossentropy', optimizer = optim, metrics = ['accuracy', f1])\n    return model\n\ndef SARNNKeras(embeddingMatrix = None, output_feats=23, embed_size = 400, max_features = DEFAULT_MAX_FEATURES, maxlen = DEFAULT_MAX_LENGTH, rnn_type = CuDNNLSTM, use_fasttext = False, trainable = True, use_additive_emb = False):\n    if use_fasttext:\n        inp = Input(shape=(maxlen, embed_size))\n        x = inp\n    else:\n        inp = Input(shape = (maxlen, ))\n        x = Embedding(input_dim = max_features, output_dim = embed_size, weights = [embeddingMatrix], trainable = trainable)(inp)\n\n\n\n    x = Bidirectional(rnn_type(128, return_sequences = True))(x)\n    x = SeqSelfAttention(\n        # attention_type = SeqSelfAttention.ATTENTION_TYPE_MUL,\n        attention_regularizer_weight=1e-4,\n    )(x)\n    # x = LayerNormalization()(x)\n    x = Dropout(0.5)(x)\n\n    x = Bidirectional(rnn_type(128, return_sequences = True))(x)\n    x = SeqWeightedAttention()(x)\n    # x = LayerNormalization()(x)\n    x = Dropout(0.5)(x)\n\n    x = Dense(64, activation = \"relu\")(x)\n    x = Dropout(0.5)(x)\n    x = Dense(output_feats, activation = \"sigmoid\")(x)\n    model = Model(inputs = inp, outputs = x)\n    optim = optimizers.Adam(lr=0.001)\n    model.compile(loss = 'binary_crossentropy', optimizer = optim, metrics = ['accuracy', f1])\n    return model\n\n\ndef HRNNCPU(embeddingMatrix = None, embed_size = 400, output_feats=23, max_features = DEFAULT_MAX_FEATURES, max_nb_sent = 3, max_sent_len = DEFAULT_MAX_LENGTH, trainable = True, use_additive_emb = False):\n    sent_inp = Input(shape = (max_sent_len, embed_size))\n    embed = Embedding(\n        input_dim = max_features,\n        output_dim = embed_size,\n        weights = [embeddingMatrix],\n        trainable = trainable\n    )(sent_inp)\n\n\n    word_lstm = Bidirectional(LSTM(128, dropout = 0.2, recurrent_dropout = 0.2))(embed)\n    sent_encoder = Model(sent_inp, word_lstm)\n\n    doc_input = Input(shape = (max_nb_sent, max_sent_len))\n    doc_encoder = TimeDistributed(sent_encoder)(doc_input)\n    sent_lstm = Bidirectional(LSTM(128, dropout = 0.2, recurrent_dropout = 0.2))(doc_encoder)\n    preds = Dense(output_feats, activation = \"sigmoid\")(sent_lstm)\n    model = Model(inputs = doc_input, outputs = preds)\n    optim = optimizers.Adam(lr=0.001)\n    model.compile(loss = 'binary_crossentropy', optimizer = optim, metrics = ['accuracy', f1])\n    return model\n\ndef HRNN(embeddingMatrix = None, embed_size = 400, output_feats=23, max_features = DEFAULT_MAX_FEATURES, max_nb_sent = 3, max_sent_len = DEFAULT_MAX_LENGTH, trainable = True, use_additive_emb = False):\n    sent_inp = Input(shape = (max_sent_len, embed_size))\n    embed = Embedding(\n        input_dim = max_features,\n        output_dim = embed_size,\n        weights = [embeddingMatrix],\n        trainable = trainable\n    )(sent_inp)\n\n    word_lstm = Bidirectional(CuDNNLSTM(128))(embed)\n    sent_encoder = Model(sent_inp, word_lstm)\n\n    doc_input = Input(shape = (max_nb_sent, max_sent_len))\n    doc_encoder = TimeDistributed(sent_encoder)(doc_input)\n    sent_lstm = Bidirectional(CuDNNLSTM(128))(doc_encoder)\n    preds = Dense(output_feats, activation = \"sigmoid\")(sent_lstm)\n    model = Model(inputs = doc_input, outputs = preds)\n    optim = optimizers.Adam(lr=0.001)\n    model.compile(loss = 'binary_crossentropy', optimizer = optim, metrics = ['accuracy', f1])\n    return model\n\n\ndef OriginalHARNNCPU(embeddingMatrix = None, output_feats=23, embed_size = 400, max_features = DEFAULT_MAX_FEATURES, max_nb_sent = 3, max_sent_len = DEFAULT_MAX_LENGTH, use_fasttext = False, trainable = True, use_additive_emb = False):\n    if use_fasttext:\n        sent_inp = Input(shape = (max_sent_len, embed_size))\n        embed = sent_inp\n    else:\n        sent_inp = Input(shape = (max_sent_len, ))\n        embed = Embedding(\n            input_dim = max_features,\n            output_dim = embed_size,\n            weights = [embeddingMatrix],\n            trainable = trainable\n        )(sent_inp)\n\n\n    word_lstm = Bidirectional(LSTM(128, dropout = 0.5, recurrent_dropout = 0.5, return_sequences = True))(embed)\n    word_att = AttLayer(context_size = 256)(word_lstm)\n    sent_encoder = Model(sent_inp, word_att)\n\n    doc_input = Input(shape = (max_nb_sent, max_sent_len))\n    doc_encoder = TimeDistributed(sent_encoder)(doc_input)\n    sent_lstm = Bidirectional(LSTM(128, dropout = 0.5, recurrent_dropout = 0.5, return_sequences = True))(doc_encoder)\n    sent_att = AttLayer(context_size = 256)(sent_lstm)\n    preds = Dense(output_feats, activation = \"sigmoid\")(sent_att)\n    model = Model(inputs = doc_input, outputs = preds)\n    optim = optimizers.Adam(lr=0.001)\n    model.compile(loss = 'binary_crossentropy', optimizer = optim, metrics = ['accuracy', f1])\n    return model\n\ndef OriginalHARNN(embeddingMatrix = None, embed_size = 400, output_feats=23, max_features = DEFAULT_MAX_FEATURES, max_nb_sent = 3, max_sent_len = DEFAULT_MAX_LENGTH, use_fasttext = False, trainable = True, use_additive_emb = False):\n    if use_fasttext:\n        sent_inp = Input(shape = (max_sent_len, embed_size))\n        embed = sent_inp\n    else:\n        sent_inp = Input(shape = (max_sent_len, ))\n        embed = Embedding(\n            input_dim = max_features,\n            output_dim = embed_size,\n            weights = [embeddingMatrix],\n            trainable = trainable\n        )(sent_inp)\n\n    word_lstm = Bidirectional(CuDNNLSTM(128, return_sequences = True))(embed)\n    word_att = AttLayer(context_size = 256)(word_lstm)\n    word_att = Dropout(0.5)(word_att)\n    sent_encoder = Model(sent_inp, word_att)\n\n    doc_input = Input(shape = (max_nb_sent, max_sent_len))\n    doc_encoder = TimeDistributed(sent_encoder)(doc_input)\n    sent_lstm = Bidirectional(CuDNNLSTM(128, return_sequences = True))(doc_encoder)\n    sent_att = AttLayer(context_size = 256)(sent_lstm)\n    sent_att = Dropout(0.5)(sent_att)\n    preds = Dense(output_feats, activation = \"sigmoid\")(sent_att)\n    model = Model(inputs = doc_input, outputs = preds)\n    optim = optimizers.Adam(lr=0.001)\n    model.compile(loss = 'binary_crossentropy', optimizer = optim, metrics = ['accuracy', f1])\n    return model\n\n\n\n\ndef HARNNCPU(embeddingMatrix = None, output_feats=23, embed_size = 400, max_features = DEFAULT_MAX_FEATURES, max_nb_sent = 3, max_sent_len = DEFAULT_MAX_LENGTH, use_fasttext = False, trainable = True, use_additive_emb = False):\n    if use_fasttext:\n        sent_inp = Input(shape = (max_sent_len, embed_size))\n        embed = sent_inp\n    else:\n        sent_inp = Input(shape = (max_sent_len, ))\n        embed = Embedding(\n            input_dim = max_features,\n            output_dim = embed_size,\n            weights = [embeddingMatrix],\n            trainable = trainable\n        )(sent_inp)\n\n\n    word_lstm = Bidirectional(LSTM(128, dropout = 0.5, recurrent_dropout = 0.5, return_sequences = True))(embed)\n    word_att = SeqWeightedAttention()(word_lstm)\n    sent_encoder = Model(sent_inp, word_att)\n\n    doc_input = Input(shape = (max_nb_sent, max_sent_len))\n    doc_encoder = TimeDistributed(sent_encoder)(doc_input)\n    sent_lstm = Bidirectional(LSTM(128, dropout = 0.5, recurrent_dropout = 0.5, return_sequences = True))(doc_encoder)\n    sent_att = SeqWeightedAttention()(sent_lstm)\n    preds = Dense(output_feats, activation = \"sigmoid\")(sent_att)\n    model = Model(inputs = doc_input, outputs = preds)\n    optim = optimizers.Adam(lr=0.001)\n    model.compile(loss = 'binary_crossentropy', optimizer = optim, metrics = ['accuracy', f1])\n    return model\n\n\n\ndef HARNN(embeddingMatrix = None, output_feats=23, embed_size = 400, max_features = DEFAULT_MAX_FEATURES, max_nb_sent = 3, max_sent_len = DEFAULT_MAX_LENGTH, use_fasttext = False, trainable = True, use_additive_emb = False):\n    if use_fasttext:\n        sent_inp = Input(shape = (max_sent_len, embed_size))\n        embed = sent_inp\n    else:\n        sent_inp = Input(shape = (max_sent_len, ))\n        embed = Embedding(\n            input_dim = max_features,\n            output_dim = embed_size,\n            weights = [embeddingMatrix],\n            trainable = trainable\n        )(sent_inp)\n\n    word_lstm = Bidirectional(CuDNNLSTM(128, return_sequences = True))(embed)\n    word_att = SeqWeightedAttention()(word_lstm)\n    word_att = Dropout(0.5)(word_att)\n    sent_encoder = Model(sent_inp, word_att)\n    plot_model(sent_encoder, to_file='{}.png'.format(\"HARNN1\"), show_shapes=True, show_layer_names=True)\n\n\n    doc_input = Input(shape = (max_nb_sent, max_sent_len))\n    doc_encoder = TimeDistributed(sent_encoder)(doc_input)\n    sent_lstm = Bidirectional(CuDNNLSTM(128, return_sequences = True))(doc_encoder)\n    sent_att = SeqWeightedAttention()(sent_lstm)\n    sent_att = Dropout(0.5)(sent_att)\n    preds = Dense(output_feats, activation = \"sigmoid\")(sent_att)\n    model = Model(inputs = doc_input, outputs = preds)\n    optim = optimizers.Adam(lr=0.001)\n    \n    model.compile(loss = 'binary_crossentropy', optimizer = optim, metrics = ['accuracy', f1])\n    return model","dbc4c729":"from sklearn.model_selection import train_test_split\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nimport argparse\nimport os\nimport numpy as np\nimport datetime\nimport pandas as pd\n# from augment import similar_augment, create_sim_dict, similar_augment_from_sim_dict\nfrom sklearn.metrics import f1_score\nfrom keras.utils.vis_utils import plot_model\nimport tensorflow as tf\n\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# tf.config.optimizer.set_jit(True)\nprint(tf.test.is_gpu_available())\n\n\ndef train_model(name, model, embedding_path, max_features, should_find_threshold, should_mix, return_prob, trainable, print_model, model_high):\n\n    model_name = name+'-'+'-'.join(\n        '.'.join(str(datetime.datetime.now()).split('.')[:-1]).split(' '))\n\n    data_path_csv = '..\/input\/kpdl-data\/data_token_1.csv'\n    # embedding_path = 'baomoi.model.bin'\n    df = pd.read_csv('..\/input\/kpdl-data\/train_remove_noise.csv')\n    category = df['Label'].unique()\n    category_to_id = {cate: idx for idx, cate in enumerate(category)}\n    id_to_category = {idx: cate for idx, cate in enumerate(category)}\n    labels, _, docs = read_csv(data_path_csv)\n    \n    train_tokenized_texts, test_tokenized_texts, labels_train, labels_test = train_test_split(docs, labels, test_size = 0.3)\n    train_tokenized_texts, valid_tokenized_texts, labels_train, labels_valid = train_test_split(train_tokenized_texts, labels_train, test_size = 0.1)\n\n    embed_size, word_map, embedding_mat = make_embedding(\n        list(train_tokenized_texts) + list(valid_tokenized_texts) +\n        list(test_tokenized_texts) if should_mix else list(train_tokenized_texts) + list(valid_tokenized_texts),\n        embedding_path,\n        max_features\n    )\n\n    texts_id_train = text_to_sequences(train_tokenized_texts, word_map)\n\n    texts_id_val = text_to_sequences(valid_tokenized_texts, word_map)\n    print('Number of train data: {}'.format(labels.shape))\n\n    # texts_id_train, texts_id_val, labels_train, labels_val = train_test_split(\n    #     texts_id, labels, test_size=0.05)\n\n    model_path = '.\/models\/{}-version'.format(model_name)\n\n    try:\n        os.mkdir('.\/models')\n    except:\n        print('Folder already created')\n    try:\n        os.mkdir(model_path)\n    except:\n        print('Folder already created')\n\n    checkpoint = ModelCheckpoint(\n        filepath='{}\/models.hdf5'.format(model_path),\n        monitor='val_f1', verbose=1,\n        mode='max',\n        save_best_only=True\n    )\n    early = EarlyStopping(monitor='val_f1', mode='max', patience=5)\n    lr_scheduler = ReduceLROnPlateau(monitor='val_f1', factor=0.2,\n                              patience=5, min_lr=1e6)\n    callbacks_list = [checkpoint, early, lr_scheduler]    \n    batch_size = 16\n    epochs = 100\n\n    model = model(\n        embeddingMatrix=embedding_mat,\n        embed_size=embed_size,\n        max_features=embedding_mat.shape[0],\n        trainable = trainable\n    )\n    if print_model:\n        plot_model(model, to_file='{}.png'.format(model_high), show_shapes=True, show_layer_names=True)\n        return\n\n    history = model.fit(\n        texts_id_train, labels_train,\n        validation_data=(texts_id_val, labels_valid),\n        callbacks=callbacks_list,\n        epochs=epochs,\n        batch_size=batch_size\n    )\n\n    model.load_weights('{}\/models.hdf5'.format(model_path))\n    prediction_prob = model.predict(texts_id_val)\n    # if should_find_threshold:\n    #     OPTIMAL_THRESHOLD = find_threshold(prediction_prob, labels_valid)\n    # else:\n    #     OPTIMAL_THRESHOLD = 0.5\n    # print('OPTIMAL_THRESHOLD: {}'.format(OPTIMAL_THRESHOLD))\n    # prediction = (prediction_prob > OPTIMAL_THRESHOLD).astype(np.int8)\n    print('F1 validation score: {}'.format(f1_score(prediction_prob.argmax(-1), labels_valid.argmax(-1), average='weighted')))\n\n    test_id_texts = text_to_sequences(test_tokenized_texts, word_map)\n    test_prediction = model.predict(test_id_texts)\n    print('F1 test score: {}'.format(f1_score(test_prediction.argmax(-1), labels_test.argmax(-1), average='weighted')))\n\n    \n    plt.figure(figsize=(20, 20))\n    conf_mat = confusion_matrix(test_prediction.argmax(-1), labels_test.argmax(-1))\n    sns.heatmap(conf_mat, annot=True, fmt='d',\n                xticklabels=id_to_category.values(), yticklabels=id_to_category.values())\n    plt.ylabel('Actual')\n    plt.xlabel('Predicted')\n    plt.savefig(model_name+'.jpg')\n    \n    plt.figure(figsize=(8,6))\n    print(history.history.keys())\n    \n    plt.plot(history.history['f1'])\n    plt.plot(history.history['val_f1'])\n    plt.title('F1 score')\n    plt.ylabel('score')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.savefig(model_name+'_f1.jpg')\n    plt.show()\n\n    # df_predicton = pd.read_csv(\".\/data\/sample_submission.csv\")\n    # if return_prob:\n    #     df_predicton[\"label\"] = test_prediction\n    # else:\n    #     df_predicton[\"label\"] = (\n    #         test_prediction > OPTIMAL_THRESHOLD).astype(np.int8)\n\n    # print('Number of test data: {}'.format(df_predicton.shape[0]))\n    # df_predicton.to_csv('{}\/prediction.csv'.format(model_path), index=False)\n\n\nmodel_dict = {\n    'RNNKeras': RNNKeras,\n    'RNNKerasCPU': RNNKerasCPU,\n    'LSTMKeras': LSTMKeras,\n    'SARNNKerasCPU': SARNNKerasCPU,\n    'SARNNKeras': SARNNKeras,\n    'TextCNN': TextCNN,\n    'LSTMCNN': LSTMCNN,\n    'VDCNN': VDCNN,\n    'HRNN': HRNN,\n    'HARNN': HARNN,\n    'OriginalHARNN': OriginalHARNN    \n}\n\nmodel = 'SARNNKeras'\nembedding = '..\/input\/kpdl-data\/baomoi.model.bin'\nmax_feats = DEFAULT_MAX_FEATURES\nfind_threshold = False\nmix = True\nprob = True\nfix_embed = False\nprint_model = False\nmodel_high = model\n\n\ntrain_model(model, model_dict[model], embedding,\n                max_feats, find_threshold, mix, prob, fix_embed,\n                print_model, model)\n","c422af2f":"### CNN.py","84373aa6":"### RNN.py","e3f2302a":"### Constant.py","ff543ed1":"### component.py","7c9b3d49":"### Main train","60490b1a":"### utils.py"}}