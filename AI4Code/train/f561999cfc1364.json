{"cell_type":{"30ad6945":"code","4f546eb0":"code","c9e4d39f":"code","5085896b":"code","bfe3d8d6":"code","83105b33":"code","b2e6b752":"code","245127d8":"code","064cf71c":"code","b2752352":"code","0e93278e":"code","037518c6":"code","985a54fd":"code","779503e4":"code","d60a6053":"code","4d743eff":"markdown","6b574956":"markdown","cc80395d":"markdown","1b136c40":"markdown","edfcddf9":"markdown","a4228754":"markdown","86e8da1b":"markdown","1e6fd65e":"markdown","1c13ebf4":"markdown","6549e88d":"markdown","89134cff":"markdown","86f79793":"markdown","b0393101":"markdown","6e92ad10":"markdown","df0e2b84":"markdown","4bab947a":"markdown","cadb87fe":"markdown","63fd7897":"markdown"},"source":{"30ad6945":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","4f546eb0":"dataset = pd.read_csv('..\/input\/position-salaries\/Position_Salaries.csv')\nX = dataset.iloc[:, 1:-1].values # Level in the data\ny = dataset.iloc[:, -1].values # salary in the data","c9e4d39f":"print(X)","5085896b":"print(y)","bfe3d8d6":"y.shape","83105b33":"y = y.reshape(len(y),1)","b2e6b752":"y.shape","245127d8":"print(y)","064cf71c":"from sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler() # scalar object of X\nsc_y = StandardScaler() # scalar object of y\nX = sc_X.fit_transform(X) # scaled values of X\ny = sc_y.fit_transform(y) # scaled values of y","b2752352":"print(X)","0e93278e":"print(y)","037518c6":"from sklearn.svm import SVR\nregressor = SVR(kernel = 'rbf') # Gaussian rbf kernal is used in here\nregressor.fit(X, y)","985a54fd":"sc_y.inverse_transform(regressor.predict(sc_X.transform([[6.5]])))","779503e4":"plt.scatter(sc_X.inverse_transform(X), sc_y.inverse_transform(y), color = 'red')\nplt.plot(sc_X.inverse_transform(X), sc_y.inverse_transform(regressor.predict(X)), color = 'blue')\nplt.title('Truth or Bluff (SVR)')\nplt.xlabel('Position level')\nplt.ylabel('Salary')\nplt.show()","d60a6053":"X_grid = np.arange(min(sc_X.inverse_transform(X)), max(sc_X.inverse_transform(X)), 0.1)\nX_grid = X_grid.reshape((len(X_grid), 1))\nplt.scatter(sc_X.inverse_transform(X), sc_y.inverse_transform(y), color = 'red')\nplt.plot(X_grid, sc_y.inverse_transform(regressor.predict(sc_X.transform(X_grid))), color = 'blue')\nplt.title('Truth or Bluff (SVR)')\nplt.xlabel('Position level')\nplt.ylabel('Salary')\nplt.show()","4d743eff":"For a better understanding of SVR if u don't have any idea what so ever, visit the link: \n[Support Vector Machining](https:\/\/towardsdatascience.com\/support-vector-machines-svm-c9ef22815589)","6b574956":"## Predicting a new result","cc80395d":"The SVR has different kernals which we will be dealing in the upcoming notebooks but let's keep this one simple.\n\nclass sklearn.svm.SVR(*, kernel='rbf', degree=3, gamma='scale', coef0=0.0, tol=0.001, C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=-1)\n\n\nVisit the link for more details: \n[SVR kernals and info](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVR.html)\n\n\nWhy did we train the model on the whole data?\n\nBecause we have very less data and hence we are training the model on entire data.","1b136c40":"## Feature Scaling","edfcddf9":"## Importing the dataset","a4228754":"## Visualising the SVR results","86e8da1b":"## Importing the libraries","1e6fd65e":"## Training the SVR model on the whole dataset","1c13ebf4":"Our model is trained on the scaled data and is related to the scaled target. \n\nIf we want to predict salary for the 6.5 level we should provide the model after scaling it using sc_X so the model predicts the value which is a scaled output of y (target)\n\nWe have to inverse transform the output\/prediction using the sc_y to convert into normal value which can be understood by us.\n\nthe sc_X.transform( )  takes a 2 Dimensional array so we have to convert the level 6.5 into a 2D array by [[6.5]] and this is supplied to the sc_X.transform( )\n\nNow we need to provide this data to the model which is regressor.predict( ) which will be predicting a scaled output in terms of y\n\nThen the sc_y.inverse_transform( ) is applied to make it look normal value.\n","6549e88d":"Let's predict the salary of a person whose level is 6.5 i.e a person who will be working in between the level of 6  and 7","89134cff":"Now we have trained the model and now we can use the model to predict values from it.","86f79793":"Here we will be using a unscalled data by inverse transform and plotting it.\n\nHere the RED points are of the original data.\n\nThe blue line is the predicted values from our models which are quiet near to the real values except for the last one. which might be considered as an outlier by our model and hence.\n\nFrom this we can be sure that the model has not been overfitted.","b0393101":"# 1.5 Support Vector Regression (SVR)","6e92ad10":"Here y is a one dimensional array, we have to convert it to a 2 Dimensional array if we want to apply standard scaling.\n\nHence we do the reshape to convert it to 2D array.","df0e2b84":"## Visualising the SVR results (for higher resolution and smoother curve)","4bab947a":"# Like this notebook then upvote it.\n\n# Need to improve it then comment below.\n\n# Enjoy Machine Learning","cadb87fe":"For the Support Vector Machining(SVM) - Support vector Regressor(SVR), we need to scale the data because it doesn't have coefficients like the one we seen in linear and polynomial regression.\n\nWe are going to scale the X (features) and y (target) seperately as they are different and for scaling and inverse scaling we will be using the same scaling as of those variable.\n\nOnly sc_X will be used for all scaling and inverse scaling\/ transform of features and the same is applicable for y.\n\nNever apply the scaling of features object to inverse scaling or transform the y (target) as their mean and standard deviation and other values saved in the scalar object will be different. so make sure to remember this one.","63fd7897":"For better understanding of current notebook for beginners go through the links:\n\n [1.1 Data Preprocessing](http:\/\/www.kaggle.com\/saikrishna20\/data-preprocessing-tools)\n\n\n[1.2 Simple linear Regression](https:\/\/www.kaggle.com\/saikrishna20\/1-2-simple-linear-regression) \n\n\n[1.3 Multiple linear Regression with Backward Elimination](http:\/\/www.kaggle.com\/saikrishna20\/1-3-multiple-linear-regression-backward-eliminat)\n\n[1.4 Polynomial Linear Regression](https:\/\/www.kaggle.com\/saikrishna20\/1-4-polynomial-linear-regression)\n\n\nIt basically tells u about the preprocessing & Linear Regression which will help u in understanding this notebook better"}}