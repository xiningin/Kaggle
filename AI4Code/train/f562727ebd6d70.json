{"cell_type":{"878b4a26":"code","14dadb34":"code","0bf33e10":"code","c799d02d":"code","9f95b1ec":"code","05dee690":"code","eb404b59":"code","8220da75":"code","c29cdfd8":"code","4dafb889":"code","1fa64623":"code","13a63180":"code","e3fbb6d1":"code","19384c74":"code","8b86717e":"code","c4b20e8a":"code","14c6c537":"code","c9c259db":"code","551df131":"code","1c865e1a":"code","be04f051":"code","c9bb97c2":"code","d626ee00":"code","3ef70144":"code","9aa9f0d7":"code","0b495bd1":"code","c888b98c":"code","2d16cba4":"code","57263ab3":"code","4d3ed5c3":"code","3d9e943a":"code","57047453":"code","6b8f484f":"markdown","808e0938":"markdown","8012ebec":"markdown","e70ac18b":"markdown","8b594357":"markdown","d5751a17":"markdown","381b0322":"markdown","c999e3ee":"markdown","681affff":"markdown","40cfe48f":"markdown","396991e1":"markdown","12def370":"markdown","c8928f7f":"markdown","89ea6bc9":"markdown","381f13c3":"markdown","d49a6d55":"markdown","c9b094cf":"markdown","2128ddd8":"markdown","e6031c5d":"markdown","f8b5ee2e":"markdown","1b800ea6":"markdown","026d6590":"markdown","eafe2656":"markdown","7db73de9":"markdown","eafe8695":"markdown","2cb01bec":"markdown","73c40976":"markdown","e4fc63eb":"markdown","38004c64":"markdown","9ab0f51b":"markdown","28dd8879":"markdown","eae715e9":"markdown","fe314558":"markdown","122dbb21":"markdown","561d5ea6":"markdown","d46d71ef":"markdown","9babc4d9":"markdown","db9153f3":"markdown","9b1a9b2f":"markdown","25353f2c":"markdown","8e1841ca":"markdown","2003af85":"markdown"},"source":{"878b4a26":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# For visualizing\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# For ignoring warning\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","14dadb34":"df = pd.read_csv('..\/input\/heart-failure-prediction\/heart.csv')\ndf.head()","0bf33e10":"# Information about the data\ndf.info()","c799d02d":"# Renaming the columns\nrenamed_columns = ['age', 'sex', 'chest_pain_type', 'resting_blood_pressure', 'cholesterol', \n                   'fasting_blood_sugar', 'rest_ecg', 'max_heart_rate_achieved','exercise_induced_angina',\n                   'st_depression', 'st_slope', 'target']\ndf.columns = renamed_columns","9f95b1ec":"# Correcting the data types\ncategorical_columns = ['sex', 'chest_pain_type', 'fasting_blood_sugar', 'rest_ecg',\n                       'exercise_induced_angina', 'st_slope']\ndf[categorical_columns] = df[categorical_columns].astype('object')","05dee690":"# Variation of Heart Attack rate with each categorical variable\nfig, ax=plt.subplots(2,3, figsize=(20,14), sharey=True)\n\nfor col, axis in zip(categorical_columns, ax.ravel()):\n    sns.countplot(x=col, data=df, ax=axis, order = np.sort(df[col].unique()))\n    for i in axis.patches:    \n        axis.text(x = i.get_x() + i.get_width()\/2, y = i.get_height()+3,\n                s = f\"{i.get_height()}\", \n                ha = 'center', size = 16, rotation = 0, color = 'black')\n# #                 bbox=dict(boxstyle=\"circle,pad=0.5\", fc='pink', ec=\"pink\", lw=2))\n\n#     axis.set_title(f'{col}', fontsize=16, y=1.01);\n    axis.set_xticklabels(np.sort(df[col].unique()), fontsize=16)\n    axis.set_ylabel('Number of People', fontsize=14);\n    axis.set_xlabel(col, fontsize=16);\n\nfig.text(0.5, 1.01, 'Frequency plot of the categorical features', ha = 'center', fontsize = 20)\nplt.tight_layout()","eb404b59":"# Also renaming the values in the catergorical features\ndf['sex'].replace({'F':'female','M':'male'}, inplace=True)\n\ndf['chest_pain_type'].replace({'TA':'typical angina','ATA':'atypical angina','NAP':'non-anginal pain','ASY':'asymptomatic'}, inplace=True)\n\ndf['fasting_blood_sugar'].replace({0:'lower than 120mg\/ml',1:'greater than 120mg\/ml'}, inplace=True)\n\ndf['rest_ecg'].replace({0:'normal',1:'ST-T wave abnormality'}, inplace=True)\n\ndf['exercise_induced_angina'].replace({0:'no',1:'yes'}, inplace=True)","8220da75":"# Total missing values in each column\ndf.isnull().sum()","c29cdfd8":"# Let's look at the outliers and the distribution of the numeric features\nnumeric_var = [i for i in df.select_dtypes(['int','float']).columns[:-1]]\nplt.style.use('seaborn')\nfig, axis = plt.subplots(5, 2, figsize = (12, 15))\nfor i, num_var in enumerate(numeric_var):\n    \n    # Checking for the outliers using boxplot\n    sns.boxplot(y = num_var, data = df, ax = axis[i][0], color = 'skyblue')\n    \n    # Checking for the distribution using kdeplot\n    sns.kdeplot(x = num_var, data = df, ax = axis[i][1], color = 'skyblue',\n               fill = True, alpha = 0.6, linewidth = 1.5)\n    \n    axis[i][0].set_ylabel(f\"{num_var}\", fontsize = 12)\n    axis[i][0].set_xlabel(None)\n    axis[i][1].set_xlabel(None)\n    axis[i][1].set_ylabel(None)\n\nfig.suptitle('Analysing Numeric Features', fontsize = 16, y = 1)\nplt.tight_layout()","4dafb889":"# Removing samples with extreme values\ndf = df[(df['resting_blood_pressure']<=180)&(df['resting_blood_pressure']>0)]\ndf = df[(df['cholesterol']<=500)&(df['cholesterol']>0)]\ndf = df[df['st_depression']<=5]","1fa64623":"# Correlation between numeric variables\nfig=plt.figure(figsize=(10,7))\naxis=sns.heatmap(df[numeric_var].corr(), annot=True, linewidths=3, square=True, cmap='Blues', fmt=\".0%\")\n\naxis.set_title('Correlation between the features', fontsize=16, y=1.05);\naxis.set_xticklabels(numeric_var, fontsize=12, rotation = 45)\naxis.set_yticklabels(numeric_var, fontsize=12, rotation= 0);","13a63180":"# Relation between numeric and categorical features\ndf_copy = df.copy()\ndf_copy['target'].replace({0:'No Heart Disease', 1:'Heart Disease'}, inplace=True)\n\n\ntarget = df_copy.groupby(['target']).mean(['age', 'resting_blood_pressure', 'cholesterol',\n                          'max_heart_rate_achieved', 'st_depression'])\ngender = df_copy.groupby(['sex']).mean(['age', 'resting_blood_pressure', 'cholesterol',\n                          'max_heart_rate_achieved', 'st_depression'])\nchest_pain_type = df_copy.groupby(['chest_pain_type']).mean(['age', 'resting_blood_pressure', 'cholesterol',\n                          'max_heart_rate_achieved', 'st_depression'])\nfasting_blood_sugar = df_copy.groupby(['fasting_blood_sugar']).mean(['age', 'resting_blood_pressure', 'cholesterol',\n                          'max_heart_rate_achieved', 'st_depression'])\nexercise_induced_angina = df_copy.groupby(['exercise_induced_angina']).mean(['age', 'resting_blood_pressure', 'cholesterol',\n                          'max_heart_rate_achieved', 'st_depression'])\nst_slope = df_copy.groupby(['st_slope']).mean(['age', 'resting_blood_pressure', 'cholesterol',\n                          'max_heart_rate_achieved', 'st_depression'])\n\nrelation = pd.concat([target, gender, chest_pain_type, exercise_induced_angina, st_slope]).reset_index()\n\nrename_list = ['Target', 'Target',\n               'Gender','Gender',\n               'chest_pain_type','chest_pain_type','chest_pain_type','chest_pain_type',\n               'fasting_blood_sugar','fasting_blood_sugar',\n               'exercise_induced_angina','exercise_induced_angina','exercise_induced_angina']\n\nrelation.rename(index= dict(zip(list(range(13)),rename_list)), inplace=True, columns={'index':'values'})\nrelation.index.names = ['Categorical Features']\nrelation = relation.round(2)\nrelation.reset_index(inplace=True)\n\nrelation","e3fbb6d1":"# Checking for class imbalance\ncolors = ['#90EE90', '#ffcccb']\nsns.set(palette = colors)\n\nfig = plt.figure(figsize = (10, 6))\nax = sns.countplot(x = 'target', data = df)\n\nfor i in ax.patches:\n    ax.text(x = i.get_x() + i.get_width()\/2, y = i.get_height()\/2, \n            s = f\"{int(i.get_height()\/len(df)*100)}%\", \n            ha = 'center', size = 50, weight = 'bold', rotation = 0, color = 'white')\n\nplt.title(\"Checking for the class imbalance\", size = 20)\n\nax.set_xticklabels(['No Heart Disease', 'Heart Disease'], fontsize=16)\nplt.xlabel('', fontsize = 16)\nplt.ylabel('Number of People', fontsize = 16);","19384c74":"# One hot encoding - Converting categorical features into encoded form of 0 and 1\ndf_encoded = pd.get_dummies(df, drop_first = True)\ndf_encoded.head()","8b86717e":"from sklearn.model_selection import train_test_split\n\nX = df_encoded.copy()\ny = X.pop('target')\n\n# Train and Test split\nX_train, X_test, y_train, y_test = train_test_split(X, y,  train_size=0.8, stratify = y) # 80% training 20% testing data","c4b20e8a":"# Checking for the class proprtion in train and test set\nprint('Class proportion in train set: \\n',y_train.value_counts()\/y_train.value_counts().sum())\nprint('-'*50)\nprint('Class proportion in test set: \\n',y_test.value_counts()\/y_test.value_counts().sum())","14c6c537":"from sklearn.preprocessing import StandardScaler\n\n# Scaling the numeric features\nscalar = StandardScaler()\n\nX_train_scaled = pd.DataFrame(scalar.fit_transform(X_train[numeric_var]), columns = numeric_var)\nX_test_scaled = pd.DataFrame(scalar.transform(X_test[numeric_var]), columns = numeric_var)\n\nX_train_final = pd.concat([X_train_scaled.reset_index(drop=True), X_train.iloc[:,5:].reset_index(drop=True)], axis=1)\nX_test_final = pd.concat([X_test_scaled.reset_index(drop=True), X_test.iloc[:,5:].reset_index(drop=True)], axis=1)","c9c259db":"# Checking the distribution of the numeric data after scaling\ncolors = ['#ADD8E6']\nsns.set(palette = colors)\nfig, ax = plt.subplots(nrows = 2, ncols = 5, figsize = (15, 7), constrained_layout = True)\n\nfor axis, num_var in zip(ax.ravel(), numeric_var): \n    sns.kdeplot(data = X_train_final, x = num_var, ax = axis,\n                fill = True, multiple = 'stack', alpha = 0.6, linewidth = 1.5)\n    axis.set_ylabel(None)\n    axis.set_xlabel(None)\n\nfor i, num_var in zip(range(0, 5), numeric_var): \n    sns.histplot(data = X_train_final, x = num_var, ax = ax[1][i])\n    ax[1][i].set_ylabel(None)\n    ax[1][i].set_xlabel(f'{num_var}', fontsize = 14)\n    \nfig.text(0.5, 1.05, 'Distribution of the numeric data after scaling', ha = 'center', fontsize = 20);","551df131":"from sklearn.linear_model import LogisticRegressionCV\n\nlr = LogisticRegressionCV(cv=5, random_state=99)\nlr.fit(X_train_final, y_train)","1c865e1a":"from sklearn.metrics import roc_auc_score\n\nprint('Train AUC-ROC score is', roc_auc_score(y_train, lr.predict_proba(X_train_final)[:, 1]))\nprint('Test AUC-ROC score is', roc_auc_score(y_test, lr.predict_proba(X_test_final)[:, 1]))","be04f051":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\n\ntree = DecisionTreeClassifier()\nparameters = {'max_depth':[3,5,7], 'min_samples_leaf':[5,10,15]}\n\ntree_clf = GridSearchCV(tree, parameters, cv=5)\ntree_clf.fit(X_train_final, y_train)","c9bb97c2":"print('Train AUC-ROC score is', roc_auc_score(y_train, tree_clf.predict_proba(X_train_final)[:, 1]))\nprint('Test AUC-ROC score is', roc_auc_score(y_test, tree_clf.predict_proba(X_test_final)[:, 1]))","d626ee00":"from sklearn.ensemble import RandomForestClassifier\n\nforest = RandomForestClassifier()\nparameters = {'max_depth':[3,5,7], 'min_samples_leaf':[5,10,15]}\n\nforest_clf = GridSearchCV(forest, parameters, cv=5)\nforest_clf.fit(X_train_final, y_train)","3ef70144":"print('Train AUC-ROC score is', roc_auc_score(y_train, forest_clf.predict_proba(X_train_final)[:, 1]))\nprint('Test AUC-ROC score is', roc_auc_score(y_test, forest_clf.predict_proba(X_test_final)[:, 1]))","9aa9f0d7":"from sklearn.svm import SVC\n\nsvm_params = {'C':[0.01,0.1,0.5,1,1.5,2,5], 'kernel':['rbf', 'linear']}\nsvm = SVC(probability=True)\n\nsvm_clf = GridSearchCV(svm, svm_params, cv=5)\nsvm_clf.fit(X_train_final, y_train)","0b495bd1":"print('Train AUC-ROC score is', roc_auc_score(y_train, svm_clf.predict_proba(X_train_final)[:, 1]))\nprint('Test AUC-ROC score is', roc_auc_score(y_test, svm_clf.predict_proba(X_test_final)[:, 1]))","c888b98c":"from sklearn.metrics import precision_score, recall_score\n\nrecall_score(y_test, lr.predict(X_test_final))\ndf_performance = pd.DataFrame(columns=['Model', 'AUC-ROC', 'Accuracy', 'Recall', 'Precision'])\n\nmodels = [lr, tree_clf, forest_clf, svm_clf]\n\nfor model in models:\n    auc = roc_auc_score(y_test, model.predict_proba(X_test_final)[:, 1])\n    accuracy = model.score(X_test_final, y_test)\n    recall = recall_score(y_test, model.predict(X_test_final))\n    precision = precision_score(y_test, model.predict(X_test_final))\n    \n    df_performance = df_performance.append({'Model':model, 'AUC-ROC':auc, 'Accuracy':accuracy,\n                                            'Recall':recall, 'Precision':precision}, ignore_index=True)\n    \ndf_performance","2d16cba4":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\npredictions = forest_clf.predict(X_test_final)\ncm = confusion_matrix(y_test, predictions, labels=forest_clf.classes_)\n\nfig, ax = plt.subplots(1,1, figsize = (10, 6), constrained_layout = True)\nsns.heatmap(cm, annot=True, linewidths=3, square=True, cmap='Blues', annot_kws={\"size\": 18}, ax=ax)\n\nax.set_title('Confusion Matrix', fontsize=16, y=1.05);\nax.set_xticklabels(['Prediction 0','Prediction 1'], fontsize=12, rotation = 0)\nax.set_yticklabels(['Actual 0','Actual 1'], fontsize=12, rotation= 0);","57263ab3":"import shap\n\n# Training a Random Forest classifier on the best parameters from the forest_clf\nforest = RandomForestClassifier(max_depth=5, min_samples_leaf=5)\nforest.fit(X_train_final, y_train)\n\nexplainer = shap.Explainer(forest)\nshap_values = explainer.shap_values(X_train_final)","4d3ed5c3":"shap.summary_plot(shap_values[1], X_train_final, plot_type='dot')","3d9e943a":"# Local interpreation for sample number 11\ni = 11\nprint(y_test.values[i])\nX_test_final.iloc[[i],:]","57047453":"# Initialize your Jupyter notebook with initjs(), otherwise you will get an error message.\nshap.initjs()\n\nexplainer = shap.TreeExplainer(forest)\nshap_values = explainer.shap_values(X_test_final)\nshap.force_plot(explainer.expected_value[1], np.array(shap_values[1])[i], features=X_test_final.loc[i],\n                feature_names=X_test_final.columns)","6b8f484f":"## Observation \ud83d\udd0e\n\n> There is a weak negetive correlation between `age` and `max_heart_rate_achieved`. As the age increases the maximum heart rate decreases.\n\n> Patients having heart disease are little older\n\n> From `age` vs. `st_depression` graph, most of the people having heart disease have high ST depression","808e0938":"There are no missing values in the data","8012ebec":"<h4><center>  <div style=\"background-color:lightgreen;border-radius:10px; padding: 10px;\">Removing the unecessary columns or values \u274c<\/div><\/center><\/h4>\n\n> All the columns look important so I won't remove any column\n\n> Let's look at the frequency of the values in the categorical features","e70ac18b":"<h3><center>  <div style=\"background-color:skyblue;border-radius:10px; padding: 10px;\">Class Imbalance measure \u2696\ufe0f<\/div><\/center><\/h3>","8b594357":"### Global Interpretation\n\n> Shows the overall effect of the features","d5751a17":"## Observation \ud83d\udd0e\n","381b0322":"<h1><center>  <div style=\"background-color:skyblue;border-radius:10px; padding: 10px;\">Cleaning the Data \ud83d\udebf<\/div><\/center><\/h1>\n\n> Data cleaning involes following steps:\n> - Correcting the data types\n> - Removing the unecessary columns or values\n> - Handling missing values\n> - Removing outliers\n> - Removing multicollinearity\n\n> Let's do them one by one","c999e3ee":"<h3><center>  <div style=\"background-color:lightpink;border-radius:10px; padding: 10px;\">Decision Tree \ud83c\udf33<\/div><\/center><\/h3>","681affff":"<h1><center>  <div style=\"background-color:skyblue;border-radius:10px; padding: 10px;\">Reading the Data \ud83d\udcd6<\/div><\/center><\/h1>","40cfe48f":"<h2><center>  <div style=\"background-color:pink;border-radius:10px; padding: 10px;\">Hope you like it, if so don't forget to upvote!<\/div><\/center><\/h2>","396991e1":"<h3><center>  <div style=\"background-color:lightblue;border-radius:10px; padding: 10px;\">Analysing the model \ud83e\uddd0<\/div><\/center><\/h3>","12def370":"#### One hot encoding","c8928f7f":"<h1><center>  <div style=\"background-color:skyblue;border-radius:10px; padding: 10px;\"> Machine Learning \ud83e\udd16<\/div><\/center><\/h1>\n\n> First we will create simple model for baseline and then move to more complex models to improve further\n\n> For evaluation purpose I am using AUC ROC score which is mostly used for classification problems\n\n**AUC ROC Curve \ud83d\udcc8**\n\n> Area Under Curve (AUC) Receiver Operating Characteristic (ROC) Curve\nThe Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes and is used as a summary of the ROC curve. The higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes.","89ea6bc9":"> In red, we have positive Shap values that show everything that pushes the sales value in the positive (1) direction. While the Shap value in blue represents everything that pushes it towards a negetive (0) direction.\n\n**Understanding the Local Impact**\n\n> `st_slope_Up` has a negetive imapct on the target, and here it value is 1, so it will tend to lower the chance of a heart disease.\n\n> `excercise_indused_angina_Y` has positive impact on the target, and here it value is 1, so it will tend to increse the chance of a heart disease.\n\n> `st_slope_Flat` has positive impact on the target, and here it value is 0, so it will tend to lower the chance of a heart disease.\n\nSimilar for other features...","381f13c3":"## Observation \ud83d\udd0e\n\n- **Chest Pain Type**\n\n> For the chest pain of the type *asymptomatic*, there is high (~74%) chance of a heart disease.\n\n> For chest pain type *atypical angina* and *non-agninal pain*, 1 of out of 5 have a heart disease, which is very less compared to 'asymptomatic' type\n\n- **Exercise Induced Angina**\n\n> 82% of the people with the angina induced due to exercise have a heart disease\n\n> People with **no** angina induced due to exercise, have less chance (1 out of 4) of a heart disease\n\n- **Fasting Blood Sugar**\n\n> 65% of the people with fasting blood sugar is more than 120 mg\/ml have a heart disease, but there is not much relation between people with FBS less than 120 mg\/ml and having a heart disease.\n\n\n- **Rest ECG**\n\n> If there is a ST-T wave abnormility in the rest ECG then there is little increse in the chance of having a heart disease.\n\n<center><img src =\"https:\/\/s3.amazonaws.com\/static.wd7.us\/9\/99\/T_wave_morphology.png\"><\/center>\n\n- **Gender**\n\n> 21% of woman and 56% of men have a heart disease. This is quite obvious as heart disease are more common to men than woman. \n\n> According to **Harvard Medical School** [report](https:\/\/www.health.harvard.edu\/heart-health\/the-heart-attack-gender-gap), Heart attacks strike men at younger ages than women but survival rates are worse in women.\n\n- **St Slope**\n\n> The ECG with st segment depression with down and flat sloping have very high chance of heart disease.\n\n> About 80% of people having `st slope` down or flat have a heart disease while it is only 13% for up slope\n\n<center><img src=\"https:\/\/litfl.com\/wp-content\/uploads\/2018\/10\/ST-segment-depression-upsloping-downsloping-horizontal.png\"><\/center>","d49a6d55":"> The class proportion is roughly equal in train and test dataset","c9b094cf":"#### Train and Test Split\n\n> Train and test split is performed after one hot encoding as it may happen that some values of categorical features which are present in train set are not available in the test set which can change the one hot encoding for that features in train and test set\n\n> Train and test split is performed before scaling the numeric features so that the test data should not learn anything about the train data","2128ddd8":"<center><img src = \"https:\/\/c.tenor.com\/O88TQgYrR2kAAAAC\/humanheart-beating.gif\"><\/center>\n\n<h1><center>  <div style=\"background-color:skyblue;border-radius:10px; padding: 10px;\">Introduction<\/div><\/center><\/h1>\n\n\ud83e\ude7a Background\n>The heart is one of the most important parts as it pumps blood around your body, delivering oxygen and nutrients to your cells and removing waste products. Every day, the average human heart beats around 100,000 times, pumping 2,000 gallons of blood through the body. Inside your body there are 60,000 miles of blood vessels. A heart attack occurs when an artery supplying your heart with blood and oxygen becomes blocked. Fatty deposits build up over time, forming plaques in your heart's arteries. If a plaque ruptures, a blood clot can form and block your arteries, causing a heart attack. \n\n\n\ud83d\udc99 Motivation\n>The signs of a woman having a heart attack are much less noticeable than the signs of a male. In women, heart attacks may feel uncomfortable squeezing, pressure, fullness, or pain in the center of the chest. It may also cause pain in one or both arms, the back, neck, jaw or stomach, shortness of breath, nausea and other symptoms. Men experience typical symptoms of heart attack, such as chest pain , discomfort, and stress. They may also experience pain in other areas, such as arms, neck , back, and jaw, and shortness of breath, sweating, and discomfort that mimics heartburn. \n\n>It is quite exciting to know, which parameters affect our heart and how! We know some of these like, cholestrol, age, blood pressure, etc. but I wanted to use data science skills to know more about this, and so I decided make this notebook!\n\n\ud83c\udfaf Goal\u00b6\n>The goal is to identify the parameters that influences the heart attack and build a ML model for the prediction of heart attack.\n","e6031c5d":"> Except `age` every features have data points beyond the IQR (Interquartile Range), but whether to call them a outlier or not requires some expertise of the field\n\n> So with some research I found that, a person may have very high cholesterol, like >500 mg\/dl but can't have 0 cholestrol, a rest blood pressure of >180 mm of Hg but not 0 and a ST depression >5\n\n> But such extreme values of samples can cause problem for training ML model so I will remove such values.","f8b5ee2e":"<h1><center> <div style=\"background-color:skyblue;border-radius:10px; padding: 10px;\">Exploratory Data Analysis \ud83d\udcca<\/div><\/center><\/h1>\n\n> I have done data analysis using tableau and PowerBI as it is easy to use, provides interactive plots in very less time and free\n\n> For tableau chart please click [here](https:\/\/public.tableau.com\/app\/profile\/prathamesh6376\/viz\/HeartDiseaseAnalysis_16396340982050\/NumericFeatures)","1b800ea6":"<h4><center>  <div style=\"background-color:lightgreen;border-radius:10px; padding: 10px;\">Handling missing values 3\ufe0f\u20e33\ufe0f\u20e33\ufe0f\u20e3...3\ufe0f\u20e3<\/div><\/center><\/h4>","026d6590":"> AUC ROC score for Logistric Regression, Decision Tree, Random Forest and SVM are in the same range though we got little improvement in case of Random Forest. So, I will be using Random Forest for further analysis, but before that lets see performace of all the models at a glance.","eafe2656":"#### Scaling Data\n\n> Feature scaling is essential for machine learning algorithms that calculate distances between data. Since the range of values of raw data varies widely, in some machine learning algorithms, objective functions do not work correctly without normalization.\n\n> Though scaling is not required for tree based algorithms like Decision trees and Random Forest.","7db73de9":"<h3><center>  <div style=\"background-color:lightpink;border-radius:10px; padding: 10px;\">Support Vector Machine (SVM) <\/div><\/center><\/h3>","eafe8695":"<h3><center>  <div style=\"background-color:lightpink;border-radius:10px; padding: 10px;\">\ud83c\udf33\ud83c\udf32\ud83c\udf84\ud83c\udf33\ud83c\udf34\ud83c\udf33 Random Forest \ud83c\udf34\ud83c\udf33\ud83c\udf32\ud83c\udf84\ud83c\udf33<\/div><\/center><\/h3>","2cb01bec":"<h4><center>  <div style=\"background-color:lightgreen;border-radius:10px; padding: 10px;\">Removing outliers \ud83c\udf88\ud83c\udf88\ud83c\udf88\ud83c\udf88\ud83c\udf90\ud83c\udf88<\/div><\/center><\/h4>","73c40976":"<h4><center>  <div style=\"background-color:lightgreen;border-radius:10px; padding: 10px;\">Removing multicollinearity \ud83e\uddd1\ud83e\uddd1<\/div><\/center><\/h4>\n\n>Multicollinearity is a problem because it undermines the statistical significance of an independent variable. Other things being equal, the larger the standard error of a regression coefficient, the less likely it is that this coefficient will be statistically significant. Also it affects storage and speed.","e4fc63eb":"> The correlation value signifies how much two features are related, for example:\n>- 100% means strongly positively related\n>- -100% means strongly negetively related\n>- 0% means not at all related\n\n> Here, the maximum correlation value is -40% between `max_heart_rate_achieved` and `age`, which is quite less. If the value is very high say, 95% then it is adviced to remove one of the features as it can cause problem to model interpretation","38004c64":"<h2><center> <div style=\"background-color:skyblue;border-radius:10px; padding: 10px;\">Numeric Features \ud83d\udd22<\/div><\/center><\/h2>\n\n<center><img src='https:\/\/i.imgur.com\/zbznpO9.png'><\/center>","9ab0f51b":"> There is no class imbalance as the number of people **without heart disease** and with **heart disease** are comparable (47% - 52%)","28dd8879":"<h2><center> <div style=\"background-color:skyblue;border-radius:10px; padding: 10px;\">Categorical Features \ud83d\udd20<\/div><\/center><\/h2>\n\n<center><img src='https:\/\/i.imgur.com\/1s4cwII.png'><\/center>\n\n### Percentage wise distribution\n<center><img src='https:\/\/i.imgur.com\/u9WhEYZ.png'><\/center>","eae715e9":"This plot is made of all the dots in the train data. It demonstrates the following information:\n> - Feature importance: Variables are ranked in descending order.\n> - Impact: The horizontal location shows whether the effect of that value is associated with a higher or lower prediction.\n> - Original value: Color shows whether that variable is high (in red) or low (in blue) for that observation.\n> - Correlation: A high level of the \u201cst_slope_Up\u201d content has a high and negetive impact on the having Heart Disease. The \u201chigh\u201d comes from the red color, and the \u201cnegetive\u201d impact is shown on the X-axis. Similarly, we will say the \u201cst_slope_Flat\u201d is positive correlated with the target variable.","fe314558":"> The numeric features looks more or less normally distributed","122dbb21":"> `stratify = y` ensures that the proportion of people with and without heart disease is same in train and test data. Let's confirm that","561d5ea6":"<h1><center>  <div style=\"background-color:lightblue;border-radius:10px; padding: 10px;\">Interpreting Machine Learning Model<\/div><\/center><\/h1>`\n\n> The SHAP library in Python has inbuilt functions to use Shapley values for interpreting machine learning models. It has optimized functions for interpreting tree-based models and a model agnostic explainer function for interpreting any black-box model for which the predictions are known.\n\n> - Global Interpretation: Look at a model\u2019s parameters and figure out at a global level how the model works\n> - Local Interpretation: Look at a single prediction and identify features leading to that prediction","d46d71ef":"### Conclusion\n\n> The Random Forest model could predict the heart disease with 89% accuracy and we obtained an AUC ROC score of 0.97 which is quite good\n\n> `st_slope` has the highest influence on predicting a heart disease. Features like `st_slope_Flat`, `excercise_indused_angina_Y` and `st_depression` increases the chances of having a heart disease while `st_slope_Up` does the opposite","9babc4d9":"%%HTML\n<div class='tableauPlaceholder' id='viz1640329320921' style='position: relative'><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' \/> <param name='embed_code_version' value='3' \/> <param name='site_root' value='' \/><param name='name' value='HeartDiseaseAnalysisonNewdata&#47;NumericFeatures' \/><param name='tabs' value='yes' \/><param name='toolbar' value='yes' \/><param name='animate_transition' value='yes' \/><param name='display_static_image' value='yes' \/><param name='display_spinner' value='yes' \/><param name='display_overlay' value='yes' \/><param name='display_count' value='yes' \/><param name='language' value='en-US' \/><param name='filter' value='publish=yes' \/><\/object><\/div>                <script type='text\/javascript'>                    var divElement = document.getElementById('viz1640329320921');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.5)+'px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https:\/\/public.tableau.com\/javascripts\/api\/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                <\/script>","db9153f3":"<h4><center>  <div style=\"background-color:lightgreen;border-radius:10px; padding: 10px;\">Correcting the data types \ud83d\udd22\ud83d\udd20<\/div><\/center><\/h4>\n\n> Before doing this I will rename the columns for easiness","9b1a9b2f":"### Local Interpretation\n\n> Local interpretation focuses on specifics of each individual and provides explanations that can lead to a better understanding of the feature contribution in smaller groups of individuals that are often overlooked by the global interpretation techniques.\n\n> If we want to know why for a perticular sample the model has predicted a class, then local interpretation is useful","25353f2c":"<h4><center>  <div style=\"background-color:skyblue;border-radius:10px; padding: 10px;\">Data Preprocessing \u2699<\/div><\/center><\/h4>\n\n> The ML models performs complex arithmatic operations and it only understands numeric data so we have to convert any catergorical features to numeric before feeding it to ML model. One of the ways and mostly used is One hot encoding.","8e1841ca":"### Observations \ud83d\udd0e\n\n> **True Positives** - There are 64 TP, it means the model has correctly identified 64 people having heart disease\n\n> **True Negetives** - There are 67 TN, it means the model has correctly identified 67 people who were not having heart disease\n\n> **False Positives** - There are 11 FP, it means the model has incorrectly identified 11 as heart disease who were actually not having it\n\n> **False Negetives** - There are 5 FN, it means the model has incorrectly identified 5 people as no heart disease who were actually having it\n\n#### The TP and TN should be as high as possible and FP and FN should be as low as possible","2003af85":"<h3><center>  <div style=\"background-color:lightpink;border-radius:10px; padding: 10px;\">  Logistic Regression<\/div><\/center><\/h3>"}}