{"cell_type":{"8170dec4":"code","ed726956":"code","95542299":"code","abaf1565":"code","7fe2d9e5":"code","03478c83":"code","909706df":"code","5ffd4812":"code","3fbc7cff":"code","a77b20a2":"code","f0e40f5d":"code","4b6745f3":"code","1f862bc8":"code","5bc3d5a0":"code","c9aa8f59":"code","46d75e8b":"code","ed2fb5de":"code","959fee88":"code","3a0090ec":"code","b1a0f8b7":"code","f76c50b6":"code","19415ba9":"code","44e0aff2":"code","cace21c5":"code","5f17a784":"code","37a2a9fc":"code","e5526898":"code","f4771a73":"code","68193798":"code","9e0edaa1":"code","3933168e":"code","2c40f6e3":"code","79a10e69":"code","3bec1ccd":"code","3a0aa822":"code","28198f39":"code","a3920e03":"code","a7498434":"code","3110568a":"code","87a4a8a0":"code","e8c71c76":"code","36bb157e":"code","2d930034":"markdown","e3a4d889":"markdown","d73a322b":"markdown","fe78476a":"markdown","5f379c88":"markdown","e99c2e65":"markdown","bb099ca3":"markdown","1cc32064":"markdown","1ce03cf7":"markdown","4468e1d4":"markdown","88b4d2f4":"markdown","17df1fa5":"markdown","c27b2de0":"markdown","bf177d7f":"markdown","b913b9a6":"markdown","626867c9":"markdown","d22102fe":"markdown","30ab3e77":"markdown","aca730d7":"markdown","24ac98ed":"markdown","95326e84":"markdown"},"source":{"8170dec4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ed726956":"training = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\ntraining['train_test'] = 1\ntest['train_test'] = 0\n\ntest['Survived'] = np.NaN\n\nall_data = pd.concat([training,test])\n","95542299":"training.info()","abaf1565":"# Numerical data\ntraining.describe()","7fe2d9e5":"# All columns of numerical data\ntraining.describe().columns","03478c83":"# numerical data\ndf_num = training[['Age','SibSp', 'Parch', 'Fare']]\n\n# categorical data\ndf_cat = training[['Survived', 'Pclass','Sex','Ticket', 'Cabin', 'Embarked']]","909706df":"for i in df_num.columns:\n    plt.hist(df_num[i], bins=50)\n    plt.title(i)\n    plt.show()","5ffd4812":"print(df_num.corr())\nsns.heatmap(df_num.corr())\n# High correlation: \n# Positive: Parch and SibSp, Parents\/Children and Siblings\/Spouses\n# Negative: Age and SibSp, Age and Siblings\/Spouses","3fbc7cff":"pd.pivot_table(training, index = 'Survived', values = ['Age','Fare','SibSp','Parch'])","a77b20a2":"for i in df_cat.columns:\n    sns.barplot(df_cat[i].value_counts().index, df_cat[i].value_counts()).set_title(i)\n    print(df_cat[i].value_counts())\n    plt.show()","f0e40f5d":"for i in ['Pclass', 'Sex', 'Embarked']:\n    print(pd.pivot_table(training, index = 'Survived', columns = i, values = 'Ticket', aggfunc = 'count'), '\\n\\n')","4b6745f3":"df_cat.Cabin\n\ntraining['cabin_multiple'] = training.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\n\ntraining['cabin_multiple'].value_counts()\n","1f862bc8":"pd.pivot_table(training, index = 'Survived', columns = 'cabin_multiple', values = 'Ticket', aggfunc = 'count')","5bc3d5a0":"# Notamos a exist\u00eancia de cabines com duas letras: exemplo f e65\ntraining['cabin_adv'] = training.Cabin.apply(lambda x : str(x)[0])","c9aa8f59":"print(training.cabin_adv.value_counts())\npd.pivot_table(training, index='Survived', columns='cabin_adv', values='Name', aggfunc='count')","46d75e8b":"training['numeric_tickets'] = training.Ticket.apply(lambda x : 1 if x.isnumeric() else 0)\ntraining['ticket_letters'] = training.Ticket.apply(lambda x : ''.join(x.split(' ')[:-1]).replace('.','').replace('\/','').lower() \n                                                  if len(x.split(' ')[:-1]) > 0 else 0)\nprint(training['numeric_tickets'].value_counts())","ed2fb5de":"# Testando se passageiros com tickets apenas num\u00e9ricos sobrevivem mais ou n\u00e3o dos que tem letras\npd.pivot_table(training, index='Survived', columns='numeric_tickets', values='Name', aggfunc='count')","959fee88":"training['ticket_letters'].value_counts()","3a0090ec":"training.Name.head(50)\ntraining['name_title'] = training.Name.apply(lambda x : x.split(',')[1].split('.')[0].strip()) # .strip() remove espa\u00e7os\n\ntraining['name_title'].value_counts()","b1a0f8b7":"#man_titles = ['Capt', 'Col', 'Don', 'Jonkheer', 'Major', 'Master', 'Mr', 'Rev', 'Sir', 'Dr']\n#training['man_titles'] = training['name_title'].apply(lambda x : x if man_titles.count(x)  else 0)\n\npd.pivot_table(training, index='Survived', columns='name_title', values='Name', aggfunc='count')","f76c50b6":"# Usar todos os feature engineerings\nall_data['cabin_multiple'] = all_data.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\nall_data['cabin_adv'] = all_data.Cabin.apply(lambda x : str(x)[0])\nall_data['numeric_tickets'] = all_data.Ticket.apply(lambda x : 1 if x.isnumeric() else 0)\nall_data['ticket_letters'] = all_data.Ticket.apply(lambda x : ''.join(x.split(' ')[:-1]).replace('.','')\n                                                   .replace('\/','').lower() if len(x.split(' ')[:-1]) > 0 else 0)\nall_data['name_title'] = all_data.Name.apply(lambda x : x.split(',')[1].split('.')[0].strip()) # .strip() remove espa\u00e7os\n\n# Preencher dados vazios com a media\n# Testar depois com a m\u00e9dia do dataset all_data em vez de training\nall_data.Age = all_data.Age.fillna(training.Age.mean())\nall_data.Fare = all_data.Fare.fillna(training.Fare.mean())\n\n# Deixar de usar as linhas em que n\u00e3o temos o embarked\nall_data.dropna(subset=['Embarked'], inplace=True)\n\n# Normalizando o fare (log neperiano (Fare + 1))\nall_data['norm_fare'] = np.log(all_data.Fare+1)\n\n# Visualiza\u00e7\u00e3o da normaliza\u00e7\u00e3o\nplt.hist(all_data.Fare, bins=50)\nplt.title('Fare - Antes da normaliza\u00e7\u00e3o')\nplt.show()\n\nplt.hist(all_data.norm_fare, bins=50)\nplt.title('Fare - Depois da normaliza\u00e7\u00e3o')\nplt.show()\n\n# TypeCast de data frame object para string\nall_data.Pclass = all_data.Pclass.astype(str)\n\n# Transforma variaveis categoricas em v\u00e1rias colunas de car\u00e1ter booleano\nall_dummies = pd.get_dummies(all_data[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'norm_fare', 'Embarked',\n                                             'cabin_adv', 'cabin_multiple','numeric_tickets','name_title', 'train_test']])\n    \nx_train = all_dummies[all_dummies.train_test == 1].drop(['train_test'], axis = 1) \nx_test = all_dummies[all_dummies.train_test == 1].drop(['train_test'], axis = 1)\n\ny_train = all_data[all_data.train_test == 1].Survived\ny_train.shape","19415ba9":"#Scale data\nfrom sklearn.preprocessing import StandardScaler\nScale = StandardScaler()\nall_dummies_scaled = all_dummies.copy()\nall_dummies_scaled[['Age', 'SibSp', 'Parch', 'norm_fare']] = Scale.fit_transform(all_dummies_scaled[['Age', 'SibSp', 'Parch', 'norm_fare']])\n\nx_train_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 1].drop(['train_test'], axis = 1) \nx_test_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 0].drop(['train_test'], axis = 1)\n\n#y_train_scaled = all_data[all_data.train_test == 1].Survived","44e0aff2":"from sklearn.model_selection import cross_validate\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC","cace21c5":"# Machine learning algorithms \nclassifiers = {}","5f17a784":"# GaussianNBNotScaled\ngnb = GaussianNB()\ncv = cross_validate(gnb, x_train, y_train, cv = 3, return_train_score = True, return_estimator = True)\nclassifiers['GaussianNBNotScaled'] = cv\n# training performance\nprint('Average performance on training set:\\n')\nprint(cv['train_score'].mean())\n \n# test performance\nprint('\\nAverage performance on test set:\\n')\nprint(cv['test_score'].mean())","37a2a9fc":"# GaussianNBScaled\ngnb = GaussianNB()\ncv = cross_validate(gnb, x_train_scaled, y_train, cv = 3, return_train_score = True, return_estimator = True)\nclassifiers['GaussianNBScaled'] = cv\n# training performance\nprint('Average performance on training set:\\n')\nprint(cv['train_score'].mean())\n\n# test performance\nprint('\\nAverage performance on test set:\\n')\nprint(cv['test_score'].mean())","e5526898":"# Logistic Regression Not Scaled\nlr = LogisticRegression(max_iter = 2000)\ncv = cross_validate(lr, x_train, y_train, cv = 3, return_train_score = True, return_estimator = True)\nclassifiers['LogisticRegressionNotScaled'] = cv\n\n# training performance\nprint('Average performance on training set:\\n')\nprint(cv['train_score'].mean())\n\n# test performance\nprint('\\nAverage performance on test set:\\n')\nprint(cv['test_score'].mean())","f4771a73":"# Logistic Regression Scaled \nlr = LogisticRegression(max_iter = 2000)\ncv = cross_validate(lr, x_train_scaled, y_train, cv = 3, return_train_score = True, return_estimator = True)\nclassifiers['LogisticRegressionScaled'] = cv\n\n# training performance\nprint('Average performance on training set:\\n')\nprint(cv['train_score'].mean())\n\n# test performance\nprint('\\nAverage performance on test set:\\n')\nprint(cv['test_score'].mean())","68193798":"# Decision Tree - not scaled\ndt = tree.DecisionTreeClassifier(random_state = 10)\ncv = cross_validate(dt,x_train,y_train,cv=5, return_train_score = True, return_estimator = True)\nclassifiers['DecisionTreeNotScaled'] = cv\n\n# training performance\nprint('Average performance on training set:\\n')\nprint(cv['train_score'].mean())\n\n# test performance\nprint('\\nAverage performance on test set:\\n')\nprint(cv['test_score'].mean())","9e0edaa1":"# Decision Tree - scaled\ndt = tree.DecisionTreeClassifier(random_state = 10)\ncv = cross_validate(dt,x_train_scaled,y_train,cv=5, return_train_score = True, return_estimator = True)\nclassifiers['DecisionTreeScaled'] = cv\n\n# training performance\nprint('Average performance on training set:\\n')\nprint(cv['train_score'].mean())\n\n# test performance\nprint('\\nAverage performance on test set:\\n')\nprint(cv['test_score'].mean())","3933168e":"# KNeighbors - not scaled\nknn = KNeighborsClassifier()\ncv = cross_validate(knn,x_train,y_train,cv=5, return_train_score = True, return_estimator = True)\nclassifiers['KNeighborsNotScaled'] = cv\n\n# training performance\nprint('Average performance on training set:\\n')\nprint(cv['train_score'].mean())\n\n# test performance\nprint('\\nAverage performance on test set:\\n')\nprint(cv['test_score'].mean())","2c40f6e3":"# KNeighbors - scaled\nknn = KNeighborsClassifier()\ncv = cross_validate(knn,x_train_scaled,y_train,cv=5, return_train_score = True, return_estimator = True)\nclassifiers['KNeighborsScaled'] = cv\n\n# training performance\nprint('Average performance on training set:\\n')\nprint(cv['train_score'].mean())\n\n# test performance\nprint('\\nAverage performance on test set:\\n')\nprint(cv['test_score'].mean())","79a10e69":"# RandomForest - not scaled\nrf = RandomForestClassifier(random_state = 10)\ncv = cross_validate(rf,x_train,y_train,cv=5, return_train_score = True, return_estimator = True)\nclassifiers['RandomForestNotScaled'] = cv\n\n# training performance\nprint('Average performance on training set:\\n')\nprint(cv['train_score'].mean())\n\n# test performance\nprint('\\nAverage performance on test set:\\n')\nprint(cv['test_score'].mean())","3bec1ccd":"# RandomForest - scaled\nrf = RandomForestClassifier(random_state = 10)\ncv = cross_validate(rf,x_train_scaled,y_train,cv=5, return_train_score = True, return_estimator = True)\nclassifiers['RandomForestScaled'] = cv\n\n# training performance\nprint('Average performance on training set:\\n')\nprint(cv['train_score'].mean())\n\n# test performance\nprint('\\nAverage performance on test set:\\n')\nprint(cv['test_score'].mean())","3a0aa822":"# SVC - not scaled\nsvc = SVC(probability = True)\ncv = cross_validate(svc, x_train, y_train,cv=5, return_train_score = True, return_estimator = True)\nclassifiers['SVCNotScaled'] = cv\n\n# training performance\nprint('Average performance on training set:\\n')\nprint(cv['train_score'].mean())\n\n# test performance\nprint('\\nAverage performance on test set:\\n')\nprint(cv['test_score'].mean())","28198f39":"# SVC - scaled\nsvc = SVC(probability = True)\ncv = cross_validate(svc,x_train_scaled,y_train,cv=5, return_train_score = True, return_estimator = True)\nclassifiers['SVCScaled'] = cv\n\n# training performance\nprint('Average performance on training set:\\n')\nprint(cv['train_score'].mean())\n\n# test performance\nprint('\\nAverage performance on test set:\\n')\nprint(cv['test_score'].mean())","a3920e03":"for i in classifiers:\n    print(i + \": \"+\"train: \"+str(classifiers.get(i)['train_score'].mean())+\" - test: \" + str(classifiers.get(i)['test_score'].mean()), end='\\n\\n')","a7498434":"from sklearn.model_selection import GridSearchCV \nfrom sklearn.model_selection import RandomizedSearchCV ","3110568a":"#simple performance reporting function\ndef clf_performance(classifier, model_name):\n    print(model_name)\n    print('Best Score: ' + str(classifier.best_score_))\n    print('Best Parameters: ' + str(classifier.best_params_))","87a4a8a0":"'''\nrf = RandomForestClassifier(n_estimators=500, criterion=\"gini\", max_depth=20, max_features=10, \n                            min_samples_leaf=2, min_samples_split=2)\n\ncv = cross_validate(rf,x_train_scaled,y_train,cv=5, return_train_score = True, return_estimator = True)\n\nrf.fit(x_train_scaled, y_train)\n\n# training performance\nprint('Average performance on training set:\\n')\nprint(cv['train_score'].mean())\n\n# test performance\nprint('\\nAverage performance on test set:\\n')\nprint(cv['test_score'].mean())\n'''\n\n'''\nrf = RandomForestClassifier(random_state = 1)\n\nparam_grid =  {'n_estimators': [450,499,500,501],\n               'criterion':['gini'],\n                                  'bootstrap': [True],\n                                  'max_depth': [19,20,21],\n                                  'max_features': [5, 10, 15],\n                                  'min_samples_leaf': [1,2],\n                                  'min_samples_split': [1,2]}\n                                  \nclf_rf = GridSearchCV(rf, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_rf = clf_rf.fit(x_train_scaled,y_train)\nclf_performance(best_clf_rf,'Random Forest')\nbest_rf = best_clf_rf.best_estimator_.fit(x_train_scaled, y_train)\n#Best parameters\n#{'bootstrap': True, 'criterion': 'gini', 'max_depth': 20, \n#'max_features': 10, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 500}\n\n#Best Parameters: \n#{'bootstrap': True, 'criterion': 'gini', 'max_depth': 20, \n#'max_features': 10, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 500}\n'''","e8c71c76":"svc = SVC(probability = True)\nparam_grid = tuned_parameters = [{'kernel': ['rbf'], 'gamma': [.1,.5,1,2,5,10],\n                                  'C': [.1, 1, 10, 100, 1000]},\n                                 {'kernel': ['linear'], 'C': [.1, 1, 10, 100, 1000]},\n                                 {'kernel': ['poly'], 'degree' : [2,3,4,5], 'C': [.1, 1, 10, 100, 1000]}]\nclf_svc = GridSearchCV(svc, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_svc = clf_svc.fit(x_train_scaled,y_train)\nbest_svc = best_clf_svc.best_estimator_.fit(x_train_scaled, y_train)\n\nclf_performance(best_clf_svc,'SVC')","36bb157e":"y_hat = best_svc.predict(x_test_scaled).astype(int)\n\nbasic_submission = {'PassengerId': test.PassengerId, 'Survived': y_hat}\nbase_submission = pd.DataFrame(data=basic_submission)\nbase_submission.to_csv('base_submission.csv', index=False)","2d930034":"### An\u00e1lise de sobreviv\u00eancia pelas letras da cabine:\n#### B,D,E - approx: 75% de sobreviv\u00eancia.\n#### C     - approx: 66% de sobreviv\u00eancia.\n#### A     - approx: 50% de sobreviv\u00eancia - poucos dados\n#### F     - approx: 62% de sobreviv\u00eancia - poucos dados\n#### Sem cabines   - 30% de sobreviv\u00eancia","e3a4d889":"# 1)Understand the shape of the data (Histogram, box plots)\n","d73a322b":"## 3) Data exploration","fe78476a":"## Information about the data[](http:\/\/)","5f379c88":"## Feature engineering","e99c2e65":"### Preprocessando os dados para o modelo\n\n1) Tirar os valores nulos do embarked\n\n2) Incluir apenas vari\u00e1veis relevantes \n('Pclass', 'Age', 'Sex', 'SibSp', 'Parch', 'Fare', 'Embarked', 'cabin_adv', 'cabin_multiple','numeric_ticket','name_title')\n\n3) Transformar dados categ\u00f3ricos em v\u00e1rias colunas com valores booleanos para o modelo\n\n4) Substituir valores nulos do Fare e Age pela m\u00e9dia (talvez usar a mediana seja bom)\n\n5) Normalizar o Fare usando logaritmos (log_10)\n\n6) Usar uma escala padr\u00e3o para os dados de 0-1  ","bb099ca3":"## 4) Feature Engineering","1cc32064":"### T\u00edtulos das pessoas\n","1ce03cf7":"Quem paga zero s\u00e3o da tripula\u00e7\u00e3o e s\u00e3o os \u00faltimos a desembarcarem.","4468e1d4":"## 5) Data preprocessing for model","88b4d2f4":"## 7) Model Tuning","17df1fa5":"## 2) Data cleaning","c27b2de0":"Fazer observa\u00e7\u00f5es dentro de cada tipo de embarque. Fazer a an\u00e1lise de cada um deles.","bf177d7f":"### Poderia ser feita a aglutina\u00e7\u00e3o de algumas classes: \n### exemplos: \n### 1-) Sir, Mr\n### 2-) Lady,Miss, Mrs(casada)","b913b9a6":"## Constru\u00e7\u00e3o do modelo:\n### Antes de ir em frente, \u00e9 bom que vejamos a performance de diferentes modelos para os par\u00e2metros padr\u00e3o. Tentamos os seguintes modelos usando o resultado de 5 valida\u00e7\u00f5es cruzadas para termos uma no\u00e7\u00e3o. Com essa primeira impress\u00e3o, podemos ver como a afina\u00e7\u00e3o dos par\u00e2metros melhora cada um dos modelos. Por\u00e9m,se um modelo tiver um resultado base melhor do que outros, n\u00e3o significa necessariamente que ele ter\u00e1 o melhor resultado final.\n\n* Naive Bayes\n* Logistic Regression\n* Decision Tree\n* K Nearest Neighbors\n* Random Forest\n* Support Vector Classifier\n* Xtreme Gradient Boosting\n* Soft Voting Classifier - All Models","626867c9":"## 9) Results","d22102fe":"## An\u00e1lise da sobreviv\u00eancia baseado se o ticket tem s\u00f3 n\u00fameros ou letras\n#### 38% de sobreviv\u00eancia se o ticket \u00e9 s\u00f3 num\u00e9rico\n#### 38% de sobreviv\u00eancia se o ticket \u00e9 num\u00e9rico com letras\n### Conclus\u00e3o: Aparentemente o fato de ter letras ou n\u00e3o, n\u00e3o influencia na sobreviv\u00eancia.","30ab3e77":"|Model|Training set|Test set|\n|-----|--------|-----------------|\n|Naive Bayes (NS)| 75.03%| 72.55%|\n|Naive Bayes (S)| 74.35%| 72.22%|\n|Logistic Regression (NS)| 84.05%| 82.12%|\n|Logistic Regression (S)| 84.17%| 82.12%|\n|Decision Tree (NS)| 99.21%| 78.74%|\n|Decision Tree (S)| 99.21%| 78.63%|\n|K Nearest Neighbors (NS)| 86.05%| 80.54%|\n|K Nearest Neighbors (S)| 86.08%| 81.56%|\n|Random Forest (NS)| 99.21%| 80.32%|\n|Random Forest (S)| 99.21%| 80.32%|\n|Support Vector Classifier (NS)| 73.03%| 72.55%|\n|Support Vector Classifier (S)| 84.50%| 83.24%|","aca730d7":"## 8) Ensemble model building","24ac98ed":"## 6) Basic model building","95326e84":"### Trajeto: S (Southampton) -> C (Cherbourg) -> Q (Queenstown)\n### As pessoas que embarcaram em S e que n\u00e3o pagaram podem ser membros da tripula\u00e7\u00e3o.\n"}}