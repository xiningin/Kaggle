{"cell_type":{"93622c26":"code","bb933635":"code","6868af3f":"code","d7c5799c":"code","70aba170":"code","92620a15":"code","6f7f903e":"code","b08369c7":"code","01c104e6":"code","6bc35a08":"code","138f8a9e":"code","00cc78cf":"code","b3425faa":"code","3efc88c7":"code","e58b2a44":"code","151ec52f":"code","28e97716":"code","2c782337":"code","51c115f6":"code","74548273":"code","55f49072":"code","9e215a26":"code","924cce86":"code","740a2f27":"code","fa7ca810":"code","7e76d0ee":"code","1636392e":"code","0e052d27":"code","dc5c8ec5":"code","65b95468":"code","baf31d2a":"code","d294daa6":"code","342fadf7":"code","b87598ab":"code","266e9ae9":"code","c8bd1949":"code","a3d0432a":"code","41870ec1":"code","4685d363":"code","6abb5872":"code","d1936d12":"code","189a0f4d":"code","e0f1447c":"code","c28d9239":"code","0ca318d0":"code","dbeb8ba4":"code","de53a50e":"code","b9167f08":"code","470e9a7a":"code","8d0356bd":"code","d4f956b6":"code","22517a20":"code","62f62aed":"code","657cc003":"code","d06f6bbd":"code","0ae49cd8":"code","1a5997be":"code","b6aed9c5":"code","f28e0f7e":"code","2df5b0d7":"code","c8b46d24":"code","67d1d3fb":"code","c5bbbed8":"code","6180b760":"code","c54a5f06":"code","b9f04252":"code","5f849de3":"code","6346d330":"code","dd41e5d7":"code","56c680ea":"code","0840e6c9":"code","d4d739ff":"code","af1d1356":"code","12331544":"code","9a46f339":"code","98a7988a":"code","b4ae8ef6":"code","93c6ab90":"code","7bdc5223":"code","f215e219":"code","899026dc":"code","bfecc619":"code","a34d41a9":"code","a83255d3":"code","80bbc4a0":"code","95c936da":"code","ab441c72":"code","f3d846ec":"code","3242941a":"code","eab4db77":"code","2e8bc4ed":"code","7087cbc1":"code","20eefd67":"code","7fdc5b70":"code","21b3f213":"code","d8b4becc":"code","9d799ef2":"code","63d7564b":"code","2988ca77":"code","e9e96505":"code","7ff8bb0d":"code","3d530df0":"code","5c104c04":"code","cee294ff":"code","36b5bbb8":"code","4e9fa6e2":"markdown","afa70805":"markdown","2c6376d7":"markdown","c87830ce":"markdown","e8b98dbc":"markdown","a9f5117e":"markdown","676b9e99":"markdown","73eadefc":"markdown","ac41d548":"markdown","fa29cb59":"markdown","f7a6c96a":"markdown","3ddb0f25":"markdown","cd121d0e":"markdown","29bcbf1c":"markdown","a8a60a32":"markdown","1b3985d1":"markdown","46e2c525":"markdown","8a028479":"markdown","30d3e5b5":"markdown","7e6117a4":"markdown","d7bbb011":"markdown","0a451b74":"markdown","0c13056f":"markdown","4c7e4594":"markdown","c066566b":"markdown","e7b539bc":"markdown","43762315":"markdown","011e6a19":"markdown","93f2b3e0":"markdown","2c097f5b":"markdown","c6372125":"markdown","de346aa0":"markdown","319ac31d":"markdown","a5202a1e":"markdown","23b9952d":"markdown","267c3995":"markdown","0797f5be":"markdown","6c0ad0ce":"markdown","a936cb4f":"markdown"},"source":{"93622c26":"# !pip list --format=freeze > requirements.txt","bb933635":"%matplotlib inline\n# generic libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport pandas as pd\nfrom joblib import Parallel, delayed\nimport pickle\nimport time\nimport plotly.graph_objects as go\n\n# machine learning\nfrom lightgbm import LGBMRegressor\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.metrics import r2_score, make_scorer\nfrom sklearn.ensemble import StackingRegressor, RandomForestRegressor\nfrom sklearn.pipeline import make_pipeline\nimport optuna\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\n\n# path and files treatment\nimport glob\nimport os","6868af3f":"# env could be 'local' or 'kaggle'\nenv = 'kaggle'\n\nif env == 'local':\n    data_folder = '.\/data'\n    output = '.\/output\/'\n    save_path = '.\/img\/'\n    if not(os.path.exists(output)):\n        os.makedirs(output)\n    if not(os.path.exists(save_path)):\n        os.makedirs(save_path)    \n\nelif env == 'kaggle':\n    data_folder = '..\/input\/optiver-realized-volatility-prediction'\n    output = '.\/output\/'\n    save_path = '.\/img\/'\n    os.makedirs(save_path)\n    os.makedirs(output)\nelse:\n    print('env variable must be defined')\n\nbk_train_fol = '\/book_train.parquet\/'\ntd_train_fol = '\/trade_train.parquet\/'\nbk_test_fol = '\/book_test.parquet\/'\ntd_test_fol = '\/trade_test.parquet\/'\n\nmodel_final = 'finalized_model.sav'\n\nRANDOM_SEED = 42\n\n# Remove non efficient (and slow) cells to be faster\nfast = False\n\n# if None take all the dataset\nnumber_of_stocks = 5\n\npd.set_option('max_rows', 300)\npd.set_option('max_columns', 300)","d7c5799c":"def load_df(df_folder, nb_stock_to_load=0, data_folder=data_folder):\n    '''load a parquet \n    \n    arguments\n    ---------------\n    data_folder (str)\n    df_folder (str)\n    nb_stock_to_load (int)\n        number of subfolders to load\n    '''\n    stock_list = os.listdir(data_folder + df_folder)\n\n    if nb_stock_to_load == 0:\n        nb_stock_to_load = len(stock_list)\n    nb_stock_to_load = min(nb_stock_to_load, len(stock_list))\n    \n    if nb_stock_to_load == 1:\n        df = pd.read_parquet(data_folder + df_folder + '\/stock_id=0')\n        df['stock_id'] = 0\n    else:\n        ## depreciated\n        # subset_paths = []\n        # for stock in stock_list[:nb_stock_to_load]:\n        #     subset_path = glob.glob(data_folder + df_folder + stock + '\/*')\n        #     subset_paths.append(subset_path[0])\n\n        subset_paths = [glob.glob(data_folder + df_folder + stock + '\/*')[0] for stock in stock_list[:nb_stock_to_load]]\n        ## doesn't work\n        # subset_paths = glob.glob(data_folder + df_folder + '\/*')[:nb_stock_to_load]\n        \n        df = pd.read_parquet(subset_paths)\n        df['stock_id'] = df['stock_id'].astype(int)\n    return df","70aba170":"###############################\n# Functions to add features\n###############################\n\ndef add_wap(df, number=1, column_prefix='wap', standard=True):\n    '''adding one wap\n\n    number (int): the position of the price to take it could be 1 or 2\n    standard (bool): use standard method to calculate wap or use a custom method\n    '''\n    if standard:\n        df[column_prefix + str(number)] = (\n            df['bid_price'+ str(number)] * df['ask_size'+ str(number)] + df['ask_price'+ str(number)] * df['bid_size'+ str(number)]) \/ (\n                df['ask_size'+ str(number)]+ df['bid_size'+ str(number)])\n    else:\n        df[column_prefix + str(number) + '_ns'] = (\n            df['bid_price'+ str(number)] * df['bid_size'+ str(number)] + df['ask_price'+ str(number)] * df['ask_size'+ str(number)]) \/ (\n                df['ask_size'+ str(number)]+ df['bid_size'+ str(number)])\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff()\n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\n\ndef add_waps(df):\n        '''add many waps'''\n        add_wap(df, 1, column_prefix='wap')\n        add_wap(df, 2, column_prefix='wap')\n        add_wap(df, 1, column_prefix='wap', standard=False)\n        add_wap(df, 2, column_prefix='wap', standard=False)\n        df['wap_p'] = ((\n                df['wap1'] * (df['ask_size1'] + df['bid_size1']) +\n                df['wap2'] * (df['ask_size2'] + df['bid_size2'])) \/\n                (df['ask_size1'] + df['bid_size1'] + df['ask_size2'] + df['bid_size2']))\n        df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n\ndef add_log_return(df, price_col, log_col_name, group='time_id'):\n        df[log_col_name] = df.groupby([group])[price_col].apply(log_return)\n\ndef add_spreads(df):\n        # # tests with ponderates features\n        # df['bid_spread_p'] = (df['bid_price1'] * df['bid_size1'] - df['bid_price2'] * df['bid_size1'])\/(df['bid_size1'] + df['bid_size2'])\n        # df['ask_spread_p'] = (df['ask_price1'] * df['ask_size1'] - df['ask_price2'] * df['ask_size1'])\/(df['ask_size1'] + df['ask_size2'])\n        # df[\"bid_ask_spread_p\"] = abs(df['bid_spread_p'] - df['ask_spread_p'])\n        df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n        df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n        df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1'])\/2)\n        df[\"bid_ask_spread1\"] = (df['ask_price1'] - df['bid_price1'])\/df['bid_price1']\n        df[\"bid_ask_spread2\"] = (df['ask_price2'] - df['bid_price2'])\/df['bid_price2']\n        df[\"bid_ask_spread_p\"] = ((df['ask_price1'] + df['ask_price2']) - (df['bid_price1'] + df['bid_price2']))\/(df['bid_price1'] + df['bid_price2'])\n\ndef add_volumes(df):\n        df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n        df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n\ndef add_EMA(df, wap_col, nb_period):\n        df[wap_col + '_' + str(nb_period) + 'sec_EWM'] = df[wap_col].ewm(span=nb_period, adjust=False).mean()\n\n###############################\n# Evaluation\n###############################\ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))","92620a15":"book_train = load_df(bk_train_fol, nb_stock_to_load=1)\nbook_train.head()","6f7f903e":"book_train.info()","b08369c7":"# Sample\nbook_train_sample = book_train[(book_train['stock_id'] == 0) & (book_train['time_id'] < 35)].copy()\nadd_wap(book_train_sample)\nfig = px.line(book_train_sample, x=\"seconds_in_bucket\", y=\"wap1\", title='WAP of stock_id_0, time_id <35', color='time_id')\nif env == 'local':\n    fig.write_image(save_path + 'wap_sample.png')\nfig.show()","01c104e6":"book_train_sample['log_return'] = book_train_sample.groupby(['time_id'])['wap1'].apply(log_return)\nbook_train_sample = book_train_sample[~book_train_sample['log_return'].isnull()] # removing each Nan of firsts time_id, ~ : invers the mask","6bc35a08":"fig = px.line(book_train_sample, x=\"seconds_in_bucket\", y=\"log_return\", title='Log return of stock_id_0, time_id <35', color='time_id')\nif env == 'local':\n    fig.write_image(save_path + 'logreturn_sample.png')\nfig.show()","138f8a9e":"# Realized volatility on our sample\nrealized_vol = book_train_sample.groupby(['time_id'])['log_return'].agg(realized_volatility)\nprint('Realized volatility for stock_id 0 :')\nfor i in realized_vol.index:\n    print(f'- time_id {i} is {round(realized_vol.loc[i], 7)}')","00cc78cf":"# Test tp know if the book and trade data are on same stocks\nos.listdir(data_folder + td_train_fol) == os.listdir(data_folder + bk_train_fol)","b3425faa":"trade_train = load_df(td_train_fol, nb_stock_to_load=2)","3efc88c7":"trade_train.head()","e58b2a44":"trade_train_sample = trade_train[(trade_train.stock_id == 0) & (trade_train.time_id < 35)]\n\nfig = px.line(trade_train_sample, x=\"seconds_in_bucket\", y=\"price\", title='Price of stock_id_0, time_id <35', color='time_id')\nif env == 'local':\n    fig.write_image(save_path + 'trade_prices_sample.png')\nfig.show()","151ec52f":"plt.figure(figsize=(12, 6))\nsns.scatterplot(data=trade_train_sample, x=\"price\", y=\"size\", hue=\"order_count\")\nif env == 'local' or env == 'kaggle':\n    plt.savefig(save_path + 'trade_sample.png')\nplt.show()","28e97716":"book_test = load_df(bk_test_fol)\nbook_test.head()","2c782337":"trade_test = load_df(td_test_fol)\ntrade_test.head()","51c115f6":"# this dataset is just a sample, it will be replaced by the real one at each submission.\nvol_test = pd.read_csv(data_folder +'\/test.csv')\nvol_test","74548273":"vol_train = pd.read_csv(data_folder +'\/train.csv')\nvol_train.head()","55f49072":"vol_train.shape","9e215a26":"vol_stock0 = vol_train[vol_train['stock_id'] == 0]\n\nsns.set_theme(style=\"ticks\")\nfig = plt.figure(figsize=(16, 6))\n# fig.suptitle('Images after equalization preprocessing', fontsize=16)\n# fig.tight_layout()\n\nplt.subplot(1, 2, 1)\nplt.title(\"Train realized volatility\")\nplt.hist(vol_stock0['target'], bins=50)\n\nplt.subplot(1, 2, 2)\nplt.title(\"Train realized volatility - log\")\nplt.hist(np.log(vol_stock0['target']), bins=50)\n\nif env == 'local' or env == 'kaggle':\n    plt.savefig(save_path + 'realized_volatility.png')\n\nplt.show()","924cce86":"###############################\n# Lists of dataset paths\n###############################\n\n# Create a list of stocks paths books from the dataset\nif number_of_stocks is None:\n    list_order_book_file_train = glob.glob(data_folder + bk_train_fol + '*')\n    list_order_trade_file_train = glob.glob(data_folder + td_train_fol + '*')\n    stock_id_max = max([int(path.split('=')[1]) for path in list_order_trade_file_train]) # files on kaggle are random sorted\nelse:\n    stock_id_max = number_of_stocks-1 # stocks start at 0\n    # take only stocks <= stock_id_max\n    list_order_book_file_train = [path for path in glob.glob(data_folder + bk_train_fol + '*') if int(path.split('=')[1]) <= stock_id_max]\n    list_order_trade_file_train = [path for path in glob.glob(data_folder + td_train_fol + '*') if int(path.split('=')[1]) <= stock_id_max]\n","740a2f27":"# select all stocks books\nlist_order_book_file_train = glob.glob(data_folder + bk_train_fol + '\/*')\nlist_order_book_file_train[:2] # sample","fa7ca810":"# specific for naive model\ndef realized_volatility_per_time_id(file_path, prediction_column_name):\n    '''load datas of one stock_id then calculate WAP, log_return\n    set a new DF and put inside realized_volatility per time_id\n    add a column with competition form : {stock_id}-{time_id} called row_id\n\n    file_path : path of subfolders with stock_id\n        example : .\/data\/book_train\/stock_id=0\n    prediction_column_name : name of the realized_volatility column\n    \n    return row_id, prediction_name columns'''\n    df_book_data = pd.read_parquet(file_path)\n    add_wap(df_book_data)\n\n    df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap1'].apply(log_return)\n    df_book_data = df_book_data[~df_book_data['log_return'].isnull()] # removing each Nan of firsts time_id, ~ : invers the mask\n\n    df_realized_vol_per_stock =  pd.DataFrame(df_book_data.groupby(['time_id'])['log_return'].agg(realized_volatility)).reset_index()\n    df_realized_vol_per_stock = df_realized_vol_per_stock.rename(columns = {'log_return':prediction_column_name})\n    \n    stock_id = file_path.split('=')[1]\n    df_realized_vol_per_stock['row_id'] = df_realized_vol_per_stock['time_id'].apply(lambda x:f'{stock_id}-{x}')\n    \n    return df_realized_vol_per_stock[['row_id',prediction_column_name]]","7e76d0ee":"def past_realized_volatility_per_stock(list_file,prediction_column_name):\n    df_past_realized = pd.DataFrame()\n    for file in list_file:\n        df_past_realized = pd.concat([df_past_realized,\n                                     realized_volatility_per_time_id(file,prediction_column_name)])\n    return df_past_realized\n\n# test on all 126 stocks \n# long ! 230 sec\nif not(fast):\n    df_past_realized_train = past_realized_volatility_per_stock(list_file=list_order_book_file_train,\n                                                            prediction_column_name='pred')\n    df_past_realized_train.head()","1636392e":"if not(fast):\n    df_naive = vol_train.copy()\n    # Let's join the output dataframe with train.csv to see the performance of the naive prediction on training set.\n    # naive prediction = predict same volatility in the next 10min window (auto realisation)\n    df_naive['row_id'] = df_naive['stock_id'].astype(str) + '-' + df_naive['time_id'].astype(str)\n    df_naive = df_naive[['row_id','target']]\n    df_naive = df_naive.merge(df_past_realized_train[['row_id','pred']], on = ['row_id'], how = 'left')\n    df_naive.head()","0e052d27":"if not(fast):\n    R2 = round(r2_score(y_true = df_naive['target'], y_pred = df_naive['pred']),3)\n    RMSPE = round(rmspe(y_true = df_naive['target'], y_pred = df_naive['pred']),3)\n    print(f'Performance of the naive prediction: R2 score: {R2}, RMSPE: {RMSPE}')","dc5c8ec5":"# list of waps for applying log return, EMA and EMA log return\n# this list is also used in the creation of aggregation dic\nwaps = ['wap1', 'wap2', 'wap1_ns', 'wap2_ns', 'wap_p']\n\ndef book_feature_eng_per_stock(file_path, waps=waps):\n    ''' Load datas of one stock_id then adding features.\n    Removing Nan rows of theses features\n    \n    file_path : path of subfolders with stock_id\n        example : .\/data\/book_train\/stock_id=0\n\n    return the df '''\n    book_train = pd.read_parquet(file_path)\n\n    add_waps(book_train)\n    \n    for wap in waps:\n        add_log_return(book_train, price_col=wap, log_col_name=wap + '_log_return')\n        for period in  [20, 100]:\n            add_EMA(book_train, wap, period)\n            EMA_col_name = wap + '_' + str(period) + 'sec_EWM'\n            add_log_return(book_train, price_col=EMA_col_name, log_col_name=EMA_col_name + '_log_return')\n            book_train['diff_' + EMA_col_name] = abs(book_train[wap] - book_train[EMA_col_name])\n    \n    add_spreads(book_train)\n    add_volumes(book_train)\n\n    # book_train = book_train[~(book_train['wap1_log_return'].isnull() | book_train['log_return2'].isnull() | book_train['log_return_p'].isnull())] # at the end ?\n    book_train = book_train.fillna(book_train.median())\n    \n    return book_train","65b95468":"# sample with stock 0\ndf_sample = book_feature_eng_per_stock(list_order_book_file_train[0])\ndf_sample.head()","baf31d2a":"# list of spreads to apply the aggregate functions\nspreads = ['bid_ask_spread1', 'bid_ask_spread2', 'bid_ask_spread_p', 'bid_spread', 'ask_spread', 'price_spread']\n\n#########################################\n# Creation of order book aggregation dic\n#########################################\nbk_feature_dic = {}\nfor wap in waps:\n    bk_feature_dic[wap + '_log_return'] = [realized_volatility]\n    # bk_feature_dic[wap] = [np.std, pd.Series.mad]\n    for period in [20, 100]:\n        EMA_col_name = wap + '_' + str(period) + 'sec_EWM'\n        # bk_feature_dic[EMA_col_name + '_log_return'] = [realized_volatility]\n        bk_feature_dic['diff_' + EMA_col_name] = [np.sum, np.std]\nfor spread in spreads:\n    bk_feature_dic[spread] = [np.sum, np.std]\n\nbk_feature_dic['total_volume'] = [np.sum, np.mean]\nbk_feature_dic['volume_imbalance'] = [np.std]\nbk_feature_dic['wap_balance'] = [np.sum, np.mean]\n\nbk_feature_dic","d294daa6":"def book_agg_form_parallele(file):\n    ''' Create a new df that aggregate data by time_id and apply the feature dic\n    add :\n    - a stock_id columns\n    - a competition form column : {stock_id}-{time_id} called row_id\n    return the new df\n    '''\n    stock_id = file.split('=')[1]\n    df_agg_stock = book_feature_eng_per_stock(file)\n    df_agg_stock = pd.DataFrame(df_agg_stock.groupby(['time_id']).agg(bk_feature_dic).reset_index())\n\n    df_agg_stock.columns = ['_'.join(col).rstrip('_') for col in df_agg_stock.columns.values]\n    df_agg_stock['row_id'] = df_agg_stock['time_id'].apply(lambda x:f'{stock_id}-{x}')\n    # df_agg_stock['stock_id'] = stock_id\n    df_agg_stock.drop('time_id', axis=1, inplace=True)\n\n    return df_agg_stock","342fadf7":"def agg_df_and_concatenate_parallel(paths_list, func):\n    ''' Create an concateneted df of preprocessed stocks df by the func'''\n\n    df_agg = Parallel(n_jobs=-1)(\n        delayed(func)(file) \n        for file in paths_list)\n    \n    df_agg = pd.concat(df_agg, ignore_index = True)\n\n    return df_agg","b87598ab":"%%time\ndf_order_agg = agg_df_and_concatenate_parallel(list_order_book_file_train, book_agg_form_parallele)\ndf_order_agg.head()","266e9ae9":"###############################\n# Functions to add features\n###############################\ndef add_amount(df):\n    df['amount'] = df['price'] * df['size']\ndef add_power(df):\n    df['power'] = (df['price'] - df['price'].shift(1))\/df['price']*df['size']\n\n# Function to count unique elements of a series\ndef count_unique(series):\n    return len(np.unique(series))","c8bd1949":"def trade_feature_eng_per_stock(file_path):\n    ''' Load datas of one stock_id then adding features.\n    Removing Nan rows of theses features\n    \n    file_path : path of subfolders with stock_id\n        example : .\/data\/trade_train\/stock_id=0\n\n    return the df '''\n    df = pd.read_parquet(file_path)\n        \n    add_log_return(df, price_col='price', log_col_name='td_log_return')\n\n    add_amount(df)\n    for period in  [20, 100]:\n        add_EMA(df, 'amount', period)\n        EMA_col_name = 'amount_' + str(period) + 'sec_EWM'\n        add_log_return(df, price_col=EMA_col_name, log_col_name=EMA_col_name + '_log_return')\n        \n    df['diff_td'] = df.seconds_in_bucket.diff() # same Nan as log_return\n    df['amount_p_order'] = df.amount \/ df.order_count\n    add_power(df)\n\n    df = df[~df['td_log_return'].isnull()]\n    return df","a3d0432a":"# sample with stock 0\ndf_sample = trade_feature_eng_per_stock(list_order_trade_file_train[0])\ndf_sample.head()","41870ec1":"#########################################\n# Creation of order book aggregation dic\n#########################################\ntd_feature_dic = {}\nfor period in  [20, 100]:\n    EMA_col_name = 'amount_' + str(period) + 'sec_EWM_log_return'\n    td_feature_dic[EMA_col_name] = [realized_volatility]\ntd_feature_dic['td_log_return'] = [realized_volatility]\n# td_feature_dic['seconds_in_bucket'] = [count_unique] # removed after feature importance analysis\ntd_feature_dic['diff_td'] = [np.std]\ntd_feature_dic['amount_p_order'] = [np.mean]\ntd_feature_dic['price'] = [np.mean]\ntd_feature_dic['amount'] = [np.std, pd.Series.mad]\ntd_feature_dic['amount_p_order'] = [np.mean, np.sum]\n# td_feature_dic['size'] = [np.mean, np.sum] # removed after feature importance analysis\ntd_feature_dic['order_count'] = [np.mean, np.sum]\n\ntd_feature_dic","4685d363":"def trade_agg_form_parallele(file):\n    ''' Create a new df that aggregate data by time_id\n    add :\n    - a stock_id columns\n    - a with competition form column : {stock_id}-{time_id} called row_id\n    return the new df\n    '''\n    stock_id = file.split('=')[1]\n    df_agg_stock = trade_feature_eng_per_stock(file)\n    df_agg_stock = pd.DataFrame(df_agg_stock.groupby(['time_id']).agg(td_feature_dic)).reset_index()\n    \n    df_agg_stock.columns = ['_'.join(col).rstrip('_') for col in df_agg_stock.columns.values]\n    df_agg_stock['row_id'] = df_agg_stock['time_id'].apply(lambda x:f'{stock_id}-{x}')\n    # df_agg_stock['stock_id'] = stock_id\n    df_agg_stock.drop('time_id', axis=1, inplace=True)\n\n    return df_agg_stock\n","6abb5872":"%%time\ndf_trade_agg = agg_df_and_concatenate_parallel(list_order_trade_file_train, trade_agg_form_parallele)\ndf_trade_agg.head()","d1936d12":"def process_final_df(df_order_agg, df_trade_agg, df_target):\n    '''select the targets of the chosen stocks\n    merge target df to order and trades df\n    return new df'''\n\n    df = df_target[df_target.stock_id <= stock_id_max].copy()\n    #  adding the same index in our books df to merge\n    df['row_id'] = df['stock_id'].astype(str) + '-' + df['time_id'].astype(str)\n\n    df = df.merge(df_order_agg, on = ['row_id'], how = 'left')\n    df = df.merge(df_trade_agg, on = ['row_id'], how = 'left')\n\n    return df    ","189a0f4d":"df_train = process_final_df(df_order_agg, df_trade_agg, vol_train)\ndf_train.head()","e0f1447c":"###########################################\n# Ploting realized volatility per stock\n###########################################\n\n# #Cr\u00e9ation d'un sous \u00e9chantillon par modalit\u00e9\ngroupes = []\nfor s in df_train['stock_id'].unique():\n    groupes.append(df_train[df_train['stock_id'] == s]['wap1_log_return_realized_volatility'])\n \n# 'OO' m\u00e9thode pour plot\nfig, ax = plt.subplots(figsize=(30,8))\n\n# Propri\u00e9t\u00e9s graphiques\nmedianprops = {'color':\"black\"}\nmeanprops = {'marker':'o', 'markeredgecolor':'black',\n            'markerfacecolor':'firebrick'}\n\nax.boxplot(groupes,\n           labels=df_train['stock_id'].unique(),\n           showfliers=False,\n           medianprops=medianprops, \n           vert=True,\n           patch_artist=True,\n           showmeans=True,\n           meanprops=meanprops)\n\nax.set(title='Distribution des wap1_log_return_realized_volatility par stock',\n      xlabel=\"Stock Id\",\n      ylabel='wap1_log_return_realized_volatility')\n\nplt.show()","c28d9239":"###############################\n# Saving preprocessed train ds\n###############################\n# df_train.to_pickle(output + 'dataset_train.bz2', compression='bz2')","0ca318d0":"# list of test books paths\nlist_order_book_file_test = glob.glob(data_folder + bk_test_fol + '*')\nlist_order_trade_file_test = glob.glob(data_folder + td_test_fol + '*')\n\n# preprocess test dataset\ndf_order_test_agg = agg_df_and_concatenate_parallel(list_order_book_file_test, book_agg_form_parallele)\ndf_trade_test_agg = agg_df_and_concatenate_parallel(list_order_trade_file_test, trade_agg_form_parallele)\n\n# Merging df\ndf_test = process_final_df(df_order_test_agg, df_trade_test_agg, vol_test)\ndf_test.head()","dbeb8ba4":"# For optuna studies\nn_trials = 10\n\nkfolds = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n\nscorer_rmspe = make_scorer(rmspe,\n    # greater_is_better=False\n    )","de53a50e":"## New idea for the dic structure\n# dic_eval = dict.fromkeys(['names', 'models', 'rmspe_scores', 'r2_scores'])\n# dic_eval","b9167f08":"dic_eval = {}\ndef evaluate(name, model, dic, X_test, y_test):\n    y_pred = model.predict(X_test)\n    R2 = round(r2_score(y_test, y_pred), 6)\n    RMSPE = round(rmspe(y_test, y_pred), 6)\n    dic[name] = [model, RMSPE, R2]\n    print(f'Performance of the {name} prediction: R2 score: {R2}, RMSPE: {RMSPE}')\n\ndef evaluateCV(name, model, dic, X_train, y_train, save=True):\n    start_time = time.time()\n    RMSPE =  round(cross_val_score(\n        model, X_train, y_train, cv=kfolds, scoring=scorer_rmspe\n    ).mean(), 6)\n    # model.fit(X_train, y_train)\n    if save:\n        dic[name] = [model, RMSPE]\n    print(f'RMSPE of the {name} prediction: {RMSPE} in {round(time.time() - start_time, 3)} sec.')\n    if not(save):\n        return RMSPE","470e9a7a":"###############################\n# Loading preprocessed train ds\n###############################\n# df_train = pd.read_pickle(output + 'dataset_train.bz2')","8d0356bd":"df_train.head()","d4f956b6":"df_train.isnull().sum()","22517a20":"df_train.fillna(df_train.median(), inplace=True)\ndf_train.isnull().sum().sum()","62f62aed":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import QuantileTransformer\n\n\nqtn = QuantileTransformer(output_distribution='normal', random_state=42)\nqtu = QuantileTransformer(output_distribution='uniform', random_state=42)\nstd = StandardScaler()\nminmax = MinMaxScaler()\n\nscalers = [qtn, qtu, std, minmax]","657cc003":"def scaler_selection(scalers):\n    rmspe_min = 1\n    for scaler in scalers:\n        X = df_train.drop(['row_id', 'target'], axis = 1)\n        y = df_train['target']\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y,\n        test_size=0.15, random_state=42, shuffle=True)\n\n        # model_xgb = make_pipeline(scaler,\n        #                     XGBRegressor(tree_method='hist', random_state=42, n_jobs= - 1))\n        if env == 'kaggle':\n            model_xgb = make_pipeline(scaler,\n            XGBRegressor(tree_method='gpu_hist', random_state=42, n_jobs= - 1))\n        else:\n            model_xgb = make_pipeline(scaler,\n                XGBRegressor(tree_method='hist', random_state=42, n_jobs= - 1))\n\n        # model_xgb.fit(X_train, y_train)\n        # evaluateCV('XGBOOST_'+ str(scaler), model_xgb, dic_eval, X_test, y_test, save=False)\n        rmspe_model = evaluateCV('XGBOOST_'+ str(scaler), model_xgb, dic_eval, X_train, y_train, save=False)\n\n        # rmspe_model = dic_eval['XGBOOST_'+ str(scaler)][1]\n        if rmspe_model < rmspe_min:\n            rmspe_min = rmspe_model\n            selected_scaler = scaler\n\n    return selected_scaler, rmspe_min\n\nselected_scaler, rmspe_scaler = scaler_selection(scalers)\nprint(f'the selected scaler is {selected_scaler}')","d06f6bbd":"X_train = df_train.drop(['row_id', 'target'], axis = 1)\n# X_val = X.values\ny_train = df_train['target']\n# y_val = y.values\n\nX_train.shape, y_train.shape","0ae49cd8":"X_test = df_test.drop(['row_id'], axis = 1)\ndf_pred = df_test[['row_id']]","1a5997be":"## We don't split anymore as we use CV\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, shuffle=False)\n# X_train.shape, X_test.shape, y_train.shape, y_test.shape","b6aed9c5":"def tune(objective, n_trials=n_trials):\n    start_time = time.time()\n    study = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n    study.optimize(objective, n_trials=n_trials, gc_after_trial=True)\n\n    params = study.best_params\n    print(\"--- %s seconds ---\" % (time.time() - start_time))\n    return params","f28e0f7e":"def ridge_objectiveCV(trial):\n\n    _alpha = trial.suggest_float(\"alpha\", 1e-8, 20, log=True)\n\n    # normalize=True to add ?\n    # model_ridge = make_pipeline(selected_scaler,\n    #     Ridge(alpha=_alpha, random_state=RANDOM_SEED))    \n    model_ridge = Ridge(alpha=_alpha, random_state=RANDOM_SEED, normalize=True)\n\n    score = cross_val_score(\n        model_ridge, X_train, y_train, cv=kfolds, scoring=scorer_rmspe\n    ).mean()\n    return score","2df5b0d7":"if not(fast):\n    ridge_params = tune(ridge_objectiveCV)","c8b46d24":"if not(fast):\n    ridge_opt = Ridge(**ridge_params, random_state=RANDOM_SEED, normalize=True)\n    ## V17\n    # ridge_opt.fit(X, y)\n    # evaluate('Ridge', ridge_opt, dic_eval, X_test, y_test)\n\n    evaluateCV('Ridge', ridge_opt, dic_eval, X_train, y_train)","67d1d3fb":"def lasso_objective(trial):\n    _alpha = trial.suggest_loguniform(\"alpha\", 0.0001, 10)\n    lasso = Lasso(alpha=_alpha, random_state=RANDOM_SEED)\n\n    score = cross_val_score(\n        lasso, X_train, y_train, cv=kfolds, scoring=scorer_rmspe\n    ).mean()\n    return score","c5bbbed8":"if not(fast):\n    lasso_params = tune(lasso_objective, n_trials=5) # long and results are bad\n    lasso_opt = Lasso(**lasso_params, random_state=RANDOM_SEED)","6180b760":"if not(fast):\n    # lasso_opt.fit(X_train, y_train)\n    # evaluate('Lasso', lasso_opt, dic_eval, X_test, y_test)\n\n    evaluateCV('Lasso', lasso_opt, dic_eval, X_train, y_train)","c54a5f06":"# def randomforest_objective(trial):\n#     _n_estimators = trial.suggest_int(\"n_estimators\", 50, 200)\n#     _max_depth = trial.suggest_int(\"max_depth\", 5, 20)\n#     _min_samp_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n#     _min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 2, 10)\n\n#     rf = RandomForestRegressor(\n#         max_depth=_max_depth,\n#         min_samples_split=_min_samp_split,\n#         min_samples_leaf=_min_samples_leaf,\n#         n_estimators=_n_estimators,\n#         n_jobs=-1,\n#         random_state=RANDOM_SEED,\n#     )\n\n#     rf.fit(X_train, y_train)\n\n#     preds = rf.predict(X_test)\n#     return rmspe(y_test, preds)\n\ndef randomforest_objectiveCV(trial):\n    _n_estimators = trial.suggest_int(\"n_estimators\", 50, 200)\n    _max_depth = trial.suggest_int(\"max_depth\", 5, 20)\n    _min_samp_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n    _min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 2, 10)\n\n    rf = RandomForestRegressor(\n        max_depth=_max_depth,\n        min_samples_split=_min_samp_split,\n        min_samples_leaf=_min_samples_leaf,\n        n_estimators=_n_estimators,\n        n_jobs=-1,\n        random_state=RANDOM_SEED,\n    )\n    score = cross_val_score(\n        rf, X_train, y_train, cv=kfolds, scoring=scorer_rmspe\n    ).mean()\n    return score","b9f04252":"if not(fast):\n    randomforest_params = tune(randomforest_objectiveCV, n_trials=5) # long, average results...\n    rf_opt = RandomForestRegressor(n_jobs=-1, random_state=RANDOM_SEED, **randomforest_params)","5f849de3":"if not(fast):\n    ## V17\n    # rf_opt.fit(X_train, y_train)\n    # evaluate('RandomForrest', rf_opt, dic_eval, X_test, y_test)\n\n    evaluateCV('RandomForrest', rf_opt, dic_eval, X_train, y_train)","6346d330":"## Old function\n# def plot_feature_importance(df_train, model):\n#     feature_importances_df = pd.DataFrame({\n#         'feature': df_train.columns,\n#         'importance_score': model.feature_importances_\n#     })\n#     fig = plt.figure(figsize=(20, 5))\n#     ax = sns.barplot(x = \"feature\", y = \"importance_score\", data = feature_importances_df)\n#     ax.set(xlabel=\"Features\", ylabel = \"Importance Score\")\n#     # plt.xticks(ha='left', rotation=45)\n#     fig.autofmt_xdate(bottom=0.2, rotation=30, ha='right')\n#     plt.show()\n#     # return feature_importances_df\n\ndef plot_feature_importance(df_train, model, name=None):\n\n    feature_imp = pd.DataFrame(sorted(zip(model.feature_importances_,df_train.columns)), columns=['Value','Feature'])\n\n    plt.figure(figsize=(20, 10))\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\n    plt.title('Features importance')\n    plt.tight_layout()\n    if (env == 'local' or env == 'kaggle') and name is not None:\n        plt.savefig(save_path + name +'_feat_imp.png')\n    plt.show()","dd41e5d7":"if env == 'kaggle':\n    xgb = XGBRegressor(tree_method='gpu_hist', random_state=42, n_jobs= - 1)\nelse:\n    xgb = XGBRegressor(tree_method='hist', random_state=42, n_jobs= - 1)","56c680ea":"# %%time\n# xgb.fit(X_train, y_train)\n# evaluate('XGBOOST', xgb, dic_eval, X_test, y_test)\n\nevaluateCV('XGBOOST', xgb, dic_eval, X_train, y_train)","0840e6c9":"if not(fast):\n    xgb.fit(X_train, y_train)\n    plot_feature_importance(X_train, xgb, 'xgb')","d4d739ff":"if env == 'kaggle':\n    lgbm = LGBMRegressor(device='gpu', random_state=42)\nelse:\n    lgbm = LGBMRegressor(device='cpu', random_state=42)\n","af1d1356":"# %%time\n# lgbm.fit(X_train, y_train)\n# evaluate('LIGHTGBM', lgbm, dic_eval, X_test, y_test)\n\nevaluateCV('LIGHTGBM', lgbm, dic_eval, X_train, y_train)","12331544":"if not(fast):\n    lgbm.fit(X_train, y_train)\n    plot_feature_importance(X_train, lgbm, 'lgbm')","9a46f339":"if not(fast):\n    features_imp_lgbm_xgb = [x\/sum(lgbm.feature_importances_) + y\/sum(xgb.feature_importances_) for x, y in zip(lgbm.feature_importances_, xgb.feature_importances_)] \n    feature_imp = pd.DataFrame(sorted(zip(features_imp_lgbm_xgb,X_train.columns)), columns=['Value','Feature'])\n\n    plt.figure(figsize=(20, 10))\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\n    plt.title('LightGBM Features (avg over folds)')\n    plt.tight_layout()\n    plt.show()\n","98a7988a":"cbr = CatBoostRegressor(iterations=500, random_seed=42)\n# Fit model\n# cbr.fit(X_train, y_train)\n# evaluate('catboost', cbr, dic_eval, X_test, y_test)\n\nevaluateCV('catboost', cbr, dic_eval, X_train, y_train)","b4ae8ef6":"# def objective_xgb(trial):\n\n#     param = {'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n#             'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n#             'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n#             'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n#             'learning_rate': trial.suggest_categorical('learning_rate', [0.012,0.014,0.016,0.018, 0.02, 0.025]),\n#             'n_estimators': trial.suggest_int('n_estimators', 500, 3000),\n#             'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11,13,15,17,20]),\n#             'min_child_weight': trial.suggest_int('min_child_weight', 1, 300)}\n\n#     if env == 'kaggle':\n#         param['tree_method'] = 'gpu_hist'\n#     else:\n#         param['tree_method'] = 'hist'\n    \n#     # model = make_pipeline(selected_scaler, XGBRegressor(**param, random_state=42))\n#     model = XGBRegressor(**param, random_state=42)\n    \n#     # pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation-rmse\")\n#     model.fit(X_train , y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=100, verbose=False)\n    \n#     preds = model.predict(X_test)\n    \n#     return rmspe(y_test, preds)\n\n\ndef objective_xgbCV(trial):\n\n    param = {\n            # 'lambda': trial.suggest_loguniform('lambda', 1e-3, 1),\n            'alpha': trial.suggest_loguniform('alpha', 1e-3, 1),\n            'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9]),\n            'subsample': trial.suggest_categorical('subsample', [0.5,0.6,0.7,0.8,1.0]),\n            'learning_rate': trial.suggest_categorical('learning_rate', [0.008,0.009,0.01,0.012,0.014,0.016,0.018, 0.02]),\n            'n_estimators': trial.suggest_int('n_estimators', 500, 3000),\n            # 'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11,13,15]),\n            # 'min_child_weight': trial.suggest_int('min_child_weight', 1, 300)\n            }\n\n    if env == 'kaggle':\n        param['tree_method'] = 'gpu_hist'\n    else:\n        param['tree_method'] = 'hist'\n    \n    # model = make_pipeline(selected_scaler, XGBRegressor(**param, random_state=42))\n    model = XGBRegressor(**param, random_state=42)\n\n    score = cross_val_score(\n        model, X_train, y_train, cv=kfolds, scoring=scorer_rmspe\n    ).mean()\n    return score","93c6ab90":"%%time\nstudy_xgb = optuna.create_study(direction='minimize', pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\nstudy_xgb.optimize(objective_xgbCV, n_trials=n_trials, gc_after_trial=True)","7bdc5223":"print('Number of finished trials:', len(study_xgb.trials))\nprint('Best trial:', study_xgb.best_trial.params)","f215e219":"if not(fast):\n    optuna.visualization.plot_optimization_history(study_xgb)","899026dc":"if not(fast):\n    optuna.visualization.plot_param_importances(study_xgb)","bfecc619":"best_xgbparams = study_xgb.best_params\nbest_xgbparams","a34d41a9":"# best_xgbparams = {'lambda': 0.050695864818244944,\n#  'alpha': 0.23319827340456734,\n#  'colsample_bytree': 0.5,\n#  'subsample': 0.8,\n#  'learning_rate': 0.02,\n#  'n_estimators': 1590,\n#  'max_depth': 9,\n#  'min_child_weight': 218}\n\nif env == 'Kaggle':\n    xgb_opt = XGBRegressor(**best_xgbparams, tree_method='gpu_hist')\nelse:\n    xgb_opt = XGBRegressor(**best_xgbparams, tree_method='hist', n_jobs= - 1)","a83255d3":"# %%time\n# xgb_opt.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=100, verbose=False)\n\n# evaluate('XGB_opt', xgb_opt, dic_eval, X_test, y_test)\n\nevaluateCV('XGB_opt', xgb_opt, dic_eval, X_train, y_train)","80bbc4a0":"# def objective_lgbm(trial):\n#         param = {\"device\": \"gpu\",\n#                 \"metric\": \"rmse\",\n#                 \"verbosity\": -1,\n#                 'learning_rate':trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n#                 \"max_depth\": trial.suggest_int(\"max_depth\", 2, 500),\n#                 \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n#                 \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n#                 \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n#                 \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 4000),\n#         #         \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 100000, 700000),\n#                 \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n#                 \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n#                 \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n#                 \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100)}\n\n#         if env == 'kaggle':\n#                 param[\"device\"] = \"gpu\"\n#         else:\n#                 param[\"device\"] = \"cpu\"\n\n#         pruning_callback = optuna.integration.LightGBMPruningCallback(trial, \"rmse\")\n#         model = LGBMRegressor(**param)\n\n#         model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False, callbacks=[pruning_callback], early_stopping_rounds=100)\n\n#         preds = model.predict(X_test)\n#         return rmspe(y_test, preds)\n\ndef objective_lgbmCV(trial):\n        param = {\n                \"metric\": \"rmse\",\n                \"verbosity\": -1,\n                'learning_rate':trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n                \"max_depth\": trial.suggest_int(\"max_depth\", 2, 500),\n                # \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n                # \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n                \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n                \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 4000),\n        #         \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 100000, 700000),\n                # \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n                \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n                \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n                \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100)}\n\n        if env == 'kaggle':\n                param[\"device\"] = \"gpu\"\n        else:\n                param[\"device\"] = \"cpu\"\n\n        # pruning_callback = optuna.integration.LightGBMPruningCallback(trial, \"rmse\")\n        model = LGBMRegressor(**param, random_state=42)\n\n        score = cross_val_score(\n        # model, X_train, y_train, cv=kfolds, scoring=scorer_rmspe, fit_params={'callbacks': [pruning_callback]}\n        # ).mean()\n        model, X_train, y_train, cv=kfolds, scoring=scorer_rmspe).mean()\n        return score","95c936da":"%%time\nstudy_lgbm = optuna.create_study(direction='minimize', pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\nstudy_lgbm.optimize(objective_lgbmCV, n_trials=n_trials, gc_after_trial=True) # n_jobs=-1 make the calcul longer !","ab441c72":"print('Number of finished trials:', len(study_lgbm.trials))\nprint('Best trial:', study_lgbm.best_trial.params)","f3d846ec":"if not(fast):\n    optuna.visualization.plot_optimization_history(study_lgbm)","3242941a":"if not(fast):\n    optuna.visualization.plot_param_importances(study_lgbm)\n","eab4db77":"best_lgbmparams = study_lgbm.best_params\nbest_lgbmparams","2e8bc4ed":"# best_lgbmparams = {'learning_rate': 0.012206112226610026,\n#     'max_depth': 176,\n#     'lambda_l1': 0.0911256640760148,\n#     'lambda_l2': 7.619751773104654e-07,\n#     'num_leaves': 87,\n#     'n_estimators': 2713,\n#     'feature_fraction': 0.6744552501464487,\n#     'bagging_fraction': 0.7249343934370382,\n#     'bagging_freq': 7,\n#     'min_child_samples': 53}\n\nif env == 'Kaggle':\n    lgbm_opt = LGBMRegressor(**best_lgbmparams, device='gpu')\nelse:\n    lgbm_opt = LGBMRegressor(**best_lgbmparams, device='cpu')\n","7087cbc1":"# %%time\n# lgbm_opt.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False, early_stopping_rounds=100)\n\n# evaluate('LIGHTGBM_opt', lgbm_opt, dic_eval, X_test, y_test)\n\nevaluateCV('LIGHTGBM_opt', lgbm_opt, dic_eval, X_train, y_train)\n","20eefd67":"if not(fast):\n    if env == 'kaggle':\n        tree_method='gpu_hist'\n        device='gpu'\n        n_jobs=None\n    else:\n        tree_method='hist'\n        device='cpu'\n        n_jobs=-1\n\n    xgb = XGBRegressor(tree_method=tree_method, random_state = RANDOM_SEED)\n    lgbm = LGBMRegressor(device=device, random_state=RANDOM_SEED)\n\n    estimators = [('lgbm_opt', lgbm_opt),\n                ('xgb_opt', xgb_opt),\n                ('lgbm', lgbm),\n                ('xgb', xgb)]\n\n    stack_reg = StackingRegressor(estimators=estimators, final_estimator=None, verbose=1, n_jobs=n_jobs)","7fdc5b70":"if not(fast):\n    evaluateCV('Stack_reg', stack_reg, dic_eval, X_train, y_train)","21b3f213":"def model_selection(dic):\n    rmspe_min = 1\n    for key in dic.keys():\n        rmspe_model = dic[key][1]\n        if rmspe_model < rmspe_min:\n            rmspe_min = rmspe_model\n            model = dic[key][0]\n            name = key\n    return model, name","d8b4becc":"dic_eval['XGBOOST'][1]","9d799ef2":"if rmspe_scaler < dic_eval['XGBOOST'][1]: #if scaler's perf is better than no scaler\n    model_to_scale, model_name = model_selection(dic_eval)\n    model = make_pipeline(selected_scaler, model_to_scale)\n    evaluateCV(model_name + '_scaled', model, dic_eval, X_train, y_train)","63d7564b":"models = [k for k in dic_eval.keys()]\nrmspe_scores = [val[1] for val in dic_eval.values()]\n\nrmspe_scores, models = (list(t) for t in zip(*sorted(zip(rmspe_scores, models))))","2988ca77":"plt.figure(figsize=(16,6))\n\nsns.barplot(x=rmspe_scores, y=models)\nplt.title('Models comparaison')\nplt.tight_layout()\n\nif (env == 'local' or env == 'kaggle'):\n    plt.savefig(save_path + 'models_comparaison.png')\nplt.show()\n","e9e96505":"model_final, model_name = model_selection(dic_eval)","7ff8bb0d":"model_final.fit(X_train, y_train)","3d530df0":"###############################\n# Save the best model\n###############################\n\nfilename = 'model_' + model_name + '.sav'\npickle.dump(model_final, open(output + filename, 'wb'))","5c104c04":"###############################\n# Load a model previously saved\n###############################\n\n# model_final = pickle.load(open(output + filename, 'rb'))","cee294ff":"###############################\n# adding prediction to df & export\n###############################\n\ndf_pred = df_pred.assign(target = model_final.predict(X_test))\ndf_pred.to_csv('submission.csv', index=False)","36b5bbb8":"pd.read_csv('submission.csv')","4e9fa6e2":"---\n\n## Final DF train\n","afa70805":"## Scaler on best model","2c6376d7":"## Stacking Regressor\n\nStack of estimators with a final regressor.\n\nStacked generalization consists in stacking the output of individual estimator and use a regressor to compute the final prediction. Stacking allows to use the strength of each individual estimator by using their output as input of a final estimator.\n","c87830ce":"### Sample\n","e8b98dbc":"### Functions and variables\n","a9f5117e":"## Naive RMSPE\n\nUn fait bien connu \u00e0 propos de la volatilit\u00e9 est qu'elle a tendance \u00e0 \u00eatre autocorr\u00e9l\u00e9e. Nous pouvons utiliser cette propri\u00e9t\u00e9 pour impl\u00e9menter un mod\u00e8le na\u00eff qui \"pr\u00e9dit\" simplement la volatilit\u00e9 r\u00e9alis\u00e9e en utilisant la volatilit\u00e9 r\u00e9alis\u00e9e au cours des 10 premi\u00e8res minutes.\n\nCalculons la volatilit\u00e9 r\u00e9alis\u00e9e de la premi\u00e8re partie de la fen\u00eatre sur le jeu de donn\u00e9e train.\n","676b9e99":"# Optiver Realized Volatility Prediction\n\n# Probl\u00e9matique\n\nLa soci\u00e9t\u00e9 Optiver, une soci\u00e9t\u00e9 de n\u00e9goce pour compte propre et un broker\/dealer pour divers instruments financiers, a lanc\u00e9e un concours sur Kaggle.  \nIl s'agit de pr\u00e9dire la volatilit\u00e9 de \"stocks\" financiers.\n\nPr\u00e9dire avec pr\u00e9cision la volatilit\u00e9 est essentiel pour la n\u00e9gociation d'options, dont le prix est directement li\u00e9 \u00e0 la volatilit\u00e9 du produit sous-jacent (ici le stock).  \nLes options ont souvent un r\u00f4le d'effet levier de l'action.  \nDans notre cas une volatilit\u00e9 importante de notre stock cr\u00e9era probablement une variation encore plus importe de l'option associ\u00e9e.\n\n## Terminologie\n\n**Stock** : action financi\u00e8re  \nEx: Apple\n<br>\n<img src=\"https:\/\/github.com\/abugeia\/P8_kaggle_competition\/blob\/master\/img\/appl_stock.PNG?raw=true\" width=\"800px\">  \n<br>\n\n**Option**  \nProduit d\u00e9riv\u00e9 qui \u00e9tablit un contrat entre un acheteur et un vendeur.  \nL'acheteur de l'option obtient le droit, et non pas l'obligation, d'acheter ou de vendre un actif sous-jacent \u00e0 un prix fix\u00e9 \u00e0 l'avance, pendant un temps donn\u00e9 ou \u00e0 une date fix\u00e9e.  \n<br>\n\n**Order book** (Carnet d'ordres)  \nListe \u00e9lectronique d'ordres d'achat et de vente pour un titre ou un instrument financier sp\u00e9cifique organis\u00e9 par niveau de prix.  \nLes ordres d'achat pr\u00e9vus sont sur le c\u00f4t\u00e9 gauche affich\u00e9s comme \"bid\" tandis que tous les ordres de vente pr\u00e9vus sont sur la droite c\u00f4t\u00e9 du livre affich\u00e9 comme \"ask\"  \n<br>\n<img src=\"https:\/\/github.com\/abugeia\/P8_kaggle_competition\/blob\/master\/img\/OrderBook3.png?raw=true\" width=\"200px\">  \n<br>\n\n**Trade book** (carnets de transactions effectu\u00e9es)  \nUn carnet d'ordres est une repr\u00e9sentation de l'intention de n\u00e9gociation sur le march\u00e9, mais le march\u00e9 a besoin d'un acheteur et d'un vendeur au m\u00eame prix pour que la transaction se produise.  \nLe trade book trace l'ensemble des transactions qui ont eu lieu\n\n**bid\/ask spread**  \n<br>\n<img src=\"https:\/\/github.com\/abugeia\/P8_kaggle_competition\/blob\/master\/img\/spread.PNG?raw=true\" width=\"350px\">\n<br><br>\n**WAP** (Weighted averaged price)  \n<br>\n<img src=\"https:\/\/github.com\/abugeia\/P8_kaggle_competition\/blob\/master\/img\/wap.PNG?raw=true\" width=\"450px\">  \n<br><br>\n\nExample :\n<br>\n<img src=\"https:\/\/github.com\/abugeia\/P8_kaggle_competition\/blob\/master\/img\/wap_bidask.PNG?raw=true\" width=\"450px\">  \n<br>\nDans cette comp\u00e9tition nous n'avons acc\u00e8s qu'aux rangs 1 & 2 des ordres.\n\n**Log return**\nPermet de comparer le cours d'une action entre deux moments.  \nEn appelant St le prix de l'action S \u00e0 l'instant t , nous pouvons d\u00e9finir le retour de log entre t1 et t2 comme\n<br>\n<img src=\"https:\/\/github.com\/abugeia\/P8_kaggle_competition\/blob\/master\/img\/logrtn.PNG?raw=true\" width=\"200px\">  \n<br>\n\n**Volatilit\u00e9**\nGr\u00e2ce aux calculs des log return sur toutes les donn\u00e9es cons\u00e9cutives du book nous pouvons d\u00e9finir la volatilit\u00e9 r\u00e9alis\u00e9e.  \nIl s'agit de la racine carr\u00e9e de la somme des log return au carr\u00e9.\n<br>\n<img src=\"https:\/\/github.com\/abugeia\/P8_kaggle_competition\/blob\/master\/img\/volatility.PNG?raw=true\" width=\"200px\">  \n<br>\n\n# Description du dataset\n\nLe dataset est constitu\u00e9 de donn\u00e9es financi\u00e8res et plus particuli\u00e8rement de carnets d'ordres et de carnets de transactions effectu\u00e9es.  \nCes deux \"book\" sont deux ensembles de fichiers s\u00e9par\u00e9s.\n\nChaque book est class\u00e9 par stock qui repr\u00e9sente un indice financier.  \nPour chaque stock nous avons plusieurs time_id.  \nCelles ci font r\u00e9f\u00e9rence \u00e0 une fen\u00eatre de valeurs r\u00e9elles de 20 min. Elles ne sont pas chronologiquement cons\u00e9cutives.  \nCertains de ces time_id sont publiques et font partie de l'\u00e9chantillon train, d'autres sont cach\u00e9 et constituent l'\u00e9chantillon de test.\n\nDans chacune de ces fen\u00eatres de 20 min nous avons acc\u00e8s aux premi\u00e8res 10 min de donn\u00e9es et nous devons pr\u00e9dire la volatilit\u00e9 des 10 min suivantes.  \nLa volatilit\u00e9 de ces derni\u00e8res 10 min nous est fournie (pour l'\u00e9chantillon train) et sera notre target.\n\n<img src=\"https:\/\/github.com\/abugeia\/P8_kaggle_competition\/blob\/master\/img\/DataBucketing.webp?raw=true\" width=\"300px\">  \n<br>\n<br>\nExample et explication :\n<br>\n<img src=\"https:\/\/github.com\/abugeia\/P8_kaggle_competition\/blob\/master\/img\/Data_chart.PNG?raw=true\" width=\"800px\"><br>\n<img src=\"https:\/\/github.com\/abugeia\/P8_kaggle_competition\/blob\/master\/img\/Data_explainations.PNG?raw=true\" width=\"600px\">\n\n# Process machine learning\n\nIl s'agit d'un probl\u00e8me de r\u00e9gression supervis\u00e9.\n\nDans un premier temps nous effectuerons un nettoyage \u00e9ventuel et explorerons nos donn\u00e9es.  \nPuis nous Ferons du feature engineering sur nos dataset de book et trade.\n\n## Evaluation\n\nL'\u00e9valuation des performances de nos pr\u00e9dictions par rapport aux donn\u00e9es de l'\u00e9chantillon de test se fera avec une m\u00e9trique impos\u00e9e : le RMSPE (Root mean square percentage error).  \nC'est ainsi une erreur quadratique moyenne normalis\u00e9e puisqu'elle s'exprime en pourcentage.\n\n<br>\n<img src=\"https:\/\/github.com\/abugeia\/P8_kaggle_competition\/blob\/master\/img\/RMSPE.PNG?raw=true\" width=\"300px\"><br>\n\nIl faut agr\u00e9ger nos donn\u00e9es sur un seul dataframe avec une ligne par time_id avant d'entra\u00eener et appliquer un mod\u00e8le.  \nEn effet voici la structure que devra avoir notre pr\u00e9diction :\n<br>\n<img src=\"https:\/\/github.com\/abugeia\/P8_kaggle_competition\/blob\/master\/img\/sub_form.PNG?raw=true\" width=\"250px\"><br>\n\nIci la colonne row_id est compos\u00e9 de {stock}-{time_id} et target est la pr\u00e9diction de notre volatilit\u00e9.\n\n## R\u00e9f\u00e9rences\n\nCes Kernels m'ont particuli\u00e8rement aid\u00e9 dans ma participation :\n\n[https:\/\/www.kaggle.com\/alexioslyon\/lgbm-baseline](https:\/\/www.kaggle.com\/alexioslyon\/lgbm-baseline)  \n[https:\/\/www.kaggle.com\/munumbutt\/feature-engineering-tuned-xgboost-lgbm](https:\/\/www.kaggle.com\/munumbutt\/feature-engineering-tuned-xgboost-lgbm)\n","73eadefc":"### Test values\n","ac41d548":"## Basic XGB model\n\n---\n","fa29cb59":"### Train values\n","f7a6c96a":"## Score visualization\n","3ddb0f25":"## Removing useless features\n","cd121d0e":"## Trade train\n","29bcbf1c":"# Variables\n","a8a60a32":"## Ridge\n\n---\n","1b3985d1":"**Process flow**  \n<br>\n<img src=\"https:\/\/github.com\/abugeia\/P8_kaggle_competition\/blob\/master\/img\/p8_process_orders.png?raw=true\" width=\"900px\"><br>\n","46e2c525":"## Test dataset\n","8a028479":"## Lasso\n\n---\n","30d3e5b5":"### Dataset\n","7e6117a4":"# Preprocessing & baseline\n\n---\n","d7bbb011":"---\n\n## Trades book train\n","0a451b74":"## CatBoost\n","0c13056f":"## Functions\n","4c7e4594":"# Submission\n","c066566b":"# Librairies\n","e7b539bc":"## Optuna Tuned LGBM\n","43762315":"# Exploration\n","011e6a19":"## Order book train\n","93f2b3e0":"## Targets \/ realized volatility\n","2c097f5b":"**Process flow**  \n<br>\n<img src=\"https:\/\/github.com\/abugeia\/P8_kaggle_competition\/blob\/master\/img\/p8_process_trades.png?raw=true\" width=\"900px\"><br>\n","c6372125":"## Book train\n","de346aa0":"# Machine learning\n\n---\n","319ac31d":"## Functions\n","a5202a1e":"## Optuna Tuned XGBoost\n\nOptuna va nous permettre de trouver nos meilleurs hyperparam\u00e8tres.  \nIl suffira ensuite d'entra\u00eener notre mod\u00e8le avec ces param\u00e8tres pour l'\u00e9valuer.\n","23b9952d":"## RandomForrest\n","267c3995":"### Sample\n","0797f5be":"## Basic LGBMRegressor model\n","6c0ad0ce":"## Book\/Trade test\n\nThese file are here just to show the shape and firsts value of the hidden 10 min window.\n","a936cb4f":"### Scalers\n"}}