{"cell_type":{"ef9374d0":"code","e7ba1916":"code","2cf75be1":"code","107d48a7":"code","cba77696":"code","1a4476d8":"code","b7786b05":"code","2c20e4e9":"code","aeb85918":"code","bc2fb02e":"code","719b6791":"code","81a98a57":"code","ea90e499":"code","56ac62ac":"code","e7f054a9":"code","056a269d":"code","b018dc96":"code","21d6ac07":"code","e762d4ec":"code","e9dc39c3":"markdown","fafd32c5":"markdown","4f95740d":"markdown","cb1ddd7b":"markdown","b7363568":"markdown","33fbb7a8":"markdown","29062e44":"markdown","f8bfbcb6":"markdown","d27a9793":"markdown","c8305368":"markdown","fa338917":"markdown","b560f2d5":"markdown","34542b21":"markdown","5fd37dd4":"markdown"},"source":{"ef9374d0":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.svm import OneClassSVM\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, recall_score, f1_score, accuracy_score, precision_score\nfrom sklearn.ensemble import VotingClassifier, RandomForestClassifier\nfrom sklearn.decomposition import PCA, IncrementalPCA, LatentDirichletAllocation\nfrom sklearn.manifold import TSNE\n\nfrom tqdm.notebook import tqdm, trange\nfrom typing import NoReturn, Union, List\nfrom mlxtend.classifier import EnsembleVoteClassifier\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom umap import UMAP","e7ba1916":"df = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndf.head()","2cf75be1":"# use a dataset sample for development purposes\ndev = False","107d48a7":"df.groupby(['Class']).Class.count().plot(kind='pie', title='Fraudulent VS Genuine Transactions')","cba77696":"df_s = df.sample(2000)\nX = df_s[[_ for _ in df.columns if _ != 'Class']]\n\npca = PCA(n_components=2)\ntsne = TSNE(n_components=2)\nump = UMAP(n_components=2)\nipca = IncrementalPCA(n_components=2)\n\nx_pca = pca.fit_transform(X)\nx_tsne = tsne.fit_transform(X)\nx_umap = ump.fit_transform(X)","1a4476d8":"fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n\nsizes = pd.Series(df_s['Class']+1).pow(5) # represent fraud with bigger point\n\naxes[0].scatter(x_pca[:, 0], x_pca[:, 1], s=sizes, c=df_s['Class'].values)\naxes[1].scatter(x_tsne[:, 0], x_tsne[:, 1], s=sizes, c=df_s['Class'].values)\naxes[2].scatter(x_umap[:, 0], x_umap[:, 1], s=sizes, c=df_s['Class'].values)\n\naxes[0].set_title('PCA')\naxes[1].set_title('t-SNE')\naxes[2].set_title('UMAP')\n\nfig.tight_layout()\nplt.show()","b7786b05":"# time and amount scaling\ndf['Time'] = StandardScaler().fit_transform(df['Time'].values.reshape(-1, 1))\ndf['Amount'] = StandardScaler().fit_transform(df['Amount'].values.reshape(-1, 1))\n\ndf_anom = df[df['Class'] == 1]\ndf_norm = df[df['Class'] == 0]\n\nif dev:\n    df_norm = df_norm.sample(5000, random_state=42)\ndf_test_norm = df_norm.sample(df_anom.shape[0])\ndf_test = pd.concat([\n    df_anom,\n    df_test_norm\n])\ndf_train = df_norm.drop(df_test_norm.index)\n\nfeature_cols = [_ for _ in df.columns if _ != 'Class']","2c20e4e9":"X_train = df_train[feature_cols]\ny_train = df_train['Class'] # will not be used\nX_test = df_test[feature_cols]\ny_test = df_test['Class'] # for evaluation\nprint('''\ntrain: [{:>8} x {:<5}]\n test: [{:>8} x {:<5}]\n'''.format(*X_train.shape, *X_test.shape))","aeb85918":"def sensitivity_keras(y_true, y_pred):\n    \"\"\"credits: https:\/\/datascience.stackexchange.com\/a\/40746\/28592\n    \n    param:\n    y_pred - Predicted labels\n    y_true - True labels \n    Returns:\n    Specificity score\n    \"\"\"\n    neg_y_true = 1 - y_true\n    neg_y_pred = 1 - y_pred\n    fp = tf.keras.backend.sum(neg_y_true * y_pred)\n    tn = tf.keras.backend.sum(neg_y_true * neg_y_pred)\n    specificity = tn \/ (tn + fp + tf.keras.backend.epsilon())\n    return specificity","bc2fb02e":"class Scaled_IsolationForest(IsolationForest):\n    \"\"\"The purpose of this sub-class is to transform prediction values from {-1, 1} to {1,0}\n    \"\"\"\n    def predict(self, X):\n        pred = super().predict(X)\n        scale_func = np.vectorize(lambda x: 1 if x == -1 else 0)\n        return scale_func(pred)\n\nclass Scaled_OneClass_SVM(OneClassSVM):\n    \"\"\"The purpose of this sub-class is to transform prediction values from {-1, 1} to {1,0}\n    \"\"\"\n    def predict(self, X):\n        return np.array([y==-1 for y in super().predict(X)])\n    \nclass NoveltyDetection_Sequential(tf.keras.models.Sequential):\n    \"\"\"This custom `tf.keras.models.Sequential` sub-class transforms autoencoder's output into {1,0}.\n    Output value is determined based on reproduction (decode) loss. If reproduction loss is more than a threashold then, the input sample is considered as anomaly (outlier).\n    Based on few experiments, 1.5 is a dissent threashold (don't as why :P). Future work: determine the threashold using a more sophisticated method.\n    \"\"\"\n    def predict(self, x, *args, **kwargs):\n        pred = super().predict(x, *args, **kwargs)\n        mse = np.mean(np.power(x - pred, 2), axis=1)\n        scale_func = np.vectorize(lambda x: 1 if x > 1.5 else 0)\n        return scale_func(mse)","719b6791":"# define early stop in order to prevent overfitting and useless training\nearly_stop = tf.keras.callbacks.EarlyStopping(\n    monitor='mse',\n    patience=10,\n    verbose=1, \n    mode='min',\n    restore_best_weights=True,\n)\n\n# it's a common practice to store the best model\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(\n    filepath='autoenc.hdf5',\n    save_best_only=True,\n    monitor='val_loss',\n    mode='min',\n    verbose=0\n)\n\ndef get_autoencoder() -> tf.keras.models.Sequential:\n    \"\"\"Build an autoencoder\n    \"\"\"\n    model = NoveltyDetection_Sequential([\n        tf.keras.layers.Dense(X_train.shape[1], activation='relu', input_shape=(X_train.shape[1], )),\n        # add some noise to prevent overfitting\n        tf.keras.layers.GaussianNoise(0.05),\n        tf.keras.layers.Dense(2, activation='relu'),\n        tf.keras.layers.Dense(X_train.shape[1], activation='relu')\n    ])\n    model.compile(optimizer='adam', \n                        loss='mse',\n                        metrics=['acc', sensitivity_keras])\n    return model","81a98a57":"clfs = {\n    'isolation_forest': {\n        'label': 'Isolation Forest',\n        'clb': Scaled_IsolationForest,\n        'params': {\n            'contamination': 'auto',\n            'n_estimators': 300\n        },\n        'predictions': None,\n        'model': None\n    },\n    'ocsvm': {\n        'label': 'OneClass SVM',\n        'clb': Scaled_OneClass_SVM,\n        'params': {\n            'kernel': 'rbf',\n            'gamma': 0.3,\n            'nu': 0.01,\n        },\n        'prediction': None,\n        'model': None\n    },\n    'auto-encoder': {\n        'label': 'Autoncoder',\n        'clb': get_autoencoder,\n        'params': {},\n        'fit_params': {\n            'x': X_train, 'y': X_train,\n            'validation_split': 0.2,\n            'callbacks': [early_stop, checkpoint],\n            'epochs': 64,\n            'batch_size': 256,\n            'verbose': 0\n        },\n        'predictions': None,\n        'model': None\n    }\n}","ea90e499":"%%time\n\nt = trange(len(clfs))\nfor name in clfs:\n    t.set_description(clfs[name]['label'])\n    clfs[name]['model'] = clfs[name]['clb'](**clfs[name]['params'])\n    if 'fit_params' in clfs[name]:\n        clfs[name]['model'].fit(**clfs[name].get('fit_params', {}))\n    else:\n        clfs[name]['model'].fit(X_train)\n    clfs[name]['predictions'] = clfs[name]['model'].predict(X_test)\n    t.update()\nt.close()","56ac62ac":"def print_eval_metrics(y_true, y_pred, name='', header=True):\n    \"\"\"Function for printing purposes\n    \"\"\"\n    if header:\n        print('{:>20}\\t{:>10}\\t{:>10}\\t{:>8}\\t{:>5}'.format('Algorith', 'Accuracy', 'Recall', 'Precision', 'f1'))\n    acc = accuracy_score(y_true, y_pred)\n    recall = recall_score(y_true, y_pred)\n    prec = precision_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n    print('{:>20}\\t{:>1.8f}\\t{:>1.8f}\\t{:>1.6f}\\t{:>1.3f}'.format(\n        name, acc, recall, prec, f1\n    ))","e7f054a9":"y_preds = np.column_stack([clfs[_]['predictions'] for _ in clfs])\nenseble_preds = []","056a269d":"hard_vot = EnsembleVoteClassifier([clfs[_]['model'] for _ in clfs], fit_base_estimators=False)\nhard_vot.fit(X_test, y_test)\nenseble_preds.append((hard_vot.predict(X_test), 'Hard Voting'))","b018dc96":"wei_hard_vot = EnsembleVoteClassifier([clfs[_]['model'] for _ in clfs], weights=[\n        0.4,\n        0.1,\n        0.8\n    ], fit_base_estimators=False)\nwei_hard_vot.fit(X_test, y_test)\nenseble_preds.append((wei_hard_vot.predict(X_test), 'Weighted Hard Voting'))","21d6ac07":"rf = RandomForestClassifier()\n\nx_tr_ens, x_ts_ens, y_tr_ens, y_ts_ens = train_test_split(y_preds, y_test, test_size=.5)\nrf.fit(x_tr_ens, y_tr_ens)","e762d4ec":"print_header = True\nfor k, v in clfs.items():\n    print_eval_metrics(y_test, v['predictions'], v['label'], print_header)\n    print_header = False\n\nprint('\\n')\n\nfor prds, l in enseble_preds:\n    print_eval_metrics(y_test, prds, l, print_header)\n    print_header = False\n\nprint('\\n')\n    \nprint_eval_metrics(\n    y_ts_ens,\n    rf.predict(x_ts_ens),\n    'Bleding using RF', False\n)","e9dc39c3":"## Future Work\n\n - Find the auto-encoding loss threashold using a more sophisticated way\n - Test more models and different configurations","fafd32c5":"### Dataset\n - `V{1-28}`: `PCA` decompossition outcome\n - `Class` label (0: normal, 1: fraudulent)\n - `Time`: Number of seconds elapsed between this transaction and the first transaction in the dataset\n - `Amount`: transaction amount","4f95740d":"The following dictionary containes the models and the parameters that we are going to use during the phase of training","cb1ddd7b":"`PCA`, `UMAP` and `t-SNE` will be used for further dimensionality reduction in order to visualize a sample of the dataset","b7363568":"### Preprocessing\n`Time` and `Amount` fields should be scaled\n\nDuring the step of pre-processing, the dataset will be splited in two parts:\n1. ~283K samples of genuine transactions (Training set)\n2. All fraudulent samples and equal number of genuine samples (Test set)\n\nNovelty detection is concered as a semi-supervised task due to the fact that only the normal samples are used during the phase of training. During the phase of evaluation, a balanced subset of genuine and fraudulent samples will be used.","33fbb7a8":"### Evaluation\n\nIt's crusial to detect fraudulent transactions, therefor a significat evaluation metric could the `simplicity`. For every trained method and ensebling method the following evaluation metrics will be calculated:\n - Accuracy\n - Recall\n - Precision\n - f1 Score","29062e44":"#### Hard Voting\nThis is one of the simplest way to combine multiple models in order to generalize better and achive better performance.","f8bfbcb6":"### Training\n\nWe are going to define some wrappers, these classes will work as adapters in order to have an abstract implementation.","d27a9793":"## Implementation\n\n![implementation](https:\/\/i.imgur.com\/Y8FWKqi.png)","c8305368":"### Enseble","fa338917":"## Resources\n1. Outlier Detection with One-Class SVMs [url](https:\/\/towardsdatascience.com\/outlier-detection-with-one-class-svms-5403a1a1878c)\n2. Niu, X., Wang, L., & Yang, X. (2019). A comparison study of credit card fraud detection: Supervised versus unsupervised. arXiv preprint arXiv:1904.10604. [url](https:\/\/arxiv.org\/abs\/1904.10604) [pdf](https:\/\/arxiv.org\/pdf\/1904.10604)\n3. Benefits of Anomaly Detection Using Isolation Forests [url](https:\/\/blog.easysol.net\/using-isolation-forests-anamoly-detection\/)\n4. scikit-learn: Voting Classifier [url](https:\/\/scikit-learn.org\/stable\/modules\/ensemble.html#voting-classifier)\n5. scikit-learn: Novelty and Outlier Detection [url](https:\/\/scikit-learn.org\/stable\/modules\/outlier_detection.html#outlier-detection)\n6. Building Autoencoders in Keras [url](https:\/\/blog.keras.io\/building-autoencoders-in-keras.html)\n7. Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016. [url](https:\/\/www.deeplearningbook.org\/)\n8. Porwal, Utkarsh, and Smruthi Mukund. \"Credit card fraud detection in e-commerce: An outlier detection approach.\" arXiv preprint arXiv:1811.02196 (2018). [url](https:\/\/arxiv.org\/abs\/1811.02196)","b560f2d5":"#### Weighted Hard Voting\nUsing weighted hard voting you can take advantage of most high-performed models","34542b21":"#### Blending\nWe are going to use the the predicted values as input to another model","5fd37dd4":"## Intro\n\ndataset: [Credit Card Fraud Detection](https:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud)\n\nkeywords: `fraud detection`, `novelty detection`, `ensembling`, `learning representation`, `semi-supervised learning`\n\nUsually, datasets related to fraud detection are highly unbalanced due to the fact that, in the of transactions, only few of then are fraudulent. Instead of trying to augment the dataset using a resample method, we are going to approach this problem as a Novelty Detection problem.\n\nOn this kernel, we are goind describe the dataset briefly and then, we will compare multiple machine learning algorithms using some evaluation metrics.\n\nMost common machine learning algorithms for this kind of tasks are the following:\n\n1. Isolation forest\n2. One class SVM\n3. Autoencoders\n\n\n### Isolation Forest\n![isolation-tree](https:\/\/i.imgur.com\/rzP7siS.png)\n\n[source](https:\/\/blog.easysol.net\/using-isolation-forests-anamoly-detection\/)\n\n> The Isolation Forest algorithm isolates observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature. The logic argument goes: isolating anomaly observations is easier because only a few conditions are needed to separate those cases from the normal observations. On the other hand, isolating normal observations require more conditions. Therefore, an anomaly score can be calculated as the number of conditions required to separate a given observation.\n\n\n### One Class SVM (OCSVM)\n![](https:\/\/ars.els-cdn.com\/content\/image\/1-s2.0-S0031320314002751-gr6.jpg)\n\n[source](https:\/\/towardsdatascience.com\/outlier-detection-with-one-class-svms-5403a1a1878c)\n\n> A One-Class Support Vector Machine is an unsupervised learning algorithm that is trained only on the \u2018normal\u2019 data, in our case the negative examples. It learns the boundaries of these points and is therefore able to classify any points that lie outside the boundary as, you guessed it, outliers.\n\n### Autoencoders\n\n[source](https:\/\/www.deeplearningbook.org\/contents\/autoencoders.html)\n\n> An autoencoder is a neural network that is trained to attempt to copy its input to its output. Internally, it has a hidden layer h that describes a code used to represent the input. The network may be viewed as consisting of two parts: an encoder function `h = f (x)` and a decoder that produces a reconstruction `r = g(h)`.\n\n![autoencoder](https:\/\/miro.medium.com\/max\/3524\/1*oUbsOnYKX5DEpMOK3pH_lg.png)\n\nIn our case, we will train the autoencoder using samples of normal transactions only. After that, we will provide a fraudulent sample as input to the model. As a result, the representation should have larger loss error. Therefore, by defining a loss threashold, this ANN model will work as novelty detection model."}}