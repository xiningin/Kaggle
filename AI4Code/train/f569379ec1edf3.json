{"cell_type":{"6d5d483f":"code","2be2ab34":"code","31b7be3f":"code","42224cd2":"code","43ba742a":"code","c7d85242":"code","6b9ac92f":"code","4408deb4":"code","995d066b":"code","10f7bb49":"code","082b0b2c":"code","3e92e437":"code","2a36ab26":"markdown","fd770b8d":"markdown","0c03befa":"markdown","a0c25ac9":"markdown","ac848275":"markdown","c40064de":"markdown","180eb884":"markdown","dc9ed2a6":"markdown","b166f3c0":"markdown","8f9b0bb5":"markdown"},"source":{"6d5d483f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2be2ab34":"SAMPLE_FILE_PATH = \"..\/input\/facial-expression-recognition-challenge\/icml_face_data.csv\/icml_face_data.csv\"\n\n\nNUM_CLASSES = 7\n\nTRAIN_HDF5 = \"\/kaggle\/working\/train.hdf5\"\nVAL_HDF5 = \"\/kaggle\/working\/val.hdf5\"\nTEST_HDF5 = \"\/kaggle\/working\/test.hdf5\"\n\nBATCH_SIZE = 128\nOUTPUT_PATH = \"\/kaggle\/working\"\n\nDATASET_MEAN_FILE = OUTPUT_PATH + \"\/rgb_mean.json\"\n\nMODEL_FILE = OUTPUT_PATH + \"\/model.h5\"\n","31b7be3f":"from tensorflow.keras.callbacks import Callback\nimport os\n\n\nclass EpochCheckpoint(Callback):\n    def __init__(self,output_path,every=5,start_at=0):\n        super(Callback,self).__init__()\n\n        self.output_path = output_path\n        self.every = every\n        self.start_epoch = start_at\n\n\n    def on_epoch_end(self, epoch, logs={}):\n        if (self.start_epoch + 1)% self.every ==0:\n            p = os.path.sep.join([self.output_path,\n                                  \"epoch_{}.hdf5\".format(self.start_epoch + 1)])\n            self.model.save(p, overwrite = True)\n            self.start_epoch += 1\n","42224cd2":"from tensorflow.keras.preprocessing.image import img_to_array\n\nclass ImageToArrayPreprocesor:\n    def __init__(self,data_format=None):\n        self.data_format = data_format\n\n    def preprocess(self,image):\n        return img_to_array(image, data_format=self.data_format)","43ba742a":"import os\nimport h5py\n\n\nclass HDF5DatasetWriter:\n    def __init__(self, dims, output_path, data_key=\"images\", buf_size=1000):\n        if os.path.exists(output_path):\n            raise ValueError(\"\u60a8\u63d0\u4f9b\u7684\u8f93\u51fa\u6587\u4ef6{}\u5df2\u7ecf\u5b58\u5728\uff0c\u8bf7\u624b\u52a8\u5220\u9664\".format(output_path))\n        self.db = h5py.File(output_path, \"w\")\n        self.data = self.db.create_dataset(data_key, dims, dtype=\"float\")\n        self.labels = self.db.create_dataset(\"labels\", (dims[0],), dtype=\"int\")\n\n        self.buf_size = buf_size\n        self.buffer = {\"data\": [], \"labels\": []}\n        self.idx = 0\n\n    def add(self, raw, label):\n        self.buffer[\"data\"].extend(raw)\n        self.buffer[\"labels\"].extend(label)\n        if len(self.buffer[\"data\"]) >= self.buf_size:\n            self.flush()\n\n    def flush(self):\n        i = self.idx + len(self.buffer[\"data\"])\n        self.data[self.idx:i] = self.buffer[\"data\"]\n        self.labels[self.idx:i] = self.buffer[\"labels\"]\n        self.idx = i\n        self.buffer = {\"data\": [], \"labels\": []}\n\n    def store_class_labels(self, class_labels):\n        dt = h5py.special_dtype(vlen=str)\n        label_dim = (len(class_labels),)\n        label_set = self.db.create_dataset(\"label_names\", label_dim, dtype=dt)\n        label_set[:] = class_labels\n\n    def close(self):\n        if len(self.buffer[\"data\"]) > 0:\n            self.flush()\n        self.db.close()\n","c7d85242":"from keras.utils.np_utils import to_categorical\nimport numpy as np\nimport h5py\n\n\nclass HDF5DatasetGenerator:\n    def __init__(self, db_file, batch_size, preprocessors=None, aug=None, binarize=True, classes=2):\n        self.batch_size = batch_size\n        # \u6570\u636e\u9884\u5904\u7406\u5668\u5217\u8868\n        self.preprocessor = preprocessors\n        # \u6570\u636e\u589e\u5f3a\u5904\u7406\u5668\u5217\u8868\n        self.aug = aug\n        self.binarize = binarize\n        self.classes = classes\n        self.db = h5py.File(db_file,'r')\n        self.numImages = self.db[\"labels\"].shape[0]\n\n    def generator(self, passes=np.inf):\n        epochs = 0\n\n        while epochs < passes:\n            for i in np.arange(0, self.numImages, self.batch_size):\n                images = self.db[\"images\"][i:i + self.batch_size]\n                labels = self.db[\"labels\"][i:i + self.batch_size]\n\n                if self.binarize:\n                    labels = to_categorical(labels, self.classes)\n                if self.preprocessor is not None:\n                    processed_image = []\n                    for image in images:\n                        for p in self.preprocessor:\n                            image = p.preprocess(image)\n                        processed_image.append(image)\n                    images = np.array(processed_image)\n                if self.aug is not None:\n                    (images, labels) = next(self.aug.flow(images, labels, batch_size=self.batch_size))\n\n                    yield images, labels\n                epochs += 1\n\n    def close(self):\n        self.db.close()\n","6b9ac92f":"from tensorflow.keras.callbacks import BaseLogger\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport json\nimport os\n\n\nclass TrainingMonitor(BaseLogger):\n    def __init__(self,fig_path,json_path=None, start_at =0):\n        super(TrainingMonitor, self).__init__()\n        self.history = {}\n        self.fig_path = fig_path\n        self.json_path = json_path\n        self.start_at = start_at\n\n    def on_train_begin(self, logs={}):\n        if self.json_path is not None:\n            if os.path.exists(self.json_path):\n                self.history = json.loads(open(self.json_path).read())\n\n                if self.start_at > 0:\n                    for k in self.history.keys():\n                        self.history[k] = self.history[k][:self.start_at]\n\n\n    def on_epoch_end(self, epoch, logs={}):\n        for (k,v) in logs.items():\n            log = self.history.get(k, [])\n            log.append(v)\n            self.history[k] =  log\n\n        if self.json_path is not None:\n            f = open(self.json_path, \"w\")\n            f.write(json.dumps(self.history))\n            f.close()\n\n\n        if len(self.history[\"loss\"]) >1:\n            N = np.arange(0, len(self.history[\"loss\"]))\n            plt.style.use(\"ggplot\")\n            plt.figure()\n            plt.plot(N, self.history[\"loss\"], label=\"train_loss\")\n            plt.plot(N, self.history[\"val_loss\"], label=\"val_loss\")\n            plt.plot(N, self.history[\"accuracy\"], label=\"train_acc\")\n            plt.plot(N, self.history[\"val_accuracy\"], label=\"val_acc\")\n            epochs = len(self.history[\"loss\"])\n            plt.title(\"Training Loss & Accuracy [Epoch {}]\".format(epochs))\n            plt.xlabel(\"Epoch #\")\n            plt.ylabel(\"Loss\/Accuracy\")\n            plt.legend()\n            plt.savefig(self.fig_path)\n            plt.close()\n","4408deb4":"import numpy as np\n# from utils.HDF5DatasetWriter import HDF5DatasetWriter\n# from config import setting\n\nprint(\"[\u4fe1\u606f] \u52a0\u8f7dcsv\u683c\u5f0f\u6570\u636e\u96c6\u6587\u4ef6\")\n\nfile = open(SAMPLE_FILE_PATH)\nfile.__next__()#\u8df3\u8fc7\u7b2c\u4e00\u884c\n(train_images, train_label) = ([], [])\n(val_images, val_label) = ([], [])\n(test_images, test_label) = ([], [])\ncount_by_label_train = {}\ncount_by_label_val = {}\ncount_by_label_test = {}\nfor row in file:\n    (label, usage, image) = row.strip().split(\",\")\n    label = int(label)\n    image = np.array(image.split(\" \"), dtype=\"uint8\")\n    image = image.reshape((48, 48))\n\n    if usage == \"Training\":\n        train_images.append(image)\n        train_label.append(label)\n        count = count_by_label_train.get(label, 0)\n        count_by_label_train[label] = count + 1\n\n    elif usage == \"PublicTest\":\n        val_images.append(image)\n        val_label.append(label)\n        count = count_by_label_val.get(label, 0)\n        count_by_label_val[label] = count + 1\n\n    elif usage == \"PrivateTest\":\n        test_images.append(image)\n        test_label.append(label)\n        count = count_by_label_test.get(label, 0)\n        count_by_label_test[label] = count + 1\n\nfile.close()\nprint(\"[\u4fe1\u606f] \u8bad\u7ec3\u96c6\u6837\u672c\u6570\u91cf\uff1a{}\".format(len(train_images)))\nprint(\"[\u4fe1\u606f] \u6821\u9a8c\u96c6\u6837\u672c\u6570\u91cf\uff1a{}\".format(len(val_images)))\nprint(\"[\u4fe1\u606f] \u6d4b\u8bd5\u96c6\u6837\u672c\u6570\u91cf\uff1a{}\".format(len(test_images)))\n#\u8bad\u7ec3\u96c6\u6837\u672c\u5206\u5e03\nprint(count_by_label_train)\n#\u6821\u6b63\u96c6\u6837\u672c\u5206\u5e03\nprint(count_by_label_val)\n#\u6d4b\u8bd5\u96c6\u6837\u672c\u5206\u5e03\nprint(count_by_label_test)\n\ndatasets = [(train_images,train_label,TRAIN_HDF5),\n            (val_images,val_label,VAL_HDF5),\n            (test_images,test_label,TEST_HDF5)]\n\nfor (images,labels,outputPath) in datasets:\n    print(\"[\u4fe1\u606f]\u6784\u5efa{}...\".format(outputPath))\n    writer = HDF5DatasetWriter((len(images),48,48),outputPath)\n\n    for (image,label) in zip(images,labels):\n        writer.add([image],[label])\n\n    writer.close()\n\n","995d066b":"from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import backend\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.layers import Activation, BatchNormalization\n\n\nclass MiniVGG13Net():\n    @staticmethod\n    def build(width, height, channel, classes, reg=0.0002):\n        model = Sequential(name=\"MiniVGG13Net\")\n        shape = (width, height, channel)\n        channel_dimension = -1\n        if backend.image_data_format == \"channel first\":\n            shape = (channel, width, height)\n            channel_dimension = 1\n        # \u7b2c\u4e00\u5377\u79ef\u5757\n        model.add(Conv2D(64, (3, 3), input_shape=shape, padding=\"same\"))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(Activation(\"relu\"))\n        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n\n        # model.add(Conv2D(64, (3, 3), input_shape=shape, padding=\"same\", kernel_regularizer=l2(reg)))\n        # model.add(Activation(\"relu\"))\n        # model.add(Conv2D(64, (3, 3), padding=\"same\", kernel_regularizer=l2(reg)))\n        # model.add(Activation(\"relu\"))\n        # model.add(BatchNormalization(axis=channel_dimension))\n        # model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n        # model.add(Dropout(0.35))\n\n        # \u7b2c\u4e8c\u5377\u79ef\u5757\n        model.add(Conv2D(128, (3, 3), padding=\"same\"))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(Activation(\"relu\"))\n        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n\n\n        # model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_regularizer=l2(reg)))\n        # model.add(Activation(\"relu\"))\n        # model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_regularizer=l2(reg)))\n        # model.add(Activation(\"relu\"))\n        # model.add(BatchNormalization(axis=channel_dimension))\n        # model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n        # model.add(Dropout(0.35))\n\n        # \u7b2c\u4e09\u5377\u79ef\u5757\n        model.add(Conv2D(256, (3, 3), padding=\"same\"))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(Activation(\"relu\"))\n        model.add(Conv2D(256, (3, 3), padding=\"same\"))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(Activation(\"relu\"))\n        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n\n\n        # model.add(Conv2D(256, (3, 3), padding=\"same\", kernel_regularizer=l2(reg)))\n        # model.add(Activation(\"relu\"))\n        # model.add(Conv2D(256, (3, 3), padding=\"same\", kernel_regularizer=l2(reg)))\n        # model.add(Activation(\"relu\"))\n        # model.add(Conv2D(256, (3, 3), padding=\"same\", kernel_regularizer=l2(reg)))\n        # model.add(Activation(\"relu\"))\n        # model.add(BatchNormalization(axis=channel_dimension))\n        # model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n        # model.add(Dropout(0.35))\n\n        # \u7b2c\u56db\u5377\u79ef\u5757\n        model.add(Conv2D(512, (3, 3), padding=\"same\"))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(Activation(\"relu\"))\n        model.add(Conv2D(512, (3, 3), padding=\"same\"))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(Activation(\"relu\"))\n        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n\n\n        # model.add(Conv2D(512, (3, 3), padding=\"same\", kernel_regularizer=l2(reg)))\n        # model.add(Activation(\"relu\"))\n        # model.add(Conv2D(512, (3, 3), padding=\"same\", kernel_regularizer=l2(reg)))\n        # model.add(Activation(\"relu\"))\n        # model.add(Conv2D(512, (3, 3), padding=\"same\", kernel_regularizer=l2(reg)))\n        # model.add(Activation(\"relu\"))\n        # model.add(BatchNormalization(axis=channel_dimension))\n        # model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n        # model.add(Dropout(0.35))\n\n        # \u7b2c\u4e94\u5377\u79ef\u5757\n        model.add(Conv2D(512,(3,3),padding=\"same\"))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(Activation(\"relu\"))\n        model.add(Dropout(0.5))\n\n        model.add(Conv2D(512, (3, 3), padding=\"same\"))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(Activation(\"relu\"))\n        model.add(Dropout(0.5))\n        model.add(MaxPooling2D(pool_size=(2,2),padding=\"same\",strides=(1,1)))\n\n        # \u7b2c\u4e00\u5168\u8fde\u63a5\u5c42\n        model.add(Flatten())\n        model.add(Dense(256, kernel_regularizer=l2(reg)))\n        # model.add(Dense(64, kernel_regularizer=l2(reg)))\n\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(Activation(\"relu\"))\n        # model.add(Dropout(0.35))\n        model.add(Dropout(0.5))\n        # \u7b2c\u4e8c\u5168\u8fde\u63a5\u5c42\n        model.add(Flatten())\n        model.add(Dense(128, kernel_regularizer=l2(reg)))\n        # model.add(Dense(64, kernel_regularizer=l2(reg)))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(Activation(\"relu\"))\n        # model.add(Dropout(0.35))\n        model.add(Dropout(0.5))\n        # \u7b2c\u4e09\u5168\u8fde\u63a5\u5c42\n        model.add(Dense(classes, kernel_regularizer=l2(reg)))\n        model.add(Activation(\"softmax\"))\n\n        return model\n\n\nif __name__ == \"__main__\":\n    model = MiniVGG13Net.build(48, 48, 1, 7, reg=0.0002)\n    print(model.summary())\n\n\n","10f7bb49":"import matplotlib\n# from config import setting\n# from utils.ImageToArrayPreprocessor import ImageToArrayPreprocessor\n# from utils.TrainingMonitor import TrainingMonitor\n# from utils.HDF5DatasetGenerator import HDF5DatasetGenerator\n# from MiniVGG13 import MiniVGG13Net\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam\nimport os\nmatplotlib.use(\"Agg\")\n\ntrain_aug = ImageDataGenerator(rotation_range=10,\n                   zoom_range = 0.1,\n                   rescale=1 \/ 255.0,\n                   fill_mode=\"nearest\")\nval_aug = ImageDataGenerator(rescale=1\/255.0)\n\niap = ImageToArrayPreprocesor()\n\ntrain_gen = HDF5DatasetGenerator(TRAIN_HDF5,\n                                 BATCH_SIZE,\n                                 aug=train_aug,\n                                 preprocessors=[iap],\n                                 classes=NUM_CLASSES)\nval_gen = HDF5DatasetGenerator(VAL_HDF5,\n                                 BATCH_SIZE,\n                                 aug=val_aug,\n                                 preprocessors = [iap],\n                                 classes=NUM_CLASSES)\n\nopt = Adam(lr = 1e-3)\nmodel = MiniVGG13Net.build(width=48,height=48,channel=1,classes=NUM_CLASSES)\nmodel.compile(loss=\"categorical_crossentropy\",optimizer=opt,metrics=[\"accuracy\"])\njson_path = os.path.sep.join([OUTPUT_PATH, \"MiniVGG13.json\"])\nfig_path = os.path.sep.join([OUTPUT_PATH, \"{}.png\".format(os.getpid())])\ncallbacks = [TrainingMonitor(fig_path=fig_path,json_path=json_path)]\nmodel.fit_generator(train_gen.generator(),\n                    steps_per_epoch=train_gen.numImages\/\/BATCH_SIZE,\n                    validation_data=val_gen.generator(),\n                    validation_steps=val_gen.numImages \/\/ BATCH_SIZE,\n                    epochs=50,\n                    max_queue_size=BATCH_SIZE*2,\n                    callbacks=callbacks,\n                    verbose=1)\nprint(\"[\u4fe1\u606f] \u4fdd\u5b58\u6a21\u578b...\")\nmodel.save(MODEL_FILE,overwrite=True)\ntrain_gen.close()\nval_gen.close()\n","082b0b2c":"# from config import setting\n#from utils.HDF5DatasetGenerator import HDF5DatasetGenerator\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import load_model\n\ntestAug = ImageDataGenerator(rescale=1\/255.0)\niap = ImageToArrayPreprocesor()\ntestGen = HDF5DatasetGenerator(TEST_HDF5,\n                               BATCH_SIZE,\n                               aug=testAug,\n                               preprocessors=[iap],\n                               classes=NUM_CLASSES)\nprint(\"[\u4fe1\u606f] \u52a0\u8f7d\u7f51\u7edc\u6a21\u578b...\")\nmodel = load_model(MODEL_FILE)\n\n# \u8bc4\u4f30\n(loss, acc) = model.evaluate_generator(testGen.generator(),\n                                     steps=testGen.numImages\/\/BATCH_SIZE,\n                                     max_queue_size=BATCH_SIZE*2)\nprint(\"[\u4fe1\u606f] \u6d4b\u8bd5\u96c6\u51c6\u786e\u7387\uff1a{:.2f}%\".format(acc*100))\ntestGen.close()\n\n","3e92e437":"import cv2\nimport imutils\nimport numpy as np\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing.image import img_to_array\n\nface_detector = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\nmodel = load_model(\".\/model.h5\")\n\nEMOTIONS = ['Angry', 'Disgust', 'Scared', 'Harry', 'Sad', 'Surprise', 'Neutral']\n\n# \u5f00\u542f\u6444\u50cf\u5934\ncapture = cv2.VideoCapture(0)\n\n# \u6301\u7eed\u91c7\u96c6\u6444\u50cf\u5934\u56fe\u50cf\u5e27\nwhile True:\n    ret, frame = capture.read()\n    # \u5e27\u56fe\u50cf\u7f29\u5c0f\u5e76\u7070\u5ea6\u5316\n    #print(frame)\n    frame = imutils.resize(frame,width=300)\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n    # \u753b\u5e03\n    canvas = np.zeros((240,300,3),dtype=\"uint8\")\n\n    frameClone = frame.copy()\n\n    # \u68c0\u6d4b\u4eba\u8138\n    rects = face_detector.detectMultiScale(\n        gray,scaleFactor=1.1,\n        minNeighbors=5,\n        minSize=(30,30),\n        flags=cv2.CASCADE_SCALE_IMAGE)\n\n    if len(rects) > 0:\n        # \u5bf9\u68c0\u6d4b\u5230\u7684\u591a\u4e2a\u4eba\u8138\u6846\u964d\u5e8f\u6392\u5e8f\n        rect = sorted(rects, reverse=True,\n                      key=lambda x:(x[2]-x[0])*(x[3]-x[1]))[0]\n        (fX, fY, fW, fH) = rect\n        roi =  gray[fY:fY+fH,fX:fX+fW]\n        roi = cv2.resize(roi,(48,48))\n        roi = roi.astype(\"float\") \/ 255.0\n        roi = img_to_array(roi)\n        roi = np.expand_dims(roi,axis=0)\n\n        predicts = model.predict(roi)[0]\n        label = EMOTIONS[predicts.argmax()]\n        cv2.putText(frameClone,label,(fX,fY-10),\n                    cv2.FONT_HERSHEY_SIMPLEX,0.5,(0, 0, 255),1, cv2.LINE_AA)\n        cv2.rectangle(frameClone,(fX,fY),(fX+fW,fY+fH),\n                      (0,0,255),1,cv2.LINE_AA)\n        for (i,(emotion,prob)) in enumerate(zip(EMOTIONS,predicts)):\n            text = \"{}: {:.2f}%\".format(emotion,prob*100)\n            w = int (prob*300)\n            cv2.rectangle(canvas,(5,(i*32)+5),(5+w,(i*32)+32),(0,0,255),-1)\n            cv2.putText(\n                canvas,\n                text,\n                (10,(i*32)+23),\n                cv2.FONT_HERSHEY_SIMPLEX,\n                0.5,\n                (255,255,255),\n                1,\n                cv2.LINE_AA\n            )\n    cv2.imshow('Emotion Detection', frameClone)\n    cv2.imshow(\"Result\",canvas)\n\n    if cv2.waitKey(1) == 27:\n        break\n\ncapture.release()\ncv2.destroyAllWindows()\n","2a36ab26":"## HDF5DatasetGenerator","fd770b8d":"## TrainingMonitor","0c03befa":"## build_hdf5","a0c25ac9":"## evaluate\n","ac848275":"## ImageToArrayPreprocessor","c40064de":"## setting","180eb884":"## HDF5DatasetWriter","dc9ed2a6":"## mini_vgg_13","b166f3c0":"## EpochCheckpoint","8f9b0bb5":"## training"}}