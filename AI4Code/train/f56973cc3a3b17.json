{"cell_type":{"14f67b2d":"code","29c27d83":"code","0aa76e09":"code","627fab14":"code","0067ceb9":"code","63e6c4d8":"code","c0747ef4":"code","3c1632d4":"code","169aecc8":"code","256e2f61":"code","76c66def":"code","fa5c2f5b":"code","4f9500a2":"code","edc2fd14":"code","cb113109":"code","8071178f":"code","bebfbfd7":"code","2b0f1bf9":"code","6b783d18":"code","5748d439":"code","77f09be3":"code","24cfabc5":"code","175d56f6":"code","857f31c1":"code","a8f1d68e":"code","e4d86741":"code","948ab483":"code","a5079365":"code","6bd6016e":"code","79eaef6b":"code","51de7765":"code","c87187b9":"code","73d1c7db":"code","c6dedb9d":"code","6957fa55":"code","f941e165":"code","282f877e":"code","c55a8290":"code","4fbfd5bf":"code","f85b3342":"code","81260937":"code","7f62bf73":"code","5e3cad24":"code","39de5b9c":"code","1b87e6ad":"code","242ab8ae":"code","2f46a78f":"code","673a92f6":"code","a31139fd":"code","fb4eac39":"code","b1054c8a":"code","0490ecef":"code","39d11017":"code","e371b9f5":"code","be4bd800":"code","a89f685d":"code","1d04e96e":"code","283b07ca":"code","a999569c":"code","b9bbe9a1":"code","7792eed8":"code","ac5cf938":"code","751bfb29":"code","afd8f681":"code","d32f0598":"code","cc4c66c3":"code","d5b68c9e":"code","e4add287":"code","9d988cab":"code","b478e26b":"code","b052efe9":"code","9638562c":"code","6ad5b254":"code","9454e6b9":"code","f0afda2f":"code","c10b25f2":"code","a80c5397":"code","1ccf5ca5":"code","e3ba240f":"code","e453f0e9":"code","1388180e":"code","09aaf5ea":"code","5f6c3528":"code","fe8abd0c":"code","2228df54":"code","f7dee773":"code","0b82972b":"code","f1b459ff":"code","236a0939":"code","3afce4ef":"code","97fcf05e":"code","21872219":"code","e5bee01d":"code","9365951b":"code","01828d3a":"code","d4906db8":"code","568687a6":"code","37cc0111":"code","184c6457":"code","d6ac11eb":"code","a99ab3ec":"code","2c66ea29":"code","a8dbb0f3":"code","e4db64cc":"code","7da20eee":"markdown","d87cc65b":"markdown","1868cd50":"markdown","73a9e6fd":"markdown","ad7ac3ee":"markdown","dee779d8":"markdown","9214cb13":"markdown","2f58128a":"markdown","45de7d8f":"markdown","5a115f95":"markdown","5f24f0ef":"markdown","72524443":"markdown","256da470":"markdown","3fbebc08":"markdown","15a69dda":"markdown","676563cc":"markdown","b7493c74":"markdown","2dd41673":"markdown","8d5c9a18":"markdown","83813b2b":"markdown","eab695d6":"markdown","aea80478":"markdown","da00f23c":"markdown","131e9a6c":"markdown","ba2a71ac":"markdown","1692ccd4":"markdown","3c448e57":"markdown","1cb79d6e":"markdown","3efb65f5":"markdown","6a1d8e44":"markdown","753ac3f1":"markdown","7f76e5c1":"markdown","48e078dd":"markdown","498d725f":"markdown","07da0b21":"markdown","b580a6f3":"markdown","c0d81a85":"markdown","b7e4344e":"markdown","2e0ed5ae":"markdown","baaa1a88":"markdown","ad99774e":"markdown","1b1b0310":"markdown","4fa64ebe":"markdown","fb29c291":"markdown","c06ee0a4":"markdown","ea9e300c":"markdown","47750e25":"markdown","566910dc":"markdown","4086ad88":"markdown","85a1fbee":"markdown","cc7e0513":"markdown","ffcff479":"markdown","ab1f366b":"markdown","f3786a89":"markdown","bd138cdd":"markdown","22af53ee":"markdown","8bf3216b":"markdown","720e02eb":"markdown","1ae0c1b3":"markdown","dc55dfc4":"markdown","185199e8":"markdown"},"source":{"14f67b2d":"# Rossmann Sales prediction Using Timer Series Method","29c27d83":"# import required libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nget_ipython().run_line_magic('matplotlib', 'inline')\nimport xgboost as xgb\nfrom time import time\nimport os\n# statistics\nfrom statsmodels.distributions.empirical_distribution import ECDF\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom fbprophet import Prophet\nimport warnings\nimport itertools\nimport statsmodels.api as sm\nfrom statsmodels.tsa.stattools import acf,pacf\nfrom statsmodels.tsa.arima_model import  ARIMA\nfrom pandas import DataFrame\n","0aa76e09":"#Train data importing\ntrain = pd.read_csv('..\/input\/sales-data\/train.csv')\ntrain.head()","627fab14":"#Shape of the dataset\ntrain.shape","0067ceb9":"#train dataset basic information\ntrain.info()","63e6c4d8":"# Train Dataset statistics\ntrain.describe()","c0747ef4":"#Store Data Importing\nstore=pd.read_csv('..\/input\/sales-data\/store.csv')\nstore.head()","3c1632d4":"# Shape Check\nstore.shape","169aecc8":"# Basic information of the store\nstore.info()","256e2f61":"#Staistics\nstore.describe()","76c66def":"# Check missing values\nstore.isnull().sum()","fa5c2f5b":"#Checking missing data at CompetitionDistance\nstore['CompetitionDistance'].fillna(store['CompetitionDistance'].median(), inplace = True)","4f9500a2":"# Missing nan value replacing with '0\nstore.fillna(0, inplace = True)\nstore.head()","edc2fd14":"# Checking basic information\nstore.info()","cb113109":"# Checking Unique values\nstore.PromoInterval.nunique()","8071178f":"# Checking value count\nstore.PromoInterval.value_counts()","bebfbfd7":"# Checking missing values in train data set\ntrain.isnull().sum()","2b0f1bf9":"# Date checking\ntrain.Date = pd.to_datetime(train.Date)","6b783d18":"# Train dataset\ntrain.head()","5748d439":"# Train set date, month, year checking\ntrain['year'] = pd.DatetimeIndex(train['Date']).year\ntrain['month'] = pd.DatetimeIndex(train['Date']).month","77f09be3":"# Train dataset\ntrain.head()","24cfabc5":"#barplot month Vs sales\nsns.barplot(x = 'month', y = 'Sales', data = train)\nplt.show()","175d56f6":"#barplot year Vs sales\nsns.barplot(x = 'year', y = 'Sales', data = train)\nplt.show()","857f31c1":"#Checking Train Data\ntrain.info()","a8f1d68e":"# Merging train and store data\ntrain_store = pd.merge(train, store, how = 'inner', on = 'Store')\n\nprint(\"In total: \", train_store.shape)\ntrain_store.head(10)","e4d86741":"# new dataset \ndf_merge = train_store.copy()\ndf_merge.info()","948ab483":"# Checking unique after merge\ntrain_store.Store.nunique()","a5079365":"# Storetype vs Sales\ntrain_store.groupby('StoreType')['Sales'].describe()","6bd6016e":"#Statistics Storetype vs customers & sales\ntrain_store.groupby('StoreType')['Customers', 'Sales'].describe()","79eaef6b":"#Statistics Storetype vs customers\ntrain_store.groupby('StoreType')['Customers'].describe()","51de7765":"# Train_store\ntrain_store.head()","c87187b9":"#Catplot month Vs Sales\nsns.catplot(data = train_store, x = 'month', y = \"Sales\", \n               col = 'StoreType', # per store type in cols\n               palette = 'plasma',\n               hue = 'StoreType',\n               row = 'Promo', # per promo in the store in rows\n               color = 'year') ","73d1c7db":"#Factorplot month Vs Customers\nsns.factorplot(data = train_store, x = 'month', y = \"Customers\", \n               col = 'StoreType', # per store type in cols\n               palette = 'plasma',\n               hue = 'StoreType',\n               row = 'Promo', # per promo in the store in rows\n               ) ","c6dedb9d":"#Catplot year Vs Sales\nsns.catplot(data = train_store, x = 'year', y = \"Sales\", \n               col = 'StoreType', # per store type in cols\n               palette = 'plasma',\n               hue = 'StoreType',\n               row = 'Promo', # per promo in the store in rows\n               color = 'year') ","6957fa55":"# Factor plot month Vs Sales\nsns.factorplot(data = train_store, x = 'month', y = \"Sales\", \n               col = 'DayOfWeek', # per store type in cols\n               palette = 'plasma',\n               hue = 'StoreType',\n               row = 'StoreType', # per store type in rows\n              ) \n","f941e165":"#Statistics\ntrain_store.groupby('DayOfWeek')['Sales'].describe()","282f877e":"#Unique check\ntrain_store[(train_store.Open == 1) & (train_store.DayOfWeek == 7)]['Store'].unique()","c55a8290":"#Nunique check\ntrain_store[(train_store.Open == 1) & (train_store.DayOfWeek == 7)]['Store'].nunique()","4fbfd5bf":"# Value count Check\ntrain_store[(train_store.Open == 1) & (train_store.DayOfWeek == 7)]['StoreType'].value_counts()","f85b3342":"# Compute the correlation matrix \n# exclude 'Open' variable\ncorr_all = train_store.drop('Open', axis = 1).corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr_all, dtype = np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize = (11, 9))\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr_all, mask = mask,\n            square = True, linewidths = .5, ax = ax, cmap = \"BuPu\")      \nplt.show()","81260937":"#plot Dayofweek Vs Sales\nsns.factorplot(data = train_store, x = 'DayOfWeek', y = \"Sales\", \n               col = 'Promo', \n               row = 'Promo2',\n               hue = 'StoreType',\n               palette = 'RdPu') ","7f62bf73":"# Dataset\ntrain_store.head()","5e3cad24":"#plot Sales Vs Date\ntrain_store.plot.line(x = 'Date', y = 'Sales')\nplt.show()","39de5b9c":"# Sales at dataframe\nsales = train_store[['Sales']]","1b87e6ad":"#Plot for sales\nsales.rolling(6).mean().plot(figsize=(20,10), linewidth=5, fontsize=20)\nplt.show()","242ab8ae":"# Basic information\ntrain_store.info()","2f46a78f":"#Merge the training dataset with the Store dataset and spliting dataset into train and test datasets\n\ntrain_store['Sales\/Customer'] = train_store['Sales']\/train_store['Customers']\ntrain_store['Sales\/Customer'].describe()","673a92f6":"# loop for features creation\ndef features_create(data):\n    \n    \n    mappings = {'0':0, 'a':1, 'b':2, 'c':3, 'd':4}\n    data.StoreType.replace(mappings, inplace=True)\n    data.Assortment.replace(mappings, inplace=True)\n    data.StateHoliday.replace(mappings, inplace=True)\n    \n    \n    data['Year'] = data.Date.dt.year\n    data['Month'] = data.Date.dt.month\n    data['Day'] = data.Date.dt.day\n    data['DayOfWeek'] = data.Date.dt.dayofweek\n    data['WeekOfYear'] = data.Date.dt.weekofyear\n    \n    \n    \n    data['CompetitionOpen'] = 12 * (data.Year - data.CompetitionOpenSinceYear) +         (data.Month - data.CompetitionOpenSinceMonth)\n    data['PromoOpen'] = 12 * (data.Year - data.Promo2SinceYear) +         (data.WeekOfYear - data.Promo2SinceWeek) \/ 4.0\n    data['CompetitionOpen'] = data.CompetitionOpen.apply(lambda x: x if x > 0 else 0)        \n    data['PromoOpen'] = data.PromoOpen.apply(lambda x: x if x > 0 else 0)\n    \n    \n  \n    month2str = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun', 7:'Jul', 8:'Aug', 9:'Sept', 10:'Oct', 11:'Nov', 12:'Dec'}\n    data['monthStr'] = data.Month.map(month2str)\n    data.loc[data.PromoInterval == 0, 'PromoInterval'] = ''\n    data['IsPromoMonth'] = 0\n    for interval in data.PromoInterval.unique():\n        if interval != '':\n            for month in interval.split(','):\n                data.loc[(data.monthStr == month) & (data.PromoInterval == interval), 'IsPromoMonth'] = 1\n\n    return data","a31139fd":"# Library for spliting the dataset\nfrom sklearn.model_selection import train_test_split","fb4eac39":"# We specify this so that the train and test data set always have the same rows, respective\ndf_train_store, df_test_store = train_test_split(train_store, train_size = 0.7, test_size = 0.3, random_state = 100)","b1054c8a":"# features\nfeatures_create(df_train_store)\nfeatures_create(train_store)\nfeatures_create(df_test_store)\nprint('Features creation finished')\n# train.head()","0490ecef":"# Heatmap\nplt.figure(figsize = (20, 10))\n\nsns.heatmap(df_train_store.corr(), annot = True, vmin = -1, vmax = 1, fmt = '.3f')","39d11017":"# Sorting with 'date'\ntrain_df = train_store.set_index('Date')","e371b9f5":"# Sales datacheck\ntrain_df['Sales'] = train_df['Sales'] * 1.0","be4bd800":"# storewise sales data\nsales_a = train_df[train_df.Store == 2]['Sales']\nsales_b = train_df[train_df.Store == 85]['Sales'].sort_index(ascending = True) # solve the reverse order\nsales_c = train_df[train_df.Store == 1]['Sales']\nsales_d = train_df[train_df.Store == 13]['Sales']","a89f685d":"f, (ax1, ax2, ax3, ax4) = plt.subplots(4, figsize = (12, 13))\n\n# store types\nsales_a.resample('W').mean().plot(ax = ax1)\nsales_b.resample('W').sum().plot(ax = ax2)\nsales_c.resample('W').sum().plot(ax = ax3)\nsales_d.resample('W').sum().plot(ax = ax4)","1d04e96e":"# Seasonal decompose\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\nf, (ax1, ax2, ax3, ax4) = plt.subplots(4, figsize = (15, 15))\n\n# monthly\ndecomposition_a = seasonal_decompose(sales_a, model = 'additive', extrapolate_trend='freq', period=1)\ndecomposition_a.trend.plot(ax = ax1)\n\ndecomposition_b = seasonal_decompose(sales_b, model = 'additive', extrapolate_trend='freq', period=1)\ndecomposition_b.trend.plot( ax = ax2)\n\ndecomposition_c = seasonal_decompose(sales_c, model = 'additive', extrapolate_trend='freq', period=1)\ndecomposition_c.trend.plot( ax = ax3)\n\ndecomposition_d = seasonal_decompose(sales_d, model = 'additive', extrapolate_trend='freq', period=1)\ndecomposition_d.trend.plot( ax = ax4)","283b07ca":"# Adfuller creation\nfrom statsmodels.tsa.stattools import adfuller\ndef stationary_check(sales):\n  result = adfuller(sales,autolag='AIC')\n  print('ADF Statistic: %f' % result[0])\n  print('p-value: %f' % result[1])\n  print('Critical Values:')\n  for key, value in result[4].items():\n\t   print(key, value)","a999569c":"#stationarity Check\nprint('Stationary Check at a:', stationary_check(sales_a))\nprint('       ')\nprint('Stationary Check at b:',stationary_check(sales_b))\nprint('       ')\nprint('Stationary Check at c:',stationary_check(sales_c))\nprint('       ')\nprint('Stationary Check at d:',stationary_check(sales_d))","b9bbe9a1":"# Plot for timeseries\ndef plot_timeseries(sales,StoreType):\n\n    fig, axes = plt.subplots(3, 1, sharex=True, sharey=False)\n    fig.set_figheight(10)\n    fig.set_figwidth(15)\n\n    decomposition= seasonal_decompose(sales, model = 'additive',freq=360)\n\n    estimated_trend = decomposition.trend\n    estimated_seasonal = decomposition.seasonal\n    estimated_residual = decomposition.resid\n\n\n\n    axes[0].plot(estimated_trend, label='Trend')\n    axes[0].legend(loc='upper left');\n\n    axes[1].plot(estimated_seasonal, 'g', label='Seasonality')\n    axes[1].legend(loc='upper left');\n\n    axes[2].plot(estimated_residual, 'r', label='Residuals')\n    axes[2].legend(loc='upper left')\n\n    plt.title('Decomposition for StoreType')","7792eed8":"# Plot 'a'\nplot_timeseries(sales_a,'a')","ac5cf938":"# Plot 'b'\nplot_timeseries(sales_b,'b')","751bfb29":"# Plot 'c'\nplot_timeseries(sales_c,'c')","afd8f681":"# Plot 'd'\nplot_timeseries(sales_d,'d')","d32f0598":"# Required Package\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf","cc4c66c3":"# figure for subplots\nplt.figure(figsize = (12, 12))\n\n# acf and pacf for A\nplt.subplot(421); plot_acf(sales_a, lags = 50, ax = plt.gca())\nplt.subplot(422); plot_pacf(sales_a, lags = 50, ax = plt.gca() )\n\n# acf and pacf for B\nplt.subplot(423); plot_acf(sales_b, lags = 50, ax = plt.gca() )\nplt.subplot(424); plot_pacf(sales_b, lags = 50, ax = plt.gca() )\n\n# acf and pacf for C\nplt.subplot(425); plot_acf(sales_c, lags = 50, ax = plt.gca() )\nplt.subplot(426); plot_pacf(sales_c, lags = 50, ax = plt.gca() )\n\n# acf and pacf for D\nplt.subplot(427); plot_acf(sales_d, lags = 50, ax = plt.gca() )\nplt.subplot(428); plot_pacf(sales_d, lags = 50, ax = plt.gca() )\n\nplt.show()","d5b68c9e":"# plot for sales\npd.plotting.autocorrelation_plot(train_store.head(10000)['Sales'])\nplt.show()","e4add287":"\"\"\"\n    Johansen cointegration test of the cointegration rank of a VECM\n\n    Parameters\n    ----------\n    endog : array_like (nobs_tot x neqs)\n        Data to test\n    det_order : int\n        * -1 - no deterministic terms - model1\n        * 0 - constant term - model3\n        * 1 - linear trend\n    k_ar_diff : int, nonnegative\n        Number of lagged differences in the model.\n\"\"\"\nfrom statsmodels.tsa.vector_ar.vecm import coint_johansen\ndef joh_output(res):\n    output = pd.DataFrame([res.lr2,res.lr1],\n                          index=['max_eig_stat',\"trace_stat\"])\n    print(output.T,'\\n')\n    print(\"Critical values(90%, 95%, 99%) of max_eig_stat\\n\",res.cvm,'\\n')\n    print(\"Critical values(90%, 95%, 99%) of trace_stat\\n\",res.cvt,'\\n')\n\njoh_model1 = coint_johansen(train_store[['Sales','Customers','Promo']],-1,1) # k_ar_diff +1 = K\njoh_output(joh_model1)","9d988cab":"# Packages:\nimport itertools\n\n# Define the p, d and q parameters to take any value between 0 and 2\np = d = q = range(0, 2)\n\n# Generate all different combinations of p, q and q triplets\npdq = list(itertools.product(p, d, q))\n\n# Generate all different combinations of seasonal p, q and q triplets\nseasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n\nprint('Examples of parameter combinations for Seasonal ARIMA...')\nprint('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\nprint('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\nprint('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))\nprint('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))","b478e26b":"# loop for resample\ndef sample(sales):\n  return(sales.resample('MS').mean())","b052efe9":"# storewise sales\ny_sale_a = sample(sales_a)\ny_sale_b = sample(sales_b)\ny_sale_c = sample(sales_c)\ny_sale_d = sample(sales_d)","9638562c":"#Exoginious loop\ndef model(i,sale):\n  ex = train_df.loc[train_df['Store'] == i,['Customers','Promo']]\n#   ex = train2.loc[train2['Store'] == i]\n#   ex.drop(['Open','PromoInterval','monthStr','Customers'],axis=1,inplace =True)\n  y_sales = sale.resample('MS').mean()\n  y_ex = ex.resample('MS').mean()\n  exog = sm.add_constant(y_ex)\n  exog.tail()\n  return y_sales , exog","6ad5b254":"#storewise exoginious sales data\ny_sale_model_a , exog_a = model(2,sales_a)\ny_sale_model_b , exog_b = model(85,sales_b)\ny_sale_model_c , exog_c = model(1,sales_c)\ny_sale_model_d , exog_d = model(13,sales_d)","9454e6b9":"#exoginious data\nexog_a = exog_a.dropna()\nexog_a.isna().sum()\n\nexog_b = exog_b.dropna()\nexog_b.isna().sum()\n\nexog_c = exog_c.dropna()\nexog_c.isna().sum()\n\nexog_d = exog_d.dropna()\nexog_d.isna().sum()","f0afda2f":"# Arima loop\ndef arima(y_sale,exog,or1,sord):\n    mod = sm.tsa.statespace.SARIMAX(y_sale,\n                                    exog,\n                                    order=or1,\n                                    seasonal_order=sord,\n                                    enforce_stationarity=False,\n                                    enforce_invertibility=False)\n\n\n\n    results = mod.fit(disp=0)\n    ## -----------------------predictions-------------------\n    pred = results.get_prediction(start=pd.to_datetime('2015-01-01'), end=pd.to_datetime('2015-07-01'), dynamic=False)\n    pred_ci = pred.conf_int()\n\n   ##------------------------plot-----------------------------\n    ax = y_sale['2013':].plot(label='observed')\n    pred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7)\n\n    ax.fill_between(pred_ci.index,\n                      pred_ci.iloc[:, 1],\n                      pred_ci.iloc[:, 1], color='k', alpha=.2)\n\n    ax.set_xlabel('Date')\n    ax.set_ylabel('sales')\n    plt.legend()\n    plt.title('Impulse Response')\n\n    plt.show()\n\n\n    y_forecasted = pred.predicted_mean\n    y_truth = y_sale['2015-01-01':'2015-07-01']\n\n   ##-------------Compute the mean square error----------------\n    mse = ((y_forecasted - y_truth) ** 2).mean()\n    rmse = np.sqrt(((y_forecasted - y_truth) ** 2).mean())\n    rmspe =  np.sqrt(np.mean((y_forecasted\/y_truth) ** 2))\n    return results , mse,rmse,rmspe","c10b25f2":"#Prediction for Sales Store Type A\nor_a = (5,1,3)\nsord_a=(1, 0, 0, 12)\nresult_a,mse_a,rmse_a,rmspe_a= arima(y_sale_model_a,exog_a,or_a,sord_a)\nprint(result_a.summary())\nprint('The Mean Squared Error of our forecasts is {}'.format(round(mse_a, 2)))\nprint('The Root Squared Error of our forecasts is {}'.format(round(rmse_a, 2)))\nprint('The Root Squared Per Error of our forecasts is {}'.format(round(rmspe_a, 2)))","a80c5397":"#Prediction for Sales Store Type B\nor_b = (3,2,3)\nsord_b=(1, 0, 0, 12)\nresult_b,mse_b,rmse_b,rmspe_b= arima(y_sale_model_b,exog_b,or_b,sord_b)\nprint(result_b.summary())\nprint('The Mean Squared Error of our forecasts is {}'.format(round(mse_b, 2)))\nprint('The Root Squared Error of our forecasts is {}'.format(round(rmse_b, 2)))\nprint('The Root Squared Per Error of our forecasts is {}'.format(round(rmspe_b, 2)))","1ccf5ca5":"#Prediction for Sales Store Type C\nor_c = (2,1,2)\nsord_c=(1, 0, 0, 12)\nresult_c,mse_c,rmse_c,rmspe_c= arima(y_sale_model_c,exog_c,or_c,sord_c)\nprint(result_c.summary())\nprint('The Mean Squared Error of our forecasts is {}'.format(round(mse_c, 2)))\nprint('The Root Squared Error of our forecasts is {}'.format(round(rmse_c, 2)))\nprint('The Root Squared Per Error of our forecasts is {}'.format(round(rmspe_c, 2)))","e3ba240f":"# creating Xgboost datset\ntrain_xg = df_train_store.sort_values(['Date'],ascending = False)\nho_test = df_train_store[:6*7*1115]\nho_train = df_train_store[6*7*1115:]","e453f0e9":"# We will consider only data of Sales>0 and Open is 1\nho_test = ho_test[ho_test[\"Open\"] != 0]\nho_test = ho_test[ho_test[\"Sales\"] > 0]\nho_train = ho_train[ho_train[\"Open\"] != 0]\nho_train = ho_train[ho_train[\"Sales\"] > 0]","1388180e":"# Dropping features\nho_train.drop(['Date','Open','PromoInterval','monthStr'],axis=1,inplace =True)\nho_test.drop(['Date','Open','PromoInterval','monthStr'],axis=1,inplace =True)\nxtest =df_test_store.drop(['Date','Open','PromoInterval','monthStr'],axis = 1)","09aaf5ea":"# Label data\nho_xtrain = ho_train.drop(['Sales'],axis=1 )\nho_ytrain = np.log1p(ho_train.Sales)\nho_xtest = ho_test.drop(['Sales'],axis=1 )\nho_ytest = np.log1p(ho_test.Sales)","5f6c3528":"# Function for RMSE & RMPSE\ndef rmspe(y, yhat):\n    return np.sqrt(np.mean((yhat\/y-1) ** 2))\n\ndef rmspe_xg(yhat, y):\n    y = np.expm1(y.get_label())\n    yhat = np.expm1(yhat)\n    return \"rmspe\", rmspe(y,yhat)","fe8abd0c":"# Evalution\ndtrain = xgb.DMatrix(ho_xtrain, ho_ytrain)\ndvalid = xgb.DMatrix(ho_xtest, ho_ytest)\nwatchlist = [(dtrain, 'train'), (dvalid, 'eval')]","2228df54":"# Iterations\n\nparams = {\"objective\": \"reg:linear\", #since it is a regression problem\n          \"booster\" : \"gbtree\",     #tree\n          \"eta\": 0.03,              #learning rate   to reduce overfitting issues\n          \"max_depth\": 10,          #depth of the tree\n          \"subsample\": 0.9,         #subsample the data prior to growing trees - overcomes overfitting\n          \"colsample_bytree\": 0.7,  #subsampling of columns for each tree\n          \"silent\": 1,              #verbosity\n          \"seed\": 10                \n          }\nnum_boost_round = 200           #countinue for 200 itertions\n\n\nprint(\"Train a XGBoost model\")\nstart = time()\ngbm = xgb.train(params, dtrain, num_boost_round, evals=watchlist, \n  early_stopping_rounds=100, feval=rmspe_xg, verbose_eval=True)\nend = time()\nprint('Training time is {:2f} s.'.format(end-start))\n\n\nprint(\"validating\")\nho_xtest.sort_index(inplace=True) \nho_ytest.sort_index(inplace=True) \nyhat = gbm.predict(xgb.DMatrix(ho_xtest))\nerror = rmspe(np.expm1(ho_ytest), np.expm1(yhat))\n\nprint('RMSPE: {:.6f}'.format(error))","f7dee773":"# RNSPE\nprint('RMSPE: {:.6f}'.format(error))","0b82972b":"# Prediction\ndf_predict = pd.DataFrame(yhat)\ndf_predict.head()","f1b459ff":"# analysis by hold-out set\nres = pd.DataFrame(data = ho_ytest)\nres['Prediction']=yhat\nres = pd.merge(ho_xtest,res, left_index= True, right_index=True)\nres['Ratio'] = res.Prediction\/res.Sales\nres['Error'] =abs(res.Ratio-1)\nres['Weight'] = res.Sales\/res.Prediction\nres.head()","236a0939":"# Worst 10 sample\nres.sort_values(['Error'],ascending=False,inplace= True)\nres[:10]","3afce4ef":"# whole correction\nprint(\"weight correction\")\nW=[(0.990+(i\/1000)) for i in range(20)]\nS =[]\nfor w in W:\n    error = rmspe(np.expm1(ho_ytest), np.expm1(yhat*w))\n    print('RMSPE for {:.3f}:{:.6f}'.format(w,error))\n    S.append(error)\nScore = pd.Series(S,index=W)\nScore.plot()\nBS = Score[Score.values == Score.values.min()]\nprint ('Best weight for Score:{}'.format(BS))","97fcf05e":"# Sales Vs Prediction\ncol_1 = ['Sales','Prediction']\ncol_2 = ['Ratio']","21872219":"# correction by store\nL=range(1115)\nW_ho=[]\nW_test=[]\nfor i in L:\n    s1 = pd.DataFrame(res[res['Store']==i+1],columns = col_1)\n    s2 = pd.DataFrame(xtest[xtest['Store']==i+1])\n    W1=[(0.990+(i\/1000)) for i in range(20)]\n    S =[]\n    for w in W1:\n        error = rmspe(np.expm1(s1.Sales), np.expm1(s1.Prediction*w))\n        S.append(error)\n    Score = pd.Series(S,index=W1)\n    BS = Score[Score.values == Score.values.min()]\n    a=np.array(BS.index.values)\n    b_ho=a.repeat(len(s1))\n    b_test=a.repeat(len(s2))\n    W_ho.extend(b_ho.tolist())\n    W_test.extend(b_test.tolist())","e5bee01d":"# weight Corretion\nyhat_new = yhat*W_ho\nerror = rmspe(np.expm1(ho_ytest), np.expm1(yhat_new))\nprint ('RMSPE for weight corretion {:6f}'.format(error))","9365951b":"# Prophet Library\nfrom fbprophet import Prophet","01828d3a":"# Sales data \nsales = train_store.rename(columns = {'Date': 'ds',\n                                'Sales': 'y'})\nsales.head()","d4906db8":"# Data set\nsales=sales[['ds','y']]\nsales.head()","568687a6":"# Plot\nax = sales.set_index('ds').plot(figsize = (12, 4))\nax.set_ylabel('Daily Number of Sales')\nax.set_xlabel('Date')\nplt.show()","37cc0111":"# Dataset\ntrain_store.head()","184c6457":"# Newdata set\ndf= train_store","d6ac11eb":"# create holidays dataframe\nstate_dates = df[(df.StateHoliday == 'a') | (df.StateHoliday == 'b') & (df.StateHoliday == 'c')].loc[:, 'Date'].values\nschool_dates = df[df.SchoolHoliday == 1].loc[:, 'Date'].values\n\nstate = pd.DataFrame({'holiday': 'state_holiday',\n                      'ds': pd.to_datetime(state_dates)})\nschool = pd.DataFrame({'holiday': 'school_holiday',\n                      'ds': pd.to_datetime(school_dates)})\n\nholidays = pd.concat((state, school))      \nholidays","a99ab3ec":"# Prophet implementation \nmy_model = Prophet(interval_width = 0.95, \n                   holidays = holidays.head(50000))\nmy_model.fit(sales)\n\n# dataframe that extends into future 6 weeks \nfuture_dates = my_model.make_future_dataframe(periods = 6*7)\n\nprint(\"First week to forecast.\")\nfuture_dates.tail(7)","2c66ea29":"# forecast\nforecast = my_model.predict(future_dates.head(10000))\n\n# preditions for last week\nforecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(7)","a8dbb0f3":"# visualizing predicions\nmy_model.plot(forecast);","e4db64cc":"my_model.plot_components(forecast);","7da20eee":"Create the training and testing dataset; We will drop the features of not help","d87cc65b":"Create Features and Label data for Test and Train","1868cd50":"#### Prediction for Sales Store Type A","73a9e6fd":"#### Month Vs Customers","ad7ac3ee":"To complete our preliminary data analysis, we can add variables describing the period of time during which competition and promotion were opened:","dee779d8":"#### Month Vs Sales at Dayofweek","9214cb13":"Retail sales for `StoreType` A and C tend to peak for the Christmas season and then decline after the holidays. We might have seen the same trend for `StoreType` D (at the bottom) but there is no information from July 2014 to January 2015 about these stores as they were closed.","2f58128a":"### Stationarity Test - ADF Test\n\n**We take four stores from store types to represent their group**:\n- Store number 2 for `StoreType` A\n- Store number 25 for `StoreType` B, \n- Store number 1 for `StoreType` C \n- Store number 13 for `StoreType` D. \n\nIt also makes sense to downsample the data from days to weeks using the `resample` method to see the present trends more clearly.","45de7d8f":"XGBoost model uses boosting technique to boost the weak learners and updates the weights to improve the model predictions.\n\nLets do some correction of the weights to improve our RMPSE score.\n\nIt can be seen that weight 0.995 gives 0.112 RMSe","5a115f95":"Aha! Eventhough the plots above showed `StoreType` B as the most selling and performant one, in reality it is not true. The highest `SalePerCustomer` amount is observed at the `StoreType` D, about 12\u20ac with `Promo` and 10\u20ac without. As for `StoreType` A and C it is about 9\u20ac. \n\nLow `SalePerCustomer` amount for `StoreType` B describes its Buyer Cart: there are a lot of people who shop essentially for \"small\" things (or in a little quantity). Plus we saw that overall this `StoreType` generated the least amount of sales and customers over the period.","5f24f0ef":"We are dealing with time series data so it will probably serve us to extract dates for further analysis. We also have two likely correlated vaiables in the dataset, which can be combined into a new feature.","72524443":"We can read these plots horizontally. Each horizontal pair is for one 'StoreType', from A to D. In general, those plots are showing the correlation of the series with itself, lagged by x time units correlation of the series with itself, lagged by x time units.\n\nThere is at two things common for each pair of plots: non randomnes of the time series and high lag-1 (which will probably need a higher order of differencing d\/D).\n\n- Type A and type B:\nBoth types show seasonalities at certain lags. For type A, it is each 12th observation with positives spikes at the 12 (s) and 24(2s) lags and so on. For type B it's a weekly trend with positives spikes at the 7(s), 14(2s), 21(3s) and 28(4s) lags. \n\n\n- Type C and type D:\nPlots of these two types are more complex. It seems like each observation is coorrelated to its adjacent observations. ","256da470":"### EDA\n\nIn this first section we go through the train and store data, handle missing values and create new features for further analysis.","3fbebc08":"### SARIMAX Models","15a69dda":"We can calculate P , Q values for our Arima model from the above graphs:\n\nThe point where our graph touches 0 for ACF graph is **Q ---> 2**\n\nThe point where our graph touches 0 for PACF graph is P ---> 2\n\nSince th above ACF and PACF plots shows that the values of Q and P should be 2,3 respectively , Lets do a grid search and test the AIC for each model to select the best p , d ,q parametes. The model having lowest AIC determines an accurate performance. The AIC measures how well a model fits the data while taking into account the overall complexity of the model. A model that fits the data very well while using lots of features will be assigned a larger AIC score than a model that uses fewer features to achieve the same goodness-of-fit. Therefore, we are interested in finding the model that yields the lowest AIC value.\n\nArima Model (Autoregressive Indegrated Moving Average) determines the forecast of Sales data\n\nWe have to decide the p,d,q for the arima model to fit the data to get the proper prediction:\n\np(Auto Regression) as the name suggests , it determines the effect of previous sales on the current sale.It takes into consideration the effect of previous sale. It is represented by : **Y{t} = c + ph1(1)Y{t-1} + phi{2}Y{t-2} + ...... + phi{p}Y{t-p} + e_{t}**\n\np =1 ,determines that the effect of previous month has been taken. ph1 determines the variable (Sales) for which we are lagging salesvalue by 1. Auto Regression model of order 1 i.e. p=1 or ARIMA(1,0,0) is represented by the following regression equation Y{t} = c + ph1(1)Y_{t-1}\n\nd = determines the diffrence taken between original series and the lagged series to achieve the stationarity. d = 0 , determins that the series was already stationary. d = 1 , determines that the series was subtracted from its lagged series to attain stationary Auto Regression model of order 1 i.e. d=1 or ARIMA(0,1,0) is represented by the following regression equation y(t_new) = y(t) - y(t-1)\n\nq(Moving Average) = extract the influence of the previous period\u2019s error terms on the current period\u2019s error MA model of order 1 i.e. q=1 or ARIMA(0,0,1) is represented by the following regression equation Y_{t} = c + et + theta{1}e_{t-1}\n\n**\u0394yt=c+\u03d51\u0394yt\u22121+\u03b81\u03f5t\u22121+\u03f5t**","676563cc":"#### Correlational Analysis","b7493c74":"As we see Prophet catches the trends and most of the time gets future values right.\n\nOne other particularly strong feature of Prophet is its ability to return the components of our forecasts. This can help reveal how daily, weekly and yearly patterns of the time series plus manyally included holidayes contribute to the overall forecasted values:","2dd41673":"We see that stores of `StoreType` C are all closed on Sundays, whereas others are most of the time opened. Interestingly enough, stores of `StoreType` D are closed on Sundays only from October to December.\n\nBt the way what are the stores which are opened on Sundays?","8d5c9a18":"#### Train DataSet","83813b2b":"All store types follow the same trend but at different scales depending on the presence of the (first) promotion `Promo` and `StoreType` itself (case for B).\n\n__Already at this point, we can see that Sales escalate towards Christmas holidays. But we'll talk about seasonalities and trends later in the Time Series Analysis section.__","eab695d6":"As mentioned before, we have a strong positive correlation between the amount of Sales and Customers of a store. We can also observe a positive correlation between the fact that the store had a running promotion (`Promo` equal to 1) and amount of `Customers`. \n\nHowever, as soon as the store continues a consecutive promotion (`Promo2` equal to 1) the number of `Customers` and `Sales` seems to stay the same or even decrease, which is described by the pale negative correlation on the heatmap. The same negative correlation is observed between the presence of the promotion in the store and the day of a week.","aea80478":"If the test statistic is less than the critical value, we fail to reject the null hypothesis and we say series is stationary\n\nIf Time is Series is not Stationary\n\nBelow are the steps that can be performed to make a timeseries stationary\n\nTake the log of data(Sales) Calcualte Moving Avergae = Log of data - Rolling value of log data Calculate weighted avergate = log of data - exponential of log data\n\nDecomposition Plots :\n\nNow lets plot the decomposition which shows the trend, Seasonality and residuals in sales for each store Type","da00f23c":"#### Year Vs Sales","131e9a6c":"I tried various combination of p,q,d in SARIMAX model and couldnot achieve RMPSE less than 1 , therefore I moved to XGBoost model to improve my model accuracy","ba2a71ac":"#### Sales Vs Month and Year","1692ccd4":"Create a new dataframe having actual , predicted values , ratio , error and weight","3c448e57":"The first plot shows that the monthly sales of store number 1 has been linearly decreasing over time and the second shows the holiays gaps included in the model. The third plot highlights the fact that the weekly volume of last week sales peaks towards the Monday of the next week, while the forth plot shows that the most buzy season occurs during the Christmas holidays.","1cb79d6e":"**There are several things here:**\n- In case of no promotion, both `Promo` and `Promo2` are equal to 0, `Sales` tend to peak on Sunday (!). Though we should note that `StoreType` C doesn't work on Sundays. So it is mainly data from `StoreType` A, B and D.\n- On the contrary, stores that run the promotion tend to make most of the `Sales` on Monday. This fact could be a good indicator for Rossmann marketing campaigns. The same trend follow the stores which have both promotion at the same time (`Promo` and `Promo2` are equal to 1).\n- `Promo2` alone doesn't seem to be correlated to any significant change in the `Sales` amount. This can be also prooved by the blue pale area on the heatmap above.","3efb65f5":"#### Yearly trend\nThe next thing to check the presence of a trend in series.","6a1d8e44":"### Future Forecasting Using Prophet","753ac3f1":"Check the worst 10 sample","7f76e5c1":"Create a evaluation list having training and testing set which will cause the XGBOOst evaluate both the sets during running .","48e078dd":"#### DayofWeek Vs Sales","498d725f":"###  XGBoost \n\nNow lets split the last 6 weeks data as hold-out set","07da0b21":"#### Month Vs Sales","b580a6f3":"#### Feature Creation \/ Feature Selection\n\nMerge the training dataset with the Store dataset and spliting dataset into train and test datasets","c0d81a85":"- Prophet plots the observed values of our time series (the black dots), the forecasted values (blue line) and the uncertainty intervals of our forecasts (the blue shaded regions).","b7e4344e":"#### Prediction for Sales Store Type C","2e0ed5ae":"### Autocorrelaion\n\nAutocorrelation, also known as serial correlation or cross-autocorrelation, is the cross-correlation of a signal with itself at different points in time. It is the correlation between values of the process at different times. Informally, it is the similarity between observations as a function of the time lag between them. It is a mathematical tool for finding repeating patterns, such as the presence of a periodic signal obscured by noise.\n\nThe next step in ourtime series analysis is to review Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) plots. \n\n**Autocorrelation Function (ACF) :** ACF is a measure of the correlation between the timeseries with a lagged version of itself. For instance at lag 5, ACF would compare series at time instant \u2018t1\u2019\u2026\u2019tn\u2019 with series at instant \u2018t1-5\u2019\u2026\u2019tn-5\u2019 (t1-5 and tn being end points).\n\n**Partial Autocorrelation Function (PACF) :** PACF measures the correlation between the timeseries with a lagged version of itself but after eliminating the variations explained by the intervening comparisons. Eg. at lag 5, it will check the correlation but remove the effects already explained by lags 1 to 4.","baaa1a88":"Create a function to calculate Rmse and RMPSe","ad99774e":"### Problem Statement\nRossmann operates over 3,000 drug stores in 7 European countries. Currently, Rossmann store managers are tasked with predicting their daily sales for up to six weeks in advance. Store sales are influenced by many factors, including promotions, competition, school and state holidays, seasonality, and locality. With thousands of individual managers predicting sales based on their unique circumstances, the accuracy of results can be quite varied. \n\n#### Brief description\n\nThis project extracts relevant features based on the given training data and some basic information of each store to construct a training data set. Given the historical sales data of 1115 stores, `to predict the sales volume in the next 6 weeks, with the store sales as a reference`.\n\n#### Goal\n- Explore the data (ECDF, handle missing values etc).\n- Analysis per store type and correlational analysis of stores activity.\n- Perform extensive Time Series Analysis (seasonal decomposition, trends, autocorrelation).\n- Predict next 6 weeks of sales using Prophet (Facebook methodology).","1b1b0310":"#### Stationary Check","4fa64ebe":"We are finished with adding new variables to the data, so now we can check the overall correlations by plotting the seaborn heatmap:","fb29c291":"We will consider only data of Sales>0 and Open is 1\n","c06ee0a4":"##### Short description:\n- Sales: the turnover for any given day (target variable).\n- Customers: the number of customers on a given day.\n- Open: an indicator for whether the store was open: 0 = closed, 1 = open.\n- Promo: indicates whether a store is running a promo on that day.\n- StateHoliday: indicates a state holiday. Normally all stores, with few exceptions, are closed on state holidays. \n- SchoolHoliday: indicates if the (Store, Date) was affected by the closure of public schools.","ea9e300c":"#### Store Dataset","47750e25":"**Conclusion**\n\nBy performing various iterations on XGBoost with different features\n\nXGBoost with 200 iterations gave an RMSE of 0.0184 which is Acceptable.","566910dc":"### Johansen cointegration test","4086ad88":"What makes a time series different from a regular regression problem? \n\n- It is time dependent. The basic assumption of a linear regression that the observations are independent doesn\u2019t hold in this case.\n- Along with an increasing or decreasing trend, most time series have some form of seasonality trends, i.e. variations specific to a particular time frame. For example, for Christmas holidays, which we will see in this dataset.\n\nWe build a time series analysis on store types instead of individual stores. The main advantage of this approach is its simplicity of presentation and overall account for different trends and seasonalities in the dataset. \n\nIn this section, we will analyse time series data: its trends, sesonalities and autocorrelation. Usually at the end of the analysis, we are able to develop a seasonal ARIMA (Autoregression Integrated Moving Average) model but it won't be our main focus today. Instead, we try to understand the data, and only later come up with the forecasts using Prophet methodology.","85a1fbee":"**Dicky - Fuller test** : It is the test which essentially provides the details whether the Null hypothesis(Data is not stationary) should be rejected against alternate hypothesis(Data is stationary) It gives the Statistical measures explaining the ADF-Statistics value which should be less than the critical values we accept \/ reject the null hypothesis based on the p-value p-value < 0.5 indicates that the null hypothesis should be rejected\n\n\n\nThus from our data it is clear that our data is stationary and doesnot change heavily with time.\n\n**The Akaike information criterion (AIC) :** It is an estimator of the relative quality of statistical models for a given set of data. Given a collection of models for the data, AIC estimates the quality of each model, relative to each of the other models. Thus, AIC provides a means for model selection.\n\nADF statistics should be less than the critical interval","cc7e0513":"#### Prediction for Sales Store Type B","ffcff479":"**Conclusion of EDA**\n\n- The most selling and crowded `StoreType` is A.\n\n\n- The best \"Sale per Customer\" `StoreType` D indicates to the higher Buyer Cart. To benefit from this fact, Rossmann can consider proposing bigger variety of its products.\n\n\n- Low `SalePerCustomer` amount for `StoreType` B indicates to the possible fact that people shop there essentially for \"small\" things. Eventhough this `StoreType` generated the least amount of sales and customers over the whole period, it shows a great potential.\n\n\n- Customers tends to buy more on Modays when there's one promotion (`Promo`) and on Sundays when there's no promotion at all (both `Promo` and `Promo1` are equal to 0).\n\n\n- Promo2 alone doesn't seem to be correlated to any significant change in the `Sales` amount.","ab1f366b":"**Required Packages**","f3786a89":"### Time-Series Analysis per Store Type","bd138cdd":"\nFunction to define the Arima model for each Store type The function will consider the Sales data as well as external factors that are effecting the sales price of the stores.\n\nThe order parameter (p,q,d) for each Sales store type is selected after a lot of test and trial\n\nAfter model training the predictions are performed for the 1st Jan - 2015 to 1st July 2015 of the train data.\n\nThe plots are created to check the accuracy of prediction of the actual vs predicted sales values by the model for each store type","22af53ee":"Hyperparameter tunning is done with the important parameters.\n\nXGBoost is run for 6000 itertion and habe an early stopping round of 100.\n\nValidation is done of the tarined model and RMPSE is displayed as sonn as the below function ends.","8bf3216b":"Overall sales seems to increase, however not for the `StoreType` C (a third from the top). Eventhough the `StoreType` A is the most selling store type in the dataset, it seems that it cab follow the same decresing trajectory as `StoreType` C did.","720e02eb":"- The forecast object here is a new dataframe that includes a column yhat with the forecast, as well as columns for components and uncertainty intervals.","1ae0c1b3":"### Reading & Understanding The Dataset","dc55dfc4":"#### Data Processing ","185199e8":"#### Split the data set into train and test"}}