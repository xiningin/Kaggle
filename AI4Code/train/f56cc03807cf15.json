{"cell_type":{"b43d4c63":"code","48d03413":"code","3aa4aafa":"code","7fb4dff4":"code","ad2efc03":"code","651b0add":"code","417db72e":"code","08f80925":"code","d3649aa6":"code","0f0709d9":"code","98eae847":"code","74db0673":"code","457c86b7":"code","9a8dabbf":"code","063ca32c":"code","35500ed9":"code","6c65261a":"code","caa4cbd6":"code","e08e2ee4":"code","12775e0e":"code","476a0ff3":"code","a0854998":"code","0c8bd550":"code","58c84689":"code","3a3c74d1":"code","ce946269":"code","3956b766":"markdown","cfc0a7fe":"markdown","d51fd6c8":"markdown","fc4fc310":"markdown","814f6d49":"markdown","9361352e":"markdown","089c5c87":"markdown","246686d4":"markdown","b58f8e63":"markdown","296dab1f":"markdown","a835aa19":"markdown"},"source":{"b43d4c63":"from sklearn.neighbors import KNeighborsClassifier\nimport xgboost as xgb\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import linear_model\nfrom sklearn.neighbors import KNeighborsRegressor\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nfrom sklearn.metrics import mean_squared_error\n\nimport os\nprint(os.listdir(\"..\/input\"))\n","48d03413":"treino = pd.read_csv(\"..\/input\/train.csv\") #base de treino completa, com os pre\u00e7os e os ids\nx_treino = treino.drop(columns=[\"Id\",\"median_house_value\"])\ny_treino = treino[\"median_house_value\"]\n\n\nteste = pd.read_csv(\"..\/input\/test.csv\") #base de teste completa, com os ids\nx_teste = teste.drop(columns = \"Id\") #atributos da base teste, sem ids\n\nteste_positions = teste[[\"latitude\",\"longitude\"]]\ntreino_positions = treino[[\"latitude\",\"longitude\"]]\n\nx_treino.describe()","3aa4aafa":"treino_si = treino.drop(columns=\"Id\")\ntab = treino_si.corr(method=\"spearman\")\ndf = pd.DataFrame(tab)\n\ndef pinta(x):\n    if abs(x) == 1:\n        color = 'black'\n    elif abs(x) >= abs(df.quantile(q=0.75).mean()):\n        color = 'indianred' \n    elif abs(x) >= abs(df.mean().mean()):\n        color = 'aqua'\n    elif abs(x) <= abs(df.mean().mean())\/6:\n        color = 'white'\n    elif abs(x) <= abs(df.mean().mean())\/3:\n        color = 'paleturquoise' \n    elif abs(x) <= abs(df.mean().mean()):\n        color = 'dodgerblue'\n    else:\n        color = 'white'\n            \n    return 'background-color: %s' % color\n    \nprint(\"Salmon : correla\u00e7\u00e3o maior que a m\u00e9dia do 3\u00ba quartil das correla\u00e7\u00f5es \")\nprint(\"Azul mais vivo, aqua : correla\u00e7\u00e3o maior que a m\u00e9dia das correla\u00e7\u00f5es \")\nprint(\"Azul mais escuro, dodgerblue : correla\u00e7\u00e3o pequena \")\nprint(\"Azul mais clara, paleturquoise : correla\u00e7\u00e3o menor que um ter\u00e7o da m\u00e9dia\")\nprint(\"Branco : correla\u00e7\u00e3o menor que um sexto da m\u00e9dia \")\ndf.style.applymap(pinta)\n","7fb4dff4":"fig, ax = plt.subplots()\nax.scatter(x = treino['median_income'], y = treino['median_house_value'] , s = 1)\nplt.ylabel('median_house_value', fontsize=13)\nplt.xlabel('median_income', fontsize=13)\nplt.title(\"Relacionamento entre renda e valor do im\u00f3vel\")\nplt.show()\n\nx_treino[\"median_age\"].hist()\nx_teste[\"median_age\"].hist()\nplt.ylabel('frequ\u00eancia', fontsize=13)\nplt.xlabel('median_age', fontsize=13)\nplt.title(\"Histograma de idades m\u00e9dias das regi\u00f5es; laranja teste e azul treino\")\n\n\nplt.subplots()\nplt.scatter(x = treino['median_age'], y = treino['median_house_value'], s=0.5)\nplt.ylabel('median_house_value', fontsize=13)\nplt.xlabel('median_age', fontsize=13)\nplt.title(\"Relacionamento entre idade m\u00e9dia e valor do im\u00f3vel\")\nplt.show()\n\nplt.subplots()\nplt.scatter(treino[\"longitude\"],treino[\"latitude\"], c= treino[\"median_age\"] , cmap = \"jet\" , s = 5)\nplt.title(\"Idade m\u00e9dia por posi\u00e7\u00e3o geogr\u00e1fica; quanto mais quente a cor, maior a idade\")\nplt.ylabel('latitude', fontsize=13)\nplt.xlabel('longitude', fontsize=13)\nplt.xlim(-130,-110)\nplt.show()","ad2efc03":"from IPython.display import Image\nfrom IPython.core.display import HTML \n\nplt.subplots\nplt.xlim(-130,-110)\nplt.plot(teste[\"longitude\"],teste[\"latitude\"],\"g.\")\nplt.show()\n\nplt.subplots\nplt.xlim(-130,-110)\nplt.scatter(treino[\"longitude\"],treino[\"latitude\"], c= treino[\"median_house_value\"] , cmap = \"jet\" , s = 10)\nplt.show()\n\nImage(url= \"http:\/\/trackurls.info\/wp-content\/uploads\/2018\/02\/a-stylized-map-of-the-state-showing-different-big-cities-pacific-ocean-and-nearby-states-california-with-counties.jpg\",width=400)\n","651b0add":"from geopy import distance\nsacramento = (38.575764,-121.478851) #posi\u00e7\u00e3o geogr\u00e1fica da capital californiana\nlosangeles = (34.0522342, -118.2436849) #posi\u00e7\u00e3o geogr\u00e1fica de Los Angeles\n\ndef tratar(data):\n    \n    nlinhas = (data.shape[0])\n    \n    data[\"distance_sacramento\"] = [0] * nlinhas\n    data[\"distance_la\"] = [0] * nlinhas\n    \n    #data[\"latitude_region\"] = [0] * nlinhas\n    #dh = data[\"longitude\"]\n    #data[\"litoral\"] = [0] * nlinhas\n    \n    for i in range(nlinhas):\n        \n        data.loc[(i,\"distance_sacramento\")] = distance.distance((data.loc[i, 'latitude'] , data.loc[i, 'longitude']),sacramento).km\n        data.loc[(i,\"distance_la\")] = distance.distance((data.loc[i, 'latitude'] , data.loc[i, 'longitude']), losangeles).km\n\n#        if data[\"latitude_region_number\"][i] >= data[\"latitude_region_number\"].quantile(q=0.75): \n#            data.loc[(i,\"latitude_region\")] = 8 #regi\u00e3o extremo sul da calif\u00f3rnia, ~LA, San Diego\n#            if -119 < dh[i] <= -117:\n#                data.loc[(i,\"litoral\")] = 1\n            \n            \n#        elif data[\"latitude_region_number\"][i] >= data[\"latitude_region_number\"].quantile(q=0.5): \n#            data.loc[(i,\"latitude_region\")] = 6 #regi\u00e3o centro sul, ~Fresno\n#            if -119.5 < dh[i] <= -118.5:\n#                data.loc[(i,\"litoral\")] = 1\n            \n            \n#        elif data[\"latitude_region_number\"][i] >= data[\"latitude_region_number\"].quantile(q=0.25):\n#            data.loc[(i,\"latitude_region\")] = 4 #regi\u00e3o centro norte, ~Sacramento\n#            if -123 < dh[i] <= -119.5:\n#                data.loc[(i,\"litoral\")] = 1            \n            \n        \n#        elif data[\"latitude_region_number\"][i] >= 1:\n#            data.loc[(i,\"latitude_region\")] = 1 #regi\u00e3o extremo norte\n#            if -125 <= dh[i] <= -122.7:\n#                data.loc[(i,\"litoral\")] = 1            \n\n               \n    #criando colunas \u00fateis\n    data[\"razao_quarto_comodo\"] = data[\"total_bedrooms\"] \/ data[\"total_rooms\"]\n    data[\"razao_comodo_imovel\"] = data[\"total_rooms\"] \/ data[\"households\"]\n    data[\"razao_pop_house\"] = data[\"population\"] \/ data[\"households\"]\n    data[\"renda_per_capita\"] = data[\"median_income\"] \/ data[\"population\"]\n    \n    data.drop(columns=[\"population\",\"total_rooms\",\"total_bedrooms\",\"median_age\",\"latitude\",\"longitude\"], inplace = True) #n\u00e3o preciso mais dessas colunas\n    \n\n\n#x_treino[\"latitude_region_number\"].describe()\n#count    14448.000000\n#mean         1.094495\n#std          0.065705\n#min          1.000000\n#25%          1.042396\n#50%          1.052227\n#75%          1.158525\n#max          1.288786","417db72e":"tratar(x_treino)\ntabe = x_treino.corr(method=\"spearman\")\ntabe.style.applymap(pinta)","08f80925":"tratar(x_teste)","d3649aa6":"x_teste.shape[1] == x_treino.shape[1] #verificando dimens\u00f5es","0f0709d9":"#### M\u00e9todo Lasso  ---- etapa 1 boosting\n\nlas = linear_model.Lasso(alpha=0.6)\nlas = las.fit(x_treino, y_treino)\npred = las.predict(x_treino)\n\nprint(\"Cross-value-score:  \",las.score(x_treino,y_treino, sample_weight=None))\n\nprint(\"Erro quadr\u00e1tico m\u00e9dio:  \" ,mean_squared_error(y_treino, pred))\n","98eae847":"#### M\u00e9todo Ridge\n\nrid = linear_model.RidgeCV(alphas=[0.1 ,0.8,0.3], cv=7)\nrid = rid.fit(x_treino,y_treino)\npred = rid.predict(x_treino)\n\nprint(\"Cross-value-score:  \",rid.score(x_treino,y_treino, sample_weight=None))\n\nprint(\"Erro quadr\u00e1tico m\u00e9dio:  \" ,mean_squared_error(y_treino, pred))\n","74db0673":"from catboost import CatBoostRegressor\n\ngato = CatBoostRegressor(learning_rate=1, depth=4,num_trees= 20, loss_function='RMSE')\nfit_gato = gato.fit(x_treino, y_treino)\npredigato = gato.predict(x_treino)\n\n\nprint(\"Erro quadr\u00e1tico m\u00e9dio:  \" ,mean_squared_error(y_treino, predigato))\n","457c86b7":"y_gato = abs (gato.predict(x_teste))\nsmit = pd.DataFrame({\"Id\": teste[\"Id\"] , \"median_house_value\": y_gato[:]})\nsmit.to_csv(\"_gatoboost_\", index = False)","9a8dabbf":"#### M\u00e9todo K Nearest Neighbors com 80 vizinhos\n\nknn = KNeighborsRegressor(n_neighbors = 80, weights = \"distance\")\nknn.fit( x_treino ,  y_treino)\n\nprint(\"Cross-value-score:  \",knn.score(x_treino,y_treino, sample_weight=None))\nprint(\"Erro quadr\u00e1tico m\u00e9dio:  \" ,mean_squared_error(y_treino, knn.predict(x_treino)))\n\nknn30 = KNeighborsRegressor(n_neighbors = 30, weights = \"distance\")","063ca32c":"### Random Forest:\n'''\nfrom sklearn.ensemble import RandomForestClassifier\n\nforest = RandomForestClassifier(n_estimators=30, criterion='gini', max_depth= 2 , \n                                    max_features='auto', min_impurity_decrease=0.3,\n                                    bootstrap=True, oob_score=False, warm_start=False, n_jobs = -1)\n\nforest.fit(x_treino,y_treino)\n\n\nprint(\"Cross-value-score:  \",forest.score(x_treino,y_treino, sample_weight=None))\n\nprint(\"Erro quadr\u00e1tico m\u00e9dio:  \" , mean_squared_error(y_treino, forest.predict(x_treino)))\n'''\n\n","35500ed9":"### Regression Tree::\nfrom sklearn.tree import DecisionTreeRegressor\n\narvore = DecisionTreeRegressor(criterion=\"mse\", max_depth=20, min_samples_split=6, \n                               min_samples_leaf=8, min_impurity_decrease=0.2, \n                               presort=True)\n\narvore.fit(x_treino, y_treino)\npred = arvore.predict(x_treino)\n\nprint(\"Cross-value-score:  \",arvore.score(x_treino,y_treino, sample_weight=None))\n\nprint(\"Erro quadr\u00e1tico m\u00e9dio:  \" ,mean_squared_error(y_treino, pred))\n\ny_arvore = arvore.predict(x_teste)\nsmit = pd.DataFrame({\"Id\": teste[\"Id\"] , \"median_house_value\": y_arvore[:]})\nsmit.to_csv(\"_arvore_\", index = False)\n","6c65261a":"### LDA ::\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nlda = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\npred = lda.fit(x_treino, y_treino).predict(x_treino)\n\nprint(\"Cross-value-score:  \",lda.score(x_treino,y_treino, sample_weight=None))\n\nprint(\"Erro quadr\u00e1tico m\u00e9dio:  \" ,mean_squared_error(y_treino, pred))\n","caa4cbd6":"### QDA :: \n\n#from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n#qda = QuadraticDiscriminantAnalysis().fit(x_treino, y_treino)\n\n#print(\"Cross-value-score:  \",qda.score(x_treino,y_treino, sample_weight=None))\n\n#print(\"Erro quadr\u00e1tico m\u00e9dio:  \" ,mean_squared_error(y_treino, qda.predict(xx)))\n\n\n#smit = pd.DataFrame({\"Id\": teste[\"Id\"] , \"median_house_value\": predtest[:]})\n#smit.to_csv(\"sub_QDA\", index = False)","e08e2ee4":"from sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import BaggingRegressor\n\nbagging = BaggingRegressor(knn30, max_samples=0.3, max_features=0.3)\nbagpred = bagging.fit(x_treino,y_treino).predict(x_teste)\n\n\nadatree = AdaBoostRegressor(arvore, n_estimators=100)\nadapred = adatree.fit(x_treino,y_treino).predict(x_teste)\n\n\nsmita = pd.DataFrame({\"Id\": teste[\"Id\"] , \"median_house_value\": bagpred[:]})\nsmita.to_csv(\"_bolsa_\", index = False)\n\nsmitb = pd.DataFrame({\"Id\": teste[\"Id\"] , \"median_house_value\": adapred[:]})\nsmitb.to_csv(\"_ada_\", index = False)\n","12775e0e":"#tentativa 1\n\nlas = las.fit(x_treino, y_treino)\npred_in = las.predict(x_treino)\nerro1 = y_treino - pred_in\n\nprederro1 = adatree.fit(x_treino, erro1).predict (x_treino)\nerro2 = erro1 - prederro1\n\nknn30.fit( x_treino ,  erro2)\nprederro2 = knn30.predict(x_treino)\nerro3 = erro2 - prederro2\n","476a0ff3":"for i in range (5,45,8):\n    print(y_treino[i] ,'=', pred_in[i] , prederro1[i] , prederro2[i])\n\nprint()\nfor i in range(5,45,8):\n    print(erro1[i],prederro1[i], 'und', erro2[i], prederro2[i])","a0854998":"#para base teste\n\ny_teste_primario = las.predict(x_teste)\nprederroa = adatree.predict(x_teste)\nprederrob = knn30.predict (x_teste)\n\ny_teste = y_teste_primario + prederroa + prederrob\n\nsmition = pd.DataFrame({\"Id\": teste[\"Id\"] , \"median_house_value\": y_teste[:]})\nsmition.to_csv(\"_boost_4\", index = False)","0c8bd550":"#tentativa 2 \n\nerroi = y_treino - predigato\n\nprederroi = adatree.fit(x_treino, erroi).predict (x_treino)\nerroii = erroi - prederroi\n\nprederroii = knn30.fit( x_treino ,  erroii).predict(x_treino)\nerroiii = erroii - prederroii\n\nfor i in range (5,45,8):\n    print(y_treino[i] ,'=', predigato[i] , prederroi[i] , prederroii[i] , erroiii[i])\n\nprint()\nfor i in range(5,45,8):\n    print(erroi[i],prederroi[i], 'und', erroii[i], prederroii[i])","58c84689":"#para base teste da tentativa 2\n\ny_teste_primario = gato.predict(x_teste)\nprederroa = arvore.predict(x_teste)\nprederrob = knn30.predict (x_teste)\n\ny_teste = y_teste_primario + prederroa + prederrob\n\nsmition = pd.DataFrame({\"Id\": teste[\"Id\"] , \"median_house_value\": y_teste[:]})\nsmition.to_csv(\"_boost_5\", index = False)","3a3c74d1":"#tentativa 3 =============\n\npred_in = lda.fit(x_treino, y_treino).predict(x_treino)\nerroi = y_treino - pred_in\n\nprederroi = adatree.fit(x_treino, erroi).predict (x_treino)\nerroii = erroi - prederroi\n\nprederroii = knn30.fit( x_treino ,  erroii).predict(x_treino)\nerroiii = erroii - prederroii\n\n\nfor i in range (5,45,8):\n    print(y_treino[i] ,'=', pred_in[i] , prederroi[i] , prederroii[i] , erroiii[i])\n\nprint()\nfor i in range(5,45,8):\n    print(erroi[i],prederroi[i], 'und', erroii[i], prederroii[i])","ce946269":"#para base teste da tentativa 3\n\ny_teste_primario = lda.predict(x_teste)\nprederroa = adatree.predict(x_teste)\nprederrob = knn30.predict (x_teste)\n\ny_teste = y_teste_primario + prederroa + prederrob\n\nsmition = pd.DataFrame({\"Id\": teste[\"Id\"] , \"median_house_value\": y_teste[:]})\nsmition.to_csv(\"_boost_6\", index = False)","3956b766":"Os diversos m\u00e9todos exercitados acima n\u00e3o surtiram muita melhora na estima\u00e7\u00e3o dos pre\u00e7os das casas. Eles seriam classificadores m\u00e9dios-fracos. Se combinar os estimadores utilizando Bagging ou Boosting, \u00e9 poss\u00edvel que o resultado melhore.","cfc0a7fe":"Para a LDA, observo que o score deu muito baixo, se comparado aos outros m\u00e9todos utilizados, se utilizar de forma crua os dados. ","d51fd6c8":"   Com essa tabela, posso perceber que total_rooms est\u00e1 altamente ligado a total_bedrooms. Da mesma forma, households a total_bedrooms de forma bem impressionante, e finalmente population e household tamb\u00e9m muito relacionados. Uma engenharia de dados nesses dados altamente correlacionados seria interessante, de forma a obter um n\u00famero de habitantes por quarto da casa, ou dividir population por total_bedrooms, por exemplo. Latitude e longitude est\u00e3o muito correlacionados por conta do formato em diagonal do estado da Calif\u00f3rnia. Pode-se substituir esses dados pela dist\u00e2ncia at\u00e9 Sacramento tamb\u00e9m.\n\n\n\n   O valor do im\u00f3vel est\u00e1 muito correlacionado, segundo o m\u00e9todo spearman, com a renda m\u00e9dia. Seria interessante, apenas como experimento, verificar o que aconteceria se apenas o median_income influenciasse no valor do im\u00f3vel.\n\n\n","fc4fc310":"Para come\u00e7ar, importarei as bibliotecas necess\u00e1rias mais comuns. As espec\u00edficas da tarefa importarei no ato de utiliz\u00e1-las. A partir da\u00ed, analisar os dados.","814f6d49":"**Testando os m\u00e9todos**\n\n\nVou agora testar os m\u00e9todos avaliando seus cross-value-score e erro quadr\u00e1tico m\u00e9dio.","9361352e":"Tentativa utilizando predi\u00e7\u00e3o CatBoost como inicial:","089c5c87":"**Tarefa 3 - Regress\u00e3o Linear**\n\n\n\n**Pre\u00e7os de resid\u00eancias da Calif\u00f3rnia**","246686d4":"Cr\u00e9ditos \u00e0s fontes de consulta:\n* Regress\u00e3o linear m\u00faltipla - Lume - UFRGS  (TCC 2.0 R29 20-12-10)\n* Scikit Learn\n* Stack Overflow\n* Stack Exchange\n* Documenta\u00e7\u00e3o Numpy Linalg\n* Cat Boost\n","b58f8e63":"Realizando o mesmo processo com os dados de teste:","296dab1f":"\u00c9 interessante verificar a disposi\u00e7\u00e3o dos im\u00f3veis num gr\u00e1fico, comparando com um mapa do estado da Calif\u00f3rnia.\nAbaixo, podemos verificar que as maiores concentra\u00e7\u00f5es dos dados de treino e tamb\u00e9m dos dados de teste est\u00e3o nas grandes cidades desse estado. Em azul, a base de teste e em vermelho, a base de treino.","a835aa19":"O objetivo do tratamento feito acima foi diminuir as correla\u00e7\u00f5es das colunas, aumentando a qualidade das informa\u00e7\u00f5es e sem deixar colunas \"in\u00fateis\" por serem praticamente combina\u00e7\u00e3o linear de outra. Os resultados podem ser vistos na tabela de correla\u00e7\u00f5es abaixo:"}}