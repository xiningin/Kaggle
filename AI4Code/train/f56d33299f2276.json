{"cell_type":{"7ddd873e":"code","608a9c3a":"code","c4a7eeb3":"code","23570ce3":"code","d4294f09":"code","7f58c6cb":"code","dd11efaf":"code","37ede05b":"code","5ce35c1e":"code","c4fe31ee":"code","cf986cfd":"code","b2ef0649":"code","5015cf84":"code","ccabb519":"code","df371c02":"code","7fbacddf":"code","8c0fe9c3":"code","10183d23":"code","ad568bf4":"code","3da72ecf":"markdown","08663e26":"markdown","ca687a0f":"markdown","3eac9b63":"markdown","8615af90":"markdown","60cefadc":"markdown","99b5e79d":"markdown","a251a5cf":"markdown","c3271175":"markdown","e0bfdc99":"markdown"},"source":{"7ddd873e":"# \u0438\u043c\u043f\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0435 \u0434\u043b\u044f \u043d\u0430\u0447\u0430\u043b\u0430 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","608a9c3a":"# \u0437\u0430\u0433\u0440\u0443\u0437\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0432 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0435\u0439 \u043a\u043e\u0434\u0438\u0440\u043e\u0432\u043a\u0435, \u0440\u0430\u0437\u043c\u0435\u0447\u0430\u044f \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u044f \u0441\u0442\u043e\u0431\u0446\u043e\u0432\nDATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\nDATASET_ENCODING = \"ISO-8859-1\"\ndata = pd.read_csv('..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv', \n                   encoding=DATASET_ENCODING, \n                   names=DATASET_COLUMNS)\ndata.head()","c4a7eeb3":"# \u0443\u0434\u0430\u043b\u0438\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 (\u0441\u0442\u043e\u043b\u0431\u0446\u044b), \u043d\u0435 \u0432\u043b\u0438\u044f\u044e\u0449\u0438\u0435 \u043d\u0430 \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0443\u044e \u0438\u0442\u043e\u0433\u043e\u0432\u0443\u044e \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044e \u0434\u0430\u043d\u043d\u044b\u0445 \ndf = data.drop(['ids', 'date', 'flag', 'user'], axis=1)\ndf.head()","23570ce3":"# \u0437\u0430\u043c\u0435\u043d\u0438\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0434\u043b\u044f \u043f\u043e\u0437\u0438\u0442\u0438\u0432\u043d\u043e\u0433\u043e \u0442\u0432\u0438\u0442\u0430 \u0441 \"4\" \u043d\u0430 \u0431\u043e\u043b\u0435\u0435 \u043f\u0440\u0438\u0432\u044b\u0447\u043d\u043e\u0435 \"1\"\n# \u0438 \u043e\u0442\u043e\u0431\u0440\u0430\u0437\u0438\u043c \u043e\u0431\u0449\u0435\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u043e\u0437\u0438\u0442\u0438\u0432\u043d\u044b\u0445 \u0438 \u043d\u0435\u0433\u0430\u0442\u0438\u0432\u043d\u044b\u0445 \u043f\u043e\u0441\u0442\u043e\u0432\ndf['target'] = df['target'].replace(4, 1)\ndf['target'].value_counts()","d4294f09":"# \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439\n# \u0432\u0438\u0434\u0438\u043c, \u0447\u0442\u043e \u043a\u043b\u0430\u0441\u0441\u044b \u0441\u0431\u0430\u043b\u0430\u043d\u0441\u0438\u0440\u043e\u0432\u0430\u043d\u044b\nax = df.groupby('target').count().plot(kind='bar', title='Target distribution', legend=False)\nax.set_xticklabels(['Negative','Positive'], rotation=0)","7f58c6cb":"# \u0441\u043e\u0445\u0440\u0430\u043d\u0438\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u043e\u0433\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 \u0438 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439 \u0432 \u0441\u043f\u0438\u0441\u043a\u0438\ntext, target = list(df['text']), list(df['target'])","dd11efaf":"import re\nimport string\nfrom nltk.stem import WordNetLemmatizer\n\n# \u0441\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u0434\u043b\u044f \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u043e\u0433\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\ndef preprocess(doc):\n    prepdoc = []\n    \n    lemmatizer = WordNetLemmatizer()\n    \n    urlptr = r'((http:\/\/)[^ ]*|(https:\/\/)[^ ]*|( www\\.)[^ ]*)'\n    usrptr = '@[^\\s]+'\n    alhptr = '[^a-zA-Z0-9]'\n    sqcptr = r'(.)\\1\\1+'\n    rplptr = r'\\1\\1'\n    \n    for text in doc:\n        # \u043f\u0440\u0438\u0432\u043e\u0434\u0438\u043c \u0432\u0435\u0441\u044c \u0442\u0435\u043a\u0441\u0442 \u043a \u043d\u0438\u0436\u043d\u0435\u043c\u0443 \u0440\u0435\u0433\u0438\u0441\u0442\u0440\u0443\n        text = text.lower()\n        # \u0437\u0430\u043c\u0435\u043d\u044f\u0435\u043c \u0441\u0441\u044b\u043b\u043a\u0438 \u043d\u0430 'URL'\n        text = re.sub(urlptr, ' URL', text)      \n        # \u0437\u0430\u043c\u0435\u043d\u044f\u0435\u043c \u0438\u043c\u044f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \u043d\u0430 'USER'\n        text = re.sub(usrptr, ' USER', text)        \n        # \u0443\u0431\u0438\u0440\u0430\u0435\u043c \u0432\u0441\u0435 \u0441\u0438\u043c\u0432\u043e\u043b\u044b, \u043e\u0442\u043b\u0438\u0447\u043d\u044b\u0435 \u043e\u0442 \u0431\u0443\u043a\u0432\u0435\u043d\u043d\u044b\u0445 \u0438\u043b\u0438 \u0446\u0438\u0444\u0440\u043e\u0432\u044b\u0445\n        text = re.sub(alhptr, ' ', text)\n        # \u043e\u0431\u0440\u0435\u0437\u0430\u0435\u043c \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u0438\u0437 \u0442\u0440\u0451\u0445 \u0438 \u0431\u043e\u043b\u0435\u0435 \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u044b\u0445 \u0431\u0443\u043a\u0432\n        text = re.sub(sqcptr, rplptr, text)\n        \n        words = ' '\n        for word in text.split():\n            # \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u043c \u043a\u043e\u0440\u043e\u0442\u043a\u0438\u0435 \u0441\u043b\u043e\u0432\u0430 \u0438 \u043f\u0440\u0438\u0432\u043e\u0434\u0438\u043c \u0441\u043b\u043e\u0432\u043e\u0444\u043e\u0440\u043c\u044b \u043a \u043b\u0435\u043c\u043c\u0435 (\u0441\u043b\u043e\u0432\u0430\u0440\u043d\u043e\u0439 \u0444\u043e\u0440\u043c\u0435)\n            if len(word) > 1:\n                word = lemmatizer.lemmatize(word)\n                words += (word + ' ')\n            \n        prepdoc.append(words)\n        \n    return prepdoc","37ede05b":"# \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u0430\u0435\u043c \u0441\u043f\u0438\u0441\u043e\u043a \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 'text' \u0438 \u043e\u0442\u043e\u0431\u0440\u0430\u0437\u0438\u043c \u0447\u0430\u0441\u0442\u044c \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u0439\n%time preptext = preprocess(text)\npreptext[:10]","5ce35c1e":"from wordcloud import WordCloud\n\n# \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u043e\u0431\u043b\u0430\u043a\u043e \u0441\u043b\u043e\u0432, \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0447\u0430\u0441\u0442\u043e \u043f\u043e\u044f\u0432\u043b\u044f\u044e\u0449\u0438\u0445\u0441\u044f \u0432 \u043f\u043e\u0437\u0438\u0442\u0438\u0432\u043d\u044b\u0445 \u0442\u0432\u0438\u0442\u0430\u0445\nwordpos = preptext[800000:]\nwc = WordCloud(max_words=1000, width=1600, height=800, \n               collocations=False).generate(' '.join(wordpos))\nplt.figure(figsize=(20, 20))\nplt.imshow(wc)","c4fe31ee":"# \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u043e\u0431\u043b\u0430\u043a\u043e \u0441\u043b\u043e\u0432, \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0447\u0430\u0441\u0442\u043e \u043f\u043e\u044f\u0432\u043b\u044f\u044e\u0449\u0438\u0445\u0441\u044f \u0432 \u043d\u0435\u0433\u0430\u0442\u0438\u0432\u043d\u044b\u0445 \u0442\u0432\u0438\u0442\u0430\u0445\nwordneg = preptext[:800000]\nwc = WordCloud(max_words=1000, width=1600, height=800, \n               collocations=False).generate(' '.join(wordneg))\nplt.figure(figsize=(20, 20))\nplt.imshow(wc)","cf986cfd":"from sklearn.model_selection import train_test_split\n\n# \u0440\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0430 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0438\u0435 \u0438 \u0438\u0441\u043f\u044b\u0442\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u043d\u0430\u0431\u043e\u0440\u044b\nX_train, X_test, y_train, y_test = train_test_split(preptext, target, test_size=0.1, random_state=0)","b2ef0649":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u043a\u043b\u0430\u0441\u0441 TfidfVectorizer, \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u044e\u0449\u0438\u0439 \u0442\u0435\u043a\u0441\u0442 \u0432 \u043c\u0430\u0442\u0440\u0438\u0446\u0443 tfidf\n# \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c ngram \u0438 \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u0438\u0435\u043c \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\nvectorizer = TfidfVectorizer(ngram_range=(1, 2), \n                             max_features=500000)\n%time vectorizer.fit(preptext)","5015cf84":"# \u0441\u043e\u0437\u0434\u0430\u0451\u043c \u0432\u0435\u043a\u0442\u043e\u0440\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\nX_train = vectorizer.transform(X_train)\nX_test  = vectorizer.transform(X_test)\n\n# \u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0441\u043b\u043e\u0432\u0430 \u0432 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u043e\u043c \u0442\u0432\u0438\u0442\u0435, \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u0432 \u043f\u043e\u043b\u0443\u0447\u0438\u0432\u0448\u0438\u0439\u0441\u044f \u0432\u0435\u043a\u0442\u043e\u0440\nvectorizer.inverse_transform(X_train[66])[0][np.argsort(X_train[66].data)]","ccabb519":"from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix \n\n# \u0441\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438 \u0438 \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432\ndef evaluate(model, matrix):\n    \n    # \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u043c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435\n    y_pred = model.predict(X_test)\n\n    # \u0432\u044b\u0432\u043e\u0434\u0438\u043c \u043c\u0435\u0442\u0440\u0438\u043a\u0438\n    print(classification_report(y_test, y_pred))\n    \n    # \u0441\u0442\u0440\u043e\u0438\u043c \u0441onfusion matrix\n    plot_confusion_matrix(matrix, X_test, y_test, \n                          display_labels=['Negative', 'Positive'], \n                          cmap='Blues', values_format=' ')   ","df371c02":"from sklearn.svm import LinearSVC\n\nsvc = LinearSVC()\nsvc_mtx = svc.fit(X_train, y_train)\n%time evaluate(svc, svc_mtx)","7fbacddf":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(max_iter=1000, n_jobs=-1)\nlogreg_mtx = logreg.fit(X_train, y_train)\n%time evaluate(logreg, logreg_mtx)","8c0fe9c3":"from sklearn.naive_bayes import BernoulliNB\n\nbnb = BernoulliNB()\nbnb_mtx = bnb.fit(X_train, y_train)\n%time evaluate(bnb, bnb_mtx)","10183d23":"from sklearn.naive_bayes import ComplementNB\n\ncnb = ComplementNB()\ncnb_mtx = cnb.fit(X_train, y_train)\n%time evaluate(cnb, cnb_mtx)","ad568bf4":"import pickle\n\n# \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043c\u0430\u0442\u0440\u0438\u0446\u044b tfidf\nfile = open('vectorizer_ngram(1,2).pickle','wb')\npickle.dump(vectorizer, file)\nfile.close()\n\n# \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0438\u0438\nfile = open('LogReg_model.pickle','wb')\npickle.dump(logreg, file)\nfile.close()","3da72ecf":"### Bernoulli Naive Bayes","08663e26":"## \u0421\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438","ca687a0f":"* \u0412 \u044d\u0442\u043e\u0439 \u0440\u0430\u0431\u043e\u0442\u0435 \u043d\u0430 \u043f\u0440\u0438\u043c\u0435\u0440\u0435 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 **Sentiment140** \u043f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u0440\u0435\u0448\u0438\u0442\u044c \u0437\u0430\u0434\u0430\u0447\u0443 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u043f\u043e\u0441\u0442\u043e\u0432 \u0432 Twitter \u043d\u0430 \u043d\u0435\u0433\u0430\u0442\u0438\u0432\u043d\u044b\u0435 \u0438 \u043f\u043e\u0437\u0438\u0442\u0438\u0432\u043d\u044b\u0435 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u043e\u0432 SVM, \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438 \u0438 \u043d\u0430\u0438\u0432\u043d\u043e\u0433\u043e \u0431\u0430\u0439\u0435\u0441\u0430.","3eac9b63":"# Tweets polarity analysis with different algorithms","8615af90":"* \u0412 \u0438\u0442\u043e\u0433\u0435 \u0432\u0438\u0434\u0438\u043c, \u0447\u0442\u043e \u043d\u0430\u0438\u043b\u0443\u0447\u0448\u0438\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u0441 f1=0.83 \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u0438\u0441\u044c \u0432 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0435 \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438 \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438, \u0437\u0430 \u043d\u0435\u0439 \u0438\u0434\u0451\u0442 \u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0439 SVM (f1=0.82). \u041b\u0443\u0447\u0448\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c \u0438 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043c\u0430\u0442\u0440\u0438\u0446\u044b \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 tfidf \u0441\u043e\u0445\u0440\u0430\u043d\u0438\u043c \u0434\u043b\u044f \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0435\u0433\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f.","60cefadc":" ## \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0438 \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445","99b5e79d":"### Complement Naive Bayes","a251a5cf":"### Linear support vector machine (SVM)","c3271175":"### Logistic regression","e0bfdc99":"## \u0412\u044b\u0431\u043e\u0440 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430 \u0438 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438"}}