{"cell_type":{"a592be77":"code","57d8083d":"code","8e6f722a":"code","8ef7119d":"code","2ea4dab8":"code","7115b473":"code","c45b303f":"code","8a7d007f":"code","8296878b":"code","4e2c709d":"code","61c8bd0e":"code","fd4ef3c5":"code","3d2140f0":"code","60448970":"code","c3a1eb4c":"code","d66a07e5":"code","34b13acc":"code","ebe887e4":"code","e15397de":"code","6113c78f":"code","c2a040e9":"code","dda293ca":"code","98333798":"markdown","39375403":"markdown","03c72776":"markdown","43cd569d":"markdown","480a66f6":"markdown","2de0399a":"markdown","eca693e0":"markdown","7bbf0000":"markdown","aae564b1":"markdown","c5a838e0":"markdown","63c784c7":"markdown","e0ab7166":"markdown","7ba1309e":"markdown","7b18c831":"markdown","3290ac35":"markdown","75de6275":"markdown","705c0034":"markdown","9b164aa0":"markdown","8737ac6e":"markdown","4b2bc81e":"markdown","e6c15f10":"markdown"},"source":{"a592be77":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Supress Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport gc\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom statistics import mean\n\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression","57d8083d":"train = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv')\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv')","8e6f722a":"train.describe().style.background_gradient(\"copper_r\")","8ef7119d":"print(\"Null values in train data\", train.isnull().sum().sum())\nprint(\"Null values in test data\", test.isnull().sum().sum())","2ea4dab8":"cols = test.columns\ncols","7115b473":"# Normalizing the features\nscaler = StandardScaler()\n\ntrain[cols] = scaler.fit_transform(train[cols])\ntest[cols] = scaler.transform(test[cols])","c45b303f":"def Trainer(model, model_name, train_data, test_data, fold):\n    test_preds = np.zeros(test_data.shape[0])\n    train_preds = np.zeros(train_data.shape[0])\n    \n    kf = StratifiedKFold(n_splits=fold,random_state=48,shuffle=True)\n    \n    train_auc=[]\n    test_auc=[]\n    \n    n=0\n    \n    for train_index, test_index in kf.split(train[cols],train['target']):\n        \n        X_train, X_test = train[cols].iloc[train_index], train[cols].iloc[test_index]\n        y_train, y_test = train['target'].iloc[train_index], train['target'].iloc[test_index]\n        \n        if model_name == 'catb':\n            model.fit(X_train, y_train, eval_set=[(X_test, y_test)], silent=True)\n        elif model_name == 'lgbm' or model_name == 'xgb':\n            model.fit(X_train, y_train, eval_set=[(X_test,y_test)], early_stopping_rounds=100, eval_metric=\"auc\", verbose=False)\n        else:\n            model.fit(X_train, y_train)\n        \n        train_preds += model.predict_proba(train_data[cols])[:,1]\/kf.n_splits\n        test_preds += model.predict_proba(test_data[cols])[:,1]\/kf.n_splits\n        \n        train_auc.append(roc_auc_score(y_train, model.predict_proba(X_train)[:, 1]))\n        test_auc.append(roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]))\n        \n        gc.collect()\n        \n        print(f\"fold: {n+1}, train_auc: {train_auc[n]}, test_auc: {test_auc[n]}\")\n        n+=1\n    print(f\"train_avg = {mean(train_auc)}, test_avg = {mean(test_auc)}\" )\n    return train_preds, test_preds","8a7d007f":"lgbm = LGBMClassifier()\nxgb = XGBClassifier()\ncatb = CatBoostClassifier()\nrad = RandomForestClassifier()\nada = AdaBoostClassifier()\ndec = DecisionTreeClassifier()\nlr = LogisticRegression()","8296878b":"lr_train, lr_test = Trainer(lr, 'lr', train, test, 5)\ndel lr\ngc.collect()\n\nsample_submission['target'] = lr_test\nsample_submission.to_csv('lr_test.csv', index=False)","4e2c709d":"lgbm_train, lgbm_test = Trainer(lgbm, 'lgbm', train, test, 5)\nimportances_df = pd.DataFrame(lgbm.feature_importances_, columns=['Feature_Importance'], index=cols).sort_values(by=\"Feature_Importance\", ascending=False)\n\ndel lgbm\ngc.collect()\n\nsample_submission['target'] = lgbm_test\nsample_submission.to_csv('lgbm_test.csv', index=False)","61c8bd0e":"importances_df.T.style.background_gradient(cmap=\"copper_r\")","fd4ef3c5":"catb_train, catb_test = Trainer(catb, 'catb', train, test, 5)\nimportances_df = pd.DataFrame(catb.feature_importances_, columns=['Feature_Importance'], index=cols).sort_values(by=\"Feature_Importance\", ascending=False)\n\ndel catb\ngc.collect()\n\nsample_submission['target'] = catb_test\nsample_submission.to_csv('catb_test.csv', index=False)","3d2140f0":"importances_df.T.style.background_gradient(cmap=\"copper_r\")","60448970":"xgb_train, xgb_test = Trainer(xgb, 'xgb', train, test, 5)\nimportances_df = pd.DataFrame(xgb.feature_importances_, columns=['Feature_Importance'], index=cols).sort_values(by=\"Feature_Importance\", ascending=False)\n\ndel xgb\ngc.collect()\n\nsample_submission['target'] = xgb_test\nsample_submission.to_csv('xgb_test.csv', index=False)","c3a1eb4c":"importances_df.T.style.background_gradient(cmap=\"copper_r\")","d66a07e5":"rad_train, rad_test = Trainer(rad, 'rad', train, test, 5)\nimportances_df = pd.DataFrame(rad.feature_importances_, columns=['Feature_Importance'], index=cols).sort_values(by=\"Feature_Importance\", ascending=False)\n\ndel rad\ngc.collect()\n\nsample_submission['target'] = rad_test\nsample_submission.to_csv('rad_test.csv', index=False)","34b13acc":"importances_df.T.style.background_gradient(cmap=\"copper_r\")","ebe887e4":"ada_train, ada_test = Trainer(ada, 'ada', train, test, 5)\nimportances_df = pd.DataFrame(ada.feature_importances_, columns=['Feature_Importance'], index=cols).sort_values(by=\"Feature_Importance\", ascending=False)\n\ndel ada\ngc.collect()\n\nsample_submission['target'] = ada_test\nsample_submission.to_csv('ada_test.csv', index=False)","e15397de":"importances_df.T.style.background_gradient(cmap=\"copper_r\")","6113c78f":"dec_train, dec_test = Trainer(dec, 'dec', train, test, 5)\nimportances_df = pd.DataFrame(dec.feature_importances_, columns=['Feature_Importance'], index=cols).sort_values(by=\"Feature_Importance\", ascending=False)\n\ndel dec\ngc.collect()\n\nsample_submission['target'] = dec_test\nsample_submission.to_csv('dec_test.csv', index=False)","c2a040e9":"importances_df.T.style.background_gradient(cmap=\"copper_r\")","dda293ca":"sample_submission['target'] = (lr_test*4 + lgbm_test*3 + catb_test*2 + xgb_test)\/10\nsample_submission.to_csv('average.csv', index=False)","98333798":"# Describing the data","39375403":"# LogisticRegression","03c72776":"# Importing data","43cd569d":"# DecisionTreeClassifier","480a66f6":"# AdaBoostClassifier","2de0399a":"# EDA Notebook\n\n#### Link: https:\/\/www.kaggle.com\/rigeltal\/tps-11-first-look-eda","eca693e0":"# Next notebook\n#### Please look in the comment section... Hope I will be able to complete it soon","7bbf0000":"# LGBMClassifier","aae564b1":"# Normalization","c5a838e0":"# Model Trainer","63c784c7":"# Weighted average","e0ab7166":"# Initialization","7ba1309e":"# Final note\n#### Thank you!\nIf you like it please upvote it. If you have suggestion please leave it in comment. Even I am beginner looking forward to learn something new. So let me know how can I improve this","7b18c831":"# Introduction\n\nThe dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting identifying spam emails via various extracted features from the email. Although the features are anonymized, they have properties relating to real-world features.\n\nSubmissions are evaluated on area under the ROC curve between the predicted probability and the observed target.\n\n","3290ac35":"### 1) What are we going to do in this notebook?\n* We are going to train many different models and see which model is performing well.\n* Feature importances of each model.\n* Then we are going to pick the best performing models and play with it in the next notebook.\n\n### 2) Please look into the comment section where I will post my insights. I will be glad if you engage in discussion in comments section.","75de6275":"# RandomForestClassifier","705c0034":"# CatBoostClassifier","9b164aa0":"# Observation\n* Logistic regression is performing well in submission which \"might\" means that the dataset is quite simple. Therefore we can use NN because simpler dataset don't need very large dataset to train NN.\n* I can see f34, f55, f43 and f8 in top 5 of almost all feature importance of models.\n* CatBoostClassifier gives id a large value in feature importance which is shocking. \n* Why? Well my idea of not dropping id column is to see what is the position of id column in feature importance and the features after the id column are waster than id column.(Hope we engage in discussion in this topic)\n* Random forest and Decision tree is overfitting the model with default parameters which in turn \"may\" confirm that the dataset is simple, given that Logistic regression performs well. ","8737ac6e":"# Checking for null values","4b2bc81e":"# XGBClassifier","e6c15f10":"# Importing libraries"}}