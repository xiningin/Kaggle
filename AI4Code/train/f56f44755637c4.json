{"cell_type":{"6404a423":"code","4010a642":"code","688eadc3":"code","e85783b3":"code","6802aa82":"code","ed6a52f8":"code","79256501":"code","1d43472d":"code","59ce0419":"code","2014bd6d":"code","fe445870":"code","9e1f5442":"code","c4dde1e2":"code","251b1ad1":"code","28285bf1":"markdown","69609df3":"markdown","a2b8c21b":"markdown","77012772":"markdown","e7f87863":"markdown","e53174fa":"markdown","b913b363":"markdown","4b2c525d":"markdown","b266a919":"markdown","c73a4dde":"markdown","c0a95804":"markdown","956632e7":"markdown","c9f60b4e":"markdown"},"source":{"6404a423":"from __future__ import print_function\nfrom __future__ import absolute_import\nfrom __future__ import division\n\n\nimport tensorflow.keras.layers as Layers\nimport tensorflow.keras.activations as Actications\nimport tensorflow.keras.models as Models\nimport tensorflow.keras.optimizers as Optimizer\nimport tensorflow.keras.metrics as Metrics\nimport tensorflow.keras.utils as Utils\nfrom keras.utils.vis_utils import model_to_dot\nimport os\nimport matplotlib.pyplot as plot\nimport cv2\nimport numpy as np\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import confusion_matrix as CM\nfrom random import randint\nfrom IPython.display import SVG\nimport matplotlib.gridspec as gridspec\nfrom keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, SeparableConv2D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.models import Sequential, Model\nfrom keras.optimizers import Adam, SGD, RMSprop\n\n\nimport warnings\n\nfrom keras.models import Model\nfrom keras.layers.core import Dense, Dropout, Activation, Reshape\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose, UpSampling2D\nfrom keras.layers.pooling import AveragePooling2D, MaxPooling2D\nfrom keras.layers.pooling import GlobalAveragePooling2D\nfrom keras.layers import Input\nfrom keras.layers.merge import concatenate\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.regularizers import l2\nfrom keras.utils.layer_utils import convert_all_kernels_in_model, convert_dense_weights_data_format\nfrom keras.utils.data_utils import get_file\nfrom keras.engine.topology import get_source_inputs\nfrom keras_applications.imagenet_utils import _obtain_input_shape\nfrom keras.applications.imagenet_utils import decode_predictions\nimport keras.backend as K\n\n# from subpixel import SubPixelUpscaling","4010a642":"def __conv_block(ip, nb_filter, bottleneck=False, dropout_rate=None, weight_decay=1e-4):\n    ''' Apply BatchNorm, Relu, 3x3 Conv2D, optional bottleneck block and dropout\n    Args:\n        ip: Input keras tensor\n        nb_filter: number of filters\n        bottleneck: add bottleneck block\n        dropout_rate: dropout rate\n        weight_decay: weight decay factor\n    Returns: keras tensor with batch_norm, relu and convolution2d added (optional bottleneck)\n    '''\n    concat_axis = 1 if K.image_data_format() == 'channels_first' else -1\n\n    x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(ip)\n    x = Activation('relu')(x)\n\n    if bottleneck:\n        inter_channel = nb_filter * 4  # Obtained from https:\/\/github.com\/liuzhuang13\/DenseNet\/blob\/master\/densenet.lua\n\n        x = Conv2D(inter_channel, (1, 1), kernel_initializer='he_normal', padding='same', use_bias=False,\n                   kernel_regularizer=l2(weight_decay))(x)\n        x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(x)\n        x = Activation('relu')(x)\n\n    x = Conv2D(nb_filter, (3, 3), kernel_initializer='he_normal', padding='same', use_bias=False)(x)\n    if dropout_rate:\n        x = Dropout(dropout_rate)(x)\n\n    return x\n\n\ndef __dense_block(x, nb_layers, nb_filter, growth_rate, bottleneck=False, dropout_rate=None, weight_decay=1e-4,\n                  grow_nb_filters=True, return_concat_list=False):\n    ''' Build a dense_block where the output of each conv_block is fed to subsequent ones\n    Args:\n        x: keras tensor\n        nb_layers: the number of layers of conv_block to append to the model.\n        nb_filter: number of filters\n        growth_rate: growth rate\n        bottleneck: bottleneck block\n        dropout_rate: dropout rate\n        weight_decay: weight decay factor\n        grow_nb_filters: flag to decide to allow number of filters to grow\n        return_concat_list: return the list of feature maps along with the actual output\n    Returns: keras tensor with nb_layers of conv_block appended\n    '''\n    concat_axis = 1 if K.image_data_format() == 'channels_first' else -1\n\n    x_list = [x]\n\n    for i in range(nb_layers):\n        cb = __conv_block(x, growth_rate, bottleneck, dropout_rate, weight_decay)\n        x_list.append(cb)\n\n        x = concatenate([x, cb], axis=concat_axis)\n\n        if grow_nb_filters:\n            nb_filter += growth_rate\n\n    if return_concat_list:\n        return x, nb_filter, x_list\n    else:\n        return x, nb_filter\n\n\ndef __transition_block(ip, nb_filter, compression=1.0, weight_decay=1e-4):\n    ''' Apply BatchNorm, Relu 1x1, Conv2D, optional compression, dropout and Maxpooling2D\n    Args:\n        ip: keras tensor\n        nb_filter: number of filters\n        compression: calculated as 1 - reduction. Reduces the number of feature maps\n                    in the transition block.\n        dropout_rate: dropout rate\n        weight_decay: weight decay factor\n    Returns: keras tensor, after applying batch_norm, relu-conv, dropout, maxpool\n    '''\n    concat_axis = 1 if K.image_data_format() == 'channels_first' else -1\n\n    x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(ip)\n    x = Activation('relu')(x)\n    x = Conv2D(int(nb_filter * compression), (1, 1), kernel_initializer='he_normal', padding='same', use_bias=False,\n               kernel_regularizer=l2(weight_decay))(x)\n    x = AveragePooling2D((2, 2), strides=(2, 2))(x)\n\n    return x\n\n\ndef __transition_up_block(ip, nb_filters, type='deconv', weight_decay=1E-4):\n    ''' SubpixelConvolutional Upscaling (factor = 2)\n    Args:\n        ip: keras tensor\n        nb_filters: number of layers\n        type: can be 'upsampling', 'subpixel', 'deconv'. Determines type of upsampling performed\n        weight_decay: weight decay factor\n    Returns: keras tensor, after applying upsampling operation.\n    '''\n\n    if type == 'upsampling':\n        x = UpSampling2D()(ip)\n    elif type == 'subpixel':\n        x = Conv2D(nb_filters, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(weight_decay),\n                   use_bias=False, kernel_initializer='he_normal')(ip)\n        x = SubPixelUpscaling(scale_factor=2)(x)\n        x = Conv2D(nb_filters, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(weight_decay),\n                   use_bias=False, kernel_initializer='he_normal')(x)\n    else:\n        x = Conv2DTranspose(nb_filters, (3, 3), activation='relu', padding='same', strides=(2, 2),\n                            kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay))(ip)\n\n    return x\n\n\ndef __create_dense_net(nb_classes, img_input, include_top, depth=40, nb_dense_block=3, growth_rate=12, nb_filter=-1,\n                       nb_layers_per_block=-1, bottleneck=False, reduction=0.0, dropout_rate=None, weight_decay=1e-4,\n                       subsample_initial_block=False, activation='softmax'):\n    ''' Build the DenseNet model\n    Args:\n        nb_classes: number of classes\n        img_input: tuple of shape (channels, rows, columns) or (rows, columns, channels)\n        include_top: flag to include the final Dense layer\n        depth: number or layers\n        nb_dense_block: number of dense blocks to add to end (generally = 3)\n        growth_rate: number of filters to add per dense block\n        nb_filter: initial number of filters. Default -1 indicates initial number of filters is 2 * growth_rate\n        nb_layers_per_block: number of layers in each dense block.\n                Can be a -1, positive integer or a list.\n                If -1, calculates nb_layer_per_block from the depth of the network.\n                If positive integer, a set number of layers per dense block.\n                If list, nb_layer is used as provided. Note that list size must\n                be (nb_dense_block + 1)\n        bottleneck: add bottleneck blocks\n        reduction: reduction factor of transition blocks. Note : reduction value is inverted to compute compression\n        dropout_rate: dropout rate\n        weight_decay: weight decay rate\n        subsample_initial_block: Set to True to subsample the initial convolution and\n                add a MaxPool2D before the dense blocks are added.\n        subsample_initial:\n        activation: Type of activation at the top layer. Can be one of 'softmax' or 'sigmoid'.\n                Note that if sigmoid is used, classes must be 1.\n    Returns: keras tensor with nb_layers of conv_block appended\n    '''\n\n    concat_axis = 1 if K.image_data_format() == 'channels_first' else -1\n\n    if reduction != 0.0:\n        assert reduction <= 1.0 and reduction > 0.0, 'reduction value must lie between 0.0 and 1.0'\n\n    # layers in each dense block\n    if type(nb_layers_per_block) is list or type(nb_layers_per_block) is tuple:\n        nb_layers = list(nb_layers_per_block)  # Convert tuple to list\n\n        assert len(nb_layers) == (nb_dense_block), 'If list, nb_layer is used as provided. ' \\\n                                                   'Note that list size must be (nb_dense_block)'\n        final_nb_layer = nb_layers[-1]\n        nb_layers = nb_layers[:-1]\n    else:\n        if nb_layers_per_block == -1:\n            assert (depth - 4) % 3 == 0, 'Depth must be 3 N + 4 if nb_layers_per_block == -1'\n            count = int((depth - 4) \/ 3)\n\n            if bottleneck:\n                count = count \/\/ 2\n\n            nb_layers = [count for _ in range(nb_dense_block)]\n            final_nb_layer = count\n        else:\n            final_nb_layer = nb_layers_per_block\n            nb_layers = [nb_layers_per_block] * nb_dense_block\n\n    # compute initial nb_filter if -1, else accept users initial nb_filter\n    if nb_filter <= 0:\n        nb_filter = 2 * growth_rate\n\n    # compute compression factor\n    compression = 1.0 - reduction\n\n    # Initial convolution\n    if subsample_initial_block:\n        initial_kernel = (7, 7)\n        initial_strides = (2, 2)\n    else:\n        initial_kernel = (3, 3)\n        initial_strides = (1, 1)\n\n    x = Conv2D(nb_filter, initial_kernel, kernel_initializer='he_normal', padding='same',\n               strides=initial_strides, use_bias=False, kernel_regularizer=l2(weight_decay))(img_input)\n\n    if subsample_initial_block:\n        x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(x)\n        x = Activation('relu')(x)\n        x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n\n    # Add dense blocks\n    for block_idx in range(nb_dense_block - 1):\n        x, nb_filter = __dense_block(x, nb_layers[block_idx], nb_filter, growth_rate, bottleneck=bottleneck,\n                                     dropout_rate=dropout_rate, weight_decay=weight_decay)\n        # add transition_block\n        x = __transition_block(x, nb_filter, compression=compression, weight_decay=weight_decay)\n        nb_filter = int(nb_filter * compression)\n\n    # The last dense_block does not have a transition_block\n    x, nb_filter = __dense_block(x, final_nb_layer, nb_filter, growth_rate, bottleneck=bottleneck,\n                                 dropout_rate=dropout_rate, weight_decay=weight_decay)\n\n    x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(x)\n    x = Activation('relu')(x)\n    x = GlobalAveragePooling2D()(x)\n\n    if include_top:\n        x = Dense(nb_classes, activation=activation)(x)\n\n    return x\n\n","688eadc3":"def DenseNet(input_shape=None, depth=40, nb_dense_block=3, growth_rate=12, nb_filter=-1, nb_layers_per_block=-1,\n             bottleneck=False, reduction=0.0, dropout_rate=0.0, weight_decay=1e-4, subsample_initial_block=False,\n             include_top=True, weights=None, input_tensor=None,\n             classes=10, activation='softmax'):\n\n\n    if weights not in {'imagenet', None}:\n        raise ValueError('The `weights` argument should be either '\n                         '`None` (random initialization) or `cifar10` '\n                         '(pre-training on CIFAR-10).')\n\n    if weights == 'imagenet' and include_top and classes != 1000:\n        raise ValueError('If using `weights` as ImageNet with `include_top`'\n                         ' as true, `classes` should be 1000')\n\n    if activation not in ['softmax', 'sigmoid']:\n        raise ValueError('activation must be one of \"softmax\" or \"sigmoid\"')\n\n    if activation == 'sigmoid' and classes != 1:\n        raise ValueError('sigmoid activation can only be used when classes = 1')\n\n    # Determine proper input shape\n    input_shape = _obtain_input_shape(input_shape,\n                                      default_size=32,\n                                      min_size=8,\n                                      data_format=K.image_data_format(),\n                                      require_flatten=include_top)\n\n    if input_tensor is None:\n        img_input = Input(shape=input_shape)\n    else:\n        if not K.is_keras_tensor(input_tensor):\n            img_input = Input(tensor=input_tensor, shape=input_shape)\n        else:\n            img_input = input_tensor\n\n    x = __create_dense_net(classes, img_input, include_top, depth, nb_dense_block,\n                           growth_rate, nb_filter, nb_layers_per_block, bottleneck, reduction,\n                           dropout_rate, weight_decay, subsample_initial_block, activation)\n\n    # Ensure that the model takes into account\n    # any potential predecessors of `input_tensor`.\n    if input_tensor is not None:\n        inputs = get_source_inputs(input_tensor)\n    else:\n        inputs = img_input\n    # Create model.\n    model = Model(inputs, x, name='densenet')\n\n\n    return model\n","e85783b3":"model = DenseNet((150, 150, 3), depth=64, nb_dense_block=3,growth_rate=4, bottleneck=True, reduction=0.5, weights=None, classes=6)","6802aa82":"model.summary()","ed6a52f8":"def get_images(directory):\n    Images = []\n    Labels = []  # 0 for Building , 1 for forest, 2 for glacier, 3 for mountain, 4 for Sea , 5 for Street\n    label = 0\n    \n    for labels in os.listdir(directory): #Main Directory where each class label is present as folder name.\n        if labels == 'glacier': #Folder contain Glacier Images get the '2' class label.\n            label = 2\n        elif labels == 'sea':\n            label = 4\n        elif labels == 'buildings':\n            label = 0\n        elif labels == 'forest':\n            label = 1\n        elif labels == 'street':\n            label = 5\n        elif labels == 'mountain':\n            label = 3\n        \n        for image_file in os.listdir(directory+labels): #Extracting the file name of the image from Class Label folder\n            image = cv2.imread(directory+labels+r'\/'+image_file) #Reading the image (OpenCV)\n            image = cv2.resize(image,(150,150)) #Resize the image, Some images are different sizes. (Resizing is very Important)\n            Images.append(image)\n            Labels.append(label)\n        \n#     return Images, Labels\n    return shuffle(Images,Labels,random_state=817328462) #Shuffle the dataset you just prepared.\n\ndef get_classlabel(class_code):\n    labels = {2:'glacier', 4:'sea', 0:'buildings', 1:'forest', 5:'street', 3:'mountain'}\n    \n    return labels[class_code]","79256501":"Images, Labels = get_images('..\/input\/seg_train\/seg_train\/') #Extract the training images from the folders.\n\nImages = np.array(Images, dtype=np.float32) #converting the list of images to numpy array.\nImages = Images\/255.0\nLabels = np.array(Labels)","1d43472d":"print(\"Shape of Images:\",Images.shape)\nprint(\"Shape of Labels:\",Labels.shape)","59ce0419":"f,ax = plot.subplots(5,5) \nf.subplots_adjust(0,0,3,3)\nfor i in range(0,5,1):\n    for j in range(0,5,1):\n        rnd_number = randint(0,len(Images))\n        ax[i,j].imshow(Images[rnd_number])\n        ax[i,j].set_title(get_classlabel(Labels[rnd_number]))\n        ax[i,j].axis('off')","2014bd6d":"from keras.utils import to_categorical\nLabels = to_categorical(Labels)","fe445870":"\"\"\"\nDefine our custom loss function.\n\"\"\"\nfrom keras import backend as K\nimport tensorflow as tf\n\ndef binary_focal_loss(gamma=2., alpha=.25):\n    \"\"\"\n    Binary form of focal loss.\n      FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)\n      where p = sigmoid(x), p_t = p or 1 - p depending on if the label is 1 or 0, respectively.\n    References:\n        https:\/\/arxiv.org\/pdf\/1708.02002.pdf\n    Usage:\n     model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n    \"\"\"\n    def binary_focal_loss_fixed(y_true, y_pred):\n        \"\"\"\n        :param y_true: A tensor of the same shape as `y_pred`\n        :param y_pred:  A tensor resulting from a sigmoid\n        :return: Output tensor.\n        \"\"\"\n        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n\n        epsilon = K.epsilon()\n        # clip to prevent NaN's and Inf's\n        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n\n        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) \\\n               -K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n\n    return binary_focal_loss_fixed\n\n\ndef categorical_focal_loss(gamma=2., alpha=.25):\n    \"\"\"\n    Softmax version of focal loss.\n           m\n      FL = \u2211  -alpha * (1 - p_o,c)^gamma * y_o,c * log(p_o,c)\n          c=1\n      where m = number of classes, c = class and o = observation\n    Parameters:\n      alpha -- the same as weighing factor in balanced cross entropy\n      gamma -- focusing parameter for modulating factor (1-p)\n    Default value:\n      gamma -- 2.0 as mentioned in the paper\n      alpha -- 0.25 as mentioned in the paper\n    References:\n        Official paper: https:\/\/arxiv.org\/pdf\/1708.02002.pdf\n        https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/backend\/categorical_crossentropy\n    Usage:\n     model.compile(loss=[categorical_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n    \"\"\"\n    def categorical_focal_loss_fixed(y_true, y_pred):\n        \"\"\"\n        :param y_true: A tensor of the same shape as `y_pred`\n        :param y_pred: A tensor resulting from a softmax\n        :return: Output tensor.\n        \"\"\"\n\n        # Scale predictions so that the class probas of each sample sum to 1\n        y_pred \/= K.sum(y_pred, axis=-1, keepdims=True)\n\n        # Clip the prediction value to prevent NaN's and Inf's\n        epsilon = K.epsilon()\n        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n\n        # Calculate Cross Entropy\n        cross_entropy = -y_true * K.log(y_pred)\n\n        # Calculate Focal Loss\n        loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy\n\n        # Sum the losses in mini_batch\n        return K.sum(loss, axis=1)\n\n    return categorical_focal_loss_fixed","9e1f5442":"optimizer = Adam(lr=0.0001, decay=1e-5)\nmodel.compile(loss=[categorical_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=optimizer)","c4dde1e2":"trained = model.fit(Images, Labels, epochs=35, shuffle=True, validation_split=0.3)","251b1ad1":"plot.plot(trained.history['acc'])\nplot.plot(trained.history['val_acc'])\nplot.title('Model accuracy')\nplot.ylabel('Accuracy')\nplot.xlabel('Epoch')\nplot.legend(['Train', 'Test'], loc='upper left')\nplot.show()\n\nplot.plot(trained.history['loss'])\nplot.plot(trained.history['val_loss'])\nplot.title('Model loss')\nplot.ylabel('Loss')\nplot.xlabel('Epoch')\nplot.legend(['Train', 'Test'], loc='upper left')\nplot.show()","28285bf1":"**We are going to apply focal loss**","69609df3":"Now, we are going to define convolutional block, dense block, transitional block. This source code was from github densenet keras version.","a2b8c21b":"The image was resized into (150, 150) so the input of the network is (150, 150, 3), we will test simple model so growth rate is 4, depth 64, and 3 of dense block were defined.","77012772":"This DenseNet function is to build up desely connected covolutional network.","e7f87863":"We are going to do categorical classification, we need to do input image normalization with maximum pixel value 255 and, one_hot encoding with to_categorical function.","e53174fa":"Let's change digits in the label to categorical vector","b913b363":"Now, lets train the model with validation split equal to 30% from training data. \nValidation split is used to determine that our model is not geting over-fitted.","4b2c525d":"Here is to make a one hot encoded vector from value of label.","b266a919":"Let's check the model with summary function.","c73a4dde":"The Training data is in shape of (Number of Training Images, Width of image, Height of image, Channel of image). ","c0a95804":"Let us look some random images of our dataset.","956632e7":"The model will be trained with categorical focal loss.","c9f60b4e":"Hello, Kagglers!\n\nIn this kernel, I will show you how to build DenseNet architecture in Tensorflow Keras and the model will be trained by focal loss.\n\nFirst, import the important libraries."}}