{"cell_type":{"1137461a":"code","ca9fc200":"code","faf8e82d":"code","5ead4831":"code","c01ee47c":"code","d6c62e71":"code","474c7e22":"code","c3794ed6":"code","0a7fe47e":"code","3ad1de5f":"code","7e5a3d82":"code","1d345034":"code","b23323db":"code","e0ff4c78":"code","863b7421":"code","166569d7":"code","3ce10109":"code","779a0493":"code","39a21b7d":"markdown","8ea7b28d":"markdown","c19f3494":"markdown","195e591a":"markdown"},"source":{"1137461a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ca9fc200":"import re\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport seaborn as sns\nimport nltk\nfrom nltk.corpus import stopwords \n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras import layers, Sequential\nfrom tensorflow.keras import optimizers","faf8e82d":"train_data = pd.read_csv(r'\/kaggle\/input\/nlp-getting-started\/train.csv', encoding='latin_1')\ntrain_data.info()\ntrain_data.head()","5ead4831":"train_data.isnull().sum()","c01ee47c":"train_x = train_data['text'].copy()\ntrain_y = train_data['target'].copy()","d6c62e71":"train_x.head()","474c7e22":"sns.set_style('whitegrid')\nsns.set(rc={'figure.figsize':(12,6)})\nsns.countplot(train_y)","c3794ed6":"stop = stopwords.words('english')\ndef clean(text):\n    \n    text = re.sub(r'http\\S+', ' ', text)\n    \n    text = re.sub(r'<.*?>', ' ', text)    \n    \n    text = re.sub(r'#\\w+', ' ', text)    \n     \n    text = re.sub(r'@\\w+', ' ', text)\n    \n    text = re.sub(r'\\d+', ' ', text)\n    \n    text = text.split()\n    \n    text = ' '.join([word for word in text if word not in stop])\n    \n    return text","0a7fe47e":"train_x_cleaned = train_x.apply(clean)\ntrain_x_cleaned.head()","3ad1de5f":"max_len = max(train_x_cleaned.apply(len))\nprint('max length: {}'.format(max_len))","7e5a3d82":"\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_x_cleaned)\nvocab_size = len(tokenizer.word_index) + 1\nx = tokenizer.texts_to_sequences(train_x_cleaned)\nx = pad_sequences(x, max_len, padding='post')\ny = train_y\nprint('train_x_clean:', train_x_cleaned[4])\nprint('*'*50)\nprint('x:',x[5])\nprint('vocabulary size:{}'.format(vocab_size))","1d345034":"print(x.shape, y.shape)\n","b23323db":"\nepoch_size =10\nbatch_size = 32\nembedding_dim = 16\noptimizer = optimizers.Adam(lr=3e-4)\n\nmodel = Sequential([\n    layers.Embedding(vocab_size, embedding_dim, input_length=max_len),\n    layers.Bidirectional(layers.GRU(256, return_sequences=True)),\n    layers.GlobalMaxPool1D(),\n#     layers.Dense(128, activation='relu'),\n#     layers.Dropout(0.4),\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(0.4),\n    layers.Dense(2, activation='sigmoid')\n])\nmodel.summary()","e0ff4c78":"model.compile(loss='sparse_categorical_crossentropy', optimizer = 'adam', metrics=['accuracy'])\nmodel.fit(x, y, epochs=epoch_size, validation_split=0.1)","863b7421":"test_data = pd.read_csv(r'\/kaggle\/input\/nlp-getting-started\/test.csv', encoding='latin_1')\ntest_data.head()","166569d7":"test_x = test_data['text'].copy()\ntest_x = test_x.apply(clean)\ntest_x = tokenizer.texts_to_sequences(test_x)\ntest_x = pad_sequences(test_x, max_len, padding='post')","3ce10109":"test_pred = np.argmax(model.predict(test_x), axis=1)\nprint(test_pred)","779a0493":"submission = pd.DataFrame({'id':test_data['id'], 'target':test_pred})\nsubmission.to_csv('submission.csv', index=False)","39a21b7d":"# clean data","8ea7b28d":"# Read data","c19f3494":"# Tokenize","195e591a":"# model"}}