{"cell_type":{"692adb23":"code","4e740ce8":"code","88a8530f":"code","250c5c5b":"code","662e495f":"code","93977e25":"code","5d02c49f":"code","9e5e09bd":"code","21ad930d":"code","189d96a6":"code","7cb151a2":"code","b4cd5a50":"code","63ddf76f":"code","2ea63656":"code","c59709be":"code","94a97ead":"code","69b71980":"code","3ff00f85":"code","2855bb86":"code","946107a1":"code","13434f19":"code","630d85c0":"code","c62d87c8":"code","a1f06759":"code","bcfd9b14":"code","0b79a4a6":"code","c95e045f":"code","76837078":"code","eeef7395":"markdown","0d4cb5be":"markdown"},"source":{"692adb23":"import torch\nimport torchvision\nimport torch.nn as nn\nimport numpy as np\nimport os\nimport copy\nimport time\nfrom torchvision import datasets, models, transforms","4e740ce8":"mean = np.array([0.5,0.5,0.5])\nstd = np.array([0.25,0.25,0.25])","88a8530f":"data_transforms = {\n    'train': transforms.Compose([\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std)\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std)\n    ]),\n}","250c5c5b":"data_dir = 'data\/hymenoptera_data'","662e495f":"image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n                                             shuffle=True, num_workers=0)\n              for x in ['train', 'val']}","93977e25":"dataset_sizes = {x : len(image_datasets[x]) for x in ['train','val']}","5d02c49f":"class_names = image_datasets['train'].classes","9e5e09bd":"print(class_names)","21ad930d":"print(dataset_sizes)","189d96a6":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n# device = torch.device('cpu')","7cb151a2":"print(device)","b4cd5a50":"model = torchvision.models.resnet18(pretrained=True)","63ddf76f":"num_features = model.fc.in_features","2ea63656":"print(num_features)","c59709be":"model.fc = nn.Linear(num_features,2)","94a97ead":"model = model.to(device)","69b71980":"loss = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(),lr = 0.01)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size = 7, gamma = 0.1)","3ff00f85":"def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for i,(inputs, labels) in enumerate(dataloaders[phase]):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        optimizer.zero_grad()\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            if phase == 'train':\n                scheduler.step()\n\n            epoch_loss = running_loss \/ dataset_sizes[phase]\n            epoch_acc = running_corrects.double() \/ dataset_sizes[phase]\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model","2855bb86":"# model.fc = nn.Linear(num_ftrs, 2)\n\n# model = model.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n\n# Observe that all parameters are being optimized\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n\n# StepLR Decays the learning rate of each parameter group by gamma every step_size epochs\n# Decay LR by a factor of 0.1 every 7 epochs\n# Learning rate scheduling should be applied after optimizer\u2019s update\n# e.g., you should write your code this way:\n# for epoch in range(100):\n#     train(...)\n#     validate(...)\n#     scheduler.step()\n\nstep_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n\nmodel = train_model(model, criterion, optimizer, step_lr_scheduler, num_epochs=2)\n","946107a1":"model = torchvision.models.resnet18(pretrained=True)\nfor param in model.parameters():\n    param.requires_grad = False","13434f19":"num_features = model.fc.in_features","630d85c0":"print(num_features)","c62d87c8":"model.fc = nn.Linear(num_features,2)","a1f06759":"model = model.to(device)","bcfd9b14":"criterion = nn.CrossEntropyLoss()","0b79a4a6":"optimizer = torch.optim.SGD(model.fc.parameters(),lr = 0.01)","c95e045f":"lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=7,gamma = 0.1)","76837078":"model = train_model(model,criterion,optimizer,lr_scheduler,num_epochs = 2)","eeef7395":"## Freezing Layer","0d4cb5be":"## Train Model"}}