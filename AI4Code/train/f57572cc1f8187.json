{"cell_type":{"23939577":"code","b36a4ca3":"code","21a27eba":"code","f50b1e2f":"code","98c0c2ab":"code","2e4d794e":"code","da92f774":"code","ba3f700d":"code","142f0b8a":"code","fcaac075":"code","ab2c7bab":"code","554d4fdd":"code","68482c32":"code","f5284fd7":"code","16130c0f":"code","1d92b88f":"code","ff91b8ff":"code","5f0dddf1":"code","540cfeab":"code","3e7b2237":"code","173611cb":"code","2a4fba03":"code","ce52eb14":"code","5face4e4":"code","16f0f872":"code","158df130":"code","21718376":"code","db609b84":"code","6ab6b0bb":"code","5be4983f":"code","2122fe12":"code","b245e2bb":"code","ad82e0d0":"markdown","c849f462":"markdown","cc63be99":"markdown","555f0076":"markdown","187c14bb":"markdown","8d0e8c65":"markdown","90a19e99":"markdown"},"source":{"23939577":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom xgboost import XGBRegressor\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\nimport random\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b36a4ca3":"train = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/test.csv')","21a27eba":"train.head()","f50b1e2f":"train.describe()","98c0c2ab":"# visualizing the distribution of the target variable\nplt.figure(figsize = (12,8))\nsns.histplot(train['target'], bins = 30, color = 'r')\nplt.title('Target Distribution');","2e4d794e":"# making a list of categorical columns in the dataset\ncategorical_col = [col for col in train.columns[:-1] if train[col].dtype == 'object']","da92f774":"# making a list of numerical columns in the dataset\nnumerical_col = [col for col in train.columns[:-1] if train[col].dtype in ['float64', 'int64'] and not col == 'id']","ba3f700d":"plt.figure(figsize=(15,10))\nsns.heatmap(train[numerical_col+['target']].corr(), annot = True, cmap = 'plasma')\nplt.xticks(rotation = 90)\nplt.yticks(rotation = 0);","142f0b8a":"train[categorical_col].nunique()","fcaac075":"# Colors to be used for plots\ncolors = [\"lightcoral\", \"sandybrown\", \"darkorange\", \"mediumseagreen\",\n          \"lightseagreen\", \"cornflowerblue\", \"mediumpurple\", \"palevioletred\",\n          \"lightskyblue\", \"sandybrown\", \"yellowgreen\", \"indianred\",\n          \"lightsteelblue\", \"mediumorchid\", \"deepskyblue\"]","ab2c7bab":"#lets see the distribution of different subcategories within categorical variables\nfor i in categorical_col:\n    print(train[i].value_counts()\/train[i].count())\n    print('*'*90)","554d4fdd":"#visualizing the subcatecories in categorical columns\nplt.figure(figsize=(15,20))\nfor i, variable in enumerate(categorical_col):\n                     plt.subplot(5,3,i+1)\n                       \n                     (train[variable].value_counts()\/train[variable].count()).plot(kind = 'barh', color = random.choice(colors))\n                     plt.tight_layout()\n                     plt.title(variable)","68482c32":"plt.figure(figsize=(15,20))\nfor i, variable in enumerate(categorical_col):\n                     plt.subplot(5,3,i+1)\n                       \n                     sns.boxplot(x = train[variable], y = train['target'])\n                     plt.tight_layout()\n                     plt.title(variable)","f5284fd7":"plt.figure(figsize=(15,20))\nfor i, variable in enumerate(numerical_col):\n                     plt.subplot(5,3,i+1)\n                       \n                     sns.histplot(train[variable],bins = 30, color=random.choice(colors))\n                     plt.tight_layout()\n                     plt.title(variable)","16130c0f":"# for setting the size of the graph\nplt.figure(figsize=(15,40))\n\nfor i, variable in enumerate(categorical_col+numerical_col):\n                     #subplots as per the number of rows and columns\n                     plt.subplot(10,3,i+1)\n                \n                     sns.scatterplot(train[variable], y = train['target'], color = random.choice(colors))\n                     plt.tight_layout()\n                     plt.title(variable)","1d92b88f":"train.shape","ff91b8ff":"# doing a train test split to avoid overfitting\nX = train[numerical_col+categorical_col]\ny = train['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=48)","5f0dddf1":"#Label encoding the categorical variables to find the mutual information\nordinal = OrdinalEncoder()\n# fit only the train data to avoid overfitting and transform into a dataframe\nlabel_X_train = pd.DataFrame(ordinal.fit_transform(X_train[categorical_col]), \n                             columns = X_train[categorical_col].columns)\nlabel_X_test = pd.DataFrame(ordinal.transform(X_test[categorical_col]), \n                             columns = X_test[categorical_col].columns)","540cfeab":"# indexing the columns\nlabel_X_train.index = X_train.index\nlabel_X_test.index = X_test.index","3e7b2237":"# final train and validation set after preprocessing the datax \nX_train_fin = pd.concat([X_train[numerical_col], label_X_train], axis = 1)\nX_test_fin = pd.concat([X_test[numerical_col], label_X_test], axis = 1)","173611cb":"# randomized search for hyperparameter tuning\nparams = {'learning_rate': [0.1,0.25],\n              'subsample': [0.8,0.85,0.9],\n              'colsample_bytree': [0.6,0.7,0.8],\n              'max_depth': [2]}\n\nmodel = RandomizedSearchCV(XGBRegressor(n_estimators = 10000,\n                  booster = 'gbtree', \n                  reg_lambda = 50,\n                  reg_alpha = 50,\n                  random_state = 42,\n                  n_jobs = -1), params, verbose = 3, cv = 5)","2a4fba03":"model.fit(X_train_fin, y_train,\n                  verbose=False,\n                  # To prevent overfitting \n                  eval_set=[(X_train_fin, y_train), (X_test_fin, y_test)],\n                  eval_metric=\"rmse\",\n                  early_stopping_rounds=100\n                  )","ce52eb14":"model.best_estimator_","5face4e4":"model.best_params_","16f0f872":"pred = model.predict(X_test_fin)","158df130":"print(np.sqrt(mean_squared_error(y_test, pred)))","21718376":"#pre processing the training data\nlabel_X_train_fin = pd.DataFrame(ordinal.fit_transform(X[categorical_col]), \n                             columns = X[categorical_col].columns)\nlabel_X_train_fin.index = X.index\n\nX_train_final = pd.concat([X[numerical_col], label_X_train_fin], axis = 1)","db609b84":"#pre processing the test dataset\ntest_fin = pd.DataFrame(ordinal.transform(test[categorical_col]), \n                             columns = test[categorical_col].columns)\ntest_fin.index = test.index\n\ntest_final = pd.concat([test[numerical_col], test_fin], axis = 1)","6ab6b0bb":"# Model hyperparameters\nxgb_params = {'n_estimators': 10000,\n              'learning_rate': 0.1,\n              'subsample': 0.9,\n              'colsample_bytree': 0.7,\n              'max_depth': 2,\n              'booster': 'gbtree', \n              'reg_lambda': 50,\n              'reg_alpha': 50,\n              'random_state': 42,\n              'n_jobs': -1}","5be4983f":"#training the entire training dataset on the parameters\nxgb = XGBRegressor(**xgb_params)\nxgb.fit(X_train_final, y, verbose = False)","2122fe12":"pred = xgb.predict(test_final)","b245e2bb":"predictions = pd.DataFrame()\npredictions[\"id\"] = test[\"id\"]\npredictions[\"target\"] = pred\n\npredictions.to_csv('submission.csv', index=False, header=predictions.columns)\npredictions.head()","ad82e0d0":"#### 'cat7', 'cat6' & 'cat4' have one category with more than 90% values","c849f462":"### Lets take a look at the categorical variables","cc63be99":"We can see there is very weak correlation between the target and numerical variables","555f0076":"### Plotting heatmap for corellation between variables","187c14bb":"### Lets see the distribution of the numerical variables","8d0e8c65":"### Looking at the corellation between independent variables and target variable","90a19e99":"### Making the final predictions on the test dataset using the hyperparameters"}}