{"cell_type":{"b7bc256c":"code","ea2653a9":"code","16f605dd":"code","dffab5f0":"code","8ea8e3d5":"code","24de8f00":"code","0b548fcc":"code","5925a5aa":"code","74fb06b9":"code","77438c1a":"code","2aa38bf0":"code","a688fe00":"code","cc3733db":"code","956591ca":"code","5481d9d5":"code","70bc1029":"code","9ea8d5de":"code","c3457562":"code","7a3f98db":"code","c51cb6f2":"code","a7616159":"code","37aab0c0":"code","15a4f6e7":"code","8dcfde9b":"code","c86ee6e4":"code","796f3657":"code","3b27040d":"code","90d5fa50":"code","d746e6e9":"code","3d2a9d37":"code","9d4f24c2":"code","bb14c36f":"markdown","30d40ca1":"markdown","1e2c7885":"markdown","9139cc1f":"markdown","9e067c91":"markdown","4a10ee49":"markdown","ab73d414":"markdown","5e2bd887":"markdown","d7f00b70":"markdown"},"source":{"b7bc256c":"!pip install -qq av","ea2653a9":"!pip install facenet-pytorch > \/dev\/null 2>&1\n!apt install zip > \/dev\/null 2>&1","16f605dd":"from google.colab import drive\ndrive.mount('\/content\/drive')","dffab5f0":"!rm -rf reface-fake-detection","8ea8e3d5":"!unzip -qq \/content\/drive\/MyDrive\/dl-creator-school\/reface-fake-detection.zip -d reface-fake-detection","24de8f00":"import os\nimport glob\nimport json\nimport cv2\n\nimport pandas as pd\nimport numpy as np\n\nimport torch\nimport torch.nn.functional as F\nimport torchvision\n\nfrom torch import nn, optim\nfrom torch.utils.data import sampler\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.utils import data\nfrom torchvision import transforms, datasets\nfrom torchsummary import summary\n\nfrom facenet_pytorch import MTCNN\n\nfrom sklearn.model_selection import train_test_split\n\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom joblib import Parallel, delayed\n\nfrom typing import List, Dict, Tuple, Union, Optional\nfrom pathlib import Path","0b548fcc":"import matplotlib.pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nplt.rcParams['figure.dpi'] = 150","5925a5aa":"device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\nprint(f'Running on device: {device}')","74fb06b9":"PATH2PROJECT = Path('')\nPATH2DRIVE = Path('\/content\/drive\/MyDrive\/dl-creator-school\/')\n\nPATH2DATA = PATH2PROJECT \/ 'reface-fake-detection'\nPATH2TRAIN = PATH2DATA \/ 'train'\nPATH2TEST = PATH2DATA \/ 'test'\n# PATH2SUBMISSIONS = PATH2DRIVE \/ 'submissions'\n# PATH2CHECKOUTS = PATH2DRIVE \/ 'checkouts'\n\nPATH2RESULT = PATH2PROJECT \/ 'reface-fake-detection-result'\nPATH2RESULT_TRAIN = PATH2RESULT \/ 'train'\nPATH2RESULT_TEST = PATH2RESULT \/ 'test'","77438c1a":"# try: PATH2SUBMISSIONS.mkdir()\n# except: pass\n# try: PATH2CHECKOUTS.mkdir()\n# except: pass\ntry: PATH2RESULT.mkdir()\nexcept: pass\ntry: PATH2RESULT_TRAIN.mkdir()\nexcept: pass\ntry: PATH2RESULT_TEST.mkdir()\nexcept: pass","2aa38bf0":"SEED = 42\nN_FRAMES = 8","a688fe00":"meta_df = pd.read_csv(PATH2DATA \/ 'train.csv')\nmeta_df.shape","cc3733db":"meta_df.sample(n=5, random_state=SEED)","956591ca":"meta_df.label.value_counts(normalize=True)","5481d9d5":"t = os.listdir(PATH2TRAIN \/ 'train')\ntemp_filename = t[np.random.randint(0, high=len(t))]\ndata = torchvision.io.read_video(str(PATH2TRAIN \/ 'train' \/ temp_filename), pts_unit='sec')\ndel t","70bc1029":"vid = data[0]\naud = data[1]\n\nprint(\"Video Shape: {}\\nAudio Shape: {}\\n\".format(vid.shape, aud.shape))","9ea8d5de":"plt.imshow(vid[10])\nplt.title(f\"DeepFake Dataset: {temp_filename}; label={meta_df[meta_df.filename == temp_filename].label.values[0]}\", loc='left', fontsize=8, pad=5);\nplt.axis(\"off\");","c3457562":"class FaceExtractor:\n    def __init__(self, detector, n_frames=None):\n        \"\"\"\n        Parameters:\n            n_frames {int} -- Total number of frames to load. These will be evenly spaced\n                throughout the video. If not specified (i.e., None), all frames will be loaded.\n                (default: {None})\n            resize {float} -- Fraction by which to resize frames from original prior to face\n                detection. A value less than 1 results in downsampling and a value greater than\n                1 result in upsampling. (default: {None})\n        \"\"\"\n\n        self.detector = detector\n        self.n_frames = n_frames\n    \n    def __call__(self, filename, save_dir):\n        \"\"\"Load frames from an MP4 video, detect faces and save the results.\n\n        Parameters:\n            filename {str} -- Path to video.\n            save_dir {str} -- The directory where results are saved.\n        \"\"\"\n\n        # Create video reader and find length\n        v_cap = cv2.VideoCapture(filename)\n        v_len = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n        # Pick 'n_frames' evenly spaced frames to sample\n        if self.n_frames is None:\n            sample = np.arange(0, v_len)\n        else:\n            sample = np.linspace(0, v_len - 1, self.n_frames).astype(int)\n\n        # Loop through frames\n        for j in range(v_len):\n            success = v_cap.grab()\n            if j in sample:\n                # Load frame\n                success, frame = v_cap.retrieve()\n                if not success:\n                    continue\n                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                frame = Image.fromarray(frame)\n                \n                save_path = os.path.join(save_dir, f'{j}.png')\n\n                self.detector([frame], save_path=save_path)\n\n        v_cap.release()","7a3f98db":"# Load face detector\nface_detector = MTCNN(margin=14, keep_all=True, factor=0.5, device=device).eval()","c51cb6f2":"# Define face extractor\nface_extractor = FaceExtractor(detector=face_detector, n_frames=N_FRAMES)","a7616159":"# Get the paths of all train videos\nall_train_videos = glob.glob(os.path.join(PATH2TRAIN \/ 'train', '*.mp4'))\nall_res_train_videos = os.listdir(PATH2RESULT_TRAIN)\n\nall_train_videos = [path for path in all_train_videos if (path.split('\/')[-1].split('.')[0] not in all_res_train_videos)]","37aab0c0":"len(glob.glob(os.path.join(PATH2TRAIN \/ 'train', '*.mp4'))), len(all_train_videos)","15a4f6e7":"# Get the paths of all test videos\nall_test_videos = glob.glob(os.path.join(PATH2TEST \/ 'test', '*.mp4'))\nall_res_test_videos = os.listdir(PATH2RESULT_TEST)\n\nall_test_videos = [path for path in all_test_videos if (path.split('\/')[-1].split('.')[0] not in all_res_test_videos)]","8dcfde9b":"def compute_face_extractor(path, save_path):\n    file_name = path.split('\/')[-1]\n\n    save_dir = os.path.join(save_path, file_name.split(\".\")[0])\n\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    # Detect all faces appear in the video and save them.\n    face_extractor(path, save_dir)","c86ee6e4":"with torch.no_grad():\n    Parallel(n_jobs=-1, verbose=5)(delayed(compute_face_extractor)(path, PATH2RESULT_TRAIN) for path in tqdm(all_train_videos))","796f3657":"with torch.no_grad():\n    Parallel(n_jobs=-1, verbose=5)(delayed(compute_face_extractor)(path, PATH2RESULT_TEST) for path in tqdm(all_test_videos))","3b27040d":"meta_df.to_csv(PATH2RESULT \/ 'train.csv', index=False)","90d5fa50":"!tar -zcvf reface-fake-detection-result.tar.gz reface-fake-detection-result","d746e6e9":"!rm \/content\/drive\/MyDrive\/dl-creator-school\/reface-fake-detection.zip","3d2a9d37":"!cp reface-fake-detection-result.tar.gz \/content\/drive\/MyDrive\/dl-creator-school\/reface-fake-detection-result.tar.gz","9d4f24c2":"!ls \/content\/drive\/MyDrive\/dl-creator-school","bb14c36f":"## Install external modules and load our data","30d40ca1":"## Face extraction process","1e2c7885":"## Problem","9139cc1f":"## Settings","9e067c91":"## Define Face Extractor","4a10ee49":"## Modules importing","ab73d414":"## Show few frames","5e2bd887":"**Task [[kaggle](https:\/\/www.kaggle.com\/c\/reface-fake-detection)]:** recognize fake videos. You need to train the binary classifier to distinguish real videos from fake ones (the provided fake data is the result of the technologies developed in Reface).\n\n****\n\n### What I should get?\n\nIn order to complete this stage, you should meet one of 2 conditions below:\n+ either make a solution with a minimum target metric value of 0.92475\n+ or be in the top 30 of all competitors.\n\n****\n\n### Evaluation\n\nThe evaluation metric for this competition is F1-Score, average='micro'. The F1 score, commonly used in information retrieval, measures accuracy using the statistics precision p and recall r. Precision is the ratio of true positives (tp) to all predicted positives (tp + fp). Recall is the ratio of true positives to all actual positives (tp + fn).\n\nThe F1 metric weights recall and precision equally, and a good retrieval algorithm will maximize both precision and recall simultaneously. Thus, moderately good performance on both will be favored over extremely good performance on one and poor performance on the other.\n\nMore information you can find at sklearn docs:\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.f1_score.html\n\n****\n\n### Submission\n\nFor each filename in the test set, you must predict either this file is fake video (label 1) or this file is real video (label 0). The file should contain a header and have the following format:\n\n```\nfilename,label\n004582.mp4,1\n003603.mp4,0\n```","d7f00b70":"## Training metadata"}}