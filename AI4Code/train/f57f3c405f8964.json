{"cell_type":{"09e5a7d0":"code","d2085883":"code","72e148ab":"code","dc546b31":"code","ce4f8919":"code","b231a6a9":"code","c8c0d75b":"code","adc2368f":"code","aac49a0c":"code","8e4dbd94":"code","1b73f20e":"code","16d0d3f8":"code","4f3ffb2d":"code","1d7a3b08":"code","7ea8a7cd":"code","aab3232e":"code","5828d43c":"code","06ee7fe7":"code","71313155":"code","a507a2bd":"code","b1c65f1f":"code","eb9368c2":"code","76560a37":"markdown","f80cb75b":"markdown","09d7ee34":"markdown","9d39001a":"markdown","ae3fff26":"markdown","d057fc37":"markdown","87751069":"markdown","0cd41ab0":"markdown","0333596a":"markdown","f54febf1":"markdown","93c6a71c":"markdown","4631acc4":"markdown"},"source":{"09e5a7d0":"pip install -U lightautoml","d2085883":"%matplotlib inline\n\n# Standard python libraries\nimport os\nimport time\nimport re\n\n# Installed libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport matplotlib.pyplot as plt\n\n# Imports from our package\nfrom lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\nfrom lightautoml.dataset.roles import DatetimeRole\nfrom lightautoml.tasks import Task\nfrom lightautoml.utils.profiler import Profiler","72e148ab":"N_THREADS = 4 # threads cnt for lgbm and linear models\nN_FOLDS = 10 # folds cnt for AutoML\nRANDOM_STATE = 42 # fixed random state for various reasons\nTEST_SIZE = 0.2 # Test size for metric check\nTIMEOUT = 3*3600 # Time in seconds for automl run","dc546b31":"np.random.seed(RANDOM_STATE)\ntorch.set_num_threads(N_THREADS)","ce4f8919":"%%time\n\ntrain_data = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/train.csv')\ntrain_data.head()","b231a6a9":"test_data = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/test.csv')\ntest_data.head()","c8c0d75b":"submission = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/sample_submission.csv')\nsubmission.head()","adc2368f":"# Now we use our almost best submission\npseudo_label_test = pd.read_csv('..\/input\/n3-tps-april-21-lightautoml-starter\/LightAutoML_compose_version_25.csv')\npseudo_label_test","aac49a0c":"test_data['Survived'] = pseudo_label_test['Survived'].values","8e4dbd94":"#ref.: https:\/\/www.kaggle.com\/jmargni\/tps-apr-2021-lightgbm-cv\n\ndef create_extra_features(data):\n    # Age fillna with mean age for each class\n    age_map = data[['Age', 'Pclass']].dropna().groupby('Pclass').mean().to_dict()\n    data.Age = data.Age.fillna(data.Pclass.map(age_map['Age']))\n\n    # Cabin, fillna with 'X' and take first letter\n    data.Cabin = data.Cabin.map(lambda x: str(x)[0].strip())\n\n    # Ticket, fillna with 'X', split string and take first split \n    data.Ticket = data.Ticket.map(lambda x:str(x).split()[0] if len(str(x).split()) > 1 else 'X')\n\n    # Fare, fillna with mean value \n    # (THE ONLY FILLNA LEFT BECAUSE HERE WE USE TEST DATASET - LightAutoML can't do it in real life because of strict distinction between train and test stages)\n    data.Fare = data.Fare.fillna(data.Fare.mean())\n\n    # Embarked, fillna with 'X' value\n    #all_df.Embarked = all_df.Embarked.fillna('X')\n    \n    # Name, take only surnames\n    #data.Name = data.Name.map(lambda x: str(x).split(',')[0])\n       \n    # let's leave this features here to use after receiving plateau from pseudolabelling\n    \n    data['FirstName'] = data.Name.map(lambda x: str(x).split(',')[0])\n    data['Surname'] = data.Name.map(lambda x: str(x).split(',')[1])\n    \n    for col in ['Name', 'FirstName', 'Surname']:\n        data['Counter_' + col] = data[col].map(data.groupby(col)['PassengerId'].count().to_dict())\n        \n    data.drop(columns = ['Name', 'Surname'], inplace = True)\n\n    \n    return data\n","1b73f20e":"all_df = pd.concat([train_data, test_data]).reset_index(drop = True)\nall_df = create_extra_features(all_df)\ntrain_data, test_data = all_df[:len(train_data)], all_df[len(train_data):]\nprint(train_data.shape, test_data.shape)","16d0d3f8":"%%time\ndef acc_score(y_true, y_pred, **kwargs):\n    return accuracy_score(y_true, (y_pred > 0.5).astype(int), **kwargs)\n\ndef roc_auc_metric(y_true, y_pred, sample_weight, **kwargs):\n    mask = (sample_weight > 1)\n    return roc_auc_score(y_true[mask], y_pred[mask])\n\ntask = Task('binary', metric = roc_auc_metric, greater_is_better=True)","4f3ffb2d":"# Add weights for metric calculation only by true targets\nall_df['weight'] = [1.01] * len(train_data) + [0.99] * len(test_data)","1d7a3b08":"%%time\n\nroles = {\n    'target': 'Survived',\n    'drop': ['PassengerId'],\n    'weights': 'weight' # To calculate metric only on real targets\n}","7ea8a7cd":"params = {\n    'metric': 'binary_logloss',\n    'n_estimators': 10000,\n    'objective': 'binary',\n    'learning_rate': 0.02,\n    'min_child_samples': 150,\n    'reg_alpha': 3e-5,\n    'reg_lambda': 9e-2,\n    'num_leaves': 20,\n    'max_depth': 16,\n    'colsample_bytree': 0.8,\n    'subsample': 0.8,\n    'subsample_freq': 2,\n    'max_bin': 240\n}\n\ncb_params = {\n    'max_depth':6,\n    'max_ctr_complexity': 5,\n    'num_trees': 50000,\n    'od_wait': 500,\n    'od_type':'Iter', \n    'learning_rate': 0.04,\n    'min_data_in_leaf': 3\n}","aab3232e":"%%time \n\ncnt_trained = 0\nresults = []\nfor it, rs in enumerate(range(2000, 2015)):\n    print('=' * 30)\n    print('START RANDOM_STATE = {}'.format(rs))\n    print('=' * 30)\n    \n    # Train AutoML\n    automl = TabularAutoML(task = task, \n                           timeout = TIMEOUT,\n                           cpu_limit = N_THREADS,\n                           general_params = {'use_algos': [['linear_l2', 'lgb', 'cb']]},\n                           selection_params = {'mode': 0},\n                           reader_params = {'n_jobs': N_THREADS, 'cv': N_FOLDS, 'random_state': rs},\n                           lgb_params = {'default_params': params, 'freeze_defaults': True},\n                           cb_params = {'default_params': cb_params, 'freeze_defaults': True})\n    \n    oof_pred = automl.fit_predict(all_df, roles = roles)\n    \n    # Predict on test\n    test_pred = automl.predict(test_data)\n    cnt_trained += 1\n    \n    # Save predictions\n    if it == 0:\n        oof_pred_full = oof_pred.data[:, 0].copy()\n        test_pred_full = test_pred.data[:, 0].copy()\n    else:\n        oof_pred_full += oof_pred.data[:, 0]\n        test_pred_full += test_pred.data[:, 0]\n    \n    # Check scores for current predict and aggregated one\n    acc_usual = acc_score(train_data['Survived'].values, oof_pred.data[:len(train_data), 0])\n    acc_full = acc_score(train_data['Survived'].values, oof_pred_full[:len(train_data)] \/ cnt_trained)\n    results.append((acc_usual, acc_full, acc_full - acc_usual))\n    \n    print('Check scores...')\n    print('OOF score: {}'.format(acc_usual))\n    print('OOF score full: {}'.format(acc_full))\n    print('Difference: {}'.format(acc_full - acc_usual))","5828d43c":"plt.figure(figsize = (20, 10))\nplt.plot(range(1, cnt_trained + 1), [res[0] for res in results], color = 'b', linewidth = 2, label = 'Usual LightAutoML model accuracy')\nplt.plot(range(1, cnt_trained + 1), [res[1] for res in results], color = 'g', linewidth = 2, label = 'Accuracy for averaged LightAutoMLs')\nplt.grid()\nplt.legend()\nplt.title('LightAutoML accuracy vs. averaged LightAutoMLs composition accuracy')\nplt.xlabel('Iteration number')\nplt.ylabel('Accuracy score')\nplt.show()","06ee7fe7":"mean_acc = np.mean([res[0] for res in results])\nmean_acc","71313155":"differences = np.array([res[1] - mean_acc for res in results])","a507a2bd":"plt.figure(figsize = (20, 10))\nplt.plot(range(1, cnt_trained + 1), differences, color = 'g', linewidth = 2, label = 'Difference')\nplt.plot(range(1, cnt_trained + 1), [np.mean(differences[0:i+1]) for i in range(len(differences))], 'b-.', linewidth = 2, label = 'Cumulative mean difference')\nplt.plot(range(1, cnt_trained + 1), [0.0 for res in results], 'r--', linewidth = 2, label = 'Zero line')\nplt.grid()\nplt.legend()\nplt.title('Difference between mean LightAutoML accuracy and averaged LightAutoMLs composition accuracy at each iteration')\nplt.xlabel('Iteration number')\nplt.ylabel('Accuracy score difference')\nplt.show()","b1c65f1f":"submission['Survived'] = ((test_pred_full \/ cnt_trained) > 0.5).astype(int)\nsubmission.to_csv('LightAutoML_compose_version_26.csv', index = False)","eb9368c2":"submission['Survived'].mean()","76560a37":"# Step 0.5. Add new features\n\n#### Important note: below we can see new features creation from [this notebook](https:\/\/www.kaggle.com\/jmargni\/tps-apr-2021-lightgbm-cv) but I have deleted all NaNs filling if they don't use test data to do it","f80cb75b":"# ========= AutoML preset usage =========\n\n\n# Step 1. Create Task","09d7ee34":"# Step 0.1. Import necessary libraries ","9d39001a":"# Step 3. Train on full data (train + pseudolabeled by best submission test) using multistart with different seed and predict on test data\n\nAs we currently have a bug in TabularUtilizedAutoML with specific configs setup (it will be fixed in the next release), we will use simple for cycle by random states to do the job. ","ae3fff26":"# Step 4. Graphic check for received results","d057fc37":"#### Everything ok, we are above zero line and cumulative mean difference in scores is growing :)","87751069":"# Step 2. Setup columns roles","0cd41ab0":"#### Please upvote if you find the notebook useful :)\n\n#### If you are new to LightAutoML, [here](https:\/\/www.kaggle.com\/c\/tabular-playground-series-apr-2021\/discussion\/231090) is the explanation discussion topic\n\n#### In this notebook we try to incorporate features from [this notebook](https:\/\/www.kaggle.com\/jmargni\/tps-apr-2021-lightgbm-cv) to our LightAutoML solution to show how it can be done in real ML tasks.\n#### To make difference inside pseudolabelling process we also added Catboost algo with params from [this](https:\/\/www.kaggle.com\/belov38\/catboost-lb) notebook\n\n### The main idea of this kernel is pseudolabelling the test data (described in details [here](https:\/\/www.kaggle.com\/c\/tabular-playground-series-apr-2021\/discussion\/231738)) and building model using our opensourced framework [LightAutoML](https:\/\/github.com\/sberbank-ai-lab\/LightAutoML).\n\n# Step 0.0. Install LightAutoML","0333596a":"# Step 0.2. Parameters ","f54febf1":"# Step 0.4. Data load ","93c6a71c":"# Step 0.3. Fix torch number of threads and numpy seed ","4631acc4":"# Step 5. Prepare submission"}}