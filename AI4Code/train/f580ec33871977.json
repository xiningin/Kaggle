{"cell_type":{"f13b67a9":"code","d7160fdd":"code","ac0cac74":"code","9b4778cb":"code","7c6ccc8a":"code","c5aba7ce":"code","6cf018f0":"code","988b59ab":"code","cf588dce":"code","c628fe7e":"code","6e379ce4":"code","192b018f":"code","8ca8f6e0":"code","60147d05":"code","ad1d5107":"code","0879a568":"code","90683c30":"code","3a161dd6":"code","ce653453":"markdown","9cbba68b":"markdown","7d564c0d":"markdown","d1d81b45":"markdown","7383afbe":"markdown"},"source":{"f13b67a9":"#Importing Libraries & checking versions\nimport warnings\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # used to plot and format it\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nnp.__version__,pd.__version__,sns.__version__","d7160fdd":"# Enhancing Presentability of data in Jupyter Notebook\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns',50)\npd.set_option('display.max_info_columns',50)","ac0cac74":"# Loading Train Dataset\ndf_data = pd.read_csv('..\/input\/train.csv')\n# Loading Test Dataset\ntest_data = pd.read_csv('..\/input\/test.csv')\n# Shape of Dataset\nprint(f'Rows: {df_data.shape[0]}')\nprint(f'Columns: {df_data.shape[1]}',end='\\n\\n')\n# Overview of Columns\nprint(f'Columns in Dataset:\\n{list(df_data.columns)}',end='\\n\\n')\nprint('Overview of each column')\ndf_data.info()","9b4778cb":"# Displaying top 5 rows of Dataset\ndf_data.head()","7c6ccc8a":"# Building set of Numerical, Categorical & Target columns\nn_col = list(df_data.loc[:,df_data.columns != 'Survived'].select_dtypes([np.int64,np.float64]).columns)\nc_col = list(df_data.select_dtypes([np.object]).columns)\nt_col = ['Survived']\n# Print list of numerical, categorical and target columns\nprint(f'Numerical Columns: {n_col}')\nprint(f'Categorical Columns: {c_col}')\nprint(f'Target Columns: {t_col}',end='\\n\\n')\n# Details of Numerical Columns\nprint(f'Details of Numerical Columns:\\n{df_data[n_col].describe()}',end='\\n\\n')\n# Details of Categorical Columns\nprint(f'Details of Categorical Columns:\\n{df_data[c_col].describe()}',end='\\n\\n')\n# Details of Target Columns\nprint(f'Details of Target Columns:\\n{df_data[t_col].describe()}',end='\\n\\n')","c5aba7ce":"# Cleaning un-necessary columns\nprint(f'Percentage of null values in each column:\\n{round(100*(df_data.isna().sum()\/len(df_data.index)),2)}')\n# Dropping columns which are insignificant for the analysis\ndf_data.drop(columns=['PassengerId'],inplace=True)\n# Cabin have 77.10 % null values, hence dropping the column\ndf_data.drop(columns=['Cabin'],inplace=True)\n# Name have all different values, which wouldt have any affect on target, hence dropping the column\ndf_data.drop(columns=['Name'],inplace=True)\n# Removing the two columns from our columns set\nn_col.remove('PassengerId')\nc_col.remove('Cabin')\nc_col.remove('Name')","6cf018f0":"# Cleaning un-necessary rows\n# Dropping all rows which have more than 20 % missing values\n# Dropping rows with most NaN value (Taking an arbitrary value >20 %)\nthreshold = int(.2*len(df_data.columns))\ndf_data.dropna(thresh=threshold, inplace=True)\n\n# Dropping all rows where Age is null, since it is an important column\ndf_data.dropna(subset=['Age'], how='any', inplace=True)\n\n# Dropping duplicate rows\ndf_data.drop_duplicates(inplace=True)","988b59ab":"# Imputing Embarked Column as it have 0.22 % null values and most values of the column is 'S'\ndf_data.fillna(value='S', inplace=True)","cf588dce":"# Shape of Dataset\nprint(f'Rows: {df_data.shape[0]}')\nprint(f'Columns: {df_data.shape[1]}')\nprint(f'Count of NaN values: {df_data.isna().sum()}')\nprint(f'Percentage of null values in each column:\\n{round(100*(df_data.isna().sum()\/len(df_data.index)),2)}')","c628fe7e":"# A function to plot univariate analysis plots for Numerical columns\ndef univariate_n(c):\n    fig, ax = plt.subplots(1,2,figsize=(12,4))\n    sns.distplot(df_data[c], ax=ax[0])\n    sns.boxplot(x=df_data[c], ax=ax[1])\n    fig.show()","6e379ce4":"# Plotting Histogram for Numerical columns\nfor c in n_col:\n    univariate_n(c)","192b018f":"# Plotting bivariate analysis plots for Numerical columns\nsns.pairplot(df_data, hue=\"Survived\")\nplt.show()","8ca8f6e0":"# Plotting Heatmap for Numerical columns\nplt.figure(figsize=(12,10))\nax = sns.heatmap(df_data[n_col].corr(), vmin=0, vmax=1, cmap=\"YlGnBu\", annot=True)\nplt.show()","60147d05":"print(f'Percentage of null values in each column:\\n{round(100*(test_data.isna().sum()\/len(test_data.index)),2)}')","ad1d5107":"test_data['Fare'].mean()","0879a568":"test_data.fillna(value='35.6271884892086', inplace=True)","90683c30":"X = df_data[n_col]\ny = df_data[t_col]\nlr = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(X, y)\ntest_predict = lr.predict(test_data[n_col])\nd = {'PassengerId': test_data['PassengerId'], 'Survived': test_predict}\ndataframe_to_export = pd.DataFrame(data=d)\nsns.countplot(dataframe_to_export['Survived'], palette = 'icefire')\n","3a161dd6":"# Exporting the Predicted values for evaluation at Kaggle\ndataframe_to_export.to_csv(path_or_buf='submission.csv', index=False)","ce653453":"## Data Cleaning\n1. Cleaning un-necessary columns\n2. Cleaning un-necessary and duplicate rows\n3. Imputing data in suitable cases\n4. Outlier Analysis and Treatment","9cbba68b":"# Simple Python Solution for Titanic: Machine Learning from Disaster\n## Objective\nIt is asked to complete the analysis of what sorts of people were likely to survive. <br\/>\nIn particular, it is asked to apply the tools of machine learning to predict which passengers survived the tragedy.","7d564c0d":"## Building Model\nBuilding a simple Logistic Regression Model","d1d81b45":"## Performing EDA\n1. Univariate Analysis\n2. Bivariate Analysis\n3. Heatmap (correlation) Analysis","7383afbe":"## Data Loading & Overview\n1. Loading data in Pandas Dataframe and overviewing it\n2. Preparation of Numerical, Categorical & Target columns\n3. Observing each one of these Numerical, Categorical & Target columns"}}