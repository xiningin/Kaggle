{"cell_type":{"e74726af":"code","32ea2fef":"code","caeb7630":"code","28b5ee49":"code","cb2348eb":"code","aee01173":"code","a5877a67":"code","b5842acf":"code","39a559ac":"code","c9a817b7":"code","d2d0a1ac":"code","750f0639":"code","b35d442d":"code","c4b15a66":"code","aabc68cd":"code","4646d9ba":"code","a106f393":"code","5fdc0307":"code","c3e2b688":"code","a7c828ae":"code","9e1d5a60":"code","7555d785":"code","9e0c6341":"code","adf604db":"code","efb0adb1":"code","2aa37c3a":"code","743fd697":"code","bb5fe0e7":"code","b595e9f0":"code","b03191ad":"code","1590e42e":"code","46f517ba":"code","d8885975":"code","b91a089b":"code","0d9e7b15":"code","64f8e6e9":"code","c15da83a":"code","069cec08":"code","3c213a88":"code","cec1d606":"code","e98af9a1":"markdown","e185cc31":"markdown","c7b212ed":"markdown","58d52973":"markdown","7d5cf8ec":"markdown","4b440eba":"markdown","e5c14c73":"markdown","a3cd6c25":"markdown","15088120":"markdown","eca3fd21":"markdown","8772f748":"markdown","5329af00":"markdown","5d8bfa25":"markdown","1ed714e4":"markdown","58bb343a":"markdown","2f6d9bad":"markdown"},"source":{"e74726af":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","32ea2fef":"!pip install skompiler","caeb7630":"# Importing the libraries necessary for the exercise.\nimport warnings\nimport pandas as pd\nimport numpy as np\nfrom skompiler import skompile\nfrom sklearn import preprocessing\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz, export_text\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import *\nfrom sklearn.model_selection import *\nfrom sklearn.exceptions import ConvergenceWarning\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n","28b5ee49":"# Reading dataset\ndf = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")\n\n# Looking at the first 5 rows of the data set\ndf.head()","cb2348eb":"# Getting general information about the data set\ndf.info()","aee01173":"# Looking at the descriptive statistics of the data set\ndf.describe().T","a5877a67":"# Before solving this problem, let's check the null values.\ndf.isnull().sum()","b5842acf":"# We can solve this problem by assigning NaN to 0 values in variables that we think are errors.\ndf[[\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\"]] = \\\n    df[[\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\"]].replace(0, np.NaN)","39a559ac":"# Now let's see how many rows we have assigned NaN instead of 0.\ndf.isnull().sum()","c9a817b7":"# With this function, we were able to separate the variables in the data set as categorical and numerical.\ndef grab_col_names(dataframe, cat_th=10, car_th=20):\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n    \n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n    \n    return cat_cols, cat_but_car, num_cols, num_but_cat\n","d2d0a1ac":"cat_cols, cat_but_car, num_cols, num_but_cat = grab_col_names(df)","750f0639":"# Setting an upper and lower limit for outliers\ndef outlier_thresholds(dataframe, variable):\n    quartile1 = dataframe[variable].quantile(0.25)\n    quartile3 = dataframe[variable].quantile(0.75)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit","b35d442d":"# The function that examines whether there is an outlier according to the threshold values we have determined.\ndef check_outlier(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):\n        return True\n    else:\n        return False","c4b15a66":"for col in num_cols:\n    print(col, check_outlier(df, col))","aabc68cd":"# Replacing outliers with upper and lower limit\ndef replace_with_thresholds(dataframe, variable):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit","4646d9ba":"for col in num_cols:\n        replace_with_thresholds(df, col)","a106f393":"df.isnull().sum()","5fdc0307":"df.pivot_table(df, index=[\"Outcome\"])","c3e2b688":"for col in df.columns:\n    df.loc[(df[\"Outcome\"] == 0) & (df[col].isnull()), col] = df[df[\"Outcome\"] == 0][col].median()\n    df.loc[(df[\"Outcome\"] == 1) & (df[col].isnull()), col] = df[df[\"Outcome\"] == 1][col].median()","a7c828ae":"\ndf.loc[(df[\"BMI\"] < 18.5), \"NEW_BMI_CAT\"] = \"Underweight\"\ndf.loc[(df[\"BMI\"] > 18.5) & (df[\"BMI\"] < 25), \"NEW_BMI_CAT\"] = \"Normal\"\ndf.loc[(df[\"BMI\"] > 25) & (df[\"BMI\"] < 30), \"NEW_BMI_CAT\"] = \"Overweight\"\ndf.loc[(df[\"BMI\"] > 30) & (df[\"BMI\"] < 40), \"NEW_BMI_CAT\"] = \"Obese\"\n\ndf.loc[(df[\"Glucose\"] < 70), \"NEW_GLUCOSE_CAT\"] = \"Low\"\ndf.loc[(df[\"Glucose\"] > 70) & (df[\"Glucose\"] < 99), \"NEW_GLUCOSE_CAT\"] = \"Normal\"\ndf.loc[(df[\"Glucose\"] > 99) & (df[\"Glucose\"] < 126), \"NEW_GLUCOSE_CAT\"] = \"Secret\"\ndf.loc[(df[\"Glucose\"] > 126) & (df[\"Glucose\"] < 200), \"NEW_GLUCOSE_CAT\"] = \"High\"\n\ndf.loc[df['SkinThickness'] < 30, \"NEW_SKIN_THICKNESS\"] = \"Normal\"\ndf.loc[df['SkinThickness'] >= 30, \"NEW_SKIN_THICKNESS\"] = \"HighFat\"\n\ndf.loc[df['Pregnancies'] == 0, \"NEW_PREGNANCIES\"] = \"NoPregnancy\"\ndf.loc[((df['Pregnancies'] > 0) & (df['Pregnancies'] <= 4)), \"NEW_PREGNANCIES\"] = \"StdPregnancy\"\ndf.loc[(df['Pregnancies'] > 4), \"NEW_PREGNANCIES\"] = \"OverPregnancy\"\n\ndf.loc[(df['SkinThickness'] < 30) & (df['BloodPressure'] < 80), \"NEW_CIRCULATION_LEVEL\"] = \"Normal\"\ndf.loc[(df['SkinThickness'] >= 30) & (df['BloodPressure'] >= 80), \"NEW_CIRCULATION_LEVEL\"] = \"CircularAtHighRisk\"\ndf.loc[((df['SkinThickness'] < 30) & (df['BloodPressure'] >= 80))\n       | ((df['SkinThickness'] >= 30) & (df['BloodPressure'] < 80)), \"NEW_CIRCULATION_LEVEL\"] = \"CircularAtMediumRisk\"\n\ndf[\"Pre_Age_Cat\"] = df[\"Age\"] * df[\"Pregnancies\"]\n\ndf[\"Ins_Glu_Cat\"] = df[\"Glucose\"] * df[\"Insulin\"]","9e1d5a60":"def label_encoder(dataframe, binary_col):\n    labelencoder = preprocessing.LabelEncoder()\n    dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])\n    return dataframe","7555d785":"binary_cols = [col for col in df.columns if df[col].dtypes == \"O\"\n               and len(df[col].unique()) == 2]","9e0c6341":"for col in df.columns:\n    label_encoder(df, col)","adf604db":"def one_hot_encoder(dataframe, categorical_cols, drop_first=False):\n    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)\n    return dataframe","efb0adb1":"ohe_cols = [col for col in df.columns if 10 >= len(df[col].unique()) > 2]","2aa37c3a":"one_hot_encoder(df, ohe_cols, drop_first=True)","743fd697":"y = df[\"Outcome\"]\nX = df.drop([\"Outcome\"], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=17)","bb5fe0e7":"cart_model = DecisionTreeClassifier(random_state=17).fit(X_train, y_train)","b595e9f0":"cart_params = {'max_depth': range(1, 11),\n               \"min_samples_split\": [2, 3, 4]}\n\ncart_cv = GridSearchCV(cart_model, cart_params, cv=10, n_jobs=-1, verbose=True)\ncart_cv.fit(X_train, y_train)","b03191ad":"cart_cv.best_params_","1590e42e":"cart_tuned = DecisionTreeClassifier(**cart_cv.best_params_).fit(X_train, y_train)","46f517ba":"# test error\ny_pred = cart_tuned.predict(X_test)\ny_prob = cart_tuned.predict_proba(X_test)[:, 1]\nprint(classification_report(y_test, y_pred))\nroc_auc_score(y_test, y_prob)","d8885975":"from sklearn.metrics import confusion_matrix","b91a089b":"confusion_matrix(y_test, y_pred)","0d9e7b15":"#Neural network module\nfrom keras.models import Sequential \nfrom keras.layers import Dense,Activation,Dropout \nfrom keras.layers.normalization import BatchNormalization \nfrom keras.utils import np_utils","64f8e6e9":"#Change the label to one hot vector\n'''\n[0]--->[1 0 0]\n[1]--->[0 1 0]\n[2]--->[0 0 1]\n'''\ny_train=np_utils.to_categorical(y_train,num_classes=2)\ny_test=np_utils.to_categorical(y_test,num_classes=2)\nprint(\"Shape of y_train\",y_train.shape)\nprint(\"Shape of y_test\",y_test.shape)","c15da83a":"# We are using keras library for neural network. Input_dim =15 means we have 15 independent features.\nmodel=Sequential()\nmodel.add(Dense(1000,input_dim=15,activation='relu'))\nmodel.add(Dense(500,activation='relu'))\nmodel.add(Dense(300,activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(2,activation='softmax'))\nmodel.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])","069cec08":"model.summary()","3c213a88":"model.fit(X_train,y_train,validation_data=(X_test,y_test),batch_size=20,epochs=10,verbose=1)","cec1d606":"# We will deploy this model on  heroku soon. Please wait..........","e98af9a1":"**Diabetes**, is a group of metabolic disorders in which there are high blood sugar levels over a prolonged period. Symptoms of high blood sugar include frequent urination, increased thirst, and increased hunger. If left untreated, diabetes can cause many complications. Acute complications can include diabetic ketoacidosis, hyperosmolar hyperglycemic state, or death. Serious long-term complications include cardiovascular disease, stroke, chronic kidney disease, foot ulcers, and damage to the eyes.","e185cc31":"You can see the describe () function of our data set above. In this table, \"min\" shows the smallest number in that variable.\n\nWe see that the smallest values of Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin and BMI variables are 0. Except for the Pregnancies variable, there is no possibility that any of these variables are 0.\n\nNot all 0 values in the \"Pima Indians Diabetes Database\" are actually 0. Empty values are also filled with 0.","c7b212ed":"# Thank You!\n#### If you like this notebook so please upvote it. If you want to improve something in this notebook so please leave a comment and it will be helpful for me as well.","58d52973":"## Missing Values\n","7d5cf8ec":"* Pregnancies: Number of times pregnant\n* Glucose: Glucose\n* BloodPressure: Blood pressure \n* SkinThickness: Triceps skin fold thickness\n* Insulin: Insulin\n* BMI: Body mass index \n* DiabetesPedigreeFunction: Diabetes pedigree function\n* Age: Age (years)\n* Outcome: The knowledge of whether there is diabetes (this is our target)","4b440eba":"## Outliers","e5c14c73":"## Label Encoding","a3cd6c25":"# Model Deployment and Demo","15088120":"## Feature Engineering","eca3fd21":"## Model","8772f748":"## Diabetes Prediction with ANN Algorithm","5329af00":"## EDA","5d8bfa25":"When the variables are examined according to Outcome's being 1 and 0, it is seen that there are differences. When filling the blank values, they should be filled in consideration of this situation.","1ed714e4":"## Data Set and Story","58bb343a":"## One-Hot Encoding","2f6d9bad":"This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage."}}