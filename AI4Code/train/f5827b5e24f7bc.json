{"cell_type":{"edfac206":"code","bbde526f":"code","29e3f603":"code","62eca474":"code","329a9d79":"code","ed821145":"code","01f51344":"code","2d37039a":"code","7ab8c5ed":"code","3f797613":"code","2c3bea6e":"code","8cf9b90c":"code","fa322d78":"code","9200494b":"code","3170af09":"code","3340d300":"code","16056bb3":"markdown","68252ee9":"markdown","c83402e8":"markdown","62ed1f45":"markdown","1fb7004a":"markdown","87d99800":"markdown","c45593b8":"markdown","d5ce479c":"markdown","4e34fdbf":"markdown","d6e08436":"markdown","f306a0c4":"markdown","d231c314":"markdown"},"source":{"edfac206":"import numpy as np\nimport scipy.stats as stats\nx = np.array([12,13,14,19,21,23])\ny = np.array([12,13,14,19,21,23,45])\ndef grubbs_test(x):\n    n = len(x)\n    mean_x = np.mean(x)\n    sd_x = np.std(x)\n    numerator = max(abs(x-mean_x))\n    g_calculated = numerator\/sd_x\n    print(\"Grubbs Calculated Value:\",g_calculated)\n    t_value = stats.t.ppf(1 - 0.05 \/ (2 * n), n - 2)\n    g_critical = ((n - 1) * np.sqrt(np.square(t_value))) \/ (np.sqrt(n) * np.sqrt(n - 2 + np.square(t_value)))\n    print(\"Grubbs Critical Value:\",g_critical)\n    if g_critical > g_calculated:\n        print(\"From grubbs_test we observe that calculated value is lesser than critical value, Accept null hypothesis and conclude that there is no outliers\\n\")\n    else:\n        print(\"From grubbs_test we observe that calculated value is greater than critical value, Reject null hypothesis and conclude that there is an outliers\\n\")\ngrubbs_test(x)\ngrubbs_test(y)","bbde526f":"import pandas as pd\nimport numpy as np\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\nout=[]\ndef Zscore_outlier(df):\n    m = np.mean(df)\n    sd = np.std(df)\n    for i in df: \n        z = (i-m)\/sd\n        if np.abs(z) > 3: \n            out.append(i)\n    print(\"Outliers:\",out)\nZscore_outlier(train['LotArea'])","29e3f603":"import pandas as pd\nimport numpy as np\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\nout=[]\ndef ZRscore_outlier(df):\n    med = np.median(df)\n    ma = stats.median_absolute_deviation(df)\n    for i in df: \n        z = (0.6745*(i-med))\/ (np.median(ma))\n        if np.abs(z) > 3: \n            out.append(i)\n    print(\"Outliers:\",out)\nZRscore_outlier(train['LotArea'])","62eca474":"import pandas as pd\nimport numpy as np\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\nout=[]\ndef iqr_outliers(df):\n    q1 = df.quantile(0.25)\n    q3 = df.quantile(0.75)\n    iqr = q3-q1\n    Lower_tail = q1 - 1.5 * iqr\n    Upper_tail = q3 + 1.5 * iqr\n    for i in df:\n        if i > Upper_tail or i < Lower_tail:\n            out.append(i)\n    print(\"Outliers:\",out)\niqr_outliers(train['LotArea'])","329a9d79":"import pandas as pd\nimport numpy as np\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\nout=[]\ndef Winsorization_outliers(df):\n    q1 = np.percentile(df , 1)\n    q3 = np.percentile(df , 99)\n    for i in df:\n        if i > q3 or i < q1:\n            out.append(i)\n    print(\"Outliers:\",out)\nWinsorization_outliers(train['Fare'])","ed821145":"import pandas as pd\nfrom sklearn.cluster import DBSCAN\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ndef DB_outliers(df):\n    outlier_detection = DBSCAN(eps = 2, metric='euclidean', min_samples = 5)\n    clusters = outlier_detection.fit_predict(df.values.reshape(-1,1))\n    data = pd.DataFrame()\n    data['cluster'] = clusters\n    print(data['cluster'].value_counts().sort_values(ascending=False))\nDB_outliers(train['Fare']) ","01f51344":"from sklearn.ensemble import IsolationForest\nimport numpy as np\nimport pandas as pd\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntrain['Fare'].fillna(train[train.Pclass==3]['Fare'].median(),inplace=True)\ndef Iso_outliers(df):\n    iso = IsolationForest( behaviour = 'new', random_state = 1, contamination= 'auto')\n    preds = iso.fit_predict(df.values.reshape(-1,1))\n    data = pd.DataFrame()\n    data['cluster'] = preds\n    print(data['cluster'].value_counts().sort_values(ascending=False))\nIso_outliers(train['Fare']) ","2d37039a":"import pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom statsmodels.graphics.gofplots import qqplot\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ndef Box_plots(df):\n    plt.figure(figsize=(10, 4))\n    plt.title(\"Box Plot\")\n    sns.boxplot(df)\n    plt.show()\nBox_plots(train['Age'])\n\ndef hist_plots(df):\n    plt.figure(figsize=(10, 4))\n    plt.hist(df)\n    plt.title(\"Histogram Plot\")\n    plt.show()\nhist_plots(train['Age'])\n\ndef scatter_plots(df1,df2):\n    fig, ax = plt.subplots(figsize=(10,4))\n    ax.scatter(df1,df2)\n    ax.set_xlabel('Age')\n    ax.set_ylabel('Fare')\n    plt.title(\"Scatter Plot\")\n    plt.show()\nscatter_plots(train['Age'],train['Fare'])\n\ndef dist_plots(df):\n    plt.figure(figsize=(10, 4))\n    sns.distplot(df)\n    plt.title(\"Distribution plot\")\n    sns.despine()\n    plt.show()\ndist_plots(train['Fare'])\n\ndef qq_plots(df):\n    plt.figure(figsize=(10, 4))\n    qqplot(df,line='s')\n    plt.title(\"Normal QQPlot\")\n    plt.show()\nqq_plots(train['Fare'])\n\n","7ab8c5ed":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\ntrain = pd.read_csv('..\/input\/cost-of-living\/cost-of-living-2018.csv')\nsns.boxplot(train['Cost of Living Index'])\nplt.title(\"Box Plot before outlier removing\")\nplt.show()\ndef drop_outliers(df, field_name):\n    iqr = 1.5 * (np.percentile(df[field_name], 75) - np.percentile(df[field_name], 25))\n    df.drop(df[df[field_name] > (iqr + np.percentile(df[field_name], 75))].index, inplace=True)\n    df.drop(df[df[field_name] < (np.percentile(df[field_name], 25) - iqr)].index, inplace=True)\ndrop_outliers(train, 'Cost of Living Index')\nsns.boxplot(train['Cost of Living Index'])\nplt.title(\"Box Plot after outlier removing\")\nplt.show()","3f797613":"#Scalling\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn import preprocessing\ntrain = pd.read_csv('..\/input\/cost-of-living\/cost-of-living-2018.csv')\nplt.hist(train['Cost of Living Index'])\nplt.title(\"Histogram before Scalling\")\nplt.show()\nscaler = preprocessing.StandardScaler()\ntrain['Cost of Living Index'] = scaler.fit_transform(train['Cost of Living Index'].values.reshape(-1,1))\nplt.hist(train['Cost of Living Index'])\nplt.title(\"Histogram after Scalling\")\nplt.show()","2c3bea6e":"#Log Transformation\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\ntrain = pd.read_csv('..\/input\/cost-of-living\/cost-of-living-2018.csv')\nsns.distplot(train['Cost of Living Index'])\nplt.title(\"Distribution plot before Log transformation\")\nsns.despine()\nplt.show()\ntrain['Cost of Living Index'] = np.log(train['Cost of Living Index'])\nsns.distplot(train['Cost of Living Index'])\nplt.title(\"Distribution plot after Log transformation\")\nsns.despine()\nplt.show()","8cf9b90c":"#cube root Transformation\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\nplt.hist(train['Age'])\nplt.title(\"Histogram before cube root Transformation\")\nplt.show()\ntrain['Age'] = (train['Age']**(1\/3))\nplt.hist(train['Age'])\nplt.title(\"Histogram after cube root Transformation\")\nplt.show()","fa322d78":"#Box-transformation\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport scipy\ntrain = pd.read_csv('..\/input\/cost-of-living\/cost-of-living-2018.csv')\nsns.boxplot(train['Rent Index'])\nplt.title(\"Box Plot before outlier removing\")\nplt.show()\ntrain['Rent Index'],fitted_lambda= scipy.stats.boxcox(train['Rent Index'] ,lmbda=None)\nsns.boxplot(train['Rent Index'])\nplt.title(\"Box Plot after outlier removing\")\nplt.show()","9200494b":"#mean imputation\nimport pandas as pd\nimport numpy as np\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\nsns.boxplot(train['Age'])\nplt.title(\"Box Plot before mean imputation\")\nplt.show()\nq1 = train['Age'].quantile(0.25)\nq3 = train['Age'].quantile(0.75)\niqr = q3-q1\nLower_tail = q1 - 1.5 * iqr\nUpper_tail = q3 + 1.5 * iqr\nm = np.mean(train['Age'])\nfor i in train['Age']:\n    if i > Upper_tail or i < Lower_tail:\n            train['Age'] = train['Age'].replace(i, m)\nsns.boxplot(train['Age'])\nplt.title(\"Box Plot after mean imputation\")\nplt.show()   ","3170af09":"#median imputation\nimport pandas as pd\nimport numpy as np\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\nsns.boxplot(train['Age'])\nplt.title(\"Box Plot before median imputation\")\nplt.show()\nq1 = train['Age'].quantile(0.25)\nq3 = train['Age'].quantile(0.75)\niqr = q3-q1\nLower_tail = q1 - 1.5 * iqr\nUpper_tail = q3 + 1.5 * iqr\nmed = np.median(train['Age'])\nfor i in train['Age']:\n    if i > Upper_tail or i < Lower_tail:\n            train['Age'] = train['Age'].replace(i, med)\nsns.boxplot(train['Age'])\nplt.title(\"Box Plot after median imputation\")\nplt.show()            \n","3340d300":"#Zero value imputation\nimport pandas as pd\nimport numpy as np\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\nsns.boxplot(train['Age'])\nplt.title(\"Box Plot before Zero value imputation\")\nplt.show()\nq1 = train['Age'].quantile(0.25)\nq3 = train['Age'].quantile(0.75)\niqr = q3-q1\nLower_tail = q1 - 1.5 * iqr\nUpper_tail = q3 + 1.5 * iqr\nfor i in train['Age']:\n    if i > Upper_tail or i < Lower_tail:\n            train['Age'] = train['Age'].replace(i, 0)\nsns.boxplot(train['Age'])\nplt.title(\"Box Plot after Zero value imputation\")\nplt.show()            \n","16056bb3":"<div align='left'><font size=\"4\" color=\"#000000\"><h1 style=\"text-transform: uppercase\"><strong>7. Isolation Forest<\/strong> <\/font><\/div>\n<hr>\n<div align='left'><font size=\"3\" color=\"#000000\">It is an clustering algorithm that belongs to the ensemble decision trees family and is similar in principle to Random Forest. <\/font><\/div> \n<hr>\n<img style=\"float: center;\"  src=\"https:\/\/miro.medium.com\/max\/875\/0*0GuMixLdSZo3V3Nh.\" width=\"450px\">\n<hr>       \n<div align='left'><font size=\"3\" color=\"#000000\">1. It classify the data point to outlier and not outliers and works great with very high dimensional data.<\/font><\/div>  <hr>     \n<div align='left'><font size=\"3\" color=\"#000000\">2. It works based on decision tree and it isolate the outliers.<\/font><\/div>  <hr>     \n<div align='left'><font size=\"3\" color=\"#000000\">3. If the result is -1, it means that this specific data point is an outlier. If the result is 1, then it means that the data point is not an outlier. <\/font><\/div> \n","68252ee9":"<div align='left'><font size=\"4\" color=\"#000000\"><h1 style=\"text-transform: uppercase\"><strong>What Next??<\/strong> <\/font><\/div>\n<hr>\n<div align='left'><font size=\"3\" color=\"#000000\">After detecting the outlier we should remove\\treat the outlier because <strong>it is a silent killer!!<\/strong> yes. <\/font><\/div> \n<hr>      \n<div align='left'><font size=\"3\" color=\"#000000\">* Outliers badly affect mean and standard deviation of the dataset. These may statistically give erroneous results.<\/font><\/div>     \n<div align='left'><font size=\"3\" color=\"#000000\">* It increases the error variance and reduces the power of statistical tests.<\/font><\/div>     \n<div align='left'><font size=\"3\" color=\"#000000\">* If the outliers are non-randomly distributed, they can decrease normality.<\/font><\/div> \n<div align='left'><font size=\"3\" color=\"#000000\">* Most machine learning algorithms do not work well in the presence of outlier. So it is desirable to detect and remove outliers.<\/font><\/div>     \n<div align='left'><font size=\"3\" color=\"#000000\">* They can also impact the basic assumption of Regression, ANOVA and other statistical model assumptions.<\/font><\/div> \n<hr>  \n<div align='left'><font size=\"3\" color=\"#000000\">With all these reasons we must be careful about outlier and treat them before build a statistical\/machine learning model. There are some techniques used to deal with outliers.<\/font><\/div> \n<hr>  \n<div align='left'><font size=\"3\" color=\"#000000\">1. Deleting observations.<\/font><\/div>     \n<div align='left'><font size=\"3\" color=\"#000000\">2. Transforming values.<\/font><\/div> \n<div align='left'><font size=\"3\" color=\"#000000\">3. Imputation.<\/font><\/div>  \n<div align='left'><font size=\"3\" color=\"#000000\">4. Separately treating<\/font><\/div> \n<hr>\n<div align='left'><font size=\"4\" color=\"#000000\"><h1 style=\"text-transform: uppercase\"><strong>Deleting observations:<\/strong> <\/font><\/div> \n<div align='left'><font size=\"3\" color=\"#000000\">We delete outlier values if it is due to data entry error, data processing error or outlier observations are very small in numbers. We can also use trimming at both ends to remove outliers. But deleting the observation is not a good idea when we have small dataset.<\/font><\/div> ","c83402e8":"<div align='left'><font size=\"4\" color=\"#000000\"><h1 style=\"text-transform: uppercase\"><strong>Imputation<\/strong> <\/font><\/div>\n<hr>\n<div align='left'><font size=\"3\" color=\"#000000\">Like imputation of missing values, we can also impute outliers. We can use mean, median, zero value in this methods. Since we imputing there is no loss of data. Here median is appropriate because it is not affected by outliers.<\/font><\/div> \n    ","62ed1f45":"<div align='center'><font size=\"5\" color=\"#00000\"><center><h1 style=\"text-transform: uppercase; text-shadow: 1px 1px;\"> Outlier Analysis <\/font><\/div> <\/h1><\/center>\n<hr> \n> <center><img src=\"https:\/\/www.almostzara.com\/wp-content\/uploads\/odd-one-out-group.jpg\" width=800 ><\/center>\n<hr> \n  \n<div align='left'><font size=\"4\" color=\"#000000\"><h1 style=\"text-transform: uppercase\"><strong>What is an Outlier? <\/strong> <\/font><\/div><\/h1>\n<div align='left'><font size=\"3\" color=\"#000000\"> Outlier is an observation that is numerically distant from the rest of the data or in a simple word it is the value which is out of the range.let\u2019s take an example to check what happens to a data set with and data set without outliers.\n<\/font><\/div>\n\n|| | Data without outlier |  | Data with outlier | \n|--||--||--|\n|**Data**| |1,2,3,3,4,5,4 |  |1,2,3,3,4,5,**400** | \n|**Mean**| |3.142 | |**59.714** |  \n|**Median**| |3|  |3|\n|**Standard Deviation**| |1.345185| |**150.057**|\n\n<div align='left'><font size=\"3\" color=\"#000000\"> As you can see, data set with outliers has significantly different mean and standard deviation. In the first scenario, we will say that average is 3.14. But with the outlier, average soars to 59.71. This would change the estimate completely.\n<\/font><\/div>\n\n> <center><img src=\"https:\/\/pbs.twimg.com\/media\/EDANCjJXkAAOSjO.jpg\" width=\"600px\"><\/center>\n> <center>The above meme makes you better understanding of outlier. \n\n<div align='left'><font size=\"3\" color=\"#000000\"> Lets take a real world example. In a company of 50 employees, 45 people having monthly salary of Rs.6,000, 5 senior employees having monthly salary of Rs.100000 each. If you calculate the average monthly salary of employees in the company is Rs.14,500, which will give you the wrong conclusion (majority of employees have lesser than 14.5k salary). But if you take median salary, it is Rs.6000 which is more sense than the average.For this reason median is appropriate measure than mean. Here you can see the effect of outlier.\n<\/font><\/div>    \n<hr>   \n<div class=\"alert alert-info\" ><font size=\"3\"><strong> Outlier <\/strong> is a commonly used terminology by analysts and data scientists as it needs close attention else it can result in wildly wrong estimations. Simply speaking, Outlier is an observation that appears far away and diverges from an overall pattern in a sample.<\/div>\n<hr>\n<div align='left'><font size=\"4\" color=\"#000000\"><h1 style=\"text-transform: uppercase\"><strong>Cause for outliers<\/strong> <\/font><\/div>\n<hr>\n<div align='left'><font size=\"3\" color=\"#000000\"> * Data Entry Errors:- Human errors such as errors caused during data collection, recording, or entry can cause outliers in data. <\/font><\/div> \n<div align='left'><font size=\"3\" color=\"#000000\"> * Data Entry Errors:- Human errors such as errors caused during data collection, recording, or entry can cause outliers in data. <\/font><\/div> \n<div align='left'><font size=\"3\" color=\"#000000\"> * Measurement Error:- It is the most common source of outliers. This is caused when the measurement instrument used turns out to be faulty. <\/font><\/div> \n<div align='left'><font size=\"3\" color=\"#000000\"> * Natural Outlier:- When an outlier is not artificial (due to error), it is a natural outlier. Most of real world data belong to this category.<\/font><\/div> \n<hr>\n<div align='left'><font size=\"4\" color=\"#000000\"><h1 style=\"text-transform: uppercase\"><strong>Outlier Detection<\/strong> <\/font><\/div>\n<hr>\n<div align='left'><font size=\"3\" color=\"#000000\"> Outlier can be of two types: Univariate and Multivariate. Above, we have discussed the example of univariate outlier. These outliers can be found when we look at distribution of a single variable. Multi-variate outliers are outliers in an n-dimensional space. <\/font><\/div> \n<hr>\n<div align='left'><font size=\"4\" color=\"#000000\"><h1 style=\"text-transform: uppercase\"><strong>Different outlier detection technique.<\/strong> <\/font><\/div>\n<hr>\n<div align='left'><font size=\"3\" color=\"#000000\"> 1. Hypothesis Testing <\/font><\/div> \n<div align='left'><font size=\"3\" color=\"#000000\"> 2. Z-score method <\/font><\/div> \n<div align='left'><font size=\"3\" color=\"#000000\"> 3. Robust Z-score<\/font><\/div> \n<div align='left'><font size=\"3\" color=\"#000000\"> 4. I.Q.R method <\/font><\/div> \n<div align='left'><font size=\"3\" color=\"#000000\"> 5. Winsorization method(Percentile Capping) <\/font><\/div> \n<div align='left'><font size=\"3\" color=\"#000000\"> 6. DBSCAN Clustering<\/font><\/div> \n<div align='left'><font size=\"3\" color=\"#000000\"> 7. Isolation Forest <\/font><\/div> \n<div align='left'><font size=\"3\" color=\"#000000\"> 8. Visualizing the data<\/font><\/div> \n<hr>\n<div align='left'><font size=\"4\" color=\"#000000\"><h1 style=\"text-transform: uppercase\"><strong>1. Hypothesis Testing(grubbs test)<\/strong> <\/font><\/div>\n<hr>\n$$\n\\begin{array}{l}{\\text { Grubbs' test is defined for the hypothesis: }} \\\\ {\\begin{array}{ll}{\\text { Ho: }}  {\\text { There are no outliers in the data set }} \\\\ {\\mathrm{H}_{\\mathrm{1}} :}  {\\text { There is exactly one outlier in the data set }}\\end{array}}\\end{array}\n$$\n$$\n\\begin{array}{l}{\\text {The Grubbs' test statistic is defined as: }} \\\\ {\\qquad G_{calculated}=\\frac{\\max \\left|X_{i}-\\overline{X}\\right|}{SD}} \\\\ {\\text { with } \\overline{X} \\text { and } SD \\text { denoting the sample mean and standard deviation, respectively. }} \\end{array}\n$$\n$$\nG_{critical}=\\frac{(N-1)}{\\sqrt{N}} \\sqrt{\\frac{\\left(t_{\\alpha \/(2 N), N-2}\\right)^{2}}{N-2+\\left(t_{\\alpha \/(2 N), N-2}\\right)^{2}}}\n$$\n\n\\begin{array}{l}{\\text { If the calculated value is greater than critical, you can reject the null hypothesis and conclude that one of the values is an outlier }}\\end{array}","1fb7004a":"<div align='left'><font size=\"4\" color=\"#000000\"><h1 style=\"text-transform: uppercase\"><strong>5. Winsorization Method(Percentile Capping)<\/strong> <\/font><\/div>\n<hr>\n<div align='left'><font size=\"3\" color=\"#000000\">This method is similar to IQR method. If a value exceeds the value of the 99th percentile and below the 1st percentile of given values are treated as outliers.<\/font><\/div> \n<hr>","87d99800":"<div align='left'><font size=\"4\" color=\"#000000\"><h1 style=\"text-transform: uppercase\"><strong>Transforming values:<\/strong> <\/font><\/div>\n<hr>\n<div align='left'><font size=\"3\" color=\"#000000\">Transforming variables can also eliminate outliers. These transformed values reduces the variation caused by extreme values.<\/font><\/div> \n<hr>   \n \n<div align='left'><font size=\"3\" color=\"#000000\">1. Scalling<\/font><\/div>     \n<div align='left'><font size=\"3\" color=\"#000000\">2. Log transformation<\/font><\/div> \n<div align='left'><font size=\"3\" color=\"#000000\">3. Cube Root Normalization<\/font><\/div>  \n<div align='left'><font size=\"3\" color=\"#000000\">4. Box-Cox transformation<\/font><\/div> \n<hr>\n    \n<div align='left'><font size=\"3\" color=\"#000000\">* These techniques convert values in the dataset to smaller values.<\/font><\/div>     \n<div align='left'><font size=\"3\" color=\"#000000\">* If the data has to many extreme values or skewed, this method helps to make your data normal.<\/font><\/div>     \n<div align='left'><font size=\"3\" color=\"#000000\">* But These technique not always give you the best results. <\/font><\/div> \n<div align='left'><font size=\"3\" color=\"#000000\">* There is no lose of data from these methods.<\/font><\/div>     \n<div align='left'><font size=\"3\" color=\"#000000\">* In all these method boxcox transformation gives the best result.<\/font><\/div> \n<hr>  ","c45593b8":"<div align='left'><font size=\"4\" color=\"#000000\"><h1 style=\"text-transform: uppercase\"><strong>2. Z-score method<\/strong> <\/font><\/div>\n<hr>\n<div align='left'><font size=\"3\" color=\"#000000\"> Using Z score method,we can find out how many standard deviations value away from the mean. <\/font><\/div> \n<img style=\"float: center;\"  src=\"https:\/\/i.pinimg.com\/originals\/cd\/14\/73\/cd1473c4c82980c6596ea9f535a7f41c.jpg\" width=\"350px\">\n<hr>\n<div align='left'><font size=\"3\" color=\"#000000\">  Figure in the left shows area under normal curve and how much area that standard deviation covers. <\/font><\/div> \n<div align='left'><font size=\"3\" color=\"#000000\"> * 68% of the data points lie between + or - 1 standard deviation.<\/font><\/div>\n<div align='left'><font size=\"3\" color=\"#000000\"> * 95% of the data points lie between + or - 2 standard deviation<\/font><\/div>\n<div align='left'><font size=\"3\" color=\"#000000\"> * 99.7% of the data points lie between + or - 3 standard deviation<\/font><\/div>\n<hr>\n<div align='left'><font size=\"3\" color=\"#000000\"> Z-score formula<\/font><\/div>\n\n\\begin{array}{l} {Z score=\\frac{ X - Mean}{Standard Deviation}}  \\end{array}\n<hr>\n<div align='left'><font size=\"3\" color=\"#000000\"> If the z score of a data point is more than 3 (because it cover 99.7% of area), it indicates that the data value is quite different from the other values. It is taken as outliers.<\/font><\/div>\n","d5ce479c":"<div align='left'><font size=\"4\" color=\"#000000\"><h1 style=\"text-transform: uppercase\"><strong>4. IQR method<\/strong> <\/font><\/div>\n<hr>\n<div align='left'><font size=\"3\" color=\"#000000\">In this method by using Inter Quartile Range(IQR), we detect outliers. IQR tells us the variation in the data set.Any value, which is beyond the range of -1.5 x IQR to 1.5 x IQR treated as outliers <\/font><\/div> \n<hr>\n<img style=\"float: center;\"  src=\" https:\/\/miro.medium.com\/max\/18000\/1*2c21SkzJMf3frPXPAR_gZA.png\" width=\"400px\">\n<hr> \n<div align='left'><font size=\"3\" color=\"#000000\">* Q1 represents the 1st quartile\/25th percentile of the data.<\/font><\/div> \n<div align='left'><font size=\"3\" color=\"#000000\">* Q2 represents the 2nd quartile\/median\/50th percentile of the data.<\/font><\/div>\n<div align='left'><font size=\"3\" color=\"#000000\">* Q3 represents the 3rd quartile\/75th percentile of the data. <\/font><\/div>\n<div align='left'><font size=\"3\" color=\"#000000\">* (Q1\u20131.5*IQR) represent the smallest value in the data set and (Q3+1.5*IQR)\n    represnt the largest value in the data set.<\/font><\/div>\n\n","4e34fdbf":"<div align='left'><font size=\"4\" color=\"#000000\"><h1 style=\"text-transform: uppercase\"><strong>8. Visualizing the data<\/strong> <\/font><\/div>\n<hr>\n<div align='left'><font size=\"3\" color=\"#000000\">Data visualization is useful for data cleaning, exploring data, detecting outliers and unusual groups, identifying trends and clusters etc. Here the list of data visualization plots to spot the outliers. <\/font><\/div> \n<hr>      \n<div align='left'><font size=\"3\" color=\"#000000\">1. Box and whisker plot (box plot).<\/font><\/div>     \n<div align='left'><font size=\"3\" color=\"#000000\">2. Scatter plot.<\/font><\/div>     \n<div align='left'><font size=\"3\" color=\"#000000\">3. Histogram. <\/font><\/div> \n<div align='left'><font size=\"3\" color=\"#000000\">4. Distribution Plot.<\/font><\/div>     \n<div align='left'><font size=\"3\" color=\"#000000\">5. QQ plot. <\/font><\/div> \n","d6e08436":"<div align='left'><font size=\"4\" color=\"#000000\"><h1 style=\"text-transform: uppercase\"><strong>3. Robust Z-score<\/strong> <\/font><\/div>\n<hr>\n<div align='left'><font size=\"3\" color=\"#000000\"> It is also called as Median absolute deviation method. It is similar to Z-score method with some changes in parameters. Since mean and standard deviations are heavily influenced by outliers, alter to this parameters we use median and absolute deviation from median. <\/font><\/div> \n<hr>\n<div align='left'><font size=\"3\" color=\"#000000\"> Robust Z-score formula <\/font><\/div> \n<div align='left'><font size=\"3\" color=\"#000000\"> \\begin{array}{l} {R.Z.score=\\frac{0.6745*( X_{i} - Median)}{MAD}}  \\end{array} <\/font><\/div> \n<div align='center'><font size=\"2.5\" color=\"#000000\"> Where MAD = median(|X-median|)<\/font><\/div> \n<hr>\n<div align='left'><font size=\"3\" color=\"#000000\"> Suppose x follows a standard normal distribution. The MAD will converge to the median of the half normal distribution, which is the 75% percentile of a normal distribution, and  N(0.75)\u22430.6745.<\/font><\/div> \n\n","f306a0c4":"<div align='left'><font size=\"4\" color=\"#000000\"><h1 style=\"text-transform: uppercase\"><strong>6. DBSCAN (Density-Based Spatial Clustering of Applications with Noise)<\/strong> <\/font><\/div>\n<hr>\n<div align='left'><font size=\"3\" color=\"#000000\">DBSCAN is a density based clustering algorithm that divides a dataset into subgroups of high density regions and identifies high density regions cluster as outliers. Here cluster -1 indicates that the cluster contains outlier and rest of clusters have no outliers. This approch is similar to the K-mean clustering. There are two parameters required for DBSCAN. DBSCAN give best result for multivariate outlier detection.<\/font><\/div> \n<hr>\n<div align='left'><font size=\"3\" color=\"#000000\">1. epsilon:  a distance parameter that defines the radius to search for nearby neighbors.<\/font><\/div> \n<div align='left'><font size=\"3\" color=\"#000000\">2. minimum amount of points required to form a cluster.<\/font><\/div> \n<img style=\"float: center;\"  src=\"https:\/\/qphs.fs.quoracdn.net\/main-qimg-384458d7ab61f88e443b5e99bcd06622\" width=\"350px\">\n<hr> \n<div align='left'><font size=\"3\" color=\"#000000\">Using epsilon and minPts, we can classify each data point as:<\/font><\/div> \n <hr>    \n<div align='left'><font size=\"3\" color=\"#000000\">Core point \u2013> a point that has at least a minimum number of other points (minPts) within its radius.\n<\/font><\/div> \n<div align='left'><font size=\"3\" color=\"#000000\">Border point \u2013> a point is within the  radius of a core point but has less than the minimum number of other points (minPts) within its own radius.<\/font><\/div>\n<div align='left'><font size=\"3\" color=\"#000000\">Noise point \u2013> a point that is neither a core point or a border point <\/font><\/div>\n","d231c314":"<div align='left'><font size=\"4\" color=\"#000000\"><h1 style=\"text-transform: uppercase\"><strong>Separately treating<\/strong> <\/font><\/div>\n<hr>\n<div align='left'><font size=\"3\" color=\"#000000\">If there are significant number of outliers and dataset is small , we should treat them separately in the statistical model. One of the approach is to treat both groups as two different groups and build individual model for both groups and then combine the output. But this technique is tedious when the dataset is large.<\/font><\/div> \n<hr>\n<div align='left'><font size=\"4\" color=\"#000000\"><h1 style=\"text-transform: uppercase\"><strong>Conclusion<\/strong> <\/font><\/div>\n<hr>\n<div align='left'><font size=\"3\" color=\"#000000\">1. Median is best measure of central tendency when the data has outlier or skewed.<\/font><\/div> \n<div align='left'><font size=\"3\" color=\"#000000\">2. Winsorization Method or Percentile Capping is the better outlier detection technique the others.<\/font><\/div> \n<div align='left'><font size=\"3\" color=\"#000000\">3. Median imputation completely remove outlier.<\/font><\/div> \n<hr>\n<div align='left'><font size=\"3\" color=\"#000000\">Outlier is one of the major problem in machine learning. If you neglect the outlier result with bad performance of the model. In this kernel I'm try to cover almost all the topics related to outliers, outlier detection, outlier treatment techniques. <\/font><\/div> \n\n<hr>   \n<div class=\"alert alert-warning\" ><font size=\"3\"><strong>Please note that some of the techniques mentioned in this kernel may not gave the best result all the time. So be careful when you try to detect\/impute outliers.<\/strong> <\/div><hr>  \n<div align='left'><font size=\"4\" color=\"#000000\"><h1 style=\"text-transform: uppercase\"><strong>References<\/strong> <\/font><\/div>\n<hr>\n<div align='left'><font size=\"3\"><a href=\"https:\/\/medium.com\/datadriveninvestor\/finding-outliers-in-dataset-using-python-efc3fce6ce32\" target=\"_blank\">1. Finding outliers in dataset using python<\/a><\/div>\n<div align='left'><font size=\"3\"><a href=\"http:\/\/www.askanalytics.in\/p\/outlier-treatment.html\" target=\"_blank\">2. Outlier Detection - Basics<\/a><\/div>\n<div align='left'><font size=\"3\"><a href=\"https:\/\/towardsdatascience.com\/ways-to-detect-and-remove-the-outliers-404d16608dba\" target=\"_blank\">3. Ways to Detect and Remove the Outliers<\/a><\/div>\n<div align='left'><font size=\"3\"><a href=\"https:\/\/www.kdnuggets.com\/2018\/12\/four-techniques-outlier-detection.html\" target=\"_blank\">4. Four Techniques for Outlier Detection - KDnuggets<\/a><\/div>\n<hr>   \n<div class=\"alert alert-success\" ><font size=\"4\"><strong>Feel free to ask any question related to this topic. I'm happy to answer. If you like my work don't hesitate to upvote. HAPPY LEARNING :) <\/strong> <\/div>\n\n<img style=\"float: center;\"  src=\"https:\/\/img.pngio.com\/thumbs-up-emoticon-thumb-up-and-down-emoji-png-image-thumbs-up-emoji-transparent-background-820_516.png\" width=\"450px\">\n\n    \n"}}