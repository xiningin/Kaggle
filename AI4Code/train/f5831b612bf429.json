{"cell_type":{"1e1e75c7":"code","478bc5ae":"code","37aea2ec":"code","b1b74967":"code","34828a61":"code","b714630f":"code","6b5bac7e":"code","ec98784b":"code","7fae6302":"code","ace5ff9d":"code","10fd683d":"markdown","57c1b4e7":"markdown"},"source":{"1e1e75c7":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\n\nfrom tqdm import tqdm","478bc5ae":"import pandas  as pd\nimport xgboost as xgb\n\n#===========================================================================\n# read in the data\n# Original kernel: https:\/\/www.kaggle.com\/carlmcbrideellis\/very-simple-xgboost-regression\n#===========================================================================\ntrain_data = pd.read_csv('..\/input\/tabular-playground-series-jan-2021\/train.csv')\ntest_data  = pd.read_csv('..\/input\/tabular-playground-series-jan-2021\/test.csv')\n\n#===========================================================================\n# select some features of interest (\"ay, there's the rub\", Shakespeare)\n#===========================================================================\nfeatures = ['cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7',\n       'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'cont14']\n\n#===========================================================================\n#===========================================================================\nX_train = train_data[features]\ny_train = train_data[\"target\"]\nfinal_X_test = test_data[features]\n\n#===========================================================================\n# XGBoost regression: \n# Parameters: \n# n_estimators  \"Number of gradient boosted trees. Equivalent to number \n#                of boosting rounds.\"\n# learning_rate \"Boosting learning rate (xgb\u2019s \u201ceta\u201d)\"\n# max_depth     \"Maximum depth of a tree. Increasing this value will make \n#                the model more complex and more likely to overfit.\" \n#===========================================================================\n# regressor=xgb.XGBRegressor(n_estimators  = 500,\n#                            learning_rate = 0.1,\n#                            max_depth     = 5)\n# regressor.fit(X_train, y_train)\n\n#===========================================================================\n# To use early_stopping_rounds: \n# \"Validation metric needs to improve at least once in every \n# early_stopping_rounds round(s) to continue training.\"\n#===========================================================================\n# perform a test\/train split \nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\n\n# params for XGB are taked from this great kernel https:\/\/www.kaggle.com\/hamzaghanmi\/xgboost-hyperparameter-tuning-using-optuna \n# by Hamza Ghanmi\n\nregressor = xgb.XGBRegressor(\n                 colsample_bytree=0.5,\n                 alpha=0.01563,\n                 #gamma=0.0,\n                 learning_rate=0.01,\n                 max_depth=15,\n                 min_child_weight=257,\n                 n_estimators=4000,                                                                  \n                 #reg_alpha=0.9,\n                 reg_lambda=0.003,\n                 subsample=0.7,\n                 random_state=2020,\n                 metric_period=100,\n                 silent=1)\n\nregressor.fit(X_train, y_train, early_stopping_rounds=6, eval_set=[(X_test, y_test)], verbose=1)","37aea2ec":"#===========================================================================\n# use the model XGB to predict the prices for the test data\n#===========================================================================\npredictions = regressor.predict(final_X_test)","b1b74967":"X = train_data.drop(['id','target'], axis=1)\nXtest = test_data.drop(['id'], axis=1)\ny = train_data['target']\n\ntrain = int(len(X)*0.9)\nXtrain, Xval = X.iloc[:train], X.iloc[train:]\nytrain, yval = y.iloc[:train], y.iloc[train:]","34828a61":"# params from this kernel https:\/\/www.kaggle.com\/kailex\/tabular-playground\n\nparams={'random_state': 33,'n_estimators':5000,\n 'min_data_per_group': 5,\n 'boosting_type': 'gbdt',\n 'num_leaves': 256,\n 'max_dept': -1,\n 'learning_rate': 0.02,\n 'subsample_for_bin': 200000,\n 'lambda_l1': 1.074622455507616e-05,\n 'lambda_l2': 2.0521330798729704e-06,\n 'n_jobs': -1,\n 'cat_smooth': 1.0,\n 'silent': True,\n 'importance_type': 'split',\n 'metric': 'rmse',\n 'feature_pre_filter': False,\n 'bagging_fraction': 0.8206341150202605,\n 'min_data_in_leaf': 100,\n 'min_sum_hessian_in_leaf': 0.001,\n 'bagging_freq': 6,\n 'feature_fraction': 0.5,\n 'min_gain_to_split': 0.0,\n 'min_child_samples': 20}","b714630f":"from lightgbm import LGBMRegressor\nfrom sklearn.model_selection import KFold\n\nN_FOLDS = 5\n\nkf = KFold(n_splits = N_FOLDS)\noof = np.zeros(len(y))\noof_vanilla = np.zeros(len(y))\npreds = np.zeros(len(Xtest))\nparams['learning_rate'] = 0.005\nparams['num_iterations'] = 5000\nfor train_ind, test_ind in tqdm(kf.split(X)):\n    Xtrain = X.iloc[train_ind]\n    Xval = X.iloc[test_ind]\n    ytrain = y.iloc[train_ind]\n    yval = y.iloc[test_ind]\n\n    model = LGBMRegressor(**params)\n    vanilla_model = LGBMRegressor()\n    \n    model.fit(Xtrain, ytrain, eval_set = ((Xval,yval)), early_stopping_rounds = 50, verbose = 0)\n    vanilla_model.fit(Xtrain, ytrain)\n    p = model.predict(Xval)\n    p_vanilla = vanilla_model.predict(Xval)\n    oof[test_ind] = p\n    oof_vanilla[test_ind] = p_vanilla\n    \n    preds += model.predict(Xtest)\/N_FOLDS\n    \nprint(f'mean square error on training data (vanilla model): {np.round(mean_squared_error(y, oof_vanilla, squared=False),5)}')    \nprint(f'mean square error on training data (with optuna tuning): {np.round(mean_squared_error(y, oof, squared=False),5)}')","6b5bac7e":"# I taked this submissions from this great kernel https:\/\/www.kaggle.com\/somayyehgholami\/results-driven-tabular-playground-series-201\n# by Somayyeh Gholami\nsub1 = pd.read_csv('..\/input\/resultsdriven-tabular-playground-series-201\/submission - 2021-01-15T023916.124.csv')\npredictions1 = sub1['target'].tolist()\nsub2 = pd.read_csv('..\/input\/resultsdriventabularplaygroundseries2011\/submission - 2021-01-16T012125.132.csv')\npredictions2 = sub2['target'].tolist()","ec98784b":"results = [y*1.0002 for x, y in zip(predictions1, predictions2)]","7fae6302":"#===========================================================================\n# write out CSV submission file\n#===========================================================================\noutput = pd.DataFrame({\"id\":test_data.id, \"target\":results})\noutput.to_csv('submission.csv', index=False)","ace5ff9d":"output","10fd683d":"#### This part is taked from this great [kernel](https:\/\/www.kaggle.com\/bowaka\/tps21-optuna-lgb-fast-hyper-parameter-tunning) by [Bowaka](https:\/\/www.kaggle.com\/bowaka)","57c1b4e7":"# Ensempling between two predictions"}}