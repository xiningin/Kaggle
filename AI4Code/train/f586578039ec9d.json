{"cell_type":{"3b035a91":"code","9abc535f":"code","1ff7f8b2":"code","c743abbd":"code","c6463700":"code","cd484260":"code","5a816221":"code","7c20dfd6":"code","919829f4":"code","6ccce830":"code","736bd6a7":"code","3503844e":"code","2c5a95c8":"code","b0c9ac15":"code","4b71868d":"markdown","50fbef55":"markdown","138caabe":"markdown","b4a8a515":"markdown","55fa080c":"markdown","7e3f75c3":"markdown","4e861ff5":"markdown"},"source":{"3b035a91":"import os\nimport gc\nimport sys\nimport time\nimport pickle\nimport random\nimport numpy as np\nimport pandas as pd\nimport datatable as dt\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\n \nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.manifold import TSNE\n\nfrom colorama import Fore, Back, Style\ny_ = Fore.YELLOW\nr_ = Fore.RED\ng_ = Fore.GREEN\nb_ = Fore.BLUE\nm_ = Fore.MAGENTA\nc_ = Fore.CYAN\nsr_ = Style.RESET_ALL\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport optuna\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoost, Pool\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\n\nfrom sklearn.metrics import roc_auc_score, mean_squared_error\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.decomposition import TruncatedSVD, PCA\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.preprocessing import (StandardScaler,RobustScaler ,\n                                   PowerTransformer,KBinsDiscretizer,\n                                   QuantileTransformer ,LabelEncoder, \n                                   OneHotEncoder,OrdinalEncoder)","9abc535f":"folder_path = '..\/input\/tabular-playground-series-feb-2021'\ntrain_data = pd.read_csv(f'{folder_path}\/train.csv')\ntest_data = pd.read_csv(f'{folder_path}\/test.csv')\nsample = pd.read_csv(f'{folder_path}\/sample_submission.csv')","1ff7f8b2":"train_data.head()","c743abbd":"cont_features = [f'cont{i}' for i in range(14)]\ncat_features = [f'cat{i}' for i in range(10)] \n# cat_features = ['cat1','cat3','cat5','cat8','cat9']\nall_features =   cat_features + cont_features\ntarget_feature = 'target'\n\nnum_bins = int(1 + np.log2(len(train_data)))\ntrain_data.loc[:,'bins'] = pd.cut(train_data['target'].to_numpy(),bins=num_bins,labels=False)\nbins = train_data['bins'].to_numpy()\n\ntarget = train_data[target_feature].to_numpy()\ntrain_data = train_data[all_features].to_numpy()\ntest_data = test_data[all_features].to_numpy()\n\nct = ColumnTransformer([('onehot',OrdinalEncoder(),slice(len(cat_features))),\n                        ('at',QuantileTransformer(),slice(len(cat_features),\n                        len(cat_features)+len(cont_features)))])\n\ntrain_data = ct.fit_transform(train_data)\ntest_data = ct.transform(test_data)","c6463700":"def rmse_score(y_true,y_pred):\n    return np.sqrt(mean_squared_error(y_true,y_pred))","cd484260":"nfolds = 5\nseed = 0\n\nparams = {'reg_alpha': 6.147694913504962,\n 'reg_lambda': 0.002457826062076097,\n 'colsample_bytree': 0.3,\n 'subsample': 0.8,\n 'learning_rate': 0.001,\n 'max_depth': 20,\n 'num_leaves': 111,\n 'min_child_samples': 285,\n'categorical_features': list(range(len(cat_features))),\n 'random_state': 48,\n'verbose':-1,\n 'n_estimators': 10000,\n 'metric': 'rmse',\n 'cat_smooth': 39}","5a816221":"lgbm_preds = np.zeros(test_data.shape[0])\n\nkfold = StratifiedKFold(n_splits=nfolds,random_state=seed)\nlgbm_scores = list()\nfor k, (train_idx, valid_idx) in enumerate(kfold.split(X=train_data,y=bins)):\n    lgb_train = lgb.Dataset(train_data[train_idx],target[train_idx])\n    lgb_valid = lgb.Dataset(train_data[valid_idx],target[valid_idx],reference=lgb_train)\n    lgb_model = lgb.train(params,\n                      lgb_train, \n                      valid_sets=[lgb_train,lgb_valid],\n                      verbose_eval=0,\n                      early_stopping_rounds=800,\n                      )\n    rmse = rmse_score(target[valid_idx],lgb_model.predict(train_data[valid_idx]))\n    print(f\"fold {k}: rmse:{rmse}\")\n    lgbm_scores.append(rmse)\n    lgbm_preds += lgb_model.predict(test_data)\/nfolds\n#     break\n\nprint(\"mean rmse score\",np.mean(lgbm_scores))","7c20dfd6":"def plot_feature_importance(model,features):\n    feature_importance = pd.DataFrame({\"feature\":features,\"importance\":model.feature_importance(importance_type='gain')})\n    feature_importance = feature_importance.sort_values(by='importance',ascending=False)\n    \n    plt.figure(figsize=(10,10))\n    plt.subplot(211)\n    sns.barplot(data=feature_importance,x='importance',y='feature')\n    \n    for idx, v in enumerate(feature_importance.importance):\n            plt.text(v, idx, \"  {:.2e}\".format(v))\n    \n    feature_importance = pd.DataFrame({\"feature\":features,\"importance\":model.feature_importance(importance_type='split')})\n    feature_importance = feature_importance.sort_values(by='importance',ascending=False)\n    \n    plt.subplot(212)\n    sns.barplot(data=feature_importance,x='importance',y='feature')\n    \n    for idx, v in enumerate(feature_importance.importance):\n        plt.text(v, idx, \"  {:.2e}\".format(v))","919829f4":"plot_feature_importance(lgb_model,all_features)","6ccce830":"params = {'l2_leaf_reg': 0.02247766515106271, \n          'max_bin': 364,\n          'subsample': 0.6708650091202213,\n          'learning_rate': 0.0010290546311954876,\n          'max_depth': 10,\n          'verbose':0,\n          'random_state': seed, \n          'min_data_in_leaf': 300,\n          'loss_function': 'RMSE',\n          'n_estimators':  1600000,\n          'rsm':0.5,\n          'early_stopping_rounds':800}","736bd6a7":"cat_preds = np.zeros(test_data.shape[0])\nkfold = StratifiedKFold(n_splits=nfolds, random_state =seed)\ncat_scores = list()\nfor train_idx, valid_idx in kfold.split(X=train_data,y=bins):\n    cat_train = Pool(train_data[train_idx],target[train_idx])\n    cat_valid = Pool(train_data[valid_idx],target[valid_idx])\n    \n    cat_model = CatBoost(params)\n    cat_model.fit(cat_train,eval_set=cat_valid)\n    score = rmse_score(target[valid_idx],cat_model.predict(train_data[valid_idx]))\n    print(f\"fold: {k}, score: {score}\")\n    cat_scores.append(score)\n    cat_preds += cat_model.predict(test_data)\/nfolds\n    \nprint('mean rmse score:',np.mean(cat_scores))","3503844e":"predictions = pd.DataFrame({\"lgbm\":lgbm_preds,'catboost':cat_preds})\nplt.figure(figsize=(7,7))\nsns.heatmap(predictions.corr(),annot=True);","2c5a95c8":"sample.target = (lgbm_preds.ravel() + cat_preds.ravel())\/2\nsample.to_csv(\"submission.csv\",index=False)\nsample.head()","b0c9ac15":"plt.figure(figsize=(15,7))\nplt.subplot(131)\nsns.distplot(sample.target)\nplt.title(\"test-target distribution\")\nplt.subplot(132)\nsns.distplot(target)\nplt.title(\"train-target distribution\")\nplt.subplot(133)\nsns.distplot(sample.target.to_numpy(),label='test')\nsns.distplot(target,label='target')\nplt.legend()\nplt.title(\"train and test target distribution\");","4b71868d":"## Catboost","50fbef55":"## Loading Data \ud83d\udcbd","138caabe":"## submission","b4a8a515":"## correlation matrix","55fa080c":"## LGBM Model","7e3f75c3":"## Data Preprocessing","4e861ff5":"## Importing Libraries \ud83d\udcd7"}}