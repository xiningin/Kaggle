{"cell_type":{"7dd60b59":"code","efe16022":"code","49e4b80f":"code","a898da00":"code","aa046d4f":"code","146abcdb":"code","fca65fd3":"code","ed704c8a":"code","6f81738e":"code","04bc7aab":"code","b8b86d2f":"code","dd75ae03":"code","42a73eb3":"code","0cf11b18":"code","0d4dbfe0":"markdown"},"source":{"7dd60b59":"#Required Libraries\nimport gym                             #for developing and comparing RL algorithms\nimport numpy as np                     #for numerical computation\nimport time                            #for representing time in code","efe16022":"# Custom Map\ncustom_map = [\n    'SFFFHFHFHF', \n    'FFFFFFFHFH',\n    'HFFFHFFFFH',\n    'HFFFFFFFFG', \n]\n\nenv = gym.make('FrozenLake-v1',)\nenv.reset()\nenv.render()\n ","49e4b80f":"env.action_space.sample()","a898da00":"env.reset()","aa046d4f":"#epsilon-Greedy Strategy\ndef eps_greedy(a, env, eps = 0.2):\n    \n    p = np.random.random()\n    \n    #Exploitation\n    if p < 1 - eps:  \n        return a\n    \n    #Exploration\n    else:           \n        return env.action_space.sample()","146abcdb":"#Run the Game\nfrom IPython.display import clear_output\n\ndef run_game(env, policy, epsilon = 0.2, gamma = 0.9, display = True, game_num = 0):\n    \n    #Initial Condition\n    state  = env.reset()\n    action = eps_greedy(policy[state], env, eps = epsilon)\n        \n    #Save State, Action, Reward\n    state_action_reward = [(state, action, 0)]\n    \n    #Run the Game \n    while True:\n        state, reward, done, _ = env.step(action)\n        if display:\n            clear_output(True)\n            print('Episode # {}'.format(game_num))\n            env.render()\n            time.sleep(0.1)\n                \n        if done:\n            state_action_reward.append((state, None, reward))\n            if display:\n                print('End of the Episode')\n            break\n        \n        else:\n            action = eps_greedy(policy[state], env, eps = epsilon)\n            state_action_reward.append((state, action, reward))\n    \n    # Calculate the Return(G)\n    G = 0\n    state_action_returns = []\n    \n    IFTerminal = True\n    \n    for state, action, rwd in reversed(state_action_reward):\n        \n        #Skip the Terminal Node\n        if IFTerminal:\n            IFTerminal = False\n        else:\n            state_action_returns.append((state, action, G))\n            \n        G = gamma * G + rwd\n    \n    #Back to the Original Order of States Visited\n    state_action_returns.reverse()\n    \n    return state_action_returns","fca65fd3":"#Monte Carlo Prediction\ndef monte_carlo(env, epsilon = 0.2, gamma = 0.9, num_games = 10000, display = False):\n    \n    #Set Initial Random Policies\n    policy = np.random.choice(env.action_space.n, env.observation_space.n)\n        \n    #Initialize Q-table\n    Q_table = np.zeros((env.observation_space.n, env.action_space.n))\n    #Dictionary to Record Returns Recieved\n    returns = {}\n    \n    for state in range(env.observation_space.n):\n        for action in range(env.action_space.n):\n            returns[(state, action)] = []\n        \n    #Keep Track of How much Q Values Change\n    delta = [] \n    \n    #Run the Algorithm for the Number of Episodes\n    for i in range(num_games):\n                \n        #Run the Game Using the Current Policy\n        state_action_returns = run_game(env, policy, epsilon, gamma, display, i)\n        \n        #Calculate Q(state, action)\n        seen_state_action = set()\n        largest_change = 0\n        \n        for state, action, G in state_action_returns:\n            \n            #Check if First Visit\n            if (state, action) not in seen_state_action:\n                returns[(state, action)].append(G)\n                previous_Q = Q_table[state, action]\n                #Calculate Mean of Returns\n                Q_table[state, action] = np.mean(returns[(state, action)])\n                seen_state_action.add((state, action))\n                largest_change = max(largest_change, np.abs(previous_Q - Q_table[state, action]))\n        \n        delta.append(largest_change)\n        \n        #Calculate the New Policy at the End of each Episode\n        for state in range(env.observation_space.n):\n                policy[state] = np.argmax(Q_table[state])\n        \n    #Record the Value Functions\n    value  = np.zeros(env.observation_space.n)\n    for state in range(env.observation_space.n):\n        value[state]  = np.max(Q_table[state])\n\n    return value, policy, delta","ed704c8a":"#Run Monte Carlo Method for 10 Episodes\ns_time = time.time()\nvalue, policy, delta = monte_carlo(env, epsilon = 0.4, gamma = 0.9, num_games = 10, display = True)\ne_time = time.time()\nprint('Run Time {} seconds'.format(e_time - s_time))","6f81738e":"#Run Monte Carlo Method for 50000 Episodes\ns_time = time.time()\nvalue, policy, delta = monte_carlo(env, epsilon = 0.2, gamma = 0.88, num_games = 10000, display = False)\ne_time = time.time()\nprint('Run Time is {} seconds'.format(e_time - s_time))","04bc7aab":"#Plot How Q Values Change over Episodes\nimport matplotlib.pyplot as plt\nplt.plot(delta)\nplt.title('Changes in Q-Values over Episodes')\nplt.xlabel('Episodes')","b8b86d2f":"#Run so many episodes\ndef run_episodes(env, policy, num_games = 1000):\n    \n    tot_rew = 0\n    state = env.reset()\n\n    for _ in range(num_games):\n        done = False\n        while not done:\n            #Select the action accordingly to the policy\n            next_state, reward, done, _ = env.step(policy[state])\n                \n            state = next_state\n            tot_rew += reward \n            if done:\n                state = env.reset()\n\n    print('Won {} of {} games!'.format(tot_rew, num_games))","dd75ae03":"#Test the Policy\nrun_episodes(env, policy, num_games = 100)","42a73eb3":"#Example of Running the Game based on Specific Policy\n#Possible Actions: 0 is left, 1 is down, 2 is right, and 3 is up\n# policy = [0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0] #Optimal Policy from Dynamic Programming Solution\nrun_game(env, policy, epsilon = .3, gamma = 0.9, display = False, game_num = 1000)","0cf11b18":"value","0d4dbfe0":"## First Visit Monte Carlo with epsilon-Greedy"}}