{"cell_type":{"41bcc359":"code","74958d65":"code","61fd6bc5":"code","6ed98db6":"code","35f22201":"code","b4ae37fb":"code","f0d3ae2a":"code","3c908a64":"code","1452f837":"code","72567eff":"code","e927b7f3":"code","4d59bef8":"markdown","b01e940d":"markdown","cac622f9":"markdown","b3f33b22":"markdown","2200859d":"markdown","ffb9ae9d":"markdown","a64ae672":"markdown","6db5e799":"markdown","c9ee43ea":"markdown","1d828099":"markdown","0826e69c":"markdown","200901fb":"markdown"},"source":{"41bcc359":"# Data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pandas as pd\n\n# SciKit Learn\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn.model_selection import cross_val_score, train_test_split\n\n# For visualizing the tree\nfrom graphviz import Source\nfrom IPython.display import SVG","74958d65":"data = pd.read_csv(\"..\/input\/Iris.csv\", index_col=\"Id\")\ndata.head()","61fd6bc5":"iris_features = [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]\niris_targets = \"Species\"\niris_train, iris_test = train_test_split(data, test_size=0.2) # We want a 80%\/20% split for training\/testing\nprint(f\"Train: {len(iris_train)} rows\\nTest: {len(iris_test)} rows\")","6ed98db6":"iris_classifier = DecisionTreeClassifier(random_state=0)\nmodel = iris_classifier.fit(X=iris_train[iris_features], y=iris_train[iris_targets])\nprint(model)","35f22201":"model.score(X=iris_test[iris_features], y=iris_test[iris_targets])","b4ae37fb":"graph = Source(export_graphviz(model, out_file=None, feature_names=iris_features, filled=True, class_names=model.classes_))\nSVG(graph.pipe(format='svg'))","f0d3ae2a":"from sklearn.ensemble import RandomForestClassifier","3c908a64":"forest = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=2)","1452f837":"forest_model = forest.fit(X=iris_train[iris_features], y=iris_train[iris_targets])\nsize, index = min((estimator.tree_.node_count, idx) for (idx,estimator) in enumerate(forest.estimators_))\nprint(f'The smallest tree has {size} nodes!')","72567eff":"smallest_tree = forest_model.estimators_[index]\nsmallest_tree = smallest_tree.fit(X=iris_train[iris_features], y=iris_train[iris_targets])\nsmallest_tree.score(X=iris_test[iris_features], y=iris_test[iris_targets])","e927b7f3":"graph = Source(export_graphviz(smallest_tree, out_file=None, feature_names=iris_features, filled=True, class_names=model.classes_))\nSVG(graph.pipe(format='svg'))","4d59bef8":"So this tree has a mere five nodes but still remains over 90% accurate. Let's see what is looks like","b01e940d":"**As you can see, this tree performs equally well with much fewer decisions. This tells us a few things, for example: most of the features are irrelevant for this prediction!**","cac622f9":"## At this point, we can stop and be happy with our tree. But can we do better?\nIf we try to build the tree using a plethora of random states, we'll find that some output better trees than others. \n\nLet's try to optimize the tree using a method called Random Forest","b3f33b22":"# Step 3. Create the Decision Tree Classifier and fit it to the data\nWe want a 80%\/20% split for training\/testing","2200859d":"# Step 5. Build 100 random Decision Trees using the RandomForestClassifier","ffb9ae9d":"# Step 4. Determine our the model's accuracy with the test data","a64ae672":"# Step 2. Extract the data into features (inputs) and targets (outputs) and create a train\/test split","6db5e799":"# Step 6. Fit the trees in the forest, and determine the 'best' one.\nIn this case, we're going to look for the tree with the lowest node count (ie the smallest tree).","c9ee43ea":"# Let's Take a look at the decision tree","1d828099":"Wow, that's much smaller than the tree we made above!","0826e69c":"# Step 1. Read the data into a Pandas DataFrame","200901fb":"Over 90% -- not too bad!"}}