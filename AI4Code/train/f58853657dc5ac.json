{"cell_type":{"227dc74a":"code","e8d5622d":"code","6f43824e":"code","bfdbd0fd":"code","702796dd":"code","802da2d4":"code","dfdfc24b":"code","625a3d27":"code","763f8efc":"code","0a245c1d":"code","ff765e47":"code","4c6162b6":"code","2fb7fc37":"code","c1b7d035":"code","72b40f02":"code","dcbb79c9":"code","93634ea3":"code","95acb8d5":"code","ae2c2061":"code","9d793ce0":"code","6789700f":"code","115917ad":"code","ed23abf0":"code","af5fe103":"code","f5fb5e6a":"code","9a0295eb":"code","934f87f0":"code","3cfe9890":"code","b041eaa1":"code","ccb177cb":"code","45904d95":"code","714a861d":"code","f3b850d2":"code","84b8b478":"code","63ced622":"code","12e437d3":"code","38130406":"code","0e2125d6":"code","aa0a90fa":"code","ad5d6b30":"code","aa1d108d":"code","69305684":"code","c532693a":"code","26f6f6fe":"markdown","f57bb198":"markdown","32e51ae3":"markdown","e908c6f9":"markdown","76ad727c":"markdown","c42fb2f3":"markdown","41d57057":"markdown"},"source":{"227dc74a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Flatten, Dense, Embedding, Dropout, GlobalAveragePooling1D, LSTM, \\\nConv1D, MaxPool1D, SpatialDropout1D, Bidirectional, GRU, Input, BatchNormalization\nfrom tensorflow.keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import KFold, RandomizedSearchCV, cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom nltk.tokenize import word_tokenize, TweetTokenizer\nfrom nltk.corpus import stopwords\nfrom string import punctuation\nfrom xgboost import XGBClassifier\nfrom tqdm.notebook import tqdm\nimport plotly.express as px\nimport joblib\n\n\nfrom transformers import BertTokenizer, TFBertModel, DistilBertTokenizerFast, TFDistilBertModel\n\n#import kaggle\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n","e8d5622d":"from nltk.corpus import stopwords\nstopwords = stopwords.words('english')\npunct = dict.fromkeys(punctuation, ' ')\ntok = TweetTokenizer()","6f43824e":"#glove_embd = joblib.load('..\/input\/pickled-glove840b300d-for-10sec-loading\/glove.840B.300d.pkl')\nglove_embd = np.load('..\/input\/pickled-glove840b300d-for-10sec-loading\/glove.840B.300d.pkl', allow_pickle= True)","bfdbd0fd":"data = pd.read_csv(r'\/kaggle\/input\/nlp-getting-started\/train.csv', encoding='utf8')\ndata.drop(columns=['id'], inplace=True)\ndata.fillna(' ', inplace=True)\ndata['text'] = data['keyword'].astype(str) + ' ' + data['location'].astype(str) + ' ' + data['text'].astype(str)\n\n#data2 = data.copy()\n#data = pd.concat([data,data2])","702796dd":"def comments_processor(data):\n    corpus = []\n    data = data.lower()\n    data = re.sub(pattern=r'(((http)(s)?|www(.)?)(:\/\/)?\\S+)', repl='', string=data)\n    data = re.sub(pattern=r'\\d', repl='', string=data)\n    data = str.translate(data, str.maketrans(punct))\n    data = re.sub(pattern=r'\\s{2,}', repl=' ', string=data)\n    data = tok.tokenize(data)\n    data = [item for item in data if item not in stopwords]\n    data = ' '.join([item for item in data if item not in stopwords])\n    \n    return data","802da2d4":"data['text'] = data['text'].apply(comments_processor)","dfdfc24b":"X_train, X_test, y_train, y_test = train_test_split(data['text'], data['target'].values, train_size = 0.85,\\\n                                                    random_state = 11, stratify = data['target'])","625a3d27":"count_vec = CountVectorizer(max_df = 0.8, min_df = 5)\ncount_vec.fit_transform(X_train)","763f8efc":"%%time\nclf = Pipeline([('vect', count_vec),\\\n                ('classifier', RandomForestClassifier( n_jobs=-1, random_state=33))])\nclf.fit(X_train, np.array(y_train))\ny_pred = clf.predict(X_test)\naccuracy_score(y_test, y_pred)","0a245c1d":"%%time\nclf2 = Pipeline([('vect', count_vec),\\\n                ('classifier', XGBClassifier(n_jobs=-1, random_state=33, eta = 0.9, max_delta_step = 5,gamma = 1))])\n\nclf2.fit(X_train, y_train)\ny_pred = clf2.predict(X_test)\naccuracy_score(y_test, y_pred) ","ff765e47":"%%time\nclf3 = Pipeline([('vect', count_vec),\\\n                ('classifier', SVC(random_state=33))])\n\nclf3.fit(X_train, y_train)\ny_pred = clf3.predict(X_test)\naccuracy_score(y_test, y_pred) ","4c6162b6":"pd.DataFrame.from_dict(classification_report(y_test, y_pred, output_dict=True)).T","2fb7fc37":"from nltk.corpus import stopwords\nstopwords = stopwords.words('english')\npunct = dict.fromkeys(punctuation, ' ')\ntok = TweetTokenizer()","c1b7d035":"data = pd.read_csv(r'\/kaggle\/input\/nlp-getting-started\/train.csv', encoding='utf8')\ndata.drop(columns=['id'], inplace=True)\ndata.fillna(' ', inplace=True)\ndata['text'] = data['keyword'].astype(str) + ' ' + data['location'].astype(str) + ' ' + data['text'].astype(str)","72b40f02":"max_len = 50\nembed_dim = 300\npad_type = 'post'\ntrun_type = 'post'","dcbb79c9":"def comments_processor(data):\n    corpus = []\n    data = data.lower()\n    data = re.sub(pattern=r'(((http)(s)?|www(.)?)(:\/\/)?\\S+)', repl='', string=data)\n    data = re.sub(pattern=r'\\d', repl='', string=data)\n    data = re.sub(pattern=r\"n't\", repl=' not', string=data)\n    data = str.translate(data, str.maketrans(punct))\n    data = re.sub(pattern=r'\\s{2,}', repl=' ', string=data)\n    data = tok.tokenize(data)\n    data = [item for item in data if item not in stopwords]\n    corpus.extend(data)\n    \n    return corpus\n    \ncorpus = [' '.join(corpus) for corpus in data['text'].apply(comments_processor)]","93634ea3":"tokenizer = Tokenizer()\n\ntokenizer.fit_on_texts(corpus)\n\nword_index = tokenizer.word_index \n\ncorpus_seq = tokenizer.texts_to_sequences(corpus)\ncorpus_padded = pad_sequences(corpus_seq, maxlen= max_len, padding= pad_type, truncating= trun_type)","95acb8d5":"len(word_index)","ae2c2061":"num_words = len(word_index)+1\n\nembedding_matrix = np.zeros((num_words,300))\noov = dict()\nfor word, i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n    try:\n        embed_vec = glove_embd[word]\n    except:\n        continue\n    if embed_vec is not None:\n       embedding_matrix[i] =  embed_vec","9d793ce0":"X_train, X_test, y_train, y_test = train_test_split(corpus_padded, data['target'].values, train_size = 0.85,\\\n                                                    random_state = 11, stratify = data['target'])","6789700f":"train_padded = np.array(X_train)\ny_train = np.array(y_train)\ntest_padded = np.array(X_test)\ny_test = np.array(y_test)","115917ad":"model = Sequential()\nembedding = Embedding(num_words, embed_dim, embeddings_initializer = Constant(embedding_matrix),\\\n                      input_length = max_len, trainable = False)\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(Conv1D(128, 8))\nmodel.add(MaxPool1D())\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(Flatten())\nmodel.add(Dense(24, activation = 'relu'))\nmodel.add(Dense(1, activation = 'sigmoid'))\n\nmodel.compile(loss = 'binary_crossentropy', optimizer =\\\n              tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), metrics = ['accuracy'])\nmodel.summary()","ed23abf0":"history = model.fit(train_padded, y_train, batch_size = 4, epochs = 12,\\\n                    validation_data = (test_padded, y_test), verbose = 2)","af5fe103":"model_rnn = Sequential()\nembedding = Embedding(num_words, embed_dim, embeddings_initializer = Constant(embedding_matrix),\\\n                      input_length = max_len, trainable = False)\nmodel_rnn.add(embedding)\nmodel_rnn.add(SpatialDropout1D(0.3))\nmodel_rnn.add(Bidirectional(LSTM(64, recurrent_dropout = 0.2, return_sequences = True)))\nmodel_rnn.add(Dropout(0.2))\nmodel_rnn.add(LSTM(32, recurrent_dropout = 0.2))\nmodel_rnn.add(Flatten())\nmodel_rnn.add(Dropout(0.2))\nmodel_rnn.add(Dense(32, activation = 'relu')) #256\nmodel_rnn.add(Dense(1, activation = 'sigmoid'))\n\nmodel_rnn.compile(loss = 'binary_crossentropy', optimizer = \\\n                  tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), metrics = ['accuracy'])\nmodel_rnn.summary()","f5fb5e6a":"history_rnn = model_rnn.fit(train_padded, y_train, batch_size = 4, epochs = 10,\\\n                            validation_data = (test_padded, y_test), verbose = 2)","9a0295eb":"model_gru = Sequential()\nembedding = Embedding(num_words, embed_dim, embeddings_initializer = Constant(embedding_matrix),\\\n                      input_length = max_len, trainable = False)\nmodel_gru.add(embedding)\nmodel_gru.add(SpatialDropout1D(0.2))\nmodel_gru.add(Bidirectional(GRU(64, recurrent_dropout = 0.2, return_sequences = True)))\nmodel_gru.add(Dropout(0.2))\nmodel_gru.add(Bidirectional(GRU(32, recurrent_dropout = 0.2)))\nmodel_gru.add(Flatten())\nmodel_gru.add(Dropout(0.2))\nmodel_gru.add(Dense(32, activation = 'relu'))\nmodel_gru.add(Dense(1, activation = 'sigmoid'))\n\nmodel_gru.compile(loss = 'binary_crossentropy', optimizer = \\\n                  tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), metrics = ['accuracy'])\nmodel_gru.summary()","934f87f0":"history_gru = model_gru.fit(train_padded, y_train, batch_size = 4, epochs = 10, validation_data = (test_padded, y_test), verbose = 2)","3cfe9890":"test_data = pd.read_csv(r'\/kaggle\/input\/nlp-getting-started\/test.csv', encoding='utf8')\ntest_data.fillna(' ', inplace=True)\ntest_data['text'] = test_data['keyword'].astype(str) + ' ' + test_data['location'].astype(str) + ' ' + test_data['text'].astype(str)","b041eaa1":"test_corpus = [' '.join(corpus) for corpus in test_data['text'].apply(comments_processor)]","ccb177cb":"test_corpus_seq = tokenizer.texts_to_sequences(test_corpus)\ntest_corpus_pad = pad_sequences(test_corpus_seq, maxlen= max_len, truncating=trun_type, padding= pad_type)","45904d95":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\nbert_layer = TFBertModel.from_pretrained('bert-base-uncased')","714a861d":"distil_tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n\ndistil_bert_layer = TFDistilBertModel.from_pretrained('distilbert-base-uncased')","f3b850d2":"max_len = 100","84b8b478":"def tweek_tokenizer(tweets, tokenizer ,max_len = max_len):\n    input_ids = []\n    \n    \n    for text in tweets:\n        \n        text = re.sub(pattern=r'(((http)(s)?|www(.)?)(:\/\/)?\\S+)', repl='', string=text)\n        #text = re.sub(pattern=r'([^a-zA-Z0-9\\s])', repl=' ', string=text)\n        text = re.sub(pattern=r'\\s{2,}', repl=' ', string=text)\n        \n        tokenized_text = tokenizer.encode_plus(text, add_special_tokens = True, max_length = max_len, \\\n                                               pad_to_max_length = True, return_tensors = 'tf')   \n        \n        input_ids.append(tokenized_text['input_ids'])    \n    \n    input_ids = np.array(input_ids).reshape([-1,max_len])\n    \n    return input_ids","63ced622":"def bert_model(bert_layer, max_len = max_len):\n    \n    input_tokens = Input(shape=(max_len), dtype= tf.int32, name='input_ids')\n    \n    bert_out = bert_layer([input_tokens])[0] #, token_type_ids, attention_mask \n    \n    bert_out = tf.keras.layers.Lambda(lambda seq : seq [:,0,:])(bert_out)\n    \n    bert_out = Dropout(0.5)(bert_out)\n    \n    dense_1 = Dense(768, activation = 'tanh', kernel_regularizer = tf.keras.regularizers.L2(l2= 0.1))(bert_out)\n    \n    drop_output = Dropout(0.3)(dense_1)\n\n    dense_out = Dense(1, activation = 'sigmoid', name= 'output2')(drop_output)\n     \n    model = Model(inputs = input_tokens, outputs = dense_out)\n    \n    model.compile(optimizer = tf.keras.optimizers.Adam(lr=2e-5, decay = 0.01), \\\n                  loss = 'binary_crossentropy', metrics = ['accuracy'])\n    \n    return model","12e437d3":"%%time\ninput_ids = tweek_tokenizer(data['text'].values, tokenizer ,max_len=max_len)","38130406":"model = bert_model(bert_layer)\nmodel.summary()","0e2125d6":"model.fit(x= [input_ids], y= np.array(data['target'].values), batch_size = 16,epochs= 3, shuffle= True,validation_split= 0.2)","aa0a90fa":"test_data = pd.read_csv(r'\/kaggle\/input\/nlp-getting-started\/test.csv', encoding='utf8')\ntest_data.fillna(' ', inplace=True)\ntest_data['text'] = test_data['keyword'].astype(str) + ' ' + test_data['location'].astype(str) + ' ' + test_data['text'].astype(str)","ad5d6b30":"%%time\ntest_input_ids= tweek_tokenizer(test_data['text'].values, tokenizer ,max_len=max_len)","aa1d108d":"%%time\ntest_pred = model.predict(test_input_ids)\ntest_pred = np.round(test_pred).astype(int)\ntest_pred = test_pred.reshape(3263)","69305684":"sample_submission = pd.DataFrame({'id' : test_data['id'].values.tolist(), 'target' : test_pred})","c532693a":"sample_submission.to_csv(r'sample_submission.csv', encoding='utf8', index= False, header = True)","26f6f6fe":"# **BERT Model**","f57bb198":"# **Predictions**","32e51ae3":"# **LSTM RNN Networks**","e908c6f9":"# **CNN Network**","76ad727c":"# **Traditional ML Techniques**","c42fb2f3":"# **Below is the code where i have tried differenct type of Neural Network**","41d57057":"# **GRU RNN Network**"}}