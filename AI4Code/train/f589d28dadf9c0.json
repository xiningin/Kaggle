{"cell_type":{"87ea695a":"code","fc4d1dd7":"code","bd0f7bdf":"code","309796c5":"code","bbc6a506":"code","4a21c2f2":"code","2fc09b22":"code","07ba61ce":"code","908675cd":"code","4a1d33a1":"code","0bcf40cb":"code","e9aa83c1":"code","1781d55c":"code","bc3d781e":"code","58ecd4fe":"code","721b5509":"code","f2efe39c":"code","7a8bbf51":"code","91426ebe":"code","9e1efbd6":"code","c1adc94a":"code","a879125c":"code","2389a322":"code","08cbe2c4":"code","26058827":"code","bb4fb3ad":"code","87695250":"code","6bfec514":"code","58034cef":"code","5aad8597":"code","013655c5":"code","6a881b86":"code","e046968d":"code","95a98a22":"code","32de3256":"code","31d545f8":"code","29a2bf22":"code","6499b85d":"code","d5d3e042":"code","c19cd149":"code","c7640b30":"code","0518b7c4":"code","5e8955ac":"code","be2fa389":"code","37cd14d7":"code","fe5b7e64":"code","a44b9e41":"code","bae06342":"code","b424e98b":"code","6de4996f":"code","4c4db8ea":"code","3089c4c7":"code","7c36be2e":"markdown","45461c47":"markdown","20ea3c19":"markdown","5ffd467d":"markdown","7b7516f1":"markdown","fa5150c9":"markdown","efc26292":"markdown","d35a2518":"markdown","a6b0ffe3":"markdown"},"source":{"87ea695a":"import numpy as np \nimport pandas as pd \nimport keras\nfrom keras.utils import to_categorical\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import regularizers\nimport matplotlib.pyplot as plt","fc4d1dd7":"#split train and test images and labels\n(train_images , train_labels), (test_images , test_labels) = mnist.load_data()","bd0f7bdf":"print(train_images.shape)\ntrain_labels","309796c5":"#reshape and normalize the images\ntrain_images = train_images.reshape((60000 , 28 * 28))\ntrain_images = train_images.astype('float32') \/ 255\ntest_images = test_images.reshape((10000 , 28 * 28))\ntest_images = test_images.astype('float32') \/ 255","bbc6a506":"#categoricaling all labels\ntrain_labels = to_categorical (train_labels)\ntest_labels = to_categorical (test_labels)","4a21c2f2":"#define validation images and labels number:20000\nx_train= train_images[:40000]\nx_validation=train_images[40000:]\nx_labels = train_labels[:40000]\ny_validation = train_labels[40000:]\n","2fc09b22":"#difine model based on dense layer\nmodel1 = Sequential()\nmodel1.add(Dense(512, activation='relu', input_shape=(28 * 28,)))\nmodel1.add(Dense(64, activation='relu'))\nmodel1.add(Dense(10, activation='softmax'))","07ba61ce":"#compile model \nmodel1.compile (optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])","908675cd":"#fit the model\nhistory1=model1.fit(train_images , train_labels ,validation_data=(x_validation,y_validation),\n                  epochs=10, batch_size=128)","4a1d33a1":"test_loss , test_accuracy = model1.evaluate(test_images , test_labels)","0bcf40cb":"history_dict1 = history1.history\nhistory_dict1.keys()","e9aa83c1":"org_loss_values = history_dict1['loss']\norg_val_loss_values = history_dict1['val_loss']\nplt.plot(org_loss_values, 'bo', label='Training loss')\nplt.plot(org_val_loss_values, 'b', label='Validation loss')\nplt.title('Training and validation loss of original method')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","1781d55c":"org_acc_values = history_dict1['accuracy']\norg_val_acc_values = history_dict1['val_accuracy']\nplt.plot(org_acc_values, 'bo', label='Training accuracy')\nplt.plot(org_val_acc_values, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy of original method')\nplt.xlabel('Epochs')\nplt.ylabel('accuracy')\nplt.legend()\nplt.show()","bc3d781e":"#difine model WITH LOWER CAPACITY\nmodel2 = Sequential()\nmodel2.add(Dense(32, activation='relu', input_shape=(28 * 28,)))\nmodel1.add(Dense(12, activation='relu'))\nmodel2.add(Dense(10, activation='softmax'))","58ecd4fe":"#compile model \nmodel2.compile (optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])","721b5509":"#fit the model\nhistory2=model2.fit(train_images , train_labels ,validation_data=(x_validation,y_validation),\n                  epochs=10, batch_size=128)","f2efe39c":"test_loss , test_accuracy = model2.evaluate(test_images , test_labels)","7a8bbf51":"history_dict2 = history2.history\nhistory_dict2.keys()","91426ebe":"low_loss_values = history_dict2['loss']\nlow_val_loss_values = history_dict2['val_loss']\nplt.plot(low_loss_values, 'bo', label='Training loss')\nplt.plot(low_val_loss_values, 'b', label='Validation loss')\nplt.title('Training and validation loss low capacity')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","9e1efbd6":"low_acc_values = history_dict2['accuracy']\nlow_val_acc_values = history_dict2['val_accuracy']\nplt.plot(low_acc_values, 'bo', label='Training accuracy')\nplt.plot(low_val_acc_values, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy low capacity')\nplt.xlabel('Epochs')\nplt.ylabel('accuracy')\nplt.legend()\nplt.show()","c1adc94a":"#difine model USING DROPOUT\nmodel3 = Sequential()\nmodel3.add(Dense(512, activation='relu', input_shape=(28 * 28,)))\nmodel3.add(Dropout(0.5))\nmodel3.add(Dense(64, activation='relu'))\nmodel3.add(Dropout(0.5))\nmodel3.add(Dense(10, activation='softmax'))","a879125c":"#compile model \nmodel3.compile (optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])","2389a322":"#fit the model\nhistory3=model3.fit(train_images , train_labels ,validation_data=(x_validation,y_validation),\n                  epochs=10, batch_size=128)","08cbe2c4":"test_loss , test_accuracy = model3.evaluate(test_images , test_labels)","26058827":"history_dict3 = history3.history\nhistory_dict3.keys()","bb4fb3ad":"drop_loss_values = history_dict3['loss']\ndrop_val_loss_values = history_dict3['val_loss']\nplt.plot(drop_loss_values, 'bo', label='Training loss')\nplt.plot(drop_val_loss_values, 'b', label='Validation loss')\nplt.title('Training and validation loss using dropout')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","87695250":"drop_acc_values = history_dict3['accuracy']\ndrop_val_acc_values = history_dict3['val_accuracy']\nplt.plot(drop_acc_values, 'bo', label='Training accuracy')\nplt.plot(drop_val_acc_values, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy usinf dropout')\nplt.xlabel('Epochs')\nplt.ylabel('accuracy')\nplt.legend()\nplt.show()","6bfec514":"#define model USING REGULARIZER\nmodel4 = Sequential()\nmodel4.add(Dense(512,kernel_regularizer=regularizers.l1(0.001), activation='relu', input_shape=(28 * 28,)))\nmodel4.add(Dense(64,kernel_regularizer=regularizers.l1(0.001), activation='relu'))\nmodel4.add(Dense(10, activation='softmax'))","58034cef":"#compile model \nmodel4.compile (optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])","5aad8597":"#fit the model\nhistory4=model4.fit(train_images , train_labels ,validation_data=(x_validation,y_validation),\n                  epochs=10, batch_size=128)","013655c5":"test_loss , test_accuracy = model4.evaluate(test_images , test_labels)","6a881b86":"history_dict4 = history4.history\nhistory_dict4.keys()","e046968d":"l1_loss_values = history_dict4['loss']\nl1_val_loss_values = history_dict4['val_loss']\nplt.plot(l1_loss_values, 'bo', label='Training loss')\nplt.plot(l1_val_loss_values, 'b', label='Validation loss')\nplt.title('Training and validation loss using l1')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","95a98a22":"l1_acc_values = history_dict4['accuracy']\nl1_val_acc_values = history_dict4['val_accuracy']\nplt.plot(l1_acc_values, 'bo', label='Training accuracy')\nplt.plot(l1_val_acc_values, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy using l1')\nplt.xlabel('Epochs')\nplt.ylabel('accuracy')\nplt.legend()\nplt.show()","32de3256":"#define model using regularizers.l2\nmodel5 = Sequential()\nmodel5.add(Dense(512,kernel_regularizer=regularizers.l2(0.001), activation='relu', input_shape=(28 * 28,)))\nmodel5.add(Dense(64,kernel_regularizer=regularizers.l2(0.001), activation='relu'))\nmodel5.add(Dense(10, activation='softmax'))","31d545f8":"#compile model \nmodel5.compile (optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])","29a2bf22":"#fit the model\nhistory5=model5.fit(train_images , train_labels ,validation_data=(x_validation,y_validation),\n                  epochs=10, batch_size=128)","6499b85d":"test_loss , test_accuracy = model5.evaluate(test_images , test_labels)","d5d3e042":"history_dict5 = history5.history\nhistory_dict5.keys()","c19cd149":"l2_loss_values = history_dict5['loss']\nl2_val_loss_values = history_dict5['val_loss']\nplt.plot(l2_loss_values, 'bo', label='Training loss')\nplt.plot(l2_val_loss_values, 'b', label='Validation loss')\nplt.title('Training and validation loss using l2')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n","c7640b30":"l2_acc_values = history_dict5['accuracy']\nl2_val_acc_values = history_dict5['val_accuracy']\nplt.plot(l2_acc_values, 'bo', label='Training accuracy')\nplt.plot(l2_val_acc_values, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy using l2')\nplt.xlabel('Epochs')\nplt.ylabel('accuracy')\nplt.legend()\nplt.show()","0518b7c4":"#define model using regularizers.l1_l2\nmodel6 = Sequential()\nmodel6.add(Dense(512,kernel_regularizer=regularizers.l1_l2(0.001), activation='relu', input_shape=(28 * 28,)))\nmodel6.add(Dense(64,kernel_regularizer=regularizers.l1_l2(0.001), activation='relu'))\nmodel6.add(Dense(10, activation='softmax'))","5e8955ac":"#compile model \nmodel6.compile (optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])","be2fa389":"#fit the model\nhistory6=model6.fit(train_images , train_labels ,validation_data=(x_validation,y_validation),\n                  epochs=10, batch_size=128)","37cd14d7":"test_loss , test_accuracy = model6.evaluate(test_images , test_labels)","fe5b7e64":"history_dict6 = history6.history\nhistory_dict6.keys()","a44b9e41":"l1_l2_loss_values = history_dict6['loss']\nl1_l2_val_loss_values = history_dict6['val_loss']\nplt.plot(l1_l2_loss_values, 'bo', label='Training loss')\nplt.plot(l1_l2_val_loss_values, 'b', label='Validation loss')\nplt.title('Training and validation loss using l1_l2')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","bae06342":"l1_l2_acc_values = history_dict6['accuracy']\nl1_l2_val_acc_values = history_dict6['val_accuracy']\nplt.plot(l1_l2_acc_values, 'bo', label='Training accuracy')\nplt.plot(l1_l2_val_acc_values, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy using l1_l2')\nplt.xlabel('Epochs')\nplt.ylabel('accuracy')\nplt.legend()\nplt.show()","b424e98b":"plt.plot(org_acc_values,'bo',label='original accuracy')\nplt.plot(low_acc_values,'r',label='low capacity accuracy')\nplt.plot(drop_acc_values,'g',label='dropout accuracy')\nplt.plot(l1_acc_values,'y',label='l1 accuracy')\nplt.plot(l2_acc_values,'p',label='l2 accuracy')\nplt.plot(l1_l2_acc_values,'b',label='l1_l2 accuracy')\nplt.title('Training accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('accuracy')\nplt.legend()\nplt.show()","6de4996f":"plt.plot(org_val_acc_values,'bo',label='original accuracy')\nplt.plot(low_val_acc_values,'r',label='low capacity accuracy')\nplt.plot(drop_val_acc_values,'g',label='dropout accuracy')\nplt.plot(l1_val_acc_values,'y',label='l1 accuracy')\nplt.plot(l2_val_acc_values,'p',label='l2 accuracy')\nplt.plot(l1_l2_val_acc_values,'b',label='l1_l2 accuracy')\nplt.title('Validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('accuracy')\nplt.legend()\nplt.show()","4c4db8ea":"plt.plot(org_loss_values,'bo',label='original loss')\nplt.plot(low_loss_values,'r',label='low capacity loss')\nplt.plot(drop_loss_values,'g',label='dropout loss')\nplt.plot(l1_loss_values,'y',label='l1 loss')\nplt.plot(l2_loss_values,'p',label='l2 loss')\nplt.plot(l1_l2_loss_values,'b',label='l1_l2 loss')\nplt.title('Training loss')\nplt.xlabel('Epochs')\nplt.ylabel('loss')\nplt.legend()\nplt.show()","3089c4c7":"plt.plot(org_val_loss_values,'bo',label='original loss')\nplt.plot(low_val_loss_values,'r',label='low capacity loss')\nplt.plot(drop_val_loss_values,'g',label='dropout loss')\nplt.plot(l1_val_loss_values,'y',label='l1 loss')\nplt.plot(l2_val_loss_values,'p',label='l2 loss')\nplt.plot(l1_l2_val_loss_values,'b',label='l1_l2 loss')\nplt.title('Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('loss')\nplt.legend()\nplt.show()","7c36be2e":"VERSION WITH LOWER CAPACITY - END-","45461c47":"**VERSION WITH LOWER CAPACITY - BEGIN-**","20ea3c19":"**ORIGINAL VERSION (OVERFITTING) - END -**","5ffd467d":"VERSION USING DROPOUT - END-","7b7516f1":"VERSION USING REGULARIZER - END-","fa5150c9":"Comparing all methods","efc26292":"VERSION USING REGULARIZER - BEGIN-","d35a2518":"VERSION USING DROPOUT - BEGIN-","a6b0ffe3":"***********************ORININAL VERSION (OVERFITTING) - BEGIN-**************************"}}