{"cell_type":{"a4b0ee8f":"code","f5d9359d":"code","25d42a70":"code","e18d784a":"code","510658a6":"code","6f36ddec":"code","582df284":"code","6c370903":"code","2eb7d5a2":"code","1d44d5ee":"code","183240c4":"code","dead9467":"code","e0b33006":"code","248d8771":"code","e7b1fd43":"code","6e988481":"code","4dc8a8a0":"code","8e278970":"code","c8429298":"code","3d61b5f6":"code","a30603c2":"code","c2575658":"code","26978cca":"code","5553a4e4":"code","d4de6828":"code","726693ed":"code","60e545d9":"code","fe638ad9":"code","5cc7a29b":"code","ced2699a":"code","49e4e570":"code","8411c81f":"code","a94425fe":"code","89ef08e4":"code","9365302f":"code","6f2f1a24":"code","f5e70091":"code","aba38098":"code","f56b23c3":"code","22b4d4a7":"code","0d8f37e7":"code","2e2781d8":"code","f2bbb9d9":"code","a4f4d87a":"code","8af41f80":"code","3d59f725":"code","ff7db88b":"code","1722d2af":"code","9d0d4953":"code","2f9bb663":"code","959f8fd5":"code","e21c7f8b":"code","9ab53b5a":"code","961adf68":"markdown","a251eb3d":"markdown","52286282":"markdown","5d5c35fc":"markdown","3c86bd06":"markdown","da3fcef7":"markdown","b67ff122":"markdown","5a732c9e":"markdown","0eadf047":"markdown","6d9e2d67":"markdown","2a715b74":"markdown","2484eece":"markdown","3fe0b5da":"markdown","5f928160":"markdown","ccdc530d":"markdown","ea34c578":"markdown","d36aaae4":"markdown","2cc0107c":"markdown","c5f75b3f":"markdown","c765dc98":"markdown","06676e8d":"markdown","7f947a65":"markdown","476415c0":"markdown","14afeddf":"markdown","3c22cfc9":"markdown","9951a498":"markdown","533a777a":"markdown","d4baf34c":"markdown","1dc3a21c":"markdown","50c81900":"markdown","b1d01ce6":"markdown","5e55d493":"markdown","71c3a252":"markdown","289c431e":"markdown","99b8ae6a":"markdown","c8371275":"markdown","ad0b1579":"markdown","883788fe":"markdown","04c77172":"markdown","03b9be9f":"markdown","d4c15b8d":"markdown","ef0a9e5d":"markdown","af480b46":"markdown","df9a866a":"markdown","87c520c2":"markdown","12c1ce2e":"markdown","7e3a7ebc":"markdown","4060c504":"markdown","01f8bce0":"markdown"},"source":{"a4b0ee8f":"import os\nimport re\nimport gc\nimport sys\nimport time\nimport math\nimport glob\nimport json\nimport spacy\nimport pandas\nimport random\nimport pickle\nimport pprint\nimport requests\nimport numpy as np\nimport cufflinks as cf\n\nfrom pathlib import Path\nfrom spacy import displacy\nfrom tqdm.notebook import tqdm\nfrom collections import Counter \nfrom collections import defaultdict\nfrom nltk.corpus import stopwords\nfrom gensim.utils import lemmatize\nfrom IPython.core.display import HTML, Markdown\n\nimport xml.etree.ElementTree\nfrom json import JSONDecodeError\nfrom urllib.error import HTTPError\nfrom xml.etree.ElementTree import ParseError\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# defining some options for displacy\ncolors = {\"KP\": \"#2fbbab\"}\noptions = {\"ents\": [\"KP\"], \"colors\": colors}\n\n# defining some options for pandas\npandas.set_option('display.max_rows', 5)\npandas.set_option('display.max_columns', None)\n\n# defining some options for cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)","f5d9359d":"v6_path = '\/kaggle\/input\/CORD-19-research-challenge'\ndf = pandas.read_csv(f'{v6_path}\/metadata.csv', dtype={\n    'sha': str,\n    'abstract': str,\n    'body_text': object,\n    'title': str,\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'WHO #Covidence': str,\n    'pmcid': str,\n    'doi': str,\n    'url': str\n}, keep_default_na=False)","25d42a70":"pdf_json_dict = dict()\nfor filename in tqdm(glob.glob(f'{v6_path}\/**\/pdf_json\/*.json', recursive=True), leave = False): \n    pdf_json_dict[json.load(open(filename, 'rb'))['paper_id']] = filename\nprint(len(pdf_json_dict), \"papers from PDF parsing\")\n\nxml_json_dict = dict()\nfor filename in tqdm(glob.glob(f'{v6_path}\/**\/pmc_json\/*.json', recursive=True), leave = False): \n    xml_json_dict[json.load(open(filename, 'rb'))['paper_id']] = filename\nprint(len(xml_json_dict), \"papers from XML parsing\")","e18d784a":"df.loc[df.index[df[\"pmcid\"] == \"PMC261871\"], ['sha', 'doi', 'title']]","510658a6":"print(json.load(open(pdf_json_dict[\"941a9a3f0138f7c01ff930023a4aa428a18170e1\"], 'rb'))['metadata']['title'])","6f36ddec":"print(json.load(open(xml_json_dict[\"PMC261871\"], 'rb'))['metadata']['title'])","582df284":"print(np.sum([1 if \"abstract\" in json.load(open(xml_json_dict[key], 'rb')) else 0 for key in tqdm(xml_json_dict, leave = False)]), \" abstracts in the XML file\")\nprint(np.sum([1 if \"body_text\" in json.load(open(xml_json_dict[key], 'rb')) else 0 for key in tqdm(xml_json_dict, leave = False)]),\" body_text in the XML file\")\nprint(np.sum([1 if (\"abstract\" in json.load(open(pdf_json_dict[key], 'rb')) and json.load(open(pdf_json_dict[key], 'rb'))[\"abstract\"] != []) else 0 for key in tqdm(pdf_json_dict, leave = False)]), \" non empty abstracts in the PDF file\")\nprint(np.sum([1 if (\"body_text\" in json.load(open(pdf_json_dict[key], 'rb')) and json.load(open(pdf_json_dict[key], 'rb'))[\"body_text\"] != []) else 0 for key in tqdm(pdf_json_dict, leave = False)]), \" non empty body_text in the PDF file\")","6c370903":"display(json.load(open(pdf_json_dict[\"a92e70c0a129a56173ef38bf186c1cd7ecfe6d9f\"], 'rb'))['abstract'])","2eb7d5a2":"print(\"For the\",len(df), \"papers in the dataset (v6)\")\nprint(\"-\", len(df[df[\"has_pdf_parse\"]]), \"papers with PDF parse\")\nprint(\"-\", len(df[df[\"has_pmc_xml_parse\"]]), \"papers with XML parse\")\nprint(\"-\", len(df[(df[\"has_pdf_parse\"]) & (df[\"has_pmc_xml_parse\"] == False) ]), \"papers with PDF parse but no XML parse\")","1d44d5ee":"print(\"For the\",len(df), \"papers in the dataset (v6)\")\nprint(\"-\", len(df[df[\"title\"] == \"\"]), \"papers without title\")\nprint(\"-\", len(df[df[\"abstract\"] == \"\"]), \"papers without abstract\")\nprint(\"-\", len(df[df[\"full_text_file\"] == \"\"]), \"papers without fulltext\")","183240c4":"print(len(df[ (df['full_text_file'] == \"\") & (df['pmcid'] != \"\") ]), \"papers without fulltext but a PMC ID as a reference\")\nprint(len(df[ (df['full_text_file'] == \"\") & (df['doi'] != \"\") ]), \"papers without fulltext but a DOI as a reference\")\nprint(len(df[ (df['full_text_file'] == \"\") & (df['Microsoft Academic Paper ID'] != \"\") ]), \"papers without fulltext but a Microsoft Academic Paper ID as a reference\")\nprint(len(df[ (df['full_text_file'] == \"\") & (df['WHO #Covidence'] != \"\") ]), \"papers without fulltext but a WHO #Covidence as a reference\")\nprint(len(df[ (df['full_text_file'] == \"\") & (df['pubmed_id'] != \"\") ]), \"papers without fulltext but a Pubmed id as a reference\")","dead9467":"tqdm.pandas()\ndf[\"sha\"] = df[\"sha\"] = df[\"sha\"].apply(lambda x: x.split(\"; \")[0])\ndf[\"abstract_f\"] = df[\"sha\"].progress_apply(lambda x : json.load(open(pdf_json_dict[x], 'rb'))['abstract'] if x in pdf_json_dict else [] )","e0b33006":"df[\"body_text\"] = df[\"cord_uid\"].apply(lambda x : [])\nfor index, meta_data in tqdm(df.iterrows(), total = len(df.index)) :\n    if meta_data[\"sha\"] != \"\" and meta_data[\"sha\"] in pdf_json_dict :\n        file = json.load(open(pdf_json_dict[meta_data[\"sha\"]], 'rb'))\n        if(file['body_text'] != []) :\n            df.at[(df[df['sha'] == meta_data[\"sha\"]].index)[0], 'body_text'] = file['body_text']\n        \n    if meta_data[\"pmcid\"] != \"\" and meta_data[\"pmcid\"] in xml_json_dict :\n        file = json.load(open(xml_json_dict[meta_data[\"pmcid\"]], 'rb'))\n        if(file['body_text'] != []) :\n            df.at[(df[df['pmcid'] == meta_data[\"pmcid\"]].index)[0], 'body_text'] = file['body_text']  ","248d8771":"del pdf_json_dict\ndel xml_json_dict\ngc.collect()","e7b1fd43":"pandas.set_option('display.max_colwidth', -1)\ndisplay(df.loc[df[df[\"pmcid\"] == \"PMC7106123\"].index, ['title', 'title_f', 'abstract', 'abstract_f']])\npandas.set_option('display.max_colwidth', 50)","6e988481":"print(\"For the\",len(df), \"papers in the dataset (v6)\")\nprint(\"-\", len(df[df[\"abstract\"] != \"\"]), \"papers with one string abstract\")\nprint(\"-\", len(df[df[\"abstract_f\"].apply(lambda x: x != [])]), \"papers with paragraph segmented abstract\")\nprint(\"-\", len(df[df[\"body_text\"].apply(lambda x: x != [])]), \"papers with fulltext\")","4dc8a8a0":"print(len(df[(df['body_text'].apply(lambda x: x == [])) ]), \"papers without fulltext\")\nprint(len(df[(df['abstract_f'].apply(lambda x: x == []))]), \"papers without paragraph segmented abstract\")\nprint(len(df[(df['abstract_f'].apply(lambda x: x == [])) & (df['abstract'] == \"\") ]), \"papers without abstract of any kind\")\nprint(len(df[(df['body_text'].apply(lambda x: x == [])) & ((df['abstract_f'].apply(lambda x: x == [])) & (df['abstract'] == \"\")) ]), \"papers without fulltext or abstract of any kind\")","8e278970":"print(len(df[ (df['body_text'].apply(lambda x: x == [])) & (df['pmcid'] != \"\") ]), \"papers without fulltext but a PMC ID as a reference\")\nprint(len(df[ (df['body_text'].apply(lambda x: x == [])) & (df['doi'] != \"\") ]), \"papers without fulltext but a DOI as a reference\")\nprint(len(df[ (df['body_text'].apply(lambda x: x == [])) & (df['Microsoft Academic Paper ID'] != \"\") ]), \"papers without fulltext but a Microsoft Academic Paper ID as a reference\")\nprint(len(df[ (df['body_text'].apply(lambda x: x == [])) & (df['WHO #Covidence'] != \"\") ]), \"papers without fulltext but a WHO #Covidence as a reference\")\nprint(len(df[ (df['body_text'].apply(lambda x: x == [])) & (df['pubmed_id'] != \"\") ]), \"papers without fulltext but a Pubmed id as a reference\")","c8429298":"pmc_empty_papers = df[ ((df['body_text'].apply(lambda x: x == [])) | (df['abstract_f'].apply(lambda x: x == []))) & (df['pmcid'] != \"\")][\"pmcid\"].tolist()\nprint(len(pmc_empty_papers))\nprint(pmc_empty_papers[:10])","3d61b5f6":"def api_call() :\n    dict_europepmc = {}\n    for paper_id in tqdm(pmc_papers) :\n        try :\n            response = requests.get(\"https:\/\/www.ebi.ac.uk\/europepmc\/webservices\/rest\/\"+str(paper_id)+\"\/fullTextXML\")\n        except HTTPError as http_err:\n            print(f\"HTTP error occurred: {http_err}\")\n        except Exception as err:\n            print(f\"An error ocurred: {err}\")\n    dict_europepmc[paper_id] = response.content\n    #pickle.dump(dict_europepmc, open(\"dict_europepmc.pickle\", \"wb\" ) )","a30603c2":"dict_europepmc = pickle.load(open(\"\/kaggle\/input\/cord19-api-call\/dict_europepmc.pickle\", 'rb'))\ndisplay(len(dict_europepmc))","c2575658":"print(len(df[ ((df['body_text'].apply(lambda x: x == [])) & (df['pmcid'] != \"\")) ]), \"articles without fulltext but with a PMCID\")\nprint(len(df[ ((df['abstract_f'].apply(lambda x: x == [])) & (df['pmcid'] != \"\")) ]), \"articles without (paragraph segmented) abstract but with a PMCID\")\nprint(len(df[ ((df['title'] == \"\") & (df['pmcid'] != \"\")) ]), \"articles without title but with a PMCID\")\ncpt_record = 0\nfor paper_id in tqdm(pmc_empty_papers) :\n    try :\n        root = xml.etree.ElementTree.fromstring(dict_europepmc[paper_id])\n        cpt_record = cpt_record + 1\n\n        abstract = []\n        for child in root.iter('abstract'):\n            if \"\".join(child.itertext()) != \"\" : \n                abstract.append({'text': \"\".join(child.itertext()) }) \n                \n        body_text = []\n        for child in root.iter('body'):\n            for p in child.iter(\"p\"):\n                if \"\".join(p.itertext()) != \"\" : \n                    body_text.append({'text': \"\".join(p.itertext()), 'cite_spans': [], 'ref_spans': [], 'section': 'Body'})  \n  \n        if(df.at[(df[df['pmcid'] == paper_id].index)[0], 'abstract_f'] == []) :\n            df.at[(df[df['pmcid'] == paper_id].index)[0], 'abstract_f'] = abstract\n        if(df.at[(df[df['pmcid'] == paper_id].index)[0], 'body_text'] == []) :\n            df.at[(df[df['pmcid'] == paper_id].index)[0], 'body_text'] = body_text\n    except ParseError as e :\n        pass\n    except KeyError as e :\n        pass\n\nprint(\"Got\",cpt_record,\" matches\")\nprint(len(df[ ((df['body_text'].apply(lambda x: x == [])) & (df['pmcid'] != \"\")) ]), \"articles without fulltext but with a PMCID\")\nprint(len(df[ ((df['abstract_f'].apply(lambda x: x == [])) & (df['pmcid'] != \"\")) ]), \"articles without (paragraph segmented) abstract but with a PMCID\")\nprint(len(df[ ((df['title'] == \"\") & (df['pmcid'] != \"\")) ]), \"articles without title but with a PMCID\")","26978cca":"pmc_empty_papers = df[ ((df['body_text'].apply(lambda x: x == [])) | (df['abstract_f'].apply(lambda x: x == []))) & (df['pmcid'] != \"\")][\"pmcid\"].tolist()\nprint(len(pmc_empty_papers))","5553a4e4":"def api_call() :\n    dict_bionlp = {}\n    for paper_id in tqdm(pmc_papers) :\n        try :\n            response = requests.get(\"https:\/\/www.ncbi.nlm.nih.gov\/research\/bionlp\/RESTful\/pmcoa.cgi\/BioC_json\/\"+str(paper_id)+\"\/unicode\")\n        except HTTPError as http_err:\n            print(f\"HTTP error occurred: {http_err}\")\n        except Exception as err:\n            print(f\"An error ocurred: {err}\")\n    dict_bionlp[paper_id] = response.content\n    pickle.dump(dict_bionlp, open(\"dict_bionlp.pickle\", \"wb\" ) )","d4de6828":"dict_bionlp = pickle.load(open(\"\/kaggle\/input\/cord19-api-call\/dict_bionlp.pickle\", 'rb'))\ndisplay(len(dict_bionlp))","726693ed":"print(len(df[ ((df['body_text'].apply(lambda x: x == [])) & (df['pmcid'] != \"\")) ]), \"articles without fulltext but with a PMCID\")\nprint(len(df[ ((df['abstract_f'].apply(lambda x: x == [])) & (df['pmcid'] != \"\")) ]), \"articles without abstract but with a PMCID\")\nprint(len(df[ ((df['title'] == \"\") & (df['pmcid'] != \"\")) ]), \"articles without title but with a PMCID\")\nfor paper_id in tqdm(pmc_empty_papers) :\n    abtract = \"\"\n    title = \"\"\n    body_text = []\n\n    if paper_id in dict_bionlp :\n        try :\n            response = (json.loads(dict_bionlp[paper_id]))\n            for doc in response[\"documents\"] :\n                cpt_record = cpt_record + 1\n                for passages in doc[\"passages\"] :\n                    if passages[\"infons\"][\"section_type\"] == \"TITLE\" and passages[\"text\"] != \"\" :\n                        title = passages[\"text\"]\n                    elif passages[\"infons\"][\"section_type\"] == \"ABSTRACT\" and passages[\"text\"] != \"\" :\n                        abstract = [{\"text\":passages[\"text\"]}]\n                    elif passages[\"text\"] != \"\" and passages[\"infons\"][\"section_type\"] != \"REF\" and passages[\"infons\"][\"section_type\"] != \"COMP_INT\" and passages[\"infons\"][\"section_type\"] != \"TABLE\" and passages[\"infons\"][\"section_type\"] != \"FIG\" :\n                        body_text.append({'text': passages[\"text\"], 'cite_spans': [], 'ref_spans': [], 'section': passages[\"infons\"][\"section_type\"]})\n\n            if(df.at[(df[df['pmcid'] == paper_id].index)[0], 'title'] == \"\") :\n                df.at[(df[df['pmcid'] == paper_id].index)[0], 'title'] = title\n            if(df.at[(df[df['pmcid'] == paper_id].index)[0], 'abstract_f'] == []) :\n                df.at[(df[df['pmcid'] == paper_id].index)[0], 'abstract_f'] = abstract\n            if(df.at[(df[df['pmcid'] == paper_id].index)[0], 'body_text'] == []) :\n                df.at[(df[df['pmcid'] == paper_id].index)[0], 'body_text'] = body_text\n        except JSONDecodeError as e :\n            pass\n            \nprint(\"Got\",cpt_record,\" matches\")\nprint(len(df[ ((df['body_text'].apply(lambda x: x == [])) & (df['pmcid'] != \"\")) ]), \"articles without fulltext but with a PMCID\")\nprint(len(df[ ((df['abstract_f'].apply(lambda x: x == [])) & (df['pmcid'] != \"\")) ]), \"articles without abstract but with a PMCID\")\nprint(len(df[ ((df['title'] == \"\") & (df['pmcid'] != \"\")) ]), \"articles without title but with a PMCID\")","60e545d9":"print(\"For the\",len(df), \"papers in the dataset (v6)\")\nprint(\"-\", len(df[df[\"abstract_f\"].apply(lambda x: x != [])]), \"papers with paragraph segmented abstract\")\nprint(\"-\", len(df[df[\"body_text\"].apply(lambda x: x != [])]), \"papers with fulltext\")","fe638ad9":"del dict_bionlp\ndel pmc_empty_papers\ndel dict_europepmc\ngc.collect()","5cc7a29b":"df['abstract_f'] = df['abstract_f'].apply(lambda x: [{\"text\" : re.sub('(\\\\n)+', ' ', a[\"text\"])} for a in x])\ndf['abstract_f'] = df['abstract_f'].apply(lambda x: [{\"text\" : re.sub('[a|A]bstract( [0-9]+)*', ' ', a[\"text\"])} for a in x])\ndf['abstract_f'] = df['abstract_f'].apply(lambda x: [{\"text\" : re.sub('[b|B]ackground(: )*', ' ', a[\"text\"])} for a in x])\ndf['abstract_f'] = df['abstract_f'].apply(lambda x: [{\"text\" : re.sub('^[\\s]*$', ' ', a[\"text\"])} for a in x])\ndf['abstract_f'] = df['abstract_f'].apply(lambda x: [{\"text\" : re.sub('\"', '', a[\"text\"])} for a in x])\ndf['abstract_fulltext'] = df['abstract_f'].apply(lambda x: \" \".join([str(a[\"text\"]) for a in x if \"text\" in a]))","ced2699a":"df['abstract_fulltext'].replace(\"\", np.nan,inplace=True)\ndf.dropna(subset=['abstract_fulltext'], inplace=True)\nprint(\"There are\",len(df),\"articles after removing missing values.\")","49e4e570":"df.drop_duplicates(subset=['abstract_fulltext'], inplace=True)\nprint(\"There are\",len(df),\"articles after removing duplicate abstracts.\")","8411c81f":"df.drop(df.index[(df.abstract_fulltext.str.len() < 500)],inplace=True)\nprint(\"There are\",len(df),\"articles after removing abstracts with few characters.\")","a94425fe":"!pip install langdetect","89ef08e4":"from langdetect import detect\ntqdm.pandas()\ndf[\"lang\"]=df.abstract_fulltext.progress_apply(lambda x: detect(x))","9365302f":"df['lang'].value_counts().iplot(kind='bar')","6f2f1a24":"df.drop(df.index[(df.lang != \"en\")],inplace=True)\ndf.reset_index(drop=True, inplace=True)\nprint(\"There are\",len(df),\"articles after removing non english abstract.\")","f5e70091":"df.drop(columns=['abstract_fulltext'], inplace=True)","aba38098":"!pip install --user kleis-keyphrase-extraction","f56b23c3":"# load library\nimport kleis.resources.dataset as kl\nfrom kleis.config.config import SEMEVAL2017\n\n# load semeval 2017 dataset\ndataset = kl.load_corpus(name=SEMEVAL2017)\n\n# recomended options\ndataset.training(features_method=\"simple-posseq\", filter_min_count=3, tagging_notation=\"BILOU\")","22b4d4a7":"for paragraph in df.at[random.randrange(len(df)), 'abstract_f'] :\n    keyphrases = dataset.label_text(paragraph[\"text\"])\n    displacy.render({\n            \"text\": paragraph[\"text\"],\n            # keyphrases are in brat-like format (we use only the span)\n            \"ents\": [{\"start\": start, \"end\": end, \"label\": \"KP\"}  for _, (_, (start, end)), _ in keyphrases],\n            \"title\": None\n    }, style=\"ent\", options=options, manual=True)","0d8f37e7":"tqdm.pandas()\ndf[\"abstract_kps\"] = df[\"abstract_f\"].progress_apply(lambda x : [[{\"span\": {\"start\": start,\"end\": end}, \"text\": kptext} for _, (_, (start, end)), kptext in dataset.label_text(paragraph[\"text\"])] for paragraph in x])","2e2781d8":"display(df.at[random.randrange(len(df)), 'abstract_kps'])","f2bbb9d9":"# return text from brat-like format\ndef brat_keyphrase_text(keyphrases):\n    return [k[\"text\"] for kps in keyphrases for k in kps]\n\n# return spans from brat-like format\ndef brat_keyphrase_span(keyphrases):\n    spans = []\n    for i, kps in enumerate(keyphrases):\n        spans.append([])\n        for k in kps:\n            spans[i].append(k[\"span\"])\n    return spans\n\n\nkeyphrases = Counter()\nkeyphrases_per_doc = defaultdict(Counter)\nkeyphrases_per_doc_spans = {}\nkeyphrase_count_docs = Counter()\nkeyphrase_paper_ids = defaultdict(set)\nfor index, row in tqdm(df.iterrows(), total = len(df.index)) :\n    # text\n    keyphrases_list = brat_keyphrase_text(row[\"abstract_kps\"]) \n    # spans\n    keyphrases_spans = brat_keyphrase_span(row[\"abstract_kps\"])\n    # count per doc\n    keyphrases_per_doc[row[\"cord_uid\"]].update(keyphrases_list)\n    keyphrases_per_doc_spans[row[\"cord_uid\"]] = keyphrases_spans\n    # total count\n    keyphrases.update(keyphrases_per_doc[row[\"cord_uid\"]])\n    # keyphrase in n docs |{d: k \\in d}|\n    keyphrase_count_docs.update(keyphrases_per_doc[row[\"cord_uid\"]].keys())\n    # documents containing keyphrase\n    for cur_key in keyphrases_per_doc[row[\"cord_uid\"]].keys():\n        keyphrase_paper_ids[cur_key].add(row[\"cord_uid\"])","a4f4d87a":"print(\"Overall keyphrases :\",len(keyphrases))\ndisplay(keyphrases.most_common(10))","8af41f80":"special_chars = re.compile(r\"\\W\")\nunderscores = re.compile(r\"[_]\\+\") \n\ndef normalize_term(t):\n    t_normalized, _ = re.subn(special_chars, \"_\", t.lower())\n    return t_normalized\n\nkeyphrase_variations = defaultdict(list)\nfor k in keyphrases.keys():\n    k_normalized = normalize_term(k)\n    keyphrase_variations[k_normalized].append(k)\n\nprint(\"Example of normalized keyphrases (%d):\\n\\n\" % (len(keyphrase_variations)), \n          json.dumps(list(keyphrase_variations.items())[:3], indent=2))","3d59f725":"keyphrases_normalized = Counter()\nfor k, c in keyphrases.items():\n    keyphrases_normalized[normalize_term(k)] += c\n\nkeyphrases_per_doc_normalized = defaultdict(Counter)\nfor paper_id, kps in keyphrases_per_doc.items():\n    for k, c in kps.items():\n        keyphrases_per_doc_normalized[paper_id][normalize_term(k)] += c\n        \nkeyphrase_count_docs_normalized = Counter()\nfor k, c_docs in keyphrase_count_docs.items():\n    keyphrase_count_docs_normalized[normalize_term(k)] += c_docs\n    \nkeyphrase_paper_ids_normalized = defaultdict(set)\nfor k, papers_ids in keyphrase_paper_ids.items():\n    keyphrase_paper_ids_normalized[normalize_term(k)] |= papers_ids","ff7db88b":"N = len(df)\nkeyphrases_normalized_idf = {k:np.log(N\/count) for k, count in keyphrase_count_docs_normalized.items()}\n\nprint(\"Keyphrases ranked by tf-idf:\")\nsorted(keyphrases_normalized_idf.items(), key=lambda x: x[1], reverse=True)[:10]","1722d2af":"query = [\"SARS CoV-2\", \"Covid 19\"]\nnormalized_query = [normalize_term(q) for q in query]\nnormalized_query","9d0d4953":"query_documents = defaultdict(dict)\nfor q in normalized_query:\n    query_documents[q][\"variations\"] = keyphrase_variations[q]\n    query_documents[q][\"docs\"] = keyphrase_paper_ids_normalized[q]\n\n# documents per term\nprint(json.dumps( {q: {\"variations\": query_documents[q][\"variations\"], \"docs\": len(query_documents[q][\"docs\"])} for q in normalized_query}, indent=2 ))","2f9bb663":"MAX_KEYPHRASES = 10 # max keyphrases \n\n# Merge docs\npapers_ids_result = set()\nfor q, r in query_documents.items():\n    papers_ids_result |= set(r[\"docs\"])\n\n# save ranked keyphrases\nranked_keyphrases_per_doc_result = defaultdict(dict)\n# for each paper_id\nfor paper_id in papers_ids_result:\n    if paper_id not in ranked_keyphrases_per_doc_result:\n        # total number of keyphrases in doc\n        cur_freq = sum(keyphrases_per_doc_normalized[paper_id].values())\n        # TF-IDF\n        cur_keyphrases = sorted(iter((k, keyphrases_normalized_idf[k]*(c\/cur_freq) ) for k, c in keyphrases_per_doc_normalized[paper_id].items()), key=lambda x: x[1], reverse=True)\n        # retrieve MAX_KEYPHRASES keyphrases\n        ranked_keyphrases_per_doc_result[paper_id][\"ranked\"] = (cur_keyphrases[:MAX_KEYPHRASES])\n        # rank query using document frequencies\n        ranked_keyphrases_per_doc_result[paper_id][\"ranked_query\"] = [ (q, keyphrases_normalized_idf[q]*(keyphrases_per_doc_normalized[paper_id][q]\/cur_freq) )\n                for q in query_documents.keys() if q in keyphrases_per_doc_normalized[paper_id]]\n\nprint(\"Example of keyphrases ranked using tf-idf: \\n\")\nfor q in query_documents:\n    print(\"Query:\", q)\n    ndocs = len(query_documents[q][\"docs\"])\n    paper_id = list(query_documents[q][\"docs\"])[random.randrange(0, ndocs)]\n    print(paper_id, json.dumps(ranked_keyphrases_per_doc_result[paper_id], indent=2))","959f8fd5":"nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\"])\nnlp.pipe_names","e21c7f8b":"MAX_DOCS = 10\nMAX_SENTS = 3\n\ndom_sum_scores = 0.0\ndocuments_sents = defaultdict(dict)\n# sentences = []\nfor paper_id in tqdm(query_documents[q][\"docs\"]) :\n    abstract_pars = nlp.pipe(map(lambda par: par[\"text\"], df.at[(df[df['cord_uid'] == paper_id].index)[0], 'abstract_f']))\n    _doc_score = 0.0\n    for i_par, doc in enumerate(abstract_pars):\n\n        ranked_kps = ranked_keyphrases_per_doc_result[paper_id][\"ranked\"] + ranked_keyphrases_per_doc_result[paper_id][\"ranked_query\"]\n        sum_ranks = sum(map(lambda x: x[1], ranked_kps)) # L1 norm\n\n        if \"text\" not in documents_sents[paper_id]:\n            documents_sents[paper_id][\"text\"] = []\n            documents_sents[paper_id][\"sents\"] = []\n\n        documents_sents[paper_id][\"text\"].append(doc.text)\n\n        cur_keyphrases_spans = []\n        if i_par < len(keyphrases_per_doc_spans[paper_id]):\n            cur_keyphrases_spans = keyphrases_per_doc_spans[paper_id][i_par]\n\n        for s in doc.sents:\n            sent_start, sent_end = s.start_char, s.end_char\n            documents_sents[paper_id][\"sents\"].append((i_par, {\"start\": sent_start, \n                                                            \"end\": sent_end, \n                                                            \"score\": 0,\n                                                            \"keyphrases_spans\": []}\n                                                          ))\n\n            _, last_sentence = documents_sents[paper_id][\"sents\"][-1]\n\n\n            for p, kscore in ranked_kps:\n                for span in cur_keyphrases_spans:\n                    kstart = span[\"start\"]\n                    kend = span[\"end\"]\n                    if kstart >= sent_start and kstart <= sent_end:\n                        last_sentence[\"score\"] += (kscore\/sum_ranks)**2 # sum(X^2)\n                        last_sentence[\"keyphrases_spans\"].append((kstart, kend))\n\n            if last_sentence[\"score\"] > 0.0:\n                last_sentence[\"score\"] = np.sqrt(last_sentence[\"score\"]) # sqrt(sum(X^2))\n                _doc_score += last_sentence[\"score\"]\n            else:\n                documents_sents[paper_id][\"sents\"].pop()\n\n    len_paragraphs = len(documents_sents[paper_id][\"text\"])\n    _doc_score = _doc_score\/ len_paragraphs if len_paragraphs > 0 else 0.0\n\n    if _doc_score > 0:\n        dom_sum_scores += _doc_score\n        documents_sents[paper_id][\"score\"] = _doc_score\n        documents_sents[paper_id][\"sents\"] = sorted(documents_sents[paper_id][\"sents\"], key=lambda y: y[1][\"score\"], reverse=True)[:MAX_SENTS]\n        # sentences += documents_sents[paper_id][\"sents\"]\n    else:\n        del documents_sents[paper_id]\n\nfor paper_id in documents_sents:\n    documents_sents[paper_id]['score'] \/= dom_sum_scores\n\ntotal_docs = len(documents_sents)\ndocuments_sents_sorted = sorted(documents_sents.items(), key=lambda d: d[1]['score'], reverse=True)[:MAX_DOCS]\n# sentence \nprint(total_docs, len(documents_sents_sorted))","9ab53b5a":"colors = {\"KP\": \"#2fbbab\"}\noptions = {\"ents\": [\"KP\"], \"colors\": colors}\n\n# for paper_id, doc in documents_sents_sorted:\nfor paper_id, doc in documents_sents_sorted:\n    paragraphs = doc['text']\n    print(\"\\nDocument id %s..., Document score = %.4f\" % (paper_id[:7], doc['score']))\n    for i_par, s in doc['sents']:\n        sent_start = s['start']\n        sent_end = s['end']\n        sent_score = s['score']\n        print(\"\\nSentence score: %f\\n\" % sent_score)\n        displacy.render({\n                \"text\": paragraphs[i_par][sent_start:sent_end],\n                \"ents\": [{\n                            \"start\": start - sent_start,\n                            \"end\": end - sent_start, \n                            \"label\": \"KP\"\n                            }  for start, end in set(s['keyphrases_spans'])],\n                \"title\": None\n        }, style=\"ent\", options=options, manual=True)\n    print(\"\\n\\n+\\n\\n\")","961adf68":"- The example below shows that we got back the paragraph segmentation dropped in the kaggle given v6 metadata","a251eb3d":"First, it is needed a set of documents to measure the term\/keyphrase frequency (TF).\n\nWe start by normalizing a query or keyphrases from a document.","52286282":"### We retrieve some of the missing data, obviously not them all\n- We would need access to some api with more content (like elsevier or springer) to get more data\n\n### Our main goal was to get paragraph segmented abstrat, and we got 30 000\n- Let's clean the limited RAM","5d5c35fc":"### But the v6 can still be improved\n- Indeed, there is no abstract in the XML file, maybe a mistake according to the given JSON schema, but probably because it's already merge in the metadata","3c86bd06":"#### Instaling kleis\n*kleis* is available in pypi over the name `kleis-keyphrase-extraction`","da3fcef7":"### Moreover, 19 062 fulltext are based only on PDF parse with no confidence on the quality of the data :","b67ff122":"## NCBI API (https:\/\/www.ncbi.nlm.nih.gov\/books\/NBK25500\/)\n- NCBI on PMCID","5a732c9e":"### And there is also a lot of missing data in the dataset :","0eadf047":"### From an other point of view, what are we still missing :","6d9e2d67":"## And there is a lot of this king of PDF parsing error in the data.\n### Thanks to the v6 update of the data, some are already easily fixable\n- We can now use the XML json to get more confident data","2a715b74":"### So Let's see our data now :","2484eece":"- Let's see how many articles for each language","3fe0b5da":"#### How to rank a sentence using the weights from keyphrases?\n\nThe simplest way is to represent each sentence an unit vector of keyphrases' weights (e.g., TF-IDF). From the resulting vector, the norm L2 can be used to obtain and scalar weight of the sentence.\n\nTo rank documents we use the average of all the sentences' weights (to improve).\n\n**Note:** Use the variables to control the max number of retrieved documents and sentences.\n\n```python\n# default\nMAX_DOCS = 10\nMAX_SENTS = 3\n```","5f928160":"- We don't need fulltext anymore, so let's clean some RAM","ccdc530d":"- But, according to the PDF json file, it's title is :","ea34c578":"#### Kleis: Extracting keyphrases from a random file\nOpenning random file to exctract keyphraes and testing keyphrase extraction over from an abstract.","d36aaae4":"### Ranking sentences and documents\n\nLoading spacy model to manipulate text easily.","2cc0107c":"### But there is a lot of crossref references in the metadata","c5f75b3f":"# The use of some API could fix the PDF parsing error and fill the missing data:\n### EUROPEPMC API (https:\/\/europepmc.org\/RestfulWebService)\n- We looked for the title, abstract and body_text ;\n- It took some time to request that much paper ;\n- We saved each content in a dictionnary with the paper ID as a key for later use ;","c765dc98":"# Part 2 - Cleaning the text from the abstract (paragraph segmented)\n- Even if we tried to get a maximum of missing data, we still missed some, so we deleted those and the duplicate in the cleaning step.","06676e8d":"### We got aproximately 400 new fulltext, and 2 500 new abstract (with paragraph segmentation).","7f947a65":"### Normalizing keyphrases\n\nThere are different representations of the same entity. It is better to normalize the keyphrases.","476415c0":"### Now we proceed to extract all the keyphrases\n- We applied the code from the exemple above to each abstract","14afeddf":"### Let's see if we can't get more data thanks to the precious data of the metadata file.","3c22cfc9":"- we made a dictionnary of the path of each file\n- \"sha\" as key for the papers from PDF parsing\n- \"pmcid\" as key for the papers from XML parsing","9951a498":"### Obtaining IDFs for each keyphrase\n\nIDF already provides a better ranking than using term\/keyphrase counts. ","533a777a":"# Completing datas before cleaning \n### By PHO, MBF, SIH and MAU (english is not our primary language, so our best apologies for any eyes bleeding due to reading this notebook)\n\nAs it has already been signified in [covid-19-literature-clustering](https:\/\/www.kaggle.com\/maksimeren\/covid-19-literature-clustering) for example, the available dataset contains duplicates and missing data. Numerous Kernels proposed to clean the available metadata, notably by deleting those duplicates or\/and incomplete data [covid-19-literature-clustering](https:\/\/www.kaggle.com\/maksimeren\/covid-19-literature-clustering), [eda-preprocessing-cleaning-cord-19-metadata](https:\/\/www.kaggle.com\/kdu4108\/eda-preprocessing-cleaning-cord-19-metadata) and [clean-metadata-file](https:\/\/www.kaggle.com\/mikehoney\/clean-metadata-file).\n\nIf some proposes way to enrich the available data (see Zac Danelly\u2019s works for example [Discussion\/14068](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/discussion\/140678) or this one from Andy White [discussion\/140597](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/discussion\/140597)), few, to our knowledge, seek to complete the missing texts and abstracts like Savanna Reid suggested it in [discussion\/138421](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/discussion\/138421). However, such an approach would be undeniably positive concerning the amount of data usable by our algorithms. Moreover, for our main work, the paragraph segmentation of the abstracts and the text is really important. Unfortunately, this segmentation is not present in the metadata CSV given in this challenge. Therefore, we decided that the first step of our approach would be to use the data provided by various APIs (we used [https:\/\/europepmc.org\/RestfulWebService](https:\/\/europepmc.org\/RestfulWebService) and [https:\/\/www.ncbi.nlm.nih.gov\/books\/NBK25500\/](https:\/\/www.ncbi.nlm.nih.gov\/books\/NBK25500\/) in this notebook) in order to complete the available dataset. We would have obviously better results with better API (like springer or eslevier). \n\nThe results of [donkeys\/languages](https:\/\/www.kaggle.com\/donkeys\/languages) shows that there are various languages available (mostly English). [discussion\/139231](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/discussion\/139231) proposed to use various tools to translate automatically languages to English, but we decided against it, having no guarantee of the results. Once the data has been completed with the APIs, we clean the remaining duplicates or missing data and drop also the non-English papers. \n\n- The first part of this notebook is about exploring, augmenting (by requesting API) the given COVID-19 dataset.\n- The second part is about cleaning the text\n\n# Ranking papers based on their Keyphrases \n\nPeople have tried to fulfill the various tasks (or help other fulfill them) in this Kaggle in quite a lot of ways. Some created search engines ([jdparsons\/biobert-corex-topic-search](https:\/\/www.kaggle.com\/jdparsons\/biobert-corex-topic-search\/notebook), [discussion\/138026](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/discussion\/138026)), some proposed tools ([ajrwhite\/covid19-tools](https:\/\/www.kaggle.com\/ajrwhite\/covid19-tools\/notebook), [discussion\/138250](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/discussion\/138250), [discussion\/139106](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/discussion\/139106)), some used knowledge graphs ([group16\/covid-19-knowledge-graph-embeddings](https:\/\/www.kaggle.com\/group16\/covid-19-knowledge-graph-embeddings), [sandyvarma\/covid-19-bert-mesh-enabled-knowledge-graph](https:\/\/www.kaggle.com\/sandyvarma\/covid-19-bert-mesh-enabled-knowledge-graph)), some used KeyPhrases [hamid3731\/keyphrase-extraction-and-graph-analysis](https:\/\/www.kaggle.com\/hamid3731\/keyphrase-extraction-and-graph-analysis), etc.  \n\nOn our end, we decided that the way we would try to fulfill the various tasks would be to propose a way to rank the various papers between themselves. Some already have proposed scoring functions like [discussion\/140726](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/discussion\/140726). Some like [discussion\/137558](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/discussion\/137558) proposed to base our ranking on the study design (with this discussion [discussion\/139355](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/discussion\/139355) directly linked to it). \n\nAt the moment, to rank our papers, we decided to use a technique similar to TextRank (used in [shahules\/cord-tools-and-knowledge-graphs](https:\/\/www.kaggle.com\/shahules\/cord-tools-and-knowledge-graphs) or [mobassir\/mining-covid-19-scientific-papers](https:\/\/www.kaggle.com\/mobassir\/mining-covid-19-scientific-papers\/notebook) for example). But contrary to the work of [21] who uses TextRank on sentences for automatic summarization, we use our ranking method to extract the most relevant KeyPhrases from a paper and then rank the papers between each other.  \n\n- The third part of this notebook is about Ranking important sentences and documents using extracted keyphrases (keywords).","d4baf34c":"## How to rank sentences and documents using keyphrases ?\n\nTo rank documents and sentences, first, we need to rank keyphrases.\n\n### The simplest way to rank keyphrases => TF-IDF\n\nUsing TF-IDF (term\/keyphrase frequency - inverse document frequency) is an inmediate alternative to rank keyphrases.\n\nTo start, we need to retrieve and load some information.","1dc3a21c":"- As we can see, there are a lot of English articles compared to those in French or other languages, we thought at first to make the translation of these articles in English so as not to lose information, but we saw that it is not practical and we haven't had good results since there are chemical formulas. \n\n- We decided to keep only the articles in English","50c81900":"# Part 1 - Exploring and Augmenting","b1d01ce6":"#### Example of file content\n\nEach keyphrase file saves spans and texts from each document.","5e55d493":"- According to its DOI, The real name of the paper is supposed to be :\n\n![naming_error_correction.PNG](attachment:naming_error_correction.PNG)","71c3a252":"## Why to use keyphrases?\n\n**Keyphrases** (also keywords or keyterms) are the most representative segments of text within the body of a document. **Extracted keyphrases** can be used in extactive summarization, using keyphrases to rank chunks of text. This approach can be used to rank  sentences and documents helping to recover important information.\n\n### Motivation: \n\n*TextRank* is a graph-based ranking model for unsupervised automatic keyphrase extraction. In the paper [TextRank: Bringing Order into Text](https:\/\/www.aclweb.org\/anthology\/W04-3252\/), it is proposed as an useful method for extractive summarization, since it produces ranked keyphrases. An implementation of this method is available for [Spacy](https:\/\/spacy.io\/) in the universe modules [pyTextRank](https:\/\/spacy.io\/universe\/project\/spacy-pytextrank). However, as mentioned in the challenge kernel [KeyPhrase Extraction and Graph analysis](https:\/\/www.kaggle.com\/hamid3731\/keyphrase-extraction-and-graph-analysis), TextRank is not the best available method. \n\nInstead of using TexkRank to extract keyphrases, we propose to use a supervised CRF-based method that *filters candidates of keyphrases* using their *PoS tag sequences* [**LIPN** at SemEval 2017 Task 10](https:\/\/www.aclweb.org\/anthology\/S17-2174\/). Several members of our team participating in the current Kaggle challenge are former integrants of the LIPN team who presented that work. \n\nWe used an improved version of the method available in the python package [kleis](https:\/\/pypi.org\/project\/kleis-keyphrase-extraction\/). The latest version of this package achieves a **F1 score of 0.405 (40.5%)** on the dataset *SemEval 2017 Task 10* for the subtask of keyphrase identification. \n\n### To consider about *kleis*:\n\n    - It is a non-rank based method to extract keyphrases\n    - It is a supervised method\n    - It is not trained over medical data\n\n### Our approach\n\nWe ranked documents and sentences using keyphrases from a supervised non-rank based method to extract keyphrases. \n\nCurrent work:\n\n    - Keyphrases are weighted using tf-idf\n    - Sentences are ranked using the weighted keyphrases\n    - Documents are ranked using the ranks of the sentences\n\nToDo:\n\n    - Improve documents' rank using similarity between keyphrases\n    - Find synonyms to improve document retrieval\n    - Train kleis over medical data\n\n### Example \n\nThe example shows a ranked document and its ranked sentences. In green are shown the extracted keyphrases using *kleis*.\n\n![example_ake.png](attachment:example_ake.png)","289c431e":"- Let's see what's the most common keyphrases we got :","99b8ae6a":"### Example of ranked documents and sentences\n\nThe example below includes the keyphrases in each ranked sentece.","c8371275":"### We got approximately 650 new fulltext and 2000 new abstracts with paragraph segmentation\n### So with all those API calls, how are our data looking now :","ad0b1579":"### So we lost the valuable paragraph segmentation from the sources \n- We need this segmentation, like in the example below, to be able to rank the best paragraph :","883788fe":"Now, we retrieve a subset of documents containing our query or keyphrases.","04c77172":"#### Configuring and importing kleis\n\nLoading SemEval2017 model.\n\n*Note:* Ignore the warning related to the corpus. The package is going to use the pre-trained models.","03b9be9f":"- We also noticed that there are articles with different languages.","d4c15b8d":"# Part 3 - Ranking important sentences and documents using extracted keyphrases (keywords).","ef0a9e5d":"- Back to the example, according to the metadata, the title of the paper with PMCID PMC261871 is :","af480b46":"### Ranking keyphrases by TF-IDF\n\nWe use the retrieved documents to get the TF-IDF of each keyphrase in them.\n\n**Note:** Use the variable to control the max number of retrieved keyphrases.\n\n```python\n# default\nMAX_KEYPHRASES = 10\n```","df9a866a":"### Next, ranking keyphrases by TF-IDF","87c520c2":"### We could use some Open Data Rest API to get some of the missing data and correct some PDF parsing errors\n### First, let's merge the data from the metadata and from the json files\n- So we used the metadata file as the source and we augmented it with all the json data ;\n- For the body text, due to the PDF parsing errors, we prioritized the xml json over the pdf json;\n- For the abstract, since the is no abstract in the xml json, we used the pdf json;","12c1ce2e":"- We don't need the json dict anymore, so let's empty the limited RAM","7e3a7ebc":"Preparing variables with the normalized keyphrases.","4060c504":"- After looking into thoses issues, we found that it's a parsing error from the pdf of the paper :\n\n![naming_error.PNG](attachment:naming_error.PNG)","01f8bce0":"## Let's look at the data (version 6)\n- there is a lot of missing data in each column, we forced the type of some column to avoir parsing errors later\n- new in v6 there is a json file from XML parse and PDF parse.\n- that's actually a really good thing, because we found a lot of PDF parsing errors in the dataset :\n### Let's show an example :\n- we have to load the metadata and the json first"}}