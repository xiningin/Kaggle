{"cell_type":{"a9f5deb9":"code","e5b3faf4":"code","8e97b886":"code","b28e6289":"code","a3e260da":"code","5aa7f743":"code","6b854f27":"code","e3d42189":"code","16e861c1":"code","9b5dc292":"markdown","637d50d5":"markdown","11875a34":"markdown","70f9eb69":"markdown","b7b2349d":"markdown","c4fa73ce":"markdown"},"source":{"a9f5deb9":"import sys\nimport keras\nimport keras as ks\nimport matplotlib\nfrom matplotlib import pyplot\nfrom keras.datasets import cifar10\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Dropout\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.optimizers import Adam","e5b3faf4":"(X_train,Y_train),(X_test,Y_test) = cifar10.load_data()\n\n# Ananlyzing the shape of the data(Should be the first step of training)\nprint(\"shape of X_train and Y_train \"+ str(X_train.shape)+\" \"+str(Y_train.shape))\nprint(\"shape of X_test and Y_test \"+ str(X_test.shape)+\" \"+str(Y_test.shape))","8e97b886":"for i in range(5):\n    pyplot.subplot(330 + 1 + i)\n    pyplot.imshow(X_train[i])\n    print(Y_train[i])\npyplot.show()","b28e6289":"def load():\n    (X_train,Y_train),(X_test,Y_test) = cifar10.load_data()\n    Y_train=ks.utils.to_categorical(Y_train)\n    Y_test=ks.utils.to_categorical(Y_test)\n    return X_train,Y_train,X_test,Y_test","a3e260da":"def normalize(train,test):\n    trainNorm=train\/255.0\n    testNorm=test\/255.0\n    return trainNorm,testNorm","5aa7f743":"def getmodel():\n    # Adding sequential to add layers sequentially as per our needs.\n    model = ks.Sequential()\n    model.add(Conv2D(32,(3,3),activation='relu',kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n    model.add(Conv2D(32,(3,3),activation='relu',kernel_initializer='he_uniform', padding='same'))\n    model.add(MaxPooling2D((2,2)))\n    model.add(Conv2D(64,(3,3),activation='relu',kernel_initializer='he_uniform', padding='same'))\n    model.add(Conv2D(64,(3,3),activation='relu',kernel_initializer='he_uniform', padding='same'))\n    model.add(MaxPooling2D((2,2)))\n    model.add(Conv2D(128,(3,3),activation='relu',kernel_initializer='he_uniform', padding='same'))\n    model.add(Conv2D(128,(3,3),activation='relu',kernel_initializer='he_uniform', padding='same'))\n    model.add(MaxPooling2D((2,2)))\n    \n    model.add(Flatten())\n    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n    model.add(Dense(10, activation='softmax'))\n    \n    opt = Adam(learning_rate=0.01,name=\"Adam\")\n    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n    return model","6b854f27":"def summarize_diagnostics(history):\n    # plot loss\n    pyplot.subplot(211)\n    pyplot.title('Cross Entropy Loss')\n    pyplot.plot(history.history['loss'], color='blue', label='train')\n    pyplot.plot(history.history['val_loss'], color='orange', label='test')\n    # plot accuracy\n    pyplot.subplot(212)\n    pyplot.title('Classification Accuracy')\n    pyplot.plot(history.history['accuracy'], color='blue', label='train')\n    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n    # save plot to file\n    filename = sys.argv[0].split('\/')[-1]\n    pyplot.savefig(filename + '_plot.png')\n    pyplot.close()","e3d42189":"def trainModel():\n    X_train,Y_train,X_test,Y_test=load()\n    X_train,X_test=normalize(X_train,X_test)\n    model=getmodel()\n    history=model.fit(X_train,Y_train,epochs=100,batch_size=64, validation_data=(X_test, Y_test), verbose=0)\n    summarize_diagnostics(history)\n    _, acc = model.evaluate(X_test, Y_test, verbose=0)\n    print('> %.3f' % (acc * 100.0))","16e861c1":"trainModel()","9b5dc292":"## Function 1 : load() --> X_train,Y_train,X_test,Y_test\n### As this is Categorical Data, we use one_hot_encoding on it\nThis creates multiple columns of different categories (As this data is in 0-9 form). So, this is performed on Y_test and Y_train.<br> This is done using to_categorical of keras.utils<br> So, finally load function to load and encode data","637d50d5":"## Function 2: normalize(train,test) --> trainNorm,testNorm\n### Normalizing Images\nWe need the data to be scaled properly (0-255). So, we normalize it from 0-1","11875a34":"### Now we fit our model and code for visualizing it","70f9eb69":"### Looking at the first 5 images and their classes","b7b2349d":"## Function 3: model()--> model\nWithout Dropout => 70%<br>\nNow we define model and change it as per our needs<br>\nAdded Dropout!<br>\nAdded Adam Optimizer","c4fa73ce":"### Loading the Dataset"}}