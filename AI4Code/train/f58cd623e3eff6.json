{"cell_type":{"5656a539":"code","087a39af":"code","da067c39":"code","479c3107":"code","b3e487bd":"code","09d1d119":"code","b2b44a9a":"code","167deac3":"code","9ee0e8e4":"code","736695d8":"code","08ed270e":"code","7b2b5b8a":"code","51693c20":"code","12c0b6b0":"code","7ef764ea":"code","492fa530":"code","c7fd5754":"code","7363d71c":"code","be20bf33":"code","431ecaaa":"code","1b57da45":"code","46698455":"code","f77190d7":"markdown","cf879483":"markdown","a9b1600f":"markdown","c29bb9d2":"markdown","4aa0799c":"markdown","29535db2":"markdown","bd87f7ee":"markdown","1c31e8ac":"markdown","ad7c0713":"markdown","e86de86e":"markdown","d0ad1fe5":"markdown","3d2b1004":"markdown"},"source":{"5656a539":"!pip install --upgrade git+https:\/\/github.com\/tusharsarkar3\/XBNet.git","087a39af":"import torch\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom XBNet.training_utils import training,predict\nfrom XBNet.models import XBNETClassifier\nfrom XBNet.run import run_XBNET ","da067c39":"df=pd.read_csv('\/kaggle\/input\/cusersmarildownloadsgermancsv\/german.csv',encoding ='ISO-8859-1',sep=\";\")\nprint(df.shape)\ndf.head()","479c3107":"y=df[['Creditability']].to_numpy()\ny[:5]","b3e487bd":"x=df.loc[:,'Duration_of_Credit_monthly':].to_numpy()\nx[:2]","09d1d119":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size= 0.20, random_state= True,stratify=y) \nx_train.shape,x_test.shape,y_train.shape,y_test.shape","b2b44a9a":"y_train=y_train.reshape((-1))\ny_train.shape","167deac3":"y_test=y_test.reshape((-1))\ny_test.shape","9ee0e8e4":"#In and out layer dimensions 100\/Set bias True\/Sigmoid \n\nmodel = XBNETClassifier(x_train,y_train,num_layers=2)","736695d8":"criterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)","08ed270e":"m,acc, lo, val_ac, val_lo = run_XBNET(x_train,x_test,y_train,y_test,model,criterion,\n                                      optimizer,epochs=25,batch_size=32)","7b2b5b8a":"import matplotlib.pyplot as plt\nplt.figure(figsize=(20,5))\nplt.subplot(1,2,1)\nplt.plot(acc,label='training accuracy')\nplt.plot(val_ac,label = 'validation accuracy')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.legend()\nplt.grid()\nplt.subplot(1,2,2)\nplt.plot(lo,label='training loss')\nplt.plot(val_lo,label = 'validation loss')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend() \nplt.grid()","51693c20":"!pip install deep_autoviml","12c0b6b0":"from deep_autoviml import deep_autoviml as deepauto","7ef764ea":"################################################################################\nkeras_model_type =  \"fast\" ## always try \"fast\", then \"fast1\", \"fast2\" and \"auto\"\n### always set early_stopping to True first and then change it to False\n#### You always need 15 max_trials to get something decent #####\nkeras_options = {\"early_stopping\": True, 'lr_scheduler': ''}  \n#### always set tuner to \"storm\" and then \"optuna\". \n# NLP char limit kicks off NLP processing. Feature Cross later.\nmodel_options = {'tuner':\"storm\", \"max_trials\": 5, 'nlp_char_limit':10,\n                 'cat_feat_cross_flag':False, }\nproject_name = 'German_Credit' ### this is the folder where the model will be saved\n################################################################################","492fa530":"preds = df.columns[2:].tolist()\ntargets = df.columns[:1].tolist()\ntarget = targets[0]\ntarget","c7fd5754":"train = pd.DataFrame(np.c_[x_train,y_train], index=range(len(x_train)), columns = preds+targets)\ntest = pd.DataFrame(np.c_[x_test,y_test], index=range(len(x_test)), columns = preds+targets)\nprint(train.shape, test.shape)\ntrain.head(2)","7363d71c":"model, cat_vocab_dict = deepauto.fit(train, target, keras_model_type=keras_model_type,\n\t\tproject_name=project_name, keras_options=keras_options,  \n\t\tmodel_options=model_options, save_model_flag=True, use_my_model='',\n\t\tmodel_use_case='', verbose=0)","be20bf33":"predictions = deepauto.predict(model, project_name, test_dataset=test,\n                                 keras_model_type=keras_model_type, \n                                 cat_vocab_dict=cat_vocab_dict)","431ecaaa":"y_test = test[target].values\ny_test[:4]","1b57da45":"y_preds = predictions[-1]\ny_preds[:4]","46698455":"from deep_autoviml import print_classification_model_stats\nprint_classification_model_stats(y_test, y_preds)","f77190d7":"The model results show that it predicts all values as ) class\n\n![image.png](attachment:0d37859f-0af5-45fe-ae3d-347c13700eae.png)","cf879483":"# In XBNet, You have to answer questions to set up the deep learning model. Some people may not understand how to answer these questions even if they know what deep learning is.","a9b1600f":"# Hope this notebook was helpful. If you liked it, pelase upvote Marilia Prata's Notebooks here:","c29bb9d2":"## We see that the model does not predict any values for the 1 class but predicts all the values as 0 class. Hence Training and Testing accuracy is only 30% - same as what the number of 1's in the validation dataset was.","4aa0799c":"## We are going to use the defaults provided by Marilia to XBNet and test it","29535db2":"# You can see that Precision is 76% while Recall is 97% on the Validation dataset and overall accuracy is 76%\n![image.png](attachment:999013f1-aa2c-47c8-96ee-2378e6db8670.png)","bd87f7ee":"# Here is the Deep Learning model that it built:\n![image.png](attachment:b5c3b47a-e900-4570-b419-05ec068b7755.png)","1c31e8ac":"XBNet Classifier on German Credit Data:\nhttps:\/\/www.kaggle.com\/mpwolke\/xbnet-creditability\n\nDeep_AutoViML on German Credit Data:\nhttps:\/\/www.kaggle.com\/mpwolke\/creditability-deep-autoviml\n\n","ad7c0713":"# Let us now compare the results to Deep_AutoViML which we will install now","e86de86e":"## The results on test set are similarly good\n![image.png](attachment:e5003e44-8550-4adc-a34f-965f44696029.png)","d0ad1fe5":"## We will now test the model on the heldout test dataset. \nWe can see that accuracy drops a bit since the dataset is too small and model probably overfit on such a small dataset. However, we can try other keras_model_type=\"fast1\", \"fast2\" etc and see whether we can get better results","3d2b1004":"# We are going to compare two Deep Learning AutoML libraries on a very difficult classification dataset: German Credit Data. \n## The first AutoML library we will try is XBNet. You can see their results here:\nhttps:\/\/www.kaggle.com\/mpwolke\/xbnet-creditability\n## XBNet developed by Tushar Sarkar https:\/\/github.com\/tusharsarkar3\/XBNet\n\n## The next AutoML library we will try is: Deep AutoViML.\n<img src=\"https:\/\/github.com\/AutoViML\/deep_autoviml\/raw\/master\/logo.jpg\" alt=\"banner\"\/>\n\n## We will use the same test-train split in both using the same random_states and everything. Only thing is we will test on the final heldout test.\n\n# If you want to see more on German Credit, you can see another great notebook by Marilia here:\n\nhttps:\/\/www.kaggle.com\/mpwolke\/creditability-deep-autoviml"}}