{"cell_type":{"506c3bc8":"code","c97dc738":"code","2f576e04":"code","13321586":"code","96759db8":"code","25c4b532":"code","5745f4fc":"code","d7f1cc0e":"code","03dd60f6":"code","f1e81ea3":"code","2d53f710":"code","898bc24c":"code","443041f7":"code","5659b43a":"code","1cf8d7c5":"code","4d62283d":"code","e25edb04":"code","acc1ecfb":"code","b29ad728":"code","2b81f789":"code","7eada736":"code","e5918e7b":"code","8331d24c":"code","dc533dc7":"code","16c31b1c":"code","6872869b":"code","3149eed9":"code","49544d67":"code","3c914c53":"code","456c6fdf":"code","16f3e882":"code","c0e94ac5":"code","2159e458":"code","de533ef5":"code","5880baf1":"code","63150615":"code","f7b2eddc":"code","908dc660":"code","803f9767":"code","966aaf79":"markdown","7638fac6":"markdown","79b95d38":"markdown","4223fa0c":"markdown","66416eae":"markdown","b9e054e4":"markdown","259bcaaa":"markdown","01d58bca":"markdown","629f49ec":"markdown","3343f8bd":"markdown","cc7b6a62":"markdown","58bfbda1":"markdown","ae4e1818":"markdown","19ed9b77":"markdown","e0aacafd":"markdown","000bbf07":"markdown","e9d9a2db":"markdown","332ecc29":"markdown","f0cc3d67":"markdown","c02cf713":"markdown","21ed89cb":"markdown","e2cc33ab":"markdown","86234b1c":"markdown","3836ddcb":"markdown","e0b24aa3":"markdown","058b8ca9":"markdown","66439ccb":"markdown","93152338":"markdown","a17753c7":"markdown","592e669c":"markdown","42b04a6e":"markdown","bf5f1b2f":"markdown","f89e4022":"markdown","efa9d9ad":"markdown","1af2ddad":"markdown","061227ec":"markdown","cf1fd62d":"markdown","158a90a6":"markdown","c5de01a8":"markdown"},"source":{"506c3bc8":"import numpy as np \nimport pandas as pd \nimport pickle\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom numba import njit\n\nimport collections\nimport math\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","c97dc738":"%%time\ntrain_pickle_file = '\/kaggle\/input\/pickling\/train.csv.pandas.pickle'\ntrain_data = pickle.load(open(train_pickle_file, 'rb'))\ntrain_data.info()","2f576e04":"# Don't launch that as it may consume a lot of memory)\n#rw = 10000\n#train_data_rolled = train_data.rolling(window=rw).mean()","13321586":"from collections import deque\n\nclass RunningMean:\n    def __init__(self, WIN_SIZE=20, n_size = 1):\n        self.n = 0\n        self.mean = np.zeros(n_size)\n        self.cum_sum = 0\n        self.past_value = 0\n        self.WIN_SIZE = WIN_SIZE\n        self.windows = collections.deque(maxlen=WIN_SIZE+1)\n        \n    def clear(self):\n        self.n = 0\n        self.windows.clear()\n\n    def push(self, x):\n        \n        x = fillna_npwhere_njit(x, self.past_value)\n        self.past_value = x\n        \n        self.windows.append(x)\n        self.cum_sum += x\n        \n        if self.n < self.WIN_SIZE:\n            self.n += 1\n            self.mean = self.cum_sum \/ float(self.n)\n            \n        else:\n            self.cum_sum -= self.windows.popleft()\n            self.mean = self.cum_sum \/ float(self.WIN_SIZE)\n\n    def get_mean(self):\n        return self.mean if self.n else np.zeros(n_size)\n\n    def __str__(self):\n        return \"Current window values: {}\".format(list(self.windows))\n\n# Temporary removing njit as it cause many bugs down the line\n# Problems mainly due to data types, I have to find where I need to constraint types so as not to make njit angry\n#@njit\ndef fillna_npwhere_njit(array, values):\n    if np.isnan(array.sum()):\n        array = np.where(np.isnan(array), values, array)\n    return array","96759db8":"a = RunningMean(WIN_SIZE=10000)\n\nfor index, row in tqdm(train_data[:100000].iterrows()): \n    a.push(np.array(row))\n    \na.get_mean()","25c4b532":"a = RunningMean(WIN_SIZE=10)\n\nfor index, row in pd.DataFrame({'col1':range(1,100)}).iterrows(): \n    a.push(np.array(row))\n    \nprint(a.get_mean())\nprint((90+91+92+93+94+95+96+97+98+99)\/10)","5745f4fc":"from __future__ import division\nimport collections\nimport math\n\n\nclass RunningStats:\n    def __init__(self, WIN_SIZE=20, n_size = 1):\n        self.n = 0\n        self.mean = 0\n        self.run_var = 0\n        self.WIN_SIZE = WIN_SIZE\n        self.past_value = 0\n        self.windows = collections.deque(maxlen=WIN_SIZE+1)\n\n    def clear(self):\n        self.n = 0\n        self.windows.clear()\n\n    def push(self, x):\n        \n        x = fillna_npwhere_njit(x, self.past_value)\n        self.past_value = x\n\n        self.windows.append(x)\n\n        if self.n < self.WIN_SIZE:\n            # Calculating first variance\n            self.n += 1\n            delta = x - self.mean\n            self.mean += delta \/ self.n\n            self.run_var += delta * (x - self.mean)\n        else:\n            # Adjusting variance\n            x_removed = self.windows.popleft()\n            old_m = self.mean\n            self.mean += (x - x_removed) \/ self.WIN_SIZE\n            self.run_var += (x + x_removed - old_m - self.mean) * (x - x_removed)\n\n    def get_mean(self):\n        return self.mean if self.n else np.zeros(n_size)\n\n    def get_var(self):\n        return self.run_var \/ (self.n) if self.n > 1 else np.zeros(n_size)\n\n    def get_std(self):\n        return math.sqrt(self.get_var())\n\n    def get_all(self):\n        return list(self.windows)\n\n    def __str__(self):\n        return \"Current window values: {}\".format(list(self.windows))\n","d7f1cc0e":"a = RunningStats(WIN_SIZE=10000)\n\nfor index, row in tqdm(train_data[:100000].iterrows()): \n    a.push(np.array(row))\n    \na.get_mean()","03dd60f6":"#rw = 10000\n#train_data_rolled_mean = train_data.rolling(window=rw).mean()\n#train_data_rolled_var = train_data.rolling(window=rw).var()\n#train_data_rolled_skew = train_data.rolling(window=rw).skew()\n#train_data_rolled_kurt = train_data.rolling(window=rw).kurt()","f1e81ea3":"#train_data_ewm = train_data.ewm(span=rw, adjust=True).mean()","2d53f710":"class RunningEWMean:\n    def __init__(self, WIN_SIZE=20, n_size = 1, lt_mean = None):\n        if lt_mean is not None:\n            self.s = lt_mean\n        else:\n            self.s = np.zeros(n_size)\n        self.past_value = np.zeros(n_size)\n        self.alpha = 2 \/(WIN_SIZE + 1)\n\n    def clear(self):\n        self.s = 0\n\n    def push(self, x):\n        \n        x = fillna_npwhere_njit(x, self.past_value)\n        self.past_value = x\n        self.s = self.alpha * x + (1 - self.alpha) * self.s\n        \n    def get_mean(self):\n        return self.s","898bc24c":"a = RunningEWMean(WIN_SIZE=10)\n\nfor index, row in pd.DataFrame({'col1':range(1,100)}).iterrows(): \n    a.push(np.array(row))\n    \nprint(a.get_mean())\nprint(pd.DataFrame({'col1':range(1,100)}).ewm(span=10, adjust=True).mean().iloc[98])","443041f7":"a = RunningEWMean(WIN_SIZE=10000)\n\nfor index, row in tqdm(train_data[:100000].iterrows()): \n    a.push(np.array(row))\n    \na.get_mean()","5659b43a":"class RunningPDA:\n    def __init__(self):\n        self.day = -1\n        self.past_mean = 0\n        self.cum_sum = 0\n        self.day_instances = 0\n        self.past_value = 0\n\n    def clear(self):\n        self.n = 0\n        self.windows.clear()\n\n    def push(self, x, date):\n        \n        x = fillna_npwhere_njit(x, self.past_value)\n        self.past_value = x\n        \n        # change of day\n        if date>self.day:\n            self.day = date\n            if self.day_instances > 0:\n                self.past_mean = self.cum_sum\/self.day_instances\n            else:\n                self.past_mean = 0\n            self.day_instances = 1\n            self.cum_sum = x\n            \n        else:\n            self.day_instances += 1\n            self.cum_sum += x\n\n    def get_mean(self):\n        return self.cum_sum\/self.day_instances\n\n    def get_past_mean(self):\n        return self.past_mean","1cf8d7c5":"a = RunningPDA()\n\nfor index, row in tqdm(train_data[:100000].iterrows()): \n    date=row['date']\n    a.push(np.array(row),date)","4d62283d":"a.get_past_mean()","e25edb04":"train_data[:200000].groupby('date').mean().iloc[30,]","acc1ecfb":"class RunningPTI:\n    def __init__(self,base_value=0):\n        self.dictionnary = {}\n        self.base_value = base_value\n        self.day = -1\n\n    def clear(self):\n        self.dictionnary = {}\n        self.base_value = 0\n\n    def push(self, x, value, date):\n        \n                # change of day\n        if date>self.day:\n            self.day = date\n            self.dictionnary = {}\n        \n        self.past_value = self.dictionnary.get(value)\n        self.dictionnary.update({value:x})\n        \n    def get_past_value(self):\n        if self.past_value is None:\n            self.past_value = self.base_value\n        return self.past_value\n    \n    def get_dict(self):\n        return self.dictionnary","b29ad728":"a = RunningPTI(base_value=np.nan * np.empty((1, 138)))\n\nfor index, row in tqdm(train_data[:10000].iterrows()): \n    f_41 = row['feature_41']\n    date = row['date']\n    a.push(np.array(row),f_41,date)\n    \na.get_past_value()","2b81f789":"#rw = 10000\n#train_data_diff = train_data.rolling(window=rw).mean().diff(rw)\n#train_data_diff_diff = train_data.rolling(window=rw).mean().diff(rw).diff(rw)","7eada736":"def get_weights(d, size):\n    w = [1.]\n    for k in range(1, size):\n        w_ = -w[-1] \/ k * (d - k + 1)\n        w.append(w_)\n    w = np.array(w[::-1]).reshape(-1, 1)\n    return w","e5918e7b":"plt.plot(get_weights(-0.75,1000000))\nplt.ylim(0,0.1)","8331d24c":"#!pip install pyinform\n#from pyinform import entropy_rate\n\n#entropy_r = lambda x: entropy_rate(x,k=2)\n\n#df[feature] = (df[feature] > df[feature].mean())\n#df[feature] = df[feature].rolling(window=rw[i]).apply(entropy_r)","dc533dc7":"del train_data","16c31b1c":"from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\n\nfrom tqdm import tqdm\nfrom random import choices\n\n\nimport kerastuner as kt\n\nphysical_devices = tf.config.list_physical_devices('GPU')\ntry:\n          tf.config.experimental.set_memory_growth(physical_devices[0], True)\nexcept:\n          # Invalid device or cannot modify virtual devices once initialized.\n    pass","6872869b":"import numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\nfrom sklearn.utils.validation import _deprecate_positional_args\n\n# modified code for group gaps; source\n# https:\/\/github.com\/getgaurav2\/scikit-learn\/blob\/d4a3af5cc9da3a76f0266932644b884c99724c57\/sklearn\/model_selection\/_split.py#L2243\nclass PurgedGroupTimeSeriesSplit(_BaseKFold):\n    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n    Allows for a gap in groups to avoid potentially leaking info from\n    train into test if the model has windowed or lag features.\n    Provides train\/test indices to split time series data samples\n    that are observed at fixed time intervals according to a\n    third-party provided group.\n    In each split, test indices must be higher than before, and thus shuffling\n    in cross validator is inappropriate.\n    This cross-validation object is a variation of :class:`KFold`.\n    In the kth split, it returns first k folds as train set and the\n    (k+1)th fold as test set.\n    The same group will not appear in two different folds (the number of\n    distinct groups has to be at least equal to the number of folds).\n    Note that unlike standard cross-validation methods, successive\n    training sets are supersets of those that come before them.\n    Read more in the :ref:`User Guide <cross_validation>`.\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of splits. Must be at least 2.\n    max_train_group_size : int, default=Inf\n        Maximum group size for a single training set.\n    group_gap : int, default=None\n        Gap between train and test\n    max_test_group_size : int, default=Inf\n        We discard this number of groups from the end of each train split\n    \"\"\"\n\n    @_deprecate_positional_args\n    def __init__(self,\n                 n_splits=5,\n                 *,\n                 max_train_group_size=np.inf,\n                 max_test_group_size=np.inf,\n                 group_gap=None,\n                 verbose=False\n                 ):\n        super().__init__(n_splits, shuffle=False, random_state=None)\n        self.max_train_group_size = max_train_group_size\n        self.group_gap = group_gap\n        self.max_test_group_size = max_test_group_size\n        self.verbose = verbose\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n        y : array-like of shape (n_samples,)\n            Always ignored, exists for compatibility.\n        groups : array-like of shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train\/test set.\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        if groups is None:\n            raise ValueError(\n                \"The 'groups' parameter should not be None\")\n        X, y, groups = indexable(X, y, groups)\n        n_samples = _num_samples(X)\n        n_splits = self.n_splits\n        group_gap = self.group_gap\n        max_test_group_size = self.max_test_group_size\n        max_train_group_size = self.max_train_group_size\n        n_folds = n_splits + 1\n        group_dict = {}\n        u, ind = np.unique(groups, return_index=True)\n        unique_groups = u[np.argsort(ind)]\n        n_samples = _num_samples(X)\n        n_groups = _num_samples(unique_groups)\n        for idx in np.arange(n_samples):\n            if (groups[idx] in group_dict):\n                group_dict[groups[idx]].append(idx)\n            else:\n                group_dict[groups[idx]] = [idx]\n        if n_folds > n_groups:\n            raise ValueError(\n                (\"Cannot have number of folds={0} greater than\"\n                 \" the number of groups={1}\").format(n_folds,\n                                                     n_groups))\n\n        group_test_size = min(n_groups \/\/ n_folds, max_test_group_size)\n        group_test_starts = range(n_groups - n_splits * group_test_size,\n                                  n_groups, group_test_size)\n        for group_test_start in group_test_starts:\n            train_array = []\n            test_array = []\n\n            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n                train_array_tmp = group_dict[train_group_idx]\n                \n                train_array = np.sort(np.unique(\n                                      np.concatenate((train_array,\n                                                      train_array_tmp)),\n                                      axis=None), axis=None)\n\n            train_end = train_array.size\n \n            for test_group_idx in unique_groups[group_test_start:\n                                                group_test_start +\n                                                group_test_size]:\n                test_array_tmp = group_dict[test_group_idx]\n                test_array = np.sort(np.unique(\n                                              np.concatenate((test_array,\n                                                              test_array_tmp)),\n                                     axis=None), axis=None)\n\n            test_array  = test_array[group_gap:]\n            \n            \n            if self.verbose > 0:\n                    pass\n                    \n            yield [int(i) for i in train_array], [int(i) for i in test_array]","3149eed9":"class CVTuner(kt.engine.tuner.Tuner):\n    def run_trial(self, trial, X, y, splits, batch_size=32, epochs=1,callbacks=None):\n        val_losses = []\n        for train_indices, test_indices in splits:\n            X_train, X_test = [x[train_indices] for x in X], [x[test_indices] for x in X]\n            y_train, y_test = [a[train_indices] for a in y], [a[test_indices] for a in y]\n            if len(X_train) < 2:\n                X_train = X_train[0]\n                X_test = X_test[0]\n            if len(y_train) < 2:\n                y_train = y_train[0]\n                y_test = y_test[0]\n            \n            model = self.hypermodel.build(trial.hyperparameters)\n            hist = model.fit(X_train,y_train,\n                      validation_data=(X_test,y_test),\n                      epochs=epochs,\n                        batch_size=batch_size,\n                      callbacks=callbacks)\n            \n            val_losses.append([hist.history[k][-1] for k in hist.history])\n        val_losses = np.asarray(val_losses)\n        self.oracle.update_trial(trial.trial_id, {k:np.mean(val_losses[:,i]) for i,k in enumerate(hist.history.keys())})\n        self.save_model(trial.trial_id, model)","49544d67":"# From https:\/\/medium.com\/@micwurm\/using-tensorflow-lite-to-speed-up-predictions-a3954886eb98\n\nclass LiteModel:\n    \n    @classmethod\n    def from_file(cls, model_path):\n        return LiteModel(tf.lite.Interpreter(model_path=model_path))\n    \n    @classmethod\n    def from_keras_model(cls, kmodel):\n        converter = tf.lite.TFLiteConverter.from_keras_model(kmodel)\n        tflite_model = converter.convert()\n        return LiteModel(tf.lite.Interpreter(model_content=tflite_model))\n    \n    def __init__(self, interpreter):\n        self.interpreter = interpreter\n        self.interpreter.allocate_tensors()\n        input_det = self.interpreter.get_input_details()[0]\n        output_det = self.interpreter.get_output_details()[0]\n        self.input_index = input_det[\"index\"]\n        self.output_index = output_det[\"index\"]\n        self.input_shape = input_det[\"shape\"]\n        self.output_shape = output_det[\"shape\"]\n        self.input_dtype = input_det[\"dtype\"]\n        self.output_dtype = output_det[\"dtype\"]\n        \n    def predict(self, inp):\n        inp = inp.astype(self.input_dtype)\n        count = inp.shape[0]\n        out = np.zeros((count, self.output_shape[1]), dtype=self.output_dtype)\n        for i in range(count):\n            self.interpreter.set_tensor(self.input_index, inp[i:i+1])\n            self.interpreter.invoke()\n            out[i] = self.interpreter.get_tensor(self.output_index)[0]\n        return out\n    \n    def predict_single(self, inp):\n        \"\"\" Like predict(), but only for a single record. The input data can be a Python list. \"\"\"\n        inp = np.array([inp], dtype=self.input_dtype)\n        self.interpreter.set_tensor(self.input_index, inp)\n        self.interpreter.invoke()\n        out = self.interpreter.get_tensor(self.output_index)\n        return out[0]","3c914c53":"TRAINING = False\nUSE_FINETUNE = True     \nFOLDS = 5\nSEED = 42\n\ntrain = pd.read_csv('..\/input\/jane-street-market-prediction\/train.csv')","456c6fdf":"nb_trade = train.groupby('date')['date'].count()\nhigh_volume_days = [i for i, x in enumerate(np.array(nb_trade > 7000)) if x]","16f3e882":"train = train.query('date > 85').reset_index(drop = True) \ntrain = train.query('date not in @high_volume_days').reset_index(drop = True) \ntrain = train.astype({c: np.float32 for c in train.select_dtypes(include='float64').columns}) #limit memory use\ntrain.fillna(train.mean(),inplace=True)\ntrain = train.query('weight > 0').reset_index(drop = True)\n#train['action'] = (train['resp'] > 0).astype('int')\ntrain['action'] =  (  (train['resp_1'] > 0.00001 ) & (train['resp_2'] > 0.00001 ) & (train['resp_3'] > 0.00001 ) & (train['resp_4'] > 0.00001 ) &  (train['resp'] > 0.00001 )   ).astype('int')","c0e94ac5":"EWM_5000 = RunningEWMean(WIN_SIZE = 5000)\nEWM_10000 = RunningEWMean(WIN_SIZE = 10000)\nEWM_20000 = RunningEWMean(WIN_SIZE = 20000)\n\ntrain_FE = []\n\nfor index, row in tqdm(train[['feature_0','feature_1']].iterrows()): \n    EWM_5000.push(np.float64(np.array(row)))\n    EWM_10000.push(np.float64(np.array(row)))\n    EWM_20000.push(np.float64(np.array(row)))\n\n    FE = {\n        'feature_0_EWM_5000' : EWM_5000.get_mean()[0],\n        'feature_1_EWM_5000' : EWM_5000.get_mean()[1],\n        'feature_0_EWM_10000' : EWM_10000.get_mean()[0],\n        'feature_1_EWM_10000' : EWM_10000.get_mean()[1],\n        'feature_0_EWM_20000' : EWM_20000.get_mean()[0],\n        'feature_1_EWM_20000' : EWM_20000.get_mean()[1],\n    }\n\n    train_FE.append(FE)\n\ntrain_FE = pd.DataFrame(train_FE)","2159e458":"train = pd.concat([train,train_FE],axis=1)\n\nfeatures = [c for c in train.columns if 'feature' in c]\n\nresp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\n\nX = train[features].values\ny = np.stack([(train[c] > 0.000001).astype('int') for c in resp_cols]).T #Multitarget\n\nf_mean = np.mean(train[features[1:]].values,axis=0)","de533ef5":"def create_autoencoder(input_dim,output_dim,noise=0.05):\n    i = Input(input_dim)\n    encoded = BatchNormalization()(i)\n    encoded = GaussianNoise(noise)(encoded)\n    encoded = Dense(640,activation='relu')(encoded)\n    decoded = Dropout(0.2)(encoded)\n    decoded = Dense(input_dim,name='decoded')(decoded)\n    x = Dense(320,activation='relu')(decoded)\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(x)\n    x = Dense(output_dim,activation='sigmoid',name='label_output')(x)\n    \n    encoder = Model(inputs=i,outputs=encoded)\n    autoencoder = Model(inputs=i,outputs=[decoded,x])\n    \n    autoencoder.compile(optimizer=Adam(0.001),loss={'decoded':'mse','label_output':'binary_crossentropy'})\n    return autoencoder, encoder","5880baf1":"def create_model(hp,input_dim,output_dim,encoder):\n    inputs = Input(input_dim)\n    \n    x = encoder(inputs)\n    x = Concatenate()([x,inputs]) #use both raw and encoded features\n    x = BatchNormalization()(x)\n    x = Dropout(hp.Float('init_dropout',0.0,0.5))(x)\n    \n    for i in range(hp.Int('num_layers',1,5)):\n        x = Dense(hp.Int('num_units_{i}',128,256))(x)\n        x = BatchNormalization()(x)\n        x = Lambda(tf.keras.activations.swish)(x)\n        x = Dropout(hp.Float(f'dropout_{i}',0.0,0.5))(x)\n    x = Dense(output_dim,activation='sigmoid')(x)\n    model = Model(inputs=inputs,outputs=x)\n    model.compile(optimizer=Adam(hp.Float('lr',0.00001,0.1,default=0.001)),loss=BinaryCrossentropy(label_smoothing=hp.Float('label_smoothing',0.0,0.1)),metrics=[tf.keras.metrics.AUC(name = 'auc')])\n    return model","63150615":"autoencoder, encoder = create_autoencoder(X.shape[-1],y.shape[-1],noise=0.1)\nif TRAINING:\n    autoencoder.fit(X,(X,y),\n                    epochs=1002,\n                    batch_size=16384, \n                    validation_split=0.1,\n                    callbacks=[EarlyStopping('val_loss',patience=10,restore_best_weights=True)])\n    encoder.save_weights('.\/encoder.hdf5')\nelse:\n    encoder.load_weights('..\/input\/running-algos-fe-for-fast-inference\/encoder.hdf5')\nencoder.trainable = False","f7b2eddc":"model_fn = lambda hp: create_model(hp,X.shape[-1],y.shape[-1],encoder)\n\ntuner = CVTuner(\n        hypermodel=model_fn,\n        oracle=kt.oracles.BayesianOptimization(\n        objective= kt.Objective('val_auc', direction='max'),\n        num_initial_points=4,\n        max_trials=60))\n\nFOLDS = 5\nSEED = 42\ntf.random.set_seed(SEED)\n\nif TRAINING:\n    gkf = PurgedGroupTimeSeriesSplit(n_splits = FOLDS, group_gap=20)\n    splits = list(gkf.split(y, groups=train['date'].values))\n    tuner.search((X,),(y,),splits=splits,batch_size=16384,epochs=300,callbacks=[EarlyStopping('val_auc', mode='max',patience=3)])\n    hp  = tuner.get_best_hyperparameters(1)[0]\n    pd.to_pickle(hp,f'.\/best_hp_{SEED}.pkl')\n    for fold, (train_indices, test_indices) in enumerate(splits):\n        model = model_fn(hp)\n        X_train, X_test = X[train_indices], X[test_indices]\n        y_train, y_test = y[train_indices], y[test_indices]\n        model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=300,batch_size=16384,callbacks=[EarlyStopping('val_auc',mode='max',patience=10,restore_best_weights=True)])\n        model.save_weights(f'.\/model_{SEED}_{fold}.hdf5')\n        model.compile(Adam(hp.get('lr')\/100),loss='binary_crossentropy')\n        model.fit(X_test,y_test,epochs=6,batch_size=16384)\n        model.save_weights(f'.\/model_{SEED}_{fold}_finetune.hdf5')\n    tuner.results_summary()\nelse:\n    models = []\n    hp = pd.read_pickle(f'..\/input\/running-algos-fe-for-fast-inference\/best_hp_{SEED}.pkl')\n    for f in range(FOLDS):\n        model = model_fn(hp)\n        if USE_FINETUNE:\n            model.load_weights(f'..\/input\/running-algos-fe-for-fast-inference\/model_{SEED}_{f}_finetune.hdf5')\n        else:\n            model.load_weights(f'..\/input\/running-algos-fe-for-fast-inference\/model_{SEED}_{f}.hdf5')\n        model = LiteModel.from_keras_model(model)\n        models.append(model)","908dc660":"ENV_REAL = True\n\nif (not TRAINING) & (not ENV_REAL):\n    \n    test_col = pd.read_pickle(f'..\/input\/dummy-environnement\/columns_df_test.pickle')\n\n    n_row = 15219\n\n    dummy_df = train.iloc[:n_row]\n\n    for (index, row) in tqdm(dummy_df.iterrows()):\n\n        time.sleep(0.009)\n        test_df = pd.DataFrame(row).transpose()[test_col]\n\n        test_df = pd.DataFrame(row).transpose()\n        pred_df = pd.DataFrame(columns=['action'], index = [index])\n\n        pred_df.action = 0","803f9767":"if (not TRAINING) & (ENV_REAL):\n    \n    import janestreet\n    env = janestreet.make_env()\n    th = 0.5\n    \n    EWM_5000 = RunningEWMean(WIN_SIZE = 5000,n_size = 2)\n    EWM_10000 = RunningEWMean(WIN_SIZE = 10000,n_size = 2)\n    EWM_20000 = RunningEWMean(WIN_SIZE = 20000,n_size = 2)\n\n    train_FE = []\n    \n    \n    for (test_df, pred_df) in tqdm(env.iter_test()):\n        \n        EWM_5000.push(np.float64(np.array(test_df[['feature_0','feature_1']])))\n        EWM_10000.push(np.float64(np.array(test_df[['feature_0','feature_1']])))\n        EWM_20000.push(np.float64(np.array(test_df[['feature_0','feature_1']])))\n\n        FE = []\n\n        FE = {\n            'feature_0_EWM_5000' : EWM_5000.get_mean()[0][0],\n            'feature_1_EWM_5000' : EWM_5000.get_mean()[0][1],\n            'feature_0_EWM_10000' : EWM_10000.get_mean()[0][0],\n            'feature_1_EWM_10000' : EWM_10000.get_mean()[0][1],\n            'feature_0_EWM_20000' : EWM_20000.get_mean()[0][0],\n            'feature_1_EWM_20000' : EWM_20000.get_mean()[0][1],\n        }\n\n        test_df_FE = pd.concat([test_df,pd.DataFrame(FE, index=[test_df.index[0]])],axis=1)\n\n        if test_df_FE['weight'].item() > 0:\n            x_tt = test_df_FE.loc[:, features].values\n            if np.isnan(x_tt[:, 1:].sum()):\n                x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n            pred = np.mean([model.predict(x_tt) for model in models],axis=0)\n            pred = np.mean(pred)\n            pred_df.action = np.where(pred >= th, 1, 0).astype(int)\n        else:\n            pred_df.action = 0\n            \n        env.predict(pred_df)","966aaf79":"Loading the data :","7638fac6":"Seems to retain some importance when an xgboost is calibrated on them, especially for higher rw. I haven't built a running algo for them yet, but I'll probably try.","79b95d38":"# Load data\n\nLoading a pickle file. Check this notebook [pickling](https:\/\/www.kaggle.com\/quillio\/pickling) if you haven't pickled your data set yet. Check this notebook [one liner to halve your memory usage](https:\/\/www.kaggle.com\/jorijnsmit\/one-liner-to-halve-your-memory-usage) if you want to reduce memory usage before pickling.","4223fa0c":"Given that exponentially weighted moving average can be calculated iteratively without any memory, I feel like it would generally be better to use such Features. I use the formula alpha = 2 \/ (N+1) that give the same 'center of mass' as the traditional mean. My implementation :","66416eae":"<a id='Dummy_Env'><\/a>\n## Dummy environnement","b9e054e4":"<a id='Moving_Moments'><\/a>\n# Moving Moments (variance, skew, kurtosis)\n\nThe aforementionned stack exchange post also implement the variance :","259bcaaa":"Note : I haven't toroughfully tested it yet.","01d58bca":"Adding some feature (long term exponentially weighted mean of feature_0 and feature_1) :","629f49ec":"Great news !","3343f8bd":"<a id='EWMA'><\/a>\n# Exponentially Weighted Moving Average\n\nPython reference implementation :","cc7b6a62":"For a streaming algorithm the idea is to build a class that allows to keep track of past values. Largely inspired from this [Stack exchange answer](https:\/\/stackoverflow.com\/questions\/5147378\/rolling-variance-algorithm). ","58bfbda1":"merge two dataframes and add columns","ae4e1818":"Which seems to match (weight 2.7143664 match the first value on the first row) :","19ed9b77":"<a id='DIFF'><\/a>\n# Differentiation\n\nDirect and second order differentation of averaged variables seems to have some importance, as is, change in overall trends have an importance for the problem of dealing with multiple securities. To be more clear :","e0aacafd":"<a id='Entropy'><\/a>\n# Entropy rate\n\nAlso mentionned in lopez de prado's book. It relate to some physical mesure of order. I am still not entirely convinced this could work here. Especially because there is a lot of different notions of entropy and all of them are rather calculatory. For reference (outside of inference) I was able to calculate entropy on sliding windows with the pyinform package (see code below). But this is rather slow and doesn't seems to provide any information gain in my early tests.\n\nHowever I have found [a streaming implementation](https:\/\/github.com\/ajcr\/rolling) that could be reimplemented for our problem so I am mentionning it.","000bbf07":"I change the days a bit to remove the day previous day 85 as the continuity of JS strategy is in question (or is that a different market regime ?)","e9d9a2db":"I still have two main problems here :\n - <s> It doesn't seem to converge properly due to some rounding error (see below)<\/s> Thanks to @magokecol for pointing the mistake out !\n - It does not handle na as is. I propose some code to use the last value at the moment, which might be pertinent for some fetaure but probably not all of them. It is also a bit slower when activated (from 1800 it\/sec to 1400 with it\/sec).\n \nThe second point is not problematic as is, but if we want to use it properly we might either want to replicate exactly what standards library (rolling) does or we want to apply the streaming algo to our whole train set, so as not to create a discrepency between the two.","332ecc29":"# Running algorithms - Feature Engineering for fast inference\n\n\nOne of the main problem in this competition is about the inference : we have to predict outcomes one by one, which take quite some time.\nThis bottleneck also impact what we can do with feature engineering as we are limited to things that calculate fast during the inference process. \nI looked what could be used iteratively to stay within this time constraint. \nIt appears that there is a whole class of algorithms - running algorithms (or sometimes streaming algorithms) - that are designed exactly for this kind of problems.\n\nI was about to publish a notebook about financial feature engineering alone, but I figured this wouldn't be very useful without inference implementations. So I decided to fuse both and here we are : in this notebook I give you some of the usual financial feature engineering tools and, along some standard python implementation, I try to provide you with streaming algorithms that will allow you to calculate them during inference. And it turns out that using numpy is **EXTREMELY FAST**. What is not in this notebook : some way to decide what parameter value to use (lag) or on which features to use this techniques (this might be the topic of another notebook \ud83d\ude09). \n\nPlease keep in mind that those implementations are my owns. So there might be some problems (well there are some problems, I even point them out), feel free to comment with a correction. \n\nIf you want to go further, you can check my other works (about [Intraday Feature Exploration](https:\/\/www.kaggle.com\/lucasmorin\/complete-intraday-feature-exploration),[Target Engineering](https:\/\/www.kaggle.com\/lucasmorin\/target-engineering-patterns-denoising), and [using yfinance to download financial data in Ptyhon](https:\/\/www.kaggle.com\/lucasmorin\/downloading-market-data)). Feel free to upvote \/ share my notebooks.\nLucas\n\n## updates :\n\nv.11 : added a complete exemple with a model\n\nv.13 : \n\n- Did some testing and modified edge case\n- Added a dummy environnement for testing\n- will probably split training and submission\n\n## Features engineering techniques :\n\n- [Moving Average (starter)](#Moving_Average) \ud83c\udfc3\ud83c\udfc3\ud83c\udfc3\n- [Moving Moments](#Moving_Moments) (variance, skew, kurtosis) \ud83c\udfc3\ud83c\udfc3\n- [Exponentially Weighted Moving Average](#EWMA) \ud83c\udfc3\ud83c\udfc3\ud83c\udfc3\n- [Past day average ](#PDA)\ud83c\udfc3\ud83c\udfc3\n- [Paste Trade Information](#PTI) \ud83c\udfc3\ud83c\udfc3\ud83c\udfc3\n- [Differentiation ](#DIFF)\n- [Fractional differentiation](#FDIFF)\n- [Entropy](#Entropy)\n\n# Application :\n\n- [Bottleneck encoder + MLP + Keras Tuner](#MLP)\n- [Dummy Environnement](#Dummy_Env)\n- [Submission](#Submission)","f0cc3d67":"It isn't really slower than the mean approach. So we might get the variance for almost free.","c02cf713":"For reference I'll leave the pandas implementation :","21ed89cb":"<a id='Moving_Average'><\/a>\n# Moving Average (starter)\n\nIt is a very standard indicator for financial time series. The goal here is to build a demo. Honestly from what I have seen so far, as we have different securities in the data running windows mean doesn't appears to be that usefull for lower windows.\nIf you want to test their importance the usual pandas way of doing that is simply :","e2cc33ab":"A test seems to run pretty fast (10000 it\/s)","86234b1c":"<a id='MLP'><\/a>\n# Application","3836ddcb":"And it is also a bit faster (11000 it\/second):","e0b24aa3":"# \ud83d\ude0d","058b8ca9":"<a id='Submission'><\/a>\n## Submission","66439ccb":"So, unless someone points out a way to make fractionnal differentiation work in our data set (or that an approximation allows for some information gain), I don't really plan to make a streaming algorithm for building it.","93152338":"Training the model :","a17753c7":"<a id='FDIFF'><\/a>\n# Fractional differentiation\n\nVery important feature engineering tool, especially for time series *. As illustrated by Marcos Lopez de Prado in his book Advances in Financial Machine Learning, It is a great tool to remove noise without removing information. \n\nThe idea is to generalise differentiation to non integer. Applying multiple stationnarity tests while slowly increasing the fractionnal differentiation level, you can get an 'optimal' level (enough differentiation to remove noise, without removing information). \n\nHowever, I havea found that for optimal level of around -0.75 - that I found in the dataset ** - we would need longer series to make the calculation meaningful. This is illustrated in the example below : the weight for the millionth instance is still above 0.02 for the first one. This is also problematic as it means that the first instances would lack a lot of information.\n\n\\* : it is rather difficult to even use time series tools here as we have many underlying securities\n\n**  : when the adf test would even converge after lenghty calculations","592e669c":"Loading the packages :","42b04a6e":"PurgedTimeSeries CV","bf5f1b2f":"**We can check that it run fast (iterrrows allow to loop trough rows of a data frame as an interable). Here using numpy array instead of pandas datframe allow to go from 1600 it\/sec to 9000+ it\/seconds when keeping track of ALL 10000 tick lagged means. Using @gogo827jz fillna method allow to breach 10000 it\/sec.** \n\n**It also shows you how easy it is to use for inference.**","f89e4022":"# Loading base packages\n\nNothing too surprising here. Collections deque data structure will help us keep track of past data.","efa9d9ad":"In this section I give you an idea of one would apply the algo for submission.\nThe model used come from this [notebook](https:\/\/www.kaggle.com\/aimind\/bottleneck-encoder-mlp-keras-tuner-8601c5). I only show where feature engineering appears.","1af2ddad":"<a id='PDA'><\/a>\n# Past day average\n\nGiven that some long term moving average appears to have some gain, I figured it would probably make sense to calculate past day average.\nI haven't given a lot of attention with a pythonic way but I think I can give it a shot in a streaming way :","061227ec":"<a id='PTI'><\/a>\n# Previous trade information from the same underlying\n\nAs mentionned [here](https:\/\/www.kaggle.com\/c\/jane-street-market-prediction\/discussion\/207709) feature_41 being constant over the day allow to find the previous instance with the same feature_41 caracteristic. As we don't exactly know what are in those feature we can't really know how the trade opportunities relate exactly but I speculate that they relate to the same underlying or are pretty close and that information about the previous trade of the day relating to the same underlying might be usefull. At the moment the implementation rely on a simple dictionnary. I suspect I can't really get below keeping 800-900 instances in memory for that feature engineering technique as I need at least one example of each trade. It is not a problem and the method is really fast (10000 it\/s).","cf1fd62d":"**Create autoencoder, MLP :**","158a90a6":"Somehow it seems to also work better than the standard average :","c5de01a8":"I implement the modifications suggested in comments + add some way to handle missing values. As above this is not problematic for the mean. I think it might get a bit more problematic for the variance, as replacing with last values will systematically lower the variance.\n\nNote : \n- I haven't tested the variance toroughfully yet\n- The post refers to a blog wich refer to another post that gives a solution for [skew and kurtosis in C++](https:\/\/www.johndcook.com\/blog\/skewness_kurtosis\/), I'll see what I can implement myself in Python.\n"}}