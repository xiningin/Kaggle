{"cell_type":{"51b53cf2":"code","28be2e03":"code","6e125166":"code","1396c75c":"code","3bbba0af":"code","c1d5c7e8":"code","a2b0d13a":"code","dcff9ce1":"code","4a9704ec":"code","4375bc83":"code","f6c2c379":"code","6a0699f1":"code","dbd35d05":"code","6345fe3e":"code","34b479ac":"code","514f4fda":"code","e1219e5e":"code","6e693aa9":"code","35a0e009":"code","9bfe55b0":"code","096055d6":"code","1ed7e1a3":"code","6865207f":"code","81dffe0d":"code","c8ad0046":"code","94b51f02":"code","b46f6ff4":"code","d66060da":"code","520d2b8a":"code","c42d53af":"code","869a871e":"code","16504668":"code","dbce9ecf":"code","3e0357aa":"code","892259fc":"markdown","d7ba6969":"markdown","99ba407d":"markdown","55459ca7":"markdown"},"source":{"51b53cf2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","28be2e03":"agent_df = pd.read_csv('\/kaggle\/input\/mlsp-hackathon\/train_HK6lq50.csv')","6e125166":"y = agent_df.is_pass\nX = agent_df.drop(['is_pass','id','test_id'],axis=1)","1396c75c":"cat_cols = [cname for cname in X.columns if X[cname].dtypes == 'object' and X[cname].nunique() < 25]\nnum_cols = [cname for cname in X.columns if X[cname].dtypes in ['int64','float64']]\nprint (cat_cols)\nprint (num_cols)\n#Utilize only these columns for Model configuration\nuse_cols = cat_cols + num_cols\nX = X[use_cols].copy()\n","3bbba0af":"#Preprocessing Numerical columns - Missing values \ndef find_null_col(dataset):\n    null_col = dataset.columns[dataset.isnull().any()]\n    null_col_sum = dataset[null_col].isnull().sum()\n    return null_col,null_col_sum\n  \n#impute by groupby on following columns -\ngroup_col1 = ['trainee_id']\ngroup_col2 = ['gender','education','city_tier']\n\n#UDF to impute Null values with the mean of the group of meaningful columns.\ndef impute_null(dataset,null_col,group_col):\n    for col in null_col:\n        dataset[col] = dataset[col].fillna(dataset.groupby(group_col)[col].transform('mean'))\n","c1d5c7e8":"null_col_X,null_col_X_sum = find_null_col(X)\nimpute_null(X,null_col_X,group_col1)\nimpute_null(X,null_col_X,group_col2)\nnull_col_X,null_col_X_sum = find_null_col(X)\nprint(\"Null Columns :\" ,null_col_X)\nprint(\"Null Columns info :\",null_col_X_sum)","a2b0d13a":"X.drop(['trainee_id'],axis=1,inplace=True)\nnum_col_x = ['program_duration', 'city_tier', 'age', 'total_programs_enrolled', 'trainee_engagement_rating']","dcff9ce1":"from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split","4a9704ec":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","4375bc83":"my_num_imputer = SimpleImputer(strategy='mean')\nSIM_X_train = pd.DataFrame(my_num_imputer.fit_transform(X_train[num_col_x]))\nSIM_X_test = pd.DataFrame(my_num_imputer.transform(X_test[num_col_x]))\nSIM_X_train.columns = X_train[num_col_x].columns\nSIM_X_test.columns = X_test[num_col_x].columns\n#SIM_X_train","f6c2c379":"my_num_scaler = StandardScaler()\nSSM_X_train = pd.DataFrame(my_num_scaler.fit_transform(SIM_X_train))\nSSM_X_test = pd.DataFrame(my_num_scaler.transform(SIM_X_test))\nSSM_X_train.columns = SIM_X_train.columns\nSSM_X_test.columns = SIM_X_test.columns\n#SSM_X_train","6a0699f1":"my_cat_imputer = SimpleImputer(strategy='most_frequent')\nSIC_X_train = pd.DataFrame(my_cat_imputer.fit_transform(X_train[cat_cols]))\nSIC_X_test = pd.DataFrame(my_cat_imputer.transform(X_test[cat_cols]))\nSIC_X_train.columns = X_train[cat_cols].columns\nSIC_X_test.columns = X_test[cat_cols].columns\n#SIC_X_train","dbd35d05":"my_ohe_encoder = OneHotEncoder(handle_unknown='ignore',sparse=False)\nOHE_X_train = pd.DataFrame(my_ohe_encoder.fit_transform(SIC_X_train))\nOHE_X_test = pd.DataFrame(my_ohe_encoder.transform(SIC_X_test))\n#OHE removes index, so we put it back\nOHE_X_train.index = SIC_X_train.index\nOHE_X_test.index = SIC_X_test.index\n#OHE_X_train","6345fe3e":"X_train_r = pd.concat([SSM_X_train,OHE_X_train],axis=1)\nX_test_r = pd.concat([SSM_X_test,OHE_X_test],axis=1)","34b479ac":"import xgboost as xgb\nfrom sklearn import metrics\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import cross_validate, GridSearchCV\n\nimport matplotlib.pylab as plt\n%matplotlib inline\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 12, 4","514f4fda":"def modelfit(alg, X_train,y_train,useTrainCV=True, cv_folds=5, early_stopping_rounds=10):\n        \n    if useTrainCV:\n        xgb_param = alg.get_xgb_params()\n        xgtrain = xgb.DMatrix(X_train.values, label=y_train.values)\n        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n                          metrics='auc', early_stopping_rounds=early_stopping_rounds)\n        alg.set_params(n_estimators=cvresult.shape[0])\n    \n    #Fit the algorithm on the data\n    alg.fit(X_train, y_train, eval_metric='auc')\n        \n    #Predict training set:\n    dtrain_predictions = alg.predict(X_train)\n    dtrain_predprob = alg.predict_proba(X_train)[:,1]\n        \n    #Print model report:\n    print (\"\\nModel Report\")\n    print (\"Accuracy : %.4g\" % metrics.accuracy_score(y_train.values, dtrain_predictions))\n    print (\"AUC Score (Train): %f\" % metrics.roc_auc_score(y_train, dtrain_predprob))\n                    \n    feat_imp = pd.Series(alg.get_booster().get_fscore()).sort_values(ascending=False)\n    feat_imp.plot(kind='bar', title='Feature Importances')\n    plt.ylabel('Feature Importance Score')","e1219e5e":"#Choose all predictors except target & IDcols\nxgb1 = XGBClassifier(\n learning_rate =0.1,\n n_estimators=200,\n max_depth=5,\n min_child_weight=1,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread=4,\n scale_pos_weight=1,\n seed=27)\n","6e693aa9":"modelfit(xgb1, X_train_r, y_train)\ny_pred1 = xgb1.predict(X_test_r)\nscore1 = roc_auc_score(y_test,y_pred1)\nprint(score1)\nprint(xgb1)","35a0e009":"#Use n_estimators = 140, since it produced best result\n#Tune max_depth and min_child_weight\nparam_test1 = {\n 'max_depth':range(3,10,2),\n 'min_child_weight':range(1,6,2)\n}\ngsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), \n param_grid = param_test1, scoring='roc_auc',n_jobs=4, cv=5)\ngsearch1.fit(X_train_r,y_train)\ngsearch1.cv_results_, gsearch1.best_params_, gsearch1.best_score_","9bfe55b0":"#Tune Gamma\nparam_test2 = {\n 'gamma':[i\/10.0 for i in range(0,5)]\n}\ngsearch2 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=9,\n min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test2, scoring='roc_auc',n_jobs=4, cv=5)\ngsearch2.fit(X_train_r,y_train)\ngsearch2.cv_results_, gsearch2.best_params_, gsearch2.best_score_","096055d6":"#Re-calibrate\nxgb2 = XGBClassifier(\n learning_rate =0.1,\n n_estimators=140,\n max_depth=9,\n min_child_weight=1,\n gamma=0.0,\n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread=4,\n scale_pos_weight=1,\n seed=27)","1ed7e1a3":"modelfit(xgb2, X_train_r, y_train)\ny_pred2 = xgb2.predict(X_test_r)\nscore2 = roc_auc_score(y_test,y_pred2)\nprint(score2)\nprint(xgb2)","6865207f":"# Tune subsample and colsample_bytree\nparam_test3 = {\n 'subsample':[i\/10.0 for i in range(6,10)],\n 'colsample_bytree':[i\/10.0 for i in range(6,10)]\n}\ngsearch3 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1, n_estimators=140, max_depth=9,\n min_child_weight=1, gamma=0.0, subsample=0.8, colsample_bytree=0.8,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test3, scoring='roc_auc',n_jobs=4, cv=5)\ngsearch3.fit(X_train_r,y_train)\ngsearch3.cv_results_, gsearch3.best_params_, gsearch3.best_score_","81dffe0d":"#Tuning Regularisation parameters\nparam_test4 = {\n 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n}\ngsearch4 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=4,\n min_child_weight=6, gamma=0.0, subsample=0.8, colsample_bytree=0.9,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test4, scoring='roc_auc',n_jobs=4, cv=5)\ngsearch4.fit(X_train_r,y_train)\ngsearch4.cv_results_, gsearch4.best_params_, gsearch4.best_score_","c8ad0046":"#Re-calibrate\nxgb3 = XGBClassifier(\n learning_rate =0.1,\n n_estimators=140,\n max_depth=4,\n min_child_weight=6,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.9,\n reg_alpha=0.01,\n objective= 'binary:logistic',\n nthread=4,\n scale_pos_weight=1,\n seed=27)","94b51f02":"modelfit(xgb3, X_train_r, y_train)\ny_pred3 = xgb3.predict(X_test_r)\nscore3 = roc_auc_score(y_test,y_pred3)\nprint(score3)\nprint(xgb3)","b46f6ff4":"#Reducing the learning rate\nxgb4 = XGBClassifier(\n learning_rate =0.05,\n n_estimators=140,\n max_depth=4,\n min_child_weight=6,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n reg_alpha=0.005,\n objective= 'binary:logistic',\n nthread=4,\n scale_pos_weight=1,\n seed=27)","d66060da":"modelfit(xgb4, X_train_r, y_train)\ny_pred4 = xgb4.predict(X_test_r)\nscore4 = roc_auc_score(y_test,y_pred4)\nprint(score4)\nprint(xgb4)","520d2b8a":"from sklearn.externals import joblib\njoblib.dump(xgb4, 'MLSP_Hackathon_XGB_GCV.pkl')","c42d53af":"test_df = pd.read_csv('\/kaggle\/input\/mlsp-hackathon\/test_wF0Ps6O.csv')\ntest_df.drp(['test_id'],axis=1,inplace=True)","869a871e":"#Removing the ID column and storing in another frame\nX_test_f = test_df.iloc[:,1:15]\nX_test_idf = test_df.iloc[:,0:1]\nprint(X_test_f.shape)\nprint(X_test_idf.shape)","16504668":"cat_cols = [cname for cname in X_test_f.columns if X_test_f[cname].dtypes == 'object' and X_test_f[cname].nunique() < 25]\nnum_cols = [cname for cname in X_test_f.columns if X_test_f[cname].dtypes in ['int64','float64']]\nprint (cat_cols)\nprint (num_cols)\n#Utilize only these columns for Model configuration\nuse_cols = cat_cols + num_cols\nX_test_f = X_test_f[use_cols].copy()\n\nnull_col_X,null_col_X_sum = find_null_col(X_test_f)\nimpute_null(X_test_f,null_col_X,group_col1)\nimpute_null(X_test_f,null_col_X,group_col2)\nnull_col_X,null_col_X_sum = find_null_col(X_test_f)\nprint(\"Null Columns :\" ,null_col_X)\nprint(\"Null Columns info :\",null_col_X_sum)\n\nX_test_f.drop(['trainee_id'],axis=1,inplace=True)\n\nmy_num_imputer = SimpleImputer(strategy='mean')\nSIM_X_test_f = pd.DataFrame(my_num_imputer.fit_transform(X_test_f[num_col_x]))\nSIM_X_test_f.columns = X_test_f[num_col_x].columns\n\nmy_num_scaler = StandardScaler()\nSSM_X_test_f = pd.DataFrame(my_num_scaler.fit_transform(SIM_X_test_f))\nSSM_X_test_f.columns = SIM_X_test_f.columns\n\nmy_cat_imputer = SimpleImputer(strategy='most_frequent')\nSIC_X_test_f = pd.DataFrame(my_cat_imputer.fit_transform(X_test_f[cat_cols]))\nSIC_X_test_f.columns = X_test_f[cat_cols].columns\n\nmy_ohe_encoder = OneHotEncoder(handle_unknown='ignore',sparse=False)\nOHE_X_test_f = pd.DataFrame(my_ohe_encoder.fit_transform(SIC_X_test_f))\nOHE_X_test_f.index = SIC_X_test_f.index\n\nX_test_fr = pd.concat([SSM_X_test_f,OHE_X_test_f],axis=1)","dbce9ecf":"MLSP_model_XGB_GCV = joblib.load('MLSP_Hackathon_XGB_GCV.pkl')\ny_test_fr = MLSP_model_XGB_GCV.predict(X_test_fr)","3e0357aa":"submission_df = pd.read_csv('\/kaggle\/input\/mlsp-hackathon\/sample_submission_vaSxamm.csv')\nsubmission_df['is_pass'] = y_test_fr\nsubmission_df.to_csv('MSLP_Hackathon_XGB_GCV_sub.csv',index=False)","892259fc":" {'gamma': 0.0},\n 0.7374252382168501)","d7ba6969":"{'max_depth': 9, 'min_child_weight': 1},\n 0.7374252382168501)","99ba407d":"{'reg_alpha': 0.01},\n 0.7278107244185575)","55459ca7":"{'colsample_bytree': 0.9, 'subsample': 0.8},\n 0.7428155416959872)"}}