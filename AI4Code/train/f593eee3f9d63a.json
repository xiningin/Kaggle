{"cell_type":{"cacdcde7":"code","85d60b3b":"code","1f0fdeb2":"code","3add27ce":"code","20eb07e8":"code","5990681d":"code","15160dfc":"code","546e80c4":"code","77e77a29":"code","7544317c":"code","351c401b":"code","fe27f18a":"code","4a513ce0":"code","45d8d1af":"code","2ce38e7a":"code","ae7dd20b":"code","29b1efe9":"code","0089159c":"code","624603c7":"code","51bbc4b3":"code","53312fac":"code","4afde866":"code","86c35953":"code","506128e2":"code","276f3f8f":"code","3ed37b74":"code","02cf6999":"code","5bb462c3":"code","6afe12a1":"code","932b329f":"markdown","9a2c86ea":"markdown","61e5c60f":"markdown","4ff7644b":"markdown","53806200":"markdown","a453259f":"markdown","3881f2b9":"markdown","1a9d0ae6":"markdown","a66d9bfc":"markdown","9ae19342":"markdown","697aa1b2":"markdown","edce94b4":"markdown","ffc6ec6f":"markdown","662ada06":"markdown"},"source":{"cacdcde7":"import time\ntime_start = time.time()","85d60b3b":"!pip install ..\/input\/pretrainedmodels\/pretrainedmodels-0.7.4\/pretrainedmodels-0.7.4\/ > \/dev\/null","1f0fdeb2":"! python ..\/input\/mlcomp\/mlcomp\/mlcomp\/setup.py","3add27ce":"# ! pip install ..\/input\/pytorch-toolbelt\/pytorch-toolbelt-develop","20eb07e8":"package_path = '..\/input\/senetunetmodelcode' # add unet script dataset\nimport sys\nsys.path.append(package_path)\n\n# Get necessary Imports\nimport pdb\nimport os\nimport cv2\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.data import DataLoader, Dataset\nfrom albumentations import (Normalize, Compose)\nfrom albumentations.pytorch import ToTensor\nimport torch.utils.data as data\nimport torchvision.models as models\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom senet_unet_model_code import Unet\n\n##########################################\n\nimport matplotlib.pyplot as plt\n\nimport albumentations as A\nfrom tqdm import tqdm_notebook\nfrom torch.jit import load\n\nfrom mlcomp.contrib.transform.albumentations import ChannelTranspose\nfrom mlcomp.contrib.dataset.classify import ImageDataset\nfrom mlcomp.contrib.transform.rle import rle2mask, mask2rle\nfrom mlcomp.contrib.transform.tta import TtaWrap\n\n###########################################\n\n# from pathlib import Path\n\n# from pytorch_toolbelt.inference import tta\n# from pytorch_toolbelt.utils.rle import rle_encode\n# from pytorch_toolbelt.utils.rle import rle_decode\n# from pytorch_toolbelt.utils.rle import rle_to_string\n# from pytorch_toolbelt.inference.functional import pad_image_tensor, unpad_image_tensor\n# from pytorch_toolbelt.modules import decoders as D\n# from pytorch_toolbelt.modules import encoders as E\n# from pytorch_toolbelt.modules.fpn import *\n\n# from models import PretrainedUNet\n\n###########################################\nimport fastai\nfrom fastai.vision import *\nfrom PIL import Image\nimport zipfile\nimport io\n\nimport gc","5990681d":"def load_hengs_clf_model():\n    # Codes from Heng's baseline\n    # This code is for classifcation model\n\n    BatchNorm2d = nn.BatchNorm2d\n\n    ###############################################################################\n    CONVERSION=[\n     'block0.0.weight',\t(64, 3, 7, 7),\t 'conv1.weight',\t(64, 3, 7, 7),\n     'block0.1.weight',\t(64,),\t 'bn1.weight',\t(64,),\n     'block0.1.bias',\t(64,),\t 'bn1.bias',\t(64,),\n     'block0.1.running_mean',\t(64,),\t 'bn1.running_mean',\t(64,),\n     'block0.1.running_var',\t(64,),\t 'bn1.running_var',\t(64,),\n     'block1.1.conv_bn1.conv.weight',\t(64, 64, 3, 3),\t 'layer1.0.conv1.weight',\t(64, 64, 3, 3),\n     'block1.1.conv_bn1.bn.weight',\t(64,),\t 'layer1.0.bn1.weight',\t(64,),\n     'block1.1.conv_bn1.bn.bias',\t(64,),\t 'layer1.0.bn1.bias',\t(64,),\n     'block1.1.conv_bn1.bn.running_mean',\t(64,),\t 'layer1.0.bn1.running_mean',\t(64,),\n     'block1.1.conv_bn1.bn.running_var',\t(64,),\t 'layer1.0.bn1.running_var',\t(64,),\n     'block1.1.conv_bn2.conv.weight',\t(64, 64, 3, 3),\t 'layer1.0.conv2.weight',\t(64, 64, 3, 3),\n     'block1.1.conv_bn2.bn.weight',\t(64,),\t 'layer1.0.bn2.weight',\t(64,),\n     'block1.1.conv_bn2.bn.bias',\t(64,),\t 'layer1.0.bn2.bias',\t(64,),\n     'block1.1.conv_bn2.bn.running_mean',\t(64,),\t 'layer1.0.bn2.running_mean',\t(64,),\n     'block1.1.conv_bn2.bn.running_var',\t(64,),\t 'layer1.0.bn2.running_var',\t(64,),\n     'block1.2.conv_bn1.conv.weight',\t(64, 64, 3, 3),\t 'layer1.1.conv1.weight',\t(64, 64, 3, 3),\n     'block1.2.conv_bn1.bn.weight',\t(64,),\t 'layer1.1.bn1.weight',\t(64,),\n     'block1.2.conv_bn1.bn.bias',\t(64,),\t 'layer1.1.bn1.bias',\t(64,),\n     'block1.2.conv_bn1.bn.running_mean',\t(64,),\t 'layer1.1.bn1.running_mean',\t(64,),\n     'block1.2.conv_bn1.bn.running_var',\t(64,),\t 'layer1.1.bn1.running_var',\t(64,),\n     'block1.2.conv_bn2.conv.weight',\t(64, 64, 3, 3),\t 'layer1.1.conv2.weight',\t(64, 64, 3, 3),\n     'block1.2.conv_bn2.bn.weight',\t(64,),\t 'layer1.1.bn2.weight',\t(64,),\n     'block1.2.conv_bn2.bn.bias',\t(64,),\t 'layer1.1.bn2.bias',\t(64,),\n     'block1.2.conv_bn2.bn.running_mean',\t(64,),\t 'layer1.1.bn2.running_mean',\t(64,),\n     'block1.2.conv_bn2.bn.running_var',\t(64,),\t 'layer1.1.bn2.running_var',\t(64,),\n     'block1.3.conv_bn1.conv.weight',\t(64, 64, 3, 3),\t 'layer1.2.conv1.weight',\t(64, 64, 3, 3),\n     'block1.3.conv_bn1.bn.weight',\t(64,),\t 'layer1.2.bn1.weight',\t(64,),\n     'block1.3.conv_bn1.bn.bias',\t(64,),\t 'layer1.2.bn1.bias',\t(64,),\n     'block1.3.conv_bn1.bn.running_mean',\t(64,),\t 'layer1.2.bn1.running_mean',\t(64,),\n     'block1.3.conv_bn1.bn.running_var',\t(64,),\t 'layer1.2.bn1.running_var',\t(64,),\n     'block1.3.conv_bn2.conv.weight',\t(64, 64, 3, 3),\t 'layer1.2.conv2.weight',\t(64, 64, 3, 3),\n     'block1.3.conv_bn2.bn.weight',\t(64,),\t 'layer1.2.bn2.weight',\t(64,),\n     'block1.3.conv_bn2.bn.bias',\t(64,),\t 'layer1.2.bn2.bias',\t(64,),\n     'block1.3.conv_bn2.bn.running_mean',\t(64,),\t 'layer1.2.bn2.running_mean',\t(64,),\n     'block1.3.conv_bn2.bn.running_var',\t(64,),\t 'layer1.2.bn2.running_var',\t(64,),\n     'block2.0.conv_bn1.conv.weight',\t(128, 64, 3, 3),\t 'layer2.0.conv1.weight',\t(128, 64, 3, 3),\n     'block2.0.conv_bn1.bn.weight',\t(128,),\t 'layer2.0.bn1.weight',\t(128,),\n     'block2.0.conv_bn1.bn.bias',\t(128,),\t 'layer2.0.bn1.bias',\t(128,),\n     'block2.0.conv_bn1.bn.running_mean',\t(128,),\t 'layer2.0.bn1.running_mean',\t(128,),\n     'block2.0.conv_bn1.bn.running_var',\t(128,),\t 'layer2.0.bn1.running_var',\t(128,),\n     'block2.0.conv_bn2.conv.weight',\t(128, 128, 3, 3),\t 'layer2.0.conv2.weight',\t(128, 128, 3, 3),\n     'block2.0.conv_bn2.bn.weight',\t(128,),\t 'layer2.0.bn2.weight',\t(128,),\n     'block2.0.conv_bn2.bn.bias',\t(128,),\t 'layer2.0.bn2.bias',\t(128,),\n     'block2.0.conv_bn2.bn.running_mean',\t(128,),\t 'layer2.0.bn2.running_mean',\t(128,),\n     'block2.0.conv_bn2.bn.running_var',\t(128,),\t 'layer2.0.bn2.running_var',\t(128,),\n     'block2.0.shortcut.conv.weight',\t(128, 64, 1, 1),\t 'layer2.0.downsample.0.weight',\t(128, 64, 1, 1),\n     'block2.0.shortcut.bn.weight',\t(128,),\t 'layer2.0.downsample.1.weight',\t(128,),\n     'block2.0.shortcut.bn.bias',\t(128,),\t 'layer2.0.downsample.1.bias',\t(128,),\n     'block2.0.shortcut.bn.running_mean',\t(128,),\t 'layer2.0.downsample.1.running_mean',\t(128,),\n     'block2.0.shortcut.bn.running_var',\t(128,),\t 'layer2.0.downsample.1.running_var',\t(128,),\n     'block2.1.conv_bn1.conv.weight',\t(128, 128, 3, 3),\t 'layer2.1.conv1.weight',\t(128, 128, 3, 3),\n     'block2.1.conv_bn1.bn.weight',\t(128,),\t 'layer2.1.bn1.weight',\t(128,),\n     'block2.1.conv_bn1.bn.bias',\t(128,),\t 'layer2.1.bn1.bias',\t(128,),\n     'block2.1.conv_bn1.bn.running_mean',\t(128,),\t 'layer2.1.bn1.running_mean',\t(128,),\n     'block2.1.conv_bn1.bn.running_var',\t(128,),\t 'layer2.1.bn1.running_var',\t(128,),\n     'block2.1.conv_bn2.conv.weight',\t(128, 128, 3, 3),\t 'layer2.1.conv2.weight',\t(128, 128, 3, 3),\n     'block2.1.conv_bn2.bn.weight',\t(128,),\t 'layer2.1.bn2.weight',\t(128,),\n     'block2.1.conv_bn2.bn.bias',\t(128,),\t 'layer2.1.bn2.bias',\t(128,),\n     'block2.1.conv_bn2.bn.running_mean',\t(128,),\t 'layer2.1.bn2.running_mean',\t(128,),\n     'block2.1.conv_bn2.bn.running_var',\t(128,),\t 'layer2.1.bn2.running_var',\t(128,),\n     'block2.2.conv_bn1.conv.weight',\t(128, 128, 3, 3),\t 'layer2.2.conv1.weight',\t(128, 128, 3, 3),\n     'block2.2.conv_bn1.bn.weight',\t(128,),\t 'layer2.2.bn1.weight',\t(128,),\n     'block2.2.conv_bn1.bn.bias',\t(128,),\t 'layer2.2.bn1.bias',\t(128,),\n     'block2.2.conv_bn1.bn.running_mean',\t(128,),\t 'layer2.2.bn1.running_mean',\t(128,),\n     'block2.2.conv_bn1.bn.running_var',\t(128,),\t 'layer2.2.bn1.running_var',\t(128,),\n     'block2.2.conv_bn2.conv.weight',\t(128, 128, 3, 3),\t 'layer2.2.conv2.weight',\t(128, 128, 3, 3),\n     'block2.2.conv_bn2.bn.weight',\t(128,),\t 'layer2.2.bn2.weight',\t(128,),\n     'block2.2.conv_bn2.bn.bias',\t(128,),\t 'layer2.2.bn2.bias',\t(128,),\n     'block2.2.conv_bn2.bn.running_mean',\t(128,),\t 'layer2.2.bn2.running_mean',\t(128,),\n     'block2.2.conv_bn2.bn.running_var',\t(128,),\t 'layer2.2.bn2.running_var',\t(128,),\n     'block2.3.conv_bn1.conv.weight',\t(128, 128, 3, 3),\t 'layer2.3.conv1.weight',\t(128, 128, 3, 3),\n     'block2.3.conv_bn1.bn.weight',\t(128,),\t 'layer2.3.bn1.weight',\t(128,),\n     'block2.3.conv_bn1.bn.bias',\t(128,),\t 'layer2.3.bn1.bias',\t(128,),\n     'block2.3.conv_bn1.bn.running_mean',\t(128,),\t 'layer2.3.bn1.running_mean',\t(128,),\n     'block2.3.conv_bn1.bn.running_var',\t(128,),\t 'layer2.3.bn1.running_var',\t(128,),\n     'block2.3.conv_bn2.conv.weight',\t(128, 128, 3, 3),\t 'layer2.3.conv2.weight',\t(128, 128, 3, 3),\n     'block2.3.conv_bn2.bn.weight',\t(128,),\t 'layer2.3.bn2.weight',\t(128,),\n     'block2.3.conv_bn2.bn.bias',\t(128,),\t 'layer2.3.bn2.bias',\t(128,),\n     'block2.3.conv_bn2.bn.running_mean',\t(128,),\t 'layer2.3.bn2.running_mean',\t(128,),\n     'block2.3.conv_bn2.bn.running_var',\t(128,),\t 'layer2.3.bn2.running_var',\t(128,),\n     'block3.0.conv_bn1.conv.weight',\t(256, 128, 3, 3),\t 'layer3.0.conv1.weight',\t(256, 128, 3, 3),\n     'block3.0.conv_bn1.bn.weight',\t(256,),\t 'layer3.0.bn1.weight',\t(256,),\n     'block3.0.conv_bn1.bn.bias',\t(256,),\t 'layer3.0.bn1.bias',\t(256,),\n     'block3.0.conv_bn1.bn.running_mean',\t(256,),\t 'layer3.0.bn1.running_mean',\t(256,),\n     'block3.0.conv_bn1.bn.running_var',\t(256,),\t 'layer3.0.bn1.running_var',\t(256,),\n     'block3.0.conv_bn2.conv.weight',\t(256, 256, 3, 3),\t 'layer3.0.conv2.weight',\t(256, 256, 3, 3),\n     'block3.0.conv_bn2.bn.weight',\t(256,),\t 'layer3.0.bn2.weight',\t(256,),\n     'block3.0.conv_bn2.bn.bias',\t(256,),\t 'layer3.0.bn2.bias',\t(256,),\n     'block3.0.conv_bn2.bn.running_mean',\t(256,),\t 'layer3.0.bn2.running_mean',\t(256,),\n     'block3.0.conv_bn2.bn.running_var',\t(256,),\t 'layer3.0.bn2.running_var',\t(256,),\n     'block3.0.shortcut.conv.weight',\t(256, 128, 1, 1),\t 'layer3.0.downsample.0.weight',\t(256, 128, 1, 1),\n     'block3.0.shortcut.bn.weight',\t(256,),\t 'layer3.0.downsample.1.weight',\t(256,),\n     'block3.0.shortcut.bn.bias',\t(256,),\t 'layer3.0.downsample.1.bias',\t(256,),\n     'block3.0.shortcut.bn.running_mean',\t(256,),\t 'layer3.0.downsample.1.running_mean',\t(256,),\n     'block3.0.shortcut.bn.running_var',\t(256,),\t 'layer3.0.downsample.1.running_var',\t(256,),\n     'block3.1.conv_bn1.conv.weight',\t(256, 256, 3, 3),\t 'layer3.1.conv1.weight',\t(256, 256, 3, 3),\n     'block3.1.conv_bn1.bn.weight',\t(256,),\t 'layer3.1.bn1.weight',\t(256,),\n     'block3.1.conv_bn1.bn.bias',\t(256,),\t 'layer3.1.bn1.bias',\t(256,),\n     'block3.1.conv_bn1.bn.running_mean',\t(256,),\t 'layer3.1.bn1.running_mean',\t(256,),\n     'block3.1.conv_bn1.bn.running_var',\t(256,),\t 'layer3.1.bn1.running_var',\t(256,),\n     'block3.1.conv_bn2.conv.weight',\t(256, 256, 3, 3),\t 'layer3.1.conv2.weight',\t(256, 256, 3, 3),\n     'block3.1.conv_bn2.bn.weight',\t(256,),\t 'layer3.1.bn2.weight',\t(256,),\n     'block3.1.conv_bn2.bn.bias',\t(256,),\t 'layer3.1.bn2.bias',\t(256,),\n     'block3.1.conv_bn2.bn.running_mean',\t(256,),\t 'layer3.1.bn2.running_mean',\t(256,),\n     'block3.1.conv_bn2.bn.running_var',\t(256,),\t 'layer3.1.bn2.running_var',\t(256,),\n     'block3.2.conv_bn1.conv.weight',\t(256, 256, 3, 3),\t 'layer3.2.conv1.weight',\t(256, 256, 3, 3),\n     'block3.2.conv_bn1.bn.weight',\t(256,),\t 'layer3.2.bn1.weight',\t(256,),\n     'block3.2.conv_bn1.bn.bias',\t(256,),\t 'layer3.2.bn1.bias',\t(256,),\n     'block3.2.conv_bn1.bn.running_mean',\t(256,),\t 'layer3.2.bn1.running_mean',\t(256,),\n     'block3.2.conv_bn1.bn.running_var',\t(256,),\t 'layer3.2.bn1.running_var',\t(256,),\n     'block3.2.conv_bn2.conv.weight',\t(256, 256, 3, 3),\t 'layer3.2.conv2.weight',\t(256, 256, 3, 3),\n     'block3.2.conv_bn2.bn.weight',\t(256,),\t 'layer3.2.bn2.weight',\t(256,),\n     'block3.2.conv_bn2.bn.bias',\t(256,),\t 'layer3.2.bn2.bias',\t(256,),\n     'block3.2.conv_bn2.bn.running_mean',\t(256,),\t 'layer3.2.bn2.running_mean',\t(256,),\n     'block3.2.conv_bn2.bn.running_var',\t(256,),\t 'layer3.2.bn2.running_var',\t(256,),\n     'block3.3.conv_bn1.conv.weight',\t(256, 256, 3, 3),\t 'layer3.3.conv1.weight',\t(256, 256, 3, 3),\n     'block3.3.conv_bn1.bn.weight',\t(256,),\t 'layer3.3.bn1.weight',\t(256,),\n     'block3.3.conv_bn1.bn.bias',\t(256,),\t 'layer3.3.bn1.bias',\t(256,),\n     'block3.3.conv_bn1.bn.running_mean',\t(256,),\t 'layer3.3.bn1.running_mean',\t(256,),\n     'block3.3.conv_bn1.bn.running_var',\t(256,),\t 'layer3.3.bn1.running_var',\t(256,),\n     'block3.3.conv_bn2.conv.weight',\t(256, 256, 3, 3),\t 'layer3.3.conv2.weight',\t(256, 256, 3, 3),\n     'block3.3.conv_bn2.bn.weight',\t(256,),\t 'layer3.3.bn2.weight',\t(256,),\n     'block3.3.conv_bn2.bn.bias',\t(256,),\t 'layer3.3.bn2.bias',\t(256,),\n     'block3.3.conv_bn2.bn.running_mean',\t(256,),\t 'layer3.3.bn2.running_mean',\t(256,),\n     'block3.3.conv_bn2.bn.running_var',\t(256,),\t 'layer3.3.bn2.running_var',\t(256,),\n     'block3.4.conv_bn1.conv.weight',\t(256, 256, 3, 3),\t 'layer3.4.conv1.weight',\t(256, 256, 3, 3),\n     'block3.4.conv_bn1.bn.weight',\t(256,),\t 'layer3.4.bn1.weight',\t(256,),\n     'block3.4.conv_bn1.bn.bias',\t(256,),\t 'layer3.4.bn1.bias',\t(256,),\n     'block3.4.conv_bn1.bn.running_mean',\t(256,),\t 'layer3.4.bn1.running_mean',\t(256,),\n     'block3.4.conv_bn1.bn.running_var',\t(256,),\t 'layer3.4.bn1.running_var',\t(256,),\n     'block3.4.conv_bn2.conv.weight',\t(256, 256, 3, 3),\t 'layer3.4.conv2.weight',\t(256, 256, 3, 3),\n     'block3.4.conv_bn2.bn.weight',\t(256,),\t 'layer3.4.bn2.weight',\t(256,),\n     'block3.4.conv_bn2.bn.bias',\t(256,),\t 'layer3.4.bn2.bias',\t(256,),\n     'block3.4.conv_bn2.bn.running_mean',\t(256,),\t 'layer3.4.bn2.running_mean',\t(256,),\n     'block3.4.conv_bn2.bn.running_var',\t(256,),\t 'layer3.4.bn2.running_var',\t(256,),\n     'block3.5.conv_bn1.conv.weight',\t(256, 256, 3, 3),\t 'layer3.5.conv1.weight',\t(256, 256, 3, 3),\n     'block3.5.conv_bn1.bn.weight',\t(256,),\t 'layer3.5.bn1.weight',\t(256,),\n     'block3.5.conv_bn1.bn.bias',\t(256,),\t 'layer3.5.bn1.bias',\t(256,),\n     'block3.5.conv_bn1.bn.running_mean',\t(256,),\t 'layer3.5.bn1.running_mean',\t(256,),\n     'block3.5.conv_bn1.bn.running_var',\t(256,),\t 'layer3.5.bn1.running_var',\t(256,),\n     'block3.5.conv_bn2.conv.weight',\t(256, 256, 3, 3),\t 'layer3.5.conv2.weight',\t(256, 256, 3, 3),\n     'block3.5.conv_bn2.bn.weight',\t(256,),\t 'layer3.5.bn2.weight',\t(256,),\n     'block3.5.conv_bn2.bn.bias',\t(256,),\t 'layer3.5.bn2.bias',\t(256,),\n     'block3.5.conv_bn2.bn.running_mean',\t(256,),\t 'layer3.5.bn2.running_mean',\t(256,),\n     'block3.5.conv_bn2.bn.running_var',\t(256,),\t 'layer3.5.bn2.running_var',\t(256,),\n     'block4.0.conv_bn1.conv.weight',\t(512, 256, 3, 3),\t 'layer4.0.conv1.weight',\t(512, 256, 3, 3),\n     'block4.0.conv_bn1.bn.weight',\t(512,),\t 'layer4.0.bn1.weight',\t(512,),\n     'block4.0.conv_bn1.bn.bias',\t(512,),\t 'layer4.0.bn1.bias',\t(512,),\n     'block4.0.conv_bn1.bn.running_mean',\t(512,),\t 'layer4.0.bn1.running_mean',\t(512,),\n     'block4.0.conv_bn1.bn.running_var',\t(512,),\t 'layer4.0.bn1.running_var',\t(512,),\n     'block4.0.conv_bn2.conv.weight',\t(512, 512, 3, 3),\t 'layer4.0.conv2.weight',\t(512, 512, 3, 3),\n     'block4.0.conv_bn2.bn.weight',\t(512,),\t 'layer4.0.bn2.weight',\t(512,),\n     'block4.0.conv_bn2.bn.bias',\t(512,),\t 'layer4.0.bn2.bias',\t(512,),\n     'block4.0.conv_bn2.bn.running_mean',\t(512,),\t 'layer4.0.bn2.running_mean',\t(512,),\n     'block4.0.conv_bn2.bn.running_var',\t(512,),\t 'layer4.0.bn2.running_var',\t(512,),\n     'block4.0.shortcut.conv.weight',\t(512, 256, 1, 1),\t 'layer4.0.downsample.0.weight',\t(512, 256, 1, 1),\n     'block4.0.shortcut.bn.weight',\t(512,),\t 'layer4.0.downsample.1.weight',\t(512,),\n     'block4.0.shortcut.bn.bias',\t(512,),\t 'layer4.0.downsample.1.bias',\t(512,),\n     'block4.0.shortcut.bn.running_mean',\t(512,),\t 'layer4.0.downsample.1.running_mean',\t(512,),\n     'block4.0.shortcut.bn.running_var',\t(512,),\t 'layer4.0.downsample.1.running_var',\t(512,),\n     'block4.1.conv_bn1.conv.weight',\t(512, 512, 3, 3),\t 'layer4.1.conv1.weight',\t(512, 512, 3, 3),\n     'block4.1.conv_bn1.bn.weight',\t(512,),\t 'layer4.1.bn1.weight',\t(512,),\n     'block4.1.conv_bn1.bn.bias',\t(512,),\t 'layer4.1.bn1.bias',\t(512,),\n     'block4.1.conv_bn1.bn.running_mean',\t(512,),\t 'layer4.1.bn1.running_mean',\t(512,),\n     'block4.1.conv_bn1.bn.running_var',\t(512,),\t 'layer4.1.bn1.running_var',\t(512,),\n     'block4.1.conv_bn2.conv.weight',\t(512, 512, 3, 3),\t 'layer4.1.conv2.weight',\t(512, 512, 3, 3),\n     'block4.1.conv_bn2.bn.weight',\t(512,),\t 'layer4.1.bn2.weight',\t(512,),\n     'block4.1.conv_bn2.bn.bias',\t(512,),\t 'layer4.1.bn2.bias',\t(512,),\n     'block4.1.conv_bn2.bn.running_mean',\t(512,),\t 'layer4.1.bn2.running_mean',\t(512,),\n     'block4.1.conv_bn2.bn.running_var',\t(512,),\t 'layer4.1.bn2.running_var',\t(512,),\n     'block4.2.conv_bn1.conv.weight',\t(512, 512, 3, 3),\t 'layer4.2.conv1.weight',\t(512, 512, 3, 3),\n     'block4.2.conv_bn1.bn.weight',\t(512,),\t 'layer4.2.bn1.weight',\t(512,),\n     'block4.2.conv_bn1.bn.bias',\t(512,),\t 'layer4.2.bn1.bias',\t(512,),\n     'block4.2.conv_bn1.bn.running_mean',\t(512,),\t 'layer4.2.bn1.running_mean',\t(512,),\n     'block4.2.conv_bn1.bn.running_var',\t(512,),\t 'layer4.2.bn1.running_var',\t(512,),\n     'block4.2.conv_bn2.conv.weight',\t(512, 512, 3, 3),\t 'layer4.2.conv2.weight',\t(512, 512, 3, 3),\n     'block4.2.conv_bn2.bn.weight',\t(512,),\t 'layer4.2.bn2.weight',\t(512,),\n     'block4.2.conv_bn2.bn.bias',\t(512,),\t 'layer4.2.bn2.bias',\t(512,),\n     'block4.2.conv_bn2.bn.running_mean',\t(512,),\t 'layer4.2.bn2.running_mean',\t(512,),\n     'block4.2.conv_bn2.bn.running_var',\t(512,),\t 'layer4.2.bn2.running_var',\t(512,),\n     'logit.weight',\t(1000, 512),\t 'fc.weight',\t(1000, 512),\n     'logit.bias',\t(1000,),\t 'fc.bias',\t(1000,),\n\n    ]\n\n    ###############################################################################\n    class ConvBn2d(nn.Module):\n\n        def __init__(self, in_channel, out_channel, kernel_size=3, padding=1, stride=1):\n            super(ConvBn2d, self).__init__()\n            self.conv = nn.Conv2d(in_channel, out_channel, kernel_size=kernel_size, padding=padding, stride=stride, bias=False)\n            self.bn   = nn.BatchNorm2d(out_channel, eps=1e-5)\n\n        def forward(self,x):\n            x = self.conv(x)\n            x = self.bn(x)\n            return x\n\n\n\n\n    #############  resnext50 pyramid feature net #######################################\n    # https:\/\/github.com\/Hsuxu\/ResNeXt\/blob\/master\/models.py\n    # https:\/\/github.com\/D-X-Y\/ResNeXt-DenseNet\/blob\/master\/models\/resnext.py\n    # https:\/\/github.com\/miraclewkf\/ResNeXt-PyTorch\/blob\/master\/resnext.py\n\n\n    # bottleneck type C\n    class BasicBlock(nn.Module):\n        def __init__(self, in_channel, channel, out_channel, stride=1, is_shortcut=False):\n            super(BasicBlock, self).__init__()\n            self.is_shortcut = is_shortcut\n\n            self.conv_bn1 = ConvBn2d(in_channel,    channel, kernel_size=3, padding=1, stride=stride)\n            self.conv_bn2 = ConvBn2d(   channel,out_channel, kernel_size=3, padding=1, stride=1)\n\n            if is_shortcut:\n                self.shortcut = ConvBn2d(in_channel, out_channel, kernel_size=1, padding=0, stride=stride)\n\n\n        def forward(self, x):\n            z = F.relu(self.conv_bn1(x),inplace=True)\n            z = self.conv_bn2(z)\n\n            if self.is_shortcut:\n                x = self.shortcut(x)\n\n            z += x\n            z = F.relu(z,inplace=True)\n            return z\n\n\n    #resnet18\n    class ResNet18(nn.Module):\n\n        def __init__(self, num_class=1000 ):\n            super(ResNet18, self).__init__()\n\n\n            self.block0  = nn.Sequential(\n                nn.Conv2d(3, 64, kernel_size=7, padding=3, stride=2, bias=False),\n                BatchNorm2d(64),\n                nn.ReLU(inplace=True),\n            )\n\n            self.block1  = nn.Sequential(\n                 nn.MaxPool2d(kernel_size=3, padding=1, stride=2),\n                 BasicBlock( 64, 64, 64, stride=1, is_shortcut=False,),\n              * [BasicBlock( 64, 64, 64, stride=1, is_shortcut=False,) for i in range(1,2)],\n            )\n            self.block2  = nn.Sequential(\n                 BasicBlock( 64,128,128, stride=2, is_shortcut=True, ),\n              * [BasicBlock(128,128,128, stride=1, is_shortcut=False,) for i in range(1,2)],\n            )\n            self.block3  = nn.Sequential(\n                 BasicBlock(128,256,256, stride=2, is_shortcut=True, ),\n              * [BasicBlock(256,256,256, stride=1, is_shortcut=False,) for i in range(1,2)],\n            )\n            self.block4 = nn.Sequential(\n                 BasicBlock(256,512,512, stride=2, is_shortcut=True, ),\n              * [BasicBlock(512,512,512, stride=1, is_shortcut=False,) for i in range(1,2)],\n            )\n            self.logit = nn.Linear(512,num_class)\n\n\n\n        def forward(self, x):\n            batch_size = len(x)\n\n            x = self.block0(x)\n            x = F.max_pool2d(x,kernel_size=3, padding=1, stride=2, ceil_mode=False)\n\n            x = self.block1(x)\n            x = self.block2(x)\n            x = self.block3(x)\n            x = self.block4(x)\n            x = F.adaptive_avg_pool2d(x,1).reshape(batch_size,-1)\n            logit = self.logit(x)\n            return logit\n\n\n    ####################################################################################################\n    def upsize(x,scale_factor=2):\n        #x = F.interpolate(x, size=e.shape[2:], mode='nearest')\n        x = F.interpolate(x, scale_factor=scale_factor, mode='nearest')\n        return x\n\n    class Swish(nn.Module):\n        def forward(self, x):\n            return x * torch.sigmoid(x)\n\n    class Decode(nn.Module):\n        def __init__(self, in_channel, out_channel):\n            super(Decode, self).__init__()\n\n            self.top = nn.Sequential(\n                nn.Conv2d(in_channel, out_channel\/\/2, kernel_size=3, stride=1, padding=1, bias=False),\n                BatchNorm2d( out_channel\/\/2),\n                nn.ReLU(inplace=True),\n                #nn.Dropout(0.1),\n\n                nn.Conv2d(out_channel\/\/2, out_channel\/\/2, kernel_size=3, stride=1, padding=1, bias=False),\n                BatchNorm2d(out_channel\/\/2),\n                nn.ReLU(inplace=True),\n                #nn.Dropout(0.1),\n\n                nn.Conv2d(out_channel\/\/2, out_channel, kernel_size=1, stride=1, padding=0, bias=False),\n                BatchNorm2d(out_channel),\n                nn.ReLU(inplace=True), #Swish(), #\n            )\n\n        def forward(self, x):\n            x = self.top(torch.cat(x, 1))\n            return x\n\n\n\n    class Net(nn.Module):\n\n        def load_pretrain(self, skip, is_print=True):\n            conversion=copy.copy(CONVERSION)\n            for i in range(0,len(conversion)-8,4):\n                conversion[i] = 'block.' + conversion[i][5:]\n            load_pretrain(self, skip, pretrain_file=PRETRAIN_FILE, conversion=conversion, is_print=is_print)\n\n        def __init__(self, num_class=5, drop_connect_rate=0.2):\n            super(Net, self).__init__()\n\n            e = ResNet18()\n            self.block = nn.ModuleList([\n               e.block0,\n               e.block1,\n               e.block2,\n               e.block3,\n               e.block4\n            ])\n            e = None  #dropped\n\n            self.decode1 =  Decode(512,     128)\n            self.decode2 =  Decode(128+256, 128)\n            self.decode3 =  Decode(128+128, 128)\n            self.decode4 =  Decode(128+ 64, 128)\n            self.decode5 =  Decode(128+ 64, 128)\n            self.logit = nn.Conv2d(128,num_class, kernel_size=1)\n\n        def forward(self, x):\n            batch_size,C,H,W = x.shape\n\n            #----------------------------------\n            backbone = []\n            for i in range( len(self.block)):\n                x = self.block[i](x)\n                #print(i, x.shape)\n\n                if i in [0,1,2,3,4]:\n                    backbone.append(x)\n\n            #----------------------------------\n            x = self.decode1([backbone[-1], ])                   #; print('d1',d1.size())\n            x = self.decode2([backbone[-2], upsize(x)])          #; print('d2',d2.size())\n            x = self.decode3([backbone[-3], upsize(x)])          #; print('d3',d3.size())\n            x = self.decode4([backbone[-4], upsize(x)])          #; print('d4',d4.size())\n            x = self.decode5([backbone[-5], upsize(x)])          #; print('d5',d5.size())\n\n            logit = self.logit(x)\n            logit = F.interpolate(logit, size=(H,W), mode='bilinear', align_corners=False)\n            return logit\n\n    class ResNet34(nn.Module):\n\n        def __init__(self, num_class=1000 ):\n            super(ResNet34, self).__init__()\n\n\n            self.block0  = nn.Sequential(\n                nn.Conv2d(3, 64, kernel_size=7, padding=3, stride=2, bias=False),\n                BatchNorm2d(64),\n                nn.ReLU(inplace=True),\n            )\n            self.block1  = nn.Sequential(\n                 nn.MaxPool2d(kernel_size=3, padding=1, stride=2),\n                 BasicBlock( 64, 64, 64, stride=1, is_shortcut=False,),\n              * [BasicBlock( 64, 64, 64, stride=1, is_shortcut=False,) for i in range(1,3)],\n            )\n            self.block2  = nn.Sequential(\n                 BasicBlock( 64,128,128, stride=2, is_shortcut=True, ),\n              * [BasicBlock(128,128,128, stride=1, is_shortcut=False,) for i in range(1,4)],\n            )\n            self.block3  = nn.Sequential(\n                 BasicBlock(128,256,256, stride=2, is_shortcut=True, ),\n              * [BasicBlock(256,256,256, stride=1, is_shortcut=False,) for i in range(1,6)],\n            )\n            self.block4 = nn.Sequential(\n                 BasicBlock(256,512,512, stride=2, is_shortcut=True, ),\n              * [BasicBlock(512,512,512, stride=1, is_shortcut=False,) for i in range(1,3)],\n            )\n            self.logit = nn.Linear(512,num_class)\n\n\n\n        def forward(self, x):\n            batch_size = len(x)\n\n            x = self.block0(x)\n            x = self.block1(x)\n            x = self.block2(x)\n            x = self.block3(x)\n            x = self.block4(x)\n            x = F.adaptive_avg_pool2d(x,1).reshape(batch_size,-1)\n            logit = self.logit(x)\n            return logit\n\n\n    class Resnet34_classification(nn.Module):\n        def __init__(self,num_class=4):\n            super(Resnet34_classification, self).__init__()\n            e = ResNet34()\n            self.block = nn.ModuleList([\n                e.block0,\n                e.block1,\n                e.block2,\n                e.block3,\n                e.block4,\n            ])\n            e = None  #dropped\n            self.feature = nn.Conv2d(512,32, kernel_size=1) #dummy conv for dim reduction\n            self.logit = nn.Conv2d(32,num_class, kernel_size=1)\n\n        def forward(self, x):\n            batch_size,C,H,W = x.shape\n\n            for i in range( len(self.block)):\n                x = self.block[i](x)\n                #print(i, x.shape)\n\n            x = F.dropout(x,0.5,training=self.training)\n            x = F.adaptive_avg_pool2d(x, 1)\n            x = self.feature(x)\n            logit = self.logit(x)\n            return logit\n\n    model_classification = Resnet34_classification()\n    model_classification.load_state_dict(torch.load(\"..\/input\/hengs-models-20190910\/00007500_model.pth\",\n                                                    map_location=lambda storage, loc: storage), strict=True)\n\n    return model_classification","15160dfc":"model_classification = load_hengs_clf_model()","546e80c4":"def get_hengs_clf_model_preds(model_classification):\n    # Dataset setup\n    class TestDataset(Dataset):\n        '''Dataset for test prediction'''\n        def __init__(self, root, df, mean, std):\n            self.root = root\n            df['ImageId'] = df['ImageId_ClassId'].apply(lambda x: x.split('_')[0])\n            self.fnames = df['ImageId'].unique().tolist()\n            self.num_samples = len(self.fnames)\n            self.transform = Compose(\n                [\n                    Normalize(mean=mean, std=std, p=1),\n                    ToTensor(),\n                ]\n            )\n\n        def __getitem__(self, idx):\n            fname = self.fnames[idx]\n            path = os.path.join(self.root, fname)\n            image = cv2.imread(path)\n            images = self.transform(image=image)[\"image\"]\n            return fname, images\n\n        def __len__(self):\n            return self.num_samples\n\n    sample_submission_path = '..\/input\/severstal-steel-defect-detection\/sample_submission.csv'\n    test_data_folder = \"..\/input\/severstal-steel-defect-detection\/test_images\"\n\n    # hyperparameters\n    batch_size = 1\n\n    # mean and std\n    mean = (0.485, 0.456, 0.406)\n    std = (0.229, 0.224, 0.225)\n\n    df = pd.read_csv(sample_submission_path)\n\n    # dataloader\n    testset = DataLoader(\n        TestDataset(test_data_folder, df, mean, std),\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=0,\n        pin_memory=True\n    )\n\n\n    # useful functions for setting up inference\n\n    def sharpen(p,t=0.5):\n            if t!=0:\n                return p**t\n            else:\n                return p\n\n    def get_classification_preds(net,test_loader):\n        test_probability_label = []\n        test_id   = []\n\n        net = net.cuda()\n        for t, (fnames, images) in enumerate(tqdm(test_loader)):\n            batch_size,C,H,W = images.shape\n            images = images.cuda()\n\n            with torch.no_grad():\n                net.eval()\n\n                num_augment = 0\n                if 1: #  null\n                    logit =  net(images)\n                    probability = torch.sigmoid(logit)\n\n                    probability_label = sharpen(probability,0)\n                    num_augment+=1\n\n                if 'flip_lr' in augment:\n                    logit = net(torch.flip(images,dims=[3]))\n                    probability  = torch.sigmoid(logit)\n\n                    probability_label += sharpen(probability)\n                    num_augment+=1\n\n                if 'flip_ud' in augment:\n                    logit = net(torch.flip(images,dims=[2]))\n                    probability = torch.sigmoid(logit)\n\n                    probability_label += sharpen(probability)\n                    num_augment+=1\n\n                probability_label = probability_label\/num_augment\n\n            probability_label = probability_label.data.cpu().numpy()\n\n            test_probability_label.append(probability_label)\n            test_id.extend([i for i in fnames])\n\n\n        test_probability_label = np.concatenate(test_probability_label)\n        return test_probability_label, test_id\n\n    # threshold for classification\n    threshold_label = [0.50,0.50,0.50,0.50,]\n\n    augment = ['null'] #['null', 'flip_lr','flip_ud','5crop'] # ['null', 'flip_lr','flip_ud'] # # #\n\n    # Get prediction for classification model\n\n    probability_label, image_id = get_classification_preds(model_classification, testset)\n    predict_label = probability_label>np.array(threshold_label).reshape(1,4,1,1)\n\n    image_id_class_id = []\n    encoded_pixel = []\n    for b in range(len(image_id)):\n        for c in range(4):\n            image_id_class_id.append(image_id[b]+'_%d'%(c+1))\n            if predict_label[b,c]==0:\n                rle=''\n            else:\n                rle ='1 1'\n            encoded_pixel.append(rle)\n\n    df_classification = pd.DataFrame(zip(image_id_class_id, encoded_pixel), columns=['ImageId_ClassId', 'EncodedPixels'])\n\n    return df_classification\n","77e77a29":"df_classification = get_hengs_clf_model_preds(model_classification)","7544317c":"del model_classification\ntorch.cuda.empty_cache()\ngc.collect()","351c401b":"df_classification.head()","fe27f18a":"def load_denis_gontcharov_segmentation_model():\n    # https:\/\/www.kaggle.com\/gontcharovd\/unet-pytorch-inference-kernel-extended-0-89648\n    #Model from https:\/\/www.kaggle.com\/gontcharovd\/unet-pytorch-inference-kernel-extended-0-89648\/comments\n    # ckpt_path = \"..\/input\/resnetmodels\/resnet18_20_epochs.pth\"\n    # ckpt_path = \"..\/input\/senetmodels\/senet50_20_epochs.pth\"\n    ckpt_path = \"..\/input\/senetmodels\/senext50_30_epochs_high_threshold.pth\"\n    device = torch.device(\"cuda\")\n    # change the encoder name in the Unet() call.\n    model_segmentation = Unet('se_resnext50_32x4d', encoder_weights=None, classes=4, activation=None)\n    model_segmentation.to(device)\n    model_segmentation.eval()\n    state = torch.load(ckpt_path, map_location=lambda storage, loc: storage)\n    model_segmentation.load_state_dict(state[\"state_dict\"])\n    model_segmentation = model_segmentation.cuda()\n    \n    return model_segmentation","4a513ce0":"denis_gontcharov_model = load_denis_gontcharov_segmentation_model()","45d8d1af":"# def load_ilya_model():\n#     device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n\n#     models_folder = Path(\"\/kaggle\/input\/segmentation-model\")\n# #     empty_model_folder = Path(\"\/kaggle\/input\/empty-model\")\n\n#     class PretrainedModel(torch.nn.Module):\n#         def __init__(self, output_features, pretrained=True):\n#             super().__init__()\n#             model = torchvision.models.resnet34(pretrained=pretrained)\n#             num_ftrs = model.fc.in_features\n#             model.fc = torch.nn.Linear(num_ftrs, output_features)\n#             self.model = model\n\n#         def forward(self, x):\n#             return self.model(x)\n# #     empty_model = PretrainedModel(2, False)\n    \n# #     pretrained_model_name = \"best_model.pt\"\n# #     empty_model.load_state_dict(torch.load(\n# #         empty_model_folder \/ pretrained_model_name,\n# #         map_location=torch.device(\"cpu\")\n# #     ))\n\n# #     empty_model.to(device)\n# #     empty_model.eval();\n    \n#     segmentation_model = PretrainedUNet(\n#     in_channels=3,\n#     out_channels=4, \n#     batch_norm=True, \n#     upscale_mode=\"bilinear\",\n#     pretrained=False\n#     )\n    \n#     pretrained_model_name = \"severstal-unet-v51.pt\"\n\n#     if pretrained_model_name is not None:\n#         segmentation_model.load_state_dict(torch.load(\n#             models_folder \/ pretrained_model_name,\n#             map_location=torch.device(\"cpu\")\n#         ))\n\n#     segmentation_model.to(device)\n#     segmentation_model.eval();\n    \n#     return segmentation_model.cuda()\n\n# ilya_model = load_ilya_model()","2ce38e7a":"nfolds = 1#4\nbs = 2\nn_cls = 4\nnoise_th = 2000 #predicted masks must be larger than noise_th\nTEST = '..\/input\/severstal-steel-defect-detection\/test_images\/'\nBASE = '..\/input\/severstal-fast-ai-256x256-crops\/'\n\ntorch.backends.cudnn.benchmark = True\n\n\n# def get_fast_ai_learn():\n\nfrom fastai.vision.learner import create_head, cnn_config, num_features_model, create_head\nfrom fastai.callbacks.hooks import model_sizes, hook_outputs, dummy_eval, Hook, _hook_inner\nfrom fastai.vision.models.unet import _get_sfs_idxs, UnetBlock\n\nclass Hcolumns(nn.Module):\n    def __init__(self, hooks:Collection[Hook], nc:Collection[int]=None):\n        super(Hcolumns,self).__init__()\n        self.hooks = hooks\n        self.n = len(self.hooks)\n        self.factorization = None \n        if nc is not None:\n            self.factorization = nn.ModuleList()\n            for i in range(self.n):\n                self.factorization.append(nn.Sequential(\n                    conv2d(nc[i],nc[-1],3,padding=1,bias=True),\n                    conv2d(nc[-1],nc[-1],3,padding=1,bias=True)))\n                #self.factorization.append(conv2d(nc[i],nc[-1],3,padding=1,bias=True))\n        \n    def forward(self, x:Tensor):\n        n = len(self.hooks)\n        out = [F.interpolate(self.hooks[i].stored if self.factorization is None\n            else self.factorization[i](self.hooks[i].stored), scale_factor=2**(self.n-i),\n            mode='bilinear',align_corners=False) for i in range(self.n)] + [x]\n        return torch.cat(out, dim=1)\n\nclass DynamicUnet_Hcolumns(SequentialEx):\n    \"Create a U-Net from a given architecture.\"\n    def __init__(self, encoder:nn.Module, n_classes:int, blur:bool=False, blur_final=True, \n                 self_attention:bool=False,\n                 y_range:Optional[Tuple[float,float]]=None,\n                 last_cross:bool=True, bottle:bool=False, **kwargs):\n        imsize = (256,256)\n        sfs_szs = model_sizes(encoder, size=imsize)\n        sfs_idxs = list(reversed(_get_sfs_idxs(sfs_szs)))\n        self.sfs = hook_outputs([encoder[i] for i in sfs_idxs])\n        x = dummy_eval(encoder, imsize).detach()\n\n        ni = sfs_szs[-1][1]\n        middle_conv = nn.Sequential(conv_layer(ni, ni*2, **kwargs),\n                                    conv_layer(ni*2, ni, **kwargs)).eval()\n        x = middle_conv(x)\n        layers = [encoder, batchnorm_2d(ni), nn.ReLU(), middle_conv]\n\n        self.hc_hooks = [Hook(layers[-1], _hook_inner, detach=False)]\n        hc_c = [x.shape[1]]\n        \n        for i,idx in enumerate(sfs_idxs):\n            not_final = i!=len(sfs_idxs)-1\n            up_in_c, x_in_c = int(x.shape[1]), int(sfs_szs[idx][1])\n            do_blur = blur and (not_final or blur_final)\n            sa = self_attention and (i==len(sfs_idxs)-3)\n            unet_block = UnetBlock(up_in_c, x_in_c, self.sfs[i], final_div=not_final, \n                blur=blur, self_attention=sa, **kwargs).eval()\n            layers.append(unet_block)\n            x = unet_block(x)\n            self.hc_hooks.append(Hook(layers[-1], _hook_inner, detach=False))\n            hc_c.append(x.shape[1])\n\n        ni = x.shape[1]\n        if imsize != sfs_szs[0][-2:]: layers.append(PixelShuffle_ICNR(ni, **kwargs))\n        if last_cross:\n            layers.append(MergeLayer(dense=True))\n            ni += in_channels(encoder)\n            layers.append(res_block(ni, bottle=bottle, **kwargs))\n        hc_c.append(ni)\n        layers.append(Hcolumns(self.hc_hooks, hc_c))\n        layers += [conv_layer(ni*len(hc_c), n_classes, ks=1, use_activ=False, **kwargs)]\n        if y_range is not None: layers.append(SigmoidRange(*y_range))\n        super().__init__(*layers)\n\n    def __del__(self):\n        if hasattr(self, \"sfs\"): self.sfs.remove()\n            \ndef unet_learner(data:DataBunch, arch:Callable, pretrained:bool=True, blur_final:bool=True,\n        norm_type:Optional[NormType]=NormType, split_on:Optional[SplitFuncOrIdxList]=None, \n        blur:bool=False, self_attention:bool=False, y_range:Optional[Tuple[float,float]]=None, \n        last_cross:bool=True, bottle:bool=False, cut:Union[int,Callable]=None, \n        hypercolumns=True, **learn_kwargs:Any)->Learner:\n    \"Build Unet learner from `data` and `arch`.\"\n    meta = cnn_config(arch)\n    body = create_body(arch, pretrained, cut)\n    M = DynamicUnet_Hcolumns if hypercolumns else DynamicUnet\n    model = to_device(M(body, n_classes=data.c, blur=blur, blur_final=blur_final,\n        self_attention=self_attention, y_range=y_range, norm_type=norm_type, \n        last_cross=last_cross, bottle=bottle), data.device)\n    learn = Learner(data, model, **learn_kwargs)\n    learn.split(ifnone(split_on, meta['split']))\n    if pretrained: learn.freeze()\n    apply_init(model[2], nn.init.kaiming_normal_)\n    return learn\nclass SegmentationLabelList(SegmentationLabelList):\n    def open(self, fn): return open_mask(fn, div=True)\n    \nclass SegmentationItemList(SegmentationItemList):\n    _label_cls = SegmentationLabelList\n\n# Setting transformations on masks to False on test set\ndef transform(self, tfms:Optional[Tuple[TfmList,TfmList]]=(None,None), **kwargs):\n    if not tfms: tfms=(None,None)\n    assert is_listy(tfms) and len(tfms) == 2\n    self.train.transform(tfms[0], **kwargs)\n    self.valid.transform(tfms[1], **kwargs)\n    kwargs['tfm_y'] = False # Test data has no labels\n    if self.test: self.test.transform(tfms[1], **kwargs)\n    return self\nfastai.data_block.ItemLists.transform = transform\n\ndef open_mask(fn:PathOrStr, div:bool=True, convert_mode:str='L', cls:type=ImageSegment,\n        after_open:Callable=None)->ImageSegment:\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", UserWarning)\n        x = PIL.Image.open(fn).convert(convert_mode)\n    if after_open: x = after_open(x)\n    x = pil2tensor(x,np.float32)\n    return cls(x)\n\n\ndef get_fast_ai_learn():\n    stats = ([0.396,0.396,0.396], [0.179,0.179,0.179])\n    #check https:\/\/www.kaggle.com\/iafoss\/256x256-images-with-defects for stats\n\n    data = (SegmentationItemList.from_folder(TEST)\n            .split_by_idx([0])\n            .label_from_func(lambda x : str(x), classes=[0,1,2,3,4])\n            .add_test(Path(TEST).ls(), label=None)\n            .databunch(path=Path('.'), bs=bs)\n            .normalize(stats))\n\n\n    learn = unet_learner(data, models.resnet34, pretrained=False)\n    learn.model.load_state_dict(torch.load(Path(BASE)\/f'models\/fold0.pth')['model'])\n    learn.model.eval()\n    return learn\n\nlearn = get_fast_ai_learn()\n\nn_cls = 4\ntta = True\nnoise_th = 2000 #predicted masks must be larger than noise_th\n\ndef post_proc(yp):\n    yp = np.argmax(yp, axis=-1)\n    for i in range(n_cls):\n        idxs = yp == i+1\n        if idxs.sum() < noise_th: \n            yp[idxs] = 0\n    return yp\n\ndef argmax_mask_to_binary_masks(mask):\n    return [(mask == i).astype(np.int8)for i in range(1,n_cls+1)]\n\n# def pred_batch_fast_ai(x):\n#     x = x.cuda()\n#     py = torch.softmax(learn.model(x),dim=1).permute(0,2,3,1).detach()\n#     if tta:\n#         flips = [[-1],[-2],[-2,-1]]\n#         for f in flips:\n#             py += torch.softmax(torch.flip(learn.model(torch.flip(x,f)),f),dim=1).permute(0,2,3,1).detach()\n#         py \/= len(flips) + 1\n#     py = py.cpu().numpy() \n#     argmax_masks = [post_proc(yp) for yp in py]\n#     binary_masks = np.array([argmax_mask_to_binary_masks(argmax_mask) for argmax_mask in argmax_masks])\n#     return binary_masks\n\ndef pred_batch_fast_ai(x):\n    x = x.cuda()\n    py = torch.softmax(learn.model(x),dim=1).detach()\n    if tta:\n        flips = [[-1],[-2],[-2,-1]]\n        for f in flips:\n            py += torch.softmax(torch.flip(learn.model(torch.flip(x,f)),f),dim=1).detach()\n        py \/= len(flips) + 1\n        \n#     print(py[:,1:].cpu().numpy().shape)\n    return py[:,1:]","ae7dd20b":"unet_se_resnext50_32x4d = \\\n    load('\/kaggle\/input\/severstalmodels\/unet_se_resnext50_32x4d.pth').cuda()\nunet_mobilenet2 = load('\/kaggle\/input\/severstalmodels\/unet_mobilenet2.pth').cuda()\nunet_resnet34 = load('\/kaggle\/input\/severstalmodels\/unet_resnet34.pth').cuda()","29b1efe9":"class Model:\n    def __init__(self, models, weights=None):\n        self.models = models\n        self.weights = weights or np.ones(len(models))\n    \n    def __call__(self, x):\n        res = []\n        x = x.cuda()\n        with torch.no_grad():\n            for model, weight in zip(self.models, self.weights):\n                res.append(torch.sigmoid(model(x))*weight)\n        res = torch.stack(res)\n        return torch.sum(res, dim=0) \/sum(self.weights)\n\nmodel = Model([unet_se_resnext50_32x4d, unet_mobilenet2, unet_resnet34,\n               denis_gontcharov_model])","0089159c":"def create_transforms(additional):\n    res = list(additional)\n    # add necessary transformations\n    res.extend([\n        ChannelTranspose()\n    ])\n    res = A.Compose(res)\n    return res\n\nimg_folder = '\/kaggle\/input\/severstal-steel-defect-detection\/test_images'\nbatch_size = 2\nnum_workers = 0\n# [0.388,0.390,0.394], [0.178,0.181,0.175]\n# Different transforms for TTA wrapper\ntransforms = [\n    [A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))],\n    [A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),A.HorizontalFlip(p=1)],\n    [A.Normalize(mean=(0.388,0.390,0.394), std=(0.178,0.181,0.175))],#dataset for fast ai\n]\n\ntransforms = [create_transforms(t) for t in transforms]\ndatasets = [TtaWrap(ImageDataset(img_folder=img_folder, transforms=t), tfms=t) for t in transforms]\nloaders = [DataLoader(d, num_workers=num_workers, batch_size=batch_size, shuffle=False) for d in datasets]","624603c7":"thresholds = [0.5, 0.5, 0.5, 0.5]\nmin_area = [600, 600, 1000, 2000]\n\nres = []\n# Iterate over all TTA loaders\ntotal = len(datasets[0])\/\/batch_size\nfor loaders_batch in tqdm_notebook(zip(*loaders), total=total):\n\n    ############## Get preds with tta for fasi ai model ################# \n    fast_ai_batch = loaders_batch[-1]\n    fast_ai_features = fast_ai_batch['features'].cuda()\n    fast_ai_pred = pred_batch_fast_ai(fast_ai_features).cpu().numpy()\n    \n    ############## Get preds with tta for ensemble ################# \n    preds = []\n    \n    loaders_batch = loaders_batch[:-1]\n    for i, batch in enumerate(loaders_batch):\n        features = batch['features'].cuda()\n        p = model(features)\n        # inverse operations for TTA\n        p = datasets[i].inverse(p)\n        preds.append(p)\n    # TTA mean\n    preds = torch.stack(preds)\n    preds = torch.mean(preds, dim=0)\n    preds = preds.detach().cpu().numpy()\n    \n    ############## Combine preds with weights ################# \n    preds = np.average([preds, fast_ai_pred], weights=(4.0, 1.0), axis=0)\n\n    \n    # Batch post processing\n    for p, file in zip(preds, loaders_batch[0]['image_file']):\n        file = os.path.basename(file)\n        # Image postprocessing\n        for i in range(4):\n            p_channel = p[i]\n            imageid_classid = file+'_'+str(i+1)\n            p_channel = (p_channel>thresholds[i]).astype(np.uint8)\n            if p_channel.sum() < min_area[i]:\n                p_channel = np.zeros(p_channel.shape, dtype=p_channel.dtype)\n\n            res.append({\n                'ImageId_ClassId': imageid_classid,\n                'EncodedPixels': mask2rle(p_channel)\n            })        \t","51bbc4b3":"df = pd.DataFrame(res)\ndf = df.fillna('')","53312fac":"(df['EncodedPixels'] == '').mean()","4afde866":"(df_classification['EncodedPixels'] == '').mean()","86c35953":"((df_classification['EncodedPixels'] == '')&(df['EncodedPixels'] == '')).mean()","506128e2":"df.loc[df_classification['EncodedPixels'] == '', 'EncodedPixels'] = ''","276f3f8f":"# a = pd.read_csv('submission.csv')\n# a[~a['EncodedPixels'].isnull()]","3ed37b74":"df.to_csv('submission.csv', index=False)","02cf6999":"time_end = time.time()\nprint((time_end - time_start))","5bb462c3":"# df['Image'] = df['ImageId_ClassId'].map(lambda x: x.split('_')[0])\n# df['Class'] = df['ImageId_ClassId'].map(lambda x: x.split('_')[1])\n# df['empty'] = df['EncodedPixels'].map(lambda x: not x)\n# df[df['empty'] == False]['Class'].value_counts()","6afe12a1":"# %matplotlib inline\n\n# df = pd.read_csv('submission.csv')[:40]\n# df['Image'] = df['ImageId_ClassId'].map(lambda x: x.split('_')[0])\n# df['Class'] = df['ImageId_ClassId'].map(lambda x: x.split('_')[1])\n\n# for row in df.itertuples():\n#     img_path = os.path.join(img_folder, row.Image)\n#     img = cv2.imread(img_path)\n#     mask = rle2mask(row.EncodedPixels, (1600, 256)) \\\n#         if isinstance(row.EncodedPixels, str) else np.zeros((256, 1600))\n#     if mask.sum() == 0:\n#         continue\n    \n#     fig, axes = plt.subplots(1, 2, figsize=(20, 60))\n#     axes[0].imshow(img\/255)\n#     axes[1].imshow(mask*60)\n#     axes[0].set_title(row.Image)\n#     axes[1].set_title(row.Class)\n#     plt.show()","932b329f":"### Create TTA transforms, datasets, loaders","9a2c86ea":"### Models' mean aggregator","61e5c60f":"### Loaders' mean aggregator","4ff7644b":"### Get heng's classification","53806200":"Approach descripton:\n\n1. Segmentation via 3 Unet networks. The predictions are being averaged. \n\n2. Thresholding and removeing small areas. This method gives 0.90672 on public LB.\n\n**Improving**:\n\n1. As many participations have seen, that is the key to remove false positives from your predictions.\n\n2. To cope with that, a classification network may be used. \n\n3. Heng CherKeng posted a classifier here: https:\/\/www.kaggle.com\/c\/severstal-steel-defect-detection\/discussion\/106462#latest-634450 resent34_cls_01, **if you remove false positives with it you should get 0.9117 on LB**","a453259f":"As the competition does not allow commit with the kernel that uses internet connection, we use offline installation","3881f2b9":"About the libraries:\n\n1. [MLComp](https:\/\/github.com\/catalyst-team\/mlcomp) is a distributed DAG  (Directed acyclic graph)  framework for machine learning with UI. It helps to train, manipulate, and visualize. All models in this kernel were trained offline via MLComp + Catalyst libraries. \n\nYou can control an execution process via Web-site\n\nDags\n![Dags](https:\/\/github.com\/catalyst-team\/mlcomp\/blob\/master\/docs\/imgs\/dags.png?raw=true)\n\nComputers\n![Computers](https:\/\/github.com\/catalyst-team\/mlcomp\/blob\/master\/docs\/imgs\/computers.png?raw=true)\n\nReports\n![Reports](https:\/\/github.com\/catalyst-team\/mlcomp\/blob\/master\/docs\/imgs\/reports.png?raw=true)\n\nCode\n![Code](https:\/\/github.com\/catalyst-team\/mlcomp\/blob\/master\/docs\/imgs\/code.png?raw=true)\n\nPlease follow [the web site](https:\/\/github.com\/catalyst-team\/mlcomp) to get the details.\n\nhttps:\/\/github.com\/catalyst-team\/mlcomp\n\n2. Catalys: High-level utils for PyTorch DL & RL research. It was developed with a focus on reproducibility, fast experimentation and code\/ideas reusing. Being able to research\/develop something new, rather then write another regular train loop. Break the cycle - use the Catalyst!\n\nhttps:\/\/github.com\/catalyst-team\/catalyst\n\nDocs and examples\n- Detailed [classification tutorial](https:\/\/github.com\/catalyst-team\/catalyst\/blob\/master\/examples\/notebooks\/classification-tutorial.ipynb) [![Open In Colab](https:\/\/colab.research.google.com\/assets\/colab-badge.svg)](https:\/\/colab.research.google.com\/github\/catalyst-team\/catalyst\/blob\/master\/examples\/notebooks\/classification-tutorial.ipynb)\n- Comprehensive [classification pipeline](https:\/\/github.com\/catalyst-team\/classification).\n\nAPI documentation and an overview of the library can be found here\n[![Docs](https:\/\/img.shields.io\/badge\/dynamic\/json.svg?label=docs&url=https%3A%2F%2Fpypi.org%2Fpypi%2Fcatalyst%2Fjson&query=%24.info.version&colorB=brightgreen&prefix=v)](https:\/\/catalyst-team.github.io\/catalyst\/index.html)","1a9d0ae6":"This kernel demonstrates:\n\n1. Results of training models with [the training kernel](https:\/\/www.kaggle.com\/lightforever\/severstal-mlcomp-catalyst-train-0-90672-offline) and achieves 0.90672 score on public LB\n\n2. Useful code in MLComp library: TtaWrapp, ImageDataset, ChannelTranspose, rle utilities\n\n3. Output statistics and basic visualization","a66d9bfc":"### Install MLComp library(offline version):","9ae19342":"## Delete false positives","697aa1b2":"### Visualization","edce94b4":"Save predictions","ffc6ec6f":"![MLComp](https:\/\/raw.githubusercontent.com\/catalyst-team\/catalyst-pics\/master\/pics\/MLcomp.png)\n![Catalyst](https:\/\/raw.githubusercontent.com\/catalyst-team\/catalyst-pics\/master\/pics\/catalyst_logo.png)","662ada06":"Histogram of predictions"}}