{"cell_type":{"57eb49d8":"code","f6425383":"code","497dbb57":"code","83df2e9c":"code","2c539642":"code","0a23029c":"code","310697a6":"code","fdbbf24f":"code","af200c9c":"code","13199a82":"code","a772376c":"code","b7ecf394":"code","ab80a0d4":"code","00af165c":"code","870af2c1":"code","6a157254":"code","f95a4d37":"code","ca0e4f93":"code","f1c5f45f":"code","ea15a5e1":"code","52d89169":"code","a573f140":"code","f32ba7ec":"code","334bfd57":"code","52dcab5a":"code","9a48f7f8":"code","eb75b866":"code","72e60d9d":"code","e03ca666":"code","b019b7fb":"code","0b69f6e4":"code","8021d6e5":"code","c00eabbd":"code","92e10a36":"code","e6555437":"code","22504395":"code","8c315780":"code","9b64747d":"code","d8d09b27":"code","6792edd8":"code","4786d001":"code","47f95f43":"code","ec581530":"code","06202caf":"markdown","872b97af":"markdown","d8373c18":"markdown","a2eb59b3":"markdown","ef98bd18":"markdown"},"source":{"57eb49d8":"import numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom sklearn.utils import shuffle\nfrom sklearn.utils import class_weight\nfrom sklearn.preprocessing import minmax_scale\n\nimport random\nimport cv2\nfrom imgaug import augmenters as iaa\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Input, BatchNormalization, GlobalAveragePooling2D\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.experimental import CosineDecay\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.applications import EfficientNetB3, ResNet50\nfrom tensorflow.keras.layers.experimental.preprocessing import RandomCrop,CenterCrop, RandomRotation","f6425383":"label = pd.read_csv('..\/input\/clothing-dataset-full\/images.csv')","497dbb57":"labels = list(label['label'].unique())","83df2e9c":"labels","2c539642":"path = '..\/input\/clothing-dataset-full\/images_compressed\/'\ny = pd.get_dummies(label['label'])\nx = pd.DataFrame()\nx['image_id'] = label['image'] + '.jpg'\n#x['label'] = label['label'] \nx['filepath'] = path+ label['image'] + '.jpg'","0a23029c":"file_list = os.listdir('..\/input\/clothing-dataset-full\/images_compressed')","310697a6":"x = list(x['filepath'].values)","fdbbf24f":"for idx,tmp in enumerate(x):\n    if (tmp in file_list):\n        print('False in ' + str(idx))","af200c9c":"path = '..\/input\/clothing-dataset-full\/images_compressed\/'\ny = pd.get_dummies(label['label'])\nx = pd.DataFrame()\nx['image_id'] = label['image'] + '.jpg'\n#x['label'] = label['label'] \nx['filepath'] = path+ label['image'] + '.jpg'","13199a82":"x = list(x['filepath'].values)\ny = list(y.values)\n\n","a772376c":"delete = [533,702,859,1659,1759]\nfor de in delete:\n    del y[de]\n    del x[de]\n    ","b7ecf394":"# batch_size = 32\n# training_data = tf.data.Dataset.from_tensor_slices((x,y))\n# #validation_data = tf.data.Dataset.from_tensor_slices((validation_df.filepath.values, validation_df.label.values))\n# def load_image_and_label_from_path(image_path, label):\n#     img = tf.io.read_file(image_path)\n#     img = tf.image.decode_jpeg(img, channels=3)\n#     img = layers.experimental.preprocessing.Rescaling(1.0 \/ 255)(img)\n#     return img, label\n\n\n# training_data = training_data.map(load_image_and_label_from_path)\n# # training_data_batches = training_data.shuffle(buffer_size=1000).batch(batch_size)","ab80a0d4":"img_size = 224\nimg_shape = (img_size,img_size,3)\nbatch_size = 32\nepochs = 20\ndropout_rate = 0.5\nnum_of_predict = len(y[0])\nlen_data = 4000","00af165c":"train_img = []\nfor idx,img in enumerate(x):\n    try:\n        tmp = cv2.imread(img\n                                    , cv2.IMREAD_COLOR)\n        tmp = cv2.resize(tmp,dsize=(img_size,img_size),\n                                interpolation=cv2.INTER_AREA)\n        train_img.append(tmp)\n    except Exception as e:\n        print(str(e)+str(idx))\ntrain_img = np.array(train_img)\n\n# delete = [533,702,859,1659,1759]\n# for de in delete:\n#     del y[de]","870af2c1":"# delete = [533,702,859,1659,1759]\n# for de in delete:\n#     del y[de]","6a157254":"print(len(train_img),len(y))","f95a4d37":"train_img = np.array(train_img)\/255.\ny = np.array(y)","ca0e4f93":"train_x = train_img[:4000]\nval_x = train_img[4000:]\ntrain_y = y[:4000]\nval_y = y[4000:]","f1c5f45f":"del train_img\ndel y","ea15a5e1":"# ResNet50 = ResNet50(\n#         include_top=False, weights='imagenet',\n#         input_shape=img_shape, classes=13\n#     )\n\n# inputs = Input(shape=img_shape)\n# ResNet50 = ResNet50(inputs) \n# pooling = layers.GlobalAveragePooling2D()(ResNet50) \n# dropout = layers.Dropout(dropout_rate)(pooling) #dropout\n# outputs = Dense(len(y[0]), activation=\"softmax\")(dropout)\n# model = Model(inputs=inputs, outputs=outputs)","52d89169":"data_augmentation_layers = tf.keras.Sequential(\n    [\n        layers.experimental.preprocessing.RandomCrop(height=img_size, width=img_size),\n        layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n        layers.experimental.preprocessing.RandomRotation(0.25),\n        layers.experimental.preprocessing.RandomZoom((-0.2, 0)),\n        layers.experimental.preprocessing.RandomContrast((0.2,0.2))\n    ]\n)","a573f140":"efficientnet = EfficientNetB3(weights=\"..\/input\/keras-efficientnetb3-no-top-weights\/efficientnetb3_notop.h5\",\n                              include_top=False, \n                              input_shape=img_shape, \n                              drop_connect_rate=dropout_rate)\n\ninputs = Input(shape=img_shape)\n#augmented = data_augmentation_layers(inputs) #Augmentation\nefficientnet = efficientnet(inputs) #efficientnet\npooling = layers.GlobalAveragePooling2D()(efficientnet) #globalaveragepooling\ndropout = layers.Dropout(dropout_rate)(pooling) #dropout\noutputs = Dense(len(val_y[0]), activation=\"softmax\")(dropout)\nmodel2 = Model(inputs=inputs, outputs=outputs)","f32ba7ec":"decay_steps = int(round(4000*0.8\/batch_size))*epochs\ncosine_decay = CosineDecay(initial_learning_rate=3e-5, decay_steps=decay_steps, alpha=0.3)\n\nmodel2.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=tf.keras.optimizers.Adam(cosine_decay), metrics=[\"accuracy\"])","334bfd57":"model2.summary()","52dcab5a":"# history = model2.fit(training_data,batch_size=32,\n#                   epochs = 1 )","9a48f7f8":"history = model2.fit(train_x,train_y, batch_size=batch_size,\n                   epochs = 25,\n                   validation_data=(val_x, val_y))","eb75b866":"model2.evaluate(x=val_x,y= val_y)","72e60d9d":"model2.save('eff_final.h5')","e03ca666":"import matplotlib.pyplot as plt\n%matplotlib inline","b019b7fb":"tt =list(range(len(history.history['loss'])))\nplt.plot(tt,history.history['loss'])\nplt.plot(tt,history.history['val_loss'])\nplt.title('loss')\nplt.show","0b69f6e4":"plt.plot(tt,history.history['accuracy'])\nplt.plot(tt,history.history['val_accuracy'])\nplt.title('accuracy')\nplt.show","8021d6e5":"import cv2\nin_path = '..\/input\/cloth-test\/testing2.png'\n\nimg = cv2.imread(in_path)\nimg = cv2.resize(img, (img_size, img_size))  # resize image to match model's expected sizing\nimg = np.reshape(img, [1, img_size, img_size, 3])\nimg = img \/ 255.\nout = model.predict(img)\n# out = np.argmax(out)\n# labels[out]","c00eabbd":"# def predict(model, in_path):\n#     img = cv2.imread(in_path)\n#     img = cv2.resize(img, (img_size, img_size))  # resize image to match model's expected sizing\n#     img = np.reshape(img, [1, img_size, img_size, 3])\n#     img = img \/ 255.\n#     out = model.predict(img)\n#     return labels[np.argmax(out)]\n","92e10a36":"# import cv2\n# in_path = '..\/input\/cloth-test\/testing2.png'\n# predict(model,in_path)","e6555437":"# img = cv2.imread(in_path)\n# plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n# plt.title(predict(model,in_path))\n\n# plt.show()","22504395":"labels","8c315780":"# in_path= '..\/input\/clothing-dataset-full\/images_compressed\/8f6822c3-862b-4126-9a35-585569aec240.jpg'\n# img = cv2.imread(in_path)\n# plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))","9b64747d":"# label[label['label']=='Other']","d8d09b27":"# labels\n# rows = 4\n# cols = 5\n# fig = plt.figure()\n# for x in labels:\n#     tmp = label[label['label']==x].iloc[0]\n#     tmp = tmp['image']\n#     in_path= '..\/input\/clothing-dataset-full\/images_compressed\/'+tmp+'.jpg'\n#     img = cv2.imread(in_path)\n#     plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))","6792edd8":"# fig = plt.figure(figsize=(12, 12)) # rows*cols \ud589\ub82c\uc758 i\ubc88\uc9f8 subplot \uc0dd\uc131\n# rows = 4\n# cols = 5\n# i = 1\n \n# #xlabels = [\"xlabel\", \"(a)\", \"(b)\", \"(c)\", \"(d)\"]\n# xlabels = ['xlabel']+labels \n# for x in labels:\n#     tmp = label[label['label']==x].iloc[1]\n#     tmp = tmp['image']\n#     in_path= '..\/input\/clothing-dataset-full\/images_compressed\/'+tmp+'.jpg'\n#     img = cv2.imread(in_path)\n#     ax = fig.add_subplot(rows, cols, i)\n#     ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n#     ax.set_xlabel(\"%s6%s\"%(xlabels[i],return_color(img)))\n#     ax.set_xticks([]), ax.set_yticks([])\n#     i += 1\n \n# plt.show()\n","4786d001":"# labels","47f95f43":"# import math\n# from collections import Counter\n\n# def find_nearest(array,value):\n#     idx = np.searchsorted(array, value, side=\"left\")\n#     if idx > 0 and (idx == len(array) or math.fabs(value - array[idx-1]) < math.fabs(value - array[idx])):\n#         return array[idx-1]\n#     else:\n#         return array[idx]\n\n# def get_color_distance(a,b):\n#     tmp = 0\n#     for x in range(3):\n#         tmp += (a[x]-b[x])**2\n#     return math.sqrt(tmp)\n\n\n# def split_map(x):\n#     return list(map(int,x.split(',')))\n\n# color_table = pd.read_csv('..\/input\/color-table\/color_table (2).csv')\n# table_list = list(color_table['rgb'].values)\n# table_list = list(map(split_map,table_list))\n# table_col = list(color_table['eng'].values)\n# def return_color(img):\n#     color=[]\n#     color.append(img[int(img.shape[0]\/2)][int(img.shape[1]\/2)].tolist()[::-1])\n#     color.append(img[int(img.shape[0]\/4)][int(img.shape[1]\/4)].tolist()[::-1])\n#     color.append(img[int(img.shape[0]*3\/4)][int(img.shape[1]\/4)].tolist()[::-1])\n#     color.append(img[int(img.shape[0]\/4)][int(img.shape[1]*3\/4)].tolist()[::-1])\n#     color.append(img[int(img.shape[0]*3\/4)][int(img.shape[1]*3\/4)].tolist()[::-1])\n    \n\n#     color_dis = []\n#     all_color=[]\n#     for col in color:\n#         color_dis = []\n#         for x in table_list:\n#             color_dis.append(get_color_distance(col,x))\n#         all_color.append(table_col[np.argmin(color_dis)])\n#     cnt = Counter(all_color)\n#     return list(cnt)[0]\n#     return table_col[np.argmin(color_dis)]","ec581530":"# in_path= '..\/input\/clothing-dataset-full\/images_compressed\/8f6822c3-862b-4126-9a35-585569aec240.jpg'\n# img = cv2.imread(in_path)","06202caf":"# Get Color","872b97af":"dddddddddddddddddddddddddddd","d8373c18":"\ud574\uc57c\ud560\uc77c  \n\uc0c9 ~~~  \n\ucd94\ucc9c \uc54c\uace0\ub9ac\uc998  (\uba85\ud655\ud788)","a2eb59b3":"tensorflow\t\t\t\ubb34\uc2e0\uc0ac\t\n'Not sure',\t\t\t\t\n 'T-Shirt',\t\ubc18\ud314\t\uc0c1\uc758\t\ubc18\ud314\ud2f0\uc154\uce20\t\n 'Shoes',\t\uc2e0\ubc1c\t\t\t\n 'Shorts',\t\ubc18\ubc14\uc9c0\t\ubc14\uc9c0\t\uc20f \ud32c\uce20\t\n 'Shirt',\t\uc154\uce20\t\uc0c1\uc758\t\uc154\uce20\/\ube14\ub77c\uc6b0\uc2a4\t\n 'Pants',\t\uae34\ubc14\uc9c0\t\ubc14\uc9c0\t\ub370\ub2d8 \ud32c\uce20, \ucf54\ud2bc \ud32c\uce20, \ub808\uae45\uc2a4, \uc288\ud2b8 \ud32c\uce20\/\uc2ac\ub799\uc2a4, \ud2b8\ub808\uc774\ub2dd\/\uc870\uac70 \ud32c\uce20, \uae30\ud0c0 \ubc14\uc9c0\t\n 'Skirt',\t\uce58\ub9c8\t\uc2a4\ucee4\ud2b8\t\uc2a4\ucee4\ud2b8 \ub2e4\t\n 'Other',\t\t\t\t\n 'Top',\t\ub098\uc2dc? \ud0d1?\t\uc0c1\uc758\t\ubbfc\uc18c\ub9e4\ud2f0\uc154\uce20\t\n 'Outwear',\t\uc544\uc6b0\ud130,\uac89\uc637\t\uc544\uc6b0\ud130\t\uc544\uc6b0\ud130\uc5d0\uc11c \uc288\ud2b8 \ube14\ub808\uc774\uc800 \ube7c\uace0 \ub2e4\t\n 'Dress',\t\ub4dc\ub808\uc2a4\t\uc6d0\ud53c\uc2a4 \t\uc6d0\ud53c\uc2a4 \ub2e4\t?\n 'Body',\t\ud55c\ubc8c\uc637?\t\ubc14\uc9c0\t\uc810\ud504\uc288\ud2b8\/\uc624\ubc84\uc62c\t\n 'Longsleeve',\t\uae34\ud314,\ub9e8\ud22c\ub9e8\t\uc0c1\uc758\t\uae34\ud314 \ud2f0\uc154\uce20,\ub9e8\ud22c\ub9e8\/\uc2a4\uc6e8\ud2b8\uc154\uce20,\ub2c8\ud2b8\/\uc2a4\uc6e8\ud130\t\n 'Undershirt',\t\t\t\t\n 'Hat',\t\ubaa8\uc790\t\t\t\n 'Polo',\t\ud3f4\ub85c\ud2f0?\t\uc0c1\uc758\t\ud53c\ucf00\/\uce74\ub77c\ud2f0\uc154\uce20\t\n 'Blouse',\t\ube14\ub77c\uc6b0\uc838\t\uc0c1\uc758\t\uc154\uce20\/\ube14\ub77c\uc6b0\uc2a4\t\n 'Hoodie',\t\ud6c4\ub4dc\ud2f0\t\uc0c1\uc758\t\ud6c4\ub4dc\ud2f0\uc154\uce20\t\n 'Skip',\t\uc6d0\ud53c\uc2a4\t\uc6d0\ud53c\uc2a4 \t\uc6d0\ud53c\uc2a4 \ub2e4\t?\n 'Blazer'\t\ube14\ub808\uc774\uc800\t\uc544\uc6b0\ud130\t\uc288\ud2b8\/\ube14\ub808\uc774\uc800\t\\","ef98bd18":"from PIL import Image\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimage = Image.open(tmp.iloc[0]['filepath'])\nplt.imshow(image)\nplt.show()"}}