{"cell_type":{"67d06916":"code","dfa22d4c":"code","225589bb":"code","d24bd2e4":"code","7e86c79f":"code","dbb32818":"code","00161336":"code","e8f57b42":"code","067b1ca3":"code","667b6fbf":"code","9d67cddf":"code","566cf83f":"code","6d52a498":"code","d96f11bc":"code","60efd7d0":"code","1cbf4228":"code","711020cd":"code","6e387b30":"code","765a3dbd":"code","4f5eedfa":"code","8a0dcd22":"code","43cb5771":"code","afffecb8":"code","a51835c9":"code","5fcf9039":"code","c4a39226":"code","e6d3c818":"code","c925dfd7":"code","45439800":"code","fafbbd31":"code","16e8a4b7":"code","ed97feb9":"code","0f9ddaa6":"code","1755bc9c":"code","d3b14e5e":"code","0ab07010":"code","9883de6a":"markdown","bb589db3":"markdown"},"source":{"67d06916":"import os\nimport cv2\n\nimport numpy as np\nimport glob\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import ImageGrid","dfa22d4c":"tumour_images=[]\nfor name in glob.glob('..\/input\/brain-mri-images-for-brain-tumor-detection\/yes\/*.jpg'): \n    image = cv2.imread(name)\n    image = cv2.resize(image,(240,240))\n    tumour_images.append(image)","225589bb":"fig = plt.figure(figsize=(10., 10.))\ngrid = ImageGrid(fig, 111, nrows_ncols=(4, 4),  axes_pad=0.1,   )\nfor ax, im in zip(grid, tumour_images[0:16]):\n    ax.imshow(im)\nplt.show()","d24bd2e4":"img_path = \"..\/input\/brain-mri-images-for-brain-tumor-detection\/yes\/Y104.jpg\"\nimage = cv2.imread(img_path)\nprint(\"width: {} pixels\".format(image.shape[1]))\nprint(\"height: {} pixels\".format(image.shape[0]))\nprint(\"channels: {}\".format(image.shape[2]))\ndim=(500,590)\nimage=cv2.resize(image, dim)","7e86c79f":"plt.imshow(image)","dbb32818":"gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY, 0.7)\nplt.imshow(image)\n","00161336":"(T, thresh) = cv2.threshold(gray, 155, 255, cv2.THRESH_BINARY)\nplt.imshow(thresh)\n","e8f57b42":"(T, threshInv) = cv2.threshold(gray, 155, 255, cv2.THRESH_BINARY_INV)\nplt.imshow(threshInv)\n","067b1ca3":"kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (10, 5))\nclosed = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\nplt.imshow(closed)","667b6fbf":"closed = cv2.erode(closed, None, iterations = 19)\nclosed = cv2.dilate(closed, None, iterations = 17)","9d67cddf":"plt.imshow(closed)","566cf83f":"ret,mask = cv2.threshold(closed, 155, 255, cv2.THRESH_BINARY) \n#apply AND operation on image and mask generated by thrresholding\nfinal = cv2.bitwise_and(image,image,mask = mask) \nplt.imshow(final)","6d52a498":"def auto_canny(image, sigma=0.33):\n    # compute the median of the single channel pixel intensities\n    v = np.median(image)\n    # apply automatic Canny edge detection using the computed median\n    lower = int(max(0, (1.0 - sigma) * v))\n    upper = int(min(255, (1.0 + sigma) * v))\n    edged = cv2.Canny(image, lower, upper)\n    # return the edged image\n    return edged\ncanny = auto_canny(closed)\nplt.imshow(canny)","d96f11bc":"(cnts, _) = cv2.findContours(canny.copy(), cv2.RETR_EXTERNAL,\ncv2.CHAIN_APPROX_SIMPLE)\ncv2.drawContours(image, cnts, -1, (0, 0, 255), 2)\nplt.imshow(image) ","60efd7d0":"import numpy as np \nimport pandas as pd \nimport os,gc,pathlib\nfrom sklearn.metrics import confusion_matrix\nfrom fastai import *\nfrom fastai.vision import *\nfrom fastai.vision.models import *\nprint(os.listdir(\"..\/input\"))\nimport torchvision.models as models","1cbf4228":"DATA_DIR='..\/input\/brain-mri-images-for-brain-tumor-detection'","711020cd":"os.listdir(f'{DATA_DIR}')","6e387b30":"data = ImageDataBunch.from_folder(DATA_DIR, train=\".\", \n                                  valid_pct=0.25,\n                                  ds_tfms=get_transforms(do_flip=True, flip_vert=True),\n                                  size=224,bs=24, \n                                  num_workers=0).normalize(imagenet_stats)\nprint(f'Classes: \\n {data.classes}')","765a3dbd":"data.show_batch(rows=10, figsize=(5,5))","4f5eedfa":"learner = create_cnn(data, models.vgg16, metrics=[accuracy], callback_fns=ShowGraph,  model_dir=\"\/tmp\/model\/\")","8a0dcd22":"learner.lr_find()\nlearner.recorder.plot()","43cb5771":"learner.fit_one_cycle(15, max_lr=slice(1e-2))","afffecb8":"learner.save('\/kaggle\/working\/model_1')","a51835c9":"learner.unfreeze()","5fcf9039":"learner.lr_find()\nlearner.recorder.plot()","c4a39226":"learner.load('\/kaggle\/working\/model_1')","e6d3c818":"learner.fit_one_cycle(15, max_lr=slice(1e-05))","c925dfd7":"learner.save('\/kaggle\/working\/stage_2_1')","45439800":"learner.fit_one_cycle(15, max_lr=slice(1e-06))","fafbbd31":"learner.save('\/kaggle\/working\/stage_2_2')","16e8a4b7":"learner.fit_one_cycle(15, max_lr=slice(1e-04))","ed97feb9":"learner.save('\/kaggle\/working\/stage_2_3')","0f9ddaa6":"learner.recorder.plot_losses()","1755bc9c":"interp = ClassificationInterpretation.from_learner(learner)","d3b14e5e":"interp.plot_top_losses(12, figsize=(10,10))","0ab07010":"interp.plot_confusion_matrix(figsize=(8,8), dpi=60)","9883de6a":"# Part 2 - FastAI","bb589db3":"# PART 1 - OpenCV"}}