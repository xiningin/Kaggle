{"cell_type":{"3011737c":"code","7eb0c1fb":"code","d19e5159":"code","851654c2":"code","49f923ed":"code","cbfef40c":"code","ce7a2af2":"code","f2b8ac57":"code","e8035ebc":"code","ce86926c":"code","12f2dd7f":"markdown","d39314dc":"markdown","809aa37a":"markdown","5a19b515":"markdown","b0ba3b05":"markdown","5f2104f0":"markdown","6b050b45":"markdown"},"source":{"3011737c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import accuracy_score\nimport random\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ntrain = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/test.csv')\n\ntrain_id = train[\"id\"]\n\ntrain.drop(\"id\", axis = 1, inplace = True)\ntrain.head(1)","7eb0c1fb":"#RandomForestRegressor\nsample_1 = [random.randint(0, len(train)) for i in range(1000)]\nsample_2 = [random.randint(0, len(train)) for i in range(2000)]\nsample_3 = [random.randint(0, len(train)) for i in range(3000)]\n\nsample = [sample_1, sample_2, sample_3]\n\nfig = plt.figure(figsize = [14,6])\nfor i in range(len(sample)):\n    X, y = train.iloc[sample[i],:-1], train.iloc[sample[i],-1]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n\n    forest = RandomForestRegressor(random_state = 1, n_jobs = -1)\n    forest.fit(X_train, y_train)\n    y_train_pred = forest.predict(X_train)\n    y_test_pred = forest.predict(X_test)\n\n    print(\"rmse train:{:.2f} \/ test:{:.2f}\".format(np.sqrt(mean_squared_error(y_train, y_train_pred)), np.sqrt(mean_squared_error(y_test, y_test_pred))))\n    print(\"r2_score train:{:.2f} \/ test:{:.2f}\".format(r2_score(y_train, y_train_pred), r2_score(y_test, y_test_pred)))\n\n    fig.add_subplot(1, 3, i+1)\n    plt.scatter(y_train, y_train_pred, label = \"train\")\n    plt.scatter(y_test, y_test_pred, label = \"test\")\n    plt.xlabel(\"raw data\")\n    plt.ylabel(\"predict data\")\n    plt.xlim(0,40)\n    plt.ylim(0,40)\n    plt.grid()","d19e5159":"#LinearRegression\nrandom.seed(0)\nsample_1 = [random.randint(0, len(train)) for i in range(1000)]\nsample_2 = [random.randint(0, len(train)) for i in range(2000)]\nsample_3 = [random.randint(0, len(train)) for i in range(3000)]\n\nsample = [sample_1, sample_2, sample_3]\n\npipe_lr = make_pipeline(StandardScaler(),\n                        LinearRegression())\n\nfig = plt.figure(figsize = [14,6])\nfor i in range(len(sample)):\n    X, y = train.iloc[sample[i],:-1], train.iloc[sample[i],-1]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n\n    pipe_lr.fit(X_train, y_train)\n    y_train_pred = pipe_lr.predict(X_train)\n    y_test_pred = pipe_lr.predict(X_test)\n\n    print(\"rmse train:{:.2f} \/ test:{:.2f}\".format(np.sqrt(mean_squared_error(y_train, y_train_pred)), np.sqrt(mean_squared_error(y_test, y_test_pred))))\n    print(\"r2_score train:{:.2f} \/ test:{:.2f}\".format(r2_score(y_train, y_train_pred), r2_score(y_test, y_test_pred)))\n\n    fig.add_subplot(1, 3, i+1)\n    plt.scatter(y_train, y_train_pred, label = \"train\")\n    plt.scatter(y_test, y_test_pred, label = \"test\")\n    plt.xlabel(\"raw data\")\n    plt.ylabel(\"predict data\")\n    plt.xlim(0,40)\n    plt.ylim(0,40)\n    plt.grid()","851654c2":"#KneighborsRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\nrandom.seed(0)\nsample_1 = [random.randint(0, len(train)) for i in range(1000)]\nsample_2 = [random.randint(0, len(train)) for i in range(2000)]\nsample_3 = [random.randint(0, len(train)) for i in range(3000)]\n\nsample = [sample_1, sample_2, sample_3]\n\n\n\nn = [3,7,20, 50]\nk = 0\nfor j in range(len(n)):\n    fig = plt.figure(figsize = [24,18])\n    for i in range(len(sample)):\n        X, y = train.iloc[sample[i],:-1], train.iloc[sample[i],-1]\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n        pipe_lr = make_pipeline(StandardScaler(),\n                                KNeighborsRegressor(n_neighbors = n[j]))\n        pipe_lr.fit(X_train, y_train)\n        y_train_pred = pipe_lr.predict(X_train)\n        y_test_pred = pipe_lr.predict(X_test)\n        \n        print(\"neighbor:{:.2f} \".format(n[j]))\n        print(\"rmse train:{:.2f} \/ test:{:.2f}\".format(np.sqrt(mean_squared_error(y_train, y_train_pred)), np.sqrt(mean_squared_error(y_test, y_test_pred))))\n        print(\"r2_score train:{:.2f} \/ test:{:.2f}\".format(r2_score(y_train, y_train_pred), r2_score(y_test, y_test_pred)))\n        print(\"-\"*50)\n\n        fig.add_subplot(5, 3, k+1)\n        plt.scatter(y_train, y_train_pred, label = \"train\")\n        plt.scatter(y_test, y_test_pred, label = \"test\")\n        plt.xlabel(\"raw data\")\n        plt.ylabel(\"predict data\")\n        plt.xlim(0,40)\n        plt.ylim(0,40)\n        plt.grid()\n        k+=1","49f923ed":"sns.distplot(train[\"loss\"])","cbfef40c":"#RandomForestRegressor delete the value of 0 loss at 1\/2\ntmp_0 = train[train[\"loss\"] ==0]\ntmp_1 = train[train[\"loss\"] !=0]\n\nn_len = len(tmp_0) \/\/ 2\ntmp_0 = tmp_0.iloc[0:n_len]\n\ntrain_del = pd.concat([tmp_0, tmp_1], axis = 0)\n# train_del = tmp_1\n\n#RandomForestRegressor\nsample_1 = [random.randint(0, len(train_del)) for i in range(1000)]\nsample_2 = [random.randint(0, len(train_del)) for i in range(2000)]\nsample_3 = [random.randint(0, len(train_del)) for i in range(3000)]\n\nsample = [sample_1, sample_2, sample_3]\n\nfig = plt.figure(figsize = [14,6])\nfor i in range(len(sample)):\n    X, y = train_del.iloc[sample[i],:-1], train_del.iloc[sample[i],-1]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n\n    forest = RandomForestRegressor(random_state = 1, n_jobs = -1)\n    forest.fit(X_train, y_train)\n    y_train_pred = forest.predict(X_train)\n    y_test_pred = forest.predict(X_test)\n\n    print(\"rmse train:{:.2f} \/ test:{:.2f}\".format(np.sqrt(mean_squared_error(y_train, y_train_pred)), np.sqrt(mean_squared_error(y_test, y_test_pred))))\n    print(\"r2_score train:{:.2f} \/ test:{:.2f}\".format(r2_score(y_train, y_train_pred), r2_score(y_test, y_test_pred)))\n\n    fig.add_subplot(1, 3, i+1)\n    plt.scatter(y_train, y_train_pred, label = \"train\")\n    plt.scatter(y_test, y_test_pred, label = \"test\")\n    plt.xlabel(\"raw data\")\n    plt.ylabel(\"predict data\")\n    plt.xlim(0,40)\n    plt.ylim(0,40)\n    plt.grid()","ce7a2af2":"#RandomForestRegressor delete the value of loss range(0 <10)\ntmp_10 = train[train[\"loss\"] >=10]\n\ntrain_del = tmp_10\n\n#RandomForestRegressor\nrandom.seed(0)\nsample_1 = [random.randint(0, len(train_del)) for i in range(1000)]\nsample_2 = [random.randint(0, len(train_del)) for i in range(2000)]\nsample_3 = [random.randint(0, len(train_del)) for i in range(3000)]\n\nsample = [sample_1, sample_2, sample_3]\n\nfig = plt.figure(figsize = [14,6])\nfor i in range(len(sample)):\n    X, y = train_del.iloc[sample[i],:-1], train_del.iloc[sample[i],-1]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n\n    forest = RandomForestRegressor(random_state = 1, n_jobs = -1)\n    forest.fit(X_train, y_train)\n    y_train_pred = forest.predict(X_train)\n    y_test_pred = forest.predict(X_test)\n\n    print(\"rmse train:{:.2f} \/ test:{:.2f}\".format(np.sqrt(mean_squared_error(y_train, y_train_pred)), np.sqrt(mean_squared_error(y_test, y_test_pred))))\n    print(\"r2_score train:{:.2f} \/ test:{:.2f}\".format(r2_score(y_train, y_train_pred), r2_score(y_test, y_test_pred)))\n\n    fig.add_subplot(1, 3, i+1)\n    plt.scatter(y_train, y_train_pred, label = \"train\")\n    plt.scatter(y_test, y_test_pred, label = \"test\")\n    plt.xlabel(\"raw data\")\n    plt.ylabel(\"predict data\")\n    plt.xlim(0,40)\n    plt.ylim(0,40)\n    plt.grid()","f2b8ac57":"#Ridge\nfrom sklearn.linear_model import Ridge\n\nrandom.seed(0)\nsample_1 = [random.randint(0, len(train_del)) for i in range(1000)]\nsample_2 = [random.randint(0, len(train_del)) for i in range(2000)]\nsample_3 = [random.randint(0, len(train_del)) for i in range(3000)]\n\nsample = [sample_1, sample_2, sample_3]\n\nfig = plt.figure(figsize = [14,6])\nfor i in range(len(sample)):\n    X, y = train.iloc[sample[i],:-1], train.iloc[sample[i],-1]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n\n    ridge=Ridge(alpha = 1.0)\n    ridge.fit(X_train, y_train)\n    y_train_pred = ridge.predict(X_train)\n    y_test_pred = ridge.predict(X_test)\n\n    print(\"rmse train:{:.2f} \/ test:{:.2f}\".format(np.sqrt(mean_squared_error(y_train, y_train_pred)), np.sqrt(mean_squared_error(y_test, y_test_pred))))\n    print(\"r2_score train:{:.2f} \/ test:{:.2f}\".format(r2_score(y_train, y_train_pred), r2_score(y_test, y_test_pred)))\n\n    fig.add_subplot(1, 3, i+1)\n    plt.scatter(y_train, y_train_pred, label = \"train\")\n    plt.scatter(y_test, y_test_pred, label = \"test\")\n    plt.xlabel(\"raw data\")\n    plt.ylabel(\"predict data\")\n    plt.xlim(0,40)\n    plt.ylim(0,40)\n    plt.grid()","e8035ebc":"#Lasso\nfrom sklearn.linear_model import Lasso\n\nrandom.seed(0)\nsample_1 = [random.randint(0, len(train_del)) for i in range(1000)]\nsample_2 = [random.randint(0, len(train_del)) for i in range(2000)]\nsample_3 = [random.randint(0, len(train_del)) for i in range(3000)]\n\nsample = [sample_1, sample_2, sample_3]\n\nfig = plt.figure(figsize = [14,6])\nfor i in range(len(sample)):\n    X, y = train.iloc[sample[i],:-1], train.iloc[sample[i],-1]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n\n    lasso=Lasso(alpha = 1.0)\n    lasso.fit(X_train, y_train)\n    y_train_pred = lasso.predict(X_train)\n    y_test_pred = lasso.predict(X_test)\n\n    print(\"rmse train:{:.2f} \/ test:{:.2f}\".format(np.sqrt(mean_squared_error(y_train, y_train_pred)), np.sqrt(mean_squared_error(y_test, y_test_pred))))\n    print(\"r2_score train:{:.2f} \/ test:{:.2f}\".format(r2_score(y_train, y_train_pred), r2_score(y_test, y_test_pred)))\n\n    fig.add_subplot(1, 3, i+1)\n    plt.scatter(y_train, y_train_pred, label = \"train\")\n    plt.scatter(y_test, y_test_pred, label = \"test\")\n    plt.xlabel(\"raw data\")\n    plt.ylabel(\"predict data\")\n    plt.xlim(0,40)\n    plt.ylim(0,40)\n    plt.grid()","ce86926c":"None of the results so far have successfully predicted values above 10. If\nShould I improve my score in the direction of accurately predicting 10 or less?","12f2dd7f":"The large loss value is completely unpredictable.\n\uff08Is the score the same as the one with all average values?\uff09\n\nSubmit score when learning with all data:8.02\n\nSince the original theme is the default prediction of loans, I would like to predict even those with a lot of losses.\nTry another Algols.\n","d39314dc":"The tendency is the same as randomforestregressor, and the value with large loss cannot be predicted.\n\nSubmit score when learning with all data \uff1a7.93","809aa37a":"Can't predict high loss due to bias in training data? Try to cut loss 0 at 1\/2.\nTry this idea in randomforestregressor for the time being.","5a19b515":"Since the loss of 10 or less has been deleted, naturally there is no predicted value of 10 or less.\nSubmit score when learning with all data \uff1a13.1\n\nInterestingly, the small value of loss can be predicted before the data is deleted. And, as usual, the large value of loss cannot be predicted.\n","b0ba3b05":"The tendency is the same as randomforestregressor, and the value with large loss cannot be predicted.\n\nSubmit score when learning with all data \uff1a\n","5f2104f0":"I deleted 0 losses\u30001\/2, but it didn't work well.\n\nSubmit score when learning with all data \uff1a8.11","6b050b45":"Machine learning with random forest as a baseline.\nSince there is a lot of data and it takes too much time to calculate, we will approach by extracting training data.\n\uff08It takes a lot of time to calculate here, so I will extract the data and proceed.\uff09"}}