{"cell_type":{"4556ed9d":"code","38be9903":"code","a68979e7":"code","751060b2":"code","49f935d7":"code","6299bf84":"code","5de28ffb":"code","0f7c56e7":"code","5a7a4180":"code","d3af4c2e":"code","ed4fedd8":"code","da5efd26":"code","5ccf08e9":"code","08a9fc22":"code","3b6539bc":"code","0dff38c5":"code","2930fbad":"code","f6197c44":"code","97d35402":"code","9ec868d2":"code","51c5186a":"code","0fceb952":"code","6930a3b5":"code","647e472e":"code","cce2aa8f":"code","d450ed46":"code","0edbc425":"code","5302e388":"code","5cb728b9":"code","67654719":"markdown"},"source":{"4556ed9d":"# Python \u22653.5 is required\nimport sys\nassert sys.version_info >= (3, 5)\n\n# Scikit-Learn \u22650.20 is required\nimport sklearn\nassert sklearn.__version__ >= \"0.20\"\n\n# Common imports\nimport numpy as np\nimport os\nimport gc\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport datetime\n%matplotlib inline \n#plotting directly without requering the plot()\n\nimport warnings\nwarnings.filterwarnings(action=\"ignore\") #ignoring most of warnings, cleaning up the notebook for better visualization\n\npd.set_option('display.max_columns', 500) #fixing the number of rows and columns to be displayed\npd.set_option('display.max_rows', 500)\n\nprint(os.listdir(\"..\/input\")) #showing all the files in the ..\/input directory\n\n# Any results you write to the current directory are saved as output. Kaggle message :D","38be9903":"train = pd.read_csv('..\/input\/train.csv', parse_dates=['first_active_month'], low_memory=True)\ntest = pd.read_csv('..\/input\/test.csv', low_memory=True, parse_dates=['first_active_month'])\ndf_new_merch = pd.read_csv('..\/input\/new_merchant_transactions.csv', parse_dates=['purchase_date'], low_memory=True)\ndf_hist_trans = pd.read_csv('..\/input\/historical_transactions.csv',parse_dates=['purchase_date'], low_memory=True)\n\nprint('Training set shape: {}'.format(train.shape))\nprint('Testing set shape: {}'.format(train.shape))\nprint('Merchants set shape: {}'.format(df_new_merch.shape))\nprint('Historical transactions set shape: {}'.format(df_hist_trans.shape))","a68979e7":"train.head()","751060b2":"test.head()","49f935d7":"print('Max date for the training set: {}'.format(train['first_active_month'].max()))\nprint('Max date for the testing set: {}'.format(test['first_active_month'].max()))","6299bf84":"train.describe()","5de28ffb":"# https:\/\/www.kaggle.com\/gpreda\/elo-world-high-score-without-blending\nfor df in [df_hist_trans, df_new_merch]:\n    df['category_2'].fillna(1.0,inplace=True)\n    df['category_3'].fillna('A',inplace=True)\n    df['year'] = df['purchase_date'].dt.year\n    df['weekofyear'] = df['purchase_date'].dt.weekofyear\n    df['month'] = df['purchase_date'].dt.month\n    df['dayofweek'] = df['purchase_date'].dt.dayofweek\n    df['authorized_flag'] = df['authorized_flag'].map({'Y':1, 'N':0})\n    df['category_1'] = df['category_1'].map({'Y':1, 'N':0}) \n    df['category_3'] = df['category_3'].map({'A':0, 'B':1, 'C':2}) ","0f7c56e7":"fig = plt.figure(figsize=(16,10))\nfor i,col in enumerate(['year','month','dayofweek','weekofyear']):\n    plt.subplot(4,1,i+1)\n    sns.countplot(df_new_merch[col])\n    plt.title('Number of transactions per {}'.format(col))\nplt.tight_layout(h_pad=0.5)   ","5a7a4180":"fig = plt.figure(figsize=(16,10))\nfor i,col in enumerate(['year','month','dayofweek','weekofyear']):\n    plt.subplot(4,1,i+1)\n    sns.countplot(df_hist_trans[col])\n    plt.title('Number of transactions per {}'.format(col))\nplt.tight_layout(h_pad=0.5)    ","d3af4c2e":"for df in [train, test]:\n    df['year'] = df['first_active_month'].dt.dayofweek\n    df['dayofweek'] = df['first_active_month'].dt.dayofweek\n    df['weekofyear'] = df['first_active_month'].dt.weekofyear\n    df['dayofyear'] = df['first_active_month'].dt.dayofyear\n    df['quarter'] = df['first_active_month'].dt.quarter\n    df['is_month_start'] = df['first_active_month'].dt.is_month_start\n    df['month'] = df['first_active_month'].dt.month\n    df['elapsed_time'] = (datetime.datetime.today() - df['first_active_month']).dt.days\nplt.subplot(3,1,1)\ntrain['target'].plot.hist(bins=30, edgecolor='black', figsize=(16,10))\nplt.title('TARGET distribution')\nplt.subplot(3,1,2)\nsns.countplot(train['year'])\nplt.title('TARGET X YEAR')\nplt.subplot(3,1,3)\nsns.countplot(train['month'])\nplt.xticks(rotation='vertical')\nplt.title('TARGET X MONTH')\n\nplt.tight_layout(h_pad=0.5)","ed4fedd8":"train[train['target'] < - 30]['target'].value_counts()","da5efd26":"plt.figure(figsize = (16, 12))\n\n\ntrain['log_target'] = np.log1p(train['target'])\n\n\n# iterate through the sources\nfor i, features in enumerate(['feature_1', 'feature_2', 'feature_3']):\n    \n    # create a new subplot for each source\n    plt.subplot(3, 1, i + 1)\n    # plot repaid loans\n    sns.kdeplot(train.loc[train['log_target'] >= 0, features], label = 'log_target >= 0')\n    # plot loans that were not repaid\n    sns.kdeplot(train.loc[train['log_target'] < 0, features], label = 'log_target < 0')\n    \n    # Label the plots\n    plt.title('Distribution of %s by log 1p target Value' % features)\n    plt.xlabel('%s' % features); plt.ylabel('Density');\n    \nplt.tight_layout(h_pad = 1.5)\n\ntrain.drop('log_target', axis=1, inplace=True)\n","5ccf08e9":"group_purch = df_hist_trans.groupby('card_id')['purchase_amount'].size().reset_index()\ngroup_purch.columns = [\"card_id\", \"count_purchase_amount\"]\n\ntrain = pd.merge(train,\n                 group_purch,\n                 on='card_id',\n                 how='left')\ntrain = train.merge(df_hist_trans[['authorized_flag','category_1','category_3','card_id']],on='card_id',how='left')\n\ntest = pd.merge(test,\n                 group_purch,\n                 on='card_id',\n                 how='left')\ntest = test.merge(df_hist_trans[['authorized_flag','category_1','category_3','card_id']],on='card_id',how='left')\n\n\n\nprint('Training set shape after merging with historical transaction: {}'.format(train.shape))\n\nprint('Testing set shape after merging with historical transaction: {}'.format(test.shape))\n\ngc.enable\ndel df_hist_trans, group_purch\ngc.collect()","08a9fc22":"train.head()","3b6539bc":"train.isnull().sum()","0dff38c5":"train.describe()","2930fbad":"fig = plt.figure(figsize=(16,5))\nplt.subplot(2,1,1)\nplt.scatter(train['count_purchase_amount'], train['target'], c='c')\nplt.subplot(2,1,2)\ntrain.set_index('first_active_month')['count_purchase_amount'].plot()","f6197c44":"df_new_merch.head()","97d35402":"group_card_temp = df_new_merch.groupby('card_id')['purchase_amount'].size().reset_index()\n\ngroup_card_temp.columns = ['card_id', 'count_new_merch_purchases']\n\ntrain = pd.merge(train,\n                group_card_temp,\n                on='card_id',\n                how='left')\n\ntest = pd.merge(test,\n                group_card_temp,\n                on='card_id',\n                how='left')\n\nprint('Training set shape after merging with new merchant transaction: {}'.format(train.shape))\n\nprint('Testing set shape after merging with new merchant transaction: {}'.format(test.shape))\n\ntrain['count_new_merch_purchases'].fillna(0,inplace=True)\ntest['count_new_merch_purchases'].fillna(0,inplace=True)\n\n\ndel group_card_temp, df_new_merch\ngc.collect()","9ec868d2":"train.head()","51c5186a":"train.isnull().sum()","0fceb952":"fig = plt.figure(figsize=(16,5))\nplt.subplot(2,1,1)\nsns.scatterplot(x='count_purchase_amount', y='target', data= train)\nplt.title('Relationship between target value and purchase amount')\nplt.subplot(2,1,2)\nsns.scatterplot(x='count_new_merch_purchases', y='target', data= train)\nplt.title('Relationship between target value and new merchants purchase amount')\nplt.tight_layout(h_pad=0.5)","6930a3b5":"fig = plt.figure(figsize=(16,5))\nplt.subplot(2,1,1)\ntrain.set_index('first_active_month')['count_purchase_amount'].plot(c='b')\nplt.title('Purchase amount through time')\nplt.subplot(2,1,2)\ntrain.set_index('first_active_month')['count_new_merch_purchases'].plot(c='r')\nplt.title('New merchantes purchase amount through time')\nplt.tight_layout(h_pad=0.5)","647e472e":"lag_list = [1, 2, 3] #creating a lag list with each month, 1, 2 and 3 months later.\n\nfor lag in lag_list: #going through the list of months\n    \n    ft_name = ('count_purchase_%s_month_before' % lag) # lag number of months before, getting the previous item count per month\n    train[ft_name] = train.groupby(['month'])['count_purchase_amount'].shift(lag)\n    test[ft_name] = test.groupby(['month'])['count_purchase_amount'].shift(lag)\n    \n    ft_name = ('count_purchase_new_merch_%s_month_before' % lag)\n    train[ft_name] = train.groupby(['month'])['count_new_merch_purchases'].shift(lag)\n    test[ft_name] = test.groupby(['month'])['count_new_merch_purchases'].shift(lag)\n\n    # Fill the empty shifted features with 0\n    train[ft_name].fillna(0, inplace=True)","cce2aa8f":"fig = plt.figure(figsize=(16,8))\ncorr_train = train.corr()\nsns.heatmap(corr_train, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\nplt.title('Correlation Heatmap');","d450ed46":"train_labels = train['target'].copy()\nmask = ['target','first_active_month','card_id']\ncols = [col for col in train.columns if col not in mask]\ntrain_prepared = train.loc[:, cols]\ntest_prepared = test.loc[:, cols]\n\nprint('Training set: {}'.format(train_prepared.shape))\nprint('testing set: {}'.format(test_prepared.shape))\n\ndel corr_train, train,test\ngc.collect()","0edbc425":" #let's use the Imputer to fill the NAN values with the median value\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler,Imputer, RobustScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nimport time #implementing in this function the time spent on training the model\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV,cross_val_score,train_test_split, KFold\nimport lightgbm as lgb\nimport gc\n\n#imputing all NaN value\npipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")), \n        #('scale', MinMaxScaler(feature_range = (0, 1))),\n        ('robustScaler', RobustScaler()),\n])\n\nnfolds = 10\nfolds = KFold(n_splits=nfolds, shuffle=True,random_state=42)\n\n\n#Generic function for making a classification model and accessing performance:\ndef fit_model(train, train_labels, test_set, params={}, \n                         fold=folds, model=None, \n                         GridSearch=False, plot_features_importances=False):\n    \n    time_start = time.perf_counter() #start counting the time\n    #creating our validation set out of the training set and labels provided\n    X_train, x_val, y_train, y_val = train_test_split(train, train_labels, test_size=0.1, random_state=42)\n    X_train = pipeline.fit_transform(X_train) #fiting and transforming the dataset using the pipeline provided\n    x_val = pipeline.fit_transform(x_val)\n    \n    test_sub = np.zeros(test_set.shape[0])\n    test_set = pipeline.fit_transform(test_set)\n    \n    predict_val = np.zeros(train.shape[0])\n    score = {}\n    \n    if model != None: grid_model = GridSearchCV(model, params,verbose=1, cv=3) #initializing the grid search model\n\n    if GridSearch:\n        grid_model.fit(X_train, y_train)\n        score_grid = grid_model.best_score_\n        \n        #predicting using the model that has been trained above\n        \n        predict_val = grid_model.predict(x_val)\n        score['MAE'] = mean_absolute_error(y_val, predict_val)\n        score['RMSE'] = np.sqrt(mean_squared_error(y_val, predict_val))\n        \n        print(\"Model Report\")\n\n        print(\"MAE: \"+ str(score[\"MAE\"]))\n        print(\"RMSE: \"+ str(score[\"RMSE\"]))\n        print('\\n')\n    \n        test_sub = grid_model.predict(test_set) \n        \n    else:\n        model = lgb.LGBMRegressor(**params, n_estimators = 5000, nthread = 4, n_jobs = -1)\n\n        for n, (index, val_index) in enumerate(folds.split(train)):\n            \n            print('Starting Fold number: %d' %n)\n            X, X_val = train.values[index], train.values[val_index]\n            Y, Y_val = train_labels[index], train_labels[val_index]\n            X = pipeline.fit_transform(X)\n            X_val = pipeline.fit_transform(X_val)\n            \n            model.fit(X, Y, \n                    eval_set=[(X, Y), (X_val, Y_val)],\n                    verbose=1000, early_stopping_rounds=200)\n            \n            predict_val = model.predict(X_val)\n            test_temp = model.predict(test_set, num_iteration=model.best_iteration_)\n            test_sub += test_temp\n            \n            if score == {}:\n                score['MAE'] = mean_absolute_error(Y_val, predict_val)\n                score['RMSE'] = np.sqrt(mean_squared_error(Y_val, predict_val))            \n            else:\n                score['MAE'] += mean_absolute_error(Y_val, predict_val)\n                score['RMSE'] += np.sqrt(mean_squared_error(Y_val, predict_val))\n\n                        \n        test_sub \/= nfolds\n                        \n        print(\"Model Report\")\n\n        print(\"MAE: \"+ str(score[\"MAE\"]\/nfolds))\n        print(\"RMSE: \"+ str(score[\"RMSE\"]\/nfolds))\n        \n        print('\\n')\n\n        \n    #################### PLOTTING FEATURES IMPORTANCE ####################\n    \n    # Sort features according to importance\n    if plot_features_importances:\n        if GridSearch:\n            # Extract feature importances\n            feature_importances = pd.DataFrame({'feature': list(train.columns), 'importance': grid_model.best_estimator_.feature_importances_})\n        else:\n            feature_importances = pd.DataFrame({'feature': list(train.columns), 'importance': model.feature_importances_})\n        \n        feature_importances = feature_importances.sort_values('importance', ascending = False).reset_index()\n\n        # Normalize the feature importances to add up to one\n        feature_importances['importance_normalized'] = feature_importances['importance'] \/ feature_importances['importance'].sum()\n\n        # Make a horizontal bar chart of feature importances\n        plt.figure(figsize = (10, 6))\n        ax = plt.subplot()\n\n        # Need to reverse the index to plot most important on top\n        ax.barh(list(reversed(list(feature_importances.index[:15]))), \n                feature_importances['importance_normalized'].head(15), \n                align = 'center', edgecolor = 'k')\n\n        # Set the yticks and labels\n        ax.set_yticks(list(reversed(list(feature_importances.index[:15]))))\n        ax.set_yticklabels(feature_importances['feature'].head(15))\n\n        # Plot labeling\n        plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    \n    time_end = time.perf_counter() #end of counting the time\n    \n    total_time = time_end-time_start #total time spent during training and cross_validation\n    \n    print(\"Amount of time spent during training the model and cross validation: %4.3f seconds\" % (total_time))\n    # Clean up memory\n    gc.enable()\n    del model, X_train, x_val, y_train, y_val,score, total_time, time_end, time_start,predict_val,test_set\n    gc.collect()\n                        \n    return test_sub","5302e388":"########## LGB ########\nparams = {\n          'num_leaves': 30,\n         'min_data_in_leaf': 20,\n         'objective': 'regression',\n         'max_depth': 4,\n         'learning_rate': 0.006,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.2,\n}\nprediction_lgb = fit_model(train_prepared,train_labels, test_prepared, params=params, plot_features_importances=True)","5cb728b9":"sub_df = pd.DataFrame({\"card_id\":test[\"card_id\"].values})\nsub_df[\"target\"] = prediction_lgb\nsub_df.to_csv(\"lgbm.csv\", index=False)\nsub_df.head()","67654719":"# Elo Merchant Category Recommendation\n## Simple EDA, feature engineering and LGB model\n\n#### thanks to all members"}}