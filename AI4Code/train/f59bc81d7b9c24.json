{"cell_type":{"38db6166":"code","206c1e6c":"code","cce8869a":"code","cb26d1f7":"code","01331391":"code","05f7b152":"code","ac6bf048":"code","0a327dcc":"code","9709799a":"code","8718d43e":"code","1614d77b":"code","0580ca41":"code","ff3c96dd":"code","55719cd4":"code","91b0f8ac":"code","2feaa81f":"code","b1370219":"code","b1194e58":"code","de359d61":"code","01ec155c":"code","a509cf8f":"code","9e98f572":"code","bd195885":"code","66aa1a21":"code","1096396a":"code","5dbdbc92":"code","2a34b770":"code","762154e9":"code","aaec4ca0":"code","7cb7bd7b":"code","e21c6362":"code","98a9cfb7":"code","f6213ad5":"code","c7830742":"code","e3179e87":"code","374ce3eb":"code","c82b96b2":"code","093b5158":"code","7fed39a7":"code","588379c9":"markdown","07337cbd":"markdown","775ccc0b":"markdown","97ee0bf8":"markdown","52c454cf":"markdown","61d12081":"markdown","78594e36":"markdown","65bff8e3":"markdown","d782402d":"markdown","411f5a70":"markdown","f0f005bf":"markdown","2e0be827":"markdown","3f88845a":"markdown","cf352e7a":"markdown","60943419":"markdown","efbad473":"markdown","d32b175f":"markdown","e2dbad19":"markdown","3ac998dd":"markdown","06b68a22":"markdown","4d8126c2":"markdown"},"source":{"38db6166":"import os\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport json\nfrom tqdm import tqdm_notebook\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.linear_model import Ridge\nimport lightgbm as lgb","206c1e6c":"from html.parser import HTMLParser\n\nclass MLStripper(HTMLParser):\n    def __init__(self):\n        self.reset()\n        self.strict = False\n        self.convert_charrefs= True\n        self.fed = []\n    def handle_data(self, d):\n        self.fed.append(d)\n    def get_data(self):\n        return ''.join(self.fed)\n\ndef strip_tags(html):\n    s = MLStripper()\n    s.feed(html)\n    return s.get_data()","cce8869a":"PATH_TO_DATA = '..\/input\/'","cb26d1f7":"def read_json_line(line=None):\n    result = None\n    try:        \n        result = json.loads(line)\n    except Exception as e:      \n        # Find the offending character index:\n        idx_to_replace = int(str(e).split(' ')[-1].replace(')',''))      \n        # Remove the offending character:\n        new_line = list(line)\n        new_line[idx_to_replace] = ' '\n        new_line = ''.join(new_line)     \n        return read_json_line(line=new_line)\n    return result","01331391":"def extract_titles_from_json(path_to_inp_json_file, path_to_out_txt_file, total_length):\n    '''\n    :param path_to_inp_json_file: path to a JSON file with train\/test data\n    :param path_to_out_txt_file: path to extracted features (here titles), one per line\n    :param total_length: we'll pass the hardcoded file length to make tqdm even more convenient\n    '''\n    with open(path_to_inp_json_file, encoding='utf-8') as inp_file, \\\n         open(path_to_out_txt_file, 'w', encoding='utf-8') as out_file:\n        for line in tqdm_notebook(inp_file, total=total_length):\n            json_data = read_json_line(line)\n            content = json_data['title'].replace('\\n', ' ').replace('\\r', ' ')\n            content_no_html_tags = strip_tags(content)\n            out_file.write(content_no_html_tags + '\\n')","05f7b152":"%%time\nextract_titles_from_json(path_to_inp_json_file=os.path.join(PATH_TO_DATA, 'train.json'),\n           path_to_out_txt_file='train_titles.txt', total_length=62313)","ac6bf048":"%%time\nextract_titles_from_json(path_to_inp_json_file=os.path.join(PATH_TO_DATA, 'test.json'),\n           path_to_out_txt_file='test_titles.txt', total_length=34645)","0a327dcc":"tfidf = TfidfVectorizer(max_features=50000, ngram_range=(1, 3))","9709799a":"%%time\nwith open('train_titles.txt', encoding='utf-8') as input_train_file:\n    X_train = tfidf.fit_transform(input_train_file)","8718d43e":"%%time\nwith open('test_titles.txt', encoding='utf-8') as input_test_file:\n    X_test = tfidf.transform(input_test_file)","1614d77b":"X_train.shape, X_test.shape","0580ca41":"train_target = pd.read_csv(os.path.join(PATH_TO_DATA, \n                                        'train_log1p_recommends.csv'), \n                           index_col='id')","ff3c96dd":"y_train = train_target['log_recommends'].values","55719cd4":"plt.hist(y_train, bins=30, alpha=.5, color='red', \n         label='original', range=(0,10));\nplt.hist(np.log1p(y_train), bins=30, alpha=.5, color='green', \n         label='log1p', range=(0,10));\nplt.legend();","91b0f8ac":"train_part_size = int(0.7 * train_target.shape[0])\nX_train_part = X_train[:train_part_size, :]\ny_train_part = y_train[:train_part_size]\nX_valid =  X_train[train_part_size:, :]\ny_valid = y_train[train_part_size:]","2feaa81f":"ridge = Ridge(random_state=17)","b1370219":"%%time\nridge.fit(X_train_part, np.log1p(y_train_part));","b1194e58":"ridge_pred = np.expm1(ridge.predict(X_valid))","de359d61":"lgb_x_train_part = lgb.Dataset(X_train_part.astype(np.float32), \n                           label=np.log1p(y_train_part))","01ec155c":"lgb_x_valid = lgb.Dataset(X_valid.astype(np.float32), \n                      label=np.log1p(y_valid))","a509cf8f":"param = {'num_leaves': 255, \n         'objective': 'mean_absolute_error',\n         'metric': 'mae'}","9e98f572":"num_round = 200\nbst_lgb = lgb.train(param, lgb_x_train_part, num_round, \n                    valid_sets=[lgb_x_valid], early_stopping_rounds=20)","bd195885":"lgb_pred = np.expm1(bst_lgb.predict(X_valid.astype(np.float32), \n                                    num_iteration=bst_lgb.best_iteration))","66aa1a21":"plt.hist(y_valid, bins=30, alpha=.5, color='red', label='true', range=(0,10));\nplt.hist(ridge_pred, bins=30, alpha=.5, color='green', label='Ridge', range=(0,10));\nplt.hist(lgb_pred, bins=30, alpha=.5, color='blue', label='Lgbm', range=(0,10));\nplt.legend();","1096396a":"ridge_valid_mae = mean_absolute_error(y_valid, ridge_pred)\nridge_valid_mae","5dbdbc92":"lgb_valid_mae = mean_absolute_error(y_valid, lgb_pred)\nlgb_valid_mae","2a34b770":"mean_absolute_error(y_valid, .4 * lgb_pred + .6 * ridge_pred)","762154e9":"%%time\nridge.fit(X_train, np.log1p(y_train));","aaec4ca0":"%%time\nridge_test_pred = np.expm1(ridge.predict(X_test))","7cb7bd7b":"lgb_x_train = lgb.Dataset(X_train.astype(np.float32),\n                          label=np.log1p(y_train))","e21c6362":"num_round = 60\nbst_lgb = lgb.train(param, lgb_x_train, num_round)","98a9cfb7":"lgb_test_pred = np.expm1(bst_lgb.predict(X_test.astype(np.float32)))","f6213ad5":"mix_pred = .4 * lgb_test_pred + .6 * ridge_test_pred","c7830742":"mean_test_target = 4.33328 ","e3179e87":"mix_test_pred_modif = mix_pred + mean_test_target - y_train.mean()","374ce3eb":"def write_submission_file(prediction, filename,\n                          path_to_sample=os.path.join(PATH_TO_DATA, \n                                                      'sample_submission.csv')):\n    submission = pd.read_csv(path_to_sample, index_col='id')\n    \n    submission['log_recommends'] = prediction\n    submission.to_csv(filename)","c82b96b2":"#write_submission_file(ridge_test_pred + mean_test_target - y_train.mean(), 'ridge_submission.csv')","093b5158":"#write_submission_file(lgb_test_pred + mean_test_target - y_train.mean(), 'lgb_submission.csv')","7fed39a7":"write_submission_file(mix_test_pred_modif, 'submission.csv')","588379c9":"# Extracting titles from raw data","07337cbd":"Supplementary function to read a JSON line without crashing on escape characters. ","775ccc0b":"Public MAEs:\n\n- Ridge \u2013 1.65253\n- LGBM \u2013 1.69407\n- Ridge-LGBM mix \u2013 1.64080\n\nAs we can see, simple blending decreases MAE for both holdout predictions and on the leaderboard. However, I don't recommend to play with blending\/stacking schemes un the beginning of the competition  \u2013 it's crucially important to come up with good features first.\n\nYou can further improve your model in various ways. I've described them in [this kernel](https:\/\/www.kaggle.com\/kashnitsky\/ridge-countvectorizer-baseline). Go and compete, good luck!","97ee0bf8":"Now we are ready to fit a linear model.","52c454cf":"Read targets from file.","61d12081":"Let's plot predictions and targets for the holdout set. Recall that these are #recommendations (= #claps) of Medium articles with the `np.log1p` transformation.","78594e36":"After `log1p`-transformation, we need to apply an inverse  `expm1`-trasformation to predictions.","65bff8e3":"# Leaderboard probing\n\nNow we apply a dirty hack. Originally, we made you come up with it by your own (as a part of an assignment, with only a hint from out side), but now it's described in [this](https:\/\/nbviewer.jupyter.org\/github\/Yorko\/mlcourse.ai\/blob\/master\/jupyter_english\/tutorials\/kaggle_leaderboard_probing_nikolai_timonin.ipynb) tutorial, written within a previous session of [mlcourse.ai](https:\/\/mlcourse.ai).\n\nSubmitting all zeros gives 4.33328. If you take a pen and a piece of paper and figure out what it means for MAE that all predictions are zeros, then you'll see that it's exactly the mean target value for the test set. We can compare it with mean target for training data and correspondingly adjust predictions. Looks like a dirty hack, however, the same thing is often done with time series prediction (even in production) - merely adjusting your predictions to a change in target variable distribution.","d782402d":"# Simple blending\nNow let's mix predictions. We's just pick up weights 0.6 for Lgbm and 0.4 for Ridge, but these are typically tuned via cross-validation. ","411f5a70":"# Validation and model training\nLet's make a 30%-holdout set. ","f0f005bf":"This function takes a JSON and forms a txt file leaving only article titles. When you resort to feature engineering and extract various features from articles, a good idea is to modify this function.","2e0be827":"# Feature engineering (simple Tf-Idf for titles)\nWe'll use a very simple feature extractor \u2013 `TfidfVectorizer`, meaning that we resort to the Bag-of-Words approach. For now, we are leaving only 50k features. ","3f88845a":"Assume you have all data downloaded from competition's [page](https:\/\/www.kaggle.com\/c\/how-good-is-your-medium-article\/data) in the PATH_TO_DATA folder and `.gz` files are ungzipped.","cf352e7a":"Finally, train both models on the full accessible training set, make predictions for the test set and form submission files. ","60943419":"Here's a simple mix","efbad473":"Path to competition data, change this if you'd like to.","d32b175f":"The following code will help to throw away all HTML tags from article content\/title.","e2dbad19":"Then, we fit a LightGBM model with `mean_absolute_error` as objective (it's important!).","3ac998dd":"## <center> Ridge and LightGBM: simple blending \n\nIn this competition, the metric is mean absolute error (MAE), so it's better to optimize it directly. We'll do it with LightGBM, a powerfull implementation of gradient boosting. Remember: with Ridge regression we optimize mean squared error (MSE), and it's not the same as optimizing MAE. However, do to sparse features, Ridge will still work better than boosting in this problem, however mixing predictions of two models will further help to decrease MAE. \nSo we'll apply a very simple method of averaging model predictions: blending. \n\n<img src='https:\/\/habrastorage.org\/webt\/gm\/ns\/jp\/gmnsjpxmgabagmi-bgialqtuhqa.png' width=30%>\n\nFinally, here we'll apply a simple hack, one from the family of leaderboard probing techniques.","06b68a22":"As we can see, the prediction is far from perfect, and we get MAE $\\approx$ 1.3 that corresponds to $\\approx$ 2.7 error in #recommendations.","4d8126c2":"Target is still somewhat skewed, though it's allready `log1p`-transformed (#claps with `log1p` transformation). Yet, we'll apply `log1p` once more time."}}