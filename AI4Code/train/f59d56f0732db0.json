{"cell_type":{"b202440e":"code","24a3b0b4":"code","917a3ec7":"code","ee2c6b7b":"code","fa68e001":"code","deac616e":"code","77d76963":"code","a71e8357":"code","aa50e449":"code","78d0565b":"code","3f9005c6":"code","1012854a":"code","f6da0bb4":"code","3b0e85b3":"code","e29b502d":"code","eb3d8dd3":"code","690b94d7":"code","85f5f275":"code","ed612efa":"code","7a2ed55c":"code","df0acc83":"code","69057767":"code","863267a1":"code","1321b953":"code","b5f41c88":"code","e62412fb":"code","1e120e83":"code","fba5341a":"code","ce432f67":"code","3d2ba107":"code","db4d2cfe":"code","a1dbb934":"code","303c1432":"code","f6acd267":"code","e77737d4":"code","1057a147":"code","cc83d541":"code","9e951245":"code","73a24786":"code","d3a169e9":"code","b4a50dff":"code","32eddb6f":"markdown","c7963e34":"markdown","06a1bf64":"markdown","35399bcd":"markdown","0a78945d":"markdown","50acea7d":"markdown","68bac252":"markdown","3f2e5aae":"markdown","5d6d69bc":"markdown","ebae7406":"markdown","b8c29671":"markdown","06dba504":"markdown","b6aa5403":"markdown","299269c0":"markdown","dde85ce0":"markdown","79c5f044":"markdown","6ab5071c":"markdown","49b0ed78":"markdown","3a0f1fc5":"markdown","12d5531e":"markdown","e7a7bb97":"markdown","39364231":"markdown","1e4b2785":"markdown","7c775316":"markdown","b3fb071c":"markdown","d268dbd1":"markdown","9131fdd3":"markdown","2a2345a9":"markdown","4f518be3":"markdown","7b448f10":"markdown","7472b933":"markdown","7225b946":"markdown","032f4f21":"markdown","4112a7c1":"markdown","7d1ace54":"markdown","eada5633":"markdown","0a3baf8a":"markdown","40cebebd":"markdown","1de38019":"markdown","aa93b568":"markdown","1c6c4a8e":"markdown","50c4104a":"markdown","9b2d7751":"markdown","c7864329":"markdown","7d43caf3":"markdown","78617153":"markdown","8fa008f9":"markdown"},"source":{"b202440e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats # QQ plot\nfrom sklearn.preprocessing import OneHotEncoder # Dummy variable\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split # Set spliting\nfrom sklearn.preprocessing import PolynomialFeatures \nfrom sklearn.linear_model import LinearRegression \nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import metrics\nimport statsmodels.api as sm\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n \nsns.set()\n\n%matplotlib inline","24a3b0b4":"dataset = pd.read_csv('..\/input\/fish-market\/Fish.csv')\ndataset.head()","917a3ec7":"dataset.rename(columns= {'Species': 'Species', 'Length1':'DimVer', 'Length2':'DimDiag', 'Length3':'DimLong'}, inplace=True)","ee2c6b7b":"dataset.describe().T","fa68e001":"print(pd.isnull(dataset).sum()) # Is there any missing value?","deac616e":"dataset.groupby('Species').size() # Luego lo veo gr\u00e1ficamente","77d76963":"species = dataset['Species'].value_counts()\nspecies = pd.DataFrame(species) # Creating DF","a71e8357":"sns.barplot(x = species.index, y = species['Species'])\nplt.title('Species vs. Quantity', fontsize = 20)\nplt.xlabel('Species', fontsize = 15)\nplt.ylabel('Fish quantity', fontsize = 15)\nplt.show()","aa50e449":"sns.heatmap(dataset.corr(), annot = True, linewidths=.5, cmap='cubehelix')\nplt.title('Correlations', fontsize = 20)\nplt.show()","78d0565b":"f, (ax1, ax2) = plt.subplots(1, 2, sharey = True)\n\nax1.plot(dataset.DimVer, dataset.DimLong, c = 'green')\nax1.set_title('DimVer vs. DimLong', c = 'green')\nax2.scatter(dataset.DimDiag, dataset.DimLong, c='red')\nax2.set_title('DimDiag vs. DimLong', c ='red')\n\nplt.ylabel('DimLong', fontsize = 20)\n\nplt.show()","3f9005c6":"dataset_2 = dataset.copy() # Copying before dropping columns\ndataset_2 = dataset_2.drop(['DimVer','DimDiag'], axis = 1) \n\ndataset_2 = dataset_2[['Species','DimLong','Height','Width','Weight']] # Rearrange columns.\n\ndataset_2.head(3)","1012854a":"stats.probplot(dataset_2['DimLong'].values, dist=\"norm\", plot=plt)\nplt.show()","f6da0bb4":"stats.probplot(dataset_2['Height'].values, dist=\"norm\", plot=plt)\nplt.show()","3b0e85b3":"stats.probplot(dataset_2['Width'].values, dist=\"norm\", plot=plt)\nplt.show()","e29b502d":"sns.boxplot(x=dataset_2['DimLong'], color = 'cyan')\nplt.title('DimLong Boxplot', fontsize = 20)\nplt.show()","eb3d8dd3":"DimLong= dataset_2['DimLong'] # Variable data\nDimLong_Q1 = DimLong.quantile(0.25) # Q1 inf limit\nDimLong_Q3 = DimLong.quantile(0.75) # Q3 sup limit\nDimLong_IQR = DimLong_Q3 - DimLong_Q1 # IQR\nDimLong_lowerend = DimLong_Q1 - (1.5 * DimLong_IQR) # q1 - 1.5 * q1\nDimLong_upperend = DimLong_Q3 + (1.5 * DimLong_IQR) # q3 + 1.5 * q3\n\nDimLong_outliers = DimLong[(DimLong < DimLong_lowerend) | (DimLong > DimLong_upperend)] # Outlier index\nDimLong_outliers","690b94d7":"sns.boxplot(x=dataset_2['Height'], color = 'cyan')\nplt.title('Height boxplot', fontsize = 20)\nplt.show()","85f5f275":"sns.boxplot(x=dataset_2['Width'], color = 'cyan')\nplt.title('Widht boxplot', fontsize = 20)\nplt.show()","ed612efa":"sns.boxplot(x=dataset_2['Weight'], color = 'cyan')\nplt.title('Weight Boxplot', fontsize = 20)\nplt.show()","7a2ed55c":"Peso = dataset_2['Weight'] \nPeso_Q1 = Peso.quantile(0.25) \nPeso_Q3 = Peso.quantile(0.75) \nPeso_IQR = Peso_Q3 - Peso_Q1 \nPeso_lowerend = Peso_Q1 - (1.5 * Peso_IQR) \nPeso_upperend = Peso_Q3 + (1.5 * Peso_IQR) \n\nPeso_outliers = Peso[(Peso < Peso_lowerend) | (Peso > Peso_upperend)] # Outlier Index\nPeso_outliers","df0acc83":"dataset_2 = dataset_2.drop([142,143,144], axis=0)\ndataset_2.describe()","69057767":"correlaciones = sns.pairplot(dataset_2, hue = \"Species\", palette = \"husl\", corner = True)","863267a1":"X = dataset_2.iloc[:,0:4].values \n\ny = dataset_2.iloc[:,-1].values","1321b953":"ct = ColumnTransformer(\n    [('one_hot_encoder', OneHotEncoder(categories='auto'), [0])],   \n    remainder='passthrough')\nX = np.array(ct.fit_transform(X), dtype=np.float)\nnp.set_printoptions(suppress=True)","b5f41c88":"X = X[:,1:]\nX.shape","e62412fb":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0) # Set Splitting\n\nregression = LinearRegression() \nregression.fit(X_train, y_train)","1e120e83":"y_pred = regression.predict(X_test) # Predictions\n\nPrediccion_y = pd.DataFrame({'Prediction_y': y_pred})\nPrediccion_y\n\nY = pd.DataFrame({'Y test': y_test}) # Df \nComparacion = Y.join(Prediccion_y) \nComparacion.head()","fba5341a":"Comparacion.plot(kind = 'bar', figsize=(15,15))\nplt.grid(which = 'both', linestyle = '-', linewidth = '0.5', color = 'green')\nplt.show()","ce432f67":"print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\nprint('R2:', metrics.r2_score(y_test, y_pred))\nAdj_r2 = 1 - (1 - metrics.r2_score(y_test, y_pred)) * (156 - 1) \/ (156 - 9 - 1)\nprint('R2 adjusted', Adj_r2)","3d2ba107":"# Adding np one array (intercept)\n\nX = np.append(np.ones((dataset_2.shape[0],1)).astype(int), values = X  , axis = 1) \n\ndef backwardElimination(x, SL):    \n    numVars = len(x[0])    \n    temp = np.zeros((X.shape[0], X.shape[1])).astype(int)   # Utilizo X con el vector de 1 agregado. \n    for i in range(0, numVars):        # Bucle para todas las variables\n        regressor_OLS = sm.OLS(y, x.tolist()).fit()        \n        maxVar = max(regressor_OLS.pvalues).astype(float)   # Me da el m\u00e1ximo valor de p de las variables\n        adjR_before = regressor_OLS.rsquared_adj.astype(float)  # Me da el valor del R2 ajustado\n        if maxVar > SL:            \n            for j in range(0, numVars - i):  # Eliminaci\u00f3n de variable si el p valor es mayor al SL pero\n                if (regressor_OLS.pvalues[j].astype(float) == maxVar):                    \n                    temp[:,j] = x[:, j]                    \n                    x = np.delete(x, j, 1)                    \n                    tmp_regressor = sm.OLS(y, x.tolist()).fit()                    \n                    adjR_after = tmp_regressor.rsquared_adj.astype(float)                    \n                    if (adjR_before >= adjR_after):      # Tiene en cuenta que el R2 ajustado no sea menor que antes                   \n                        x_rollback = np.hstack((x, temp[:,[0,j]]))                        \n                        x_rollback = np.delete(x_rollback, j, 1)     \n                        print (regressor_OLS.summary())                        \n                        return x_rollback                    \n                    else:                        \n                        continue    \n    regressor_OLS.summary()    \n    return x \n \nSL = 0.05\nX_opt = X[:,:]\nX_Modeled = backwardElimination(X_opt, SL)","db4d2cfe":"residuals = Comparacion['Y test'] - Comparacion['Prediction_y']\ntemp = {'Residuals' : residuals, 'Predicted_y' : Comparacion['Prediction_y']}\nresidual_df = pd.DataFrame(temp)\nresidual_df.head()","a1dbb934":"plt.scatter(residual_df['Residuals'], residual_df['Predicted_y'], c = 'red', s = 25, alpha = 0.8, label = 'Residuals')\nplt.title('Residuals vs. Predicted values', fontsize = 20)\nplt.xlabel('Residuals', fontsize = 15)\nplt.legend()\nplt.ylabel('Predicted values', fontsize = 15)\nplt.show()","303c1432":"poly_reg = PolynomialFeatures(degree = 2) # Depende del set de datos que tenga.\nX_poly = poly_reg.fit_transform(X_train) # Cambia la columna de variables dependientes a polin\u00f3micas\n\nlin_reg_2 = LinearRegression() # Creo el modelo de regresi\u00f3n lineal polin\u00f3mica.\nlin_reg_2.fit(X_poly, y_train)","f6acd267":"X_test_poly = poly_reg.fit_transform(X_test)\ny_pred_pol = lin_reg_2.predict(X_test_poly)","e77737d4":"Prediccion_y = pd.DataFrame({'Prediction_y': y_pred_pol})\n\nY = pd.DataFrame({'Y test': y_test}) # Df \nComparacion = Y.join(Prediccion_y) \nComparacion.head()","1057a147":"Comparacion.plot(kind = 'bar', figsize=(15,15))\nplt.grid(which = 'both', linestyle = '-', linewidth = '0.5', color = 'green')\nplt.show()","cc83d541":"print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_pol))  \nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_pol))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_pol)))\nprint('R2:', metrics.r2_score(y_test, y_pred_pol))","9e951245":"RF_reg = RandomForestRegressor(n_estimators = 1000, random_state = 123)\nRF_reg.fit(X_poly, y_train)\n\ny_pred_rf = RF_reg.predict(X_test_poly)","73a24786":"Prediccion_y = pd.DataFrame({'Prediction_y': y_pred_rf})\n\nY = pd.DataFrame({'Y test': y_test}) # Df con los valores reales y los predichos\nComparacion = Y.join(Prediccion_y) \nComparacion.head()","d3a169e9":"Comparacion.plot(kind = 'bar', figsize=(15,15))\nplt.grid(which = 'both', linestyle = '-', linewidth = '0.5', color = 'green')\nplt.show()","b4a50dff":"print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_rf))  \nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_rf))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_rf)))\nprint('R2:', metrics.r2_score(y_test, y_pred_rf))","32eddb6f":"*Training*","c7963e34":"**Loading and getting to know the dataset**","06a1bf64":"**Dropping one column of the dummy variable to avoid multicollinearity**","35399bcd":"**Conclusion:** They are normally distributed (enough for a linear model to work).","0a78945d":"**Conclusion:** We have reached the best model so far! The score is great and there are no negative predicions.","50acea7d":"**Correlations between variables**","68bac252":"**Species quantity**","3f2e5aae":"- Weight: in grams\n- Lenght1, Lenght2, Lenght3: Different proportions in cm.\n- Height: in cm\n- Width: in cm\n- Species: The only categorical column.","5d6d69bc":"**Conclusion:**\n- The residual distribution is random, so we can rule out autocorrelation and heteroscedasticity issues.","ebae7406":"**Plotting to see this last correlation between proportions.**","b8c29671":"IQR method","06dba504":"**Backward regression:** I will check if the amount of features used was right or there was any in excess.\n\n-  I will add a one array to my independent variables to prepare the data.\n- I will define a function where a feature will be dropped if the p-value is above the SL, but will also analyze if the R2 adjust value decreases or increases. This will keep the optimal number of features.s de R2 ajustado para quedarse con la mejor cantidad de variables independientes \u00f3ptimas.\n- As a previous analysis was made about correlation and multicolinearity there should be no changes in the amount of features.","b6aa5403":"**Polynomial regression**","299269c0":"**Let's visualize the tendency**","dde85ce0":"**Residual analysis**","79c5f044":"**Random Forest Regressor**","6ab5071c":"**Conclusion:** Although the score value decreases, we can see less negative predictions. Let's see what happens with a more robust model.","49b0ed78":"*Height Variable*","3a0f1fc5":"Hi! I'm new to ML so I'd be glad to receive any feedback!","12d5531e":"*DimLong variable*","e7a7bb97":"*Height Variable*","39364231":"**Conclusi\u00f3n:** I will drop DimVer and DimDiag as they give the same amount of information that DimLong.","1e4b2785":"- Weight is correlated to DimVer, DimDia, DimLong, Height and Width (> 0.7).\n- DimVer and DimDiag are correlated with DimLong: **multicollinearity.**","7c775316":"*Predictions*","b3fb071c":"*Weight Variable*","d268dbd1":"*The analysis objective is to predict different types of fish species weight, given their proportions.* ","9131fdd3":"*DimLong Variable*","2a2345a9":"**Set splitting and Linear Regression model**","4f518be3":"There are 7 species in the set: Bream has the most amount of subjects and Whitefish has the least.","7b448f10":"**OHE**","7472b933":"There are no outliers.","7225b946":"**Conclusi\u00f3n:** The chosen amount of features was the optimal in terms of R2 adjusted and p-values.","032f4f21":"There are no outliers.","4112a7c1":"**Predicting and visualizing**","7d1ace54":"Thanks for reaching the end!!","eada5633":"**Are the independent variables Normally distributed? (this would improve the precision of a linear model)**\n\n- Using Q-Q method, I'll generate a normally distributed set (ideal), and will compare it with mine.\n- As the dataset is relatively small, there will be deviations to the normal distribution at the beginning and ending of the set.","0a3baf8a":"There are no missing values in the set","40cebebd":"**Outlier analysis**","1de38019":"*Width variable*","aa93b568":"**Splitting variables**","1c6c4a8e":"## Fish weight prediction","50c4104a":"**Conclusion:** Drop subjetcs N\u00b0: 142,143 and 144","9b2d7751":"**Importing modules**","c7864329":"*Width Variable*","7d43caf3":"**Conclusion:**  The model has a high score. However, we can see that some predicted values are far from accurate, given in some cases negative values. Let's see if we can improve this.","78617153":"*There are three outliers*","8fa008f9":"There is an outlier, let's seek and destroy it."}}