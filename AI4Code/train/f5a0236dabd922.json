{"cell_type":{"ae5c472e":"code","146a6000":"code","14539305":"code","4196f8c7":"code","a4d374c2":"code","8ea6373f":"code","de4701da":"code","deb8e46c":"code","0e6ff883":"code","35e354d7":"markdown","8f4332b3":"markdown","e0997bdc":"markdown","f19cf2fd":"markdown","5e3243bd":"markdown","d957c06f":"markdown","69421a61":"markdown","903cb661":"markdown"},"source":{"ae5c472e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","146a6000":"import numpy as np\nimport pandas as pd\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport geopandas as gpd\nimport folium","14539305":"dataset = pd.read_csv(\"..\/input\/crimeanalysis\/crime_by_state_rt.csv\")","4196f8c7":"dataset['STATE\/UT'] = dataset['STATE\/UT'].str.title()\ndataset[\"STATE\/UT\"].unique()","a4d374c2":"# numerical cols\ncols = ['Murder', 'Assault on women', 'Kidnapping and Abduction', 'Dacoity', \n        'Robbery', 'Arson', 'Hurt', 'Prevention of atrocities (POA) Act', \n        'Protection of Civil Rights (PCR) Act', 'Other Crimes Against SCs']","8ea6373f":"# import district level shape files\ndist_gdataset = gpd.read_file('..\/input\/india-district-wise-shape-files\/output.shp')\n\n# group by state\nstates_gdataset = dist_gdataset.dissolve(by='statename').reset_index() \n\n# just select statename and geometry column\nstates_gdataset = states_gdataset[['statename', 'geometry']]","de4701da":"# replace state's name\nstates_gdataset['statename'] = states_gdataset['statename'].replace('Ladakh', 'Jammu & Kashmir')\nstates_gdataset['statename'] = states_gdataset['statename'].replace('Telangana', 'Andhra Pradesh')\nstates_gdataset['statename'] = states_gdataset['statename'].replace('Andaman & Nicobar Islands', 'A & N Islands')\nstates_gdataset['statename'] = states_gdataset['statename'].replace('Chhatisgarh', 'Chhattisgarh')\nstates_gdataset['statename'] = states_gdataset['statename'].replace('Dadra & Nagar Haveli', 'D & N Haveli')\nstates_gdataset['statename'] = states_gdataset['statename'].replace('Orissa', 'Odisha')\nstates_gdataset['statename'] = states_gdataset['statename'].replace('Pondicherry', 'Puducherry')\nstates_gdataset['statename'] = states_gdataset['statename'].replace('NCT of Delhi', 'Delhi')\n\n# group 10 years of data\nstates_dataset = dataset.groupby('STATE\/UT')[cols].sum().reset_index()\nstates_dataset.head()","deb8e46c":"# merge shape file with count file\nstates_full = pd.merge(states_gdataset, states_dataset, left_on='statename', right_on='STATE\/UT', how='left')\nstates_full.head()","0e6ff883":"fig, axes = plt.subplots(3, 4, figsize=(20, 15))\nfig.suptitle('No. of Hate crimes from 2001-2012', fontsize=16)\ncmap = 'bwr'\n\nstates_full.plot(column='Murder', ax=axes[0,0], cmap=cmap)   \naxes[0,0].set_title('Murder')\naxes[0,0].set_axis_off()                                       \n\nstates_full.plot(column='Assault on women', ax=axes[0,1], cmap=cmap)   \naxes[0,1].set_title('Assault on women')\naxes[0,1].set_axis_off()          \n\nstates_full.plot(column='Kidnapping and Abduction', ax=axes[0,2], cmap=cmap)   \naxes[0,2].set_title('Kidnapping and Abduction')\naxes[0,2].set_axis_off()          \n\nstates_full.plot(column='Dacoity', ax=axes[0, 3], cmap=cmap)   \naxes[0, 3].set_title('Dacoity')\naxes[0, 3].set_axis_off()          \n\nstates_full.plot(column='Robbery', ax=axes[1,0], cmap=cmap)   \naxes[1,0].set_title('Robbery')\naxes[1,0].set_axis_off()          \n\nstates_full.plot(column='Arson', ax=axes[1,1], cmap=cmap)   \naxes[1,1].set_title('Arson')\naxes[1,1].set_axis_off()    \n\nstates_full.plot(column='Hurt', ax=axes[1,2], cmap=cmap)   \naxes[1,2].set_title('Hurt')\naxes[1,2].set_axis_off()          \n\nstates_full.plot(column='Prevention of atrocities (POA) Act', ax=axes[1,3], cmap=cmap)   \naxes[1,3].set_title('Prevention of atrocities (POA) Act')\naxes[1,3].set_axis_off()          \n\nstates_full.plot(column='Protection of Civil Rights (PCR) Act', ax=axes[2,0], cmap=cmap)   \naxes[2,0].set_title('Protection of Civil Rights (PCR) Act')\naxes[2,0].set_axis_off()  \n\nstates_full.plot(column='Other Crimes Against SCs', ax=axes[2,1], cmap=cmap)   \naxes[2,1].set_title('Other Crimes Against SCs')\naxes[2,1].set_axis_off()  \n\naxes[2,2].set_axis_off()  \n\naxes[2,3].set_axis_off()  \n\nplt.show()","35e354d7":"Data is picked from the National Crime Records Bureau of India.\nRepresenting the state wise and district wise crimes against scheduled castes during the years 2001 to 2012. ","8f4332b3":"***This notebook covers step 4 - We'll perform exploratory analysis using descriptive and graphical statistics to look for potential problems, patterns, classifications, correlations and comparisons in the processed dataset.***","e0997bdc":"<a id=\"1\"><\/a> <br>\n# 1. Importing Libraries And Loading Datasets","f19cf2fd":"Import all required python libraries that will be used for exploratory data analysis and data visualization","5e3243bd":"<a id=\"ch2\"><\/a>\n# A Data Science Framework\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don't put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the actual problem we are trying to solve.\n2. **Gather the Data:** John Naisbitt wrote in his 1984 (yes, 1984) book Megatrends, we are \u201cdrowning in data, yet staving for knowledge.\" So, chances are, the dataset(s) already exist somewhere, in some format. It may be external or internal, structured or unstructured, static or streamed, objective or subjective, etc. As the saying goes, you don't have to reinvent the wheel, you just have to know where to find it. In the next step, we will worry about transforming \"dirty data\" to \"clean data.\"\n3. **Prepare Data for Consumption:** This step is often referred to as data wrangling, a required process to turn \u201cwild\u201d data into \u201cmanageable\u201d data. Data wrangling includes implementing data architectures for storage and processing, developing data governance standards for quality and control, data extraction (i.e. ETL and web scraping), and data cleaning to identify aberrant, missing, or outlier data points.\n4. **Perform Exploratory Analysis:** ***Anybody who has ever worked with data knows, garbage-in, garbage-out (GIGO). Therefore, it is important to deploy descriptive and graphical statistics to look for potential problems, patterns, classifications, correlations and comparisons in the dataset. In addition, data categorization (i.e. qualitative vs quantitative) is also important to understand and select the correct hypothesis test or data model.***\n5. **Model Data:** Like descriptive and inferential statistics, data modeling can either summarize the data or predict future outcomes. Your dataset and expected results, will determine the algorithms available for use. It's important to remember, algorithms are tools and not magical wands or silver bullets. You must still be the master craft (wo)man that knows how-to select the right tool for the job. An analogy would be asking someone to hand you a Philip screwdriver, and they hand you a flathead screwdriver or worst a hammer. At best, it shows a complete lack of understanding. At worst, it makes completing the project impossible. The same is true in data modelling. The wrong model can lead to poor performance at best and the wrong conclusion (that\u2019s used as actionable intelligence) at worst.\n6. **Validate and Implement Data Model:** After you've trained your model based on a subset of your data, it's time to test your model. This helps ensure you haven't overfit your model or made it so specific to the selected subset, that it does not accurately fit another subset from the same dataset. In this step we determine if our [model overfit, generalize, or underfit our dataset](http:\/\/docs.aws.amazon.com\/machine-learning\/latest\/dg\/model-fit-underfitting-vs-overfitting.html).\n7. **Optimize and Strategize:** This is the \"bionic man\" step, where you iterate back through the process to make it better...stronger...faster than it was before. As a data scientist, your strategy should be to outsource developer operations and application plumbing, so you have more time to focus on recommendations and design. Once you're able to package your ideas, this becomes your \u201ccurrency exchange\" rate.","d957c06f":"### Load the preprocessed dataset from step 3","69421a61":"**Contents**\n1. [Importing Libraries And Loading Datasets](#1)\n1. [Geographical Visualizations](#2)","903cb661":"<a id=\"2\"><\/a> <br>\n# 2. Geographical Visualizations"}}