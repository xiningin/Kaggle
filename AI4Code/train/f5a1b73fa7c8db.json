{"cell_type":{"d832d311":"code","390b6f53":"code","3513a522":"code","5bc82832":"code","80495d3b":"code","b9c05bdd":"code","dae22b59":"code","661cd7a6":"code","89ee2b69":"code","5d127e4c":"code","1e9d2843":"code","d2ffe9c9":"code","00dbb9c8":"code","152d084f":"code","b441cf43":"code","7895be91":"code","fa8708a5":"code","4e3ed07b":"code","d379acec":"code","0a81f803":"code","21a4e12b":"code","c650dd2b":"code","77222d68":"code","7ba4a244":"code","27111371":"code","32ef0b47":"code","57ef6c8f":"code","f0966bf9":"code","eb3fc967":"code","fdc2aa51":"code","94f5b251":"code","eb7434be":"code","cc5ac904":"code","d98beff0":"code","9ad9cd72":"code","fdfd40ed":"code","68c92e97":"code","13ddee68":"code","759beea8":"code","9537b230":"code","76334b52":"code","d1dc5764":"code","89116d98":"code","19019156":"code","039a7a5c":"code","8ed847d1":"code","8a70785f":"code","dacf0c90":"code","23c62e68":"code","c724c353":"code","2bb31c67":"code","f29adfb6":"code","9fd88603":"code","be5b6e4e":"code","d5d3af16":"code","d8c46080":"code","5bfb342e":"code","8a364de7":"code","b70513a9":"code","49c0f801":"code","4ca3221e":"code","9b9bd53c":"code","9faf5a0d":"code","ed7f17a5":"code","0b02cc0f":"code","46084f12":"code","e9e98840":"code","8aa5a7f0":"code","91ee21d7":"code","09c0e616":"code","ac09d901":"code","87aa6ca5":"markdown","185c3c59":"markdown","cbc22ff6":"markdown","00198412":"markdown","03937eb8":"markdown","b473da1b":"markdown","1a78a08f":"markdown","a352b8c8":"markdown","565b4bbd":"markdown","65d428bd":"markdown","3bb70b6a":"markdown","addbebc4":"markdown","e39dc733":"markdown","5c83e43c":"markdown","9abb60b8":"markdown","f6698da3":"markdown","33e50c3a":"markdown","282dd9dc":"markdown","2b5207e4":"markdown","69c72941":"markdown","0782eb81":"markdown","45ae6e94":"markdown","74f4efb1":"markdown","41f68897":"markdown","9b8e8a58":"markdown","3eb635e8":"markdown","87380ada":"markdown","e240dfe9":"markdown","07d6df19":"markdown","f661cfea":"markdown","b09978d3":"markdown","7aa63851":"markdown","6157370c":"markdown","daa3029e":"markdown","795dfc61":"markdown","1d9560f3":"markdown","dd9c71be":"markdown","643a9074":"markdown","08a79c2d":"markdown","5a37b25c":"markdown","574cd4bd":"markdown","31a1a744":"markdown","e285a0ea":"markdown","d56ac4f9":"markdown","1b755b8f":"markdown","85bfc6dd":"markdown","62c1f3eb":"markdown","063d020c":"markdown","3c035b8d":"markdown","9b49aba1":"markdown","6f10dd5b":"markdown","47cd1e05":"markdown","3b88ea91":"markdown","b0a6c096":"markdown","3904c54e":"markdown","b828e720":"markdown","c6492dde":"markdown","8ba10fed":"markdown","ea7bda37":"markdown","d8102c59":"markdown","0a460780":"markdown","05e69bc9":"markdown","3929a26e":"markdown","a69bf4f9":"markdown","cc2a0486":"markdown","ad6092f7":"markdown"},"source":{"d832d311":"from sklearn import datasets\nimport numpy as np","390b6f53":"iris = datasets.load_iris()","3513a522":"X = iris.data[:,[2,3]]\ny = iris.target\n\nprint('Class labels:', np.unique(y))","5bc82832":"from sklearn.model_selection import train_test_split","80495d3b":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 1, stratify = y)","b9c05bdd":"print('Labels counts in y:', np.bincount(y))\nprint('Labels counts in y_train', np.bincount(y_train))\nprint('Labels counts in y_test', np.bincount(y_test))","dae22b59":"from sklearn.preprocessing import StandardScaler","661cd7a6":"sc = StandardScaler()\nsc.fit(X_train)\n\nX_train_std = sc.transform(X_train)\nX_test_std = sc.transform(X_test)","89ee2b69":"from sklearn.linear_model import Perceptron","5d127e4c":"ppn = Perceptron(max_iter=40, eta0 = 0.1, random_state=1)\nppn.fit(X_train_std,y_train)","1e9d2843":"y_pred = ppn.predict(X_test_std)","d2ffe9c9":"print('Miscalssified samples: %d'%(y_test != y_pred).sum())","00dbb9c8":"from sklearn.metrics import accuracy_score","152d084f":"print('Accuracy: {0:.2f}'.format(accuracy_score(y_test,y_pred)))","b441cf43":"#alternative\nprint('Accuracy: {0:.2f}'.format(ppn.score(X_test_std,y_test)))","7895be91":"from matplotlib.colors import ListedColormap\nimport matplotlib.pyplot as plt","fa8708a5":"def plot_decision_regions(X, y, classifiers, test_idx = None, resolution = 0.02):\n    \n    markers = ('s','x','o','^','v')\n    colors = ('red','blue','lightgreen','gray','cyan')\n    cmap = ListedColormap(colors[:len(np.unique(y))])\n    \n    x1_min, x1_max = X[:,0].min() - 1, X[:,0].max() + 1\n    x2_min, x2_max = X[:,1].min() - 1, X[:,1].max() + 1\n    \n    xx1, xx2 = np.meshgrid(\n        np.arange(x1_min, x1_max, resolution),\n        np.arange(x2_min, x2_max, resolution)\n    )\n    \n    Z = classifiers.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n    Z = Z.reshape(xx1.shape)\n    \n    plt.contourf(xx1, xx2, Z, cmap = cmap, alpha = 0.3)\n    plt.xlim(xx1.min(), xx1.max())\n    plt.ylim(xx2.min(), xx2.max())\n    \n    for idx, cl in enumerate(np.unique(y)):\n        plt.scatter(x = X[y==cl, 0], y = X[y==cl, 1], alpha = 0.8, c = colors[idx], marker = markers[idx], label = cl, edgecolor='black')\n        \n    if test_idx:\n        X_test, y_test = X[test_idx, :], y[test_idx]\n        \n        plt.scatter(X_test[:,0], X_test[:,1], c = '', edgecolor='black', alpha = 1.0, linewidth = 1, marker = 'o', s = 100, label = 'test set')","4e3ed07b":"X_combined_std = np.vstack([X_train_std,X_test_std])\ny_combined = np.hstack([y_train,y_test])","d379acec":"plot_decision_regions(X = X_combined_std, y = y_combined, classifiers= ppn, test_idx= range(105,150))\nplt.xlabel('petal length [stanarized]')\nplt.ylabel('petal width [standarized]')\nplt.legend(loc = 'upper left')\nplt.show()","0a81f803":"def sigmoid(z):\n    return 1.0 \/ (1.0 + np.exp(-z))","21a4e12b":"z = np.arange(-7,7,0.1)\nphi_z = sigmoid(z)\nplt.plot(z,phi_z)\nplt.axvline(0.0,c = 'k')\nplt.ylim(-0.1,1.1)\nplt.xlabel('z')\nplt.ylabel('$\\phi (z)$')\nplt.yticks([0.0,0.5,1.0])\nax = plt.gca()\nax.yaxis.grid(True)","c650dd2b":"def cost_1(z):\n    return -np.log(sigmoid(z))\n\ndef cost_0(z):\n    return -np.log(1 - sigmoid(z))","77222d68":"z = np.arange(-10,10,0.1)\nphi_z = sigmoid(z)","7ba4a244":"c1 = [cost_1(x) for x in z] \nc2 = [cost_0(x) for x in z]","27111371":"plt.plot(phi_z, c1, label = 'J(w) if y = 1')\nplt.plot(phi_z, c2, label = 'J(w) if y = 0', color = 'red')\nplt.legend(loc = \"upper left\")\nplt.show()","32ef0b47":"class LogisticRegressionGD(object):\n    \n    def __init__(self, epoch = 50, learning_rate = 0.01, random_state = 1):\n        self.epoch = epoch\n        self.learning_rate = learning_rate\n        self.random_state = random_state\n        \n    def net_input(self, X):\n        return np.dot(X,self.w_[1:]) + self.w_[0]\n    \n    def predict(self, X):\n        return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)\n    \n    def activation(self,z):\n        return 1. \/ (1. + np.exp(-z))\n        \n    def fit(self, X, y):\n        rgen = np.random.RandomState(self.random_state)\n        self.w_ = rgen.normal(loc = 0.0, scale = 0.01, size = 1 + X.shape[1])\n        self.costs_ = []\n        \n        for i in range(self.epoch):\n            net_input = self.net_input(X)\n            output = self.activation(net_input)\n            errors = (y - output)\n            self.w_[1:] += self.learning_rate * X.T.dot(errors)\n            self.w_[0] += self.learning_rate * errors.sum()\n            \n            cost = (-y.dot(np.log(output))) - ((1 - y).dot(np.log(1-output)))\n            self.costs_.append(cost)\n            \n        return self\n            ","57ef6c8f":"X_train_01_subset = X_train[(y_train == 0) | (y_train == 1)] #because our model is binary model, so we are taking only two classes\ny_train_01_subset = y_train[(y_train)==0 | (y_train == 1)]","f0966bf9":"print('The shape of our X and y is', X_train_01_subset.shape, y_train_01_subset.shape)","eb3fc967":"model = LogisticRegressionGD(learning_rate=0.05, epoch = 1000)","fdc2aa51":"model.fit(X_train_01_subset, y_train_01_subset)","94f5b251":"plot_decision_regions(X_train_01_subset,y_train_01_subset, model)","eb7434be":"from sklearn.linear_model import LogisticRegression","cc5ac904":"model = LogisticRegression(C=100.0, random_state=1) #X_train_std is standarize data, rememver standarization help in gradient descent optimization, it help in converging earlier\nmodel.fit(X_train_std, y_train)","d98beff0":"plot_decision_regions(X_combined_std, y_combined, model, range(105,150))\nplt.xlabel('Petal length [standarized]')\nplt.ylabel('Petal length [standarized]')\nplt.legend(loc = 'upper left')\nplt.show()","9ad9cd72":"model.predict_proba(X_test_std[:3,:])\n","fdfd40ed":"model.predict_proba(X_test_std[:3,:]).sum(axis=1) #total probability sum will be 1","68c92e97":"model.predict_proba(X_test_std[0,:].reshape(1,-1))","13ddee68":"model.coef_","759beea8":"#Plotting the L2-regularization path for the two weight coefficients:\n#for visualising how decreasing C will increase the strength of regularization\nweights, params = [], []\n\nfor c in np.arange(-5,5):\n    model = LogisticRegression(C=10. ** c, random_state=1)\n    model.fit(X_train_std, y_train)\n    weights.append(model.coef_[1]) # we are considering weight of our class 1 in iris datasets \n    params.append(10.**c)","9537b230":"weights = np.array(weights)\nplt.plot(params, weights[:,0], label = 'petal length')\nplt.plot(params, weights[:,1], label = 'petal width', linestyle = '--')\nplt.ylabel('weights coefficient')\nplt.xlabel('C')\nplt.legend(loc = 'upper left')\nplt.xscale('log')\nplt.show()","76334b52":"from sklearn.svm import SVC","d1dc5764":"#initializing the model, here C is the inverse of regularization parameter\nmodel = SVC(kernel='linear', C = 1.0, random_state=1)\n\n#fitting the model\nmodel.fit(X_train_std, y_train)","89116d98":"plot_decision_regions(X_combined_std, y_combined, model, range(105,150))\nplt.xlabel('petal length [standarized]')\nplt.ylabel('petal width [standarized]')\nplt.legend(loc = 'upper left')\nplt.show()","19019156":"#creating nonlinear classification data\n\nnp.random.seed(1)\n\nX_xor = np.random.randn(200,2)\ny_xor = np.logical_xor(X_xor[:,0] > 0,\n                       X_xor[:, 1] > 0\n                      )\n\n\ny_xor = np.where(y_xor, 1 ,-1)\n","039a7a5c":"plt.scatter(X_xor[y_xor == 1, 0], X_xor[y_xor==1, 1], c = 'b', marker = 'x', label = '1')\nplt.scatter(X_xor[y_xor == -1, 0], X_xor[y_xor == -1, 1], c = 'r', marker = 's', label = '1')\nplt.xlim([-3,3])\nplt.ylim([-3,3])\nplt.legend(loc = 'best')\nplt.show()","8ed847d1":"model = SVC(kernel='rbf', random_state=1, gamma=0.1, C = 10.0)\nmodel.fit(X_xor, y_xor)\n","8a70785f":"plot_decision_regions(X_xor, y_xor, model)\nplt.legend(loc = 'upper left')\nplt.show()","dacf0c90":"model = SVC(kernel='rbf', random_state=1, gamma=10, C = 10.0)\nmodel.fit(X_xor, y_xor)","23c62e68":"plot_decision_regions(X_xor, y_xor, model)\nplt.legend(loc = 'best')\nplt.show()","c724c353":"#visualizing different impurity measures for suppose class 1 in binary class classification p is the probaility of class 1, 1-p is the probaility of other class.\n\ndef gini(p):\n    return 2 * ( p - p ** 2)\n\ndef entropy(p):\n    return -p * np.log2(p) - (1 - p) * np.log2(1-p)\n\ndef error(p):\n    return 1 - np.max([p,1-p])\n","2bb31c67":"x =  np.arange(0.0, 1.0, 0.01) #probability have range 0 to 1 inclusive.\n\nent = [entropy(p) if p!=0 else None for p in x]\nsc_ent = [e * 0.5 if e else None for e in ent] #scaled entropy\nerr = [error(i) for i in x]\n","f29adfb6":"fig = plt.figure()\nax = plt.subplot(111)\n\nfor i, lab, ls, c in zip([ent,sc_ent,gini(x),err],['Entropy', 'Entropy (scaled)', 'Gini Impurity', 'Misclassification Error'],['-','-','--','-.'],['black','lightgray','red','green','cyan']):\n    line = ax.plot(x,i,label = lab, linestyle = ls, lw = 2, color = c)\nax.legend(loc = 'upper center', ncol= 5, bbox_to_anchor = [0.5,1.15])\nax.axhline(y = 0.5, linewidth = 1, color = 'k', linestyle = '--')\nax.axhline(y = 1, linewidth = 1, color = 'k', linestyle = '--')\nplt.ylim([0,1.1])\nplt.xlabel('p(i=1)')\nplt.ylabel('Impurity Index')\nplt.show()","9fd88603":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree","be5b6e4e":"model = DecisionTreeClassifier(\n    criterion='gini',\n    max_depth=4,\n    random_state=1\n)","d5d3af16":"model.fit(X_train,y_train)","d8c46080":"X_combined = np.vstack([X_train,X_test])\ny_combined = np.hstack([y_train,y_test])","5bfb342e":"plot_decision_regions(X_combined,y_combined,model, range(105,150))\nplt.xlabel('petal length [cm]')\nplt.ylabel('petal width [cm]')\nplt.legend(loc = 'upper left')\nplt.show()","8a364de7":"fig = plt.figure(figsize=  (10,15))\ntree.plot_tree(\n    model,\n    rounded=True,\n    filled=True,\n    feature_names=['petal length [cm]', 'petal width [cm]'],\n    class_names= ['Setosa', 'Versicolor', 'Virginica'],\n    fontsize=12,\n    impurity=True\n\n)\n\nplt.savefig('decision_tree_1.png')","b70513a9":"#Let's change our depth to 3\nmodel = DecisionTreeClassifier(\n        criterion='gini',\n        max_depth=3,\n        random_state=1\n        \n)","49c0f801":"model.fit(X_train,y_train)","4ca3221e":"plot_decision_regions(X_combined,y_combined, model, range(105,150))","9b9bd53c":"fig = plt.figure(figsize=(10,15))\ntree.plot_tree(\n    model,\n    feature_names=['petal length [cm]', 'petal width [cm]'],\n    class_names=['Setosa','versicolor','Virginica'],\n    rounded = True,\n    filled=True,\n    fontsize=12\n)\nplt.show()","9faf5a0d":"from sklearn.ensemble import RandomForestClassifier","ed7f17a5":"forest = RandomForestClassifier(\n        criterion='gini',\n        n_estimators=25,\n        random_state=1,\n        n_jobs=-1\n)","0b02cc0f":"forest.fit(X_train,y_train)","46084f12":"plot_decision_regions(X_combined, y_combined, forest, range(105,150))\nplt.xlabel('petal length')\nplt.ylabel('petal width')\nplt.legend(loc = 'best')\nplt.show()","e9e98840":"fig = plt.figure(figsize=(10,10))\n\nfor i in range(len(forest.estimators_)):\n    plt.subplot(5,5,i+1)\n    tree.plot_tree(\n            forest.estimators_[i],\n            rounded = True,\n            filled = True\n    )\nplt.savefig('RandomForest.png')","8aa5a7f0":"from sklearn.neighbors import KNeighborsClassifier","91ee21d7":"model = KNeighborsClassifier(\n        n_neighbors=5,\n        p = 2,\n        metric= 'minkowski'\n)","09c0e616":"model.fit(X_train_std, y_train)","ac09d901":"plot_decision_regions(X_combined_std, y_combined, model, range(105,150))\nplt.xlabel('petal length standarized')\nplt.ylabel('petal width standarized')\nplt.legend(loc = 'upper left')\nplt.show()","87aa6ca5":"SVM is all about maximizing the margin between classes or the hyperplane seperating the classes.\n\nThe margin is defined as the distance between the seperating hyperplane (decision-boundary) and the training samples that are closest to this hyperplane, which are so called support vectors.\n\nThe idea behind having Decision boundaries with large margins is that they tend to have a lower generalization error whereas models with small margins are more prone to overfitting.","185c3c59":"one of the benefit of SVM is that it can be easily **kernelized** to solve a nonlinear classification problems.\n\nhttps:\/\/towardsdatascience.com\/kernel-function-6f1d2be6091\n\n**What is kernel  ?**\n\nIn machine learning, a kernel is usually used to refer to the kernel trick, a method of using a linear classifier to solve a non-linear problem. It entails transforming linearly inseparable data to linearly separable ones.\n\nThe kernel function is what is applied on each data instance to map the original non-linear observations into a higher-dimensional space in which they become separable.","cbc22ff6":"$ if y = 1; J(\\phi{(z), y; w)} = -\\log{(\\phi{(z)})} $\n  \n$ if y = 0; J(\\phi{(z), y; w)} = -\\log(1 - \\phi{(z)}) $\n    \n","00198412":"Support Vector Machine (SVM) can be considered as an extension of perceptron.","03937eb8":"# A tour of Machine Learning Classifiers Using scikit-learn","b473da1b":"The activation function that we will be using here is a sigmoid function, our threshold value will be 0.5","1a78a08f":"The random Forest Algorithm can be summarised in four simple steps:-\n\n1. Draw a random **bootstrap** sample of size n (with replacement).\n2. Grow a decision tree from the bootstrap sample. At each node:<br>\n    a. Randomly select d features without replacement.<br>\n    b. Split the node using the feature that provides the best split according to the objective function, Information Gain.\n\n3. Repeat the step 1 and 2 *k* times\n4. Aggregate the prediction by each tree to assign the class label by **majority vote*","a352b8c8":"# Tackling overfitting via regularization","565b4bbd":"# Decision Tree Learning","65d428bd":"# Learning the weights of the Logistic Cost Function","3bb70b6a":"## Sigmoid Function","addbebc4":"$ \\xi $ is called a slack variable, The motivation for introducing the slack variable was that the linear constraints need to be relaxed for nonlinearity separable data to allow the convergence of the optimization in the presence of misclassifications, under appropriate cost penilization.","e39dc733":"Similar to Perceptron and Adaline, the logistic regression model is also a linear model for binary classification that can be extended to mutliclass classification, via the OvR technique (One Vs Rest)","5c83e43c":"**The Curse of Dimensionality**\n<br>\nKNN is very susceptible to overfitting due to curse of dimensionality.\n\nThe COD describes the phenomenon where the feature space becomes increasing sparse (scattered) for an increasing number of dimensions of a fixed-size training dataset. \n\nSince, we can't perform regularization to solve overfitting problem in KNN, here we can use Feature Selection and dimensionality reduction technique such as PCA.","9abb60b8":"# Maximum margin classification with support vector machines","f6698da3":"There are two distance metric that we use in Knn, Euclidean Distance and Manhattan distance.\n\n*Note:- Data should be standarized before using KNN*<br>\n*The right choice of k is (n_neighbors) crucial to find balance between overfitting and underfitting*<br>\n*k value should be odd to avoid the balancing of classess*\n\n**Imp:- KNN get affected by outliers and Imbalance Dataset**","33e50c3a":"To solve a nonlinear problem using an SVM, we would transform the training data onto a higher-dimensional feature space via a mapping function $ \\phi $ and train a linear SVM model to classify the data in this new feature space. Then we can use the same mapping funcion $ \\phi $ to transform new, unseen data to classify it using the linear SVM model.\n\nHowever one problem with this mapping approach is that the construction of the new features is computationally very expensive, especially if we are dealing with high-dimensional data. This is where Kernel-trick comes into the picture. One of the most widely used kernels is the **Radial Basis Function**\n\n**Note:- Feature Scaling should be done before using Radial Basis Function or Gaussian Function**\n\n$ \\sigma ^2 $ High Variance, lower Bias","282dd9dc":"## Dealing with a nonlinearity separable case using slack variables","2b5207e4":"We can observe that as C decreases weight coeffcient shrink thus increasing the regularization.","69c72941":"## Hypothesis\n\n$ \\phi{(z)} = \\frac {1} {1 + \\exp ^{-z}} $\n\nwhere,\n$ z =  W^{T}X $","0782eb81":"a popular machine learning algorithm that perform well most of the time ,due to its low variance and low bias properties.","45ae6e94":"In the resulting plot, we can see that the decision boundary around the classes are much tighter using a relatively large value of $ \\gamma $.\n\nAlthough the model fits the training dataset very well, such a classifier will likely have a high generalization error on unseen data. This illustrates that the $ \\gamma $ paramter also plays an important role in controlling overfitting\n\n**Increaseing gamma increase overfitting**","74f4efb1":"# K-nearest neighbors ","41f68897":"## Maximizing Information Gain","9b8e8a58":"# Logistic Regression with Gradient Descent Optimization","3eb635e8":"scikit-learn Logistic Regression model support multi-class classification through OvR technique.","87380ada":"So , the cost function by applying L2 regularization will become\n\n$ J(w) = \\sum _{i=1} ^{n} [ -y ^{(i)} \\log {(\\phi{(z ^{(i)})})} - (1 - y ^{(i)})\\log {(1 - \\phi{(z ^{(i)}})} ] + \\frac {\\lambda} {2} ||w|| ^2 $","e240dfe9":"Perceptron algorithm never converges on datasets that aren't perfectly linearly separable, which is why the use of the percpetron algorithm is typically not recommended in practice. It is the Biggest Dis-advantage of the Perceptron.","07d6df19":"We can consider Logistic Regression as a type of Adaptive Linear neural Network since it uses activation function, in this case activation function is sigmoid function.","f661cfea":"As we can see the Information gain is simply the difference between Impurity of parent node and the sum of the impurities of child node, the lower the impurity of the child (it is desirable to have lower impurity i.e lower entropy) nodes, the larger the information gain.","b09978d3":"as can be observe in the above plot, we would not be able to separate samples from the positive and negative class very well using a linear hyperplane as a decision boundary via the linear logistic regression or linear SVM model","7aa63851":"# The Graident Descent Learning Algorithm For Logistic Regression\n\nGradient Descent is use to minimize the cost function","6157370c":"# Logistic Regression v\/s Support Vector Machine\n\n<br>\n<em>\n\nIn practical classification tasks, linear logistic regression and linear SVM often yield very similar results.\n\n**Logistic Regression tries to maximize the conditional likelihoods of the training data, which make it more prone to outliers than SVMs, which often care about the points that are closest to the decision boundary (support vectors).**\n\n**On the other hands, logistic regression model is simple and can be easily implemented**\n\n**Logistic Regression Model can be easily updated, which is attractive when working with streaming data.**\n\n<\/em>","daa3029e":"# Random Forests, an Ensemble Bagging approach","795dfc61":"## Visualizing our tree","1d9560f3":"We will discuss following things here:-\n- Introduction to robust and popular algorithms for classification, such as logistic regression, support vector machines, and decision trees\n- Examples and explanations using the scikit-learn machine learning library, which provides a wide variety of machine learning algorithms via user-friendly python API\n- Discussions about the strengths and weakness of classifiers with linear and non-linear decision boundaries","dd9c71be":"Entropy\n\n$ I _H (t) = - \\sum ^c _{(i=1)} p(i|t) log _2 p(i|t) $\n\nwhere,\n\np(i|t) -> it is the proportion of the samples that belong to class c for a particular node t.\n\nThus, entropy is therefore 0 if all samples belong to the same class, truly pure. entorpy is maximal (1) if we have uniform class distribution.\n\n**Note:- Both Gini impurity and entropy typically yields very similar results, though entropy is more computationally expenseive.**","643a9074":"## Cost Function\n\n$ J(w) = \\sum \\frac {1} {2} (\\phi{(z)} - y) ^{2} $\n\n$ J(w) = \\sum _{i=1} ^{n} [ -y ^{(i)} \\log {(\\phi{(z ^{(i)})})} - (1 - y ^{(i)})\\log {(1 - \\phi{(z ^{(i)}})} ] $","08a79c2d":"The objective function of the SVM becomes the maximization of the margin by maximizing $ \\frac {2} {||w||} $ under the constraint that the samples are classified correctly.","5a37b25c":"Here,\n\nf -> feature to perform the split\n\n$ D _p and D _j $ are the dataset of parent and jth child node\n\nI -> Entropy measure or impurity measures\n\n$ N _p $ -> Total number of samples at the parent node\n\n$ N _j $ -> Total number of samples in jth child node","574cd4bd":"The five main steps that are involved in training a machine learning algorithm can be summarised as follows:\n1. Selecting features and collecting training samples.\n2. Choosing a performance metric.\n3. Choosing a classifier and optimization algorithm.\n4. Evaluating the performance of the model.\n5. Tuning the algorithm.","31a1a744":"# Using the kernel trick to find separating hyperplanes in high-dimensional space","e285a0ea":"# Splitting Data For Evaluation","d56ac4f9":"Using the decision algorithm, we start at the tree root and split the data on the feature that results in the largest **Information Gain**. In an iterative process, we can repeat this splitting procedure at each child node until leaves are pure. In practice, this can result in a very deep tree with many nodes, which can easily lead to overfitting. Thus by default Decision Tree model have *Low bias and High Variance*. To resolve this we want to **prune** the tree by setting a limit for the maximal depth of the tree.","1b755b8f":"The weight update in logistic regression is equal to weight update in Adaline. i.e\n\n$ w _j := w _j + n \\sum _{i=1} ^{n} ( y ^{(i)} - \\phi {(z ^{(i)})}) x _j ^{(i)} $","85bfc6dd":"# Solving nonlinear problems using a kernel SVM","62c1f3eb":"We can think of this model as breaking our data by making decision based on asking a series of questions.","063d020c":"# Decision Plot Curves","3c035b8d":"The main advantage of memory-based approach is that the model immediately adapts as we collect new training data. However the down side is that the computational complexity grown linearly with increase in the number of samples.","9b49aba1":"As we can see in above plot our kernel SVM separates the XOR data relatively well. Here gamma can be consider as a cut-off parameter for Gaussian Sphere, What that means is that if $ \\gamma $ increases, will result in tighter or bumpier decision boundary","6f10dd5b":"We can consider Information Gain (IG) as the average of all the Entropy (I), Entropy is the measure of purity of split or randomness.","47cd1e05":"No single classifier works best across all possible scenarios. (No Free Lunch Theorem)\n\nIt is always recommended to compare the performance of at least a handful of different learning algorithms to select the best model for the particular problem; these may differ in the number of features or samples, the amount of noise in a dataset, and whether the classes are linearly separable or not.","3b88ea91":"**overfitting** is a common problem in machine learning, where a model performs well on training data but does not generalize well to unseen data (test data). If a model suffer from overfitting we can also say that model have high variance (there is trade off between bias and variance), which can be cause by having too many parameters (dimensions).\n\n**underfitting** is a problem in which our model doesn not capture the pattern of data, we also can say that model have high bias.\n\none way of finding a good bias-variance tradeoff is to tune the complexity of the model via regularization.\n\n**Regularization** is a very useful method to handle collinearity (hig correlation among feature), filter out noise from data, and eventually prevent overfitting.\n\nThe concept behind regularization is to introduce additional information (bias) to penalize extreme parameter values. There are two type of regularization, L1 and L2, the most common among them is L2 regularization.\n\n$ \\frac {\\lambda} {2} ||w|| ^2 = \\frac {\\lambda} {2} \\sum ^m _{j=1} w ^2 _j $, norm of a vector\n \nHere,$ \\lambda $ is the regularization parameter. By increasing the value of a $\\lambda$, we increase the regularization strength. In scikit learn LogisticRegression Model, C is directly related to $\\lambda$, which is it inverse\n\n**Note** :- For regularization to work properly all features should be on a comparable scale so we must do feature scaling of our data","b0a6c096":"# Choosing a classification algorithm","3904c54e":"# Training a logistic regression model with scikit-learn","b828e720":"The steps followed are:- \n\n1. Choose the number of k and a distance metric.\n2. Find the k-nearest neighbors of the sample that we want to classify.\n3. Assign the class label by majority vote.\n    ","c6492dde":"Stratified sampliling equally divide class labels in training set and testing set.","8ba10fed":"$ IG(D _p, f)  = I(D _p) - \\sum ^m _{(j=1)} \\frac {N _j} {N _p} I (D _j) $","ea7bda37":"## Maximum Margin Intuition","d8102c59":"The three impurity measures or splitting criteria that are commonly used in binary decision trees are **Gini Impurity** $ (I _G) $, **entropy** $ (I _H) $, and the **classification error** $ (I _E) $","0a460780":"remember, sampling with replacement (probability are independant of samples), the samples are independant and have a covariance of zero.\n\n**NOTE:- An advantage of random forest is that we don't have to worry so much about choosing good hyperparameter values.**\n\n**We typically don't need to prune the random forest, also random forest is robust to noise in dataset**\n\n**The only thing we need to take care about is k in step 3, or how many decision tree model we want to generate**\n*Typically, the larger the number of trees (k), the better the performance of the random forest classifier at the expense of an increased computational cost.*","05e69bc9":"# Modelling Class probailites via Logistic Regression","3929a26e":"Please do upvote if you like the kernel","a69bf4f9":"## Building a Decision Tree","cc2a0486":"Another impurity measure is the classification error:-\n\n$ I _E = 1 - max\\{p(i|t)\\} $\n\nThis is useful criteria for **pruning** but not recommended for growing a decision tree, since it is less sensitive to changes in class probabilities of the nodes.","ad6092f7":"KNN is a non-parametric models. It is described as instance-based learning. Model based on instance based learning are characterized by memorizing the dataset."}}