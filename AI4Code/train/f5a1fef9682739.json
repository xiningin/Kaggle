{"cell_type":{"58339ce7":"code","2247713b":"code","74df8a5c":"code","245fc54b":"code","ebfe9089":"code","a7494fa5":"code","14826e2e":"code","f42aa50d":"code","432d7ef5":"code","3ff4f73d":"code","178c267e":"code","e6967d13":"code","80a70e59":"code","f1be60d2":"code","1a7cae4e":"code","49afb74c":"code","55cb9c85":"code","87e516c5":"code","389ad44c":"code","2e454334":"code","f7cd33db":"code","84c27b71":"code","c24422dc":"code","e867c152":"code","ed2fd4d4":"code","b6f52a53":"code","faf5ed55":"code","6821d760":"code","12f7f934":"markdown","fa166428":"markdown","10e46045":"markdown","aa240a75":"markdown","85be9c72":"markdown","6ee9c6f8":"markdown","1f163030":"markdown","9cea37e2":"markdown","d8ba940e":"markdown","b53226e4":"markdown","30c68f9a":"markdown","23d18650":"markdown","5ec102c4":"markdown","89a48551":"markdown","9469d574":"markdown","e96bbcf1":"markdown","9412c2db":"markdown","bc37fbfa":"markdown","1b44b0a4":"markdown","95b03e4f":"markdown","3eb04342":"markdown","1f2c171a":"markdown","c7a45a72":"markdown","01533733":"markdown","56a1948b":"markdown","bc2e7a96":"markdown","2c528620":"markdown","a593f456":"markdown","866be08f":"markdown","e8c581e2":"markdown","ec775bd6":"markdown","e2568dcd":"markdown","b4ddf05e":"markdown","17b19f8c":"markdown","b8fd53b8":"markdown","3c010a03":"markdown","2608178c":"markdown","1dd57378":"markdown","031b5128":"markdown","2303bf0b":"markdown","186b3cef":"markdown","dac742ca":"markdown","d9a3f738":"markdown"},"source":{"58339ce7":"import pydot\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nsns.set()\n\n%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport sklearn\nfrom sklearn import metrics\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import export_graphviz, DecisionTreeClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.exceptions import NotFittedError\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras import optimizers\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\nfrom IPython.display import display","2247713b":"# Some useful functions we'll use in this notebook\ndef display_confusion_matrix(target, prediction, score=None):\n    cm = metrics.confusion_matrix(target, prediction)\n    plt.figure(figsize=(6,6))\n    sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square=True, cmap='Blues_r')\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')\n    if score:\n        score_title = 'Accuracy Score: {0}'.format(round(score, 5))\n        plt.title(score_title, size = 14)\n    classification_report = pd.DataFrame.from_dict(metrics.classification_report(target, prediction, output_dict=True), orient='index')\n    display(classification_report.round(2))\n    \ndef visualize_tree(tree, feature_names):\n    with open(\"dt.dot\", 'w') as f:\n        export_graphviz(tree, out_file=f, feature_names=feature_names)\n    try:\n        subprocess.check_call([\"dot\", \"-Tpng\", \"dt.dot\", \"-o\", \"dt.png\"])\n    except:\n        exit(\"Could not run dot, ie graphviz, to produce visualization\")\n        \ndef draw_missing_data_table(df):\n    total = df.isnull().sum().sort_values(ascending=False)\n    percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    return missing_data","74df8a5c":"# Path of datasets\npath_train = '..\/input\/train.csv'\npath_test = '..\/input\/test.csv'","245fc54b":"# Create dataframe for training dataset and print five first rows as preview\ntrain_df_raw = pd.read_csv(path_train)\ntrain_df_raw.head()","ebfe9089":"# Compute some basical statistics on the dataset\ntrain_df_raw.describe()","a7494fa5":"train_df_raw.info()","14826e2e":"draw_missing_data_table(train_df_raw)","f42aa50d":"figure, axes = plt.subplots(1,1,figsize=(20, 6))\nplot = sns.catplot(x=\"Embarked\", y=\"Fare\", hue=\"Sex\", data=train_df_raw, palette=('Set1'), kind=\"bar\", ax=axes)\nplt.close(plot.fig)\nplt.show()\ndisplay(train_df_raw[train_df_raw['Embarked'].isnull()])","432d7ef5":"# Let's plot some histograms to have a previzualisation of some of the data ...\ntrain_df_raw.drop(['PassengerId'], 1).hist(bins=50, figsize=(20,15))\nplt.show()","3ff4f73d":"def preprocess_data(df):\n    \n    processed_df = df\n        \n    ########## Deal with missing values ##########\n    \n    # As we saw before, the two missing values for embarked columns can be replaced by 'C' (Cherbourg)\n    processed_df['Embarked'].fillna('C', inplace=True)\n    \n    # We replace missing ages by the mean age of passengers who belong to the same group of class\/sex\/family\n    processed_df['Age'] = processed_df.groupby(['Pclass','Sex','Parch','SibSp'])['Age'].transform(lambda x: x.fillna(x.mean()))\n    processed_df['Age'] = processed_df.groupby(['Pclass','Sex','Parch'])['Age'].transform(lambda x: x.fillna(x.mean()))\n    processed_df['Age'] = processed_df.groupby(['Pclass','Sex'])['Age'].transform(lambda x: x.fillna(x.mean()))\n    \n    # We replace the only missing fare value for test dataset and the missing values of the cabin column\n    processed_df['Fare'] = processed_df['Fare'].interpolate()\n    processed_df['Cabin'].fillna('U', inplace=True)\n    \n    ########## Feature engineering on columns ##########\n    \n    # Create a Title column from name column\n    processed_df['Title'] = pd.Series((name.split('.')[0].split(',')[1].strip() for name in train_df_raw['Name']), index=train_df_raw.index)\n    processed_df['Title'] = processed_df['Title'].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    processed_df['Title'] = processed_df['Title'].replace(['Mlle', 'Ms'], 'Miss')\n    processed_df['Title'] = processed_df['Title'].replace('Mme', 'Mrs')\n    processed_df['Title'] = processed_df['Title'].map({\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5})\n    \n    # Filling Age missing values with mean age of passengers who have the same title\n    processed_df['Age'] = processed_df.groupby(['Title'])['Age'].transform(lambda x: x.fillna(x.mean()))\n\n    # Transform categorical variables to numeric variables\n    processed_df['Sex'] = processed_df['Sex'].map({'male': 0, 'female': 1})\n    processed_df['Embarked'] = processed_df['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n    \n    # Create a Family Size, Is Alone, Child and Mother columns\n    processed_df['FamillySize'] = processed_df['SibSp'] + processed_df['Parch'] + 1\n    processed_df['FamillySize'][processed_df['FamillySize'].between(1, 5, inclusive=False)] = 2\n    processed_df['FamillySize'][processed_df['FamillySize']>5] = 3\n    processed_df['IsAlone'] = np.where(processed_df['FamillySize']!=1, 0, 1)\n    processed_df['IsChild'] = processed_df['Age'] < 18\n    processed_df['IsChild'] = processed_df['IsChild'].astype(int)\n    \n    # Modification of cabin column to keep only the letter contained corresponding to the deck of the boat\n    processed_df['Cabin'] = processed_df['Cabin'].str[:1]\n    processed_df['Cabin'] = processed_df['Cabin'].map({cabin: p for p, cabin in enumerate(set(cab for cab in processed_df['Cabin']))})\n    \n    # Create a ticket survivor column which is set to 1 if an other passenger with the same ticket survived and 0 else\n    # Note : this implementation is ugly and unefficient, if sombody found a way to do it easily with pandas (it must be a way), please comment the kernel with your solution !\n    processed_df['TicketSurvivor'] = pd.Series(0, index=processed_df.index)\n    tickets = processed_df['Ticket'].value_counts().to_dict()\n    for t, occ in tickets.items():\n        if occ != 1:\n            table = train_df_raw['Survived'][train_df_raw['Ticket'] == t]\n            if sum(table) != 0:\n                processed_df['TicketSurvivor'][processed_df['Ticket'] == t] = 1\n    \n    # These two columns are not useful anymore\n    processed_df = processed_df.drop(['Name', 'Ticket', 'PassengerId'], 1)    \n    \n    return processed_df","178c267e":"# Let's divide the train dataset in two datasets to evaluate perfomance of the machine learning models we'll use\ntrain_df = train_df_raw.copy()\nX = train_df.drop(['Survived'], 1)\nY = train_df['Survived']\n\nX = preprocess_data(X)\n# We scale our data, it is essential for a smooth working of the models. Scaling means that each columns as a 0 mean and a 1 variance\nsc = StandardScaler()\nX = pd.DataFrame(sc.fit_transform(X.values), index=X.index, columns=X.columns)\n    \n# Split dataset for model testing\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n\nX_train.head()","e6967d13":"# Create and train model on train data sample\nlg = LogisticRegression(solver='lbfgs', random_state=42)\nlg.fit(X_train, Y_train)\n\n# Predict for test data sample\nlogistic_prediction = lg.predict(X_test)\n\n# Compute error between predicted data and true response and display it in confusion matrix\nscore = metrics.accuracy_score(Y_test, logistic_prediction)\ndisplay_confusion_matrix(Y_test, logistic_prediction, score=score)","80a70e59":"dt = DecisionTreeClassifier(min_samples_split=15, min_samples_leaf=20, random_state=42)\ndt.fit(X_train, Y_train)\ndt_prediction = dt.predict(X_test)\n\nscore = metrics.accuracy_score(Y_test, dt_prediction)\ndisplay_confusion_matrix(Y_test, dt_prediction, score=score)","f1be60d2":"visualize_tree(dt, X_test.columns)\n! dot -Tpng dt.dot > dt.png","1a7cae4e":"svm = SVC(gamma='auto', random_state=42)\nsvm.fit(X_train, Y_train)\nsvm_prediction = svm.predict(X_test)\n\nscore = metrics.accuracy_score(Y_test, svm_prediction)\ndisplay_confusion_matrix(Y_test, svm_prediction, score=score)","49afb74c":"rf = RandomForestClassifier(n_estimators=200, random_state=42)\nrf.fit(X_train, Y_train)\nrf_prediction = rf.predict(X_test)\n\nscore = metrics.accuracy_score(Y_test, rf_prediction)\ndisplay_confusion_matrix(Y_test, rf_prediction, score=score)","55cb9c85":"def build_ann(optimizer='adam'):\n    \n    # Initializing our ANN\n    ann = Sequential()\n    \n    # Adding the input layer and the first hidden layer of our ANN with dropout\n    ann.add(Dense(units=32, kernel_initializer='glorot_uniform', activation='relu', input_shape=(13,)))\n    \n    # Add other layers, it is not necessary to pass the shape because there is a layer before\n    ann.add(Dense(units=64, kernel_initializer='glorot_uniform', activation='relu'))\n    ann.add(Dropout(rate=0.5))\n    ann.add(Dense(units=64, kernel_initializer='glorot_uniform', activation='relu'))\n    ann.add(Dropout(rate=0.5))\n    \n    # Adding the output layer\n    ann.add(Dense(units=1, kernel_initializer='glorot_uniform', activation='sigmoid'))\n    \n    # Compiling the ANN\n    ann.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return ann","87e516c5":"opt = optimizers.Adam(lr=0.001)\nann = build_ann(opt)\n# Training the ANN\nhistory = ann.fit(X_train, Y_train, batch_size=16, epochs=30, validation_data=(X_test, Y_test))","389ad44c":"# Predicting the Test set results\nann_prediction = ann.predict(X_test)\nann_prediction = (ann_prediction > 0.5) # convert probabilities to binary output\n\n# Compute error between predicted data and true response and display it in confusion matrix\nscore = metrics.accuracy_score(Y_test, ann_prediction)\ndisplay_confusion_matrix(Y_test, ann_prediction, score=score)","2e454334":"n_folds = 10\ncv_score_lg = cross_val_score(estimator=lg, X=X_train, y=Y_train, cv=n_folds, n_jobs=-1)\ncv_score_dt = cross_val_score(estimator=dt, X=X_train, y=Y_train, cv=n_folds, n_jobs=-1)\ncv_score_svm = cross_val_score(estimator=svm, X=X_train, y=Y_train, cv=n_folds, n_jobs=-1)\ncv_score_rf = cross_val_score(estimator=rf, X=X_train, y=Y_train, cv=n_folds, n_jobs=-1)\ncv_score_ann = cross_val_score(estimator=KerasClassifier(build_fn=build_ann, batch_size=16, epochs=20, verbose=0),\n                                 X=X_train, y=Y_train, cv=n_folds, n_jobs=-1)","f7cd33db":"cv_result = {'lg': cv_score_lg, 'dt': cv_score_dt, 'svm': cv_score_svm, 'rf': cv_score_rf, 'ann': cv_score_ann}\ncv_data = {model: [score.mean(), score.std()] for model, score in cv_result.items()}\ncv_df = pd.DataFrame(cv_data, index=['Mean_accuracy', 'Variance'])\ncv_df","84c27b71":"plt.figure(figsize=(20,8))\nplt.plot(cv_result['lg'])\nplt.plot(cv_result['dt'])\nplt.plot(cv_result['svm'])\nplt.plot(cv_result['rf'])\nplt.plot(cv_result['ann'])\nplt.title('Models Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Trained fold')\nplt.xticks([k for k in range(n_folds)])\nplt.legend(['logreg', 'tree', 'randomforest', 'ann', 'svm'], loc='upper left')\nplt.show()","c24422dc":"class EsemblingClassifier:\n    \n    def __init__(self, verbose=True):\n        self.ann = build_ann(optimizer=optimizers.Adam(lr=0.001))\n        self.rf = RandomForestClassifier(n_estimators=300, max_depth=11, random_state=42)\n        self.svm = SVC(random_state=42)\n        self.trained = False\n        self.verbose = verbose\n        \n    def fit(self, X, y):\n        if self.verbose:\n            print('-------- Fitting models --------')\n        self.ann.fit(X, y, epochs=30, batch_size=16, verbose=0)\n        self.rf.fit(X, y)\n        self.svm.fit(X, y)\n        self.trained = True\n    \n    def predict(self, X):\n        if self.trained == False:\n            raise NotFittedError('Please train the classifier before making a prediction')\n        if self.verbose:\n            print('-------- Making and combining predictions --------')\n        predictions = list()\n        pred_ann = self.ann.predict(X)\n        pred_ann = (pred_ann > 0.5)*1\n        pred_rf = self.rf.predict(X)\n        pred_svm = self.svm.predict(X)\n        for n in range(len(pred_ann)):\n            combined = pred_ann[n] + pred_rf[n] + pred_svm[n]\n            p = 0 if combined == 1 or combined == 0 else 1\n            predictions.append(p)\n        return predictions","e867c152":"ens = EsemblingClassifier()\nens.fit(X_train, Y_train)\nens_prediction = ens.predict(X_test)\nscore = metrics.accuracy_score(Y_test, ens_prediction)\ndisplay_confusion_matrix(Y_test, ens_prediction, score=score)","ed2fd4d4":"test_df_raw = pd.read_csv(path_test)\ntest = test_df_raw.copy()\ntest = preprocess_data(test)\ntest = pd.DataFrame(sc.fit_transform(test.values), index=test.index, columns=test.columns)\ntest.head()","b6f52a53":"# Create and train model on train data sample\nmodel_test = EsemblingClassifier()\nmodel_test.fit(X, Y)\n\n# Predict for test data sample\nprediction = model_test.predict(test)\n\nresult_df = test_df_raw.copy()\nresult_df['Survived'] = prediction\nresult_df.to_csv('submission.csv', columns=['PassengerId', 'Survived'], index=False)","faf5ed55":"result_df[['PassengerId', 'Survived']].head(10)","6821d760":"import scipy.stats as stats\n\nplt.figure(figsize=(20, 8))\nsub = 2*[0.77511] + 3*[0.77990] + 10*[0.78468] + 11*[0.78947] + 2*[0.79425] + 2*[0.79904]\nsns.distplot(sub)\nplt.show()","12f7f934":"![](https:\/\/www.researchgate.net\/profile\/Kiret_Dhindsa\/publication\/323969239\/figure\/fig10\/AS:607404244873216@1521827865007\/The-K-fold-cross-validation-scheme-133-Each-of-the-K-partitions-is-used-as-a-test.png)","fa166428":"![](https:\/\/www.machinelearningtutorial.net\/wp-content\/uploads\/2017\/01\/bias-variance-tradeoff.svg)","10e46045":"## **2. Features engineering** <a id=\"fe\"><\/a>","aa240a75":"## **4. Finding the best model using k-folds cross validation** <a id=\"choose\"><\/a>","85be9c72":"### **3.3 SVM**","6ee9c6f8":"### We are not done yet ! What about the results ? I've tried to make 30 submissions with this classifier, here are the results :\n    \n| Score   \t |   Nb of occurrences \t|\n|---------\t    |  -------------------------- |\n| 0.77511 \t|                2                |\n| 0.77990 \t|                3             \t  |\n| 0.78468 \t|               10               |\n| 0.78947 \t|               11    \t         |\n| 0.79425 \t|                2                |\n| 0.79904 \t|                2     \t          |\n\n\n### Although I know that we can do much better, the 0.79904 still places us in the top 16%. On the other hand, we see that our homemade classifier has reduced the variance so it is reliable and constant in its performance (something that is not tested by the leaderboard but still important for a data sceintist). Moreover, our solution remains simple and accessible even for beginners. \n\n### Thank you for your reading, feel free to fork this kernel and improve it, and enjoy datascience :D","1f163030":"# **Complete Titanic tutorial with ML, NN & Ensembling**","9cea37e2":"## **5. Ensembling: creating a homemade classifier** <a id=\"ensembling\"><\/a>","d8ba940e":"### **3.2 Decision tree**","b53226e4":"### Welcome on this Titanic tutorial ! It is aimed for beginners but whatever your level you could read it, and if you find a way to improve it I encourage you to fork this notebook and contribute by adding a better solution !","30c68f9a":"### Logistic regression is the \"hello world\" of machine learning algorithms. It is very simple to understand how it works, [here](https:\/\/towardsdatascience.com\/understanding-logistic-regression-9b02c2aec102) is a good article which cover theory of this algorithm.","23d18650":"### In this notebook, we are going to predict wether a passenger of the famous boat will survive or not. By doing this, we will go through several topics and fundamental techniques of machine learning. Here is a list of these techniques and some additional resources that you can consult to find out more: ","5ec102c4":"### Both passengers are female who paid 80 dollars as fare for their tickets. Moreover, they have the same ticket and cabin, so they probably had to board at the same place! According to the distribution above, the more probable embarked value for them is Cherbourg (C). We'll replace these two missing values later during features engineering part. \n\n### Finally, let's plot some histograms to visualise the distributions of our variables:","89a48551":"### **Introduction to metrics**","9469d574":"## **6. Apply our homemade model on test dataset and submit on kaggle** <a id=\"submission\"><\/a>","e96bbcf1":"## **3. Try several models** <a id=\"trymodels\"><\/a>","9412c2db":"### **3.5 Artificial neural network**","bc37fbfa":"### To evaluate our models on the test set for this classification problem, we are going to use several metrics, which will be displayed into a confusion matrix (to easily see the false positive and the false negative predicted by the model, i.e. respectivey type I & II errors). From those two types of error, some metrics can be computed : the F1 score, the Recall, the accuracy. You can find on the image below a quick summary of what is a confusion matrix, how to read it and what are those metrics:","1b44b0a4":"### There is 77% of missing data in the cabin column, it's usually way too much for this column to be exploitable, but as we have a small amount of data, we will still try to use it in feature engineering. \n### For the age, we will either interpolate missing values or we will fill it with the mean for the corresponding category (in term of class, age, sex) of passenger. \n### There is only two missing values for the embarked column, let's try to replace it. Below is the distribution of Embarked according to Fare and sex, and the two observations with missing \"Embarked\" value. Let's look at there two observations and choose the best matching embarked value according to their fare value and sex:","95b03e4f":"### SVMs aim at solving classification problems by finding good decision boundaries between two sets of points belonging to two different categories. To understand how it works, you can refer to [this webpage](https:\/\/www.svm-tutorial.com\/2014\/11\/svm-understanding-math-part-1\/).","3eb04342":"### With this first exploration, we can see that :\n\n* Only aproximately 35% of passengers survived ...\n* More than the half of passengers are in the lowest class (pclass = 3)\n* Most of the fare tickets are below 50\n* Majority of passengers are alone (sibsp and parch)\n\n*Note : this EDA is not complete at all since it is not the purpose of this kernel to make a deep exploration of data. However, you can look at my [**EDA kernel**](https:\/\/www.kaggle.com\/nhlr21\/titanic-colorful-eda) for this competition if this interests you.*","1f2c171a":"### Random forest is a robust & practical algorithm based on decision trees. It outperforms almost always the two previous algorithm we saw. If you want to find out more about this model, [here](https:\/\/medium.com\/@williamkoehrsen\/random-forest-simple-explanation-377895a60d2d) is a good start.","c7a45a72":"### Ensembling is the science of combining classifiers to improve the accuracy of a models. Moreover, it diminushes the variance of our model making it more reliable. You can start learning about ensembling [here](https:\/\/towardsdatascience.com\/two-is-better-than-one-ensembling-models-611ee4fa9bd8) !\n\n### We are going to make our own classifier. To do that, we'll create a class with two methods (fit \/ predict, just as the other classifiers we used from sckitlearn and keras). In the fit method, we just train our 3 classifiers on training data. In the predict method, we make a prediction with each of the 3 classifier and combine it : if two or more classifiers classified the passenger as a survivor, our homemade EsembleClassifier classify it as survivor. Else, it'll predict that the passenger did not survived.","01533733":"## **7. Results** <a id=\"results\"><\/a>","56a1948b":"![](https:\/\/i.ytimg.com\/vi\/yuMNWt6S0ZA\/maxresdefault.jpg)","bc2e7a96":"![](https:\/\/image.slidesharecdn.com\/qconrio-machinelearningforeveryone-150826200704-lva1-app6892\/95\/qcon-rio-machine-learning-for-everyone-51-638.jpg?cb=1440698161)","2c528620":"### **3.4 Random forest**","a593f456":"### Neural networks are more complex and more powerful algorithm than standars machine learning, it belongs to deep learning models. To build a neural network, we are going to use Keras. Keras is a high level API for tensorflow, which is a tensor-manipulation framework made by google. Keras allows you to build neural networks by assembling blocks (which are the layers of our neural network). For more details, [here](https:\/\/elitedatascience.com\/keras-tutorial-deep-learning-in-python) is a great keras tutorial. ","866be08f":"### My advice is to group all the transformations to be done on the dataset in a single function. This way, you can apply the same changes to the training dataset and the test dataset easily. Moreover, if you want to add a modification, you'll have to do it only in the function!","e8c581e2":"## **Imports and useful functions**","ec775bd6":"### Let's check which one of our previously implemented model is the best one with this method. We will not only compute the mean but also the variance, because a good model needs to have the lowest possible variance in addition to have a low bias:","e2568dcd":"## **Table of Contents**","b4ddf05e":"*Check out this beautiful distribution of kaggle scoring for our homemade classifier !*","17b19f8c":"### All models seems to have a good accuracy and nearly the same variance, it seems that there is no \"best model\". Indeed, there is no one model which seems truly better than the other. In fact, if we make submissions with all of these models, we will obtain approximately the same score. Moreover, the variance is a little bit too high for saying that these models are reliable: 0.05 variance means that the same model can score 0.75 and 0.8, which is not very convenient.\n### To obtain a better score, we will, in the next part, build our own classifier which will combine predictions from a random forest, an svm classifier and a keras neural networks. The diversity from these 3 very different models will increase the quality of our predictions and reduce the variance !","b8fd53b8":"### Our new model seems to be quite performing ! You can try to train and validate it several time on train_test_split, you'll see that the variance is not high so our model is also quite constant in its performances.\n### Let's try this new model on the test dataset now !","3c010a03":"### The precision we calculated above for those 4 different models does not mean anything. In fact, if we execute each cell again, we could have sightly different accuracy, because we trained again our models ! We need to verify which model has the best accuracy over several training steps ! We can do it using cross validation method, which consists of dividing out training set in k parts (folds) and evaluating k times using successively each part as the test set and the 9 other parts as the training set. Therefore, we can compute a mean error over the 10 trainings of our model:","2608178c":"### Decision tree is a quite intuitive model, easy to vizualize and interpret. Here, we are even going to display our tree to improve our understanding on how the algorithm manage to classify our samples:\n\n### To find out more about decision trees: [DT](https:\/\/medium.com\/@chiragsehra42\/decision-trees-explained-easily-28f23241248)\n","1dd57378":"1. [**Data exploration**](#data_exploration)\n2. [**Feature egineering**](#fe)\n3. [**Try several models**](#trymodels)\n4. [**Choosing the best model**](#choose)\n5. [**Esembling - Homemade classifier**](#ensembling)\n6. [**Submission**](#submission)\n7. [**Results**](#results)  \n","031b5128":"### **3.1 Logistic regression**","2303bf0b":"## **1. Data exploration** <a id=\"data_exploration\"><\/a>","186b3cef":"![title](dt.png)","dac742ca":"### To understand better the above implemented architecture and dive deeply (that's the case to say !) in this exciting field, I highly recommend you to read the great book **Deep Learning with Python** (link at the beginning of the kernel) written by Fran\u00e7ois Chollet, the creator of the keras framework.","d9a3f738":"[*EDA | Data exploration*](https:\/\/medium.com\/python-pandemonium\/introduction-to-exploratory-data-analysis-in-python-8b6bcb55c190)  \n[*Features engineering*](https:\/\/adataanalyst.com\/machine-learning\/comprehensive-guide-feature-engineering\/)  \n[*Evaluating a model over one training | metrics*](https:\/\/machinelearningmastery.com\/metrics-evaluate-machine-learning-algorithms-python\/)  \n[*Evaluating a model over several trainings | k-fold cross validation*](https:\/\/towardsdatascience.com\/train-test-split-and-cross-validation-in-python-80b61beca4b6)  \n[*Neural network with keras*](https:\/\/elitedatascience.com\/keras-tutorial-deep-learning-in-python)  \n[*Deep Learning with python*](https:\/\/www.manning.com\/books\/deep-learning-with-python)  \n[*Ensembling*](https:\/\/mlwave.com\/kaggle-ensembling-guide\/)"}}