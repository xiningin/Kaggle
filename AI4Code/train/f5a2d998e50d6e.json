{"cell_type":{"4b24018a":"code","d87c733e":"code","bd613e90":"code","8d9e7514":"code","747db3b8":"code","c4b01b55":"code","450e1372":"code","8b5d9281":"code","b5dcc123":"code","ddca12c3":"code","9046c609":"code","0523021d":"code","887c957e":"code","bca51f42":"code","8bedfc34":"code","3d3299f9":"code","7bda3f62":"code","3d8747b2":"code","47d8f8c1":"code","249e7c24":"code","08d32cd6":"code","74b85040":"code","c22f4d48":"code","d0f46477":"code","f3383afa":"code","93788c14":"code","8fdaed39":"code","974811a6":"code","34170290":"code","9a8c7d53":"code","7e2cbd48":"code","aa758a19":"code","4b1271af":"code","130240b7":"code","0a61b2d3":"code","9e651bfb":"code","b0828ef2":"code","1617233c":"code","798e12d1":"code","76dfcfb2":"code","6f109e47":"code","d5fd4784":"code","f538cbc8":"code","b9348546":"code","8d825d26":"code","b60666e6":"code","c4fb6a0d":"code","78422519":"code","5710c2b3":"code","41a7d040":"code","cf89cdc8":"markdown","4c55f70f":"markdown","ae9ca692":"markdown","f4846493":"markdown","c364e698":"markdown","c002d808":"markdown","73eb4a28":"markdown","9292296b":"markdown","dbbf747f":"markdown","6a89ea5e":"markdown","ef431ed9":"markdown","b4a4e897":"markdown","b1fd69dd":"markdown","cf20cfc8":"markdown","fdd9a243":"markdown","6e72a020":"markdown"},"source":{"4b24018a":"!conda install gdcm -c conda-forge -y","d87c733e":"! python -m pip install pillow","bd613e90":"import pandas as pd\nimport numpy as np\nimport cv2\nimport os\nimport re\nimport pydicom\nimport warnings\nimport glob\nimport ast\nimport math\nimport wandb\nfrom PIL import Image\nimport albumentations as A\nimport torch\nfrom matplotlib import pyplot as plt\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nwarnings.filterwarnings(\"ignore\")","8d9e7514":"def visualize(**images):\n    \"\"\"PLot images in one row.\"\"\"\n    n = len(images)\n    plt.figure(figsize=(16, 5))\n    for i, (name, image) in enumerate(images.items()):\n        plt.subplot(1, n, i + 1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.title(' '.join(name.split('_')).title())\n        plt.imshow(image)\n    plt.show()","747db3b8":"DIR_INPUT = '..\/input\/siim-covid19-detection\/'\nDIR_TRAIN = '..\/input\/siim-covid19-detection\/train\/'\nDIR_TEST = '..\/input\/siim-covid19-detection\/test\/'","c4b01b55":"#study numbers\nlen(os.listdir(DIR_TRAIN)),len(os.listdir(DIR_TEST))","450e1372":"sample_sub = pd.read_csv(DIR_INPUT+'sample_submission.csv')\nsample_sub = sample_sub.sort_values('id')\nprint(len(sample_sub))\nsample_sub.head(5)","8b5d9281":"image_lel = pd.read_csv(DIR_INPUT+'train_image_level.csv')\nimage_lel = image_lel.sort_values('StudyInstanceUID')\nprint('image,',len(image_lel))\nprint('study,',len(image_lel['StudyInstanceUID'].unique()))\nimage_lel.head(5)","b5dcc123":"#\u8bad\u7ec3\u96c6label\u4fe1\u606f\u3002\nlabel_type = dict()\nfor x in image_lel[['label']].iterrows():\n    label = x[1].values[0].split(' ')[0]\n    if label not in label_type:\n        label_type[label] = 0\n    label_type[label] += 1\nlabel_type","ddca12c3":"study_lel = pd.read_csv(DIR_INPUT+'train_study_level.csv')\nstudy_lel = study_lel.sort_values('id')\nprint(len(study_lel))\nstudy_lel.head(5)","9046c609":"sample_id = \"..\/input\/siim-covid19-detection\/train\/27f8616263ed\/00eab0514ef6\/7e4a4ef3ff9f.dcm\"\ndicom = pydicom.dcmread(sample_id)\ndicom","0523021d":"plt.imshow(dicom.pixel_array, cmap='bone')\nprint(dicom.pixel_array.shape) #\u56fe\u7247\u5c3a\u5bf8","887c957e":"image_lel['boxes'][2498],image_lel['label'][2498]#image label","bca51f42":"image_lel.head(2)","8bedfc34":"study_lel['StudyInstanceUID'] = '-1'\ndef studylevel(x):\n    x['StudyInstanceUID'] = x['id'].split('_')[0]\n    return x\n\nstudy_lel = study_lel.apply(lambda x:studylevel(x), axis = 1)\nstudy_lel.head(2)","3d3299f9":"#\u68c0\u67e5\u6709\u6ca1\u6709\u91cd\u590d\u7684study\u53f7\nflag = study_lel['StudyInstanceUID'].duplicated()\nflag.any()","7bda3f62":"del study_lel['id']\ntrain_df = pd.merge(image_lel,study_lel,how='left',on='StudyInstanceUID')\ntrain_df.head(5)","3d8747b2":"for row in train_df.iterrows():\n    if row[1][0].split('_')[0] == '7e4a4ef3ff9f':\n        print(row)","47d8f8c1":"#\u67e5\u770b\u5355\u68c0\u67e5\u591a\u56fe\u7247\u7684\u60c5\u51b5\ndup = train_df.groupby('StudyInstanceUID').count()>1\ndup = dup[dup['id'] == True].index\nrepeat_df = train_df[train_df['StudyInstanceUID'].isin(dup)]\nprint('\u91cd\u590d\u68c0\u67e5\u7684\u56fe\u7247\u6570\u91cf\uff1a',len(repeat_df))\nrepeat_df.head(6)","249e7c24":"def fix_inverted_radiograms(data, img):\n    '''Fixes inverted radiograms - with PhotometricInterpretation == \"MONOCHROME1\"\n    data: the .dcm dataset\n    img: the .dcm pixel_array'''\n    \n    if data.PhotometricInterpretation == \"MONOCHROME1\":\n        img = np.amax(img) - img\n    \n    img = img - np.min(img)\n    img = img \/ np.max(img)\n    img = (img * 255).astype(np.uint8)\n    \n    return img\ndef get_image_metadata(study_id, df):\n    '''Returns the label and bounding boxes (if any)\n    for a speciffic study id.'''\n    \n    data = df[df[\"StudyInstanceUID\"] == study_id]\n    \n    if data[\"Negative for Pneumonia\"].values == 1:\n        label = \"negative_for_pneumonia\"\n    elif data[\"Typical Appearance\"].values == 1:\n        label = \"typical\"\n    elif data[\"Indeterminate Appearance\"].values == 1:\n        label = \"indeterminate\"\n    else:\n        label = \"atypical\"\n        \n    bbox = list(data[\"boxes\"].values)\n    \n    return label, bbox\ndef return_coords(box):\n    '''Returns coordinates from a bbox'''\n    # Get the list of dictionaries\n    box = ast.literal_eval(box)[0]\n    # Get the exact x and y coordinates\n    x1, y1, x2, y2 = box[\"x\"], box[\"y\"], box[\"x\"] + box[\"width\"], box[\"y\"] + box[\"height\"]\n    # Save coordinates\n    return (int(x1), int(y1), int(x2), int(y2))","08d32cd6":"def show_dcm_info(study_ids, df):\n    '''Show .dcm images along with description.'''\n    wandb_logs = []\n    \n    fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(21,10))\n\n    # Get .dcm paths\n    dcm_paths = [glob.glob(f\"..\/input\/siim-covid19-detection\/train\/{study_id}\/*\/*\")[0]\n                 for study_id in study_ids]\n    datasets = [pydicom.dcmread(path) for path in dcm_paths]\n    images = [apply_voi_lut(dataset.pixel_array, dataset) for dataset in datasets]\n\n    # Loop through the information\n    for study_id, data, img, i in zip(study_ids, datasets, images, range(2*3)):\n        # Fix inverted images\n        img = fix_inverted_radiograms(data, img)\n\n        # Below function available in functions section ;)\n        label, bbox = get_image_metadata(study_id, df)\n        \n        # Check for bounding box and add if it's the case\n        try: \n            # For no bbox, the list is [nan]\n            no_box = math.isnan(bbox[0])\n            pass\n        except TypeError:\n            # Retrieve the bounding box\n            all_coords = []\n            for box in bbox:\n                all_coords.append(return_coords(box))\n\n            for (x1, y1, x2, y2) in all_coords:\n                cv2.rectangle(img, (x1, y1), (x2, y2), (0, 0, 0), 15)\n                cv2.putText(img, label, (x1, y1-14), \n                            cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 0, 0), 4)\n                \n        # Plot the image\n        x = i \/\/ 3\n        y = i % 3\n        \n        axes[x, y].imshow(img, cmap=\"bone\")\n        axes[x, y].set_title(f\"Label: {label} \\n Sex: {data.PatientSex} | Body Part: {data.BodyPartExamined}\", \n                  fontsize=14, weight='bold')\n        axes[x, y].axis('off');\n        \n        # Save to W&B\n        wandb_logs.append(wandb.Image(img, \n                                      caption=f\"Label: {label} \\n Sex: {data.PatientSex} | Body Part: {data.BodyPartExamined}\"))","74b85040":"# \u5178\u578b\u75c7\u72b6\u7684\u6837\u672c\u56fe\u793a\nshow_dcm_info(study_ids=[\"72044bb44d41\", \"5b65a69885b6\", \"6aa32e76f998\",\n                         \"c9ffe6312921\", \"082cafb03942\", \"d3e83031ebea\"], \n              df=train_df)","c22f4d48":"#\u975e\u5178\u578b\u75c7\u72b6\u7684\u6837\u672c\u56fe\u793a\nshow_dcm_info(study_ids=[\"f807cd855d31\", \"8087e3bc0efe\", \"7249de10ed69\",\n                         \"e300a4e86207\", \"4bac6c7da8b8\", \"f2d30ac37f7b\"], \n              df=train_df)","d0f46477":"#\u4e0d\u786e\u5b9a\u75c7\u72b6\u7684\u6837\u672c\u56fe\u793a\nshow_dcm_info(study_ids=[\"b949689a9ef1\", \"fe7e6015560d\", \"feffa20fac13\",\n                         \"747483509d0e\", \"c70369caef91\", \"1e1b4b1b53cb\"], \n              df=train_df)","f3383afa":"# \u65e0\u80ba\u708e\u75c7\u72b6\u7684\u6837\u672c\u56fe\u793a\nshow_dcm_info(study_ids=[\"612ea5194007\", \"db14e640e037\", \"d4ab797396b4\",\n                         \"6ae8a88c4b0c\", \"b3cf474bee3b\", \"0ba55e5422ab\"], \n              df=train_df)","93788c14":"image_ids = train_df['id'].unique()\nvalid_ids = image_ids[-700:]\ntrain_ids = image_ids[:-700]\nlen(train_ids),len(valid_ids)","8fdaed39":"valid_df = train_df[train_df['id'].isin(valid_ids)]\ntrain_df = train_df[train_df['id'].isin(train_ids)]\nvalid_df.head(2)","974811a6":"from torch.utils.data import DataLoader, Dataset","34170290":"img = cv2.imread ('..\/input\/siim-covid19-resized-to-256px-jpg\/train\/000a312787f2.jpg',1)\nimg.shape\nvisualize(\n    image = img,\n)","9a8c7d53":"meta = pd.read_csv('..\/input\/siim-covid19-resized-to-256px-jpg\/meta.csv')\nmeta.head()\n\nmeta_dict = dict()\nfor row in meta.iterrows():\n    meta_dict[row[1][0]] = [row[1][1]\/256,row[1][2]\/256]\n# meta_dict","7e2cbd48":"def img2tensor(img,dtype:np.dtype=np.float32): ## float 32\/16\n    if img.ndim==2 : img = np.expand_dims(img,2)\n    img = np.transpose(img,(2,0,1))\n    return torch.from_numpy(img.astype(dtype, copy=False))\n\ndef get_bbox(img_id, box):\n    box_list= []\n    b_list = box.split(' ')\n    \n    for i in range(len(box.split(' '))\/\/6):\n        box_list.append([float(b_list[i*6+2])\/meta_dict[img_id][1], float(b_list[i*6+3])\/meta_dict[img_id][0],\n                         float(b_list[i*6+4])\/meta_dict[img_id][1], float(b_list[i*6+5])\/meta_dict[img_id][0]])\n#         box_list.append([float(b_list[i*6+2])\/meta_dict[img_id][0], float(b_list[i*6+3])\/meta_dict[img_id][1],\n#                          float(b_list[i*6+4])\/meta_dict[img_id][0], float(b_list[i*6+5])\/meta_dict[img_id][1]])\n    return np.array(box_list).astype('float64')\n\ndef get_image(line):\n    # LINE 4,5,6,7 -> NP\uff0cTA\uff0cIA\uff0cAA\n    factors = line[2].split(' ')\n    \n    num = len(factors)\/\/6\n    \n    return np.array([1 if factors[i*6]=='opacity' else 0 for i in range(num)])\n\n#     return np.array([[line[4], line[5], line[6], line[7],] for i in range(num)]).astype('int64')\n#     return np.array([i-4 for i in range(4,8) if line[i]==1 for j in range(num)]).astype('int64')\n","aa758a19":"train_df.values[0]","4b1271af":"class COVID19Dataset(Dataset):\n    def __init__(self, df, pth, transforms=None):\n        super().__init__()\n        self.ids = df[\"id\"].unique()\n        self.df = df\n        self.pth = pth\n        self.transforms = transforms\n        \n    def __getitem__(self, index):\n        img_id = self.ids[index].split('_')[0]\n        img = cv2.imread (self.pth+img_id+'.jpg',1)\n        \n        line = self.df.values[index]\n        boxes = get_bbox(img_id, line[2])\n        image_labels = None\n        image_confid = None\n        image_labels = get_image(line)\n        \n        target = dict()\n        target['boxes'] = torch.from_numpy(boxes)\n        target['labels'] = torch.from_numpy(image_labels)\n#         target['study_confid'] = torch.from_numpy(study_labels.astype('float32'))\n        \n        return img2tensor(img\/255), target\n\n    def __len__(self):\n        return self.ids.shape[0]\n        \n        \ntrain_dataset = COVID19Dataset(train_df,'..\/input\/siim-covid19-resized-to-256px-jpg\/train\/', True)\n# valid_dataset = COVID19Dataset(valid_df, None)","130240b7":"x,y = train_dataset[0]\nx,y","0a61b2d3":"import torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor","9e651bfb":"model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)","b0828ef2":"num_classes = 2\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\ndevice","1617233c":"train_dataset = COVID19Dataset(train_df,'..\/input\/siim-covid19-resized-to-256px-jpg\/train\/', True)\nvalid_dataset = COVID19Dataset(valid_df,'..\/input\/siim-covid19-resized-to-256px-jpg\/train\/', True)\nlen(train_dataset), len(valid_dataset)","798e12d1":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ntrain_data_loader = DataLoader(\n    train_dataset,\n    batch_size=10,\n    shuffle=True,\n    num_workers=4,\n    collate_fn=collate_fn\n)\n\nvalid_data_loader = DataLoader(\n    valid_dataset,\n    batch_size=10,\n    shuffle=False,\n    num_workers=4,\n    collate_fn=collate_fn\n)","76dfcfb2":"# number = 7\n# images, targets = next(iter(train_data_loader))\n# images = list(image.to(device) for image in images)\n# targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n# boxes = targets[number]['boxes'].cpu().numpy().astype(np.int32)\n# sample = images[number].permute(1,2,0).cpu().numpy()\n# fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\n# for box in boxes:\n#     cv2.rectangle(sample,\n#                   (box[0], box[1]),\n#                   (box[2], box[3]),\n#                   (0, 220, 0), 3)\n    \n# ax.set_axis_off()\n# ax.imshow(sample)","6f109e47":"class Averager:\n    def __init__(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n\n    def send(self, value):\n        self.current_total += value\n        self.iterations += 1\n\n    @property\n    def value(self):\n        if self.iterations == 0:\n            return 0\n        else:\n            return 1.0 * self.current_total \/ self.iterations\n\n    def reset(self):\n        self.current_total = 0.0\n        self.iterations = 0.0","d5fd4784":"model.to(device)\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)\nnum_epochs = 10","f538cbc8":"loss_hist = Averager()\nitr = 1\n\nfor epoch in range(num_epochs):\n    loss_hist.reset()\n    \n    print(f'epoch {epoch} starting ...')\n    for images, targets in train_data_loader:\n        \n        images_t = list(image.to(device) for image in images)\n        targets_t = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        \n        loss_dict = model(images_t, targets_t)\n#         print(loss_dict)\n        losses = sum(loss for loss in loss_dict.values())\n        loss_value = losses.item()\n\n        loss_hist.send(loss_value)\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n        if itr % 100 == 0:\n            print(f\"Iteration #{itr} loss: {loss_hist.value}\")\n        itr += 1\n#         break\n    \n    # update the learning rate\n    if lr_scheduler is not None:\n        lr_scheduler.step()\n\n    print(f\"Epoch #{epoch} loss: {loss_hist.value}\")\n    print(\"Saving epoch's state...\")\n    torch.save(model.state_dict(), f\"model_state_epoch_{epoch}.pth\")\n    break","b9348546":"model.eval()\nres = model([images_t[0]])\nres","8d825d26":"targets_t[0]","b60666e6":"TH = 0.6\ndef clean_res(res, th):\n    for re in res:\n        i = 0\n        for i in range(len(re['scores'])):\n            if re['scores'][i] < th:\n                re['boxes'] = re['boxes'][:i]\n                re['labels'] = re['labels'][:i]\n                re['scores'] = re['scores'][:i]\n                break\n                \n        if i == 0:\n            re['boxes'] = torch.Tensor([[0,0,1,1]])\n            re['labels'] = torch.Tensor([0])\n            re['scores'] = torch.Tensor([1])\n    return res\n\ndef record(res, row, test_name):\n    PredictionString = ''\n    for i in range(len(res['labels'])):\n        if res['labels'][i] == 1:\n            PredictionString += 'opacity '\n            PredictionString += str(res['scores'][i].item()) + ' '\n            \n            for bx in range(len(res['boxes'][i])):\n                if bx%2 == 0:\n                    PredictionString += str(res['boxes'][i][bx].item()*meta_dict[test_name][1]) + ' '\n                else:\n                    PredictionString += str(res['boxes'][i][bx].item()*meta_dict[test_name][0]) + ' '\n        else:\n            PredictionString += 'none 1 0 0 1 1'\n            break\n    row['PredictionString'] = PredictionString\n    return row\n        \ndef predict(row):\n    test_type = row['id'].split('_')[1]\n    if test_type == 'study': return row\n        \n    test_name = row['id'].split('_')[0]\n    img = cv2.imread (test_path+test_name+'.jpg',1)\n    img = img2tensor(img\/255)\n    img = img.to(device)\n    res = model([img])\n    res = clean_res(res, TH)\n    \n    row = record(res[0], row, test_name)\n    return row","c4fb6a0d":"number = 0\nimages = [images_t[0].cpu()]\ntargets = [targets_t[0]]\n\nimages = list(image.to(device) for image in images)\ntargets = [{k: v.to(device) for k, v in t.items()} for t in targets]\nboxes = targets[number]['boxes'].cpu().detach().numpy().astype(np.int32)\nsample = images[number].permute(1,2,0).cpu().numpy()\nfig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\nfor box in boxes:\n    cv2.rectangle(sample,\n                  (box[0], box[1]),\n                  (box[2], box[3]),\n                  (0, 220, 0), 3)\n    \nax.set_axis_off()\nax.imshow(sample)","78422519":"number = 0\nimages = [images_t[0].cpu()]\ntargets = clean_res(res, 0.6)\n\nimages = list(image.to(device) for image in images)\ntargets = [{k: v.to(device) for k, v in t.items()} for t in targets]\nboxes = targets[number]['boxes'].cpu().detach().numpy().astype(np.int32)\nsample = images[number].permute(1,2,0).cpu().numpy()\nfig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\nfor box in boxes:\n    cv2.rectangle(sample,\n                  (box[0], box[1]),\n                  (box[2], box[3]),\n                  (0, 220, 0), 3)\n    \nax.set_axis_off()\nax.imshow(sample)","5710c2b3":"# model.eval()\nsample_sub = pd.read_csv('..\/input\/siim-covid19-detection\/sample_submission.csv')\ntest_path = '..\/input\/siim-covid19-resized-to-256px-jpg\/test\/'\n\nsample_sub = sample_sub.apply(lambda x:predict(x), axis = 1)","41a7d040":"sample_sub.to_csv('submission.csv')","cf89cdc8":"# \u6570\u636e\u589e\u5f3a","4c55f70f":"data_link: thanks to https:\/\/www.kaggle.com\/xhlulu\/siim-covid19-resized-to-256px-jpg","ae9ca692":"\u4e00\u4e2a\u751f\u75c5\u7684\u75c5\u4eba\uff0c\u4e0d\u4e00\u5b9a\u6bcf\u5f20\u56fe\u7247\u90fd\u80fd\u89c2\u5bdf\u5230\u75c7\u72b6\u3002\u66f4\u6709\u751a\u8005\uff0c\u6bcf\u5f20\u56fe\u7247\u90fd\u6ca1\u6709\u95ee\u9898\uff0c\u8fd8\u88ab\u8bca\u65ad\u6709\u6bdb\u75c5\u3002\u75c5\u4eba\u591a\u6b21\u7167\u7247\u7684\u539f\u56e0\uff0c\u89c2\u5bdf\u89c4\u5f8b\uff0c\u5927\u81f4\u5224\u65ad\u4e3a\u75c5\u4eba\u4e00\u6b21\u53d6\u56fe\u672a\u68c0\u6d4b\u5230\u7ed3\u679c\uff0c\u91cd\u65b0\u518d\u53d6\u56fe\u3002","f4846493":"# \u6570\u636e\u96c6","c364e698":"**study,series,image,patients**\u5bf9\u5e94\u5173\u7cfb\uff1a \u4e00\u4e2a\u60a3\u8005\uff08patient\uff09\u53ef\u4ee5\u505a\u591a\u6b21\u68c0\u67e5\uff08study\uff09\uff0c\u4e00\u6b21\u68c0\u67e5\u5305\u542b\u591a\u4e2a\u68c0\u67e5\u90e8\u4f4d\uff08series\uff09\uff0c\u800c\u6bcf\u4e2a\u68c0\u67e5\u90e8\u4f4d\u90fd\u6709\u4e00\u5f20\u6216\u591a\u5f20\u76f8\u5e94\u7684\u5f71\u50cf\u56fe\u50cf\uff08image\uff09\uff08https:\/\/blog.csdn.net\/qq_38330148\/article\/details\/107094334\uff09 \n\n\u8bad\u7ec3\u96c6\uff1a\u51716334\u4e2a\u56fe\u7247\uff0c6054\u6b21\u68c0\u67e5\u3002\u6709\u90e8\u5206\u5355\u6b21\u68c0\u67e5\u5b58\u5728\u591a\u4e2a\u56fe\u7247\uff08\u591a\u4e2a\u90e8\u4f4d\u7684\u7167\u7247\uff09\uff0c\u53c2\u8003\u4f8b\u5b50\uff1a\u6d4b\u8bd5\u96c6\u4e2d\u76840847751da0f7\u53f7\u68c0\u67e5\u3002","c002d808":"# \u6a21\u578b","73eb4a28":"\u56fe\u50cf\u6587\u4ef6\u53ef\u89c6\u5316\u89c2\u5bdf","9292296b":"\u6d4b\u8bd5\u7ed3\u679c\u4e2d\u65e2\u5305\u542bstudy\u7ed3\u679c\uff0c\u4e5f\u5305\u542bimage\u7ed3\u679c\u3002","dbbf747f":"# \u524d\u8a00\uff1a\n**\u4ece\u96f6\u5f00\u59cb\u6478\u7d22COVID19\u6bd4\u8d5b\u5965\u4e49,\u9519\u8bef\u5730\u65b9\u8bf7\u968f\u65f6\u6307\u51fa\u3002**\n\n\u6211\u4eec\u662f\u77e5\u8bc6\u7684\u642c\u8fd0\u5de5\uff0c\u4ea4\u6d41\u8d34\uff0c\u76ee\u524d\u8fd8\u6709\u5f88\u591a\u4e0d\u61c2\u7684\uff0c\u53ef\u968f\u65f6\u4ea4\u6d41\u3002","6a89ea5e":"# \u9884\u6d4b\u7ed3\u679c","ef431ed9":"code references: https:\/\/www.kaggle.com\/pestipeti\/vinbigdata-fasterrcnn-pytorch-train\/notebook","b4a4e897":"**\u68c0\u67e5\u89d2\u5ea6\u7814\u7a76\uff1a\u72af\u4e86\u4ec0\u4e48\u6bdb\u75c5\u3002**\n\n\u56db\u7c7b\uff1aNegative for Pneumonia\uff0cTypical Appearance\uff0cIndeterminate Appearance\uff0cAtypical Appearance\n\n\u5927\u6982\u610f\u601d\u5c31\u662f\uff1a\u6ca1\u6bdb\u75c5\uff0c\u5178\u578bCOVID19\u80ba\u708e, \u672a\u786e\u5b9a\u7684\u6837\u5f0f\uff0c \u975e\u5178\u578bCOVID19\u80ba\u708e\u6837\u5f0f\n\n\u6211\u7684\u7406\u89e3\uff1aopacity\u8bf4\u660e\u56fe\u7247\u53d1\u73b0\u4e86\u95ee\u9898\uff0c \u4e00\u4e2a\u6216\u591a\u4e2a\u56fe\u7247\u7efc\u5408\u8bca\u65ad\u5224\u65ad\u75c5\u4eba\u662f\u5426\u5b58\u5728\uff1a\u5178\u578bCOVID19\u80ba\u708e, \u672a\u786e\u5b9a\u7684\u6837\u5f0f\uff0c \u975e\u5178\u578bCOVID19\u80ba\u708e\u6837\u5f0f\u3002 \u662f\u5426\u5982\u6b64\u8fd8\u9700\u8fdb\u4e00\u6b65\u53d1\u6398\u3002","b1fd69dd":"# \u6837\u672c\u8be6\u60c5\uff1a\n\nBody Part Examined\uff1a \u6d4b\u8bd5\u90e8\u4f4d; Study Instance UID: Study\u7f16\u53f7\uff1b Series Instance UID\uff1a Series\u7f16\u53f7\uff1b SOP Instance UID\uff1a image\u7f16\u53f7\uff1b Patient's Sex\uff1a \u60a3\u8005\u6027\u522b\uff1b Patient ID\uff1a \u60a3\u8005\u7f16\u53f7\n","cf20cfc8":"code references: https:\/\/www.kaggle.com\/andradaolteanu\/siim-covid-19-box-detect-dcm-metadata","fdd9a243":"# \u8bad\u7ec3","6e72a020":"**\u56fe\u50cf\u89d2\u5ea6\u7814\u7a76\uff1a\u56fe\u50cf\u6709\u65e0\u5f02\u5e38\u60c5\u51b5**\n\nopacity: \u80ba\u5185\u78e8\u73bb\u7483\u5bc6\u5ea6\u5f71\u3002\n\n\u80ba\u5185\u78e8\u73bb\u7483\u5bc6\u5ea6\u5f71(ground\u2014glass opacity\uff0cGGO)\u662f\u6307\u9ad8\u5206\u8fa8\u7387CT(high\u2014resolution CT\uff0cHRCT)\u56fe\u50cf\u4e0a\u8868\u73b0\u4e3a\u5bc6\u5ea6\u8f7b\u5ea6\u589e\u52a0\uff0c\u4f46\u5176\u5185\u7684\u652f\u6c14\u7ba1\u8840\u7ba1\u675f\u4ecd\u53ef\u663e\u793a\uff0c\u89c1\u4e8e\u5404\u79cd\u708e\u75c7\u3001\u6c34\u80bf\u3001\u7ea4\u7ef4\u5316\u53ca\u80bf\u7624\u7b49\u75c5\u53d8\u3002"}}