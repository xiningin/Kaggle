{"cell_type":{"7f9db4ec":"code","3d4f29d5":"code","506c7309":"code","bc69c323":"code","c687b88d":"code","0f9ee8a8":"code","1845ddcd":"code","e27ae8e5":"code","35872bf4":"code","837bd860":"code","8585b4c5":"code","ffba0efd":"code","29fdffc7":"code","14188590":"code","5582582a":"code","3ae13b3a":"code","b056bbd7":"code","ee9b5345":"code","773cea8c":"code","f01e25c7":"code","7ba010e7":"code","9168c885":"code","7f80b21b":"markdown","5ff64831":"markdown","d2f1f74c":"markdown","bfb80179":"markdown","114536dd":"markdown","3442f197":"markdown","3143f195":"markdown","6592ae63":"markdown","2177bd36":"markdown","d4b22c4d":"markdown","1a521a8d":"markdown","8e1f1bc7":"markdown","a68bd4b3":"markdown","cf7eb3eb":"markdown","4d1db608":"markdown","bd5c0719":"markdown","6ffce57a":"markdown","13dfbd7d":"markdown","ff7f82ad":"markdown","6999ae4b":"markdown","06fededd":"markdown","6fb00207":"markdown","e28954a7":"markdown","d4ab6d93":"markdown","73e413e1":"markdown","746c9c7d":"markdown","42f3dc13":"markdown","5192087c":"markdown","9f930668":"markdown"},"source":{"7f9db4ec":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import (\n    RandomizedSearchCV,\n    cross_validate,\n    train_test_split,\n)\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.linear_model import LogisticRegression\n\nfrom scipy.stats import expon, lognorm, loguniform, randint, uniform\n\nimport altair as alt\nalt.renderers.enable('kaggle')\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt","3d4f29d5":"sms_df = pd.read_csv(\"\/kaggle\/input\/sms-spam-collection-dataset\/spam.csv\", encoding='latin_1')\nsms_df","506c7309":"sms_df = sms_df.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\nsms_df = sms_df.rename(columns={\"v1\": \"target\", \"v2\": \"sms\"})\nsms_df","bc69c323":"train_df, test_df = train_test_split(sms_df, test_size=0.2, random_state=2018)\n\ntrain_df","c687b88d":"train_df.shape","0f9ee8a8":"pd.DataFrame(sms_df['target'].value_counts(normalize=True), index=None)","1845ddcd":"string_spam = ' '.join([elem for elem in train_df.query('target == \"spam\"').sms.tolist()])\nstring_ham = ' '.join([elem for elem in train_df.query('target == \"ham\"').sms.tolist()])\n\nwordcloud_spam = WordCloud(max_font_size=50).generate(string_spam)\nwordcloud_ham = WordCloud(max_font_size=50).generate(string_ham)\n\nf, axs = plt.subplots(2,2,figsize=(15,15)) \nplt.subplot(1, 2, 1)\nplt.imshow(wordcloud_spam, interpolation='bilinear')\nplt.axis(\"off\")\nplt.title(\"World Cloud for spam emails\")\n\nplt.subplot(1, 2, 2)\nplt.imshow(wordcloud_ham, interpolation='bilinear')\nplt.axis(\"off\")\nplt.title(\"World Cloud for non-spam emails\")\n\nplt.show()","e27ae8e5":"X_train, y_train = train_df['sms'], train_df['target']\nX_test, y_test = test_df['sms'], test_df['target']","35872bf4":"vec = CountVectorizer(max_features=1000, stop_words='english')\n\nX_counts = vec.fit_transform(X_train)\nbow_sms = pd.DataFrame(\n    X_counts.toarray(), columns=vec.get_feature_names(), index=X_train\n)\nbow_sms","837bd860":"bow_sms.shape","8585b4c5":"visualize_sms = bow_sms.T\nvisualize_sms['length'] = visualize_sms.sum(axis=1)\nvisualize_sms = pd.DataFrame(visualize_sms.loc[:, 'length']).reset_index()\nvisualize_sms = visualize_sms.sort_values(['length'], ascending=False)\n\nimport altair as alt\nalt.Chart(visualize_sms.iloc[0:20,:]).mark_bar().encode(\ny=alt.Y('index', sort='x'),\nx='length')","ffba0efd":"dummy_model = DummyClassifier(strategy='prior')\ndummy_model.fit(X_train, y_train)\n\nscoring_metrics = [\"accuracy\", \"roc_auc\"]\n\nresults = pd.DataFrame(cross_validate(dummy_model, X_train, y_train, return_train_score=True, scoring=scoring_metrics))\npd.DataFrame(results.mean()).T","29fdffc7":"pipe = make_pipeline(CountVectorizer(stop_words='english', max_features=1000), LogisticRegression())\npipe.fit(X_train, y_train)\n\nresults = pd.DataFrame(cross_validate(pipe, X_train, y_train, return_train_score=True, scoring=scoring_metrics))\npd.DataFrame(results.mean()).T","14188590":"X_train","5582582a":"pipe_lr = make_pipeline(CountVectorizer(), LogisticRegression())\n\nparam_dists = {\n    \"countvectorizer__max_features\": randint(low=100, high=2000),\n    \"countvectorizer__stop_words\": ['english', None],\n    \"logisticregression__C\": loguniform(1e-3, 1e3),\n    \"logisticregression__class_weight\": ['balanced', None],\n    \"logisticregression__max_iter\": np.arange(500, 2000, 50)\n}","3ae13b3a":"model_random = RandomizedSearchCV(\n        pipe_lr,\n        param_dists,\n        n_iter=50,\n        n_jobs=-1,\n        random_state=2018,\n        return_train_score=True,\n        scoring=scoring_metrics,\n        refit=\"roc_auc\"\n    )\nmodel_random.fit(X_train, y_train);","b056bbd7":"results = pd.DataFrame(model_random.cv_results_)\nresults = results.loc[:, ['param_countvectorizer__max_features', 'param_countvectorizer__stop_words',\n                          'param_logisticregression__C', 'param_logisticregression__class_weight',\n                          'param_logisticregression__max_iter', 'mean_train_roc_auc', 'mean_test_roc_auc',\n                          'mean_train_accuracy', 'mean_test_accuracy', 'rank_test_roc_auc']] \nresults.sort_values(by='mean_test_roc_auc', ascending=False).T","ee9b5345":"roc_auc_plot = alt.Chart(results,\n          title='Distribution of ROC-AUC score basis hyperparameters'\n          ).mark_point().encode(\n    x=alt.X('param_countvectorizer__max_features',\n            title='max_features'),\n    y=alt.Y('mean_test_roc_auc',\n            title='ROC-AUC Validation score',\n            scale=alt.Scale(zero=False)),\n    color=alt.Color('param_logisticregression__C',\n                    title='C',\n                    scale=alt.Scale(reverse=True)),\n    size=alt.Size('param_logisticregression__max_iter',\n                  title='max_iter'))\n\naccuracy_plot = alt.Chart(results,\n          title='Distribution of Accuracy score basis hyperparameters'\n          ).mark_point().encode(\n    x=alt.X('param_countvectorizer__max_features',\n            title='max_features'),\n    y=alt.Y('mean_test_accuracy',\n            title='Accuracy Validation score',\n            scale=alt.Scale(zero=False)),\n    color=alt.Color('param_logisticregression__C',\n                    title='C',\n                    scale=alt.Scale(reverse=True)),\n    size=alt.Size('param_logisticregression__max_iter',\n                  title='max_iter'))\n\naccuracy_plot | roc_auc_plot","773cea8c":"best_model = model_random.best_estimator_\n\nfinal_results = dict(results.sort_values(['rank_test_roc_auc']).reset_index(drop=True).iloc[0,:])\nfor key, value in final_results.items():\n    print(f\"{key}: {value}\")","f01e25c7":"best_model.named_steps[\"logisticregression\"].classes_","7ba010e7":"best_model.fit(X_train, y_train);\ntext_columns = best_model.named_steps[\"countvectorizer\"].get_feature_names()\ncoefs = {\n    \"coefficient\": best_model.named_steps[\"logisticregression\"].coef_.flatten().tolist(),\n    \"magnitude\": np.absolute(best_model.named_steps[\"logisticregression\"].coef_.flatten().tolist()),\n }\ncoef_df = pd.DataFrame(coefs, index=text_columns).sort_values(\"magnitude\", ascending=False)\ncoef_df.head(n=10)","9168c885":"model_random.score(X_test, y_test)","7f80b21b":"Since, we have used Logistic Regression as our model, which is interpretable by looking at the coefficient value. The default class is `ham` as shown below which is associated with the `0` value of probability. The higher values of feature coefficients mean how positively or negatively do the feature increase or decrease the possibility of getting a `spam` message.","5ff64831":"We actually have two different sets of hyperparamters(total 4):\n1. CountVectorizer has two: **max_features** and **stop_words**.\n2. We use two for Logisitic Regression: \n    * **C**: C is regularization parameter and the strength of the regularization is inversely proportional to C. Must be strictly positive.\n    * **penalty**: We will be using two choices here: \n        1. `l2` : add a L2 penalty term and it is the default choice\n        2. `l1` : which is L1 penalty which can be used to reduce sparse data features. ","d2f1f74c":"We observe that a lot of the features associated with `spam` are:  \ntxt, uk, service, claim, free, etc which are acutally present in `spam` messages. Misspellings such as txt are common in spam mails.","bfb80179":"## 3. Visualizing all the models trained by Randomized Search","114536dd":"- We have $4457$ training example texts and observe that only $13.4%$ is `spam`. \n- We can classify this as class imbalance.\n- We can tackle this by two methods\n    1. Subsampling and increasing the ratio of `spam` marked emails.\n    2. Use metrics like Recall, Precision, F-1, ROC-AUC score to judge our model.\n- Here, we would use the F-1 score to maintain a balance between recall and precision scores. Choosing recall or precision instead of selecting F-1 is context dependent.","3442f197":"# 3. EDA","3143f195":"- We transform our text column using sklearn's `CountVectorizer`. Alterante method to do so is TD-IDF which focuses on frequencies of words and its proportions in the document. `CountVectorizer` is used for calculing an individual word count in the text.\n- We use two parameters of `CountVectorizer` which are:\n    1. `max_features` limits the number of distinct individual tokens\/words we want to be counted.\n    2. `stop_words`=english eliminates the common English words like is, a, the, etc.\n- Ultimately, we fit and tranform the data to observe the columns of the transformed data.\n- `get_feature_names` is used to extract the word names of the top 1000 occuring max_features.","6592ae63":"# 8. Evaluating the best model on test set","2177bd36":"# 7. Hyperparameter Optimization","d4b22c4d":"- With Dummy Classifier, we get 86.63% training accuracy and 86.63% validation accuracy. \n- We get high scores because using the `prior` strategy, DummyClassifier marks all its targets as the most occuring target value which is `ham` which incidentally makes up around more than 80% of the dataset.\n- The score are almost the same maybe due to the ratio of spam and non-spam messages being evenly distributed between train and validation sets.\n- ROC-AUC score of 0.5 means that our model is as good as taking flipping a coin. It is predicting random class or constant class for all the examples.","1a521a8d":"# 5. Visualizing the transformed data","8e1f1bc7":"# 6. Building our ML model","a68bd4b3":"# 9. Further improvements on the analysis\n\n- There are better ways to deal with class imbalance such as undersampling.\n- For visualizing our performance on the test set, we can use the following:\n    1. Classification report\n    2. Confusion Matrix\n    3. Average AP score\n    4. Plotting the ROC_AUC curve\n- We have avoided using SVM in this analysis because it is more time consuming than LR. SVM should be preferred if we want to eliminate some of the irrelevant features.","cf7eb3eb":"## 1. Choosing max_features:\nWith lesser number of features, we are likely to underfit the training data as we run the risk of missing out on key features for modeling the dataset. Overfitting is likely to happen when we take a lot more words\/features which add to the complexity of the model.\n\n## 2. Problems with english stop words:\nWords like `the` can sometimes be important to mark important proper nouns such as `The Taj Mahal`. So, we should use this feature with a pinch of salt. A better approach would be to pass a relevant list of such stop_words instead of using the default enlgish stop words.","4d1db608":"# 4. Using CountVectorizer to handle text column","bd5c0719":"- RandomizedSearch is better than using GridSearch because of the randomness attached to the selection of hyperparameter values over a range.\n- Explanation of the arguments used:\n    1. n_iter: number of different sets of hyperparameter values to be run\/ iterations to be done.\n    2. n_jobs: setting it to -1 allows multiple cores of computers to run different iterations simultaneously.\n    3. refit: this is one of the elements from the scoring list (['accuracy', 'roc_auc']), which is to be used to rank the best model.","6ffce57a":"## 1. Making a grid of different hyperparameter values","13dfbd7d":"Yay! Final model score on the test dataset does even better than the validation score.","ff7f82ad":"# 1. Reading dataset","6999ae4b":"Our final ROC_AUC valdiation score (called test score here) is slightly better than the default case in `Section 6`.","06fededd":"# 2. Data splitting into train and test","6fb00207":"We can see some misspelled words such as `ur` and `gt` which could be short forms for your and got. There are other common recurring words such as `just`, `free`, `ok` and `it`.","e28954a7":"Using CountVectorizer with stop_words='english' and max_features=1000 and Logistic Regression, we get 98.91% training accuracy and 97.64% validation accuracy. We get better train and validation scores with Logistic Regression in comparison to DummyClassifier.","d4ab6d93":"## 4. Examine important features associated with the model","73e413e1":"Whoops! We have three columns which do not have valueable information. So, we will drop these columns. We also rename the first two columns as per our use. Here, we call the text column as `sms` and the target column as `target`.","746c9c7d":"The above plots show that:\n- Lower values of `C`, correspond to higher ROC-AUC score\n- Higher number of `max_features`, correspond to higher ROC-AUC score\n\nThe code below prints all the best hyperparameter values and the scores associated with it.","42f3dc13":"# 0. Importing packages","5192087c":"## 2. Using Randomized Search to get the best hyperparameter values","9f930668":"Note: Sklearn calls the validation score outputs from its `cross_validate` as test score and not validation score.\n\nWe have defined two scoring metrics `accuracy` (default) and `ROC-AUC` score for assessing the model. `ROC-AUC` score gives us a fair evaluation of the model performance of our set due to the class imbalance. Another reason for choosing `ROC-AUC` over `F1` score is that the former gives score over different thresholds while `F1` value is for the default threshold."}}