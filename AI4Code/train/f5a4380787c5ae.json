{"cell_type":{"be16d604":"code","30157713":"code","9bc472e8":"code","7650d8f8":"code","c3166b5e":"code","6228686c":"code","f98da6e1":"code","21ad775c":"code","ff50b38c":"code","bdc59ad5":"code","d4895991":"code","0504e53d":"code","fe8b74c4":"code","cb208cd4":"code","054eca92":"code","45b94b19":"code","ea158c54":"code","ddf6126f":"code","b5957079":"code","f3bf5e55":"code","aebdb525":"code","1a7ceaa6":"code","5646952c":"code","c5ffe4bb":"code","98110161":"code","c4590f1a":"code","b252c289":"code","2ff456da":"code","156067ed":"code","afb56f2d":"code","330fb0a4":"code","b1d75512":"code","6cc061d1":"code","447713f9":"code","49747822":"code","2b36bd9e":"code","487f5336":"code","626b0f28":"code","5380df2d":"code","2284c7c4":"code","ff50fff3":"code","8cbb0cd7":"code","d04a561d":"code","899c7ef7":"code","acf4bdee":"code","06d80dfc":"code","f22acf8f":"code","4789fa82":"code","580cf0ba":"code","c9085ba0":"code","8bdb82e2":"code","b22585e1":"markdown","aeb9f529":"markdown","e01ad4b9":"markdown","61f084c7":"markdown","21c2c137":"markdown","8fe9afa5":"markdown","7817ad99":"markdown","b47ccbad":"markdown","c4263934":"markdown","b1f8bf64":"markdown","c8ebfad1":"markdown","ac784f3f":"markdown","6aa8b1bd":"markdown","cd179c33":"markdown","ddec89e5":"markdown","aab2706b":"markdown","3aa672f3":"markdown","d07079d8":"markdown","66f99418":"markdown","38a00d92":"markdown","fa4567b0":"markdown","89597a4d":"markdown","92a2674c":"markdown","a417d0fd":"markdown","ddfdfea0":"markdown","b5fb0226":"markdown","60afe9dd":"markdown","b2007d99":"markdown","534fbe42":"markdown","40fe21eb":"markdown","911beeb1":"markdown","8b1e2bb8":"markdown","38113335":"markdown","b3c61b49":"markdown","e1f46bab":"markdown","893e1f0f":"markdown","3da4db56":"markdown","16851364":"markdown","b447d545":"markdown","c07b1c42":"markdown","1596a4c0":"markdown","b8688457":"markdown","54f6a303":"markdown","d5dc966f":"markdown","9948a9ed":"markdown","063fa196":"markdown"},"source":{"be16d604":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n%matplotlib inline\nsns.set_style('darkgrid')\ncolors = [\"windows blue\",\"amber\", \"greyish\", \"faded green\", \"dusty purple\"]\nsns.set_palette(sns.xkcd_palette(colors))\nsns.set_palette('hls')\nsns.set_context(\"notebook\", 1.5)\nalpha = 0.7","30157713":"test  = pd.read_csv('..\/input\/test.csv')\ntrain = pd.read_csv('..\/input\/train.csv')","9bc472e8":"train.info()","7650d8f8":"test.info()","c3166b5e":"train.head()","6228686c":"test.head()","f98da6e1":"plt.figure(figsize=(18,5))\nplt.subplot(121)\nsns.heatmap(train.isnull(), yticklabels=False, cbar=False)\nplt.subplot(122)\nsns.heatmap(test.isnull(), yticklabels=False, cbar=False)\nplt.show()","21ad775c":"plt.figure(figsize=(18,7))\nplt.subplot(121)\nsns.boxplot(x='Pclass', y='Age', data=train)\nplt.title('Training set')\nplt.subplot(122)\nsns.boxplot(x='Pclass', y='Age', data=test)\nplt.title('Test set')\nplt.show()","ff50b38c":"# Compute the average age per class using both training and testing datasets. \nAge_Pclass1 = 0.5*(train[train['Pclass']==1]['Age'].mean() + test[test['Pclass']==1]['Age'].mean())\nAge_Pclass2 = 0.5*(train[train['Pclass']==2]['Age'].mean() + test[test['Pclass']==2]['Age'].mean())\nAge_Pclass3 = 0.5*(train[train['Pclass']==3]['Age'].mean() + test[test['Pclass']==3]['Age'].mean())\n\nprint(Age_Pclass1, Age_Pclass2, Age_Pclass3)","bdc59ad5":"# Compute the average fare per class using both training and testing datasets. \nFare_Pclass1 = 0.5*(train[train['Pclass']==1]['Fare'].mean() + test[test['Pclass']==1]['Fare'].mean())\nFare_Pclass2 = 0.5*(train[train['Pclass']==2]['Fare'].mean() + test[test['Pclass']==2]['Fare'].mean())\nFare_Pclass3 = 0.5*(train[train['Pclass']==3]['Fare'].mean() + test[test['Pclass']==3]['Fare'].mean())\n\nprint(Fare_Pclass1, Fare_Pclass2, Fare_Pclass3)","d4895991":"def input_age(cols):\n    \n    Pclass = cols[0]\n    Age = cols[1]\n\n    if pd.isnull(Age):\n        if Pclass==1:\n            return Age_Pclass1      \n        if Pclass==2:\n            return Age_Pclass2\n        else:\n            return Age_Pclass3\n    else:\n        return Age","0504e53d":"def input_fare(cols):\n    \n    Pclass = cols[0]\n    Fare = cols[1]\n\n    if pd.isnull(Fare):\n        if Pclass==1:\n            return Fare_Pclass1      \n        if Pclass==2:\n            return Fare_Pclass2\n        else:\n            return Fare_Pclass3\n    else:\n        return Fare","fe8b74c4":"# Replace missing values\ntrain['Age'] = train[['Pclass','Age']].apply(input_age, axis=1)\ntest['Age'] = test[['Pclass','Age']].apply(input_age, axis=1)\n\ntrain['Fare'] = train[['Pclass','Fare']].apply(input_age, axis=1)\ntest['Fare'] = test[['Pclass','Fare']].apply(input_age, axis=1)","cb208cd4":"plt.figure(figsize=(18,5))\nplt.subplot(121)\nsns.heatmap(train.isnull(), yticklabels=False, cbar=False)\nplt.subplot(122)\nsns.heatmap(test.isnull(), yticklabels=False, cbar=False)\nplt.show()","054eca92":"train.drop('Cabin', axis=1, inplace=True)\ntest.drop('Cabin', axis=1, inplace=True)\n#test.dropna(inplace=True)","45b94b19":"plt.figure(figsize=(18,5))\nplt.subplot(121)\nsns.heatmap(train.isnull(), yticklabels=False, cbar=False)\nplt.subplot(122)\nsns.heatmap(test.isnull(), yticklabels=False, cbar=False)\nplt.show()","ea158c54":"train.drop(['Name','Ticket'], inplace=True, axis=1)\ntest.drop(['Name', 'Ticket'], inplace=True, axis=1)","ddf6126f":"train.head()","b5957079":"test.head()","f3bf5e55":"sns.pairplot(train)","aebdb525":"plt.figure(figsize=(14,12))\nsns.heatmap(train.corr(), annot=True, square=True)","1a7ceaa6":"plt.figure(figsize=(20,15))\nplt.subplot(331)\nsns.countplot(x='Survived', data=train, color='grey', alpha=alpha)\nplt.subplot(332)\nsns.countplot(x='Sex', data=train, color='grey', alpha=alpha)\nplt.subplot(333)\nsns.countplot(x='Pclass', data=train, color='grey', alpha=alpha)\nplt.subplot(334)\nsns.countplot(x='Embarked', data=train, color='grey', alpha=alpha)\nplt.subplot(335)\nsns.countplot(x='SibSp', data=train, color='grey', alpha=alpha)\nplt.subplot(336)\nsns.countplot(x='Parch', data=train, color='grey', alpha=alpha)\nplt.subplot(337)\nsns.distplot(train['Age'], color='grey', kde=False, bins=20)\nplt.subplot(338)\nsns.distplot(train['Fare'], color='grey', kde=False, bins=30)\n\nplt.tight_layout()","5646952c":"plt.figure(figsize=(20,5))\nplt.subplot(121)\nsns.countplot(x='Sex', data=train, hue='Survived')\nplt.title('Survival by sex')\nplt.subplot(122)\nsns.countplot(x='Pclass', data=train, hue='Survived')\nplt.title('Survival by class')\nplt.show()\n\nsns.catplot(x='Sex', hue='Survived', col='Pclass', data=train, kind=\"count\")\nplt.show()","c5ffe4bb":"plt.figure(figsize=(20,5))\nplt.subplot(121)\nsns.countplot(x='SibSp', data=train, hue='Survived')\nplt.title('Survival by number of siblings')\nplt.legend(['Died', 'Survived'], loc=1)\nplt.subplot(122)\nsns.countplot(x='Parch', data=train, hue='Survived')\nplt.title('Survival by number of children')\nplt.legend(['Died', 'Survived'], loc=1)\nplt.show()","98110161":"plt.figure(figsize=(20,5))\nsns.catplot(x='SibSp', hue='Survived', col='Pclass', data=train, kind=\"count\")\nplt.show()","c4590f1a":"plt.figure(figsize=(20,5))\nsns.catplot(x='SibSp', hue='Survived', col='Sex', data=train, kind=\"count\")\nplt.show()","b252c289":"train_1class = train[train['Pclass']==1]\ntrain_2class = train[train['Pclass']==2]\ntrain_3class = train[train['Pclass']==3]","2ff456da":"g=sns.catplot(x='SibSp', hue='Survived', col='Sex', data=train_1class, kind='count')\ng.fig.suptitle('1st class', fontsize=25)\nplt.show()\ng=sns.catplot(x='SibSp', hue='Survived', col='Sex', data=train_2class, kind='count')\ng.fig.suptitle('2nd class', fontsize=25)\nplt.show()\ng=sns.catplot(x='SibSp', hue='Survived', col='Sex', data=train_3class, kind='count', col_order=['female', 'male'])\ng.fig.suptitle('3th class', fontsize=25)\nplt.show()","156067ed":"g=sns.catplot(x='Parch', hue='Survived', col='Sex', data=train_1class, kind='count')\ng.fig.suptitle('1st class', fontsize=25)\nplt.show()\ng=sns.catplot(x='Parch', hue='Survived', col='Sex', data=train_2class, kind='count')\ng.fig.suptitle('2nd class', fontsize=25)\nplt.show()\ng=sns.catplot(x='Parch', hue='Survived', col='Sex', data=train_3class, kind='count', col_order=['female', 'male'])\ng.fig.suptitle('3th class', fontsize=25)\nplt.show()","afb56f2d":"plt.figure(figsize=(20,5))\nplt.subplot(121)\nsns.countplot(x='Embarked', data=train, hue='Survived')\nplt.subplot(122)\nsns.countplot(x='Embarked', hue='Pclass', data=train, palette='Set2')\nplt.show()","330fb0a4":"sex_train = pd.get_dummies(train['Sex'],drop_first=True)\nsex_test  = pd.get_dummies(test['Sex'],drop_first=True)\n\n#embarked_train = pd.get_dummies(train['Embarked'],drop_first=True)\n#embarked_test  = pd.get_dummies(test['Embarked'],drop_first=True)\n\ntrain.drop(['Sex', 'Embarked'],axis=1,inplace=True)\ntest.drop(['Sex', 'Embarked'],axis=1,inplace=True)\n\ntrain = pd.concat([train,sex_train],axis=1)\ntest  = pd.concat([test,sex_test],axis=1)","b1d75512":"train.head()","6cc061d1":"test.head()","447713f9":"X_train = train.drop(['Survived', 'PassengerId'], axis=1)\ny_train = train['Survived']\nX_test = test.drop(['PassengerId'], axis=1)","49747822":"X_train0, X_test0, y_train0, y_test0 = train_test_split(X_train, y_train, test_size=0.3, random_state=101)","2b36bd9e":"logmodel = LogisticRegression()\nlogmodel.fit(X_train0,y_train0)\npredictions = logmodel.predict(X_test0)\nacc_lr = round(accuracy_score(y_test0, predictions) * 100, 2)\nprint('Accuracy (LR): {}'.format(acc_lr))","487f5336":"coefficients = pd.DataFrame(logmodel.coef_[0], X_train0.columns)\ncoefficients.columns = ['Coefficient']\ncoefficients","626b0f28":"error_rate=[]\nfor i in range(1,40):\n    KNN = KNeighborsClassifier(n_neighbors=i)\n    KNN.fit(X_train0,y_train0)\n    pred_i = KNN.predict(X_test0)\n    error_rate.append(np.mean(pred_i != y_test0))","5380df2d":"plt.figure(figsize=(10,5))\nplt.plot(range(1,40), error_rate, linestyle='--', color='blue', marker='o',markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","2284c7c4":"knn = KNeighborsClassifier(n_neighbors=11)\nknn.fit(X_train0, y_train0)\npredictions = knn.predict(X_test0)\nacc_knn = round(accuracy_score(y_test0, predictions) * 100, 2)\nprint('Accuracy (KNN): {}'.format(acc_knn))","ff50fff3":"dt = DecisionTreeClassifier()\ndt.fit(X_train0, y_train0)\npredictions = dt.predict(X_test0)\nacc_dt = round(accuracy_score(y_test0, predictions) * 100, 2)\nprint('Accuracy (Decision Tree): {}'.format(acc_dt))","8cbb0cd7":"error_rate=[]\nfor i in range(1,150):\n    RF = RandomForestClassifier(n_estimators=i)\n    RF.fit(X_train0,y_train0)\n    pred_i = RF.predict(X_test0)\n    error_rate.append(np.mean(pred_i != y_test0))","d04a561d":"plt.figure(figsize=(10,5))\nplt.plot(range(1,150), error_rate, linestyle='--', color='blue', marker='o',markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. n_estimators')\nplt.xlabel('n_estimators')\nplt.ylabel('Error Rate')","899c7ef7":"rfc = RandomForestClassifier(n_estimators=100)\nrfc.fit(X_train0, y_train0)\npredictions = rfc.predict(X_test0)\nacc_rfc = round(accuracy_score(y_test0, predictions) * 100, 2)\nprint('Accuracy (Random Forest): {}'.format(acc_rfc))","acf4bdee":"param_grid = {'C':[0.1,1,10,100], 'gamma':[1,0.1,0.01,0.001,0.0001]}\ngrid = GridSearchCV(SVC(), param_grid, verbose=3)\ngrid.fit(X_train0, y_train0)","06d80dfc":"grid.best_estimator_","f22acf8f":"predictions = grid.predict(X_test0)","4789fa82":"acc_svc = round(accuracy_score(y_test0, predictions) * 100, 2)\nprint('Accuracy (SVM): {}'.format(acc_svc))","580cf0ba":"models = pd.DataFrame({\n    'Model': ['Logistic Regression', 'KNN', 'Decision Tree', 'Random Forest', 'SVM'],\n    'Accuracy': [acc_lr, acc_knn, acc_dt, acc_rfc, acc_svc]})\nmodels","c9085ba0":"rfc = RandomForestClassifier(n_estimators=100)\nrfc.fit(X_train, y_train)\npredictions = rfc.predict(X_test)","8bdb82e2":"predictions_df = pd.DataFrame(predictions, test['PassengerId'])\npredictions_df.columns = ['Survived']\npredictions_df.to_csv(\"predictions_rfc.csv\")","b22585e1":"# -------------------------------------------------------\n# Titanic analysis and survival prediction model \n# -------------------------------------------------------\n\n## 0. Load data\n## 1. Data cleaning\n## 2. Analysis\n        2.1. Overview\n        2.2. Sex, class and survival\n        2.3. Siblings and children\n        2.4. Boarding port\n## 3. Predicting the Survived feature\n\n### 3.1. Accuracy of the methods\n\n Logistic regression\n\n K nearest neighbors\n\n Decision Tree \n \n Random Forest \n \n Support Vector Machines\n\n## 3.2. Preditions with test set and submission file\n\n# -------------","aeb9f529":"### Decision Tree","e01ad4b9":"### Comparing the accuracy of the models","61f084c7":"## 2.4. Boarding port ","21c2c137":"Finally, we delete the columns we will not use: Name, Ticket.","8fe9afa5":"# 1. Data cleaning","7817ad99":"We use the best parameters of SVM to obtain the predictions","b47ccbad":"In the following section we will perform an analysis of the training set. We will use the test set in the prediction section to evaluate the model we will construct.","c4263934":"## 2.2. Sex, class and survival","b1f8bf64":"It seems that to have children (right figure) was an incentive to survive. I suppose that families had priority to board the lifeboat. Also, people who traveled with siblings were more likely to survive (left figure). Let's see if these insights are related to the sex and class features. ","c8ebfad1":"Therefore, people who traveled whit siblings were more likely to survive but not if they belonged to the third class. ","ac784f3f":"# 2. Analysis","6aa8b1bd":"# 3. Predicting the Survived feature","cd179c33":"# 0. Load data","ddec89e5":"## 3.2. Preditions with test set and submission file","aab2706b":"### Insights for siblings\n\n1. At first glance it seems that to have siblings is important to survive.\n2. Men died independently of the number of siblings. \n2. This is only true for women belonged to the first and second class.\n3. Third class women wihtout sibling were more likely to survive.","3aa672f3":"Let's move now to analyze the importance of the size of the family (Parch)","d07079d8":"We take K=11 because it gives us the lower error","66f99418":"First of all we need to find the number of estimators that gives us the lower error","38a00d92":"We take n_estimators = 100","fa4567b0":"Again the sex is crucial to figure out if one survived or not. Let's mix class and sex in order to obtain some insights ","89597a4d":"First of all we need to find the number of neighbors that gives us the lower error","92a2674c":"### Support Vector Machines (SVM)","a417d0fd":"There are not many features strongly correlated. The most correlated is the number of sibling on board (SibSp) and the number of parents with children (Parch).","ddfdfea0":"### General insights\n\n1. The typical passenger profile is a single man in his twenties\/thirties (without children or siblings on board) who traveled in third class and embarked in Southampton.\n2. More people died than survived.\n3. More men were on board than women. \n4. More passengers in third class than in the other two. In first and second class the numbr of passenger is quite similar. \n5. Passenger traveled alone or with one sibling. \n6. Few children were on board.","b5fb0226":"Now we have no missing values neither in Age column nor Fare column. Cabin column has too many Nan's.","60afe9dd":"### Logistic regression ","b2007d99":"Therefore, the method which gives us the best accuracy is **Random Forest**. We use it to predict the class Survived in the test set","534fbe42":"## 2.3. Siblings and family size","40fe21eb":"#### Insights \n1. As we expected, the sex is the most important feature in order to figure out if a passenger died or not. \n2. The second most important feature is, as we previously inferred, the class. ","911beeb1":"### Insights\n\n1. The chance to live or to die depends on the the sex and the class. \n2. More men died than survived, but more women survived than died. In the third class the number of dead women is approximately equal to the number of women who survived.\n3. The weight of the sex feature is higher than the weight of the class in order to predict the 'Survived' feature. A man would probably die in the Titanic, but a woman could survive even if she traveled in the third class. ","8b1e2bb8":"### 2.3.1. Family size ","38113335":"### K nearest neighbors","b3c61b49":"### 2.3.1. Siblings","e1f46bab":"In the following figure it is plotted in yellow the Nan's values in both train and test datasets.","893e1f0f":"### Random Forest ","3da4db56":"##\u00a02.1. Overview","16851364":"We use the GridSearchCV function in order to find the best parameters to the SVM classification model.","b447d545":"### Save the predictions file","c07b1c42":"###\u00a0Converting categorical features","1596a4c0":"Note that there are too many missing values in the Age column. We can fill these values from the information we have in this column. For example, we can calculate the average of the age per passenger class group and replace the age missing value of a passenger with the corresponding age per class he\/she belongs. And the missing fare value(s) can be replaced by the mean fare of the class in which the passenger traveled. ","b8688457":"### Create features","54f6a303":"### Insights\n\n1. Different number of rows in train and test.\n2. The training set has Survived as variable but the test set not. We will predict from the training set if a passenger in test set survived or not.","d5dc966f":"We choose the 30% of our trining data to test the accuracy of the different methods we use.","9948a9ed":"### Insights for family size\n\n1. It seems that to have children is important to survive.\n2. This is only true for women belonged to the first and second class. Men died independently of the number of children.\n3. Again, third class women who traveled alone were more likely to survived. ","063fa196":"##\u00a03.1. Accuracy of the methods"}}