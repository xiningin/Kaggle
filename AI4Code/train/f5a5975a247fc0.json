{"cell_type":{"0ab50c20":"code","cfa15b9f":"code","0a6b6f0b":"code","fc0c70d2":"code","66488c59":"code","d18ddeb1":"code","bc6183cc":"code","c7848403":"code","8359af31":"code","92e91d7f":"code","106b33eb":"code","8cdf087e":"code","31090815":"code","fb8a1b14":"code","230932d4":"code","1182d893":"code","90dbb35c":"code","ad3306eb":"code","92ee6898":"code","4b85112d":"code","7c805e4c":"code","9a4a1d02":"code","995dbc16":"code","cc3fc84f":"code","228b6469":"code","582bcf34":"code","3470688f":"markdown","eb0c1f44":"markdown","4ef55c40":"markdown","bd5dbd6e":"markdown","6a0ce8d8":"markdown","82fdbf47":"markdown","6b16ce6a":"markdown","23ad100f":"markdown","9e3268b8":"markdown","f2becf44":"markdown","4f339dd6":"markdown","7c693443":"markdown","77cee5ba":"markdown","8a872447":"markdown","e67369b9":"markdown","377fcc98":"markdown","47234981":"markdown","a25160f3":"markdown","884a36b5":"markdown","166fe70e":"markdown","3fc777b6":"markdown","d919f9da":"markdown","542d74f6":"markdown"},"source":{"0ab50c20":"import numpy as np \n\nimport pandas as pd \n\nimport os\n\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import confusion_matrix\n\nimport time\n\nfrom lightgbm import LGBMClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')\n","cfa15b9f":"data = pd.read_csv(\"..\/input\/bioresponse\/train.csv\")\nprint(data.head(10))","0a6b6f0b":"print(\"Missing Values\",data.isnull().sum().sum())\nprint(\"Column names\",data.columns)","fc0c70d2":"# select the float columns\ndf_float = data.select_dtypes(include=[np.float])\nprint(\"Float Columns\",df_float.columns)\n\n# select int columns\ndf_int = data.select_dtypes(include=[np.int])\nprint(\"Int columns\",df_int.columns)\n\n# select object columns\ndf_object = data.select_dtypes(include=[object])\nprint(\"object columns\",df_object.columns)","66488c59":"g1 = sns.countplot(x=data[\"Activity\"])\n    \n\ng1.set_xticklabels(g1.get_xticklabels(),rotation=45)\nplt.show()","d18ddeb1":"Scaleddata = MinMaxScaler().fit_transform(data)\ndata = pd.DataFrame(Scaleddata,columns=data.columns)","bc6183cc":"pca = PCA().fit(data)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');","c7848403":"# Reduce dimensionality through PCA\n\ny = data[\"Activity\"]\nX = data.drop(columns=['Activity'])\n\npca = PCA(n_components=750)\nX_pca = pca.fit_transform(X) \n\n\n# Then reduce further with t-sne\ntsne = TSNE(n_components=2, verbose=1, perplexity=50, n_iter=1000, learning_rate=200)\ntsne_results = tsne.fit_transform(X_pca[:,:])\n\ndf_tsne = pd.DataFrame(tsne_results, columns=['comp1', 'comp2'])\ndf_tsne['label'] = y[:]\nsns.scatterplot(x='comp1', y='comp2', data=df_tsne, hue='label')","8359af31":"train = pd.read_csv(\"..\/input\/bioresponse\/train.csv\")\ntest = pd.read_csv(\"..\/input\/bioresponse\/test.csv\")\n\n\n# Label for predicting a strong adherence to the testing sets\ntrain['label'] = 0\ntest['label'] = 1\n\ntraining = train.drop('Activity',axis=1) \n\n# Combine testing and training sets\ncombine = training.append(test)\ny =combine['label']\ncombine.drop(columns = ['label'],inplace=True)\n\n\nmodel = LGBMClassifier(n_estimators = 50)\ndrop_list = []\n\nfor col in combine.columns:\n    score = cross_val_score(model,pd.DataFrame(combine[col]),y,cv=2,scoring='roc_auc')\n    #print(score)\n    if (np.mean(score) > 0.7):\n        drop_list.append(col)\n        print(\"Column with covariate shift:\", col)","92e91d7f":"if len(drop_list)==0: \n    print(\"No presence of covariate shift\")","106b33eb":"data = pd.read_csv(\"..\/input\/bioresponse\/train.csv\")\n\nX = data\nX = X.drop(columns='Activity')\n\ny = data['Activity']\ny = y.values\ny = y.reshape((len(y), 1))\n\n# split into train and test sets\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=32)\n\ncolumns  = X_train.columns\n\n# Normalizing Column values\nfor col in columns:\n    MinMax = MinMaxScaler()\n    \n    X_train_arr = X_train[col].astype(float).values\n    X_test_arr = X_test[col].astype(float).values   \n        \n            \n    X_train_arr = MinMax.fit_transform(X_train_arr.reshape(-1,1))\n    X_test_arr = MinMax.transform(X_test_arr.reshape(-1,1))\n            \n    X_train[col]  = X_train_arr \n    X_test[col]   = X_test_arr\n        \n","8cdf087e":"#fit_params={\"early_stopping_rounds\":100, \n           # \"eval_metric\" : 'binary_logloss', \n          #\"eval_set\" : [(X_test,y_test)],\n          #'eval_names': ['valid'],\n          #'verbose': 100,\n          #'categorical_feature': 'auto'}\n\n#param_test ={  'n_estimators': [50,100,200,400, 700, 1000],\n  #'colsample_bytree': [0.7, 0.8],\n  #'max_depth': [15,20,25],\n  #'num_leaves': [50, 100, 200],\n  #'reg_alpha': [1.1, 1.2, 1.3],\n #'reg_lambda': [1.1, 1.2, 1.3],\n# 'min_split_gain': [0.3, 0.4],\n#'subsample': [0.7, 0.8, 0.9],\n # 'subsample_freq': [20]}","31090815":"#clf = LGBMClassifier(random_state=314, silent=True, metric='None', n_jobs=2)\n#model = RandomizedSearchCV(\n   #estimator=clf, param_distributions=param_test, \n   # scoring='neg_log_loss',\n   # cv=3,\n   # refit=True,\n  # random_state=314,\n  # verbose=True)","fb8a1b14":"#model.fit(X_train, y_train, **fit_params)\n#print('Best score reached: {} with params: {} '.format(model.best_score_, model.best_params_))","230932d4":"model = LGBMClassifier(random_state=314, silent=True, n_jobs=2,subsample_freq = 20, subsample = 0.9, \n                       reg_lambda = 1.2, reg_alpha = 1.1,num_leaves= 200, n_estimators = 700, \n                       min_split_gain =  0.4, max_depth =  15, colsample_bytree = 0.8)","1182d893":"model.fit(X_train, y_train)\npred = model.predict_proba(X_test)\n\nprint(\"Log Loss Probability: \",log_loss(y_test,pred))","90dbb35c":"pred = model.predict(X_test)\ny_test = y_test.flatten()\n\n#print(np.shape(y_test))\n\ndata = {'y_Actual':    y_test,\n        'y_Predicted': pred\n        }\n\ndf = pd.DataFrame(data, columns=['y_Actual','y_Predicted'])\nconfusion_matrix = pd.crosstab(df['y_Actual'], df['y_Predicted'], rownames=['Actual'], colnames=['Predicted'])\n\nsns.heatmap(confusion_matrix, annot=True)\nplt.show()","ad3306eb":"def plotImp(model, X , num = 20):\n    \n    feature_imp = pd.DataFrame({'Value':model.feature_importances_,'Feature':X.columns})\n    plt.figure(figsize=(100, 500))    \n    sns.set(font_scale = 5)\n    \n    #columns = feature_imp.sort_values(by=\"Value\",ascending=False)[0:num]['Feature'].to_list()\n    \n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", \n                                                        ascending=False)[0:num])\n    plt.title('LightGBM Feature imprtances')\n    plt.tight_layout()\n    \n    plt.show()\n    \n    \nplotImp(model, X_train , num =300)\n","92ee6898":"X_train = pd.read_csv(\"..\/input\/bioresponse\/train.csv\")\nX_test =  pd.read_csv(\"..\/input\/bioresponse\/test.csv\")\n\ny_train = X_train[\"Activity\"]\n\nX_train.drop(columns=[\"Activity\"],inplace=True)\n\n\ncolumns  = X_train.columns\n\n# Normalizing column values\nfor col in columns:\n    MinMax = MinMaxScaler()\n    \n    X_train_arr = X_train[col].astype(float).values\n    X_test_arr = X_test[col].astype(float).values   \n        \n            \n    X_train_arr = MinMax.fit_transform(X_train_arr.reshape(-1,1))\n    X_test_arr = MinMax.transform(X_test_arr.reshape(-1,1))\n            \n    X_train[col]  = X_train_arr \n    X_test[col]   = X_test_arr","4b85112d":"model = LGBMClassifier(random_state=314, silent=True, n_jobs=2,subsample_freq = 20, subsample = 0.9, \n                       reg_lambda = 1.2, reg_alpha = 1.1,num_leaves= 200, n_estimators = 700, \n                       min_split_gain =  0.4, max_depth =  15, colsample_bytree = 0.8)","7c805e4c":"model.fit(X_train,y_train)","9a4a1d02":"predicted_prob = model.predict_proba(X_test)","995dbc16":"Probability = predicted_prob[:,1]\nMoleculeId = np.array(range(1,len(X_test)+1))","cc3fc84f":"submission = pd.DataFrame()\nsubmission[\"MoleculeId\"] = MoleculeId\nsubmission['PredictedProbability'] = Probability","228b6469":"submission.to_csv('submission.csv',index=None)","582bcf34":"print(submission)","3470688f":"## Dimensionality Reduction","eb0c1f44":"- Summary\n  - Feature D10, D8, D101,D104 seems to play a major role in prediction\n  - However, Feature elimination through feature importance seems to not improve the scores.","4ef55c40":"# Implement model on Test Set","bd5dbd6e":"# Check for covariate shift between training and testing data set","6a0ce8d8":"- In case of drug trials, there could be a difference between the testing environments.\n- This could lead to discrepancy while predicting test features, if the test features contain a different data distribution compared to training data\n- Below, let's check if the difference actually exists","82fdbf47":"- This is a high dimensional data set with too many dimensions. Let's use PCA and t-sne to see if we can extract features","6b16ce6a":"<img src=\"https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2017\/07\/07230628\/plot1.png\">\n\n\n\n\n\n","23ad100f":"- Check for Missing values\n- Check name of columns\n- Find categorical and numerical columns","9e3268b8":"- This project deals with the prediction of biological response based on molecular data in each column. \n\n- Drug development depends on the biological response with respect to many factors. Machine learning can help us understand if the drug makes a difference with regard to treatment. \n\n- Each column represents molecular data.\n\n- Since, they are many feature columns. The challenge would be to reduce the feature space.\n\n","f2becf44":"- Fair share of distribution for both labels. No need for sampling techniques","4f339dd6":"## References\n\n- https:\/\/towardsdatascience.com\/why-you-are-using-t-sne-wrong-502412aab0c0\n- https:\/\/www.analyticsvidhya.com\/blog\/2017\/07\/covariate-shift-the-hidden-problem-of-real-world-data-science\/\n- https:\/\/datatofish.com\/confusion-matrix-python\/","7c693443":"- No clear seggregation of classes\n- Not a good idea to take PCA and t-sne data for prediction\n- We will have to utilize the entire dataset","77cee5ba":"- Plot confusion Matrix","8a872447":"- Find Distribution of the labels","e67369b9":"# Feature Importance","377fcc98":"## Read Data","47234981":"<img src=\"https:\/\/www.pbiforum.net\/mag\/wp-content\/uploads\/2017\/11\/shutterstock_31312693.jpg\">","a25160f3":"- We need 500 components to describe 95 - 100 % of the variance","884a36b5":"## Libraries","166fe70e":"- Let's look at the cumulative variance with repsect to the number of the components","3fc777b6":"Would love to recieve feedback!","d919f9da":"- Steps\n  - General understanding of the data\n  - Dimensionality Reduction through PCA and t-sne\n  - Check for covariate shift between training and test set\n  - Hyperparameter search through RandomizedSearchCV\n  - Feature Importance of the individual column data\n  - Implementation of model on test set","542d74f6":"# Implementation with LightGBM and Hyperparameter search through RandomizedSearchCV"}}