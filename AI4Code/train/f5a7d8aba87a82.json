{"cell_type":{"d4c9be63":"code","51e78733":"code","b53a2dd0":"code","ebbc7652":"code","fc50dcf4":"code","6a5dedec":"code","f41679fa":"code","7505fc91":"code","5a39f80d":"code","0c3a7123":"code","f7a4d028":"code","3b76463b":"code","4ea2e966":"code","df0e4b12":"code","588b07ab":"code","9e953962":"code","5389b6f4":"code","3d9ebfba":"code","00d5b08b":"code","fd6e110f":"code","c1052842":"code","ea0e6d5b":"code","3e3dfad9":"code","350e816c":"code","61a884c1":"code","f26f4663":"code","38a1bae7":"code","98629291":"code","6e547742":"code","ff42f516":"code","63f1d168":"code","d1f764aa":"code","b85ac33d":"code","175af2d7":"code","dee91aea":"code","57773958":"markdown","a62630bc":"markdown","d62cc82b":"markdown","ab42c4cc":"markdown","3158c14d":"markdown","d743c16b":"markdown","6afffb77":"markdown","c9b330ee":"markdown","a4ca2a11":"markdown","042f5f01":"markdown","d251ef93":"markdown","b67cb143":"markdown","38b06f7c":"markdown","66d5dcf3":"markdown","875979ae":"markdown","301f1a8b":"markdown","1baf3076":"markdown","b62ef8e7":"markdown","ea4dfbfe":"markdown","69814b2f":"markdown","49bbc757":"markdown","ed669d89":"markdown","c3f48b8a":"markdown","d1b0de36":"markdown","18bbcd3f":"markdown","11aab8f7":"markdown","bfdfe255":"markdown","45c76695":"markdown","5d19e735":"markdown","a9928f17":"markdown"},"source":{"d4c9be63":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","51e78733":"#1\ndf_hearth_failure = pd.read_csv('\/kaggle\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')\ndf_hearth_failure.head()","b53a2dd0":"df_hf = df_hearth_failure[['DEATH_EVENT', 'time', 'age', 'high_blood_pressure', 'ejection_fraction', 'serum_creatinine', 'serum_sodium']].copy()\ndf_hf.head()","ebbc7652":"age_text = lambda x: 'low' if(x<=56) else ('high' if(x>=73) else 'medium')\ndf_hf['age_text'] = df_hf['age'].apply(age_text)\ndf_hf.head()","fc50dcf4":"df_hf = df_hf.drop(['age'], axis=1)\ndf_hf.head()","6a5dedec":"from sklearn.preprocessing import OneHotEncoder\nimport numpy as np\n\nencoder = OneHotEncoder(handle_unknown='ignore')\n\nencoder.fit(np.c_[df_hf['age_text']])\nencoder.categories","f41679fa":"transformed = encoder.transform(np.c_[df_hf['age_text']])\ndf_oh = pd.DataFrame(transformed.toarray())\ndf_oh.columns = ['high', 'low', 'medium']\ndf_oh.head()","7505fc91":"df_hf[['age_text0', 'age_text1', 'age_text2']] = df_oh[['high', 'low', 'medium']]\ndf_hf = df_hf.drop(['age_text'], axis=1)\ndf_hf.head()","5a39f80d":"df_hf[['ejection_fraction', 'serum_creatinine', 'serum_sodium']].describe()","0c3a7123":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(df_hf[['ejection_fraction', 'serum_creatinine', 'serum_sodium']])\nprint(scaler.mean_)","f7a4d028":"df_hf[['ejection_fraction_sc', 'serum_creatinine_sc', 'serum_sodium_sc']] = scaler.transform(df_hf[['ejection_fraction', 'serum_creatinine', 'serum_sodium']])\ndf_hf.head()","3b76463b":"askbjd = df_hf[['ejection_fraction', 'serum_creatinine', 'serum_sodium']]#saving just in case\ndf_hf = df_hf.drop(['ejection_fraction', 'serum_creatinine', 'serum_sodium'], axis=1)","4ea2e966":"df_hf['serum_creatinine_sc_log'] = askbjd['serum_creatinine'].apply(np.log)\ndf_hf['serum_creatinine_sc_log'].hist()","df0e4b12":"df_hf['serum_creatinine_sc'].hist()\naskbjd['serum_creatinine_sc'] = df_hf['serum_creatinine_sc']\ndf_hf = df_hf.drop(['serum_creatinine_sc'], axis=1)","588b07ab":"df_hf.head()","9e953962":"df_hf.head()\ny = df_hf['DEATH_EVENT']\nX = np.c_[df_hf[['age_text0', 'age_text1', 'age_text2', 'high_blood_pressure', 'ejection_fraction_sc', 'serum_creatinine_sc_log', 'serum_sodium_sc']]]\nprint('success')","5389b6f4":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint('success')\n","3d9ebfba":"from sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression(random_state=0)\nlog_reg.fit(X_train, y_train)","00d5b08b":"y_train_pred = log_reg.predict(X_train)\ny_test_pred = log_reg.predict(X_test)\ny_test_pred_naive = pd.Series([int(round(y.sum()\/len(y))) for _ in range(len(y_test))])\nprint('predicted')","fd6e110f":"len(y_test_pred_naive) == len(y_test)","c1052842":"from sklearn.metrics import accuracy_score\nprint('TRAIN SCORE')\nprint(accuracy_score(y_train ,y_train_pred))\n\nprint('TEST SCORE')\nprint(accuracy_score(y_test ,y_test_pred))\n\nprint('NAIVE SCORE')\nprint(accuracy_score(y_test ,y_test_pred_naive))\n","ea0e6d5b":"from sklearn.metrics import confusion_matrix\nprint('Format')\nprint('TN, FP')\nprint('FN, TP')\nprint('TRAIN CM SCORE')\nprint(confusion_matrix(y_train ,y_train_pred))\nprint('TEST CM SCORE')\nprint(confusion_matrix(y_test ,y_test_pred))\n\nprint('NAIVE CM SCORE')\nprint(confusion_matrix(y_test ,y_test_pred_naive))","3e3dfad9":"import xgboost as xgb\n\nX_trainxg, X_valid, y_trainxg, y_valid = train_test_split(X_train, y_train, test_size=0.05, random_state=42)\n\n\nmodel=xgb.XGBClassifier(learning_rate=0.05, n_estimators=1000)\nmodel.fit(X_trainxg, y_trainxg, eval_set=[(X_valid, y_valid)], early_stopping_rounds=2500, verbose=False)","350e816c":"xg_pred = model.predict(X_test)\nprint(accuracy_score(xg_pred ,y_test))","61a884c1":"i_see_dead_people= df_hf[df_hf['DEATH_EVENT'] == 1].copy()\ni_see_dead_people.head()","f26f4663":"y = i_see_dead_people['time']\nX = i_see_dead_people[['age_text0', 'age_text1', 'age_text2', 'high_blood_pressure', 'ejection_fraction_sc', 'serum_creatinine_sc_log', 'serum_sodium_sc']]\nX.head()","38a1bae7":"X = np.c_[X]","98629291":"from sklearn.model_selection import train_test_split\nX_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X, y, test_size=0.2, random_state=42)\nprint('success')","6e547742":"from sklearn.linear_model import LinearRegression\n\nlr_model = LinearRegression()\n\nlr_model.fit(X_train_r, y_train_r)","ff42f516":"y_train_pred_r = lr_model.predict(X_train_r)\ny_test_pred_r = lr_model.predict(X_test_r)\ny_test_pred_naive_r = pd.Series([y_train_r.sum()\/len(y_train_r) for _ in range(len(y_test_pred_r))])\nprint('Done')","63f1d168":"from sklearn.metrics import mean_squared_error\nprint('TRAIN MSE')\nprint(mean_squared_error(y_train_r, y_train_pred_r))\nprint('TEST MSE')\nprint(mean_squared_error(y_test_r, y_test_pred_r))\nprint('NAIVE MSE')\nprint(mean_squared_error(y_test_r, y_test_pred_naive_r))\n","d1f764aa":"from sklearn.metrics import mean_absolute_error\nprint('TRAIN MAE')\nprint(mean_absolute_error(y_train_r, y_train_pred_r))\nprint('TEST MAE')\nprint(mean_absolute_error(y_test_r, y_test_pred_r))\nprint('NAIVE MAE')\nprint(mean_absolute_error(y_test_r, y_test_pred_naive_r))\n","b85ac33d":"from sklearn.neighbors import KNeighborsClassifier\n\nNN = KNeighborsClassifier(n_neighbors=2, algorithm='ball_tree')\nNN.fit(X_train_r, y_train_r)\n","175af2d7":"nny_train_pred_r = NN.predict(X_train_r)\nnny_test_pred_r = NN.predict(X_test_r)\nprint('Done')","dee91aea":"from sklearn.metrics import mean_absolute_error\nprint('TRAIN MAE')\nprint(mean_absolute_error(y_train_r, nny_train_pred_r))\nprint('TEST MAE')\nprint(mean_absolute_error(y_test_r, nny_test_pred_r))\nprint('NAIVE MAE')\nprint(mean_absolute_error(y_test_r, y_test_pred_naive_r))","57773958":"Use \u201cfrom sklearn.metrics import mean_square_error\u201d and \u201cfrom sklearn.metrics import mean_absolute_error\u201d and calculate the train, test, and test_naive scores.\n\n","a62630bc":"Testing XGBoost","d62cc82b":"Use the \u201cfrom sklearn.model_selection import train_test_split\u201d and split X and y into X_train_r, X_test_r, y_train_r and y_test_r. \n\nUsing a 20% test data size.","ab42c4cc":"Conclusion: Our model is crap.\n","3158c14d":"\n<p>Using \u201cfrom sklearn.metricsimport accuracy_score\u201d and calculate the train, test, and test_naive accuracy.****","d743c16b":"Train a logistic regression algorithm on the training data.","6afffb77":"A little bit better.","c9b330ee":"2) We are only going to use the feature columns \n\u201cDEATH_EVENT, time, age, high_blood_pressure, ejection_fraction, serum_creatinine, serum_sodium\u201d.\n","a4ca2a11":"From the prepped dataset above extract all columns, but only the rows where \u201cDEATH_EVENT\u201d is True (or 1).","042f5f01":"Make prediction using the training and test data, name the prediction variables y_train_pred_r and y_test_pred_r. \n\n\nCreate a y_test_pred_naive_r which has the same shape as y_test_r, but only predicts the average time for the patients who died.","d251ef93":"Using the \u201cfrom sklearn.model_selection import train_test_split\u201d and split X and y into X_train, X_test, y_train and y_test. Using a 20% test data size.","b67cb143":"<h1> Classification","38b06f7c":"Little bit better :)","66d5dcf3":"Make prediction using the training and test data, name the prediction variables y_train_pred and y_test_pred. \n\nCreate a y_test_pred_naive which has the same shape as y_test, but only predicts the most common label.","875979ae":"<h1> 5\n    ","301f1a8b":"Introduction:\n\nCardiovascular diseases (CVDs) are thenumber 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worlwide.\nHeart failure is a common event caused by CVDs and this dataset contains 12 features that can be used to predict mortality by heart failure.","1baf3076":"\n4-b) Adding the three columns to the dataframe with the names \u201c[age_text0, age_text1 and age_text2]","b62ef8e7":"Train a linearregression algorithm on the training data","ea4dfbfe":"3) Creating a new column called \u201cage_text\u201d, where the value is \u201clow\u201d for age<=56, \u201chigh\u201d for age>= 73 and \u201cmedium\u201d for ages between.","69814b2f":"5) Using \u201cfrom sklearn.preprocessing import StandardScaler\u201d and scaling the columns \u201cejection_fraction, serum_creatinine, serum_sodium\u201d.\n\n\nAdding the columns back to the dataframe with the column names \u201cejection_fraction_sc, serum_creatinine_sc, serum_sodium_sc\u201d, and removing the columns \u201cejection_fraction, serum_creatinine, serum_sodium\u201d.","49bbc757":"6) Log scaling for any column where that seams desirable. ","ed669d89":"From the data with only death instances, extract the column \u201ctime\u201d as y, \nand \u201cage_text0, age_text1, age_text2, high_blood_pressure, ejection_fraction_sc, serum_creatinine_sc, serum_sodium_sc\u201d as X.","c3f48b8a":"From the prepped dataset above, extracting the column \u201cDEATH_EVENT\u201d as y, and \u201cage_text0, age_text1, age_text2, high_blood_pressure, ejection_fraction_sc, serum_creatinine_sc, serum_sodium_sc\u201d as X.","d1b0de36":"<h1> Regression\n   ","18bbcd3f":"Using \u201cfromsklearn.metricsimportconfustion_matrix\u201d and print the train, test, and test_naive confusion matrix.","11aab8f7":"Use the \u201cnp.c_[ ]\u201d to get X and y in numpy format ready for training.","bfdfe255":"4) Using the \u201cfrom sklearn.preprocessing import OneHotEncoder\u201d and doing a one hotencoding of \u201cage_text\u201d \n","45c76695":"3) remove the age column","5d19e735":"Try other models example k-nearest-neighbor, SVM. DecisionTrees, Xgboost, try hyperparameter tuning them. \n\nHow good an MSE and MAE are you able to get on the test data?","a9928f17":"The naive model got 35 correct predictions while our model got 41.\nSo our model is better.\n"}}