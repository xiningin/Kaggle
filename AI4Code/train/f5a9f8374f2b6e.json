{"cell_type":{"672e15b7":"code","8171ef5e":"code","b5ccf285":"code","8ea62297":"code","1d805af9":"code","a6de14c2":"code","d5d020f7":"code","d95d29d4":"code","ed7b2127":"code","6ba23443":"code","d87634f9":"code","6f657737":"code","f2b282fa":"code","0ccabab2":"code","60ae8300":"code","4fb85f61":"code","5e0c2a20":"code","57a3d1c2":"code","afa82a56":"code","5f7ddc70":"code","06327e57":"code","c9fbade1":"code","29123bb3":"code","2d25616f":"code","539ee01b":"code","b558bfe2":"code","01a4884a":"code","fad07380":"code","96666fc2":"code","2cb262ed":"code","369e0132":"code","f017010c":"code","959b2ff5":"code","8efbb6a2":"code","77a1a19a":"code","06a24f75":"code","4361b46c":"code","6f3ed020":"code","d708b79b":"code","822324e2":"code","a571e15e":"code","7aed0636":"code","2c064b5d":"code","ccbb4a95":"code","b412b34e":"code","f14d9b41":"code","d444cf33":"code","62d8e120":"code","d3afeefa":"code","c0c0e59b":"code","435b4e22":"code","7a4d5737":"code","456f0adc":"code","665df467":"code","a0893dce":"code","9dbed03d":"code","6161db33":"code","fe9a6c9a":"code","d2d0da8f":"code","7f8f9751":"code","d72d40e8":"code","7d829cfa":"code","e95b97f1":"code","edd990fc":"code","6b24fa11":"code","84a90969":"code","46312f75":"code","d26c734c":"code","559f899e":"code","e03679ae":"code","be56496d":"code","aaf7538d":"code","76182273":"code","2c01e4c7":"code","56652f9b":"code","4cfe8e06":"code","505829e7":"code","49d00350":"code","19382ff8":"code","0ca2112a":"code","2d650900":"code","c220de11":"code","61ae99ed":"code","bcab6e10":"code","9e21ba93":"code","b0f74a39":"code","0e88f02d":"code","2480ff6b":"code","a8d49946":"code","8cdfadc7":"markdown","91b8b66b":"markdown","c7d024bd":"markdown","ba292968":"markdown","c2d96989":"markdown","5e6f1106":"markdown","7a24ee42":"markdown","5c329514":"markdown","2544ffc4":"markdown","8c1fc6ba":"markdown","4822fc03":"markdown","266d77b9":"markdown","c177eb9b":"markdown","84da2e3f":"markdown","fc9ae98c":"markdown","c823fe3f":"markdown","6fa3163e":"markdown","685927d4":"markdown","25bd7366":"markdown","cf9530f3":"markdown","c1a62048":"markdown","bed2f1ea":"markdown","01018878":"markdown","ea060a14":"markdown","69e6b788":"markdown","52ce185f":"markdown","3aa88ff9":"markdown","fe075334":"markdown","6e7ba596":"markdown","38edbd93":"markdown","1873d605":"markdown","a190863e":"markdown","f25183b4":"markdown","905aa355":"markdown","ef101eb5":"markdown","c31f3f8e":"markdown","b8e637a1":"markdown","b07efa2c":"markdown","1d07ce70":"markdown","56b3e7d1":"markdown","4eb913a5":"markdown","6a0896f0":"markdown","9a518430":"markdown","83a42f20":"markdown","9b5978f2":"markdown","8f0e31d9":"markdown","e9160efa":"markdown","71ac12e4":"markdown","837c3214":"markdown","d472fbe5":"markdown","41a06cfc":"markdown","d07ee7a1":"markdown","68ddddea":"markdown","e125f15d":"markdown","b55607e5":"markdown","a14701ea":"markdown","b2686a1a":"markdown","1e123858":"markdown","13c70753":"markdown","c4f8a61f":"markdown","b90d2192":"markdown","a2d9bf5e":"markdown","44877337":"markdown","634235d5":"markdown","42ddf652":"markdown","073721f9":"markdown","32f7304d":"markdown","87151d80":"markdown","dbead123":"markdown","1907b7e1":"markdown","da687e5b":"markdown","ebe41e5e":"markdown","c6122691":"markdown","be3ca49d":"markdown","27340bb9":"markdown","78dd1cec":"markdown","10c667ef":"markdown","8bd0505c":"markdown","a8815a64":"markdown","f3ebb79e":"markdown","dfc0d3d8":"markdown","0d9f79c9":"markdown","0ee2d466":"markdown","af6986ec":"markdown","6f69a690":"markdown","4ca6aa03":"markdown","3e6b39de":"markdown","248f3306":"markdown","6a109ecc":"markdown","768cbfc2":"markdown"},"source":{"672e15b7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8171ef5e":"# Scientific Libraries\nimport pandas as pd\nimport numpy as np\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport seaborn as sns\nstyle.use('fivethirtyeight')\n# Plotly\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.offline as py\n# Booster for rendering\nimport plotly.io as pio\npio.renderers.default = 'iframe'\n\n# Warning ignorance\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Scaling\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, PowerTransformer, RobustScaler\n\n# Selection\nfrom scipy.stats import chi2_contingency\n\n# Machine Learning Algorithm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import cross_val_score,StratifiedKFold\n\n# Metrics Evaluation\nfrom sklearn.metrics import accuracy_score, precision_score \nfrom sklearn.metrics import recall_score, f1_score\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix \nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import plot_confusion_matrix\n\n# Hyperparameter Tuning\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV","b5ccf285":"# Import Train Dataset\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\n# Import Test Dataset\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","8ea62297":"# preview train dataset\ntrain.head()","1d805af9":"# Preview test dataset\ntest.head()","a6de14c2":"# Shape for train & test\nprint('Train dataset consists of ',train.shape[0],'rows &',\n     train.shape[1],'columns')\nprint('Test dataset consists of ',test.shape[0],'rows &',\n     test.shape[1],'columns')","d5d020f7":"print('Train columns :\\n',train.columns,'\\n')\nprint('Test columns :\\n',test.columns)","d95d29d4":"# Get dataset information for train\nprint('Train information :\\n')\nprint(train.info(),'\\n')","ed7b2127":"# Get dataset information for test\nprint('Test information :\\n')\nprint(test.info())","6ba23443":"# Identify missing values for train dataset\nfeature_train = train.isna().sum().keys().tolist()\nmissing_train = train.isna().sum().values.tolist()\nmv_train = pd.DataFrame(list(zip(feature_train, missing_train)), \n                  columns=['feature','missing_value'])\nmv_train['%missing'] = round(((mv_train['missing_value']\/train.shape[0])*100),2)\n\n\n# Identify missing values for test dataset\nfeature_test = test.isna().sum().keys().tolist()\nmissing_test = test.isna().sum().values.tolist()\nmv_test = pd.DataFrame(list(zip(feature_test, missing_test)), \n                  columns=['feature','missing_value'])\nmv_test['%missing'] = round(((mv_test['missing_value']\/test.shape[0])*100),2)\n\n\nprint('Missing Values for Train Data \\n',mv_train,'\\n') \nprint('Missing Values for Test Data \\n',mv_test)","d87634f9":"# Checking zero values\nfor col in list(train.columns):\n    print(col + '  ' + str(train[train[col]==0].shape[0]))","6f657737":"# Investigate how old the passengers who have 0 fare \ntrain[train['Fare']==0]","f2b282fa":"# Investigate how old the passengers who have 0 fare for test data \ntest[test['Fare']==0]","0ccabab2":"# Check duplicated values based on all features\ntrain.duplicated().sum()","60ae8300":"# Check duplicated values based on selected features\ncheck = ['PassengerId','Name','Ticket']\n# Create looping to check\nfor col in check:\n    print('Total duplicated for',col,': ', train.duplicated(subset=col).sum())","4fb85f61":"# Numerical data\nnums = ['PassengerId', 'Age','SibSp', 'Parch', 'Fare']\n# Categorical data\ncats =['Survived','Pclass','Name', 'Sex', 'Ticket', \n       'Cabin', 'Embarked']","5e0c2a20":"# Only display numerical data\ntrain.describe()","57a3d1c2":"# for categorical data\ntrain.describe(include='object')","afa82a56":"plt.figure(figsize=(5,6))\n# Count plot\nax = sns.countplot(train['Survived']);\nplt.xlabel('');\nplt.ylabel('Number of Passengers');\nplt.xticks([0,1],['Not_Survived','Survived']);\nplt.yticks([],[]);\nplt.title('How many passengers did survive & die from the the Titanic shipwreck ?')\n\n# Annotation\n\nfor bar in ax.patches:\n    total = format(bar.get_height(), '.0f')\n    x = bar.get_x() + bar.get_width() \/ 2 - 0.1\n    y = bar.get_y()-40 + bar.get_height()\n    ax.annotate(total, (x, y), size = 16,\n                weight='bold', color = 'w')","5f7ddc70":"plt.figure(figsize=(20,5))\n# Pie chart\nplt.subplot(142)\ntrain['Survived'].value_counts().plot.pie(\n    autopct='%1.1f%%',startangle=90, \n    labels=['Not_Survived','Survived']);\nplt.ylabel('');\nplt.title('Survived & Died Ratio');","06327e57":"plt.figure(figsize=(20,10))\nfor i in range(0, len(nums)):\n    plt.subplot(2,3, i+1)\n    sns.histplot(train[nums[i]], kde = True);\n    plt.ylabel('Number of Passengers')\n    plt.tight_layout();","c9fbade1":"plt.figure(figsize=(12,4))\nfor i in range(0,len(nums)):\n    plt.subplot(1, len(nums), i+1)\n    sns.boxplot(y = train[nums[i]])\n    plt.ylabel(nums[i],size=10)\n    plt.yticks(size = 10)\n    plt.tight_layout()","29123bb3":"# Identify total outlier using IQR\nfor col in nums:\n    \n    # Menghitung nilai IQR\n    Q1 = train[col].quantile(0.25)\n    Q3 = train[col].quantile(0.75)\n    IQR = Q3 - Q1\n    \n    # Define value \n    nilai_min = train[col].min()\n    nilai_max = train[col].max()\n    lower_lim = Q1 - (1.5*IQR)\n    upper_lim = Q3 + (1.5*IQR)\n    \n    # Identify low outlier\n\n    if (nilai_min < lower_lim):\n        print('Low outlier is found in column',col,'<', lower_lim,'\\n')\n        #display total low outlier \n        print('Total of Low Outlier in column',col, ':', len(\n            list(train[train[col] < lower_lim].index)),'\\n')\n    elif (nilai_max > upper_lim):\n        print('High outlier is found in column',col,'>', upper_lim,'\\n')\n        #display total high outlier \n        print('Total of High Outlier in column',col, ':', len(\n            list(train[train[col] > upper_lim].index)),'\\n')\n        \n    else:\n        print('Outlier is not found in column',col,'\\n')   ","2d25616f":"categorical = ['Pclass','Sex','Embarked']\n# Looping for countplot\nfor col in categorical:\n    plt.figure(figsize=(20, 5))\n    # Countplot\n    plt.subplot(141);\n    ax = sns.countplot(x = train[col], \n                       order=train[col].value_counts().index);\n    \n    for p in ax.patches:\n        value = format(p.get_height(), '.0f') \n        x = p.get_x() + p.get_width() \/ 2 - 0.15\n        y = p.get_y() + p.get_height() - 30\n        ax.annotate(value, (x, y), size = 16, weight='bold')\n    plt.xlabel('')\n    plt.ylabel('')\n    plt.yticks([],[])\n    plt.title('Number of Passengers \\nAgainst '+col, size = 16, weight = 'bold')\n    plt.tight_layout();\n    \n    # Pie chart\n    plt.subplot(142);\n    train[col].value_counts().plot.pie(\n        autopct='%1.1f%%', textprops={'fontsize': 12}); \n    plt.title('Passengers Percentage \\nAgainst '+col, size = 16, weight = 'bold')\n    plt.ylabel('');","539ee01b":"sns.heatmap(train.corr(), cmap = 'Blues', fmt='.2f', annot=True);","b558bfe2":"plt.figure(figsize=(7,7))\np=sns.pairplot(train, markers = '+', diag_kind = 'kde', \n                 hue = 'Survived');\n# title\nnew_title = 'Survived ?'\np._legend.set_title(new_title);\n\n# replace labels\nnew_labels = ['No', 'Yes']\nfor t, l in zip(p._legend.texts, new_labels): t.set_text(l);","01a4884a":"def barplot(data,feature, num_or_str) :\n    \n    bar1 = data[(data['Survived'] == 0)]\n    bar2 = data[(data['Survived'] != 0)]\n    ratio = pd.DataFrame(pd.crosstab(data[feature],train['Survived']), )\n    ratio['%_survived'] = round((ratio[1] \/ (ratio[1] + ratio[0]) * 100),2)\n    if num_or_str == True:\n        ratio = ratio.sort_values(0, ascending = False)\n\n    \n    trace1 = go.Bar(x=bar1[feature].value_counts().keys().tolist(),\n                    y=bar1[feature].value_counts().values.tolist(),\n                    name='Not_Survived',opacity = 0.8, marker=dict(\n                    color='green'))\n\n    \n    trace2 = go.Bar(x=bar2[feature].value_counts().keys().tolist(),\n                    y=bar2[feature].value_counts().values.tolist(),\n                    name='Survived', opacity = 0.8, marker=dict(\n                    color='red'))\n    \n    trace3 =  go.Scatter(x=ratio.index, y=ratio['%_survived'], \n                         yaxis = 'y2', name='% Survived', \n                         opacity = 0.6, marker=dict(color='black',\n                                                    line=dict(color='#000000',width=0.5)))\n\n    layout = dict(title =  str(feature)+' Against Survived',xaxis=dict(), \n                  yaxis=dict(title= 'Number of Passengers'), \n                  yaxis2=dict(autorange= True, \n                              overlaying= 'y', \n                              anchor= 'x', \n                              side= 'right',\n                              zeroline=False,\n                              showgrid= False,\n                              title= '% Survived'))\n\n    fig = go.Figure(data=[trace1, trace2, trace3], layout=layout)\n    py.iplot(fig)\n    \ndef single_hist(x,hue):\n    hist = sns.histplot(data=train, x=x, hue=hue, kde=True);\n    return (hist)\n\ndef hist_kde(key,value,x,hue):\n    plt.figure(figsize=(15,5))\n    sex = train[train[key]==value]\n    kde = sns.histplot(data=sex, x=x, hue=hue, kde=True);\n    plt.tight_layout()\n    return(kde)","fad07380":"barplot(train,'Sex',True)","96666fc2":"plt.figure(figsize=(12,5))\nsingle_hist('Age','Survived');\nplt.legend(['Survived','Not_Survived']);\nplt.ylabel('Number of Passengers');","2cb262ed":"# Distribution of Age of Male Passengers against Survived\nhist_kde('Sex','male','Age','Survived');\nplt.legend(['Survived','Not_Survived']);\nplt.title('Distribution of Age of Male Passengers \\n Who Survived or Not');\n\n# Distribution of Age of Female Passengers against Survived\nhist_kde('Sex','female','Age','Survived');\nplt.legend(['Survived','Not_Survived']);\nplt.title('Distribution of Age of Female Passengers \\n Who Survived or Not');","369e0132":"barplot(train,'Parch',False)","f017010c":"barplot(train,'SibSp',False)","959b2ff5":"plt.figure(figsize=(15,8))\nsingle_hist('Fare','Survived')\nplt.legend(['Survived','Not_Survived']);\nplt.ylabel('Number of Passengers');","8efbb6a2":"barplot(train,'Embarked',True)","77a1a19a":"# Duplicate dataframe for pre processing\ntr_pr1 = train.copy()\nts_pr1 = test.copy()","06a24f75":"# Drop PassengerID both train & test data\ntr_pr1.drop('PassengerId',1, inplace=True) # Train data\nts_pr1.drop('PassengerId',1, inplace=True) # Test data","4361b46c":"# Dropna Embarked\ntr_pr1.dropna(subset=['Embarked'],inplace=True)\n\n# Fillna Age with mean\ntr_pr1['Age'].fillna(tr_pr1['Age'].mean(),inplace=True)\n\n# Fillna Cabin with 0\ntr_pr1['Cabin'].fillna(0,inplace=True)\n\n# Replace 0 values in Fare with median due to the distribution is skewed\nmedian_fare = tr_pr1['Fare'].median(skipna=True)\ntr_pr1['Fare']=tr_pr1.Fare.mask(tr_pr1.Fare == 0,median_fare)","6f3ed020":"# Check the missing values\nfeature = tr_pr1.isna().sum().keys().tolist()\nmissing = tr_pr1.isna().sum().values.tolist()\nmv_check = pd.DataFrame(list(zip(feature, missing)), \n                  columns=['feature','missing_value'])\nmv_check['%missing'] = round(((mv_check['missing_value']\/train.shape[0])*100),2)\nmv_check","d708b79b":"# Fillna Age with mean\nts_pr1['Age'].fillna(ts_pr1['Age'].mean(),inplace=True)\n\n# Fillna Cabin with 0\nts_pr1['Cabin'].fillna(0,inplace=True)\n\n# Fillna Fare with median\nts_pr1['Fare'].fillna(ts_pr1['Fare'].median(), inplace=True)\n\n# Replace 0 values in Fare with median \nfare = ts_pr1['Fare'].median(skipna=True)\nts_pr1['Fare']=ts_pr1.Fare.mask(ts_pr1.Fare == 0,fare)","822324e2":"ts_pr1.isna().sum()","a571e15e":"# Drop duplicated values for train data\ntr_pr1.drop_duplicates(subset='Ticket', inplace=True)","7aed0636":"# Create function to get total_cabin\ndef total_cabin(data):\n    # Get total cabin for each passenger\n    list_total = [] # Initiate empty list\n    for i,row in data.iterrows(): # looping in every rows in dataframe\n        if row['Cabin']== 0:        # if the value is 0\n            total = 0               # total cabin is 0\n        else:                                         # if the value isn't 0\n            total = len(str(row['Cabin']).split(' ')) # we'll sum the cabin for every passenger \n        list_total.append(total) # add the empty list with the total value \n    data['total_cabin'] =  list_total","2c064b5d":"# Create function to extract cabin_location\ndef cabin_loc(data):\n    # Get cabin type for each passenger\n    list_loc = []\n    for i, row in data.iterrows():\n        if row['Cabin'] == 0:\n            cabin_loc = 'No Cabin'\n        else:\n            cabin_loc = str(row['Cabin'])[0]\n        list_loc.append(cabin_loc)\n    data['cabin_loc'] = list_loc","ccbb4a95":"# Create function to extract title from Name column\ndef title(data):\n    list_title = []\n    for i, row in data.iterrows():\n        title = row['Name'].split(',')[1].split('.')[0]\n        list_title.append(title)\n    data['Title'] = list_title","b412b34e":"# Get total cabin for each passenger\ntotal_cabin(tr_pr1)","f14d9b41":"tr_pr1['total_cabin'].value_counts()","d444cf33":"# Total Cabin against Survived\npd.pivot_table(tr_pr1, index = 'Survived', columns = 'total_cabin',\n               values = 'Name' ,aggfunc ='count')","62d8e120":"total_cabin_gr = tr_pr1.groupby(['total_cabin','Survived'])['Name'].count().reset_index()\ntotal_cabin_gr.rename(columns={'Name':'Passenger'}, inplace=True)\ntotal_cabin_gr['Total_Passenger'] = total_cabin_gr.groupby(['total_cabin'])['Passenger'].transform('sum')\ntotal_cabin_gr['Ratio %'] = round(((total_cabin_gr['Passenger'] \/ total_cabin_gr['Total_Passenger'])*100),2)\ntotal_cabin_gr","d3afeefa":"# Get cabin loc for each passenger\ncabin_loc(tr_pr1)","c0c0e59b":"tr_pr1['cabin_loc'].value_counts()","435b4e22":"# Cabin Location against Survived\npd.pivot_table(tr_pr1, index = 'Survived', columns = 'cabin_loc',\n               values = 'Name' ,aggfunc ='count')","7a4d5737":"# Get the passengers' title\ntitle(tr_pr1)","456f0adc":"tr_pr1['Title'].value_counts()","665df467":"# Passengers' title against Survived\npd.pivot_table(tr_pr1, index = 'Survived', columns = 'Title',\n               values = 'Name' ,aggfunc ='count')","a0893dce":"# Get total cabin for each passenger\ntotal_cabin(ts_pr1)","9dbed03d":"# Get cabin type for each passenger\ncabin_loc(ts_pr1)","6161db33":"# Get the passengers' title\ntitle(ts_pr1)","fe9a6c9a":"new_num = ['Age', 'SibSp', 'Parch','Fare','total_cabin']\nplt.figure(figsize=(12,8))\nfor i in range(0, len(new_num)):\n    plt.subplot(2,3, i+1)\n    sns.histplot(tr_pr1[new_num[i]], kde=True)\n    plt.tight_layout()","d2d0da8f":"# Apply log transformation\ntr_pr1['SibSp_log'] = (tr_pr1['SibSp']+1).apply(np.log)\ntr_pr1['Parch_log'] = (tr_pr1['Parch']+1).apply(np.log)\ntr_pr1['Fare_log'] = (tr_pr1['Fare']+1).apply(np.log)\ntr_pr1['total_cabin_log'] = (tr_pr1['total_cabin']+1).apply(np.log)","7f8f9751":"log_num = ['SibSp_log', 'Parch_log','Fare_log','total_cabin_log']\nplt.figure(figsize=(12,8))\nfor i in range(0, len(log_num)):\n    plt.subplot(2,3, i+1)\n    sns.histplot(tr_pr1[log_num[i]], kde=True)\n    plt.tight_layout()","d72d40e8":"plt.figure(figsize=(12,8))\nfor i in range(0, len(new_num)):\n    plt.subplot(2,3, i+1)\n    sns.histplot(ts_pr1[new_num[i]], kde=True, color = 'Orange')\n    plt.tight_layout()","7d829cfa":"# Apply log transformation\nts_pr1['SibSp_log'] = (ts_pr1['SibSp']+1).apply(np.log)\nts_pr1['Parch_log'] = (ts_pr1['Parch']+1).apply(np.log)\nts_pr1['Fare_log'] = (ts_pr1['Fare']+1).apply(np.log)\nts_pr1['total_cabin_log'] = (ts_pr1['total_cabin']+1).apply(np.log)","e95b97f1":"plt.figure(figsize=(12,8))\nfor i in range(0, len(log_num)):\n    plt.subplot(2,3, i+1)\n    sns.histplot(ts_pr1[log_num[i]], kde=True, color='Orange')\n    plt.tight_layout()","edd990fc":"# Scaling for Train data\ntr_pr1['Age_scale'] = RobustScaler().fit_transform(tr_pr1['Age'].values.reshape(len(tr_pr1), 1))\ntr_pr1['SibSp_scale'] = RobustScaler().fit_transform(tr_pr1['SibSp_log'].values.reshape(len(tr_pr1), 1))\ntr_pr1['Parch_scale'] = RobustScaler().fit_transform(tr_pr1['Parch_log'].values.reshape(len(tr_pr1), 1))\ntr_pr1['Fare_scale'] = RobustScaler().fit_transform(tr_pr1['Fare_log'].values.reshape(len(tr_pr1), 1))\ntr_pr1['total_cabin_scale'] = StandardScaler().fit_transform(tr_pr1['total_cabin_log'].values.reshape(len(tr_pr1), 1))","6b24fa11":"# Scaling for Test data\nts_pr1['Age_scale'] = RobustScaler().fit_transform(ts_pr1['Age'].values.reshape(len(ts_pr1), 1))\nts_pr1['SibSp_scale'] = RobustScaler().fit_transform(ts_pr1['SibSp_log'].values.reshape(len(ts_pr1), 1))\nts_pr1['Parch_scale'] = RobustScaler().fit_transform(ts_pr1['Parch_log'].values.reshape(len(ts_pr1), 1))\nts_pr1['Fare_scale'] = RobustScaler().fit_transform(ts_pr1['Fare_log'].values.reshape(len(ts_pr1), 1))\nts_pr1['total_cabin_scale'] = StandardScaler().fit_transform(ts_pr1['total_cabin_log'].values.reshape(len(ts_pr1), 1))","84a90969":"# Train dataset\ncats_train = ['Pclass', 'Sex', 'Embarked', 'Title', 'cabin_loc']\nfor cat in cats_train:\n    onehots_train = pd.get_dummies(tr_pr1[cat], prefix=cat)\n    tr_pr1 = tr_pr1.join(onehots_train)\n\ntr_pr1.drop(columns=cats_train,inplace=True)","46312f75":"# Test dataset\ncats_test = ['Pclass', 'Sex', 'Embarked', 'Title', 'cabin_loc']\nfor cat in cats_test:\n    onehots_test = pd.get_dummies(ts_pr1[cat], prefix=cat)\n    ts_pr1 = ts_pr1.join(onehots_test)\n\nts_pr1.drop(columns=cats_test,inplace=True)","d26c734c":"tr_pr1.drop(['Name','Ticket','Cabin'],1, inplace=True)\nts_pr1.drop(['Name','Ticket','Cabin'],1, inplace=True)","559f899e":"tr_pr1['Title_Dona'] = 0\nts_pr1['Title_ Mlle'] = 0\nts_pr1['Title_ Mme'] = 0\nts_pr1['Title_Don'] = 0\nts_pr1['cabin_loc_T'] = 0\nts_pr1['Title_Major'] = 0\nts_pr1['Title_Lady'] = 0\nts_pr1['Title_Jonkheer'] = 0","e03679ae":"X_train = tr_pr1.drop(['Age', 'SibSp', 'Parch', 'Fare', 'total_cabin', 'SibSp_log',\n       'Parch_log', 'Fare_log', 'total_cabin_log','Survived'],1)\ny_train = tr_pr1['Survived']\nX_test = ts_pr1.drop(['Age', 'SibSp', 'Parch', 'Fare', 'total_cabin', 'SibSp_log',\n       'Parch_log', 'Fare_log', 'total_cabin_log'],1)","be56496d":"# Create function for cross validation score\ndef cross_val(Model, X_train, y_train, cval):\n    model = Model # initiate model\n    kfold = StratifiedKFold(n_splits=cval, random_state=42, shuffle=True)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n    cv_mean = cv_results.mean()\n    cv_std = cv_results.std()\n    return round(cv_mean,4), round(cv_std,4)","aaf7538d":"# Inititate algorithm\nlr = LogisticRegression(random_state=42)\ndt = DecisionTreeClassifier(random_state=42)\nrf = RandomForestClassifier(random_state=42)\nknn = KNeighborsClassifier(n_neighbors=5)\nsvc = SVC(random_state=42)\ngb = GradientBoostingClassifier(random_state=42)\nxgb = XGBClassifier(random_state=42,eval_metric='mlogloss')\n\n# Create function to make the result as dataframe \ndef model_cv_comparison(X_train,y_train):  \n    \n    # Logistic Regression\n    lr_cv_score_mean, lr_cv_score_std = cross_val(lr, X_train, y_train, 10)\n    # Decision Tree\n    dt_cv_score_mean, dt_cv_score_std = cross_val(dt, X_train, y_train, 10)\n    # Random Forest\n    rf_cv_score_mean, rf_cv_score_std = cross_val(rf, X_train, y_train, 10)\n    #KNN\n    knn_cv_score_mean, knn_cv_score_std = cross_val(knn, X_train, y_train, 10)\n    # SVC\n    svc_cv_score_mean, svc_cv_score_std = cross_val(svc, X_train, y_train, 10)\n    # GB\n    gb_cv_score_mean, gb_cv_score_std = cross_val(gb, X_train, y_train, 10)\n    # XGBoost\n    xgb_cv_score_mean, xgb_cv_score_std = cross_val(xgb, X_train, y_train, 10)\n    \n    \n    models = ['Logistic Regression','Decision Tree','Random Forest',\n             'KNN','SVC','Gradient Boosting','XGBoost']\n    cv_mean = [lr_cv_score_mean, dt_cv_score_mean, rf_cv_score_mean, \n                   knn_cv_score_mean, svc_cv_score_mean,gb_cv_score_mean, xgb_cv_score_mean]\n    cv_std = [lr_cv_score_std, dt_cv_score_std, rf_cv_score_std, \n                   knn_cv_score_std, svc_cv_score_std, gb_cv_score_std, xgb_cv_score_std]\n    \n    model_comparison = pd.DataFrame(data=[models, cv_mean, cv_std]).T.rename(\n                                                            {0: 'Model',\n                                                             1: 'CV_Mean',\n                                                             2: 'CV_Stdev'}, axis=1)\n    \n    return model_comparison","76182273":"model_cv_comparison(X_train,y_train)","2c01e4c7":"# List Hyperparameters \npenalty = ['l2','l1','elasticnet']\nC = [0.0001, 0.001, 0.002, 0.01] \nhyperparameters = dict(penalty=penalty, C=C)\n\n# Initiate the model\nlogres = LogisticRegression(random_state=42) \nmodel_lr = RandomizedSearchCV(logres, hyperparameters, cv=10, random_state=42,  scoring='accuracy')\n\n# Fitting Model & Evaluation\nmodel_lr.fit(X_train, y_train)\nprint(model_lr.best_estimator_)\nprint(model_lr.best_score_)","56652f9b":"# Hyperparameter lists to be tested\nmax_depth = list(range(1,10)) \nmin_samples_split = list(range(5,10)) \nmin_samples_leaf = list(range(5,15)) \nmax_features = ['auto', 'sqrt', 'log2'] \ncriterion = ['gini','entropy']\nsplitter = ['best','random']\n\n# Initiate hyperparameters\nhyperparameters = dict(max_depth=max_depth, \n                       min_samples_split=min_samples_split, \n                       min_samples_leaf=min_samples_leaf,\n                       max_features=max_features,\n                       criterion = criterion,\n                       splitter = splitter)\n\n# Initiate the model\nmodel_dt = RandomizedSearchCV(dt, hyperparameters, cv=10, random_state=42,  scoring='accuracy')\n\n# Fitting Model & Evaluation\nmodel_dt.fit(X_train, y_train)\nprint(model_dt.best_estimator_)\nprint(model_dt.best_score_)","4cfe8e06":"# Hyperparameter lists to be tested\nmax_depth = list(range(1,10)) \nmin_samples_split = list(range(5,10)) \nmin_samples_leaf = list(range(5,15)) \nmax_features = ['auto', 'sqrt', 'log2'] \ncriterion = ['gini','entropy']\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\nbootstrap = [True,False]\n\n# Initiate hyperparameters\nhyperparameters = dict(max_depth=max_depth, \n                       min_samples_split=min_samples_split, \n                       min_samples_leaf=min_samples_leaf,\n                       max_features=max_features,\n                       criterion = criterion,\n                       n_estimators = n_estimators,\n                       bootstrap = bootstrap)\n# Initiate model\nmodel_rf = RandomizedSearchCV(rf, hyperparameters, cv=10, random_state=42,  scoring='accuracy')\n\n# Fitting Model & Evaluation\nmodel_rf.fit(X_train, y_train)\nprint(model_rf.best_estimator_)\nprint(model_rf.best_score_)","505829e7":"#List Hyperparameters that we want to tune.\nleaf_size = list(range(1,50))\nn_neighbors = list(range(1,30))\np=[1,2]\n\n#Convert to dictionary\nhyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n\n# Initiate model\nmodel_knn = RandomizedSearchCV(knn, hyperparameters, cv=10, random_state=42,  scoring='accuracy')\n\n# Fitting Model & Evaluation\nmodel_knn.fit(X_train, y_train)\nprint(model_knn.best_estimator_)\nprint(model_knn.best_score_)","49d00350":"# Hyperparameter lists to be tested\nkernel = ['linear', 'poly', 'rbf', 'sigmoid']\nC = [0.0001, 0.001, 0.002, 0.1] \ngamma = ['scale', 'auto']\ndegree = [0, 1, 2, 3, 4, 5, 6]\n\n#Convert to dictionary\nhyperparameters = dict(kernel=kernel, C=C, gamma=gamma, degree = degree)\n\n# Initiate model\nmodel_svc = RandomizedSearchCV(svc, hyperparameters, cv=10, random_state=42, \n                           scoring='accuracy')\n\n# Fitting Model & Evaluation\nmodel_svc.fit(X_train, y_train)\nprint(model_svc.best_estimator_)\nprint(model_svc.best_score_)","19382ff8":"# Hyperparameter lists to be tested\nmax_depth = list(range(1,10)) \nmin_samples_split = list(range(5,10)) \nmin_samples_leaf = list(range(5,15)) \nmax_features = ['auto', 'sqrt', 'log2'] \nlearning_rate = [0.05,0.1,0.02,0.5]\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\nloss = ['deviance', 'exponential']\ncriterion = ['friedman_mse', 'mse', 'mae']\n\n# Initiate hyperparameters\nhyperparameters = dict(max_depth=max_depth, \n                       min_samples_split=min_samples_split, \n                       min_samples_leaf=min_samples_leaf,\n                       max_features=max_features,\n                       learning_rate=learning_rate,\n                       n_estimators=n_estimators,\n                       loss = loss,\n                       criterion = criterion)\n\n# Initiate the model\nmodel_gb = RandomizedSearchCV(gb, hyperparameters, cv=10, random_state=42,  scoring='accuracy')\n\n# Fitting Model & Evaluation\nmodel_gb.fit(X_train, y_train)\nprint(model_gb.best_estimator_)\nprint(model_gb.best_score_)","0ca2112a":"# Hyperparameter lists to be tested\nhyperparameters={\n \"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n \"min_child_weight\" : [ 1, 3, 5, 7 ],\n \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ]    \n}\n# Initiate the model\nmodel_xgb = RandomizedSearchCV(xgb, hyperparameters, cv=10, random_state=42,  scoring='accuracy')\n\n# Fitting Model & Evaluation\nmodel_xgb.fit(X_train, y_train)\nprint(model_xgb.best_estimator_)\nprint(model_xgb.best_score_)\n","2d650900":"# Inititate algorithm\nlr_tuned = LogisticRegression(C=0.01, random_state=42)\ndt_tuned = DecisionTreeClassifier(max_depth=3, max_features='log2', min_samples_leaf=11,\n                       min_samples_split=5, random_state=42)\nrf_tuned = RandomForestClassifier(criterion='entropy', max_depth=2, max_features='sqrt',\n                       min_samples_leaf=10, min_samples_split=9,\n                       n_estimators=1200, random_state=42)\nknn_tuned = KNeighborsClassifier(leaf_size=6, n_neighbors=21, p=1)\nsvc_tuned = SVC(C=0.1, degree=1, kernel='poly', random_state=42)\ngb_tuned = GradientBoostingClassifier(learning_rate=0.02, max_depth=1, max_features='sqrt',\n                           min_samples_leaf=12, min_samples_split=8,\n                           n_estimators=1400, random_state=42)\nxgb_tuned = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.7, eval_metric='mlogloss',\n              gamma=0.1, gpu_id=-1, importance_type='gain',\n              interaction_constraints='', learning_rate=0.2, max_delta_step=0,\n              max_depth=4, min_child_weight=5,\n              monotone_constraints='()', n_estimators=100, n_jobs=4,\n              num_parallel_tree=1, random_state=42, reg_alpha=0, reg_lambda=1,\n              scale_pos_weight=1, subsample=1, tree_method='exact',\n              validate_parameters=1, verbosity=None)\n\n# Create function to make the result as dataframe \ndef model_cv_comparison_tuned(X_train,y_train):  \n    \n    # Logistic Regression\n    lr_cv_score_mean_t, lr_cv_score_std_t = cross_val(lr_tuned, X_train, y_train, 10)\n    # Decision Tree\n    dt_cv_score_mean_t, dt_cv_score_std_t = cross_val(dt_tuned, X_train, y_train, 10)\n    # Random Forest\n    rf_cv_score_mean_t, rf_cv_score_std_t = cross_val(rf_tuned, X_train, y_train, 10)\n    #KNN\n    knn_cv_score_mean_t, knn_cv_score_std_t = cross_val(knn_tuned, X_train, y_train, 10)\n    # SVC\n    svc_cv_score_mean_t, svc_cv_score_std_t = cross_val(svc_tuned, X_train, y_train, 10)\n    # GB\n    gb_cv_score_mean_t, gb_cv_score_std_t = cross_val(gb_tuned, X_train, y_train, 10)\n    # XGBoost\n    xgb_cv_score_mean_t, xgb_cv_score_std_t = cross_val(xgb_tuned, X_train, y_train, 10)\n    \n    \n    models_tuned = ['Logistic Regression','Decision Tree','Random Forest',\n             'KNN','SVC','Gradient Boosting','XGBoost']\n    cv_mean_tuned = [lr_cv_score_mean_t, dt_cv_score_mean_t, rf_cv_score_mean_t, \n               knn_cv_score_mean_t, svc_cv_score_mean_t,gb_cv_score_mean_t,\n               xgb_cv_score_mean_t]\n    cv_std_tuned = [lr_cv_score_std_t, dt_cv_score_std_t, rf_cv_score_std_t, \n              knn_cv_score_std_t, svc_cv_score_std_t, gb_cv_score_std_t,\n              xgb_cv_score_std_t]\n    \n    model_comparison_tuned = pd.DataFrame(data=[models_tuned, cv_mean_tuned, cv_std_tuned]).T.rename(\n                                                            {0: 'Model',\n                                                             1: 'CV_Mean',\n                                                             2: 'CV_Stdev'}, axis=1)\n    \n    return model_comparison_tuned","c220de11":"model_cv_comparison_tuned(X_train,y_train)","61ae99ed":"# Logistic Regression\nmodel_lr = lr_tuned\nmodel_lr.fit(X_train, y_train)\npredictions_lr = model_lr.predict(X_test)\n\noutput_lr = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': predictions_lr})\noutput_lr.to_csv('my_submission_LR_Tuned.csv', index=False)\nprint(\"My LR submission was successfully saved!\")","bcab6e10":"# Decision Tree\nmodel_dt = dt_tuned\nmodel_dt.fit(X_train, y_train)\npredictions_dt = model_dt.predict(X_test)\n\noutput_dt = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': predictions_dt})\noutput_dt.to_csv('my_submission_DT_Tuned.csv', index=False)\nprint(\"My DT submission was successfully saved!\")","9e21ba93":"# Random Forest\nmodel_rf = rf_tuned\nmodel_rf.fit(X_train, y_train)\npredictions_rf = model_rf.predict(X_test)\n\noutput_rf = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': predictions_rf})\noutput_rf.to_csv('my_submission_RF_Tuned.csv', index=False)\nprint(\"My RF submission was successfully saved!\")","b0f74a39":"# KNN\nmodel_knn = knn_tuned\nmodel_knn.fit(X_train, y_train)\npredictions_knn = model_knn.predict(X_test)\n\noutput_knn = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': predictions_knn})\noutput_knn.to_csv('my_submission_KNN_Tuned.csv', index=False)\nprint(\"My KNN submission was successfully saved!\")","0e88f02d":"# SVC\nmodel_svc = svc_tuned\nmodel_svc.fit(X_train, y_train)\npredictions_svc = model_svc.predict(X_test)\n\noutput_svc = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': predictions_svc})\noutput_svc.to_csv('my_submission_SVC_Tuned.csv', index=False)\nprint(\"My SVC submission was successfully saved!\")","2480ff6b":"# Gradient Boosting\nmodel_gb = gb_tuned\nmodel_gb.fit(X_train, y_train)\npredictions_gb = model_gb.predict(X_test)\n\noutput_gb = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': predictions_gb})\noutput_gb.to_csv('my_submission_GB_Tuned.csv', index=False)\nprint(\"My GB submission was successfully saved!\")","a8d49946":"# XGBoost\nmodel_xgb = xgb_tuned\nmodel_xgb.fit(X_train, y_train)\npredictions_xgb = model_xgb.predict(X_test)\n\noutput_xgb = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': predictions_xgb})\noutput_xgb.to_csv('my_submission_XGB_Tuned.csv', index=False)\nprint(\"My XGB submission was successfully saved!\")","8cdfadc7":"**The highest average cross validation score is KNN 0.8132 & 0.051 for standar deviation. But let's try hyper-parameter tuning**","91b8b66b":"<font size ='4'>**Summary :**<\/font>\n- Survive is the target, 0 value is the passengers who didn't survive\n- `SibSp` & `Parch`, 0 value means the passengers didn't travel with parent, children, sibling, or spouse\n- `Fare` shouldn't has 0 value which means cruising for free, but maybe free for infant. let's dig more about this.","c7d024bd":"### 2.8.3. Box Plot","ba292968":"**The best score is XGBoost.**","c2d96989":"<font size ='4'>**Summary :**<\/font>  \n- Name is unique\n- Around 65% passengers are male and the rests are female.\n- Ticket number has around 23% duplicated values\n- Cabin feature also has duplicated values, but we can assume that some passengers can share the cabin.\n- Most of passengers embarked from Southampton Port.","5e6f1106":"## 4.4.3. Random Forest","7a24ee42":"<font size='4'>**Train Data**<\/font>  ","5c329514":"<font size ='4'>**Summary :**<\/font>  \n- Passengers who didn't have cabin were more likely to survive\n- No passenger survived with 3 cabin\n- No passenger died with 4 cabin","2544ffc4":"Apart from the distributions, the range of values is also an important aspect of features. The above plot tell us that the features have different scales. We also can identify outlier values, like `Age`, `SibSp`, `Parch`, & `Fare` have outlier values. ","8c1fc6ba":"<font size ='4'>**Summary :**<\/font>  \nFeature `SibSp`, `Parch`, & `Fare` have right-skewed distribution. We'll visualize box plot to identify the outlier.","4822fc03":"## 4.2. Modelling","266d77b9":"<font size ='4'>**Summary :**<\/font>  \n- Passengers who didn't have cabin were more likely to survive\n- No passenger survived in deck T\n","c177eb9b":"# 4. Modelling & Prediction","84da2e3f":"<font size='4'>**Do the more expensive ticket fares influence to survive?**<\/font>","fc9ae98c":"<font size='3.8'>*The passengers who bought ticket under 100 dollars are more likely to die, while the passengers with the ticket more than 270 dollars can survive. It seems that there are outlier values in this feature*<\/font>","c823fe3f":"### 2.8.2. Histogram Plot","6fa3163e":"## 4.4.5. SVC","685927d4":"<font size ='4'>**Summary :**<\/font>  \n- Train dataset consists of 12 columns & 891 rows. Sample is only 40% from the actual number of passengers.\n- Our target is `Survived`, others we call them features\n- There are 3 data types, such as float, int, & object\n- We can see that some of columns have missing values, such as `Age`, `Cabin`, & `Embarked`.\n- Categorical : `Survived`,`Name`, `Sex`, `Ticket`, `Cabin`, `Embarked`, `Pclass`.\n- Numerical : `PassengerId`, `Age`, `SibSp`, `Parch`, `Fare`.","25bd7366":"<font size='3.8'>*Number of parents or children influence to survive. The passengers who traveled with more 3 parents or children are tend to die, especially none survived with 4 or 6 parents or children*<\/font>","cf9530f3":"<font size='3.5'>There are 4 features with the missing values, such as `Age`, `Cabin`, `Fare`, & `Embarked`.<\/font>","c1a62048":"No passangers survived in the range of age more than 64 and less than 80 years old.","bed2f1ea":"**All missing values for test data have been handled.**","01018878":"## 2.7. Separate Numerical & Categorical Data","ea060a14":"### 3.2.3. Scaling","69e6b788":"<font size ='4'>**Summary :**<\/font>  \n- The lowest age is around 4 months old or baby, and the highest is 80 years old. The average of age is 29 years old. Distribution of age looks normal.\n- average of ticket fare are around $32. Fare distribution looks skewed.","52ce185f":"<font size ='4'>**Summary :**<\/font>\n\n- around 50% of passengers are from lower socio-economic, followed by upper passengers 24.2%, and the least passengers from middle class.\n- 64.8% passengers are male, and the rests are female.\n- 72.4% passengers embarked from Southampton Port, followed by Cherbourg Port 18.9%, and 8.7% passengers embarked from Queenstown Port.","3aa88ff9":"total outlier values are 386 or 43% of the dataset.","fe075334":"## 2.9. Multivariate Analysis","6e7ba596":"<font size ='4'>**Summary :**<\/font>  \n- **38.4% Passengers or equal to 342 survived** from the Titanic shipwreck.\n- **61.6% Passengers or equal to 342 died** from the Titanic shipwreck.","38edbd93":"<font size='4'>**Which Gender Had Better Chance to Survive**<\/font>","1873d605":"## 3.1. Data Cleaning","a190863e":"### 3.1.1. Drop Irrelevant Features","f25183b4":"<font size='4'>**Do the number of parents & children influence to survived ?**<\/font>","905aa355":"### 2.9.2. Pair Plot","ef101eb5":"<font size='4'>**Do the number of siblings & spouse influence to survived ?**<\/font>","c31f3f8e":"# 3. Data Pre Processing","b8e637a1":"## 4.4.2. Decision Tree","b07efa2c":"## 2.4. Data Information","1d07ce70":"## 4.5. Tuned Model","56b3e7d1":"# 2. Exploratory Data Analysis (EDA)","4eb913a5":"### 3.1.3. Drop Duplicated Values","6a0896f0":"## 2.1. Import Library","9a518430":"<font size='4'>**Train Data**<\/font>","83a42f20":"## 4.6. Prediction","9b5978f2":"## 1.3. Objective  \nTo achieve the goals, we have to take the actions through building a predictive model that answers the goals using passenger data (ie name, age, gender, socio-economic class, etc).","8f0e31d9":"## 4.4.6. Gradient Boosting","e9160efa":"<font size ='4'>**Summary :**<\/font>\n\n- The upper-class passengers are tend to survive than middle & lower class. The passengers in the range of age between 0.42 months - 60 years old and traveled with their families had a greater chance to survive. You will get what you pay!\n- The more families the less chance to survive","71ac12e4":"### 2.9.1. Heatmap Correlation","837c3214":"### 2.8.2. Target Visualization","d472fbe5":"## 2.2. Load Dataset","41a06cfc":"Probability of male passengers to survived are lower than female, especially in the range of age 15 - 55. But we can see that male passenger who under 5 years old can survive better.","d07ee7a1":"### 2.9.3. Plot With Hue = 'Survived'","68ddddea":"### 2.8.4. Categorical Plot","e125f15d":"## 1.1. Problem Statement  \nThe sinking of the Titanic is one of the most infamous shipwrecks in history.  \n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in **the death of 1502 out of 2224 passengers and crew**.  \n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.  ","b55607e5":"<font size='4'>**Test Data**<\/font>","a14701ea":"<font size='4'>**The passengers who boarded from which port survived the most ?**<\/font>","b2686a1a":"## 4.4.4. KNN","1e123858":"## 4.4.7. XG Boost","13c70753":"<font size='3.8'>*Women are more likely to survive compared to men*<\/font>","c4f8a61f":"## 2.6. Duplicated Values","b90d2192":"## 4.4. Hyper-parameter Tuning  \n\nI will do hyper-parameters optimization for the algorithms. The accuracy of the algorithms depend on the optimal set of parameters. This time, I'll use RandomizedSearchCV for the automation of hyper-parameters tuning, so that the processing will be faster than using GridSearchCV.\n\n","a2d9bf5e":"### 3.2.2. Log Transformation","44877337":"<font size='4'>**How about age distribution against survived ?**<\/font>","634235d5":"## 2.3. Columns","42ddf652":"## 2.8. Univariate Analysis","073721f9":"## 1.2. Goals \n\nThe goals of this machine learning project :  \n- to know what sorts of people were more likely to survive or die of the Titanic disaster at the beginning of the 20th century \n- to know what features which the most influence the passengers to survive","32f7304d":"### 3.2.1. Extraction","87151d80":"<font size ='4'>**Summary :**<\/font>  \nThere are 210 rows or around 23% duplicated values based on Ticket number.","dbead123":"## 2.2. Data Shape","1907b7e1":"### 2.8.1 Descriptive Statistic","da687e5b":"# 1. Problem Understanding","ebe41e5e":"<font size ='4'>**Summary :**<\/font> \n- For train dataset, feature `cabin` has the highest missing value 77.10%, followed by `age` 19.87%, and the least is `Embarked` 0.22%.  If `cabin` is not correlated with the target, we'll drop it. \n- For test dataset, feature `cabin` also has the highest missing value 78.23% of data, followed by `age` 20.57%, & the least is `Fare`. ","c6122691":"## 4.1. Separate Feature & Target ","be3ca49d":"## 3.2. Feature Engineering  ","27340bb9":"<font size ='4'>**Summary :**<\/font>\n\n- Target `Survived` has positive correlation with `Fare` and negative correlation with `Pclass`, but both of them are weak correlation. It means that the upper class passengers & costly ticket tend to survive.\n- Feature `Pclass` has moderate negative correlation with `Fare` & `Age`. The higher passengers class the more expensive ticket. And upper class passengers are mostly not dominated with young people.\n- Feature `SibSp` has moderate positive correlation with `Parch` and weak negative correlation with `Age` & `Fare`.","78dd1cec":"<font size='4'>**Test Data**<\/font>  ","10c667ef":"<font size ='4'>**Summary :**<\/font>  \n- Test dataset consists of 11 columns & 418 rows.\n- Target `Survived` is not available due to this data will be used to test our model\n- We can see that some of columns have missing values, such as `Age`, `Cabin`, & `Fare.","8bd0505c":"<font size ='4'>**Summary :**<\/font>  \nThere are 17 passengers for both train & test dataset who have 0 for ticket fare. It looks all of these passengers are not infant, some have age data and some don't have age data. The conclusion that I'll fill these datas later.","a8815a64":"<font size='3.8'>*Passengers embarked from Cherbourg Port had the highest survived ratio compared to others, while the least who embarked from Queenstown Port*<\/font>","f3ebb79e":"# Predicting Passengers Who Survived or Not in Titanic Cruise\n","dfc0d3d8":"<font size='4'>**Train Data**<\/font>","0d9f79c9":"**All missing values for train data have been handled.**","0ee2d466":"### 4.4.1. Logistic Regression","af6986ec":"## 2.5. Missing Values","6f69a690":"<font size='3.8'>*Number of sibings & spouse influence to survive. The more siblings or spouse aboard the less chance to survive. The survived ratio starts decreasing after 1, it means that the passengers who traveled with more 1 sibling\/spouse are hard to survive.*<\/font>","4ca6aa03":"## 2.3. Preview Data","3e6b39de":"### 3.2.3. One Hot Encoding","248f3306":"<font size='4'>**Test Data**<\/font>  \nI also extract in Test data for modelling purpose.","6a109ecc":"### 3.1.2. Handle Missing Values","768cbfc2":"<font size ='4'>**Summary :**<\/font>  \nThe greater title of Passengers didn't guarantee they survived"}}