{"cell_type":{"0567a90d":"code","4adae1e4":"code","14968667":"markdown","2dac1bc5":"markdown","269ad7ac":"markdown","0b1e2b4b":"markdown","ac4ebb9b":"markdown","1c0869e6":"markdown"},"source":{"0567a90d":"# 1.Correlation Matrix\n# import pandas as pd\n# import seaborn as sns\n\n# df = pd.read_csv('data.csv')\n\n# sns.heatmap(df.drop('target', axis=1), cmap='cool', annot=True)","4adae1e4":"# 2. Variance Inflation Factor (VIF)\n# import pandas as pd\n# import matplotlib.pyplot as plt\n# import statsmodels.formula.api as smf\n# from statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# data = pd.read_csv('dara.csv', index_col=0)\n# y = df['target'] # dependent variable\n# X = df.drop('target', axis=1)\n\n# vif = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n\n# print(vif)","14968667":"\u201cA better way to assess multi-collinearity is to compute the variance inflation factor (VIF)\u2026.. As a rule of thumb, a VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity\u2026.. When faced with the problem of collinearity, there are two simple solutions. The first is to drop one of the problematic variables from the regression\u2026.. The second solution is to combine the collinear variables together into a single predictor. For instance, we might take the average\u2026\u201d - from the book <Introduction to Statistical Learning with Applications in R>","2dac1bc5":"So based on the few places that I have been to and read(above). Here is what I can conclude.\n1. Normally, the test data is not so different from the train data. Therefore, multicollinearity does not hurt the prediction accuracy. However, when it is different (in terms of what? refer to the notes\/links above), it WILL hurt your accuracy\n2. Multicollinearity as a whole will hurt your interpretability of the model(s) being used. This means that, you will not know definitely how one feature affect the model, given that all other features are constant, but since it has correlation with some other features, it is hard to determine it.\n3. Decision trees are not affected by multicollinearity. Why? You can refer here - https:\/\/datascience.stackexchange.com\/questions\/31402\/multicollinearity-in-decision-tree or other relevant sources that you can find online.","269ad7ac":"Okay, that's it! Quick and simple! :)","0b1e2b4b":"**Multicollinearity**\n\n1. Multicollinearity in your training dataset should only reduce predictive performance in the test dataset if the covariance between variables in your training and test datasets is different. If the covariance structure (and consequently the multicollinearity) is similar in both training and test datasets - https:\/\/stats.stackexchange.com\/questions\/361247\/multicollinearity-and-predictive-performance\n\n2. There is an important qualifier in the continuation of the first cited quote from Wikipedia: \"Multicollinearity does not reduce the predictive power or reliability of the model as a whole, at least within the sample data set\" (emphasis added).\nUnless there is a singular design matrix, multicollinearity does not prevent fitting a model to an individual data sample. I take that to be the point of the first Wikipedia quote. A regression model can capture the data sample well, including all of the peculiarities and noise of the particular data sample, in the presence of multicollinearity.\nThe problems arise when you try to apply the model outside the original data set to the underlying population. Multicollinearity will severely affect performance outside of the original data sample, as you recognize. - https:\/\/stats.stackexchange.com\/questions\/190075\/does-multicollinearity-affect-performance-of-a-classifier\n\n3. It's a problem for causal inference - or rather, it indicates difficulties in causal inference - but it's not a particular problem for prediction\/forecasting (unless it's so extreme that it prevents model convergence or results in singular matrices, and then you won't get predictions anyway).- https:\/\/stats.stackexchange.com\/questions\/268966\/is-multicollinearity-really-a-problem\n\n4. Your question is on the impact of multicollinearity on prediction. The answers offered here focused on the impact of coefficients. Indeed, multicollinearity inflates the standard error of the coefficients resulting in their instability. This instability creates a risk in coefficient interpretation and can mislead the hypothesis testing (e.g., t-tests) of coefficients. So, from this perspective, I agree with the other answers. BUT, you didn\u2019t ask the question of multicollinearity and coefficient interpretation. You asked about prediction. Multicollinearity does not affect the overall fit or the predictions from the model. This is not to advocate throwing in redundant variables. Indeed, if two predictor variables are almost one and the same (e.g., GDP and GNP), then it is a bit silly to have both variables in the model (even if their inclusion doesn\u2019t have adverse effects on prediction). It seems more reasonable to exclude one of the variables on the principle of model parsimony (everything equal, go with the simpler model). - https:\/\/www.quora.com\/Do-I-care-about-multicollinearity-or-interaction-when-my-goal-is-prediction\n\n5. First of all, handling multicollinearity is not mandatory depending on your purpose and your model. For example, tree based models such as decision tree, bagging, random forest, and boosting are robust to multicollinearity issues in their aspect of prediction accuracy.- https:\/\/www.quora.com\/How-do-you-handle-multicollinearity-in-data-science-models\n\n6. As mentioned, 0.2 is nothing to worry about in your specific case. But to answer the question, \"how seriously should I consider the effect of multicollinearity in my regression model?\" recognize that multicollinearity does not reduce the ability of the model to be predictive (you may still do a good job predicting your response variable in the presence of multicollinearity). It does, however, affect the quality of the interpretation of the individual predictors. In other words, in the presence of multicollinearity you may not have reliable results (indicated by large standard errors) about any individual \u03b2 coefficient. - https:\/\/stats.stackexchange.com\/questions\/137425\/how-seriously-should-i-consider-the-effects-of-multicollinearity-in-my-regressio\n\n7. Finally, since these issues affect the interpretability of the models, or the ability to make inferences based on the results, we can safely say that a multicollinearity or collinearity will not affect the results of predictions from decision trees. During inference from the decision tree models though, it is important to take how each feature may be affected by another into account to help make valuable business decisions.- https:\/\/medium.com\/future-vision\/collinearity-what-it-means-why-its-bad-and-how-does-it-affect-other-models-94e1db984168\n\n8. Regardless, if you are just in the business of predicting, you don\u2019t really care if there is a collinearity, but to have a more interpretable model, you should avoid features that have a very high (~R\u00b2 > .8) being contained in the features. \nWe want to use these models to help us to understand the world around us, and figure out where to take our data exploration. Therefore when applying linear regression, you may want to use different models for prediction and one for interpretation\/inference.\n- https:\/\/medium.com\/future-vision\/collinearity-what-it-means-why-its-bad-and-how-does-it-affect-other-models-94e1db984168","ac4ebb9b":"There are 2 ways multicollinearity is usually checked - https:\/\/towardsdatascience.com\/https-towardsdatascience-com-multicollinearity-how-does-it-create-a-problem-72956a49058\n1. Correlation Matrix\n2. Variance Inflation Factor (VIF)","1c0869e6":"Okay guys, I have compiled a list of explanations and its links so if you have trouble you can just go to the links and read more :)"}}