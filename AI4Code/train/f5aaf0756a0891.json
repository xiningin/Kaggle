{"cell_type":{"0d42a00e":"code","e508011c":"code","6082f29c":"code","7303c76c":"code","32af99bc":"code","5bd2ec17":"code","c3bf873a":"code","4839db56":"code","4a97f21f":"code","52bfffc6":"code","c035d05d":"code","4898eb02":"code","c98079f5":"code","efba2d93":"code","2c18e396":"code","4bd4f986":"code","7c376b5e":"code","4189e36c":"code","35ea3584":"code","d1157632":"code","f7178394":"code","ba823f3d":"code","5a648a65":"code","cd3a8617":"code","8bab7f27":"code","415a049b":"code","4999b53f":"code","28ea18cf":"code","a95a5460":"code","a6a04c71":"code","ac6a8ac6":"code","8e5d8285":"code","8a1cf60d":"code","8235b215":"code","00a33e8c":"code","481ae74e":"code","8a4617aa":"code","fd2c0120":"code","9d6c183b":"code","29b4e5df":"code","c62ee208":"code","18c0d4da":"code","4c253ad2":"markdown","576570f0":"markdown","9bb63854":"markdown","f4f768f0":"markdown","d8cba1d5":"markdown","d6a13834":"markdown","8e734d5a":"markdown","60bb4862":"markdown","a251c955":"markdown","b19a60ed":"markdown","2675400b":"markdown","794cb841":"markdown","dc0f6ff3":"markdown","c7d8a862":"markdown","2a1e59c4":"markdown","47fad6b2":"markdown","074001b8":"markdown","9f6aa74d":"markdown","b5d64e76":"markdown"},"source":{"0d42a00e":"#import the pandas library.\n#we need to handle with arrays. So, we import numpy library as well.\nimport pandas as pd\nimport numpy as np\n\ntrain = pd.read_csv('https:\/\/raw.githubusercontent.com\/gurusabarish\/Titanic-prediction\/master\/train.csv')\ntest = pd.read_csv('https:\/\/raw.githubusercontent.com\/gurusabarish\/Titanic-prediction\/master\/test.csv')","e508011c":"#Print the first few rows of train dataset.\ntrain.head()","6082f29c":"# we dont use name and Ticket columns. So, we can drop those columns.\ntrain.drop(['Name', 'Ticket'],axis=1, inplace=True)","7303c76c":"#find the number of rows and columns\ntrain.shape","32af99bc":"#print the columns with his datatype and non-null(filled) counts.\ntrain.info()","5bd2ec17":"# We should few columns in test data as well as train dta\ntest.drop(['Name', 'Ticket'],axis=1, inplace=True)","c3bf873a":"#import libraries\n# we will find \"how many peoples are have the same value in respective column?\"\n\nimport matplotlib.pyplot as plt\ntrain.hist(figsize=(20,10), color='maroon', bins=20)\nplt.show()","4839db56":"# Count of missing data in each columns\ntrain.isnull().sum()","4a97f21f":"# Find the percentage of missing data with respective columns.\ncolumns = train.columns\nfor i in columns:\n    percentage = (train[i].isnull().sum()\/891)*100\n    print(i,\"\\t %.2f\" %percentage)","52bfffc6":"#Cabin has almost 80% missing data.So, we are drop the column.\ntrain.drop(['Cabin'], axis=1, inplace=True)","c035d05d":"#Embrked column has only few NAN value.So, drop the row which is contain the missing data.\ntrain.dropna(subset=['Embarked'], inplace=True)","4898eb02":"# Fill the NAN values with mean in Age column\ntrain['Age'].fillna(train['Age'].mean(), inplace=True)","c98079f5":"# Check the columns are don't contain the NAN values.\ntrain.isnull().any()","efba2d93":"test.drop(['Cabin'], axis=1, inplace=True)","2c18e396":"test.isnull().sum()","4bd4f986":"test['Age'].fillna(train['Age'].mean(), inplace=True)\ntest['Fare'].fillna(train['Fare'].mean(), inplace=True)","7c376b5e":"test.isnull().any()","4189e36c":"# Import the function from sklearn library\nfrom sklearn import preprocessing\nlabel = preprocessing.LabelEncoder()","35ea3584":"# Find out the columns which is have object data type and store the column names.\nobject_columns = []\nfor i in train.columns:\n    if train[i].dtype==object:\n        object_columns.append(i)\nprint(object_columns)","d1157632":"# Convert the columns's object data to numerical data.\nfor i in object_columns:\n    train[i] = label.fit_transform(train[i])","f7178394":"# Verify the dataset is contain only numerical values.\ntrain.info()","ba823f3d":"test_object_columns = []\nfor i in test.columns:\n    if test[i].dtype==object:\n        test_object_columns.append(i)\nprint(test_object_columns)","5a648a65":"for i in test_object_columns:\n    test[i] = label.fit_transform(test[i])","cd3a8617":"test.info()","8bab7f27":"x = train.drop('Survived', axis=1)\ny = train['Survived']","415a049b":"from sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier","4999b53f":"cross_val_score(LogisticRegression(), x, y).mean()","28ea18cf":"cross_val_score(SVC(), x, y).mean()","a95a5460":"cross_val_score(RandomForestClassifier(), x, y).mean()","a6a04c71":"cross_val_score(GaussianNB(), x, y).mean()","ac6a8ac6":"cross_val_score(DecisionTreeClassifier(), x, y).mean()","8e5d8285":"# Split the train dataset to build model.\nfrom sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(x, y, test_size=0.30, random_state=5)","8a1cf60d":"from sklearn.metrics import confusion_matrix,accuracy_score\ndef evaluate(model, x_test, y_test):\n  prediction = model.predict(x_test)\n  print(accuracy_score(y_test, prediction))","8235b215":"from sklearn.model_selection import GridSearchCV# Create the parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [4,6,8,10,12],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'min_samples_leaf': [1, 3, 4, 5],\n    'min_samples_split': [1, 2, 4, 5],\n    'n_estimators': [100, 200, 300, 1000]\n}# Create a based model\nrf = RandomForestClassifier()# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)","00a33e8c":"# Fit the grid search to the data\ngrid_search.fit(x_train, y_train)\nbest = grid_search.best_params_","481ae74e":"grid_search.best_estimator_","8a4617aa":"model = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n                       criterion='gini', max_depth=12, max_features='auto',\n                       max_leaf_nodes=None, max_samples=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=5,\n                       min_weight_fraction_leaf=0.0, n_estimators=200,\n                       n_jobs=None, oob_score=False, random_state=None,\n                       verbose=0, warm_start=False)\nmodel.fit(x_train,y_train)","fd2c0120":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix,accuracy_score\n\npredictions = model.predict(x_test)\nprint(classification_report(y_test, predictions))\nprint(confusion_matrix(y_test, predictions))\nprint(accuracy_score(y_test, predictions))","9d6c183b":"test.head()","29b4e5df":"survived=model.predict(test)\nsurvived","c62ee208":"ids = test['PassengerId']\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': survived })\noutput.head()","18c0d4da":"output.to_csv('output.csv', index=False)","4c253ad2":"> # Test data","576570f0":"> # Train data","9bb63854":"# Find the best algorithm","f4f768f0":"> # Cross valitation","d8cba1d5":"# Handle the missing data","d6a13834":"# Evaluation","8e734d5a":"> # Test data","60bb4862":"# **Import dataset**","a251c955":"> # Train data","b19a60ed":"[![](https:\/\/cdn.iconscout.com\/icon\/free\/png-256\/github-109-438058.png)](https:\/\/github.com\/gurusabarish\/Titanic-prediction)\n# Train dataset: [https:\/\/github.com\/gurusabarish\/Titanic-prediction\/blob\/master\/train.csv](https:\/\/github.com\/gurusabarish\/Titanic-prediction\/blob\/master\/train.csv)\n# Test dataset: [https:\/\/github.com\/gurusabarish\/Titanic-prediction\/blob\/master\/test.csv](https:\/\/github.com\/gurusabarish\/Titanic-prediction\/blob\/master\/test.csv)\n> Github profile: [https:\/\/github.com\/gurusabarish](https:\/\/github.com\/gurusabarish)","2675400b":"> # Train data","794cb841":"# Visualization","dc0f6ff3":"# Change data into numerical","c7d8a862":"# Random forest model","2a1e59c4":"> # Test data","47fad6b2":"# Find the best parameters for our model using grid search","074001b8":"> # From cross validation, We can say randomforest is the best model for our dataset.","9f6aa74d":"# Analize the dataset","b5d64e76":"# Train and test split"}}