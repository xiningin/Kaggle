{"cell_type":{"cd4cee1c":"code","5b9b0f79":"code","77d6405f":"code","30280588":"code","b32801b4":"code","4c0277dc":"code","c7f2dbf0":"code","a0246523":"code","f0a2a10e":"code","e7dee206":"code","1a93a65c":"code","4caddf6c":"code","5e005cdd":"code","a4ad1bd7":"code","5ac3e6b8":"code","bf16c994":"code","f25918a9":"code","3081e68f":"code","33afd8de":"code","a01e47be":"code","abaee1ca":"code","9d27b0af":"markdown","88317845":"markdown","17515d0a":"markdown","3352f06e":"markdown","1a5d27be":"markdown","cf0e5628":"markdown","1a50659f":"markdown","779dc297":"markdown","db5522b3":"markdown","a6ca3d7d":"markdown","9a9adf0b":"markdown","82f7964c":"markdown","2bf7457d":"markdown","3416d71b":"markdown","8c908dfe":"markdown","19c5469b":"markdown","dd81677b":"markdown"},"source":{"cd4cee1c":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom keras.datasets import mnist","5b9b0f79":"df = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_train.csv')\ndf.head()","77d6405f":"categories = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\nidx_category = {k: v for v, k in enumerate(categories)}","30280588":"plt.figure(figsize=(14, 12))\n\nfor i in range(0,20):\n    splt = plt.subplot(7, 10, i+1)\n    plt.imshow(df.iloc[:, 1:].values[i].reshape(28, 28))\n    plt.title(\"{}\".format(categories[df.iloc[:, 0].values[i]]))\n    plt.xticks([])\n    plt.yticks([])\n\nplt.tight_layout()","b32801b4":"labels = df.iloc[:, 0].to_numpy()\ntrain = df.iloc[:, 1:].to_numpy()","4c0277dc":"train = train.reshape(train.shape[0], 28, 28)","c7f2dbf0":"params = {}\nparams['epochs'] = 100\nparams['batch_size'] = 64\nparams['nx_g'] = 100\nparams['nh_g'] = 128\nparams['nh_d'] = 128\nparams['lr'] = 1e-3\nparams['dr'] = 1e-4\nparams['image_size'] = 28\nparams['display_epochs'] = 5","a0246523":"theta = {}\ngamma = {}\n\n# Generator\ntheta['W0_g'] = np.random.randn(params['nx_g'], params['nh_g']) * np.sqrt(2. \/ params['nx_g'])  # 100x128\ntheta['b0_g'] = np.zeros((1, params['nh_g']))  # 1x100\n\ntheta['W1_g'] = np.random.randn(params['nh_g'], params['image_size'] ** 2) * np.sqrt(2. \/ params['nh_g'])  # 128x784\ntheta['b1_g'] = np.zeros((1, params['image_size'] ** 2))  # 1x784\n\n# Discriminator\ntheta['W0_d'] = np.random.randn(params['image_size'] ** 2, params['nh_d']) * np.sqrt(2. \/ params['image_size'] ** 2)  # 784x128\ntheta['b0_d'] = np.zeros((1, params['nh_d']))  # 1x128\n\ntheta['W1_d'] = np.random.randn(params['nh_d'], 1) * np.sqrt(2. \/ params['nh_d'])  # 128x1\ntheta['b1_d'] = np.zeros((1, 1))  # 1x1","f0a2a10e":"def sigmoid(x):\n    return 1. \/ (1. + np.exp(-x))\n\n\ndef dsigmoid(x):\n    y = sigmoid(x)\n    return y * (1. - y)\n\n\ndef dtanh(x):\n    return 1. - np.tanh(x) ** 2\n\n\ndef lrelu(x, alpha=1e-2):\n    return np.maximum(x, x * alpha)\n\n\ndef dlrelu(x, alpha=1e-2):\n    dx = np.ones_like(x)\n    dx[x < 0] = alpha\n    return dx","e7dee206":"def sample_images(images, epoch, show):\n        images = np.reshape(images, (params['batch_size'], params['image_size'], params['image_size']))\n\n        fig = plt.figure(figsize=(4, 4))\n\n        for i in range(16):\n            plt.subplot(4, 4, i + 1)\n            plt.imshow(images[i] * 127.5 + 127.5, cmap='gray')\n            plt.axis('off')\n\n        if show == True:\n            plt.show()\n        else:\n            plt.close()","1a93a65c":"def forward_generator(z):\n        gamma['z0_g'] = np.dot(z, theta['W0_g']) + theta['b0_g']\n        gamma['a0_g'] = lrelu(gamma['z0_g'], alpha=0)\n\n        gamma['z1_g'] = np.dot(gamma['a0_g'], theta['W1_g']) + theta['b1_g']\n        gamma['a1_g'] = np.tanh(gamma['z1_g'])\n     \n    \ndef forward_discriminator(x):\n        gamma['z0_d'] = np.dot(x, theta['W0_d']) + theta['b0_d']\n        gamma['a0_d'] = lrelu(gamma['z0_d'])\n\n        z1_d = np.dot(gamma['a0_d'], theta['W1_d']) + theta['b1_d']\n        a1_d = sigmoid(z1_d)\n        return z1_d, a1_d","4caddf6c":"def update(theta, grads, lr):\n    for i,t in enumerate(theta):\n        theta[i] -= grads[i] * lr","5e005cdd":"def combine_real_fake_grads(real_grads, fake_grads):\n    grads = np.array(real_grads) + np.array(fake_grads)\n    return grads","a4ad1bd7":"def backward_discriminator(x_real, z1_real, a1_real, x_fake, z1_fake, a1_fake):\n        da1_real = -1. \/ (a1_real + 1e-8)  # 64x1\n\n        dz1_real = da1_real * dsigmoid(z1_real)  # 64x1\n        db1_real = np.sum(dz1_real, axis=0, keepdims=True)\n        dW1_real = np.dot(gamma['a0_d'].T, dz1_real)\n\n        da0_real = np.dot(dz1_real, theta['W1_d'].T)\n        dz0_real = da0_real * dlrelu(gamma['z0_d'])\n        \n        db0_real = np.sum(dz0_real, axis=0, keepdims=True)\n        dW0_real = np.dot(x_real.T, dz0_real)\n\n        # fake input gradients -np.log(1 - a1_fake)\n        da1_fake = 1. \/ (1. - a1_fake + 1e-8)\n\n        dz1_fake = da1_fake * dsigmoid(z1_fake)\n        db1_fake = np.sum(dz1_fake, axis=0, keepdims=True)\n        dW1_fake = np.dot(gamma['a0_d'].T, dz1_fake)\n\n        da0_fake = np.dot(dz1_fake, theta['W1_d'].T)\n        dz0_fake = da0_fake * dlrelu(gamma['z0_d'], alpha=0)\n        \n        db0_fake = np.sum(dz0_fake, axis=0, keepdims=True)\n        dW0_fake = np.dot(x_fake.T, dz0_fake)\n\n        # Combine gradients for real & fake images\n        grads = combine_real_fake_grads(np.array([dW0_real, dW1_real, db0_real, db1_real]), np.array([dW0_fake, dW1_fake, db0_fake, db1_fake]))\n\n        # Update gradients\n        update([theta['W0_d'], theta['W1_d'], theta['b0_d'], theta['b1_d']], grads, params['lr'])","5ac3e6b8":"def backward_generator(z, x_fake, z1_fake, a1_fake):\n        da1_d = -1.0 \/ (a1_fake + 1e-8)  # 64x1\n\n        dz1_d = da1_d * dsigmoid(z1_fake)\n        da0_d = np.dot(dz1_d, theta['W1_d'].T)\n        dz0_d = da0_d * dlrelu(gamma['z0_d'])\n        dx_d = np.dot(dz0_d, theta['W0_d'].T)\n\n        # Backprop through Generator\n        dz1_g = dx_d * dtanh(gamma['z1_g'])\n        dW1_g = np.dot(gamma['a0_g'].T, dz1_g)\n        db1_g = np.sum(dz1_g, axis=0, keepdims=True)\n\n        da0_g = np.dot(dz1_g, theta['W1_g'].T)\n        dz0_g = da0_g * dlrelu(gamma['z0_g'], alpha=0)\n        dW0_g = np.dot(z.T, dz0_g)\n        db0_g = np.sum(dz0_g, axis=0, keepdims=True)\n\n        # Update gradients\n        update([theta['W0_g'], theta['W1_g'], theta['b0_g'], theta['b1_g']], [dW0_g, dW1_g, db0_g, db1_g], params['lr'])","bf16c994":"def preprocess_data(x, y):\n        x_train = []\n        y_train = []\n\n        # limit the data to a subset of digits from 0-9\n        for i in range(y.shape[0]):\n            if y[i] in params['numbers']:\n                x_train.append(x[i])\n                y_train.append(y[i])\n\n        x_train = np.array(x_train)\n        y_train = np.array(y_train)\n\n        # limit the data to full batches only\n        num_batches = x_train.shape[0] \/\/ params['batch_size']\n        x_train = x_train[: num_batches * params['batch_size']]\n        y_train = y_train[: num_batches * params['batch_size']]\n\n        # flatten the images (_,28,28)->(_, 784)\n        x_train = np.reshape(x_train, (x_train.shape[0], -1))\n\n        # normalise the data to the range [-1,1]\n        x_train = (x_train.astype(np.float32) - 127.5) \/ 127.5\n\n        # shuffle the data\n        idx = np.random.permutation(len(x_train))\n        x_train, y_train = x_train[idx], y_train[idx]\n        return x_train, y_train, num_batches","f25918a9":"discim_losses = []  # stores the disciminator losses\ngener_losses = []  # stores the generator losses","3081e68f":"def loss_fn(a1_d_real, a1_d_fake):\n    # Cross Entropy\n    discim_loss = np.mean(-np.log(a1_d_real) - np.log(1 - a1_d_fake))\n    discim_losses.append(discim_loss)\n\n    gener_loss = np.mean(-np.log(a1_d_fake))\n    gener_losses.append(gener_loss)\n    \n    return gener_loss, discim_loss","33afd8de":"X_train, X_test, y_train, y_test = train_test_split(train, labels, test_size=0.33, random_state=42)","a01e47be":"# Chose the item to GAN\nparams['numbers'] = [idx_category['t-shirt']]","abaee1ca":"# preprocess input; note that labels aren't needed\nx_train, _, num_batches = preprocess_data(X_train, y_train)\n\nfor epoch in range(params['epochs']):\n    for i in range(num_batches):\n        # Prepare unput and z - noise\n        x_real = x_train[i * params['batch_size']: (i + 1) * params['batch_size']]\n        z = np.random.normal(0, 1, size=[params['batch_size'], params['nx_g']])  # 64x100\n\n        # Forward\n        forward_generator(z)\n\n        z1_d_real, a1_d_real = forward_discriminator(x_real)\n        z1_d_fake, a1_d_fake = forward_discriminator(gamma['a1_g'])\n\n        # Cross Entropy\n        gener_loss, discim_loss = loss_fn(a1_d_real, a1_d_fake)\n        \n        # Backward\n        backward_discriminator(x_real, z1_d_real, a1_d_real, gamma['a1_g'], z1_d_fake, a1_d_fake)\n        backward_generator(z, gamma['a1_g'], z1_d_fake, a1_d_fake)\n\n    if epoch % params['display_epochs'] == 0:\n        print(\"Epoch : \", epoch, \" Loss: \", gener_loss)\n        sample_images(gamma['a1_g'], epoch, show=True)\n\n    # reduce learning rate after every epoch\n    params['lr'] = params['lr'] * (1.0 \/ (1.0 + params['dr'] * epoch))","9d27b0af":"<a id='exploratory'><\/a>\n# Exploratory Data Analysis","88317845":"<a id='forward'><\/a>\n### Forward functions","17515d0a":"# GAN from scratch","3352f06e":"![gan_architecture-1.png](attachment:gan_architecture-1.png)","1a5d27be":"<a id='activation'><\/a>\n### Activation functions and derivatives","cf0e5628":"The structure of the code and Image designed by Christina Kouridi<br>\nhttps:\/\/christinakouridi.blog","1a50659f":"A generative adversarial network (GAN) is a class of machine learning frameworks designed by Ian Goodfellow and his colleagues in 2014. Two neural networks contest with each other in a game (in the sense of game theory, often but not always in the form of a zero-sum game). Given a training set, this technique learns to generate new data with the same statistics as the training set. For example, a GAN trained on photographs can generate new photographs that look at least superficially authentic to human observers, having many realistic characteristics. Though originally proposed as a form of generative model for unsupervised learning, GANs have also proven useful for semi-supervised learning, fully supervised learning, and reinforcement learning.\n\nhttps:\/\/en.wikipedia.org\/wiki\/Generative_adversarial_network","779dc297":"### Complementary functions","db5522b3":"### Setup parameters","a6ca3d7d":"<a id='loss'><\/a>\n### Cross-Entropy Loss Function","9a9adf0b":"### Split data by batches and label","82f7964c":"<a id='plot'><\/a>\n### Plot image function","2bf7457d":"<a id='definition'><\/a>\n# Definition","3416d71b":"<a id='implementation'><\/a>\n# Implementation","8c908dfe":"## Content table\n1. [Definition](#definition)<br>\n2. [Exploratory Analysis](#exploratory)<br>\n3. [Implementation](#implementation)<br>\n    3.1 [Activation Functions](#activation)<br>\n    3.2 [Plot Function](#plot)<br>\n    3.3 [Forward Functions](#forward)<br>\n    3.4 [Backward Functions](#backward)<br>\n    3.5 [Cross-Entropy Loss Function](#loss)<br>\n    3.3 [Training](#training)<br>","19c5469b":"<a id=\"backward\"\/><\/a>\n### Backward functions","dd81677b":"<a id='training'><\/a>\n### Training"}}