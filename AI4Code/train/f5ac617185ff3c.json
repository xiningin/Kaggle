{"cell_type":{"6ab76346":"code","6ba02ac9":"code","29c9c215":"code","38dc1f40":"code","8a504eb6":"code","702611e2":"code","cc590082":"code","62bb4f4c":"code","f2ca75a3":"code","cc5edbf6":"code","7a4b9267":"code","2aaad032":"code","edf5d013":"code","9f51c0c2":"code","6f346c3b":"code","19858d9f":"code","7a22bc04":"code","b84481df":"code","8001304e":"code","20fae218":"code","6241484f":"code","eaa7c641":"code","d0793b3f":"code","893152df":"code","b81d5073":"code","a7b469e2":"code","a99f71a7":"code","d57450ae":"code","db3ec7cc":"code","0bb81b1a":"code","bf0f4a07":"code","59207911":"code","0ee62010":"code","d2b193bf":"code","e64e466c":"code","5531069b":"code","a02357f2":"code","c8486c79":"code","afc0902c":"code","9a1ca16d":"code","ea78c3e0":"code","75d62c00":"code","a72889b7":"code","61b27f76":"code","a054f846":"code","c51aced6":"code","53c73e9b":"code","085303fc":"code","16fb19f2":"code","5666274d":"code","f8e0a23b":"code","eef920c5":"code","59858437":"markdown","5e09f1a1":"markdown","94c5ca36":"markdown","2ddc0ad8":"markdown","654fd90c":"markdown","24541dbc":"markdown","fc743455":"markdown","7307a5db":"markdown","b41b5f87":"markdown","dff76893":"markdown","e10082d9":"markdown","993b85dc":"markdown","cdc5621a":"markdown","f9ea0e26":"markdown","22d56210":"markdown","91effb7a":"markdown","59cbc6a4":"markdown","6b6f6210":"markdown","1560cdd3":"markdown","9bb5e2ba":"markdown","e05b7399":"markdown","4cda72c5":"markdown","d34f3f39":"markdown","3fb781b9":"markdown","b9902b9a":"markdown","5fde2a74":"markdown","6314a6d0":"markdown","f438c6fe":"markdown","45a88c83":"markdown","67dd7ac6":"markdown","f2652148":"markdown","1842c205":"markdown","e5e2922d":"markdown","82c082ba":"markdown","440e3b9e":"markdown","f44facd7":"markdown","42f14ec6":"markdown"},"source":{"6ab76346":"#Import the required packages\nimport pandas as pd\nimport numpy as np\nfrom numpy.random import seed\nfrom numpy.random import randint\nimport re\nimport gc\nimport glob\nimport json\nimport time\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nfrom tqdm import tqdm\nprint(\"Basic Packages Loaded\")","6ba02ac9":"rebuild = True # if True the df_covid dataframe which hosts all data is build from scratch , else the earlier generated dataframe is reloaded\ntrainDoc2Vec = True # if True Doc2Vec Model would be re-trained else stored model is loaded from previous run\nfilename = 'df_covid_Apr-16-2020.csv'\ncsv_path = filename","29c9c215":"if rebuild :\n    # Code adopted from https:\/\/www.kaggle.com\/maksimeren\/covid-19-literature-clustering\n    #Load Metadata\n    root_path = '\/kaggle\/input\/CORD-19-research-challenge\/'\n    metadata_path = f'{root_path}\/metadata.csv'\n    meta_df = pd.read_csv(metadata_path, dtype={\n        'pubmed_id': str,\n        'Microsoft Academic Paper ID': str, \n        'doi': str})\n    # Load all Json \n    root_path = '\/kaggle\/input\/CORD-19-research-challenge\/'\n    all_json = glob.glob(f'{root_path}\/**\/*.json', recursive=True)\n    print(len(all_json))\n    \n    #A File Reader Class which loads the json and make data available\n    class FileReader:\n        def __init__(self, file_path):\n            with open(file_path) as file:\n                content = json.load(file)\n                self.paper_id = content['paper_id']\n                self.abstract = []\n                self.body_text = []\n                # Abstract\n                try:\n                    if content['abstract']:\n                        for entry in content['abstract']:\n                            self.abstract.append(entry['text'])  \n                except KeyError:\n                    #do nothing\n                    pass \n                # Body text\n                for entry in content['body_text']:\n                    self.body_text.append(entry['text'])\n                self.abstract = '\\n'.join(self.abstract)\n                self.body_text = '\\n'.join(self.body_text)\n        def __repr__(self):\n            return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'\n        \n    first_row = FileReader(all_json[0])\n    print(first_row)\n        \n    #Utiliity to add line breaks so that titles and abstracts can be displayed on hoover\n    def get_breaks(content, length):\n         data = \"\"\n         words = content.split(' ')\n         total_chars = 0\n\n         # add break every length characters\n         for i in range(len(words)):\n            total_chars += len(words[i])\n            if total_chars > length:\n                data = data + \"<br>\" + words[i]\n                total_chars = 0\n            else:\n                data = data + \" \" + words[i]\n         return data\n    \n    #Create a dictionary which is eventually copied into a dataframe \n    dict_ = None\n    dict_ = {'paper_id': [], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'journal': [], 'title_summary': [],'abstract_summary': [], 'publish_year': [], 'publish_date': [],'doi': []}\n    for idx, entry in enumerate(all_json):\n        if idx % (len(all_json) \/\/ 10) == 0:\n            print(f'Processing index: {idx} of {len(all_json)}')\n        content = FileReader(entry)\n\n        # get metadata information\n        meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n        # no metadata, skip this paper\n        if len(meta_data) == 0:\n            continue\n\n        dict_['paper_id'].append(content.paper_id)\n        dict_['abstract'].append(content.abstract)\n        dict_['body_text'].append(content.body_text)\n\n        # also create a column for the summary of abstract to be used in a plot\n        if len(content.abstract) == 0: \n            dict_['abstract_summary'].append(\"Not Available\")\n        elif len(content.abstract.split(' ')) > 100:\n            # abstract provided is too long for plot, take first 100 words append with ...\n            info = content.abstract.split(' ')[:100]\n            summary = get_breaks(' '.join(info), 40)\n            dict_['abstract_summary'].append(summary + \"...\")\n        else:\n            # abstract is short enough\n            summary = get_breaks(content.abstract, 40)\n            dict_['abstract_summary'].append(summary)\n\n        # get metadata information\n        meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n\n        try:\n            # if more than one author\n            authors = meta_data['authors'].values[0].split(';')\n            if len(authors) > 2:\n                # more than 2 authors, may be problem when plotting, so take first 2 append with ...\n                dict_['authors'].append(\". \".join(authors[:2]) + \"...\")\n            else:\n                # authors will fit in plot\n                dict_['authors'].append(\". \".join(authors))\n        except Exception as e:\n            # if only one author - or Null valie\n            dict_['authors'].append(meta_data['authors'].values[0])\n\n        # add the title information, add breaks when needed\n        try:\n            title = get_breaks(meta_data['title'].values[0], 40)\n            dict_['title_summary'].append(title)\n            dict_['title'].append(meta_data['title'].values[0])\n        # if title was not provided\n        except Exception as e:\n            dict_['title_summary'].append(\"Not Available\")\n            dict_['title'].append(\"Not Available\")\n\n\n        # add the journal information\n        dict_['journal'].append(meta_data['journal'].values[0])\n\n        #add the year where available from the meta data \n        dict_['publish_year'].append((pd.DatetimeIndex(meta_data['publish_time']).year).values[0])\n        \n        #add the date as a separate column \n        dict_['publish_date'].append((meta_data['publish_time']).values[0])\n\n        #add the doi where available from the meta data \n        dict_['doi'].append(meta_data['doi'].values[0])\n\n    #print(len(dict_['paper_id']), len(dict_['abstract']),len(dict_['body_text']),len(dict_['authors']),len(dict_['title']),len(dict_['journal']),len(dict_['title_summary']),len(dict_['abstract_summary']),len(dict_['publish_year']),len(dict_['doi']))\n    df_covid = pd.DataFrame(dict_, columns=['paper_id', 'abstract', 'body_text', 'authors', 'title', 'journal', 'title_summary','abstract_summary', 'doi', 'publish_year','publish_date'])\n    \n    #Format the doi so that it can be used as a link\n    def doi_url(d):\n        if d=='':\n            return \"Not Available\"\n        if str(d).startswith('http:\/\/'):\n            return str(d)\n        elif str(d).startswith('doi.org'):\n            return f'http:\/\/{str(d)}'\n        else:\n            return f'http:\/\/doi.org\/{str(d)}'\n    df_covid['doi'] = df_covid['doi'].apply(lambda x: doi_url(x))\n    \n    #Drop duplicates of where the body text is same\n    df_covid.drop_duplicates(['body_text'], inplace=True)\n    df_covid['body_text'].describe(include='all')\n    \n    #Mark cells where data is not available\n    df_covid.isna().sum()\n    \n    #Mark missing data so that the same is available while printing\n    df_covid['title'].replace(np.nan, 'Not Available', regex=True, inplace=True)\n    df_covid['journal'].replace(np.nan, 'Not Available', regex=True, inplace=True)\n    df_covid['authors'].replace(np.nan, 'Not Available', regex=True, inplace=True)\n    df_covid['abstract'].replace(np.nan, 'Not Available', regex=True, inplace=True)\n    df_covid['abstract'].replace(\"\", 'Not Available', regex=True, inplace=True)\n    \n    #Provide a timestamp and store the csv file so that it can be directly loaded\n    t = time.localtime()\n    timestamp = time.strftime('%b-%d-%Y', t)\n    filename = (\"df_covid_\" + timestamp +'.csv')\n    df_covid.to_csv(filename , index = False) \nelse :\n    df_covid = pd.read_csv(csv_path)","38dc1f40":"# Helper function to draw word clouds to analyze content which is either to short or long to ensure that they are relevant\nfrom wordcloud import WordCloud, STOPWORDS\nstopwords = set(STOPWORDS)\n\ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stopwords,\n        max_words=1000,\n        max_font_size=40, \n        scale=3,\n        random_state=1 # chosen at random by flipping a coin; it was heads\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(12, 12))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()\n","8a504eb6":"bodylengths = []\nfor ind in df_covid.index: \n    bodylengths.append(len(df_covid['body_text'][ind]))\nbodylengths = np.array(bodylengths)\nprint(\"Mean {}\".format(bodylengths.mean()))\nprint(\"Min {}\".format(bodylengths.min()))\nprint(\"Max {}\".format(bodylengths.max()))\nprint(\"Bottom 1 Percentile {}\".format(np.percentile(bodylengths, 1)))","702611e2":"plt.hist(bodylengths,range=[0, 5000]) \nplt.title('Word Count of Papers')\nplt.xlabel('Word Count')\nplt.ylabel('Number of Papers')\nplt.show()","cc590082":"df_covid_short = df_covid[(df_covid['body_text'].apply(lambda x: len(x))<2200)]\nprint(df_covid_short.shape)\nprint(df_covid_short[(df_covid_short['publish_year'] == 2020)].shape)\ndf_covid_short.head()","62bb4f4c":"show_wordcloud(df_covid_short['body_text'])","f2ca75a3":"df_covid = df_covid[(df_covid['body_text'].apply(lambda x: len(x))>2200)]\ndf_covid.reset_index(drop=True, inplace = True)","cc5edbf6":"plt.hist(bodylengths,range=[100000, 200000]) \nplt.title('Word Count of Papers')\nplt.xlabel('Word Count')\nplt.ylabel('Number of Papers')\nplt.show()","7a4b9267":"df_covid_long = df_covid[(df_covid['body_text'].apply(lambda x: len(x))>160000)]\nprint(df_covid_long.shape)\nprint(df_covid_long[(df_covid_long['publish_year'] == 2020)].shape)\ndf_covid_long.head()","2aaad032":"show_wordcloud(df_covid_long['body_text'])","edf5d013":"import gensim\nfrom gensim import models, similarities\nfrom gensim.models.doc2vec import TaggedDocument \nfrom gensim.models.doc2vec import Doc2Vec\nfrom sklearn.manifold import TSNE","9f51c0c2":"if trainDoc2Vec :\n    # Train on the complete Body Text of the papers. \n    df_covid['body_text_clean'] = df_covid['body_text'].apply(lambda x: gensim.parsing.preprocess_string(x))\n    tagged_data  = [TaggedDocument(doc,[i]) for i, doc in enumerate(list(df_covid['body_text_clean']))]\n    # Using distributed memory\u2019 (PV-DM) algorithm\n    doc2vecmodel = gensim.models.doc2vec.Doc2Vec(dm=1, vector_size=50, min_count=5, epochs=10, seed=42, workers=4)\n    doc2vecmodel.build_vocab(tagged_data)\n    doc2vecmodel.train(tagged_data, total_examples=doc2vecmodel.corpus_count, epochs=doc2vecmodel.epochs)\n    doc2vecmodel.save('covid19doc2vec.model')\n    print('Doc2Vec Model Build')\nelse :\n    doc2vecmodel = gensim.models.Doc2Vec.load(\"covid19doc2vec.model\")\n    print('Doc2Vec Model Loaded') \n","6f346c3b":"#perplexity of 5 and learning rate of 500 gives good results\ntsne5 = TSNE(n_components=2, perplexity=5, learning_rate = 500)\ntsne20 = TSNE(n_components=2, perplexity=20, learning_rate = 500)\ndoc2vec_tsne5 = tsne5.fit_transform(doc2vecmodel.docvecs.vectors_docs)\ndoc2vec_tsne20 = tsne20.fit_transform(doc2vecmodel.docvecs.vectors_docs)\nprint(\"tSNE Cordinates Ready for Doc2Vec vectors\")","19858d9f":"from ipywidgets import interact\nper = [5,20]\n@interact\ndef update_tSNE(perplexity = per):\n    if perplexity == 5 :\n        a = doc2vec_tsne5[:,0]\n        b = doc2vec_tsne5[:,1]\n    else : \n        a = doc2vec_tsne20[:,0]\n        b = doc2vec_tsne20[:,1]\n    \n    fig = go.Figure()\n    fig.add_trace(go.Scatter(x=a, y=b,mode='markers'))\n    fig.show()\n","7a22bc04":"doc2vec_tsne = doc2vec_tsne5 ","b84481df":"#Helper functions for referencing data from document index\ndef get_docid(n):\n    return df_covid['paper_id'][n]\ndef get_title(n):\n    title = df_covid['title'][n]\n    return title\ndef get_text_body(n):\n    body = df_covid['body_text'][n]\n    return body\ndef get_abstract(n):\n    body = df_covid['abstract'][n]\n    return body\ndef get_xy_cordinates(n) :\n    return doc2vec_tsne[n]\ndef get_abstract_formatted(n):\n    body = df_covid['abstract_summary'][n]\n    return body\ndef get_title_formatted(n):\n    title = df_covid['title_summary'][n]\n    return title\n","8001304e":"word2vec_root_path = '\/kaggle\/input\/covid-19-word2vec-model-and-vectors\/'\nword2vec_filename = 'covid19word2vec.model'\nword2vecfile =  word2vec_root_path + word2vec_filename\nw2vecmodel = gensim.models.Word2Vec.load(word2vecfile)","20fae218":"print(w2vecmodel.wv.most_similar('covid', topn=10))","6241484f":"print(w2vecmodel.wv.most_similar('death', topn=10))","eaa7c641":"print(w2vecmodel.wv.most_similar('hypertension', topn=10))","d0793b3f":"Outcome = [\"Death\", \"ICU Admission\", 'Mechanical Ventilation', \"Organ Failure\", \"Sepsis\", \"Discharge\"]\n      \n\ndeath_synonyms = ['demise',\n                  'fatal',\n                 'mortal',\n                 'critically ill']   \n\nicu_admissions = ['icu admission','intensive care','care unit', 'requiring icu', 'icu length']\n           \nventilator_synonyms = ['mechanical ventilation',\n                       'respiratory failure',\n                       'intubation',\n                       'oxygen therapy',\n                       'endotracheal intubation'\n                       'respiratory distress',\n                       'arterial oxygen',\n                       'ventilatory support',\n                       'ventilatory'\n                      ]\n\norgan_failure_synonyms = ['organ failure',\n                          'organ dysfunction',\n                          'renal failure',\n                          'multiple organ',\n                          'multiorgan failure',\n                          'kidney injury',\n                          'renal replacement',\n                          'renal dysfunction']\n\nsepsis_synonyms = ['sepsis','septic shock', 'refractory septic']\n\ndischarge_synonym = ['full recovery','discharge', 'recovery', 'recover within','recovered']","893152df":"Diseases = [\"Coronaviruses\",\"ARDS\",\"SARS\", \"MERS\",\"Covid-19\"]\nComorbidities = [\"Diabetes\",\"Hypertension\",\"Immunodeficiency\", \"Cancer\", \"Respiratory\", \"Immunity\"]\nOtherRisks = [\"Age\", \"Gender\", \"Body Weight\", \"Smoking\", \"Climate\", \"Transmission\"]","b81d5073":"\ncovid19_synonyms = ['covid',\n                    'coronavirus disease 19',\n                    'sars cov 2', # Note that search function replaces '-' with ' '\n                    '2019 ncov',\n                    '2019ncov',\n                    r'2019 n cov\\b',\n                    r'2019n cov\\b',\n                    'ncov 2019',\n                    r'\\bn cov 2019',\n                    'coronavirus 2019',\n                    '2019 novel coronavirus',\n                    'wuhan pneumonia',\n                    'wuhan virus',\n                    'wuhan coronavirus',\n                    r'coronavirus 2\\b']\n\nsars_synonyms = [r'\\bsars\\b',\n                 'severe acute respiratory syndrome']\n\nmers_synonyms = [r'\\bmers\\b',\n                 'middle east respiratory syndrome']\n\ncorona_synonyms = ['corona', r'\\bcov\\b']\n\nards_synonyms = ['acute respiratory distress syndrome',\n                 r'\\bards\\b']\n\ndiabetes_synonyms = [\n    'diabet', # picks up diabetes, diabetic, etc.\n    'insulin', # any paper mentioning insulin likely to be relevant\n    'blood sugar',\n    'blood glucose',\n    'ketoacidosis',\n    'hyperglycemi', # picks up hyperglycemia and hyperglycemic\n]\n\nhypertension_synonyms = [\n    'hypertension',\n    'blood pressure',\n    r'\\bhbp\\b', # HBP = high blood pressure\n    r'\\bhtn\\b' # HTN = hypertension\n]\nimmunodeficiency_synonyms = [\n    'immunodeficiency',\n    r'\\bhiv\\b',\n    r'\\baids\\b'\n    'granulocyte deficiency',\n    'hypogammaglobulinemia',\n    'asplenia',\n    'dysfunction of the spleen',\n    'spleen dysfunction',\n    'complement deficiency',\n    'neutropenia',\n    'neutropaenia', # alternate spelling\n    'cell deficiency' # e.g. T cell deficiency, B cell deficiency\n]\n\ncancer_synonyms = [\n    'cancer',\n    'malignant tumour',\n    'malignant tumor',\n    'melanoma',\n    'leukemia',\n    'leukaemia',\n    'chemotherapy',\n    'radiotherapy',\n    'radiation therapy',\n    'lymphoma',\n    'sarcoma',\n    'carcinoma',\n    'blastoma',\n    'oncolog'\n]\n\nchronicresp_synonyms = [\n    'chronic respiratory disease',\n    'asthma',\n    'chronic obstructive pulmonary disease',\n    r'\\bcopd',\n    'chronic bronchitis',\n    'emphysema'\n]\n\nimmunity_synonyms = [\n    'immunity',\n    r'\\bvaccin',\n    'innoculat'\n]\n\nage_synonyms = ['median age',\n                'mean age',\n                'average age',\n                'elderly',\n                r'\\baged\\b',\n                r'\\bold',\n                'young',\n                'teenager',\n                'adult',\n                'child'\n               ]\n\nsex_synonyms = ['sex',\n                'gender',\n                r'\\bmale\\b',\n                r'\\bfemale\\b',\n                r'\\bmales\\b',\n                r'\\bfemales\\b',\n                r'\\bmen\\b',\n                r'\\bwomen\\b'\n               ]\n\nbodyweight_synonyms = [\n    'overweight',\n    'over weight',\n    'obese',\n    'obesity',\n    'bodyweight',\n    'body weight',\n    r'\\bbmi\\b',\n    'body mass',\n    'body fat',\n    'bodyfat',\n    'kilograms',\n    r'\\bkg\\b', # e.g. 70 kg\n    r'\\dkg\\b'  # e.g. 70kg\n]\n\nsmoking_synonyms = ['smoking',\n                    'smoke',\n                    'cigar', # this picks up cigar, cigarette, e-cigarette, etc.\n                    'nicotine',\n                    'cannabis',\n                    'marijuana'\n]\n\n\nclimate_synonyms = [\n    'climate',\n    'weather',\n    'humid',\n    'sunlight',\n    'air temperature',\n    'meteorolog', # picks up meteorology, meteorological, meteorologist\n    'climatolog', # as above\n    'dry environment',\n    'damp environment',\n    'moist environment',\n    'wet environment',\n    'hot environment',\n    'cold environment',\n    'cool environment',\n    'latitiude',\n    'tropical'\n]\n\ntransmission_synonyms = [\n    'transmiss', # Picks up 'transmission' and 'transmissibility'\n    'transmitted',\n    'incubation',\n    'environmental stability',\n    'airborne',\n    'via contact',\n    'human to human',\n    'through droplets',\n    'through secretions',\n    r'\\broute',\n    'exportation'\n]\n","a7b469e2":"# Help Function to count number of occurences of a pattern in text, Needed for Keyword based analysis\ndef count_number_of_occurences(pattern,text) :\n  return re.subn(pattern, '', text)[1]\n\ndef check_thematic_diseases(disease, n):\n    if disease==\"Covid-19\" :\n        keywords=covid19_synonyms\n    elif disease==\"SARS\" :\n        keywords=sars_synonyms\n    elif disease==\"MERS\" :    \n        keywords=mers_synonyms\n    elif disease==\"Coronaviruses\" : \n        keywords=corona_synonyms\n    elif disease==\"ARDS\" :\n        keywords=ards_synonyms \n    \n    for dis in keywords:\n        #Exact Keyword Check, may need to put a threshold of number of occurences\n        status = (pd.Series(get_title(n).lower()).str.contains(dis , na=False) |\n                     pd.Series(get_text_body(n).lower()).str.contains(dis, na=False))\n        #Fuzzy Check TODO\n        #Word2Vec Check TODO\n        if status.bool() :\n            break\n    return status.bool()\n\ndef check_thematic_comorbidities(com, n):\n    if com==\"Diabetes\" :\n        keywords=diabetes_synonyms\n    elif com==\"Hypertension\" :\n        keywords=hypertension_synonyms\n    elif com==\"Immunodeficiency\" :    \n        keywords=immunodeficiency_synonyms\n    elif com==\"Cancer\" : \n        keywords=cancer_synonyms\n    elif com==\"Respiratory\" :\n        keywords=chronicresp_synonyms \n    elif com==\"Immunity\" :\n        keywords=immunity_synonyms    \n    \n    for como in keywords:\n        #Exact Keyword Check, may need to put a threshold of number of occurences is results in a larger size\n        status = (pd.Series(get_title(n).lower()).str.contains(como , na=False) |\n                     pd.Series(get_text_body(n).lower()).str.contains(como, na=False))\n        if status.bool() :\n            break\n    return status.bool()\n\ndef check_otherrisk_factors(com, n):\n    if com==\"Age\" :\n        keywords=age_synonyms\n    elif com==\"Gender\" :\n        keywords=sex_synonyms\n    elif com==\"Body Weight\" :    \n        keywords=bodyweight_synonyms\n    elif com==\"Smoking\" : \n        keywords=smoking_synonyms\n    elif com==\"Climate\" :\n        keywords=climate_synonyms \n    elif com==\"Transmission\" :\n        keywords=transmission_synonyms    \n    \n    for risk in keywords:\n        #Exact Keyword Check, may need to put a threshold of number of occurences is results in a larger size\n        status = (pd.Series(get_title(n).lower()).str.contains(risk , na=False) |\n                     pd.Series(get_text_body(n).lower()).str.contains(risk, na=False))\n        if status.bool() :\n            break\n    return status.bool()\n\ndef check_outcomes(out, n):\n    if out==\"Death\" :\n        keywords=death_synonyms\n    elif out==\"ICU Admission\" :\n        keywords=icu_admissions\n    elif out==\"Mechanical Ventilation\" :    \n        keywords=ventilator_synonyms\n    elif out==\"Organ Failure\" : \n        keywords=organ_failure_synonyms\n    elif out==\"Sepsis\" :\n        keywords=sepsis_synonyms\n    elif out==\"Discharge\" :\n        keywords=discharge_synonym   \n    \n    for risk in keywords:\n        #Exact Keyword Check, may need to put a threshold of number of occurences is results in a larger size\n        status = (pd.Series(get_title(n).lower()).str.contains(risk , na=False) |\n                     pd.Series(get_text_body(n).lower()).str.contains(risk, na=False))\n        if status.bool() :\n            break\n    return status.bool()\n\nprint(\"Sub Keywords and Help Functions defined for Outcomes, Diseases , Comorbidities & Other Risk Factors \")","a99f71a7":"# To be extended as per top impacted areas in each geoghraphy\ncontinental_regions = {\n    'asia': 'asia|china|korea|japan|hubei|wuhan|malaysia|singapore|hong kong',\n    'east_asia': 'east asia|china|korea|japan|hubei|wuhan|hong kong',\n    'south_asia': 'south asia|india|pakistan|bangladesh|sri lanka',\n    'se_asia': r'south east asia|\\bse asia|malaysia|thailand|indonesia|vietnam|cambodia|viet nam',\n    'europe': 'europe|italy|france|spain|germany|austria|switzerland|united kingdom|ireland',\n    'africa': 'africa|kenya',\n    'middle_east': 'middle east|gulf states|saudi arabia|\\buae\\b|iran|persian',\n    'south_america': 'south america|latin america|brazil|argentina',\n    'north_america': 'north america|usa|united states|canada|caribbean',\n    'australasia': 'australia|new zealand|oceania|australasia|south pacific'\n}\n\n# Tag the Primary Geography for the study, based on number of occurences\ndef tag_primary_study_geography(text):\n    score = []\n    for cr, s in continental_regions.items():\n        count=0\n        splits = s.split('|')\n        for reg in splits:\n            count+= count_number_of_occurences(reg,text)\n        score.append(count)\n    if ((len(set(score)) == 1) & (score[0]==0)):\n        tag = \"unknown\"\n    else :\n        tag = list(continental_regions.keys())[score.index(max(score))]\n    return tag\n\ndf_covid['region'] = df_covid['body_text'].apply(lambda x: tag_primary_study_geography(x.lower())) \n\nprint(\"Geographical Tagging completed on the Dataframe \")","d57450ae":"df_covid['abstract'].replace(np.nan, 'Not Available', regex=True, inplace=True)\nresearch_method = {\n    'Systematic Review':'cohen\\'s d|cohen\\'s kappa|cochrane review|database search|databases searched|difference between means|d-pooled|difference in means|electronic search|heterogeneity|pooled relative risk|meta-analysis|pooled adjusted odds ratio|pooled aor|pooled odds ratio|pooled or|pooled risk ratio|pooled rr|prisma|search criteria|search strategy|search string|systematic review',\n    'Randomized':'blind|consort|control arm|double-blind|placebo|randomisation|randomised|randomization method|randomized|randomized clinical trial|randomized controlled trial|rct|treatment arm|treatment effect',\n    'Non-Randomized':'allocation method|blind|control arm|double-blind|non-randomised|non-randomized|non randomized|placebo|pseudo-randomised|pseudo-randomized|quasi-randomised|quasi-randomized|treatment arm|treatment effect',\n    'Ecological Regression':'correlation|correlations|per capita|r-squared|adjusted hazard ratio|censoring|confounding|covariates|cox proportional hazards|demographics|enroll|enrolled|enrollment|eligibility criteria|etiology|gamma|hazard ratio|kaplan-meier|lognormal|longitudinal|median time to event|non-comparative study|potential confounders|recruit|recruited|recruitment|right-censored|survival analysis|time-to-event analysis|time series|time-series|time varying|time-varying|truncated|weibull',\n    'Prospective Cohort': 'baseline|prospective|prospectively|prospective cohort|relative risk|risk ratio|rr|chart review|ehr|health records|medical records|etiology|exposure status|risk factor analysis|risk factors|cohort|followed|loss to follow-up|patients|subjects|adjusted odds ratio|aor|log odds|logistic regression|odds ratio',\n    'Time Series Analysis': 'adjusted hazard ratio|censoring|confounding|covariates|cox proportional hazards|demographics|enroll|enrolled|enrollment|eligibility criteria|etiology|gamma|hazard ratio|kaplan-meier|lognormal|longitudinal|median time to event|non-comparative study|potential confounders|recruit|recruited|recruitment|right-censored|survival analysis|time-to-event analysis|time series|time-series|time varying|time-varying|truncated|weibull',\n    'Retrospective Cohort' : 'cohen\\'s kappa|data abstraction forms|data collection instrument|eligibility criteria|inter-rater reliability|potential confounders|retrospective|retrospective chart review|retrospective cohort|chart review|ehr|health records|medical records|etiology|exposure status|risk factor analysis|risk factors|cohort|followed|loss to follow-up|patients|subjects|adjusted odds ratio|aor|log odds|logistic regression|odds ratio',\n    'Cross Sectional' :'cross sectional|cross-sectional|prevalence survey|case-control|data collection instrument|eligibility criteria|matching case|matched case|matching criteria|matched criteria|number of controls per case|non-response bias|potential confounders|psychometric evaluation of instrument|questionnaire development|response rate|survey instrument|chart review|ehr|health records|medical records|etiology|exposure status|risk factor analysis|risk factors|adjusted odds ratio|aor|log odds|logistic regression|odds ratio',\n    'Case Control':'case-control|data collection instrument|eligibility criteria|matching case|matched case|matching criteria|matched criteria|number of controls per case|non-response bias|potential confounders|psychometric evaluation of instrument|questionnaire development|response rate|survey instrument|chart review|ehr|health records|medical records|etiology|exposure status|risk factor analysis|risk factors|adjusted odds ratio|aor|log odds|logistic regression|odds ratio',\n    'Case Study': 'case report|case series|etiology|frequency|risk factors',\n    'Simulation': 'bootstrap|computer model|computer modelling|forecast|forcasting|mathematical model|mathematical modelling|model simulation|monte carlo|simulate|simulation|simulated|synthetic data|synthetic dataset|cohort|followed|loss to follow-up|patients|subjects'\n}\n\n# Tag the Primary Research Method for the study, based on number of occurences\ndef tag_primary_research_method(text):\n    score = []\n    for cr, s in research_method.items():\n        count=0\n        splits = s.split('|')\n        for reg in splits:\n            count+= count_number_of_occurences(reg,text)\n        score.append(count)\n    if ((len(set(score)) == 1) & (score[0]==0)):\n        tag = \"unknown\"\n    else :\n        tag = list(research_method.keys())[score.index(max(score))]\n    return tag\n\ndf_covid['researchdesign']=\"\"\n\ndf_covid['researchdesign'] = df_covid['title'].apply(lambda x: tag_primary_research_method(x))\ndf_covid['researchdesign'] = df_covid[df_covid['researchdesign']=='unknown']['abstract'].apply(lambda x: tag_primary_research_method(x))\n\ndf_covid['researchdesign'].replace(np.nan, 'unknown', regex=True, inplace=True)\n\nprint(\"Research Study Tags completed for all papers in the Dataframe\")","db3ec7cc":"import random\ndef evidencegapmap(dataset, x_column, y_column, xy_column=None, bubble_column=None, bubble_text=None, bubble_link=None, time_column=None, size_column=None, color_column=None,   \n               xbin_list=None, ybin_list=None,xbin_size=100, ybin_size=100, x_title=None, y_title=None, title=None, colorbar_title=None,\n               scale_bubble=10, colorscale=None, marker_opacity=None, marker_border_width=None,show_slider=True, show_button=True, show_colorbar=True, show_legend=None, \n               width=None, height=None):\n    ''' Makes the animated and interactive bubble charts from a given dataset.'''\n    \n    # Initialize the number of bins \n    xbin_range = [0,(len(xbin_list)-1)]\n    ybin_range = [0,(len(ybin_list)-1)]\n    #Initialize Axes range                                  \n    x_range=[0,0] \n    y_range=[0,0]\n    # Set category_column as None and update it as color_column only in case\n    # color_column is not None and categorical, in which case set color_column as None\n    category_column = None\n    if color_column: # Can be numerical or categorical\n        if dataset[color_column].dtype.name in ['category', 'object', 'bool']:\n            category_column = color_column\n            color_column = None\n    # Set the plotting mode for the plots inside a cell\n    if xy_column :\n        mode = 'nlpmode'\n        xmax = max(map(lambda xy: xy[0], list(dataset[xy_column])))\n        xmin = min(map(lambda xy: xy[0], list(dataset[xy_column])))\n        ymax = max(map(lambda xy: xy[1], list(dataset[xy_column])))\n        ymin = min(map(lambda xy: xy[1], list(dataset[xy_column])))\n        xshift = (xmax + xmin)\/2\n        yshift = (ymax + ymin)\/2\n        xy_scale= max(xmax-xmin, ymax-ymin)\n        #print(\"xmax {}, xmin {}, ymax {}, ymin {}, xshift {}, yshift {} xy_scale {}\".format(xmax, xmin, ymax, ymin, xshift, yshift, xy_scale))\n    else :\n        mode = 'randommode'\n        xy_scale = 1\n        xshift=yshift =0\n    \n    # Set the variables for making the grid\n    if time_column:\n        years = dataset[time_column].unique()\n    else:\n        years = None\n        show_slider = False\n        show_button = False\n        \n    column_names = [x_column, y_column]\n    \n    column_names.append(bubble_column)\n    if xy_column:\n        column_names.append(xy_column)\n    if bubble_text:\n        column_names.append(bubble_text)\n    if bubble_link:\n        column_names.append(bubble_link)\n    \n    if size_column:\n        column_names.append(size_column)\n    \n    if color_column:\n        column_names.append(color_column)\n        \n        \n    # Make the grid\n    if category_column:\n        categories = dataset[category_column].unique()\n        col_name_template = '{}+{}+{}_grid'\n        grid = make_grid_with_categories(dataset, column_names, time_column, category_column, years, categories)\n        if show_legend is None:\n            showlegend = True\n        else: \n            showlegend = show_legend\n\n        \n    # Set the layout\n    if show_slider:\n        slider_scale = years\n    else:\n        slider_scale = None\n                \n    figure, sliders_dict = set_layout(x_title, y_title, title, show_slider, slider_scale, show_button, showlegend, width, height)\n    \n    if size_column:\n        sizeref = 2.*max(dataset[size_column])\/(scale_bubble**2) # Set the reference size for the bubbles\n    else:\n        sizeref = None\n\n    # Add the frames\n    if category_column:\n        # Add the base frame\n        for category in categories:\n            if time_column:\n                year = min(years) # The earliest year for the base frame\n                col_name_template_year = col_name_template.format(year, {}, {})\n            else:\n                col_name_template_year = '{}+{}_grid'\n            trace = get_trace(grid, col_name_template_year, x_column, y_column, xy_column, \n                              bubble_column,bubble_text, bubble_link, size_column, \n                              sizeref, scale_bubble, marker_opacity, marker_border_width, mode=mode,category=category, xsize=xbin_size, ysize=ybin_size,\n                              xy_scale=xy_scale, xshift=xshift, yshift=yshift)\n            figure['data'].append(trace)\n           \n        # Add time frames\n        if time_column: # Only if time_column is not None\n            for year in years:\n                frame = {'data': [], 'name': str(year)}\n                for category in categories:\n                    col_name_template_year = col_name_template.format(year, {}, {})\n                    trace = get_trace(grid, col_name_template_year, x_column, y_column, xy_column, \n                                      bubble_column, bubble_text, bubble_link, size_column, \n                                      sizeref, scale_bubble, marker_opacity, marker_border_width ,mode=mode, category=category, xsize=xbin_size, ysize=ybin_size,\n                                      xy_scale=xy_scale, xshift=xshift, yshift=yshift)\n                    \n                    frame['data'].append(trace)\n\n                    figure['frames'].append(frame) \n\n                if show_slider:\n                    add_slider_steps(sliders_dict, year)\n                \n    else:\n        # Add the base frame\n        if time_column:\n            year = min(years) # The earliest year for the base frame\n            col_name_template_year = col_name_template.format(year, {})\n        else:\n            col_name_template_year = '{}_grid'\n        trace = get_trace(grid, col_name_template_year, x_column, y_column, xy_column, \n                          bubble_column, bubble_text, bubble_link, size_column, \n                          sizeref, scale_bubble, marker_opacity, marker_border_width,\n                          color_column, colorscale, show_colorbar, colorbar_title, mode=mode, xsize=xbin_size, ysize=ybin_size,\n                          xy_scale=xy_scale, xshift=xshift, yshift=yshift)\n       \n        figure['data'].append(trace)\n        \n        # Add time frames\n        if time_column: # Only if time_column is not None\n            for year in years:\n                col_name_template_year = col_name_template.format(year, {})\n                frame = {'data': [], 'name': str(year)}\n                trace = get_trace(grid, col_name_template_year, x_column, y_column, xy_column,\n                                  bubble_column, bubble_text, bubble_link,size_column, \n                                  sizeref, scale_bubble, marker_opacity, marker_border_width,\n                                  color_column, colorscale, show_colorbar, colorbar_title, mode=mode, xsize=xbin_size, ysize=ybin_size, \n                                  xy_scale=xy_scale, xshift=xshift, yshift=yshift)\n\n                frame['data'].append(trace)\n                figure['frames'].append(frame) \n                if show_slider:\n                    add_slider_steps(sliders_dict, year) \n    # Set ranges for the axes\n   \n    x_range = set_range(dataset[x_column], xbin_size)\n    y_range = set_range(dataset[y_column], ybin_size)\n    \n    figure['layout']['xaxis']['range'] = x_range\n    figure['layout']['yaxis']['range'] = y_range\n        \n    if show_slider:\n        figure['layout']['sliders'] = [sliders_dict]\n    \n    tracepoint = draw_evidence_gap_map_structure_horzero(xbin_list,ybin_list,xbin_size,ybin_size )\n    figure['data'].append(tracepoint)\n    for i in range(len(ybin_list)+1): \n        tracepoint = draw_evidence_gap_map_structure_hor(i, xbin_list,ybin_list,xbin_size,ybin_size )\n        figure['data'].append(tracepoint)\n    tracepoint = draw_evidence_gap_map_structure_verzero(xbin_list,ybin_list,xbin_size,ybin_size )\n    figure['data'].append(tracepoint)\n    for i in range(len(xbin_list)+1): \n        tracepoint = draw_evidence_gap_map_structure_ver(i, xbin_list,ybin_list,xbin_size,ybin_size )\n        figure['data'].append(tracepoint)\n    return figure\n\ndef draw_evidence_gap_map_structure_horzero(x_list=None, y_list=None,xbin=100, ybin=100):\n    number_of_xcats = len(x_list)\n    number_of_ycats = len(y_list)\n    draw_horizontals_zero= {\n        'x': [int((xbin\/2)+i*(xbin)) for i in range(number_of_xcats)],\n        'y': [0 for i in range(number_of_xcats)],\n        'text': [x_list[line] for line in range(number_of_xcats)],\n        'mode': 'lines+text',\n        'textposition': 'bottom center',\n        'showlegend': False\n    }\n    return draw_horizontals_zero\ndef draw_evidence_gap_map_structure_hor(linenum=1, x_list=None, y_list=None,xbin=100, ybin=100):\n    number_of_xcats = len(x_list)\n    number_of_ycats = len(y_list)\n    draw_horizontals = {\n        'x': [int(i*xbin) for i in range(number_of_xcats+1)],\n        'y': [int(linenum*(ybin)) for i in range(number_of_xcats+1)],\n        'text': \"\",\n        'mode': 'lines',\n        'showlegend': False\n    }\n    return draw_horizontals\ndef draw_evidence_gap_map_structure_verzero(x_list=None, y_list=None,xbin=100, ybin=100):\n    number_of_xcats = len(x_list)\n    number_of_ycats = len(y_list)\n    draw_verticals_zero= {\n        'x': [0 for i in range(number_of_ycats)],\n        'y': [int((ybin\/2)+i*(ybin)) for i in range(number_of_ycats)],\n        'text': [y_list[line] for line in range(number_of_ycats)],\n        'mode': 'lines+text',\n        'textposition': 'middle left',\n        'showlegend': False\n    }\n    return draw_verticals_zero\ndef draw_evidence_gap_map_structure_ver(linenum=1, x_list=None, y_list=None,xbin=100, ybin=100):\n    number_of_xcats = len(x_list)\n    number_of_ycats = len(y_list)\n    draw_verticals = {\n        'x': [int(linenum*(xbin)) for i in range(number_of_ycats+1)],\n        'y': [int(i*ybin) for i in range(number_of_ycats+1)],\n        'text': \"\",\n        'mode': 'lines',\n        'showlegend': False\n    }\n    return draw_verticals\n    \ndef make_grid_with_categories(dataset, column_names, time_column, category_column, years=None, categories=None):\n    '''Makes the grid for the plot as a pandas DataFrame.'''\n    \n    grid = pd.DataFrame()\n    if categories is None:\n        categories = dataset[category_column].unique()\n    if time_column:\n        col_name_template = '{}+{}+{}_grid'\n        if years is None:\n            years = dataset[time_column].unique()\n            \n        for year in years:\n            for category in categories:\n                dataset_by_year_and_cat = dataset[(dataset[time_column] == int(year)) & (dataset[category_column] == category)]\n                for col_name in column_names:\n                    # Each column name is unique\n                    temp = col_name_template.format(year, col_name, category)\n                    if dataset_by_year_and_cat[col_name].size != 0:\n                        grid = grid.append({'value': list(dataset_by_year_and_cat[col_name]), 'key': temp}, ignore_index=True) \n    else:\n        col_name_template = '{}+{}_grid'\n        for category in categories:\n            dataset_by_cat = dataset[(dataset[category_column] == category)]\n            for col_name in column_names:\n                # Each column name is unique\n                temp = col_name_template.format(col_name, category)\n                if dataset_by_cat[col_name].size != 0:\n                        grid = grid.append({'value': list(dataset_by_cat[col_name]), 'key': temp}, ignore_index=True) \n    return grid\n\n \ndef set_layout(x_title=None, y_title=None, title=None, show_slider=True, slider_scale=None, show_button=True, show_legend=False,\n            width=None, height=None):\n    '''Sets the layout for the figure.'''\n    \n    # Define the figure object as a dictionary\n    figure = {\n        'data': [],\n        'layout': {},\n        'frames': []\n    }\n    \n    # Start with filling the layout first\n    \n    figure = set_2Daxes(figure, x_title, y_title)\n        \n    figure['layout']['title'] = title    \n    figure['layout']['hovermode'] = 'closest'\n    figure['layout']['showlegend'] = show_legend\n    figure['layout']['margin'] = dict(l=60, b=50, t=50, r=60, pad=10)\n    \n    \n    if width:\n        figure['layout']['width'] = width\n    if height:\n        figure['layout']['height'] = height\n    \n    # Add slider for the time scale\n    if show_slider: \n        sliders_dict = add_slider(figure, slider_scale)\n    else:\n        sliders_dict = {}\n    \n    # Add a pause-play button\n    if show_button:\n        add_button(figure)\n        \n    # Return the figure object\n    return figure, sliders_dict\n\ndef set_2Daxes(figure, x_title=None, y_title=None):\n    '''Sets 2D axes'''\n    \n    figure['layout']['xaxis'] = {'title': x_title, 'autorange': False, 'showgrid': False, 'zeroline': False, 'showline': False, 'ticks': '',\n    'showticklabels': False, 'automargin': True}\n    figure['layout']['yaxis'] = {'title': y_title, 'autorange': False, 'showgrid': False, 'zeroline': False, 'showline': False, 'ticks': '',\n    'showticklabels': False, 'automargin': True} \n        \n    return figure\n    \n        \ndef add_slider(figure, slider_scale):\n    '''Adds slider for animation'''\n    \n    figure['layout']['sliders'] = {\n        'args': [\n            'slider.value', {\n                'duration': 400,\n                'ease': 'cubic-in-out'\n            }\n        ],\n        'initialValue': min(slider_scale),\n        'plotlycommand': 'animate',\n        'values': slider_scale,\n        'visible': True\n    }\n    \n    sliders_dict = {\n        'active': 0,\n        'yanchor': 'top',\n        'xanchor': 'left',\n        'currentvalue': {\n            'font': {'size': 20},\n            'prefix': 'Year:',\n            'visible': True,\n            'xanchor': 'right'\n        },\n        'transition': {'duration': 300, 'easing': 'cubic-in-out'},\n        'pad': {'b': 10, 't': 50},\n        'len': 0.9,\n        'x': 0.1,\n        'y': 0,\n        'steps': []\n    }\n    \n    return sliders_dict\n\ndef add_slider_steps(sliders_dict, year):\n    '''Adds the slider steps.'''\n    \n    slider_step = {'args': [\n        [year],\n        {'frame': {'duration': 300, 'redraw': False},\n         'mode': 'immediate',\n       'transition': {'duration': 300}}\n     ],\n     'label': str(year),\n     'method': 'animate'}\n    sliders_dict['steps'].append(slider_step)\n    \ndef add_button(figure):\n    '''Adds the pause-play button for animation'''\n    \n    figure['layout']['updatemenus'] = [\n        {\n            'buttons': [\n                {\n                    'args': [None, {'frame': {'duration': 500, 'redraw': False},\n                             'fromcurrent': True, 'transition': {'duration': 300, 'easing': 'quadratic-in-out'}}],\n                    'label': 'Play',\n                    'method': 'animate'\n                },\n                {\n                    'args': [[None], {'frame': {'duration': 0, 'redraw': False}, 'mode': 'immediate',\n                    'transition': {'duration': 0}}],\n                    'label': 'Pause',\n                    'method': 'animate'\n                }\n            ],\n            'direction': 'left',\n            'pad': {'r': 10, 't': 87},\n            'showactive': False,\n            'type': 'buttons',\n            'x': 0.1,\n            'xanchor': 'right',\n            'y': 0,\n            'yanchor': 'top'\n        }\n    ]\n    \ndef set_range(values, size): \n    ''' Finds the axis range for the figure.'''\n    \n    rmin = int(min([return_xbin_cords(x, size) for x in values]))-size\/2\n    rmax = int(max([return_xbin_cords(x, size) for x in values]))+size\/2\n    \n        \n    return [rmin, rmax] \n\n# To be used later when individual Risk Factos can be plotted\ndef return_xbin_cords(x_binnum, sizebin):\n    # generate some random integers to fit in the research papers in a cell\n    values = random.randint((-sizebin\/2+5),(sizebin\/2-5))\n    #Plots start at (0, 0)\n    xbin_cords = sizebin\/2 + (x_binnum*sizebin) + values\n    return int(xbin_cords)\n\n# To be used later when individual Risk Factos can be plotted\ndef return_ybin_cords(y_binnum, sizebin):\n    # generate some random integers to fit in the research papers in a cell\n    values = random.randint((-sizebin\/2+5),sizebin\/2-5)\n    #Plots start at (0, 0)\n    ybin_cords = sizebin\/2 + (y_binnum*sizebin) + values\n    return int(ybin_cords)\n\n# To be used later when individual Risk Factos can be plotted\ndef return_xy_cords_nlp(a, xy, sizebin, axes, scale, shift):\n    if axes=='x':\n        margin = 10\n        # generate some random integers to fit in the research papers in a cell\n        # remove a margin of 10 from the size of bin so effectively available size is 90 if bin is 100\n        values = ((xy[0]-shift)\/scale)*(sizebin - 10)\n        #Plots start at (0, 0)\n        x_cords = sizebin\/2 + (a*sizebin) + values\n        return int(x_cords)\n    else :\n        # generate some random integers to fit in the research papers in a cell\n        # remove a margin of 10 from the size of bin so effectively available size is 90 if bin is 100\n        values = ((xy[1]-shift)\/scale)*(sizebin - 10)\n        #Plots start at (0, 0)\n        y_cords = sizebin\/2 + (a*sizebin) + values\n        return int(y_cords)\n    \ndef return_text_by_category_in_bin(grid,category,xbinnum,ybinnum,template, xcol, ycol, column, bubbletext, link, size):\n    indicesx=[]\n    indicesy=[]\n    for idx, row in grid[grid['key'].str.contains(category)].iterrows():\n        if row['key']==template.format(xcol, category):\n            for i, xx in enumerate(row['value']):\n                if (xx==xbinnum):\n                    indicesx.append(i)\n        if row['key']==template.format(ycol, category):\n            for i, yy in enumerate(row['value']):\n                if (yy==ybinnum):\n                    indicesy.append(i) \n    matchindex = list(set(indicesx) & set(indicesy))\n    textoverall=[]\n    textcol=[]\n    texttext=[]\n    textlink=[]\n    textrelevance=[]\n    for idx, row in grid[grid['key'].str.contains(category)].iterrows():\n        for i, val in enumerate(matchindex):\n            if row['key']==template.format(column, category):\n                textcol.append('<b>Title:<\/b>'+ str(row['value'][val]))\n            if bubbletext:\n                if row['key']==template.format(bubbletext, category):\n                    texttext.append('<br><b>Summary:<\/b>'+ str(row['value'][val]))\n            if link:\n                if row['key']==template.format(link, category):\n                    textlink.append('<br><b>Link:<\/b>'+ str(row['value'][val]))   \n            if size:\n                if row['key']==template.format(size, category):\n                    textrelevance.append('<br><b>Relevance:<\/b>'+ str(row['value'][val]))\n    for idx, val in enumerate(textcol):\n        # Display top 8 of relevant  and the highlighted \n        if idx==0:\n            textall = \"\"\n        else: \n            textall ='<br>----------------------------------------<br>'\n        textall = textall + textcol[idx]\n        if bubbletext:\n            textall = textall + texttext[idx]\n        if link:\n            textall = textall + textlink[idx] \n        if size:\n            textall = textall + textrelevance[idx]\n        textoverall.append(textall)\n        # Plotly only able to handle only upto 9 datapoints in hovertext\n        # TODO ensure that the closest point being hovered is always included\n        if idx==8 :\n            break\n    return \"\".join(textoverall)    \n\n# The size is used to categorize in High (top 10% percentile), Medium ( to 50% ) and Rest as Low\ndef return_transformed_size(size, comparewith):\n    if size > np.percentile(comparewith, 90):\n        return size*1.25\n    elif size > np.percentile(comparewith, 50):\n        return size\n    else :\n        return size\/1.25\n    \ndef get_trace(grid, col_name_template, x_column, y_column,xy_column, bubble_column, bubble_text, bubble_link,size_column=None, \n            sizeref=1, scale_bubble=10, marker_opacity=None, marker_border_width=None,\n            color_column=None, colorscale=None, show_colorbar=True, colorbar_title=None, mode=None, category=None, xsize=100, ysize=100, \n            xy_scale=1, xshift=0, yshift=0):\n    ''' Makes the trace for the data as a dictionary object that can be added to the figure or time frames.'''\n    try:\n        if mode =='randommode':\n            trace = {\n                    'x': [return_xbin_cords(x, xsize) for x in grid.loc[grid['key']==col_name_template.format(x_column, category), 'value'].values[0]],\n                    'y': [return_ybin_cords(y, ysize) for y in grid.loc[grid['key']==col_name_template.format(y_column, category), 'value'].values[0]],\n                    'text': [i + '<br><b>Summary:<\/b>' + j + '<br><b>Link:<\/b>' + k for i, j, k in zip(grid.loc[grid['key']==col_name_template.format(bubble_column, category), 'value'].values[0], grid.loc[grid['key']==col_name_template.format(bubble_text, category), 'value'].values[0],grid.loc[grid['key']==col_name_template.format(bubble_link, category), 'value'].values[0])],\n                    'hovertemplate': '<b>Title:<\/b>%{text}<extra><\/extra>',\n                    'mode': 'markers'\n            }\n        else:\n            trace = {\n                    'x': [return_xy_cords_nlp(x,xy, xsize, 'x', xy_scale, xshift) for x, xy in zip(grid.loc[grid['key']==col_name_template.format(x_column, category), 'value'].values[0],grid.loc[grid['key']==col_name_template.format(xy_column, category), 'value'].values[0])],\n                    'y': [return_xy_cords_nlp(y,xy, ysize, 'y', xy_scale, yshift) for y, xy in zip(grid.loc[grid['key']==col_name_template.format(y_column, category), 'value'].values[0],grid.loc[grid['key']==col_name_template.format(xy_column, category), 'value'].values[0])],\n                    'text': [return_text_by_category_in_bin(grid,category,x,y,col_name_template,x_column,y_column,bubble_column,bubble_text,bubble_link,size_column) for x, y  in zip(grid.loc[grid['key']==col_name_template.format(x_column,category), 'value'].values[0],grid.loc[grid['key']==col_name_template.format(y_column, category), 'value'].values[0])],\n                    'hovertemplate': '%{text}<extra><\/extra>',\n                    'mode': 'markers'\n            }\n        if size_column:\n                trace['marker'] = {\n                    'sizemode': 'diameter',\n                    'sizeref': sizeref,\n                    'size': [return_transformed_size(size, grid.loc[grid['key']==col_name_template.format(size_column, category), 'value'].values[0]) \n                             for size in grid.loc[grid['key']==col_name_template.format(size_column, category), 'value'].values[0]],\n                }\n        else:\n                trace['marker'] = {\n                    'size': 10*scale_bubble,\n                }\n\n        if marker_opacity:\n                trace['marker']['opacity'] = marker_opacity\n\n        if marker_border_width:\n                trace['marker']['line'] = {'width': marker_border_width}\n\n        if color_column:\n                    trace['marker']['color'] = grid.loc[grid['key']==col_name_template.format(color_column), 'value'].values[0]\n                    trace['marker']['colorbar'] = {'title': colorbar_title}\n                    trace['marker']['colorscale'] = colorscale\n\n        if category:\n                trace['name'] = category\n    except:\n        trace = {\n            'x': [],\n            'y': [],\n            }\n\n    return trace\n","0bb81b1a":"task2 = ['Data on potential risks factors for COVID 19, Wuhan Coronaviruses, sars cov 2, ncov 2019, coronavirus 2019, wuhan pneumoni ',\n'Smoking, pre-existing pulmonary disease',\n'Co-infections (determine whether co-existing respiratory, viral infections make the virus more transmissible or virulent) and other co-morbidities like hypertension and diabetes',\n'Neonates and pregnant women',\n'Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences.',\n'Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors', \n'Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups',\n'Susceptibility of populations',\n'Public health mitigation measures that could be effective for control', \n'Studies that cover risk factor analysis,cross sectional case control,prospective case control,matched case control, medical records review, seroprevalence survey and syndromic surveillance',\n 'health status (diabetes, hypertension, heart disease, pregnancy, neonates, cancer, smoking status, history of lung disease, local climate, elderly, small children, immune compromised groups, age deciles among adults between the ages of 15 and 65, race\/ethnicity, insurance status, housing status).',\n 'latitude, temperature, humidity. Covariates include: social distancing policies, population density, demographics (e.g., socioeconomic status, access to health services), access to testing.',\n 'symptoms (cough, fever, sputum production, diarrhea, shortness of breath, sleep disruption, fatigue, etc.) and lab results (COVID-19 by PCR, chest CT scan, leucocyte counts, neutrophils, lymphocytes, hemoglobin, platelets, liver function abnormality, alanine aminotransferase (ALT) , aspartate aminotransferase (AST), lactate dehydrogenase, renal function damage, blood urea nitrogen, serum creatinine, procalcitonin (PCT), IL-6, C-reactive protein (CRP)',\n 'social distancing directives, postponing nonessential medical services',\n 'mandatory quarantine of exposed health workers',\n 'COPD, COPD severity,GOLD score, FEV1\/FVC, FEV1 % of normal']","bf0f4a07":"task2processed = gensim.parsing.preprocess_string(' '.join(task2))\ntask2vector = doc2vecmodel.infer_vector(task2processed)\n# Print the Doc2Vec vector for the Research Objectives query\nprint(task2vector)","59207911":"shortlistdocs = 300","0ee62010":"# Return the top matching documents for the risk task\nsimilar_docs = doc2vecmodel.docvecs.most_similar([task2vector], topn=shortlistdocs)\n# Find similar doc and convert back to doc_id from index and print top 10 relevant research\ncount=0\nfor i, score in similar_docs:\n    print(\"Title: {0} \\n Paper Id: {1} \\n Score: {2} \\n\".format(get_title(i),get_docid(i),score))\n    count+=1\n    if count==5:\n        break","d2b193bf":"#Build the dataframe for the plot\nplot_data_dis = pd.DataFrame(columns=['doc_id','title','publish_year','doi','abstract','region','x','y','xy_column','relevance', 'researchdesign'])\n# Load from CSV does not work in NLP mode and hence regenerate dataframe\n#if generatefiles:\nnewi=0\nfor i, scoreofdoc in similar_docs:\n    for dis in Diseases:\n        for out in Outcome:\n            if (check_outcomes(out, i) & check_thematic_diseases(dis, i)) :\n                plot_data_dis.loc[newi] = \"\"\n                plot_data_dis['doc_id'][newi] = df_covid['paper_id'][i]\n                plot_data_dis['title'][newi] = df_covid['title_summary'][i]\n                plot_data_dis['publish_year'][newi] = int(df_covid['publish_year'][i])\n                plot_data_dis['doi'][newi] = df_covid['doi'][i]\n                plot_data_dis['abstract'][newi] = df_covid['abstract_summary'][i]\n                plot_data_dis['region'][newi] = df_covid['region'][i]\n                plot_data_dis['x'][newi] = Outcome.index(out)\n                plot_data_dis['y'][newi] = Diseases.index(dis)\n                plot_data_dis['xy_column'][newi] = get_xy_cordinates(i)\n                plot_data_dis['relevance'][newi]= int(scoreofdoc*100)\n                plot_data_dis['researchdesign'][newi]=df_covid['researchdesign'][i]\n                newi+=1\n# Make format changes to data column for display \nconvert_dict = {'relevance': int,'publish_year': int}\nplot_data_dis = plot_data_dis.astype(convert_dict)\n#plot_data_dis.to_csv('disease-outcome.csv',index=False)","e64e466c":"from __future__ import division\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode()\n# Time based plot for Disease vs Outcome\nplot_data_dis.sort_values('publish_year', inplace=True)\nplot_data_dis.reset_index(drop=True, inplace=True)\n\nfigure = evidencegapmap(dataset=plot_data_dis, x_column='x', y_column='y',xy_column='xy_column',\n  bubble_column='title', bubble_link='doi', time_column='publish_year', size_column='relevance', color_column='region',xbin_list=Outcome, ybin_list = Diseases,\n  xbin_size=100, ybin_size = 100, x_title=\"Outcome\", y_title=\"Disease\", title='Disease vs Outcome (Year wise, Mode: NLP))',scale_bubble=5, marker_opacity=0.5,height=600, width=900)\niplot(figure)","5531069b":"#Clean Up as plotting is RAM consuming\ndel plot_data_dis\ngc.collect()","a02357f2":"shortlistdocs = 600","c8486c79":"similar_docs_otherrisks = doc2vecmodel.docvecs.most_similar([task2vector], topn=shortlistdocs)\n# Plot the evidence gap map for Outcomes vs OtherRisk Factors for COVID 19\nplot_data_otherrisks = pd.DataFrame(columns=['doc_id','title','publish_year','publish_date','journal','doi','abstract','region','x','y','xy_column','relevance', 'researchdesign', 'otherrisks', 'outcome'])\nnewi=0\nfor i, scoreofdoc in similar_docs_otherrisks:\n    for com in OtherRisks:\n        for out in Outcome:\n            if (check_otherrisk_factors(com, i) & check_outcomes(out, i)):\n                plot_data_otherrisks.loc[newi] = \"\"\n                plot_data_otherrisks['doc_id'][newi] = df_covid['paper_id'][i]\n                plot_data_otherrisks['publish_date'][newi] = df_covid['publish_date'][i]\n                #plot_data_otherrisks['authors'][newi] = df_covid['authors'][i]\n                plot_data_otherrisks['journal'][newi] = df_covid['journal'][i]\n                plot_data_otherrisks['title'][newi] = df_covid['title_summary'][i]\n                plot_data_otherrisks['publish_year'][newi] = int(df_covid['publish_year'][i])\n                plot_data_otherrisks['doi'][newi] = df_covid['doi'][i]\n                plot_data_otherrisks['abstract'][newi] = df_covid['abstract_summary'][i]\n                plot_data_otherrisks['region'][newi] = df_covid['region'][i]\n                plot_data_otherrisks['x'][newi] = OtherRisks.index(com)\n                plot_data_otherrisks['y'][newi] = Outcome.index(out)\n                plot_data_otherrisks['xy_column'][newi] = get_xy_cordinates(i)\n                plot_data_otherrisks['relevance'][newi]= int(scoreofdoc*100)\n                plot_data_otherrisks['researchdesign'][newi]=df_covid['researchdesign'][i]\n                plot_data_otherrisks['otherrisks'][newi] = com\n                plot_data_otherrisks['outcome'][newi]= out\n                newi+=1\n# Make format changes to data column for display \nconvert_dict = {'relevance': int,'publish_year': int}\nplot_data_otherrisks = plot_data_otherrisks.astype(convert_dict)\n#plot_data_otherrisks.to_csv('outcome-otherrisks.csv',index=False)   ","afc0902c":"from plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode()\nfigure = evidencegapmap(dataset=plot_data_otherrisks, x_column='x', y_column='y',\n  bubble_column='title',bubble_text='abstract', bubble_link='doi', size_column='relevance', color_column='researchdesign',xbin_list=OtherRisks, ybin_list = Outcome,\n  xbin_size=100, ybin_size = 100, x_title=\"Other Risks\", y_title=\"Outcomes\", title='Outcome vs Other Risk Factors (Mode:Random)',scale_bubble=4, marker_opacity=0.6,height=800)\niplot(figure)","9a1ca16d":"resultsother = pd.DataFrame(columns=['Date','Title','URL','Journal','Severe','Severe Significant','Severe Adjusted','Severe Calculated','Fatality','Fatality Significant','Fatality Adjusted','Fatality Calculated','Multivariate adjustment', 'Design', 'Study population','Risk'])\n\n#Filter out primarily the papers from 2020\nplot_data_otherrisks = plot_data_otherrisks.loc[plot_data_otherrisks['publish_year'].astype({'publish_year': int}) == 2020]\n\nfor index, row in plot_data_otherrisks.iterrows(): \n    resultsother.loc[index] = \"\"\n    resultsother['Date'][index]=plot_data_otherrisks['publish_date'][index]\n    resultsother['Title'][index]=plot_data_otherrisks['title'][index]\n    resultsother['URL'][index]=plot_data_otherrisks['doi'][index]\n    resultsother['Journal'][index]=plot_data_otherrisks['journal'][index]\n    resultsother['Design'][index]=plot_data_otherrisks['researchdesign'][index]\n    resultsother['Risk'][index] = OtherRisks[plot_data_otherrisks['x'][index]]\n    \nresultsother.drop_duplicates(['Title'], inplace=True)\n\n    \nfor iter,orisk in  enumerate(OtherRisks):\n    filen = orisk + '.csv'\n    numdocs = resultsother['Title'].loc[resultsother['Risk']==orisk].nunique()\n    if numdocs:\n        resultsother.loc[resultsother['Risk']==orisk].iloc[:,0:-1].to_csv(filen, index=False) \n        print('TOP RESULTS FROM ' + str(numdocs) + ' PAPERS ON OUTCOMES AND '+ orisk +' RELATED RISK FACTORS PUBLISHED IN 2020' + '\\n')\n        print('TOP RESULT TITLE: ' + resultsother['Title'].loc[resultsother['Risk']==orisk].iloc[0] + '\\n')\n        print('REFER OUTPUT FILE {} FOR DETAILED RESULTS \\n'.format(filen))\n        print('---------------------------------------------\\n')\n","ea78c3e0":"#Clean Up\ndel resultsother\ndel plot_data_otherrisks\ngc.collect()","75d62c00":"shortlistdocs = 750","a72889b7":"similar_docs_comob = doc2vecmodel.docvecs.most_similar([task2vector], topn=shortlistdocs)\nplot_data_comob = pd.DataFrame(columns=['doc_id','title','publish_year','publish_date','journal','doi','abstract','region','x','y','xy_column','relevance', 'researchdesign', 'comorbidity', 'outcome'])\nnewi=0 \nfor i, scoreofdoc in similar_docs_comob:\n    for com in Comorbidities:\n        for out in Outcome:\n            if (check_thematic_comorbidities(com, i) & check_outcomes(out, i)):\n                plot_data_comob.loc[newi] = \"\"\n                plot_data_comob['doc_id'][newi] = df_covid['paper_id'][i]\n                plot_data_comob['publish_date'][newi] = df_covid['publish_date'][i]\n                #plot_data_comob['authors'][newi] = df_covid['authors'][i]\n                plot_data_comob['journal'][newi] = df_covid['journal'][i]\n                plot_data_comob['title'][newi] = df_covid['title_summary'][i]\n                plot_data_comob['publish_year'][newi] = int(df_covid['publish_year'][i])\n                plot_data_comob['doi'][newi] = df_covid['doi'][i]\n                plot_data_comob['abstract'][newi] = df_covid['abstract_summary'][i]\n                plot_data_comob['region'][newi] = df_covid['region'][i]\n                plot_data_comob['x'][newi] = Comorbidities.index(com)\n                plot_data_comob['y'][newi] = Outcome.index(out)\n                plot_data_comob['xy_column'][newi] = get_xy_cordinates(i)\n                plot_data_comob['relevance'][newi]= int(scoreofdoc*100)\n                plot_data_comob['researchdesign'][newi]=df_covid['researchdesign'][i]\n                plot_data_comob['comorbidity'][newi] = com\n                plot_data_comob['outcome'][newi]= out\n                newi+=1\n# Make format changes to data column for display \nconvert_dict = {'relevance': int}\nplot_data_comob = plot_data_comob.astype(convert_dict) \n#plot_data_comob.to_csv('outcome-comorbidities.csv',index=False)","61b27f76":"# Plot the evidence gap map for Outcomes vs Comorbidity\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode()\n\nfigure = evidencegapmap(dataset=plot_data_comob, x_column='x', y_column='y',\n  bubble_column='title',bubble_text='abstract', bubble_link='doi', size_column='relevance', color_column='researchdesign',xbin_list=Comorbidities, ybin_list = Outcome,\n  xbin_size=100, ybin_size = 100, x_title=\"Comorbidities\", y_title=\"Outcomes\", title='Outcome vs Comorbidity (Mode Random)',scale_bubble=4, marker_opacity=0.6,height=800)\niplot(figure)","a054f846":"ComorbiditiesFilter = [\"Diabetes\",\"Hypertension\",\"Immunodeficiency\", \"Cancer\", \"Respiratory\", \"Immunity\"]\nOutcomeFilter = [\"Any\",\"Death\", \"ICU Admission\", 'Mechanical Ventilation', \"Organ Failure\", \"Sepsis\", \"Discharge\"]\nDesignFilter = [\"Any\",\"Systematic Review\",\"Randomized\",\"Non-Randomized\",\"Ecological Regression\",\"Prospective Cohort\",\"Time Series Analysis\",\"Retrospective Cohort\",\"Cross Sectional\",\"Case Control\",\"Case Study\",\"Simulation\", \"Unknown\"]\nfrom ipywidgets import interact\n@interact\ndef search_articles(xbin=ComorbiditiesFilter,\n                    ybin=OutcomeFilter,\n                    design=DesignFilter,\n                    num_results=['All','Top10'],\n                    relevance = [\"Any\",\">75\"],\n                    publish_year=[\"2020\", \"Any\"],\n                    download=['No','Yes'],):\n   \n    \n    select_cols = ['title', 'publish_year', 'abstract','relevance', 'researchdesign', 'x','y','doi'] \n\n    # Filter for Relevance \n    if relevance == '>75':\n        relevanceval = 75\n    elif relevance == 'Any':\n        relevanceval = 0\n    \n    global results \n    \n    #Filter for Relevance\n    results = plot_data_comob[select_cols].loc[plot_data_comob['relevance'] > relevanceval]\n    results = results.sort_values(by=['relevance'], ascending=False)\n\n    # Filter for xbin\n    results['x']= results['x'].apply(lambda x: Comorbidities[x])\n    results['y']= results['y'].apply(lambda y: Outcome[y])    \n    if (xbin=='Any') & (ybin=='Any') :\n        pass\n    elif (xbin=='Any') :\n        results = results.loc[results['y'] == ybin]    \n    elif (ybin=='Any') :\n        results = results.loc[results['x'] == xbin]  \n    else:\n        results = results.loc[(results['x'] == xbin) & (results['y'] == ybin)]  \n\n        \n    \n    # Filter for Design\n    if (design == 'Any'):\n        pass\n    elif (design == 'Unknown'):\n        results = results.loc[results['researchdesign'] == 'unknown']     \n    else:\n        results = results.loc[results['researchdesign'] == design] \n    \n      # Publish Year \n    if publish_year == 'Any':\n        pass\n    else:\n        results = results.loc[results['publish_year'].astype({'publish_year': int}) == int(publish_year)]\n    \n   \n    # Number of Results \n    if num_results == 'Top10':\n        numresults = min(10,len(results.index))\n    elif num_results == 'All' :\n        numresults = len(results.index)\n\n        \n    results = results.head(numresults)\n    \n    # Output Results Yes or No \n    if download==\"Yes\": \n        if (len(results.index) == 0):\n            print('NO RESULTS')\n\n            return None\n        else:\n            results.to_csv('egm_search_results_comorbidities.csv')\n            numdocs = results['title'].nunique()\n            print('TOP RESULTS FROM ' + str(numdocs) + ' PAPERS ON COMORBIDITIES AND OUTCOMES' + '\\n')\n            print('TITLE: ' + results.iloc[0]['title'] + '\\n')\n            print('REFER OUTPUT FILE {} FOR RESULTS WITH BIN AND OTHER DETAILS'.format(\"egm_search_results_comorbidities.csv\"))\n            return results[select_cols]","c51aced6":"resultscomob = pd.DataFrame(columns=['Date','Title','URL','Journal','Severe','Severe Significant','Severe Adjusted','Severe Calculated','Fatality','Fatality Significant','Fatality Adjusted','Fatality Calculated','Multivariate adjustment', 'Design', 'Study population', 'Comorbidity'])\n\n#Filter out primarily the papers from 2020\nplot_data_comob = plot_data_comob.loc[plot_data_comob['publish_year'].astype({'publish_year': int}) == 2020]\n\nfor index, row in plot_data_comob.iterrows(): \n    resultscomob.loc[index] = \"\"\n    resultscomob['Date'][index]=plot_data_comob['publish_date'][index]\n    resultscomob['Title'][index]=plot_data_comob['title'][index]\n    resultscomob['URL'][index]=plot_data_comob['doi'][index]\n    resultscomob['Journal'][index]=plot_data_comob['journal'][index]\n    resultscomob['Design'][index]=plot_data_comob['researchdesign'][index]\n    resultscomob['Comorbidity'][index]= Comorbidities[plot_data_comob['x'][index]]\n\nresultscomob.drop_duplicates(['Title'], inplace=True)\n\nfor iter,como in  enumerate(Comorbidities):\n    filen = como + '.csv'\n    numdocs = resultscomob['Title'].loc[resultscomob['Comorbidity']==como].nunique()\n    if numdocs:\n        resultscomob.loc[resultscomob['Comorbidity']==como].iloc[:,0:-1].to_csv(filen, index=False) \n        print('TOP RESULTS FROM ' + str(numdocs) + ' PAPERS ON OUTCOMES AND '+ como +' RELATED RISK PUBLISHED IN 2020' + '\\n')\n        print('TOP RESULT TITLE: ' + resultscomob['Title'].loc[resultscomob['Comorbidity']==como].iloc[0] + '\\n')\n        print('REFER OUTPUT FILE {} FOR DETAILED RESULTS'.format(filen))\n        print('---------------------------------------------\\n')\n    ","53c73e9b":"#Clean Up\ndel resultscomob\ngc.collect()","085303fc":"!pip install transformers\n!pip install bert-extractive-summarizer\nimport torch\nfrom transformers import BertForQuestionAnswering\nfrom transformers import BertTokenizer\nfrom summarizer import Summarizer\nsummarizationmodel = Summarizer()","16fb19f2":"model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')","5666274d":"# no abstract provided, create a summary of the abstract . Return the summary \ndef summarize_text (full_body_text) :\n    if len(full_body_text) > 0:\n        try:\n            # ratio of words approximated as ratio of sentences for summarization and abstracts from first 2500 chars\n            res = summarizationmodel(full_body_text[0:2500],ratio=0.4)\n            full = ''.join(res).capitalize()\n            return full\n        except ValueError:\n            return \"Not Available\"\n    else :\n        return \"Not Available\"","f8e0a23b":"# Code adopted from https:\/\/mccormickml.com\/2020\/03\/10\/question-answering-with-a-fine-tuned-BERT\/\ndef answer_question(question, answer_text):\n    '''\n    Returns the `answer_text` and scores . If the answer start is same as answer end or if answer start is greater than answer end, Not Found is returned    \n    \n    '''            \n    # Apply the tokenizer to the input text, treating them as a text-pair. Trim to 512 max tokens \n    input_ids = tokenizer.encode(question, answer_text, max_length=512, pad_to_max_length=True)\n\n    # Search the input_ids for the first instance of the `[SEP]` token.\n    sep_index = input_ids.index(tokenizer.sep_token_id)\n\n    # The number of segment A tokens includes the [SEP] token istelf.\n    num_seg_a = sep_index + 1\n\n    # The remainder are segment B.\n    num_seg_b = len(input_ids) - num_seg_a\n\n    # Construct the list of 0s and 1s.\n    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n\n    # There should be a segment_id for every input token.\n    assert len(segment_ids) == len(input_ids)\n\n    # Run our example question through the model.\n    start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text.\n                                    token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from answer_text\n\n    # Find the tokens with the highest `start` and `end` scores.\n    answer_start = torch.argmax(start_scores).item()\n    answer_end = torch.argmax(end_scores).item()\n    start_score = torch.max(start_scores).item()\n    end_score = torch.max(end_scores).item()\n    \n    # Get the string versions of the input tokens.\n    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n\n    # Start with the first token.\n    answer = tokens[answer_start]\n\n    # Select the remaining answer tokens and join them with whitespace.\n    for i in range(answer_start + 1, answer_end + 1):\n        \n        # If it's a subword token, then recombine it with the previous token.\n        if tokens[i][0:2] == '##':\n            answer += tokens[i][2:]\n        \n        # Otherwise, add a space then the token.\n        else:\n            answer += ' ' + tokens[i]\n            \n    # Tensor objects need to be deleted as they take a lot of overheads and can create RAM outage \n    del start_scores, end_scores, tokens\n    if answer_start >=  answer_end :\n        start_score = end_score = -10\n        answer = \"Not Found\"\n    return answer, start_score, end_score","eef920c5":"from ipywidgets import widgets\nfrom IPython.display import display\nfrom ipywidgets import Button, Layout, VBox\nimport operator\nout = widgets.Output()\n\nsearchpapers = results.drop_duplicates(subset=\"title\")\n\ndef validatesearch():\n    # print \"validating\"\n    if len(search.value) < 500 :\n        return True\n    else:\n        print(\"Please enter upto 500 characters only \")\n        return False\n    \ndef responsesearch(change):\n    if validatesearch():\n        pass\n        \nsearch = widgets.Textarea(\n    value='Type your research question here. You can shortlist your search space by choosing the appropriate Bin(s) from above.',\n    description='Question:',\n    disabled=False,\n    layout=Layout(width='50%', height='100px')\n)\n\nsearch.observe(responsesearch, names=\"value\")\n\nbutton = Button(description='Search',layout=Layout(width='80px', height='40px'))\n\nbox_layout = Layout(display='flex',\n                    flex_flow='column',\n                    width ='100%',\n                    align_items = 'center')\n\nsearchbox = VBox(children=[search,button, out], layout=box_layout)\n\ndisplay(searchbox)\n\ndef responsebutton(b):\n    answers=[]\n    count=1\n    searchpapers = results.drop_duplicates(subset=\"title\")\n    for index, row in searchpapers.iterrows():\n        #print(search.value,row['abstract'])\n        if row['abstract'] != 'Not Available' :\n            answer, sstart, send = answer_question(search.value,row['abstract'])\n            answers.append([index, answer, sstart+send])\n        else :\n            #print(\"Abstract missing , generating from full body text for Paper {}\".format(count))\n            #identify row from the covid dataframe , summarize from the full body text and \n            doi_match_index = df_covid.index[df_covid['doi'] == row['doi']].tolist()\n            if doi_match_index :\n                text_to_summary=df_covid['body_text'].iloc[doi_match_index[0]]\n                sum= summarize_text(text_to_summary) \n                answer, sstart, send = answer_question(search.value,sum)\n                answers.append([index, answer, sstart+send])\n        print(\"Completed Analysis of Paper {} of {}\".format(count, len(searchpapers.index)))\n        count+=1\n            \n    answers = sorted(answers, key=operator.itemgetter(2), reverse = True) \n    print(answers)\n    print('\\n')\n    print(\"Top Answer :-- {} \".format(str(answers[0][1])))\n    print(\"2nd Best Answer :-- {} \".format(str(answers[1][1])))\n    print(\"3rd Best Answer :-- {} \".format(str(answers[2][1])))\n\n    \nbutton.on_click(responsebutton)","59858437":"*Helper functions used for tagging logic here ..*","5e09f1a1":"### Analyze short texts\n***\n\nCheck for the length of the body texts and drop text which are very short as they would create false positives while making recommendations","94c5ca36":"*Given below the Research Objectives that we have considered* ","2ddc0ad8":"*Geographical Tagging to identify any regional factors. The tagging needs to be extended on an ongoing basis for the top impacted areas and research output*","654fd90c":"### Time based plot for Disease vs Outcome (NLP MODE)\n\nThe positioning in the bin of a paper indicates similarity \/ dissimilarity. On hover papers in a bin from a region are displayed together\n\nSize of a bubble indicates relevance to our Research Objective (Task 2 Risk Factors). In a production system the display and performance on hoover can be further enhanced.\n\n*In Evidence Gap Map a paper can belong to multiple bins and hence we build a separate dataframe for its display.*","24541dbc":"### Load the Word2Vec model \n***\n\nWe have decided to keep this in a separate kernel as the usage of Word2Vec is not a must have requirement, however the same has been used to fine tune the choice of keywords. Further this was necessary owing to RAM overflow issues we experienced. \n\nDoc2Vec Model also provides word vectors which could also have been useful for this purpose, however as we have pre-processed the words (stemmed etc.) and not considered biagrams etc. we preferred training a separate Word2Vector model. \n\nSciBERT, BioBERT were also considered , however they were not finally chosen as these models need a context for word embeddings and Word2Vec was a better fit for our usage scenarios. \n\n*However in a situation where the research is ongoing on a topic and pre designed models have to be used , these transformer based and other domain specific models build on much larger corpous of documents would be a preferred approach.*","fc743455":"### Outcome vs Other Risk Factors (Age, Weight, Smoking etc.)\n\nThe positioning inside a bin is random for this plot. This mode is more user friendly for display , however similarity\/dissimilarity is lost \n\nRelevance of the paper to the Research Question is shown with size of the bubble. The colour indicates the study design.\n\n*The results are saved by Risk type as per column suggestions made on [this discussion](https:\/\/www.kaggle.com\/antgoldbloom\/aipowered-literature-review-csvs\/discussion\/143376) and as per the  [kernel here](https:\/\/www.kaggle.com\/ajrwhite\/covid-19-literature-review-data-load#Load-Risk-Factors). Owing to limited time some of the fields are left empty as these need significant efforts for completion. An approach is presented in Section 5, which can be useful for completing using a BERT trained QA system*","7307a5db":"The results are saved by Comorbidities types as separate files.","b41b5f87":"*Refer this section of code for more details on the sub area keywords used* . ","dff76893":"### Outcome vs Comorbidities (Hypertension, Diabetes etc.)\n\nThe positioning inside a bin in this plot is random. Relevance and study design are indicated.\n\nAn interactive widgets is provided for downloading and displaying of relevant research papers (*bin wise, year, number of results, study design etc.*)\n\nPlease note that for displaying the interactive widget, the kernel has to be executed! (edit mode). *Known issue, refer [here](https:\/\/www.kaggle.com\/questions-and-answers\/33450) and [here](https:\/\/github.com\/jupyter-widgets\/ipywidgets\/issues\/2360) (github)*\n\n*An image of the widget as available in edit mode is provided below.*\n\n![egmexplore](https:\/\/raw.githubusercontent.com\/mb7419\/egm\/master\/miscimages\/egmexplorer.png)\n","e10082d9":"### Analyze long texts\n***","993b85dc":"Around 500 articles are identified as very short (less than a page of text) and including them may increase chances of false positives while making recommendations.  Around 100 are from 2020 and are identified as short communications , like letters to the editor on COVID etc. . \n\nOn closer analysis we decided to drop articles less than 2200 characters","cdc5621a":"*Vectorize the Research Objectives*","f9ea0e26":"### Build and Train the Doc2Vec model\n***\n\nThe Doc2Vec vector size dimensions may need further tuning. We can have smaller sized vectors which give higher similarity ratings , however the same also produce more false negatives. ","22d56210":"# Thematic Q&A of Risks with Evidence Gap Maps\n\n### Background\n\nThe objective of the CORD-19 initiative is to use the advances in Natural Language Processing (NLP) for analysing and drawing actionable insights from the fast expanding literature in the area (as per [estimates](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/discussion\/142298) it has a steep growth rate owing to the interest around the globe in dealing with the COVID-19 crisis). The urgent need, as we understood, is to develop a framework that can help the healthcare community to zero in on relevant research for addressing research gaps and driving policy initiatives. \n\n### Goals and Objectives\n\nThis kernel proposes a thematic approach, that can help users to identify and focus on areas of work requiring urgent help and attention. Evidence Gap Maps are used in Medical literature for identifying gaps in research and deciding on policy actions. This kernel builds on the core of an Evidence Gap Map and extends it with the power of NLP and AI.  The attempt is to identify from an expanding NLP and ML toolsets, state of the art and legacy tools which can be be best fit for the purpose. \n\nThe primary submission is made for identifying Risk Factors (***Task 2 of the CORD 19 challenge***). This kernel has however been shared in all Tracks as it can be useful in other areas as well. Given below a summary of the goals we address \n\n![objectives](https:\/\/raw.githubusercontent.com\/mb7419\/egm\/master\/miscimages\/obj.png)\n\n\n\n### Implementation Approach\n\n***\n\nThe technical approach followed is to train a Doc2Vec model on the full body text of the papers. Similarity measures are used to identify, Top N papers to the **Task 2** Research Objectives. A semi manual approach is used for keyword identification, and subsequently used to categorize papers into themes. Evidence Gap Maps are then plotted and the thematic data is made available for further processing. As a sample downstream application a BERT based Q&A system is used to answer specific research questions. A block diagram is presented below.\n\n![blockdiagram](https:\/\/raw.githubusercontent.com\/mb7419\/egm\/master\/miscimages\/block.png)\nThe kernel is organized into the following sections\n\n1. **Section 1** : Pre processing and Data preperation \n2. **Section 2** : Model Development\n3. **Section 3** : Keyword Identification\n4. **Section 4** : Evidence Gap Maps & Evaluation\n5. **Section 5** : Q&A for specific Research Questions\n\n\nThe concept is based on [this discussion thread](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/discussion\/137027) and inputs from Savanna Reid ([@savannareid](https:\/\/www.kaggle.com\/savannareid)) an Epidemiologist (subject matter expert). \n\nCode wherever reused is acknowledged in the relevant sections. In particular would like to mention the work by Andy White ([@arjwhite](https:\/\/www.kaggle.com\/ajrwhite)) with [thematic keyword tagging](https:\/\/www.kaggle.com\/ajrwhite\/covid-19-thematic-tagging-with-regular-expressions), which has been very useful and leveraged in Section 3.\n\nSincere gratitude to all the selfless souls who have worked tirelessly on the challenge, often focussing on the necessary stuff, rather than competing to win.\n\n","91effb7a":"## Section 2: Model Development\n*** \n\nDoc2Vec model for document to document similarity is the preferred NLP approach and hence is chosen. Since t-SNE a non linear model was prefered over SVD\/PCA and other linear models for visualization of high dimensional Doc2Vec vectors.\n\nIn this section we train the Doc2Vec model on full body text of the corpus of documents. Two dimensional t-SNE coordinates created for the Doc2Vec vectors are used later used for plotting the different papers.\n\nA word2vec model is trained separately ([made available as a separate kernel](https:\/\/www.kaggle.com\/uplytics\/covid-19-word2vec-model-and-vectors)) and the output of the model is included in this kernel to identify keywords for a regex based tagging of documents.\n\nGensim libraries are used for the Word2Vec and Doc2Vec modelling\n","59cbc6a4":"## Section 5 : Q&A for specific Research Questions\n***\n\nThe results are available for download in Section 4 by the different themes (bin wise), thus making it easy to navigate and reach papers of a users interest. Next task we look at in this section is to give the user an expert Q&A system to identify relvant literature and answers than can provide insights to specific research questions of interest for the user. \n\n*An example scenario is the exploration of the issue raised around the hypertension and the related medicines and the relation to high morbidity rates. This was the original research question which intrigued me and a very hot topic of discussion in the medical community e.g. [here](http:\/\/www.nephjc.com\/news\/covidace2) .\n\nA typical search based approach would provide a long list of options, however the proposed thematic Q&A approach , make the research more focused and productive.\n\nWe use the pretrained (for SQUAD) BERT transformer and use the associated BERT tokenizer.\n\nA domain trained Q&A system build on BIOBERT or SCIBERT would have performed better, however the generic BERT system is used owing to the ease of use and it integrates well with the Kaggle Python 3 environment. \n\nSince several papers in the courpus are missing the abstracts a BERT Extraction Summarizer is used for building summary text on the fly, from the first page of the body text (2500 characters). For the intended use (to identify answers from summary text) it suits the purpose better than the more state of the art summarization mechanisms like Abstractive Summarization (BART etc.)","6b6f6210":"Around 350 articles are identified as very long and including them may affect performance and may not be very value added.  In particular the ones from 2020 are not directly related to COVID. However we decide to retain them in the corpus as including code to drop such texts could affect future additions of longer documents which may be highly relevant.\n","1560cdd3":"The list of matching documents by different risk factors are now saved as separate files","9bb5e2ba":"*The implementation of the widget is provided here and can be experienced by executing this kernel*","e05b7399":"### What Next for this kernel ?\n\nI hope this contribution is found as a useful contribution by the NLP and health care community in building our response for addressing some of the COVID challenges that lie ahead !! \n\nIn summary Evidence Maps are a useful tool to identify research gaps, provides ease of access to thematically view search results. Year wise plotting of resarch artifacts can be a useful way for tracking the progress of reseearch areas and building up of a comprehensive literature review and other pertinent analysis. \n\nSome of the possible future areas of work can be in automating some of the tasks of a literature review and also buidlding NLP based Expert systems for healthcare. \n\nThe CORD project as a whole I also see as a step foreward in leveraging NLP in the research helical and it has been very fullfilling to participate and contribute on Kaggle for this !!","4cda72c5":"*The plots are not very different for different perplexity values , however these are well spread out from the center and we choose the tSNE coordinates obtained with perplexity value of 5. The intention of this plot is not clustering of documents and hence a simple scatter plot is used and no categorization variable is used for a classical multi couloured t-SNE plot*","d34f3f39":"*Refer here for code for the Summarization and question answering system *","3fb781b9":"**Research Methodology Tags** are as per discussion on [this thread](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/discussion\/139355) and as per inputs from [@savavanareid](https:\/\/www.kaggle.com\/savannareid) \n\nThe taxonomy followed\n* Systematic Review\n* Randomized\n* Non-Randomized\n* Ecological Regression\n* Prospective Cohort\n* Time Series Analysis\n* Retrospective Cohort \n* Cross Sectional\n* Case Control\n* Case Study\n* Simulation\n* Unknown","b9902b9a":"****Note** : Since keyword idedentification is an ongoing effort and requires Subject Matter Expertise, we have not spend significant time and effort in fine tuning these keywords or in keyword mining, which could have increased accuracy of our results.**","5fde2a74":"To speed up execution when there are no changes in the CORD19 database, flags are provided for directly loading the dataframe and Doc2Vec model.","6314a6d0":"## Section 4: Evidence Gap Maps & Evaluation\n***\n\nThis is the heart of this submission and needed significant time and efforts to develop. The inspiration for this has come from the discussion [on this thread](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/discussion\/137027) and the initial conceptualization in the image below and as per inputs on the thread.\n\n![egm](https:\/\/raw.githubusercontent.com\/mb7419\/egm\/master\/miscimages\/egm.jpg)\n\n\nWhen conceptualizing the same , the challenges have been two fold\n\n1. The primary challenge has been in conceptualizing these using AI. The initial discussion was to take a supervised learning approach, where a Subject Matter Expert would categorize a subset of studies and then use classification. However this we realized early was not feasible, owing to the efforts required and the approach not being very scalable. There is [usage of Evidence Maps for analyzing COVID-19 research](https:\/\/www.nornesk.no\/forskningskart\/NIPH_mainMap.html) however it primarily uses subject matter experts to categorize studies into bins.\n\n1. The second challenge has been in implementation as none of the popular open sourced plotting packages provide a ready to use graph type that supports for plotting two categorical variables as Bins (as per our analysis) and also creating plots insides these bins. The approach we took was to create a package, leveraging an existing implementation available [here](https:\/\/github.com\/AashitaK\/bubbly).\n\nThe current approach of combining Doc2Vec and Keywords provides a scalable method however Keyword identification and mining requires manual interventions. \n\nA PyPy python package has now been created and [hosted here](https:\/\/github.com\/mb7419\/egm). *The installation is available as **pip install egm**. *\n\nThe code is however included in this kernel as it makes it easier to develop and enhance.  \n\nCurrently two modes (Random and NLP) as described further in this section are supported.","f438c6fe":"*Install the Hugging Face BERT transformer library. *","45a88c83":"Use t-SNE to get the two dimensional coordinates for the documents in the corpus. These coordinates indicate similarity \/ dissimilarity between papers in two dimensions.\n\n*Play around with the perplexity and learning rate values to ensure that the output is well spread out. t-SNE requires some ammount of tuning for it to avoid getting stuck in local minima's *","67dd7ac6":"Plot histogram for short papers","f2652148":"This section provides an implementation of a Q&A widget which takes in a questiion and identifies the top 3 answers from the shortlisted papers. \n\nFor the demo on this functionality the kernel has to be in the edit mode (known issue with ipywidgets as already highlighted above). An image of a sample question and answer is provided below with a response to the query on the odd's ratio in a Top 10 search on the Hypertension and Death bin from the Evidence Gap Map. The results below indicate the paper id, the actual text found as an answer and the score which is used to rank order the results. The Top 3 answers are also mentioned, however more relevant when the usage is as a expert system for answering specific research questions (e.g. *does hypertension medicines raise the mortality rates related to covid ?* ).\n\nThere is scope of improvement in the Q&A system as it's behaviour is currently sensitive to keywords included (response varies with minor changes in text), however the purpose of including it in the kernel is to showcase how applications like Q&A, Clustering , Topic Modelling, Prediction and Recommendations can be more effective as  proposed (context specific thematic searches) .\n\n![oddsratio](https:\/\/raw.githubusercontent.com\/mb7419\/egm\/master\/miscimages\/qa.png)","1842c205":"The below outcome related keywords were identified and reviewed by an epidemiologist Savanna Reid (@savannareid).\n\n*Recommendations from the thresher.io who offered a free account as per discussion [here](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/discussion\/137027) was also very beneficial for identifying long tailed phrases (trigrams) .*\n","e5e2922d":"\n## Section 1 : Pre processing and Data preperation\n***\n\n\n<div id=\"sec1\">\nWe load the Metadata and JSON files from the CORD 19 Database and create a pandas dataframe for evaluation.\n<\/div>","82c082ba":"*Provided here is the code to identify the Top N matching documents with the top 5 matching given below *","440e3b9e":"## Section 3: Keyword Identification\n*** \n\nEnhancing accuracy of tagging of appropriate themes (bins in the planned evidence gap maps) needed keyword matching. The mechanism to identify papers in each bin is through a regex based search for the exact words and phrases (biagrams and trigrams).\n\nA list of curated keywords for the different themes were needed and identified by us for the purpose. \n\nThree different mechanisms followed by us for the same were\n\n1. Word2Vec recommendations for Top N similar words \n2. Recommendations as provided by a software from off the shelf software \n3. Inputs from Subject Matter Experts and other kernels in this competition\n\nGiven below a sample of few searches on diseases, outcomes and risk factors that can be done using word2vec.","f44facd7":"### Research Objectives and EGM\n\nThe main research objective targeted by this kernel is to address,  \n\n**What do we know about COVID-19 risk factors?** and its sub objectives as included in the Task 2 of the CORD challenge\n\nThese are also augmented to a limited extend by including inputs from the work by Savanna Reid (Subject Matter Expert) as[ provided here](https:\/\/docs.google.com\/spreadsheets\/d\/1t2e3CHGxHJBiFgHeW0dfwtvCG4x0CDCzcTFX7yz9Z2E\/edit#gid=389064679) to identify the sub objectives of each risk factor. \n\nWe find similar documents using Doc2Vec to these objectives and define relevance as the dot product of the Doc2Vector as the vector for the Research Objectives. The Keywords then narrow down each of the papers to each bin of the Evidence Gap Map. The number of similar documents which Doc2Vec would consider for including on the EGM plot is a configurable parameter (we have experimented with 300 to 1200 documents). \n\n*Some of the papers identified [as key contributions](https:\/\/www.kaggle.com\/covid-19-contributions) can also be added to the below inputs to increase the accuracy of the solution. However as the aim of this kernel is to provide an unsupervised generic framework, we dont use the same.  *\n","42f14ec6":"### Evidence Gap Map (EGM) Python Package\n\nThe usage of the Evidence Gap Map in any other kernel can be made with the following commands\n\n!pip install egm\nfrom egm.egm import evidencegapmap\n\n*Alternatively, the code here can be included directly and we have taken the same approach in this kernel as we found it easier to debug and enhance functionality more easily* \n"}}