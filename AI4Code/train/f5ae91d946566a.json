{"cell_type":{"713cfccc":"code","0c06dbf7":"code","d500309b":"code","3daa75c5":"code","f0564628":"code","f29fd1ef":"code","4be1238e":"code","7fefa865":"code","d5ca1dea":"code","03e8786c":"code","507c4826":"code","24185d92":"code","ce5d007d":"code","59085449":"code","5a041e75":"code","bc4e18ba":"code","ec15c337":"code","b859eaba":"markdown","a9895ca4":"markdown","17eaf2a2":"markdown","6cb8ee19":"markdown","f82a3d77":"markdown","a68f18d8":"markdown"},"source":{"713cfccc":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image, display, Markdown\nimport matplotlib.cm as cm\nfrom sklearn.metrics import confusion_matrix,accuracy_score, classification_report\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.preprocessing import image\nfrom time import perf_counter\nimport seaborn as sns\n\ndef printmd(string):\n    # Print with Markdowns    \n    display(Markdown(string))","0c06dbf7":"# Load the paths\nfire_path = '..\/input\/fire-dataset\/fire_dataset\/fire_images\/'\nfire_path_list = os.listdir(fire_path)\nfire_path_list = [fire_path + f for f in fire_path_list]\n\nno_fire_path = '..\/input\/fire-dataset\/fire_dataset\/non_fire_images\/'\nno_fire_path_list = os.listdir(no_fire_path)\nno_fire_path_list = [no_fire_path + f for f in no_fire_path_list]","d500309b":"df = pd.DataFrame({'Path': fire_path_list + no_fire_path_list})\ndf['Label'] = df['Path'].apply(lambda x: 0 if 'non_fire' in x else 1)\ndf['Label_String'] = df['Label'].apply(lambda x: 'no fire' if x == 0 else 'fire')\n\n# Shuffle\ndf = df.sample(frac = 1.0).reset_index(drop = True)\n\n# Display the first lines\npd.options.display.max_colwidth = 200\ndf.head()","3daa75c5":"df['Label_String'].value_counts().plot.bar(color = ['red','gray'])\nplt.title('Number of pictures', fontsize = 18)\nplt.xticks(rotation = 0, fontsize = 15) \nplt.show()","f0564628":"# Display some pictures of the dataset\nfig, axes = plt.subplots(nrows=4, ncols=6, figsize=(15, 8),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    img = image.load_img(df['Path'].iloc[i])\n    ax.imshow(img, cmap = 'gray')\n    title = df['Label_String'].iloc[i]\n    ax.set_title(title, fontsize = 15)\nplt.tight_layout(pad=0.5)\nplt.show()","f29fd1ef":"df_original = df.copy()\n\n# Split into training, test and validation sets\nval_index = int(df_original.shape[0]*0.1)\n\ntrain_df = df_original.iloc[val_index:]\ntest_df = df_original.iloc[:val_index]","4be1238e":"# Display the shapes of the sets\ntrain_df.shape, test_df.shape","7fefa865":"train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n    validation_split=0.1\n)\n\ntest_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n)\n\ntrain_images = train_generator.flow_from_dataframe(\n    dataframe=train_df,\n    x_col='Path',\n    y_col='Label_String',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=True,\n    seed=0,\n    subset='training',\n    rotation_range=30,\n    zoom_range=0.15,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.15,\n    horizontal_flip=True,\n    fill_mode=\"nearest\"\n)\n\nval_images = train_generator.flow_from_dataframe(\n    dataframe=train_df,\n    x_col='Path',\n    y_col='Label_String',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=True,\n    seed=0,\n    subset='validation',\n    rotation_range=30,\n    zoom_range=0.15,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.15,\n    horizontal_flip=True,\n    fill_mode=\"nearest\"\n)\n\ntest_images = test_generator.flow_from_dataframe(\n    dataframe=test_df,\n    x_col='Path',\n    y_col='Label_String',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=False\n)","d5ca1dea":"# Load the pretained model\npretrained_model = tf.keras.applications.MobileNetV2(\n    input_shape=(224, 224, 3),\n    include_top=False,\n    weights='imagenet',\n    pooling='avg'\n)\npretrained_model.trainable = False","03e8786c":"inputs = pretrained_model.input\n\nx = tf.keras.layers.Dense(128, activation='relu')(pretrained_model.output)\nx = tf.keras.layers.Dense(128, activation='relu')(x)\n\noutputs = tf.keras.layers.Dense(2, activation='softmax')(x)\n\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.5, min_lr=0.00001)\n\nhistory = model.fit(\n    train_images,\n    validation_data=val_images,\n    batch_size = 32,\n    epochs=5,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=2,\n            restore_best_weights=True),\n            learning_rate_reduction\n    ]\n)","507c4826":"fig, axes = plt.subplots(2, 1, figsize=(15, 10))\nax = axes.flat\n\npd.DataFrame(history.history)[['accuracy','val_accuracy']].plot(ax=ax[0])\nax[0].set_title(\"Accuracy\", fontsize = 15)\nax[0].set_ylim(0,1.1)\n\npd.DataFrame(history.history)[['loss','val_loss']].plot(ax=ax[1])\nax[1].set_title(\"Loss\", fontsize = 15)\nplt.show()","24185d92":"import cv2\n\ndef preprocessing(path):\n    img = image.load_img(path)\n    img = tf.keras.preprocessing.image.img_to_array(img)\/255\n    img = cv2.resize(img, (224,224))\n    return img\n\n# Preprocess the test images\npath_lst = test_df['Path'].values\nX_test = [preprocessing(p) for p in path_lst]\nX_test = np.array(X_test)\n\n# Predict the label\npred = model.predict(X_test)\npred = np.argmax(pred,axis=1)\n\n# Inverse the 1 and 0 in the label to adapt to the mapping of the neural network\npred = [0 if x == 1 else 1 for x in pred]\n\n\ny_test = test_df['Label']\n\n# Get the accuracy score\nacc = accuracy_score(y_test,pred)\n\n# Display the results\nprintmd(f'## {acc*100:.2f}% accuracy on the test set')","ce5d007d":"# Get the strings of the prediction\npred = ['fire' if x == 1 else 'no fire' for x in pred]\ny_test = test_df['Label_String']","59085449":"print(classification_report(pred,y_test))","5a041e75":"# Display a confusion matrix\ncf_matrix = confusion_matrix(y_test, pred, normalize='true')\nplt.figure(figsize = (10,8))\nsns.heatmap(cf_matrix, annot=True, xticklabels = sorted(set(y_test)), yticklabels = sorted(set(pred)),cbar=False)\nplt.title('Normalized Confusion Matrix\\n', fontsize = 23)\nplt.xlabel(\"Predicted Classes\",fontsize=15)\nplt.ylabel(\"True Classes\",fontsize=15)\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15,rotation=0)\nplt.show()","bc4e18ba":"def get_img_array(img_path, size):\n    img = tf.keras.preprocessing.image.load_img(img_path, target_size=size)\n    array = tf.keras.preprocessing.image.img_to_array(img)\n    # We add a dimension to transform our array into a \"batch\"\n    # of size \"size\"\n    array = np.expand_dims(array, axis=0)\n    return array\n\ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    # First, we create a model that maps the input image to the activations\n    # of the last conv layer as well as the output predictions\n    grad_model = tf.keras.models.Model(\n        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n    )\n\n    # Then, we compute the gradient of the top predicted class for our input image\n    # with respect to the activations of the last conv layer\n    with tf.GradientTape() as tape:\n        last_conv_layer_output, preds = grad_model(img_array)\n        if pred_index is None:\n            pred_index = tf.argmax(preds[0])\n        class_channel = preds[:, pred_index]\n\n    # This is the gradient of the output neuron (top predicted or chosen)\n    # with regard to the output feature map of the last conv layer\n    grads = tape.gradient(class_channel, last_conv_layer_output)\n\n    # This is a vector where each entry is the mean intensity of the gradient\n    # over a specific feature map channel\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    # We multiply each channel in the feature map array\n    # by \"how important this channel is\" with regard to the top predicted class\n    # then sum all the channels to obtain the heatmap class activation\n    last_conv_layer_output = last_conv_layer_output[0]\n    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n\n    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n    heatmap = tf.maximum(heatmap, 0) \/ tf.math.reduce_max(heatmap)\n    return heatmap.numpy()\n\ndef save_and_display_gradcam(img_path, heatmap, cam_path=\"cam.jpg\", alpha=0.4):\n    # Load the original image\n    img = tf.keras.preprocessing.image.load_img(img_path)\n    img = tf.keras.preprocessing.image.img_to_array(img)\n\n    # Rescale heatmap to a range 0-255\n    heatmap = np.uint8(255 * heatmap)\n\n    # Use jet colormap to colorize heatmap\n    jet = cm.get_cmap(\"jet\")\n\n    # Use RGB values of the colormap\n    jet_colors = jet(np.arange(256))[:, :3]\n    jet_heatmap = jet_colors[heatmap]\n\n    # Create an image with RGB colorized heatmap\n    jet_heatmap = tf.keras.preprocessing.image.array_to_img(jet_heatmap)\n    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n    jet_heatmap = tf.keras.preprocessing.image.img_to_array(jet_heatmap)\n\n    # Superimpose the heatmap on original image\n    superimposed_img = jet_heatmap * alpha + img\n    superimposed_img = tf.keras.preprocessing.image.array_to_img(superimposed_img)\n\n    # Save the superimposed image\n    superimposed_img.save(cam_path)\n\n    # Display Grad CAM\n#     display(Image(cam_path))\n    \n    return cam_path\n    \npreprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\ndecode_predictions = tf.keras.applications.mobilenet_v2.decode_predictions\n\nlast_conv_layer_name = \"Conv_1\"\nimg_size = (224,224)\n\n# Remove last layer's softmax\nmodel.layers[-1].activation = None","ec15c337":"# Display the part of the pictures used by the neural network to classify the pictures\nnrows = 10\nfig, axes = plt.subplots(nrows=nrows, ncols=2, figsize=(15, 5 * nrows),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\ni = 0\nfor i, nrow in enumerate(range(nrows)):\n    \n    img_path = test_df.Path.iloc[i]\n    title = f\"True: {test_df.Label_String.iloc[i]}\\nPredicted: {pred[i]}\"\n    \n    # Original Picture\n    img = image.load_img(img_path)\n    axes[nrow,0].imshow(img)\n    axes[nrow,0].set_title('ORIGINAL PICTURE\\n' + title)\n    \n    # Calculate Grad-CAM class activation\n    img_array = preprocess_input(get_img_array(img_path, size=img_size))\n    heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n    cam_path = save_and_display_gradcam(img_path, heatmap)\n    img = plt.imread(cam_path)\n    axes[nrow,1].imshow(img)\n    axes[nrow,1].set_title('GRAD-CAM CLASS ACTIVATION\\n' + title)\n\nplt.tight_layout()\nplt.show()","b859eaba":"# 4. Visualize the result <a class=\"anchor\" id=\"4\"><\/a>","a9895ca4":"# 1. Load and transform the dataset<a class=\"anchor\" id=\"1\"><\/a>","17eaf2a2":"# Visual Explanations from Deep Networks Grad-CAM\ud83d\udd25\n## \\# Data Augmentation\n## \\# Generator\n\n![fire](https:\/\/i.imgur.com\/PPxFrEo.png)\n\n# Table of contents\n\n[<h3>1. Load and transform the dataset<\/h3>](#1)\n\n[<h3>2. Split the data and create the generator<\/h3>](#2)\n\n[<h3>3. Create and train the model<\/h3>](#3)\n\n[<h3>4. Visualize the result<\/h3>](#4)\n\n[<h3>5. Class activation heatmap for image classification<\/h3>](#5)\n\n\n# Context\nThe dataset was created by my team during the NASA Space Apps Challenge in 2018, the goal was using the dataset to develop a model that can recognize the images with fire.\n\n# Content\nData was collected to train a model to distinguish between the images that contain fire (fire images) and regular images (non-fire images), so the whole problem was binary classification. Data is divided into 2 folders, fireimages folder contains 755 outdoor-fire images some of them contains heavy smoke, the other one is non-fireimages which contain 244 nature images (eg: forest, tree, grass, river, people, foggy forest, lake, animal, road, and waterfall). Data is skewed, which means the 2 classes(folders) doesn't have an equal number of samples, so make sure that you have a validation set with an equally-sized number of images per class (eg: 40 images of both fire and non-fire classes).","6cb8ee19":"# 2. Split the data and create the generator<a class=\"anchor\" id=\"2\"><\/a>","f82a3d77":"# 3. Create and train the model<a class=\"anchor\" id=\"3\"><\/a>","a68f18d8":"# 5. Class activation heatmap for image classification<a class=\"anchor\" id=\"5\"><\/a>\n## Grad-CAM class activation visualization\n*Code adapted from keras.io*"}}