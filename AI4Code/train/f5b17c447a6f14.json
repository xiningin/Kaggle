{"cell_type":{"a00b198a":"code","07425418":"code","6c460caa":"code","1dbe2f94":"code","93b81387":"code","48a63cb8":"code","57e34dcf":"code","7cbdf313":"code","60db1d01":"code","f60536b4":"code","132ff61f":"code","d7a65870":"code","ae22bb59":"code","b50c1e99":"code","358cdf4c":"code","e7cc604c":"code","ae4e66ba":"code","2fd58ef2":"code","1cd3ed3d":"code","16e277a1":"code","cf67ccb6":"code","b98751bc":"code","2d93c818":"code","fe73fe54":"code","a36dd474":"code","1e625e72":"code","4269d063":"code","28c8cd1d":"code","acdf1e43":"code","2bab30e7":"code","bfbb1aa9":"code","be803e49":"code","fa86efa9":"code","3a5cb3c6":"code","7ae59264":"code","3dcd0bc8":"code","549d1bc4":"code","0bce3863":"code","0e495e19":"code","a0d4a179":"code","841bf5d3":"code","9456a585":"code","b8ada210":"code","61342363":"code","db2467f1":"code","62f308af":"code","01b423cc":"code","10159dcd":"code","769aa438":"markdown","4513e753":"markdown","64a44e30":"markdown","6a0837bc":"markdown","2b6a4a02":"markdown","71b94ffa":"markdown","5db8cfc1":"markdown","325da759":"markdown","9849bf8b":"markdown","e7b0d5dc":"markdown","583ebc98":"markdown","2712c25b":"markdown","559f4f3e":"markdown","00009256":"markdown","c4ae0295":"markdown","1ea8fc5b":"markdown","b6d5aa5b":"markdown","1b4adaa8":"markdown","8d3868f7":"markdown","aaea40d6":"markdown","d9d1a5fb":"markdown","54974923":"markdown","925513d2":"markdown","c038cdfb":"markdown","938ba828":"markdown","f4047ddf":"markdown","07841795":"markdown","f505c569":"markdown","e472fd98":"markdown","d4cab8de":"markdown","fcfc8115":"markdown","ea932041":"markdown","c505f92a":"markdown","9a820ec0":"markdown","ff2ccc53":"markdown","62a1ed5c":"markdown","77d3505d":"markdown","f6bde954":"markdown","55eb2f62":"markdown"},"source":{"a00b198a":"import pandas as pd\nimport numpy as np\n# we are going with direct import method\ndf = pd.read_csv('..\/input\/telecom-users-dataset\/telecom_users.csv')","07425418":"df.head()","6c460caa":"df.info()","1dbe2f94":"# Missing Values\ndf.isna().sum()","93b81387":"df.head().T","48a63cb8":"df.info()","57e34dcf":"#Function to convert the Total column to Contineous feature\ndef convert_to_contineous(feature):\n    df[feature]=pd.to_numeric(df[feature], errors='coerce')","7cbdf313":"convert_to_contineous('TotalCharges')","60db1d01":"df.info()","f60536b4":"df.isna().sum()","132ff61f":"df=df.dropna(axis=0)","d7a65870":"# CustomerID is the id of the customer with corresponding details.\n#this information may not be requried for analysis and modeling as the customerID will be all unique values.\n# so we can drop the features\ndf.drop(['customerID','Unnamed: 0'], axis=1, inplace=True)","ae22bb59":"df.head()","b50c1e99":"import matplotlib.pyplot as plt\nimport seaborn as sns","358cdf4c":"# Feature SeniorCitizen is numeric categorical values. it contains 1 and 0. however, for EDA purpose let us convert the feature to categorical.\ndf['SeniorCitizen']=df['SeniorCitizen'].astype('category')","e7cc604c":"df.describe().T","ae4e66ba":"int_feat = df.select_dtypes(exclude=['object','category']).columns\nfig, ax = plt.subplots(nrows=2, ncols = 2, figsize=(15,8), constrained_layout=True)\nax=ax.flatten()\nfor c,i in enumerate(int_feat):\n    sns.histplot(df[i], ax=ax[c], bins=10)\n    ax[c].set_title(i)","2fd58ef2":"cat_cols = df.select_dtypes(include=['object','category']).columns\nfig, ax = plt.subplots(nrows=5, ncols=4, figsize=(15,15), constrained_layout=True)\nax=ax.flatten()\nfor x,i in enumerate(cat_cols):\n    sns.countplot(x=df[i], ax=ax[x])","1cd3ed3d":"# Churn is the Target variable, let us to the analysis with the Target variable\nfig, ax = plt.subplots(nrows=2, ncols = 2, figsize=(15,8), constrained_layout=True)\nax=ax.flatten()\nfor c,i in enumerate(int_feat):\n    sns.histplot(data=df, x=i, ax=ax[c], bins=10, hue='Churn', kde=True)\n    ax[c].set_title(i)","16e277a1":"fig, ax = plt.subplots(nrows=2, ncols = 2, figsize=(15,8), constrained_layout=True)\nax=ax.flatten()\nfor c,i in enumerate(int_feat):\n    sns.boxplot(data=df, x=i, ax=ax[c], y='Churn')\n    ax[c].set_title(i)","cf67ccb6":"cat_cols = df.select_dtypes(include=['object','category']).columns.to_list()\ncat_cols.remove('Churn')\nfig, ax = plt.subplots(nrows=4, ncols=4, figsize=(15,15), constrained_layout=True)\nax=ax.flatten()\nfor x,i in enumerate(cat_cols):\n    sns.countplot(data=df,x=i, ax=ax[x], hue='Churn')","b98751bc":"fig, ax = plt.subplots(ncols=2, nrows=2, figsize=(12,10))\nax[0,0].pie(df['gender'].value_counts(), autopct='%.2f%%', labels=df['gender'].unique())\nax[0,0].set_title('Telecom users by Gender')\nax[0,1].pie(df['SeniorCitizen'].value_counts(), autopct='%.2f%%', labels=['Non-Senior Citizen','Senior Citizer'])\nax[0,1].set_title('Telecom users by Gender')\nax[1,1].pie(df[df['SeniorCitizen']==1]['gender'].value_counts(), autopct='%.2f%%', labels=['Male','Female'])\nax[1,1].set_title('Out of Senior Citizer how many are Male & Female')\nax[1,0].pie(df[df['SeniorCitizen']==0]['gender'].value_counts(), autopct='%.2f%%', labels=['Male','Female'])\nax[1,0].set_title('Out of Non-Senior Citizer how many are Male & Female')\nplt.show()","2d93c818":"fig, ax=plt.subplots(ncols=3,nrows=1, figsize=(12,10))\nj=0\nfig.suptitle('Gender usage of Intenet types')\nfor i in df['InternetService'].unique():\n    ax[j].pie(df[df['InternetService']==i]['gender'].value_counts(),  labels=['Male','Female'], autopct='%.2f%%');\n    ax[j].set_title(i)\n    j=j+1","fe73fe54":"sns.countplot(x=df['InternetService'], hue=df['Churn'])","a36dd474":"sns.jointplot(data=df,x='MonthlyCharges',y='TotalCharges', kind='kde', hue='Churn')","1e625e72":"sns.scatterplot(data=df, x='MonthlyCharges', y='TotalCharges', hue='Churn', alpha=0.5)","4269d063":"sns.pairplot(df,hue='Churn', corner=True )","28c8cd1d":"sns.heatmap(df.corr(), annot=True)","acdf1e43":"# Feature engineering - convert the object features to integer based on the category\ndf=df.replace('Yes',1)\ndf=df.replace('No',0)\ndf=df.replace('No internet service',0)\ndf=df.replace('No phone service',0)\ndf=df.replace('Fiber optic',2)\ndf=df.replace('DSL',1)\ndf=df.replace('Male',1)\ndf=df.replace('Female',0)\ndf=pd.get_dummies(data=df, columns=['Contract','PaymentMethod'],drop_first=True )","2bab30e7":"df","bfbb1aa9":"#Segregate predictors vs target attributes \nX=df.drop(['Churn'], axis=1)\ny=df['Churn']","be803e49":"y.value_counts()","fa86efa9":"1587\/4389*100","3a5cb3c6":"sample_data=pd.concat([df[df['Churn']==0].sample(1587),df[df['Churn']==1]])\nX=sample_data.drop('Churn', axis=1)\ny=sample_data['Churn']","7ae59264":"y.value_counts()","3dcd0bc8":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)","549d1bc4":"X_train.describe()","0bce3863":"X_test.describe()","0e495e19":"print(f\"Train target variable: {y_train.value_counts()}\")\nprint(f\"Train target variable: {y_test.value_counts()}\")","a0d4a179":"from sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\nX_train = scale.fit_transform(X_train[int_feat])\nX_test = scale.transform(X_test[int_feat])","841bf5d3":"# Decision Tree model\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.tree import DecisionTreeClassifier\ndt_model = DecisionTreeClassifier(criterion = 'entropy' )\ndt_model.fit(X_train, y_train)\nprint(f\"Accuracy score for Train data: {dt_model.score(X_train , y_train)}\")\ntest_pred = dt_model.predict(X_test)\nprint(f\"Accuracy Score for Test data: {accuracy_score(y_test, test_pred)}\")","9456a585":"print(classification_report(y_test, test_pred))","b8ada210":"# Ensemble Techniques - Basic Models with weak classifiers\nfrom sklearn.ensemble import AdaBoostClassifier, BaggingClassifier,  GradientBoostingClassifier, RandomForestClassifier\nmodels = [RandomForestClassifier(), AdaBoostClassifier(), AdaBoostClassifier(base_estimator=dt_model),BaggingClassifier(), GradientBoostingClassifier()]\nfor model in models:\n    model.fit(X_train,y_train)\n    print(f\"{model} : Results\\n\")\n    print(f\"Accuracy score for Train data: {model.score(X_train , y_train)}\")\n    test_pred = model.predict(X_test)\n    print(f\"Accuracy Score for Test data: {accuracy_score(y_test, test_pred)}\\n\")\n    print(f\"Classification report \\n {classification_report(y_test,test_pred)}\\n\")\n    \n    ","61342363":"n_estimators=[60,70,80,90,100,110,120,130,140,150]\nfor i in n_estimators:\n    model = GradientBoostingClassifier(n_estimators=i, random_state=1)\n    model.fit(X_train,y_train)\n    print(f\"{model} no of estimators:{i} : Results\\n\")\n    print(f\"Accuracy score for Train data: {model.score(X_train , y_train)}\")\n    test_pred = model.predict(X_test)\n    print(f\"Accuracy Score for Test data: {accuracy_score(y_test, test_pred)}\\n\")","db2467f1":"model = GradientBoostingClassifier(n_estimators=100, random_state=1)\nmodel.fit(X_train,y_train)\nprint(f\"{model} no of estimators:{100} : Results\\n\")\nprint(f\"Accuracy score for Train data: {model.score(X_train , y_train)}\")\ntest_pred = model.predict(X_test)\nprint(f\"Accuracy Score for Test data: {accuracy_score(y_test, test_pred)}\\n\")","62f308af":"print(classification_report(y_test, test_pred))","01b423cc":"sns.heatmap(confusion_matrix(y_test, test_pred), annot=True, fmt='2d')","10159dcd":"import pickle\npickle.dump(model, open('model.sav','wb'))","769aa438":"### Observations:\n1. Genere: There is no much difference in Genere base customer. very few count differences.\n2. Senior Citizers: comparitively less count of senior citizen as customer. \n3. Partner - Feature contains more or less same size of counts.   \nLet do further analysis using multivariated analysis which helps for modeling","4513e753":"###  Perform detailed statistical analysis on the data","64a44e30":"The sample differnce in Churn and non churn for the categorical features have no much difference appart from the imbalance in the dataset","6a0837bc":"### Pickle the selected model for future use. ","2b6a4a02":"## 2. Data cleansing","71b94ffa":"#### Standard Scaler","5db8cfc1":"Here the accuracy score for the Train data is high whic is aroung 100%, where in Test data is low - around 66%. which means the model is overfitted on Training data.  \nThis is expected in terms of Tree based modeling as it tend to overfit on the Trianing data. \nSo let us try to use various techiniques to find which works best for this dataset","325da759":"### Missing value treatment ","9849bf8b":"## Multivariated Analysis","e7b0d5dc":"## 3. Data analysis & visualisation","583ebc98":"### Convert categorical attributes to continuous using relevant functional knowledge ","2712c25b":"People using Fiberoptics seems to have impact on customer Churn value. however, Hypothesis will help us to prove this right","559f4f3e":"There is no multicolunery seen in the dataset for numerical features","00009256":"### Contineous Features","c4ae0295":"#### Train test split","1ea8fc5b":"There are 10 null values in Total Charges after converted the value to Integer. this is because the value may contain the empty space in it. we can consider dropping the null value as the count is less than 1% of total size of dataset. ","b6d5aa5b":"# GUI development","1b4adaa8":"There when we check the outlier for whole data there is no much outliers.however, when we consider doing the multivariated analysis, there are some seen outliers for the samples between churn and non churn. Since the objective of the project is to use the Ensemble methods, the Ensemble methods have no impact on outliers. so, outlier treatment is not required here","8d3868f7":"## 1. Import and warehouse data:","aaea40d6":"## Model training, testing and tuning: ","d9d1a5fb":"**Feature Total Charges is contineous value. however it is shown as Objec (ie., \"string\"). we will check and update it accordingly**","54974923":"### Drop attribute\/s if required using relevant functional knowledge ","925513d2":"There is imbalance in Target variable with almost 64% data in 0 and 36% data in 1. Let us try to undersample it using sample option in Padas","c038cdfb":"#### Model with 100 estimators has high accuracy rate between both Training and test","938ba828":"### Decision Tree model - \n***Lets us try to use basic Decision Tree model to check the accuracy based on the dataset***","f4047ddf":"***Observations:***\n1. Model is giving us the accuracy score of around 80% on train data and 75% on test data. the output is comparitively good compared to other models. \n2. Precision and Recall score is good.\n3. There are definetly room for improvment on the score by using Oversampling techinique","07841795":"We can't see clear cluster or liner relationship in the datasets, hense Tree based algorithm might work well. ","f505c569":"There are lot of Yes\/No values, let us replace it with 1 or 0. assumption in bold\n\n    PhoneService - is the telephone service connected (Yes, No) - (1,0)\n    MultipleLines - are multiple phone lines connected (Yes, No, No phone service) - (1,0,0)\n    InternetService - client's Internet service provider (DSL, Fiber optic, No) -(2,1,0)\n    OnlineSecurity - is the online security service connected (Yes, No, No internet service)-(1,0,0)\n    OnlineBackup - is the online backup service activated (Yes, No, No internet service)-(1,0,0)\n    DeviceProtection - does the client have equipment insurance (Yes, No, No internet service)-(1,0,0)\n    TechSupport - is the technical support service connected (Yes, No, No internet service)-(1,0,0)\n    StreamingTV - is the streaming TV service connected (Yes, No, No internet service)-(1,0,0)\n    StreamingMovies - is the streaming cinema service activated (Yes, No, No internet service)-(1,0,0)\n    Contract - type of customer contract (Month-to-month, One year, Two year) - (month-to-month - 1, One Year - 12, Two Year = 24)\n    PaperlessBilling - whether the client uses paperless billing (Yes, No) - (1,0)\n    PaymentMethod - payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic))\n    MonthlyCharges - current monthly payment\n    TotalCharges - the total amount that the client paid for the services for the entire time\n    Churn - whether there was a churn (Yes or No) - (0,1)","e472fd98":"### Observations:\n1. Tenure has opposite effect on Churn \"Yes\" or \"No\". Churn customers have minimum tenure, where in no churn customers prefer longer tenure\n2. Monthly Charges &  Total charges have no visible difference apart from the imbalance in the data.","d4cab8de":"# Data pre-processing:","fcfc8115":"### Observations:\n1. Tenure is ranging between 1 to 70 year. most prefered tenure are less than 5 and 70 years.\n2. Monthly charges raning between 20 to 120. most customers are in 20-30 range\n3. Total Charges - most customers total charges are between 5000","ea932041":"#### Feature details\n\u2022 Customers who left within the last month \u2013 the column is called Churn   \n \u2022 Services that each customer has signed up for \u2013 phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies  \n \u2022 Customer account information \u2013 how long they\u2019ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges  \n \u2022 Demographic info about customers \u2013 gender, age range, and if they have partners and dependents ","c505f92a":"## Bivariated Analysis","9a820ec0":"## Univariated Analysis","ff2ccc53":"### Hypertuning the Gradient boosting techinque","62a1ed5c":"### Handling Imbalance in the dataset using Pandas sample function","77d3505d":"**No missing values found in the dataset**","f6bde954":"# <font color='Green'> Ensemble - Project <\/font>\n\n### Problem Statement:\nA telecom company wants to use their historical customer data to predict behaviour to retain customers. You can analyse all relevant customer data and develop focused customer retention programs. \n\n### Objective:\nBuild a model that will help to identify the potential customers who have a higher probability to churn. This help the company to understand the pinpoints and patterns of customer churn and will increase the focus on strategising customer retention. ","55eb2f62":"***Bias and Variance comparision:***\nGradeientBoostingClassifier seems to work better compared to other models. Bais & Variance of the model seems to be similar. other 3 models are seems to be overfitting. So, lets take GradientBoostingClassifier and fine tune with Hyperparameters.  \nAlso, the Recall and Precission values are more or less similar. f1 score is around 75% which tells us the Models works find on the dataset. Further fine tunining might help to improve the model accuracy."}}