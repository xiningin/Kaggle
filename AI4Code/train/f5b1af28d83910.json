{"cell_type":{"3dd99dbd":"code","9f758171":"code","307c6489":"code","f18828e3":"code","6b0244c8":"code","7d15ddd3":"code","16718d52":"code","7180fd15":"code","bd2728cc":"code","9077b9ed":"code","b8c266e2":"code","8862fb3c":"code","a0518271":"code","fa5640a9":"code","c05f7958":"code","8fba865c":"code","17590884":"code","1f0da0d8":"code","ea6f23de":"code","58d2083e":"code","9037422b":"code","64aad50b":"code","fa9dce70":"code","a7fe81ec":"code","1679693a":"code","32408889":"code","4d6805b1":"code","db62f3f9":"code","81ac3aef":"code","bb563fc2":"code","eb0f45a6":"code","fba0a772":"code","85f16c14":"code","75f056f5":"code","49e82036":"code","c8fefbd0":"code","ac56e7aa":"code","ef3927f0":"code","4c6a554f":"code","95d1c7c1":"code","5711b215":"code","1b583455":"code","556a4081":"code","29e7ee1e":"code","7c104d00":"code","7a385221":"code","2d85a2ab":"code","eb60e4c5":"code","8697f0a7":"code","a3265466":"code","360b8ab7":"code","a8557237":"code","7bee8b02":"code","2a8c4ac2":"code","5223958e":"code","1fcb90c5":"code","4ec78e82":"code","3b896b80":"code","3eae5d8c":"code","1f2cc0e7":"code","812f6833":"code","db84a438":"markdown","08a5964d":"markdown","5b86ab81":"markdown","fc7fc72a":"markdown","4309dd24":"markdown","55a0adb3":"markdown","fc963546":"markdown","436a44b6":"markdown","c59705f1":"markdown","21850772":"markdown","70023fbf":"markdown","fcaea3cb":"markdown","f65d3771":"markdown","c7c63127":"markdown","38752ff5":"markdown","5b38b1e9":"markdown","f889ae01":"markdown","f49e8be7":"markdown","6a998f63":"markdown","aa8807bb":"markdown"},"source":{"3dd99dbd":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","9f758171":"df = pd.read_csv('..\/input\/inverter-data-set\/Inverter Data Set.csv')\ndf.shape","307c6489":"df.head()","f18828e3":"#\u5217\u51fa\u6240\u6709\u6b04\u4f4d\u53ca\u5176\u8cc7\u6599\u985e\u578b\uff1a\u5168\u90e8\u90fd\u662ffloat\ndf.dtypes","6b0244c8":"#\u67e5\u770b\u7f3a\u5931\u503c\uff1a\ndef missing_values_table(df):\n    mis_val = df.isnull().sum()\n    #print(mis_val)\n    mis_val_percent = 100*mis_val\/len(df) \n    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n    mis_val_table_ren_columns=mis_val_table.rename(\n    columns = {0: 'Missing Values', 1: '% of total values'})\n    mis_val_table_ren_columns =mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:,1]!=0].sort_values('% of total values',ascending=False).round(1)\n    print('your selected has' +str(df.shape[1])+'columns.\\n' 'there are' + str(mis_val_table_ren_columns.shape[0])+ ' columns that have missing values')\n    \n    return mis_val_table_ren_columns","7d15ddd3":"df_misstb = missing_values_table(df)\n#-->\u6c92\u6709\u7f3a\u5931\u503c","16718d52":"df.shape","7180fd15":"df.hist(figsize=(17,17))","bd2728cc":"df['n_k'].describe()","9077b9ed":"#\u986f\u793a\u6240\u6709\u6b04\u4f4d\u548ctarget\uff08u_a\/b\/c_k-1\uff09\u7684\u76f8\u95dc\n#from kirgson's notebook\ncorr = df.corr()\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nplt.figure(figsize=(14,14))\n_ = sns.heatmap(corr, mask=mask, cmap=cmap, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","b8c266e2":"#\u5404\u680f\u4f4d\u548c\u76ee\u6807(target)\u7684\u76f8\u5173\u7cfb\u6570\u6d4f\u89c8\ntargets = ['u_a_k-1','u_b_k-1','u_c_k-1']\nfor i in targets:\n    df_corr = df.corr()[i].sort_values()\n\n    # print the strongest correlation coefficients(positive & negative)\n    print('strongest positive correlations with target: \\n', df_corr.tail(8))\n    print('strongest negative correlations with target: \\n', df_corr.head(8))","8862fb3c":"## rows\u6709\u6309\u7167\u6642\u9593\u9806\u5e8f\u6392\u5217\uff0cphase a\u7684d, i, u\u96a8\u8457\u6642\u9593k\u7684\u8b8a\u5316\u56fe\u3002\n# 2500 samples\/switch\np_sample= 5000\n\ncol_iak1=df['i_a_k-1'].head(p_sample)\np_iak1 = plt.subplot(4,1,1)\ncol_iak1.plot(use_index = True, figsize = (20,10), title='phase a')\nplt.setp(p_iak1.get_xticklabels(), visible=False)\np_iak1.set_ylabel('Phase Currents in A')\n\ncol_uak1 = df['u_a_k-1'].head(p_sample)\np_uak1 = plt.subplot(4,1,2)\ncol_uak1.plot(use_index = True)\nplt.setp(p_uak1.get_xticklabels(), visible=False)\np_uak1.set_ylabel('Mean Phase Voltages in V')\n\ncol_udck1 = df['u_dc_k-1'].head(p_sample)\np_udck1 = plt.subplot(4,1,3)\ncol_udck1.plot(use_index = True)\nplt.setp(p_udck1.get_xticklabels(), visible=False)\np_udck1.set_ylabel('DC-link voltage in V')\n\ncol_nk = df['n_k'].head(p_sample)\np_nk = plt.subplot(4,1,4)\ncol_nk.plot(use_index = True)\np_nk.set_ylabel('Speed')","a0518271":"import matplotlib.pyplot as plt\nimport pandas as pd\n%matplotlib inline","fa5640a9":"#\u4e09\u500b\u76f8\u4f4d\u4e4b\u9593\u7684currents\u4e4b\u9593\u7684\u95dc\u4fc2\u5716\uff1a\n# p_iabc = df[['i_a_k','i_b_k','i_c_k']].head(5000)\n# pd.plotting.scatter_matrix(p_iabc, alpha=0.2)\n#\u4e09\u500b\u76f8\u4f4d\u4e4b\u9593\u7684voltages\u4e4b\u9593\u7684\u95dc\u4fc2\u5716\uff1a\n# p_uabc = df[['u_a_k-1','u_b_k-1','u_c_k-1']].head(5000)\n# pd.plotting.scatter_matrix(p_uabc, alpha=0.2)","c05f7958":"# u, udc, n, d, i \u5169\u5169\u4e4b\u9593\u7684\u95dc\u4fc2\u5716\np_5 = df[['u_a_k-1','u_dc_k-1','i_a_k-1','n_k','d_a_k-2']].head(5000)\npd.plotting.scatter_matrix(p_5, alpha=0.2)","8fba865c":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split","17590884":"# Data Cutting\ndfx1 = df[['i_a_k','i_a_k-1','i_b_k','i_b_k-1','i_c_k','i_c_k-1','u_dc_k','u_dc_k-1','d_a_k-2','d_a_k-3','d_b_k-2','d_b_k-3','d_c_k-2','d_c_k-3']]\ndfy1 = df[['u_a_k-1','u_b_k-1','u_c_k-1','u_dc_k-2']]","1f0da0d8":"# dfx1.shape\ndfy1.shape","ea6f23de":"x_train1, x_test1, y_train1, y_test1 = train_test_split(dfx1, dfy1, test_size = 0.2)\nx_train1, x_val1, y_train1, y_val1 = train_test_split(x_train1, y_train1, test_size = 0.25)","58d2083e":"print(x_train1.shape, x_val1.shape, x_test1.shape, y_train1.shape, y_val1.shape, y_test1.shape)","9037422b":"# 3 targets y\ny_tr1a = y_train1['u_a_k-1']\ny_tr1b = y_train1['u_b_k-1']\ny_tr1c = y_train1['u_c_k-1']\n\ny_va1a = y_val1['u_a_k-1']\ny_va1b = y_val1['u_b_k-1']\ny_va1c = y_val1['u_c_k-1']\n\ny_te1a = y_test1['u_a_k-1']\ny_te1b = y_test1['u_b_k-1']\ny_te1c = y_test1['u_c_k-1']","64aad50b":"from sklearn import preprocessing","fa9dce70":"# Standardization\nscaler = preprocessing.StandardScaler()\n\nscaler.fit(x_train1)\nx_train1_std = scaler.transform(x_train1)\nx_val1_std = scaler.transform(x_val1)\nx_test1_std = scaler.transform(x_test1)","a7fe81ec":"# \u6aa2\u67e5\u78ba\u8a8d\u6a19\u6e96\u5316\u5f8c\u7684\u5747\u503c\nprint('mean of standardized test dataset', round(x_test1_std[:,5].mean()))\nprint('std of standardized test dataset', round(x_train1_std[:,8].std()))","1679693a":"x_train1.columns","32408889":"# fix\ndf00 = pd.DataFrame(x_test1, columns = x_train1.columns)","4d6805b1":"df00['udc'] = y_test1['u_dc_k-2']","db62f3f9":"df00","81ac3aef":"df00['pred00'] = df00['d_a_k-2']*df00['udc']","bb563fc2":"df00['true'] = pd.DataFrame(y_te1a)","eb0f45a6":"from sklearn.metrics import mean_squared_error\n\nmse1a = mean_squared_error(df00['true'] , df00['pred00'])\nmse1a","fba0a772":"from keras import models\nfrom keras import layers\n# nn1: 1 hidden layer\nnn1 = models.Sequential()\nnn1.add(layers.Dense(units=30, activation = 'relu', input_shape =(14,)))\nnn1.add(layers.Dense(units=30, activation = 'relu'))\nnn1.add(layers.Dense(units=1))\nnn1.compile(loss='mse', optimizer='Adam', metrics=['mse'])","85f16c14":"# nn2: k=5, u=3 (u=50: bad)\nnn2 = models.Sequential()\nnn2.add(layers.Dense(units=30, activation = 'relu', input_shape =(14,)))\nnn2.add(layers.Dense(units=30, activation = 'relu'))\nnn2.add(layers.Dense(units=30, activation = 'relu'))\nnn2.add(layers.Dense(units=30, activation = 'relu'))\nnn2.add(layers.Dense(units=30, activation = 'relu'))\nnn2.add(layers.Dense(units=30, activation = 'relu'))\nnn2.add(layers.Dense(units=1))\nnn2.compile(loss='mse', optimizer='Adam', metrics=['mse'])","75f056f5":"nnfit1 = nn1.fit(x_train1_std, y_tr1a, \n                    epochs=30, batch_size=500,\n                   validation_data=(x_val1_std, y_va1a))","49e82036":"# Training history Visualization\ntra_loss = nnfit1.history['loss']\nte_loss = nnfit1.history['val_loss']\n\nepoch_count = range(1, len(tra_loss)+1)\nplt.plot(epoch_count, tra_loss,'r--')\nplt.plot(epoch_count, te_loss, 'b-')\n\nplt.legend(['training loss','test loss'])\nplt.xlabel('epoch')\nplt.ylabel('loss')","c8fefbd0":"# nn2:\nnn2fit1 = nn2.fit(x_train1_std, y_tr1a, \n                    epochs=30, batch_size=500,\n                   validation_data=(x_val1_std, y_va1a))","ac56e7aa":"# Training history Visualization\ntra_loss = nnfit2.history['loss']\nte_loss = nnfit2.history['val_loss']\n\nepoch_count = range(1, len(tra_loss)+1)\nplt.plot(epoch_count, tra_loss,'r--')\nplt.plot(epoch_count, te_loss, 'b-')\n\nplt.legend(['training loss','test loss'])\nplt.xlabel('epoch')\nplt.ylabel('loss')","ef3927f0":"nnfit2 = nn2.fit(x_train1_std, y_tr1b, \n                    epochs=30, batch_size=500,\n                   validation_data=(x_test1_std, y_te1b))","4c6a554f":"tra_loss2 = nnfit2.history['loss']\nte_loss2 = nnfit2.history['val_loss']\nepoch_count = range(1, len(tra_loss2)+1)\nplt.plot(epoch_count, tra_loss2,'r--')\nplt.plot(epoch_count, te_loss2, 'b-')\n\nplt.legend(['training loss','test loss'])\nplt.xlabel('epoch')\nplt.ylabel('loss')","95d1c7c1":"nnfit3 = nn2.fit(x_train1_std, y_tr1c, \n                    epochs=30, batch_size=500,\n                   validation_data=(x_test1_std, y_te1c))","5711b215":"tra_loss3 = nnfit3.history['loss']\nte_loss3 = nnfit3.history['val_loss']\nepoch_count = range(1, len(tra_loss3)+1)\nplt.plot(epoch_count, tra_loss3,'r--')\nplt.plot(epoch_count, te_loss3, 'b-')\n\nplt.legend(['training loss','test loss'])\nplt.xlabel('epoch')\nplt.ylabel('loss')","1b583455":"from sklearn.ensemble import RandomForestRegressor","556a4081":"# time: 4m50s\nrfr = RandomForestRegressor(n_estimators=100)\nrfr.fit(x_train1_std, y_tr1a)","29e7ee1e":"rfrpred = rfr.predict(x_test1_std)","7c104d00":"from sklearn.metrics import mean_squared_error\nmserfr = mean_squared_error(y_te1a, rfrpred)\nmserfr","7a385221":"y_te1a","2d85a2ab":"rfrpred","eb60e4c5":"feat_importance = rfr.feature_importances_","8697f0a7":"x_colname = list(x_train1.columns)","a3265466":"feat_impor_df = pd.DataFrame({'feature': x_colname, 'importance': feat_importance})","360b8ab7":"feat_impor_df.sort_values('importance', ascending=False)","a8557237":"# cutting dataset\ndfx2 = df[['u_a_k-1','u_b_k-1','u_c_k-1','d_a_k-3','d_b_k-3','d_c_k-3','i_a_k-3','i_b_k-3','i_c_k-3','i_a_k-2','i_b_k-2','i_c_k-2','u_dc_k-3','u_dc_k-2']]\ndfy2 = df[['d_a_k-2','d_b_k-2','d_c_k-2']]\nx_train2, x_test2, y_train2, y_test2 = train_test_split(dfx2, dfy2, test_size = 0.3)","7bee8b02":"# x_train2, x_test2, y_train2, y_test2 = train_test_split(dfx2, dfy2, test_size = 0.2)\n# x_train2, x_val2, y_train2, y_val2 = train_test_split(x_train2, y_train2, test_size = 0.25)","2a8c4ac2":"print(x_train2.shape, x_test2.shape, y_train2.shape, y_test2.shape)","5223958e":"# 3 targets y\ny_tr2a = y_train2['d_a_k-2']\ny_tr2b = y_train2['d_b_k-2']\ny_tr2c = y_train2['d_c_k-2']\n\n# y_va1a = y_val1['u_a_k-1']\n# y_va1b = y_val1['u_b_k-1']\n# y_va1c = y_val1['u_c_k-1']\n\ny_te2a = y_test2['d_a_k-2']\ny_te2b = y_test2['d_b_k-2']\ny_te2c = y_test2['d_c_k-2']","1fcb90c5":"# Standardization\nscaler2 = preprocessing.StandardScaler()\n\nscaler2.fit(x_train2)\nx_train2_std = scaler2.transform(x_train2)\n# x_val2_std = scaler2.transform(x_val2)\nx_test2_std = scaler2.transform(x_test2)","4ec78e82":"# \u6aa2\u67e5\u78ba\u8a8d\u6a19\u6e96\u5316\u5f8c\u7684\u5747\u503c\nprint('mean of standardized test dataset', round(x_test2_std[:,5].mean()))\nprint('std of standardized test dataset', round(x_train2_std[:,8].std()))","3b896b80":"nnfit2a = nn2.fit(x_train2_std, y_tr2a,\n                  epochs=30, batch_size=300,\n                   validation_data=(x_test2_std, y_te2a))","3eae5d8c":"tra_loss2a = nnfit2a.history['loss']\nte_loss2a = nnfit2a.history['val_loss']\nepoch_count = range(1, len(tra_loss2a)+1)\nplt.plot(epoch_count, tra_loss2a,'r--')\nplt.plot(epoch_count, te_loss2a, 'b-')\n\nplt.legend(['training loss','test loss'])\nplt.xlabel('epoch')\nplt.ylabel('loss')","1f2cc0e7":"nnfit2b = nn2.fit(x_train2_std, y_tr2b,\n                  epochs=30, batch_size=300,\n                   validation_data=(x_test2_std, y_te2b))\n# loss\ntra_loss2b = nnfit2b.history['loss']\nte_loss2b = nnfit2b.history['val_loss']\nepoch_count = range(1, len(tra_loss2b)+1)\nplt.plot(epoch_count, tra_loss2b,'r--')\nplt.plot(epoch_count, te_loss2b, 'b-')\n\nplt.legend(['training loss','test loss'])\nplt.xlabel('epoch')\nplt.ylabel('loss')","812f6833":"nnfit2c = nn2.fit(x_train2_std, y_tr2c,\n                  epochs=30, batch_size=300,\n                   validation_data=(x_test2_std, y_te2c))\ntra_loss2c = nnfit2c.history['loss']\nte_loss2c = nnfit2c.history['val_loss']\nepoch_count = range(1, len(tra_loss2c)+1)\nplt.plot(epoch_count, tra_loss2c,'r--')\nplt.plot(epoch_count, te_loss2c, 'b-')\n\nplt.legend(['training loss','test loss'])\nplt.xlabel('epoch')\nplt.ylabel('loss')","db84a438":"## Model 1: Inverter Model \n#### Defined: Supervised - Regression problem\n#### 0. Ideal model(Baseline) 1. Neural Network; 2. Random Forest","08a5964d":"**3. 14 features --> target 3: d_c_k-2**","5b86ab81":"## Data Preprocessing & Visualization","fc7fc72a":"#### 1. 14 features --> target 1: d_a_k-2","4309dd24":"### Visualization\n#### Features in Phase a (head 5000 samples)","55a0adb3":"### Preprocessing: Dataset cutting + Standadization","fc963546":"### Understanding:\ndataset: (row 1_time k = row 2_time k-1 = row 3_time k-2)\n#### TASK: build 2 models (The inputs are without n_k, try and add it to see its contribution?)\uff1a\n#### 1. Black-box inverter model: (model of ideal inverter: u_x_k-1= d_x_k-2 * u_dc_k-2)\n#### inputs: d_a\/b\/c_k-3, d_a\/b\/c_k-2, i_a\/b\/c_k-1, i_a\/b\/c_k, u_dc_k-1,u_dc_k,\n#### targets: u_a\/b\/c_k-1\n#### 2. Black-box inverter compensation scheme:\n#### inputs: u_a\/b\/c_k-1, d_a\/b\/c_k-3, i_a\/b\/c_k-3, i_a\/b\/c_k-2, u_dc_k-3, u_dc_k-2,\n#### targets: d_a\/b\/c_k-2","436a44b6":"### 0. The Ideal Inverter Model : u_x_k-1 = d_x_k-2 * u_dc_k-2 (baseline)\n(phase a)","c59705f1":"### Standardization","21850772":"#### (1) 14features --> target1: u_a_k-1","70023fbf":"#### 2. 14 features --> target 2: d_b_k-2 ","fcaea3cb":"### Feature Engineering\ndomain knowledge...","f65d3771":"#### Build a Neural Network: 3 Layers","c7c63127":"### 2. RandomForest\n#### 14 features --> target: u_a_k-1","38752ff5":"### EDA -Correlation Coefficients:  between columns + between features and targets","5b38b1e9":"## Model 2: Inverter Compensation Scheme\n#### Defined: Supervised - Regression problem","f889ae01":"#### (2) 14features --> target2: u_b_k-1","f49e8be7":"### 1. Neural Network\n#### To Be Continued: try different units (10\/20\/30\/40\/50) and more hidden layers(2\/3\/4)","6a998f63":"#### (3) 14features --> target3: u_c_k-1","aa8807bb":"### 1. Neural Network\n#### To Be Continued: try different units (10\/20\/30\/40\/50) & k (1\/2\/3)"}}