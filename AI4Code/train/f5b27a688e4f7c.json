{"cell_type":{"8fe34381":"code","91d3a4db":"code","7acc6977":"code","e14bca87":"code","7a6452c7":"code","af7f5bb3":"code","eaf1cc1d":"code","302ef14c":"code","3c00a48d":"code","7f5a98a2":"code","d69d20c9":"code","f2f62df6":"code","50f1ba33":"code","d2e8bad4":"code","2d475bf3":"code","02a70055":"code","945ff0c3":"code","81c0dede":"code","f7d5eaaa":"code","3505c35d":"code","ff99f435":"code","62f031a2":"code","db83418b":"code","449934f4":"code","cf7704ca":"code","9639d1d2":"code","df8ac580":"code","db2eb181":"code","2d55bb95":"code","f3ca4ede":"code","2066f81c":"code","234a31af":"code","3502a5f8":"code","ff2ddb40":"code","de89e426":"code","785c2768":"code","e7eb6fe5":"code","366a30f7":"code","752a9cc5":"code","96a3a72d":"code","5c4b8291":"code","fb446a7b":"code","fa580c93":"code","47f57051":"code","09ca5a2d":"code","a4a54662":"code","ca16d588":"code","2099e1fb":"code","7188ece8":"code","648b8259":"code","a4607d78":"code","7c099af2":"code","443b7470":"code","162bcb6a":"code","32723173":"code","948ee457":"code","9f3a0f0a":"code","0552b925":"code","f2501919":"code","fd2c71be":"code","9b505899":"code","379805d5":"code","2d60e91b":"code","ac5d6177":"code","2aaf02f2":"code","7b9a8d0f":"code","fa70ff3e":"code","9761bc23":"code","6707f5fb":"code","6b32873d":"code","7904c930":"code","a0fb79f5":"code","055994a5":"code","0462f116":"code","99f52427":"code","46bb0fa6":"code","cfe319e6":"code","4a429395":"code","4847228b":"code","aed70d47":"code","fbd42451":"code","df92b426":"code","a55535ab":"code","25167828":"code","34d52270":"code","4e3f3193":"code","71cbc6ad":"code","273f47b9":"code","b9de9be5":"code","4353a408":"code","93cb1333":"code","ac45ee00":"code","67956b3a":"code","14996b58":"code","e2cbe5c4":"code","a35cc98b":"code","b3ad6c5d":"code","e47e443b":"code","d45e9992":"code","5297f599":"code","bb8b8ca6":"code","53dc8157":"code","1934ac1f":"code","21e7d603":"code","5fbfaf68":"code","1a3c2bcc":"code","bd4bdd5b":"code","79c2b53e":"code","94032376":"code","e2b9fe02":"code","183a1667":"code","8821d09d":"code","f76e6e86":"code","01dce495":"code","8b9ed0fb":"code","51b2c774":"code","22c78629":"code","8e5188bf":"code","8c19e4e8":"markdown","6df51792":"markdown","2d6cb36c":"markdown","f7deb60e":"markdown","01755646":"markdown","9c886bc6":"markdown","ce930c44":"markdown","c5924e34":"markdown","dad2d4e9":"markdown","27efad73":"markdown","ca9ee0e4":"markdown","eec995a4":"markdown","43c417bd":"markdown","b39edab9":"markdown"},"source":{"8fe34381":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","91d3a4db":"# Import related libraries\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport gc\nimport psutil\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelEncoder,StandardScaler\nfrom sklearn.decomposition import PCA, KernelPCA\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import (confusion_matrix,accuracy_score,precision_score,recall_score,f1_score,\n                             make_scorer,classification_report,roc_auc_score,roc_curve,\n                             average_precision_score,precision_recall_curve)\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,VotingClassifier\n\nfrom imblearn.pipeline import make_pipeline\nfrom imblearn.under_sampling import OneSidedSelection\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.under_sampling import EditedNearestNeighbours\nfrom imblearn.under_sampling import TomekLinks\nfrom imblearn.under_sampling import RandomUnderSampler\n\npd.set_option('display.max_columns', None)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nRANDOM_SEED = 101\n\nimport collections\nfrom mpl_toolkits import mplot3d","7acc6977":"sub_file = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\nsub_file.head()","e14bca87":"\n\ntrain = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain.head()\n\n","7a6452c7":"val = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nval.head()","af7f5bb3":"\n\ntrain.columns\n\n","eaf1cc1d":"val.columns\n","302ef14c":"train['Survived'].value_counts().plot.bar()","3c00a48d":"\n\ntrain.info()\n\n","7f5a98a2":"train.isnull().mean()","d69d20c9":"\n\ntrain.shape\n\n","f2f62df6":"train.describe()","50f1ba33":"target = 'Survived'","d2e8bad4":"\n\n\"Braund, Mr. Owen Harris\".split(',')[1].split()[0][:-1]\n\n","2d475bf3":"\n\ntrain[\"Name\"].map(lambda x: x.split(',')[1].split()[0][:-1]).value_counts().plot.bar()\n\n","02a70055":"val[\"Name\"].map(lambda x: x.split(',')[1].split()[0][:-1]).value_counts().plot.bar()\n","945ff0c3":"def get_salutation_map(df,var,rare):\n    sal_dict = {}\n    for sal, count in df[var].value_counts().to_dict().items():\n        count = int(count)\n        if count < 10:\n            sal_dict[sal] = rare\n        else:\n            sal_dict[sal] = sal\n    return sal_dict","81c0dede":"train[\"Salutation\"] = train[\"Name\"].map(lambda x: x.split(',')[1].split()[0][:-1])\n","f7d5eaaa":"train.head()","3505c35d":"get_salutation_map(train,\"Salutation\",\"Rare\")","ff99f435":"train[\"Salutation\"] = train[\"Name\"].map(lambda x: x.split(',')[1].split()[0][:-1])\ntrain[\"Salutation\"] = train[\"Salutation\"].map(get_salutation_map(train,'Salutation','Rare'))\ntrain.head(2)\n","62f031a2":"train[\"Salutation\"].value_counts().plot.bar()","db83418b":"\n\nsns.countplot(x=\"Salutation\",data=train)\n\n","449934f4":"train.head()\n","cf7704ca":"sns.boxplot(y = 'Age',\n            x = 'Salutation', \n            data = train)\nplt.xlabel('Saluation')\nplt.ylabel('Age')\nplt.title('Distribution of Age with respect to Saluations', fontsize = 10)","9639d1d2":"sns.boxplot(y = 'Fare',\n            x = 'Salutation', \n            data = train)\nplt.xlabel('Saluation')\nplt.ylabel('Fare')\nplt.title('Distribution of Fare with respect to Saluations', fontsize = 10)","df8ac580":"\n\ntrain['SibSp'].unique()\n\n","db2eb181":"\n\ntrain['SibSp'].nunique()\n\n","2d55bb95":"train['SibSp'].value_counts().plot.bar()","f3ca4ede":"train['Parch'].unique()","2066f81c":"\n\ntrain['Parch'].value_counts().plot.bar()\n\n","234a31af":"train[\"Family_Size\"] = train[\"SibSp\"] + train[\"Parch\"]\ntrain[\"Family_Size\"].unique()\n","3502a5f8":"\n\n(train[\"Family_Size\"].value_counts(normalize=True)*100).plot.bar()\n\n","ff2ddb40":"train[\"Family_Size\"].value_counts(normalize=True)*100","de89e426":"def get_family_size_map(df,var):\n    fam_dict = {}\n    for size, pct in (df[var].value_counts(normalize=True)*100).to_dict().items():\n        if size == 0:\n            fam_dict[size] = \"Alone\"\n        elif (size != 0) & (pct > 10.0):\n            fam_dict[size] = \"Small\"\n        else:\n            fam_dict[size] = \"Large\"\n    return fam_dict","785c2768":"\n\ntrain[\"Family_Size\"] = train[\"Family_Size\"].map(get_family_size_map(train,'Family_Size'))\ntrain.head(2)\n\n","e7eb6fe5":"sns.boxplot(y = 'Age',\n            x = 'Family_Size', \n            data = train)\nplt.xlabel('Family_Size')\nplt.ylabel('Age')\nplt.title('Distribution of Age with respect to Family_Size', fontsize = 10)","366a30f7":"sns.boxplot(y = 'Age',\n            x = 'Family_Size', \n            hue = 'Pclass',\n            data = train)\nplt.xlabel('Family_Size')\nplt.ylabel('Age')\nplt.title('Distribution of Age with respect to Family_Size', fontsize = 10)","752a9cc5":"sns.boxplot(y = 'Fare',\n            x = 'Family_Size', \n            data = train)\nplt.xlabel('Family_Size')\nplt.ylabel('Fare')\nplt.title('Distribution of Fare with respect to Family_Size', fontsize = 10)","96a3a72d":"train.isnull().mean()\n","5c4b8291":"\n\ntrain['had_Cabin'] = np.where(train['Cabin'].isna(),0,1)\n\n","fb446a7b":"train.head()\n","fa580c93":"train['Cabin'].dropna().map(lambda x:x[0]).value_counts()\n","47f57051":"train['Cabin'] = train['Cabin'].fillna(\"M\")\ntrain['Cabin'] = train['Cabin'].map(lambda x: x[0])\n","09ca5a2d":"\n\ntrain.head()\n\n","a4a54662":"sns.boxplot(y = 'Age',\n            x = 'Cabin',\n            data = train)\nplt.xlabel('Cabin')\nplt.ylabel('Age')\nplt.title('Distribution of Age with respect to Cabin', fontsize = 10)","ca16d588":"sns.boxplot(y = 'Cabin',\n            x = 'Fare',\n            data = train)\nplt.xlabel('had_Cabin')\nplt.ylabel('Fare')\nplt.title('Distribution of Fare with respect to had_Cabin', fontsize = 10)","2099e1fb":"\n\ntrain.head()\n\n","7188ece8":"train.groupby(['Salutation','had_Cabin'])","648b8259":"mean_dict = {}\nfor k, df in train.groupby(['Salutation','Family_Size','had_Cabin']):\n    if df['Age'].isnull().sum() != 0:\n        mean_dict[k] = df[\"Age\"].mean()\nmean_dict","a4607d78":"for k,v in mean_dict.items():\n    train.loc[(train[\"Salutation\"] == k[0]) & (train[\"Family_Size\"] == k[1]) & (train[\"had_Cabin\"] == k[2]) & (train[\"Age\"].isna()), \"Age\"] = v","7c099af2":"train['Embarked'].value_counts()","443b7470":"train['Embarked'] = train['Embarked'].fillna(train['Embarked'].mode().values[0])","162bcb6a":"train.isnull().sum()","32723173":"\n\ntrain.head()\n\n","948ee457":"\n\nnum_cols = ['Age','Fare']\ncat_cols = ['Pclass','Sex','Embarked','Cabin','had_Cabin','Salutation','Family_Size']\n\n","9f3a0f0a":"for col in num_cols:\n    fig = plt.figure(figsize = (10,5))\n    ax = fig.add_subplot(111)\n    ax = sns.distplot(train[col], color=\"m\", label=\"Skewness : %.2f\"%(train[col].skew()))\n    ax.set_xlabel(col)\n    ax.set_ylabel(\"Frequency\")\n    ax.legend(loc='best')\n    ax.set_title('Frequency Distribution of {}'.format(col), fontsize = 15)","0552b925":"fig = plt.figure(figsize = (50,15))\nj = 1\nfor cat_col in cat_cols:\n    ax = fig.add_subplot(1,len(cat_cols),j)\n    sns.countplot(x = cat_col,\n                  data = train,\n                  ax = ax)\n    ax.set_xlabel(cat_col)\n    ax.set_ylabel(\"Frequency\")\n    ax.set_title('Frequency Distribution for individual classes in {}'.format(cat_col), fontsize = 10)\n    j = j + 1\n","f2501919":"sns.pairplot(train[num_cols])","fd2c71be":"for col in num_cols:\n    fig = plt.figure(figsize = (15,4))\n    ax = fig.add_subplot(111)\n    j = 0\n    for key, df in train.groupby([target]):\n        ax = sns.kdeplot(df[col], shade = True, label=key)\n        ax.set_xlabel(col)\n        ax.set_ylabel(\"Frequency\")\n        ax.legend(loc=\"best\")\n        fig.suptitle('Frequency Distribution of {}'.format(col), fontsize = 10)\n        j = j + 1\n","9b505899":"for col in num_cols:\n    fig = plt.figure(figsize = (15,4))\n    j = 1\n    for key, df in train.groupby([target]):\n        ax = fig.add_subplot(1,train[target].nunique(),j)\n        ax = sns.distplot(df[col], label=\"Skewness : %.2f\"%(df[col].skew()))\n        ax.set_xlabel(key)\n        ax.set_ylabel(\"Frequency\")\n        ax.legend(loc=\"best\")\n        fig.suptitle('Frequency Distribution of {}'.format(col), fontsize = 10)\n        j = j + 1\n","379805d5":"for num_col in num_cols:\n    fig = plt.figure(figsize = (30,10))\n    j = 1\n    for cat_col in cat_cols:\n        ax = fig.add_subplot(1,len(cat_cols),j)\n        sns.boxplot(y = train[num_col],\n                    x = train[cat_col], \n                    data = train, \n                    ax = ax)\n        ax.set_xlabel(cat_col)\n        ax.set_ylabel(num_col)\n        ax.set_title('Distribution of {} with respect to {}'.format(num_col,cat_col), fontsize = 10)\n        j = j + 1","2d60e91b":"for num_col in num_cols:\n    fig = plt.figure(figsize = (30,10))\n    j = 1\n    for cat_col in cat_cols:\n        ax = fig.add_subplot(1,len(cat_cols),j)\n        sns.boxplot(y = train[num_col],\n                    x = train[cat_col],\n                    hue = target,\n                    data = train, \n                    ax = ax)\n        ax.set_xlabel(cat_col)\n        ax.set_ylabel(num_col)\n        ax.set_title('Distribution of {} with respect to {}'.format(num_col,cat_col), fontsize = 10)\n        j = j + 1\n","ac5d6177":"train_data = pd.get_dummies(train,columns=cat_cols,drop_first=True)\ntrain_data.head(2)","2aaf02f2":"explore_data, validation_data = train_test_split(train_data, test_size = 0.2, random_state=RANDOM_SEED, stratify=train[target])","7b9a8d0f":"train_data, test_data = train_test_split(explore_data, test_size = 0.2, random_state=RANDOM_SEED)\n","fa70ff3e":"def handle_outliers_per_target_class(df,var,target,tol):\n    gdf = df[df[target] == 1]\n    var_data = gdf[var].values\n    q25, q75 = np.percentile(var_data, 25), np.percentile(var_data, 75)\n    \n    print('Outliers handling for {}'.format(var))\n    print('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\n    \n    iqr = q75 - q25\n    print('IQR {}'.format(iqr))\n    \n    cut_off = iqr * tol\n    lower, upper = q25 - cut_off, q75 + cut_off\n    \n    print('Cut Off: {}'.format(cut_off))\n    print('{} Lower: {}'.format(var,lower))\n    print('{} Upper: {}'.format(var,upper))\n    \n    outliers = [x for x in var_data if x < lower or x > upper]\n\n    print('Number of Outliers in feature {} in {}: {}'.format(var,key,len(outliers)))\n\n    print('{} outliers:{}'.format(var,outliers))\n\n    print('----' * 25)\n    print('\\n')\n    print('\\n')\n        \n    return list(df[(df[var] > upper) | (df[var] < lower)].index)","9761bc23":"outliers_wrt_target = []\nfor num_col in num_cols:\n    outliers_wrt_target.extend(handle_outliers_per_target_class(train_data,num_col,target,1.5))\noutliers_wrt_target = list(set(outliers_wrt_target))\n\ntrain_data = train_data.drop(outliers_wrt_target)\n","6707f5fb":"train_data[\"Fare\"] = np.where(train_data[\"Fare\"] != 0,np.log(train_data[\"Fare\"]),np.log(0.00001))\ntest_data[\"Fare\"] = np.where(test_data[\"Fare\"] != 0,np.log(test_data[\"Fare\"]),np.log(0.00001))\nvalidation_data[\"Fare\"] = np.where(validation_data[\"Fare\"] != 0,np.log(validation_data[\"Fare\"]),np.log(0.00001))\n","6b32873d":"X_train = train_data.drop(['PassengerId', 'Survived', 'Name', 'SibSp', 'Parch', 'Ticket'],axis=1)\ny_train = train_data[target]","7904c930":"X_test = test_data.drop(['PassengerId', 'Survived', 'Name', 'SibSp', 'Parch', 'Ticket'],axis=1)\ny_test = test_data[target]\n","a0fb79f5":"X_val = validation_data.drop(['PassengerId', 'Survived', 'Name', 'SibSp', 'Parch', 'Ticket'],axis=1)\ny_val = validation_data[target]","055994a5":"y_enc = LabelEncoder()\ny_train = y_enc.fit_transform(y_train)\ny_test = y_enc.transform(y_test)\ny_val = y_enc.transform(y_val)\n","0462f116":"X_train.head()","99f52427":"\n\nsc = StandardScaler()\nX_train[num_cols] = sc.fit_transform(X_train[num_cols])\nX_test[num_cols] = sc.transform(X_test[num_cols])\nX_val[num_cols] = sc.transform(X_val[num_cols])\n\n","46bb0fa6":"\n\nsc.mean_\n\n","cfe319e6":"\n\nsc.var_\n\n","4a429395":"X_train.shape","4847228b":"\n\nclf = LogisticRegression()\n\n","aed70d47":"clf.fit(X_train,y_train)","fbd42451":"\n\nclf.intercept_\n\n","df92b426":"y_pred = clf.predict(X_test)\n","a55535ab":"y_pred\n","25167828":"\n\nconfusion_matrix(y_test,y_pred)\n\n","34d52270":"accuracy_score(y_test,y_pred)","4e3f3193":"classification_models = ['LogisticRegression',\n                         'SVC',\n                         'DecisionTreeClassifier',\n                         'RandomForestClassifier',\n                         'AdaBoostClassifier']","71cbc6ad":"cm = []\nacc = []\nprec = []\nrec = []\nf1 = []\nmodels = []\nestimators = []","273f47b9":"for classfication_model in classification_models:\n    \n    model = eval(classfication_model)()\n    \n    model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    \n    models.append(type(model).__name__)\n    estimators.append((type(model).__name__,model))\n    cm.append(confusion_matrix(y_test,y_pred))\n    acc.append(accuracy_score(y_test,y_pred))\n    prec.append(precision_score(y_test,y_pred))\n    rec.append(recall_score(y_test,y_pred))\n    f1.append(f1_score(y_test,y_pred))","b9de9be5":"vc = VotingClassifier(estimators)\nvc.fit(X_train,y_train)","4353a408":"\n\ny_pred = vc.predict(X_test)\n    \nmodels.append(type(vc).__name__)\n\ncm.append(confusion_matrix(y_test,y_pred))\nacc.append(accuracy_score(y_test,y_pred))\nprec.append(precision_score(y_test,y_pred))\nrec.append(recall_score(y_test,y_pred))\nf1.append(f1_score(y_test,y_pred))\n\n","93cb1333":"model_dict = {\"Models\":models,\n             \"CM\":cm,\n             \"Accuracy\":acc,\n             \"Precision\":prec,\n             \"Recall\":rec,\n             \"f1_score\":f1}","ac45ee00":"model_df = pd.DataFrame(model_dict)\nmodel_df","67956b3a":"model_df.sort_values(by=['Accuracy','f1_score','Recall','Precision'],ascending=False,inplace=True)\nmodel_df\n","14996b58":"\n\nmodel_param_grid = {}\n\n","e2cbe5c4":"\n\nmodel_param_grid['LogisticRegression'] = {'penalty' : ['l1', 'l2'],\n                                          'C' : np.logspace(0, 4, 10)}\n\n","a35cc98b":"model_param_grid['SVC'] = [{'kernel': ['rbf'], \n                            'gamma': [1e-2, 1e-3, 1e-4, 1e-5],\n                            'C': [0.001, 0.10, 0.1, 10, 25, 50, 100, 1000]},\n                           {'kernel': ['sigmoid'],\n                            'gamma': [1e-2, 1e-3, 1e-4, 1e-5],\n                            'C': [0.001, 0.10, 0.1, 10, 25, 50, 100, 1000]},\n                           {'kernel': ['linear'], \n                            'C': [0.001, 0.10, 0.1, 10, 25, 50, 100, 1000]},\n                           {'kernel': ['poly'], \n                            'degree' : [0, 1, 2, 3, 4, 5, 6]}\n                          ]","b3ad6c5d":"model_param_grid['DecisionTreeClassifier'] = {'criterion' : [\"gini\",\"entropy\"],\n                                              'max_features': ['auto', 'sqrt', 'log2'],\n                                              'min_samples_split': [10,11,12,13,14,15],\n                                              'min_samples_leaf':[1,2,3,4,5,6,7]}","e47e443b":"model_param_grid['RandomForestClassifier'] = {'n_estimators' : [50,100,150,200],\n                                              'criterion' : [\"gini\",\"entropy\"],\n                                              'max_features': ['auto', 'sqrt', 'log2'],\n                                              'class_weight' : [\"balanced\", \"balanced_subsample\"]}","d45e9992":"model_param_grid['AdaBoostClassifier'] = {'n_estimators' : [25,50,75,100],\n                                          'learning_rate' : [0.001,0.01,0.05,0.1,1,10],\n                                          'algorithm' : ['SAMME', 'SAMME.R']}","5297f599":"from sklearn.model_selection import GridSearchCV\ndef tune_parameters(model_name,model,params,cv,scorer,X,y):\n    best_model = GridSearchCV(estimator = model,\n                              param_grid = params,\n                              scoring = scorer,\n                              cv = cv,\n                              n_jobs = -1).fit(X, y)\n    print(\"Tuning Results for \", model_name)\n    print(\"Best Score Achieved: \",best_model.best_score_)\n    print(\"Best Parameters Used: \",best_model.best_params_)\n    return best_model.best_estimator_\n","bb8b8ca6":"\n\nfrom sklearn.metrics import make_scorer\n\n# Define scorer\ndef roc_metric(y_test, y_pred):\n    score = roc_auc_score(y_test, y_pred)\n    return score\n\n","53dc8157":"\n\n# Scorer function would try to maximize calculated metric\nroc_scorer = make_scorer(roc_metric,greater_is_better=True)\n\n","1934ac1f":"best_estimators = []\n","21e7d603":"for m_name, m_obj in estimators:\n    best_estimators.append((m_name,tune_parameters(m_name,\n                                                   m_obj,\n                                                   model_param_grid[m_name],\n                                                   10,\n                                                   roc_scorer,\n                                                   X_train,\n                                                   y_train)))","5fbfaf68":"best_estimators","1a3c2bcc":"tuned_vc = VotingClassifier(best_estimators)\ntuned_vc.fit(X_train,y_train)\n","bd4bdd5b":"y_pred = tuned_vc.predict(X_test)\n","79c2b53e":"confusion_matrix(y_test,y_pred)","94032376":"\n\naccuracy_score(y_test,y_pred)\n\n","e2b9fe02":"precision_score(y_test,y_pred)","183a1667":"recall_score(y_test,y_pred)","8821d09d":"f1_score(y_test,y_pred)","f76e6e86":"# Importing the Keras libraries and packages\nimport keras\nfrom keras.utils import plot_model\nfrom keras.models import Model,Sequential,load_model\nfrom keras.layers import Input, Flatten, Dense, Dropout\nfrom keras.layers.merge import concatenate\nfrom keras import backend as K\nfrom keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau","01dce495":"\n\ndef nn_model(X,y,optimizer,kernels):\n    input_shape = X.shape[1]\n       \n    if(len(np.unique(y)) == 2):\n        op_neurons = 1\n        op_activation = 'sigmoid'\n        loss = 'binary_crossentropy'\n    else:\n        op_neurons = len(np.unique(y))\n        op_activation = 'softmax'\n        loss = 'categorical_crossentropy'\n    \n    classifier = Sequential()\n    classifier.add(Dense(units = input_shape,\n                         kernel_initializer = kernels,\n                         activation = 'relu',\n                         input_dim = input_shape))\n    classifier.add(Dense(units = 8,\n                         kernel_initializer = kernels,\n                         activation = 'relu'))\n    classifier.add(Dense(units = 4,\n                         kernel_initializer = kernels,\n                         activation = 'relu'))\n    classifier.add(Dropout(rate = 0.25))\n    classifier.add(Dense(units = op_neurons,\n                         kernel_initializer = kernels,\n                         activation = op_activation))\n    \n    classifier.compile(optimizer = optimizer,\n                       loss = loss,\n                       metrics = ['accuracy'])\n    \n    classifier.summary()\n    return classifier","8b9ed0fb":"\n\nmodel = nn_model(X_train,y_train,'adam','he_uniform')\nhistory = model.fit(X_train,\n                    y_train,\n                    batch_size = 64,\n                    epochs = 1000,\n                    validation_data=(X_test, y_test))","51b2c774":"his_df = pd.DataFrame(history.history)\nhis_df.shape","22c78629":"plt.plot(his_df['loss'])\nplt.plot(his_df['val_loss'])\nplt.title(\"Loss Plot\")\nplt.legend([\"train\",\"test\"])","8e5188bf":"plt.plot(his_df['accuracy'])\nplt.plot(his_df['val_accuracy'])\nplt.title(\"Accuracy Plot\")\nplt.legend([\"train\",\"test\"])","8c19e4e8":"****Define custom Scorer function****","6df51792":"# Exploratory Data Analysis","2d6cb36c":"****Stacking Ensemble****","f7deb60e":"****Data Transformations****","01755646":"> ****Baseline models****","9c886bc6":"****Implementing Neural Network****","ce930c44":"****Run iterations for all the trained baseline models****","c5924e34":"****Hyper parameter Tuning****","dad2d4e9":"****Function to perform Grid Search with Cross Validation****","27efad73":"****Bivariate Analysis****","ca9ee0e4":" ****Feature Engineering****","eec995a4":"****Classification****","43c417bd":"****Univariate Analysis****","b39edab9":"# Modelling"}}