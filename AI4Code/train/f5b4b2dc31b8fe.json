{"cell_type":{"00948834":"code","29f7b1ca":"code","5e21cbd1":"code","9cb20881":"code","e3d2208a":"code","5ebfc73d":"code","501391db":"code","3ea56545":"code","d61e261b":"code","df18581a":"code","75010968":"code","1d1d65ba":"code","0b0cf113":"code","fd57f3e9":"code","c0b0d320":"code","65d4bd35":"code","77a11991":"code","8c2cc0f6":"code","ed4e8a78":"code","b931c74a":"code","1f55dafd":"code","4bb4c718":"code","93055360":"code","611cecd4":"code","e3a5b7af":"code","c4152c3c":"code","f97f2a28":"code","f4255cd2":"code","aa4fe093":"code","1073aaf4":"code","a24d13f0":"code","9809c677":"code","7fe1ef44":"code","e98fd4ba":"code","b66ab1d1":"code","e69a3b06":"code","9c9852c5":"code","40b90b31":"code","6a491554":"code","62feb869":"code","c96bca7d":"code","9d9ba80b":"markdown","6644e80f":"markdown","82d45d27":"markdown","7d2a0b21":"markdown","6361a159":"markdown","93084413":"markdown","0564b9d3":"markdown","c9e2f3c3":"markdown","3fb06d88":"markdown","fa168da8":"markdown","f63c9b63":"markdown","510c93d0":"markdown","3ff2fdf8":"markdown","5c524725":"markdown","80500936":"markdown","2d8fa33c":"markdown","f947ac67":"markdown","3e61e131":"markdown","0adba2c7":"markdown","0e8acb67":"markdown","d687f8c4":"markdown","597c5b09":"markdown","4902fb90":"markdown"},"source":{"00948834":"!ls ..\/input\/male-daan-schnell-mal-klassifizieren\/train.csv","29f7b1ca":"import numpy as np\nimport pandas as pd\npd.read_csv('..\/input\/male-daan-schnell-mal-klassifizieren\/train.csv').head()","5e21cbd1":"np.genfromtxt?\nmy_data = np.genfromtxt('..\/input\/male-daan-schnell-mal-klassifizieren\/train.csv', delimiter=',',skip_header=1)","9cb20881":"df = pd.read_csv('..\/input\/male-daan-schnell-mal-klassifizieren\/train.csv',index_col='Id')\ndf.head()","e3d2208a":"#.values enth\u00e4lt den Numpy-Array:\nX = df[['X1','X2']].values \ny = df.y.values","5ebfc73d":"X","501391db":"from sklearn.model_selection import train_test_split\ntrain_test_split?","3ea56545":"Xtrain,Xtest,ytrain,ytest=train_test_split(X,y,train_size=0.8,random_state=42)\nXtrain.shape,Xtest.shape","d61e261b":"class myKNN(object):\n    def __init__(self,k):\n        self.k=k","df18581a":"class myKNN(object):\n    def __init__(self,k):\n        self.k=k\n        \n    def fit(self,Xtrain,ytrain):\n        self.X=Xtrain\n        self.y=ytrain\n        \n    def visualize_dataset(self):\n        pass","75010968":"import matplotlib.pyplot as plt\nplt.scatter(Xtrain[:,0],Xtrain[:,1],c=ytrain,s=0.5);\nplt.colorbar();","1d1d65ba":"class myKNN(object):\n    def __init__(self,k):\n        self.k=k\n        \n    def fit(self,Xtrain,ytrain):\n        self.X=Xtrain\n        self.y=ytrain\n        \n    def visualize_dataset(self):\n        pass","0b0cf113":"clf=myKNN(7)\nclf.fit(Xtrain,ytrain)\nclf.visualize_dataset() #ups, da passiert noch nichts!","fd57f3e9":"myXtrain = Xtrain[:5,:]\nmyytrain = ytrain[:5]\n\ndisplay(myXtrain)\nmyytrain","c0b0d320":"#dies ist unser mit kNN zu klassifizierende Datenpunkt:\nmyXtest = Xtrain[6,:]\nmyXtest","65d4bd35":"#Da nur eine Dimension nicht \u00fcbereinstimmt, wird der kleinere Array \n#entlang der nicht \u00fcbereinstimmenden Dimension repliziert:\nmyXtrain.shape,myXtest.shape","77a11991":"#wir subtrahieren komponentenweise:\ndisplay(myXtrain-myXtest)\n#wir quadrieren die Differenzen der Komponenten:\ndisplay((myXtrain-myXtest)**2)\n#wir summieren die Differenzquadrate entlang der Zeilen, also der Achse 1:\ndisplay(np.sum((myXtrain-myXtest)**2,axis=1))\n#wir ziehen die Wurzel, um die Distanzen (anstatt der Distanzen im Quadrat) zu erhalten:\nnp.sqrt(np.sum((myXtrain-myXtest)**2,axis=1))","8c2cc0f6":"#Damit ergeben sich die Abst\u00e4nde zu:\ndistanzen=np.sqrt(np.sum((myXtrain-myXtest)**2,axis=1))\ndistanzen.shape","ed4e8a78":"min_index = np.argmin(distanzen)\nprint(f'Die {min_index+1}-te Zeile enth\u00e4lt das Minimum. Dieses betr\u00e4gt {distanzen[min_index]:1.3f}.')","b931c74a":"k=3","1f55dafd":"#np.argsort? #uncomment me!","4bb4c718":"sortierte_indices = np.argsort(distanzen)\nsortierte_indices","93055360":"#offenbar sortiert np.argsort aufsteigend- genau so, wie wir es brauchen:\ndistanzen[sortierte_indices]","611cecd4":"display(myXtrain[sortierte_indices[:k]])\n#und viel wichtiger- die Klassenzugeh\u00f6rigkeiten dieser k n\u00e4chsten Nachbarn!\nnearestLabels = myytrain[sortierte_indices[:k]]\nnearestLabels","e3a5b7af":"#np.bincount? #uncomment us!\n#np.argmax?\nnp.bincount(np.array([0, 1, 7, 3, 2, 1, 1]))","c4152c3c":"counts = np.bincount(nearestLabels)\ncounts","f97f2a28":"yhat = np.argmax(counts)\nyhat","f4255cd2":"nearestLabels = myytrain[sortierte_indices[:4]] \ncounts = np.bincount(nearestLabels)\nnearestLabels,counts","aa4fe093":"#Der Index in den obigen counts-Array ist gleichzeitig das Klassenlabel\n#Annahme hier: die Klassen sind von 0 bis n durchnummeriert\n\n#Wo taucht \u00fcberall die maximale Anzahl an counts auf?\nindices = np.argwhere(counts == counts.max())# Siehe np.argwhere-Doku!\nprint(indices)\nif len(indices)>1: \n    #w\u00e4hle zuf\u00e4llig einen davon:\n    import random\n    yhat = random.choice(indices.ravel())\nelse:\n    yhat=indices.ravel()[0]\nyhat","1073aaf4":"class myKNN(object): #Verbesserungsm\u00f6glichkeit: Erbe von sklearn.BaseEstimator\n    \"\"\"\n    Eine eigene Implementierung des k-NN-Algorithmus mit euklidischem Abstand.\n    \"\"\"\n    def __init__(self,k):\n        \"\"\"\n        k: Anzahl n\u00e4chste Nachbarn.\n        \"\"\"\n        #Bei der Instanziierung merken wir uns die Parameterwerte\n        self.k=k\n        \n    def fit(self,Xtrain,ytrain):\n        \"\"\"\n        Lerne die Trainingsdaten. Die Labels m\u00fcssen von 0 bis c-1 durchnummeriert sein (c: Anzahl Klassen).\n        \"\"\"\n        self.X=Xtrain\n        assert np.all(np.unique(ytrain)==np.arange(ytrain.max()+1)),'Klassenlabel m\u00fcssen von 0 bis c-1 durchnummeriert sein.'\n        self.y=ytrain\n        \n        self.distanzen = None\n        \n    def visualize_dataset(self):\n        pass\n    \n    def predict_single_row(self,Xrow):\n        \"\"\"\n        Gibt eine Vorhersage f\u00fcr das Klassenlabel f\u00fcr eine Zeile zur\u00fcck.\n        \"\"\"\n        assert Xrow.shape[0]==1 or Xrow.ndim==1,f'Kann nur eine Zeile verarbeiten. Shape war {Xrow.shape}'\n        #Berechne die euklidischen Distanzen zu Xrow:\n        distanzen = np.sum((self.X-Xrow)**2,axis=1)\n        #Extrahiere die Labels der k kleinsten Abst\u00e4nde:\n        sortierte_indices = np.argsort(distanzen)\n        nearestLabels = self.y[sortierte_indices[:self.k]]\n        #Z\u00e4hle, wie h\u00e4ufig welcher Wert vorkommt:\n        counts = np.bincount(nearestLabels)\n        \n        #counts enth\u00e4lt an der i-ten Stelle die H\u00e4ufigkeit der Klasse i in den k-NN\n        \n        maxcounts=counts[counts == counts.max()]\n        #Der Index in den counts-Array ist gleichzeitig das Klassenlabel\n        #Annahme hier: die Klassen sind von 0 bis n durchnummeriert\n        indices = np.argwhere(counts == counts.max())\n        if len(indices)>1: #Wenn die h\u00e4ufigste Klasse in den k-NN nicht eindeutig ist...\n            #...w\u00e4hle zuf\u00e4llig eine davon:\n            import random\n            yhat = random.choice(indices.ravel())\n        else:\n            yhat = indices.ravel()[0]\n        return yhat\n    \n    def predict(self,X):\n        \"\"\"\n        Vorhersage f\u00fcr mehrere Instanzen.\n        \"\"\"\n        if X.shape[0]==1:\n            return self.predict_single_row(X)\n        else:\n            yhat = []\n            for row in X:\n                yhat.append(self.predict_single_row(row.reshape(1,-1)))\n            return np.array(yhat)\n    \n    def predict2(self,X):\n        \"\"\"Gleich wie self.predict, nur k\u00fcrzer, aber minimal langsamer.\"\"\"\n        return np.apply_along_axis(self.predict_single_row,1,X)","a24d13f0":"clf = myKNN(k=3)\nclf.fit(Xtrain,ytrain)\nXtrain.shape","9809c677":"ytrain_hat = clf.predict(Xtrain)","7fe1ef44":"ytrain_hat = clf.predict2(Xtrain)","e98fd4ba":"ytrain_hat","b66ab1d1":"from sklearn.neighbors import KNeighborsClassifier\nclf_sklearn = KNeighborsClassifier(n_neighbors=3)\nclf_sklearn.fit(Xtrain,ytrain)\n","e69a3b06":"ytrain_hat_sklearn = clf.predict(Xtrain)","9c9852c5":"#pr\u00fcfe, dass alle Vorhersagen unserer Eigenimplementierung und jene von Scikit-Learn gleich sind:\nnp.all(ytrain_hat_sklearn == ytrain_hat)","40b90b31":"Xtest = pd.read_csv('..\/input\/male-daan-schnell-mal-klassifizieren\/test.csv',index_col='Id').values","6a491554":"yhat_test = clf.predict(Xtest)\nnp.all(yhat_test == clf_sklearn.predict(Xtest))","62feb869":"submission = pd.Series(yhat_test,name='y')\nsubmission.index.name='Id'\nsubmission.to_csv('Submission.txt',header=True)","c96bca7d":"!head Submission.txt","9d9ba80b":"Nun ben\u00f6tigen wir aber nicht nur den kleinsten Abstand, sondern die $k$ kleinsten Abst\u00e4nde. Eine nette Numpy-\u00dcbung...wie w\u00fcrden Sie diese Abst\u00e4nde finden? Tipp: es gibt eine Numpy-Funktion daf\u00fcr!","6644e80f":"Welche dieser Distanzen ist die kleinste? Dazu ist `np.argmin` da:","82d45d27":"# L\u00f6sung 3.Schritt: Welches Label taucht unter den n\u00e4chsten Nachbarn am h\u00e4ufigsten auf?","7d2a0b21":"# KNN selbstgemacht","6361a159":"Dann m\u00fcssten wir nun den Abstand berechnen: $d([X1,X2],[X1',X2'])=\\sqrt{(X1-X1')^2+(X2-X2')^2}$  \nNumpy's broadcasting-Regeln machen dies ganz einfach:","93084413":"Splitte die Daten in Trainings- und Testdaten:","0564b9d3":"Okay, Pandas- brauchen wir hier nicht wirklich! Wir wollen einen Numpy-Array f\u00fcr $X$ und $y$:  \n(Wir h\u00e4tten die Daten mit `np.genfromtxt(dateiname, delimiter=',',skip_header=1)` auch direkt in einen Numpy-Array laden k\u00f6nnen- aber welche Spalte w\u00e4re dann genau `y`?)","c9e2f3c3":"# L\u00f6sung 1.Schritt: In Numpy Abst\u00e4nde eines Punktes zu $n$ anderen Punkten berechnen  \nAngenommen wir haben einen (viel zu kleinen, aber \u00fcbersichtlichen) Trainingsdatensatz `myXtrain` und *einen* neuen Datenpunkt `myXtest`:","3fb06d88":"Eine bessere L\u00f6sung w\u00e4re es, sklearn.BaseEstimator als Basisklasse zu benutzen, oder gleich `sklearn.neighbors.KNeighborsClassifier`.\nDieser Klasse wollen wir nun eine .fit und eine .predict-Methode geben. Die .fit-Methode ist super-einfach!","fa168da8":"# Wir programmieren einen $k$-NN-Klassifikator\nParametrische Modelle lernen Parameter, aus denen dann Vorhersagen berechnet werden. Beispiele sind Neuronale Netze (die Gewichte sind Parameter), oder auch Entscheidungsb\u00e4ume (Parameter: die Merkmale und ihre Schwellwerte) oder Supportvektormaschinen (die Lage der trennenden Hyperebene). \nNichtparametrische Modelle wie $k$-NN ben\u00f6tigen keine solchen Parameter. Entsprechend einfach ist das Lernen!\n\nWir beginnen mit einer einfachen Klasse. Diese ben\u00f6tigt lediglich einen Konstruktor, um den Wert von $k$ anzunehmen:","f63c9b63":"Nun haben wir die Klassenlabels der n\u00e4chsten Nachbarn. Welches davon taucht am h\u00e4ufigsten auf? Wir m\u00fcssen deren H\u00e4ufigkeiten z\u00e4hlen. Verstehen Sie, wie dies mit np.bincount gemacht werden kann?","510c93d0":"Obige Zeile w\u00e4re eigentlich okay (sie l\u00e4dt die Daten!), aber die Indexspalte ist dupliziert. Sagen wir pandas, dass 'Id' die Index-Spalte ist.","3ff2fdf8":"Wir speichern einfach die Trainingsdaten ab!  ","5c524725":"**Aufgabe:** Die folgenden Zeilen visualisieren die Trainingsdaten. Machen Sie daraus eine Methode `myKNN.visualize_dataset` von myKNN, welches Xtrain und ytrain nimmt und den folgenden Plot ausgibt:****","80500936":"# L\u00f6sung 2.Schritt: In Numpy die $k$ kleinsten Abst\u00e4nde eines Punktes zu $n$ anderen Punkten berechnen\nGenauer: Wir suchen die Zeilenindices in unseren Array mit den k kleinsten Distanzeintr\u00e4gen.\n\nDazu sollten wir uns f\u00fcr unser Beispiel auf ein bestimmtes $k$ festlegen:","2d8fa33c":"Hier der Code, der Ihnen eine Starthilfe sein soll:","f947ac67":"# L\u00f6sung 4.Schritt: kNN-Klasse f\u00fcr einen einzigen Testdatenpunkt\nHier f\u00fchren wir alles obige zusammen in eine Klasse. Wenn wir auf diese Weise einen Datenpunkt klassifizieren k\u00f6nnen, dann brauchen wir anschliessend nur noch eine `for`-Schleife, um mehrere Datenpunkte zu bearbeiten.","3e61e131":"## Das Kernst\u00fcck: die .predict-Methode\nDer KNN-Klassifikator ist nun trainiert. Was uns wirklich interessiert, ist die Frage, wie nun eine Vorhersage gemacht wird! \n\n\nIm Prinzip haben wir es verstanden: \n- Berechne die Distanzen aller Trainingsdatenpunkte zu unserem neuen Punkt, \n- finde die die $k$ n\u00e4chsten Nachbarn\n- gib das Label zur\u00fcck das unter diesen $k$ n\u00e4chsten Nachbarn am h\u00e4ufigsten vorkommt.\n\nAlso z.B. sollte f\u00fcr die Jupyter-Zelle\n\n    Xvalid=np.array([[0,0]])\n    myKNN.predict(Xvalid)\n    \ndie Antwort `np.array([1])` lauten, wie auch in `sklearn`:\n\n    >>>from sklearn.neighbors import KNeighborsClassifier\n    >>>clf = KNeighborsClassifier(n_neighbors=5)\n    >>>clf.fit(Xtrain,ytrain)\n    >>>Xvalid=np.array([[0,0]])\n    >>>clf.predict(Xvalid)\n    array([1])\n\n(Die \">>>\" geben nur den \"Prompt\" in einer Textkonsole an und m\u00fcssen nicht eingegeben werden.) F\u00fcr mehrere Datenpunkte analog:\n\n    >>>Xvalid=np.array([[0,0],[1,2],[-3,4],[77,-53]])\n    >>>myKNN.predict(Xvalid)\n    array([1, 1, 0, 1])\n    \n**Aufgabe:** Implementieren Sie diese Methode. \n\nIm Folgenden zeige ich eine Musterl\u00f6sung in Schritten. Widerstehen Sie bitte der Versuchung, nach unten zu scrollen, und sich die L\u00f6sung anzusehen! Sie lernen nur etwas, wenn Sie ein neues Notebook nehmen, und sich selber der Herausforderung stellen. \n    ","0adba2c7":"Dann sollten wir anders vorgehen. Die untenstehende L\u00f6sung setzt (zur Vereinfachung voraus, dass die Klassenlabels $0,\\ldots, c-1$ sind.","0e8acb67":"Nun wird's tricky! Was suchen wir genau? Im Array der $k=3$ n\u00e4chsten Labels `array([0, 1, 0])` steht zweimal die 0, was die h\u00e4ufigste Klasse ist. Im Array `counts` steht daf\u00fcr an Nullter Stelle die 2. Daher ist die Klasse 0 die h\u00e4ufigste. Der folgende Befehl gibt dies im Allgemeinen (f\u00fcr ungerades $k$) korrekt aus:","d687f8c4":"Damit sind die $k$ n\u00e4chsten Elemente leicht extrahiert! Wir bleiben noch bei unserem \u00fcbersichtlichen kleinen Datensatz:","597c5b09":"Ein problematischer Spezialfall ergibt sich, wenn mehrere Klassen gleich h\u00e4ufig vorkommen! Schauen wir uns den Fall $k=4$ an:","4902fb90":"Insgesamt haben wir also den $k$-NN-Algorithmus von Scikit-Learn nachgebaut, allerdings bedeutend langsamer!"}}