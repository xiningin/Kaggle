{"cell_type":{"7b091638":"code","f3b86ab4":"code","4e73ccd7":"code","c08aeb86":"code","c43e8f96":"code","849ec3d4":"markdown","85faf2a7":"markdown","c391e3ba":"markdown","5412f30b":"markdown"},"source":{"7b091638":"from gensim.models import FastText\nfrom multiprocessing import Pool\nimport sqlite3 as sql\nimport numpy as np\nimport logging\nimport time\nimport re\n\ndb = '''..\/input\/english-wikipedia-articles-20170820-sqlite\/enwiki-20170820.db'''\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)","f3b86ab4":"def get_query(select, db=db):\n    '''\n    1. Connects to SQLite database (db)\n    2. Executes select statement\n    3. Return results and column names\n    \n    Input: 'select * from analytics limit 2'\n    Output: ([(1, 2, 3)], ['col_1', 'col_2', 'col_3'])\n    '''\n    with sql.connect(db) as conn:\n        c = conn.cursor()\n        c.execute(select)\n        col_names = [str(name[0]).lower() for name in c.description]\n    return c.fetchall(), col_names\n\ndef tokenize(text, lower=True):\n    '''\n    1. Strips apostrophes\n    2. Searches for all alpha tokens (exception for underscore)\n    3. Return list of tokens\n\n    Input: 'The 3 dogs jumped over Scott's tent!'\n    Output: ['the', 'dogs', 'jumped', 'over', 'scotts', 'tent']\n    '''\n    text = re.sub(\"'\", \"\", text)\n    if lower:\n        tokens = re.findall('''[a-z_]+''', text.lower())\n    else:\n        tokens = re.findall('''[A-Za-z_]''', text)\n    return tokens\n    \ndef get_section(rowid):\n    '''\n    1. Construct select statement\n    2. Retrieves section_text\n    3. Tokenizes section_text\n    4. Returns list of tokens\n\n    Input: 100\n    Output: ['the','austroasiatic','languages','in',...]\n    '''\n    select = '''select section_text from articles where rowid=%d''' % rowid\n    doc, _ = get_query(select)\n    tokens = tokenize(doc[0][0])\n    return tokens\n       \nclass Corpus():\n    def __init__(self, rowids):\n        self.rowids = rowids\n        self.len = len(rowids)\n\n    def __iter__(self):\n        rowids = np.random.choice(self.rowids, self.len, replace=False)\n        with Pool(processes=4) as pool:\n            docs = pool.imap_unordered(get_section, rowids)\n            for doc in docs:\n                yield doc\n\n    def __len__(self):\n        return self.len","4e73ccd7":"select = '''select distinct rowid from articles'''\nrowids, _ = get_query(select)\nrowids = [rowid[0] for rowid in rowids]","c08aeb86":"start = time.time()\n# To keep training time reasonable, let's just look at a random 10K section text sample.\nsample_rowids = np.random.choice(rowids, 10000, replace=False)\ndocs = Corpus(sample_rowids)\nfasttext = FastText(docs, min_count=100, size=100)\nend = time.time()\nprint('Time to train fasttext from generator: %0.2fs' % (end - start))","c43e8f96":"fasttext = FastText.load('..\/input\/english-wikipedia-articles-20170820-models\/enwiki_2017_08_20_fasttext.model')","849ec3d4":"For now, let's load a pre-trained model and explore how to use it.","85faf2a7":"First step, grab the index we'll be iterating over. In this case, we want to use section text, so let's use the implicit column: **rowid**.","c391e3ba":"# Tutorial: FastText\nThis is a basic guide to efficiently training a FastText model on the English Wikipedia dump using Gensim.","5412f30b":"Now let's train a FastText model. Ideally, we'd split the section text into sentences, but feeding section text as a block performs well. "}}