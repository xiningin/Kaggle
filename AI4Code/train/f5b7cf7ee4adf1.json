{"cell_type":{"6e257488":"code","3973846f":"code","2ac085c8":"code","281dc08c":"code","2577c751":"code","78229c17":"code","b1c1057f":"code","780c7384":"code","79d4d968":"code","832f5c7a":"code","546bf256":"code","f331fedf":"code","a522e278":"code","e470a6e1":"code","5164ed21":"code","3817d882":"code","79c07334":"code","a00d6a5c":"code","8fe051ba":"code","e82b71a7":"code","1a4ef920":"code","295cce0d":"code","242eac8a":"code","18a8572a":"code","97182aac":"code","e58222e0":"markdown","e642b8f6":"markdown","fdc9020b":"markdown","1267d56b":"markdown","dfe2d6e1":"markdown","9fddd69f":"markdown","2b0a6acd":"markdown","84af8723":"markdown","2b7d74d0":"markdown","5c68c9c2":"markdown","d14de2e0":"markdown","818d5c83":"markdown"},"source":{"6e257488":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom tabulate import tabulate\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nsns.set(style=\"darkgrid\")\nsns.set_palette(sns.cubehelix_palette(8))\n\n","3973846f":"data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\nprint(data.describe())\ndata.head()\n","2ac085c8":"table = pd.DataFrame(data.nunique())\ntable[\"null\"]=data.isnull().sum()\ntable[\"dtype\"] = data.dtypes\nheader = [\"Column\", \"Unique Values\",\"Null Values\",\"Datatype\"]\nprint(tabulate(table,headers=header,tablefmt=\"fancy_grid\"))\n\n","281dc08c":"test = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntable = pd.DataFrame(test.nunique())\ntable[\"null\"]=test.isnull().sum()\ntable[\"dtype\"] = test.dtypes\nheader = [\"Column\", \"Unique Values\",\"Null Values\",\"Datatype\"]\nprint(tabulate(table,headers=header,tablefmt=\"fancy_grid\"))\n","2577c751":"# Training\ndata.loc[data[\"Age\"] < 1, \"Age\"] = 0\ndata[\"Embarked\"] = data[\"Embarked\"].replace({\"C\":\"Cherbourg\",\"Q\":\"Queenstown\", \"S\":\"Southampton\"})\n\ndata = data.drop(\"Cabin\", axis =1) \ndata = data.dropna(subset=[\"Embarked\"])\ndata[\"index\"] = range(len(data))\ndata = data.set_index(\"index\")\n\n# Test \ntest.loc[test[\"Age\"] < 1, \"Age\"] = 0\ntest = test.drop(\"Cabin\", axis =1) \ntest[\"Embarked\"] = test[\"Embarked\"].replace({\"C\":\"Cherbourg\",\"Q\":\"Queenstown\", \"S\":\"Southampton\"})\n\ntest.sort_values(\"Fare\").tail() \ntest[\"Fare\"] = test[\"Fare\"].replace({np.nan:test.groupby(\"Pclass\")[\"Fare\"].mean()[3]})\n","78229c17":"print(data.corr()[\"Age\"].sort_values(ascending = False))","b1c1057f":"# As seen Pclass and SibSp are the strongest correlating features\nproc_data = [data.copy(), test.copy()]\n\n\n# I then divide SibSp into 3 groups (0,1,1+) and matches these with the the groups of Pclass. Createing 9 groups in total, I then assign the median age value of each group as the missing age.\n\nfor k in range(2):\n    proc_data[k][\"group\"] = 0\n    proc_data[k][\"naAge\"] = 0\n    for i in range(len(proc_data[k])):\n        if pd.isna(proc_data[k][\"Age\"][i]):\n            proc_data[k][\"naAge\"][i] = 1\n        if proc_data[k][\"SibSp\"][i] == 0:\n            if proc_data[k][\"Pclass\"][i] == 1:\n                proc_data[k][\"group\"][i] = 1\n            elif proc_data[k][\"Pclass\"][i] == 2:\n                proc_data[k][\"group\"][i] = 2\n            elif proc_data[k][\"Pclass\"][i] == 3:\n                proc_data[k][\"group\"][i] = 3\n        if proc_data[k][\"SibSp\"][i] == 1:\n            if proc_data[k][\"Pclass\"][i] == 1:\n                proc_data[k][\"group\"][i] = 4\n            elif proc_data[k][\"Pclass\"][i] == 2:\n                proc_data[k][\"group\"][i] = 5\n            elif proc_data[k][\"Pclass\"][i] == 3:\n                proc_data[k][\"group\"][i] = 6\n        if proc_data[k][\"SibSp\"][i] > 1:\n            if proc_data[k][\"Pclass\"][i] == 1:\n                proc_data[k][\"group\"][i] = 7\n            elif proc_data[k][\"Pclass\"][i] == 2:\n                proc_data[k][\"group\"][i] = 8\n            elif proc_data[k][\"Pclass\"][i] == 3:\n                proc_data[k][\"group\"][i] = 9\n    \n    med1 = np.median(proc_data[k][\"Age\"][(proc_data[k][\"group\"]==1) & (proc_data[k][\"naAge\"]==0)])\n    med2 = np.median(proc_data[k][\"Age\"][(proc_data[k][\"group\"]==2) & (proc_data[k][\"naAge\"]==0)])\n    med3 = np.median(proc_data[k][\"Age\"][(proc_data[k][\"group\"]==3) & (proc_data[k][\"naAge\"]==0)])\n    med4 = np.median(proc_data[k][\"Age\"][(proc_data[k][\"group\"]==4) & (proc_data[k][\"naAge\"]==0)])\n    med5 = np.median(proc_data[k][\"Age\"][(proc_data[k][\"group\"]==5) & (proc_data[k][\"naAge\"]==0)])\n    med6 = np.median(proc_data[k][\"Age\"][(proc_data[k][\"group\"]==6) & (proc_data[k][\"naAge\"]==0)])\n    med7 = np.median(proc_data[k][\"Age\"][(proc_data[k][\"group\"]==7) & (proc_data[k][\"naAge\"]==0)])\n    med8 = np.median(proc_data[k][\"Age\"][(proc_data[k][\"group\"]==8) & (proc_data[k][\"naAge\"]==0)])\n    med9 = np.median(proc_data[k][\"Age\"][(proc_data[k][\"group\"]==9) & (proc_data[k][\"naAge\"]==0)])\n    \n    for i in range(len(proc_data[k])):\n        if np.isnan(proc_data[k][\"Age\"][i]): \n            if proc_data[k][\"group\"][i] == 1:\n                proc_data[k][\"Age\"][i] = med1\n            if proc_data[k][\"group\"][i] == 2:\n                proc_data[k][\"Age\"][i] = med2\n            if proc_data[k][\"group\"][i] == 3:\n                proc_data[k][\"Age\"][i] = med3\n            if proc_data[k][\"group\"][i] == 4:\n                proc_data[k][\"Age\"][i] = med4\n            if proc_data[k][\"group\"][i] == 5:\n                proc_data[k][\"Age\"][i] = med5\n            if proc_data[k][\"group\"][i] == 6:\n                proc_data[k][\"Age\"][i] = med6\n            if proc_data[k][\"group\"][i] == 7:\n                proc_data[k][\"Age\"][i] = med7\n            if proc_data[k][\"group\"][i] == 8:\n                proc_data[k][\"Age\"][i] = med8\n            if proc_data[k][\"group\"][i] == 9:\n                proc_data[k][\"Age\"][i] = med9\n                \n    proc_data[k] = proc_data[k].drop([\"group\",\"naAge\"], axis=1) \n","780c7384":"fig, ax = plt.subplots(1,3,figsize=(20,5))\nsns.countplot(data[\"Survived\"],ax=ax[0],palette=\"Set2\").set_title(\"Survived\")\nsns.countplot(data[\"Embarked\"],ax=ax[1],palette=\"Set2\").set_title(\"Embarked\")\nsns.countplot(data[\"Pclass\"],ax=ax[2],palette=\"Set2\").set_title(\"Ticket Class\")\nplt.show()\n\n\nfig, ax = plt.subplots(1,3,figsize=(20,5))\nsns.countplot(data[\"Sex\"],ax=ax[0],palette=\"Set2\").set_title(\"Sex\")\nsns.countplot(data[\"SibSp\"],ax=ax[1],palette=\"Set2\").set_title(\"Siblings & Spouses\")\nsns.countplot(data[\"Parch\"],ax=ax[2],palette=\"Set2\").set_title(\"Parents & Children\")\nplt.show()\n\n\nfig, ax = plt.subplots(1,2,figsize=(20,5))\nsns.distplot(data[\"Fare\"], color=\"g\", ax=ax[0], kde=False).set_title(\"Fare\")\nsns.distplot(proc_data[0][\"Age\"], color=\"g\", ax=ax[1], kde=False).set_title(\"Age\")\nplt.show()","79d4d968":"width = 1      \ngroupgap=1\n\ny1=[np.average(data[\"Survived\"][data[\"Sex\"] == \"male\"])*100,\n     np.average(data[\"Survived\"][data[\"Sex\"] == \"female\"])*100]\n\ny2=[np.average(proc_data[0][\"Survived\"][proc_data[0][\"Age\"] < 15])*100,\n          np.average(proc_data[0][\"Survived\"][proc_data[0][\"Age\"] >= 15])*100]\n\ny3=[np.average(data[\"Survived\"][data[\"Pclass\"] == 1])*100,\n     np.average(data[\"Survived\"][data[\"Pclass\"] == 2])*100,\n     np.average(data[\"Survived\"][data[\"Pclass\"] == 3])*100]\n\n\nx1 = np.arange(len(y1))\nx2 = np.arange(len(y2))+groupgap+len(y1)\nx3 = np.arange(len(y3))+groupgap+len(y1)+groupgap+len(y2)\n\n\nind = np.concatenate((x1,x2,x3))\nfig, ax = plt.subplots()\nrects1 = ax.bar(x1, y1, width, color=\"r\",  edgecolor= \"black\")\nrects2 = ax.bar(x2, y2, width, color=\"b\",  edgecolor= \"black\")\nrects3 = ax.bar(x3, y3, width, color=\"g\",  edgecolor= \"black\")\n\nax.set_ylabel(\"Survived (%)\",fontsize=14)\nax.set_xticks(ind)\nplt.xticks(rotation=45)\n\nax.set_xticklabels((\"Male\", \"Female\",\"Child\",\"Adult\",\"First Class\",\"Second Class\",\"Third Class\"),fontsize=14)\nax.set_title(\"Groups Rate of Survival\")\n\nplt.show()\n","832f5c7a":"for k in range(2):\n    proc_data[k][\"male\"] = proc_data[k][\"Sex\"].replace({\"male\":1,\"female\":0})\n    \n    proc_data[k] = proc_data[k].drop([\"Sex\",\"Name\",\"Ticket\",\"PassengerId\"], axis=1) \n    \n    L_E = LabelEncoder()\n    proc_data[1][\"Embarked\"] = proc_data[1][\"Embarked\"].astype(str) # seems to be something strange here so this one is needed\n    proc_data[k][\"Embarked\"] = L_E.fit_transform(proc_data[k][\"Embarked\"])\n\n    proc_data[k][\"Fare\"] = pd.qcut(proc_data[k][\"Fare\"], 4, labels=[0, 1, 2, 3]).astype(int)\n    proc_data[k][\"Age\"] = pd.qcut(proc_data[k][\"Age\"], 4, labels=[0, 1, 2, 3]).astype(int)\n\n\n\n","546bf256":"for i in range(2):\n    proc_data[i][\"family\"] = proc_data[i][\"SibSp\"] + proc_data[i][\"Parch\"]\n    proc_data[i] = proc_data[i].drop([\"SibSp\", \"Parch\"], axis = 1)\n    sns.countplot(proc_data[i][\"family\"],palette=\"Set2\").set_title(\"Family Size\")","f331fedf":"for i in range(2):    \n    proc_data[i][\"alone\"] = 0\n    proc_data[i].loc[proc_data[i][\"family\"] == 0, \"alone\"] = 1\n    proc_data[i] = proc_data[i].drop(\"family\", axis =1)\n","a522e278":"proc_data[0]","e470a6e1":"train = proc_data[0]\n\ntable = pd.DataFrame(train.corr()[\"Survived\"].sort_values(ascending = False))\ntable[\"Description\"] = [\"0: Died 1: Survived\",\n     \"0:(0,8.05] 1:(8.05, 15.646] 2:(15.646, 33] 3:(33, 512.329]\\n 25% of the sample in each group\",\n     \"0:(0,21] 1:(21, 26] 2:(26, 35] 3:(35, 80]\\n 25% of the sample in each group\",\n     \"0:Cherbourg 1:Queenstown 2:Southampton\",\n     \"0:With family 1:Alone\",\n     \"1:First Class 2:Second 3:Third\",\n     \"0:Female 1:Male\"]\nheader = [\"Feature\", \"Correlation\", \"Description\"]\nprint(tabulate(table,headers=header,tablefmt=\"fancy_grid\"))\n\n","5164ed21":"\nfrom sklearn.preprocessing import MinMaxScaler \n\nscaler = MinMaxScaler()\n\ntrain = pd.DataFrame(scaler.fit_transform(train),columns = train.columns)\n\nX_train = train.drop([\"Survived\"],axis=1)\nY_train = train[\"Survived\"]\n\n\nX_test = pd.DataFrame(scaler.fit_transform(proc_data[1]),columns = proc_data[1].columns)\n\n","3817d882":"from sklearn.model_selection import train_test_split\nx_train, x_valid, y_train, y_valid = train_test_split(X_train, Y_train, test_size = 0.30, random_state = 42)\n\n","79c07334":"rand_forr = RandomForestClassifier(n_estimators = 100)\n\nrand_forr.fit(x_train, y_train)\nY_pred = rand_forr.predict(x_valid)\n\nfrom sklearn.metrics import accuracy_score ,roc_auc_score, precision_score, confusion_matrix\nprint('Random Forest Model Validation set\\n')\n\nprint('Accuracy Score: {}\\n\\nPrecision Score: {}\\n\\nConfusion Matrix:\\n {}\\n\\nAUC Score: {}'\n      .format(accuracy_score(y_valid,Y_pred), precision_score(y_valid,Y_pred),confusion_matrix(y_valid,Y_pred), roc_auc_score(y_valid,Y_pred)))\n","a00d6a5c":"# Hyperparameter tuning for RandomForest\n# Create a parameter grid to sample from during fitting\n\n# Parameters:\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n","8fe051ba":"from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n\n# Use the random grid to narrow down the possible best hyperparameters\nrf = RandomForestClassifier()\n\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n\n# Fit the random search model\nrf_random.fit(x_train, y_train)","e82b71a7":"# Find best parameters\nrf_random.best_params_","1a4ef920":"# Find the specific best parameters by searching every combination close to our earlier findings\n\n# Create a new parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [10,20, 30],\n    'max_features': [2, 3],\n    'min_samples_leaf': [1, 2],\n    'min_samples_split': [1, 2, 3],\n    'n_estimators': [800, 1000, 1200]\n}\n\n# Create a based model\nrf = RandomForestClassifier()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)\n\ngrid_search.fit(x_train, y_train)\n\ngrid_search.best_params_","295cce0d":"# Validation of RF \n# Using F1, accuracy, and AUC\n\nrf_best = RandomForestClassifier(n_estimators = 1000, min_samples_split = 2,\n                                 min_samples_leaf = 1, max_features = 2,\n                                 max_depth = 10, bootstrap = True)\n\nrf_best.fit(x_train, y_train)\ny_pred_best = rf_best.predict(x_valid)\n\nprint('Accuracy Score: {}\\n\\nPrecision Score: {}\\n\\nConfusion Matrix:\\n {}\\n\\nAUC Score: {}'\n      .format(accuracy_score(y_valid,y_pred_best), precision_score(y_valid,y_pred_best),confusion_matrix(y_valid,y_pred_best), roc_auc_score(y_valid,y_pred_best)))\n","242eac8a":"submission_pred = rf_best.predict(X_test)\n","18a8572a":"submission = pd.DataFrame(test[\"PassengerId\"])\nsubmission[\"Survived\"] = submission_pred.astype(int)\nsubmission.tail(15)\n","97182aac":"submission.to_csv(\"..\/working\/submit.csv\", index=False)\n","e58222e0":"**Training Set**","e642b8f6":"# 5. Feature Engineering\nAfter gaining better knowledge of the data I start to process it further before building the predictions. I replace the categorical *Sex* column with the binary *male*. The columns for *Name*, *Ticket* and *PassengerId* I deem unuseful for the task at hand and deletes them to reduce complexity. I also transform *Embarked* to numerical and make *Fare* and *Age* into quartiles, also to reduce complexity among the variables. In the case for *SibSp* and *Parch* we know from the summary statistics that 0 is the by far largest group for both variables. Therefore, I want to combine them into a binary. I start by creating the new variable *family*, which is the summation of the two older variables. Since a family size of 0 is by far the largest I decide to make it the binary *alone*. A summary of the variables I'm using for the models, together with their correlation with *Survived* can be found in the table below.","fdc9020b":"# 2. A first look at the Data\nI start by looking at the *train dataset*. It consists of 12 different columns and 891 rows. Some columns are categorical while others are numerical. I run through a couple of pandas functions go get a quick overview. As can be seen is there is a worryingly high number of missing values, both in the Cabin and Age column.\n\nI then also do the same operations on the *test dataset*. Which have 418 rows and 11 columns, missing the earlier *Survived* of course. ","1267d56b":"# 7. Conclusion\nAs seen all the predictions give quite similar results and accuracy scores, however since the random forest gives the best I choose it for my submission. \n\n\nThanks for reading, any critic or tips are highly appreciated!","dfe2d6e1":"# 6. Classification Models\nFor my predictions, I use four different models. I collect all the results and their accuracy score, how well the predictions within the training set matches to the real values.","9fddd69f":"This first model does not give very accuarate results. Therefore, I will perform some hyperparameter tuning to find a better fit.","2b0a6acd":"# 1. Packages & Settings\nFor handling the data I use pandas and for mathematical operations numpy. LabelEncoder to transform the data to integers and tabulate to present things in nice tables. From sklearn I use four different learning methods.","84af8723":"# 4. Summary Statistics\nTo get to know the datasets better and to begin to draw some conclusions I construct a few graphs based on the training set to easier visualize what we are working with.\n\nIt would be expected that some groups have a higher chance of survival than others, such as women, children and people from first class. To explore this another plot is created. I define children as passengers below 15 years of age and when studying the data one can see that these to a higher extent are traveling with a parent. From the graph, it can be seen that the assumption holds, especially in the case of sex.\n","2b7d74d0":"The score is a little better but still not very good. But will have to do for now.","5c68c9c2":"# Introduction\nThis is the notebook for my submission for the \"Titanic: Machine Learning from Disaster\" competition. It is also my first notebook on Kaggle. I've have tried to write it pedagogical and easy to follow with explanations and motivations for my choices both for my own sake and others new to the field of data science. It is divided into the following sections: \n1. Packages & Settings\n2. A First Look at the Data\n3. Data Cleaning & Preprocessing\n4. Summary Statistics\n5. Feature Engineering\n6. Prediction Models\n7. Conclusion\n","d14de2e0":"# 3. Data Cleaning & Preprocessing\n\n**Missing Values**\n\nThese missing values need to be handled before we can proceed with any modeling. Since *Cabin* holds such a high number of missing values (77 and 78 %, training respectively test) and I can't think of an immediate need for the column I deem it useless and therefore deletes it. I also delete the two rows containing missing values in *Embarked* since only two rows shouldn't make a considerable difference. Deleing rows and columns like this does mean a loss of information and should in many cases be avoided. The missing values among *Fare* in the test set and *Age* in both on the other hand I choose not to delete. In the test set, I can't delete rows since my prediction must be for the entire set and in training, I think that the column holds too much significant information and that the 177 rows constitute a too large share (20 %) of the total amount of rows. Instead, I will try to estimate these values.\n\nI assume that *Fare* and *Pclass* is is highly correlated, therefore I check what ticket class the passenger has (class 3) and assigns that class' average to the passenger\n\nThe *Age* column will be estimated with help by the column's correlation with the other variables. ","818d5c83":"**Test Set**"}}