{"cell_type":{"5b4b9976":"code","277b290d":"code","00d60cb8":"code","2edba235":"code","a9d490f1":"code","5713334c":"code","9d81eae0":"code","b292d83c":"code","662f6be8":"code","7bd9be8c":"code","a48b7b1d":"code","471cbf19":"code","b9523e20":"code","dd7e1816":"code","157a5977":"code","6408366c":"code","2fd6bef5":"code","aa4bb2a2":"code","c0ad6e31":"code","ccc64e86":"code","d5987e0d":"code","58996014":"code","bc55c440":"code","4b01d1c1":"code","36d2517e":"code","0fddb26c":"code","96f7612c":"code","b25f458e":"code","f62b77e9":"code","06cc4f3b":"code","e4cb6562":"code","04893af3":"code","92e33f64":"code","3e4fd4f6":"code","bc67364d":"code","e93e7607":"code","fe11432f":"code","70e431a8":"code","63c488b3":"code","2a31d5b8":"code","d5921e06":"code","10a18a26":"code","0317bcde":"code","3f3d1de6":"code","14a362a9":"code","dec493e6":"code","dd2e75d4":"code","1665eb75":"code","321f4b30":"code","443d175c":"code","e7eea16d":"code","fdd7e448":"code","8e9d05ab":"code","af53f4e3":"code","b6d6c847":"code","6a1001ae":"code","b55b1e49":"code","d60ef48d":"code","5fd89c98":"code","761b1bb6":"code","dbe1a318":"code","4bc57dc3":"code","44b47a2e":"code","4fda83db":"code","abbef1fd":"code","26c1d47e":"code","47903f16":"code","21c588a8":"code","342411be":"code","e2d6bafd":"code","542658d1":"code","01b73355":"code","aed8bc28":"code","bafffe8b":"code","d7cd9e43":"code","a757a352":"markdown","677c6175":"markdown","c3396461":"markdown","32cef38e":"markdown","4f35378e":"markdown","9214a7a1":"markdown","6f8f82f0":"markdown","2b665fbc":"markdown","7421bb29":"markdown","f0e041a4":"markdown","46fb4eeb":"markdown","2d54f327":"markdown","a90455c6":"markdown","c386676d":"markdown","64c66669":"markdown","cc72948c":"markdown","2100d98c":"markdown","ba2a8922":"markdown","bb185200":"markdown","3bd56f1c":"markdown","71a51f35":"markdown","6c8ae39b":"markdown","1286ac34":"markdown","d320ab7e":"markdown","eacee86d":"markdown","1816a696":"markdown","3d5fcb15":"markdown","27548d43":"markdown","f3fecd5d":"markdown","0a2c3813":"markdown","b4d7b6e0":"markdown","07311a81":"markdown","419407b8":"markdown","c95d2b6d":"markdown","b95d0d00":"markdown","8efec548":"markdown","3178a3de":"markdown","c85648ae":"markdown","6d5cbabd":"markdown","4cac09d7":"markdown","91c11f14":"markdown","3647cb33":"markdown","992e1ad4":"markdown","d951b9f4":"markdown","2dc7e9b2":"markdown","f89ec6bd":"markdown","0a1e9ed2":"markdown"},"source":{"5b4b9976":"import os\nimport numpy as np\nimport pandas as pd","277b290d":"def load_housing_data():\n    DATA_PATH = '..\/input\/hands-on-machine-learning-housing-dataset\/housing.csv'\n    data = pd.read_csv(DATA_PATH)\n    return data","00d60cb8":"housing = load_housing_data()","2edba235":"housing.head()","a9d490f1":"housing.info()","5713334c":"housing['ocean_proximity'].value_counts()","9d81eae0":"housing.describe()","b292d83c":"%matplotlib inline\nimport matplotlib.pyplot as plt\nhousing.hist(bins=50, figsize=(20,15))","662f6be8":"def split_train_test(data, test_ratio):\n    shuffled_indices = np.random.permutation(len(data)) #returns a shuffled numpy array\n    test_set_size = int(len(data) * test_ratio)\n    test_indices = shuffled_indices[:test_set_size]\n    train_indices = shuffled_indices[test_set_size:]\n    return data.iloc[train_indices], data.iloc[test_indices]\n\n##so the np.random.permutation shuffles the order of the rows in a DataFrame and returns an np array    \n#df_test = pd.DataFrame({'column_1':[1,2,3,4], 'column_2':[5,6,7,8]})\n#print(df_test)\n#np.random.permutation(df_test)\n#np.random.permutation(10)","7bd9be8c":"#help(np.random.permutation)\n#help(pd.DataFrame.iloc)","a48b7b1d":"train_set, test_set = split_train_test(housing, 0.2)","471cbf19":"len(train_set)","b9523e20":"len(test_set)","dd7e1816":"from zlib import crc32\ndef test_set_check(identifier, test_ratio):\n    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32\n\ndef split_train_test_by_id(data, test_ratio, id_column):\n    ids = data[id_column]\n    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n    return data.loc[~in_test_set], data.loc[in_test_set] # ~ is used to compare binary numbers\n\n## The lambda keyword is used to create small anonymous functions.\n## A lambda function can take any number of arguments, but can only have one expression.\n## The expression is evaluated and the result is returned.","157a5977":"#help(pd.DataFrame.loc) # Access a group of rows and columns by label(s) or a boolean array.\n#help(pd.DataFrame.apply) #apply a function along the axis of a DataFrame","6408366c":"housing_with_id = housing.reset_index() #adds an 'index' column\ntrain_set, test_set = split_train_test_by_id(housing_with_id, 0.2, 'index')\nhousing_with_id","2fd6bef5":"housing_with_id[\"id\"] = housing[\"longitude\"]*1000 + housing[\"latitude\"]\ntrain_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"id\")\nhousing_with_id","aa4bb2a2":"from sklearn.model_selection import train_test_split\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\nprint(len(train_set), \"\/\",len(test_set))","c0ad6e31":"#Function below converts bin values into discrete intervals\nhousing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n                               bins=[0., 1.5, 3.0, 4.5, 6.0, np.inf],\n                               labels=[1, 2, 3, 4, 5])\nhousing","ccc64e86":"np.inf","d5987e0d":"#help(pd.cut)","58996014":"housing[\"income_cat\"].hist()","bc55c440":"housing","4b01d1c1":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(housing, housing['income_cat']):\n    strat_train_set = housing.loc[train_index]\n    strat_test_set = housing.loc[test_index]","36d2517e":"strat_test_set['income_cat'].value_counts() \/ len(strat_test_set)","0fddb26c":"housing['income_cat'].value_counts() \/ len(housing)","96f7612c":"strat_train_set","b25f458e":"for set_ in (strat_train_set, strat_test_set):\n    set_.drop(labels=[\"income_cat\"], axis=1, inplace=True)","f62b77e9":"strat_train_set","06cc4f3b":"exp_train_set = strat_train_set.copy() ###explore only the TRAIN set","e4cb6562":"exp_train_set.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")","04893af3":"###`alpha` creates a better visualization, wich highlights high density areas\nexp_train_set.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)","92e33f64":"exp_train_set.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n                   s=exp_train_set['population']\/100, label='population', figsize=(10,7),\n                   c='median_house_value', cmap=plt.get_cmap('jet'), colorbar=True)\nplt.legend()","3e4fd4f6":"#help(exp_train_set.plot)","bc67364d":"corr_matrix = exp_train_set.corr()##returns a DataFrame\ncorr_matrix","e93e7607":"corr_matrix['median_house_value'].sort_values(ascending=False)","fe11432f":"from pandas.plotting import scatter_matrix\n##below are the most promissing attributes\nattributes = ['median_house_value', 'median_income', 'total_rooms', 'housing_median_age']\nscatter_matrix(exp_train_set[attributes], figsize=(12,8))## thiss is a pandas function","70e431a8":"exp_train_set.plot(kind='scatter', x='median_income', y='median_house_value', alpha=0.1)","63c488b3":"exp_train_set['rooms_per_household'] = exp_train_set['total_rooms'] \/ exp_train_set['households']\nexp_train_set['bedrooms_per_rooms'] = exp_train_set['total_bedrooms'] \/ exp_train_set['total_rooms']\nexp_train_set['population_per_household'] = exp_train_set['population'] \/ exp_train_set['households']","2a31d5b8":"corr_matrix = exp_train_set.corr()\ncorr_matrix['median_house_value'].sort_values(ascending=False)","d5921e06":"## drop() creates a copy of the data and does not affect strat_train_set\nprep_train_set = strat_train_set.drop('median_house_value', axis=1)\nprep_train_set_labels = strat_train_set['median_house_value'].copy()\nprep_train_set","10a18a26":"# Option 1:\n# prep_train_set.dropna(subset=[\"total_bedrooms\"])\n#\n# Option 2:\n# prep_train_set.drop(total_bedrooms, axis=1)\n#\n# Option 3:\n# median = prep_train_set['total_bedrooms'].median()\n# prep_train_set['total_bedrooms'].fillna(median, inplace=True)","0317bcde":"from sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(strategy=\"median\")\n\n## the line below is necessary because we have to have only numeric values\nprep_train_set_num = prep_train_set.drop('ocean_proximity', axis=1)\n\nimputer.fit(prep_train_set_num)## THIS IS A \"TRAINNED\" IMPUTER\n## imputer has simply computed the median of each attribute and stored the result in its statistics_ instance variable\nimputer.statistics_\n\n## now i can use this \"trained\" imputer to transform the training set by replacing missing values with the learned medians:\nX = imputer.transform(prep_train_set_num)## The result is a plain numpy array containing the transformed features.\n                                         ## If you want tto put it back into a pandas DataFrame, it's simple:\n\nprep_train_set_tr = pd.DataFrame(X, columns=prep_train_set_num.columns,\n                                 index=prep_train_set_num.index)\n\n\n","3f3d1de6":"prep_train_set_num.median().values","14a362a9":"prep_train_set_tr","dec493e6":"imputer.strategy","dd2e75d4":"# prep_train_set_cat = strat_train_set['ocean_proximity'] ## this line creates a Series\nprep_train_set_cat = strat_train_set[['ocean_proximity']] ## this one creates a DataFrame\nprep_train_set_cat.head(10)","1665eb75":"type(prep_train_set_cat)","321f4b30":"from sklearn.preprocessing import OrdinalEncoder\nordinal_encoder = OrdinalEncoder()\nprep_train_set_cat_encoded = ordinal_encoder.fit_transform(prep_train_set_cat)\nprep_train_set_cat_encoded[:10]","443d175c":"ordinal_encoder.categories_","e7eea16d":"from sklearn.preprocessing import OneHotEncoder\ncat_encoder = OneHotEncoder()\nprep_train_set_cat_1hot = cat_encoder.fit_transform(prep_train_set_cat)\nprep_train_set_cat_1hot","fdd7e448":"prep_train_set_cat_1hot.toarray() ## this is a scipy module","8e9d05ab":"# help(prep_train_set_cat_1hot.toarray)","af53f4e3":"### THE CODE IN THIS CELL I DID NOT UNDERSTAND VERY WELL\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nrooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\n\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n        self.add_bedrooms_per_room = add_bedrooms_per_room\n    def fit(self, X, y=None):\n        return self  # nothing else to do\n    def transform(self, X, y=None):\n        rooms_per_household = X[:, rooms_ix] \/ X[:, household_ix]\n        population_per_household = X[:, population_ix] \/ X[:, household_ix]\n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:, bedrooms_ix] \/ X[:, rooms_ix]\n            return np.c_[X, rooms_per_household, population_per_household,\n                         bedrooms_per_room]\n        else:\n            return np.c_[X, rooms_per_household, population_per_household]\n\nattr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\nprep_train_set_extra_attribs = attr_adder.transform(prep_train_set.values)","b6d6c847":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('attribs_adder', CombinedAttributesAdder()),\n    ('std_scaler', StandardScaler()),\n])\n\nprep_train_set_num_transformed = num_pipeline.fit_transform(prep_train_set_num)\n\n########################## mass transformation\nfrom sklearn.compose import ColumnTransformer\n\nnum_attribs = list(prep_train_set_num)\ncat_attribs = ['ocean_proximity']\n\nfull_pipeline = ColumnTransformer([## this is the full pipeline for the data transformation\n    ('num', num_pipeline, num_attribs),\n    ('cat', OneHotEncoder(), cat_attribs)\n])\n\nprep_train_set_prepared = full_pipeline.fit_transform(prep_train_set)","6a1001ae":"prep_train_set_prepared","b55b1e49":"prep_train_set_labels","d60ef48d":"housing_prepared, housing_labels = prep_train_set_prepared, prep_train_set_labels\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)","5fd89c98":"some_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint('Predictions: ', lin_reg.predict(some_data_prepared))\nprint('Labels:', list(some_labels))","761b1bb6":"from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nprint(\"Linear, Root Mean Squared Error:\", lin_rmse)","dbe1a318":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(housing_prepared, housing_labels)\n\nhousing_predictions = tree_reg.predict(housing_prepared)\ntree_mse = mean_squared_error(housing_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\nprint(\"Tree, Root Mean Squared Error:\", tree_rmse)","4bc57dc3":"from sklearn.model_selection import cross_val_score\ntree_scores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n                         scoring=\"neg_mean_squared_error\", cv=10) ## 10 folds\ntree_rmse_scores = np.sqrt(-tree_scores)\ntree_scores","44b47a2e":"def display_scores(scores):\n    print('Scores:', scores)\n    print('Mean:', scores.mean())\n    print('Standard Deviation:', scores.std())\n\ndisplay_scores(tree_rmse_scores)","4fda83db":"lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n                             scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)","abbef1fd":"from sklearn.ensemble import RandomForestRegressor\nforest_reg = RandomForestRegressor()\nforest_reg.fit(housing_prepared, housing_labels)\n\nhousing_predictions = forest_reg.predict(housing_prepared)\nforest_mse = mean_squared_error(housing_labels, housing_predictions)\nforest_rmse = np.sqrt(forest_mse)\nprint(\"Tree, Root Mean Squared Error:\", forest_rmse)","26c1d47e":"forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n                         scoring=\"neg_mean_squared_error\", cv=10) ## 10 folds\nforest_rmse_scores = np.sqrt(-forest_scores)\ndisplay_scores(forest_rmse_scores)","47903f16":"import joblib","21c588a8":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]}\n]\n\nforest_reg = RandomForestRegressor()\n\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n                           scoring='neg_mean_squared_error',\n                           return_train_score=True)\n\ngrid_search.fit(housing_prepared, housing_labels)","342411be":"grid_search.best_params_","e2d6bafd":"grid_search.best_estimator_","542658d1":"cvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres['mean_test_score'], cvres['params']):\n    print(np.sqrt(-mean_score), params)","01b73355":"feature_importances = grid_search.best_estimator_.feature_importances_\nfeature_importances","aed8bc28":"extra_attribs = ['rooms_per_hhold', 'pop_per_hhold', 'bedrooms_per_rooms']\ncat_encoder = full_pipeline.named_transformers_['cat']\ncat_one_hot_attribs = list(cat_encoder.categories_[0])\nattributes = num_attribs + extra_attribs + cat_one_hot_attribs\nsorted(zip(feature_importances, attributes), reverse=True)","bafffe8b":"final_model = grid_search.best_estimator_\n\nX_test = strat_test_set.drop('median_house_value', axis=1)\ny_test = strat_test_set[\"median_house_value\"].copy()\n\nX_test_prepared = full_pipeline.transform(X_test)\n\nfinal_predictions = final_model.predict(X_test_prepared)\n\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)\nfinal_rmse","d7cd9e43":"from scipy import stats\nconfidence = 0.95\nsquared_errors = (final_predictions - y_test) ** 2\nnp.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n                         loc=squared_errors.mean(),\n                         scale=stats.sem(squared_errors)))","a757a352":"creatind a new dataframe, but with an id:","677c6175":"`ocean_proximity` is a categorical attribute. But most machine learning algorithms prefer to work with numbers, so let's converts these categories from text to numbers","c3396461":"# Experimenting with attribute Combinations","32cef38e":"# The stratified `train` and `test` set are the official ones\nNow that i stratified the median income, i'm ready to do stratified sampling based on the income cathegory. \"StratifiedSuffleSplit\" sklearn' class will help:","4f35378e":"# Grid Search","9214a7a1":"**Using the split_train_test() function:**","6f8f82f0":"Congratulations to me, i've successfully fine-tuned my best model!","2b665fbc":"# Studying the data:","7421bb29":"That's right: the Decision Tree model is overfitting so badly that it performs worse than the linear regression model.","f0e041a4":"**Testing the RandomForestRegressor model:**","46fb4eeb":"# Creating a unique caracterisc for each row, 3rd way:","2d54f327":"**A few new things**","a90455c6":"# Evaluating the system on the Test Set","c386676d":"In some cases, such a point estimate of the generalization error will not be quite enough to convince you to launch: what if it is just 0.1% better than the model currently in production? You might want to have an idea of how precise this estimate is. For this, you can compute a 95% confidence interval for the generalization error using `scipy.stats.t.interval()`:","64c66669":"**Testing DecisionTreeRegressor model:**","cc72948c":"**But those indices are not that unique, for exmple if the data gets changed. One must choose a better unique characteristc, for exemple longitude together with latitude wont change. A better aproach is try to combine them.**","2100d98c":"# creating strata for \"median_income\"","ba2a8922":"Now, removing 'income_cat' attribute so the data is back to its original state:","bb185200":"Looking at the new correlations","3bd56f1c":"# Now with scikit-learn","71a51f35":"# Creating a test set, 2nd way:\n**This way the test set won't contain instances that have been in the train set**","6c8ae39b":"**Data Cleaning:**","1286ac34":"# Selecting and training a model","d320ab7e":"# Looking For Correlations","eacee86d":"# Preparing the Data for Machine Learning Algorithms","1816a696":"**A few new things:**","3d5fcb15":"Create a copy for exploration, so that i can play with it without harming the training set","27548d43":"Printing each one of the combination results:","f3fecd5d":"**Pipeline:**","0a2c3813":"Now some cross-validation with `RandomForestRegressor`:","b4d7b6e0":"At this step i'm testing on the training set, this is a good way of evaluationg overfitting.","07311a81":"Below a pica-mega-blaster vizualization !!!","419407b8":"**The linear regression model underfitted the data, so i'll try a more powerful model, `DecisionTreeRegressor`:**","c95d2b6d":"Scikit_learn provides a `OneHotEncoder` class to convert categorical values into one-hot vectors:","b95d0d00":"# Fine-tunning the model\nLet's assume that now I have a shorlist of promissing models. I now need to fine tune them.","8efec548":"**Now I have 3 options: **\n1. Get rid of corresponding districts;\n2. Get rid of the whole attribute;\n3. Set the values to some value.","3178a3de":"But sklearn provides a handy class to take care of missing values: `SimpleImputer`","c85648ae":"# Creating a test set, 1st way:\n**This way of spliting test and training set is not good, because every time I run the code, the test set changes, that is, there are instances in the new test set that were previously in the training set. Doing things this ways makes the test set corrupted, poluted with the training one.**","6d5cbabd":"The Error with the `DecisionTreeRegressor` is zero, that means that the model overfitted the data.","4cac09d7":"So, the Random Forest performed much better than the previous models, but is overfitting.","91c11f14":"So `DecisionTreeRegressor` did not performe that well, now doing the same cross validation for `LinearRegression`:","3647cb33":"# Better Evaluation Using Cross-Validation","992e1ad4":"# Visualizing the data to gain insights","d951b9f4":"The most promissing attribute is  the `median_income`, so i'll zoom in on their correlation scatterplot:","2dc7e9b2":"Below, we can see that the stratification was successfull","f89ec6bd":"# Custom Transformers","0a1e9ed2":"**Testing LinearRegression model:**"}}