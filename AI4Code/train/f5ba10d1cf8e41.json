{"cell_type":{"8590a7f1":"code","79826699":"code","10e7a6d2":"code","a7e84c2f":"code","2bd256a4":"code","328a3e01":"code","8f5b4f1d":"code","e4244df2":"code","853bf53a":"code","71f2865f":"code","abfadd47":"code","03a4b04b":"code","32900db1":"code","a0264dfb":"code","bd9ed8f1":"code","d52c005d":"code","667fc470":"code","e5a6a515":"code","a0bfefb6":"code","ae3c8c6f":"code","bddb041f":"code","901e76bb":"code","e0fefb17":"code","69eac485":"code","af4d8c0b":"code","ae98ea8c":"code","14987d2f":"code","37d47652":"code","3547f6cc":"code","9472fa15":"code","b60f8796":"code","57aa7f02":"code","2518996a":"markdown","258663d7":"markdown","187f29b5":"markdown","a1657fe8":"markdown","953144b9":"markdown","1b7dc3d3":"markdown","60bb6fac":"markdown","080263df":"markdown","62bccc93":"markdown","72a1920e":"markdown","da27b066":"markdown","2c9509d9":"markdown","7699ae04":"markdown","19ca24e0":"markdown","ff96064f":"markdown","93033a6a":"markdown","20113587":"markdown","fadd9fcf":"markdown","6bdff7ef":"markdown","95497214":"markdown","ddbd29ff":"markdown","b5235efe":"markdown","2c3bf9e6":"markdown","ddd9ffff":"markdown","e8869226":"markdown"},"source":{"8590a7f1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","79826699":"X_train = pd.read_csv(\"\/kaggle\/input\/saas-2020-fall-cx-kaggle-compeition\/train_features.csv\")\ny_train = pd.read_csv(\"\/kaggle\/input\/saas-2020-fall-cx-kaggle-compeition\/train_targets.csv\")\nX_test = pd.read_csv(\"\/kaggle\/input\/saas-2020-fall-cx-kaggle-compeition\/test_features.csv\")\nsample_submission = pd.read_csv(\"\/kaggle\/input\/saas-2020-fall-cx-kaggle-compeition\/sample_submission.csv\")","10e7a6d2":"df = pd.concat((X_train, X_test), axis=0)","a7e84c2f":"# space for sick plots","2bd256a4":"bad_feature = []","328a3e01":"def feature_dropper(bad_feature):\n    for feature in bad_feature:\n        del X_train[feature]\n        del X_train[feature]","8f5b4f1d":"# optional\nfrom sklearn.decomposition import PCA\n# implement your own PCA function here","e4244df2":"def one_hot_encoding(df):\n    df = pd.get_dummies(df)\n    return df \n","853bf53a":"# Splitting up our engineered df back into training and test\nX_train = df[:X_train.shape[0]]\ny_train = y_train\nX_test = df[X_test.shape[0]:]","71f2865f":"from sklearn.metrics import mean_squared_error\n\ndef evaluate(y_pred, y_true):\n    \"\"\"Returns the RMSLE(y_pred, y_true)\"\"\"\n    return np.sqrt(mean_squared_error(y_true, y_pred))","abfadd47":"from sklearn.model_selection import train_test_split\n\ntrain_X, valid_X, train_y, valid_y = train_test_split(X_train, y_train, random_state=666)","03a4b04b":"#K-Fold Cross Validation code","32900db1":"from sklearn.linear_model import LinearRegression","a0264dfb":"# instantiating linear regression object (model)\nlm = LinearRegression()\n\n# fitting model on training sets\nlm.fit(train_X, train_y)\n\n# using model to predict on validation set\ny_valid_pred = lm.predict(valid_X)\n\n# IMPORTANT: This model is a \"dumb\" model that predicts negative values for some movie revenues\n# However, because we are using RMLSE we cannot have negative predictions\n# Ideally you create a better model that doesn't predict negative revenues\ny_valid_pred[y_valid_pred < 0] = 0\n\n# evaluating prediction on validation set\nevaluate(y_valid_pred, valid_y)","bd9ed8f1":"from sklearn.linear_model import Lasso","d52c005d":"# YOUR CODE HERE","667fc470":"from sklearn.model_selection import KFold\n\nkf = KFold(n_splits=5)\n\nalphas = [1e-3, 5e-3, 1e-2, 5e-2, 0.1]\n\ncv_scores = np.zeros(len(alphas))\n\nfor alphai, alpha in enumerate(alphas):\n    print('Training alpha =', alpha, end='\\t')\n    scores = np.zeros(5)\n    for i, (train_index, test_index) in enumerate(kf.split(X_train)):\n        # YOUR CODE HERE\n    cv_scores[alphai] = scores.mean()\n    print('RMSLE = ', cv_scores[alphai])","e5a6a515":"best_alpha = alphas[np.argmax(cv_scores)]\nbest_alpha","a0bfefb6":"model = Lasso(alpha=best_alpha)\nmodel.fit(train_X, np.log(train_y))\ntraining_accuracy = # YOUR CODE HERE\nvalidation_accuracy = # YOUR CODE HERE\n\nprint('Training accuracy', training_accuracy)\nprint('Validation accuracy', validation_accuracy)","ae3c8c6f":"from sklearn.ensemble import RandomForestRegressor","bddb041f":"# YOUR CODE HERE","901e76bb":"# YOUR CODE HERE","e0fefb17":"from xgboost import train","69eac485":"params = {\n    'eta': # YOUR CODE HERE\n    'max_depth': # YOUR CODE HERE\n    'subsample': # YOUR CODE HERE\n    'colsample_bytree': # YOUR CODE HERE\n    'silent': # YOUR CODE HERE\n}","af4d8c0b":"from xgb import run_xgb\nxgb_preds = run_xgb(...) # change this","ae98ea8c":"1d arra","14987d2f":"sample_submission.shape","37d47652":"#model here should be your best model based on your validation accuracy; pred should be an array\n\n#replace the following line with model.predict(X_test) or similar statements to generate your predictions\npred = np.ones((sample_submission.shape[0], 1))","3547f6cc":"pred.shape","9472fa15":"#sanity check: you are predictiing 9289 targets\n#this statement should return true\npred.shape[0] == 9289","b60f8796":"sample_submission['SALE PRICE'] = pred","57aa7f02":"sample_submission.to_csv(\"submission.csv\", index=False)","2518996a":"<span id=\"validation\"\/>\n\n### 3.1 Validation and Evaluation\n\nOur Kaggle competition uses Root-Mean-Square-Error (RMSE). In mathematical notation, it is:\n\n$$\\text{RMSE}(\\hat{y}, y) = \\sqrt{\\frac{1}{n} \\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}$$","258663d7":"### Dimensionality Reduction\nWhen the data has high dimensions, it is very useful to use PCA to lower the dimension of the data during feature engineering. \nSince we only have around 20 features, this is not necessary. But it could potentially help with your kaggle score.\nPS: PCA is designed for continuous variables, so maybe you should try ignore categorical columns for PCA.","187f29b5":"<span id=\"modeling\"\/>\n\n## 3. Modeling\n\nFor each of the models we try, make sure you also run the [Prediction](#prediction) cells at the bottom, so you can submit your predictions to the competition! This is how we'll be making sure you're keeping up with the project.","a1657fe8":"<span id=\"reg\" \/>\n\n### 3.3 Regularized Regression\n\nFit a [LASSO regression model](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.Lasso.html) to your data with $\\lambda = 1$","953144b9":"#### Validation\n\nUse the [train_test_split](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.train_test_split.html) function to split up your training data into a training set and a validation set. The default size of the validation set is 20% of the full training data here.","1b7dc3d3":"When we do EDA and feature engineering on a dataset, we often examine the training features and the test features together, so when you do complex feature engineering and data cleaning, you don't need to do twice or worry about your transformations not applying to test set.","60bb6fac":"### Feature Selection\nNot all features are useful. What are some features you can\/should get rid of in this dataset? And why should you get rid of them?","080263df":"<span id=\"random-forest\"\/>\n\n### 3.4 Random Forest\n\nFit a random forest model to your data and report your RMSE.\n\n**NOTE:** If you're finding that your model is performing worse than your linear regression, make sure you tune the parameters to the RandomForestRegressor!\n\nTry to understand what the parameters mean by looking at the Decision Trees lecture.","62bccc93":"#### K-Fold Cross Validation","72a1920e":"## Prediction","da27b066":"## One-Hot Encoding\nSome algorithms(ex. decision trees) can deal with categorical data directly while others need a bit pre-processing to do so.\nOne-Hot Encoding is one of many pre-processing methods to handle categorical data. For more information on one-hot encoding, read this article: https:\/\/machinelearningmastery.com\/why-one-hot-encode-data-in-machine-learning\/","2c9509d9":"<span id=\"eda\"><\/span>\n\n## 1. Exploratory Data Analysis\n\nProvide at least two plots that demonstrate interesting aspects of the dataset, and especially certain features' influence on the target variable, revenue.\n","7699ae04":"<span id=\"nn\" \/>\n\n### 3.5 Neural Network","19ca24e0":"## Feel Free to do more feature engineering on df! All the methods listed above are ones to help you get started.","ff96064f":"#### Evaluation\n\nComplete the function below.","93033a6a":"This section is optional.\n\nTrain a neural network on the data. Report your RMSE.\n\n**NOTE**: Neural Networks require a lot of time to train and it is better to use GPU to train them. Kaggle provides free weekly GPU usage(37 hours\/week). To use GPU, choose 'GPU' in the Accelerator from Settings located on the right side of your screen.","20113587":"## Submission","fadd9fcf":"# Career Exploration Kaggle Competition: Real Estate Price Prediction\n\n### Table Of Contents\n\n* [1. Exploratory Data Analysis](#eda)\n* [2. Feature Engineering](#feature-engineering)\n* [3. Modeling](#modeling)\n    * [3.1 Validation and Evaluation](#validation)\n    * [3.2 Linear Regression](#linear-regression)\n    * [3.3 Regularized Regression](#reg)\n    * [3.4 Random Forest](#random-forest)\n    * [3.5 Neural Network](#nn)\n    * [3.6 XGBoost](#xgb)\n\n\n### Hosted by and maintained by the [Students Association of Applied Statistics (SAAS)](https:\/\/saas.berkeley.edu).  Authored by Derek Cai(dcai@berkeley.edu).\n","6bdff7ef":"## 2. Feature Engineering\n\nThe data you are given is already pretty clean(no Nan values). But real datasets can get a lot messier and require a lot of data cleaning beforehand. As a general rule of thumb, data cleaning should be done before or with feature engineering.","95497214":"#### 3.3.1 Hyperparameter Tuning\n\nPerform [3-fold cross-validation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.KFold.html) on the parameter $\\lambda$, which is called **alpha** when you pass it into Lasso. Find the best parameter of $\\lambda \\in \\{0.001, 0.005, 0.01, 0.05, 0.1\\}$ and report the **RMSE** on the validation set if you use this parameter. \nPS: The given $\\lambda$ list may not contain the optimal $\\lambda$ for the model. Feel free to find better ones!","ddbd29ff":"<span id=\"xgb\" \/>\n\n### 3.6 XGBoost (Stretch)\n\nNow that we've tried many different types of classifiers, it's time to bring out the big guns.\n\nBelow are hyperparameters for an XGBoost model: tinker around with these to achieve the best validation score (below). Learn about what some of the hyperparameters mean [here](https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html#xgboost.train).\n\n**NOTE**: Feel free to reach out on slack if you run into any trouble <3","b5235efe":"<span id=\"linear-regression\"\/>\n\n### 3.2 Linear Regression\n\nFit a linear regression model to your data and report your RMSE.","2c3bf9e6":"## Data Loading","ddd9ffff":"The validation method above is usable but not that robust. K-Fold Cross-Validation should be better. Feel free to set up your own K-Fold cross-validation scheme. For more information, please read https:\/\/towardsdatascience.com\/cross-validation-a-beginners-guide-5b8ca04962cd.","e8869226":"## Note: Please upload\/share your work in the Notebook section after the competition deadline! This will give you a chance to showcase your work to other fellows in CX :)"}}