{"cell_type":{"cb9fddf7":"code","c6652667":"code","4742fe59":"code","3d0e36bc":"code","c50ca7d4":"code","d1f7a73f":"code","1b6afe3f":"code","2b5e4d29":"code","59a29493":"code","ebbbe425":"code","e648c862":"code","55987e2f":"code","d534974b":"code","0a247fa4":"code","0aa23057":"code","242490dd":"code","0be83b4a":"code","3e263752":"code","90981696":"code","3056e5f4":"code","0b78cd92":"code","e8e97c39":"markdown","f4d4d30b":"markdown","4f725a8e":"markdown"},"source":{"cb9fddf7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nplt.rcParams['figure.figsize'] = [20, 8]\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport gc\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","c6652667":"#https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df\n\n\ndef import_data(file, nrows):\n    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n    df = pd.read_csv(file, parse_dates=True, keep_date_col=True, nrows=500000)\n    df = reduce_mem_usage(df)\n    return df","4742fe59":"nrows = 500000 # for faster calculations\ntrain = import_data(\"..\/input\/train.csv\", nrows)","3d0e36bc":"dic = {'A': 0, 'B': 1, 'C': 2, 'D': 3}\ndic1 = {'CA':0,'DA':1,'SS':3,'LOFT':4}\ntrain[\"event\"] = train[\"event\"].apply(lambda x: dic[x])\ntrain[\"event\"] = train[\"event\"].astype('int8')\ntrain['experiment'] = train['experiment'].apply(lambda x: dic1[x])\ny = train.event\ntrain.drop(['event'], axis=1, inplace=True)","c50ca7d4":"train = np.array(train)\ny = np.array(y)","d1f7a73f":"def plot_y(y, text):\n    plt.hist(y)\n    plt.title('Target')\n    plt.ylabel('Count')\n    plt.xlabel(text)\n\ndef plot_data (train,y, text):\n    pca = PCA(n_components=2,copy=False)\n    train_pca = pca.fit_transform(train)\n\n    plt.scatter(train_pca[:,0], train_pca[:,1],c=y, edgecolor='none', alpha=0.9,\n            cmap=plt.cm.get_cmap('seismic', 4))\n    plt.title(text)\n    plt.xlabel('component 1')\n    plt.ylabel('component 2')\n    plt.colorbar()\n    del train, y","1b6afe3f":"plt.hist(y)\nplt.title('Target')\nplt.ylabel('Count')\nplt.xlabel('Target values')","2b5e4d29":"%%time\nplot_data(train, y, 'Original Data')","59a29493":"gc.collect()","ebbbe425":"%%time\nfrom imblearn.over_sampling import SMOTE, ADASYN\ntrain_sm, y_sm = SMOTE().fit_resample(train, y)","e648c862":"plot_y(y_sm, 'SMOTE')","55987e2f":"plot_data(train_sm, y_sm, 'SMOTE')","d534974b":"%%time\nfrom imblearn.over_sampling import SMOTE, ADASYN\ntrain_ad, y_ad = ADASYN().fit_resample(train, y)","0a247fa4":"plot_y(y_ad, 'ADASYN')","0aa23057":"plot_data(train_ad,y_ad ,'ADASYN')","242490dd":"%%time\nfrom imblearn.under_sampling import RandomUnderSampler\ncc = RandomUnderSampler(random_state=42)\ntrain_cc, y_cc = cc.fit_resample(train, y)","0be83b4a":"plot_y(y_cc, 'Random Under Sampler')","3e263752":"plot_data(train_cc, y_cc, 'Random Under Sampler')","90981696":"%%time\nfrom imblearn.under_sampling import RepeatedEditedNearestNeighbours\nrenn = RepeatedEditedNearestNeighbours()\ntrain_ren, y_ren= renn.fit_resample(train, y)","3056e5f4":"plot_y(y_ren, 'Repeated Edited Nearest Neighbours')","0b78cd92":"plot_data(train_ren, y_ren, 'Repeated Edited Nearest Neighbours')","e8e97c39":"More examples and documentation are on site: https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/user_guide.html","f4d4d30b":"****Under-sampling****","4f725a8e":"****Over-sampling****"}}