{"cell_type":{"29ba3a0a":"code","2f0a32b9":"code","3905e51b":"code","939da42c":"code","bf5c465d":"code","fb73d356":"code","67c1a890":"code","4cbf673a":"code","dd596a07":"code","c77084c8":"code","4445e0c5":"code","180713bd":"code","505f3cea":"code","cdb18482":"code","6bfab15c":"code","f02afc70":"code","12fa7c54":"markdown","b377f5ac":"markdown","1f9e030a":"markdown","10c24900":"markdown"},"source":{"29ba3a0a":"# Import the necessary packages\n\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport plotly.express as px\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport sklearn\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix\nfrom sklearn.metrics import r2_score, roc_auc_score, roc_curve, classification_report\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport warnings\nwarnings.filterwarnings('ignore')","2f0a32b9":"# Import and read dataset\n\ninput_ = \"..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv\"\ndata = pd.read_csv(input_)\ndf = data.copy()\n\ndata.head(10)","3905e51b":"data.describe()","939da42c":"inp_data = data.drop(data[['DEATH_EVENT']], axis=1)\nout_data = data[['DEATH_EVENT']]\n\nscaler = MinMaxScaler()\ninp_data = scaler.fit_transform(inp_data)\n\nX_train, X_test, y_train, y_test = train_test_split(inp_data, out_data, test_size=0.2, random_state=42)","bf5c465d":"print(\"X_train Shape : \", X_train.shape)\nprint(\"X_test Shape  : \", X_test.shape)\nprint(\"y_train Shape : \", y_train.shape)\nprint(\"y_test Shape  : \", y_test.shape)","fb73d356":"from sklearn.neural_network import MLPClassifier\n\nmlpc_model = MLPClassifier(random_state=42)\nmlpc_model.fit(X_train, y_train)\ny_pred = mlpc_model.predict(X_test)\n\nprint('Accuracy Score: {:.4f}'.format(accuracy_score(y_test, y_pred)))\nprint('SVC f1-score  : {:.4f}'.format(f1_score(y_pred, y_test)))\nprint('SVC precision : {:.4f}'.format(precision_score(y_pred, y_test)))\nprint('SVC recall    : {:.4f}'.format(recall_score(y_pred, y_test)))\nprint(\"\\n\",classification_report(y_pred, y_test))","67c1a890":"mlpc_model = MLPClassifier(random_state=42).fit(X_train, y_train)\ny_pred = mlpc_model.predict(X_test)\n\nprint('Accuracy Score: {:.4f}'.format(accuracy_score(y_test, y_pred)))\nprint('SVC f1-score  : {:.4f}'.format(f1_score(y_pred, y_test)))\nprint('SVC precision : {:.4f}'.format(precision_score(y_pred, y_test)))\nprint('SVC recall    : {:.4f}'.format(recall_score(y_pred, y_test)))\nprint(\"\\n\",classification_report(y_pred, y_test))","4cbf673a":"out_data.value_counts()","dd596a07":"activation = list(['identity', 'logistic', 'tanh', 'relu'])\nsolver = list(['lbfgs', 'sgd', 'adam'])\nalpha = list([0.0001, 0.05])\nlearning_rate = list(['constant','adaptive'])\nhidden_layer_sizes = list([(50,50,50), (50,100,50), (100,)])\n\nparam_grid = dict(\n    activation = activation,\n    solver = solver,\n    alpha = alpha,\n    learning_rate = learning_rate,\n    hidden_layer_sizes = hidden_layer_sizes\n)\n\nmlp = MLPClassifier(max_iter=100)\nclf = GridSearchCV(mlp, param_grid, cv=10, n_jobs=-1, verbose=2)\nclf.fit(X_train, y_train)\nclf.best_params_","c77084c8":"mlpc_model = MLPClassifier(\n    activation=\"identity\",\n    alpha=0.0001,\n    hidden_layer_sizes= (50, 50, 50),\n    learning_rate= 'constant',\n    solver= 'adam',\n    random_state=42).fit(X_train, y_train)\ny_pred = mlpc_model.predict(X_test)\n\nprint('Accuracy Score: {:.4f}'.format(accuracy_score(y_test, y_pred)))\nprint('SVC f1-score  : {:.4f}'.format(f1_score(y_pred, y_test)))\nprint('SVC precision : {:.4f}'.format(precision_score(y_pred, y_test)))\nprint('SVC recall    : {:.4f}'.format(recall_score(y_pred, y_test)))\nprint(\"\\n\",classification_report(y_pred, y_test))","4445e0c5":"cf_matrix = confusion_matrix(y_pred, y_test)\nsns.heatmap((cf_matrix \/ np.sum(cf_matrix)*100), annot = True, fmt=\".2f\", cmap=\"Blues\")","180713bd":"from imblearn.over_sampling import SMOTE\n\nsms = SMOTE(random_state=12345)\nX_res, y_res = sms.fit_sample(inp_data, out_data)\nX_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\n\nprint(\"X_train Shape : \", X_train.shape)\nprint(\"X_test Shape  : \", X_test.shape)\nprint(\"y_train Shape : \", y_train.shape)\nprint(\"y_test Shape  : \", y_test.shape)","505f3cea":"activation = list(['identity', 'logistic', 'tanh', 'relu'])\nsolver = list(['lbfgs', 'sgd', 'adam'])\nalpha = list([0.0001, 0.05])\nlearning_rate = list(['constant','adaptive'])\nhidden_layer_sizes = list([(50,50,50), (50,100,50), (100,)])\n\nparam_grid = dict(\n    activation = activation,\n    solver = solver,\n    alpha = alpha,\n    learning_rate = learning_rate,\n    hidden_layer_sizes = hidden_layer_sizes\n)\n\nmlp = MLPClassifier(max_iter=100)\nclf = GridSearchCV(mlp, param_grid, cv=10, n_jobs=-1, verbose=2)\nclf.fit(X_train, y_train)\nclf.best_params_","cdb18482":"mlpc_model = MLPClassifier(\n    activation=\"relu\",\n    alpha=0.05,\n    hidden_layer_sizes= (100,),\n    learning_rate= 'adaptive',\n    solver= 'lbfgs',\n    random_state=42).fit(X_train, y_train)\ny_pred = mlpc_model.predict(X_test)\n\nprint('Accuracy Score: {:.4f}'.format(accuracy_score(y_test, y_pred)))\nprint('SVC f1-score  : {:.4f}'.format(f1_score(y_pred, y_test)))\nprint('SVC precision : {:.4f}'.format(precision_score(y_pred, y_test)))\nprint('SVC recall    : {:.4f}'.format(recall_score(y_pred, y_test)))\nprint(\"\\n\",classification_report(y_pred, y_test))","6bfab15c":"cf_matrix = confusion_matrix(y_pred, y_test)\nsns.heatmap((cf_matrix \/ np.sum(cf_matrix)*100), annot = True, fmt=\".2f\", cmap=\"Blues\")","f02afc70":"scores = [] \nfor i in range(0,500): \n    X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2)\n    mlpc_model = MLPClassifier(\n        activation=\"relu\",\n        alpha=0.05,\n        hidden_layer_sizes= (100,),\n        learning_rate= 'adaptive',\n        solver= 'lbfgs',\n        random_state=42).fit(X_train, y_train)\n    y_pred = mlpc_model.predict(X_test)\n    scores.append(accuracy_score(y_pred, y_test)) \n\nplt.hist(scores)\nplt.show()","12fa7c54":"![](https:\/\/miro.medium.com\/max\/450\/0*9Cq7aMClmaYZitMs.png)\n\n- Softmax function: In classification tasks, we generally use a Softmax function as the Activation Function in the Output layer of the Multi Layer Perceptron to ensure that the outputs are probabilities and they add up to 1. The Softmax function takes a vector of arbitrary real-valued scores and squashes it to a vector of values between zero and one that sum to one. So, in this case,\n    - Probability (Pass) + Probability (Fail) = 1\n- tanh: takes a real-valued input and squashes it to the range [-1, 1]\n![](https:\/\/miro.medium.com\/proxy\/1*WNTLbBRWFiHPoXvyZ6s9eg.png)\n- ReLU: ReLU stands for Rectified Linear Unit. It takes a real-valued input and thresholds it at zero (replaces negative values with zero)\n    - f(x) = max(0, x)\n![](https:\/\/miro.medium.com\/max\/280\/0*5L8QNpkolk_u4Jce)\n\nThe below figures shows several other activation functions.\n![](https:\/\/miro.medium.com\/max\/677\/0*CAQoC6lJxfgiGSMV)\n\nImportance of Bias: The main function of Bias is to provide every node with a trainable constant value (in addition to the normal inputs that the node receives). See this link to learn more about the role of bias in a neuron.\nEvery Neural Network has 2 main parts:\n\n![](https:\/\/miro.medium.com\/proxy\/0*fBugyScIqJNFI3N6.png)\n- Feed Forward Propogation\/Forward Propogation.\n- Backward Propogation\/Back propogation.\n\nlets look at each individual parts.\n\n## Feed Forward Propogation:\nAll weights in the network are randomly assigned. Assume the weights of the connections from the inputs to that node are w1, w2 and w3.\nThe network then takes the first training example as input (we know that for inputs 35 and 67, the probability of Pass is 1).\n- Input to the network = [35, 67]\n- Desired output from the network (target) = [1, 0]\nThen output V from the node in consideration can be calculated as below (f is an activation function such as sigmoid):\n- V = f (1*w1 + 35*w2 + 67*w3)\nSimilarly, outputs from the other node in the hidden layer is also calculated. The outputs of the two nodes in the hidden layer act as inputs to the two nodes in the output layer. This enables us to calculate output probabilities from the two nodes in output layer.\nSuppose the output probabilities from the two nodes in the output layer are 0.4 and 0.6 respectively (since the weights are randomly assigned, outputs will also be random). We can see that the calculated probabilities (0.4 and 0.6) are very far from the desired probabilities (1 and 0 respectively), hence the network in Figure 5 is said to have an \u2018Incorrect Output\u2019.\n\n![](https:\/\/miro.medium.com\/max\/788\/0*ZgzgNhasdD-pg1DZ)\n\n## Back Propagation and Weight Updation:\nWe calculate the total error at the output nodes and propagate these errors back through the network using Backpropagation to calculate the gradients. Then we use an optimization method such as Gradient Descent to \u2018adjust\u2019 all weights in the network with an aim of reducing the error at the output layer.\n\nSuppose that the new weights associated with the node in consideration are w4, w5 and w6 (after Backpropagation and adjusting weights).\n![](https:\/\/miro.medium.com\/max\/788\/0*HGCUIWoEqxeOZrkf)\n\nIf we now input the same example to the network again, the network should perform better than before since the weights have now been adjusted to minimize the error in prediction. As shown in Figure 7, the errors at the output nodes now reduce to [0.2, -0.2] as compared to [0.6, -0.4] earlier. This means that our network has learnt to correctly classify our first training example.\n![](https:\/\/miro.medium.com\/max\/788\/0*FGPL1xL-vO2sP6K3)\n\nWe repeat this process with all other training examples in our dataset. Then, our network is said to have learnt those examples.\nIf we now want to predict whether a student studying 25 hours and having 70 marks in the mid term will pass the final term, we go through the forward propagation step and find the output probabilities for Pass and Fail.","b377f5ac":"## Reporting\nI evaluated the results I found with Confusion Matrix, the results are as follows:\n\n**Correctly predicted -> %81.67 (244 of 299 predict are correct)**\n- True Negative -> %56.67 -> Those who were predicted not to die and who did not die\n- True Positive -> %25.00 -> Those who were predicted to die and who did die\n\n**Wrong predicted-> %18.33 (55 of 406 predict are wrong)**\n- False Positive -> %16.67 -> Those who were predicted to die but who did not die\n- False Negative -> %01.67 -> Those who were predicted to not die but who did die","1f9e030a":"---\n\n## After the SMOTE process","10c24900":"![nn](https:\/\/i.ibb.co\/Qks8Skn\/nn-kaggle.png)\n\n- **ML Part 1** - Logistic Regression\n- **ML Part 2** - K-Nearest Neighbors (KNN)\n- **ML Part 3** - Support Vector Machine (SVM)\n- **ML Part 4 - Artificial Neural Network (NN)**\n- **ML Part 5** - Classification and Regression Tree (CART)\n- **ML Part 6** - Random Forests\n- **ML Part 7** - Gradient Boosting Machines (GBM)\n- **ML Part 8** - XGBoost\n- **ML Part 9** - LightGBM\n- **ML Part 10** - CatBoost\n\n---\n\n## What are Neural networks?\nNeural networks are set of algorithms inspired by the functioning of human brian. Generally when you open your eyes, what you see is called data and is processed by the Nuerons(data processing cells) in your brain, and recognises what is around you. That\u2019s how similar the Neural Networks works. They takes a large set of data, process the data(draws out the patterns from data), and outputs what it is.\n\n\n## What they do ?\nNeural networks sometimes called as Artificial Neural networks(ANN\u2019s), because they are not natural like neurons in your brain. They artifically mimic the nature and funtioning of Neural network. ANN\u2019s are composed of a large number of highly interconnected processing elements (neurones) working in unison to solve specific problems.\n\nANNs, like people,like child, they even learn by example. An ANN is configured for a specific application, such as pattern recognition or data classification,Image recognition, voice recognition through a learning process.\nNeural networks (NN) are universal function approximaters so that means neural networks can learn an approximation of any function f() such that.\n\ny = f(x)\n\n![](https:\/\/miro.medium.com\/max\/563\/0*aWIO7eB6E4-cIkK9.gif)\n\n## Why use Neural networks?\nNeural networks, with their remarkable ability to derive meaning from complicated or imprecise data, can be used to extract patterns and detect trends that are too complex to be noticed by either humans or other computer techniques. A trained neural network can be thought of as an \u201cexpert\u201d in the category of information it has been given to analyse. This expert can then be used to provide projections given new situations of interest and answer \u201cwhat if\u201d questions.\nOther advantages include:\n1. Adaptive learning: An ability to learn how to do tasks based on the data given for training or initial experience.\n2. Self-Organisation: An ANN can create its own organisation or representation of the information it receives during learning time.\n\n## Network layers\nThe commonest type of artificial neural network consists of three groups, or layers, of units: a layer of \u201cinput\u201d units is connected to a layer of \u201chidden\u201d units, which is connected to a layer of \u201coutput\u201d units.\n- **Input units:** The activity of the input units represents the raw information that is fed into the network. this also called input layer.\n- **Hidden units:** The activity of each hidden unit is determined by the activities of the input units and the weights on the connections between the input and the hidden units. this also called hidden layer.\n- **Output units:** The behaviour of the output units depends on the activity of the hidden units and the weights between the hidden and output units. this also called output layer.\n\n![](https:\/\/miro.medium.com\/max\/563\/0*BSxP3AHxBe_IevHC.png)\n\nyou can check several other Neural networks and their layers here. so that you will come to know how they looks.\nThis simple type of network is interesting because the hidden units are free to construct their own representations of the input. The weights between the input and hidden units determine when each hidden unit is active, and so by modifying these weights, a hidden unit can choose what it represents.\nBefore we look into entire\/Deep Neural Network lets look into a single neuron.\n\n## A Single Neuron\nThe basic unit of computation in a neural network is the neuron, often called as a node or unit. It receives input from some other nodes, or from an external source and computes an output. Each input has an associated weight (w), which is assigned on the basis of its relative importance to other inputs. The node applies a function f (defined below) to the weighted sum of its inputs as in figure below.\n\n![](https:\/\/miro.medium.com\/max\/630\/0*AtZ_izuDW8SDH53r.png)\n\nThe above network takes numerical inputs X1 and X2 and has weights w1 and w2 associated with those inputs. Additionally, there is another input 1 with weight b (called the Bias) associated with it.\n\n## Activation function:\nThe output Y from the neuron is computed as shown in the Figure above. The function f is non-linear and is called the Activation Function. The purpose of the activation function is to introduce non-linearity into the output of a neuron. This is important because most real world data is non linear and we want neurons to learn these non linear representations.\nEvery activation function (or non-linearity) takes a single number and performs a certain fixed mathematical operation on it. There are several activation functions you may encounter in practice:\n- Sigmoid: takes a real-valued input and squashes it to range between 0 and 1\n    - \u03c3(x) = 1 \/ (1 + exp(\u2212x))"}}