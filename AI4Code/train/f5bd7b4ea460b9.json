{"cell_type":{"b94546c5":"code","0a882e04":"code","8022d47b":"code","495b8420":"code","4c016ea3":"code","6ed97aa0":"code","cd2eca1d":"code","7fb165fd":"code","63610feb":"code","7ea165ed":"code","5ab6c7f5":"code","ec05072e":"code","52e3916f":"code","5306d7a2":"code","db9c55b8":"code","6258225c":"code","8d6c12cb":"code","62b4be48":"code","6ac79338":"code","0f2af7b8":"code","7be63a2b":"code","6a724f5e":"code","c33c76b4":"code","0ccfdbab":"code","9776a548":"code","0051d73c":"code","b2f87509":"code","0707a5a7":"code","b177b072":"code","384e4c79":"code","8f01ef51":"code","a0771e4d":"code","161c14f0":"markdown","522b1b19":"markdown","f0869990":"markdown","d9998926":"markdown","143f3505":"markdown","71704977":"markdown","558bbe6c":"markdown","169dc663":"markdown","4866bc03":"markdown","11327a6e":"markdown","e47368e6":"markdown","32799504":"markdown","e80ebb23":"markdown","242022a0":"markdown","f5315e5d":"markdown","93d335f8":"markdown","bb23a22a":"markdown","f9483156":"markdown","cd916785":"markdown"},"source":{"b94546c5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0a882e04":"import matplotlib\nimport seaborn as sns\n%matplotlib inline\n%config InlineBackend.figure_format = 'svg'\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\nimport pandas as pd\nimport numpy as np\n\nfrom tqdm import tqdm\n\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.datasets import make_blobs\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom ipywidgets import interactive\n\nfrom collections import defaultdict\n\n\nimport folium\nimport re\n\n\ncols = ['#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231', '#911eb4',\n        '#46f0f0', '#f032e6', '#bcf60c', '#fabebe', '#008080', '#e6beff', \n        '#9a6324', '#fffac8', '#800000', '#aaffc3', '#808000', '#ffd8b1', \n        '#000075', '#808080']*10","8022d47b":"pip install hdbscan","495b8420":"df = pd.read_csv(\"..\/input\/taxi-data\/taxi_data.csv\")\ndf.head()","4c016ea3":"print(f\" We have {df.duplicated(subset=['LON','LAT']).sum()} dublicates in the dataset\")","6ed97aa0":"print(f\" We have {df.isnull().sum()} missing values in the dataset\")","cd2eca1d":"df.isnull().sum()","7fb165fd":"print(f\"Dataset shape before dublicates and misisgn values {df.shape}\")\ndf.dropna(inplace=True)\ndf.drop_duplicates(subset=['LON','LAT'],keep=\"first\",inplace=True)\nprint(f\"Dataset shape after dublicates and misisgn values {df.shape}\")","63610feb":"X = df[['LON','LAT']]\nX","7ea165ed":"sns.jointplot(data=df,x='LON',y='LAT')","5ab6c7f5":"#This looks like California all right, but other than that it is hard to see any particular pattern. \n#Setting the alpha option to 0.1 makes it much easier to visualize the places where there is a high density of data points\ndf.plot(x=\"LON\",y=\"LAT\",kind=\"scatter\",alpha=0.4,figsize=(20,15))\n#Now that\u2019s much better: you can clearly see the high-density areas, namely the Bay Area and around Los Angeles and San Diego","ec05072e":"taxi_map = folium.Map(location=[df[\"LAT\"].mean(),df[\"LON\"].mean(),],zoom_start=9,tiles=\"Stamen Toner\")\ntaxi_map # This is the map we will use","52e3916f":"taxi_map = folium.Map(location=[df[\"LAT\"].mean(),df[\"LON\"].mean(),],zoom_start=9,tiles=\"OpenStreetMap\")\nfor row_number, row in df.iterrows():\n    folium.CircleMarker(\n        location = [row[\"LAT\"],row[\"LON\"]],\n        radius = 5,\n        popup = df[\"NAME\"],\n        color= \"#1787FE\",\n        fill=True,\n        fill_color=\"yellow\").add_to(taxi_map)\ntaxi_map    ","5306d7a2":"from IPython.display import Image\nurl=\"https:\/\/i.stack.imgur.com\/FQhxk.jpg\"\nImage(url,width=800, height=800)","db9c55b8":"url=\"https:\/\/i.stack.imgur.com\/vc01j.png\"\nImage(url,width=800, height=800)\n","6258225c":"from sklearn.cluster import KMeans\nloss=list()\nfor i in range(1,100):\n    kmeans=KMeans(n_clusters= i, init=\"k-means++\")\n    kmeans.fit(X)\n    loss.append(kmeans.inertia_)\nsns.set_style(\"darkgrid\")\nplt.figure(figsize=(12,10))\nplt.plot(range(1,100), loss)\nplt.title(\"Elbow Method\")\nplt.xlabel(\"Number of Cluster\")  \nplt.ylabel(\"loss\")\nplt.show()","8d6c12cb":"kmeans=KMeans(n_clusters=20,init=\"k-means++\")\npredicted_clusters=kmeans.fit_predict(X)\nsilhouette_score(X,predicted_clusters)","62b4be48":"cluster_df=pd.DataFrame(predicted_clusters,columns=[\"KMeans Clusters\"])\nnew_df=pd.concat([X, cluster_df], axis=1)\nnew_df","6ac79338":"new_df.dropna(inplace=True)","0f2af7b8":"new_df.groupby(\"KMeans Clusters\").mean()\n#It is apparnt that Annual Income and Spending Score plays important role in the number of clusters","7be63a2b":"plt.figure(figsize=(12,10))\nsns.scatterplot(x=new_df[\"LAT\"],y= new_df[\"LON\"],hue=new_df[\"KMeans Clusters\"],palette=\"magma\")","6a724f5e":"k=20\ndef create_map(df, cluster_column):\n    taxi_map = folium.Map(location=[df[\"LAT\"].mean(), df[\"LON\"].mean()], zoom_start=9, tiles='OpenStreetMap')\n\n    for row_number, row in df.iterrows():\n\n    # get a colour\n        #cluster_colour = cols[row[cluster_column]]\n\n        folium.CircleMarker(\n                            location= [row[\"LAT\"],row[\"LON\"]],\n                            radius=5,\n                            popup= row[cluster_column],\n                            color=\"yellow\",\n                            fill=True,\n                            fill_color=\"blue\"\n    ).add_to(taxi_map)\n    return taxi_map\n\nclustered_map = create_map(new_df, \"KMeans Clusters\")\nprint(f'K={k}')\nprint(f'Silhouette Score: {silhouette_score(X, predicted_clusters)}')\n\ntaxi_map.save('kmeans_20.html')\n","c33c76b4":"clustered_map","0ccfdbab":"best_silhouette, best_k = -1, 0\n\nfor k in tqdm(range(2, 100)):\n    model = KMeans(n_clusters=k, random_state=1).fit(X)\n    class_predictions = model.predict(X)\n    \n    curr_silhouette = silhouette_score(X, class_predictions)\n    if curr_silhouette > best_silhouette:\n        best_k = k\n        best_silhouette = curr_silhouette\n        \nprint(f'K={best_k}')\nprint(f'Silhouette Score: {best_silhouette}') ","9776a548":"len(X)","0051d73c":"model = DBSCAN(eps=0.01, min_samples=5).fit(X)\nclass_predictions = model.labels_\nclass_predictions","b2f87509":"kmeans=KMeans(n_clusters=20,init=\"k-means++\")\npredicted_clusters=kmeans.fit_predict(X)\nsilhouette_score(X,predicted_clusters)","0707a5a7":"from scipy.cluster import hierarchy \nhier=hierarchy.dendrogram(hierarchy.linkage(X, method=\"ward\"))\nplt.title(\"Dendogram of Hierarchical Clustering\")\nplt.xlabel(\"Observation Points\")\nplt.ylabel(\"Euclidean Distance\")\nplt.show() ","b177b072":"from sklearn.cluster import AgglomerativeClustering\nac=AgglomerativeClustering(n_clusters=20, affinity=\"euclidean\",linkage=\"ward\")\nagglomerative_clusters= ac.fit_predict(X)\nagglomerative_clusters","384e4c79":"silhouette_score(X,agglomerative_clusters)","8f01ef51":"df3= pd.DataFrame(agglomerative_clusters, columns=[\"Agglomerative Clusters\"])\nnew_df=pd.concat([new_df,df3],axis=1)\nnew_df","a0771e4d":"fig, axes = plt.subplots(1, 2, figsize=(18, 10), sharey=True)\nfig.suptitle('Distribution of Cluster')\n\n# KMeans Clustering\nsns.scatterplot(ax=axes[0], x=new_df[\"LAT\"], y=new_df[\"LON\"],hue=new_df[\"KMeans Clusters\"],palette=\"viridis\")\naxes[0].set_title(\"According to KMeans Clusters\")\n#Agglomerative Clustering\nsns.scatterplot(ax=axes[1], x=new_df[\"LAT\"], y=new_df[\"LON\"],hue=new_df[\"Agglomerative Clusters\"],palette=\"viridis\")\naxes[0].set_title(\"According to Agglomerative Clusters\")","161c14f0":"## 2.VISUALIZING GEOGRAPHICAL DATA","522b1b19":"<font color=\"purple\">\nBased on a set of points (let\u2019s think in a bidimensional space as exemplified in the figure), DBSCAN groups together points that are close to each other based on a distance measurement (usually Euclidean distance) and a minimum number of points. It also marks as outliers the points that are in low-density regions.\nParameters:\nThe DBSCAN algorithm basically requires 2 parameters:\neps: specifies how close points should be to each other to be considered a part of a cluster. It means that if the distance between two points is lower or equal to this value (eps), these points are considered neighbors.\nminPoints: the minimum number of points to form a dense region. For example, if we set the minPoints parameter as 5, then we need at least 5 points to form a dense region.\n\neps: if the eps value chosen is too small, a large part of the data will not be clustered. It will be considered outliers because don\u2019t satisfy the number of points to create a dense region. On the other hand, if the value that was chosen is too high, clusters will merge and the majority of objects will be in the same cluster. The eps should be chosen based on the distance of the dataset (we can use a k-distance graph to find it), but in general small eps values are preferable.\nminPoints: As a general rule, a minimum minPoints can be derived from a number of dimensions (D) in the data set, as minPoints \u2265 D + 1. Larger values are usually better for data sets with noise and will form more significant clusters. The minimum value for the minPoints must be 3, but the larger the data set, the larger the minPoints value that should be chosen","f0869990":"## 3. KMEANS CLUSTERING","d9998926":"## 4. DBSCAN","143f3505":"## 3.3. Implementing K Means Clustering and Evaluation the Performance of the Algorithm","71704977":"<font color=\"purple\">\nAs we can see, we can have best cluster value when number of cluster is equal to 16","558bbe6c":"<font color=\"purple\">\nThe first observation about data is shown in top-left of the figure above.\n\nIn the step 1 in the algorithm, each observation is randomly assigned to a cluster.\n\nIn the step 2a in the algorithm,the cluster centroid for each cluster is computed, which are shown as large colored disk as shown top-right of the figure.\n\nInitially these centroids are almost overlapping as we can see from the figure because initial cluster assignments are chosen randomly.\n\nIn the step 2a in the algorithm(bottom-left of the figure above), each observation is assigned to the neares centroid.\n\nIn bottom-center of the figure above, step 2a once again is performed which lead to new cluster centroids.\n\nWe basically keep repeating these steps until there is no new cluster which means data points are being reassigned to a new cluster centroid.\n\nAt the bottom-right, we have the results obtained after about 10 iterations","169dc663":"<font color=\"purple\">\nAnother method to find best k value is to use silhouette_score. Silhouette score is used to evaluate the quality of clusters created using clustering algorithms such as K-Means in terms of how well samples are clustered with other samples that are similar to each other.\n\nThe value of Silhouette score varies from -1 to 1. If the score is 1, the cluster is dense and well-separated than other clusters. A value near 0 represents overlapping clusters with samples very close to the decision boundary of the neighbouring clusters. A negative score [-1, 0] indicate that the samples might have got assigned to the wrong clusters.","4866bc03":"<font color=\"purple\">\nThe algorithm\n\n  -Choose a number of Clusters K\n    \n  -Randomly assign each point to a specific cluster\n    \n  -Until clusters stop changing, repeat the following steps:\n    \n  -For each cluster, compute the cluster's centroid by taking the mean vector points in the cluster\n    \n  -Assign each data point to the cluster for which the centroid is the closest","11327a6e":"<font color=\"purple\">","e47368e6":"## 3.1. How The Algorithm Works:","32799504":"<font color=\"purple\">\nAs we can see the output above, 5 rows has been dropped from the dataset which is not sp much to affect out future work.","e80ebb23":"## 5.Hierarchical Clustering","242022a0":"## 1. EXPLORATORY DATA ANALYSIS","f5315e5d":"<font color=\"purple\">\nTo perform K Mean Clustering, we have to decide how many clusters we expect in the data.\n\nThere is no easy answer for choosing a best K Value, but we can use elbow method to achieve a good K Value for the algorithm.\n\nThe basic idea behind this method is that it plots the various values of cost with changing k. As the value of K increases, there will be fewer elements in the cluster. So average distortion will decrease. The lesser number of elements means closer to the centroid. So, the point where this distortion declines the most is the elbow point.\n\nFirst of all, we compute the sum of squared errors(SSE) for some values of K(example, 2,3,4,6,etc).The SSE is the sum of squared distance between each member of the cluster and its centroid.\n\nIf we plot K value against the SSE,the error decreases as the K Value increases because if the number of cluster increases, they should be smaller. Accordingly the distortion becomes also smaller.\n\nFrom this perspective, the idea of using elbow method is to choose K Value at which SSE decreases abruptly, and this produces an elbow effect as we can see in the following picture.","93d335f8":"<font color=\"purple\">\nNow we will fill map with the data we have","bb23a22a":"<font color=\"purple\">\nFirst we will find out the dublucates and mising values.","f9483156":"## 3.2. How to Find Best k cluster:","cd916785":"<font color=\"purple\">\nIn the plot above, we see the number of cluster on the x-axis and within group sum of squares.\n\nWe try to choose a K Value where we won't get much information by increasing the number of classes, which means that we will not significantly within groups sum of squares by increase the number of clusters.\n\nThe number 2 or 3 is the most ideal in this plot because it is the first point the elbow shape happens and avoid to increase the number clusters more."}}