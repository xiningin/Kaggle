{"cell_type":{"db6814b0":"code","4af7906a":"code","f89394c8":"code","3912fd75":"code","99137a6b":"code","2f08ae68":"code","f288701a":"code","a4442117":"code","1e815e57":"code","fb9e7380":"code","b62548af":"code","a871e33b":"code","bb32dbe3":"code","36cb45f6":"code","b35e8cb8":"code","4eaa325a":"code","a7f53338":"code","2768ab15":"code","092ba2be":"code","7c1a1883":"code","832ae6e0":"code","2ab9386b":"code","44594b22":"markdown"},"source":{"db6814b0":"import numpy as np\nimport pandas as pd\nimport os\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\n\nfrom skimage.color import rgb2lab, lab2rgb, rgb2gray, gray2rgb\n\nfrom tqdm import tqdm\nimport torch\nfrom torch import nn\nfrom torchvision import models,datasets\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport torchvision.transforms as transforms\nimport torchvision\nfrom torchvision.utils import make_grid\nimport warnings\nwarnings.filterwarnings(action='ignore')\ntorch.manual_seed(0)","4af7906a":"sample_img = cv2.imread('..\/input\/100-bird-species\/train\/AFRICAN CROWNED CRANE\/001.jpg')\nb,g,r = cv2.split(sample_img)\nimg2 = cv2.merge([r,g,b])\nplt.imshow(img2)\nprint(img2.shape)","f89394c8":"def RGB2LAB(data):\n    data = data.numpy()\n    data = data.transpose(1,2,0)\n    lab = rgb2lab(data)\n    lab = (lab + [0,128,128]) \/ [100,255,255]\n    return torch.from_numpy(lab.transpose(2,0,1))","3912fd75":"def extractGray(batchSize, yuv):\n    lst = []\n    for data in yuv:\n        lst.append(data[0])\n    return np.asarray(lst).reshape(batchSize, 1, 224, 224)","99137a6b":"train_root = '..\/input\/100-bird-species\/train'\nvalid_root = '..\/input\/100-bird-species\/valid'\ntrain_transform = transforms.Compose([\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n])\ntrain_dataset = datasets.ImageFolder(train_root,transform=train_transform)\nvalid_dataset = datasets.ImageFolder(valid_root,transform=train_transform)\nindex = list(range(len(train_dataset)))\nsampler = SubsetRandomSampler(index[:30000])\nvalid_loader = DataLoader(valid_dataset,batch_size=128,drop_last=True)\ntrain_loader = DataLoader(train_dataset,batch_size=128,drop_last=True,sampler=sampler)","2f08ae68":"test_root = '..\/input\/100-bird-species\/test'\ntest_transform = transforms.Compose([\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n])\ntest_dataset = datasets.ImageFolder(test_root,transform=test_transform)\ntest_loader = DataLoader(test_dataset,batch_size=128,drop_last=True)","f288701a":"data,label = next(iter(train_loader))\nprint(data.shape,label.shape)","a4442117":"data,label = next(iter(valid_loader))\nprint(data.shape,label.shape)","1e815e57":"data,label = next(iter(test_loader))\nprint(data.shape,label.shape)","fb9e7380":"plt.imshow(data[1].permute(1,2,0))","b62548af":"class AutoEncoder(nn.Module):\n    \n    def __init__(self):\n        super(AutoEncoder, self).__init__()\n        # 1 x 224 x 224\n        self.conv1 = nn.Conv2d(1, 32, 3, 2, 1)\n        self.bn1 = nn.BatchNorm2d(32)\n        # 16 x 112 x 112\n        self.conv2 = nn.Conv2d(32,64 ,3, 2, 1)\n        self.bn2 = nn.BatchNorm2d(64)\n        # 32 x 56 x 56\n        self.conv3 = nn.Conv2d(64,128, 3, 2, 1)\n        self.bn3 = nn.BatchNorm2d(128)\n        # 64 x 28 x 28\n        self.conv4 = nn.Conv2d(128,256, 3, 2, 1)\n        self.bn4 = nn.BatchNorm2d(256)\n        # 128 x 14 x 14\n        self.conv5 = nn.Conv2d(256,512, 3, 1, 1)\n        self.bn5 = nn.BatchNorm2d(512)\n        # 256 x 14 x 14\n        self.conv6 = nn.Conv2d(512,256, 3, 1, 1)\n        self.bn6 = nn.BatchNorm2d(256)\n        # 256 x 28 x 28\n        self.conv7 = nn.Conv2d(256,128, 3, 1, 1)\n        self.bn7 = nn.BatchNorm2d(128)\n        # 128 x 56 x 56\n        self.conv8 = nn.Conv2d(128,64, 3, 1, 1)\n        self.bn8 = nn.BatchNorm2d(64)\n        # 64 x 112 x 112\n        self.conv9 = nn.Conv2d(64, 32, 3, 1, 1)\n        self.bn9 = nn.BatchNorm2d(32)\n        # 32 x 224 x 224\n        self.out  = nn.Conv2d(32,2, 3, 1, 1)\n        # 2 x 224 x 224\n        self.up = nn.Upsample(scale_factor=2)\n        self.swish = nn.Hardswish()\n        self.tanh = nn.Tanh()\n        \n    def forward(self, x):\n        down_1 = self.swish(self.bn1(self.conv1(x)))\n        down_2 = self.swish(self.bn2(self.conv2(down_1)))\n        down_3 = self.swish(self.bn3(self.conv3(down_2)))\n        down_4 = self.swish(self.bn4(self.conv4(down_3)))\n        down_5 = self.swish(self.bn5(self.conv5(down_4)))\n        up_1 = self.up(self.swish(self.bn6(self.conv6(down_5))))\n        up_2 = self.up(self.swish(self.bn7(self.conv7(up_1))))\n        up_3 = self.up(self.swish(self.bn8(self.conv8(up_2))))\n        up_4 = self.up(self.swish(self.bn9(self.conv9(up_3))))\n        out = self.tanh(self.out(up_4))\n        output = torch.cat([x,out],dim=1)\n        return output","a871e33b":"model = AutoEncoder()\nmodel","bb32dbe3":"batch_size = 32\nimg_size = 224\n\nsample_input = torch.ones(size=(batch_size,1,img_size,img_size))\noutput = model(sample_input)\nprint(output.size())","36cb45f6":"def show_tensor_images(image_tensor, num_images=16, size=(3, 224, 224)):\n    image_tensor = (image_tensor + 1) \/ 2\n    image_unflat = image_tensor.detach().cpu()\n    image_grid = make_grid(image_unflat[:num_images], nrow=4)\n    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n    plt.show()","b35e8cb8":"device = ('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\noptimizer = torch.optim.Adamax(model.parameters(),lr=0.0003)\ncriterion = nn.MSELoss()","4eaa325a":"def PSNR_loss(pred,real):\n    mse_loss = torch.nn.L1Loss()\n    mse = mse_loss(real,pred)\n    max_pixel = 255.0\n    psnr = 20 * torch.log10(max_pixel \/ torch.sqrt(mse))\n    return psnr","a7f53338":"n_epochs = 300\nbest_PSNR = 0\ntrain_loss = np.zeros(n_epochs)\nvalid_loss = np.zeros(n_epochs)\ntrain_psnr = np.zeros(n_epochs)\nvalid_psnr = np.zeros(n_epochs)\nPSNR = []\nfor e in range(n_epochs):\n    model.train()\n    print(f'epoch: {e+1}\/{n_epochs}')\n    for real, _ in tqdm(train_loader):\n        real = real.to(device).float()\n        \n        gray = extractGray(128,real.cpu().numpy())\n        grayv = torch.from_numpy(gray).to(device).float()\n        \n        optimizer.zero_grad()\n        train_logits = model(grayv)\n        loss = criterion(train_logits,real).float()\n        loss.backward()\n        \n        psnr = PSNR_loss(train_logits,real).float()\n        optimizer.step()\n        train_loss[e] += loss.item()\n        train_psnr[e] += psnr.item()\n    train_loss[e] \/= len(train_loader)\n    train_psnr[e] \/= len(train_loader)\n    model.eval()\n    with torch.no_grad():\n        for real,_ in tqdm(valid_loader):\n            real = real.to(device)\n            gray = extractGray(128,real.cpu().numpy())\n            grayv = torch.from_numpy(gray).to(device).float()\n            \n            logits = model(grayv)\n            loss = criterion(logits,real).float()\n            psnr = PSNR_loss(logits,real).float()\n            valid_loss[e] += loss.item()\n            valid_psnr[e] += psnr.item()\n    valid_loss[e] \/= len(valid_loader)\n    valid_psnr[e] \/= len(valid_loader)\n            \n    print(\"Epochs:{}\\tTrain_loss loss: {:.3f}\\tTrain_PSNR: {:.3f}\".format(\n            e+1,train_loss[e],train_psnr[e]))\n    print('Epochs:{}\\tVlidation_loss: {:.3f}\\tValidation_pSNR: {:.3f}'.format(\n            e+1,valid_loss[e],valid_psnr[e]))\n    show_tensor_images(logits)  \n    show_tensor_images(real)         \n    if best_PSNR < psnr:\n        best_PSNR = psnr\n        print('Validation PSNR is increased {:.3f} ---> {:.3f}'.format(best_PSNR,valid_psnr[e]))\n        torch.save(model,'.\/best_psnr_generator.pt')\n        patience = 0\n    else:\n        patience += 1\n        if patience > 30:\n            break\n            print('Training is meet Early Stopping so End Training...\\n Best PSNR:{:.3f}'.format(best_PSNR))\n","2768ab15":"plt.figure(figsize=(15,10))\nplt.plot(train_loss,label='train_loss')\nplt.plot(valid_loss,label='valid_loss')\nplt.legend()\nplt.grid()\nplt.show()","092ba2be":"plt.figure(figsize=(15,10))\nplt.plot(train_psnr,label='train_PSNR')\nplt.plot(valid_psnr,label='valid_PSNR')\nplt.legend()\nplt.grid()\nplt.show()","7c1a1883":"model = AutoEncoder()\nmodel = torch.load('.\/best_psnr_generator.pt')\nwith torch.no_grad():\n    model.eval()\n    for real,_ in test_loader:\n        real = real.to(device).float()\n        gray = extractGray(128,real.cpu().numpy())\n        grayv = torch.from_numpy(gray).to(device)\n        fake_img = model(grayv)\n        PSNR = PSNR_loss(fake_img,grayv).detach().cpu().numpy()","832ae6e0":"print('Result of Test set PSNR:{:.3f}'.format(PSNR))","2ab9386b":"out_img = torch.squeeze(fake_img.cpu().data)\nprint(out_img.size())\n\nfor i in range(10):\n    plt.subplot(1,2,1)\n    plt.imshow(torch.squeeze(real[i]).cpu().numpy().transpose(1,2,0))\n    plt.subplot(1,2,2)\n    plt.imshow(out_img[i].numpy().transpose(1,2,0))\n    plt.show()","44594b22":"# Model"}}