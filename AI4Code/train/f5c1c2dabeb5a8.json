{"cell_type":{"1ed651d2":"code","1d217e0e":"code","940d4104":"code","7f3efd8b":"code","ea7a898e":"code","7bd7d4f2":"code","42c63500":"code","1fbf7543":"code","8a37d863":"code","703c3d3f":"code","2e056157":"code","30537397":"code","12ba7528":"code","a5d8c448":"code","e34b6140":"markdown","e7213e57":"markdown","0821202d":"markdown","967be3f2":"markdown","b5043a59":"markdown","5aa963e4":"markdown","63e6ee62":"markdown","0fafcc96":"markdown","d07a58ce":"markdown","0b2d790d":"markdown","2b4bea7a":"markdown","2cec303f":"markdown","3bd24754":"markdown"},"source":{"1ed651d2":"#Install Text Processing Library\n!pip install texthero -q","1d217e0e":"# Generic\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n\n# TensorFlow\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\n#Library for Text Processing\nimport texthero as hero\n\n#Sk Learn Library\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n#Garbage\nimport gc\n\n#Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#Tabulate\nfrom tabulate import tabulate","940d4104":"# Raw Data\nurl = '..\/input\/fakenewsdetection\/news.csv'\nraw_data = pd.read_csv(url, header='infer')\n\n#For Simplicity we'll focus on Title & Label columns\ndata = raw_data[['title','label']]","7f3efd8b":"print(\"Total Records: \",data.shape[0])","ea7a898e":"#Creating a new column with processed text\ndata['title_clean'] = data['title'].pipe(hero.clean)","7bd7d4f2":"#Encode the Label to convert it into numerical values [Fake = 0; Real = 1]\nlab_enc = LabelEncoder()\n\n#Applying to the dataset\ndata['label'] = lab_enc.fit_transform(data['label'])","42c63500":"#Inspect\ndata.head()","1fbf7543":"# Data Split with Original Title Split into training[90%] & test[10%]\nx_train,x_test,y_train,y_test = train_test_split(data['title'], data.label, test_size=0.1, random_state=0)\n\n# Data Split with Cleaned Title Split into training[90%] & test[10%]\nxc_train,xc_test,yc_train,yc_test = train_test_split(data['title_clean'], data.label, test_size=0.1, random_state=0)","8a37d863":"# Pre-Trained Text Embedding Model & Layer Definition\nEmbed = 'https:\/\/tfhub.dev\/google\/tf2-preview\/gnews-swivel-20dim\/1'\nTrainable_Module = True\nhub_layer = hub.KerasLayer(Embed, input_shape=[], dtype=tf.string, trainable=Trainable_Module)\n\n# Build Model (Original Title Text)\nmodel = tf.keras.Sequential()\nmodel.add(hub_layer)           #pre-trained text embedding layer\nmodel.add(tf.keras.layers.Dense(16, activation='relu'))\nmodel.add(tf.keras.layers.Dense(1))\n\n# Build Model (Cleaned Title Text)\nmodel_c = tf.keras.Sequential()\nmodel_c.add(hub_layer)           #pre-trained text embedding layer\nmodel_c.add(tf.keras.layers.Dense(16, activation='relu'))\nmodel_c.add(tf.keras.layers.Dense(1))\n\n\nprint(\" -- Original Title Text Model Summary --\")\nmodel.summary()\nprint('\\n')\nprint(\" -- Cleaned Title Text Model Summary --\")\nmodel_c.summary()","703c3d3f":"# Model Compile (Original Title Text)\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy']\n              )\n\n# Model Compile (Cleaned Title Text)\nmodel_c.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy']\n              )","2e056157":"EPOCHS = 20           #feel free to change this\nBATCH_SIZE = 256      #feel free to change this\n\nhistory = model.fit(x_train,y_train, batch_size = BATCH_SIZE,\n                    epochs = EPOCHS, validation_split= 0.1,\n                    verbose=1)","30537397":"EPOCHS = 20           #feel free to change this\nBATCH_SIZE = 256      #feel free to change this\n\nhistory_c = model_c.fit(xc_train,yc_train, batch_size = BATCH_SIZE,\n                    epochs = EPOCHS, validation_split= 0.1,\n                    verbose=1)","12ba7528":"#Original Title Text\naccr = model.evaluate(x_test,y_test, verbose=0)\n#print(\"Model Accuracy on Original Title Text: \",'{:.2%}'.format(accr[1]))\n\n#Cleaned Title Text\naccr_c = model_c.evaluate(xc_test,yc_test, verbose=0)\n#print(\"Model Accuracy on Cleaned Title Text: \",'{:.2%}'.format(accr_c[1]))\n\ntab_data = [ [ \"Model Trained on Original Title Text\", '{:.2%}'.format(accr[0]), '{:.2%}'.format(accr[1]) ],\n             [ \"Model Trained on Cleaned Title Text\", '{:.2%}'.format(accr_c[0]), '{:.2%}'.format(accr_c[1]) ]\n           ]\n\nprint(tabulate(tab_data, headers=['','LOSS','ACCURACY'], tablefmt='pretty'))","a5d8c448":"# 5 Randomly Sampled Records from Original Title Text\npred_df = pd.DataFrame({ 'Random_News_Titles' : x_test.sample(n=5, random_state=1) })\npred_df.reset_index(inplace=True,drop=True)\n\n# Function to convert numerical label to string\ndef label2str(x):\n    if x == 0:\n        return \"Fake\"\n    else:\n        return \"Real\"\n\n# Function to highlight mismatching column values\ndef highlight(x):\n    y = 'yellow'\n    g = 'green'\n\n    mismtch = x['ModelPred_OgTxt'] != x['ModelPred_CleandTxt']\n    mtch = x['ModelPred_OgTxt'] == x['ModelPred_CleandTxt']\n    \n\n    df1 = pd.DataFrame('background-color: ', index=x.index, columns=x.columns)\n    \n    df1['ModelPred_OgTxt'] = np.where(mismtch, 'background-color: {}'.format(y), df1['ModelPred_OgTxt'])\n    df1['ModelPred_OgTxt'] = np.where(mtch, 'background-color: {}'.format(g), df1['ModelPred_OgTxt'])\n    \n    df1['ModelPred_CleandTxt'] = np.where(mismtch, 'background-color: {}'.format(y), df1['ModelPred_CleandTxt'])\n    df1['ModelPred_CleandTxt'] = np.where(mtch, 'background-color: {}'.format(g), df1['ModelPred_CleandTxt'])\n    \n    return df1    \n    \n\n# Add a new column with predictions\npred_df['ModelPred_OgTxt'] = [label2str(x) for x in model.predict_classes(pred_df.Random_News_Titles)]\npred_df['ModelPred_CleandTxt'] = [label2str(x) for x in model_c.predict_classes(pred_df.Random_News_Titles)]\n\n\n# Applying the highlight style to the prediction dataframe\npred_df.style.apply(highlight, axis=None)","e34b6140":"The layers are stacked sequentially to build the classifier:\n\n* The first layer is a TensorFlow Hub layer. This layer has our pre-trained Model to map a sentence into its embedding vector. The pre-trained text embedding model splits the sentence into tokens, embeds each token and then combines the embedding. The resulting dimensions are: (num_examples, embedding_dimension).\n\n* This fixed-length output vector is piped through a fully-connected (Dense) layer with 16 hidden units.\n\n* The last layer is densely connected with a single output node.\n\n\n### Loss function and optimizer\nOur model needs a loss function and an optimizer for training. Since the dataset has a binary classification and the model outputs logits (a single-unit layer with a linear activation), we'll use the binary_crossentropy loss function.","e7213e57":"## Libraries","0821202d":"## Data","967be3f2":"## Build Model\n\nIn this example, the input data consists of sentences. The labels to predict are either 0 (Fake) or 1 (Real).\n\nThe 'title' data (both orginal & cleaned) will need to be converted to embedding vectors and this can be done in first layer of our model. We will use the pre-trained \"text embedding\" as the first layer. The pros for doing this is:\n\n* No Need for Text Processing\n* Good Use of Tranfer Learning\n* Simple to process due to fixed size\n\nFor this tutorial we shall use the TensorFlow's pre-trained text embedding model details of which are mentioned below:\n\n**Model** = [tf2-preview\/gnews-swivel-20dim](https:\/\/tfhub.dev\/google\/tf2-preview\/gnews-swivel-20dim\/1)\n\nThis is a small model which has been trained 130GB corpus and has 20 dimensions. \n\n\nThere are few other text embedding models to choose from, however they are out-of-scope for this tutorial:\n\n* [google\/tf2-preview\/gnews-swivel-20dim-with-oov\/1](https:\/\/tfhub.dev\/google\/tf2-preview\/gnews-swivel-20dim-with-oov\/1) - same as google\/tf2-preview\/gnews-swivel-20dim\/1, but with 2.5% vocabulary converted to OOV buckets. This can help if vocabulary of the task and vocabulary of the model don't fully overlap.\n\n* [google\/tf2-preview\/nnlm-en-dim50\/1](https:\/\/tfhub.dev\/google\/tf2-preview\/nnlm-en-dim50\/1) -  much larger model with ~1M vocabulary size and 50 dimensions.\n\n* [google\/tf2-preview\/nnlm-en-dim128\/1](https:\/\/tfhub.dev\/google\/tf2-preview\/nnlm-en-dim128\/1) - even larger model with ~1M vocabulary size and 128 dimensions.\n\n\n\nNow let us first create a Keras layer that uses a TensorFlow Hub model to embed the sentences. Note that no matter the length of the input text, the output shape of the embeddings is: (num_examples, embedding_dimension).\n\n\n\n**Note: Just for the sake of this tutorial we're going to create 2 seperate Models that we'll train on Original Title Text & Cleaned Title Text.**","b5043a59":"## Evaluate Model","5aa963e4":"## Prediction","63e6ee62":"## Data Split","0fafcc96":"# Text Classification - Transfer Learning (Tutorial)\n\n**Model** = [tf2-preview\/gnews-swivel-20dim](https:\/\/tfhub.dev\/google\/tf2-preview\/gnews-swivel-20dim\/1)\n\nIn this notebook, we're going to learn to use a **Pre-Trained TensorFlow Model** to perform a **Text Classification**. We'll download the Model from TensorFlow Hub and Train it on the [Fake New Detection Dataset](https:\/\/www.kaggle.com\/prakashkumar27\/fakenewsdetection). \n\nThe notebook classifies news as REAL or FAKE using the text. This is an example of binary\u2014or two-class\u2014classification, an important and widely applicable kind of machine learning problem. This tutorial demonstrates the basic application of transfer learning with TensorFlow Hub and Keras.\n\nThe notebook uses tf.keras, a high-level API to build and train models in TensorFlow, and TensorFlow Hub, a library and platform for transfer learning.\n\nThe notebook is specifically designed as a starter-code so that anyone who is interested in performing Text Classification using a Pre-Trained Model can simply fork it. \n\nAs always, I'll keep this notebook clean & well organized for easy understanding. Please do consider it to UPVOTE if you find it useful :-).\n","d07a58ce":"**Note: In this tutorial we are going to train our Model on both the orginal and the cleaned title text.**","0b2d790d":"## Train Model - Cleaned Title Text","2b4bea7a":"**As observed above, our Model's accuracy is slightly better with Original Title Text.**","2cec303f":"## Train Model - Original Title Text","3bd24754":"## Data Prep"}}