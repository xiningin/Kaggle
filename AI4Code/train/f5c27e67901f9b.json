{"cell_type":{"aa4a899c":"code","586334cb":"code","c6d710db":"code","37f1c934":"code","58de5046":"code","02e69b6d":"code","8d41a947":"code","7b17c713":"code","2c23f717":"code","05149029":"code","6e5623cf":"code","1f84851b":"code","b59775ab":"code","a1396a1c":"code","6aeb8793":"code","813d6064":"code","dc37faef":"code","0ad30389":"markdown"},"source":{"aa4a899c":"import os,time,tqdm,random,gc\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom plotly.offline import init_notebook_mode, iplot, plot\nimport plotly as py\nimport plotly.express as px\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nfrom sklearn.utils import shuffle\nfrom IPython.display import clear_output\n\n# !pip uninstall -y transformers\n# !pip install transformers\n!pip install nlp\n\nimport transformers\nimport tokenizers\nimport nlp\nimport tensorflow as tf\n\n\nos.environ[\"WANDB_API_KEY\"] = \"0\"\n\ndef seed_all(seed=2001):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n    \ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync)\n  \n    \nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","586334cb":"model_name = 'bert-base-multilingual-cased'\nmax_len = 50","c6d710db":"original = pd.read_csv('..\/input\/contradictory-my-dear-watson\/train.csv')\nmnli = nlp.load_dataset(path='glue', name='mnli')\n\nclear_output(wait=True)","37f1c934":"mnli_train = pd.DataFrame(mnli['train'])\nmnli_valid1 = pd.DataFrame(mnli['validation_matched'])\nmnli_valid2 = pd.DataFrame(mnli['validation_mismatched'])\n\nmnli = pd.concat([mnli_train,mnli_valid1,mnli_valid2])","58de5046":"mnli = mnli[['premise','hypothesis','label']]\nmnli = mnli.rename(columns = {0 : 'premise', 1: 'hypothesis',2: 'label' })","02e69b6d":"original = original[['premise','hypothesis','label']].sample(len(original)\/\/(8*strategy.num_replicas_in_sync)*8*strategy.num_replicas_in_sync)\nlen(original)","8d41a947":"train = mnli\ntrain","7b17c713":"tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)","2c23f717":"%%time\ntrain_encoded = tokenizer.batch_encode_plus(train[['premise','hypothesis']].values.tolist(),pad_to_max_length=True,max_length=max_len,return_tensors='tf')\nvalid_encoded = tokenizer.batch_encode_plus(original[['premise','hypothesis']].values.tolist(),pad_to_max_length=True,max_length=max_len,return_tensors='tf')","05149029":"def build_model(l):\n    bert_encoder = transformers.TFBertModel.from_pretrained(model_name)\n    \n    input_words_ids = tf.keras.layers.Input(shape=(max_len),dtype=tf.int32,name='input_ids')\n    input_mask = tf.keras.layers.Input(shape=(max_len,),dtype=tf.int32,name='attention_mask')\n    input_type_ids = tf.keras.layers.Input(shape=(max_len,),dtype=tf.int32,name='token_type_ids')\n    \n    embedding = bert_encoder([input_words_ids,input_mask,input_type_ids])[0]\n\n    \n    output = tf.keras.layers.Dense(3,activation='softmax')(embedding[:,0,:])\n    \n    \n    model = tf.keras.models.Model(inputs=[input_words_ids,input_mask,input_type_ids],outputs=output)\n    \n\n    model.compile(\n        optimizer = tf.keras.optimizers.Adam(lr=l),\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    return model\n\nwith strategy.scope():\n    model = build_model(l=1e-5)\n    model.summary()","6e5623cf":"hist = model.fit(dict(train_encoded),train.label.values,epochs=10,batch_size=128*strategy.num_replicas_in_sync,verbose=1,validation_data=(dict(valid_encoded),original.label.values),validation_batch_size=8*strategy.num_replicas_in_sync)","1f84851b":"test = pd.read_csv('..\/input\/contradictory-my-dear-watson\/test.csv')\n\ntest_encoded = tokenizer.batch_encode_plus(test[['premise','hypothesis']].values.tolist(),pad_to_max_length=True,max_length=max_len,return_tensors='tf')","b59775ab":"preds = [np.argmax(i) for i in model.predict(dict(test_encoded))]\n\nsubmission = test.id.copy().to_frame()\nsubmission['prediction'] = preds\nsubmission.to_csv('submission.csv',index=False)","a1396a1c":"hist_df = pd.DataFrame(hist.history)\nhist_df['epoch'] = np.arange(1,len(hist_df)+1)","6aeb8793":"hist_df","813d6064":"py.offline.init_notebook_mode()\ntrain_acc =go.Scatter(x=hist_df['epoch'],y=hist_df['accuracy'],mode = \"lines+markers\",name='train_acc')\nval_acc =go.Scatter(x=hist_df['epoch'],y=hist_df['val_accuracy'],mode = \"lines+markers\",name='valid_acc')\n\ndata = [train_acc, val_acc]\nlayout = dict(title = 'Accuracy',\n              xaxis= dict(title= 'epoch',ticklen= 1,zeroline= False)\n             )\n\nfig = dict(data = data, layout = layout)\niplot(fig)","dc37faef":"train_acc =go.Scatter(x=hist_df['epoch'],y=hist_df['loss'],mode = \"lines+markers\",name='loss')\nval_acc =go.Scatter(x=hist_df['epoch'],y=hist_df['val_loss'],mode = \"lines+markers\",name='valid_loss')\n\ndata = [train_acc, val_acc]\nlayout = dict(title = 'Loss',\n              xaxis= dict(title= 'epoch',ticklen= 1,zeroline= False)\n             )\n\nfig = dict(data = data, layout = layout)\niplot(fig)","0ad30389":"## Let's use the original dataset as a validation one "}}