{"cell_type":{"e6640a70":"code","c6c19cb0":"code","f7f84149":"code","23b6d470":"code","7ba544c0":"code","79255a88":"code","ae3a64ff":"code","6d124fa4":"code","369996cb":"code","c76e97fb":"code","ed5905fe":"code","5c7d6889":"code","09c94657":"code","5b3a4776":"code","f5a56f03":"code","396d2fbd":"code","fa497205":"code","62c24dc5":"code","2b254e42":"code","2f679719":"code","fd8b7ac0":"code","a7b3ca70":"code","6227e84d":"code","99137139":"code","6af06757":"code","6d08dff0":"code","08dfa56d":"code","1e6af18d":"code","b90b2bf4":"code","37f1fdae":"markdown","460bc5ee":"markdown","390a676f":"markdown","ba2f9866":"markdown","8215f453":"markdown","86e36e17":"markdown"},"source":{"e6640a70":"import os\nimport tensorflow as tf\nimport numpy as np","c6c19cb0":"path_to_file='..\/input\/human-conversation-training-data\/human_chat.txt'","f7f84149":"text=open(path_to_file,'rb').read().decode(encoding='utf-8')","23b6d470":"import re\ntext=re.sub(r'Human: ','',text)","7ba544c0":"print(\"The length of characters in the text of database\",len(text))","79255a88":"#Creating the vocabulary of words\nvocab=sorted(set(text))","ae3a64ff":"print(vocab[:40])","6d124fa4":"char2idx={u:i for i,u in enumerate(vocab)}\nprint(char2idx)","369996cb":"idx2char=np.array(vocab)\nprint(idx2char)","c76e97fb":"#Convert the text to int\ntext_as_int=np.array([char2idx[c] for c in text])\nprint(text_as_int)","ed5905fe":"print('{')\nfor char,_ in zip(char2idx, range(20)):\n    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\nprint('  ...\\n}')","5c7d6889":"# Show how the first 13 characters from the text are mapped to integers\nprint ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))","09c94657":"# The maximum length sentence we want for a single input in characters\nseq_length = 100\nexamples_per_epoch = len(text)\/\/(seq_length+1)\n\n# Create training examples \/ targets\nchar_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)","5b3a4776":"sequences = char_dataset.batch(seq_length+1, drop_remainder=True)","f5a56f03":"def split_input_target(chunk):\n    input_text = chunk[:-1]\n    target_text = chunk[1:]\n    return input_text, target_text\n\ndataset = sequences.map(split_input_target)","396d2fbd":"# Batch size\nBATCH_SIZE = 64\n\n# Buffer size to shuffle the dataset\n# (TF data is designed to work with possibly infinite sequences,\n# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n# it maintains a buffer in which it shuffles elements).\nBUFFER_SIZE = 10000\n\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n\ndataset","fa497205":"# Length of the vocabulary in chars\nvocab_size = len(vocab)\n\n# The embedding dimension\nembedding_dim = 256\n\n# Number of RNN units\nrnn_units = 1024","62c24dc5":"def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n  model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n                              batch_input_shape=[batch_size, None]),\n    tf.keras.layers.GRU(rnn_units,\n                        return_sequences=True,\n                        stateful=True,\n                        recurrent_initializer='glorot_uniform'),\n    tf.keras.layers.Dense(vocab_size)\n  ])\n  return model","2b254e42":"model = build_model(\n  vocab_size = len(vocab),\n  embedding_dim=embedding_dim,\n  rnn_units=rnn_units,\n  batch_size=BATCH_SIZE)","2f679719":"model.summary()","fd8b7ac0":"def loss(labels, logits):\n  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)","a7b3ca70":"model.compile(optimizer='adam',loss=loss,metrics=['accuracy'])","6227e84d":"# Directory where the checkpoints will be saved\ncheckpoint_dir = '.\/training_checkpoints'\n# Name of the checkpoint files\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n\ncheckpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    save_weights_only=True)","99137139":"history=model.fit(dataset, epochs=50,callbacks=[checkpoint_callback])","6af06757":"tf.train.latest_checkpoint(checkpoint_dir)","6d08dff0":"model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n\nmodel.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n\nmodel.build(tf.TensorShape([1, None]))","08dfa56d":"model.summary()","1e6af18d":"def generate_text(model, start_string):\n  # Evaluation step (generating text using the learned model)\n\n  # Number of characters to generate\n  num_generate = 1000\n\n  # Converting our start string to numbers (vectorizing)\n  input_eval = [char2idx[s] for s in start_string]\n  input_eval = tf.expand_dims(input_eval, 0)\n\n  # Empty string to store our results\n  text_generated = []\n\n  # Low temperatures results in more predictable text.\n  # Higher temperatures results in more surprising text.\n  # Experiment to find the best setting.\n  temperature = 1.0\n\n  # Here batch size == 1\n  model.reset_states()\n  for i in range(num_generate):\n      predictions = model(input_eval)\n      # remove the batch dimension\n      predictions = tf.squeeze(predictions, 0)\n\n      # using a categorical distribution to predict the character returned by the model\n      predictions = predictions \/ temperature\n      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n\n      # We pass the predicted character as the next input to the model\n      # along with the previous hidden state\n      input_eval = tf.expand_dims([predicted_id], 0)\n\n      text_generated.append(idx2char[predicted_id])\n\n  output_string=''.join(text_generated)\n  print(output_string)","b90b2bf4":"s=input(\"Enter the text: \")\ns='Human 1: '+s\ngenerate_text(model, start_string=s)","37f1fdae":"Now we will rebuild the model from the saved weigths","460bc5ee":"#Vectorize the text\nBefore training, we need to map strings to a numerical representation. Create two lookup tables: one mapping characters to numbers, and another for numbers to characters.","390a676f":"Create test and training batches.","ba2f9866":"Add an optimizer and loss function","8215f453":"Model building using RNN","86e36e17":"#The fun part\nLet's generate text using the model and predictions on the given dataset nature."}}