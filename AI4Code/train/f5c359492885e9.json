{"cell_type":{"a40d33ac":"code","0deba5f3":"code","0cbccc8b":"code","b0152c8e":"code","4df47561":"code","010f77db":"code","28ba244b":"code","3eff74c9":"code","75168ecc":"code","1749ca5b":"code","3a462882":"code","53fbcbea":"code","8c735c68":"code","bb0c1fba":"code","0b90a6e8":"code","2a303f5e":"code","53c61b6c":"code","20e78f47":"code","402245a4":"code","c6946cd6":"code","40eb3e94":"code","ca550def":"code","25be7630":"code","5aff6e46":"code","9d161464":"code","c33f2795":"code","51eefaf0":"code","cf7b474c":"code","2003ec23":"code","e4179059":"markdown","27bff450":"markdown","17d06eee":"markdown","f54a8fb0":"markdown","3a67b1c3":"markdown","201678ae":"markdown","42e68725":"markdown","ba9c04a2":"markdown","b79e606e":"markdown","2a379b49":"markdown","aad9b43c":"markdown","8cbdb5f1":"markdown","e62dee3f":"markdown","413805f0":"markdown","bccd409c":"markdown","bada7108":"markdown","d296c308":"markdown","1bb673e7":"markdown","6b4f0ab8":"markdown","659322e4":"markdown","467e3f10":"markdown","bcc76d88":"markdown","f590a76f":"markdown","9e8c969c":"markdown","032de29b":"markdown","2c61e04f":"markdown","ef1d50a3":"markdown","a01312e4":"markdown","241c9fd7":"markdown"},"source":{"a40d33ac":"'''General Header for Python Operations'''\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nimport seaborn as sns\nimport warnings\nimport os\n\n# set graphics and print options\n%matplotlib inline\nplt.style.use('ggplot')\nplt.rcParams[\"figure.figsize\"] = (15,20)\npd.set_option('precision', 3)\nnp.set_printoptions(precision=3)\n\n# hide warnings\nwarnings.filterwarnings('ignore')\n\n# print input files for dataset\nfiles_dict = {}\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        files_dict[filename.split('.')[0]] = os.path.join(dirname, filename)\n        print(files_dict[filename.split('.')[0]])\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0deba5f3":"# Import Datasets\ndata_dict = {}\nfor file in files_dict.keys():\n    data_dict[file] = pd.read_csv(files_dict[file])\n","0cbccc8b":"#Examine Data\nfor df in data_dict.keys():\n    print(f'\\n {df}')\n    display(data_dict[df].head())\n    display(data_dict[df].info())","b0152c8e":"# Check For NaN's\ntrain_df = data_dict['train']\nprint('Any Features with NaN?')\nany(train_df.isna().sum() > 0)","4df47561":"# Check Balance of training set classes\ntrain_df.groupby('target').describe()","010f77db":"# check feature cadinality\nfor col in train_df.columns:\n    print(f'{col}: {train_df[col].nunique()} unique values.')","28ba244b":"# Look at descriptive stats for each column\nfor col in train_df.columns[:-1]:\n    tmp = train_df[col].describe()\n    tmp = [tmp['mean'],tmp['50%'],tmp['std'],tmp['min'],tmp['max']]\n    print(f'{col}: Mean {tmp[0]:0.4f}, Med: {tmp[1]:0.4f}, Std: {tmp[2]:0.4f}, Range: ({tmp[3]}, {tmp[4]})')","3eff74c9":"# check normality of features\nfor col in train_df.columns[:-1]:\n    print(f'{col}: SW test p-value = {stats.shapiro(train_df[col]).pvalue}')","75168ecc":"# Examine Hist of all features\n_= train_df[train_df.columns[1:-1]].hist(figsize=(15,40), layout=(15,5), bins=40)","1749ca5b":"# Examine Hist of all features with power transforms and iterate to find best by hand\n_= train_df[train_df.columns[1:-1]].pow(1\/2.).hist(figsize=(15,40), layout=(15,5), bins=40)","3a462882":"# Examine intra-feature correlations\nm = train_df[train_df.columns[1:-1]].corr()\nmsk = np.triu(np.ones_like(m, dtype=bool))\nplt.figure(figsize=(20,20))\n_=sns.heatmap(m, mask = msk, cmap = 'coolwarm', annot=False, cbar=False)\n","53fbcbea":"# Examine singular values\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import PowerTransformer\n\nsclr = PowerTransformer(method='yeo-johnson', standardize=True)\n\npca = PCA().fit(sclr.fit_transform(train_df[train_df.columns[1:-1]]))\nplt.figure(figsize=(10,7))\nplt.plot(np.arange(1,len(train_df.columns[1:-1])+1),np.cumsum(pca.explained_variance_ratio_))\nplt.hlines(0.95, *plt.xlim(), colors='k', linestyles='dotted', alpha = 0.5)\nplt.ylabel('Cumulative Explained Var Ratio')\nplt.xlabel('Number of Components')\n_= plt.title('PCA of Raw Features', fontweight='bold')","8c735c68":"# Find number of components to explain 95% of var\npca = PCA(n_components=0.95).fit(sclr.fit_transform(train_df[train_df.columns[1:-1]]))\nprint(f'95% Var Number of Components: {pca.n_components_}, Explained Variance: {np.sum(pca.explained_variance_ratio_)}')","bb0c1fba":"# Cap data at 99th percentile\ntrain_df[train_df.columns[1:-1]].clip(upper = train_df[train_df.columns[1:-1]].quantile(0.99), axis = 1, inplace = True)","0b90a6e8":"# Encode Target variable\nfrom sklearn.preprocessing import LabelEncoder\ntarget_le = LabelEncoder()\ntrain_df['target_enc'] = target_le.fit_transform(train_df['target'])","2a303f5e":"# Save raw feature column names\nraw_cols = list(train_df.columns[1:-2])","53c61b6c":"from sklearn.model_selection import train_test_split\n\npca_tr = PCA(n_components=70)\n\n# Separate Targets and Features\nX = pd.DataFrame(pca_tr.fit_transform(sclr.fit_transform(train_df[raw_cols])))\nX_test = pd.DataFrame(pca_tr.fit_transform(sclr.fit_transform(data_dict['test'][raw_cols])))\ny = train_df['target_enc']\n\n\nprint(f'X shape: {X.shape}')\nprint(f'y shape: {y.shape}')\nprint()\n\n# Stratified train validation split\nX_train, X_val, y_train, y_val = train_test_split(X, y, stratify=y, random_state= 42, test_size=0.20)\nprint(f'X Train {X_train.shape}')\nprint(f'X Val {X_val.shape}')\nprint(f'X Test {X_test.shape}')\nprint(f'y Train {y_train.shape}')\nprint(f'y Val {y_val.shape}')","20e78f47":"# Check Category Percentages from stratification\nprint('Original Splits:')\nprint(y.value_counts() \/ len(y), '\\n')\nprint('Training Splits:')\nprint(y_train.value_counts() \/ len(y_train), '\\n')\nprint('Validation Splits:')\nprint(y_val.value_counts() \/ len(y_val), '\\n')","402245a4":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import log_loss, accuracy_score\nimport joblib\n\ndef clfr_perfomance(y_true,X_, model):\n    print(f'Accuracy: {accuracy_score(y_true,model.predict(X_)):0.4f}')\n    print(f'Log Loss Function: {log_loss(y_true,model.predict_proba(X_)):0.4f}')","c6946cd6":"from sklearn.utils import all_estimators\n\nestimators = all_estimators()\n\nfor name, class_ in estimators:\n    if hasattr(class_, 'predict_proba'):\n        print(name)","40eb3e94":"from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier\n\nmdls = {'rf': RandomForestClassifier(n_jobs=-1),\n       'gbc': GradientBoostingClassifier(),\n       'mlp': MLPClassifier()}\n\nprms = {'rf': {'n_estimators': [2**i for i in range(3,8)],\n               'max_depth':  [8,16,32,64,None]},\n        'gbc': {'n_estimators': [250,500],\n               'max_depth':  [1,5,9],\n               'learning_rate': [0.001,0.01,0.1]},\n        'mlp': {'hidden_layer_sizes': [(10,),(50,),(100,),(200,)],\n               'activation': ['logistic','tanh','relu'],\n               'learning_rate': ['constant','invscaling','adaptive']}\n        \n        }","ca550def":"def train_model(key,models,params):\n    print('Training model: {}'.format(key))\n    gs_cv = GridSearchCV(models[key],params[key], cv = 5,\n                         scoring='neg_log_loss', n_jobs = -1, verbose=1)\n    best_est = gs_cv.fit(X_train, y_train)\n    print('Best Estimator: {}'.format(best_est.best_params_))\n    print('Best Estimator Score: {}'.format(best_est.best_score_))\n    joblib.dump(best_est.best_estimator_,'{}_tr.pkl'.format(key))","25be7630":"from keras.models import Sequential\nfrom keras.layers import *\nfrom keras.losses import SparseCategoricalCrossentropy\n\n# Define the model\nmodel = Sequential()\nmodel.add(Dense(70, input_dim = 70, activation='tanh', name='input'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(10, activation='relu', name = 'hidden'))\nmodel.add(Dense(9, activation='sigmoid', name = 'output')) \nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  \n\nmodel.summary()","5aff6e46":"from keras.callbacks import EarlyStopping,ReduceLROnPlateau\nlr_dec = ReduceLROnPlateau(monitor='loss')\nes_callback = EarlyStopping(monitor='loss', restore_best_weights=True)","9d161464":"from sklearn.preprocessing import OneHotEncoder\nohc = OneHotEncoder(sparse=False)\n\n# fit model\nnp.random.seed(42)\nhist = model.fit(X_train.values,ohc.fit_transform(y_train.values.reshape(-1,1)),verbose = 1,\n                 epochs=200,callbacks=[lr_dec, es_callback])","c33f2795":"model.evaluate(x=X_val.values,y=ohc.fit_transform(y_val.values.reshape(-1,1)))","51eefaf0":"X_test['id'] = data_dict['test']['id']\nsub_df = data_dict['sample_submission']\nsub_df['id'] = X_test['id']\npred = model.predict(X_test.drop(columns=['id']))\nsub_df[[x for x in sub_df.columns if x != 'id']] = pred \n#sub_df.set_index(columns = 'id', inplace=True)\nsub_df.head()","cf7b474c":"sub_df.set_index('id', drop=True).head()","2003ec23":"# write submission to file\nsub_df.set_index('id', drop=True).to_csv('submission.csv')","e4179059":"# Model Selection\nThe competition metric is log loss.","27bff450":"## Check Validation Score\n","17d06eee":"### Callbacks","f54a8fb0":"# Prepare Data for ML\n\n### Cap data since it's all non-negative and skrewed right","3a67b1c3":"\nCV Score for MLP min: 1.7457966814685377, max: 1.756144311735689, mean: 1.7520049992817097, median: 1.7515735323667034","201678ae":"Training model: gbc <br>\nFitting 5 folds for each of 18 candidates, totalling 90 fits <br>\n*Did not converge in a reasonable amount of time*","42e68725":"All values are non-negative and appear to be badly skewed right","ba9c04a2":"# Final Model Training and Optimization\nThe multilayer perceptron performed the best, so we will use a Keras MLP as the final model","b79e606e":"# EDA","2a379b49":"## Setup Gridsearch with crossvalidation to opt hyperparams","aad9b43c":"## Create Submission","8cbdb5f1":"None of them look great, but the square root is as good as any other transform.\n\nLooking at correlation between features...","e62dee3f":"# Train random forest\ntrain_model('rf',mdls,prms)","413805f0":"Training model: mlp <br>\nFitting 5 folds for each of 36 candidates, totalling 180 fits<br>\nBest Estimator: {'activation': 'tanh', 'hidden_layer_sizes': (10,), 'learning_rate': 'invscaling'}<br>\nBest Estimator Score: -1.7609277700753556<br>","bccd409c":"Looks like there are some highly correlated features, so we'll look at performing pca.","bada7108":"### Encode the Target","d296c308":"As expected, the data is all skewed right, so we'll transform it via Box-Cox method.","1bb673e7":"None of the features are normally distributed","6b4f0ab8":"Since we need to predict class probablies, find all the classifiers with predict_proba function in sklearn","659322e4":"The classes are highly imbalanced. Classes 3 and 4 are extremely under represented. Models where initially built having undersampled the majorities to even out the data set but resulted in a data set that was too small. So, training should be done on the data as is.","467e3f10":"Training model: rf <br>\nFitting 5 folds for each of 25 candidates, totalling 125 fits<br>\nBest Estimator: {'max_depth': 16, 'n_estimators': 128}<br>\nBest Estimator Score: -1.7731683449052305<br>\n","bcc76d88":"## Perform Yeo-Johnson Transform, Standardization, PCA on transformed data, separate out features and target, split in to train and validation sets","f590a76f":"There are roughly 68 components required to explain 95% of the variance in the data, so we'll use PCA to eliminate the extra feature count.","9e8c969c":"# Train gradient boosted tree classifier\ntrain_model('gbc',mdls,prms)","032de29b":"from sklearn.model_selection import cross_val_score\nmlp = MLPClassifier(activation='tanh',hidden_layer_sizes = (10,),\n                    learning_rate='invscaling')\ncv_mlp = cross_val_score(mlp, X_train,y_train, scoring='neg_log_loss', cv = 10)\nprint(f'CV Score for MLP min: {np.min(-cv_mlp)}, max: {np.max(-cv_mlp)}, mean: {np.mean(-cv_mlp)}, median: {np.median(-cv_mlp)}')","2c61e04f":"# Train multilayer perceptron classifier\ntrain_model('mlp',mdls,prms)\n","ef1d50a3":"### Train Model","a01312e4":"# Train Models for Evaluation","241c9fd7":"- train.csv - the training data, one product (id) per row, with the associated features (feature_*) and class label (target)\n\n- test.csv - the test data; you must predict the probability the id belongs to each class\n\n- sample_submission.csv - a sample submission file in the correct format"}}