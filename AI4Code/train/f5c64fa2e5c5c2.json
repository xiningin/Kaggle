{"cell_type":{"1fae5c0d":"code","27fe9fcb":"code","641a9621":"code","7b63e530":"code","618f107e":"code","0e57cddf":"code","bbd18562":"code","976b52b6":"code","192a0b2e":"code","dbc4ec81":"code","9a4b7bfd":"code","9dd96cf0":"code","64c38076":"code","c6f076e7":"code","35f021d9":"code","97a7906a":"code","e1aedf2e":"code","602c4a41":"code","0eb11765":"code","8f3f6110":"code","ba8db9e2":"code","0e6293ac":"code","145bb01f":"code","c169af70":"code","39aa7488":"code","81410af3":"code","ac31c7a6":"code","c98368dd":"code","87220afa":"markdown","0039c8a9":"markdown","65d7afa3":"markdown","434aab4d":"markdown","b4046bd0":"markdown","e8b74324":"markdown","d3edfe7e":"markdown","9fd1623a":"markdown","38fd193e":"markdown","b4ea5c20":"markdown","4b045fad":"markdown","b4259a78":"markdown","b5479bd5":"markdown","c2efc74c":"markdown","2a96c16c":"markdown","c5704743":"markdown","9f02c778":"markdown","a65feacd":"markdown","6a95f609":"markdown","4b416de5":"markdown","97443d5b":"markdown","f3671fc7":"markdown","38ee48ae":"markdown","812f2820":"markdown","ddc04d1d":"markdown","6040c04f":"markdown","dd57c26d":"markdown","020bdb83":"markdown","bb707a1e":"markdown","277ecb8d":"markdown","879e5d4d":"markdown","f91014f7":"markdown","bcdb16e4":"markdown","8fac4f03":"markdown","c507d0bf":"markdown","6f2f92a8":"markdown","3681a490":"markdown","1018335d":"markdown","ced44a75":"markdown","9c88337d":"markdown","699c8612":"markdown","c31180dd":"markdown","c26cc009":"markdown","b6ce4ad5":"markdown","c7058514":"markdown"},"source":{"1fae5c0d":"#load libraries and packages\nfrom sklearn.metrics import make_scorer, accuracy_score ,classification_report,f1_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom sklearn.cluster import AgglomerativeClustering\nimport scipy.cluster.hierarchy as sch\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nimport sklearn.metrics as sk\nfrom sklearn import preprocessing\nimport matplotlib.pylab as pylab\nimport matplotlib.pyplot as plt\nfrom pandas import get_dummies\nimport matplotlib as mpl\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport warnings\nimport sklearn\nimport scipy\nimport numpy\nimport json\nimport sys\nimport csv\nimport os\n\nfrom IPython.core.interactiveshell import InteractiveShell         #to display multiple outputs in same cell\nInteractiveShell.ast_node_interactivity = \"all\"\n","27fe9fcb":"data = pd.read_csv(\"..\/input\/wine-quality\/winequality.csv\")\ndata","641a9621":"#identify datatypes of features\ndata.dtypes","7b63e530":"#statistical analysis\ndata.describe()","618f107e":"#find number of wines that are classified 'good' and 'bad'\ndata.good.value_counts()","0e57cddf":"#plotting histogram of all features\ndata.hist(figsize=(15,20))","bbd18562":"#regression plot of chlorides vs quality\nf,ax=plt.subplots(figsize=(10,10))\nsns.regplot(x='chlorides',y='quality',data=data)\nplt.title('regression plot of chlorides and quality')","976b52b6":"#regression plot of alcohol vs quality\nf,ax=plt.subplots(figsize=(10,10))\nsns.regplot(x='alcohol',y='quality',data=data)\nplt.title('regression plot of alcohol and quality')","192a0b2e":"#countplot of quality based on goodness feature\nsns.catplot(x='quality',data=data,height=5,aspect=3,hue='good',kind='count')\nplt.title('countplot of qulaity')","dbc4ec81":"#correltaion matrix\/heatmap\ndata.corr()\nf,ax=plt.subplots(figsize=(10,10))\nsns.heatmap(data.corr())\nplt.title('heat map')","9a4b7bfd":"#remove unwanted columns\ndf=data.drop(['quality','free sulfur dioxide'],axis=1)","9dd96cf0":"data.isnull().sum()","64c38076":"#one hot encoding\ndf=pd.get_dummies(data)","c6f076e7":"#creating x and y dataframes\nx=df.drop('good',axis=1)\ny=df[['good']]","35f021d9":"#split into train and test data\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=100)","97a7906a":"# Perform random forest with grid search to optimize model\nrfc=RandomForestClassifier(random_state=42)\nparam_grid = { \n    'n_estimators': [200,300,400],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}\nrandom = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5).fit(x_train, y_train)\nrandom.predict(x_train)\nrandom.predict(x_test)","e1aedf2e":"print('train model metrics')\nconfusion_matrix(y_train,random.predict(x_train))\nprint('accuracy-',round(random.score(x_train,y_train) * 100, 2))\nprint('f1 score-',f1_score(y_train,random.predict(x_train)))\nprint(\" \")\nprint('test model metrics')\nconfusion_matrix(y_test,random.predict(x_test))\nprint('accuracy-',round(random.score(x_test,y_test) * 100, 2))\nprint('f1 score-',f1_score(y_test,random.predict(x_test)))\n","602c4a41":"#predict probabilites\nrf_prob=random.predict_proba(x_train)\nrf_prob=rf_prob[:,1]\nfalse_positive_rate1, true_positive_rate1, threshold1 = roc_curve(y_train, rf_prob)\nprint('auc_score for random forest(train): ', roc_auc_score(y_train, rf_prob))\n\n# Plot ROC curves\nplt.subplots(1, figsize=(5,5))\nplt.title('Receiver Operating Characteristic(train) - random forest')\nplt.plot(false_positive_rate1, true_positive_rate1)\nplt.plot([0, 1], ls=\"--\")\nplt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n\n\nrf_prob_test=random.predict_proba(x_test)\nrf_prob_test=rf_prob_test[:,1]\nfalse_positive_rate2, true_positive_rate2, threshold2 = roc_curve(y_test, rf_prob_test)\nprint('auc_score for random forest(test): ', roc_auc_score(y_test, rf_prob_test))\n\n# Plot ROC curves\nplt.subplots(1, figsize=(5,5))\nplt.title('Receiver Operating Characteristic(test) - random forest')\nplt.plot(false_positive_rate2, true_positive_rate2)\nplt.plot([0, 1], ls=\"--\")\nplt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n\n","0eb11765":"#correltaion matrix\/heatmap\ndata.corr()\nf,ax=plt.subplots(figsize=(10,10))\nsns.heatmap(df.corr())\nplt.title('heat map')","8f3f6110":"#one hot encoding to deal with categorical data\nx_onehot=pd.get_dummies(x)","ba8db9e2":"#data normalisation to bring data of every feature on a same scale\nx_scale = StandardScaler().fit_transform(x_onehot)","0e6293ac":"#elbow method with inertia to find n clusters\nwcss = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n    kmeans.fit(x_scale)\n    wcss.append(kmeans.inertia_)\nplt.plot(range(1, 11), wcss)\nplt.title('Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()","145bb01f":"#check silhouette score\n# Instantiate a scikit-learn K-Means model\nmodel = KMeans(random_state=0)\n\n# Instantiate the KElbowVisualizer with the number of clusters and the metric \nvisualizer = KElbowVisualizer(model, k=(2,10), metric='silhouette', timings=False)\n\n# Fit the data and visualize\nvisualizer.fit(x_scale)    \nvisualizer.poof()   ","c169af70":"#applying kmeans algorith\nkmeans = KMeans(n_clusters=2, init='k-means++', max_iter=300, n_init=10, random_state=0)\npred_y = kmeans.fit_predict(x_scale)\nplt.scatter(x_scale[:,0],x_scale[:,1],c=pred_y,cmap='viridis')\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')\nplt.show()\n#calculating davies bouldin score\nsklearn.metrics.davies_bouldin_score(x_scale,pred_y)","39aa7488":"#comparisons using mean\nx_kmeans=x.copy()\nx_kmeans['labels']=pred_y\nx_kmeans.groupby('labels').mean()\ndata.groupby('color').mean()","81410af3":"#hierarchical clustering-plotting dendrogram\ndendrogram = sch.dendrogram(sch.linkage(x_scale, method='ward'))","ac31c7a6":"#applying agglomerative clustering algorithm\nmodel = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')\nmodel.fit_predict(x_scale)\nlabels = model.labels_\n#plotting clusters on scatter plot\nplt.figure(figsize=(10, 7))\nplt.scatter(x_scale[labels==0, 0], x_scale[labels==0, 1], s=50, marker='o', color='red')\nplt.scatter(x_scale[labels==1, 0], x_scale[labels==1, 1], s=50, marker='o', color='blue')\nsklearn.metrics.davies_bouldin_score(x_scale,labels)","c98368dd":"x_hrcl=x.copy()\nx_hrcl['labels']=labels\nx_hrcl.groupby('labels').mean()\ndata.groupby('color').mean()\n","87220afa":"It seems like our dataset is imbalanced with only around 20 percent of wines that are classified 'good'","0039c8a9":"Before we begin to build a cluster model in unsupervised learning , the most important parameter to decide is to determine the number of clusters . Lets explore some methods within K-Means to decide on it.\n","65d7afa3":"Lets load the data and do some explorations","434aab4d":"Again it can be inferred from the tabular column that our clustering model has performed fairly well enough to group objects based on color feature.","b4046bd0":"# Elbow method to identify number of clusters.","e8b74324":"# Data normalisation","d3edfe7e":"Observing the heatmap, it is found that 'quality' column is highly correlated to 'good'column. Also, correlation between 'free sulfur dioxide' and 'total sulfur dioxide' is high. Thus it is logical to remove one among them to reduce redundancy. Since , 'free sulfur dioxide' is part of the 'total sulfur dioxide' ,lets remove 'free sulfur dioxide'.Besides, 'color' column is removed to check whether final clusters formed have similarites based on color features.","9fd1623a":"# Agglomerative Clustering","38fd193e":"Lets begin by loading the required packages","b4ea5c20":"# Problem statement:\n\n1)To make use of available wine quality data and train the model to predict wine quality\n\n2)To build a clustering of wine datasets based on their content","4b045fad":"Since we have already explored our dataset with visualizations, lets skip that part. However , there is always a scope for some change in  building a new model. Lets plot a correlation matrix to check that.","b4259a78":"It can be inferred from the above data that our dataset contains 6497 rows and 14 columns whose description are as follows\n\n1)fixed acidity : most acids involved with wine are fixed or nonvolatile (do not evaporate readily) \n\n2)volatile acidity:the amount of acetic acid in wine, which at too high  levels can lead to an unpleasant, vinegar taste\n\n3)citric acid : found in small quantities, citric acid can add 'freshness' and flavor to wines\n\n4)residual sugar : the amount of sugar remaining after fermentation stops, it's rare to find wines with less than 1 gram\/liter and wines with greater than 45 grams\/liter are considered sweet\n\n5)chlorides : the amount of salt in the wine\n\n6)free sulfur dioxide : the free form of SO2 exists in equilibrium between molecular SO2 (as a dissolved gas) and bisulfite ion; it prevents microbial growth and the oxidation of wine\n\n7)total sulfur dioxide : amount of free and bound forms of S02; in low concentrations, SO2 is mostly undetectable in wine, but at free SO2 concentrations over 50 ppm, SO2 becomes evident in the nose and taste of wine\n\n8)density : the density of water is close to that of water depending on the percent alcohol and sugar content\n\n9)pH : describes how acidic or basic a wine is on a scale from 0 (very acidic) to 14 (very basic); most wines are between 3-4 on the pH scale\n\n10)sulphates : a wine additive which can contribute to sulfur dioxide gas (S02) levels, wich acts as an antimicrobial and antioxidant\n\n11)alcohol : the percent alcohol content of the wine\n\n12)quality : output variable (based on sensory data, score between 0 and 10)\n\n13)good : goodness quality of wine (whether good or not)\n\n14)color : colour of the wine (either red or white)\n","b5479bd5":"# Build model","c2efc74c":"# plot roc curve and calculate auc\n","2a96c16c":"# confusion matrix, accuracy and f1 score","c5704743":"Before we calculate auc and plot roc, we need to generate outputs in the form of probabilities. Thus we use 'predict_proba' function for the same","9f02c778":"# Apply one hot encoding to convert categorical columns\n","a65feacd":"# Wine Quality Prediction and Clustering","6a95f609":"# Separating independant and dependant variables","4b416de5":"# one-hot encoding","97443d5b":"#            THANK YOU","f3671fc7":"# Correlation matrix and heatmap\n","38ee48ae":"The above plot gives us an insights into how different features are distributed","812f2820":"# K-Means Clustering Model\n\n","ddc04d1d":"# Comparisons to check similarties","6040c04f":"Observing the heatmap, it is found that 'quality' column is highly correlated to 'good'column as previously determined by countplot. Besides, correlation between 'free sulfur dioxide' and 'total sulfur dioxide' is also high. Thus it is logical to remove one among them to reduce redundancy. Since , 'free sulfur dioxide' is part of the 'total sulfur dioxide' ,lets remove 'free sulfur dioxide'.","dd57c26d":"Based on the above obsevation, it is found that wines that are rated 7 or above are categorised as good. Thus , it makes no logical sense to apply machine learning algorithms here. Just to keep the scope of building ML model alive here, lets remove column 'quality' from the dataset for the sake of predictive analytics.","020bdb83":"observing the above regression plot , the qulaity of the wine is inversely proportional to the presence of chlorides in wine.","bb707a1e":"lets move on to the other type of clustering algorithm called 'agglomerative clustering' which is grouped under hierarchical clustering. Just like elbow method, we have dendrogram plot to identify number of clusters in hierarchical clustering.\n\nA dendrogram is a diagram that shows the hierarchical relationship between objects. It is most commonly created as an output from hierarchical clustering. The main use of a dendrogram is to work out the best way to allocate objects to clusters. ","277ecb8d":"The above graph depicts that the highest silhouette score is obtained when we select number of groupings to be 2.Thus moving forward to apply k-Means algorithm.","879e5d4d":"Lucky we dont have any missing values","f91014f7":"# check missing values","bcdb16e4":"The key to interpreting a dendrogram is to focus on the height at which any two objects are joined together which indicates the order in which the clusters were joined.From the dendrogram plot above, it can be inferred that number of clusters can be selected as 2.","8fac4f03":"# Build model with Random forest","c507d0bf":"The above two tabular comparisons shows that our obtained clusters have statistics which are more or less similar to that of the original dataframe when grouped by 'color' feature. The similarities in mean values of features like 'fixed acidity','alcohol','density' etc. signifies the fact that wines are closely grouped as per their colours. ","6f2f92a8":"# Data Visualizations","3681a490":"Lets move on to the second part of our problem i.e, to build a cluster model","1018335d":"There u go! Random forest has lived up to its expectation and we see a perfect predictions on both train and test model","ced44a75":"observing the above regression plot , the qulaity of the wine is directly proportional to the presence of alcohol content in wine.","9c88337d":"# Comparisons to check similarity","699c8612":"#  Clustering model (Unsupervised learning)","c31180dd":"Usually , the number of clusters is decided by observing the elbow plot and where the inertia seems to level off. However ,we cannot decide on where the inertia seems to level off(can't decide between 2 and 4) in this case to select the value for number of clusters. Thus , we shall move on to elbow method by  'silhouette score' to identify number of clusters.","c26cc009":"# Exploratory data analysis","b6ce4ad5":"The Davies\u2013Bouldin index (DBI) (introduced by David L. Davies and Donald W. Bouldin in 1979) is a metric for evaluating clustering algorithms. This is an internal evaluation scheme, where the validation of how well the clustering has been done is made using quantities and features inherent to the dataset.","c7058514":"# Create test and train data"}}