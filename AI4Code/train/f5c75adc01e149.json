{"cell_type":{"2d054592":"code","8b91479e":"code","621d8837":"code","4ce2b4c5":"code","117a2761":"code","a9e870b0":"code","ef64148b":"code","a9de4f8e":"code","24d2aab1":"code","991374fc":"code","48356fdb":"code","a76438c5":"code","f58c5c50":"code","371016e4":"code","55ed7b74":"code","95fc0590":"code","6d43137d":"code","0b5819a4":"code","58996ca3":"code","9676c1ee":"code","da478b06":"code","7a1abdc8":"code","a62fe99b":"code","3bfd4704":"code","b1949383":"code","9231dc81":"code","57077663":"code","735501bc":"code","48fa2816":"code","cafc84cb":"code","9ec02892":"code","82b79dcd":"code","4c7966eb":"code","130dafaa":"code","b109892b":"code","7ed126c9":"code","6e720f34":"code","a9c3e905":"code","e5652440":"code","5d863aa8":"code","d3651aaa":"code","d01296e8":"code","12799236":"code","d1a9cb12":"code","74cd1a29":"code","95ce0bf7":"code","ddc19d57":"code","9f320aa4":"code","dc982a01":"code","6fdac5f3":"code","b5763e22":"code","e921a9dc":"code","fc6b08bf":"code","769a2fb8":"code","8963cc1e":"code","12b240f8":"code","09f93cda":"code","48547a30":"code","d446da42":"code","94455876":"code","4e131cd8":"code","1852d04e":"code","5f17f51a":"code","d4b236d9":"code","ad349bda":"code","528b8ec2":"code","0ca72a88":"code","2f029676":"code","632b0a64":"code","53b8e910":"code","f49aba33":"code","7a9caf1f":"code","54daa4a1":"code","e84f5782":"code","6c5ddcee":"code","590411ed":"code","ef5148fc":"markdown","a4b38ea0":"markdown","0d3dce6c":"markdown","397df5a0":"markdown","b76659d2":"markdown","02e4a78a":"markdown","2a11a830":"markdown","f2f56f3b":"markdown","8b693cac":"markdown","2917409e":"markdown","63cbc74f":"markdown","d465524e":"markdown","5d88a865":"markdown","0debb8a0":"markdown","04202acb":"markdown","ec9e8081":"markdown","c73437a1":"markdown","710d0560":"markdown","79f160e0":"markdown","22429931":"markdown","0aaaa71e":"markdown","99bbd914":"markdown","a0fa40d7":"markdown","aa044b65":"markdown","3ac12392":"markdown","6b678406":"markdown","77212288":"markdown","1b3d800c":"markdown","bf4a9ec3":"markdown","54338631":"markdown"},"source":{"2d054592":"# Suppressing Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","8b91479e":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","621d8837":"# Importing Pandas and NumPy\nimport pandas as pd, numpy as np, seaborn as sns,matplotlib.pyplot as plt","4ce2b4c5":"pd.set_option('display.max_columns', None)","117a2761":"# Importing all datasets\nchurn_data = pd.read_csv(\"\/kaggle\/input\/telecom-churn-data-sets\/churn_data.csv\")\nchurn_data.head()","a9e870b0":"customer_data = pd.read_csv(\"\/kaggle\/input\/telecom-churn-data-sets\/customer_data.csv\")\ncustomer_data.head()","ef64148b":"internet_data = pd.read_csv(\"\/kaggle\/input\/telecom-churn-data-sets\/internet_data.csv\")\ninternet_data.head()","a9de4f8e":"# Merging on 'customerID'\ndf_1 = pd.merge(churn_data, customer_data, how='inner', on='customerID')","24d2aab1":"# Final dataframe with all predictor variables\ntelecom = pd.merge(df_1, internet_data, how='inner', on='customerID')","991374fc":"# Let's see the head of our master dataset\ntelecom.head()","48356fdb":"# Let's check the dimensions of the dataframe\ntelecom.shape","a76438c5":"# let's look at the statistical aspects of the dataframe\ntelecom.describe()","f58c5c50":"# Let's see the type of each column\ntelecom.info()","371016e4":"#The varaible was imported as a string we need to convert it to float\n# telecom['TotalCharges'] = telecom['TotalCharges'].astype(float) \ntelecom.TotalCharges = pd.to_numeric(telecom.TotalCharges, errors='coerce')","55ed7b74":"telecom.info()","95fc0590":"\nplt.figure(figsize=(20,40))\nplt.subplot(10,2,1)\nax = sns.distplot(telecom['tenure'], hist=True, kde=False, \n             bins=int(180\/5), color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4})\nax.set_ylabel('# of Customers')\nax.set_xlabel('Tenure (months)')\nplt.subplot(10,2,2)\nax = sns.countplot(x='PhoneService', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,3)\nax =sns.countplot(x='Contract', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,3)\nax =sns.countplot(x='Contract', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,4)\nax =sns.countplot(x='PaperlessBilling', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,5)\nax =sns.countplot(x='PaymentMethod', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,6)\nax =sns.countplot(x='Churn', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,7)\nax =sns.countplot(x='gender', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,8)\nax =sns.countplot(x='SeniorCitizen', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,9)\nax =sns.countplot(x='Partner', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,10)\nax =sns.countplot(x='Dependents', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,11)\nax =sns.countplot(x='MultipleLines', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,12)\nax =sns.countplot(x='InternetService', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,13)\nax =sns.countplot(x='OnlineSecurity', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,14)\nax =sns.countplot(x='OnlineBackup', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,15)\nax =sns.countplot(x='DeviceProtection', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,16)\nax =sns.countplot(x='TechSupport', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,17)\nax =sns.countplot(x='StreamingTV', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,18)\nax =sns.countplot(x='StreamingMovies', data=telecom)\nax.set_ylabel('# of Customers')\nplt.subplot(10,2,19)\nax = sns.distplot(telecom['MonthlyCharges'], hist=True, kde=False, \n             bins=int(180\/5), color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4})\nax.set_ylabel('# of Customers')\nax.set_xlabel('MonthlyCharges')\nplt.subplot(10,2,20)\nax = sns.distplot(telecom['TotalCharges'], hist=True, kde=False, \n             bins=int(180\/5), color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4})\nax.set_ylabel('# of Customers')\nax.set_xlabel('TotalCharges');","6d43137d":"sns.pairplot(telecom)\nplt.show()","0b5819a4":"plt.figure(figsize=(25, 10))\nplt.subplot(1,3,1)\nsns.boxplot(x = 'tenure', y = 'Churn', data=telecom)\nplt.subplot(1,3,2)\nsns.boxplot(x = 'MonthlyCharges', y = 'Churn', data=telecom)\nplt.subplot(1,3,3)\nsns.boxplot(x = 'TotalCharges', y = 'Churn', data=telecom)\nplt.show()","58996ca3":"# List of variables to map\n\nvarlist =  ['PhoneService', 'PaperlessBilling', 'Churn', 'Partner', 'Dependents']\n\n# Defining the map function\ndef binary_map(x):\n    return x.map({'Yes': 1, \"No\": 0})\n\n# Applying the function to the housing list\ntelecom[varlist] = telecom[varlist].apply(binary_map)","9676c1ee":"telecom.head()","da478b06":"# Creating a dummy variable for some of the categorical variables and dropping the first one.\ndummy1 = pd.get_dummies(telecom[['Contract', 'PaymentMethod', 'gender', 'InternetService']], drop_first=True)\n\n# Adding the results to the master dataframe\ntelecom = pd.concat([telecom, dummy1], axis=1)","7a1abdc8":"telecom.head()","a62fe99b":"# Creating dummy variables for the remaining categorical variables and dropping the level with big names.\n\n# Creating dummy variables for the variable 'MultipleLines'\nml = pd.get_dummies(telecom['MultipleLines'], prefix='MultipleLines')\n# Dropping MultipleLines_No phone service column\nml1 = ml.drop(['MultipleLines_No phone service'], 1)\n#Adding the results to the master dataframe\ntelecom = pd.concat([telecom,ml1], axis=1)\n\n# Creating dummy variables for the variable 'OnlineSecurity'.\nos = pd.get_dummies(telecom['OnlineSecurity'], prefix='OnlineSecurity')\nos1 = os.drop(['OnlineSecurity_No internet service'], 1)\n# Adding the results to the master dataframe\ntelecom = pd.concat([telecom,os1], axis=1)\n\n# Creating dummy variables for the variable 'OnlineBackup'.\nob = pd.get_dummies(telecom['OnlineBackup'], prefix='OnlineBackup')\nob1 = ob.drop(['OnlineBackup_No internet service'], 1)\n# Adding the results to the master dataframe\ntelecom = pd.concat([telecom,ob1], axis=1)\n\n# Creating dummy variables for the variable 'DeviceProtection'. \ndp = pd.get_dummies(telecom['DeviceProtection'], prefix='DeviceProtection')\ndp1 = dp.drop(['DeviceProtection_No internet service'], 1)\n# Adding the results to the master dataframe\ntelecom = pd.concat([telecom,dp1], axis=1)\n\n# Creating dummy variables for the variable 'TechSupport'. \nts = pd.get_dummies(telecom['TechSupport'], prefix='TechSupport')\nts1 = ts.drop(['TechSupport_No internet service'], 1)\n# Adding the results to the master dataframe\ntelecom = pd.concat([telecom,ts1], axis=1)\n\n# Creating dummy variables for the variable 'StreamingTV'.\nst =pd.get_dummies(telecom['StreamingTV'], prefix='StreamingTV')\nst1 = st.drop(['StreamingTV_No internet service'], 1)\n# Adding the results to the master dataframe\ntelecom = pd.concat([telecom,st1], axis=1)\n\n# Creating dummy variables for the variable 'StreamingMovies'. \nsm = pd.get_dummies(telecom['StreamingMovies'], prefix='StreamingMovies')\nsm1 = sm.drop(['StreamingMovies_No internet service'], 1)\n# Adding the results to the master dataframe\ntelecom = pd.concat([telecom,sm1], axis=1)","3bfd4704":"telecom.head()","b1949383":"# We have created dummies for the below variables, so we can drop them\ntelecom = telecom.drop(['Contract','PaymentMethod','gender','MultipleLines','InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n       'TechSupport', 'StreamingTV', 'StreamingMovies'], 1)","9231dc81":"# Checking for outliers in the continuous variables\nnum_telecom = telecom[['tenure','MonthlyCharges','SeniorCitizen','TotalCharges']]","57077663":"# Checking outliers at 25%, 50%, 75%, 90%, 95% and 99%\nnum_telecom.describe(percentiles=[.25, .5, .75, .90, .95, .99])","735501bc":"# Adding up the missing values (column-wise)\ntelecom.isnull().sum()","48fa2816":"print('No. of Null Records for TotalCharges:',telecom.TotalCharges.isnull().sum())","cafc84cb":"print('No. of Records for TotalCharges:',len(telecom))","9ec02892":"print('No. of non Records for TotalCharges:',len(telecom)-telecom.TotalCharges.isnull().sum())","82b79dcd":"# Checking the percentage of missing values\nround(100*(telecom.isnull().sum()\/len(telecom.index)), 2)","4c7966eb":"telecom = telecom.dropna()\ntelecom = telecom.reset_index(drop=True)\n\n","130dafaa":"# Checking percentage of missing values after removing the missing values\nround(100*(telecom.isnull().sum()\/len(telecom.index)), 2)","b109892b":"from sklearn.model_selection import train_test_split","7ed126c9":"# Putting feature variable to X\nX = telecom.drop(['Churn','customerID'], axis=1)\n\nX.head()","6e720f34":"# Putting response variable to y\ny = telecom['Churn']\n\ny.head()","a9c3e905":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","e5652440":"from sklearn.preprocessing import StandardScaler","5d863aa8":"scaler = StandardScaler()\n\nX_train[['tenure','MonthlyCharges','TotalCharges']] = scaler.fit_transform(X_train[['tenure','MonthlyCharges','TotalCharges']])\n\nX_train.head()","d3651aaa":"X_test[['tenure','MonthlyCharges','TotalCharges']] = scaler.transform(X_test[['tenure','MonthlyCharges','TotalCharges']])\n\nX_test.head()","d01296e8":"### Checking the Churn Rate\nchurn = (sum(telecom['Churn'])\/len(telecom['Churn'].index))*100\nchurn","12799236":"# Importing matplotlib and seaborn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","d1a9cb12":"# Let's see the correlation matrix \nplt.figure(figsize = (25,25))        # Size of the figure\nsns.heatmap(telecom.corr(),annot = True,cmap=\"tab20c\")\nplt.show()","74cd1a29":"plt.figure(figsize=(10,8))\ntelecom.corr()['Churn'].sort_values(ascending = False).plot(kind='bar');","95ce0bf7":"X_test = X_test.drop(['MultipleLines_No','OnlineSecurity_No','OnlineBackup_No','DeviceProtection_No','TechSupport_No',\n                       'StreamingTV_No','StreamingMovies_No'], 1)\nX_train = X_train.drop(['MultipleLines_No','OnlineSecurity_No','OnlineBackup_No','DeviceProtection_No','TechSupport_No',\n                         'StreamingTV_No','StreamingMovies_No'], 1)","ddc19d57":"plt.figure(figsize = (25,25))\nsns.heatmap(X_train.corr(),annot = True,cmap=\"tab20c\")\nplt.show()","9f320aa4":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\nmodel = GradientBoostingClassifier(n_estimators=125,max_depth=5)\n\n","dc982a01":"# fit the model with the training data\nmodel.fit(X_train,y_train)","6fdac5f3":"# predict the target on the train dataset\npredict_train = model.predict(X_train)\npredict_train","b5763e22":"trainaccuracy = accuracy_score(y_train,predict_train)\nprint('accuracy_score on train dataset : ', trainaccuracy)","e921a9dc":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif.tail()","fc6b08bf":"features_to_remove = vif.loc[vif['VIF'] >= 4.99,'Features'].values\nfeatures_to_remove = list(features_to_remove)\nprint(features_to_remove)","769a2fb8":"X_train = X_train.drop(columns=features_to_remove, axis = 1)\nX_train.head()","8963cc1e":"X_test = X_test.drop(columns=features_to_remove, axis = 1)\nX_test.head()","12b240f8":"# fit the model with the training data\nmodel.fit(X_train,y_train)","09f93cda":"# predict the target on the train dataset\npredict_train = model.predict(X_train)\npredict_train","48547a30":"trainaccuracy = accuracy_score(y_train,predict_train)\nprint('accuracy_score on train dataset : ', trainaccuracy)","d446da42":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","94455876":"from sklearn import metrics\n# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train, predict_train )\nprint(confusion)\n","4e131cd8":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","1852d04e":"# Let's see the sensitivity of our model\ntrainsensitivity= TP \/ float(TP+FN)\ntrainsensitivity","5f17f51a":"# Let us calculate specificity\ntrainspecificity= TN \/ float(TN+FP)\ntrainspecificity","d4b236d9":"# Calculate false postive rate - predicting churn when customer does not have churned\nprint(FP\/ float(TN+FP))","ad349bda":"# Positive predictive value \nprint (TP \/ float(TP+FP))","528b8ec2":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","0ca72a88":"draw_roc(y_train,predict_train)","2f029676":"#Looking at the confusion matrix again","632b0a64":"from sklearn.metrics import precision_score, recall_score\nprecision_score(y_train,predict_train)","53b8e910":"recall_score(y_train,predict_train)","f49aba33":"# predict the target on the test dataset\npredict_test = model.predict(X_test)\nprint('Target on test data\\n\\n',predict_test)","7a9caf1f":"confusion2 = metrics.confusion_matrix(y_test, predict_test )\nprint(confusion2)","54daa4a1":"# Let's check the overall accuracy.\ntestaccuracy= accuracy_score(y_test,predict_test)\ntestaccuracy","e84f5782":"# Let's see the sensitivity of our lmodel\ntestsensitivity=TP \/ float(TP+FN)\ntestsensitivity","6c5ddcee":"# Let us calculate specificity\ntestspecificity= TN \/ float(TN+FP)\ntestspecificity","590411ed":"# Let us compare the values obtained for Train & Test:\nprint(\"Train Data Accuracy    :{} %\".format(round((trainaccuracy*100),2)))\nprint(\"Train Data Sensitivity :{} %\".format(round((trainsensitivity*100),2)))\nprint(\"Train Data Specificity :{} %\".format(round((trainspecificity*100),2)))\nprint(\"Test Data Accuracy     :{} %\".format(round((testaccuracy*100),2)))\nprint(\"Test Data Sensitivity  :{} %\".format(round((testsensitivity*100),2)))\nprint(\"Test Data Specificity  :{} %\".format(round((testspecificity*100),2)))","ef5148fc":"# VIF","a4b38ea0":"# Final Observation:","0d3dce6c":"#### Checking for Missing Values and Inputing Them","397df5a0":"GBM is a boosting algorithm used when we deal with plenty of data to make a prediction with high prediction power. Boosting is actually an ensemble of learning algorithms which combines the prediction of several base estimators in order to improve robustness over a single estimator. It combines multiple weak or average predictors to a build strong predictor.","b76659d2":"#### Checking the Correlation Matrix","02e4a78a":"Now we don't have any missing values","2a11a830":"# Plotting the ROC Curve","f2f56f3b":"### Step 6: Looking at Correlations","8b693cac":"#### Checking for Outliers","2917409e":"#### Dropping highly correlated dummy variables","63cbc74f":"It means that 11 * 100\/7043 = 0.1561834%, best is to remove these observations from the analysis","d465524e":"# VIF","5d88a865":"### Step 5: Feature Scaling","0debb8a0":"#### Dropping the repeated variables","04202acb":"### Step 2: Inspecting the Dataframe","ec9e8081":"# GBM","c73437a1":"### Step 4: Test-Train Split","710d0560":"# EDA","79f160e0":"After dropping highly correlated variables now let's check the correlation matrix again.","22429931":"#### For categorical variables with multiple levels, create dummy features (one-hot encoded)","0aaaa71e":"From the distribution shown above, you can see that there no outliers in your data. The numbers are gradually increasing.","99bbd914":"## Precision and Recall","a0fa40d7":"We have almost 27% churn rate","aa044b65":"Now you can see that you have all variables as numeric.","3ac12392":"#### Combining all data files into one consolidated dataframe","6b678406":"### Step 7: Model Building\nLet's start by splitting our data into a training set and a test set.","77212288":"#### Converting some binary variables (Yes\/No) to 0\/1","1b3d800c":"### Step 1: Importing and Merging Data","bf4a9ec3":"### Step 11: Making predictions on the test set","54338631":"## Telecom Churn Case Study\nWith 21 predictor variables we need to predict whether a particular customer will switch to another telecom provider or not. In telecom terminology, this is referred to as churning and not churning, respectively."}}