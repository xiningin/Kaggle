{"cell_type":{"23ef039e":"code","88ed9470":"code","6a4fdf32":"code","6356e5ef":"code","02564176":"code","ecd57829":"code","e1e50222":"code","28bae12b":"code","3da994d8":"code","79bda91b":"code","7e637f2e":"code","c30dabd1":"code","fd239462":"markdown","d2ac142f":"markdown","9e2a3812":"markdown","d7b68a84":"markdown","98ea61e9":"markdown","10b42ac8":"markdown"},"source":{"23ef039e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","88ed9470":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.font_manager\n%matplotlib inline","6a4fdf32":"titanic_data = pd.read_csv('..\/input\/titanic\/train.csv', index_col='PassengerId')\ntitanic_test_data = pd.read_csv('..\/input\/titanic\/train.csv', index_col='PassengerId')","6356e5ef":"titanic_data.head()","02564176":"titanic_data.describe()","ecd57829":"fig, ax = plt.subplots()\nplt.rcParams['font.sans-serif'] = 'Arial'\nplt.rcParams['font.family'] = 'sans-serif'\nplt.rcParams['text.color'] = '#909090'\nplt.rcParams['axes.labelcolor']= '#909090'\nplt.rcParams['xtick.color'] = '#909090'\nplt.rcParams['ytick.color'] = '#909090'\nplt.rcParams['font.size']=12\n\ncolor_palette_list = ['#009ACD', '#ADD8E6', '#63D1F4', '#0EBFE9',   \n                      '#C1F0F6', '#0099CC']\n\nsurvived = titanic_data['Survived'].value_counts(dropna=False)\ntotal = survived[0] + survived[1]\nper_survived = survived[1]\/total * 100\nper_dead = survived[0]\/total * 100\n\n\nlabels = ['Survivor', \n         'Deceased']\npercentages = [per_survived, per_dead]\nexplode=(0.05,0)\nax.pie(percentages, explode=explode, labels=labels,  \n       colors=color_palette_list[0:2], autopct='%1.1f%%', \n       shadow=False, startangle=-400,   \n       pctdistance=1.2,labeldistance=1.5)\nax.axis('equal')\nax.set_title(\"Surviving people on dataset\")\nax.legend(frameon=False, bbox_to_anchor=(1.5,0.8))","e1e50222":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# Function for comparing different approaches\ndef score_dataset(X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators=10, random_state=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)","28bae12b":"#import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load the data\n#data = pd.read_csv('..\/input\/melbourne-housing-snapshot\/melb_data.csv')\n\n# Select target\ny = titanic_data.Survived\n\n# To keep things simple, we'll use only numerical predictors\ntitanic_predictors = titanic_data.drop(['Survived'], axis=1)\nX = titanic_predictors.select_dtypes(exclude=['object'])\n\n# Divide data into training and validation subsets\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                      random_state=0)","3da994d8":"from sklearn.impute import SimpleImputer\n\n# Imputation\nmy_imputer = SimpleImputer()\nimputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\nimputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n\n\n\n# Imputation removed column names; put them back\nimputed_X_train.columns = X_train.columns\nimputed_X_valid.columns = X_valid.columns\n\nprint(\"MAE from Approach 2 (Imputation):\")\nprint(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))","79bda91b":"#def score_dataset(X_train, X_valid, y_train, y_valid):\n\nmodel = RandomForestRegressor(n_estimators=10, random_state=0)\nmodel.fit(imputed_X_train, y_train)\npreds = model.predict(imputed_X_valid)\n\npreds.round()","7e637f2e":"X = titanic_test_data.select_dtypes(exclude=['object'])\nimputed_X = pd.DataFrame(my_imputer.fit_transform(X))\nimputed_X.columns = X.columns\nfinal_preds = model.predict(imputed_X).round()\nlen(final_preds)\n","c30dabd1":"#pd.DataFrame(titanic_test_data.PassengerId)\ntitanic_test_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntitanic_test_data.PassengerId\n\noutput = pd.DataFrame({'Id': titanic_test_data.PassengerId,'Survived': final_preds})\n#output.to_csv('submission.csv', index=False)\noutput","fd239462":"# Imputation of data","d2ac142f":"# Predict values to save into the competence","9e2a3812":"# Random Forest","d7b68a84":"# Preparing data","98ea61e9":"a = titanic_data['Survived'].value_counts(dropna=False)\na[0]","10b42ac8":"As we can see, there are 891 rows of data, all the values are numerical, and only a age have missing values. To complete the data, we will try the imputation method. But first let's analize a little bit more the data:"}}