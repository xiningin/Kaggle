{"cell_type":{"78988d92":"code","1def541a":"code","c6bfa9d4":"code","525b91b0":"code","32622565":"code","d943c875":"code","7a281ee5":"code","92eb683f":"code","5076a4e0":"code","9c7bacec":"code","05a951a8":"code","2ba6b93a":"code","53b37789":"code","d4436f23":"code","cfd36f89":"code","6ec893c5":"code","9445fff6":"code","546bc21f":"code","7204888b":"code","83b54b13":"code","62c42473":"code","fb1b0129":"code","d6f42f3e":"code","92f3b94c":"code","80180952":"code","8bed6963":"code","c911b8d6":"code","74855605":"code","f7c65357":"code","f239d206":"code","0fde6982":"code","bfffaa79":"code","d0aa6724":"code","458a26b6":"code","f7e0fc94":"code","019721aa":"code","6f814911":"code","78e54357":"code","64cf32ee":"code","ff342510":"code","56e45b08":"code","b0a0ac0d":"code","fe74da44":"code","0b2d9406":"code","c607b0fe":"code","26766abf":"code","83438692":"code","cf386b4a":"code","3be95c29":"markdown","95bb6314":"markdown","e02251dc":"markdown","33b6921b":"markdown","820ff782":"markdown","8a33c041":"markdown","821e8da3":"markdown"},"source":{"78988d92":"import warnings \nwarnings.filterwarnings('ignore')\nfrom tqdm.notebook import tqdm","1def541a":"import pandas as pd\nimport numpy as np\nimport random\n\nimport pyLDAvis\nimport pyLDAvis.gensim_models","c6bfa9d4":"import nltk\nnltk.download('all')\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.sentiment import SentimentIntensityAnalyzer","525b91b0":"#libraries regarding topic modelling\nfrom gensim import corpora\nfrom gensim.models.ldamulticore import LdaMulticore\n\nfrom gensim.models import CoherenceModel\nfrom gensim import matutils","32622565":"df_car = pd.read_csv('..\/input\/reviews-of-5-car-brands\/5 Car Brand Reviews\/car_5_brands.csv')\ndf_car","d943c875":"sia = SentimentIntensityAnalyzer()\n\ndef sentimenter(x) :\n  result = sia.polarity_scores(x)\n  return result","7a281ee5":"''' we have classified sentiments based on compound score & we have decided tweak the limits a bit. '''\ndf_car['sentiment score'] = df_car['review'].apply(sentimenter)\ndef sent_score_string(x):\n  if (x['compound']>= 0.05):\n    return 'positive'\n  elif (x['compound']> -0.05 and x['compound']< 0.05) :\n    return 'neutral'\n  elif (x['compound'] <= -0.05):\n    return 'negative'\n\ndf_car['relative sentiments'] = df_car['sentiment score'].apply(sent_score_string)","92eb683f":"df_car","5076a4e0":"#these numbers tell how many reviews of each kind are present\ndf_car['relative sentiments'].value_counts()","9c7bacec":"df_car.brand_name.value_counts()","05a951a8":"df_car.drop('Unnamed: 0',axis=1,inplace=True)","2ba6b93a":"df_car.info()","53b37789":"df_car['Rating'].unique()\ndf_car = df_car[df_car['Rating']<=4]","d4436f23":"df_car = df_car[df_car['relative sentiments']!='positive']","cfd36f89":"stop_words = list(stopwords.words('english'))","6ec893c5":"stop_words = list(stopwords.words('english'))","9445fff6":"stop_words.extend(['porsche,' 'mercede','comfortsport', 'mercedes','mercedes-benz', 'honda','toyota','audi', 'benz','bentley','lexus',\n                  'nissan','volvo','drive','nt','like','vehicle','infiniti','good','miles','corvette','come','edmund','lotus','diego','snake',\n                 'porsche', 'cayman','bought','year','minute','chicago','car','home', 'work','think','suv','people','edmunds',\n                  'cabriolet','lexuss','japan','husband','baby','range', 'rover','cadillac','cadillacs','michelin','texas','second',\n                   'awsome','one','now', 'take', 'give', 'new','levinson','road','love','sedan','wife','sport','bang','tank',\n                   'truck','lemon','imho','pathfinder','infinity','convertible','allroad','conv','bike','ski','grocery','mclass'\n                  ,'hardtop','club','hubby','child','zoom','test','etc','brain','ashamed','carmax','alpina','rocketship','great','germany',\n                  'autobahn','mercedez','bmw'])","546bc21f":"df_car.review","7204888b":"from nltk.stem import WordNetLemmatizer \n\n# Initialise the Wordnet Lemmatizer\nlemmatizer = WordNetLemmatizer()","83b54b13":"def tokenisation_pos_stopword_lemmatize(x):\n  ''' This function was created to tokenise,POS tagging, Remove Stopwords & then lemmatise the Reviews. '''\n  tokens = nltk.word_tokenize(x)\n  #print (nltk.pos_tag(tokens))\n  #POS-tagging\n  tags = nltk.pos_tag(tokens)\n  pos_tags_words = [t for t in tags if t[1] in[\"JJ\",\"JJR\",\"JJS\",\"NN\",\"NNP\",\"NNS\",\"NNPS\",\"VB\", \"VBD\" ,\"VBG\" ,\"VBN\" ,\"VBP\", \"VBZ\"] ]\n  #stop-word removal\n  filtered_words = [t[0] for t in pos_tags_words]\n  filtered_words2 = [w for w in filtered_words if not w.lower() in stop_words]\n  #lemmatization with lowercase function\n  lemmatized_output = [lemmatizer.lemmatize(w).lower() for w in filtered_words2]\n  return lemmatized_output","62c42473":"df_car['review'] = df_car['review'].apply(tokenisation_pos_stopword_lemmatize)","fb1b0129":"df_car = df_car.reset_index()\ndf_car.drop('index',axis=1,inplace=True)\ndf_car","d6f42f3e":"big_array = []\nfor i in range(len(df_car['review'])):\n    big_array.extend(df_car['review'][i])","92f3b94c":"docs1 = [' '.join(big_array)]","80180952":"lng_sent1 =[ i for i in docs1 ]\ndocs = lng_sent1","8bed6963":"from nltk.corpus import webtext\n  \n# use to find bigrams, which are pairs of words\nfrom nltk.collocations import BigramCollocationFinder\nfrom nltk.metrics import BigramAssocMeasures\n\nbiagram_collocation = BigramCollocationFinder.from_words(big_array)\nbiagram_collocation.apply_freq_filter(3)\nbigram_list = biagram_collocation.nbest(BigramAssocMeasures.likelihood_ratio, 15)\n\nbigram_list","c911b8d6":"''' These lines show exact frequencies for bigrams.'''\nbigram_fd = nltk.FreqDist(nltk.bigrams(big_array))\n#bigram_fd.most_common()","74855605":"#we have extracted tuples inside tuples ( there are two tuples inside this tuple )\nout = [item for t in bigram_fd.most_common() for item in t] \n\nlst_tup=[] #this contains bigrams in a tuple form\nlst_num=[] #this contains the frequency of bigrams\nfor i in tqdm(out):\n    j=out.index(i)\n    if j%2==0:\n        lst_tup.append(out[j])\n    else:\n        lst_num.append(out[j])\n\n# print(lst1)\n# print(lst_num)","f7c65357":"#we have extracted tuples inside the first tuple using the above method\nlst_final = [] # this will contain the bigram in a 'X_Y' format\nlst_std = [item for t in lst_tup for item in t]\nfor i in tqdm(lst_std):\n  j=lst_std.index(i)\n  if j%2==0:\n    t=lst_std[j]+'_' + lst_std[j+1]\n    lst_final.append(t)\n\n#print(lst_final)","f239d206":"bgm_wth_freq = []\n\nfor i in tqdm(range(len(lst_final))):\n  if lst_num[i] >= 3:\n    l = [lst_final[i]] * lst_num[i]\n    bgm_wth_freq.extend(l)\n  else:\n    break\n ","0fde6982":"random.shuffle(bgm_wth_freq)\nlng_sent45 =[ i for i in bgm_wth_freq ]\ndocs = lng_sent45","bfffaa79":"# Loading Libraries\nfrom nltk.collocations import TrigramCollocationFinder\nfrom nltk.metrics import TrigramAssocMeasures\n  \ntrigram_collocation = TrigramCollocationFinder.from_words(big_array)\n# trigram_list = trigram_collocation.apply_freq_filter(3)\n  \ntrigram_list = trigram_collocation.nbest(TrigramAssocMeasures.likelihood_ratio, 15)\ntrigram_list","d0aa6724":"''' These lines show exact frequencies for trigrams.'''\ntrigram_fd = nltk.FreqDist(nltk.trigrams(big_array))\ntrigram_fd.most_common()","458a26b6":"from numpy import array\n\na = array(docs)\nprint (a.shape)\n","f7e0fc94":"# Tokenize the documents.\nfrom nltk.tokenize import RegexpTokenizer\n\n# Split the documents into tokens.\ntokenizer = RegexpTokenizer(r'\\w+')\nfor idx in range(len(docs)):\n    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n\n# Remove numbers, but not words that contain numbers.\ndocs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n\n# Remove words that are only one character.\ndocs = [[token for token in doc if len(token) > 1] for doc in docs]","019721aa":"# Lemmatize the documents.\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\ndocs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]","6f814911":"# Compute bigrams.\nfrom gensim.models import Phrases\n\n# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\nbigram = Phrases(docs, min_count=20)\nfor idx in tqdm(range(len(docs))):\n    for token in bigram[docs[idx]]:\n        if '_' in token:\n            # Token is a bigram, add to document.\n            docs[idx].append(token)","78e54357":"# Remove rare and common tokens.\nfrom gensim.corpora import Dictionary\n\n# Create a dictionary representation of the documents.\ndictionary = Dictionary(docs)\n\n# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n#dictionary.filter_extremes(no_below=20, no_above=0.5)","64cf32ee":"# Bag-of-words representation of the documents.\ncorpus = [dictionary.doc2bow(doc) for doc in docs]","ff342510":"print('Number of unique tokens: %d' % len(dictionary))\nprint('Number of documents: %d' % len(corpus))","56e45b08":"# Train LDA model.\nfrom gensim.models import LdaModel\n\n# Set training parameters.\nnum_topics = 30\nchunksize = 2000\npasses = 20\niterations = 400\neval_every = None  # Don't evaluate model perplexity, takes too much time.\n\n# Make a index to word dictionary.\ntemp = dictionary[0]  # This is only to \"load\" the dictionary.\nid2word = dictionary.id2token\n\nmodel = LdaModel(\n    corpus=corpus,\n    id2word=id2word,\n    chunksize=chunksize,\n    alpha=0.1,\n    eta= 0.001,\n    iterations=iterations,\n    num_topics=num_topics,\n    passes=passes,\n    eval_every=eval_every,\n    random_state = 123\n)","b0a0ac0d":"print(model.print_topics())","fe74da44":"top_topics = model.top_topics(corpus) #, num_words=20)\n\n# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\navg_topic_coherence = sum([t[1] for t in top_topics]) \/ num_topics\nprint('Average topic coherence: %.4f.' % avg_topic_coherence)\n\nfrom pprint import pprint\npprint(top_topics)","0b2d9406":"# Compute Coherence Score using c_v\ncoherence_model_lda = CoherenceModel(model= model, texts=docs, dictionary=dictionary, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","c607b0fe":"# Compute Coherence Score using UMass\ncoherence_model_lda = CoherenceModel(model= model, texts=docs, dictionary=dictionary, coherence=\"u_mass\")\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","26766abf":"def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n    \"\"\"\n    Compute c_v coherence for various number of topics\n\n    Parameters:\n    ----------\n    dictionary : Gensim dictionary\n    corpus : Gensim corpus\n    texts : List of input texts\n    limit : Max num of topics\n\n    Returns:\n    -------\n    model_list : List of LDA topic models\n    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n    \"\"\"\n    coherence_values = []\n    model_list = []\n    for num_topics in range(start, limit, step):\n        model=LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n        model_list.append(model)\n        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n        coherence_values.append(coherencemodel.get_coherence())\n\n    return model_list, coherence_values","83438692":"model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=docs, start=22, limit=35, step=1)\n# Show graph\nimport matplotlib.pyplot as plt\nlimit=35; start=22; step=1;\nx = range(start, limit, step)\nplt.style.use(\"fivethirtyeight\")\nplt.plot(x, coherence_values)\nplt.xlabel(\"Num Topics\")\nplt.ylabel(\"Coherence score\")\nplt.legend((\"coherence_values\"), loc='best')\nplt.show()","cf386b4a":"#Visualize the topic\npyLDAvis.enable_notebook()\nLDAvis_prepared = pyLDAvis.gensim_models.prepare(model, corpus=corpus, dictionary = dictionary,sort_topics=False)\nLDAvis_prepared","3be95c29":"#Feature Extraction\n","95bb6314":"#Sentiment Analysis","e02251dc":"# Visualisations After Topic Modelling with  PyLDAVis","33b6921b":"#Preprocessing","820ff782":"#Topic Modelling","8a33c041":"##Biigram ( Collocations With 2 words )","821e8da3":"##Trigram ( Collocations With 3 words )"}}