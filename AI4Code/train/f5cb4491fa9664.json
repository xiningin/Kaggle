{"cell_type":{"c8641201":"code","c4f9face":"code","b94d2521":"code","7a48bb34":"code","787e311e":"code","b368edbd":"code","32f7ef9a":"code","b82b071e":"code","d4454115":"code","8f8bea88":"code","cd798708":"code","c0865d41":"code","56276ac7":"code","59ce127d":"code","c38e361b":"code","488c261a":"code","fb97a6f2":"code","c88ebdf0":"code","b06658b9":"code","ba428848":"code","5e9db708":"code","1db67980":"code","eafedc21":"code","9b9b0537":"code","4eb425a2":"code","dec40d78":"code","a7d49b58":"code","89017db5":"code","aa742cce":"code","1fcc1511":"code","a9e99cce":"code","47aba6f8":"code","d5b3b3a5":"code","4c1f1dc9":"code","0ee40ad0":"code","af1cf1ba":"code","e7ee1ae0":"code","62df2cb5":"code","161fad2e":"code","7f0f2a00":"code","49085700":"code","8d896869":"code","5e6c3749":"code","fa3b4875":"code","d7b37735":"code","8ccce130":"code","d7832dc1":"code","fe83147b":"code","37dba8bf":"code","58aae32d":"code","0a4ae33a":"code","837e5f3c":"markdown","57e8b622":"markdown","e1646a80":"markdown","0ba9ec5d":"markdown","12997ee8":"markdown","3a0bebba":"markdown","4819a8a1":"markdown","40d226f7":"markdown","d51b8fb1":"markdown","5445e347":"markdown","bdc86421":"markdown","8d03dc5c":"markdown","08b1474d":"markdown","a6b1d01e":"markdown","dd8ad827":"markdown","40ef9e03":"markdown","22ced8f7":"markdown","a640ec9e":"markdown","1a1a64b6":"markdown","2b199852":"markdown","b2d798d4":"markdown","724222ca":"markdown","572cf463":"markdown","7cd5f9e9":"markdown","1e37607c":"markdown","fd890150":"markdown","e823785c":"markdown","0e7a0bc6":"markdown","e492948c":"markdown","af9b13ec":"markdown","e56b7d94":"markdown","6488c513":"markdown","e0ec37a9":"markdown","553e632d":"markdown","1153cfb7":"markdown","40673b07":"markdown","72f43b7a":"markdown","e237cb98":"markdown","45b32b6b":"markdown","f6c0a716":"markdown","a25b9848":"markdown","5657a0af":"markdown","beb3ae53":"markdown","6ea35c58":"markdown","9020a815":"markdown","34c708ea":"markdown","09cba33d":"markdown","d07882ec":"markdown","4c0336b9":"markdown","39bf72de":"markdown","08c6eac8":"markdown","de0a3dbc":"markdown","73469402":"markdown","58f4029a":"markdown","5ea93b32":"markdown","83d7aa32":"markdown","df0531fb":"markdown","e5a4c743":"markdown","503360bd":"markdown"},"source":{"c8641201":"import numpy as np\nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')","c4f9face":"dataset = pd.read_excel(\"..\/input\/east-west-airlines\/EastWestAirlines.xlsx\", sheet_name='data')","b94d2521":"dataset.head()","7a48bb34":"# Column rename.\n\ndataset= dataset.rename(columns={'ID#':'ID', 'Award?':'Award'})","787e311e":"# not going to falloe EDA step here since it is already done in link1.(Above cell)\n# as we know ID & award will not make much contribution during clutering. we will drop both columns.\n\ndataset1 =  dataset.drop(['ID','Award'], axis=1)\ndataset1.head(2)","b368edbd":"\nfrom sklearn.preprocessing import StandardScaler\n\nstd_df = StandardScaler().fit_transform(dataset1)      # this will used for kmeans\nstd_df.shape","32f7ef9a":"# Using Minmaxscaler for accuracy result comparison\n\nfrom sklearn.preprocessing import MinMaxScaler\nminmax = MinMaxScaler()\n\nminmax_df = minmax.fit_transform(dataset1)\nminmax_df.shape","b82b071e":"# applying PCA on std_df\n\n# we are considering 95% variance in n_components to not loose any data.\n\nfrom sklearn.decomposition import PCA\npca_std = PCA(random_state=10, n_components=0.95)\npca_std_df= pca_std.fit_transform(std_df)","d4454115":"# eigenvalues..\n\nprint(pca_std.singular_values_)","8f8bea88":"# variance containing in each formed PCA\n\nprint(pca_std.explained_variance_ratio_*100)","cd798708":"# Cummulative variance ratio..\n\n# this will give an idea of, at how many no. of PCAs, the cummulative addition of\n#........variance will give much information..\n\ncum_variance = np.cumsum(pca_std.explained_variance_ratio_*100)\ncum_variance","c0865d41":"# applying PCA on minmax_df\n\nfrom sklearn.decomposition import PCA\n\npca_minmax =  PCA(random_state=10, n_components=0.95)\npca_minmax_df = pca_minmax.fit_transform(minmax_df)","56276ac7":"# eigenvalues..\n\nprint(pca_minmax.singular_values_)","59ce127d":"# variance containing in each formed PCA\n\nprint(pca_minmax.explained_variance_ratio_*100)","c38e361b":"# 1. How many number of clusters? n_clusters?\n\n# Since true labels are not known..we will Silhouette Coefficient (Clustering performance evaluation)\n# knee Elbow graph method\n\n#Import the KElbowVisualizer method\nfrom sklearn.cluster import KMeans\nfrom yellowbrick.cluster import KElbowVisualizer\n\n\n# Instantiate a scikit-learn K-Means model. we will check for two diff hyperparameters value effect.\nmodel1 = KMeans(random_state=0,n_jobs=-1,)\nmodel2 = KMeans(random_state=10, n_jobs=-1, max_iter=500, n_init=20,)\n\n# Instantiate the KElbowVisualizer with the number of clusters and the metric\nvisualizer1 = KElbowVisualizer(model1, k=(2,10), metric='silhouette', timings=False)\nvisualizer2 = KElbowVisualizer(model2, k=(2,10), metric='silhouette', timings=False)\n# Fit the data and visualize\nprint('model1')\nvisualizer1.fit(pca_std_df)    \nvisualizer1.poof()\nplt.show()\n\nprint('model2')\nvisualizer2.fit(pca_std_df)    \nvisualizer2.poof()\nplt.show()","488c261a":"from sklearn.metrics import silhouette_score\n\nlist1= [2,3,4,5,6,7,8,9]  # always start number from 2.\n\nfor n_clusters in list1:\n    clusterer1 = KMeans(n_clusters=n_clusters, random_state=0,n_jobs=-1)\n    cluster_labels1 = clusterer1.fit_predict(pca_std_df)\n    sil_score1= silhouette_score(pca_std_df, cluster_labels1)\n    print(\"For n_clusters =\", n_clusters,\"The average silhouette_score is :\", sil_score1)","fb97a6f2":"# 1. How many number of clusters? n_clusters?\n\n# Since true labels are not known..we will Silhouette Coefficient (Clustering performance evaluation)\n# knee Elbow graph method\n\n#Import the KElbowVisualizer method\nfrom sklearn.cluster import KMeans\nfrom yellowbrick.cluster import KElbowVisualizer\n\n\n# Instantiate a scikit-learn K-Means model. we will check for two diff hyperparameters value effect.\nmodel3 = KMeans(random_state=0,n_jobs=-1)\nmodel4 = KMeans(random_state=10, n_jobs=-1, max_iter=500, n_init=20)\n\n# Instantiate the KElbowVisualizer with the number of clusters and the metric\nvisualizer3 = KElbowVisualizer(model3, k=(2,10), metric='silhouette', timings=False)\nvisualizer4 = KElbowVisualizer(model4, k=(2,10), metric='silhouette', timings=False)\n# Fit the data and visualize\nprint('model3')\nvisualizer3.fit(pca_minmax_df)    \nvisualizer3.poof()\nplt.show()\n\nprint('model4')\nvisualizer4.fit(pca_minmax_df)    \nvisualizer4.poof()\nplt.show()","c88ebdf0":"from sklearn.metrics import silhouette_score\n\nlist1= [2,3,4,5,6,7,8,9]  # always start number from 2.\n\nfor n_clusters in list1:\n    clusterer2 = KMeans(n_clusters=n_clusters, random_state=0,n_jobs=-1)\n    cluster_labels2 = clusterer1.fit_predict(pca_minmax_df)\n    sil_score2= silhouette_score(pca_std_df, cluster_labels2)\n    print(\"For n_clusters =\", n_clusters,\"The average silhouette_score is :\", sil_score2)","b06658b9":"# we have found good number of cluster = 6\n# model building using cluster numbers = 6\n\nmodel1 = KMeans(n_clusters=6, random_state=0,n_jobs=-1)\ny_predict1 = model1.fit_predict(pca_std_df)\ny_predict1.shape","ba428848":"# these are nothing but cluster labels...\n\ny_predict1","5e9db708":"# y_predict & cluster labels both are same use any one of them to avoid further confusion.\n\nmodel1.labels_","1db67980":"# cluster centres associated with each lables\n\nmodel1.cluster_centers_","eafedc21":"# within-cluster sum of squared\n\n# The lower values of inertia are better and zero is optimal.\n# Inertia is the sum of squared error for each cluster. \n# Therefore the smaller the inertia the denser the cluster(closer together all the points are)\n\nmodel1.inertia_","9b9b0537":"model1.score(pca_std_df) \n\n# it is opposite value of sum of squared value..avoid to use it. It is bit confusing","4eb425a2":"# this will give what hyper parameter is used in model.\n\n\nmodel1.get_params()","dec40d78":"from yellowbrick.cluster import SilhouetteVisualizer\n\nfig,(ax1,ax2) = plt.subplots(1,2,sharey=False)\nfig.set_size_inches(15,6)\n\n\n\nsil_visualizer1 = SilhouetteVisualizer(model1,ax= ax1, colors=['#922B21','#5B2C6F','#1B4F72','#32a84a','#a83232','#323aa8'])\nsil_visualizer1.fit(pca_std_df)\n\n\n# 2nd Plot showing the actual clusters formed\n\nimport matplotlib.cm as cm\ncolors1 = cm.nipy_spectral(model1.labels_.astype(float) \/ 6) # 6 is number of clusters\nax2.scatter(pca_std_df[:, 0], pca_std_df[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n                c=colors1, edgecolor='k')\n\n# Labeling the clusters\ncenters1 = model1.cluster_centers_\n# Draw white circles at cluster centers\nax2.scatter(centers1[:, 0], centers1[:, 1], marker='o',c=\"white\", alpha=1, s=200, edgecolor='k')\n\nfor i, c in enumerate(centers1):\n    ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,s=50, edgecolor='k')\n\n\nax2.set_title(label =\"The visualization of the clustered data.\")\nax2.set_xlabel(\"Feature space for the 1st feature\")\nax2.set_ylabel(\"Feature space for the 2nd feature\")\n\nplt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n                  \"with n_clusters = %d\" % 6),fontsize=14, fontweight='bold')\n\nsil_visualizer1.show()\nplt.show()\n","a7d49b58":"# Creating dataframe of cluster lables..\n\nmodel1_cluster = pd.DataFrame(model1.labels_.copy(), columns=['Kmeans_Clustering'])","89017db5":"# Concating model1_Cluster df with main dataset copy\n\nKmeans_df = pd.concat([dataset.copy(), model1_cluster], axis=1)\nKmeans_df.head()","aa742cce":"# Plotting barplot using groupby method to get visualize how many row no. in each cluster\n\nfig, ax = plt.subplots(figsize=(10, 6))\nKmeans_df.groupby(['Kmeans_Clustering']).count()['ID'].plot(kind='bar')\nplt.ylabel('ID Counts')\nplt.title('Kmeans Clustering (pca_std_df)',fontsize='large',fontweight='bold')\nax.set_xlabel('Clusters', fontsize='large', fontweight='bold')\nax.set_ylabel('ID counts', fontsize='large', fontweight='bold')\nplt.yticks(fontsize=15)\nplt.xticks(fontsize=15)\nplt.show()","1fcc1511":"# Applying Dendrogram on PCA data. Or you may apply it on Standardized\/normalized indepedent variable data.\n# Here diffrent linkage method from hyperparameter is used to see diff between methods for understanding. \n# Ward method is commanly used since it is simpler to visualize understanding.\n# Find number of cluster's using color coding of dendrogram. Each color indicates one cluster.\n\n# import scipy.cluster.hierarchy as shc\n# for methods in ['single','complete','average','weighted','centroid','median','ward']: \n#     plt.figure(figsize =(20, 6)) \n    \n#     dict = {'fontsize':24,'fontweight' :16, 'color' : 'blue'}\n    \n#     plt.title('Visualising the data, Method- {}'.format(methods),fontdict = dict) \n#     Dendrogram1 = shc.dendrogram(shc.linkage(pca_std_df, method = methods,optimal_ordering=False))\n    \n# Note: the execution of this cell takes time so i have attached output graphs below","a9e99cce":"from sklearn.cluster import AgglomerativeClustering\nn_clusters = [2,3,4,5,6,7,8]  # always start number from 2.\n\nfor n_clusters in n_clusters:\n    for linkages in [\"ward\", \"complete\", \"average\", \"single\"]:\n        hie_cluster1 = AgglomerativeClustering(n_clusters=n_clusters,linkage=linkages) # bydefault it takes linkage 'ward'\n        hie_labels1 = hie_cluster1.fit_predict(pca_std_df)\n        silhouette_score1 = silhouette_score(pca_std_df, hie_labels1)\n        print(\"For n_clusters =\", n_clusters,\"The average silhouette_score with linkage-\",linkages, ':',silhouette_score1)\n    print()\n","47aba6f8":" ### Here i have avoded to apply dendrogram since it takes time to run code.","d5b3b3a5":"from sklearn.cluster import AgglomerativeClustering\nn_clusters = [2,3,4,5,6,7,8]  # always start number from 2.\n\nfor n_clusters in n_clusters:\n    for linkages in [\"ward\", \"complete\", \"average\", \"single\"]:\n        hie_cluster2 = AgglomerativeClustering(n_clusters=n_clusters,linkage=linkages) # bydefault it takes linkage 'ward'\n        hie_labels2 = hie_cluster2.fit_predict(pca_minmax_df)\n        silhouette_score2 = silhouette_score(pca_minmax_df, hie_labels2)\n        print(\"For n_clusters =\", n_clusters,\"The average silhouette_score with linkage-\",linkages, ':',silhouette_score2)\n    print()","4c1f1dc9":"agg_clustering = AgglomerativeClustering(n_clusters=5, linkage='average')\ny_pred_hie = agg_clustering.fit_predict(pca_std_df)\nprint(y_pred_hie.shape)\ny_pred_hie","0ee40ad0":"# Cluster numbers\n\nagg_clustering.n_clusters_","af1cf1ba":"# cluster labels for each point\n\nagg_clustering.labels_","e7ee1ae0":"# Number of leaves in the hierarchical tree.\n\nagg_clustering.n_leaves_","62df2cb5":"# The estimated number of connected components in the graph.\n\nagg_clustering.n_connected_components_","161fad2e":"# The children of each non-leaf node. Values less than n_samples correspond to leaves of \n#......the tree which are the original samples. A node i greater than or equal to n_samples \n#.........is a non-leaf node and has children children_[i - n_samples]. Alternatively at the \n#...........i-th iteration, children[i][0] and children[i][1] are merged to form node n_samples + i\n\nagg_clustering.children_","7f0f2a00":"# Clustering Score\n\n(silhouette_score(pca_std_df, agg_clustering.labels_)*100).round(3)","49085700":"# Plotting Dendrogram.\n\nimport scipy.cluster.hierarchy as shc\nfor methods in ['average']: \n    plt.figure(figsize =(20, 6)) \n    \n    dict = {'fontsize':24,'fontweight' :16, 'color' : 'blue'}\n    \n    plt.title('Visualising the data, Method- {}'.format(methods),fontdict = dict) \n    Dendrogram2 = shc.dendrogram(shc.linkage(pca_std_df, method = methods,optimal_ordering=False))","8d896869":"# Creating dataframe of cluster lables..\n\nhie_cluster = pd.DataFrame(agg_clustering.labels_.copy(), columns=['Hie_Clustering'])","5e6c3749":"# Concating model1_Cluster df with main dataset copy\n\nhie_df = pd.concat([dataset.copy(), hie_cluster], axis=1)\nhie_df .head()","fa3b4875":"# Plotting barplot using groupby method to get visualize how many row no. in each cluster\n\nfig, ax = plt.subplots(figsize=(10, 6))\nhie_df.groupby(['Hie_Clustering']).count()['ID'].plot(kind='bar')\nplt.ylabel('ID Counts')\nplt.title('Hierarchical Clustering (pca_std_df)',fontsize='large',fontweight='bold')\nax.set_xlabel('Clusters', fontsize='large', fontweight='bold')\nax.set_ylabel('ID counts', fontsize='large', fontweight='bold')\nplt.yticks(fontsize=15)\nplt.xticks(fontsize=15)\nplt.show()","d7b37735":"Kmeans_df.groupby(['Kmeans_Clustering']).count()","8ccce130":"hie_df.groupby(['Hie_Clustering']).count()","d7832dc1":"# Groupby Cluster lables\n\ncount_df = Kmeans_df.groupby(['Kmeans_Clustering']).count()\ncount_df","fe83147b":"# Total numbers in each cluster..\n\ncount = count_df.xs('ID' ,axis = 1)\ncount.plot(kind='bar', title= 'Nuber Counts')\nplt.show()","37dba8bf":"# Sorting elements based on cluster label assigned and taking average for insights.\n\ncluster1 = pd.DataFrame(Kmeans_df.loc[Kmeans_df.Kmeans_Clustering==0].mean(),columns= ['Cluster1_avg'])\ncluster2 = pd.DataFrame(Kmeans_df.loc[Kmeans_df.Kmeans_Clustering==1].mean(),columns= ['Cluster2_avg'])\ncluster3 = pd.DataFrame(Kmeans_df.loc[Kmeans_df.Kmeans_Clustering==2].mean(),columns= ['Cluster3_avg'])\ncluster4 = pd.DataFrame(Kmeans_df.loc[Kmeans_df.Kmeans_Clustering==3].mean(),columns= ['Cluster4_avg'])\ncluster5 = pd.DataFrame(Kmeans_df.loc[Kmeans_df.Kmeans_Clustering==4].mean(),columns= ['Cluster5_avg'])","58aae32d":"avg_df = pd.concat([cluster1,cluster2,cluster3,cluster4,cluster5],axis=1)\navg_df","0a4ae33a":"# Extract and plot one Column data .xs method\nfor i , row in avg_df.iterrows():\n    fig = plt.subplots(figsize=(8,6))\n    j = avg_df.xs(i ,axis = 0)\n    plt.title(i, fontsize=16, fontweight=20)\n    j.plot(kind='bar',fontsize=14)\n    plt.show()\n    print()","837e5f3c":"![](https:\/\/github.com\/ShrikantUppin\/Clustering\/blob\/main\/dendro1_single.png?raw=true)","57e8b622":"##### First Stage: Taking stadardization transformation data & PCA applied on it.\n***","e1646a80":"## 5.1.4 Putting Cluster lables into original dataset And analysis of the same.","0ba9ec5d":"## 6.2 Run Hierarchical Clustering.(Agglomerative Clustering)","12997ee8":"![](https:\/\/github.com\/ShrikantUppin\/Clustering\/blob\/main\/dendro1_median.png?raw=true)","3a0bebba":" ## I am using two transformation..since to ckeck how cluster numbers varry with diff transformation.","4819a8a1":"# Kmeans Clustering.","40d226f7":"* I have applied EDA to analyze dataset.Discovered correlation between diff variables and found no colinearity.\n\n\n* Applied Standardazation & MinMaxScalar transformation on the data to use Principle componets analysis effectively.\n\n\n* I have used & analyzed two clustering techniques here..i) KMeans & ii) Hierarchical Clusterig.\n\n* By applying clustering on diff. PCA obtained with diff transformation data shows fluctuation in model score. So finally the MinMaxScalr found less score so not used for further model building.\n\n\n* KMeans clustering is sensitive to outliers.Since produced min. sil. score. whereas hierarchical gives max. in this case.\n\n\n* Hierarchical clustering given total cluster number=5, but when i comapred both kmean & Hierarchical clustering, the hierarchical clustering contains approx 99% data in cluster 1 only. So this will not useful to analyze diff. customer. So i continued with KMeans.","d51b8fb1":"#### Conclusion:\n    \n * By taking pca_minmax_df data it gives minimum number of cluster =4 with silhoette score = 0.1476\n \n \n * If we check silhouette score with standardize data pca kmeans model 0.36>0.14 So we will PCA which is applied on normalizatied data. Since the score is very less. Best score alwways close to +1.","5445e347":"# Step 6: Hierarchical Clustering Algorithm","bdc86421":"1. [Sklearn Clustering Notes](https:\/\/scikit-learn.org\/stable\/modules\/clustering.html)\n\n\n2. [Kmeans clustering sklearn](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)\n\n***\ni) Parallel K-means Clustering:\n    \n* [ i) Parallel K-means Clustering. Sklearn demo 1](https:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_kmeans_assumptions.html#sphx-glr-auto-examples-cluster-plot-kmeans-assumptions-py)\n* [ ii) Parallel K-means Clustering Kmeans using different cluster quality metrics](https:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_kmeans_digits.html#sphx-glr-auto-examples-cluster-plot-kmeans-digits-py)\n    \n    \nii) MiniBatch Kmeans\n    \n* [i) comparision between kmeans & Mini batch kmeans](https:\/\/scikitlearn.org\/stable\/auto_examples\/cluster\/plot_mini_batch_kmeans.html#sphx-glr-auto-examples-cluster-plot-mini-batch-kmeans-py)\n* [ii) Clustering text documents using k-means](https:\/\/scikit-learn.org\/stable\/auto_examples\/text\/plot_document_clustering.html#sphx-glr-auto-examples-text-plot-document-clustering-py)\n* [iii) Online learning of a dictionary of parts of faces](https:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_dict_face_patches.html#sphx-glr-auto-examples-cluster-plot-dict-face-patches-py)\n\n     \n***\n\n3. [Clustering performance evaluation](https:\/\/scikit-learn.org\/stable\/modules\/clustering.html#clustering-evaluation)","8d03dc5c":" #### Conclusion:\n \n* Since we don't know true labels so we have used silhoutte score method to determine good k numbers based on score.\n\n\n* Also it is proved that even if we feed data either PCA or scaled data outliers will always affect in kmeans clustering. Also when random_state value, max_iter number n_int chnaged the clustering numbers changed.\n\n\n* So before using KMeans clustering it is better to have discussion or decision on outliers.","08b1474d":"****","a6b1d01e":"# Step 2: Load dataset","dd8ad827":" #### Method1:","40ef9e03":"# Step 7: Conclusion Between Kmeans & Hierarchical","22ced8f7":" #### Conclusion:\n    \n * By taking standardize data pca it gives number of cluster =6 with silhoette score = 0.36","a640ec9e":"##### Second Stage: Taking MinMaxScalar transformation data & PCA applied on it.","1a1a64b6":"##### First Stage: Taking stadardization transformation data & PCA applied on it.\n***","2b199852":" ### standardization","b2d798d4":"## 5.1.2 Run K-Means:","724222ca":" ### 4.1 Running PCA of standardized data.","572cf463":"# Step 1: Import libraries required.","7cd5f9e9":"## 6.4 Putting Cluster lables into original dataset And analysis of the same.","1e37607c":" #### Method 2: Silhouette Score method.","fd890150":"# Step 5: KMeans Clustering.\n***","e823785c":" Conclusion: Although max score is with cluster numbers 4. I have selected tcluster number = 5","0e7a0bc6":" Conclusion:\n \n * by applying PCA on standardized data with 95% variance it gives 8 PCA components.","e492948c":"![](https:\/\/github.com\/ShrikantUppin\/Clustering\/blob\/main\/Kmeans_ID_column.png?raw=true)","af9b13ec":"![](https:\/\/github.com\/ShrikantUppin\/Clustering\/blob\/main\/dendr1_weighted.png?raw=true)","e56b7d94":" * Conclusion: We will go with Kmeans Clustering Since in hierarchiacl clustering all data is gathherd in one cluster only.","6488c513":"Data feature information availabele in https:\/\/github.com\/ShrikantUppin\/Clustering\/tree\/main\/1.%20Data%20preprocessing%20%26%20PCA%20output","e0ec37a9":" ### MinMaxScalar","553e632d":" Conclusion: dataset with MinMax transformation & PCA applied on it doesn't imporoved score here. So i am taking previous cell code conclusion.","1153cfb7":"![](https:\/\/github.com\/ShrikantUppin\/Clustering\/blob\/main\/dendro1_average.png?raw=true)","40673b07":" #### Method 2","72f43b7a":"# Conclusion:","e237cb98":" #### Method1: By using Dendrogram","45b32b6b":"## 5.1.3 visualizing silhoutte score..for different cluster","f6c0a716":"![](https:\/\/github.com\/ShrikantUppin\/Clustering\/blob\/main\/task.jpg?raw=true)","a25b9848":"[Hierarchical Clustering Sklearn](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.cluster.AgglomerativeClustering.html)","5657a0af":"![](https:\/\/github.com\/ShrikantUppin\/Clustering\/blob\/main\/dendrogram1.png?raw=true)","beb3ae53":" ## Important Links:","6ea35c58":"# Step 4: PCA","9020a815":" #### method2:","34c708ea":"![](https:\/\/github.com\/ShrikantUppin\/Clustering\/blob\/main\/Hierarchical_Clustering.png?raw=true)","09cba33d":"# Cluster Anlysis for Future Decision (Kmeans clustering)","d07882ec":"![](https:\/\/github.com\/ShrikantUppin\/Clustering\/blob\/main\/dendro1_complete.png?raw=true)","4c0336b9":"Conclusion :\n    \n* As decided need to focous on first two clusters..\n\n\n* Considering past Award status(0 or 1) cluster number1 shows award avg as 0.56 that means people awarded with scheme. \n\n\n* Similarly for cluster number 2 award avg. is minimum. Since avg is low that means now maximim award given to cluster number1 in the past. Need to add future schemes for cluster number two members on different bnus points & different card tranction bonus. ","39bf72de":"Conclusion:\n\n* In cluster number 0 & 1 there are more customers. Need to focus on cluster 0 & 1. (clusters avg. is taken &  renamed from next code onwards)\n\n***","08c6eac8":" Qiuck Notes: \n\nlinkage{\u201cward\u201d, \u201ccomplete\u201d, \u201caverage\u201d, \u201csingle\u201d}, default=\u201dward\u201d\nWhich linkage criterion to use. The linkage criterion determines which distance to use between sets of observation. The algorithm will merge the pairs of cluster that minimize this criterion.\n\n* ward minimizes the variance of the clusters being merged.\n\n* average uses the average of the distances of each observation of the two sets.\n\n* complete or maximum linkage uses the maximum distances between all observations of the two sets.\n\n* single uses the minimum of the distances between all observations of the two sets.","de0a3dbc":" Conclusion:\n \n * By applying PCA on MinMaxscalr transformation data gives 5 PCA components.","73469402":"## 6.1 How many numbers of cluster. Also deciding which transformation data we will use to build model.","58f4029a":"![](https:\/\/github.com\/ShrikantUppin\/Clustering\/blob\/main\/dendro1_ward.png?raw=true)","5ea93b32":"<p align='justify'> The key operation in hierarchical agglomerative clustering is to repeatedly combine the two nearest clusters into a larger cluster. There are three key questions that need to be answered first: <\/p>","83d7aa32":" ### 4.2 Running PCA of MinMaxscalar data.","df0531fb":"## 5.1.1 How many numbers of cluster. Also deciding which transformation data we will use to build model.","e5a4c743":"##### Second Stage: Taking MinMax transformation data & PCA applied on it.\n***","503360bd":"# Step 3: Data preprocessing "}}