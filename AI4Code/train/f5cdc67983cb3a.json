{"cell_type":{"fa28a14a":"code","3be3ce72":"code","412df58b":"code","189bd846":"code","4fdc7e88":"code","a596e742":"code","d79e10fd":"code","d635eb48":"code","701348bb":"code","ad7ca6cd":"code","e0c6c6fe":"code","ec1d6605":"code","ff8148fd":"code","20b3e299":"code","90cac739":"code","7234ee9d":"code","923c5aa3":"code","b5511137":"code","9b52d8f8":"code","d0d531cd":"code","ccf82fba":"code","9144409b":"code","0b3caeec":"code","3f7a3bd2":"code","90e80d8b":"code","b9495788":"code","e0e1e408":"markdown","9add8b77":"markdown","9c20103d":"markdown","23783b28":"markdown","a2b299e5":"markdown","da80dc54":"markdown","1583e4cc":"markdown","dec23af9":"markdown"},"source":{"fa28a14a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3be3ce72":"!pip install -U spacy","412df58b":"!python -m spacy download en_core_web_lg","189bd846":"!python -m spacy download en_core_web_sm","4fdc7e88":"!pip install wordcloud","a596e742":"import spacy\nnlp = spacy.load('en_core_web_sm')","d79e10fd":"#Code by Paul Mooney\n\nfriston_file = '..\/input\/open-access-karl-fristons-papers-txt\/23133414.txt'\nwith open(friston_file) as f: # The with keyword automatically closes the file when you are done\n    print (f.read(3000))","d635eb48":"import spacy\nnlp = spacy.load(\"en_core_web_sm\")\n\ndoc = nlp(\"So, why are you reading this paper?\")\nfor token in doc:\n    print(token.text, token.pos_, token.dep_)","701348bb":"# Create a nlp object\ndoc = nlp(\"The answer is fairly simple: you are compelled to selectively sample sensory input that conforms to your predictions\")","ad7ca6cd":"nlp.pipe_names","e0c6c6fe":"nlp.disable_pipes('tagger', 'parser')","ec1d6605":"nlp.pipe_names","ff8148fd":"import spacy\n\nnlp = spacy.load('en_core_web_sm')\ndoc = nlp(\"in which behavior is cast as active Bayesian inference that is constrained by prior beliefs\")\nfor token in doc:\n    print(token.text)","20b3e299":"nlp = spacy.load('en_core_web_sm')\n\n# Create an nlp object\ndoc = nlp(\"If behavior is optimal, then it maximizes value\")\n \n# Iterate over the tokens\nfor token in doc:\n    # Print the token and its part-of-speech tag\n    print(token, token.tag_, token.pos_, spacy.explain(token.tag_))","90cac739":"from spacy import displacy\n\ndoc = nlp(\"The same objective: to maximize the Bayesian model evidence averaged over time.\")\ndisplacy.render(doc, style=\"dep\" , jupyter=True)","7234ee9d":"nlp = spacy.load('en_core_web_sm')\n\n# Create an nlp object\ndoc = nlp(\"The principle of least action, the principle of maximum entropy, the principle of minimum redundancy and the principle of maximum information transfer.\")\n \n# Iterate over the tokens\nfor token in doc:\n    # Print the token and its part-of-speech tag\n    print(token.text, \"-->\", token.dep_)","923c5aa3":"spacy.explain(\"nsubj\"), spacy.explain(\"ROOT\"), spacy.explain(\"aux\"),spacy.explain('nmod'), spacy.explain(\"advcl\"), spacy.explain(\"dobj\")","b5511137":"nlp = spacy.load('en_core_web_sm')\n\n# Create an nlp object\ndoc = nlp(\"Cost functions that are used to guide action in optimal control can be absorbed into prior beliefs in active inference.\")\n \n# Iterate over the tokens\nfor token in doc:\n    # Print the token and its part-of-speech tag\n    print(token.text, \"-->\", token.lemma_)","9b52d8f8":"nlp = spacy.load('en_core_web_sm')\n\n# Create an nlp object\ndoc = nlp(\"Solutions based upon cost or reward functions that are an integral part of optimal control theory and reinforcement learning\")\n \nsentences = list(doc.sents)\nlen(sentences)","d0d531cd":"for sentence in sentences:\n     print (sentence)","ccf82fba":"nlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"Solution to this variational problem is given by a stochastic controller called the Bayesian control rule, which implements adaptive behavior as a mixture of experts.\")\n#See the entity present\nprint(doc.ents)\nfor ent in doc.ents:\n    print(ent.text, ent.start_char, ent.end_char, ent.label_)","9144409b":"from spacy import displacy\nnlp = spacy.load(\"en_core_web_sm\")\ndoc= nlp(u\"\"\"This work\nillustrates the close connections between minimizing (relative)\nentropy and the ensuing active Bayesian inference that we will appeal\nto the later.In summary, current approaches to partially observed MDPs\nand stochastic optimal control minimize cumulative cost using the same\nprocedures employed by maximum likelihood and approximate Bayesian\ninference schemes. Indeed, the formal equivalence between optimal\ncontrol and estimation was acknowledged by Kalman at the inception of\nBayesian filtering schemes. \"\"\")\n\nentities=[(i, i.label_, i.label) for i in doc.ents]\nentities","0b3caeec":"displacy.render(doc, style = \"ent\",jupyter = True)","3f7a3bd2":"nlp = spacy.load(\"en_core_web_lg\")\ntokens = nlp(\"Relationship between entropy, surprise and Bayesian model evidence in Section \u201cBayes-optimal control without cost functions,\u201d The motivation for free energy minimization in more detail.\")\n\nfor token in tokens:\n    print(token.text, token.has_vector, token.vector_norm, token.is_oov)","90e80d8b":"nlp = spacy.load(\"en_core_web_lg\")  # make sure to use larger model!\ntokens = nlp(\"Free energy principle states that the sufficient statistics of the conditional probability and action minimize free energy The first equality in Equation expresses free energy as a Gibbs energy (expected under the conditional distribution) minus the entropy of the conditional distribution.\")\n\nfor token1 in tokens:\n    for token2 in tokens:\n        print(token1.text, token2.text, token1.similarity(token2))","b9495788":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nnlp = spacy.load(\"en_core_web_lg\")  # make sure to use larger model!\ntokens = nlp(\"Free utility is fundamentally different from variational free energy,it is a functional of choice probabilities over hidden states. Variational free energy is a function of observed states. Crucially, free utility depends on a cost function, while free energy does not. The free energy principle is based on the invariant or ergodic solution 2012. In other words, value is (log) evidence or negative surprise.\")\n\nnewText =''\nfor word in tokens:\n if word.pos_ in ['ADJ', 'NOUN']:\n  newText = \" \".join((newText, word.text.lower()))\n\nwordcloud = WordCloud(stopwords=STOPWORDS, colormap='Reds', background_color=\"blue\").generate(newText)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","e0e1e408":"#Sentence Boundary Detection (SBD)","9add8b77":"#Entity Detection","9c20103d":"#Tokenization","23783b28":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcTs2mQUEvVkSBa0j1z0LHkM6taXr-9BDXiMLA&usqp=CAU)home.hellodriven.com","a2b299e5":"#Lemmatization","da80dc54":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcTvuNlT2OyjZ9vcUoNgz_20HfnL6tszy18lXw&usqp=CAU)youtube.com","1583e4cc":"#Named Entity Recognition (NER)","dec23af9":"#Part-Of-Speech (POS) Tagging"}}