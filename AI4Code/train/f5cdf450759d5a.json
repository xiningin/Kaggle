{"cell_type":{"3ee552eb":"code","c6d67e16":"code","24b52123":"code","ee5878b9":"code","952955a9":"code","4d55d24f":"code","b2fce95a":"code","fd55898b":"code","ad13d4ae":"code","78a0bb49":"code","728d845b":"code","d07e42f3":"code","5059a9fc":"code","67132ede":"code","199bba99":"code","4a821765":"code","7538ab7f":"code","e504750e":"code","0c2be56a":"code","f0f495aa":"code","344e6ae9":"code","bde6bb49":"code","404e3aa0":"code","b07fc1d6":"code","f90a9e21":"code","c340fe16":"code","693453d2":"code","a23382cc":"code","612640cb":"code","d44a5d12":"code","e982f891":"code","349421e7":"code","47526818":"code","9096b4c5":"code","23fd4f0a":"code","19ec3902":"code","29aab144":"code","abe1e6b7":"code","2ac3028b":"code","9aa67bfb":"code","8f8cdf53":"code","50ccce12":"code","10d4295e":"code","c31083ee":"code","b187448e":"code","bec65267":"code","dca53dc5":"code","a839615b":"code","5116d54b":"code","713586b0":"code","0900527d":"code","fdb777d8":"code","81448319":"code","767f0df7":"code","2538b73c":"code","e5904378":"code","daba1605":"code","860e27c2":"code","afbf7a80":"code","d6ddf96d":"code","2a08bea0":"code","f4fd07c6":"code","72cfc5dc":"code","031c9e59":"code","e7b18e85":"code","4ae0aada":"code","b779fc91":"code","a2c42bf3":"code","8b898f89":"code","086edf4c":"code","752fc4dc":"code","3c052463":"code","e136749b":"markdown","820b80aa":"markdown","c798ec7e":"markdown","50bbe9fe":"markdown","a7248e3a":"markdown","3cf39651":"markdown","fb061d12":"markdown","84562b16":"markdown","b6ac412f":"markdown","c46dbba7":"markdown","b4589e1f":"markdown","74f5f0ca":"markdown","ffaed24f":"markdown","70a66bbc":"markdown","c6497448":"markdown","c645d324":"markdown","c0045548":"markdown","1f495484":"markdown","6e08a860":"markdown","bfb6bdcc":"markdown","9a4f38f1":"markdown","30c1de53":"markdown","557bba1b":"markdown","e760505d":"markdown","8fb8c39c":"markdown","9f5354df":"markdown","b6ebe637":"markdown","69b2c3d0":"markdown","22d3d585":"markdown","66d6fd56":"markdown","2cf8a9ff":"markdown","c42be91f":"markdown","5d6363a0":"markdown","c15e6963":"markdown","80e2906b":"markdown","bdb37369":"markdown","dbfe8968":"markdown","e6ba583f":"markdown"},"source":{"3ee552eb":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline ","c6d67e16":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","24b52123":"train_data.head(1)","ee5878b9":"test_data.head()","952955a9":"print(\"train_data_shape\",train_data.shape)\nprint(\"test_data_shape\",test_data.shape)","4d55d24f":"train_data.info()","b2fce95a":"test_data.info()","fd55898b":"# train data\ntrain_data['Pclass']=train_data['Pclass'].astype('object')\ntrain_data['Survived']=train_data['Survived'].astype('object')\n# test data\ntest_data['Pclass']=test_data['Pclass'].astype('object')","ad13d4ae":"train_data.info()\nprint(\"@@@@@@\")\ntest_data.info()","78a0bb49":"train_data.describe()","728d845b":"test_data.describe()","d07e42f3":"# percentage of missing values in train_data\ntrain_data.isnull().sum()","5059a9fc":"test_data.isnull().sum()","67132ede":"plt.figure(figsize=(8,8))\nsns.violinplot(x='Age',data=train_data)\nplt.show()","199bba99":"train_data['Age'].fillna(train_data['Age'].median(), inplace=True)\ntest_data['Age'].fillna(test_data['Age'].median(), inplace=True)","4a821765":"pd.set_option('display.max_rows',891)\ntrain_data['Cabin'].value_counts()","7538ab7f":"train_data['Cabin'] = train_data['Cabin'].replace(np.nan,'Q')\ntest_data['Cabin'] = test_data['Cabin'].replace(np.nan,'Q')","e504750e":"train_data['Embarked'].value_counts()","0c2be56a":"train_data.Embarked.mode()","f0f495aa":"train_data['Embarked'] = train_data['Embarked'].replace(np.nan,'S')","344e6ae9":"train_data.head(2)","bde6bb49":"train_data.isnull().sum()","404e3aa0":"train_data.head(3)","b07fc1d6":"plt.figure(figsize = (10,10))\nsns.heatmap(train_data.corr(),annot = True,cmap=\"tab20c\")\nplt.show()","f90a9e21":"test_data['Fare'].fillna(test_data['Fare'].median(),inplace=True)","c340fe16":"test_data.isnull().sum()","693453d2":"plt.figure(figsize=(9,6))\nsns.countplot('Survived',hue='Pclass',data=train_data,palette='twilight')","a23382cc":"plt.figure(figsize=(9,6))\nsns.countplot('Survived',hue='Sex',data=train_data,palette='twilight')","612640cb":"plt.figure(figsize=(8,5))\nsns.barplot('Embarked','Survived',data=train_data,palette='muted')","d44a5d12":"plt.figure(figsize=(8,8))\nsns.violinplot(y='Fare',x='Survived',hue='Survived',data=train_data)\nplt.show()","e982f891":"plt.figure(figsize=(8,5))\nsns.violinplot(x=\"Survived\", y = \"Age\",data = train_data,palette='plasma',size=6)","349421e7":"train_data.Cabin[train_data.Survived==1].value_counts(ascending=False)","47526818":"train_data.Cabin[train_data.Survived==0].value_counts(ascending=False)","9096b4c5":"train_data['Family'] = train_data['Parch']+train_data['SibSp']+1\ntest_data['Family'] = test_data['Parch']+test_data['SibSp']+1\n# drop the Parch and SibSp columns\ntrain_data = train_data.drop(['Parch','SibSp'],axis=1)\ntest_data = test_data.drop(['Parch','SibSp'],axis=1)","23fd4f0a":"train_data.head(3)","19ec3902":"plt.figure(figsize=(8,5))\nsns.barplot('Family','Survived',data=train_data,palette='muted')","29aab144":"train_data = train_data.drop(['Name','Ticket'], axis = 1)\ntest_data = test_data.drop(['Name','Ticket'], axis = 1)","abe1e6b7":"train_data['Cabin'].nunique()","2ac3028b":"train_data = train_data.drop(['Cabin'], axis = 1)\ntest_data = test_data.drop(['Cabin'], axis = 1)","9aa67bfb":"train_data.info()","8f8cdf53":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ntrain_data[['Age', 'Fare','Family']]= scaler.fit_transform(train_data[['Age', 'Fare','Family']])\ntrain_data.head()","50ccce12":"# Scaling on test data\ntest_data[['Age', 'Fare','Family']]= scaler.transform(test_data[['Age', 'Fare','Family']])\ntest_data.head()","10d4295e":"# Creating a dummy variable for some of the categorical variables .\ndummies = pd.get_dummies(train_data[['Pclass', 'Sex','Embarked']], drop_first=True)\n\n# Adding the results to the master dataframe\ntrain_data = pd.concat([train_data, dummies], axis=1)\ntrain_data.head()","c31083ee":"# Drop the existing columns\ntrain_data=train_data.drop(['Pclass', 'Sex','Embarked'],axis=1)\ntrain_data.head()","b187448e":"# One-Hot Encoding on test data\ndummies = pd.get_dummies(test_data[['Pclass', 'Sex','Embarked']], drop_first=True)\n\n# Adding the results to the master dataframe\ntest_data = pd.concat([test_data, dummies], axis=1)\ntest_data.head()","bec65267":"# Drop the existing columns\ntest_data=test_data.drop(['Pclass', 'Sex','Embarked'],axis=1)\ntest_data.head()","dca53dc5":"train_data.info()","a839615b":"train_data['Survived']=train_data['Survived'].astype('uint8')","5116d54b":"# Spliting a dataset\nX_train=train_data.drop(['Survived','PassengerId'], axis=1)\ny_train=train_data[\"Survived\"]","713586b0":"print(\"X_train shape\", X_train.shape)\nprint(\"y_train shape\", y_train.shape)","0900527d":"# drop uneccesary column from test data\ntest=test_data.drop(['PassengerId'], axis=1).copy()","fdb777d8":"print(\"test_data shape\", test_data.shape)","81448319":"# Logistic regression model\nfrom sklearn.linear_model import LogisticRegression\nL = LogisticRegression()","767f0df7":"# Fitting the model on our trained dataset.\nL.fit(X_train,y_train)","2538b73c":"# Making Predictions\ny_pred = L.predict(test)","e5904378":"# Calculating the accuracy of the model\nprint(\"Accuracy:\",round(L.score(X_train, y_train)*100,2))","daba1605":"# printing the features coefficient\nL.coef_","860e27c2":"# List of features\nfeatures = ['Age', 'Fare','Family','Pclass_2','Pclass_3','Sex_male','Embarked_Q','Embarked_S']","afbf7a80":"# Listing the features in according to importance\ncoeff = pd.DataFrame(X_train.columns)\ncoeff.columns = ['Features']\ncoeff[\"Correlation\"] = pd.Series(L.coef_[0])\n\ncoeff.sort_values(by='Correlation', ascending=False)","d6ddf96d":"plt.figure(figsize=(8,5))\nsns.barplot('Correlation','Features',data=coeff,palette='magma')","2a08bea0":"# Importing decision tree classifier from sklearn library\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Fitting the decision tree with default hyperparameters, apart from\n# max_depth which is 5 so that we can plot and read the tree.\ndt_default = DecisionTreeClassifier(max_depth=5)\ndt_default.fit(X_train, y_train)","f4fd07c6":"# Making Predictions\ny_pred = dt_default.predict(test)","72cfc5dc":"# Calculating the accuracy\nprint(\"Accuracy:\",round(dt_default.score(X_train, y_train)*100,2))","031c9e59":"important_feature = pd.Series(dt_default.feature_importances_,index=features).sort_values(ascending=False)\nimportant_feature","e7b18e85":"sns.barplot(important_feature.values,important_feature.index,palette='bone')","4ae0aada":"from sklearn.ensemble import RandomForestClassifier\nRF= RandomForestClassifier(n_estimators=100,random_state=22)","b779fc91":"# Fitting the model on our trained dataset.\nRF.fit(X_train,y_train)","a2c42bf3":"# Making Predictions\ny_pred = RF.predict(test)","8b898f89":"# Calculating the accuracy\nprint(\"Accuracy:\",round(RF.score(X_train, y_train)*100,2))","086edf4c":"feature_imp = pd.Series(RF.feature_importances_,index=features).sort_values(ascending=False)\nfeature_imp","752fc4dc":"sns.barplot(feature_imp.values,feature_imp.index,palette='bone')","3c052463":"submission = pd.DataFrame({\n        \"PassengerId\": test_data[\"PassengerId\"],\n        \"Survived\": y_pred\n    })\nsubmission.to_csv('titanic_survival.csv', index=False)","e136749b":"Insight :\nPassengers who are young have a high survival rate.","820b80aa":"# 6. Cabin ","c798ec7e":"# Build a model","50bbe9fe":"* Logistic Regression is statistical model which is used to model the probability of the certain class or event such as Pass\/Fail, Win\/loss and Survived\/Non-Survived etc.","a7248e3a":"* We need to convert target variable data type(object) as dummies variables(uint8) else it will pop up \"Unknown Label\" error message while creating a model","3cf39651":"3. Above correlation shows that Survived column is highly coorelated with Fare.","fb061d12":"# * Decision Tree Classifier","84562b16":"* # Few columns are misclassify datatypes as they are categorical but seem to be numerical","b6ac412f":"2. Missing value in Cabin column shows that Cabin has not been allocated to few passengers.Therefore, they might be using the deck. Therefore impute with some random variable as 'Q'","c46dbba7":"#  One-Hot Encoding","b4589e1f":"Insight :\n1. Passengers with 2,3,4 family members have more survial chance as campared to family memebers count as 1,5,6,7,8,11.\n2. Passenger with family members count as 8,11 have no survival rate as they become more panic and start collecting their family members before they exit.\n3. Single Passengers also have low survival rate.","74f5f0ca":"# * Random forest Classifiers","ffaed24f":"# Data Preparation","70a66bbc":"1. Numerical features : Age,Parch,SibSp,Fare,Ticket\n2. Categorical features : Survived,Pclass,Sex,Cabin,Embarked","c6497448":"*  # *** Conclusion :**\n*  Logistic Regression :-\n*  a) Accuracy - 80.81\n*  b) Top 3 features - Fare, Embarked_Q,Family\n* \n*  Decision tree classifier :-\n*  a) Accuracy - 84.06\n*  b) Top 3 features - Sex_male,Pclass3,Family\n*  \n*  Random Forest Classifier :-\n*  a) Accuracy - 97.98 \n*  b) Top 3 features - Fare, Age, Sex_Male","c645d324":"* Features : Age,Fare,Family,Pclass_2,Pclass_3,Sex_male,Embarked_Q,Embarked_S","c0045548":"# Reading, Understanding and Visualising the data","1f495484":"# 5. Age","6e08a860":"# Scaling","bfb6bdcc":"# * * New Feature","9a4f38f1":"# 4. Fare","30c1de53":"* Decision tree Classifiers create a classification model by building a decision tree. Each node in a tree specifies a test on attributes, each branch descending from that node corresponds to one of the possible values for that attributes.","557bba1b":"Insight :\n1. Total of 206 passengers survived from cabin 'Q', they don't had cabin they were available on the deck.They were of class 3 passengers.\n2. Total of 481 passengers also died from Cabin 'Q'.","e760505d":"Insight :\n1. Passenger coming from Cherbourg port has more survival rate.","8fb8c39c":"Insight:\n1. Survival rate of Female is more as compared to male.as they evacuated first","9f5354df":"1. Age column has outliers therefore impute values via median","b6ebe637":"# Import Libraries","69b2c3d0":"Insight :\n1. Passenger for P-class 1 survived more as compared to other class.\n2. Passenger for P-class 3 are more in number who couldn't survived.","22d3d585":"* Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes or mean prediction of the individual trees.","66d6fd56":"# 2. Sex","2cf8a9ff":"# **1. Pclass**","c42be91f":"# EDA","5d6363a0":"Insight :\n1. Passengers taking premium ticket provide them safer seat.","c15e6963":"# ****Problem Statement****\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\n> > In this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?using passenger data (ie name, age, gender, socio-economic class, etc).","80e2906b":"# Analyse the target variable with other response variables.","bdb37369":"# Logistic Regression","dbfe8968":"* Cabin column has many unique values. Thus it will not helps the model to capture the generic behaviour. Therfore Cabin column should be drop from the dataframe","e6ba583f":"# 3. Embarked"}}