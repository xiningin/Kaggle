{"cell_type":{"57f3e110":"code","a4089b41":"code","0f12b841":"code","ef2f2a54":"code","08ef9607":"code","af86afef":"code","46348f84":"code","0c930fca":"code","4ea9d149":"code","d62bcc27":"code","a5c4aa6a":"code","73e98897":"code","4cca88fc":"code","dc572857":"code","f131aa1c":"code","766c3c3e":"code","c627b68e":"code","efeec821":"code","59dfeb2a":"code","5347685c":"code","7ddf4abd":"code","4de268c3":"code","75fd173e":"code","ee2ad7da":"code","b3435d3b":"markdown","26f08a06":"markdown","e6c87bbf":"markdown","9a03fc8f":"markdown","8275a27c":"markdown"},"source":{"57f3e110":"import pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")","a4089b41":"# reading train and test dataset\ntrain = pd.read_csv('..\/input\/college-to-corporate-data\/final_train.csv', sep=';')\ntest = pd.read_csv('..\/input\/college-to-corporate-data\/final_test.csv', sep=';')\ndel train['id']\ndel test['id']\ntrain.shape,test.shape","0f12b841":"#basic analysis of dataset\ntrain.isnull().sum()","ef2f2a54":"train.info()","08ef9607":"# function maps categorical in numeric form.\n# degree_type had 3 unique values, other categorical columns had two unique vales\/ Therefore I decided to not use one-hot encoding.\ndef cat_convertor(df):\n    cat_mapping_dict = {\n        'gender':{'Female':0,'Male':1},\n        'ssc_board':{'Central Board':1,'State Board':0},\n        'hsc_board':{'Central Board':1,'State Board':0},\n        'hsc_stream':{'Commerce':0,'Science':1, 'Arts':2},\n        'degree_type':{'Commerce & Mgmt':0,'Other Degree':1,'Science & Tech':2},\n        'work_exp':{'No':0, 'Yes':1},\n        'stream':{'Finance':0, 'HR':1}\n    }\n    df['gender'] = df['gender'].map(cat_mapping_dict['gender'])\n    df['ssc_board'] = df['ssc_board'].map(cat_mapping_dict['ssc_board'])\n    df['hsc_board'] = df['hsc_board'].map(cat_mapping_dict['hsc_board'])\n    df['hsc_stream'] = df['hsc_stream'].map(cat_mapping_dict['hsc_stream'])\n    df['degree_type'] = df['degree_type'].map(cat_mapping_dict['degree_type'])\n    df['work_exp'] = df['work_exp'].map(cat_mapping_dict['work_exp'])\n    df['stream'] = df['stream'].map(cat_mapping_dict['stream'])\n    return df\n\ntrain = cat_convertor(train)\ntest = cat_convertor(test)","af86afef":"# Scope of improvement - my additional features didnot add much signal to the final score.\n\n# def feature_engineering(df):\n#     df['school_pct'] = df['ssc_pct']+df['hsc_pct']\n#     df['school_pct_avg'] = df['school_pct']\/2.0\n    \n#     df['college_pct'] = df['degree_pct']+df['mba_pct']\n#     df['mba_pct_avg'] = df['school_pct']\/2.0\n    \n#     df['all_pct'] = df['ssc_pct']+df['hsc_pct'] + df['degree_pct']+df['mba_pct']\n#     df['all_pct_avg'] = df['all_pct']\/4.0\n\n#     temp = df.groupby('gender')['mba_pct'].median().reset_index(name='mba_pct_by_gender')\n#     df = df.merge(temp, how='left', on='gender')\n#     df['mba_pct_by_gender'] = df['mba_pct']\/df['mba_pct_by_gender']\n    \n#     temp = df.groupby('degree_type')['degree_pct'].median().reset_index(name='degree_pct_by_degree_type')\n#     df = df.merge(temp, how='left', on='degree_type')\n#     df['degree_pct_by_degree_type'] = df['degree_pct']\/df['degree_pct_by_degree_type']\n    \n#     return df","46348f84":"# functions tries to remove typos from percent columns\n# no body who has scored less than 33% get a valid certificate.\n# no one can score more than 100#\n# It's a common mistake of writing [ab.cd] to [a.bcd] or [abc.d], so I tried to fix this issue here. \n#converts 4.89 -> 48.9\n#converts 987.23 -> 98.723\n#converts 1003.58 -> 3.0 (scope of improvement)\n\n\ndef removing_outliers(series):\n    IQR= series.quantile(.75) - series.quantile(.25)\n    lower_boundary = series.quantile(.25) - (IQR*1.5) #36\n    upper_boundary = series.quantile(.75) + (IQR*1.5) #100\n    series = series.apply(lambda x: x*10 if x<10 else x)\n    series = series.apply(lambda x: x*.1 if (x>100 and x<1000) else x)\n    series = series.apply(lambda x: float(str(int(x))[2:]) if x>1000 else x)\n    series = series.apply(lambda x: np.nan if x<lower_boundary else x)\n    series = series.apply(lambda x: np.nan if x>upper_boundary else x)\n    return series\n\ntrain['ssc_pct'] = removing_outliers(train['ssc_pct'])\ntrain['hsc_pct'] = removing_outliers(train['hsc_pct'])\ntrain['degree_pct'] = removing_outliers(train['degree_pct'])\ntrain['emptest_pct'] = removing_outliers(train['emptest_pct'])\ntrain['mba_pct'] = removing_outliers(train['mba_pct'])\n\ntest['ssc_pct'] = removing_outliers(test['ssc_pct'])\ntest['hsc_pct'] = removing_outliers(test['hsc_pct'])\ntest['degree_pct'] = removing_outliers(test['degree_pct'])\ntest['emptest_pct'] = removing_outliers(test['emptest_pct'])\ntest['mba_pct'] = removing_outliers(test['mba_pct'])","0c930fca":"train.head()","4ea9d149":"X = train.drop('status', axis=1)\ny = train['status']\nX.columns","d62bcc27":"#using iterative imputer to fill up the missing values.\n# using extratreesclassifier (it is fast compared to other trees methods) instead of gbdt(by default) gave me improvement.\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.ensemble import ExtraTreesRegressor\nimp_mean = IterativeImputer(estimator=ExtraTreesRegressor(n_estimators=10, random_state=0), random_state=0)\nX_imputed = imp_mean.fit_transform(X)\ntest = imp_mean.fit_transform(test)","a5c4aa6a":"#converting the categorical values to the nearest interger values\nX = pd.DataFrame(X_imputed, columns=X.columns)\nX['gender'] = X['gender'].apply(lambda x: int(round(x)))\nX['ssc_board'] = X['ssc_board'].apply(lambda x: int(round(x)))\nX['hsc_board'] = X['hsc_board'].apply(lambda x: int(round(x)))\nX['hsc_stream'] = X['hsc_stream'].apply(lambda x: int(round(x)))\nX['degree_type'] = X['degree_type'].apply(lambda x: int(round(x)))\nX['work_exp'] = X['work_exp'].apply(lambda x: int(round(x)))\nX['stream'] = X['stream'].apply(lambda x: int(round(x)))\nX.head()","73e98897":"test = pd.DataFrame(test, columns=X.columns)\ntest['gender'] = test['gender'].apply(lambda x: int(round(x)))\ntest['ssc_board'] = test['ssc_board'].apply(lambda x: int(round(x)))\ntest['hsc_board'] = test['hsc_board'].apply(lambda x: int(round(x)))\ntest['hsc_stream'] = test['hsc_stream'].apply(lambda x: int(round(x)))\ntest['degree_type'] = test['degree_type'].apply(lambda x: int(round(x)))\ntest['work_exp'] = test['work_exp'].apply(lambda x: int(round(x)))\ntest['stream'] = test['stream'].apply(lambda x: int(round(x)))\ntest.head()","4cca88fc":"#pca gave me a minimal improvement.\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaledX = scaler.fit_transform(X)\nscaledTest = scaler.transform(test)\n\npca = PCA(n_components=0.90,random_state=0)\npca.fit(scaledX)\n\npca_X = pca.transform(scaledX)\npca_Test = pca.transform(scaledTest)\nprint('pca_X.shape ',pca_X.shape)\nprint('pca_Test.shape', pca_Test.shape)\n\nprint('X.shape ',X.shape)\nprint('Test.shape', test.shape)\n\npca_X = pd.DataFrame(pca_X)\npca_Test = pd.DataFrame(pca_Test)\n\npca_X.columns = ['pca_'+str(col) for col in pca_X]\npca_Test.columns = ['pca_'+str(col) for col in pca_Test]\n\nX = pd.concat([X,pca_X],axis=1)\ntest = pd.concat([test,pca_Test],axis=1)\nprint('combined X shape',X.shape)\nprint('combined test shape',test.shape)","dc572857":"X.head()","f131aa1c":"# didnot show any imrpovement\n\n# X['ssc_pct'] = np.log1p(X['ssc_pct'])\n# X['hsc_pct'] = np.log1p(X['hsc_pct'])\n# X['degree_pct'] = np.log1p(X['degree_pct'])\n# X['emptest_pct'] = np.log1p(X['emptest_pct'])\n# X['mba_pct'] = np.log1p(X['mba_pct'])\n\n# test['ssc_pct'] = np.log1p(test['ssc_pct'])\n# test['hsc_pct'] = np.log1p(test['hsc_pct'])\n# test['degree_pct'] = np.log1p(test['degree_pct'])\n# test['emptest_pct'] = np.log1p(test['emptest_pct'])\n# test['mba_pct'] = np.log1p(test['mba_pct'])","766c3c3e":"\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nX_train,X_test,y_train,y_test=train_test_split(X,y,random_state=2020,test_size=0.20,shuffle=True,stratify=y)","c627b68e":"# xgb cross val\n\nimport xgboost as xgb\nmodel_xgb = xgb.XGBClassifier(colsample_bylevel= 1, learning_rate= 0.1,max_depth=10, n_estimators= 1000, verbosity = 0)\nresult = cross_val_score(estimator=model_xgb,X=X_train,y=y_train,cv=10)\nprint(result)\nprint(result.mean())\n\n# NOTE:\n# The output of XGB before adding pca features were - \n# xgb\n# 0.8912709022526982\n# 0.8956617202207225\n# 0.8901668383506782\n# 0.8819781852691254\n# 0.8926499994374159\n# Average:  0.8903455291061281","efeec821":"# lgbm cross val\nimport lightgbm as lgb\nmodel_lgb = lgb.LGBMClassifier(learning_rate=0.2, n_estimators= 1000, verbosity = 0)\nresult = cross_val_score(estimator=model_lgb,X=X_train,y=y_train,cv=10)\nprint(result)\nprint(result.mean())","59dfeb2a":"# catboost cross val\nfrom catboost import CatBoostClassifier\nmodelcat = CatBoostClassifier(random_state=42, max_depth=6, n_estimators=1000, verbose=0,learning_rate=.1)\nresult = cross_val_score(estimator=modelcat,X=X_train,y=y_train,cv=10)\nprint(result)\nprint(result.mean())","5347685c":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score","7ddf4abd":"# SMOTE NOT WORKING\n# from imblearn.over_sampling import SMOTE\n# from collections import Counter\n\nfrom catboost import CatBoostClassifier\n\nfinal_preds = []\nsfold, scores = StratifiedKFold(n_splits=5, shuffle=True, random_state=9), list()\nfor train_idx, test_idx in sfold.split(X, y):\n    x_train, x_test = X.iloc[train_idx], X.iloc[test_idx]\n    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n    # num_class0, num_class1 = Counter(y_train)[0], Counter(y_train)[1]\n    # sm = SMOTE(random_state=27, sampling_strategy={0: int(1.23*num_class0), 1: int(3.27*num_class1)})\n    # x_train, y_train = sm.fit_resample(x_train, y_train)\n    modelcat = CatBoostClassifier(random_state=0, max_depth=10, n_estimators=1000, verbose=500,learning_rate=.01)\n    modelcat.fit(x_train, y_train)\n    preds = modelcat.predict(x_test)\n    score = f1_score(y_test, preds, average=\"weighted\")\n    scores.append(score)\n    test_pred = modelcat.predict(test)\n    final_preds.append(test_pred)\n#         print(score)\n    print(\"Average:----> \", sum(scores)\/len(scores))","4de268c3":"import matplotlib.pyplot as plt\n%matplotlib inline\n\ndef feature_importance(model, X_train):\n    fI = model.feature_importances_\n    print(fI)\n    names = X_train.columns.values\n    ticks = [i for i in range(len(names))]\n    plt.bar(ticks, fI)\n    plt.xticks(ticks, names,rotation = 90)\n    plt.show()\n\nfeature_importance(modelcat, X)","75fd173e":"# output is based on majority voting, output is what 3 or more models of catboost decide.\nfinal_preds_df = pd.DataFrame(final_preds)\nfinal_preds_ = final_preds_df.T\nfinal_preds_['voting_output'] = final_preds_.sum(axis=1)\nfinal_preds_['combined_output'] = final_preds_['voting_output'].apply(lambda x: 1 if x>2 else 0)\nfinal_preds_.head()","ee2ad7da":"test_submission = pd.read_csv('..\/input\/college-to-corporate-data\/final_test.csv', sep=';')\ntest_submission['status'] = final_preds_['combined_output']\ntest_submission = test_submission[['id','status']]\ntest_submission.head(10)","b3435d3b":"#### Seeing the above corss-val score, I decided to do feature engineering and hyperparameter tuning with only catboost in focus","26f08a06":"### If you have read my notebook till here then please **UPVOTE** and **add comments**.\n### If you have any doubts\/suggestions then please write them below in the comments section. I'll try to answer them.","e6c87bbf":"## Approach\n* The problem statement was a binary classification problem, where 'status' was the binary output, with values 1 or 0. Status denoted whether a student is placed or not.\n* basic analysis and eda were done to find out the datatypes, missing values, outliers, and the distribution of the data.\n* 'cat_convertor' function maps the categorical column values into numeric form.\n* 'removing_outliers' function removes outliers is pct columns by fixing typos (4.89 -> 48.9), (778.98 -> 77.898). It also uses InterQuartile Range (IQR) to make the remaining outliers as NaN.\n* For filling up the missing values, I used IterativeImputer from Sklearn with extratreesclassifier as an estimator, extratreesclassifier showed improvement above gbdt which is the default estimator.\n* For categorical columns with integer values, After imputing the data I run the round(int()) command to again convert the float value to the nearest integer value.\n* Adding PCA columns with 95% variance distribution bumped the numbers. MinMaxScaler was used on the dataset before PCA.\n* I applied a cross_val_score for xgb, lgbm, catboost base models. catboost gave me the best results. Hence I decided to move with catboost + hyperparameter tuning.\n* Things that didn't work out for me - a)smoothing of pct columns. b)SMOTE c) I tried to combined features like percentage of candidates divided by the average percentage of candidates grouped by gender\/stream\/degree_type ad total sum of percentage, the difference between percentage, etc","9a03fc8f":"### This Code secured **first** position in the conducted during **Machine Learning Developer Summit(MLDS 2021)** held between 11-13th feb.\n \nThe hackathon was conducted by - [LEAPS](https:\/\/leapsapp.analyttica.com\/home)\n  \nMLDS summit link - [MLDS link](https:\/\/virtualmlds.analyticsindiasummit.com\/community\/#\/home)\n  \nhackathon link - [hackathon link](https:\/\/leapsapp.analyttica.com\/hackathons\/MLDS) ","8275a27c":"## EDA\n\n* Candidates who have studied hsc and ssc in central board have a higher chance of getting placed as compared to students who have studied hsc and ssc in stateboard.\n* The proportion of train-test split done in the provided datasets - 70:30.\n* In the training dataset, 27.3% of students were placed.\n* Out of the placed population, 3.1% was female42.9% of candidates had taken science as hsc_stream.\n* 37.4% of males are being placed in the total population.\n* 'Commerce and mgmt' is the most popular degree type chosen by students. \n* 'work_exp' has the highest numbers of missing records."}}