{"cell_type":{"31b090fe":"code","24832a4a":"code","18ad6f56":"code","6cd604a5":"code","5bf108e2":"code","fded97d0":"code","c98aeed4":"code","4d6b92e5":"code","a0fac404":"code","ff8d7624":"code","0c6ad83b":"code","ca5c0f72":"code","d55d6308":"code","9a839832":"markdown","82894492":"markdown","d007fa80":"markdown","d586b6c2":"markdown","5cb14520":"markdown","5d07088b":"markdown","e566f193":"markdown","78cefe91":"markdown"},"source":{"31b090fe":"import numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\ntrain_df = pd.read_csv(\"..\/input\/quora-insincere-questions-classification\/train.csv\")\n# \u53ea\u4fdd\u755950\u4e07\u7684\u6570\u636e\u4f5c\u4e3a\u8bad\u7ec3\u96c6\ndf_train = train_df.loc[:500000, :]\ndf_train, df_valid = train_test_split(df_train, test_size=0.1)\n","24832a4a":"print(df_train.target.values[:10])\nprint(df_train.head(n=2))","18ad6f56":"# print(df_train.head(n=10))\nprint(df_train.columns)\n\n# create a Vocabulary using the question_text\n\nfrom keras.preprocessing.text import Tokenizer\ndef get_vocab(df, num_words=20000):\n    \"\"\"\n    get the dictionary using the df\n    \"\"\"\n    tokenizer = Tokenizer(num_words=num_words)\n    texts = df.question_text.tolist()\n    tokenizer.fit_on_texts([item.lower() for item in texts])\n    return tokenizer\n# \u4f7f\u7528\u8bad\u7ec3\u6570\u636e\u96c6\u6784\u5efa\u4e00\u4e2a\u5173\u4e8e\u8bad\u7ec3\u96c6\u7684tokenizer\ntokenizer = get_vocab(df_train)","6cd604a5":"print(tokenizer.texts_to_sequences([\"I Love you\"]))\nprint(tokenizer.texts_to_sequences([\"To be a better man.\"]))","5bf108e2":"%%time\nfrom keras.preprocessing.sequence import pad_sequences\n\nMAX_LENGTH = 40\ntrain_X = np.array(pad_sequences(tokenizer.texts_to_sequences(df_train.question_text.tolist()), maxlen=MAX_LENGTH, padding = 'post'))\nvalid_X = np.array(pad_sequences(tokenizer.texts_to_sequences(df_valid.question_text.tolist()), maxlen=MAX_LENGTH, padding = 'post'))\nprint(train_X.shape)\n\ntrain_y, valid_y = np.array(df_train.target.values), np.array(df_valid.target.values)","fded97d0":"print(df_train.question_text[0], train_X[0])","c98aeed4":"# \u8fd9\u6b65\u662f\u628atxt\u6587\u6863\u8f6c\u6210\u5411\u91cf \u641c\u7684\u522b\u4eba\u7684\u4ee3\u7801\n# using embedding here to get the numpy array for later useage\nembeddings_index = {}\nfile = open('..\/input\/quora-insincere-questions-classification\/embeddings\/glove.840B.300d\/glove.840B.300d.txt')\nfor line in tqdm(file):\n    values = line.split(\" \")\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nfile.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","4d6b92e5":"print(tokenizer.num_words)","a0fac404":"vocab_size = len(tokenizer.word_index.items())\n# create a weight matrix for words in training docs\nembedding_matrix = np.random.normal(loc=0, scale=1.0, size=(vocab_size+1, 300))\nfor word, i in tokenizer.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","ff8d7624":"from keras.models import Sequential\nfrom keras.layers import RNN, LSTM, Dropout, Flatten, Embedding, SpatialDropout1D, Dense","0c6ad83b":"# define model\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size+1, 300, input_length=MAX_LENGTH, weights=[embedding_matrix]))\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","ca5c0f72":"model.summary()","d55d6308":"model.fit(train_X, train_y, epochs=2, verbose=1, batch_size=256)","9a839832":"**\u6784\u5efa\u4e00\u4e2amatrix\uff0c\u8fd9\u4e2amatrix\u548c\u4e4b\u524d\u5efa\u7acb\u7684\u8bcd\u5178 tokenizer.word_index \u8981\u4e00\u4e00\u5bf9\u5e94\u5176\u8d77\u6765\uff0c\u4e4b\u540e\u4f7f\u7528\u8fd9\u4e2amatrix\u6765\u4f5c\u4e3aembedding\u7684\u521d\u59cb\u5316**","82894492":"\u4f7f\u7528\u5e8f\u5217\u5316\u65b9\u6cd5sequence\u6765\u5b9e\u73b0\u5bf9\u53e5\u5b50\u7684padding","d007fa80":"## \u76f4\u63a5\u8fdb\u884c\u8bad\u7ec3","d586b6c2":"# \u6570\u636e\u5206\u6790\u90e8\u5206\n\n1. \u9996\u5148\u6784\u5efa\u4e00\u4e2a\u8bcd\u5178\uff0c\u8fd9\u4e2a\u8bcd\u5178\u4f1a\u628a\u5355\u8bcd\u5bf9\u5e94\u6210\u7d22\u5f15\n2. \u901a\u8fc7\u4f7f\u7528keras\u5185\u7f6e\u7684\u5206\u6790\u5de5\u5177\u53d6\u51fa\u8bad\u7ec3\u96c6\u4e2d\u6700\u4e3a\u5e38\u7528\u768420000\u4e2a\u5355\u8bcd\u4f5c\u4e3a\u8bcd\u5178\n","5cb14520":"## \u4f7f\u7528keras\u6765\u6784\u5efa\u4e00\u4e2a\u6a21\u578b","5d07088b":"* \u9996\u5148\u5efa\u7acb\u4e00\u4e2a\u5173\u4e8e\u6240\u6709\u5355\u8bcd\u7684embedding\u77e9\u9635\uff0c\uff0c\u628a\u8fd9\u4e2a\u77e9\u9635\u4fdd\u5b58\u8d77\u6765\uff0c\u4e4b\u540e\u4f7f\u7528\u8fd9\u4e2a\u77e9\u9635\u6765\u521d\u59cb\u5316\u6a21\u578b\u540e\u9762\u7684embedding-layer\u5c31\u884c\u4e86\u3002","e566f193":"# \u8bad\u7ec3\u4e24\u8f6e\uff0c\u6bcf\u4e00\u6b21batch_size \u662f128","78cefe91":"# \u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u8bcd\u5411\u91cf\uff0c\u5e76\u4e14\u6784\u5efa\u548c\u4e4b\u524d\u8bcd\u5178\u4e2d\u7684\u4e00\u4e2a\u6620\u5c04\u5173\u7cfb\u51fa\u6765\u3002"}}