{"cell_type":{"ce6098b3":"code","5ddeef67":"code","56bd1465":"code","51c409b8":"code","c441cd71":"code","2713cdf6":"code","ca58bb6b":"code","a6eafefa":"code","4945ffae":"code","1dc2bc3a":"markdown","b8ab53df":"markdown","f363a2dc":"markdown","d4247244":"markdown","3cbfbd8c":"markdown","daf88466":"markdown","90726e7d":"markdown","6d247e80":"markdown","0e42613b":"markdown"},"source":{"ce6098b3":"# importing necessary libraries\nimport cv2\nimport time\nimport numpy as np\nimport tensorflow.keras.backend as K\nfrom sklearn.metrics import f1_score\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, Input\nfrom tensorflow.keras import optimizers\nimport tensorflow as tf","5ddeef67":"# Defining the model architecture.\ndef create_model():\n    K.clear_session()\n    ip = Input(shape = (150,150,1))\n    z = Conv2D(filters = 32, kernel_size = (64,64), padding='same', input_shape = (150,150,1), activation='relu')(ip)\n    z = Conv2D(filters = 64, kernel_size = (16,16), padding='same', input_shape = (150,150,1), activation='relu')(z)\n    z = Conv2D(filters = 128, kernel_size = (4,4), padding='same', input_shape = (150,150,1), activation='relu')(z)\n    z = MaxPool2D(pool_size = (4,4))(z)\n    z = Flatten()(z)\n    z = Dense(32, activation='relu')(z)\n    op = Dense(6, activation='softmax')(z)\n    model = Model(inputs=ip, outputs=op)\n    return model\n\n# Loading the pretrained weights.\nmodel = create_model()\nmodel.load_weights('model_weights.h5')","56bd1465":"# Detecting the hand using YOLOv3 or HAAR cascade.\ndef hand_detect(img, type='haar'):\n\n    cv2.imshow('output', img)\n    key = cv2.waitKey(20)\n    if key == 27:  # exit on ESC\n        cv2.destroyAllWindows()\n\n    hand_img = img.copy()\n    if type=='yolo':\n        width, height, inference_time, results = yolo.inference(img) # getting the hand inferences using YOLOv3.\n    elif type=='haar':\n        results = cascade.detectMultiScale(hand_img, scaleFactor=1.3, minNeighbors=7) # getting the hand inferences using HAAR cascade.\n\n    if len(results) > 0:\n        if type=='yolo':\n            _,_,_, x, y, w, h = results[0] # returning the hand location.\n            return x,y,150,150\n        elif type=='haar':\n            x, y, w, h = results[0] # returning the hand location.\n            return x-50,y-70,150,150\n    else:\n        return []","51c409b8":"# Bringing all together.\ndef main():\n    roi = []\n    # Initial loop for detecting the fist\/palm and defining a ROI.\n    while(len(roi) == 0):\n        ret, frame = cap.read()\n        frame = cv2.flip(frame, 1)\n        roi = hand_detect(frame)\n\n    cv2.destroyAllWindows()\n    ret = tracker.init(frame, roi) # initializing the tracker using ROI.\n    cv2.destroyAllWindows()\n\n    c = 0\n    d = c+1\n\n    while True:\n        ret, frame = cap.read() # getting the input frames\n        frame = cv2.flip(frame, 1)\n        success, roi = tracker.update(frame) # tracking the ROI in the new input frame\n        (x,y,w,h) = tuple(map(int, roi)) # defining the ROI box's coordinates.\n        if success:\n            pt1 = (x,y)\n            pt2 = (x+w, y+h)\n            square  = frame[y:y+h, x:x+w] # extracting the image(hand gesture) inside ROI.\n            gray = cv2.cvtColor(square, cv2.COLOR_BGR2GRAY) # converting the image to grayscale for contour detection.\n            gray_ = cv2.medianBlur(gray, 7) # applying blur to reduce the salt and pepper noise.\n            hand = contours(gray, th=150)[0] # converting the grayscale image to binary.\n            im = np.array([hand]) # adding a last dimension to the image array to make it model input compatible.\n            im = im.reshape(-1,150,150,1) # reshaping the image to be perfectly model compatible.\n            result = pred(im) # getting the class label for the image from pretrained model.\n            \n            cv2.rectangle(frame, pt1, pt2, (255,255,0), 3) # Drawing a rectangle around the detected ROI.\n            emo = icon(result) # Getting the emoji corresponding to the predicted class label.\n\n            frame_copy = paste(frame, emo) # Pasting the emoji image over the ROI in input frame\n            # Displaying the extracted ROI (what the model sees for prediction) on the top left corner.\n            (a,b) = hand.shape\n            for i in range(3):\n                hand = hand*255\n                frame_copy[0:a, 0:b, i] = hand\n\n        # retry the above steps if the tracker fails to detect a hand.\n        else:\n            cv2.putText(frame, 'Failed to detect object', (100,200), cv2.FONT_HERSHEY_SIMPLEX, 2, 255)\n            cv2.imshow('output', frame_copy)\n            roi = []\n\n            while(len(roi) == 0):\n                ret, frame = cap.read()\n                frame = cv2.flip(frame, 1)\n                roi = hand_detect(frame)\n            continue\n        cv2.imshow('output', frame_copy)\n        if cv2.waitKey(1) & 0xFF == 27:\n            break\n\n    cap.release()\n    cv2.destroyAllWindows()","c441cd71":"# This function defines the contour of the hand gesture and converts the image to binary.\ndef contours(diff, th=100):\n    _ , thresholded = cv2.threshold(diff, th, 255, cv2.THRESH_BINARY_INV)\n    contours, hierarchy = cv2.findContours(thresholded.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    if len(contours) == 0:\n        return None\n    else:\n        hand_segment = max(contours, key=cv2.contourArea)\n    return (thresholded, hand_segment)","2713cdf6":"# Predicting the class label of the hand gesture in ROI.\ndef pred(img):\n    return model.predict(img).argmax(axis=1)[0]","ca58bb6b":"# This function returns the file path to the emoji picture.\ndef icon(x, e=emoji):\n    return e[x - 1]","a6eafefa":"# Pasting the emoji image over the ROI.\ndef paste(frame, s_img, pt1, pt2):\n    x, y = pt1\n    x_w, y_h = pt2\n    n = min(x_w-x, y_h-y)\n    s_img = cv2.resize(s_img, dsize=(n,n))\n    l_img = frame\n    (x_offset,y_offset,_) = (frame.shape)\n    (y_offset,x_offset) = (x_offset\/\/2 - 72, y_offset\/\/2 - 72)\n    y1, y2 = y_offset, y_offset + s_img.shape[0]\n    x1, x2 = x_offset, x_offset + s_img.shape[1]\n\n    alpha_s = s_img[:, :, 3] \/ 255.0\n    alpha_l = 1.0 - alpha_s\n\n    for c in range(0, 3):\n        l_img[y:y_h, x:x_w, c] = (alpha_s * s_img[:, :, c] + alpha_l * l_img[y:y_h, x:x_w, c])\n        \n    return l_img","4945ffae":"# Loading the 5 emoji images into a list\nemoji = []\nfor i in range(1,6):\n    img = cv2.imread(f'.\/emoji\/{i}.png', -1)\n    emoji.append(img)\n\n# Defining the hand detection methods haar cascads and YOLO.\ncascade = cv2.CascadeClassifier('haarcascade\/fist.xml')\nyolo = YOLO(\".\/models\/cross-hands.cfg\", \".\/models\/cross-hands.weights\", [\"hand\"])\n# Defining multiple tracking objects for tracking the hand.\ntk = [cv2.TrackerBoosting_create(), cv2.TrackerMIL_create(), cv2.TrackerKCF_create(), cv2.TrackerTLD_create(), cv2.TrackerCSRT_create()]\ntracker = tk[2]\ncap = cv2.VideoCapture(0)\nmain()","1dc2bc3a":"### A few days back, I saw this cool app on the internet that would look at your hand using the webcam and based on the palm gesture, it tries to come up with a similar emoji.\n### The idea was simple yet amusing to watch. Next thing I thought was why not code something similar since it would be a decent hands-on project for anyone beginning with deeplearning.","b8ab53df":"# Hand gesture classification using CNN (Part-2).\n### Find Part-1 [here](https:\/\/www.kaggle.com\/sarthakvajpayee\/hand-gesture-to-emoji-part-1)","f363a2dc":"### In the dataset we have around 3000 images for each of the 5 class labels.\n### Let's take a look at the emoji images that correspond to the 5 class lables:\n<img src=\"https:\/\/i.ibb.co\/B46MbgQ\/Screen-Shot-2020-08-15-at-3-38-40-AM.png\" alt=\"Screen-Shot-2020-08-15-at-3-38-40-AM\" border=\"0\">","d4247244":"## Project overview: I've divided the project into 2 parts:\n### 1. Part 1: In this part we'll look at how to design and train a CNN that can predict the input image of the hand gesture and predict the class label.\n### 2. Part 2: In this part we'll take the input from user and predict the hand gesture in realtime. The steps are:\n- Capturing image stream (video) from the user's webcam.\n- Detecting the hand\n- Segmenting out the hand gesture image\n- Predicting the class label of the hand gesture\n- Replacing the hand gesture from the original input with the emoji picture that corresponds to the predicted class.\n","3cbfbd8c":"## Let's begin with the part 2 of this project. We'll go through 7 steps in this part:\n* Taking the input frame using the user's webcam.\n* Searching for a fist in the input frame using pre-trained YOLOv3.\n* If we find a fist, we'll localize it by defining a box around it. The box will be our region of interest (ROI).\n* We'll also add a tracking functionality for the ROI so that it keeps the fist\/palm as the ROI even if the hand moves anywhere inside the frame.\n* Next we'll extract the image inside the ROI from the complete image and feed this segmented image to the pretrained model (from part 1).\n* The next step would be to get the predicted class label from the pretrained model and replace the ROI with the emoji associated with the predicted class.\n* Finally, we'll repeat the above steps in a loop so that the processes keep on repeating on the input image frames coming from the video.\n\n### We'll start by importing libraries and defining some functions.","daf88466":"## About the data:\n### Since this is a 5 class classification problem, we need data for each of the 5 classes to train our model. To make things even simpler and fast, we'll be using binary images for training. Let's take a look at some of the images from each of the 5 classes that we'll be using.\n<img src=\"https:\/\/i.ibb.co\/6vmhbCD\/download-2.png\" alt=\"download-2\" border=\"0\">","90726e7d":"## YOLO vs HAAR cascade\n#### YOLO is much accurate and robust than haar cascade but it takes much more time than haar cascade to process the image.  Whereas Haar cascade being faster, has its own drawback. It sometimes misses to detect the fist or palm im low lighting condition. So, if you are running this code on a micro-controller or a low end PC, Haar cascade would be a good choice.","6d247e80":"### Thanks for reading. Hope you found this project useful! \n### Feedbacks are most welcomed.\n### Find all the files on my github repo using [this link](https:\/\/github.com\/SarthakV7\/AI-Hand-gesture-emoji-detection).","0e42613b":"<img src=\"https:\/\/i.ibb.co\/7nwYnCB\/Screen-Shot-2020-08-14-at-4-35-56-AM.png\" alt=\"Screen-Shot-2020-08-14-at-4-35-56-AM\" border=\"0\" \/>"}}