{"cell_type":{"35aa9754":"code","89eb9133":"code","e9f191b1":"code","a37b2226":"code","09c8bfd1":"code","0a63a860":"code","4caa8c7b":"markdown","690b4f7a":"markdown","e929bf4b":"markdown","045241ba":"markdown","92dac756":"markdown"},"source":{"35aa9754":"#Import\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.python.data import Dataset\nimport keras\nfrom keras.utils import to_categorical\nfrom keras import models\nfrom keras import layers\n\n\ndf = pd.read_csv('..\/input\/covtype.csv')","89eb9133":"#Select predictors\nx = df[df.columns[:54]]\n#Target variable \ny = df.Cover_Type","e9f191b1":"x.head()","a37b2226":"#Split data into train and test \nx_train, x_test, y_train, y_test = train_test_split(x, y , train_size = 0.7, random_state =  90)\n\nmodel = keras.Sequential([\n keras.layers.Dense(64, activation=tf.nn.relu,                  \n input_shape=(x_train.shape[1],)),\n keras.layers.Dense(64, activation=tf.nn.relu),\n keras.layers.Dense(8, activation=  'softmax')\n ])\n\nmodel.compile(optimizer=tf.train.AdamOptimizer(),\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nhistory1 = model.fit(\n x_train, y_train,\n epochs= 26, batch_size = 60,\n validation_data = (x_test, y_test))","09c8bfd1":"from sklearn import preprocessing\n\n\n#Select numerical columns which needs to be normalized\ntrain_norm = x_train[x_train.columns[0:10]]\ntest_norm = x_test[x_test.columns[0:10]]\n\n# Normalize Training Data \nstd_scale = preprocessing.StandardScaler().fit(train_norm)\nx_train_norm = std_scale.transform(train_norm)\n\n#Converting numpy array to dataframe\ntraining_norm_col = pd.DataFrame(x_train_norm, index=train_norm.index, columns=train_norm.columns) \nx_train.update(training_norm_col)\nprint (x_train.head())\n\n# Normalize Testing Data by using mean and SD of training set\nx_test_norm = std_scale.transform(test_norm)\ntesting_norm_col = pd.DataFrame(x_test_norm, index=test_norm.index, columns=test_norm.columns) \nx_test.update(testing_norm_col)\nprint (x_test.head())","0a63a860":"history2 = model.fit(\n x_train, y_train,\n epochs= 26, batch_size = 60,\n validation_data = (x_test, y_test))","4caa8c7b":"<p> As you can see, accuracy is low and is unchanging per epoch, now let's try using normalized data<\/p>","690b4f7a":"<p> Accuracy has improved significantly and the model now learns per epoch, which is better compared to before <\/p>\n <h1> Explanation <\/h1>\n<p>   The model with the unnormalized data didn't learn in 26 epochs because different features do not have similar ranges of values, as a result, gradients oscillate back and forth and take a long time before it can finally find its way to the global\/local minimum. In order to remedy the model learning problem, data normalization was used, which make sures that the different features take on similar ranges of values so that gradient descents can converge more quickly <\/p>","e929bf4b":"<h1> Normalization <\/h1>\n    <p> Normalization is a data preprocessing technique for machine learning with the goal of changing the values of numeric columns in the dataset to a common scale, without distorting differences in the ranges of values. It is not required to do normalization everytime, it is only required when numerical features have a significant difference in terms of ranges <\/p> \n\n<p> For example, take age and income. Age ranges from 1-100 usually and income usually ranges from 0-50,000 and higher. Income is significantly larger than age and when we do further analysis like multivariate linear regression, income influences the result more than the age feature due it its higher value. However, it doesn't necessarily mean that income is more important than age as a predictor. In order to see the effects of normalization, let's create 2 Neural networks, where we do normalization for one and not for the other. <\/p>","045241ba":"<h1> Unnormalized <\/h1> ","92dac756":"<h1> Footnote and reference <\/h1>\nI still am new to the field of machine learning and data science in general, feedback and additional resources would definitely help! The source I used for this kernel is https:\/\/medium.com\/@urvashilluniya\/why-data-normalization-is-necessary-for-machine-learning-models-681b65a05029. I thought it would be nice to try it myself in this kernel as a way of taking notes and might as well publish it to share it with others. Thanks for reading!"}}