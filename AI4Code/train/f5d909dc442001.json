{"cell_type":{"06f5bc1b":"code","e5f427bb":"code","1177fbdf":"code","1f031ac3":"code","979a79f1":"code","6297bff4":"code","a6f2eddd":"code","51f2d4eb":"code","96d9cff1":"code","f362d502":"code","1bb66910":"code","c30ac8e7":"code","e19cf467":"code","a3782012":"code","e1304c9e":"code","fcc56b66":"code","6ed5bcd2":"code","ca500108":"code","d79a0f6c":"code","5b3a3abd":"code","509326a0":"code","17d5b6f3":"code","89c9c37d":"code","1b30f3c1":"code","2e709deb":"code","c1c4a0a2":"code","1408ef06":"code","86d8fabc":"code","af0debec":"code","7b76bf2e":"code","7ab87a23":"code","244a07b2":"code","002741aa":"code","7f442790":"code","408146fb":"code","902511c6":"code","780bfc61":"code","db4fd5f6":"code","9943a859":"code","23dd9f56":"code","e800b79d":"code","596a2fd7":"code","cc25c159":"code","93619d6e":"code","29e9ed1a":"code","28d54f2b":"code","b43bd755":"code","4f424e1e":"code","3b7ee065":"code","a1717839":"code","e8c64059":"code","1a7fcc18":"code","3cc37513":"code","5d409c63":"code","798cbbb3":"code","9b7c10c9":"code","85ccb66e":"code","1ff39ac1":"code","55b8ed6c":"code","1b665c2e":"code","fedc4e64":"code","96a21696":"code","8206ff9d":"code","9a334cab":"code","1541888f":"code","2ff93cd4":"code","60b0b0f2":"code","79f7d538":"code","cf049a29":"code","ff341048":"code","74e578db":"code","d9c6553a":"code","eaaad34a":"code","30b74bb1":"code","d49e8b1c":"code","f2c87a01":"code","d4127891":"code","aefb574a":"code","c788ccf2":"code","6235fdee":"code","b7aed445":"code","a04a68ef":"code","90f89440":"code","d2c1efc0":"code","2aae0e3e":"code","05b68849":"code","c27afc90":"code","96c3cd23":"code","6e8f131d":"code","99c5b173":"code","51a63638":"code","6a5cd341":"code","f1af538a":"code","55be1186":"code","71a76cf7":"code","cb2c0807":"code","02e2b858":"code","c15ac4fe":"code","3c6ab46f":"code","bb179b89":"code","f48d0461":"code","6f620880":"code","8488061a":"code","756d67cc":"code","8c6faa8f":"code","6e6c9911":"code","1eac8e04":"code","dd2f4397":"code","304f5835":"code","489c913e":"code","c96bc39f":"code","f0925471":"code","ff8fb823":"code","988565f3":"code","d5c06f18":"code","794f2ae4":"code","0d2e3252":"code","a716265d":"code","3293be2d":"code","d7c749b4":"code","a4c9948d":"code","dcb5f0b8":"code","6b35ef6a":"code","3374f465":"markdown","69354518":"markdown","a81263b1":"markdown","ee1cf9c5":"markdown","35436842":"markdown","de6d8844":"markdown","c3981d35":"markdown","7a2de0d7":"markdown","b7edcc5c":"markdown","c2754305":"markdown","2922d7aa":"markdown","b60852dd":"markdown","e0f6bbf4":"markdown","0f58e2d7":"markdown","c0dcc007":"markdown","f4f7c2e5":"markdown","eb6a354c":"markdown","bbde434e":"markdown","5bf571b7":"markdown","7241de7f":"markdown","da10e42a":"markdown","f9172e42":"markdown","943fe02a":"markdown","ab62dd8f":"markdown","1aec86e6":"markdown","0231fd04":"markdown","e193f007":"markdown","6928f8c3":"markdown","4cadad3b":"markdown","555bbb47":"markdown","1567cf55":"markdown","76b767b2":"markdown","b4cabc89":"markdown","6c56cd1a":"markdown","07e50aa2":"markdown","b060ad5e":"markdown","5f0dbb3d":"markdown","770f4c5e":"markdown","582bee8d":"markdown","7cc94d75":"markdown","005e1420":"markdown","af87d1e4":"markdown","7a03e6ea":"markdown","dc0c8140":"markdown","f752823b":"markdown","ac4b9fd1":"markdown","92ea21f2":"markdown"},"source":{"06f5bc1b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e5f427bb":"import warnings\nwarnings.filterwarnings('ignore')","1177fbdf":"# Load the train_dataset, train_salaries, test_dataset\ntrain_sal = pd.read_csv('\/kaggle\/input\/salarypredictions\/train_salaries.csv')\ntrain_df = pd.read_csv('\/kaggle\/input\/salarypredictions\/train_dataset.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/salarypredictions\/test_dataset.csv')","1f031ac3":"train_sal.head(2)","979a79f1":"train_df.head(2)","6297bff4":"# Adding salary data to train_dataset\ntrain_df1 = train_df.merge(train_sal,on='jobId')","a6f2eddd":"# Using pandas profile for basic EDA\nimport pandas_profiling \ntrain_df1.profile_report()","51f2d4eb":"# Know all the names of the columns\u00b6\ntrain_df.columns","96d9cff1":"# Check which columns are having categorical, numerical or boolean values of train_dataset\n\ntrain_df.info()","f362d502":"# Check which columns are having categorical, numerical or boolean values of test_dataset\ntrain_df.info()","1bb66910":"train_df1.describe()","c30ac8e7":"test_df.describe()","e19cf467":"print('train data length: {}'.format(len(train_df1)))\nprint('test data length: {}'.format(len(test_df)))","a3782012":"# get how many unique values are in train_dataset\n\ntrain_df1.nunique()","e1304c9e":"# get how many unique values are in test_dataset\ntest_df.nunique()","fcc56b66":"# Check for missing values in all the columnns of the train_dataset\ntrain_df1.isnull().sum()","6ed5bcd2":"# Check for missing values in all the columnns of the test_dataset\ntest_df.isnull().sum()","ca500108":"# drop jobId and companyId from train_dataset\n\ncol = ['jobId','companyId']\ntrain_df1.drop(col,inplace=True,axis=1)\ntrain_df1","d79a0f6c":"# drop jobId and companyId from test_dataset\n\ncol = ['jobId','companyId']\ntest_df.drop(col,inplace=True,axis=1)\ntest_df","5b3a3abd":"# looping on whole dataset for geting list of categorical and numerical data column name and storing in respective list variable\n\ncategorical_col = [i for i in train_df1.columns if train_df1[i].dtype == 'object']\nnumerical_col = [i for i in train_df1.columns if train_df1[i].dtype != 'object']","509326a0":"# Correlation metrix using pandas\ntrain_df1.corr()","17d5b6f3":"# Correlation metrix using seaborn\n\nimport seaborn as sns\n\nsns.heatmap(train_df1.corr())","89c9c37d":"# import necessary libraries for chi-square test\nfrom scipy.stats import chi2_contingency\nfrom scipy.stats import chi2\n\n# creating function for performing chi-sqaure test on two columns\ndef perform_chi_square_test(var_1,var_2):\n    \n    #Contingency Table\n    contingency_table = pd.crosstab(train_df1[var_1],train_df1[var_2])\n    \n    #Observed Values\n    observed_values = contingency_table.values\n    \n    #Expected Values\n    b = chi2_contingency(contingency_table)\n    expected_values = b[3]\n    \n    #Degree of Freedom\n    no_of_rows = len(contingency_table.iloc[0:,0])\n    no_of_columns = len(contingency_table.iloc[0,0:])\n    degree_f=(no_of_rows-1)*(no_of_columns-1)\n    print(\"Degree of Freedom: \",degree_f)\n    \n    #Significance Level 5%\n    alpha = 0.05\n    print('Significance level: ',alpha)\n    \n    #chi-square statistic\n    chi_square = sum([(o-e)**2.\/e for o,e in zip(observed_values,expected_values)])\n    chi_square_statistic = chi_square[0]+chi_square[1]\n    print(\"chi-square statistic: \",chi_square_statistic)\n    \n    #critical_value\n    critical_value=chi2.ppf(q=1-alpha,df=degree_f)\n    print('critical_value:',critical_value)\n    \n    #p-value\n    p_value = 1-chi2.cdf(x=chi_square_statistic,df=degree_f)\n    print('p-value:',p_value)\n    \n    # conditional statements for checking chi-sqaure test condition for hypothesis selection based on chi_square_statistic and critical_value     \n    if chi_square_statistic>=critical_value:\n        print(\"Reject H0,There is a relationship between 2 categorical variables\")\n    else:\n        print(\"Retain H0,There is no relationship between 2 categorical variables\")\n    \n    # conditional statements for checking chi-sqaure test condition for hypothesis selection based on p_value and alpha\n    if p_value<=alpha:\n        print(\"Reject H0,There is a relationship between 2 categorical variables\")\n    else:\n        print(\"Retain H0,There is no relationship between 2 categorical variables\")\n    ","1b30f3c1":"# looping on categorical data list and use function for performing chi-square test on columns from dataset\nfor i in categorical_col:\n    for j in categorical_col:\n        if i == j:\n            pass\n        else:\n            print('chi-square test on: %s %s'%(i,j))\n            print()\n            perform_chi_square_test(i,j)\n            print('-----'*15,'\\n')","2e709deb":"# Scatter plot using matplotlib \nimport matplotlib.pyplot as plt\n\n# create function for ploting scatterplot between two columns of dataset\ndef scatr_plt(i,j):\n    plt.figure(figsize=(20,6))\n    plt.scatter(x=train_df1[i],y=train_df1[j])\n    plt.xlabel(i)\n    plt.ylabel(j)\n    plt.legend()\n    plt.show()\n\n# Loop through numerical data list and use function to scatter plot between two columns\nfor i in numerical_col:\n    for j in numerical_col:\n        if i == j:\n            pass\n        else:\n            scatr_plt(i,j)\n            ","c1c4a0a2":"### Histogram \n\nfor i in numerical_col:\n    plt.figure(figsize=(20,6))\n    plt.hist(train_df1[i])\n    plt.title(i)\n    plt.show()","1408ef06":"import seaborn as sns","86d8fabc":"train_df1.groupby(by='jobType').count()","af0debec":"# group data by jobType and plot count plot\nplt.figure(figsize=(20,6))\ntrain_df1.groupby(by='jobType').count().plot.bar()\nplt.show()","7b76bf2e":"# group data by degree and plot count plot\nplt.figure(figsize=(20,6))\ntrain_df1.groupby(by='degree').count().plot.bar()\nplt.show()","7ab87a23":"# group data by major and plot count plot\nplt.figure(figsize=(20,6))\ntrain_df1.groupby(by='major').count().plot.bar()\nplt.show()","244a07b2":"# group data by industry and plot count plot\nplt.figure(figsize=(20,6))\ntrain_df1.groupby(by='industry').count().plot.bar()\nplt.show()","002741aa":"# group data by yearsExperience and plot count plot\nplt.figure(figsize=(20,6))\ntrain_df1.groupby(by='yearsExperience').count().plot.bar()\nplt.show()","7f442790":"# group data by milesFromMetropolis and plot count plot\nplt.figure(figsize=(20,6))\ntrain_df1.groupby(by='milesFromMetropolis').count().plot.bar()\nplt.show()","408146fb":"# goup data by salary and plot count plot\nplt.figure(figsize=(20,6))\ntrain_df1.groupby(by='salary').count().plot.bar()\n\nplt.show()","902511c6":"# box plot for yearsExperience column\nplt.figure(figsize=(20,6))\ntrain_df1.boxplot('yearsExperience')\nplt.show()","780bfc61":"train_df1.columns","db4fd5f6":"# box plot for milesFromMetropolies column\nplt.figure(figsize=(20,6))\ntrain_df1.boxplot('milesFromMetropolis')\nplt.show()","9943a859":"# box plot for salary column\nplt.figure(figsize=(20,6))\ntrain_df1.boxplot('salary')\nplt.show()","23dd9f56":"# box plot using seaborn \n# box plot for yearsExperience column\n# box plot for mileFromMetropolis column\n# box plot for salary column\n\ncol = ['yearsExperience','milesFromMetropolis','salary']\nfor i in col:\n    plt.figure(figsize=(20,6))\n    sns.boxplot(train_df1[i])\n    plt.show()","e800b79d":"# violin plot for yearsExperience and milesFromMetropolis columns\nplt.figure(figsize=(20,6))\nsns.violinplot(x='yearsExperience', y='milesFromMetropolis', data=train_df1, palette='rainbow')\nplt.show()","596a2fd7":"# violin plot for yearsExperience and salary columns\nplt.figure(figsize=(20,6))\nsns.violinplot(x='yearsExperience', y='salary', data=train_df1, palette='rainbow')\nplt.show()","cc25c159":"# violin plot for milesFromMetropolis from salary columns\nplt.figure(figsize=(20,6))\nsns.violinplot(y='salary', x='milesFromMetropolis', data=train_df1, palette='rainbow')\nplt.show()","93619d6e":"# boxen plot for yearsExperience and salary columns\nplt.figure(figsize=(20,6))\nsns.boxenplot(x='yearsExperience', y='salary', data=train_df1, palette='rainbow')\nplt.show()","29e9ed1a":"# boxen plot for yearsExperience and milesFromMetropolis columns\nplt.figure(figsize=(20,6))\nsns.boxenplot(x='yearsExperience',y='milesFromMetropolis', data=train_df1, palette='rainbow')\nplt.show()","28d54f2b":"# boxen plot for milesFromMetropolis from salary columns\nplt.figure(figsize=(20,6))\nsns.boxenplot(y='salary',x='milesFromMetropolis', data=train_df1, palette='rainbow')\nplt.show()","b43bd755":"# point plot for yearsExperience and milesFromMetropolis columns\nplt.figure(figsize=(20,6))\nsns.pointplot(x='yearsExperience',y='milesFromMetropolis', data=train_df1, palette='rainbow')\nplt.show()","4f424e1e":"# point plot for yearsExperience aminmax_scale salary columns\nplt.figure(figsize=(20,6))\nsns.pointplot(x='yearsExperience', y='salary', data=train_df1, palette='rainbow')\nplt.show()","3b7ee065":"# point plot for milesFromMetropolis from salary columns\nplt.figure(figsize=(20,6))\nsns.pointplot(y='salary',x='milesFromMetropolis', data=train_df1, palette='rainbow')\nplt.show()","a1717839":"# count plot of whole datset based on yearsExperience\n\nplt.figure(figsize=(20,6))\nsns.countplot(train_df1['yearsExperience'],palette='rainbow')\nplt.show()","e8c64059":"# count plot of whole datset based on milesFromMetropolis\nplt.figure(figsize=(20,6))\nsns.countplot(train_df1['milesFromMetropolis'],palette='rainbow')\nplt.show()","1a7fcc18":"# count plot of whole datset based on salary\nplt.figure(figsize=(20,6))\nsns.countplot(train_df1['salary'],palette='rainbow')\nplt.show()","3cc37513":"# swarm plot for yearsExperience and milesFromMetropolis columns\nplt.figure(figsize=(20,6))\nsns.swarmplot(x='yearsExperience',y='milesFromMetropolis', data=train_df1[:50000], palette='rainbow')\nplt.show()","5d409c63":"# swarm plot for yearsExperience and salary columns\nplt.figure(figsize=(20,6))\nsns.swarmplot(x='yearsExperience',y='salary', data=train_df1[:50000], palette='rainbow')\nplt.show()","798cbbb3":"# swarm plot for milesFromMetropolis and salary columns\nplt.figure(figsize=(20,6))\nsns.swarmplot(x='milesFromMetropolis',y='salary', data=train_df1[:50000], palette='rainbow')\nplt.show()","9b7c10c9":"# combine boxen and swarm plot for yearsExperience and milesFromMetropolis columns\nplt.figure(figsize=(20,6))\nax = sns.swarmplot(x='yearsExperience',y='milesFromMetropolis', data=train_df1[:50000], zorder=0)\n\nsns.boxenplot(x='yearsExperience', y='milesFromMetropolis', data=train_df1[:50000],showfliers=False,ax=ax)\nplt.show()","85ccb66e":"# combine boxen and swarm plot for yearsExperience and salary columns\nplt.figure(figsize=(20,6))\nax = sns.swarmplot(x='yearsExperience',y='salary', data=train_df1[:50000], zorder=0)\n\nsns.boxenplot(x='yearsExperience', y='salary', data=train_df1[:50000],ax=ax)\nplt.show()","1ff39ac1":"# combine boxen and swarm plot for milesFromMetropolis and salary columns\nplt.figure(figsize=(20,6))\nax = sns.swarmplot(x='milesFromMetropolis',y='salary', data=train_df1[:50000],zorder=0.5)\n\nsns.boxenplot(x='milesFromMetropolis', y='salary', data=train_df1[:50000],showfliers=False,ax=ax)\nplt.show()","55b8ed6c":"# strip plot between milesFromMetropolis and  yearsExperience columns\nplt.figure(figsize=(20,6))\nsns.stripplot(x='yearsExperience', y='milesFromMetropolis', data=train_df1[:50000], palette='rainbow')\nplt.show()","1b665c2e":"# strip plot between yearsExperience , salary columns\nplt.figure(figsize=(20,6))\nsns.stripplot(x='yearsExperience', y='salary', data=train_df1[:50000], palette='rainbow')\nplt.show()","fedc4e64":"# strip plot between milesFromMetropolis and salary columns\nplt.figure(figsize=(20,6))\nsns.stripplot(x='milesFromMetropolis', y='salary', data=train_df1[:50000], palette='rainbow')\nplt.show()","96a21696":"# import statsmodle library for vif \nimport statsmodels.api as sm","8206ff9d":"# creating a dataframe of just numerical values from train_dataset \ninp_df = pd.DataFrame(train_df1[numerical_col],columns=numerical_col)\n\n# target values from train_dataset\ny = train_df1['salary']\n\n# numerical values column names\nnum_col = [i for i in numerical_col if i != 'salary' ]\nnum_col","9a334cab":"# loop for calculating VIF for each feature.\nfor i in range(0,len(num_col)):\n    # taking one column as target variable\n    y = inp_df['salary']\n    # taking all other remaining columns as fetaure variable\n    X = inp_df[num_col]\n  # Instantiating the statsmodel\n    model = sm.OLS(y,X)\n  # fiting the OLS model on y and x\n    result = model.fit()\n  # geting the r^2 value of results.\n    r_sqrd = result.rsquared\n  # calculating vif value\n    vif = round(1\/(1-r_sqrd),2)\n    \n    print(f'R Square value of {num_col[i]} columns is {round(r_sqrd,2)} keeping all other columns as features Variance inflation Factor of {num_col[i]} columns is {vif}')\n    ","1541888f":"from scipy import stats","2ff93cd4":"# function to perform anova test between two variables.\ndef perform_anova(x,y):\n    # creating dataframe of two variables of interest\n    train_anova = train_df1[[x,y]]\n  # gouping the data in new dataframe\n    group = train_anova.groupby(x).count().reset_index()\n  # print grouped data\n    print(group)\n  \n   # getting list of unique values from new dataframe for first variable\n    unique_majors = train_anova[x].unique()\n  # looping through each value present in list of unique values to plot probplot\n    for major in unique_majors:\n        stats.probplot(train_anova[train_anova[x] == major][y],dist='norm',plot=plt)\n        plt.title(\"Probability Plot - \" +  major)\n        plt.show()\n  \n  # calculate ratio of the largest to the smallest sample standard deviation\n    ratio = train_anova.groupby(x).std().max() \/ train_anova.groupby(x).std().min()\n    print(ratio)\n  \n  # Create ANOVA backbone table with empty string value, columns names -> 'Source of Variation', 'SS', 'df', 'MS', 'F', 'P-value', 'F crit'\n    data = [['Between Groups', '', '', '', '', '', ''], ['Within Groups', '', '', '', '', '', ''], ['Total', '', '', '', '', '', '']]\n    anova_table = pd.DataFrame(data,columns = ['Source of Variation','SS','df','MS','F','P-value','F crit'])\n    anova_table.set_index('Source of Variation',inplace = True)\n  # calculate SSTR and update anova table, with Source of variation = 'Between Groups'\n    x_bar = train_anova[y].mean()\n    SSTR = train_anova.groupby(x).count() * (train_anova.groupby(x).mean() - x_bar)**2\n    anova_table['SS']['Between Groups'] = SSTR[y].sum()\n  # calculate SSE and update anova table, with Source of variation = 'Within Groups'\n    SSE = (train_anova.groupby(x).count() - 1) * train_anova.groupby(x).std()**2\n    anova_table['SS']['Within Groups'] = SSE[y].sum()\n  # calculate SSTR and update anova table, with Source of variation = 'Total'\n    SSTR = SSTR[y].sum() + SSE[y].sum()\n    anova_table['SS']['Total'] = SSTR\n  \n  # update degree of freedom, for each groups 'Between Groups', 'Within Groups', 'Total'\n    anova_table['df']['Between Groups'] = train_anova[x].nunique() - 1\n    anova_table['df']['Within Groups'] = train_anova.shape[0] - train_anova[x].nunique()\n    anova_table['df']['Total'] = train_anova.shape[0] - 1\n  \n  # calculate MS\n    anova_table['MS'] = anova_table['SS'] \/ anova_table['df']\n  # calculate F \n    F = anova_table['MS']['Between Groups'] \/ anova_table['MS']['Within Groups']\n    anova_table['F']['Between Groups'] = F\n  # p-value\n    anova_table['P-value']['Between Groups'] = 1 - stats.f.cdf(F,anova_table['df']['Between Groups'],\n                                                               anova_table['df']['Within Groups'])\n  # F critical \n    \n  # alpha value\n    alpha = 0.05\n  # possible hypothesis types \"right-tailed, left-tailed, two-tailed\" , choose one type and calculate F critcial and add to backbone dataframe\n    tail_hypothesis_type = \"two-tailed\"\n    if tail_hypothesis_type == \"two-tailed\":\n        alpha \/= 2\n    anova_table['F crit']['Between Groups'] = stats.f.ppf(1-alpha, anova_table['df']['Between Groups'],\n                                                          anova_table['df']['Within Groups'])\n  # Final ANOVA Table\n    print(anova_table)\n\n  # The p-value approach\n    print(\"Approach 1: The p-value approach to hypothesis testing in the decision rule\")\n    conclusion = \"Failed to reject the null hypothesis.\"\n    if anova_table['P-value']['Between Groups'] <= alpha:\n        conclusion = \"Null Hypothesis is rejected.\"\n    print(\"F-score is:\", anova_table['F']['Between Groups'],\n          \" and p value is:\",anova_table['P-value']['Between Groups'])    \n    print(conclusion)\n      \n  # The critical value approach\n    print(\"\\n--------------------------------------------------------------------------------------\")\n    print(\"Approach 2: The critical value approach to hypothesis testing in the decision rule\")\n    conclusion = \"Failed to reject the null hypothesis.\"\n    if anova_table['F']['Between Groups'] > anova_table['F crit']['Between Groups']:\n        conclusion = \"Null Hypothesis is rejected.\"\n    print(\"F-score is:\", anova_table['F']['Between Groups'], \" and critical value is:\", anova_table['F crit']['Between Groups'])\n    print(conclusion)\n    ","60b0b0f2":"# perform anova test on major and salary\nperform_anova('major','salary')","79f7d538":"# perform anova test on jobType and salary\nperform_anova('jobType','salary')","cf049a29":"# perform anova test on degree and salary\nperform_anova('degree','salary')","ff341048":"# perform anova test on industry and salary\nperform_anova('industry','salary')","74e578db":"# perform anova test on jobType and yearsExperience\nperform_anova('jobType','yearsExperience')","d9c6553a":"# perform anova test on degree and yearsExperience\nperform_anova('degree','yearsExperience')","eaaad34a":"# perform anova test on major and yearsExperience\nperform_anova('major','yearsExperience')","30b74bb1":"# perform anova test on industry and yearsExperiencea\nperform_anova('industry','yearsExperience')","d49e8b1c":"import scipy\nfrom scipy.cluster import hierarchy as hc\n\nX = train_df1[1:50000]\ncorr = np.round(scipy.stats.spearmanr(X).correlation, 4)\ncorr_condensed = hc.distance.squareform(1-corr)\nz = hc.linkage(corr_condensed, method='average')\nfig = plt.figure(figsize=(16,10))\ndendrogram = hc.dendrogram(z, labels=X.columns, orientation='left', leaf_font_size=16)\nplt.show()","f2c87a01":"from sklearn.preprocessing import MinMaxScaler","d4127891":"# Helper function for scaling all the numerical data using MinMaxScalar\ndef scaled(data,cols):\n    data = data[cols]\n    trans = MinMaxScaler()\n    data = trans.fit_transform(data)\n    scaled_data = pd.DataFrame(data)\n    scaled_data.columns = ['yearsExperience','milesFromMetropolis','salary']\n    return scaled_data","aefb574a":"# Making a list of the column names to be scaled \ncols = [i for i in train_df1.columns if train_df1[i].dtypes != 'object']\n# passing data and column name for scaling\nscaled_data = scaled(train_df1,cols)","c788ccf2":"# Importing OneHotEncoder for encoding the categorical data\nfrom sklearn.preprocessing import OneHotEncoder as ohe\n\n\n# class for containing all functionality required for OneHotEncoding\nclass OneHotEncoder(ohe):   # class inherits from sklearn.preprocessing.OneHotEncode\n    def __init__(self,**kwargs):\n        super(OneHotEncoder,self).__init__(**kwargs)\n        self.fit_flag = False   # check on encoder fitting\n    \n    # helper function to fit data  \n    def fit(self,X,**kwargs):\n        out = super().fit(X)   # acessing fit method from sklearn.preprocessing.OneHotEncode\n        self.fit_flag = True\n        return out\n     \n    # helper function to transform data  \n    def transform(self,X,**kwargs):\n        sparse_matrix = super(OneHotEncoder,self).transform(X)\n        transf_columns = self.transf_columns_name(X=X)  # transf_columns_name method to access class names\n        d_out = pd.DataFrame(sparse_matrix.toarray(),columns=transf_columns,index=X.index) # making Df using col names returned by transf_columns_name method \n        return d_out\n\n    # helper function to fit and transform data \n    def fit_transform(self,X,**kwargs):\n        self.fit(X)\n        return self.transform(X)\n    \n    # helper function to get new column names after fitting and tranforming data from sklearn.preprocessing import OneHotEncoder\n    def transf_columns_name(self,X):\n        transf_columns = []  \n        for col_indx,col_name in enumerate(X.columns):\n            counter = 0\n            while counter < len(self.categories_[col_indx]):\n                transf_columns.append(f'{col_name}-{self.categories_[col_indx][counter]}')\n                counter += 1\n        return transf_columns # to transform ","6235fdee":"train_df1.head()","b7aed445":"# Split the features and the target\nfeatures = train_df1[categorical_col]\ntarget = scaled_data.salary","a04a68ef":"# Features\nfeatures.head()","90f89440":"# Target\ntarget.head()\n","d2c1efc0":"# passing features dataframe for one hot encoding process\n\nencoder_1hot = OneHotEncoder()\nencoded_features = encoder_1hot.fit_transform(features)\nscaled = scaled_data[['yearsExperience','milesFromMetropolis']]\nencoded_features = pd.concat([encoded_features,scaled],axis=1)\nencoded_features.shape","2aae0e3e":"encoded_features.head()","05b68849":"scaled_data","c27afc90":"encoded_features = encoded_features[:50000]\ntarget = target[:50000]","96c3cd23":"#importing Sklearn library for spliting train dataset into train and test dataset\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_x,test_x,train_y,test_y = train_test_split(encoded_features,target,test_size=.3,random_state=108)","6e8f131d":"# importing necessary libraries for geting metrics of models\nfrom sklearn.metrics import mean_squared_error,r2_score\n\n# Function for calculating RMSE \ndef root_mean_squared_error(y_true,y_pred):\n    # initially say no error\n    eroor = 0 \n    # loop for all samples in true and pred list\n    for ytr,ypr in zip(y_true,y_pred):\n        # calcuting sqrd error and add to error with root\n        eroor += (ytr-ypr)**2\n    # calcuting mean error\n    return f'Test RMSE: {np.sqrt(eroor \/ len(y_true))}'\n        \n# Function for calculating all the relevant metrics\n## Function to calculate mse\ndef mean_squared_error(y_true,y_pred):\n                                \n    # initially say no error\n    eroor = 0 \n    # loop for all samples in true and pred list\n    for ytr,ypr in zip(y_true,y_pred):\n        # calcuting sqrd error and add to error\n        eroor += (ytr-ypr)**2\n    # return mean error\n    return f'Test MSE: {eroor \/ len(y_true)}'\n\n## Function to calculate mae   \ndef mean_absolute_error(y_true,y_pred):\n    error = 0\n    for ytr, ypr in zip(y_true,y_pred):\n        error += np.abs(ytr - ypr)\n    return f'Test MAE: {error \/ len(y_true)}'\n\n## Function to calculate r2_score\ndef r2_scr(y_true,y_pred):\n    nume = 0\n    den = 0\n    true_value_mean = np.mean(y_true)\n    for ytr,ypr in zip(y_true,y_pred):\n        # updating nume\n        nume += (ytr-ypr)**2 \n        # updating den\n        den += (ytr-true_value_mean)**2\n        ratio = nume \/ den\n    return f'Test r2_scr: {1-ratio}'","99c5b173":"# Helper function for Visualizing importance of all the features in the dataset for the prediction\ndef features_imp(x,f_imp):\n    # creating dataframe for feature name and feature importance\n    features = x.columns\n    df = {'features':features,'imp':f_imp}\n    df = pd.DataFrame(df)\n    # grouping all data and sorting in descending order\n    df = df.sort_values('imp',ascending=False,ignore_index=True)\n    print()\n    # ploting feature importance data using boxenplot\n    fig,ax = plt.subplots(figsize=(12,6))\n    ax = sns.boxenplot(x='imp',y='features',data=df)\n    ax.grid()\n    ax.set_title('importance')\n    ax.set_xlabel('feature importance')\n    ax.set_ylabel('column')\n    # return fig, ax\n    return fig,ax","51a63638":"%%time\n\n# Fit a Linear Regression model to the train dataset\n\n# Import LinearRegressor\nfrom sklearn.linear_model import LinearRegression\n\n# Instantiate the model\nmodel = LinearRegression()\n# Fit the model to the data\nmodel.fit(train_x,train_y)\nf_imp = model.coef_\n# print score of the model by calling function\nprint(root_mean_squared_error(test_y,model.predict(test_x)))\nprint(mean_squared_error(test_y,model.predict(test_x)))\nprint(r2_scr(test_y,model.predict(test_x)))\nprint(mean_absolute_error(test_y,model.predict(test_x)))\n# visualizing the importance of features.\nfeatures_imp(train_x,f_imp)\nplt.show()","6a5cd341":"%%time\n# Fit a Random Forest Regressor model to the train dataset\n\n# Import RandomForrestRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Instantiate the model\nmodel = RandomForestRegressor()\n\n# Fit the model to the data\nmodel.fit(train_x,train_y)\nf_imp = model.feature_importances_\n# print score of the model by calling function\nprint(root_mean_squared_error(test_y,model.predict(test_x)))\nprint(mean_squared_error(test_y,model.predict(test_x)))\nprint(r2_scr(test_y,model.predict(test_x)))\nprint(mean_absolute_error(test_y,model.predict(test_x)))\n# visualizing the importance of features.\nfeatures_imp(train_x,f_imp)\nplt.show()","f1af538a":"%%time\n# Fit a K-Neighbour Regressor model to the train dataset\n\n# Import KNeighbourRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.inspection import permutation_importance\n# Instantiate the model\nmodel = KNeighborsRegressor()\n# Fit the model to the data\nmodel.fit(train_x,train_y)\nresults = permutation_importance(model,train_x,train_y)\n# get importance\nf_imp = results.importances_mean\n\n# print score of the model by calling function\nprint(root_mean_squared_error(test_y,model.predict(test_x)))\nprint(mean_squared_error(test_y,model.predict(test_x)))\nprint(r2_scr(test_y,model.predict(test_x)))\nprint(mean_absolute_error(test_y,model.predict(test_x)))\n# visualizing the importance of features.\nfeatures_imp(train_x,f_imp)\nplt.show()","55be1186":"%%time\n# Fit a Gradient Boosting Regressor model to the train dataset\n\n# Import GradientBoostingRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n# Instantiate the model\nmodel = GradientBoostingRegressor()\n# Fit the model to the data\nmodel.fit(train_x,train_y)\nf_imp = model.feature_importances_\n# print score of the model by calling function\nprint(root_mean_squared_error(test_y,model.predict(test_x)))\nprint(mean_squared_error(test_y,model.predict(test_x)))\nprint(r2_scr(test_y,model.predict(test_x)))\nprint(mean_absolute_error(test_y,model.predict(test_x)))\n# visualizing the importance of features.\nfeatures_imp(train_x,f_imp)\nplt.show()","71a76cf7":"%%time\n# Fit a Decision Tree Regressor model to the train dataset\n\n# Import DecisionTreeRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Instantiate the model\nmodel = DecisionTreeRegressor()\n# Fit the model to the data\nmodel.fit(train_x,train_y)\nf_imp = model.feature_importances_\n# print score of the model by calling function\nprint(root_mean_squared_error(test_y,model.predict(test_x)))\nprint(mean_squared_error(test_y,model.predict(test_x)))\nprint(r2_scr(test_y,model.predict(test_x)))\nprint(mean_absolute_error(test_y,model.predict(test_x)))\n# visualizing the importance of features.\nfeatures_imp(train_x,f_imp)\nplt.show()","cb2c0807":"%%time\n\n# Fit a AdaBoost Regressor model to the train dataset\n\n# Import AdaBoostRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\n# Instantiate the model\nmodel = AdaBoostRegressor()\n# Fit the model to the data\nmodel.fit(train_x,train_y)\nf_imp = model.feature_importances_\n# print score of the model by calling function\nprint(root_mean_squared_error(test_y,model.predict(test_x)))\nprint(mean_squared_error(test_y,model.predict(test_x)))\nprint(r2_scr(test_y,model.predict(test_x)))\nprint(mean_absolute_error(test_y,model.predict(test_x)))\n# visualizing the importance of features.\nfeatures_imp(train_x,f_imp)\nplt.show()","02e2b858":"%%time\n# Fit a XGB Regressor model to the train dataset\n\n# Import XGBRegressor\nfrom xgboost import XGBRegressor\n# Instantiate the model\nmodel = XGBRegressor()\n# Fit the model to the data\nmodel.fit(train_x,train_y)\nf_imp = model.feature_importances_\n# print score of the model by calling function\nprint(root_mean_squared_error(test_y,model.predict(test_x)))\nprint(mean_squared_error(test_y,model.predict(test_x)))\nprint(r2_scr(test_y,model.predict(test_x)))\nprint(mean_absolute_error(test_y,model.predict(test_x)))\n# visualizing the importance of features.\nfeatures_imp(train_x,f_imp)\nplt.show()","c15ac4fe":"%%time\n# Fit a lightgbm Regressor model to the train dataset\n\n# Import lightgbm\nfrom lightgbm import LGBMRegressor\n# Instantiate the model\nmodel = LGBMRegressor()\n# Fit the model to the data\nmodel.fit(train_x,train_y)\nf_imp = model.feature_importances_\n# print score of the model by calling function\nprint(root_mean_squared_error(test_y,model.predict(test_x)))\nprint(mean_squared_error(test_y,model.predict(test_x)))\nprint(r2_scr(test_y,model.predict(test_x)))\nprint(mean_absolute_error(test_y,model.predict(test_x)))\n# visualizing the importance of features.\nfeatures_imp(train_x,f_imp)\nplt.show()","3c6ab46f":"train_x.shape,train_y.shape,test_x.shape,test_y.shape","bb179b89":"from lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error as MSE, mean_absolute_error as MAE,median_absolute_error as MedAE, r2_score as r2_scr\n# Helper function for comparing models metrics\ndef models_performance(models,models_name,train_x,test_x,train_y,test_y):\n    data = {'Metric':['MSE','MAE','MeAE','RMSE','r2_scr']}\n    df_train = pd.DataFrame(data)\n    df_test = pd.DataFrame(data)\n\n    for (m,model_name) in zip(models,models_name):\n        model = m()\n        model.fit(train_x,train_y)\n        pred_train_y = model.predict(train_x)\n        pred_test_y = model.predict(test_x)\n        # storing results in list\n        results = [MSE(pred_train_y,train_y),MSE(pred_test_y,test_y), MAE(pred_train_y,train_y),MAE(pred_test_y,test_y),\n                   MedAE(pred_train_y,train_y),MedAE(pred_test_y,test_y),np.sqrt(MSE(pred_train_y,train_y)),\n                   np.sqrt(MSE(pred_test_y,test_y)),r2_scr(pred_train_y,train_y),r2_scr(pred_test_y,test_y)]\n        # using indexing grabin train results only\n        df_train[model_name] = [results[0],results[2],results[4],results[6],results[8]]\n        # using indexing grabin test results only\n        df_test[model_name] = [results[1],results[3],results[5],results[7],results[9]]\n    return df_train,df_test\n\n###############\n###############\n# list of models object and name\nmodels = [LGBMRegressor,XGBRegressor,AdaBoostRegressor,DecisionTreeRegressor,GradientBoostingRegressor,RandomForestRegressor]\nmodels_name = ['LGDM','XGB','AdaBo','DT','GB','RF']\n\n# use function for comparing models by passing list of models object, names, train and test data\ntrain_model_perform,test_model_perform = models_performance(models,models_name,train_x,test_x,train_y,test_y)","f48d0461":"train_model_perform","6f620880":"test_model_perform","8488061a":"# printing rmse comparision of model on train and test\n\ntrain_model_perform.loc[4:]","756d67cc":"test_model_perform.loc[4:]","8c6faa8f":"# Helper function to perform hyper parameter tunning with RandomizedSearchCV\ndef rs_cv(model,train_x,train_y,param):\n    from sklearn.model_selection import RandomizedSearchCV\n  # Random search of parameters, using 3 fold cross validation,\n  # search across 100 different combinations, and use all available cores\n    randomizer = RandomizedSearchCV(estimator=model,param_distributions=param,n_iter=100,cv=3,verbose=True,n_jobs=-1,random_state=108)\n    randomizer.fit(train_x,train_y)\n  # print best parameters\n    print('Best Params:{}'.format(randomizer.best_params_))","6e6c9911":"# create RandomForest parameters dict for tunning\nrf_param_grid = {'n_estimators':[100,200,250,300,400,500],\n                 'max_depth':[10,20,40,50,80],\n                 'min_samples_split':[2,4,6],\n                 'min_samples_leaf':[2,3,4],\n                 'max_features':['auto','sqrt']\n                }\n# passing data for hyper parameter tunning with Randomized search cv\nrs_cv(RandomForestRegressor(),train_x,train_y,param=rf_param_grid)","1eac8e04":"# create GradientBoostRegressor parameters dict for tunning\ngb_param_grid = {'eta':[0.01,0.015,0.025,0.05,0.1],\n                 'gamma':[0.05-0.1,0.3,0.5,0.7,0.9,1.0]\n                 'learning_rate':[0.0001,0.001,0.01,0.1,0.2],\n                 'alpha':[0,0.1,0.5,1.0],\n                 'subsample':[0.6,0.7,0.8,0.9,1.0],\n                 'max_depth':[3,5,7,9,12,15,17,25]\n                }\n\n# passing data for hyper parameter tunning with Randomized search cv\nrs_cv(GradientBoostingRegressor(),train_x,train_y,param=gb_param_grid)","dd2f4397":"# create KNNRegressor parameters dict for tunning\nfrom sklearn.neighbors import KNeighborsRegressor\nknn_param_grid = {'n_neighbors':[2,4,8,16,32,40,50],\n                 'p':[1,2,3]}\n\n# passing data for hyper parameter tunning with Randomized search cv\nrs_cv(KNeighborsRegressor(),train_x,train_y,param=knn_param_grid)","304f5835":"# create DecisionTreeRegressor parameters dict for tunning\ndt_param_grid = {'spliter':['random','best'],\n                'max_depth':[3,5,7,9,12,15,17],\n                'min_samples_leaf':[2,3,4,6,8],\n                'min_weight_fraction_leaf':[0.01,0.1,0.2,0.4,0.5],\n                'max_features':[None,'auto','log2','sqrt']\n                }\n# passing data for hyper parameter tunning with Randomized search cv\nrs_cv(DecisionTreeRegressor(),train_x,train_y,param=dt_param_grid)","489c913e":"# create AdaBoostRegressor parameters dict for tunning\nadb_param_grid = {'n_estimators':[10,30,40,50],\n                 'learning_rate':[0.001,0.005,0.01,0.1]}\n\n# passing data for hyper parameter tunning with Randomized search cv\nrs_cv(AdaBoostRegressor(),train_x,train_y,param=adb_param_grid)","c96bc39f":"# create XGBoostRegressor parameters dict for tunning\nxgb_param_grid = {'eta':[0.01,0.015,0.025,0.05,0.10],\n                 'gamma':[0.05-0.1,0.3,0.5,0.7,0.9,1.0]\n                 'learning_rate':[0.0001,0.001,0.01,0.1,0.2],\n                 'alpha':[0,0.1,0.5,1.0],\n                 'subsample':[0.6,0.7,0.8,0.9,1.0],\n                 'max_depth':[3,5,7,9,12,15,17,25],\n                 'colsample_bytree':[0.2,0.3,0.4]\n                }\n\n# passing data for hyper parameter tunning with Randomized search cv\nrs_cv(XGBRegressor(),train_x,train_y,param=xgb_param_grid)","f0925471":"# Helper function to perform hyper parameter tunning with GridSearchCV\ndef grids(model,train_x,train_y,grid):\n    from sklearn.model_selection import GridSearchCV\n    # Grid search of parameters, using 5 fold cross validation\n    g_randomizer = GridSearchCV(estimator=model,param_grid=grid,scoring='neg_mean_absolute_error',\n                        cv=5,n_jobs=3,verbose=1,return_train_score=True)\n    #fit model_cv\n    g_randomizer.fit(train_x,train_y)\n    # print best parameters\n    print('Best Params: {}'.format(g_randomizer.best_params_))\n    # print best score\n    print('Best Score: {}'.format(g_randomizer.best_score_))","ff8fb823":"# create parameters dict in list for tunning\nrf_param_grid = {'n_estimators':[100,200,250,300,400,500],\n                 'max_depth':[10,20,40,50,80],\n                 'min_samples_split':[2,4,6],\n                 'min_samples_leaf':[2,3,4],\n                 'max_features':['auto','sqrt']\n                }\n\n# passing data for hyper parameter tunning with Gridsearchcv\ngrids(RandomForestRegressor(),train_x,train_y,grid=rf_param_grid)","988565f3":"# test data\ntest_df.head()","d5c06f18":"obj_df = test_df[['yearsExperience','milesFromMetropolis']]\nobj_cols = obj_df.columns","794f2ae4":"# passing test data for scaling, similarly as done for train data\ndef scaled(data,cols):\n    data = data[cols]\n    trans = MinMaxScaler()\n    data = trans.fit_transform(data)\n    scaled_data = pd.DataFrame(data)\n    scaled_data.columns = ['yearsExperience','milesFromMetropolis']\n    return scaled_data\n# Making a list of the column names to be scaled \ntest_cols = [i for i in test_df.columns if test_df[i].dtypes != 'object']\n# passing data and column name for scaling\ntest_scaled_data = scaled(obj_df,obj_cols)","0d2e3252":"test_scaled_data.head()","a716265d":"# passing test dataset for one hot encoding process\ntest_cat_cols = [i for i in test_df.columns if test_df.dtypes[i] == 'object']\ncat_df = test_df[test_cat_cols]\n\nencoder_1hot = OneHotEncoder()\nencoded_features = encoder_1hot.fit_transform(cat_df)\ntranformed_test_set = pd.concat([encoded_features,test_scaled_data],axis=1)\ntranformed_test_set","3293be2d":"encoded_features.shape,tranformed_test_set[:50000].shape","d7c749b4":"encoded_features.columns","a4c9948d":"tranformed_test_set.columns","dcb5f0b8":"model = RandomForestRegressor()\nmodel.fit(train_x,train_y)\ntest_pred_y = model.predict(tranformed_test_set[:50000])","6b35ef6a":"# creating a dataframe of predicted results\npred_50000 = pd.DataFrame(test_pred_y,columns='Predicted_salary')\n\n# predicted values in dataframe\npred_50000.head(20)","3374f465":"### Now working with the test dataset provided","69354518":"As this is a `Regression Problem` and when it comes to Regression most commonly used evaluation metrics are:\n- Mean absolute error (MAE) => smaller better => `con:` cant tell over\\under fitting\n- Mean squared error (MSE) => `con:` sensitive to outliers\n- Root mean squared error (RMSE) => small values postulates that error made by model has a small deviation from true values\n- Root mean squared logarithmic error (RMSLE)\n- Mean percentage error (MPE)\n- Mean absolute percentage error (MAPE)\n- R2","a81263b1":"\nyearsExperience and salary are positively correlated.\n\nyearsExperience and milesFromMetropolis have no correlation.\n\nmilesFromMetropolis and salary are weakly negatively correlated.","ee1cf9c5":"# RMSE of all model on train and test data","35436842":"##### Using default metricess for more accuracy","de6d8844":"# GridSearch CrossValidation","c3981d35":"# Count Plot","7a2de0d7":"# ANOVA Test","b7edcc5c":"# Light Gradient Boosted Machine Regressor","c2754305":"**`Observation`**<br>\nIt is hard to put labels on all transformed columns, so we are trying to make a class with some fucntions in it which will help us in naming all transformed columns  <br>\n[sklearn.preprocessing.OneHotEncoder](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OneHotEncoder.html)","2922d7aa":"# Linear Regression","b60852dd":"##### Feature importance for different algorith","e0f6bbf4":"# Modeling","0f58e2d7":"# KNeighbors Regressor","c0dcc007":"# Strip Plot","f4f7c2e5":"# Violin Plot","eb6a354c":"[matplotlib-figure-axes-explained-in-detail](https:\/\/pub.towardsai.net\/day-3-of-matplotlib-figure-axes-explained-in-detail-d6e98f7cd4e7)","bbde434e":"# Groupby","5bf571b7":"# Histogram","7241de7f":"# Hyperparameter Tunning\n* RamdomizedSearchCV\n* Grid Search\n","da10e42a":"# Dendrogram","f9172e42":"from sklearn.ensemble import RandomForestRegressor\n\ncol_names = train_x.columns\n####### initialize the model\nmodel = RandomForestRegressor()\n######## fit the model\nmodel.fit(train_x,train_y)\nimportances = model.feature_importances_\nidxs = np.argsort(importances)\n\nplt.figure(figsize=(10,8))\nplt.title('Feature Importance by using Random Forest')\n#sns.boxenplot(range(len(idxs)),importances[idxs],palette='rainbow')\nplt.barh(range(len(idxs)), importances[idxs])\nplt.yticks(range(len(idxs)),[col_names[i] for i in idxs])\nplt.xlabel('Feature Importance')\nplt.show()","943fe02a":"# Box Plot","ab62dd8f":"# Point Plot","1aec86e6":"# Chi-square Test","0231fd04":"# Feature Importance","e193f007":"# Random Forest Regressor","6928f8c3":"# Basic EDA\n### Identifying number of features or columns","4cadad3b":"# Variance inflation factor (VIF)","555bbb47":"### Subset of train dataset\nPloting process of swarm plot was taking huge time because of large dataset <br>\nSo, we take a subset of 50000 samples from train datset and plot it for interpretation","1567cf55":"# Gradient Boosting Regressor","76b767b2":"# Scatter Plot","b4cabc89":"[choosing-the-right-metric-for-machine-learning-models](https:\/\/medium.com\/usf-msds\/choosing-the-right-metric-for-machine-learning-models-part-1-a99d7d7414e4)","6c56cd1a":"# Scaling\n[correct-way-of-normalizing-and-scaling](https:\/\/stackoverflow.com\/questions\/63746182\/correct-way-of-normalizing-and-scaling-the-mnist-dataset)","07e50aa2":"# DecisionTree Regressor","b060ad5e":"# AdaBoost Regressor","5f0dbb3d":"# XGBoost Regressor","770f4c5e":"# Combine plot","582bee8d":"# Swarm Plot\nploting data on 50000 of 1000000 sample for clear visualization","7cc94d75":"# Correlation Matrix","005e1420":"### Get the total number of samples in the dataset using the len() function","af87d1e4":"# Comparing all model based on metric","7a03e6ea":"# OneHot Encoding","dc0c8140":"### Know more mathematical relations of the dataset like count, min, max values, standarad deviation values, mean and different percentile values","f752823b":"# RamdomizedSearch CrossValidation","ac4b9fd1":"########## Importing OneHotEncoder for encoding the categorical data\nfrom sklearn.preprocessing import OneHotEncoder\n\n########## class for containing all functionality required for OneHotEncoding      \ndef onehot(data,cols):\n########## helper function to fit data  \n    data = data[cols]\n    ohe = OneHotEncoder()\n    ohe.fit(data)\n    # helper function to transform data  \n    data = ohe.transform(data)\n    # helper function to fit and transform data \n    data = ohe.fit_transform(data)\n    # helper function to get new column names after fitting and tranforming data\n    col_name = ohe.get_feature_names()\n    hot_data = pd.DataFrame(data,columns=col_name)\n    print(data)\n    ##print(hot_data)\n\nonehot(train_df1,categorical_col)","92ea21f2":"# Boxenplot"}}