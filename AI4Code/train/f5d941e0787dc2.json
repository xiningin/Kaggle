{"cell_type":{"a6dcf89c":"code","9172e92f":"code","829cc48f":"code","b2dbe2f1":"code","dab25819":"code","17e60ecf":"code","ffeeea40":"code","e8336579":"code","3ba75339":"code","f6d7d297":"code","524e0a2e":"code","a502ae83":"code","d1f144e1":"code","a39d23d0":"code","d348c3b1":"markdown","66e56b84":"markdown","4378d5e3":"markdown","347954e0":"markdown","7b0ce0e2":"markdown","400103e2":"markdown","c047d0b1":"markdown"},"source":{"a6dcf89c":"import torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\nimport pandas as pd\nfrom torch.utils.data.dataset import Dataset\nfrom torch.utils.data import DataLoader\nimport numpy as np\nfrom sklearn.model_selection import train_test_split","9172e92f":"df = pd.read_csv('..\/input\/battles.csv')\ndf.head(5)","829cc48f":"#\u00a0no los interesa el n\u00famero de la batalla as\u00ed que a la basura!\ndata = df.drop(columns=['battle_number']).values\ndata.shape, data[:,0].min(), data[:,0].max()","b2dbe2f1":"class BattleDataset(Dataset):\n    def __init__(self, battle_list):\n        self.battle_list = battle_list\n        \n    def __getitem__(self, index):\n        first, second, result = self.battle_list[index]\n        return torch.tensor(first-1).long(), torch.tensor(second-1).long(), torch.tensor(result).float()\n\n    def __len__(self):\n        return self.battle_list.shape[0]","dab25819":"# valid_split_idx = int(data.shape[0] * 0.9)\n# train_data = data[:valid_split_idx]\n# valid_data = data[valid_split_idx:]\n\ntrain_data, valid_data = train_test_split(data, test_size=0.1, random_state=99)\n\ntrain_dataset = BattleDataset(train_data)\nvalid_dataset = BattleDataset(valid_data)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, num_workers=2, shuffle=True) \nvalid_loader = DataLoader(valid_dataset, batch_size=8, num_workers=2) ","17e60ecf":"# probemos que los loaders devuelvan los datos que esperamos...\nfirst, second, result = next(iter(train_loader))\nfirst, second, result","ffeeea40":"class EmbeddingModel(nn.Module):\n    def __init__(self, embedding_size, dropout):\n        super(EmbeddingModel, self).__init__()\n        \n        self.embedding = nn.Embedding(800, embedding_size) #800 es el numero de pokemon...\n        \n        self.out = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(embedding_size*2, 1),\n            nn.Sigmoid()\n        )\n                \n    def forward(self, first, second):\n        first_embedding = self.embedding(first)\n        second_embedding = self.embedding(second)\n        combined = torch.cat((first_embedding, second_embedding), dim=-1)\n        return self.out(combined)","e8336579":"# usamos un embedding size de 512 y un dropout de 0.1 porque me da a mi la gana, y punto\nmodel = EmbeddingModel(embedding_size=512, dropout=0.1)","3ba75339":"pred = model.forward(first, second)\npred.shape","f6d7d297":"best_loss = 9999999 # usaremos esto para solo ir guardando los mejores modelos","524e0a2e":"optimizer = optim.Adam(model.parameters(), lr=0.001) \ncriterion = nn.MSELoss() # lo l\u00f3gico ser\u00eda usar BCELoss pero MSELoss aprende m\u00e1s rapido yoquese :\/ ...\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)","a502ae83":"epochs = 20\n\nmodel.cuda()\nfor e in range(1, epochs+1):\n    batch = 0\n    total_loss = 0\n    model.train()\n    scheduler.step()\n    for x1, x2, y in train_loader:\n        batch += 1\n        y = y.unsqueeze(-1).cuda()\n        x1, x2 = x1.cuda(), x2.cuda()\n        optimizer.zero_grad()\n        \n        pred = model.forward(x1, x2)    \n        loss = criterion(pred, y)\n                \n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        \n        if batch % 1000 == 0:\n            print(f\"Epoch {e} ({batch}\/{len(train_loader)}): loss {total_loss \/ batch}\")\n            \n    model.eval()\n    valid_loss = 0\n    valid_accuracy = 0\n    with torch.no_grad():\n        for x1, x2, y in valid_loader:\n            y = y.unsqueeze(-1).cuda()\n            x1, x2 = x1.cuda(), x2.cuda()\n            \n            pred = model.forward(x1, x2)\n            loss = criterion(pred, y)\n            \n            valid_loss += loss.item()\n            \n            correct = (pred >= 0.5).long() == y.long()\n            accuracy = correct.float().mean()\n        \n            valid_accuracy += accuracy           \n            \n    print(f\"Epoch {e} ({len(train_loader)}\/{len(train_loader)}): loss {total_loss \/ len(train_loader)} - valid_loss {valid_loss \/ len(valid_loader)} - valid_acc {valid_accuracy \/ len(valid_loader)}\")\n    \n    if valid_loss < best_loss:\n        best_loss = valid_loss\n        torch.save(model.state_dict(), 'model.pt')\n        print(\"model SAVED\")\n","d1f144e1":"#\u00a0nos aseguramos de cargar el modelo que ha obtenido menor valid_loss\nmodel.load_state_dict(torch.load('model.pt'))","a39d23d0":"test_df = pd.read_csv('..\/input\/test.csv')\ntest_data = test_df.values\n\nout = []\nmodel.eval()\nwith torch.no_grad():\n    for battle, first, second in test_data:\n        x1 = torch.tensor(first-1).long().unsqueeze(0).cuda()\n        x2 = torch.tensor(second-1).long().unsqueeze(0).cuda()\n        pred = model.forward(x1, x2).item()\n        \n        out.append([battle, int(pred >= 0.5)])\n    \nout = np.array(out)\n\nout[:10]\n\nsubmission = pd.DataFrame({'battle_number':out[:,0],'Winner':out[:,1]})\nsubmission.to_csv('submission.csv', columns=['battle_number', 'Winner'], index=False)","d348c3b1":"Hacemos un train\/validation split de 10% para el validation lo que nos deja 90% para el training.  \nA continuaci\u00f3n creamos nuestros loaders con batch size de 8, de nuevo, nada raro...","66e56b84":"# PokEmbeddings\n\nVale, pues yo voy a ser el raro que proponga una soluci\u00f3n fuera de lo com\u00fan: Usar PokEmbeddings\nYa os aviso de que el modelo NO es el mejor, pero funciona soprendentemente bien teniendo en cuenta que no realizo absolutamente NADA de feature engineering, de hecho, no utilizo los datos de los pokemon para absolutamente nada.\n\n## Y entonces c\u00f3mo funciona?\nEl sistema aprender\u00e1 a gerar un vector \u00fanico para cada Pok\u00e9mon, este vector (donde reside la magia) lo entrenaremos sencillamente con las parejas de Pok\u00e9mon para que aprenda a inventarse qu\u00e9 caracter\u00edsticas se asocian a que un Pok\u00e9mon gane a otro. Sin querer, puede que este vector est\u00e9 aprendiendo cosas parecidas al tipo, la fuerza, u otras combinaciones de par\u00e1metros que definen cuando un Pok\u00e9mon ser\u00e1 el ganador","4378d5e3":"##\u00a0Y qu\u00e9 mas... ?\n\nA los amantes del deep learning, os animo a que intent\u00e9is salir de las soluciones clasicas e intent\u00e9is cosas como esto de no usar feature, sino dejar que el modelo las aprenda!!\n\nCon suerte, alguno de vosotros con un poco m\u00e1s de visi\u00f3n que yo da con una soluci\u00f3n capaz de funcionar tan bien como las soluciones t\u00edpicas usando random forest y similares, pero sin usar ni una sola feature, demostrando la supremac\u00eda absoluta del deep learning \ud83d\ude05\n\n## Por qu\u00e9 el modelo no funciona mejor?\nPues no estoy del todo seguro, pero un detalle importante por el que esta soluci\u00f3n tal cual est\u00e1 aqui nunca funcionar\u00e1 mejor que usar features, es que al parecer hay Pok\u00e9mon que **no aparecen nunca en el training set**, pero SI est\u00e1n presentes en los tests, por lo que este modelo es incapaz de aprender a generar embeddings para esos Pok\u00e9mon. Minipunto para el que se le ocurra una forma de generar embeddings para estos pok\u00e9mon que no aparecen en el training set :D","347954e0":"Y pues ya est\u00e1, obtenemos un 93% o as\u00ed de validation accuracy que luego en el test se queda en un 92% o as\u00ed.\n\nNo es ni ser\u00e1 el mejor modelo para este problema, pero funciona la mar de bien teniendo en cuenta que no est\u00e1 usando absolutamente ning\u00fan par\u00e1metro de los pokemon salvo el n\u00famero, yo lo considerar\u00eda un pedazo de logro :P (y me quedo tan ancho)","7b0ce0e2":"Solo necesitamos cargar el CSV de las batallas \u00af\\\\_(\u30c4)_\/\u00af","400103e2":"##\u00a0El Modelo\nAhora vamos a definir nuestro modelo! Vamos a hacerlo muy sencillo, centrandonos \u00fanicamente en lo relevante: los embeddings!\n\nUsaremos una \u00fanica capa de embeddings cuyos pesos se compartir\u00e1n para ambos Pok\u00e9mon de la batalla, concatenaremos los embeddigs de ambos pokemon y los pasaremos por una \u00fanica capa lineal para generar el output (nos comemos el **deep** de **deep neural networks**)","c047d0b1":"A continuaci\u00f3n crearemos nuestro generador, esto nos ayudar\u00e1 a cargar los datos de las batallas en batches de forma sencilla y a convertir los datos a tensores de PyTorch para poder usarlos, nada importante por aqu\u00ed\n\nPD: restamos -1 a los indices de los pokemon para que empiece a contar en 0 en vez de en 1"}}