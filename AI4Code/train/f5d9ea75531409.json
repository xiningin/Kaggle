{"cell_type":{"8a735be8":"code","56a9f49d":"code","084e7a42":"code","e7c66028":"code","caae64e9":"code","b4c5ae3e":"code","b09c7f3d":"code","b87d0747":"code","1a6718f5":"code","17ab2683":"code","2d50eb45":"markdown","b1c40364":"markdown","ef08abfe":"markdown","128da9d1":"markdown","1f17f7b8":"markdown","ad3233c2":"markdown","6308e10b":"markdown","c39647ad":"markdown","32e23d20":"markdown","92c0ca7e":"markdown","4d104fd8":"markdown"},"source":{"8a735be8":"# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings('ignore')","56a9f49d":"dataset= pd.read_csv(\"..\/input\/train.csv\")","084e7a42":"%matplotlib inline\nimport seaborn\nseaborn.set() \n\n#-------------------Survived\/Died by Class -------------------------------------\nsurvived_class = dataset[dataset['Survived']==1]['Pclass'].value_counts()\ndead_class = dataset[dataset['Survived']==0]['Pclass'].value_counts()\ndf_class = pd.DataFrame([survived_class,dead_class])\ndf_class.index = ['Survived','Died']\ndf_class.plot(kind='bar',stacked=True, figsize=(5,3), title=\"Survived\/Died by Class\")\n\nClass1_survived= df_class.iloc[0,0]\/df_class.iloc[:,0].sum()*100\nClass2_survived = df_class.iloc[0,1]\/df_class.iloc[:,1].sum()*100\nClass3_survived = df_class.iloc[0,2]\/df_class.iloc[:,2].sum()*100\nprint(\"Percentage of Class 1 that survived:\" ,round(Class1_survived),\"%\")\nprint(\"Percentage of Class 2 that survived:\" ,round(Class2_survived), \"%\")\nprint(\"Percentage of Class 3 that survived:\" ,round(Class3_survived), \"%\")\n\n# display table\nfrom IPython.display import display\ndisplay(df_class)","e7c66028":"#-------------------Survived\/Died by SEX------------------------------------\n   \nSurvived = dataset[dataset.Survived == 1]['Sex'].value_counts()\nDied = dataset[dataset.Survived == 0]['Sex'].value_counts()\ndf_sex = pd.DataFrame([Survived , Died])\ndf_sex.index = ['Survived','Died']\ndf_sex.plot(kind='bar',stacked=True, figsize=(5,3), title=\"Survived\/Died by Sex\")\n\n\nfemale_survived= df_sex.female[0]\/df_sex.female.sum()*100\nmale_survived = df_sex.male[0]\/df_sex.male.sum()*100\nprint(\"Percentage of female that survived:\" ,round(female_survived), \"%\")\nprint(\"Percentage of male that survived:\" ,round(male_survived), \"%\")\n\n# display table\nfrom IPython.display import display\ndisplay(df_sex) ","caae64e9":"#-------------------- Survived\/Died by Embarked ----------------------------\n\nsurvived_embark = dataset[dataset['Survived']==1]['Embarked'].value_counts()\ndead_embark = dataset[dataset['Survived']==0]['Embarked'].value_counts()\ndf_embark = pd.DataFrame([survived_embark,dead_embark])\ndf_embark.index = ['Survived','Died']\ndf_embark.plot(kind='bar',stacked=True, figsize=(5,3))\n\nEmbark_S= df_embark.iloc[0,0]\/df_embark.iloc[:,0].sum()*100\nEmbark_C = df_embark.iloc[0,1]\/df_embark.iloc[:,1].sum()*100\nEmbark_Q = df_embark.iloc[0,2]\/df_embark.iloc[:,2].sum()*100\nprint(\"Percentage of Embark S that survived:\", round(Embark_S), \"%\")\nprint(\"Percentage of Embark C that survived:\" ,round(Embark_C), \"%\")\nprint(\"Percentage of Embark Q that survived:\" ,round(Embark_Q), \"%\")\n\nfrom IPython.display import display\ndisplay(df_embark)","b4c5ae3e":"X = dataset.drop(['PassengerId','Cabin','Ticket','Fare', 'Parch', 'SibSp'], axis=1)\ny = X.Survived                       # vector of labels (dependent variable)\nX=X.drop(['Survived'], axis=1)       # remove the dependent variable from the dataframe X\n\nX.head(20)","b09c7f3d":"# ----------------- Encoding categorical data -------------------------\n\n# encode \"Sex\"\nfrom sklearn.preprocessing import LabelEncoder\nlabelEncoder_X = LabelEncoder()\nX.Sex=labelEncoder_X.fit_transform(X.Sex)\n\n\n# encode \"Embarked\"\n\n# number of null values in embarked:\nprint ('Number of null values in Embarked:', sum(X.Embarked.isnull()))\n\n# fill the two values with one of the options (S, C or Q)\nrow_index = X.Embarked.isnull()\nX.loc[row_index,'Embarked']='S' \n\nEmbarked  = pd.get_dummies(  X.Embarked , prefix='Embarked'  )\nX = X.drop(['Embarked'], axis=1)\nX= pd.concat([X, Embarked], axis=1)  \n# we should drop one of the columns\nX = X.drop(['Embarked_S'], axis=1)\n\nX.head()","b87d0747":"#-------------- Taking care of missing data  -----------------------------\n\nprint ('Number of null values in Age:', sum(X.Age.isnull()))\n \n\n# -------- Change Name -> Title ----------------------------\ngot= dataset.Name.str.split(',').str[1]\nX.iloc[:,1]=pd.DataFrame(got).Name.str.split('\\s+').str[1]\n# ---------------------------------------------------------- \n\n\n#------------------ Average Age per title -------------------------------------------------------------\nax = plt.subplot()\nax.set_ylabel('Average age')\nX.groupby('Name').mean()['Age'].plot(kind='bar',figsize=(13,8), ax = ax)\n\ntitle_mean_age=[]\ntitle_mean_age.append(list(set(X.Name)))  #set for unique values of the title, and transform into list\ntitle_mean_age.append(X.groupby('Name').Age.mean())\ntitle_mean_age\n#------------------------------------------------------------------------------------------------------\n\n\n#------------------ Fill the missing Ages ---------------------------\nn_traning= dataset.shape[0]   #number of rows\nn_titles= len(title_mean_age[1])\nfor i in range(0, n_traning):\n    if np.isnan(X.Age[i])==True:\n        for j in range(0, n_titles):\n            if X.Name[i] == title_mean_age[0][j]:\n                X.Age[i] = title_mean_age[1][j]\n#--------------------------------------------------------------------    \n\nX=X.drop(['Name'], axis=1)\n\n       ","1a6718f5":"for i in range(0, n_traning):\n    if X.Age[i] > 18:\n        X.Age[i]= 0\n    else:\n        X.Age[i]= 1\n\nX.head()","17ab2683":"#-----------------------Logistic Regression---------------------------------------------\n# Fitting Logistic Regression to the Training set\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(penalty='l2',random_state = 0)\n\n# Applying k-Fold Cross Validation\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X=X , y=y , cv = 10)\nprint(\"Logistic Regression:\\n Accuracy:\", accuracies.mean(), \"+\/-\", accuracies.std(),\"\\n\")\n\n\n\n#-----------------------------------K-NN --------------------------------------------------\n\n# Fitting K-NN to the Training set\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors = 9, metric = 'minkowski', p = 2)\n\n\n# Applying k-Fold Cross Validation\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X=X , y=y , cv = 10)\nprint(\"K-NN:\\n Accuracy:\", accuracies.mean(), \"+\/-\", accuracies.std(),\"\\n\")\n\n\n#---------------------------------------SVM -------------------------------------------------\n\n# Fitting Kernel SVM to the Training set\nfrom sklearn.svm import SVC\nclassifier = SVC(kernel = 'rbf', random_state = 0)\n\n# Applying k-Fold Cross Validation\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X=X , y=y , cv = 10)\nprint(\"SVM:\\n Accuracy:\", accuracies.mean(), \"+\/-\", accuracies.std(),\"\\n\")\n\n\n#---------------------------------Naive Bayes-------------------------------------------\n\n# Fitting Naive Bayes to the Training set\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\n\n# Applying k-Fold Cross Validation\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X=X , y=y , cv = 10)\nprint(\"Naive Bayes:\\n Accuracy:\", accuracies.mean(), \"+\/-\", accuracies.std(),\"\\n\")\n\n\n\n#----------------------------Random Forest------------------------------------------\n\n# Fitting Random Forest Classification to the Training set\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 0)\n\n# Applying k-Fold Cross Validation\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X=X , y=y , cv = 10)\nprint(\"Random Forest:\\n Accuracy:\", accuracies.mean(), \"+\/-\", accuracies.std())\n","2d50eb45":"Now let's import the csv file with the training dataset. You can download it from [here](https:\/\/www.kaggle.com\/c\/titanic\/data).  The explanation of the features (each column from the dataset) is also presented in this link. ","b1c40364":"You may wonder why are we still keeping the **\"Name\"** column. In fact the name does not seem to have influence, it does not matter if a person is named Owen or William, however this column has the title located after the Surname and the comma (\"Mr\", \"Mrs\", \"Miss\", etc.) which can be useful.  \n\nIf we take a look at the table X displayed previously we can see many missing values for the **\"Age\"** column. Removing these rows with missing values would involve removing 177 rows (which is quite a lot!) and we would have less information to create the model. In some cases, it is acceptable to take the average of the column and replace the null values, nonetheless in this case, it is possible to estimate the age of the person by their title, present in the **\"Name\"** column.   \n\nTherefore, I will first identify the different titles presented and then average the Age for each title. We can provide this averaged Age found for each title to the people with missing Age values, accordingly to their title in **\"Name\"**. \n\nAfter using the information in **\"Name\"** we can drop this column. ","ef08abfe":"We can also make feature transformation. For example, we could transform the **\"Age\"** feature in order to simplify it. We could distinguish the youngsters (age less than 18 years) from the adults.  \n\n","128da9d1":"We can see, from this displayed DataFrame, that **\"Sex\"** and **\"Embarked\"** are categorical features and have strings instead of numeric values. We need to encode these strings into numeric data, so the algorithm can perform its calculations. \n\nFor the **\"Sex\"** feature we can use the **LabelEncoder** class from  **sklearn.preprocessing** library. \n\nAnother way of doing this is by using the **get_dummies** from **pandas**. We will be using this to encode the **\"Embarked\"** feature. But first, as **\"Embarked\"** has two NaN values we need to take care of these missing values. In this approach, I will provide the 'S' category because it is the most frequent in the data. After this, it is then possible to use the **get_dummies** and get three new columns (Embarked_C,\tEmbarked_Q, Embarked_S) which are called dummy variables (they assign \u20180\u2019 and \u20181\u2019 to indicate membership in a category). The previous **\"Embarked\"** can be dropped from X as it will not be needed anymore and we can now concatenate the X dataframe with the new **\"Embarked\"** which has the three dummy variables. Finally, as the number of dummy variables necessary to represent a single feature is equal to the number of categories in that feature minus one, we can remove one of the dummies created, lets say Embarked_S, for example. This will not remove any information because by having the values from Embarked_C and\tEmbarked_Q the algorithm can easily understand the values from the remaining dummy variable (when Embarked_C and Embarked_Q are '0' Embarked_S will be '1', otherwise it will be '0').  ","1f17f7b8":"As we can see, from all the 5 classifiers tested in this tutorial, **Random Forest** got better results. \n\nAfter changing the test set by performing the same transformations done in the training set we can then use the **Random Forest** model created and do the predictions. The submission of these predictions was scored 0.77990 in Kaggle.  \n\nHope this tutorial was useful in some way. For a more detailed tutorial in the Titanic challenge I recommend this [tutorial](http:\/\/ahmedbesbes.com\/how-to-score-08134-in-titanic-kaggle-challenge.html).  ","ad3233c2":"Having the data preprocessed we can now provide the data to different classifiers and see which one performs better in creating a model of classification for this data. \n\nWe will use cross validation, which is a model validation technique to evaluate how well a model will generalize to an independent data set. Python has the **cross_val_score** class from **sklearn.model_selection** library to perform cross validation. ","6308e10b":"First let's start by importing the essential libraries to work with dataframes (**pandas**), numeric values (**numpy**) and visualization (**matplotlib.pyplot**).","c39647ad":"## 2. Data cleaning, Feature selection and Feature engineering\nThe preprocessing of the data is a quite crucial part. If we just give the dataset without cleaning it, most probably the results will not be good! So, in this step we will preprocess the training dataset and this will involve feature selection, data cleaning, and feature engineering.   \n\nI will start with feature selection. As we saw previously, **\"P-Class\", \"Sex\", \"Age\"** and **\"Embarked\"** showed some relation with Survived rate. Thus, I will drop the remaining features, except **\"Name\"** because it will be useful in a further step of the cleaning process. ","32e23d20":"## 1. Data exploration and visualization  \n\nFor a good start, we should look at the dataset. Analyze the features and think which could be useful to predict the survival rate. The features that probably may have an influence are: the **\"P-class\"** (expect to see more survival for higher class), the **\"Sex\"** and **\"Age\"** (\"women and children first\"), and let's say **\"Embarked\"** also. \n\nWe will now plot some graphs to confirm if these features show some relation with the survival rate. These plots were based in the graphs presented [here](http:\/\/ahmedbesbes.com\/how-to-score-08134-in-titanic-kaggle-challenge.html).  ","92c0ca7e":"# Titanic Competition from Kaggle\n\nThe \"Titanic: Machine Learning from Disaster\" is a good competition to get started with ML hands-on. So, for beginners in ML I highly recommend it a try.\n\nI created this code using python to predict the survival labels for the test set in this competition. The highest score I got was 0.77990 for the accuracy of the model. In the following paragraphs I will present the steps I went through to get this score.\n\n**Note:** Keep in mind that this tutorial is just as a simple starting point and will be useful for beginners. Many more explorations and optimizations could be done. If you have any comments about this tutorial please let me know. \n\nIn this tutorial I will present basic steps of a data science pipeline:\n\n#### 1. Data exploration and visualization  \n   - Explore dataset\n   - Choose important features and visualize them according to survival\/non-survival\n   \n#### 2. Data cleaning, Feature selection and Feature engineering\n   - Null values\n   - Encode categorical data\n   - Transform features\n   \n#### 3. Test different classifiers \n   - Logistic Regression (LR)\n   - K-NN\n   - Support Vector Machines (SVM)\n   - Naive Bayes\n   - Random Forest (RF)\n \n ","4d104fd8":"Now, we can say that we have a quite well clean dataset to provide to our classifier algorithm. \n\n\n## 3. Test different classifiers\n"}}