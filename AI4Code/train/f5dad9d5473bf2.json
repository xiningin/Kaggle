{"cell_type":{"adeec437":"code","a51c283a":"code","10f75a99":"code","34cd0280":"code","80c8f630":"code","48312bab":"code","a3c7f2ab":"code","1ddd92fc":"code","f8d190aa":"code","3a09f3db":"code","b16d8417":"code","d966e973":"code","2602239f":"code","7111b1f0":"code","6ed887e2":"code","e5acbec6":"code","44670136":"code","6debe10a":"code","26fd33cb":"code","88bb31e9":"code","d0b9ea85":"code","2e21d472":"code","b5f2d3dc":"code","0360e286":"code","08305081":"code","98112b5c":"code","dd2447df":"code","c45d3fa2":"code","fd99aa0f":"code","9c4b915c":"code","c557f42d":"code","cc087fd8":"markdown"},"source":{"adeec437":"import numpy as np \nimport pandas as pd\nimport os\n\nimport keras\nfrom keras.models import Sequential, Input, Model\nfrom keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout, TimeDistributed, Flatten\nfrom keras.preprocessing.sequence import pad_sequences\n\nprint(os.listdir(\"..\/input\"))","a51c283a":"dframe = pd.read_csv(\"..\/input\/ner.csv\", encoding = \"ISO-8859-1\", error_bad_lines=False)\ndframe.head()","10f75a99":"dataset=dframe.drop(['Unnamed: 0', 'lemma', 'next-lemma', 'next-next-lemma', 'next-next-pos',\n       'next-next-shape', 'next-next-word', 'next-pos', 'next-shape',\n       'next-word', 'prev-iob', 'prev-lemma', 'prev-pos',\n       'prev-prev-iob', 'prev-prev-lemma', 'prev-prev-pos', 'prev-prev-shape',\n       'prev-prev-word', 'prev-shape', 'prev-word','shape'],axis=1)","34cd0280":"dataset.head(10)","80c8f630":"class SentenceGetter(object):\n    \n    def __init__(self, dataset):\n        self.n_sent = 1\n        self.dataset = dataset\n        self.empty = False\n        agg_func = lambda s: [(w, t) for w,t in zip(s[\"word\"].values.tolist(), s[\"tag\"].values.tolist())]\n        self.grouped = self.dataset.groupby(\"sentence_idx\").apply(agg_func)\n        self.sentences = [s for s in self.grouped]\n    \n    def get_next(self):\n        try:\n            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n            self.n_sent += 1\n            return s\n        except:\n            return None","48312bab":"getter = SentenceGetter(dataset)\nsentences = getter.sentences\nsentences[1]","a3c7f2ab":"sentences = [[s[0].lower() for s in sent] for sent in getter.sentences]\nsentences[1]","1ddd92fc":"labels = [[s[1] for s in sent] for sent in getter.sentences]\nprint(labels[1])","f8d190aa":"maxlen = max([len(s) for s in sentences])\nprint ('Maximum sequence length:', maxlen)","3a09f3db":"# Check how long sentences are so that we can pad them\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use(\"ggplot\")","b16d8417":"plt.hist([len(s) for s in sentences], bins=50)\nplt.show()","d966e973":"words = np.array([x.lower() if isinstance(x, str) else x for x in dataset[\"word\"].values])\nwords = list(set(words))\nwords.append('unk')\nwords.append('pad')\nn_words = len(words); n_words","2602239f":"tags = list(set(dataset[\"tag\"].values))\nn_tags = len(tags)\nn_tags","7111b1f0":"word2idx = {w: i for i, w in enumerate(words)}\ntag2idx = {t: i for i, t in enumerate(tags)}","6ed887e2":"tag2idx","e5acbec6":"word2idx['unk']","44670136":"X = [[word2idx.get(w,'27420') for w in s] for s in sentences]\nX[1]","6debe10a":"y = [[tag2idx.get(l) for l in lab] for lab in labels]\ny[1]","26fd33cb":"X = pad_sequences(maxlen=140, sequences=X, padding=\"post\", value=n_words-1)","88bb31e9":"word2idx['pad'], n_words-1","d0b9ea85":"y = pad_sequences(maxlen=140, sequences=y, padding=\"post\", value=tag2idx[\"O\"])","2e21d472":"y[1]","b5f2d3dc":"X[1]","0360e286":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)","08305081":"y_train = keras.utils.to_categorical(y_train)\nprint(X_train.shape, y_train.shape)","98112b5c":"model = Sequential()\n\nmodel.add(Embedding(n_words, 50))\nmodel.add(Bidirectional(LSTM(140, return_sequences=True)))\nmodel.add(Bidirectional(LSTM(140, return_sequences=True)))\nmodel.add(TimeDistributed(Dense(n_tags, activation=\"softmax\")))\nmodel.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\nmodel.fit(X_train, y_train, batch_size=150, epochs=5, verbose=1, validation_split=0.2)","dd2447df":"pred = model.predict(X_test)   ","c45d3fa2":" pred","fd99aa0f":"# input = Input(shape=(140,))\n# model = Embedding(n_words, 50)(input)\n# model = Bidirectional(LSTM(units=140, return_sequences=True))(model)\n# model = Bidirectional(LSTM(units=140, return_sequences=True))(model)\n# # reshape = keras.layers.Reshape((-1, 140, 1))(model)\n\n# out = TimeDistributed(Dense(18, activation=\"softmax\"))(model)  # softmax output layer\n# model = Model(input, out)\n\n# model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\n# model.fit(X_train, y_train, batch_size=150, epochs=1, verbose=1)","9c4b915c":"model.summary()","c557f42d":"i = 5\np = model.predict(np.array([X_test[i]]))\np = np.argmax(p, axis=-1)\nprint(\"{:14} ({:5}): {}\".format(\"Word\", \"True\", \"Pred\"))\nfor w,pred in zip(X_test[i],p[0]):\n    print(\"{:14}: {}\".format(words[w],tags[pred]))","cc087fd8":"**Functional API**"}}