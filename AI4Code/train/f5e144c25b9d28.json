{"cell_type":{"2f13a360":"code","478cab3d":"code","4d72d9a7":"code","6e4af88f":"code","505c1249":"code","13a31e31":"code","dcd98e42":"code","178b156b":"code","b03a6ae5":"code","12005dd3":"code","fb4e5fa3":"code","a7c38f81":"code","952047fc":"code","43ff8cb4":"code","8ee5abde":"code","ca4a143d":"code","b2adfbec":"code","65ab0ea9":"code","5ab085e8":"code","fa8c178f":"code","cfefc383":"code","47e3efa9":"code","47f6c855":"code","d4cdaeef":"code","94918636":"code","497c93dc":"code","c034b948":"code","4e02e67e":"code","6108c660":"code","2c9f9813":"code","b7080ddc":"code","82a3caa8":"code","3135c495":"code","31733f27":"code","30f273d7":"code","f925c315":"markdown","79172d0e":"markdown","45f47136":"markdown","6d66a70c":"markdown","c8de721a":"markdown","2ffb248f":"markdown","922b3753":"markdown","66561cf1":"markdown","a5d38c27":"markdown","85bda5f6":"markdown","98ea604d":"markdown","d357f697":"markdown","43dbd4ad":"markdown","cebe43f0":"markdown","c1cd769c":"markdown","539cdc1e":"markdown","10415721":"markdown","b467c1a3":"markdown","a7eff6d3":"markdown","57452914":"markdown","4c1732d1":"markdown","ab30df70":"markdown","56c7260a":"markdown","52733aff":"markdown","a6720c46":"markdown","d6d61228":"markdown","e0a59229":"markdown","3d20b4e9":"markdown","2829e400":"markdown","364335c1":"markdown","3e8036b1":"markdown","e93cfaf2":"markdown","55ab96a0":"markdown","03efa17a":"markdown","0db5e0a5":"markdown","2f9af133":"markdown","9038334e":"markdown","669f7d31":"markdown","1d89d466":"markdown","37716b45":"markdown","4e64db52":"markdown","4fe13290":"markdown","7f4cb16e":"markdown","8dc55107":"markdown","02fd1f42":"markdown","f28298db":"markdown","a5a88758":"markdown"},"source":{"2f13a360":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# import plot libraries\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# import stats to analyze and fit histograms\nfrom scipy import stats \n\n# import ML libraries\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\n\n","478cab3d":"# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nprint('importing:\\n')\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","4d72d9a7":"# open raw data\ntrain_data_full = pd.read_csv('..\/input\/house-prices-dataset\/train.csv')\ntest_data_full = pd.read_csv('..\/input\/house-prices-dataset\/test.csv')\n\ntrain_data_full.tail()","6e4af88f":"# target to model is the 'SalePrice' column\n\n# Remove rows with missing target, separate target from predictors\ntrain_data_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = train_data_full.SalePrice # new df with target\n\n# remove target from features dataframe\ntrain_data = train_data_full.drop(['SalePrice'], axis=1)","505c1249":"fig = sns.distplot(train_data_full['SalePrice'])","13a31e31":"train_data_full['SalePrice'].describe()","dcd98e42":"train_data_full.corr()['SalePrice'].sort_values(ascending=False).head(15)","178b156b":"# Select categorical columns\ncategorical_cols = {cname for cname in train_data.columns if train_data[cname].dtype == \"object\"}\n\n# Select numerical columns\nnumerical_cols = {cname for cname in train_data.columns if train_data[cname].dtype in ['int64', 'float64']}","b03a6ae5":"# check columns with missing data\nmissing_data_cols = set(train_data.columns[train_data.isna().any()].tolist())\n\n# display the fraction of missing data\ndf_pct_missing = pd.DataFrame((len(y) - train_data[missing_data_cols].count())\/len(y))*100\ndf_pct_missing.columns = ['Missing data [%]']\ndf_pct_missing.sort_values('Missing data [%]')","12005dd3":"# drop columns that have less than 25% values that are NAN\n\ncolumns_to_drop = train_data.count()[train_data.count()<0.75*max(train_data.count())].index.tolist()\ncolumns_to_drop = set(columns_to_drop) # convert to set to avoid multiple instances\nprint(\"We drop the following columns because more than 25% of the entries are missing: \\n\",columns_to_drop)","fb4e5fa3":"# Visualize all numerical features\nn=len(train_data[numerical_cols].columns) # number of plots\nf, axes = plt.subplots(nrows=(n-1)\/\/4 +1,ncols=4,squeeze=False,figsize=(18,4*((n-1)\/\/4 +1))) # represent them on 4 columnms\nf.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.4, hspace=0.4) # increase space between plots\n\nfor col, ax in zip(numerical_cols, axes.flatten()[:n]):\n    sns.regplot(data=train_data_full,x=col,y='SalePrice', ax=ax)\n\nplt.show()","a7c38f81":"# show numerical columns that have missing data but are not in dropped columns list\ncols = (missing_data_cols & numerical_cols) - columns_to_drop\ntrain_data[cols].head()","952047fc":"# let's have a look at the distribution of the columns that have missing data in order to make the best inference\n\n# Visualize all features\nn=len(train_data[cols].columns) # number of plots\nf, axes = plt.subplots((n-1)\/\/3 +1,3, figsize=(18,6*((n-1)\/\/3 +1))) # represent them on 3 columnms\n\nfor col, ax in zip(cols, axes.flatten()[:n]):\n    #sns.countplot(x=col, data=train_data_filtered, ax=ax)\n    sns.distplot(a=train_data[col][(train_data[col].notnull())], ax=ax,fit=stats.norm)\n\nplt.show()","43ff8cb4":"# Visualize all categorical features\nn=len(train_data[categorical_cols].columns) # number of plots\nf, axes = plt.subplots(nrows=(n-1)\/\/4 +1,ncols=4,squeeze=False,figsize=(18,4*((n-1)\/\/4 +1))) # represent them on 4 columnms\nf.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.4, hspace=0.4) # increase space between plots\n\nfor col, ax in zip(categorical_cols, axes.flatten()[:n]):\n    sns.boxplot(data=train_data_full,x=col,y='SalePrice', ax=ax)\n\nplt.show()","8ee5abde":"# Columns that will be one-hot encoded\nlow_cardinality_cols = set([col for col in categorical_cols if train_data[col].nunique() <= 5])\n\n# Columns that will be dropped from the dataset\nhigh_cardinality_cols = categorical_cols - low_cardinality_cols\n\nprint('Categorical columns that will be one-hot encoded: \\n', low_cardinality_cols)\nprint('\\nCategorical columns that will be dropped from the dataset: \\n', high_cardinality_cols)","ca4a143d":"# show low cardinality categorical columns that have missing data\ncols = (missing_data_cols & low_cardinality_cols) \n\n# Visualize all features\nn=len(train_data[cols].columns) # number of plots\n\nf, axes = plt.subplots(nrows=(n-1)\/\/4 +1,ncols=4,squeeze=False,figsize=(18,4*((n-1)\/\/4 +1))) # represent them on 4 columnms\nf.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.4, hspace=0.4) # increase space between plots\n\nfor col, ax in zip(cols, axes.flatten()[:n]):\n    sns.countplot(x=col, data=train_data, ax=ax)\n\nplt.show()","b2adfbec":"col_val = 'Electrical'\nval_list = train_data_full[col_val].unique()\n\nprint(\"Price variations for the feature %s: \\n\"%col_val)\ndf = train_data_full.groupby(col_val).mean()['SalePrice']\nprint(df.sort_values())","65ab0ea9":"sns.boxplot(data=train_data_full,x=col_val,y='SalePrice')","5ab085e8":"# Function to control the columns to drop \n\ndef drop_column(dataset, min_instances_per_feature = 0.75, max_cardinality = 5):\n    \"\"\" \n    Select which columns have to be dropped from dataset\n    \n    inputs:\n    dataset = dataframe \n    min_instances_per_feature = minimum fraction of data per features to keep it in data set\n    max_cardinality = highest cardinality accepted for categorical columns\n    \n    outputs:\n    columns_to_drop = set of columns to drop\n    fitered_dataset = dataset without colmuns\n    \"\"\"\n    \n    missing_data_cols = set(dataset.columns[dataset.isna().any()].tolist())\n    \n    # 1) drop features that have too many missing values\n    columns_to_drop = set(dataset.count()[dataset.count() < min_instances_per_feature*max(dataset.count())].index.tolist())\n\n    # 2) drop categorical features with high cardinality\n    low_cardinality_cols = set([col for col in categorical_cols if train_data[col].nunique() <= max_cardinality])\n    high_cardinality_cols = categorical_cols - low_cardinality_cols\n    columns_to_drop = columns_to_drop | high_cardinality_cols \n\n    # 3) drop categorical features with missing data\n    missing_data_cat_cols = (missing_data_cols & categorical_cols) \n    columns_to_drop = columns_to_drop | missing_data_cat_cols\n    \n    fitered_dataset = dataset.drop(columns_to_drop,axis=1)\n    \n    return [columns_to_drop, fitered_dataset]","fa8c178f":"[columns_to_drop,fitered_train_data] = drop_column(train_data, min_instances_per_feature = 0.75, max_cardinality = 5)\nprint(columns_to_drop)","cfefc383":"# Keep selected columns only\nmy_cols = list((categorical_cols | numerical_cols) - columns_to_drop)","47e3efa9":"def preprocess_data(data):\n    # preprocess DataFrame 'data'\n    # Drop irrelevant columns from DataFrame before passing it to this function\n    \n    # Select categorical and numerical columns from the datafrome 'data'\n    cat_cols = {cname for cname in data.columns if data[cname].dtype == \"object\"}\n    num_cols = {cname for cname in data.columns if data[cname].dtype in ['int64', 'float64']}\n    \n    \n    # Preprocessing for numerical data\n    numerical_transformer = SimpleImputer(strategy='most_frequent')\n\n    # Preprocessing for categorical data\n    categorical_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='most_frequent')),\n        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n    \n\n    # Bundle preprocessing for numerical and categorical data\n    # Let's put remainder='passthrough' so that if more columns are present than those specified in the transformers, we keep them in data set.\n    # This might lead to errors but such errors will force us to have a closer look at the data\n    preprocessor = ColumnTransformer(\n    transformers=[('num', numerical_transformer, list(num_cols)),\n        ('cat', categorical_transformer, list(cat_cols))],\n                                  remainder='passthrough') \n    \n    return preprocessor","47f6c855":"def MAE_score_model(X,y,model):\n    # Compute the MAE by train-test split on the features X and target y (80-20 split)\n    # model can be chosen for comparison\n    \n    preprocessor = preprocess_data(X)\n\n    # Bundle preprocessing and modeling code in a pipeline\n    my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', model)\n                     ],verbose=False)\n    \n    # Break off validation set from training data\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,random_state=0)\n    \n    \n    # Preprocessing of training data, fit model \n    my_pipeline.fit(X_train, y_train)\n\n    # Preprocessing of validation data, get predictions\n    preds = my_pipeline.predict(X_valid)\n    \n    return mean_absolute_error(y_valid, preds)","d4cdaeef":"def plot_predict_error(X,y,model):\n    # Make a plot of the prediction error on the validation  data from a train-test split\n    \n    preprocessor = preprocess_data(X)\n\n    # Bundle preprocessing and modeling code in a pipeline\n    my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', model)\n                     ],verbose=False)\n    \n    # Break off validation set from training data\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,random_state=0)\n    \n    \n    # Preprocessing of training data, fit model \n    my_pipeline.fit(X_train, y_train)\n\n    # Preprocessing of validation data, get predictions\n    preds = my_pipeline.predict(X_valid)\n    \n    # plot error\n    fig = plt.figure(figsize=(18,4))\n    fig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.3, hspace=0.3) # increase space between plots\n    ax0 = fig.add_subplot(131) # add subplot 1 (121 = 1 row, 2 columns, first plot)\n    ax1 = fig.add_subplot(132) # add subplot 2 \n    ax2 = fig.add_subplot(133) # add subplot 3 \n    \n    ax0.scatter(y_valid,preds-y_valid)\n    ax0.set_title(\"error plot\")\n    ax0.set_xlabel(\"price\")\n    ax0.set_ylabel(\"error on price\")\n    \n    MSE = mean_absolute_error(y_valid, preds)\n    ax1.scatter(y_valid,preds-y_valid)\n    ax1.set_title(\"zoom on error plot\")\n    ax1.set_xlabel(\"price\")\n    ax1.set_ylabel(\"error on price\")\n    ax1.set_ylim((-3*MSE,3*MSE))\n    \n    ax2.hist(preds-y_valid, bins = 20, range=(-3*MSE,3*MSE))\n    ax2.set_title(\"error histogram\")\n    ax2.set_xlabel(\"error on price\")\n    ax2.set_ylabel(\"counts\")","94918636":"def MAE_CV_score_model(X,y,model):\n    # Compute the MAE by cross-validation on the features X and target y\n    # model can be chosen for comparison\n    \n    \n    preprocessor = preprocess_data(X)\n\n    # Bundle preprocessing and modeling code in a pipeline\n    my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', model)\n                     ],verbose=False)\n    \n    # Multiply by -1 since sklearn calculates *negative* MAE\n    scores = -1 * cross_val_score(my_pipeline, X, y,\n                              cv=5,\n                              scoring='neg_mean_absolute_error')\n    \n    return scores.mean()","497c93dc":"from sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()\nMAE = MAE_CV_score_model(train_data[['OverallQual']],y,model)\nprint('MAE = %.0f '%(MAE))","c034b948":"model = LinearRegression()\nMAE = MAE_CV_score_model(train_data[list(set(my_cols) & numerical_cols)],y,model)\nprint('MAE = %.0f '%(MAE))","4e02e67e":"model = LinearRegression()\nMAE = MAE_CV_score_model(train_data[my_cols],y,model)\nprint('MAE = %.0f '%(MAE))","6108c660":"# test impact of parameter n_estimators\n\nfor n_estimators in [50,100,500,1000,5000]:\n    model = RandomForestRegressor(n_estimators, random_state=0)\n    MAE = MAE_score_model(train_data[my_cols],y,model)\n    print('MAE = %.0f for n_estimators = %d'%(MAE, n_estimators))\n    ","2c9f9813":"n_estimators=500\nmodel = RandomForestRegressor(n_estimators, random_state=0)\nMAE = MAE_score_model(train_data[list(set(my_cols) & numerical_cols)],y,model)\nprint('Numerical features only: MAE = %.0f for n_estimators = %d'%(MAE, n_estimators))","b7080ddc":"from sklearn.linear_model import Lasso\n\nfor alpha in [0,0.001,0.1,0.2,0.5]:\n    model = Lasso(random_state=0, alpha=alpha, max_iter=10^6) \n    MAE = MAE_score_model(train_data[my_cols],y,model)\n    print('MAE = %.0f for alpha = %f '%(MAE, alpha))","82a3caa8":"from sklearn.neighbors import KNeighborsRegressor\n\nfor n_neighbors in [3,5,10,20]:\n    model = KNeighborsRegressor(n_neighbors=n_neighbors) \n    MAE = MAE_score_model(train_data[my_cols],y,model)\n    print('MAE = %.0f for n_neighbors = %d '%(MAE, n_neighbors))","3135c495":"from sklearn.ensemble import GradientBoostingRegressor\n\n# let' try gradient boost\n\nfor n_estimators in [10,50,100,500,1000]:\n    learning_rate = 0.1\n    model = GradientBoostingRegressor(random_state=0, n_estimators=n_estimators, learning_rate=learning_rate) \n    MAE = MAE_score_model(train_data[my_cols],y,model)\n    print('MAE = %.0f for n_estimators = %d and learning_rate = %f'%(MAE, n_estimators, learning_rate))","31733f27":"for n_estimators in [10,50,100,500,1000]:\n    learning_rate = 0.2\n    model = GradientBoostingRegressor(random_state=0, n_estimators=n_estimators, learning_rate=learning_rate) \n    MAE = MAE_score_model(train_data[my_cols],y,model)\n    print('MAE = %.0f for n_estimators = %d and learning_rate = %f'%(MAE, n_estimators, learning_rate))","30f273d7":"plot_predict_error(train_data[my_cols],y,model)","f925c315":"## 3.3 Random Forest\nLet's try another class of models, namely the Random forest algorithm. There are many more parameters to that model, let's swipe over 'n_estimators'","79172d0e":"These graphs are harder to read than the scatter plots for the numerical data. Yet, they can be useful to see wheather a category impacts the Price significantly or not. Mind the fact that it is not easy to see how many missing data there are from these graphs.","45f47136":"Maybe we are not using the good parameters here and the model could do better. We will fine-tune this later.","6d66a70c":"This quick analysis shows that interpolating missing values for categorical values is far from trivial. We can try a few strategies:\n1. drop those columns\n2. replace by most frequent value\n3. choose manually the features we want to keep and the strategy we apply on them","c8de721a":"Such a model doesn't seem adapted to our problem. I believe it is mostly useful in image analysis.","2ffb248f":"Humm the mean price is quite correlated to the electrical type... Let's check the box plot again.","922b3753":"These graphs give an idea of the relevance of each feature to estimate the SalePrice. Using a regression plot might be misleading as we could be biased towards linear dependencies, but it is still informative (for instance for features like 'OverallQual'). It is also useful to spot some outliers and\/or some features that are dubious; for instance, it makes no sense that the 'OverallCond' feature has a negative correlation with the price (we can see it doesn'y, but the regression line is too sensitive to outliers).\n\nAlso, an interesting observation is that some features like 'EnclosedPorch' have a lot of values equal to 0. This most likely means that there is no value at all, and the actual entry could be NAN. The doesn't mean that there is no information in such a feature; for instance we could transform 'EnclosedPorch' into a boolean that codes if the value is 0 or not. We will see that later on.","66561cf1":"The traget will be the SalePrice. We can directly remove the rows that do not have any value for the price, and split the dataframes between the target \"y\" and the features \"train_data\". ","a5d38c27":"Let us see how the SalePrice correlates to the other numerical features.","85bda5f6":"Some data are missing:","98ea604d":"Now let us display the count plots for the categorical features that have missing data in a histogram. Keeping an eye on the count numbers is important here to verify how many entries are missing.","d357f697":"Let us define split the categorical and the numerical features.","43dbd4ad":"We can make a function to control our strategy to drop the columns. At this point, we will do the following:\n* drop features that have too many missing data (fraction can be tuned)\n* drop high cardinality columns (can be tuned)\n* drop categorical features with missing data (should be refined)","cebe43f0":"## 3.4 Lasso","c1cd769c":"A simple way to filter some categorical data out is to investigate the cardinality of each feature. A little bit arbitrarity, let us drop columns with a cardinality greater than 5 at this point.","539cdc1e":"## 2.1 Dataframe and target\n\nLet's open the data and analyze its contents.","10415721":"Ok, so a simple linear fit of that feature gives us an error of about 33k\\$. Not so bad for a mean price of 180k\\$. \n\nOur next models will have to do much better than that. How much better do we get if we include all the numerical columns in the game?","b467c1a3":"## 3.2 Benchmarking and linear regressions\nWe want to benchmark the power of what we do: using complicated models is fun but should also serve some purpose. To this end, let's start evaluate the most simple model possible (the kind we would make in Excel in the 90's): a linear model on the feature that turned out to be best correlated with the price (namely 'OverallQual').","a7eff6d3":"It seems like we get to better results than with the linear regression. it seems that the results do not improve much after n_estimators = 500, so let's keep that value.\n\nLet's try to see if in this case the model makes good use of the categorical features.","57452914":"# 3. Results from Machine Learning models <a class=\"anchor\" id=\"ML_results\"><\/a>","4c1732d1":"# 1. Import Libraries <a class=\"anchor\" id=\"import_libraries\"><\/a>\nLet's import the libraries and data needed for this competition.","ab30df70":"The main price statistics are the following.","56c7260a":"## 2.2 Analyze the column contents and define filtering strategy","52733aff":"## 2.3 Numerical columns","a6720c46":"## 2.4 Categorical columns","d6d61228":"From these graphs, we can infere that:\n* LotFrontage: missing data can rather safely be replaced by the median value (better than the mean as the distribution is assymmetric)\n* GarageYrBlt: missing data could be replaced by the most frequent one\n* MasVnrArea: this case seems a bit different: the zero's are probably missing data themselves, which means that there are actually much more missing data than what we first estimated. Either we discard this feature, either we compute the mean\/median after excluding the zeros \n\nIn a first iteration, we will use a bruteforce method that deals with all features with a same strategy, the best inference strategy should be \"most frequent\". ","e0a59229":"## 3.1 Defining functions\n\nFrom the data analysis above, we can process the data with the following function:","3d20b4e9":"There are essentially 2 cases here:\n1. features in which one values is by far more likely\n2. features in which two or more values have equal probability\n\nThere isn't a single best interpolation technique here: we could replace some by the most frequent value (Electrical for instance) but it could induce a bias on other features in which 2 values are equally possible (for instance BsmtFinType1).\n\nThe simplest way is to simply discard categorical features that have missing data. Let's see how much information we are losing by doing this by focusing on the 'Electrical' feature.","2829e400":"It seems our models get at best to an mean error of around 16.5k$. \n\nFinally, let see how this error is distributed.","364335c1":"That's much better, but no a game changer yet. Let's see if the categorical features could help somehow.","3e8036b1":"# 4. Conclusion and next steps <a class=\"anchor\" id=\"conclusion\"><\/a>\n\nUsing a very rough data cleaning method, we obtained a mean error that is twice lower than if we fitting the best feature linearly. This is good, but not great. Clearly, more data cleaning is needed to refine the predictions. I will work on that in the coming weeks after documenting a bit.\n\nThe main improvement points could be:\n* finding and discarding outliers\n* better selection of categorical data, and how to fit them\n* investigation of intra-correlations between selected features\n* operations on the target to provide a better fit\n\nSome of these aspects are covered in this excellent notebook by Chee Su Goh: This notebook can give you a very insightful information about concepts beyond the current commit: https:\/\/www.kaggle.com\/cheesu\/house-prices-1st-approach-to-data-science-process#3.-Feature-Selection-&-Engineering\n\nI will improve this notebook soon as well :-) I'll be happy to read your ideas and comments!","e93cfaf2":"Let's define some fuctions to evaluate our models.","55ab96a0":"There is not a big improvement here. This is either due to the fact that a linear regression model is not well suited for categorical features, or that there is not so much relevant information in them (also, we dropped a lot of them).\n\nAnother issue might come from the fact that the OneHotEncoding introduced new features that are linearly dependent, which can impact such kind of models. This is described here: [https:\/\/towardsdatascience.com\/one-hot-encoding-multicollinearity-and-the-dummy-variable-trap-b5840be3c41a](http:\/\/) We will not deal with it at this point.","03efa17a":"Let's have a look to the categorical features now. Box plots are a great way to get an idea of the number of instances (cardinality), the outliers, and whether there is a correlation with the price.","0db5e0a5":"We can see that some columns have only very little data missing while others are basically empty. We can get rid of the last 3 that are almost empty. Let us drop all features that have more than 25% missing data. We will decide below what we will do with the other data.","2f9af133":"# 2. Analyze data <a class=\"anchor\" id=\"analyse_data\"><\/a>","9038334e":"## TOC:\n* [1. Import Libraries](#import_libraries)\n* [2. Analyze data](#analyse_data)\n* [3. Results from Machine Learning models](#ML_results)\n* [4. Conclusion and next steps](#conclusion)","669f7d31":"## 3.5 Nearest neighbors","1d89d466":"Numerical features best analyzed in a scatter plot versus the target. This gives a visual indication of the correlation, the outliers, and how missing data could be interpolated.","37716b45":"Let's go back to our missing data and see more closely how to deal with them.","4e64db52":"Let's have a quick look at other models available on the scikit webpage:\nhttps:\/\/scikit-learn.org\/stable\/supervised_learning.html#supervised-learning\n\nWe will try:\n* Lasso\n* Nearest Neighbors\n* Gradient boost","4fe13290":"This doesn't look so great: the error magnitude does not depend on the price. While making an error of 16k\\$ is OK on a house of 500k\\$, it is a very bad deal on a house of 100k\\$. Also, the histogram is not symmetric. This could be finetune by choosing another criterion than the MAE for our fit.\n\nFinally, we can see that there is a clear outlier. Verifying bad data in the sale price could improve the data quite a lot.","7f4cb16e":"Again, there's not a big diference in the MAE. Maybe there is not much value to our categorical features, or we didn't clean them well.","8dc55107":" Let us see the histogram of the SalePrice.","02fd1f42":"# Introduction\nThis notebook serves as a training for verifying if I master the concepts of the Kaggle courses and can reproduce some results, and how I can improve them. I tried to explain my approach as clearly as I could. It is also part of the \"House Prices: Advanced Regression Techniques\" competition. It should combine all the material in the courses, and I made some additional data analysis to see where the limitations could be.\n\nI will develop models based on the concepts seen in the 2 courses about Machine Learning. Results are not submitted yet because I think they could be improved after covering the \"Feature Engineering\" course. Yet I tried to make the concepts and code clear below, so feel free to copy and comment! :-)","f28298db":"## 2.4 Columns to drop","a5a88758":"## 3.6 Gradient boost"}}