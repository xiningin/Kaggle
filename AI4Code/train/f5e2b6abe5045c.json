{"cell_type":{"583fa867":"code","4bfb6e7f":"code","29257256":"code","30937c8d":"code","cdee3f7a":"code","2209f4c6":"code","82127244":"code","28026bc7":"code","a216e269":"code","6fafe4e3":"code","c6bdf69b":"code","aabd7afe":"code","f807316b":"code","01ac1d03":"code","113fcd27":"code","43015e1c":"code","4b376a68":"code","7c57f343":"code","13c5b854":"code","886ef3b9":"code","fec03098":"code","9a5b4fd1":"code","00743c04":"code","a3983df5":"code","01552c65":"code","09ecc046":"markdown","572428f3":"markdown","c6f5fe61":"markdown","f4fa4546":"markdown","28dc016e":"markdown","e3632203":"markdown","173ff8f8":"markdown","34fa061f":"markdown","3cb78ab3":"markdown","e95c1fbc":"markdown","420ad76e":"markdown","902df751":"markdown","0d28eef5":"markdown","62c92a46":"markdown","b435d3b2":"markdown","a8fdb297":"markdown","73be920e":"markdown","ab31c582":"markdown","ff6f5dcf":"markdown","b0473101":"markdown","1fba06b2":"markdown","e10d16d1":"markdown","914eac4b":"markdown","a75be348":"markdown","209e195e":"markdown","96f186bd":"markdown"},"source":{"583fa867":"#Import all the necessary packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs","4bfb6e7f":"#Number of Centroids\nk=5\ncolor=[\"darkturquoise\",\"darkorange\",\"teal\",\"darkviolet\",\"tomato\", \"yellowgreen\",\"hotpink\",\"gold\"]\n\n#Creates a list of lists (A of B) where length of A is 500, lengths of all B is 2, centers is the number of clusters \nX,_=make_blobs(n_samples=500, n_features=2, centers=k, random_state=10)\n\n#If you want to generate a correspoding y vector too, uncomment the below statement and run this cell\n#X,y =make_blobs(n_samples=500,n_features=2,centers=k,random_state=10)\n\n#Sample the shape to know how many (rows,columns) are present in your synthetic dataset\n#This dataset has 2 columns which are \"x coordinate\" and \"y coordinate\" and each row is a datapoint\nprint(\"Shape = \" + str(X.shape))\n\nprint(\"Sample:\")\nprint(X[:5])","29257256":"#DEFINE ALL THE FUNCTIONS & VARIABLES WE NEED TO DO CLUSTERING\n#Create an empty dictionary to store the cluster name, centroid and points associated with the cluster.\nclusters={}\n\ndef initializeClusterCentroids():\n  #For each cluster, initialize its properties\n  for i in range(k):\n      center = np.random.randint(-8, 10, size= (X.shape[1],), )\n      points = []\n      \n      cluster = {\n          'center' : center,\n          'points' : points,\n          'color' : color[i]\n      }\n      clusters[i] = cluster #looping over clusters dictionary \n\n#Calculate euclidean distance between v1 and v2\ndef distance(v1, v2):\n    return np.sum((v2-v1)**2)**0.5\n\ndef assignPointsToCluster(clusters, X):\n    for ix in range(X.shape[0]):#iterate for all datapoints\n\n        #Foe each datapoint, find the distance to k centroids\n        distance_of_i = [] \n        for kx in range(k):\n            d = distance(X[ix], clusters[kx]['center']) #we have find distance b\/w \n            #all the point with all the five (k) cluster centers\n            distance_of_i.append(d)\n          \n        #Based on the distances to k centroid, pick which cluster to assign it to\n        #here we want find the minimum distance of cluster centers\n        cluster_to_choose = np.argmin(distance_of_i)\n        clusters[cluster_to_choose]['points'].append(X[ix]) \n\n#STEP 3: UPDATE CLUSTER CENTROIDS\ndef updateCluster(clusters):\n    for kx in range(k):\n        cluster_points = clusters[kx]['points']\n        \n        cluster_points = np.array(cluster_points)\n        #here pts are array of list but we want array of array so\n        \n        if len(cluster_points)>0:\n            new_center = np.mean(cluster_points, axis=0) #axis is along rows so will \n            #find mean of all the feature seprately it will give 2,0 when we have cluster pts 50,2\n            clusters[kx]['center'] = new_center\n            clusters[kx]['points'] = [] # clear the points in a cluster list (emptying pts)\n            #as we have to do step 2 after step 3 again till not converge","30937c8d":"#We need to visualize this too. Lets define a function that can do that!\ndef plotclusters(clusters, plot):\n    for kx in range(k):\n        cluster_points = clusters[kx]['points']\n        cluster_color = clusters[kx]['color']\n        cluster_center = clusters[kx]['center']\n        cluster_points = np.array(cluster_points)\n        \n        #plotting points associated \/nearest to the cluster centers\n        if len(cluster_points) > 0:\n          plot.scatter(cluster_points[:, 0], cluster_points[:, 1], s = 2, c = cluster_color)\n        \n        plot.scatter(cluster_center[0], cluster_center[1], s = 250, c = cluster_color, marker=\"o\")","cdee3f7a":"#Lets create a 6 panel plot showing how the cluster centroids evolve with each iteration\nfig, axs = plt.subplots(2, 3)\nfig.set_size_inches(24, 15)\n\n#DATAPOINTS\naxs[0, 0].scatter(X[:,0],X[:,1], s = 2, c = 'black')\naxs[0, 0].set_title('Dataset')\n\n#CENTROID INITIALIZATION\n#Lets call our function that intializes the centroids\ninitializeClusterCentroids()\n\n#Now, lets plot the centroids it has initalized\naxs[0, 1].set_title('Centroid Initialization')\naxs[0, 1].scatter(X[:, 0], X[:, 1], s = 2, c = 'black')\nfor i in range(k):#for all the clusters\n    center = clusters[i]['center'] #its cluster's center\n    axs[0, 1].scatter(center[0], center[1], c=clusters[i]['color'], s=250, marker=\"o\")\n\n#CLUSTERING EPOCH 1\naxs[0, 2].set_title('Iteration 1')\nassignPointsToCluster(clusters, X)\nplotclusters(clusters, axs[0, 2])\nupdateCluster(clusters)\n\n#CLUSTERING EPOCH 2\naxs[1, 0].set_title('Iteration 2')\nassignPointsToCluster(clusters, X)\nplotclusters(clusters, axs[1, 0])\nupdateCluster(clusters)\n\n#CLUSTERING EPOCH 3\naxs[1, 1].set_title('Iteration 3')\nassignPointsToCluster(clusters, X)\nplotclusters(clusters, axs[1, 1])\nupdateCluster(clusters)\n\n#CLUSTERING EPOCH 4\naxs[1, 2].set_title('Iteration 4')\nassignPointsToCluster(clusters, X)\nplotclusters(clusters, axs[1, 2])\nupdateCluster(clusters)\n\n# Hide x labels and tick labels for top plots and y ticks for right plots.\nfor ax in axs.flat:\n    ax.label_outer()","2209f4c6":"from sklearn.cluster import KMeans\n#Check out all functionalities at https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.cluster.KMeans.html\n\nkmeans_demo = KMeans(n_clusters=5) #random centroid intialization\n#kmeans_demo = KMeans() #What if we dont initialize the number of clusters?\n#kmeans_demo = KMeans(n_clusters=5, init='k-means++') #Does smart centroid initialization help?\n\n#Let us fit our synthetic datapoints into the kmeans object\nkmeans_demo.fit(X)","82127244":"#Lets inspect the clustering model\nprint(\"INERTIA = \" + str(kmeans_demo.inertia_))\nprint(\"ITERATIONS TO CONVERGE = \" + str(kmeans_demo.n_iter_))\nprint(\"CLUSTER CENTROIDS = \")\nprint(str(kmeans_demo.cluster_centers_))\nprint()\n\n#using the colours we defined for the clusters at the start\ncolors_toplot = []\nfor label in kmeans_demo.labels_:\n  colors_toplot.append(color[label])\n\nplt.scatter(X[:,0], X[:, 1], c = colors_toplot, s=2)\nplt.scatter(kmeans_demo.cluster_centers_[:,0], kmeans_demo.cluster_centers_[:,1], c = 'black', s=250, marker = \"o\")\nplt.gcf().set_size_inches(12,8)\nplt.show()","28026bc7":"#Let us try this out without defining the cluster counts and see what the optimal number of clusters are\nkmeans_demo2 = KMeans() #What if we dont initialize the number of clusters?\n\n#Let us fit our synthetic datapoints into the kmeans object\nkmeans_demo2.fit(X)","a216e269":"#Lets inspect the clustering model\nprint(\"INERTIA = \" + str(kmeans_demo2.inertia_))\nprint(\"ITERATIONS TO CONVERGE = \" + str(kmeans_demo2.n_iter_))\nprint(\"CLUSTER CENTROIDS = \")\nprint(str(kmeans_demo2.cluster_centers_))\nprint()\n\n#using the colours we defined for the clusters at the start\ncolors_toplot = []\nfor label in kmeans_demo2.labels_:\n  colors_toplot.append(color[label])\n\nplt.scatter(X[:,0], X[:, 1], c = colors_toplot, s=2)\nplt.scatter(kmeans_demo2.cluster_centers_[:,0], kmeans_demo2.cluster_centers_[:,1], c = 'black', s=250, marker = \"o\")\nplt.gcf().set_size_inches(12,8)\nplt.show()","6fafe4e3":"import numpy as np\nfrom skimage import io\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans","c6bdf69b":"image_location = 'https:\/\/user-images.githubusercontent.com\/42711978\/87903363-71f5a900-ca79-11ea-9317-3c7dc7eccc27.jpg'","aabd7afe":"image = io.imread(image_location)\n\n#Lets check out the dimensions of the numerical representation of the image\nprint(\"Image shape = \" + str(image.shape))\n\n#View the image using matplotlib\nplt.imshow(image)\nplt.axis(False)\nplt.show()\n\n#Extract the numerical representations of pixels with (R,G,B) values\n#it converts the MxNx3 image into a Kx3 matrix where K=MxN \n# Each row is now a vector in the 3-D space of RGB\nall_pixels = image.reshape((-1,3))\nprint(\"Total Pixels = \" + str(len(all_pixels)))\n\n#Peek into the data to understand coordinates\nprint(\"Data Sample: \")\nall_pixels[:3]","f807316b":"#Update the optimal value of k below\nk=3\n\n#Build a clustering model on the pixels\nkmeans = KMeans(n_clusters=k)\nkmeans.fit(all_pixels)\nkmeans_centroids = np.array(kmeans.cluster_centers_, dtype=\"uint\") \n\nfig, axs = plt.subplots(1, k)\nfig.set_size_inches(3 * k, 3, forward=True)\nfor clusternumber in range(0,len(kmeans_centroids)):\n  centroid = np.zeros((100,100,3))\n  centroid[:,:,:] = kmeans_centroids[clusternumber]\n  axs[clusternumber].imshow(centroid\/255)\n  axs[clusternumber].text(50,50,str(clusternumber),fontsize=20)\n  axs[clusternumber].axis(\"off\")\n\nprint(\"CLUSTER-LEVEL DOMINANT COLOURS (K = \" + str(k) + \")\")\nplt.show()","01ac1d03":"#The predictions of the cluster number for each pixels\nkmeans.labels_","113fcd27":"#Let us \"compress\" the image\n#This is done by assigning each pixel to be the dominant colour of the cluster it belongs to.\nimage_compressed = np.zeros((all_pixels.shape[0], 3), dtype=\"uint\")\nfor ix in range(all_pixels.shape[0]):\n    image_compressed[ix] = kmeans_centroids[kmeans.labels_[ix]]\nimage_compressed = image_compressed.reshape(image.shape)","43015e1c":"#Let us compare our image and compressed image\nfig, axs = plt.subplots(1, 2)\nfig.set_size_inches(20, 10, forward=True)\n\naxs[0].imshow(image)\naxs[0].text(50,50,\"Original Image\", fontsize=12, c = 'black', backgroundcolor = 'white')\naxs[0].axis(False)\n\naxs[1].imshow(image_compressed)\naxs[1].text(50,50,\"Processed Image\", fontsize=12, c = 'black', backgroundcolor = 'white')\naxs[1].axis(False)\n\nplt.show()\n","4b376a68":"#Importing Libraries\nfrom __future__ import print_function\n%matplotlib inline\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as image\nplt.style.use(\"ggplot\")\n\nfrom skimage import io\nfrom sklearn.cluster import KMeans\n\nfrom ipywidgets import interact, interactive, fixed, interact_manual, IntSlider\nimport ipywidgets as widgets","7c57f343":"plt.rcParams['figure.figsize'] = (20, 12)","13c5b854":"#Data Preprocessing\nimg = io.imread('..\/input\/images\/4beach.jpg')\nax = plt.axes(xticks=[], yticks=[])\nimg_data = (img \/ 255.0).reshape(-1, 3)\nax.imshow(img)","886ef3b9":"class plot_utils:\n    def __init__(self, img_data, title, num_pixels=10000, colors=None):\n        self.img_data = img_data\n        self.title = title\n        self.num_pixels = num_pixels\n        self.colors = colors\n\n    def colorSpace(self):\n        if self.colors is None:\n            self.colors = self.img_data\n\n        rand = np.random.RandomState(42)\n        index = rand.permutation(self.img_data.shape[0])[:self.num_pixels]\n        colors = self.colors[index]\n        R, G, B = self.img_data[index].T\n        fig, ax = plt.subplots(1, 2, figsize=(12,8))\n        ax[0].scatter(R, G, color=colors, marker='.')\n        ax[0].set(xlabel='Red', ylabel='Green', xlim=(0, 1), ylim=(0, 1))\n        ax[1].scatter(R, B, color=colors, marker='.')\n        ax[1].set(xlabel='Red', ylabel='Blue', xlim=(0, 1), ylim=(0, 1))\n        fig.suptitle(self.title, size=20)","fec03098":"x = plot_utils(img_data, title='Input color space: 16 million possible colors')\nx.colorSpace()\n#so their is sharp transition from one color space to other so we have to make it segmented transition \n#it has very smooth gradient","9a5b4fd1":"from sklearn.cluster import MiniBatchKMeans\n#high resolution images ==MiniBatchKMeans==>small batches ","00743c04":"kmeans = MiniBatchKMeans(16).fit(img_data)\nk_colors = kmeans.cluster_centers_[kmeans.predict(img_data)] #here we are predicting 16 colors on the img \n\ny = plot_utils(img_data, colors=k_colors, title=\"Reduced color space: 16 colors\")\ny.colorSpace()","a3983df5":"img_dir = \"..\/input\/images\/\"\n#adding dropdown menu and a slider using Ipythonwidgets\n#https:\/\/ipywidgets.readthedocs.io\/en\/latest\/examples\/Using%20Interact.html","01552c65":"@interact #decorater \ndef color_compression(image=os.listdir(img_dir), k=IntSlider(min=1,max=256,step=1,value=16,\n                                                             continuous_update=False,\n                                                             layout=dict(width='100%'))):\n    \n    #continuous_update will be false so only update when we realease the slider\n    #width here is sliderlayout \n    input_img = io.imread(img_dir + image)\n    img_data = (input_img \/ 255.0).reshape(-1, 3)\n    \n    kmeans = MiniBatchKMeans(k).fit(img_data)\n    k_colors = kmeans.cluster_centers_[kmeans.predict(img_data)]\n    #After K-means has converged, load the large image into your program and \n    #replace each of its pixels with the nearest of the centroid colors you found\n    #from the small image. \n    k_img = np.reshape(k_colors, (input_img.shape))\n    \n    fig, (ax1, ax2) = plt.subplots(1, 2)#this show 1 row but 2 col means one row has 2 images\n    fig.suptitle('K-means Clustering (Dominant colors Extraction)', fontsize=50)\n    \n    ax1.set_title('Compressed')\n    ax1.set_xticks([])\n    ax1.set_yticks([])\n    ax1.imshow(k_img)\n    \n    ax2.set_title('Original')\n    ax2.set_xticks([])\n    ax2.set_yticks([])\n    ax2.imshow(input_img)\n    \n    plt.subplots_adjust(top=0.85)\n    plt.show()","09ecc046":"# **K-means**\n- Common clustering technique, helps to solve many problems efficiently.\n- one of the simplest and popular unsupervised machine learning algorithms.\n- unsupervised algorithms means that our datasets using only input vectors(X) without referring to labelled, outcomes(Y).\n-a common technique for statistical data analysis\n## **The objective of K-means is simple:**\n### Group similar data points together and discover underlying patterns. To achieve this objective, K-means looks for a fixed number (k) of clusters in a dataset.\nIn other words, the K-means algorithm identifies k number of centroids, and then allocates every data point to the nearest cluster,The \u2018means\u2019 in the K-means refers to averaging of the data; that is, finding the centroid.\n\n\n# **Problem:** \n### We are opening a Cake Shops and we have list of locations of BUYERS who frequently order Cake we want to find out optimal locations of Cake Shops where they should be opened?\n(just taking example)","572428f3":"## Implement K-Means Algorithm","c6f5fe61":"## **How the K-means algorithm works?**\n\n","f4fa4546":"# Application :-","28dc016e":"Thats interesting! Even though the data visually has 5 clusters, KMeans has found 8 to be the optimal number of clusters!","e3632203":"# Step-2 : Assign each Buyer to its nearest Cake Shop.","173ff8f8":"init : {'k-means++', 'random'} or ndarray of shape             (n_clusters, n_features), default='k-means++'\n    Method for initialization, defaults to 'k-means++':\n\n    'k-means++' : selects initial cluster centers for k-mean\n    clustering in a smart way to speed up convergence. See section\n    Notes in k_init for more details.\n\n    'random': choose k observations (rows) at random from data for\n    the initial centroids.\n\n    If an ndarray is passed, it should be of shape (n_clusters, n_features)\n    and gives the initial centers.\n\nn_init : int, default=10\n    Number of time the k-means algorithm will be run with different\n    centroid seeds. The final results will be the best output of\n    n_init consecutive runs in terms of inertia.","34fa061f":"# Step-4 : Repeat Steps 2 and 3 Until Convergence.","3cb78ab3":"**1. Cluster assignment**<br>\nthe algorithm goes through each of the data points and depending on which cluster is closer, It assigns the data points to one of the cluster centroids.<br>\n**2. Updation centroid**<br>\nK-means moves the centroids to the average of the points in a cluster. In other words, the algorithm calculates the average of all the points in a cluster and moves the centroid to that average location.\nThis process is repeated until there is no change in the clusters. K is chosen randomly or by giving specific initial starting points by the us.\n","e95c1fbc":"![cake_meme](https:\/\/user-images.githubusercontent.com\/42711978\/87532315-908e2580-c6b0-11ea-8eff-01000290d667.jpg)\n","420ad76e":"# Step-1 : Initialise Cake Shops randomly.","902df751":"![img21](https:\/\/user-images.githubusercontent.com\/42711978\/87532691-29bd3c00-c6b1-11ea-8b03-ba6aef024807.png)\n","0d28eef5":"#### Dataset\nMake_blobs in sklearn is a nice functionaity which gives us a way to create synthetic grouped datapoints which serve as very good datasets to visualize clustering algorithms.\n\n#### Algorithm\nThe main goal of this section is to write out the algorithm for K-Means Clustering without the use of external ML packages and to understand each step that is performed.\n\n#### Objective\nRunning this code on our synthetic dataset will allow us to understand how the model iteratively clusters.","62c92a46":"To process the learning data, the K-means algorithm in data mining starts with a first group of randomly selected centroids, which are used as the beginning points for every cluster, and then performs iterative (repetitive) calculations to optimize the positions of the centroids","b435d3b2":"**Above can also be shown as**","a8fdb297":"### **Cluster Quality**\nOur goal here is not just to make many clusters but to make the most meaningful clusters. We can measure the quality of clusters using a metric called Interia.\n       \n* **Inertia** is the average distortion of the clusters. \\n\",\n* **Distortion** is calculated at the cluster level. For each cluster, it calculates the sum of squared \\\"error\\\" where error is considered as the distance of a every datapoint from its cluster centroid.\n\nThink about this, if we have 500 data-points, the model can create 500 clusters, thus making distortion zero and the \\\"best quality\\\". Measuring the performance just based on distortion is not good enough. Therefore, picking 'k' is a very important step in building a clustering model.\n\n\n### **How do we choose 'k'?**\n\nA fundamental step for any clustering algorithm is to determine the optimal number of clusters (k) into which the data may be clustered.\n       \n![](https:\/\/user-images.githubusercontent.com\/42711978\/87970313-2c6cc680-cae1-11ea-865b-a775d7d06ec1.png)\n\n**WCSS** is an ideal way to figure out the right number of clusters would be to calculate the Within-Cluster-Sum-of-Squares (WCSS). WCSS is the sum of squares of the distances of each data point in all clusters to their respective centroids. The objective is to minimise this value.\n        ![](https:\/\/www.researchgate.net\/profile\/Chirag_Deb\/publication\/320986519\/figure\/fig8\/AS:560163938422791@1510564898246\/Result-of-the-elbow-method-to-determine-optimum-number-of-clusters.png)<br>\nThe Elbow method runs k-means clustering on the dataset for a range of values for k (say from 1-10) and records the avearage distortion for each of these k values. We expect this graph to produce a long-tail (as we increase the clusters, the distortions will reduce) and the goal is to identify the \\\"elbow\\\" which optimizes the number of clusters without overfitting.","73be920e":"**TRY THIS OUT!**\n\n- Increase the value of k to see how you can represent the least resolution and best quality version of your image.\n- Try out different images to explore this cool feature.\n- Think of ways to optimally decide k without manual intervention\n- Are there other applications other than compression? (hint: filters)","ab31c582":"![img](https:\/\/user-images.githubusercontent.com\/42711978\/87543560-97be2f00-c6c2-11ea-8ab3-f7fd89605cfb.png)\n\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f\n![SegmentLocal](https:\/\/user-images.githubusercontent.com\/42711978\/87918133-9e1e2380-ca93-11ea-9237-bbe5ef68c084.gif \"segment\")\n","ff6f5dcf":"### K-means Image Compression with Interactive Controls","b0473101":"![img2133](https:\/\/user-images.githubusercontent.com\/42711978\/87532558-f7abda00-c6b0-11ea-9f0c-f5730efa1eca.png)\n","1fba06b2":"## Clustering with Images\nIf you are given an image, can you use clustering to build any cool image applications?\n\n### Image Compression\n\nAn image is made of pixels and each pixel has a colour associated with it. Every colour can be represented in a numeric scale. High resolution images are rich in numerical pixel data and are thus heavy. If you were to \"compress\" the image, the easiest thing to do would be to reduce the number of colours being represented by the image.\n\nSay you are given an image, and it visually has 5 main sections\/colors. Can you cluster each pixel into one of these 5 sections\/colors and recolour","e10d16d1":"![img2](https:\/\/user-images.githubusercontent.com\/42711978\/87532632-1316e500-c6b1-11ea-9b78-64e16a818b60.png)\n","914eac4b":"# Using KMeans++ from sklearn\n","a75be348":"**Optimal value for k:**  Since this image has 3 main sections (sky, forest and sea) we can set k = 3 so the points are clustered to be one of these sections. Try out different values to see how the image representation changes!","209e195e":"# Step-3 : Update the Shop Location by taking mean of Buyers assigned to it.","96f186bd":"![img213](https:\/\/user-images.githubusercontent.com\/42711978\/87532792-48233780-c6b1-11ea-9cf9-be5acc7f2de2.png)\n"}}