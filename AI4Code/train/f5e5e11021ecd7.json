{"cell_type":{"ba30f91a":"code","df4057f0":"code","7e33cada":"code","ac90fc9f":"code","ef688afd":"code","4add27ae":"code","83a9ef61":"code","31e08d30":"code","b2ae5567":"code","cd1cba94":"code","388cc178":"code","7848cf87":"code","da0b2be5":"code","db86bd19":"code","7722c5d3":"code","d2db8ce7":"code","71497f33":"code","266076b3":"code","d6becd06":"code","c97df487":"code","1a042ec4":"code","61103a2c":"code","5ee9cd67":"code","252876d6":"code","dc4e3627":"code","9292157c":"code","aa28fb12":"code","0b7d5f3b":"code","0b382d88":"code","f3edb5ab":"code","5a995f03":"code","8a4930d5":"code","6578f91a":"code","609ab138":"code","bb998ea2":"code","47628e58":"code","717be7cb":"code","e6dd0946":"markdown","cfb9d652":"markdown"},"source":{"ba30f91a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","df4057f0":"df = pd.read_csv('\/kaggle\/input\/housesalesprediction\/kc_house_data.csv')","7e33cada":"# lets see if the dataset has null values along the columns\ndf.isnull().sum()","ac90fc9f":"df.info()","ef688afd":"df.describe().transpose()","4add27ae":"df = df.drop('id',axis=1)","83a9ef61":"# distribution of target\n\nplt.figure(figsize=(10,5))\nsns.histplot(df['price'])","31e08d30":"# distribution of number of bedrooms (potential good variable)\nsns.countplot(x=df['bedrooms'])","b2ae5567":"plt.figure(figsize=(10,5))\nsns.boxplot(x='bedrooms',y='price',data=df)","cd1cba94":"# square footage of interior housing living space versus price\n\nplt.figure(figsize=(10,5))\nsns.scatterplot(x='price',y='sqft_living',data=df)","388cc178":"# longitude distribution over price\n\nplt.figure(figsize=(12,8))\nsns.scatterplot(x='price',y='long',data=df)","7848cf87":"# latitude distribution over price\n\nplt.figure(figsize=(12,8))\nsns.scatterplot(x='price',y='lat',data=df)","da0b2be5":"# plotting all latitude and longitude, with price as hue\n\nplt.figure(figsize=(12,8))\nsns.scatterplot(x='long',y='lat',data=df,hue='price')","db86bd19":"df.sort_values('price',ascending=False)['price'].head(20)","7722c5d3":"len(df)*(0.01)","d2db8ce7":"non_top_1_perc = df.sort_values('price',ascending=False).iloc[216:]","71497f33":"# plotting all latitude and longitude, with price as hue, and eliminating higher 1% prices\n\nplt.figure(figsize=(12,8))\nsns.scatterplot(x='long',y='lat',\n                data=non_top_1_perc,\n                hue='price',\n                palette='RdYlGn',\n                edgecolor=None,\n                alpha=0.2,\n                s=10)","266076b3":"# from the map, it looks like nearwater houses have higher prices. Lets check:\n\nplt.figure(figsize=(12,8))\nsns.boxplot(x='waterfront',y='price',data=df)","d6becd06":"# feature engineering from date column\n\ndf['date'] = pd.to_datetime(df['date'])\ndf['month'] = df['date'].apply(lambda d: d.month)\ndf['year'] = df['date'].apply(lambda d: d.year)\ndf['day'] = df['date'].apply(lambda d: d.day)\n\ndf = df.drop('date',axis=1)","c97df487":"plt.figure(figsize=(12,8))\nsns.boxplot(x='year',y='price',data=df)","1a042ec4":"plt.figure(figsize=(12,8))\nsns.boxplot(x='month',y='price',data=df)","61103a2c":"plt.figure(figsize=(12,8))\nsns.boxplot(x='day',y='price',data=df)","5ee9cd67":"df.groupby('month').mean()['price'].plot()","252876d6":"df.groupby('year').mean()['price'].plot()","dc4e3627":"df.groupby('day').mean()['price'].plot()","9292157c":"# train\/test split\n\nX = df.drop('price',axis=1)\ny = df['price']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42)","aa28fb12":"# scaling values\n\nscaler = MinMaxScaler()\n\nX_train= scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nprint(X_train.shape)\nprint(X_test.shape)","0b7d5f3b":"# setting and compiling model\n\nmodel = Sequential()\n\nmodel.add(Dense(19,activation='relu'))\nmodel.add(Dense(19,activation='relu'))\nmodel.add(Dense(19,activation='relu'))\nmodel.add(Dense(19,activation='relu'))\nmodel.add(Dense(1))\n\nmodel.compile(optimizer='adam', loss='mse')","0b382d88":"# training model\n\nmodel.fit(x=X_train,\n          y=y_train.values,\n          validation_data=(X_test,y_test.values),\n          batch_size=128, \n          epochs=400,\n          verbose=1)","f3edb5ab":"loss_hist = pd.DataFrame(model.history.history)","5a995f03":"loss_hist.plot()","8a4930d5":"# predicting on new data\n\npreds = model.predict(X_test)","6578f91a":"# evaluation on Test Data\n# https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#regression-metrics\n\nprint(f\"MAE: {mean_absolute_error(y_test, preds)}\")\nprint(f\"MSE: {mean_squared_error(y_test, preds)}\")\nprint(f\"RMSE: {np.sqrt(mean_squared_error(y_test, preds))}\")\nprint(f\"Explained variance: {explained_variance_score(y_test, preds)}\")","609ab138":"print(df['price'].mean())\nprint(df['price'].median())","bb998ea2":"# our predictions\nplt.scatter(y_test, preds)\n\n# perfect predictions\nplt.plot(y_test,y_test,'r')","47628e58":"# plotting error distribution\n\nerrors = y_test.values.reshape(6484, 1) - preds\nsns.displot(errors)","717be7cb":"# predict on a new house\n\nnew_house = df.drop('price',axis=1).iloc[0]\n\nnew_house = scaler.transform(new_house.values.reshape(-1, 21))\n\nmodel.predict(new_house)","e6dd0946":"### Exploratory data analysis","cfb9d652":"### now lets prepare data and run simple neural network regression model"}}