{"cell_type":{"585a5e17":"code","cfa75115":"code","66eb6104":"code","605bbbf1":"code","946c70ca":"code","cab185cb":"code","94ea7c03":"code","cc1cebc1":"code","f15b0929":"code","059eb70d":"code","1efc44ca":"code","75d3876e":"code","6fc24894":"code","dca5cd9a":"code","0f976630":"code","a966a570":"code","e2d8fc2b":"code","7a0733a7":"code","cb41a490":"code","dcaa0f46":"code","f96ff045":"code","78d012e1":"code","9c84d5d1":"code","86a112c0":"code","b403253b":"code","81f7f12f":"code","9631ff41":"code","690f5090":"code","929155ae":"code","522c7ee3":"code","8ae7adf3":"code","db6c8516":"code","e5c9d27f":"code","a1925147":"code","dc119f30":"code","7f72d076":"code","196e7f7e":"code","a64564bc":"code","c6e8f454":"code","fffd3738":"code","fd623b08":"code","4f8c250c":"markdown","526a7e7f":"markdown","051e905f":"markdown","558123a2":"markdown","111d2af9":"markdown","62beba10":"markdown","60c278fe":"markdown","de09530d":"markdown","80d5fd20":"markdown","d7fdc74f":"markdown","36f16d35":"markdown","e0713661":"markdown","e5801831":"markdown","8638cbcb":"markdown","e044b8af":"markdown","63360536":"markdown","c385915d":"markdown"},"source":{"585a5e17":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cfa75115":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n#load_given_train_datasets\ntrain_data = pd.read_csv('..\/input\/learn-ai-bbc\/BBC News Train.csv')","66eb6104":"train_data.head()","605bbbf1":"print(\"NUMBER OF DATA POINTS -\",train_data.shape[0])\nprint(\"NUMBER OF FEATURES -\",train_data.shape[1])\nprint(\"FEATURES -\",train_data.columns.values)","946c70ca":"train_data['Category'].value_counts()","cab185cb":"#checking_null_values\ntrain_data.isna().sum()","94ea7c03":"target_category = train_data['Category'].unique()\nprint(target_category)","cc1cebc1":"news_cat = train_data['Category'].value_counts()\n\nplt.figure(figsize=(10,6))\nmy_colors = ['r','g','c','m','b']\nnews_cat.plot(kind='bar', color=my_colors)\nplt.grid()\nplt.xlabel(\"News Categories\")\nplt.ylabel(\"Datapoints Per Category\")\nplt.title(\"Distribution of Datapoints Per Category\")\nplt.show()","f15b0929":"import warnings\nwarnings.filterwarnings(\"ignore\")\nfrom nltk.corpus import stopwords\nimport nltk\nimport re\n\n#loading_the_stop_words_from_nltk_library_\nstop_words = set(stopwords.words('english'))\n\ndef txt_preprocessing(total_text, index, column, df):\n    if type(total_text) is not int:\n        string = \"\"\n        \n        #replace_every_special_char_with_space\n        total_text = re.sub('[^a-zA-Z0-9\\n]', ' ', total_text)\n        \n        #replace_multiple_spaces_with_single_space\n        total_text = re.sub('\\s+',' ', total_text)\n        \n        #converting_all_the_chars_into_lower_case\n        total_text = total_text.lower()\n        \n        for word in total_text.split():\n        #if_the_word_is_a_not_a_stop_word_then_retain_that_word_from_the_data\n            if not word in stop_words:\n                string += word + \" \"\n        \n        df[column][index] = string","059eb70d":"#train_data_text_processing_stage_\nfor index, row in train_data.iterrows():\n    if type(row['Text']) is str:\n        txt_preprocessing(row['Text'], index, 'Text', train_data)\n    else:\n        print(\"THERE IS NO TEXT DESCRIPTION FOR ID :\",index)\n\ntrain_data.head()","1efc44ca":"from sklearn.model_selection import train_test_split\nX_train = train_data\ny_train = train_data['Category']\n\nX_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size=0.20, stratify=y_train, random_state=0)","75d3876e":"print(\"NUMBER OF DATA POINTS IN TRAIN DATA :\", X_train.shape[0])\nprint(\"NUMBER OF DATA POINTS IN CROSS VALIDATION DATA :\", X_cv.shape[0])","6fc24894":"#this_returns_a_dict_keys_as_class_labels_and_values_as_the_number_of_data_points_in_that_class\n\nimport numpy as np\ntrain_class_distribution = X_train['Category'].value_counts().sort_index()\ncv_class_distribution = X_cv['Category'].value_counts().sort_index()\n\n#distribution_of y_i's_in_train_data\nplt.figure(figsize=(10,6))\nmy_colors = ['r', 'g', 'b', 'k', 'y']\ntrain_class_distribution.plot(kind='bar', color=my_colors)\nplt.xlabel('CATEGORY')\nplt.ylabel('DATA POINTS PER CATEGORY (CLASS)')\nplt.title('DISTRIBUTION OF y_i IN TRAIN DATA')\nplt.grid()\nplt.show()\n\n#-(train_class_distribution.values):_the_minus_sign_will_returns_in_decreasing_order\nsorted_yi = np.argsort(-train_class_distribution.values)\nfor i in sorted_yi:\n    print('NUMBER OF DATA POINTS IN CLASS', i+1, ':',train_class_distribution.values[i],'(',np.round((train_class_distribution.values[i]\/X_train.shape[0]*100), 3),'%)')\n\nprint(\"-.\"*50)\n#distribution_of y_i's_in_cv_data\nplt.figure(figsize=(10,6))\nmy_colors = ['r', 'g', 'b', 'k', 'y']\ncv_class_distribution.plot(kind='bar', color=my_colors)\nplt.xlabel('CATEGORY')\nplt.ylabel('DATA POINTS PER CATEGORY (CLASS)')\nplt.title('DISTRIBUTION OF y_i IN CROSS VALIDATION DATA')\nplt.grid()\nplt.show()\n\nsorted_yi = np.argsort(-cv_class_distribution.values)\nfor i in sorted_yi:\n    print('NUMBER OF DATA POINTS IN CLASS', i+1, ':',cv_class_distribution.values[i],'(',np.round((cv_class_distribution.values[i]\/X_cv.shape[0]*100), 3),'%)')","dca5cd9a":"#building a CountVectorizer with all the words that occured minimum 3 times in train data\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ntext_vectorizer = CountVectorizer(min_df=3)\ntrain_text_ohe = text_vectorizer.fit_transform(X_train['Text'])\n\n#getting all the feature names (words)\ntrain_text_features = text_vectorizer.get_feature_names()\n\n#train_text_ohe.sum(axis=0).A1 will sum every row and returns (1*number of features) vector\ntrain_text_fea_counts = train_text_ohe.sum(axis=0).A1\n\n#zip(list(text_features),text_fea_counts) will zip a word with its number of times it occured\ntext_fea_dict = dict(zip(list(train_text_features),train_text_fea_counts))\n\nprint(\"Total Number of Unique Words in Train Data :\",len(train_text_features))","0f976630":"from sklearn.preprocessing import normalize #normalize every feature\n\ntrain_text_ohe = normalize(train_text_ohe, axis=0)\n\n#we use the same vectorizer that was trained on train data\ncv_text_ohe = text_vectorizer.transform(X_cv['Text'])\n\n#don't forget to normalize every feature\ncv_text_ohe = normalize(cv_text_ohe, axis=0)","a966a570":"#this_function_plots_the_confusion_matrices_given_y_i_and_y_i_hat_\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\ndef plot_confusion_matrix(test_y, predict_y):\n\n    C = confusion_matrix(test_y, predict_y) #confusion_mat\n    A =(((C.T)\/(C.sum(axis=1))).T) #recall_mat\n    B =(C\/C.sum(axis=0)) #precision_mat\n    \n    labels = [1,2,3,4,5,6]\n    \n    #representing_C_in_heatmap_format\n    print(\"-\"*40, \"Confusion Matrix\", \"-\"*40)\n    plt.figure(figsize=(20,7))\n    sns.heatmap(C, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()\n    \n    #representing_B_in_heatmap_format\n    print(\"-\"*40, \"Precision Matrix (Columm Sum=1)\", \"-\"*40)\n    plt.figure(figsize=(20,7))\n    sns.heatmap(B, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()\n    \n    #representing_A_in_heatmap_format\n    print(\"-\"*40, \"Recall Matrix (Row Sum=1)\", \"-\"*40)\n    plt.figure(figsize=(20,7))\n    sns.heatmap(A, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()","e2d8fc2b":"from sklearn.linear_model import SGDClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import log_loss\n\nalpha = [0.00001, 0.0001, 0.001, 0.1, 1, 10, 100, 1000]\n\ncv_log_error_array = []\n\nfor i in alpha:\n    print(\"For Alpha =\", i)\n    clf = MultinomialNB(alpha=i)\n    clf.fit(train_text_ohe, y_train)\n    \n    nb_sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    nb_sig_clf.fit(train_text_ohe, y_train)\n    \n    sig_clf_probs = nb_sig_clf.predict_proba(cv_text_ohe)\n    \n    cv_log_error_array.append(log_loss(y_cv, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    #to avoid rounding error while multiplying probabilites we use log-probability estimates\n    print(\"Log Loss :\",log_loss(y_cv, sig_clf_probs)) \n\nfig, ax = plt.subplots()\nax.plot(np.log10(alpha), cv_log_error_array,c='g')\n\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)), (np.log10(alpha[i]),cv_log_error_array[i]))\n    \nplt.grid()\nplt.xticks(np.log10(alpha))\nplt.title(\"Cross Validation Error for Each Alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error Measure\")\nplt.show()\n\nbest_alpha = np.argmin(cv_log_error_array)\n\nclf = MultinomialNB(alpha=alpha[best_alpha])\nclf.fit(train_text_ohe, y_train)\n\nnb_sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nnb_sig_clf.fit(train_text_ohe, y_train)\n\npredict_y = nb_sig_clf.predict_proba(train_text_ohe)\nprint('For values of best alpha =', alpha[best_alpha],\"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n\npredict_y = nb_sig_clf.predict_proba(cv_text_ohe)\nprint('For values of best alpha =', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))","7a0733a7":"from sklearn.metrics import accuracy_score\npredicted_y = nb_sig_clf.predict(cv_text_ohe)\ntrain_accuracy = (nb_sig_clf.score(train_text_ohe, y_train)*100)\ncv_accuracy = (accuracy_score(predicted_y, y_cv)*100)\n\nprint(\"Naive Bayes Train Accuracy -\",train_accuracy)\nprint(\"Naive Bayes CV Accuracy -\",cv_accuracy)","cb41a490":"plot_confusion_matrix(y_cv, nb_sig_clf.predict(cv_text_ohe.toarray()))","dcaa0f46":"from sklearn.metrics import classification_report\nprint(classification_report(predicted_y, y_cv, target_names=target_category))","f96ff045":"from sklearn.linear_model import LogisticRegression\n\n#train a logistic regression + calibration model using text features which are one-hot encoded\nalpha = [10 ** x for x in range(-5, 1)]\n\ncv_log_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42) #loss='log'_means_logistic_regression\n    clf.fit(train_text_ohe, y_train)\n    \n    lr_sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    lr_sig_clf.fit(train_text_ohe, y_train)\n    \n    predict_y = lr_sig_clf.predict_proba(cv_text_ohe)\n    cv_log_error_array.append(log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n    \n    print('For values of alpha =',i,\"The log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\n    \nplt.grid()\nplt.title(\"Cross Validation Error for Each Alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error Measure\")\nplt.show()\n\nbest_alpha = np.argmin(cv_log_error_array)\n\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_text_ohe, y_train)\n\nlr_sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nlr_sig_clf.fit(train_text_ohe, y_train)\n\npredict_y = lr_sig_clf.predict_proba(train_text_ohe)\nprint('For values of best alpha =', alpha[best_alpha],\"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n\npredict_y = lr_sig_clf.predict_proba(cv_text_ohe)\nprint('For values of best alpha =', alpha[best_alpha],\"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))","78d012e1":"predicted_y = lr_sig_clf.predict(cv_text_ohe)\ntrain_accuracy = (lr_sig_clf.score(train_text_ohe, y_train)*100)\ncv_accuracy = (accuracy_score(predicted_y, y_cv)*100)\n\nprint(\"Logistic Regression Train Accuracy -\",train_accuracy)\nprint(\"Logistic Regression CV Accuracy -\",cv_accuracy)","9c84d5d1":"plot_confusion_matrix(y_cv, lr_sig_clf.predict(cv_text_ohe.toarray()))","86a112c0":"print(classification_report(predicted_y, y_cv, target_names=target_category))","b403253b":"from sklearn.ensemble import RandomForestClassifier\n\nalpha = [100,200,500,1000,2000]\nmax_depth = [5, 10]\n\ncv_log_error_array = []\n\nfor i in alpha:\n    for j in max_depth:\n        print(\"for n_estimators =\",i,\"and max depth =\",j)\n        \n        clf = RandomForestClassifier(n_estimators=i, criterion='gini', max_depth=j, random_state=42, n_jobs=-1)\n        clf.fit(train_text_ohe, y_train)\n        \n        rf_sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n        rf_sig_clf.fit(train_text_ohe, y_train)\n        \n        sig_clf_probs = rf_sig_clf.predict_proba(cv_text_ohe)\n        \n        cv_log_error_array.append(log_loss(y_cv, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n        print(\"Log Loss :\",log_loss(y_cv, sig_clf_probs)) \n\nfig, ax = plt.subplots()\nfeatures = np.dot(np.array(alpha)[:,None],np.array(max_depth)[None]).ravel()\nax.plot(features, cv_log_error_array,c='g')\n\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[int(i\/2)],max_depth[int(i%2)],str(txt)), (features[i],cv_log_error_array[i]))\n    \nplt.grid()\nplt.title(\"Cross Validation Error for Each Alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error Measure\")\nplt.show()\n\nbest_alpha = np.argmin(cv_log_error_array)\n\nclf = RandomForestClassifier(n_estimators=alpha[int(best_alpha\/2)], criterion='gini', max_depth=max_depth[int(best_alpha%2)], random_state=42, n_jobs=-1)\nclf.fit(train_text_ohe, y_train)\n\nrf_sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nrf_sig_clf.fit(train_text_ohe, y_train)\n\npredict_y = rf_sig_clf.predict_proba(train_text_ohe)\nprint('For values of best estimator =', alpha[int(best_alpha\/2)], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n\npredict_y = rf_sig_clf.predict_proba(cv_text_ohe)\nprint('For values of best estimator =', alpha[int(best_alpha\/2)], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))","81f7f12f":"predicted_y = rf_sig_clf.predict(cv_text_ohe)\ntrain_accuracy = (rf_sig_clf.score(train_text_ohe, y_train)*100)\ncv_accuracy = (accuracy_score(predicted_y, y_cv)*100)\n\nprint(\"Random Forest Train Accuracy -\",train_accuracy)\nprint(\"Random Forest CV Accuracy -\",cv_accuracy)","9631ff41":"plot_confusion_matrix(y_cv, rf_sig_clf.predict(cv_text_ohe.toarray()))","690f5090":"print(classification_report(predicted_y, y_cv, target_names=target_category))","929155ae":"from sklearn.neighbors import KNeighborsClassifier\n\nalpha = [5, 11, 15, 21, 31, 41, 51, 99]\n\ncv_log_error_array = []\n\nfor i in alpha:\n    print(\"for alpha =\", i)\n    clf = KNeighborsClassifier(n_neighbors=i) \n    clf.fit(train_text_ohe, y_train) #knn may not good at handling large dimensionality\n    \n    knn_sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    knn_sig_clf.fit(train_text_ohe, y_train)\n    \n    sig_clf_probs = knn_sig_clf.predict_proba(cv_text_ohe)\n    cv_log_error_array.append(log_loss(y_cv, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    #to avoid rounding error while multiplying probabilites we use log-probability estimates\n    print(\"Log Loss :\",log_loss(y_cv, sig_clf_probs)) \n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for Each Alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error Measure\")\nplt.show()\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = KNeighborsClassifier(n_neighbors=alpha[best_alpha])\nclf.fit(train_text_ohe, y_train)\n\nknn_sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nknn_sig_clf.fit(train_text_ohe, y_train)\n\npredict_y = knn_sig_clf.predict_proba(train_text_ohe)\nprint('For values of best alpha =', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n\npredict_y = knn_sig_clf.predict_proba(cv_text_ohe)\nprint('For values of best alpha =', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))","522c7ee3":"predicted_y = knn_sig_clf.predict(cv_text_ohe)\ntrain_accuracy = (knn_sig_clf.score(train_text_ohe, y_train)*100)\ncv_accuracy = (accuracy_score(predicted_y, y_cv)*100)\n\nprint(\"K Nearest Neighbor Train Accuracy -\",train_accuracy)\nprint(\"K Nearest Neighbor CV Accuracy -\",cv_accuracy)","8ae7adf3":"plot_confusion_matrix(y_cv, knn_sig_clf.predict(cv_text_ohe.toarray()))","db6c8516":"print(classification_report(predicted_y, y_cv, target_names=target_category))","e5c9d27f":"from prettytable import PrettyTable\n\nx = PrettyTable()\n\nx.field_names = [\"Model\", \"Train Log-Loss\", \"CV Log-Loss\", \"Train Accuracy\", \"CV Accuracy\"]\nx.add_row(['Multinomial Naive Bayes', '0.060', '0.156', '99.58', '95.63'])\nx.add_row(['Logistic Regression', '0.034', '0.132', '100.0', '96.64'])\nx.add_row(['Random Forest', '0.071', '0.231', '99.49', '92.95'])\nx.add_row(['K Nearest Neighbor', '0.452', '0.531', '93.45', '83.22'])\nprint(x)","a1925147":"test_data = pd.read_csv(\"..\/input\/learn-ai-bbc\/BBC News Test.csv\")\ntest_data.shape","dc119f30":"test_data.head()","7f72d076":"#checking null values\ntest_data.isna().sum()","196e7f7e":"#test_data_text_processing_stage_\nfor index, row in test_data.iterrows():\n    if type(row['Text']) is str:\n        txt_preprocessing(row['Text'], index, 'Text', test_data)\n    else:\n        print(\"THERE IS NO TEXT DESCRIPTION FOR ID :\",index)\n\ntest_data.head()","a64564bc":"#we use the same vectorizer that was trained on train data\ntest_text_ohe = text_vectorizer.transform(test_data['Text'])\n\n#don't forget to normalize every feature\ntest_text_ohe = normalize(test_text_ohe, axis=0)","c6e8f454":"#lr_sig_clf is the same CalibratedClassifierCV which is used in Logistic Regression Model\ntest_final_ohe = lr_sig_clf.predict(test_text_ohe)\ntest_final_list = test_final_ohe.tolist()\n\ntest_final_list[:5]","fffd3738":"test_data['Category'] = test_final_list\n\ntest_data.head(20)","fd623b08":"test_data = test_data.drop(\"Text\", axis=1)\ntest_data.head(20)","4f8c250c":"**<h2><font color=red>1.<\/font><font color=green> Multinomial Naive Bayes<\/font><\/h2>**","526a7e7f":"**Use the same CalibratedClassifierCV which is used in Logistic Regression Model to predict the target (Category) of Test Set.**","051e905f":"**<h2><font color=red>3.<\/font><font color=green> Random Forest Classifier<\/font><\/h2>**","558123a2":"**<font color=blue>Confusion Matrix<\/font>**","111d2af9":"**<font color=red>Pretty Table<\/font>**","62beba10":"**<font color=blue>One Hot Encoding of Text Data (BoW)<\/font>**","60c278fe":"**<font color=blue>Train-Test Split<\/font>**","de09530d":"**<h2><font color=red>2.<\/font><font color=green> Logistic Regression<\/font><\/h2>**","80d5fd20":"**<h2><font color=red>4.<\/font><font color=green> K Nearest Neighbor<\/font><\/h2>**","d7fdc74f":"**From the above table it seems that, Logistic Regression Model works quite well as compared to other models as it has 96.64% CV (Test) accuracy. So we will apply Logistic Regression on given Testing dataset.**","36f16d35":"**<font color=blue>Distribution of Target Varibles in Train and CV Datasets<\/font>**","e0713661":"<h3><font color=red>Test Set<\/font><\/h3>","e5801831":"**<font color=blue>One Hot Encoding of Text Data (BoW)<\/font>**","8638cbcb":"**<font color=blue>Data Preprocessing<\/font>**","e044b8af":"**<font color=blue>Data Preprocessing<\/font>**","63360536":"**<font color=blue>Categories<\/font>**","c385915d":"**<font color=blue>Distribution of Datapoints Per Category<\/font>**"}}