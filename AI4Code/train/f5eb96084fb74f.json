{"cell_type":{"bc4f9d81":"code","1769e0e5":"code","d8b63a9a":"code","035ee3ff":"code","e411d2e2":"code","0743b8fe":"code","d6048f4b":"code","cc6af7bd":"markdown","d7075508":"markdown","50b1fa3d":"markdown","e5bc1e38":"markdown","3e89910b":"markdown","4f7241b2":"markdown"},"source":{"bc4f9d81":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\nimport re\nimport nltk\nfrom nltk.stem import PorterStemmer\nimport matplotlib.pyplot as plt\npd.options.display.max_colwidth = 200\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1769e0e5":"os.listdir('\/kaggle\/input\/nlp-getting-started')","d8b63a9a":"training_data = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntraining_data.keyword = training_data.keyword.astype(str)\n\ntesting_data = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ntesting_data.keyword = testing_data.keyword.astype(str)","035ee3ff":"# training_data.head()\n# training_data.shape\n# training_data.describe\n# training_data.info()\n\n# testing_data.head()\n# testing_data.shape\n# testing_data.describe\n# testing_data.info()","e411d2e2":"# function to check missing values in the dataset\n\ndef check_missing_data(df):\n    total = df.isnull().sum()\n#     print(total)\n    percentage = round(total \/ df.shape[0] *100)\n#     print(percentage)\n    missing_data = pd.concat([total, percentage], axis=1, keys= ['Total', 'Percent']).sort_values(by='Percent', ascending = False)\n    missing_data = missing_data[missing_data['Total'] > 0]\n    return missing_data\n\n# check_missing_data(training_data)\n# check_missing_data(testing_data)","0743b8fe":"wpt = nltk.WordPunctTokenizer()\nstop_words = nltk.corpus.stopwords.words('english')\nps = PorterStemmer()\n\n# function to execute various pre-processing steps\n\ndef pre_process_data(row):\n    \n    # punctuation removal step\n    row['text'] = re.sub(r'[^a-zA-Z\\s]', '', row['text'], re.I|re.A)\n    \n    # lower casing step\n    row['keyword'] = row['keyword'].lower()\n    row['text'] = row['text'].lower()\n    \n    # tokenization step\n    row['text'] = wpt.tokenize(row['text'])\n    \n    # stop word removal step\n    row['text'] = [token for token in row['text'] if token not in stop_words]\n    \n    # stemming step\n    row['text'] = [ps.stem(token) for token in row['text']]\n    \n    return row","d6048f4b":"training_data = training_data.apply(pre_process_data, axis=1)\ntesting_data = testing_data.apply(pre_process_data, axis=1)","cc6af7bd":"**UNDERSTANDING DATA**","d7075508":"**PRE-PROCESSING DATA**","50b1fa3d":"**CHECKING FILES IN THE DIRECTORY**","e5bc1e38":"**EXTRACTING TRAINING AND TESTING DATA INTO A PANDAS DATAFRAME**","3e89910b":"**DESCRIBING THE TRAINING AND TESTING DATA**","4f7241b2":"**IMPORTING NECESSARY PACKAGES**\n"}}