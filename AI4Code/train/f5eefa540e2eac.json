{"cell_type":{"ecdf8230":"code","fb3a54c1":"code","4b9bd339":"code","c02bc2c2":"code","4f04cb7d":"code","cb356bd3":"code","ce1751c8":"code","9175d506":"code","bd3e72f3":"code","ee9f57ef":"code","23b4b4dd":"code","1a004cd6":"markdown","1b3d390f":"markdown","e3141731":"markdown","c5ca4478":"markdown","ea35f4ac":"markdown","74638ac0":"markdown","e1cc6f2f":"markdown","e5322c34":"markdown","fa7cd1a6":"markdown","42b6ce9d":"markdown","23a4cae0":"markdown","3f310e3c":"markdown","28ded402":"markdown","692cc3d6":"markdown","a01b1f8d":"markdown","a3afe82a":"markdown","5d687327":"markdown","0b0ae75d":"markdown","17f5c94c":"markdown","0cf85ed0":"markdown","94282b34":"markdown","4f7953c0":"markdown","7da5eb1c":"markdown","5d1481d7":"markdown","5cf7f8b1":"markdown","7ab9f882":"markdown","e86ded5c":"markdown","d9e40164":"markdown","b3561b92":"markdown","94a5226d":"markdown","9ac33c21":"markdown","986be217":"markdown","2ea6a2da":"markdown","65dd50c3":"markdown","088637d4":"markdown"},"source":{"ecdf8230":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pylab as plt\nimport seaborn as sns\nimport os\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/nflplayingsurface2\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","fb3a54c1":"summary =  pd.read_csv('\/kaggle\/input\/nflplayingsurface2\/summary.csv')","4b9bd339":"#### CHARTS CONF\n\nimport matplotlib as mpl\n\ntorneo = [\"#2E3C4D\",\"#F15B24\", \"#0E71A3\", \"#52C2B8\",\"#42BAE1\", \"#95a5a6\"]\nsns.set_palette(torneo)\n\nmpl.rcParams['axes.facecolor'] = 'white'\nmpl.rcParams['font.family'] = 'open-sans'\nmpl.rcParams['axes.edgecolor'] = '#2E3C4D'\nmpl.rcParams['xtick.color'] = '#2E3C4D'\nmpl.rcParams['ytick.color']='#2E3C4D'\nmpl.rcParams['grid.linewidth'] = 3","c02bc2c2":"#### VELOCITY MAX\nplt.figure(figsize=(16,6))\nplt.subplot(121)\nsns.distplot(summary[summary['is_injury']==0]['v_sm_max'].fillna(0), label='No injury', bins=100)\nsns.distplot(summary[summary['is_injury']==1]['v_sm_max'].fillna(0),label='Injury')\nplt.xlabel('Max Velocity')\nplt.legend(ncol=2)\nsns.despine() \nplt.subplot(122)\ng = sns.violinplot(\n    x='FieldType',\n    y='v_sm_max',\n    hue='is_injury',\n    data=summary,\n    split=True\n)\nplt.ylabel('Max Velocity')\nplt.xlabel('')\nsns.despine() \n# plt.legend(g, ['Not Injury','Injury'], ncol=2)","4f04cb7d":"#### APPROACH VELOCITY MAX\nplt.figure(figsize=(16,6))\nplt.subplot(121)\nsns.distplot(summary[summary['is_injury']==0]['app_v_max'].fillna(0), label='No injury', bins=100)\nsns.distplot(summary[summary['is_injury']==1]['app_v_max'].fillna(0),label='Injury')\nplt.xlabel('Max Approach Velocity for Turns in Play')\nplt.legend(ncol=2)\nsns.despine() \nplt.subplot(122)\ng = sns.violinplot(\n    x='FieldType',\n    y='app_v_max',\n    hue='is_injury',\n    data=summary,\n    split=True\n)\nplt.ylabel('Max Approach Velocity for Turns in Play')\nplt.xlabel('')\nsns.despine() \n# plt.legend(['Not Injury','Injury'], ncol=2)","cb356bd3":"#### APPROACH ACCELERATION MAX\nplt.figure(figsize=(16,6))\nplt.subplot(121)\nsns.distplot(summary[summary['is_injury']==0]['app_a_max'].fillna(0), label='No injury', bins=100)\nsns.distplot(summary[summary['is_injury']==1]['app_a_max'].fillna(0),label='Injury')\nplt.xlim(-5, 20)\nplt.xlabel('Max Approach Acceleration for Turns in Play')\nplt.legend(ncol=2)\nsns.despine() \nplt.subplot(122)\ng = sns.violinplot(\n    x='FieldType',\n    y='app_a_max',\n    hue='is_injury',\n    data=summary,\n    split=True\n)\nplt.ylabel('Max Approach Acceleration for Turns in Play')\nplt.xlabel('')\nsns.despine() \nplt.ylim(-5, 20);","ce1751c8":"#### APPROACH ACCELERATION MIN\nplt.figure(figsize=(16,6))\nplt.subplot(121)\nsns.distplot(summary[summary['is_injury']==0]['app_a_min'].fillna(0), label='No injury', bins=100)\nsns.distplot(summary[summary['is_injury']==1]['app_a_min'].fillna(0),label='Injury')\nplt.xlim(-20,5)\nplt.xlabel('Min Approach Acceleration for Turns in Play')\nplt.legend(ncol=2)\nsns.despine() \nplt.subplot(122)\ng = sns.violinplot(\n    x='FieldType',\n    y='app_a_min',\n    hue='is_injury',\n    data=summary,\n    split=True\n)\nplt.ylabel('Min Approach Acceleration for Turns in Play')\nplt.xlabel('')\nsns.despine() \nplt.ylim(-20,5);\n# plt.legend(g, ['Not Injury','Injury'], ncol=2)","9175d506":"\n### COD VS COO MEAN \nplt.figure(figsize=(16,6))\nplt.subplot(121)\nsns.distplot(summary[summary['is_injury']==0]['cod_coo_mean'].fillna(0), label='No injury', bins=100)\nsns.distplot(summary[summary['is_injury']==1]['cod_coo_mean'].fillna(0),label='Injury')\nplt.xlim(0,20)\nplt.xlabel('Mean COD vs. COO')\nplt.legend(ncol=2)\nsns.despine() \nplt.subplot(122)\ng = sns.violinplot(\n    x='FieldType',\n    y='cod_coo_mean',\n    hue='is_injury',\n    data=summary,\n    split=True\n)\nplt.ylabel('Mean COD vs. COO')\nplt.xlabel('')\nsns.despine() \nplt.ylim(0,20);\n# plt.legend(g, ['Not Injury','Injury'], ncol=2)\n","bd3e72f3":"### CENTRIPETAL ACCELERATION\nplt.figure(figsize=(16,6))\nplt.subplot(121)\nsns.distplot(summary[summary['is_injury']==0]['cent_acc_0.25_mean'].fillna(0), label='No injury', bins=100)\nsns.distplot(summary[summary['is_injury']==1]['cent_acc_0.25_mean'].fillna(0),label='Injury')\nplt.xlim(0,3)\nplt.xlabel('Centripetal Acceleration Mean')\nplt.legend(ncol=2)\nsns.despine() \nplt.subplot(122)\ng = sns.violinplot(\n    x='FieldType',\n    y='cent_acc_0.25_mean',\n    hue='is_injury',\n    data=summary,\n    split=True\n)\nplt.ylabel('Centripetal Acceleration Mean')\nplt.xlabel('')\nsns.despine() \nplt.ylim(0,3);\n# plt.legend(g, ['Not Injury','Injury'], ncol=2)","ee9f57ef":"plt.figure(figsize=(20,8))\nplt.subplot(231)\nplt.title('Centripetal Acceleration Mean')\nsns.distplot(summary[summary['FieldType']=='Natural']['cent_acc_0.25_mean'].fillna(0), label='Natural', bins=100)\nsns.distplot(summary[summary['FieldType']=='Synthetic']['cent_acc_0.25_mean'].fillna(0),label='Synthetic')\nplt.xlabel('')\nplt.xlim(0,3)\nplt.legend(ncol=1, loc='upper right')\nsns.despine() \nplt.subplot(232)\nplt.title('Mean COD vs. COO')\nsns.distplot(summary[summary['FieldType']=='Natural']['cod_coo_mean'].fillna(0), label='Natural', bins=100)\nsns.distplot(summary[summary['FieldType']=='Synthetic']['cod_coo_mean'].fillna(0),label='Synthetic')\nplt.xlim(0,20)\nsns.despine() \nplt.legend(ncol=1, loc='upper right')\nplt.xlabel('')\nplt.subplot(233)\nplt.title('Max Approach Velocity')\nsns.distplot(summary[summary['FieldType']=='Natural']['app_v_max'].fillna(0), label='Natural', bins=100)\nsns.distplot(summary[summary['FieldType']=='Synthetic']['app_v_max'].fillna(0),label='Synthetic')\n#plt.xlim(0,20)\nsns.despine() \nplt.legend(ncol=1, loc='upper right')\nplt.xlabel('')\nplt.subplot(234)\nplt.title('Max Approach Acceleration')\nsns.distplot(summary[summary['FieldType']=='Natural']['app_a_max'].fillna(0), label='Natural', bins=100)\nsns.distplot(summary[summary['FieldType']=='Synthetic']['app_a_max'].fillna(0),label='Synthetic')\n#plt.xlim(0,20)\nsns.despine() \nplt.legend(ncol=1, loc='upper right')\nplt.xlabel('');\nplt.subplot(235)\nplt.title('Max Velocity')\nsns.distplot(summary[summary['FieldType']=='Natural']['v_sm_max'].fillna(0), label='Natural', bins=100)\nsns.distplot(summary[summary['FieldType']=='Synthetic']['v_sm_max'].fillna(0),label='Synthetic')\n#plt.xlim(0,20)\nsns.despine() \nplt.legend(ncol=1, loc='upper right')\nplt.xlabel('');\nplt.subplot(236)\nplt.title('Min Approach Acceleration for Turns in Play')\nsns.distplot(summary[summary['FieldType']=='Natural']['app_a_min'].fillna(0), label='Natural', bins=100)\nsns.distplot(summary[summary['FieldType']=='Synthetic']['app_a_min'].fillna(0),label='Synthetic')\nplt.xlim(-26,0)\nsns.despine() \nplt.legend(ncol=1, loc='upper right')\nplt.xlabel('');","23b4b4dd":"\"\"\"\nHELPERS FOR THE NFL SURFACE ANALYSIS.\n\"\"\"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom tqdm import tqdm\n### Statistical Packages\n\nfrom scipy.stats import levene, ttest_ind, normaltest, wilcoxon, ranksums, mannwhitneyu\n\n\n###  OSCULATING RADIUS\ndef _m(x_start,y_start, x_end,y_end):\n    \"\"\"\n        Calculates the slope between two points\n    \"\"\"\n    return (y_end-y_start)\/(x_end-x_start)\n\ndef x_center(x1,x2,x3, y1,y2,y3):\n    \"\"\"\n        Calculates x-center of Osculating circle across 3 points \n    \"\"\"\n    m1 = _m(x1,y1,x2,y2) # slope point 1 to point 2\n    m2 = _m(x2,y2,x3,y3) # slope point 2 to point 3\n    \n    return ( m1*m2*(y1-y3) + m2*(x1+x2) - m1*(x2+x3) ) \/ ( 2*(m2-m1) )\n\ndef y_center(x1, x2, y1,y2, x_center):\n    \"\"\"\n        Calculates y-center of Osculating circle across 3 points\n    \"\"\"\n    m1 = _m(x1,y1, x2,y2)\n    \n    return -(1\/m1)*(x_center - (x1+x2)\/2) + (y1+y2)\/2\n\ndef distance(x1,y1, x2,y2):\n    \"\"\"\n        Eucledian Distance\n    \"\"\"\n    return np.sqrt((y2-y1)**2 + (x2-x1)**2)\n\n\ndef radius(x1,y1,x2,y2,x3,y3, max_number=None):\n    \"\"\"\n        Calculates the radius of oscilating circle using 3 points.\n    Params:\n    -------\n        - max_number (in case of a straight line)\n    \"\"\"    \n    max_number = max_number or np.inf\n    try:\n        x_c = x_center(x1,x2,x3, y1,y2,y3)\n    except ZeroDivisionError:\n        return np.inf\n      \n    y_c = y_center(x1,x2,y1,y2, x_c)\n    \n    return distance(x_c, y_c, x1, y1)\n\n\n### CENTRIPETAL ACCELERATION\ndef cent_acc(velocity, radius):\n    return (velocity**2)\/radius\n\n\n\n### ADJUSTED RAMER-DOUGLAS-PECKER #############################################\ndef euc_distance(a, b):\n    \"\"\"\n        Eucledian Distance.\n    \"\"\"\n    return np.sqrt((a[0] - b[0]) ** 2 + (a[1] - b[1]) ** 2)\n\n\ndef point_line_distance(point, start, end):\n    \"\"\"\n        Calculates distance from point to start+end straight line.\n    TODO:\n    -----\n        - debug and understand the formulas\n    \"\"\"\n    if (start == end):\n        return euc_distance(point, start)\n    else:\n        n = abs(\n            (end[0] - start[0]) * (start[1] - point[1]) -\n            (start[0] - point[0]) * (end[1] - start[1])\n        )\n        d = np.sqrt(\n            (end[0] - start[0]) ** 2 + (end[1] - start[1]) ** 2\n        )\n        return n \/ d\n\n    \ndef rdp(points, epsilon, reduce=True):\n    \"\"\"\n    Reduces a series of points to a simplified version that loses detail, but\n    maintains the general shape of the series.\n    Params:\n    -------\n        - epsilon: tolerance;\n        - reduce: whether to reduce vector size or replace with simpl. lines;\n    \"\"\"\n    dmax = 0.0\n    index = 0\n    \n    for i in range(1, len(points) - 1):\n        d = point_line_distance(points[i], points[0], points[-1])\n        if d > dmax:\n            index = i\n            dmax = d\n\n    if dmax >= epsilon:\n        results = rdp(points[:index+1], epsilon, reduce=reduce)[:-1] + \\\n                                    rdp(points[index:], epsilon, reduce=reduce)\n#         print (results)\n    else:\n        ### KEEP ONLY FIRST AND LAST POINT OF THE SEQUENCE IF NO POINT MEETS \n        ### THE CRITERIA OF GREATER THAN EPSILON (TOLERANCE)\n        if reduce:\n            results = [points[0], points[-1]]\n        \n        ### RE-INSERT VALUES USING SIMPLIFIED POINTS\n        else:\n            ### m,c  = create_eq(points[0][0], points[0][1], points[-1][0], points[-1][1])\n            ### m and c are the beta and alpha variables for the y=ax + b equation\n            ### we can shift the y vertically by applying it to the values\n            ### not the otimal solution, as proportions get distorted\n            results = []\n            for i in range(len(points)):\n                if i>0 and i<(len(points)-1):\n                    if points[0][0] == points[-1][0] and \\\n                                                points[0][1] == points[-1][1]:\n                        ## a == b, so c needs to be the same value.\n                        results.append([\n                            points[0][0], points[0][1] \n                        ])\n                    else:\n                        px, py = orthoProjection(\n                            points[0][0], points[0][1],\n                            points[-1][0], points[-1][1],\n                            points[i][0], points[i][1]\n                        )\n                        results.append([px, py])\n                else:\n                    results.append(points[i])\n        \n    return results\n\ndef orthoProjection(ax, ay, bx, by, cx, cy):\n    '''\n    Params:\n    -------\n        - ax, ay: first point coordinates;\n        - bx, by: last point coordinates;\n        - cx, cy: point outside of trajectory;\n    TODO:\n    -----\n        - debug and understand the formulas\n    Source:\n    -------\n        - https:\/\/stackoverflow.com\/questions\/55230528\/\\\n                                    find-point-where-altitude-meets-base-python\n    '''\n    abx = bx - ax\n    aby = by - ay\n    acx = cx - ax\n    acy = cy - ay\n    t = (abx * acx + aby * acy) \/ (abx * abx + aby * aby)\n    px = ax + (t * abx)\n    py = ay + (t * aby)\n    \n    return px, py\n\n\n### TURNING POINTS BEYOND THRESHOLD\ndef angle(vect):\n    \"\"\"\n    Source: https:\/\/stackoverflow.com\/questions\/14631776\/\\\n                    calculate-turning-points-pivot-points-in-trajectory-path\n    Returns the angles between vectors.\n\n    Parameters:\n    -----------\n        - dir is a 2D-array of shape (N,M) representing N vectors in \n        M-dimensional space.\n\n    The return value is a 1D-array of values of shape (N-1,), with each value\n    between 0 and pi.\n\n    0 implies the vectors point in the same direction\n    pi\/2 implies the vectors are orthogonal\n    pi implies the vectors point in opposite directions\n    \"\"\"\n    dir2 = vect[1:]\n    dir1 = vect[:-1]\n    theta = np.arccos((dir1*dir2).sum(axis=1)\/(\n                np.sqrt((dir1**2).sum(axis=1)*(dir2**2).sum(axis=1))\n            ))\n    \n    return theta\n\ndef angles(vect, padded=True):\n    \"\"\"\n    Returns angles across all vectors (np.float16).\n    \"\"\"\n    ### GET THE DIRECTION VECTORS. FOR X-Y COORDINATES, THIS HAS DIMENSION N-1\n    directions = np.diff(vect, axis=0)\n    \n    ### GET THE ANGLES OF DIRECTION COORDINATES.\n    ### IT RETURNS A DIMENSION N-1 AS WELL (or N_vect-2)\n    theta = angle(directions)\n    \n    ### Note: THE ANGLES THEREFORE IMPACT ALL POINTS BUT THE FIRST AND LAST\n    \n    if padded:\n        '''\n        Add pad to angles for first and last point.\n        We choose to add 0. as the value as 0. means no change in direction,\n        which would represent a similar significance for starting and end point\n        '''\n        return np.nan_to_num(\n                    np.pad(\n                        theta, pad_width=1, mode='constant', constant_values=0\n                    )\n                ,0).astype(np.float16)\n    \n    return theta\n\n\ndef _reduce_size(tracking, verbose=True):\n    if verbose: print ('Reduzing variable sizes ...')\n    ### REDUCING DATA SIZE\n    tracking['time'] = (tracking['time']).astype(np.float32)\n    tracking['x'] = tracking['x'].astype(np.float16)\n    tracking['y'] = tracking['y'].astype(np.float16)\n    tracking['dir'] = tracking['dir'].astype(np.float16)\n    tracking['dis'] = tracking['dis'].astype(np.float16)\n    tracking['o'] = tracking['o'].astype(np.float16)\n    tracking['s'] = tracking['s'].astype(np.float16)\n    return tracking\n\n\ndef _simplify_route(tracking, tolerance, verbose=True):\n    ### SIMPLIFY ROUTE\n    if verbose: print ('Simplifying Route ... ')\n    g = tracking.groupby('PlayKey').cumcount()\n    L = tracking[['PlayKey','x','y']].set_index(['PlayKey',g])\\\n            .groupby(level=0)\\\n            .apply(lambda x: x.values.tolist())\\\n            .tolist()\n    \n    new_coords = []\n    for my_seq in tqdm(L):\n        tmp = rdp(my_seq, epsilon=tolerance, reduce=False)\n        assert len(tmp)==len(my_seq)\n        new_coords.extend(tmp)\n    \n    simp = pd.DataFrame(new_coords, columns=[f'x_{tolerance}', f'y_{tolerance}'])\n    tracking = tracking.join(simp)\n    ###\n    return tracking\n    \n\n\ndef _get_angles(tracking, tolerance, verbose=True):\n    ### CALCULATE ANGLES\n    if verbose: print ('Calculating angles ...')\n    with np.errstate(divide='ignore',invalid='ignore'):\n        ser = tracking.groupby('PlayKey').apply(\n                    lambda x: angles(x[[f'x_{tolerance}', f'y_{tolerance}']].values)\n                )\n        \n    angless = pd.DataFrame(ser, columns=['angles']).explode('angles').reset_index()\n    tracking[f'angle_{tolerance}'] = angless['angles']\n    ###\n    return tracking\n\ndef _sharp_turns(tracking, tolerance, sharp_turn, verbose=True):\n    ### IDENTIFY SHARP TURNS\n    if verbose: print ('Calculating Sharp Turns ...')\n    tracking['sharp_turn'] = (\n                                tracking[f'angle_{tolerance}']>sharp_turn\n                            ).astype(np.int8)\n    ###\n    return tracking\n\ndef _euc_dist(tracking, verbose=True):\n    ### DISTANCE FROM PREVIOUS ROW\n    if verbose: print ('Calculating Euc Dist ...')\n    tracking['euc_dis'] = (np.sqrt(\n        (tracking.y - tracking.groupby('PlayKey').y.shift(1)) ** 2 + \n        (tracking.x - tracking.groupby('PlayKey').x.shift(1)) ** 2\n    ))\n    \n    tracking['euc_dis'] = tracking['euc_dis'].fillna(0).replace(np.inf, 0)\n    ###\n    ### CALCULATE CUMULATIVE DISTANCE\n    if verbose: print ('Calcualting Cum Dist ...')\n    tracking['cum_dis'] = tracking.groupby('PlayKey')['euc_dis'].cumsum()\n    ###\n    \n    return tracking\n\n\ndef _speed(tracking, max_speed=15, verbose=True):\n     ### CALCULATING VELOCITY\n    if verbose: print ('Calculating Velocity ...')\n    tracking['v'] = (tracking['euc_dis']\/((tracking['time'] - \\\n                        tracking.groupby('PlayKey')['time'].shift(1)))).fillna(0)\n    \n    \n    ### Replace very high calculated speeds with provided speed in dataset\n    if verbose: print (f'\\t Capping values to: {max_speed} ...')\n    tracking['v'] = np.where(tracking.v > max_speed, tracking.s, tracking.v)\n    tracking['v'] = np.where(tracking.v - tracking.s > 3, tracking.s, tracking.v)\n    ###\n    \n    ### Apply smoothing function to speed\n    if verbose: print ('\\t Smoothening Velocity ...')\n    tracking['v_sm'] = tracking.groupby('PlayKey')['v'].transform(\n        lambda x: x.rolling(window=5, center=True).mean().fillna(x)\n    )\n    tracking['v_sm'] = round(tracking.v_sm, 3)\n    ###\n    \n    ### Categorize speed\n    if verbose: print ('\\t Categorizing Velocity ...')\n    conditions = [\n        (tracking.v_sm < 0.25),\n        (tracking.v_sm >= 0.25) & (tracking.v_sm < 2),\n        (tracking.v_sm >= 2) & (tracking.v_sm < 3.5),\n        (tracking.v_sm >= 3.5) & (tracking.v_sm < 4.5),\n        (tracking.v_sm >= 4.5) & (tracking.v_sm < 6),\n        (tracking.v_sm >= 6)\n    ]\n    choices = [\n        'standing',\n        'walking',\n        'trotting',\n        'jogging',\n        'running',\n        'sprinting'\n    ]\n    tracking['s_cat'] = np.select(conditions, choices)\n    return tracking\n\n\ndef _acceleration(tracking, verbose=True):\n    ### ACCELERATION\n    if verbose: print ('Calculating Acceleration (second displacement derivative)...')\n    tracking['a'] = (\n            (tracking['v_sm'] - tracking.groupby('PlayKey')['v_sm'].shift(1))\/\\\n            ((tracking['time'] - tracking.groupby('PlayKey')['time'].shift(1)))\n    ).fillna(0)\n    return tracking\n\n\ndef _jerk(tracking, verbose=True):\n    ### Calculate jerk using 10 frames of acceleration\n    if verbose: print ('Calculating Jerk (third derivative)...')\n    tracking['jerk'] = (\n        tracking.a - tracking.groupby('PlayKey').a.shift(10)\n    ) \/ (\n        tracking.time - tracking.groupby('PlayKey').time.shift(10)\n    )\n    tracking.jerk.fillna(0, inplace=True)\n    tracking['jerk'] = round(tracking.jerk, 3)\n    ###\n    return tracking\n\ndef _o_radius(tracking, tolerance, tl, max_radius=200, verbose=True):\n    ### OSCULATING RADIUS\n    if verbose: print ('Calculating Osculating Radius ...')\n    tracking['radius'] = radius(\n        tracking.groupby('PlayKey')['x'].shift(tl), \n        tracking.groupby('PlayKey')['y'].shift(tl),\n        tracking['x'], \n        tracking['y'],\n        tracking.groupby('PlayKey')['x'].shift(-tl), \n        tracking.groupby('PlayKey')['y'].shift(-tl)\n    ).fillna(np.inf).astype(np.float16)\n    \n    tracking[f'radius_{tolerance}'] = radius(\n        tracking.groupby('PlayKey')[f'x_{tolerance}'].shift(tl), \n        tracking.groupby('PlayKey')[f'y_{tolerance}'].shift(tl),\n        tracking[f'x_{tolerance}'], \n        tracking[f'y_{tolerance}'],\n        tracking.groupby('PlayKey')[f'x_{tolerance}'].shift(-tl), \n        tracking.groupby('PlayKey')[f'y_{tolerance}'].shift(-tl)\n    ).fillna(np.inf).astype(np.float16)\n    \n    #tracking['radius'] = tracking['radius']\n    #tracking[f'radius_{tolerance}'] = tracking[f'radius_{tolerance}']\n    ###\n    ###REPLACE INFINITE RADIUS WITH MAX_RADIUS\n    #tracking[f'radius_{tolerance}'] =  tracking[f'radius_{tolerance}'].clip(0,max_radius)\n    ###\n    return tracking\n\ndef _cent_acc(tracking, tolerance, max_cent_acc, verbose=True):\n    ### CENTRIPETAL ACCELERATION\n    if verbose: print ('Calculating Centripetal Acceleration ...')\n    tracking['cent_acc'] = cent_acc(\n                                tracking['v_sm'], tracking['radius']\n                        ).fillna(0).clip(0,max_cent_acc).astype(np.float16) \n    tracking[f'cent_acc_{tolerance}'] = cent_acc(\n                                tracking['v_sm'], tracking[f'radius_{tolerance}']\n                        ).fillna(0).clip(0,max_cent_acc).astype(np.float16) \n    \n    ###\n    return tracking\n\ndef _coo_cod(tracking,verbose=True):\n    ### MARK CHANGE OF DIRECTION AND CHANGE OF ORIENTATION\n    # Adjusted COD\n    if verbose: print ('Calculating Adjusted COD ...')\n    tracking['last_dir'] = tracking.groupby('PlayKey')['dir'].shift(1).fillna(0)\n    tracking['cod'] = np.abs(np.where(\n        abs(tracking['dir'] - tracking['last_dir']) < 180,\n        tracking['dir'] - tracking['last_dir'],\n        np.where(\n            tracking['last_dir'] > 180,\n            tracking['dir'] + (360 - tracking['last_dir']),\n            -((360 - tracking['dir']) + tracking['last_dir'])\n        )\n    ))\n    \n    # Adjusted COO\n    if verbose: print ('Calculating Adjusted COO ...')\n    tracking['last_o'] = tracking.groupby('PlayKey')['o'].shift(1).fillna(0)\n    tracking['coo'] = np.abs(np.where(\n        abs(tracking['o'] - tracking['last_o']) < 180,\n        tracking['o'] - tracking['last_o'],\n        np.where(\n            tracking['last_o'] > 180,\n            tracking['o'] + (360 - tracking['last_o']),\n            -((360 - tracking['o']) + tracking['last_o'])\n        )\n    ))\n    \n    tracking.drop(['last_o','last_dir'], axis=1, inplace=True)\n    \n    if verbose: print ('\\t Calculating coo vs cod ...')\n    tracking['cod_coo']= np.abs(tracking['coo'] - tracking['cod'])\n    \n    ### tracking['cod_coo'] = np.abs(180-np.abs(180-np.abs(tracking['cod']-tracking['coo'])))\n    ###\n    return tracking\n\ndef _snap_timeout(tracking, verbose=True):\n    if verbose: print ('Removing frames before snap ...')\n    if verbose: print (f'\\t Len tracking: {len(tracking)}')\n    if verbose: print (f'\\t Unique plays ids: {tracking[\"PlayKey\"].nunique()}')\n    ### REMOVE BEFORE BALL SNAP:\n    ### events identified as \"snap\": ['ball_snap','kickoff','snap_direct','kickoff_play','free_kick_play','free_kick']\n    tracking = tracking.sort_values(['PlayKey','time']).reset_index(drop=True)\n    \n    before = tracking[tracking['event']\\\n                .isin(['ball_snap','kickoff','snap_direct','kickoff_play','free_kick_play','free_kick'])]\\\n                [['PlayKey']].drop_duplicates()\n    before['index'] = before.index\n    \n    assert len(before) == before['PlayKey'].nunique()\n    \n    tracking = tracking.merge(before, on='PlayKey', how='left')\n    tracking['index'] = tracking['index'].fillna(0)\n    \n    tracking = tracking[tracking.index >= tracking['index']]\n    tracking.drop('index', inplace=True, axis=1)\n    if verbose: print (f'\\t Len tracking removing snaps: {len(tracking)}')\n    if verbose: print (f'\\t Unique plays ids: {tracking[\"PlayKey\"].nunique()}')\n    ###\n    \n    tracking = tracking.reset_index(drop=True)\n    \n    ### REMOVE AFTER TIMEOUTS\n    if verbose: print ('Removing frames after timeouts ...')\n    after = tracking[tracking['event'].isin([\n        'timeout_tv', 'timeout', 'timeout_away', 'timeout_booth_review', 'timeout_home',\n        'timeout_quarter', 'timeout_halftime', 'two_minute_warning', 'play_submit\\t'\n    ])][['PlayKey']].drop_duplicates()\n    after['index'] = after.index\n    \n    assert len(after) == after['PlayKey'].nunique()\n    \n    tracking = tracking.merge(after, on='PlayKey', how='left')\n    tracking['index'] = tracking['index'].fillna(len(tracking)+1)\n    \n    tracking = tracking[tracking.index <= tracking['index']]\n    tracking.drop('index', inplace=True, axis=1)\n    if verbose: print (f'\\t Len tracking removing timeouts: {len(tracking)}')\n    if verbose: print (f'\\t Unique plays ids: {tracking[\"PlayKey\"].nunique()}')\n    ###\n    return tracking\n\n\n### CLEAN WEATHER DATA\ndef _clean_weather(plays):\n    plays['Weather'] = plays['Weather'].str.lower()\n    plays['Weather'] = plays['Weather'].replace({\n        # Clear weather\n        'clear and cool': 'clear',\n        'clear and cold': 'clear',\n        'clear and warm': 'clear',\n        'sunny and warm': 'clear',\n        'sunny and cold': 'clear',\n        'sunny, windy': 'clear',\n        'clear and sunny': 'clear',\n        'sunny, highs to upper 80s': 'clear',\n        'fair': 'clear',\n        'heat index 95': 'clear',\n        'sunny': 'clear',\n        'clear skies': 'clear',\n        'sunny and clear': 'clear',\n        'sunny skies': 'clear',\n        'mostly sunny skies': 'clear',\n        'party cloudy': 'clear',\n        'clear to partly cloudy': 'clear',\n        'partly clouidy': 'clear',\n        'partly cloudy': 'clear',\n        'mostly sunny': 'clear',\n        # Rain\n        'showers': 'rain',\n        'scattered showers': 'rain',\n        'light rain': 'rain',\n        'rainy': 'rain',\n        'rain shower': 'rain',\n        'cloudy with periods of rain, thunder possible. winds shifting to wnw, 10-20 mph.': 'rain',\n        'cloudy, rain': 'rain',\n        'cloudy, 50% change of rain': 'rain',\n        'rain likely, temps in low 40s.': 'rain',\n        # Cloudy\n        'sun & clouds': 'cloudy',\n        'partly clear': 'cloudy',\n        'partly sunny': 'cloudy',\n        'coudy': 'cloudy',\n        'cloudy and cool': 'cloudy',\n        'cloudy and cold': 'cloudy',\n        'mostly coudy': 'cloudy',\n        'mostly cloudy': 'cloudy',\n        'cloudy, fog started developing in 2nd quarter': 'cloudy',\n        'overcast': 'cloudy',\n        'rain chance 40%': 'cloudy',\n        '30% chance of rain': 'cloudy',\n        '10% chance of rain': 'cloudy',\n        'cloudy, chance of rain': 'cloudy',\n        'cloudy, light snow accumulating 1-3\"': 'snow',\n        'heavy lake effect snow': 'snow',\n        # Indoor\n        'controlled climate': 'indoor',\n        'indoors': 'indoor',\n        'n\/a indoor': 'indoor',\n        'n\/a (indoors)': 'indoor',\n        # Unknown\n        'cold': 'unknown',\n        'hazy': 'unknown',\n        np.nan: 'unknown'\n    }).fillna('unknown')\n    \n    return plays\n    \ndef _clean_stadium_type(plays):\n    plays['StadiumType'] = plays['StadiumType'].replace({\n        'Oudoor': 'Outdoor',\n        'Outdoors': 'Outdoor',\n        'Ourdoor': 'Outdoor',\n        'Outddors': 'Outdoor',\n        'Outdor': 'Outdoor',\n        'Outside': 'Outdoor',\n        'Outdoor Retr Roof-Open': 'Outdoor',\n        'Retr. Roof - Open': 'Outdoor',\n        'Retr. Roof-Open': 'Outdoor',\n        'Domed, Open': 'Outdoor',\n        'Domed, open': 'Outdoor',\n        'Open': 'Outdoor',\n        'Indoor, Open Roof': 'Outdoor',\n        'Heinz Field': 'Outdoor',\n        'Bowl': 'Outdoor',\n        'Retractable Roof': 'Outdoor',\n        'Indoors': 'Indoor',\n        'Closed Dome': 'Indoor',\n        'Domed, closed': 'Indoor',\n        'Retr. Roof-Closed': 'Indoor',\n        'Indoor, Roof Closed': 'Indoor',\n        'Retr. Roof Closed': 'Indoor',\n        'Retr. Roof - Closed': 'Indoor',\n        'Dome, closed': 'Indoor',\n        'Dome': 'Indoor',\n        'Domed': 'Indoor',\n        'Cloudy': 'Unknown'\n    })\n    return plays\n\ndef _clean_play_type(plays, verbose=True):\n    if verbose: print ('Cleaning play types ... ')\n    plays['PlayType'] = plays['PlayType'].replace({\n        'Kickoff Not Returned': 'Punt or Kickoff Not Returned',\n        'Kickoff Returned': 'Punt or Kickoff Returned',\n        'Punt Returned': 'Punt or Kickoff Not Returned',\n        'Punt Not Returned': 'Punt or Kickoff Returned',\n        'Extra Point': 'Field Goal',\n        '0': 'Unknown',\n        np.nan: 'Unknown'\n    })\n    return plays\n\n\ndef _rest_days(plays, verbose=True):\n    ### CALCULATING DAYS OF REST\n    if verbose: print ('Calculating rest days ... ')\n    plays['season'] = np.where(plays.PlayerDay < 200, 1, 2)\n    player_days = plays[['PlayerKey', 'PlayerDay', 'season']].drop_duplicates().sort_values(['PlayerKey', 'PlayerDay'])\n    player_days['days_rest'] = player_days.PlayerDay - player_days.groupby(['PlayerKey', 'season']).shift(1).PlayerDay\n    \n    plays = plays.merge(player_days[['PlayerKey', 'PlayerDay', 'days_rest']], on=['PlayerKey', 'PlayerDay'])\n    plays.days_rest.fillna(plays.groupby('season').days_rest.transform('max'), inplace=True)\n    ###\n    return plays\n\ndef _app_v(tracking, tolerance, verbose=True):\n    ### CALCULATE APPROACH VELOCITY (WINDOW=5)\n    if verbose: print ('Calculating approach velocity (window=5) ... ')\n    tracking['app_v'] = tracking.groupby('PlayKey')['v_sm'].rolling(window=5).mean().values\n    # take the approach velocity as the velocity immediatly before the curve\n    tracking['app_v'] = tracking['app_v'].shift(1)\n    tracking.loc[\n        tracking[tracking[f'angle_{tolerance}']<=0].index, 'app_v'         \n    ] = np.nan\n    ###\n    return tracking\n\ndef _app_a(tracking, tolerance, verbose=True):\n    if verbose: print ('Calculating approach accelration (window=5) ... ')\n    tracking['app_a'] = ( tracking.groupby('PlayKey')['a'].shift(1) -\\\n                            tracking.groupby('PlayKey')['a'].shift(6) ) \/\\\n            (tracking.groupby('PlayKey')['time'].shift(1) - \\\n             tracking.groupby('PlayKey')['time'].shift(6) )\n            \n    tracking.loc[\n        tracking[tracking[f'angle_{tolerance}']<=0].index, 'app_a'         \n    ] = np.nan\n    return tracking\n\n\n### LOAD DATA FOR SPECIFIC POSITIONS ONLY\ndef load_position_data(path = 'data\/', position = 'Linebacker', tolerance = 0.25, \n                       sharp_turn = 0.75, max_radius = np.inf, max_cent_acc = 20,\n                       max_speed = 15, time_lapse_deciseconds =1, verbose=True):\n    \n    # silence runtime warning on division by zero \/ nan\n    #with NP.errstate(divide='ignore',invalid='ignore'):\n    \"\"\"\n    Parameters:\n    -----------\n        - path = 'data\/' ## data path\n        - position = 'Linebacker' ## position to analyze\n        - tolerance = 0.25 ## tolerance in yards for RDP algorithm simplifcation\n        - sharp_turn = 0.75 ## min threshold to consider for a sharp turn (in radians)\n        - max_radius = np.inf ## maximum osculating radius for straight lines (infinity)\n        - max_cent_acc = 20 ## maximum centripetal acceleration accepted (for outliers)\n        - max_speed = 15 ## maximum speed in yards\/s to cap data (for outliers)\n        - time_lapse_deciseconds =1 ## number of frames to use for osc radius calculation\n    \"\"\"\n    \n    ### LOAD DATASETS\n    if verbose: print ('Loading datasets ...')\n    injuries = pd.read_csv(f'{path}InjuryRecord.csv')\n    plays = pd.read_csv(f'{path}PlayList.csv')\n    tracking = pd.read_csv(f'{path}PlayerTrackData.csv')\n    \n    plays = _clean_weather(plays)\n    plays = _clean_stadium_type(plays)\n    \n    tracking = _reduce_size(tracking, verbose=verbose)\n    \n    ### FILTER POSITION\n    if position:\n        if verbose: print (f'Filtering only for {position} ...')\n        ### FILTER BY POSITION (to ease on data analysis)\n        plays = plays[plays['RosterPosition']==position]\n        injuries = injuries[injuries['PlayerKey'].isin(plays['PlayerKey'])]\n        \n    tracking = tracking.merge(plays[['PlayKey','PlayerKey']], on='PlayKey')\n    \n    ### SORT TRACKING (NEEDED)\n    if verbose: print ('Sorting for play and time (ascending) ...')\n    tracking = tracking.sort_values(['PlayKey','time']).reset_index(drop=True)\n    ###\n    \n    ### filter out the sketchy play with weird speed: \n    tracking = tracking[tracking['PlayKey'] != '38214-18-30']\n    ###\n    \n    tracking = _simplify_route(tracking, tolerance=tolerance, verbose=verbose)\n    tracking = _get_angles(tracking, tolerance=tolerance, verbose=verbose)\n    tracking = _sharp_turns(tracking,  tolerance=tolerance, sharp_turn=sharp_turn, verbose=verbose)\n    tracking = _euc_dist(tracking, verbose=verbose)\n    tracking = _speed(tracking, max_speed=max_speed, verbose=verbose)\n    tracking = _acceleration(tracking, verbose=verbose)\n    tracking = _jerk(tracking, verbose=verbose)\n    tracking = _o_radius(tracking, tolerance=tolerance, tl=time_lapse_deciseconds, max_radius=max_radius, verbose=verbose)\n    tracking = _cent_acc(tracking, tolerance=tolerance, max_cent_acc=max_cent_acc, verbose=verbose)\n    tracking = _coo_cod(tracking,verbose=verbose)\n    tracking = _snap_timeout(tracking, verbose=verbose)\n    plays = _rest_days(plays, verbose=verbose)\n    plays = _clean_play_type(plays, verbose=verbose)\n    tracking = _app_v(tracking, tolerance=tolerance, verbose=verbose)\n    tracking = _app_a(tracking, tolerance=tolerance, verbose=verbose)\n    \n    ### MARK INJURY PLAYS\n    #player who eventually gets injured\n    if verbose: print ('Adding Flags for injury ...')\n    tracking['is_injury_player'] = (tracking['PlayerKey'].isin(injuries['PlayerKey'])\n                                                                ).astype(np.int8)\n    \n    tracking['game_id'] = tracking['PlayKey'].str.extract('(^\\d+-\\d+)-')\n    tracking['is_injury_game'] =  (tracking['game_id'].isin(injuries['GameID'])\n                                                                ).astype(np.int8)\n    #play in which player gets injured\n    tracking['is_injury'] = (tracking['PlayKey'].isin(injuries['PlayKey'])\n                                                                ).astype(np.int8)\n    ####\n    \n    ### FILL NULL EVENTS WITH LATEST EVENT\n    tracking['event'] = tracking.groupby('PlayKey')['event'].fillna(method='ffill')\n    ###\n    \n    ### ADD BOOLEAN WHETHER ANGLE EXISTS (ANY TURN)\n    tracking['tp'] = (tracking[f'angle_{tolerance}']>0).astype(int)\n    ###\n    \n    ### MERGE TRACKING WITH PLAYS\n    if verbose: print ('Merging with plays ... ')\n    tracking = tracking.merge(\n            plays[[\n                'GameID', 'PlayKey','RosterPosition','PlayerDay',\n                'PlayerGame','StadiumType','FieldType','Temperature','Weather',\n                'PlayType','PlayerGamePlay', 'days_rest'\n            ]], \n            how='left', on='PlayKey'\n    )\n    ###\n    \n    ### DANGEROUS TURN\n    tracking['dangerous_acc'] = (tracking[f'cent_acc_{tolerance}'] > 10).astype(np.int8)\n    tracking['dangerous_turn'] = ((tracking['sharp_turn']>0)&(tracking['dangerous_acc']>0)).astype(np.int8)\n        \n    \n    tracking = tracking.sort_values(['game_id','PlayerGamePlay']).reset_index(drop=True)\n\n    ### CALCULATE CUMULATIVE VARS\n    #tracking['cum_tp'] = tracking.groupby('game_id')['tp'].cumsum()\n    #tracking['cum_dangerous_acc'] = tracking.groupby('game_id')['dangerous_acc'].cumsum()\n    #tracking['cum_dangerous_turn'] = tracking.groupby('game_id')['dangerous_turn'].cumsum()\n    #tracking['cum_sharp_turn'] = tracking.groupby('game_id')['sharp_turn'].cumsum()\n    ###\n    \n    return tracking, injuries, plays\n\n\ndef summarize(tracking, injuries, tolerance):\n    \n    tracking[f'angle_{tolerance}'] = tracking[f'angle_{tolerance}'].astype(float)\n    tracking['StadiumType'] = tracking['StadiumType'].fillna('Unknown')\n    tracking['PlayType'] = tracking['PlayType'].fillna('Unknown')\n    \n    def rng(val):\n        \"\"\"Range between min and max\"\"\"\n        return np.max(val) - np.min(val)\n    \n    summary = tracking.groupby([\n            'PlayKey','PlayerKey','PlayerDay','game_id','RosterPosition',\n            'PlayerGame','StadiumType','FieldType','Temperature','Weather',\n            'PlayType','PlayerGamePlay','is_injury', 'is_injury_player', 'is_injury_game'\n        ]).agg({\n            'a':['mean','median','std','max','min'],\n            'v_sm':['mean','median','std','max','min'],\n            'o':['mean','median','std','max','min'],\n            'dir':['mean','median','std','max','min'],\n            'cent_acc':['mean','median','std', 'max','min'],\n            f'cent_acc_{tolerance}':['mean','median','std', 'max','min'],\n            'coo':['mean','median','std','max','min'],\n            'cod':['mean','median','std','max','min'],\n            'radius':['mean','median','std','max','min'],\n            'jerk':['mean','median','std','max','min'],\n            'radius_0.25':['mean','median','std','max','min'],\n            'cod_coo':['mean','median','std','max','min'],\n            'time':  rng,\n            'euc_dis': 'sum',\n            'sharp_turn': 'sum',\n            f'angle_{tolerance}': 'mean',\n            'tp': 'sum',\n            'dangerous_acc': 'sum',\n            'dangerous_turn': 'sum',\n            'app_v': ['mean', 'max', 'min'],\n            'days_rest': 'mean'\n        })\n    summary.columns = [f'{i[0]}_{i[1]}' for i in summary.columns]\n    summary = summary.reset_index()\n    \n    assert summary['PlayKey'].nunique() == tracking['PlayKey'].nunique()\n    \n    ### MARK LAST PLAY FOR INJURIES AS THE INJURY PLAY (ASSUMPTION)\n    ix = summary[summary['game_id'].isin(\n                injuries[injuries['PlayKey'].isnull()].GameID\n            )].groupby(['game_id']).agg(\n                {'PlayerGamePlay': 'idxmax'}\n            )['PlayerGamePlay']\n    \n    summary.loc[ix, 'is_injury'] = 1\n    summary['rain'] = (summary.Weather == 'rain').astype(int)\n    ###\n    return summary\n\n\n### PLOT PLAY\ndef plot_play(df, injuries, play=None, tolerance=0.25):\n    '''\n    df is the tracking data\n    '''\n    if not play:\n        play = np.random.choice(injuries[\n                injuries['PlayKey'].isin(df['PlayKey'])==True\n        ]['PlayKey'].unique())\n        \n    print (f'Play selected: {play}')\n    print (injuries[injuries['PlayKey'] == play])\n        \n    sample = df[df.PlayKey == play]\n    \n    ix_ball_snap = sample[sample['event']=='ball_snap'].index[0] ## 4 == ball_snap\n    sample = sample[sample.index>=ix_ball_snap]\n    \n    turns = sample[(sample['sharp_turn']==1)&(sample['event']!='huddle_start_offense')]\n\n    plt.figure(figsize=(16,8))\n\n    plt.subplot(2, 2, 1)\n    plt.title('Original Route', fontsize=16)\n    plt.plot(sample['x'], sample['y'], label='Route')\n    plt.plot(sample[f'x_{tolerance}'], sample[f'y_{tolerance}'], label='Simplified Route')\n    plt.scatter(sample.x, sample.y, alpha=0.3)\n    plt.scatter(sample[:1].x, sample[:1].y, color=\"green\", s=[50], alpha=0.7, label='start')\n    plt.scatter(sample[-1:].x, sample[-1:].y, color=\"red\", s=[50], alpha=0.7, label='end')\n    plt.scatter(\n            turns['x'], \n            turns['y'], label='Sharp Turn', c=turns['v'], s=[200], alpha=0.7);\n    cbar = plt.colorbar()\n    cbar.ax.set_ylabel('Speed on sharp turns', rotation=90)\n    plt.legend()\n\n    plt.subplot(2, 2, 2)\n\n    neg_acc = sample.copy()\n    neg_acc.loc[neg_acc[neg_acc['a']>=0].index, 'a'] = np.nan\n\n    plt.title('Acceleration vs Velocity', fontsize=16)\n    plt.plot(sample.time, sample.a, label='Acceleration')\n    plt.plot(sample['time'], sample['a'].rolling(window=10).mean(), label='Acceleration ma 10')\n    plt.plot(neg_acc['time'], neg_acc['a'], c='red')\n    plt.plot(sample['time'], sample['v_sm'], label='Velocity (sm)')\n    plt.xticks([i for i in sample['time'] if i%4==0], rotation='vertical', fontsize=8)\n    plt.legend()\n\n    plt.subplot(2,2,3)\n\n    plt.title('Direction vs. Orientation', fontsize=16)\n    plt.plot(sample['time'], sample['o']-sample['o'].shift(1), label='Orientation')\n    plt.plot(sample['time'], sample['dir']-sample['dir'].shift(1), label='Direction')\n    plt.xticks([i for i in sample['time'] if i%4==0], rotation='vertical', fontsize=8);\n    plt.legend()\n\n    plt.subplot(2,2,4)\n    plt.title('Cent. Acceleration vs. Osc. Radius', fontsize=16)\n    plt.plot(sample['time'], sample[f'cent_acc_{tolerance}'], label='Cent. Acceleration')\n    plt.plot(sample['time'], (1\/sample[f'radius_{tolerance}'])*10, label='Osculating radius')\n    plt.xticks([i for i in sample['time'] if i%4==0], rotation='vertical', fontsize=8);\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n    \n    plt.figure(figsize=(12,16))\n    plt.subplot(6,1,1)\n    plt.plot(sample['time'], sample['a'], label='Acceleration')\n    plt.plot(sample['time'], sample['a'].rolling(window=10).mean(), label='Acceleration ma 10')\n    plt.plot(sample['time'], sample['a'].ewm(span=10,adjust=False).mean(), label='Acceleration ewm')\n    plt.scatter(\n            sample[sample['sharp_turn']>0]['time'], \n            sample[sample['sharp_turn']>0]['a'], label='Sharp Turn', s=[200], alpha=0.7);\n\n    plt.xticks([i for i in sample['time'] if i%4==0], rotation='vertical', fontsize=8)\n\n    plt.legend()\n    plt.subplot(6,1,2)\n    plt.plot(sample['time'], sample['v_sm']*3.292, label='Velocity (Km\/hr)')\n    plt.plot(sample['time'], sample['s']*3.292, label='Speed (Km\/hr)')\n    plt.scatter(\n            sample[sample['sharp_turn']>0]['time'], \n            sample[sample['sharp_turn']>0]['v_sm']*3.292, label='Sharp Turn', s=[200], alpha=0.7);\n    plt.xticks([i for i in sample['time'] if i%4==0], rotation='vertical', fontsize=8)\n    plt.legend()\n    \n    plt.subplot(6,1,3)\n    plt.plot(sample['time'], sample[f'cent_acc_{tolerance}'], label='Cent. Acceleration Simplified')\n    plt.plot(sample['time'], sample['cent_acc'], label='Cent. Acceleration')\n    plt.scatter(\n            sample[sample['sharp_turn']>0]['time'], \n            sample[sample['sharp_turn']>0][f'cent_acc_{tolerance}'], label='Sharp Turn', s=[200], alpha=0.7);\n    plt.xticks([i for i in sample['time'] if i%4==0], rotation='vertical', fontsize=8)\n    plt.legend()\n    \n    plt.subplot(6,1,4)\n    plt.scatter(\n            sample[sample['sharp_turn']>0]['time'], \n            1\/sample[sample['sharp_turn']>0][f'radius_{tolerance}'], label='Sharp Turn', s=[200], alpha=0.7);\n    plt.plot(sample.time, (1\/sample[f'radius_{tolerance}']), label='Osculating radius Simplified (1\/r)')\n    plt.plot(sample.time, (1\/sample['radius']), label='Osculating radius (1\/r)')\n    plt.xticks([i for i in sample['time'] if i%4==0], rotation='vertical', fontsize=8)\n    plt.legend();\n    \n    plt.subplot(6,1,5)\n    plt.plot(sample['time'], sample['dir'], label='Direction')\n    plt.plot(sample['time'], sample['dir']-sample['dir'].shift(1), label='Direction')\n    plt.xticks([i for i in sample['time'] if i%4==0], rotation='vertical', fontsize=8)\n    plt.legend()\n    \n    plt.subplot(6,1,6)\n    plt.plot(sample['time'], sample['o'], label='Orientation')\n    plt.plot(sample['time'], sample['o']-sample['o'].shift(1), label='Orientation')\n    plt.xticks([i for i in sample['time'] if i%4==0], rotation='vertical', fontsize=8)\n    plt.legend()\n    plt.show();\n    \n### Function for Permutation Test\ndef exact_mc_perm_test(xs, ys, nmc):\n    \"\"\"\n    Monte-Carlo Simulation for permutation test\n    Params\n    ------\n        - xs: sample1\n        - ys: sample2\n        - nmc: iterations\n    \"\"\"\n    n, k = len(xs), 0\n    diff = np.abs(np.mean(xs) - np.mean(ys))\n    zs = np.concatenate([xs, ys])\n    for j in range(nmc):\n        np.random.shuffle(zs)\n        k += diff <= np.abs(np.mean(zs[:n]) - np.mean(zs[n:]))\n    return k \/ nmc\n\n\ndef mc_ranksum_sampled(xs, pop, nmc, func, **kwargs):\n    \"\"\"\n    Code to compare sample to multiple draws of population\n    \n    Params:\n    -------\n        - xs: smaller sample\n        - pop: population\n        - func: wilcoxon \/ ttest_ind \/  ranksums \/ mannwhitneyu\n        - nmc: iterations\n    \"\"\"\n    n, k = len(xs), []\n    for j in range(nmc):\n        zc = np.random.choice(pop, size=n)\n        k.append(func(xs, zc, **kwargs)[1])\n\n    ## bonfarroni correction\n    return np.array(k)","1a004cd6":"For every three points A (x1, y1), B (x2, y2), C (x3, y3), we can calculate D (xc,yc):\n\n![image.png](attachment:image.png)\n\n$x_c = \\dfrac{m_1m_2(y_1-y_3) + m_2(x_1+x_2) - m_1(x_2+x_3)}{2(m_2-m_1)}$\n\n$y_c = y_{Perp} = - \\dfrac{1}{m_1}(x_c - \\dfrac{x_1+x_2}{2}) + \\dfrac{y_1 + y_2}{2} $\n\nwhere\n\n$m1= \\dfrac{\\Delta y_{AB}}{\\Delta x_{AB}}$ ; $m2= \\dfrac{\\Delta y_{BC}}{\\Delta x_{BC}}$\n\nThen we can calculate the Eucledian distance from D to any of the other points to get the radius, e.g. point A.\n\n$ OR = \\sqrt[2]{(y_1-y_c)^2 + (x_1 - x_c)^2} $\n\nAnd the centripetal acceleration AC:\n\n$ AC = \\dfrac{v^2}{OR} $\n\nLastly, we can estimate the approach velocity to understand the conditions in which players start their turns:\n\n$ V_{app} = \\dfrac{\\sum_{i=1}^{n}v_i}{n} $  \n\nwhere $n$ is the window of previous frames to average.\n\n","1b3d390f":"## Methodology: Data Treatment \n\nOne of the key indicators of stress on lower limb joints relates to lateral and horizontal movements, largely associated to change of directions and how players perform such turns. In order to accurately understand the real change in direction, we need to be able to capture players' geospacial movement that involves lateral force, while ignoring the noise from small oscilations in coordinations for each decisecond captured. Not all small change in the x and y coordinates in the tracking data _actually_ represent a movement from the body that may result in [hinge] joint pressure or tension. Because our injuries are primarily in hinge joints, we want to be able to capture movements that impact the lower body more so than the ball-and-socket joints (shoulders and hips). \n\n### Simplified Routes\n\nWe decided it could be benefitial to smoothen the routes performed by players, particularly with the interest to capture change of directions that were impactful for the biomechanical body parts hereby studied. In order to do so, we used an adjusted version of the Ramer-Douglas-Pecker algorithm with a tolerance of +-0.25 yards (+-9 inches). The original algorithm implementation removes points that are within a tolerance, or epsilon, of the major direction changes. One of the most common implementations is via a recursive approach that starts with the most distant point from the start and end points and then removes all points that are within the tolerance level. If a point falls without, the algorithm moves to that point and perform the same reduction, all the way until all data has been reduced.\n\nIn our implementation, we modified the algorithm to replace the coordinates that were supposed to be eliminated with the orthogonal projection of those points in the simplified line. The replacement was chosen in order to maintain a final vector of the same lenght as the original vector, thus keeping the rest of the tracking data associated with a location in time. We considered both the orthogonal projection as well as a one-axis displacement of the coordinates in order to just fall in the simplified line. The orthogonal projection keeps the best amount of similarity to the original route by maintaining the proportional distance between the adjacent points, thus being preferred for points replacement.\n\nIn the figure below we can understand the difference between original route, simplified and reduced (original implementation), and simplified with replacement (orthogonal projection onto reduced route).\n\n![image.png](attachment:image.png)","e3141731":"## Logistic Regression\n\nAfter the 2-step variable filtering process, we were left with the following variables:\n\n\n- Centripetal Acceleration for simplified route (mean)\n- Change of Direction (mean)\n- Jerk (std and min)\n- Change of direction vs. change of orientation (mean, std and max)\n- Sharp turn\n- Turning points\n- Approach velocity (max, std)\n- Natural Field\n- Rain\n- Position Dummies (Quarterback, Tight End, etc)\n\nAfter oversampling, standardization and training the data with 70% of the oversampled data, we generated a logistic model with ana accuracy rate of 88.18% and AUC ROC of 0.95 (See figures below).","c5ca4478":"# Conclusion and Implications\n\nIn the present work we analyzed the dynamics of player movements in relationship to injury likelihood. We found that:\n- Average approach velocity and centripetal acceleration differ for plays that end up in injuries. This could point to specific biomechanics that differ on a player level (how players perform a turn during a play) as well as to how players who end-up injured performed wrong turns. It is hard to grasp the causation link, and further research is recommended, particularly on player mechanics.\n- There is little evidence that field type per se links to higher injury rates, but combined with other variables, as well as to position and weather categories, it shows significance in affecting injury likelihood. Synthetic turf increases the likelihood of injuries if we hold all other variables constant.\n- The number of sharp turns and the change between orientation and direction impact considerably the likelihood of injury. Given the theoretical background of hinge joints, this should be an easy target for improvement in training and conditioning. Equipment to limit body rotation might be useful in preventing lower-limb injuries.\n","ea35f4ac":"### Sharp Turns\n\nNot only were we interested in identifying the centripetal acceleration exerced in turns, we also calculated the angle for each turn after the routes were simplified, and defined that any turn with an inner angle lower than 45 degrees (based on the literature aforementioned) should be considered a sharp turn. \n\nThe external angles could be calculated using the dot product of the vectors connecting three adjacent points:\n\n$\\theta = \\arccos \\left( \\frac{\\vec{AB}\\cdot \\vec{BC}}{ \\|\\vec{AB}\\| \\, \\|\\vec{BC}\\|}\\right).$\n\nBecause our final vector of angles will have length $N-2$, where the two missing points are the start and end of the play, we decided to pad the extremities with 0 (zero) to return the same size vector as the one used for the calculation. Also, conceptually, because on the first and last point there are no movement, the choice of 0 for the angle value does not misrepresent the data, and it could be analogous to the lack of a value.\n\nThe definition of a sharp turn was then calculated by whether the angle (in radians) was greater than 0.75 (~43 degrees)","74638ac0":"### Maximum Approach Acceleration (For Turns Only)\n\nSummary of statistical results:\n\n| Wilcoxon Signed Rank (MCS) | Welch's t-test | TTest (MCS) | Wilcoxon Ranksum | Wilcoxon Ranksum (MCS) | Mann\u2013Whitney | Mann\u2013Whitney (MCS) |\n|:-|:-|:-|:-|:-|:-|:-|\n| 0.6743 | 0.0542 | 0.7477 | 0.0134 | 0.6190 | 0.0067 | 0.4432 |","e1cc6f2f":"### Change of Direction vs. Change of Orientation (Mean)\n\nAnother important variable we looked at was the difference in the change of direction in comparison to the difference in the change of orientation (current frame past prior frame). In other words, how much did the direction change more - or less - than the orientation (or we can think of it as an asymetric rotation of the body). As we can see, we also idenfied that there was a larger absolute mean difference in asymetric rotation for plays with an injury in comparison to those without.\n\nSummary of statistical results:\n\n| Wilcoxon Signed Rank (MCS) | Welch's t-test | TTest (MCS) | Wilcoxon Ranksum | Wilcoxon Ranksum (MCS) | Mann\u2013Whitney | Mann\u2013Whitney (MCS) |\n|:-|:-|:-|:-|:-|:-|:-|\n| 0.0623 | <0.001 | 0.1055 | <0.001 | 0.0546 | <0.001 | 0.0162 |","e5322c34":"## Helper Code","fa7cd1a6":"### Centripetal Acceleration\n\nOne of the key and most important variables is the centripetal acceleration. As aforementioned, centripetal acceleration is the result of the ratio force\/mass being applied laterally when players perform turns, and therefore a potential high indicator of pressure being exerced. Even though we observed a significant difference when directly comparing both groups, in the Monte Carlo Simulation, in 3 out of every 4 random sample from non-injury the means were different enough to pass our significance test. Below is the distribution of centripetal acceleration for our simplified routes:\n\nSummary of statistical results:\n\n| Wilcoxon Signed Rank (MCS) | Welch's t-test | TTest (MCS) | Wilcoxon Ranksum | Wilcoxon Ranksum (MCS) | Mann\u2013Whitney | Mann\u2013Whitney (MCS) |\n|:-|:-|:-|:-|:-|:-|:-|\n| 0.7651 | 0.0277 | 0.7574 | 0.0480 | 0.8049 | 0.0240 | 0.6487 |","42b6ce9d":"# Findings","23a4cae0":"### Maximum Approach Velocity (For Turns Only)\n\nSimilarly, the approach velocity, or the mean velocity for the past five frames prior to turning points (TP), was also identified as significantly different across plays with injury in comparison to those without injury. The approach velocity was calculated **on the simplified route**, meaning that the approach velocity only exists in for frames with a turn, and not when the player is running in a straight line. For instance, if there were three turns in a play, there would be only three observations of approach velocity, even if the play had a lot more than three frames. \n\nSummary of statistical results:\n\n| Wilcoxon Signed Rank (MCS) | Welch's t-test | TTest (MCS) | Wilcoxon Ranksum | Wilcoxon Ranksum (MCS) | Mann\u2013Whitney | Mann\u2013Whitney (MCS) |\n|:-|:-|:-|:-|:-|:-|:-|\n| 0.0033 | <0.001 | 0.0024 | <0.001 | 0.0017 | <0.001 | <0.001 |\n\nBoth the maximum and the standard deviation moments were statistically significant and discerning enough from injury to non-injury plays. As tricky as it can be to look at the standard deviation for in-play data, it points out to the fact that change in velocity within play is as important as maximum velocity values reached.\n","3f310e3c":"# Bibliography\n\n- Boguszewski DV, Joshi NB, Yang PR, Markolf KL, Petrigliano FA, McAllister DR. Location of the natural knee axis for internal-external tibial rotation. Knee. 2016 Dec;23(6):1083-1088 \n- Gupton M, Terreberry RR. StatPearls. StatPearls Publishing; Treasure Island (FL): Dec 6, 2018. Anatomy, Bony Pelvis and Lower Limb, Knee.\n- Gupton M, Terreberry RR. Anatomy, Hinge Joints. In: StatPearls. Treasure Island (FL): StatPearls Publishing; 2019 Jan-. Available from: https:\/\/www.ncbi.nlm.nih.gov\/books\/NBK518967\/\n- Wroble, R. R., Nepola, J. V., & Malvitz, T. A. (1988). Ankle dislocation without fracture. Foot & ankle, 9(2), 64-74.\n- Duthon, V. B., Barea, C., Abrassart, S., Fasel, J. H., Fritschy, D., & M\u00e9n\u00e9trey, J. (2006). Anatomy of the anterior cruciate ligament. Knee surgery, sports traumatology, arthroscopy, 14(3), 204-213.\n- Wilcoxon, Frank (1945),\"Individual comparisons by ranking methods\" Biometrics Bulletin. 1 (6), Dec, p80\u201383.\n- H.B. Mann and D.R. Whitney, \u201cOn a Test of Whether one of Two Random Variables is Stochastically Larger than the Other,\u201d The Annals of Mathematical Statistics, vol. 18, no. 1, pp. 50-60, 1947.\n- Lowry, Richard. \u201cConcepts and Applications of Inferential Statistics\u201d. Chapter 8. https:\/\/web.archive.org\/web\/20171022032306\/http:\/\/vassarstats.net:80\/textbook\/ch8pt1.html\n- Bursac, Z., Gauss, C.H., Williams, D.K. et al. Purposeful selection of variables in logistic regression. Source Code Biol Med 3, 17 (2008) doi:10.1186\/1751-0473-3-17\n- Wu, Shin-Ting; Marquez, Mercedes (2003). \"A non-self-intersection Douglas-Peucker algorithm\". 16th Brazilian Symposium on Computer Graphics and Image Processing (SIBGRAPI 2003). Sao Carlos, Brazil: IEEE. pp. 60\u201366. CiteSeerX 10.1.1.73.5773","28ded402":"## Introduction\n\nInjuries are a common aspect of professional athletes' lives, particularly in high-contact sports. It is an inherent by-product of exposing the body to extreme performance conditions. There are many competing factors contributing to injuries, both in the endogenous \/ biomechanical and the exogenous \/ external domains. The range of variables associated with observed injuries might be related to conditioning, training, lifestyle (nutrition, sleep, exercise) and to external influences (hits, field conditions, weather). American Football is no exception to professional sports, and one of the challenges in the current nature of the game is the minimization of situations in which injuries are more likely to occur.\n\nThe present analysis aims to identify specific dynamics in players' movements (routes) and game conditions that impact injury likelihoods. We are particularly interested in developing a framework to assess the change in risk from players moving in different ways, given the available game conditions. Although there are limitations in year-over-year data consistency, data availablilty and player-level biomechanical information, we tried to establish guidelines and points of research that can help in assessing the stress on particular parts of the body at points in time. Below are the hypotheses explored in the present work:\n\n1. Change of direction (COD) intensity impacts lower-limb injuriy likelihoods.\n2. Field type confounds the impact on lower-limb injuries.\n3. Assymetric rotation (the divergence in change of direction and change of orientation) impacts injuriy likelihood.\n\nAdditionaly, the present work **aims to provide a consisten framework for analyzing geospatial data to study injury**.","692cc3d6":"### Data Available\n\n**Play Dataset**:\n\n- 267,005 unique plays described;\n- There are multiple definitions for the same types of Stadiums (e.g. 'outdoor' \/ 'outdoors');\n- There are no clear weather categories;\n- There are 11 types of plays described, out of which 2 are non-identifiable (Null, '0');\n- There are 10 unique values for RosterPositions;\n- There are 2 unique values for FieldType (Synthetic, Natural);\n- There are 23 unique values for Position, out of which 1 is non-identifiable ('Missing Data');\n- There are 10 unique PositionGroup, our of which 1 is non-identifiable ('Missing Data');\n    \n**Tracking Dataset**:\n\n- There are 266,960 unique plays, which tells us that for 45 plays in the plays dataset we do not have tracking data;\n- There are 79 unique event names\n    \n**Injuries Dataset**:\n\n- There are 105 injuries described, out of which only 76 have a _PlayKey_ associated with it. This might tells us even though we do not know when exactly the injury occured, we know the player and previous plays \/ games that led to the specific injury;\n- There are 5 body parts injuries, with the majority of injuries impacting the Knee and the Ankle","a01b1f8d":"# Future Research\n\nThere is plenty of potential for improvements in the work hereby presented. From a data perspective, it would be meaningful to understand the biomechanics differences associated to each player in a play, as well as information about mass, weight and some key tracking metrics on past performance. It would also help to see plays from the perspective of all players, so to understand if certain movements resulted from the pressure of other players within the same area, ball movement or any characterized manoeuvre. In terms of analysis, the current framework could be extended to map actual play variable distributions and compare each play without any aggregation but rather on the actual development of each variable from frame to frame. It could also be expanded in terms to try different tolerance levels for the simplification of routes, as well as curve-linear approaches to turns, taking into consideration real-life arc movements as opposed to osculating circles from three consecutive points.","a3afe82a":"### Minimum Approach Acceleration, or Maximum Approach Deceleration (For Turns Only)\n\nSummary of statistical results:\n\n| Wilcoxon Signed Rank (MCS) | Welch's t-test | TTest (MCS) | Wilcoxon Ranksum | Wilcoxon Ranksum (MCS) | Mann\u2013Whitney | Mann\u2013Whitney (MCS) |\n|:-|:-|:-|:-|:-|:-|:-|\n| 0.8986 | 0.2814 | 0.9487 | 0.0947 | 0.8601 | 0.0473 | 0.7402 |","5d687327":"\n\n\n![image.png](attachment:image.png)","0b0ae75d":"![image.png](attachment:image.png)","17f5c94c":"# Appendix - Script","0cf85ed0":"## Methodology: Statistical Testing\n\n\n### Mean, Rank and Distribution\n\nOne of the approaches taken in the present study, particularly when it comes to play analysis, was to identify if variables differed in plays with injury vs. without injury. Two approaches were taken: \n\n**_A direct statistical test between the two groups:_**\n\nHere the goal was to test for divergence in the mean \/ rank \/ distribution of our two subgroups (with vs. without injuries). That is, do plays that result in injury have higher\/lower\/equal average value for our metrics? We chose to use three particular tests:\n\n- Welch's T-Test: test equality of means (without rigidity on equality of variances)\n- Wilcoxon Rank-Sum Test: test equality of mean ranks (non-parametric)\n- Mann\u2013Whitney U Test:  test whether a randomly selected value from plays with injury will be less than or greater than a randomly selected value from without injury\n\n\n**_Multiple statistical tests between plays with injury and samples taken from plays without injuries._**\n\nThe rationale for testing samples of plays without injuries with the plays with injuries is two-fold: to account for the fact the p-values can quickly validate very small differences in population means when comparing the two groups directly (due to the large size) and to test the intuition that we could generate similar distributions, often enough, by simply drawing ramdom samples from the non-injury dataset. We used a Monte-Carlo Simulation approach, with $N=10000$, and for each iteration we:\n\n- Generated a random sample of non-injury plays\n- Computed the three tests above, between that sample and the injury sample, plus a paired non-parametric Wilcoxon signed-rank test.\n- Stored the p-value of the 4 tests in a vector. \n- Calculated how many of those values were below our $\\alpha=0.05$ threshold and stored the value as a percentage of total iterations.\n\nThe way for us to read the results for these simulations is not on the likelihood of the results being draw at random, but rather the likelihood that the tests would be **above** our $\\alpha=0.05$ threshold (and therefore, not significant). To give an example, when the value is 0.80, it means that 80% of the time, or in our case in 8000 of the simulations, we could not reject the null hypothesis that the mean in the randomly chosen sample of non-injury plays is equal to that of injuried plays.\n\nNote that the choice for non-parametric tests was done after we could not verify normality and equal variances in all of our variables being studied, and to provide a more robust, albeit at times less powerful, test across the board.","94282b34":"# Methodology\n\n- **Data Treatment**:\n    - Remove movements before ball-snap and after timeout (considered noisy for injury likelihood estimations)\n    - Estimate injury plays for missing `PlayKey` based on last play in injury game\n    - Calculate diagonal (Eucledian) distance between adjacent points for each timeframe\n    - Calculate velocity, and smoothen it with Gaussian distribution\n    - Calculate acceleration\n    - Simplify the routes using an adjusted version of the Ramer-Douglas-Pecker algorithm with tolerance of 0.25 yards (9 inches)\n    - Identify the number of turns per play\n    - Calculate the osculating radius (OR) for every two adjacent points (within one tenth of a second)\n    - Calculate the centripetal acceleration given the osculating radius and smoothened velocity\n    - Calculate the angle changes in the routes and identify a threshold for _sharp turns_ (0.75 radians for external angles)\n    - Identify curves or turning points (TP) with dangerous centripetal accelration (`dangerous_acc`)\n    - Calculate jerk, the derivative of the acceleration\n    - Calculate the difference between the change in direction (over past frame) and the change in orientation (over past frame)\n\n\n- **Statistical Testing**:\n    - Identify variables for which the distribution, rank or mean differ between injury vs. non_injury plays;\n    - Run statistical test on plays with injury vs. without injury;\n    - Run statistical test on plays with injury vs. random samples of plays without injury (using Monte-Carlo Simulation)\n\n\n- **Modeling**:\n    - Logistic Regression on Play Level\n        - Generate dummy variables for categorical fields (`RosterPosition`, `PlayType`, `FieldType`)\n        - Oversample injury observations using SMOTE;\n        - Standardize Variables;\n        - Keep only meaningful variables (using recursive feature elimination);\n        - Apply a logistic regression with 30% holdout data\n        ","4f7953c0":"### Maximum Velocity\n\nWhen treating all plays as independent, we see that some variables related to the route identification seem to impact the injury likelihood. Maximum velocity is one of the variables with the clearest impact, which can be visualized in the Figure below. In all of our 7 statistical tests, maximum velocity per play showed to have a difference in plays with injury vs. plays without injuries.\n\nSummary of statistical results:\n\n| Wilcoxon Signed Rank (MCS) | Welch's t-test | TTest (MCS) | Wilcoxon Ranksum | Wilcoxon Ranksum (MCS) | Mann\u2013Whitney | Mann\u2013Whitney (MCS) |\n|:-|:-|:-|:-|:-|:-|:-|\n| 0.0029 | <0.001 | 0.0013 | <0.001 | 0.0015 | <0.001 | <0.001 |\n","7da5eb1c":"## Background Research\n\n#### Biomechanics\n\nHinge joints (knee, ankle) are those allowing extension primarily in one plane (Boguszewski et al, 2016), usually acting like a small door or valve opening and closing the one-plane movement of certain body parts. Although they are capable of supporting large amount of work and stability, they are not as mobile as other joints in the human body (Gupton & Terreberry, 2019), such as ball-and-socket joints (shoulder and hips). For that reason, certain types of injuries in hinge joints are related to laterall and horizontal impact or movement, such as ligament injuries in knees (MCL, ACL) and dislocations in ankles (Wroble, 1988). ACL injuries are particularly related to strong change of directions (Duthon et al, 2006).\n\n![No_return_position_ACL.png](attachment:No_return_position_ACL.png)\nFigure: Point of no return - ACL. Source: Physiopedia.\n\n#### External Conditions\n\nThree game conditions are considered for the present study: weather, stadium type and field type. Stadium type is particularly relevant in conjunction with weather conditions, as it will add a higher exposure of athletes to external conditions. Conceptually, stadium types should confound weather impact on performance and injury. In terms of field type, or surface type, Kent et al. (2015) tested different performances of American football cleats on different grasses and artificial surfaces, with an indication that artificial turf generated significantly greater peak horizontal forces. Similarly, Mack et al. (2018) used a cohort study to show results from the NFL that artificial turfs tend to generate higher injury rates due to the release pattern of cleats in comparison to natural grass.\n\n#### Injuries in Sports\n\nDos\u2019Santos et al (2018) extensively analyzed the literature available in change of direction (COD), particularly the impact of angle and velocity in different types of injuries, considering also COD biomechanics for different types of sports, and different athletes. One of the findings show the difference in impact for angle in the change of direction, usually peaking at internal angles lower than 45 degrees (or external angles greater than 135 degrees). They also found that angle and velocity regulate the progression and regression in change of direction intensity, impacting stress on lower limb joints.\n","5d1481d7":"## Field Type\n\n\nSimilarly to how we investigated the differences in key variables for plays with or without injury, we can use a similar approach controlling for field type. However, the key difference here is the size of both samples. Due to the magnitude, the significance of most statistical test will not be a good measurement to identify the actual difference, as very small variations in the distributions will likely be enough for valid statistical tests. Nearly all variables tested using the methods for innjury were statistically significant in the case of field type. For instance, in the Figure below we can see how similar the distributions are, and even look at the absolute difference in the mean between the two groups for a better understanding of this phenomenon:\n\n| Variable | Mean Absolute Difference |\n|:-|:-|\n|Centripetal Acceleration - Mean | 0.0012 |\n|COD vs COO - Mean | 0.0173 |\n|Approach Velocity - Max | 0.0377 |\n|Approach Acceleration - Max | 0.02228 |\n|Velocity Max - Max | 0.03872 |\n|Approach Acceleration - Min | 0.0419 |\n","5cf7f8b1":"### Change of Direction vs. Change of Orientation\n\nThe data provided has information about the absolute orientation in degrees as well as the absolute direction in degrees for each frame. However, there seems to exist two dissonances in the data: between seasons, and between the represented vs actual orientation. Even if the absolute values pose problems across different plays, the **change within** the same play can still be used as a good indicator of relative movement. Even more importantly, the difference between the change of direction and the change of orientation can be used as a good indicator of situations in which the upper\/lower body moved with different rotation than the overall player direction. As we are concerned with the impact on the joints of players' displacements, this difference can be seen as an indicator of assymetric rotation.\n\nAdditionally, the change of direction in our data cannot tell us the path used for the change, whether the inner or outer path. In other words, if the initial direction was 1 and the player moves to have a direction of 350, then the calculated difference would be -349 degrees. In terms of how the movement was made, it could have been a rotation on the outside, where the player moved 349 degrees, or on the inside, where he moved 11 degrees. The unkown information is, did the player rotate to his right or his left? For that reason we chose to look at any directional change as the inner-most change in direction, which in practice means rotations would range from 0 to 180 degrees (or 0 to $\\pi$ radians). This approach is inherently used in the angle calculation aforementioned.\n\nTherefore for each frame within a play, the change of direction $\\Delta{Dir}$ and change of direction $ \\Delta{O}$ can be defined as:\n\nif $|Dir_n - Dir_{n-1}| < 180$  then $\\Delta{Dir} = Dir_n - Dir_{n-1} $ \n\nelse if $ Dir_{n-1} > 180 $ then $ \\Delta{Dir} = Dir_n + (360-Dir_{n-1}) $ \n\nelse $ \\Delta{Dir} = -((360-Dir_{n}) + Dir_{n-1}) $ \n\nThe difference from $\\Delta{Dir}$ to $ \\Delta{O}$ is our measurement for asymmetric rotation.","7ab9f882":"### Route after transformations:\n\n![image.png](attachment:image.png)","e86ded5c":"Recent studies have raised an interesting question about the safety of artificial turf in comparison to natural grass in the NFL. Some of the key differences relate to how cleats interact with the grass and the impact on lower-limb injuries for synthetic turf. The question is two-fold: \n\n- Do players movements differ because of the characteristics of the ground in synthetic vs. natural grass fields? or\n- Do players change their movement choices in the presence of artficial turf?\n\nHinge joints (knee, ankle) injuries are largely related to lateral impact in change of directions performed during plays, and hence we spent a great portion of our work developing a framework to map routes as a function of turning points, sharp angles, and approach speed. Velocity and angle are brought up on the literature as key factors for lower body injuries.\n\nAlthough we fail to observe a very clear relationship of field type with injury likelihood alone, we did observe its impact in combination with 13 other variables. Some of the key predictors found were:\n\n- Centripetal acceleration used in turns, \n- Change of direction relative to change in orientation\n- Total number of sharp turns in a play\n- Approach velocity\n- Player position\n- Field Type\n- Presence of rain\n\nFurther research is recommended, particularly with more detailed data to enhance the calculation of fatigue, to account for player biomechanics and change of direction techniques. ","d9e40164":"### Turning Point and Change of Direction\/Orientation\n\nWith simplified routes, it is easier to identify key changes of directions happening on a play, as well as the sharpness of a turn. We labeled them TP (turning point) for the coordinates where there is a change in the angle of the two most adjacent points to the current (N+1 and N-1); The change of direction (COD) and change of orientation (COO) were calculated based on the `dir` and `o` variables available. The TP is the (xi, yi) where the COD happened. Note that we did not take into account the absolute direction of a play, but rather the relative rotation in comparison to the previous frame. Values can range from 0-3.14 (Pi), where 0 represents no change in direction and 3.14 (Pi), a 180-degree rotation. \n\n### Players' Biomechanics\n\nBiomechanics and techniques change from player to player, and can play a big role in preventing injuries. With the current data available, we did not have player-related data (weight, height, age) not to mention any information about lifestyles or performance-related measurements. However, with the player-position data available, we decided to assume that both body composition and biomechanics training were relatively similar among all players in the same position, and divergent enough to the other positions. This assumption ressonates specially in professional sports, where constant funneling of talent would lead positions to have very adapted body types, shapes and conditioning for top performance among all other athletes. We used the player position as a stratification factor for the data analysis.\n\n\n### Osculating Radius, Centripetal Acceleration and Approach Velocity\n\n![image.png](attachment:image.png)\n\nWith the identification of similar biomechanics and of all turning points, we can then estimate the radius of the arcs created across the three most adjacent points at the time of the turn, the osculating radius (OR).Such radius can be used to estimate the centripetal acceleration being exerced in each turn, which in reality relates to the lateral force being exerced in function of the player's mass, and the potential for impacts in the lower-limb joints. To calculate the oscilating radius, we can find the intersection of the perpendicular bisectors of the 2 lines joining every 3 points in a player's route, and calculate the x,y coordinates for the center of the circle (and the distance to each of the three points using the eucledian distance). Likewise, and as identified in the literature, the approach velocity is of high inmportance when understanding injuries related to change of directions. Because we have very granular frames, we decided to use the rolling mean of the previous five frames prior to the turn for the calculation of approach velocity (AV).\n\nBelow is a step by step explanation [1](https:\/\/www.intmath.com\/applications-differentiation\/8-radius-curvature.php):\n","b3561b92":"```python\n## Execution Code\n### Generate clean datasets \/ prepped datasets:\n\n### OPTION 1: CALCULATE FROM SCRATCH\n# TOLERANCE = 0.25\n\n# tracking, injuries, plays = load_position_data(\n#     path = 'data\/', position = 'Linebacker', tolerance = TOLERANCE, \n#     sharp_turn = 0.75, max_radius = np.inf, max_cent_acc = 20,\n#     max_speed = 15, time_lapse_deciseconds =1, verbose=True\n# )\n\n# summary = summarize(tracking=tracking, injuries=injuries, tolerance=TOLERANCE)\n\n### OPTION 2: RE-LOAD SUMMARY ALREADY CALCULATED\n# summary = pd.read_csv('summary.csv.gz', compression='gzip')\n\n### PLOT A RANDOM PLAY\n# plot_play(tracking, injuries, tolerance=TOLERANCE)\n\n\n### RUN STATISTICAL TESTS FOR INJURY VS NON-INJURY\n\nres = []\nCONTROL = 'is_injury'\ninjury1 = summary[summary[CONTROL]==1]\ninjury0 = summary[summary[CONTROL]==0]\n\nmtc = 10000\nalpha = 0.05\n\nfor var in summary.columns:\n    if var not in [\n            'Unnamed: 0','PlayKey','PlayerKey','PlayerDay','game_id','RosterPosition',\n            'PlayerGame','StadiumType','FieldType','Temperature','Weather',\n            'PlayType','PlayerGamePlay','is_injury', 'is_injury_player', 'is_injury_game',\n            'rain'\n        ] and not var.startswith('radius') : ## run only for numerical variables and ignore radius (infinity and NaN)\n      \n        VAR = var\n        CONTROL = 'is_injury'\n        results = []\n        \n        for func in [\n                wilcoxon, ## population ranks \n                ttest_ind, ## population means (equal_var for Welch\u2019s t-test, removing assumption of equal var)\n                ranksums, ## value from one population will be less than or greater than a randomly selected value from a second population\n                mannwhitneyu ## value from one population will be less than or greater than a randomly selected value from a second population\n                ]:\n            \n            i1 = injury1[injury1[VAR].isnull()==False]\n            i0 = injury0[injury0[VAR].isnull()==False]\n            \n            if func.__name__ != 'wilcoxon':  \n                ### Wilcoxon only works for paired samples\n                try: \n                    overall = func(i1[VAR], i0[VAR])[1]\n                except ValueError:\n                    overall = np.nan\n                \n                results.append(overall)\n            try:\n                if func.__name__ == 'ttest_ind':\n                    ### set equal variance assumption to False\n                    tests = mc_ranksum_sampled(i1[VAR], i0[VAR], mtc, func=func, equal_var=False)\n                else:\n                    tests = mc_ranksum_sampled(i1[VAR], i0[VAR], mtc, func=func)\n                results.append(tests[np.where(tests>=alpha)].shape[0]\/mtc)\n                \n            except ValueError:\n                ### in case values are the same or explode to infinity\n                results.append(np.nan)\n                \n        res.append([VAR]+results)\n        \n### CREATE TABLES WITH ALL P-VALUES AND RESULTS OF TESTS FOR ALL VARIABLES\ndf = pd.DataFrame(res, columns = ['var','wilcoxon','ttest_o','ttest','ranksums_o','ranksums','mw_o','mw'])\n\n### SAVE DATA IF NECESSARY\ndf.to_csv('means_difference_injury_test.csv',index=False)\n\n### PRINT MARKDOWN TABLE RESULTS:\nfor row in df[df['mw_o']<=0.05].values:\n    print ('| '.join([str(i) for i in row]))\n\n\n### RUN STATISTICAL TESTS FOR INJURY VS NON-INJURY\n\nCONTROL = 'FieldType'\nnatural= summary[summary[CONTROL]=='Natural']\nsynthetic = summary[summary[CONTROL]=='Synthetic']\n\nalpha = 0.05\n\nres = []\nfor var in summary.columns:\n    if var not in [\n            'Unnamed: 0','PlayKey','PlayerKey','PlayerDay','game_id','RosterPosition',\n            'PlayerGame','StadiumType','FieldType','Temperature','Weather',\n            'PlayType','PlayerGamePlay','is_injury', 'is_injury_player', 'is_injury_game',\n            'rain'\n        ] and not var.startswith('radius') :\n        VAR = var\n        CONTROL = 'is_injury'\n        results = []\n        for func in [\n                ttest_ind, ## population means (equal_var for Welch\u2019s t-test, removing assumption of equal var)\n                ranksums, ## value from one population will be less than or greater than a randomly selected value from a second population\n                mannwhitneyu ## value from one population will be less than or greater than a randomly selected value from a second population\n                ]:\n            \n            i1 = natural[natural[VAR].isnull()==False]\n            i0 = synthetic[synthetic[VAR].isnull()==False]\n            \n            try:\n                overall = func(i1[VAR], i0[VAR])[1]\n            except ValueError:\n                overall = np.nan\n                \n            results.append(overall)\n                \n        res.append([VAR]+results)\n\n\ndf1 = pd.DataFrame(res, columns=['var','ttest','ranksums','mw'])\n\n### SAVE DATA IF NECESSARY\ndf1.to_csv('means_difference_field_test.csv',index=False)\n\n\n\n### ADD NATURAL FIELD AS A BOOLEAN VARIABLE TO THE MODEL\nsummary['natural_field'] = (summary['FieldType']=='Natural').astype(int)\n\n### boolean mark if the play is a punt or kickoff (1, else 0)\nsummary['Is_Punt_Kickoff'] = (summary['PlayType'].isin([\n            'Punt or Kickoff Not Returned','Punt',\n            'Kickoff','Punt or Kickoff Returned'\n])).astype(int)\n\n### player position (manually generate one-hot encoding \/ dummies)\n  \nfor position in summary['RosterPosition'].unique():\n    if position != 'Linebacker': ## zero index variable chosen\n        summary[f'dm_{position}'] = 0\n        summary.loc[summary['RosterPosition']==position, f'dm_{position}'] = 1\n        \n        \n####### CODE FOR LOGISTIC REGRESSION\n\n### SELECT VARIABLES WITH P-VALUE FOR MW TEST <= 0.05 FOR LOGISTIC MODEL\nvars_to_use = df[df['mw_o']<=0.05]['var'].unique().tolist() + \\\n    ['natural_field','Is_Punt_Kickoff','Temperature', 'rain'] + \\\n    [i for i in summary.columns if i.startswith('dm_')]\n###\n\n### REPLACE NULL VALUES TO ZERO \/ INF (ONLY A FEW VALUES BY VARIABLE)\nsummary1 = summary[vars_to_use+['is_injury']].fillna(0).replace({np.inf: 0})\n###\n\nY=summary1['is_injury']\nX=summary1[vars_to_use]\n\n### OVERSAMPLE (NEEDED OTHERWISE MODEL WILL PREDICT ALL 0'S AND HAVE A GREAT\n### ACCURACY\n\n### IMPORT PACKAGES NEEDED, AS NEEDED\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\n\n### USING Synthetic Minority Over-sampling Technique (SMOTE)\nos = SMOTE(random_state=0)\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\ncolumns = X_train.columns\n\n### RECREATE THE DATA\nos_data_X,os_data_y=os.fit_sample(X_train, y_train)\nos_data_X = pd.DataFrame(data=os_data_X,columns=columns )\nos_data_y= pd.DataFrame(data=os_data_y,columns=['is_injury'])\n\n\nfrom sklearn import preprocessing\nscaler = preprocessing.StandardScaler().fit(os_data_X)\n\nX_scaled = scaler.transform(os_data_X)\n\n\n### IMPORT MODELING PACKAGES\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\n## VERY LARGE MAX_ITER TO PREVENT NON-CONVERSION IN LIKELIHOOD ESTIMATION\nmodel = LogisticRegression(max_iter=1000, solver='lbfgs') \n\n## USING RECURSIVE FEATURE ELIMINATION\nrfe = RFE(model, 20)\nrfe = rfe.fit(X_scaled, os_data_y)\n\n## KEEP ONLY MEANINGFUL VARIABLES.\nnew_os_data_X = pd.DataFrame(X_scaled, columns=os_data_X.columns)[np.array(vars_to_use)[rfe.support_]]\n\n## OPTIONAL - VISUALIZE LOGIT RELATIONSHIP AND STATS USING STATSMODEL\nimport statsmodels.api as sm\nlogit_model=sm.Logit(os_data_y,new_os_data_X)\nresult=logit_model.fit()\n#print(result.summary())\nprint(result.summary2())\n\n## RUN LOGISTIC REGRESSION\nX_train, X_test, y_train, y_test = train_test_split(new_os_data_X, os_data_y, test_size=0.3, random_state=0)\nmodel = LogisticRegression(max_iter=5000)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\nprint(f'Accuracy of on test set: {model.score(X_test, y_test):.2%}')\n\n\n\nfrom sklearn.metrics import plot_confusion_matrix, roc_curve, auc\n\n\n### PLOT CONFUSION MATRIX\n\ntitles_options = [(\"Confusion matrix, without normalization\", None),\n                  (\"Normalized confusion matrix\", 'true')]\nfor title, normalize in titles_options:\n    disp = plot_confusion_matrix(model, X_test, y_test,\n                                 display_labels=[0,1],\n                                 cmap='cubehelix',\n                                 normalize=normalize)\n    disp.ax_.set_title(title)\n\n    print(title)\n    print(disp.confusion_matrix)\n\n\n### PLOT ROC\n\nprobs = model.predict_proba(X_test)\npreds = probs[:,1]\nfpr, tpr, threshold = roc_curve(y_test, preds)\nroc_auc = auc(fpr, tpr)\n\n\nplt.figure(figsize=(7,5))\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1], '--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n\n```\n","94a5226d":"Below is a summary of all statistics that had a significant p-value at least for the Mann-Whitney test:\n\n|var| Wilcoxon Signed Rank (MCS) | Welch's t-test | TTest (MCS) | Wilcoxon Ranksum | Wilcoxon Ranksum (MCS) | Mann\u2013Whitney | Mann\u2013Whitney (MCS) |\n|:-|:-|:-|:-|:-|:-|:-|:-|\n| a_mean|0.8443|0.0575|0.7353|0.0485|0.8035|0.0242|0.6526|\n| a_std|0.3984|<0.001|0.3247|0.003|0.4208|0.0015|0.263|\n| a_max|0.5178|<0.001|0.2522|0.0142|0.6301|0.0071|0.447|\n| a_min|0.4269|0.0064|0.5042|0.0023|0.3933|0.0012|0.2345|\n| v_sm_median|0.9859|0.8179|0.9981|0.0817|0.8983|0.0409|0.7645|\n| v_sm_std|<0.001|<0.001|<0.001|<0.001|<0.001|<0.001|<0.001|\n| v_sm_max|0.0029|<0.001|0.0013|<0.001|0.0015|<0.001|<0.001|\n| o_std|0.6549|0.0137|0.6154|0.019|0.6671|0.0095|0.4873|\n| o_min|0.3624|0.0099|0.4598|<0.001|0.2194|<0.001|0.0999|\n| cent_acc_median|0.8679|0.0936|0.8805|0.0773|0.8668|0.0367|0.7092|\n| cent_acc_max|0.8338|0.0276|0.8269|0.0989|0.8781|0.0495|0.7569|\n| cent_acc_0.25_mean|0.7651|0.0278|0.7574|0.0481|0.8049|0.024|0.6487|\n| cent_acc_0.25_std|0.6284|0.0131|0.6569|0.0117|0.601|0.0058|0.4138|\n| cent_acc_0.25_max|0.6666|0.0216|0.6882|0.0153|0.6504|0.0076|0.4577|\n| coo_mean|0.8438|0.1691|0.8884|0.0491|0.8032|0.0245|0.645|\n| cod_mean|0.3717|<0.001|0.3688|0.0046|0.4672|0.0023|0.2866|\n| cod_median|0.6252|0.0213|0.6603|0.0181|0.6679|0.009|0.4846|\n| cod_std|0.72|0.012|0.6537|0.0346|0.7558|0.0173|0.5953|\n| cod_max|0.7269|0.0254|0.7086|0.031|0.7422|0.0155|0.5656|\n| jerk_std|0.8061|0.0341|0.7616|0.0552|0.7904|0.0276|0.6588|\n| jerk_max|0.8112|0.0362|0.7845|0.0718|0.8501|0.0359|0.7078|\n| jerk_min|0.7103|<0.001|0.3182|0.0608|0.8219|0.0304|0.6912|\n| cod_coo_mean|0.0623|<0.001|0.1055|<0.001|0.0546|<0.001|0.0162|\n| cod_coo_median|0.3126|<0.001|0.3438|0.0022|0.3658|0.0011|0.2061|\n| cod_coo_std|0.1215|<0.001|0.1051|<0.001|0.1137|<0.001|0.0493|\n| cod_coo_max|0.1461|<0.001|0.118|<0.001|0.1386|<0.001|0.0617|\n| time_rng|0.688|0.0098|0.704|0.017|0.6562|0.0085|0.4809|\n| euc_dis_sum|0.2613|0.0012|0.3559|<0.001|0.242|<0.001|0.1106|\n| sharp_turn_sum|0.1631|<0.001|0.1764|<0.001|0.1939|<0.001|0.0843|\n| angle_0.25_mean|0.411|0.0016|0.457|0.0047|0.4841|0.0024|0.295|\n| tp_sum|0.1126|<0.001|0.1343|<0.001|0.0775|<0.001|0.0296|\n| dangerous_acc_sum|0.1107|<0.001|0.1229|<0.001|0.1163|<0.001|0.042|\n| app_v_mean|0.6092|0.0054|0.5796|0.0198|0.6837|0.0099|0.4958|\n| app_v_std|<0.001|<0.001|<0.001|<0.001|<0.001|<0.001|<0.001|\n| app_v_max|0.0033|<0.001|0.0024|<0.001|0.0017|<0.001|<0.001|\n| app_v_min|0.6194|0.0614|0.8738|0.003|0.4046|0.0015|0.2361|\n| app_a_max|0.6743|0.0542|0.7477|0.0135|0.619|0.0067|0.4432|\n| app_a_min|0.8986|0.2815|0.9487|0.0948|0.8601|0.0474|0.7402|","9ac33c21":"## Methodology: Modeling\n\nFor the present work one of the ideal outcomes is to be able to accurately predict the impact of route movement and external factors in the likelihood of injury for a play. Although more advanced techniques could be implemented to increment our power of prediction, we need to make sure there is a certain degree of interpretability in the variables present in the model. The case for clear variable impacts becomes stronger if we take into account potential course of actions for injury prevention. For that reason, a Logistic Regression was chosen, given its flexibility as a linear model and the availability of easily-interpretable coefficients.\n\nWe treated injury as the dependent variable (binary) and field type as one independent variable.\n\n${\\displaystyle \\ell =\\log _{b}{\\frac {p_{injury}}{1-p_{injury}}}=\\beta _{0}+\\beta _{1}FT+\\beta _{2}x_{2}+...+\\beta _{n}x_{n}}$\n\nwhere $FT$ is the field type represented as a binary variable (1 for natural field, 0 for artificial turf).\n\n### Additional independent variables\n\nFor the modeling procedure, we had available all variables aggregated on the play level, together with the type of aggregation used. For instance, for velocity, we had the mean velocity in a play, as well as the median, standard deviation, min and max. The reason why we added multiple aggregations was to capture the different moments of the variables distribution within a play.\n\nThe variables summarized and the aggregations used were:\n\n- acceleration (mean, median, std, max, min)\n- velocity (mean, median, std, max, min)\n- orientation (mean, median, std, max, min)\n- direction (mean, median, std, max, min)\n- centripetal acceleration (mean, median, std, max, min)\n- centripetal acceleration on simplified route (mean, median, std, max, min)\n- change of orientation(mean, median, std, max, min)\n- change of direction (mean, median, std, max, min)\n- jerk (mean, median, std, max, min)\n- radius on simplified route (mean, median, std, max, min)\n- change of direction vs change of orientation (absolute difference) (mean, median, std, max, min)\n- time (range)\n- eucledian distance (sum)\n- sharp turns (sum)\n- angle of simplified route (mean)\n- turning points (sum)\n- dangerous acceleration (sum)\n- dangerous turns (sum)\n- approach velocity (mean, max, min)\n- days rest (mean)\n\nAdditionally we had categorical variables associated to each player \/ play \/ game that were used for the modeling pool:\n\n- Rain (binary variable as to whether it rained)\n- RosterPosition (transformed into dummy variables (with $N-1$ features)\n- Natural Field (binary)\n- Temperature (as provided)\n\nIn order to filter the available variables to more meaningful ones, two elimination tests were performed:\n\n- Based on the distribution tests (Mann\u2013Whitney U test)\n- Based on the 20 most informative variables (using Recursive Feature Elimination)\n\nAll variables were standardized for modeling purposes.","986be217":"### Other Variables Considered\n\nOther variables considered for understanding routes and injuries are listed below:\n\n- Velocity (Corrected for extreme values using Gaussian Distribution centered on the current frame)\n- Acceleration\n- Sharp Turn counts (Binary variable as whether it was a sharp turn)\n- Turning Points in the route (Binary variable as whether it was a turn)\n- Dangerous Centripetal Acceleration (using a threshold of 10 for maximum centripetal acceleration. Binary variable)\n- Approach acceleration (Average accleration for past 5 frames prior to turn)\n- Jerk (third derivative of displacement)","2ea6a2da":"We also ran a logit model with the final entirety of the data, and below are the overall estimates for the whole logit estimates:\n\n```\n======================================================================\nModel:                Logit             Pseudo R-squared:  0.576      \nDependent Variable:   is_injury         AIC:               219773.9815\nDate:                 2020-01-02 06:15  BIC:               219990.6010\nNo. Observations:     373612            Log-Likelihood:    -1.0987e+05\nDf Model:             19                LL-Null:           -2.5897e+05\nDf Residuals:         373592            LLR p-value:       0.0000     \nConverged:            1.0000            Scale:             1.0000     \nNo. Iterations:       8.0000                                          \n----------------------------------------------------------------------\n                      Coef.  Std.Err.     z     P>|z|   [0.025  0.975]\n----------------------------------------------------------------------\ncent_acc_0.25_mean    0.7129   0.0077   93.1766 0.0000  0.6979  0.7279\ncod_mean             -0.6613   0.0140  -47.2488 0.0000 -0.6887 -0.6338\njerk_std             -1.1702   0.0108 -107.8719 0.0000 -1.1914 -1.1489\njerk_min             -1.1104   0.0098 -112.8316 0.0000 -1.1297 -1.0911\ncod_coo_mean          1.1187   0.0168   66.6812 0.0000  1.0858  1.1516\ncod_coo_std          -1.0500   0.0241  -43.5806 0.0000 -1.0973 -1.0028\ncod_coo_max           0.8617   0.0168   51.4303 0.0000  0.8288  0.8945\nsharp_turn_sum        1.5871   0.0154  103.3114 0.0000  1.5570  1.6172\ntp_sum               -1.8726   0.0164 -113.9550 0.0000 -1.9048 -1.8404\napp_v_std             0.1419   0.0141   10.0280 0.0000  0.1141  0.1696\napp_v_max             0.5946   0.0168   35.3491 0.0000  0.5617  0.6276\nnatural_field        -0.8304   0.0057 -144.7485 0.0000 -0.8417 -0.8192\nrain                 -0.5869   0.0100  -58.7926 0.0000 -0.6065 -0.5673\ndm_Quarterback       -0.5525   0.0093  -59.3395 0.0000 -0.5708 -0.5343\ndm_Wide Receiver     -0.9315   0.0053 -175.2884 0.0000 -0.9420 -0.9211\ndm_Defensive Lineman -0.9767   0.0072 -134.7340 0.0000 -0.9909 -0.9625\ndm_Tight End         -0.5818   0.0093  -62.4100 0.0000 -0.6001 -0.5635\ndm_Safety            -1.0039   0.0057 -175.0056 0.0000 -1.0151 -0.9926\ndm_Cornerback        -0.9696   0.0065 -149.8957 0.0000 -0.9822 -0.9569\ndm_Offensive Lineman -1.3320   0.0090 -148.6933 0.0000 -1.3496 -1.3145\n======================================================================\n```\n\nAs we can see, there are some interesting findings in relation to what we observed while exploring the data:\n\n- Average centripetal acceleration, average asymmetric rotation, number of sharp turns and maximum approach velocity have a positive impact on injury likelihood;\n- Injuries are less likely to occur in natural fields, _ceteris paribus_\n- Surprisingly rain was negatively related to the odds of injury, perhaps due to the attrition loss in wet surfaces\n- Certain positions had a lower likelihood of injuries. However, we take this data with caution given the small stratified sample size","65dd50c3":"# NFL 1st and Future - Analytics\n## Change of direction and injury likelihood","088637d4":"# Executive Summary"}}