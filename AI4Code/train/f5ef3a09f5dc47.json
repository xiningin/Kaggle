{"cell_type":{"0ef09c48":"code","6b7f6aa4":"code","dac58310":"code","559ac928":"code","162a809d":"code","5abc8578":"code","74f6218b":"code","7214e21c":"markdown","fb998519":"markdown","7e1309a7":"markdown","92fa14e7":"markdown","a6c395e3":"markdown"},"source":{"0ef09c48":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n!pip install pandas_bokeh\nimport pandas_bokeh\npandas_bokeh.output_notebook()\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, roc_curve, auc, plot_roc_curve\nfrom sklearn.decomposition import PCA\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6b7f6aa4":"df = pd.read_csv('\/kaggle\/input\/divorce-prediction\/divorce_data.csv',sep=\";\")#, header=None)#.to_numpy()\ndf.describe()","dac58310":"X = df.to_numpy()[:,:54]\ny = df.to_numpy()[:,54]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\ndef eval_perf(model, X_test, y_test):\n    \n    y_pred = model.predict(X_test)\n    \n    accuracy = accuracy_score(y_pred, y_test)\n    \n    cm = confusion_matrix(y_test, y_pred)\n    tn = cm[0][0]\n    fp = cm[0][1]\n    fn = cm[1][0]\n    tp = cm[1][1]\n    sensitivity = tp \/ (fn + tp)\n    specificity = tn \/ (fp + tn)\n    f1 = f1_score(y_test, y_pred)\n\n    print('accuracy: \\t', round(accuracy,2))\n    print('f1: \\t\\t',     round(f1,2))\n    print('sensitivity:\\t', round(sensitivity,2))\n    print('specificity:\\t', round(specificity,2))\n\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n    disp.plot() \n\n    plot_roc_curve(model, X_test, y_test)  \n    plt.show() ","559ac928":"print('=========================')\nprint(\"=== LogisticRegression ===\")\nprint('=========================')\nmodel = LogisticRegression().fit(X_train, y_train)\neval_perf(model, X_test, y_test)\n\nprint('=========================')\nprint(\"=== KNeighborsClassifier ===\")\nprint('=========================')\nmodel = KNeighborsClassifier().fit(X_train, y_train)\neval_perf(model, X_test, y_test)\n\nprint('=========================')\nprint(\"=== GaussianNB ===\")\nprint('=========================')\nmodel = GaussianNB().fit(X_train, y_train)\neval_perf(model, X_test, y_test)","162a809d":"pca = PCA(n_components=2)\npca_X = pca.fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(pca_X, y, test_size=0.2, random_state=0)","5abc8578":"def plot_boundry(model):\n\n    # Create color maps\n    from matplotlib.colors import ListedColormap\n    cmap_light = ListedColormap(['orange', 'cornflowerblue'])\n    cmap_bold = ['darkorange', 'darkblue']\n\n    h = .02  # step size in the mesh\n    \n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    x_min, x_max = pca_X[:, 0].min() - .1, pca_X[:, 0].max() + .1\n    y_min, y_max = pca_X[:, 1].min() - .1, pca_X[:, 1].max() + .1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.figure(figsize=(8, 6))\n    plt.contourf(xx, yy, Z, cmap=cmap_light)\n\n    # Plot also the training points\n    sns.scatterplot(x=pca_X[:, 0], y=pca_X[:, 1], hue=y, palette=cmap_bold, alpha=1.0, edgecolor=\"black\")\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.title(str(model) + \" Classification Boundry\")\n    plt.xlabel('PCA1')\n    plt.ylabel('PCA2')\n\n    plt.show()","74f6218b":"print('=========================')\nprint(\"=== LogisticRegression ===\")\nprint('=========================')\nmodel = LogisticRegression().fit(X_train, y_train)\neval_perf(model, X_test, y_test)\nplot_boundry(model)\n\nprint('=========================')\nprint(\"=== KNeighborsClassifier ===\")\nprint('=========================')\nmodel = KNeighborsClassifier().fit(X_train, y_train)\neval_perf(model, X_test, y_test)\nplot_boundry(model)\n\nprint('=========================')\nprint(\"=== GaussianNB ===\")\nprint('=========================')\nmodel = GaussianNB().fit(X_train, y_train)\neval_perf(model, X_test, y_test)\nplot_boundry(model)","7214e21c":"<a id=\"plot\"><\/a>\n# Plotting Boundry Points\nVisualizing the boundry points of the classifiers to get a sense of how they are making classifications and which points are caught or missed.","fb998519":"<a id=\"fit\"><\/a>\n# Fitting the Models and Evaluating Performance","7e1309a7":"<a id=\"pca\"><\/a>\n# Principal Component Analysis\nPerforming PCA on the dataset to graph the data in 2D","92fa14e7":"<a id=\"analyze\"><\/a>\n# Importing and Analyzing the Data\nAll responses were collected on a 5 point scale (0=Never, 1=Seldom, 2=Averagely, 3=Frequently, 4=Always).","a6c395e3":"# Table of Contents\n* [Importing and Analyzing the Data](#analyze)\n* [Fitting the Models and Evaluating Performance](#fit)\n* [Principle Component Analysis](#pca)\n    - [Plotting Boundry Points](#plot)"}}