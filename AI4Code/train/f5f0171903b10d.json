{"cell_type":{"fac465d0":"code","77a08d9f":"code","1b64f4ca":"code","06b30db8":"code","22727503":"code","fd72afc8":"code","86d8d52d":"code","07c795aa":"code","e8226539":"code","a61e9b8c":"code","f7a17b9e":"code","7fddfaf2":"code","f564f7d2":"code","caab8a14":"code","9a54ee62":"code","551004cc":"code","e4903b1b":"code","3938a318":"code","6aa2b5b7":"code","9293f2ce":"code","04cbe130":"code","0b7168cc":"code","39ce2888":"code","4f92a022":"code","cb7632f4":"code","ccab5118":"code","50dc33ea":"code","f4cc268c":"code","647ba55a":"code","f27d655d":"code","b1ec8c21":"code","377405bc":"code","38c0e79f":"code","e5b8c9ce":"code","fd2c0533":"code","b4109de4":"markdown","bcbfbb12":"markdown","8f0f4a13":"markdown","57334305":"markdown","de9b8b40":"markdown","7b768c41":"markdown","640d9fa4":"markdown"},"source":{"fac465d0":"import os\nimport time\n\nimport math\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns; sns.set(style=\"ticks\", color_codes=True)\n\nfrom sklearn.metrics import mean_absolute_error as MAE, mean_squared_error as MSE\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.feature_selection import RFE\n\nimport matplotlib.pyplot as plt\nfrom IPython.display import display","77a08d9f":"pd.set_option('display.max_rows', 50)\npd.set_option('display.max_columns', 100)\npd.set_option('display.width', 256)\n\nfrom pandas.core.common import SettingWithCopyWarning\n\nimport warnings\nwarnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)","1b64f4ca":"data_root = '..\/input\/walmart-recruiting-store-sales-forecasting'\ndatasets = dict()\nfor ds in ['train', 'test']:\n    dataset = pd.read_csv(f\"{data_root}\/{ds}.csv.zip\", sep=',', header=0,\n                          names=['Store', 'Dept', 'Date', 'weeklySales', 'isHoliday'] if ds=='train'\n                           else ['Store', 'Dept', 'Date', 'isHoliday'])\n    features = pd.read_csv(f\"{data_root}\/features.csv.zip\", sep=',', header=0,\n                           names=['Store', 'Date', 'Temperature', 'Fuel_Price', \n                                  'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', \n                                  'CPI', 'Unemployment', 'IsHoliday']).drop(columns=['IsHoliday'])\n    stores = pd.read_csv(f\"{data_root}\/stores.csv\", names=['Store', 'Type', 'Size'], sep=',', header=0)\n    dataset = dataset.merge(stores, how='left').merge(features, how='left')\n\n    dataset['Date'] = pd.to_datetime(dataset['Date'])\n    # dataset[\"isTomorrowHoliday\"] = dataset[\"isHoliday\"].shift(-1).fillna(False)\n    display(dataset.head())\n    \n    datasets[ds] = dataset","06b30db8":"datasets['train'][datasets['train'].weeklySales<=0]","22727503":"datasets['train'].dtypes","fd72afc8":"def describe_missing_values(df: pd.DataFrame):\n    miss_val = df.isnull().sum()\n    miss_val_percent = 100 * df.isnull().sum() \/ len(df)\n    miss_val_table = pd.concat([miss_val, miss_val_percent], axis=1)\n    miss_val_table_ren_columns = miss_val_table.rename(\n        columns = {0: 'Missing Values', \n                   1: '% of Total Values',}\n    )\n    miss_val_table_ren_columns = miss_val_table_ren_columns[\n        miss_val_table_ren_columns.iloc[:,1] != 0\n    ].sort_values('% of Total Values', ascending=False).round(1)\n    \n    print(f\"Dataframe has {df.shape[1]} columns,\")\n    print(f\"\\t\\t {miss_val_table_ren_columns.shape[0]} columns that have missing values.\")\n\n    return miss_val_table_ren_columns\n\n\ndef visualize_distribution_of_missing_values(df: pd.DataFrame):\n    df_nan_check = df.isna().sum().sort_values()\n    df_nan_check = df_nan_check.to_dict()\n    df_not_nan = []\n\n    nan_cols = 0\n\n    for key, value in df_nan_check.items():\n        df_nan_check[key] = int(value\/len(df)*100)\n        if df_nan_check[key] >= 80:\n            nan_cols += 1\n        else:\n            df_not_nan.append(key)\n\n    # Visualize\n    plt.figure(figsize=(9, 6))\n    plt.suptitle('Distribution of Empty Values', fontsize=19)\n    plt.bar(df_nan_check.keys(), df_nan_check.values())\n    plt.xticks(rotation=69)\n    plt.show()\n    \n\nfor ds in ['train', 'test']:\n    print(f'\\n\\n{ds}-set:')\n    print(describe_missing_values(datasets[ds]))\n    # visualize_distribution_of_missing_values(dataset)","86d8d52d":"def scatter(dataset, column):\n    plt.figure()\n    plt.scatter(dataset[column] , dataset['weeklySales'], alpha=0.169)\n    plt.ylabel('weeklySales')\n    plt.xlabel(column)","07c795aa":"for col in ['Fuel_Price', 'Size', 'CPI', 'Type', 'isHoliday', 'Unemployment', 'Temperature', 'Store', 'Dept']:\n    scatter(datasets['train'], col)","e8226539":"fig = plt.figure(figsize=(18, 14))\ncorr = datasets['train'].corr()\nc = plt.pcolor(corr)\nplt.yticks(np.arange(0.5, len(corr.index), 1), corr.index)\nplt.xticks(np.arange(0.5, len(corr.columns), 1), corr.columns, rotation=45)\nfig.colorbar(c)","a61e9b8c":"for ds in datasets.keys():\n    # make holidays more specific\n    datasets[ds]['Holiday_Type'] = None\n    datasets[ds].loc[(datasets[ds]['isHoliday']==True) & \n                     (datasets[ds]['Date'].dt.month==2), 'Holiday_Type'] = 'Super_Bowl'\n    datasets[ds].loc[(datasets[ds]['isHoliday']==True) & \n                     (datasets[ds]['Date'].dt.month==9), 'Holiday_Type'] = 'Labor_Day'\n    datasets[ds].loc[(datasets[ds]['isHoliday']==True) & \n                     (datasets[ds]['Date'].dt.month==11), 'Holiday_Type'] = 'Thanksgiving' \n    datasets[ds].loc[(datasets[ds]['isHoliday']==True) & \n                     (datasets[ds]['Date'].dt.month==12), 'Holiday_Type'] = 'Christmax'\n    datasets[ds].drop(columns=['isHoliday'], inplace=True)\n    \n    # 1-hot encoding for categorical features\n    datasets[ds] = pd.get_dummies(datasets[ds], columns=[\"Type\", \"Holiday_Type\"])\n    \n    # data imputation\n    datasets[ds].fillna(value=0, inplace=True)\n    display(datasets[ds].head())","f7a17b9e":"arr = datasets['train'].weeklySales.values\n\nsns.distplot(arr)\nplt.show()\n\nMAX_SALES = 100_000\narr = arr \/ MAX_SALES\nsns.distplot(arr)\nplt.show()\n\nsns.distplot(np.where(arr>MAX_SALES, MAX_SALES, arr))\nplt.show()","7fddfaf2":"for col in datasets['train'].columns:\n    if col in ['Store', 'Dept', 'Date']:\n        continue\n    if col not in list(datasets['test'].columns):\n        datasets['test'][col] = 0\n\ndatasets['train'].rename(columns={'weeklySales': 'Weekly_Sales'}, inplace=True)\n# datasets['train']['Weekly_Sales'][datasets['train']['Weekly_Sales']<0] = 0\ndatasets['train']['Weekly_Sales'] = datasets['train']['Weekly_Sales'] \/ MAX_SALES\ndatasets['train']['Previous_Sales'] = datasets['train'].groupby(['Store', 'Dept'])['Weekly_Sales'].shift(1)\ndatasets['train']['Previous_Sales'].fillna(value=0, inplace=True)\n\nfeature_names = [col for col in datasets['train'] if col not in ['Previous_Sales', 'Weekly_Sales']]\ndatasets['train'] = datasets['train'][feature_names+['Previous_Sales', 'Weekly_Sales']]\ndatasets['test'] = datasets['test'][feature_names]\n        \ndisplay(datasets['train'].head())\ndisplay(datasets['test'].head())","f564f7d2":"stats = datasets['train'].groupby([\"Store\", \"Dept\"])['Date'].agg(['count']).value_counts(sort=False)\nstats = stats.to_frame('#(store, dept)')\ndisplay(stats)\nstats.reset_index(drop=True, inplace=True)\nplt.plot(stats.index, stats.values)","caab8a14":"data_train, data_test = datasets['train'].copy(), datasets['test'].copy()","9a54ee62":"window_size = 4\nstride = 1\nX_all, y_all = [], []\nfor name, group in data_train.groupby([\"Store\", \"Dept\"]):\n    data_group = group.sort_values(by=['Date'], ascending=True)\n    data_group = data_group.drop(columns=[\"Store\", \"Dept\", \"Date\"]).to_numpy()\n    if data_group.shape[0] < window_size\/3:\n        continue\n\n    # Padding\n    n_samples = (len(data_group) - window_size) \/ stride\n    if n_samples != int(n_samples):\n        n_pads = (math.ceil(n_samples) - n_samples) * stride\n        if abs(n_pads-round(n_pads)) > 1e-7:\n            raise ValueError(f\"n_pads={n_pads} must be INT\")\n        n_pads = int(n_pads)\n        \n        data_padded = np.zeros(shape=(len(data_group)+n_pads, data_group.shape[1]))\n        data_padded[-data_group.shape[0]:, :] = data_group\n        data_group = data_padded\n    \n    X_group = data_group[:, :-1]\n    y_group = data_group[:, -1]\n\n    n_samples = int((len(data_group) - window_size) \/ stride)\n    for s in range(n_samples):\n        s_start = s * stride\n        X_sample = X_group[s_start:s_start+window_size, :]\n        y_sample = y_group[s_start+window_size]\n        X_all.append(X_sample.T)\n        y_all.append(y_sample.T)\n        \nX_all, y_all = np.array(X_all), np.array(y_all)\nprint('Total samples:', X_all.shape, y_all.shape)","551004cc":"data_test[['Previous_Sales', 'Weekly_Sales']] = None\ndata_group_extended = dict()\n\nfor name, test_group in data_test.groupby([\"Store\", \"Dept\"]):\n    \n    train_group = data_train[(data_train.Store==name[0]) & (data_train.Dept==name[1])]\n\n    ######################################################\n    # Concatenate train-set & test-set per (store, dept) #\n    ######################################################\n    data_group = pd.concat([train_group, test_group]) if len(train_group) > 0 else test_group\n    data_group.sort_values(by=['Date'], ascending=True, inplace=True)\n    # print(data_group[['Date', 'Size', 'Temperature', 'Weekly_Sales']].to_string())\n    # if data_group.duplicated(subset=['Store', 'Dept', 'Date'], keep=False).sum() > 0:\n    #     print(name)\n    data_group.reset_index(drop=True, inplace=True)\n    # display(data_group[data_group.Weekly_Sales.isna()])\n    \n    data_group_extended[name] = data_group\n    \nprint(len(data_group_extended))","e4903b1b":"!pip install --ignore-installed tsai","3938a318":"from tsai.all import *\n\nimport torch\n\ndef torch2np(tensor: torch.Tensor) -> np.array:\n    if torch.cuda.is_available():\n        tensor = tensor.cpu()\n    return tensor.numpy()","6aa2b5b7":"scorer = make_scorer(MSE, greater_is_better=False)","9293f2ce":"# Machine-Learning models\nML_models = [\n    # (RocketClassifier, {'num_kernels': 10_000}),\n    (MiniRocketClassifier, {'num_features': 10_000, 'max_dilations_per_kernel': 32}),\n    (MiniRocketVotingClassifier, {'num_features': 10_000, 'max_dilations_per_kernel': 32, 'n_estimators': 3}),\n]","04cbe130":"ML_max_samples = 70_000\nif len(X_all) > ML_max_samples:\n    X_ml, _, y_ml, _ = train_test_split(X_all, y_all, train_size=ML_max_samples, random_state=20_03_21, shuffle=True)\nelse:\n    X_ml, y_ml = X_all, y_all\nprint('Samples for ML model:\\t', X_ml.shape, y_ml.shape)\n\nX_train, X_val, y_train, y_val = train_test_split(X_ml, y_ml, \n                                                  train_size=0.69 if len(X_ml) < ML_max_samples else ML_max_samples\/\/2, \n                                                  random_state=4_10_20, \n                                                  shuffle=True)\nprint(X_train.shape, y_train.shape)\nprint(X_val.shape, y_val.shape)","0b7168cc":"# model = MiniRocketRegressor(num_features=10_000, \n#                             max_dilations_per_kernel=window_size,\n#                             normalize_features=False,\n#                             verbose=True,\n#                             scoring=scorer)\n\n# print(\"Training MiniRocket ...\")\n# timer.start(False)\n# model.fit(X_train, y_train)\n# t = timer.stop()\n# print(f\"\\t ... in {t}\")","39ce2888":"# y_pred = model.predict(X_val)\n# error = MSE(y_val, y_pred, squared=False) # Root-MSE\n# print(f'Val Error: {error:.5f}')","4f92a022":"# results = []\n# for name, (data_group, indices, X_test) in data_group_extended.items():\n#     if len(data_group) != X_test.shape[0]:\n#         raise ValueError(f\"{name} - {len(data_group)} != {X_test.shape[0]}\")\n#     else:\n#         y_pred = model.predict(X_test)\n#         data_group.Weekly_Sales = y_pred\n#         data_group['Date'] = pd.to_datetime(data_group.Date, format='%Y-%m-%d %H:%M:%S')\n#         data_group['Id'] = data_group['Store'].astype(int).apply(str) + '_' \\\n#                           + data_group['Dept'].astype(int).apply(str) + '_' \\\n#                           + data_group['Date'].dt.strftime('%Y-%m-%d')\n#         results.append(data_group[['Id', 'Weekly_Sales']])\n        \n# results = pd.concat(results)\n# results.Weekly_Sales = results.Weekly_Sales.apply(np.exp)\n# results.to_csv('submission_MiniRocket.csv', index=False)","cb7632f4":"# if not os.path.isdir('models'):\n#     os.mkdir('models')\n# model.save('MiniRocket')","ccab5118":"# Deep-Learning models\nDL_models = {\n    \"InceptionTime\": (InceptionTime, {'nf': 32, 'ks': window_size}), \n    \"InceptionTimePlus\": (InceptionTimePlus, {'nf': 32, 'ks': window_size, 'bottleneck': True, 'depth': 6, 'dilation': 1, 'stride': 1}), \n    \"TSTransformer\": (TST, {'max_seq_len': window_size*2, 'd_model': 32, 'd_ff': 16, 'n_layers': 2, 'n_heads': 4, }), \n    \"TSTransformerPlus\": (TSTPlus, {'max_seq_len': window_size*2, 'd_model': 32, 'd_ff': 16, 'n_layers': 2, 'n_heads': 4, }), \n}","50dc33ea":"X_train, X_val, y_train, y_val = train_test_split(X_all, y_all, \n                                                  train_size=0.69, \n                                                  random_state=4_10_20, \n                                                  shuffle=True)\nX_dl, y_dl, splits = combine_split_data([X_train, X_val], [y_train, y_val])","f4cc268c":"transformations = [None, [TSRegression()]]\nbatch_transformations = [TSStandardize(by_sample=False, by_var=False)]\ndsets = TSDatasets(X_dl, y_dl, splits=splits, tfms=transformations, inplace=True)\ndloaders = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[64, 32], batch_tfms=batch_transformations, num_workers=0)","647ba55a":"MODEL_NAME = 'TSTransformer'\nmodel, model_params = DL_models[MODEL_NAME]\nmodel = create_model(model, dls=dloaders, **model_params)\nlearner = Learner(dls=dloaders, model=model, metrics=[mae, rmse], opt_func=Adam)","f27d655d":"lr_min, lr_steepest = learner.lr_find(start_lr=1e-7, end_lr=10, num_it=1_690)\nlr_min, lr_steepest","b1ec8c21":"learner.fit_one_cycle(n_epoch=10, lr_max=lr_min)","377405bc":"# valid_probas, valid_targets, valid_preds = learner.get_preds(dl=dloaders.valid, with_decoded=True)\n# acc = torch2np((valid_targets==valid_preds).float().mean())","38c0e79f":"model.parameters","e5b8c9ce":"pip install pandas==1.1.5","fd2c0533":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nresults = []\nN_features = X_dl.shape[1]\n\nfor name, data_group in data_group_extended.items():\n    predict_indices = list(data_group[data_group.Weekly_Sales.isna()].index)\n    if len(predict_indices) == 0:\n        continue\n        \n    # Prediction\n    for idx in predict_indices:\n        # Padding, if needed\n        if idx >= window_size:\n            X_test = data_group.to_numpy()[idx-window_size:idx, 3:-1].T\n        else:\n            X_test = data_group.to_numpy()[:idx, 3:-1]\n            X_padded = np.zeros(shape=(window_size-idx, N_features))\n            X_test = np.vstack([X_padded, X_test]).T\n        X_test = np.expand_dims(X_test.astype(np.float64), axis=0)\n        X_test = torch.Tensor(X_test).to(device)\n        y_pred = learner.get_X_preds(X_test)[0]\n        y_pred = torch2np(y_pred.detach())\n        data_group.loc[idx, 'Weekly_Sales'] = y_pred[0]\n\n    # Append\n    data_group['Date'] = pd.to_datetime(data_group.Date, format='%Y-%m-%d %H:%M:%S')\n    data_group['Id'] = data_group['Store'].astype(int).apply(str) + '_' \\\n                      + data_group['Dept'].astype(int).apply(str) + '_' \\\n                      + data_group['Date'].dt.strftime('%Y-%m-%d')\n    results.append(data_group.loc[predict_indices, ['Id', 'Weekly_Sales']])\n        \nresults = pd.concat(results)\nresults.Weekly_Sales = results.Weekly_Sales * MAX_SALES\nresults.to_csv(f'submission_{MODEL_NAME}.csv', index=False)","b4109de4":"## MiniRocket","bcbfbb12":"## Deep Learning","8f0f4a13":"# **Data Manipulation**","57334305":"# **Modeling**","de9b8b40":"# **Data Exploration**","7b768c41":"## **Input Shape**: (N_samples, N_features, Max_seq_len)","640d9fa4":"# **Data Loading**"}}