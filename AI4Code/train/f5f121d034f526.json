{"cell_type":{"435beddd":"code","98083ad1":"code","5f6fb3d6":"code","5cfb2011":"code","e46cdd11":"code","ed6a4eb2":"code","92e57094":"code","c3e5274e":"code","f33211c7":"code","4c177afb":"code","00874531":"code","5124b858":"markdown"},"source":{"435beddd":"!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev","98083ad1":"import os\nos.environ['XLA_USE_BF16'] = \"1\"\nos.environ['XLA_TENSOR_ALLOCATOR_MAXSIZE'] = '100000000'\nimport torch\nimport pandas as pd\nfrom scipy import stats\nimport numpy as np\n\nfrom tqdm import tqdm\nfrom collections import OrderedDict, namedtuple\nimport torch.nn as nn\nfrom torch.optim import lr_scheduler\nimport joblib\n\nimport logging\nimport transformers\nfrom transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule, XLMRobertaTokenizer, XLMRobertaModel, XLMRobertaConfig\nimport sys\nfrom sklearn import metrics, model_selection","5f6fb3d6":"import warnings\nimport torch_xla\nimport torch_xla.debug.metrics as met\nimport torch_xla.distributed.data_parallel as dp\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.utils.utils as xu\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.test.test_utils as test_utils\nimport warnings\nwarnings.filterwarnings(\"ignore\")","5cfb2011":"class BERTDatasetTraining:\n    def __init__(self, X=None):\n        self.X = X\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, item):\n        \n        ids = self.X[item][0]\n        targets = self.X[item][1]\n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'targets': torch.tensor(targets, dtype=torch.float)\n        }","e46cdd11":"class CustomRoberta(nn.Module):\n    def __init__(self):\n        super(CustomRoberta, self).__init__()\n        self.num_labels = 1\n        self.roberta = transformers.XLMRobertaModel.from_pretrained(\"xlm-roberta-large\", output_hidden_states=False, num_labels=1)\n        self.dropout = nn.Dropout(p=0.2)\n        self.classifier = nn.Linear(1024, self.num_labels)\n\n    def forward(self,\n                input_ids=None,\n                attention_mask=None,\n                position_ids=None,\n                head_mask=None,\n                inputs_embeds=None):\n\n        _, o2 = self.roberta(input_ids,\n                               attention_mask=attention_mask,\n                               position_ids=position_ids,\n                               head_mask=head_mask,\n                               inputs_embeds=inputs_embeds)\n\n        logits = self.classifier(o2)       \n        outputs = logits\n        return outputs","ed6a4eb2":"model = CustomRoberta();","92e57094":"%%time\n# load pre-tokenized cached data\nX_train = np.load(\"..\/input\/sample-tpu-xlmr-pytorch-pad-on-fly\/x_train_tokenized.npy\", allow_pickle=True)\nX_valid = np.load(\"..\/input\/sample-tpu-xlmr-pytorch-pad-on-fly\/x_valid_tokenized.npy\", allow_pickle=True)\n\nX_train.shape, X_valid.shape","c3e5274e":"'''\npoor score for 240k; tune params maybe;\n'''\nX_train = X_train[:120000]\nimport gc; gc.collect()","f33211c7":"gc.collect();\ntrain_targets = X_train[:,1]\nvalid_targets = X_valid[:,1]\n\ntrain_dataset = BERTDatasetTraining(X_train)\nvalid_dataset = BERTDatasetTraining(X_valid)\ngc.collect()","4c177afb":"def run():\n    \n    def loss_fn(outputs, targets):\n        return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))\n\n    def train_loop_fn(data_loader, model, optimizer, device, scheduler=None, epoch=None):\n        \n        model.train()\n        \n        for bi, d in enumerate(data_loader):\n            \n            ids = d[\"ids\"]\n            targets = d[\"targets\"]\n\n            ids = ids.to(device, dtype=torch.long)\n            targets = targets.to(device, dtype=torch.float)\n\n            optimizer.zero_grad()\n            outputs = model(\n                input_ids=ids,\n                attention_mask = (ids>0).to(device),\n            )\n            \n            loss = loss_fn(outputs, targets)\n            \n            if bi % 100 == 0:\n                xm.master_print(f'bi={bi}, loss={loss}')\n\n            loss.backward()\n            xm.optimizer_step(optimizer)\n            \n            if scheduler is not None:\n                scheduler.step()\n        \n        model.eval();\n        # NB model is cached here because it somewhat works this way for 8 cores;\n        # DON'T ASK WHY; ;)\n        xm.save(model.state_dict(), f\"xlm_roberta_model_{epoch}.bin\")\n        \n    def eval_loop_fn(data_loader, model, device):\n        \n        model.eval()\n        fin_targets = []\n        fin_outputs = []\n        for bi, d in enumerate(data_loader):\n            \n            if bi %50 == 0:\n                xm.master_print(f'EVAL bi={bi}')\n            \n            ids = d[\"ids\"]\n            targets = d[\"targets\"]\n\n            ids = ids.to(device, dtype=torch.long)\n            targets = targets.to(device, dtype=torch.float)\n\n            outputs = model(\n                input_ids=ids,\n                attention_mask = (ids>0).to(device),\n            )\n\n            targets_np = targets.cpu().detach().numpy().tolist()\n            outputs_np = outputs.cpu().detach().numpy().tolist()\n            fin_targets.extend(targets_np)\n            fin_outputs.extend(outputs_np)    \n\n        return fin_outputs, fin_targets\n\n    \n    MAX_LEN = 128\n    TRAIN_BATCH_SIZE = 8\n    EPOCHS = 2 # change\n\n    tokenizer = transformers.XLMRobertaTokenizer.from_pretrained('xlm-roberta-large')\n\n    train_sampler = torch.utils.data.distributed.DistributedSampler(\n          train_dataset,\n          num_replicas=xm.xrt_world_size(),\n          rank=xm.get_ordinal(),\n          shuffle=True,\n    )\n\n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=TRAIN_BATCH_SIZE,\n        sampler=train_sampler,\n        drop_last=True,\n        num_workers=2,\n    )\n\n    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n          valid_dataset,\n          num_replicas=xm.xrt_world_size(),\n          rank=xm.get_ordinal(),\n          shuffle=False)\n\n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=8,\n        sampler=valid_sampler,\n        drop_last=False,\n        num_workers=1,\n    )\n\n    device = xm.xla_device();\n    model.to(device);\n\n    param_optimizer = list(model.named_parameters())\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n\n    lr = 1e-4 * xm.xrt_world_size()\n    num_train_steps = int(len(train_dataset) \/ TRAIN_BATCH_SIZE \/ xm.xrt_world_size() * EPOCHS)\n    xm.master_print(f'num_train_steps = {num_train_steps}, world_size={xm.xrt_world_size()}')\n\n    optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n\n    for epoch in range(EPOCHS):\n        para_loader = pl.ParallelLoader(train_data_loader, [device])\n        train_loop_fn(para_loader.per_device_loader(device), model, optimizer, device, scheduler=None, epoch=epoch)\n        para_loader = pl.ParallelLoader(valid_data_loader, [device])\n        o, t = eval_loop_fn(para_loader.per_device_loader(device), model, device)\n        auc = metrics.roc_auc_score(np.array(t) >= 0.5, o)\n        del o,t\n        gc.collect()\n        xm.master_print(f'AUC = {auc}')","00874531":"%%time\n\ndef _mp_fn(rank, flags):\n    a = run()\n\nFLAGS={}\nxmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')","5124b858":"- Original Abhisekh's code\n- Data setup from https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-xlm-roberta\n- Inspirations from @tanlikesmath https:\/\/www.kaggle.com\/tanlikesmath\/xlm-roberta-pytorch-xla-tpu\n- Special Thanks To Pytorch-XLA Devs!!"}}