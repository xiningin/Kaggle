{"cell_type":{"61374cbb":"code","07eb2689":"code","7be4fc0e":"code","cb5238fb":"code","40749dde":"code","0fbfde08":"code","107a778b":"code","db16d0b8":"code","84f8d8b6":"code","4ff6eda5":"code","a438bd79":"markdown","4a3047e5":"markdown","97278b77":"markdown","8de2f8b2":"markdown","09661762":"markdown","3a0a65f1":"markdown","1b3620eb":"markdown","aebe3906":"markdown","7f6ce319":"markdown","1fbe1c92":"markdown"},"source":{"61374cbb":"# Install additional packages\n!pip install learnergy\n!pip install tqdm","07eb2689":"import torch\nimport numpy as np\nimport pandas as pd\n\nfrom torch import nn, optim\nfrom tqdm import tqdm\n\nfrom learnergy.models.bernoulli import RBM\nfrom torch.utils.data import DataLoader, TensorDataset","7be4fc0e":"# Loading training and testing sets\ntrain = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')","cb5238fb":"# Extracts information regarding the availability of non-null values\nprint(train.info(), test.info())\n\n# Let us print some information regarding how a row of data looks like\nprint(train.head(), test.head())","40749dde":"# Gathers the training samples and labels and convert them to numpy arrays\nx_train = train.iloc[:, 1:].to_numpy()\ny_train = train['label'].to_numpy()\n\n# Gathers the testing samples and convert to a numpy array\nx_test = test.to_numpy()\n\n# Normalizes both training and testing data\nx_train = x_train \/ 255\nx_test = x_test \/ 255","0fbfde08":"# Creates a TensorDataset from training samples and labels\ntrain_set = TensorDataset(torch.Tensor(x_train), torch.Tensor(y_train).long())\n\n# Creates a simple tensor from testing samples\n# Note that we can directly iterate through tensors as they implement __getitem__ and __len__\ntest_set = torch.Tensor(x_test)","107a778b":"# Creates an RBM\nmodel = RBM(n_visible=784, n_hidden=384, steps=1, learning_rate=0.1,\n            momentum=0, decay=0, temperature=1, use_gpu=True)\n\n# Trains an RBM\nmse, pl = model.fit(train_set, batch_size=128, epochs=50)","db16d0b8":"# Defines some input variables\nbatch_size = 128\nn_classes = 10\nfine_tune_epochs = 100\n\n# Creates the fully-connected layer to append on top of RBM\nfc = nn.Linear(model.n_hidden, n_classes)\n\n# Checks if model uses GPU\nif model.device == 'cuda':\n    # If yes, put fully-connected on GPU\n    fc = fc.cuda()\n\n# Cross-Entropy loss is used for the discriminative fine-tuning\ncriterion = torch.nn.CrossEntropyLoss()\n\n# Creates the optimzers\noptimizer = [optim.Adam(model.parameters(), lr=0.001),\n             optim.Adam(fc.parameters(), lr=0.001)]\n\n# Creates the training batches\ntrain_batch = DataLoader(train_set, batch_size=batch_size, shuffle=False, num_workers=1)\n\n# For amount of fine-tuning epochs\nfor e in range(fine_tune_epochs):\n    print(f'Epoch {e+1}\/{fine_tune_epochs}')\n\n    # Resetting metrics\n    train_loss, val_acc = 0, 0\n    \n    # For every possible batch\n    for x_batch, y_batch in tqdm(train_batch):\n        # For every possible optimizer\n        for opt in optimizer:\n            # Resets the optimizer\n            opt.zero_grad()\n        \n        # Flatenning the samples batch\n        x_batch = x_batch.reshape(x_batch.size(0), model.n_visible)\n\n        # Checking whether GPU is avaliable and if it should be used\n        if model.device == 'cuda':\n            # Applies the GPU usage to the data and labels\n            x_batch = x_batch.cuda()\n            y_batch = y_batch.cuda()\n\n        # Passing the batch down the model\n        y = model(x_batch)\n\n        # Calculating the fully-connected outputs\n        y = fc(y)\n        \n        # Calculating loss\n        loss = criterion(y, y_batch)\n        \n        # Propagating the loss to calculate the gradients\n        loss.backward()\n        \n        # For every possible optimizer\n        for opt in optimizer:\n            # Performs the gradient update\n            opt.step()\n\n        # Adding current batch loss\n        train_loss += loss.item()\n\n    print(f'Loss: {train_loss \/ len(train_batch)}')","84f8d8b6":"# Creates the testing batches\ntest_batch = DataLoader(test_set, batch_size=len(test_set), shuffle=False, num_workers=1)\n\n# Calculates the test predictions for the model\nfor x_batch in test_batch:\n    # Flatenning the testing samples batch\n    x_batch = x_batch.reshape(x_batch.size(0), model.n_visible)\n\n    # Checking whether GPU is avaliable and if it should be used\n    if model.device == 'cuda':\n        # Applies the GPU usage to the data and labels\n        x_batch = x_batch.cuda()\n        y_batch = y_batch.cuda()\n\n    # Passing the batch down the model\n    y = model(x_batch)\n\n    # Calculating the fully-connected outputs\n    y = fc(y)\n\n    # Calculating predictions\n    _, preds = torch.max(y, 1)","4ff6eda5":"# Creates the submission data and outputs to a .csv file\noutput = pd.DataFrame({'ImageId': np.arange(1, len(test_set)+1), 'Label': preds.numpy()})\noutput.to_csv('mnist-rbm_submission.csv', index=False)","a438bd79":"## Predicting Unseen Data\n\nAfter fine-tuning the model, we can now use it to perform the final predictions. Note that as mentioned before, we  can directly input the test set tensor to the `DataLoader`.","4a3047e5":"## Final Outputting\n\nThe last step is to output the final result into a .csv file holding the images' identifiers and their predictions. Such a file will be ready to be submitted into the competition.","97278b77":"## Importing the Packages\n\nFollowing up, we are going to define all required imports to work within this notebook. In such case, we will be using `Pandas` to perform the data's input and output, as well as `Numpy` to perform the transiction between data frames and arrays.\n\nAdditionally, we will be using `PyTorch` and some of its classes to transform the arrays into datasets. Finally, we will be importing the `tqdm` and the `RBM` class from `Learnergy`.","8de2f8b2":"## Preparing the Data\n\nWith the pre-processing ready, we can convert them into `x` and `y` numpy arrays. Additionally, it is interesting to perform a simple normalization over the data, so it lies within the interval $[0, 1]$.","09661762":"## Pre-Processing the Data\n\nBefore preparing the data and creating the corresponding features and labels arrays, it is important to perform an exploratory analysis over the data, pre-process it for missing values or even remove some unwanted information that might impact our training procedure.\n\nAs `MNIST` is already a ready-to-go dataset, we will only be checking their data-frame information, as well as some initial samples.","3a0a65f1":"## Fine-Tuning to Downstream Classification Task\n\nAn unsupervised RBM model can be seen as a feature extractor module, where it encodes features into a low\/high dimensional latent-space.\n\nTherefore, we need to add a downstream task in order to perform the classification task. Such a task is accomplished by adding a fully-connected layer on top of the RBM and fine-tuning the whole model on the training data.","1b3620eb":"# Data Handling\n\n## Loading the Data\n\nThe first step should be pretty straightforward. Just fire up `Pandas` and loads the provided `.csv` files.","aebe3906":"## Creating PyTorch Datasets\n\nWith the arrays in hand, we will create tensors and further transform them to a `Dataset` class. Note that regarding the testing samples, we can directly input `PyTorch` tensors to the `DataLoader`.","7f6ce319":"# Installation\n\n## Adding Learnergy\n\nInitially, we are required to install any additional packages that might be used within this notebook. The [Restricted Boltzmann Machine](https:\/\/www.cs.toronto.edu\/~hinton\/absps\/guideTR.pdf) Python-based implementation is available at the [Learnergy](https:\/\/github.com\/gugarosa\/learnergy) package, which can be installed through `pip`.\n\nWe will also install the `tqdm` package, which will provide nice and clearer progress bars for our training\/predicting fine-tuning functions.","1fbe1c92":"# Modelling\n\n## Unsupervisely Training a Restricted Boltzmann Machine\n\nWith the data prepared, it is possible to instantiate an `RBM` class and fit the training data. There are several arguments that one can apply in the `RBM` class, as follows:\n\n* `n_visible` (int): Amount of visible units;\n* `n_hidden` (int): Amount of hidden units;\n* `steps` (int): Number of Gibbs' sampling steps;\n* `learning_rate` (float): Learning rate;\n* `momentum` (float): Momentum parameter;\n* `decay` (float): Weight decay used for penalization;\n* `temperature` (float): Temperature factor;\n* `use_gpu` (boolean): Whether GPU should be used or not.\n\nNote that `n_visible` should be set to the amount of features present in the dataset, e.g., MNIST has $784$ features."}}