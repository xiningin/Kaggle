{"cell_type":{"4e4f9c39":"code","c7747e04":"code","45224c59":"code","a577371b":"code","2a28b16b":"code","613fc458":"code","29a023f2":"code","0f7de92a":"code","7cb6daaa":"code","e6822c6b":"code","bb3c66a1":"code","e5090a6d":"code","67c342f9":"code","9114f77f":"code","33d80681":"code","5d6a4cf6":"code","a7f6ad38":"code","f972432d":"code","1ef3bc4d":"code","139dce6e":"code","263bd9c8":"code","8af35b39":"code","8d925b45":"code","7abf46db":"code","4ecb66aa":"code","4115335a":"code","8d10e4bb":"code","097eb483":"code","07b0bcd2":"code","121987ee":"code","e9c94f1e":"code","cd154f43":"code","5f38e134":"code","fe7ae4e3":"code","2e178f50":"code","7ba9d48f":"code","5397a143":"code","17f61997":"code","bdeeabe7":"code","d52b4e3f":"code","c4cb58ce":"code","1aaf530a":"code","7d2913de":"code","a8277be7":"code","66914f30":"code","b625a81e":"code","067943f3":"code","9d22db32":"code","ed1b8b91":"code","94ca435d":"code","f30d2d69":"code","a280ed3e":"code","71d4c3c2":"code","2a18b089":"code","d7f8506d":"markdown","e6b42962":"markdown","1c39b64d":"markdown","55870099":"markdown","e1c3a46c":"markdown","657319a4":"markdown","b30400a1":"markdown","8dcf484a":"markdown","d0a869a1":"markdown","13bd0c79":"markdown","37d86238":"markdown","bd9d82c5":"markdown","d09fa617":"markdown","4f6a05cd":"markdown","07bab2d6":"markdown","1a5dfa53":"markdown","d31301e2":"markdown","84b4cb8f":"markdown","c13fb5fb":"markdown","f5e57d24":"markdown","1ae24bcb":"markdown","c753f084":"markdown","d9409318":"markdown","7f5f073a":"markdown","b319268f":"markdown","6a65d3b6":"markdown","54e48969":"markdown","3cd75d79":"markdown","551b8242":"markdown","59c2dd73":"markdown","40f25c4d":"markdown","afc7fef7":"markdown","5c995075":"markdown","bf1baa05":"markdown","b19b9c45":"markdown","ff64367a":"markdown","44c41254":"markdown","a45a70e8":"markdown","690fb20e":"markdown","bea1fa05":"markdown","027caf87":"markdown","ee677b48":"markdown","6ae8c6f1":"markdown","cf337b54":"markdown","c05ca274":"markdown","f692771f":"markdown","83d1f642":"markdown","8303a180":"markdown","32c6e433":"markdown","ee31b0c0":"markdown","3b255af1":"markdown","518b629b":"markdown","abef8390":"markdown","5e8e4eb6":"markdown","af5c1976":"markdown","2723cfe1":"markdown","096c034c":"markdown","49567482":"markdown","92ad01ff":"markdown","6efdce01":"markdown","83308735":"markdown","f3cbae74":"markdown","97ccf641":"markdown","b7e0843b":"markdown","3b3d8365":"markdown","c5b4267d":"markdown","e6d28a4f":"markdown","85be02b9":"markdown","10bb990e":"markdown","a122f8ff":"markdown","2e32d85f":"markdown","774b36e0":"markdown","84bde506":"markdown","86d07927":"markdown","561821c0":"markdown","bbc0118b":"markdown","d1a86552":"markdown","7cd34181":"markdown","3958b6a9":"markdown","ce09b8e8":"markdown","f75a3c78":"markdown","59055d6e":"markdown","39979f19":"markdown","3090a0c3":"markdown","abdc8ac0":"markdown","4c874a47":"markdown","5dcf504c":"markdown"},"source":{"4e4f9c39":"#Importing necessary libraries\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nimport numpy as np\nfrom numpy.random import seed\nfrom numpy.random import randn\nimport seaborn as sns\nfrom PyAstronomy import pyasl\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nsns.set_style('whitegrid')\n\n%matplotlib inline","c7747e04":"a=[1,2,3,4,5,6]","45224c59":"computed_mean = np.mean(a)\nprint(f\"The computed mean is {computed_mean}\")\n\ninterval = stats.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=stats.sem(a))\nprint(f\"The mean has a confidence limit of {interval}\")","a577371b":"b = [2.2,2.21,2.22,2.23,2.24,2.25]","2a28b16b":"computed_mean = np.mean(b)\nprint(f\"The computed mean is {computed_mean}\")\n\ninterval = stats.t.interval(0.95, len(b)-1, loc=np.mean(b), scale=stats.sem(b))\nprint(f\"The mean has a confidence limit of {interval}\")","613fc458":"data1 = np.random.normal(0, 1, size=50)\ndata2 = np.random.normal(2, 1, size=50)","29a023f2":"true_mu = 0\n# Checking for data1\nonesample_results = stats.ttest_1samp(data1, true_mu)\n\nonesample_results","0f7de92a":"twosample_results = stats.ttest_ind(data1, data2)\ntwosample_results","7cb6daaa":"x1 = [0.0571, 0.0813, 0.0831, 0.0976, 0.0817, 0.0859, 0.0735, 0.0659, 0.0923, 0.0836]\nx2 = [0.0873, 0.0662, 0.0672, 0.0819, 0.0749, 0.0649, 0.0835, 0.0725]\nx3 = [0.0974, 0.1352, 0.0817, 0.1016, 0.0968, 0.1064, 0.105]\nx4 = [0.1033, 0.0915, 0.0781, 0.0685, 0.0677, 0.0697, 0.0764, 0.0689]\nx5 = [0.0703, 0.1026, 0.0956, 0.0973, 0.1039, 0.1045]\n\nstats.f_oneway(x1, x2, x3, x4, x5)","e6822c6b":"x1 = [0.0571, 0.0813, 0.0831, 0.0976, 0.0817, 0.0859, 0.0735, 0.0659, 0.0923, 0.0836]\nx2 = [0.0873, 0.0662, 0.0672, 0.0819, 0.0749, 0.0649, 0.0835, 0.0725]\nx3 = [0.0974, 0.1352, 0.0817, 0.1016, 0.0968, 0.1064, 0.105]\nx4 = [0.1033, 0.0915, 0.0781, 0.0685, 0.0677, 0.0697, 0.0764, 0.0689]\nx5 = [0.0703, 0.1026, 0.0956, 0.0973, 0.1039, 0.1045]","bb3c66a1":"stats.bartlett(x1, x2, x3, x4, x5)","e5090a6d":"stats.levene(x1, x2, x3, x4, x5)","67c342f9":"# Dummy data\nx = np.random.normal(0, 2, 10000)   # create random values based on a normal distribution","9114f77f":"# The histogram plot\npd.Series(x).hist()\nplt.title('Gaussian Distribution')","33d80681":"print( 'Excess kurtosis of normal distribution (should be 0): {}'.format( stats.kurtosis(x) ))\nprint( 'Skewness of normal distribution (should be 0): {}'.format( stats.skew(x) ))","5d6a4cf6":"# Dummy data\nweibull_x = np.random.weibull(10., 10000)   # create random values based on a weibull distribution.","a7f6ad38":"# The histogram plot\npd.Series(weibull_x).hist()\nplt.title('Weibull Distribution')","f972432d":"print( 'Excess kurtosis of weibull distribution: {}'.format( stats.kurtosis(weibull_x) ))\nprint( 'Skewness of weibull distribution: {}'.format( stats.skew(weibull_x) ))","1ef3bc4d":"# seed the random number generator\nseed(1)\n# generate univariate observations\ndata = 5 * randn(100) + 50\n# summarize\nprint('mean=%.3f stdv=%.3f' % (np.mean(data), np.std(data)))","139dce6e":"plt.hist(data);","263bd9c8":"from statsmodels.graphics.gofplots import qqplot","8af35b39":"qqplot(data, line='s');","8d925b45":"stat, p = stats.shapiro(data)\nprint('Statistics=%.3f, p=%.3f' % (stat, p))\n# interpret\nalpha = 0.05\nif p > alpha:\n    print('Sample looks Gaussian (fail to reject H0)')\nelse:\n    print('Sample does not look Gaussian (reject H0)')","7abf46db":"stat, p = stats.normaltest(data)\nprint('Statistics=%.3f, p=%.3f' % (stat, p))\n# interpret\nalpha = 0.05\nif p > alpha:\n    print('Sample looks Gaussian (fail to reject H0)')\nelse:\n    print('Sample does not look Gaussian (reject H0)')","4ecb66aa":"result = stats.anderson(data)\nprint('Statistic: %.3f' % result.statistic)\np = 0\nfor i in range(len(result.critical_values)):\n    sl, cv = result.significance_level[i], result.critical_values[i]\n    if result.statistic < result.critical_values[i]:\n        print('%.3f: %.3f, data looks normal (fail to reject H0)' % (sl, cv))\n    else:\n        print('%.3f: %.3f, data does not look normal (reject H0)' % (sl, cv))","4115335a":"observed_values=np.array([18,21,16,7,15])\nexpected_values=np.array([22,19,44,8,16])\n\nstats.chisquare(observed_values, f_exp=expected_values)","8d10e4bb":"np.random.seed(12345678)  #fix random seed to get the same result\nn1 = 200  # size of first sample\nn2 = 300  # size of second sample","097eb483":"rvs1 = stats.norm.rvs(size=n1, loc=0., scale=1)\nrvs2 = stats.norm.rvs(size=n2, loc=0.5, scale=1.5)\nstats.ks_2samp(rvs1, rvs2)","07b0bcd2":"import matplotlib.pyplot as plt\nfrom sklearn.ensemble import IsolationForest\n\nrng = np.random.RandomState(42)\n\n# Generate train data\nX = 0.3 * rng.randn(100, 2)\nX_train = np.r_[X + 2, X - 2]\n\n# Generate some regular novel observations\nX = 0.3 * rng.randn(20, 2)\nX_test = np.r_[X + 2, X - 2]\n\n# Generate some abnormal novel observations\nX_outliers = rng.uniform(low=-4, high=4, size=(20, 2))\n\n# fit the model\nclf = IsolationForest(max_samples=100,random_state=rng)\nclf.fit(X_train)\ny_pred_train = clf.predict(X_train)\ny_pred_test = clf.predict(X_test)\ny_pred_outliers = clf.predict(X_outliers)\n\n# plot the line, the samples, and the nearest vectors to the plane\nxx, yy = np.meshgrid(np.linspace(-5, 5, 50), np.linspace(-5, 5, 50))\nZ = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\nplt.figure(figsize=(15,8))\n\nplt.title(\"IsolationForest\")\nplt.contourf(xx, yy, Z, cmap=plt.cm.Blues_r)\n\nb1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white',\n                 s=20, edgecolor='k')\nb2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='green',\n                 s=20, edgecolor='k')\nc = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='red',\n                s=20, edgecolor='k')\nplt.axis('tight')\nplt.xlim((-5, 5))\nplt.ylim((-5, 5))\nplt.legend([b1, b2, c],\n           [\"training observations\",\n            \"new regular observations\", \"new abnormal observations\"],\n           loc=\"upper left\");","121987ee":"from scipy.stats import t, zscore\n\ndef grubbs(X, test='two-tailed', alpha=0.05):\n\n    '''\n    Performs Grubbs' test for outliers recursively until the null hypothesis is\n    true.\n\n    Parameters\n    ----------\n    X : ndarray\n        A numpy array to be tested for outliers.\n    test : str\n        Describes the types of outliers to look for. Can be 'min' (look for\n        small outliers), 'max' (look for large outliers), or 'two-tailed' (look\n        for both).\n    alpha : float\n        The significance level.\n\n    Returns\n    -------\n    X : ndarray\n        The original array with outliers removed.\n    outliers : ndarray\n        An array of outliers.\n    '''\n    print(\"Original data:\",X)\n    Z = zscore(X, ddof=1)  # Z-score\n    N = len(X)  # number of samples\n\n    # calculate extreme index and the critical t value based on the test\n    if test == 'two-tailed':\n        extreme_ix = lambda Z: np.abs(Z).argmax()\n        t_crit = lambda N: t.isf(alpha \/ (2.*N), N-2)\n    elif test == 'max':\n        extreme_ix = lambda Z: Z.argmax()\n        t_crit = lambda N: t.isf(alpha \/ N, N-2)\n    elif test == 'min':\n        extreme_ix = lambda Z: Z.argmin()\n        t_crit = lambda N: t.isf(alpha \/ N, N-2)\n    else:\n        raise ValueError(\"Test must be 'min', 'max', or 'two-tailed'\")\n\n    # compute the threshold\n    thresh = lambda N: (N - 1.) \/ np.sqrt(N) * \\\n        np.sqrt(t_crit(N)**2 \/ (N - 2 + t_crit(N)**2))\n\n    # create array to store outliers\n    outliers = np.array([])\n\n    # loop throught the array and remove any outliers\n    while abs(Z[extreme_ix(Z)]) > thresh(N):\n\n        # update the outliers\n        outliers = np.r_[outliers, X[extreme_ix(Z)]]\n        # remove outlier from array\n        X = np.delete(X, extreme_ix(Z))\n        # repeat Z score\n        Z = zscore(X, ddof=1)\n        N = len(X)\n    print(\"Cleaned Data\",X ,\"Outlier:\",outliers)\n    print(\"---\")\n    return X, outliers\n\n\n\n# setup some test arrays\nX = np.arange(-5, 6)\nX1 = np.r_[X, 100]\nX2 = np.r_[X, -100]\n\n# test the two-tailed case\nY, out = grubbs(X1)\nassert out == 100\nY, out = grubbs(X2)\nassert out == -100\n\n# test the max case\nY, out = grubbs(X1, test='max')\nassert out == 100\nY, out = grubbs(X2, test='max')\nassert len(out) == 0\n\n# test the min case\nY, out = grubbs(X1, test='min')\nassert len(out) == 0\nY, out = grubbs(X2, test='min')\nassert out == -100","e9c94f1e":"from sklearn.datasets import load_boston","cd154f43":"boston = load_boston()\nx = boston.data\ny = boston.target\ncolumns = boston.feature_names\n#create the dataframe\nboston_df = pd.DataFrame(boston.data)\nboston_df.columns = columns\nboston_df.head()","5f38e134":"boston_df.shape","fe7ae4e3":"sns.boxplot(x=boston_df['DIS'])","2e178f50":"fig, ax = plt.subplots(figsize=(16,8))\nax.scatter(boston_df['INDUS'], boston_df['TAX'])\nax.set_xlabel('Proportion of non-retail business acres per town')\nax.set_ylabel('Full-value property-tax rate per $10,000')\nplt.show()","7ba9d48f":"z = np.abs(stats.zscore(boston_df))\nprint(z)","5397a143":"threshold = 3\nprint(np.where(z > 3))","17f61997":"print(z[55][1])","bdeeabe7":"z_filtered_df= boston_df[(z < 3).all(axis=1)]\nz_filtered_df.shape","d52b4e3f":"Q1 = boston_df.quantile(0.25)\nQ3 = boston_df.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","c4cb58ce":"filtered_df =boston_df[~((boston_df < (Q1 - 1.5 * IQR))|(boston_df > (Q3 + 1.5 * IQR))).any(axis=1)]\nfiltered_df.head()","1aaf530a":"filtered_df.shape","7d2913de":"x = np.array([float(x) for x in \"-0.25 0.68 0.94 1.15 1.20 1.26 1.26 1.34 1.38 1.43 1.49 1.49 \\\n          1.55 1.56 1.58 1.65 1.69 1.70 1.76 1.77 1.81 1.91 1.94 1.96 \\\n          1.99 2.06 2.09 2.10 2.14 2.15 2.23 2.24 2.26 2.35 2.37 2.40 \\\n          2.47 2.54 2.62 2.64 2.90 2.92 2.92 2.93 3.21 3.26 3.30 3.59 \\\n          3.68 4.30 4.64 5.34 5.42 6.01\".split()])\n\n# Apply the generalized ESD\nr = pyasl.generalizedESD(x, 10, 0.05, fullOutput=True)\n\nprint(\"Number of outliers: \", r[0])\nprint(\"Indices of outliers: \", r[1])\nprint(\"        R      Lambda\")\nfor i in range(len(r[2])):\n    print(\"%2d  %8.5f  %8.5f\" % ((i+1), r[2][i], r[3][i]))\n\n# Plot the \"data\"\nplt.plot(x, 'b.')\n# and mark the outliers.\nfor i in range(r[0]):\n    plt.plot(r[1][i], x[r[1][i]], 'rp')\nplt.show()","a8277be7":"# Get some data\nx = np.random.normal(0.,0.1,50)\n\n# Introduce outliers\nx[27] = 1.0\nx[43] = -0.66\n\n# Run distance based outlier detection\nr = pyasl.pointDistGESD(x, 5)\n\nprint(\"Number of outliers detected: \", r[0])\nprint(\"Indices of these outliers: \", r[1])\n\nplt.plot(x, 'b.')\nfor i in range(len(r[1])):\n    plt.plot(r[1][i], x[r[1][i]], 'rp')\nplt.show()","66914f30":"# Generate some \"data\"\nx = np.arange(100)\ny = np.random.normal(x*0.067, 1.0, len(x))\n\n# Introduce an outliers\ny[14] = -5.0\ny[67] = +9.8\n\n# Find outliers based on a linear (deg = 1) fit.\n# Assign outlier status to all points deviating by\n# more than 3.0 standard deviations from the fit,\n# and show a control plot.\niin, iout = pyasl.polyResOutlier(x, y, deg=1, stdlim=3.0, controlPlot=True)\n\n# What about the outliers\nprint(\"Number of outliers: \", len(iout))\nprint(\"Indices of outliers: \", iout)\n\n# Remove outliers\nxnew, ynew = x[iin], y[iin]\n\n# Plot result (outlier in red)\nplt.plot(x, y, 'r.')\nplt.plot(xnew, ynew, 'bp');","b625a81e":"iin, iout = pyasl.slidingPolyResOutlier(x, y, 20, deg=1, stdlim=3.0, controlPlot=True)\n\n# What about the outliers\nprint(\"Number of outliers: \", len(iout))\nprint(\"Indices of outliers: \", iout)\n\n# Remove outliers\nxnew, ynew = x[iin], y[iin]\n\n# Plot result (outlier in red)\nplt.plot(x, y, 'r.')\nplt.plot(xnew, ynew, 'bp')\n","067943f3":"dataframe = pd.read_csv('..\/input\/diabetes.csv')\narr = dataframe.values\nX = arr[:,0:8]\ny = arr[:,8]\n# feature extraction\ntest = SelectKBest(score_func=chi2, k=4)\nfit = test.fit(X, y)\n# summarize scores\nnp.set_printoptions(precision=3)\nprint(fit.scores_)\nfeatures = fit.transform(X)\n# summarize selected features\nprint(features[0:5,:])","9d22db32":"dataframe[['Glucose','Insulin','BMI','Age']].head()","ed1b8b91":"X = arr[:,0:8]\ny = arr[:,8]\nmodel = LogisticRegression()\nrfe = RFE(model, 3)\nfit = rfe.fit(X, y)\nprint(\"Num Features:\", fit.n_features_)\nprint(\"Selected Features:\", fit.support_)\nprint(\"Feature Ranking: %s\", fit.ranking_)","94ca435d":"# The best 3 features\ndataframe.iloc[:,:-1].columns[fit.support_]","f30d2d69":"X = arr[:,0:8]\ny = arr[:,8]\n# feature extraction\npca = PCA(n_components=3)\nfit = pca.fit(X)\n# summarize components\nprint(\"Explained Variance:\", fit.explained_variance_ratio_)\nprint(fit.components_)","a280ed3e":"X = arr[:,0:8]\ny = arr[:,8]\n# feature extraction\nlda = LDA(n_components=3)\nfit = lda.fit(X,y)\n# summarize components\nprint(\"Explained Variance:\", fit.explained_variance_ratio_)","71d4c3c2":"X = arr[:,0:8]\ny = arr[:,8]\n# feature extraction\nmodel = ExtraTreesClassifier()\nmodel.fit(X, y)\nprint(model.feature_importances_)","2a18b089":"dataframe.columns","d7f8506d":"### d. Anderson Darling Test","e6b42962":"### h. Outlier detection based on polynomial fit\n","1c39b64d":"Isolation forest\u2019s basic principle is that outliers are few and far from the rest of the observations. To build a tree (training), the algorithm randomly picks a feature from the feature space and a random split value ranging between the maximums and minimums. This is made for all the observations in the training set. To build the forest a tree ensemble is made averaging all the trees in the forest.","55870099":"Since our p-value is greater than our significance level, we have good evidence to not reject the null-hypothesis. This is our expected result because the data was collected from a normal distribution.","e1c3a46c":"As the p value is higher than the significance level, all input samples are from populations with equal variance i.e. we do not reject the null hypothesis.","657319a4":"\\begin{align}\nAsk\\:questions,\\:the\\:data\\:will\\:confess.\n\\end{align}","b30400a1":"Box plot uses the IQR method to display data and outliers(shape of the data) but **in order to be get a list of identified outlier, we will need to use the mathematical formula and retrieve the outlier data.**\n\nThe interquartile range (IQR), also called the midspread or middle 50%, or technically H-spread, is a measure of statistical dispersion, being equal to the difference between 75th and 25th percentiles, or between upper and lower quartiles, \n\n\\begin{align}\nIQR = Q3 \u2212 Q1.\n\\end{align}\n\nIn other words, the IQR is the first quartile subtracted from the third quartile; these quartiles can be clearly seen on a box plot on the data.\n\nIt is a measure of the dispersion similar to standard deviation or variance, but is much more robust against outliers.","8dcf484a":"### a. Univariate Selection","d0a869a1":"**Note** : No feature scaling has been done in these exampls. A good way of carrying out feature selection is by first scaling the features and then fitting them to the model.","13bd0c79":"The chi-squared goodness of fit test or Pearson\u2019s chi-squared test is **used to assess whether a set of data is consistent with proposed values for the parameters. **\n \nNull hypothesis : Assume the observed values are similar as the expected values.","37d86238":"The one-way ANOVA tests the null hypothesis that **two or more groups have the same population mean**. The test is applied to samples from two or more groups, possibly with differing sizes.","bd9d82c5":"Statistical tests can be used to select those features that have the strongest relationship with the output variable.","d09fa617":"The Levene test also tests the null hypothesis that **all input samples are from populations with equal variances.** Levene\u2019s test is an alternative to Bartlett\u2019s test bartlett in the case where there are significant deviations from normality.","4f6a05cd":"So, the data point\u200a\u2014\u200a55th record on column ZN is an outlier.","07bab2d6":"---","1a5dfa53":"### c. Two sample t-Test for Equal Means","d31301e2":"A One Sample T-Test is a statistical test used to evaluate the null hypothesis that the mean **m** of a 1D sample dataset of independant observations is equal to the true mean **\u03bc** of the population from which the data is sampled. In other words, our null hypothesis is that,\n\n\\begin{align}\nm=\u03bc\n\\end{align}\n\nFor all of the tests, I will be using a significance level of 0.05. If the p value is below 0.05, we will be rejecting the null hypothesis.\n\nLet us generate some random data from the Normal Distriubtion. We will sample 50 points from a normal distribution with mean $\u03bc$=0 and variance $\u03c3^{2}=1$ and from another with mean $\u03bc$=2 and variance $\u03c3^{2}$=1.\n","84b4cb8f":"# *6. Feature Selection*","c13fb5fb":"\\begin{align}\nThe\\;end\n\\end{align}\n\n---","f5e57d24":"# *1. Location*","1ae24bcb":"The kernel focuses on Exploratory Data Anaysis using classical statistical techniques. I have used (and referenced) quantitative techniques as well as graphical techniques to perform various tests. \n\nLet\u2019s look at the different tests that can be carried out on any set of data:\n\n1.\t[Location](#1.-Location)\n\n      a. [Confidence Limits for the Mean](#a.-Confidence-Limits-for-the-Mean)\n        \n      b. [One sample t-Test](#b.-One-sample-t-Test)\n    \n      c. [Two sample t-Test for Equal Means](#c.-Two-sample-t-Test-for-Equal-Means)\n   \n      d. [One-Way ANOVA test](#d.-One-Way-ANOVA-test)\n    \n2.\t[Scale or variability or spread](#2.-Scale-or-variability-or-spread)\n\n      a. [Bartlett Test](# a.-Bartlett-Test)\n    \n      b. [Levene Test]( #b.-Levene-Test)\n    \n3.\t[Skewness and Kurtosis](#3.-Skewness-and-Kurtosis)\n\n4.\t[Distributional Measures](#4.-Distributional-Measures)\n\n      a. [Quantile-Quantile Plot](#a.-Quantile-Quantile-Plot)\n    \n      b. [Shapiro-Wilk test](#b.-Shapiro-Wilk-Test)\n    \n      c. [D\u2019Agostino\u2019s K^2 Test](#c.-D\u2019Agostino\u2019s-K^2-Test)\n    \n      d. [Anderson Darling Test](#d.-Anderson-Darling-Test)\n        \n      e. [Chi-Squared Goodness of Fit Test](#e.-Chi-Squared-Goodness-of-Fit-Test)\n        \n      f. [Kolmogorov-Smirnov Test](#f.-Kolmogorov-Smirnov-Test)\n    \n5.\t[Outlier detection](#5.-Outlier-detection)\n\n      a. [Isolation Forest](#a.-Isolation-Forest)\n    \n      b. [Grubbs Test](#b.-Grubbs-Test)\n    \n      c. [Box plot and scatter plot](#c.-Box-plot-and-scatter-plot)\n    \n      d. [Z-Score](#d.-Z-Score)\n    \n      e. [IQR Score](#e.-IQR-Score)\n        \n      f. [Generalized Extreme Deviate Test](#f.-Generalized-Extreme-Deviate-Test)\n        \n      g. [Distance-based outlier detection](#g.-Distance-based-outlier-detection)\n        \n      h. [Outlier detection based on polynomial fit](#h.-Outlier-detection-based-on-polynomial-fit)\n\n6.\t[Feature Selection](#6.-Feature-Selection)\n\n      a. [Univariate Selection](#a.-Univariate-Selection)\n        \n      b. [Recursive Feature Elimination](#b.-Recursive-Feature-Elimination)\n        \n      c. [Principal Component Analysis](#c.-Principal-Component-Analysis)\n        \n      d. [Linear Discriminant Analysis](#d.-Linear-Discriminant-Analysis)\n        \n      e. [Feature Importance](#e.-Feature-Importance)\n    \n\n---","c753f084":"Since our p-value is much less than the significance level of 0.05, then with great evidence we can reject our null hypothesis of identical means. ","d9409318":"Grubbs' test  is used to **detect a single outlier in a univariate data set that follows an approximately normal distribution.**\n\nIf you suspect more than one outlier may be present, it is recommended that you use either the Tietjen-Moore test or the generalized extreme studentized deviate test instead of the Grubbs' test.\n","7f5f073a":"### e. IQR Score","b319268f":"### b. Recursive Feature Elimination","6a65d3b6":"As the p value is higher than the significance level, all input samples are from populations with equal variance i.e. we do not reject the null hypothesis.","54e48969":"As the p-value is very low than the significance level of 0.05, the null hypothesis is rejected.","3cd75d79":"\n### d. One-Way ANOVA test","551b8242":"### b. Shapiro-Wilk test","59c2dd73":"Above plot shows three points between 10 to 12, these are outliers as there are not included in the box of other observation i.e no where near the quartiles. Box plot is useful for univariate data.","40f25c4d":"You can see that we are given an importance score for each attribute where the larger score, the more important the attribute.","afc7fef7":"# *2. Scale or variability or spread*","5c995075":"The Shapiro-Wilk test evaluates a data sample and **quantifies how likely it is that the data was drawn from a Gaussian distribution.** \n\nIn practice, the Shapiro-Wilk test is believed to be a reliable test of normality, although there is some suggestion that the test may be suitable for smaller samples of data, e.g. thousands of observations or fewer.","bf1baa05":"### b. Levene Test","b19b9c45":"You can see that RFE chose the the top 3 features as Pregnancies, BMI and DiabetesPedigreeFunction.\n\nThese are marked True in the support_ array and marked with a choice \u201c1\u201d in the ranking_ array.","ff64367a":"The algorithm implemented here is based on a polynomial fit to the data. After the fit is subtracted, the residuals are calculated. Based on their standard deviation, points with residuals deviating by more than the specified number of standard deviations from the fit are identified. Implementations with a single polynomial fit and a sliding window are available.","44c41254":"**This tests whether 2 samples are drawn from the same distribution.**\n\nNull Hypothesis : Assume the samples are from the same distribution.","a45a70e8":"### g. Distance-based outlier detection","690fb20e":"The Recursive Feature Elimination (or RFE) works by recursively removing attributes and building a model on those attributes that remain.\n\nIt uses the model accuracy to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute.\n\nThe example below uses RFE with the logistic regression algorithm to select the top 3 features.\n","bea1fa05":"The two-sample t-test is used to determine if **two population means are equal.** ","027caf87":"The D\u2019Agostino\u2019s K^2 test **calculates summary statistics from the data, namely kurtosis and skewness, to determine if the data distribution departs from the normal distribution.**\n\nSkew is a quantification of how much a distribution is pushed left or right, a measure of asymmetry in the distribution.\nKurtosis quantifies how much of the distribution is in the tail. It is a simple and commonly used statistical test for normality.","ee677b48":"Bartlett\u2019s test tests the null hypothesis that **all input samples are from populations with equal variances.** For samples from significantly non-normal populations, Levene\u2019s test is more robust.","6ae8c6f1":"Looking at the plot above, we can most of data points are lying bottom left side but there are points which are far from the population like top right corner","cf337b54":"**The Z-score is the signed number of standard deviations by which the value of an observation or data point is above the mean value of what is being observed or measured.**\n\nThe intuition behind Z-score is to describe any data point by finding their relationship with the Standard Deviation and Mean of the group of data points.\n\nWhile calculating the Z-score we re-scale and center the data and look for data points which are too far from zero. These data points which are way too far from zero will be treated as the outliers. In most of the cases a threshold of 3 or -3 is used i.e if the Z-score value is greater than or less than 3 or -3 respectively, that data point will be identified as outliers.","c05ca274":"Don\u2019t be confused by the results. The first array contains the list of row numbers and second array respective column numbers, which mean z[55][1] has a Z-score higher than 3.","f692771f":"---","83d1f642":"### d. Linear Discriminant Analysis","8303a180":"### c. D\u2019Agostino\u2019s K^2 Test","32c6e433":" The data is likely drawn from a Gaussian distribution.","ee31b0c0":"The example below uses the chi squared (chi^2) statistical test for non-negative features to select 4 of the best features from the Pima Indians onset of diabetes dataset.","3b255af1":"### b. Grubbs Test","518b629b":"### a. Quantile-Quantile Plot","abef8390":"We can reject the null hypothesis since the p-value is below 0.05.","5e8e4eb6":"# Statistical sidewalk (A dive into classical statistics for EDA)","af5c1976":"Confidence limits for the mean **are an interval estimate for the mean.** \n\nInterval estimates are often desirable because the estimate of the mean varies from sample to sample. Instead of a single estimate for the mean, a confidence interval generates a lower and upper limit for the mean. The interval estimate gives an indication of how much uncertainty there is in our estimate of the true mean. The narrower the interval, the more precise is our estimate.","2723cfe1":"Creating random data,","096c034c":"### e. Chi-Squared Goodness of Fit Test","49567482":"### d. Z-Score","92ad01ff":"---","6efdce01":"The generalized ESD test requires approximate normal distribution for the data points, which\u2014for example in the case of a spectrum\u2014can be a harsh limitation.\n\nThis function applies the generalized ESD test to the distances between adjacent data points, which are then requires to be distributed approximately normally. It will characterize a data point as an outlier, only if the distances to its right and left neighbor are abnormal as judged by the generalized ESD.","83308735":"### b. One sample t-Test","f3cbae74":"### f. Kolmogorov-Smirnov Test","97ccf641":"### c. Box plot and scatter plot","b7e0843b":"### c. Principal Component Analysis","3b3d8365":"---","c5b4267d":"Anderson-Darling Test (Theodore Anderson and Donald Darling) is a statistical test that can be used **to evaluate whether a data sample comes from one of among many known data samples.**\n\nIt can be used to check whether a data sample is normal. The test is a modified version of a more sophisticated nonparametric goodness-of-fit statistical test called the Kolmogorov-Smirnov test.\n\nA feature of the Anderson-Darling test is that it returns a list of critical values rather than a single p-value. This can provide the basis for a more thorough interpretation of the result.\n\nThe anderson() SciPy function implements the Anderson-Darling test. It takes as parameters the data sample and the name of the distribution to test it against. By default, the test will check against the Gaussian distribution (dist=\u2019norm\u2019).\n\nWe can interpret the results by failing to reject the null hypothesis that the data is normal if the calculated test statistic is less than the critical value at a chosen significance level.","e6d28a4f":"You can see the scores for each attribute and the 4 attributes chosen (those with the highest scores) are Glucose, Insulin, BMI and Age.","85be02b9":"Running the example creates the QQ plot showing the scatter plot of points in a diagonal line, closely fitting the expected diagonal pattern for a sample from a Gaussian distribution.","10bb990e":"# *5. Outlier detection*","a122f8ff":"So, above code removed around 90+ rows from the dataset i.e. outliers have been removed.","2e32d85f":"### a. Confidence Limits for the Mean","774b36e0":"# *4. Distributional Measures*","84bde506":"### f. Generalized Extreme Deviate Test","86d07927":"### e. Feature Importance","561821c0":"Principal Component Analysis (or PCA) **uses linear algebra to transform the dataset into a compressed form.**\n\nGenerally this is called a data reduction technique. A property of PCA is that you can choose the number of dimensions or principal component in the transformed result.","bbc0118b":"The generalized ESD (Extreme Studentized Deviate) test is **used to detect one or more outliers in a univariate data set that follows an approximately normal distribution.**","d1a86552":"The following code will remove the outliers from the dataset,","7cd34181":"---","3958b6a9":"In a QQ plot, **a perfect match for the normal distribution will be shown by a line of dots on a 45-degree angle from the bottom left of the plot to the top right.** It can be used to check if a distribution is normal or not.","ce09b8e8":"Bagged decision trees like Random Forest and Extra Trees can be used to estimate the importance of features.","f75a3c78":"Since our p-value is much less than our significance level, then with great evidence we can reject our null hypothesis of identical means. This is in alignment with our setup, since we sampled from two different normal pdfs with different means.","59055d6e":"### a. Bartlett Test","39979f19":" For unimodal continuous distributions, a skewness value > 0 means that there is more weight in the right tail of the distribution and < 0 means that there is more weight in the left tail of the distribution.","3090a0c3":"Example Kernel : https:\/\/www.kaggle.com\/leandrovrabelo\/t-test-on-alcohol-consumption","abdc8ac0":"Linear Discriminant Analysis (or LDA) uses linear algebra to transform the dataset into a compressed form while separating the dependent variable values as much as possible. ","4c874a47":"# *3. Skewness and Kurtosis*","5dcf504c":"### a. Isolation Forest"}}