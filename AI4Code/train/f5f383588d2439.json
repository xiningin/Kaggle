{"cell_type":{"313cb984":"code","34fc3440":"code","1dc5dd06":"code","7df4a6e3":"code","83a49f11":"code","5ca596a7":"code","f9ef1640":"code","3c9da17b":"code","515f2aa9":"code","7e7bc1fc":"code","5ad74091":"code","16542da9":"code","c5d9b245":"code","ae6ecf2c":"code","c308b079":"markdown","532391fc":"markdown","03e90d1e":"markdown","a3dd2ee3":"markdown","577d435a":"markdown","83696b17":"markdown","08034e28":"markdown","1667b5b2":"markdown","f9d6e0ce":"markdown","24020615":"markdown","515a639d":"markdown","9b1b6c53":"markdown","2272c4b4":"markdown","52789d5e":"markdown","a01d96f4":"markdown","ae215152":"markdown","fcf563aa":"markdown","69716adf":"markdown","be8e49b6":"markdown","e040c75d":"markdown","a9db2f16":"markdown"},"source":{"313cb984":"from keras import callbacks\nfrom keras import layers\nfrom keras import models\nfrom keras import regularizers\nfrom keras.utils import to_categorical\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd","34fc3440":"%matplotlib inline","1dc5dd06":"train_data_df = pd.read_csv('..\/input\/train.csv')\ntrain_data = train_data_df.values","7df4a6e3":"X_train = train_data[:, 1:]","83a49f11":"X_train = X_train.astype('float16') \/ 255.0","5ca596a7":"X_train = X_train.reshape((42000, 28, 28, 1))","f9ef1640":"y_train = train_data[:, 0]\ny_train = to_categorical(y_train)","3c9da17b":"test_data_df = pd.read_csv('..\/input\/test.csv')\ntest_data = test_data_df.values\n\nX_test = test_data\nX_test = X_test.astype('float16') \/ 255.0\nX_test = X_test.reshape((28000, 28, 28, 1))","515f2aa9":"# use this callback during exploratory training\n# stop_early_callback = callbacks.EarlyStopping(monitor='val_acc', \n#                                               patience=3, \n#                                               restore_best_weights=True,\n#                                               verbose=1)\n\n# use this callback during \"production\" training, when we don't set aside a validation dataset\nstop_early_callback = callbacks.EarlyStopping(monitor='acc', \n                                              patience=5, \n                                              restore_best_weights=True,\n                                              verbose=1)\n\ncallbacks_list = [stop_early_callback]","7e7bc1fc":"model = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1))) \nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu')) \nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\n\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(128, activation='relu'))\nmodel.add(layers.Dropout(0.4))\nmodel.add(layers.Dense(10, activation='softmax'))\n\nmodel.compile(optimizer='rmsprop', \n              loss='categorical_crossentropy', \n              metrics=['acc'])\n\nvalidation_split = 0.0 # we'll need this variable later, when deciding whether to graph our validation figures\n\nhistory = model.fit(X_train, \n                    y_train, \n                    callbacks=callbacks_list, \n                    epochs=30, \n                    batch_size=256, \n                    validation_split=validation_split, \n                    verbose=2)","5ad74091":"epochs = range(1, len(history.history['loss']) + 1)\nplt.plot(epochs, history.history['loss'], 'ro', label='training loss')\nif validation_split > 0:\n    plt.plot(epochs, history.history['val_loss'], 'r', label='val loss')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend()\nplt.show()\n\nplt.plot(epochs, history.history['acc'], 'bo', label='training acc')\nif validation_split > 0:\n    plt.plot(epochs, history.history['val_acc'], 'b', label='val acc')\nplt.xlabel('epochs')\nplt.ylabel('acc')\nplt.legend()","16542da9":"predictions_one_hot = model.predict(X_test)","c5d9b245":"predictions_ints = [np.argmax(prediction_one_hot) for prediction_one_hot in predictions_one_hot]","ae6ecf2c":"with open('predictions.csv', 'w') as predictions_file:\n    predictions_file.write('ImageId,Label' + '\\n')\n    for i in range(len(test_data)):\n        predictions_file.write(f'{i + 1},{predictions_ints[i]}' + '\\n')","c308b079":"Make a CSV containing our predictions, using Kaggle's required format.","532391fc":"When experimenting with hyperparamter values, it's helpful to see graphs of four values after each epoch:\n* training loss\n* validation loss\n* training accuracy\n* validation accuracy ","03e90d1e":"We *could* read the Kaggle-provided data file directly into a Numpy array, but Pandas has easier-to-understand methods for handling CSVs. So instead, we'll read the data file into a Pandas DataFrame and then convert that into a Numpy array.","a3dd2ee3":"Do most of the same preprocessing as described above (minus processing the targets), but this time on our test data.","577d435a":"Before investing time in building a neural net, we should understand what would constitute an improvement over random guessing. \n\nSince each image is one of ten digits, random guessing has a 10% chance of getting the digit right. **So if our model achieves of accuracy of >10% against the test dataset, we win.**","83696b17":"## Set up imports and magics","08034e28":"## Build and train a convolutional neural net","1667b5b2":"Use a callback to halt training if it hasn't seen any improvement in validation accuracy (during exploratory training runs) or training accuracy (during \"production\" runs, when when we train on the entire dataset with nothing held aside for validation) over the last three training epochs. It also realoads the weights from whichever epoch produced the highest validation accuracy. Sweet!","f9d6e0ce":"Convert the training data (which consists of images) to 28x28 matrices, representing the 28 pixel height, 28 pixel width of the images.","24020615":"## Determine baseline","515a639d":"## Make predictions for Kaggle's test data","9b1b6c53":"Cast the training data as 16-bit floats, which most GPUs find faster to deal with than 32-bit floats. Also, scale the training data so it ranges from 0 to 1.0. Neural nets usually operate better with this range of input data.","2272c4b4":"## Preprocess data","52789d5e":"Our model has three convolution layers in the base and one dense layer in the head. Experimentation with this dataset revealed that L1 and L2 regularizers have no effect. BachNormalization layers have no effect. The dropout layer in the head does marginally reduce overfitting.\n\nAll hyperparameters were set through long trial and error. It was easy to mess up validation accuracy, but very hard to get any higher than 99.0%.","a01d96f4":"The CNN's output is one-hot encoded, so we need to convert it from (for example) **[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]** to **2** since the latter is the form that Kaggle expects in submissions.","ae215152":"## Graph our progress","fcf563aa":"## Establish Keras callbacks","69716adf":"Per naming standards, store our training data in *X_train*. Exclude the labels, since we'll store that in a separate *y_train* variable. ","be8e49b6":"Make sure loss and accuracy charts appear directly in this notebook.","e040c75d":"Store the labels in *y_train* (per naming conventions), and convert them to one-hot encoding.","a9db2f16":"We need **keras** for the neural net, **matplotlib** for charting our loss and accuracy across epochs, **numpy** for decoding one-hot predictions, and **pandas** for loading the data."}}