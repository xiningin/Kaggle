{"cell_type":{"e8ce4de2":"code","d8280c4d":"code","13b7782b":"code","d06ac7f2":"code","87249d57":"code","4eab191a":"code","dff4a141":"code","ab88905e":"code","bb7498be":"code","59252aca":"code","8104c99e":"code","1e60c623":"code","ce916eb8":"code","8cb46859":"code","70b137cb":"code","b9caed92":"code","aa2ba40c":"code","a8cde9a1":"code","bd9e1bb7":"code","a3638979":"code","40603c3f":"code","5833cc35":"code","2da2f6a3":"code","61d8c325":"code","c924cc8f":"code","1212d182":"code","b15a6df5":"code","410c56a6":"code","8112dd2e":"code","a0b2e982":"markdown","06982868":"markdown","744fdbd4":"markdown","1b687f61":"markdown","7d502ddc":"markdown","076e0a2b":"markdown","30aa2c3d":"markdown","aa0445ba":"markdown","e1507c6c":"markdown","a1efc79e":"markdown","afb21329":"markdown","c7cf8e1a":"markdown","e5e0b7a3":"markdown","74f7d803":"markdown","a3439739":"markdown","3547b714":"markdown","c23b917f":"markdown","5c112082":"markdown","c567e7a7":"markdown","20e08018":"markdown","7fba8247":"markdown","df9d16bc":"markdown","d0b8cf4a":"markdown","212694c3":"markdown","699f8be3":"markdown","1451b808":"markdown","6f24766b":"markdown","f79ee1b6":"markdown","e7396cac":"markdown","38cee1ab":"markdown","96404305":"markdown","3318b404":"markdown","0f6fce1b":"markdown","2989a200":"markdown","8f325a0c":"markdown","bf934214":"markdown","158f348f":"markdown","8caf4702":"markdown","014de1c4":"markdown","f9ad9636":"markdown","5316ea78":"markdown","818affe1":"markdown","5d120696":"markdown","f51dc9c3":"markdown","24a933cf":"markdown","6a99854c":"markdown","b21fb771":"markdown","981677b9":"markdown"},"source":{"e8ce4de2":"# p-values calculation settings for distance and MIC\ncalc_p_values = False\np_values_iterations = 100","d8280c4d":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport scipy.stats\nfrom scipy.spatial import distance\nimport matplotlib.pyplot as plt\n%matplotlib inline","13b7782b":"pip install minepy","d06ac7f2":"from scipy.spatial.distance import pdist, squareform\nimport copy\nfrom minepy import MINE\nmine = MINE(alpha=0.6, c=15, est=\"mic_approx\")\n\ndef distcorr(Xval, Yval, pval=True, nruns=500):\n    \"\"\" Compute the distance correlation function, returning the p-value.\n    Based on Satra\/distcorr.py (gist aa3d19a12b74e9ab7941)\n    >>> a = [1,2,3,4,5]\n    >>> b = np.array([1,2,9,4,4])\n    >>> distcorr(a, b)\n    (0.76267624241686671, 0.266)\n    \"\"\"\n    X = np.atleast_1d(Xval)\n    Y = np.atleast_1d(Yval)\n    if np.prod(X.shape) == len(X):\n        X = X[:, None]\n    if np.prod(Y.shape) == len(Y):\n        Y = Y[:, None]\n    X = np.atleast_2d(X)\n    Y = np.atleast_2d(Y)\n    n = X.shape[0]\n    if Y.shape[0] != X.shape[0]:\n        raise ValueError('Number of samples must match')\n    a = squareform(pdist(X))\n    b = squareform(pdist(Y))\n    A = a - a.mean(axis=0)[None, :] - a.mean(axis=1)[:, None] + a.mean()\n    B = b - b.mean(axis=0)[None, :] - b.mean(axis=1)[:, None] + b.mean()\n\n    dcov2_xy = (A * B).sum() \/ float(n * n)\n    dcov2_xx = (A * A).sum() \/ float(n * n)\n    dcov2_yy = (B * B).sum() \/ float(n * n)\n    dcor = np.sqrt(dcov2_xy) \/ np.sqrt(np.sqrt(dcov2_xx) * np.sqrt(dcov2_yy))\n\n    if pval:\n        greater = 0\n        for i in range(nruns):\n            Y_r = copy.copy(Yval)\n            np.random.shuffle(Y_r)\n            if distcorr(Xval, Y_r, pval=False)[0] > dcor:\n                greater += 1\n                \n        return (dcor, greater \/ float(nruns))\n    else:\n        return (dcor, np.nan)\n    \ndef miccorr(X, Y, pval=True, nruns=500):\n    mine.compute_score(X, Y)\n    \n    micc = mine.mic()\n    \n    if pval:\n        greater = 0\n        for i in range(nruns):\n            Y_r = copy.copy(Y)\n            np.random.shuffle(Y_r)\n            if miccorr(X, Y_r, pval=False)[0] > micc:\n                greater += 1\n                \n        return (micc, greater \/ float(nruns))\n    else:\n        return (micc, np.nan)\n    ","87249d57":"import time\n    \nclass catchtime(object):\n    def __enter__(self):\n        self.t = time.time()\n        return self\n\n    def __exit__(self, type, value, traceback):\n        self.t = time.time() - self.t","4eab191a":"from sklearn.feature_selection import mutual_info_regression\n\ndef numerical_correlations(X, Y):\n    fig, ax = plt.subplots(1,1,figsize=(15,7))\n    sns.scatterplot(x=X, y=Y)\n    \n    pear = []\n    spea = []\n    kend = []\n    dist = []\n    mic = []\n    mut = []\n    \n    with catchtime() as t_pear:\n        pearsons = scipy.stats.pearsonr(X, Y)\n        \n    with catchtime() as t_spea:\n        spearmans = scipy.stats.spearmanr(X, Y)\n\n    with catchtime() as t_kend:\n        kendall = scipy.stats.kendalltau(X, Y)\n\n    with catchtime() as t_dist:\n        distan = distcorr(X, Y, calc_p_values, p_values_iterations)\n        \n    with catchtime() as t_mic:\n        micc = miccorr(X, Y, calc_p_values, p_values_iterations)\n        \n    with catchtime() as t_mut:\n        mutual_info = mutual_info_regression(X.reshape(-1, 1), Y)[0], 0\n        \n    pear = [pearsons[0], pearsons[1], t_pear.t]\n    spea = [spearmans[0], spearmans[1], t_spea.t]\n    kend = [kendall[0], kendall[1], t_kend.t]\n    dist = [distan[0], distan[1], t_dist.t]\n    mic = [micc[0], micc[1], t_mic.t]\n    mut = [mutual_info[0], mutual_info[1], t_mut.t]\n    \n    df = pd.DataFrame({\"index\": [\"coefficient\", \"p_value\", \"runtime\"], \"pearson\": pear, \"spearman\":spea, \"kendall\": kend, \"distance\":dist, \"mic\": mic}).set_index(\"index\")\n    df = df.T\n    df[\"runtime_multiple\"] = round(df[\"runtime\"] \/ min (df[\"runtime\"]))\n    return df","dff4a141":"import math\n\n# range randomization\ndef randomize(y, division):\n\n    ran = y.max() - y.min()\n\n    if math.isnan(ran):\n        ran = 1\n        \n    np.random.seed(0)\n    y = y + np.random.rand(len(X)) * ran \/ division\n    \n    return y","ab88905e":"np.random.seed(0)\n\nX = np.arange(-5, 5, 0.01)\ny = np.random.rand(len(X))\n\ndf = numerical_correlations(X, randomize(y, 20))\n\ndf.apply(lambda s: s.apply(lambda x: format(x, 'g')))","bb7498be":"X = np.arange(-5, 5, 0.01)\ny = X\n\ndf = numerical_correlations(X, randomize(y, 10))\n\ndf.apply(lambda s: s.apply(lambda x: format(x, 'g')))","59252aca":"X = np.arange(-5, 5, 0.01)\ny = X.copy()\n\nX_outliers = list(np.arange(-5, 5, 0.5))\n\nfor index, x in enumerate(X):\n    if round(x, 5) in X_outliers:\n        y[index] = y[index] + np.random.rand(1) * 20 * (1 if np.random.random() < 0.5 else -1)\n            \ndf = numerical_correlations(X, randomize(y, 10))\n\ndf.apply(lambda s: s.apply(lambda x: format(x, 'g')))","8104c99e":"X = np.arange(-5, 5, 0.01)\ny = -X\n\ndf = numerical_correlations(X, randomize(y, 10))\n\ndf.apply(lambda s: s.apply(lambda x: format(x, 'g')))","1e60c623":"X = np.arange(0.005, 5, 0.005)\ny = np.log(X)\n\ndf = numerical_correlations(X, randomize(y, 15))\n\ndf.apply(lambda s: s.apply(lambda x: format(x, 'g')))","ce916eb8":"X = np.arange(-5, 5, 0.01)\ny = X**2\n\ndf = numerical_correlations(X, randomize(y, 10))\n\ndf.apply(lambda s: s.apply(lambda x: format(x, 'g')))","8cb46859":"X = np.arange(-5, 5, 0.01)\ny = np.cos(X)\n\ndf = numerical_correlations(X, randomize(y, 10))\n\ndf.apply(lambda s: s.apply(lambda x: format(x, 'g')))","70b137cb":"from scipy.stats import chi2_contingency, chi2\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.metrics.cluster import normalized_mutual_info_score\nfrom sklearn.metrics.cluster import adjusted_mutual_info_score\n\ndef categorical_correlations(X, Y):\n    \n    chi = []\n    cram = []\n    mic = []\n    muti = []\n    nmuti = []\n    amuti = []\n    \n    fig, ax = plt.subplots(1,1,figsize=(15,7))\n    sns.countplot(x=X, hue=Y)\n\n    with catchtime() as t_chi:\n        chisquared = chi2_contingency(observed=pd.crosstab(X, Y, margins=False))\n        \n        crit = chi2.ppf(q=0.95, df=chisquared[2] - 1)\n\n        print(\"Chi2 critical value: \" + str(crit))\n\n        p_value = 1 - chi2.cdf(x=chisquared[0], df=chisquared[2] - 1)\n\n    with catchtime() as t_cram:\n        cramer = cramers_corrected_stat(X, Y)\n\n#     with catchtime() as t_mic:\n#         micc = miccorr(LabelEncoder().fit_transform(X), LabelEncoder().fit_transform(Y), calc_p_values, p_values_iterations)\n        \n    with catchtime() as t_muti:\n        mutual_info = mutual_info_classif(LabelEncoder().fit_transform(X).reshape(-1, 1), LabelEncoder().fit_transform(Y), discrete_features=True)[0], np.nan\n    \n    with catchtime() as t_nmuti:\n        nmutual_info = normalized_mutual_info_score(LabelEncoder().fit_transform(X), LabelEncoder().fit_transform(Y)), np.nan\n        \n#     with catchtime() as t_amuti:\n#         amutual_info = adjusted_mutual_info_score(LabelEncoder().fit_transform(X), LabelEncoder().fit_transform(Y)), 0\n        \n    chi = [chisquared[0], chisquared[1], t_chi.t]\n    cram = [cramer[0], cramer[1], t_cram.t]\n#     mic = [micc[0], micc[1], t_mic.t]\n    muti = [mutual_info[0], mutual_info[1], t_muti.t]\n    nmuti = [nmutual_info[0], nmutual_info[1], t_nmuti.t]\n#     amuti = [amutual_info[0], amutual_info[1], t_amuti.t]\n    \n    df = pd.DataFrame({\"index\": [\"coefficient\", \"p_value\", \"runtime\"], \"Chi2\": chi, \"Cramer's V\":cram, \"Mutual Information\": muti,\"Normalized Mutual Information\": nmuti}).set_index(\"index\")\n    df = df.T\n    df[\"runtime_multiple\"] = round(df[\"runtime\"] \/ min (df[\"runtime\"]))\n    return df\n\ndef cramers_corrected_stat(X, Y):\n    \"\"\" calculate Cramers V statistic for categorial-categorial association.\n        uses correction from Bergsma and Wicher, \n        Journal of the Korean Statistical Society 42 (2013): 323-328\n    \"\"\"\n    crosstab = pd.crosstab(X, Y, margins=False)\n    \n    chi2 = chi2_contingency(crosstab)\n    \n    n = crosstab.sum().sum()\n    \n    phi2 = chi2[0]\/n\n    r,k = crosstab.shape\n    \n    phi2corr = max(0, phi2 - ((k-1)*(r-1))\/(n-1))\n    rcorr = r - ((r-1)**2)\/(n-1)\n    kcorr = k - ((k-1)**2)\/(n-1)\n\n    return np.sqrt(phi2corr \/ min( (kcorr-1), (rcorr-1))), chi2[1]","b9caed92":"np.random.seed(12)\n\nanimal = np.random.choice(a=[\"turkey\", \"cow\", \"goat\", \"sheep\", \"chicken\"], p=[0.2, 0.2, 0.2, 0.2, 0.2], size=10000)\nproduct = np.random.choice(a=[\"egg\", \"milk\"], p=[0.5, 0.5], size=10000)\ndf = pd.DataFrame({\"animal\": animal, \"product\": product})\ncategorical_correlations(df[\"animal\"], df[\"product\"])","aa2ba40c":"# 2 categorical feature imbalance\nnp.random.seed(12)\n\nanimal = np.random.choice(a=[\"turkey\", \"cow\", \"goat\", \"sheep\", \"chicken\"], p=[0.45, 0.1, 0.25, 0.15, 0.05], size=10000)\nproduct = np.random.choice(a=[\"egg\", \"milk\"], p=[0.2, 0.8], size=10000)\ndf = pd.DataFrame({\"animal\": animal, \"product\": product})\ncategorical_correlations(df[\"animal\"], df[\"product\"])","a8cde9a1":"# Balanced data\nnp.random.seed(12)\n\nanimal = np.random.choice(a=[\"turkey\", \"cow\", \"goat\", \"sheep\", \"chicken\"], p=[0.2, 0.2, 0.2, 0.2, 0.2], size=10000)\nproduct = np.random.choice(a=[\"egg\", \"milk\"], p=[0.5, 0.5], size=10000)\ndf = pd.DataFrame({\"animal\": animal, \"product\": product})\ndf.loc[(df[\"animal\"] == \"turkey\"), \"product\"] = \"egg\"\ncategorical_correlations(df[\"animal\"], df[\"product\"])","bd9e1bb7":"# Imbalanced data\nnp.random.seed(12)\n\nanimal = np.random.choice(a=[\"turkey\", \"cow\", \"goat\", \"sheep\", \"chicken\"], p=[0.45, 0.1, 0.25, 0.15, 0.05], size=10000)\nproduct = np.random.choice(a=[\"egg\", \"milk\"], p=[0.2, 0.8], size=10000)\ndf = pd.DataFrame({\"animal\": animal, \"product\": product})\ndf.loc[(df[\"animal\"] == \"turkey\"), \"product\"] = \"egg\"\ncategorical_correlations(df[\"animal\"], df[\"product\"])","a3638979":"# Imbalanced data\nnp.random.seed(12)\n\nanimal = np.random.choice(a=[\"turkey\", \"cow\", \"goat\", \"sheep\", \"chicken\"], p=[0.45, 0.1, 0.25, 0.15, 0.05], size=10000)\nproduct = np.random.choice(a=[\"egg\", \"milk\"], p=[0.2, 0.8], size=10000)\ndf = pd.DataFrame({\"animal\": animal, \"product\": product})\ndf.loc[(df[\"animal\"] == \"chicken\"), \"product\"] = \"egg\"\ncategorical_correlations(df[\"animal\"], df[\"product\"])","40603c3f":"np.random.seed(12)\n\nanimal = np.random.choice(a=[\"turkey\", \"cow\", \"goat\", \"sheep\", \"chicken\"], p=[0.2, 0.2, 0.2, 0.2, 0.2], size=10000)\nproduct = np.random.choice(a=[\"egg\", \"milk\"], p=[0.5, 0.5], size=10000)\ndf = pd.DataFrame({\"animal\": animal, \"product\": product})\ndf.loc[(df[\"animal\"] == \"turkey\"), \"product\"] = \"egg\"\ndf.loc[(df[\"animal\"] == \"goat\"), \"product\"] = \"milk\"\ndf.loc[(df[\"animal\"] == \"sheep\"), \"product\"] = \"milk\"\ndf.loc[(df[\"animal\"] == \"cow\"), \"product\"] = \"milk\"\ndf.loc[(df[\"animal\"] == \"chicken\"), \"product\"] = \"egg\"\ncategorical_correlations(df[\"animal\"], df[\"product\"])","5833cc35":"np.random.seed(12)\n\nanimal = np.random.choice(a=[\"turkey\", \"cow\", \"goat\", \"sheep\", \"chicken\"], p=[0.2, 0.2, 0.2, 0.2, 0.2], size=10000)\nproduct = np.random.choice(a=[\"egg\", \"milk\"], p=[0.5, 0.5], size=10000)\ndf = pd.DataFrame({\"animal\": animal, \"product\": product})\ndf.loc[(df[\"animal\"] == \"turkey\"), \"product\"] = \"turkey egg\"\ndf.loc[(df[\"animal\"] == \"goat\"), \"product\"] = \"goat milk\"\ndf.loc[(df[\"animal\"] == \"sheep\"), \"product\"] = \"sheep milk\"\ndf.loc[(df[\"animal\"] == \"cow\"), \"product\"] = \"cow milk\"\ndf.loc[(df[\"animal\"] == \"chicken\"), \"product\"] = \"chicken egg\"\ncategorical_correlations(df[\"animal\"], df[\"product\"])","2da2f6a3":"import scipy.stats as stats\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy.stats import chi2_contingency, chi2\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.metrics.cluster import normalized_mutual_info_score\nfrom sklearn.metrics.cluster import adjusted_mutual_info_score\nfrom sklearn.preprocessing import StandardScaler\n\ndef categorical_numerical_correlations(X, Y, group_list):\n    \n    pbs = []\n    ano = []\n    muti = []\n    nmuti = []\n    amuti = []\n    krus = []\n    \n    bins = [-4, -3, -2, -1, 0, 1, 2, 3, 4]\n    \n    \n    fig, ax = plt.subplots(1,1,figsize=(15,7))\n    sns.stripplot(x=X, y=Y)\n    sns.pointplot(x=X, y=Y)\n    \n\n    with catchtime() as t_pbs:\n        point_biserial = stats.pointbiserialr(LabelEncoder().fit_transform(X), Y)\n\n    with catchtime() as t_ano:\n        \n        anova = stats.f_oneway(*group_list)\n        \n        a = len(X.value_counts())\n        N = len(X)\n        dfn = a - 1\n        dfd = N - a\n\n        crit = stats.f.ppf(0.95, dfn, dfd)\n        print(\"F critical value: \" + str(crit))\n\n\n    with catchtime() as t_muti:\n        mutual_info = mutual_info_classif(LabelEncoder().fit_transform(X).reshape(-1, 1), np.searchsorted(bins, Y))[0], np.nan\n    \n    with catchtime() as t_nmuti:\n        nmutual_info = normalized_mutual_info_score(LabelEncoder().fit_transform(X), np.searchsorted(bins, Y)), np.nan\n        \n    with catchtime() as t_amuti:\n        amutual_info = adjusted_mutual_info_score(LabelEncoder().fit_transform(X), np.searchsorted(bins, Y)), np.nan\n        \n    pbs = [point_biserial[0], point_biserial[1], t_pbs.t]\n    ano = [anova[0], anova[1], t_ano.t]\n    muti = [mutual_info[0], mutual_info[1], t_muti.t]\n    nmuti = [nmutual_info[0], nmutual_info[1], t_nmuti.t]\n    amuti = [amutual_info[0], amutual_info[1], t_amuti.t]\n    \n    df = pd.DataFrame({\"index\": [\"coefficient\", \"p_value\", \"runtime\"], \"ANOVA\":ano, \"Normalized mutual information\": nmuti, \"Adjusted mutual information\": amuti}).set_index(\"index\")\n    df = df.T\n    df[\"runtime_multiple\"] = round(df[\"runtime\"] \/ min (df[\"runtime\"]))\n    return df","61d8c325":"np.random.seed(10)\n\nY = np.arange(-5, 5, 0.01)\nX = np.random.choice(a=[\"turkey\", \"cow\", \"goat\", \"sheep\", \"chicken\"], p=[0.2, 0.2, 0.2, 0.2, 0.2], size=len(Y))\n\ndf = pd.DataFrame({\"animal\": X, \"molecules\": Y})\n\ndf[\"molecules\"] = (df[\"molecules\"] - df[\"molecules\"].mean()) \/ df[\"molecules\"].std()\ncategorical_numerical_correlations(df[\"animal\"], df[\"molecules\"], df.groupby(\"animal\")[\"molecules\"].apply(list))","c924cc8f":"np.random.seed(10)\n\nY = np.arange(-5, 5, 0.01)\nX = np.random.choice(a=[\"turkey\", \"cow\", \"goat\", \"sheep\", \"chicken\"], p=[0.5, 0.05, 0.2, 0.15, 0.1], size=len(Y))\n\ndf = pd.DataFrame({\"animal\": X, \"molecules\": Y})\n\ndf[\"molecules\"] = (df[\"molecules\"] - df[\"molecules\"].mean()) \/ df[\"molecules\"].std()\ncategorical_numerical_correlations(df[\"animal\"], df[\"molecules\"], df.groupby(\"animal\")[\"molecules\"].apply(list))","1212d182":"# Balanced data\n\nnp.random.seed(10)\n\nY = np.arange(-5, 5, 0.01)\n\nX = np.random.choice(a=[\"turkey\", \"cow\", \"goat\", \"sheep\", \"chicken\"], p=[0.2, 0.2, 0.2, 0.2, 0.2], size=len(Y))\ndf = pd.DataFrame({\"animal\": X, \"molecules\": Y})\ndf.loc[(df[\"animal\"] == \"chicken\"), \"molecules\"] = (np.random.rand(len(df[df[\"animal\"] == \"chicken\"]), 1)) * 3\n\ndf[\"molecules\"] = (df[\"molecules\"] - df[\"molecules\"].mean()) \/ df[\"molecules\"].std()\ncategorical_numerical_correlations(df[\"animal\"], df[\"molecules\"], df.groupby(\"animal\")[\"molecules\"].apply(list))","b15a6df5":"# Balanced data\nnp.random.seed(10)\n\nY = np.arange(-5, 5, 0.01)\nX = np.random.choice(a=[\"turkey\", \"cow\", \"goat\", \"sheep\", \"chicken\"], p=[0.2, 0.2, 0.2, 0.2, 0.2], size=len(Y))\n\ndf = pd.DataFrame({\"animal\": X, \"molecules\": Y})\ndf[\"molecules\"] = (df[\"molecules\"] - df[\"molecules\"].mean()) \/ df[\"molecules\"].std()\n\ndf.loc[(df[\"animal\"] == \"chicken\"), \"molecules\"] = (np.random.rand(len(df[df[\"animal\"] == \"chicken\"]), 1)) * 3\ndf.loc[(df[\"animal\"] == \"goat\"), \"molecules\"] = (np.random.rand(len(df[df[\"animal\"] == \"goat\"]), 1)) - 2\n\ndf[\"molecules\"] = (df[\"molecules\"] - df[\"molecules\"].mean()) \/ df[\"molecules\"].std()\ncategorical_numerical_correlations(df[\"animal\"], df[\"molecules\"], df.groupby(\"animal\")[\"molecules\"].apply(list))","410c56a6":"# Imbalanced data\nnp.random.seed(10)\n\nY = np.arange(-5, 5, 0.01)\nX = np.random.choice(a=[\"turkey\", \"cow\", \"goat\", \"sheep\", \"chicken\"], p=[0.2, 0.5, 0.1, 0.15, 0.05], size=len(Y))\ndf = pd.DataFrame({\"animal\": X, \"molecules\": Y})\n\ndf.loc[(df[\"animal\"] == \"turkey\"), \"molecules\"] = ((np.random.rand(len(df[df[\"animal\"] == \"turkey\"]), 1)) ** 2)\ndf.loc[(df[\"animal\"] == \"cow\"), \"molecules\"] = ((np.random.rand(len(df[df[\"animal\"] == \"cow\"]), 1)) ** (1\/2))\ndf.loc[(df[\"animal\"] == \"goat\"), \"molecules\"] = ((np.random.rand(len(df[df[\"animal\"] == \"goat\"]), 1)) * 2)\ndf.loc[(df[\"animal\"] == \"sheep\"), \"molecules\"] = ((np.random.rand(len(df[df[\"animal\"] == \"sheep\"]), 1)) * 3)\ndf.loc[(df[\"animal\"] == \"chicken\"), \"molecules\"] = ((np.random.rand(len(df[df[\"animal\"] == \"chicken\"]), 1)) * 0.2)\n\ndf[\"molecules\"] = (df[\"molecules\"] - df[\"molecules\"].mean()) \/ df[\"molecules\"].std()\ncategorical_numerical_correlations(df[\"animal\"], df[\"molecules\"], df.groupby(\"animal\")[\"molecules\"].apply(list))","8112dd2e":"# Balanced data\nnp.random.seed(10)\n\nY = np.arange(-5, 5, 0.01)\nX = np.random.choice(a=[\"turkey\", \"cow\", \"goat\", \"sheep\", \"chicken\"], p=[0.2, 0.2, 0.2, 0.2, 0.2], size=len(Y))\ndf = pd.DataFrame({\"animal\": X, \"molecules\": Y})\n\ndf.loc[(df[\"animal\"] == \"turkey\"), \"molecules\"] = ((np.random.rand(len(df[df[\"animal\"] == \"turkey\"]), 1)) * 0) + 2\ndf.loc[(df[\"animal\"] == \"cow\"), \"molecules\"] = ((np.random.rand(len(df[df[\"animal\"] == \"cow\"]), 1)) * 0) + 4\ndf.loc[(df[\"animal\"] == \"goat\"), \"molecules\"] = ((np.random.rand(len(df[df[\"animal\"] == \"goat\"]), 1)) * 0) - 3\ndf.loc[(df[\"animal\"] == \"sheep\"), \"molecules\"] = ((np.random.rand(len(df[df[\"animal\"] == \"sheep\"]), 1)) * 0) + 0\ndf.loc[(df[\"animal\"] == \"chicken\"), \"molecules\"] = ((np.random.rand(len(df[df[\"animal\"] == \"chicken\"]), 1)) * 0) - 1.5\n\ndf[\"molecules\"] = (df[\"molecules\"] - df[\"molecules\"].mean()) \/ df[\"molecules\"].std()\ncategorical_numerical_correlations(df[\"animal\"], df[\"molecules\"], df.groupby(\"animal\")[\"molecules\"].apply(list))","a0b2e982":"ANOVA indicates that it is basically impossible that this dataset could be generated randomly. Mutual information coefficiens also show correlation, however Adjusted MI is always a bit more pessimistic.","06982868":"Results are pretty much the same as in the previous test. Looks like mic is the king of non-linear functions.","744fdbd4":"We can see good correlation from all coefficients, which is also expected since ranges are dependent. Runtime values stay in the same ranges except for distance and mic, where runtime has decreased significantly in respect to the previous test.","1b687f61":"Table of contents:\n\n[1. Important definitions](#section-1)\n\n[2. Correlation vs. Causation](#section-2)\n\n[3. P-value](#section-3)\n\n[4. Multicollinearity](#section-4)\n\n[5. Data categories](#section-5)\n\n[6. Numerical vs numerical correlation](#section-6)\n\n[7. Categorical vs categorical correlation](#section-7)\n\n[8. Numerical vs categorical correlation](#section-8)","7d502ddc":"<a id=\"section-3\"><\/a>\n# P-value #\n\nWhen we are talking about correlation coefficients, we also need to take a look at p-value. The p-value tells us how much we can\/can't trust the coefficients.\nIt is actually a probability that the result (coefficient) occurred by random chance - it ranges between 0 and 1 (0 and 100%).\n\nTherefore we are aiming for the lowest possible p-value where the acceptable value is <0.05 (<5% probability that the result occured by random chance).","076e0a2b":"All coefficients clearly show the randomness of data that is actually random.","30aa2c3d":"Adding imbalance to the other categorical features doesn't affect the coefficients, which still show the randomness of data.","aa0445ba":"Mutual information coefficients indicate almost perfect correlation, however I can't seem to achieve the perfect score with standardization of data. There is also an odd negative ANOVA coefficient?","e1507c6c":"All of the coefficient values are close to 0 indicating no correlation, which is ok since both ranges are random. Looking at the runtime, we can see vast differences. Pearson's is by far the fastest - 6x faster than spearman while distance and mic are in the completely different league.\n\nNote: if you want to experiment faster, make sure you turn off p-value calculations by setting the parameter calc_p_values to False.","a1efc79e":"<a id=\"section-7\"><\/a>\n# Categorical vs categorical correlation #\n\nCategorical features are extremelly common in most of the datasets. Despite that, finding correlation between two categorical features requires a bit of effort.\n\nIn my experiments I will use nominal categorical data. While ordinal data, can be calculated using Spearman's or Kendall's correlation coefficients which I already described at Numerical vs Numerical data correlation.\n\nBasically there are two ways of calculating correlations:\n- Statistical hypothesis testing => chi-square test\n- Probability theory => mutual information\n\nI will compare the following tests \/ coefficients:\n- Chi-square test for testing the null hypothesis of independence between two variables. Chi-square statistic has to be greater to its critical value to indicate correlation.\n- Cramer's V coefficient is a Chi-square test with the normalization layer on top, which puts it into the 0 and 1 range.\n- Mutual information measures the amount of information one can obtain from one random variable given another. Basicaly it measures the distance between two probability distribution.\n- Normalized mutual information is Mutual Information with the normalization layer on top which puts it into the 0 and 1 range.\n\n\n| Coefficient                    | Perfect correlation value | No correlation value |\n|--------------------------------|---------------------------|----------------------|\n| Chi-square                     |     > Critical value      |           0          |\n| Cramer's V                     |             1             |           0          |\n| Mutual information             |            > 1            |           0          |\n| Normalized mutual information  |             1             |           0          |","afb21329":"We now have a perfect one-way correlation. Knowing the animal, we also know the product, however it doesn't work the other way around.\nCramer's V detected the perfect correlation, which is also a bit too optimistic since there is no two-way correlation.\nI believe mutual information coefficients are closer to the actual truth.\n\nNow lets try to make a perfect two-way correlation.","c7cf8e1a":"## Creating perfect correlation ##","e5e0b7a3":"## Monotonic function ##\n\nMonotonic function is a special non linear function. Its slope always remains either positive or negative. ","74f7d803":"I repeated the test with negative function, which indicates the first differences in coefficient values. The first three detected negative correlation, on the other hand distance and mic stayed the same as in previous test.","a3439739":"Enforcing the correlation in bigger categories impacted the coefficients in a positive way, boosting the correlation amount. Cramer's V is indicating high correlation while mutual information coefficients show moderate correlation. I believe Cramer's V is a bit too optimistic.\n\nLet's try enforcing correlation on a \"chicken\" category, which is much smaller.","3547b714":"## Linear function ##","c23b917f":"## Creating perfect correlation ##","5c112082":"## Key takeaways ##\n\n- Point biserial correlation is the way to go when working with binary categorical feature\n- ANOVA coefficient is harder to read since you need to compare the result to the critial value\n- ANOVA coefficient is extremelly sensitive to even the smallest deviations in data\n- ANOVA coefficient is not working good with small imbalanced datasets\n- Adjusted Mutual Information is more pessimistic than Normalized Mutual Information, it is also a bit slower\n- Both Mutual Information scores are much better for reading the correlation between two features","c567e7a7":"<a id=\"section-5\"><\/a>\n# Data categories #\n\nData categorization:\n- Numerical data\n    * Discrete data - limited to the whole numbers only; number of employees, number of fixed assets etc.\n    * Continuous data - it can take any decimal value between two numbers; time, price etc.\n- Categorical data\n    * Nominal data - non parametric data which can be ordered; age group [toodler, chid, teenager, adult], climate [very hot, hot, warm, cold, very cold] etc.\n    * Ordinal data - non parametric data which can't be ordered; sex, race etc.\n\nEach category has different ways to calculate correlation. Some of the correlation coefficients can be shared accross categories while others are category specific.\n\nNumerical vs numerical (discrete and continuous data):\n- Pearson's coefficient\n- Spearman's rank coefficient\n- Kendall's rank coefficient\n- Distance coefficient\n- Maximal information coefficient (MIC)\n\nCategorical vs categorical:\n- Ordinal\n    * Spearman's rank coefficient\n    * Kendall's rank coefficient\n- Nominal\n    * Chi-square test\n    * Cramer's V\n    * Mutual information\n    * Normalized mutual information\n\nNumerical vs categorical:\n- ANOVA\n- Normalized mutual information\n- Adjusted mutual information\n- *Point biserial correlation\n\n*Categorical data has to be binary","20e08018":"<a id=\"section-6\"><\/a>\n# Numerical vs numerical correlation #\n\nI believe almost everybody knows Pearsons's coefficient, which is also the default value for pandas .corr() function. But there are also many other coefficients.\n\nCoefficients with the types of correlations they can detect:\n\n| Coefficient         | Linear | Monotonic | Non-linear | Negative | \n|---------------------|--------|-----------|------------|----------|\n| Pearson's           |    X   |           |            |     X    |\n| Spearman's          |    X   |     X     |            |     X    |\n| Kendall's           |    X   |     X     |            |     X    |\n| Distance            |    X   |     X     |      X     |          |\n| Maximal information |    X   |     X     |      X     |          |\n\n\n\nCorrelation values:\n\n| Coefficient         | Perfect correlation value | Perfect negative correlation value | No correlation value |\n|---------------------|---------------------------|------------------------------------|----------------------|\n| Pearson's           |             1             |                 -1                 |           0          |\n| Spearman's          |             1             |                 -1                 |           0          |\n| Kendall's           |             1             |                 -1                 |           0          |\n| Distance            |             1             |                  1                 |           0          |\n| Maximal information |             1             |                  1                 |           0          |\n\n\nNow lets do some testing with continuous datasets.\n\nEdit: I will hide my helper functions but fill free to check them out!","7fba8247":"## Random balanced categories correlation ##","df9d16bc":"We added some outliers to the mix and clearly we get the best performance from spearman and distance correlation.","d0b8cf4a":"## Random imbalanced categories correlation ##","212694c3":"## Select features which have good correlation with the target they said ##\n\nEvery machine learning course teaches us to select features which have good correlation with the target feature. As a beginner I thought this will be an easy job, just calculate correlation coefficient and select features. However when working with different datasets, I stumbled upon a problem where no feature had any correlation with the target (at least the numbers said so).\n\nTherefore I did some research and found out that this is a very deep topic. I made a notebook with some of my findings in case somebody will find it useful.","699f8be3":"## Negative linear function ##","1451b808":"As soon as there is some feature dependence, ANOVA clearly indicates it, however it only tells us that there is a difference in means between groups of data. On the other hand Mutual information coefficients clearly look at a bigger picture and only indicate relatively small correlation betwe","6f24766b":"## Non-linear function correlation ##","f79ee1b6":"## Key takeaways ##\n\n- Ordinal categorical data correlation can be calculated using Spearman's or Kendall's rank correlation\n- Nominal categorical data correlation can be calculated using Chi-squared \/ Cramer's V or (Normalized) mutual information coefficient.\n- Chi-squared coefficient is harder to read since you need to compare the result to the critial value\n- Cramer's V coefficient seems a bit too optimistic, especially with imbalanced categorical variables\n- Cramer's V is considering one-way correlation only\n- Normalized mutual information coefficient considers two-way correlation, which seem closer to the actual truth","e7396cac":"Adding more feature dependence increases the ANOVA as well as mutual information coefficients.","38cee1ab":"Sources:\n\nhttps:\/\/en.wikipedia.org\/wiki\/Correlation_and_dependence\n\nhttps:\/\/en.wikipedia.org\/wiki\/Correlation_coefficient\n\nhttps:\/\/www.investopedia.com\/terms\/p\/p-value.asp\n\nhttps:\/\/www.dictionary.com\/browse\/causation\n\nhttps:\/\/medium.com\/@outside2SDs\/an-overview-of-correlation-measures-between-categorical-and-continuous-variables-4c7f85610365\n\nhttps:\/\/nitin9809.medium.com\/data-types-for-ml-beginners-b94fc9d88ed\n\nhttps:\/\/letstalkaboutscience.wordpress.com\/2013\/07\/30\/scientific-inquiry-and-critical-thinking\/\n\nhttps:\/\/gist.github.com\/wladston\/c931b1495184fbb99bec\n\nhttps:\/\/www.investopedia.com\/terms\/m\/multicollinearity.asp\n\nhttps:\/\/en.wikipedia.org\/wiki\/One-way_analysis_of_variance\n\nhttps:\/\/www.statology.org\/point-biserial-correlation-python\/\n\nhttp:\/\/mlwiki.org\/index.php\/One-Way_ANOVA_F-Test\n\nhttps:\/\/passel2.unl.edu\/view\/lesson\/9beaa382bf7e\/8\n\nhttps:\/\/en.wikipedia.org\/wiki\/Cram%C3%A9r%27s_V\n\nhttps:\/\/www.spss-tutorials.com\/cramers-v-what-and-why\/\n\nhttps:\/\/support.minitab.com\/en-us\/minitab-express\/1\/help-and-how-to\/modeling-statistics\/anova\/how-to\/one-way-anova\/interpret-the-results\/key-results\/\n\nhttps:\/\/www.dummies.com\/education\/math\/business-statistics\/how-to-find-the-critical-values-for-an-anova-hypothesis-using-the-f-table\/\n\nhttp:\/\/www.socr.ucla.edu\/Applets.dir\/F_Table.html","96404305":"Non-linear function can clearly be detected only by distance and mic coefficient. Especially mic performs really good. Other coefficients show no correlation. Let's try some other non-linear functions.","3318b404":"<a id=\"section-8\"><\/a>\n# Numerical vs categorical correlation #\n\nThe last remaining combination of features that should be covered is numerical vs categorical correlation.\n\nCorrelation coeficients:\n- Point biserial correlation coefficient compares binary and continuos features in a similar way to the pearson's coefficient between numerical features\n- One-way ANOVA is a technique which is comparing the means of different groups. Numerical data has to be distributed normally\n- Normalized mutual information is Mutual Information with the normalization layer on top which puts it into the 0 and 1 range\n- Adjusted mutual information is a variation of Mutual Information for comparing clusters of data\n\n| Coefficient                    | Perfect correlation value | Perfect negative correlation value | No correlation value |\n|--------------------------------|---------------------------|------------------------------------|----------------------|\n| Point biserial correlation     |             1             |                 -1                 |           0          |\n| ANOVA                          |     > Critical value      |          > Critical value          |   < Critical value   |\n| Normalized mutual information  |             1             |                  1                 |           0          |\n| Adjusted mutual information    |             1             |                  1                 |           0          |\n\nIn my experiments, I will ignore Point-biserial correlation, since I won't use binary categorical feature.","0f6fce1b":"## Linear function with outliers ##","2989a200":"ANOVA coefficient got slightly above the critical value. However this depends on the random seed of the values. Feel free to change it and expore the differences.\nOn the other hand, Mutual information coefficients still show the randomness of data.","8f325a0c":"## Random numbers correlation ##","bf934214":"Although we have pretty good correlation from all coefficients, we can clearly see that spearman's stands out. This is also the first test, where pearson's is not performing very good.","158f348f":"All coefficients clearly show the absence of correlation between features. Calculation time is a bit faster with mutual information coefficients, but there is no significant difference.","8caf4702":"Chart shows a clear correlation between \"turkey\" and \"egg\" categories. Chi2 only indicated that there is a correlation (null hypothesiss is rejected) but there is no strenght of correlation. Strenght can be found looking at the Cramer's V, which shows a moderate correlation between two features. On the other hand, mutual information cofficiens detected low correlation between features.\n\nLets try the same on the imbalanced dataset.","014de1c4":"## Key takeaways ##\n\n- Before calculating the correlation coefficient we need to find out the correlation type - linear, monotonic or non-linear (using scatter plot)\n- Pearson's coefficient should only be used for linear correlations, preferably without outliers\n- Spearman's coefficient should be used for monotonic correlations and linear correlations with outliers\n- Maximal information coefficient (MIC) shines with non-linear correlations, however it is computationaly very expensive\n\nNote: monotonic and non-linear correlations can sometimes be converted to linear but this is the topic for another notebook","f9ad9636":"## Random imbalanced categories correlation ##","5316ea78":"Like expected, all of the coefficients now show the perfect correlation.","818affe1":"<a id=\"section-2\"><\/a>\n# Correlation vs. Causation #\n\n![](https:\/\/letstalkaboutscience.files.wordpress.com\/2013\/07\/sharks-icon.png)\n\nThe first thing we need to know is the difference between correlation and causation. Lets see the difference with the following example:\n\n1. Ice cream consumption is correlated to high temperatures\n\n2. High temperatures are correlated to high swimming activity\n\n3. High swimming activity is correlated to number of shark attacks\n\n\nWe need to ask ourselves if there is a cause to effect relationship. All of the above seem to pass the cause\/effect check, therefore we have causation.\n\nWhat about this one:\n4. Ice cream consumption is correlated to number of shark attacks\n\nIt seems a bit odd but there actually is a correlation. However our common sense tells us there is no cause and effect, therefore we do not have causation.\n\n\nThe conclusion is that correlation does not automatically result into causation. What we are actually looking for in machine learning is causation!","5d120696":"## Adding feature dependence ##","f51dc9c3":"<a id=\"section-1\"><\/a>\n# Imporant definitions #\n\n\n> - independent features - columns of data in the dataset which are used to make a prediction\n> - target feature - a column of data in the dataset that is being predicted\n> - correlation - a statistical measure that expresses the extent to which two variables are related\n> - correlation coefficient - a numerical measure of some type of correlation\n> - causation - the relation of cause to effect\n> - p-value - a measure of the probability that an observed difference could have occurred just by random chance\n> - multicollinearity - happens when one predictor variable in a multiple regression model can be linearly predicted from the others with a high degree of accuracy","24a933cf":"All coefficient values decreased. Cramer's V dropped to the level of balanced data score.","6a99854c":"<a id=\"section-4\"><\/a>\n# Multicollinearity #\n\nMulticollinearity occures when we have multiple features which tells us the same info. For example two boolean features - is_male and is_female:\n\n| id | is_male | is_female |\n|----|---------|-----------|\n| 1  | TRUE    | FALSE     |\n| 2  | FALSE   | TRUE      |\n| 3  | FALSE   | TRUE      |\n\n\nIt seems a bit stupid to have two features for gender but that can occure quite often when creating dummy columns (one-hot encoding).\nBoth of the features are clearly correlated and they will result in less reliable model. This is really important for regression models, on the other hand tree based models are immune to multicollinearity.","b21fb771":"## Random balanced categories correlation ##","981677b9":"## Adding feature dependence ##"}}