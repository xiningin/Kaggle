{"cell_type":{"6a0dd978":"code","8806f320":"code","53f8d44a":"code","c5291ada":"code","ffb7f0df":"code","fc1bc56d":"code","e4d1e0ba":"code","0cbbcacc":"code","d8de492c":"code","9b921c6e":"code","e361cf41":"code","490637a2":"code","ad3eef85":"code","36ca9f61":"code","5802176e":"code","1d21de43":"code","e3dadff9":"code","52457991":"code","a654406a":"code","d6c3cf7b":"code","6c4b67e1":"code","b47748c8":"code","e8e3e9cc":"code","b43b675f":"code","c4f24731":"code","d0701910":"code","dae0c631":"code","776ab13a":"code","7a46bcaf":"code","ae3d7e07":"code","5e1d90a9":"code","28d2e073":"markdown","79b3914f":"markdown","46bec560":"markdown","bed3306e":"markdown","108cb688":"markdown","6afb7919":"markdown","d90303ca":"markdown","21d0cec0":"markdown","7fc12e26":"markdown","157190e9":"markdown","ff2a0531":"markdown","cba360af":"markdown","8d2b4fd9":"markdown","86d6928f":"markdown","883a4c76":"markdown","a4d36088":"markdown","40aa5224":"markdown","1bdcf142":"markdown","71c8efd8":"markdown","ac9245be":"markdown","98ed8383":"markdown","5a3e09f2":"markdown","99cbf784":"markdown","b4988bc9":"markdown","8eb40d97":"markdown","1ad159f8":"markdown","ef3a2e77":"markdown","ef369565":"markdown","4d957669":"markdown","c0986c93":"markdown","c54b8522":"markdown","7cec3be2":"markdown","70664d01":"markdown","f13fd7df":"markdown","d0e3a7c3":"markdown","4a1eb9d6":"markdown","8dd40a3d":"markdown","422f7685":"markdown","9c8b8bcc":"markdown","f2a52d2a":"markdown","08769e5b":"markdown","7777adf1":"markdown","04f1170d":"markdown","3dda56e0":"markdown","39432723":"markdown","65470ed8":"markdown","d5bfd4c4":"markdown","b23f9f8c":"markdown","fdd894c9":"markdown","fadf9aeb":"markdown","bdd7fe5c":"markdown","761a6279":"markdown","1f1385cf":"markdown","ba4b85f8":"markdown","8dba54f0":"markdown","733d1a0d":"markdown","d406fc26":"markdown","1054578c":"markdown","8bc76159":"markdown","6d02bc70":"markdown"},"source":{"6a0dd978":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8806f320":"red_wine = pd.read_csv('red-wine.csv')\nred_wine.head()\nred_wine.shape\n\n# Q1: Input_shape for model\ninput_shape = [len(red_wine.columns[:-1])] # must be number with list format\n\n# Q2: Define linear model\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nmodel = keras.Sequential([\n    layers.Dense(units=1, input_shape=input_shape)\n])\n\n# Q3: Check the weight and bias\nw, b = model.weights","53f8d44a":"# Plot the output of an untrained linear model\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nmodel = keras.Sequential([\n    layers.Dense(1, input_shape=[1]),\n])\n\nx = tf.linspace(-1.0, 1.0, 100)\ny = model.predict(x)\n\nplt.figure(dpi=100)\nplt.plot(x, y, 'k')\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.xlabel(\"Input: x\")\nplt.ylabel(\"Target y\")\nw, b = model.weights # you could also use model.get_weights() here\nplt.title(\"Weight: {:0.2f}\\nBias: {:0.2f}\".format(w[0][0], b[0]))\nplt.show()","c5291ada":"concrete = pd.read_csv('concrete.csv')\nconcrete.head()\n\n# Q1: Input_shape for model\ninput_shape = [len(concrete.columns[:-1])]\n\n# Define model with Hidden Layers \n#    with three hidden layers, each having 512 units and the ReLU activation\n#    an output layer of one unit and no activation\nmodel = keras.Sequential([\n    layers.Dense(units=512, activation='relu', input_shape=input_shape),\n    layers.Dense(units=512, activation='relu'),\n    layers.Dense(units=512, activation='relu'),\n    layers.Dense(units=1)\n])\n\n# Q3: Rewrite Activation Layers\nmodel = keras.Sequential([\n    layers.Dense(512, input_shape=[8]),\n    layers.Activation('relu'),\n    layers.Dense(512),\n    layers.Activation('relu'),\n    layers.Dense(512),\n    layers.Activation('relu'),\n    layers.Dense(1),\n])","ffb7f0df":"# Alternatives to ReLU\nrelu_activation_layer = layers.Activation('relu')\nelu_activation_layer = layers.Activation(\"elu\")\nselu_activation_layer = layers.Activation('selu')\nswish_activation_layer = layers.Activation('swish')\n\nx = tf.linspace(-3.0, 3.0, 100)\n\nplt.figure(dpi=100)\nplt.plot(x, relu_activation_layer(x),'b-', label=\"relu\")\nplt.plot(x, elu_activation_layer(x), 'g--', label='elu')\nplt.plot(x, selu_activation_layer(x), 'r-.', label='selu')\nplt.plot(x, swish_activation_layer(x), 'k:', label='swish')\nplt.xlim(-3, 3)\nplt.xlabel(\"Input\")\nplt.ylabel(\"Output\")\nplt.legend()\nplt.show();","fc1bc56d":"import pandas as pd\nfrom IPython.display import display\n\n# add `dl-course-data`\nred_wine = pd.read_csv('..\/input\/dl-course-data\/red-wine.csv')\nred_wine.head","e4d1e0ba":"# Create training and validation splits\ndf_train = red_wine.sample(frac=0.7, random_state=0)\ndf_valid = red_wine.drop(df_train.index)\ndisplay(df_train.head(4))\n\n# Scale to [0, 1]\nmax_ = df_train.max(axis=0) # for each column\nmin_ = df_train.min(axis=0) # for each column\ndf_train = (df_train - min_) \/ (max_ - min_)\ndf_valid = (df_valid - min_) \/ (max_ - min_)\n\n# Split features and target\nX_train = df_train.drop('quality', axis=1)\ny_train = df_train['quality']\n\nX_valid = df_valid.drop('quality', axis=1)\ny_valid = df_valid['quality']","0cbbcacc":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\n# number of columns should be the input_shape for the model\n# chosen a three-layer network with over 1500 neurons\nmodel = keras.Sequential([\n    layers.Dense(512, activation='relu', input_shape=[11]),\n    layers.Dense(512, activation='relu'),\n    layers.Dense(512, activation='relu'),\n    layers.Dense(1),\n])\n\n# After defining the model, compile in the optimizer and loss function.\nmodel.compile(optimizer='adam',loss='mae')","d8de492c":"# start training the model\n#    batch_size: to feed the optimizer N rows of the training data at a time  \n#    epochs: to do N times all the way through the dataset\n\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=256,\n    epochs=10,\n)","9b921c6e":"# a better way to view the loss though is to plot it.\n# `fit` method in fact keeps a record of the loss produced during training\n\nimport pandas as pd\nhistory_df = pd.DataFrame(history.history)\nhistory_df['loss'].plot();","e361cf41":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import make_column_transformer, make_column_selector\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nfuel = pd.read_csv('..\/input\/dl-course-data\/fuel.csv')\n\nX = fuel.copy()\n# Remove target\ny = X.pop('FE')\n\npreprocessor = make_column_transformer(\n    (StandardScaler(),\n     make_column_selector(dtype_include=np.number)),\n    (OneHotEncoder(sparse=False),\n     make_column_selector(dtype_include=object)),\n)\n\nX = preprocessor.fit_transform(X)\ny = np.log(y) # log transform target instead of standardizing\n\ninput_shape = [X.shape[1]]\nprint(\"Input shape: {}\".format(input_shape))\n\n\n# defining model, compiling and fitting\nmodel = keras.Sequential([\n    layers.Dense(128, activation='relu', input_shape=input_shape),\n    layers.Dense(128, activation='relu'),    \n    layers.Dense(64, activation='relu'),\n    layers.Dense(1),\n])\nmodel.compile(optimizer='adam', loss='mae')\nhistory = model.fit(X, y, batch_size=128, epochs=200)\n\n# ploting loss values\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[5:, ['loss']].plot();","490637a2":"learning_rate = 0.05\nbatch_size = 32\nnum_examples = 256\n\nanimate_sgd(\n    learning_rate=learning_rate,\n    batch_size=batch_size,\n    num_examples=num_examples,\n    # You can also change these, if you like\n    steps=50, # total training steps (batches seen)\n    true_w=3.0, # the slope of the data\n    true_b=2.0, # the bias of the data\n)","ad3eef85":"import pandas as pd\nfrom IPython.display import display\n\nred_wine = pd.read_csv('..\/input\/dl-course-data\/red-wine.csv')\n\n# Create training and validation splits\ndf_train = red_wine.sample(frac=0.7, random_state=0)\ndf_valid = red_wine.drop(df_train.index)\ndisplay(df_train.head(4))\n\n# Scale to [0, 1]\nmax_ = df_train.max(axis=0)\nmin_ = df_train.min(axis=0)\ndf_train = (df_train - min_) \/ (max_ - min_)\ndf_valid = (df_valid - min_) \/ (max_ - min_)\n\n# Split features and target\nX_train = df_train.drop('quality', axis=1)\nX_valid = df_valid.drop('quality', axis=1)\ny_train = df_train['quality']\ny_valid = df_valid['quality']","36ca9f61":"from tensorflow import keras\nfrom tensorflow.keras import layers, callbacks\n\nearly_stopping = callbacks.EarlyStopping(\n    min_delta=0.001, # minimium amount of change to count as an improvement\n    patience=20, # how many epochs to wait before stopping\n    restore_best_weights=True,\n)\n\nmodel = keras.Sequential([\n    layers.Dense(512, activation='relu', input_shape=[11]),\n    layers.Dense(512, activation='relu'),\n    layers.Dense(512, activation='relu'),\n    layers.Dense(1),\n])\n\nmodel.compile(optimizer='adam',loss='mae')","5802176e":"history = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=256,\n    epochs=500,\n    callbacks=[early_stopping], # put callbacks in a list\n    verbose=0,  # turn off training log\n)\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot();\nprint(\"Keras stopped the training well before the full 500 epochs!\")\nprint(\"Minimum validation loss: {}\".format(history_df['val_loss'].min()))","1d21de43":"import pandas as pd\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.model_selection import GroupShuffleSplit\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks\n\nspotify = pd.read_csv('..\/input\/dl-course-data\/spotify.csv')\n\nX = spotify.copy().dropna()\ny = X.pop('track_popularity')\nartists = X['track_artist']\n\nfeatures_num = ['danceability', 'energy', 'key', 'loudness', 'mode',\n                'speechiness', 'acousticness', 'instrumentalness',\n                'liveness', 'valence', 'tempo', 'duration_ms']\nfeatures_cat = ['playlist_genre']\n\npreprocessor = make_column_transformer(\n    (StandardScaler(), features_num),\n    (OneHotEncoder(), features_cat),\n)\n\n# We'll do a \"grouped\" split to keep all of an artist's songs in one\n# split or the other. This is to help prevent signal leakage.\ndef group_split(X, y, group, train_size=0.75):\n    splitter = GroupShuffleSplit(train_size=train_size)\n    train, test = next(splitter.split(X, y, groups=group))\n    return (X.iloc[train], X.iloc[test], y.iloc[train], y.iloc[test])\n\nX_train, X_valid, y_train, y_valid = group_split(X, y, artists)\n\nX_train = preprocessor.fit_transform(X_train)\nX_valid = preprocessor.transform(X_valid)\ny_train = y_train \/ 100 # popularity is on a scale 0-100, so this rescales to 0-1.\ny_valid = y_valid \/ 100\n\ninput_shape = [X_train.shape[1]]\nprint(\"Input shape: {}\".format(input_shape))","e3dadff9":"# Underfitting\nmodel = keras.Sequential([\n    layers.Dense(1, input_shape=input_shape),\n])\nmodel.compile(\n    optimizer='adam',\n    loss='mae',\n)\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=512,\n    epochs=50,\n    verbose=0, # suppress output since we'll plot the curves\n)\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[0:, ['loss', 'val_loss']].plot()\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()));","52457991":"# Overfitting\nmodel = keras.Sequential([\n    layers.Dense(128, activation='relu', input_shape=input_shape),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(1)\n])\nmodel.compile(\n    optimizer='adam',\n    loss='mae',\n)\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=512,\n    epochs=50,\n    verbose=0\n)\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()));","a654406a":"# Early Stopping Callbacks\nearly_stopping = callbacks.EarlyStopping(\n    patience=5,\n    min_delta=0.001,\n    restore_best_weights=True\n)\n\nmodel = keras.Sequential([\n    layers.Dense(128, activation='relu', input_shape=input_shape),\n    layers.Dense(64, activation='relu'),    \n    layers.Dense(1)\n])\nmodel.compile(\n    optimizer='adam',\n    loss='mae',\n)\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=512,\n    epochs=50,\n    callbacks=[early_stopping],\n    verbose=0\n)\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()));","d6c3cf7b":"import matplotlib.pyplot as plt\n\nplt.style.use('seaborn-whitegrid')\n# Set Matplotlib defaults\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\n\n\nimport pandas as pd\nred_wine = pd.read_csv('..\/input\/dl-course-data\/red-wine.csv')\n\n# Create training and validation splits\ndf_train = red_wine.sample(frac=0.7, random_state=0)\ndf_valid = red_wine.drop(df_train.index)\n\n# Split features and target\nX_train = df_train.drop('quality', axis=1)\nX_valid = df_valid.drop('quality', axis=1)\ny_train = df_train['quality']\ny_valid = df_valid['quality']","6c4b67e1":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel = keras.Sequential([\n    layers.Dense(1024, activation='relu', input_shape=[11]),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    layers.Dense(1024, activation='relu'),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    layers.Dense(1024, activation='relu'),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    layers.Dense(1),\n])\n\nmodel.compile(optimizer='adam',loss='mae')","b47748c8":"history = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=256,\n    epochs=100,\n    verbose=0,\n)\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot();","e8e3e9cc":"from tensorflow.keras import layers\nfrom tensorflow.keras import callbacks\n\nspotify = pd.read_csv('spotify.csv')\n\nX = spotify.copy().dropna()\ny = X.pop('track_popularity')\nartists = X['track_artist']\n\nfeatures_num = ['danceability', 'energy', 'key', 'loudness', 'mode',\n                'speechiness', 'acousticness', 'instrumentalness',\n                'liveness', 'valence', 'tempo', 'duration_ms']\nfeatures_cat = ['playlist_genre']\n\npreprocessor = make_column_transformer(\n    (StandardScaler(), features_num),\n    (OneHotEncoder(), features_cat),\n)\n\ndef group_split(X, y, group, train_size=0.75):\n    splitter = GroupShuffleSplit(train_size=train_size)\n    train, test = next(splitter.split(X, y, groups=group))\n    return (X.iloc[train], X.iloc[test], y.iloc[train], y.iloc[test])\n\nX_train, X_valid, y_train, y_valid = group_split(X, y, artists)\n\nX_train = preprocessor.fit_transform(X_train)\nX_valid = preprocessor.transform(X_valid)\ny_train = y_train \/ 100\ny_valid = y_valid \/ 100\n\ninput_shape = [X_train.shape[1]]\nprint(\"Input shape: {}\".format(input_shape))","b43b675f":"model = keras.Sequential([\n    layers.Dense(128, activation='relu', input_shape=input_shape),\n    layers.Dropout(0.3),\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(0.3),\n    layers.Dense(1)\n])\n\nmodel.compile(\n    optimizer='adam',\n    loss='mae',\n)\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=512,\n    epochs=50,\n    verbose=0,\n)\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()))","c4f24731":"import pandas as pd\n\nconcrete = pd.read_csv('concrete.csv')\ndf = concrete.copy()\n\ndf_train = df.sample(frac=0.7, random_state=0)\ndf_valid = df.drop(df_train.index)\n\nX_train = df_train.drop('CompressiveStrength', axis=1)\nX_valid = df_valid.drop('CompressiveStrength', axis=1)\ny_train = df_train['CompressiveStrength']\ny_valid = df_valid['CompressiveStrength']\n\ninput_shape = [X_train.shape[1]]","d0701910":"# model without BatchNormalization cannot be trained\nmodel = keras.Sequential([\n    layers.BatchNormalization(),            # <---------\n    layers.Dense(512, activation='relu'),\n    layers.BatchNormalization(),            # <---------\n    layers.Dense(512, activation='relu'),\n    layers.BatchNormalization(),            # <---------\n    layers.Dense(512, activation='relu'),\n    layers.BatchNormalization(),            # <---------\n    layers.Dense(1),\n])\n\nmodel.compile(\n    optimizer='sgd',\n    loss='mae',\n    metrics=['mae'],\n)\nEPOCHS = 100\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=64,\n    epochs=EPOCHS,\n    verbose=0,\n)\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[0:, ['loss', 'val_loss']].plot()\nprint((\"Minimum Validation Loss: {:0.4f}\").format(history_df['val_loss'].min()))","dae0c631":"import pandas as pd\nfrom IPython.display import display\n\nion = pd.read_csv('..\/input\/dl-course-data\/ion.csv', index_col=0)\ndisplay(ion.head())\n\ndf = ion.copy()\ndf['Class'] = df['Class'].map({'good': 0, 'bad': 1})\n\ndf_train = df.sample(frac=0.7, random_state=0)\ndf_valid = df.drop(df_train.index)\n\nmax_ = df_train.max(axis=0)\nmin_ = df_train.min(axis=0)\n\ndf_train = (df_train - min_) \/ (max_ - min_)\ndf_valid = (df_valid - min_) \/ (max_ - min_)\ndf_train.dropna(axis=1, inplace=True) # drop the empty feature in column 2\ndf_valid.dropna(axis=1, inplace=True)\n\nX_train = df_train.drop('Class', axis=1)\nX_valid = df_valid.drop('Class', axis=1)\ny_train = df_train['Class']\ny_valid = df_valid['Class']","776ab13a":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\n# define the model with final layer include a 'sigmoid' activation\nmodel = keras.Sequential([\n    layers.Dense(4, activation='relu', input_shape=[33]),\n    layers.Dense(4, activation='relu'),    \n    layers.Dense(1, activation='sigmoid'), # <-------------\n])\n\n# Add the cross-entropy loss and accuracy metric to the model with its `compile` method\nmodel.compile(\n    optimizer='adam',\n    loss='binary_crossentropy', # For two-class problems \n    metrics=['binary_accuracy'],\n)\n\nearly_stopping = keras.callbacks.EarlyStopping(\n    patience=10,\n    min_delta=0.001,\n    restore_best_weights=True,\n)\n\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=512,\n    epochs=1000,\n    callbacks=[early_stopping],\n    verbose=0, # hide the output because we have so many epochs\n)","7a46bcaf":"history_df = pd.DataFrame(history.history)\n# Start the plot at epoch 5\nhistory_df.loc[5:, ['loss', 'val_loss']].plot()\nhistory_df.loc[5:, ['binary_accuracy', 'val_binary_accuracy']].plot()\n\nprint((\"Best Validation Loss: {:0.4f}\" +\\\n      \"\\nBest Validation Accuracy: {:0.4f}\")\\\n      .format(history_df['val_loss'].min(), \n              history_df['val_binary_accuracy'].max()))","ae3d7e07":"import pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\n\nhotel = pd.read_csv('..\/input\/dl-course-data\/hotel.csv')\n\nX = hotel.copy()\ny = X.pop('is_canceled')\n\nX['arrival_date_month'] = \\\n    X['arrival_date_month'].map(\n        {'January':1, 'February': 2, 'March':3,\n         'April':4, 'May':5, 'June':6, 'July':7,\n         'August':8, 'September':9, 'October':10,\n         'November':11, 'December':12}\n    )\n\nfeatures_num = [\n    \"lead_time\", \"arrival_date_week_number\",\n    \"arrival_date_day_of_month\", \"stays_in_weekend_nights\",\n    \"stays_in_week_nights\", \"adults\", \"children\", \"babies\",\n    \"is_repeated_guest\", \"previous_cancellations\",\n    \"previous_bookings_not_canceled\", \"required_car_parking_spaces\",\n    \"total_of_special_requests\", \"adr\",\n]\nfeatures_cat = [\n    \"hotel\", \"arrival_date_month\", \"meal\",\n    \"market_segment\", \"distribution_channel\",\n    \"reserved_room_type\", \"deposit_type\", \"customer_type\",\n]\n\ntransformer_num = make_pipeline(\n    SimpleImputer(strategy=\"constant\"), # there are a few missing values\n    StandardScaler(),\n)\ntransformer_cat = make_pipeline(\n    SimpleImputer(strategy=\"constant\", fill_value=\"NA\"),\n    OneHotEncoder(handle_unknown='ignore'),\n)\n\npreprocessor = make_column_transformer(\n    (transformer_num, features_num),\n    (transformer_cat, features_cat),\n)\n\n# stratify - make sure classes are evenlly represented across splits\nX_train, X_valid, y_train, y_valid = \\\n    train_test_split(X, y, stratify=y, train_size=0.75)\n\nX_train = preprocessor.fit_transform(X_train)\nX_valid = preprocessor.transform(X_valid)\n\ninput_shape = [X_train.shape[1]]","5e1d90a9":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel = keras.Sequential([\n    layers.BatchNormalization(input_shape=input_shape),\n    layers.Dense(256, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.3),\n    layers.Dense(256, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.3),\n    layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_accuracy'])\n\nearly_stopping = keras.callbacks.EarlyStopping(\n    patience=5,\n    min_delta=0.001,\n    restore_best_weights=True,\n)\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=512,\n    epochs=200,\n    callbacks=[early_stopping],\n    verbose=0\n)\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"Cross-entropy\")\nhistory_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot(title=\"Accuracy\");","28d2e073":"### Stacking Dense Layers\nHow layers are stacked to get complex `data transformation`. A stack of dense layers makes a `fully-connected` network.\n![](https:\/\/i.imgur.com\/Y5iwFQZ.png)\nThe layers before the output layer are sometimes called `hidden`.  \n\nFinal (output) layer is a linear unit (no activation function) making this network appropriate to regression task, where we are trying to predict some arbitrary numeric value.\n- `Classification` might requires activation function.","79b3914f":"#### Learning Rate and Batch Size\nIn the above animation, line only makes a small shift in the direction of each batch. The size of these shifts is determined by the `learning rate`. \n- A `smaller learning rate` means the network needs to see more minibatches before its weights converge to their best values.\n\nThe `learning rate` and the `size of minibatches` are the two parameters that have the largest effect on how the SGD training proceeds. For most work it won't be necessary to do an extensive hyperparameter search to get satisfactory results. `Adam` is an `SGD algorithm` that has an adaptive learning rate that makes it suitable for most problems without any parameter tuning, `self tuning`. `Adam is a great general-purpose optimizer`.","46bec560":"### Exercise: Overfitting and Underfitting\nHow to improve training outcomes by including an early stopping callback to prevent overfitting.\nUse `spotify` dataset to predict the popularity of a song based on various audio features, like `'tempo', 'danceability', and 'mode'`.","bed3306e":"### Exercise: Deep Neural Networks\n[Concrete strength](https:\/\/www.kaggle.com\/c\/dat300-2018-concrete) datasets to predict the compressive strength of concrete manufactured according to various recipes.","108cb688":"Whole family of variants of the `relu` activation -- `elu`, `selu`, and `swish`. Sometimes one activation will perform better than another on a given task. The `ReLU` activation tends to do well on most problems. [documentation](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/activations)","6afb7919":"### Loss Function\n`Loss function` tells a network what problem to solve and measures the disparity between the the target's true value and the value the model predicts. \n\n`Regression` problems, where the task is to predict some numerical value --> calories in 80 Cereals, rating in `Red Wine Quality`.\nA common `loss function` for regression problems is the `mean absolute error (MAE)`.  \n- For each prediction `y_pred`, `MAE` measures the disparity from the true target `y_true` by an absolute difference \n    - `abs(y_true - y_pred)`\n    - mean absolute error is the average length between the fitted curve and the data points.\n![](https:\/\/i.imgur.com\/VDcvkZN.png)\n\nOther `loss functions` for regression problems are `mean-squared error (MSE)` or `Huber loss`.\n\nDuring training, the model will use the loss function as a guide for finding the correct values of its weights (`lower loss is better`). ","d90303ca":"### Linear Units in Keras\nThe easiest way to create a model in `Keras` is through `keras.Sequential`, which creates a neural network as a stack of layers. Can define a linear model accepting three features (`'sugars', 'fiber', and 'protein'`) to produce a single output `calories`.\n\n```\nmodel = keras.Sequential([\n    layers.Dense(units=1, input_shape=[3])\n])\n```\n- `units`: how many outputs wanted\n- `input_shape`: ensures the model will accept three features as input (`'sugars', 'fiber', and 'protein'`)","21d0cec0":"[Detecting the Higgs Boson With TPUs](https:\/\/www.kaggle.com\/minyannaing\/detecting-the-higgs-boson-with-tpus\/edit)","7fc12e26":"### Example - Train a Model with Early Stopping\nWill increase the capacity of that network but also add an early-stopping callback to prevent overfitting.","157190e9":"# Dropout and Batch Normalization","ff2a0531":"### Accuracy and Cross-Entropy\n`Accuracy` is one of the many metrics in use for measuring success on a classification problem. Accuracy is the ratio of correct predictions to total predictions: `accuracy = number_correct \/ total`. \n- if a model always predict correctly, the accuracy score is `1.0`\n\nAccuracy cannot be used as a loss function, `SGD` needs a loss function that changes `smoothly`, but `accuracy`, being a ratio of counts, changes in `jumps`. So, need to chhose a substitute (`cross-entropy` function) to act as loss funciton.\n- loss function defines the objective of the network during training.\n- with regression, goal was to minimizee the distance between expected outcome and predeicted outcome (`MAE` was chosen to measure the distance)\n- for classification, want a distance between `probabilities` that `cross-entropy` provides.\n    - `Cross-entropy` is a sort of measure for the distance from one probability distribution to another.\n![](https:\/\/i.imgur.com\/DwVV9bR.png)\nIdea is that the network to predict the correct class with probability `1.0`. \n- The further away the predicted probability is from `1.0`, the greater will be the `cross-entropy loss`.","cba360af":"Load the `Concrete` dataset to explore how batch normalization can fix problems in training. The data will not be pre-normalized as previous.","8d2b4fd9":"---","86d6928f":"Each example in the training data consists of some `features, inputs` together with an expected `target, output`. For `80 Cereals` datasets, features as `'sugar', 'fiber', and 'protein'` and target as `'calories'`.\n\nIn addition to the training data,\n- A `loss function` that measures how good the network's predictions are.\n- An `optimizer` that can tell the network how to change its weights.","883a4c76":"### Making Probabilities with the Sigmoid Function\nThe `cross-entropy` and `accuracy` functions both require probabilities as inputs, numbers from `0` to `1`. To covert the real-valued outputs produced by a dense layer into probabilities, can use a new kind of activation function, `sigmoid activation`.\n![](https:\/\/i.imgur.com\/FYbRvJo.png)\nTo get the final class prediction, need to define a `threshold` probability. Mostly use `0.5` so that rounding will give the correct class: A `0.5` threshold is what Keras uses by default with its [accuracy metric](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/metrics\/BinaryAccuracy).\n- below 0.5 means the class with label 0 and 0.5 or \n- above means the class with label 1.","a4d36088":"---","40aa5224":"`Why is input_shape a Python list?`\n- The data we'll use in this course will be tabular data, like in a Pandas dataframe. We'll have one input for each feature in the dataset. The features are arranged by column, so we'll always have `input_shape=[num_columns]`. \n- The reason Keras uses a list here is to permit use of more complex datasets. Image data, for instance, might need three dimensions: `[height, width, channels]`.","1bdcf142":"### Exercise: Binary Classification\nTo predict hotel cancellations with a binary classifier using `Hotel Cancellations` dataset.","71c8efd8":"---","ac9245be":"### Exercise: Dropout and Batch Normalization\nHow batch normalization can successfully train models on difficult datasets.\nUse `spotify` dataset to predict the popularity of a song based on various audio features, like `'tempo', 'danceability', and 'mode'`.","98ed8383":"# Detecting the Higgs Boson With TPUs","5a3e09f2":"#### Building Sequential Models\n`Sequential` model will connect together a list of layers in order from first to last:\n- first layer gets input\n- last layer produces output\n```\nmodel = keras.Sequential([\n    # the hidden ReLU layers\n    layers.Dense(units=4, activation='relu', input_shape=[2]),\n    layers.Dense(units=3, activation='relu'),\n    # the linear output layer \n    layers.Dense(units=1),\n])\n```","99cbf784":"### Multiple Inputs\n`80 Cereals` dataset has many more features than just `sugars`. To expand the model with more input features, can add more input connections to neuron, one for each additional feature. Output will be sum of bias and multiplication of `each input to weight`.\n![](https:\/\/i.imgur.com\/vyXSnlZ.png)\nOutput will be `y = w0x0 + w1x1 + w2x2 + b`.\n\nA linear unit with two inputs will fit a plane, and a unit with more inputs than that will fit a hyperplane.","b4988bc9":"### Batch Normalization\n`Batch normalization (batchnorm)`, helps correct training that is slow or unstable. With neural networks, it's generally a good idea to put all of your data on a common scale using `scikit-learn's` [StandardScaler](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html) or [MinMaxScaler](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.MinMaxScaler.html). `SGD` will shift the network weights in proportion to how large an activation the data produces. \n\nIf it's good to normalize the data before it goes into the network, maybe also normalizing inside the network would be better! A special kind of layer that can do this is `batch normalization layer`. \n- looks at each batch as it comes in, \n- first normalizing the batch with its own mean and standard deviation, and then \n- also putting the data on a new scale with two trainable rescaling parameters.\n\n`Batchnorm` is added as an aid to the optimization process. Models with batchnorm tend to need fewer epochs to complete training. If having some trouble during training, should consider adding `batch normalization`.","8eb40d97":"### Exercise: Stochastic Gradient Descent\n[Fuel Economy](https:\/\/www.kaggle.com\/sarita19\/fuel-consumption) dataset will be used to explore the effect of the learning rate and batch size on SGD and to predict the fuel economy of an automobile given features","1ad159f8":"#### Adding Dropout\nIn `Keras`, the dropout rate argument `rate` defines what percentage of the input units to shut off. Put the `Dropout layer` just before the layer that want to apply dropout:\n```\nkeras.Sequential([\n    # ...\n    layer.Dropout(rate=0.3), # apply 30% dropout to the next layer\n    layer.Dense(16),\n    # ...\n])\n```","ef3a2e77":"Keras will keep a history of the training and validation loss over the epochs for the training model. How to interpret these learning curves and how we can use them to guide model development. Can examine at learning curve with `underfitting` and `overfitting`.","ef369565":"### Exercise: A Single Neuron\n[Red Wine Quality](https:\/\/www.kaggle.com\/uciml\/red-wine-quality-cortez-et-al-2009) datases consists of physiochemical measurements from about 1600 Portuguese red wines. Also included is a quality rating for each wine from blind taste-tests.","4d957669":"#### Adding Batch Normalization\n```\nlayers.Dense(16, activation='relu'),\nlayers.BatchNormalization(),\n```\nOR\n```\nlayers.Dense(16),\nlayers.BatchNormalization(),\nlayers.Activation('relu'),\n```","c0986c93":"### Adding the Loss and Optimizer\nAfter defining a model, can add a `loss function` and `optimizer` with the model's `compile` method:\n```\nmodel.compile(\n    optimizer=\"adam\",\n    loss=\"mae\",\n)\n```\nThe `gradient` is a vector that tells us in what direction the weights need to go. It tells us how to change the weights to make the loss change `fastest`. Used `gradient decent` to descend loss curve towards minimum. `Stochastic` means `determined by chance`. Our training is stochastic because the minibatches are random samples from the dataset and called SGD!","c54b8522":"### Example - Red Wine Quality\n`Red Wine Quality` dataset consists of physiochemical measurements from about 1600 Portuguese red wines. Also included is a quality rating for each wine from blind taste-tests.\n\nData have rescaled each feature to lie in the interval `[0,1]` since neural netwroks perform best when their inputs are on a common scale.","7cec3be2":"### What is Deep Learning?\nMost impressive in AI have been `deep learning`. *Natural language translation, image recognition, and game playing* are all tasks where deep learning models have neared or even exceeded human-level performance.\n\n`Deep learning` is an approach to machine learning characterized by deep stacks of computations. Power and Scalability `neural networks` become the defining model of deep learning. Neural networks are composed of neurons, where each neuron individually performs only a simple computation.","70664d01":"#### Adding Early Stopping\n`Keras` includes early stopping in training through a `callback` which is a function run every training times. The early stopping callback will run after every epoch. Keras has [pre-defined callbacks](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/callbacks) and can [create callbacks](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/callbacks\/LambdaCallback).\n\n```\nfrom tensorflow.keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(\n    min_delta=0.001,     # minimium amount of change to count as an improvement\n    patience=20,         # how many epochs to wait before stopping\n    restore_best_weights=True,\n)\n```\nAbove callback define: `If there hasn't been at least an improvement of 0.001 in the validation loss over the previous 20 epochs, then stop the training and keep the best model you found.`","f13fd7df":"# Binary Classification","d0e3a7c3":"### The Optimizer - Stochastic Gradient Descent\n`Optimizer` solve the network and it is an algorithm that adjusts the weights to minimize the loss. Virtually all of the optimization algorithms used in deep learning belong to a family called `stochastic gradient descent`. They are iterative algorithms that train a network in steps.\n1. Sample some training data and run it through the network to make predictions.\n2. Measure the loss between the predictions and the true values.\n3. Finally, adjust the weights in a direction that makes the loss smaller.\n\nDo this over and over until the loss is as small as the the desired value.\n![](https:\/\/i.imgur.com\/rFI1tIk.gif)\n\nEach iteration's sample of training data is called a `minibatch` or `batch`, while a complete round of the training data is called an `epoch`. The `number of epochs` you train for is how many times the network will see each training example. The animation shows:\n- linear model from Lesson-1 being trained with `SGD`. \n- pale red dots depict the entire training set, while the solid red dots are the minibatches.\n- every time `SGD` sees a new minibatch, it will shift the `weights` (`w` slope and `b` y-intercept) toward their correct values on that batch.\n- Batch after batch, the line eventually converges to its best fit.\n- loss gets smaller as the weights get closer to their true values.","4a1eb9d6":"### Example - Using Dropout and Batch Normalization\n`Red Wine` dataset will be uesd.","8dd40a3d":"### Early Stopping\nWhen a model is too eagerly learning noise, the validation loss may start to increase during training. To prevent this, training should be stopped  when it seems the validation loss isn't decreasing anymore. Interrupting the training this way is called `early stopping`.\n\n![](https:\/\/i.imgur.com\/eP0gppr.png)\n- once detecting the validation loss is starting to rise again, reset the weights back to where the minimum occured so that the model will not continue to learn noise and overfit the data.\n- Training with early stopping less danger of stopping the training too early before the network has finished learning signal. \n- preventing overfitting from training too long, early stopping can also prevent underfitting from not training long enough.","422f7685":"### The Linear Unit\nA `neuron` (or `unit`) with one input looks like: `y = wx * b`\n![](https:\/\/i.imgur.com\/mfOlDR6.png)\n- `x` is input. Its connection to neuron has a `weight, w`. For the input x, what reaches the neuron is `w * x`. A neural network `learns` by modifying its weights.\n- `b` is a special kind of weight called `bias`. Bias doesn't have any data associated with input. Value reach to neuron is `1 * b`. The bias enables the neuron to modify the output independently of its inputs.\n- `y` is the value the neuron ultimately outputs. Neuron sums up all the values it receives through its connections. `y = w * x + b`","9c8b8bcc":"### The Activation Function\nTwo dense layers with nothing in between are not better than a single dense layer by itself. Dense layers by `themselves` can never move us out of the world of lines and planes. Need `nonlinear` activation function. \n![](https:\/\/i.imgur.com\/OLSUEYT.png)\nWithout `activation functions`, neural networks can only learn `linear` relationships. In order to `fit curves`, we'll need to use activation functions.\n\nAn `activation function, ReLU` is simply some function apply to each of a layer's outputs (its activations). The most common is the `rectifier function` --> `max(0,x)`.\n![](https:\/\/i.imgur.com\/aeIyAlF.png)\n`Rectifier function` has a graph that's a line with the negative part \"rectified\" to zero. Outputs of a neuron will put a `bend` in the data, moving us away from simple lines. \n\nApplying a ReLU activation to a linear unit means the output becomes `max(0, w * x + b)`.\n![](https:\/\/i.imgur.com\/eFry7Yu.png)\n","f2a52d2a":"Will apply neural networks to another common machine learning problem: `classification`. The main difference is in the loss function used and in what kind of outputs want the final layer to produce.","08769e5b":"# A Single Neuron","7777adf1":"### Binary Classification\nClassification into one of two classes is a common machine learning problem. In raw data, the classes might be represented by strings like `\"Yes\" and \"No\"`, or `\"Dog\" and \"Cat\"`. Before using this raw data, need to assign `class label`: `0` and `1`.\n\nClass label can be assigned:\n- `df['Class] = df['Class'].map({'good': 0, 'bad': 1})`","04f1170d":"Why not try one of our Getting Started competitions?\n* Classify images with TPUs in [Petals to the Metal](https:\/\/www.kaggle.com\/c\/tpu-getting-started)\n* Create art with GANs in [I'm Something of a Painter Myself](https:\/\/www.kaggle.com\/c\/gan-getting-started)\n* Classify Tweets in [Real or Not? NLP with Disaster Tweets](https:\/\/www.kaggle.com\/c\/nlp-getting-started)\n* Detect contradiction and entailment in [Contradictory, My Dear Watson](https:\/\/www.kaggle.com\/c\/contradictory-my-dear-watson)","3dda56e0":"---","39432723":"### Interpreting the Learning Curves\nInformation in training data can be two kinds: `signal` and `noise`. \n- The signal is the part that generalizes, the part that help model make predictions from new data.\n- The noise is that part that is `only` true of the training data; the noise is all of the random fluctuation that comes from data in the real-world or all of the incidental, non-informative patterns that can't actually help the model predictions.\n\nTrain a model by choosing weights or parameters that minimize the loss on a training set. Need to evaluate it on a new set of data, `validation data`. When training a model, plot the loss on training set epoch by epoch. And nned to plot validation data too. These plots we call the `learning curves`. \n![](https:\/\/i.imgur.com\/tHiVFnM.png)\n- The training loss will go down either when the model learns signal or when it learns noise. \n- The validation loss will go down only when the model learns signal.\n- when a model learns signal both curves go down, but when it learns noise a gap is created in the curves. \n    - The size of the gap tells you how much noise the model has learned.\n\n![](https:\/\/i.imgur.com\/eUF6mfo.png)\nThis trade-off indicates that there can be two problems that occur when training a model: `not enough signal` or `too much noise`.\n- `Underfitting` the training set is when the loss is not as low as it could be because the model hasn't learned enough signal.\n- `Overfitting` the training set is when the loss is not as low as it could be because the model learned too much noise.\n\n`The trick to training deep learning models is finding the best balance between the two.`\n\n","65470ed8":"---","d5bfd4c4":"Can create animation by using different values.\n\n| `learning_rate` | `batch_size` | `num_examples` |\n|-----------------|--------------|----------------|\n| 0.05            | 32           | 256            |\n| 0.05            | 2            | 256            |\n| 0.05            | 128          | 256            |\n| 0.02            | 32           | 256            |\n| 0.2             | 32           | 256            |\n| 1.0             | 32           | 256            |\n| 0.9             | 4096         | 8192           |\n| 0.99            | 4096         | 8192           |","b23f9f8c":"There are dozens of kinds of layers to add to a model, [Keras docs](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/). Some are like dense layers and define connections between neurons, and others can do preprocessing or transformations of other sorts. Will learn two kinds of special layers, not containing any neurons themselves, but that add some functionality that can sometimes benefit a model in various ways.","fdd894c9":"# Overfitting and Underfitting","fadf9aeb":"# Deep Neural Networks","bdd7fe5c":"To build deep neural networks using `Keras` and `Tensorflow`.\n* create a `fully-connected` neural network architecture\n* apply neural nets to two classic ML problems: `regression` and `classification`\n* train neural nets with `stochastic gradient descent`, and\n* improve performance with `dropout, batch normalization`, and other techniques","761a6279":"### Example - Binary Classification\n[Lonosphere](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Ionosphere) dataset contains features obtained from radar signals focused on the ionosphere layer of the Earth's atmosphere. The task is to determine whether the signal shows the presence of some object, or just empty air.","1f1385cf":"### Example - The Linear Unit as a Model\nSingle neuron models are `linear models`. Example with [80 Cereals](https:\/\/www.kaggle.com\/crawford\/80-cereals) datasets. \n\nTraining a model with `sugars` (grams of sugars per serving) as input and `calories` (calories per serving) as output. The bias is `b=90` and the weight is `w=2.5` will result.\n![](https:\/\/i.imgur.com\/yjsfFvY.png)","ba4b85f8":"How can build neural networks capable of learning the complex kinds of relationships deep neural networks. Key idea is `modularity`, building up a complex network from simpler functional units. ","8dba54f0":"# Stochastic Gradient Descent","733d1a0d":"### Layers\nNeural networks typically organize their neurons into `layers`. When we collect together linear units having a common set of inputs, we get a dense layer.\n![](https:\/\/i.imgur.com\/2MA4iMV.png)\n\nA `layer` in Keras is a very general kind of thing. A layer can be, essentially, any kind of `data transformation`. Many layers, like the `convolutional` and `recurrent` layers, `transform data` through use of neurons and differ primarily in the pattern of connections they form.","d406fc26":"### Capacity\nA model's c`apacity` refers to the size and complexity of the patterns it is able to learn. For neural networks:\n- by how many neurons it has and how they are connected together.\n- if the network is underfitting, should increase the capacity. Two ways to increase\n    - by making it wider (more units to existing layers) --> have an easier time learning more linear relationships \n    - by making it deeper (adding more layers) --> prefer more nonlinear ones.\n\n```\nmodel = keras.Sequential([\n    layers.Dense(16, activation='relu'),\n    layers.Dense(1),\n])\n\nwider = keras.Sequential([\n    layers.Dense(32, activation='relu'),\n    layers.Dense(1),\n])\n\ndeeper = keras.Sequential([\n    layers.Dense(16, activation='relu'),\n    layers.Dense(16, activation='relu'),\n    layers.Dense(1),\n])\n```","1054578c":"### Dropout\n`Dropout laer` helps to correct overfitting. Randomly drop out some fraction of a layer's input units every step of training, making it much harder for the network to learn those spurious patterns in the training data. \n![](https:\/\/i.imgur.com\/a86utxY.gif)\n- dropout as creating a kind of ensemble of networks.\n- predictions will no longer be made by one big network, but instead by a committee of smaller networks.\n- individuals in the committee tend to make different kinds of mistakes, but be right at the same time, making the committee as a whole better than any individual.\n    - same idea as random forests as an ensemble of decision trees","8bc76159":"Figure to define mode.\n![](https:\/\/i.imgur.com\/V04o59Z.png)","6d02bc70":"---"}}