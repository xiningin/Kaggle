{"cell_type":{"dd304177":"code","cff83bd6":"code","df1a8e85":"code","879398e9":"code","47239f84":"code","d3b53cf8":"code","290dfdc1":"code","838ab3bf":"code","514c4c59":"code","374f5d2e":"code","fb8ccf82":"code","ca52323a":"code","5d35e0db":"code","cf03646e":"code","36eae1cb":"markdown","a04717e1":"markdown","99d4877c":"markdown","734d363d":"markdown","3960f5fd":"markdown","5a4bfebe":"markdown","8c8c27a2":"markdown","83513f8e":"markdown","3c3f7b75":"markdown","9faa39b6":"markdown","c1c61579":"markdown","3980fb28":"markdown","d7aa5eff":"markdown"},"source":{"dd304177":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim","cff83bd6":"def prepare_sequence(seq, to_ix):\n    \"\"\"Input: takes in a list of words, and a dictionary containing the index of the words\n    Output: a tensor containing the indexes of the word\"\"\"\n    idxs = [to_ix[w] for w in seq]\n    return torch.tensor(idxs, dtype=torch.long)\n\n# This is the example training data\ntraining_data = [\n    (\"the dog happily ate the big apple\".split(), [\"DET\", \"NN\", \"ADV\", \"V\", \"DET\", \"ADJ\", \"NN\"]),\n    (\"everybody read that good book quietly in the hall\".split(), [\"NN\", \"V\", \"DET\", \"ADJ\", \"NN\", \"ADV\", \"PRP\", \"DET\", \"NN\"]),\n    (\"the old head master sternly scolded the naughty children for \\\n     being very loud\".split(), [\"DET\", \"ADJ\", \"ADJ\", \"NN\", \"ADV\", \"V\", \"DET\", \"ADJ\",  \"NN\", \"PRP\", \"V\", \"ADJ\", \"NN\"]),\n    (\"i love you loads\".split(), [\"PRN\", \"V\", \"PRN\", \"ADV\"])\n]\n#  These are other words which we would like to predict (within sentences) using the model\nother_words = [\"area\", \"book\", \"business\", \"case\", \"child\", \"company\", \"country\", \n               \"day\", \"eye\", \"fact\", \"family\", \"government\", \"group\", \"hand\", \"home\", \n               \"job\", \"life\", \"lot\", \"man\", \"money\", \"month\", \"mother\", \"food\", \"night\", \n               \"number\", \"part\", \"people\", \"place\", \"point\", \"problem\", \"program\", \n               \"question\", \"right\", \"room\", \"school\", \"state\", \"story\", \"student\", \n               \"study\", \"system\", \"thing\", \"time\", \"water\", \"way\", \"week\", \"woman\", \n               \"word\", \"work\", \"world\", \"year\", \"ask\", \"be\", \"become\", \"begin\", \"can\", \n               \"come\", \"do\", \"find\", \"get\", \"go\", \"have\", \"hear\", \"keep\", \"know\", \"let\", \n               \"like\", \"look\", \"make\", \"may\", \"mean\", \"might\", \"move\", \"play\", \"put\", \n               \"run\", \"say\", \"see\", \"seem\", \"should\", \"start\", \"think\", \"try\", \"turn\", \n               \"use\", \"want\", \"will\", \"work\", \"would\", \"asked\", \"was\", \"became\", \"began\", \n               \"can\", \"come\", \"do\", \"did\", \"found\", \"got\", \"went\", \"had\", \"heard\", \"kept\", \n               \"knew\", \"let\", \"liked\", \"looked\", \"made\", \"might\", \"meant\", \"might\", \"moved\", \n               \"played\", \"put\", \"ran\", \"said\", \"saw\", \"seemed\", \"should\", \"started\", \n               \"thought\", \"tried\", \"turned\", \"used\", \"wanted\" \"worked\", \"would\", \"able\", \n               \"bad\", \"best\", \"better\", \"big\", \"black\", \"certain\", \"clear\", \"different\", \n               \"early\", \"easy\", \"economic\", \"federal\", \"free\", \"full\", \"good\", \"great\", \n               \"hard\", \"high\", \"human\", \"important\", \"international\", \"large\", \"late\", \n               \"little\", \"local\", \"long\", \"low\", \"major\", \"military\", \"national\", \"new\", \n               \"old\", \"only\", \"other\", \"political\", \"possible\", \"public\", \"real\", \"recent\", \n               \"right\", \"small\", \"social\", \"special\", \"strong\", \"sure\", \"true\", \"white\", \n               \"whole\", \"young\", \"he\", \"she\", \"it\", \"they\", \"i\", \"my\", \"mine\", \"your\", \"his\", \n               \"her\", \"father\", \"mother\", \"dog\", \"cat\", \"cow\", \"tiger\", \"a\", \"about\", \"all\", \n               \"also\", \"and\", \"as\", \"at\", \"be\", \"because\", \"but\", \"by\", \"can\", \"come\", \"could\", \n               \"day\", \"do\", \"even\", \"find\", \"first\", \"for\", \"from\", \"get\", \"give\", \"go\", \n               \"have\", \"he\", \"her\", \"here\", \"him\", \"his\", \"how\", \"I\", \"if\", \"in\", \"into\", \n               \"it\", \"its\", \"just\", \"know\", \"like\", \"look\", \"make\", \"man\", \"many\", \"me\", \n               \"more\", \"my\", \"new\", \"no\", \"not\", \"now\", \"of\", \"on\", \"one\", \"only\", \"or\", \n               \"other\", \"our\", \"out\", \"people\", \"say\", \"see\", \"she\", \"so\", \"some\", \"take\", \n               \"tell\", \"than\", \"that\", \"the\", \"their\", \"them\", \"then\", \"there\", \"these\", \n               \"they\", \"thing\", \"think\", \"this\", \"those\", \"time\", \"to\", \"two\", \"up\", \"use\", \n               \"very\", \"want\", \"way\", \"we\", \"well\", \"what\", \"when\", \"which\", \"who\", \"will\", \n               \"with\", \"would\", \"year\", \"you\", \"your\"]\n\nword_to_ix = {} # This is the word dictionary which will contain the index to each word\n\nfor sent, tags in training_data:\n    for word in sent:\n        if word not in word_to_ix.keys():\n            word_to_ix[word] = len(word_to_ix)\nfor word in other_words:\n    if word not in word_to_ix.keys():\n            word_to_ix[word] = len(word_to_ix)\n\n# print(word_to_ix) # Just have a look at what it contains\n\ntag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2, \"ADJ\": 3, \"ADV\": 4, \"PRP\": 5, \"PRN\": 6} # This dictionary contains the indices of the tags\n\nEMBEDDING_DIM = 64\nHIDDEN_DIM = 64\n","df1a8e85":"class LSTMTagger(nn.Module):\n    def __init__(self, embedding_dim, hidden_dim, vocab_size, target_size):\n        super(LSTMTagger, self).__init__()\n        \n        self.hidden_dim = hidden_dim\n        \n        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n        self.hidden2tag = nn.Linear(hidden_dim, target_size)\n        \n    def forward(self, sentence):\n        embeds = self.word_embeddings(sentence)\n        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n        tag_scores = F.log_softmax(tag_space, dim=1)\n        return tag_scores","879398e9":"\n# Here I initialize the model with all the necesarry parameters\nmodel = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix.keys()), len(tag_to_ix.keys()))\n\n# Define the loss function as the Negative Log Likelihood loss (NLLLoss)\nloss_function = nn.NLLLoss()\n\n# We will be using a simple SGD optimizer\noptimizer = optim.SGD(model.parameters(), lr=0.1)\n\n# The test sentence\nseq1 = \"everybody read the book and ate the food\".split()\nseq2 = \"she like my dog\".split()\nprint(\"Running a check on the model before training.\\nSentences:\\n{}\\n{}\".format(\" \".join(seq1), \" \".join(seq2)))\nwith torch.no_grad():\n    for seq in [seq1, seq2]:\n        inputs = prepare_sequence(seq, word_to_ix)\n        tag_scores = model(inputs)\n        _, indices = torch.max(tag_scores, 1)\n        ret = []\n        for i in range(len(indices)):\n            for key, value in tag_to_ix.items():\n                if indices[i] == value:\n                    ret.append((seq[i], key))\n        print(ret)\n    \nprint(\"Training Started\")\nfor epoch in range(300):\n    for sentence, tags in training_data:\n        model.zero_grad()\n        sentence_in = prepare_sequence(sentence, word_to_ix)\n        targets = prepare_sequence(tags, tag_to_ix)\n        \n        tag_scores = model(sentence_in)\n        \n        loss = loss_function(tag_scores, targets)\n        loss.backward()\n        optimizer.step()\nprint(\"Training Finished!!!\\nAgain testing on unknown data\")\nwith torch.no_grad():\n    for seq in [seq1, seq2]:\n        inputs = prepare_sequence(seq, word_to_ix)\n        tag_scores = model(inputs)\n        _, indices = torch.max(tag_scores, 1)\n        ret = []\n        for i in range(len(indices)):\n            for key, value in tag_to_ix.items():\n                if indices[i] == value:\n                    ret.append((seq[i], key))\n        print(ret)","47239f84":"import nltk\n\ntagged_sentence = nltk.corpus.treebank.tagged_sents(tagset='universal')\nprint(\"Number of Tagged Sentences \",len(tagged_sentence))","d3b53cf8":"# Let's have a look at some sentences\n\nprint(tagged_sentence[:3][:10])","290dfdc1":"def word_to_ix(word, ix):\n    return torch.tensor(ix[word], dtype = torch.long)\n\ndef char_to_ix(char, ix):\n    return torch.tensor(ix[char], dtype= torch.long)\n\ndef tag_to_ix(tag, ix):\n    return torch.tensor(ix[tag], dtype= torch.long)\n\ndef sequence_to_idx(sequence, ix):\n    return torch.tensor([ix[s] for s in sequence], dtype=torch.long)\n\n\nword_to_idx = {}\ntag_to_idx = {}\nchar_to_idx = {}\nfor sentence in tagged_sentence:\n    for word, pos_tag in sentence:\n        if word not in word_to_idx.keys():\n            word_to_idx[word] = len(word_to_idx)\n        if pos_tag not in tag_to_idx.keys():\n            tag_to_idx[pos_tag] = len(tag_to_idx)\n        for char in word:\n            if char not in char_to_idx.keys():\n                char_to_idx[char] = len(char_to_idx)","838ab3bf":"word_vocab_size = len(word_to_idx)\ntag_vocab_size = len(tag_to_idx)\nchar_vocab_size = len(char_to_idx)\n\nprint(\"Unique words: {}\".format(len(word_to_idx)))\nprint(\"Unique tags: {}\".format(len(tag_to_idx)))\nprint(\"Unique characters: {}\".format(len(char_to_idx)))","514c4c59":"import random\n\ntr_random = random.sample(list(range(len(tagged_sentence))), int(0.95 * len(tagged_sentence)))\n\ntrain = [tagged_sentence[i] for i in tr_random]\ntest = [tagged_sentence[i] for i in range(len(tagged_sentence)) if i not in tr_random]","374f5d2e":"WORD_EMBEDDING_DIM = 1024\nCHAR_EMBEDDING_DIM = 128\nWORD_HIDDEN_DIM = 1024\nCHAR_HIDDEN_DIM = 1024\nEPOCHS = 70","fb8ccf82":"class DualLSTMTagger(nn.Module):\n    def __init__(self, word_embedding_dim, word_hidden_dim, char_embedding_dim, char_hidden_dim, word_vocab_size, char_vocab_size, tag_vocab_size):\n        super(DualLSTMTagger, self).__init__()\n        self.word_embedding = nn.Embedding(word_vocab_size, word_embedding_dim)\n        \n        self.char_embedding = nn.Embedding(char_vocab_size, char_embedding_dim)\n        self.char_lstm = nn.LSTM(char_embedding_dim, char_hidden_dim)\n        \n        self.lstm = nn.LSTM(word_embedding_dim + char_hidden_dim, word_hidden_dim)\n        self.hidden2tag = nn.Linear(word_hidden_dim, tag_vocab_size)\n        \n    def forward(self, sentence, words):\n        embeds = self.word_embedding(sentence)\n        char_hidden_final = []\n        for word in words:\n            char_embeds = self.char_embedding(word)\n            _, (char_hidden, char_cell_state) = self.char_lstm(char_embeds.view(len(word), 1, -1))\n            word_char_hidden_state = char_hidden.view(-1)\n            char_hidden_final.append(word_char_hidden_state)\n        char_hidden_final = torch.stack(tuple(char_hidden_final))\n        \n        combined = torch.cat((embeds, char_hidden_final), 1)\n\n        lstm_out, _ = self.lstm(combined.view(len(sentence), 1, -1))\n        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n        \n        tag_scores = F.log_softmax(tag_space, dim=1)\n        return tag_scores","ca52323a":"model = DualLSTMTagger(WORD_EMBEDDING_DIM, WORD_HIDDEN_DIM, CHAR_EMBEDDING_DIM, CHAR_HIDDEN_DIM, word_vocab_size, char_vocab_size, tag_vocab_size)\n\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n\nif use_cuda:\n    model.cuda()\n\n# Define the loss function as the Negative Log Likelihood loss (NLLLoss)\nloss_function = nn.NLLLoss()\n\n# We will be using a simple SGD optimizer\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\n# The test sentence\nseq = \"everybody eat the food . I kept looking out the window , trying to find the one I was waiting for .\".split()\nprint(\"Running a check on the model before training.\\nSentences:\\n{}\".format(\" \".join(seq)))\nwith torch.no_grad():\n    words = [torch.tensor(sequence_to_idx(s[0], char_to_idx), dtype=torch.long).to(device) for s in seq]\n    sentence = torch.tensor(sequence_to_idx(seq, word_to_idx), dtype=torch.long).to(device)\n        \n    tag_scores = model(sentence, words)\n    _, indices = torch.max(tag_scores, 1)\n    ret = []\n    for i in range(len(indices)):\n        for key, value in tag_to_idx.items():\n            if indices[i] == value:\n                ret.append((seq[i], key))\n    print(ret)\n# Training start\nprint(\"Training Started\")\naccuracy_list = []\nloss_list = []\ninterval = round(len(train) \/ 100.)\nepochs = EPOCHS\ne_interval = round(epochs \/ 10.)\nfor epoch in range(epochs):\n    acc = 0 #to keep track of accuracy\n    loss = 0 # To keep track of the loss value\n    i = 0\n    for sentence_tag in train:\n        i += 1\n        words = [torch.tensor(sequence_to_idx(s[0], char_to_idx), dtype=torch.long).to(device) for s in sentence_tag]\n        sentence = [s[0] for s in sentence_tag]\n        sentence = torch.tensor(sequence_to_idx(sentence, word_to_idx), dtype=torch.long).to(device)\n        targets = [s[1] for s in sentence_tag]\n        targets = torch.tensor(sequence_to_idx(targets, tag_to_idx), dtype=torch.long).to(device)\n        \n        model.zero_grad()\n        \n        tag_scores = model(sentence, words)\n        \n        loss = loss_function(tag_scores, targets)\n        loss.backward()\n        optimizer.step()\n        loss += loss.item()\n        _, indices = torch.max(tag_scores, 1)\n#         print(indices == targets)\n        acc += torch.mean(torch.tensor(targets == indices, dtype=torch.float))\n        if i % interval == 0:\n            print(\"Epoch {} Running;\\t{}% Complete\".format(epoch + 1, i \/ interval), end = \"\\r\", flush = True)\n    loss = loss \/ len(train)\n    acc = acc \/ len(train)\n    loss_list.append(float(loss))\n    accuracy_list.append(float(acc))\n    if (epoch + 1) % e_interval == 0:\n        print(\"Epoch {} Completed,\\tLoss {}\\tAccuracy: {}\".format(epoch + 1, np.mean(loss_list[-e_interval:]), np.mean(accuracy_list[-e_interval:])))","5d35e0db":"import matplotlib.pyplot as plt\nplt.plot(accuracy_list, c=\"red\", label =\"Accuracy\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Value\")\nplt.legend()\nplt.show()\n\nplt.plot(loss_list, c=\"blue\", label =\"Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Value\")\nplt.legend()\nplt.show()","cf03646e":"with torch.no_grad():\n    words = [torch.tensor(sequence_to_idx(s[0], char_to_idx), dtype=torch.long).to(device) for s in seq]\n    sentence = torch.tensor(sequence_to_idx(seq, word_to_idx), dtype=torch.long).to(device)\n        \n    tag_scores = model(sentence, words)\n    _, indices = torch.max(tag_scores, 1)\n    ret = []\n    for i in range(len(indices)):\n        for key, value in tag_to_idx.items():\n            if indices[i] == value:\n                ret.append((seq[i], key))\n    print(ret)","36eae1cb":"### Downloading Data\nFirst we will download data from nltk, a very popular python natural language processing toolkit, incase you haven't heard of it.\n\nKindly refer to their site for more information: https:\/\/www.nltk.org\/","a04717e1":"# Introduction\nAll of us, irrespective of our current level of expertise have come across LSTMs. They are intriguing in my opinion, some kind of a magical boxes which can take in sequences and spit out very accurate results.\n\nHowever, although LSTMs are very good at what it does, deep learning newbies (read wannabees) like me cannot help think \"they are too complicated\", or \"coding LSTM based models isnt my cup of tea\". I have been there,.... hmmm... to be honest, some parts of me thinks that I am still there, inspite of working in Deep Learning architectures for over a year. But although LSTMs are actually very difficult concepts to grasp mathematically (remember \"backpropagation through time\" - sounds like some sci fi movie IMHO), coding them is not as difficult, thanks to PyTorch's simple coding style. Like always the simplicity with which we can develop complicated LSTM based RNNs in PyTorch is another example why it just brushes aside the competition (read. Tensorflow and its sidekick Keras).\n\nIn this notebook, I will try and explain how to code up a Part of Speech tagger. One simple one and a somewhat complicated one for POS Tagging. So lets get started.\n\n##### P.S.: I do not work for PyTorch or Facebook (although I really wish I did). My lame attempt at promoting PyTorch is just because I am a fan of its simplicity and interactivity.\n\n##### P.S. 2: In case my insignificant post comes across someone who develops Tensorflow\/Keras or anyone in Google in general, please know that I am a huge fan of all the things you've done from scratch in order to promote deep learning and playing a major part in making it what it is today. ","99d4877c":"### Singe Test after Training","734d363d":"## The model classes\nHere I will construct the model classes, the Word LSTM tagger and the Character LSTM Tagger.\nThe LSTM in both cases takes in a sequence (words or characters), embeds the sequence into an embedding space (dimension of the space is a hyperparameter), and runs the LSTM model.\n\nIn our case, first the word level LSTM will take in a sequence of words and convert them into the word embedding space.\nSimilarly, it will take the words sequentially  and run the character level LSTM model, which will first take in the sequence of characters in each word and project it in the character embedding space and then run the LSTM model and take its hidden state and feed it back to the word LSTM model.\n\nUsing the character level hidden representation for every word as well as the word embedding, the word level model then runs LSTM on the sequence of words, and outputs the predictions for every tag. This prediction is later processed to find the corresponding tags.","3960f5fd":"### Defining the model parameters here","5a4bfebe":"### Performance:\nSo you might have guessed that this model is not that great in itself. Actually it is not supposed to be good. The words it can predict accurately are mostly words which the model had encountered during training, and remembers them during test time. Whenever it encounters a new word, it fails badly. ","8c8c27a2":"### Result:\nIt seems our model is able to learn the words well even in the short training.\n\nIn the next kernel, we will explore deeper LSTM models based on similar lines.\n\nThe deeper model can be found here.\nhttps:\/\/www.kaggle.com\/krishanudb\/deep-pos-tagger\/","83513f8e":"### Part 1: Simple small LSTM tagger\nThe below code is a very lame attempt at developing a LSTM for part of speech tagging. One glance at the code and you'll probably realize its too simplistic. But for beginners it will be useful IMHO.","3c3f7b75":"## Train\/Test Splitting the data","9faa39b6":"### Importing all the necesarry packages","c1c61579":"Now we will construct a dictionary for all these:\n1. A word\/tag dictionary,\n2. A letter\/character dictionary\n3. A POS tag dictionary","3980fb28":"#### Model\nThe below code is for making a very simple LSTM Model in PyTorch.\nThe class LSTMTagger must subclass the nn.Module class of PyTorch so that it can get all the functionalities of the nn.Module class. Very simple.\nIf this is confusing to you, dont worry just now just try and remember it as a convention.","d7aa5eff":"## Part 2: Somewhat complicated LSTM Model\nIn this part, we (including me!! This is my first attempt) will learn how to make a slightly more complicated LSTM Model which does the same thing: takes in a sentence and predicts all the POS Tags of the words.\n\nThe model will comprise of two LSTM based models, one for character wise and the other for word wise tag classification. So without further adieu lets get started."}}