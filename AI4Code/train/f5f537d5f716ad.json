{"cell_type":{"0e4f0917":"code","13ef77b1":"code","a43fb83b":"code","472f0118":"code","3348d709":"code","ba82573e":"code","8cabfddf":"code","fdfb8474":"code","028394ba":"code","5882d6f1":"code","203b34c9":"code","636d918e":"code","ea89324a":"code","934ea98f":"code","e32041bc":"code","fdbd0d39":"code","105d4c33":"code","fcaa7028":"code","2befdd82":"code","19d6aef1":"code","bd1c0fb6":"code","dfe14688":"code","7e60dd89":"code","0d84481c":"code","120737aa":"code","3ecd00d4":"code","0fe0d93d":"code","fb8ec230":"code","770df266":"code","13f70ffe":"code","e1b6568b":"code","97e2adba":"code","81be5188":"code","2728c5f4":"code","9198fbf6":"code","e3d835e0":"code","b718c151":"code","fa91ae39":"code","b1e48276":"code","77cbdfa1":"code","904843df":"code","49fbe341":"code","97065910":"code","bd70a1d5":"code","7bf68b6d":"code","936509a1":"code","5859e6eb":"code","71603d84":"code","67949d58":"code","ea83b9bd":"code","c40c214d":"code","14955700":"code","c853dd07":"code","c7d15021":"code","8d82c629":"code","7d9d2473":"code","1246f8e1":"code","b5685fcc":"code","bb67730c":"code","7a7b80f1":"code","c259c862":"code","a6434495":"code","7de73117":"code","023b57e5":"code","f2ec5c71":"code","f15cf8e5":"code","1fc2f7c5":"code","32f10823":"code","6638ec38":"code","35686c65":"code","0f8f7d22":"code","4e8a4305":"code","ec057a5e":"code","f0dde441":"code","b1f9b79d":"code","d3f221ed":"code","9904b040":"code","7bfc4716":"code","373ac0a3":"code","6fcc37e5":"code","e85703e4":"code","3ce4053f":"code","1c4dde78":"code","94119f18":"code","53009be6":"code","97213526":"code","5506030d":"code","9160790e":"code","135f3601":"code","3861adc7":"code","4b618497":"code","94a169b8":"code","04ee31c8":"code","2c6ac84d":"code","5da46a39":"code","830da7e4":"code","a22987c2":"code","765f0343":"code","2493c94a":"code","51d2aa22":"code","cd805b94":"code","891e4a8c":"code","e2873916":"code","10c163ae":"code","33557d76":"code","77c35314":"code","6c575870":"code","ffc61d74":"code","a1f21f19":"code","375cf403":"code","0e048ad4":"code","17187489":"code","541f5d63":"code","918c2ce5":"code","da7ddc90":"code","bb61a988":"code","b45454f4":"code","679fc0e4":"code","65a8bf77":"code","a35e1960":"code","a3d20db4":"code","84c04ee0":"code","36841e3a":"code","7cdcd083":"code","892dcdbf":"code","371d5ee8":"code","ef6fd113":"code","67dd7cc1":"code","c207afb8":"code","32b9df5f":"code","044ae5c4":"code","e9289218":"code","31bd52fa":"code","38389055":"code","48049853":"code","c64a57d3":"code","6445428b":"code","a06fc4db":"code","8fec7eb5":"code","71d3e2d6":"code","6cbcc999":"code","8816930e":"code","84100fc7":"code","25f92108":"code","63b04202":"code","f0c0ff11":"code","87f20b5a":"code","197382a6":"code","7799a4cc":"code","4a0a4fed":"code","1768c462":"code","6fdea5d9":"code","bb9370a4":"code","8e2bdcdc":"code","2426b779":"code","04cbdab2":"code","c2405fe4":"code","721c1dc6":"code","011a707c":"code","aaef2451":"code","3c87cd13":"code","02862cce":"code","cb4f8b60":"code","0f0cde50":"code","25545953":"code","08a51108":"code","f839a153":"code","eea81194":"code","aa18810e":"code","c7e9d7b0":"code","348686d1":"code","2464192f":"code","aa03d16f":"code","7b9694cf":"code","4c4a1808":"code","ef5202be":"code","34c57d45":"code","dce6ebf5":"code","1d4b4f52":"code","b851ee12":"code","bf306a5e":"code","44597312":"code","4f575874":"code","ba8cbb20":"code","d6e726a7":"code","276aab7c":"code","5d0d43c8":"code","97af16c7":"code","95456de8":"code","fbdeecc5":"code","9a8504f1":"code","b2d502eb":"code","2e77cbb5":"code","f5d8aea2":"code","fcb29f48":"code","5d986bc1":"code","46146e3b":"code","5065b680":"code","d55bd3e0":"code","fd15f42c":"code","e253a371":"code","3b0d5797":"code","a293d277":"code","bcad9d71":"code","855dd4db":"code","4a9ad638":"code","cdcac2bd":"code","af869eaf":"code","cbc37e80":"code","e27fa458":"code","1dfa5475":"code","92c236c2":"code","47b89174":"code","02bf5533":"code","98f6c724":"code","5e7794b1":"code","e53c0ac8":"code","54e31adb":"code","fc48fe6f":"code","4848d15d":"code","21aa31ce":"code","603b7cab":"code","246bf986":"code","1d86aa1a":"code","c728c6ed":"code","d49fba55":"code","a77264c6":"code","411b122f":"code","dadce0f6":"code","93f7b14c":"code","c4135ff5":"code","f0f05cdd":"code","9ed1681e":"code","9813e8aa":"code","91f9805c":"code","eba53a52":"code","4cabf158":"code","02a32124":"code","59c3bc95":"code","44b2c95e":"code","76a3d88b":"code","18cf4f2b":"code","43238b85":"code","b5290a5a":"code","55e6fe6a":"code","4dd82e73":"code","0e2cadeb":"code","2b9cdbc3":"code","cd4929f3":"code","ed77eb3f":"code","f14209a6":"code","9393b5fb":"code","5dd7394b":"code","bb467674":"code","7cceac2e":"code","53582176":"code","a8cf8fc8":"code","781ea812":"code","08541d67":"code","9bfff970":"code","c4ae5d29":"code","85c7f85e":"code","d2ec8830":"code","db690cb2":"code","77390d28":"code","227af306":"code","2af644ad":"code","c03846df":"code","e4d32388":"code","c0965301":"code","2523b7c1":"code","a431ae33":"code","92bf3457":"code","db9571f4":"code","2e920d87":"code","6845b025":"code","33f0dace":"code","e5c0ec71":"code","bb97c756":"code","7535c405":"code","e916b616":"code","9a88579b":"code","201a5f12":"code","e41b7e73":"code","4d0891e4":"code","48f8dc52":"code","b882b847":"code","b4710e99":"code","41f10e24":"code","b9564852":"code","eb9ef734":"code","c9f05c4e":"code","de9f578a":"code","48abcb72":"code","dcfab3bc":"code","25175366":"code","157dc714":"code","d168a190":"code","afba3a5f":"code","70cfb526":"code","49b51c8c":"code","7e2eb153":"code","1575eb37":"code","16b0e35f":"code","ef567373":"code","79c61ffe":"code","eada7d49":"code","b035b2f9":"code","b11f4752":"code","d9a76280":"code","3e27a57b":"code","678bb5dc":"code","591807e0":"code","9cdc10c4":"code","367588c3":"code","9dd0bb50":"code","9b03eae4":"code","48d013cd":"code","ff0d0a67":"code","02b09e6d":"code","e50e02d1":"code","16b11890":"code","f8ca2977":"code","c2ea30f1":"code","9eaa9d84":"code","dce712a9":"code","343692fc":"code","ea2b4f03":"code","4e675aba":"code","2ce0d8fd":"code","4bc57054":"code","496f210a":"code","59e7ebfc":"code","006348a2":"code","19102bde":"code","8ac47493":"code","0487fe9a":"code","7143477c":"code","072321f2":"code","026e62c4":"code","95c2050b":"code","88b140ef":"code","440b0eed":"code","5ba84985":"code","d32db700":"code","ce7deeab":"code","a551c74c":"code","9f129583":"code","f39ed098":"code","e782984f":"code","8b42c056":"code","f51664f6":"code","ab56f52f":"code","7a973c2c":"code","99220c40":"code","adaac79e":"code","32edd725":"code","30518a7d":"code","947784fb":"code","fffd41f0":"code","f2e40608":"code","45ce9317":"code","a0ae1704":"code","caf19d90":"code","6495455d":"code","f7cf1237":"code","e9dc9b07":"code","f7d33f96":"code","9b6df577":"code","f79a26aa":"code","a3040197":"code","07fab8d1":"code","0c0882e0":"code","35076367":"code","59c6f8b5":"code","ed0dbbc2":"code","706081e0":"code","72ba99af":"code","31a6c151":"code","e539c7a0":"code","38195464":"code","26b7589f":"code","ec2380ed":"code","5e82b0d3":"code","393d3a7b":"code","d6897908":"code","39dd501c":"code","728b5d62":"code","dfb692f2":"code","a3246cd4":"code","cb41bd6b":"markdown","ee52bce4":"markdown","83081d96":"markdown","50067e16":"markdown","3dc7d69d":"markdown","55681de9":"markdown","6854c29e":"markdown","5752e549":"markdown","00bd615f":"markdown","7f7e3d5a":"markdown","0c0ff6c3":"markdown","c3194555":"markdown","309ff447":"markdown","ea8095d9":"markdown","2755a408":"markdown","2aecbb5c":"markdown","d9611d2b":"markdown","0cf5af9d":"markdown","9aa3a1ee":"markdown","437c6327":"markdown","18125ac9":"markdown","18e9073f":"markdown","ae4c4fcd":"markdown","7f1edde0":"markdown","53e37791":"markdown","100271ec":"markdown","2f27eaa8":"markdown","17a263ba":"markdown","17895513":"markdown","cb9fe02f":"markdown","6fd803c4":"markdown","e618b353":"markdown","18265c36":"markdown","3c0e8916":"markdown","cfdae353":"markdown","596176c4":"markdown","074a1424":"markdown","bd44b3bf":"markdown","b8d3fdc0":"markdown","33a53817":"markdown","5e0d0f31":"markdown","decd8cb4":"markdown","409f97e0":"markdown","502a46ec":"markdown","a8b17015":"markdown","90b2c370":"markdown","14bb55b1":"markdown","5ff394a6":"markdown","217bb3f4":"markdown","febe1ed5":"markdown","3c7f816a":"markdown","8eb70d91":"markdown","13cade98":"markdown","276e50b3":"markdown","280875fd":"markdown","1e46218b":"markdown","7564bc0a":"markdown","b52905b0":"markdown","cc02358d":"markdown","13e4f710":"markdown","d6c3afce":"markdown","9071dc9d":"markdown","f3710d6e":"markdown","9367f194":"markdown","cc72c323":"markdown","c7b0591c":"markdown","3152cf8c":"markdown","5ac28276":"markdown","5ce9a031":"markdown","cafccde7":"markdown","ca3f3862":"markdown","79ded085":"markdown","2fae55f3":"markdown","b77052a7":"markdown","17f92ba8":"markdown","7c91004b":"markdown","7024497b":"markdown","c0b8f425":"markdown","75b91816":"markdown","b1960d9c":"markdown","90905289":"markdown","7fb166a2":"markdown","f63e73a8":"markdown","016f65d9":"markdown","3c9de7d9":"markdown","5ac71dab":"markdown","98e8c501":"markdown","27a5d1d2":"markdown","9dd19785":"markdown","3a20eb6e":"markdown","217777d5":"markdown","57b1d861":"markdown","c16125bb":"markdown","bbeaa0f1":"markdown","c2ceda9b":"markdown","1c40e6e4":"markdown","04a7153c":"markdown","7a15c6dc":"markdown","0846cc04":"markdown","4a63fcdd":"markdown","050340bd":"markdown","befa3d02":"markdown","af710ae4":"markdown","ec363296":"markdown","be545152":"markdown","0902a86a":"markdown","bb38a30e":"markdown","2c4e0f44":"markdown","e976d65a":"markdown","2b08fe45":"markdown","843da04d":"markdown","bde5917e":"markdown","b176be8e":"markdown","28d648e7":"markdown","ecdbdc80":"markdown","aa784bd1":"markdown","22d23fc5":"markdown","3e26b7a9":"markdown","80577d8e":"markdown","56979f72":"markdown"},"source":{"0e4f0917":"# !pip3 install -I seaborn==0.11.0\nimport sys\nimport numpy as np\nimport pandas as pd\nimport math\n\n# Plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Diplaying\nfrom IPython.core.display import display, HTML\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Statistics\nfrom scipy.stats import boxcox\n\n# EDA\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Preprocessing\nimport category_encoders as ce","13ef77b1":"# CONSTANTS\nRANDOM_STATE = 42","a43fb83b":"input_path = \"\/kaggle\/input\/house-prices-advanced-regression-techniques\/\"","472f0118":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy.stats import norm\nfrom scipy.stats import boxcox\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n\nclass EDA:\n    \n    df = np.nan\n    df_desc = np.nan\n    target = np.nan\n    analysis = pd.DataFrame(columns=[\"column\",\"key\",\"value\"]) # dataframe with all the actions to be taken for each column\n    potential_features = {} # dictionary with potential features that we may have to create\n    collinear = pd.DataFrame() # a dataframe that will include potential multicollinear columns to be droped.\n    \n    def __init__(self, df, target, category_sensibility=10, forced_number=[], forced_binary=[], forced_category=[], to_excel=False):\n        '''\n        init EDA\n        Class that allow a quick analysis of the dataset variables.\n        Parameters:\n        df: dataset to analyze\n        target: name of the target column\n        category_sensibility: number of unique values below which a numerical variable will be analyzed as a category\n        forced_category: list of columns that we want to be treated as category even though they may not look like\n        forced_binary: list of columns that we want to be treated as binary\n        '''\n\n        # variables locales\n        MAX_UNIQUE_VALUES_PRINTED = 15 # Maximum number of values to be ploted. For large categories.\n\n        # Copy the original info to the class\n        self.df = df.copy()\n        self.target = target\n        \n        # We will create a descriptive dataset that will help us do the analysis\n        df_desc = df.describe(include=\"all\").T\n\n        # Create a number of vars that will describe each column --> WARNING: it is necessary to sort them later in the code. If not, they will not show.\n        df_desc[\"type\"] = np.nan\n        df_desc[\"type_isforced\"] = np.nan\n        df_desc[\"count\"] = np.nan\n        df_desc[\"nulls\"] = np.nan\n        df_desc[\"nulls_perc\"] = np.nan\n        df_desc[\"unique\"] = np.nan\n        _s = pd.Series(np.nan, dtype='object')\n        df_desc[\"unique_values\"] = _s\n        _s = pd.Series(np.nan, dtype='object')\n        df_desc[\"outliers\"] = _s\n        df_desc[\"has_outliers\"] = np.nan\n        df_desc[\"mode\"] = np.nan\n        df_desc[\"freq\"] = np.nan\n        df_desc[\"freq_perc\"] = np.nan\n        df_desc[\"skew\"] = np.nan\n\n\n        # Loop throught the dataset columns\n        for column in df_desc.index:        \n            # common descriptors\n            df_desc.loc[column,\"dtype\"] = df[column].dtype\n            df_desc.loc[column,\"count\"] = df[column].count()\n            df_desc.loc[column,\"nulls\"] = df[df[column].isna()][column].size\n            df_desc.loc[column,\"nulls_perc\"] = round(df_desc.loc[column,\"nulls\"]\/df[column].size,2)\n            df_desc.loc[column,\"unique\"] = df[column].nunique()\n\n            if df_desc.loc[column,\"unique\"] <= MAX_UNIQUE_VALUES_PRINTED: \n                df_desc.at[column,\"unique_values\"] = list(df[column].unique())\n            else:\n                df_desc.at[column,\"unique_values\"] = []\n\n            # Depeding on the DATATYPE\n            # FIRST, we make sure this is not a FORCED NUMBER\n            if (column in forced_number):\n                # Type of data\n                df_desc.loc[column,\"type\"] = \"number\"\n                df_desc.loc[column,\"type_isforced\"] = 1\n\n                # Check for outliers\n                bounds, df_desc.loc[column,\"has_outliers\"] = self.outliers(column, type=\"IQR\")\n                df_desc.at[column,\"outliers\"] = bounds\n                # skweness\n                df_desc.loc[column,\"skew\"] = df[column].skew()\n            # SECOND, we make sure this is not a forced FORCED CATEGORY\n            elif (column in forced_category):\n                # Tyep of data\n                df_desc.loc[column,\"type\"] = \"category\"\n                df_desc.loc[column,\"type_isforced\"] = 1\n\n                if (df[column].dtype==\"int\") |  (df[column].dtype==\"float\"):\n                    df_desc.loc[column,\"min\"] = df[column].min()\n                    df_desc.loc[column,\"max\"] = df[column].max()\n\n                # Modes and frequencies\n                _s_modefreq = df[column].value_counts().sort_values(ascending=False)\n                if (_s_modefreq.size < df[column].size): \n                    df_desc.loc[column,\"mode\"] = _s_modefreq.index[0]\n                    df_desc.loc[column,\"freq\"] = _s_modefreq.iloc[0]\n                    df_desc.loc[column,\"freq_perc\"] = round(_s_modefreq.iloc[0]\/df_desc.loc[column,\"count\"],2)\n            # THIRD, we make sure this is not a FORCED BINARY\n            elif (column in forced_binary):\n                # Type of data\n                df_desc.loc[column,\"type\"] = \"binary\"\n                df_desc.loc[column,\"type_isforced\"] = 1\n\n            # FOURTH INT OR FLOAT (and not forced, in case)\n            elif (((df[column].dtype == \"int\") | (df[column].dtype == \"float\")) & (column not in forced_category) & (column not in forced_binary)):\n\n                # Is it numeric , binary or categorical\n                if ((df_desc.loc[column,\"nulls\"]==0) & (df_desc.loc[column,\"unique\"]==2) & (0 in list(df_desc.loc[column,\"unique_values\"]))):\n                    df_desc.loc[column,\"type\"] = \"binary\"\n                    df_desc.loc[column,\"type_isforced\"] = -1\n                else:\n                    if ((df_desc.loc[column,\"nulls\"]>0) & (df_desc.loc[column,\"unique\"]==2) & (0 in list(df_desc.loc[column,\"unique_values\"]))):\n                        df_desc.loc[column,\"type\"] = \"binary\"\n                        df_desc.loc[column,\"type_isforced\"] = -1\n                    else:\n                        if ((df_desc.loc[column,\"unique\"]<=category_sensibility)):\n                            df_desc.loc[column,\"type\"] = \"category\"\n                            df_desc.loc[column,\"type_isforced\"] = -1\n                        else:\n                            df_desc.loc[column,\"type\"] = \"number\"\n                            df_desc.loc[column,\"type_isforced\"] = 0\n                            # Outliers\n                            bounds, df_desc.loc[column,\"has_outliers\"] = self.outliers(column, type=\"IQR\")\n                            df_desc.at[column,\"outliers\"] = bounds\n                            # skweness\n                            df_desc.loc[column,\"skew\"] = df[column].skew()\n\n                # Modes and frequencies only if category\n                if ((df_desc.loc[column,\"type\"] == \"category\") | (df_desc.loc[column,\"type\"] == \"binary\")):\n                    _s_modefreq = df[column].value_counts().sort_values(ascending=False)\n                    # Si la serie de frecuencias es m\u00e1s peque\u00f1a que la serie de la columna, entonces tiene algo de sentido informarlo\n                    if (_s_modefreq.size < df[column].size): \n                        df_desc.loc[column,\"mode\"] = _s_modefreq.index[0]\n                        df_desc.loc[column,\"freq\"] = _s_modefreq.iloc[0]\n                        df_desc.loc[column,\"freq_perc\"] = round(_s_modefreq.iloc[0]\/df_desc.loc[column,\"count\"],2)\n            # FIFTH, objects\n            elif df[column].dtype == \"object\":\n                # Type of data\n                df_desc.loc[column,\"type\"] = \"category\"\n                df_desc.loc[column,\"type_isforced\"] = 0\n\n                # Modes and frequencies\n                if (df_desc.loc[column,\"type\"] == \"category\"):\n                    _s_modefreq = df[column].value_counts().sort_values(ascending=False)\n                    if ((_s_modefreq.size < df[column].size) & (_s_modefreq.size>0)): # we include >0. If there are no unique values (all nulls) it would crash\n                        df_desc.loc[column,\"mode\"] = _s_modefreq.index[0]\n                        df_desc.loc[column,\"freq\"] = _s_modefreq.iloc[0]\n                        df_desc.loc[column,\"freq_perc\"] = round(_s_modefreq.iloc[0]\/df_desc.loc[column,\"count\"],2)\n            elif df[column].dtype == \"bool\":\n                # Type of data\n                df_desc.loc[column,\"type\"] = \"binary\"\n                df_desc.loc[column,\"type_isforced\"] = 1\n\n                _s_modefreq = df[column].value_counts().sort_values(ascending=False)\n                if ((_s_modefreq.size < df[column].size) & (_s_modefreq.size>0)): # # we include >0. If there are no unique values (all nulls) it would crash\n                    df_desc.loc[column,\"mode\"] = _s_modefreq.index[0]\n                    df_desc.loc[column,\"freq\"] = _s_modefreq.iloc[0]\n                    df_desc.loc[column,\"freq_perc\"] = round(_s_modefreq.iloc[0]\/df_desc.loc[column,\"count\"],2)\n            else:\n                print(\"******************************* columna\", column, \"es de tipo\", df[column].dtype)\n\n        # Sort the columns for better comprehension\n        columns = [\n            'dtype',\n            'type',\n            'type_isforced', \n            'count', \n            'nulls', \n            'nulls_perc', \n            'unique', \n            'unique_values',\n            'outliers', \n            'has_outliers',\n            'skew',\n            'mean', \n            'std', \n            'min', \n            '25%', \n            '50%', \n            '75%', \n            'max', \n            'mode', \n            'freq', \n            'freq_perc'\n        ]\n        df_desc = df_desc[columns]\n\n        # Excel generation if needed\n        if to_excel: \n            df_desc.insert(loc=0,column=\"NOTES\",value=np.nan)\n            df_desc.insert(loc=1,column=\"DROP\",value=np.nan)\n            df_desc.insert(loc=2,column=\"ACTIONS\",value=np.nan)\n            df_desc.to_excel(\"df_desc.xlsx\")\n            df_desc.drop([\"NOTES\",\"DROP\",\"ACTIONS\"], axis=1, inplace=True)\n\n        # Update self\n        self.df_desc = df_desc.copy()\n        del df_desc\n\n    def outliers(self, column, type='IQR'):\n        '''\n        OUTLIERS\n        Method that returns outliers thresholds\n        Par\u00e1metros\n        column: column name to analyze\n        type: outliers calculation (IQR or stdev)\n        \n        Returns\n        [lower_outbound, upper_outbound]: lower threshold, upper threshold\n        True if outliers, False if no\n        '''\n\n        if 'IQR':\n            quantile25 = self.df[column].quantile(0.25)\n            quantile75 = self.df[column].quantile(0.75)\n            IQR = quantile75-quantile25\n            upper_bound = quantile75 + 1.5*IQR\n            lower_bound = quantile25 - 1.5*IQR\n        else: #std\n            mean = self.df[column].mean()\n            std = self.df[column].std()\n            margin = 3 * std\n            lower_bound = mean - margin\n            upper_bound = mean + margin\n\n        has_outliers = False\n        if ((self.df[self.df[column]<lower_bound][column].count()>0) | (self.df[self.df[column]>upper_bound][column].count()>0)): \n            has_outliers = True\n            bounds = [lower_bound, upper_bound]\n        else:\n            bounds = np.nan\n\n        return bounds, has_outliers\n    \n    def univar(self, column, log=False, sort=\"weight\", fillna=np.nan):\n        '''\n        uni\n        Function to do a univariant analysis of the corresponding column of the dataset\n        \n        Parameters\n        column: column to analyze\n        log: for logistic y scale\n        sort: weight (default) or relevance (target)\n        fillna: value to replace nulls in categorical descriptions (if wanted)\n    \n        Returns\n        Variable visualization\n        '''\n        MAX_CATEGORIES_TO_PLOT = 300\n        MIN_DATA_TO_NODE = 400\n\n        l_data = [\"dtype\",\"type\",\"type_isforced\",\"count\",\"nulls\",\"nulls_perc\"]\n        # We add the different descriptors depending on the type of data\n        if (self.df_desc.loc[column,\"type\"]==\"binary\"):\n            l_data.extend([\"mean\"])\n        elif (self.df_desc.loc[column,\"type\"]==\"category\"):\n            l_data.extend([\"unique\",\"min\",\"max\",\"mode\",\"freq\",\"freq_perc\"])\n        else: #number\n            l_data.extend([\"mean\",\"std\",\"min\",\"25%\",\"50%\",\"75%\",\"max\",\"mode\",\"freq\",\"freq_perc\"])\n        display(HTML(pd.DataFrame(self.df_desc.loc[column,l_data]).T.to_html()))\n\n        # crosstable when column and target are category\/binary\n        if ((self.df_desc.loc[column,\"type\"]!=\"number\") & (self.df_desc.loc[self.target,\"type\"]!=\"number\")):\n            \n            display(HTML(\"********** <b>Valores \u00fanicos vs. Target (not nulls)<\/b> *************\"))\n            _df = pd.crosstab(self.df[column], self.df[self.target], margins= True, margins_name='Count')\n            _df.sort_values(by=\"Count\", ascending=False, inplace=True)\n            _df.insert(0,\"%Weight\",value=0)\n            _df.insert(1,\"%Positive\",value=0)\n            _df[\"%Weight\"] = round((_df.iloc[:,1]+_df.iloc[:,2])\/(self.df[column].value_counts().sum()),2)\n            _df[\"%Positive\"] = round(_df.iloc[:,2]\/_df.iloc[:,4],2)\n            _df = _df.reindex(columns=list(_df.columns[[-1]].append(_df.columns[:-1])))\n            display(HTML(_df.to_html()))\n        \n        # if COLUMN is NUMBER\n        if (self.df_desc.loc[column,\"type\"]==\"number\"):\n            # Univariant analysis\n            fig, axes = plt.subplots(3 if column != self.target else 2, 1, figsize=(10,7), sharex=True)\n            fig.tight_layout(pad=3)\n            plt.xticks(rotation=90)\n            axes[0].set_title(\"{} distribution (skw: {})\".format(column,round(self.df[column].skew(),2)))\n            axes[0].grid(True)\n            axes[0].margins(0.2)\n            try:\n                sns.distplot(\n                    self.df[column], \n                    ax = axes[0], \n                    fit = norm, \n                )\n            except: pass\n            \n            axes[1].set_title(\"{} distribution\".format(column))\n            axes[1].grid(True)\n            axes[1].margins(0.2)\n            sns.boxplot(\n                x = self.df[column], \n                ax = axes[1],  \n            )\n            \n            # Bivariant analysis, only if not target\n            if column != self.target:\n                # if TARGET is NUMBER\n                if self.df_desc.loc[self.target,\"type\"]==\"number\":\n                    axes[2].set_title(\"{} vs {}\".format(column, self.target))\n                    axes[2].grid(True)\n                    axes[2].margins(0.2)\n                    sns.regplot(\n                        x = self.df[column],\n                        y = self.df[self.target], \n                        ax = axes[2], \n                    )\n            \n                # if TARGET is BINARY or CATEGORY\n                if self.df_desc.loc[self.target,\"type\"]!=\"number\":\n\n                    # Graficamos s\u00f3lo con los datos que tienen suficiente volumen\n                    _pivot = pd.pivot_table(self.df, index=[column], values=self.target, aggfunc=[np.mean, len])\n                    _pivot.reset_index(inplace=True)\n                    _pivot.columns = _pivot.columns.get_level_values(0)\n                    _pivot = _pivot[_pivot[\"len\"]>=MIN_DATA_TO_NODE] #Eliminamos los registros que no aportan volumen suficiente\n\n                    plt.figure(figsize=(15,3))\n                    plt.xticks(rotation=90)\n                    plt.title(\"{} (order) vs % {} with more than {} records\".format(column, self.target, MIN_DATA_TO_NODE))\n                    _pivot.sort_values(by=\"mean\", ascending=False, inplace=True)\n                    sns.barplot(\n                        x=_pivot[column],\n                        y=_pivot[\"mean\"],\n                        order=_pivot[column]\n                    )\n                    plt.show()\n\n                    plt.figure(figsize=(15,3))\n                    plt.xticks(rotation=90)\n                    plt.title(\"{} vs % {} (order) with more than {} records\".format(column, self.target, MIN_DATA_TO_NODE))\n                    _pivot.sort_values(by=column, ascending=False, inplace=True)\n                    sns.barplot(\n                        x=_pivot[column],\n                        y=_pivot[\"mean\"],\n                        order=pd.Series(_pivot[column].unique()).sort_values().to_list()\n                    )\n                    plt.show()\n\n                    plt.figure(figsize=(15,3))\n                    plt.xticks(rotation=90)\n                    plt.title(\"{} vs % {} with more than {} records (order)\".format(column, self.target, MIN_DATA_TO_NODE))\n                    _pivot.sort_values(by=\"len\", ascending=False, inplace=True)\n                    sns.barplot(\n                        x=_pivot[column],\n                        y=_pivot[\"mean\"],\n                        order=_pivot[column].to_list()\n                    )\n                    plt.show()\n                \n            plt.show()\n            \n            # if TARGET is NUMBER (this is part of the unvariant analysis)\n            if self.df_desc.loc[self.target,\"type\"]==\"number\":\n                # And we finally check the potential transformation to obtain a normal distribution\n                self.column_potential_transformation(column)\n                    \n        # if CATEGORY or BINARY\n        if (self.df_desc.loc[column,\"type\"]==\"category\") | (self.df_desc.loc[column,\"type\"]==\"binary\"):\n            fig, axes = plt.subplots(2, 1, figsize=(10,5), sharex=True)\n            fig.tight_layout(pad=3)\n            plt.xticks(rotation=90)\n            # Univariant analysis\n            # If the number of categories is too high we show a warning\n            num_categories = self.df[column].nunique()\n            if (num_categories >= MAX_CATEGORIES_TO_PLOT):\n                axes[0].text(0.5, 0.5, \"<b>WARNING: TOO MANY CATEGORIES TO PLOT<\/b>\", size=24, ha='center', va='center')\n            else:\n                s_ = self.df[column].fillna(fillna)\n                s_ = s_.value_counts().sort_values(ascending=False) \n                axes[0].set_title(\"{} distribution\".format(column))\n                axes[0].grid(True)\n                axes[0].margins(0.2)\n                sns.barplot(\n                    x = s_.index, \n                    y = s_, \n                    log = log,\n                    order = s_.index, \n                    ax = axes[0], \n                )\n                rects = axes[0].patches\n                labels = [str(int(np.round(100*i\/s_.sum(),0)))+\"%\" for i in s_]\n                for rect, label in zip(rects, labels):\n                    height = rect.get_height()\n                    axes[0].text(rect.get_x() + rect.get_width()\/2, height + 5, label, ha='center', va='bottom')\n            \n            # Bivariant analysis, only if not target\n            if column != self.target:\n                # vs NUMERIC TARGET\n                if self.df_desc.loc[self.target,\"type\"]==\"number\":\n                    axes[1].set_title(\"{} vs {} distribution\".format(column, self.target))\n                    axes[1].grid(True)\n                    axes[1].margins(0.2)\n                    sns.boxplot(\n                        x = self.df[column].fillna(fillna), \n                        y = self.df[self.target], \n                        order = s_.index, \n                        ax = axes[1], \n                    )\n                # vs BINARY or CATEGORY TARGET\n                # we can use positive ratios, easier to plot and understand\n                if self.df_desc.loc[self.target,\"type\"]!=\"number\":\n                    _df = pd.DataFrame(self.df.groupby(column)[self.target].mean()).reset_index()\n                    columns = _df.columns[:-1].tolist()\n                    columns.append(\"%\" + self.target)\n                    _df.columns = columns\n                    _df.sort_values(by=column, inplace=True)\n\n                    axes[1].set_title(\"{} (orden) vs % {} distribution\".format(column,self.target))\n                    axes[1].grid(True)\n                    axes[1].margins(0.2)\n                    sns.barplot(\n                        x=_df[column],\n                        y=_df[\"%\"+self.target],\n                        order=pd.Series(self.df[column].unique()).sort_values().to_list(), # Mismo orden que los anteriores\n                        ax = axes[1]\n                    )\n                plt.show()\n        \n        # To finish, we print the next column :-)\n        column_i = self.df.columns.tolist().index(column)\n        if (column_i+1) == len(self.df.columns): print(\"--> no more columns\")\n        if (column_i+1) < len(self.df.columns): print(\"--> next column ({} of {}): {}\".format(column_i+1+1, len(self.df.columns), self.df.columns[column_i+1]))\n\n    def multivar(self):\n        '''\n        multi\n        Function that plots the multivariant analysis of numerical columns in the dataset\n        \n        parameters\n        none\n        \n        returns\n        visualization of correlation matrix and vif values\n        '''\n        \n        # Correlation\n        corr_ = self.df.corr()\n\n        # Generate a mask for the upper triangle\n        mask = np.triu(np.ones_like(corr_, dtype=bool))\n\n        # Set up the matplotlib figure\n        f, ax = plt.subplots(figsize=(25, 25))\n\n        # Generate a custom diverging colormap\n        #sns.set_theme(style=\"white\")\n        cmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n        # Draw the heatmap with the mask and correct aspect ratio\n        sns.heatmap(corr_, annot=True, mask=mask, cmap=cmap, vmax=1, center=0,\n                  square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n        plt.show()\n        \n        #Variance Inflation Factor, only for numeric variables\n        numeric_columns = self.df_desc[self.df_desc[\"type\"]==\"number\"].index.to_list()\n        numeric_columns.remove(self.target) # we do not want the target to be part of the VIF\n        vif = pd.DataFrame()\n        vif[\"variables\"] = numeric_columns\n        vif[\"VIF\"] = [variance_inflation_factor(self.df[numeric_columns].dropna().values, i) for i in range(len(numeric_columns))]\n        display(vif)\n        \n        # We store the columns that are potentially multicollinear\n        self.collinear = vif[vif[\"VIF\"]>=5.0]\n        \n        \n    def column_boxcox(self, column):\n        xt, lmdba = boxcox(self.df[column])\n        plt.title(\"{} boxcox distribution\".format(column))\n        plt.grid(True)\n        plt.margins(0.2)\n        sns.distplot(\n            xt, \n            fit = norm, \n        )\n        plt.show()\n        print(\"Lambda: {}\".format(lmdba))\n    \n    \n    def column_potential_transformation(self, column):\n        s = self.df[column].dropna() # we must get rid of nulls, as we will be performing some calculations\n        \n        has_nulls = len(s)<len(self.df[column])\n        has_neg = s.min()<0\n        has_zeros = len(s[s==0])>0\n        \n        plt.figure(figsize=(15,5))\n        plt.tight_layout(pad=0.5)\n        \n        plt.subplot(2, 3, 1)\n        try:\n            s_sqr_ = s**0.5\n            sns.distplot(s_sqr_, fit = norm)\n        except: pass\n        plt.title('Square Root (swk: ' + str(round(s_sqr_.skew(),2)) + ')')\n        plt.subplot(2, 3, 4)\n        sns.boxplot(s_sqr_)\n\n        plt.subplot(2, 3, 2)\n        try: \n            s_rec_ = 1\/s\n            sns.distplot(s_rec_, fit = norm)\n        except: pass\n        plt.title('Reciproque (skw: ' + str(round(s_rec_.skew(),2)) + ')')\n        plt.subplot(2, 3, 5)\n        sns.boxplot(s_rec_)\n        \n\n        # For the log, let's check if data is positive\n        # If negative, nothing can be done. If zero values, we can log(x+1)\n        plt.subplot(2, 3, 3)\n        s_log_ = np.nan\n        if not has_zeros and not has_neg:\n            s_log_ = np.log(s)\n        elif has_zeros and not has_neg:\n            s_log_ = np.log(s + 1)\n        try:\n            sns.distplot(s_log_, fit = norm)\n        except: pass\n        plt.title('Logaritmic (skw: ' + str(round(s_log_.skew(),2)) + ')')\n        plt.subplot(2, 3, 6)\n        sns.boxplot(s_log_)\n\n        plt.show()\n    \n    def add_analysis(self, column, key, value=np.nan):\n        '''\n        add_analysis\n        parameters:\n        column: name of the dataset's column\n        key: what is the analysis about\n        value: the value of the kind\n        '''\n        self.analysis = self.analysis.append({'column':column, 'key':key, 'value':value}, ignore_index=True)\n    \n    def add_potential_feature(self, feature, description=\"\"):\n        '''\n        add_potential_feature\n        parameters:\n        feature: name of the potential feature\n        description: string with the description of this feature\n        '''\n        self.potential_features[feature] = {'description':description}","3348d709":"data = pd.read_csv(input_path + \"train.csv\")\nsubmission = pd.read_csv(input_path + \"test.csv\")","ba82573e":"# We are going to use the Id as de index of the dataset\ndata.set_index(\"Id\", inplace=True)\nsubmission.set_index(\"Id\", inplace=True)","8cabfddf":"cEDA = EDA(\n    data, \n    target=\"SalePrice\", \n    category_sensibility=30, \n    forced_number = [\"3SsnPorch\",\"PoolArea\",\"MiscVal\"], \n)","fdfb8474":"cEDA.univar(\"SalePrice\")","028394ba":"cEDA.df[\"SalePrice\"] = np.log(cEDA.df[\"SalePrice\"]) # no zero values\ncEDA = EDA(\n    cEDA.df, \n    target=\"SalePrice\", \n    category_sensibility=30, \n    forced_number = [\"3SsnPorch\",\"PoolArea\",\"MiscVal\"], \n)\ncEDA.univar(\"SalePrice\")","5882d6f1":"cEDA.add_analysis(\"SalePrice\", \"log\")","203b34c9":"cEDA.univar(\"MSSubClass\")","636d918e":"cEDA.add_analysis(\"MSSubClass\", \"description\", \"3 categories sum up to more than 67% of records (20, 60, 50). 60 and 120 are the ones with higher prices. Non ordinal categories.\")\ncEDA.add_analysis(\"MSSubClass\", \"ohe\", \"\")","ea89324a":"cEDA.univar(\"MSZoning\")","934ea98f":"cEDA.add_analysis(\"MSZoning\", \"description\", \"2 categories sum up to more than 94% of records. RL and FV show slightly higher prices. Non ordinal categories\")\ncEDA.add_analysis(\"MSZoning\", \"ohe\", \"\")","e32041bc":"cEDA.univar(\"LotFrontage\")","fdbd0d39":"cEDA.add_analysis(\"LotFrontage\", \"description\", \"Correlation with target. Correct skweeness. Drop extreme outlier.\")\ncEDA.add_analysis(\"LotFrontage\", \"sqrt\", \"\")\ncEDA.add_analysis(\"LotFrontage\", \"outliers\", \">14\")","105d4c33":"cEDA.univar(\"LotArea\")","fcaa7028":"cEDA.add_analysis(\"LotArea\", \"description\", \"Correlation with target. Distribution can be 'normalized', and there are some values that are extreme outliers.\")\ncEDA.add_analysis(\"LotArea\", \"sqrt\", \"\")\ncEDA.add_analysis(\"LotArea\", \"outliers\", \">300\")","2befdd82":"cEDA.add_potential_feature(\"LotFrontvsArea\",\"Ratio between the frontage size and lot area. If included, LotFrontage should be DEL\")","19d6aef1":"cEDA.univar(\"Street\")","bd1c0fb6":"cEDA.add_analysis(\"Street\", \"description\", \"Only one category. All streets are paved.\")\ncEDA.add_analysis(\"Street\", \"drop\", \"\")","dfe14688":"cEDA.univar(\"Alley\")","7e60dd89":"cEDA.add_analysis(\"Alley\", \"description\", \"Paved alleys have higher house prices\")\ncEDA.add_analysis(\"Alley\", \"ohe\")","0d84481c":"cEDA.univar(\"LotShape\")","120737aa":"cEDA.add_analysis(\"LotShape\", \"description\", \"2 categories sum up more than 96% of the records. Regular lotshapes seem to have slightly lower prices.\")\ncEDA.add_analysis(\"LotShape\", \"ohe\")","3ecd00d4":"cEDA.univar(\"LandContour\")","0fe0d93d":"cEDA.add_analysis(\"LandContour\", \"description\", \"1 category has 90% of the data. Small variation in prices depending on the category.\")\ncEDA.add_analysis(\"LandContour\", \"ohe\")","fb8ec230":"cEDA.univar(\"Utilities\")","770df266":"cEDA.add_analysis(\"Utilities\", \"drop\", \"Only one category\")","13f70ffe":"cEDA.univar(\"LotConfig\")","e1b6568b":"cEDA.add_analysis(\"LotConfig\", \"description\", \"2 categories sum up more than 90% of the data.\")\ncEDA.add_analysis(\"LotConfig\", \"ohe\")","97e2adba":"cEDA.univar(\"LandSlope\")","81be5188":"cEDA.add_analysis(\"LandSlope\",\"description\",\"Almost all records (>95%) are Gtl.\")\ncEDA.add_analysis(\"LandSlope\",\"ohe\")","2728c5f4":"cEDA.univar(\"Neighborhood\")","9198fbf6":"cEDA.add_analysis(\"Neighborhood\",\"description\",\"distributed categories. 2 of them gather 25% of the data. NridgHT, NonRidge, StoneBr and Veenker, have higher prices.\")\ncEDA.add_analysis(\"Neighborhood\",\"ohe\")","e3d835e0":"cEDA.univar(\"Condition1\")","b718c151":"cEDA.add_analysis(\"Condition1\",\"description\",\"2 categories sum up 92% of the data. Does not look relevant at all.\")","fa91ae39":"cEDA.univar(\"Condition2\")","b1e48276":"cEDA.add_analysis(\"Condition2\",\"description\",\"1 category has 99% of the data. Does not look relevant at all.\")\n","77cbdfa1":"cEDA.add_potential_feature(\"Condition\",\"mixture of COndition1 and Condition2, as they are two same charactersitics that can be encoded the same way\")\ncEDA.add_analysis(\"Condition1\",\"drop\")\ncEDA.add_analysis(\"Condition2\",\"drop\")","904843df":"cEDA.univar(\"BldgType\")","49fbe341":"cEDA.add_analysis(\"BldgType\",\"description\",\"1 category has 84% of the data (1 family buildings). Both 1Fam and TwnhsE seem to show slightly hier prices.\")\ncEDA.add_analysis(\"BldgType\",\"ohe\")","97065910":"cEDA.univar(\"HouseStyle\")","bd70a1d5":"cEDA.add_analysis(\"HouseStyle\",\"description\",\"1 category has 84% of the data (1 family buildings). Both 1Fam and TwnhsE seem to show slightly hier prices.\")\ncEDA.add_analysis(\"HouseStyle\",\"ohe\")","7bf68b6d":"cEDA.univar(\"OverallQual\")","936509a1":"cEDA.add_analysis(\"OverallQual\",\"description\",\"Positive correlation with price\")","5859e6eb":"cEDA.univar(\"OverallCond\")","71603d84":"cEDA.add_analysis(\"OverallCond\",\"description\",\"Potential correlation with OverallQual - multicollinearity?\")","67949d58":"cEDA.univar(\"YearBuilt\")","ea83b9bd":"# Let's see if there are inconsistencies between YearBuilt, YearRemodAdd and YrSold\ndisplay(cEDA.df[cEDA.df[\"YearBuilt\"]>cEDA.df[\"YearRemodAdd\"]][[\"YearBuilt\",\"YearRemodAdd\",\"YrSold\"]])\ndisplay(cEDA.df[cEDA.df[\"YearBuilt\"]>cEDA.df[\"YrSold\"]][[\"YearBuilt\",\"YearRemodAdd\",\"YrSold\"]])\ndisplay(cEDA.df[cEDA.df[\"YearRemodAdd\"]>cEDA.df[\"YrSold\"]][[\"YearBuilt\",\"YearRemodAdd\",\"YrSold\"]])","c40c214d":"cEDA.add_analysis(\"YearBuilt\",\"describe\",\"Highly skweed distribution, to the left. Potential outliers (houses <1890 aprox). Positive correlation with SalePrice.\")\ncEDA.add_analysis(\"YearBuilt\",\"feature\",\"YearsOld\")\ncEDA.add_analysis(\"YearBuilt\",\"drop_id\",\"524\")\ncEDA.add_analysis(\"YearBuilt\",\"drop\",\"The year itself offers no extra info. Wait until the end to drop, as it may be needed for other calculations.\")","14955700":"cEDA.univar(\"YearRemodAdd\")","c853dd07":"cEDA.add_analysis(\"YearRemodAdd\",\"description\",\"Same distribution as YearBuilt.\")\ncEDA.add_analysis(\"YearRemodAdd\",\"feature\",\"RemodRatio. Ratio of the house being remodeled. The smaller, the better. YearsRemodSinceBuilt\/YearsOld\")\ncEDA.add_analysis(\"YearRemodAdd\",\"drop\")","c7d15021":"cEDA.univar(\"RoofStyle\")","8d82c629":"cEDA.add_analysis(\"RoofStyle\",\"description\",\"2 categories add 98% of the records. Not a relevant variable.\")\ncEDA.add_analysis(\"RoofStyle\",\"ohe\")","7d9d2473":"cEDA.univar(\"RoofMatl\")","1246f8e1":"cEDA.add_analysis(\"RoofMatl\",\"description\",\"1 category accounts for 98% of the records. Not a relevant variable.\")\ncEDA.add_analysis(\"RoofMatl\",\"ohe\")","b5685fcc":"cEDA.univar(\"Exterior1st\")","bb67730c":"cEDA.add_analysis(\"Exterior1st\",\"description\",\"4 categories add up to 80% of the records. VinylSd, CementBd and BrkFace tend to show slightly higher prices.\")","7a7b80f1":"cEDA.univar(\"Exterior2nd\")","c259c862":"cEDA.add_analysis(\"Exterior2nd\",\"description\",\"4 categories add up to 80% of the records. VinylSd, CementBd and BrkFace tend to show slightly higher prices. Very similiar behaviour as Exterior1st --> Potential correlation - C2 test.\")\ncEDA.add_analysis(\"Exterior2nd\",\"replace\",\"{'Wd Shng':'Wd Sdng','Brk Cmn':'BrkComm'}\")","a6434495":"cEDA.add_analysis(\"Exterior1st\",\"ohe_multi\",\"Mixture of Exterior1st and Exterior1st, as they are two same charactersitics that can be encoded the same way\")\ncEDA.add_analysis(\"Exterior1st\",\"drop\",\"ohe together with Exterior2nd\")\ncEDA.add_analysis(\"Exterior2nd\",\"drop\",\"ohe together with Exterior1st\")","7de73117":"cEDA.univar(\"MasVnrType\")","023b57e5":"# Let's check the null values. The masonry veneer may be related to the exterior materials. Therefore, we may be able to guess the null values from there.\ndisplay(cEDA.df[cEDA.df[\"MasVnrType\"].isna()][[\"MasVnrType\",\"MasVnrArea\",\"Exterior1st\"]])\ndisplay(pd.crosstab(cEDA.df[\"MasVnrType\"], cEDA.df[\"Exterior1st\"]))","f2ec5c71":"cEDA.add_analysis(\"MasVnrType\",\"description\",\"Stone veeners show higher prices. Null values will be filled with the mode 'None'.\")\ncEDA.add_analysis(\"MasVnrType\",\"fillna\",\"None\")\ncEDA.add_analysis(\"MasVnrType\",\"ohe\",\"\")","f15cf8e5":"cEDA.univar(\"MasVnrArea\")","1fc2f7c5":"# In this case, we see that there are a lot of 0 values, those corresponding to the MasVnrType = 'None'\n# The nulls correspond to the same records as MasVnrType. \ndisplay(cEDA.df[cEDA.df[\"MasVnrArea\"].isnull()][[\"MasVnrType\",\"MasVnrArea\",\"SalePrice\"]])\ndisplay(cEDA.df[cEDA.df[\"MasVnrType\"].isnull()][\"MasVnrArea\"].sum())","32f10823":"cEDA.add_analysis(\"MasVnrArea\",\"description\",\"All 'None' MasVnrTypes have areas of 0\")\ncEDA.add_analysis(\"MasVnrArea\",\"fillna\",0)\ncEDA.add_analysis(\"MasVnrArea\",\"outliers\",\">1000\")","6638ec38":"cEDA.univar(\"ExterQual\", fillna=\"null\")","35686c65":"cEDA.add_analysis(\"ExterQual\",\"description\",\"the higher the quality, the more expensive the price. Positive correlation, although more blurry when high evaluation is done.\")\ncEDA.add_analysis(\"ExterQual\",\"oe\",\"[('Ex',5), ('Gd',4), ('TA',3), ('Fa',2), ('Po',1)]\")","0f8f7d22":"cEDA.univar(\"ExterCond\", fillna=\"null\")","4e8a4305":"cEDA.add_analysis(\"ExterCond\",\"description\",\"the better the condition, the more expensive the price. Positive correlation, but lesser than in the case of ExterQual.\")\ncEDA.add_analysis(\"ExterCond\",\"oe\",\"[('Ex',5), ('Gd',4), ('TA',3), ('Fa',2), ('Po',1)]\")","ec057a5e":"cEDA.univar(\"Foundation\")","f0dde441":"cEDA.add_analysis(\"Foundation\",\"description\",\"Foundations built with Concrete show higher prices. 2 type of materials are the most common, Concrete and CBlock.\")\ncEDA.add_analysis(\"Foundation\",\"ohe\")","b1f9b79d":"cEDA.univar(\"BsmtQual\", fillna=\"No\")","d3f221ed":"# We have a cardinal category for the basement quality (height). There are some null values. Let's see what they tell us.\ncEDA.df[cEDA.df[\"BsmtQual\"].isnull()][[\"BsmtQual\",\"BsmtCond\",\"SalePrice\"]]","9904b040":"cEDA.add_analysis(\"BsmtQual\",\"description\",\"The better the quality of the basement, the higher the price. Nulls will be filled with the mode TA.\")\ncEDA.add_analysis(\"BsmtQual\",\"fillna\",\"No\")\ncEDA.add_analysis(\"BsmtQual\",\"oe\",\"[('Ex',5), ('Gd',4), ('TA',3), ('Fa',2), ('Po',1), ('No', 0)]\")","7bfc4716":"cEDA.univar(\"BsmtCond\")","373ac0a3":"cEDA.add_analysis(\"BsmtCond\",\"description\",\"Almost all values are tagged as TA (mean ). Nulls will be filled with 'Uninformed' label. Same behaviour as BsmtQual, but concentrated in one category.\")\ncEDA.add_analysis(\"BsmtCond\",\"fillna\",\"No\")\ncEDA.add_analysis(\"BsmtCond\",\"oe\",\"[('Ex',5), ('Gd',4), ('TA',3), ('Fa',2), ('Po',1), ('No', 0)]\")","6fcc37e5":"cEDA.univar(\"BsmtExposure\", fillna=\"NoBsmt\")","e85703e4":"cEDA.add_analysis(\"BsmtExposure\",\"description (Uninf, No, Mn, Av, Gd)\",\"Almost all values are tagged as No (no exposure) (mean ). Nulls will be filled with 'Uninformed' label. The better the exposure, the higher the price.\")\ncEDA.add_analysis(\"BsmtExposure\",\"fillna\",\"NoBsmt\")\ncEDA.add_analysis(\"BsmtExposure\",\"oe\",\"[('Gd',4), ('Av',3), ('Mn',2), ('No',1), ('NoBsmt', 0)]\")","3ce4053f":"cEDA.univar(\"BsmtFinType1\")","1c4dde78":"cEDA.add_analysis(\"BsmtFinType1\",\"fillna\",\"NoBsmt\")\ncEDA.add_analysis(\"BsmtFinType1\",\"oe\",\"[('GLQ',6), ('ALQ',5), ('BLQ',4), ('Rec',3), ('LwQ',2), ('Unf',1), ('NoBsmt', 0)]\")","94119f18":"cEDA.univar(\"BsmtFinSF1\")","53009be6":"cEDA.add_analysis(\"BsmtFinSF1\",\"description\",\"Positive correlation with Price. Non-normal distribution.\")\ncEDA.add_analysis(\"BsmtFinSF1\",\"sqrt\")\ncEDA.add_analysis(\"BsmtFinSF1\",\"outliers\",\">60\")","97213526":"cEDA.univar(\"BsmtFinType2\")","5506030d":"cEDA.add_analysis(\"BsmtFinType2\",\"description\",\"1 category includes 88% of the data. Probably not relevant.\")\ncEDA.add_analysis(\"BsmtFinType2\",\"fillna\",\"NoBsmt\")\ncEDA.add_analysis(\"BsmtFinType2\",\"oe\",\"[('GLQ',6), ('ALQ',5), ('BLQ',4), ('Rec',3), ('LwQ',2), ('Unf',1), ('NoBsmt', 0)]\")","9160790e":"cEDA.univar(\"BsmtFinSF2\")","135f3601":"cEDA.add_analysis(\"BsmtFinSF2\",\"description\",\"No correlation with Price. Non-normal distribution, not transformable. Not a relevant variable.\")\ncEDA.add_analysis(\"BsmtFinSF2\",\"drop\")","3861adc7":"cEDA.univar(\"BsmtUnfSF\")","4b618497":"cEDA.add_analysis(\"BsmtUnfSF\",\"description\",\"Very small correlation with Price. Non-normal distribution due to high volumne of finished basements - 0 sf. Non relevant variable.\")","94a169b8":"cEDA.univar(\"TotalBsmtSF\")","04ee31c8":"cEDA.add_analysis(\"TotalBsmtSF\",\"description\",\"Relevant positive correlation.\")\ncEDA.add_analysis(\"TotalBsmtSF\",\"outliers\",\">6000\")","2c6ac84d":"s_ = cEDA.df[\"BsmtFinSF1\"] + cEDA.df[\"BsmtFinSF2\"] + cEDA.df[\"BsmtUnfSF\"] - cEDA.df[\"TotalBsmtSF\"]\ns_[s_>0]","5da46a39":"cEDA.add_analysis(\"BsmtFinSF1\",\"ratio\",\"TotalBsmtSF\")\ncEDA.add_analysis(\"BsmtFinSF2\",\"ratio\",\"TotalBsmtSF\")\ncEDA.add_analysis(\"BsmtPercUnfSF\",\"ratio\", \"TotalBsmtSF\")","830da7e4":"cEDA.univar(\"Heating\")","a22987c2":"cEDA.add_analysis(\"Heating\",\"description\",\"98% of data corresponds to GasA. Non relevant variable.\")\ncEDA.add_analysis(\"Heating\",\"ohe\")","765f0343":"cEDA.univar(\"HeatingQC\")","2493c94a":"cEDA.add_analysis(\"HeatingQC\",\"description\",\"Slightly positive correlation. Ordinal category.\")\ncEDA.add_analysis(\"HeatingQC\",\"oe\",\"[('Ex',5), ('Gd',4), ('TA',3), ('Fa',2), ('Po',1)]\")","51d2aa22":"cEDA.univar(\"CentralAir\")","cd805b94":"cEDA.add_analysis(\"CentralAir\",\"description\",\"Houses with CentralAir tend to show higher prices. Most houses include CentralAir.\")\ncEDA.add_analysis(\"CentralAir\",\"binary\",\"[('Y',1),('N',0)]\")","891e4a8c":"cEDA.univar(\"Electrical\", fillna=\"null\")","e2873916":"# Let's check the null value\ncEDA.df[cEDA.df[\"Electrical\"].isnull()][[\"YearBuilt\"]]","10c163ae":"# Let's see how other houses built in the same year  in terms of Electrical. \ncEDA.df[(cEDA.df[\"YearBuilt\"]==2006)][\"Electrical\"].value_counts()","33557d76":"# Let's see if there is a correlation between the year the house was built and the type of Electrical installation.\ncross_ = pd.crosstab(cEDA.df[\"Electrical\"],cEDA.df[\"YearBuilt\"])\nplt.figure(figsize=(7,5))\nsns.heatmap(cross_, cmap=\"Blues\")","77c35314":"cEDA.add_analysis(\"Electrical\",\"description\", \"91% of houses have SBrkr electrical system. Looks like not having this system may affect the house price. Currently, there is only one kind of Electrical being used (SBrkr).\")\ncEDA.add_analysis(\"Electrical\",\"fillna\",\"SBrkr\")\ncEDA.add_analysis(\"Electrical\",\"ohe\")\n","6c575870":"cEDA.univar(\"1stFlrSF\")","ffc61d74":"cEDA.add_analysis(\"1stFlrSF\",\"description\",\"Positive correlation with price. \")\ncEDA.add_analysis(\"1stFlrSF\",\"outliers\",\">4000\")\ncEDA.add_analysis(\"1stFlrSF\",\"ratio\",\"TotalSF\")","a1f21f19":"cEDA.univar(\"2ndFlrSF\")","375cf403":"cEDA.add_analysis(\"2ndFlrSF\",\"description\",\"Positive correlation with Price. The presence of just 1 floor houses alters the normal distribution of 2nd floor square feet. \")\ncEDA.add_analysis(\"2ndFlrSF\",\"outliers\",\">1800\")\ncEDA.add_analysis(\"2ndFlrSF\",\"ratio\",\"TotalSF\")\ncEDA.add_analysis(\"2ndFlrSF\",\"feature\",\"TotalSF = 1stFlrSF + 2ndFlrSF\")\ncEDA.add_analysis(\"2ndFlrSF\",\"feature\", \"NumFlrs = Number of floors (2 if 2ndFlrSF>0)\")","0e048ad4":"cEDA.univar(\"LowQualFinSF\")","17187489":"cEDA.add_analysis(\"LowQualFinSF\",\"description\",\"Non relevant variable. Although identified as categorical, it is really a numerical variable.\")\ncEDA.add_analysis(\"LowQualFinSF\",\"drop\")","541f5d63":"cEDA.univar(\"GrLivArea\")","918c2ce5":"cEDA.add_analysis(\"GrLivArea\",\"description\",\"Positive correlation with price. Potential correlation with other areas.\")\ncEDA.add_analysis(\"GrLivArea\",\"outliers\", \">4000\")\ncEDA.add_analysis(\"GrLivArea\",\"to_ratio\", \"TotalSF\")","da7ddc90":"cEDA.univar(\"BsmtFullBath\")","bb61a988":"cEDA.add_analysis(\"BsmtFullBath\",\"description\",\"Houses mainly have 0 or 1 full bathroom. If a bathroom exists, the price seems to be a little bit higher.\")","b45454f4":"cEDA.univar(\"BsmtHalfBath\")","679fc0e4":"cEDA.add_analysis(\"BsmtHalfBath\",\"description\",\"Non relevant variable. Only few houses have half bathroom in the basement, and there seems to be no difference in terms of price.\")","65a8bf77":"cEDA.univar(\"FullBath\")","a35e1960":"cEDA.add_analysis(\"FullBath\",\"description\",\"Positive correlation with price. More bathrooms, higher price.\")","a3d20db4":"cEDA.univar(\"HalfBath\")","84c04ee0":"cEDA.add_analysis(\"HalfBath\",\"description\",\"Having one helfbath seems to increase the price.\")","36841e3a":"cEDA.univar(\"BedroomAbvGr\")","7cdcd083":"cEDA.add_analysis(\"BedroomAbvGr\",\"description\",\"3, 2 and 4 bedrooms are the most common number of bedrooms. Prices tend to increase from 1 to 5 bedrooms.\")","892dcdbf":"cEDA.univar(\"KitchenAbvGr\")","371d5ee8":"cEDA.add_analysis(\"KitchenAbvGr\",\"description\",\"95% of houses have 1 kitchen. Having a second (or third) kitchen does not seem to have a positive impact in price.\")","ef6fd113":"cEDA.univar(\"KitchenQual\")","67dd7cc1":"cEDA.add_analysis(\"KitchenQual\",\"description\",\"The better the kitchen quality, the higher the price.\")\ncEDA.add_analysis(\"KitchenQual\",\"oe\",\"[('Ex',5),('Gd',4),('TA',3),('Fa',2),('Po',1)]\")","c207afb8":"cEDA.univar(\"TotRmsAbvGrd\")","32b9df5f":"cEDA.add_analysis(\"TotRmsAbvGrd\",\"description\",\"The more rooms, the higher the price.\")","044ae5c4":"cEDA.univar(\"Functional\")","e9289218":"cEDA.add_analysis(\"Functional\",\"description\",\"93% of data corresponds to Typ. Because of small category volumes, this variable is non relevant\")\ncEDA.add_analysis(\"Functional\",\"oe\",\"('Typ',8),('Min1',7),('Min2',6),('Mod',5),('Maj1',4),('Maj2',3),('Sev',2),('Sal',1)\")","31bd52fa":"cEDA.univar(\"Fireplaces\")","38389055":"cEDA.add_analysis(\"Fireplaces\",\"description\",\"Having at least one fireplace increases the house price.\")","48049853":"cEDA.univar(\"FireplaceQu\", fillna=\"NoFp\")","c64a57d3":"cEDA.add_analysis(\"FireplaceQu\",\"Fillna (No) | OrdinalEncoding (No, Po, Fa, TA, Gd, Ex)\",\"A better fireplace quality seems to impact on the price, although there is a big number of NaN\")\ncEDA.add_analysis(\"FireplaceQu\",\"fillna\",\"NoFp\")\ncEDA.add_analysis(\"FireplaceQu\",\"oe\",\"('Ex',5),('Gd',4),('TA',3),('Fa',2),('Po',1),('NoFp',0)\")","6445428b":"cEDA.univar(\"GarageType\", fillna=\"No\")","a06fc4db":"cEDA.add_analysis(\"GarageType\",\"description\",\"Having a attached or built-in basement seems to favour price\")\ncEDA.add_analysis(\"GarageType\",\"fillna\",\"NoGrg\")\ncEDA.add_analysis(\"GarageType\",\"ohe\")","8fec7eb5":"cEDA.univar(\"GarageYrBlt\")","71d3e2d6":"# Let's check if the houses are usually built with the garage\ndf_ = cEDA.df[[\"SalePrice\",\"GarageType\"]]\ndf_[\"diffYr\"] = cEDA.df[\"GarageYrBlt\"] - cEDA.df[\"YearBuilt\"]\ndf_sameyear_ = df_[df_[\"diffYr\"]<=0].dropna()\ndf_diffyear_ = df_[df_[\"diffYr\"]>1].dropna()\ndf_nogarage_ = df_[df_[\"GarageType\"].isna()]\ndisplay(df_sameyear_[\"GarageType\"].value_counts())\ndisplay(df_diffyear_[\"GarageType\"].value_counts())\ndisplay(df_sameyear_[\"GarageType\"].value_counts().sum() + df_diffyear_[\"GarageType\"].value_counts().sum())\ndisplay(\"{} of all houses with garage are built with it, and {} afterwards\".format(round(df_sameyear_[\"GarageType\"].value_counts().sum()\/(df_sameyear_[\"GarageType\"].value_counts().sum() + df_diffyear_[\"GarageType\"].value_counts().sum()),2),round(df_diffyear_[\"GarageType\"].value_counts().sum()\/(df_sameyear_[\"GarageType\"].value_counts().sum() + df_diffyear_[\"GarageType\"].value_counts().sum()),2)))","6cbcc999":"fig, axes = plt.subplots(1, 3, figsize=(10,5), sharex=True, sharey=True)\nsns.boxplot(\n    y = df_nogarage_[\"SalePrice\"],\n    ax = axes[0], \n)\naxes[0].set_title(\"No garage\")\nsns.boxplot(\n    y = df_sameyear_[\"SalePrice\"],\n    ax = axes[1], \n)\naxes[1].set_title(\"Same year or +1\")\nsns.boxplot(\n    y = df_diffyear_[\"SalePrice\"],\n    ax = axes[2], \n)\naxes[2].set_title(\"year >1\")\nplt.show()","8816930e":"fig, axes = plt.subplots(1, 2, figsize=(10,5), sharey=True)\naxes[0].grid(True)\naxes[0].set_title(\"Same year +1\")\nsns.boxplot(\n    data = df_sameyear_, \n    x = 'GarageType', \n    y = \"SalePrice\", \n    ax = axes[0], \n)\naxes[1].grid(True)\naxes[1].set_title(\">1\")\nsns.boxplot(\n    data = df_diffyear_, \n    x = \"GarageType\",\n    y = \"SalePrice\", \n    ax = axes[1], \n    order = [\"Attchd\",\"Detchd\"]\n)\nplt.show()\n\ndf_diffyear_ = df_diffyear_[df_diffyear_[\"GarageType\"].isin([\"Detchd\",\"Attchd\"])]    \ndisplay(HTML(\"<b>Price vs garage built year (>1)<\/b>\"))\nsns.lmplot(\n    data = df_diffyear_, \n    x = \"diffYr\",\n    y = \"SalePrice\", \n    hue = 'GarageType', \n    col = \"GarageType\", \n    legend = True, \n)\nplt.show()","84100fc7":"cEDA.add_analysis(\"GarageYrBlt\",\"description\",\"Looks like if your house had a garage built at the same time of the house or during the previous year, the prices can be higher. When building a garage after the house, there seems not to be a significant impact depending on the difference in years.\")\ncEDA.add_analysis(\"GarageYrBlt\",\"fillna\",999)\ncEDA.add_analysis(\"GarageYrBlt\",\"feature\",\"BltWithGarage. Informs if a house has been built with a garage or not. Built with garage means the YearBlt and GarageYrBlt are the same or differ in 1 year. This feature could substitute GarageYrBlt.\")\ncEDA.add_analysis(\"GarageYrBlt\",\"drop\")","25f92108":"cEDA.univar(\"GarageFinish\", fillna=\"NoGrg\")","63b04202":"cEDA.add_analysis(\"GarageFinish\",\"description\",\"Positive correlation with price.\")\ncEDA.add_analysis(\"GarageFinish\",\"filla\",\"No\")\ncEDA.add_analysis(\"GarageFinish\",\"oe\",\"[('Fin',3),('RFn',2),('Unf',1),('No',0)]\")","f0c0ff11":"cEDA.univar(\"GarageCars\")","87f20b5a":"cEDA.add_analysis(\"GarageCars\",\"description\",\"Price increases with number of cars. The standard is 2 cars.\")","197382a6":"cEDA.univar(\"GarageArea\")","7799a4cc":"cEDA.add_analysis(\"GarageArea\",\"description\",\"Positive correlation with price. Potential correlation with number of cars.\")\ncEDA.add_analysis(\"GarageArea\",\"outliers\",\">1250\")","4a0a4fed":"# Let's see if there is a high correlation between area and cars or if we can potentially combine both values \nfig, axes = plt.subplots(1, 2, figsize=(13,5))\naxes[0].set_title(\"Area vs Cars\")\nsns.regplot(\n    data = cEDA.df, \n    x = \"GarageArea\", \n    y = \"GarageCars\", \n    ax = axes[0], \n)\ndf_ = cEDA.df[[\"SalePrice\",\"GarageCars\",\"GarageArea\"]]\ndf_[\"GarageAreaPerCar\"] = df_[\"GarageCars\"]\/df_[\"GarageArea\"]\naxes[1].set_title(\"Cars\/Area vs Price\")\nsns.regplot(\n    data = df_, \n    x = \"GarageAreaPerCar\", \n    y = \"SalePrice\", \n    ax = axes[1], \n)\nplt.show()","1768c462":"cEDA.univar(\"GarageQual\", fillna=\"NoGrg\")","6fdea5d9":"cEDA.add_analysis(\"GarageQual\",\"description\",\"Mostly all garages have TA Quality. Does not seem to be a relevant variable.\")\ncEDA.add_analysis(\"GarageQual\",\"fillna\",\"NoGrg\")\ncEDA.add_analysis(\"GarageQual\",\"oe\",\"('Ex',5),('Gd',4),('TA',3),('Fa',2),('Po',1),('NoGrg',0)\")","bb9370a4":"cEDA.univar(\"GarageCond\", fillna=\"NoGrg\")","8e2bdcdc":"cEDA.add_analysis(\"GarageCond\",\"description\",\"Most garages have TA conditions. Not relevant.\")\ncEDA.add_analysis(\"GarageCond\",\"fillna\",\"NoGrg\")\ncEDA.add_analysis(\"GarageCond\",\"oe\",\"('Ex',5),('Gd',4),('TA',3),('Fa',2),('Po',1),('NoGrg',0)\")","2426b779":"cEDA.univar(\"PavedDrive\")","04cbdab2":"cEDA.add_analysis(\"PavedDrive\",\"description\",\"Having a non or partialy paved driveway seems to limit the potential of price.\")\ncEDA.add_analysis(\"PavedDrive\",\"oe\",\"('Y',2), ('P',1), ('N',0)\")","c2405fe4":"cEDA.univar(\"WoodDeckSF\")","721c1dc6":"cEDA.add_analysis(\"WoodDeckSF\",\"description\",\"Slightly positive correlation. Difficult to transform, as there are a lot of values that are 0 (no wooddeck)\")","011a707c":"cEDA.univar(\"OpenPorchSF\")","aaef2451":"cEDA.add_analysis(\"OpenPorchSF\",\"description\",\"Slightly positive correlation. Probably also correlated with WoodDeckSF.\")","3c87cd13":"cEDA.univar(\"EnclosedPorch\")","02862cce":"# Let's see what happens if we get rid of the 0 values.\ns_ = cEDA.df[cEDA.df[\"EnclosedPorch\"]==0][\"SalePrice\"]\nfig, axes = plt.subplots(1, 2, figsize=(10,5 ), sharey=True)\naxes[0].set_title(\"EnclosedPorch (No) vs Price\")\nsns.boxplot(y=s_, ax=axes[0])\ns_ = cEDA.df[cEDA.df[\"EnclosedPorch\"]>0][\"SalePrice\"]\naxes[1].set_title(\"EnclosedPorch (Yes) vs Price\")\nsns.boxplot(y=s_, ax=axes[1])\nplt.show()\n\n# Is there correlation depending on the size of the porc?\ndf_ = cEDA.df[cEDA.df[\"EnclosedPorch\"]>0][[\"EnclosedPorch\",\"SalePrice\"]]\nsns.regplot(\n    data = df_, \n    x = \"EnclosedPorch\", \n    y = \"SalePrice\"\n)","cb4f8b60":"cEDA.add_analysis(\"EnclosedPorch\",\"description\",\"75% of all data has 0 SF of enclosed porch. Houses without EnclosedPorch show higher prices.\")","0f0cde50":"cEDA.univar(\"3SsnPorch\")","25545953":"cEDA.add_analysis(\"3SsnPorch\",\"description\",\"Almost all houses do not have 3SsnPorch. Non relevant variable.\")","08a51108":"cEDA.univar(\"ScreenPorch\")","f839a153":"cEDA.add_analysis(\"ScreenPorch\",\"description\",\"Most houses have no ScreenPorch. If they do, there is a slight positive correlation with price.\")","eea81194":"cEDA.univar(\"PoolArea\")","aa18810e":"cEDA.add_analysis(\"PoolArea\",\"description\",\"Almost all houses do not have PoolArea. Irreleveant variable.\")\ncEDA.add_analysis(\"PoolArea\",\"drop\")","c7e9d7b0":"cEDA.univar(\"PoolQC\", fillna=\"No\")","348686d1":"cEDA.add_analysis(\"PoolQC\",\"description\",\"Irrelevant variable\")\ncEDA.add_analysis(\"PoolQC\",\"drop\")","2464192f":"cEDA.univar(\"Fence\",fillna=\"NoFnc\")","aa03d16f":"cEDA.add_analysis(\"Fence\",\"description\",\"81% of houses have no Fence. If they do, the predominant is MnPrv. Looks like an irrelevant variable. Not sure if the categories are ordinal.\")\ncEDA.add_analysis(\"Fence\",\"fillna\",\"NoFnc\")\ncEDA.add_analysis(\"Fence\",\"ohe\")","7b9694cf":"cEDA.univar(\"MiscFeature\", fillna=\"NoMiscF\")","4c4a1808":"cEDA.add_analysis(\"MiscFeature\",\"description\",\"Only 54 houses have some kind of miscellaneous feature. Having a second garage and a tennis court may affect the price, but there is almost no data. Irrelevant variable.\")\ncEDA.add_analysis(\"MiscFeature\",\"fillna\",\"NoMscF\")\ncEDA.add_analysis(\"MiscFeature\",\"pivot\",\"MiscVal\")","ef5202be":"cEDA.univar(\"MiscVal\")","34c57d45":"cEDA.add_analysis(\"MiscVal\",\"outliers\",\">8000\")\ncEDA.add_analysis(\"MiscVal\",\"pivot\",\"MiscFeatures\")","dce6ebf5":"cEDA.univar(\"MoSold\")","1d4b4f52":"cEDA.add_analysis(\"MoSold\",\"description\",\"Not relevant.\")\ncEDA.add_analysis(\"MoSold\",\"drop\")","b851ee12":"cEDA.univar(\"YrSold\")","bf306a5e":"cEDA.add_analysis(\"YrSold\",\"description\",\"Not relevant. Looks like there has not been an increase in price with the years.\")\ncEDA.add_analysis(\"YrSold\",\"drop\")","44597312":"cEDA.univar(\"SaleType\")","4f575874":"cEDA.add_analysis(\"SaleType\",\"description\",\"87% of sold houses were 2nd hand, and 8% new houses. New houses show slightly higher prices.\")\ncEDA.add_analysis(\"SaleType\",\"ohe\")","ba8cbb20":"cEDA.univar(\"SaleCondition\")","d6e726a7":"# Seems to have correlation with SaleType\ncross_ = pd.crosstab(\n    cEDA.df[\"SaleType\"], \n    cEDA.df[\"SaleCondition\"]\n)\ncross_","276aab7c":"cEDA.add_analysis(\"SaleCondition\",\"description\",\"82% of sold houses are Normal, 9% partial (new houses) and 7% abnormal.\")\ncEDA.add_analysis(\"SaleCondition\",\"ohe\")","5d0d43c8":"cEDA.multivar()","97af16c7":"cEDA.collinear","95456de8":"class BayesianTargetEncoder(object):\n    '''\n    BayesianTargetEncoder\n    developed by Matt Motoki\n    https:\/\/www.kaggle.com\/mmotoki\/avito-target-encoding\n    https:\/\/towardsdatascience.com\/target-encoding-and-bayesian-target-encoding-5c6a6c58ae8c\n    '''    \n    def __init__(self, group):\n        '''\n        __init__\n        parameters:\n        group: name of the categorical variable to be transformed\n        '''\n        self.group = group\n        self.stats = None\n        \n    # get counts from df\n    def fit(self, df, target_col):\n        self.prior_mean = np.mean(df[target_col])\n        stats = df[[target_col, self.group]].groupby(self.group)\n        stats = stats.agg(['sum', 'count'])[target_col]    \n        stats.rename(columns={'sum': 'n', 'count': 'N'}, inplace=True)\n        stats.reset_index(level=0, inplace=True)           \n        self.stats = stats\n        \n    # extract posterior statistics\n    def transform(self, df, stat_type, N_min=1):\n        \n        df_stats = pd.merge(df[[self.group]], self.stats, how='left')\n        n = df_stats['n'].copy()\n        N = df_stats['N'].copy()\n        \n        # fill in missing\n        nan_indexs = np.isnan(n)\n        n[nan_indexs] = self.prior_mean\n        N[nan_indexs] = 1.0\n        \n        # prior parameters\n        N_prior = np.maximum(N_min-N, 0)\n        alpha_prior = self.prior_mean*N_prior\n        beta_prior = (1-self.prior_mean)*N_prior\n        \n        # posterior parameters\n        alpha = alpha_prior + n\n        beta =  beta_prior + N-n\n        \n        # calculate statistics\n        if stat_type=='mean':\n            num = alpha\n            dem = alpha+beta\n                    \n        elif stat_type=='mode':\n            num = alpha-1\n            dem = alpha+beta-2\n            \n        elif stat_type=='median':\n            num = alpha-1\/3\n            dem = alpha+beta-2\/3\n        \n        elif stat_type=='var':\n            num = alpha*beta\n            dem = (alpha+beta)**2*(alpha+beta+1)\n                    \n        elif stat_type=='skewness':\n            num = 2*(beta-alpha)*np.sqrt(alpha+beta+1)\n            dem = (alpha+beta+2)*np.sqrt(alpha*beta)\n\n        elif stat_type=='kurtosis':\n            num = 6*(alpha-beta)**2*(alpha+beta+1) - alpha*beta*(alpha+beta+2)\n            dem = alpha*beta*(alpha+beta+2)*(alpha+beta+3)\n\n        else:\n            num = self.prior_mean\n            dem = np.ones_like(N_prior)\n            \n        # replace missing\n        value = num\/dem\n        value[np.isnan(value)] = np.nanmedian(value)\n        return value\n        ","fbdeecc5":"class ProcessDataset:\n    '''\n    ProcessDataset\n    Class that handles data processing. \n    \n    Construct\n    data: dataset with all the data\n    cEDA: instance of the EDA class, with all the analysis run before\n    submission: if a test dataset for submission purposes exists, it can be included so that independent variables are also transformed.\n    '''\n    \n    data = np.nan # will include the processed data\n    submission = np.nan # will include the processed submission dataset\n    index_submission = [] # list with the indexes of the test dataset, when kaggle provides it. That allows us to process the X_test at the same time.\n    origColumns = [] # original X columns to be processed\n    analysis = np.nan # dataframe with the EDA analysis\n    collinear = np.nan # dataframe with the EDA collinearity analysis\n    html_report = \"<h1>Dataset Processing Report<\/h1>\"\n    target = np.nan # target of the database\n    \n    def __init__(self, data, cEDA, submission = None):\n        self.data = data.copy()\n        self.collinear = cEDA.collinear\n        self.analysis = cEDA.analysis\n        self.analysis[\"processed\"] = 0 # 0=not processed | -1=error | 1=success\n        if submission is not None:\n            self.index_submission = submission.index\n            self.data = pd.concat([self.data, submission])\n        self.target = cEDA.target\n        \n        # store columns to loop through\n        self.origColumns = self.data.columns\n        \n        # initialize html_report\n        self.html_report += str(cEDA.analysis.to_html())\n        \n        display(HTML(f'<li>Act on collinearity.<\/li><li>You can proceed with the first column: {self.origColumns[0]}<\/li>'))\n    \n    def drop_collinear(self, vif_thresh=5):\n        '''\n        Function that drop collinear columns from both the X and the analysis dataframes\n        \n        parameters:\n        none\n        \n        returns:\n        none\n        '''\n        \n        if len(self.collinear)>0:\n            t_ = '<h3>--> COLLINEARITY Results<\/h3><br>'\n            t_ += 'Original Data shape: ' + str(self.data.shape) + \"<br>\"\n            \n            # Drop from Data\n            columns_to_drop = self.collinear[self.collinear[\"VIF\"]>=vif_thresh][\"variables\"].to_list()\n            self.data.drop(columns_to_drop, axis=1, inplace=True)\n            \n            # store columns to loop through\n            self.origColumns = self.data.columns\n            \n            # Drop from analysis\n            index_to_drop = self.analysis[self.analysis[\"column\"].isin(columns_to_drop)].index\n            self.analysis.drop(index_to_drop, axis=0, inplace=True)\n            \n            t_ += 'Columns with vif>=' + str(vif_thresh) + ' have been dropped: ' + str(columns_to_drop) + '.<br>'\n            t_ += 'New X shape: ' + str(self.data.shape) + \"<br>\"\n            \n            display(HTML(t_))\n        \n            # Add to report\n            self.update_report(t_)\n\n    def show_var(self, column):\n        '''\n        show_var\n        Function that gives the information of the column\n        parameters:\n        column: name of the column\n        returns:\n        analysis info visualization of the corresponding variable\n        '''\n        \n        s_ = pd.Series(self.analysis[self.analysis[\"column\"]==column][\"value\"].values, index=self.analysis[self.analysis[\"column\"]==column][\"key\"].values)\n        t_ = '<b>{}<\/b>'.format(column) + '<br>'\n        if \"description\" in s_.index.to_list():\n            t_ += '<u>Analysis<\/u>: {}'.format(s_.loc[\"description\"]) + '<br>'\n            s_.drop(\"description\", inplace=True)\n        if len(s_)>0:\n            t_ += \"<u>Actions<\/u>:\"\n            for i in range(len(s_)):\n                t_ += '<li>' + str(s_.index[i]) + \": \" + str(s_.iloc[i]) + \"<\/li>\"\n        \n        display(HTML(t_))\n        \n        iColumn = self.origColumns.to_list().index(column)\n        if (iColumn+1) == len(self.origColumns): display(HTML('No more columns'))\n        else: display(HTML(f'Next column: {self.origColumns[iColumn+1]}'))                \n    \n    def set_others(s, num_categories=0, min_elem_categories=0, categories=np.nan, others_name=\"Others\", fill_nulls_value=np.nan):\n        #----------------------------------------------------\n        #----------------------------------------------------\n        # CATEGORY_SET_OTHERS\n        # Funci\u00f3n que permite categorizar un n\u00famero X de categor\u00edas para una serie\n        # Par\u00e1metros\n        # s: serie a categorizar\n        # num_categories: n\u00famero de categor\u00edas a incluir, de la m\u00e1s a la menos frecuente\n        # others_name: nombre de la categor\u00eda Others\n        # fill_nulls_value: Aprovechamos para rellenar los valores nulos con su propia categor\u00eda\n        #\n        # Retorna\n        # 1. Serie categorizada o dataframe concatenado\n        #----------------------------------------------------\n        #----------------------------------------------------\n        # Primero tratamos los nulls\n        if (fill_nulls_value is not np.nan): s.fillna(fill_nulls_value, inplace=True)\n\n        if (categories is not np.nan):\n            topCategories = categories\n        elif (num_categories > 0):\n            topCategories = s.value_counts().head(num_categories)\n            topCategories = topCategories.index.tolist()\n        elif (min_elem_categories > 0):\n             _s = s.value_counts()\n             topCategories = _s[_s>=min_elem_categories].index.tolist()\n\n\n        # A\u00f1adimos el valor nulo a las categor\u00edas\n        if (fill_nulls_value is not np.nan): topCategories.append(fill_nulls_value)\n\n        # A\u00f1adimos la categor\u00eda adicional de Others\n        topCategories.append(others_name)\n        s = pd.Categorical(s, categories=topCategories).fillna(others_name) ## Las que no encajan con la lista se asocian a Others\n\n        return s\n\n    # ENCODERS\n    def to_ohe(self, column, n_1=True, new_name=None, repeat=False):\n        '''\n        to_ohe\n        Function that transforms column to ohe and updates the X dataframe\n        Parameters\n        column: name of the variable to transform or list of columns for multiple ohe fusion.\n        n_1: if True, drop one of the columns, as the value can be defined with the rest.\n        new_name: if a new name for the column must be given\n        repeat: if column is a list, it indicates that two or more values from the different columns cannot be repeated.\n        \n        Return\n        Updated dataframe\n        '''\n        #first, we check if we have a column or a list of columns\n        multiple = isinstance(column, list)\n        if multiple: n_1=False # if joining multiple ohe's, we cannot do n-1 as the information would be incomplete\n        \n        if not multiple:\n            # prefix initialization\n            prefix = column\n\n            # get_dummies (generate OHE)\n            dumm_ = pd.get_dummies(self.data[column], prefix=prefix, prefix_sep=\"_\")\n            if n_1: dumm_.drop(dumm_.columns[-1], axis=1, inplace=True) # we get rid of the last column, specially in regression\n            t_ = '<h3>--> ' + column + ' OHE Results (n_1: ' + str(n_1) + ')<\/h3><br>'\n            t_ += 'OHE generated a total of ' + str(len(dumm_.columns)) + ' columns.<br>'\n            t_ += 'Original Data shape: ' + str(self.data.shape) + \"<br>\"\n\n            # Concat OHE columns\n            self.data = pd.concat([self.data.drop(column, axis=1), dumm_], axis=1)\n            t_+= f'New Data shape: {str(self.data.shape)} ({str(len(dumm_.columns))} OHE -1 dropped)<br>'\n            display(HTML(t_))\n\n            # Update analysis\n            self.update_analysis(column, \"ohe\", 1)\n        else:\n            dumm_ = pd.DataFrame()\n            # We generate a dummy for each column in the list\n            for iColumn, nColumn in enumerate(column):\n                coldumm_ = pd.get_dummies(self.data[nColumn], prefix=new_name, prefix_sep=\"_\")\n                # loop through the new dummied dataframe\n                for nColumnDumm in coldumm_.columns:\n                    if nColumnDumm in dumm_.columns: # if the col already exists, we must add the result\n                        dumm_[nColumnDumm] = dumm_[nColumnDumm] + coldumm_[nColumnDumm]\n                        if not repeat:\n                            dumm_.iloc[dumm_[nColumnDumm]>0,dumm_.columns.to_list().index(nColumnDumm)]=1\n                    else: # if the col does not exist, we create the column\n                        dumm_[nColumnDumm] = coldumm_[nColumnDumm]\n            \n            t_ = f'<h3>--> {str(column)} (MULTIPLE) OHE Results (n_1: {str(n_1)}, repeat: {str(repeat)})<\/h3><br>'\n            t_ += f'OHE generated a total of {str(len(dumm_.columns))} columns.<br>'\n            t_ += f'Original Data shape: {str(self.data.shape)}<br>'\n            \n            # Concat OHE columns\n            self.data = pd.concat([self.data.drop(column, axis=1), dumm_], axis=1)\n            t_+= f'New Data shape: {str(self.data.shape)} ({str(len(dumm_.columns))} OHE -{len(column)} dropped)<br>'\n            display(HTML(t_))\n\n            # Update analysis\n            # self.update_analysis(column, \"ohe\", 1) #TODO\n        \n        # Add to report\n        self.update_report(t_)\n    \n    def to_oe(self, column, mapping):\n        '''\n        to_oe\n        Function that transforms column to ordinal encoding and updates the X dataframe\n        Parameters\n        column: name of the variable to transform\n        mapping: the value map (list of tupples)\n        \n        Return\n        Updated dataframe\n        '''\n        \n        t_ = '<h3>--> ' + column + ' OE Results<\/h3><br>'\n        \n        # map formating (we need list dictionaries)\n        dict_map = {}\n        for i in range(len(mapping)):\n            dict_map[mapping[i][0]] = mapping[i][1]\n        \n        s_ = self.data[column].map(dict_map) # we store it in a temp variable to check if everything went well before proceeding\n        if (len(s_[s_.isna()])>0): # that means that either the mapping was not correct, or that there are still null values\n            t_ += 'WARNING: The transformed column would still have null values. Therefore, not able to encode the variable.<br>'\n            # Update analysis\n            self.update_analysis(column, \"oe\", -1)\n        else:\n            t_ += 'Original Data shape: ' + str(self.data.shape) + \"<br>\"\n            dfprev_ = pd.DataFrame(self.data[column].value_counts()).reset_index().set_index(column)\n            self.data[column] = s_\n            t_ += 'New data shape: ' + str(self.data.shape) + '<br>'\n            dfpost_ = pd.DataFrame(self.data[column].value_counts()).reset_index().set_index(column)\n            df_ = dfprev_.join(dfpost_, lsuffix=\"_prev\", rsuffix=\"_post\")\n            t_ += df_.to_html()\n            # Update analysis\n            self.update_analysis(column, \"oe\", 1)\n        \n        display(HTML(t_))\n        \n        # Add to report\n        self.update_report(t_)\n    \n    def to_binary(self, column, mapping):\n        '''\n        to_binary\n        Function that transforms column to binary encoding and updates the X dataframe\n        Parameters\n        column: name of the variable to transform\n        mapping: the value map (list of tupples)\n        \n        Return\n        Updated dataframe\n        '''\n        \n        t_ = '<h3>--> ' + column + ' BINARY Results<\/h3><br>'\n        \n        # map formating (we need list dictionaries)\n        dict_map = {}\n        for i in range(len(mapping)):\n            dict_map[mapping[i][0]] = mapping[i][1]\n        \n        s_ = self.data[column].map(dict_map) # we store it in a temp variable to check if everything went well before proceeding\n        if (len(s_[s_.isna()])>0): # that means that either the mapping was not correct, or that there are still null values\n            t_ += 'WARNING: The transformed column would still has null values. Therefore, not able to encode the variable.<br>'\n            # Update analysis\n            self.update_analysis(column, \"binary\", -1)\n        else:\n            t_ += 'Original Data shape: ' + str(self.data.shape) + \"<br>\"\n            dfprev_ = pd.DataFrame(self.data[column].value_counts()).reset_index().set_index(column)\n            self.data[column] = s_\n            t_ += 'New X shape: ' + str(self.data.shape) + '<br>'\n            dfpost_ = pd.DataFrame(self.data[column].value_counts()).reset_index().set_index(column)\n            df_ = dfprev_.join(dfpost_, lsuffix=\"_prev\", rsuffix=\"_post\")\n            t_ += df_.to_html()\n            # Update analysis\n            self.update_analysis(column, \"binary\", 1)\n        \n        display(HTML(t_))\n        \n        # Add to report\n        self.update_report(t_)\n    \n    def to_pivot(self, column, value_column, new_name):\n        '''\n        to_pivot\n        Function that implements a pivot as an ohe\n        Parameters\n        column: name of the column that will be used for ohe columns\n        value_column: name of the column that will have the values\n        new_name: if a new name for the column must be given\n        \n        Return\n        Updated dataframe\n        '''\n        \n        df_ = self.data[[column, value_column]]\n        index_name = df_.index.name\n        df_.reset_index(drop=False, inplace=True)\n        df_columns = [index_name, column, value_column] #Id is the default name that reset_index gives when the index has no name\n        pv_ = df_.pivot(index=[index_name], columns=[column], values=[value_column]).add_prefix(new_name + \"_\") # the pivot is like an ohe but with values instead of 1\/0\n        pv_.columns = pv_.columns.get_level_values(1)\n        pv_.fillna(0, inplace=True)\n        \n        t_ = f'<h3>--> {column} PIVOT Results (column: {column}, value_column: {value_column})<\/h3><br>'\n        t_ += f'PIVOT generated a total of {str(len(pv_.columns))} columns.<br>'\n        t_ += 'Original Data shape: ' + str(self.data.shape) + \"<br>\"\n        \n        # join the dataframe with the pivot\n        self.data = pd.concat([self.data.drop([column, value_column], axis=1), pv_], axis=1)\n        \n        t_+= f'New Data shape: {str(self.data.shape)} ({str(len(pv_.columns))} PIVOT -2 dropped)<br>'\n        display(HTML(t_))\n\n        # Update analysis\n        self.update_analysis(column, \"pivot\", 1)\n        \n        # Add to report\n        self.update_report(t_)\n        \n    \n    # TRANSFORMATIONS\n    def to_sqrt(self, column):\n        '''\n        to_sqrt\n        Function that transforms column to sqrt and updates X dataframe\n        Parameters\n        column: name of the variable to transform\n        \n        Return\n        Updated dataframe\n        '''\n        has_neg = self.data[column].min() < 0\n        \n        t_ = '<h3>--> ' + column + ' SQRT Results<\/h3><br>'\n        if has_neg:\n            t_ += 'WARNING: The column has negative values. Therefore, not able to sqrt transform the variable.<br>'\n            \n            # Update analysis\n            self.update_analysis(column, \"sqrt\", -1)\n        else:\n            t_ += 'Original X shape: ' + str(self.data.shape) + ' - mean: ' + str(round(self.data[column].mean(),2)) + ' - skew: ' + str(round(self.data[column].skew(),2)) + '<br>'\n            self.data[column] = self.data[column]**0.5\n            t_ += 'New X shape: ' + str(self.data.shape) + ' - mean: ' + str(round(self.data[column].mean(),2)) + ' - skew: ' + str(round(self.data[column].skew(),2)) + '<br>'\n            \n            # Update analysis\n            self.update_analysis(column, \"sqrt\", 1)\n        \n        display(HTML(t_))\n        \n        # Add to report\n        self.update_report(t_)\n    \n    def to_inv(self, column):\n        '''\n        to_sqrt\n        Function that transforms column to 1\/column and updates X dataframe\n        Parameters\n        column: name of the variable to transform\n        \n        Return\n        Updated dataframe\n        '''\n        has_zeros = len(self.data[self.data[column]==0]) > 0\n        \n        t_ = '<h3>--> ' + column + ' INV Results<\/h3><br>'\n        if has_zeros:\n            t_ += 'WARNING: The column has negative values. Therefore, not able to inverse transform the variable.<br>'\n            \n            # Update analysis\n            self.update_analysis(column, \"inv\", -1)\n        else:\n            t_ += 'Original Data shape: ' + str(self.data.shape) + ' - mean: ' + str(round(self.data[column].mean(),2)) + ' - skew: ' + str(round(self.data[column].skew(),2)) + '<br>'\n            self.data[column] = self.data[column]**0.5\n            t_ += 'New Data shape: ' + str(self.data.shape) + ' - mean: ' + str(round(self.data[column].mean(),2)) + ' - skew: ' + str(round(self.data[column].skew(),2)) + '<br>'\n            \n            # Update analysis\n            self.update_analysis(column, \"inv\", 1)\n        \n        display(HTML(t_))\n        \n        # Add to report\n        self.update_report(t_)\n        \n    def to_log(self, column):\n        '''\n        to_log\n        Function that transforms column to log and updates X dataframe\n        Parameters\n        column: name of the variable to transform\n        \n        Return\n        Updated dataframe\n        '''\n        has_neg = self.data[column].min() < 0\n        has_zeros = len(self.data[self.data[column]==0]) > 0\n        \n        t_ = '<h3>--> ' + column + ' LOG Results<\/h3><br>'\n        \n        if has_neg:\n            t_ += 'WARNING: The column has negative values. Therefore, not able to log transform the variable.<br>'\n            \n            # Update analysis\n            self.update_analysis(column, \"sqrt\", -1)\n        else:\n            t_ += 'Original Data shape: ' + str(self.data.shape) + ' - mean: ' + str(round(self.data[column].mean(),2)) + ' - skew: ' + str(round(self.data[column].skew(),2)) + '<br>'\n\n            if not has_zeros:\n                self.data[column] = np.log(self.data[column])\n            elif has_zeros:\n                self.data[column] = np.log(self.data[column] + 1)\n                t_ += '--> note: variable has zeros. Log(s+1) has been applied ot avoid infinity.<br>'\n            t_ += 'New Data shape: ' + str(self.data.shape) + ' - mean: ' + str(round(self.data[column].mean(),2)) + ' - skew: ' + str(round(self.data[column].skew(),2)) + '<br>'\n            # Update analysis\n            self.update_analysis(column, \"sqrt\", 1)\n        \n        display(HTML(t_))\n        \n        # Add to report\n        self.update_report(t_)\n    \n    def to_ratio(self, column, base, rounded=2):\n        '''\n        to_ratio\n        Function that obtains a ratio of the column values from a base\n        Parameters\n        column: name of the variable to transform\n        base: column that is used as base for the ratio\n        rounded = number of decimals\n        \n        Return\n        Updated dataframe\n        '''\n        \n        t_ = f'<h3>--> {column} RATIO Results<\/h3><br>'\n        self.data[column] = round(self.data[column]\/self.data[base],2)\n        t_ += f'Column has been transformed to ratio base: {base}<br>'\n        \n        display(HTML(t_))\n        \n        # Update analysis\n        self.update_analysis(column, \"ratio\", 1)\n        \n        # Add to report\n        self.update_report(t_)\n        \n        \n    def drop_outliers(self, column, threshold):\n        '''\n        drop_outliers\n        Function that drops outliers that go beyond a threshold from both X and y datasets\n        Parameters\n        column: name of the variable to transform\n        threshold: to be determined as an outlier\n        \n        Return\n        Updated dataframe\n        \n        IMPORTANT: outliers of the test set MUST NOT BE REMOVED.\n        '''\n        t_ = '<h3>--> ' + column + ' DROP OUTLIERS Results<\/h3><br>'\n        t_ += 'Original Data shape: ' + str(self.data.shape) + ' - mean: ' + str(round(self.data[column].mean(),2)) + ' - skew: ' + str(round(self.data[column].skew(),2)) + '<br>'\n        index_to_drop = eval(\"self.data[self.data[column]\" + threshold + \"].index.to_list()\")\n        index_to_drop = [e for e in index_to_drop if e not in self.index_submission]\n        self.data.drop(index_to_drop, axis=0, inplace=True)\n        t_ += 'New Data shape: ' + str(self.data.shape) + ' - mean: ' + str(round(self.data[column].mean(),2)) + ' - skew: ' + str(round(self.data[column].skew(),2)) + '<br>'\n        display(HTML(t_))\n        \n        # Update analysis\n        self.update_analysis(column, \"outliers\", 1)\n        \n        # Add to report\n        self.update_report(t_)\n    \n    def drop(self, column):\n        '''\n        drop\n        Function that drops a variable\n        Parameters\n        column: name of the variable to drop\n        \n        Return\n        Updated dataframe\n        '''\n        t_ = '<h3>--> ' + column + ' DROP Results<\/h3><br>'\n        t_ += 'Original Data shape: ' + str(self.data.shape) + '<br>'\n        self.data = self.data.drop(column, axis=1)\n        t_ += 'New Data shape: ' + str(self.data.shape) + '<br>'\n        display(HTML(t_))\n        \n        # Update analysis\n        self.update_analysis(column, \"drop\", 1)\n        \n        # Add to report\n        self.update_report(t_)\n    \n    def drop_id(self, column, ids):\n        '''\n        drop_id\n        Function that drops records\n        Parameters\n        column: just for reporting purposes\n        ids: string or list of id label\n        \n        Return\n        Updated dataframe\n        '''\n        t_ = '<h3>--> ' + column + ' DROP_ID Results<\/h3><br>'\n        t_ += 'Original Data shape: ' + str(self.data.shape) + '<br>'\n        self.data = self.data.drop(ids, axis=0)\n        t_ += 'New Data shape: ' + str(self.data.shape) + '<br>'\n        display(HTML(t_))\n        \n        # Update analysis\n        self.update_analysis(column, \"drop_id\", 1)\n        \n        # Add to report\n        self.update_report(t_)\n    \n    def replace(self, column, mapping):\n        '''\n        replace\n        Function that replaces some misspelled strings\n        Parameters\n        column: name of the variable to drop\n        mapping: dictionary of pairs (\"find\":\"replace\")\n        \n        Return\n        Updated dataframe\n        '''\n        t_ = f'<h3>--> {column} REPLACE Results<\/h3><br>'\n        self.data[column].replace(mapping, inplace=True)\n        t_ = f'The following changes have been successfully performed {str(mapping)}.'\n        \n        # Update analysis\n        self.update_analysis(column, \"replace\", 1)\n        \n        # Add to report\n        self.update_report(t_)        \n        \n    def fillna(self, column, value):\n        '''\n        fillna\n        Function that fills the null values\n        Parameters\n        column: name of the variable to drop\n        value: as parameter of pd.fillna\n        \n        Return\n        Updated dataframe\n        '''\n        t_ = '<h3>--> ' + column + ' FILLNA Results<\/h3><br>'\n        t_ += 'Original Data shape: ' + str(self.data.shape) + ' - num. nulls: ' + str(self.data[column].isna().sum()) + '<br>'\n        self.data[column].fillna(value, inplace=True)\n        t_ += 'New Data shape: ' + str(self.data.shape) + ' - num. nulls: ' + str(self.data[column].isna().sum()) + '<br>'\n        display(HTML(t_))\n        \n        # Update analysis\n        self.update_analysis(column, \"fillna\", 1)\n        \n        # Add to report\n        self.update_report(t_)\n        \n    def update_analysis(self, column, action, result):\n        \n        self.analysis.loc[(self.analysis[\"column\"]==column) & (self.analysis[\"key\"]==action),\"processed\"] = result\n    \n    def update_report(self, t_):\n        \n        self.html_report += \"<div style='margin-top: 15px'>\" + t_ + \"<div>\"\n        \n    def show_report(self):\n        \n        display(HTML(self.html_report))\n        \n    def get_datasets(self):\n        '''\n        get_datasets\n        Function that splits X and y into data and submission if the instance was propperly informed\n        \n        parameters:\n        none\n        \n        return X, y, X_submission\n        '''\n        if (len(self.index_submission) > 0):\n            X = self.data.loc[~self.data.index.isin(self.index_submission)].drop(self.target, axis=1)\n            y = self.data.loc[X.index][self.target]\n            X_submission = self.data.loc[self.data.index.isin(self.index_submission)].drop(self.target, axis=1)\n            # there is no y_test, as it is part of a submission to kaggle\n            \n        else:\n            X = self.data.drop(self.target, axis=1)\n            y = self.data.loc[X.index][self.target]\n            X_submission = None\n        \n        return X, y, X_submission\n            ","9a8504f1":"cProc = ProcessDataset(data, cEDA, submission)","b2d502eb":"cProc.show_var(\"MSSubClass\")","2e77cbb5":"cProc.to_ohe(\"MSSubClass\")","f5d8aea2":"cProc.show_var(\"MSZoning\")","fcb29f48":"cProc.to_ohe(\"MSZoning\")","5d986bc1":"cProc.show_var(\"LotArea\")","46146e3b":"cProc.to_sqrt(\"LotArea\")","5065b680":"cProc.drop_outliers(\"LotArea\",\">300\")","d55bd3e0":"cProc.show_var(\"Street\")","fd15f42c":"cProc.drop(\"Street\")","e253a371":"cProc.show_var(\"Alley\")","3b0d5797":"cProc.to_ohe(\"Alley\")","a293d277":"cProc.show_var(\"LotShape\")","bcad9d71":"cProc.to_ohe(\"LotShape\")","855dd4db":"cProc.show_var(\"LandContour\")","4a9ad638":"cProc.to_ohe(\"LandContour\")","cdcac2bd":"cProc.show_var(\"Utilities\")","af869eaf":"cProc.drop(\"Utilities\")","cbc37e80":"cProc.show_var(\"LotConfig\")","e27fa458":"cProc.to_ohe(\"LotConfig\")","1dfa5475":"cProc.show_var(\"LandSlope\")","92c236c2":"cProc.to_ohe(\"LandSlope\")","47b89174":"cProc.show_var(\"Neighborhood\")","02bf5533":"cProc.to_ohe(\"Neighborhood\")","98f6c724":"cProc.show_var(\"Condition1\")\ncProc.show_var(\"Condition2\")","5e7794b1":"cProc.drop(\"Condition1\")\ncProc.drop(\"Condition2\")","e53c0ac8":"cProc.show_var(\"BldgType\")","54e31adb":"cProc.to_ohe(\"BldgType\")","fc48fe6f":"cProc.show_var(\"HouseStyle\")","4848d15d":"cProc.to_ohe(\"HouseStyle\")","21aa31ce":"cProc.show_var(\"OverallQual\")","603b7cab":"cProc.show_var(\"OverallCond\")","246bf986":"cross_ = pd.crosstab(cProc.data[\"OverallQual\"], cProc.data[\"OverallCond\"])\nsns.heatmap(cross_, cmap=\"Blues\")","1d86aa1a":"cProc.show_var(\"YearBuilt\")\ncProc.show_var(\"YearRemodAdd\")","c728c6ed":"cProc.drop_id(\"YearBuilt\",524)","d49fba55":"cProc.data[\"YearsOld\"] = cProc.data[\"YearBuilt\"] - cProc.data[\"YrSold\"]","a77264c6":"# We want RemodRatio to be closer to 1 if the remodelation has been done close to the Selling Year.\ncProc.data[\"RemodRatio\"] = (cProc.data[\"YearBuilt\"] - cProc.data[\"YearRemodAdd\"])\/cProc.data[\"YearsOld\"]","411b122f":"cProc.data[cProc.data[\"RemodRatio\"]<0.0][[\"YearBuilt\",\"YearRemodAdd\",\"YrSold\",\"YearsOld\",\"RemodRatio\"]]","dadce0f6":"cProc.data[cProc.data[\"RemodRatio\"].isna()][[\"YearBuilt\",\"YearRemodAdd\",\"YrSold\",\"YearsOld\",\"RemodRatio\"]]","93f7b14c":"cProc.data.loc[1877, \"YearRemodAdd\"] = cProc.data.loc[1877, \"YearBuilt\"]\ncProc.data.loc[2296, \"YearRemodAdd\"] = cProc.data.loc[2296, \"YearBuilt\"]\ncProc.data.loc[2550, \"YearBuilt\"] = cProc.data.loc[2550, \"YrSold\"]\ncProc.data.loc[2550, \"YearRemodAdd\"] = cProc.data.loc[2550, \"YrSold\"]\n\n# And recalculate again\ncProc.data[\"RemodRatio\"] = (cProc.data[\"YearBuilt\"] - cProc.data[\"YearRemodAdd\"])\/cProc.data[\"YearsOld\"]","c4135ff5":"cProc.data[cProc.data[\"RemodRatio\"]<0.0][\"RemodRatio\"]","f0f05cdd":"cProc.data.loc[cProc.data[\"RemodRatio\"].isna(),\"RemodRatio\"]=1","9ed1681e":"sns.distplot(cProc.data[\"RemodRatio\"])","9813e8aa":"cProc.drop(\"YearRemodAdd\")","91f9805c":"cProc.show_var(\"RoofStyle\")","eba53a52":"cProc.to_ohe(\"RoofStyle\")","4cabf158":"cProc.show_var(\"RoofMatl\")","02a32124":"cProc.to_ohe(\"RoofMatl\")","59c3bc95":"cProc.show_var(\"Exterior1st\")\ncProc.show_var(\"Exterior2nd\")","44b2c95e":"cProc.replace(\"Exterior2nd\", {'Wd Shng':'Wd Sdng','Brk Cmn':'BrkComm'})","76a3d88b":"cProc.to_ohe([\"Exterior1st\",\"Exterior2nd\"], new_name=\"Exterior\")","18cf4f2b":"cProc.show_var(\"MasVnrType\")","43238b85":"cProc.fillna(\"MasVnrType\", \"None\")","b5290a5a":"cProc.to_ohe(\"MasVnrType\")","55e6fe6a":"cProc.show_var(\"MasVnrArea\")","4dd82e73":"cProc.fillna(\"MasVnrArea\",0)","0e2cadeb":"cProc.drop_outliers(\"MasVnrArea\",\">1000\")","2b9cdbc3":"cProc.show_var(\"ExterQual\")","cd4929f3":"cProc.to_oe(\"ExterQual\",[('Ex',5), ('Gd',4), ('TA',3), ('Fa',2), ('Po',1)])","ed77eb3f":"cProc.show_var(\"ExterCond\")","f14209a6":"cProc.to_oe(\"ExterCond\", [('Ex',5), ('Gd',4), ('TA',3), ('Fa',2), ('Po',1)])","9393b5fb":"cProc.show_var(\"Foundation\")","5dd7394b":"cProc.to_ohe(\"Foundation\")","bb467674":"cProc.show_var(\"BsmtQual\")","7cceac2e":"cProc.fillna(\"BsmtQual\",\"No\")","53582176":"cProc.to_oe(\"BsmtQual\",[('Ex',5), ('Gd',4), ('TA',3), ('Fa',2), ('Po',1), ('No', 0)])","a8cf8fc8":"cProc.show_var(\"BsmtCond\")","781ea812":"cProc.fillna(\"BsmtCond\",\"No\")","08541d67":"cProc.to_oe(\"BsmtCond\",[('Ex',5), ('Gd',4), ('TA',3), ('Fa',2), ('Po',1), ('No', 0)])","9bfff970":"cProc.show_var(\"BsmtExposure\")","c4ae5d29":"cProc.fillna(\"BsmtExposure\",\"NoBsmt\")","85c7f85e":"cProc.to_oe(\"BsmtExposure\",[('Gd',4), ('Av',3), ('Mn',2), ('No',1), ('NoBsmt', 0)])","d2ec8830":"cProc.show_var(\"BsmtFinType1\")","db690cb2":"cProc.fillna(\"BsmtFinType1\", \"NoBsmt\")","77390d28":"cProc.to_oe(\"BsmtFinType1\",[('GLQ',6), ('ALQ',5), ('BLQ',4), ('Rec',3), ('LwQ',2), ('Unf',1), ('NoBsmt', 0)])","227af306":"cProc.show_var(\"BsmtFinType2\")","2af644ad":"cProc.fillna(\"BsmtFinType2\", \"NoBsmt\")","c03846df":"cProc.to_oe(\"BsmtFinType2\",[('GLQ',6), ('ALQ',5), ('BLQ',4), ('Rec',3), ('LwQ',2), ('Unf',1), ('NoBsmt', 0)])","e4d32388":"cProc.show_var(\"Heating\")","c0965301":"cProc.to_ohe(\"Heating\")","2523b7c1":"cProc.show_var(\"HeatingQC\")","a431ae33":"cProc.to_oe(\"HeatingQC\", [('Ex',5), ('Gd',4), ('TA',3), ('Fa',2), ('Po',1)])","92bf3457":"cProc.show_var(\"CentralAir\")","db9571f4":"cProc.to_binary(\"CentralAir\",[('Y',1),('N',0)])","2e920d87":"cProc.show_var(\"Electrical\")","6845b025":"cProc.fillna(\"Electrical\",\"SBrkr\")","33f0dace":"cProc.to_ohe(\"Electrical\")","e5c0ec71":"cProc.show_var(\"1stFlrSF\")\ncProc.show_var(\"2ndFlrSF\")","bb97c756":"cProc.data[\"TotalSF\"] = cProc.data[\"1stFlrSF\"] + cProc.data[\"2ndFlrSF\"]\ncProc.data[\"1stFlrSF\"] = round(cProc.data[\"1stFlrSF\"]\/cProc.data[\"TotalSF\"],2)\ncProc.data[\"2ndFlrSF\"] = round(cProc.data[\"2ndFlrSF\"]\/cProc.data[\"TotalSF\"],2)\ncProc.data[\"NumFlrs\"] = cProc.data[\"2ndFlrSF\"].apply(lambda x: 1 if x==0 else 2)","7535c405":"sns.boxplot(cProc.data[\"TotalSF\"])\nplt.show()\nsns.distplot(cProc.data[\"TotalSF\"], fit=norm)\nplt.show()","e916b616":"sns.distplot(pow(cProc.data[\"TotalSF\"],0.5), fit=norm)\nplt.show()","9a88579b":"cProc.to_sqrt(\"TotalSF\")","201a5f12":"sns.boxplot(cProc.data[\"TotalSF\"])","e41b7e73":"cProc.drop_outliers(\"TotalSF\",\">=65\")","4d0891e4":"sns.boxplot(cProc.data[\"TotalSF\"])","48f8dc52":"cProc.data[cProc.data[\"TotalSF\"]>65].index","b882b847":"cProc.show_var(\"LowQualFinSF\")","b4710e99":"cProc.drop(\"LowQualFinSF\")","41f10e24":"cProc.show_var(\"GrLivArea\")","b9564852":"cProc.to_sqrt(\"GrLivArea\")","eb9ef734":"cProc.to_ratio(\"GrLivArea\",\"TotalSF\")","c9f05c4e":"sns.boxplot(cProc.data[\"GrLivArea\"])","de9f578a":"cProc.show_var(\"BsmtFullBath\")","48abcb72":"cProc.show_var(\"BsmtHalfBath\")","dcfab3bc":"cProc.show_var(\"FullBath\")","25175366":"cProc.show_var(\"HalfBath\")","157dc714":"cProc.show_var(\"BedroomAbvGr\")","d168a190":"cProc.show_var(\"KitchenAbvGr\")","afba3a5f":"cProc.show_var(\"KitchenQual\")","70cfb526":"cProc.to_oe(\"KitchenQual\", [('Ex',5),('Gd',4),('TA',3),('Fa',2),('Po',1)])","49b51c8c":"cProc.fillna(\"KitchenQual\",cEDA.df_desc.loc[\"KitchenQual\",\"mode\"])","7e2eb153":"cProc.to_oe(\"KitchenQual\", [('Ex',5),('Gd',4),('TA',3),('Fa',2),('Po',1)])","1575eb37":"cProc.show_var(\"TotRmsAbvGrd\")","16b0e35f":"cProc.show_var(\"Functional\")","ef567373":"cProc.to_oe(\"Functional\",[('Typ',8),('Min1',7),('Min2',6),('Mod',5),('Maj1',4),('Maj2',3),('Sev',2),('Sal',1)])","79c61ffe":"cProc.data[cProc.data[\"Functional\"].isna()][\"Functional\"]","eada7d49":"cProc.fillna(\"Functional\",cEDA.df_desc.loc[\"Functional\",\"mode\"])","b035b2f9":"cProc.to_oe(\"Functional\",[('Typ',8),('Min1',7),('Min2',6),('Mod',5),('Maj1',4),('Maj2',3),('Sev',2),('Sal',1)])","b11f4752":"cProc.show_var(\"Fireplaces\")","d9a76280":"cProc.show_var(\"FireplaceQu\")","3e27a57b":"cProc.fillna(\"FireplaceQu\",\"NoFp\")","678bb5dc":"cProc.to_oe(\"FireplaceQu\",[('Ex',5),('Gd',4),('TA',3),('Fa',2),('Po',1),('NoFp',0)])","591807e0":"cProc.show_var(\"GarageType\")","9cdc10c4":"cProc.fillna(\"GarageType\",\"NoGrg\")","367588c3":"cProc.to_ohe(\"GarageType\")","9dd0bb50":"cProc.show_var(\"GarageYrBlt\")","9b03eae4":"cProc.fillna(\"GarageYrBlt\",999)","48d013cd":"s_ = cProc.data[\"GarageYrBlt\"] - cProc.data[\"YearBuilt\"]\ns_ = s_.apply(lambda x: 1 if (x==0) | (x==1) else 0)\ncProc.data[\"BuiltWithGarage\"] = s_\ncProc.drop(\"GarageYrBlt\")","ff0d0a67":"cProc.show_var(\"GarageFinish\")","02b09e6d":"cProc.fillna(\"GarageFinish\",\"NoGrg\")","e50e02d1":"cProc.to_oe(\"GarageFinish\",[('Fin',3),('RFn',2),('Unf',1),('NoGrg',0)])","16b11890":"cProc.show_var(\"GarageCars\")","f8ca2977":"cProc.show_var(\"GarageArea\")","c2ea30f1":"cProc.drop_outliers(\"GarageArea\",\">1250\")","9eaa9d84":"cProc.show_var(\"GarageQual\")","dce712a9":"cProc.fillna(\"GarageQual\",\"NoGrg\")","343692fc":"cProc.to_oe(\"GarageQual\",[('Ex',5),('Gd',4),('TA',3),('Fa',2),('Po',1),('NoGrg',0)])","ea2b4f03":"cProc.show_var(\"GarageCond\")","4e675aba":"cProc.fillna(\"GarageCond\",\"NoGrg\")","2ce0d8fd":"cProc.to_oe(\"GarageCond\",[('Ex',5),('Gd',4),('TA',3),('Fa',2),('Po',1),('NoGrg',0)])","4bc57054":"cProc.show_var(\"PavedDrive\")","496f210a":"cProc.to_oe(\"PavedDrive\",[('Y',2), ('P',1), ('N',0)])","59e7ebfc":"cProc.show_var(\"WoodDeckSF\")","006348a2":"cProc.show_var(\"OpenPorchSF\")","19102bde":"cProc.show_var(\"EnclosedPorch\")","8ac47493":"cProc.show_var(\"3SsnPorch\")","0487fe9a":"cProc.show_var(\"ScreenPorch\")","7143477c":"cProc.show_var(\"PoolArea\")","072321f2":"cProc.drop(\"PoolArea\")","026e62c4":"cProc.show_var(\"PoolQC\")","95c2050b":"cProc.drop(\"PoolQC\")","88b140ef":"cProc.show_var(\"Fence\")","440b0eed":"cProc.fillna(\"Fence\",\"NoFnc\")","5ba84985":"cProc.to_ohe(\"Fence\")","d32db700":"cProc.show_var(\"MiscFeature\")\ncProc.show_var(\"MiscVal\")","ce7deeab":"cProc.fillna(\"MiscFeature\",\"NoMscF\")","a551c74c":"cProc.drop_outliers(\"MiscVal\",\">8000\")","9f129583":"cProc.to_pivot(\"MiscFeature\",\"MiscVal\",\"MiscFeatureVal\")","f39ed098":"cProc.show_var(\"MoSold\")","e782984f":"cProc.drop(\"MoSold\")","8b42c056":"cProc.show_var(\"YrSold\")","f51664f6":"cProc.drop(\"YrSold\")","ab56f52f":"cProc.show_var(\"SaleType\")","7a973c2c":"cProc.to_ohe(\"SaleType\")","99220c40":"cProc.show_var(\"SaleCondition\")","adaac79e":"cProc.to_ohe(\"SaleCondition\")","32edd725":"cProc.show_var(\"SalePrice\")","30518a7d":"cProc.to_log(\"SalePrice\")","947784fb":"cProc.collinear","fffd41f0":"cProc.data.drop([\"LotFrontage\",\"YearBuilt\",\"BsmtFinSF1\",\"BsmtFinSF2\",\"BsmtUnfSF\",\"GrLivArea\"], axis=1, inplace=True)","f2e40608":"cProc.data.isna().sum()[cProc.data.isna().sum()!=0]","45ce9317":"cProc.data.loc[cProc.data[\"TotalBsmtSF\"].isna(),\"TotalBsmtSF\"]=cEDA.df_desc.loc[\"TotalBsmtSF\",\"mean\"]","a0ae1704":"cProc.data.loc[cProc.data[\"TotalBsmtSF\"].isna(),\"TotalBsmtSF\"]","caf19d90":"cEDA.df_desc.loc[[\"BsmtFullBath\",\"BsmtHalfBath\"],:]","6495455d":"cProc.data.loc[cProc.data[\"BsmtFullBath\"].isna(),\"BsmtFullBath\"] = cEDA.df_desc.loc[\"BsmtFullBath\",\"mode\"]\ncProc.data.loc[cProc.data[\"BsmtHalfBath\"].isna(),\"BsmtHalfBath\"] = cEDA.df_desc.loc[\"BsmtHalfBath\",\"mode\"]","f7cf1237":"cProc.data.loc[cProc.data[\"GarageCars\"].isna(),\"GarageCars\"]","e9dc9b07":"cProc.data.loc[cProc.data[\"GarageCars\"].isna(),\"GarageCars\"] = cEDA.df_desc.loc[\"GarageCars\",\"mode\"]\ncProc.data.loc[cProc.data[\"GarageArea\"].isna(),\"GarageArea\"] = cEDA.df_desc.loc[\"GarageArea\",\"mean\"]","f7d33f96":"cProc.data[cProc.data[\"RemodRatio\"].isna()]","9b6df577":"cEDA.df_desc.loc[[\"YearBuilt\",\"YearRemodAdd\",\"YrSold\"]]","f79a26aa":"cProc.data.isna().sum()[cProc.data.isna().sum()!=0]","a3040197":"cProc.data.info(verbose=True)","07fab8d1":"X_data, y_data, X_submission = cProc.get_datasets()","0c0882e0":"# Importaciones espec\u00edficas para el MODELAMIENTO\nfrom scipy.stats import norm, boxcox\nfrom sklearn import model_selection as ms # model assesment and model selection strategies\nfrom sklearn.model_selection import KFold\n\n# Metrics\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.tree import export_graphviz\nimport graphviz\nfrom sklearn.linear_model import LinearRegression","35076367":"# Let's split dev\/training data from validation data. Validation data will allow us to benchmark the different models.\nX_dev, X_val, y_dev, y_val = ms.train_test_split(X_data, y_data, test_size=0.2, shuffle=True, random_state=RANDOM_STATE)\ndisplay(f'{X_dev.shape} | {y_dev.shape} | {X_val.shape} | {y_val.shape}')","59c6f8b5":"kf = KFold(n_splits=6, random_state=RANDOM_STATE)\nfor train_index, test_index in kf.split(X_dev):\n    X_train = X_dev.iloc[train_index,:]\n    y_train = y_dev.iloc[train_index]\n    X_test = X_dev.iloc[test_index,:]\n    y_test = y_dev.iloc[test_index]\n    lr = LinearRegression()\n    lr.fit(X_train, y_train) # trained\n    \n    y_train_pred = lr.predict(X_train)\n    y_test_pred = lr.predict(X_test)\n    rmse_train = mean_squared_error(y_train, y_train_pred, squared=False)\n    rmse_test = mean_squared_error(y_test, y_test_pred, squared=False)\n    print(f\"RMSE - train {rmse_train} - test {rmse_test} - diff {round(100*(rmse_test-rmse_train)\/rmse_train,2)}%\")\n\nlr = LinearRegression()\nlr.fit(X_dev, y_dev) # trained\n\ny_dev_pred = lr.predict(X_dev)\ny_val_pred = lr.predict(X_val)\nrmse_dev = mean_squared_error(y_dev, y_dev_pred, squared=False)\nrmse_val = mean_squared_error(y_val, y_val_pred, squared=False)\nprint(f\"RMSE - dev {rmse_dev} - val {rmse_val} - diff {round(100*(rmse_val-rmse_dev)\/rmse_dev,2)}%\")","ed0dbbc2":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\n\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom statsmodels.stats.diagnostic import normal_ad\nfrom statsmodels.stats.stattools import durbin_watson\n\nimport statsmodels.api as sm\n\nclass LinearRegressionValidator():\n    '''\n    LinearRegressionValidator\n    Class that validate a linear regression model\n    https:\/\/towardsdatascience.com\/how-do-you-check-the-quality-of-your-regression-model-in-python-fa61759ff685\n    \n    parameters:\n    y: target (ground truth)\n    y_pred: predicted target\n    X: predictors\n    lrmodel: linear regression model\n    '''\n    y = None # target\n    y_pred = None # predicted target\n    X = None # predictors\n    lrmodel = None # regression model\n    err = None # errors\n    \n    def __init__(self, y, y_pred, X, lrmodel):\n        self.y = y\n        self.y_pred = y_pred\n        self.lrmodel = lrmodel\n        self.X = X\n    \n    def validate(self, all=False):\n        '''\n        validate\n        Function that validates the main assumptions of linear regressions\n        Thanks to https:\/\/jeffmacaluso.github.io\/post\/LinearRegressionAssumptions\/\n        '''\n        \n        p_value_thresh = 0.05 # (5% double sided)\n        \n        self.err = self.y_pred - self.y\n        \n        display(HTML(\"<h1>Linear Regression Validation Report<\/h1>\"))\n        \n        display(HTML(\"<h3>Metrics<\/h3>\"))\n        display(HTML(\n            f\"<b>R2:<\/b> {r2_score(self.y, self.y_pred)} <br> \\\n            <b>RMSE:<\/b> {mean_squared_error(self.y, self.y_pred, squared=False)}\"\n        ))\n        \n        display(HTML(\"<h3>Linear Assumption<\/h3>\"))\n        # Predicted vs Real\n        fig = plt.figure(figsize=(4, 3))\n        df_ = pd.DataFrame(self.y.rename(\"y\"), index=self.y.index).join(pd.DataFrame(self.y_pred.rename(\"y_pred\"), index=self.y_pred.index))\n        sns.scatterplot(x='y', y='y_pred', data=df_)\n        # Plotting the diagonal line\n        min_ = max(self.y.min(),self.y_pred.min())\n        max_ = max(self.y.max(),self.y_pred.max())\n        plt.plot((min_, max_), (min_, max_), color='darkorange', linestyle='--')\n        plt.title('Real vs. Predicted')\n        plt.show()\n        \n        display(HTML(\"<h3>Normal Assumption of errors - Anderson-Darling<\/h3>\"))\n        # Using the Anderson-Darling test for normal distribution\n        p_value = normal_ad(self.err)[1]\n        if p_value <= p_value_thresh: display(HTML(f'<span style=\"color:red;font-weight:bold\">Normal assumption not satisfied (p_value: {p_value})<\/span> --> confidence intervals will likely be affected. Try to perform nonliear transformations on variables. Info on QQPlot: <a href=\"https:\/\/seankross.com\/2016\/02\/29\/A-Q-Q-Plot-Dissection-Kit.html\">here<\/a>.'))\n        \n        # Normal error distribution\n        fig, axes = plt.subplots(1, 2, figsize=(8,3))\n        plt.tight_layout(pad=0.3)\n        sns.distplot(\n            self.y, \n            fit = norm, \n            ax = axes[0], \n        )\n        axes[0].set_title(f\"Error normality (skw: {round(self.err.skew(),2)})\")\n        \n        #qqplot\n        sm.qqplot(self.err, line ='q', ax=axes[1])\n        axes[1].set_title(f\"qqplot\")\n        \n        plt.show()\n        \n        display(HTML(\"<h3>Assumption of non-autocorrelation<\/h3>\"))\n        # Assumes that there is no autocorrelation in the residuals. If there is\n        # autocorrelation, then there is a pattern that is not explained due to\n        # the current value being dependent on the previous value.\n        # This may be resolved by adding a lag variable of either the dependent\n        # variable or some of the predictors.\n        # Durbin-Watson Test\n        # Values of 1.5 < d < 2.5 generally show that there is no autocorrelation in the data\n        # 0 to 2< is positive autocorrelation\n        # >2 to 4 is negative autocorrelation\n        durbinWatson = durbin_watson(self.err)\n        if durbinWatson < 1.5:\n            display(HTML('<span style=\"color:red;font-weight:bold\">Signs of positive autocorrelation<\/span>'))\n        elif durbinWatson > 2.5:\n            display(HTML('<span style=\"color:red;font-weight:bold\">Signs of negative autocorrelation<\/span>'))\n        else:\n            display(HTML('No signs of autocorrelation'))\n        \n        display(HTML(\"<h3>Assumption of Random error vs predictors - Homoscedasticity<\/h3>\"))\n        # If heteroscedasticity: Variables with high ranges may be the cause\n        # For the dependent variable, use rates or per capita ratios instead of raw variables. That may change the project.\n        # \n        \n        fig = plt.figure(figsize=(4, 3))\n        sns.scatterplot(x=self.y_pred, y=self.err)\n        plt.title('Error vs. Predicted')\n        plt.show()\n        if all:\n            # Random errors vs predictors\n            num_pred = len(self.X.columns)\n            fig_columns = 3\n            fig, axes = plt.subplots(math.ceil(num_pred\/fig_columns), fig_columns, figsize=(4*fig_columns, math.ceil(num_pred\/fig_columns)*3))\n            #plt.tight_layout(pad=1)\n            row = 0\n            col = 0\n            for i in range(num_pred):\n                axes[row, col].axhline(y=0, linewidth=4, color='r')\n                axes[row, col].set_title(f\"Err vs {self.X.columns[i]}\")\n                sns.scatterplot(\n                    x = self.X.iloc[:,i], \n                    y = self.err, \n                    ax = axes[row, col]\n                )\n                if ((i+1)%fig_columns == 0): col = 0 \n                else: col+=1\n                if col == 0: row += 1\n            plt.show()\n            \n        # summarize feature importance\n        df_imp_ = pd.DataFrame(self.lrmodel.coef_, index=self.X.columns, columns=[\"importance\"])\n        df_imp_[\"importance_abs\"] = df_imp_[\"importance\"].abs()\n        df_imp_.sort_values(by=\"importance_abs\", ascending=False, inplace=True)\n        df_imp_.drop(\"importance_abs\", axis=1, inplace=True)\n        s_imp_ = df_imp_[\"importance\"]\n        # plot feature importance\n        fig = plt.figure(figsize=(15,4))\n        plt.xticks(rotation=90)\n        sns.barplot(x=s_imp_.index, y=s_imp_.values)\n        plt.show()","706081e0":"oVal = LinearRegressionValidator(y_val, pd.Series(y_val_pred, index=y_val.index), X_val, lr)\noVal.validate()","72ba99af":"data.iloc[oVal.err[oVal.err>=.03].index,:]","31a6c151":"data.iloc[oVal.err[oVal.err<-0.025].index,:]","e539c7a0":"from xgboost import XGBRegressor\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import GridSearchCV","38195464":"class GridSearchCVResults():\n    \n    '''\n    GridSearchCVResults\n    Class that shows GridSearchCV results in a more friendly way\n    \n    parameters:\n    GridSearchCV: GridSearchCV trained model\n    greater_is_better: False if the score is better the bigger (e.g R2) or better the smaller (eg RMSE)\n    '''\n    \n    d_res = None # cv_results dictionary\n    df_res = None # dataframe with the tabulated results\n    split_cols = [] # list with the params that vary\n    greater_is_better = True # False if the score is better the bigger (e.g R2) or better the smaller (eg RMSE)\n    \n    def __init__(self, oGridSearchCV, greater_is_better = True):\n        \n        self.d_res = oGridSearchCV.cv_results_\n        self.df_res = pd.DataFrame(self.d_res[\"params\"])\n        self.greater_is_better = greater_is_better\n        \n        for key in self.d_res:\n            if (str(key).find(\"split\"))!=-1: #it is a split results key\n                self.split_cols.append(key)\n                self.df_res[key] = pd.Series(self.d_res[key])\n        self.df_res.insert(0, \"rank_test_score\", pd.Series(self.d_res[\"rank_test_score\"]))\n        self.df_res.insert(1, \"mean_test_score\", pd.Series(self.d_res[\"mean_test_score\"]))\n        self.df_res.insert(2, \"std_test_score\", pd.Series(self.d_res[\"std_test_score\"]))\n        self.df_res.insert(3, \"mean_train_score\", pd.Series(self.d_res[\"mean_train_score\"]))\n        self.df_res.insert(4, \"std_train_score\", pd.Series(self.d_res[\"std_train_score\"]))\n        self.df_res.insert(5, \"%mean_diff\", round(100*(pd.Series(self.d_res[\"mean_test_score\"]) - pd.Series(self.d_res[\"mean_train_score\"]))\/pd.Series(self.d_res[\"std_train_score\"]),2))\n        \n        # and now, we leave only the columns that have different information\n        self.df_res.sort_values(by=\"rank_test_score\", inplace=True)\n        \n        self.show()\n    \n    def show(self):\n        '''\n        show\n        Function that plots the different score values for each ranked combination\n        '''\n        split_cols = [x for x in self.df_res.columns if x.find(\"split\")!=-1] # only get split columns\n        melt_ = self.df_res.melt(id_vars=[\"rank_test_score\"], value_vars=split_cols)\n        melt_[\"variable\"] = melt_[\"variable\"].apply(lambda x: \"test\" if x.find(\"_test_\")!=-1 else \"train\")\n        if not self.greater_is_better: melt_[\"value\"] = abs(melt_[\"value\"])\n        fig, axes = plt.subplots(2, 1, figsize=(12,7), sharex=False)\n        sns.lineplot(data=melt_, x=\"rank_test_score\", y=\"value\", hue=\"variable\", ax=axes[0])\n        axes[0].set_title(\"Score per rank (train and test)\")\n        axes[0].set_xticks(melt_[\"rank_test_score\"])\n        axes[0].grid()\n        sns.lineplot(x=self.df_res[\"rank_test_score\"], y=np.abs(self.df_res[\"%mean_diff\"]), ax=axes[1])\n        axes[1].set_title(\"DiffScore per rank (test vs train)\")\n        axes[1].set_xticks(melt_[\"rank_test_score\"])\n        axes[1].grid()\n        plt.tight_layout(pad=1)\n        plt.show()\n        \n    def get_params(self, rank=1):\n        '''\n        get_params\n        Function that prints the parameters of the selected ranked option\n        \n        params:\n        rank: number of rank to display\n        '''\n        display(HTML(f'<b>Showing parameters for rank {rank}<\/b><br>'))\n        cols_to_show = [ele for ele in self.df_res.columns if ele not in self.split_cols]\n        display(self.df_res.loc[self.df_res[\"rank_test_score\"]==(rank),cols_to_show].T)\n    \n    def show_results(self):\n        '''\n        show_results\n        Method that returns all the data of cv_results_ in a tabular format\n        '''\n        display(self.df_res)\n","26b7589f":"# https:\/\/towardsdatascience.com\/beyond-grid-search-hypercharge-hyperparameter-tuning-for-xgboost-7c78f7a2929d\nparameters = {\n    #'nthread':[1], \n    'boos': ['gblinear'], # or gblinear\n    #'gamma' : \n    'objective':['reg:squarederror'],\n    'learning_rate': [0.1], #so called `eta` value\n    'max_depth': range(3,10),\n    #'min_child_weight': [11],\n    #'silent': [1],\n    'subsample': [0.7], \n    'colsample_bytree': [0.8], \n    'colsample_bylevel': [0.8], \n    'n_estimators': [40, 45, 50, 100], #number of trees, change it to 1000 for better results\n    # 'missing':[-999],\n    'seed': [RANDOM_STATE]\n}\n\ngscv = GridSearchCV(\n    XGBRegressor(),\n    param_grid = parameters,\n    scoring=make_scorer(mean_squared_error, squared=True, greater_is_better=False),  \n    cv = 5,\n    refit = True,\n    return_train_score = True, \n    verbose = 0, \n)\ngscv.fit(X_dev, y_dev)","ec2380ed":"xgbmodel_best = gscv.best_estimator_","5e82b0d3":"oGSCVResults = GridSearchCVResults(gscv, greater_is_better=False)","393d3a7b":"oGSCVResults.get_params(14)","d6897908":"xgbmodel = XGBRegressor(\n    booster = \"gbtree\", \n    colsample_bylevel = 0.8,\n    colsample_bytree = 0.8, \n    learning_rate = 0.1, \n    max_depth = 3, \n    n_estimators = 50, \n    objective = 'reg:squarederror', \n    seed = RANDOM_STATE, \n    subsample = 0.7, \n)\nxgbmodel.fit(X_dev, y_dev)","39dd501c":"y_train_pred = xgbmodel.predict(X_train)\ny_val_pred = xgbmodel.predict(X_val)\ndisplay(mean_squared_error(y_train, y_train_pred, squared=True))\ndisplay(mean_squared_error(y_val, y_val_pred, squared=True))","728b5d62":"y_submission_pred = xgbmodel.predict(X_submission)","dfb692f2":"submission = pd.DataFrame({'Id':X_submission.index, 'SalePrice': np.exp(y_submission_pred)})\nsubmission","a3246cd4":"submission.to_csv('nbg_submission.csv',index=False)","cb41bd6b":"## Heating","ee52bce4":"## GarageCond","83081d96":"## Street","50067e16":"## LowQualFinSF","3dc7d69d":"Let's see the relationship between GrLivArea and 1st and 2nd FloorSF.","55681de9":"### MSSubClass","6854c29e":"The test (submission) set has a null value. We will update it with TA, as this is the mode.","5752e549":"I will be using the dataset_description function. This function detects the type of each columns of the dataset and describes its main caracteristics. From there, it is pretty easy to plot the distributions and behaviours vs the target to make the EDA easier.","00bd615f":"## MiscFeature","7f7e3d5a":"## TotalBsmtSF","0c0ff6c3":"## RoofStyle","c3194555":"# ML imports","309ff447":"We still have some null values in these columns, coming from the test data.","ea8095d9":"When building a house, the most common thing is to have an attached garage built. If built after the house, it is usually a detached garage.","2755a408":"## 2ndFlrSF","2aecbb5c":"## Condition1","d9611d2b":"## BsmtFinSF1","0cf5af9d":"# Global variables and constants","9aa3a1ee":"# Loading data","437c6327":"# EDA - Univariant Analysis","18125ac9":"## GarageYrBlt","18e9073f":"## OverallCond","ae4c4fcd":"## FullBath","7f1edde0":"# Baseline model","53e37791":"## BsmtFinSF2","100271ec":"## Foundation","2f27eaa8":"## EnclosedPorch","17a263ba":"As expected, Looks like if your house had a garage built at the same time of the house or during the previous year, the prices can be higher. When building a garage after the house, there seems not to be a significant impact depending on the difference in years.\n\nIf there is no garage, we will set GarageYrBlt to +999.","17895513":"## MasVnrType","cb9fe02f":"Let's see if the areas sum up correctly --> BsmtFinSF1 + BsmtFinSF2 + BsmtUnfSF = TotalBsmtSF","6fd803c4":"## PoolQC","e618b353":"We will update those values with the mode, being carefull, as some of them have already been transformed.","18265c36":"# EDA - Multivariant analysis","3c0e8916":"## GrLivArea","cfdae353":"## BedroomAbvGr","596176c4":"## GarageType","074a1424":"## PavedDrive","bd44b3bf":"## KitchenAbvGr","b8d3fdc0":"## COLUMNS","33a53817":"## Alley","5e0d0f31":"## MoSold","decd8cb4":"Houses with EnclosedPorch tend to have lower prices. But, if the house has an EnclosedPorch, the price increases with the number of square feet.","409f97e0":"\"None\" is the most common values when having \"VinylSd, CemntSd or Wd Sndg\" Exterior1st. \"None\" is also the the mode of MasVnrType.","502a46ec":"## BsmtFullBath","a8b17015":"## CentralAir","90b2c370":"## OverallQual","14bb55b1":"## 3SsnPorch","5ff394a6":"## LotShape","217bb3f4":"This Id is from the submission dataset, therefore, we must not delete it.","febe1ed5":"## LotConfig","3c7f816a":"## BldgType","8eb70d91":"## BsmtFinType1","13cade98":"## Fireplaces","276e50b3":"## TotRmsAbvGrd","280875fd":"No object columns --> all data has been correctly processed","1e46218b":"## SaleType","7564bc0a":"## WoodDeckSF","b52905b0":"We already have the test dataset ready after finishing preprocessing tasks. We just need to predict and generate the CSV file.","cc02358d":"## ExterCond","13e4f710":"## ExterQual","d6c3afce":"## MSZoning","9071dc9d":"## Functional","f3710d6e":"## Neighborhood","9367f194":"# Test dataset","cc72c323":"At a first glance, there is considerable overfitting when doing the k-fold, as test RMSE are bigger than the train ones. Same thing happens when training the model with the dev dataset and validating with val.","c7b0591c":"## BsmtCond","3152cf8c":"## LandSlope","5ac28276":"## HeatingQC","5ce9a031":"There are some misspelled values that must be corrected.","cafccde7":"## HouseStyle","ca3f3862":"## General","79ded085":"## LotFrontage","2fae55f3":"## BsmtHalfBath","b77052a7":"## Let's finally check Collinearity","17f92ba8":"## GarageFinish","7c91004b":"## YrSold","7024497b":"# Data preparation","c0b8f425":"## OpenPorchSF","75b91816":"## Electrical","b1960d9c":"And finally, all nans must be set to 1","90905289":"The mode in Functional is Typ, with more than 90% of the values.","7fb166a2":"### Target transformation","f63e73a8":"## YearBuilt","016f65d9":"## LandContour","3c9de7d9":"We have 695 nulls that correspond to No Fireplace.","5ac71dab":"## KitchenQual","98e8c501":"## YearRemodAdd","27a5d1d2":"## BsmtQual","9dd19785":"## GarageCars","3a20eb6e":"## MasVnrArea","217777d5":"# XGBoost","57b1d861":"## FireplaceQu","c16125bb":"The test data has some inconsistencies. ","bbeaa0f1":"Around the 60s, FuseX technologies where deprecated and not used any more. From then, only SBrkr is used.","c2ceda9b":"## SaleCondition","1c40e6e4":"## TARGET - SalePrice","04a7153c":"## Exterior1st","7a15c6dc":"## Utilities","0846cc04":"Building the garage after the house may have a negative impact on price. Houses without a garage tend to have lower prices as seen before.","4a63fcdd":"## RoofMatl","050340bd":"## BsmtExposure","befa3d02":"## HalfBath","af710ae4":"## Condition 2","ec363296":"# Imports","be545152":"## MiscVal","0902a86a":"## BsmtFinType2","bb38a30e":"From ranks 1 to 13 we have a lot of overfitting, we are going to retrain the model with rank 8 configuration and see what happens with Validation Data.","2c4e0f44":"## PoolArea","e976d65a":"## LotArea","2b08fe45":"## Fence","843da04d":"## BsmtUnfSF","bde5917e":"**Correlation** \nNo values above 0.9, except the two targets (we will get rid of SalePrice) during preprocessing.\n\n**Multicollinearity**\nWe find high multicollinearity in multiple variables. We must get rid of those variables to avoid an increase in the standard deviation of the predictions.","b176be8e":"Transform to have a normal distribution. Houses above 500k are prone to be considered as extreme outliers, but the log distribution will probably modify this behavior.","28d648e7":"## 1stFlrSF","ecdbdc80":"## Exterior2nd","aa784bd1":"# Support functions","22d23fc5":"## ScreenPorch","3e26b7a9":"Let's see the records that have higher residuals.","80577d8e":"## GarageArea","56979f72":"## GarageQual"}}