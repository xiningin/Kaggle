{"cell_type":{"9ffadc0e":"code","252ee4df":"code","22a12489":"code","f324861d":"code","3469b330":"code","40a02cbb":"code","a9464efd":"code","7bf24bcd":"code","b6074498":"code","30b4499c":"code","cc1c4338":"code","591bb1fd":"code","a50460a9":"code","c0392d60":"code","57d8d0c3":"code","a9d224aa":"code","af0b29eb":"code","f94bf0b5":"code","8b8cf912":"code","71565458":"code","c47687c8":"code","23bf2ea7":"code","0009e172":"code","7db460d7":"code","9538d8ae":"code","b6f86516":"markdown","7dafa83a":"markdown","522097d3":"markdown"},"source":{"9ffadc0e":"import pandas as pd\nimport numpy as np\nimport sklearn as sk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nfrom xgboost import XGBClassifier\nfrom hyperopt import tpe, hp, fmin, STATUS_OK,Trials\nfrom hyperopt.pyll.base import scope\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport optuna\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold , StratifiedKFold\nimport lightgbm\nfrom sklearn.metrics import confusion_matrix\nimport warnings\nwarnings.filterwarnings(\"ignore\")","252ee4df":"def preprocess(df):\n\n    del df['Unnamed: 0']\n    del df['customerID']\n    df['TotalCharges']=pd.to_numeric(df['TotalCharges'],errors='coerce')\n    \n    \n    #Code labels\n\n    df[\"Churn\"].replace({\"Yes\": 1, \"No\": 0}, inplace=True)\n    df.dropna(axis=0,inplace=True)\n\n    for col in df.columns.tolist():\n        #print(col)\n        if df[col].dtype == object:\n            df[col] = df[col].fillna(df[col].mode()[0])\n            #Get Dummies\n            df = pd.get_dummies(df, columns=[col])\n        else:\n            df[col] = df[col].fillna(df[col].mean())   \n           \n        print(\"done col=\",col)\n    \n   \n    \n    \n    return df","22a12489":"# Loading data\ndata = pd.read_csv('..\/input\/telecom-users-dataset\/telecom_users.csv')\ndata.head()","f324861d":"fig, ax = plt.subplots(figsize=(8, 6))\nsns.countplot(x=data['Churn'],ax=ax)","3469b330":"fig, ax = plt.subplots(figsize=(8, 6))\nsns.countplot(x=data['gender'], hue=data['Churn'],ax=ax)","40a02cbb":"fig, ax = plt.subplots(figsize=(8, 6))\nsns.countplot(x=data['InternetService'], hue=data['Churn'],ax=ax)","a9464efd":"fig, ax = plt.subplots(figsize=(8, 6))\nsns.countplot(x=data['PhoneService'], hue=data['Churn'],ax=ax)","7bf24bcd":"fig, ax = plt.subplots(figsize=(8, 6))\nsns.countplot(x=data['StreamingMovies'], hue=data['Churn'],ax=ax)","b6074498":"fig, ax = plt.subplots(figsize=(8, 6))\nsns.countplot(x=data['StreamingMovies'], hue=data['InternetService'],ax=ax)\nplt.show()","30b4499c":"data.info()","cc1c4338":"data = preprocess(data)","591bb1fd":"labels = data['Churn']\ndel data['Churn']\nX_train,X_test,y_train,y_test = sk.model_selection.train_test_split(data,labels)","a50460a9":"# Feature selection using select from model\n\nimport numpy as np\nfrom sklearn.linear_model import LassoCV\n\nlasso = RandomForestClassifier().fit(X_train, y_train)\nimportance = np.abs(lasso.feature_importances_)\nfeature_names = X_train.columns.tolist()\n(pd.Series(lasso.feature_importances_, index=X_train.columns)\n   .nlargest(4)\n   .plot(kind='barh')) \n\n\n#Feature selection from model Lasso\nfrom sklearn.feature_selection import SelectFromModel\nfrom time import time\n\nthreshold = np.sort(importance)[-40] + 0.01\n\ntic = time()\nsfm = SelectFromModel(lasso, threshold=threshold).fit(X_train, y_train)\n\n","c0392d60":"X_transform = sfm.transform(data)\n\n# Oversampling with smote\nfrom imblearn.over_sampling import SMOTE\noversample = SMOTE()\nX_transform, labels = oversample.fit_resample(X_transform, labels)\n\n\nX_train,X_test,y_train,y_test = sk.model_selection.train_test_split(X_transform,labels)","57d8d0c3":"print(\"Selected Features\")\n\nfor i in range(0,len(sfm.get_support())):\n    if sfm.get_support()[i]==True:\n       print(data.columns.tolist()[i])","a9d224aa":"# Tuning with optuna\ndef objective(trial , data = X_transform , target = labels):\n    X_train , X_test , y_train , y_test = train_test_split(data , target , \\\n            test_size = 0.31 , random_state = 420)\n    \n    params = {\n        'reg_alpha' : trial.suggest_loguniform('reg_alpha' , 1e-5 , 10),\n        'reg_lambda' : trial.suggest_loguniform('reg_lambda' , 1e-5 , 10),\n        'num_leaves' : trial.suggest_int('num_leaves' , 11 , 300),\n        'learning_rate' : trial.suggest_uniform('learning_rate' , 0 , 0.1),\n        'max_depth' : trial.suggest_int('max_depth' , 5 , 20),\n        'n_estimators' : trial.suggest_int('n_estimators' , 1 , 9999),\n        'min_child_samples' : trial.suggest_int('min_child_samples' , 1 , 100),\n        'min_child_weight' : trial.suggest_loguniform('min_child_weight' , 1e-5 , 1),\n        'subsample' : trial.suggest_uniform('subsample' , 0 , 1.0),\n        'colsample_bytree' : trial.suggest_loguniform('colsample_bytree' , 1e-5 , 1),\n        'random_state' : trial.suggest_categorical('random_state' , [0,42,2021,555]),\n        'metric' : 'auc',\n        'device_type' : 'gpu',\n        #'tree_method': 'gpu_hist',\n        #'eval_metric': 'logloss' \n    }\n    model = lightgbm.LGBMClassifier(**params)\n    model.fit(X_train , y_train)\n    preds = model.predict(X_test)\n    tn, fp, fn, tp = confusion_matrix(y_test,preds).ravel()\n    score = tp\/(tp+fp)\n\n    return score","af0b29eb":"study = optuna.create_study(direction = 'maximize' , study_name = 'Gbt')\nstudy.optimize(objective , n_trials = 20)\nprint('numbers of the finished trials:' , len(study.trials))\nprint('the best params:' , study.best_trial.params)\nprint('the best value:' , study.best_value)","f94bf0b5":"import plotly\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","8b8cf912":"optuna.visualization.plot_optimization_history(study)","71565458":"optuna.visualization.plot_param_importances(study)","c47687c8":"optuna.visualization.plot_parallel_coordinate(study)","23bf2ea7":"#model = XGBClassifier(**study.best_trial.params)\nmodel = lightgbm.LGBMClassifier(**study.best_trial.params)\n\nmodel.fit(X_train,y_train)\npred = model.predict(X_test)","0009e172":"fig, ax = plt.subplots(figsize=(10, 10))\nmetrics.plot_confusion_matrix(model,X_test,y_test,ax=ax)\nprint(metrics.accuracy_score(y_test,pred))","7db460d7":"fig, ax = plt.subplots(figsize=(10, 10))\nmetrics.plot_roc_curve(model,X_test,y_test,ax=ax)","9538d8ae":"from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import plot_precision_recall_curve\n\nfig, ax = plt.subplots(figsize=(12, 10))\nplot_precision_recall_curve(model,X_test,y_test,ax=ax)","b6f86516":"Figure above shows that users without internet service are less likely to stop using telecom, due to their inability to access internet without telecom service.","7dafa83a":"Figure above demonstrates that our dataset is imbalanced in terms of number of Churned obs. But, in still worth trying to build model without resampling.","522097d3":"Regarding gender of an user, its evident that number of churns remains stable across all genders"}}