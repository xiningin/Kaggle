{"cell_type":{"b19e17bd":"code","288dbc24":"code","caa1ea22":"code","1ef197ea":"code","8a73e637":"code","cf1f28b1":"code","f8b2eb11":"code","56f85c4a":"code","130eb890":"code","48973e53":"code","0c21653a":"code","02a65d50":"code","d272e76e":"code","a4e878a0":"code","4d6deb9f":"code","76c273a2":"code","708fe9b1":"code","7653636b":"code","730aa20c":"code","90a28d7a":"code","cf618491":"code","2d276e08":"code","5b67eb29":"code","fa8d0061":"code","0f49c120":"code","26ec1777":"code","4f45854b":"code","52af1ed4":"code","9b93dbbd":"code","022bf3a0":"code","a4cf25ce":"code","04e8fc42":"code","3e9edabc":"code","9429667e":"code","5d81935d":"code","d01f4130":"code","2917275a":"code","3b7ff682":"code","09ddeed3":"code","9f40385b":"code","6a629a2c":"code","598b0878":"code","c391a0e3":"code","3b81f915":"code","60bb91cf":"code","0cb1284a":"code","e8ad6158":"code","c7e84443":"code","3075f4fb":"code","d291d340":"code","d60e9403":"code","b0e4e6f3":"code","bfa6296d":"code","d82675b7":"code","a442a685":"code","581a74a0":"code","650cbedf":"code","55a4a97c":"code","a0252cde":"code","bb849ce9":"code","b2a74d85":"code","a066b97a":"code","ea80955f":"code","356300a5":"code","6edf04bc":"code","1e85fae8":"code","b8797780":"code","ea78d2d5":"code","b5f3a9d7":"code","913d136d":"code","0df20b54":"code","027414b4":"code","23210b7f":"code","3b4ad1e4":"code","d64064ce":"code","c47cc6d0":"code","b3d37f12":"code","78a78537":"code","8abaeb42":"code","f9f07fd8":"markdown","de8c09ea":"markdown","a432e8c3":"markdown","f3648e87":"markdown","ecfeef53":"markdown","22aed478":"markdown","eefbfd52":"markdown","4e8d9494":"markdown","1e3b1b30":"markdown","5cee411d":"markdown"},"source":{"b19e17bd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os, re, pathlib, random, string, torch, sklearn, time\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nos.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\";","288dbc24":"# installing libraries in order to use models like BERT, XLNet, RoBERTa, etc\n!pip install simpletransformers\n!pip install openpyxl","caa1ea22":"# additional imports\nfrom simpletransformers.classification import ClassificationModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom simpletransformers.model import TransformerModel\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom mlxtend.plotting import plot_confusion_matrix","1ef197ea":"# Load all datasets used for building training set for anger detection model\njoy_path='..\/input\/emotions\/SentenceWithEmotion_TRAINING_JOY.csv'\nj_path = pathlib.Path(joy_path)\njoy_df=pd.read_csv(j_path, encoding=None, sep=',')\n\nlove_path='..\/input\/emotions\/SentenceWithEmotion_TRAINING_LOVE.csv'\nl_path = pathlib.Path(love_path)\nlove_df=pd.read_csv(l_path, encoding=None, sep=',')\n\nsadness_path='..\/input\/emotions\/SentenceWithEmotion_TRAINING_SADNESS.csv'\nsad_path = pathlib.Path(sadness_path)\nsadness_df=pd.read_csv(sad_path, encoding=None, sep=',')\n\nanger_orig_path='..\/input\/emotions\/SentenceWithEmotion_TRAINING_ANGER.csv'\nanger_original_path = pathlib.Path(anger_orig_path)\nanger_orig_df=pd.read_csv(anger_original_path, encoding=None, sep=',')\n\nanger_path='..\/input\/anger-dir\/golden_standart_anger.csv'\nang_path = pathlib.Path(anger_path)\nanger_df=pd.read_csv(ang_path, encoding=None, sep=',')\n\n# Newly mined jira comments - 200677\nall_path='..\/input\/anger-dir\/df_all.csv'\na_path = pathlib.Path(all_path)\nall_df=pd.read_csv(a_path, encoding=None, sep=',')\n\n# Newly mined jira comments split by sentence - 397245\nall_path_s='..\/input\/anger-dir\/df_all_tokenized.csv'\na_path_s = pathlib.Path(all_path_s)\ndf_new_jira_tokenized = pd.read_csv(a_path_s, encoding=None, sep=',')","8a73e637":"# choose proportion - real proportion is +1\nprop_non_A = 4","cf1f28b1":"# check number of non-anger cases\nnon_anger_cases_an_or_ds = anger_orig_df[anger_orig_df['anger'] != 'x']\nlen(non_anger_cases_an_or_ds)","f8b2eb11":"# sample datasets to divide by emotions\ndf_full = pd.concat([joy_df, love_df, sadness_df, anger_orig_df])\nsampled_joy = (df_full[df_full['joy'] == 'x'])#.sample(n = 240)\nsampled_love = (df_full[df_full['love'] == 'x'])#.sample(n = 240)\nsampled_sadness = (df_full[df_full['sadness'] == 'x'])#.sample(n = 240)\nnumber_cases_emotion = len(sampled_joy)+len(sampled_love)+len(sampled_sadness)\nneutral_cases = df_full[(df_full[\"joy\"] !=\"x\") & (df_full[\"love\"] !=\"x\")\n                        & (df_full[\"sadness\"] !=\"x\") & (df_full[\"anger\"] !=\"x\")]\nsampled_neutral = neutral_cases.sample(n = 723* prop_non_A + (723-number_cases_emotion))\nn1 = neutral_cases[neutral_cases['joy'] == 'x']\nn2 = neutral_cases[neutral_cases['love'] == 'x']\nn3 = neutral_cases[neutral_cases['sadness'] == 'x']\nn4 = neutral_cases[neutral_cases['anger'] == 'x']\nnumber_cases_emotion,len(sampled_neutral),len(sampled_neutral) + number_cases_emotion,len(n1),len(n2),len(n3), len(n4)","56f85c4a":"len(df_full[df_full['love'] == 'x']) +len(df_full[df_full['joy'] == 'x'])+len(df_full[df_full['sadness'] == 'x']),240*3","130eb890":"# combine cases with emotion - joy, love, sadness and neutral\nneutrals_2_to_1 = pd.concat([sampled_joy, sampled_love, sampled_sadness, sampled_neutral])\nneutrals_2_to_1 = neutrals_2_to_1.sample(frac=1)\ndf_neutrals_2_to_1 = neutrals_2_to_1.drop(columns=['love', 'joy', 'surprise', 'anger', 'probability',\n                                        'fear', 'neutral', 'probablity', 'notes', 'sadness'])\ndf_neutrals_2_to_1['emotion'] = 'not_anger'\ndf_neutrals_2_to_1","48973e53":"# drop and rename columns\nanger_df['emotion'] = 'anger'\nanger_df = anger_df.drop(columns=['myanger_direction'])\nanger_df.rename(columns={'myunique_id': 'id', 'mydeveloper_comment': 'comment'}, inplace=True)\nlen(anger_df)","0c21653a":"# combine anger + sampled other cases\n# the name df_anger_y_n_2_to_1 is outdaded. the proportion is determined by the variable: prop_non_A\ndf_anger_y_n_2_to_1 = pd.concat([anger_df, df_neutrals_2_to_1])\nlen(df_anger_y_n_2_to_1)","02a65d50":"# check for duplicates\nduplicateDFRow = df_anger_y_n_2_to_1[df_anger_y_n_2_to_1['id'].duplicated()]\nprint(len(duplicateDFRow))","d272e76e":"# sorting by id\ndf_anger_y_n_2_to_1.sort_values(\"id\", inplace = True)\n\n# making a copy with duplicates for testing\ndf_anger_y_n_2_to_1_dup = df_anger_y_n_2_to_1.copy()","a4e878a0":"# dropping ALL duplicate values\ndf_anger_y_n_2_to_1.drop_duplicates(subset =\"id\", keep = False, inplace = True)\n\n# shuffle the data\ndf_anger_y_n_2_to_1 = df_anger_y_n_2_to_1.sample(frac=1)\nlen(df_anger_y_n_2_to_1)","4d6deb9f":"len(df_anger_y_n_2_to_1_dup), len(df_anger_y_n_2_to_1)","76c273a2":"# check if there are still duplicates\nduplicateDFRow = df_anger_y_n_2_to_1[df_anger_y_n_2_to_1['id'].duplicated()]\nif len(duplicateDFRow)>0:\n    print (True)\nelse:\n    print (False)","708fe9b1":"# Jira dataset with duplicate preprocess\ndf_anger_y_n_2_to_1_dup = df_anger_y_n_2_to_1_dup[['comment','emotion']]\ndf_anger_y_n_2_to_1_dup.replace({'not_anger': 0, 'anger': 1}, inplace=True)\n\n# Jira dataset without duplicate preprocess\ndf_anger_y_n_2_to_1 = df_anger_y_n_2_to_1[['comment','emotion']]\ndf_anger_y_n_2_to_1.replace({'not_anger': 0, 'anger': 1}, inplace=True)","7653636b":"df_anger_y_n_2_to_1_dup['emotion'].value_counts()","730aa20c":"df_full = pd.concat([joy_df, love_df, sadness_df])\nsampled_joy = (df_full[df_full['joy'] == 'x']).sample(n = 121)\nsampled_love = (df_full[df_full['love'] == 'x']).sample(n = 121)\nsampled_sadness = (df_full[df_full['sadness'] == 'x']).sample(n = 120)\nneutral_cases = df_full[(df_full[\"joy\"] !=\"x\") & (df_full[\"love\"] !=\"x\") & (df_full[\"sadness\"] !=\"x\")]\nsampled_neutral = neutral_cases.sample(n = 361)\nn1 = neutral_cases[neutral_cases['joy'] == 'x']\nn2 = neutral_cases[neutral_cases['love'] == 'x']\nn3 = neutral_cases[neutral_cases['sadness'] == 'x']\nlen(n1),len(n2),len(n3)","90a28d7a":"sampled_df = pd.concat([sampled_joy, sampled_love, sampled_sadness, sampled_neutral])\n# shuffle the rows\nsampled_df = sampled_df.sample(frac=1)\ndf_not_anger = sampled_df.drop(columns=['love', 'joy', 'surprise', 'anger',\n                                        'fear', 'neutral', 'probablity', 'sadness'])\ndf_not_anger['emotion'] = 'not_anger'\ndf_not_anger","cf618491":"anger_path='..\/input\/anger-dir\/golden_standart_anger.csv'\nang_path = pathlib.Path(anger_path)\nanger_df=pd.read_csv(ang_path, encoding=None, sep=',')\n\nanger_df['emotion'] = 'anger'\nanger_df = anger_df.drop(columns=['myanger_direction'])\nanger_df.rename(columns={'myunique_id': 'id', 'mydeveloper_comment': 'comment'}, inplace=True)\nanger_df","2d276e08":"df_anger_y_n = pd.concat([anger_df, df_not_anger])\ndf_anger_y_n","5b67eb29":"duplicateDFRow = df_anger_y_n[df_anger_y_n['id'].duplicated()]\nprint(len(duplicateDFRow))","fa8d0061":"# sorting by id\ndf_anger_y_n.sort_values(\"id\", inplace = True)\n\n# making a copy with duplicates for testing\ndf_anger_y_n_dup = df_anger_y_n.copy()","0f49c120":"# dropping ALL duplicate values\ndf_anger_y_n.drop_duplicates(subset =\"id\", keep = False, inplace = True)\n\n# shuffle the data\ndf_anger_y_n = df_anger_y_n.sample(frac=1)\n\ndf_anger_y_n","26ec1777":"len(df_anger_y_n_dup), len(df_anger_y_n)","4f45854b":"# case dataset to csv\ndf_anger_y_n.to_csv ('df_mixed.csv', index = False, header=True)","52af1ed4":"# import it to check if everything is ok\npath_df='.\/df_mixed.csv'\npath_m = pathlib.Path(path_df)\ndf_mixed=pd.read_csv(path_m, encoding=None, sep=',')\ndf_mixed","9b93dbbd":"#check duplicates\nduplicateDFRow = df_mixed[df_mixed['id'].duplicated()]\nif len(duplicateDFRow)>0:\n    print (True)\nelse:\n    print (False)","022bf3a0":"# Jira dataset with duplicate preprocess\ndf_anger_y_n_dup = df_anger_y_n_dup[['comment','emotion']]\ndf_anger_y_n_dup.replace({'not_anger': 0, 'anger': 1}, inplace=True)\ndf_anger_y_n_dup","a4cf25ce":"# Jira dataset without duplicate preprocess\ndf_mixed = df_mixed[['comment','emotion']]\ndf_mixed.replace({'not_anger': 0, 'anger': 1}, inplace=True)\ndf_mixed","04e8fc42":"train_df, test_df = train_test_split(df_mixed, test_size=0.2)\nlen(df_mixed), len(train_df), len(test_df)","3e9edabc":"test_df['emotion'].value_counts()","9429667e":"train_df['emotion'].value_counts()","5d81935d":"Anger_exc = pd.read_excel('..\/input\/anger-excel\/Emotions_GoldSandard_andAnnotation.xlsx',\n                   sheet_name=\"Anger_all\")\nAnger_excel = Anger_exc[Anger_exc['Gold Label'] == 'ANGER']\nFear_exc = pd.read_excel('..\/input\/anger-excel\/Emotions_GoldSandard_andAnnotation.xlsx',\n                   sheet_name=\"Fear_all\")\nFear_excel = Fear_exc[Fear_exc['Gold Label'] == 'FEAR']\nJoy_exc = pd.read_excel('..\/input\/anger-excel\/Emotions_GoldSandard_andAnnotation.xlsx',\n                   sheet_name=\"Joy_all\")\nJoy_excel = Joy_exc[Joy_exc['Gold Label'] == 'JOY']\nLove_exc = pd.read_excel('..\/input\/anger-excel\/Emotions_GoldSandard_andAnnotation.xlsx',\n                   sheet_name=\"Love_all\")\nLove_excel = Love_exc[Love_exc['Gold Label'] == 'LOVE']\nSadness_exc = pd.read_excel('..\/input\/anger-excel\/Emotions_GoldSandard_andAnnotation.xlsx',\n                   sheet_name=\"Sadness_all\")\nSadness_excel = Sadness_exc[Sadness_exc['Gold Label'] == 'SADNESS']\nSurprise_exc = pd.read_excel('..\/input\/anger-excel\/Emotions_GoldSandard_andAnnotation.xlsx',\n                   sheet_name=\"Surprise_all\")\nSurprise_excel = Surprise_exc[Surprise_exc['Gold Label'] == 'SURPRISE']","d01f4130":"excel_all = pd.concat([Anger_excel, Fear_excel, Joy_excel,\n                       Love_excel, Sadness_excel, Surprise_excel])\nexcel_all = excel_all[['Text','Gold Label']]\nexcel_all['emotion'] = (excel_all['Gold Label'] == \"ANGER\").astype(int)\nexcel_all = excel_all.sample(frac=1)\nexcel_all.rename(columns={\"Text\": \"comment\"}, inplace = True)\nexcel_all['Gold Label'].value_counts(), print('Dataset length:',len(excel_all)),","2917275a":"excel_prep = excel_all[['comment','emotion']]\nexcel_prep.reset_index(drop=True, inplace=True)\nexcel_prep['emotion'].value_counts()","3b7ff682":"# check if there are duplicates in the excel dataset - Stack Overflow dataset\nexcel_duplicates = excel_prep[excel_prep['comment'].duplicated()]\nif len(excel_duplicates)>0:\n    print (True, ', there are:', len(excel_duplicates), 'duplicates')\nelse:\n    print (False)","09ddeed3":"# check any duplicate: Anger is 0\nexcel_list_duplicates = excel_duplicates['comment'].to_list()\nexcel_prep[excel_prep['comment'] == excel_list_duplicates[1]]","9f40385b":"# dropping ALL duplicate values from the Stack Overflow dataset\nexcel_no_dup = excel_prep.drop_duplicates(subset =\"comment\", keep = False)\n\nif len(excel_no_dup[excel_no_dup['comment'].duplicated()])>0:\n    print (True)\nelse:\n    print (False, '; Dataset length is:', len(excel_no_dup))","6a629a2c":"excel_no_dup['emotion'].value_counts()","598b0878":"1766+819","c391a0e3":"# with multi-labelled cases\n# Mixing the Jira and Stack Overflow datasets - resulting in Anger : Non-Anger = 3.7:1\nJ_and_SO = pd.concat([excel_prep,df_anger_y_n_2_to_1])\nJ_and_SO['emotion'].value_counts()","3b81f915":"# without multi-labelled cases\n# Mixing the Jira and Stack Overflow datasets - resulting in Anger : Non-Anger = 3.6:1\nJ_and_SO_no_mul = pd.concat([excel_no_dup,df_anger_y_n_2_to_1_dup])\nJ_and_SO_no_mul['emotion'].value_counts()","60bb91cf":"# check if there are still duplicates\n# there are no cases present in both datasets as expected since one is from J, other is from SO\nduplicateDFRow = J_and_SO_no_mul[J_and_SO_no_mul['comment'].duplicated()]\nif len(duplicateDFRow)>0:\n    print (True)\nelse:\n    print (False)","0cb1284a":"# Choosing one of the 4 datasets for anger detection\ndataset = J_and_SO_no_mul\nlen(dataset)","e8ad6158":"# Splitting the dataset into training, test and evaluation sets\nrest_df, test_df = train_test_split(dataset, test_size=0.2)\ntrain_df, eval_df = train_test_split(rest_df, test_size=0.2)\nlen(dataset), len(eval_df), len(train_df), len(test_df), len(eval_df)+len(train_df)+len(test_df)","c7e84443":"# Create a TransformerModel with modified attributes\n\nmodel = TransformerModel('bert', 'bert-base-cased', num_labels=4,\n                         args={'overwrite_output_dir': True})","3075f4fb":"#Train the model\nmodel.train_model(train_df)","d291d340":"result, model_outputs, wrong_predictions = model.eval_model(eval_df)","d60e9403":"def f1_multiclass(labels, preds):\n      return f1_score(labels, preds, average='micro')\n\nresult, model_outputs, wrong_predictions = model.eval_model(eval_df, f1=f1_multiclass, acc=accuracy_score)\nresult","b0e4e6f3":"print('Recal,     ', \"F1 score,   \", \"Acuracy,   \", \"Cross-entropy loss\")","bfa6296d":"# make predictions\nto_predict = test_df['comment'].to_list()\npredictions = model.predict(to_predict)\ntest_df['predictions'] = predictions[0]\ntest_df","d82675b7":"test_df['emotion'].value_counts()","a442a685":"# check false pos and false neg\nfalse_neg = test_df[(test_df[\"emotion\"] ==0) & (test_df[\"predictions\"] ==1)]\nfalse_negatives = false_neg['comment'].to_list()\nfalse_pos = test_df[(test_df[\"emotion\"] ==1) & (test_df[\"predictions\"] ==0)]\nfalse_positives = false_pos['comment'].to_list()\n#len(false_negatives), false_negatives[0], len(false_positives), false_positives[0]","581a74a0":"tn, fp, fn, tp = confusion_matrix(test_df[\"emotion\"].to_list(), test_df[\"predictions\"].to_list()).ravel()\ntn, fp, fn, tp","650cbedf":"Precision = tp\/(tp+fp)\nRecall = tp\/(tp+fn)\nPrecision, Recall","55a4a97c":"# produce confusion matrix\ncf_matrix = confusion_matrix(test_df[\"emotion\"], test_df[\"predictions\"])\nbinary = np.array([[4, 1],\n                   [1, 2]])\n\nfig, ax = plot_confusion_matrix(conf_mat=cf_matrix,\n                                show_absolute=True,\n                                show_normed=True,\n                                colorbar=True,\n                                class_names = ['not anger','anger'])\nfig.show()","a0252cde":"len(df_new_jira_tokenized)#.head(2)","bb849ce9":"# make a copy and drop duplicated\nall_df_test = (df_new_jira_tokenized).copy()\nall_df_test = all_df_test.dropna()\nto_predict_t = (all_df_test['text_tokenized']).to_list()","b2a74d85":"# classify new anger cases\npredictions_t = model.predict(to_predict_t)","a066b97a":"all_df_test['predictions'] = predictions_t[0]","ea80955f":"all_df_test['predictions'].value_counts()","356300a5":"new_anger = all_df_test[all_df_test[\"predictions\"] ==1]\nanger = new_anger['text_tokenized'].to_list()","6edf04bc":"len(anger)","1e85fae8":"# save file to pdf\nall_df_test.to_csv ('test_pred.csv', index = False, header=True)","b8797780":"#load the golden dataset\ngolden_path='..\/input\/anger-dir\/golden_standart_anger.csv'\ngold_path = pathlib.Path(golden_path)\ngold_standart = pd.read_csv(gold_path, encoding=None, sep=',')","ea78d2d5":"# convert the direction column to int\ngold_standart['labels'] = gold_standart['myanger_direction'].replace(['oth','ob','self'],[0,1,2])\ngold_stand = gold_standart[['mydeveloper_comment','labels']]\ngold_stand.values.tolist()","b5f3a9d7":"dataset.isna().any().any()","913d136d":"# Choosing the golden dataset for training the anger direction model\ndataset = gold_stand\nlen(dataset)\n\n# Splitting the dataset into training, test and evaluation sets\nrest_df, test_df = train_test_split(dataset, test_size=0.2)\ntrain_df, eval_df = train_test_split(rest_df, test_size=0.2)\nlen(dataset), len(eval_df), len(train_df), len(test_df), len(eval_df)+len(train_df)+len(test_df)","0df20b54":"train_df, val_df = train_test_split(dataset,test_size =0.2, random_state =500)\nval_df['labels'].value_counts()","027414b4":"# MODEL V1\n\n#setting uo the model arguments\nmodel_type = 'bert' #as this is little bit daster than BERT and gives almost same peformance\nmodel_name = 'bert-base-cased'\nseed = 100\nmodel_args =  {'fp16': False,\n               'train_batch_size': 8,\n               'gradient_accumulation_steps': 2,\n#                'do_lower_case': True,\n               'learning_rate': 2e-5,\n               'overwrite_output_dir': True,\n               'manual_seed': seed,\n               'num_train_epochs': 2}\n\n#buid model & train\n\nmodel = ClassificationModel(model_type, model_name,num_labels=5, args=model_args)","23210b7f":"# MODEL V2\n\n# Create a TransformerModel with modified attributes\n\nmodel = TransformerModel('bert', 'bert-base-uncased', num_labels=4,\n                         args={'overwrite_output_dir': True,\n                               'train_batch_size': 6,\n                               'learning_rate': 2e-5,\n                               'num_train_epochs': 2\n                              })","3b4ad1e4":"model.train_model(train_df,acc=accuracy_score)","d64064ce":"#Validation\n\nresult, model_outputs, wrong_predictions = model.eval_model(val_df,acc=accuracy_score)\n\n#validation result\nresult","c47cc6d0":"#to_pred = np.asarray(anger)\ndf_anger_class = pd.DataFrame(anger, columns =['text'])\ndf_anger_class['text'].head(2)","b3d37f12":"predictions, raw_output = model.predict(df_anger_class['text'].to_list())","78a78537":"# predict the direction of the anger cases\ndf_anger_class['predictions'] = predictions\ndf_anger_class['predictions'].value_counts()","8abaeb42":"anger_2 = df_anger_class[df_anger_class['predictions']==2].reset_index()#['text'][6]\nanger_2['text'].to_list()","f9f07fd8":"#### 723 - golden dataset\n#### 723 - mixed other with:\n   ##### (50% : 361 neutral)\n   ##### (50% specific: 16% 120 joy, 16% 120 love, 16% 120 sadness)","de8c09ea":"## Jira and Stack Overflow","a432e8c3":"## Data from EmotionDatasetMSR18 - Stack Overflow dataset","f3648e87":"### Experimental Jira dataset selection X:Y","ecfeef53":"#### Final dataset options:\n##### excel_prep - Stack Overflow dataset with 'multi-labelled cases'\n##### excel_no_dup - Stack Overflow dataset without 'multi-labelled cases'\n##### df_anger_y_n_dup - Jira dataset with 'multi-labelled cases' consisting of:\n    723 - golden dataset\n    723 - mixed other with:\n        (50% : 361 neutral)\n        (50% specific: 16% 120 joy, 16% 120 love, 16% 120 sadness)\n##### df_mixed - Jira dataset without 'multi-labelled cases' (only difference is lack of 'multi-labelled cases' - around 20 cases)\n##### df_anger_y_n_2_to_1 - Jira dataset with 'multi-labelled cases' consisting of:\n    723 - golden dataset\n    1446 - mixed other with:\n        (50% neutral)\n        (50% specific: 16% joy, 16% love, 16% sadness)\n##### df_anger_y_n_2_to_1_dup - Jira dataset without 'multi-labelled cases' (only difference is lack of 'multi-labelled cases' - around 53\n##### J_and_SO_no_mul - without multi-labelled cases; Mixing the Jira and Stack Overflow datasets - resulting in Anger : Non-Anger = 3.6:1","22aed478":"## Try the model to classify new anger cases","eefbfd52":"### Jira 50:50 dataset for training selection\nthe cells until the next markdown cell are how this dataset is produced","4e8d9494":"#### Make the predictions able to be downloaded \/ new version\n<a href=\".\/test_pred.csv\"> Download File <\/a>","1e3b1b30":"## Classifier for anger direction","5cee411d":"## Classifier for anger detection"}}