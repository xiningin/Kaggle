{"cell_type":{"53f12da6":"code","4ce531f6":"code","5982f423":"code","b67049eb":"code","76672e95":"code","1e990222":"code","023e8cae":"code","626167ac":"code","aee30b7c":"code","431bd858":"code","42db8ca2":"code","45944ca6":"code","5324cf63":"code","5efdf423":"code","2c73cd4e":"code","08f17217":"code","5600c61d":"code","0be130ce":"code","80a0718d":"code","791fbdd0":"code","1ec1ae1b":"markdown","a144b535":"markdown","b2fb4b85":"markdown","376f3324":"markdown","efbcdd8c":"markdown","88aa905e":"markdown","2e810efe":"markdown","592139a7":"markdown","b3ed6dab":"markdown","088ae843":"markdown","d5184b9b":"markdown"},"source":{"53f12da6":"# Tools\nimport os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import metrics # Prediction quality tools\nfrom sklearn.metrics import mean_absolute_error #Error measurer\nfrom sklearn.metrics import accuracy_score # Modeling acccuracy measurer\nimport matplotlib.pyplot as plt # Ploting library\nimport seaborn as sns # Statistical graphics\nfrom sklearn.model_selection import train_test_split # To split train set into training and validation, before prediction\nfrom sklearn.metrics import mean_absolute_error # To test the MAE splitting training dataframe.\nfrom scipy import stats","4ce531f6":"# File opennings\ntest = pd.read_csv(\"\/kaggle\/input\/home-data-for-ml-course\/test.csv\")\ntrain = pd.read_csv(\"\/kaggle\/input\/home-data-for-ml-course\/train.csv\")\ntotalset = pd.concat([train,test])","5982f423":"# Data initial checks:\ntotalset.info()","b67049eb":"sns.scatterplot(train.SalePrice, train.LotArea)","76672e95":"# Dropping outliers of 'LotArea'\ntrain = train.drop(train[(train['LotArea']>50000) | (train['SalePrice']>500000)].index)\nsns.scatterplot(train.SalePrice, train.LotArea)","1e990222":"# Join train and test dataframes to improve visualization and imputations:\ntotalset = pd.concat([train,test])\n\n# Converting all categorical features to numeric labeled:\nfor i in totalset:\n    if np.dtype(totalset[i])==object:\n        totalset[i] = pd.Categorical(totalset[i])\n        totalset[i] = totalset[i].cat.codes\n        \n# Recovering train and test data sets after transformation\ntrain = totalset.loc[totalset.Id<1461]\ntest = totalset.loc[totalset.Id>=1461]","023e8cae":"# Mapping the correlations:\nplt.figure(figsize=(70,30))\nsns.heatmap(train.corr(), annot=True, cmap=plt.cm.Greens)\nplt.show()","626167ac":"mod_features=['OverallQual','1stFlrSF','TotRmsAbvGrd','Fireplaces','MasVnrArea','Foundation','FireplaceQu','BsmtFinSF1',\n'LotArea','OpenPorchSF','WoodDeckSF','2ndFlrSF','GarageCond','HalfBath','GarageQual','CentralAir','Electrical','PavedDrive',\n'SaleCondition','BsmtUnfSF','BsmtFullBath','Neighborhood','RoofStyle','HouseStyle','BedroomAbvGr','BsmtCond','BsmtFinType2',\n'Functional','Exterior2nd','ExterCond','Exterior1st','Condition1','ScreenPorch','RoofMatl','MoSold','Street','3SsnPorch',\n'SaleType','OverallCond','BldgType','MiscFeature','MSSubClass','Alley','Heating','EnclosedPorch','KitchenAbvGr',\n'BsmtExposure','MSZoning','Fence','GarageType','LotShape','GarageFinish','BsmtQual']\ntest[mod_features].info()","aee30b7c":"# Joining the dataframes to improve the estimations\ntotalset = pd.concat([train,test], axis=0)\ntotalset.corrwith(totalset.MasVnrArea).sort_values(ascending=False)","431bd858":"# Picking OverallQual as the estimator because of the best correlation with 'MasVnrArea', which has many 'NaN's:\ntrain[\"MasVnrArea\"].fillna(train.groupby(\"OverallQual\")[\"MasVnrArea\"].transform(\"mean\"), inplace=True)\ntest[\"MasVnrArea\"].fillna(test.groupby(\"OverallQual\")[\"MasVnrArea\"].transform(\"mean\"), inplace=True)","42db8ca2":"test[pd.isnull(test).BsmtFinSF1]    ","45944ca6":"test[pd.isnull(test).BsmtUnfSF]","5324cf63":"test[pd.isnull(test).BsmtFullBath]","5efdf423":"test.at[660, 'BsmtFinSF1']=0\ntest.at[660, 'BsmtUnfSF']=0\ntest.at[660, 'BsmtFullBath']=0\ntest.at[728, 'BsmtFullBath']=0","2c73cd4e":"# Final check of the data sets to ensure that are no missing 'NaN':\ntrain[mod_features].info()\ntest[mod_features].info()","08f17217":"# Importing models to check:\nfrom sklearn.ensemble import GradientBoostingRegressor  # 1st\nfrom sklearn.compose import TransformedTargetRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor # 2nd\nfrom sklearn.ensemble import RandomForestRegressor # 3rd\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.linear_model import GammaRegressor\nfrom sklearn.linear_model import HuberRegressor\nfrom sklearn.linear_model import PassiveAggressiveRegressor\nfrom sklearn.linear_model import PoissonRegressor\nfrom sklearn.linear_model import RANSACRegressor\nfrom sklearn.linear_model import TheilSenRegressor\nfrom sklearn.linear_model import TweedieRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.tree import ExtraTreeRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import ARDRegression\nfrom sklearn.linear_model import LogisticRegression","5600c61d":"# Splitting the train dataframe to check the different possible models:\ntrain_x, val_x, train_y, val_y = train_test_split(train[mod_features],train['SalePrice'],test_size=0.3, random_state=1)","0be130ce":"# Choosing the model:\nMOD=GradientBoostingRegressor(loss='huber',n_estimators=300)\nMOD.fit(train_x,train_y)\nval_pred = MOD.predict(val_x)\nmean_absolute_error(val_pred,val_y).round(decimals=0).astype(int)","80a0718d":"# Final modeling:\nMOD.fit(train[mod_features],train.SalePrice)\nSP_pred = MOD.predict(test[mod_features])","791fbdd0":"# File writing:\noutputs=pd.DataFrame({\"Id\": test['Id'], \"SalePrice\": SP_pred})\noutputs.to_csv('output8.csv', index=False)","1ec1ae1b":"Before start modeling, it is necessary to fill all 'NaN' values of the chosen features:","a144b535":"## Modeling:\n\nSince it is a typical problem to predict future values on a multiple variable and testing ambient, Regressor-type models seem better:","b2fb4b85":"It seems that house 2121 does not have a basement, consequently, let's fill all its 'NaN' values with 0, (same to house 2189 'BsmtFullBath'):","376f3324":"As suspected, above $ 500,000 selling price or 50,000 sq.ft there are sparse points which can confuse the model. So...","efbcdd8c":"## Feature Engineering:","88aa905e":"Since the lot size and sale price are wide open variables, distortions can occur on their relation. A good starting point is to check that:","2e810efe":"There is one NaN related to many basement features. Let's find if it relates to the same property:","592139a7":"Now, both sets have completed features. So, it is possible to model:","b3ed6dab":"Although listed 21 models above, once testing all, it will be seen that the decision-tree-like models perform better. That is the reason to choose GBR, it ensembles decision trees to optimize the loss. Loss function 'huber' was chosen to optimize the loss reduction even more, same with n_estimators=300, increasing the number of trees of the GBR model. More than 300 penalize processing time w\/o score increasing, from 100 (std) to 300, it is possible to optimize MAE.","088ae843":"The heatmap is wide, but once downloading it and checking it in a good image viewer, it can be seen the correlations of 'SalePrice' with all other features. 'OverallQual' has over 80% of correlation with 'SalePrice'.\n\nSo, by removing features with over 50% of correlation with 'OverallQual' in order to avoid noise in the model and also by removing features with less than 5% correlation with 'SalePrice' for the same reason, these are the remaining features to be in the model: ","d5184b9b":"## Initiating..."}}