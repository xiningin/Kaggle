{"cell_type":{"ca66c631":"code","1faa0cfe":"code","ce2816d1":"code","32fbd23b":"code","19dfb590":"code","b61c284e":"code","cd1be5aa":"code","6c80b653":"code","9646f9d2":"code","770edea1":"code","bf6b5b7a":"code","e1790bb4":"code","fddd179e":"code","74a8dec1":"code","24593f03":"code","373ecac4":"code","ec337102":"code","0ebce82e":"code","a5f12214":"code","2f5c02b8":"code","472db104":"code","422a7592":"code","6330ad7a":"code","b52c405a":"code","d6f2e747":"code","62d0fe01":"code","0532fea4":"code","b7c58ff0":"code","a867630a":"code","c4539709":"code","aec40347":"code","386e47de":"code","9e70de3f":"code","e84e5d0c":"code","a07d3afb":"code","60542daf":"code","05ee8a0b":"code","21560132":"code","9e63cba6":"code","ffcb8c20":"code","f01148fe":"code","55efcf2e":"code","207d49f9":"code","7377e84f":"code","4fc28513":"code","679a9634":"markdown","a0d36fb2":"markdown","f3d1f743":"markdown","90eba455":"markdown","f08d0e1c":"markdown","56810f23":"markdown","6ea5b673":"markdown","fb58a492":"markdown","fb2e6908":"markdown","6faa0905":"markdown","a39f6834":"markdown","84acc5f4":"markdown","d5afd171":"markdown","07d211d0":"markdown","1ea21728":"markdown","df8adec8":"markdown","f6e53a13":"markdown","57d8688f":"markdown","06ef9ec5":"markdown","d81cbfb8":"markdown","8b76911a":"markdown","4fa34616":"markdown","2df018a1":"markdown","b7fa968c":"markdown","8f8d6e5b":"markdown","ba29689a":"markdown","1728274e":"markdown","e37efef1":"markdown","8f548f30":"markdown","b57153bd":"markdown","24215e46":"markdown","f00c1a9a":"markdown","4fae9ab9":"markdown","f63d7387":"markdown","2c83120c":"markdown","98832e26":"markdown","0d4b50f8":"markdown"},"source":{"ca66c631":"# Importing the necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\")\nimport seaborn as sns\nimport scipy","1faa0cfe":"# importing the dataset from kaggle datasets\ndataset=pd.read_csv(\"..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv\")","ce2816d1":"# View first five rows\ndataset.head()","32fbd23b":"data=dataset.drop(['age','trtbps','chol','thalachh','oldpeak'],axis=1)\nfor i in data:\n    print(i)\n    \n    print(dataset[i].value_counts())\n    print(\"------------\"*5)","19dfb590":"dataset.head()","b61c284e":"# Taking only Contineous variables\ndata1=dataset.drop(['sex','cp','fbs','restecg','exng','slp','caa','thall','output'],axis=1)","cd1be5aa":"data1.describe()","6c80b653":"# info of the dataset\ndataset.info()","9646f9d2":"\ncorrPearson = dataset.corr(method=\"pearson\")\nfigure = plt.figure(figsize=(10,8))\nsns.heatmap(corrPearson,annot=True,cmap='RdYlGn', vmin=-1, vmax=+1)\nplt.title(\"PEARSON\")\nplt.xlabel(\"COLUMNS\")\nplt.ylabel(\"COLUMNS\")\nplt.show()","770edea1":"# Checking missing values\ndataset.isnull().sum()","bf6b5b7a":"figure = plt.figure(figsize=(13,8))\n\nplt.boxplot(data1)","e1790bb4":"for i in data1:\n    if (i=='trtbps' or i=='chol' or i=='thalachh' or i=='oldpeak'):\n        print(i)\n        q1=data1[i].quantile(0.25)\n        q3=data1[i].quantile(0.75)\n        iqr=q3-q1\n        low=q1-(0.5*iqr)\n        high=q3+(0.5*iqr)\n        data1[i][data1[i]<low]=data1[i].median()\n        data1[i][data1[i]>high]=data1[i].median()\n        ","fddd179e":"plt.boxplot(data1)","74a8dec1":"# we can check all the histogram visualisation\nfigure=plt.figure(figsize=(10,8))\ndataset.hist(figsize=(18,10))","24593f03":"# output distribution\nsns.countplot('output',data=dataset)","373ecac4":"sns.countplot(x='sex',hue='output',data=dataset)","ec337102":"sns.jointplot(data = dataset, x = 'age', y = 'chol', hue = 'output', palette='dark', height = 10, s = 100, alpha = 0.5)","0ebce82e":"figure=plt.figure(figsize=(16,8))\nsns.countplot(x='age',hue='output',data=dataset)","a5f12214":"figure=plt.figure(figsize=(22,8))\nsns.countplot(x='thalachh',hue='output',data=dataset)","2f5c02b8":"dataset.groupby(['sex','output'])['chol'].mean()","472db104":"dataset.groupby(['sex','output'])['thalachh'].mean()","422a7592":"dataset.groupby(['sex','output'])['age'].mean()","6330ad7a":"for i in dataset:\n    print(i,' : ',scipy.stats.skew(dataset[i], axis=0, bias=True))\n    ","b52c405a":"# Dividing the dependent and independent variables.\n# Droping columns \ny=dataset['output']\nx=dataset.drop(['output','sex','fbs','exng'],axis=1)","d6f2e747":"# All independent variables\nx.head()","62d0fe01":"x['age']=x['age'].apply(lambda x: np.log(x))\nx['trtbps']=x['trtbps'].apply(lambda x: 1\/x)\nx['chol']=x['chol'].apply(lambda x: 1\/(x))\nx['thalachh']=x['thalachh'].apply(lambda x: (x))","0532fea4":"x.head()","b7c58ff0":"from sklearn.preprocessing import MinMaxScaler\nscale=MinMaxScaler()\nx=scale.fit_transform(x)\nx=pd.DataFrame(x)","a867630a":"x.head()","c4539709":"from sklearn.model_selection import train_test_split,cross_val_score, cross_val_predict,GridSearchCV,StratifiedKFold\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=15,stratify=y)","aec40347":"# importing the all classification models from libraries\nfrom sklearn.tree import DecisionTreeClassifier   #importing model\nfrom sklearn.neighbors import KNeighborsClassifier #import method\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score,recall_score,confusion_matrix,precision_score,classification_report, plot_confusion_matrix\n","386e47de":"classifier=XGBClassifier()\nclassifier.fit(x_train,y_train)\ny_pre=classifier.predict(x_train)\ny_pred=classifier.predict(x_test)\ny_pre = classifier.predict(x_train)\nprint(\"validation confusion matrix\")\nprint(confusion_matrix(y_test,y_pred))\nprint(\"---------------\"*8)\nprint(\"cross validation score\")\nprint(cross_val_score(classifier,x_test,y_test,cv=10,verbose=False).mean())\nprint(\"---------------\"*8)\nprint(\"Validation Classification report \")\nprint(classification_report(y_test, y_pred))\nprint(\"---------------\"*8)\nprint(\"Training Classification report \")\nprint(classification_report(y_train, y_pre))\n","9e70de3f":"fig, ax = plt.subplots(figsize = (10, 6), dpi = 100)\nplot_confusion_matrix(classifier, x_test, y_test, ax = ax);","e84e5d0c":"\nclassifier = DecisionTreeClassifier(criterion = 'entropy') #creating algorithm\nclassifier.fit(x_train, y_train) #applying on model\ny_pred = classifier.predict(x_test)\ny_pre = classifier.predict(x_train)\nprint(\"validation confusion matrix\")\nprint(confusion_matrix(y_test,y_pred))\nprint(\"---------------\"*8)\nprint(\"cross validation score\")\nprint(cross_val_score(classifier,x_test,y_test,cv=10,verbose=False).mean())\nprint(\"---------------\"*8)\nprint(\"Validation Classification report \")\nprint(classification_report(y_test, y_pred))\nprint(\"---------------\"*8)\nprint(\"Training Classification report \")\nprint(classification_report(y_train, y_pre))\n\n","a07d3afb":"fig, ax = plt.subplots(figsize = (10, 6), dpi = 100)\nplot_confusion_matrix(classifier, x_test, y_test, ax = ax);","60542daf":"classifier=KNeighborsClassifier(n_neighbors=50)     #create algorithm \nclassifier.fit(x_train,y_train)\ny_pred=classifier.predict(x_test) #predicting model\ny_pre = classifier.predict(x_train)\nprint(\"validation confusion matrix\")\nprint(confusion_matrix(y_test,y_pred))\nprint(\"---------------\"*8)\nprint(\"cross validation score\")\nprint(cross_val_score(classifier,x_test,y_test,cv=10,verbose=False).mean())\nprint(\"---------------\"*8)\nprint(\"Validation Classification report \")\nprint(classification_report(y_test, y_pred))\nprint(\"---------------\"*8)\nprint(\"Training Classification report \")\nprint(classification_report(y_train, y_pre))\n","05ee8a0b":"fig, ax = plt.subplots(figsize = (10, 6), dpi = 100)\nplot_confusion_matrix(classifier, x_test, y_test, ax = ax);","21560132":"classifier=LogisticRegression(random_state=10)\nclassifier.fit(x_train,y_train)\ny_pred=classifier.predict(x_test)\ny_pre = classifier.predict(x_train)\nprint(\"validation confusion matrix\")\nprint(confusion_matrix(y_test,y_pred))\nprint(\"---------------\"*8)\nprint(\"cross validation score\")\nprint(cross_val_score(classifier,x_test,y_test,cv=10,verbose=False).mean())\nprint(\"---------------\"*8)\nprint(\"Validation Classification report \")\nprint(classification_report(y_test, y_pred))\nprint(\"---------------\"*8)\nprint(\"Training Classification report \")\nprint(classification_report(y_train, y_pre))\n","9e63cba6":"fig, ax = plt.subplots(figsize = (10, 6), dpi = 100)\nplot_confusion_matrix(classifier, x_test, y_test, ax = ax);","ffcb8c20":"classifier=GaussianNB()\nclassifier.fit(x_train,y_train)\ny_pred=classifier.predict(x_test)\ny_pre = classifier.predict(x_train)\nprint(\"validation confusion matrix\")\nprint(confusion_matrix(y_test,y_pred))\nprint(\"---------------\"*8)\nprint(\"cross validation score\")\nprint(cross_val_score(classifier,x_test,y_test,cv=10,verbose=False).mean())\nprint(\"---------------\"*8)\nprint(\"Validation Classification report \")\nprint(classification_report(y_test, y_pred))\nprint(\"---------------\"*8)\nprint(\"Training Classification report \")\nprint(classification_report(y_train, y_pre))","f01148fe":"fig, ax = plt.subplots(figsize = (10, 6), dpi = 100)\nplot_confusion_matrix(classifier, x_test, y_test, ax = ax);","55efcf2e":"classifier=RandomForestClassifier(n_estimators=1500,criterion='entropy')\nclassifier.fit(x_train,y_train)\ny_pred=classifier.predict(x_test)\ny_pre = classifier.predict(x_train)\nprint(\"validation confusion matrix\")\nprint(confusion_matrix(y_test,y_pred))\nprint(\"---------------\"*8)\nprint(\"cross validation score\")\nprint(cross_val_score(classifier,x_test,y_test,cv=10,verbose=False).mean())\nprint(\"---------------\"*8)\nprint(\"Validation Classification report \")\nprint(classification_report(y_test, y_pred))\nprint(\"---------------\"*8)\nprint(\"Training Classification report \")\nprint(classification_report(y_train, y_pre))\n\n","207d49f9":"fig, ax = plt.subplots(figsize = (10, 6), dpi = 100)\nplot_confusion_matrix(classifier, x_test, y_test, ax = ax);","7377e84f":"classifier=SVC(kernel='poly')\nclassifier.fit(x_train,y_train)\ny_pred=classifier.predict(x_test)\ny_pre = classifier.predict(x_train)\nprint(\"validation confusion matrix\")\nprint(confusion_matrix(y_test,y_pred))\nprint(\"---------------\"*8)\nprint(\"cross validation score\")\nprint(cross_val_score(classifier,x_test,y_test,cv=10,scoring='accuracy').mean())\nprint(\"---------------\"*8)\nprint(\"Validation Classification report \")\nprint(classification_report(y_test, y_pred))\nprint(\"---------------\"*8)\nprint(\"Training Classification report \")\nprint(classification_report(y_train, y_pre))\n","4fc28513":"fig, ax = plt.subplots(figsize = (10, 6), dpi = 100)\nplot_confusion_matrix(classifier, x_test, y_test, ax = ax);","679a9634":"* in above code we are grouping the sex and output with chol.\n* we clearly see that, sex 0(female) and output 0(not get heart attack) peoples are having the average chol is 241.29\n* sex 0(female) and output 1( get heart attack) peoples are having the average chol is 239.54\n* sex 1(male) and output 0(not get heart attack) peoples are having the average chol is 239.73\n*sex 1(male) and output 1(get heart attack) peoples are having the average chol is 236.13\n\n\n* similarlly in below also with different variables ","a0d36fb2":"Checking correlation between the variables.","f3d1f743":"# XGBoost","90eba455":"# In below, I build all classification models and finalised the best model from all models.","f08d0e1c":"# splitting","56810f23":"# KNN","6ea5b673":"* From above boxplot Visualisation we clearly see that having outliers in the data.\n* for removing outliers in below code i replaced the outliers with median.\n","fb58a492":"* By above Visualisation, we obseve that mostly males are getting Heart Attack than Females.","fb2e6908":"# Logistic regression","6faa0905":" * From above we observe that there are no missing values in the data.","a39f6834":"* Mostly the persons are having the age between (41-54) getting the heart attack. ","84acc5f4":"# Random Forest","d5afd171":"In below cell, i am taking only categorical variables for checking the value counts of each categorical variable.\n\nFrom that, we clearly knows the distribution of each label in the variable. ","07d211d0":"# Missing values","1ea21728":"# vizualisations","df8adec8":"* Here i applied MinMaxScaler, to bring the all variables between 0 to 1. ","f6e53a13":"* From above visualisation we observe that there was no correlation between the independent variables.","57d8688f":"# Transformations","06ef9ec5":"# Final model","d81cbfb8":"# Skewness","8b76911a":"* we see that in dataset are having both positive and negative skewness.","4fa34616":"# modeling","2df018a1":"# Correlation","b7fa968c":"* Here splitting the data into training and testing. in this i taken 80% as training data and 20% as testing data Randomly selected. ","8f8d6e5b":"# SVM","ba29689a":"# outliers","1728274e":"* In below i preformed some retuning techniques ","e37efef1":"* from above boxplot we can checkout clearly, the outliers are replaced with median.","8f548f30":"Calculating skewness of the variables","b57153bd":"# Navie Bayes","24215e46":"* By obseving the above vizualisation, we decide that the data should be in normal distribution and balanced. ","f00c1a9a":"from above data we observe that,[ sex,fbs,exng] features are having some imbalance between the labels. So there is a chance to get a bias problem.","4fae9ab9":"# scaling","f63d7387":"* the above visualisation represents between age and chol with output.\n* Mostly the age between 40-60 and chol between 200-270 persons are getting heart attack.","2c83120c":"Support vector machine(SVM) is the final model because of having the high recall score compare to the other models. \nRecall Score= 94%","98832e26":"In the given dataset, [sex,cp,fbs,restecg,exng,slp,caa,thall,output] this categorical variables are already encoded.","0d4b50f8":"# Decision Tree"}}