{"cell_type":{"a0f48f2e":"code","7e90998c":"code","941a6a10":"code","b9537b54":"code","b2737849":"code","554da963":"code","6a4a80d3":"code","401588c9":"code","f4fdce4a":"code","285f1a9e":"code","fd6cbf42":"code","547915ae":"code","e98807af":"code","6de0894c":"code","679ecda5":"code","a7ddad8f":"code","95c32711":"code","0d17b9af":"code","05ef6489":"code","fbc1eb49":"code","b969802d":"code","0f0df61b":"code","ae302b43":"code","62b6c687":"code","57947d8d":"code","f8a8fc47":"markdown","077916f0":"markdown","d35b14f8":"markdown","aec76ad7":"markdown","8e888a04":"markdown"},"source":{"a0f48f2e":"import warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.multiclass import OneVsOneClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import VotingClassifier\n\nimport seaborn as sn\n\nhbeat_signals = pd.read_csv(\"..\/input\/mitbih-arrhythmia-database-de-chazal-class-labels\/DS1_signals.csv\", header=None)\nhbeat_labels = pd.read_csv(\"..\/input\/mitbih-arrhythmia-database-de-chazal-class-labels\/\/DS1_labels.csv\", header=None)\n\nprint(\"+\"*50)\nprint(\"Signals Info:\")\nprint(\"+\"*50)\nprint(hbeat_signals.info())\nprint(\"+\"*50)\nprint(\"Labels Info:\")\nprint(\"+\"*50)\nprint(hbeat_labels.info())\nprint(\"+\"*50)","7e90998c":"hbeat_signals.head()","941a6a10":"hbeat_signals.describe()","b9537b54":"hbeat_signals = hbeat_signals.sub(0.5, axis=0)\nhbeat_signals.describe()","b2737849":"# Collect data of different hheartbeats in different lists\n#class 0\ncl_0_idx = hbeat_labels[hbeat_labels[0] == 0].index.values\ncl_N = hbeat_signals.iloc[cl_0_idx]\n#class 1\ncl_1_idx = hbeat_labels[hbeat_labels[0] == 1].index.values\ncl_S = hbeat_signals.iloc[cl_1_idx]\n#class 2\ncl_2_idx = hbeat_labels[hbeat_labels[0] == 2].index.values\ncl_V = hbeat_signals.iloc[cl_2_idx]\n#class 3\ncl_3_idx = hbeat_labels[hbeat_labels[0] == 3].index.values\ncl_F = hbeat_signals.iloc[cl_3_idx]\n\n# make plots for the different hbeat classes\nplt.subplot(221)\nfor n in range(3):\n    cl_N.iloc[n].plot(title='Class N (0)', figsize=(10,8))\nplt.subplot(222)\nfor n in range(3):\n    cl_S.iloc[n].plot(title='Class S (1)')\nplt.subplot(223)\nfor n in range(3):\n    cl_V.iloc[n].plot(title='Class V (2)')\nplt.subplot(224)\nfor n in range(3):\n    cl_F.iloc[n].plot(title='Class F (3)')\n","554da963":"#check if missing data\nprint(\"Column\\tNr of NaN's\")\nprint('+'*50)\nfor col in hbeat_signals.columns:\n    if hbeat_signals[col].isnull().sum() > 0:\n        print(col, hbeat_signals[col].isnull().sum()) \n","6a4a80d3":"joined_data = hbeat_signals.join(hbeat_labels, rsuffix=\"_signals\", lsuffix=\"_labels\")\n\n#rename columns\njoined_data.columns = [i for i in range(180)]+['class']","401588c9":"joined_data.head()","f4fdce4a":"joined_data.describe()","285f1a9e":"categories_counts = joined_data['class'].value_counts()\nprint(categories_counts)","fd6cbf42":"print(\"class\\t%\")\njoined_data['class'].value_counts()\/len(joined_data)","547915ae":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2,random_state=42)\n\nfor train_index, test_index in split.split(joined_data, joined_data['class']):\n    strat_train_set = joined_data.loc[train_index]\n    strat_test_set = joined_data.loc[test_index]    ","e98807af":"print(\"class\\t%\")\nstrat_train_set['class'].value_counts()\/len(strat_train_set)","6de0894c":"print(\"class\\t%\")\nstrat_test_set['class'].value_counts()\/len(strat_test_set)","679ecda5":"train_df = strat_train_set\ntest_df  = strat_test_set","a7ddad8f":"from sklearn.utils import resample\n\ndf_0 = train_df[train_df['class']==0]\ndf_1 = train_df[train_df['class']==1]\ndf_2 = train_df[train_df['class']==2]\ndf_3 = train_df[train_df['class']==3]\n\ndf_0_downsample = resample(df_0,replace=True,n_samples=10000,random_state=122)\ndf_1_upsample   = resample(df_1,replace=True,n_samples=10000,random_state=123)\ndf_2_upsample   = resample(df_2,replace=True,n_samples=10000,random_state=124)\ndf_3_upsample   = resample(df_3,replace=True,n_samples=10000,random_state=125)\n\ntrain_df=pd.concat([df_0_downsample,df_1_upsample,df_2_upsample,df_3_upsample])","95c32711":"plt.figure(figsize=(10,5))\n\nplt.subplot(2,2,1)\nplt.plot(df_0.iloc[0,:180])\n\nplt.subplot(2,2,2)\nplt.plot(df_1.iloc[0,:180])\n\nplt.subplot(2,2,3)\nplt.plot(df_2.iloc[0,:180])\n\nplt.subplot(2,2,4)\nplt.plot(df_3.iloc[0,:180])\n\nplt.show()","0d17b9af":"from keras.utils.np_utils import to_categorical\n\ntarget_train = train_df['class']\ntarget_test  = test_df['class']\ny_train = to_categorical(target_train)\ny_test  = to_categorical(target_test)","05ef6489":"X_train = train_df.iloc[:,:180].values\nX_test  = test_df.iloc[:,:180].values\n\nX_train = X_train.reshape(len(X_train), X_train.shape[1], 1)\nX_test  = X_test.reshape(len(X_test), X_test.shape[1], 1)","fbc1eb49":"import keras\nfrom keras import models\nfrom keras import layers\n\nmodel = models.Sequential()\nmodel.add(layers.GaussianNoise(0.01, input_shape=(X_train.shape[1], X_train.shape[2])))\n\nmodel.add(layers.Conv1D(64, 16, activation='relu'))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.MaxPool1D(pool_size=4, strides=2, padding=\"same\"))\n\nmodel.add(layers.Conv1D(64, 12, activation='relu'))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.MaxPool1D(pool_size=3, strides=2, padding=\"same\"))\n\nmodel.add(layers.Conv1D(64, 8, activation='relu'))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.MaxPool1D(pool_size=2, strides=2, padding=\"same\"))\n\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dropout(0.1))\nmodel.add(layers.Dense(32, activation='relu'))\nmodel.add(layers.Dropout(0.1))\nmodel.add(layers.Dense(4, activation='softmax'))\n\nprint(model.summary())","b969802d":"model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])","0f0df61b":"from keras.callbacks import EarlyStopping\ncallbacks = [EarlyStopping(monitor='val_loss', patience=8)]\n\nhistory = model.fit(X_train, y_train, callbacks=callbacks, validation_data=(X_test, y_test), epochs = 20, batch_size = 128)","ae302b43":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nepochs = range(1, len(acc)+1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.legend()","62b6c687":"from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\nimport seaborn as sns\n\ny_pred = model.predict_classes(X_test)\n\ny_test_category = y_test.argmax(axis=-1)\n\n# Creates a confusion matrix\ncm = confusion_matrix(y_test_category, y_pred) \n\n# Transform to df for easier plotting\ncm_df = pd.DataFrame(cm,\n                     index   = ['N', 'S', 'V', 'F', ], \n                     columns = ['N', 'S', 'V', 'F', ])\n\nplt.figure(figsize=(10,10))\nsns.heatmap(cm_df, annot=True, fmt=\"d\", linewidths=0.5, cmap='Blues', cbar=False, annot_kws={'size':14}, square=True)\nplt.title('Kernel \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test_category, y_pred)))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","57947d8d":"from sklearn.metrics import classification_report\nprint(classification_report(y_test_category, y_pred, target_names=['N', 'S', 'V', 'F']))","f8a8fc47":"# Heartbeat classification from ECG morphology using Machine learning.\n\n## Motivation\n\nAcording to [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Heart_arrhythmia) \nArrhythmia affects millions of people in the world. In Europe and North America, as of 2014, atrial fibrillation affects about 2% to 3% of the population. Atrial fibrillation and atrial flutter resulted in 112,000 deaths in 2013, up from 29,000 in 1990. Sudden cardiac death is the cause of about half of deaths due to cardiovascular disease and about 15% of all deaths globally. About 80% of sudden cardiac death is the result of ventricular arrhythmias. Arrhythmias may occur at any age but are more common among older people. Arrhythmias are coused by problems with the electrical conduction system of the heart. A number of tests can help with diagnosis including an electrocardiogram (ECG) and Holter monitor. Regarding ECG, the diagnosis is based on the carefully analysis that a specialized doctor perform on the shape and structure of the independent heartbeats. This process is tedious and requires time. \n![ecg](.\/pics\/ecg.png)\n\nIn this work, we aim to classify the heart beats extracted from an ECG using machine learning, based only on the lineshape (morphology) of the individual heartbeats. The goal would be to develop a method that automatically detects anomallies and help for the prompt diagnosis of arrythmia.\n\n## Data\n\nThe original data comes from the [MIT-BIH Arrythmia database](https:\/\/physionet.org\/content\/mitdb\/1.0.0\/). Some details of the dataset are briefly summarized below:\n\n+ 48.5 hour excerpts of two-channel ambulatory ECG recordings\n+ 48 subjects studied by the BIH Arrhythmia Laboratory between 1975 and 1979.\n+ 23 recordings randomly selected from a set of 4000 24-hour ambulatory ECG recordings collected from a mixed at Boston's Beth Israel Hospital.\n+ 25 recordings were selected from the same set to include less common but clinically significant arrhythmias.\n+ Two or more cardiologists independently annotated each record (approximately 110,000 annotations in all).\n\nAlthough the one that its currently being used here is taken from [kaggle](https:\/\/www.kaggle.com\/alexandrefarb\/mitbih-arrhythmia-database-de-chazal-class-labels). In this dataset the single heartbeats from the ECG were extracted using the [Pam-Tompkins algorithm](https:\/\/en.wikipedia.org\/wiki\/Pan-Tompkins_algorithm). Each row of the dataset represents a QRS complex as the one schematically shown below:\n<img src=\".\/pics\/qrs.png\" width=\"300\">\nThese QRS are taken from the MLII lead from the ECG. As observed in the firts figure above, there is also the V1 lead, which is not used in this work.\nFor further details on how the data was generated, the interested can read the original paper by [Chazal et al.](https:\/\/www.ncbi.nlm.nih.gov\/pubmed\/15248536)\n\nThe different arrythmia classes are:\n\n0. Normal\n1. Supraventricular ectopic beat\n2. Ventricular ectopic beat\n3. Fusion Beat\n\nThis means that we are facing a multi-class classification problem with four classes. \n\n## Strategy.\n\nTo achieve our goal we will go the following way:\n\n1. Data standardisation\n2. Selection of three promising ML algorithms.\n3. Fine tunning of the best models\n4. Model comparison\n5. Build  a Neural Network\n6. Compare \n\nRather important, in order to evaluate the performance of our models is to choose the appropiate metrics. In this case we will be checking the confusion matrix and the f1 score with macro averaging.\n\n\n## Data Loading and first insights.\n","077916f0":"Let's have a look at how the data looks like for the different types of heartbeats.","d35b14f8":"This means that there are no missing values to fill. We can now proceed to check if there are some correlations on the data.","aec76ad7":"Let's now check if the train data fulfills the stratified conditions after the split was done.","8e888a04":"Nice, we see that the amount of data with classes 0 to 3 in the train set maps to those from the original data.\n\nWe are ready to pick some ML models to start training with our data.\nWe will use a brute force approach in the sense that we will try several models at once. For each model, we will do a 5-fold cross validation and depending on its metrics we will choose the best among them. To do that we will write a simple function that takes a list of models, and perfom the cross validation for each and prints its metrics, i.e., confusion matrix, precission, recall and f1 score."}}