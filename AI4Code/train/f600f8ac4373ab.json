{"cell_type":{"2d7961b1":"code","f9279d52":"code","86e0dcf4":"code","35741597":"code","0d96214f":"code","f5d92975":"code","d7835095":"code","45f2bf4e":"code","3c60c01b":"code","f2010739":"code","5a4d66c0":"code","7d785224":"code","d998b4b1":"code","b6dd2647":"code","07244f03":"code","deac5e98":"markdown","0466d4b9":"markdown","98867d9f":"markdown","cb4a55c5":"markdown","0d9f07a6":"markdown"},"source":{"2d7961b1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f9279d52":"pip install demoji","86e0dcf4":"import matplotlib.pyplot as plt\nimport keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport tensorflow.keras.utils as ku\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport string\nimport re\nimport demoji\nimport emoji\ndemoji.download_codes()","35741597":"data = pd.read_csv('\/kaggle\/input\/twitter-airline-sentiment\/Tweets.csv')\ndata","0d96214f":"virgin_neg, virgin_eq, virgin_pos = (data['airline_sentiment'].loc[data['airline']=='Virgin America'].value_counts())\nunited_neg, united_eq, united_pos = (data['airline_sentiment'].loc[data['airline']=='United'].value_counts())\nsouthwest_neg, southwest_eq, southwest_pos = (data['airline_sentiment'].loc[data['airline']=='Southwest'].value_counts())\ndelta_neg, delta_eq, delta_pos = (data['airline_sentiment'].loc[data['airline']=='Delta'].value_counts())\nUSairways_neg, USairways_eq, USairways_pos = (data['airline_sentiment'].loc[data['airline']=='US Airways'].value_counts())\namerican_neg, american_eq, american_pos = (data['airline_sentiment'].loc[data['airline']=='American'].value_counts())\n\nprint('Sentiment type by airline:\\n')\nprint('Virgin America: positive =',virgin_pos, 'negative =',virgin_neg, 'neutral =', virgin_eq)\nprint('United: positive =',united_pos, 'negative =',united_neg, 'neutral =', united_eq)  \nprint('Southwest: positive =',southwest_pos, 'negative =',southwest_neg, 'neutral =', southwest_eq)\nprint('Delta: positive =',delta_pos, 'negative =',delta_neg, 'neutral =', delta_eq)\nprint('US Airways: positive =',USairways_pos, 'negative =',USairways_neg, 'neutral =', USairways_eq)\nprint('American: positive =',american_pos, 'negative =',american_neg, 'neutral =', american_eq)","f5d92975":"plt.xlabel('Airline')\nplt.ylabel('Sentiment count')\nX = ['Virgin America', 'United', 'Southwest', 'Delta', 'US Airways', 'American']\npositive = [virgin_pos, united_pos, southwest_pos, delta_pos, USairways_pos, american_pos]\nnegative = [virgin_neg, united_neg, southwest_neg, delta_neg, USairways_neg, american_neg]\nneutral = [virgin_eq, united_eq, southwest_eq, delta_eq, USairways_eq, american_eq]\nbr1 = np.arange(6) \nbr2 = [x + 0.25 for x in br1] \nbr3 = [x + 0.25 for x in br2]\n\nplt.bar(br1, positive, color='b', label='positive', width=0.25)\nplt.bar(br2, negative, color='r', label='negative', width=0.25)\nplt.bar(br3, neutral, color='g', label='neutral', width=0.25)\nplt.xticks([r+0.25 for r in range(6)], X)\nplt.title('Types of sentiment count per airline:')\nplt.legend()\nplt.grid()\nplt.show()\n\n","d7835095":"StopWords = set(stopwords.words('english'))\n\ndef text_preprocess(text):\n    #text = ' '.join([word.lower() for word in text.split() if word.lower() not in StopWords])\n    text = ' '.join([word for word in text.split() if ('@' not in word and 'http' not in word and 'https' not in word and '#' not in word)])\n    trans = str.maketrans('','',string.punctuation)\n    text = text.translate(trans)\n    return text\n\ndata['text'] = data['text'].apply(text_preprocess)\ndata.head()        \n    ","45f2bf4e":"X = data['text']\nX = X.tolist()\ndef lemmatize(data):\n    lemmatizer = WordNetLemmatizer()\n    lem_data = []\n    for text in data:\n        lem_text = ''\n        for word in text.split():\n            word = lemmatizer.lemmatize(word)\n            word = lemmatizer.lemmatize(word, pos='v')\n            lem_text = lem_text + ' ' + word\n        lem_data.append(lem_text)\n    return lem_data\n        \nX_lem = lemmatize(X)\nprint(X_lem)","3c60c01b":"#Handling emojis in data, replacing them with them corresponding text words.\nemoji2text = {}\nfor text in X_lem:\n    emoji2text.update(demoji.findall(text))\n    \nemoji2text","f2010739":"X_lem_demoji = []\nfor text in X_lem:\n    for emoji, txt in emoji2text.items():\n        if emoji in text:\n            text = re.sub(emoji, txt+' ', text)\n    X_lem_demoji.append(text)\nX_lem_demoji","5a4d66c0":"label_encoder = LabelEncoder()\ny = label_encoder.fit_transform(data['airline_sentiment'])\nX_train, X_test, y_train, y_test = train_test_split(X_lem_demoji, y, random_state=0)\ny_train_true = y_train\ny_train = ku.to_categorical(y_train, num_classes=3)\ny_test_true = y_test\ny_test = ku.to_categorical(y_test, num_classes=3)\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_train)\nvocab_size = len(tokenizer.word_index)\nmax_len = 100\ntrain_seq = tokenizer.texts_to_sequences(X_train)\ntrain_pad = pad_sequences(train_seq, maxlen=max_len)\ntest_seq = tokenizer.texts_to_sequences(X_test)\ntest_pad = pad_sequences(test_seq, maxlen=max_len)","7d785224":"model = keras.Sequential([\n    keras.layers.Embedding(vocab_size+1, 300, input_length=max_len),\n    keras.layers.SpatialDropout1D(0.5),\n    keras.layers.Bidirectional(keras.layers.LSTM(64, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)),\n    keras.layers.Bidirectional(keras.layers.LSTM(32, dropout=0.3, recurrent_dropout=0.3)),\n    keras.layers.Dense(300, activation='relu'),\n    keras.layers.Dense(3, activation='softmax')\n])\n\nopt = keras.optimizers.Adam(learning_rate=0.01)\nmodel.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\nhistory = model.fit(train_pad, y_train, epochs=20, batch_size=512, validation_data=(test_pad, y_test))","d998b4b1":"y_pred = model.predict_classes(test_pad)\ny_pred","b6dd2647":"#accuracy with Bi-LSTM neural network\nprint(accuracy_score(y_test_true, y_pred))","07244f03":"#Tfidf Vectorization, predictions using Logistic Regression.\ntfidf = TfidfVectorizer(ngram_range=(1,2), min_df=5, max_df=0.9)\nX_train_tfidf = tfidf.fit_transform(X_train)\nX_test_tfidf = tfidf.transform(X_test)\n\nclf = LogisticRegression(max_iter=1000).fit(X_train_tfidf, y_train_true)\ny_pred = clf.predict(X_test_tfidf)\nprint('Accuracy with Logistic Regression and tfidf Vectorization',accuracy_score(y_test_true, y_pred))","deac5e98":"# III. Text Preprocessing","0466d4b9":"# IV. Tokenization and Lemmatization","98867d9f":"# V. Training model","cb4a55c5":"# II. Some EDA(not really)","0d9f07a6":"# I. Importing data and required libraries"}}