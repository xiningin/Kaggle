{"cell_type":{"63daa26e":"code","89aef08f":"code","20cbfdb0":"code","4d6cc79f":"code","9a514714":"code","a44103e5":"code","91f7b01b":"code","f47e8d42":"code","60d59040":"code","403bd4ae":"code","776c14cf":"code","8963e5b8":"markdown","0954a800":"markdown","04af748f":"markdown"},"source":{"63daa26e":"import pandas as pd\nimport seaborn as sns\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression, Ridge\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.linear_model import  Perceptron,SGDClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,StackingClassifier, VotingClassifier,BaggingClassifier\nfrom sklearn.metrics import f1_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom copy import deepcopy as d\nimport numpy as np\nimport time","89aef08f":"df = pd.read_csv(\"\/kaggle\/input\/csgo-round-winner-classification\/csgo_round_snapshots.csv\")","20cbfdb0":"df.head()","4d6cc79f":"random_state=2021\ndf= df.sample(frac=0.25, random_state=random_state)\ny = df['round_winner']\ny = y.map({'CT': 1, 'T': 0}).astype(int)\nX = pd.get_dummies(df.drop(columns=['round_winner']))\nX,y = X.values, y.values \n","9a514714":"def get_voting_classifier_score(models,X_train,X_test,y_train,y_test):\n    ensemble = VotingClassifier(d(models))\n    ensemble.fit(X_train,y_train)\n    return f1_score(ensemble.predict(X_test),y_test.values)\n\ndef get_stacking_classifier_score(models,X_train,X_test,y_train,y_test):\n    ensemble = StackingClassifier(d(models))\n    ensemble.fit(X_train,y_train)\n    return f1_score(ensemble.predict(X_test),y_test.values)\n\nensemble_models = [(\"p\",Perceptron(random_state=random_state)),('rf',DecisionTreeClassifier(random_state=random_state)),('mlp',MLPClassifier(random_state=random_state)),(\"knc\",KNeighborsClassifier())]\nboosting_models = [CatBoostClassifier(random_state=random_state,logging_level='Silent'),LGBMClassifier(random_state=random_state),XGBClassifier(random_state=random_state)]\n\nmodels = {\n    'voting_classifier' :  VotingClassifier(d(ensemble_models)),\n    'stacking_classifier' : StackingClassifier(d(ensemble_models)),\n    'bagging_classifier_tree' : BaggingClassifier(DecisionTreeClassifier(random_state=random_state)),\n    'bagging_clasifier_boosting' : BaggingClassifier(CatBoostClassifier(random_state=random_state,logging_level='Silent')),\n    'catboost': CatBoostClassifier(random_state=random_state,logging_level='Silent'),\n    'lgboost' : LGBMClassifier(random_state=random_state),\n    'xgboost' : XGBClassifier(random_state=random_state),\n    'perceptron' : Perceptron(random_state=random_state),\n    'decision_tree' : DecisionTreeClassifier(random_state=random_state),\n    'mlp': MLPClassifier(random_state=random_state),\n    'k_neighbours_classifier': KNeighborsClassifier(),\n}\n\ndef score_models(models: dict, X, y,random_state, splits=6):\n    \n    scores_dict = {}\n    \n    for model_name in reversed(list(models.keys())):\n        model = models[model_name]\n        start_time = time.time()\n        scores = train_and_evaluate_model(model,X,y,random_state=random_state,splits=splits)\n        end_time = time.time()\n        scores_dict[model.__class__.__name__] = scores\n        print(f\"Model {model_name} scored {scores.mean()}, it took {end_time-start_time}s\")\n        \n    return scores_dict\n\ndef train_and_evaluate_model(model,X,y,splits,random_state):\n    \n    cumulative_f1_score = []\n    kf = KFold(n_splits=splits,random_state=random_state,shuffle=True)\n    \n    for train_index, test_index in kf.split(X):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        model_ = d(model)\n        model_.fit(X_train,y_train)\n        cumulative_f1_score.append(f1_score(model_.predict(X_test),y_test))\n        \n    return np.array(cumulative_f1_score)","a44103e5":"scores = score_models(models, X, y,random_state=random_state)","91f7b01b":"scores","f47e8d42":"import base64\ncode = \"\"\"\ndef get_voting_classifier_score(models,X_train,X_test,y_train,y_test):\n    ensemble = VotingClassifier(d(models))\n    ensemble.fit(X_train,y_train)\n    return f1_score(ensemble.predict(X_test),y_test.values)\n\ndef get_stacking_classifier_score(models,X_train,X_test,y_train,y_test):\n    ensemble = StackingClassifier(d(models))\n    ensemble.fit(X_train,y_train)\n    return f1_score(ensemble.predict(X_test),y_test.values)\n\nensemble_models = [(\"p\",Perceptron(random_state=random_state)),('rf',DecisionTreeClassifier(random_state=random_state)),('mlp',MLPClassifier(random_state=random_state)),(\"knc\",KNeighborsClassifier())]\nboosting_models = [CatBoostClassifier(random_state=random_state,logging_level='Silent'),LGBMClassifier(random_state=random_state),XGBClassifier(random_state=random_state)]\n\nmodels = {\n    'voting_classifier' :  VotingClassifier(d(ensemble_models)),\n    'stacking_classifier' : StackingClassifier(d(ensemble_models)),\n    'bagging_classifier_tree' : BaggingClassifier(DecisionTreeClassifier(random_state=random_state)),\n    'bagging_clasifier_boosting' : BaggingClassifier(CatBoostClassifier(random_state=random_state,logging_level='Silent')),\n    'catboost': CatBoostClassifier(random_state=random_state,logging_level='Silent'),\n    'lgboost' : LGBMClassifier(random_state=random_state),\n    'xgboost' : XGBClassifier(random_state=random_state),\n    'perceptron' : Perceptron(random_state=random_state),\n    'decision_tree' : DecisionTreeClassifier(random_state=random_state),\n    'mlp': MLPClassifier(random_state=random_state),\n    'k_neighbours_classifier': KNeighborsClassifier(),\n}\n\ndef score_models(models: dict, X, y,random_state, splits=6):\n    \n    scores_dict = {}\n    \n    for model_name in reversed(list(models.keys())):\n        model = models[model_name]\n        start_time = time.time()\n        scores = train_and_evaluate_model(model,X,y,random_state=random_state,splits=splits)\n        end_time = time.time()\n        scores_dict[model.__class__.__name__] = scores\n        print(f\"Model {model_name} scored {scores.mean()}, it took {end_time-start_time}s\")\n        \n    return scores_dict\n\ndef train_and_evaluate_model(model,X,y,splits,random_state):\n    \n    cumulative_f1_score = []\n    kf = KFold(n_splits=splits,random_state=random_state,shuffle=True)\n    \n    for train_index, test_index in kf.split(X):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        model_ = d(model)\n        model_.fit(X_train,y_train)\n        cumulative_f1_score.append(f1_score(model_.predict(X_test),y_test))\n        \n    return np.array(cumulative_f1_score)\n\n\"\"\"\n\ncode_bytes = code.encode('ascii')\nbase64_bytes = base64.b64encode(code_bytes)\nbase64_message = base64_bytes.decode('ascii')\n","60d59040":"print(base64_message)","403bd4ae":"base64_bytes = base64_message.encode('ascii')\nmessage_bytes = base64.b64decode(base64_bytes)\nmessage = message_bytes.decode('ascii')","776c14cf":"print(message)","8963e5b8":"## Czym jest ensemble? \nEnsemble (inaczej metoda grupowania) to metoda uczenia maszynowego, kt\u00f3ra \u0142\u0105czy wiele modeli w celu uzyskania modelu bardziej optymailnego, ni\u017c jego sk\u0142adowe.\nJednym z podstawowych metod ensemblingu jest tak zwane 'G\u0142osowanie Wi\u0119kszo\u015bciowe'. Polega ono na wybraniu najcz\u0119\u015bciej wyst\u0119puj\u0105cej klasy w przypadku klasyfikacji. W przypadku regresji o poodbnej metodzie polegaj\u0105cej na u\u015brednianiu wynik\u00f3w wielu modeli m\u00f3wimy po prostu u\u015brednianie.","0954a800":"# Zadanie Domowe\n- Zrobi\u0107 boxplot dla wi\u0119kszej iloo\u015bci fold\u00f3w.\n- Przygotuj bli\u017cniaczy notebook dla problemu regresji korzystaj\u0105c z boostingu i modeli sklearnowch korzystaj\u0105c ze [zbioru danych](https:\/\/www.kaggle.com\/mokar2001\/house-price-tehran-iran).","04af748f":"### Opis metody - Voting classifier\n\nJedn\u0105 z najbardziej podstawowych metod ensemblingu jest wcze\u015bniej wspomniany ensembling przez glosowanie, wybierana zostaje decyzja wi\u0119kszo\u015bci w przypadfku gdy jest to mo\u017cliwe. Dla problemu klasyfikacji tej metody mo\u017cna u\u017cy\u0107 korzystaj\u0105c z klasy [VotingClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.VotingClassifier.html) z biblioteki scikit learn.\n\n### Opis metody - Stacking classifier\n\nKolejn\u0105 popularn\u0105 metod\u0105 jest [Stacking Classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.StackingClassifier.html). Polega ona na z\u0142\u0105czeniu outputu wszystkich klasyfikator\u00f3w w jeden wektor, nast\u0119pnie model okre\u015blany jako \"ko\u0144cowy klasyfikator\" uczy si\u0119 wybiera\u0107 ostateczny wynik na podstawie wcze\u015bniej wspomnianej listy.\n\n![](https:\/\/www.researchgate.net\/profile\/Bayu-Adhi-Tama\/publication\/318260780\/figure\/fig2\/AS:600512453943296@1520184734369\/Classifier-ensemble-using-stacking.png)\n\n### Opis metody - Bagging classifier\n\nTrzeci\u0105 przyk\u0142adow\u0105 metod\u0105 ensemblingu jest [Bagging Classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.BaggingClassifier.html). Metoda ta polega na rozdzieleniu zbioru treningowego, a nast\u0119pnie trenuje ka\u017cdy klasyfikator na osobnym podzbiorze danych, a nast\u0119pnie z\u0142\u0105czenie ich w spos\u00f3b zaprezentowany w przypadku Voting Classfiera by otrzyma\u0107 ostateczn\u0105 predykcj\u0119. G\u0142\u00f3wn\u0105 si\u0142\u0105 baggingu jest proces randomizacji modeli, kt\u00f3re mog\u0105 by\u0107 dowolnym modelem klasyfikacyjnym (dla problemu klasyfikacji), dzi\u0119ki czemu w przeciwie\u0144stwie do votingu ka\u017cdy model jest w stanie uchwy\u0107i\u0107 osobn\u0105 charakterystyk\u0119 ze zbioru.\n\n![](https:\/\/vitalflux.com\/wp-content\/uploads\/2020\/09\/Screenshot-2020-09-08-at-4.17.30-PM.png)\n\n### Opis metody - Boosting classifier\n\nOstatni\u0105 omawian\u0105 metod\u0105 jest boosting. Sama idea boostingu polega na stworzeniu pocz\u0105tkowego modelu, a nast\u0119pnie douczaniu kolejnych modeli ze zwi\u0119kszon\u0105 wag\u0105 na danych, na kt\u00f3rych poprzednie modele zawiod\u0142y. Na ko\u0144cu dokonuje si\u0119 u\u015bredniania wynik\u00f3w w przypadku regresji lub g\u0142osowania wi\u0119kszo\u015bciowego w przypadku klasyfikacji.\n\n![](https:\/\/ichi.pro\/assets\/images\/max\/724\/1*85QHtH-49U7ozPpmA5cAaw.png)"}}