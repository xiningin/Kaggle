{"cell_type":{"64082ec2":"code","14d002ed":"code","6116fd56":"code","6c72fb8d":"code","b302c777":"code","8fcd35d0":"code","e5611069":"code","082c1e16":"code","12f790a4":"code","d9be13b9":"code","1c0c6b2b":"code","67cda3bc":"code","e1515a40":"code","98a70ba0":"code","361ede4b":"code","2300df6c":"code","0d396b2c":"code","9bb3c387":"markdown","7cd571a8":"markdown","0eb98a09":"markdown","a92a190c":"markdown","a175aa8e":"markdown","0378d553":"markdown","1497f142":"markdown","ad7792aa":"markdown","8a6a017c":"markdown","4634b2bf":"markdown"},"source":{"64082ec2":"import numpy as np \nimport pandas as pd \nimport os\nimport json\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.auto import tqdm\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_scheduler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom torch.utils.data import DataLoader\nimport torch\nfrom datasets import Dataset\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","14d002ed":"list_dic = []\nwith open('\/kaggle\/input\/news-category-dataset\/News_Category_Dataset_v2.json') as json_file:\n    for the_file in json_file:\n        list_dic.append(json.loads(the_file))","6116fd56":"len(list_dic)","6c72fb8d":"data = pd.DataFrame(list_dic)","b302c777":"data.head(3)","8fcd35d0":"plt.figure(figsize=(13, 7))\nplt.title('Amount of News Based On Category')\nsns.countplot(data=data, x='category')\nplt.xticks(rotation=90)\nplt.tight_layout()","e5611069":"date = data.date.value_counts().index.sort_values()\nnews_on_category = {}\ncategory = data.category.value_counts().index[:3]\n\nprogress_bar = tqdm(range(len(date) * len(category)))\n\nfor c in category:\n    the_list = []\n    for d in date:\n        val = data[(data['category'] == c) & (data['date'] == d)]\n        the_list.append(len(val))\n        progress_bar.update(1)\n    news_on_category[c] = the_list\n    \ndf_the_list = pd.DataFrame(news_on_category, index=date)","082c1e16":"plt.figure(figsize=(15, 7))\nsns.lineplot(data=df_the_list)","12f790a4":"labels = data['category']\ntext = [data.headline[i] + '. ' + data.short_description[i] for i in range(len(data))]\n\ndataset = pd.DataFrame({'text':text, 'labels':labels})","d9be13b9":"tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\ndef tokenize_function(inp):\n    return tokenizer(inp['text'], padding='max_length', truncation=True)\n\ndef make_dataloader(labs, size_train, size_test, batch_size):\n    train_per_label = int(size_train \/ len(labs))\n    test_per_label = int(size_test \/ len(labs))\n    size = train_per_label + test_per_label\n    \n    train_dataset = pd.DataFrame()\n    test_dataset = pd.DataFrame()\n    \n    for l in labs:\n        add = dataset[dataset.labels == l].sample(size)\n        train_add = add.head(train_per_label)\n        test_add = add.iloc[train_per_label:test_per_label, :]\n        train_dataset = pd.concat([train_dataset, train_add])\n        test_dataset = pd.concat([test_dataset, test_add])\n        \n    train_dataset = train_dataset.sample(frac=1)\n    test_dataset = test_dataset.sample(frac=1)\n    \n    label_encoder = LabelEncoder().fit(train_dataset['labels'])\n    train_dataset['labels'] = label_encoder.transform(train_dataset['labels'])\n    test_dataset['labels'] = label_encoder.transform(test_dataset['labels'])\n    \n    train_dataset = Dataset.from_pandas(train_dataset)\n    test_dataset = Dataset.from_pandas(test_dataset)\n    \n    tokenized_train = train_dataset.map(tokenize_function, batched=True)\n    tokenized_test = test_dataset.map(tokenize_function, batched=True)\n\n    tokenized_train = tokenized_train.remove_columns(['__index_level_0__', 'text'])\n    tokenized_test = tokenized_test.remove_columns(['__index_level_0__', 'text'])\n\n    tokenized_train.set_format('torch')\n    tokenized_test.set_format('torch')\n\n    train_dataloader = DataLoader(tokenized_train, batch_size=batch_size)\n    test_dataloader = DataLoader(tokenized_test, batch_size=batch_size)\n    \n    return train_dataloader, test_dataloader, label_encoder","1c0c6b2b":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\ndef build_a_model(model, optimizer, num_epochs, the_dataloader, the_scheduler, num_training_steps):\n    num_training_steps = len(the_dataloader) * num_epochs\n    progress_bar = tqdm(range(num_training_steps))\n    model = model.to(device)\n    model.train()\n    for epoch in range(num_epochs):\n        for batch in the_dataloader:\n            batch = {k:v.to(device) for k, v in batch.items()}\n            outputs = model(**batch)\n            loss = outputs.loss\n            loss.backward()\n            optimizer.step()\n            the_scheduler.step()\n            optimizer.zero_grad()\n            progress_bar.update(1)\n    return model","67cda3bc":"labels_used = list(data.category.value_counts().index)\ntrain_dataloader, test_dataloader, encoder = make_dataloader(labels_used, 41000, 41, 10)","e1515a40":"category = list(encoder.classes_)\nclasses = [i for i in range(len(category))]\nencoding = pd.DataFrame({'category':category, 'classes':classes})\nencoding.to_pickle('.\/encoder_1.pkl')","98a70ba0":"model = AutoModelForSequenceClassification.from_pretrained('bert-base-cased', num_labels=len(labels_used))\noptimizer = AdamW(model.parameters(), lr=5e-5)\nnum_epochs = 5\nnum_training_steps = len(train_dataloader) * num_epochs\nlr_scheduler = get_scheduler('linear', optimizer=optimizer, num_warmup_steps=0, \n                             num_training_steps=num_training_steps)\n\nmodel = build_a_model(model, optimizer, num_epochs, train_dataloader, lr_scheduler, num_training_steps)","361ede4b":"test_dataset = dataset.sample(2000)\ntest_dataset['labels'] = encoder.transform(test_dataset['labels'])\ntest_dataset = Dataset.from_pandas(test_dataset)\ntest_dataset = test_dataset.remove_columns('__index_level_0__')\n\ntokenized_test = test_dataset.map(tokenize_function, batched=True)\ntokenized_test = tokenized_test.remove_columns('text')\ntokenized_test.set_format('torch')\n\ntest_dataloader = DataLoader(tokenized_test, batch_size=10)","2300df6c":"progress_bar = tqdm(range(len(test_dataloader)))\ntargets = []\npredictions = []\n\nmodel.eval()\nfor batch in test_dataloader:\n    labels = batch['labels']\n    batch.pop('labels', None)\n    batch = {k:v.to(device) for k, v in batch.items()}\n    with torch.no_grad():\n        outputs = model(**batch)\n    logits = outputs.logits\n    preds = torch.argmax(logits, dim=-1).cpu().numpy()\n    for t, p in zip(labels, preds):\n        targets.append(t)\n        predictions.append(p)\n    progress_bar.update(1)\n    \nvalid_score = accuracy_score(targets, predictions)\nprint('accuracy of valid is {:.3f}'.format(valid_score))","0d396b2c":"torch.save(model.state_dict(), '.\/model_1.pt')","9bb3c387":"Now, we will see how many data from each categories that we have.","7cd571a8":"The accuracy that we get is 67.4%. Not so good but this also trained from a short text. There are also 40 group classification here which we can say too many. If we want to get a better performance, one of the most obvious way to do is making the text longer. By doing this, we can make the machine understand more about the context. Not like us, who understand sarcasm and names of athletes or celebrities, machine unfortunately doesn't. We all know that to make good classification, we need to understand the information in the text. ","0eb98a09":"We can see from above, that news about politics, entertainment and wellness are dominating. To be honest, if politics and entertainement are two genres of news that are the most dominant, it doesn't surprise me at all but wellnes? Come on... Anyway it just proved that people are struggling these days huh. ","a92a190c":"# Importing Libraries\n\nImport all the necessary libraries. In this project we used a pretrained bert model and train them using pyTorch. So we also imported transformers and torch libraries in order to do that. ","a175aa8e":"# Exploratory Data Analysis","0378d553":"Then what if we looked at the trend of these top 3 news category.","1497f142":"# Building Model\nWe use **Bert-Base-Cased** as our pretrained model here. It is reliable and quite light to be trained too.\n\nThe steps that we will go through to train the model is :\n\n**1**. Preparing to dataframe with only text and labels. Text here is headline added with short description.\n\n**2**. Selecting labels.\n\n**3**. Selecting training data and test data from the dataframe based on the labels.\n\n**4**. Tokenize the training data and test data\n\n**5**. Make train dataloader and test dataloader\n\n**6**. Load pre-trained model, optimizer and learning rate scheduler if you need.\n\n**7**. Train the model.\n\n**8**. Evaluate the model.\n\n**9**. Save the model.","ad7792aa":"# Evaluate","8a6a017c":"# Importing Data","4634b2bf":"From the above, we can see that the the data that we have isn't complete. It can't be a good representation of news that circling around us everyday. The news about wellness are more than news about politics and enterteinment before one point of time then suddenly it became zero until the end of the trend. Something peculiar that can be a proof that this data is not a good representation. \n\nAnyway, we are here not to analyze the trend but to analyze the content to later predict what the category the news would go into. "}}