{"cell_type":{"93f93c4b":"code","21d663ef":"code","2a34b3ff":"code","21d4811b":"code","2fbf1c9f":"code","66191d90":"code","a79bcc81":"code","403e5979":"code","5e8c7d2a":"code","f7bac413":"code","488304ac":"code","e541347d":"code","2868496b":"code","cdc62167":"code","0e37b0e7":"code","c8c92fbe":"code","8d21e5c8":"code","3dede96d":"code","067312c8":"code","43a9ee8e":"code","0593d75a":"code","f7800a48":"code","be2e3ba2":"code","6673df31":"code","86097c67":"code","b2a4d1eb":"code","a0ccb73e":"code","6c02a400":"code","7748edfe":"code","751bc726":"code","2ccc587c":"code","44e99fcd":"code","54c3dece":"code","81fe36b2":"code","00d0a7af":"code","456ce567":"code","5c4dc3a8":"code","cd8e33c4":"code","7eaaae12":"code","e04ad5e6":"code","aa159ecb":"code","4e9ceb71":"code","a2814d8c":"code","552b0edd":"code","d8dfe134":"code","bea73653":"markdown","d964f809":"markdown","20ce3095":"markdown","00c9dfad":"markdown","2d7b97fa":"markdown","1c315b16":"markdown","a1445758":"markdown","38384fa2":"markdown","7a55b1f9":"markdown","47bf434c":"markdown","0ffeed4e":"markdown","29963ddd":"markdown","6360eb9a":"markdown","d9ff414e":"markdown","8fa8ca2e":"markdown","ee68a2ab":"markdown","e961b5c8":"markdown","237b1c9e":"markdown","d0777898":"markdown","1ee0e2c3":"markdown","699dd7d2":"markdown","a7cb28ba":"markdown","643de663":"markdown","4a9beca1":"markdown","a3f98827":"markdown","5ac7700f":"markdown","3b4f8a6c":"markdown","5abf2537":"markdown","14369db7":"markdown","7b20d590":"markdown","3ef395b7":"markdown","d31fef57":"markdown","06503fa9":"markdown"},"source":{"93f93c4b":"%config Completer.use_jedi = False # for auto completion although doesn't work always\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns # data visualization library\nimport matplotlib.pyplot as plt # data visualization library\nimport scipy.stats as stats # library of statistical functions\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline # Pipeline of transforms with a final estimator\nfrom sklearn.feature_selection import SelectFromModel # Selecting features based on importance weights.\nfrom sklearn.model_selection import train_test_split # Splitting the dataset into the Training set and Test set\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport warnings\nwarnings.filterwarnings(\"ignore\") # warnings filter to never print matching warnings","21d663ef":"df = pd.read_csv('\/kaggle\/input\/automobile-dataset\/Automobile_data.csv', na_values = ['?'])","2a34b3ff":"df_eda=df.copy()","21d4811b":"df_eda.head()","2fbf1c9f":"df_eda.info()","66191d90":"df_eda.isnull().sum()","a79bcc81":"dict = {}\nfor i in list(df_eda.columns):\n    dict[i] = df_eda[i].value_counts().shape[0]\n\npd.DataFrame(dict,index=[\"unique count\"]).transpose()","403e5979":"# separating the features and target \nX = df_eda.drop(['price'],axis=1)\ny = df_eda[['price']]","5e8c7d2a":"col_cat = list(df_eda.dtypes[df_eda.dtypes == np.object].index)\ncol_num = list(df_eda.dtypes[df_eda.dtypes == np.number].index)\nprint(col_cat)\nprint(col_num)","f7bac413":"fig, axs = plt.subplots(len(col_num))\nfig.set_figwidth(8)\nfig.set_figheight(45)\ni=0\nfor col in col_num:\n    sns.boxplot(y=df_eda[col], ax=axs[i])\n    i=i+1","488304ac":"def boxplot_limit(x):\n    Q1 = x.quantile(0.25)\n    Q3 = x.quantile(0.75)\n    IQR = Q3 - Q1\n    Upper_Fence = Q3 + (1.5 * IQR)\n    Lower_Fence = Q1 - (1.5 * IQR)\n    max_boxplot = max(x[x<Upper_Fence])\n    min_boxplot = min(x[x>Lower_Fence])\n    return [max_boxplot, min_boxplot]","e541347d":"def number_of_outliers(col,X):\n    outlier =[]\n    up_lim = boxplot_limit(X[col])[0]\n    low_lim = boxplot_limit(X[col])[1]\n    for x in X[col]:\n        if ((x> up_lim) or (x<low_lim)):\n             outlier.append(x)\n    return len(outlier)","2868496b":"col_num2 = col_num.copy()\ncol_num2.remove('price')\nfor col in col_num2:\n    num_outliers = number_of_outliers(col,df_eda)\n    if num_outliers>0 and num_outliers<10:\n        df_eda.drop(df_eda[df_eda[col] > boxplot_limit(df_eda[col])[0]].index,inplace=True)\n        df_eda.drop(df_eda[df_eda[col] < boxplot_limit(df_eda[col])[1]].index,inplace=True)\n        print(df_eda.shape)","cdc62167":"print(df_eda.shape)","0e37b0e7":"fig, axs = plt.subplots(len(col_num))\nfig.set_figwidth(8)\nfig.set_figheight(45)\ni=0\nfor col in col_num:\n    sns.boxplot(y=df_eda[col], ax=axs[i])\n    i=i+1","c8c92fbe":"from pandas.plotting import scatter_matrix\nsns.pairplot(df_eda[col_num],height = 2.5,aspect = 1, corner= True)","8d21e5c8":"# corrmat = df_eda.corr()\n# f, ax = plt.subplots(figsize=(15, 15))\n# matrix = np.triu(df_eda.corr())\n# sns.heatmap(corrmat, square=True, annot=True, fmt='.1g',  cbar=False, mask=matrix)","3dede96d":"# copying Price column\nprice = df_eda['price']\n# Create correlation matrix\ncorr_matrix = df_eda.corr().abs()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find features with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(upper[column] > 0.8)]\n\n# Drop features \ndf_eda.drop(to_drop, axis=1, inplace=True)\n\n#Adding Price column back\ndf_eda['price'] = price","067312c8":"print(df_eda.info())\ncol_remove_corr = df_eda.columns.tolist()","43a9ee8e":"df_eda = df_eda[df_eda['price'].notna()]","0593d75a":"# df_eda = df_eda[df_eda['num-of-doors'].notna()]","f7800a48":"col_cat = list(df_eda.dtypes[df_eda.dtypes == np.object].index)\ncol_num = list(df_eda.dtypes[df_eda.dtypes == np.number].index)","be2e3ba2":"imputer = SimpleImputer(missing_values=np.nan, strategy='median')\ndf_eda[col_num] = imputer.fit_transform(df_eda[col_num])\ndf_eda.info()","6673df31":"imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\ndf_eda[col_cat] = imputer.fit_transform(df_eda[col_cat])\ndf_eda.info()","86097c67":"df_eda.skew(axis = 0, skipna = True)","b2a4d1eb":"# skewCol = ['wheel-base','compression-ratio']\n\n# fig, axs = plt.subplots(2,2)\n# fig.set_figwidth(8)\n# fig.set_figheight(10)\n# i=0\n# for col in skewCol:\n#     sns.histplot(y=df_eda[col], ax=axs[i,0],kde=True)\n#     stats.probplot(df_eda[col], dist=\"norm\", plot=axs[i,1])\n#     i=i+1","a0ccb73e":"# col = 'wheel-base'\n# fig, axs = plt.subplots(3)\n# fig.set_figwidth(8)\n# fig.set_figheight(15)\n# sns.kdeplot(df_eda[col],color='Purple',fill=True, ax=axs[0])\n# print(\"Old skew of %s: %.2f\" % (col,df_eda[col].skew(axis = 0, skipna = True)))\n# # Removing the skewness using a log function and checking the distribution again\n# df_eda[col] = df_eda[col].map(lambda i : np.log(i) if i > 0 else 0)\n# sns.kdeplot(df_eda[col],color='Orange',fill=True, ax=axs[1])\n# stats.probplot(df_eda[col], dist=\"norm\", plot=axs[2])\n# print(\"New skew of %s: %.2f\" % (col,df_eda[col].skew(axis = 0, skipna = True)))","6c02a400":"# col = 'compression-ratio'\n# fig, axs = plt.subplots(3)\n# fig.set_figwidth(8)\n# fig.set_figheight(15)\n# sns.kdeplot(df_eda[col],color='Purple',fill=True, ax=axs[0])\n# print(\"Old skew of %s: %.2f\" % (col,df_eda[col].skew(axis = 0, skipna = True)))\n# # Removing the skewness using a boxcox function and checking the distribution again\n# df_eda[col] = stats.boxcox(df_eda[col])[0]\n# sns.kdeplot(df_eda[col],color='Orange',fill=True, ax=axs[1])\n# stats.probplot(df_eda[col], dist=\"norm\", plot=axs[2])\n# print(\"New skew of %s: %.2f\" % (col,df_eda[col].skew(axis = 0, skipna = True)))","7748edfe":"global features_col\nglobal features_cat\nglobal features_num","751bc726":"class RemoveOutlier():\n    def __init__(self):\n        pass\n    def fit(self, X, y=None):\n        \n        return self\n    def transform(self,X, y=None):\n        return X","2ccc587c":"class RemoveCorrelatedColumns():\n    def __init__(self):\n        pass\n    def fit(self, X, y=None):\n        return self\n    def transform(self,X, y=None):\n        global features_col\n        global features_cat\n        global features_num\n        if ('price' in col_remove_corr):\n            col_remove_corr.remove('price')\n        X = X[col_remove_corr]\n        # Updating the feature lists\n        features_col = list(X.columns)\n        features_cat = list(X.dtypes[X.dtypes == np.object].index)\n        features_num = list(X.dtypes[X.dtypes == np.number].index)\n#         print(X.info())\n        return X","44e99fcd":"class ImputeColumns():\n    def __init__(self):\n        imputer_numeric = SimpleImputer(missing_values=np.nan, strategy='median')\n        self.imputer_numeric = imputer_numeric\n        imputer_category = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n        self.imputer_category = imputer_category\n    def fit(self, X, y=None):\n#         print(\"\\nImputeColumns fit:\")\n        self.imputer_numeric.fit(X[features_num].values)\n        self.imputer_category.fit(X[features_cat].values)\n        return self\n    def transform(self, X, y=None):\n#         print(\"\\nImputeColumns trans:\")\n        X[features_num] = self.imputer_numeric.transform(X[features_num].values)\n        X[features_cat] = self.imputer_category.transform(X[features_cat].values)\n#         print(X.shape)\n        return X","54c3dece":"class SkewCorrection():\n    def __init__(self):\n        pass\n    def fit(self, X, y=None):\n#         print(\"\\nSkewCorrection fit:\")\n        return self # nothing else to do\n    def transform(self, X):\n#         print(\"\\nSkewCorrection trans:\")\n        # Removing the skewness using a log function\n        X['wheel-base'] = X['wheel-base'].map(lambda i : np.log(i) if i > 0 else 0)\n        # Removing the skewness using a boxcox function\n        X['compression-ratio'] = stats.boxcox(X['compression-ratio'])[0]\n#         print(X.shape)\n        return X","81fe36b2":"class CategoryEncoder():\n    def __init__(self):\n        enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n        self.enc = enc\n        self.column_name = []\n    def fit(self, X, y=None):\n        self.enc.fit(X[features_cat].values)\n        self.column_name=self.enc.get_feature_names(features_cat)\n        return self\n    def transform(self, X, y=None):\n        print(\"\\nCategoryEncoder:\")\n#         X.reset_index(inplace=True)\n        df_one_hot = self.enc.transform(X[features_cat].values)\n        df_one_hot = pd.DataFrame(df_one_hot, columns=self.column_name)\n        X = pd.concat([X[features_num],df_one_hot],axis=1)\n#         print(X.head(10))\n#         print(X.info())\n        return X","00d0a7af":" \nclass FeatureScaler():\n    def __init__(self):\n        sc = StandardScaler()\n        self.sc=sc\n    def fit(self, X, y=None):\n        self.sc.fit(X[features_num])\n        return self\n    def transform(self, X, y=None):\n#         print(\"\\nFeatureScaler:\")\n        X[features_num] = self.sc.transform(X[features_num])\n#         print(X.info())\n        return X","456ce567":"# Choosing important features (feature importance)\nfrom sklearn.ensemble import RandomForestRegressor\nclass FeatureSelector():\n    def __init__(self):\n        rfr = RandomForestRegressor(random_state=42)\n        sfm = SelectFromModel(rfr, threshold=0.01)\n        self.sfm = sfm\n        self.feature_name = []\n    def fit(self, X, y):\n#         print(\"\\nFeatureSelector fit:\")\n        self.sfm.fit(X,y)\n        feature_idx = self.sfm.get_support()\n        self.feature_name = X.columns[feature_idx]\n        return self\n    def transform(self, X, y=None):\n#         print(\"\\nFeatureSelector:\")\n        X = self.sfm.transform(X)\n        X = pd.DataFrame(X, columns=self.feature_name)\n#         print(X.head())\n        return X","5c4dc3a8":"df = df[df['price'].notna()]","cd8e33c4":"from sklearn.model_selection import train_test_split\ndf_train, df_test = train_test_split(df, test_size = 0.2, random_state = 42)\ndf_train.reset_index(inplace=True)\ndf_test.reset_index(inplace=True)","7eaaae12":"# separating the features and target \nX_train = df_train.drop(['price'],axis=1)\ny_train = df_train[['price']]\nX_test = df_test.drop(['price'],axis=1)\ny_test = df_test[['price']]","e04ad5e6":"features_col = list(X_train.columns)\nfeatures_cat = list(X_train.dtypes[X_train.dtypes == np.object].index)\nfeatures_num = list(X_train.dtypes[X_train.dtypes == np.number].index)","aa159ecb":"print(\"\\nPipeline:\")\ntrain_pipeline = Pipeline([\n    ('remove_correlated_columns', RemoveCorrelatedColumns()),\n    ('imputer', ImputeColumns()),\n    ('skew_correction', SkewCorrection()),\n    ('std_scaler', FeatureScaler()),\n    ('one_hot_encoder', CategoryEncoder()),\n    ('feature_selector',FeatureSelector())\n#     ('pca', PCA(n_components=0.95))\n#     ('lda', LinearDiscriminantAnalysis(n_components=1))\n], verbose=True)\n\ntrain_pipeline.fit(X=X_train,y=y_train)\nX_train = train_pipeline.transform(X=X_train)\nprint(X_train.info())","4e9ceb71":"from sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error","a2814d8c":"from sklearn.model_selection import cross_val_score\nmodels = [\n    ('LinearRegression', LinearRegression()),\n    ('SVR',SVR(kernel = 'rbf')),\n    ('DecisionTreeRegressor',DecisionTreeRegressor(random_state = 0)),\n    ('RandomForestRegressor',RandomForestRegressor(random_state = 0)),\n    ('GradientBoostingRegressor', GradientBoostingRegressor(random_state = 0))\n]\n\n\n\nprint(\"The mean squared error of the models are :\")\nfor model_name, model in models:\n    model.fit(X_train, y_train)\n    \n    # Reversing Log transformation\n    # The R2 score and MSE are too high, this is because the model is predicting one value to be extreamly high.\n    # By replacing it with 0 the R2 score and MSE improved\n#     y_pred[y_pred>100]=0\n    #  Cross Validation average score on Training Data\n    mse = cross_val_score(estimator = model, scoring='neg_mean_squared_error',\n                          X = X_train, y = y_train, cv = 10)\n    r2_score = cross_val_score(estimator = model, scoring='r2',\n                          X = X_train, y = y_train, cv = 10)\n    print(model_name, \": MSE : \", mse.mean()*100)\n    print(model_name, \": MSE STD: \", mse.std()*100)\n    print(model_name, \": R2 Score: \", r2_score.mean()*100)\n    print(model_name, \": R2 Score STD: \", r2_score.std()*100)","552b0edd":"X_train2, X_val, y_train2, y_val = train_test_split(X_train, y_train, test_size = 0.1, random_state = None)\nmodel = GradientBoostingRegressor()\nmodel.fit(X_train2, y_train2)\ny_pred = model.predict(X_val)\n# y_pred[y_pred>100]=0\nsns.regplot(x = y_val, y = y_pred)","d8dfe134":"# model = GradientBoostingRegressor()\n# model.fit(X_train, y_train)\n# y_pred = model.predict(X_test)\n# # y_pred[y_pred>100]=0\n# sns.regplot(x = y_test, y = y_pred)","bea73653":"Encoding categorical data","d964f809":"Removing Outliers","20ce3095":"GradientBoostingRegressor is performing the best","00c9dfad":"# Understanding the data","2d7b97fa":"Data preprocessing pipeline","1c315b16":"Droping the rows with Price null","a1445758":"Importing libraries","38384fa2":"Splitting the dataset into the Training set and Test set","7a55b1f9":"Droping the rows with Price null","47bf434c":"Feature Scaling","0ffeed4e":"Reading Data","29963ddd":"Get number of outliers","6360eb9a":"# Modeling","d9ff414e":"Removing the skewness","8fa8ca2e":"Preparing the data from scratch using pipeline to avoid target leakage from test set","ee68a2ab":"Skewness along the index axis","e961b5c8":"Taking care of missing data","237b1c9e":"# Prepare the Data","d0777898":"Histogram & Q-Q Plot","1ee0e2c3":"Box Plot","699dd7d2":"Remove Highly Correlated columns","a7cb28ba":"Getting Unique values of each column","643de663":"# Data Preprocessing","4a9beca1":"Splitting columns into features and dependent columns","a3f98827":"Taking care of missing data","5ac7700f":"Correlation Matrix","3b4f8a6c":"Packages","5abf2537":"Base Modeling","14369db7":"Removing the rows with outliers","7b20d590":"Splitting Column names based on Data Types","3ef395b7":"Droping the rows with num-of-doors null","d31fef57":"Scatter Matrix plots","06503fa9":"Remove Highly Correlated columns"}}