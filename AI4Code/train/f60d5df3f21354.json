{"cell_type":{"e87bbf0d":"code","09194371":"code","9bb8079e":"code","9f80caf5":"code","26b108cd":"code","18fe4f17":"code","c9cc395c":"code","9184456a":"code","23e17f8d":"code","24f525e8":"code","3cbd6533":"code","a4de814b":"code","7c37d75f":"markdown","44ce0ca7":"markdown","5edcec37":"markdown","aea5c138":"markdown","2cb0f1bb":"markdown","f4ed8365":"markdown"},"source":{"e87bbf0d":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\n\nfrom sklearn.metrics import confusion_matrix, classification_report","09194371":"data = pd.read_json('..\/input\/sarcasm-detection-through-nlp\/Sarcasm_Headlines_Dataset.json', lines=True)","9bb8079e":"data","9f80caf5":"data.info()","26b108cd":"def get_sequences(texts, tokenizer, train=True, max_seq_length=None):\n    sequences = tokenizer.texts_to_sequences(texts)\n    \n    if train == True:\n        max_seq_length = np.max(list(map(len, sequences)))\n    \n    sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_seq_length, padding='post')\n    \n    return sequences","18fe4f17":"def preprocess_inputs(df):\n    df = df.copy()\n    \n    # Drop article_link column\n    df = df.drop('article_link', axis=1)\n    \n    # Split df into X and y\n    y = df['is_sarcastic']\n    X = df['headline']\n    \n    # Train-test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True, random_state=1)\n    \n    # Create and fit tokenizer\n    tokenizer = tf.keras.preprocessing.text.Tokenizer()\n    tokenizer.fit_on_texts(X_train)\n    \n    print(\"Vocab length:\", len(tokenizer.word_index) + 1)\n    \n    # Get sequence data\n    X_train = get_sequences(texts=X_train, tokenizer=tokenizer, train=True)\n    X_test = get_sequences(texts=X_test, tokenizer=tokenizer, train=False, max_seq_length=X_train.shape[1])\n    \n    print(\"Sequence length:\", X_train.shape[1])\n    \n    return X_train, X_test, y_train, y_test","c9cc395c":"X_train, X_test, y_train, y_test = preprocess_inputs(data)","9184456a":"X_train","23e17f8d":"y_train.value_counts()","24f525e8":"inputs = tf.keras.Input(shape=(40,))\nx = tf.keras.layers.Embedding(\n    input_dim=24846,\n    output_dim=64\n)(inputs)\nx = tf.keras.layers.Flatten()(x)\nx = tf.keras.layers.Dense(128, activation='relu')(x)\nx = tf.keras.layers.Dense(128, activation='relu')(x)\noutputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n\nmodel.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=[\n        'accuracy',\n        tf.keras.metrics.AUC(name='auc')\n    ]\n)\n\nprint(model.summary())","3cbd6533":"history = model.fit(\n    X_train,\n    y_train,\n    validation_split=0.2,\n    batch_size=32,\n    epochs=100,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=3,\n            restore_best_weights=True\n        )\n    ]\n)","a4de814b":"results = model.evaluate(X_test, y_test, verbose=0)\n\nprint(\"Accuracy: {:.2f}%\".format(results[1] * 100))\nprint(\"     AUC: {:.5f}\".format(results[2]))\n\ny_pred = np.squeeze(model.predict(X_test) >= 0.5).astype(np.int)\ncm = confusion_matrix(y_test, y_pred, labels=[0, 1])\nclr = classification_report(y_test, y_pred, labels=[0, 1], target_names=[\"No Sarcasm\", \"Sarcasm\"])\n\nplt.figure(figsize=(6, 6))\nsns.heatmap(cm, annot=True, fmt='g', vmin=0, cmap='Blues', cbar=False)\nplt.xticks(ticks=[0.5, 1.5], labels=[\"No Sarcasm\", \"Sarcasm\"])\nplt.yticks(ticks=[0.5, 1.5], labels=[\"No Sarcasm\", \"Sarcasm\"])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n\nprint(\"Classification Report:\\n----------------------\\n\", clr)","7c37d75f":"# Getting Started","44ce0ca7":"# Training","5edcec37":"# Results","aea5c138":"# Task for Today  \n\n***\n\n## News Headline Sarcasm Detection  \n  \nGiven *news headlines*, let's try to predict whether a given headline contains **sarcasm**.  \n  \nWe will use a TensorFlow\/Keras text model with word embeddings to make our predictions.","2cb0f1bb":"# Data Every Day  \n\nThis notebook is featured on Data Every Day, a YouTube series where I train models on a new dataset each day.  \n\n***\n\nCheck it out!  \nhttps:\/\/youtu.be\/fpvnx9pNRYc","f4ed8365":"# Preprocessing"}}