{"cell_type":{"393faf73":"code","8133ad47":"code","a304c8ea":"code","83b717a5":"code","ca5f3759":"code","f7bd51e2":"code","ad0e0489":"code","27c2f896":"code","0e2efc83":"code","e98ccf03":"code","d8cf1fa6":"code","3922aaba":"code","b66abc3a":"code","64a5d216":"code","4f73e76d":"code","19bf9abc":"code","cf0cb6ef":"code","a1f2789b":"code","15017657":"code","90937593":"code","09899430":"code","fb4fb199":"code","f517a0c6":"markdown","c864f5a2":"markdown","4a9de2b1":"markdown","20bd574a":"markdown","fb3c5d8b":"markdown","4f42ef0a":"markdown","60b3be82":"markdown","c006e7aa":"markdown","1518605c":"markdown","75ea5c9e":"markdown","b4185689":"markdown","3574e3bd":"markdown","ff72cde0":"markdown"},"source":{"393faf73":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\npd.set_option('max_colwidth', None)","8133ad47":"path = '\/kaggle\/input\/bcg-manually-reviewed-cleaned'\nfile = f'{path}\/manually_reviewed_cleaned.csv'\ndf = pd.read_csv(file, encoding = \"ISO-8859-1\")","a304c8ea":"question_names = ['first_year','last_year','is_mandatory','timing','strain','has_revaccinations','revaccination_timing','location', 'manufacturer', 'company', 'groups']\n\ndf.columns = ['alpha_2_code', 'country', 'url', 'filename', 'is_pdf','Comments',\n              'Snippet'] + question_names + ['snippet_len', 'text_len']","83b717a5":"df.info()","ca5f3759":"df_fy = df[df['first_year'].notna()][['alpha_2_code','country','url', 'filename','first_year']].reset_index()","f7bd51e2":"f\"Working with {df_fy.shape[0]} positive examples.\"","ad0e0489":"def read_text(row):\n    code = row['alpha_2_code']\n    filename=row['filename'].replace('.txt', '')\n    filename = f'\/kaggle\/input\/hackathon\/task_1-google_search_txt_files_v2\/{code}\/{filename}.txt'\n    \n    with open(filename, 'r') as file:\n        data = file.read()#.replace('\\n', ' ')\n    return data\n\nimport spacy\nnlp = spacy.load('en_core_web_sm')\n\ndef get_snippets(text):\n    '''\n        Returns sentences in the text which contain more than 5 tokens and at least one verb.\n    '''\n    return [sent.text.strip() for sent in nlp(text).sents \n                 if len(sent.text.strip().split()) > 5 and any([token.pos_ == 'VERB' for token in sent])]","27c2f896":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport tqdm\n\nnegative_examples = []\n\nfor _, row in tqdm.tqdm(df_fy.iterrows()):\n    text = read_text(row)\n    snippets = get_snippets(text)\n\n    tfidf_vectorizer = TfidfVectorizer()\n    tfidf_matrix = tfidf_vectorizer.fit_transform(snippets)\n\n    sim = cosine_similarity(tfidf_vectorizer.transform([row['first_year']]),tfidf_matrix)\n    res = pd.DataFrame()\n    res['snippet'] = snippets\n    res['sim'] = sim[0]\n    low_sim = res[res['sim']<0.1]['snippet'].values\n    negative_examples.extend(low_sim)","0e2efc83":"df_data = pd.DataFrame({'snippet': df_fy['first_year'], 'label': 1})\ndf_data = df_data.append(pd.DataFrame({'snippet': negative_examples, 'label': 0}), ignore_index=True)","e98ccf03":"f\"Working with {df_data.shape[0]} examples in total.\"","d8cf1fa6":"df_data['snippet_len'] = df_data['snippet'].apply(len)\ndf_data.drop(df_data[df_data['snippet_len'] > 350].index, inplace=True)","3922aaba":"f\"Working with {df_data.shape[0]} examples in total.\"","b66abc3a":"df_data['label'].value_counts(normalize=True)","64a5d216":"from sklearn.model_selection import train_test_split, cross_validate\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC","4f73e76d":"X_train, X_test, y_train, y_test = train_test_split(df_data['snippet'], df_data['label'], test_size=0.33, random_state=42, stratify=df_data['label'])","19bf9abc":"C = 1\nclfs = [('Dummy', DummyClassifier(strategy='prior')),\n        ('RF', RandomForestClassifier(n_estimators=100, max_depth=2, class_weight='balanced')),\n       ('LogReg', LogisticRegression(random_state=0, solver='lbfgs', class_weight='balanced')),\n       ('SVM SGD', SGDClassifier(max_iter=1000, tol=1e-3, class_weight='balanced')),\n       ('SVM linear', SVC(kernel='linear', C=C, class_weight='balanced')),\n       ('SVM RBF', SVC(kernel='rbf', C=C, class_weight='balanced')),\n       ('SVM Poly', SVC(kernel='poly', C=C, class_weight='balanced'))\n       ]\n\n\nfor name, clf in clfs:\n    pipeline = Pipeline([\n    ('tfidf', TfidfVectorizer()),\n    ('clf', clf),\n    ])\n    \n    scores = cross_validate(pipeline, X_train, y_train, cv=5, scoring=('accuracy', 'f1', 'roc_auc'), return_train_score=True)\n\n    print(\"{:10s} {:5s} | Train: {:.3f}, Test: {:.3f}\".format(name, 'ACC', np.mean(scores['train_accuracy']), np.mean(scores['test_accuracy'])))\n    \n    print(\"{:10s} {:5s} | Train: {:.3f}, Test: {:.3f}\".format(name, 'F1', np.mean(scores['train_f1']), np.mean(scores['test_f1'])))\n    \n    print(\"{:10s} {:5s} | Train: {:.3f}, Test: {:.3f}\".format(name, 'AUC', np.mean(scores['train_roc_auc']), np.mean(scores['test_roc_auc'])))\n    \n    print()","cf0cb6ef":"from sklearn.metrics import accuracy_score, f1_score, classification_report\n\npipeline = Pipeline([\n    ('tfidf', TfidfVectorizer()),\n    ('clf', LogisticRegression(random_state=0, solver='lbfgs', class_weight='balanced')),\n])\n    \npipeline.fit(X_train, y_train)\npred = pipeline.predict(X_test)\npred_proba = pipeline.predict_proba(X_test)[:,1]","a1f2789b":"result = pd.DataFrame({'proba': pred_proba, 'label': y_test, 'text': X_test})","15017657":"result.sort_values('proba', ascending=False).head(10)","90937593":"accuracy_score(y_test, pred)","09899430":"f1_score(y_test, pred)","fb4fb199":"print(classification_report(y_test, pred))","f517a0c6":"From the manually reviewed dataset we have the positive examples. But to gather the negative ones we'll need to process the whole texts, containing the positive snippets. Then, we take only those parts of the texts that are different enough from our positive snippets. We use cosine distance to measure their similarity.","c864f5a2":"## Model evaluation","4a9de2b1":"Here we check how many snippets we have for each question.","20bd574a":"The results look promising - using the classifier we can extract 2 valid answers with no false positives!","fb3c5d8b":"Now let's combine the positive and negative examples:","4f42ef0a":"## Model exploration","60b3be82":"As mentioned, the dataset is highly imbalanced.","c006e7aa":"We want to remove snippets that are longer than 350 chars, since this is a requirement in the task.","1518605c":"# Motivation\nWe have some manually labeled data about which snippet answers which question in the manually reviewed file. This can be used to create a supervised dataset for each of the 8 questions. It contains positive examples, i.e snippets that answer a question, but we can extract negative ones by looking at the rest of the text.\n\nThis notebook creates a labeled dataset for the first question (\"Which is the First Year of the BCG Policy?\") and experiments with different classifiers on it.\nThe dataset is highly imbalanced, as is the expectation for the real data - only a small number of snippets will be a valid answer for a particular question.","75ea5c9e":"## Analyse questions","b4185689":"## Create negative examples","3574e3bd":"We see that the 'first_year' question has the most snippets - 21. This is why we chose it for this experiment.\n\nFor some questions there are very few or even no snippets at all.","ff72cde0":"To choose a model, we look at three metrics - accuracy, F1 score and AUC. The Logistic Regression seems to have highest scores on all of them."}}