{"cell_type":{"a1d22a8e":"code","efd94805":"code","5a7b83f2":"code","738b3b4b":"code","2c822e47":"code","05523412":"code","305483ab":"code","0008cd4f":"code","690b0b5f":"code","871f409e":"code","d01b4c0b":"code","7d343110":"code","b39c7c1a":"code","42c739ad":"code","6c024e86":"code","3d6ec071":"code","8650a8ee":"code","aa5024fb":"code","fd2eb5fc":"code","196e6eb7":"code","2dd04bfd":"code","73cd201a":"code","6832086d":"code","cce9f677":"code","8ad35bf9":"code","d2bd08a2":"code","ec0a8556":"markdown","6043013d":"markdown"},"source":{"a1d22a8e":"import xgboost as xgb\nimport numpy as np\nimport pandas as pd\nimport random\nimport optuna\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error","efd94805":"data0 = pd.read_csv(\"..\/input\/water-quality\/waterQuality1.csv\")\ndata0","5a7b83f2":"data0.info()","738b3b4b":"data0['is_safe'].value_counts()","2c822e47":"data0['is_safe']=data0['is_safe'].map({'0':0,'1':1,'#NUM!':0})","05523412":"ammo=[]\nfor i in range(len(data0)):\n    s=data0.loc[i,'ammonia']\n    if s=='#NUM!':\n        ammo+=[0]\n    else:\n        ammo+=[float(s)]\ndata0['ammonia']=ammo","305483ab":"n=len(data0)\nN=[]\nfor i in range(n):\n    N+=[i]\nrandom.shuffle(N)","0008cd4f":"dataY=data0['is_safe']\ndataX=data0.drop('is_safe',axis=1)\n\ntrainY=dataY.loc[N[0:(n\/\/4)*3]]\ntrainX=dataX.loc[N[0:(n\/\/4)*3]]\n\ntestY=dataY.loc[N[(n\/\/4)*3:]]\ntestX=dataX.loc[N[(n\/\/4)*3:]]","690b0b5f":"columns=dataX.columns.to_list()\nprint(columns)","871f409e":"data=trainX\ntarget=trainY","d01b4c0b":"def objective(trial,data=data,target=target):\n    \n    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.2,random_state=42)\n    param = {\n        'objective': trial.suggest_categorical('objective',['reg:logistic','reg:tweedie']), \n        'tree_method': trial.suggest_categorical('tree_method',['hist']),  # 'gpu_hist','hist'\n        'lambda': trial.suggest_loguniform('lambda',1e-3,10.0),\n        'alpha': trial.suggest_loguniform('alpha',1e-3,10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.008,0.01,0.012,0.014,0.016,0.018,0.02]),\n        'n_estimators': trial.suggest_categorical('n_estimators', [1000,2000,4000,8000]),\n        'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11,13,15,17,20]),\n        'random_state': trial.suggest_categorical('random_state', [24,48,2020]),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1,300),\n        'use_label_encoder': trial.suggest_categorical('use_label_encoder',[False])\n    }\n    model = xgb.XGBClassifier(**param)      \n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=100,verbose=False)\n    preds = model.predict(test_x)\n    rmse = mean_squared_error(test_y, preds,squared=False)\n    \n    return rmse","7d343110":"study = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=16)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","b39c7c1a":"study.trials_dataframe()","42c739ad":"# shows the scores from all trials\noptuna.visualization.plot_optimization_history(study)","6c024e86":"# interactively visualizes the hyperparameters and scores\noptuna.visualization.plot_parallel_coordinate(study)","3d6ec071":"# shows the evolution of the search\noptuna.visualization.plot_slice(study)","8650a8ee":"# parameter interactions on an interactive chart.\noptuna.visualization.plot_contour(study, params=['colsample_bytree','max_depth'])","aa5024fb":"# Visualize parameter importances.\noptuna.visualization.plot_param_importances(study)","fd2eb5fc":"# Visualize empirical distribution function\noptuna.visualization.plot_edf(study)","196e6eb7":"Best_trial=study.best_trial.params\nprint(Best_trial)","2dd04bfd":"train=trainX\ntest=testX","73cd201a":"preds = np.zeros((testX.shape[0]))\nkf = KFold(n_splits=5,random_state=48,shuffle=True)\nfor trn_idx, test_idx in kf.split(train[columns],target):\n    X_tr,X_val=train[columns].iloc[trn_idx],train[columns].iloc[test_idx]\n    y_tr,y_val=target.iloc[trn_idx],target.iloc[test_idx]\n    model = xgb.XGBClassifier(**Best_trial)\n    model.fit(X_tr,y_tr,eval_set=[(X_val,y_val)],early_stopping_rounds=100,verbose=False)\n    preds+=model.predict(test[columns])\/kf.n_splits   ###### predict_proba\n    rmse=mean_squared_error(y_val, model.predict(X_val),squared=False)\n    print(rmse)","6832086d":"print(preds.shape)\nprint(preds[0])","cce9f677":"subm=pd.DataFrame(testY)\nsubm['predicted'] = np.where(preds<0.5,0,1).astype(int)\nsubm","8ad35bf9":"ANS=list(testY)\nPRED=list(subm['predicted'])","d2bd08a2":"accuracy=accuracy_score(ANS,PRED)\nprint(accuracy)","ec0a8556":"#### Objective candidate for XGBoost\n* Objective candidate: survival:aft\n* Objective candidate: binary:hinge\n* Objective candidate: multi:softmax\n* Objective candidate: multi:softprob\n* Objective candidate: rank:pairwise\n* Objective candidate: rank:ndcg\n* Objective candidate: rank:map\n* Objective candidate: reg:squarederror\n* Objective candidate: reg:squaredlogerror\n* Objective candidate: reg:logistic\n* Objective candidate: reg:pseudohubererror\n* Objective candidate: binary:logistic\n* Objective candidate: binary:logitraw\n* Objective candidate: reg:linear\n* Objective candidate: count:poisson\n* Objective candidate: survival:cox\n* Objective candidate: reg:gamma\n* Objective candidate: reg:tweedie","6043013d":"# XGBoost with Optuna tuning\n* doc: \nhttps:\/\/github.com\/optuna\/optuna"}}