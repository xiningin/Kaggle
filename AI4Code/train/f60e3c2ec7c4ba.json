{"cell_type":{"83ed177e":"code","20601e09":"code","6a02210a":"code","7e44a65d":"code","2ab8be6e":"code","853dcafb":"code","2fa4d4d3":"code","44d328ce":"code","7c055c82":"code","b0298ac1":"code","d12d76cd":"code","635cc089":"code","ab69d3ae":"code","6785bb8a":"code","fb44af67":"code","e8786039":"code","d0a251fe":"code","75d77928":"code","91bd372f":"code","5647ab2f":"code","9f2ef72d":"code","a0a96748":"code","45fdbcba":"code","3aee4015":"code","2faf0bef":"code","5043f5f1":"code","244dd2fd":"code","aaaeb292":"code","b823e15d":"code","864cf60f":"code","27e66332":"code","f5f2ee39":"code","41d9aea6":"code","42b4fb37":"code","d96373d4":"code","94a11a6b":"code","39b06267":"code","dd5bdf50":"code","ee071907":"code","d1d9ef5a":"markdown","e5ab6975":"markdown","32096b56":"markdown","9f0273a8":"markdown","0adb485a":"markdown","1fd405ae":"markdown","7d6338b8":"markdown","4b004a00":"markdown","19b756d0":"markdown","14a063af":"markdown","97efe074":"markdown","150efd6b":"markdown","12f48c51":"markdown","4dc76a27":"markdown","ae8d17c3":"markdown","a9b74843":"markdown","7468b194":"markdown","f1bc0289":"markdown","96a16bc4":"markdown","94b2b0a4":"markdown","d57429a2":"markdown"},"source":{"83ed177e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom wordcloud import WordCloud\nfrom collections import Counter\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport re,gc\nfrom string import punctuation\nprint(os.listdir(\"..\/input\"))\n# Any results you write to the current directory are saved as output.","20601e09":"import nltk\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk import FreqDist\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom nltk.stem import SnowballStemmer,WordNetLemmatizer\nstemmer=SnowballStemmer('english')\nlemma=WordNetLemmatizer()","6a02210a":"import datetime\nimport lightgbm as lgb\nfrom scipy import stats\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier,RadiusNeighborsClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC,SVC\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier,ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\npd.set_option('max_colwidth',400)\n","7e44a65d":"train = pd.read_csv('..\/input\/train.tsv', sep=\"\\t\")\ntest = pd.read_csv('..\/input\/test.tsv', sep=\"\\t\")\nsub = pd.read_csv('..\/input\/sampleSubmission.csv', sep=\",\")","2ab8be6e":"train.head(10)","853dcafb":"print('Average count of phrases per sentence in train is {0:.0f}.'.format(train.groupby('SentenceId')['Phrase'].count().mean()))\nprint('Average count of phrases per sentence in test is {0:.0f}.'.format(test.groupby('SentenceId')['Phrase'].count().mean()))","2fa4d4d3":"print('Number of phrases in train: {}. Number of sentences in train: {}.'.format(train.shape[0], len(train.SentenceId.unique())))\nprint('Number of phrases in test: {}. Number of sentences in test: {}.'.format(test.shape[0], len(test.SentenceId.unique())))","44d328ce":"print('Average word length of phrases in train is {0:.0f}.'.format(np.mean(train['Phrase'].apply(lambda x: len(x.split())))))\nprint('Average word length of phrases in test is {0:.0f}.'.format(np.mean(test['Phrase'].apply(lambda x: len(x.split())))))","7c055c82":"text = ' '.join(train.loc[train.Sentiment == 4, 'Phrase'].values)\ntext_trigrams = [i for i in ngrams(text.split(), 3)]","b0298ac1":"text = ' '.join(train.loc[train.Sentiment == 4, 'Phrase'].values)\ntext = [i for i in text.split() if i not in stopwords.words('english')]\ntext_trigrams = [i for i in ngrams(text, 3)]\nCounter(text_trigrams).most_common(30)","d12d76cd":"def clean_review(review_col):\n    review_corpus=[]\n    for i in range(0,len(review_col)):\n        review=str(review_col[i])\n        review=re.sub('[^a-zA-Z]',' ',review)\n        #review=[stemmer.stem(w) for w in word_tokenize(str(review).lower())]\n        review=[lemma.lemmatize(w) for w in word_tokenize(str(review).lower())]\n        review=' '.join(review)\n        review_corpus.append(review)\n    return review_corpus","635cc089":"train['csen']=clean_review(train.Phrase.values)\ntest['csen']=clean_review(test.Phrase.values)","ab69d3ae":"y = train['Sentiment']","6785bb8a":"xtrain, xvalid, ytrain, yvalid = train_test_split(train.csen.values, y, \n                                                  stratify=y, \n                                                  random_state=42, \n                                                  test_size=0.1, shuffle=True)","fb44af67":"vectorizer = TfidfVectorizer(min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\nfull_text = list(train['csen'].values)\nvectorizer.fit(full_text)","e8786039":"xtrain_tfv =  vectorizer.transform(xtrain)\nxvalid_tfv = vectorizer.transform(xvalid)\nxtest_tfv = vectorizer.transform(test['csen'].values)","d0a251fe":"clf = LogisticRegression(C=1.0)\nclf.fit(xtrain_tfv, ytrain)\npredictions = clf.predict(xvalid_tfv)\nprint(\"accuracy_score\",accuracy_score(yvalid, predictions))","75d77928":"lr = clf.predict(xtest_tfv)\ndel clf,predictions\ngc.collect()","91bd372f":"clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nclf.fit(xtrain_tfv, ytrain)\npredictions = clf.predict(xvalid_tfv)\nprint(\"accuracy_score\",accuracy_score(yvalid, predictions))\n","5647ab2f":"xgb = clf.predict(xtest_tfv)\ndel clf,predictions\ngc.collect()","9f2ef72d":"clf = MultinomialNB()\nclf.fit(xtrain_tfv, ytrain)\npredictions = clf.predict(xvalid_tfv)\nprint(\"accuracy_score\",accuracy_score(yvalid, predictions))","a0a96748":"mnb = clf.predict(xtest_tfv)\ndel clf,predictions\ngc.collect()","45fdbcba":"clf = AdaBoostClassifier()\nclf.fit(xtrain_tfv, ytrain)\npredictions = clf.predict(xvalid_tfv)\nprint(\"accuracy_score\",accuracy_score(yvalid, predictions))","3aee4015":"adboost = clf.predict(xtest_tfv)\ndel clf,predictions\ngc.collect()","2faf0bef":"clf = KNeighborsClassifier()\nclf.fit(xtrain_tfv, ytrain)\npredictions = clf.predict(xvalid_tfv)\nprint(\"accuracy_score\",accuracy_score(yvalid, predictions))","5043f5f1":"knc = clf.predict(xtest_tfv)\ndel clf,predictions\ngc.collect()","244dd2fd":"clf = LinearSVC()\nclf.fit(xtrain_tfv, ytrain)\npredictions = clf.predict(xvalid_tfv)\nprint(\"accuracy_score\",accuracy_score(yvalid, predictions))","aaaeb292":"lsvc = clf.predict(xtest_tfv)\ndel clf,predictions\ngc.collect()","b823e15d":"clf = GradientBoostingClassifier()\nclf.fit(xtrain_tfv, ytrain)\npredictions = clf.predict(xvalid_tfv)\nprint(\"accuracy_score\",accuracy_score(yvalid, predictions))","864cf60f":"gbc = clf.predict(xtest_tfv)\ndel clf,predictions\ngc.collect()","27e66332":"clf = ExtraTreesClassifier()\nclf.fit(xtrain_tfv, ytrain)\npredictions = clf.predict(xvalid_tfv)\nprint(\"accuracy_score\",accuracy_score(yvalid, predictions))","f5f2ee39":"etc = clf.predict(xtest_tfv)\ndel clf,predictions\ngc.collect()","41d9aea6":"clf = DecisionTreeClassifier()\nclf.fit(xtrain_tfv, ytrain)\npredictions = clf.predict(xvalid_tfv)\nprint(\"accuracy_score\",accuracy_score(yvalid, predictions))","42b4fb37":"dtc = clf.predict(xtest_tfv)\ndel clf,predictions\ngc.collect()","d96373d4":"sub['Sentiment'] = pd.DataFrame(lr)\nsub.to_csv('lr.csv',index=False)\nsub['Sentiment'] = pd.DataFrame(xgb)\nsub.to_csv('xgb.csv',index=False)\nsub['Sentiment'] = pd.DataFrame(mnb)\nsub.to_csv('mnb.csv',index=False)\nsub['Sentiment'] = pd.DataFrame(lsvc)\nsub.to_csv('lsvc.csv',index=False)\nsub['Sentiment'] = pd.DataFrame(etc)\nsub.to_csv('etc.csv',index=False)\nsub['Sentiment'] = pd.DataFrame(knc)\nsub.to_csv('knc.csv',index=False)\nsub['Sentiment'] = pd.DataFrame(dtc)\nsub.to_csv('dtc.csv',index=False)","94a11a6b":"df = pd.DataFrame(lr,columns=['lr'])","39b06267":"df['xgb'] = xgb\ndf['mnb'] = mnb\ndf['lsvc'] = lsvc\ndf['etc'] = etc\ndf['knc'] = knc\ndf['dtc'] = dtc\ndf.head(15)","dd5bdf50":"sub['Sentiment'] = df.mode(axis=1)\nsub['Sentiment'] = sub.Sentiment.astype(int)","ee071907":"sub.to_csv('submission.csv',index=False)","d1d9ef5a":"### XGBClassifier","e5ab6975":"### Load Data Set","32096b56":"## Road Map We must have to follow \n* NLP Library for Preprocessing and Cleaning\n* Load all Classification Model Packages for Sentiment\n* Load Data Set\n* Analyse the Data and Take same few insights\n* PreProcessing Function\n* Split the Data Train and Validation\n* Generate Feature using TfidfVectorizer\n* Load the all classification model\n        * XGBClassifier\n        * LogisticRegression\n        * MultinomialNB\n        * AdaBoostClassifier\n        * KNeighborsClassifier\n        * LinearSVC\n        * GradientBoostingClassifier\n        * ExtraTreesClassifier\n        * DecisionTreeClassifier\n\n## Hello Kagglers you have fork this notebook and  improve your score using parameter tunning technique\n* Gride Search\n* Random Search\n* Bayes Search so on","9f0273a8":"### DecisionTreeClassifier","0adb485a":"### ExtraTreesClassifier ","1fd405ae":"### AdaBoostClassifier","7d6338b8":"### KNeighborsClassifier","4b004a00":"### NLP Library for Preprocessing and Cleaning","19b756d0":"### MultinomialNB","14a063af":"### LinearSVC","97efe074":"### Load all Classification Model Packages for Sentiment","150efd6b":"<img src=\"https:\/\/cdn.steemitimages.com\/DQmQZCf7ME7Haj3X3MzXtG8R8JtGmTpuh5NXDSd3wKueva7\/rottentomatoes.png\" \/>","12f48c51":"\n**<p>Rotten Tomatoes is an American review-aggregation website for film and television. The company was launched in August 1998 by three undergraduate students at the University of California, Berkeley: Senh Duong, Patrick Y. Lee and Stephen Wang. The name \"Rotten Tomatoes\" derives from the practice of audiences throwing rotten tomatoes when disapproving of a poor stage performance<\/p>**\n\n#### Evalution\n\n<p>Submissions are evaluated on classification accuracy (the percent of labels that are predicted correctly) for every parsed phrase. The sentiment labels are:<\/p>\n\n* 0 - negative\n* 1 - somewhat negative\n* 2 - neutral\n* 3 - somewhat positive\n* 4 - positive","4dc76a27":"### GradientBoostingClassifier","ae8d17c3":"### PreProcessing Function","a9b74843":"### Analyse the Data and Take same few insights","7468b194":"### Split the Data Train and Validation","f1bc0289":"### LogisticRegression","96a16bc4":"# Approaching (Almost) Any NLP Problem for Classification on Kaggle\n\nIn this post I'll talk about approaching natural language processing problems on Kaggle. As an example, we will use the data from this competition. I have create a very basic all classification model first and then improve algorithm parameter. \n\n### Cover all Classification Algorithm \n* XGBClassifier\n* LogisticRegression\n* MultinomialNB\n* AdaBoostClassifier\n* KNeighborsClassifier\n* LinearSVC\n* GradientBoostingClassifier\n* ExtraTreesClassifier\n* DecisionTreeClassifier\n\nNote : You can also use other classification algorithm **which is load in Library (Classification Model Packages for Sentiment)**\n\n**Important Note : ** * you must use del and gc function because of same time kernal ram and memory is full so you have to delete object and clear the ram*\n\n### Blending Technique Apply on Best Score Algorithm\n* XGBClassifier \n* LogisticRegression \n* MultinomialNB \n* KNeighborsClassifier \n* LinearSVC \n* ExtraTreesClassifier\n* DecisionTreeClassifier","94b2b0a4":"### Blending Technique Apply on Best Score\n* XGBClassifier \n* LogisticRegression \n* MultinomialNB \n* KNeighborsClassifier \n* LinearSVC \n* ExtraTreesClassifier\n* DecisionTreeClassifier","d57429a2":"### Generate Feature using TfidfVectorizer"}}