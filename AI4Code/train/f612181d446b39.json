{"cell_type":{"8e766483":"code","01781e89":"code","85e4fb4d":"code","ada9a1b1":"code","096db869":"code","ba055a72":"code","bbb33b60":"code","4eb99fa9":"code","de45dba4":"code","0126022e":"code","eefbce57":"code","67319489":"code","3d59d989":"code","0e9c734c":"code","a92d793b":"code","8bc65fa0":"code","248fbddf":"code","526c6d03":"code","81c6d788":"code","a3379610":"code","c9e9c4cd":"code","9e896f05":"code","c5a32a14":"code","0e21db18":"markdown","a023dba4":"markdown","2884a87a":"markdown","6425ffa2":"markdown","e8026ab0":"markdown","29410c4d":"markdown","ec0411d4":"markdown","ad8c80b3":"markdown","18a30c07":"markdown","e033d75e":"markdown","a80a0476":"markdown","0b9b52a0":"markdown","feb5d3f6":"markdown"},"source":{"8e766483":"import numpy as np\nimport pandas as pd \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, accuracy_score\nfrom sklearn.datasets import make_classification\nfrom imblearn.over_sampling import RandomOverSampler,SMOTE, ADASYN\nfrom imblearn.under_sampling import RandomUnderSampler, TomekLinks,EditedNearestNeighbours,RepeatedEditedNearestNeighbours,AllKNN, ClusterCentroids,NearMiss","01781e89":"X,y=make_classification(n_samples=1000,\n                        weights=[0.9,0.1],\n                       n_features=10,\n                        n_informative=2,\n                       n_redundant=8,\n                        flip_y=0.4,\n                       shift= None,\n                       random_state=42)\n                       ","85e4fb4d":"X=pd.DataFrame(X, columns=['f1','f2','f3','f4','f5','f6','f7','f8','f9','f10'])","ada9a1b1":"X.head()","096db869":"y=pd.DataFrame(y, columns=['result'])","ba055a72":"y.head()","bbb33b60":"y=y.result","4eb99fa9":"print(f'Shape of X: {X.shape}')\nprint(f'Shape of y: {y.shape}')","de45dba4":"print(f'Number of 0: {y.value_counts()[0]}')\nprint(f'Number of 1: {y.value_counts()[1]}')","0126022e":"X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=42)","eefbce57":"log_model=LogisticRegression()\nlog_model.fit(X_train,y_train)\npred=log_model.predict(X_test)\n\n\nprint(f'Error Rate: {np.sqrt(mean_squared_error(y_test,pred))}')\nprint(f'Accuracy_score: {accuracy_score(y_test,pred)}')","67319489":"def oversampler(method,X,y,random_state=42):\n    mt=method(random_state=random_state)\n    X_resampled, y_resampled = mt.fit_resample(X,y)\n    print(f'Shape of X: {X_resampled.shape}')\n    print(f'Shape of y: {y_resampled.shape}')\n    print(f'Number of 0: {len(y_resampled[y_resampled==0])}')\n    print(f'Number of 1: {len(y_resampled[y_resampled==1])}')\n    X_train,X_test,y_train,y_test=train_test_split(X_resampled,y_resampled,random_state=random_state)\n    log_model=LogisticRegression()\n    log_model.fit(X_train,y_train)\n    pred=log_model.predict(X_test)\n\n    print(f'Error Rate: {np.sqrt(mean_squared_error(y_test,pred))}')\n    print(f'Accuracy_score: {accuracy_score(y_test,pred)}')","3d59d989":"oversampler(method=RandomOverSampler,X=X,y=y)","0e9c734c":"oversampler(method=SMOTE,X=X,y=y)","a92d793b":"oversampler(method=ADASYN,X=X,y=y)","8bc65fa0":"def undersampler(method,X,y,random_state=42,random_state_open=1):\n    if random_state_open==1:\n        \n        mt=method(random_state=random_state)\n        X_resampled, y_resampled = mt.fit_resample(X,y)\n        print(f'Shape of X: {X_resampled.shape}')\n        print(f'Shape of y: {y_resampled.shape}')\n        print(f'Number of 0: {len(y_resampled[y_resampled==0])}')\n        print(f'Number of 1: {len(y_resampled[y_resampled==1])}')\n        X_train,X_test,y_train,y_test=train_test_split(X_resampled,y_resampled,random_state=random_state)\n        log_model=LogisticRegression()\n        log_model.fit(X_train,y_train)\n        pred=log_model.predict(X_test)\n        print(f'Error Rate: {np.sqrt(mean_squared_error(y_test,pred))}')\n        print(f'Accuracy_score: {accuracy_score(y_test,pred)}')\n    else:\n        mt=method()\n        X_resampled, y_resampled = mt.fit_resample(X,y)\n        print(f'Shape of X: {X_resampled.shape}')\n        print(f'Shape of y: {y_resampled.shape}')\n        print(f'Number of 0: {len(y_resampled[y_resampled==0])}')\n        print(f'Number of 1: {len(y_resampled[y_resampled==1])}')\n        X_train,X_test,y_train,y_test=train_test_split(X_resampled,y_resampled,random_state=random_state)\n        log_model=LogisticRegression()\n        log_model.fit(X_train,y_train)\n        pred=log_model.predict(X_test)\n        print(f'Error Rate: {np.sqrt(mean_squared_error(y_test,pred))}')\n        print(f'Accuracy_score: {accuracy_score(y_test,pred)}')  ","248fbddf":"undersampler(method=RandomUnderSampler,X=X,y=y)","526c6d03":"undersampler(method=RandomUnderSampler,X=X,y=y,random_state_open=0)","81c6d788":"undersampler(method=EditedNearestNeighbours,X=X,y=y,random_state_open=0)","a3379610":"undersampler(method=RepeatedEditedNearestNeighbours,X=X,y=y,random_state_open=0)","c9e9c4cd":"undersampler(method=AllKNN,X=X,y=y,random_state_open=0)","9e896f05":"undersampler(method=ClusterCentroids,X=X,y=y)","c5a32a14":"for i in range (1,4):\n    mt=NearMiss(version=i)\n    X_resampled, y_resampled = mt.fit_resample(X,y)\n    print(f'Shape of X: {X_resampled.shape}')\n    print(f'Shape of y: {y_resampled.shape}')\n    print(f'Number of 0: {len(y_resampled[y_resampled==0])}')\n    print(f'Number of 1: {len(y_resampled[y_resampled==1])}')\n    X_train,X_test,y_train,y_test=train_test_split(X_resampled,y_resampled,random_state=42)\n    log_model=LogisticRegression()\n    log_model.fit(X_train,y_train)\n    pred=log_model.predict(X_test)\n    print(f'Error Rate: {np.sqrt(mean_squared_error(y_test,pred))}')\n    print(f'Accuracy_score: {accuracy_score(y_test,pred)}')\n    print('*'*20)\n    print()","0e21db18":"Let's start with creating an imbalanced dataset. For this purpose we will use \"make_classification()\". \n\nWe manipulate the \"weights\" parameter so number of one of the classified objects is much higher than the other one.\n\nTo make things harder for the machine learning algorithm, we will also use 10 features but only two of them will be valuable for classification problems. For the same purpose, we will set the \"flip_y\" parameter as 0.4.","a023dba4":"## 2.4. ClusterCentroids\n\"ClusterCentroids\" method clusters randomly assigned samples, then creates new sample points whose features are average of the samples in the clusters. Finally, it deletes the original samples. This way the sample size is decreased.","2884a87a":"## 1.1. Naive Random Over-Sampling\nIn this method, we synthetically copy data samples and randomly place them into the dataset to decrease the imbalance.","6425ffa2":"## 2.1. Random Under Sampling\nIn this method, we randomly choose data from the majority group and get rid of them until our dataset becomes balanced.","e8026ab0":"## 2.5. NearMiss\nNearMiss algorithm is another tool in the toolbox for balancing imbalanced datasets. There are 3 ways to run this algorithm. In version 1,\n1. Average closest distances of samples belong to the majority class to N minority samples are calculated.\n2. Samples that have the smallest average distance are selected.\n3. Selected samples are removed.,\n\nIn version 2,\n1. Average farthest distances of samples belong to the majority class to N minority samples are calculated.\n2. Samples that have the smallest average distance are selected.\n3. Selected samples are removed.\n\nIn version 3,\n1. Samples belong to the minority class are selected and grouped.\n2. Distance between this group and samples that are belong majority class is calculated. \n3. Sample that has the largest distance is eliminated.","29410c4d":"## 2.2. TomeLinks\nIn this method, we cluster the minority and majority samples based on their location then we remove samples that belong to the majority class in each cluster.\n![5.PNG](attachment:5.PNG)","ec0411d4":"# 2. Under-sampling","ad8c80b3":"## 1.2. Synthetic Minority Oversampling Technique (SMOTE)\nRandomly copying data samples increases the chance of overfitting. To avoid overfitting we can use SMOTE method. SMOTE synthetically produces new data samples by calculating distance between two nearest data points and  multiplying this distance with random number between 0 and 1. ![Ekran%20Al%C4%B1nt%C4%B1s%C4%B12.PNG](attachment:Ekran%20Al%C4%B1nt%C4%B1s%C4%B12.PNG)","18a30c07":" # 1.3. Adaptive Synthetic Sampling Approach (ADASYN)\nADASYN method is similar to the SMOTE method. In this approach, we again create samples between two neighbors but this time, to avoid linear correlation between features and samples take place, we slightly shift the synthetically produced samples from their actual positions.\n![Ekran%20Al%C4%B1nt%C4%B1s%C4%B1%203.PNG](attachment:Ekran%20Al%C4%B1nt%C4%B1s%C4%B1%203.PNG)","e033d75e":"# 1. Oversampling\n","a80a0476":"Thanks...","0b9b52a0":"Imbalanced datasets are one of the most annoying problems for machine learning algorithms. When we train machine learning algorithms with these datasets, the prediction success becomes meaningless. If almost all y's are 1 in the training dataset, the machine learning model always predicts 1 and when we evaluate these predictions with validation splits, we will have a low error rate. On the other hand, it becomes useless. We can't use these algorithms to predict an unknown variable.\n\nSo we have to manipulate the dataset so that the machine learning algorithm learns efficiently (Increased error rate does not always mean the model is worse). There are several ways to handle these cases and in this notebook, I'll try to show them to you. Hope you'll like it.","feb5d3f6":"## 2.3. Edited Nearest Neighbours, Repeated Edited Nearest Neighbours, AllKNN\n\nThe \"Edited Nearest Neighbours\" (ENN) method uses the kNN algorithm to get rid of the points that belong to the majority class. The \"Repeated Edited Nearest Neighbours\" method repeats the same algorithm a couple of times. AllKNN is similar to the \"RepeatedNearestNeighbours\" but it also modifies the neighborhood size every time it runs."}}