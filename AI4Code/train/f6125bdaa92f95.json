{"cell_type":{"199ee125":"code","d1ea7d8f":"code","b4b11535":"code","fed9f556":"code","b546c6ca":"code","dcb88356":"code","67c78ba0":"code","208056d7":"code","6fce9726":"code","4aef9bfa":"code","d4adb5b4":"code","27eccb81":"code","461dd27a":"code","b98885aa":"code","cfbc2cef":"code","db0539e6":"code","afbf36b9":"code","8040c2a9":"code","00e6af35":"code","64ac766d":"code","35071b34":"code","4091d536":"code","41162f90":"code","f5412616":"code","fb248f1a":"code","43dd1360":"code","f633e6d7":"code","db9fbc71":"code","0a1c0d1f":"code","2d9e5ac3":"code","3b4cf2f4":"code","3b13f7bd":"code","af778354":"code","38d1fe35":"code","761ac49d":"code","a833fc0c":"code","1d09e064":"code","1f67b921":"code","05f99448":"code","b2d0bc03":"code","0936007a":"code","110c674b":"code","83553e55":"code","3c2d95a1":"code","a21cf17d":"code","8320714d":"code","70819dd0":"code","0818db6e":"code","cac7fe70":"code","29cebcd7":"code","e9f5e3a5":"code","fe976f95":"code","5fde820d":"code","828d7e38":"code","47c9fb97":"code","89e83c47":"code","a4437a6f":"code","18a1b8ee":"code","e356f4c1":"code","d619a331":"markdown","1097e0e5":"markdown","d10869f2":"markdown","cb821a31":"markdown","c904e949":"markdown","499ec389":"markdown","fa835d82":"markdown","3d0f4b4a":"markdown","ec0ff1c3":"markdown","88570c5b":"markdown","df40e49d":"markdown","b740bfab":"markdown","cc954595":"markdown","4ad21495":"markdown","04c15253":"markdown","a79d9ff1":"markdown","f0db09ad":"markdown","c4aa8580":"markdown","d9973985":"markdown","ad779cd8":"markdown","4a11782b":"markdown","0e751c82":"markdown","b320a8b1":"markdown","5fe14b14":"markdown","52055d77":"markdown","67710e7c":"markdown","d7c31544":"markdown"},"source":{"199ee125":"import pandas as pd\nimport seaborn as sns\nimport plotly.express as xp\nimport plotly.graph_objects as go\nimport numpy as np\nfrom datetime import datetime\nimport missingno\nimport yaml\nfrom collections import Counter\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV,ShuffleSplit\nfrom sklearn.manifold import TSNE\nfrom sklearn.linear_model import RidgeClassifier, LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom catboost import CatBoostClassifier\n\n\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\npalette = ['#3aa833',\"#6b856a\",\"#354014\"]\nsns.palplot(palette)","d1ea7d8f":"def split_to_onehot(df, col):\n    \"\"\"\n    This method converts features separated by '|' into one-hot vectors.\n    Additionally it drops unnecessary values, which present only in \n    test set \/ train set or have only one value.\n    \"\"\"\n    # Getting all unique ganres values.\n    unique = []\n    for i in df.index:\n        unique.extend(df.loc[i,col].split(\"|\"))\n    if \"\" in unique:\n        unique.remove(\"\")\n    unique = list(set(unique))\n    \n    # Putting values into binary form \n    onehot = df.loc[:,[\"Category\"]]\n    onehot[unique] = np.zeros((len(unique),), dtype = np.int8)\n    for i in df.index:\n        g = set(df.loc[i,col].split(\"|\"))\n        for j in g:\n            if j!=\"\":\n                onehot.loc[i,j] = 1\n                \n    # Dropping unnecessary values            \n    _a = onehot.groupby(\"Category\").sum()\n    only_one = list(_a.sum()[_a.sum()==1].index)\n    only_train = list(_a.loc[\"none\"][_a.loc[\"none\"]==0].index)\n    only_test = list(_a.loc[[\"like\",'dislike']].sum()[_a.loc[[\"like\",'dislike']].sum()==0].index)\n    _a = set(only_one + only_train + only_test)\n    onehot = onehot.drop(_a, axis=1)\n    \n    return onehot\n\ndef onehot_to_tsne2(df, title):\n    \"\"\"\n    This method converts one-hot representation into two tsne values.\n    Such operation is needed to shrink the dimentionality of the dataset\n    \"\"\"\n    onehot = df.drop(\"Category\",axis=1)\n    embedding = TSNE(n_components=2, init=\"pca\")\n    embedded = embedding.fit_transform(onehot)\n    embedded = pd.DataFrame(embedded,columns=[f\"{title}_tsne1\",f\"{title}_tsne2\"])\n    return embedded\n\ndef plot_commulative_onehot(onehot):\n    \"\"\"\n    Method of plotting commulative values of the one hot feature representation\n    \"\"\"\n    _df = onehot.groupby(\"Category\").sum()\n    fig = go.Figure()\n    for i in range(len(_df.index)):\n        k = _df.index[i]\n        x,y=[],[]\n        for g in _df.columns:\n            if _df.loc[k,g]!=0:\n                x.append(g)\n                y.append(_df.loc[k,g])\n        fig.add_trace(go.Bar(x=x, y=y,name=k,marker=dict(color=palette[i])))\n    fig.show()","b4b11535":"PATH = \"..\/input\/mymusicalprefrences\/\" \ntrain = pd.read_csv(f\"{PATH}train.csv\")\ntest = pd.read_csv(f\"{PATH}test.csv\")\ndescription = yaml.load(open(f\"{PATH}Description.yaml\",'r'),Loader=yaml.FullLoader)\ndf = pd.concat([train,test]).reset_index(drop=True)\ntr_mask = ~df.Category.isna()","fed9f556":"df.columns = [i.strip() for i in df.columns]\nprint(set(df.columns))","b546c6ca":"df.describe()","dcb88356":"# Displaing of None values in the dataset\nmissingno.bar(df, color=palette, figsize=(30,2))","67c78ba0":"cat_features = {\"Artists\",\"Track\",\"Version\",\"Artists_Genres\",\"Album\",\"Album_type\",\"Labels\",\"Vocal\",\"Country\",\"Key\"}\ncon_features = {\"Duration\",\"Release_year\",\"BPM\",\"Energy\",\"Dancebility\",\"Happiness\"}\ndisplay(df[cat_features].head())\ndisplay(df[con_features].head())","208056d7":"sns.pairplot(df[list(con_features)+[\"Category\"]],palette=palette[:2], hue=\"Category\")","6fce9726":"# For more easy usage of the Category feature\ndf[\"Category\"] = df[\"Category\"].fillna(\"none\").replace({0:\"dislike\",1:\"like\"})","4aef9bfa":"description[\"Key\"]","d4adb5b4":"xp.scatter(df, x=\"Key\", y=\"Track\",color=\"Category\", height=500, color_discrete_sequence=palette)","27eccb81":"# We correct strings and replace some ambivalent values\ndf[\"isMajor\"], df[\"Key\"] = df[\"Key\"].apply(lambda x: x.split(\" \")[1]), df[\"Key\"].apply(lambda x: x.split(\" \")[0])\ndf.loc[:,\"Key\"] = df[\"Key\"].replace({\"D\u266d\": \"C#\", \"E\u266d\": \"D#\", \"G\u266d\": \"F#\", \"A\u266d\": \"G#\",\"B\u266d\":\"A#\"})\nxp.scatter(df, x=\"Key\", y=\"Track\",color=\"Category\", height=500, color_discrete_sequence=palette)","461dd27a":"# We put the Major\/Minor part into new feature, to make it more easy for our model to fit on it\ndf.loc[:,\"isMajor\"] = (df[\"isMajor\"]==\"Major\").astype(int)\n_df = df.groupby([\"isMajor\",\"Category\"], as_index=False).count()\nxp.bar(_df,x=\"isMajor\", y=\"Track\",color=\"Category\", height=400, color_discrete_sequence=palette)","b98885aa":"_df = df.copy(deep=True)\n_df[\"Key_percise\"] = _df[\"Key\"] +\"_major:\"+ _df[\"isMajor\"].astype(str)\n_df = _df.groupby([\"Key_percise\",\"Category\"], as_index=False).count()\nxp.bar(_df, x=\"Key_percise\", y=\"Track\", color=\"Category\", height=500, color_discrete_sequence=palette)","cfbc2cef":"df[list(set(df[\"Key\"].values))] = OneHotEncoder().fit_transform(df[[\"Key\"]]).toarray()\ndf = df.drop(\"Key\", axis=1)","db0539e6":"description[\"Release year\"]","afbf36b9":"xp.scatter(df, x=\"Release_year\", y=\"Track\",color=\"Category\", height=500, color_discrete_sequence=palette)","8040c2a9":"# Lets create the decade feature, to detect some music of 80th, 90th etc. as a specific janre\ndf.loc[:,\"Release_decade\"] = (df.loc[:,\"Release_year\"]\/\/10 * 10)\n# Cause of the small number of values, we will put all <90th toone value, called 80th\ndf.loc[df.loc[:,\"Release_decade\"]<1990,\"Release_decade\"] = 1980 \n_df = df.groupby([\"Release_decade\",\"Category\"], as_index=False).count()\nxp.bar(_df,x=\"Release_decade\", y=\"Track\",color=\"Category\",height=500, color_discrete_sequence=palette)","00e6af35":"description[\"Artists Genres\"]","64ac766d":"ganres_onehot = split_to_onehot(df, \"Artists_Genres\")\nplot_commulative_onehot(ganres_onehot)","35071b34":"genres_embedded = onehot_to_tsne2(ganres_onehot, \"Genres\")\n_df = genres_embedded.copy(deep=True)\n_df[[\"Category\",\"Artists_Genres\"]] = df[[\"Category\",\"Artists_Genres\"]]\nxp.scatter(_df,x=\"Genres_tsne1\",y=\"Genres_tsne2\",color=\"Category\", hover_data=[\"Artists_Genres\"], height=500, color_discrete_sequence=palette)","4091d536":"df = pd.concat([df,genres_embedded], axis=1)\ndf = df.drop(\"Artists_Genres\", axis=1)","41162f90":"for k in [\"Energy\",\"Happiness\",\"Dancebility\",\"BPM\"]:\n    print(f\"{k}:{description[k]}\")","f5412616":"df[\"BPM\"] = df[\"BPM\"].apply(lambda x: str(x)[1:] if str(x)[0]=='`' else x)\ndf[['Energy', 'Happiness', 'Dancebility','BPM']] = df[['Energy', 'Happiness', 'Dancebility','BPM']].fillna(0)\ndf[['Energy%', 'Happiness%', 'Dancebility%']] = df[['Energy', 'Happiness', 'Dancebility']].apply(lambda x: x\/sum(x), axis=1)\ndf[['Energy%', 'Happiness%', 'Dancebility%']] = df[['Energy%', 'Happiness%', 'Dancebility%']].fillna(0)","fb248f1a":"print(description[\"Labels\"])","43dd1360":"df.Labels = df.Labels.fillna('NA')\nlabels_onehot = split_to_onehot(df, \"Labels\")\nplot_commulative_onehot(labels_onehot)","f633e6d7":"labels_embedded = onehot_to_tsne2(labels_onehot, \"Labels\")\n_df = labels_embedded.copy(deep=True)\n_df[[\"Category\",\"Labels\"]] = df[[\"Category\",\"Labels\"]]\nxp.scatter(_df,x=\"Labels_tsne1\",y=\"Labels_tsne2\",color=\"Category\", hover_data=[\"Labels\"], height=500, color_discrete_sequence=palette)","db9fbc71":"df = pd.concat([df,labels_embedded[[\"Labels_tsne1\",\"Labels_tsne2\"]]], axis=1)\ndf = df.drop(\"Labels\", axis=1)","0a1c0d1f":"print(description[\"Artists\"])","2d9e5ac3":"df.Artists = df.Artists.fillna(\"NA\")\nallstars = []\nfor i in df.index:\n    allstars.extend(df.loc[i, \"Artists\"].split(\"|\"))\nlen(set(allstars))","3b4cf2f4":"# We will put some threshold, not to put some rare artists into one-hot vector.\nthreshold = 3\nothers = Counter(allstars)\nothers = [k for k in others if others[k]<=threshold]\nlen(others)","3b13f7bd":"# Drop all artists who are just in test set or just in train set\nin_train, in_test = [], []\nfor i in df.loc[tr_mask].index:\n    in_train.extend(df.loc[i, \"Artists\"].split(\"|\"))\nfor i in df.loc[~tr_mask].index:\n    in_test.extend(df.loc[i, \"Artists\"].split(\"|\"))\n    \nonly_test = set(in_test) - set(in_train)\nonly_train = set(in_train) - set(in_test)\ndisplay(len(only_test))\ndisplay(len(only_train))","af778354":"allstars = list(set(allstars) - set(others) - only_test - only_train)\nprint(len(allstars))\nothers = set(others) | only_test | only_train\nprint(len(others))","38d1fe35":"res = []\ndef prune(x):\n    vector = np.zeros(len(allstars)+1) #for others\n    x = [i for i in x.split(\"|\")]\n    for i in range(len(allstars)):\n        vector[i]=1 if allstars[i] in x else 0\n    if len(x)>sum(vector):\n        vector[-1]=1\n    res.append(vector)\n\ndf[\"Artists\"].apply(prune)\nonehot_artists = pd.DataFrame(res, columns = allstars+[\"Others\"], index=df.index)","761ac49d":"onehot_artists","a833fc0c":"df[\"Other_Artists\"] = onehot_artists[\"Others\"]\nonehot_artists = onehot_artists.drop(\"Others\", axis=1)\nonehot_artists[\"Category\"] = df[\"Category\"]","1d09e064":"plot_commulative_onehot(onehot_artists)","1f67b921":"artists_embedded = onehot_to_tsne2(onehot_artists, \"Artists\")\n_df = artists_embedded.copy(deep=True)\n_df[[\"Category\",\"Artists\"]] = df[[\"Category\",\"Artists\"]]\nxp.scatter(_df,x=\"Artists_tsne1\",y=\"Artists_tsne2\",color=\"Category\", hover_data=[\"Artists\"], height=500, color_discrete_sequence=palette)","05f99448":"df = pd.concat([df,artists_embedded[[\"Artists_tsne1\",\"Artists_tsne2\"]]], axis=1)\ndf = df.drop(\"Artists\", axis=1)","b2d0bc03":"for i in [\"Track\", \"Version\", \"Album_type\"]:\n    print(description[i])","0936007a":"artists_encoder = LabelEncoder()\ndf[\"Track\"] = artists_encoder.fit_transform(df[\"Track\"])","110c674b":"_df = df.groupby([\"Version\",\"Category\"], as_index=False).count()\nxp.bar(_df,x=\"Version\",y=\"Id\",color=\"Category\", color_discrete_sequence=palette)","83553e55":"df[\"Version\"] = df[\"Version\"].fillna(\"NA\")\nversions = set(df[\"Version\"])\ndf[list(versions)] = OneHotEncoder().fit_transform(df[[\"Version\"]]).toarray()\ndf = df.drop([\"Version\",\"NA\"], axis=1)","3c2d95a1":"_df = df.groupby([\"Album_type\",\"Category\"], as_index=False).count()\nxp.bar(_df,x=\"Album_type\",y=\"Id\",color=\"Category\", color_discrete_sequence=palette)","a21cf17d":"df[\"Album_type\"] = df[\"Album_type\"].fillna(\"NA\")\nversions = set(df[\"Album_type\"])\ndf[list(versions)] = OneHotEncoder().fit_transform(df[[\"Album_type\"]]).toarray()\ndf = df.drop([\"Album_type\",\"NA\"], axis=1)","8320714d":"print(description[\"Album\"])","70819dd0":"df[\"Album\"] = df[\"Album\"].fillna(\"NA\")\nganres_onehot = split_to_onehot(df, \"Album\")\nplot_commulative_onehot(ganres_onehot)","0818db6e":"album_embedded = onehot_to_tsne2(onehot_artists, \"Album\")\n_df = album_embedded.copy(deep=True)\n_df[[\"Category\",\"Album\"]] = df[[\"Category\",\"Album\"]]\nxp.scatter(_df,x=\"Album_tsne1\",y=\"Album_tsne2\",color=\"Category\", hover_data=[\"Album\"], height=500, color_discrete_sequence=palette)","cac7fe70":"df = pd.concat([df,album_embedded[[\"Album_tsne1\",\"Album_tsne2\"]]], axis=1)\ndf = df.drop(\"Album\", axis=1)","29cebcd7":"print(description[\"Vocal\"])","e9f5e3a5":"df[\"Vocal\"] = df[\"Vocal\"].fillna('N')\nonehot = np.zeros((len(df),2))\nfor i in range(len(df)):\n    v = df.iloc[i][\"Vocal\"]\n    if v == 'F':\n        onehot[i] = [1,0]\n    elif v == 'M':\n        onehot[i] = [0,1]\n    elif v == 'F|M':\n        onehot[i] = [1,1]\ndf[[\"Fem_voc\",\"Mal_voc\"]] = onehot\ndf = df.drop(\"Vocal\",axis=1)","fe976f95":"print(description[\"Country\"])","5fde820d":"df[\"Country\"] = df[\"Country\"].fillna(\"NA\")\ncountry_onehot = split_to_onehot(df, \"Country\")\nplot_commulative_onehot(country_onehot)","828d7e38":"country_onehot = country_onehot.drop(\"Category\", axis=1)\ndf = pd.concat([df,country_onehot], axis=1)\ndf = df.drop(\"Country\", axis=1)","47c9fb97":"df","89e83c47":"x, y = df.loc[tr_mask].iloc[:,2:], df.loc[tr_mask,\"Category\"]\ndeploy = df.loc[~tr_mask].iloc[:,2:]","a4437a6f":"model_c = RandomForestClassifier()\nmodel_c.fit(x,y)","18a1b8ee":"sample = pd.read_csv(f\"{PATH}sample_submition.csv\")\nsample[\"Category\"] = model_c.predict(deploy)\nsample[\"Category\"] = (sample[\"Category\"]==\"like\").astype(int)\nsample","e356f4c1":"sample.to_csv(\"submission.csv\", index=False)","d619a331":"# 1. Explorational Data Analysis \n\nOn this part we analise the data. We have to check what kind of values do we have in it. How many categorical and continuouse attributes do we have. If there are any None values and what can we do with it.","1097e0e5":"### 2.7.1 Tracks","d10869f2":"From the analisys of this feature we can say:\n* Major Tracks has more \"likes\" then Minor ones - **190\/142=1.34** vs **161\/172=0.94**\n* D# looks like the most disliked Key but at the same time - there are not much tracks on this key, may not be much significant.\n* The biggest proportion of likes\/dislikes is in A Major key - **11\/4 = 2.75** and C Major key - **36\/17 = 2.117**","cb821a31":"## 3 Model selection","c904e949":"From the analisys of this feature we can say:\n* There are some favorite artists, such as: **twenty one pilots, radiohead, monatik, rhcp, \u0441\u043a\u0440\u0438\u043f\u0442\u043e\u043d\u0438\u0442, etc.**\n* There are some artists that i do not like much: **morgenshtern, 6ix9ine, \u042d\u043b\u0434\u0436\u0435\u0439, Lady GaGa, Pitbul etc.**","499ec389":"## 2.4 Energy,Happiness,Dancebility, BPM","fa835d82":"Some methods, which will help us to work with values, separated with \"|\". Use them in case you do not know how to process such values. Also, other ideas are welcome.","3d0f4b4a":"## 2.8 Vocal","ec0ff1c3":"## 2.2 Release year feature","88570c5b":"## 2.5 Labels","df40e49d":"In this part I represent the simple example of model selection. \n\nRidgeClassifierWe will take one specific algorythm (RidgeClassifier - to show the concept of hyperrparameter tuning) and try to find the best configuration of the model. At the same time we will apply shuffle split to validate our model. The best result, should be the best solution (but usually may not be the best one on the test set). Experiment with the different technologies, libraries and approaches, to achieve the better result.","b740bfab":"Nothing to do here. We will just add some propotional values of the same features.","cc954595":"## 2.7 Tracks, Version, Album_type","4ad21495":"## 2.6 Artists","04c15253":"From the analisys of this feature we can say:\n* There are some ganres that I do not like (or not really): **dance, house, dnb, latinfolk, epicmetal, electronics, ruspop**\n* There are some ganres that I do mostly like: **rock, indie, pop, rnd, soul, numetal**","a79d9ff1":"### 2.7.2 Version","f0db09ad":"We can see, that we have to much values in one-hot vector representation. It may cause much problems, during the fitting of our model (it couldn't find distinctive dependencies in sparse matrix). In this case we will apply TSNE as a handy solution, to transfer our n columns into two, without losing the information. I recommend you to experiment with this step during your work.","c4aa8580":"## 2.8 Album","d9973985":"In this cases we do not have much features (so our one-hot representation wont be too spars) so we will apply onehot or label encoders (dependong of the number of features) to encode out values","ad779cd8":"## 2.3 Genres features","4a11782b":"### 2.7.3 Album_type","0e751c82":"## 2.1 Key feature","b320a8b1":"Before we start we need to download the train\/test sets. To simplify the operations with the dataset and its visualisation - we will put all the data togather and create the mask, to separate train and test sets.","5fe14b14":"# 2. Data Preparation and Feature Engeneering\n\nIn this part we will analyze data attributes, and transform them to the for strict form. During this process we will analize the features and find some supposed dependancies in the data. For that cases, that may be transformed - we will create new features, which will describe our data better. I the end of this step we will get the dataset, ready for modeling with ML algorythms.","52055d77":"## 2.9 Country","67710e7c":"This feature is the most complicated one. For the example, lets assume, that the main artist for the track - named the 1st. Others will be named as collaborators.This is cause for not to make the one-hot matrix too spars putting all the artists to the single columns. Experiment with this feature - the result of its preprocessing may influence significantly to the final result.","d7c31544":"From the analisys of this feature we can get the porportion of likes\/ dislikes:\n\n| Decade | like\/dislike  | koeff |\n| ------ |:-------------:| -----:|\n| 1980s  | 9\/6           | 1.5   |\n| 1990s  | 16\/8          | 2     |\n| 2000s  | 81\/90         | 0.9   |\n| 2010s  | 213\/153       | 1.39  |\n| 2020s  | 32\/57         | 0.56  |\n\nNumber of values in decades less then 1990 - may not be destinctive, but according to the koefficient - this music i like the most (this is true).\n\nAt the same time, the modern music (Like, two last years) is not in my top ."}}