{"cell_type":{"17f203d3":"code","53e9386d":"code","d3d3a056":"code","c9e80db1":"code","62f3a9fe":"code","0bcc2d3b":"code","ab97ff73":"code","ab5554f4":"code","b18d4321":"code","9ebdf84c":"code","f1e0b58a":"code","3a78378f":"code","7f9720ea":"code","1d54ec63":"code","e3ac2b2d":"code","d4b76a03":"code","b361bdc3":"code","5ee8bd86":"code","8a817921":"code","e6c93e87":"code","25ced845":"code","a9af927c":"code","be91f57b":"code","7448e3ff":"code","d2fd713e":"code","04489957":"code","a7449392":"code","338456bb":"code","2cb204f5":"code","fc5d0240":"code","282ac968":"code","250874db":"code","3c53e347":"code","5ab1fac7":"markdown","5e8cd704":"markdown","fe488320":"markdown","b66b2b6c":"markdown","91c62f23":"markdown","3061991e":"markdown","3735285d":"markdown","6e46ec2e":"markdown","a61d1812":"markdown","faa7e7bb":"markdown","484bfe9c":"markdown","71b2d91a":"markdown","51a69419":"markdown","4baf17b2":"markdown","da34f2cf":"markdown","f40b5d62":"markdown","f0009e7c":"markdown","ca6bc881":"markdown","2df645c0":"markdown","e3f7e871":"markdown"},"source":{"17f203d3":"import pandas as pd\nimport numpy as np\nimport cv2\nimport os\nimport re\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\nimport matplotlib.patches as patches\nimport seaborn as sns # plotting","53e9386d":"path = '..\/input\/global-wheat-detection'\ndir_train = os.path.join(path,'train')\ntrain_df=pd.read_csv(os.path.join(path,'train.csv'))\ntrain_df.head()","d3d3a056":"fig, ax = plt.subplots(1)\nax.imshow(plt.imread(os.path.join(path,'train',(train_df['image_id'][3]+'.jpg'))))\nxy = [834.0,95.0]\nw = 74.0\nh = 160.0\nrect = patches.Rectangle(xy, w, h, linewidth=2, edgecolor='b', facecolor='none')\nax.add_patch(rect)\nplt.show()","c9e80db1":"train_df[['x','y','w','h']] = 0\ntrain_df[['x','y','w','h']]= np.stack(train_df['bbox'].apply(lambda x:np.fromstring(x[1:-1], sep=','))).astype(np.float)\n\ntrain_df.drop(columns=['bbox','source'], inplace=True)","62f3a9fe":"train_df['class'] = \"wheat_head\"\ntrain_df.head()","0bcc2d3b":"image_ids = train_df['image_id'].unique()\nprint(\"Total number of images = \",len(train_df['image_id']))\nprint(\"Number of Unique images = \",len(image_ids))","ab97ff73":"split_len = round(len(image_ids)*0.8)\n\ntrain_ids = image_ids[:split_len]\nvalid_ids = image_ids[split_len:]\n\ntrain = train_df[train_df['image_id'].isin(train_ids)]\nvalid = train_df[train_df['image_id'].isin(valid_ids)]\n\nvalid.shape, train.shape","ab5554f4":"print(f'Minumum number of wheat heads: {max(train_df[\"image_id\"].value_counts())}')\nprint(f'Minimum number of wheat heads: {len(train_df)\/train_df[\"image_id\"].nunique()}')\n\nprint(\"Total number of images = \",len(train_df['image_id']))\nprint(f'Number of Unique images: {len(image_ids)}')\nprint(f'Total number of images: {len(os.listdir(os.path.join(path, \"train\")))}')","b18d4321":"sns.displot(train_df['image_id'].value_counts(), kde=False)\nplt.xlabel('# of wheat heads')\nplt.ylabel('# of images')\nplt.title('# of wheat heads vs. # of images')\nplt.show()","9ebdf84c":"import albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch\nimport torchvision\n\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\n\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SequentialSampler","f1e0b58a":"class WheatDataset(Dataset):\n    def __init__(self, dataframe, image_dir, transforms=None):\n        super().__init__()\n\n        self.image_ids = dataframe['image_id'].unique()\n        self.df = dataframe\n        self.image_dir = image_dir\n        self.transforms = transforms\n        # dataframe.unique() = \uc720\uc77c\ud55c \uac12 \ucc3e\uae30\n        # dataframe.value_counts() = \uc720\uc77c\ud55c \uac12 \ubcc4 \uac2f\uc218 \uc138\uae30\n        \n    def __getitem__(self, index: int):\n\n        image_id = self.image_ids[index]\n        records = self.df[self.df['image_id'] == image_id]\n\n        image = cv2.imread(f'{self.image_dir}\/{image_id}.jpg', cv2.IMREAD_COLOR)\n        # convert imreaded image BGR to RGB\n        # opencv2 loads image by BGR order\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        # convert image 0 ~ 255 to 0 ~ 1\n        image \/= 255.0\n        \n        boxes = records[['x', 'y', 'w', 'h']].values\n        boxes[:, 2] = boxes[:, 0] + boxes[:, 2] # x + w\n        boxes[:, 3] = boxes[:, 1] + boxes[:, 3] # y + h\n        # boxes -> left up corner & right bottom corner\n        \n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        area = torch.as_tensor(area, dtype=torch.float32)\n        \n        \n        # there is only one class\n        labels = torch.ones((records.shape[0],), dtype=torch.int64)\n        \n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n        \n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = labels\n        # target['masks'] = None\n        target['image_id'] = torch.tensor([index])\n        target['area'] = area\n        target['iscrowd'] = iscrowd\n\n        if self.transforms:\n            sample = {\n                'image': image,\n                'bboxes': target['boxes'],\n                'labels': labels\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n            \n            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n            # permute : \ubaa8\ub4e0 \ucc28\uc6d0\uc758 \uc21c\uc11c\ub97c \uc7ac\ubc30\uce58 0->1, 1->0\n            # instead torch.tensor(sample['bboxes'])\ub85c \uc368\ub3c4 \ubb34\ubc29\n\n        #return image, target, image_id\n        return {\n            'image_id': image_id,\n            'image':image,\n            'boxes':boxes,\n            'area':area,\n        }\n\n    def __len__(self) -> int:\n        #return self.image_ids.shape[0]\n        return len(self.image_ids)","3a78378f":"# Code from Data Augmentation Tutorial\nclass WheatDataset(Dataset):\n    def __init__(self, dataframe, image_dir, transforms=None):\n        super().__init__()\n\n        self.image_ids = dataframe['image_id'].unique()\n        self.df = dataframe\n        self.image_dir = image_dir\n        self.transforms = transforms\n        # dataframe.unique() = \uc720\uc77c\ud55c \uac12 \ucc3e\uae30\n        # dataframe.value_counts() = \uc720\uc77c\ud55c \uac12 \ubcc4 \uac2f\uc218 \uc138\uae30\n         \n    def __len__(self):\n        return len(self.image_ids)\n    \n    def __getitem__(self, index):\n        image_id = self.image_ids[index]\n        records = self.df[self.df['image_id'] == image_id]\n\n        image = cv2.imread(f'{self.image_dir}\/{image_id}.jpg', cv2.IMREAD_COLOR)\n        # convert imreaded image BGR to RGB\n        # opencv2 loads image by BGR order\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        # convert image 0 ~ 255 to 0 ~ 1\n        image \/= 255.0\n        \n        \n        ###\n        boxes = records[['x', 'y', 'w', 'h']].values\n        boxes[:, 2] = boxes[:, 0] + boxes[:, 2] # x + w\n        boxes[:, 3] = boxes[:, 1] + boxes[:, 3] # y + h\n        # boxes -> left up corner & right bottom corner\n        \n        n_objects = len(records)\n        boxes, areas = [], []\n        for i in range(n_objects):\n            x_min = records.iloc[i]['x']\n            x_max = records.iloc[i]['y']\n            y_min = records.iloc[i]['x'] + records.iloc[i]['w']\n            y_max = records.iloc[i]['y'] + records.iloc[i]['h']\n            a_rea = records.iloc[i]['w'] * records.iloc[i]['h']\n            boxes.append([x_min, y_min, x_max, y_max])\n            areas.append(a_rea)\n        \n\n        return {\n            'image_id': image_id,\n            'image': image,\n            'boxes': boxes,\n            'area': areas,\n        }","7f9720ea":"# Albumentations\n# ToTensorV2 : convert image and mask to torch.Tensor\ndef get_train_transform():\n    return A.Compose([\n        A.Flip(0.5),\n        A.augmentations.transforms.Cutout(num_holes=3, max_h_size=64, max_w_size=64, fill_value = 0, p=0.5),\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n\ndef get_valid_transform():\n    return A.Compose([\n        A.Cutout(num_holes=3, max_h_size=32, max_w_size=32, fill_value=0, p=0.5),\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})","1d54ec63":"def mixup(images, bboxes, areas, alpha=1.0):\n    \"\"\"\n    Randomly mixes the given list if images with each other\n    \n    :param images: The images to be mixed up\n    :param bboxes: The bounding boxes (labels)\n    :param areas: The list of area of all the bboxes\n    :param alpha: Required to generate image wieghts (lambda) using beta distribution. In this case we'll use alpha=1, which is same as uniform distribution\n    \"\"\"\n    # Generate random indices to shuffle the images\n    indices = torch.randperm(len(images))\n    shuffled_images = images[indices]\n    shuffled_bboxes = bboxes[indices]\n    shuffled_areas = areas[indices]\n    \n    # Generate image weight (minimum 0.4 and maximum 0.6)\n    lam = np.clip(np.random.beta(alpha, alpha), 0.4, 0.6)\n    print(f'lambda: {lam}')\n    \n    # Weighted Mixup\n    mixedup_images = lam*images + (1 - lam)*shuffled_images\n    \n    mixedup_bboxes, mixedup_areas = [], []\n    for bbox, s_bbox, area, s_area in zip(bboxes, shuffled_bboxes, areas, shuffled_areas):\n        mixedup_bboxes.append(bbox + s_bbox)\n        mixedup_areas.append(area + s_area)\n    \n    return mixedup_images, mixedup_bboxes, mixedup_areas, indices.numpy()","e3ac2b2d":"class Averager:\n    def __init__(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n\n    def send(self, value):\n        self.current_total += value\n        self.iterations += 1\n\n    @property\n    def value(self):\n        if self.iterations == 0:\n            return 0\n        else:\n            return 1.0 * self.current_total \/ self.iterations\n\n    def reset(self):\n        self.current_total = 0.0\n        self.iterations = 0.0","d4b76a03":"#def collate_fn(batch):\n#    return tuple(zip(*batch))\ndef collate_fn(batch):\n    images, bboxes, areas, image_ids = ([] for _ in range(4))\n    for data in batch:\n        images.append(data['image'])\n        bboxes.append(data['boxes'])\n        areas.append(data['area'])\n        image_ids.append(data['image_id'])\n\n    return np.array(images), np.array(bboxes), np.array(areas), np.array(image_ids)\n        \n\n#train_dataset = WheatDataset(train, dir_train, get_train_transform())\n#valid_dataset = WheatDataset(valid, dir_train, get_valid_transform())\ntrain_dataset = WheatDataset(train, dir_train)\nvalid_dataset = WheatDataset(valid, dir_train)\n\n# split the dataset in train and test set\nindices = torch.randperm(len(train_dataset)).tolist()\n\ntrain_data_loader = DataLoader(\n    train_dataset,\n    batch_size=16,\n    shuffle=False,\n    num_workers=4,\n    collate_fn=collate_fn\n)\n\nvalid_data_loader = DataLoader(\n    valid_dataset,\n    batch_size=8,\n    shuffle=False,\n    num_workers=4,\n    collate_fn=collate_fn\n)","b361bdc3":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","5ee8bd86":"#images, targets, image_ids = next(iter(train_data_loader))\n#images = list(image.to(device) for image in images)\n#targets = [{k: v.to(device) for k, v in t.items()} for t in targets]","8a817921":"#boxes = targets[2]['boxes'].cpu().numpy().astype(np.float32)\n#sample = images[2].permute(1,2,0).cpu().numpy()","e6c93e87":"images, bboxes, areas, image_ids = next(iter(train_data_loader))\naug_images, aug_bboxes, aug_areas, aug_indices = mixup(images, bboxes, areas)","25ced845":"def read_image(image_id, image_dir):\n    image = cv2.imread(f'{image_dir}\/{image_id}.jpg', cv2.IMREAD_COLOR)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n    image \/= 255.0\n    return image\n\ndef get_bbox(bboxes, col, color='white', bbox_format='pascal_voc'):\n    for i in range(len(bboxes)):\n        if bbox_format == 'pascal_voc':\n            rect = patches.Rectangle(\n                (bboxes[i][0], bboxes[i][1]),\n                bboxes[i][2] - bboxes[i][0], \n                bboxes[i][3] - bboxes[i][1], \n                linewidth=2, \n                edgecolor=color, \n                facecolor='none')\n        else:\n            rect = patches.Rectangle(\n                (bboxes[i][0], bboxes[i][1]),\n                bboxes[i][2], \n                bboxes[i][3], \n                linewidth=2, \n                edgecolor=color, \n                facecolor='none')\n            \n        # Add the patch to the Axes\n        col.add_patch(rect)","a9af927c":"fig, ax = plt.subplots(nrows=5, ncols=3, figsize=(15, 30))\nfor index in range(5):\n    image_id = image_ids[index]\n    image = read_image(image_id,dir_train)\n\n    get_bbox(bboxes[index], ax[index][0], color='red')\n    ax[index][0].grid(False)\n    ax[index][0].set_xticks([])\n    ax[index][0].set_yticks([])\n    ax[index][0].title.set_text('Original Image #1')\n    ax[index][0].imshow(image)\n    \n    image_id = image_ids[aug_indices[index]]\n    image = read_image(image_id,dir_train)\n    get_bbox(bboxes[aug_indices[index]], ax[index][1], color='red')\n    ax[index][1].grid(False)\n    ax[index][1].set_xticks([])\n    ax[index][1].set_yticks([])\n    ax[index][1].title.set_text('Original Image #2')\n    ax[index][1].imshow(image)\n\n    get_bbox(aug_bboxes[index], ax[index][2], color='red')\n    ax[index][2].grid(False)\n    ax[index][2].set_xticks([])\n    ax[index][2].set_yticks([])\n    ax[index][2].title.set_text(f'Augmented Image: lambda * image1 + (1 - lambda) * image2')\n    ax[index][2].imshow(aug_images[index])\nplt.show()","be91f57b":"fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 30))\n\nsample = read_image(image_ids[2], dir_train)\nboxes = get_bbox(bboxes[2], ax, color='red')\n#for box in boxes:\n#    cv2.rectangle(sample,\n#                 (box[0], box[1]),\n#                 (box[2], box[3]),\n#                 (220, 0, 0), 3)\nax.set_axis_off()\nax.imshow(sample)","7448e3ff":"# load a model; pre-trained on COCO\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\nmodel","d2fd713e":"print(model.roi_heads.box_predictor.cls_score.in_features)\nprint(model.roi_heads.box_predictor.cls_score)\nprint(model.roi_heads.box_predictor)","04489957":"num_classes = 2 # wheat or not(background)\n\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n\n# replace the pre-trained model's head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\nprint(model.roi_heads.box_predictor.cls_score.in_features)\nprint(model.roi_heads.box_predictor.cls_score)\nprint(model.roi_heads.box_predictor)","a7449392":"model.to(device)\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.003, momentum=0.9, weight_decay=0.0005)\nlr_scheduler = None\n\nnum_epochs = 2","338456bb":"loss_hist = Averager()\nitr = 1\n\nfor epoch in range(num_epochs):\n    loss_hist.reset()\n    \n    for images, targets, image_ids in train_data_loader:\n        \n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        \n        # Set loss function\n        loss_dict = model(images, targets)\n        \n        losses = sum(loss for loss in loss_dict.values())\n        loss_value = losses.item()\n        loss_hist.send(loss_value)\n        \n        # Set parameters\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n        \n        # Display option\n        if itr % 50 == 0:\n            print(f\"Iteration #{itr} loss: {loss_value}\")\n        \n        itr += 1\n        \n    # Learning rate scheduler\n    if lr_scheduler is not None:\n        lr_scheduler.step()\n    \n    print(f\"Epoch #{epoch} loss: {loss_hist.value}\")","2cb204f5":"images, targets, image_ids = next(iter(valid_data_loader))","fc5d0240":"images = list(img.to(device) for img in images)\ntargets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\nboxes = targets[1]['boxes'].cpu().numpy().astype(np.int32)\nsample = images[1].permute(1,2,0).cpu().numpy()","282ac968":"model.eval()\ncpu_device = torch.device(\"cpu\")\n\noutputs = model(images)\noutputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]","250874db":"fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\nfor box in boxes:\n    cv2.rectangle(sample,\n                  (box[0], box[1]),\n                  (box[2], box[3]),\n                  (220, 0, 0), 3)\n    \nax.set_axis_off()\nax.imshow(sample)","3c53e347":"torch.save(model.state_dict(), 'fasterrcnn_resnet50_fpn.pth')","5ab1fac7":"### Set parameter\n\n* optimizer: SGD<br>\n* learning rate: 0.005<br>\n* momentum: 0.9<br>\n* weight decay: 0.0005<br>\n* learning scheduler: No<br>\n* number of epochs: 2","5e8cd704":"## Global Wheat Detection with Pytorch\nThis notebook is made for basic understanding about the dataset and flow of the detection with pytorch.<br>\nThe reference notebook is [here.](https:\/\/www.kaggle.com\/pestipeti\/pytorch-starter-fasterrcnn-train)\n* Pytorch starter - FasterRCNN Train\n<br>\n\nThe goal of this competition [Global Wheat Detection](https:\/\/www.kaggle.com\/c\/global-wheat-detection\/overview) is bounding box prediction around each wheat head in images that have them. <br>\nIf there are no wheat heads, there is no bounding boxes.","fe488320":"### Check the loaded image","b66b2b6c":"### 2. MixUp\n\n[Reference: Data Augmentation Tutorial: Basic, Cutout, Mixup](https:\/\/www.kaggle.com\/kaushal2896\/data-augmentation-tutorial-basic-cutout-mixup)","91c62f23":"Also <code>Albumentations<\/code> provides <code>Cutout<\/code> function.<br>\nIt can easily adjust directly in your image augmentation.","3061991e":"## 3. Build Dataset\n\n### Options\n1. More augmentation\n2. MixUp\n3. CutMix\n4. Cutout -> with <code>albumentations<\/code>\n4. <s>Add new dataset<\/s>","3735285d":"If you have any gpu with cuda, make the profram running on GPU, otherwise running on CPU.","6e46ec2e":"### Number of images","a61d1812":"### Distribution of number of bbox","faa7e7bb":"So there are 3422 images in traing directory and 3373 images are annotated, that means there are 49 images which do not have any wheat heads in it (without annotation)","484bfe9c":"### Split train data into Train and Validation\nTo prevent same image going to train and validation dataset, the unique id value is used.","71b2d91a":"## 2. Data EDA\n* Reference Notebook [Global Wheat Detection: Starter EDA](https:\/\/www.kaggle.com\/kaushal2896\/global-wheat-detection-starter-eda)","51a69419":"## 4. Train model","4baf17b2":"### 1. Albumentations - fast image augmentation libarary & Cutout<br>\n\n<code>Albumentations<\/code> is an image augmentation library. It can substitute <code>torchvision.transforms<\/code>. Albumentation is much faster and easily replace. \n\nThe detail materials from [albumentations github](https:\/\/github.com\/albumentations-team\/albumentations_examples)\n\n![image.png](attachment:image.png)","da34f2cf":"### **Files**\n* train.csv - the training data\n* sample_submission.csv - a sample submission file in the correct format\n* train.zip - training images\n* test.zip - test images","f40b5d62":"### Split Bounding Boxes into X, Y, Width, Height Column","f0009e7c":"<code>iscrowd<\/code> means the condition of labeling, whether many objects with tiny size are treated as a grouping box or not. In image labeling, object could be hided or enclosed.","ca6bc881":"## 1. Data Preprocessing\n### Load Data","2df645c0":"### Load Pretrained model on COCO\n<code>out_features<\/code> becomes 2 which is as same as our target.","e3f7e871":"### **Columns**\n* image_id - the unique image ID\n* width, height - the width and height of the images\n* bbox - a bounding box, formatted as a Python-style list of [xmin, ymin, width, height]\n* etc.\n\nFrom the Data Explorer, the size of train images are same, 1024 x 1024.<br>\nTo make simple code, bounding box(bbox) is needed to be splited."}}