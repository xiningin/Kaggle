{"cell_type":{"b5586788":"code","6adc753b":"code","0a7ca550":"code","f7b8879a":"code","315c70da":"code","deb6b511":"code","da2d2fa2":"code","d0f78e52":"code","deccf65d":"code","218d6454":"code","04f770a7":"code","ab22e111":"code","2ee5bf19":"code","32ef3fc2":"code","7c130dee":"code","d34422be":"code","566e8e70":"code","d81c4a7d":"code","ebe7e8b5":"code","8072311a":"code","0d0500c4":"code","465c7aa5":"code","d214b0c4":"code","53c85734":"markdown","51161a3f":"markdown","bc7d7190":"markdown","8d03be6d":"markdown","6439332b":"markdown","7b1e688a":"markdown","6dae28d0":"markdown","d5315cc6":"markdown","fce12c72":"markdown","8c00bd7f":"markdown","c031d9dc":"markdown","fde8be87":"markdown","7cae7352":"markdown","99791e1b":"markdown","12a34aec":"markdown","cdb28d7d":"markdown","32572506":"markdown","37cacc86":"markdown","ace21da3":"markdown","4f1472b1":"markdown","65726d13":"markdown"},"source":{"b5586788":"pip install alibi","6adc753b":"%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_boston\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom alibi.explainers import ALE, plot_ale","0a7ca550":"data = load_boston()\nfeature_names = data.feature_names\nX = data.data\ny = data.target\nprint(feature_names)","f7b8879a":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","315c70da":"lr = LinearRegression()","deb6b511":"lr.fit(X_train, y_train)","da2d2fa2":"mean_squared_error(y_test, lr.predict(X_test))","d0f78e52":"rf = RandomForestRegressor()\nrf.fit(X_train, y_train)","deccf65d":"mean_squared_error(y_test, rf.predict(X_test))","218d6454":"FEATURE = 'RM'\nindex = np.where(feature_names==FEATURE)[0][0]\n\nfig, ax = plt.subplots()\nax.scatter(X_train[:, index], lr.predict(X_train));\n\nax.set_xlabel(FEATURE);\nax.set_ylabel('Value in $1000\\'s');","04f770a7":"lr_ale = ALE(lr.predict, feature_names=feature_names, target_names=['Value in $1000\\'s'])\nrf_ale = ALE(rf.predict, feature_names=feature_names, target_names=['Value in $1000\\'s'])","ab22e111":"lr_exp = lr_ale.explain(X_train)\nrf_exp = rf_ale.explain(X_train)","2ee5bf19":"lr_exp.feature_names","32ef3fc2":"plot_ale(lr_exp, fig_kw={'figwidth':10, 'figheight': 10});","7c130dee":"plot_ale(lr_exp, features=['RM']);","d34422be":"lower_index = np.where(lr_exp.feature_values[5] < 8)[0][-1]\nupper_index = np.where(lr_exp.feature_values[5] > 8)[0][0]\nsubset = X_train[(X_train[:, 5] > lr_exp.feature_values[5][lower_index])\n                 & (X_train[:, 5] < lr_exp.feature_values[5][upper_index])]\nprint(subset.shape)","566e8e70":"subset_pred = lr.predict(subset).mean()\nsubset_pred","d81c4a7d":"mean_pred = lr.predict(X_train).mean()\nmean_pred","ebe7e8b5":"subset_pred - mean_pred","8072311a":"#Crime level\nplot_ale(lr_exp, features=['CRIM']);","0d0500c4":"axes = plot_ale(rf_exp, fig_kw={'figwidth':10, 'figheight': 10});","465c7aa5":"fig, ax = plt.subplots()\nplot_ale(lr_exp, features=['RM'], ax=ax, line_kw={'label': 'Linear regression'});\nplot_ale(rf_exp, features=['RM'], ax=ax, line_kw={'label': 'Random forest'});","d214b0c4":"fig, ax = plt.subplots(5, 3, sharey='all');\n\nplot_ale(lr_exp, ax=ax, fig_kw={'figwidth':10, 'figheight': 10},\n         line_kw={'label': 'Linear regression'});\nplot_ale(rf_exp, ax=ax, line_kw={'label': 'Random forest'});","53c85734":"This difference is the total increase in prive in 1000's for neigbourhoods with the average room number close to 8. We have subtracted the mean averaged over entire dataset from prediction on subset(RM~8) since the ALE is always relative to the average prediction.","51161a3f":"Because the model is non-linear, the ALE plots are non-linear and non-monotonic in some cases. Similar to the previous examples,the ALE value at a point is the relative feature effect with respect to the mean feature effect. \n\nFrom these plots, it is seen that that the feature RM has the largest impact on the prediction. ","bc7d7190":"PLotting ALE first on Linear Regression model(linear) -","8d03be6d":"Similarly , now ALE is applied on Random Forest model (non-linear) -","6439332b":"Let us take another example to analyse the advantage and need of using ALE over other methods-","7b1e688a":"ALE is a global explanation method that takes in complete data for which the model feature effects are computed.","6dae28d0":"INTERPRETATION -\nIt is clearly seen that the graph is increasing in nature(positive correlation) i.e. as the value of RM increases, the y axis that shows the price value in 1000's also increases.\n\nTo study the influence of all feaatures and not just RM we require ALE to block the effects of other features and discuss the impact of a particular feature.\n","d5315cc6":"The mean prediction on this subset is -","fce12c72":"ALE plot though being a global interpretation method and thus taking the whole dataset in consideration, gives feature effects for all values separately, unlike PDP and other feature importance plots. For example, in the above plot, the dots show the value of ALE on y axis for every value of the feature CRIM. The individual dots in the plot shows individual values instead of calculating the importance in general. For eg, at CRIM ~ 30, the ALE value is -3 showing that the model predicts a decrease of $3000 in price(higher the crime level, the lower is the price).\n\nFrom interval 30 to 85, due to lack of or no data, it has simply interpolated the line.\n\nThus, these can help assess in which areas of the feature space, the estimated results of feature importance are more reliable.","8c00bd7f":"The above plot can be interpreted as follows -\nFor a particular value of RM on x, the y axis shows how much increase or decrease(positive-increase, negative-decrease) is done on the price. \nThe ALE on the y-axis of the plot above is in the units of the prediction variable which, in this case, is the value of the house in $1000's. \n\nFor example, the ALE value for the point RM=8 is ~7.5. this means that the for neighbourhoods for which the average number of rooms is ~8 the model predicts an increase of ~$7500 due to feature RM. On the other hand, for neighbourhoods with an average number of rooms lower than ~6.25, the impact on the prediction becomes negative, i.e. a smaller number of rooms lowers the predicted value.\n\nAll ALE values are relative to the average prediction which is discussed in detail below-","c031d9dc":"### Comparing ALE for Linear Regression and Random Forest for all the features","fde8be87":"Below shown is a scatter plot between the feature 'RM' vs the model predictions.","7cae7352":"Applying ALE","99791e1b":"The dataset used is Boston Housing Prices(regression).","12a34aec":"2 separate models, one linear and one non linear, have been applied for analysis -\n### 1. Linear Regression","cdb28d7d":"###2. Random Forest","32572506":"# Accumulated Local Effects (ALE)  -\n\nfor determining the feature effects on a model and how these work on linear and non linear models.","37cacc86":"INTERPRETATION-\n\nWhile the linear regression feature effects of RM are positively correlated (the higher the no. of rooms, the higher the price), the random forest feature effects are increasing in a non linear fashion.\n","ace21da3":"The neighbourhoods for which the average number of rooms are close to 8 -","4f1472b1":"To study the interpretation, let us consider one feature such as effect of 'RM' and analyse.","65726d13":"The mean prediction averaged across the whole dataset is-"}}