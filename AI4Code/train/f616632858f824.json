{"cell_type":{"e571e5d5":"code","2fef0240":"code","47a1cd2c":"code","1e09ae80":"code","9e1eb916":"code","76fce9cd":"code","68efd111":"code","c147d4f7":"code","7d264d2e":"code","70a43ed9":"code","4e782242":"code","5369c614":"code","3bfb8939":"code","2ae5b714":"code","ff2111f1":"code","672d3175":"code","8af77f09":"code","7c661ab2":"code","4a725a1a":"code","3d7ee7db":"code","8d231d82":"code","21c13f35":"code","deb4530f":"code","b7c91650":"code","066fa5d1":"code","7879b85b":"code","fdd12f17":"code","d93a3715":"code","62987aca":"code","ecb1c835":"code","a89771cf":"code","8c66572d":"code","96a80b09":"code","c420b17d":"code","27b38770":"code","8ce78ffc":"code","3e73b714":"code","b011098b":"code","09bb83d6":"markdown","b5603f4d":"markdown","1a0b190d":"markdown","de4fe674":"markdown","7085da7d":"markdown","ba1b935c":"markdown","47b4752a":"markdown","f4fe4123":"markdown","71138f5c":"markdown","b968fdd1":"markdown","016624b5":"markdown","8374f994":"markdown","5003e90d":"markdown","ac61bea7":"markdown","89a93eda":"markdown","bdc85e6c":"markdown","97f6f77e":"markdown","af2d01e9":"markdown","c68e6803":"markdown","f62e8899":"markdown","e238ff33":"markdown","a9c2ea28":"markdown","c5bfc3d1":"markdown"},"source":{"e571e5d5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import layers\nfrom keras import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\nfrom keras.callbacks import EarlyStopping,ModelCheckpoint\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2fef0240":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntrain.info()","47a1cd2c":"age_mean = train['Age'].mean()\ntrain['Age'].fillna(age_mean, inplace = True)\ntrain['Embarked'].fillna('S', inplace = True)\ntrain.info()","1e09ae80":"bins = pd.IntervalIndex.from_tuples([(0, 10), (10, 18), (18, 25), (25, 40), (40, 60), (60, 80)])\ntrain['Age'] = pd.cut(train['Age'], bins)\n\ntrain.head()","9e1eb916":"bins = pd.IntervalIndex.from_tuples([(0, 7.91), (7.91, 14.454), (14.454, 31), (31, 512.329)])\ntrain['Fare'] = pd.cut(train['Fare'], bins)\n\ntrain","76fce9cd":"train['FamilySize'] = train['SibSp'] + train['Parch'] + 1\n    \n\ntrain['IsAlone'] = 0\ntrain.loc[train['FamilySize'] == 1, 'IsAlone'] = 1\n\ntrain","68efd111":"df = train[['Sex','Fare','IsAlone','FamilySize','Pclass','Age','Embarked']]\ndf","c147d4f7":"X = pd.get_dummies(df)\ny = pd.get_dummies(train[['Survived']])\n\nX_train = X[:650]\nX_val = X[240:]\ny_train = y[:650]\ny_val = y[240:]\n\nX","7d264d2e":"model = Sequential()\nmodel.add(Dense(18, input_dim=18, activation='relu'))\nmodel.add(Dense(72, activation='relu'))\nmodel.add(Dense(54, activation='relu'))\nmodel.add(Dense(36, activation='relu'))\nmodel.add(Dense(18, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))","70a43ed9":"model.compile(\n    optimizer=SGD(learning_rate=0.01),  # Optimizer\n    # Loss function to minimize\n    loss=keras.losses.BinaryCrossentropy(),\n    # List of metrics to monitor\n    metrics=['accuracy']\n)","4e782242":"callback1 = EarlyStopping(monitor='val_loss', \n                         patience=50,\n                         restore_best_weights=True\n                        )\n\n\ncheckpoint_filepath = '\/tmp\/checkpoint'\n\nmodel_checkpoint_callback = ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=True,\n    monitor='val_loss',\n    mode='min',\n    save_best_only=True)\n\nhistory = model.fit(\n    X_train,\n    y_train,\n    batch_size=1,\n    epochs=1000,\n    callbacks=[callback1,model_checkpoint_callback],\n    validation_data= (X_val, y_val)\n    )\n        # We pass some validation for\n        # monitoring validation loss and metrics\n        # at the end of each epoch","5369c614":"# The model weights (that are considered the best) are loaded into the model.\nmodel.load_weights(checkpoint_filepath)","3bfb8939":"from sklearn.metrics import confusion_matrix\n\ntestingNN = model.predict(\n    X_val,\n    steps=None,\n    callbacks=None,\n    max_queue_size=10,\n    workers=1,\n    use_multiprocessing=False,\n)\n\nprediction = []\nfor res in testingNN:\n    if res[0]>0.435:\n        prediction.append(1)\n    else:\n        prediction.append(0)\n\ny_true = np.array(y_val)\npd.DataFrame(confusion_matrix(y_true, prediction))","2ae5b714":"test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntest['Age'].fillna(age_mean, inplace = True)\ntest.info()","ff2111f1":"bins = pd.IntervalIndex.from_tuples([(0, 7.91), (7.91, 14.454), (14.454, 31), (31, 512.329)])\ntest['Fare'] = pd.cut(test['Fare'],bins)\n\nbins = pd.IntervalIndex.from_tuples([(0, 10), (10, 18), (18, 25), (25, 40), (40, 60), (60, 80)])\ntest['Age'] = pd.cut(test['Age'],bins)\n\ntest['FamilySize'] = test['SibSp'] + test['Parch'] + 1\n    \n\ntest['IsAlone'] = 0\ntest.loc[test['FamilySize'] == 1, 'IsAlone'] = 1\n\n\n\ndf2 = test[['Sex','Fare','IsAlone','FamilySize','Pclass','Age','Embarked']]\nX_test = pd.get_dummies(df2)\nX_test","672d3175":"Results = model.predict(\n    X_test,\n    batch_size=1,\n    steps=None,\n    callbacks=None,\n    max_queue_size=10,\n    workers=1,\n    use_multiprocessing=False,\n)","8af77f09":"predicted = []\nfor res in Results:\n    if res[0]>=0.435:\n        predicted.append(1)\n    else:\n        predicted.append(0)","7c661ab2":"final_df = pd.DataFrame(test[\"PassengerId\"]) \nfinal_df[\"Survived\"] = predicted\nfinal_df","4a725a1a":"final_df.to_csv('NN.csv',index=False)","3d7ee7db":"from sklearn.preprocessing import LabelEncoder\nlabel = LabelEncoder()\ndicts = {}\n\n\ntrain_dtc = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntrain_dtc = train_dtc.drop(['PassengerId','Name','Cabin','Ticket','Fare'],axis=1)\n\ntrain_dtc = pd.DataFrame(train_dtc.dropna())\nverif_dtc = pd.DataFrame(train_dtc['Survived'])\ntrain_dtc = (train_dtc.drop(['Survived'],axis=1))\n\ntrain_dtc = pd.get_dummies(train_dtc)\n\nprint(verif_dtc.info(),train_dtc.info())","8d231d82":"from sklearn.ensemble import RandomForestClassifier\nnp.random.seed(42)\n\nrfc=RandomForestClassifier(n_estimators = 100, \n                           max_features='auto', \n                           criterion='entropy',\n                           max_depth=10)\n\nrfc.fit(X_train,y_train.values.ravel())\nrfc_preds= rfc.predict(X_val)\n\nfrom sklearn.metrics import accuracy_score\n\naccuracy_score(y_val.values.ravel(),rfc_preds)","21c13f35":"res2 = pd.DataFrame(test[\"PassengerId\"]) \nrfc_preds= rfc.predict(X_test)\nresult2 = pd.DataFrame(rfc_preds)\nres2['Survived'] = result2\nres2","deb4530f":"res2.to_csv('RFC.csv',index=False)","b7c91650":"train_dtc = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntrain_dtc = train_dtc.drop(['PassengerId','Name','Cabin','Ticket','Fare'],axis=1)\n\ntrain_dtc = pd.DataFrame(train_dtc.dropna())\nverif_dtc = pd.DataFrame(train_dtc['Survived'])\ntrain_dtc = (train_dtc.drop(['Survived'],axis=1))\n\n\nprint(verif_dtc.info(),train_dtc.info())","066fa5d1":"test_dtc = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\ntest_dtc = test_dtc.drop(['PassengerId','Name','Cabin','Ticket','Fare'],axis=1)\ntest_dtc['Age'].fillna(age_mean, inplace = True)\n\nprint(test_dtc.info())","7879b85b":"train_dtc = pd.get_dummies(train_dtc)\ntest_dtc = pd.get_dummies(test_dtc)","fdd12f17":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier(random_state=0)\ndtc.fit(train_dtc,verif_dtc)","d93a3715":"result_dtc = dtc.predict(test_dtc)\nres3 = pd.DataFrame(test[\"PassengerId\"]) \nresult3 = pd.DataFrame(result_dtc)\nres3['Survived'] = result3\nres3","62987aca":"res3.to_csv('DTC.csv',index=False)","ecb1c835":"from sklearn.neighbors import KNeighborsClassifier\n\ntrain_knc = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntrain_knc = train_knc.drop(['PassengerId','Name','Cabin','Ticket','Fare'],axis=1)\n\ntrain_knc = pd.DataFrame(train_knc.dropna())\nverif_knc = pd.DataFrame(train_knc['Survived'])\ntrain_knc = (train_knc.drop(['Survived'],axis=1))\ntrain_knc = pd.get_dummies(train_knc)\n\nprint(verif_knc.info(),train_knc.info())","a89771cf":"test_dtc = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\ntest_dtc = test_dtc.drop(['PassengerId','Name','Cabin','Ticket','Fare'],axis=1)\ntest_dtc['Age'].fillna(age_mean, inplace = True)\ntest_dtc = pd.get_dummies(test_dtc)\nprint(test_dtc.info())","8c66572d":"knc = KNeighborsClassifier(n_neighbors=7)\nknc.fit(train_knc,verif_knc.values.ravel())\nres_knc =  knc.predict(test_dtc)","96a80b09":"res4 = pd.DataFrame(test[\"PassengerId\"]) \nresult4 = pd.DataFrame(res_knc)\nres4['Survived'] = result4\nres4","c420b17d":"res4.to_csv('KNC.csv',index=False)","27b38770":"sum_res = pd.DataFrame([predicted,\nrfc_preds,\nresult_dtc,\nres_knc])\na = sum_res.sum()","8ce78ffc":"res = pd.DataFrame(a, columns=['data'])\nres['Survived'] = (res.data>=2).astype(int)","3e73b714":"res = res.drop(['data'], axis=1)\n\nfinal = pd.DataFrame(test[\"PassengerId\"])\nfinal['Survived'] = res\nfinal","b011098b":"final.to_csv('Total.csv',index=False)","09bb83d6":"First, I ensure that the NaN values for the ages are replaced by the Age's mean.","b5603f4d":"# Summation of all results","1a0b190d":"# Getting the Test Data","de4fe674":"Select the data that is relevat to the problem. Name is not adding any information for example, so I decide to drop it.","7085da7d":"Creation of the NN. The choice of neurons'number was done by testing the efficiency of the system. The number was increased until the precision stopped rising or if the system started to overfit.","ba1b935c":"In order to obtain a more precise result, I decided to compare and use all the results.\nI will look at how many times my systems decicded that someone wwould live.","47b4752a":"# Decision Tree","f4fe4123":"# Training the NN","71138f5c":"**Hi Kaggle Community!\nThis is my first public submission and I have to apologize now: sorry if the format or anything really is not in the way it is supposed to be.\nI tried a very simple solution and it works, I hope it is understandable!\nEnjoy!**","b968fdd1":"Same Process for the Fare.","016624b5":"# Creating The NN","8374f994":"Then, I create buckets for the Age in order to have a clearer data.","5003e90d":"# Random Forest Classifier","ac61bea7":"Assign value 0 or 1 to your outputs.","89a93eda":"# Titanic Challenge","bdc85e6c":"When preparing the data, apply the same preparation as the one done to the Train dataframe.","97f6f77e":"Separate the data between training and validation. I assign a large majority to trainingof course.","af2d01e9":"# Data Preparation","c68e6803":"The callbacks will ensure that the NN has finished training. I do not expect the NN to train for 1K epochs, but you never know.\nrestore_best_weights=True means that I will keep the weights from the best epoch (accuracy or val_accuracy wise, depending which converges first).","f62e8899":"# KNeighbors Classifier","e238ff33":"Here I wanted to try a random forest classifier, to see if I could get a better precision. I did on the training data but the result was not better than my NN.","a9c2ea28":"Get the Test data and fill the gaps in the Age Column.","c5bfc3d1":"Create a csv file for submission."}}