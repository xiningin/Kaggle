{"cell_type":{"cd454196":"code","73bd53e1":"code","4d6e7e40":"code","bf79aae3":"code","7e2e2e02":"code","9a004ec0":"code","273efcdb":"code","60654ddd":"code","ff5ac014":"code","2c87849a":"code","6fd3d25b":"code","68ff8bb7":"code","f822e757":"code","a33905f2":"code","194461c0":"code","9350c0d7":"code","eb8898e7":"code","284e2a1a":"code","4f18b524":"code","b0cdb251":"code","0144ea26":"code","4e6186fd":"code","0c2904e9":"code","97b2e74f":"code","a3731e3b":"code","2fff14fb":"code","67e7d05e":"code","a8da4c64":"code","530def5d":"code","fcff8c2c":"code","f34ab379":"code","776525d1":"code","fbf30d52":"code","9a2e9920":"code","0e8b36ee":"code","586ae170":"code","3ab7b6cc":"code","7891bb20":"code","cc240511":"code","43194656":"code","225e8103":"code","56f48662":"code","aa437b50":"code","31526515":"code","db42a760":"code","518f3e47":"code","c0246738":"code","63fa17f8":"markdown","09d3717e":"markdown","291bf60b":"markdown","394ab575":"markdown","b2a19957":"markdown","45860cae":"markdown","7c1e6fbb":"markdown","5c082a74":"markdown","3b298caa":"markdown","056ddb3f":"markdown","89684a15":"markdown","87744758":"markdown","17f595fd":"markdown","120544b5":"markdown","e798aa81":"markdown","0dfa8a1b":"markdown"},"source":{"cd454196":"import warnings\nwarnings.filterwarnings(\"ignore\")","73bd53e1":"%matplotlib inline\n\nimport numpy as np\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","4d6e7e40":"import os\nprint(os.listdir(\"..\/input\"))","bf79aae3":"data=pd.read_csv('..\/input\/HR_comma_sep.csv')","7e2e2e02":"data.head(5)","9a004ec0":"data.tail(5)","273efcdb":"data.info()","60654ddd":"data.shape","ff5ac014":"data.sample(10)","2c87849a":"# now we have to change the sales to the department and the salary to the numerical values for the better results and better understanding.\ndata.rename(columns={'sales':'department'},inplace=True)\n\ndata['salary']=data['salary'].map({'low':1,'medium':2,'high':3})","6fd3d25b":"data.head()","68ff8bb7":"data.describe()","f822e757":"#How many employees works in each department?\nprint(data['department'].value_counts())","a33905f2":"#How many employees per salary range?\nprint(data['salary'].value_counts())","194461c0":"#4.3 How many employees per salary range and department?\ntable=data.pivot_table(values='satisfaction_level',index='department',columns='salary',aggfunc=np.count_nonzero)\ntable","9350c0d7":"f, axes=plt.subplots(3,3, figsize=(10,10) , sharex=True) \n\nplt.subplots_adjust(wspace=0.5)\n                     \nsns.despine(left=True)\n                    \nsns.boxplot( x= 'satisfaction_level', data=data, orient='v',ax=axes[0,0])\nsns.boxplot( x ='last_evaluation' , data=data, orient='v' , ax =axes[0,1])\nsns.boxplot(x='number_project',data=data, orient='v' , ax =axes[1,0])\nsns.boxplot(x='salary',data=data, orient='v' , ax =axes[1,1])\n              \n","eb8898e7":"plt.figure(figsize=(4,5))\nsns.boxplot( x= 'time_spend_company',  data=data, orient='v');","284e2a1a":"corr=data.corr()\ncorr","4f18b524":"sns.set(style='white')\n\nmask = np.zeros_like(corr, dtype=np.bool)\n\nmask[np.triu_indices_from(mask)] = True\n\n# Inserir a figura\nf, ax = plt.subplots(figsize=(13,8))\n\ncmap = sns.diverging_palette(10,220, as_cmap=True)\n\n#Desenhar o heatmap com a m\u00e1scara\nax = sns.heatmap(corr, mask=mask, cmap=cmap, vmax= .5, annot=True, annot_kws= {'size':11}, square=True, xticklabels=True, yticklabels=True, linewidths=.5, \n           cbar_kws={'shrink': .5}, ax=ax)\nax.set_title('Correlation between variables', fontsize=20);","b0cdb251":"#hypothesis\n\n\n# How many employees left the company?\n\nprint(data['left'].value_counts(),)\nprint(data['left'].value_counts()[1],\"employees left the company\")","0144ea26":"# the plot show the amount of employees that stayed and left the company.\nplt.figure(figsize=(4,5))\nax=sns.countplot(data.left)\ntotal=float(len(data))\n\nfor p in ax.patches:\n    height= p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}'.format(height\/total),\n            ha=\"center\"\n           )\nplt.title('Stayed or Left', fontsize=16)\n","4e6186fd":"#First Hypothesis The first hypothesis is that salary is the reason why the employees left the company. Let's see if is this correct.\nj = sns.factorplot(x='salary', y='left', kind='bar', data=data)\nplt.title('Employees that left by salary level', fontsize=14)\nj.set_xticklabels(['High', 'Medium', 'Low']);","0c2904e9":"#In the graphic Salaries by department is possible to see the distribuition of the salaries by department.\n\nh = sns.factorplot(x = 'salary', hue='department', kind ='count', size = 5,aspect=1.5, data=data, palette='Set1' )\nplt.title(\"Salaries by department\", fontsize=14)\nh.set_xticklabels(['High', 'Medium', 'Low']);","97b2e74f":"#The first hypothesis looks very weak to be the main reason why the employees left the company.\nsns.set()\nplt.figure(figsize=(12,6))\nsns.barplot(x='department',y='salary',hue='left', data=data)\nplt.title('Salary Comaprison', fontsize=14);","a3731e3b":"#second hypothesis \n#The second hypothesis is: employees leave the company because work is not safe.\n\nsns.factorplot(x='Work_accident',y='left',kind='bar', data=data)\nplt.title('Employess that had work accident',fontsize=16);\n","2fff14fb":"print(data.Work_accident.sum())\nprint(data.Work_accident.mean())\nprint((data[data['left']==1]['Work_accident']).sum())      ","67e7d05e":"#third hypothesis\n#Is this company a good place to grow professionally?\nsns.factorplot(x='promotion_last_5years', y='left', kind='bar', data=data)\nplt.title('Employees who have been promoted in the last 5 years', fontsize=16);\n","a8da4c64":"print(data.promotion_last_5years.sum())\nprint(data.promotion_last_5years.mean())\n\n","530def5d":"plt.figure()\nbins=np.linspace(1.0 , 11, 10)\nplt.hist(data[data['left']==1]['time_spend_company'], bins=bins, alpha=1, label='Employees Left'\n)\nplt.hist(data[data['left']==0]['time_spend_company'], bins=bins, alpha = 0.5, label = 'Employee Stayed')\n\nplt.grid(axis='x')\nplt.xticks(np.arange(2,11))\nplt.xlabel('time_spend_company')\nplt.title('Years in the Company',fontsize=16)\nplt.legend(loc='best');","fcff8c2c":"plt.figure(figsize =(7,7))\nbins = np.linspace(0.305, 1.0001, 14)\nplt.hist(data[data['left']==1]['last_evaluation'], bins=bins, alpha=1, label='Employees Left')\nplt.hist(data[data['left']==0]['last_evaluation'], bins=bins, alpha = 0.5, label = 'Employee Stayed')\nplt.title('Employees Performance', fontsize=14)\nplt.xlabel('last_evaluation')\nplt.legend(loc='best');","f34ab379":"poor_performance_left = data[(data.last_evaluation <= 0.62) & (data.number_project == 2) & (data.left == 1)]\nprint('poor_performance_left:',len(poor_performance_left))\n\npoor_performance_stayed = data[(data.last_evaluation > 0.62) & (data.number_project == 2) & (data.left == 1)]\nprint('poor_performance_stayed:',len(poor_performance_stayed))\n\nprint('\\n')\n\nhigh_performance_left= data[(data.last_evaluation <= 0.62) & (data.number_project >=5) & (data.left == 1)]\nhigh_performance_stayed= data[(data.last_evaluation > 0.8) & (data.number_project >=5) & (data.left == 0)]\nprint('high_performance_left:',len(high_performance_left))\nprint('high_performance_stayed', len(high_performance_stayed))\n\nplt.figure(figsize =(7,5))\nbins = np.linspace(1.5,7.5, 7)\nplt.hist(data[data['left']==1]['number_project'], bins=bins, alpha=1, label='Employees Left')\nplt.hist(data[data['left']==0]['number_project'], bins=bins, alpha = 0.5, label = 'Employee Stayed')\nplt.title('Number of projects', fontsize=14)\nplt.xlabel('number_ projects')\nplt.legend(loc='best');","776525d1":"plt.figure(figsize =(7,5))\nbins = np.linspace(80,315, 15)\nplt.hist(data[data['left']==1]['average_montly_hours'], bins=bins, alpha=1, label='Employees Left')\nplt.hist(data[data['left']==0]['average_montly_hours'], bins=bins, alpha = 0.5, label = 'Employee Stayed')\nplt.title('Working Hours', fontsize=14)\nplt.xlabel('average_montly_hours')\nplt.xlim((70,365))\nplt.legend(loc='best');","fbf30d52":"groupby_number_projects = data.groupby('number_project').mean()\ngroupby_number_projects = groupby_number_projects['average_montly_hours']\nprint(groupby_number_projects)\nplt.figure(figsize=(7,5))\ngroupby_number_projects.plot();","9a2e9920":"\n\nwork_less_hours_left = data[(data.average_montly_hours < 200) & (data.number_project == 2) & (data.left == 1)]\nprint('work_less_hours_left:',len(work_less_hours_left))\n\nwork_more_hours_left = data[(data.average_montly_hours > 240) & (data.number_project >=5 ) & (data.left == 1)]\nprint('work_more_hours_left:',len(work_more_hours_left))","0e8b36ee":"plt.figure(figsize =(7,5))\nbins = np.linspace(0.006,1.000, 15)\nplt.hist(data[data['left']==1]['satisfaction_level'], bins=bins, alpha=1, label='Employees Left')\nplt.hist(data[data['left']==0]['satisfaction_level'], bins=bins, alpha = 0.5, label = 'Employee Stayed')\nplt.title('Employees Satisfaction', fontsize=14)\nplt.xlabel('satisfaction_level')\nplt.xlim((0,1.05))\nplt.legend(loc='best');","586ae170":"groupby_time_spend = data.groupby('time_spend_company').mean()\ngroupby_time_spend['satisfaction_level']\n\n","3ab7b6cc":"sns.set()\nsns.set_context(\"talk\")\nax = sns.factorplot(x=\"number_project\", y=\"satisfaction_level\", col=\"time_spend_company\",col_wrap=4, size=3, color='blue',sharex=False, data=data)\nax.set_xlabels('Number of Projects');","7891bb20":"func_living = data[(data.last_evaluation >= 0.70) | (data.time_spend_company >=4) | (data.number_project >= 5)]\n\ncorr2 = func_living.corr()\n\nsns.set(style='white')\n\nmask = np.zeros_like(corr2, dtype=np.bool)\n\nmask[np.triu_indices_from(mask)] = True\n\n# Insert the graphic\nf, ax = plt.subplots(figsize=(13,8))\n\ncmap = sns.diverging_palette(10,220, as_cmap=True)\n\n#Draw heat map mask\nax = sns.heatmap(corr2, mask=mask, cmap=cmap, vmax= .5, annot=True, annot_kws= {'size':11}, square=True, xticklabels=True, yticklabels=True, linewidths=.5, \n           cbar_kws={'shrink': .5}, ax=ax)\nax.set_title('Correlation: Why Valuable Employees Tend to Leave', fontsize=20);","cc240511":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection  import cross_val_score\nimport matplotlib.pyplot as plt","43194656":"index = data.index\ncolumns = data.columns\nvalues = data.values\nprint(columns)\n\ny=data['left']\n\nx=data[['satisfaction_level','last_evaluation','number_project','average_montly_hours','time_spend_company','Work_accident','promotion_last_5years','salary']]\n","225e8103":"from sklearn.datasets import load_breast_cancer\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_curve,auc\n\nprint('Shape of x:',x.shape,' Shape of y:', y.shape)\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25, random_state=0)\nprint('Shape of x:',x_train.shape,' Shape of y:', y_train.shape)","56f48662":"from sklearn.neighbors import KNeighborsClassifier\nclf=KNeighborsClassifier(n_neighbors=5)\nclf.fit(x_train,y_train)\ny_pred=clf.predict(x_test)\nprint(y_pred)\nresults = confusion_matrix(y_test, y_pred)\nprint ('Confusion Matrix :')\nprint(results)\nprint('report',classification_report(y_test,y_pred))\nprint('Accuracy',accuracy_score(y_test, y_pred))\n\nfpr,tpr,threshold=roc_curve(y_test,y_pred)\nplt.plot(fpr,tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC ')\nplt.show()","aa437b50":"from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(max_depth=2, random_state=0)\nclf.fit(x_train,y_train)\ny_pred=clf.predict(x_test)\nprint(y_pred)\nresults = confusion_matrix(y_test, y_pred)\nprint ('Confusion Matrix :')\nprint(results)\nprint('report',classification_report(y_test,y_pred))\nprint('Accuracy',accuracy_score(y_test, y_pred))\n\nfpr,tpr,threshold=roc_curve(y_test,y_pred)\nplt.plot(fpr,tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC ')\nplt.show()","31526515":"from sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier()\nclf.fit(x_train,y_train)\ny_pred=clf.predict(x_test)\nprint(y_pred)\nresults = confusion_matrix(y_test, y_pred)\nprint ('Confusion Matrix :')\nprint(results)\nprint('report',classification_report(y_test,y_pred))\nprint('Accuracy',accuracy_score(y_test, y_pred))\n\nfpr,tpr,threshold=roc_curve(y_test,y_pred)\nplt.plot(fpr,tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC ')\nplt.show()","db42a760":"from sklearn.ensemble import GradientBoostingClassifier\nclf = GradientBoostingClassifier()\nclf.fit(x_train,y_train)\ny_pred=clf.predict(x_test)\nprint(y_pred)\nresults = confusion_matrix(y_test, y_pred)\nprint ('Confusion Matrix :')\nprint(results)\nprint('report',classification_report(y_test,y_pred))\nprint('Accuracy',accuracy_score(y_test, y_pred))\n\nfpr,tpr,threshold=roc_curve(y_test,y_pred)\nplt.plot(fpr,tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC ')\nplt.show()","518f3e47":"#SVM CLASSIFIER\n\nfrom sklearn.svm import SVC\nclf = SVC()\nclf.fit(x_train,y_train )\nSVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\ndecision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\nmax_iter=-1, probability=False, random_state=None, shrinking=True,\ntol=0.001, verbose=False)\ny_pred=clf.predict(x_test)\nprint(y_pred)\nresults = confusion_matrix(y_test, y_pred)\nprint ('Confusion Matrix :')\nprint(results)\nprint('report',classification_report(y_test,y_pred))\nprint('Accuracy',accuracy_score(y_test, y_pred))\n\nfpr,tpr,threshold=roc_curve(y_test,y_pred)\nplt.plot(fpr,tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC ')\nplt.show()","c0246738":"from sklearn.ensemble import AdaBoostClassifier\nada = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2), n_estimators=1000, learning_rate=0.2)\nada.fit(x_train, y_train)\ny_pred=ada.predict(x_test)\nprint(y_pred)\nresults = confusion_matrix(y_test, y_pred)\nprint ('Confusion Matrix :')\nprint(results)\nprint('report',classification_report(y_test,y_pred))\nprint('Accuracy',accuracy_score(y_test, y_pred))\n\nfpr,tpr,threshold=roc_curve(y_test,y_pred)\nplt.plot(fpr,tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC ')\nplt.show()","63fa17f8":"<h3>Performance Analysis<\/h3>\nThere are 2 distincts groups of employees. A group with poor performance and other with high performance employees. It's natural that employees that don't work well leave the company, but the main problem is that the high performance employees is leaving too and it's necessary to understand why.","09d3717e":"\n<h1>1. The Human Resources Dataset  <\/h1>\n\n\nThe Human Resources Analytics is a simulated dataset from Kaggle and the focus is to understand why the best and most experienced employees is leaving the company. By the exploration of this dataset it is possible to extract good insights of a problems that the Human Resource department deals daily. In many industries retain their best employees its a question of long term strategy, and can impact the companies growth or put in financial risk, mainly if the employees leave to work at the competitor.\n","291bf60b":"<h3><b>From the employees that left with high performance, 4 or more years in the company and working on 5 or more project had:<\/b><\/h3>\n\n* Low satisfaction level,\n* Worked more hours\n* Haven\u00b4t been promoted in the last five years.","394ab575":"<b> Load the data <\/b> <br>\nTo load the dataset we use a Pandas method called read_csv that read CSV(comma-separated) files and covert into DataFrame.","b2a19957":"<b> In descriptive analysis is very useful to use graphics to represent the data. For that, is necessary to import the libraries:<br><\/b>\n\n<b>Matplotlib<\/b>: is a plotting lybrary, usefull to plot statistical graphics. See: www.matplotlib.org<br>\n\n<b>Seaborn<\/b>: is a library based on matplotlib that can draw attrative statistical graphics. See: seaborn.pydata.org\/index.html\n\n","45860cae":"<b> sample <\/b> is a easy way to get a few data quickly.","7c1e6fbb":"<h1> 4. Descripitve Analysis <\/h1>\nThe descripitve Analysis is used to simplify and summarize the mainly characteristics of the dataset. In other words, show what kind of information the dataset has. The Pandas method describe generates a descriptive statistics that summarize the central tendency, dispersion and shape of the dataset. ","5c082a74":"\n<h1> 3. Preprossesing the dataset <\/h1>\n\nBefore starting the process, its important to answer if it's clear what kind of problem we are dealing with, because in many cases isn't so simple to identify it. A good understanding of the problem will help to choose the right data mining and machine learning techniques to make the right predictions. Thus, the first step, is preprocessing the data to look for missing, incomplete or noise values, because, in real word, the raw datas can be collect from many sources like sensors, websites, public data and many others.\n\nTo start the step of preprossing the dataset is neccessary to import some useful Python libraries.\n\n    Numpy: Is a fundamental package to use linear algebra and random number capabilities. See: www.numpy.org\/\n\n    Pandas: Is a package to work with relacional data as tables. See: pandas.pydata.org\/\n\n","3b298caa":"\n<h1>2. Exploratory Data Analysis (EDA) <\/h1>\n\nExploratory data analysis employs a variety of techniques (mostly statistical graphics) before making inferences from data. It is essencial to examine all variables in the dataset to:","056ddb3f":"<b>So we can now see the problem with highly evaluated employees who leave. <br>\n1. They have lower satisfaction level<br>\n2. They have more number of projects\n3. They have higher monthly hours\n4. They have also spent more time in company\n5. They have lower salary\n6. They have not been promoted in the last 5 years <\/b>","89684a15":"Let's see the first 5 lines of the dataset. The <b> head <\/b> method list first N rows from the DataFrame and the method <b> tail <\/b>, returns the last N rows.","87744758":"<h1>Correlation Analysis <\/h1>\nThe correlation is a very useful statitiscal analysis that describes the degree of relationship between two variables. Let\u00b4s see the table below and the heat map to see what relationship are in the data.\n\nIn the heat map is possible to see:\n\n* Negative correlation of (-0.39) between satisfaction_level and the employees that left the company.<br>\n* The highest positive correlation is between number of projects and average monthly hours (0.42).<br>\n* Last_evaluation is high correlated to number_project(0.35)and average_monthly_hours(0.34).<br>\n* Work_accident have a low negative correlation(-0.15)and salary (-0.16) with employees that left.","17f595fd":"<h1> Feature engineering ","120544b5":"<h1> <b>  Human Resource Analytics Kaggle\n <\/b> <br> <\/h1> ~ <b>Abhay Puri\n","e798aa81":"<b><h3>When the employees becames unsatisfayed? <br> <\/b> <\/h3>\nIn next results it is clear the drop in satisfaction when employees are working on 6 or more projects.","0dfa8a1b":"Other useful method is <b> info <\/b> that shows a summary of the dataset, like number of observations, columns, variable type and the total memory usage. The dataset have 14999 observations, 10 columns and with no null values. The data types of the variables are divided in 2 float, 6 integer and 2 object."}}