{"cell_type":{"bcf58bea":"code","cc83b186":"code","8a742c09":"code","3f249f88":"code","3566710c":"code","ddd5ec7a":"code","7ca25df3":"code","667fdc89":"code","6077d850":"code","220ad9f9":"code","a28aade3":"code","a01bca05":"code","ece89341":"code","4aac5290":"code","7b977c42":"code","6f9d1984":"code","92927f67":"code","3fc46f2c":"code","b396bcf9":"code","281dc390":"code","d1dcf79d":"code","ac1d1187":"code","44e38b1b":"code","9fae5215":"code","c39cb821":"code","be1c0a59":"code","e10694a2":"code","ca2a2322":"code","412161ef":"code","393364c5":"code","699585b5":"code","b08b2771":"code","3ef8806e":"code","66737c81":"code","b01d9c0c":"code","d604106b":"code","07989877":"code","a18daf4a":"code","aeaef8ec":"code","ad412252":"code","f550dec0":"code","924d2bfe":"code","ed57eeab":"code","b17cd7b3":"code","a2b05995":"code","4f45704b":"code","68665d8e":"code","9ff02004":"code","f6f291da":"code","304b2362":"code","c5bfccab":"code","88bbe841":"code","391b17b4":"code","0a06ec5e":"code","8c7c37b0":"code","da288b29":"code","52f2b82c":"code","55d17269":"code","36d015d4":"code","78596f4b":"markdown","2ee917e7":"markdown","d2d1a6bd":"markdown","60c0a684":"markdown","8ffe0f79":"markdown","e08afadc":"markdown","aeb8bdbe":"markdown","7b3e86c3":"markdown","3f83bc90":"markdown","6254876d":"markdown","f17f6572":"markdown","b4bdd303":"markdown","5a1ce620":"markdown","00792f2f":"markdown","27c73d1a":"markdown","4fb3a5e8":"markdown","a84b51b2":"markdown","0fe45bb3":"markdown","38914b3c":"markdown","b85556dd":"markdown","40c23050":"markdown","5bceeba3":"markdown","5ecd7bae":"markdown","a1694159":"markdown","ddada3d9":"markdown","6a13c18e":"markdown","d9046365":"markdown","849fcb35":"markdown","ff30f863":"markdown","4df39726":"markdown","a3115732":"markdown","564d53b6":"markdown","65c72118":"markdown","2b3cf972":"markdown","d92096ab":"markdown","35d91434":"markdown","02620e35":"markdown"},"source":{"bcf58bea":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cc83b186":"# Basic\nimport pandas as pd\nimport numpy as np\nimport random\n\n# Plots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\n# Misc.\nimport warnings\nwarnings.filterwarnings('ignore')","8a742c09":"train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest  = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\nsample= pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")","3f249f88":"train.head()","3566710c":"print('Data shape: ', train.shape)\nprint('There are %d instances' %train.shape[0])\nprint('There are %d features' %train.shape[1])","ddd5ec7a":"# Create correlation matrix\ncorrmat = train.corr()\n# corrmat\n\nplt.figure(figsize=(10, 10))\nax = sns.heatmap(corrmat, square=True, vmax=1, vmin=-1)\nax.set_title('Correlation Heatmap of Housing Pricing Train data')\nplt.show()","7ca25df3":"train['OverallQual']","667fdc89":"# Set seaborn theme\nsns.set(style='darkgrid', palette='muted')","6077d850":"# Example of positive correlation\ntarget = 'SalePrice'\nvar1 = 'OverallQual'\nvar2 = 'GrLivArea'\n\n\nfig, (ax1, ax2)  = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\nsns.boxplot(x=var1, y=target, data=train, ax=ax1)\nax1.set_title('Correlation values %.3f' %corrmat.loc[target, var1])\nsns.scatterplot(x=var2, y=target, data=train, ax=ax2)\nax2.set_title('Correlation values %.3f' %corrmat.loc[target, var2])\nfig.tight_layout()\nplt.show()","220ad9f9":"train.shape","a28aade3":"total = train.isna().sum().sort_values(ascending=False)\npercent = total\/len(train)\n\nmissing = pd.concat([total, percent], axis=1)\nmissing.columns = ['total', 'percentage']\n\n# let's also see corresponding correlation values\ncorr_tmp = corrmat.SalePrice\ncorr_tmp.name = 'corrval'\nmissingcorr = missing.merge(corr_tmp, how='outer', left_index=True,right_index=True).sort_values(by='percentage', ascending=False)\nmissingcorr.head(20)","a01bca05":"# Dropping columns\ndel_cols = missingcorr[missingcorr['percentage'] > missingcorr.loc['Electrical', 'percentage']].index\ndel_cols\nprint('Initial data shape:', train.shape)\n\n# The train_nona refers to train data without NaN values\ntrain_nona = train.drop(columns=del_cols)\nprint('After dropping columns:', train_nona.shape)\n\ntrain_nona = train_nona.dropna(axis=0, how='any')\nprint('After dropping instance of `Electrical`: ', train_nona.shape)\n\nprint('Total missing values in data after cleaning: ', train_nona.isna().sum().sum())","ece89341":"highcorr = corrmat.SalePrice.sort_values(ascending=False)\n\n# Pick correlation values that are > 0.5\nhighcorr = highcorr[highcorr > 0.5]\nhighcorr","4aac5290":"# Correlation matrix with above features\nhighcorrmat = corrmat.loc[highcorr.index, highcorr.index]\n\nhighcorrmat","7b977c42":"# Correlation heatmap\nplt.figure(figsize=(8, 5))\nsns.heatmap(highcorrmat, annot=True, fmt='.2f')\nplt.show()","6f9d1984":"# Create new dataframe containing selected features \ndrop_cols = ['GarageArea', '1stFlrSF', 'TotRmsAbvGrd', 'YearBuilt', 'YearRemodAdd']\n\n# Select highest correlated features\nsel_train = train_nona[highcorrmat.index]\n\n# Remove feature with multicollinearity\nsel_train = sel_train.drop(columns=drop_cols)\n\nsel_train.shape","92927f67":"sns.pairplot(sel_train)\nplt.show()","3fc46f2c":"# Check index of those instance \nprint(sel_train['GrLivArea'].sort_values()[-2:].index)\nprint(sel_train['TotalBsmtSF'].sort_values()[-1:].index)","b396bcf9":"# We can just delete the instance from GrLivArea\nsel_train = sel_train.drop(index=sel_train['GrLivArea'].sort_values()[-2:].index)\n\n# Check to see if thsoe outliers have been removed\nfig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(12, 5))\nfig.suptitle('After Removing Outliers')\nsns.scatterplot(x='GrLivArea', y='SalePrice', data=sel_train, ax=ax1)\nsns.scatterplot(x='TotalBsmtSF', y='SalePrice', data=sel_train, ax=ax2)\nplt.show()","281dc390":"from scipy.stats import norm","d1dcf79d":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 4))\nsns.distplot(sel_train.SalePrice, kde=True, fit=norm, ax=ax1)\n_ = stats.probplot(sel_train.SalePrice, plot = ax2)\nfig.tight_layout()\nplt.show()","ac1d1187":"# Applying log transformation\nsel_train['SalePrice'] = np.log1p(sel_train.SalePrice)","44e38b1b":"# See effect of log transformation \nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 4))\nsns.distplot(sel_train.SalePrice, kde=True, fit=norm, ax=ax1)\n_ = stats.probplot(sel_train.SalePrice, plot = ax2)\nfig.tight_layout()\nplt.show()","9fae5215":"# Plot GrLiveArea\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 4))\nsns.distplot(sel_train.GrLivArea, kde=True, fit=norm, ax=ax1)\n_ = stats.probplot(sel_train.GrLivArea, plot = ax2)\nfig.tight_layout()\nplt.show()","c39cb821":"# Apply log transformation\nsel_train.GrLivArea = np.log(sel_train.GrLivArea)","be1c0a59":"# See effect of log transformation \nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 4))\nsns.distplot(sel_train.GrLivArea, kde=True, fit=norm, ax=ax1)\n_ = stats.probplot(sel_train.GrLivArea, plot = ax2)\nfig.tight_layout()\nplt.show()","e10694a2":"# Plot TotalBsmtSF\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 4))\nsns.distplot(sel_train.TotalBsmtSF, kde=True, fit=norm, ax=ax1)\n_ = stats.probplot(sel_train.TotalBsmtSF, plot = ax2)\nfig.tight_layout()\nplt.show()","ca2a2322":"# transform data\nsel_train['TotalBsmtSF'] = np.log1p(sel_train.TotalBsmtSF)","412161ef":"# See effect of log transformation\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 4))\nsns.distplot(sel_train.TotalBsmtSF[sel_train.TotalBsmtSF>0], kde=True, fit=norm, ax=ax1)\n_ = stats.probplot(sel_train.TotalBsmtSF[sel_train.TotalBsmtSF>0], plot = ax2)\nfig.tight_layout()\nplt.show()","393364c5":"# Split the target variable from the predictors\ny = sel_train.SalePrice\nX = sel_train.drop(columns='SalePrice')","699585b5":"# Models\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso, LassoCV, Ridge, RidgeCV\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV\nfrom sklearn.metrics import mean_squared_error","b08b2771":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)","3ef8806e":"# Function to compute RMSE (Root Mean Squared Error), using 5-fold CV\ndef rmse(model, X, y, cv):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv))\n    return rmse.mean()","66737c81":"lin = LinearRegression()","b01d9c0c":"rmse_sc = rmse(lin, X, y, 5)\nrmse_sc","d604106b":"# Create List to append dictionary of scores and model\nall_scores = []","07989877":"all_scores.append(dict(model='OLD', score=rmse_sc))","a18daf4a":"# let's try a default value alpha =1 \nridge = Ridge(alpha=1)","aeaef8ec":"rmse(ridge, X, y, cv=5)","ad412252":"# Selection for alphas\nalphas = np.logspace(5,-5,50)\nalphas","f550dec0":"# Ridge CV is a ridge regression with built-in CV implementation\nridgecv = RidgeCV(alphas=alphas, scoring='neg_mean_squared_error')\nridgecv.fit(X, y)\n\n# RidgeCV gives us the model trained with best alpha values\nridgecv.alpha_","924d2bfe":"mod = Ridge(alpha=ridgecv.alpha_)\nmod.fit(X, y)","ed57eeab":"rmse(mod, X, y, cv=5)","b17cd7b3":"# Computing rmse\nrmse_sc = rmse(ridgecv, X, y, cv=5)\nrmse_sc","a2b05995":"all_scores.append(dict(model='Ridge', score=rmse_sc))","4f45704b":"# Try lasso with 1 alpha values\nlasso = Lasso(alpha=1)\nrmse(lasso, X, y, cv=5)","68665d8e":"# Find new alphas\nlassocv = LassoCV(alphas=alphas)\nlassocv.fit(X, y)\nlassocv.alpha_","9ff02004":"# Compute rmse and add to list\nrmse_sc = rmse(lassocv, X, y, cv=5)\nrmse_sc","f6f291da":"all_scores.append(dict(model='Lasso', score=rmse_sc))","304b2362":"elasticcv = ElasticNetCV(alphas=alphas)\nelasticcv.fit(X, y)","c5bfccab":"# Compute rmse and add to list\nrmse_sc = rmse(elasticcv, X, y, cv=5)\nrmse_sc","88bbe841":"all_scores.append(dict(model='ElasticNet', score=rmse_sc))","391b17b4":"pd.DataFrame(all_scores).sort_values(by='score')","0a06ec5e":"# Desired columns\nwant_cols = X.columns\n\nsel_test = test[want_cols]","8c7c37b0":"# Imput missing values with an average, notice the column with missing values\nsel_test.isna().sum()","da288b29":"sel_test.GarageCars = sel_test.GarageCars.fillna(round(sel_test.GarageCars.mean()))\nsel_test.TotalBsmtSF = sel_test.TotalBsmtSF.fillna(sel_test.TotalBsmtSF.mean())","52f2b82c":"# After imputing missing values\nsel_test.isna().sum().sum()","55d17269":"# Predict\nypred = ridgecv.predict(sel_test)\n\n# Creating output csv\nresult = pd.DataFrame({sample.columns[0] : sample['Id'],\n                        sample.columns[1] : ypred\n})\n","36d015d4":"result.to_csv('.\/20210629-housing-ridge.csv', index=False)","78596f4b":"## Observe missing values","2ee917e7":"# Acknowledgement \n\nNotebooks from which I inspired the most:\n- https:\/\/www.kaggle.com\/houcembenmansour\/house-price-prediction\n- https:\/\/www.kaggle.com\/rbyron\/simple-linear-regression-models\n- https:\/\/www.kaggle.com\/apapiu\/regularized-linear-models","d2d1a6bd":"## Pick important features\nLet's check the correlation matrix once again and sort them descendingly","60c0a684":"## Results summary ","8ffe0f79":"### Ordinary Least Square (OLS), Regression\nFirst is let's use ordinary least square (OLS) model, this is basic linear regression without regularization.   \nThe definition of regularization will not be discussed extensively in this course.","e08afadc":"For now we only need to focus on the last row `SalePrice`:\n- positively correlated : `OverallQual`, `GrLivArea`\n- negatively correlated : `None`\nThere are no variales that has high negative correlation with `SalePrice` \nLet see correlation values of those two variable in positive correlated and plot them to see what patterns they have","aeb8bdbe":"## Pairplot & Outliers\nWe can see the plot between features altogether using pair plot, it helps us to see if patterns or outliers that may exist","7b3e86c3":"## Grab desired columns and impute missing values","3f83bc90":"### Elastic Net\nElastic net is a combination of both L1 and L2 regularization.  \nWe will skip using the default alpha values, and immediately jump to using ElasticNetCV","6254876d":"#### `TotalBsmtSF`","f17f6572":"This is examples of and definition of positive correlation, value of one variable increase as the other increase, the same logic applies to negative correlation.  \nWe will further observed which feature has high positive correlation and choose them as our features to train model","b4bdd303":"## Predict using Ridge","5a1ce620":"# Test","00792f2f":"### Lasso Regression\nThe same logic from before, applies to Lasso model","27c73d1a":"## Getting to know data","4fb3a5e8":"## Training","a84b51b2":"Notice that couple of values are zeros, these values can be transformed using log.   \nTo solve this, we can only apply log transformation for the non-zero values.","0fe45bb3":"# Objective:\nThe goal of this work is to build a model that can correctly predict `SalePrice`  \nThis notebook is targeted to give exposure to beginner (myself) to work with continuous target variable, we will only implement basic model such as:\n- Simple Linear Regression (OLS)\n- Ridge Regression (Regression with L2 Regularization)\n- Lasso Regression (Regression with L1 Regularization)\n- Elastic (Regression with combination of L1 and L2)\n\nWe will use RMSE as a metric","38914b3c":"We can see that rmse with new alpha gives better (lower) rmse than default values.  \nBut still the rmse with new alpha is similar to previous two.","b85556dd":"# Feature Engineering\nThis section will cover the following:\n- pick which features to build our model, based on highest correlation values\n- check normality of features, normalized them if needed","40c23050":"# EDA\n\nThis section will explore the data, goal:\n- get to know the dataset, how many features etc.\n- quick overview of feature correlation with dependent variable","5bceeba3":"If you ever heard of term `lambda` as regularization term, in this scikit module it's defined by the variable `alpha`, itgoverns how much we want to regularize the model, it's a hyperparameter, meaning we can set this value according to our will that gives the best metric we concern about, in this case `rmse`, let's create numbers for alpha","5ecd7bae":"Recall that `GarageCars` only contains integer values, so let's obey that. ","a1694159":"Oops, it gives the same values as before with OLS, don't worry it happens. Notice that also in this case, using default alpha value or new alpha value doesn't really affect rmse_sc.\n\nAlso one thing to note is that, ridgecv returns the ridge model trained with best alpha, ","ddada3d9":"We can see `SalePrice` is not normal, the distribution has positive skewness, and qq plot shows it doesn't follows diagonal line. Let's apply log transformation!","6a13c18e":"## Correlation heatmap\nGoal: see the correlation between each features, correlation range between -1 ant 1, the values near -1 or 1 shows stronge negative correlation or positive correlation respectively, while weak correlation indicated by values closer to zero. The goal is to see which features has high correlation (either positive or negative) with `SalePrice`","d9046365":"Same phenomenon as before, `GrLivArea` also experience positive skewness","849fcb35":"### `SalePrice`","ff30f863":"### Ridge Regression\nIf you have studied about L1 and L2 Regularization, it's good thing to know them by different name: \n- L1 is also called Lasso\n- L2 is also called Ridge\n\nSome source I recommend to read on:\n- https:\/\/towardsdatascience.com\/l1-and-l2-regularization-methods-ce25e7fc831c\n- https:\/\/stats.stackexchange.com\/questions\/866\/when-should-i-use-lasso-vs-ridge\n- https:\/\/stats.stackexchange.com\/questions\/200416\/is-regression-with-l1-regularization-the-same-as-lasso-and-with-l2-regularizati","4df39726":"# Housing Price for Beginner using Basic Ridge, Lasso, Elastic","a3115732":"What can we can we learn here?:\n- Notice that not all variables has correlation values, this indicates that corresponding features are categorical (non-numeric features)\n- The first four feature contains lots of missing data > 80%, we can delete those feature entirely and pretend they don't exist\n- Same thing goes for `LotFrontage` and `FireplaceQu`, also notice that it has low correlation, so dropping it entirely won'e be a problem\n- Interesting percentage values for features `GarageXXX`, notice they have save number, those missing values the belong to same instances, for now let's just drop them, plus the correlation values is not that high\n- The same logic applies to `BsmtXX` and `MasVnrXX`, although `MasVnrXX` has relatively low missing values, but for now let's just drop its column\n- `Electrical` only has one missing values, in this case we can delete the row","564d53b6":"Few observations regarding the plot against `SalePrice`:\n- There are two highest values in `GrLivArea` that doesn't follow the trends\n- There are one instances with highest values in `TotalBsmtSF` that doesn't follow the trends\n\nLet's delete those values\n\nThere some three instance in `GarageCars` when it equals to 4, that doesn't follow the trends, but we will ignore it now","65c72118":"#### `GrLivArea`","2b3cf972":"## Normality\nI consider this section to be quite advance, since we actually can proceed building model using data from before.\nBut since I've just learned about it, I thought I might as well put it here.\n\nThis section will check whether or not each features with numeric values follows normal distribution.  \nThis is done because data with normal distribution is favorable in machine learning settings.   \nHere we will apply log + 1 transformation to convert non-normal distribution to normal distribution.\n\nWe will also do this for columns with continuos values, i.e. `SalePrice`, `GrLivArea` and `TotalBsmtSF`","d92096ab":"Now we got some meaningful features that probably best to train our model, but do we need them all?  \nLet's take a look at correlation heatmap with these features one more time","35d91434":"Current results suggest Ridge Regression gives the lowest RSME score, let's use it to predict test data","02620e35":"Now, let me introduce **multicollinearity**, this term means that there are high correlation between two independent variables\/predictors\/features, this can be cause a problem when we train our model. The example from above would be `GarageCars` and `GarageArea`, those two variables are highly correlated, this means we can drop one variable and keep the other. This makes the train data less redundant and also reduce its dimension, such that making our model less complex.\n\nAnother example of multicollinearity from above heatmap:\n- `TotalBsmtSF` and `1stFlrSF`\n- `GrLivArea` and `TotRmsAbvGrd`\n\nIntuitively, we understand why they have high correlations, because essentially both variables indicates the same thing, for example, the number of cars you can fit in your garage (`GarageCars`) implicitly dictates the area of the garage itself (`GarageArea`). The same goes for other pairs\n\nSo how to decide which feature to drop among those pair? For now, let's pick feature with higher correlation to our target variable, we pick:\n- `GarageCars`\n- `TotalBsmtSF`\n- `GrLivArea`\n\nAnd we drop: `GarageArea`, `1stFlrSF`, `TotRmsAbvGrd`  \nFor now, let's make it simplere and remove those in lowest three as well: `YearBuilt`, `YearRemodAdd`\n\nLet's remove those features"}}