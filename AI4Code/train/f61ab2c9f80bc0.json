{"cell_type":{"6bfd0a2b":"code","831c9154":"code","55bc51ed":"code","9fcf55b4":"code","b337051d":"code","de832565":"code","5a1a7e18":"code","b5a2ce88":"code","ef2cde26":"code","5d2d62c0":"code","840a8a12":"code","94c78f4c":"code","8f8b4ec1":"code","222462e1":"code","255b97e7":"code","74abbc29":"code","06d905c2":"code","42bff484":"code","bf854113":"code","c1372b9b":"code","112ffa1c":"code","0637b646":"code","2aec42b2":"code","ef24c690":"code","a2c70d7a":"code","7f24e72e":"code","6b188889":"code","c2f680fe":"code","0554313c":"code","460c8060":"code","f4c93ed7":"code","06fdb4f5":"code","a8d12fa5":"code","c4c4f2b7":"code","94f24d7a":"code","d6918341":"code","8d8d0982":"code","0f9a406b":"code","2a863dfc":"code","3dca7979":"code","97ddf1f6":"code","3044e787":"code","69c5772e":"code","059b00c1":"code","0a3c49b9":"code","3403a115":"code","c3925503":"code","48206673":"code","792df3a9":"code","151eaf52":"code","16b4122e":"code","f0501021":"code","b1d5e617":"code","b08a0946":"code","4e35939a":"code","8c835297":"code","ea724d20":"code","7a52e538":"code","464b918e":"code","99e18212":"code","0a2b1471":"code","562e06aa":"code","49681ba6":"code","32e0af25":"code","28703dcb":"code","4b8a2759":"code","0453eb74":"code","ef2696e9":"code","2152c533":"code","fc77f7ec":"code","f5526c4a":"code","70009a65":"code","dda895de":"code","267f682e":"code","2c51e1db":"code","bfdea984":"markdown","15b7ff40":"markdown","31a89998":"markdown","4571871a":"markdown","2c8eb862":"markdown","f71370f2":"markdown","a3e4928f":"markdown","84931391":"markdown","8d3d9ba8":"markdown","3e160cbb":"markdown","942d4ea7":"markdown","93a9c5b5":"markdown","723188cf":"markdown","28da3068":"markdown","c19e4492":"markdown","3458d5b9":"markdown","bffeb149":"markdown","9f1dc9d8":"markdown","8ab4ed37":"markdown","3455e4f4":"markdown","dadf1d17":"markdown","10cc31e6":"markdown","d497b906":"markdown","6d38cdbb":"markdown","6444867c":"markdown","e6b3aa75":"markdown","eec15a43":"markdown","d361087e":"markdown","631195da":"markdown","b089e970":"markdown","d12c55d5":"markdown","973e4376":"markdown","be512cde":"markdown","620df4ee":"markdown","72bb1dc9":"markdown","b9112069":"markdown","a3b84f47":"markdown","51d8f110":"markdown","5f4251eb":"markdown","6dc819e2":"markdown","6fd39f0b":"markdown","ef79669f":"markdown","40feb17d":"markdown"},"source":{"6bfd0a2b":"!pip install kmodes","831c9154":"# Standards Librairies import\n%matplotlib inline\nimport os\nfrom time import time\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, iplot\nimport plotly.graph_objs as go\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import homogeneity_score, adjusted_rand_score, adjusted_mutual_info_score\nfrom sklearn.cluster import DBSCAN\nfrom sklearn import metrics\nfrom sklearn.preprocessing import normalize\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.cluster import MeanShift, estimate_bandwidth\nfrom yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer, InterclusterDistance\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.metrics import silhouette_score\nimport scipy.cluster.hierarchy as shc\n\n# Clustering Librairies import\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import MeanShift, estimate_bandwidth\nfrom yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer, InterclusterDistance\nfrom kmodes.kprototypes import KPrototypes\n\n# Personnal scripts\nimport olist_functions as fct\n\nmpl.rcParams[\"figure.figsize\"] = (12, 8)\ninit_notebook_mode()\nwarnings.filterwarnings('ignore')","55bc51ed":"# root path for cleaned dataset\nroot_path = '..\/input\/segmentation-des-clients-d-un-site-e-commerce-nb1'\ndata = pd.read_csv(root_path + \"\/olist-customers-segmentation.csv\",\n                   index_col=0)\ndata.head(6)","9fcf55b4":"data.shape","b337051d":"data.drop([\"customer_zip_code_prefix\",\n           \"customer_city\",\n           \"customer_state\",\n           \"Flag and name\",\n           \"mean_price_order\",\n           \"mean_nb_items\"], axis=1, inplace=True)","de832565":"categorical_features = list(data.select_dtypes(exclude=['int64', 'float64', 'uint8']).columns)\ncategorical_features","5a1a7e18":"numerical_features = list(data.select_dtypes(include=['int64','float64', 'uint8']).columns)\nnumerical_features","b5a2ce88":"scaler = MinMaxScaler()\n\npreprocessor = ColumnTransformer([\n    ('scaler', scaler, numerical_features)])","ef2cde26":"X = data.copy()\n# Elbow method\nkmeans_visualizer = Pipeline([\n    (\"preprocessor\", preprocessor),\n    (\"kelbowvisualizer\", KElbowVisualizer(KMeans(),K=(4,12)))])\nkmeans_visualizer.fit(X)\nkmeans_visualizer.named_steps['kelbowvisualizer'].show()","5d2d62c0":"# Best K in Elbow\nK = kmeans_visualizer.named_steps['kelbowvisualizer'].elbow_value_\n\n# Silhouette Visualizer\nsilhouette_visualizer = Pipeline([\n    (\"preprocessor\", preprocessor),\n    (\"silhouettevisualizer\", SilhouetteVisualizer(KMeans(K)))])\nsilhouette_visualizer.fit(X)\nsilhouette_visualizer.named_steps['silhouettevisualizer'].show()","840a8a12":"# Elbow method with differents metrics\nmetrics = [\"silhouette\", \"calinski_harabasz\"]\ni = 0\n\nfig, axes = plt.subplots(nrows=1, ncols=2, sharex=False, sharey=False, figsize=(24,8))\nfor m in metrics:\n    kmeans_visualizer = Pipeline([\n        (\"preprocessor\", preprocessor),\n        (\"kelbowvisualizer\", KElbowVisualizer(KMeans(),\n                                              K=(4,10),\n                                              metric=m,\n                                              ax=axes[i]))])\n    kmeans_visualizer.fit(X)\n    kmeans_visualizer.named_steps['kelbowvisualizer'].finalize()\n    i+=1\n\nplt.show()","94c78f4c":"# Intercluster distance Map with best k\ndistance_visualizer = Pipeline([\n    (\"preprocessor\", preprocessor),\n    (\"distancevisualizer\", InterclusterDistance(KMeans(K)))])\ndistance_visualizer.fit(X)\ndistance_visualizer.named_steps['distancevisualizer'].show()","8f8b4ec1":"# KMeans Pipeline with best K\nkmeans_model = Pipeline([(\"preprocessor\", preprocessor),\n                         (\"kmeans\", KMeans(K))])\nkmeans_model.fit(X)\n\n# Kmeans labels\nkmeans_labels = kmeans_model.named_steps['kmeans'].labels_\ndata[\"kmeans_label\"] = kmeans_labels","222462e1":"kmeans_clusters_means = data.groupby(\"kmeans_label\").mean().reset_index()\nkmeans_clusters_means","255b97e7":"X_scaled = preprocessor.fit_transform(X)\nX_scaled = pd.DataFrame(X_scaled, index=X.index, columns=X.columns)\nX_scaled[\"kmeans_label\"] = kmeans_labels\nX_scaled_clusters = X_scaled.groupby(\"kmeans_label\").mean()\nX_scaled_clusters","74abbc29":"def plot_radars(data, group):\n\n    scaler = MinMaxScaler()\n    data = pd.DataFrame(scaler.fit_transform(data), \n                        index=data.index,\n                        columns=data.columns).reset_index()\n    \n    fig = go.Figure()\n\n    for k in data[group]:\n        fig.add_trace(go.Scatterpolar(\n            r=data[data[group]==k].iloc[:,1:].values.reshape(-1),\n            theta=data.columns[1:],\n            fill='toself',\n            name='Cluster '+str(k)\n        ))\n\n    fig.update_layout(\n        polar=dict(\n        radialaxis=dict(\n          visible=True,\n          range=[0, 1]\n        )),\n        showlegend=True,\n        title={\n            'text': \"Comparaison des moyennes par variable des clusters\",\n            'y':0.95,\n            'x':0.5,\n            'xanchor': 'center',\n            'yanchor': 'top'},\n        title_font_color=\"blue\",\n        title_font_size=18)\n\n    fig.show()","06d905c2":"plot_radars(data=X_scaled_clusters,\n            group=\"kmeans_label\")","42bff484":"categories_col = ['books_cds_media',\n                  'fashion_clothing_accessories',\n                  'flowers_gifts',\n                  'groceries_food_drink',\n                  'health_beauty',\n                  'home_furniture',\n                  'other',\n                  'sport',\n                  'technology',\n                  'toys_baby']\nX_bis = X.drop(categories_col, axis=1)\nX_bis.head(6)","bf854113":"def clustering_eval(preprocessor, model, data, metric, elbow=True, mds=False, KBest=None):\n    \n    if((elbow==True) & (mds==True)):\n        ncols=3\n    elif((elbow==False) | (mds==False)):\n        ncols=2\n    else:\n        ncols=1\n        \n    fig, axes = plt.subplots(nrows=1, ncols=ncols, sharex=False, sharey=False, figsize=(24,8))\n    \n    ax=0\n    if(elbow==True):\n        # Elbow visualizer\n        kmeans_visualizer = Pipeline([\n            (\"preprocessor\", preprocessor),\n            (\"kelbowvisualizer\", KElbowVisualizer(model,K=(4,12), metric=metric, ax=axes[ax]))])\n        kmeans_visualizer.fit(data)\n        KBest = kmeans_visualizer.named_steps['kelbowvisualizer'].elbow_value_\n        kmeans_visualizer.named_steps['kelbowvisualizer'].finalize()\n        ax+=1\n    \n    # Set best K\n    K = KBest\n    model.set_params(n_clusters=K)\n\n    # Silhouette Visualizer\n    silhouette_visualizer = Pipeline([\n        (\"preprocessor\", preprocessor),\n        (\"silhouettevisualizer\", SilhouetteVisualizer(model, ax=axes[ax]))])\n    silhouette_visualizer.fit(data)\n    silhouette_visualizer.named_steps['silhouettevisualizer'].finalize()\n    ax+=1\n    \n    # Intercluster distance Map with best k\n    if(mds==True):\n        distance_visualizer = Pipeline([\n            (\"preprocessor\", preprocessor),\n            (\"distancevisualizer\", InterclusterDistance(model, ax=axes[ax]))])\n        distance_visualizer.fit(data)\n        distance_visualizer.named_steps['distancevisualizer'].finalize()\n    \n    return K\n    plt.show()","c1372b9b":"K = clustering_eval(preprocessor=MinMaxScaler(), \n                model=KMeans(), \n                data=X_bis, \n                metric=\"distortion\",\n                elbow=True,\n                mds=False,\n                KBest=None)","112ffa1c":"# KMeans Pipeline with best K\nkmeans_model_bis = Pipeline([(\"preprocessor\", MinMaxScaler()),\n                             (\"kmeans\", KMeans(K))])\nkmeans_model_bis.fit(X_bis)\n\n# Kmeans labels\nkmeans_labels_bis = kmeans_model_bis.named_steps['kmeans'].labels_\n\n# Scale X\nscaler = MinMaxScaler()\nX_scaled_bis = scaler.fit_transform(X_bis)\nX_scaled_bis = pd.DataFrame(X_scaled_bis, index=X_bis.index, columns=X_bis.columns)\nX_scaled_bis[\"kmeans_label\"] = kmeans_labels_bis\n\n# Group by cluster\nX_scaled_clusters_bis = X_scaled_bis.groupby(\"kmeans_label\").mean()\n\n# Plot Radar chart\nplot_radars(data=X_scaled_clusters_bis,\n            group=\"kmeans_label\")","0637b646":"# PCA Pipeline\npca = Pipeline([(\"preprocessor\", MinMaxScaler()),\n                (\"pca\", PCA(svd_solver='full'))])\npca.fit(X_bis)\nX_projected = pca.transform(X_bis)","2aec42b2":"# Explained variance\nvarexpl = pca.named_steps['pca'].explained_variance_ratio_*100\n\n# Plot of cumulated variance\nplt.figure(figsize=(12,8))\nplt.bar(np.arange(len(varexpl))+1, varexpl)\n\ncumSumVar = varexpl.cumsum()\nplt.plot(np.arange(len(varexpl))+1, cumSumVar,c=\"red\",marker='o')\nplt.axhline(y=95, linestyle=\"--\", \n            color=\"green\",\n            linewidth=1)\n\nlimit = 95\nvalid_idx = np.where(cumSumVar >= limit)[0]\nmin_plans = valid_idx[cumSumVar[valid_idx].argmin()]+1\nplt.axvline(x=min_plans, linestyle=\"--\", \n            color=\"green\",\n            linewidth=1)\n\nplt.xlabel(\"rang de l'axe d'inertie\")\nplt.xticks(np.arange(len(varexpl))+1)\nplt.ylabel(\"pourcentage d'inertie\")\nplt.title(\"{}% de la variance totale est expliqu\u00e9e\"\\\n          \" par les {} premiers axes\".format(limit,\n                                            min_plans))\nplt.show(block=False)","ef24c690":"def cerle_corr(pcs, n_comp, pca, axis_ranks, \n               labels=None, label_rotation=0):\n    \n    fig=plt.figure(figsize=(20,n_comp*5))\n    count=1\n    for d1, d2 in axis_ranks:\n        if d2 < n_comp:\n            \n            # initialisation de la figure\n            #fig.subplots_adjust(left=0.1,right=0.9,bottom=0.1,top=0.9)\n            ax=plt.subplot(int(n_comp\/2),2,count)\n            ax.set_aspect('equal', adjustable='box') \n            \n            #d\u00e9termination des limites du graphique\n            ax.set_xlim(-1,1) \n            ax.set_ylim(-1,1) \n\n            #affichage des fl\u00e8ches \n            ax.quiver(np.zeros(pcs.shape[1]), np.zeros(pcs.shape[1]),\n                       pcs[d1,:],pcs[d2,:], \n                       angles='xy', scale_units='xy', scale=1, \n                       color=\"grey\", alpha=0.5)\n            # et noms de variables\n            for i,(x,y) in enumerate(pcs[[d1,d2]].T):\n                ax.annotate(labels[i],(x,y),\n                             ha='center', va='center',\n                             fontsize='14',color=\"#17aafa\", alpha=0.8) \n\n            #ajouter les axes \n            ax.plot([-1,1],[0,0],linewidth=1, color='grey', ls='--') \n            ax.plot([0,0],[-1,1],linewidth=1, color='grey', ls='--')\n\n            #ajouter un cercle \n            cercle = plt.Circle((0,0),1,color='#17aafa',fill=False) \n            ax.add_artist(cercle) \n\n            # nom des axes, avec le pourcentage d'inertie expliqu\u00e9\n            ax.set_xlabel('F{} ({}%)'.format(d1+1, \n                                          round(100*pca.explained_variance_ratio_[d1],1)))\n            ax.set_ylabel('F{} ({}%)'.format(d2+1, \n                                          round(100*pca.explained_variance_ratio_[d2],1)))\n\n            ax.set_title(\"Cercle des corr\u00e9lations (F{} et F{})\".format(d1+1, d2+1))\n            count+=1\n            \n    plt.suptitle(\"Cercles des corr\u00e9lations sur les {} premiers axes\".format(n_comp),\n                 y=.9, color=\"blue\", fontsize=18)        \n    plt.show(block=False)","a2c70d7a":"# Principal component space\npcs = pca.named_steps['pca'].components_\n\n# Plot correlation circle\ncerle_corr(pcs,\n           6,\n           pca.named_steps['pca'],\n           [(0,1),(2,3),(4,5)],\n           labels = np.array(X_bis.columns))","7f24e72e":"# KMeans Pipeline with best K for PCA results\nkmeans_model_pca = Pipeline([(\"preprocessor\", MinMaxScaler()),\n                             (\"kmeans\", KMeans(K))])\nkmeans_model_pca.fit(X_projected[:,:4])\n\n# Kmeans labels\npca_kmeans_labels = kmeans_model_pca.named_steps['kmeans'].labels_\nX_scaled_bis[\"kmeans_label_pca\"] = pca_kmeans_labels\nX_scaled_clusters_pca = X_scaled_bis.groupby(\"kmeans_label_pca\").mean()\nX_scaled_clusters_pca.iloc[:,:-1]","6b188889":"plot_radars(data=X_scaled_clusters_pca.iloc[:,:-1],\n            group=\"kmeans_label_pca\")","c2f680fe":"clustering_eval(preprocessor=MinMaxScaler(), \n                model=KMeans(), \n                data=X_projected[:,:4], \n                metric=\"distortion\",\n                elbow=False,\n                mds=True,\n                KBest=K)","0554313c":"labels = pca_kmeans_labels\npca_data = X_projected[:,:4]","460c8060":"def bench_k_means(model, name, data, labels):\n    t0 = time()\n    estimator = make_pipeline(MinMaxScaler(), model).fit(data)\n    fit_time = time() - t0\n    results = [name, fit_time, estimator[1].inertia_]\n    \n    # Test differents metrics on pred labels\n    clustering_metrics = [\n        homogeneity_score,\n        adjusted_rand_score,\n        adjusted_mutual_info_score]\n    results += [m(labels, estimator[1].labels_) for m in clustering_metrics]\n    \n    # Show the results\n    formatter_result = (\"{:9s}\\t{:.3f}s\\t{:.0f}\\t{:.3f}\"\n                        \"\\t{:.3f}\\t{:.3f}\")\n    print(formatter_result.format(*results))","f4c93ed7":"print(\"Scores de stabilit\u00e9 \u00e0 l'initialisation\")\nprint(53 * '_')\nprint('Iteration\\tFitTime\\tInertia\\tHomo\\tARI\\tAMI')\nprint(53 * '_')\n\nfor i in range(10):\n    imodel = KMeans(n_clusters=K, n_init=1, init=\"k-means++\")\n    bench_k_means(model = imodel, name=\"Iter \"+str(i), \n                  data=pca_data, labels=labels)\n\nprint(53 * '_')","06fdb4f5":"X_ter = X.iloc[:,:-1]\nX_ter[\"favorite_sale_month\"] = pd.to_datetime(X_ter[\"favorite_sale_month\"], format='%m').dt.month_name()\nX_ter[\"mean_payment_sequential\"] = np.where(X_ter[\"mean_payment_sequential\"] > 1, \"Multiple\", \"Single\")\nX_ter[\"mean_payment_installments\"] = np.where(X_ter[\"mean_payment_installments\"] > 1, \"Multiple\", \"Single\")\nX_ter.head(6)","a8d12fa5":"for c in X_ter.select_dtypes(exclude=\"object\").columns:\n    scaler = MinMaxScaler()\n    X_ter[c] = scaler.fit_transform(np.array(X_ter[c]).reshape(-1, 1))","c4c4f2b7":"# Categorical features index\ncat_cols = [X_ter.columns.get_loc(c) for c in X_ter.select_dtypes(include=\"object\").columns]","94f24d7a":"#Choosing optimal K\ncost = []\nfor num_clusters in list(range(4,9)):\n    n_clusters=num_clusters\n    kproto = KPrototypes(n_clusters, init='Cao',n_init = 1,verbose=1)\n    kproto.fit_predict(X_ter, categorical=cat_cols)\n    cost.append(kproto.cost_)","d6918341":"plt.figure(figsize=(12,8))\nplt.plot(cost)\nplt.xticks(np.arange(0,6,1), np.arange(4,10,1))\nplt.axvline(x=2, linestyle=\"--\", \n            color=\"green\",\n            linewidth=1)\nplt.xlabel(\"K\")\nplt.ylabel(\"Cost\")\nplt.title(\"Cost of K-Prototypes for K between 4 and 9\",\n          fontsize=18, color=\"b\")\nplt.show()","8d8d0982":"kproto = KPrototypes(n_clusters= 6, init='Cao',n_init = 1,verbose=1)\nkproto_labels = kproto.fit_predict(X_ter, categorical=cat_cols)","0f9a406b":"X_ter[\"kproto_labels\"] = kproto_labels\nX_ter.groupby(\"kproto_labels\").mean()","2a863dfc":"# Initial period of 12 months\ndata_init = fct.make_dataset(dpath=\"..\/input\/brazilian-ecommerce\/\", initial=True, period=2)","3dca7979":"# Remove categories\ndata_init.drop(categories_col, axis=1, inplace=True)\ndata_init.head(6)","97ddf1f6":"olist_path = \"..\/input\/brazilian-ecommerce\/\"\norders = pd.read_csv(olist_path+\"olist_orders_dataset.csv\")\norders = orders.loc[orders.order_status == \"delivered\", \n                    \"order_purchase_timestamp\"]\norders = pd.to_datetime(orders)\ntime_delta = int((orders.max() - orders.min())\/np.timedelta64(1,'M'))\nprint(\"La p\u00e9riode compl\u00e8te des commandes porte sur {} mois.\".format(time_delta))","3044e787":"# Kmeans on initial period\nkmeans_init = Pipeline([(\"preprocessor\", MinMaxScaler()),\n                        (\"kmeans\", KMeans(K, random_state=42))])\nkmeans_init.fit(data_init)\ninit_labels = kmeans_init.named_steps['kmeans'].labels_","69c5772e":"ari_score = []\nfor p in np.arange(2,(time_delta-12),2):\n    # Create dataset for period\n    data_period = fct.make_dataset(dpath=\"..\/input\/brazilian-ecommerce\/\", \n                                   initial=False, \n                                   period=p)\n    data_period.drop(categories_col, axis=1, inplace=True)\n    \n    # Filter on initial customer client\n    data_period = data_period[data_period.index.isin(data_init.index)]\n    \n    # K-Means\n    kmeans_p = Pipeline([(\"preprocessor\", MinMaxScaler()),\n                         (\"kmeans\", KMeans(K, random_state=42))])\n    kmeans_p.fit(data_period)\n    p_labels = kmeans_p.named_steps['kmeans'].labels_\n    \n    # Calculate ARI score\n    ari_p = adjusted_rand_score(init_labels, kmeans_p[1].labels_)\n    ari_score.append([p,ari_p])","059b00c1":"ari_score = pd.DataFrame(ari_score, columns=[\"periode\", \"ARI\"])\n\n# plot ARI Score\nfig = plt.figure(figsize=(12,8))\nsns.lineplot(data=ari_score, x=\"periode\", y=\"ARI\")\nplt.axhline(y=ari_score.iloc[2][1], linestyle=\"--\", \n            color=\"green\",\n            xmax=0.5,\n            linewidth=1)\nplt.axvline(x=ari_score.iloc[2][0], linestyle=\"--\", \n            color=\"green\",\n            ymax=1.1-(ari_score.iloc[2][1]),\n            linewidth=1)\nplt.xlabel(\"P\u00e9riode (mois)\")\nplt.ylabel(\"Score ARI\")\nplt.title(\"Stabilit\u00e9 temporelle de la segmentation par K-Means\",\n          fontsize=18,\n          color=\"b\")\nplt.show()","0a3c49b9":"categories_col = ['books_cds_media',\n                  'fashion_clothing_accessories',\n                  'flowers_gifts',\n                  'groceries_food_drink',\n                  'health_beauty',\n                  'home_furniture',\n                  'other',\n                  'sport',\n                  'technology',\n                  'toys_baby',\n                 ]\ndata_bis = data.drop(categories_col ,axis = 1)","3403a115":"scaler = MinMaxScaler()\n\npreprocessor = ColumnTransformer([\n    ('scaler', scaler, data_bis)])","c3925503":"X = data_bis.copy()\nX.head(10)","48206673":"# Step 3: Preprocessing the data\n\n# Scaling the data to bring all the attributes to a comparable level\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n  \n# Normalizing the data so that \n# the data approximately follows a Gaussian distribution\nX_normalized = normalize(X_scaled)\n  \n# Converting the numpy array into a pandas DataFrame\nX_normalized = pd.DataFrame(X_normalized)","792df3a9":"# Explained variance\npca = PCA()\npca.fit(X_normalized)\n\nprint(pca.explained_variance_)\nprint(pca.explained_variance_ratio_)","151eaf52":"# Explained variance\npca = PCA()\npca.fit(X_normalized)\nvarexpl = pca.explained_variance_ratio_*100\n\n# Plot of cumulated variance\nplt.figure(figsize=(12,8))\nplt.bar(np.arange(len(varexpl))+1, varexpl)\n\ncumSumVar = varexpl.cumsum()\nplt.plot(np.arange(len(varexpl))+1, cumSumVar,c=\"red\",marker='o')\nplt.axhline(y=90, linestyle=\"--\", \n            color=\"green\",\n            linewidth=1)\n\nlimit = 90\nvalid_idx = np.where(cumSumVar >= limit)[0]\nmin_plans = valid_idx[cumSumVar[valid_idx].argmin()]+1\nplt.axvline(x=min_plans, linestyle=\"--\", \n            color=\"green\",\n            linewidth=1)\n\nplt.xlabel(\"rang de l'axe d'inertie\")\nplt.xticks(np.arange(len(varexpl))+1)\nplt.ylabel(\"pourcentage d'inertie\")\nplt.title(\"{}% de la variance totale est expliqu\u00e9e\"\\\n          \" par les {} premiers axes\".format(limit,\n                                            min_plans))\nplt.show(block=False)","16b4122e":"# 4 : R\u00e9duire la dimensionnalit\u00e9 des Donn\u00e9es\n\npca = PCA(n_components = 2)\nX_principal = pca.fit_transform(X_normalized)\nX_principal = pd.DataFrame(X_principal)\nX_principal.columns = ['P1', 'P2']","f0501021":"import numpy as np\nfrom sklearn.model_selection import train_test_split","b1d5e617":" X_train, X_test,  = train_test_split(\n                     X_principal,  test_size=0.8, random_state=0)","b08a0946":"# Dendograms are used to divide a given cluster into many different clusters.\n\n# Step 5: Visualizing the working of the Dendograms\n\nplt.figure(figsize =(12, 12))\nplt.title('Visualising the data')\nDendrogram = shc.dendrogram((shc.linkage(X_train, method ='ward')))","4e35939a":" y_train, y_test,  = train_test_split(\n                     X_principal,  test_size=0.2, random_state=0)","8c835297":"# Step 6: Building and Visualizing the different clustering models for different values of k\n# k = 2\n\nac2 = AgglomerativeClustering(n_clusters = 2)\n  \n# Visualizing the clustering\nplt.figure(figsize =(6, 6))\nplt.scatter(y_test['P1'], y_test['P2'],\n            c = ac2.fit_predict(y_test), cmap ='rainbow')\nplt.show()","ea724d20":"# k = 3\n\nac3 = AgglomerativeClustering(n_clusters = 3)\n  \nplt.figure(figsize =(6, 6))\nplt.scatter(y_test['P1'], y_test['P2'],\n            c = ac3.fit_predict(y_test), cmap ='rainbow')\nplt.show()","7a52e538":"# k = 4\n\nac4 = AgglomerativeClustering(n_clusters = 4)\n  \nplt.figure(figsize =(6, 6))\nplt.scatter(y_test['P1'], y_test['P2'],\n            c = ac4.fit_predict(y_test), cmap ='rainbow')\nplt.show()","464b918e":"# k = 5\n\nac5 = AgglomerativeClustering(n_clusters = 5)\n  \nplt.figure(figsize =(6, 6))\nplt.scatter(y_test['P1'], y_test['P2'],\n            c = ac5.fit_predict(y_test), cmap ='rainbow')\nplt.show()","99e18212":"# k = 6\n\nac6 = AgglomerativeClustering(n_clusters = 6)\n  \nplt.figure(figsize =(6, 6))\nplt.scatter(y_test['P1'], y_test['P2'],\n            c = ac6.fit_predict(y_test), cmap ='rainbow')\nplt.show()","0a2b1471":"# k = 7\n\nac7 = AgglomerativeClustering(n_clusters = 7)\n  \nplt.figure(figsize =(6, 6))\nplt.scatter(y_test['P1'], y_test['P2'],\n            c = ac7.fit_predict(y_test), cmap ='rainbow')\nplt.show()","562e06aa":"# Step 7: Evaluating the different models and Visualizing the results.\n\n# Appending the silhouette scores of the different models to the list\nsilhouette_scores = []\nsilhouette_scores.append(\n        silhouette_score(y_test, ac2.fit_predict(y_test)))\nsilhouette_scores.append(\n        silhouette_score(y_test, ac3.fit_predict(y_test)))\nsilhouette_scores.append(\n        silhouette_score(y_test, ac4.fit_predict(y_test)))\nsilhouette_scores.append(\n        silhouette_score(y_test, ac5.fit_predict(y_test)))\nsilhouette_scores.append(\n        silhouette_score(y_test, ac6.fit_predict(y_test)))\nsilhouette_scores.append(\n        silhouette_score(y_test, ac7.fit_predict(y_test)))","49681ba6":"# Plotting a bar graph to compare the results\nk = [2, 3, 4, 5, 6, 7]\nplt.bar(k, silhouette_scores)\nplt.xlabel('Number of clusters', fontsize = 20)\nplt.ylabel('S(i)', fontsize = 20)\nplt.show()","32e0af25":"Xbis_train,Xbis_test   = train_test_split(\n                     X_normalized,  test_size=0.8, random_state=0)","28703dcb":"from scipy.cluster.hierarchy import linkage,dendrogram\nZ = linkage(Xbis_train,method='ward',metric='euclidean')\n#affichage du dendrogramme\nplt.title(\"CAH\")\ndendrogram(Z,labels=Xbis_train.index,orientation='top',color_threshold=0)\nplt.show()\n","4b8a2759":"from scipy.cluster.hierarchy import ward, fcluster\n#mat\u00e9rialisation des 6 classes (hauteur t = 38\nplt.title('CAH avec mat\u00e9rialisation des 6 classes')\ndendrogram(Z,labels=Xbis_train.index,orientation='top',color_threshold=38)\nplt.show()\n#d\u00e9coupage \u00e0 la hauteur t = 38==> identifiants de 6 groupes obtenus\ngroupes_cah = fcluster(Z,t=38,criterion='distance')\nprint(groupes_cah)\n#index tri\u00e9s des groupes\nidg = np.argsort(groupes_cah)\n#affichage des observations et leurs groupes\nprint(pd.DataFrame(X_train.index[idg],groupes_cah[idg]))\n","0453eb74":"Xbis_train = pd.DataFrame(Xbis_train, index=Xbis_train.index, columns=Xbis_train.columns)\nXbis_train_clusters = Xbis_train.mean()\nXbis_train_clusters","ef2696e9":"# Numpy array of all the cluster labels assigned to each data point\ndb_default = DBSCAN(eps = 0.025, min_samples = 5).fit(X_principal)\nlabels = db_default.labels_","2152c533":"np.unique(labels)","fc77f7ec":"# Step 5: Building the clustering model\nfrom sklearn import metrics\nlabels_true = X.index\n# Numpy array of all the cluster labels assigned to each data point\ndb_default = DBSCAN(eps = 0.025, min_samples = 5).fit(X_principal)\ncore_samples_mask = np.zeros_like(db_default.labels_, dtype=bool)\ncore_samples_mask[db_default.core_sample_indices_] = True\nlabels = db_default.labels_\n\n# Number of clusters in labels, ignoring noise if present.\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\nn_noise_ = list(labels).count(-1)\n\nprint('Estimated number of clusters: %d' % n_clusters_)\nprint('Estimated number of noise points: %d' % n_noise_)\nprint(\"Homogeneity: %0.4f\" % metrics.homogeneity_score(labels_true, labels))\nprint(\"Completeness: %0.4f\" % metrics.completeness_score(labels_true, labels))\nprint(\"V-measure: %0.4f\" % metrics.v_measure_score(labels_true, labels))\nprint(\"Adjusted Rand Index: %0.4f\"\n      % metrics.adjusted_rand_score(labels_true, labels))\nprint(\"Adjusted Mutual Information: %0.4f\"\n      % metrics.adjusted_mutual_info_score(labels_true, labels))\nprint(\"Silhouette Coefficient: %0.4f\"\n      % metrics.silhouette_score(X, labels))","f5526c4a":"# Plot result\nimport matplotlib.pyplot as plt\n\n# Black removed and is used for noise instead.\nunique_labels = set(labels)\ncolors = [plt.cm.Spectral(each)\n          for each in np.linspace(0, 1, len(unique_labels))]\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        # Black used for noise.\n        col = [0, 0, 0, 1]\n\n    class_member_mask = (labels == k)\n\n    xy = X_principal[class_member_mask & core_samples_mask]\n    plt.plot(xy.iloc[:, 0], xy.iloc[:, 1], 'o', markerfacecolor=tuple(col),\n             markeredgecolor='k', markersize=14)\n\n    xy = X_principal[class_member_mask & ~core_samples_mask]\n    plt.plot(xy.iloc[:, 0], xy.iloc[:, 1], 'o', markerfacecolor=tuple(col),\n             markeredgecolor='k', markersize=6)\n\nplt.title('Estimated number of clusters: %d' % n_clusters_)\nplt.show()","70009a65":"# Numpy array of all the cluster labels assigned to each data point\ndb_default = DBSCAN(eps = 0.405, min_samples = 5).fit(X_normalized)\nlabels = db_default.labels_","dda895de":"np.unique(labels)","267f682e":"# Step 5: Building the clustering model\nfrom sklearn import metrics\nlabels_true = X.index\n# Numpy array of all the cluster labels assigned to each data point\ndb_default = DBSCAN(eps = 0.405, min_samples = 5).fit(X_normalized)\ncore_samples_mask = np.zeros_like(db_default.labels_, dtype=bool)\ncore_samples_mask[db_default.core_sample_indices_] = True\nlabels = db_default.labels_\n\n# Number of clusters in labels, ignoring noise if present.\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\nn_noise_ = list(labels).count(-1)\n\nprint('Estimated number of clusters: %d' % n_clusters_)\nprint('Estimated number of noise points: %d' % n_noise_)\nprint(\"Homogeneity: %0.4f\" % metrics.homogeneity_score(labels_true, labels))\nprint(\"Completeness: %0.4f\" % metrics.completeness_score(labels_true, labels))\nprint(\"V-measure: %0.4f\" % metrics.v_measure_score(labels_true, labels))\nprint(\"Adjusted Rand Index: %0.4f\"\n      % metrics.adjusted_rand_score(labels_true, labels))\nprint(\"Adjusted Mutual Information: %0.4f\"\n      % metrics.adjusted_mutual_info_score(labels_true, labels))\nprint(\"Silhouette Coefficient: %0.4f\"\n      % metrics.silhouette_score(X, labels))","2c51e1db":"# Plot result\nimport matplotlib.pyplot as plt\n\n# Black removed and is used for noise instead.\nunique_labels = set(labels)\ncolors = [plt.cm.Spectral(each)\n          for each in np.linspace(0, 1, len(unique_labels))]\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        # Black used for noise.\n        col = [0, 0, 0, 1]\n\n    class_member_mask = (labels == k)\n\n    xy = X_normalized[class_member_mask & core_samples_mask]\n    plt.plot(xy.iloc[:, 0], xy.iloc[:, 1], 'o', markerfacecolor=tuple(col),\n             markeredgecolor='k', markersize=14)\n\n    xy = X_normalized[class_member_mask & ~core_samples_mask]\n    plt.plot(xy.iloc[:, 0], xy.iloc[:, 1], 'o', markerfacecolor=tuple(col),\n             markeredgecolor='k', markersize=6)\n\nplt.title('Estimated number of clusters: %d' % n_clusters_)\nplt.show()","bfdea984":"# Sommaire\n\n1. Clustering avec l'algorithme du K-Means\n2. R\u00e9duction dimensionnelle - PCA\n3. K-Means apr\u00e8s r\u00e9duction de dimenssions\n4. Stabilit\u00e9 \u00e0 l'initialisation du K-Means\n5. Clustering avec l'algorithme du K-Prototypes\n6. Stabilit\u00e9 temporelle de la segmentation\n7. Clustering avec DBSCAN\n8. Herarchiqual clustering selon Ward","15b7ff40":"# Preprocessing\n\nNous allons supprimer customer_zip_code_prefix et customer_city, customer_state pour ne conserver que la variable haversine_distance pour indiquer une localisation \"large\" des clients, ce qui \u00e9vitera de focaliser la segmentation sur la localisation du client.\n\nNous allons \u00e9galement supprimer mean_price_order et mean_nb_items puisque nous avons vu dans le Notebook de nettoyage et exploration Olist que ces variables sont fortement corr\u00e9l\u00e9es aux totaux pour le moment.","31a89998":"# Stabilit\u00e9 temporelle de la segmentation\n\n\nDans le but d'\u00e9tablir un contrat de maintenance de l'algorithme de segmentation client, nous devons tester sa stabilit\u00e9 dans le temps et voir, par exemple, \u00e0 quel moment les clients changent de Cluster.\n\nPour cela, nous devons recalculer toutes les features en fonction d'une p\u00e9riode donn\u00e9e. Le script r\u00e9alisant ces calculs est disponible dans le module annexe olist_functions","4571871a":"# R\u00e9duction dimensionnelle - PCA\n\nNous allons r\u00e9aliser une Analyse en Composantes Principales pour v\u00e9rifier s'il est possible de cat\u00e9goriser nos clients avec moins de variables. L'objectif \u00e9tant de conserver une inertie de 95%","2c8eb862":"Nous allons ensuite d\u00e9terminer la p\u00e9riode totale \u00e0 couvrir ","f71370f2":"# Coefficient de silhouette\n\nPour v\u00e9rifier si ce clustering fonctionne, nous allons utiliser SilhouetteVisualizer pour afficher le coefficient de silhouette pour un \u00e9chantillonage de chaque cluster. Cela permet de visualiser la densit\u00e9 et la s\u00e9paration des clusters.","a3e4928f":"On remarque cette fois encore que les r\u00e9sultats obtenus sont similaires au K-Means, les cat\u00e9gories produits l'emportent sur les autres variables. Le K-Prototypes ayant un temps d'entrainement et de pr\u00e9diction plus long, nous conserverons donc la segmentation avec le mod\u00e8le K-Means.","84931391":"# Cercle des corr\u00e9lations","8d3d9ba8":"# Clustering sans les cat\u00e9gories produits","3e160cbb":"### We now determine the optimal number of clusters using a mathematical technique. Here, We will use the Silhouette Scores for the purpose.","942d4ea7":"M\u00e9thode du coude : d\u00e9termination du meilleur K\nUn pipeline SKLearn est cr\u00e9\u00e9 pour y inclure le preprocessing et la visualisation de la m\u00e9thode du coude de notre KMeans.","93a9c5b5":"# CAH sur echantillon (car dataset de plus de 93000 observations)\n\n### Classification ascendante hi\u00e9rarchique\n","723188cf":"# Interpr\u00e9tation m\u00e9tier des clusters\n\n* Groupe 1 : Clients proches g\u00e9ographiquement avec de courts d\u00e9lais de livraison, commandant principalement en d\u00e9but d'ann\u00e9e pour des montants faibles. Ils paient avec 1 type de moyen de paiement et avec un nombre faible d'\u00e9ch\u00e9ances. Les avis de ces clients sont tr\u00e8s bons.\n* Groupe 2 : Clients de fin d'ann\u00e9e. Ils r\u00e8glent avec plusieurs moyens de paiement pour des montants faibles. Ces clients sont g\u00e9ographiquement peu \u00e9loign\u00e9s et les d\u00e9lais de livraison sont courts. Les avis de ces clients sont \u00e9galement tr\u00e8s bons.\n* Groupe 3 : Regroupe les clients qui utilisent plusieurs moyens de paiement et un nombre important d'\u00e9ch\u00e9ances. Ils ont tendence \u00e0 espacer les d\u00e9lais entre 2 commandes. Les avis de ces clients sont \u00e9galement tr\u00e8s bons.\n* Groupe 4 : Ce sont des clients m\u00e9contants (les avis sont mauvais). Les d\u00e9lais de livraison sont tr\u00e8s importants et les frais de port \u00e9lev\u00e9s en raison de l'\u00e9loignement g\u00e9ographique. Ce sont cependant les clients qui ont le plus d\u00e9pens\u00e9 et ont achet\u00e9 un grand nombre d'articles. En revanche, le nombre de commandes pass\u00e9es sur le site est moyen.\n* Groupe 5 : Ces clients ont pass\u00e9 un nombre important de commandes et sont satisfaits. ils paient comptant pour un montant moyen de commande. Ils sont g\u00e9ographiquement proches.\n\nNous allons \u00e0 pr\u00e9sent r\u00e9aliser une r\u00e9duction dimensionnelle pour v\u00e9rifier si le clustering est r\u00e9alisable sur un nombre r\u00e9duit de variables sans perturber les groupes","28da3068":"# Conclusion","c19e4492":"## En conclusion on a partitionn\u00e9 notre data set clients en utilisant diffents algorithmes dont K-Means,K-Prototypes, Hierarchiqual agglomerative clustering selon Ward, et DBSCAN, les resultas malgre certainnes imperfections tournent autour de 5 a 6 clusters ,mais le plus probants quant au score (silhouette), fit-time,ainsi que son score \u00e0 l'initialisation  est le modele K-Means dont on garde les resultats.\n\n## Interpr\u00e9tation m\u00e9tier des clusters selon  K-Means\n\n### Groupe 1 : Clients proches g\u00e9ographiquement avec de courts d\u00e9lais de livraison, commandant principalement en d\u00e9but d'ann\u00e9e pour des montants faibles. Ils paient avec 1 type de moyen de paiement et avec un nombre faible d'\u00e9ch\u00e9ances. Les avis de ces clients sont tr\u00e8s bons.\n\n### Groupe 2 : Clients de fin d'ann\u00e9e. Ils r\u00e8glent avec plusieurs moyens de paiement pour des montants faibles. Ces clients sont g\u00e9ographiquement peu \u00e9loign\u00e9s et les d\u00e9lais de livraison sont courts. Les avis de ces clients sont \u00e9galement tr\u00e8s bons.\n\n### Groupe 3 : Regroupe les clients qui utilisent plusieurs moyens de paiement et un nombre important d'\u00e9ch\u00e9ances. Ils ont tendence \u00e0 espacer les d\u00e9lais entre 2 commandes. Les avis de ces clients sont \u00e9galement tr\u00e8s bons.\n\n### Groupe 4 : Ce sont des clients m\u00e9contants (les avis sont mauvais). Les d\u00e9lais de livraison sont tr\u00e8s importants et les frais de port \u00e9lev\u00e9s en raison de l'\u00e9loignement g\u00e9ographique. Ce sont cependant les clients qui ont le plus d\u00e9pens\u00e9 et ont achet\u00e9 un grand nombre d'articles. En revanche, le nombre de commandes pass\u00e9es sur le site est moyen.\n\n### Groupe 5 : Ces clients ont pass\u00e9 un nombre important de commandes et sont satisfaits. ils paient comptant pour un montant moyen de commande. Ils sont g\u00e9ographiquement proches.\n\n### Quant \u00e0 la maintenance Il faudra donc pr\u00e9voir celle du programme de segmentation tous les 6 mois dans un premier temps puis re-tester cette stabilit\u00e9 temporelle au fil du temps afin de l'affiner,  en veillant \u00e0  red\u00e9finir les segments clients \u00e0 chaque maintenance.","3458d5b9":"Les diff\u00e9rentes it\u00e9rations montrent des inerties proches, une bonne homog\u00e9n\u00e9it\u00e9 et un score AMI proche de 1. Nous pouvons donc en d\u00e9duire que la stabilit\u00e9 \u00e0 l'initialisation du mod\u00e8le K-Means est bonne","bffeb149":"\n# Explication de l'algorithme\n\nK-Prototypes Le K-Prototype est l'algorithme de clustering qui est la combinaison de K-Means et K-Mode. Dans l'algorithme du K-modes, la distance est mesur\u00e9e par le nombre d'attributs cat\u00e9goriels communs partag\u00e9s par les deux points de donn\u00e9es. K-Prototypes mesure la distance entre les variables num\u00e9riques \u00e0 l'aide de la distance euclidienne (comme le K-Means), mais mesure \u00e9galement la distance entre les entit\u00e9s cat\u00e9gorielles en utilisant le nombre de cat\u00e9gories correspondantes. Les principales \u00e9tapes sont les suivantes : 1. S\u00e9lection k prototypes initiaux dans l'ensemble de donn\u00e9es X. Il doit y en avoir un pour chaque cluster. 2. Allouer chaque objet de X \u00e0 un cluster dont le prototype est le plus proche de lui. Cette allocation se fait en consid\u00e9rant la mesure de dissimilarit\u00e9. 3. Une fois tout les objets allou\u00e9s \u00e0 un cluster, tester \u00e0 nouveau la similitude des objets par rapport aux prototypes actuels. Si un individu est plus proche d'un autre cluster, les prototypes des deux clusters sont mis \u00e0 jour.\n\nPour cet algorithme traitant les donn\u00e9es mixtes, nous allons convertir quelques variables en variables cat\u00e9gorielles (comme les mois d'achat par exemple) pour leur donner plus de sens.","9f1dc9d8":"Malheureusement ici, la segmentation se base principalement sur les cat\u00e9gories de produit achet\u00e9es. Le poids de ces features masque les autres axes de cat\u00e9gorisation, nous allons donc r\u00e9aliser un nouveau K-Means en supprimant ces variables (nous pourrons ensuite ajouter la valeur la plus fr\u00e9quente pour chaque groupe)","8ab4ed37":"Puis on initialise l'algorithme :","3455e4f4":"Nous allons dans un premier temps grouper les individus par cluster pour analyser les moyennes :","dadf1d17":"Gr\u00e2ce \u00e0 la m\u00e9thode du coude bas\u00e9e sur le score de distortion (somme moyenne des carr\u00e9s des distances aux centres), une segementation en  K=6 clusters serait la meilleure option","10cc31e6":"# Clustering avec l'algorithme du K-Means\n\n## Explication de l'algorithme\n\n K-Means C\u2019est l\u2019un des algorithmes de clustering les plus r\u00e9pandus. Il permet d\u2019analyser un jeu de donn\u00e9es caract\u00e9ris\u00e9es par un ensemble de descripteurs, afin de regrouper les donn\u00e9es \u201csimilaires\u201d en groupes *(ou clusters).\n\nApr\u00e8s avoir initialis\u00e9 des centro\u00efdes en prenant des donn\u00e9es au hasard dans le jeu de donn\u00e9es, K-means alterne plusieurs fois ces deux \u00e9tapes pour optimiser les centro\u00efdes et leurs groupes : \n\n1. Regrouper chaque objet autour du centro\u00efde le plus proche. \n2. Replacer chaque centro\u00efde selon la moyenne des descripteurs de son groupe.\n\nL\u2019algorithme a converg\u00e9 apr\u00e8s un certain nombre d'it\u00e9rations et trouve un d\u00e9coupage stable du jeu de donn\u00e9es. Cependant, le nombre de Clusters est \u00e0 d\u00e9finir par l'utilisateur. \n\n## M\u00e9thode du coude\n \nKElbowVisualizer de Yellowbrick impl\u00e9mente la m\u00e9thode du \u00abcoude\u00bb pour s\u00e9lectionner le nombre optimal de clusters en ajustant le mod\u00e8le K-Means avec une plage de valeurs pour K. Si le graphique en courbes ressemble \u00e0 un bras, alors le \u00abcoude\u00bb (le point d'inflexion sur la courbe) est une bonne indication du nombre de K optimal.","d497b906":"On remarque donc ici que la r\u00e9duction de dimenssion offre les m\u00eames axes de segmentation. Il est donc possible de r\u00e9duire le nombre de features en entr\u00e9e et d'utiliser les variables synth\u00e9tiques de la PCA pour segmenter notre fichier client, ce d'autant que les scores silhouette sont ici meilleurs compar\u00e9s au donn\u00e9es brutes.","6d38cdbb":"## Eboulis des valeurs propres","6444867c":"## DBSCAN clustering de la donn\u00e9e brute (sans PCA)","e6b3aa75":"On remarque ici que pour la m\u00e9trique silhouette score, le nombre de K est \u00e9galement de 6. Pour le score Calinski Harabasz, le meilleur K est plus incertain. Les scores sur la r\u00e9partition en 6 clusters semblent \u00eatre meilleurs. Nous allons donc conserver ce clustering en 6 groupes","eec15a43":"Sur ce plot des scores ARI obtenus sur les it\u00e9rations par p\u00e9riode de 2 mois, on remarque une forte inflexion apr\u00e8s 6 mois sur les clients initiaux.\n\nIl faudra donc pr\u00e9voir la maintenance du programme de segmentation tous les 6 mois dans un premier temps puis re-tester cette stabilit\u00e9 temporelle au fil du temps afin de l'affiner. Il sera donc n\u00e9cessaire de red\u00e9finir les segments clients \u00e0 chaque maintenance.","d361087e":"#                  Segmentation des clients du site E-Commerce Olist\n\nOlist souhaite obtenir une segmentation de ses clients utilisable au quotidien par leur \u00e9quipe marketing dans leurs campagnes de communication.\nL'objectif est de comprendre les diff\u00e9rents types d'utilisateurs gr\u00e2ce \u00e0 leur comportement et \u00e0 leurs donn\u00e9es personnelles anonymis\u00e9es.\n\nNous allons donc fournir ici une description actionable de la segmentation et de sa logique sous-jacente pour une utilisation optimale. Nous devrons \u00e9galement fournir une analyse de la stabilit\u00e9 des segments au cours du temps (dans le but d'\u00e9tablir un contrat de maintenance).\n\nNous utilisererons donc des m\u00e9thodes non supervis\u00e9es pour regrouper ensemble des clients de profils similaire","631195da":"### # Thus, with the help of the silhouette scores, it is concluded that the optimal number of clusters for the given data and clustering technique is 6","b089e970":"# Stabilit\u00e9 \u00e0 l'initialisation du K-Means\n\nNotre algorithme du K-Means test\u00e9 ici est initialis\u00e9 avec la m\u00e9thode K-Means++, ce qui r\u00e9duit d\u00e9j\u00e0 les effets al\u00e9atoires de l'initialisation des centro\u00efdes. Cependant, vous allons tester la stabilit\u00e9 \u00e0 l'initialisation en entrainant plusieurs fois le mod\u00e8le sans fixer le RandomState.\n\nPour les labels initiaux, nous allons conserver les clusters calcul\u00e9s dans le dernier mod\u00e8le (avec PCA) puis les comparer gr\u00e2ce \u00e0 l'indice de Rand ajust\u00e9 (ARI) pour chacune des it\u00e9rations.","d12c55d5":"Pour d\u00e9terminer le moment o\u00f9 les clients changent de cluster, nous allons it\u00e9rer le K-Means sur toute la p\u00e9riode avec des deltas de 2 mois et calculer le score ARI, en prenant garde \u00e0 bien comparer les m\u00eames clients (ceux des 12 mois initiaux).\n\nIci le d\u00e9roul\u00e9 est cr\u00e9\u00e9 directement dans le notebook pour explication, en production, toutes ses op\u00e9rations seront regroup\u00e9es dans une classe Python pour automatiser le processus.","973e4376":"On peut ainsi voir parfaitement les variables qui contribuent le plus \u00e0 chaque axe. Par exemple, \n\n* la variable synth\u00e9tique F2 repr\u00e9sentera les p\u00e9riodes d'achats.\n\n* La variable synth\u00e9tique F4 quant \u00e0 elle repr\u00e9sente l'aspect g\u00e9ographique.\n\nNous allons donc int\u00e9grer \u00e0 notre pipeline Kmeans une PCA sur 5 composantes pour v\u00e9rifier si la r\u00e9duction dimensionnelle r\u00e9duit la qualit\u00e9 de la segmentation ","be512cde":"Le score de chaque \u00e9chantillon est calcul\u00e9 en faisant la moyenne du coefficient de silhouette (diff\u00e9rence entre la distance moyenne intra-cluster et la distance moyenne du cluster le plus proche pour chaque \u00e9chantillon), normalis\u00e9e par la valeur maximale. Cela nous donne un score entre -1 et 1, qui nous permet de d\u00e9terminer si la s\u00e9paration est efficace ou si les points sont assign\u00e9s au mauvais cluster.\n\nIci, les clusters semblent relativement bien r\u00e9partis et les s\u00e9parations sont claires avec cependant quelques erreurs sur l'un des clusters.\n\nA pr\u00e9sent, nous allons tester d'autres types de m\u00e9triques pour trouver le meilleur K :\n\n* Silouhette : rapport moyen entre la distance intra-cluster et la distance du cluster le plus proche,\n* Calinski Harabasz : rapport entre la dispersion des grappes dans et entre les groupes.","620df4ee":"On prepare ensuite le preprocessor :","72bb1dc9":"Il faut donc conserver 5 axes principaux pour expliquer la variance \u00e0 95%","b9112069":"Sur cette projection en 2D, on remarque que les diff\u00e9rents clusters sont bien s\u00e9par\u00e9s sur les 2 premi\u00e8res composantes principales. Le clustering semble donc performant et il faut a pr\u00e9sent identifier les composantes m\u00e9tier de chaque cluster.\n\nAnalyse des diff\u00e9rents clusters\n\nA pr\u00e9sent, nous allons entrainer notre KMeans avec le K optimal s\u00e9lectionn\u00e9 et affecter son cluster \u00e0 chaque client. Ainsi, nous pourrons analyser les diff\u00e9rences entre chaque cluster :","a3b84f47":"## Classification ascendante hi\u00e9rarchique\n   ### Utilisation du package \u00ab scipy \u00bb\n","51d8f110":"Pour l'algorithme K-Prototypes, nous n'avons pas besoin d'encoder les variables cat\u00e9gorielles. Nous allons donc normaliser uniquement les varaibles num\u00e9riques ","5f4251eb":"Projetons \u00e0 pr\u00e9sent ces diff\u00e9rentes moyennes sur un Radar plot avec la librairie Ploty pour visualiser les diff\u00e9rences entre cluster :","6dc819e2":"# Clustering avec l'algorithme du K-Prototypes","6fd39f0b":"# CLUSTERING PAR DBSCAN","ef79669f":"# K-Means apr\u00e8s r\u00e9duction de dimenssions","40feb17d":"# Distances intercluster"}}