{"cell_type":{"b37d8c5c":"code","f8d87fb5":"code","d5bd4a7f":"code","c92acc2d":"code","4cbb50e8":"code","e578e2dd":"code","458a00fa":"code","2571dca9":"code","4653c429":"code","5b50b2c7":"code","169041e9":"code","e1d13eee":"code","a85009b9":"code","49e22ca8":"code","731bbbc6":"code","aae8fa07":"code","69e585a1":"code","4ac645d3":"code","71edcdcc":"code","90212573":"code","42bb2511":"code","598d9da5":"code","71c2b5d8":"code","33c6c2ee":"code","16cefd50":"code","48c8bc7f":"code","c8c90d1e":"markdown","81a7622e":"markdown","41cc4bdf":"markdown","2626daad":"markdown","bce482b3":"markdown","3b80e774":"markdown","18c73062":"markdown","922fcf21":"markdown","dca628b0":"markdown","5f7fe4e9":"markdown","b365309c":"markdown","700b8119":"markdown","0a0e5cf0":"markdown","8d5d5128":"markdown","8ecf6f2c":"markdown","7f8ce869":"markdown","a5676633":"markdown","e5b8f748":"markdown"},"source":{"b37d8c5c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f8d87fb5":"import matplotlib.pyplot as plt\nimport os\nimport re\nimport string\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom transformers import BertTokenizer, TFBertModel, BertConfig, BertTokenizerFast \nprint(tf.__version__)","d5bd4a7f":"data = pd.read_csv(\"..\/input\/tweet-sentiment-extraction\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tweet-sentiment-extraction\/test.csv\")","c92acc2d":"data = data[data.sentiment != \"neutral\"]\ndata = data[pd.notnull(data.selected_text)]","4cbb50e8":"data.head()","e578e2dd":"max_len = 384\n\ntokenizer = BertTokenizerFast.from_pretrained(\"..\/input\/spanbert-pt\/spanbert-base-cased\/\")","458a00fa":"class TrainTweetExample:\n    def __init__(self, sentiment, text, start_char_idx, selected_text, all_answers):\n        self.sentiment = sentiment\n        self.text = text\n        self.start_char_idx = start_char_idx\n        self.selected_text = selected_text\n        self.all_answers = all_answers\n        self.skip = False\n\n    def preprocess(self):\n        text = self.text\n        sentiment = self.sentiment\n        selected_text = self.selected_text\n        start_char_idx = self.start_char_idx\n\n        # Clean text, sentiment, and answer\n        sentiment = \" \".join(str(sentiment).split())\n        text = \" \".join(str(text).split())\n        answer = \" \".join(str(selected_text).split())\n\n        # Find end character index of answer in text\n        end_char_idx = start_char_idx + len(answer)\n        if end_char_idx >= len(text):\n            self.skip = True\n            return\n\n        # Mark the character indexes in text that are in answer\n        is_char_in_ans = [0] * len(text)\n        for idx in range(start_char_idx, end_char_idx):\n            is_char_in_ans[idx] = 1\n\n        # Tokenize text\n        tokenized_text = tokenizer.encode_plus(text, return_offsets_mapping=True, max_length = max_len)\n\n        # Find tokens that were created from answer characters\n        ans_token_idx = []\n        for idx, (start, end) in enumerate(tokenized_text.offset_mapping):\n            if sum(is_char_in_ans[start:end]) > 0:\n                ans_token_idx.append(idx)\n\n        if len(ans_token_idx) == 0:\n            self.skip = True\n            return\n\n        # Find start and end token index for tokens from answer\n        start_token_idx = ans_token_idx[0]\n        end_token_idx = ans_token_idx[-1]\n\n        # Tokenize sentiment\n        tokenized_sentiment = tokenizer.encode_plus(sentiment, return_offsets_mapping=True, max_length = max_len)\n\n        # Create inputs\n        input_ids = tokenized_text.input_ids + tokenized_sentiment.input_ids[1:]\n        token_type_ids = [0] * len(tokenized_text.input_ids) + [1] * len(\n            tokenized_sentiment.input_ids[1:]\n        )\n        attention_mask = [1] * len(input_ids)\n\n        # Pad and create attention masks.\n        # Skip if truncation is needed\n        padding_length = max_len - len(input_ids)\n        if padding_length > 0:  # pad\n            input_ids = input_ids + ([0] * padding_length)\n            attention_mask = attention_mask + ([0] * padding_length)\n            token_type_ids = token_type_ids + ([0] * padding_length)\n        elif padding_length < 0:  # skip\n            self.skip = True\n            return\n\n        self.input_ids = input_ids\n        self.token_type_ids = token_type_ids\n        self.attention_mask = attention_mask\n        self.start_token_idx = start_token_idx\n        self.end_token_idx = end_token_idx\n        self.context_token_to_char = tokenized_text.offset_mapping","2571dca9":"def create_tweet_examples(data):\n    tweet_examples = []\n    all_answers = data.selected_text.values\n    count = 0\n    for index, row in data.iterrows():\n        sentiment = row['sentiment']\n        text = row['text']\n        selected_text = row['selected_text']\n\n        try: ## turn the data into TrainTweetExample objects\n            start_char_idx = row.text.index(row.selected_text.split()[0])\n            tweet_eg = TrainTweetExample(\n                sentiment, text, start_char_idx, selected_text, all_answers\n            )\n            tweet_eg.preprocess()\n            tweet_examples.append(tweet_eg)\n        except: ## keep track of the number that can't be tokenized\/processed\n            count += 1 \n    print(\"Couldn't process\",count,\"points\")\n    return tweet_examples","4653c429":"class TestTweetExample:\n    def __init__(self, sentiment, text):\n        self.sentiment = sentiment\n        self.text = text\n        self.skip = False\n\n    def preprocess(self):\n        text = self.text\n        sentiment = self.sentiment\n\n        # Clean text, answer and sentiment\n        sentiment = \" \".join(str(sentiment).split())\n        text = \" \".join(str(text).split())\n\n        # Tokenize text\n        tokenized_text = tokenizer.encode_plus(text, return_offsets_mapping=True, max_length = max_len)\n\n        # Tokenize sentiment\n        tokenized_sentiment = tokenizer.encode_plus(sentiment, return_offsets_mapping=True, max_length = max_len)\n\n        # Create inputs\n        input_ids = tokenized_text.input_ids + tokenized_sentiment.input_ids[1:]\n        token_type_ids = [0] * len(tokenized_text.input_ids) + [1] * len(\n            tokenized_sentiment.input_ids[1:]\n        )\n        attention_mask = [1] * len(input_ids)\n\n        # Pad and create attention masks.\n        # Skip if truncation is needed\n        padding_length = max_len - len(input_ids)\n        if padding_length > 0:  # pad\n            input_ids = input_ids + ([0] * padding_length)\n            attention_mask = attention_mask + ([0] * padding_length)\n            token_type_ids = token_type_ids + ([0] * padding_length)\n        elif padding_length < 0:  # skip\n            self.skip = True\n            return\n\n        self.input_ids = input_ids\n        self.token_type_ids = token_type_ids\n        self.attention_mask = attention_mask\n        self.text_token_to_char = tokenized_text.offset_mapping\n        self.start_token_idx = 0\n        self.end_token_idx = 0","5b50b2c7":"def create_tweet_examples_test(data):\n    tweet_examples = []\n    all_answers = None\n    count = 0\n    for index, row in data.iterrows():\n        sentiment = row['sentiment']\n        text = row['text']\n        selected_text = None\n\n        try: ## turn the data into TestTweetExample objects\n            start_char_idx = 0\n            tweet_eg = TestTweetExample(\n                sentiment, text\n            )\n            tweet_eg.preprocess()\n            tweet_examples.append(tweet_eg)\n        except: ## keep track of the number that can't be tokenized\/processed\n            count += 1\n    print(\"Couldn't process\",count,\"points\")\n    return tweet_examples","169041e9":"def create_inputs_targets(tweet_examples):\n    dataset_dict = {\n        \"input_ids\": [],\n        \"token_type_ids\": [],\n        \"attention_mask\": [],\n        \"start_token_idx\": [],\n        \"end_token_idx\": [],\n    }\n    for item in tweet_examples:\n        if item.skip == False:\n            for key in dataset_dict:\n                dataset_dict[key].append(getattr(item, key))\n    for key in dataset_dict:\n        dataset_dict[key] = np.array(dataset_dict[key])\n\n    x = [\n        dataset_dict[\"input_ids\"],\n        dataset_dict[\"token_type_ids\"],\n        dataset_dict[\"attention_mask\"],\n    ]\n    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]]\n    return x, y","e1d13eee":"from sklearn.model_selection import train_test_split\n\ntrain, validation = train_test_split(data, test_size = 0.1)","a85009b9":"train_tweet_examples = create_tweet_examples(train)\nx_train, y_train = create_inputs_targets(train_tweet_examples)\nprint(f\"{len(train_tweet_examples)} training points created.\")\n\neval_tweet_examples = create_tweet_examples(validation)\nx_eval, y_eval = create_inputs_targets(eval_tweet_examples)\nprint(f\"{len(eval_tweet_examples)} evaluation points created.\")","49e22ca8":"test_tweet_examples = create_tweet_examples_test(test)\nx_test, _ = create_inputs_targets(test_tweet_examples)\nprint(f\"{len(test_tweet_examples)} test points created.\")","731bbbc6":"def create_model():\n\n    PATH = '..\/input\/spanbert-pt\/spanbert-base-cased\/'\n    encoder = TFBertModel.from_pretrained(PATH, from_pt = True)\n    \n## QA Model\n    input_ids = layers.Input(shape=(max_len ,), dtype=tf.int32)\n    token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)\n    embedding = encoder(\n         input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask\n    )[0]\n    \n\n    start_logits = layers.Dropout(0.3)(embedding)\n    start_logits = layers.Conv1D(128,2,padding='same')(start_logits)\n    start_logits = layers.LeakyReLU()(start_logits)\n    start_logits = layers.Conv1D(64,2,padding='same')(start_logits)\n    start_logits = layers.Dense(1, name=\"start_logit\")(start_logits)\n    start_logits = layers.Flatten()(start_logits)\n\n    end_logits = layers.Dropout(0.3)(embedding)\n    end_logits = layers.Conv1D(128,2,padding='same')(end_logits)\n    end_logits = layers.LeakyReLU()(end_logits)\n    end_logits = layers.Conv1D(64,2,padding='same')(end_logits)\n    end_logits = layers.Dense(1, name=\"end_logit\")(end_logits)\n    end_logits = layers.Flatten()(end_logits)\n\n    start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n    end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n\n    model = keras.Model(\n        inputs=[input_ids, token_type_ids, attention_mask],\n        outputs=[start_probs, end_probs],\n    )\n    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    optimizer = keras.optimizers.Adam(lr=5e-5)\n    model.compile(optimizer=optimizer, loss=[loss, loss])\n    return model","aae8fa07":"use_tpu = False\nif use_tpu:\n    # Create distribution strategy\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n    # Create model\n    with strategy.scope():\n        model = create_model()\nelse:\n    model = create_model()\n\nmodel.summary()","69e585a1":"def normalize_text(text):\n    text = text.lower()\n\n    # Remove punctuations\n    exclude = set(string.punctuation)\n    text = \"\".join(ch for ch in text if ch not in exclude)\n\n    # Remove articles\n    regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n    text = re.sub(regex, \" \", text)\n\n    # Remove extra white space\n    text = \" \".join(text.split())\n    return text\n\n\nclass ExactMatch(keras.callbacks.Callback):\n    \"\"\"\n    Each `TweetExample` object contains the character level offsets for each token\n    in its input text. We use them to get back the span of text corresponding\n    to the tokens between our predicted start and end tokens.\n    All the ground-truth answers are also present in each `TweetExample` object.\n    We calculate the percentage of data points where the span of text obtained\n    from model predictions matches one of the ground-truth answers.\n    \"\"\"\n\n    def __init__(self, x_eval, y_eval):\n        self.x_eval = x_eval\n        self.y_eval = y_eval\n\n    def on_epoch_end(self, epoch, logs=None):\n        pred_start, pred_end = self.model.predict(self.x_eval)\n        count = 0\n        eval_examples_no_skip = [_ for _ in eval_tweet_examples if _.skip == False]\n        for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n            tweet_eg = eval_examples_no_skip[idx]\n            offsets = tweet_eg.context_token_to_char\n            start = np.argmax(start)\n            end = np.argmax(end)\n            if start >= len(offsets):\n                continue\n            pred_char_start = offsets[start][0]\n            if end < len(offsets):\n                pred_char_end = offsets[end][1]\n                pred_ans = tweet_eg.text[pred_char_start:pred_char_end]\n            else:\n                pred_ans = tweet_eg.text[pred_char_start:]\n\n            normalized_pred_ans = normalize_text(pred_ans)\n            normalized_true_ans = [normalize_text(_) for _ in tweet_eg.all_answers]\n            if normalized_pred_ans in normalized_true_ans:\n                count += 1\n        acc = count \/ len(self.y_eval[0])\n        print(f\"\\nepoch={epoch+1}, exact match score={acc:.2f}\")","4ac645d3":"batch_size = 8\ndiv = len(x_train[0]) - (len(x_train[0]) % batch_size)\nx = list(np.array(x_train)[:,:div]) ## inputs must be divisible by batch size \ny = list(np.array(y_train)[:,:div])","71edcdcc":"exact_match_callback = ExactMatch(x_eval, y_eval)\nmodel.fit(\n    list(np.array(x)),\n    list(np.array(y)),\n    epochs=1,\n    verbose=1,\n    batch_size=batch_size,\n    callbacks=[exact_match_callback],\n)","90212573":"pred = model.predict(x_test)\nnp.array(pred).shape","42bb2511":"test['prediction'] = np.zeros(len(test.text.values))","598d9da5":"pred_start, pred_end = pred\ncount = 0\n\npred_text = []\nfor idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n    tweet_eg = test_tweet_examples[idx]\n    if (tweet_eg.skip == True):\n        pred_text.append(\"\")\n    else: \n        offsets = tweet_eg.text_token_to_char\n        start = np.argmax(start)\n        end = np.argmax(end)\n        if start >= len(offsets):\n            continue\n        pred_char_start = offsets[start][0]\n        if end < len(offsets):\n            pred_char_end = offsets[end][1]\n            pred_ans = tweet_eg.text[pred_char_start:pred_char_end + 1]\n        else:\n            pred_ans = tweet_eg.text[pred_char_start:]\n\n        normalized_pred_ans = normalize_text(pred_ans)\n        #print(normalized_pred_ans)\n        pred_text.append(normalized_pred_ans)\n        test['prediction'][idx] = normalized_pred_ans","71c2b5d8":"test.prediction[test.prediction == 0] = test.text[test.prediction == 0]\ntest.prediction[test.sentiment == \"neutral\"] = test.text[test.sentiment==\"neutral\"]\ntest.prediction[test.prediction ==''] = test.text[test.prediction =='']\n\ndef clean_text(text):\n    words = str(text).split()\n    words = [x for x in words if not x.startswith(\"http\")]\n    words = \" \".join(words)\n    return words\n\ntest['prediction'] = test['prediction'].apply(clean_text)","33c6c2ee":"test[:50]","16cefd50":"evaluation = test.textID.copy().to_frame()\nevaluation['selected_text'] = test['prediction']\nevaluation","48c8bc7f":"evaluation.to_csv(\"submission.csv\", index=False)","c8c90d1e":"Now, we train the model. I'm only using one epoch, but feel free to update and explore different training setups.","81a7622e":"Now, we can split the data into training and validation sets, and apply the preprocessing to the training and testing sets. We want to keep track of the number of data points successfully processed, and if there are any that caused errors.","41cc4bdf":"The competition was restricted to CPU\/GPU, but I found that TPUs greatly sped up training time. To use a TPU, just select it in the menu and change the \"use_tpu\" value below to True.","2626daad":"Let's use the model to predict the selected text for the test set.","bce482b3":"## Generating Predictions","3b80e774":"The example from the Keras website (https:\/\/keras.io\/examples\/nlp\/text_extraction_with_bert\/ is for the SQUAD dataset, one of the most famous text extraction datasets. In that dataset, there is a \"context\" (e.g. a paragraph), a \"question\", and an \"answer\" (which is a subset of the context). In this case, we can treat the tweet as the context, the sentiment as the question, and the selected text of the tweet as the answer.","18c73062":"## Data Preprocessing","922fcf21":"## Creating & Training Model","dca628b0":"We're going to use SpanBERT, a version of BERT specifically trained for predicting spans, as our transformer.\n\nFor more information about SpanBERT: https:\/\/github.com\/facebookresearch\/SpanBERT\n\nFor more information about BERT: https:\/\/towardsdatascience.com\/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270\n\nTo learn about transformers in general: https:\/\/medium.com\/inside-machine-learning\/what-is-a-transformer-d07dd1fbec04\n\nFeel free to explore different transformers- there are many great options!","5f7fe4e9":"We do the same thing for the testing data, which only has the text and sentiment of the tweet.","b365309c":"The first step is to tokenize the text, which turns it into an array of numbers.","700b8119":"Now, we can create the model that uses the SpanBERT transformer, and puts other layers on top of it. Shout-out to this notebook (https:\/\/www.kaggle.com\/al0kharba\/tensorflow-roberta-0-712) for helping me put together the layers.","0a0e5cf0":"Below, we create an object for the input data. This tokenizes the text and selected text, finds the indices of the selected text in the text, and prepares the data for input to BERT.","8d5d5128":"Now, we can just write our submission file!","8ecf6f2c":"Now, we can go back and fill in any gaps, like neutral tweets, and remove hyperlinks from the final results.","7f8ce869":"Here, we're going to adapt this example from the Keras website (https:\/\/keras.io\/examples\/nlp\/text_extraction_with_bert\/) to apply it to the Tweet Sentiment Extraction competition. We're going to take the text of a tweet and its sentiment, and return the part of the text that embodies its sentiment (positive, negative, or neutral).","a5676633":"Below is how we're going to evaluate the results of the model.","e5b8f748":"I've noticed that for the neutral tweets, most of the time the \"selected text\" is just the whole tweet. So, I'm going to go by that assumption, and only predict the selected text for the positive or negative tweets."}}