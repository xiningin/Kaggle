{"cell_type":{"9a0fe24a":"code","a53a8cbf":"code","e44075e5":"code","4dc14fa6":"code","7e8c4862":"code","6912bbdc":"code","795dce15":"code","c8809225":"code","385ca1c3":"code","98812b4b":"code","be7a7f7e":"code","ca98325f":"code","6034fb7d":"code","471d5ef3":"code","a66c5b3b":"code","6271f971":"code","1f4a434c":"code","c9b59e0d":"code","b1adb70b":"code","4b4f8bb0":"code","ae5ff8e0":"code","a10f6b83":"code","3e21a96c":"code","eeeac2de":"code","29652234":"code","4905bdfb":"code","cf090ffa":"code","0e2a936d":"code","ed2c885f":"code","6040a236":"code","b2197fee":"code","b74e4820":"code","b362ae2e":"code","753899e2":"code","2037262a":"code","506ade2f":"code","97d7de6a":"code","849a2bf8":"code","4a342316":"code","268b34ad":"code","4232cea6":"code","8427f925":"code","f92a777b":"code","04979789":"code","d70d5097":"code","a99c376c":"code","b97c34cd":"code","9b16fcfa":"code","16344357":"code","06f34ab4":"code","d5b74791":"code","a5c78683":"code","1eca6d52":"code","4453480e":"code","2702b240":"code","49af2922":"code","5b67f3a5":"code","b286a4c6":"code","faaa1e66":"code","ef41aaad":"code","2d126141":"code","46a35488":"code","c6b72fa2":"markdown","959754f6":"markdown","9dde2cab":"markdown","a32a0aca":"markdown","86e24a1a":"markdown","a234439b":"markdown","77f14633":"markdown","c0bfb960":"markdown","27c5ebfc":"markdown","8ea99b03":"markdown","30cff7c5":"markdown","c6a4cdab":"markdown","cbf0018f":"markdown","65960abb":"markdown","46a23afa":"markdown","fb6bb51c":"markdown","5d143d94":"markdown","b3065eac":"markdown","144af286":"markdown","cd06cfc5":"markdown","940c8514":"markdown","d5a353ed":"markdown","f36443e5":"markdown","31679a7d":"markdown","6dc65e1d":"markdown","fb4dcc4c":"markdown","9593a4e2":"markdown","bf9dd392":"markdown","8d14aad2":"markdown","8c7e7ee8":"markdown"},"source":{"9a0fe24a":"!pip install wget","a53a8cbf":"import wget\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.metrics import mean_squared_error as mse\nimport matplotlib.pyplot as plt","e44075e5":"nltk.download('stopwords')","4dc14fa6":"# Keras imports\nfrom tensorflow.keras import Sequential,Model\nfrom tensorflow.keras import initializers\nfrom tensorflow.keras.layers import Dense,MaxPool1D,Dropout,Embedding,Activation,Convolution1D,add,BatchNormalization,Flatten,Reshape,Concatenate,LSTM,GRU\nfrom tensorflow.keras.callbacks import ModelCheckpoint,ReduceLROnPlateau,EarlyStopping,Callback\nfrom tensorflow.keras.optimizers import Adam,SGD,Adamax,RMSprop\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.utils import plot_model\n\n","7e8c4862":"dj = pd.read_csv('..\/input\/stocknews\/upload_DJIA_table.csv')\nnews= pd.read_csv('..\/input\/stocknews\/RedditNews.csv')","6912bbdc":"dj.head()","795dce15":"dj.isnull().sum() # No missing data","c8809225":"news.isnull().sum()","385ca1c3":"news.head()","98812b4b":"print(dj.shape)\nprint(news.shape)","be7a7f7e":"# compare the num of unique dates\nprint(len(set(dj['Date'])))\n# news \nprint(len(set(news['Date'])))","ca98325f":"# Remove the extra dates that are in news\nnews = news[news['Date'].isin(dj.Date)]","6034fb7d":"# compare the num of unique dates\nprint(len(set(dj['Date'])))\n# news \nprint(len(set(news['Date'])))","471d5ef3":"dj= dj.drop(['High','Low','Close','Volume','Adj Close'],axis=1)\ndj.head()","a66c5b3b":"# Calculate the difference in opening prices betweeb the following and current day\n# The  model will try to predict the change in open value based on today's news\ndj= dj.set_index('Date')\ndj.head()","6271f971":"# Target variable = Tomorrow's open price - today's open price\ndj =-1 * dj.diff(periods=1)\ndj.head()","1f4a434c":"dj['Date']=dj.index\ndj= dj.reset_index(drop=True)","c9b59e0d":"dj.head()","b1adb70b":"# Remove top row since it has null value\ndj = dj[dj.Open.notnull()]","4b4f8bb0":"dj.isnull().sum()","ae5ff8e0":"# Create a list of opening prices and their corresponding headlines from the news\n# Define\/Intialize the variables\nprice= []\nheadlines=[]\n\n# for all the rows in dataframe\nfor row in dj.iterrows():\n    # define a variable to store all headlines for the day\n    daily_headlines=[]\n    # Spot the date in given row\n    date= row[1]['Date']\n    # Store the price for the data\n    price.append(row[1]['Open'])\n    for row_ in news[news.Date==date].iterrows():\n        daily_headlines.append(row_[1]['News'])\n        \n    # Apeend the headlines for the date\n    headlines.append(daily_headlines)\n    # Track progress\n    if len(price) % 500 == 0:\n        print(len(price))","a10f6b83":"# Check how headlines looks like\nprint(headlines[:1])\nprint(price[:1])\n","3e21a96c":"# price for top 3 headlines\nprice[:3]","eeeac2de":"# Normalize opening prices (target values)\nmax_price= max(price)\nmin_price= min(price)\nmean_price= np.mean(price)\ndef normalise(price):\n    return ((price - min_price)\/(max_price-min_price))","29652234":"norm_price=[]\nfor p in price:\n    norm_price.append(normalise(p))","4905bdfb":"# Check that normalization worked well\nprint(min(norm_price))\nprint(max(norm_price))\nprint(np.mean(norm_price))","cf090ffa":"# Compare the number of headlines for each day\nprint(max(len(i) for i in headlines))\nprint(min(len(i) for i in headlines))\nprint(np.mean([len(i) for i in headlines]))","0e2a936d":"norm_price[:3]","ed2c885f":"# remove contractions\ndef decontracted(phrase):\n    if \"'\" in phrase:\n        # specific\n        phrase = re.sub(r\"won't\", \"will not\", phrase)\n        phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n        # general\n        phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n        phrase = re.sub(r\"\\'re\", \" are\", phrase)\n        phrase = re.sub(r\"\\'s\", \" is\", phrase)\n        phrase = re.sub(r\"\\'d\", \" would\", phrase)\n        phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n        phrase = re.sub(r\"\\'t\", \" not\", phrase)\n        phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n        phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\ntext = \"I should've gone to dentist so my teeth wouldn't hurt\"\ntext1 = \"But I'm good now\"\nprint(decontracted(text))\nprint(decontracted(text1))\n","6040a236":"def clean_text(text):\n    '''Remove unwanted characters and format the text to create fewer nulls word embeddings'''\n    \n    # convert words into lower case\n    text= text.lower()\n    \n    # Replace contraction with their longest forms\n    if True:\n        text = text.split()\n        new_text = []\n        # Remove the contractions\n        for word in text:\n            new_text.append(decontracted(word))\n        # Recreate the sentence\n        text= ' '.join(new_text)\n        \n    # Format words and remove unwanted characters\n    text = re.sub(r'&amp;','', text)\n    text= re.sub(r'0,0','00', text)\n    text = re.sub(r'[_\"\\-;%()|.,+&=*%.,!?:#@\\[\\]]', ' ', text)\n    text = re.sub(r'\\'', ' ', text)\n    text = re.sub(r'\\$', ' $ ', text)\n    text = re.sub(r'u s ', ' united states ', text)\n    text = re.sub(r'u n ', ' united nations ', text)\n    text = re.sub(r'u k ', ' united kingdom ', text)\n    text = re.sub(r'j k ', ' jk ', text)\n    text = re.sub(r' s ', ' ', text)\n    text = re.sub(r' yr ', ' year ', text)\n    text = re.sub(r' l g b t ', ' lgbt ', text)\n    text = re.sub(r'0km ', '0 km ', text)\n    \n    # Remove stopwords\n    text=text.split()\n    stops= set(stopwords.words('english'))\n    text = [w for w in text if w not in stops]\n    text= \" \".join(text)\n    \n    return text","b2197fee":"# Clean the headlines\nclean_headlines=[]\nfor daily_headlines in headlines:\n    clean_daily_headlines=[]\n    for headline in daily_headlines:\n        clean_daily_headlines.append(clean_text(headline))\n    clean_headlines.append(clean_daily_headlines)","b74e4820":"# Take a look at some headlines to ensure everything was cleaned well\nclean_headlines[:3]","b362ae2e":"print('Roughly the number of unique words in English: {}'.format(len({word: None for headlines in clean_headlines for headline in headlines for word in headline.split()})))","753899e2":"# Create the word vocab \nimport collections\nwords = [word for headlines in clean_headlines for headline in headlines for word in headline.split()]\nword_counts = collections.Counter(words)","2037262a":"word_counts","506ade2f":"# Load GloVe's embeddings\nembeddings_index={}\nwith open('..\/input\/glove840b300dtxt\/glove.840B.300d.txt', encoding='utf-8') as f:\n    for line in f:\n        values= line.split(' ')\n        word= values[0]\n        embedding = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = embedding\n        \nprint('Word embeddings:', len(embeddings_index))","97d7de6a":"# Limit the vocab that we will use to words that appear >= threshold or are in GloVe\n\n# Define threshold\nthreshold= 10\n\n# dictionary to convert words to integers\nvocab_to_int={}\n\nvalue=0\nfor word,count in word_counts.items():\n    if count>=threshold or word in embeddings_index:\n        vocab_to_int[word]= value\n        value +=1","849a2bf8":"vocab_to_int","4a342316":"len(vocab_to_int)","268b34ad":"# Special tokens that will be added to our vocab\ncodes= [\"<UNK>\",\"<PAD>\"]\n\n# Add codes to vocab\nfor code in codes:\n    vocab_to_int[code]= len(vocab_to_int)\n    \n# Dictionary to convert intergers to words\nint_to_vocab={}\nfor word, value in vocab_to_int.items():\n    int_to_vocab[value]= word\n\nusage_ratio= round(len(vocab_to_int)\/ len(word_counts),4)*100\n\nprint(\"Total number of Unique Words:\", len(word_counts))\nprint(\"Number of words we will use:\", len(vocab_to_int))\nprint(\"Percent of Words we will use: {}%\".format(usage_ratio))","4232cea6":"# Need to use 300 for embedding dimnesion to match GloVe's vecors\nembedding_dim=300\n\nnb_words= len(vocab_to_int)\n# Create matrix with default values of zero\nword_embedding_matrix= np.zeros((nb_words,embedding_dim))\nfor word, i in vocab_to_int.items():\n    if word in embeddings_index:\n        word_embedding_matrix[i]= embeddings_index[word]\n    else:\n        # If word not in GloVe, create a random embedding for it\n        new_embedding= np.array(np.random.uniform(-1.0,1.0,embedding_dim))\n        embeddings_index[word]= new_embedding\n        word_embedding_matrix[i]= new_embedding\n        \n# Check if value matches len(vocab_to_int)\nprint(len(word_embedding_matrix))","8427f925":"print(len(vocab_to_int))","f92a777b":"# Change the text from words to integers\n# If word is not in vocab, replace it with <UNK> (unknown)\nword_count = 0\nunk_count = 0\n\nheadlines_sequence = []\n\nfor daily_headline in clean_headlines:\n    daily_headlines_seq = []\n    for headline in daily_headline:\n        headline_seq = []\n        for word in headline.split():\n            word_count += 1\n            if word in vocab_to_int:\n                headline_seq.append(vocab_to_int[word])\n            else:\n                headline_seq.append(vocab_to_int[\"<UNK>\"])\n                unk_count += 1\n        daily_headlines_seq.append(headline_seq)\n    headlines_sequence.append(daily_headlines_seq)\n\nunk_percent = round(unk_count\/word_count,4)*100\n\nprint(\"Total number of words in headlines:\", word_count)\nprint(\"Total number of UNKs in headlines:\", unk_count)\nprint(\"Percent of words that are UNK: {}%\".format(unk_percent))","04979789":"headlines_sequence[:1]","d70d5097":"# Find the length of headlines\nlengths = []\nfor headlines in headlines_sequence:\n    for headline in headlines:\n        lengths.append(len(headline))\n\n# Create a dataframe so that the values can be inspected\nlengths = pd.DataFrame(lengths, columns=['counts'])","a99c376c":"lengths.describe()","b97c34cd":"max_headline_length = 16\nmax_daily_length = 200\npad_headlines = []\n\n# For each date in all the dates available\nfor headlines in headlines_sequence:\n    pad_daily_headlines = []\n    # for each headline for each date\n    for headline in headlines:\n        # Add headline if it is less than max length\n        if len(headline) <= max_headline_length:\n            for word in headline:\n                pad_daily_headlines.append(word)\n        # Limit headline if it is more than max length  \n        else:\n            headline = headline[:max_headline_length]\n            for word in headline:\n                pad_daily_headlines.append(word)\n    \n    # Pad daily_headlines if they are less than max length\n    if len(pad_daily_headlines) < max_daily_length:\n        for i in range(max_daily_length-len(pad_daily_headlines)):\n            pad = vocab_to_int[\"<PAD>\"]\n            pad_daily_headlines.append(pad)\n    # Limit daily_headlines if they are more than max length\n    else:\n        pad_daily_headlines = pad_daily_headlines[:max_daily_length]\n    pad_headlines.append(pad_daily_headlines)","9b16fcfa":"x_train, x_test, y_train, y_test = train_test_split(pad_headlines, norm_price, test_size = 0.15, random_state = 2)\n\nx_train = np.array(x_train)\nx_test = np.array(x_test)\ny_train = np.array(y_train)\ny_test = np.array(y_test)","16344357":"# Check the lengths\nprint(len(x_train))\nprint(len(x_test))","06f34ab4":"filter_length= 5\ndropout=0.5\nlearning_rate=0.001\nweights= initializers.TruncatedNormal(mean=0.0,stddev=0.1,seed=2)\nnb_filter=16\nrnn_output_size = 128\nhidden_dims = 128","d5b74791":"def build_model():\n    \n    model=Sequential()\n    # Layer 1 - Embedding\n    model.add(Embedding(nb_words, embedding_dim,weights=[word_embedding_matrix],input_length=max_daily_length))\n    model.add(Dropout(dropout))\n    \n    # Layer 2 - Convolution with dropout\n    model.add(Convolution1D(filters=nb_filter,kernel_size= filter_length, padding='same',activation='relu'))\n    model.add(Dropout(dropout))\n    \n    # Layer 3 - Convolution 2 with dropout\n    model.add(Convolution1D(filters=nb_filter,kernel_size= filter_length, padding='same',activation='relu'))\n    model.add(Dropout(dropout))\n    \n    # Layer 4- RNN with dropout\n    model.add(LSTM(rnn_output_size,activation=None,kernel_initializer=weights,dropout=dropout))\n    \n    # Layer 5 - Dense FFN with Dropout\n    model.add(Dense(hidden_dims,kernel_initializer=weights))\n    model.add(Dropout(dropout))\n    \n    model.add(Dense(1,kernel_initializer=weights, name='output'))\n    \n    model.compile(loss='mean_squared_error',optimizer=Adam(lr=learning_rate,clipvalue=1.0))\n    \n    return model","a5c78683":"model = build_model()\nprint()\nsave_best_weights = 'best_weights.h5'\n\ncallbacks = [ModelCheckpoint(save_best_weights, monitor='val_loss', save_best_only=True),\n            EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto'),\n            ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=3)]\n\nhistory = model.fit([x_train],\n                    y_train,\n                    batch_size=128,\n                    epochs=100,\n                    validation_split=0.15,\n                    verbose=True,\n                    shuffle=True,\n                    callbacks = callbacks)\nprint(model.summary())","1eca6d52":"predictions= model.predict([x_test], verbose=True)","4453480e":"# Compare testing loss to training and validating loss\nmse(y_test, predictions)","2702b240":"mae(y_test, predictions)","49af2922":"# revert predictions back to actual scale\ndef unnormalize(price):\n    '''Revert values to their unnormalized amounts'''\n    price= price * (max_price-min_price) + min_price\n    return price","5b67f3a5":"# Store back-scaled predictions\nunnorm_predictions = []\nfor pred in predictions:\n    unnorm_predictions.append(unnormalize(pred))\n    \n# store back-scaled actuals\nunnorm_y_test = []\nfor y in y_test:\n    unnorm_y_test.append(unnormalize(y))","b286a4c6":"# calculate the median absolute error for the predictions\nmae(unnorm_y_test,unnorm_predictions)","faaa1e66":"pd.Series(unnorm_y_test).describe()","ef41aaad":"def news_to_int(news):\n    '''Convert your created news into integers'''\n    ints=[]\n    for word in news.split():\n        if word in vocab_to_int:\n            ints.append(vocab_to_int[word])\n        else:\n            ints.append(vocab_to_int['<UNK>'])\n    return ints","2d126141":"def padding_news(news):\n    '''Adjusts the length of your created news to fit the model's input values.'''\n    padded_news= news\n    if len(padded_news) < max_daily_length:\n        for i in range(max_daily_length- len(padded_news)):\n            padded_news.append(vocab_to_int[\"<PAD>\"])\n    elif len(padded_news) > max_daily_length:\n        padded_news = padded_news[:max_daily_length]\n    return padded_news","46a35488":"# Default news that you can use\ncreate_news =  \"Woman says note from Chinese 'prisoner' was hidden in new purse. \\\n               21,000 AT&T workers poised for Monday strike \\\n               housands march against Trump climate policies in D.C., across USA \\\n               Kentucky judge won't hear gay adoptions because it's not in the child's \\\"best interest\\\" \\\n               Multiple victims shot in UTC area apartment complex \\\n               Drones Lead Police to Illegal Dumping in Riverside County | NBC Southern California \\\n               An 86-year-old Californian woman has died trying to fight a man who was allegedly sexually assaulting her 61-year-old friend. \\\n               Fyre Festival Named in $5Million+ Lawsuit after Stranding Festival-Goers on Island with Little Food, No Security. \\\n               The \\\"Greatest Show on Earth\\\" folds its tent for good \\\n               U.S.-led fight on ISIS have killed 352 civilians: Pentagon \\\n               Woman offers undercover officer sex for $25 and some Chicken McNuggets \\\n               Ohio bridge refuses to fall down after three implosion attempts \\\n               Jersey Shore MIT grad dies in prank falling from library dome \\\n               New York graffiti artists claim McDonald's stole work for latest burger campaign \\\n               SpaceX to launch secretive satellite for U.S. intelligence agency \\\n               Severe Storms Leave a Trail of Death and Destruction Through the U.S. \\\n               Hamas thanks N. Korea for its support against \u2018Israeli occupation\u2019 \\\n               Baker Police officer arrested for allegedly covering up details in shots fired investigation \\\n               Miami doctor\u2019s call to broker during baby\u2019s delivery leads to $33.8 million judgment \\\n               Minnesota man gets 15 years for shooting 5 Black Lives Matter protesters \\\n               South Australian woman facing possible 25 years in Colombian prison for drug trafficking \\\n               The Latest: Deal reached on funding government through Sept. \\\n               Russia flaunts Arctic expansion with new military bases\"\n\nclean_news= clean_text(create_news)\nint_news = news_to_int(clean_news)\npad_news = padding_news(int_news)\npad_news = np.array(pad_news).reshape((1,-1))\npred = model.predict([pad_news])\nprice_change = unnormalize(pred)\nprint(\"The Dow should open: {} from the previous open.\".format(np.round(price_change[0][0],2)))","c6b72fa2":"## General Data flow for a Text Related Business Problem","959754f6":"## 2. Create the model","9dde2cab":"## Limit the length of a day's news to 200 words, and the length of any headline to 16 words. These values are chosen to not have an excessively long training time and balance the number of headlines used and the number of words from each headline.","a32a0aca":"Below is the code necessary to make your own predictions. I found that the predictions are most accurate when there is no padding included in the input data. In the create_news variable, I have some default news that you can use, which is from April 30th, 2017. Just change the text to whatever you want, then see the impact your new headline will have.","86e24a1a":"## Clean up the headlines list","a234439b":"## We are going to use Glove embeddings to initialize our weights while designing our neural network. Let's load the same so that we can ensure our headline corpus' vocabulary matches where possible with Glove Embedding vocabulary.","77f14633":"## 4. Predict using the model","c0bfb960":"![title](resources\/textmining.png)","27c5ebfc":"![basic](resources\/basic_intent.png)","8ea99b03":"## 3. Fit the model","30cff7c5":"**Reference**: https:\/\/nlp.stanford.edu\/projects\/glove\/","c6a4cdab":"## Clean up the price list","cbf0018f":"## It is not necessary that we will have embeddings for all the words in Glove. So to limit such cases by limiting vocabulary by applying simple logic:  Remove the words that are \"rare\" and are not available in Glove ","65960abb":"# Problem Statement & Reference Architecture\n\n* **Aim**: Use Reddit News Headlines to predict the movement of Dow Jones Industrial Average.   \n\n\n* **Data Source**: https:\/\/www.kaggle.com\/aaron7sun\/stocknews \n\n\n* **Data Description**: Dow Jones details on Open, High, Low and Close for each day from 2008-08-08 to 2016-07-01 and headlines for those dates from Reddit News. \n\n\n* **Methodology**: For this project, we will use GloVe to create our word embeddings and CNNs followed by LSTMs to build our model. This model is based off the work done in this paper https:\/\/www.aclweb.org\/anthology\/C\/C16\/C16-1229.pdf.","46a23afa":"# Model Building","fb6bb51c":"<table size=\"100\">\n    <tr>\n        <td>headlines<\/td>\n        <td>price<\/td>\n    <\/tr>\n    <tr>\n        <td>headline-1, headline-2 ..., headline-n<\/td>\n        <td>211.48<\/td>\n    <\/tr>\n<\/table>","5d143d94":"## The CNN-RNN architecture\n![cnn-rnn](resources\/cnn-1d-rnn.jpg)","b3065eac":"## 1. Define the hyperparameters","144af286":"# A note on Word Embeddings","cd06cfc5":"## For the words which are common within headlines but are absent in Glove corpus, we will have to randomly initialize them. Over the training, those values will be finetuned along with those of Glove vectors.","940c8514":"## Combine the two datasets - For each date, get all the headlines and the price","d5a353ed":"## Make Your Own Predictions","f36443e5":"![word_embed](resources\/wordvectors.png)","31679a7d":"## Inspect the data","6dc65e1d":"\n# Predicting the Dow Jones with News","fb4dcc4c":"## Convert the word sequences to equivalent integer sequences so that it can be used as input to the model","9593a4e2":"## Split data into training and testing sets.\n## Validating data will be created during training.","bf9dd392":"# Imports","8d14aad2":"# Installation Prerequisites","8c7e7ee8":"## Ensure that the variations in the number of news headlines each day and length of each headlines are handled by taking an average number of headlines each day and average length per headline "}}