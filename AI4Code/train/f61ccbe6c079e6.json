{"cell_type":{"8a5c40bb":"code","249da493":"code","8690f3fb":"code","fcba0bbe":"code","ab73ee7c":"code","79ea0c79":"code","714b72d5":"code","06910f83":"code","3a1cd5d4":"code","2765cad1":"code","f49d4050":"code","740056f8":"code","9dc73076":"code","f82c5452":"code","d8f9ac02":"code","dc22cc29":"code","43ef9f6d":"code","d014769e":"code","8970ee39":"code","0b38947b":"code","d7fa8b25":"code","3e5d6c56":"code","3c39032d":"code","f1eab36e":"code","1534c1d9":"code","61622212":"code","a2ba23e8":"code","6a1e0f59":"markdown","7aeef0d4":"markdown","38860419":"markdown","19f59961":"markdown","16eb817b":"markdown","5803f9d7":"markdown","2d6840fa":"markdown","7a55dfd3":"markdown","b0ad506b":"markdown","57382ab1":"markdown","c4fcf2d8":"markdown"},"source":{"8a5c40bb":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom dateutil import parser\n%matplotlib inline\n\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix,classification_report,accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nimport pickle\nfrom lightgbm import LGBMClassifier\nprint('Library Loaded')","249da493":"df = pd.read_csv('..\/input\/diabetes\/diabetes.csv')#,engine='python')\ndf.shape","8690f3fb":"df.head()","fcba0bbe":"df.isnull().sum()","ab73ee7c":"\n# Correlation matrix\ncorrmat = df.corr()\nfig = plt.figure(figsize = (12, 12))\n\nsns.heatmap(corrmat, vmax = 1, square = True,annot=True,vmin=-1)\nplt.show()","79ea0c79":"df.hist(figsize=(12,12))\nplt.show()","714b72d5":"sns.pairplot(df,hue='Outcome')","06910f83":"#check for unbalance \ndf.Outcome.value_counts()","3a1cd5d4":"df.columns","2765cad1":"print(\"# rows in dataframe {0}\".format(len(df)))\nprint(\"-------------------------------------------\")\nprint(\"# rows missing Glucose: {0}\".format(len(df.loc[df.Glucose == 0 ])))\nprint(\"# rows missing BloodPressure: {0}\".format(len(df.loc[df.BloodPressure == 0 ])))\nprint(\"# rows missing SkinThickness: {0}\".format(len(df.loc[df.SkinThickness == 0 ])))\nprint(\"# rows missing insulin: {0}\".format(len(df.loc[df.Insulin == 0 ])))\nprint(\"# rows missing bmi: {0}\".format(len(df.loc[df.BMI == 0 ])))\nprint(\"# rows missing Age: {0}\".format(len(df.loc[df.Age == 0 ])))\nprint(\"# rows missing Pregnancies: {0}\".format(len(df.loc[df.Pregnancies == 0 ])))\nprint(\"# rows missing DiabetesPedigreeFunction: {0}\".format(len(df.loc[df.DiabetesPedigreeFunction == 0 ])))","f49d4050":"X = df.drop('Outcome',axis=1) # predictor feature coloumns\ny = df.Outcome\n\n\nX_train , X_test , y_train , y_test = train_test_split(X, y, test_size = 0.20, random_state = 10)\n\nprint('Training Set :',len(X_train))\nprint('Test Set :',len(X_test))\nprint('Training labels :',len(y_train))\nprint('Test Labels :',len(y_test))","740056f8":"# from sklearn.preprocessing import Imputer\nfrom sklearn.impute import SimpleImputer\n#impute with mean all 0 readings\n\nfill = SimpleImputer(missing_values = 0 , strategy =\"mean\")\n\nX_train = fill.fit_transform(X_train)\nX_test = fill.fit_transform(X_test)","9dc73076":"print('Training Set :',len(X_train))\nprint('Test Set :',len(X_test))\nprint('Training labels :',len(y_train))\nprint('Test Labels :',len(y_test))","f82c5452":"def FitModel(X_train,y_train,X_test,y_test,algo_name,algorithm,gridSearchParams,cv):\n    np.random.seed(10)\n   \n    \n    grid = GridSearchCV(\n        estimator=algorithm,\n        param_grid=gridSearchParams,\n        cv=cv, scoring='accuracy', verbose=1, n_jobs=-1)\n    \n    \n    grid_result = grid.fit(X_train, y_train)\n    best_params = grid_result.best_params_\n    pred = grid_result.predict(X_test)\n    cm = confusion_matrix(y_test, pred)\n   # metrics =grid_result.gr\n    print(pred)\n    #pickle.dump(grid_result,open(algo_name,'wb'))\n   \n    print('Best Params :',best_params)\n    print('Classification Report :',classification_report(y_test,pred))\n    print('Accuracy Score : ' + str(accuracy_score(y_test,pred)))\n    print('Confusion Matrix : \\n', cm)","d8f9ac02":"# Create regularization penalty space\npenalty = ['l1', 'l2']\n\n# Create regularization hyperparameter space\nC = np.logspace(0, 4, 10)\n\n# Create hyperparameter options\nhyperparameters = dict(C=C, penalty=penalty)\n\nFitModel(X_train,y_train,X_test,y_test,'LogisticRegression',LogisticRegression(),hyperparameters,cv=5)","dc22cc29":"param ={\n            'n_estimators': [100, 500, 1000,1500, 2000],\n            'max_depth' :[2,3,4,5,6,7],\n    'learning_rate':np.arange(0.01,0.1,0.01).tolist()\n           \n        }\n\nFitModel(X_train,y_train,X_test,y_test,'XGBoost',XGBClassifier(),param,cv=5)","43ef9f6d":"param ={\n            'n_estimators': [100, 500, 1000,1500, 2000],\n           \n        }\nFitModel(X_train,y_train,X_test,y_test,'Random Forest',RandomForestClassifier(),param,cv=5)","d014769e":"param ={\n            'C': [0.1, 1, 100, 1000],\n            'gamma': [0.0001, 0.001, 0.005, 0.1, 1, 3, 5]\n        }\nFitModel(X_train,y_train,X_test,y_test,'SVC',SVC(),param,cv=5)","8970ee39":"y.value_counts()","0b38947b":"from imblearn.over_sampling import SMOTE\nsm =SMOTE(random_state=42)\nX_res_OS , Y_res_OS = sm.fit_resample(X,y)\npd.Series(Y_res_OS).value_counts()","d7fa8b25":"X_train , X_test , y_train , y_test = train_test_split(X_res_OS, Y_res_OS, test_size = 0.20, random_state = 10)\n\nprint('Training Set :',len(X_train))\nprint('Test Set :',len(X_test))\nprint('Training labels :',len(y_train))\nprint('Test Labels :',len(y_test))","3e5d6c56":"\nfill = SimpleImputer(missing_values = 0 , strategy =\"mean\")\n\nX_train = fill.fit_transform(X_train)\nX_test = fill.fit_transform(X_test)","3c39032d":"print('Training Set :',len(X_train))\nprint('Test Set :',len(X_test))\nprint('Training labels :',len(y_train))\nprint('Test Labels :',len(y_test))","f1eab36e":"# Create regularization penalty space\npenalty = ['l1', 'l2']\n\n# Create regularization hyperparameter space\nC = np.logspace(0, 4, 10)\n\n# Create hyperparameter options\nhyperparameters = dict(C=C, penalty=penalty)\n\nFitModel(X_train,y_train,X_test,y_test,'LogisticRegression',LogisticRegression(),hyperparameters,cv=5)","1534c1d9":"param ={\n            'n_estimators': [100, 500, 1000,1500, 2000],\n            'max_depth' :[2,3,4,5,6,7],\n    'learning_rate':np.arange(0.01,0.1,0.01).tolist()\n           \n        }\n\nFitModel(X_train,y_train,X_test,y_test,'XGBoost',XGBClassifier(),param,cv=5)","61622212":"param ={\n            'n_estimators': [100, 500, 1000,1500, 2000],\n           \n        }\nFitModel(X_train,y_train,X_test,y_test,'Random Forest',RandomForestClassifier(),param,cv=5)","a2ba23e8":"param ={\n            'C': [0.1, 1, 100, 1000],\n            'gamma': [0.0001, 0.001, 0.005, 0.1, 1, 3, 5]\n        }\nFitModel(X_train,y_train,X_test,y_test,'SVC',SVC(),param,cv=5)","6a1e0f59":"# Logistic Regression","7aeef0d4":"# SVC  - After Over sampling","38860419":"# XgBoost  - After Over sampling","19f59961":"# Feature Engineering","16eb817b":"# XgBoost","5803f9d7":"# Random Forest","2d6840fa":"# Random Forest  - After Over sampling","7a55dfd3":"# Model Building and Evaluation","b0ad506b":"# Balancing the Dataset - Over Sampling","57382ab1":"# SVC","c4fcf2d8":"# Logistic Regression - After Over sampling"}}