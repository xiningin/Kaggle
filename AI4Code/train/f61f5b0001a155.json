{"cell_type":{"eddcb370":"code","efefc6db":"code","39f09e9e":"code","ce5d49ce":"code","f394769f":"code","e384db1c":"code","14887624":"code","de3dd4f9":"code","efe0fc14":"code","c4c44d9d":"code","0e47d788":"code","68dcd9fe":"code","53a7673c":"code","413d5539":"code","2b1f2114":"code","2583b3b2":"code","5738fc09":"code","0992fb07":"code","d59a4954":"code","59bf13bd":"code","9a2f09d9":"code","8e2a7e34":"code","72f46acf":"code","1eb23dcf":"code","8ea01107":"code","e91cbdde":"code","7485276e":"code","f3754cce":"code","4f7e9308":"code","89e62bc2":"code","4237625b":"code","058e3736":"code","e54d8d8b":"code","a8150063":"code","954dac82":"code","f699db0d":"code","c29d2200":"code","ef208774":"code","2fa56e25":"code","ca1ea123":"code","10647a8f":"code","b12e0a14":"code","7997a0e1":"code","abe9f8db":"code","7c642cbd":"code","84e977cf":"markdown","f28d551e":"markdown","b1a4d7c4":"markdown","9944c918":"markdown","d8645d2a":"markdown","b744f4ef":"markdown","a549a463":"markdown","f7e74817":"markdown","0ed88fba":"markdown","889b259a":"markdown","98025a71":"markdown","8ff38fc7":"markdown","61b11885":"markdown","1a47e800":"markdown","458ea1d8":"markdown","12a8e12f":"markdown","47fdf9d9":"markdown","bb26fcf9":"markdown","9739d63b":"markdown","1739df75":"markdown","30a9eeb3":"markdown","0fe1a413":"markdown","ca1870c5":"markdown","343ca6cc":"markdown"},"source":{"eddcb370":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set()\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objects as go\n\nfrom PIL import Image\nimport pydicom\nfrom skimage.io import imread\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.cluster import KMeans\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","efefc6db":"from matplotlib import rcParams\n\nrcParams['figure.figsize'] = 12,8","39f09e9e":"from os import listdir\nlistdir(\"..\/input\/\")","ce5d49ce":"base = \"..\/input\/siim-isic-melanoma-classification\/\"\nmodels = \"..\/input\/pytorch-pretrained-image-models\/\"\nimagestats = \"..\/input\/siimisic-melanoma-classification-image-stats\/\"","f394769f":"train_data = pd.read_csv(base + \"train.csv\")\ntrain_data.head()","e384db1c":"train_data.dtypes","14887624":"100*train_data.isnull().mean()","de3dd4f9":"train_data.nunique()","efe0fc14":"columns = ['sex', 'anatom_site_general_challenge', 'diagnosis', 'age_approx']\nfor column in columns:\n    uniques = train_data[column].unique().tolist()\n    print(column, ' - ', uniques)","c4c44d9d":"test_data =  pd.read_csv(base + \"test.csv\")\ntest_data.head()","0e47d788":"!pip install chart_studio\nimport plotly.express as px\nimport chart_studio.plotly as py\nimport plotly.graph_objs as go\nfrom plotly.offline import iplot","68dcd9fe":"# Total number of images in the dataset(train+test)\nprint(\"Total images in Train set: \",train_data['image_name'].count())\nprint(\"Total images in Test set: \",test_data['image_name'].count())","53a7673c":"groupped_data = train_data.groupby(['benign_malignant', 'sex']).agg({'target': 'count'})\ngroupped_data['%'] = groupped_data.groupby(level=0).apply(lambda x: 100*x\/x.sum())\ngroupped_data","413d5539":"groupped_data = train_data.groupby(['diagnosis']).agg({'age_approx': 'mean'})\ngroupped_data","2b1f2114":"groupped_data = train_data.groupby(['diagnosis', 'anatom_site_general_challenge']).agg({'target': 'count'})\ngroupped_data['%'] = groupped_data.groupby(level=0).apply(lambda x: 100*x\/x.sum())\ngroupped_data = groupped_data.drop(['target'], axis=1)\ngroupped_data = groupped_data.reset_index()","2583b3b2":"g = sns.barplot(x='anatom_site_general_challenge', y='%', hue = 'diagnosis', data = groupped_data)\ng.legend(loc='upper right')","5738fc09":"groupped_data = train_data.groupby('patient_id').agg({'diagnosis': lambda x: x.nunique()}).reset_index()\n\nids = groupped_data.loc[groupped_data['diagnosis'] > 1, 'patient_id'].unique().tolist()\nprint('Patients with more than one diagnosis', len(set(ids)))\n\ndata = train_data.loc[train_data['patient_id'].isin(ids), ]\nids = data.loc[data['target'] == 1, 'patient_id'].unique().tolist()\nprint('Patients with melanoma and other diagnosis', len(ids))\ndata = data.loc[data['patient_id'].isin(ids), ]\ndata = data.sort_values(by=['patient_id', 'age_approx', 'image_name'])\ndata['d'] = 1","0992fb07":"ids = data['patient_id'].unique().tolist()\nl = []\nk = 0\n\nfor i, id in enumerate(ids):\n    d = data.loc[data['patient_id'] == id, ].copy()\n    d = d.reset_index(drop=True)\n    index = d.loc[d['target'] == 1, ].index.values[0]\n    if index == 0:\n        k = k+1\n        continue\n    else:\n        d = d[d.index < index]\n    groupped_data = d.groupby(['benign_malignant', 'diagnosis']).agg({'target': 'count'})\n    groupped_data['%'] = groupped_data.groupby(level=0).apply(lambda x: 100*x\/x.sum())\n    groupped_data = groupped_data.reset_index()\n    l.append(groupped_data)\nprint(k)\nprint(len(l))","d59a4954":"groupped_data = train_data.groupby(['sex', 'diagnosis']).agg({'target': 'count'})\ngroupped_data['%'] = groupped_data.groupby(level=0).apply(lambda x: 100*x\/x.sum())\ngroupped_data","59bf13bd":"groupped_data = train_data.groupby(['sex', 'diagnosis']).agg({'age_approx': 'mean'})\ngroupped_data","9a2f09d9":"sns.distplot(train_data.loc[train_data['target'] == 0, 'age_approx'], label='Benign')\nsns.distplot(train_data.loc[train_data['target'] == 1, 'age_approx'], label='Malignant')\nplt.legend()","8e2a7e34":"columns = ['benign_malignant', 'sex', 'target']\ntrain_data[columns].groupby(['benign_malignant', 'sex']).count()","72f46acf":"# gender vs target\ntar=train_data.groupby(['target','sex'])['benign_malignant'].count().to_frame().reset_index()\ntar.style.background_gradient(cmap='Reds')  ","1eb23dcf":"sns.catplot(x='target',y='benign_malignant', hue='sex',data=tar,kind='bar')\nplt.ylabel('Count')\nplt.xlabel('benign:0 vs malignant:1')","8ea01107":"# melanoma vs average age\nplt.figure()\ntrain_data.groupby(['benign_malignant']).mean()['age_approx'].plot.bar(x = 'Diagnosis Type', y = 'Average age', rot = 0)\nplt.title('Benign\/Malignant vs Average Age')\nplt.xlabel('Diagnosis Outcome')\nplt.ylabel('Average Approx. Age')\nplt.show()","e91cbdde":"basepath = \"..\/input\/siim-isic-melanoma-classification\/\"\nmodelspath = \"..\/input\/pytorch-pretrained-image-models\/\"\nimagestatspath = \"..\/input\/siimisic-melanoma-classification-image-stats\/\"","7485276e":"import os\nexample_files = os.listdir(basepath + \"train\/\")[0:2]\nexample_files","f3754cce":"test_image_stats = pd.read_csv(imagestatspath +  \"test_image_stats.csv\")\ntest_image_stats.head(1)","4f7e9308":"plot_test = True","89e62bc2":"if plot_test:\n    N = test_image_stats.shape[0]\n    selected_data = test_image_stats\n    my_title = \"Test image statistics\"\nelse:\n    N = train_image_stats.shape[0]\n    selected_data = train_image_stats\n    my_title = \"Train image statistics\"\n\ntrace1 = go.Scatter3d(\n    x=selected_data.img_mean.values[0:N], \n    y=selected_data.img_std.values[0:N],\n    z=selected_data.img_skew.values[0:N],\n    mode='markers',\n    text=selected_data[\"rows\"].values[0:N],\n    marker=dict(\n        color=selected_data[\"columns\"].values[0:N],\n        colorscale = \"Jet\",\n        colorbar=dict(thickness=10, title=\"image columns\", len=0.8),\n        opacity=0.4,\n        size=2\n    )\n)\n\nfigure_data = [trace1]\nlayout = go.Layout(\n    title = my_title,\n    scene = dict(\n        xaxis = dict(title=\"Image mean\"),\n        yaxis = dict(title=\"Image standard deviation\"),\n        zaxis = dict(title=\"Image skewness\"),\n    ),\n    margin=dict(\n        l=0,\n        r=0,\n        b=0,\n        t=0\n    ),\n    showlegend=True\n)\n\nfig = go.Figure(data=figure_data, layout=layout)\npy.iplot(fig, filename='simple-3d-scatter')","4237625b":"!pip install efficientnet","058e3736":"\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import auc\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.callbacks import CSVLogger\n\nimport numpy as np # the most important library in python\nimport pandas as pd\n\n\nimport efficientnet.keras as efn","e54d8d8b":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","a8150063":"def decode_image(image):\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0\n    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n    return image","954dac82":"def augmentation_pipeline(image, label):\n    image = tf.image.random_flip_left_right(image)\n    return image, label","f699db0d":"def read_tfrecord(example, labeled):\n    tfrecord_format = {\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([], tf.int64)\n    } if labeled else {\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"image_name\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, tfrecord_format)\n    image = decode_image(example['image'])\n    if labeled:\n        label = tf.cast(example['target'], tf.int32)\n        return image, label\n    idnum = example['image_name']\n    return image, idnum","c29d2200":"def read_labeled_tfrecord(example):\n    tfrec_format = {\n        'image'                        : tf.io.FixedLenFeature([], tf.string),\n        'image_name'                   : tf.io.FixedLenFeature([], tf.string),\n        'patient_id'                   : tf.io.FixedLenFeature([], tf.int64),\n        'sex'                          : tf.io.FixedLenFeature([], tf.int64),\n        'age_approx'                   : tf.io.FixedLenFeature([], tf.int64),\n        'anatom_site_general_challenge': tf.io.FixedLenFeature([], tf.int64),\n        'diagnosis'                    : tf.io.FixedLenFeature([], tf.int64),\n        'target'                       : tf.io.FixedLenFeature([], tf.int64)\n    }           \n    example = tf.io.parse_single_example(example, tfrec_format)\n    return example['image'], example['target']\n\n\ndef read_unlabeled_tfrecord(example, return_image_name):\n    tfrec_format = {\n        'image'                        : tf.io.FixedLenFeature([], tf.string),\n        'image_name'                   : tf.io.FixedLenFeature([], tf.string),\n    }\n    example = tf.io.parse_single_example(example, tfrec_format)\n    return example['image'], example['image_name'] if return_image_name else 0\n\n \ndef prepare_image(img, augment=True, dim=256):    \n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.cast(img, tf.float32) \/ 255.0\n    \n    if augment:\n        img = transform(img,DIM=dim)\n        img = tf.image.random_flip_left_right(img)\n        #img = tf.image.random_hue(img, 0.01)\n        img = tf.image.random_saturation(img, 0.7, 1.3)\n        img = tf.image.random_contrast(img, 0.8, 1.2)\n        img = tf.image.random_brightness(img, 0.1)\n                      \n    img = tf.reshape(img, [dim,dim, 3])\n            \n    return img\n\ndef count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) \n         for filename in filenames]\n    return np.sum(n)","ef208774":"class AdvancedHairAugmentation:\n    \"\"\"\n    Impose an image of a hair to the target image\n\n    Args:\n        hairs (int): maximum number of hairs to impose\n        hairs_folder (str): path to the folder with hairs images\n    \"\"\"\n\n    def __init__(self, hairs: int = 5, hairs_folder: str = \"\"):\n        self.hairs = hairs\n        self.hairs_folder = hairs_folder\n\n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img (PIL Image): Image to draw hairs on.\n\n        Returns:\n            PIL Image: Image with drawn hairs.\n        \"\"\"\n        n_hairs = random.randint(0, self.hairs)\n        \n        if not n_hairs:\n            return img\n        \n        height, width, _ = img.shape  # target image width and height\n        hair_images = [im for im in os.listdir(self.hairs_folder) if 'png' in im]\n        \n        for _ in range(n_hairs):\n            hair = cv2.imread(os.path.join(self.hairs_folder, random.choice(hair_images)))\n            hair = cv2.flip(hair, random.choice([-1, 0, 1]))\n            hair = cv2.rotate(hair, random.choice([0, 1, 2]))\n\n            h_height, h_width, _ = hair.shape  # hair image width and height\n            roi_ho = random.randint(0, img.shape[0] - hair.shape[0])\n            roi_wo = random.randint(0, img.shape[1] - hair.shape[1])\n            roi = img[roi_ho:roi_ho + h_height, roi_wo:roi_wo + h_width]\n\n            # Creating a mask and inverse mask\n            img2gray = cv2.cvtColor(hair, cv2.COLOR_BGR2GRAY)\n            ret, mask = cv2.threshold(img2gray, 10, 255, cv2.THRESH_BINARY)\n            mask_inv = cv2.bitwise_not(mask)\n\n            # Now black-out the area of hair in ROI\n            img_bg = cv2.bitwise_and(roi, roi, mask=mask_inv)\n\n            # Take only region of hair from hair image.\n            hair_fg = cv2.bitwise_and(hair, hair, mask=mask)\n\n            # Put hair in ROI and modify the target image\n            dst = cv2.add(img_bg, hair_fg)\n\n            img[roi_ho:roi_ho + h_height, roi_wo:roi_wo + h_width] = dst\n                \n        return img\n\n    def __repr__(self):\n        return f'{self.__class__.__name__}(hairs={self.hairs}, hairs_folder=\"{self.hairs_folder}\")'","2fa56e25":"def get_ce_loss():   \n    criterion = torch.nn.CrossEntropyLoss()\n    return criterion","ca1ea123":"def get_wce_loss(train_targets):\n    weights = compute_class_weight(y=train_targets,\n                                   class_weight=\"balanced\",\n                                   classes=np.unique(train_targets))    \n    class_weights = torch.FloatTensor(weights)\n    if device.type==\"cuda\":\n        class_weights = class_weights.cuda()\n    criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n    return criterion","10647a8f":"class MulticlassFocalLoss(torch.nn.Module):\n    \n    def __init__(self, train_targets=None, gamma=2):\n        super(MulticlassFocalLoss, self).__init__()\n        self.gamma = gamma\n        if train_targets is None:\n            self.class_weights = None\n        else:\n            weights = compute_class_weight(y=train_targets,\n                                   class_weight=\"balanced\",\n                                   classes=np.unique(train_targets))    \n            self.class_weights = torch.FloatTensor(weights)\n            if device.type==\"cuda\":\n                self.class_weights = self.class_weights.cuda()\n    \n    def forward(self, input, target):\n        if self.class_weights is None:\n            ce_loss = F.cross_entropy(input, target, reduction='none')\n        else:\n            ce_loss = F.cross_entropy(input, target, reduction='none', weight=self.class_weights)\n        pt = torch.exp(-ce_loss)\n        loss = (1-pt)**self.gamma * ce_loss\n        return torch.mean(loss)","b12e0a14":"\n# USE VERBOSE=0 for silent, VERBOSE=1 for interactive, VERBOSE=2 for commit\nVERBOSE = 0\nDISPLAY_PLOT = True\n\nskf = KFold(n_splits=FOLDS,shuffle=True,random_state=SEED)\noof_pred = []; oof_tar = []; oof_val = []; oof_names = []; oof_folds = [] \npreds = np.zeros((count_data_items(files_test),1))\n\nfor fold,(idxT,idxV) in enumerate(skf.split(np.arange(15))):\n    \n    # DISPLAY FOLD INFO\n    if DEVICE=='TPU':\n        if tpu: tf.tpu.experimental.initialize_tpu_system(tpu)\n    print('#'*25); print('#### FOLD',fold+1)\n    print('#### Image Size %i with EfficientNet B%i and batch_size %i'%\n          (IMG_SIZES[fold],EFF_NETS[fold],BATCH_SIZES[fold]*REPLICAS))\n    \n    # CREATE TRAIN AND VALIDATION SUBSETS\n    files_train = tf.io.gfile.glob([GCS_PATH[fold] + '\/train%.2i*.tfrec'%x for x in idxT])\n    if INC2019[fold]:\n        files_train += tf.io.gfile.glob([GCS_PATH2[fold] + '\/train%.2i*.tfrec'%x for x in idxT*2+1])\n        print('#### Using 2019 external data')\n    if INC2018[fold]:\n        files_train += tf.io.gfile.glob([GCS_PATH2[fold] + '\/train%.2i*.tfrec'%x for x in idxT*2])\n        print('#### Using 2018+2017 external data')\n    np.random.shuffle(files_train); print('#'*25)\n    files_valid = tf.io.gfile.glob([GCS_PATH[fold] + '\/train%.2i*.tfrec'%x for x in idxV])\n    files_test = np.sort(np.array(tf.io.gfile.glob(GCS_PATH[fold] + '\/test*.tfrec')))\n    \n    # BUILD MODEL\n    K.clear_session()\n    with strategy.scope():\n        model = build_model(dim=IMG_SIZES[fold],ef=EFF_NETS[fold])\n        \n    # SAVE BEST MODEL EACH FOLD\n    sv = tf.keras.callbacks.ModelCheckpoint(\n        'fold-%i.h5'%fold, monitor='val_loss', verbose=0, save_best_only=True,\n        save_weights_only=True, mode='min', save_freq='epoch')\n   \n    # TRAIN\n    print('Training...')\n    history = model.fit(\n        get_dataset(files_train, augment=True, shuffle=True, repeat=True,\n                dim=IMG_SIZES[fold], batch_size = BATCH_SIZES[fold]), \n        epochs=EPOCHS[fold], callbacks = [sv,get_lr_callback(BATCH_SIZES[fold])], \n        steps_per_epoch=count_data_items(files_train)\/BATCH_SIZES[fold]\/\/REPLICAS,\n        validation_data=get_dataset(files_valid,augment=False,shuffle=False,\n                repeat=False,dim=IMG_SIZES[fold]), #class_weight = {0:1,1:2},\n        verbose=VERBOSE\n    )\n    \n    print('Loading best model...')\n    model.load_weights('fold-%i.h5'%fold)\n    \n    # PREDICT OOF USING TTA\n    print('Predicting OOF with TTA...')\n    ds_valid = get_dataset(files_valid,labeled=False,return_image_names=False,augment=True,\n            repeat=True,shuffle=False,dim=IMG_SIZES[fold],batch_size=BATCH_SIZES[fold]*4)\n    ct_valid = count_data_items(files_valid); STEPS = TTA * ct_valid\/BATCH_SIZES[fold]\/4\/REPLICAS\n    pred = model.predict(ds_valid,steps=STEPS,verbose=VERBOSE)[:TTA*ct_valid,] \n    oof_pred.append( np.mean(pred.reshape((ct_valid,TTA),order='F'),axis=1) )                 \n    #oof_pred.append(model.predict(get_dataset(files_valid,dim=IMG_SIZES[fold]),verbose=1))\n    \n    # GET OOF TARGETS AND NAMES\n    ds_valid = get_dataset(files_valid, augment=False, repeat=False, dim=IMG_SIZES[fold],\n            labeled=True, return_image_names=True)\n    oof_tar.append( np.array([target.numpy() for img, target in iter(ds_valid.unbatch())]) )\n    oof_folds.append( np.ones_like(oof_tar[-1],dtype='int8')*fold )\n    ds = get_dataset(files_valid, augment=False, repeat=False, dim=IMG_SIZES[fold],\n                labeled=False, return_image_names=True)\n    oof_names.append( np.array([img_name.numpy().decode(\"utf-8\") for img, img_name in iter(ds.unbatch())]))\n    \n    # PREDICT TEST USING TTA\n    print('Predicting Test with TTA...')\n    ds_test = get_dataset(files_test,labeled=False,return_image_names=False,augment=True,\n            repeat=True,shuffle=False,dim=IMG_SIZES[fold],batch_size=BATCH_SIZES[fold]*4)\n    ct_test = count_data_items(files_test); STEPS = TTA * ct_test\/BATCH_SIZES[fold]\/4\/REPLICAS\n    pred = model.predict(ds_test,steps=STEPS,verbose=VERBOSE)[:TTA*ct_test,] \n    preds[:,0] += np.mean(pred.reshape((ct_test,TTA),order='F'),axis=1) * WGTS[fold]\n    \n    # REPORT RESULTS\n    auc = roc_auc_score(oof_tar[-1],oof_pred[-1])\n    oof_val.append(np.max( history.history['val_auc'] ))\n    print('#### FOLD %i OOF AUC without TTA = %.3f, with TTA = %.3f'%(fold+1,oof_val[-1],auc))\n    \n    # PLOT TRAINING\n    if DISPLAY_PLOT:\n        plt.figure(figsize=(15,5))\n        plt.plot(np.arange(EPOCHS[fold]),history.history['auc'],'-o',label='Train AUC',color='#ff7f0e')\n        plt.plot(np.arange(EPOCHS[fold]),history.history['val_auc'],'-o',label='Val AUC',color='#1f77b4')\n        x = np.argmax( history.history['val_auc'] ); y = np.max( history.history['val_auc'] )\n        xdist = plt.xlim()[1] - plt.xlim()[0]; ydist = plt.ylim()[1] - plt.ylim()[0]\n        plt.scatter(x,y,s=200,color='#1f77b4'); plt.text(x-0.03*xdist,y-0.13*ydist,'max auc\\n%.2f'%y,size=14)\n        plt.ylabel('AUC',size=14); plt.xlabel('Epoch',size=14)\n        plt.legend(loc=2)\n        plt2 = plt.gca().twinx()\n        plt2.plot(np.arange(EPOCHS[fold]),history.history['loss'],'-o',label='Train Loss',color='#2ca02c')\n        plt2.plot(np.arange(EPOCHS[fold]),history.history['val_loss'],'-o',label='Val Loss',color='#d62728')\n        x = np.argmin( history.history['val_loss'] ); y = np.min( history.history['val_loss'] )\n        ydist = plt.ylim()[1] - plt.ylim()[0]\n        plt.scatter(x,y,s=200,color='#d62728'); plt.text(x-0.03*xdist,y+0.05*ydist,'min loss',size=14)\n        plt.ylabel('Loss',size=14)\n        plt.title('FOLD %i - Image Size %i, EfficientNet B%i, inc2019=%i, inc2018=%i'%\n                (fold+1,IMG_SIZES[fold],EFF_NETS[fold],INC2019[fold],INC2018[fold]),size=18)\n        plt.legend(loc=3)\n        plt.show()  \n","7997a0e1":"import numpy as np\nimport pandas as pd\nimport os\nfrom matplotlib import pyplot as plt","abe9f8db":"adias_submission = pd.DataFrame(dict(image_name=image_names, target=preds[:,0]))\nadias_submission = submission.sort_values('image_name') \nadias_submission.to_csv('adias_submission.csv', index=False)","7c642cbd":"adias_submission.head()","84e977cf":"  ADIAS kernel - @copyright adias team\n* this notebook is owned by adias team and the team members have the right to edit,change,and customise this kernel.\n\n* the team members may add this kernel to their kaggle profile and remove this header if they wanted to\n","f28d551e":"### Weighted cross entropy loss\n\n\n$$L_{bce} = - \\sum_{n}^{N} \\sum_{k}^{2} \\alpha_{k} \\cdot t_{n,k} \\cdot \\log(y_{n,k}) $$","b1a4d7c4":"![image.png](attachment:image.png)","9944c918":"### Cross entropy loss\n\n\n$$L_{bce} = - \\sum_{n}^{N} \\sum_{k}^{2} t_{n,k} \\cdot \\log(y_{n,k}) = \\sum_{n}^{N} \\cdot l_{bce}$$\n\n$$l_{bce} = - \\sum_{k}^{2} t_{n,k} \\cdot \\log(y_{n,k}) $$","d8645d2a":"## General Info","b744f4ef":"$$L_{focal} = - \\sum_{n}^{N} \\sum_{k}^{2} \\alpha_{k} \\cdot t_{n,k} \\cdot (1-y_{n,k})^{\\gamma} \\cdot \\log(y_{n,k})$$","a549a463":"## Evaluation","f7e74817":"#### target vs. sex","0ed88fba":"# Machine Learning\/DL","889b259a":"# Start..","98025a71":"Introductory information on Melanoma from  [Anshul Sharma's notebook](https:\/\/www.kaggle.com\/anshuls235\/siim-isic-melanoma-analysis-eda-prediction)","8ff38fc7":"## Loading labeled Tensorflow records","61b11885":"![image.png](attachment:image.png)","1a47e800":"# Data visualization and EDA","458ea1d8":"# TPU confugiration","12a8e12f":"### Focal entropy loss","47fdf9d9":"## 2-Decoding data","bb26fcf9":"### Basic statistics","9739d63b":"# <a id='mel'>1. What is Melanoma?<\/a>\n<a href='#toc'><span class=\"label label-info\">Go back to Table of Contents<\/span><\/a>\n## -> Overview\n<img src='https:\/\/www.mayoclinic.org\/-\/media\/kcms\/gbs\/patient-consumer\/images\/2013\/11\/15\/17\/43\/ds00190_-ds00439_im04411_mcdc7_melanomathu_jpg.jpg' style=\"width:500px;height:300px;\">\nMelanoma, the most serious type of skin cancer, develops in the cells (melanocytes) that produce melanin \u2014 the pigment that gives your skin its color. Melanoma can also form in your eyes and, rarely, inside your body, such as in your nose or throat.\n\nThe exact cause of all melanomas isn't clear, but exposure to ultraviolet (UV) radiation from sunlight or tanning lamps and beds increases your risk of developing melanoma. Limiting your exposure to UV radiation can help reduce your risk of melanoma.\n\nThe risk of melanoma seems to be increasing in people under 40, especially women. Knowing the warning signs of skin cancer can help ensure that cancerous changes are detected and treated before the cancer has spread. Melanoma can be treated successfully if it is detected early.\n## -> Symptoms\nMelanomas can develop anywhere on your body. They most often develop in areas that have had exposure to the sun, such as your back, legs, arms and face.\n\nMelanomas can also occur in areas that don't receive much sun exposure, such as the soles of your feet, palms of your hands and fingernail beds. These hidden melanomas are more common in people with darker skin.\n\nThe first melanoma signs and symptoms often are:\n\nA change in an existing mole\nThe development of a new pigmented or unusual-looking growth on your skin\nMelanoma doesn't always begin as a mole. It can also occur on otherwise normal-appearing skin.\n## -> Causes\n<img src='https:\/\/www.mayoclinic.org\/-\/media\/kcms\/gbs\/patient-consumer\/images\/2013\/11\/15\/17\/40\/ds00190_-ds00439_-ds00924_-ds00925_im02400_c7_skincancerthu_jpg.jpg' style=\"width:500px;height:300px;\">\nMelanoma occurs when something goes wrong in the melanin-producing cells (melanocytes) that give color to your skin.\n\nNormally, skin cells develop in a controlled and orderly way \u2014 healthy new cells push older cells toward your skin's surface, where they die and eventually fall off. But when some cells develop DNA damage, new cells may begin to grow out of control and can eventually form a mass of cancerous cells.\n\nJust what damages DNA in skin cells and how this leads to melanoma isn't clear. It's likely that a combination of factors, including environmental and genetic factors, causes melanoma. Still, doctors believe exposure to ultraviolet (UV) radiation from the sun and from tanning lamps and beds is the leading cause of melanoma.\n\nUV light doesn't cause all melanomas, especially those that occur in places on your body that don't receive exposure to sunlight. This indicates that other factors may contribute to your risk of melanoma.","1739df75":"## working with hair augmentation","30a9eeb3":"# Final submission","0fe1a413":"#### target vs. age_approx","ca1870c5":"## 3-Augmenting the data","343ca6cc":"## Train Model\n"}}