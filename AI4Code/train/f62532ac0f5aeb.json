{"cell_type":{"790bf257":"code","aaff2051":"code","25bd4a50":"code","358f6d32":"code","211889a7":"code","c13839e7":"code","221d1fd5":"code","28fb08fb":"code","d4522d1b":"code","f1d3e3d7":"code","c94fdba9":"code","35411d29":"code","c17d0173":"code","4c776cd8":"code","551dbde0":"code","516d52ab":"code","1eb0b80b":"code","8f6c945c":"code","5e9d9197":"code","5d726385":"code","5bd13375":"code","5729ae6c":"code","34f128ab":"code","9a31d433":"code","42dcdc04":"code","377e0db1":"code","47eeed91":"code","281a2cf0":"code","e431a776":"code","c9b9eef5":"code","c4247d9a":"code","d5674bfe":"code","9b476f37":"markdown","4122ee1e":"markdown","b2d79a4c":"markdown","a61220fe":"markdown","706729ba":"markdown","02fff9ff":"markdown","322de07d":"markdown","f9111cc8":"markdown","4b89127e":"markdown","339e54b0":"markdown","d6f8f56d":"markdown","c3703fc9":"markdown"},"source":{"790bf257":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","aaff2051":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold,RepeatedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\n\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMRegressor\n\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\nimport warnings\nwarnings.filterwarnings('ignore')","25bd4a50":"from sklearn.linear_model import Ridge\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import Lasso\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest, chi2, f_regression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis","358f6d32":"from sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest","211889a7":"import seaborn as sns","c13839e7":"name = ['Time','ActivePower', 'AmbientTemperatue', 'BearingShaftTemperature', 'Blade1PitchAngle', 'Blade2PitchAngle', 'Blade3PitchAngle', 'ControlBoxTemperature', 'GearboxBearingTemperature', 'GearboxOilTemperature', 'GeneratorRPM', 'GeneratorWinding1Temperature', 'GeneratorWinding2Temperature', 'HubTemperature', 'MainBoxTemperature', 'NacellePosition', 'ReactivePower', 'RotorRPM', 'TurbineStatus', 'WTG', 'WindDirection']\n\ndf = pd.read_csv('..\/input\/wind-power-forecasting\/Turbine_Data.csv',names = name, parse_dates = True, header = 0)\n\ndf['Time'] = pd.to_datetime(df.index, yearfirst = True,)","221d1fd5":"fiddler1 = pd.read_csv('..\/input\/wind-power-forecasting\/Turbine_Data.csv', parse_dates = True, index_col = 0,header = 0) # here we are making a new copy of the dataset to mess around with.\n\nfiddler1.head()\nfiddler1.tail()","28fb08fb":"fiddler1.drop(columns = ['WTG','ControlBoxTemperature'], inplace = True) #We drop WTG because every value was the same\n\nfiddler1.index = pd.to_datetime(fiddler1.index) # Here we are converting the index to a pd datetime format\n","d4522d1b":"intrap_df = pd.read_csv('..\/input\/wind-power-forecasting\/Turbine_Data.csv', parse_dates = True, index_col = 0,header = 0)\n\nintrap_df.drop(columns = ['WTG','ControlBoxTemperature'], inplace = True) #We drop WTG because every value was the same\n\nintrap_df.index = pd.to_datetime(fiddler1.index) # Here we are converting the index to a pd datetime format\n\n","f1d3e3d7":"mode_df = intrap_df.copy()\n\nforward_fill = intrap_df.copy()","c94fdba9":"fiddler1.info()\nprint('-'*30)\n\nfiddler1.isnull().sum() #we use this to check for missing values","35411d29":"target = ['ActivePower']","c17d0173":"columns = ['AmbientTemperatue' ,'BearingShaftTemperature', 'Blade1PitchAngle', 'Blade2PitchAngle' ,'Blade3PitchAngle','GearboxBearingTemperature', 'GearboxOilTemperature','GeneratorRPM','GeneratorWinding1Temperature','GeneratorWinding2Temperature','HubTemperature','MainBoxTemperature','NacellePosition','ReactivePower','RotorRPM','TurbineStatus', 'WindDirection','WindSpeed']\n\n\nfor n in columns:\n    fiddler1[n].fillna(fiddler1[n].median(),inplace = True)\n    intrap_df[n].fillna(intrap_df[n].mean(),inplace = True)\n    mode_df[n].fillna(mode_df[n].mode(),inplace = True)\n    \n","4c776cd8":"fiddler1.describe(datetime_is_numeric = True) #We can see our data summarized from a statistical perspective with the describe() function","551dbde0":"fiddler1.info()\nprint('-'*30)\n\nfiddler1.isnull().sum() #we use this to check for missing values\n\n\nintrap_df.isnull().sum()\n\nmode_df.info()","516d52ab":"# code to drop the rows where the Active Power is not available.\n\ndf_all = [fiddler1,intrap_df,forward_fill]\n\nfor df in df_all:\n    df.dropna(axis=0,how = 'any',inplace = True)\n    print(df.isnull().sum())\n\n","1eb0b80b":"forward_fill.head()","8f6c945c":"pd.unique(list(fiddler1.index.year)) # here we are checking the years present in our data.","5e9d9197":"fiddler1['Year'] = fiddler1.index.year\nfiddler1['month'] = fiddler1.index.month\nfiddler1['hour'] = fiddler1.index.hour","5d726385":"fiddler1.loc['2018']","5bd13375":"sns.heatmap(fiddler1.corr()) #Using corr() we can find out how the variables are related to each other","5729ae6c":"sns.heatmap(forward_fill.corr())","34f128ab":"column = ['AmbientTemperatue' ,'BearingShaftTemperature', 'Blade1PitchAngle', 'Blade2PitchAngle' ,'Blade3PitchAngle','GearboxBearingTemperature', 'GearboxOilTemperature','GeneratorRPM','GeneratorWinding1Temperature','GeneratorWinding2Temperature','HubTemperature','MainBoxTemperature','NacellePosition','ReactivePower','RotorRPM','TurbineStatus', 'WindDirection']","9a31d433":"x_median = forward_fill[columns]\n\ny_median = forward_fill[target]","42dcdc04":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import GridSearchCV\n\n\n    ","377e0db1":"\n\n#remove the hashtag in the line below for train_test_split()\nx_train,x_test,y_train,y_test = train_test_split(x_median,y_median,test_size=0.3, random_state=1)\n\n#note: this is an alternative to train_test_split\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 )","47eeed91":"x_train","281a2cf0":"from sklearn.preprocessing import MinMaxScaler\n\n\nnorm = MinMaxScaler().fit(x_train)\n\nxtrain_norm = norm.transform(x_train)\n\nxtest_norm = norm.transform(x_test)","e431a776":"xtrain_norm = pd.DataFrame(xtrain_norm)\nxtest_norm = pd.DataFrame(xtest_norm)","c9b9eef5":"from sklearn                        import metrics, svm\nfrom sklearn.linear_model           import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\n\ndef LinearRegressionModel(xtrain,xtest,ytrain,ytest):\n    \n    pca = PCA()\n\n    pca.fit(xtrain)\n\n    pca_train = pca.transform(xtrain)\n    pca_test = pca.transform(xtest)\n\n\n    X_new = SelectKBest(f_regression, k=10).fit_transform(xtrain,ytrain)\n\n    X_new_test = SelectKBest(f_regression, k=10).fit_transform(xtest,ytest) \n    \n    clf = LinearRegression()\n    clf.fit(pca_train, y_train)\n    \n    print(\"LinearRegression\")\n    pred = clf.predict(pca_test)\n\n    pred_train = clf.predict(pca_train)\n    \n    print(\"Test Evaluation: \")\n    print(np.sqrt(mean_squared_error(ytest,pred)))\n    print(r2_score(y_test,pred))\n    print(\"Train Evaluation: \\n\")\n    print(np.sqrt(mean_squared_error(ytrain,pred_train)))\n    print(r2_score(y_train,pred_train))\n    \n    return \n\n\n\n\n","c4247d9a":"x_new = pd.DataFrame(X_new)","d5674bfe":"LinearRegressionModel(xtrain_norm,xtest_norm,y_train,y_test)\n","9b476f37":"# 6) Building a model","4122ee1e":"Using the heatmap function provided by the seaborn library ","b2d79a4c":"# 4) Handling null values","a61220fe":"We can see that after filling all the null values with it's median we were able to get rid of many of the empty columns. Now we have only 2 columns with null values. Every value in the ControlBoxTemperature column is 0 as we can see in the data, therefore we deem it to be redundant and drop it entirely. For the ActivePower we can drop all the rows with null values.","706729ba":"# 5) Visualization","02fff9ff":"# 1) We import all the libraries necessay\n\nWe import pandas for reading and handling the data. Numpy provides us with very efficient ways to perform mathematical operations on arrays as well as easy array creation. Sklearn provides us with the tools that we need to build a model in the later stages of the pipeline. Seaborn and matplotlab are two important and highly used libraries for displaying and plotting graphs","322de07d":"We will now extract the month and the year from the index, and create two new columns.","f9111cc8":"# 4) Performing EDA","4b89127e":"As we can see, we have managed to get rid of all the null values, now let us perform EDA and then build a model. Since all the values are a number we dont need to perform  categorical imputation.","339e54b0":"We can see that there are many missing values which would either need to be filled or removed. We can also infer that most of the columns are floats, so they dont need to be encoded anyhow. However WTG column, is of the object type, so we need to handle that as well.","d6f8f56d":"Here we are normalizing our dataset, so that the difference in units doesn't affect our model.","c3703fc9":"# 2) Reading the data"}}