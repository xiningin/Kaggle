{"cell_type":{"13558621":"code","de1a6358":"code","5412e16c":"code","da71fa29":"code","0e22376c":"code","f413fec3":"code","be01ef4d":"code","f11a6776":"code","42721a0d":"code","39cb3aab":"code","8470721f":"code","db8e5960":"code","a9206b51":"code","6b25e46d":"code","14298200":"code","2f739a69":"code","856a817d":"code","689d6095":"code","6febc8c3":"code","56198952":"code","53c1dc96":"code","13b4540e":"code","af5a2f32":"code","7e80b61f":"code","f7545389":"code","a3381052":"code","db74ddcc":"code","b4c4b509":"code","0602ee4f":"code","7369ab98":"code","bcaa2535":"code","46122e03":"code","bd212732":"code","f7884534":"code","1fca9945":"code","9907da87":"code","a4c0e072":"code","e7b19ceb":"markdown","81c19052":"markdown","2e270d7f":"markdown","fd39064b":"markdown","41f722df":"markdown","5d360455":"markdown","d2414329":"markdown","6cfaec34":"markdown","135cc07e":"markdown","12befece":"markdown","10263c2e":"markdown","75156dda":"markdown","4f44e42c":"markdown","5bea353d":"markdown","08372da2":"markdown","21ae5c8b":"markdown","3074044b":"markdown","c5b11e68":"markdown","86e99c37":"markdown","17e117e0":"markdown","5f0af49a":"markdown","f5fb8d10":"markdown","3beaf90b":"markdown","d6ff8853":"markdown","672e9048":"markdown","a13ff26b":"markdown","c526806d":"markdown","721a1eaa":"markdown","73809914":"markdown"},"source":{"13558621":"import warnings\nwarnings.filterwarnings('ignore')","de1a6358":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns","5412e16c":"sns.set(style='whitegrid')","da71fa29":"data = pd.read_csv('..\/input\/heart.csv')\nprint(F\"Null values? {data.isnull().values.any()}\")","0e22376c":"data.head()","f413fec3":"plt.figure(figsize=(7, 5))\ncount_per_class = [len(data[data['target'] == 0]),len(data[data['target'] == 1])]\nlabels = [0, 1]\ncolors = ['yellowgreen', 'lightblue']\nexplode = (0.05, 0.1)\nplt.pie(count_per_class, explode=explode, labels=labels, \n        colors=colors,autopct='%4.2f%%',shadow=True, startangle=45)\nplt.title('Examples per class')\nplt.axis('equal')\nplt.show()","be01ef4d":"plt.figure(figsize=(7, 5))\ncount_per_class = [len(data[data['sex'] == 0]),len(data[data['sex'] == 1])]\nlabels = ['Female', 'Male']\ncolors = ['lightgreen', 'gold']\nexplode = (0.05, 0.1)\nplt.pie(count_per_class, explode=explode, labels=labels, \n        colors=colors,autopct='%4.2f%%',shadow=True, startangle=70)\nplt.title('Gender shares')\nplt.axis('equal')\nplt.show()","f11a6776":"plt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nsns.kdeplot(data['age'], data['sex'], shade=True)\nplt.title('Age-sex density estimate')\nplt.subplot(1, 2, 2)\nsns.distplot(data['age'])\nplt.title('Age distribution')\nplt.show()","42721a0d":"plt.figure(figsize=(8, 6))\nsns.distplot(data[data.target == 0]['chol'], label='without heart disease')\nsns.distplot(data[data.target == 1]['chol'], label='with heart disease')\nplt.xlabel('serum cholestoral in mg\/dl')\nplt.title('serum cholestoral per class')\nplt.legend()\nplt.show()","39cb3aab":"plt.figure(figsize=(8, 6))\nsns.distplot(data[data.target == 0]['thalach'], label='without heart disease')\nsns.distplot(data[data.target == 1]['thalach'], label='with heart disease')\nplt.title('maximum heart rate achieved per class')\nplt.xlabel('maximum heart rate achieved')\nplt.legend()\nplt.show()","8470721f":"plt.figure(figsize=(12,8))\nsns.heatmap(data.corr(), annot=True, linewidths=2, cmap=\"YlGnBu\")\nplt.show()","db8e5960":"data.groupby('target')['trestbps'].describe()","a9206b51":"ax2 = sns.jointplot(\"target\", \"trestbps\", data=data, kind=\"reg\", color='r')\nax2.set_axis_labels('target','resting blood pressure')\nplt.show()","6b25e46d":"X = data.values[:, :13]\ny = data.values[:, 13]","14298200":"import eli5\nfrom sklearn.linear_model import LogisticRegression\nfrom eli5.sklearn import PermutationImportance\n\nlogistic_regression = LogisticRegression(penalty='l1')\nlogistic_regression.fit(X, y)\nperm_imp = PermutationImportance(logistic_regression, random_state=42).fit(X, y)\neli5.show_weights(perm_imp, feature_names = data.columns.tolist()[:13])\n","2f739a69":"from sklearn.model_selection import GridSearchCV, KFold\nfrom sklearn.metrics import accuracy_score\n\ndef nested_kfold_cv(model, param_grid, X, y, outer_metric=accuracy_score,\n                    scoring='accuracy' , k1=10, k2=3, verbose = 1, n_jobs=3, shuffle=True):\n    scores = []\n    estimators = []\n    kf = KFold(n_splits=k1, shuffle=shuffle)\n    for train_index, test_index in kf.split(X):\n        X_train = X[train_index]\n        X_test = X[test_index]\n        y_train = y[train_index]\n        y_test = y[test_index]\n        grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=k2,\n                                   verbose=verbose, n_jobs=n_jobs, scoring=scoring)\n        grid_search.fit(X=X_train, y=y_train)\n        estimator = grid_search.best_estimator_\n        estimators.append(estimator)\n        estimator.fit(X_train, y_train)\n        scores.append(outer_metric(estimator.predict(X_test), y_test))\n    return estimators, scores","856a817d":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import recall_score, confusion_matrix","689d6095":"tree_model = AdaBoostClassifier(\n    base_estimator=DecisionTreeClassifier(max_depth=1),\n    random_state=42\n)","6febc8c3":"tree_params = {\n    'n_estimators': [25, 50, 75]\n}\nestimators, tree_scores = nested_kfold_cv(tree_model, tree_params, X, y, outer_metric=recall_score,\n                                     scoring='f1' , k1=10, k2=5, verbose = 0, n_jobs=4, shuffle=True)","56198952":"print(f\"Average recall: {np.mean(tree_scores)}\")","53c1dc96":"from sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.preprocessing import Normalizer, StandardScaler\nfrom sklearn.svm import SVC","13b4540e":"svm_model = Pipeline(steps=[\n    ('standard_scaler', StandardScaler()),\n    ('feature_selection', SelectKBest(f_classif)), # params: k\n    ('svm', SVC(kernel='rbf', random_state=42)) # params: gamma, C\n])","af5a2f32":"svm_grid = {\n    'feature_selection__k': [10, 12, 13],\n    'svm__C': [3, 5, 10, 15, 20, 25, 30, 35],\n    'svm__gamma': [0.0005, 0.001, 0.005, 0.01, 0.05, 0.1],\n    \n}\nestimators, svm_scores = nested_kfold_cv(svm_model, svm_grid, X, y, outer_metric=recall_score,\n                                     scoring='f1' , k1=10, k2=5, verbose = 0, n_jobs=4, shuffle=True)","7e80b61f":"print(f\"Average recall: {np.mean(svm_scores)}\")","f7545389":"from sklearn.linear_model import LogisticRegression","a3381052":"log_model = Pipeline(steps=[\n    ('feature_selection', SelectKBest(f_classif)), # params: k\n    ('log', LogisticRegression()) # params:  C\n])","db74ddcc":"log_grid = {\n    'log__C': [0.01, 0.1, 0.5, 1, 3, 5],\n    'feature_selection__k': [5, 9, 10, 12, 13],\n}\nestimators, lr_scores = nested_kfold_cv(log_model, log_grid, X, y, outer_metric=recall_score,\n                                     scoring='f1' , k1=10, k2=5, verbose = 0, n_jobs=4, shuffle=True)","b4c4b509":"print(f\"Average recall: {np.mean(lr_scores)}\")","0602ee4f":"from sklearn.neighbors import KNeighborsClassifier","7369ab98":"knn_model = Pipeline(steps=[\n    ('standard_scaler', StandardScaler()),\n    ('knn', KNeighborsClassifier(weights='distance')) # params: n_neighbors\n])","bcaa2535":"knn_grid = {\n    'knn__n_neighbors': [3, 5, 7, 10, 12, 15, 17, 20],\n}\nestimators, knn_scores = nested_kfold_cv(knn_model, knn_grid, X, y, outer_metric=recall_score,\n                                     scoring='f1' , k1=10, k2=5, verbose = 0, n_jobs=4, shuffle=True)","46122e03":"print(f\"Average recall: {np.mean(knn_scores)}\")","bd212732":"from sklearn.neural_network import MLPClassifier","f7884534":"nn_model = Pipeline(steps=[\n    ('standard_scaler', StandardScaler()),\n    ('nn', MLPClassifier(max_iter=400)) # params:\n])","1fca9945":"nn_grid = {\n    'nn__solver': ['adam', 'lbfgs']\n}\nestimators, nn_scores = nested_kfold_cv(nn_model, nn_grid, X, y, outer_metric=recall_score,\n                                     scoring='f1' , k1=10, k2=5, verbose = 0, n_jobs=4, shuffle=True)","9907da87":"print(f\"Average recall: {np.mean(nn_scores)}\")","a4c0e072":"results = pd.DataFrame({'KNN': knn_scores, 'Logistic regression': lr_scores, 'SVC': svm_scores, 'AdaBoost': tree_scores, 'Neural network': nn_scores})\nresults.boxplot(figsize=(8, 6))","e7b19ceb":"## **5. Classification**","81c19052":"## **0. Before we begin**","2e270d7f":"### Maximum heart rate achieved per class distribution","fd39064b":"Nested cross-validation is used to train a model in which hyperparameters also need to be optimized. I've used it to fight off overly-optimistic scores.<br\/>\nMore about nested cross-validation:<br\/>\nhttps:\/\/www.elderresearch.com\/blog\/nested-cross-validation <br\/>\nhttps:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_nested_cross_validation_iris.html","41f722df":"**Q:** *Why Recall?* <br\/>\n**A:** Our classifier should be sensitive to false negatives. For this dataset, false negative is a person that has heart disease but our classifier decided that the person does not have any heart problems. In other words, classifier said that the ill person is healthy. On the other side, false positive is a person that does not have any heart diseases and our classifier decided that person is ill. In that case, the person will run more tests and conclude it does not have any heart problems.","5d360455":"**Model interpretation:** We can see that the number of major vessels colored by fluoroscopy and chest pain type are the most important features for correct classification.","d2414329":"### Number of examples per class","6cfaec34":"### Resting blood pressure per class","135cc07e":"### Gender shares in dataset","12befece":"### Serum cholestoral per class distribution","10263c2e":"## **1. Data exploration**","75156dda":"### Features heatmap","4f44e42c":"### **5.3. Logistic Regression** ","5bea353d":"### Age-sex distribution","08372da2":"## **6. Classification results overview**","21ae5c8b":"### **5.5. Neural network**","3074044b":"## **3. Appropriate metric? Recall!**","c5b11e68":"# Heart Disease Dataset","86e99c37":"### **5.4. KNN**","17e117e0":"### Dataset sample","5f0af49a":"## **2. Feature importances (for L1-regularized Logistic Regression)**","f5fb8d10":"**Attribute Information:**\n> 1. **age** - age\n> 2. **sex** - (1 = male; 0 = female)\n> 3. **cp** - chest pain type (4 values) \n> 4. **trestbps** - resting blood pressure \n> 5. **chol** - serum cholestoral in mg\/dl \n> 6. **fbs** - fasting blood sugar > 120 mg\/dl\n> 7. **restecg** - resting electrocardiographic results (values 0,1,2)\n> 8. **thalach** - maximum heart rate achieved \n> 9. **exang** - exercise induced angina \n> 10. **oldpeak** -  ST depression induced by exercise relative to rest \n> 11. **slope** - the slope of the peak exercise ST segment \n> 12. **ca** -  number of major vessels (0-3) colored by flourosopy \n> 13. **thal** - 3 = normal; 6 = fixed defect; 7 = reversable defect","3beaf90b":"### Kernel goals:\n\n* Data exploration\n* Find important features for L1-regularized Logistic regression\n* Propose correct scoring metrics for this dataset\n* Fight off overly-optimistic score\n* Compare results of various classifiers","d6ff8853":"Please  **comment** or **upvote** this kernel.","672e9048":"### **5.1. AdaBoost**","a13ff26b":"Classes are well balanced!","c526806d":"### **5.2. SVM**","721a1eaa":"## **4. Nested cross-validation (way to fight off overly-optimistic score)**","73809914":"\n### _**Thanks for reading!**_"}}