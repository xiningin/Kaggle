{"cell_type":{"7073f7ec":"code","e46561e4":"code","781ea185":"code","2dfaa498":"code","16b6315f":"code","ccd7bfd3":"code","700935a1":"code","4b91f9a4":"code","be056522":"code","bca192b8":"code","730d3f1e":"code","6f97548d":"code","35b0a7c6":"code","e67db1ff":"code","4b385ffb":"code","e575e037":"code","cf1b6452":"code","61a27bd8":"code","8a0544c7":"code","d75f5a86":"code","79a1a2de":"code","5f2aee6f":"code","526a2d66":"code","ffaa94b1":"code","ac80eb13":"code","e65daf63":"code","56d3d76f":"code","c5f69457":"code","c5ae87b9":"code","e46cdee7":"code","96e30b2e":"code","1e6407f9":"code","468b916d":"code","a2d212e8":"code","b1a9e1ed":"code","02958e2e":"code","4c821db0":"code","91ef54c0":"code","4704fb79":"code","a937dea2":"code","4932a549":"code","dbafbafd":"code","6aedc942":"code","d51d669b":"code","b05d2f90":"code","9dab9813":"code","5936b956":"code","92c7b0e1":"code","bfa7fbe3":"code","13f48929":"code","83a46e41":"markdown","79bc3538":"markdown","788b20bc":"markdown","97926caa":"markdown","b643107d":"markdown","a953403b":"markdown","99b3b938":"markdown","03e177c5":"markdown","bbd160fe":"markdown","09ae1bb6":"markdown","9b803cbf":"markdown","b0d4cf81":"markdown","f7ed3480":"markdown","5ee71800":"markdown","6d9d0cc7":"markdown","2f3fc0d0":"markdown"},"source":{"7073f7ec":"#import the libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.utils import resample\n\n#sklearn package \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn. preprocessing import LabelEncoder,StandardScaler\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import ComplementNB,MultinomialNB,GaussianNB \n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n#model evaluation\nfrom sklearn.metrics import accuracy_score,classification_report, confusion_matrix,recall_score,precision_score,f1_score\n","e46561e4":"#read the dataset\ndata = pd.read_json('..\/input\/news-category-dataset\/News_Category_Dataset_v2.json',lines=True) # lines for avoid the trailing error\ncolumn = data.columns\ncolumn","781ea185":"data.drop(['link','date'],axis=1,inplace = True)\ndata.head(4)","2dfaa498":"data.describe()","16b6315f":"data.info()","ccd7bfd3":"print(\"The shape of the dataset-------->\",data.shape)\nprint(\"The number of null values ------>\")\nprint(data.isnull().sum())\ncolumn = data.columns\nprint(\"The column present there-------->\",column)","700935a1":"print(\"The total number category present here------------->\",data['category'].nunique())\ncategory=data['category'].value_counts()\nprint(category)","4b91f9a4":"plt.figure(figsize=(25,8))\nsns.barplot(x=category.index,y=category.values)\nplt.title(\"The distribution of categories\")\nplt.xlabel(\"Category\")\nplt.ylabel(\"The number of samples\")\n\nplt.xticks(rotation=60,fontsize = 14)\nplt.show()","be056522":"#pie chart \nplt.figure(figsize=(20,20))\nplt.pie(category.values, autopct=\"%1.1f%%\", labels=category.index)\nplt.show()\nplt.savefig(r\".\/category_pie.png\")","bca192b8":"plt.figure(figsize=(25,13))\nsns.barplot(y=category.index,x=category.values)\nplt.title(\"The distribution of categories\")\nplt.xlabel(\"Category\")\nplt.ylabel(\"The number of samples\")\n\nplt.yticks(rotation=0,fontsize = 16)\nplt.show()\nplt.savefig(r\".\/category_bar.png\")","730d3f1e":"#delete the dublicate values\ndata.duplicated().sum() # count the total duplicate samples","6f97548d":"data.drop_duplicates(keep='last',inplace=True)","35b0a7c6":"#there can be dublicate of author names so check for the dublicate headline and short discription\ndata.duplicated(subset=['headline', 'short_description']).sum()","e67db1ff":"data.drop_duplicates(subset=['headline', 'short_description'],inplace=True,keep='last')","4b385ffb":"print(\"THe length of the datset after dublicate deletion------>\",data.shape)","e575e037":"data.isnull().sum()","cf1b6452":"# there is no null value instead of null they are blank so we need to check for the blank placess and delete that\ndata[data['headline'] == '']","61a27bd8":"# drop the blank values\nheadline_blank = data['headline'] == ''\ndata = data[~headline_blank]\nprint(\"THe length of the datset ------>\",data.shape)","8a0544c7":"#drop the blank short describtion column\ndescription_blank = data['short_description']==''\nprint(\"the lenth of the blank description samples----->\",len(data[description_blank]))\ndata = data[~description_blank]\nprint(\"THe length of the datset ---------------------->\",data.shape)","d75f5a86":"#drop the null author samples\nauthor_blank = data['authors']==''\nprint(\"the lenth of the blank auhtor samples---------->\",len(data[author_blank]))\ndata = data[~author_blank]\nprint(\"THe length of the datset ---------------------->\",data.shape)","79a1a2de":"data.head(5)","5f2aee6f":"#author \n#auhtor plot\nauhtor_count = data['authors'].value_counts()\n\nplt.figure(figsize=(25,18))\nsns.barplot(y=auhtor_count[:25].index,x=auhtor_count[:25].values)\nplt.title(\"The distribution of authors\")\nplt.xlabel(\"Author Name\")\nplt.ylabel(\"The number of samples\")\n\nplt.yticks(rotation=0,fontsize = 18)\nplt.show()\nplt.savefig(r\".\/author_bar.png\")","526a2d66":"category = data['category'].value_counts()\nlist(category.index)","ffaa94b1":"cateo_keep = (data['category'] == 'POLITICS') | (data['category'] == 'WELLNESS' )| (data['category'] == 'ENTERTAINMENT') | (data['category'] == 'TRAVEL') | \\\n            (data['category'] == 'STYLE & BEAUTY') | (data['category'] == 'PARENTING' )| (data['category'] == 'HEALTHY LIVING') | (data['category'] == 'QUEER VOICES') | \\\n              (data['category'] == 'FOOD & DRINK') | (data['category'] == 'BUSINESS' )| (data['category'] == 'COMEDY') | (data['category'] == 'PARENTS') | (data['category'] == 'SPORTS') | (data['category'] == 'HOME & LIVING' )| (data['category'] == 'BLACK VOICES')\ndata = data[cateo_keep]\n\n","ac80eb13":"category = data['category'].value_counts()\ncategory","e65daf63":"data_1 = data[data['category'] == 'POLITICS']\ndata_1 = resample(data_1, replace=False, n_samples=3000, random_state=123)\ndata_2 = data[data['category'] == 'WELLNESS']\ndata_2 = resample(data_2, replace=False, n_samples=3000, random_state=123)\ndata_3 = data[data['category'] == 'ENTERTAINMENT']\ndata_3 = resample(data_3, replace=False, n_samples=3000, random_state=123)\ndata_4 = data[data['category'] == 'TRAVEL']\ndata_4 = resample(data_4, replace=False, n_samples=3000, random_state=123)\ndata_5 = data[data['category'] == 'STYLE & BEAUTY']\ndata_5 = resample(data_5, replace=False, n_samples=3000, random_state=123)\ndata_6 = data[data['category'] == 'PARENTING']\ndata_6 = resample(data_6, replace=False, n_samples=3000, random_state=123)\ndata_7 = data[data['category'] == 'HEALTHY LIVING']\ndata_7 = resample(data_7, replace=False, n_samples=3000, random_state=123)\ndata_8 = data[data['category'] == 'QUEER VOICES']\ndata_8 = resample(data_8, replace=False, n_samples=3000, random_state=123)\ndata_9 = data[data['category'] == 'FOOD & DRINK']\ndata_9 = resample(data_9, replace=False, n_samples=3000, random_state=123)\ndata_10 = data[data['category'] == 'BUSINESS']\ndata_10 = resample(data_10, replace=False, n_samples=3000, random_state=123)\ndata_11 = data[data['category'] == 'COMEDY']\ndata_11 = resample(data_11, replace=False, n_samples=3000, random_state=123)\ndata_12= data[data['category'] == 'PARENTS']\ndata_12 = resample(data_12, replace=False, n_samples=3000, random_state=123)\ndata_13= data[data['category'] == 'SPORTS']\ndata_13 = resample(data_13, replace=False, n_samples=3000, random_state=123)\ndata_14 = data[data['category'] == 'HOME & LIVING']\ndata_14 = resample(data_14, replace=False, n_samples=3000, random_state=123)\ndata_15 = data[data['category'] == 'BLACK VOICES']\ndata_15 = resample(data_15, replace=False, n_samples=3000, random_state=123)\n\ncato_list = [data_1 , data_2 , data_3 , data_4 ,data_5 , data_6 , data_7, data_8 , data_9 , data_10, data_11 , data_12 , data_13, data_14 ,data_15]\n\ndata = pd.concat(cato_list)\ndata['category'].value_counts()","56d3d76f":"category = data['category'].value_counts()\nplt.figure(figsize=(25,13))\nsns.barplot(y=category.index,x=category.values)\nplt.title(\"The distribution of categories\")\nplt.xlabel(\"Category\")\nplt.ylabel(\"The number of samples\")\n\nplt.yticks(rotation=0,fontsize = 16)\nplt.show()\nplt.savefig(r\".\/category_bar.png\")","c5f69457":"data['text'] = data['headline']+'-'+data['short_description']","c5ae87b9":"data.head(4)","e46cdee7":"#drop the other columns\ndata.drop(['authors','headline','short_description'],axis=1,inplace=True)","96e30b2e":"print(\"The lenth of the datset-------------------->\",data.shape)\ndata.head(4)","1e6407f9":"from sklearn.utils import shuffle\ndata = shuffle(data)\ndata.reset_index(inplace=True, drop=True) \ndata.head(4)","468b916d":"\"\"\"corpus=[]\nfor i in range(100000):\n    text = data.iloc[i,1]\n    \n    text = text.lower()\n    text = re.sub('[^a-z0-9]',' ',text)\n    text = text.split()\n    \n    s = PorterStemmer()\n    text = [s.stem(word) for word in text if not word in set(stopwords.words('english')) ]\n    text = ' '.join(text)\n    corpus.append(text)\n    \n    if i%1000==0:\n        print(i,end='->')\"\"\"\n","a2d212e8":"\"\"\"corpus = pd.read_csv('..\/input\/corpus\/corpus.csv')\ncorpus\"\"\"","b1a9e1ed":"#train and test split\nX = data['text']\n#label encoding the target\nlabel = LabelEncoder()\ny = label.fit_transform(data['category'])\n\n#split the train and test dataset\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.1,random_state=2)","02958e2e":"print(\"The X_train shape----->\",X_train.shape)\nprint('The X_text shape------>',X_test.shape)\nprint(\"THe y_train shape----->\",y_train.shape)\nprint(\"The y_test shape------>\",y_test.shape)","4c821db0":"#ifidf vectorizer\n\nvecto =  TfidfVectorizer(stop_words='english',max_df = 0.99,min_df=10,\n                                   ngram_range=(1, 2),lowercase=True, max_features=5000)\nvecto = vecto.fit(X_train)\n\nX_train = vecto.transform(X_train).toarray()\nX_test = vecto.transform(X_test).toarray()\nX_train.shape","91ef54c0":"print(vecto.get_feature_names())","4704fb79":"tfidf_df = pd.DataFrame(X_train,columns = vecto.get_feature_names())\ntfidf_df.head(4)","a937dea2":"print(\"The X_train shape----->\",X_train.shape)\nprint('The X_text shape------>',X_test.shape)\nprint(\"THe y_train shape----->\",y_train.shape)\nprint(\"The y_test shape------>\",y_test.shape)","4932a549":"\ndef classifier_scores(y_train,y_test, pred_train, pred_test):\n    \n    print()\n    print(\"Train data accuracy score: \", accuracy_score(y_train,pred_train))    \n    print(\"Test data accuracy score: \", accuracy_score(y_test,pred_test))\n    print()\n    print(\"Recall score on train data: \", recall_score(y_train,pred_train, average='macro'))\n    print(\"Recall score on test data: \",recall_score( y_test,pred_test, average='macro'))\n    print()\n    \n    print(\"Precision score on train data: \",precision_score(y_train,pred_train, average='macro'))\n    print(\"Precision score on test data: \",precision_score(y_test,pred_test, average='macro'))\n    print()\n    print(\"F1 score on train data: \",f1_score(y_train,pred_train, average='macro'))\n    print(\"F1 score on test data: \",f1_score(y_test,pred_test, average='macro'))","dbafbafd":"print(\"Multinamial NB----------------------------------->\")\nmultinb = MultinomialNB()\nmultinb.fit(X_train , y_train)\n\ny_train_pred = multinb.predict(X_train)\ny_test_pred = multinb.predict(X_test)\nclassifier_scores(y_train,y_test,y_train_pred,y_test_pred)","6aedc942":"print(\"Compiment NB----------------------------------->\")\ncompnb = ComplementNB(alpha=1.0)\ncompnb.fit(X_train , y_train)\n\ny_train_pred = compnb.predict(X_train)\ny_test_pred = compnb.predict(X_test)\nclassifier_scores(y_train,y_test,y_train_pred,y_test_pred)","d51d669b":"#model training\ngaussion_NB = GaussianNB()\ngaussion_NB.fit(X_train , y_train)\n\ny_train_pred = gaussion_NB.predict(X_train)\ny_test_pred = gaussion_NB.predict(X_test)\nclassifier_scores(y_train,y_test,y_train_pred,y_test_pred)","b05d2f90":"#logistic Regresssion\n\nlog_reg = LogisticRegression()\n\nlog_reg.fit(X_train , y_train)\n\ny_train_pred = log_reg.predict(X_train)\ny_test_pred = log_reg.predict(X_test)\nclassifier_scores(y_train,y_test,y_train_pred,y_test_pred)","9dab9813":"#logistric regression more accuracy\nlog_reg_hyper = LogisticRegression(solver='liblinear',n_jobs=-1,penalty='l2',)\nlog_reg_hyper.fit(X_train , y_train)\n\ny_train_pred = log_reg_hyper.predict(X_train)\ny_test_pred = log_reg_hyper.predict(X_test)\nclassifier_scores(y_train,y_test,y_train_pred,y_test_pred)","5936b956":"n_com = [500,1000,2000,3000]\ndef models_prepare():\n    model = {}\n    for n in n_com:\n        s = [('svd',TruncatedSVD(n_components = n)),('logistric',LogisticRegression())]\n        model[str(n)] = Pipeline(steps = s)\n    return model\nmodels = models_prepare()\nmodels","92c7b0e1":"\nfor name,model in models.items():\n    model.fit(X_train,y_train)\n    \n    y_pred_train = model.predict(X_train)\n    y_pred_test = model.predict(X_test)\n    print(\"The Logistric Regression Trained with svd n_components {} \".format(name))\n    \n    classifier_scores(y_train,y_test,y_pred_train,y_pred_test)","bfa7fbe3":"import pickle\n# save the model to disk\n\nprint(\"Loding your model..................\")\nfile_path = '.\/logistricRegression_text_classi.sav'\npickle.dump(log_reg, open(file_path, 'wb'))\n \nprint('Model saved......')\n \n","13f48929":"# load the model from disk\nloaded_model = pickle.load(open(file_path, 'rb'))\nresult = loaded_model.score(X_test, y_test)\nprint(result)","83a46e41":"# Column Combinning","79bc3538":"# save the best model for later use","788b20bc":"# Handling null values","97926caa":"# Model training","b643107d":"\nwe can drop the Style, Education, College and Environment they are having very less number of sample, which may lead to less accuracy and f1 score.","a953403b":"## Multinomial Naive Bayes","99b3b938":"# Balance the category data","03e177c5":"\n# Tokazitation and Count Vectorization","bbd160fe":"There are unqual number of sample in each category, so we can drop some category and make it balanced","09ae1bb6":"### Model training with SVD","9b803cbf":"# test cleaning","b0d4cf81":"# handling Dublicate and null values","f7ed3480":"## logistic Regresssion","5ee71800":"## Complement Naive Bayes","6d9d0cc7":"## Gaussian Naive Bayes","2f3fc0d0":"# Category walkthrough"}}