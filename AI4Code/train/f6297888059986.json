{"cell_type":{"e94c3a59":"code","db88851d":"code","04df9693":"code","66f61da8":"code","503a2d1c":"code","a0e07a27":"code","18716ee6":"code","69e5975a":"code","551b7121":"code","ae12dbf6":"code","ab5a44b8":"code","d137f2ef":"code","8ceaf28c":"code","69a8621f":"code","110107fc":"code","b911146a":"code","8a70603c":"code","9e0feda1":"code","9b611280":"code","7911f0b2":"code","d94629e7":"code","996a3d7f":"code","d4fc3bb1":"code","a9638547":"code","6b2bc80f":"code","cb83538a":"code","fcae5d5a":"code","e7546c5a":"code","a207a966":"code","56622881":"markdown","050d0271":"markdown","e8cd05f5":"markdown","c70b47e8":"markdown","565d1a2c":"markdown","d125c81e":"markdown","d57fa4b5":"markdown"},"source":{"e94c3a59":"import itertools\nfrom time import sleep\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gc\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm_notebook\nfrom sklearn.model_selection import train_test_split\npd.set_option('display.max_rows', 600)\npd.set_option('display.max_columns', 50)\nsns.set(rc={'figure.figsize':(20, 10)})\nimport xgboost as xgb","db88851d":"\n\nitem_categories = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\nitems = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/items.csv')\nshops = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/shops.csv')\n\ntrain = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\ntest = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv')\n\ngroupby_cols = ['date_block_num', 'shop_id', 'item_id']","04df9693":"sns.set_context(\"talk\", font_scale=1.4)\nsales_month = pd.DataFrame(train.groupby(['date_block_num']).sum().item_cnt_day).reset_index()\nsales_month.columns = ['date_block_num', 'sum_items_sold']\nsns.barplot(x ='date_block_num', y='sum_items_sold', \n            data=sales_month.reset_index());\nplt.plot(sales_month.sum_items_sold)\nplt.title('Distribution of the sum of sales per month')\ndel sales_month\n","66f61da8":"comb_shop_item = pd.DataFrame(train[['date_block_num', 'shop_id', \n                                     'item_id']].drop_duplicates().groupby('date_block_num').size()).reset_index()\ncomb_shop_item.columns = ['date_block_num', 'item-shop_comb']\nsns.barplot(x ='date_block_num', y='item-shop_comb', data=comb_shop_item);\nplt.plot(comb_shop_item['item-shop_comb']);\nplt.title('Number of combinations shop-it with sales per month')\ndel comb_shop_item","503a2d1c":"sns.set_context(\"talk\", font_scale=1)\nsales_month_shop_id = pd.DataFrame(train.groupby(['shop_id']).sum().item_cnt_day).reset_index()\nsales_month_shop_id.columns = ['shop_id', 'sum_sales']\nsns.barplot(x ='shop_id', y='sum_sales', data=sales_month_shop_id, palette='Paired')\nplt.title('Distribution of sales per shop');\ndel sales_month_shop_id","a0e07a27":"sns.set_context(\"talk\", font_scale=1.4)\nsales_item_id = pd.DataFrame(train.groupby(['item_id']).sum().item_cnt_day)\nplt.xlabel('item id')\nplt.ylabel('sales')\nplt.plot(sales_item_id);","18716ee6":"sns.set_context(\"talk\", font_scale=0.8)\nsales_item_cat = train.merge(items, how='left', on='item_id').groupby('item_category_id').item_cnt_day.sum()\nsns.barplot(x ='item_category_id', y='item_cnt_day',\n            data=sales_item_cat.reset_index(), \n            palette='Paired'\n           );\ndel sales_item_cat","69e5975a":"train = train[train.item_price < 100000]\ntrain = train[train.item_cnt_day < 1001]\n\nmedian = train[(train.shop_id == 32) & (train.item_id == 2973) & (train.date_block_num == 4) & (\n            train.item_price > 0)].item_price.median()\ntrain.loc[train.item_price < 0, 'item_price'] = median","551b7121":"train.loc[train.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\ntrain.loc[train.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\ntrain.loc[train.shop_id == 10, 'shop_id'] = 11\ntest.loc[test.shop_id == 10, 'shop_id'] = 11","ae12dbf6":"test['date_block_num'] = 34","ab5a44b8":"category = items[['item_id', 'item_category_id']].drop_duplicates()\ncategory.set_index(['item_id'], inplace=True)\ncategory = category.item_category_id\ntrain['category'] = train.item_id.map(category)","d137f2ef":"item_categories['meta_category'] = item_categories.item_category_name.apply(lambda x: x.split(' ')[0])\nitem_categories['meta_category'] = pd.Categorical(item_categories.meta_category).codes\nitem_categories.set_index(['item_category_id'], inplace=True)\nmeta_category = item_categories.meta_category\ntrain['meta_category'] = train.category.map(meta_category)","8ceaf28c":"shops['city'] = shops.shop_name.apply(lambda x: str.replace(x, '!', '')).apply(lambda x: x.split(' ')[0])\nshops['city'] = pd.Categorical(shops['city']).codes\ncity = shops.city\ntrain['city'] = train.shop_id.map(city)","69a8621f":"year = pd.concat([train.date_block_num, train.date.apply(lambda x: int(x.split('.')[2]))], axis=1).drop_duplicates()\nyear.set_index(['date_block_num'], inplace=True)\nyear = year.date.append(pd.Series([2015], index=[34]))","110107fc":"month = pd.concat([train.date_block_num, train.date.apply(lambda x: int(x.split('.')[1]))], axis=1).drop_duplicates()\nmonth.set_index(['date_block_num'], inplace=True)\nmonth = month.date.append(pd.Series([11], index=[34]))","b911146a":"all_shops_items = []\n\nfor block_num in train['date_block_num'].unique():\n    unique_shops = train[train['date_block_num'] == block_num]['shop_id'].unique()\n    unique_items = train[train['date_block_num'] == block_num]['item_id'].unique()\n    all_shops_items.append(np.array(list(itertools.product([block_num], unique_shops, unique_items)), dtype='int32'))\n\ndf = pd.DataFrame(np.vstack(all_shops_items), columns=groupby_cols, dtype='int32')\ndf = df.append(test, sort=True)","8a70603c":"df['ID'] = df.ID.fillna(-1).astype('int32')\ndf['year'] = df.date_block_num.map(year)\ndf['month'] = df.date_block_num.map(month)\ndf['category'] = df.item_id.map(category)\ndf['meta_category'] = df.category.map(meta_category)\ndf['city'] = df.shop_id.map(city)\ntrain['category'] = train.item_id.map(category)","9e0feda1":"\ngb = train.groupby(by=groupby_cols, as_index=False).agg({'item_cnt_day': ['sum']})\ngb.columns = [val[0] if val[-1] == '' else '_'.join(val) for val in gb.columns.values]\ngb.rename(columns={'item_cnt_day_sum': 'target'}, inplace=True)\ndf = pd.merge(df, gb, how='left', on=groupby_cols)\n\ngb = train.groupby(by=['date_block_num', 'item_id'], as_index=False).agg({'item_cnt_day': ['sum']})\ngb.columns = [val[0] if val[-1] == '' else '_'.join(val) for val in gb.columns.values]\ngb.rename(columns={'item_cnt_day_sum': 'target_item'}, inplace=True)\ndf = pd.merge(df, gb, how='left', on=['date_block_num', 'item_id'])\n\ngb = train.groupby(by=['date_block_num', 'shop_id'], as_index=False).agg({'item_cnt_day': ['sum']})\ngb.columns = [val[0] if val[-1] == '' else '_'.join(val) for val in gb.columns.values]\ngb.rename(columns={'item_cnt_day_sum': 'target_shop'}, inplace=True)\ndf = pd.merge(df, gb, how='left', on=['date_block_num', 'shop_id'])\n\ngb = train.groupby(by=['date_block_num', 'category'], as_index=False).agg({'item_cnt_day': ['sum']})\ngb.columns = [val[0] if val[-1] == '' else '_'.join(val) for val in gb.columns.values]\ngb.rename(columns={'item_cnt_day_sum': 'target_category'}, inplace=True)\ndf = pd.merge(df, gb, how='left', on=['date_block_num', 'category'])\n\ngb = train.groupby(by=['date_block_num', 'item_id'], as_index=False).agg({'item_price': ['mean', 'max']})\ngb.columns = [val[0] if val[-1] == '' else '_'.join(val) for val in gb.columns.values]\ngb.rename(columns={'item_price_mean': 'target_price_mean', 'item_price_max': 'target_price_max'}, inplace=True)\ndf = pd.merge(df, gb, how='left', on=['date_block_num', 'item_id'])","9b611280":"df['target_price_mean'] = np.minimum(df['target_price_mean'], df['target_price_mean'].quantile(0.99))\ndf['target_price_max'] = np.minimum(df['target_price_max'], df['target_price_max'].quantile(0.99))\n\ndf.fillna(0, inplace=True)\ndf['target'] = df['target'].clip(0, 20)\ndf['target_zero'] = (df['target'] > 0).astype('int32')","7911f0b2":"%%time\n\nfor enc_cols in [['shop_id', 'category'], ['shop_id', 'item_id'], ['shop_id'], ['item_id']]:\n\n    col = '_'.join(['enc', *enc_cols])\n    col2 = '_'.join(['enc_max', *enc_cols])\n    df[col] = np.nan\n    df[col2] = np.nan\n\n    for d in tqdm_notebook(df.date_block_num.unique()):\n        f1 = df.date_block_num < d\n        f2 = df.date_block_num == d\n\n        gb = df.loc[f1].groupby(enc_cols)[['target']].mean().reset_index()\n        enc = df.loc[f2][enc_cols].merge(gb, on=enc_cols, how='left')[['target']].copy()\n        enc.set_index(df.loc[f2].index, inplace=True)\n        df.loc[f2, col] = enc['target']\n\n        gb = df.loc[f1].groupby(enc_cols)[['target']].max().reset_index()\n        enc = df.loc[f2][enc_cols].merge(gb, on=enc_cols, how='left')[['target']].copy()\n        enc.set_index(df.loc[f2].index, inplace=True)\n        df.loc[f2, col2] = enc['target']","d94629e7":"def downcast(df):\n    float32_cols = [c for c in df if df[c].dtype == 'float64']\n    int32_cols = [c for c in df if df[c].dtype in ['int64', 'int16', 'int8']]\n\n    df[float32_cols] = df[float32_cols].astype(np.float32)\n    df[int32_cols] = df[int32_cols].astype(np.int32)\n\n    return df\ndf.fillna(0, inplace=True)\ndf = downcast(df)","996a3d7f":"%%time\n\nshift_range = [1, 2, 3, 4, 5, 12]\n\nshifted_columns = [c for c in df if 'target' in c]\n\nfor shift in tqdm_notebook(shift_range):\n    shifted_data = df[groupby_cols + shifted_columns].copy()\n    shifted_data['date_block_num'] = shifted_data['date_block_num'] + shift\n\n    foo = lambda x: '{}_lag_{}'.format(x, shift) if x in shifted_columns else x\n    shifted_data = shifted_data.rename(columns=foo)\n\n    df = pd.merge(df, shifted_data, how='left', on=groupby_cols).fillna(0)\n    df = downcast(df)\n\n    del shifted_data\n    gc.collect()\n    sleep(1)","d4fc3bb1":"df = downcast(df)","a9638547":"drop_columns = [c for c in df if c[-1] not in ['1', '2', '3', '4', '5', '6', '7', '8', '9', '0'] and c.startswith('target')]\ndrop_columns += ['ID']\nfeatures = df.columns.difference(drop_columns)","6b2bc80f":"f0 = df.date_block_num < 34\nf1 = df.date_block_num == 34\n\ntrain, val = train_test_split(df[f0], test_size=0.2, stratify=df[f0]['target'])\ntest = df[f1]\n\nTrain = xgb.DMatrix(train[features], train['target'])\nVal = xgb.DMatrix(val[features], val['target'])\nTest = xgb.DMatrix(test[features])","cb83538a":"del df\ngc.collect()","fcae5d5a":"%%time\n\nxgb_params = {\n    'eval_metric': 'rmse',\n    'lambda': '0.171', \n    'gamma': '0.124',\n    'booster': 'gbtree', \n    'alpha': '0.170',\n    'objective': 'reg:squarederror',\n    'colsample_bytree': '0.715',\n    'subsample': '0.874', \n    'silent': True,\n    'min_child_weight': 26,\n    'eta': '0.148',\n    'max_depth': 6,\n    'tree_method': 'gpu_hist', \n    'n_gpus': 1\n}\n\n\nmodel = xgb.train(xgb_params, Train, 1500, [(Train, 'Train'), (Val, 'Val')], early_stopping_rounds=10, verbose_eval=1)","e7546c5a":"test['item_cnt_month'] = model.predict(Test).clip(0, 20)","a207a966":"\ntest[['ID', 'item_cnt_month']].sort_values('ID').to_csv('submission.csv', index=False)\npickle.dump(model, open('xgb.pickle', 'wb'))","56622881":"Exploratory Data Analaysis (EDA)","050d0271":"\nMean encoded features","e8cd05f5":"Predict test data","c70b47e8":"\nAggregations","565d1a2c":"Save submission & model","d125c81e":"\nAdd new features","d57fa4b5":"\nOutliers"}}