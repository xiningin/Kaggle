{"cell_type":{"fd53b6ab":"code","85764044":"code","7350819c":"code","7e4c1f9a":"code","67dba3da":"code","a7dd12d2":"code","67ff1ba0":"code","04fe0860":"code","20cfd72e":"code","33ab6e5a":"code","7002791e":"code","c16000c6":"code","e62e0cad":"code","51d2f590":"code","3b801be6":"code","a27eac9f":"code","3136eb22":"code","34d96e3f":"code","a3d344a2":"code","7c7c0a67":"code","11f54596":"code","c84f5c90":"code","c7da6c8c":"code","4cddb065":"code","6c1eba2a":"code","63a6c2d5":"code","c3616814":"code","43e27984":"code","78c913b9":"code","a7b05d8b":"code","89fe5953":"code","4e7dc300":"code","c7f13217":"code","8d07b458":"code","abb8f44b":"code","a70c5884":"code","7bf4b7c8":"code","5565c0f9":"code","5b7a5d2e":"code","0681dbfe":"code","4b35f409":"code","a587caa3":"code","26d9a08d":"code","49a17898":"code","eb9879dc":"code","e1fb08e8":"code","e5862ffb":"code","38a8e574":"code","65b835c2":"code","98ec5629":"markdown","eb85dba9":"markdown","a45ce28e":"markdown","9c5a1aa4":"markdown","fb8422ae":"markdown","8a168fc8":"markdown","5ce91fd7":"markdown","094c6e3f":"markdown","a9d484ac":"markdown","84ca7d7d":"markdown","95163df5":"markdown","73edd482":"markdown"},"source":{"fd53b6ab":"from scipy import stats","85764044":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","7350819c":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom tqdm import tqdm_notebook as tqdm","7e4c1f9a":"!pip install impyute","67dba3da":"from impyute.imputation.cs import fast_knn","a7dd12d2":"working_data = pd.read_csv('..\/input\/nielsenhackathon\/Hackathon_Working_Data.csv')\nvalidation_data = pd.read_csv('..\/input\/nielsenhackathon\/Hackathon_Validation_Data.csv')\nmapping_file= pd.read_csv('..\/input\/nielsenhackathon\/Hackathon_Mapping_File.csv')\nideal_data = pd.read_csv('..\/input\/nielsenhackathon\/Hackathon_Ideal_Data.csv')\nsubmit = pd.read_csv('..\/input\/nielsenhackathon\/Sample Submission.csv')","67ff1ba0":"print('Working Data')\nworking_data.head()\nprint('Validation Data')\nvalidation_data.head()\nprint('Ideal Data')\nideal_data.head()\nsubmit.head()","04fe0860":"ideal_data_grouped = ideal_data.groupby(['STORECODE', 'MONTH', 'GRP'])['VALUE'].sum().reset_index()","20cfd72e":"fig, ax = plt.subplots(3,1, sharex =True)\n\nfor i, month in enumerate(['M1','M2','M3']):\n    df = ideal_data_grouped[(ideal_data_grouped['MONTH']==month) & (ideal_data_grouped['STORECODE']=='P1')].copy()\n    ax[i].bar(df['GRP'], df['VALUE'], align='center')","33ab6e5a":"fig, ax = plt.subplots(3,1, sharex =True, figsize=(12,8))\nplt.subplots_adjust(hspace=0.8)\nfor i, month in enumerate(['M1','M2','M3']):\n    df = ideal_data_grouped[(ideal_data_grouped['MONTH']==month) & (ideal_data_grouped['STORECODE']=='P2')].copy()\n    _ =ax[i].bar(df['GRP'], df['VALUE'], align='center')\n    _ =ax[i].set_xticklabels( '', rotation=90)\n    _ =ax[i].set_ylabel('TOTALVALUE')\n    _ =ax[i].set_xlabel('GRP')\n    _ =ax[i].set_title(f'TOTALVALUE FOR DIFFERENT GRP FOR STORE P2 IN MONTH {month}')","7002791e":"fig, ax = plt.subplots(3,1, sharex =True)\n\nfor i, month in enumerate(['M1','M2','M3']):\n    df = ideal_data_grouped[(ideal_data_grouped['MONTH']==month) & (ideal_data_grouped['STORECODE']=='P3')].copy()\n    ax[i].bar(df['GRP'], df['VALUE'], align='center')","c16000c6":"set(working_data['GRP'].unique()) - set(validation_data['GRP'].unique())","e62e0cad":"set(validation_data['GRP'].unique()) - set(working_data['GRP'].unique())","51d2f590":"#Fixing the space in the working data group\nworking_data['GRP'] = working_data['GRP'].apply(lambda x: ' '.join(x.split()))","3b801be6":"set(working_data['GRP'].unique()) - set(validation_data['GRP'].unique())","a27eac9f":"set(validation_data['GRP'].unique()) - set(working_data['GRP'].unique())","3136eb22":"validation_data['key']=[1]*validation_data.shape[0]","34d96e3f":"list_of_days = list(range(1,32))","a3d344a2":"whole_incomplete_data = pd.merge(validation_data, pd.DataFrame({'key':[1]*31,'DAY':list_of_days}), on='key').drop('key',axis=1)","7c7c0a67":"validation_data.drop('key',axis =1, inplace=True)","11f54596":"working_data_grp_grouped = working_data.groupby(['STORECODE','MONTH','GRP','DAY'])['VALUE'].sum().reset_index()","c84f5c90":"working_data_grp_grouped.head()","c7da6c8c":"whole_partial_data = pd.merge(whole_incomplete_data, working_data_grp_grouped, on =['STORECODE','MONTH','GRP','DAY'], how='left')","4cddb065":"whole_partial_data_table_2 = pd.pivot_table(whole_partial_data,values='VALUE',index =['STORECODE','MONTH','DAY'], columns='GRP',dropna=False).reset_index().set_index(['STORECODE','DAY','MONTH'])","6c1eba2a":"whole_partial_data_table_2.head()","63a6c2d5":"# Data null initially\nwhole_partial_data_table_2.isna().sum().sum()\/ (whole_partial_data_table_2.shape[0] * whole_partial_data_table_2.shape[1])","c3616814":"whole_partial_data_table_2.loc[('N1',4)]","43e27984":"whole_partial_data_table_2.loc[('N1',4)].fillna(whole_partial_data_table_2.loc[('N1',4)].mean())","78c913b9":"## algo3- Same store, same day, same group, different month - same data\nfor store in tqdm(list(map(lambda x: 'N'+str(x), range(1,11)))):\n    for day in range(1,32):\n        whole_partial_data_table_2.loc[(store,day)].fillna(whole_partial_data_table_2.loc[(store,day)].mean(), inplace=True)","a7b05d8b":"whole_partial_data_table_2.head()","89fe5953":"# Data null after first imputation step\nwhole_partial_data_table_2.isna().sum().sum()\/ (whole_partial_data_table_2.shape[0] * whole_partial_data_table_2.shape[1])","4e7dc300":"algo3_baseline_data  = whole_partial_data_table_2.stack().reset_index().groupby(['STORECODE','MONTH','GRP'])[0].sum().reset_index()","c7f13217":"algo3_baseline_data.rename(columns = {0:'TOTALVALUE'}, inplace=True)","8d07b458":"algo3_baselined_submission = pd.merge(validation_data,algo3_baseline_data, how='left',on=['STORECODE','MONTH','GRP']).fillna(0).drop(['STORECODE','MONTH','GRP'], axis=1)\nalgo3_baselined_submission.columns = ['ID','TOTALVALUE']\nalgo3_baselined_submission['TOTALVALUE'] = algo3_baselined_submission['TOTALVALUE'].astype('int')","abb8f44b":"algo3_baselined_submission.to_csv('algo3_baseline_check.csv',index=False)","a70c5884":"# Data null from intermediate stage before grouping\nwhole_partial_data_table_2.isna().sum().sum()\/ (whole_partial_data_table_2.shape[0] * whole_partial_data_table_2.shape[1])","7bf4b7c8":"whole_partial_data_table_3 = whole_partial_data_table_2.reset_index().set_index(['STORECODE','MONTH','DAY'])","5565c0f9":"# Data null initially in new table\nwhole_partial_data_table_3.isna().sum().sum()\/ (whole_partial_data_table_3.shape[0] * whole_partial_data_table_3.shape[1])","5b7a5d2e":"# with pd.option_context('max_rows',31):\n#     whole_partial_data_table_3.loc[('N2', 'M1')]","0681dbfe":"# for i, x in whole_partial_data_table_3.loc[('N2', 'M1')].iterrows():\n#     if any([not np.isnan(i) for i in x.values]):\n#         whole_partial_data_table_3.loc[('N2', 'M1',i),:] = [0 if np.isnan(i) else i for i in x.values] \n# with pd.option_context('max_rows',31):\n#     whole_partial_data_table_3.loc[('N2', 'M1')]","4b35f409":"# whole_partial_data_table_3.loc[('N2', 'M1')] = fast_knn(whole_partial_data_table_3.loc[('N2', 'M1')].values, k =30)\n# with pd.option_context('max_rows',31):\n#     whole_partial_data_table_3.loc[('N2', 'M1')]","a587caa3":"for month in tqdm(['M1','M2','M3']):\n    for store in tqdm(list(map(lambda x: 'N'+str(x), range(1,11)))):\n        nan_present= False\n        for i, x in whole_partial_data_table_3.loc[(store,month)].iterrows():\n            if any([not np.isnan(i) for i in x.values]):\n                whole_partial_data_table_3.loc[(store,month,i),:] = [0 if np.isnan(i) else i for i in x.values] \n                #replacing nan values with 0 if even one purchase was recorded,\\\n            else:\n                nan_present = True\n                #leaving the whole thing as it is. To be imputed later.\n        if nan_present:\n            print(f\"Imputing in ({month, store})\")\n            #Mean of Whole data other than the outlier        \n    #         df = whole_partial_data_table_3.loc[(store, month)].copy()\n    #         whole_partial_data_table_3.loc[(store,month)].fillna(df[np.abs(df-df.mean()) <= (3*df.std())].mean(), inplace=True)\n            #KNN\n            whole_partial_data_table_3.loc[(store,month)] = fast_knn(whole_partial_data_table_3.loc[(store,month)].values, k =30)","26d9a08d":"# Data null finally in new table\nwhole_partial_data_table_3.isna().sum().sum()\/ (whole_partial_data_table_3.shape[0] * whole_partial_data_table_3.shape[1])","49a17898":"whole_partial_data_table_3.head()","eb9879dc":"whole_complete_data_2 = pd.DataFrame(whole_partial_data_table_3.stack()).reset_index().rename(columns={0:'VALUE'})\nwhole_complete_data_2.head()","e1fb08e8":"whole_complete_data_with_id = pd.merge(whole_partial_data, whole_complete_data_2, on = ['STORECODE','MONTH','DAY','GRP'], how = 'inner').drop('VALUE_x',axis=1)","e5862ffb":"whole_complete_data_with_id.head()","38a8e574":"submit = whole_complete_data_with_id.groupby('ID')['VALUE_y'].sum().reset_index()\nsubmit.columns = ['ID','TOTALVALUE']\nsubmit['TOTALVALUE'] = submit['TOTALVALUE'].astype('int')\nsubmit.head()","65b835c2":"submit.to_csv('final_submission.csv',index=False)","98ec5629":"## Importing libraries","eb85dba9":"## Improvement using Assumption 1","a45ce28e":"## Imputation using fast knn","9c5a1aa4":"## Initialising Procedure","fb8422ae":"## Idea derivation","8a168fc8":"## Reading Data","5ce91fd7":"> Obtained Score - 2359.0151807577","094c6e3f":"NOTE: Dependencies for the project are listed below:\n- scipy==1.2.1\n- pandas==0.25.1 \n- numpy==1.17.2\n- matplotlib==3.1.2\n- tqdm==4.41.0\n- impyute==0.0.8","a9d484ac":"## Some anomalous groups ","84ca7d7d":"## Algorithm Idea\n### Using the observation `Same store, same group, different month => Same value`, derived assumption 2\n\n### `Same store, same group, same day, different month => similar value`","95163df5":"> Obtained Score - 2389.4291343469","73edd482":">Conclusion - Same store, same group, different month => Same value"}}