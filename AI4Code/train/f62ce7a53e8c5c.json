{"cell_type":{"44999eb4":"code","ec70688e":"code","4034e6ae":"code","a7db45d1":"code","2e11f0f3":"code","66c2da11":"code","34d15da7":"code","cc96284b":"code","ba20cc40":"code","fab8d668":"markdown"},"source":{"44999eb4":"# Generating 1D data \nimport numpy as np \nimport matplotlib.pyplot as plt\n\n# Number of data points\nN = 100\n\nX = np.random.uniform(low=0, high=100, size=N)\n\n# Making y = 2x + 1 + some gaussian or normal noise (assumption of linear regression itself)\nY = 2 * X + 1 + np.random.normal(scale=10, size=N)\n\n# Plotting the data to see if it looks linear\nplt.scatter(X, Y, edgecolors='black', color=\"red\")\nplt.show()","ec70688e":"# Applying linear regression\n# After doing the OLS minimizations we get below eqautions for w and b\ndenominator = X.dot(X) - X.mean() * X.sum()\nw = ( X.dot(Y) - Y.mean()*X.sum() ) \/ denominator\nb = ( Y.mean() * X.dot(X) - X.mean() * X.dot(Y) ) \/ denominator","4034e6ae":"Yhat = w * X + b","a7db45d1":"plt.scatter(X, Y, edgecolors='black')\nplt.plot(X, Yhat, color=\"red\")\nplt.show()","2e11f0f3":"# It seems our model (line) has done pretty well in identifying the relationship. Of course we made up the data. \n# Let's look at the weights \nprint(\"w: \", w, \" and b: \", b)","66c2da11":"# w which is the slope of line is close to 2 (as in our original data) and bias term is having 1 and the gaussian \n# noise which we added","34d15da7":"# calcualting MSE \nmse = (Yhat - Y).dot(Yhat - Y)\/N\nprint(\"MSE: \", mse)","cc96284b":"# MSE or even RMSE doesn't really tell about the model. So we cxalculate R^2 which tells how good our model fit the \n# data\nss_res = Y - Yhat\nss_tot = Y - Y.mean()\nr2 = 1 - ss_res.dot(ss_res)\/ss_tot.dot(ss_tot)\nprint(\"The r-squared is: \", r2)","ba20cc40":"# This value of r-suared is pretty good, as it can be maximum of 1. This means our data is modeled properly by our \n# model","fab8d668":"Linear Regression assumes that output variable can be modeled as linear combination of input features. \n\nMathematically, it looks like this:\n\nY = X.W + b, where w is weight vector containing weights for each feature, Y is dependent variable, X is independent features and b is the bias. The reason why we take a transpose here for w is because in statistics, by default when we talk about a vector its gonna be a column vector. \n\nAssuming we have N points and one feature, then X will be (Nx1) vector, b is a scalar. Since we only have one feature, w will be weight associated with it and its dimension will be (1x1) (you can say a scalar). But if we would have D features then X would have been (NxD) and w would been of shape (Dx1). "}}