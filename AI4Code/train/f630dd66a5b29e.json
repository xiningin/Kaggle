{"cell_type":{"33917e7e":"code","19334d0b":"code","0f66c37f":"code","48d09e7a":"code","9c128eaa":"code","bced7a19":"code","54a09bbb":"code","42e03f2a":"code","7e6329dd":"code","87a6d193":"code","bdeaf237":"code","350ed798":"code","e14db61c":"code","db64e48d":"markdown","5e5a7b4d":"markdown","8f457125":"markdown","73dec837":"markdown","d309f1f0":"markdown","a5e116ce":"markdown","77823b30":"markdown"},"source":{"33917e7e":"import pandas as pd\nimport os\nprint(os.listdir(\"..\/input\"))","19334d0b":"df = pd.read_csv(\"..\/input\/UserRatings1.csv\")\nprint(df.shape)\ndf.head()","0f66c37f":"df = df.melt(id_vars=\"JokeId\",value_name=\"rating\")","48d09e7a":"df.dropna(inplace=True)\ndf.variable = df.variable.str.replace(\"User\",\"\")\ndf.rename(columns={\"variable\":\"User\"},inplace=True)\nprint(df.shape)\ndf.head()","9c128eaa":"df[\"mean_joke_rating\"] = df.groupby(\"JokeId\")[\"rating\"].transform(\"mean\")\ndf[\"mean_user_rating\"] = df.groupby(\"User\")[\"rating\"].transform(\"mean\")\ndf[\"user-count\"] = df.groupby(\"User\")[\"rating\"].transform(\"count\")\ndf[\"joke-count\"] = df.groupby(\"JokeId\")[\"rating\"].transform(\"count\")\ndf.describe()","bced7a19":"df.head()","54a09bbb":"df = df.merge(pd.read_csv(\"..\/input\/JokeText.csv\"),on=\"JokeId\")\ndf.tail()","42e03f2a":"df.rating.mean()","7e6329dd":"df.rating.median()","87a6d193":"df3 = df.drop_duplicates(subset=[\"JokeId\"]).loc[:,['JokeId', 'mean_joke_rating', 'joke-count', 'JokeText']].sort_values('mean_joke_rating',ascending=False)\ndf3.head()\n","bdeaf237":"for j in list(df3.head().JokeText): print (j)","350ed798":"# The least funny?\nfor j in list(df3.tail().JokeText): print (j)","e14db61c":"df.to_csv(\"jokerRatingsMerged.csv.gz\",index=False,compression=\"gzip\")","db64e48d":"### Melt data into tidy format: 1 row per Joke X User Rating\n* we'll also drop items that were not rated (NaNs\/blanks), although they could be included, e.g. for implicit feedback systems or features based on # jokes rated. \n    * We only have 100 jokes in this version of the data","5e5a7b4d":"* More sorting by median top jokes or bayesian goes here - I leave it as an exercise to the reader :)","8f457125":"#### Note that we have a very dense user-item interaction matrix here! \n* users rated 56-100 jokes = \"no cold start\" problem!","73dec837":"## Load the sample of the Jester jokes recomendation dataset \n* Melt data into tidy format\n* Add  target based features for creating and evaluating recommender system (e.g. normalize by baseline of the user or Item's average or median rating)\n* Minor EDA\n\n* Original data (including more ratings +- more jokes): http:\/\/eigentaste.berkeley.edu\/dataset\/ ","d309f1f0":"## The funniest jokes?\n* Note that the mean and median are very different here!.\n* We might even want to sort by Z-score (i.e normalize by each user's average rating first, as some people are just sourpusses, ).\n* We could even get the [bayesian average](http:\/\/www.evanmiller.org\/bayesian-average-ratings.html) : http:\/\/www.evanmiller.org\/bayesian-average-ratings.html  ,  https:\/\/fulmicoton.com\/posts\/bayesian_rating\/   \n    * i.e use the prior mean rating of all jokes, and look at how many times a joke was rated","a5e116ce":"## Merge with the text data\n* Optional - can use for additional  metadata or pretrained embeddings. Makes file larger and diverges from simplest triplet ({user} {item} {rating}) format ","77823b30":"### Export"}}