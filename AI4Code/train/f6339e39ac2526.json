{"cell_type":{"a02a6f25":"code","53093bbf":"code","1c2dc688":"code","0253aa0e":"code","eea9e04a":"code","29c483bb":"code","c5db2d46":"code","73af9326":"code","0d6a591c":"code","05ea4381":"code","b41f8998":"code","19b50f33":"code","3305c58f":"code","007394da":"code","8746b45f":"code","8f97b1fc":"code","ecbe36fb":"code","9487745e":"code","f2ce618e":"code","565a4704":"code","6ffa74ce":"code","88b9b345":"code","6407d259":"code","eeccec98":"code","00a1caec":"code","7bfdeba0":"code","96720eab":"code","546e5843":"code","63604731":"code","5e278ca8":"code","5d709ec9":"code","bd6cc9a0":"code","3416f0cc":"code","30587cfe":"code","3571648d":"code","c2336333":"code","df8df961":"code","0620e774":"code","852ecb0b":"code","620c4b8e":"code","97aeeea7":"code","99b10e6b":"code","341a9ff8":"code","d8c125c9":"code","7c26dda8":"code","4fc648a6":"code","001c9f61":"code","4d16008c":"code","ef8b8b5b":"code","3c3ee7a4":"markdown","1e35ba37":"markdown","452fc0f4":"markdown","1ff1d5c2":"markdown","e6adbc51":"markdown","14d5bd07":"markdown","605c0ecb":"markdown","f34e103b":"markdown","5dc45e42":"markdown","f4983193":"markdown","aa081d6f":"markdown","0429dd0d":"markdown","04a01c95":"markdown","c12775c3":"markdown","053f62b2":"markdown","6eb95471":"markdown","713fd343":"markdown","79bc8dac":"markdown","d4d567da":"markdown"},"source":{"a02a6f25":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Regular EDA ( Exploratory data analysis ) and plotting libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# we want our plots to appear inside the notebook\n%matplotlib inline\n\n# Models from SCikit -Learn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\n\n# Model Evaluations\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nimport warnings\nwarnings.filterwarnings(\"ignore\")","53093bbf":"print(os.listdir('..\/input'))","1c2dc688":"nRowsRead = 1000 # specify 'None' if want to read whole file\ndf= pd.read_csv('..\/input\/diabetes.csv', delimiter=',', nrows = nRowsRead)\ndf.dataframeName = 'diabetes.csv'\nnRow, nCol = df.shape\nprint(f'There are {nRow} rows and {nCol} columns')","0253aa0e":"df.head(5)","eea9e04a":"df.tail()","29c483bb":"df[\"Outcome\"].value_counts()","c5db2d46":"df[\"Outcome\"].value_counts().plot(kind=\"bar\",color=[\"salmon\",\"lightblue\"])","73af9326":"df.info()","0d6a591c":"df.describe()","05ea4381":"# Create another figure\nplt.figure(figsize=(10,6))\n\n# Scatter with positive examples\nplt.scatter(df.Age[df.Outcome==1],\n           df.Pregnancies[df.Outcome==1],c=\"salmon\")\n\n# Scatter with negative examples\nplt.scatter(df.Age[df.Outcome==0],\n           df.Pregnancies[df.Outcome==0],\n           c=\"lightblue\")\n\n# Add some helpful info\nplt.title(\"Diabetics in Pregnant women\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Pregnancies\")\nplt.legend([\"Diabetics\",\"No Diabetics\"]);","b41f8998":"data = pd.read_csv('..\/input\/diabetes.csv', delimiter=',', nrows = nRowsRead) \ndf = pd.DataFrame(data, columns= ['Age','Pregnancies','BMI','DiabetesPedigreeFunction','Outcome'])\n\n","19b50f33":"data.query('Age > 70',inplace = True)\n\nprint(data)","3305c58f":"# Create a plot of crosstab\npd.crosstab(df.Outcome,df.Age).plot(kind =\"bar\",\n                                   figsize=(10,6),\n                                   color=[\"salmon\",\"lightblue\"])\nplt.title(\"Diabetics frequency related to Age\")\nplt.xlabel(\"0 = No Diabetics, 1 = Diabetics\")\nplt.ylabel(\"Age\")\nplt.legend([\"Diabetics\",\"No Diabetics\"]);\nplt.xticks(rotation=0);","007394da":"df.Age.plot.hist();","8746b45f":"df.BMI.plot.hist();","8f97b1fc":"df.corr()","ecbe36fb":"corr_matrix = df.corr()\nfig,ax = plt.subplots(figsize =(15,10))\nax = sns.heatmap(corr_matrix,\n                annot = True,\n                linewidth=0.5,\n                fmt=\".2f\",\n                cmap=\"YlGnBu\");\nbottom,top = ax.get_ylim()\nax.set_ylim(bottom + 0.5,top-0.5)","9487745e":"df.head()","f2ce618e":"#Split data into X and Y\nX = df.drop(\"Outcome\",axis =1)\ny = df[\"Outcome\"]","565a4704":"# Split data into train and test sets\nnp.random.seed(42)\n\n# Split into train & test set\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2)","6ffa74ce":"# Put models in a dictionary\n\nmodels = { \"Logistic Regression\": LogisticRegression(),\n            \"Naive Bayes\": GaussianNB(),\n            \"Stochastic Gradient\": SGDClassifier(),\n            \"KNeighbors Classifier\": KNeighborsClassifier(),\n            \"DecisionTree Classifier\": DecisionTreeClassifier(),\n            \"RandomForest Classifier\": RandomForestClassifier(),\n            \"Support Vector Machine\": SVC()}\n# Create a function to fit and score models\n\ndef fit_and_score(models, X_train,X_test,y_train,y_test):\n    \"\"\"\n    Fits and evaluates given machine learning models.\n    models: a dict of different SCikit-Learn machine learning models\n    X_train : training data(no labels)\n    X_test: testing data (no labels)\n    y_train: training labels\n    y_test: test labels\n    \"\"\"\n    # set random seeed\n    np.random.seed(42)\n    # Make a dictionary to keep model scores\n    model_scores = {}\n    # Loop through models\n    for name, model in models.items():\n        # Fit the model to the data\n        model.fit(X_train,y_train)\n        # Evaluate the model and append its score to model scores\n        model_scores[name] = model.score(X_test,y_test)\n    return model_scores","88b9b345":"model_scores = fit_and_score(models=models,\n                             X_train= X_train,\n                             X_test= X_test,\n                            y_train=y_train,\n                            y_test=y_test)\nmodel_scores","6407d259":"model_compare = pd.DataFrame(model_scores, index=[\"accuracy\"])\nmodel_compare.T.plot.bar();","eeccec98":"from sklearn.neighbors import KNeighborsClassifier\n# Let's tune KNN\ntrain_scores=[]\ntest_scores= [] \n\n# create a list of different values for n_nieghbors\nneighbors = range(1,21)\n\n# Setup KNN instance\nknn = KNeighborsClassifier()\n\n# Loop through different n_neighbors\nfor i in neighbors:\n    knn.set_params(n_neighbors=i)\n    \n    # Fit the algorithm\n    knn.fit(X_train,y_train)\n    \n    # update the training scores list\n    train_scores.append(knn.score(X_train,y_train))\n    \n    # update the test scores list\n    test_scores.append(knn.score(X_test,y_test))","00a1caec":"plt.plot(neighbors,train_scores, label = \"Train score\")\nplt.plot(neighbors,test_scores, label = \"Test score\")\nplt.xticks(np.arange(1, 21, 1))\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Model score\")\nplt.legend()\n\nprint(f\" Maximum KNN score on the test data:{max(test_scores)*100:.2f}%\")","7bfdeba0":"# Create a hyperparameter grid for LogisticRegression\n\nlog_reg_grid = {\"C\":np.logspace(-4, 4, 20),\n               \"solver\":[\"liblinear\"]}\n\n# Create a hyperparameter grid for RandomForestClassifier\nrf_grid = {\"n_estimators\": np.arange(10, 1000, 50),\n           \"max_depth\": [None, 3, 5, 10],\n           \"min_samples_split\": np.arange(2, 20, 2),\n           \"min_samples_leaf\": np.arange(1, 20, 2)}","96720eab":"# Tune LogisticRegression\n\nnp.random.seed(42)\n\n# Setup random hyperparameter search for LogisticRegression\nrs_log_reg = RandomizedSearchCV(LogisticRegression(),\n                                param_distributions=log_reg_grid,\n                                cv=5,\n                                n_iter=20,\n                                verbose=True)\n\n# Fit random hyperparameter search model for LogisticRegression\nrs_log_reg.fit(X_train, y_train)","546e5843":"rs_log_reg.best_params_","63604731":"rs_log_reg.score(X_test, y_test)","5e278ca8":"# Tune RandomForestClassifier\n# Setup random seed\nnp.random.seed(42)\n\n# Setup random hyperparameter search for RandomForestClassifier\nrs_rf = RandomizedSearchCV(RandomForestClassifier(), \n                           param_distributions=rf_grid,\n                           cv=5,\n                           n_iter=20,\n                           verbose=True)\n\n# Fit random hyperparameter search model for RandomForestClassifier()\nrs_rf.fit(X_train, y_train)","5d709ec9":"# Find the best hyperparameters\nrs_rf.best_params_","bd6cc9a0":"# Evaluate the randomized search RandomForestClassifier model\nrs_rf.score(X_test, y_test)","3416f0cc":"\n# Different hyperparameters for our LogisticRegression model\nlog_reg_grid = {\"C\": np.logspace(-4, 4, 30),\n                \"solver\": [\"liblinear\"]}\n\n# Setup grid hyperparameter search for LogisticRegression\ngs_log_reg = GridSearchCV(LogisticRegression(),\n                          param_grid=log_reg_grid,\n                          cv=5,\n                          verbose=True)\n\n# Fit grid hyperparameter search model\ngs_log_reg.fit(X_train, y_train);","30587cfe":"# Check the best hyperparmaters\ngs_log_reg.best_params_","3571648d":"# Evaluate the grid search LogisticRegression model\ngs_log_reg.score(X_test, y_test)","c2336333":"# Confusion matrix\nprint(confusion_matrix(y_test, y_preds))","df8df961":"\n# Make predictions with tuned model\ny_preds = gs_log_reg.predict(X_test)","0620e774":"# Import Seaborn\nimport seaborn as sns\n\n# Increase font size\nsns.set(font_scale=1.5) \ndef plot_conf_mat(y_test, y_preds):\n    \"\"\"\n    Plots a confusion matrix using Seaborn's heatmap().\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(3, 3))\n    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\n                     annot=True, # Annotate the boxes\n                     cbar=False)\n    plt.xlabel(\"Predicted label\") # predictions go on the x-axis\n    plt.ylabel(\"True label\") # true labels go on the y-axis \n    \nplot_conf_mat(y_test, y_preds)","852ecb0b":"print(classification_report(y_test, y_preds))","620c4b8e":"# Create a new classifier with best parameters\nclf = LogisticRegression(C=0.20433597178569418,\n                         solver=\"liblinear\")","97aeeea7":"# Cross-validated accuracy\ncv_acc = cross_val_score(clf,\n                         X,\n                         y,\n                         cv=5,\n                         scoring=\"accuracy\")\ncv_acc","99b10e6b":"cv_acc = np.mean(cv_acc)\ncv_acc","341a9ff8":"# Cross-validated precision\ncv_precision = cross_val_score(clf,\n                         X,\n                         y,\n                         cv=5,\n                         scoring=\"precision\")\ncv_precision=np.mean(cv_precision)\ncv_precision","d8c125c9":"# Cross-validated recall\ncv_recall = cross_val_score(clf,\n                         X,\n                         y,\n                         cv=5,\n                         scoring=\"recall\")\ncv_recall = np.mean(cv_recall)\ncv_recall","7c26dda8":"# Cross-validated f1-score\ncv_f1 = cross_val_score(clf,\n                         X,\n                         y,\n                         cv=5,\n                         scoring=\"f1\")\ncv_f1 = np.mean(cv_f1)\ncv_f1","4fc648a6":"# Visualize cross-validated metrics\ncv_metrics = pd.DataFrame({\"Accuracy\": cv_acc,\n                           \"Precision\": cv_precision,\n                           \"Recall\": cv_recall,\n                           \"F1\": cv_f1},\n                          index=[0])\n\ncv_metrics.T.plot.bar(title=\"Cross-validated classification metrics\",\n                      legend=False);","001c9f61":"# Fit an instance of LogisticRegression\nclf = LogisticRegression(C=0.20433597178569418,\n                         solver=\"liblinear\")\n\nclf.fit(X_train, y_train);","4d16008c":"# Match coef's of features to columns\nfeature_dict = dict(zip(df.columns, list(clf.coef_[0])))\nfeature_dict","ef8b8b5b":"# Visualize feature importance\nfeature_df = pd.DataFrame(feature_dict, index=[0])\nfeature_df.T.plot.bar(title=\"Feature Importance\",legend = False);","3c3ee7a4":"Let's take a quick look at what the data looks like:","1e35ba37":"### Diabetes Related to Age","452fc0f4":"The next hidden code cells define functions for plotting data. Click on the \"Code\" button in the published kernel to reveal the hidden code.","1ff1d5c2":"Now you're ready to read in the data and use the plotting functions to visualize the data.","e6adbc51":"### Hyerparameter Tuning","14d5bd07":"### Hyperparamter Tuning with GridSearchCV\nSince our LogisticRegression model provides the best scores so far, we'll try and improve them again using GridSearchCV...","605c0ecb":"### Now we've got Hyperparamter grids setup for each of our models, let tune them using RandomizedSearchCV","f34e103b":"## Exploratory Analysis\nTo begin this exploratory analysis, first use `matplotlib` to import libraries and define functions for plotting the data. Depending on the data, not all plots will be made. (Hey, I'm just a kerneling bot, not a Kaggle Competitions Grandmaster!)","5dc45e42":"### Hyperparameter tuning with RandomizedSearchCV\n\nwe're going to tune:\n * LogisticRegression()\n * RandomForestClassifier()\n...using RandomizedSearchCV\n","f4983193":"### Introducing different Models\n1. Logistic Regression - In this Model, the probabilities describing the possible outcomes of a single trial are modelled using a logistic function.\n2. Naive Bayes - This model is based on Bayes\u2019 theorem with the assumption of independence between every pair of features\n3. Stochastic Gradient Descent - It is a simple and very efficient approach to fit linear models.\n4. K-Nearest Neighbours - Classification is computed from a simple majority vote of the k nearest neighbours of each point.\n5. Decision Tree - Given a data of attributes together with its classes, a decision tree produces a sequence of rules that can be used to classify the data.\n6. Random Forest - Random forest classifier is a meta-estimator that fits a number of decision trees on various sub-samples of datasets and uses average to improve the predictive accuracy of the model and controls over-fitting\n7. Support Vector Machine - Support vector machine is a representation of the training data as points in space separated into categories by a clear gap that is as wide as possible.\nLet us find out which model best fits for this requirement.","aa081d6f":"### Diabetes related to BMI","0429dd0d":"Scatter and density plots:","04a01c95":"### Conclusion:\n\nIn this study, shows that women getting a higher chance of diabetes other than Age, BMI, pregnancies, \"Diabetes Pedigree function\" should be considered as one of the important factor.It shows the Diabetes mellitus history in relatives and the genetic relationship of those relatives to the patient may be a cause This measure of genetic influence gave us an idea of the hereditary risk one might have with the onset of diabetes mellitus.so,Feature importance would be given to DiabetesPedigreeFunction for further analysis in the prediction of Diabetes.","c12775c3":"There is 1 csv file in the current version of the dataset:\n","053f62b2":"### Sources:\n    https:\/\/rpubs.com\/ikodesh\/53189#:~:text=According%20to%20http%3A%2F%2Fwww,family%20history%20to%20predict%20how","6eb95471":"### Now we 've got a baseline model... and we know a model's first predictions and always what we should based our next steps off.\n\nWhat should we do?\n\nLet's look at the following:\n\nHyper parameter tuning\nFeature importance\nConfusion matrix\nCross - validation\nPrecision\nRecall\nF1 Score\nClassification report\nROC curve\nArea under the Curve(AUC)","713fd343":"### Let's check 1st file: ..\/input\/diabetes.csv","79bc8dac":"## Introduction\nGreetings from the Kaggle bot! This is an automatically-generated kernel with starter code demonstrating how to read in the data and begin exploring. Click the blue \"Edit Notebook\" or \"Fork Notebook\" button at the top of this kernel to begin editing.","d4d567da":"### Evaluting our tuned machine learning classifier, beyond accuracy\n1. ROC curve and AUC score\n2. Confusion matrix\n3. Classification report\n4. Precision\n5. Recall\n6. F1-score ...\nIt would be great if cross-validation was used where possible.\nTo make comparisons and evaluate our trained model, first we need to make predictions."}}