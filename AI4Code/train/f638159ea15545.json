{"cell_type":{"6dcb7609":"code","bcb6fc4f":"code","1d3d4a56":"code","63b14490":"code","9495c403":"code","8963f198":"code","345ffa41":"code","9bd30346":"code","20023b6e":"code","620ee9a7":"code","c7b9b965":"code","b5297194":"code","53c7f13a":"code","1b57a2c4":"code","47b9d6a7":"code","5a6d77b1":"code","e90d56c8":"code","1286ca31":"code","eb48e067":"code","7be0bc86":"code","2c7eb9a4":"code","1d829d1f":"code","04621314":"code","466c698c":"code","e10e8005":"code","f3d04221":"code","066414e6":"code","f2206644":"markdown","0a44838d":"markdown","3464b1ac":"markdown","345e66b4":"markdown","4aca2562":"markdown","2037e443":"markdown","89abc697":"markdown","3115bedf":"markdown","606322f2":"markdown","46966d07":"markdown","7bc55c8a":"markdown","5a99d0b4":"markdown","8fb57b86":"markdown","4351a814":"markdown","bb47dc5e":"markdown","ce3b37d8":"markdown","6c270f32":"markdown","6312af7b":"markdown","cf288a3e":"markdown","1f3d0c8d":"markdown","c05a5709":"markdown","241291ec":"markdown","0adff339":"markdown","e1ea9879":"markdown","f8d3dcba":"markdown"},"source":{"6dcb7609":"import pickle\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport nltk\nimport gensim\nimport csv","bcb6fc4f":"data = pd.read_csv('..\/input\/capital-contries\/capital_country\/capitals.txt', delimiter=' ')\ndata.columns = ['city1', 'country1', 'city2', 'country2']\n\n# print first five elements in the DataFrame\ndata.head(5)","1d3d4a56":"#embeddings = gensim.models.KeyedVectors.load_word2vec_format(r\"C:\\Users\\Naser\\OneDrive\\Desktop\\Desktop folders\\Jupytor new\\NLP\\GoogleNews-vectors-negative300\\GoogleNews-vectors-negative300.bin\",\n#                                               binary = True)","63b14490":"def get_embeddings_from_google_data ():\n    \n    f = open('..\/input\/capital-contries\/capital_country\/capitals.txt', 'r').read()\n    set_words = set(nltk.word_tokenize(f))\n    \n    select_words = words = ['man', 'woman','king', 'queen', 'oil', 'gas', 'happy', 'sad', 'city', 'town', 'village', 'country', 'continent', 'petroleum', 'joyful', 'fear', 'anger','worry', 'stress', 'fear', 'calm','nurse', 'doctor', 'housekeeper', 'engineer']\n    for w in select_words:\n        set_words.add(w)\n    \n    word_embeddings = {}\n    for word in embeddings.index_to_key:\n        if word in set_words:\n            word_embeddings[word] = embeddings[word]\n            \n    pickle.dump( word_embeddings, open( \"..\/input\/capital-contries\/capital_country\/word_embeddings_MYsubset.p\", \"wb\" ) )        ","9495c403":"#get_embeddings_from_google_data ()","8963f198":"word_embeddings = pickle.load(open(\"..\/input\/capital-contries\/capital_country\/word_embeddings_MYsubset.p\", \"rb\"))","345ffa41":"word_embeddings['Amman']","9bd30346":"len(word_embeddings['Amman'])","20023b6e":"def cosine_similarity(A, B):\n    \n    A_B_dot_product = np.dot(A, B)\n\n    norm_of_A = np.sqrt(np.dot(A,A))\n    #or we can use np.linalg.norm(A)\n\n    norm_of_B = np.sqrt(np.dot(B,B))\n    #or we can use np.linalg.norm(B)\n\n    cos_sim = A_B_dot_product \/ (norm_of_A * norm_of_B)\n\n    return cos_sim","620ee9a7":"cosine_similarity(word_embeddings['man'], word_embeddings['king'])","c7b9b965":"cosine_similarity(word_embeddings['woman'], word_embeddings['king'])","b5297194":"cosine_similarity(word_embeddings['man'], word_embeddings['queen'])","53c7f13a":"cosine_similarity(word_embeddings['woman'], word_embeddings['queen'])","1b57a2c4":"def euclidean(A, B):\n    \n    # euclidean distance\n    d = np.linalg.norm(A-B)\n\n    return d","47b9d6a7":"def get_country(city1, country1, city2, embeddings):\n    \"\"\"\n    Input:\n        city1: a string (the capital city of country1)\n        country1: a string (the country of capital1)\n        city2: a string (the capital city of country2)\n        embeddings: a dictionary where the keys are words and values are their embeddings\n    Output:\n        countries: a dictionary with the most likely country and its similarity score\n    \"\"\"\n    \n    # store the city1, country 1, and city 2 in a set called group\n    group = set((city1, country1, city2))\n    \n    # get embeddings of city 1\n    city1_emb = word_embeddings[city1]\n\n    # get embedding of country 1\n    country1_emb = word_embeddings[country1]\n\n    # get embedding of city 2\n    city2_emb = word_embeddings[city2]\n    \n    # get embedding of country 2\n    vec = country1_emb - city1_emb + city2_emb\n    \n    \n    # initialize similarity to -1 so we can search to a better match every eteration \n    similarity = -1\n    \n    \n    # looping through the words in embeddings Dict\n    for word in embeddings.keys():\n        \n        # if the word at iteration i not in our group\n        if word not in group:\n            \n            # get the embedding vector for the word\n            word_embedding = embeddings[word]\n            # caculate the similarity between the word and our vector \n            cur_similarity = cosine_similarity(vec, word_embedding)\n            # if cur_similarity is bigger than the one we initialized\n            \n            if cur_similarity > similarity:\n                # update similarity to search for a better one\n                similarity = cur_similarity\n                \n                country = (word, similarity)\n                \n    return country","5a6d77b1":"# Testing your function.\nget_country('Athens', 'Greece', 'Cairo', word_embeddings)","e90d56c8":"# Testing your function.\nget_country('Cairo', 'Egypt', 'Madrid', word_embeddings)","1286ca31":"# Testing your function.\nget_country('Beirut', 'Lebanon', 'Amman', word_embeddings)","eb48e067":"# Testing your function.\nget_country('Ankara', 'Turkey', 'Amman', word_embeddings)","7be0bc86":"# read each line in capitals.txt split the line and append it to 'evaluation_data' list\n\nevaluation_data = []\n\nwith open(\"..\/input\/capital-contries\/capital_country\/capitals.txt\") as f:\n    \n    file_content = f.readlines()\n    \nfor example in file_content:\n    \n    evaluation_data.append(example.split())","2c7eb9a4":"evaluation_data[:5]","1d829d1f":"def compute_model_accuracy(data, word_embeddings):\n    \n    # initialize number of correct predictions to 0\n    correct_prediction = 0\n    # number of examples \n    num_examples = len(data)\n    \n    for example in data: \n        \n        # for each example get the cities and countries and their embeddings\n        city_1 = example[0]\n        city_1_embedding = word_embeddings[city_1]\n        country_1 = example[1]\n        country_1_embedding = word_embeddings[country_1]\n        \n        city_2 = example[2]\n        city_2_embedding = word_embeddings[city_2]\n        country_2 = example[3]\n        country_2_embedding = word_embeddings[country_2]\n        \n        # get the model prediction \n        model_prediction, _ = get_country(city_1, country_1, city_2 , word_embeddings)\n        true_target = country_2\n        \n        # if the model prediciton is right we will add to correct_prediction\n        if model_prediction == true_target:\n            correct_prediction += 1\n            \n    accuracy = correct_prediction \/ num_examples\n    \n    return accuracy","04621314":"model_acc = compute_model_accuracy(evaluation_data, word_embeddings)\nprint(\"Our Model Accuracy is: {:.2f}\".format(model_acc))","466c698c":"def compute_pca(X, n_components=2):\n    \n    \"\"\"\n    Input:\n        X: of dimension (m,n) where each row corresponds to a word vector\n        n_components: Number of components you want to keep.\n    Output:\n        X_reduced: data transformed in 2 dims\/columns + regenerated original data\n    \"\"\"\n    \n    # mean center the data\n    X_demeaned = X - np.mean(X,axis=0)\n    \n    # calculate the covariance matrix\n    covariance_matrix = np.cov(X_demeaned, rowvar=False)\n    \n    # calculate eigenvectors & eigenvalues of the covariance matrix\n    eigen_vals, eigen_vecs = np.linalg.eigh(covariance_matrix)\n    \n    # sort eigenvalue in increasing order \n    idx_sorted = np.argsort(eigen_vals)\n    \n    # reverse the order so that it's from highest to lowest.\n    idx_sorted_decreasing = idx_sorted[::-1]\n    \n    # sort the eigen values by idx_sorted_decreasing\n    eigen_vals_sorted = eigen_vals[idx_sorted_decreasing]\n    \n    # sort eigenvectors using the idx_sorted_decreasing indices\n    eigen_vecs_sorted = eigen_vecs[:,idx_sorted_decreasing]\n    \n    # select the first n eigenvectors (n is desired dimension\n    # of rescaled data array, or dims_rescaled_data)\n    eigen_vecs_subset = eigen_vecs_sorted[:,0:n_components]\n    \n    # transform the data by multiplying the transpose of the eigenvectors \n    # with the transpose of the de-meaned data\n    # Then take the transpose of that product.\n    X_reduced = np.dot(eigen_vecs_subset.transpose(),X_demeaned.transpose()).transpose()\n    \n    return X_reduced","e10e8005":"def get_vectors(embeddings, words):\n    \"\"\"\n    Input:\n        embeddings: a word \n        fr_embeddings:\n        words: a list of words\n    Output: \n        X: a matrix where the rows are the embeddings corresponding to the rows on the list\n        \n    \"\"\"\n    m = len(words)\n    X = np.zeros((1, 300))\n    for word in words:\n        english = word\n        eng_emb = embeddings[english]\n        X = np.row_stack((X, eng_emb))\n    X = X[1:,:]\n    return X","f3d04221":"words = ['oil', 'gas', 'happy', 'sad', 'city', 'town',\n         'village', 'country', 'continent', 'petroleum', 'joyful']\n\n# given a list of words and the embeddings, it returns a matrix with all the embeddings\nX = get_vectors(word_embeddings, words)\n\nprint('You have 11 words each of 300 dimensions thus X.shape is:', X.shape)","066414e6":"# We have done the plotting for you. Just run this cell.\nresult = compute_pca(X, 2)\nplt.scatter(result[:, 0], result[:, 1])\nfor i, word in enumerate(words):\n    plt.annotate(word, xy=(result[i, 0] - 0.05, result[i, 1] + 0.1))\n\nplt.show()","f2206644":"**euclidean** function will take two vectors A and B and return the euclidean distance between them.","0a44838d":"load_word2vec_format will Load KeyedVectors from a file produced by the original C word2vec-tool format.\n\nI already done this step so I will comment it","3464b1ac":"# Predict the Countries from Capitals","345e66b4":"A and B will be a vectors that represent two words \n\n**cosine_similarity** wii take two vectors **A** and **B** and return numerical number which representing the **cosine similarity** between **A** and **B**","4aca2562":"- When the cosine similarity between A and B equal to 1 then  A  and B are identical. \n\n\n- When the cosine similarity between A and B equal to -1 then  A  and B are opposite.\n\n\n- When the cosine similarity between A and B equal to 0 then  A  and B are orthogonal (or perpendicular)","2037e443":"The function **get_country** takes in three words, and the embeddings dictionary. \n","89abc697":"we will deserialize the file word_embeddings_MYsubset.p into python dict. to get word embeddings for for our selected words and store them in **word_embeddings** variable","3115bedf":"## 3.4 Model Evaluation","606322f2":"# 1- Imports","46966d07":"## 3.3- Finding the country of each capital","7bc55c8a":"# 2- Data","5a99d0b4":"Thank you for reading, I hope you enjoyed and benefited from it.\n\nIf you have any questions or notes please leave it in the comment section.\n\nIf you like this notebook please press upvote and thanks again.","8fb57b86":"#### As we can see queen is more similar to woman than man is ","4351a814":"**get_embeddings_from_google_data** Function will go through the embeddings we got from load_word2vec_format in the previos step and get the embeddings for each word in the capitals.txt file in addition to the words in the list select_words. \n\nThen the embeddings for the words we want will be saved into a serialized python dict. in the file word_embeddings_MYsubset.p","bb47dc5e":"# 5- Thank you","ce3b37d8":"For example lets see the word embedding for the capital of **my country Jordan which is Amman**","6c270f32":"## 3.2- Euclidean distance","6312af7b":"# 4- Visualization using Principle Component Analysis (PCA)","cf288a3e":"### Google data ","1f3d0c8d":"# 3- The model ","c05a5709":"#### As we can see king is more similar to man than woman is ","241291ec":"As we can see the words (sad, joyful, happy) are close to each others since they are feelings, also the words (oil, gas, petroleum) are close to each other, and finnaly the words (village, town, city, country, continent) are close close to each other. ","0adff339":"## 3.1- cosine_similarity","e1ea9879":"### Capitals ","f8d3dcba":"As we can see the vector is 300 D that represent the capital of Jordan Amman"}}