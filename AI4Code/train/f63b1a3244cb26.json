{"cell_type":{"d7ac1bc0":"code","954504bd":"code","44d972df":"code","011e7fe3":"code","3900f20b":"code","04f62f07":"code","3c112e5e":"code","d9b0f13e":"code","a1dfe79b":"code","8875314b":"code","40929f48":"code","2114eef8":"code","472c99ef":"code","2fa16732":"code","dedfb006":"code","8b10a5db":"code","a8f7bb92":"code","95dc007b":"code","927fe110":"code","5054ad62":"code","efa0fdb0":"code","a761c741":"code","51b1c7ae":"code","cbe850ed":"code","4a4651c1":"code","008d6043":"code","2f1c83c6":"code","9d5003a3":"code","afd0af2d":"code","00518a34":"code","57f15f8b":"code","600b051b":"code","824595fd":"code","a8c1a3e6":"code","7d005e95":"code","dcf07f96":"code","8400d54d":"code","9729c7ad":"code","5546449f":"code","f830709e":"code","8b4070e9":"code","d0c6e9ff":"code","3260d1da":"code","e1660368":"code","f04f2275":"code","90886bed":"code","fe661488":"code","4fdbab80":"code","80c858a3":"code","7cc559f6":"code","74f36efd":"code","5ff285ce":"code","f61509d8":"code","ddb04394":"code","2f1a7307":"code","f2273745":"code","e87983ff":"code","aef39f12":"code","fb402faf":"code","23849a1f":"code","a4f8f808":"code","5a3eebb8":"code","32bde6d4":"code","c376735d":"code","2b789f35":"code","f3005aef":"code","444bb6ef":"code","1eab6a89":"code","af0842c7":"code","df9bca53":"code","5b1195fe":"code","e21081c3":"code","6e6d93b3":"code","df7e8711":"code","21df926f":"code","15993ee6":"code","9f0fbc96":"code","dc577497":"code","79320fb3":"code","8c97ddc5":"code","5591c458":"code","a996de37":"code","dd2fcc01":"code","7c1cf454":"code","54951633":"code","22f25cd0":"code","9f4e6f83":"code","515dd30d":"code","3109def4":"code","c9acf814":"code","4bd6ea29":"code","86b002f1":"code","11228de1":"code","7bf98cee":"code","f77d46ab":"code","599e4fbf":"code","f81168bf":"code","fb27ba6c":"code","e667e8aa":"code","1d5e8f3d":"code","161714fb":"code","19231d8f":"code","8351c61f":"code","691fe27a":"markdown","5d7fc8a5":"markdown","33d7412f":"markdown","16007348":"markdown","345fecbc":"markdown","c1384e8f":"markdown","e417d98e":"markdown","954727de":"markdown","062e9ef6":"markdown","fc2204a3":"markdown","29a6ce64":"markdown","1bf25ba1":"markdown","93012afb":"markdown","e66befcb":"markdown","6c53747d":"markdown","2ffb5e5c":"markdown","c90baaaf":"markdown","c6e395b1":"markdown","6f2dd1d9":"markdown","eb3effd1":"markdown","ed32cb54":"markdown","f76cf452":"markdown","b49dc98e":"markdown","5279a769":"markdown","eb943fcc":"markdown","8180fe1c":"markdown","c22504bf":"markdown","da6154fc":"markdown","b5e2bcc0":"markdown","022da3f4":"markdown","d1e13000":"markdown","dc528363":"markdown","4bd9b9e5":"markdown","d10deef5":"markdown","34acb428":"markdown","48aefc62":"markdown","21be1f0b":"markdown","354aa956":"markdown","9f3f54f7":"markdown","1b1ebb8d":"markdown","3b70477d":"markdown","230128e3":"markdown","75a7815d":"markdown","efd1336a":"markdown","19999976":"markdown","cb202bd5":"markdown","13f1d614":"markdown","b5c0846b":"markdown","6bee95b7":"markdown","035c4aaa":"markdown","5077fa85":"markdown","6eb2dae6":"markdown","557e97c8":"markdown","98f2d4a4":"markdown","5c5cb6a8":"markdown","8882e241":"markdown","8ef4fdc5":"markdown","1407e39b":"markdown","9ddf5e4d":"markdown","a5b6159e":"markdown","dbc8c59a":"markdown","fd27a9c0":"markdown","d4d7c703":"markdown","ca6471c8":"markdown","f73ddd7f":"markdown","815db3af":"markdown","a2944ba7":"markdown","daf3d22e":"markdown","d2cf5251":"markdown","a17e9365":"markdown","e3b510aa":"markdown","5af5a6e6":"markdown","ce5d9de9":"markdown","d4c22d1c":"markdown","cf95e693":"markdown","a3b904c1":"markdown","398dde3d":"markdown","426da959":"markdown","8988057f":"markdown","33172ba2":"markdown","cded5c93":"markdown","47e01f84":"markdown","c94273ff":"markdown","5685a6da":"markdown","244da950":"markdown","d4ca519f":"markdown","62e0352b":"markdown","ceb235c3":"markdown","a2817fc7":"markdown","126a6e70":"markdown","de99b4b9":"markdown","d7a3c79a":"markdown","e7cdcc65":"markdown","ec480aec":"markdown","c5156534":"markdown","1fdd1889":"markdown","255f731f":"markdown","d9bbc8c4":"markdown","85e0bba0":"markdown"},"source":{"d7ac1bc0":"import pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport random as rnd","954504bd":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC,LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","44d972df":"train_df = (\"..\/input\/titanic\/train.csv\")\ntest_df = (\"..\/input\/titanic\/test.csv\")","011e7fe3":"train_df = pd.read_csv(train_df)\ntest_df = pd.read_csv(test_df)","3900f20b":"combine = [train_df,test_df]","04f62f07":"train_df.tail()","3c112e5e":"train_df.info()","d9b0f13e":"train_df.isnull().sum()","a1dfe79b":"test_df.info()","8875314b":"test_df.isnull().sum()","40929f48":"train_df.shape","2114eef8":"test_df.shape","472c99ef":"train_df.describe(include='all')","2fa16732":"test_df.describe(include='all')","dedfb006":"def bar_chart(feature):\n    survived = train_df[train_df['Survived']==1][feature].value_counts()\n    notsurvived = train_df[train_df['Survived']==0][feature].value_counts()\n    df= pd.DataFrame([survived,notsurvived])\n    df.index = ['Survived','Not Survived']\n    df.plot(kind = 'bar',stacked=False,figsize=(9,5))","8b10a5db":"bar_chart('Pclass')","a8f7bb92":"bar_chart('Sex')","95dc007b":"women = train_df.loc[train_df.Sex=='female']['Survived']\nrate_women = sum(women)\/len(women)*100\nprint('women Survived',rate_women, '%')","927fe110":"men = train_df.loc[train_df.Sex=='male']['Survived']\nrate_men = sum(men)\/len(men)*100\nprint ('men survived',rate_men,'%')","5054ad62":"bar_chart('SibSp')","efa0fdb0":"bar_chart('Parch')","a761c741":"bar_chart('Embarked')","51b1c7ae":"train_df.columns","cbe850ed":"C_matrix = sns.heatmap(train_df[['Survived', 'Age', 'SibSp',\n       'Parch',  'Fare']].corr(),\n                       annot=True,fmt='.2f',cmap ='coolwarm')","4a4651c1":"age_sur = sns.FacetGrid(train_df, col = 'Survived')\nage_sur = age_sur.map(sns.distplot,'Age')","008d6043":"g = sns.kdeplot(train_df['Age'][(train_df['Survived']==0 & train_df['Age'].notnull())],\n                color = 'green',shade =  True)\ng = sns.kdeplot(train_df['Age'][(train_df['Survived']==1 & train_df['Age'].notnull())],\n                color = 'Red',shade =  True)\ng.set_xlabel('Age')\ng.set_ylabel('Frequency')\ng = g.legend(['Not Survived','Survived'])","2f1c83c6":"fig = plt.figure(figsize=(10,10))\nax1 = plt.subplot(2,1,1)\nax1 = sns.countplot(x = 'Pclass', hue = 'Survived', data = train_df)\nax1.set_title('Survival Rate')\nax1.set_xticklabels(['1 Upper','2 Middle','3 Lower'])\nax1.set_ylim(0,400)\nax1.set_xlabel('Ticket Class')\nax1.set_ylabel('Count')\nax1.legend(['No','yes'])","9d5003a3":"fig = plt.figure(figsize=(10,10))\nax2 = plt.subplot(2,1,2)\nsns.pointplot(x='Pclass',y='Survived',data = train_df)\nax2.set_xlabel('Ticket Class')\nax2.set_ylabel('Percent Survived')\nax2.set_title('Survival Percentage')","afd0af2d":"g= sns.factorplot(x='SibSp', y='Survived',data =train_df,kind = 'bar', palette = 'muted',size = 5)\n","00518a34":"g = sns.factorplot(x='Parch',y='Survived',data= train_df,kind = 'bar',size =5,palette='muted')","57f15f8b":"fig = plt.figure(figsize=(10,5))\nsns.swarmplot(x='Pclass',y='Fare',data=train_df,hue= 'Survived')\n","600b051b":"train_df = train_df.drop(['PassengerId'],axis=1)","824595fd":"sns.heatmap(train_df.isnull())","a8c1a3e6":"train_df.isnull().sum()","7d005e95":"import re\ndeck = {'A':1,'B':2,'C':3,'D':4,'E':5,'F':6,'G':7,'H':8}\ndata = [train_df,test_df]\n\nfor dataset in data:\n    dataset['Cabin'] = dataset['Cabin'].fillna('H0')\n    dataset['Deck']  = dataset['Cabin'].map(lambda x: re.compile('([a-z,A-Z]+)').search(x).group())\n    dataset['Deck']  = dataset['Deck'].map(deck)\n    dataset['Deck']  = dataset['Deck'].fillna(0)\n    dataset['Deck']  = dataset['Deck'].astype(int)","dcf07f96":"train_df = train_df.drop(['Cabin'],axis = 1)\ntest_df = test_df.drop(['Cabin'],axis = 1)","8400d54d":"age_df = [train_df,test_df]\nfor data in age_df:\n    mean = train_df['Age'].mean()\n    std = test_df['Age'].std()\n    is_null = data['Age'].isnull().sum()\n    # generate random numbers\n    age_rand = np.random.randint(mean - std,mean + std,size = is_null)\n    age_copy = data['Age'].copy()\n    age_copy[np.isnan(age_copy)] = age_rand\n    data['Age'] = age_copy\n    data['Age'] = train_df['Age'].astype(int)\n\n\n","9729c7ad":"train_df['Age'].isnull().sum()","5546449f":"train_df['Embarked'].describe()","f830709e":"top = 'S'\ndata = [train_df,test_df]\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].fillna(top)","8b4070e9":"train_df.info()","d0c6e9ff":"test_df.info()","3260d1da":"test_df['Fare'].isnull().sum()","e1660368":"data= [train_df,test_df]\nfor dataset in data:\n    dataset['Fare'] = dataset['Fare'].fillna(0)\n    dataset['Fare'] = dataset['Fare'].astype(int)","f04f2275":"test_df['Fare'].isnull().sum()","90886bed":"data = [train_df,test_df]\nTitle = {'Mr':1,'Miss':2,'Mrs':3,'Master':4,'Rare':5}\n\nfor dataset in data:\n    dataset['Title'] = dataset.Name.str.extract('([A-Za-z]+)\\.',expand = False)\n#Replace title with more common one\n    dataset['Title'] = dataset['Title'].replace(['Lady','Countess','Capt','Col','Don','Dr', \n                                                'Major','Rev','Sir','Jonkheer','Dona'],'Rare')\n    dataset['Title'] = dataset['Title'].replace('Ms','Miss')\n    dataset['Title'] = dataset['Title'].replace('Mlle','Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme','Mrs')\n    dataset['Title'] = dataset['Title'].map(Title)\n    dataset['Title'] = dataset['Title'].fillna(0)","fe661488":"dataset['Title'].unique()","4fdbab80":"train_df = train_df.drop(['Name'],axis = 1)\ntest_df = test_df.drop(['Name'],axis = 1)","80c858a3":"gender = {'male':0,'female':1}\ndata = [train_df,test_df]\n\nfor dataset in data:\n    dataset['Sex'] =dataset['Sex'].map(gender)","7cc559f6":" train_df['Ticket'].describe()","74f36efd":"train_df = train_df.drop(['Ticket'],axis = 1)\ntest_df = test_df.drop(['Ticket'],axis = 1)","5ff285ce":"port = {'S':0,'C':1,'Q':2}\ndata = [train_df,test_df]\n\nfor dataset in data:\n    dataset['Embarked'] =dataset['Embarked'].map(port)","f61509d8":"data = [train_df,test_df]\nfor dataset in data:\n    dataset['Age'] = dataset['Age'].astype(int)\n    dataset.loc[dataset['Age'] <=11,'Age'] =0\n    dataset.loc[(dataset['Age'] >11) & (dataset['Age']<=18),'Age'] = 1\n    dataset.loc[(dataset['Age'] >18) & (dataset['Age']<=22),'Age'] = 2\n    dataset.loc[(dataset['Age'] >22) & (dataset['Age']<=27),'Age'] = 3\n    dataset.loc[(dataset['Age'] >27) & (dataset['Age']<=33),'Age'] = 4\n    dataset.loc[(dataset['Age'] >33) & (dataset['Age']<=40),'Age'] = 5\n    dataset.loc[(dataset['Age'] >40) & (dataset['Age']<=66),'Age'] = 6\n    dataset.loc[dataset['Age']>66,'Age'] = 6\n    \n    ","ddb04394":"train_df['Age'].value_counts()","2f1a7307":"train_df.head(10)","f2273745":"plt.hist(train_df['Fare'],bins = 30)\nplt.xlabel('Fare')\nplt.ylabel('Count')\nplt.title('Distribution of Fare')\nplt.show()","e87983ff":"train_df['Fare'] = pd.qcut(train_df['Fare'],4)\nlbl = LabelEncoder()\ntrain_df['Fare'] = lbl.fit_transform(train_df['Fare'])\n    ","aef39f12":"test_df['Fare'] = pd.qcut(test_df['Fare'],4)\nlbl = LabelEncoder()\ntest_df['Fare'] = lbl.fit_transform(test_df['Fare'])","fb402faf":"plt.hist(train_df['Fare'],bins = 30)\nplt.xlabel('Fare')\nplt.ylabel('Count')\nplt.title('Distribution of Fare')\nplt.show()","23849a1f":"#train_df[\"Fare\"] = train_df[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)","a4f8f808":"#test_df[\"Fare\"] = test_df[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)","5a3eebb8":"data = [train_df,test_df]\nfor dataset in data:\n    dataset['Age_Pclass'] = dataset['Age'] * dataset['Pclass']","32bde6d4":"data = [train_df,test_df]\nfor dataset in data:\n    dataset['Relative'] = dataset['SibSp']+dataset['Parch']","c376735d":"data = [train_df,test_df]\nfor dataset in data:\n    dataset['Fare_Person'] = dataset['Fare']\/(dataset['Relative']+1)\n    dataset['Fare_Person'] = dataset['Fare_Person'].astype(int)","2b789f35":"train_df.head(10)","f3005aef":"X_train = train_df.drop('Survived',axis = 1)\ny_train = train_df['Survived']\nX_test = test_df.drop('PassengerId',axis = 1).copy()","444bb6ef":"# Logisitic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train,y_train)\ny_pred = logreg.predict(X_test)\n","1eab6a89":"#Checking the accuracy\nlogistic_accuracy = round(logreg.score(X_train,y_train)*100,2)\nprint(round(logistic_accuracy,2),'%')","af0842c7":"#Decesion Tree\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train,y_train)\n\ny_pred = decision_tree.predict(X_test)\n","df9bca53":"decision_tree_accuracy = round(decision_tree.score(X_train,y_train) * 100,2)\nprint(round(decision_tree_accuracy,2),'%')","5b1195fe":"# Perceptron\nperceptron = Perceptron(max_iter=5)\nperceptron.fit(X_train,y_train)\n\ny_pred = perceptron.predict(X_test)","e21081c3":"perceptron_accuracy = round(perceptron.score(X_train,y_train)* 100,2)\nprint(round(perceptron_accuracy,2),'%')","6e6d93b3":"# Randon Forest\nrand_forest = RandomForestClassifier(n_estimators=100)\nrand_forest.fit(X_train,y_train)\n\ny_pred = rand_forest.predict(X_test)","df7e8711":"rand_forest_accuracy = round(rand_forest.score(X_train,y_train)*100,2)\nprint(round(rand_forest_accuracy,2),'%')","21df926f":"# Naive Bayes\ngaussian = GaussianNB()\ngaussian.fit(X_train,y_train)\n\ny_pred = gaussian.predict(X_test)","15993ee6":"gaussian_accuracy = round(gaussian.score(X_train,y_train)*100,2)\nprint(round(gaussian_accuracy,2),'%')","9f0fbc96":"#KNN\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train,y_train)\n\ny_pred = knn.predict(X_test)","dc577497":"knn_accuracy = round(knn.score(X_train,y_train)*100,2)\nprint(round(knn_accuracy,2),'%')","79320fb3":"#LinearSVC\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train,y_train)\n\ny_pred = linear_svc.predict(X_test)","8c97ddc5":"linear_svc_accuracy = round(linear_svc.score(X_train,y_train)*100,2)\nprint(round(linear_svc_accuracy,2),'%')","5591c458":"#SVC\nsvc = SVC(gamma='auto')\nsvc.fit(X_train,y_train)\n\ny_pred = svc.predict(X_test)","a996de37":"svc_accuracy = round(svc.score(X_train,y_train)*100,2)\nprint(round(svc_accuracy,2),'%')","dd2fcc01":"coeff_df = pd.DataFrame(train_df.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df['Correlation'] = pd.Series(linear_svc.coef_[0])","7c1cf454":"coeff_df.sort_values(by='Correlation',ascending=False)","54951633":"model_evaluation = pd.DataFrame({\n    'Model':['LogisticRegression','DecisionTreeClassifier','Perceptron','RandomForestClassifier',\n             \n             'GaussianNB','KNeighborsClassifier','LinearSVC','SVC'],\n    \n    'Score':[logistic_accuracy,decision_tree_accuracy,perceptron_accuracy,rand_forest_accuracy,\n             gaussian_accuracy,knn_accuracy,linear_svc_accuracy,svc_accuracy,]})\nmodel_evaluation.sort_values(by='Score',ascending = False)","22f25cd0":"from sklearn.model_selection import cross_val_score\nrand_forest = RandomForestClassifier(n_estimators=100)\nscore = cross_val_score(rand_forest,X_train,y_train,cv = 10,scoring='accuracy')","9f4e6f83":"print('Score',score)\nprint('Mean',round(score.mean()*100),2)\nprint('Satandered Deviation',score.std())","515dd30d":"from sklearn.model_selection import cross_val_score\n\naccuracy =cross_val_score(estimator=rand_forest,X=X_train,y=y_train,cv= 10)\n\naccuracy.mean()","3109def4":"accuracy.std()","c9acf814":"rand_forest.fit(X_train,y_train)\nimportance = pd.DataFrame({\n    'Feature':X_train.columns,'importance':np.round(rand_forest.feature_importances_,3)})\nimportance = importance.sort_values('importance',ascending=False).set_index('Feature')","4bd6ea29":"importance","86b002f1":"importance.plot.bar()","11228de1":"train_df = train_df.drop('Parch',axis = 1)\ntest_df = test_df.drop('Parch',axis = 1)","7bf98cee":"#Train Random Forest again\nrand_forest = RandomForestClassifier(n_estimators=100,oob_score= True)\nrand_forest.fit(X_train,y_train)\n\ny_pred = rand_forest.predict(X_test)\n","f77d46ab":"rand_forest.score(X_train,y_train)\nrand_forest_accuracy = round(rand_forest.score(X_train,y_train)*100,2)\nprint('Accuracy: ',round(rand_forest_accuracy,2),'%')","599e4fbf":"print('OOB Score',round(rand_forest.oob_score_,4)*100,'%')","f81168bf":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\nprediction = cross_val_predict(rand_forest,X_train,y_train,cv = 3)\nconfusion_matrix(y_train,prediction)","fb27ba6c":"from sklearn.metrics import precision_score,recall_score\nprint('Percision : ',precision_score(y_train,prediction))\nprint('Recall :',recall_score(y_train,prediction))","e667e8aa":"from sklearn.metrics import f1_score\nf1_score(y_train,prediction)","1d5e8f3d":"y_scores = rand_forest.predict_proba(X_train)","161714fb":"y_scores = y_scores[:,1]","19231d8f":"from sklearn.metrics import roc_auc_score\nr_a_score = roc_auc_score(y_train,y_scores)\nprint('ROC-AUC-Score: ',r_a_score)","8351c61f":"submission = pd.DataFrame({\n    'PassengerId':test_df['PassengerId'],\n    'Survived':y_pred\n    })\nsubmission.to_csv('submission.csv',index = False)","691fe27a":"Let us see the distribution Fare value","5d7fc8a5":"### Model Evaluation \u2af2<a id='section11'><\/a> <a href=\"#top\"> Back to Top\u25b2<\/a>","33d7412f":"After removing 'Parch' feature,Random forest classifier got good accuracy.Non significant features will lead to overfitting.\n","16007348":"###### Cabin","345fecbc":"######  Crating function for barchart","c1384e8f":"![9fb8dc8cc5a0f8e2c17d6ccc1f952f38.png](attachment:9fb8dc8cc5a0f8e2c17d6ccc1f952f38.png)","e417d98e":"### Create Machine Learning Model for Prediction \u2af2<a id='section10'><\/a> <a href=\"#top\"> Back to Top\u25b2<\/a>\n","954727de":"<html>\n  <body>\n     <a id=\"top\"><\/a>","062e9ef6":"Sex is highest positive coeffficient and second Parch","fc2204a3":"The F1 Score is used to measure test's accuracy and it balance the use of precision and recall to do it.it consider both precision and recall of the test to compute the score.","29a6ce64":"The score is good enough to submit, so we go for final step","1bf25ba1":"* Pclass---- (1 = 1st, 2 = 2nd, 3 = 3rd)\n* Sex------- (Male or Female)\n* SibSp----- (Number of siblings and spouse)\n* ParCh----- (Number of parents and children)\n* Embarked-(Port of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)\n* Cabin----- (cabin Number)","93012afb":"###### Fare per Person","e66befcb":"Log Transformation","6c53747d":"### Handling Categorical features \u2af2<a id='section8'><\/a> <a href=\"#top\"> Back to Top\u25b2<\/a>","2ffb5e5c":"### Data variable notes \u2af2<a id='section3'><\/a> <a href=\"#top\"> Back to Top\u25b2<\/a>\n","c90baaaf":"Notice the deviation of passengers with parents and children cont 3.","c6e395b1":"Cross validation is very useful technique for assessing the performance of \nmachine learning models.","6f2dd1d9":"We will analyse the question: \u201cwhat sorts of people were more likely to survive?\u201d through Visualisations,further will go with Data preprocessing,Feature Engineering,Building Machine Learning Model,finally Prediction and Submission.","eb3effd1":"### Conclusion \u2af2<a id='section20'><\/a> <a href=\"#top\"> Back to Top\u25b2<\/a>","ed32cb54":"OOB error also called out of bag estimate is used for measuring the prediction error in random Forest","f76cf452":"##### SibSp","b49dc98e":"### Getting Dataset  \u2af2<a id='section2'><\/a> <a href=\"#top\"> Back to Top\u25b2<\/a>","5279a769":"High precision means that algoritham returns more relevant result than irrelevant one(Percentage of result which are relevant).While high recall means that algoritham returned most of the relevant result(Percentage of total relevant result correctly classified by algoritham).","eb943fcc":"Noticed that peak for survival of children aged between 0 and 5","8180fe1c":"This model **Scored 0.7799** in Leaderboard and **Top 38%**.Always welcomes your valuable comments to improve the model.\n\nIf you found this notebook helpful or you just liked it,some \ud83d\udc4d**Upvotes**\ud83d\udc4d would be very much appreciated \ud83d\ude0a","c22504bf":"K-Fold cross validation is common type of cross validation is performed by partitioning the original training data set in to k equal subset.Each sub set is called Fold.The result of K-Fold cross validation would be k differenet scores.We then need to compute the mean and standered deviation of these scores.","da6154fc":"### Overview","b5e2bcc0":"The ROC AUC score is the corresponding score to the ROC AUC Curve.It is simply calculated by measuring the area under the curve,which is called AUC.","022da3f4":"Since Embarked have only 2 missing value we are trying to fill with most common one.","d1e13000":"###### Embarked","dc528363":"###### Fare","4bd9b9e5":"**Calculating % of Woman and Men survived**","d10deef5":"### Submission \u2af2<a id='section19'><\/a> <a href=\"#top\"> Back to Top\u25b2<\/a>\n","34acb428":"### Create new  Features \u2af2<a id='section9'><\/a> <a href=\"#top\"> Back to Top\u25b2<\/a>","48aefc62":"To check the correlation of other features we need to explore more","21be1f0b":"This didstribution is leftside skew,to resolve this there are two possible approches.\n<br>\n1,Barke down Fare feature in to equal bins.\n<br>\n2,Log transformation for normal distribution.\n<br>\nLet us choose the first one,because after using Log transformation, F1 Score going down,for that we can use qcut function","354aa956":"There are 4 categorical features let us convert one by one along with 'Fare' as it is float.","9f3f54f7":"### Feature Analysis \u2af2<a  id='section5'><\/a> <a href=\"#top\"> Back to Top\u25b2<\/a>","1b1ebb8d":"Precision is the number of correct positive results divided by the number of all positive results,and Recall is number of correct positive results divided by the number of positive results that should have been returned.The F1 score can be interpreted as a weighted average of the precision and recall,where F1 Score reaches its best value at 1 and worst at 0.","3b70477d":"Passenger with more siblings or spouse have less chance to survive","230128e3":"#### Precision and Recall \u2af2<a id='section16'><\/a> <a href=\"#top\"> Back to Top\u25b2<\/a>","75a7815d":"#### OOB(Out of bag) \u2af2<a id='section14'><\/a> <a href=\"#top\"> Back to Top\u25b2<\/a>","efd1336a":"### Check null  and missing value \u2af2 <a id='section4'><\/a>  <a href=\"#top\"> Back to Top\u25b2<\/a>","19999976":"Survival rate vs Siblings or Spouse on Board","cb202bd5":"##### Embarked(Port of Embarkation)\nC = Cherbourg, Q = Queenstown, S = Southampton\n","13f1d614":"##### In both Survived and Not survived section shows that passenders are embarked from Southampton(S)\n\n\n","b5c0846b":"Pclass contain highest fare have highest level of survival","6bee95b7":"#### F1 Score \u2af2<a id='section17'><\/a> <a href=\"#top\"> Back to Top\u25b2<\/a>\n","035c4aaa":"We can compute the score of each feature to drop any unwanted features.The total impoprtance score value should be 1.","5077fa85":"##### Age ","6eb2dae6":"Now we can calculate the coefficient of features to validate our decesion for feature creation","557e97c8":"Same can be done as follows:","98f2d4a4":"### Contents<a name=\"top\"><\/a>","5c5cb6a8":"###### Fare","8882e241":"Create new feature using title of Name","8ef4fdc5":"This shows that Random Forest model has acccuracy of 81% with standered deviation of 3.9% .\nso that accuracy of our model can differ approximate + or- 3.9%.","1407e39b":"#### K-Fold cross Validation \u2af2<a id='section12'><\/a> <a href=\"#top\"> Back to Top\u25b2<\/a>","9ddf5e4d":"###### Fare","a5b6159e":"##### Survived- People with no families with them likely survived\n##### Not Survived- People with no families are likely not survived","dbc8c59a":"As per the above details we Parch does not play significant role,so we will remove.","fd27a9c0":"### Correlation \u2af2<a id='section6'><\/a> <a href=\"#top\"> Back to Top\u25b2<\/a>\n\n###### Correlation matrix for showing correlation coeficients between variables","d4d7c703":"##### Survived- The first class people more likely survived\n##### Not Survived- The third class people more likely not survived","ca6471c8":"Now we can evaluate our model to choose the best one for our problem","f73ddd7f":"As there is 681 unique count ,we will drop ticket feature from our dataset.","815db3af":"###### Ticket","a2944ba7":"#### Feature Importance \u2af2  <a id='section13'><\/a> <a href=\"#top\"> Back to Top\u25b2<\/a>\n","daf3d22e":"###### Fare have a significant correlation with  Survival probability","d2cf5251":"We need to drop 'PassengerId' from Train set as it does not contribute any value to survival probability.","a17e9365":"Cabin value is mapped to new feature( column 'Deck' )with numerical value,So we can drop Cabin column for Train set and Test set","e3b510aa":"As per our  analysis we have to deal with below columns\nAge 177 missing values,  Cabin 684 missing values,  Embarked 2 missinig values .\n","5af5a6e6":"##### pclass: \n1st = Upper,\n2nd = Middle,\n3rd = Lower\n\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\nsibsp: The dataset defines family relations in this way..\nSibling = brother, sister, stepbrother, stepsister\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\nparch: The dataset defines family relations in this way..\nParent = mother, father\nChild = daughter, son, stepdaughter, stepson\nSome children travelled only with a nanny, therefore parch=0 for them.","ce5d9de9":"#### Confusion Metrix \u2af2<a id='section15'><\/a> <a href=\"#top\"> Back to Top\u25b2<\/a>","d4c22d1c":"#### Age","cf95e693":"[1. **Importing the Libraries**](#section1)\n<br>\n[2. **Getting Data Set**](#section2)\n<br>\n[3. **Data variable Notes**](#section3)\n<br>\n[4. **Check null  and Missing value**](#section4)\n<br>\n[5. **Feature Analysis & Visualization**](#section5)\n<br>\n[6. **Correlation**](#section6)\n<br>\n[7. **Feature Engineeering**](#section7)\n<br>\n[8. **Handling Categorical Features**](#section8)\n<br>\n[9. **Create New Features**](#section9)\n<br>\n[10. **Create Machine Learning Model for Prediction**](#section10)\n<br>\n[11. **Model evaluation**](#section11)<br>\n[ \u2a00 K-Fold cross Validation](#section12)<br>\n[\u2a00  Feature Importance](#section13)<br>\n[ \u2a00 OOB(Out of bag)](#section14)<br>\n[ \u2a00 Confusion Metrix](#section15)<br>\n[ \u2a00 Precision and Recall](#section16)<br>\n[\u2a00  F1 Score](#section17)<br>\n[\u2a00  ROC AUC Score](#section18)<br>   \n[12. **Submission**](#section19)<br>\n[13. **Conclusion**](#section20)\n","a3b904c1":"### Importing the Libraries \u2af2 <a id='section1'><\/a> <a href=\"#top\"> Back to Top\u25b2<\/a>","398dde3d":"As per the model evaluation Decesion tree and Random Forest showing the highest score.Let us eavluate the performanceof Random Forest with K-Fold cross validation.","426da959":"Letus handle each missing value column","8988057f":"###### Embarked","33172ba2":"###### New feature Age Group","cded5c93":"###### Age Multiplied with Pclass","47e01f84":"The higher the class, more likely to have survived.","c94273ff":"###### Explore Age distribution with kde plot","5685a6da":"F1 Score is not much high, so we go for ROC_AUS_Score","244da950":"###### Sex","d4ca519f":"###### Parch","62e0352b":"485 passerngers were correctly classified as not Survived(true Negative),239 passengeres were correctly classified as Survived(True Positive),64 passengeres were wronly classified as Not Survived(False nagative),98 Passengeres were wrongly classified as Survived(false positive)","ceb235c3":"##### Survived- People with more than two siblings or spouse are likely survived\n##### Not Survived- People with no siblings or spouse(single) are likely not survived","a2817fc7":"#### ROC AUC Score \u2af2<a id='section18'><\/a> <a href=\"#top\"> Back to Top\u25b2<\/a>","126a6e70":"### Feature Engineering \u2af2<a id='section7'><\/a> <a href=\"#top\"> Back to Top\u25b2<\/a>","de99b4b9":"###### Add relative column","d7a3c79a":"From this you can see that almost 75% of the women on board survived, whereas only 19% of the men lived to tell about it. Gender seems to be such a strong indicator of survival,","e7cdcc65":"Other feature 'Age' have missing value will be calculated with mean,is_null and standered deviation of Age.","ec480aec":"We notice that age distributions are not the same in survived  and notsurvived population.\nPassenger between 60-80 have less survived.\nIt seems that young passengers have more chance to survive.","c5156534":"##### Survived- Females are more likely survived\n##### Not Survivied- Males are more likely Not survived","1fdd1889":"#### Create bar chart for below categorical features","255f731f":"The challenge is we have to use machine learning to create a model that predicts which passengers survived the Titanic shipwreck. <br>\n          We have been provided with two type of dataset training set (train.csv) and test set (test.csv).So far we have gone through data cleaning,handling missing values , feature engineering and visualization to understand importance of each features so that we could get best result out of from our data set.\nThe training set should be used to build your machine learning models and it is provided with input(Independent variable) and output(dependent variable) means passengers Survived.in our case X_train is our input and y_train is our output.<br>\nOnce we trained our model we will predict test set (test.csv) in our case 'X_test' and we will get predicted value in our case 'y_pred'.We are checking accuracy of our prediction for each model and we are submitting the best one to the Kaggle.","d9bbc8c4":"###### Pclass","85e0bba0":"###### Missing Data"}}