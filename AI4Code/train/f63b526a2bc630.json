{"cell_type":{"d11ab84f":"code","01537f13":"code","47e1be84":"code","c5fe3f9a":"code","c8c7b7fe":"code","9b9c719f":"code","3a17d734":"code","9bd0524e":"code","5568b27b":"code","8423f7af":"code","287941d1":"code","2abaff3e":"code","6bc92f02":"code","74fd405d":"code","ae4bac31":"code","377f377a":"code","4adba8a7":"code","b8817626":"code","4682ed57":"code","cff92ea1":"code","244eda04":"code","c0124b8e":"code","2c8dff43":"code","04b7179b":"code","2bdf9d74":"code","f39efee7":"code","781dac9a":"code","117fa7da":"code","582e55b8":"code","e3d73fd7":"code","664347c1":"code","fafe215f":"code","0fe963fc":"code","09bf1e4a":"code","cc02572d":"code","4c76feb1":"code","12bd4db9":"code","c6174e6c":"code","94840abc":"code","2319c932":"code","a4c8f493":"code","8b361565":"code","b7004b0f":"code","b55222ef":"code","efc64900":"code","308fd2b0":"code","9bd97148":"code","062483a1":"markdown","4cd32ed5":"markdown","0fb7f10c":"markdown","bef8f154":"markdown","a8169647":"markdown","924ce0fa":"markdown","aa31ce00":"markdown","66f7b9b0":"markdown"},"source":{"d11ab84f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","01537f13":"import matplotlib.pyplot as plt\nimport seaborn as sns","47e1be84":"train = pd.read_csv(\"..\/input\/santander-customer-satisfaction\/train.csv\") \ntest = pd.read_csv(\"..\/input\/santander-customer-satisfaction\/test.csv\") ","c5fe3f9a":"# over 370 columns in data\ntrain.shape, test.shape","c8c7b7fe":"#most of columns are hard to know \nnp.array(train.columns.to_list()[:10])","9b9c719f":"# some of values are quite big ex) imp_ent_var16_ult1, imp_op_var39_comer_ult1\n# some of values are only 0 ex) ind_var2_0\n# some of values are seem like percent ex) ind_var13\npd.options.display.max_columns = 380\ntrain.describe()","3a17d734":"cols = train.columns","9bd0524e":"list(set([col.split('_')[0] for col in cols]))","5568b27b":"#some columns have \"special\" tag on front\nind_cols = cols[[col.split('_')[0] == 'ind' for col in cols]]\nimp_cols = cols[[col.split('_')[0] == 'imp' for col in cols]]\nsaldo_cols = cols[[col.split('_')[0] == 'saldo' for col in cols]]\ndelta_cols = cols[[col.split('_')[0] == 'delta' for col in cols]]\nnum_cols = cols[[col.split('_')[0] == 'num' for col in cols]]\nlen(ind_cols), len(imp_cols), len(saldo_cols), len(delta_cols), len(num_cols)","8423f7af":"#'YES' there are special tag in colunms\n# 'ind' are 0 or 1\ntrain[ind_cols].describe()","287941d1":"#'imp' are quite big gap between min and max and most of values are 0\ntrain[imp_cols].describe()","2abaff3e":"#'saldo' similar 'imp' are quite big gap between min and max\ntrain[saldo_cols].describe()","6bc92f02":"#'delta' have same max_values\ntrain[delta_cols].describe()","74fd405d":"#'num' num_with var45 is little bit more special than others\ntrain[num_cols].describe()","ae4bac31":"# TARGET is imblanced\ntrain.TARGET.value_counts()","377f377a":"# no nan value\ntrain.isna().sum().sum()","4adba8a7":"#-9999999 seem like null value, have to replace common value '2'\ntrain.var3.value_counts()","b8817626":"# will do later\n# train.loc[train.var3 == -999999, 'var3'] = 2\n# test.loc[test.var3 == -999999, 'var3'] = 2","4682ed57":"#not bad... some people think var15 is age\ntrain.var15.hist()","cff92ea1":"# most of low_value are satisfied customers\nsns.FacetGrid(train, hue= 'TARGET',height= 5).map(sns.kdeplot, 'var15').add_legend()","244eda04":"# 99 is seem like null values\ntrain.var36.value_counts()","c0124b8e":"# 99... half and half\nsns.FacetGrid(train, hue= 'TARGET',height= 5).map(sns.kdeplot, 'var36').add_legend()","2c8dff43":"# most of 3 are TARGET 0\ntemp = train[train.var36 !=99]\nsns.FacetGrid(temp, hue= 'TARGET',height= 5).map(sns.kdeplot, 'var36').add_legend()","04b7179b":"# most of are 0\ntrain.var21.value_counts()","2bdf9d74":"# var21 0 is 0 but hard to guess\ntemp = train[train.var21 ==0]\ntemp.TARGET.value_counts()","f39efee7":"# most of 3 are TARGET 0\ntemp = train[train.var21 !=0]\nsns.FacetGrid(temp, hue= 'TARGET',height= 5).map(sns.kdeplot, 'var21').add_legend()","781dac9a":"# most of are 0\ntrain.var38.value_counts()","117fa7da":"#left-skewed ditribution... need to normalize\ntrain.var38.hist()","582e55b8":"# look better\ntrain.var38 = train.var38.apply(lambda x : np.log(x))\ntest.var38 = test.var38.apply(lambda x : np.log(x))\ntrain.var38.hist()","e3d73fd7":"def prerpocessing(data):\n    #var3 -999999 to 2\n    data.loc[data.var3 == -999999, 'var3'] = 2\n    #var38 to log\n    data.var38 = data.var38.apply(lambda x : np.log(x))\n    return data","664347c1":"train = pd.read_csv(\"..\/input\/santander-customer-satisfaction\/train.csv\") \ntest = pd.read_csv(\"..\/input\/santander-customer-satisfaction\/test.csv\") \npre_train = prerpocessing(train)\npre_test= prerpocessing(test)","fafe215f":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier","0fe963fc":"lr = LogisticRegression(random_state= 86)\nrc = RandomForestClassifier(random_state= 86)\nxg = XGBClassifier(eval_metric= 'auc',random_state= 86)\nlc = LGBMClassifier(random_state= 86)","09bf1e4a":"y = train.TARGET\nX = train.drop(['ID','TARGET'], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X,y,stratify=y,test_size = 0.2, random_state= 86)","cc02572d":"lr.fit(X_train,y_train)\nlr_pred = lr.predict_proba(X_test)\nlr_result = roc_auc_score(y_test, lr_pred[:,1])\nprint(lr_result)","4c76feb1":"# most important are so small to remove som\nlr_fi = pd.DataFrame(np.abs(lr.coef_[0]), columns = ['fi'], index=X.columns).sort_values('fi',ascending= False)\nlr_fi.head(10)","12bd4db9":"rc.fit(X_train, y_train)\nrc_pred = rc.predict_proba(X_test)\nrc_result = roc_auc_score(y_test, rc_pred[:,1])\nprint(rc_result)","c6174e6c":"# most important columns are var38, var15\nrc_fi = pd.DataFrame(rc.feature_importances_, columns = ['fi'], index=X.columns).sort_values('fi',ascending= False)\nrc_fi.head(10)","94840abc":"%%time\nxg.fit(X_train, y_train)\nxg_pred = xg.predict_proba(X_test)\nxg_result = roc_auc_score(y_test, xg_pred[:,1])\nprint(xg_result)","2319c932":"# most_columns are not important. have to remove some \nxg_fi = pd.DataFrame(xg.feature_importances_, columns = ['fi'], index=X.columns).sort_values('fi',ascending= False)\nxg_fi.head(10)","a4c8f493":"%%time\nlc.fit(X_train, y_train)\nlc_pred = lc.predict_proba(X_test)\nlc_result = roc_auc_score(y_test, lc_pred[:,1])\nprint(lc_result)","8b361565":"# most_columns are not important. have to remove some \nlc_fi= pd.DataFrame(lc.feature_importances_, columns = ['fi'], index=X.columns).sort_values('fi',ascending= False)\nlc_fi.head(10)","b7004b0f":"lr_fi.columns = ['lr']\nrc_fi.columns = ['rc']\nlc_fi.columns = ['lc']\nxg_fi.columns = ['xg']\nfi_df = lr_fi.merge(rc_fi, right_index = True, left_index= True)\nfi_df = fi_df.merge(xg_fi, right_index = True, left_index= True)\nfi_df = fi_df.merge(lc_fi, right_index = True, left_index= True)","b55222ef":"#RF, lightgbm are little bit similar\nfi_df.sort_values('lc',ascending= False)","efc64900":"lc.fit(X,y)\ntest_X = test.drop('ID',axis= 1)\ntest_pred = lc.predict_proba(test_X)","308fd2b0":"submission = pd.DataFrame({'ID':test.ID,'TARGET':test_pred[:,1]})\nsubmission.to_csv('submission.csv',index = False)","9bd97148":"submission['TARGET'].sum()","062483a1":"### lg ","4cd32ed5":"# See each columns as I can","0fb7f10c":"### xg","bef8f154":"### Logistic regreesion","a8169647":"### try baseline lightgbm model","924ce0fa":"# try baseline","aa31ce00":"### Subject\nFor each ID in the test set, you must predict a \"probability\" for the TARGET variable. The file should contain a header and have the following format\n-> The important thing is probability.. so actually isn't classification contest","66f7b9b0":"### RF"}}