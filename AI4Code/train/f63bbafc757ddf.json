{"cell_type":{"531d0d6b":"code","c474c585":"code","173e7593":"code","f4d6f72f":"code","ce778c5b":"code","e26486f1":"code","1af88ae3":"code","6b7aee23":"code","719c162b":"code","2eaa2fed":"code","bbfe7768":"code","a34f0d5d":"code","6fe5acf0":"code","248e2555":"code","9069004d":"code","bd590a76":"code","a13eea0a":"code","d3798831":"code","2bbf7516":"code","6ae9714b":"code","ef304ff1":"code","cf26b76a":"code","32b2a52b":"code","2cf7d9d6":"code","e48f2984":"code","f885df5e":"code","76c18995":"code","d9e47d5f":"code","e9dea901":"code","5c8e8d4a":"code","d6f91cbd":"code","33f29fde":"code","dca6dafa":"code","edb3b6c5":"code","dd99f7ac":"code","e66ba669":"code","d19fb77e":"code","9b32df63":"code","50c85206":"code","a576f280":"code","75a2cda2":"code","6f703fe2":"code","dceb7f6c":"code","486a2e82":"code","6bfa076b":"code","46dc8d8b":"code","bf8eb4ce":"code","8dec148f":"code","c36d4afb":"code","b9ef7d55":"code","b3329457":"code","5e31d48d":"markdown","85f0b465":"markdown","d3eaa100":"markdown","8f937a32":"markdown","2688a191":"markdown","8a77a4fa":"markdown","dd5419d7":"markdown","4fc7c700":"markdown","fb974f04":"markdown","1f685e59":"markdown","4282f9d3":"markdown","4bd65dbb":"markdown","92bd1e6a":"markdown","fae329d9":"markdown","79fd58e0":"markdown","09fc0973":"markdown","824a041d":"markdown","3f034fef":"markdown","556a967b":"markdown","5a23644b":"markdown","1cfc4c86":"markdown","3652a9dc":"markdown","b290f99c":"markdown","72047a54":"markdown","98aed9c0":"markdown","ce7b121e":"markdown","abfe0bc7":"markdown","9d70f192":"markdown","ef7ad6bf":"markdown","21004874":"markdown","d3716e93":"markdown","54aae021":"markdown","e2b7104e":"markdown"},"source":{"531d0d6b":"import math\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom datetime import date\nimport numpy as np\nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c474c585":"dataset = pd.read_csv('\/kaggle\/input\/marketing-data\/marketing_data.csv')\ndataset.head()","173e7593":"dataset.shape","f4d6f72f":"dataset.columns","ce778c5b":"dataset.columns[dataset.isnull().sum() > 0] # Income is the only columns with null values","e26486f1":"dataset.columns = dataset.columns.str.replace(' ', '') # Removing unwanted spaces from all columns\ndataset.Income.isnull().sum() # 24 null values","1af88ae3":"dataset.Income.str.contains('$').sum()","6b7aee23":"dataset.Income = dataset.Income.str.replace('$', '') \ndataset.Income = dataset.Income.str.replace(' ', '') # Couldnt convert to float since there were some spaces \ndataset.Income = dataset.Income.str.replace(',', '') # Couldnt convert to float cause of ,\ndataset['Income'] = dataset['Income'].astype(float)","719c162b":"print(dataset['Education'].value_counts())\nprint(dataset['Country'].value_counts())","2eaa2fed":"grouped = dataset.groupby(['Country','Education']) \ngrouped = grouped.agg(np.mean)\ngrouped['Income']","bbfe7768":"for i in dataset.loc[dataset.Income.isnull()].index:\n    dataset.iloc[i,4] = math.floor(grouped.loc[dataset.iloc[i][27],dataset.iloc[i][2]][2])\n    # Income for nan at i pos = floor(grouped_data(Country at i, Education at i)[2])\ndataset.Income.isnull().sum()","a34f0d5d":"dataset.nunique()","6fe5acf0":"dataset.Marital_Status.value_counts()","248e2555":"dataset['Marital_Status'] = dataset['Marital_Status'].replace(['Alone','YOLO','Absurd'],'Single')\ndataset.Marital_Status.value_counts()","9069004d":"num_coln = dataset.select_dtypes(include=np.number).columns.tolist()\nbins=10\nj=1\nfig = plt.figure(figsize = (20, 30))\nfor i in num_coln:\n    plt.subplot(7,4,j)\n    plt.boxplot(dataset[i])\n    j=j+1\n    plt.xlabel(i)\n    plt.legend()\nplt.show()","bd590a76":"dataset.drop(dataset[(dataset['Income']>200000)|(dataset['Year_Birth']<1920)].index,inplace=True)","a13eea0a":"dataset.rename(columns = {'Year_Birth':'Age'}, inplace = True)\ndataset['Age'] = dataset.Age.apply(lambda x: 2021-x)","d3798831":"dataset['MntTotal'] = np.sum(dataset.filter(regex='Mnt'), axis=1)","2bbf7516":"dataset['TotalPurchases'] = np.sum(dataset.filter(regex='Purchases'),axis=1)","6ae9714b":"dataset['AvgWeb'] = round(dataset['NumWebPurchases']\/dataset['NumWebVisitsMonth'],2)\ndataset.fillna({'AvgWeb' : 0},inplace=True) # Handling for cases where division by 0 may yield unwanted results\ndataset.replace(np.inf,0,inplace=True)","ef304ff1":"dataset['Children'] = dataset['Kidhome'] + dataset['Teenhome']","cf26b76a":"dataset['Dt_Customer'] = pd.to_datetime(dataset['Dt_Customer'] )","32b2a52b":"dataset.rename(columns = {'Dt_Customer':'TotalEnrollDays'}, inplace = True)\ndataset['TotalEnrollDays'] = pd.to_datetime(date.today()) - dataset['TotalEnrollDays']\ndataset['TotalEnrollDays'] = [int(str(dataset['TotalEnrollDays'][x])[:4]) for x in dataset.index]","2cf7d9d6":"dataset.head()","e48f2984":"dataset.columns","f885df5e":"plt.figure(figsize=(15,20))\nmask = np.triu(dataset.corr())\nsns.heatmap(round(dataset.corr(),2),\n            annot=True,\n            vmin=-1,vmax=1,center=0,\n            cmap='bwr',\n            mask=mask,\n            cbar_kws = {'orientation':'horizontal'}\n           )","76c18995":"storepur = pd.DataFrame(dataset.corr()[\"NumStorePurchases\"])\nstorepur = storepur.drop(['NumStorePurchases','ID'])\nstorepur = storepur[(storepur.NumStorePurchases > 0.5) | (storepur.NumStorePurchases<-0.3)]\nstorepur = storepur.sort_values(by=['NumStorePurchases'])\nplt.figure(figsize=(12,8))\na=storepur.index\nb=storepur.NumStorePurchases\nax = sns.barplot(x=a,y=b,palette='coolwarm')\nplt.bar_label(ax.containers[0],padding=-15)\nplt.xticks(rotation=10)\nplt.title(\"Significant Correlation for NumStorePurchases\")","d9e47d5f":"totalpur = dataset.groupby('Country').agg(np.sum).filter(regex='TotalPurchases')\ntotalpur['People'] = dataset['Country'].value_counts()\ntotalpur['PurchPerPerson'] = round(totalpur['TotalPurchases']\/totalpur['People'],2)\ntotalpur = totalpur.sort_values(by=['TotalPurchases'])\ntotalpur","e9dea901":"plt.figure(figsize=(12,8))\nax = sns.barplot(x=totalpur.index,y=totalpur.TotalPurchases,palette='coolwarm')\nplt.bar_label(ax.containers[0],padding=0)\nax2 = ax.twinx()\nax2 = sns.lineplot(x=totalpur.index,y=totalpur.PurchPerPerson)\nax2.set_ylabel('Purchases Per Person')\nfor i in range(8):\n    plt.text(i,totalpur['PurchPerPerson'][i],totalpur['PurchPerPerson'][i],bbox=dict(facecolor='white', alpha=0.5))","5c8e8d4a":"goldpur = dataset.loc[:,['Income','NumStorePurchases','MntGoldProds']]\ngoldpur['GoldPer'] = round((goldpur['MntGoldProds']*100)\/goldpur['Income'],2)\ngoldpur.drop(['Income'],axis=1,inplace=True)\ngoldpur.shape # Gold spent in absolute terms","d6f91cbd":"goldpur1 = goldpur[goldpur['GoldPer']>goldpur['GoldPer'].mean()] \ngoldpur1.shape # Above average gold spent in terms of percent of income","33f29fde":"goldpur2 = goldpur[goldpur['MntGoldProds']>goldpur['MntGoldProds'].mean()]\ngoldpur2.shape # Above average gold spent in absolute terms","dca6dafa":"from scipy.stats import kendalltau\nprint('When amount on gold is spent in absolute terms\\n')\nprint(kendalltau(goldpur['NumStorePurchases'],goldpur['GoldPer']))\nprint(kendalltau(goldpur['NumStorePurchases'],goldpur['MntGoldProds']))\nprint('\\nWhen above average amount on gold is spent in terms of percent of income\\n')\nprint(kendalltau(goldpur1['NumStorePurchases'],goldpur1['GoldPer']))\nprint(kendalltau(goldpur1['NumStorePurchases'],goldpur1['MntGoldProds']))\nprint('\\nWhen above average amount on gold is spent in absolute terms\\n')\nprint(kendalltau(goldpur2['NumStorePurchases'],goldpur2['GoldPer']))\nprint(kendalltau(goldpur2['NumStorePurchases'],goldpur2['MntGoldProds']))","edb3b6c5":"fishpur = dataset.loc[:,['Education','Marital_Status','MntFishProducts']]\nfishpur['FishPer'] = round((fishpur['MntFishProducts']*100) \/ dataset['Income'],2)\nfishpur = fishpur.groupby(['Marital_Status','Education']).agg(np.mean)\nfishpur = fishpur.reset_index()\nphd = fishpur[fishpur['Education']=='PhD']\nphd","dd99f7ac":"dataset['MntFishProducts'].describe()","e66ba669":"fig,ax = plt.subplots(1,3,gridspec_kw={'width_ratios': [8,2,8]},figsize=(15, 7))\n\nax1 = sns.lineplot(x='Marital_Status', y='FishPer', data=fishpur, hue='Education',ax=ax[0],palette='mako')\nax1.get_legend().remove()\nfor i in [4,14,19,24]:\n    ax1.text((i+1)\/5-1,phd['FishPer'][i],(str(round(phd['FishPer'][i],2))+'%'),bbox=dict(facecolor='white', alpha=0.5))\ni=9\nax1.text((i+1)\/5-1,phd['FishPer'][i],(str(round(phd['FishPer'][i],2))+'%'),bbox=dict(facecolor='yellow', alpha=0.5))\n    \nax[1].axis('off')\n\nax3 = sns.lineplot(x='Marital_Status', y='MntFishProducts', data=fishpur, hue='Education',ax=ax[2],palette='mako')\nax3.get_legend().remove()\nfor i in [4,14,19,24]:\n    ax3.text((i+1)\/5-1,phd['MntFishProducts'][i],('$'+str(round(phd['MntFishProducts'][i],2))),bbox=dict(facecolor='white', alpha=0.5))\ni=9\nax3.text((i+1)\/5-1,phd['MntFishProducts'][i],('$'+str(round(phd['MntFishProducts'][i],2))),bbox=dict(facecolor='yellow', alpha=0.5))\n    \naxLine, axLabel = ax1.get_legend_handles_labels()\nfig.legend(axLine,axLabel,loc='center', bbox_to_anchor=(0.5, 0.5))","d19fb77e":"fishpur = pd.DataFrame(dataset.corr()[\"MntFishProducts\"])\nfishpur = fishpur.drop(['MntFishProducts','ID'])\nfishpur = fishpur[(fishpur.MntFishProducts > 0.45) | (fishpur.MntFishProducts<-0.2)]\nfishpur = fishpur.sort_values(by=['MntFishProducts'])\nplt.figure(figsize=(12,8))\nax = sns.barplot(x=fishpur.index,y=fishpur.MntFishProducts,palette='coolwarm')\nplt.bar_label(ax.containers[0],padding=-15)\nplt.xticks(rotation=20)\nplt.title(\"Significant Correlation for MntFishProducts\")","9b32df63":"print(kendalltau(dataset['MntFishProducts'],dataset['Children']))","50c85206":"geo = dataset[['Country','AcceptedCmp1','AcceptedCmp2','AcceptedCmp3','AcceptedCmp4','AcceptedCmp5']]\ngeo = geo.groupby('Country').sum()\ngeo['Total'] = dataset['Country'].value_counts()\ngeo = geo[['AcceptedCmp1','AcceptedCmp2','AcceptedCmp3','AcceptedCmp4','AcceptedCmp5']].div(geo.Total, axis=0)\ngeo = round(geo.multiply(100),3)\ngeo = geo.drop('ME')\ngeo","a576f280":"geo = geo.melt(ignore_index=False)\ngeo = geo.reset_index()\ngeo.head()","75a2cda2":"plt.figure(figsize=(15, 7))\nax1 = sns.lineplot(x='Country', y='value',hue='variable',data=geo,palette='turbo')\nax1.legend(loc='best', bbox_to_anchor=(1, 0.5))","6f703fe2":"pie1 = dataset.filter(regex='Cmp').sum().sort_values(ascending=False)\npie1 = pd.DataFrame(pie1)\npie1 = pie1.reset_index()\npie1","dceb7f6c":"import plotly.express as px\nfig = px.pie(pie1,names='index',values=0, color_discrete_sequence=px.colors.sequential.dense)\nfig.show()","486a2e82":"pie2 = dataset.filter(regex='Mnt').sum().sort_values(ascending=False)\npie2.drop('MntTotal',inplace=True)\npie2 = pd.DataFrame(pie2)\npie2 = pie2.reset_index()\npie2","6bfa076b":"fig = px.pie(pie2,names='index',values=0, color_discrete_sequence=px.colors.sequential.dense)\nfig.show()","46dc8d8b":"pie3 = dataset.filter(regex='Num').sum().sort_values(ascending=False)\npie3 = pie3.drop(labels='NumWebVisitsMonth')\npie3 = pd.DataFrame(pie3)\npie3 = pie3.reset_index()\npie3","bf8eb4ce":"fig = px.pie(pie3,names='index',values=0, color_discrete_sequence=px.colors.sequential.dense)\nfig.show()","8dec148f":"def agerange(x):\n    lis = []\n    for i in x:\n        if i>18 and i<25:\n            lis.append('18-24')\n        elif i>=25 and i<36:\n            lis.append('25-35')\n        elif i>=36 and i<51:\n            lis.append('36-50')\n        elif i>=51 and i<66:\n            lis.append('51-65')\n        else:\n            lis.append('66+')\n    return lis","c36d4afb":"def incomerange(x):\n    lis = []\n    for i in x:\n        i=i\/1000\n        if i<=30:\n            lis.append('Upto 30k')\n        elif i>30 and i<=50:\n            lis.append('30-50k')\n        elif i>50 and i<=75:\n            lis.append('50-75k')\n        elif i>75 and i<=100:\n            lis.append('75-100+')\n        else:\n            lis.append('100k+')\n    return lis","b9ef7d55":"df = dataset.loc[:,['Age','Income','Country','Education','Marital_Status','Children']]\ndf['AgeRange'] = agerange(df['Age'])\ndf['IncomeRange'] = incomerange(df['Income'])\ndf = df.drop(columns=['Age','Income'])\ndf.head()","b3329457":"from plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nfig = make_subplots(rows=3, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}],\n                                           [{'type':'domain'}, {'type':'domain'}],\n                                           [{'type':'domain'}, {'type':'domain'}]])\nx=1\ny=1\nfor i in df.columns:\n    lst=[0.05]\n    labels=df[:][i].value_counts().index\n    zeros=len(labels)-1\n    \n    for j in range(zeros):\n        lst.append(0)\n    \n    fig.add_trace(go.Pie(\n                    title=i,\n                    title_font_size=20,\n                    labels=labels,\n                    values=df[:][i].value_counts().values,\n                    textinfo='label+percent',\n                    pull=lst,\n                    showlegend=False,\n                    marker_line_width=1,\n                    marker_line_color='black',\n                    marker_colors=px.colors.sequential.dense\n                    ),\n              row=x, col=y)\n    y=y+1\n    if(y==3):\n        y=1\n        x=x+1\n        \nfig.update_layout(\n    autosize=False,\n    width=1000,\n    height=1000)\n    \n\nfig.show()","5e31d48d":"Married PhD candidates do not spend a significant amount or percentage of their income on fish. As per percent of their income, they rank at the bottom and in absolute terms they rank below mean. ","85f0b465":"https:\/\/plotly.com\/python\/reference\/pie\/","d3eaa100":"## Fish has Omega 3 fatty acids which are good for the brain. Accordingly, do \"Married PhD candidates\" have a significant relation with amount spent on fish? What other factors are significantly related to amount spent on fish? (Hint: use your knowledge of interaction variables\/effects)","8f937a32":"In terms of total number of purchases, **USA doesn't** seem to be in a good position. In fact it is the **lowest** (excluding ME due to only 3 records). However if we look at the purchases made per person in a country, then USA tops the chart. So this market holds potential to attract more revenues as per this metric followed by CA and SA. Marketing campaigns can be held here to see if they converge to SP levels as number of customers rise or if they stabilize thus yielding higher revenues. ","2688a191":"### Which channels are underperforming?","8a77a4fa":"#### Customer Personality Analysis\/Finding the average customer using **clustering techniques** to summarize customer segments. This will be part 2 of this notebook.   \n\n#### Edit 1: Link to Part 2 of this notebook : https:\/\/www.kaggle.com\/namanoberoi\/part-2-personality-segmentation\n\n#### If you have made it this far and learned something new, please consider **upvoting**. Open to feedback for improvement as this is the first analytics dataset that I have worked on.\n\n### Thanks !\n\n## Special mention to **jackdaoud** for providing this dataset.","dd5419d7":"### Which marketing campaign is most successful?","4fc7c700":"## ***Feature Engineering***\n\n   1. Replace Birth year with age as it is more meaningful. We can further divide age into intervals to segment the population.  \n   2. Find the total amount spent on various products so that we can find what percentage of income is spent on which  \n   3. Find the total number of purchases made through different channels which can help find the percentage these channels contribute  \n   4. Average of purchases per visit to the website  \n   5. Find the total number of children at home  \n   6. Modify date of enrollment to total days since enrollment  ","fb974f04":"Just created this for visualisation purpose and see the correlation of various attributes. Although there are a number of features with discrete data, will stick to Pearson correlation instead of Kendall Tau for **simplicity**. Kendall Tau should be ideally used but when you compare the correlation value between Pearson and Kendall for this dataset, you will more or less have the same conclusion. ","1f685e59":"We can see some clear outliers in **Income and Birth Year**. We will remove the rows where the Income is greater than $200,000 and birth year is less than 1920. For other columns, we cannot blindly remove these outliers as there could be cases where the requirement for these products is high by the user. Maybe the consumer is hosting a party or an event or is more comfortable getting his products from a particular channel. ","4282f9d3":"The columns can easily be understood from their names.\n\nColumns beginning with **Mnt** denotes the amount that was spent by the consumer on certain types of products. This value will be in terms of dollars as the income is measured in dollars based on the first few entries. We can compare for the '$' sign with the total number of entries to see if there exists any discrepancy. \n\nColumns beginning with **Num** denote a number. These values would not have any sign($,cm,kg,etc.) associated with them and are simply numbers. \n\nColumns beginning with **Accepted** tell us whether a marketing campaign has been successful or not. 5 different campaigns were held. A value of 1 denotes it was successful while a value of 0 denotes it wasn't.\n\n***Tasks at hand:***\n  \n1. Check for missing values and impute them.\n2. Check whether all income in the dataset is in dollars.    \n        - If you notice, there exists a space in the column name. We will have to fix it first.   \n3. Check for data values which are extreme (Outliers).    \n4. Making meaningful columns such as age from DOB, total number of purchases from all channels, total amount spent on all products, etc.  ","4bd65dbb":"## Is there a significant relationship between geographical regional and success of a campaign?","92bd1e6a":"There are 2240 entries out of which 24 have null values. That gives us 2216 entries with income in dollars. We can now proceed to remove this sign and convert the data into float. After this, will fill these 24 values based on the **mean of values from Education and Country**, assuming an idealistic scenario where the more educated you are, the more you will earn and that different countries have different salary standards as per their labour laws and currency strength. ","fae329d9":"Will check for any values in the dataset that may cause error in analysis. ","79fd58e0":"**Wine** is the best performing product in terms of revenue and revenue per customer. It contributes to ~50% of the total revenues. ","09fc0973":"# Section 1 : Exploratory Data Analysis","824a041d":"### Which products are performing best?","3f034fef":"### What does the average customer look like for this company?","556a967b":"Importing data for use","5a23644b":"There seems to be **no relation** between geographical region and success of a campaign. Although we can say that campaign 2 had the worst results among all campaigns. ","1cfc4c86":"This question is a bit vague. \"Spent above *average amount* on gold\" could be in terms of absolute value which doesn't take income into account. This could skew the data. People with more income might spend more. This could constitute a major portion for a low income customer who would rather buy less of the product. Secondly, **above average** raises the question, relative to what? People in a country? All customers ? A CMOs input would be essential here to clarify this. A better measure, however, would be how much percent of their income is the person spending on gold.   \n\nHave calculated for 6 different scenarios that could play out here of which in 2 there is no correlation.  \n\nBased on the question technically, the scenario that fits is \"When above average amount on gold is spent in terms of percent of income\". The **correlation is 0.04388 with pvalue equal to 0.09627**. Hence we conclude there is **no correlation** between people who spent an above average amount on gold in the last 2 years and in store purchases. \n\nDo check the code to understand this point.","3652a9dc":"### Your supervisor insists that people who buy gold are more conservative. Therefore, people who spent an above average amount on gold in the last 2 years would have more in store purchases. Justify or refute this statement using an appropriate statistical test","b290f99c":"Based on the number of unique values, **Marital status** seems to have unusually large set of values. Will look into this. Other columns have unique values within expectation.","72047a54":"Number of store purchases has a ***positive*** relation with -  \n\n   1. **Amount spent on wine** : A possible reason for this could be that the consumer would like to evaluate the different wines available in the store. Sin products are usually not allowed to be marketed as per government laws. A consumer might want to try another brand\/product. Second reason why this could be possible is due to store policies of verifying the buyers identity. A teenager can easily disguise himself\/herself online while ordering the products.   \n   2. **Income** : One possible reason to explain this is that low income customers generally buy products in bulk to avail some discounts as opposed to those with higher income. If one buys a product in bulk, then the need to go to the store reduces.  \n   3. Other factors include total amount spend, number of catalog and number of web purchases.  \n      \nThere is a ***negative*** relation with -  \n\n   1. **Number of children at home**, especially kids : One explanation could be the difficulty to handle them in store, hence parent prefer to get the supplies in one go. Parents may prefer to look at other channels such as a website if they are at home looking after the kids.    \n   2. **Number of website visits per month** : It is evident if one goes online and purchases a product, then the need to go the store reduces. A higher number of visits to a website may entice a user to order online due to the convenience of physically not going to the store. Open ended. ","98aed9c0":"**Deals purchases** is underperforming. This needs to be investigated further to check it's viability and the factors that are dragging it down. ","ce7b121e":"**Campaign 4** has been the most successful but by a small margin.","abfe0bc7":"# What's next ?","9d70f192":"### Does US fare significantly better than the Rest of the World in terms of total purchases?","ef7ad6bf":"Now that the dataset is ready, we can proceed to the next section of the analysis.  \nFrom 2240 * 28 to 2236 * 32","21004874":"# Section 2 & 3: Statistical Analysis and Data visualization","d3716e93":"Amount spent on fish has a ***positive*** relation with -  \n\n   1. **Amount spent on fruits, sweets and meat** : A possible reason for this could be that the customers are buying groceries for the week. This would lead them to cover different categories of products along with fish products as they will not order again and again. \n   2. **Income** : One possible reason to explain this is that low income customers would prefer to buy cheaper fish products compared to high income customers. \n   3. Other factors include total amount spend, number of catalog and number of store purchases.  \n      \nThere is a ***negative*** relation with -  \n\n   1. **Number of children at home** : One explanation could be that children do not like fish products and hence there is no incentive for parents to spend more amount here. A good marketing and packaging can be thought of here to grab this portion of the market.\n   2. **Number of website visits per month** : More visits to the website indicate the customers affinity to place orders online. However fish products are better bought in store as the customer can better judge the quality of fish. Maybe the customer would like to see the nutrition or type of fish in a canned product which the website fails to display despite extensive search.","54aae021":"### What factors are significantly related to the number of store purchases?","e2b7104e":"The method to find the average customer here is by finding the majority in each of the 6 different categories. This idea is, however, flawed because we are taking each category separately. A more accurate measure would be to cluster the records and then find the average customer. However for the purpose of this notebook, will not proceed with that.     \n\nAnother way to find the average customer could be to take the average values of each category but it fails to segment the customer on other data attributes and can be erroneous due to uneven distribution of values.   \n\nAs per the stated method, the average customer -   \n1. Belongs to SP  \n2. Is Graduated  \n3. In a relationship i.e. married or together   \n4. Has at least a child  \n5. Born before the year 1986 considering the current year is 2021  \n6. Has an income between 30-75k   \n\nOther factors such as number of enrolled days, favored channel or the product they prefer can be similarly segmented based on methods above to find more information about the average customer.  \n"}}