{"cell_type":{"48c5f756":"code","361606d2":"code","cb9b577e":"code","1d6d0ca3":"code","9c28fa9a":"code","83d4c9f3":"code","c5fd5f03":"code","13c11009":"code","997dd3b3":"code","1f9bd599":"code","b3f0507b":"code","90357eb3":"code","d23e5637":"code","964667bd":"code","f72d0225":"code","db9a4f71":"code","38610339":"code","708b81d9":"code","ec47d963":"code","c92688ae":"code","a2291035":"code","9343bf37":"code","eca8ede6":"code","f1f2c849":"markdown","a573c78d":"markdown","a960ef84":"markdown","8c073381":"markdown","5676ebf0":"markdown","f38edd8e":"markdown","059e2678":"markdown","74477686":"markdown","dc7b9c7b":"markdown","2115f3ec":"markdown","a6a64e2b":"markdown","fae0170c":"markdown","46dc0df5":"markdown","02eb2f0d":"markdown","735f161c":"markdown","85e11039":"markdown","ce57fb7c":"markdown","dc7b093e":"markdown","1e3fb83d":"markdown","27752ff1":"markdown"},"source":{"48c5f756":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler,OrdinalEncoder\nfrom scipy import stats\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split","361606d2":"train = pd.read_csv('..\/input\/kfold\/train_folds.csv')\ntest = pd.read_csv('..\/input\/30-days-of-ml\/test.csv')\nsubmission = pd.read_csv('..\/input\/30-days-of-ml\/sample_submission.csv')","cb9b577e":"train.head()","1d6d0ca3":"test.head()","9c28fa9a":"print(train.isnull().sum())\nprint('#'*40)\nprint(test.isnull().sum())","83d4c9f3":"useful_features = [col for col in train.columns if col not in ['id','target','kfolds']]","c5fd5f03":"cat_features = [col for col in useful_features if col.startswith('cat')]","13c11009":"num_features =  [col for col in useful_features if not col.startswith('cat')]","997dd3b3":"cat_features","1f9bd599":"num_features","b3f0507b":"plt.figure(figsize=(10,8))\nsns.boxplot(data=train[num_features]).set(title='Outlier detection for Train dataset')","90357eb3":"plt.figure(figsize=(10,8))\nsns.boxplot(data=test[num_features]).set(title='Outlier detection for Test dataset')","d23e5637":"def remove_outlier_IQR(df):\n    Q1=df.quantile(0.25)\n    Q3=df.quantile(0.75)\n    IQR=Q3-Q1\n    df_final=df[~((df<(Q1-1.5*IQR)) | (df>(Q3+1.5*IQR)))]\n    return df_final","964667bd":"train[num_features] = remove_outlier_IQR(train[num_features])","f72d0225":"test[num_features] = remove_outlier_IQR(test[num_features])","db9a4f71":"plt.figure(figsize=(10,8))\nsns.boxplot(data=train[num_features]).set(title='Outlier removal of Train dataset')","38610339":"plt.figure(figsize=(10,8))\nsns.boxplot(data=train[num_features]).set(title = 'Outlier removal of Test dataset')","708b81d9":"transformers = ColumnTransformer(\n    [(\"ordinary_encoder\", OrdinalEncoder(), cat_features),\n     (\"standardize\", StandardScaler(), num_features)],\n    remainder=\"passthrough\"\n)\n\n","ec47d963":"import optuna\nfrom sklearn.model_selection import cross_val_score\n\ndef objective(trial):\n    \n    \n\n    X_train, X_valid, y_train, y_valid = train_test_split(train[useful_features], train['target'], random_state=42, test_size=0.1)\n    \n    X_train = transformers.fit_transform(X_train)\n    X_valid  = transformers.transform(X_valid)\n    \n    tree_method = trial.suggest_categorical('tree_method',['gpu_hist'])\n    n_estimators = trial.suggest_int(\"n_estimators\", 1000, 5000)\n    max_depth = trial.suggest_int(\"max_depth\", 1, 10)\n    learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 1e-1, log=True)\n    gamma = trial.suggest_float(\"gamma\", 0.1, 1.0, step=0.1)\n    min_child_weight = trial.suggest_int(\"min_child_weight\", 1, 7, step=2)\n    subsample = trial.suggest_float(\"subsample\", 0.5, 1.0, step=0.1)\n    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.5, 1.0, step=0.1)\n    reg_alpha = trial.suggest_float(\"reg_alpha\", 0., 1.0, step=0.1)\n    reg_lambda = trial.suggest_float(\"reg_lambda\", 0., 1.0, step=0.1)\n    \n    \n    model = XGBRegressor(n_estimators=n_estimators,\n                         max_depth=max_depth,\n                         learning_rate=learning_rate,\n                         gamma=gamma,\n                         min_child_weight=min_child_weight,\n                         colsample_bytree=colsample_bytree,\n                         subsample=subsample,\n                         reg_alpha=reg_alpha,\n                         reg_lambda=reg_lambda,\n                         n_jobs=-1, \n                         tree_method='gpu_hist', \n                         gpu_id=0)\n    \n    model.fit(X_train, y_train)\n    \n    y_hat = model.predict(X_valid)\n    \n    return mean_squared_error(y_valid, y_hat, squared=False)\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=100)","c92688ae":"study.best_params","a2291035":"best_params = study.best_params","9343bf37":"final_predictions = []\nfor fold in range(5):\n    xtrain =  train[train.kfolds != fold].reset_index(drop=True)\n    xvalid = train[train.kfolds == fold].reset_index(drop=True)\n    xtest = test.copy()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    xtest = xtest[useful_features]\n    \n   \n    xtrain = transformers.fit_transform(xtrain)\n    xvalid = transformers.transform(xvalid)\n    xtest = transformers.fit_transform(xtest)\n   \n    \n  \n    \n    model = XGBRegressor(**best_params)\n    model.fit(xtrain, ytrain,eval_set=[(xvalid,yvalid)],early_stopping_rounds=100,verbose=False)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_predictions.append(test_preds)\n    print(fold, mean_squared_error(yvalid, preds_valid, squared=False))","eca8ede6":"preds = np.mean(np.column_stack(final_predictions), axis=1)\nsubmission.target = preds\nsubmission.to_csv(\"submission.csv\", index=False)","f1f2c849":"**Thanks for reading !** \n\nHope you liked it.\n\nplease suggest if there are any corrections required in this notebook.","a573c78d":"# Plan of Action \n\n- Using XGboost Regressor with some basic feature engineering and Hyperparameter tuning using Optuna to predict the target variable.\n\nIn this notebook, I have used training data set on which i have already applied kfold with 5 number of splits and it contains column 'kfolds' with values equal to number of fold (0,1,2,3,4). You can find the notebook and dataset here - https:\/\/www.kaggle.com\/omkarborikar\/kfold","a960ef84":"Best parameters are :- ","8c073381":"# **Creating submission file**","5676ebf0":"From both the plots we can observe that there are outliers present for columns 'cont0', 'cont6', 'cont8'.\n\nWe use IQR(Interquartile range) to remove the outliers.","f38edd8e":"We store categorical features in seperate list called 'cat_features' which is used later in the notebook to apply ordinal encoding.","059e2678":"# **Encoding and Standardizing**\n\nHere ColumnTransformer is used which performs Ordinal encoding and Standardization (StandardScaler) on categorical features and Numerical features respectively.\n\nWe fit and transform this ColumnTransformer to our train data and transform this ColumnTransformer on Validation and test data.","74477686":"# Feature Engineering","dc7b9c7b":"# **Feature selection**\n\nFrom all the columns in training data set 'id' column does not add any value for predicting the target variable. we also don't need 'target' and 'kfold' columns. We store rest of the columns in the list 'useful_features'","2115f3ec":"Similarly , We store numerical features in seperate list called 'num_features' which is used later in the notebook to apply Standarscaling and removing outliers.","a6a64e2b":"Taking a look at train and test data with the help of head().","fae0170c":"# **Fitting and predicting values with XGboost .**\n\nFitting and predicting values with XGBoost using best parameters","46dc0df5":"Reading train, test and submission data.","02eb2f0d":"**Hyperparameter Turning using Optuna**","735f161c":"# Handling Outliers\n\n**Checking if any outliers are present in train and test data.**\n\nWe use Boxplot from seaborn library to check if there are any outliers present in numerical features.","85e11039":"Checking if outliers are removed or not.","ce57fb7c":"Checking if train data or test data has null values or not.","dc7b093e":"From above 2 plots we can confirm that there are no more outliers in our dataset.","1e3fb83d":"Simply displaying Categorical and Numerical columns.","27752ff1":"we can see that train and test data does not have any null values, so no need for imputation."}}