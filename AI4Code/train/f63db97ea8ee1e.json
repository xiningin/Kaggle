{"cell_type":{"f90f0683":"code","39e67def":"code","b40b8c4f":"code","1878ee82":"code","c045baef":"code","c3cff356":"code","8bb8b513":"code","8986d0ab":"code","b8646089":"code","739a0ca2":"code","350d9eca":"code","a2dee875":"code","6ed1b89a":"code","b31052cf":"code","9d1d18c9":"code","d8c8e261":"code","a3a99bc1":"code","e3078c4d":"code","471d0e59":"code","9c38b50a":"code","5459faed":"code","a4e63691":"code","c593b5be":"code","bc45f875":"code","dabfe9ae":"code","2c0bba66":"markdown","4dabba63":"markdown","9be2cbb1":"markdown","9a4040e6":"markdown","14ab28d9":"markdown"},"source":{"f90f0683":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","39e67def":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom bayes_opt import BayesianOptimization\nfrom sklearn import metrics","b40b8c4f":"df = pd.read_csv('..\/input\/avacado-price-prediction\/Avocado.csv')\ndf.drop(['Unnamed: 0'],axis =1,inplace=True )\ndf.head()","1878ee82":"df.info() ","c045baef":"# ENCODING\n\ndef ordinal_encoding(df,column,ordering) :\n    df =df.copy()\n    df[column] = df[column].apply(lambda x : ordering.index(x))\n    return df","c3cff356":"date_ordering = sorted(df['Date'].unique())","8bb8b513":"df = ordinal_encoding(df,'Date',date_ordering)","8986d0ab":"df = pd.get_dummies(df,columns=['region'])","b8646089":"df = pd.get_dummies(df,columns=['type'])","739a0ca2":"# After that process our new df(data) \n\ndf.head()","350d9eca":"reg_X = df.drop(['AveragePrice'],axis=1)\nreg_y = df['AveragePrice']","a2dee875":"X_train, X_test, y_train, y_test = train_test_split(reg_X, reg_y, test_size=0.2, random_state=0)\n\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\n\ncoeff_df = pd.DataFrame(regressor.coef_, reg_X.columns, columns=['Coefficient'])\ncoeff_df\n\ny_pred = regressor.predict(X_test)\ny_pred\n\ndf_pred = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndf_pred.head()","6ed1b89a":"print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","b31052cf":"## XGBOOST ALGORITHM MODELLING ","9d1d18c9":"model1 = xgb.XGBRegressor(nthread=10)","d8c8e261":"xgb_r = xgb.XGBRegressor(objective ='reg:linear',\n                  n_estimators = 10, seed = 123)\ntrain_X, test_X, train_y, test_y = train_test_split(reg_X, reg_y,\n                      test_size = 0.3, random_state = 123)\nxgb_r.fit(train_X, train_y)","a3a99bc1":"xg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.35,\n                max_depth = 6, alpha = 10, n_estimators = 300)","e3078c4d":"xg_reg.fit(X_train,y_train)\n\npreds = xg_reg.predict(X_test)","471d0e59":"xgb_r.fit(train_X, train_y)\n  \n# Predict the model\npred = xgb_r.predict(test_X)\n  \n# RMSE Computation\nrmse = np.sqrt(MSE(test_y, pred))\nprint(\"RMSE : % f\" %(rmse))","9c38b50a":"def xgbc_cv(max_depth,learning_rate,n_estimators,reg_alpha):\n    \n    estimator_function = xgb.XGBRegressor(max_depth=int(max_depth),\n                                           learning_rate= learning_rate,\n                                           n_estimators= int(n_estimators),\n                                           reg_alpha = reg_alpha,\n                                           nthread = -1,\n                                          \n                                           seed = seed)\n        # Fit the estimator\n    estimator_function.fit(X_train,y_train)\n    \n\n    return 1\/MSE(y_test,estimator_function.predict(X_test))\n\n\n\ngp_params = {\"alpha\": 1e-10}\nseed = 112\nhyperparameter_space = {\n    'max_depth': (3, 15),\n    'learning_rate': (0, 1),\n    'n_estimators' : (50,600),\n    'reg_alpha': (0,1)\n}\n\nxgbcBO = BayesianOptimization(f = xgbc_cv, \n                             pbounds =  hyperparameter_space,\n                             random_state = seed,\n                             verbose = 10)\n\n\nxgbcBO.maximize(init_points=2,n_iter=6,acq='ucb', kappa= 3, **gp_params)","5459faed":"optimum_parameter=xgbcBO.max\noptimum_parameter[\"params\"]\n## that codes give us optimum parameters for xgboost","a4e63691":"xg_reg1 = xgb.XGBRegressor(learning_rate = 0.2946386696029385,\n max_depth= 10,\n n_estimators =571,\n reg_alpha =  0.8663005771251282)\n\n## xgboost algorithm with optimum parameters","c593b5be":"xg_reg1.fit(X_train,y_train)\n\npreds1 = xg_reg1.predict(X_test)","bc45f875":"rmse = np.sqrt(MSE(y_test, preds1))\nprint(\"RMSE: %f\" % (rmse))\nmse= rmse**2\nprint(\"MSE: %f\" % (mse))","dabfe9ae":"metrics.r2_score(y_test,preds1)","2c0bba66":"Root Mean Squared Error: 0.26 is too high for this dataset.Thus we have to find another way.Xgboost algorithm will be usefull !","4dabba63":"We have to arrange ' Region' and 'Date ' columns.( object to dummy) ","9be2cbb1":"RMSE :  0.183120 is cool but not enough ! So , i used bayesian optimization for get optimum parameters ","9a4040e6":"Preprocessing Model","14ab28d9":"## BAYESIAN OPTIMIZATION "}}