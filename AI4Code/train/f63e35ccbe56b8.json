{"cell_type":{"60d3ea86":"code","3e29f8be":"code","6502e4fa":"code","18d66c5d":"code","db6a10bd":"code","8e7cb8b7":"code","501983f0":"code","5237758d":"code","b2128c64":"code","09947854":"code","0799d705":"code","7ca4e4c2":"code","4f2fc495":"code","d8e1e14f":"code","a3da10bd":"code","1654d4fe":"code","ba77a8a5":"code","26aab548":"code","db938689":"code","f45a75d7":"code","b4afd926":"code","52ac87aa":"code","efc8d48f":"code","d72e04d6":"code","0181fa3d":"code","893367ba":"code","cf99bfb9":"code","2aec6b60":"code","cf676f2e":"code","001b86c3":"code","bcbc82da":"code","70ef0fab":"code","60d83724":"code","97f5e2d1":"code","b5360dc6":"code","aacc13f7":"code","18e2373c":"code","87b170b5":"code","fbf71df5":"code","18eacba9":"code","e41b38c1":"code","53591238":"code","9a59fe1c":"code","07205ed3":"code","226de3c7":"code","19a3c96d":"code","e4a65a35":"markdown","e9490135":"markdown","537a2ac5":"markdown","1225cd48":"markdown","827baed7":"markdown","3961beca":"markdown"},"source":{"60d3ea86":"import numpy as np \nimport pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nps = PorterStemmer()\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nimport matplotlib.pyplot as plt","3e29f8be":"#read dataset\ndf=pd.read_csv('..\/input\/fake-news\/train.csv')","6502e4fa":"df.head()","18d66c5d":"x= df.drop('label',axis=1)","db6a10bd":"x.head(2)","8e7cb8b7":"y = df['label']","501983f0":"df.shape","5237758d":"df.info()","b2128c64":"df.isnull().sum()","09947854":"df=df.dropna()","0799d705":"df.head()","7ca4e4c2":"df['title'][3]","4f2fc495":"messeges =df.copy()","d8e1e14f":"messeges.reset_index(inplace=True)","a3da10bd":"messeges.head()","1654d4fe":"import nltk\nnltk.download('stopwords')\ncorpus = []\nfor i in range(0, len(messeges)):\n    review = re.sub('[^a-zA-Z]', ' ', messeges['title'][i])\n    review = review.lower()\n    review = review.split()\n    \n    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n    review = ' '.join(review)\n    corpus.append(review)","ba77a8a5":"corpus[6]","26aab548":"cv = CountVectorizer(max_features=5000,ngram_range=(1,3))\nX = cv.fit_transform(corpus).toarray()","db938689":"# show resulting vocabulary; the numbers are not counts, they are the position in the sparse vector.\ncv.vocabulary_","f45a75d7":"X.shape","b4afd926":"from wordcloud import WordCloud\nreal=' '.join(list(messeges[messeges['label']==0]['title']))\nreal=WordCloud(width=512, height=512).generate(real)\nplt.figure(figsize=(5,5),facecolor='k')\nplt.imshow(real)\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","52ac87aa":"fake=' '.join(list(messeges[messeges['label']==1]['title']))\nfake=WordCloud(width=512, height=512).generate(fake)\nplt.figure(figsize=(5,5),facecolor='k')\nplt.imshow(fake)\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","efc8d48f":"y=messeges['label']","d72e04d6":"## Divide the dataset into Train and Test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)","0181fa3d":"cv.get_feature_names()[:20]","893367ba":"cv.get_params()","cf99bfb9":"count_df = pd.DataFrame(X_train, columns=cv.get_feature_names())","2aec6b60":"count_df.head()","cf676f2e":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    See full source and example: \n    http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\n    \n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","001b86c3":"from sklearn.naive_bayes import MultinomialNB\nclassifier=MultinomialNB()\nfrom sklearn import metrics\nimport numpy as np\nimport itertools","bcbc82da":"classifier.fit(X_train, y_train)\npred = classifier.predict(X_test)\nscore = metrics.accuracy_score(y_test, pred)\nprint(\"accuracy:   %0.3f\" % score)\ncm = metrics.confusion_matrix(y_test, pred)\nplot_confusion_matrix(cm, classes=['FAKE', 'REAL'])","70ef0fab":"classifier.fit(X_train, y_train)\npred = classifier.predict(X_test)\nscore = metrics.accuracy_score(y_test, pred)\nscore","60d83724":"y_train.shape","97f5e2d1":"train=pd.read_csv(\"..\/input\/fake-news\/train.csv\")\ntest=pd.read_csv(\"..\/input\/fake-news\/test.csv\")\ntest.info()\n#test['label']='t'\ntrain.info()","b5360dc6":"from sklearn.feature_extraction.text import TfidfTransformer\n\ntest=test.fillna(' ')\ntrain=train.fillna(' ')\ntest['total']=test['title']+' '+test['author']+test['text']\ntrain['total']=train['title']+' '+train['author']+train['text']\n\n#tfidf\ntransformer = TfidfTransformer(smooth_idf=False)\ncount_vectorizer = CountVectorizer(ngram_range=(1, 2))\ncounts = count_vectorizer.fit_transform(train['total'].values)\ntfidf = transformer.fit_transform(counts)","aacc13f7":"targets = train['label'].values\ntest_counts = count_vectorizer.transform(test['total'].values)\ntest_tfidf = transformer.fit_transform(test_counts)\n\n#split in samples\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(tfidf, targets, random_state=0)","18e2373c":"from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n                              AdaBoostClassifier)\n\nExtr = ExtraTreesClassifier(n_estimators=5,n_jobs=4)\nExtr.fit(X_train, y_train)\nprint('Accuracy of ExtrTrees classifier on training set: {:.2f}'\n     .format(Extr.score(X_train, y_train)))\nprint('Accuracy of Extratrees classifier on test set: {:.2f}'\n     .format(Extr.score(X_test, y_test)))","87b170b5":"from sklearn.tree import DecisionTreeClassifier\n\nAdab= AdaBoostClassifier(DecisionTreeClassifier(max_depth=3),n_estimators=5)\nAdab.fit(X_train, y_train)\nprint('Accuracy of Adaboost classifier on training set: {:.2f}'\n     .format(Adab.score(X_train, y_train)))\nprint('Accuracy of Adaboost classifier on test set: {:.2f}'\n     .format(Adab.score(X_test, y_test)))","fbf71df5":"RnFr = RandomForestClassifier(n_estimators=5)\nRnFr.fit(X_train, y_train)\nprint('Accuracy of RandomForest classifier on training set: {:.2f}'\n     .format(RnFr.score(X_train, y_train)))\nprint('Accuracy of RandomForest classifier on test set: {:.2f}'\n     .format(RnFr.score(X_test, y_test)))","18eacba9":"from sklearn.naive_bayes import MultinomialNB\n\nNB = MultinomialNB()\nNB.fit(X_train, y_train)\nprint('Accuracy of NB  classifier on training set: {:.2f}'\n     .format(NB.score(X_train, y_train)))\nprint('Accuracy of NB classifier on test set: {:.2f}'\n     .format(NB.score(X_test, y_test)))","e41b38c1":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(max_iter=1000,C=1e5,dual=False)\nlogreg.fit(X_train, y_train)\nprint('Accuracy of Lasso classifier on training set: {:.2f}'\n     .format(logreg.score(X_train, y_train)))\nprint('Accuracy of Lasso classifier on test set: {:.2f}'\n     .format(logreg.score(X_test, y_test)))","53591238":"targets = train['label'].values\nlogreg = LogisticRegression(max_iter=1000,dual=False)\nlogreg.fit(counts, targets)\n\nexample_counts = count_vectorizer.transform(test['total'].values)\npredictions = logreg.predict(example_counts)\npred=pd.DataFrame(predictions,columns=['label'])\npred['id']=test['id']\npred.groupby('label').count()","9a59fe1c":"classifier=MultinomialNB(alpha=0.1)","07205ed3":"previous_score=0\nfor alpha in np.arange(0,1,0.1):\n    sub_classifier=MultinomialNB(alpha=alpha)\n    sub_classifier.fit(X_train,y_train)\n    y_pred=sub_classifier.predict(X_test)\n    score = metrics.accuracy_score(y_test, y_pred)\n    if score>previous_score:\n        classifier=sub_classifier\n    print(\"Alpha: {:.1f}, Score : {}\".format(alpha,score))","226de3c7":"from sklearn import metrics\nfrom sklearn.calibration import CalibratedClassifierCV\nfig, ax_arr = plt.subplots(nrows = 2, ncols = 3, figsize = (20,15))\n\nprobs = Extr.predict_proba(X_test)\npreds = probs[:,1]\nfprextr, tprextr, thresholdextr = metrics.roc_curve(y_test, preds)\nroc_aucextr = metrics.auc(fprextr, tprextr)\n\nax_arr[0,0].plot(fprextr, tprextr, 'b', label = 'AUC = %0.2f' % roc_aucextr)\nax_arr[0,0].plot([0, 1], [0, 1],'r--')\nax_arr[0,0].set_title('Extra Tree ',fontsize=20)\nax_arr[0,0].set_ylabel('True Positive Rate',fontsize=20)\nax_arr[0,0].set_xlabel('False Positive Rate',fontsize=15)\nax_arr[0,0].legend(loc = 'lower right', prop={'size': 16})\n\n\n\nprobs = Adab.predict_proba(X_test)\npreds = probs[:,1]\nfpradab, tpradab, thresholdadab = metrics.roc_curve(y_test, preds)\nroc_aucadab = metrics.auc(fpradab, tpradab)\n\nax_arr[0,1].plot(fpradab, tpradab, 'b', label = 'AUC = %0.2f' % roc_aucadab)\nax_arr[0,1].plot([0, 1], [0, 1],'r--')\nax_arr[0,1].set_title('ADA Boost ',fontsize=20)\nax_arr[0,1].set_ylabel('True Positive Rate',fontsize=20)\nax_arr[0,1].set_xlabel('False Positive Rate',fontsize=15)\nax_arr[0,1].legend(loc = 'lower right', prop={'size': 16})\n\n\n\nprobs = RnFr.predict_proba(X_test)\npreds = probs[:,1]\nfprrnfr, tprrnfr, thresholdrnfr = metrics.roc_curve(y_test, preds)\nroc_aucrnfr = metrics.auc(fprrnfr, tprrnfr)\n\nax_arr[0,2].plot(fprrnfr, tprrnfr, 'b', label = 'AUC = %0.2f' % roc_aucrnfr)\nax_arr[0,2].plot([0, 1], [0, 1],'r--')\nax_arr[0,2].set_title('Random Forest ',fontsize=20)\nax_arr[0,2].set_ylabel('True Positive Rate',fontsize=20)\nax_arr[0,2].set_xlabel('False Positive Rate',fontsize=15)\nax_arr[0,2].legend(loc = 'lower right', prop={'size': 16})\n\n\n\nprobs = NB.predict_proba(X_test)\npreds = probs[:,1]\nfprdnb, tprdnb, thresholddnb = metrics.roc_curve(y_test, preds)\nroc_aucdnb = metrics.auc(fprdnb, tprdnb)\n\nax_arr[1,0].plot(fprdnb, tprdnb, 'b', label = 'AUC = %0.2f' % roc_aucdnb)\nax_arr[1,0].plot([0, 1], [0, 1],'r--')\nax_arr[1,0].set_title('Naive Bayes ',fontsize=20)\nax_arr[1,0].set_ylabel('True Positive Rate',fontsize=20)\nax_arr[1,0].set_xlabel('False Positive Rate',fontsize=15)\nax_arr[1,0].legend(loc = 'lower right', prop={'size': 16})\n\n\n\n\nprobs = logreg.predict_proba(X_test)\npreds = probs[:,1]\nfprlog, tprlog, thresholdlog = metrics.roc_curve(y_test, preds)\nroc_auclog = metrics.auc(fprlog, tprlog)\n\nax_arr[1,1].plot(fprlog, tprlog, 'b', label = 'AUC = %0.2f' % roc_auclog)\nax_arr[1,1].plot([0, 1], [0, 1],'r--')\nax_arr[1,1].set_title('Lasso ',fontsize=20)\nax_arr[1,1].set_ylabel('True Positive Rate',fontsize=20)\nax_arr[1,1].set_xlabel('False Positive Rate',fontsize=15)\nax_arr[1,1].legend(loc = 'lower right', prop={'size': 16})\n\n\n\nax_arr[1,2].plot(fprextr, tprextr, 'b', label = 'Extre Tree', color='black')\nax_arr[1,2].plot(fpradab, tpradab, 'b', label = 'ADA Boost', color='blue')\nax_arr[1,2].plot(fprrnfr, tprrnfr, 'b', label = 'Random Forest', color='brown')\nax_arr[1,2].plot(fprdnb, tprdnb, 'b', label = 'Naive Bayes', color='green')\nax_arr[1,2].plot(fprlog, tprlog, 'b', label = 'Lasso', color='grey')\nax_arr[1,2].set_title('Receiver Operating Comparison ',fontsize=20)\nax_arr[1,2].set_ylabel('True Positive Rate',fontsize=20)\nax_arr[1,2].set_xlabel('False Positive Rate',fontsize=15)\nax_arr[1,2].legend(loc = 'lower right', prop={'size': 16})\n\nplt.subplots_adjust(wspace=0.2)\nplt.tight_layout()","19a3c96d":"pred.to_csv('countvect5.csv', index=False)","e4a65a35":"### Ensemble learning","e9490135":"## TFIDF","537a2ac5":"### Multinomial Classifier with Hyperparameter","1225cd48":"## Counter Vectorization\n### Bag of Words","827baed7":"### ROC Curve","3961beca":"### Mulinomial Naive Bayes Theorem"}}