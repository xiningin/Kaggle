{"cell_type":{"0f8ad534":"code","6746b512":"code","1e0f9f72":"code","8a4c420f":"code","270c312c":"code","f31b8d18":"code","d0f6470d":"code","3c32c16d":"code","1dceab8c":"code","59c4a6ad":"code","d52ea819":"code","50ba196b":"code","bb89c850":"code","5b45f64d":"code","730f773a":"code","c30648c8":"code","6d17de02":"code","6ae66702":"code","7a893d9f":"code","45f4f167":"code","45c34f87":"code","95075fe2":"code","89859fec":"code","28f01b22":"code","c36668f5":"code","a0165bfe":"code","2d301a00":"code","d7b5df07":"code","a57d35cd":"code","b9381355":"code","45815f58":"markdown","6f0b70cb":"markdown","8cc5061f":"markdown","e881181b":"markdown","816bc47d":"markdown","b60f3434":"markdown","90380a02":"markdown","341d47ff":"markdown"},"source":{"0f8ad534":"!ls ..\/input","6746b512":"import configparser\nimport glob\nimport os\nimport pandas as pd\nimport subprocess\nimport sys\nimport tarfile \nfrom urllib.request import urlretrieve","1e0f9f72":"FILEURL = \"https:\/\/www.rondhuit.com\/download\/ldcc-20140209.tar.gz\"\nFILEPATH = \"\/work\/data\/ldcc-20140209.tar.gz\"\nEXTRACTDIR = \"\/work\/data\/livedoor\/\"\n!mkdir -p {EXTRACTDIR}","8a4c420f":"%%time\nurlretrieve(FILEURL, FILEPATH)\n\nmode = \"r:gz\"\ntar = tarfile.open(FILEPATH, mode) \ntar.extractall(EXTRACTDIR) \ntar.close()","270c312c":"def extract_txt(filename):\n    with open(filename) as text_file:\n        # 0: URL, 1: timestamp\n        text = text_file.readlines()[2:]\n        text = [sentence.strip() for sentence in text]\n        text = list(filter(lambda line: line != '', text))\n        return ''.join(text)\n    \ncategories = [ \n    name for name \n    in os.listdir( os.path.join(EXTRACTDIR, \"text\") ) \n    if os.path.isdir( os.path.join(EXTRACTDIR, \"text\", name) ) ]\n\ncategories = sorted(categories)\ncategories    ","f31b8d18":"table = str.maketrans({\n    '\\n': '',\n    '\\t': '\u3000',\n    '\\r': '',\n})\ntable","d0f6470d":"%%time\n\nall_text = []\nall_label = []\n\nfor cat in categories:\n    files = glob.glob(os.path.join(EXTRACTDIR, \"text\", cat, \"{}*.txt\".format(cat)))\n    files = sorted(files)\n    body = [ extract_txt(elem).translate(table) for elem in files ]\n    label = [cat] * len(body)\n    \n    all_text.extend(body)\n    all_label.extend(label)","3c32c16d":"df = pd.DataFrame({'text' : all_text, 'label' : all_label})\\\n    .sample(frac=1, random_state=23).reset_index(drop=True)\ndf.shape","1dceab8c":"df.head()","59c4a6ad":"df[:len(df) \/\/ 5].to_csv( os.path.join(EXTRACTDIR, \"test.tsv\"), sep='\\t', index=False)\ndf[len(df) \/\/ 5:len(df)*2 \/\/ 5].to_csv( os.path.join(EXTRACTDIR, \"dev.tsv\"), sep='\\t', index=False)\ndf[len(df)*2 \/\/ 5:].to_csv( os.path.join(EXTRACTDIR, \"train.tsv\"), sep='\\t', index=False)\n\n### 1\/5 of full training data.\n# df[:len(df) \/\/ 5].to_csv( os.path.join(EXTRACTDIR, \"test.tsv\"), sep='\\t', index=False)\n# df[len(df) \/\/ 5:len(df)*2 \/\/ 5].to_csv( os.path.join(EXTRACTDIR, \"dev.tsv\"), sep='\\t', index=False)\n# df[len(df)*2 \/\/ 5:].sample(frac=0.2, random_state=23).to_csv( os.path.join(EXTRACTDIR, \"train.tsv\"), sep='\\t', index=False)","d52ea819":"sys.path.append(\"\/kaggle\/input\/pretrained-bert-including-scripts\/master\/bert-master\")\nimport modeling\nimport optimization\nimport tensorflow as tf","50ba196b":"import sys\nsys.path.insert(0, \"\/kaggle\/input\/bert-ja-spm\/repository\/yoheikikuta-bert-japanese-004ee6f\/src\")\n\nimport tokenization_sentencepiece as tokenization\nfrom run_classifier import LivedoorProcessor\nfrom run_classifier import model_fn_builder\nfrom run_classifier import file_based_input_fn_builder\nfrom run_classifier import file_based_convert_examples_to_features\nfrom utils import str_to_value","bb89c850":"import configparser\nimport json\nimport glob\nimport os\nimport tempfile\n\nCONFIGPATH = '\/kaggle\/input\/bert-ja-spm\/repository\/yoheikikuta-bert-japanese-004ee6f\/config.ini'\nconfig = configparser.ConfigParser()\nconfig.read(CONFIGPATH)\nbert_config_file = tempfile.NamedTemporaryFile(mode='w+t', encoding='utf-8', suffix='.json')\nbert_config_file.write(json.dumps({k:str_to_value(v) for k,v in config['BERT-CONFIG'].items()}))\nbert_config_file.seek(0)\nbert_config = modeling.BertConfig.from_json_file(bert_config_file.name)","5b45f64d":"MAX_SEQ_LENGTH = 72\nFINETUNE_OUTPUT_DIR = '\/kaggle\/input\/bert-finetuned-for-livedoor-news-in-japanese\/finetune_output'\noutput_ckpts = glob.glob(\"{}\/model.ckpt*data*\".format(FINETUNE_OUTPUT_DIR))\nlatest_ckpt = sorted(output_ckpts)[-1]\nFINETUNED_MODEL_PATH = latest_ckpt.split('.data-00000-of-00001')[0]","730f773a":"class FLAGS(object):\n    '''Parameters.'''\n    def __init__(self):\n        self.model_file = \"\/kaggle\/input\/bert-ja\/wiki-ja.model\"\n        self.vocab_file = \"\/kaggle\/input\/bert-ja\/wiki-ja.vocab\"\n        self.do_lower_case = True\n        self.use_tpu = False\n        self.output_dir = \"\/dummy\"\n        self.data_dir = EXTRACTDIR\n        self.max_seq_length = MAX_SEQ_LENGTH\n        self.init_checkpoint = FINETUNED_MODEL_PATH\n        self.predict_batch_size = 4\n        \n        # The following parameters are not used in predictions.\n        # Just use to create RunConfig.\n        self.master = None\n        self.save_checkpoints_steps = 1\n        self.iterations_per_loop = 1\n        self.num_tpu_cores = 1\n        self.learning_rate = 0\n        self.num_warmup_steps = 0\n        self.num_train_steps = 0\n        self.train_batch_size = 0\n        self.eval_batch_size = 0","c30648c8":"FLAGS = FLAGS()","6d17de02":"processor = LivedoorProcessor()\nlabel_list = processor.get_labels()","6ae66702":"tokenizer = tokenization.FullTokenizer(\n    model_file=FLAGS.model_file, vocab_file=FLAGS.vocab_file,\n    do_lower_case=FLAGS.do_lower_case)\n\ntpu_cluster_resolver = None\nis_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\nrun_config = tf.contrib.tpu.RunConfig(\n    cluster=tpu_cluster_resolver,\n    master=FLAGS.master,\n    model_dir=FLAGS.output_dir,\n    save_checkpoints_steps=FLAGS.save_checkpoints_steps,\n    tpu_config=tf.contrib.tpu.TPUConfig(\n        iterations_per_loop=FLAGS.iterations_per_loop,\n        num_shards=FLAGS.num_tpu_cores,\n        per_host_input_for_training=is_per_host)\n)","7a893d9f":"model_fn = model_fn_builder(\n    bert_config=bert_config,\n    num_labels=len(label_list),\n    init_checkpoint=FLAGS.init_checkpoint,\n    learning_rate=FLAGS.learning_rate,\n    num_train_steps=FLAGS.num_train_steps,\n    num_warmup_steps=FLAGS.num_warmup_steps,\n    use_tpu=FLAGS.use_tpu,\n    use_one_hot_embeddings=FLAGS.use_tpu\n)\n\n\nestimator = tf.contrib.tpu.TPUEstimator(\n    use_tpu=FLAGS.use_tpu,\n    model_fn=model_fn,\n    config=run_config,\n    train_batch_size=FLAGS.train_batch_size,\n    eval_batch_size=FLAGS.eval_batch_size,\n    predict_batch_size=FLAGS.predict_batch_size\n)","45f4f167":"predict_examples = processor.get_test_examples(FLAGS.data_dir)\npredict_file = tempfile.NamedTemporaryFile(\n    mode='w+t', encoding='utf-8', suffix='.tf_record'\n)\n\nfile_based_convert_examples_to_features(\n    predict_examples, label_list,\n    FLAGS.max_seq_length, tokenizer,\n    predict_file.name\n)\n\npredict_drop_remainder = True if FLAGS.use_tpu else False\npredict_input_fn = file_based_input_fn_builder(\n    input_file=predict_file.name,\n    seq_length=FLAGS.max_seq_length,\n    is_training=False,\n    drop_remainder=predict_drop_remainder)","45c34f87":"result = estimator.predict(input_fn=predict_input_fn)","95075fe2":"%%time\nresult = list(result)","89859fec":"result[:2]","28f01b22":"test_df = pd.read_csv(os.path.join(EXTRACTDIR, \"test.tsv\"), sep='\\t')","c36668f5":"test_df['predict'] = [ label_list[elem['probabilities'].argmax()] for elem in result ]","a0165bfe":"test_df.head()","2d301a00":"sum( test_df['label'] == test_df['predict'] ) \/ len(test_df)","d7b5df07":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix","a57d35cd":"print(classification_report(test_df['label'], test_df['predict']))","b9381355":"print(confusion_matrix(test_df['label'], test_df['predict']))","45815f58":"## \u30e9\u30a4\u30d6\u30c9\u30a2\u30cb\u30e5\u30fc\u30b9\u306e\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9","6f0b70cb":"## Predict using the finetuned model","8cc5061f":"Data preprocessing.","e881181b":"Save data as tsv files.  \ntest:dev:train = 2:2:6. To check the usability of finetuning, we also prepare sampled training data (1\/5 of full training data).","816bc47d":"Read test data set and add prediction results.","b60f3434":"A littel more detailed check using `sklearn.metrics`.","90380a02":"\u4ee5\u4e0a\u3001BERT\u3067\u65e5\u672c\u8a9e\u30cb\u30e5\u30fc\u30b9\u306e\u30b8\u30e3\u30f3\u30eb\u3092\u4e88\u6e2c\u3057\u307e\u3057\u305f\u3002","341d47ff":"# \u65e5\u672c\u8a9eBERT\u3067\u30cb\u30e5\u30fc\u30b9\u306e\u30b8\u30e3\u30f3\u30eb\u3092\u4e88\u6e2c\n\nyoheikikuta\u3055\u3093\u306e[Finetuning of the pretrained Japanese BERT model](https:\/\/github.com\/yoheikikuta\/bert-japanese\/blob\/master\/notebook\/finetune-to-livedoor-corpus.ipynb)\u3092\u53c2\u8003\u306bKaggle\u3067\u65e5\u672c\u8a9eBERT\u3092\u4f7f\u3063\u3066\u30cb\u30e5\u30fc\u30b9\u306e\u30b8\u30e3\u30f3\u30eb\u3092\u4e88\u6e2c\u3057\u307e\u3059\u3002\n\n\u5fc5\u8981\u306a\u3082\u306e\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\u3002\n\n- [Pre-trained BERT, including scripts](https:\/\/www.kaggle.com\/supertaz\/pretrained-bert-including-scripts)\n- [Pretrained Japanese BERT model](https:\/\/www.kaggle.com\/vochicong\/bert-ja)\n- [Scripts for Japanese BERT model](https:\/\/www.kaggle.com\/vochicong\/bert-ja-spm)\n- [BERT finetuned for Livedoor news in Japanese](https:\/\/www.kaggle.com\/vochicong\/bert-finetuned-for-livedoor-news-in-japanese)\n- [Livedoor \u30cb\u30e5\u30fc\u30b9\u30b3\u30fc\u30d1\u30b9](https:\/\/www.rondhuit.com\/download.html)\n"}}