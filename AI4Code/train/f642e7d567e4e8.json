{"cell_type":{"50418743":"code","2f0c8603":"code","d9c49460":"code","7edd47de":"code","30c54eba":"code","4aa0aabf":"code","7d8a348b":"code","601d3bf5":"code","f87ff0f6":"code","0bd20dbf":"code","25a71d5e":"code","ce3af985":"code","8532c7d9":"code","c842b290":"code","42e13f58":"code","a1e06b88":"code","6cbeb499":"code","db1a2e59":"code","5c9b80da":"code","e2c1655c":"code","1bb3385b":"code","175d9eec":"code","81206f93":"code","719b997f":"code","318434f7":"markdown","db2def1f":"markdown","3dfddedd":"markdown","27f5fc07":"markdown","c7b4d948":"markdown","9e2f7b63":"markdown","ceacc301":"markdown","0ccbcb3f":"markdown","cd10ae36":"markdown","811b3d63":"markdown","55eed494":"markdown","3c28a4cc":"markdown","72a42246":"markdown","b4067672":"markdown"},"source":{"50418743":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2f0c8603":"#To support both python 2 and python 3\nfrom __future__ import division, print_function, unicode_literals\n\n#Common Imports\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\n#To plot pretty figures\n%matplotlib inline\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n\n#Ignore Warnings\nimport warnings\nwarnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n\nnp.random.seed(42)","d9c49460":"import tarfile\nfrom six.moves import urllib\n\nDOWNLOAD_ROOT = \"https:\/\/raw.githubusercontent.com\/ageron\/handson-ml\/master\/\"\nHOUSING_PATH = os.path.join(\"datasets\", \"housing\")\nHOUSING_URL = DOWNLOAD_ROOT + \"datasets\/housing\/housing.tgz\"\n\ndef fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n    os.makedirs(housing_path, exist_ok=True)\n    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n    urllib.request.urlretrieve(housing_url, tgz_path)\n    housing_tgz = tarfile.open(tgz_path)\n    housing_tgz.extractall(path=housing_path)\n    housing_tgz.close()\n    return\n\nfetch_housing_data()","7edd47de":"def load_housing_data(housing_path=HOUSING_PATH):\n    csv_path = os.path.join(housing_path, \"housing.csv\")\n    return pd.read_csv(csv_path)\n\nhousing_data = load_housing_data()\nprint(housing_data.head())","30c54eba":"#Getting the info about the dataset\nprint(housing_data.info())","4aa0aabf":"numerical_attributes = [\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\", \"total_bedrooms\", \"population\",\n                        \"households\", \"median_income\"]\ncategorical_attributes = [\"ocean_proximity\"]\nprint(housing_data.describe())","7d8a348b":"#Visualising The Data\n%matplotlib inline\nhousing_data.hist(bins=50, figsize=(20,15))\nplt.show()","601d3bf5":"from sklearn.model_selection import StratifiedShuffleSplit\n\nhousing_data[\"income_cat\"] = np.ceil(housing_data[\"median_income\"]\/1.5)\nhousing_data[\"income_cat\"].where(housing_data[\"income_cat\"]>5, 5.0, inplace=True)\ndata = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in data.split(housing_data, housing_data[\"income_cat\"]):\n    strat_train_set = housing_data.loc[train_index]\n    strat_test_set = housing_data.loc[test_index]\n\n# Dropping the \"income_cat\" attribute\nfor x in (strat_train_set, strat_test_set):\n    x.drop(\"income_cat\", axis=1, inplace=True)\n\ntraining_set = strat_train_set.copy()\ntesting_set = strat_test_set.copy()","f87ff0f6":"training_set.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1, \n                  s=training_set[\"population\"]\/100, label=\"Population\", c=\"median_house_value\",\n                  cmap=plt.get_cmap(\"jet\"), colorbar=True)","0bd20dbf":"correlation_matrix = training_set.corr()\nprint(correlation_matrix[\"median_house_value\"].sort_values(ascending=True))","25a71d5e":"from pandas.plotting import scatter_matrix\nscatter_matrix(training_set[[\"median_house_value\", \"total_rooms\", \"housing_median_age\", \"median_income\"]], figsize=(12, 8))","ce3af985":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\n\npredictors = training_set.drop(\"median_house_value\", axis=1)\nlabels = training_set[\"median_house_value\"].copy()\nimputer = SimpleImputer(strategy=\"median\")","8532c7d9":"rooms_ix, bedrooms_ix, population_ix, household_ix = [\n    list(predictors.columns).index(col)\n    for col in (\"total_rooms\", \"total_bedrooms\", \"population\", \"households\")]\nclass AttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedrooms_per_room=True):\n        self.add_bedrooms_per_room = add_bedrooms_per_room\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        rooms_per_household = X[:, rooms_ix]\/X[:, household_ix]\n        population_per_household = X[:, population_ix]\/X[:, household_ix]\n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:, bedrooms_ix]\/X[:, rooms_ix]\n            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n        else:\n            return np.c_[X, rooms_per_household, population_per_household]\n\nnumerical_pipeline = Pipeline([\n            ('imputer', SimpleImputer(strategy=\"median\")),\n            ('attribute_adder', AttributesAdder()),\n            ('Scaler', StandardScaler()),\n            ])\n\ncategorical_pipeline = Pipeline([\n            ('encoding', OneHotEncoder()),\n            ])\n\nfull_pipeline = ColumnTransformer([\n                (\"num\", numerical_pipeline, numerical_attributes),\n                (\"cat\", categorical_pipeline, categorical_attributes),\n            ])\nprepared_data = full_pipeline.fit_transform(predictors)\nprint(prepared_data)","c842b290":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\n\nlin_reg = LinearRegression()\nlin_reg.fit(prepared_data, labels)\npredictions = lin_reg.predict(prepared_data)\nrmse = np.sqrt(mean_squared_error(labels, predictions))\nvalidation_scores = cross_val_score(lin_reg, prepared_data, labels, scoring=\"neg_mean_squared_error\", cv=10)\nvalidation_scores = np.sqrt(-validation_scores)  #root mean squared error\nprint(\"ROOT MEAN SQUARED ERROR = \", rmse)\nprint(\"VALIDATION SCORES = \", validation_scores)","42e13f58":"from sklearn.preprocessing import PolynomialFeatures\npoly_features = PolynomialFeatures(degree=2, include_bias=False)\nX_poly = poly_features.fit_transform(prepared_data)\nP_reg = LinearRegression()\nP_reg.fit(X_poly, labels)\npredictions = P_reg.predict(X_poly)\nrmse = np.sqrt(mean_squared_error(labels, predictions))\nvalidation_scores = cross_val_score(P_reg, X_poly, labels, scoring=\"neg_mean_squared_error\", cv=10)\nvalidation_scores = np.sqrt(-validation_scores)  #root mean squared error\nprint(\"ROOT MEAN SQUARED ERROR = \", rmse)\nprint(\"VALIDATION SCORES = \", validation_scores)","a1e06b88":"from sklearn.linear_model import Ridge\nridge_reg = Ridge(alpha=1, solver=\"cholesky\")\nridge_reg.fit(prepared_data, labels)\npredictions = ridge_reg.predict(prepared_data)\nrmse = np.sqrt(mean_squared_error(labels, predictions))\nvalidation_scores = cross_val_score(ridge_reg, prepared_data, labels, scoring=\"neg_mean_squared_error\", cv=10)\nvalidation_scores = np.sqrt(-validation_scores)  #root mean squared error\nprint(\"ROOT MEAN SQUARED ERROR = \", rmse)\nprint(\"VALIDATION SCORES = \", validation_scores)","6cbeb499":"from sklearn.linear_model import Lasso\nlasso_reg = Lasso(alpha=0.1, max_iter=100000, tol=0.01)\nlasso_reg.fit(prepared_data, labels)\npredictions = lasso_reg.predict(prepared_data)\nrmse = np.sqrt(mean_squared_error(labels, predictions))\nvalidation_scores = cross_val_score(lasso_reg, prepared_data, labels, scoring=\"neg_mean_squared_error\", cv=10)\nvalidation_scores = np.sqrt(-validation_scores)  #root mean squared error\nprint(\"ROOT MEAN SQUARED ERROR = \", rmse)\nprint(\"VALIDATION SCORES = \", validation_scores)","db1a2e59":"from sklearn.svm import LinearSVR\nsvm_reg = LinearSVR(epsilon=10, C=10000)\nsvm_reg.fit(prepared_data, labels)\npredictions = svm_reg.predict(prepared_data)\nrmse = np.sqrt(mean_squared_error(labels, predictions))\nvalidation_scores = cross_val_score(svm_reg, prepared_data, labels, scoring=\"neg_mean_squared_error\", cv=10)\nvalidation_scores = np.sqrt(-validation_scores)  #root mean squared error\nprint(\"ROOT MEAN SQUARED ERROR = \", rmse)\nprint(\"VALIDATION SCORES = \", validation_scores)","5c9b80da":"from sklearn.svm import SVR\nsvm_reg = SVR(kernel=\"poly\", degree=2, C=10000, epsilon=0.1)\nsvm_reg.fit(prepared_data, labels)\npredictions = svm_reg.predict(prepared_data)\nrmse = np.sqrt(mean_squared_error(labels, predictions))\nvalidation_scores = cross_val_score(svm_reg, prepared_data, labels, scoring=\"neg_mean_squared_error\", cv=10)\nvalidation_scores = np.sqrt(-validation_scores)  #root mean squared error\nprint(\"ROOT MEAN SQUARED ERROR = \", rmse)\nprint(\"VALIDATION SCORES = \", validation_scores)","e2c1655c":"from sklearn.tree import DecisionTreeRegressor\ntree_reg = DecisionTreeRegressor(random_state=42)\ntree_reg.fit(prepared_data, labels)\npredictions = tree_reg.predict(prepared_data)\nrmse = np.sqrt(mean_squared_error(labels, predictions))\nvalidation_scores = cross_val_score(svm_reg, prepared_data, labels, scoring=\"neg_mean_squared_error\", cv=10)\nvalidation_scores = np.sqrt(-validation_scores)  #root mean squared error\nprint(\"ROOT MEAN SQUARED ERROR = \", rmse)\nprint(\"VALIDATION SCORES = \", validation_scores)","1bb3385b":"from sklearn.ensemble import RandomForestRegressor\nforest_reg = RandomForestRegressor(n_estimators=10, random_state=42)\nforest_reg.fit(prepared_data, labels)\npredictions = forest_reg.predict(prepared_data)\nrmse = np.sqrt(mean_squared_error(labels, predictions))\nvalidation_scores = cross_val_score(svm_reg, prepared_data, labels, scoring=\"neg_mean_squared_error\", cv=10)\nvalidation_scores = np.sqrt(-validation_scores)  #root mean squared error\nprint(\"ROOT MEAN SQUARED ERROR = \", rmse)\nprint(\"VALIDATION SCORES = \", validation_scores)","175d9eec":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n            {'n_estimators': [3, 10, 20, 25, 30], 'max_features': [2, 4, 6, 8], 'bootstrap': [True, False]}\n            ]\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring=\"neg_mean_squared_error\", return_train_score=True)\ngrid_search.fit(prepared_data, labels)\nprint(\"The best parameters for Random Forest Regressor is : \", grid_search.best_params_)\nscores = grid_search.cv_results_\nfor mean_score, params in zip(scores[\"mean_test_score\"], scores[\"params\"]):\n    print(np.sqrt(-mean_score), params)","81206f93":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nparam_distribs = [\n            {'n_estimators': randint(low=1, high=200),\n             'max_features': randint(low=1, high=8),\n             'bootstrap': [True, False]}\n            ]\nrandom_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs, cv=5,\n                                   n_iter=10, scoring=\"neg_mean_squared_error\", random_state=42)\nrandom_search.fit(prepared_data, labels)\nprint(\"The best parameters for Random Forest Regressor is : \", random_search.best_params_)\nscores = random_search.cv_results_\nfor mean_score, params in zip(scores[\"mean_test_score\"], scores[\"params\"]):\n    print(np.sqrt(-mean_score), params)","719b997f":"final_model = random_search.best_estimator_\nx_test = testing_set.drop(\"median_house_value\", axis=1)\ny_test = testing_set[\"median_house_value\"].copy()\n\nx_test_prepared = full_pipeline.transform(x_test)\nfinal_predictions = final_model.predict(x_test_prepared)\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)\nprint(\"The root mean squared error of the test data is :\", final_rmse)","318434f7":"**3. RIDGE REGRESSION**","db2def1f":"**4. LASSO REGRESSION**","3dfddedd":"# **VISUALIZING THE DATA TO GAIN INSIGHTS**","27f5fc07":"# ****TRAINING A MODEL****\n****1. LINEAR REGRESSION****","c7b4d948":"# **SPLITTING THE DATA**","9e2f7b63":"**8. RANDOM FOREST REGREESOR**","ceacc301":"# **FINE TUNING THE SELECTED MODEL**","0ccbcb3f":"# **GET THE DATA**","cd10ae36":"# **NOW WE PREPARE THE DATA FOR ML ALGORITHM**","811b3d63":"# **FINAL MODEL**","55eed494":"**7. DESICION TREE REGRESSOR**","3c28a4cc":"**2. POLYNOMIAL REGRESSION**","72a42246":"**6. KERNELIZED SVM REGRESSOR**","b4067672":"**5. SVM REGRESSOR**"}}