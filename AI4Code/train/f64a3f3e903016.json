{"cell_type":{"4d73572d":"code","3e46756f":"code","e176de43":"code","9e562f4c":"code","b722194b":"code","20094994":"code","bcd7ea49":"code","bb3097bf":"code","a23b4ba5":"code","2b7eb0df":"code","f6aca0d9":"code","237187ad":"code","a9a30103":"code","d2de76ba":"code","053a5ee1":"code","cfe1deed":"code","3b5b7c5b":"code","ab0c4958":"code","e8988916":"code","5ba89b8b":"code","70cb4e8a":"code","88b4ecd9":"code","c3584402":"code","60864482":"code","00c405ae":"code","b42d4753":"code","193ce013":"code","25645fd3":"code","b976f527":"code","a1116368":"markdown","d33914f3":"markdown","b3677c8a":"markdown","0eb9e0a3":"markdown"},"source":{"4d73572d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3e46756f":"!nvidia-smi","e176de43":"import tensorflow as tf\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output","9e562f4c":"path = \"\/kaggle\/input\/landscape-pictures\/\"\nnames = os.listdir(path)\n#Number of images to use (n)\nn = None\ntr_urls = np.copy(names[:n])\nprint('''\nAll images {}\nTrain images {}\n'''.format(len(names),len(tr_urls)))","b722194b":"#The resolution of the training images will be 256x256 with 3 channels\ndef open_image(filename):\n    img = tf.cast(tf.image.decode_jpeg(tf.io.read_file(path + \"\/\" + filename),channels=3),tf.float32)\n    img = tf.image.resize(img,[256,256],method='area')\n    img = img\/127.5-1\n    return img","20094994":"train_dataset = tf.data.Dataset.from_tensor_slices(tr_urls)\ntrain_dataset = train_dataset.map(open_image,num_parallel_calls=tf.data.AUTOTUNE)\ntrain_dataset = train_dataset.batch(32)","bcd7ea49":"for img in train_dataset.take(1):\n    plt.imshow(img[0],interpolation=\"spline16\")\n    plt.show()","bb3097bf":"!pip install tensorflow-addons","a23b4ba5":"from tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Model,Sequential\n#Even though I import Instance Normalization, I don't recommend using it for this architecture\nfrom tensorflow_addons.layers import InstanceNormalization","2b7eb0df":"def Upsample(filters,size=3,use_drop=False):\n    init = tf.random_normal_initializer(0,0.02)\n    layer = Sequential()\n    layer.add(Conv2DTranspose(filters,\n                              size,\n                              kernel_initializer=init,\n                              padding='same',\n                              strides=2,\n                              use_bias=False,))\n    layer.add(BatchNormalization())\n    if use_drop:\n        layer.add(Dropout(0.5))\n    layer.add(ReLU())\n    return layer","f6aca0d9":"def Generator():\n    input_noise = Input(shape=[512,])\n    layers = [\n        Dense(512*4*4),\n        Reshape((4,4,512)),\n        Upsample(512,use_drop=True),\n        Upsample(512,use_drop=True),\n        Upsample(512),\n        Upsample(256),\n        Upsample(128),\n        Upsample(64),\n    ]\n    last = Conv2DTranspose(3,4,padding='same',activation='tanh')\n    x = input_noise\n    for layer in layers:\n        x = layer(x)\n    last = last(x)\n    return Model(input_noise,last)","237187ad":"def Downsample(filters,size=4,strides=2,use_instance=True):\n    init = tf.random_normal_initializer(0,0.02)\n    layer = Sequential()\n    layer.add(Conv2D(filters,\n                     size,\n                     strides=strides,\n                     kernel_initializer=init,\n                     padding='same',\n                     use_bias=not use_instance))\n    if use_instance:\n        layer.add(BatchNormalization())\n    layer.add(LeakyReLU(0.2))\n    return layer","a9a30103":"#The type of discriminator used is PatchGAN type\ndef Discriminator():\n    input_image = Input(shape=[256,256,3])\n    layers = [\n        Downsample(64,use_instance=False),\n        Downsample(128),\n        Downsample(256),\n        Downsample(512),\n    ]\n    last = Conv2D(1,4,padding='same')\n    x = input_image\n    for layer in layers:\n        x = layer(x)\n    last = last(x)\n    return Model(input_image,last)","d2de76ba":"generator = Generator()\ndiscriminator = Discriminator()","053a5ee1":"generator.summary()","cfe1deed":"discriminator.summary()","3b5b7c5b":"generator_optimizer = tf.keras.optimizers.Adam(1e-4,beta_1=0.5)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1e-4,beta_1=0.5)","ab0c4958":"#To save and restore data\nckpt_path = \"\/kaggle\/output\/\/kaggle\/working\/checkpoints\/\"\nckpt_prefix = os.path.join(ckpt_path,\"ckpt\")\nckpt = tf.train.Checkpoint(\n    generator = generator,\n    discriminator = discriminator,\n    generator_optimizer = generator_optimizer,\n    discriminator_optimizer = discriminator_optimizer\n)","e8988916":"ckpt.save(ckpt_prefix)","5ba89b8b":"!ls \/kaggle\/output\/kaggle\/working\/checkpoints\/","70cb4e8a":"ckpt.restore(tf.train.latest_checkpoint(ckpt_path))","88b4ecd9":"def generate_images():\n    noise = tf.random.normal([4,512])\n    preds = generator(noise,training=True)*0.5+0.5\n    plt.figure(figsize=(7,7))\n    for i,img in enumerate(preds):\n        plt.subplot(2,2,i+1)\n        plt.axis('off')\n        plt.imshow(img,interpolation=\"spline16\")\n    plt.show()","c3584402":"generate_images()","60864482":"loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)","00c405ae":"def generator_loss(fake_out):\n    return loss_object(tf.ones_like(fake_out),fake_out)\n\ndef discriminator_loss(real_out,fake_out):\n    real_loss = loss_object(tf.ones_like(real_out),real_out)\n    gen_loss = loss_object(tf.zeros_like(fake_out),fake_out)\n    return real_loss + gen_loss","b42d4753":"@tf.function\ndef train_step(img):\n    noise = tf.random.normal([tf.shape(img)[0],512])\n    with tf.GradientTape() as gen_tape,tf.GradientTape() as disc_tape:\n        preds = generator(noise,training=True)\n        \n        real_out = discriminator(img,training=True)\n        fake_out = discriminator(preds,training=True)\n        \n        gen_loss = generator_loss(fake_out)\n        disc_loss = discriminator_loss(real_out,fake_out)\n        \n        gen_grads = gen_tape.gradient(gen_loss,generator.trainable_variables)\n        disc_grads = disc_tape.gradient(disc_loss,discriminator.trainable_variables)\n        \n        generator_optimizer.apply_gradients(zip(gen_grads,generator.trainable_variables))\n        discriminator_optimizer.apply_gradients(zip(disc_grads,discriminator.trainable_variables))","193ce013":"def train(epochs=1):\n    for epoch in range(epochs):\n        clear_output()\n        generate_images()\n        i = 0\n        print(\"Epoch {}\".format(epoch+1))\n        for img in train_dataset:\n            train_step(img)\n            print(\".\",end=\"\")\n            i+=1\n            if i%100==0:\n                print()","25645fd3":"train(25)","b976f527":"train(25)","a1116368":"# ***Defining the discriminator***","d33914f3":"# ***Defining the generator***","b3677c8a":"# ***Training***","0eb9e0a3":"# ***Importing libraries and training data***"}}