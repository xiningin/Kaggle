{"cell_type":{"bca2d458":"code","33310d6c":"code","709b4e91":"code","25cfd2ae":"code","c3021a1f":"code","9dd77b87":"code","0af6c00d":"code","e69b04cc":"code","3810c057":"code","a64f60a5":"code","d0de678a":"code","011638f0":"code","5c3db952":"code","e5ca8612":"code","6eb47fcc":"code","d6862646":"code","9e6aeb0f":"code","837abf18":"code","9d145148":"code","d67f56d5":"code","06101dba":"code","deaf7c6d":"code","a45c2df2":"code","dd668af6":"code","7a38ff3d":"code","7bf77e10":"code","de139a67":"code","9d745c1e":"code","2ba02dc4":"code","6b672fd5":"code","81478f0a":"code","727e8781":"code","041b19ef":"code","8316802f":"code","d09e5249":"code","7ec175ea":"code","7441453a":"code","be2ec5a2":"code","4124ab0e":"code","fce18aaa":"code","76b2f646":"code","9ba5025a":"code","e4b74637":"code","f32ccd50":"code","135941dc":"code","5d7b389c":"code","e9cb9d9b":"code","4129bea7":"code","9f29b038":"code","f716a143":"code","88132fe4":"code","2b69cbfb":"code","3475b758":"code","2f0d927c":"code","2334c1c1":"code","fd622bba":"code","721d9f7a":"code","980777f7":"code","89993066":"code","22ad5db4":"code","ec3c61fd":"code","2ade9663":"code","d6b5a809":"code","358798cb":"code","6854389c":"code","567efe2c":"code","d0a70709":"code","8e7fe961":"code","cc76d9a3":"code","0e0aa26b":"code","8a123dbb":"code","fc8e99cc":"code","1f304c6b":"code","2b6148f2":"code","24c8375e":"code","54b24536":"code","18da6430":"code","203e684d":"code","c204b7db":"code","c2b2399e":"code","034169e6":"code","bb8c4d7c":"code","841687e3":"code","56294ce5":"code","40313c04":"code","a6a4970a":"code","db3ccafd":"code","4aef9d57":"code","13c37d57":"code","af1eb4c2":"code","c513be4d":"code","6a72da7c":"code","ed3d5ddd":"code","837fe8e1":"code","5038adbd":"code","01b01eb6":"code","382e32f9":"code","c6e4c610":"code","4fdf70f2":"code","e418aeae":"code","c96eff80":"code","5a8da551":"code","3135c402":"code","3537d5b9":"code","06eb2ac0":"code","18f14a57":"code","4f0b1f7f":"code","d46d0133":"code","b83d362a":"code","174cef1a":"code","be794db9":"code","88335997":"code","04fc1818":"code","ee7fc5b1":"code","beb21f22":"code","49e45936":"code","3c02b39b":"code","fa5d7cf5":"code","815eb9be":"code","66c3e2e6":"code","dbb2204b":"code","337287ba":"code","3e61e72b":"code","5d82be84":"code","03a4b0d0":"code","71727c03":"code","96be66eb":"code","cf290e66":"code","df853741":"code","1b0a9eaf":"code","b4528d63":"code","9e9e5155":"code","4315c095":"code","fff4afbb":"code","4850fca7":"code","a0e27e8e":"code","f0d985d5":"code","f05853ec":"code","8e8de123":"code","22be1c5a":"code","92eaa923":"code","a424f192":"code","234e6293":"code","2622b5e7":"code","c06a21d5":"code","81e3cacc":"code","94d5e4c0":"code","000f5f75":"code","42683970":"code","3eaf2d8f":"code","6cc666ff":"code","f0f298f9":"code","bf08eb37":"code","03db91ad":"code","285db821":"code","6700b392":"code","adb7e5bf":"code","22c97129":"code","77a120ea":"code","c5285cfb":"code","79100e08":"code","88882b8d":"code","99d7ab68":"code","0c231941":"code","6c7513cb":"code","f6597684":"code","a9cddd25":"code","3aa3291a":"code","b212a460":"code","852856bd":"code","a6b402ff":"code","18908784":"code","39fc3575":"code","373b9084":"code","67413085":"code","7334dfb2":"code","65894683":"code","71eb8a62":"code","b44dd32d":"code","a8251860":"code","3c3beac7":"code","a978707f":"code","c8c79de7":"code","8023900c":"code","2292e429":"code","39f46e5e":"code","cd2ec93b":"code","94fdda7a":"code","3c5d3058":"code","4689b33b":"markdown","4182401a":"markdown","4aab55e4":"markdown","02cad2e5":"markdown","09a322f9":"markdown","252c5193":"markdown","41fa05ec":"markdown","9e08d05e":"markdown","ef47f8ac":"markdown","ca2ee909":"markdown","b75e2339":"markdown","b3dfcd62":"markdown","4b32c7d9":"markdown","52381a57":"markdown","421e6325":"markdown","0ff16fdd":"markdown","0c1b7e6f":"markdown","5d8de678":"markdown","03295c3e":"markdown","26c5db02":"markdown","283c2b23":"markdown","749eccc4":"markdown","8541d003":"markdown","7caf1a76":"markdown","1a39fd08":"markdown","cf7581ef":"markdown","3b7fde22":"markdown","7495e3c7":"markdown","237f896f":"markdown","03c82759":"markdown","3223433a":"markdown","6957a76e":"markdown","8d8b5ab9":"markdown","4b128d4b":"markdown","26baade4":"markdown","0af161c8":"markdown","cb8db5fc":"markdown","014b5d7d":"markdown","a4ac5875":"markdown","660fea5b":"markdown"},"source":{"bca2d458":"import numpy as np\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split, GridSearchCV,cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import scale \nfrom sklearn import model_selection\nfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import BaggingRegressor\n\nfrom warnings import filterwarnings #filtre hatalarini ignore edip kapatmak icin kullandim\nfilterwarnings('ignore')","33310d6c":"hit = pd.read_csv(\"..\/input\/hitters\/Hitters.csv\")\ndf = hit.copy()\ndf = df.dropna()\ndms = pd.get_dummies(df[['League', 'Division', 'NewLeague']])\ny = df[\"Salary\"]\nX_ = df.drop(['Salary', 'League', 'Division', 'NewLeague'], axis=1).astype('float64')\nX = pd.concat([X_, dms[['League_N', 'Division_W', 'NewLeague_N']]], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.25, \n                                                    random_state=42)","709b4e91":"knn_model= KNeighborsRegressor().fit(X_train,y_train) #KNN modelimizi olusturdum","25cfd2ae":"knn_model.n_neighbors # 5 adeti komsu olarak alacagiz en yakin 5 elemana gore tahmin yapilacak yani","c3021a1f":"y_pred=knn_model.predict(X_test)","9dd77b87":"np.sqrt(mean_squared_error(y_test,y_pred)) #test hatamizi 426 olarak verdi","0af6c00d":"RMSE = []            #farkli k degerleri icin gozlem yapmak istedigimden bir dongu yazdim 1 den 10 a kadar k degerleri \n                     #icin veri setini ayri ayri egittim ve hata hesabi yaptim her iterasyonda model fit edicez ve tahmin\n                    #islemi yapicaz\nfor k in range(10): \n    k=k+1\n    knn_model=KNeighborsRegressor(n_neighbors=k).fit(X_train,y_train)\n    y_pred=knn_model.predict(X_train)\n    rmse=np.sqrt(mean_squared_error(y_train,y_pred)) #hesapladigimiz hata train hatasi test degil\n    RMSE.append(rmse)\n    print(\"k=\",k,\" icin RMSE degeri: \",rmse )","e69b04cc":"from sklearn.model_selection import GridSearchCV\n\n#Grid search bir olasi parametre setinin verilip, tum olasi kombinasyonlarin denenmesi anlamina gelir","3810c057":"knn_params={\"n_neighbors\":np.arange(1,30,1)} #dictionary yapisi kullaniyoruz","a64f60a5":"np.arange(1,30,1) ","d0de678a":"knn= KNeighborsRegressor()","011638f0":"knn_cv_model=GridSearchCV(knn,knn_params,cv=10)","5c3db952":"knn_cv_model.fit(X_train,y_train)","e5ca8612":"knn_cv_model.best_params_[\"n_neighbors\"] #en iyi k degeri 8 olarak bulunmus oldu","6eb47fcc":"RMSE = [] \nRMSE_CV = []\nfor k in range(10):\n    k = k+1\n    knn_model = KNeighborsRegressor(n_neighbors = k).fit(X_train, y_train)\n    y_pred = knn_model.predict(X_train) \n    rmse = np.sqrt(mean_squared_error(y_train,y_pred)) \n    rmse_cv = np.sqrt(-1*cross_val_score(knn_model, X_train, y_train, cv=10, \n                                         scoring = \"neg_mean_squared_error\").mean())\n    RMSE.append(rmse) \n    RMSE_CV.append(rmse_cv)\n    print(\"k =\" , k , \"i\u00e7in RMSE de\u011feri: \", rmse, \"RMSE_CV de\u011feri: \", rmse_cv )\n","d6862646":"knn_tuned = KNeighborsRegressor(n_neighbors = knn_cv_model.best_params_[\"n_neighbors\"]) #k degerini valide edilmis olan\n                                                                    #modeldeki buldugumuz en iyi K degerini verdik yani 8","9e6aeb0f":"knn_tuned.fit(X_train, y_train)  #modelimiz fit ettik","837abf18":"y_pred= knn_tuned.predict(X_test) #y tahmin degerimizi X_test degerleri uzerinden elde ettik","9d145148":"np.sqrt(mean_squared_error(y_test, y_pred)) #en son y tahmin degerimizle gercek y_test degerlerini karsilastirarak\n                                            #test hatamizi hesaplamis olduk","d67f56d5":"hit = pd.read_csv(\"..\/input\/hitters\/Hitters.csv\")\ndf = hit.copy()\ndf = df.dropna()\ndms = pd.get_dummies(df[['League', 'Division', 'NewLeague']])\ny = df[\"Salary\"]\nX_ = df.drop(['Salary', 'League', 'Division', 'NewLeague'], axis=1).astype('float64')\nX = pd.concat([X_, dms[['League_N', 'Division_W', 'NewLeague_N']]], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.25, \n                                                    random_state=42)","06101dba":"X_train=pd.DataFrame(X_train[\"Hits\"]) #degiskenlerimizi Hits olarak sectim bagimli degisken ayni Sallary\nX_test=pd.DataFrame(X_test[\"Hits\"])","deaf7c6d":"from sklearn.svm import SVR","a45c2df2":"svr_model= SVR(\"linear\").fit(X_train,y_train)","dd668af6":"svr_model.predict(X_train)[0:10] #10 tahmin degerini gozlemledim","7a38ff3d":"print(\"y= {0} + {1} x\".format(svr_model.intercept_[0],            #model denklemi model sabiti\n                              svr_model.coef_[0][0]))             #beta1 katsayimiz","7bf77e10":"X_train[\"Hits\"][0:1]","de139a67":"-48.69756097561513 + 4.969512195122093*91  #ilk degerin bu olmasi gerekiyor 91 verildiginde yukardaki kurdugum modelde\n                                           #yerine koyma yaptim ve  predict yaptigimiz degerlerdede ilk deger ayni cikti\n                                           #demekki basit model denklemimiz dogru ","9d745c1e":"y_pred= svr_model.predict(X_train) #y tahmin degerimiz","2ba02dc4":"plt.scatter(X_train,y_train)\nplt.plot(X_train,y_pred,c=\"r\")","6b672fd5":"from sklearn.linear_model import LinearRegression     #dogrusal regresyonun formulasyonunu yapacagim simdi\nlm_model= LinearRegression().fit(X_train,y_train)\nlm_pred=lm_model.predict(X_train)\nprint(\"y={0} +{1} x\".format(lm_model.intercept_,lm_model.coef_[0])) #farkli bir prit kullanimi {0} ve {1} yazilan yerler\n                                                                    #.format() fonksiyonun 1. ve 2. elemanina denk gelir","81478f0a":"-8.814095480334345 + 5.172456135470686*91 #bunun verdigi tahmin degeri ise 461 oldu SVR in tahmini 403 du\n                                        #2 side ayni islemi yapan 2 farkli algoritma 2 side farkli tahminlerde bulundu\n    #bu tahminlerdeki farkliligin sebebi : alttaki grafikten anlasilabiliyor 2 sininde cizdigi dogrular farkli","727e8781":"plt.scatter(X_train, y_train, alpha=0.5, s=23)\nplt.plot(X_train, lm_pred, color='b')         #dogrusal model\nplt.plot(X_train, y_pred, color='r')    #SVR model\n\nplt.xlabel(\"At\u0131\u015f Say\u0131s\u0131(Hits)\")\nplt.ylabel(\"Maa\u015f (Salary)\")          #Linear algoritma daha yukarda tahmin vermisti 461 verdiginde SVR 403 vermisti \n                            #dolayisiyla mavi olan cizgi Linear model\n    \n    \n    #Dogru farkinin sebebi gorulecegi uzere birsuru aykiri gozlem var 2000 2500 civarlarinda bu linear modeli yukari \n    #cekmektedir En kucuk kareler yonteminden dolayi ama bu aykiri gozlemlerin etkisi SVR da daha dusuktur yani aslinda\n    #aykiri gozlemleri SVR daha iyi ele alir ve onlarin etkisini azaltir olmasi gereken aslinda budur o yuzden modeller\n    #arasinda bu tahmin farklari olustu.","041b19ef":"print(\"y= {0} + {1} x\".format(svr_model.intercept_[0],svr_model.coef_[0][0]))  ","8316802f":"svr_model.predict([[91]]) #yukardaki formatta elle koyup yapmistik fonksiyon ile yapmak istedigimizde bu sekilde olur","d09e5249":"y_pred=svr_model.predict(X_test)   #test hatamizi hesaplayacagiz simdi","7ec175ea":"np.sqrt(mean_squared_error(y_test,y_pred))   # y test hatimiz 459 olarak cikti","7441453a":"svr_params =  {\"C\":np.arange(0.1,2,0.1)}    #0.1 den 2 ye kadar 0.1 lik artislarla artan bir array olusturduk\nsvr_cv_model= GridSearchCV(svr_model,svr_params,cv=10).fit(X_train,y_train)","be2ec5a2":"svr_cv_model.best_params_ #bize onerdigi parametre 0.1 olarak geldi","4124ab0e":"pd.Series(svr_cv_model.best_params_)[0] #bu degeri aldim arasindan pd.Series kullanarak 0.1 degerini best_params_ dan \u00e7ektim","fce18aaa":"svr_tuned = SVR(\"linear\", C=pd.Series(svr_cv_model.best_params_)[0]).fit(X_train,y_train) #burda Tune etmek icin SVR da C degerine 0.1 degerini atadim","76b2f646":"y_pred=svr_tuned.predict(X_test)","9ba5025a":"np.sqrt(mean_squared_error(y_test,y_pred)) #test hatamiz 458 olarak bulundu      #burda sadece 1 tane ba\u011f\u0131ms\u0131z degisken kullandigim icin 458 cikti\n\n#yani assagidaki islemleri yapmaz isem derlemesi 10 15 dk suruyor ama daha dogru ve iyi bir parametre getiriyor 458 den hata oranimiz tum bagimsiz degiskenleri \n#aldigimizda 367 geldi baya buyuk bir iyilestirme 80 birimlik bir iyelestirme cok yuksek ve su ana kadar aldigimiz en dusuk hata degeri\n\n#X_train=pd.DataFrame(X_train[\"Hits\"]) \n#X_test=pd.DataFrame(X_test[\"Hits\"])       ","e4b74637":"np.sqrt(mean_squared_error(y_test,y_pred))","f32ccd50":"np.random.seed(3)\n\nx_sim = np.random.uniform(2, 10, 145)\ny_sim = np.sin(x_sim) + np.random.normal(0, 0.4, 145)\n\nx_outliers = np.arange(2.5, 5, 0.5)\ny_outliers = -5*np.ones(5)\n\nx_sim_idx = np.argsort(np.concatenate([x_sim, x_outliers]))\nx_sim = np.concatenate([x_sim, x_outliers])[x_sim_idx]\ny_sim = np.concatenate([y_sim, y_outliers])[x_sim_idx]","135941dc":"\nfrom sklearn.linear_model import LinearRegression\nols = LinearRegression()\nols.fit(np.sin(x_sim[:, np.newaxis]), y_sim)\nols_pred = ols.predict(np.sin(x_sim[:, np.newaxis]))\n\nfrom sklearn.svm import SVR\neps = 0.1\nsvr = SVR('rbf', epsilon = eps)\nsvr.fit(x_sim[:, np.newaxis], y_sim)\nsvr_pred = svr.predict(x_sim[:, np.newaxis])","5d7b389c":"plt.scatter(x_sim, y_sim, alpha=0.5, s=26)\nplt_ols, = plt.plot(x_sim, ols_pred, 'g')\nplt_svr, = plt.plot(x_sim, svr_pred, color='r')\nplt.xlabel(\"Ba\u011f\u0131ms\u0131z De\u011fi\u015fken\")\nplt.ylabel(\"Ba\u011f\u0131ml\u0131 De\u011fi\u015fken\")\nplt.ylim(-5.2, 2.2)\nplt.legend([plt_ols, plt_svr], ['EKK', 'SVR'], loc = 4);","e9cb9d9b":"hit = pd.read_csv(\"..\/input\/hitters\/Hitters.csv\")\ndf = hit.copy()\ndf = df.dropna()\ndms = pd.get_dummies(df[['League', 'Division', 'NewLeague']])\ny = df[\"Salary\"]\nX_ = df.drop(['Salary', 'League', 'Division', 'NewLeague'], axis=1).astype('float64')\nX = pd.concat([X_, dms[['League_N', 'Division_W', 'NewLeague_N']]], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.25, \n                                                    random_state=42)","4129bea7":"svr_rbf = SVR(\"rbf\").fit(X_train, y_train) #model kurma k\u0131sm\u0131","9f29b038":"y_pred=svr_rbf.predict(X_test) #tahmin degerimiz","f716a143":"np.sqrt(mean_squared_error(y_test,y_pred)) #test hata degerimiz","88132fe4":"svr_params={\"C\": [0.1,0.4,5,10,20,30,40,50]}\nsvr_cv_model=GridSearchCV(svr_rbf,svr_params,cv=10)\nsvr_cv_model.fit(X_train,y_train) #modeli kurduk ","2b69cbfb":"svr_cv_model.best_params_     #en iyi parametreyi ogreniyoruz","3475b758":"pd.Series(svr_cv_model.best_params_)[0] #50 degerine eristik pandas serileri kullanarak","2f0d927c":"svr_tuned=SVR(\"rbf\",C=pd.Series(svr_cv_model.best_params_)[0]).fit(X_train,y_train)","2334c1c1":"y_pred=svr_tuned.predict(X_test)\nnp.sqrt(mean_squared_error(y_test,y_pred)) #428 degerine ulastim 460 dan 428 e dusmus 32 birimlik bir dusus diger yontemlere\n                                           #gore biraz fazla cikti hata","fd622bba":"hit = pd.read_csv(\"..\/input\/hitters\/Hitters.csv\")\ndf = hit.copy()\ndf = df.dropna()\ndms = pd.get_dummies(df[['League', 'Division', 'NewLeague']])\ny = df[\"Salary\"]\nX_ = df.drop(['Salary', 'League', 'Division', 'NewLeague'], axis=1).astype('float64')\nX = pd.concat([X_, dms[['League_N', 'Division_W', 'NewLeague_N']]], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.25, \n                                                    random_state=42)","721d9f7a":"from sklearn.preprocessing import StandardScaler","980777f7":"scaler=StandardScaler()\nscaler.fit(X_train)","89993066":"X_train_scaled=scaler.transform(X_train)","22ad5db4":"X_test_scaled=scaler.transform(X_test)","ec3c61fd":"from sklearn.neural_network import MLPRegressor","2ade9663":"mlp_model = MLPRegressor(hidden_layer_sizes=(100,20)).fit(X_train_scaled,y_train) #modeli olusturduk\n                                                                                  #gizli katman sayisi uzerinde oynadik","d6b5a809":"mlp_model.n_layers_ #katman sayisi","358798cb":"mlp_model.hidden_layer_sizes #gizli katman sayisi","6854389c":"mlp_model.predict(X_train_scaled)[0:10] #tahmin edilmis degerler","567efe2c":"y_pred=mlp_model.predict(X_test_scaled)","d0a70709":"np.sqrt(mean_squared_error(y_test,y_pred)) #452 bulundu hata degeri","8e7fe961":"#BURADA sozluk yapisi kullanarak tune etmek icin icine atip deneyecegimiz parametreleri liste olarak olusturduk ve hepsini\n#verip en iyi sonucu verenleri kullancagiz\nmlp_params = {\"alpha\":[0.1,0.01,0.02,0.005],\n              \"hidden_layer_sizes\": [(20,20),(100,50,150),(300,200,150)],\n              \"activation\":[\"relu\",\"logistic\"]\n             }","cc76d9a3":"#mlp_cv_model=GridSearchCV(mlp_model,mlp_params,cv=10) #Cross validation yontemiyle modelimizi ve yukarida olusturdugumuz\n                                                      #parametreleri kullanarak en iyi parametrelere ulasacagiz","0e0aa26b":"#mlp_cv_model.fit(X_train_scaled,y_train) ","8a123dbb":"#mlp_cv_model.best_params_                    #en iyi parametrelere ulastik\n\n#{'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (100, 50, 150)} bu sonucu aldim en ideal tune icin \n#parametreler bunlar geldi","fc8e99cc":"#mlp_tuned = MLPRegressor(alpha=0.01,hidden_layer_sizes=(100,50,150),activation=\"relu\") #tuned edilmis modeli olusturdum\n                                                                                    #elde ettigimiz parametre degerleriyle","1f304c6b":"#mlp_tuned.fit(X_train_scaled,y_train)","2b6148f2":"#y_pred=mlp_tuned.predict(X_test_scaled)","24c8375e":"#np.sqrt(mean_squared_error(y_test,y_pred))  #tune edilmis hata 361 olarak cikti. SU ana kadar en dusuk hata degerini\n                                            #veren model yapay sinir agi oldu.","54b24536":"hit = pd.read_csv(\"..\/input\/hitters\/Hitters.csv\")\ndf = hit.copy()\ndf = df.dropna()\ndms = pd.get_dummies(df[['League', 'Division', 'NewLeague']])\ny = df[\"Salary\"]\nX_ = df.drop(['Salary', 'League', 'Division', 'NewLeague'], axis=1).astype('float64')\nX = pd.concat([X_, dms[['League_N', 'Division_W', 'NewLeague_N']]], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.25, \n                                                    random_state=42)","18da6430":"X_train = pd.DataFrame(X_train[\"Hits\"])         #burda sadece tek parametre aliyoruz tum parametreleri alirsak hata degeri \n                                                #duser ama cok uzun surede sonuclanir\nX_test = pd.DataFrame(X_test[\"Hits\"])","203e684d":"cart_model = DecisionTreeRegressor(min_samples_split = 2)","c204b7db":"?cart_model","c2b2399e":"cart_model.fit(X_train, y_train)","034169e6":"X_grid = np.arange(min(np.array(X_train)),max(np.array(X_train)), 0.01) \nX_grid = X_grid.reshape((len(X_grid), 1))  \nplt.scatter(X_train, y_train, color = 'red') \nplt.plot(X_grid, cart_model.predict(X_grid), color = 'blue')  \nplt.title('CART REGRESON A\u011eACI')  \nplt.xlabel('At\u0131\u015f Say\u0131s\u0131(Hits)') \nplt.ylabel('Maa\u015f (Salary)') ;","bb8c4d7c":"!pip install skompiler","841687e3":"from skompiler import skompile","56294ce5":"#print(skompile(cart_model.predict).to('python\/code'))","40313c04":"x = [91]","a6a4970a":"(345.2011551724138 if x[0] <= 117.5 else ((((1300.0 if x[0] <= 118.5 else     #bu kod yukarda print yaptirdigimiz yerden geldi\n    641.0) if x[0] <= 122.5 else 1468.5236666666667) if x[0] <= 125.5 else \n    621.9679230769232) if x[0] <= 143.0 else (958.6111111111111 if x[0] <= \n    150.5 else 2460.0) if x[0] <= 151.5 else 499.1666666666667 if x[0] <= \n    157.5 else 892.5402413793104) if x[0] <= 225.5 else 1975.0)","db3ccafd":"cart_model.predict(X_test)[0:5]","4aef9d57":"cart_model.predict([[91]]) #predict ettigimiz degeri eldde ettik","13c37d57":"y_pred =cart_model.predict(X_test)","af1eb4c2":"np.sqrt(mean_squared_error(y_test, y_pred)) #ilkel test hata hesabini yaptik pred degeri uzerinden","c513be4d":"cart_model = DecisionTreeRegressor() \ncart_model.fit(X_train,y_train)\ny_pred = cart_model.predict(X_test)","6a72da7c":"np.sqrt(mean_squared_error(y_test,y_pred))","ed3d5ddd":"cart_params = {\"min_samples_split\": range(2,100),       #araliklari veriyoruz \n         \"max_leaf_nodes\": range(2,10)}","837fe8e1":"cart_cv_model = GridSearchCV(cart_model,cart_params,cv=10) #GridSearch ile modeli CV olusturuyoruz","5038adbd":"cart_cv_model.fit(X_train,y_train)","01b01eb6":"cart_cv_model.best_params_  #en iyi parametrelerini aldik modelin","382e32f9":"cart_tuned= DecisionTreeRegressor(max_leaf_nodes=9, min_samples_split=76) #tune edilmis modeli olusturuyoruz","c6e4c610":"cart_tuned.fit(X_train,y_train)","4fdf70f2":"y_pred= cart_tuned.predict(X_test)","e418aeae":"np.sqrt(mean_squared_error(y_test,y_pred)) #test hatamiz 423 e geriledi ","c96eff80":"hit = pd.read_csv(\"..\/input\/hitters\/Hitters.csv\")\ndf = hit.copy()\ndf = df.dropna()\ndms = pd.get_dummies(df[['League', 'Division', 'NewLeague']])\ny = df[\"Salary\"]\nX_ = df.drop(['Salary', 'League', 'Division', 'NewLeague'], axis=1).astype('float64')\nX = pd.concat([X_, dms[['League_N', 'Division_W', 'NewLeague_N']]], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.25, \n                                                    random_state=42)\n","5a8da551":"bag_model = BaggingRegressor(bootstrap_features = True)\nbag_model.fit(X_train, y_train)","3135c402":"bag_model.n_estimators","3537d5b9":"bag_model.estimators_","06eb2ac0":"bag_model.estimators_samples_","18f14a57":"bag_model.estimators_features_","4f0b1f7f":"bag_model.estimators_[1]","d46d0133":"y_pred = bag_model.predict(X_test)","b83d362a":"np.sqrt(mean_squared_error(y_test, y_pred))","174cef1a":"iki_y_pred = bag_model.estimators_[1].fit(X_train, y_train).predict(X_test)","be794db9":"np.sqrt(mean_squared_error(y_test, iki_y_pred))","88335997":"yedi_y_pred = bag_model.estimators_[4].fit(X_train, y_train).predict(X_test)","04fc1818":"np.sqrt(mean_squared_error(y_test, yedi_y_pred))","ee7fc5b1":"bag_model = BaggingRegressor(bootstrap_features = True)\nbag_model.fit(X_train, y_train)","beb21f22":"bag_params = {\"n_estimators\": range(2,20)}","49e45936":"bag_cv_model = GridSearchCV(bag_model, bag_params, cv = 10)","3c02b39b":"bag_cv_model.fit(X_train, y_train)","fa5d7cf5":"bag_cv_model.best_params_","815eb9be":"bag_tuned = BaggingRegressor( n_estimators = 14, random_state = 45)","66c3e2e6":"bag_tuned.fit(X_train, y_train)","dbb2204b":"y_pred = bag_tuned.predict(X_test)","337287ba":"np.sqrt(mean_squared_error(y_test, y_pred))","3e61e72b":"hit = pd.read_csv(\"..\/input\/hitters\/Hitters.csv\")\ndf = hit.copy()\ndf = df.dropna()\ndms = pd.get_dummies(df[['League', 'Division', 'NewLeague']])\ny = df[\"Salary\"]\nX_ = df.drop(['Salary', 'League', 'Division', 'NewLeague'], axis=1).astype('float64')\nX = pd.concat([X_, dms[['League_N', 'Division_W', 'NewLeague_N']]], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.25, \n                                                    random_state=42)\n","5d82be84":"from sklearn.ensemble import RandomForestRegressor","03a4b0d0":"rf_model = RandomForestRegressor(random_state = 42)","71727c03":"rf_model.fit(X_train, y_train)","96be66eb":"rf_model.predict(X_test)[0:5]","cf290e66":"y_pred = rf_model.predict(X_test)","df853741":"np.sqrt(mean_squared_error(y_test, y_pred))","1b0a9eaf":"rf_params = {'max_depth': list(range(1,10)),\n            'max_features': [3,5,10,15],\n            'n_estimators' : [100, 200, 500, 1000, 2000]}","b4528d63":"rf_model = RandomForestRegressor(random_state = 42)","9e9e5155":"rf_cv_model = GridSearchCV(rf_model, \n                           rf_params, \n                           cv = 10, \n                            n_jobs = -1) #n_jobs e\u015f zamanl\u0131 yap\u0131lacak i\u015flemleri devreye al\u0131r ve i\u015flemcilerin \n                                         #tam performans \u015fekilde kullan\u0131lmas\u0131n\u0131 sa\u011flar","4315c095":"#rf_cv_model.fit(X_train, y_train)             #burada bilerek yorum sat\u0131r\u0131 yapt\u0131m \u00e7\u00fcnk\u00fc i\u015flemcinin hesaplamas\u0131 cok uzun\n                                              #suruyor bu 2 kisim sonucunda gelen en iyi parametreleri tuned model altinda\n                                              #atadim","fff4afbb":"#rf_cv_model.best_params_","4850fca7":"rf_tuned = RandomForestRegressor(max_depth  = 8, \n                                 max_features = 3, \n                                 n_estimators =200)","a0e27e8e":"rf_tuned.fit(X_train, y_train)","f0d985d5":"y_pred = rf_tuned.predict(X_test)","f05853ec":"np.sqrt(mean_squared_error(y_test, y_pred))","8e8de123":"Importance = pd.DataFrame({\"Importance\": rf_tuned.feature_importances_*100},\n                         index = X_train.columns)","22be1c5a":"Importance.sort_values(by = \"Importance\", \n                       axis = 0, \n                       ascending = True).plot(kind =\"barh\", color = \"r\")\n\nplt.xlabel(\"De\u011fi\u015fken \u00d6nem D\u00fczeyleri\")    #burada degiskenlerin bagimli degiskene etkilerine gore anlamliliklarini\n                                        #gosteriyor bu graph","92eaa923":"from sklearn.ensemble import GradientBoostingRegressor","a424f192":"gbm_model = GradientBoostingRegressor()\ngbm_model.fit(X_train, y_train)","234e6293":"y_pred = gbm_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","2622b5e7":"gbm_params = {\n    'learning_rate': [0.001, 0.01, 0.1, 0.2],\n    'max_depth': [3, 5, 8,50,100],\n    'n_estimators': [200, 500, 1000, 2000],\n    'subsample': [1,0.5,0.75],\n}","c06a21d5":"#gbm = GradientBoostingRegressor()                  #buralari kapattim cunku cok uzun zaman aliyor en iyi parametreleri\n                                                    #bulmak\n#gbm_cv_model = GridSearchCV(gbm, gbm_params, cv = 10, n_jobs = -1, verbose = 2)\n                                                #njobs islemin daha hizli yapilmasi icin islemciyi kullanan bir parametre\n                                                                    #verbose ise iterasyonlari gozlemlemek icin kullanilir\n\n#gbm_cv_model.fit(X_train, y_train)","81e3cacc":"#gbm_cv_model.best_params_           #en iyi parametreler","94d5e4c0":"gbm_tuned = GradientBoostingRegressor(learning_rate = 0.1,  \n                                      max_depth = 5, \n                                      n_estimators = 200, \n                                      subsample = 0.5)\n\ngbm_tuned = gbm_tuned.fit(X_train,y_train)","000f5f75":"y_pred = gbm_tuned.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","42683970":"Importance = pd.DataFrame({\"Importance\": gbm_tuned.feature_importances_*100},\n                         index = X_train.columns)","3eaf2d8f":"Importance.sort_values(by = \"Importance\", \n                       axis = 0, \n                       ascending = True).plot(kind =\"barh\", color = \"r\")\n\nplt.xlabel(\"De\u011fi\u015fken \u00d6nem D\u00fczeyleri\")     #tekrar degisken onem duzeylerini cizdirdik bagimli deger uzerinde","6cc666ff":"hit = pd.read_csv(\"..\/input\/hitters\/Hitters.csv\")\ndf = hit.copy()\ndf = df.dropna()\ndms = pd.get_dummies(df[['League', 'Division', 'NewLeague']])\ny = df[\"Salary\"]\nX_ = df.drop(['Salary', 'League', 'Division', 'NewLeague'], axis=1).astype('float64')\nX = pd.concat([X_, dms[['League_N', 'Division_W', 'NewLeague_N']]], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.25, \n                                                    random_state=42)\n","f0f298f9":"#!pip install xgboost","bf08eb37":"import xgboost as xgb","03db91ad":"DM_train = xgb.DMatrix(data = X_train, label = y_train)\nDM_test = xgb.DMatrix(data = X_test, label = y_test)","285db821":"from xgboost import XGBRegressor","6700b392":"xgb_model = XGBRegressor().fit(X_train, y_train)","adb7e5bf":"y_pred = xgb_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","22c97129":"xgb_model","77a120ea":"xgb_grid = {\n     'colsample_bytree': [0.4, 0.5,0.6,0.9,1], \n     'n_estimators':[100, 200, 500, 1000],\n     'max_depth': [2,3,4,5,6],\n     'learning_rate': [0.1, 0.01, 0.5]\n}\n","c5285cfb":"#xgb = XGBRegressor()                              #Gene uzun zaman aldigi icin parametre bulma islemini kapattim\n\n#xgb_cv = GridSearchCV(xgb, \n#                      param_grid = xgb_grid, \n#                      cv = 10, \n#                      n_jobs = -1,             \n#                      verbose = 2)             \n\n\n#xgb_cv.fit(X_train, y_train)","79100e08":"#xgb_cv.best_params_","88882b8d":"xgb_tuned = XGBRegressor(colsample_bytree = 0.9, \n                         learning_rate = 0.01, \n                         max_depth = 5, \n                         n_estimators = 1000) \n\nxgb_tuned = xgb_tuned.fit(X_train,y_train)","99d7ab68":"y_pred = xgb_tuned.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","0c231941":"hit = pd.read_csv(\"..\/input\/hitters\/Hitters.csv\")\ndf = hit.copy()\ndf = df.dropna()\ndms = pd.get_dummies(df[['League', 'Division', 'NewLeague']])\ny = df[\"Salary\"]\nX_ = df.drop(['Salary', 'League', 'Division', 'NewLeague'], axis=1).astype('float64')\nX = pd.concat([X_, dms[['League_N', 'Division_W', 'NewLeague_N']]], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.25, \n                                                    random_state=42)\n","6c7513cb":"#!pip install lightgbm","f6597684":"from lightgbm import LGBMRegressor","a9cddd25":"#conda install -c conda-forge lightgbm   #pip install ile hata aldigim icin conda ile indirme islemi yaptim","3aa3291a":"from lightgbm import LGBMRegressor","b212a460":"lgbm = LGBMRegressor()\nlgbm_model = lgbm.fit(X_train, y_train)","852856bd":"y_pred = lgbm_model.predict(X_test, \n                            num_iteration = lgbm_model.best_iteration_)","a6b402ff":"np.sqrt(mean_squared_error(y_test, y_pred))","18908784":"lgbm_model","39fc3575":"lgbm_grid = {\n    'colsample_bytree': [0.4, 0.5,0.6,0.9,1],\n    'learning_rate': [0.01, 0.1, 0.5,1],\n    'n_estimators': [20, 40, 100, 200, 500,1000],\n    'max_depth': [1,2,3,4,5,6,7,8] }\n\nlgbm = LGBMRegressor()\nlgbm_cv_model = GridSearchCV(lgbm, lgbm_grid, cv=10, n_jobs = -1, verbose = 2)","373b9084":"#lgbm_cv_model.fit(X_train, y_train)","67413085":"#lgbm_cv_model.best_params_","7334dfb2":"lgbm_tuned = LGBMRegressor(learning_rate = 0.1, \n                           max_depth = 7, \n                           n_estimators = 40,\n                          colsample_bytree = 0.6)\n\nlgbm_tuned = lgbm_tuned.fit(X_train,y_train)","65894683":"y_pred = lgbm_tuned.predict(X_test)","71eb8a62":"np.sqrt(mean_squared_error(y_test, y_pred))","b44dd32d":"hit = pd.read_csv(\"..\/input\/hitters\/Hitters.csv\")\ndf = hit.copy()\ndf = df.dropna()\ndms = pd.get_dummies(df[['League', 'Division', 'NewLeague']])\ny = df[\"Salary\"]\nX_ = df.drop(['Salary', 'League', 'Division', 'NewLeague'], axis=1).astype('float64')\nX = pd.concat([X_, dms[['League_N', 'Division_W', 'NewLeague_N']]], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.25, \n                                                    random_state=42)\n","a8251860":"!pip install catboost","3c3beac7":"from catboost import CatBoostRegressor","a978707f":"catb = CatBoostRegressor()\ncatb_model = catb.fit(X_train, y_train)","c8c79de7":"y_pred = catb_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","8023900c":"catb_grid = {\n    'iterations': [200,500,1000,2000],\n    'learning_rate': [0.01, 0.03, 0.05, 0.1],\n    'depth': [3,4,5,6,7,8] }","2292e429":"catb = CatBoostRegressor()\ncatb_cv_model = GridSearchCV(catb, catb_grid, cv=5, n_jobs = -1, verbose = 2)","39f46e5e":"#catb_cv_model.fit(X_train, y_train)","cd2ec93b":"#catb_cv_model.best_params_","94fdda7a":"catb_tuned = CatBoostRegressor(iterations = 200, \n                               learning_rate = 0.01, \n                               depth = 8)\n\ncatb_tuned = catb_tuned.fit(X_train,y_train)","3c5d3058":"y_pred = catb_tuned.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","4689b33b":"## Tahmin","4182401a":"# SVR (Destek Vekt\u00f6r Regresyonu)","4aab55e4":"## Tahmin","02cad2e5":"## Tahmin","09a322f9":"## Model Tuning","252c5193":"## Model Tuning","41fa05ec":"## Model Tuning","9e08d05e":"# CatBoost","ef47f8ac":"## Tahmin","ca2ee909":"### cok vakit aldigi icin ustteki kismi kapadim best parametreyi bulmak suruyor","b75e2339":"## Model Tuning","b3dfcd62":"## Model Tuning","4b32c7d9":"# Bagged Trees Regresyon","52381a57":"### tune edilmis final modeli kuruyoruz","421e6325":"## Model Tuning","0ff16fdd":"# KNN","0c1b7e6f":"# Random Forests","5d8de678":"NOT: ilk RMSE degerleri valide edilmemis ikinci RMSE degerleri ise valide edilmis model uzerinden elde edilen hatalar (Hepsi train hatalari) \n\nOnemli nokta 2. RMSE degerleri 293 ile 300 arasinda degismis bunlar dogrulanmis, burada kucuk olani aramiyoruz kucuk olan aldatici (ornek valide en dusuk RMSE degeri CV siz 2 idi aslinda direkt train seti uzerinden ilkelce yaptigimizda cikan en dusuk deger 179 iken model Tuning ile cross validationu ortaya koyup bir arama yaptigimizda buldugumuz deger 8 oldu. ) ilki 179 dan 271 e kadar degismis cok yuksek bir standart sapma 2. de ise 293-301 arasinda bir degisim var. Kurulacak olan ve degerlendirecegimiz test hatasini hep valide edilmis model uzerinden degerlendirmemiz gerek.","03295c3e":"# Light GBM","26c5db02":"## Tahmin","283c2b23":"## Tahmin","749eccc4":"# Gradient Boosting Machines","8541d003":"## Model Tuning","7caf1a76":"## Tahmin","1a39fd08":"## Tahmin","cf7581ef":"# Cok Katmanli Algilayici","3b7fde22":"# CART","7495e3c7":"# Do\u011frusal Olmayan Regreson Modelleri","237f896f":"### Final Modeli","03c82759":"## Tahmin","3223433a":"## Tahmin","6957a76e":"## Tahmin","8d8b5ab9":"## Model Tuning","4b128d4b":"## Model Tuning","26baade4":"# Dorusal Olmayan SVR","0af161c8":"# Model Tuning","cb8db5fc":"### final modeli olusturuyorum","014b5d7d":"# Buradan itibaren sadece kodlari yazdim not almadim her algoritmanin tuning islemi ve parametreleri farkli ama genelde yapilan islem ayni oldugu icin sadece kodlarin ve algoritmalarin uygulanisini yaptim","a4ac5875":"# XGBoost","660fea5b":"## Model Tuning"}}