{"cell_type":{"fc9c6368":"code","fa23e049":"code","9cf79e55":"code","7dfc6709":"code","56c6ba32":"code","d8d2aa04":"code","a7445e22":"code","d1aa2e84":"code","c5adca68":"code","c346b7b4":"code","dab391a0":"code","92b4753c":"code","787234bd":"code","4068ee0a":"code","a01ba17f":"code","f9f42191":"code","b3fc7613":"code","2c27307d":"code","c3dc893c":"code","9f81cdd1":"code","cc6695c8":"markdown","62079c4d":"markdown","87a8b9aa":"markdown","a2eb29ff":"markdown","2dae8fbc":"markdown","f6932446":"markdown"},"source":{"fc9c6368":"import numpy as np \nimport pandas as pd \n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","fa23e049":"import tensorflow as tf\nfrom tensorflow import keras","9cf79e55":"from keras.preprocessing.image import load_img, array_to_img, img_to_array\nfrom keras.models import Model\nfrom keras.layers import Conv2D, LeakyReLU, BatchNormalization, MaxPool2D,Conv2DTranspose, concatenate,Input","7dfc6709":"# Paths to our images and masks folders\nimgs_dir = '\/kaggle\/input\/segmentation-full-body-mads-dataset\/segmentation_full_body_mads_dataset_1192_img\/images'\nmasks_dir = '\/kaggle\/input\/segmentation-full-body-mads-dataset\/segmentation_full_body_mads_dataset_1192_img\/masks'","56c6ba32":"imgs_arr = np.zeros((len(os.listdir(imgs_dir)), 256, 256, 3), dtype = np.float32)\nmasks_arr = np.zeros((len(os.listdir(masks_dir)), 256, 256, 1), dtype = np.float32)\n\n\n\nfor index in range(len(os.listdir(imgs_dir))):\n    img = load_img(os.path.join(imgs_dir, os.listdir(imgs_dir)[index]), target_size = (256, 256, 3))\n    imgs_arr[index] = img_to_array(img)\n    \n\n\nfor index in range(len(os.listdir(masks_dir))):\n    img = load_img(os.path.join(masks_dir, os.listdir(masks_dir)[index]), target_size = (256, 256, 1), color_mode=\"grayscale\" )\n    masks_arr[index] = img_to_array(img)\n\n\nimgs_arr \/= 255.\nmasks_arr \/= 255.\n\nx_train = imgs_arr[10:]\ny_train = masks_arr[10:]\n\nx_test = imgs_arr[0:10]\ny_test = masks_arr[0:10]","d8d2aa04":"# Plotting image and its mask \nimport matplotlib.pyplot as plt\nf = plt.figure(figsize=(10,10))\nf.add_subplot(1,2, 1)\nplt.imshow(imgs_arr[0])\nf.add_subplot(1,2, 2)\nplt.imshow(masks_arr[0])\nplt.show(block=True)","a7445e22":"# Convolutional blocks\ndef conv_block(inputs,n_filters,max_pool=True):\n    x = Conv2D(n_filters,3,padding='same',kernel_initializer='he_normal',use_bias=False)(inputs)\n    x = BatchNormalization()(x)\n    x = LeakyReLU(0.2)(x)\n    x = Conv2D(n_filters,3,padding='same',kernel_initializer='he_normal',use_bias=False)(x)\n    x = BatchNormalization()(x)\n    x = LeakyReLU(0.2)(x)\n    skip = x\n    if max_pool:\n        next_layer = MaxPool2D()(x)\n    else:\n        next_layer = x\n    return next_layer, skip\n\n# upsampling block \ndef up_block(reg_inputs,skip_inputs,n_filters):\n    x = Conv2DTranspose(n_filters,3,2,padding='same')(reg_inputs)\n    x = concatenate([x,skip_inputs],axis=3)\n    x = conv_block(x,n_filters,max_pool=False)[0]\n    return x\n    ","d1aa2e84":"# Unet model\ndef unet(input_size=(256,256,3),number_of_classes=1):\n    inputs = Input(shape=input_size)\n    cb1 = conv_block(inputs,32)\n    cb2 = conv_block(cb1[0],64)\n    cb3 = conv_block(cb2[0],128)\n    cb4 = conv_block(cb3[0],256)\n    cb5 = conv_block(cb4[0],512,max_pool=False)\n    \n    up1 = up_block(cb5[0],cb4[1],256)\n    up2 = up_block(up1,cb3[1],128)\n    up3 = up_block(up2,cb2[1],64)\n    up4 = up_block(up3,cb1[1],32)\n    \n    conv1 = Conv2D(32,3,padding='same',kernel_initializer='he_normal',use_bias=False)(up4)\n    bn = BatchNormalization()(conv1)\n    lrl = LeakyReLU(0.2)(bn)\n    outputs = Conv2D(number_of_classes,1,padding='same',activation='sigmoid')(lrl)\n    \n    unet = Model(inputs=inputs,outputs=outputs)\n    return unet","c5adca68":"from keras import backend as K\n# Jaccard coeficient or IoU\ndef jaccard_coef(y_true,y_pred,smooth=1):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f*y_pred_f)\n    return (intersection+smooth)\/(K.sum(y_true_f)+K.sum(y_pred_f)-intersection+smooth)\n# Jaccard loss\ndef jaccard_loss(y_true,y_pred,smooth=1):\n    # We are multiplying IoU with -1 because we are trying to maximize the IoU so it will get bigger every iteration\n    return -jaccard_coef(y_true,y_pred,smooth)","c346b7b4":"# Defining our model\nmodel = unet()\n\n# Plotting our model\nkeras.utils.plot_model(\n    model,\n    to_file=\"model.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    dpi=60\n)","dab391a0":"model.compile(optimizer=keras.optimizers.Adam(1e-3),loss=[jaccard_loss],metrics=[jaccard_coef])","92b4753c":"history = model.fit(x_train,y_train,batch_size=32,epochs=20,validation_split=0.2)","787234bd":"# Plotting loss change over epochs\nx = [i for i in range(20)]\nplt.plot(x,history.history['loss'])\nplt.title('change in loss over epochs')\nplt.legend(['jaccard_loss'])\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.show()","4068ee0a":"# Plotting IoU change over epochs, this will be same as loss just inverted so you can skip this part!\nx = [i for i in range(20)]\nplt.plot(x,history.history['jaccard_coef'])\nplt.title('change in jaccard coefitient over epochs')\nplt.legend(['jaccard coefitient'])\nplt.xlabel('epochs')\nplt.ylabel('jaccard coefitient')\nplt.show()","a01ba17f":"# Creating predictions on our test set\npredictions = model.predict(x_test)","f9f42191":"# Because we used sigmoid as the activation of our last conv layer, value of each pixel is probability that it belongs to a human body.\n#  We will use 50% probabilty as the treshold that represents is the pixel a part of a human body or not\ndef create_mask(predictions,input_shape=(256,256,1)):\n    mask = np.zeros(input_shape)\n    mask[predictions>0.5] = 1\n    return mask","b3fc7613":"# Ploting results for one image\ndef plot_results_for_one_sample(sample_index):\n    mask = create_mask(predictions[sample_index])\n    f = plt.figure(figsize=(20,20))\n    f.add_subplot(1,4,1)\n    plt.title('Input image')\n    plt.imshow(x_test[sample_index])\n    f.add_subplot(1,4,2)\n    plt.title('Real mask')\n    plt.imshow(y_test[sample_index])\n    f.add_subplot(1,4,3)\n    plt.title('Predicted mask')\n    plt.imshow(mask)\n    f.add_subplot(1,4,4)\n    plt.title(\"Mask + image\")\n    plt.imshow(x_test[sample_index]*mask)","2c27307d":"plot_results_for_one_sample(0)","c3dc893c":"plot_results_for_one_sample(1)","9f81cdd1":"plot_results_for_one_sample(2)","cc6695c8":"# Loading our data\n\nWe will have only 10 test images which is enough, one thing we can add here is data augmentation.","62079c4d":"# I hope you like this notebook! If you have any questions about it, feel free to ask!","87a8b9aa":"# Importing libraries that we will use","a2eb29ff":"<h1 style=\"text-align:center;background-color:#0e4b90;color:white\">UNET for body segmentation<\/h1><br>\n\n<p style=\"text-align:center\">In this notebook we will implement UNET (from scratch) for body segmentation task. We will use tensorflow 2.x as library of choice<\/p>","2dae8fbc":"<h1 style=\"text-align:center;background-color:#0e4b90;color:white\">UNET model<\/h1><br>\n\n<p style=\"text-align:center\">Before we talk about UNET, lets understand what is the task of this model.<br>\nUnlike classic classification where we predict a class for a whole image, in semantic segmentation we want to predict a class for every single pixel <br>\n(in this case is it a human or just background, which is binary classification). This technique is also used for autonomous driving task where our algorithm <br>\nclassifies what pixels are cars, people, trafic lights, trafic signs, etc. <br>\nSo how do we build this model? One of the models that is proven to be good is UNET<\/p>\n<div style=\"text-align:center\"><img src=\"https:\/\/camo.githubusercontent.com\/d55a437337d0e08c6a082714959253d80b81ce4e6c18e94688d9aff16e3bf2f8\/68747470733a2f2f6c6d622e696e666f726d6174696b2e756e692d66726569627572672e64652f70656f706c652f726f6e6e656265722f752d6e65742f752d6e65742d6172636869746563747572652e706e67\"\/ style=\"width:600px;height:400px;\"><\/div><br><br>\n<p style=\"text-align:center\">Unet has two parts in its arhitecture. First part is the contracting part (encoder), and the second is the expanding part(decoder)<br>\nIn the encoder we have a standard cnn like arhitecture with two Conv2D layers followed by a MaxPool (except in the bottleneck layer (5th cnn layer))<br>\nIn the decoder we have a transposed convolution with stride of 2 and same padding which will double the dimensions of our input image.<br>\nAfter that transposed convolution, we will concatenate its output with the output of coresponding convolutional layer in the encoder.<br>\nWe did this concatenation so that our decoder knows about feature representations that our convolutional layers produce, which will help us to classify pixles of the image more easily.<br><br>\nSo lets build it! (keep in mind that this implementation is not the same as the original paper. I have added batch normalization and leaky relu and i have used same padding instead of valid).<\/p>\n","f6932446":"<h1 style=\"text-align:center;background-color:#0e4b90;color:white\">Custom loss<\/h1><br>\n<p style=\"text-align:center\">In segmentation, using accuracy for a model evaluation metric is not a good idea. Why?<br>\nWell lets take a hypothetical case where we are trying to segment really small objects on an image and lets say that they take about 3% of pixels in the whole image.<br>\nIf we predict that the image does not have any of this small objects (a mask full of 0s) the accuracy of our model will be 97% which is not a good representation of the true performance of our model.<br>But there is another metric that help us with this problem and it is called IoU (Intersection over Union).<\/p><br><br>\n<div style=\"text-align:center\"><img src=\"https:\/\/miro.medium.com\/max\/1838\/0*nm8gO9HZoLU7IES2\" style=\"width:600px;height:400px;\"><\/div><br><br>\n<p style=\"text-align:center\">IoU,  also known as the Jaccard similarity coefficient, is a statistic used for gauging the similarity and diversity of sample sets.<br>\nThe greater the intersection, the more precise the output is. We can look at intersection as a true positive (we have predicted the class of the pixel that it truly is),<br>\nand the non overlaping parts as false negatives and false positives<br>\nFrom this we can build a better metric for evaluating our model, and also create a custom loss that will try to maximize our IoU. This loss is known as Jaccard loss.<\/p>"}}