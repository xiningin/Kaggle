{"cell_type":{"105f9620":"code","7eda6ba0":"code","6db56fd7":"code","06ffa9a3":"code","4a4c03ad":"code","90991229":"code","e557d21a":"code","80b9ca13":"code","3c6328e3":"code","0dcfb13e":"code","3bf1a140":"code","71b5e412":"markdown","26fce878":"markdown","2a0f55f3":"markdown","7a171713":"markdown","d62ece70":"markdown","5fbaae0e":"markdown","df92ac93":"markdown"},"source":{"105f9620":"import pydot\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nsns.set()\n\n%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport sklearn\nfrom sklearn import metrics\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import export_graphviz, DecisionTreeClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.exceptions import NotFittedError\n\n# import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n\nfrom IPython.display import display","7eda6ba0":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")","6db56fd7":"def draw_missing_data_table(df):\n    total = df.isnull().sum().sort_values(ascending=False)\n    percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    return missing_data\n\ndraw_missing_data_table(train)","06ffa9a3":"def preprocess_data(df):\n    \n    processed_df = df\n        \n    ########## Deal with missing values ##########\n    \n    # As we saw before, the two missing values for embarked columns can be replaced by 'C' (Cherbourg)\n    processed_df['Embarked'].fillna('C', inplace=True)\n    \n    # We replace missing ages by the mean age of passengers who belong to the same group of class\/sex\/family\n    processed_df['Age'] = processed_df.groupby(['Pclass','Sex','Parch','SibSp'])['Age'].transform(lambda x: x.fillna(x.mean()))\n    processed_df['Age'] = processed_df.groupby(['Pclass','Sex','Parch'])['Age'].transform(lambda x: x.fillna(x.mean()))\n    processed_df['Age'] = processed_df.groupby(['Pclass','Sex'])['Age'].transform(lambda x: x.fillna(x.mean()))\n    \n    # We replace the only missing fare value for test dataset and the missing values of the cabin column\n    processed_df['Fare'] = processed_df['Fare'].interpolate()\n    processed_df['Cabin'].fillna('U', inplace=True)\n    \n    ########## Feature engineering on columns ##########\n    \n    # Create a Title column from name column\n    processed_df['Title'] = pd.Series((name.split('.')[0].split(',')[1].strip() for name in train['Name']), index=train.index)\n    processed_df['Title'] = processed_df['Title'].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    processed_df['Title'] = processed_df['Title'].replace(['Mlle', 'Ms'], 'Miss')\n    processed_df['Title'] = processed_df['Title'].replace('Mme', 'Mrs')\n    processed_df['Title'] = processed_df['Title'].map({\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5})\n    \n    # Filling Age missing values with mean age of passengers who have the same title\n    processed_df['Age'] = processed_df.groupby(['Title'])['Age'].transform(lambda x: x.fillna(x.mean()))\n\n    # Transform categorical variables to numeric variables\n    processed_df['Sex'] = processed_df['Sex'].map({'male': 0, 'female': 1})\n    processed_df['Embarked'] = processed_df['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n    \n    # Create a Family Size, Is Alone, Child and Mother columns\n    processed_df['FamillySize'] = processed_df['SibSp'] + processed_df['Parch'] + 1\n    processed_df['FamillySize'][processed_df['FamillySize'].between(1, 5, inclusive=False)] = 2\n    processed_df['FamillySize'][processed_df['FamillySize']>5] = 3\n    processed_df['IsAlone'] = np.where(processed_df['FamillySize']!=1, 0, 1)\n    processed_df['IsChild'] = processed_df['Age'] < 18\n    processed_df['IsChild'] = processed_df['IsChild'].astype(int)\n    \n    # Modification of cabin column to keep only the letter contained corresponding to the deck of the boat\n    processed_df['Cabin'] = processed_df['Cabin'].str[:1]\n    processed_df['Cabin'] = processed_df['Cabin'].map({cabin: p for p, cabin in enumerate(set(cab for cab in processed_df['Cabin']))})\n    \n    # Create a ticket survivor column which is set to 1 if an other passenger with the same ticket survived and 0 else\n    # Note : this implementation is ugly and unefficient, if sombody found a way to do it easily with pandas (it must be a way), please comment the kernel with your solution !\n    processed_df['TicketSurvivor'] = pd.Series(0, index=processed_df.index)\n    tickets = processed_df['Ticket'].value_counts().to_dict()\n    for t, occ in tickets.items():\n        if occ != 1:\n            table = train['Survived'][train['Ticket'] == t]\n            if sum(table) != 0:\n                processed_df['TicketSurvivor'][processed_df['Ticket'] == t] = 1\n    \n    # These two columns are not useful anymore\n    processed_df = processed_df.drop(['Name', 'Ticket', 'PassengerId'], 1)    \n    \n    return processed_df","4a4c03ad":"# Let's divide the train dataset in two datasets to evaluate perfomance of the machine learning models we'll use\ntrain_df = train.copy()\nX = train_df.drop(['Survived'], 1)\nY = train_df['Survived']\n\nX = preprocess_data(X)\n# We scale our data, it is essential for a smooth working of the models. Scaling means that each columns as a 0 mean and a 1 variance\nsc = StandardScaler()\nX = pd.DataFrame(sc.fit_transform(X.values), index=X.index, columns=X.columns)\n    \n# Split dataset for model testing\nX_train, X_valid, y_train, y_valid = train_test_split(X, Y, test_size=0.2, random_state=42)\n\nX_train.head()","90991229":"def build_ann(optimizer='adam'):\n    \n    # Initializing our ANN\n    ann = Sequential()\n    \n    # Adding the input layer and the first hidden layer of our ANN with dropout\n    ann.add(Dense(units=32, kernel_initializer='glorot_uniform', activation='relu', input_shape=(13,)))\n    \n    # Add other layers, it is not necessary to pass the shape because there is a layer before\n    ann.add(Dense(units=64, kernel_initializer='glorot_uniform', activation='relu'))\n    ann.add(Dropout(rate=0.5))\n    ann.add(Dense(units=64, kernel_initializer='glorot_uniform', activation='relu'))\n    ann.add(Dropout(rate=0.5))\n    \n    # Adding the output layer\n    ann.add(Dense(units=1, kernel_initializer='glorot_uniform', activation='sigmoid'))\n    \n    # Compiling the ANN\n    ann.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return ann","e557d21a":"opt = optimizers.Adam(lr=0.001)\nann = build_ann(opt)\n# Training the ANN\nhistory = ann.fit(X_train, y_train, batch_size=16, epochs=30, validation_data=(X_valid, y_valid))","80b9ca13":"test = preprocess_data(test)\ntest = pd.DataFrame(sc.fit_transform(test.values), index=test.index, columns=test.columns)\ntest.head()","3c6328e3":"# Predicting the Test set results\nann_prediction = ann.predict(test)\nann_prediction = (ann_prediction > 0.5) # convert probabilities to binary output\nn = len(ann_prediction)\npred_test = [1 if ann_prediction[i]>0.5 else 0 for i in range(n)]\nlen(pred_test)","0dcfb13e":"result_df = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")\nresult_df['Survived'] = pred_test\nresult_df.to_csv('submission2.csv', columns=['PassengerId', 'Survived'], index=False)","3bf1a140":"result_df.head()","71b5e412":"# refine and split train data","26fce878":"# Load Data","2a0f55f3":"# Preprocess test data","7a171713":"# Preprocessing (train ,test)","d62ece70":"# Check Missing value","5fbaae0e":"# Make ANN Modeling","df92ac93":"# Predict Test Data and Submit"}}