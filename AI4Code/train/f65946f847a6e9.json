{"cell_type":{"7d779fbb":"code","80c2ad12":"code","4c417e51":"code","94aae539":"code","f0207b0c":"code","4b5a5f54":"code","3b0bcd77":"code","19cfce42":"code","9cd5c136":"code","8cc04425":"code","1402129c":"code","d604a38a":"code","29da50f0":"code","09ef9746":"code","bc70a323":"code","8b0ad4d3":"code","6a943e56":"code","47535b0d":"code","5e005e13":"code","7557341e":"code","bded31fb":"code","516262d5":"code","96388834":"code","b7517bc6":"code","04815eac":"code","6149048a":"code","0ca4a691":"code","a86e84b5":"code","709126df":"code","848e4711":"code","fe821cb9":"code","76b1f04c":"code","1ea969cb":"code","5e3079da":"code","7119b179":"code","b77cbe17":"code","6150e902":"code","76711bd0":"code","74e780c8":"code","29b9a8e4":"code","b1805e78":"code","ba574f52":"code","8febd5b5":"code","83d8d264":"markdown","1796c57f":"markdown","9b222e72":"markdown","566c9902":"markdown","89284a39":"markdown","0ecc700c":"markdown","42faf15d":"markdown","e2b65155":"markdown","6cd7d446":"markdown","36640977":"markdown","87067158":"markdown","d9f59f4b":"markdown","97ea890d":"markdown","8a3e628d":"markdown","fcb7a028":"markdown"},"source":{"7d779fbb":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (8, 8)\nplt.rcParams[\"figure.dpi\"] = 150\nplt.rcParams[\"font.size\"] = 14\nplt.rcParams['font.family'] = ['sans-serif']\nplt.rcParams['font.sans-serif'] = ['DejaVu Sans']\nplt.style.use('ggplot')\nsns.set_style(\"whitegrid\", {'axes.grid': False})","80c2ad12":"import pandas as pd\nimport numpy as np\nfrom skimage.color import label2rgb, gray2rgb\nfrom skimage.transform import resize\nfrom skimage.io import imread\nfrom scipy.ndimage import distance_transform_edt\nfrom pathlib import Path\nfrom tqdm import tqdm\ntqdm.pandas()","4c417e51":"img_dir = Path('..') \/ 'input' \/ 'pulmonary-chest-xray-abnormalities'\nimg_df = pd.DataFrame(dict(path = list(img_dir.glob('**\/*.*'))))\nimg_df['img_id'] = img_df['path'].map(lambda x: x.stem)\nimg_df['folder'] = img_df['path'].map(lambda x: x.parent.stem)\nimg_df['group'] = img_df['path'].map(lambda x: x.parent.parent.stem)\nimg_df = img_df[img_df['path'].map(lambda x: x.suffix[1:].lower() in {'png', 'jpg'})]\nimg_df","94aae539":"IMG_SIZE = (512, 512)\ndef read_seg_map(in_row):\n    \"\"\"Read segmentation maps as images\"\"\"\n    right_img = imread(in_row['rightMask'], as_gray=True)\n    left_img = imread(in_row['leftMask'], as_gray=True)\n    comb_img = np.clip(255.0*(right_img+left_img), 0, 255).astype('uint8')\n    rs_img = resize(comb_img, IMG_SIZE)\n    dm_img = distance_transform_edt(rs_img)\n    dm_img = 255.0*dm_img\/dm_img.max()\n    return np.expand_dims(dm_img.clip(0, 255).astype('uint8'), -1)\ncolorize = lambda x: (gray2rgb(x)*255).clip(0, 255).astype('uint8')[:, :, :3]","f0207b0c":"img_pairs_df = img_df.\\\n    pivot_table(\n        columns='folder', \n        index='img_id', \n        values='path', \n        aggfunc='first').\\\n    dropna().\\\n    sample(50, random_state=2019)\nimg_pairs_df['segmap'] = img_pairs_df.progress_apply(read_seg_map, axis=1)\nimg_pairs_df['rgb_image'] = img_pairs_df['CXR_png'].progress_map(lambda x: \n                                                                 colorize(\n                                                                     resize(\n                                                                         imread(x, as_gray=True),\n                                                                         IMG_SIZE\n                                                                     ) \n                                                                 )\n                                                                )\nprint(img_pairs_df.shape[0])\nimg_pairs_df.sample(1)","4b5a5f54":"def show_row(n_axs, c_row, channel_wise=True):\n    (ax1, ax2, ax3) = n_axs\n    ax1.imshow(c_row['rgb_image'].squeeze())\n    ax1.axis('off')\n    \n    segmap = c_row['segmap']\n    col_map = (segmap[:, :, 0]).astype('int')\n    ax2.imshow(segmap[:, :, 0])\n    ax2.axis('off')\n    \n    ax3.imshow(label2rgb(image=c_row['rgb_image'].squeeze(), label=col_map.astype('int'), bg_label=0))\n    ax3.axis('off')\n    \nfig, m_axs = plt.subplots(3, min(len(img_pairs_df), 3), figsize=(15, 5))\n\nfor (c_idx, c_row), n_axs in zip(img_pairs_df.iterrows(), m_axs.T):\n    show_row(n_axs, c_row)","3b0bcd77":"from sklearn.model_selection import train_test_split\ntrain_df, valid_df = train_test_split(img_pairs_df, test_size=0.2, random_state=2019)\ntrain_df = train_df.copy()\nvalid_df = valid_df.copy() # make the datasets independent of the input","19cfce42":"from keras import layers, models, losses\nfrom keras.utils.vis_utils import model_to_dot\nfrom IPython.display import SVG\nimport keras.backend as K\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    A simple DICE implementation without any weighting\n    \"\"\"\n    y_t = K.batch_flatten(y_true)\n    y_p = K.batch_flatten(y_pred)\n    return 2.0 * K.sum(y_t * y_p) \/ (K.sum(y_t) + K.sum(y_p) + K.epsilon())\n\ndef dice_loss(y_true, y_pred):\n    \"\"\"\n    A simple inverted dice to use as a loss function\n    \"\"\"\n    return 1 - dice_score(y_true, y_pred)\ndice_loss = losses.mean_squared_error","9cd5c136":"from albumentations import (\n    PadIfNeeded,\n    HorizontalFlip,\n    VerticalFlip,    \n    CenterCrop,    \n    Crop,\n    Compose,\n    Transpose,\n    RandomRotate90,\n    ElasticTransform,\n    GridDistortion, \n    OpticalDistortion,\n    RandomSizedCrop,\n    OneOf,\n    CLAHE,\n    RandomBrightnessContrast,    \n    RandomGamma,\n    HueSaturationValue,\n    MedianBlur, \n    MotionBlur,\n    Blur,\n    RandomFog,\n    Rotate\n)","8cc04425":"print(img_pairs_df['segmap'].iloc[0].max())\ntile_size = (128, 128)\naug = Compose([\n    Rotate(limit=15, p=0.5),\n    OneOf([\n        ElasticTransform(p=0.5, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n        GridDistortion(p=0.5),\n        OpticalDistortion(p=1, distort_limit=2, shift_limit=0.5)                  \n        ], p=0.8),\n    RandomFog(p=0.1),\n    CLAHE(p=0.1),\n    OneOf([\n        RandomBrightnessContrast(p=0.5),    \n        RandomGamma(p=0.5),\n        HueSaturationValue(hue_shift_limit=10, sat_shift_limit=10, val_shift_limit=0, p=0.5),\n        HueSaturationValue(hue_shift_limit=30, sat_shift_limit=10, val_shift_limit=0, p=0.25),\n    ]),\n    Blur(p=0.2),\n    RandomSizedCrop(min_max_height=(48, 256), width=tile_size[0], height=tile_size[1])\n])","1402129c":"aug_df = pd.concat([\n    train_df.sample(n=16, replace=True, random_state=i).\\\n        apply(\n            lambda x: pd.Series(\n                aug(image=x['rgb_image'], \n                    mask=(x['segmap']).astype('uint8')\n                   )\n            ), 1) \n    for i in tqdm(range(16))], ignore_index=True).\\\n    rename(columns={'image': 'rgb_image', 'mask': 'segmap'})","d604a38a":"sample_aug_df = aug_df.sample(12)\nfig, m_axs = plt.subplots(3, len(sample_aug_df), figsize=(15, 5))\nfor (c_idx, c_row), n_axs in zip(sample_aug_df.iterrows(), m_axs.T):\n    show_row(n_axs, c_row)","29da50f0":"X_train = np.stack(aug_df['rgb_image'], 0)\ny_train = np.stack(aug_df['segmap'], 0)\/255.0\nprint(X_train.shape, y_train.shape)","09ef9746":"X_valid = np.stack(valid_df['rgb_image'], 0)\ny_valid = np.stack(valid_df['segmap'], 0)\/255.0\nprint(X_valid.shape, y_valid.shape)","bc70a323":"from IPython.display import clear_output\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\ndef get_callbacks(in_model):\n    weight_path=\"{}_weights.best.hdf5\".format(in_model.name)\n\n    checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n                                 save_best_only=True, mode='min', save_weights_only = True)\n\n    reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=3, verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=0.0001)\n    early = EarlyStopping(monitor=\"val_loss\", \n                          mode=\"min\", \n                          patience=15) # probably needs to be more patient, but kaggle time is limited\n    return [checkpoint, early, reduceLROnPlat], weight_path\n\ndef fit_model(in_model, epochs=50, batch_size=16, loss_func='binary_crossentropy'):\n    in_model.compile(loss=loss_func, metrics=['binary_accuracy', dice_score, 'mae'], optimizer='adam')\n    callback_list, weight_path = get_callbacks(in_model)\n    out_results = in_model.fit(X_train, y_train, \n                                   epochs=epochs,\n                                   batch_size=batch_size,\n                                   validation_data=(X_valid, y_valid), \n                                   callbacks=callback_list)\n    in_model.load_weights(weight_path)\n    in_model.save(weight_path.replace('_weights', '_model'))\n    clear_output()\n    v_keys = [k for k in out_results.history.keys() if 'val_{}'.format(k) in out_results.history.keys()]\n    fig, m_axs = plt.subplots(1, len(v_keys), figsize=(16, 4))\n    for c_key, c_ax in zip(v_keys, m_axs):\n        c_ax.plot(out_results.history[c_key], 'r-', label='Training')\n        val_vec = out_results.history['val_{}'.format(c_key)]\n        c_ax.plot(val_vec, 'b-', label='Validation (Best: {:2.2%})'.format(np.nanmin(val_vec) if 'loss' in c_key else np.nanmax(val_vec)))\n        c_ax.set_title(c_key)\n        c_ax.legend()\n    fig.savefig(weight_path.replace('.hdf5', '.png'))\n    return out_results","8b0ad4d3":"def show_training(in_model):\n    sample_aug_df['predictions'] = [x for x in in_model.predict(np.stack(sample_aug_df['rgb_image'], 0))]\n    fig, m_axs = plt.subplots(5, len(sample_aug_df), figsize=(15, 10))\n    m_axs[1, 0].set_title('Ground-Truth')\n    m_axs[3, 0].set_title('Prediction')\n    for (c_idx, c_row), (ax1, ax2, ax3, ax4, ax5) in zip(sample_aug_df.iterrows(), \n                                     m_axs.T):\n        show_row((ax1, ax2, ax3), c_row)\n        show_row((ax1, ax4, ax5), {'rgb_image': c_row['rgb_image'], 'segmap': c_row['predictions']})\ndef show_validation(in_model):\n    valid_df['predictions'] = [x for x in in_model.predict(np.stack(valid_df['rgb_image'], 0))]\n    fig, m_axs = plt.subplots(5, len(valid_df), figsize=(8, 12))\n    m_axs[1, 0].set_title('Ground-Truth')\n    m_axs[3, 0].set_title('Prediction')\n    for (c_idx, c_row), (ax1, ax2, ax3, ax4, ax5) in zip(valid_df.iterrows(), \n                                     m_axs.T):\n        show_row((ax1, ax2, ax3), c_row)\n        show_row((ax1, ax4, ax5), {'rgb_image': c_row['rgb_image'], 'segmap': c_row['predictions']})\n        ","6a943e56":"from keras.models import Model\nfrom keras.layers import BatchNormalization, Conv2D, Conv2DTranspose, MaxPooling2D, Dropout, UpSampling2D, Input, concatenate\nfrom keras import layers\n\ndef conv2d_block(\n    inputs, \n    use_batch_norm=True, \n    dropout=0.3, \n    filters=16, \n    kernel_size=(3,3), \n    activation='leakyrelu', \n    kernel_initializer='he_normal', \n    double_layer=True,\n    padding='same'):\n    c = inputs\n    for _ in range(2 if double_layer else 1):\n        c = Conv2D(filters, kernel_size, activation='linear', kernel_initializer=kernel_initializer, padding=padding, use_bias=not use_batch_norm) (inputs)\n        if use_batch_norm:\n            c = BatchNormalization()(c)\n        if dropout > 0.0:\n            c = Dropout(dropout)(c)\n        if activation.lower().startswith('leaky'):\n            c = layers.LeakyReLU(0.1)(c)\n        else:\n            c = layers.Activation(activation)(c)\n    return c\n\ndef basic_unet(\n    input_shape,\n    num_classes=1,\n    dropout=0.0, \n    filters=64,\n    num_layers=4,\n    use_deconv=False,\n    crop_output=True,\n    output_name='OutMask',\n    output_activation='sigmoid'): # 'sigmoid' or 'softmax'\n    \"\"\"taken from https:\/\/github.com\/karolzak\/keras-unet\"\"\"\n    # Build U-Net model\n    inputs = Input(input_shape)\n    x = BatchNormalization()(inputs)   \n\n    down_layers = []\n    for l in range(num_layers):\n        x = conv2d_block(inputs=x, filters=filters, use_batch_norm=False, dropout=0.0, padding='same')\n        down_layers.append(x)\n        x = MaxPooling2D((2, 2), strides=2) (x)\n        filters = filters*2 # double the number of filters with each layer\n\n    x = Dropout(dropout)(x)\n    x = conv2d_block(inputs=x, filters=filters, use_batch_norm=False, dropout=0.0, padding='same')\n\n    for conv in reversed(down_layers):\n        filters \/\/= 2 # decreasing number of filters with each layer \n        if use_deconv:\n            x = Conv2DTranspose(filters, (2, 2), strides=(2, 2), padding='same') (x)\n        else:\n            x = UpSampling2D((2, 2))(x)\n        \n        x = concatenate([x, conv])\n        x = conv2d_block(inputs=x, filters=filters, use_batch_norm=False, dropout=0.0, padding='same')\n    \n    outputs = Conv2D(num_classes, (1, 1), \n                     activation=output_activation, \n                     name=output_name if not crop_output else 'Pre_{}'.format(output_name)) (x)    \n    if crop_output:\n        lay_17_crop = layers.Cropping2D((2**num_layers, 2**num_layers))(outputs)\n        outputs = layers.ZeroPadding2D((2**num_layers, 2**num_layers), name=output_name)(lay_17_crop)\n\n    model = Model(inputs=[inputs], outputs=[outputs], name='VanillaUNET')\n    return model","47535b0d":"simple_unet = basic_unet((None, None, 3), num_classes=1, num_layers=4, dropout=0.1, filters=32)","5e005e13":"dot_mod = model_to_dot(simple_unet, show_shapes=True, show_layer_names=False)\ndot_mod.set_rankdir('UD')\nSVG(dot_mod.create_svg())","7557341e":"fit_model(simple_unet, epochs=10, loss_func='binary_crossentropy', batch_size=16)","bded31fb":"plt.imshow(simple_unet.predict(np.random.uniform(0, 255, size=(1, 128, 128, 3)))[0, :, :, 0])","516262d5":"show_training(simple_unet)","96388834":"show_validation(simple_unet)","b7517bc6":"pre_unet = basic_unet((None, None, 3), num_classes=1, num_layers=2, filters=8)\npre_unet.name = \"FirstGuess\"\npre_unet.summary()","04815eac":"def recursive_unet_block(\n    input_shape,\n    num_classes=1,\n    dropout=0.0, \n    filters=16,\n    num_layers=4,\n    use_deconv=False,\n    crop_output=True,\n    resnet_style=False):\n    \"\"\"a unet model that can be run multiple times\"\"\"\n    # Build U-Net model\n    inputs = Input(input_shape, name='InImage')\n    bn_img = BatchNormalization()(inputs)\n    last_mask = Input(input_shape[:2]+(num_classes,), name='InMask')\n    down_layers = [[], []]\n    \n    c_pair = [bn_img, last_mask]\n    for l in range(num_layers):\n        for i, x in enumerate(c_pair):\n            if (l>0) and (i==0): # combine the mask and input channels\n                x = concatenate(c_pair)\n            \n            x = conv2d_block(inputs=x, filters=filters, use_batch_norm=False, dropout=0.0, padding='same')\n            down_layers[i].append(x)\n            x = MaxPooling2D((2, 2), strides=2)(x)\n            c_pair[i] = x\n        \n        filters = filters*2 # double the number of filters with each layer\n    \n    x = concatenate(c_pair)\n    x = Dropout(dropout)(x)\n    x = conv2d_block(inputs=x, filters=filters, use_batch_norm=False, dropout=0.0, padding='same')\n\n    for conv_img, conv_mask in zip(reversed(down_layers[0]), reversed(down_layers[1])):\n        filters \/\/= 2 # decreasing number of filters with each layer \n        if use_deconv:\n            x = Conv2DTranspose(filters, (2, 2), strides=(2, 2), padding='same') (x)\n        else:\n            x = UpSampling2D((2, 2))(x)\n        \n        x = concatenate([x, conv_img, conv_mask])\n        x = conv2d_block(inputs=x, filters=filters, use_batch_norm=False, dropout=0.0, padding='same')\n    \n    outputs = Conv2D(num_classes, (1, 1), activation='tanh' if resnet_style else 'sigmoid') (x)   \n    gate_prefeature = conv2d_block(inputs=x, filters=filters, use_batch_norm=True, dropout=0.0, padding='same')\n    gates = Conv2D(num_classes, (1, 1), activation='sigmoid') (gate_prefeature)   \n    if resnet_style:\n        outputs = layers.add([last_mask, outputs]) # resnet style\n    \n    if crop_output:\n        out_lay_17_crop = layers.Cropping2D((2**num_layers, 2**num_layers))(outputs)\n        outputs = layers.ZeroPadding2D((2**num_layers, 2**num_layers), name='OutRecursive')(out_lay_17_crop)\n        \n        gate_lay_17_crop = layers.Cropping2D((2**num_layers, 2**num_layers))(gates)\n        gates = layers.ZeroPadding2D((2**num_layers, 2**num_layers), name='OutGate')(gate_lay_17_crop)\n    \n    model = Model(inputs=[inputs, last_mask], outputs=[outputs, gates], name='RecursiveBlock')\n    return model","6149048a":"rec_block = recursive_unet_block(input_shape=(None, None, 3), num_classes=1, num_layers=4, filters=16)","0ca4a691":"dot_mod = model_to_dot(rec_block, show_shapes=True, show_layer_names=False)\ndot_mod.set_rankdir('UD')\ndot_mod.write_svg('rec_block.svg')\nSVG(dot_mod.create_svg())","a86e84b5":"def build_rec_unet(recursive_steps, add_noise=False):\n    in_img = Input((None, None, 3))\n    first_seg = pre_unet(in_img)\n    out_seg_list = [layers.Lambda(lambda x: x, name='Iter{:02}'.format(0))(first_seg)]\n    for i in range(1, recursive_steps):\n        if add_noise:\n            noisy_seg = layers.GaussianNoise(0.1)(out_seg_list[-1]) # jitter things a bit\n        else:\n            noisy_seg = out_seg_list[-1]\n        out_seg, out_gate = rec_block([in_img, noisy_seg])\n        flipped_gate = layers.Lambda(lambda x: 1-x)(out_gate)\n        new_seg = layers.add([\n            layers.multiply([out_seg, out_gate]),\n            layers.multiply([noisy_seg, flipped_gate])\n        ], name='Iter{:02}'.format(i)) # name the outputs sensibly\n        out_seg_list.append(new_seg)\n    return Model(inputs=[in_img], outputs=out_seg_list, name='RecUNET')\nrec_unet = build_rec_unet(4)","709126df":"dot_mod = model_to_dot(rec_unet, show_shapes=True, show_layer_names=True)\ndot_mod.set_rankdir('LR')\ndot_mod.write_svg('rec_unet.svg')\nSVG(dot_mod.create_svg())","848e4711":"rec_unet.summary()","fe821cb9":"from keras import losses\ndef fit_rec_model(in_model, \n                  epochs=50, \n                  batch_size=8,\n                 loss_weight_min=0.1):\n    rec_steps = len(in_model.outputs)\n    # stagger losses from being mostly dice to being mostly BCE\n    loss_funcs = {\n        'Iter{:02d}'.format(i): lambda x,y: (1-k)*dice_loss(x,y)+k*losses.binary_crossentropy(x,y) \n        for i,k in enumerate(np.linspace(loss_weight_min, 1-loss_weight_min, rec_steps))\n    }\n    all_results = []\n    # progressively grow the training\n    for c_step in range(rec_steps):\n        base_weights = np.zeros(rec_steps)\n        base_weights[:c_step] = 1\n        base_weights[c_step] = 3\n        loss_weights = base_weights\/np.sum(base_weights)\n\n        in_model.compile(loss=loss_funcs, \n                         metrics=['binary_accuracy', dice_score, 'mae'], \n                         optimizer='adam', \n                         loss_weights=loss_weights.tolist())\n\n        callback_list, weight_path = get_callbacks(in_model)\n\n        out_results = in_model.fit(X_train, [y_train]*(rec_steps), \n                                       epochs=epochs,\n                                       batch_size=batch_size,\n                                       validation_data=(X_valid, [y_valid]*(rec_steps)), \n                                       callbacks=callback_list)\n        all_results += [out_results]\n        in_model.load_weights(weight_path)\n    \n        clear_output()\n        v_keys = [k for k in out_results.history.keys() if 'val_{}'.format(k) in out_results.history.keys()]\n        fig, m_axs = plt.subplots(1, len(v_keys), figsize=(4*len(v_keys), 4))\n        for c_key, c_ax in zip(v_keys, m_axs):\n            c_ax.plot(out_results.history[c_key], 'r-', label='Training')\n            val_vec = out_results.history['val_{}'.format(c_key)]\n            c_ax.plot(val_vec, 'b-', label='Validation (Best: {:2.2%})'.format(np.nanmin(val_vec) if 'loss' in c_key else np.nanmax(val_vec)))\n            c_ax.set_title(c_key)\n            c_ax.legend()\n        fig.savefig(weight_path.replace('.hdf5', '.png'))\n\n        in_model.save(weight_path.replace('_weights', '_model'))\n    out_df = pd.concat([pd.DataFrame(keras_history.history).\\\n                            assign(train_block=i).\\\n                            reset_index().\\\n                            rename(columns={'index': 'inner_epoch'})\n                        for i, keras_history in enumerate(all_results)],\n                      ignore_index=True).\\\n        reset_index().\\\n        rename(columns={'index': 'epoch'})\n    return out_df","76b1f04c":"rec_fit_df = fit_rec_model(rec_unet, epochs=50)\nrec_fit_df","1ea969cb":"full_fit_df = pd.melt(rec_fit_df, id_vars=['train_block','epoch', 'inner_epoch']).query('variable!=\"lr\"')\nfull_fit_df['split'] = full_fit_df['variable'].map(lambda x: 'validation' if x.startswith('val_') else 'training')\nfull_fit_df['clean_variable'] = full_fit_df['variable'].map(lambda x: x.replace('val_', ''))\nfull_fit_df['model'] = full_fit_df['clean_variable'].map(lambda x: x.split('_')[0] if '_' in x else 'all')\nfull_fit_df['variable'] = full_fit_df['clean_variable'].map(lambda x: '_'.join(x.split('_')[1:]) if '_' in x else x)\nfull_fit_df.to_csv('rec_values.csv', index=False)\nfull_fit_df.head(5)","5e3079da":"sns.catplot(data=full_fit_df.query('variable!=\"loss\"'), \n            x='epoch', \n            y='value', \n            col='split', \n            hue='model', \n            row='variable', \n            kind='point', \n            sharey='row')","7119b179":"sns.catplot(data=full_fit_df.query('variable!=\"loss\"'), \n            x='model', \n            y='value', \n            col='split', \n            hue='epoch', \n            row='variable', \n            kind='point', \n            sharey='row')","b77cbe17":"full_fit_df.query('variable==\"binary_accuracy\"').query('split==\"validation\"').pivot_table(columns='model', values='value', index=['epoch'])","6150e902":"in_vec = Input((None, None, 3))\nmulti_out = rec_unet(in_vec)\nsimple_rec_unet = Model(inputs=[in_vec], \n                        outputs=[multi_out[-1]])","76711bd0":"show_training(simple_rec_unet)","74e780c8":"show_validation(simple_rec_unet)","29b9a8e4":"def show_multistep(in_model, in_df, samples=2):\n    steps = len(in_model.output_names)\n    fig, m_axs = plt.subplots(2*(steps)+1, samples, figsize=(6*samples, 4*((2+steps)*2+1)))\n    m_axs[1, 0].set_title('Ground-Truth')\n    \n    for (c_idx, c_row), n_axs in zip(in_df.sample(samples, random_state=0).iterrows(), \n                                     m_axs.T):\n        ax1 = n_axs[0]\n        show_row([ax1]+n_axs[1:3].tolist(), c_row)\n        in_img = np.expand_dims(c_row['rgb_image'], 0)\n        out_segs = in_model.predict(in_img)\n        for i, (last_seg, seg_name) in enumerate(zip(out_segs, in_model.output_names)):\n            j = 2*i+1\n            show_row([ax1]+n_axs[j:j+2].tolist(), {'rgb_image': c_row['rgb_image'], 'segmap': last_seg[0]})\n            \n            m_axs[j, 0].set_title(seg_name)\n            m_axs[j+1, 0].set_title(seg_name)","b1805e78":"show_multistep(rec_unet, sample_aug_df, samples=4)","ba574f52":"show_multistep(rec_unet, valid_df)","8febd5b5":"show_multistep(rec_unet, valid_df)","83d8d264":"## Deep Learning Section","1796c57f":"# Build and Train Models","9b222e72":"# Simple U-Net","566c9902":"# Setup and Loading","89284a39":"### Glue the beast together","0ecc700c":"## Setup the Augmentation","42faf15d":"## Result Code","e2b65155":"# Create the static datasets","6cd7d446":"## Training Code","36640977":"## Show other components","87067158":"# Vanilla U-Net","d9f59f4b":"# Recursive Model\nThe recursive model starts with a simple U-Net to get the initial segmentation and then ","97ea890d":"### Progressively Growing Training\nHere we train the model one output at a time starting with the simple first layer and then progressively adding outputs and de-weighting the earlier stages. The idea should be to get a more stable result than trying the train the whole mess at once.","8a3e628d":"# Overview\n## Model\nHere the challenge is to accurately produce a normalized distance map instead of a segmentation (which in standard computer vision requires several iterations). The model choice goes one step beyond the [Gated RNN U-Net](https:\/\/www.kaggle.com\/kmader\/gated-rnn-u-net-model) where the output segmentation is fed into the same model again as a _seed_ segmentation for making the next prediction. \nHere we add an additional output called _gate_ which is used to combine the output with the previous segmentation (using multiplication). ","fcb7a028":"### Make a simple, single input, single output model"}}