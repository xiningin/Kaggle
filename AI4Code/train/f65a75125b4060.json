{"cell_type":{"7935afc2":"code","3f74bfbb":"code","f579f869":"code","182cf523":"code","ef29a326":"code","bb8e91a7":"code","e1f05b95":"code","f86d5fc1":"code","6e6816a4":"code","8684e4f6":"code","3cb38159":"code","96401c72":"code","664c9062":"code","3758d22c":"code","00f6a631":"code","a395d048":"code","022b5d9b":"code","6e8f05e3":"code","71990234":"code","52f586eb":"code","df4309ac":"code","798d7c0c":"code","82209df3":"code","e33e7b90":"code","43a4fd8a":"code","bf2c7c96":"code","df35db91":"code","2509a55f":"code","7f803b27":"code","dea1161c":"code","1a587323":"code","9078f669":"code","6bc8e835":"code","35168b1b":"code","0ec0701e":"code","f3ae066a":"code","ac423fbd":"code","b4652f2c":"code","3a0383d4":"code","808bdc19":"code","7f24beb9":"code","5c5e943b":"code","16b06998":"code","b8c02d28":"code","13bbfc56":"code","47ca451b":"code","ede8b835":"code","c63eafe6":"code","2be8eeee":"code","6b638dd9":"code","60702046":"markdown","f6a7ed43":"markdown","5148df35":"markdown","c89748b7":"markdown","0a5e3ba3":"markdown","5a7f6ce0":"markdown","5307a1c0":"markdown","4cc617b9":"markdown","30924acc":"markdown","86c2840b":"markdown","1b6505e8":"markdown","01e1dde4":"markdown","d59aef8c":"markdown","ead65592":"markdown","d5d4dffe":"markdown","5e4b4598":"markdown","c76bc0d4":"markdown","ac0f3870":"markdown","fd2789b4":"markdown"},"source":{"7935afc2":"!pip install transformers\n!pip install seqeval\n!pip install -U torch","3f74bfbb":"# %cd drive\/MyDrive\/layoutlm_CORD\/","f579f869":"import numpy as np\nimport pickle\nimport random\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as plt\n\n#### METRICS ####\nfrom seqeval.metrics import (\n    classification_report,\n    f1_score,\n    precision_score,\n    recall_score)\n\n##### UTILS ######\nimport torch\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler, Dataset\nfrom torch.utils.data.distributed import DistributedSampler\nfrom tqdm import tqdm, trange\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss, MSELoss\nfrom transformers import BertConfig, BertModel, BertPreTrainedModel, get_linear_schedule_with_warmup, AdamW, BertTokenizerFast, BertForTokenClassification\nfrom torch.nn import LayerNorm as BertLayerNorm","182cf523":"## Import Data\npath = '..\/input\/cord-preprocessed-dataset\/'\ntrain = pickle.load(open(path + 'train.pkl', 'rb'))\nval = pickle.load(open(path + 'val.pkl', 'rb'))\ntest = pickle.load(open(path + 'test.pkl', 'rb'))","ef29a326":"for index, elem in enumerate(train[2]):\n  for i,e in enumerate(elem):\n    if not all(np.array(e) >= 0):\n      print(e)\n      print(index, i)","bb8e91a7":"train[2][669][0] = [0, 426, 213, 396]\ntrain[2][669][3] = [0, 466, 112, 435]\ntrain[2][669][6] = [0, 529, 223, 508]","e1f05b95":"for index, elem in enumerate(test[2]):\n  for i,e in enumerate(elem):\n    if not all(np.array(e) >= 0):\n      print(e)\n      print(index, i)","f86d5fc1":"test[2][88][17] = [0, 891, 106, 862]","6e6816a4":"all_labels = [item for sublist in train[1] for item in sublist] + [item for sublist in val[1] for item in sublist] + [item for sublist in test[1] for item in sublist]","8684e4f6":"from collections import Counter\nCounter(all_labels)","3cb38159":"replacing_labels = {'menu.etc': 'O', 'mneu.itemsubtotal': 'O', 'menu.sub_etc': 'O', 'menu.sub_unitprice': 'O', 'menu.vatyn': 'O',\n                  'void_menu.nm': 'O', 'void_menu.price': 'O', 'sub_total.othersvc_price': 'O'}","96401c72":"def replace_elem(elem):\n  try:\n    return replacing_labels[elem]\n  except KeyError:\n    return elem\ndef replace_list(ls):\n  return [replace_elem(elem) for elem in ls]\ntrain[1] = [replace_list(ls) for ls in train[1]]\nval[1] = [replace_list(ls) for ls in val[1]]\ntest[1] = [replace_list(ls) for ls in test[1]]","664c9062":"all_labels = [item for sublist in train[1] for item in sublist] + [item for sublist in val[1] for item in sublist] + [item for sublist in test[1] for item in sublist]\nCounter(all_labels)","3758d22c":"labels = list(set(all_labels))","00f6a631":"LAYOUTLM_PRETRAINED_MODEL_ARCHIVE_MAP = {}\n\nLAYOUTLM_PRETRAINED_CONFIG_ARCHIVE_MAP = {}\n\n\nclass LayoutlmConfig(BertConfig):\n    pretrained_config_archive_map = LAYOUTLM_PRETRAINED_CONFIG_ARCHIVE_MAP\n    model_type = \"bert\"\n\n    def __init__(self, max_2d_position_embeddings=1024, **kwargs):\n        super().__init__(**kwargs)\n        self.max_2d_position_embeddings = max_2d_position_embeddings","a395d048":"class LayoutlmEmbeddings(nn.Module):\n    def __init__(self, config):\n        super(LayoutlmEmbeddings, self).__init__()\n        self.word_embeddings = nn.Embedding(\n            config.vocab_size, config.hidden_size, padding_idx=0\n        )\n\n        ## Positional Embeddings\n        self.position_embeddings = nn.Embedding(\n            config.max_position_embeddings, config.hidden_size\n        )\n        self.x_position_embeddings = nn.Embedding(\n            config.max_2d_position_embeddings, config.hidden_size\n        )\n        self.y_position_embeddings = nn.Embedding(\n            config.max_2d_position_embeddings, config.hidden_size\n        )\n        self.h_position_embeddings = nn.Embedding(\n            config.max_2d_position_embeddings, config.hidden_size\n        )\n        self.w_position_embeddings = nn.Embedding(\n            config.max_2d_position_embeddings, config.hidden_size\n        )\n        \n        \n        self.token_type_embeddings = nn.Embedding(\n            config.type_vocab_size, config.hidden_size\n        )\n\n        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n        # any TensorFlow checkpoint file\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(\n        self,\n        input_ids,\n        bbox,\n        token_type_ids=None,\n        position_ids=None,\n        inputs_embeds=None,\n    ):\n        seq_length = input_ids.size(1)\n        if position_ids is None:\n            position_ids = torch.arange(\n                seq_length, dtype=torch.long, device=input_ids.device\n            )\n            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n\n        words_embeddings = self.word_embeddings(input_ids)\n        position_embeddings = self.position_embeddings(position_ids)\n        left_position_embeddings = self.x_position_embeddings(bbox[:, :, 0])\n        upper_position_embeddings = self.y_position_embeddings(bbox[:, :, 3])\n        right_position_embeddings = self.x_position_embeddings(bbox[:, :, 2])\n        lower_position_embeddings = self.y_position_embeddings(bbox[:, :, 1])\n        h_position_embeddings = self.h_position_embeddings(\n            bbox[:, :, 1] - bbox[:, :, 3]\n        )\n        w_position_embeddings = self.w_position_embeddings(\n            bbox[:, :, 2] - bbox[:, :, 0]\n        )\n        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n\n        embeddings = (\n            words_embeddings\n            + position_embeddings\n            + left_position_embeddings\n            + upper_position_embeddings\n            + right_position_embeddings\n            + lower_position_embeddings\n            + h_position_embeddings\n            + w_position_embeddings\n            + token_type_embeddings\n        )\n        embeddings = self.LayerNorm(embeddings)\n        embeddings = self.dropout(embeddings)\n        return embeddings","022b5d9b":"class LayoutlmModel(BertModel):\n\n    config_class = LayoutlmConfig\n    pretrained_model_archive_map = LAYOUTLM_PRETRAINED_MODEL_ARCHIVE_MAP\n    base_model_prefix = \"bert\"\n\n    def __init__(self, config):\n        super(LayoutlmModel, self).__init__(config)\n        self.embeddings = LayoutlmEmbeddings(config)\n        self.init_weights()\n\n    def forward(\n        self,\n        input_ids,\n        bbox,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n    ):\n        if attention_mask is None:\n            attention_mask = torch.ones_like(input_ids)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n\n        # We create a 3D attention mask from a 2D tensor mask.\n        # Sizes are [batch_size, 1, 1, to_seq_length]\n        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n        # this attention mask is more simple than the triangular masking of causal attention\n        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        extended_attention_mask = extended_attention_mask.to(\n            dtype=next(self.parameters()).dtype\n        )  # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n        if head_mask is not None:\n            if head_mask.dim() == 1:\n                head_mask = (\n                    head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n                )\n                head_mask = head_mask.expand(\n                    self.config.num_hidden_layers, -1, -1, -1, -1\n                )\n            elif head_mask.dim() == 2:\n                head_mask = (\n                    head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)\n                )  # We can specify head_mask for each layer\n            head_mask = head_mask.to(\n                dtype=next(self.parameters()).dtype\n            )  # switch to fload if need + fp16 compatibility\n        else:\n            head_mask = [None] * self.config.num_hidden_layers\n\n        embedding_output = self.embeddings(\n            input_ids, bbox, position_ids=position_ids, token_type_ids=token_type_ids\n        )\n        encoder_outputs = self.encoder(\n            embedding_output, extended_attention_mask, head_mask=head_mask\n        )\n        sequence_output = encoder_outputs[0]\n        pooled_output = self.pooler(sequence_output)\n\n        outputs = (sequence_output, pooled_output) + encoder_outputs[\n            1:\n        ]  # add hidden_states and attentions if they are here\n        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)","6e8f05e3":"class LayoutlmForTokenClassification(BertPreTrainedModel):\n    config_class = LayoutlmConfig\n    pretrained_model_archive_map = LAYOUTLM_PRETRAINED_MODEL_ARCHIVE_MAP\n    base_model_prefix = \"bert\"\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.bert = LayoutlmModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    def forward(\n        self,\n        input_ids,\n        bbox,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n    ):\n\n        outputs = self.bert(\n            input_ids=input_ids,\n            bbox=bbox,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n        )\n\n        sequence_output = outputs[0]\n\n        sequence_output = self.dropout(sequence_output)\n        logits = self.classifier(sequence_output)\n\n        outputs = (logits,) + outputs[\n            2:\n        ]  # add hidden states and attention if they are here\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            # Only keep active parts of the loss\n            if attention_mask is not None:\n                active_loss = attention_mask.view(-1) == 1\n                active_logits = logits.view(-1, self.num_labels)[active_loss]\n                active_labels = labels.view(-1)[active_loss]\n                loss = loss_fct(active_logits, active_labels)\n            else:\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            outputs = (loss,) + outputs\n\n        return outputs  # (loss), scores, (hidden_states), (attentions)","71990234":"def set_seed(seed): ## for reproductibility\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)","52f586eb":"class CordDataset(Dataset):\n    def __init__(self, examples, tokenizer, labels, pad_token_label_id):\n        features = convert_examples_to_features(\n            examples,\n            labels,\n            max_seq_length,\n            tokenizer,\n            cls_token_at_end=False,\n            # xlnet has a cls token at the end\n            cls_token=tokenizer.cls_token,\n            cls_token_segment_id=0,\n            sep_token=tokenizer.sep_token,\n            sep_token_extra=False,\n            # roberta uses an extra separator b\/w pairs of sentences, cf. github.com\/pytorch\/fairseq\/commit\/1684e166e3da03f5b600dbb7855cb98ddfcd0805\n            pad_on_left=False,\n            # pad on the left for xlnet\n            pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n            pad_token_segment_id=0,\n            pad_token_label_id=pad_token_label_id,\n        )\n\n        self.features = features\n        # Convert to Tensors and build dataset\n        self.all_input_ids = torch.tensor(\n            [f.input_ids for f in features], dtype=torch.long\n        )\n        self.all_input_mask = torch.tensor(\n            [f.input_mask for f in features], dtype=torch.long\n        )\n        self.all_segment_ids = torch.tensor(\n            [f.segment_ids for f in features], dtype=torch.long\n        )\n        self.all_label_ids = torch.tensor(\n            [f.label_ids for f in features], dtype=torch.long\n        )\n        self.all_bboxes = torch.tensor([f.boxes for f in features], dtype=torch.long)\n\n    def __len__(self):\n        return len(self.features)\n\n    def __getitem__(self, index):\n        return (\n            self.all_input_ids[index],\n            self.all_input_mask[index],\n            self.all_segment_ids[index],\n            self.all_label_ids[index],\n            self.all_bboxes[index],\n        )\n\nclass InputFeatures(object):\n    \"\"\"A single set of features of data.\"\"\"\n\n    def __init__(\n        self,\n        input_ids,\n        input_mask,\n        segment_ids,\n        label_ids,\n        boxes\n    ):\n        assert (\n            0 <= all(boxes) <= 1000\n        ), \"Error with input bbox ({}): the coordinate value is not between 0 and 1000\".format(\n            boxes\n        )\n        self.input_ids = input_ids\n        self.input_mask = input_mask\n        self.segment_ids = segment_ids\n        self.label_ids = label_ids\n        self.boxes = boxes\n\ndef convert_examples_to_features(\n    examples,\n    label_list,\n    max_seq_length,\n    tokenizer,\n    cls_token_at_end=False,\n    cls_token=\"[CLS]\",\n    cls_token_segment_id=1,\n    sep_token=\"[SEP]\",\n    sep_token_extra=False,\n    pad_on_left=False,\n    pad_token=0,\n    cls_token_box=[0, 0, 0, 0],\n    sep_token_box=[1000, 1000, 1000, 1000],\n    pad_token_box=[0, 0, 0, 0],\n    pad_token_segment_id=0,\n    pad_token_label_id=-1,\n    sequence_a_segment_id=0,\n    mask_padding_with_zero=True,\n):\n    \"\"\" Loads a data file into a list of `InputBatch`s\n        `cls_token_at_end` define the location of the CLS token:\n            - False (Default, BERT\/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n            - True (XLNet\/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n    \"\"\"\n\n    label_map = {label: i for i, label in enumerate(label_list)}\n\n    features = []\n    for i in range(len(examples[0])):\n        width, height = 1000, 1000\n        words = examples[0]\n        labels = examples[1]\n        boxes = examples[2]\n\n        tokens = []\n        token_boxes = []\n        label_ids = []\n        for word, label, box in zip(\n            words[i], labels[i], boxes[i]\n        ):\n            if len(word) < 1: # SKIP EMPTY WORD\n              continue\n            word_tokens = tokenizer.tokenize(word)\n            tokens.extend(word_tokens)\n            token_boxes.extend([box] * len(word_tokens))\n            # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n            label_ids.extend(\n                [label_map[label]] + [pad_token_label_id] * (len(word_tokens) - 1))\n\n        # Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.\n        special_tokens_count = 3 if sep_token_extra else 2\n        if len(tokens) > max_seq_length - special_tokens_count:\n            tokens = tokens[: (max_seq_length - special_tokens_count)]\n            token_boxes = token_boxes[: (max_seq_length - special_tokens_count)]\n            label_ids = label_ids[: (max_seq_length - special_tokens_count)]\n\n        # The convention in BERT is:\n        # (a) For sequence pairs:\n        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n        #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n        # (b) For single sequences:\n        #  tokens:   [CLS] the dog is hairy . [SEP]\n        #  type_ids:   0   0   0   0  0     0   0\n        #\n        # Where \"type_ids\" are used to indicate whether this is the first\n        # sequence or the second sequence. The embedding vectors for `type=0` and\n        # `type=1` were learned during pre-training and are added to the wordpiece\n        # embedding vector (and position vector). This is not *strictly* necessary\n        # since the [SEP] token unambiguously separates the sequences, but it makes\n        # it easier for the model to learn the concept of sequences.\n        #\n        # For classification tasks, the first vector (corresponding to [CLS]) is\n        # used as as the \"sentence vector\". Note that this only makes sense because\n        # the entire model is fine-tuned.\n        tokens += [sep_token]\n        token_boxes += [sep_token_box]\n        label_ids += [pad_token_label_id]\n        if sep_token_extra:\n            # roberta uses an extra separator b\/w pairs of sentences\n            tokens += [sep_token]\n            token_boxes += [sep_token_box]\n            label_ids += [pad_token_label_id]\n        segment_ids = [sequence_a_segment_id] * len(tokens)\n\n        if cls_token_at_end:\n            tokens += [cls_token]\n            token_boxes += [cls_token_box]\n            label_ids += [pad_token_label_id]\n            segment_ids += [cls_token_segment_id]\n        else:\n            tokens = [cls_token] + tokens\n            token_boxes = [cls_token_box] + token_boxes\n            label_ids = [pad_token_label_id] + label_ids\n            segment_ids = [cls_token_segment_id] + segment_ids\n\n        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n        # tokens are attended to.\n        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n\n        # Zero-pad up to the sequence length.\n        padding_length = max_seq_length - len(input_ids)\n        if pad_on_left:\n            input_ids = ([pad_token] * padding_length) + input_ids\n            input_mask = (\n                [0 if mask_padding_with_zero else 1] * padding_length\n            ) + input_mask\n            segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n            label_ids = ([pad_token_label_id] * padding_length) + label_ids\n            token_boxes = ([pad_token_box] * padding_length) + token_boxes\n        else:\n            input_ids += [pad_token] * padding_length\n            input_mask += [0 if mask_padding_with_zero else 1] * padding_length\n            segment_ids += [pad_token_segment_id] * padding_length\n            label_ids += [pad_token_label_id] * padding_length\n            token_boxes += [pad_token_box] * padding_length\n\n        assert len(input_ids) == max_seq_length\n        assert len(input_mask) == max_seq_length\n        assert len(segment_ids) == max_seq_length\n        assert len(label_ids) == max_seq_length\n        assert len(token_boxes) == max_seq_length\n\n        features.append(\n            InputFeatures(\n                input_ids=input_ids,\n                input_mask=input_mask,\n                segment_ids=segment_ids,\n                label_ids=label_ids,\n                boxes=token_boxes,\n            )\n        )\n    return features\n","df4309ac":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","798d7c0c":"ls","82209df3":"model_path = '..\/input\/layoutlm\/layoutlm-large-uncased'\nnum_labels = len(labels)\nconfig_class, model_class, tokenizer_class = LayoutlmConfig, LayoutlmForTokenClassification, BertTokenizerFast\nconfig = config_class.from_pretrained(model_path, num_labels=num_labels)\ntokenizer = tokenizer_class.from_pretrained(model_path, do_lower_case=True)\nmodel = model_class.from_pretrained(model_path, from_tf=bool(\".ckpt\" in model_path), config=config)\nmodel.to(device)","e33e7b90":"sum_ = []\nfor x in train[0]:\n  sum_.append(len(x))\nprint(max(sum_))\nprint(min(sum_))\nprint(sum(sum_)\/len(sum_))","43a4fd8a":"len(labels)","bf2c7c96":"max_seq_length = 150\npad_token_label_id = CrossEntropyLoss().ignore_index\ntrain_dataset = CordDataset(train, tokenizer, labels, pad_token_label_id)\nvalidation_dataset = CordDataset(val, tokenizer, labels, pad_token_label_id)\nmodel_type = 'layoutlm'","df35db91":"train_batch_size = 8\nlearning_rate = 1e-4\nadam_epsilon = 1e-8\nweight_decay = 0.0\nnum_train_epochs = 4 ## To fine-tune (adding drop out so that It can lead to overfit less)\nmax_steps = 0\ngradient_accumulation_steps = 1\nmax_grad_norm = 1.0\nwarmup_steps = 0\nseed = 42\n\ntrain_sampler = RandomSampler(train_dataset)\ntrain_dataloader = DataLoader(\n        train_dataset,\n        sampler=train_sampler,\n        batch_size=train_batch_size,\n        collate_fn=None,\n    )\nvalid_sampler = RandomSampler(validation_dataset)\nvalid_dataloader = DataLoader(\n        validation_dataset,\n        sampler=valid_sampler,\n        batch_size=train_batch_size,\n        collate_fn=None,\n    )\nif max_steps > 0:\n    t_total = max_steps\n    num_train_epochs = (\n        args.max_steps\n        \/\/ (len(train_dataloader) \/\/ gradient_accumulation_steps)\n        + 1\n    )\nelse:\n    t_total = (\n        len(train_dataloader)\n        \/\/ gradient_accumulation_steps\n        * num_train_epochs\n    )\nno_decay = [\"bias\", \"LayerNorm.weight\"]\noptimizer_grouped_parameters = [\n        {\n            \"params\": [\n                p\n                for n, p in model.named_parameters()\n                if not any(nd in n for nd in no_decay)\n            ],\n            \"weight_decay\": weight_decay,\n        },\n        {\n            \"params\": [\n                p\n                for n, p in model.named_parameters()\n                if any(nd in n for nd in no_decay)\n            ],\n            \"weight_decay\": 0.0,\n        },\n    ]\noptimizer = AdamW(\n        optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon\n    )\nscheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total\n    )","2509a55f":"def results(preds, out_label_ids, labels, loss_):\n#   global preds\n  global out_label_list\n  global preds_list\n  preds = np.argmax(preds, axis=2)\n\n  label_map = {i: label for i, label in enumerate(labels)}\n\n  out_label_list = [[] for _ in range(out_label_ids.shape[0])]\n  preds_list = [[] for _ in range(out_label_ids.shape[0])]\n  print(\"-->\", out_label_ids.shape)\n  for i in range(out_label_ids.shape[0]):\n      for j in range(out_label_ids.shape[1]):\n          if out_label_ids[i, j] != pad_token_label_id:\n              out_label_list[i].append(label_map[out_label_ids[i][j]])\n              preds_list[i].append(label_map[preds[i][j]])\n\n  results = {\n      \"loss\": loss_,\n      \"precision\": precision_score(out_label_list, preds_list),\n      \"recall\": recall_score(out_label_list, preds_list),\n      \"f1\": f1_score(out_label_list, preds_list),\n  }\n  return results","7f803b27":"global_step = 0\nmodel.zero_grad()\ntrain_iterator = trange(int(num_train_epochs), desc=\"Epoch\")\nset_seed(seed)\nfor _ in train_iterator:\n  #epoch_iterator = tqdm(\n      #train_dataloader, desc=\"Iteration\")\n  tr_loss = 0.0\n  nb_train_steps = 0\n  preds_train = None\n  out_label_ids = None\n  for step, batch in enumerate(train_dataloader):\n      if (step<=76):\n        continue\n#       print(batch[0].shape, batch[1].shape, batch[2].shape, batch[3].shape, batch[4].shape, step)\n#       if batch[0].shape[0]!=8:\n#         continue\n      if step==80: break\n      print(step)\n      model.train()\n      inputs = {\n          \"input_ids\": batch[0].to(device),\n          \"attention_mask\": batch[1].to(device),\n          \"labels\": batch[3].to(device),\n      }\n      if model_type in [\"layoutlm\"]:\n          inputs[\"bbox\"] = batch[4].to(device)\n      inputs[\"token_type_ids\"] = (\n          batch[2].to(device) if model_type in [\"bert\", \"layoutlm\"] else None)\n      try:\n        outputs = model(**inputs)\n      except Exception as e:\n        print(\"==>:\",e, step, batch[0].shape, batch[1].shape, batch[2].shape, batch[3].shape, batch[4].shape)\n        continue\n      # model outputs are always tuple in pytorch-transformers (see doc)\n      loss, logits = outputs[0], outputs[1]\n      loss.backward()\n\n      tr_loss += loss.item()\n      #if (step+1) % 25 == 0:\n        #print(f\"Train Epoch : {step+1}\/{len(train_dataloader)}\")\n\n      if (step + 1) % gradient_accumulation_steps == 0:\n          torch.nn.utils.clip_grad_norm_(\n                  model.parameters(), max_grad_norm\n              )\n          optimizer.step()\n          scheduler.step()  # Update learning rate schedule\n          model.zero_grad()\n          global_step += 1\n      nb_train_steps += 1\n      if preds_train is None:\n          preds_train = logits.detach().cpu().numpy()\n          out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n      else:\n          preds_train = np.append(preds_train, logits.detach().cpu().numpy(), axis=0)\n          out_label_ids = np.append(\n              out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n  res = results(preds_train, out_label_ids, labels, tr_loss\/len(train_dataloader))\n  print('Train Results', res)\n\n  ###### EVALUATION #######\n\n  #epoch_iterator = tqdm(valid_dataloader, desc=\"Iteration\")\n  eval_loss = 0.0\n  nb_eval_steps = 0\n  preds_val = None\n  out_label_ids = None\n  model.eval()\n  for step, batch in enumerate(valid_dataloader):\n    with torch.no_grad():\n      inputs = {\n          \"input_ids\": batch[0].to(device),\n          \"attention_mask\": batch[1].to(device),\n          \"labels\": batch[3].to(device),\n      }\n      if model_type in [\"layoutlm\"]:\n          inputs[\"bbox\"] = batch[4].to(device)\n      inputs[\"token_type_ids\"] = (\n          batch[2].to(device) if model_type in [\"bert\", \"layoutlm\"] else None)\n      # model outputs are always tuple in pytorch-transformers (see doc)\n      outputs = model(**inputs)\n      tmp_eval_loss, logits = outputs[:2]\n      eval_loss += tmp_eval_loss.item()\n    nb_eval_steps += 1\n    if preds_val is None:\n      preds_val = logits.detach().cpu().numpy()\n      out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n    else:\n      preds_val = np.append(preds_val, logits.detach().cpu().numpy(), axis=0)\n      out_label_ids = np.append(\n          out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0\n      )\n  eval_loss = eval_loss \/ nb_eval_steps\n  res = results(preds_val, out_label_ids, labels, eval_loss)\n  print('Validation results',res)","dea1161c":"preds_tr = np.argmax(preds_train, axis=2)\n\nlabel_map = {i: label for i, label in enumerate(labels)}\n\nout_label_list_tr = [[] for _ in range(out_label_ids.shape[0])]\npreds_list_tr = [[] for _ in range(out_label_ids.shape[0])]","1a587323":"label_map","9078f669":"# preds_train, out_label_ids\nfor i in range(out_label_ids.shape[0]):\n  for j in range(out_label_ids.shape[1]):\n      if out_label_ids[i, j] != pad_token_label_id:\n          out_label_list_tr[i].append(label_map[out_label_ids[i][j]])\n          print(i,j, preds_tr[i][j])\n          preds_list_tr[i].append(label_map[preds_tr[i][j]])","6bc8e835":"global_step = 0\nmodel.zero_grad()\ntrain_iterator = trange(int(num_train_epochs), desc=\"Epoch\")\nset_seed(seed)\nfor _ in train_iterator:\n  #epoch_iterator = tqdm(\n      #train_dataloader, desc=\"Iteration\")\n  tr_loss = 0.0\n  nb_train_steps = 0\n  preds_train = None\n  out_label_ids = None\n  for step, batch in enumerate(train_dataloader):\n      if (step<=77):# or step==79):\n        continue\n      if (step==81):\n        break\n      # print(batch[0].shape, batch[1].shape, batch[2].shape, batch[3].shape, batch[4].shape, step)\n      # if batch[0].shape[0]!=8:\n      #   continue\n      # print(step)\n      model.train()\n      inputs = {\n          \"input_ids\": batch[0].to(device),\n          \"attention_mask\": batch[1].to(device),\n          \"labels\": batch[3].to(device),\n      }\n      if model_type in [\"layoutlm\"]:\n          inputs[\"bbox\"] = batch[4].to(device)\n      inputs[\"token_type_ids\"] = (\n          batch[2].to(device) if model_type in [\"bert\", \"layoutlm\"] else None)\n      try:\n        outputs = model(**inputs)\n      except Exception as e:\n        print(e, batch[0].shape, batch[1].shape, batch[2].shape, batch[3].shape, batch[4].shape)\n      # model outputs are always tuple in pytorch-transformers (see doc)\n      loss, logits = outputs[0], outputs[1]\n      loss.backward()\n\n      tr_loss += loss.item()\n      #if (step+1) % 25 == 0:\n        #print(f\"Train Epoch : {step+1}\/{len(train_dataloader)}\")\n\n      if (step + 1) % gradient_accumulation_steps == 0:\n          torch.nn.utils.clip_grad_norm_(\n                  model.parameters(), max_grad_norm\n              )\n          optimizer.step()\n          scheduler.step()  # Update learning rate schedule\n          model.zero_grad()\n          global_step += 1\n      nb_train_steps += 1\n      if preds_train is None:\n          preds_train = logits.detach().cpu().numpy()\n          out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n      else:\n          preds_train = np.append(preds_train, logits.detach().cpu().numpy(), axis=0)\n          out_label_ids = np.append(\n              out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n  res = results(preds_train, out_label_ids, labels, tr_loss\/len(train_dataloader))\n  print('Train Results', res)\n\n  ###### EVALUATION #######\n\n  #epoch_iterator = tqdm(valid_dataloader, desc=\"Iteration\")\n  eval_loss = 0.0\n  nb_eval_steps = 0\n  preds_val = None\n  out_label_ids = None\n  model.eval()\n  for step, batch in enumerate(valid_dataloader):\n    with torch.no_grad():\n      inputs = {\n          \"input_ids\": batch[0].to(device),\n          \"attention_mask\": batch[1].to(device),\n          \"labels\": batch[3].to(device),\n      }\n      if model_type in [\"layoutlm\"]:\n          inputs[\"bbox\"] = batch[4].to(device)\n      inputs[\"token_type_ids\"] = (\n          batch[2].to(device) if model_type in [\"bert\", \"layoutlm\"] else None)\n      # model outputs are always tuple in pytorch-transformers (see doc)\n      outputs = model(**inputs)\n      tmp_eval_loss, logits = outputs[:2]\n      eval_loss += tmp_eval_loss.item()\n    nb_eval_steps += 1\n    if preds_val is None:\n      preds_val = logits.detach().cpu().numpy()\n      out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n    else:\n      preds_val = np.append(preds_val, logits.detach().cpu().numpy(), axis=0)\n      out_label_ids = np.append(\n          out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0\n      )\n  eval_loss = eval_loss \/ nb_eval_steps\n  res = results(preds_val, out_label_ids, labels, eval_loss)\n  print('Validation results',res)","35168b1b":"!pip install -U torch","0ec0701e":"!pip freeze | grep torch","f3ae066a":"!pip install -U torch","ac423fbd":"torch.save(model.state_dict(), 'customllm.pt')","b4652f2c":"ls","3a0383d4":"for index, e in enumerate(test[2]):\n  for index_,l in enumerate(e):\n    if l[1] < l[3]:\n      print(index, index_)\n      print(l)","808bdc19":"test[2][54][15] = [105, 738, 498, 726]","7f24beb9":"test_dataset = CordDataset(test, tokenizer, labels, pad_token_label_id)","5c5e943b":"# Passing the test Dataset containing labels for getting the report. Metric Report from Validation is not reliable.\ndef report_test(preds, out_label_ids, labels): \n  preds = np.argmax(preds, axis=2)\n\n  label_map = {i: label for i, label in enumerate(labels)}\n\n  out_label_list = [[] for _ in range(out_label_ids.shape[0])]\n  preds_list = [[] for _ in range(out_label_ids.shape[0])]\n\n  for i in range(out_label_ids.shape[0]):\n      for j in range(out_label_ids.shape[1]):\n          if out_label_ids[i, j] != pad_token_label_id:\n              out_label_list[i].append(label_map[out_label_ids[i][j]])\n              preds_list[i].append(label_map[preds[i][j]])\n\n  results = {\n      \"precision\": precision_score(out_label_list, preds_list),\n      \"recall\": recall_score(out_label_list, preds_list),\n      \"f1\": f1_score(out_label_list, preds_list),\n  }\n  return results, classification_report(out_label_list, preds_list)","16b06998":"def results_test(preds, out_label_ids, labels):\n  preds = np.argmax(preds, axis=2)\n\n  label_map = {i: label for i, label in enumerate(labels)}\n  # global out_label_list\n  out_label_list = [[] for _ in range(out_label_ids.shape[0])]\n  preds_list = [[] for _ in range(out_label_ids.shape[0])]\n\n  for i in range(out_label_ids.shape[0]):\n      for j in range(out_label_ids.shape[1]):\n          if out_label_ids[i, j] != pad_token_label_id:\n              out_label_list[i].append(label_map[out_label_ids[i][j]])\n              preds_list[i].append(label_map[preds[i][j]])\n\n  return out_label_list","b8c02d28":"# test_sampler = RandomSampler(test_dataset)\ntest_dataloader = DataLoader(\n        test_dataset,\n        # sampler=test_sampler,\n        batch_size=1,\n        collate_fn=None,\n    )\nnb_eval_steps = 0\npreds_test = None\nout_label_ids = None\nmodel.eval()\nfor step, batch in enumerate(test_dataloader):\n  with torch.no_grad():\n    inputs = {\n        \"input_ids\": batch[0].to(device),\n        \"attention_mask\": batch[1].to(device),\n        \"labels\": batch[3].to(device),\n    }\n    if model_type in [\"layoutlm\"]:\n        inputs[\"bbox\"] = batch[4].to(device)\n    inputs[\"token_type_ids\"] = (\n        batch[2].to(device) if model_type in [\"bert\", \"layoutlm\"] else None)\n    # model outputs are always tuple in pytorch-transformers (see doc)\n    try:\n      outputs = model(**inputs)\n    except Exception as e:\n      print(e, step, batch[0].shape, batch[1].shape, batch[2].shape, batch[3].shape, batch[4].shape)\n      continue\n    _, logits = outputs[:2]\n  if preds_test is None:\n    preds_test = logits.detach().cpu().numpy()\n    out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n  else:\n    preds_test = np.append(preds_test, logits.detach().cpu().numpy(), axis=0)\n    out_label_ids = np.append(\n        out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0\n    )","13bbfc56":"# res, report = results_test(preds_test, out_label_ids, labels)\nres = results_test(preds_test, out_label_ids, labels)\nval_r, report = report_test(preds_test, out_label_ids, labels)","47ca451b":"print(report)","ede8b835":"val_r","c63eafe6":"!pip install prettytable","2be8eeee":"from prettytable import PrettyTable\nx = PrettyTable()\nx.field_names = [\"Text\", \"Tags\"]\nfor i in range(len(res)):\n  if (i==11 or i==45):\n    continue\n  text = test[0][i]\n  result = res[i]\n  for j in range(len(test[0][i])):\n    try:\n      x.add_row([text[j], result[j]])\n    except:\n      pass\n  x.add_row([\"=\"*25, \"=\"*25])","6b638dd9":"print(x)","60702046":"# TRAIN","f6a7ed43":"# Generate our training \/ validation set","5148df35":"This function is for the evaluation (loss, f1, precision , recall)","c89748b7":"# Load models","0a5e3ba3":"Now we have to save all the unique labels in a list. (mandatory for the tokenclassification)","5a7f6ce0":"This is for making the structure of our data match the one needed by the model (mandatory)","5307a1c0":"This is the LayoutlmModel (equivalent to BertModel except that this one has an extra input called bbox.","4cc617b9":"As we can see there are some labels that contains few examples, I decided to replace them by the \"neutral\" label \"O\"","30924acc":"This is simply a code to see if there are some negative bounding boxes (I find some, they were close to zero, then I set them to zero)","86c2840b":"This is the NEW embedding that takes into account the position embedding, the rest is the same as in bert, we sum the token embedding, positional embedding, all the position embedding and the token type embedding","1b6505e8":"This is only the class config (same as in bert (BertConfig))","01e1dde4":"## Results:","d59aef8c":"# Labels Analysis","ead65592":"# Models","d5d4dffe":"Credit: https:\/\/github.com\/omarsou\/layoutlm_CORD","5e4b4598":"Load the data","c76bc0d4":"This is the equivalent of BertForTokenClassification except that the base model is the LayoutlmModel.","ac0f3870":"# Some aux functions","fd2789b4":"# TEST"}}