{"cell_type":{"dcc4a0e6":"code","a1bc4f78":"code","09feea6a":"code","ef2c0fb9":"code","2b4086da":"code","ce3fc217":"code","839efd07":"code","421ec53f":"code","2066ec7b":"code","925a5bda":"code","357af89f":"code","585eafb3":"code","fbc80490":"code","72d76dea":"code","9a9bf2fd":"code","9ce3c579":"code","96072dea":"code","a72e4133":"code","1f5ff2c3":"code","69906300":"code","0e7d46a5":"code","36e22cf0":"code","bd03d4cd":"code","f833b4b2":"code","9a6d9121":"code","d3fe4965":"code","2de47917":"code","abade65c":"code","f8efc877":"code","b16b1410":"code","99a7b61c":"code","82964ef5":"code","c08e7b2e":"code","396d6838":"code","1efb884f":"code","4cc1fdaa":"code","772f6abc":"code","052c92cf":"code","43c02c43":"code","978a92ab":"code","63a17054":"code","399dc6f7":"code","bcd8278f":"markdown","b3b08a2b":"markdown","478bc38e":"markdown","adeb4430":"markdown","d876d1cc":"markdown","b6c0045f":"markdown","33edaf76":"markdown","b2a101a0":"markdown","b12f83f1":"markdown","5da02ecf":"markdown","bbf6a475":"markdown","13dc7ab2":"markdown","950e031e":"markdown","637a4560":"markdown"},"source":{"dcc4a0e6":"# import libraries and packages \nimport numpy as np \nimport pandas as pd \nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom scipy.stats import uniform, randint\nfrom sklearn import model_selection,linear_model, metrics\nfrom sklearn.metrics import auc, accuracy_score, confusion_matrix, roc_auc_score, classification_report\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV, KFold, RandomizedSearchCV, train_test_split\n\nimport xgboost as xgb\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a1bc4f78":"# read data into a DataFrame\ncredit_df = pd.read_csv(\"\/kaggle\/input\/credit-risk-dataset\/credit_risk_dataset.csv\")","09feea6a":"# check the data size\ncredit_df.shape","ef2c0fb9":"Nan_per = credit_df.isnull().sum()\/credit_df.shape[0]*100\nNan_per.round(2)","2b4086da":"# check the mode, median for the two features\nprint('person_emp_length mode {}'.format(credit_df['person_emp_length'].mode()[0]))\nprint('person_emp_length median {}'.format(credit_df['person_emp_length'].median()))\nprint('loan_int_rate mode {}'.format(credit_df['loan_int_rate'].mode()[0]))\nprint('loan_int_rate median {}'.format(credit_df['loan_int_rate'].median()))","ce3fc217":"# fill NaN with the mode\ncredit_df['person_emp_length'].fillna(credit_df['person_emp_length'].mode()[0], inplace=True)\ncredit_df['loan_int_rate'].fillna(credit_df['loan_int_rate'].median(), inplace=True)","839efd07":"# check the nans are replaced \ncredit_df.isnull().sum()","421ec53f":"# numerical variebles\nnum_cols = pd.DataFrame(credit_df[credit_df.select_dtypes(include=['float', 'int']).columns])\n# print the numerical variebles\nnum_cols.columns","2066ec7b":"# drop the label column 'loan status' before visualization\nnum_cols_hist = num_cols.drop(['loan_status'], axis=1)\n# visualize the distribution for each varieble\nplt.figure(figsize=(12,16))\n\nfor i, col in enumerate(num_cols_hist.columns):\n    idx = int('42'+ str(i+1))\n    plt.subplot(idx)\n    sns.distplot(num_cols_hist[col], color='forestgreen', \n                 kde_kws={'color': 'indianred', 'lw': 2, 'label': 'KDE'})\n    plt.title(col+' distribution', fontsize=14)\n    plt.ylabel('Probablity', fontsize=12)\n    plt.xlabel(col, fontsize=12)\n    plt.xticks(fontsize=12)\n    plt.yticks(fontsize=12)\n    plt.legend(['KDE'], prop={\"size\":12})\n\nplt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.35,\n                    wspace=0.35)\nplt.show()","925a5bda":"# decribe the dataset\ncredit_df.describe()","357af89f":"# clean the dataset and drop outliers\ncleaned_credit_df = credit_df[credit_df['person_age']<=100]\ncleaned_credit_df = cleaned_credit_df[cleaned_credit_df['person_emp_length']<=60]\ncleaned_credit_df = cleaned_credit_df[cleaned_credit_df['person_income']<=4e6]","585eafb3":"# get the cleaned numberical variebles\ncleaned_num_cols = pd.DataFrame(cleaned_credit_df[cleaned_credit_df.select_dtypes(include=['float', 'int']).columns])","fbc80490":"corr = cleaned_num_cols.corr().sort_values('loan_status', axis=1, ascending=False)\ncorr = corr.sort_values('loan_status', axis=0, ascending=True)\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask, k=1)] = True\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(8, 6))\n    ax = sns.heatmap(corr, mask=mask, vmin=corr.loan_status.min(), \n                     vmax=corr.drop(['loan_status'], axis=0).loan_status.max(),\n                     square=True, annot=True, fmt='.2f',\n                     center=0, cmap='RdBu',annot_kws={\"size\": 12})","72d76dea":"# get the categorical variebles \ncat_cols = pd.DataFrame(cleaned_credit_df[cleaned_credit_df.select_dtypes(include=['object']).columns])\ncat_cols.columns","9a9bf2fd":"# one-hot encode the catogorical variebles\nencoded_cat_cols = pd.get_dummies(cat_cols)\ncat_cols_corr = pd.concat([encoded_cat_cols, cleaned_credit_df['loan_status']], axis=1)\ncorr = cat_cols_corr.corr().sort_values('loan_status', axis=1, ascending=False)\ncorr = corr.sort_values('loan_status', axis=0, ascending=True)\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask, k=1)] = True\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(16, 10))\n    ax = sns.heatmap(corr, mask=mask, vmin=corr.loan_status.min(), \n                     vmax=corr.drop(['loan_status'], axis=0).loan_status.max(), \n                     square=True, annot=True, fmt='.2f',\n                     center=0, cmap='RdBu',annot_kws={\"size\": 10})","9ce3c579":"# concat the numerical and one-hot encoded categorical variebles\ncleaned_credit_df = pd.concat([cleaned_num_cols, encoded_cat_cols], axis=1)\ncleaned_credit_df.head()","96072dea":"# check the cleaned dataset size \nprint ('The cleaned dataset has {} rows and {} columns'.format(cleaned_credit_df.shape[0], \n                                                               cleaned_credit_df.shape[1]))\nprint ('The cleaned dataset has {} numerical features and {} categorical features'\n       .format(len(cleaned_num_cols.columns)-1, len(encoded_cat_cols.columns)))","a72e4133":"# Split Train and Test Sets\nlabel = cleaned_credit_df['loan_status'] # labels\nfeatures = cleaned_credit_df.drop('loan_status',axis=1) # features\nx_train, x_test, y_train, y_test = model_selection.train_test_split(features, label, \n                                                                    random_state=42, test_size=.30)\nprint('The train dataset has {} data\\nThe test dataset has {} data'.\n      format(x_train.shape[0], x_test.shape[0]))\n","1f5ff2c3":"# define a model assess function to test a few model performance\ndef model_assess(model, name='Default'):\n    '''\n    This function is used to test model performance \n    \n    Input: model, defined classifer\n    Output: print the confusion matrix\n    \n    '''\n    \n    model.fit(x_train, y_train)\n    preds = model.predict(x_test)\n    preds_proba = model.predict_proba(x_test)\n    print(name, '\\n',classification_report(y_test, model.predict(x_test)))","69906300":"#KNN\nknn = KNeighborsClassifier(n_neighbors=150)\nmodel_assess(knn, name='KNN')\n#Logistic Regression\nlg = LogisticRegression(random_state=42)\nmodel_assess(lg, 'Logistic Regression')\n# Dicision trees\nD_tree = DecisionTreeClassifier(max_depth=10, min_samples_split=2, min_samples_leaf=1, random_state=42)\nmodel_assess(D_tree, 'DecisionTree Classifier')\n#XGB\nxgb = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42) \nmodel_assess(xgb, 'XGBoost')","0e7d46a5":"#ROC AUC\nfig = plt.figure(figsize=(8,5))\nplt.plot([0, 1], [0, 1],'r--')\n\n#KNN\npreds_proba_knn = knn.predict_proba(x_test)\nprobsknn = preds_proba_knn[:, 1]\nfpr, tpr, thresh = metrics.roc_curve(y_test, probsknn)\naucknn = roc_auc_score(y_test, probsknn)\nplt.plot(fpr, tpr, label=f'KNN, AUC = {str(round(aucknn,3))}')\n\n#Logistic Regression\npreds_proba_lg = lg.predict_proba(x_test)\nprobslg = preds_proba_lg[:, 1]\nfpr, tpr, thresh = metrics.roc_curve(y_test, probslg)\nauclg = roc_auc_score(y_test, probslg)\nplt.plot(fpr, tpr, label=f'Logistic Regression, AUC = {str(round(auclg,3))}')\n\n#DecisionTree Classifier\npreds_proba_D_tree = D_tree.predict_proba(x_test)\nprobsD_tree = preds_proba_D_tree[:, 1]\nfpr, tpr, thresh = metrics.roc_curve(y_test, probsD_tree)\nauclg = roc_auc_score(y_test, probsD_tree)\nplt.plot(fpr, tpr, label=f'DecisionTree Classifier, AUC = {str(round(auclg,3))}')\n\n#XGBoost\npreds_proba_xgb = xgb.predict_proba(x_test)\nprobsxgb = preds_proba_xgb[:, 1]\nfpr, tpr, thresh = metrics.roc_curve(y_test, probsxgb)\naucxgb = roc_auc_score(y_test, probsxgb)\nplt.plot(fpr, tpr, label=f'XGBoost, AUC = {str(round(aucxgb,3))}')\nplt.ylabel(\"True Positive Rate\", fontsize=12)\nplt.xlabel(\"False Positive Rate\", fontsize=12)\nplt.title(\"ROC curve\")\nplt.rcParams['axes.titlesize'] = 16\nplt.legend()\nplt.show()","36e22cf0":"feature_importance = pd.DataFrame({'feature': x_train.columns, \n                                   'importance': xgb.feature_importances_})\n\nnew_features_df = feature_importance[feature_importance['importance']>0\n                                    ].sort_values(by=['importance'],ascending=False)","bd03d4cd":"sns.set(context='paper', style='ticks',  font='sans-serif', \n        font_scale=1.2, color_codes=True, rc=None)\nfigure, ax = plt.subplots(figsize=(8, 5))\nax=sns.barplot(data = new_features_df[:10],\n              y='feature',\n              x='importance',\n              palette='Blues_d') # rocket, Blues_d\nax.set_title('feature importance', fontsize=14)\nax.set_xlabel('importance', fontsize=13)\nax.set_ylabel('feature', fontsize=13)\nplt.show()","f833b4b2":"# print the xgb base model\nxgb","9a6d9121":"# RandomizedSearchCV hyperparameter tuning\nparams = {\n    \"colsample_bytree\": uniform(0.9, 0.1), # 0.9-1 0.9 is the lower bound, 0.1 is the range\n    \"gamma\": uniform(0.2, 0.3),# 0.2-0.5\n    \"learning_rate\": uniform(0.2, 0.2), # 0.2-0.4 \n    \"max_depth\": randint(4, 6), # 4, 5, 6\n    \"n_estimators\": randint(100, 300), # 100-300\n    \"subsample\": uniform(0.9, 0.1) # 0.9-1\n}\n\nRandom_CV = RandomizedSearchCV(xgb, param_distributions=params, random_state=42, \n                            n_iter=100, cv=3, verbose=2, n_jobs=16, return_train_score=True)\n\nRandom_CV.fit(x_train, y_train)","d3fe4965":"# function to return the top selcted models\ndef report_best_scores(results, n_top=3):\n    for i in range(1, n_top + 1):\n        candidates = np.flatnonzero(results['rank_test_score'] == i)\n        for candidate in candidates:\n            print(\"Model with rank: {0}\".format(i))\n            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n                  results['mean_test_score'][candidate],\n                  results['std_test_score'][candidate]))\n            print(\"Parameters: {0}\".format(results['params'][candidate]))\n            print(\"\")\nreport_best_scores(Random_CV.cv_results_, 3)","2de47917":"Random_best_xgb = Random_CV.best_estimator_\nRandom_best_xgb.fit(x_train, np.ravel(y_train)) \npreds_proba_Random = Random_best_xgb.predict_proba(x_test)\nprobs_Random = preds_proba_Random[:, 1]\nRandom_bestauc = roc_auc_score(y_test, probs_Random)\nprint ('xgb base model AUROC socre: {}'.format(aucxgb))\nprint ('xgb best model using RandomizedSearchCV AUROC socre: {}'.format(Random_bestauc))","abade65c":"# GridSearchCV hyperparameter tuning\nparams = {\n    \"colsample_bytree\": [0.9, 0.91],\n    \"gamma\": [0.45],\n    \"learning_rate\": [0.26], # default 0.1 \n    \"max_depth\": [5], # default 3\n    \"n_estimators\": [150, 157, 160], # default 100\n    \"subsample\": [0.98, 0.97, 0.96]\n}\n\nGrid_CV = GridSearchCV(xgb, param_grid=params, cv=3, verbose=1, n_jobs=16, return_train_score=True)\nGrid_CV.fit(x_train, y_train)","f8efc877":"report_best_scores(Grid_CV.cv_results_, 3)","b16b1410":"Grid_best_xgb = Grid_CV.best_estimator_\nGrid_best_xgb.fit(x_train, np.ravel(y_train)) \npreds_proba_Grid = Grid_best_xgb.predict_proba(x_test)\nprobs_Grid = preds_proba_Grid[:, 1]\nGrid_bestauc = roc_auc_score(y_test, probs_Grid)\nprint ('xgb base model AUROC socre: {}'.format(aucxgb))\nprint ('xgb best model using RandomizedSearchCV AUROC socre: {}'.format(Random_bestauc))\nprint ('xgb best model using GridSearchCV AUROC socre: {}'.format(Grid_bestauc))","99a7b61c":"# display feature and their importance of the best model\nfeature_importance = pd.DataFrame({'feature': x_train.columns, \n                                   'importance': Grid_best_xgb.feature_importances_})\n\nnew_features_df = feature_importance[feature_importance['importance']>0\n                                    ].sort_values(by=['importance'],ascending=False)\n\nsns.set(context='paper', style='ticks',  font='sans-serif', \n        font_scale=1.2, color_codes=True, rc=None)\nfigure, ax = plt.subplots(figsize=(8, 5))\nax=sns.barplot(data = new_features_df[:10],\n              y='feature',\n              x='importance',\n              palette='Blues_d') # rocket, Blues_d\nax.set_title('feature importance', fontsize=14)\nax.set_xlabel('importance', fontsize=13)\nax.set_ylabel('feature', fontsize=13)\nplt.show()\n","82964ef5":"# display the top 10 important features\nnew_features_df.head(10)","c08e7b2e":"# select the top 20 features and then retrain the model\nnew_features = new_features_df['feature'][0:20]\nnew_features","396d6838":"# Split Train and Test Sets\nnew_features_df = pd.DataFrame(cleaned_credit_df[new_features])\nnew_features_df.shape","1efb884f":"x_train1, x_test1, y_train1, y_test1 = model_selection.train_test_split(new_features_df, label, \n                                                                    random_state=42, test_size=.30)\nprint('The train dataset has {} data\\nThe test dataset has {} data'.\n      format(x_train.shape[0], x_test.shape[0]))","4cc1fdaa":"# RandomizedSearchCV hyperparameter tuning\nparams = {\n    \"colsample_bytree\": uniform(0.9, 0.1), # 0.9-1 0.9 is the lower bound, 0.1 is the range\n    \"gamma\": uniform(0.2, 0.3),# 0.2-0.5\n    \"learning_rate\": uniform(0.2, 0.2), # 0.2-0.4 \n    \"max_depth\": randint(4, 6), # 4, 5, 6\n    \"n_estimators\": randint(100, 300), # 100-300\n    \"subsample\": uniform(0.9, 0.1) # 0.9-1\n}\n\nRandom_CV = RandomizedSearchCV(xgb, param_distributions=params, random_state=42, \n                            n_iter=100, cv=3, verbose=2, n_jobs=16, return_train_score=True)\n\nRandom_CV.fit(x_train1, y_train1)","772f6abc":"report_best_scores(Random_CV.cv_results_, 3)","052c92cf":"Random_best_xgb = Random_CV.best_estimator_\nRandom_best_xgb.fit(x_train, np.ravel(y_train)) \npreds_proba_Random = Random_best_xgb.predict_proba(x_test1)\nprobs_Random = preds_proba_Random[:, 1]\nRandom_bestauc = roc_auc_score(y_test1, probs_Random)\nprint ('xgb base model AUROC socre: {}'.format(aucxgb))\nprint ('xgb best model using RandomizedSearchCV AUROC socre: {}'.format(Random_bestauc))","43c02c43":"preds = Grid_best_xgb.predict_proba(x_test) # 1st col = pred val, 2nd col = pred prob\n\npred_probs = pd.DataFrame(preds[:,1],columns = ['Default Probability'])\n\npd.concat([pred_probs, y_test.reset_index(drop=True)],axis=1)\nthresh = np.linspace(0,1,41)\nthresh","978a92ab":"def optimize_threshold(predict,thresholds =thresh, y_true = y_test):\n    data = predict\n    \n    def_recalls = []\n    nondef_recalls = []\n    accs =[]\n\n    \n    for threshold in thresholds:\n        # predicted values for each threshold\n        data['loan_status'] = data['Default Probability'].apply(lambda x: 1 if x > threshold else 0 )\n        \n        accs.append(metrics.accuracy_score(y_true, data['loan_status']))\n        \n        stats = metrics.precision_recall_fscore_support(y_true, data['loan_status'], zero_division=0)\n        \n        def_recalls.append(stats[1][1])\n        nondef_recalls.append(stats[1][0])\n        \n        \n    return accs, def_recalls, nondef_recalls\n\naccs, def_recalls, nondef_recalls = optimize_threshold(pred_probs)\n","63a17054":"figure = plt.subplots(figsize=(8, 6))\nplt.plot(thresh,def_recalls)\nplt.plot(thresh,nondef_recalls)\nplt.plot(thresh,accs)\nplt.xlabel(\"Probability Threshold\")\nplt.legend([\"Default Recall\",\"Non-default Recall\",\"Model Accuracy\"])\nplt.show()","399dc6f7":"optim_threshold = accs.index(max(accs))\n\nprint('The model accuracy is {} using the optimal probabilty threshold'\n      .format(round(accs[optim_threshold],3)))\n\nprint ('The optimal probabilty threshold is {}'.format(thresh[optim_threshold]))","bcd8278f":"### Threshold Optimization","b3b08a2b":"**Observation:** All of the distributions are positive skewed.\n\n* `person_age`: Most people are 20 to 60 years old. In the following analysis, to be more general, people age > 100 will be droped.\n* `person_emp_length`: Most people have less than 40 years of employment. People with employment > 60 years will be droped.\n* `person_income`: It seems that there are outliers which has to be removed (> 4 million).\n* For all other variables, the distribution is more uniform across the whole range, thus they will be kept.\n","478bc38e":"**Observation:** \n\n* `person_income`, `person_emp_length`, and `person_age`: has negative effect on loan_status being default, which means the larger these variebles, the less likely the person is risky.\n* `loan_percent_income`, `loan_int_rate`, and `loan_amnt`: has postive effect on loan_status being default, which means the larger these variebles, the more likely the person is risky.","adeb4430":"## Exploratory Analysis <a class='anchor' id='Section_2'>","d876d1cc":"**Obeservation:**  \n* `person_emp_length` is the person employment history, to be more conservative, the nan values are replaced with mode, which is 0 year.\n* `loan_int_rate` is the loan income rate, to be more conservative, the nan values are replaced with 10.99, which is the median","b6c0045f":"### Feature importance","33edaf76":"### Hyperparameter Tuning","b2a101a0":"**Observation**\n\nit turns out after dropping some features, the model is only improved to 0.955, thus no furhter GridSearchCV is performed. And the original GridSearchCV best model will be used for futher threshhold optimizaiton. ","b12f83f1":"**Observation**\n\nThe top 5 important features includes:\n* `person_home_ownership_RENT`\n* `person_home_ownership_OWN`\n* `loan_grade_C`\n* `loan_percent_income`\n* `person_home_ownership_MORTGAGE`","5da02ecf":"## Project Overview <a class='anchor' id='Section_1'>\n\nCredit default risk is the risk that a lender takes the chance that a borrower fails to make required payments of the loan. \n\nIn this project, I utilized different learning algorithms including KNN, Logistic regression, decision tree, and the popular XGBoost to find the best algorithms, and on top of that, I also implemented RandomizedSearchCV and GridSearchCV to fine tune the hyperparamters and further improve the model. \n\nThe final model has a 0.939 accuracy score and 0.957 AUROC score.\n","bbf6a475":"** Discussion**\n\n* The XGBClassifier has the best performance with 0.954 AUROC score compared to other three classifiers KNN, Logistic regression, and decision tree using the base model.\n\n* Using RandomizedSearchCV to fast optimize hyperparamters, the model AUROC is improved to 0.9563\n\n* With further fine tuning around those hyperparameters using GridSearchCV, the final best model has a 0.9571 AUROC score. \n\n* The optimal probability threshold for the best model is 0.55 resulting accuracy 0.939.\n\n","13dc7ab2":"**Obeservation:** \n* Only two columns of data contains NaN, \n* `person_emp_length` contains **2.75%** NaN and `loan_int_rate` contains **9.56%** NaN","950e031e":"## Modeling","637a4560":"### Evaluate different algorithms"}}