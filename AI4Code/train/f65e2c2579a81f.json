{"cell_type":{"07da958e":"code","b71be25c":"code","188664f8":"code","a1915593":"code","7630ded4":"code","1fa56996":"code","606c12da":"code","40cef046":"code","959f5f19":"code","272dcdc6":"code","d674a004":"code","e3524e7d":"code","3f52dc0a":"code","d77f722e":"code","009f020e":"code","c144f3b9":"code","8fbfd5f3":"code","bf9ff4a0":"code","f78cd2b8":"code","794c0d6c":"code","32c14e27":"code","1959af9f":"code","4ea7201f":"code","298320c1":"code","b1c652f1":"code","3e8f54e4":"code","a070e123":"code","2bf0121a":"code","35cc6c31":"code","c953f2d2":"code","4e72fc06":"code","98ba286c":"code","06a4ecbb":"code","007c0bef":"code","2bf1ab15":"code","253afcb6":"code","a93a3a78":"code","d4b9d796":"code","f85d21c9":"code","0d06837f":"code","8963242a":"code","4de7cc1c":"code","71cbd70b":"code","be7bb789":"code","7a3101f0":"code","2161e174":"code","1136c70f":"code","e50c917b":"code","803f8b12":"code","437d62b1":"code","7e69763d":"code","b283118e":"code","7edf7743":"code","cb92f134":"code","b1327749":"code","bc5668be":"code","b914de66":"code","00af6743":"code","0d60cc31":"code","737b9fff":"code","ecbb84d6":"code","4a8686bb":"code","fe61a59e":"code","4979d39f":"code","c3a6c896":"code","d8791206":"code","369abff5":"code","92827950":"code","0bc3e78b":"markdown","747f4f6c":"markdown","e0178995":"markdown","fd29f66b":"markdown","83fae4db":"markdown","c5dfa68f":"markdown","1bbb9dce":"markdown","18d84f58":"markdown","cd464bfc":"markdown","174ff22f":"markdown","96d8c499":"markdown","c1477111":"markdown","26223870":"markdown","56a6f6ed":"markdown","4f059d6e":"markdown","a2eb9b45":"markdown","4981623a":"markdown","1b0d9807":"markdown","986b0dcd":"markdown","86574a1d":"markdown","0e31337e":"markdown"},"source":{"07da958e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b71be25c":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n\n#Import models for classification task\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import precision_score,recall_score, accuracy_score,confusion_matrix, plot_confusion_matrix, classification_report, auc,roc_auc_score\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\n\nimport xgboost\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Activation, Dense, BatchNormalization, Dropout\nfrom tensorflow.keras import optimizers\n\nprint(f\"Tensorflow Version: {tf.version.VERSION}\")\n\n\nimport missingno\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","188664f8":"train = pd.read_csv(\"..\/input\/income-adult\/adult_data.csv\")\ntest = pd.read_csv(\"..\/input\/income-adult\/adult_test.csv\")","a1915593":"train.head()","7630ded4":"test.head()","1fa56996":"train.columns","606c12da":"#Column names have spaces on either end, so remove them\nnew_cols = [col.strip() for col in train.columns]\ntrain.columns = new_cols\ntest.columns = new_cols","40cef046":"train.columns","959f5f19":"#Separate training and test set into features and target\n\nX_train = train.drop([\"salary\"], axis = 1)\ny_train = train[[\"salary\"]]\n\nX_test = test.drop([\"salary\"], axis = 1)\ny_test = test[[\"salary\"]]","272dcdc6":"X_train.shape, y_train.shape","d674a004":"X_test.shape, y_test.shape","e3524e7d":"X_train.dtypes","3f52dc0a":"#separate numerical and categorical variables\nnum_cols = X_train.select_dtypes(include = \"int64\")\ncat_cols = X_train.select_dtypes(include = \"object\")\n\nnum_cols.head()","d77f722e":"cat_cols.head()","009f020e":"sns.countplot(train[\"salary\"])\nplt.title(\"Count of variable to predict\")\nplt.show()","c144f3b9":"print(f\"There are {len(cat_cols.columns)} categorical variables in the training set.\")","8fbfd5f3":"#Plot count plots for all categorical variables\nplt.figure(figsize = (23,15))\n\nfor i,var in enumerate(cat_cols.columns):\n    plt.subplot(4,2,i+1)\n    sns.countplot(X_train[var])\nplt.subplots_adjust(hspace = 0.4)\nplt.show()\n    ","bf9ff4a0":"print(f\"There are {len(num_cols.columns)} numerical variables in the training set.\")","f78cd2b8":"plt.figure(figsize= (23,12))\nfor i,var in enumerate(num_cols.columns):\n    plt.subplot(2,3,i + 1)\n    sns.histplot(X_train[var])\nplt.show()","794c0d6c":"\nprint(X_train.isna().mean())\nprint(X_train.isnull().mean())\nmissingno.matrix(X_train, figsize = (10,10))\nplt.show()","32c14e27":"print(X_test.isna().mean())\nprint(X_test.isnull().mean())\n\nmissingno.matrix(X_test, figsize = (10,10))\nplt.show()","1959af9f":"X_train.head()","4ea7201f":"#Replace <= 50k with 0 and >50k with 1 for modelling\n\n#print(y_train[\"salary\"].unique(), y_test[\"salary\"].unique())\ny_train = y_train.replace({y_train[\"salary\"].unique()[0]: 0, y_train[\"salary\"].unique()[1] : 1})\ny_test = y_test.replace({y_test[\"salary\"].unique()[0]: 0, y_test[\"salary\"].unique()[1] : 1})","298320c1":"print(f\"The shape of X_train is {X_train.shape}.\")\nprint(f\"The shape of X_test is {X_test.shape}.\")\n\nX_train_len = len(X_train)\nX_test_len = len(X_test)","b1c652f1":"X = pd.concat([X_train,X_test], axis = 0)\nprint(f\"The shape of X is {X.shape}.\")\n\nX.head()","3e8f54e4":"#One Hot Encoding\nX_new = pd.get_dummies(X, columns = cat_cols.columns)\nprint(X_new.shape)\nX_new.head()","a070e123":"#Recreate X_train and X_test with one hot encoded features\nX_train = X_new.iloc[0:X_train_len]\nX_test = X_new.iloc[X_train_len:]\n\nprint(f\"The shape of X_train is {X_train.shape}.\")\nprint(f\"The shape of X_test is {X_test.shape}.\")","2bf0121a":"X_train.head()","35cc6c31":"#Split into validation\/test set\n\n\nX_train, X_val, y_train, y_val = train_test_split(X_train,y_train, test_size = 0.3, random_state = 1, stratify = y_train)","c953f2d2":"print(f\"The shape of X_train is {X_train.shape}.\")\nprint(f\"The shape of X_val is {X_val.shape}.\")\nprint(f\"The shape of X_test is {X_test.shape}.\")\n\nprint(f\"The shape of y_train is {y_train.shape}.\")\nprint(f\"The shape of y_val is {y_val.shape}.\")\nprint(f\"The shape of y_test is {y_test.shape}.\")","4e72fc06":"plt.subplot(1,3,1)\nsns.countplot(y_train[\"salary\"])\nplt.title(\"Train Data\")\n\nplt.subplot(1,3,2)\nsns.countplot(y_val[\"salary\"])\nplt.title(\"Validation Data\")\n\nplt.subplot(1,3,3)\nsns.countplot(y_test[\"salary\"])\nplt.title(\"Test Data\")\nplt.subplots_adjust(wspace = 2)\nplt.show()","98ba286c":"sc = StandardScaler()\n\nX_train[num_cols.columns] =  sc.fit_transform(X_train[num_cols.columns])\nX_val[num_cols.columns] =  sc.transform(X_val[num_cols.columns])\nX_test[num_cols.columns] =  sc.transform(X_test[num_cols.columns])\n\nX_test.head()","06a4ecbb":"y_test[\"salary\"].value_counts()","007c0bef":"log_reg = LogisticRegression()\n\n\nlog_reg.fit(X_train,y_train)\n\nprint(f\"Accuracy on the training set is {log_reg.score(X_train,y_train)}.\")\nprint(f\"Accuracy on the validation set is {log_reg.score(X_val,y_val)}.\")\nprint(f\"Accuracy on the test set is {log_reg.score(X_test,y_test)}.\")","2bf1ab15":"params = {\"penalty\" : [\"l1\", \"l2\", \"elasticnet\"],\n         \"C\": [0.01, 0.05, 0.1, 0.5, 1, 2]}\n\ngrid_cv = GridSearchCV(LogisticRegression(n_jobs = -1),params)\n\ngrid_cv.fit(X_train, y_train)","253afcb6":"print(grid_cv.best_params_)\n#Best params are the default values so no need for a new model","a93a3a78":"log_preds = log_reg.predict(X_test)\nlog_cm = confusion_matrix(y_test,log_preds)\nprint(log_cm)\n","d4b9d796":"log_precision =precision_score(y_test, log_preds)\nlog_recall=recall_score(y_test, log_preds)\nlog_accuracy = accuracy_score(y_test, log_preds)\n\nprint(\"Precision = {}\".format(log_precision))\nprint(\"Recall = {}\".format(log_recall))\nprint(\"Accuracy = {}\".format(log_accuracy))\n\nprint(\"Area under the curve: {}.\".format(roc_auc_score(y_test,log_reg.decision_function(X_test)))) ","f85d21c9":"knn = KNeighborsClassifier(n_jobs = -1)\nknn.fit(X_train,y_train)\n\nprint(f\"Accuracy on the training set is {knn.score(X_train,y_train)}.\")\nprint(f\"Accuracy on the validation set is {knn.score(X_val,y_val)}.\")\nprint(f\"Accuracy on the test set is {knn.score(X_test,y_test)}.\")","0d06837f":"validation_accuracies = []\n\nfor i in range(5,50,5):\n    knn = KNeighborsClassifier(n_jobs = -1,n_neighbors = i )\n    knn.fit(X_train,y_train)\n    validation_accuracies.append(knn.score(X_val, y_val))\n    ","8963242a":"plt.plot(range(5,50,5), validation_accuracies)\nplt.title(\"Validation accuracies for different k values\")\nplt.xlabel(\"k values\")\nplt.ylabel(\"Accuracy\")\nplt.show()","4de7cc1c":"k_values = range(5,50,5)\n\nprint(f\"The best k-value is: {k_values[validation_accuracies.index(max(validation_accuracies))]}.\")","71cbd70b":"knn_best = KNeighborsClassifier(n_jobs = -1,n_neighbors = 20 )\nknn_best.fit(X_train,y_train)","be7bb789":"knn_preds = knn_best.predict(X_test)\nknn_cm = confusion_matrix(y_test,knn_preds)\nprint(knn_cm)\n\nknn_precision =precision_score(y_test, knn_preds)\nknn_recall = recall_score(y_test, knn_preds)\nknn_accuracy = accuracy_score(y_test, knn_preds)\n\nprint(\"Precision = {}\".format(knn_precision))\nprint(\"Recall = {}\".format(knn_recall))\nprint(\"Accuracy = {}\".format(knn_accuracy))\n","7a3101f0":"xgb = xgboost.XGBClassifier(nthread = -1)\nxgb.fit(X_train,y_train)","2161e174":"print(f\"Accuracy on the training set is {xgb.score(X_train,y_train)}.\")\nprint(f\"Accuracy on the validation set is {xgb.score(X_val,y_val)}.\")\nprint(f\"Accuracy on the test set is {xgb.score(X_test,y_test)}.\")","1136c70f":"parameters = {\n     \"eta\"    : [0.05, 0.15,0.30 ] ,\n     \"n_estimators\" : [ 50,100,200],\n     #\"min_child_weight\" : [ 1, 5, 7 ],\n     #\"gamma\"            : [ 0.0, 0.2 , 0.4 ],\n     #\"colsample_bytree\" : [ 0.3, 0.5 , 0.7 ],\n    \"learning_rate\":[0.001,0.01,0.1]\n     }\n\nrandom_cv = RandomizedSearchCV(xgboost.XGBClassifier(nthreads = -1),\n                    parameters, n_jobs=-1,\n                    cv=3, random_state = 0)\n\nrandom_cv.fit(X_train, y_train)","e50c917b":"random_cv.best_params_","803f8b12":"xgb_best = xgboost.XGBClassifier(nthreads = -1,n_estimators = 200, learning_rate = 0.1, eta = 0.15 )\nxgb_best.fit(X_train,y_train)","437d62b1":"xgb_preds = xgb_best.predict(X_test)\nxgb_cm = confusion_matrix(y_test,xgb_preds)\nprint(xgb_cm)\n\nxgb_precision =precision_score(y_test, xgb_preds)\nxgb_recall = recall_score(y_test, xgb_preds)\nxgb_accuracy = accuracy_score(y_test, xgb_preds)\n\nprint(\"Precision = {}\".format(xgb_precision))\nprint(\"Recall = {}\".format(xgb_recall))\nprint(\"Accuracy = {}\".format(xgb_accuracy))\n    ","7e69763d":"rf = RandomForestClassifier(n_jobs = -1, random_state = 0)\nrf.fit(X_train, y_train)","b283118e":"print(f\"Accuracy on the training set is {rf.score(X_train,y_train)}.\")\nprint(f\"Accuracy on the validation set is {rf.score(X_val,y_val)}.\")\nprint(f\"Accuracy on the test set is {rf.score(X_test,y_test)}.\")","7edf7743":"params = {\"n_estimators\":range(10,400,50),\n         \"max_depth\":range(5,100,10),\n          \"criterion\":[\"gini\", \"entropy\"],\n          \"min_samples_split\":range(2,10,2)\n         }\n\nrandom_cv = RandomizedSearchCV(RandomForestClassifier(), params, n_jobs = -1, random_state = 1)\n\nrandom_cv.fit(X_train,y_train)","cb92f134":"random_cv.best_params_","b1327749":"validation_accuracies = []\n\nfor i in range(160,300,10):\n    rf = RandomForestClassifier(n_jobs = -1,n_estimators = i , random_state = 0)\n    rf.fit(X_train,y_train)\n    validation_accuracies.append(rf.score(X_val, y_val))","bc5668be":"plt.plot(range(160,300,10), validation_accuracies)\nplt.title(\"Validation accuracies for different number of estimators\")\nplt.xlabel(\"number of estimators\")\nplt.ylabel(\"Accuracy\")\nplt.show()","b914de66":"n_estimators = range(160,300,10)\n\nprint(f\"The best number of estimators is: {n_estimators[validation_accuracies.index(max(validation_accuracies))]}.\")","00af6743":"rf_best = RandomForestClassifier(min_samples_split = 6, max_depth = 25, criterion = 'entropy', n_estimators = 200)\nrf_best.fit(X_train,y_train)","0d60cc31":"rf_preds = rf_best.predict(X_test)\nrf_cm = confusion_matrix(y_test,rf_preds)\nprint(rf_cm)\n\nrf_precision =precision_score(y_test, rf_preds)\nrf_recall = recall_score(y_test, rf_preds)\nrf_accuracy = accuracy_score(y_test, rf_preds)\n\nprint(\"Precision = {}\".format(rf_precision))\nprint(\"Recall = {}\".format(rf_recall))\nprint(\"Accuracy = {}\".format(rf_accuracy))","737b9fff":"def build_nn_model(metric = \"accuracy\", learning_rate = 0.01):\n    \n    model = Sequential()\n    \n    model.add(Dense(32, input_shape = (X_train.shape[1],)))\n    model.add(Dense(64, Activation(\"relu\")))\n    model.add(Dense(128, Activation(\"relu\")))\n    model.add(Dense(128, Activation(\"relu\")))\n    model.add(Dense(1))\n    \n    learning_rate = learning_rate\n    optimizer = optimizers.Adam(learning_rate)\n    model.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits = True),\n                 optimizer = optimizer,\n                 metrics = [metric])\n    \n    return model\n\nmodel = build_nn_model(metric = \"binary_accuracy\")\nmodel.summary()","ecbb84d6":"EPOCHS = 30\nbatch_size = 16\n\n\nhistory = model.fit(\n    X_train,\n    y_train,\n    batch_size = batch_size, \n    epochs = EPOCHS,\n    verbose = 1,\n    validation_data = (X_val,y_val))","4a8686bb":"score = model.evaluate(X_test, y_test)\nscore","fe61a59e":"hist = pd.DataFrame(history.history)\nhist[\"epoch\"] = history.epoch\nhist.head()","4979d39f":"plt.figure(figsize = (12,8))\nplt.plot(history.history[\"binary_accuracy\"])\nplt.plot(history.history[\"val_binary_accuracy\"])\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.legend([\"Training Accuracy\",\"Validation Accuracy\"])\nplt.show()","c3a6c896":"nn_preds = model.predict(X_test)\nnn_preds = (nn_preds > 0.5)\n\nnn_precision =precision_score(y_test, nn_preds)\nnn_recall = recall_score(y_test, nn_preds)\nnn_accuracy = accuracy_score(y_test, nn_preds)\n\nprint(\"Precision = {}\".format(nn_precision))\nprint(\"Recall = {}\".format(nn_recall))\nprint(\"Accuracy = {}\".format(nn_accuracy))","d8791206":"#Logistic\nlog_results = [\"Logistic Regression\",log_precision, log_recall, log_accuracy]\n\n#Knn\nknn_results = [\"K-Nearest Neighbours\",knn_precision, knn_recall, knn_accuracy]\n\n#Random Forest\nrf_results = [\"Random Forest\",rf_precision, rf_recall, rf_accuracy]\n\n#XGBoost\nxgb_results = [\"XGBoost\",xgb_precision, xgb_recall, xgb_accuracy]\n\n#Neural Network\nnn_results = [\"Neural Network\",nn_precision, nn_recall, nn_accuracy]\n\n\nall_results = pd.DataFrame([log_results, knn_results, rf_results, xgb_results, nn_results],columns = [\"Model\",\"Precision\", \"Recall\", \"Accuracy\"])\n\nall_results","369abff5":"fig = plt.figure(figsize = (10,10))\nall_results.plot(x = \"Model\", y = [\"Precision\", \"Recall\", \"Accuracy\"], kind = \"bar\")\nplt.legend(loc = \"upper right\", bbox_to_anchor=(1.3, 1))\nplt.show()","92827950":"#PLot Cm\nplt.figure(figsize = (6,5))\nax = plt.subplot()\nsns.heatmap(xgb_cm, ax=ax, annot = True)\nax.set_ylabel(\"True Labels\")\nax.set_xlabel(\"Predicted Labels\")\nax.set_title(\"Confusion Matrix for XGBoost Classifier\")\nplt.show()","0bc3e78b":"# Data Cleaning\n## Check for missing values and null values","747f4f6c":"### Neural Network","e0178995":"### Logistic Regression","fd29f66b":"### Test set","83fae4db":"### K-nearest Neighbors","c5dfa68f":"The random forest is clearly overfitting","1bbb9dce":"### Training set","18d84f58":"### XGBoost","cd464bfc":"## Comparing Models","174ff22f":"## Plot features for better understanding","96d8c499":"Grid Search for tuning","c1477111":"Create a validation set from the training data for model fine tuning","26223870":"### One Hot Encoding\n\nCombine test and training set to one hot encode ensuring all variables are taken into account","56a6f6ed":"### Clean label column","4f059d6e":"Manually calculate the best k-value using the validation set","a2eb9b45":"## Import Data","4981623a":"## Modelling","1b0d9807":"Separate training and test set the same way it was previously","986b0dcd":"### Random Forest","86574a1d":"Ensure the data is stratified properly after split as the dataset is unbalanced","0e31337e":"Scale the data "}}