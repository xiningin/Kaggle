{"cell_type":{"68ab0a4b":"code","5702fa97":"code","e16b3c18":"code","235e399d":"code","fea771e9":"code","aa6bf033":"code","2c8a4d1d":"code","84a4b2cf":"code","6acb2ff8":"code","aad094a1":"code","e9f79f22":"code","6e84a057":"code","c24de77a":"code","1f6bba3e":"code","7e3dfeed":"markdown","72eb31bc":"markdown","e214dccb":"markdown","65f54ff7":"markdown","ebd61820":"markdown","59db55dd":"markdown","13192ca4":"markdown","c577015f":"markdown","203ef36a":"markdown"},"source":{"68ab0a4b":"import numpy as np\nimport pandas as pd\nimport seaborn as sns \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, classification_report\nimport pandas_profiling as pp","5702fa97":"\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e16b3c18":"df = pd.read_csv(\"..\/input\/health-care-data-set-on-heart-attack-possibility\/heart.csv\")","235e399d":"df.isnull().sum()\ndf.info()\n\npp.ProfileReport(df)","fea771e9":"y = df['target']\nX = df.drop('target', axis=1)","aa6bf033":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)","2c8a4d1d":"# importing libraries\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\n\n\nclfs = {\n    'LogisticR': LogisticRegression(),\n    'SGD': SGDClassifier(penalty='elasticnet', alpha=0.005),\n    'Random Forest': RandomForestClassifier(n_estimators=1000),\n    'SVC': LinearSVC(C=1, loss='hinge', max_iter=10000),\n    'KNN': KNeighborsClassifier(n_neighbors=3),\n    'GNB': GaussianNB()\n}\n\n# Training & Testing loop\n\nfor i, (name, clf) in enumerate(clfs.items()):\n    if name == 'LogisticR':\n        log = pd.Series(clf.fit(X_train, y_train).predict(X_test))\n    elif name == 'SGD':\n        sgd = pd.Series(clf.fit(X_train, y_train).predict(X_test))\n    elif name == \"Random Forest\":\n        randomforest = pd.Series(clf.fit(X_train, y_train).predict(X_test))\n    elif name == \"SVC\":\n        svc = pd.Series(clf.fit(X_train, y_train).predict(X_test))\n    elif name == \"GNB\":\n        gnb = pd.Series(clf.fit(X_train, y_train).predict(X_test))\n    elif name == \"KNN\":\n        knn = pd.Series(clf.fit(X_train, y_train).predict(X_test))","84a4b2cf":"preds = pd.concat([log, sgd, randomforest, svc, gnb, knn], axis=1, keys=['log','sgd','rf','svc','gnb','knn'])","6acb2ff8":"from sklearn.metrics import f1_score\nfor i in preds.columns:\n    print('F1_score of %s model is %f' % (i,f1_score(y_test,preds[i])))","aad094a1":"from sklearn.preprocessing import PolynomialFeatures\npoly_features = PolynomialFeatures(degree=2)\n\nX_train_poly = poly_features.fit_transform(X_train)\nX_test_poly = poly_features.fit_transform(X_test)\n\ncross_val_score(SGDClassifier(penalty='elasticnet', alpha=0.0001), X_test_poly, y_test, cv=3)","e9f79f22":"from sklearn.model_selection import learning_curve\n\ntrain_sizes, train_scores, test_scores = learning_curve(GaussianNB(), \n                                                        X, \n                                                        y,\n                                                        # Number of folds in cross-validation\n                                                        cv=10,\n                                                        # Evaluation metric\n                                                        scoring='f1',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 30))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"F1 Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","6e84a057":"from sklearn.metrics import precision_recall_curve\n\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\n\ny_scores = gnb.predict_proba(X_train)[:,1]\n#For SGDClassifier, use decision_function.\n\nprecisions, recalls, thresholds = precision_recall_curve(y_train, y_scores)\n\ndef plot_prc (precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], 'b--', label='Precision')\n    plt.plot(thresholds, recalls[:-1], 'g-', label='Recall')\n    plt.xlabel('Thresholds')\n    plt.legend(loc='center left')\n    plt.ylim([0,1])\n\nplot_prc(precisions, recalls, thresholds)","c24de77a":"from sklearn.metrics import recall_score\ny_pred = (gnb.predict_proba(X_test)[:,1] >= 0.1).astype(bool) \n\ny_pred2 = (gnb.predict_proba(X_test)[:,1] >= 0.9).astype(bool) ","1f6bba3e":"print('If we set the threshold to 0.1, then we get a recall score of %s' % recall_score(y_test, y_pred))\nprint('If we set the threshold to 0.9, then we get a recall score of %s' % recall_score(y_test, y_pred2))","7e3dfeed":"**Apparently, the model with the best performance is SGD.**\n**Let's try it with polynomial features.**","72eb31bc":"## Profiling Report","e214dccb":"## Data extraction","65f54ff7":"**We can observe, that F1 score stabilizes as the model gets more samples to train on.**","ebd61820":"## Preprocessing","59db55dd":"## Plotting learning curves","13192ca4":"**And this is how we can shift the decision boundry to play with the precision\/recall proportion.**","c577015f":"**Okay, so the CV score of polynomial features ges lower as we add new dimensions. Logically, the relationships between features are more linear-like than quadratic or cubic.**","203ef36a":"**For this task I used 5 most popular classification models. Performance estimator = f1_score.**"}}