{"cell_type":{"f5495c35":"code","3f80400c":"code","6b36b922":"code","20983e67":"code","5667b0bf":"code","3ee59ca8":"code","297016b3":"code","c45afd51":"code","d56aa787":"code","b38dc72d":"code","1c6e779b":"code","0ebb467d":"code","64f98fae":"code","41330b71":"code","d89c1678":"code","959a6b4d":"code","0557b007":"code","a6cf25e5":"code","6c60c41b":"code","6cd535cf":"code","ecbed55d":"code","2754769a":"code","5c42ee98":"code","550e6777":"code","2c53263b":"code","b7a48c66":"code","526cd040":"code","15873a61":"code","05ccbc21":"code","5b4e4df3":"code","92b9111f":"markdown","8a245e81":"markdown","79a98f11":"markdown","c8b809c3":"markdown","a452b28e":"markdown","60600a6b":"markdown","695c623c":"markdown","33702d85":"markdown","ced7947e":"markdown","e36d5b17":"markdown","6e7d432e":"markdown","c5b8c47f":"markdown","7d6b73e1":"markdown","6f12a5af":"markdown","2721b130":"markdown","6ff1b3e5":"markdown","90fa9f0e":"markdown","de9b7cd6":"markdown","afd19923":"markdown","d37ccff5":"markdown","d3efa317":"markdown","ee5627bf":"markdown","27e0b70c":"markdown","1e2306b8":"markdown","b8ad05a2":"markdown","fdffbf7f":"markdown","d74dc81b":"markdown"},"source":{"f5495c35":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n\nsns.set_theme()","3f80400c":"df = pd.read_csv(\"..\/input\/lung-cancer\/survey lung cancer.csv\")\n\ndf.head()","6b36b922":"print(\"Number of cells with null values: \" , df.isnull().sum().sum())","20983e67":"print(\"Number of cells with null values: \" , df.duplicated().sum())","5667b0bf":"df = df.drop_duplicates()","3ee59ca8":"df.describe()","297016b3":"fig, axes = plt.subplots(4, 4, figsize=(25, 15))\nfig.suptitle('Different feature distributions')\n\naxes = axes.reshape(16,)\n\nfor i,column in enumerate(df.columns):\n    sns.histplot(ax = axes[i],data = df, x= column)\n","c45afd51":"sns.histplot(data = df, x= \"AGE\",kde=True)","d56aa787":"freqs = df['LUNG_CANCER'].value_counts()\nprint(\"Ratio of NO\/YES : \", freqs[1]\/freqs[0])","b38dc72d":"encoder = LabelEncoder()\n\ndf['GENDER'] = encoder.fit_transform(df['GENDER'])\ndf['LUNG_CANCER'] = encoder.fit_transform(df['LUNG_CANCER'])","1c6e779b":"sns.heatmap(df.corr())","0ebb467d":"# This helper function was taken from https:\/\/stackoverflow.com\/a\/26883880 \n# and modified for our task. Special thanks to watsonic !\n\n\n\ndef topn(df,n):\n    npa = df.values\n    \n    npa = np.tril(npa, -1)\n    topn_ind = np.argpartition(npa,-n,None)[-n:] #flatend ind, unsorted\n    topn_ind = topn_ind[np.argsort(npa.flat[topn_ind])][::-1] #arg sort in descending order\n    cols,indx = np.unravel_index(topn_ind,npa.shape,'F') #unflatten, using column-major ordering\n    \n    return ([df.columns[c] for c in cols],[df.index[i] for i in indx])","64f98fae":"max_corr_x , max_corr_y = topn(df.corr(),6)\n\nprint(\"Correlations are the following\".center(60),end=\"\\n\\n\")\n\nfor i in range(len(max_corr_x)):\n    print(f\"\\t{max_corr_x[i]} <-> {max_corr_y[i]}\".center(50))","41330b71":"fig, axes = plt.subplots(3, 2, figsize=(25, 15))\nfig.suptitle('Relations')\n\naxes = axes.reshape(6,)\n\nfor i in range(len(max_corr_x)):\n    sns.scatterplot(ax = axes[i],data = df, x= max_corr_x[i],y=max_corr_y[i])","d89c1678":"col_num = len(df.columns)\nfig, axes = plt.subplots(12, 10, figsize=(55, 55))\nfig.suptitle('All Relations')\n\naxes = axes.reshape(120,)\n\ncnt = 0\nfor i in range(col_num):\n    for j in range(i+1,col_num):\n        sns.scatterplot(ax = axes[cnt],data = df, x= df.columns[i],y=df.columns[j])\n        \n        cnt += 1","959a6b4d":"df = df.sort_values(by='LUNG_CANCER')","0557b007":"no_df = df.iloc[:freqs[1]]\nyes_df = pd.concat([df, no_df,no_df]).drop_duplicates(keep=False)","a6cf25e5":"random_yes_df = yes_df.sample(n = 2*freqs[1])\ntrain_df = pd.concat([random_yes_df,no_df])\ntest_df = pd.concat([yes_df, random_yes_df,random_yes_df]).drop_duplicates(keep=False)","6c60c41b":"xtr = np.array(train_df.drop(columns = ['LUNG_CANCER'])) # X train\nytr = np.array(train_df['LUNG_CANCER']) # y train\n\nxte = np.array(test_df.drop(columns = ['LUNG_CANCER'])) # X test\nyte = np.array(test_df['LUNG_CANCER']) # y test","6cd535cf":"scaler = MinMaxScaler(copy=True, feature_range=(0, 1))\n\nxtr = scaler.fit_transform(xtr)\nxte = scaler.fit_transform(xte)","ecbed55d":"print(xtr.shape)\nprint(ytr.shape)\nprint()\nprint(xte.shape)\nprint(yte.shape)","2754769a":"model1 = DecisionTreeClassifier()\nmodel1.fit(xtr,ytr)\n\npredictions = model1.predict(xte)\nconf_matrix = metrics.confusion_matrix(yte, predictions)\n\nsns.heatmap(conf_matrix,annot=True,cmap='Blues',fmt='g',cbar=False)","5c42ee98":"model2 = RandomForestClassifier(n_estimators = 500,max_depth = 25,criterion=\"entropy\")\nmodel2.fit(xtr,ytr)\n\npredictions = model2.predict(xte)\nconf_matrix = metrics.confusion_matrix(yte, predictions)\n\nsns.heatmap(conf_matrix,annot=True,cmap='Blues',fmt='g',cbar=False)","550e6777":"model3 = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss',learning_rate=0.01)\nmodel3.fit(xtr,ytr)\n\npredictions = model3.predict(xte)\nconf_matrix = metrics.confusion_matrix(yte, predictions)\n\nsns.heatmap(conf_matrix,annot=True,cmap='Blues',fmt='g',cbar=False)","2c53263b":"model4 = LogisticRegression()\nmodel4.fit(xtr,ytr)\n\n\npredictions = model4.predict(xte)\nconf_matrix = metrics.confusion_matrix(yte, predictions)\n\nsns.heatmap(conf_matrix,annot=True,cmap='Blues',fmt='g',cbar=False)","b7a48c66":"model5 = KNeighborsClassifier(n_neighbors=2)\nmodel5.fit(xtr,ytr)\n\npredictions = model5.predict(xte)\nconf_matrix = metrics.confusion_matrix(yte, predictions)\n\nsns.heatmap(conf_matrix,annot=True,cmap='Blues',fmt='g',cbar=False)","526cd040":"model6 = SVC(kernel = 'rbf',gamma=1)\nmodel6.fit(xtr, ytr)\n\npredictions = model6.predict(xte)\nconf_matrix = metrics.confusion_matrix(yte, predictions)\n\nsns.heatmap(conf_matrix,annot=True,cmap='Blues',fmt='g',cbar=False)","15873a61":"optim = Adam(\n    learning_rate=0.0008,\n)\n\nmodel7 = Sequential()\n\nmodel7.add(Dense(32, input_shape=(xtr.shape[1],), activation='relu',kernel_initializer='normal'))\nmodel7.add(Dense(21, activation='relu',kernel_initializer='normal'))\nmodel7.add(Dense(14, activation='relu',kernel_initializer='normal'))\n\n\nmodel7.add(Dense(1, activation='sigmoid'))\nmodel7.summary()\n\nes = EarlyStopping(monitor='val_accuracy', mode='max',patience=5,restore_best_weights=False)\n\n\nmodel7.compile(optimizer=optim, loss='binary_crossentropy',metrics='accuracy')","05ccbc21":"history = model7.fit(xtr,ytr,callbacks = [es],epochs=100,batch_size=1,validation_data = (xte,yte),shuffle=True)","5b4e4df3":"predictions = (model7.predict(xte) > 0.5).astype('int32').reshape(xte.shape[0],)\nconf_matrix = metrics.confusion_matrix(yte, predictions)\n\nsns.heatmap(conf_matrix,annot=True,cmap='Blues',fmt='g',cbar=False)","92b9111f":"### Statistical description","8a245e81":"<p style=\"font-size: 120%\"> Now , our purpose is to find features with max correlation (except diagonal ones ). Let's code that. <\/p>","79a98f11":"### Logistic Regression","c8b809c3":"## Artificial Neural Networks ","a452b28e":"### Distributions","60600a6b":"### Decision Tree Classifier","695c623c":"# Building models","33702d85":"## Other standart algorithms","ced7947e":"# Conclusion","e36d5b17":"As we can see, there are several duplicate values, let's drop them","6e7d432e":"### Support Vector Machine","c5b8c47f":"<p style=\"font-size: 120%\"> As we already mentioned, our data is near to severe imbalance, so we will do the following:  <br>\n1) Extract all records with NO for LUNG_CANCER <br>\n2) Pick random records with YES for LUNG_CANCER <br>\n3) Split collected data into train and test <br> \n4) Pass to model<\/p>","7d6b73e1":"### Random Forest Classifier","6f12a5af":"### Correlation matrix and additional plots","2721b130":"<p style=\"font-size: 120%\"> Mapping is the following: <br><br>\n    For GENDER <br>\n    1 - M (Male) <br>\n    0 - F (Female)<br><br>For LUNG_CANCER <br>\n    1 - YES <br>\n    0 - NO  <br><\/p>","6ff1b3e5":"### XGBoost","90fa9f0e":"# A bit of analysis","de9b7cd6":"<p style=\"font-size: 120%\"> For sake of simplicity we will take top 6 relations and visualize them. <\/p>","afd19923":"<p style=\"font-size: 120%\"> As we can see result is far from 1, hence we have to consider this fact during training process. <\/p>","d37ccff5":"### KNN","d3efa317":"# Reading data","ee5627bf":"Not that much impressed tbh. Let's have a look at all of them.","27e0b70c":"<p style=\"font-size: 120%\"> On the distribution plot of LUNG_CANCER we can see that it is imbalanced. Let's calculate ratio. <\/p>","1e2306b8":"## Tree-based models","b8ad05a2":"<p style=\"font-size: 120%\"> In conclusion, since, unfortunately, amount of data is low, we coundn't do that much. Additionally, problem with imbalanced data decreased number of training samples what might lead to inconsistent results. I am pretty sure with more data we could achieve better results and see clear picture of what is happening <\/p>","fdffbf7f":"# Cleaning data","d74dc81b":"# Preparing data for model training"}}