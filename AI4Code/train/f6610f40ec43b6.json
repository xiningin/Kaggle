{"cell_type":{"837c4844":"code","c65efd04":"code","af872c0c":"code","c0e8e320":"code","023c95e0":"code","1ddd163c":"code","19751d4f":"code","c8b65dfe":"code","d6529fa7":"code","bd83c791":"code","667b4644":"code","5bd4be71":"code","62ac8b4a":"code","7932e706":"code","ce90558e":"code","d2ae5905":"code","6162da75":"code","c39a7915":"code","8449035a":"code","a9f1273b":"code","ae47dfe0":"code","6e747d01":"code","cc5d7f0b":"code","42d14bf0":"code","1550bfcc":"code","1dcc70e9":"code","6d2e73fb":"code","562df818":"markdown","55827d6a":"markdown","75bbb7cd":"markdown","4c4c5874":"markdown","be2da463":"markdown","2fea6204":"markdown"},"source":{"837c4844":"import os\n\nimport numpy as np\n\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.autograd as autograd\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom torchvision.datasets import CIFAR10\n\nimport matplotlib.pyplot as plt\n\nfrom datetime import datetime\n\nfrom tqdm.notebook import tqdm","c65efd04":"MODEL_STORAGE_FOLDER = '.\/checkpoints'\n\n# Explicit seed picking for the shake of reproducibility\nseed = 42\nrandom.seed(seed)\ntorch.manual_seed(seed)\n\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False","af872c0c":"def cuda(x):\n    if torch.cuda.is_available():\n        x = x.cuda()\n\n    return x","c0e8e320":"def normalize_real_images(real_images):\n    min_value = real_images.min()\n    max_value = real_images.max()\n\n    normalized_images = 2 * (real_images - min_value) \/ (max_value - min_value) - 1\n\n    return normalized_images","023c95e0":"# Data preprocessing to set the image values from range [0, 1] to [-1, 1]\npreprocessing_ops = transforms.Compose([transforms.ToTensor()])\n\n# Dataset download\ntrain_dataset = CIFAR10(root=\".\/data\", train=True, transform=preprocessing_ops, download=True)\ntest_dataset = CIFAR10(root=\"..\/data\", train=False, transform=preprocessing_ops, download=True)\n\n# Class label of class to use for training (5 = dogs)\nclass_label = 5\n\n# Obtain indices of given class instances\ntrain_idx = np.where(np.array(train_dataset.targets) == class_label)\ntest_idx = np.where(np.array(test_dataset.targets) == class_label)\n\n# Retrieve class instances\ntrain_data = train_dataset.data[train_idx]\ntest_data = test_dataset.data[test_idx]\n\ndata = np.concatenate((train_data, test_data))\n\n# Final dataset\nreal_images = torch.from_numpy(np.transpose(data, (0,3,1,2))).float()\nreal_images = cuda(normalize_real_images(real_images))\n\nprint(\"\\nReal image dataset preprocessed and readied, consisting in {} images\".format(len(real_images)))","1ddd163c":"def image_visualization_pretreatment(img):\n    min = img.min()\n    max = img.max()\n\n    if not (min == 0 and img.max == 0):\n        img -= min\n        img \/= max - min\n\n    return img","19751d4f":"def visualize_batch(batch, save=False, epoch=0):\n    batch = image_visualization_pretreatment(batch)\n    plt.figure(figsize=(10,10))\n    plt.axis(\"off\")\n    plt.imshow(np.transpose(vutils.make_grid(batch, padding=2, normalize=False), (1,2,0)))\n\n    if save:\n        if not os.path.exists('.\/output\/synth_imgs'):\n            os.makedirs('.\/output\/synth_imgs')\n            \n        plt.savefig(\"output\/synth_imgs\/{} : epoch {}.png\".format(datetime.now().strftime(\"%H:%M:%S\"), epoch))\n\n    plt.show()\n\n    ","c8b65dfe":"# Real images exploration\nvisualization_batch = real_images[:64]\n\nvisualize_batch(visualization_batch.cpu())","d6529fa7":"#############################################\n# Archichecture parameters\n\ngen_filters = (1024, 512, 256)\ndisc_filters = (256, 512, 1024)\n\nleaky_relu_alpha = 0.2\n\ninput_latent_vector_size = 100\n\n#############################################\n# Training parameters\n\nbatch_size = 64\nepochs = 1000\n\nlr = 1e-4\nopt_betas = (0.5, 0.999)\n\nimg_size = 32\nimg_channels = 3\n\nreal_label = 1\nsynth_label = -1\n\ngenerator_overtrain = 1\ncritic_overtrain = 5\n\ngradient_penalty_lambda = 10\n\n#############################################\n# Storage and visualization parameters\ncheck_interval = 50\nstorage_interval = 100","bd83c791":"class Generator(nn.Module):\n    def __init__(self, filter_sizes, leaky_relu_alpha, input_lantent_vector_size, bias=True, bnorm_affine=True):\n        super(Generator, self).__init__()\n      \n        # Network architecture\n        # Input Tconv | out:[512 x 512 x 1024]\n        self.input = nn.Sequential(\n            nn.ConvTranspose2d(in_channels=input_lantent_vector_size, out_channels=filter_sizes[0], kernel_size=4, stride=1, padding=0, bias=bias),\n            nn.BatchNorm2d(num_features=filter_sizes[0], affine=bnorm_affine),\n            nn.ReLU(inplace=True))\n        \n        # Hidden Tconv 1 | out:[256 x 256 x 512]\n        self.hidden_tconv_1 = nn.Sequential(\n            nn.ConvTranspose2d(in_channels=filter_sizes[0], out_channels=filter_sizes[1], kernel_size=4, stride=2, padding=1, bias=bias),\n            nn.BatchNorm2d(num_features=filter_sizes[1], affine=bnorm_affine),\n            nn.ReLU(inplace=True))\n\n        # Input Tconv 2 | out:[128 x 128 x 256]\n        self.hidden_tconv_2 = nn.Sequential(\n            nn.ConvTranspose2d(in_channels=filter_sizes[1], out_channels=filter_sizes[2], kernel_size=4, stride=2, padding=1, bias=bias),\n            nn.BatchNorm2d(num_features=filter_sizes[2], affine=bnorm_affine),\n            nn.ReLU(inplace=True))\n        \n        # Output Tconv | out:[32 x 32 x 64]\n        self.output = nn.Sequential(\n            nn.ConvTranspose2d(in_channels=filter_sizes[2], out_channels=3, kernel_size=4, stride=2, padding=1, bias=bias),\n            nn.Tanh())\n        \n        self.input_lantent_vector_size = input_lantent_vector_size\n\n    def forward(self, x):\n        features = self.input(x)\n        \n        features = self.hidden_tconv_1(features)\n        features = self.hidden_tconv_2(features)\n\n        output = self.output(features)\n\n        return output\n\n","667b4644":"class Critic(nn.Module):\n    def __init__(self, filter_sizes, leaky_relu_alpha, bias=True):\n        super(Critic, self).__init__()\n\n        # Network architecture\n        # Input conv | out:[16 x 16 x 128]\n        self.input = nn.Sequential (\n            nn.Conv2d(in_channels=img_channels, out_channels=filter_sizes[0], kernel_size=4, stride=2, padding=1, bias=bias),\n            nn.LeakyReLU(leaky_relu_alpha, inplace=True))\n        \n        # Hidden conv 1 | out:[8 x 8 x 256]\n        self.hidden_conv_1 = nn.Sequential(\n            nn.Conv2d(in_channels=filter_sizes[0], out_channels=filter_sizes[1], kernel_size=4, stride=2, padding=1, bias=bias),\n            nn.LeakyReLU(leaky_relu_alpha, inplace=True))\n        \n        # Hidden conv 2  | out:[4 x 4 x 512]\n        self.hidden_conv_2 = nn.Sequential(\n            nn.Conv2d(in_channels=filter_sizes[1], out_channels=filter_sizes[2], kernel_size=4, stride=2, padding=1, bias=bias),\n            nn.LeakyReLU(leaky_relu_alpha, inplace=True))\n        \n        # Out conv | out:[1 x 1 x 1]\n        self.output = nn.Sequential(\n            nn.Conv2d(in_channels=filter_sizes[2], out_channels=1, kernel_size=4, stride=1, padding=0, bias=bias))\n\n\n\n    def forward(self, x):\n        features = self.input(x)\n\n        features = self.hidden_conv_1(features)\n        features = self.hidden_conv_2(features)\n\n        output = self.output(features)\n\n        return output\n        ","5bd4be71":"# Custom weights initialization\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv2d') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm2d') != -1 and m.weight is not None:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)","62ac8b4a":"generator = Generator(gen_filters, leaky_relu_alpha=leaky_relu_alpha, input_lantent_vector_size=100)\ncritic = Critic(disc_filters, leaky_relu_alpha=leaky_relu_alpha)\n\ngenerator = cuda(generator)\ncritic = cuda(critic)\n\ngenerator_losses = []\ncritic_losses = []\n\ninitial_epoch = 0","7932e706":"print(generator)","ce90558e":"print(critic)","d2ae5905":"# Tensors to control the sign of the gradients during backward propagation\none = cuda(torch.tensor(1, dtype=torch.float))\nmone = cuda(one * -1)\n\noptimizer_generator = optim.Adam(generator.parameters(), lr=lr, betas=opt_betas)\noptimizer_critic = optim.Adam(critic.parameters(), lr=lr, betas=opt_betas)\n","6162da75":"def load_trainign_checkpoint():\n    checkpoint_path = os.path.join('..\/input\/wgan-gp-cifar10-dogs-with-pytorch\/checkpoints', 'checkpoint', 'checkpoint.pickle')\n    checkpoint = torch.load(checkpoint_path)\n    \n    generator.load_state_dict(checkpoint['generator'])\n    critic.load_state_dict(checkpoint['critic'])\n    optimizer_generator.load_state_dict(checkpoint['optimizer_generator'])\n    optimizer_critic.load_state_dict(checkpoint['optimizer_critic'])\n\n    global initial_epoch \n    global generator_losses\n    global critic_losses\n\n    initial_epoch = checkpoint['epoch']\n    generator_losses = checkpoint['generator_losses']\n    critic_losses = checkpoint['critic_losses']\n","c39a7915":"try:\n    load_trainign_checkpoint()\n    print(\"Model loaded!\",\n          \"\\nCurrent training status:\",\n          \"\\n\\tEpoch: {}\".format(initial_epoch),\n          \"\\n\\tGenerator Loss: {}\".format(generator_losses[-1]),\n          \"\\n\\tWasserstein Loss: {}\".format(critic_losses[-1]))\n    \n    epochs = initial_epoch + epochs\nexcept:\n    print(\"No model found to be loaded!\")\n    critic.apply(weights_init)\n    generator.apply(weights_init)\n","8449035a":"# Fixed input vector to check the progress of the algorithm\nfixed_input_vector = torch.randn(64, input_latent_vector_size, 1, 1)\n\nfixed_input_vector = cuda(fixed_input_vector)","a9f1273b":"def calculate_gradient_penalty(D, real_data, synth_data, batch_size, gp_lambda):\n    alpha = torch.FloatTensor(batch_size,1,1,1).uniform_(0,1)\n    alpha = alpha.expand(batch_size, real_images.size(1), real_images.size(2), real_images.size(3))\n    alpha = cuda(alpha.contiguous().view(batch_size, 3, 32, 32))\n    \n    interpolates = cuda(alpha * real_data + ((1 - alpha) * synth_data))\n    interpolates = autograd.Variable(interpolates, requires_grad=True)\n\n    critic_interpolates = D(interpolates)\n\n    grad_outputs = cuda(torch.ones(critic_interpolates.size()))\n\n    gradients = autograd.grad(outputs=critic_interpolates, inputs=interpolates,\n                              grad_outputs=grad_outputs, create_graph=True, \n                              retain_graph=True, only_inputs=True)[0]\n                              \n    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * gp_lambda\n    \n    return gradient_penalty\n    ","ae47dfe0":"def train_generator():\n    for p in critic.parameters():\n        p.requires_grad = False\n\n    # Data preparation\n    input_latent_vector = cuda(torch.randn(batch_size, input_latent_vector_size, 1, 1))\n\n    synth_batch = generator(input_latent_vector)\n    labels = torch.full((batch_size,), real_label, dtype=torch.float)\n    \n    synth_batch = cuda(synth_batch)\n    labels = cuda(labels)\n    \n    # Gradient values cleaning\n    generator.zero_grad()\n\n    # Batch processing\n    output = critic(synth_batch).view(-1)\n\n    # Loss calculation\n    loss = output.mean()\n\n    # Gradient propagation\n    loss.backward(mone)\n\n    # Weight update (Learning happening!)\n    optimizer_generator.step()\n    \n    return loss","6e747d01":"def train_critic(real_batch):\n    for p in critic.parameters():\n        p.requires_grad = True\n\n    # Data preparation\n    input_latent_vector = cuda(torch.randn(batch_size, input_latent_vector_size, 1, 1))\n\n    synth_batch = generator(input_latent_vector)\n    synth_batch = cuda(synth_batch)\n\n    real_labels = torch.full((batch_size,), real_label, dtype=torch.float)\n    real_labels = cuda(real_labels)\n    synth_labels = torch.full((batch_size,), synth_label, dtype=torch.float)\n    synth_labels = cuda(synth_labels)\n\n    # Gradient values cleaning\n    critic.zero_grad()\n\n    # Batch processing\n    output = critic(real_batch).view(-1)\n\n    # Loss calculation\n    loss_real = output.mean()\n\n    # Gradient propagation\n    loss_real.backward(mone)\n\n    # Batch processing\n    output = critic(synth_batch).view(-1)\n\n    # Loss calculation\n    loss_synth = output.mean()\n\n    # Gradient propagation\n    loss_synth.backward(one)\n    \n    # Gradient penalty term calculation\n    gradient_penalty = calculate_gradient_penalty(critic, real_batch, synth_batch, batch_size, gradient_penalty_lambda)\n    \n    # Gradient propagation\n    gradient_penalty.backward()\n \n    # Weight update (Learning happening!)\n    optimizer_critic.step()\n\n    return loss_synth - loss_real + gradient_penalty ","cc5d7f0b":"iters_per_epoch = len(real_images) \/\/ (batch_size * critic_overtrain)\n\nstart_time = datetime.now()\n\ncheckpoint_path = os.path.join(MODEL_STORAGE_FOLDER, 'checkpoint')\nos.makedirs(checkpoint_path, exist_ok=True)\n\nprint(\"Training log: Start [{}] Starting epoch: {} (-> {})\".format(start_time.strftime(\"%H:%M:%S\"), initial_epoch, epochs))\n\nfor epoch in range(initial_epoch, epochs+1):\n    epoch_idx = torch.randperm(real_images.size(0))\n    epoch_start_time = datetime.now()\n\n    for i in range(iters_per_epoch):\n\n        for j in range(critic_overtrain):\n            iter_idx = epoch_idx[(i+j)*batch_size : (i+j+1)*batch_size]\n            real_batch = real_images[iter_idx]\n\n            loss_critic = train_critic(real_batch)\n        \n        for _ in range(generator_overtrain):\n            loss_generator = train_generator()\n\n    generator_losses.append(loss_generator)\n    critic_losses.append(loss_critic)\n\n    if epoch % check_interval == 0:\n        elapsed_time =  \"\\nEpoch elapsed time: {} [Total elapsed time {}]\".format(datetime.now() - epoch_start_time, datetime.now() - start_time)\n        losses_print = \"\\nGenerator loss: {} \\nWasserstein loss: {}\".format(loss_generator, loss_critic)\n\n        print(\"Training log: Epoch {} done [{}]{}{}\".format(epoch, datetime.now().strftime(\"%H:%M:%S\"), elapsed_time, losses_print))\n\n        # Visualize status of generation\n        visualization_batch = generator(fixed_input_vector)\n        visualize_batch(visualization_batch.cpu().detach(), True, epoch)\n    \n    if epoch % storage_interval == 0:\n        # Model state storage\n        storage_path = os.path.join(checkpoint_path, \"checkpoint.pickle\".format(epoch))\n        storage_dict = {'epoch': epoch,\n                        'generator_losses': generator_losses,\n                        'critic_losses' : critic_losses,\n                        'generator': generator.state_dict(),\n                        'critic': critic.state_dict(),\n                        'optimizer_generator': optimizer_generator.state_dict(),\n                        'optimizer_critic': optimizer_critic.state_dict()}\n\n        torch.save(storage_dict, storage_path)\n        \n    initial_epoch = epoch\n\n","42d14bf0":"def calculate_conv_output_size(input_size, padding, kernel_size, stride, dilation=1):\n    output = ((input_size + 2 * padding - dilation * (kernel_size - 1) - 1) \/ stride) + 1\n\n    return output","1550bfcc":"print(calculate_conv_output_size(32, 1, 3, 1))","1dcc70e9":"def calculate_Tconv_output_size(input_size, padding, kernel_size, stride, output_padding=0, dilation=1):\n    output = (input_size - 1) * stride - 2 * padding + dilation * (kernel_size - 1) + output_padding + 1\n\n    return output","6d2e73fb":"calculate_Tconv_output_size(16, 1, 4, 2)","562df818":"## Model Architecture","55827d6a":"## Data Preprocessing","75bbb7cd":"## Visualization","4c4c5874":"# Deep Convolutional Generative Adversarial Network in Pytorch","be2da463":"---\n---\n\n## Toolkit","2fea6204":"## Imports"}}