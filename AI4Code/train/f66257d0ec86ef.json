{"cell_type":{"860bd8b3":"code","bf04fabe":"code","815059d0":"code","09235536":"code","ccbf4806":"code","56a15532":"code","022b7f1a":"code","17a518d3":"code","515d178e":"code","24b42475":"code","15643f01":"code","1ead1a67":"code","6534c0e6":"code","0ad37233":"code","ef6ddb72":"code","6bf16140":"code","eb51caea":"code","438890f5":"code","0d45c0d9":"code","a33aabd7":"code","a26a835d":"code","183623bb":"code","730ff4ec":"code","b230e3eb":"code","2d4c6ac5":"code","87478ed5":"code","48937f67":"code","db236e14":"code","cb7ad033":"code","ffc32192":"code","e85c93a7":"code","dc371971":"code","3a89b565":"code","f20975b9":"code","551e11d5":"code","5a262717":"code","9957650d":"code","f361b65e":"code","f8060462":"code","50b6f382":"code","9fc70228":"code","9b0db8f9":"code","8ec9f476":"code","27a51a2e":"code","c063d7b6":"code","056b81cf":"code","8c8b0ad3":"code","9738d80b":"code","ade43989":"code","0377e637":"code","3e28b793":"code","1b0b1508":"code","2de8c044":"code","e4267be1":"code","000a13c2":"code","f5e79789":"code","d8170e4d":"code","e390199a":"code","b6770ce6":"code","3a5dda1f":"code","7cae3de5":"code","61326f7c":"code","a90d618e":"code","6de6453c":"code","7b642cba":"code","f4013739":"code","04060b1c":"code","f29a015b":"code","c5c76f7c":"code","6a6cc194":"markdown","f00af2cd":"markdown","4bfe2d34":"markdown","431cc5ab":"markdown"},"source":{"860bd8b3":"#Now let's open it with pandas\nimport pandas as pd\nfrom pandas import Series,DataFrame\n# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n%matplotlib inline\nimport time\nimport random\n\n","bf04fabe":"# Set up the Titanic csv file as a DataFrame\ntitanic_train = pd.read_csv('..\/input\/train.csv')\ntitanic_test = pd.read_csv('..\/input\/test.csv')\ntitanic_ytest = pd.read_csv('..\/input\/gender_submission.csv')","815059d0":"titanic_train.head()","09235536":"titanic_test.head()","ccbf4806":"titanic_ytest.head()","56a15532":"\nresult = pd.merge(titanic_ytest,titanic_test, how='inner', on=['PassengerId'])\nresult=result.append(titanic_train)\nresult.head()\n\n","022b7f1a":"titanic=result","17a518d3":"titanic.head()","515d178e":"titanic.isna().any()","24b42475":"# Actual replacement of the missing value using median value.\ntitanic = titanic.fillna((titanic.median()))\ntitanic.head()","15643f01":"titanic.isna().any()","1ead1a67":"titanic.describe()","6534c0e6":"#Correlation with Quality with respect to attributes\ntitanic.corrwith(titanic.Survived).plot.bar(\n        figsize = (20, 10), title = \"Correlation with quality\", fontsize = 15,\n        rot = 45, grid = True)","0ad37233":"## Correlation Matrix\nsns.set(style=\"white\")\n\n# Compute the correlation matrix\ncorr = titanic.corr()","ef6ddb72":"corr.head()","6bf16140":"# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(18, 15))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","eb51caea":"titanic.columns","438890f5":"#Assigning and dividing the dataset\nX = titanic.drop(columns=['Survived','Embarked','Cabin','Name','Ticket'],axis=1)\ny=titanic['Survived']","0d45c0d9":"X.isna().any()","a33aabd7":"y.head()","a26a835d":"data=titanic.drop(columns=['Survived','Embarked','Cabin','Name','Ticket'],axis=1)\ndata.head()","183623bb":"#Encoding categorical data\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\n#Encoding the Dependent Variable\nlabelencoder_y = LabelEncoder()\n\nX['Sex'] = labelencoder_y.fit_transform(X['Sex'])\n\n\n","730ff4ec":"X.head()","b230e3eb":"#Encoding categorical data\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\n#Encoding the Dependent Variable\nlabelencoder_y = LabelEncoder()\n\ndata['Sex'] = labelencoder_y.fit_transform(data['Sex'])\n\ndata.head()\n","2d4c6ac5":"features_label = data.columns[1:]","87478ed5":"features_label\n","48937f67":"#Fitting Random Forest Classification to the Training set\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 200, criterion = 'entropy', random_state = 0)\nclassifier.fit(X, y)\nimportances = classifier.feature_importances_\nindices = np. argsort(importances)[::-1]\nfor i in range(X.shape[1]):\n    print (\"%2d) %-*s %f\" % (i+1, 30, features_label[i],importances[indices[i]]))","db236e14":"plt.title('Feature Importances')\nplt.bar(range(X.shape[1]),importances[indices], color=\"green\", align=\"center\")\nplt.xticks(range(X.shape[1]),features_label, rotation=90)\nplt.xlim([-1, X.shape[1]])\nplt.show()","cb7ad033":"\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection  import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 5)","ffc32192":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train2 = pd.DataFrame(sc.fit_transform(X_train))\nX_test2 = pd.DataFrame(sc.transform(X_test))\nX_train2.columns = X_train.columns.values\nX_test2.columns = X_test.columns.values\nX_train2.index = X_train.index.values\nX_test2.index = X_test.index.values\nX_train = X_train2\nX_test = X_test2","e85c93a7":"from sklearn.decomposition import PCA\npca = PCA(n_components = None)\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\nexplained_variance = pca.explained_variance_ratio_\npd.DataFrame(explained_variance)","dc371971":"#### Model Building ####\n\n### Comparing Models\n\n## Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0, penalty = 'l1')\nclassifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nresults = pd.DataFrame([['Logistic Regression', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\nprint(results)","3a89b565":"## SVM (rbf)\nfrom sklearn.svm import SVC\nclassifier = SVC(random_state = 0, kernel = 'rbf')\nclassifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['SVM (RBF)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)\nprint(results)","f20975b9":"## Randomforest\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(random_state = 0, n_estimators = 100,\n                                    criterion = 'entropy')\nclassifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Random Forest (n=100)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)\nprint(results)","551e11d5":"#we ttok the highest accuracy model svm rbf classifier","5a262717":"## K-fold Cross Validation\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X= X_train, y = y_train,\n                             cv = 100)\nprint(\"SVM Classifier Accuracy: %0.2f (+\/- %0.2f)\"  % (accuracies.mean(), accuracies.std() * 2))","9957650d":"# Round 1: SVM tuning\nparameters = [{ 'C': [1,  100],'kernel': ['linear']},\n              { 'C': [1,  100],'kernel': ['rbf'], 'gamma': [0.1, 0.2, 0.3, 0.4, 0.5]}]\n# Make sure classifier points to the RF model\nfrom sklearn.model_selection import GridSearchCV\ngrid_search = GridSearchCV(estimator = classifier, \n                           param_grid = parameters,\n                           scoring = \"accuracy\",\n                           cv = 10,\n                           n_jobs = -1)\n\nt0 = time.time()\ngrid_search = grid_search.fit(X_train, y_train)\nt1 = time.time()\nprint(\"Took %0.2f seconds\" % (t1 - t0))\n\nrf_best_accuracy = grid_search.best_score_\nrf_best_parameters = grid_search.best_params_\nrf_best_accuracy, rf_best_parameters","f361b65e":"# Predicting Test Set\ny_pred = grid_search.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['SVM (n=100, GSx2 + Gini)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)\nresults","f8060462":"import matplotlib.pyplot as plt\nimport itertools\n\nfrom sklearn import svm, datasets\nfrom sklearn.metrics import confusion_matrix\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n","50b6f382":"# Making the Confusion Matrix \nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nnp.set_printoptions(precision=2)\n\n# Plot normalized confusion matrix\nplot_confusion_matrix(cm,classes=[0,1])\nsns.set(rc={'figure.figsize':(6,6)})\nplt.show()\n","9fc70228":"#Let's see how our model performed\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","9b0db8f9":"# Let's import what we'll need for the analysis and visualization\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","8ec9f476":"# Let's first check gender\nsns.countplot('Sex',data=result)\nsns.set(rc={'figure.figsize':(6,6)})","27a51a2e":"# Now let's seperate the genders by classes, remember we can use the 'hue' arguement here!\nsns.countplot('Pclass',data=result,hue='Sex')","c063d7b6":"# We'll treat anyone as under 16 as a child, and then use the apply technique with a function to create a new column\n\n\n# First let's make a function to sort through the sex \ndef male_female_child(passenger):\n    # Take the Age and Sex\n    age,sex = passenger\n    # Compare the age, otherwise leave the sex\n    if age < 16:\n        return 'child'\n    else:\n        return sex\n    \n\n# We'll define a new column called 'person', remember to specify axis=1 for columns and not index\nresult['person'] = result[['Age','Sex']].apply(male_female_child,axis=1)","056b81cf":"# Let's see if this worked, check out the first ten rows\nresult[0:10]","8c8b0ad3":"# Let's try the factorplot again!\nsns.countplot('Pclass',data=result,hue='person')","9738d80b":"# Quick way to create a histogram using pandas\nresult['Age'].hist(bins=70)","ade43989":"# We could also get a quick overall comparison of male,female,child\nresult['person'].value_counts()","0377e637":"# Another way to visualize the data is to use FacetGrid to plot multiple kedplots on one plot\n\n# Set the figure equal to a facetgrid with the pandas dataframe as its data source, set the hue, and change the aspect ratio.\nfig = sns.FacetGrid(result, hue=\"Sex\",aspect=4)\n\n# Next use map to plot all the possible kdeplots for the 'Age' column by the hue choice\nfig.map(sns.kdeplot,'Age',shade= True)\n\n# Set the x max limit by the oldest passenger\noldest = result['Age'].max()\n\n#Since we know no one can be negative years old set the x lower limit at 0\nfig.set(xlim=(0,oldest))\n\n#Finally add a legend\nfig.add_legend()","3e28b793":"# We could have done the same thing for the 'person' column to include children:\n\nfig = sns.FacetGrid(result, hue=\"person\",aspect=4)\nfig.map(sns.kdeplot,'Age',shade= True)\noldest = result['Age'].max()\nfig.set(xlim=(0,oldest))\nfig.add_legend()","1b0b1508":"# Let's do the same for class by changing the hue argument:\nfig = sns.FacetGrid(result, hue=\"Pclass\",aspect=4)\nfig.map(sns.kdeplot,'Age',shade= True)\noldest = result['Age'].max()\nfig.set(xlim=(0,oldest))\nfig.add_legend()","2de8c044":"# Let's get a quick look at our dataset again\nresult.head()","e4267be1":"# First we'll drop the NaN values and create a new object, deck\ndeck = result['Cabin'].dropna()","000a13c2":"# Quick preview of the decks\ndeck.head()","f5e79789":"# So let's grab that letter for the deck level with a simple for loop\n\n# Set empty list\nlevels = []\n\n# Loop to grab first letter\nfor level in deck:\n    levels.append(level[0])    \n\n# Reset DataFrame and use factor plot\ncabin_df = DataFrame(levels)\ncabin_df.columns = ['Cabin']\nsns.countplot('Cabin',data=cabin_df,palette='winter_d')","d8170e4d":"# Redefine cabin_df as everything but where the row was equal to 'T'\ncabin_df = cabin_df[cabin_df.Cabin != 'T']\n#Replot\nsns.countplot('Cabin',data=cabin_df,palette='summer')","e390199a":"# Now we can make a quick factorplot to check out the results, note the x_order argument, used to deal with NaN values\nsns.countplot('Embarked',data=result,hue='Pclass',order=['C','Q','S'])","b6770ce6":"# Let's start by adding a new column to define alone\n\n# We'll add the parent\/child column with the sibsp column\nresult['Alone'] =  result.Parch + result.SibSp\nresult['Alone']","3a5dda1f":"# Look for >0 or ==0 to set alone status\nresult['Alone'].loc[result['Alone'] >0] = 'With Family'\nresult['Alone'].loc[result['Alone'] == 0] = 'Alone'\n\n# Note it's okay to ignore an  error that sometimes pops up here. For more info check out this link\nurl_info = 'http:\/\/stackoverflow.com\/questions\/20625582\/how-to-deal-with-this-pandas-warning'","7cae3de5":"# Let's check to make sure it worked\nresult.head()","61326f7c":"# Now let's get a simple visualization!\nsns.countplot('Alone',data=result,palette='Blues')","a90d618e":"# Let's start by creating a new column for legibility purposes through mapping (Lec 36)\nresult[\"Survivor\"] = result.Survived.map({0: \"no\", 1: \"yes\"})\n\n# Let's just get a quick overall view of survied vs died. \nsns.countplot('Survivor',data=result,palette='Set1')","6de6453c":"# Let's use a factor plot again, but now considering class\nsns.pointplot('Pclass','Survived',data=result)","7b642cba":"# Let's use a factor plot again, but now considering class and gender\nsns.pointplot('Pclass','Survived',hue='person',data=result)","f4013739":"# Let's use a linear plot on age versus survival\nsns.lmplot('Age','Survived',data=result)","04060b1c":"# Let's use a linear plot on age versus survival using hue for class seperation\nsns.lmplot('Age','Survived',hue='Pclass',data=result,palette='winter')","f29a015b":"# Let's use a linear plot on age versus survival using hue for class seperation\ngenerations=[10,20,40,60,80]\nsns.lmplot('Age','Survived',hue='Pclass',data=result,palette='winter',x_bins=generations)","c5c76f7c":"sns.lmplot('Age','Survived',hue='Sex',data=result,palette='winter',x_bins=generations)","6a6cc194":"# Model Training","f00af2cd":"# Model Evaluation","4bfe2d34":"# Visualisation","431cc5ab":"# Feature Engineering"}}