{"cell_type":{"e8bdab56":"code","36346aee":"code","1ea41572":"code","7f113912":"code","88cf9a6f":"code","b4d63fa4":"code","c569ec88":"code","e31e91a6":"code","a7cf9720":"code","8a174faa":"code","34a9b374":"code","80e2bb21":"code","cbd9f9ef":"code","0deae558":"code","b7fb1c30":"code","9269eb2d":"code","72985d27":"code","5cb2639c":"code","b25d13b6":"code","f6f51d59":"code","0c90eb3e":"code","fd2ea569":"code","46f03dbe":"code","14e210c9":"code","de35eab6":"code","c7247e34":"code","d40faec6":"code","27dd9555":"code","232c5f7c":"code","dbc9cbb9":"code","53637e5d":"code","2f61cae2":"code","d4bef225":"code","1fb7c1c7":"code","36dd1d2e":"code","a98acb65":"code","3fff20a9":"code","831b1f07":"code","fda551a2":"code","5d11504d":"code","fc60faa0":"code","e030ec2b":"code","4abfca4e":"code","14caf169":"code","40697dc6":"code","43b75416":"code","8c079aee":"code","5d70eea1":"code","21c7721d":"code","f9168a75":"code","0301c9b2":"code","1b17c6b8":"code","d867fab6":"code","15d8e98e":"code","8c6993db":"code","2b30cc5f":"code","b74511fc":"code","c3e7ab5b":"code","97683b4b":"code","7747e645":"code","e1051a5a":"code","d9054e99":"code","562ea4d1":"code","b0fa4249":"code","d7dffa19":"code","09ddf2de":"code","111ee019":"code","44839ebf":"code","42578ea8":"code","db31a557":"code","2431713b":"code","82d1ad4b":"code","9dec117b":"code","29d9e142":"code","37eb4805":"code","1aff47aa":"code","4c79d973":"code","d1758e83":"code","5b4b4eee":"code","a9492046":"code","85ffc27c":"code","db5c89d7":"code","499f5d19":"code","f849ce7b":"code","6d830aac":"code","add146c1":"code","bc3b8056":"code","e44dd825":"code","d87f4d71":"code","78f08c57":"code","1d937eec":"code","2f305091":"code","efe4c46f":"code","24bce55e":"markdown","433d8366":"markdown","dd24c542":"markdown","37c897ae":"markdown","4edbec8c":"markdown","a59b03c7":"markdown","aa9d2d5f":"markdown","b7bad04f":"markdown","2eab86be":"markdown","3dae7d9d":"markdown","219fb444":"markdown","109ef2b5":"markdown","0bfa5747":"markdown","896a9f3c":"markdown","9f14eb00":"markdown","608f4f59":"markdown","c4947a1d":"markdown","5dcd6620":"markdown","1a7fcdc5":"markdown","1c0a1035":"markdown","463ebeb2":"markdown","07d03a2b":"markdown","d8a76466":"markdown","43cf4dd0":"markdown","e880604a":"markdown","f86b81e2":"markdown","f741085f":"markdown","bd3f3e63":"markdown","f5ace2a4":"markdown","a6f8aba2":"markdown","cba2ba89":"markdown","2f51e31d":"markdown","968e68a4":"markdown","847ed09c":"markdown","299a582d":"markdown","787249b0":"markdown","34c79276":"markdown","4c03f99b":"markdown","d6366c6a":"markdown","4ae952a1":"markdown","51032821":"markdown","25d39a47":"markdown","9a8cac51":"markdown","0d2b74d2":"markdown","754de15d":"markdown","1641d150":"markdown","2bd7da84":"markdown","25a3fab0":"markdown","1676667b":"markdown"},"source":{"e8bdab56":"# import 'Pandas' \nimport pandas as pd \n\n# import 'Numpy' \nimport numpy as np\n\n# import subpackage of Matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\n# import 'Seaborn' \nimport seaborn as sns\n\n# to suppress warnings \nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\n# import train-test split \nfrom sklearn.model_selection import train_test_split\n\n# import StandardScaler to perform scaling\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.preprocessing import MinMaxScaler \n\n\n# import various functions from sklearn \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.metrics import confusion_matrix, classification_report \nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import tree\nfrom IPython.display import Image \nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import StackingClassifier","36346aee":"# display all columns of the dataframe\npd.options.display.max_columns = None\n\n# display all rows of the dataframe\npd.options.display.max_rows = None\n\n# return an output value upto 6 decimals\npd.options.display.float_format = '{:.6f}'.format","1ea41572":"df=pd.read_csv(\"\/kaggle\/input\/bank-marketing-dataset\/bank.csv\")","7f113912":"df.head()","88cf9a6f":"df.shape","b4d63fa4":"df.info()","c569ec88":"df.describe()","e31e91a6":"df.describe(include='object')","a7cf9720":"df.isnull().sum()","8a174faa":"plt.figure(figsize=(15, 8))\n\nsns.heatmap(df.isnull(), cbar=False)\n\nplt.show()","34a9b374":"cor=df.corr()\nplt.figure(figsize=(15, 8))\n\nsns.heatmap(cor,annot=True)\n\nplt.show()","80e2bb21":"df.describe(include=object)","cbd9f9ef":"df_categoric_features = df.select_dtypes(include='object').drop(['deposit'], axis=1)\nfig, ax = plt.subplots(3, 2, figsize=(25, 20))\nfor variable, subplot in zip(df_categoric_features, ax.flatten()):\n    countplot = sns.countplot(y=df[variable], ax=subplot )\n    countplot.set_ylabel(variable, fontsize = 30) \nplt.tight_layout()   \nplt.show()","0deae558":"for i in df_categoric_features:\n    print(i.upper())\n    print(df[i].value_counts())\n    print( )","b7fb1c30":"df_num=df.select_dtypes(include=np.number)\nplt.figure(figsize=(15, 8))\nfor i in df_num:\n    sns.boxplot(df[i])\n    plt.show()","9269eb2d":"plt.rcParams['figure.figsize'] = [15,8]\ndf.drop('deposit', axis = 1).hist()\nplt.tight_layout()\nplt.show()  \nprint('Skewness:')\ndf.drop('deposit', axis = 1).skew()","72985d27":"df_num=df.select_dtypes(include=np.number)\nX_scaler = StandardScaler()\nnum_scaled = X_scaler.fit_transform(df_num)\nX = pd.DataFrame(num_scaled, columns = df_num.columns)\nX.head()","5cb2639c":"X.shape","b25d13b6":"X.skew()","f6f51d59":"df_cat=df.select_dtypes(exclude=np.number)","0c90eb3e":"df_cat.head()","fd2ea569":"df_cat=df_cat.drop('deposit',axis=1)","46f03dbe":"df_cat.head()","14e210c9":"X_encode=pd.get_dummies(df_cat,columns=df_cat.columns)\nX_encode.head()","de35eab6":"X_encode.shape","c7247e34":"X.shape","d40faec6":"x=pd.concat([X,X_encode],axis=1)","27dd9555":"x.shape","232c5f7c":"x.head()","dbc9cbb9":"plt.figure(figsize=(20,10))\nsns.countplot(df.deposit)\nplt.show()","53637e5d":"df['deposit'].value_counts()","2f61cae2":"y=df['deposit']\nfor i in range(len(y)):\n    if y[i] == 'yes':\n        y[i] = 1\n    else:\n        y[i] = 0 \ny=y.astype('int')\ny.value_counts()","d4bef225":"X_train, X_test, y_train, y_test = train_test_split(x, y, test_size =   0.3, random_state = 10) ","1fb7c1c7":"score_card = pd.DataFrame(columns=[\"Model Name\",'Prob.Cutoff',\"Stability\",\"r2_score\", 'AUC', 'Precision', 'Recall',\n                                       'Accuracy', 'Kappa', 'f1-score'])\ndef update_score_card(Model_name,model,cutoff='-',stability=\"Stable\"):\n    y_pred_prob = model.predict(X_test)\n    y_pred = [ 0 if x < cutoff else 1 for x in y_pred_prob]\n    global score_card\n    score_card = score_card.append({\"Model Name\":Model_name,\n                                    \"Prob.Cutoff\":cutoff,\n                                    'Stability': stability,\n                                    \"r2_score\":model.prsquared,\n                                    'AUC' : metrics.roc_auc_score(y_test, y_pred_prob),\n                                    'Precision': metrics.precision_score(y_test, y_pred),\n                                    'Recall': metrics.recall_score(y_test, y_pred),\n                                    'Accuracy': metrics.accuracy_score(y_test, y_pred),\n                                    'Kappa':metrics.cohen_kappa_score(y_test, y_pred),\n                                    'f1-score': metrics.f1_score(y_test, y_pred)}, \n                                    ignore_index = True)\n    return score_card","36dd1d2e":"def get_test_report(model):\n    test_pred = model.predict(X_test)\n    return(classification_report(y_test, test_pred))","a98acb65":"def logisticRegression(x,y,lr):\n    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size =   0.3, random_state = 10) \n    \n    # describes info about train and test set \n    print(\"Number transactions X_train dataset: \", X_train.shape) \n    print(\"Number transactions y_train dataset: \", y_train.shape) \n    print(\"Number transactions X_test dataset: \", X_test.shape) \n    print(\"Number transactions y_test dataset: \", y_test.shape) \n    \n    # train the model on train set \n    lr.fit(X_train, y_train) \n    \n    predictions = lr.predict(X_test) \n\n    # print classification report \n    print(classification_report(y_test, predictions)) \n\n    cm = confusion_matrix(y_test, predictions, labels=lr.classes_)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=lr.classes_)\n    disp.plot() ","3fff20a9":"def plot_roc(model):\n    y_pred_prob = model.predict_proba(X_test)[:,1]\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n    plt.plot(fpr, tpr)\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.title('ROC curve for Bank marketing Classifier', fontsize = 15)\n    plt.xlabel('False positive rate (1-Specificity)', fontsize = 15)\n    plt.ylabel('True positive rate (Sensitivity)', fontsize = 15)\n    plt.text(x = 0.02, y = 0.9, s = ('AUC Score:',round(roc_auc_score(y_test, y_pred_prob),4)))\n    plt.grid(True)","831b1f07":"def plot_confusion_matrix(model):\n    y_pred = model.predict(X_test)\n    cm = confusion_matrix(y_test, y_pred)\n    conf_matrix = pd.DataFrame(data = cm,columns = ['Predicted:0','Predicted:1'], index = ['Actual:0','Actual:1'])\n    sns.heatmap(conf_matrix, annot = True, fmt = 'd', cmap = ListedColormap(['lightskyblue']), cbar = False, \n                linewidths = 0.1, annot_kws = {'size':25})\n    plt.xticks(fontsize = 20)\n    plt.yticks(fontsize = 20)\n    plt.show()","fda551a2":"import statsmodels.api as sm\nlogreg = sm.Logit(y_train, X_train).fit()\nprint(logreg.summary())","5d11504d":"print('AIC: ',logreg.aic)","fc60faa0":"df_odds = pd.DataFrame(np.exp(logreg.params), columns= ['Odds']) \ndf_odds","e030ec2b":"y_pred_prob = logreg.predict(X_test)\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\nyoudens_table = pd.DataFrame({'TPR': tpr,\n                             'FPR': fpr,\n                             'Threshold': thresholds})\nyoudens_table['Difference'] = youdens_table.TPR - youdens_table.FPR\nyoudens_table = youdens_table.sort_values('Difference', ascending = False).reset_index(drop = True)\nyoudens_table.head()","4abfca4e":"y_pred_prob = logreg.predict(X_test)\ny_pred_prob.head()","14caf169":"y_pred_prob = logreg.predict(X_test)\ny_pred = [ 0 if x < 0.69 else 1 for x in y_pred_prob]\nprint(classification_report(y_test, y_pred))","40697dc6":"cm = confusion_matrix(y_test, y_pred)\nconf_matrix = pd.DataFrame(data = cm,columns = ['Predicted:0','Predicted:1'], index = ['Actual:0','Actual:1'])\nsns.heatmap(conf_matrix, annot = True, fmt = 'd', cmap = ListedColormap(['lightskyblue']), cbar = False,linewidths = 0.1, annot_kws = {'size':25})\nplt.xticks(fontsize = 20)\nplt.yticks(fontsize = 20)\nplt.show()","43b75416":"TN = cm[0,0]\nTP = cm[1,1]\nFP = cm[0,1]\nFN = cm[1,0]","8c079aee":"precision = TP \/ (TP+FP)\nprint('Precision:',precision)\nrecall = TP \/ (TP+FN)\nprint('Recall:',recall)\nspecificity = TN \/ (TN+FP)\nprint('Specificity:',specificity)\nf1_score = 2*((precision*recall)\/(precision+recall))\nprint('f1_score:',f1_score)\naccuracy = (TN+TP) \/ (TN+FP+FN+TP)\nprint('Accuracy:',accuracy)","5d70eea1":"kappa = cohen_kappa_score(y_test, y_pred)\nprint('kappa value:',kappa)","21c7721d":"fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.plot([0, 1], [0, 1],'r--')\nplt.title('ROC curve for Bank marketing Classifier (Full Model)', fontsize = 15)\nplt.xlabel('False positive rate (1-Specificity)', fontsize = 15)\nplt.ylabel('True positive rate (Sensitivity)', fontsize = 15)\nplt.text(x = 0.02, y = 0.9, s = ('AUC Score:', round(metrics.roc_auc_score(y_test, y_pred_prob),4)))\nplt.grid(True)","f9168a75":"update_score_card(\"Simple Logistic Regression\",logreg,cutoff=0.69,stability=\"Stable\")","0301c9b2":"def update_score_card(Model_name,model,cutoff=\"-\",stability=\"Stable\"):\n    y_pred_prob = model.predict_proba(X_test)[:,1]\n    y_pred=model.predict(X_test)\n    global score_card\n    score_card = score_card.append({\"Model Name\":Model_name,\n                                    \"Prob.Cutoff\":cutoff,\n                                    'Stability': stability,\n                                    \"r2_score\":metrics.r2_score(y_test, y_pred),\n                                    'AUC' : metrics.roc_auc_score(y_test, y_pred_prob),\n                                    'Precision': metrics.precision_score(y_test, y_pred),\n                                    'Recall': metrics.recall_score(y_test, y_pred),\n                                    'Accuracy': metrics.accuracy_score(y_test, y_pred),\n                                    'Kappa':metrics.cohen_kappa_score(y_test, y_pred),\n                                    'f1-score': metrics.f1_score(y_test, y_pred)}, \n                                    ignore_index = True)\n    return score_card","1b17c6b8":"gnb = GaussianNB()\ngnb_model = gnb.fit(X_train, y_train)\nplot_confusion_matrix(gnb_model)","d867fab6":"test_report = get_test_report(gnb_model)\nprint(test_report)","15d8e98e":"plot_roc(gnb_model)","8c6993db":"update_score_card(\"gNB Classifier\",gnb_model,stability=\"Moderate\")","2b30cc5f":"knn_classification = KNeighborsClassifier(n_neighbors = 3)\nknn_model = knn_classification.fit(X_train, y_train)\nplot_confusion_matrix(knn_model)","b74511fc":"test_report = get_test_report(knn_model)\nprint(test_report)","c3e7ab5b":"plot_roc(knn_model)","97683b4b":"update_score_card(\"KNN Classifier\",knn_model,stability=\"Moderate\")","7747e645":"decision_tree_classification =DecisionTreeClassifier(criterion = 'entropy', random_state = 10)\ndecision_tree = decision_tree_classification.fit(X_train, y_train)","e1051a5a":"test_report = get_test_report(decision_tree)\nprint(test_report)","d9054e99":"plot_roc(decision_tree)","562ea4d1":"update_score_card(\"Decision Tree Classifier\",decision_tree,stability=\"Good\")","b0fa4249":"rf_classification = RandomForestClassifier(n_estimators = 10, random_state = 10)\nrf_model = rf_classification.fit(X_train, y_train)","d7dffa19":"test_report = get_test_report(rf_model)\nprint(test_report) ","09ddf2de":"plot_roc(rf_classification)","111ee019":"update_score_card(\"Random Forest Classifier\",rf_model,stability=\"Good\")","44839ebf":"ada_model = AdaBoostClassifier(n_estimators = 40, random_state = 10)\nada_model.fit(X_train, y_train)","42578ea8":"plot_confusion_matrix(ada_model)","db31a557":"test_report = get_test_report(ada_model)\nprint(test_report) ","2431713b":"plot_roc(ada_model)","82d1ad4b":"update_score_card(\"ADAboost classifier\",ada_model,stability=\"Moderate\")","9dec117b":"gboost_model = GradientBoostingClassifier(n_estimators = 150, max_depth = 10, random_state = 10)\ngboost_model.fit(X_train, y_train)","29d9e142":"plot_confusion_matrix(gboost_model)","37eb4805":"test_report = get_test_report(gboost_model)\nprint(test_report) ","1aff47aa":"plot_roc(gboost_model)","4c79d973":"update_score_card(\"Gradient boost classifier\",gboost_model,stability=\"Moderate\")","d1758e83":"xgb_model = XGBClassifier(max_depth = 10, gamma = 1)\nxgb_model.fit(X_train, y_train)","5b4b4eee":"plot_confusion_matrix(xgb_model)","a9492046":"test_report = get_test_report(xgb_model)\nprint(test_report) ","85ffc27c":"plot_roc(xgb_model)","db5c89d7":"update_score_card(\"XGBClassifier\",xgb_model,stability=\"Moderate\")","499f5d19":"base_learners = [('rf_model', RandomForestClassifier(criterion = 'entropy', max_depth = 10, max_features = 'sqrt', \n                                                     max_leaf_nodes = 8, min_samples_leaf = 5, min_samples_split = 2, \n                                                     n_estimators = 50, random_state = 10)),\n                 ('KNN_model', KNeighborsClassifier(n_neighbors = 17, metric = 'euclidean')),\n                 ('NB_model', GaussianNB())]\nstack_model = StackingClassifier(estimators = base_learners, final_estimator = GaussianNB())\nstack_model.fit(X_train, y_train)","f849ce7b":"plot_confusion_matrix(stack_model)","6d830aac":"test_report = get_test_report(stack_model)\nprint(test_report)","add146c1":"plot_roc(stack_model)","bc3b8056":"update_score_card(\"Stacking classifier\",stack_model,stability=\"Moderate\")","e44dd825":"tuned_paramaters = {'n_neighbors': np.arange(1, 25, 2),\n                   'metric': ['hamming','euclidean','manhattan','Chebyshev']}\nknn_classification = KNeighborsClassifier()\nknn_grid = GridSearchCV(estimator = knn_classification, \n                        param_grid = tuned_paramaters, \n                        cv = 5, \n                        scoring = 'accuracy')\nknn_grid.fit(X_train, y_train)\nprint('Best parameters for KNN Classifier: ', knn_grid.best_params_, '\\n')","d87f4d71":"knn_classification = KNeighborsClassifier(metric='euclidean',n_neighbors=19)\nknn_model_tuned = knn_classification.fit(X_train, y_train)\nplot_confusion_matrix(knn_model_tuned)","78f08c57":"print(get_test_report(knn_model_tuned))","1d937eec":"plot_roc(knn_model_tuned)","2f305091":"update_score_card(\"KNN classifier tuned\",knn_model_tuned,stability=\"Moderate\")","efe4c46f":"plot_roc(gnb_model)\nplot_roc(knn_model)\nplot_roc(knn_model_tuned)\nplot_roc(decision_tree)\nplot_roc(rf_classification)\nplot_roc(ada_model)\nplot_roc(gboost_model)\nplot_roc(xgb_model)\nplot_roc(stack_model)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\n\nplt.plot([0, 1], [0, 1],'r--')\n\nplt.title('ROC curve analysis', fontsize = 15)\nplt.xlabel('False positive rate (1-Specificity)', fontsize = 15)\nplt.ylabel('True positive rate (Sensitivity)', fontsize = 15)\n\nplt.legend(prop={'size':13}, loc='lower right')\nplt.grid(True)","24bce55e":"## Stacking Classifier","433d8366":"**1. Check data types**","dd24c542":"**2. For categorical features, we use .describe(include=object)**","37c897ae":"### Data Attributes ","4edbec8c":"<a id='xgboost'><\/a>\n\n## XG Boost","a59b03c7":"<a id='Read_Data'><\/a>\n## 3. Read Data","aa9d2d5f":"The classification goal is to predict if the client will subscribe (yes\/no) a term deposit (variable y).\n\n\n**Attribute Information:**\n\n1 - age (numeric)\n\n2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n\n3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n\n4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n\n5 - default: has credit in default? (categorical: 'no','yes','unknown')\n\n6 - housing: has housing loan? (categorical: 'no','yes','unknown')\n\n7 - loan: has personal loan? (categorical: 'no','yes','unknown')\n\n8 - contact: contact communication type (categorical: 'cellular','telephone')\n\n9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n\n10 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n\n11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n\n12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n\n13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n\n14 - previous: number of contacts performed before this campaign and for this client (numeric)\n\n15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n\n16 - balance: balance od the customer\n\n17 - y - has the client subscribed a term deposit? (binary: 'yes','no')","b7bad04f":"<a id='Missing_Values'><\/a>\n### 4.1.4 Missing Values","2eab86be":"<a id='numerical'><\/a>\n### 4.1.6 Analyze Numerical Variables","3dae7d9d":"1. Accuracy score for selected features for different models range between 0.75 - 0.94\n\n2. Precision Score for selected features for different models range between 0.74 - 0.95\n\n3. Gaussian Na\u00efve Bayes Classifier has overall low precision, recall, accuracy and kappa score compared to other models built.\n\n4. Simple Logistic Regression for selected features has a stable nature whereas both Random Forest and Decision Tree has Good nature of stability.","219fb444":"#### Corelation heatmap","109ef2b5":"<a id='gradboost'><\/a>\n\n## Gradient Boost","0bfa5747":"<a id='imbalance data'><\/a>\n# Handling the imbalanced data","896a9f3c":"**Compare the performance of the Different Models built**","9f14eb00":"<a id='log_full model'><\/a>\n# Logistic Regression Full Model","608f4f59":"<a id='set_options'><\/a>\n## 2. Set Options","c4947a1d":"<a id='categorical'><\/a>\n### 4.1.6 Analyze Categorical Variables\n\nCategorical variables are those in which the values are labeled categories. The values, distribution, and dispersion of categorical variables are best understood with bar plots.","5dcd6620":"### Visualize Missing Values using Heatmap","1a7fcdc5":"The next step after implementing a machine learning algorithm is to find out how effective is the model based on metric and datasets. Different performance metrics are used to evaluate different Machine Learning Algorithms. For example a classifier used to distinguish between images of different objects; we can use classification performance metrics such as, Precision score,accuracy score , recall score and Cross val score etc.\n\nThe machine learning model cannot be simply tested using the training set, because the output will be prejudiced, because the process of training the machine learning model has already tuned the predicted outcome to the training dataset. Therefore in order to estimate the generalization error, the model is required to test a dataset which it hasn\u2019t seen yet; giving birth to the term testing dataset.\n\nTherefore for the purpose of testing the model, we would require a labelled dataset. This can be achieved by splitting the training dataset into training dataset and testing dataset. This can be achieved by various techniques such as, k-fold cross validation.","1c0a1035":"## Problem Statement","463ebeb2":"<a id='data_preparation'><\/a>\n## 4. Data Analysis and Preparation","07d03a2b":"According to the analysis made across, a target customer profile can be established. The most responsive customers possess following features:\n\n-> Feature 1: age < 30 or age > 60      \n-> Feature 2: students or retired people       \n-> Feature 3: specific months (dec, may, oct)     \n\nBy applying the supervised learning classification techniques, using ensemble learning models, and boosting technqiues the estimation models were successfully built. With all the respective models, the bank will be able to predict a customer's response in the telemarketing campaign before calling the customer. In this way, the bank can allocate more marketing efforts to the clients who are classified as highly likely to accept term deposits, and call less to those who are unlikely to make term deposits. ","d8a76466":"<a id='knngrid'><\/a>\n# KNN model Tuned","43cf4dd0":"**This reveals a clear relationship among age, balance, duration, and campaign.**\n\n        To investigate more about correlation, a correlation matrix was plotted with all qualitative variables. Clearly, \u201ccampaign outcome\u201d has a strong correlation with \u201cduration\u201d, a moderate correlation with \u201cprevious contacts\u201d, and mild correlations between \u201cbalance\u201d, \u201cmonth of contact\u201d and \u201cnumber of campaign\u201d. Their influences on campaign outcome will be investigated further in the machine learning part.","e880604a":"<h1><center>Bank marketing analyis using Machine Learning<\/center><\/h1>","f86b81e2":"<a id='encoding'><\/a>\n### Encoding the categorical variable","f741085f":"<a id='guassiannb_model'><\/a>\n# Guassian naive bayes model","bd3f3e63":"<a id='Data_Shape'><\/a>\n### 4.1.1 Data Dimension","f5ace2a4":"<a id='Target variable'><\/a>\n### 4.1.8 Target variable","a6f8aba2":"<a id='boosting'><\/a>\n\n# BOOSTING TECHNIQUES","cba2ba89":"<a id='knn_model'><\/a>\n# KNN model","2f51e31d":"<a id='import_lib'><\/a>\n## 1. Import Libraries","968e68a4":"**There are no missing values in the dataset**","847ed09c":"<ul>\n    <li>Correlation is the extent of linear relationship among numeric variables<\/li>\n    <li>It indicates the extent to which two variables increase or decrease in parallel<\/li>\n    <li>The value of a correlation coefficient ranges between -1 and 1<\/li>\n    <li> Correlation among multiple variables can be represented in the form of a matrix. This allows us to see which pairs are correlated<\/li>\n    <\/ul>\n    ","299a582d":"<a id='ADAboost'><\/a>\n\n## ADABoost","787249b0":"<a id='correlation'><\/a>\n### 4.1.5 Correlation","34c79276":"<a id='Data_Types'><\/a>\n### 4.1.2 Data Types\nData has a variety of data types. The main types stored in pandas dataframes are object, float, int64, bool and datetime64. In order to learn about each attribute, it is always good for us to know the data type of each column.","4c03f99b":"<a id='Scaling the data'><\/a>\n###  4.1.7 Scaling The Data","d6366c6a":"<a id='Summary_Statistics'><\/a>\n### 4.1.3 Summary Statistics\n**1. For numerical variables, we use .describe()**","4ae952a1":"## Confusion matrix","51032821":"**Which metric did we choose and why?**","25d39a47":"<a id='randomforest'><\/a>\n\n# RANDOM FOREST","9a8cac51":"<a id='Data_Understanding'><\/a>\n### 4.1 Understand the Dataset","0d2b74d2":"<a id='decisiontree'><\/a>\n# DECISION TREE","754de15d":"**Which model has better performance on the test set?**","1641d150":"<a id='Train test split'><\/a>\n# Train Test Split","2bd7da84":"For binary classification model evaluation between random forest and logistic regression, our work focused on four distinct simulated datasets:    \n(1) increasingthe variance in the explanatory and noise variables,     \n(2) increasing the number of noise variables,     \n(3) increasing the number of explanatory variables,     \n(4) increasing the number of observations.\n\nTo benchmark and comparing classification scores between different classification models built, metrics such as accuracy, area under the curve, true positive rate, false positive rate, and precision were analyzed.\n\nKNN classifier tuned has got better accuracy score compared to other models, hence we can say that it has a better performance.","25a3fab0":"Improve bank marketing of a bank by analyzing their past marketing campaign data and recommending which customer to target.\n\nThe aim of this project is to devise such a machine leaning prediction algorithm, the bank can better target its customers and channelize its mrketing efforts. ","1676667b":"# Conclusion "}}