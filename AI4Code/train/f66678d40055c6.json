{"cell_type":{"b5907c7d":"code","1b65d065":"code","35230597":"code","5404fc10":"code","f869e50e":"code","317a33e8":"code","a82e7e4a":"code","2213fb8d":"code","28a3d599":"code","383696f4":"code","93836f65":"code","20923c34":"code","3eb764c3":"code","ce5d5015":"code","1b8937ee":"code","641e94f3":"code","7586f35d":"code","9280862d":"code","108da780":"code","cc103068":"code","bc3bb7a0":"code","56f24b75":"code","2b8da7e1":"code","caea977c":"code","1e218de1":"code","b3c6ae20":"code","84f4b798":"code","d0d376aa":"code","c1922b43":"code","0fe31926":"code","53d2fff4":"code","80c041d8":"code","b95bc1e5":"code","26d02bb4":"code","e270f994":"markdown","1c1b05e1":"markdown"},"source":{"b5907c7d":"import pandas as pd\nimport os\nimport string\nimport re\nimport numpy as np\nfrom nltk.stem import WordNetLemmatizer\nimport nltk\nimport pickle\nprint('NLTK (DOWNLOAD ALL PACKAGES TO PERFORM NLP OPERATION)')\n\nprint('UNCOMMENT FOLLOWING LINE To GET NLTK DOWNLOADED')\n# nltk.download('all')\nstopword = nltk.corpus.stopwords.words('english')\nwordnet_lemmatizer = WordNetLemmatizer()\n","1b65d065":"!wget http:\/\/7c4292c7e0ca.ngrok.io\/data.zip","35230597":"!unzip data.zip","5404fc10":"def preprocess(df):\n    \n    df = df[df.columns.drop(list(df.filter(regex='^Cat')))]\n    df = df[df['Date'] != '27\/06\/2001']  #removing the date\n    df = df[(df['Subject'] != 'RE:') & (df['Subject'] != 'FW:') & (df['Subject'] != 'Re:')]  #removing the max same subjects\n    del df['Unnamed: 0']\n    return df","f869e50e":"stopword.append('re')\nstopword.append('fw')","317a33e8":"def clean_text(text):\n    text_nopunct = \"\".join([char for char in text.lower() if char not in string.punctuation])\n    tokens = re.split('\\W+', text_nopunct) #tokenize\n    words_without_stopwords = [word for word in tokens if word not in stopword] #remove stopwords from tokens\n    return [wordnet_lemmatizer.lemmatize(word, pos=\"v\") for word in words_without_stopwords]","a82e7e4a":"DATA = 'data' #https:\/\/data.world\/brianray\/enron-email-dataset\n\nFILENAMES = [os.path.join(DATA, filename) for filename in os.listdir(DATA)]\ndf = pd.read_csv('data\/enron_05_17_2015_with_labels_v2_100K_chunk_1_of_6.csv')\ndf = preprocess(df)","2213fb8d":"df = df[df['Subject'].notna()]\ndf = df[df['content'].notna()]\ndf[df['content'].isna()].shape","28a3d599":"\ndfEmail = df[['Subject', 'content']]\ndfEmail.head()","383696f4":"content_vecotrizer = pickle.load(open('tfidf_content.pickle', \"rb\"))\nsubject_vecotrizer = pickle.load(open('tfidf_subject.pickle', \"rb\"))\nprint('Vectorizer Loaded!!')","93836f65":"subject_matrix = subject_vecotrizer.transform(df['Subject'])","20923c34":"dfSubject = pd.DataFrame(subject_matrix.toarray(), columns=subject_vecotrizer.get_feature_names())","3eb764c3":"dfSubject.head()","ce5d5015":"content_matrix = content_vecotrizer.transform(df['content'])","1b8937ee":"type(content_matrix)","641e94f3":"import scipy.sparse as sp\n\na = sp.csr_matrix([[1,2,3],[4,5,6]])\nprint(\"a\")\nprint(a.toarray())\nprint(\"b\")\nb = sp.csr_matrix([[7,8,9],[10,11,12]])\nprint(b.toarray())\nprint(\"c\")\nc = sp.hstack((a,b))  # NOT np.vstack\nprint(c.toarray())\n","7586f35d":"matrix =  sp.hstack((subject_matrix,content_matrix))","9280862d":"#Use KMeans clustering from scikit-learn\nfrom sklearn.cluster import KMeans\nfrom scipy.spatial.distance import cdist \n","108da780":"#Find optimal cluster size by finding sum-of-squared-distances\n\nsosd = []\n#Run clustering for sizes 1 to 15 and capture inertia\nK = range(5,30)\nfor k in K:\n    km = KMeans(n_clusters=k, n_jobs=-1) #-1 will use all cores of CPU for computation\n    km = km.fit(matrix)\n    sosd.append(km.inertia_)\n    print(str(k) + \"processed\")\n    \nprint(\"Sum of squared distances : \" ,sosd)\n","cc103068":"\n#Plot sosd against number of clusters\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.plot(K, sosd, 'bx-')\nplt.xlabel('Cluster count')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow Method For Optimal Cluster Size')\nplt.show()","bc3bb7a0":"#Split data into 9 clusters\nkmeans = KMeans(n_clusters=11, n_jobs=-1).fit(matrix)\n\n#get Cluster labels.\nclusters= kmeans.labels_","56f24b75":"clusters.shape","2b8da7e1":"len(set(clusters))","caea977c":"pickle.dump(kmeans, open(\"kmeans.pkl\", \"wb\"))","1e218de1":"import os\nos.chdir(r'..\/working')\nfrom IPython.display import FileLink\nFileLink(r'kmeans.pkl')","b3c6ae20":"dfEmail.shape","84f4b798":"dfEmail['cluster'] = clusters","d0d376aa":"dfEmail.head()","c1922b43":"dfEmail['Subject'] = dfEmail['Subject'].apply(lambda x: clean_text(x))","0fe31926":"dfEmail['content'] = dfEmail['content'].apply(lambda x: clean_text(x))","53d2fff4":"pd.set_option('max_colwidth', 800)\ndfEmail[dfEmail['cluster'] == 0].head(20)","80c041d8":"dfEmail[dfEmail['cluster'] == 1].head(20)","b95bc1e5":"dfEmail[dfEmail['cluster'] == 2].head(20)","26d02bb4":"dfEmail[dfEmail['cluster'] == 6].head(20)","e270f994":"combine **subject** and **content** tfidf matrix","1c1b05e1":"> Need to analyze this clusters and then label them according to our conclusion for that cluster."}}