{"cell_type":{"856e47eb":"code","688d36cd":"code","b1a2b168":"code","67780949":"code","b6cf90dc":"code","1fbdb105":"code","73c53054":"code","b653e441":"code","bebbf0fc":"code","6b662549":"code","284dc2e6":"code","cc69e3ef":"code","7d575cf2":"code","729128b0":"code","a16f5a58":"code","d7694ce5":"code","51b1f601":"code","c90c008e":"code","e507e49c":"code","33e8b8d6":"code","1ddd815d":"code","24d0fed7":"code","472166c9":"code","4ad611a5":"code","0cc94db6":"code","a3d0ae98":"code","f8efb3cd":"markdown","5560d91f":"markdown","b19e5144":"markdown","9c9d01b4":"markdown","7b3a2d51":"markdown","05852fb5":"markdown","e9ef0753":"markdown","0ac7234c":"markdown","38e290ba":"markdown","38f73c90":"markdown","30561f55":"markdown","668f30ad":"markdown"},"source":{"856e47eb":"import numpy as np # Matrix Operations (Matlab of Python)\nimport pandas as pd # Work with Datasources\nimport matplotlib.pyplot as plt # Drawing Library\n\nfrom PIL import Image\n\nimport torch # Like a numpy but we could work with GPU by pytorch library\nimport torch.nn as nn # Nural Network Implimented with pytorch\nimport torchvision # A library for work with pretrained model and datasets\n\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\n\nimport glob\nimport os\n\n%matplotlib inline\n\nimage_size = (100, 100)\nimage_row_size = image_size[0] * image_size[1]","688d36cd":"class CatDogDataset(Dataset):\n    def __init__(self, path, transform=None):\n        self.classes   = os.listdir(path)\n        self.path      = [f\"{path}\/{className}\" for className in self.classes]\n        self.file_list = [glob.glob(f\"{x}\/*\") for x in self.path]\n        self.transform = transform\n        \n        files = []\n        for i, className in enumerate(self.classes):\n            for fileName in self.file_list[i]:\n                files.append([i, className, fileName])\n        self.file_list = files\n        files = None\n        \n    def __len__(self):\n        return len(self.file_list)\n\n    def __getitem__(self, idx):\n        fileName = self.file_list[idx][2]\n        classCategory = self.file_list[idx][0]\n        im = Image.open(fileName)\n        if self.transform:\n            im = self.transform(im)\n        return im.view(-1), classCategory","b1a2b168":"mean = [0.485, 0.456, 0.406]\nstd  = [0.229, 0.224, 0.225]\ntransform = transforms.Compose([\n                                transforms.Resize(image_size), \n                                transforms.Grayscale(),\n                                transforms.ToTensor(), \n                                transforms.Normalize(mean, std)])","67780949":"path    = '..\/input\/training_set\/training_set'\ndataset = CatDogDataset(path, transform=transform)","b6cf90dc":"def imshow(source):\n    plt.figure(figsize=(10,10))\n    imt = (source.view(-1, image_size[0], image_size[0]))\n    imt = imt.numpy().transpose([1,2,0])\n    imt = (std * imt + mean).clip(0,1)\n    plt.subplot(1,2,2)\n    plt.imshow(imt)","1fbdb105":"imshow(dataset[0][0])\nimshow(dataset[2][0])\nimshow(dataset[6000][0])","73c53054":"shuffle     = True\nbatch_size  = 64\nnum_workers = 0\ndataloader  = DataLoader(dataset=dataset, \n                         shuffle=shuffle, \n                         batch_size=batch_size, \n                         num_workers=num_workers)","b653e441":"class MyModel(torch.nn.Module):\n    def __init__(self, in_feature):\n        super(MyModel, self).__init__()\n        self.fc1     = torch.nn.Linear(in_features=in_feature, out_features=500)\n        self.fc2     = torch.nn.Linear(in_features=500, out_features=100)\n        self.fc3     = torch.nn.Linear(in_features=100, out_features=1)\n\n    def forward(self, x):\n        x = F.relu( self.fc1(x) )\n        x = F.relu( self.fc2(x) )\n        x = F.softmax( self.fc3(x), dim=1)\n        return x","bebbf0fc":"model = MyModel(image_row_size)\nprint(model)","6b662549":"criterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.95)\n\nepochs   = 10\nfor epoch in range(epochs):\n    for i, (X,Y) in enumerate(dataloader):\n#         x, y = dataset[i]\n        yhat = model(X)\n        loss = criterion(yhat.view(-1), Y)\n        break\n","284dc2e6":"yhat.view(-1).size()\n# loss = criterion(yhat, y)","cc69e3ef":"device = torch.device('cpu')\n# if torch.cuda.is_available():\n#     device = torch.device('cuda')\n\nD_in, H, D_out = image_row_size, 100, 2\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(D_in, 500),\n    torch.nn.ReLU(),\n    torch.nn.Linear(500, 100),\n    torch.nn.ReLU(),\n    torch.nn.Linear(100, 2),\n    torch.nn.Sigmoid()\n).to(device)\n\n# layers = []\n# layers.append(nn.Linear(D_in, 500))\n# layers.append(nn.ReLU())\n# layers.append(nn.Linear(500, 100))\n# layers.append(nn.ReLU())\n# layers.append(nn.Linear(100, 2))\n# layers.append(nn.Sigmoid())\n\n# model = nn.Sequential(*layers)\n\n\nprint(model)","7d575cf2":"model(x)","729128b0":"class CatDogDataset(Dataset):\n    def __init__(self, path, transform=None):\n        self.classes   = os.listdir(path)\n        self.path      = [f\"{path}\/{className}\" for className in self.classes]\n        self.file_list = [glob.glob(f\"{x}\/*\") for x in self.path]\n        self.transform = transform\n        \n        files = []\n        for i, className in enumerate(self.classes):\n            for fileName in self.file_list[i]:\n                files.append([i, className, fileName])\n        self.file_list = files\n        files = None\n        \n    def __len__(self):\n        return len(self.file_list)\n\n    def __getitem__(self, idx):\n        fileName = self.file_list[idx][2]\n        classCategory = self.file_list[idx][0]\n        im = Image.open(fileName)\n        if self.transform:\n            im = self.transform(im)\n        return im, classCategory","a16f5a58":"mean = [0.485, 0.456, 0.406]\nstd  = [0.229, 0.224, 0.225]\ntransform = transforms.Compose([\n                                transforms.Resize(image_size), \n                                transforms.Grayscale(),\n                                transforms.ToTensor(), \n                                transforms.Normalize(mean, std)])\n\n\n\npath    = '..\/input\/training_set\/training_set'\ndataset = CatDogDataset(path, transform=transform)\n\nshuffle     = True\nbatch_size  = 64\nnum_workers = 0\ndataloader  = DataLoader(dataset=dataset, \n                         shuffle=shuffle, \n                         batch_size=batch_size, \n                         num_workers=num_workers)","d7694ce5":"class MyCNNModel(torch.nn.Module):\n    def __init__(self):\n        super(MyCNNModel, self).__init__()\n        self.relu    = torch.nn.ReLU()\n        self.fc1     = torch.nn.Linear(in_features=1, out_features=500)\n        self.fc2     = torch.nn.Linear(in_features=500, out_features=100)\n        self.fc3     = torch.nn.Linear(in_features=100, out_features=2)\n    def forward(self, x):\n#         torch.nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(3,3)),\n#         torch.nn.ReLU(),\n#         torch.nn.MaxPool2d((2,2)),\n#         torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3,3)),\n#         torch.nn.ReLU(),\n#         torch.nn.MaxPool2d((2,2)),\n#         torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3,3)),\n#         torch.nn.ReLU(),\n#         torch.nn.MaxPool2d((2,2)),\n#         torch.nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3,3)),\n#         torch.nn.ReLU(),\n#         torch.nn.MaxPool2d((2,2)),\n#         Flatten(),\n#         torch.nn.Linear(128*3*3, 512),\n#         torch.nn.ReLU(),\n#         torch.nn.Linear(512, 1),\n#         torch.nn.Sigmoid()\n        return x","51b1f601":"class Flatten(torch.nn.Module):\n    def forward(self, input):\n        return input.view(input.size(0), -1)","c90c008e":"model = torch.nn.Sequential(\n            torch.nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3,3)),\n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d((2,2)),\n            torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3,3)),\n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d((2,2)),\n            torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3,3)),\n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d((2,2)),\n            torch.nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3,3)),\n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d((2,2)),\n            Flatten(),\n            torch.nn.Linear(1152, 512),\n            torch.nn.ReLU(),\n            torch.nn.Linear(512, 1),\n            torch.nn.Sigmoid()\n)","e507e49c":"model","33e8b8d6":"x, y = dataset[0]\nxx = x.unsqueeze(0)","1ddd815d":"model(xx)","24d0fed7":"num_classes = 2\nclass UnaryNet(nn.Module):\n    def __init__(self):\n        super(UnaryNet, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, 5)\n        self.conv2 = nn.Conv2d(10, 20, 5)\n\n        self.fc1_mean = nn.Linear(9680 , 140)\n        self.fc2_mean = nn.Linear(140, num_classes)\n\n    def forward(self, x):\n        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n        x = x.view(-1, 9680 )\n        \n        mean = F.relu(self.fc1_mean(x))\n        mean = self.fc2_mean(mean)\n\n        return mean\n","472166c9":"x, y = dataset[0]","4ad611a5":"\nnet = UnaryNet()\n","0cc94db6":"xx = x.unsqueeze(0)","a3d0ae98":"net(xx)","f8efb3cd":"## Create Dataset","5560d91f":"### Make a Instance of NN Class","b19e5144":"### Create NN Class","9c9d01b4":"# Introdution\n* ### 1- Creat a Simple Deep Neural Network\n* #### 1-1 Create a Dataset Class and Transformation\n* ### 1- Creat a Simple Deep Neural Network\n\n\n## Create Dataset Class","7b3a2d51":"# 2. Sequential Model","05852fb5":"# 1. Neural Network Model","e9ef0753":"## Display Sample Image","0ac7234c":"# 4. Convolution Neural Network, Sequential","38e290ba":"# 3. Convolution Neural Network","38f73c90":"## Create DataLoader","30561f55":"### Train Model","668f30ad":"## Define Transoformation And Augmentation \n\n[Image Augmentatin](https:\/\/imgaug.readthedocs.io\/en\/latest\/):\nimgaug is a library for image augmentation in machine learning experiments. It supports a wide range of augmentation techniques, allows to easily combine these, has a simple yet powerful stochastic interface, can augment images and keypoints\/landmarks on these and offers augmentation in background processes for improved performance.\n\n[Augmentor](https:\/\/github.com\/mdbloice\/Augmentor) And \n[Augmentor Doc](https:\/\/augmentor.readthedocs.io\/en\/master\/)\n\n\n[GoogleBlog, Improving Deep Learning Performance with AutoAugment](https:\/\/ai.googleblog.com\/2018\/06\/improving-deep-learning-performance.html):\nResults\nOur AutoAugment algorithm found augmentation policies for some of the most well-known computer vision datasets that, when incorporated into the training of the neural network, led to state-of-the-art accuracies. By augmenting ImageNet data we obtain a new state-of-the-art accuracy of 83.54% top1 accuracy and on CIFAR10 we achieve an error rate of 1.48%, which is a 0.83% improvement over the default data augmentation designed by scientists. On SVHN, we improved the state-of-the-art error from 1.30% to 1.02%. Importantly, AutoAugment policies are found to be transferable \u2014 the policy found for the ImageNet dataset could also be applied to other vision datasets (Stanford Cars, FGVC-Aircraft, etc.), which in turn improves neural network performance. \n\nWe are pleased to see that our AutoAugment algorithm achieved this level of performance on many different competitive computer vision datasets and look forward to seeing future applications of this technology across more computer vision tasks and even in other domains such as audio processing or language models. The policies with the best performance are included in the appendix of the paper, so that researchers can use them to improve their models on relevant vision tasks. \n\n[Unofficial implementation](https:\/\/github.com\/DeepVoltaire\/AutoAugment) And\n[Sample Notebook](https:\/\/github.com\/DeepVoltaire\/AutoAugment\/blob\/master\/AutoAugment_Exploration.ipynb)"}}