{"cell_type":{"8f41d6a6":"code","b6d6da18":"code","9a67e13b":"code","150fd730":"code","70fb5d02":"code","28d7d2ec":"code","859efd7d":"code","131868a0":"code","97a19cc9":"code","af5fd007":"code","8dbfe10f":"code","939b614e":"code","9000a76e":"code","3210e85e":"code","4a11229e":"code","ce757ad5":"markdown","5cb7aba6":"markdown","b800ec74":"markdown","a821208b":"markdown","0c4ef9ef":"markdown","8dd1e762":"markdown","d5881b64":"markdown","8e7879f8":"markdown","07fb8094":"markdown","abc4e087":"markdown"},"source":{"8f41d6a6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","b6d6da18":"train_filepath = \"..\/input\/train.csv\"\ntest_filepath = \"..\/input\/test.csv\"\nBuilding_Structure_filepath = \"..\/input\/Building_Structure.csv\"\nBuilding_Ownership_Use_filepath = \"..\/input\/Building_Ownership_Use.csv\"\nres_train_filepath = \"merged_train.csv\"\nres_test_filepath = \"merged_test.csv\"\n\ntrain = pd.read_csv(train_filepath)\nBuilding_Ownership_Use = pd.read_csv(Building_Ownership_Use_filepath)\ntest = pd.read_csv(test_filepath)\nBuilding_Structure = pd.read_csv(Building_Structure_filepath)\n\nBuilding_Info = pd.merge(Building_Ownership_Use,Building_Structure,on=['building_id','vdcmun_id','district_id','ward_id'],how='inner')\n\nres_train = pd.merge(train,Building_Info,on=['building_id','vdcmun_id','district_id'],how='inner')\nres_test = pd.merge(test,Building_Info,on=['building_id','vdcmun_id','district_id'],how='inner')\n\nres_train.to_csv(res_train_filepath,index=False)\nres_test.to_csv(res_test_filepath,index=False)","9a67e13b":"# read the merged dataset\nmtrain_filepath = \"merged_train.csv\"\nmtest_filepath = \"merged_test.csv\"\n\ntrain = pd.read_csv(mtrain_filepath)\ntest = pd.read_csv(mtest_filepath)\n","150fd730":"train.head(n=4)","70fb5d02":"test.head(n=4)","28d7d2ec":"train['dist_vdc_ward_id'] = train['district_id'].map(str) + train['vdcmun_id'].map(str) + train['ward_id'].map(str)\ntest['dist_vdc_ward_id'] = test['district_id'].map(str) + test['vdcmun_id'].map(str) + test['ward_id'].map(str)\n\ntrain['count_floors_diff'] = train['count_floors_pre_eq'] - train['count_floors_post_eq']\ntest['count_floors_diff'] = test['count_floors_pre_eq'] - test['count_floors_post_eq']\n\ntrain['height_ft_diff'] = train['height_ft_pre_eq'] - train['height_ft_post_eq']\ntest['height_ft_diff'] = test['height_ft_pre_eq'] - test['height_ft_post_eq']\n\ntrain['risk_count'] = train['has_geotechnical_risk_other'] + train['has_geotechnical_risk_liquefaction'] + train['has_geotechnical_risk_landslide'] + train['has_geotechnical_risk_flood'] + train['has_geotechnical_risk_rock_fall'] + train['has_geotechnical_risk_land_settlement'] + train['has_geotechnical_risk_fault_crack']\ntest['risk_count'] = test['has_geotechnical_risk_other'] + test['has_geotechnical_risk_liquefaction'] + test['has_geotechnical_risk_landslide'] + test['has_geotechnical_risk_flood'] + test['has_geotechnical_risk_rock_fall'] + test['has_geotechnical_risk_land_settlement'] + test['has_geotechnical_risk_fault_crack']\n","859efd7d":"from sklearn.preprocessing import Imputer\n\nimput = Imputer(strategy='most_frequent')\nimput1 = Imputer(strategy='mean')\ntrain['count_families'] = imput1.fit_transform(train[['count_families']]).astype('int')\ntrain['has_repair_started'] = imput.fit_transform(train[['has_repair_started']]).astype('int')\n\ntest['has_repair_started'] = imput.fit_transform(test[['has_repair_started']]).astype('int')\n","131868a0":"train['has_geotechnical_risk'] = train['has_geotechnical_risk'].map(int)\ntrain['has_secondary_use'] = train['has_secondary_use'].map(int)\n\ntest['has_geotechnical_risk'] = test['has_geotechnical_risk'].map(int)\ntest['has_secondary_use'] = test['has_secondary_use'].map(int)\ntest['count_families'] = test['count_families'].map(int)\n\ngrade_enc = {\n\t'Grade 1':1,\n\t'Grade 2':2,\n\t'Grade 3':3,\n\t'Grade 4':4,\n\t'Grade 5':5\n}\n\ntrain_y = pd.Series([grade_enc[i] for i in train['damage_grade']])\n","97a19cc9":"train_x = train.drop(columns=['building_id','damage_grade'])\ntest_x = test.drop(columns=['building_id'])","af5fd007":"print(test_x.shape)","8dbfe10f":"from sklearn import preprocessing\n\nlabel_enc = preprocessing.LabelEncoder()\n\nfor col in test_x.columns.values:\n    if test_x[col].dtype == 'object':\n        data = train_x[col].append(test_x[col])\n        label_enc.fit(data)\n        train_x[col] = label_enc.transform(train_x[col])\n        test_x[col] = label_enc.transform(test_x[col])","939b614e":"import time\nfrom sklearn.ensemble import RandomForestClassifier\n\nprev_time = time.time()\n\nclf = RandomForestClassifier(n_jobs=-1,n_estimators=100,max_features=.33)\nclf.fit(train_x,train_y)\n\nnew_time = time.time()\n\ntotal_time = round((new_time - prev_time),2)\nprint('total time : ',total_time)","9000a76e":"import time\n\nprev_time = time.time()\n\npredictions_train = clf.predict(train_x)\npredictions_test = clf.predict(test_x)\n\nnew_time = time.time()\n\ntotal_time = round((new_time - prev_time),2)\nprint('total time : ',total_time)","3210e85e":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\nprint(\"Train Accuracy : \",accuracy_score(train_y,predictions_train))\n\nprint(\"\\nConfusion Matrix : \")\nprint(confusion_matrix(train_y,predictions_train))","4a11229e":"predictions_test = pd.Series(predictions_test)\n\npredict_test = 'Grade ' + predictions_test.map(str)\n\nsubm = {\n    'building_id' : test['building_id'],\n    'damage_grade' : predict_test\n}\nsubmissions = pd.DataFrame(subm)\nsubmissions = submissions.set_index('building_id')\n\nprint(submissions.head())\n\nsubmissions.to_csv('submission.csv')","ce757ad5":"feature engineering","5cb7aba6":"handling null values","b800ec74":"training model using random forest","a821208b":"drop unwanted columns","0c4ef9ef":"evaluating model!","8dd1e762":"predicting target values","d5881b64":"label encoding....\n(running this section multiple times is idempotent)","8e7879f8":"feature transformation","07fb8094":"preparing submission....","abc4e087":"merge the datasets (only have to perform once)"}}