{"cell_type":{"fe4cbfed":"code","f4c566b0":"code","e19a5dce":"code","ca0abf92":"code","e1b941e7":"code","efa22aab":"code","2f4c1470":"code","07e80b33":"code","897b9b1e":"code","1c101ada":"code","74fbda90":"code","43577aa9":"code","196468da":"code","dbb52f74":"code","62bd8f86":"markdown","6ea1666e":"markdown","637d65a3":"markdown","e482e576":"markdown","72161741":"markdown","06abfce1":"markdown","2792840f":"markdown","a9fb9ae5":"markdown","4e7b10df":"markdown"},"source":{"fe4cbfed":"import os\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport os\nfrom glob import glob\nimport seaborn as sns\nfrom PIL import Image\nnp.random.seed(11) # It's my lucky number\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\nfrom sklearn.metrics import accuracy_score\nimport itertools\n\nimport keras\nfrom keras.utils.np_utils import to_categorical # used for converting labels to one-hot-encoding\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras import backend as K\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.applications.resnet50 import ResNet50\nfrom keras import backend as K \n","f4c566b0":"folder_benign_train = '..\/input\/data\/train\/benign'\nfolder_malignant_train = '..\/input\/data\/train\/malignant'\n\nfolder_benign_test = '..\/input\/data\/test\/benign'\nfolder_malignant_test = '..\/input\/data\/test\/malignant'\n\nread = lambda imname: np.asarray(Image.open(imname).convert(\"RGB\"))\n\n# Load in training pictures \nims_benign = [read(os.path.join(folder_benign_train, filename)) for filename in os.listdir(folder_benign_train)]\nX_benign = np.array(ims_benign, dtype='uint8')\nims_malignant = [read(os.path.join(folder_malignant_train, filename)) for filename in os.listdir(folder_malignant_train)]\nX_malignant = np.array(ims_malignant, dtype='uint8')\n\n# Load in testing pictures\nims_benign = [read(os.path.join(folder_benign_test, filename)) for filename in os.listdir(folder_benign_test)]\nX_benign_test = np.array(ims_benign, dtype='uint8')\nims_malignant = [read(os.path.join(folder_malignant_test, filename)) for filename in os.listdir(folder_malignant_test)]\nX_malignant_test = np.array(ims_malignant, dtype='uint8')\n\n# Create labels\ny_benign = np.zeros(X_benign.shape[0])\ny_malignant = np.ones(X_malignant.shape[0])\n\ny_benign_test = np.zeros(X_benign_test.shape[0])\ny_malignant_test = np.ones(X_malignant_test.shape[0])\n\n\n# Merge data \nX_train = np.concatenate((X_benign, X_malignant), axis = 0)\ny_train = np.concatenate((y_benign, y_malignant), axis = 0)\n\nX_test = np.concatenate((X_benign_test, X_malignant_test), axis = 0)\ny_test = np.concatenate((y_benign_test, y_malignant_test), axis = 0)\n\n# Shuffle data\ns = np.arange(X_train.shape[0])\nnp.random.shuffle(s)\nX_train = X_train[s]\ny_train = y_train[s]\n\ns = np.arange(X_test.shape[0])\nnp.random.shuffle(s)\nX_test = X_test[s]\ny_test = y_test[s]","e19a5dce":"# Display first 15 images of moles, and how they are classified\nw=40\nh=30\nfig=plt.figure(figsize=(12, 8))\ncolumns = 5\nrows = 3\n\nfor i in range(1, columns*rows +1):\n    ax = fig.add_subplot(rows, columns, i)\n    if y_train[i] == 0:\n        ax.title.set_text('Benign')\n    else:\n        ax.title.set_text('Malignant')\n    plt.imshow(X_train[i], interpolation='nearest')\nplt.show()","ca0abf92":"y_train = to_categorical(y_train, num_classes= 2)\ny_test = to_categorical(y_test, num_classes= 2)","e1b941e7":"# With data augmentation to prevent overfitting \nX_train = X_train\/255.\nX_test = X_test\/255.","efa22aab":"# See learning curve and validation curve\n\ndef build(input_shape= (224,224,3), lr = 1e-3, num_classes= 2,\n          init= 'normal', activ= 'relu', optim= 'adam'):\n    model = Sequential()\n    model.add(Conv2D(64, kernel_size=(3, 3),padding = 'Same',input_shape=input_shape,\n                     activation= activ, kernel_initializer='glorot_uniform'))\n    model.add(MaxPool2D(pool_size = (2, 2)))\n    model.add(Dropout(0.25))\n\n    model.add(Conv2D(64, kernel_size=(3, 3),padding = 'Same', \n                     activation =activ, kernel_initializer = 'glorot_uniform'))\n    model.add(MaxPool2D(pool_size = (2, 2)))\n    model.add(Dropout(0.25))\n\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu', kernel_initializer=init))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.summary()\n\n    if optim == 'rmsprop':\n        optimizer = RMSprop(lr=lr)\n\n    else:\n        optimizer = Adam(lr=lr)\n\n    model.compile(optimizer = optimizer ,loss = \"binary_crossentropy\", metrics=[\"accuracy\"])\n    return model\n\n# Set a learning rate annealer\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=5, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=1e-7)\n\n","2f4c1470":"input_shape = (224,224,3)\nlr = 1e-5\ninit = 'normal'\nactiv = 'relu'\noptim = 'adam'\nepochs = 50\nbatch_size = 64\n\nmodel = build(lr=lr, init= init, activ= activ, optim=optim, input_shape= input_shape)\n\nhistory = model.fit(X_train, y_train, validation_split=0.2,\n                    epochs= epochs, batch_size= batch_size, verbose=0, \n                    callbacks=[learning_rate_reduction]\n                   )\n                   \n# list all data in history\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","07e80b33":"K.clear_session()\ndel model\ndel history","897b9b1e":"# define 3-fold cross validation test harness\nkfold = KFold(n_splits=3, shuffle=True, random_state=11)\n\ncvscores = []\nfor train, test in kfold.split(X_train, y_train):\n  # create model\n    model = build(lr=lr, \n                  init= init, \n                  activ= activ, \n                  optim=optim, \n                  input_shape= input_shape)\n    \n    # Fit the model\n    model.fit(X_train[train], y_train[train], epochs=epochs, batch_size=batch_size, verbose=0)\n    # evaluate the model\n    scores = model.evaluate(X_train[test], y_train[test], verbose=0)\n    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n    cvscores.append(scores[1] * 100)\n    K.clear_session()\n    del model\n    \nprint(\"%.2f%% (+\/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))","1c101ada":"# Fitting model to all data\nmodel = build(lr=lr, \n              init= init, \n              activ= activ, \n              optim=optim, \n              input_shape= input_shape)\n\nmodel.fit(X_train, y_train,\n          epochs=epochs, batch_size= batch_size, verbose=0,\n          callbacks=[learning_rate_reduction]\n         )\n\n# Testing model on test data to evaluate\ny_pred = model.predict_classes(X_test)\n\nprint(accuracy_score(np.argmax(y_test, axis=1),y_pred))","74fbda90":"# save model\n# serialize model to JSON\nmodel_json = model.to_json()\n\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)\n    \n# serialize weights to HDF5\nmodel.save_weights(\"model.h5\")\nprint(\"Saved model to disk\")\n\n# Clear memory, because of memory overload\ndel model\nK.clear_session()","43577aa9":"input_shape = (224,224,3)\nlr = 1e-5\nepochs = 50\nbatch_size = 64\n\nmodel = ResNet50(include_top=True,\n                 weights= None,\n                 input_tensor=None,\n                 input_shape=input_shape,\n                 pooling='avg',\n                 classes=2)\n\nmodel.compile(optimizer = Adam(lr) ,\n              loss = \"binary_crossentropy\", \n              metrics=[\"accuracy\"])\n\nhistory = model.fit(X_train, y_train, validation_split=0.2,\n                    epochs= epochs, batch_size= batch_size, verbose=2, \n                    callbacks=[learning_rate_reduction]\n                   )\n\n# list all data in history\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\n","196468da":"# Train ResNet50 on all the data\nmodel.fit(X_train, y_train,\n          epochs=epochs, batch_size= epochs, verbose=0,\n          callbacks=[learning_rate_reduction]\n         )\n","dbb52f74":"# Testing model on test data to evaluate\ny_pred = model.predict(X_test)\nprint(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)))\n\n# save model\n# serialize model to JSON\nresnet50_json = model.to_json()\n\nwith open(\"resnet50.json\", \"w\") as json_file:\n    json_file.write(resnet50_json)\n    \n# serialize weights to HDF5\nmodel.save_weights(\"resnet50.h5\")\nprint(\"Saved model to disk\")","62bd8f86":"# Step 7: Testing the model\n\nFirst the model has to be fitted with all the data, such that no data is left out.","6ea1666e":"# Step 3: Categorical Labels\nTurn labels into one hot encoding","637d65a3":"# Step 8: ResNet50\nThe CNN above is not a very sophisticated model, thus the resnet50, is also tried","e482e576":"# Step 4 : Normalization\nNormalize all Values of the pictures by dividing all the RGB values by 255","72161741":"# Step 6: Cross-Validating Model\n","06abfce1":"##### Mole Classifier Kernel\nSkin cancer is the most common human malignancy, is primarily diagnosed visually, beginning with an initial clinical screening and followed potentially by dermoscopic analysis, a biopsy and histopathological examination. Automated classification of skin lesions using images is a challenging task owing to the fine-grained variability in the appearance of skin lesions.\n\nThe dataset is taken from the ISIC (International Skin Image Collaboration) Archive. It consists of 1800 pictures of benign moles and 1497 pictures of malignant classified moles. The pictures have all been resized to low resolution (224x224x3) RGB. The task of this kernel is to create a model, which can classify a mole visually into benign and malignant. \n\nAs the dataset is pretty balanced, the model will be tested on the accuracy score, thus (TP + TN)\/(ALL).\n\nIt has 2 different classes of skin cancer which are listed below :<br>\n**1. Benign <br>**\n**2. Malignant <br>**\n\nIn this kernel I will try to detect 2 different classes of moles using Convolution Neural Network with keras tensorflow in backend and then analyse the result to see how the model can be useful in practical scenario.<br>\n\nIn this kernel I have followed following 14 steps for model building and evaluation which are as follows : <br>\n**Step 1 : Importing Essential Libraries**<br>\n**Step 2: Loading pictures and making Dictionary of images and labels** <br>\n**Step 3: Categorical Labels** <br>\n**Step 4: Normalization** <br>\n**Step 5: Train and Test Split** <br>\n**Step 6: Model Building **<br>\n**Step 7: Cross-validating model**<br>\n**Step 8: Testing model**<br>\n**Step 9: ResNet50** <br>","2792840f":"# Step 1 : importing Essential Libraries","a9fb9ae5":"# Step 2 : Loading pictures and making Dictionary of images and labels\nIn this step I load in the pictures and turn them into numpy arrays using their RGB values. As the pictures have already been resized to 224x224, there's no need to resize them. As the pictures do not have any labels, these need to be created. Finally, the pictures are added together to a big training set and shuffeled.","4e7b10df":"# Step 5: Model Building \n## CNN\nI used the Keras Sequential API, where you have just to add one layer at a time, starting from the input.\n\nThe first is the convolutional (Conv2D) layer. It is like a set of learnable filters. I choosed to set 64 filters for the two firsts conv2D layers. Each filter transforms a part of the image (defined by the kernel size) using the kernel filter. The kernel filter matrix is applied on the whole image. Filters can be seen as a transformation of the image.\n\nThe CNN can isolate features that are useful everywhere from these transformed images (feature maps).\n\nThe second important layer in CNN is the pooling (MaxPool2D) layer. This layer simply acts as a downsampling filter. It looks at the 2 neighboring pixels and picks the maximal value. These are used to reduce computational cost, and to some extent also reduce overfitting. We have to choose the pooling size (i.e the area size pooled each time) more the pooling dimension is high, more the downsampling is important.\n\nCombining convolutional and pooling layers, CNN are able to combine local features and learn more global features of the image.\n\nDropout is a regularization method, where a proportion of nodes in the layer are randomly ignored (setting their wieghts to zero) for each training sample. This drops randomly a propotion of the network and forces the network to learn features in a distributed way. This technique also improves generalization and reduces the overfitting.\n\n'relu' is the rectifier (activation function max(0,x). The rectifier activation function is used to add non linearity to the network.\n\nThe Flatten layer is use to convert the final feature maps into a one single 1D vector. This flattening step is needed so that you can make use of fully connected layers after some convolutional\/maxpool layers. It combines all the found local features of the previous convolutional layers.\n\nIn the end i used the features in one fully-connected (Dense) layer which is just artificial an neural networks (ANN) classifier."}}