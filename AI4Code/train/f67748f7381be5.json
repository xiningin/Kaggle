{"cell_type":{"454e7540":"code","a240630a":"code","39f79f08":"code","b6640b5b":"code","03deceb7":"code","291d763d":"code","55817ed8":"code","215955c7":"code","af4d4ae5":"code","898bb2a9":"code","b149c561":"code","40a06d61":"code","517b9d3e":"code","c592dea2":"markdown","61c5d7e7":"markdown","d1a815fa":"markdown","c3184535":"markdown","44b6b797":"markdown","543a989d":"markdown","daa164ee":"markdown","cfceb92f":"markdown","fb88f7fb":"markdown","41adfe6f":"markdown","d7a5a9b3":"markdown","40790a6b":"markdown","27edf9a2":"markdown","afd69e5f":"markdown","aa852ea8":"markdown","4bc8950d":"markdown","6aae2657":"markdown","b9f3ac52":"markdown","1edb8ff1":"markdown","ab571068":"markdown","a1f2ee20":"markdown","6166ab68":"markdown","a7a5e7f7":"markdown","68d8f0b0":"markdown","b6c4deb9":"markdown","33668f53":"markdown","99691fc6":"markdown"},"source":{"454e7540":"# libraries\nimport glob\nimport re\n\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nfrom wordcloud import WordCloud, STOPWORDS\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# read data\n\n## products dataset\npath = '..\/input\/learnplatform-covid19-impact-on-digital-learning\/'\nproducts_df = pd.read_csv(path + \"products_info.csv\")\nproducts_df.columns = [x.lower().replace(' ','_') for x in products_df.columns]\n\n## districts dataset\ndistricts_df = pd.read_csv(path +\"districts_info.csv\")\n#districts_df.state = districts_df.state.replace('District Of Columbia','District of Columbia')\n\n## engagement dataset\npath = '..\/input\/learnplatform-covid19-impact-on-digital-learning\/engagement_data\/' \nall_files = glob.glob(path + \"\/*.csv\")\n\nli = []\n\nfor filename in all_files:\n    df = pd.read_csv(filename, index_col=None, header=0)\n    district_id = filename.split(\"\/\")[-1].split(\".\")[0]\n    df[\"district_id\"] = district_id\n    li.append(df)\nengagement_df = pd.concat(li)\nengagement_df = engagement_df.reset_index(drop=True)\n\n# summary\n\ndf_list = [\n    districts_df,\n    products_df,\n    engagement_df\n]\n\ndf_name = [\n    'districts_df',\n    'products_df',\n    'engagement_df'\n]\n\ncols = [\n    'dataframe',\n    'column', \n    'dtype', \n    'Non-Null Count', \n    'Null Count',\n    'unique'\n    \n]\n\nframes=[]\n\n\nfor i in range(len(df_list)):\n    df = df_list[i].copy()\n    a = df.dtypes.reset_index().rename(columns = {'index':'column',0:'dtype'})\n    b = df.count().reset_index().rename(columns = {'index':'column',0:'Non-Null Count'})\n    c = df.isnull().sum().reset_index().rename(columns = {'index':'column',0:'Null Count'})\n    temp = a.merge(b,on = 'column').merge(c,on = 'column')\n    \n    dct = {col: len(df[col].unique()) for col in df.columns}\n    df_unique = pd.DataFrame({\n    'column':dct.keys(),\n    'unique':dct.values(),\n    })\n    temp = temp.merge(df_unique,on = 'column')\n    temp['dataframe'] = df_name[i]\n    frames.append(temp)","a240630a":"# products_df\n\nproducts_df['primary_function_main'] = products_df['primary_essential_function'].apply(lambda x: x.split(' - ')[0] if x == x else x)\nproducts_df['primary_function_sub'] = products_df['primary_essential_function'].apply(lambda x: x.split(' - ')[1] if x == x else x)\n\n# Synchronize similar values\nproducts_df['primary_function_sub'] = products_df['primary_function_sub'].replace({'Sites, Resources & References' : 'Sites, Resources & Reference'})\n#products_df.drop(\"Primary Essential Function\", axis=1, inplace=True)\n\ntemp_sectors = products_df['sector(s)'].str.get_dummies(sep=\"; \")\ntemp_sectors.columns = [f\"sector_{re.sub(' ', '', c)}\" for c in temp_sectors.columns]\nproducts_df = products_df.join(temp_sectors)\n#products_df.drop(\"Sector(s)\", axis=1, inplace=True)\n\n#del temp_sectors\n\n# engagement_df\n\nengagement_df['time'] = pd.to_datetime(engagement_df['time'])\n\ntemp = engagement_df[['time']].drop_duplicates('time')\ntemp['week'] = temp['time'].apply(lambda x: x.isocalendar()[1])\nengagement_df = engagement_df.merge(temp,on ='time')\n\nengagement_df['lp_id'] = engagement_df['lp_id'].fillna(-1).astype(int)\nengagement_df['district_id'] = engagement_df['district_id'].fillna(-1).astype(int)\n\nengagement_df_mix = engagement_df.merge(\n    districts_df[['district_id','state']],\n    on = 'district_id'\n)\nengagement_df_mix = engagement_df_mix.merge(\n    products_df[['lp_id','product_name','sector_Corporate', 'sector_HigherEd','sector_PreK-12']],\n    on = 'lp_id'\n)","39f79f08":"# map plot: districts\n\nus_state_abbrev = {\n    'Alabama': 'AL',\n    'Alaska': 'AK',\n    'American Samoa': 'AS',\n    'Arizona': 'AZ',\n    'Arkansas': 'AR',\n    'California': 'CA',\n    'Colorado': 'CO',\n    'Connecticut': 'CT',\n    'Delaware': 'DE',\n    'District Of Columbia': 'DC',\n    'Florida': 'FL',\n    'Georgia': 'GA',\n    'Guam': 'GU',\n    'Hawaii': 'HI',\n    'Idaho': 'ID',\n    'Illinois': 'IL',\n    'Indiana': 'IN',\n    'Iowa': 'IA',\n    'Kansas': 'KS',\n    'Kentucky': 'KY',\n    'Louisiana': 'LA',\n    'Maine': 'ME',\n    'Maryland': 'MD',\n    'Massachusetts': 'MA',\n    'Michigan': 'MI',\n    'Minnesota': 'MN',\n    'Mississippi': 'MS',\n    'Missouri': 'MO',\n    'Montana': 'MT',\n    'Nebraska': 'NE',\n    'Nevada': 'NV',\n    'New Hampshire': 'NH',\n    'New Jersey': 'NJ',\n    'New Mexico': 'NM',\n    'New York': 'NY',\n    'North Carolina': 'NC',\n    'North Dakota': 'ND',\n    'Northern Mariana Islands':'MP',\n    'Ohio': 'OH',\n    'Oklahoma': 'OK',\n    'Oregon': 'OR',\n    'Pennsylvania': 'PA',\n    'Puerto Rico': 'PR',\n    'Rhode Island': 'RI',\n    'South Carolina': 'SC',\n    'South Dakota': 'SD',\n    'Tennessee': 'TN',\n    'Texas': 'TX',\n    'Utah': 'UT',\n    'Vermont': 'VT',\n    'Virgin Islands': 'VI',\n    'Virginia': 'VA',\n    'Washington': 'WA',\n    'West Virginia': 'WV',\n    'Wisconsin': 'WI',\n    'Wyoming': 'WY'\n}\n\ndistricts_df['state_abbrev'] = districts_df['state'].replace(us_state_abbrev)\ndistricts_info_by_state = districts_df['state_abbrev'].value_counts().to_frame().reset_index(drop=False)\ndistricts_info_by_state.columns = ['state_abbrev', 'num_districts']\n\ntemp = pd.DataFrame({\n    'state_abbrev':us_state_abbrev.values(),\n})\n\ntemp = temp.merge(districts_info_by_state,on='state_abbrev',how='left').fillna(0)\ntemp['num_districts'] = temp['num_districts'].astype(int)\n\nfig = go.Figure()\nlayout = dict(\n    title_text = \"Number of Available School Districts per State\",\n    title_font_color=\"black\",\n    geo_scope='usa',\n)    \n\n\nfig.add_trace(\n    go.Choropleth(\n        locations=temp.state_abbrev,\n        zmax=1,\n        z = temp.num_districts,\n        locationmode = 'USA-states', # set of locations match entries in `locations`\n        marker_line_color='black',\n        geo='geo',\n        colorscale=px.colors.sequential.Greys, \n        \n    )\n)\n            \nfig.update_layout(layout)   \nfig.show()","b6640b5b":"# bar plot: districts\n\nplt.style.use('default')\nplt.figure(figsize=(14,8))\n\nplotting = sns.countplot(\n    y=\"state\",\n    data=districts_df,\n    order=districts_df.state.value_counts().index,\n    palette=\"Greys_d\",\n    linewidth=3\n)\n\nfor container in plotting.containers:\n    plotting.bar_label(container,fontsize=16)\n\n#Text\nplotting.text(x = -5, y = -4.2, s = \"State Distribution\",fontsize = 24, weight = 'bold', alpha = .90);\nplotting.text(x = -5, y = -3, s = \"Distribution of United States\",fontsize = 16, alpha = .85)\nplotting.text(x = 31.2, y = 0.08, s = 'Highest', weight = 'bold',fontsize = 14)\nplotting.text(x = 1.7, y = 22.3, s = 'Lowest', weight = 'bold',fontsize = 14)\n\nplt.yticks(fontsize=14)\nplt.xticks(fontsize=14)\n\n\n\nplt.show()","03deceb7":"# heatmap: districts -> locale\n\ntemp = districts_df.groupby('locale').pp_total_raw.value_counts().to_frame()\ntemp.columns = ['amount']\n\ntemp = temp.reset_index(drop=False)\n\ntemp = temp.pivot(index='locale', columns='pp_total_raw')['amount']\ntemp = temp[['[4000, 6000[', '[6000, 8000[', '[8000, 10000[', '[10000, 12000[',\n       '[12000, 14000[', '[14000, 16000[', '[16000, 18000[', \n       '[18000, 20000[', '[20000, 22000[', '[22000, 24000[', ]]\n\n\ntemp1 = districts_df.groupby('locale')['pct_black\/hispanic'].value_counts().to_frame()\ntemp1.columns = ['amount']\n\ntemp1 = temp1.reset_index(drop=False)\ntemp1 = temp1.pivot(index='locale', columns='pct_black\/hispanic')['amount']\n\ntemp2 = districts_df.groupby('locale')['pct_free\/reduced'].value_counts().to_frame()\ntemp2.columns = ['amount']\n\ntemp2 = temp2.reset_index(drop=False)\n\ntemp2 = temp2.pivot(index='locale', columns='pct_free\/reduced')['amount']\n\nplt.style.use('default')\n\nfig, [[ax1, ax2], [ax3, ax4]] = plt.subplots(nrows=2, ncols=2, figsize=(24,18))\n\nsns.countplot(data=districts_df, x='locale', ax=ax1, palette='Greys_d')\nax1.text(x = -0.5, y = 120, s = \"Locale Distribution\",fontsize = 24, weight = 'bold', alpha = .90);\nax1.xaxis.set_tick_params(labelsize=16)\n\nfor container in ax1.containers:\n    ax1.bar_label(container,fontsize=16)\n\nsns.heatmap(temp1.fillna(0), annot=True,  cmap='Greys', ax=ax2,annot_kws={\"fontsize\":14})\nax2.set_title('Heatmap: locale and pct_black\/hispanic',fontsize=16,loc='left')\nax2.xaxis.set_tick_params(labelsize=16)\nax2.yaxis.set_tick_params(labelsize=16)\n\nsns.heatmap(temp.fillna(0), annot=True,  cmap='Greys', ax=ax3,annot_kws={\"fontsize\":14})\nax3.set_title('Heatmap: locale and pp_total_raw',fontsize=16,loc='left')\nax3.xaxis.set_tick_params(labelsize=16)\nax3.yaxis.set_tick_params(labelsize=16)\n\nsns.heatmap(temp2.fillna(0), annot=True,  cmap='Greys', ax=ax4,annot_kws={\"fontsize\":14})\nax4.set_title('Heatmap: locale and pct_free\/reduced',fontsize=16,loc='left')\nax4.xaxis.set_tick_params(labelsize=16)\nax4.yaxis.set_tick_params(labelsize=16)\n\n\nplt.show()","291d763d":"plt.style.use('default')\nnames = ['sector_Corporate', 'sector_HigherEd','sector_PreK-12']\ncounts = [products_df[x].sum() for x in names]\n\ntemp_bar = pd.DataFrame({\n    'sector':names,\n    'count':counts\n}).sort_values('count',ascending = False)\n\ntemp = products_df.groupby('primary_function_main')[names].sum()\n\n#fig, [ax1, ax2 ]= plt.subplots(nrows=1, ncols=2, figsize=(12,6))\nplt.figure(figsize=(18,18))\n\nplt.subplot(3,2,1)\nax = sns.barplot(x=\"sector\", y=\"count\", data=temp_bar,palette ='Greys_d')\nfor container in ax.containers:\n    ax.bar_label(container,fontsize=12)\n\nplt.subplot(3,2,2)\n\nsns.heatmap(temp.T, annot=True,  cmap='Greys',annot_kws={\"fontsize\":10},fmt='g')\n\nplt.text(x = -6, y = -0.25, s = \"Sectors Distribution\",fontsize = 18, weight = 'bold', alpha = .90);\n\nplt.show()","55817ed8":"# pieplot: products\n\ncolor = [\n    'darkgray',\n    'silver',\n    'lightgray',\n    'gainsboro',\n]\n\nproducts_df[\"primary_function_main\"].value_counts().plot(\n    kind = 'pie', \n    autopct='%1d%%', \n    figsize=(6,6), \n    colors=color,\n    wedgeprops={\"edgecolor\":\"k\",'linewidth': 0.8,},\n    textprops={'color':\"black\"},\n    startangle=0)\nplt.text(x = -1.4, y = 1.1, s = \"Categories\",fontsize = 18, weight = 'bold', alpha = .90);\nplt.show()","215955c7":"# pieplot: products -> subcategories\n\nplt.style.use('default')\nplt.figure(figsize=(18,8))\n\ntemp = products_df[products_df.primary_function_main == 'LC']\nax = sns.countplot(\n    data=temp, \n    y='primary_function_sub',\n    order=temp.primary_function_sub.value_counts().index,\n    palette ='Greys_d'\n)\n\nfor container in ax.containers:\n    ax.bar_label(container,fontsize=16)\n\n\n#plt.title('Sub-Categories in Primary Function LC')\nplt.text(x = -50, y = -0.8, \n         s = \"Sub-Categories in Primary Function LC\",fontsize = 24, weight = 'bold', alpha = .90);\n\nplt.text(x = 105, y =0.08, s = 'Highest', weight = 'bold',fontsize=16)\nplt.text(x = 7, y = 6, s = 'Lowest', weight = 'bold',fontsize=16)\nplt.yticks(fontsize=16)\nplt.xticks(fontsize=16)\n\nplt.show()","af4d4ae5":"dct = {\n    'Savvas Learning Company | Formerly Pearson K12 Learning': 'Savvas Learning Company'\n}\n\ntemp = products_df['provider\/company_name'].value_counts().reset_index()\ntemp.columns = ['provider\/company_name','count']\ntemp = temp.replace( {\n    'Savvas Learning Company | Formerly Pearson K12 Learning': 'Savvas Learning Company'\n})\n\nn = 15\ntemp = temp.sort_values('count',ascending = False).head(n)\n\nplt.style.use('default')\nplt.figure(figsize=(18,8))\n\nax = sns.barplot(\n    data=temp, \n    y='provider\/company_name',\n    x='count',\n    palette ='Greys_d'\n)\n\nfor container in ax.containers:\n    ax.bar_label(container,fontsize=15)\n    \nplt.text(x = -7, y = -1, \n         s = f\"Top {n} provider\/company name\",fontsize = 20, weight = 'bold', alpha = .90);\n   \nplt.text(x = 31, y =0.08, s = 'Highest', weight = 'bold',fontsize=16)\nplt.text(x = 3, y = 14.2, s = 'Lowest', weight = 'bold',fontsize=16)\nplt.yticks(fontsize=16)\n\n\nplt.yticks(fontsize=16)\nplt.xticks(fontsize=16)\nplt.show()","898bb2a9":"cloud = WordCloud(\n    width=1080,\n    height=270,\n    colormap='Greys',\n    background_color='white'\n    ).generate(\" \".join(products_df['product_name'].astype(str)))\n\nplt.figure(figsize=(22, 10))\nplt.imshow(cloud)\nplt.axis('off');","b149c561":"group_01 = (engagement_df_mix.groupby('product_name')['engagement_index'].mean()\/1000).reset_index().sort_values('engagement_index',ascending = False)\ngroup_01['engagement_index'] = group_01['engagement_index'].apply(lambda x: round(x,2))\nless_1 = len(group_01.loc[lambda x:x['engagement_index']<1])\n\n\nplt.style.use('default')\nplt.figure(figsize=(14,8))\n\nplotting = sns.barplot(\n    y=\"product_name\",\n    x = \"engagement_index\",\n    data=group_01.head(20),\n    palette=\"Greys_d\",\n\n)\n\nfor container in plotting.containers:\n    plotting.bar_label(container,fontsize=14)\n\nplt.text(x = -3.5, y = -3, \n         s = \"Mean daily page-load events in top 20 tools\",fontsize = 20, weight = 'bold', alpha = .90);\n\nplt.text(x = -3.5, y = -2, \n         s = \"per 1 student\",fontsize = 14,  alpha = .90);\n\nplt.text(x = 11, y =0.1, s = 'Highest', weight = 'bold',fontsize=14)\nplt.text(x = 1, y = 19.2, s = 'Lowest', weight = 'bold',fontsize=14)\nplt.yticks(fontsize=16)\nplt.xticks(fontsize=16)\nplt.show()","40a06d61":"col = 'week'\n\ngroup_04  = (engagement_df_mix.groupby(['product_name',col])['engagement_index'].mean()\/1000).reset_index().sort_values('engagement_index',ascending = False)\n\ng_high = group_01.head(3)['product_name']\ngroup_04_top = group_04.loc[lambda x: x.product_name.isin(g_high)]\n\nstates = group_04['product_name'].unique()\ntimes= group_04[col].unique()\n\nindex = pd.MultiIndex.from_product([states,times], names = [\"product_name\", col])\n\ndf_complete = pd.DataFrame(index = index).reset_index().fillna(0)\n\ngroup_04 = df_complete.merge(group_04,on = ['product_name',col],how='left').fillna(0)\n\nn = 3\ng_high = group_04.groupby('product_name')['engagement_index'].sum().sort_values(ascending=False).head(n).index.to_list()\n\n\ncolors = [    \n    'lightgray', \n    'dimgray', \n    'black', \n    'firebrick', \n    'darkred']\npalette_01 = {x:'lavender' for x in group_04['product_name'].unique() if x not in g_high}\npalette_02 = {g_high[i]:colors[i] for i in range(n)}\n\nplt.style.use('default')\nplt.figure(figsize=(20,6))\n\n\nsns.lineplot(\n    data=group_04.loc[lambda x: ~x.product_name.isin(g_high)], \n    x=col, \n    y=\"engagement_index\", \n    hue='product_name',\n    legend = False,\n    palette=palette_01,\n    linewidth = 1.\n\n    )\n\nsns.lineplot(\n    data=group_04.loc[lambda x: x.product_name.isin(g_high)], \n    x=col, \n    y=\"engagement_index\", \n    hue='product_name',\n    palette=palette_02,\n    linewidth = 1.\n\n    )\n\n\nplt.text(x = -2, y =23.7, s = 'Mean daily page-load events in top 3 tools', weight = 'bold',fontsize=14)\nplt.text(x = -2, y =22.3, s = 'by products and time, per 1 student',fontsize=12)\n\n\nplt.text(x = 12, y =20.7, s = '1,000 cases of COVID', weight = 'bold',fontsize=8)\nplt.text(x = 37, y =20.7, s = '1st September', weight = 'bold',fontsize=8)\n\n\nplt.axvline(x = 11, color = 'black', linestyle='--',linewidth = 0.5)\nplt.axvline(x = 36, color = 'black', linestyle='--',linewidth = 0.5)\nplt.show()","517b9d3e":"group_02 = (engagement_df_mix.groupby(['state','product_name'])['engagement_index'].mean()\/1000)\\\n            .reset_index().sort_values('engagement_index',ascending = False).fillna(0)\n\ngripo_02_top = group_02.loc[lambda x: x.product_name.isin(g_high)]\ngripo_02_top['engagement_index'] = gripo_02_top['engagement_index'].apply(lambda x: round(x,2))\n#gripo_02_top = gripo_02_top.loc[lambda x: x['engagement_index']>0]\n\n\nplt.style.use('default')\n\ng = sns.FacetGrid(gripo_02_top,hue='product_name',col = 'product_name',height=4, col_wrap= 3  )\ng.map(sns.barplot, \"engagement_index\",\"state\", palette=\"Greys_d\",)\n\ng.fig.set_size_inches(15, 8)\ng.fig.subplots_adjust(top=0.81, right=0.86)\n\naxes = g.axes.flatten()\nfor ax in axes:\n    for container in ax.containers:\n        ax.bar_label(container,fontsize=8)\n\n\nplt.text(x = -50, y = -4, s = \"Mean daily page-load events in top 3 tools\",fontsize = 16, weight = 'bold', alpha = .90);\nplt.text(x = -50, y = -3, s = \"by state and products, per 1 student\",fontsize = 14,  alpha = .90);\nplt.show()","c592dea2":"<img src=\"https:\/\/gitlab.com\/FAAM\/learnplatform\/-\/raw\/master\/images\/2.png\"  width=\"400\" height=\"200\" alt=\"centered image\" >","61c5d7e7":"## References\n\n* [Diverging Color Maps for Scientific Visualization (Expanded) - Kenneth Moreland](https:\/\/www.kennethmoreland.com\/color-maps\/ColorMapsExpanded.pdf)\n\n* [Kaggle Competitions](https:\/\/www.kaggle.com\/c\/learnplatform-covid19-impact-on-digital-learning):\n    * [Enthusiast to Data Professional - What changes?](https:\/\/www.kaggle.com\/spitfire2nd\/enthusiast-to-data-professional-what-changes\/)\n    * [How To Approach Analytics Challenges](https:\/\/www.kaggle.com\/iamleonie\/how-to-approach-analytics-challenges)\n    * [Most popular tools in 2020 Digital Learning](https:\/\/www.kaggle.com\/michau96\/most-popular-tools-in-2020-digital-learning)","d1a815fa":"### Visualization: Locales\n\nLocales are separated into 4 categories: **Suburb**,**Rural**, **City** and **Town**, where most of the locales are concentrated in the *Suburb* category (104).\n\nFor the `pct_black\/hispanic` variable, *Rural* and *Town* categories concentrate their entire population close to the interval $ [0,0.2 [$, while for the others sectors this percentage is varied.\n\nFor `pctfree\/reduced` and `pp_total_raw` indicators, the distribution for each location is different, although they tend to focus on a particular interval.\n","c3184535":"* **Summary**:","44b6b797":"## Overview of the Dataset\n\nThe objective of this section is to be able to read and give an interpretation to each one of the available datasets, analyzing column by column. For each dataset we will make a brief description:\n\n* **File**: File name (`.csv`).\n* **Shape**: Dimensionality of datasets.\n* **Description**: Basic description of the dataset.\n* **Top 5 rows**: Show first 5 rows + explanation for some columns.\n* **Summary**: Summary of datasets.","543a989d":"### Visualization: Products\n\nAfter understanding the functionality of each of the tools, it is necessary to understand the distribution of the tools. The first thing is to study the distribution of the providers of the products we have, where:\n\n* 258 providers have 1 occurrences.\n* 18 providers have 2 occurrences.\n* 9 providers have 3 occurrences.\n* 2 providers have 4 occurrences.\n* 2 providers have 6 occurrences.\n* 1 provider have 30 occurrences.\n\n\n\nBased on this, only the top 15 providers will be displayed.","daa164ee":"## EDA\nExploratory data analysis is the most important part of the challenge, since this will make the difference between the winner and the other participants. You should keep in mind that your visualizations must be able to simply and easily summarize the datasets. Also, it is hoped that the proposed visualizations can help to understand behaviors that are not easy to analyze with a simple table.\n\nVisualizations will be made in [matplotlib](https:\/\/matplotlib.org\/), [seaborn](https:\/\/seaborn.pydata.org\/) y [plotly](https:\/\/plotly.com\/python\/). Based on the article by [*Diverging Color Maps for Scientific Visualization (Expanded) - Kenneth Moreland*](https:\/\/www.kennethmoreland.com\/color-maps\/ColorMapsExpanded.pdf), we will occupy `Grays` scale  next to the technique: dark text on a light background.\n\n\n> **Note**: Visualizations made on this notebook are static. You can use different tools to be able to make dynamic visualizations ([Altair](https:\/\/altair-viz.github.io\/), [plotly](https:\/\/plotly.com\/python\/), etc.). You can also perform tools like [Streamlit](https:\/\/streamlit.io\/) to make Dashboards. On the other hand, if you fully understand python visualization tools and have knowledge of HTML\/CSS, you can make beautiful notebook presentations like [this one](https:\/\/www.kaggle.com\/spitfire2nd\/enthusiast-to-data-professional-what-changes).","cfceb92f":"Let's study the temporal behavior (at the level of weeks) of these tools during the year 2020, where the most  three used tools will be shown with different colors, while the other tools will be visualized but with the same color (in order to understand their distribution).\n\n> **Note**: The proposed analysis can be carried out at the day level and analyzing through time series each of the tools during the year 2020.","fb88f7fb":"<img src=\"https:\/\/gitlab.com\/FAAM\/learnplatform\/-\/raw\/master\/images\/districts.png\"  class=\"center\" >","41adfe6f":"### 2. Products\n\n* **File**: `products_info.csv`\n* **Shape**: $372$ rows $\\times$  $6$ columns.\n* **Description**: for each school district, there is an additional file that contains the engagement for each tool for everyday in 2020. \n* **Top 5 rows:**:\n<img src=\"https:\/\/gitlab.com\/FAAM\/learnplatform\/-\/raw\/master\/images\/products.png\" >","d7a5a9b3":"### Visualization: Districts\n\nFirst of all, I am interested how diverse the available school districts are. As you can see in below plot, the available data does not cover all the states in the U.S. . The states with the most available school districts are CT (30) and UT (29) while there are also states with only one school district (FL, TN, NY, AZ).\n","40790a6b":"* **Summary**:","27edf9a2":"<img src=\"https:\/\/gitlab.com\/FAAM\/learnplatform\/-\/raw\/master\/images\/4.png\" >","afd69e5f":"### Visualization:  primary_function_main\n\nContinuing the analysis of the `primary_function_main` variable, it was observed that most of these are in the` LC` category (77%). Within this category, its subcategory is analyzed, where the predominant subcategory is `Sites, Resources & Reference` (101).","aa852ea8":"With regard to products, there are about 372 different products.\n\nWe can make a word cloud to be able to analyze in a different way, words by themselves that are repeated the most in the `product_name` variable.","4bc8950d":"\n\nMain objective is understand of the best way the challenge [LearnPlatform COVID-19 Impact on Digital Learning](https:\/\/www.kaggle.com\/c\/learnplatform-covid19-impact-on-digital-learning\/overview) proposed by [Kaggle](https:\/\/www.kaggle.com\/).\n\nThe steps to follow are:\n\n* **Overview of the Dataset**: Understanding the datasets available.\n* **Preprocessing**: Preprocessing of the datasets available.\n* **EDA**: Exploratory data analysis using visualization tools in Python.\n\n\n\n> **Note**: My analysis is inspired by several of the notebooks that different profiles have uploaded to the challenge, so some graphics or images belong to these authors. The most important ones will be found in the references. On the other hand,  my project is available in [Jupyter Book](https:\/\/jupyterbook.org\/intro.html), click in the following [link](https:\/\/faam.gitlab.io\/learnplatform\/learnplatform\/index.html).","6aae2657":"## Summary\n* Depending on what you want to achieve you might want to carefully preselect districts. Note that we approach in this notebook might not necessarily suit your individual purposes.\n* When looking at digital learning, you might want to spend sometime in figuring out which districts actually applied digital learning","b9f3ac52":"To understand more in detail the use of these products, we will analyze the use of these products with respect to the variable `engagement_index`. The first graph is related to the average `engagement_index` (per student) for the year 2020, where the first 15 products will be displayed.\n\nAn important fact is that 362 products have an average of less than 1!.\n","1edb8ff1":"# Basic analysis:  COVID-19 Impact on Digital Learning","ab571068":"### 1.  Districts\n\n* **File**: `districts_info.csv`.\n* **Shape**: $233$ rows $\\times$  $7$ columns.\n* **Description**: file contains information about each school district.\n* **Top 5 rows:**:","a1f2ee20":"### Visualization: Sectors\n\nSectors are separated into 3 categories: **sector_Corporate**, **sector_HigherEd** and **sector_PreK-12**, donde la categor\u00eda mayoritaria corresponde a *sector_PreK-12* (350). On the other hand, analyzing the `primary_function_main` variable, all sectors are focused on the` LC` category. It is worth mentioning that the distribution of the other categories remains almost the same between sectors.","6166ab68":"<img src=\"https:\/\/gitlab.com\/FAAM\/learnplatform\/-\/raw\/master\/images\/1.png\" width=\"400\" height=\"200\" alt=\"centered image\" >","a7a5e7f7":"## Preprocessing\n\nPreprocessing is an important step in any analytics competition. It helps you to handle your data more efficiently. However, please note that the way I preprocess the data may not be suited for your analysis purposes. Therefore, before you begin preprocessing your data, think about which data you would like to keep and\/or modify and which data is not relevant for your analysis.\n\n* one-hot encoding the product sectors\n* splitting up the primary essential function into main and sub category\n\n> **Note**: Preprocessing varies if you see other notebooks of this challenge.\nThe processing will depend on the understanding of each of the datasets and the extra information that you may have.","68d8f0b0":"* **Summary**:","b6c4deb9":"Now, we can understand the `engagement index` for the most important tools about districts, where the districts of * Wisconsin *, * Missouri * and * Virginia * have the highest `engagement index` among the three most used tools.","33668f53":"<img src=\"https:\/\/gitlab.com\/FAAM\/learnplatform\/-\/raw\/master\/images\/3.png\"  width=\"400\" height=\"200\" alt=\"centered image\">","99691fc6":"### 3. Engagement\n* **File**: `engagement_data\/*.csv`.\n* **Shape**: $22324190$ rows $\\times$  $5$ columns.\n* **Description**: file contains information about each school district. The files can be joined by the key columns `district_id` and `lp_id`.\n* **Top 5 rows:**:\n<img src=\"https:\/\/gitlab.com\/FAAM\/learnplatform\/-\/raw\/master\/images\/engagement.png\">"}}