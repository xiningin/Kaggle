{"cell_type":{"699a24fe":"code","6c76cfee":"code","89d9dbaf":"code","3038588b":"code","01756277":"code","ef82c00d":"code","50ddf508":"code","e6221e98":"code","2d869c6a":"code","4af75f81":"code","33372059":"code","6351ea07":"code","172ff63d":"code","8217365f":"code","a9e94e39":"code","7f338dc9":"code","7bbbd258":"code","81841d60":"code","70b697ee":"code","4b50e511":"code","6270b444":"code","d904e11b":"code","e7c36a06":"code","9075707c":"code","52755f59":"code","522c4bc8":"code","406a945f":"code","984b1454":"code","f812f4d8":"code","8b201516":"code","ff972a9c":"code","7db0f1e9":"code","f6bc0aca":"code","d2265b8f":"code","e7d11d76":"code","f6569518":"code","66a0e64a":"code","5741f730":"code","25602600":"code","3f6046cf":"code","da6c86bb":"code","735de368":"code","0d271b2a":"code","20b797c1":"code","22319e62":"code","6a92266b":"code","c401f64b":"code","0c305dc7":"code","bed28b03":"code","f58bc690":"code","2f9c9885":"code","1e5ecc96":"code","896652f6":"code","d2db3f92":"code","fcc837e9":"code","b79a3dbf":"code","a47d6560":"code","c428a6c1":"code","d7c7480c":"code","7b250cd7":"code","2811c2bc":"code","4a1b87ea":"code","fa62fab8":"code","458dc7ed":"code","4a95bdc0":"code","8c5a4a9d":"code","0da3316b":"code","403e3cbb":"code","4fef51e7":"code","f594d1b4":"code","0974f26e":"markdown","9f275a9d":"markdown","6a1fdd1e":"markdown","7d3730f7":"markdown","fd77be0b":"markdown","241020c0":"markdown","0cfdedd6":"markdown","34ef3516":"markdown","cbe09b92":"markdown","42913341":"markdown","6d48e886":"markdown","e33cf3c2":"markdown","ee3ba4e3":"markdown","c7e8e3fe":"markdown","90c0675a":"markdown","5d49ea80":"markdown","cadec03d":"markdown","5f296ee5":"markdown","74f5af6b":"markdown","a2aa8bf4":"markdown","b37ff365":"markdown","243a632a":"markdown","71219591":"markdown","410dd25b":"markdown","346c840d":"markdown","0ad93487":"markdown","0a9af58f":"markdown","c3eb4824":"markdown","1becb9d1":"markdown","4a2266d0":"markdown","4213fb54":"markdown","baaa4152":"markdown","ccde963b":"markdown","16db5d00":"markdown","eca1769a":"markdown","7a40e894":"markdown","a5a78288":"markdown","396c9e1c":"markdown","2eb4ae18":"markdown","94965603":"markdown"},"source":{"699a24fe":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport plotly.express as px\nimport re\nfrom functools import reduce\n\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\n\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import r2_score\n\nsns.set()","6c76cfee":"# The original data\ndf = pd.read_csv(\"..\/input\/summer-products-and-sales-in-ecommerce-wish\/summer-products-with-rating-and-performance_2020-08.csv\")","89d9dbaf":"df.head().T","3038588b":"df.duplicated().sum()","01756277":"df.drop_duplicates(inplace=True)\nraw_df = df.copy() # Copy of the original Data after dropping the duplicates","ef82c00d":"fig = px.bar(df.isna().sum().sort_values().reset_index(), x=\"index\", y=0, title=\"Evaluating Missing values\")\nfig.update_layout({\"yaxis\": {\"title\": \"Missing values\"}})\nfig.show()","50ddf508":"df[\"merchant_profile_picture\"].unique()","e6221e98":"print(df.loc[df[\"merchant_profile_picture\"].isna(), \"merchant_has_profile_picture\"].unique())\nprint(df.loc[~df[\"merchant_profile_picture\"].isna(), \"merchant_has_profile_picture\"].unique())\n\ndf.drop([\"merchant_profile_picture\"], axis=1, inplace=True)","2d869c6a":"df[\"has_urgency_banner\"].value_counts()","4af75f81":"df[\"urgency_text\"].value_counts()","33372059":"print(df.loc[df[\"has_urgency_banner\"].isna(), \"urgency_text\"].unique())\nprint(df.loc[~df[\"has_urgency_banner\"].isna(), \"urgency_text\"].unique())\n\ndf.drop([\"urgency_text\"], axis=1, inplace=True)\ndf[\"has_urgency_banner\"] = df[\"has_urgency_banner\"].fillna(0)","6351ea07":"rating_cols = [\"rating_one_count\", \"rating_two_count\", \"rating_three_count\", \"rating_four_count\", \"rating_five_count\", \"rating\", \"rating_count\"]\n\nno_vote_df = df.loc[df[rating_cols].isna().any(axis=1), rating_cols]\ndf.loc[no_vote_df.index, rating_cols] = 0\n\ndf.loc[no_vote_df.index, rating_cols].head()","172ff63d":"df[\"origin_country\"].value_counts()","8217365f":"df[\"countries_shipped_to\"].value_counts()","a9e94e39":"df.drop(\"origin_country\", axis=1, inplace=True)","7f338dc9":"df[\"product_color\"].value_counts()","7bbbd258":"# The urls of each product\ndf.loc[df[\"product_color\"].isna(), [\"product_url\"]].head()","81841d60":"missing_color_vectors = [\n    \"#F6F7F1\",\n    \"#09041E\",\n    \"#D24D41\",\n    \"#6EACE1\",\n    \"#323232\",\n    \"#CECECE\",\n    \"#E1E1EB\",\n    \"#1D0E25\",\n    \"#9EA59D\",\n    \"#272727\",\n    \"#FFD1DE\",\n    \"#B37264\",\n    \"#050505\",\n    \"#120F1A\",\n    \"#F9F9F9\",\n    \"#C6363F\",\n    \"#B7EDEC\",\n    \"#4DA4C8\",\n    \"#E7E6E4\",\n    \"#C7955E\",\n    \"#D3560A\",\n        np.nan,\n    \"#68166B\",\n    \"#97E8D7\",\n    \"#A6C2D1\",\n        np.nan,\n    \"#374757\",\n    \"#E94875\",\n    \"#EC2A13\",\n    \"#4C4C4C\",\n    \"#B9B4A9\",\n    \"#0E0809\",\n    \"#EBEEF4\",\n    \"#DEEEEF\",\n    \"#7F7181\",\n    \"#E1E1E1\",\n    \"#CFE5D8\",\n    \"#E0D5D4\",\n    \"#F4F7FB\",\n    \"#74636D\",\n    \"#E1E6EE\"\n]\nassert len(missing_color_vectors) == df.loc[df[\"product_color\"].isna(),:].shape[0], f\"There are missing colors, {len(missing_color_vectors)}\"\ndf.loc[df[\"product_color\"].isna(), [\"product_color\"]] = missing_color_vectors\ndf.loc[df[\"product_color\"] == \"multicolor\", [\"product_color\"]] = np.nan","70b697ee":"custom_color_rainbow = {\n    \"coffee\": \"#381E07\",\n    \"floral\": \"#E4A593\",\n    \"rose\": \"pink\",\n    \"leopard\": \"#F2D24A\",\n    \"leopardprint\": \"#F2D24A\",\n    \"camouflage\": \"green\",\n    \"army\": \"green\",\n    \"camel\": \"#C19A6B\",\n    \"wine\": \"#940F22\",\n    \"apricot\": \"#F3C8AB\",\n    \"burgundy\": \"#7D2F3D\",\n    \"jasper\": \"#D0393C\",\n    \"claret\": \"#940F22\",\n    \"rainbow\": \"white\",\n    \"star\": \"yellow\",\n    \"nude\": \"pink\"\n}\n\ndef get_rgb(color):\n    \"\"\" \n    Returns the rgb vector if the color exists on matplotlib. \n    This function is a bit messy with the nested try except statements, but performance is not critical \n    and it works for now, but I should clean it later \"\"\"\n    \n    # TODO: THere is definitely a more elegant implementation\n    if color in custom_color_rainbow.keys():\n        return matplotlib.colors.to_rgb(custom_color_rainbow[color])\n\n    try:\n        return matplotlib.colors.to_rgb(color)\n    except:\n        base_colors = [\"blue\", \"green\", \"red\", \"white\", \"black\", \"gold\", \"yellow\", \"pink\", \"purple\", \"orange\", \"grey\", \"khaki\"]\n        simplified_color = [c for c in base_colors if c in color]\n        try:\n            return matplotlib.colors.to_rgb(simplified_color[0])\n        except:\n            return np.nan\n        \nrgb_colors = df.loc[~df[\"product_color\"].isna(), \"product_color\"].apply(get_rgb)","4b50e511":"rgb_colors_dict = [{\"r\": r[0], \"g\": r[1], \"b\": r[2]} for r in rgb_colors]\nrgb_colors_dict_df = pd.DataFrame(rgb_colors_dict, index=rgb_colors.index)\ndf[\"r\"] = rgb_colors_dict_df[\"r\"]\ndf[\"g\"] = rgb_colors_dict_df[\"g\"]\ndf[\"b\"] = rgb_colors_dict_df[\"b\"]\n\n# Fill the missing values with the mean of the column\ndf[\"r\"].fillna(df[\"r\"].mean(), inplace=True)\ndf[\"g\"].fillna(df[\"g\"].mean(), inplace=True)\ndf[\"b\"].fillna(df[\"b\"].mean(), inplace=True)\n\n# Drop the useless columns\ndf.drop([\"product_color\", \"product_url\", \"product_picture\"], axis=1, inplace=True)","6270b444":"df[\"product_variation_size_id\"].value_counts()","d904e11b":"def clean_sizes(s: str) -> str:\n    return re.findall(r\"M|X?[SsLl](?!\\w+)\", s)\n\ndef convert_us_to_eu(s: str) -> str:\n    number = re.findall(\"\\d+\", s[0])[0]\n    \n    eu_to_letter = {\n        (0, 36): \"XS\",\n        (36, 40): \"S\",\n        (40, 44): \"M\",\n        (44, 48): \"L\",\n        (48, 52): \"XL\",\n        (52, 60): \"XXL\"\n    }\n    return [v for k, v in eu_to_letter.items() if k[0]<int(number)<k[1]][0]\n     \noriginal_sizes = df[\"product_variation_size_id\"].dropna().unique()\nchanged_to_letter = [re.sub(r\"EU\\s*\\d+\", convert_us_to_eu, s) for s in original_sizes]\nfiltered_sizes = [clean_sizes(s) for s in changed_to_letter]","e7c36a06":"original_sizes = df[\"product_variation_size_id\"].dropna()\nchanged_to_letter = [re.sub(r\"EU\\s*\\d+\", convert_us_to_eu, s) for s in original_sizes]\nfiltered_sizes = [clean_sizes(s) for s in changed_to_letter]","9075707c":"df.loc[original_sizes.index, \"product_size\"] = [c[0].lower() if c != [] else np.nan for c in filtered_sizes ]\ndf[\"product_size\"].fillna(\"M\", inplace=True)\ndf[\"product_size\"].value_counts()","52755f59":"df[\"product_size\"] = OrdinalEncoder().fit_transform(df[\"product_size\"].values.reshape(-1, 1))\ndf[\"product_size\"].value_counts()","522c4bc8":"df.drop(\"product_variation_size_id\", axis=1, inplace=True)","406a945f":"df[\"merchant_id\"].value_counts()","984b1454":"df[\"merchant_title\"].value_counts()","f812f4d8":"# Surprisingly there are more unique values here than unique merchants\ndf[\"merchant_info_subtitle\"].value_counts()","8b201516":"df[[\"merchant_info_subtitle\", \"merchant_rating_count\", \"merchant_rating\"]]","ff972a9c":"df.describe()","7db0f1e9":"df.drop([\"merchant_id\", \"merchant_title\", \"merchant_name\", \"merchant_info_subtitle\"], axis=1, inplace=True)","f6bc0aca":"df[\"currency_buyer\"].value_counts()","d2265b8f":"df.drop(\"currency_buyer\", inplace=True, axis=1)","e7d11d76":"df[\"n_tags\"] = df[\"tags\"].apply(lambda x: len(x.split(\",\")))","f6569518":"swords = stopwords.words('english')\n\ndef clean_text(s: str) -> str:\n    \"\"\" Cleans the strings from the titles and the tags\"\"\"\n    \n    # Only Keep letters\n    processed_s = re.sub(r\"[^a-z]\", \" \", s.lower())\n    \n    ps = PorterStemmer()\n    \n    # stemmed words with Porter Lemantizer\n    stemmed_s = [ps.stem(s) for s in processed_s.split()]\n    \n    unique_tags = list(set(stemmed_s))\n    \n    # Filter stop words\n    cleaned_text = [w for w in unique_tags if (w not in swords and len(w) > 2)]\n    \n    return cleaned_text\n\nall_tags = (df[\"tags\"] + df[\"title\"] + df[\"title_orig\"]).values\nprocessed_tags = [clean_text(s) for s in all_tags]","66a0e64a":"len(set(reduce(lambda a,b : a+b, processed_tags)))","5741f730":"# Each of these will be a binary column\ntags_list = [\n    r\"\\bmen\",\n    r\"\\bwomen\",\n    \"shirt\",\n    \"robe\",\n    \"dress\",\n    \"skirt\",\n    \"underwear\",\n    \"swim\",\n    \"nightwear\",\n    \"sleepwear\",\n    \"shorts\"\n]\n\ndef build_tags_dict(tags_list_per_product: list) -> dict:\n    \"\"\" Returns a dict with 0 or 1, any of the tags_list were found  on the tags per sample\"\"\"\n    return {tag.lstrip('\\\\b'): any(re.findall(tag, \" \".join(tags_list_per_product))) for tag in tags_list}","25602600":"\ntesting_set = processed_tags[3]\nprint(build_tags_dict(testing_set))\nprint()\nprint(testing_set)","3f6046cf":"cols_df = pd.DataFrame([build_tags_dict(t) for t in processed_tags])\ncols_df.head()","da6c86bb":"df = df.merge(cols_df, left_index=True, right_index=True)","735de368":"test_index = np.random.randint(df.shape[0])\ndf.loc[test_index, cols_df.columns.values.tolist() + [\"title\", \"tags\", \"title_orig\"]].to_dict()","0d271b2a":"(df[\"title\"].str.lower() == df[\"title_orig\"].str.lower() ).sum()","20b797c1":"df.drop([\"tags\", \"title\", \"title_orig\"], axis=1, inplace=True)","22319e62":"df[\"theme\"].value_counts()","6a92266b":"df[\"crawl_month\"].value_counts()","c401f64b":"df.drop([\"theme\", \"crawl_month\", \"shipping_option_name\"], axis=1, inplace=True)","0c305dc7":"df[\"product_id\"].value_counts().value_counts()","bed28b03":"n_id_counts = df[\"product_id\"].value_counts()\nduplicated_ids = n_id_counts[n_id_counts > 1].index\ncomp_ids = df[df[\"product_id\"].isin(duplicated_ids)].sort_values(\"product_id\").set_index(\"product_id\")\ncomp_ids.T","f58bc690":"raw_df.loc[raw_df[\"product_id\"].isin(duplicated_ids), [\"merchant_id\", \"product_id\"]].groupby(\"merchant_id\").count().squeeze().min()","2f9c9885":"df = df.sort_values(\"has_urgency_banner\", ascending=False).drop_duplicates(\"product_id\")\ndf.drop(\"product_id\", inplace=True, axis=1)","1e5ecc96":"df.describe().T","896652f6":"\nto_bool_cols = [\"uses_ad_boosts\", \"shipping_is_express\", \"badge_local_product\", \"badge_product_quality\", \"has_urgency_banner\", \"merchant_has_profile_picture\"]\ndf[to_bool_cols] = df[to_bool_cols].astype(bool)","d2db3f92":"df.dtypes","fcc837e9":"assert not df.isna().any().any()","b79a3dbf":"df[\"inventory_total\"].min()","a47d6560":"# Just some usefull variables\ncont_cols = df.select_dtypes(exclude=\"bool\").columns\nbool_cols = df.select_dtypes(\"bool\").columns","c428a6c1":"px.imshow(df[cont_cols].corr(), width=1000, height=1000)","d7c7480c":"df.drop([\"rating_five_count\", \"rating_four_count\", \"rating_three_count\", \"rating_two_count\", \"rating_one_count\"], axis=1, inplace=True)","7b250cd7":"scatter_matrix_cols = [\"price\", \"units_sold\", \"rating\", \"merchant_rating\", \"rating_count\"]\npx.scatter_matrix(df[scatter_matrix_cols], width=1000, height=1000)","2811c2bc":"px.box(df[bool_cols.values.tolist() + [\"units_sold\"]].melt(id_vars=\"units_sold\"), x=\"variable\", y=\"units_sold\", color=\"value\", title=\"Sold Unites Based on the Boolean Columns\")","4a1b87ea":"X = df.drop(\"units_sold\", axis=1)\ny = df[\"units_sold\"]\nreg = make_pipeline(StandardScaler(), ElasticNet(alpha=0.5))\nreg.fit(X, y)\npd.Series(reg[-1].coef_, index=X.columns).sort_values().plot.bar(figsize=(20, 5))\n\nr2 = r2_score(y, reg.predict(X))\nplt.title(f\"R2: {round(r2, 2)}\")\nplt.show()","fa62fab8":"no_counts_df = df.drop([\"rating_count\", \"merchant_rating_count\"], axis=1)","458dc7ed":"X = no_counts_df.drop(\"units_sold\", axis=1)\ny = no_counts_df[\"units_sold\"]\nreg = make_pipeline(StandardScaler(), ElasticNet(alpha=0.5))\nreg.fit(X, y)\npd.Series(reg[-1].coef_, index=X.columns).sort_values().plot.bar(figsize=(20, 5))\n\nr2 = r2_score(y, reg.predict(X))\nplt.title(f\"R2: {round(r2, 2)}\")\nplt.show()","4a95bdc0":"df[\"profit\"] = df[\"retail_price\"] - df[\"price\"]\ndf[\"profit\"].hist(bins=40, figsize=(20, 5))\nplt.title(\"Difference between retail_price and price\")\nplt.show()","8c5a4a9d":"X = df[\"rating_count\"].to_frame()\ny = df[\"units_sold\"]\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True)\nreg = make_pipeline(StandardScaler(), ElasticNet(0.5))\nreg.fit(X_train, y_train)\npredition = reg.predict(X_test)\ntest_r2 = r2_score(y_test, predition)\nprint(f\"Test r2_score: {round(test_r2, 2)}\")","0da3316b":"# Make sold units prediction\ndef get_results(reg, df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    reg is the pre trained pipelin (In this case with the included scaler)\n    df is the original Data Frame\n    \"\"\"\n    df[\"predicted_sold_units\"] = reg.predict(df[\"rating_count\"].to_frame())\n\n    df[\"profit\"] = (df[\"retail_price\"] - df[\"price\"]) * df[\"predicted_sold_units\"]\n\n    df[\"units_to_order\"] = np.ceil(np.min(df[\"predicted_sold_units\"] - df[\"inventory_total\"], 0))\n    return df\n\nresults_df = get_results(reg, raw_df)","403e3cbb":"profitable_products = results_df[[\"profit\", \"product_id\"]].set_index(\"product_id\").squeeze().sort_values(ascending=False)\nprofitable_products.head(10)","4fef51e7":"results_df[\"profit\"].hist(bins=100, figsize=(20, 5))","f594d1b4":"results_df.loc[results_df[\"product_id\"].isin(profitable_products.head().index), [\"units_to_order\", \"product_id\"]]","0974f26e":"# Checking columns Individually\n\n## *merchant_profile_picture* and *merchant_has_profile_picture*\nThis column contains a link for the profile picture. No information can be easily obtained from here, unless we would do something with the pixels of the pictures (Not going to do that for now).\nSince there is already a column indicating if the merchant has a profile picture, then I will just check that that column matches this one and if so, discard the *merchant_profile_picture* columns.","9f275a9d":"## *tags*, *title*, *title_orig*\nThese are tags. A lot of interesting features can be drawn from here, together with the titles.\nI will combine these three columns, and then try to create classes from the obtained words using some NLP tricks.\n\nI will also create a column for the number of tags for each product.","6a1fdd1e":"## Convert binary columns to boolean\nAs a final step, we will convert all the binary columns to boolean","7d3730f7":"## *product_id*\nSome product ids seem to be repeated, I will check if these products are actually repeated or not.","fd77be0b":"## Assert that there are no missing values","241020c0":"### Check all the unique tags\nThere are 3987 tags in this dataset","0cfdedd6":"## *product_color*, *product_picture* and *product_url*\nThere are 101 colors (see bellow), some of them appear very often while others do not. \n\nOne option to deal with this would be to aggregate all the colors into similar groups (for example *denimblue* goes into *blue*), to reduce the number of categories.\n\nHowever, I will replace the colors by the correspondent rgb vector (Hopefully, I will be able to the the rgb vectors for the majority of these colors from matplotlib.\n\nThe column *product_color* has missing values. I would fill them based on the *product_picture* columns, but none of those links work, which means that I will drop the column *product_picture* at the end anyway. \n\nThe *product_url* link works, so I should be able to fill the missing colors from there. To do this, I will just go to the link and use a color picker on top of the product to get rgb (I could do this with a scrapper but this way it's faster by hand for now, specially because the product pages are hidden behind a login). \n\nNaturally, there will be products with different colors, in that case I will just select one of them based on the one that jumps to my attention first.\n\nThere are also 20 with color description multicolor. I will average out the color of these ones as well. \n\nThere were two links that did not work, fot those I will assume the mean color of each rgb vector.\n","34ef3516":"# Most profitable products","cbe09b92":"### Regression analysis conclusion\nThe rating_count is overwhelmingly important for the units_sold, which to me only indicates that products that have been around for longer (Hence have more ratings) have sold more. For the reason, I decided to remove this column from the remainder of the anaylsis (as well as the *merchant_rating_count*).\n\nThe results changed without this column, and we were only able to get an R2 of 7%. ","42913341":"## Scatter Matrix","6d48e886":"## Checking missing values","e33cf3c2":"The difference seems to be in the urgency banner column, everything else is the same. I will also check if they come from the same merchant, just to make sure, but then I will discard the products_ids duplicated that do not have th eurgency banner. ","ee3ba4e3":"## Checking product ratings\n\nIn this section we will check the columns:\n\n- rating_one_count\n- rating_two_count\n- rating_third_count\n- rating_fourth_count\n- rating_five_count\n- rating\n- rating_count\n\nThe first 5 columns have missing values. Thee turned out to be products without any rating, altough for some reason the rating of these products is 5, event though they have a rating_count of 0. \n\nFor these products, I will change the rating and all the rating counts to 0, indicating that they did not get any votes yet.","c7e8e3fe":"# EDA","90c0675a":"Verify the number of duplicated product ids per merchant. If there was any 1 here, it meant that some of the same product ids were allocated to different merchants\n","5d49ea80":"## *currency_buyer*\nAll the currency is in Euro, so this column will be droped","cadec03d":"### Creating Columns for the relevant tags\nWe wil now use the created tags to create binary columns with the information that we want. Some of these columns can be:\n\n- Men\n- Women\n- Shirt\n- Robe\n- Pajamas\n\netc..\n\n(We need two columns for Man and Woman on the offchance that we find something for both and for neither genders)","5f296ee5":"## Regression analysis\nWe will now fit a linear regression with regularization (elasticnet) to try and figure out, which features are actually more important.","74f5af6b":"## *origin_country* and *countries_shipped_to*\nThe great majority of the dataset has origin in **CN**. This column has a really low variance, and so I will discard it, given that there is no statistical significant information that I can obtain from it.\n\n*countries_shipped_to* is a numerical value. \n","a2aa8bf4":"\n# Model for selecting products\nNow I will create a model that, identifies which products should be bought. This model will be a regression model using the available information, to predict the number of units sold. Then, the number of units sold will be multiplied by the difference between price and retail price. \n\nTo indicate if more of some product should be aquired, then the predicted sold amount will be subtracted from the number of units currently in stock.\n\nThere seem to be negative differences between the retail and price column, indicating that some products are aquired at a more expensive that they are sold?? Still, I will not change the values of this column","b37ff365":"## Checking the boolean columns\nNow we can check the boolean columns to see which sell the most","243a632a":"## Create the regression model using only the rating_counts column\nOrdinarly I would use cross validation for model testing, but in here I am going to go with normal train test split because it will be easier to integrate this to get the results","71219591":"## Correlation\nThere seems to be a really strong positive correlation  between the rating_counts and the number of units sold. \n\nThis basically means that products that generate more traffic and have been around for longer have sold more units.\n\nGiven that there is a lot of correlation between the individual score counts, and the total rating couts, I will remove the five individual score counts.\n\nAlso, it seems that the *units_sold* is not exactly a continuous columns, but rather a categorical column. Still, I will treat this problem as a regression problem. ","410dd25b":"### Processing the tags","346c840d":"# Conclusion\nIn this notebook I build a simple model (a simple linear regression) to indicate which products are more profitable and how many units of these products should be ordered. \n\nI find this problem really interesting, I wouls really like to work more on it with more data, to build a system like a stock optimization algorithm and something similar. \n\nI did not turn this into a proper pipeline, because I feel like this is it is easier to understand and change the notebook, but maybe in the future I will do so. ","0ad93487":"## *has_urgency_banner* and *urgency_text*\nIt seems that there are only *urgency_text* in French, one indicating limited quantity and another indicating price reduction. Only one of them is \"R\u00e9duction sur les achats en gros\", as seen bellow. I wil check if this column matches the *has_urgency_banner*, and if so, discard this column. \n\nThe column *has_urgency_banner* has values either 1 or nan, so I will turn the nan values of this column into 0.","0a9af58f":"Drop the duplicated that do not have the urgency banner","c3eb4824":"# Products to order\nGet the units to order from the most profitable products","1becb9d1":"#### Is the title equal to the original title\nCheck how frequently is the title the same as the original title","4a2266d0":"# Predicting Successfull products\nIn this task, I will be identifying the characteristics that make a product sell. \n\nIn the end, I built a really simple model (basically a linear regression) that indicates which products are more likely to return profit (based on the predicted sold_units and the difference between the retail_price and the price).\n\nIn this notebook I present the steps sequentially, only with a couple of functions. I should turn this into a proper pipeline in the future. ","4213fb54":"## *themes*, *crawl_month*  and *shipping_option_name*\nCan be removed because the only contain one value or very few values","baaa4152":"##  Conclusion Regression analysis\nUsing the regression analysis we concluded that the number of ratings is the most important factor for determining the number of units sold. \n\nI believe this is however, not that relevant, because it merely implies that products that have been on store for longer, sold more. \n\nWhen I removed this column, the R2 square score lowered significantly.\n\nAlso, none of the tags I created seem to have that much of an effect. ","ccde963b":"## Droping Duplicates\nRemoving duplicated entries","16db5d00":"## Check if there are products out of stock\nIt is important to figure out if there are no products out of stock, because if this is the case then that could mean that some products did not sell because they were out of stock. That does not seem to be the case fortunately. ","eca1769a":"# Note on this notebook\nIn this notebook I made a mistake in intrepreting the columns *price* and *retail_price*, as noted by the dataset creator. Please read the comments :)","7a40e894":"On average, it seems that when you have profile picture, and \"badge product quality\" and the swim and nightwear tags you will sell more. However, it is important to note that these distinctions are highly unbalanced.","a5a78288":"## *product_validation_size_id*\nThis is the size variation. I will first process the columns into XS, S, M, L and XL and afterwards I will use ordinal encoding on it. For the nan column, I will assume \"M\". \n\nThere are of course products here on which this does not apply, given that these measures only apply to clothing. Still, if all the other despartments have the same size \"M\", that should not be a problem (At first glance anyway).\n\nA lot of these sizes are from different scales, so I need to convert them to the same scale.\n\nI used this side for this:\nhttps:\/\/www.blitzresults.com\/en\/european-sizes\/","396c9e1c":"## *merchant_id*, *merchant_info_subtitle*, *merchant_title* and *merchant_name*, *merchant_rating*, *merchant_rating_count*\n\n- *merchant_id* is a a single id for each merchant and will be discarded\n- *merchant_title* does not seem to contain much information and will be discarded\n- *merchant_name* is just the name and will be discarded\n- *merchant_info_subtitle* contains the same information as *merchant_rating_count* and another evaluation metric that does not seem to be the same as *merchant_rating*. Ultimately this column will be discarded\n","2eb4ae18":"#### Verifying the function the created routine","94965603":"# Modelling\nWe now have only numeric columns, now we can and take some conclusions from the data"}}