{"cell_type":{"273a7571":"code","a9ef7108":"code","02a4cb92":"code","1a158af1":"code","4dfb6db6":"code","3e218c3f":"code","b00d9bd9":"code","e0231539":"code","280ed2a2":"code","7fd2d5da":"code","5cc43392":"code","b70b5acd":"code","6b891764":"code","07375422":"code","9eac71a8":"code","a307f9dd":"code","c8298e9d":"code","c7d6b7a1":"code","3981d2fd":"code","011f859a":"code","5b9788d0":"code","2ea320b2":"code","9ddae23e":"code","1ff6e39e":"markdown","6a8abf0e":"markdown","29d23450":"markdown","070e5f64":"markdown","38e2872b":"markdown","d6a54c7c":"markdown","89504087":"markdown","17d26b4e":"markdown","524a6325":"markdown","67c7b8d3":"markdown","ae4afb5e":"markdown","492bc94c":"markdown","bc83b273":"markdown"},"source":{"273a7571":"import pandas as pd\n\ntrain = pd.read_csv(\"..\/input\/train.csv\", nrows = 1000)\ntest = pd.read_csv(\"..\/input\/test.csv\", nrows = 1000)\nprint(train.head())","a9ef7108":"import numpy as np\n\nprint(np.mean(train.target))","02a4cb92":"max_words = np.max([len(i.split(\" \")) for i in train.question_text])\nprint(max_words)","1a158af1":"import numpy as np\n\n# loading embedding: https:\/\/www.kaggle.com\/sudalairajkumar\/a-look-at-different-embeddings\nEMBEDDING_FILE = '..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n\nwords_embedding = set(embeddings_index.keys())\nprint(len(embeddings_index))","4dfb6db6":"print(embeddings_index[\"the\"])","3e218c3f":"print(len(embeddings_index[\"the\"]))","b00d9bd9":"list_train_words = \" \".join(train.question_text).split(\" \")\nwords_training = set(list_train_words)\nprint(len(words_training.intersection(words_embedding))\/len(words_training))","e0231539":"print(words_training.difference(words_embedding))","280ed2a2":"from collections import Counter\n\nfrequencies = Counter(words_training.difference(words_embedding))\nprint(frequencies)","7fd2d5da":"import operator\n\nsorted_frequencies = sorted(frequencies.items(), key=operator.itemgetter(1), reverse=True)\nprint(sorted_frequencies)","5cc43392":"embedding_matrix = np.zeros((len(words_training), 300))\nmapping = {}\n\nfor index, word in enumerate(words_training):\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector\n        mapping[word] = index\n        \nprint(embedding_matrix)","b70b5acd":"print(embedding_matrix.__class__)\nprint(embedding_matrix.shape)","6b891764":"labels = train.target\nsentences = [i.split(\" \") for i in train.question_text]\nvocab_size = len(set([item for sublist in sentences for item in sublist]))\nprint(vocab_size)","07375422":"print(sentences[0])","9eac71a8":"print(mapping[sentences[0][0]])","a307f9dd":"for i in sentences[0:2]:\n    for j in i:\n        if j in mapping.keys():\n            print(mapping[j])","c8298e9d":"for i in sentences[0:2]:\n    print([mapping[j] for j in i if j in mapping.keys()])","c7d6b7a1":"input_sequences = [[mapping[j] for j in i if j in mapping.keys()] for i in sentences]","3981d2fd":"from keras.preprocessing.sequence import pad_sequences\n\npadded_docs = pad_sequences(input_sequences, maxlen=max_words, padding='post')","011f859a":"from keras import backend as K\n\ndef f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n    \n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","5b9788d0":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import Embedding\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 300, input_length=max_words, weights=[embedding_matrix], trainable=False))\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=[f1])\nprint(model.summary())","2ea320b2":"model.fit(padded_docs, labels, epochs=0)","9ddae23e":"loss, f1 = model.evaluate(padded_docs, labels, verbose=0)\nprint('F1: %f' % f1)","1ff6e39e":"# Import the embedding","6a8abf0e":"# Checking the coverage","29d23450":"# Build the embedding matrix\n\nFollowing the [Keras blog](https:\/\/blog.keras.io\/using-pre-trained-word-embeddings-in-a-keras-model.html), an embedding matrix can be created. The first step is to allocate space for the embedding matrix. The columns are the same as in the embedding index (300 in this case) and the rows are as many as the number of the training records. Words not found in embedding will be all-zeros.","070e5f64":"# Read the file","38e2872b":"Following [Machine Mastery](https:\/\/machinelearningmastery.com\/use-word-embedding-layers-deep-learning-keras\/), the input file is padded in order to have all input sequences of equal size padding after (*post*) until a maximum of *maxwords* (50):","d6a54c7c":"What is the maximum length of a sentence (in terms of number of words)?","89504087":"The model can be then sketched. For the moment, a very simple network:","17d26b4e":"Printing the missing words (those not present in the embedding):","524a6325":"Sorting by the number of appearences (following [stackoverflow](https:\/\/stackoverflow.com\/questions\/613183\/how-do-i-sort-a-dictionary-by-value)):","67c7b8d3":"# Train the model\n\nFollowing [Theo Viel's kernel](https:\/\/www.kaggle.com\/theoviel\/improve-your-score-with-text-preprocessing-v2) the f1 metric is implemented:","ae4afb5e":"# Prepare the input\n\nPrepare the data taking all tokens (for the moment, only splitting on spaces):","492bc94c":"# Count the frequencies","bc83b273":"What is the fraction of insincere questions?"}}