{"cell_type":{"6d1567f8":"code","9fe29fff":"code","508a6c85":"code","9da205a7":"code","0464dd2f":"code","496ca9f6":"code","f8447c3a":"code","e4a6f849":"code","a6cf352f":"code","f4561e25":"code","40066d97":"code","f4db36d1":"code","88a7b8b0":"code","cd91fe3a":"code","74cdd0ad":"code","13d02ee2":"code","feeff7a5":"code","32e94e8a":"code","5a9e32c9":"code","d5a80846":"code","4b1af546":"code","103b78d6":"code","a716db45":"code","867f4f94":"code","02543c9f":"code","d9859db0":"code","6587e020":"code","01348194":"code","7fc6ebc5":"code","04148db9":"markdown","ae7c861c":"markdown","ba2ade7b":"markdown","af15d140":"markdown","8ac97e65":"markdown","fad9c86f":"markdown","4b63a1f4":"markdown","f96b5016":"markdown","6557d4cb":"markdown","e711cde8":"markdown","82fb0cef":"markdown","c6af48b8":"markdown","7f114a24":"markdown","b0457564":"markdown","9409ed60":"markdown","d418f8fa":"markdown","583573d4":"markdown","5fa283cc":"markdown","60a507a5":"markdown","497c1ae8":"markdown","07cacfe8":"markdown","f8566bef":"markdown","8bfe05e8":"markdown","8e81c4ac":"markdown","d5479949":"markdown"},"source":{"6d1567f8":"%load_ext autoreload\n%autoreload 2","9fe29fff":"%matplotlib inline\n\nfrom fastai.imports import *\nfrom fastai.structured import *\nfrom sklearn.ensemble import RandomForestRegressor\nfrom IPython.display import display\nfrom sklearn import metrics","508a6c85":"# Set the plot sizes\nset_plot_sizes(12,14,16)","9da205a7":"PATH = \"..\/input\/\"\ndf_raw = pd.read_csv(f'{PATH}train.csv', nrows=100000, parse_dates=['pickup_datetime'], dtype={'passenger_count': 'int8', 'fare_amount': 'float16'}, \n                     usecols=['fare_amount', 'pickup_datetime','pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude','passenger_count'])\ndf_raw_test = pd.read_csv(f'{PATH}test.csv', parse_dates=['pickup_datetime'], dtype={'passenger_count': 'int8'})","0464dd2f":"# Expands the summary tables if there are a lot of columns androws\ndef display_all(df):\n    with pd.option_context(\"display.max_rows\", 1000, \"display.max_columns\", 1000): \n        display(df)","496ca9f6":"# Shows the last 5 rows of the traning set\ndisplay_all(df_raw.tail().T)","f8447c3a":"# Shows summary of training set\ndisplay_all(df_raw.describe(include='all').T)","e4a6f849":"# Shows summary of test set\ndisplay_all(df_raw_test.describe(include='all').T)","a6cf352f":"plt.figure(figsize=(20, 4))\ndf_raw['pickup_datetime'].groupby([df_raw[\"pickup_datetime\"].dt.year, df_raw[\"pickup_datetime\"].dt.month]).count().plot(kind=\"bar\")\nplt.title('Traing Set Rides per Month and Year')\nplt.show()","f4561e25":"plt.figure(figsize=(20, 4))\ndf_raw_test['pickup_datetime'].groupby([df_raw_test[\"pickup_datetime\"].dt.year, df_raw_test[\"pickup_datetime\"].dt.month]).count().plot(kind=\"bar\")\nplt.title('Test Set Rides per Month and Year')\nplt.show()","40066d97":"df_raw[df_raw['fare_amount'] < 0]","f4db36d1":"# Large negative longitudes\ndf_raw[df_raw['pickup_longitude'] < -75]","88a7b8b0":"# Large positive longitudes\ndf_raw[df_raw['pickup_longitude'] > -73]","cd91fe3a":"# Small positive latitudes\ndf_raw[df_raw['pickup_latitude'] < 40]","74cdd0ad":"# Large positive latitudes\ndf_raw[df_raw['pickup_latitude'] > 42]","13d02ee2":"df_raw.shape","feeff7a5":"df_raw = df_raw[df_raw['pickup_longitude'] > -76]\ndf_raw = df_raw[df_raw['pickup_longitude'] < -73]\ndf_raw = df_raw[df_raw['pickup_latitude'] > 40]\ndf_raw = df_raw[df_raw['pickup_latitude'] < 44]","32e94e8a":"df_raw.shape","5a9e32c9":"# Converts all strings to categorical features\ntrain_cats(df_raw)","d5a80846":"# Splits dates into subcomponenets\nadd_datepart(df_raw, 'pickup_datetime')","4b1af546":"df_raw.info()","103b78d6":"# Splits the data into independent and dependent features and keeps a column 'nas' that keeps track of features that had missing values and had to be imputed\ndf, y, nas = proc_df(df_raw, 'fare_amount')","a716db45":"m = RandomForestRegressor(n_estimators=30, min_samples_leaf=3, oob_score=True, n_jobs=-1)\nm.fit(df, y)","867f4f94":"# Calculate Feature Importance\nfi = rf_feat_importance(m, df); fi[:10]","02543c9f":"# Shows the table above visually\nfi.plot('cols', 'imp', figsize=(10,6), legend=False)\nplt.title('Feature Importance by Feature');","d9859db0":"def plot_fi(fi): return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)","6587e020":"# Same visualisation but easier to see which feature contributes how much\nplot_fi(fi[:30])\nplt.title('Feature Importance by Feature');","01348194":"from scipy.cluster import hierarchy as hc","7fc6ebc5":"corr = np.round(scipy.stats.spearmanr(df).correlation, 4)\ncorr_condensed = hc.distance.squareform(1-corr)\nz = hc.linkage(corr_condensed, method='average')\nfig = plt.figure(figsize=(16,10))\ndendrogram = hc.dendrogram(z, labels=df.columns, orientation='left', leaf_font_size=16)\nplt.title('Feature Similarities')\nplt.show()","04148db9":"## Similar Features","ae7c861c":"## Initial Processing","ba2ade7b":"### Negative Fare Amount","af15d140":"## Conclusion and Next Steps","8ac97e65":"## Feature Importance","fad9c86f":"I will delete all outliers to keep things simple, even those that could potentially be amended by changing the decmal place.","4b63a1f4":"## Comparing Number of Taxi Rides in Training and Test Set","f96b5016":"## Load the Libraries","6557d4cb":"The only features that are quite similar to each other are Day of Year, Month and Week as well as Time Elapsed and Year. This doesn't seem very useful as the numbers are probably just very similar but I will need to confirm this.\n\nThe way to read the plot is the following. The x-axis shows the similarity or distance between the features. The longer the lines are the less similar two features are.","e711cde8":"The summaries show some interesting things:\n\n1. Training and test set have the same start and end dates - let's look at this in more detail in a bit\n2. In the training set there seem to be some outliers in latitudes and longitudes. There aren't any obvious outliers in the test set.\n3. The training set can have negative fare mounts\n4. The training set has rides with 0 passengers\n5. The means of both datasets are fairly similar but the standard deviation of training set is way bigger than that of the test set, also indicating outliers","82fb0cef":"In this kernel I'll try something different from the standard Exploratory Data Analysis. I will use Machine Learning for my EDA and base my Feature Engineering on the findings. I will use a Random Forest as it is fast to implement and provides Feature Importances. I can then use the Feature Importances to focus my time on the most important features. If a feature turns out to be 100 times more important than another feature, then I will spend 100 times more time on it.\n\nThe steps I will take are:\n\n1. Read in a subset of the entire dataset\n2. Do some initial data processing, feature engineering and handle missing data using the fast AI library\n3. Run a simple Random Forest\n4. Get the Feature Importances\n5. Create a cluster analysis to find similarities between features and plot them in a dendrogram","c6af48b8":"## Outliers","7f114a24":"Interestingly the number of rides per month and year is fairly constant in the training set but there are marked spikes in the test set. It is quite possible that this may affect prediction errors if time plays an important role. It's too early to say but certainly something to keep in mind.","b0457564":"## Initial Model","9409ed60":"In conclusion I would say the following:\n\n1. Geographical features were by far the most important and some additional features based on them should be built such as Manhattan distance\n2. Additional data sources including weather or holidays would be a useful complement\n3. Do a similar analysis on test and train data that was done on months and years but on longitudes and latitudes - this can also be done on the entire dataset if you read it in in chunks\n4. Look at missing data in more detail. I have skimmed over this here and simply imputed them using the mean\n5. Do an actual prediction. Random Forest gives good intial results but other algorithms will be necessary to increase the accuracy. Neural networks have the advantage that they can extrapolate, i.e. make a prediction on data (like longitudes outside the training set longitudes) they haven't seen before, which is something a Random Forest cannot do. RF will do a prediction but it won't be very good.\n6. Remove features with low feature importance and check the results. That may help with overfitting.\n    ","d418f8fa":"### Pickup Longitude Outliers","583573d4":"## Read in the Data","5fa283cc":"### Off-the-Shelf Data Type Conversion and Feature Engineering","60a507a5":"Now let's find the most important features. This will give us an idea on which features to focus on the most. And what we find is that longitude and latitude data matters the most. In fact there is a big gap between those four features and the next one, which is Time Elapsed and then there is another big gap.","497c1ae8":"* **Small Positive Latitudes:** This is a similar story to the **Large Positive Longitudes**\n* **Large Positive Latitudes:** There seem to be two outliers. The 401 might be 40.1 in fact.","07cacfe8":"* **Negative longitude outliers: ** The -736 might be -73.6 instead\n* **Positive longitude outliers:** There are a lot of cases with incorrect longitudes and lattitudes in fact because they are all set to 0. These rows need to go.","f8566bef":"### Delete Outliers","8bfe05e8":"I am reading in 100k rows from the training set. This will be enough to do a fast, initial EDA. I am also not reading in 'key' of the training set as it is just an id and I predefine the datatypes to save memory space. It is probably also neceassary to reduce the longitude and lattitude data to float32 from float64, which will slightly affect their values but reduce memory significantly. This is however only important if you read in the whole dataset.","8e81c4ac":"I deleted 2008 rows.","d5479949":"### Pickup Latitude Outliers"}}