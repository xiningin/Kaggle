{"cell_type":{"76116c4d":"code","429b1341":"code","fcf12e59":"code","a11130a4":"code","c51177f7":"code","6c6235f6":"code","43058fd1":"code","66edfe73":"code","77899b94":"code","5ea10421":"code","a29e5b3d":"code","238641fa":"code","b8dbde3d":"code","5a30643e":"code","13be74ed":"markdown","f83dd54c":"markdown","a40be72c":"markdown","eaea5dcb":"markdown","7f365719":"markdown","f410d44e":"markdown","d71d775a":"markdown","3001b867":"markdown","eea50975":"markdown","21c9b78b":"markdown"},"source":{"76116c4d":"# install gluonts package\n!pip install gluonts\n!pip install sentence_transformers","429b1341":"# load and clean data\nimport pandas as pd\nimport numpy as np\n\ntrain_all = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-2\/train.csv\")\nLOG_TRANSFORM = False\n\ndef preprocess(\n    df: pd.DataFrame,\n    log_transform: bool = LOG_TRANSFORM\n):\n    \n    # set index\n    df = df.set_index('Date')\n\n    # fill 'NaN' Province\/State values with Country\/Region values\n    df['Province_State'] = df['Province_State'].fillna(df['Country_Region'])\n\n    # take difference of fatalities and cases\n    df[['ConfirmedCases', 'Fatalities']] = df[['ConfirmedCases', 'Fatalities']].diff()\n    df = df.fillna(0)\n    \n    df._get_numeric_data()[df._get_numeric_data() < 0] = 0\n    assert df.isnull().sum().all() == 0\n    \n    # convert target values to log scale\n    if log_transform:\n        df[['ConfirmedCases', 'Fatalities']] = np.log1p(\n            df[['ConfirmedCases', 'Fatalities']].values\n    )\n    \n    return df\n\ndef split(\n    df: pd.DataFrame, \n    date: str = '2020-03-19', \n):\n\n    train = df.loc[df.index < date] \n    test = df.loc[df.index >= date]\n    return train, test\n\ntrain_all = preprocess(train_all)\ntrain, test = split(train_all)","fcf12e59":"# plot confirmed cases and fatalities in train\nimport matplotlib.pyplot as plt\nfrom gluonts.dataset.util import to_pandas\nfrom gluonts.dataset.common import ListDataset\n\ndef plot_observations(\n    target: str = 'ConfirmedCases',\n    cumulative: bool = False,\n    log_transform: bool = LOG_TRANSFORM\n):\n    fig = plt.figure(figsize=(15, 6.1), facecolor=\"white\",  edgecolor='k')\n    \n    local_train = train.copy()\n    local_test = test.copy()\n    if log_transform:\n        local_train[['ConfirmedCases', 'Fatalities']] = np.expm1(\n            local_train[['ConfirmedCases', 'Fatalities']].values\n        )\n        local_test[['ConfirmedCases', 'Fatalities']] = np.expm1(\n            local_test[['ConfirmedCases', 'Fatalities']].values\n        )\n    \n    if cumulative:\n        cum_train = local_train.groupby(['Province_State', 'Country_Region'])[['ConfirmedCases', 'Fatalities']].cumsum()\n        cum_train = cum_train.groupby('Date').sum()\n        cum_test = local_test.groupby(['Province_State', 'Country_Region'])[['ConfirmedCases', 'Fatalities']].cumsum()\n        cum_test = cum_test.groupby('Date').sum() + cum_train.tail(1).values\n    else:\n        cum_train = local_train.groupby('Date').sum()\n        cum_test = local_test.groupby('Date').sum()\n\n    train_ds = ListDataset(\n        [{\"start\": cum_train.index[0], \"target\": cum_train[target].values}],\n        freq = \"D\",\n    )\n    test_ds = ListDataset(\n        [{\"start\": cum_test.index[0], \"target\": cum_test[target].values}],\n        freq = \"D\",\n    )\n    \n    for tr, te in zip(train_ds, test_ds):\n        tr = to_pandas(tr)\n        te = to_pandas(te)\n        tr.plot(linewidth=2, label = f'train {target}')\n        tr[-1:].append(te).plot(linewidth=2, label = f'test {target}')\n    \n    plt.axvline(cum_train.index[-1], color='purple') # end of train dataset\n    type_string = 'Cumulative' if cumulative else 'Daily'\n    plt.title(f'{type_string} number of {target} globally', fontsize=16)\n    plt.legend(fontsize=16)\n    plt.grid(which=\"both\")\n    plt.show()\n    \nplot_observations('ConfirmedCases')\nplot_observations('Fatalities')\nplot_observations('ConfirmedCases', cumulative = True)\nplot_observations('Fatalities', cumulative = True)","a11130a4":"# generate BERT embeddings\nfrom sentence_transformers import SentenceTransformer\n\nplaces = []\nfor idx,row in train.iterrows():\n    if row['Province_State']!=row['Country_Region']:\n        places.append(row['Province_State']+\", \"+row[\"Country_Region\"])\n    else:\n        places.append(row['Country_Region'])\nplaces = np.unique(places)\n\nmodel = SentenceTransformer('bert-base-nli-mean-tokens')\ncountry_embeddings = model.encode(list(places))\nembedding_dim = len(country_embeddings[0])\n\nprovince_state = [p.split(',')[0] if p!=\"Korea, South\" else p for p in places] #error catching\ncountry_region = [p.split(',')[1][1:] if (len(p.split(','))==2 and p!='Korea, South') else p for p in places]\nembed_df = pd.DataFrame(\n    np.concatenate(\n        [\n            np.array(province_state).reshape(-1,1),\n            np.array(country_region).reshape(-1,1),\n            country_embeddings\n        ]\n        ,axis=1)\n)\nembed_df.columns=['Province_State','Country_Region']+list(range(embedding_dim))","c51177f7":"from sklearn.preprocessing import OrdinalEncoder\n\ndef join_with_embeddings(\n    df: pd.DataFrame,\n    embed_df: pd.DataFrame\n):\n    \n    # join, delete merge columns\n    new_df = df.reset_index().merge(\n        embed_df,\n        left_on = [\"Province_State\",'Country_Region'],\n        right_on = [\"Province_State\",'Country_Region'],\n        how = 'left'\n    ).set_index('Date')\n    \n    #make sure no NaN in dataframe\n    assert new_df.isnull().sum().sum()==0\n    return new_df\n\ndef encode(\n    df: pd.DataFrame\n):\n    \"\"\" encode 'Province\/State' and 'Country\/Region' categorical variables as numerical ordinals\"\"\"\n    \n    enc = OrdinalEncoder()\n    df[['Province_State', 'Country_Region']] = enc.fit_transform(\n        df[['Province_State', 'Country_Region']].values\n    )\n    return df, enc\n\njoin_df = join_with_embeddings(train_all, embed_df)\nall_df, enc = encode(join_df)\ntrain_df, test_df = split(all_df)\n_, val_df = split(all_df, date = '2020-03-06')","6c6235f6":"from gluonts.dataset.common import ListDataset\nfrom gluonts.dataset.field_names import FieldName\nimport typing\n\nREAL_VARS = list(range(embedding_dim))\n\ndef build_dataset(\n    frame: pd.DataFrame,\n    target: str = 'Fatalities',\n    cat_vars: typing.List[str] = ['Province_State', 'Country_Region'],\n    real_vars: typing.List[int] = REAL_VARS\n):\n    return ListDataset(\n        [\n            {\n                FieldName.START: df.index[0], \n                FieldName.TARGET: df[target].values,\n                FieldName.FEAT_STATIC_CAT: df[cat_vars].values[0],\n                FieldName.FEAT_STATIC_REAL: df[real_vars].values[0]\n            }\n            for g, df in frame.groupby(by=['Province_State', 'Country_Region'])\n        ],\n        freq = \"D\",\n    )\n\ntraining_data_fatalities = build_dataset(train_df)\ntraining_data_cases = build_dataset(train_df, target = 'ConfirmedCases')\ntraining_data_fatalities_all = build_dataset(all_df)\ntraining_data_cases_all = build_dataset(all_df, target = 'ConfirmedCases')\nval_data_fatalities = build_dataset(val_df)\nval_data_cases = build_dataset(val_df, target = 'ConfirmedCases')","43058fd1":"from gluonts.model.deepar import DeepAREstimator\nfrom gluonts.trainer import Trainer\nfrom gluonts.distribution import NegativeBinomialOutput\nimport mxnet as mx\nimport numpy as np\n\n# set random seeds for reproducibility\nmx.random.seed(0)\nnp.random.seed(0)\n\ndef fit(\n    training_data: ListDataset,\n    validation_data: ListDataset = None,\n    pred_length: int = 13,\n    epochs: int = 30,\n    weight_decay: float = 5e-5,\n    log_preds: bool = LOG_TRANSFORM,\n):\n    estimator = DeepAREstimator(\n        freq=\"D\", \n        prediction_length=pred_length,\n        context_length=pred_length\/\/2,\n        use_feat_static_cat = True,\n        use_feat_static_real = True,\n        cardinality = [train['Province_State'].nunique(), train['Country_Region'].nunique()],\n        distr_output=NegativeBinomialOutput(),\n        trainer=Trainer(\n            epochs=epochs,\n            learning_rate=0.005, \n            batch_size=64,\n            weight_decay=weight_decay,\n            learning_rate_decay_factor=0.1,\n            patience=15\n        ),\n    )\n    _, trained_net, predictor = estimator.train_model(\n        training_data = training_data, \n        validation_data = validation_data\n    )\n    \n    return predictor, trained_net\n\npredictor_fatalities, net = fit(training_data_fatalities, val_data_fatalities)\npredictor_cases, case_net = fit(training_data_cases, val_data_cases)\npredictor_fatalities_all, all_net = fit(training_data_fatalities_all, pred_length=30, epochs=60)\npredictor_cases_all, all_case_net = fit(training_data_cases_all, pred_length=30, epochs=60)","66edfe73":"from gluonts.dataset.util import to_pandas\nimport matplotlib.pyplot as plt\nfrom typing import List\n\n## make it run sorted_samples code again!\ndef plot_forecast(\n    predictor,\n    train_df: pd.DataFrame,\n    location: List[str] = ['Italy', 'Italy'],\n    target: str = 'Fatalities',\n    cat_vars: typing.List[str] = ['Province_State', 'Country_Region'],\n    real_vars: typing.List[int] = REAL_VARS,\n    cumulative: bool = True,\n    log_preds: bool = LOG_TRANSFORM,\n    show_gt: bool = True,\n    start_offset: int = 0, \n    fontsize: int = 16,\n    save: bool = False\n):\n    fig = plt.figure(figsize=(15, 6.1), facecolor=\"white\",  edgecolor='k')\n\n    # plot train observations, true observations from public test set, and forecasts\n    location_tr = enc.transform(np.array(location).reshape(1,-1))\n    tr_df = train_df[np.all((train_df[['Province_State', 'Country_Region']].values == location_tr), axis=1)]\n\n    train_obs = ListDataset(\n        [{\n            FieldName.START: tr_df.index[0], \n            FieldName.TARGET: tr_df[target].values,\n            FieldName.FEAT_STATIC_REAL: tr_df[real_vars].values[0],\n            FieldName.FEAT_STATIC_CAT: tr_df[cat_vars].values[0],\n        }],\n        freq = \"D\",\n    )\n    te_df = test_df[np.all((test_df[['Province_State', 'Country_Region']].values == location_tr), axis=1)]\n\n    test_gt = ListDataset(\n        [{\"start\": te_df.index[0], \"target\": te_df[target].values}],\n        freq = \"D\",\n    )\n\n    for train_series, gt, forecast in zip(train_obs, test_gt, predictor.predict(train_obs)):\n        \n        train_series = to_pandas(train_series)\n        gt = to_pandas(gt)\n        \n        if start_offset:\n            train_series = train_series[start_offset:]\n        \n        # connect train series visually (either to GT or to forecast)\n        if show_gt:\n            train_series[train_series.index[-1] + pd.DateOffset(1)] = gt.iloc[0]\n        else:\n            train_series[train_series.index[-1] + pd.DateOffset(1)] = forecast.median[:1][0]\n            \n        # log and\/or cumulative transforms\n        if log_preds:\n            train_series = np.expm1(train_series)\n            gt = np.expm1(gt)\n            forecast.samples = np.expm1(forecast.samples) \n            forecast._sorted_samples_value = None\n        if cumulative:\n            train_series = train_series.cumsum()\n            gt = gt.cumsum() + train_series.iloc[-2]\n            forecast.samples = np.cumsum(forecast.samples, axis=1) + train_series.iloc[-2]\n            forecast._sorted_samples_value = None\n\n        # plot\n        train_series.plot(linewidth=2, label = 'train series')\n        if show_gt:\n            gt.plot(linewidth=2, label = 'test ground truth')\n            \n        # plot layout\n        type_string = 'Cumulative' if cumulative else 'Daily'\n        plt.title(\n            f'{len(forecast.median)} day forecast: {type_string} number of {target} in {location[0]}', \n            fontsize=fontsize\n        )\n        plt.legend(fontsize = fontsize)\n        plt.grid(which='both')\n        if save:\n            forecast.plot(\n                color='g', \n                prediction_intervals=[50.0, 90.0], \n                show_mean = True,\n                output_file = f'{len(forecast.median)} day forecast {type_string} number of {target} in {location[0]}'\n            )\n        else:\n            forecast.plot(color='g', prediction_intervals=[50.0, 90.0], show_mean = True)\n        forecast._sorted_samples_value = None\n        plt.show()\n","77899b94":"# plot public leaderboard case forecasts\nplot_forecast(predictor_cases, train_df, ['Italy', 'Italy'], target='ConfirmedCases')\nplot_forecast(predictor_cases, train_df, ['Iran', 'Iran'], target='ConfirmedCases')\nplot_forecast(predictor_cases, train_df, ['Spain', 'Spain'], target='ConfirmedCases')\nplot_forecast(predictor_cases, train_df, ['Korea, South', 'Korea, South'], target='ConfirmedCases')\nplot_forecast(predictor_cases, train_df, ['New York', 'US'], target='ConfirmedCases')","5ea10421":"# plot private leaderboard case forecasts\nplot_forecast(predictor_cases_all, all_df, ['Italy', 'Italy'], show_gt = False, target='ConfirmedCases')\nplot_forecast(predictor_cases_all, all_df, ['Iran', 'Iran'], show_gt = False, target='ConfirmedCases')\nplot_forecast(predictor_cases_all, all_df, ['New York', 'US'], show_gt = False, target='ConfirmedCases')\nplot_forecast(predictor_cases_all, all_df, ['Hubei', 'China'], show_gt = False, target='ConfirmedCases')\nplot_forecast(predictor_cases_all, all_df, ['Korea, South', 'Korea, South'], show_gt = False, target='ConfirmedCases')\n","a29e5b3d":"from gluonts.evaluation.backtest import make_evaluation_predictions\nfrom gluonts.model.forecast import Forecast\nfrom gluonts.gluonts_tqdm import tqdm\nfrom gluonts.dataset.util import to_pandas\nimport json\nfrom typing import Dict, Union, Tuple, List\n\n# copied from https:\/\/github.com\/awslabs\/gluon-ts\/blob\/master\/src\/gluonts\/evaluation\/_base.py\ndef extract_pred_target(\n    time_series: Union[pd.Series, pd.DataFrame], forecast: Forecast\n) -> np.ndarray:\n    \n    assert forecast.index.intersection(time_series.index).equals(\n        forecast.index\n    ), (\n        \"Cannot extract prediction target since the index of forecast is outside the index of target\\n\"\n        f\"Index of forecast: {forecast.index}\\n Index of target: {time_series.index}\"\n    )\n\n    # cut the time series using the dates of the forecast object\n    return np.atleast_1d(\n        np.squeeze(time_series.loc[forecast.index].transpose())\n    )\n\ndef msle(target, forecast):\n    return np.mean(np.square(np.log1p(forecast) - np.log1p(target)))\n\n# bootstrapped and edited from https:\/\/github.com\/awslabs\/gluon-ts\/blob\/master\/src\/gluonts\/evaluation\/_base.py\ndef get_metrics_per_ts(\n    time_series: Union[pd.Series, pd.DataFrame], forecast: Forecast, keys: List[str]\n) -> Dict[str, Union[float, str, None]]:\n    pred_target = np.array(extract_pred_target(time_series, forecast))\n    \n    try:\n        mean_fcst = forecast.mean\n    except:\n        mean_fcst = None\n    median_fcst = forecast.quantile(0.5)\n\n    metrics = {\n        \"Province_State\": keys[0],\n        \"Country_Region\": keys[1],\n        \"MSLE_on_mean\": msle(pred_target, mean_fcst)\n        if mean_fcst is not None\n        else None,\n        \"MSLE_on_median\": msle(pred_target, median_fcst)\n    }\n\n    return metrics\n\n# bootstrapped and edited from https:\/\/github.com\/awslabs\/gluon-ts\/blob\/master\/src\/gluonts\/evaluation\/_base.py\ndef get_aggregate_metrics(\n    metric_per_ts: pd.DataFrame\n) -> Tuple[Dict[str, float], pd.DataFrame]:\n    agg_funs = {\n        \"MSLE_on_mean\": \"mean\",\n        \"MSLE_on_median\": \"mean\",\n    }\n\n    assert (\n        set(metric_per_ts.columns) >= agg_funs.keys()\n    ), \"The some of the requested item metrics are missing.\"\n\n    totals = {\n        key: metric_per_ts[key].agg(agg) for key, agg in agg_funs.items()\n    }\n    totals[\"RMSLE_on_mean\"] = np.sqrt(totals[\"MSLE_on_mean\"])\n    totals[\"RMSLE_on_median\"] = np.sqrt(totals[\"MSLE_on_median\"])\n\n    return totals, metric_per_ts\n\ndef evaluate(\n    data_df: pd.DataFrame, \n    predictor_fatalities,\n    predictor_cases,\n    num_samples: int = 100,\n    log_preds: bool = LOG_TRANSFORM,\n):\n    \n    all_data_fat = build_dataset(all_df)\n    all_data_case = build_dataset(all_df, target = 'ConfirmedCases')\n\n    rows = []\n    with tqdm(\n        zip(training_data_fatalities, all_data_fat, predictor_fatalities.predict(training_data_fatalities)),\n        total=len(training_data_fatalities),\n        desc=\"Evaluating Fatalities Predictor\",\n    ) as it, np.errstate(invalid=\"ignore\"):\n        for train, ts, f in it:\n            \n            keys = enc.inverse_transform(ts['feat_static_cat'].reshape(1,-1))[0]\n            train = to_pandas(train)\n            ts = to_pandas(ts)\n            \n            # undo log\n            if log_preds:\n                train = np.expm1(train)\n                ts = np.expm1(ts)\n                f.samples = np.expm1(f.samples) \n                \n            f.samples = np.cumsum(f.samples, axis=1) + train.cumsum().iloc[-1]\n            rows.append(get_metrics_per_ts(ts.cumsum(), f, keys))\n            \n    metrics_per_ts_fat = pd.DataFrame(rows, dtype=np.float64)\n    agg_metrics, metrics_per_ts = get_aggregate_metrics(metrics_per_ts_fat)\n    print(json.dumps(agg_metrics, indent=4))\n\n    rows = []\n    with tqdm(\n        zip(training_data_cases, all_data_case, predictor_cases.predict(training_data_cases)),\n        total=len(all_data_case),\n        desc=\"Evaluating Cases Predictor\",\n    ) as it, np.errstate(invalid=\"ignore\"):\n        for train, ts, f in it:\n            \n            keys = enc.inverse_transform(ts['feat_static_cat'].reshape(1,-1))[0]\n            train = to_pandas(train)\n            ts = to_pandas(ts)\n            \n            # undo log\n            if log_preds:\n                train = np.expm1(train)\n                ts = np.expm1(ts)\n                f.samples = np.expm1(f.samples) \n                 \n            f.samples = np.cumsum(f.samples, axis=1) + train.cumsum().iloc[-1]\n            rows.append(get_metrics_per_ts(ts.cumsum(), f, keys))\n            \n    metrics_per_ts_case = pd.DataFrame(rows, dtype=np.float64)\n    agg_metrics, metrics_per_ts = get_aggregate_metrics(metrics_per_ts_case)\n    print(json.dumps(agg_metrics, indent=4))\n    \n    return metrics_per_ts_fat, metrics_per_ts_case\n    \nmetrics_per_ts_fat, metrics_per_ts_case = evaluate(all_df, predictor_fatalities, predictor_cases)","238641fa":"print('Best fit ConfirmedCases series (mean prediction, on validation set):\\n')\nprint(metrics_per_ts_case.sort_values(by='MSLE_on_mean').head(10))\nprint('\\nWorst fit ConfirmedCases series (mean prediction, on validation set):\\n')\nprint(metrics_per_ts_case.sort_values(by='MSLE_on_mean', ascending=False).head(10))","b8dbde3d":"from sklearn import manifold\nfrom matplotlib.ticker import NullFormatter\n\n# get list of countries with most fatalities\ndef visualize_embedding(\n    trained_net,\n    province_state = True,\n    sort_key: str = 'Fatalities',\n    top_n: int = 15\n):\n    group_key = 'Province_State' if province_state else 'Country_Region'\n    countries = train_all.groupby(group_key).sum().sort_values(\n        by=sort_key, \n        ascending = False\n    ).head(top_n).index.tolist()\n\n    # visualize 2-D projection of learned Country\/Region embedding space with TSNE\n    e_key = 0 if province_state else 1\n    embedding = trained_net.collect_params()[f'{trained_net.name}_featureembedder0_cat_{e_key}_embedding_weight'].data()\n    proj = manifold.TSNE(init='pca', random_state = 0).fit_transform(embedding.asnumpy())\n\n    # plot\n    fig = plt.figure(figsize=(15, 6.1), facecolor=\"white\",  edgecolor='k')\n    ax = plt.gca()\n    for country in countries:\n        idx = np.where(enc.categories_[e_key] == country)[0][0]\n        plt.scatter(\n            proj[idx, 0], \n            proj[idx, 1], \n            cmap=plt.cm.Spectral, \n            label = country\n        )\n        ax.annotate(\n            country, \n            (proj[idx, 0], proj[idx, 1]), \n            fontsize=16,\n        )\n    ax.xaxis.set_major_formatter(NullFormatter())\n    ax.yaxis.set_major_formatter(NullFormatter())\n    title_string = 'Country\/Region' if not province_state else 'Province\/State'\n    plt.title(f\"TSNE Visualization of Learned {group_key} Embedding Space\", fontsize=16)\n    plt.show()\n\nvisualize_embedding(all_case_net, sort_key = 'ConfirmedCases')\nvisualize_embedding(all_case_net, sort_key = 'ConfirmedCases', province_state=False)","5a30643e":"# generate submission csv\n\ndef aggregate(\n    all_data: ListDataset,\n    train_data: ListDataset, \n    train_data_all: ListDataset, \n    predictor,\n    predictor_all,\n    log_preds: bool = LOG_TRANSFORM,\n    mean: bool = False,\n):\n    \n    aggregates = []\n    for train, train_all, public_forecast, private_forecast in zip(\n        train_data,\n        train_data_all,\n        predictor.predict(train_data),\n        predictor_all.predict(train_data_all)\n    ):\n        \n        train = to_pandas(train)\n        train_all = to_pandas(train_all)\n        \n        # undo log\n        if log_preds:\n            train = np.expm1(train)\n            train_all = np.expm1(train_all)\n            public_forecast.samples = np.expm1(public_forecast.samples) \n            private_forecast.samples = np.expm1(private_forecast.samples) \n            \n        # accumulate\n        ts = train.cumsum()\n        ts_all = train_all.cumsum()\n        public_forecast.samples = np.cumsum(public_forecast.samples, axis=1) + ts.iloc[-1]\n        private_forecast.samples = np.cumsum(private_forecast.samples, axis=1) + ts_all.iloc[-1]\n    \n        # concatenate\n        public_f = public_forecast.mean if mean else public_forecast.median\n        private_f = private_forecast.mean if mean else private_forecast.median\n        aggregates.append(np.concatenate((public_f, private_f)))  \n    \n    return aggregates\n\ndef submit(\n    filename: str,\n    mean: bool = False,\n):\n    \n    # aggregate fatalities\n    fatalities = aggregate(\n        build_dataset(all_df), \n        training_data_fatalities,\n        training_data_fatalities_all,\n        predictor_fatalities,\n        predictor_fatalities_all,\n        mean = mean\n    )\n\n    # aggregate cases\n    cases = aggregate(\n        build_dataset(all_df, target = 'ConfirmedCases'), \n        training_data_cases,\n        training_data_cases_all,\n        predictor_cases,\n        predictor_cases_all,\n        mean = mean\n    )\n\n    # load test csv \n    sub_df = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-2\/test.csv\")\n\n    # fill 'NaN' Province\/State values with Country\/Region values\n    sub_df['Province_State'] = sub_df['Province_State'].fillna(sub_df['Country_Region'])\n\n    # get forecast ids\n    ids = []\n    for _, df in sub_df.groupby(by=['Province_State', 'Country_Region']):\n        ids.append(df['ForecastId'].values)\n\n    # create submission df\n    submission = pd.DataFrame(\n        list(zip(\n            np.array(ids).flatten(),\n            np.array(cases).flatten(),\n            np.array(fatalities).flatten()\n        )), \n        columns = ['ForecastId', 'ConfirmedCases', 'Fatalities']\n    )\n    submission.to_csv(filename, index=False)\n\nsubmission = submit('submission.csv', mean = True)","13be74ed":"# Fit DeepAR Model Estimates\n\nThe DeepAR model was proposed by David Salinas, Valentin Flunkert, and Jan Gasthaus in \"DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks\" (https:\/\/arxiv.org\/abs\/1704.04110). The approach trains an autoregressive RNN to produces time-variant parameters of a specified distribution on a large collection of related time series. The learned distribution can then be used to produce probabilistic forecasts. Here we use the authors' *GluonTS* implementation (https:\/\/gluon-ts.mxnet.io\/index.html).\n\nWe believe the probabilistic nature of the DeepAR forecasts is a feature that differentiates our approach from others we have seen so far. Specifically, the ability to provide both confidence intervals and point estimates allows one to better understand the range of possible trajectories, from the worst-case scenario, to the best-case scenario, to the expected scenario. ","f83dd54c":"## Plot 13 day case forecasts (March 19th - March 31st)","a40be72c":"## Plot 30 day case forecasts (April 1st - April 30th)","eaea5dcb":"# Data Augmentation\n\n>1. Static 'Province_State' categorical feature, for which model learns embedding\n2. Static 'Country_Region' categorical feature, for which model learns embedding\n3. BERT sentence embeddings for concatentatin of 'Province_State' and 'Country_Region'\n\nCollecting complete datasets for all regions in this dataset is difficult and time-consuming. The complexity of dealing with missing values and incomplete data entries makes merging multiple datasets for data augmentation difficult for this competition. To encode some knowledge about the world to our model, we use [BERT](https:\/\/arxiv.org\/abs\/1810.04805), a natural language processing model. Specifically, we use the [sentence embeddings](https:\/\/github.com\/UKPLab\/sentence-transformers) obtained from this model as a continuous-valued latent vector that represent the world. This has the advantage of encoding a very large corpus of textual data about the world, with no need for model training and with continuous features for all regions.\n","7f365719":"# Data Exploration","f410d44e":"# Sort series by individual contribution to error","d71d775a":"# Calculate metrics on public test","3001b867":"# Plot predictions from fit model parameters","eea50975":"# Visualize learned categorical embeddings\n\nHere we visualize the 2-D projection of the Province\/State and Country\/Region embedding spaces learned by the model using the TSNE algorithm","21c9b78b":"# Data Preprocessing\n\n>1. take difference of data\n>2. fill 'NaN' Province\/State values with Country\/Region values\n>3. apply log transformation to target values"}}