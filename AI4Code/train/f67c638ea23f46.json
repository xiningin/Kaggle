{"cell_type":{"656f6da7":"code","f0cddd1c":"code","743cbaec":"code","826beb7c":"code","244172ea":"code","0412fdbc":"code","adb7c21f":"code","19b18637":"code","48e229c5":"code","0bf68163":"code","b6ff0571":"code","45a88fb8":"code","2088d3dd":"code","38c857cc":"code","1e01b750":"code","e5849e10":"code","6ed72c95":"code","679c002e":"code","baf76a6f":"code","608a467d":"code","04ea29fc":"code","073990a1":"code","e158cd85":"code","b8782cbd":"code","e11387b2":"code","616489e6":"code","9897c77e":"code","4a9e6081":"code","96008c47":"code","37103d09":"code","aec72464":"code","250a6573":"code","f15f2b04":"code","554ab798":"markdown","ce52556d":"markdown","952e84f5":"markdown","2c3294c8":"markdown","e8fa4c2c":"markdown","1e44fd4b":"markdown","c55dda06":"markdown","4cac4496":"markdown","8cb7912e":"markdown","3d6785d8":"markdown","14043632":"markdown","1fe130b6":"markdown","2987654a":"markdown","1245d892":"markdown","bc9389fc":"markdown","a0237b3c":"markdown","cdb9cb4e":"markdown","d595d6e1":"markdown","844bea0c":"markdown","3991c1f8":"markdown","47d6cf47":"markdown","3bdf1673":"markdown","898f5f18":"markdown","05c4c1af":"markdown","eae3da98":"markdown","9072deab":"markdown","c141a357":"markdown","51344e93":"markdown","337ee540":"markdown","26d731a9":"markdown"},"source":{"656f6da7":"# A dependency of the preprocessing for BERT inputs\n!pip data.shapeinstall -q tensorflow-text","f0cddd1c":"!pip install -q tf-models-official","743cbaec":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text\nfrom official.nlp import optimization  # to create AdamW optimizer\n\n#for visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ntf.get_logger().setLevel('ERROR')","826beb7c":"data = pd.read_csv(r'..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv')","244172ea":"data.shape","0412fdbc":"data.head(10)","adb7c21f":"data.isnull().sum()","19b18637":"labeling = {\n    'positive':1, \n    'negative':0\n}\n\ndata['sentiment'] = data['sentiment'].apply(lambda x : labeling[x])\n# Output first ten rows\ndata.head(10)\n","48e229c5":"# The distribution of sentiments\ndata.groupby('sentiment').count().plot(kind='bar')","0bf68163":"# Calculate review lengths\nreview_len = pd.Series([len(review.split()) for review in data['review']])\n\n# The distribution of review text lengths\nreview_len.plot(kind='box')","b6ff0571":"sns.set_theme(\n    context='notebook',\n    style='darkgrid',\n    palette='deep',\n    font='sans-serif',\n    font_scale=1,\n    color_codes=True,\n    rc=None,\n)\n\nplt.figure(figsize = (10,12))\nsns.histplot(review_len)","45a88fb8":"fig = plt.figure(figsize=(14,7))\ndata['length'] = data.review.str.split().apply(len)\nax1 = fig.add_subplot(122)\nsns.histplot(data[data['sentiment']==1]['length'], ax=ax1,color='green')\ndescribe = data.length[data.sentiment==1].describe().to_frame().round(2)\n\nax2 = fig.add_subplot(121)\nax2.axis('off')\nfont_size = 14\nbbox = [0, 0, 1, 1]\ntable = ax2.table(cellText = describe.values, rowLabels = describe.index, bbox=bbox, colLabels=describe.columns)\ntable.set_fontsize(font_size)\nfig.suptitle('Distribution of text length for positive sentiment reviews.', fontsize=16)\n\nplt.show()","2088d3dd":"fig = plt.figure(figsize=(14,7))\nax1 = fig.add_subplot(122)\nsns.histplot(data[data['sentiment']==0]['length'], ax=ax1,color='red')\ndescribe = data.length[data.sentiment==0].describe().to_frame().round(2)\n\nax2 = fig.add_subplot(121)\nax2.axis('off')\nfont_size = 14\nbbox = [0, 0, 1, 1]\ntable = ax2.table(cellText = describe.values, rowLabels = describe.index, bbox=bbox, colLabels=describe.columns)\ntable.set_fontsize(font_size)\nfig.suptitle('Distribution of text length for Negative sentiment reviews.', fontsize=16)\n\nplt.show()","38c857cc":"from wordcloud import WordCloud\nplt.figure(figsize = (20,20)) # Negative Review Text\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(data[data.sentiment == 0].review))\nplt.imshow(wc , interpolation = 'bilinear')","1e01b750":"from wordcloud import WordCloud\nplt.figure(figsize = (20,20)) # Positive Review Text\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(data[data.sentiment == 1].review))\nplt.imshow(wc , interpolation = 'bilinear')","e5849e10":"bert_model_name = 'small_bert\/bert_en_uncased_L-4_H-512_A-8'  \n\ntfhub_handle_encoder = 'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-4_H-512_A-8\/1'\ntfhub_handle_preprocess = 'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3'\n\nprint(f'BERT model selected           : {tfhub_handle_encoder}')\nprint(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')","6ed72c95":"bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)","679c002e":"text_test = ['this is such an amazing movie!']\ntext_preprocessed = bert_preprocess_model(text_test)\n\nprint(f'Keys       : {list(text_preprocessed.keys())}')\nprint(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\nprint(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\nprint(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\nprint(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')","baf76a6f":"bert_model = hub.KerasLayer(tfhub_handle_encoder)","608a467d":"def build_classifier_model():\n  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n  encoder_inputs = preprocessing_layer(text_input)\n  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n  outputs = encoder(encoder_inputs)\n  net = outputs['pooled_output']\n  net = tf.keras.layers.Dropout(0.1)(net)\n  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n  return tf.keras.Model(text_input, net)","04ea29fc":"classifier_model = build_classifier_model()","073990a1":"tf.keras.utils.plot_model(classifier_model)","e158cd85":"loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\nmetrics = tf.metrics.BinaryAccuracy()","b8782cbd":"epochs = 5\nsteps_per_epoch = 625 #tf.data.experimental.cardinality(train_data).numpy()\nnum_train_steps = steps_per_epoch * epochs\nnum_warmup_steps = int(0.1*num_train_steps)\n\ninit_lr = 3e-5\noptimizer = optimization.create_optimizer(init_lr=init_lr,\n                                          num_train_steps=num_train_steps,\n                                          num_warmup_steps=num_warmup_steps,\n                                          optimizer_type='adamw')","e11387b2":"classifier_model.compile(optimizer=optimizer,\n                         loss=loss,\n                         metrics=metrics)","616489e6":"X_train = train_data['text']\ny_train = train_data['label']\nX_valid = valid_data['text']\ny_valid = valid_data['label']","9897c77e":"X = data['review']\nfrom sklearn.model_selection import train_test_split\n#X_train,X_test,y_train,y_test = train_test_split(data.review,data.sentiment,random_state = 0 , stratify = data.sentiment)\n\n#valid_size=5000\n#X_valid, y_valid = X_train[-valid_size:], y_train[-valid_size:]\n#X_test, y_test = X_train[:-valid_size], y_train[:-valid_size]\nX_train, X_test, y_train, y_test = train_test_split(data.review,data.sentiment, test_size=0.2, random_state=1)\n\nX_train, X_val, y_train, y_val  = train_test_split(X_train, y_train, test_size=0.16, random_state=1) # 0.25 x 0.8 = 0.2\n#print('Train Set ->', X_train.shape, y_train.shape)\n#print('Validation Set ->', X_valid.shape, y_valid.shape)\n#print('Test Set ->', X_test.shape, y_test.shape)\n\nprint('Number of reviews in the total set : {}'.format(len(X)))\nprint('Number of reviews in the training set : {}'.format(len(X_train)))\nprint('Number of reviews in the validation set : {}'.format(len(X_valid)))\nprint('Number of reviews in the testing set : {}'.format(len(X_test)))","4a9e6081":"print(f'Training model with {tfhub_handle_encoder}')\nhistory = classifier_model.fit(X_train, y_train,\n                               validation_data=(X_valid, y_valid),\n                               epochs=epochs)","96008c47":"#X_test = test_data['text']\n#y_test = test_data['label']\nloss, accuracy = classifier_model.evaluate(X_test, y_test)\n\nprint(f'Loss: {loss}')\nprint(f'Accuracy: {accuracy}')","37103d09":"history_dict = history.history\nprint(history_dict.keys())\n\nacc = history_dict['binary_accuracy']\nval_acc = history_dict['val_binary_accuracy']\nloss = history_dict['loss']\nval_loss = history_dict['val_loss']\n\nepochs = range(1, len(acc) + 1)\nfig = plt.figure(figsize=(10, 6))\nfig.tight_layout()\n\nplt.subplot(2, 1, 1)\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss, 'r', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\n# plt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(2, 1, 2)\nplt.plot(epochs, acc, 'r', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')","aec72464":"dataset_name = 'imdb_2'\nsaved_model_path = '.\/{}_bert'.format(dataset_name.replace('\/', '_'))\n\nclassifier_model.save(saved_model_path, include_optimizer=False)","250a6573":"reloaded_model = tf.saved_model.load(saved_model_path)","f15f2b04":"def print_my_examples(inputs, results):\n  result_for_printing = \\\n    [f'input: {inputs[i]:<30} : score: {results[i][0]:.6f}'\n                         for i in range(len(inputs))]\n  print(*result_for_printing, sep='\\n')\n  print()\n\n\nexamples = [\n    'this is such an amazing movie!',  # this is the same sentence tried earlier\n    'The movie was great!',\n    'The movie was meh.',\n    'The movie was idiotic.',\n    'The movie was terrible...'\n]\n\nreloaded_results = tf.sigmoid(reloaded_model(tf.constant(examples)))\noriginal_results = tf.sigmoid(classifier_model(tf.constant(examples)))\n\nprint('Results from the saved model:')\nprint_my_examples(examples, reloaded_results)\nprint('Results from the model in memory:')\nprint_my_examples(examples, original_results)","554ab798":"## Model training\n\nNext we train a model since we have the preprocessing module, BERT encoder, data, and classifier.","ce52556d":"## Defining model\n\nWe have created a very simple fine-tuned model, with the preprocessing model, the selected BERT model, one Dense and a Dropout layer.","952e84f5":"Now, let us visualize how long our sentences are in the training data.","2c3294c8":"Now we encode the positive sentiment to 1 and negative sentiment to 0","e8fa4c2c":"Next we compile the model with the loss, metric and optimizer.","1e44fd4b":"## EDA","c55dda06":"## Saving Model\n\nSaving our fine-tuned model for later use.","4cac4496":"### Loss function\n\nSince this is a binary classification problem and the model outputs a probability (a single-unit layer), we'll use `losses.BinaryCrossentropy` loss function.","8cb7912e":"\n## Installing and importing dependencies\n\nFirst let us import all the modules and packages that will be required.\n","3d6785d8":"Thus there are 50000 rows and 2 columns in the `data` dataframe ","14043632":" Testing our model on any sentence we want, by adding to the `examples` variable below.","1fe130b6":"## The preprocessing model\n\nText inputs need to be transformed to numeric token ids and arranged in several Tensors before being input to BERT. TensorFlow Hub provides a matching preprocessing model for each of the BERT models discussed above, which implements this transformation using TF ops from the TF.text library. It is not necessary to run pure Python code outside your TensorFlow model to preprocess text.\n\nThe preprocessing model must be the one referenced by the documentation of the BERT model. \n\nNote: You will load the preprocessing model into a [hub.KerasLayer](https:\/\/www.tensorflow.org\/hub\/api_docs\/python\/hub\/KerasLayer) to compose your fine-tuned model. This is the preferred API to load a TF2-style SavedModel from TF Hub into a Keras model.","2987654a":"## Evaluate the model\n\nLet's see how the model performs. Two values will be returned. Loss (a number which represents the error, lower values are better), and accuracy.","1245d892":"#### WORDCLOUD FOR NEGATIVE TEXT (LABEL - 0)","bc9389fc":"## How to use BERT (Fine-tuning)\n\nUsing BERT for a specific task is relatively straightforward:\n\nBERT can be used for a wide variety of language tasks, while only adding a small layer to the core model:\n\n * Classification tasks such as sentiment analysis are done similarly to Next Sentence classification, by adding a classification layer on top of the Transformer output for the [CLS] token.\n * In Question Answering tasks (e.g. SQuAD v1.1), the software receives a question regarding a text sequence and is required to mark the answer in the sequence. Using BERT, a Q&A model can be trained by learning two extra vectors that mark the beginning and the end of the answer.\n * In Named Entity Recognition (NER), the software receives a text sequence and is required to mark the various types of entities (Person, Organization, Date, etc) that appear in the text. Using BERT, a NER model can be trained by feeding the output vector of each token into a classification layer that predicts the NER label.\n \n \nRefs: https:\/\/towardsdatascience.com\/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270","a0237b3c":"Now, we define the test and validation variables to input in the model for evaluation and prediction.","cdb9cb4e":"As we can see the `sentiment` column has 2 values:\n\n   * Negative Sentiment\n   * Postive Sentiment\n  ","d595d6e1":"This means that the no. of positive reviews is equal to the no. of negative reviews in the dataset. This is a good thing since it means our dataset is not skewed.","844bea0c":"#### WORDCLOUD FOR POSITIVE TEXT (LABEL - 1)","3991c1f8":"## BERT\n\nBERT (Bidirectional Encoder Representations from Transformers) is a recent paper published by researchers at Google AI Language. It has caused a stir in the Machine Learning community by presenting state-of-the-art results in a wide variety of NLP tasks, including Question Answering (SQuAD v1.1), Natural Language Inference (MNLI), and others.\n\nBERT\u2019s key technical innovation is applying the bidirectional training of Transformer, a popular attention model, to language modelling. This is in contrast to previous efforts which looked at a text sequence either from left to right or combined left-to-right and right-to-left training. The paper\u2019s results show that a language model which is bidirectionally trained can have a deeper sense of language context and flow than single-direction language models. In the paper, the researchers detail a novel technique named Masked LM (MLM) which allows bidirectional training in models in which it was previously impossible.\n\nPaper: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https:\/\/arxiv.org\/abs\/1810.04805)","47d6cf47":"Thus, there are no missing values in any of the columns of the dataset.","3bdf1673":"## Loading and Exploring dataset\n\n\n\nThere is one file provided to us in this dataset:\n\n * `IMDB-Dataset.csv`: The CSV file containing all the reviews and their polarity.\n \nLet's read the file.\n","898f5f18":"## NOTE : \n\nVersion 1 of the script uses the dataset [imdb-dataset-sentiment-analysis-in-csv-format](https:\/\/www.kaggle.com\/columbine\/imdb-dataset-sentiment-analysis-in-csv-format) whereas Version 2 uses the dataset [imdb-dataset-of-50k-movie-reviews](https:\/\/www.kaggle.com\/lakshmi25npathi\/imdb-dataset-of-50k-movie-reviews)\n\nPlease select appropriate version according to the dataset analyzed","05c4c1af":"As you can see, now you have the 3 outputs from the preprocessing that a BERT model would use (`input_words_id`, `input_mask` and `input_type_ids`).\n\nSome other important points:\n- The input is truncated to 128 tokens. \n- The `input_type_ids` only have one value (0) because this is a single sentence input. For a multiple sentence input, it would have one number for each input.\n\nSince this text preprocessor is a TensorFlow model, It can be included in your model directly.","eae3da98":"### Optimizer\n\nFor fine-tuning, let's use the same optimizer that BERT was originally trained with: the \"Adaptive Moments\" (Adam). This optimizer minimizes the prediction loss and does regularization by weight decay (not using moments), which is also known as [AdamW](https:\/\/arxiv.org\/abs\/1711.05101).","9072deab":"## Loading model from TensorFlow Hub\n\nHere you can choose which BERT model you will load from TensorFlow Hub and fine-tune. There are multiple BERT models available.\n\n  - [BERT-Base](https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/3), [Uncased](https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/3) and [seven more models](https:\/\/tfhub.dev\/google\/collections\/bert\/1) with trained weights released by the original BERT authors.\n  - [Small BERTs](https:\/\/tfhub.dev\/google\/collections\/bert\/1) have the same general architecture but fewer and\/or smaller Transformer blocks, which lets you explore tradeoffs between speed, size and quality.\n  - [ALBERT](https:\/\/tfhub.dev\/google\/collections\/albert\/1): four different sizes of \"A Lite BERT\" that reduces model size (but not computation time) by sharing parameters between layers.\n  - [BERT Experts](https:\/\/tfhub.dev\/google\/collections\/experts\/bert\/1): eight models that all have the BERT-base architecture but offer a choice between different pre-training domains, to align more closely with the target task.\n  - [Electra](https:\/\/tfhub.dev\/google\/collections\/electra\/1) has the same architecture as BERT (in three different sizes), but gets pre-trained as a discriminator in a set-up that resembles a Generative Adversarial Network (GAN).\n  - BERT with Talking-Heads Attention and Gated GELU [[base](https:\/\/tfhub.dev\/tensorflow\/talkheads_ggelu_bert_en_base\/1), [large](https:\/\/tfhub.dev\/tensorflow\/talkheads_ggelu_bert_en_large\/1)] has two improvements to the core of the Transformer architecture.\n\nThe model documentation on TensorFlow Hub has more details and references to the\nresearch literature. Follow the links above, or click on the [`tfhub.dev`](http:\/\/tfhub.dev) URL\nprinted after the next cell execution.\n\nHere, we have used a Small BERT (with fewer parameters) since they are faster to fine-tune. If you like a small model but with higher accuracy, ALBERT might be your next option. If you want even better accuracy, choose\none of the classic BERT sizes or their recent refinements like Electra, Talking Heads, or a BERT Expert.\n","c141a357":"\n## Overview\n\nThis script performs EDA and then fine-tunes BERT to perform sentiment analysis on a dataset of plain-text IMDB movie reviews. In addition to training a model, it preprocesses text into an appropriate format.\n","51344e93":"Reloading saved model","337ee540":"Let's try the preprocessing model on sample text and see the output:","26d731a9":"For the fine-tuning I have used the`pooled_output` array which represents each input sequence as a whole. The shape is [batch_size, H]. You can think of this as an embedding for the entire movie review."}}