{"cell_type":{"20dc5f95":"code","35720687":"code","3d881033":"code","f18f6a0a":"code","0a94c6f5":"code","7b8e6195":"code","a15795b3":"code","6a288e28":"code","da04f91b":"code","1a877033":"code","1ea4c3ad":"code","d5c192f2":"code","907400f2":"code","2aad13a6":"code","0d7be9de":"code","ae9b99d1":"code","b9363ceb":"code","cd6211ad":"code","75224a29":"code","c353ae76":"code","e0fae278":"code","5722c00f":"code","95a74f1a":"code","85713c56":"code","54ad0e5c":"code","54ee2bce":"code","a6fa4692":"code","01027e3f":"code","5edc934a":"code","5f711f64":"code","a9112b5b":"code","1e048734":"code","467057fc":"code","61c95951":"code","263586b1":"code","fc23fa3a":"code","bfaa73bb":"code","d67246d8":"code","bcf39c2d":"markdown","e61c4f2c":"markdown","3f7a6e37":"markdown","59221517":"markdown","705aac92":"markdown","174e8558":"markdown","48fd4df6":"markdown","c2bbf320":"markdown","ae21ef98":"markdown","0b2f5443":"markdown","55d2d9f9":"markdown","3e528e48":"markdown","6e246364":"markdown","13a84979":"markdown","69f04d4c":"markdown","264b0a64":"markdown","eae64754":"markdown"},"source":{"20dc5f95":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","35720687":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, OrdinalEncoder\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split as tts, RandomizedSearchCV\nfrom sklearn.metrics import classification_report, confusion_matrix, recall_score, f1_score, precision_score, roc_auc_score, roc_curve, auc\nfrom lightgbm.sklearn import LGBMClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier \nimport six\nimport sys\nsys.modules['sklearn.externals.six'] = six\nfrom imblearn.over_sampling import SVMSMOTE\nfrom imblearn.ensemble import EasyEnsembleClassifier\nfrom sklearn.inspection import permutation_importance\nimport eli5\nfrom eli5.sklearn import PermutationImportance","3d881033":"missing_values = [\"n\/a\", \"na\", \"--\", \"NONE\", \"None\", \"none\", \"NA\", \"N\/A\",'inf','-inf', '?', 'Null', 'NULL']\ntrain_data = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv', na_values = missing_values)\ntrain_data.drop(['enrollee_id', 'city'], 1, inplace=True)\ntrain_data.head()","f18f6a0a":"train_data.shape","0a94c6f5":"train_data.info()","7b8e6195":"print(train_data.company_size.value_counts())\ntrain_data['company_size'] = train_data['company_size'].replace('10\/49', np.nan)\nprint(\"==============================\")\nprint(train_data.company_size.value_counts())","a15795b3":"for col_name in train_data.columns:\n  if (train_data[col_name].dtypes == 'int64' or train_data[col_name].dtypes == 'float64' or train_data[col_name].dtypes == 'object'):\n    unique_cat = len(train_data[col_name].unique())\n    print(\"Feature '{col_name}' has '{unique_cat}' unique categories\".format(col_name = col_name, unique_cat = unique_cat))","6a288e28":"train_data.isnull().sum()","da04f91b":"to_LabelEncode = train_data[['gender', 'relevent_experience',\n       'enrolled_university', 'education_level', 'major_discipline',\n       'experience', 'company_size', 'company_type', 'last_new_job']]\n\nle = LabelEncoder()\ntrain_temp = to_LabelEncode.astype(\"str\").apply(le.fit_transform)\ntrain_final = train_temp.where(~to_LabelEncode.isna(), to_LabelEncode)","1a877033":"train_data.drop(['gender', 'relevent_experience','enrolled_university', 'education_level', 'major_discipline','experience', 'company_size', 'company_type', 'last_new_job'],1,inplace=True)","1ea4c3ad":"train_data = train_final.join(train_data)\ntrain_data","d5c192f2":"lr = LinearRegression()\nmice_imputer = IterativeImputer(random_state=42, estimator=lr, max_iter=10, n_nearest_features=2, imputation_order = 'roman')\ntrain_final_df = mice_imputer.fit_transform(train_data)\n\ntrain_final_df = pd.DataFrame(train_final_df)\ntrain_final_df.columns = ['gender', 'relevent_experience', 'enrolled_university', 'education_level', 'major_discipline',\n                                                         'experience', 'company_size', 'company_type', 'last_new_job', 'city_development_index', 'training_hours', 'target']\n                                                        \ntrain_final_df","907400f2":"final_train = train_final_df.copy()\nfinal_train.isnull().sum()","2aad13a6":"final_train.target.value_counts()","0d7be9de":"X = final_train.drop('target',1)\ny = final_train.target\n\nX_train,X_test,y_train,y_test = tts(X,y,test_size=0.25, random_state=42)\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","ae9b99d1":"svm_smote = SVMSMOTE(sampling_strategy='minority', random_state=42, k_neighbors=5)\nX_svm_smote, y_svm_smote = svm_smote.fit_resample(X,y)\n\nX_train_svm, X_test_svm, y_train_svm, y_test_svm = tts(X_svm_smote,y_svm_smote, test_size=0.25, random_state=42)\n\nsc = StandardScaler()\nX_train_svm = sc.fit_transform(X_train_svm)\nX_test_svm = sc.transform(X_test_svm)","b9363ceb":"def evaluate(model, X_test, y_test):\n    y_pred = model.predict(X_test)\n    errors = abs(y_pred - y_test)\n    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n    print(classification_report(y_test,y_pred))\n    print(confusion_matrix(y_test,y_pred))\n    print('Recall Score = ',recall_score(y_test, y_pred))\n    print('Precision Score = ',precision_score(y_test, y_pred))\n    print('F1 score = ', f1_score(y_test,y_pred))\n\n    return evaluate","cd6211ad":"def train_auc_roc_curve(model, X_test, y_test, X_train, y_train):\n  base_fpr,base_tpr,base_threshold = roc_curve(y_train, model.predict(X_train))\n  plt.plot([0,1])\n  plt.plot(base_fpr,base_tpr)\n  print(\"auc score :\",auc(base_fpr,base_tpr))\n  \n\n  return train_auc_roc_curve","75224a29":"easy_lgbm = EasyEnsembleClassifier(base_estimator= LGBMClassifier(random_state=42), n_estimators=250, n_jobs=1,\n                       random_state=42, replacement=True,\n                       sampling_strategy='auto', verbose=0,\n                       warm_start=True)\neasy_lgbm.fit(X_train_svm, y_train_svm)\nevaluate(easy_lgbm, X_test_svm, y_test_svm)","c353ae76":"print(classification_report(y_train_svm,easy_lgbm.predict(X_train_svm)))\nprint(confusion_matrix(y_train_svm,easy_lgbm.predict(X_train_svm)))\nprint('Recall Score = ',recall_score(y_train_svm,easy_lgbm.predict(X_train_svm)))\nprint('Precision Score = ',precision_score(y_train_svm,easy_lgbm.predict(X_train_svm)))","e0fae278":"print(f1_score(y_train_svm, easy_lgbm.predict(X_train_svm)))\nprint(f1_score(y_test_svm, easy_lgbm.predict(X_test_svm)))\n\npredict_proba_easy_lgbm = pd.DataFrame(easy_lgbm.predict_proba(X_test_svm))\npredict_proba_easy_lgbm","5722c00f":"eli5_permutation = PermutationImportance(estimator = easy_lgbm, scoring = 'f1', random_state=42, n_iter = 5)\neli5_permutation.fit(X_test_svm, y_test_svm)","95a74f1a":"eli5_permutation.feature_importances_.T.reshape(-1,1)","85713c56":"eli5.show_weights(eli5_permutation, feature_names = X.columns.to_list())","54ad0e5c":"feature_importance_with_eli5=pd.DataFrame(np.hstack((np.array([X.columns[0:]]).T, eli5_permutation.feature_importances_.T.reshape(-1,1))), columns=['feature', 'importance'])\nfeature_importance_with_eli5['importance']=pd.to_numeric(feature_importance_with_eli5['importance'])\nfeature_importance_with_eli5.sort_values(by='importance', ascending=False)","54ee2bce":"plt.figure(figsize = (15,8))\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\n# We sort by importance and get the features\nsns.barplot(x = 'importance', y = 'feature', data = feature_importance_with_eli5, \n            order = feature_importance_with_eli5.sort_values('importance', ascending=False).feature) ","a6fa4692":"train_auc_roc_curve(easy_lgbm, X_test_svm, y_test_svm, X_train_svm, y_train_svm)","01027e3f":"missing_values = [\"n\/a\", \"na\", \"--\", \"NONE\", \"None\", \"none\", \"NA\", \"N\/A\",'inf','-inf', '?', 'Null', 'NULL']\ntest_data = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv', na_values= missing_values)\ntest_data.drop(['enrollee_id', 'city'], 1, inplace=True)\ntest_data.head()","5edc934a":"test_data.isnull().sum()","5f711f64":"test_data['company_size'] = test_data['company_size'].replace('10\/49', np.nan)\ntest_data['company_size'].value_counts()","a9112b5b":"to_LabelEncode_test = test_data[['gender', 'relevent_experience',\n       'enrolled_university', 'education_level', 'major_discipline',\n       'experience', 'company_size', 'company_type', 'last_new_job']]\n\ntest_temp = to_LabelEncode_test.astype(\"str\").apply(le.fit_transform)\ntest_final = test_temp.where(~to_LabelEncode_test.isna(), to_LabelEncode_test)","1e048734":"test_data.drop(['gender', 'relevent_experience','enrolled_university', 'education_level', 'major_discipline','experience', 'company_size', 'company_type', 'last_new_job'],1,inplace=True)","467057fc":"test_data = test_final.join(test_data)","61c95951":"test_final_df = mice_imputer.fit_transform(test_data)\n\ntest_final_df = pd.DataFrame(test_final_df)\ntest_final_df.columns = ['gender', 'relevent_experience', 'enrolled_university', 'education_level', 'major_discipline',\n                                                         'experience', 'company_size', 'company_type', 'last_new_job', 'city_development_index', 'training_hours']\n                                                        \ntest_final_df","263586b1":"test_final_df = sc.transform(test_final_df)","fc23fa3a":"prediction = pd.DataFrame(easy_lgbm.predict(test_final_df))\nprediction.value_counts()","bfaa73bb":"threshold = 0.5\nmy_pred = np.where(prediction>threshold,'Will join the company','Will not join the company')\n\nmy_pred = my_pred.T.reshape(-1,1)\nmy_pred = pd.DataFrame(my_pred, columns=['Decision'])\nmy_pred","d67246d8":"my_pred = my_pred.join(pd.DataFrame(easy_lgbm.predict_proba(test_final_df)), lsuffix='_right', rsuffix='_left')\nmy_pred = my_pred.rename({0 : 'Probablity of not joining', 1 : 'Probablity of joining'}, axis=1)\nmy_pred","bcf39c2d":"MICE (Multiple Imputation by Chained Equations) Imputation. Its a multiple imputation method, it is generally better than  single imputation method like mean imputation.","e61c4f2c":"Splitting into X and y and than standardizing it using Standard Scaler","3f7a6e37":"Importing train data","59221517":"Heavy class imbalance is present in the data","705aac92":"I used eli5 library to find out feature importance.","174e8558":"Applied SVMSmote, I also applied different variants of Smote like SMOTE, SMOTE-NC, KmeansSMOTE, AdasysMOTE, BorderlineSMOTE. KmeansSMOTE also gave me very good result but due some compatibility issue of kmeansSmote which uses sklearn version 0.20 only and MICE imputation required newer version of sklearn version, so I switched kmeans smote to SVMSmote","48fd4df6":"This is a very good score","c2bbf320":"Applied EasyEnsembleClassifier of imblearn","ae21ef98":"This shows the lowest and highest importance of every feature","0b2f5443":"Importing all libraries","55d2d9f9":"That's it, the project is completed.\n\nWhat I have done:\n1. Loaded Libraries and train data\n2. Deleted the unwanted columns.\n3. Cleaned some Human Error\n4. Label Encoded the data\n5. Missing value Imputation via MICE technique\n6. Checked for Class Imbalance\n7. Splitted data into X and y, Standardized it.\n8. Applied SVMSmote and solved class imbalance issue.\n9. Applied Easy Ensemble Classifier Model with base estimator as Default LGBMClassifier of Imblearn package\n10. Checked the feature importance according to the model using eli5 library\n11. Finalized the Easy Ensemble Classifier model with base estimator as Default LGBMClassifier.\n12. Predicted on Test Data. ","3e528e48":"Test Data","6e246364":"Just checking total unique values in every column","13a84979":"Predicting the f1 score of both train and test(validation) and printing the probablity of prediction","69f04d4c":"Now we dont have any null values","264b0a64":"Gender is most important factor to understand whether he or she will change the job or not, followed by City Development Index and Company Type","eae64754":"In the dataset, there is some Human error in column company size i.e. Oct-49 and in pandas it was printed as 10\/49, so we need to convert into np.nan(NaN)"}}