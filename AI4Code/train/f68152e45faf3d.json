{"cell_type":{"adcb8193":"code","e80e2ead":"code","32930c2f":"code","cbd4829e":"code","f7f8109b":"code","975e4be7":"code","45db78e8":"code","72249244":"code","30b38972":"code","22887120":"code","333f2643":"code","afb8baa6":"code","3585233e":"code","6c1dc539":"code","7f6fe798":"code","15f073eb":"code","ccb23174":"code","7562c859":"code","e274e08a":"code","688522a5":"code","17de7871":"code","7fb59d8e":"code","2b57beb7":"code","54df9f2f":"code","78a6940e":"code","4027c761":"code","8391e56e":"code","0bf1e7c7":"code","fd78d443":"code","901e4ce6":"markdown","57a7cebc":"markdown","1024510e":"markdown","fe870075":"markdown","24c18890":"markdown","0a6a060c":"markdown","206755f8":"markdown","dd47c104":"markdown","d7694111":"markdown","273ff63e":"markdown","be614412":"markdown"},"source":{"adcb8193":"# conda install -c nvidia -c rapidsai -c numba -c conda-forge -c defaults cudf\n","e80e2ead":"# load pkgs \nimport pandas as pd\n# import cudf  # turn GPU to use\nimport sys, os, shutil\nimport numpy as np\nfrom tqdm.notebook import tqdm\nimport warnings\n# warnings.filterwarnings(\"ignore\")\n\n# Machine Learning pkgs\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport optuna\nimport xgboost as xgb\n\nimport datatable as dt\nimport gc\n# plotting stuff\nfrom pandas.plotting import lag_plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\ncolorMap = sns.light_palette(\"blue\", as_cmap=True)\nimport psutil\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","32930c2f":"def reduce_mem_usage(df, verbose=True):\n    numerics = set(['int16', 'int32', 'int64', 'float16', 'float32', 'float64'])\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    def helper(dfCol):\n        colType = dfCol.dtypes\n        if colType in numerics:\n            cMin = dfCol.min()\n            cMax = dfCol.max()\n            if str(colType)[:3] == \"int\":\n                if cMin > np.iinfo(np.int8).min and cMax < np.iinfo(np.int8).max:\n                    dfCol = dfCol.astype(np.int8)\n                elif cMin > np.iinfo(np.int16).min and cMax < np.iinfo(np.int16).max:\n                    dfCol = dfCol.astype(np.int16)\n                elif cMin > np.iinfo(np.int32).min and cMax < np.iinfo(np.int32).max:\n                    dfCol = dfCol.astype(np.int32)\n                elif cMin > np.iinfo(np.int64).min and cMax < np.iinfo(np.int64).max:\n                    dfCol = dfCol.astype(np.int64)  \n            else:\n                if cMin > np.finfo(np.float16).min and cMax < np.finfo(np.float16).max:\n                    dfCol = dfCol.astype(np.float16)\n                elif cMin > np.finfo(np.float32).min and cMax < np.finfo(np.float32).max:\n                    dfCol = dfCol.astype(np.float32)\n                else:\n                    dfCol = dfCol.astype(np.float64)    \n        return dfCol    \n    df = train.apply(helper, axis = 0)\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","cbd4829e":"%%time\n\n# load data\ntrain  = pd.read_csv('\/kaggle\/input\/jane-street-market-prediction\/train.csv')\nmeta_features = pd.read_csv('..\/input\/jane-street-market-prediction\/features.csv')\ntest = pd.read_csv('..\/input\/jane-street-market-prediction\/example_test.csv')\n\n# reduce memory\n# train  = reduce_mem_usage(pd.read_csv('\/kaggle\/input\/jane-street-market-prediction\/train.csv'))\n# meta_features = reduce_mem_usage(pd.read_csv('..\/input\/jane-street-market-prediction\/features.csv'))\n\nprint(f'train data shape is {train.shape}.')\n# print(f'test data shape is {test.shape}.')\nprint(f'features shape is {meta_features.shape}.')\ntrain.head()","f7f8109b":"# print(psutil.virtual_memory().percent)","975e4be7":"# train.describe()","45db78e8":"# print(train.columns)  #date+weight+resp1-4+resp+features0-129+ts-id = 138 meta features\n# print()\n\n# # check unique tsid\n# if len(train.ts_id.unique()) == 2390491:\n#     print('Each ts_id is unique')","72249244":"# def get_feature_mean(df):\n#     \"\"\" \n#     \"\"\"\n#     feature_list = []\n#     for column in df.columns:\n#         feature_median = train[column].mean(axis=1)\n#         df[column].fillna(feature_median)\n#         feature_list.append(feature_median)\n#     return df, feature_list\n\n# filled_train = get_feature_mean()\n\n# featrue_mean = train.apply(lambda x: pd.DataFrame.mean(x, skipna=True), axis=0)","30b38972":"train.mean()","22887120":"# for col_name, col_mean in zip(train.columns, featrue_mean):\n#     train[col_name].fillna(col_mean)\ntrain.fillna(train.mean(), inplace=True)","333f2643":"# remove all the rows with weight = 0\ntrain = train.query('weight > 0').reset_index(drop=True)\nprint(f'Shape of weight=0 removed train dataset: {train.shape}')\n\n# Sort the data\ntrain.sort_values(by=['date', 'ts_id'], inplace=True)","afb8baa6":"# summmarize the massing value\nfeatures_w_missing = list(train.columns[train.isna().sum()>0])\nfeatures_wo_mssing = list(train.columns[train.isna().sum()==0])\nprint(f'Number of features with missing value = {len(train.isnull().sum()[train.isna().sum()>0])}')\n\nN_FEATURES = len(features_w_missing)\nnan_val = train.isna().sum()[train.isna().sum() > 0].sort_values(ascending=False)\n# print(nan_val)\n\nfig, axs = plt.subplots(figsize=(20, 18))\nsns.barplot(y = nan_val.index[0:N_FEATURES], x = nan_val.values[0:N_FEATURES], alpha = 0.7)\nplt.title(f'Missing values of train dataset (Top {N_FEATURES} features)')\nplt.xlabel('Number of Missing values')\nplt.show()","3585233e":"# Break the train df into parts (w missing values, 4 levels) and wo missing values\n\nfeatures_w_missing_g0_list = list(train.columns[train.isna().sum()>=100000])\nprint(features_w_missing_g0_list)\nfeatures_w_missing_g1_list = list(train.columns[(train.isna().sum()>=40000) & (train.isna().sum()<100000)])\nprint(features_w_missing_g1_list)\nfeatures_w_missing_g2_list = list(train.columns[(train.isna().sum()>=3000) & (train.isna().sum()<40000)])\nprint(features_w_missing_g2_list)\nfeatures_w_missing_g3_list = list(train.columns[(train.isna().sum()<3000) & (train.isna().sum()>0)])\nprint(features_w_missing_g3_list)\n\nprint(len(features_w_missing_g0_list)+len(features_w_missing_g1_list)+len(features_w_missing_g2_list)+len(features_w_missing_g3_list))\n\ntrain_w_missing_g0 = train[train.columns & features_w_missing_g0_list]\nprint(f'shape of group 0 is {train_w_missing_g0.shape}.')\ntrain_w_missing_g1 = train[train.columns & features_w_missing_g1_list]\nprint(f'shape of group 1 is {train_w_missing_g1.shape}.')\ntrain_w_missing_g2 = train[train.columns & features_w_missing_g2_list]\nprint(f'shape of group 2 is {train_w_missing_g2.shape}.')\ntrain_w_missing_g3 = train[train.columns & features_w_missing_g3_list]\nprint(f'shape of group 3 is {train_w_missing_g3.shape}.')\n\ntrain_wo_missing = train[train.columns & features_wo_mssing]\nprint(f'shape of wo-missing values data is {train_wo_missing.shape}.')","6c1dc539":"# Check the distribution of data df group 0\n\nfig, axes = plt.subplots(nrows=3, ncols=5, figsize=(20, 15))\npalette = itertools.cycle(sns.color_palette())\n\nfor i, column in enumerate(train_w_missing_g0.columns):\n    sns.distplot(train_w_missing_g0[column], ax=axes[i\/\/5, i%5], color=next(palette))\n    \n# axes.set_title('Group 0 Distribution', size=12)","7f6fe798":"# Check the distribution of data df group 1\n\nfig, axes = plt.subplots(nrows=3, ncols=6, figsize=(20, 15))\npalette = itertools.cycle(sns.color_palette())\n\nfor i, column in enumerate(train_w_missing_g1.columns):\n    sns.distplot(train_w_missing_g1[column], ax=axes[i\/\/6, i%6], color=next(palette))\n# axes.set_title('Group 1 Distribution', size=12)","15f073eb":"# Check the distribution of data df group 2\n\nfig, axes = plt.subplots(nrows=4, ncols=8, figsize=(25, 20))\npalette = itertools.cycle(sns.color_palette())\n\nfor i, column in enumerate(train_w_missing_g2.columns):\n    sns.distplot(train_w_missing_g2[column], ax=axes[i\/\/8, i%8], color=next(palette))","ccb23174":"# Check the distribution of data df group 3\n\nfig, axes = plt.subplots(nrows=5, ncols=5, figsize=(20, 20))\npalette = itertools.cycle(sns.color_palette())\n\nfor i, column in enumerate(train_w_missing_g3.columns):\n    sns.distplot(train_w_missing_g3[column], ax=axes[i\/\/5, i%5], color=next(palette))","7562c859":"# based on the plot above, a fluffy to deal with missing value is to fill with median of each feature\n\nMEDIAN = train.median()  # group var\nx_train = train.fillna(MEDIAN)","e274e08a":"# Generating 0 or 1 values on the basis of resp features and storing it to 'action' column\n# It will serve as our test data \ntrain['action'] = (train['resp'] > 0 ).astype('int')\ntrain.head()","688522a5":"X = train.loc[:, train.columns.str.contains('feature')]\ny = train.loc[:, 'action']\n\ndel train  # free some space \n\n# Splitting X,y into train and validation data \nx_train, x_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)","17de7871":"# Created the Xgboost specific DMatrix data format from the numpy array to optimise memory consumption\ndtrain = xgb.DMatrix(x_train, label=y_train)\ndvalid = xgb.DMatrix(x_valid, label=y_valid)","7fb59d8e":"# check imbalance of responses\n\nsns.set_palette(\"hls\")\nax = sns.barplot(y_train.value_counts().index, y_train.value_counts()\/len(y_train))\nax.set_title(\"Proportion of trades with action=0 and action=1\")\nax.set_ylabel(\"Percentage\")\nax.set_xlabel(\"Action\")\nsns.despine();","2b57beb7":"def objective(trial):\n    \n# params specifies the XGBoost hyperparameters to be tuned\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 400, 600),\n        'max_depth': trial.suggest_int('max_depth', 10, 20),\n        'learning_rate': trial.suggest_uniform('learning_rate', 0.01, .1),\n        'subsample': trial.suggest_uniform('subsample', 0.50, 1),\n        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.50, 1),\n        'gamma': trial.suggest_int('gamma', 0, 10),\n        'tree_method': 'gpu_hist',  \n        'objective': 'binary:logistic'\n    }\n    \n    bst = xgb.train(params, dtrain)\n    preds = bst.predict(dvalid)\n    pred_labels = np.rint(preds)\n    \n# trials will be evaluated based on their accuracy on the test set\n    accuracy = sklearn.metrics.accuracy_score(y_valid, pred_labels)\n    return accuracy","54df9f2f":"study = optuna.create_study()\nstudy.optimize(objective,n_trials=5) \n\nprint('Best trial: score {}, params {}'.format(study.best_trial.value, study.best_trial.params))","78a6940e":"best_params = study.best_trial.params\nbest_params['tree_method'] = 'gpu_hist'      #gpu_hist is really fast\nbest_params['objective'] = 'binary:logistic'\n\ndel x_train, x_valid, y_train, y_valid, dtrain, dvalid  #free some space","4027c761":"# Fit the XGBoost classifier with optimal hyperparameters\nclf = xgb.XGBClassifier(**best_params)","8391e56e":"%time clf.fit(X, y)  #Used the whole training data","0bf1e7c7":"from tqdm import tqdm\nimport janestreet\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set","fd78d443":"for (test_df, pred_df) in tqdm(iter_test):\n    if test_df['weight'].item() > 0:\n        X_test = test_df.loc[:, test_df.columns.str.contains('feature')]\n        y_preds = clf.predict(X_test)\n        pred_df.action = y_preds\n    else:\n        pred_df.action = 0\n    env.predict(pred_df)","901e4ce6":"### fill N\/A per column","57a7cebc":"### Fitting classifier on test data\u00b6\n","1024510e":"- label and simulate the outliers ","fe870075":"- The criteria of a successful trasaction is determined if the resp is greater than 0. This criteria should be justified.","24c18890":"## First Edition: XGBoost for Jane Street data","0a6a060c":"This way of filling missing value is a possible way but may have better way. Thinking about the distribution or maybe correlations of feature data. EX. For those that are skewed, regression can be used? If perfectly Gaussian, avg looks good. Need discussion.","206755f8":"- QQ Plot","dd47c104":"### Data Cleaning\n\n1. Ignore all the rows with weight = 0 since trades with weight = 0 were intentionally included in the dataset for completeness, although such trades will not contribute towards the scoring evaluation.\n2. Sort the data based on date and ts_id order (time series property)\n3. Check the distribution of missing value per feature 0 - 129, insight about how to fill missing value\n4. Correlation analysis on resp features? (potential to drop some features?)\n5. Check imbalance data (for binary classification problem)\n6. Generating 0 or 1 values on the basis of resp features and storing it to 'action' column\n7. date > 80","d7694111":"### Data description\n\n- This dataset contains an anonymized set of features, feature_{0...129}, representing real stock market data. Each row in the dataset represents a trading opportunity, for which you will be predicting an action value: 1 to make the trade and 0 to pass on it. \n- Each trade has an associated weight and resp, which together represents a return on the trade. \n- The date column is an integer which represents the day of the trade, while ts_id represents a time ordering. \n- In addition to anonymized feature values, you are provided with metadata about the features in features.csv.\n- In the training set, train.csv, you are provided a resp value, as well as several other resp_{1,2,3,4} values that represent returns over different time horizons. These variables are not included in the test set. Trades with weight = 0 were intentionally included in the dataset for completeness, although such trades will not contribute towards the scoring evaluation.","273ff63e":"### Creating Train and Test DataFrame","be614412":"### XGBoost "}}