{"cell_type":{"6fc828ce":"code","33c102f2":"code","8d65ba8a":"code","f27daa2f":"code","76797751":"code","349cccaf":"code","d22e4902":"code","b237ea81":"code","392e863a":"code","2df66282":"code","46764be1":"code","0617a5e3":"code","bbf731af":"code","d4f5e870":"code","bbea75cd":"code","6dd1046f":"code","d9c564e7":"code","4e228f7a":"markdown","c6afea1f":"markdown","6a7297f5":"markdown","d0040d37":"markdown","c4992fc2":"markdown","2b9ce5ac":"markdown"},"source":{"6fc828ce":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","33c102f2":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict","8d65ba8a":"hit = pd.read_csv(\"\/kaggle\/input\/hitters\/Hitters.csv\")\ndf = hit.copy()\ndf = df.dropna()\ndms = pd.get_dummies(df[['League', 'Division', 'NewLeague']])\ny = df[\"Salary\"]\nX_ = df.drop(['Salary', 'League', 'Division', 'NewLeague'], axis=1).astype('float64')\nX = pd.concat([X_, dms[['League_N', 'Division_W', 'NewLeague_N']]], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","f27daa2f":"from sklearn.cross_decomposition import PLSRegression, PLSSVD","76797751":"pls_model = PLSRegression().fit(X_train, y_train)","349cccaf":"pls_model.coef_","d22e4902":"X_train.head()","b237ea81":"pls_model.predict(X_train)[0:10]","392e863a":"y_pred = pls_model.predict(X_train)","2df66282":"np.sqrt(mean_squared_error(y_train, y_pred))","46764be1":"r2_score(y_train, y_pred)","0617a5e3":"y_pred = pls_model.predict(X_test)","bbf731af":"np.sqrt(mean_squared_error(y_test, y_pred))","d4f5e870":"#CV\ncv_10 = model_selection.KFold(n_splits=10, shuffle=True, random_state=1)\n\n\n#loop to calculate error\nRMSE = []\n\nfor i in np.arange(1, X_train.shape[1] + 1):\n    pls = PLSRegression(n_components=i)\n    score = np.sqrt(-1*cross_val_score(pls, X_train, y_train, cv=cv_10, scoring='neg_mean_squared_error').mean())\n    RMSE.append(score)\n\n#Visualization of results\nplt.plot(np.arange(1, X_train.shape[1] + 1), np.array(RMSE), '-v', c = \"r\")\nplt.xlabel('Components')\nplt.ylabel('RMSE')\nplt.title('Salary');","bbea75cd":"pls_model = PLSRegression(n_components = 2).fit(X_train, y_train)","6dd1046f":"y_pred = pls_model.predict(X_test)","d9c564e7":"np.sqrt(mean_squared_error(y_test, y_pred))","4e228f7a":"# Partial Least Squares Regression (PLS)\n","c6afea1f":"If the graph is examined, RMSE values for the salary variable and the number of components are observed. We can say that if the number of components is a value like 2, it gives a lower RMSE value compared to other cases.","6a7297f5":"## Model","d0040d37":"The regression model is constructed by reducing the variables to fewer components and without multiple linear connection problems. Components are created in a way that summarizes the covariance with the dependent variable in the highest way.\nWe should use Cross-Validation (CV) to determine the optimum number of components.","c4992fc2":"## Prediction","2b9ce5ac":"## Model Tuning"}}