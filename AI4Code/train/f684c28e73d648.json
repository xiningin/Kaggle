{"cell_type":{"92ad178d":"code","29abb617":"code","48902b9f":"code","259b3c86":"code","f7e5f42a":"code","a6021d88":"code","fed58938":"code","2b580459":"code","60a93320":"code","9d16e9e7":"code","cf14f9c7":"code","95ca0411":"code","007b822e":"code","f01618d3":"code","5f6170ca":"code","795fcd6c":"code","e148090a":"code","d7d3e069":"code","f15530cb":"code","5928046a":"code","c8a3c4ea":"code","d655dac7":"code","064c1c94":"code","91160e6c":"code","019b1f90":"code","e474c232":"code","30a48075":"code","b37c15c6":"code","eef89133":"code","78eeb686":"code","4f53fca8":"code","ff092938":"code","dd068573":"markdown","6c5c7e6e":"markdown","9afd2fa1":"markdown","257e9e7a":"markdown","a4429c14":"markdown","7fa4af94":"markdown","c571be03":"markdown","2e60be73":"markdown","632af116":"markdown","7f3c122a":"markdown","022e9324":"markdown","ece29c60":"markdown","89dd8692":"markdown","f4ca0dd2":"markdown","f95fdd52":"markdown","271c3721":"markdown","7d01dabc":"markdown","49828a0a":"markdown","7c743e11":"markdown","10012339":"markdown","0c630b4f":"markdown","36d3b37c":"markdown","4dbe3997":"markdown","1de2882c":"markdown","8a9580c7":"markdown","9f798850":"markdown","a3b9dcb9":"markdown"},"source":{"92ad178d":"import numpy as np\nimport pandas as pd\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","29abb617":"!nvidia-smi","48902b9f":"import matplotlib.pyplot as plt\n%matplotlib inline","259b3c86":"import sklearn\nimport sklearn.model_selection\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision","f7e5f42a":"torch.cuda.is_available()","a6021d88":"train_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')","fed58938":"train_df","2b580459":"X_train_all = train_df.loc[:, train_df.columns != \"label\"].values.reshape((-1, 28, 28)) \/ 255\ny_train_all = train_df['label'].values\nX_test = test_df.loc[:, test_df.columns != \"label\"].values.reshape((-1, 28, 28)) \/ 255\n\nprint('X_train_all.shape: {}'.format(X_train_all.shape))\nprint('X_train_all.__class__: {}'.format(X_train_all.__class__))\nprint('y_train_all.shape: {}'.format(y_train_all.shape))\nprint('y_train_all.__class__: {}'.format(y_train_all.__class__))\nprint('X_test.shape: {}'.format(X_test.shape))\nprint('X_test.__class__: {}'.format(X_test.__class__))","60a93320":"X_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(\n    X_train_all,\n    y_train_all,\n    test_size=0.1,\n    shuffle=True,\n    stratify=y_train_all,\n    random_state=79508,\n)\n\nprint('X_train.shape: {}'.format(X_train.shape))\nprint('X_val.shape: {}'.format(X_val.shape))\nprint('y_train.shape: {}'.format(y_train.shape))\nprint('y_val.shape: {}'.format(y_val.shape))","9d16e9e7":"X_train[0]","cf14f9c7":"for i in range(10):\n    plt.imshow(X_train[i], cmap='gray')\n    plt.title('label = {}'.format(y_train[i]))\n    plt.show()","95ca0411":"y_unq = np.unique(y_train_all)\ny_unq","007b822e":"bin_edges = np.concatenate([y_unq, y_unq[[-1]] + 1])\nplt.hist(y_train, bins=bin_edges)\nplt.xlabel('Class label')\nplt.ylabel('Count')\nplt.title('Training label distribution')\nplt.show()\nplt.hist(y_val, bins=bin_edges)\nplt.xlabel('Class label')\nplt.ylabel('Count')\nplt.title('Validation set label distribution')\nplt.show()","f01618d3":"import torch.optim as optim\nfrom torchvision import datasets, transforms","5f6170ca":"from collections import defaultdict\nimport time","795fcd6c":"class Net(nn.Module):\n    def __init__(self, activation_fn=None, n_targets=10):\n        super(Net, self).__init__()\n        if activation_fn is None:\n            self.activation_fn = F.relu\n        else:\n            self.activation_fn = activation_fn\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, n_targets)\n\n    def forward(self, x):\n        x = self.activation_fn(F.max_pool2d(self.conv1(x), 2))\n        x = self.activation_fn(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = self.activation_fn(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        #return F.log_softmax(x, dim=1)\n        return x","e148090a":"def train(model, device, train_loader, optimizer, epoch, log_interval):\n    model.train()\n    train_log = defaultdict(list)\n    t_log = time.time()\n    n_samples = 0\n    for batch_idx, (data, target) in enumerate(train_loader):\n        t0 = time.time()\n        data, target = data.to(device), target.to(device).long()\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.cross_entropy(output, target)\n        t1 = time.time()\n        loss.backward()\n        t2 = time.time()\n        optimizer.step()\n        t3 = time.time()\n        n_samples += data.shape[0]\n        if batch_idx % log_interval == 0:\n            pred = output.max(1, keepdim=True)[1]\n            correct = pred.eq(target.view_as(pred)).sum().item()\n\n            train_log['n_iter'].append(epoch * len(train_loader) + batch_idx + 1)\n            train_log['n_samples'].append(n_samples + (epoch - 1) * len(train_loader.dataset))\n            train_log['loss'].append(loss.detach())\n            train_log['accuracy'].append(100. * correct \/ data.shape[0])\n            train_log['time_batch'].append(t3 - t0)\n            train_log['time_batch_forward'].append(t1 - t0)\n            train_log['time_batch_backward'].append(t2 - t1)\n            train_log['time_batch_update'].append(t3 - t2)\n            t4 = time.time()\n            train_log['time_batch_avg'].append((t4 - t_log) \/ log_interval)\n            print(\n                'Train Epoch: {} [{:5d}\/{:5d} ({:3.0f}%)]'\n                '\\tLoss: {:.6f}'\n                '\\tTime: {:.4f}ms\/batch'.format(\n                    epoch, n_samples, len(train_loader.dataset),\n                    100. * (batch_idx + 1) \/ len(train_loader), loss.item(),\n                    1000 * (t4 - t_log) \/ log_interval,\n                )\n            )\n            t_log = time.time()\n    return train_log\n\ndef test(model, device, test_loader, log_interval):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    preds = []\n    targets = []\n    num_batches = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            num_batches += 1\n            data, target = data.to(device), target.to(device).long()\n            output = model(data)\n            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n            preds.append(pred.cpu().numpy())\n            targets.append(target.cpu().numpy())\n    \n    # Remove list nesting\n    preds = np.concatenate(preds).squeeze()\n    targets = np.concatenate(targets).squeeze()\n\n    test_loss \/= len(test_loader.dataset)\n    accuracy = 100. * correct \/ len(test_loader.dataset)\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}\/{} ({:.3f}%)\\n'\n        ''.format(\n            test_loss,\n            correct, len(test_loader.dataset), accuracy,\n            )\n        )\n    return test_loss, accuracy\n\ndef main(\n        batch_size=64,\n        test_batch_size=None,\n        n_epoch=10,\n        lr=0.01,\n        momentum=0.5,\n        no_cuda=False,\n        seed=1,\n        net=Net,\n        activation_fn=F.relu,\n        n_targets=10,\n        log_interval=10,\n        ):\n\n    if test_batch_size is None:\n        test_batch_size = batch_size\n\n    use_cuda = not no_cuda and torch.cuda.is_available()\n\n    # For reproducible models\n    torch.manual_seed(seed)\n\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    # Simple data loader, without augmentations\n    # Note that we have a call to expand_dims.\n    # This is to insert the channel dimension. Conv2D uses NCHW ordering.\n    train_loader = torch.utils.data.DataLoader(\n        torch.utils.data.TensorDataset(\n            torch.Tensor(np.expand_dims(X_train, axis=1)).float(),\n            torch.Tensor(y_train).long(),\n        ),\n        batch_size=batch_size,\n        shuffle=True,\n    )\n    val_loader = torch.utils.data.DataLoader(\n        torch.utils.data.TensorDataset(\n            torch.Tensor(np.expand_dims(X_val, axis=1)).float(),\n            torch.Tensor(y_val).long(),\n        ),\n        batch_size=batch_size,\n        shuffle=True,\n    )\n\n    model = net(activation_fn=activation_fn, n_targets=n_targets).to(device)\n    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n\n    train_log = defaultdict(list)\n    val_log = defaultdict(list)\n\n    for epoch in range(1, n_epoch + 1):\n        epoch_log = train(model, device, train_loader, optimizer, epoch, log_interval)\n        for key, value in epoch_log.items():\n            train_log[key] += value\n        \n        val_loss, val_accuracy = test(model, device, val_loader, log_interval)\n        val_log['loss'].append(val_loss)\n        val_log['accuracy'].append(val_accuracy)\n    \n    return model, train_log, val_log","d7d3e069":"def collapse_nested_list(l):\n    return [a for b in l for a in b]\n\ndef plot_training_log(train_log, val_log):\n    #xx_train = np.arange(0, len(train_log['loss']), 1 \/ len(train_log['loss'][0]))\n    xx_train = np.array(train_log['n_samples']) \/ 1000\n    n_epoch = len(val_log['loss'])\n    xx_val = np.arange(1, n_epoch + 1) * xx_train[-1] \/ n_epoch\n    \n    plt.plot(xx_train, train_log['loss'], 'b')\n    # We prepend the first train loss score so there are enough datapoints to plot\n    # a line, even with a single epoch.\n    plt.plot(np.concatenate([[0], xx_val]), [train_log['loss'][0]] + val_log['loss'], 'ro-')\n    plt.title('Loss (lower is better)')\n    plt.xlabel('Number of samples presented (1000s)')\n    plt.ylabel('Cross-Entropy Loss')\n    plt.show()\n    \n    plt.plot(xx_train, train_log['accuracy'], 'b')\n    plt.plot(np.concatenate([[0], xx_val]), [train_log['accuracy'][0]] + val_log['accuracy'], 'ro-')\n    plt.title('Accuracy (higher is better)')\n    plt.xlabel('Number of samples presented (1000s)')\n    plt.ylabel('Accuracy (%)')\n    plt.show()\n    \n    if n_epoch > 1:\n        plt.plot(xx_train, train_log['accuracy'], 'b')\n        plt.plot(np.concatenate([[0], xx_val]), [10] + val_log['accuracy'], 'ro-')\n        plt.ylim([90, 100])\n        plt.title('Accuracy, zoomed in')\n        plt.xlabel('Number of samples presented (1000s)')\n        plt.ylabel('Accuracy (%)')\n        plt.show()\n    \n    plt.plot(xx_train, 1000 * np.array(train_log['time_batch_avg']), 'b')\n    plt.title('Training speed (lower is better)')\n    plt.xlabel('Number of samples presented (1000s)')\n    plt.ylabel('Duration per batch (ms)')\n    plt.show()","f15530cb":"# Check we can train a single epoch on GPU\nt0 = time.time()\nmodel, train_log, val_log = main(no_cuda=False, n_epoch=1)\nt1 = time.time()\nprint('Training completed in {} seconds.'.format(t1 - t0))\nplot_training_log(train_log, val_log)","5928046a":"# Try on CPU to compare compute time\nt0 = time.time()\nmodel, train_log, val_log = main(no_cuda=True, n_epoch=1)\nt1 = time.time()\nprint('Training completed in {} seconds.'.format(t1 - t0))\nplot_training_log(train_log, val_log)","c8a3c4ea":"# Train model for 10 epochs\nt0 = time.time()\nmodel, train_log, val_log = main(n_epoch=10)\nt1 = time.time()\nprint('Training completed in {} seconds.'.format(t1 - t0))\nplot_training_log(train_log, val_log)","d655dac7":"print(model)","064c1c94":"try:\n    import torchsummary\n    torchsummary.summary(model)\nexcept ImportError:\n    print('The torchsummary package is not available.')","91160e6c":"def count_parameters(model, only_trainable=True):\n    if only_trainable:\n        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n    else:\n        return sum(p.numel() for p in model.parameters())","019b1f90":"print('Total params    : {}'.format(count_parameters(model, False)))\nprint('Trainable params: {}'.format(count_parameters(model, True)))","e474c232":"test_loader = torch.utils.data.DataLoader(\n    torch.utils.data.TensorDataset(\n        torch.Tensor(np.expand_dims(X_test, axis=1)).float(),\n    ),\n    batch_size=64,\n    shuffle=False,\n)","30a48075":"def predict(model, test_loader, device=None):\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")\n    elif isinstance(device, str):\n        device = torch.device(device)\n    model.eval()\n    preds = []\n    with torch.no_grad():\n        for data, in test_loader:\n            data = data.to(device)\n            output = model(data)\n            pred = output.max(1, keepdim=True)[1]\n            preds.append(pred.cpu().numpy())\n    # Remove list nesting\n    preds = np.concatenate(preds).squeeze()\n    return preds","b37c15c6":"preds = predict(model, test_loader)","eef89133":"preds.shape","78eeb686":"out_df = pd.DataFrame(\n    {'ImageId': np.arange(1, preds.shape[0] + 1),\n     'Label': preds}\n)","4f53fca8":"out_df.to_csv('submission.csv', index=False)","ff092938":"!head submission.csv","dd068573":"We now have a 90\/10 stratified split for train and validation.","6c5c7e6e":"Next, let's make a validation set we can use for local evaluation.","9afd2fa1":"## Next steps\nThis notebook will get you set up with building MNIST models using Pytorch on the GPU provided by Kaggle. We trained a small model for only 10 epochs (with the actual model training itself taking under 20 seconds). The result isn't a particularly good model! It is very easy to build on this model and improve it, but this notebook should give you a good starting point to do that. Feel free to fork it and adapt it, and try out new things.","257e9e7a":"## Inspecting the data\nCheck our images look right.","a4429c14":"Write output to CSV file.","7fa4af94":"We've got 0s at the extremities, and can see values approaching but not exceeding 1.0. Looks good to me!\n\nNow lets check the images look sensible and are paired up with the right labels.","c571be03":"We can see that the GPU is, overall, maybe about 4-5x faster than CPU (2.2-4.5 seconds vs 12 seconds). This is less than we would expect from prior knowledge of GPU speed-ups and our training time per batch plots (3ms\/batch vs 19ms\/batch). Presumably part of the reason it is slower is because there an overhead in setting up the model on the GPU.\n\nAlso, it should be noted that Kaggle gives you less CPU power when you are working on a GPU-enabled notebook. If you were working on a CPU-only notebook, it would be around 9 seconds to run a single epoch (only 75% of the CPU time we observed here).","2e60be73":"Yes, we have a Tesla K80. Great!\n\nWhen you run this command, it should tell you the stats for the GPU available, the GPU's temperature, available memory, and which processes are running on the GPU. Hopefully that will be no processes, since we haven't started running any yet.\n\nIf you are on a server without GPU compute installed, the nvidia-smi command will not be defined and you will receive the following error:\n```\n\/bin\/sh: 1: nvidia-smi: not found\n```\nTo resolve this, change the Settings for your Kaggle kernel so it is \"GPU on\" instead of \"GPU off\".","632af116":"Let's check we do have access to a GPU.","7f3c122a":"We need to convert our data from the dataframe into a matrix of inputs (X) and target outputs (y).\nSince MNIST is a small dataset, we can load all the data into memory at once to do this in one step. For bigger image datasets, this would not be possible.","022e9324":"This should be `True`. If you see `False` here, something is wrong and torch isn't going to be able to run on the GPU.\n\nPossible causes to seeing `False` at this point:\n- You don't have a GPU (see above with regards to the `nvidia-smi` check)\n- You have a CPU-only version of pytorch installed, in which case you need to install the gpu enabled version\n- You don't have the CUDA library\n- You don't have the CUDA library path included on either the `PATH` or `LD_LIBRARY_PATH` variables\n- You have the wrong version of CUDA for the version of pytorch\n- Probably other possiblities\n\nYou might encounter some of those problems if you download and run this notebook locally. However, if you're running this on Kaggle's server with the GPU option turned on, everything should run smoothly. When you run it on Kaggle, everything should already be installed correctly.","ece29c60":"To run 10 epochs took us around 19 seconds. For comparison, running this same code block on the CPU (not shown here) would have taken 118 seconds (6x speed-up). If we were on a CPU-only Kaggle kernel server (also not shown), it would have taken 79 seconds (4x speed-up).","89dd8692":"## Make predictions","f4ca0dd2":"Check file looks okay - columns are correct way around, etc.","f95fdd52":"Note that in the function `predict`, we are inferring the device that the model is on by assuming `no_cuda` was set to `False`. If you forced the model to run on the CPU earlier, you'll get an error at this step. You can fix that by `preds = predict(model, test_loader, 'cpu')`.","271c3721":"## Main\nBased on Pytorch example found here https:\/\/github.com\/pytorch\/examples\/blob\/master\/mnist\/main.py","7d01dabc":"## Loading data\nPytorch comes with a built-in MNIST data loader. However, we can't use that because we need to use the train\/test split offered by Kaggle to correctly work with the framework of this competition. Instead, we will load up the data in the train and test csv files shown above.","49828a0a":"That was a surprisingly large amount of '2's. That's a reminder that we should check our label balance!","7c743e11":"Check how many parameters our model has.","10012339":"If the model is on the GPU, we have to transfer the data to the GPU to run it through the model. We aren't in `main()` any more, so we try to work out where the model is. Unfortunately there isn't a function we can call to check for sure.\n\nYou could alternatively move the model to the CPU with `model.cpu()` and then do the prediction step on the CPU, then we'd definitely know where it is.","0c630b4f":"Inspect the structure of our model","36d3b37c":"Here, I dump out the contents of the array encoding the first image in the training set. We want to inspect the raw values to make sure the normalisation was done correctly (we are in the [0, 1] range and not [0, 255] or [0, 1\/255]). We also want to make sure we have 0 as the background (black) and 1 as the foreground (white).\n\nThese bugs can be missed if you just make a plot of the image, because the plotting function will automatically rescale the image and if you're using a pseudo colormap, you may not be sure which way around the colourmap is.","4dbe3997":"Let's also check to make sure torch thinks we can use the GPU.","1de2882c":"# Testing GPU-enabled Notebooks for MNIST with Pytorch\nIn this notebook, we build a simple CNN model for Kaggle (the Pytorch example CNN, in fact) and compare how long it takes to train on GPU compared with CPU.\n\nFirst of all, when running this notebook on Kaggle you need to go to the Settings pane on the right of the notebook and turn on the GPU option. When you do that, the notebook will restart, moving from a CPU-only to a GPU enabled server.","8a9580c7":"Our figures look correct, and the labels match up with the actual digits.\nAlso, we can see our stratified split has preserved the data distribution.\n\nThe surprisingly large number of \"2\" digits in our ten sample images (5\/10) was just random happenstance.","9f798850":"## Importing modules\nImport matplotlib, sklearn and pytorch dependencies.","a3b9dcb9":"We expect to get a performance similar to the validation score of 96.9%, and we actually get 96.2%."}}