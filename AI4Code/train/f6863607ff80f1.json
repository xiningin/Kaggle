{"cell_type":{"f7cbb428":"code","8f09b570":"code","86289358":"code","38fe26a9":"code","0117f1ac":"code","faa3ccc4":"code","63f864cb":"code","69cdf451":"code","ba524e9d":"code","fee5c831":"code","8743e5dd":"code","8cc025fc":"code","b028f0d9":"code","5d63767e":"code","f7e2bf68":"code","c78fa378":"code","db25223d":"code","41b9f2aa":"code","98c4bd81":"code","f9b8bf8e":"code","78042f20":"code","4ac7171e":"code","7df77a1f":"code","824ad8d0":"code","75e094ba":"code","92c9b6d7":"code","0ff05c11":"code","cb99fcad":"code","5513ef0f":"code","e66f3ae0":"code","20417e82":"code","67cc0a71":"code","55df102b":"code","3c4712c1":"code","b54a5407":"code","fe4d25b2":"code","3ee52e8b":"code","2727f8da":"code","9e5cfef5":"code","531a273f":"code","6313ffef":"code","29b8cd50":"code","e246d894":"code","3c9845a3":"code","c24d56b1":"code","529fd56f":"code","d31a8926":"code","dfc013b7":"code","659ccb16":"code","ad7ed001":"code","59a0ced7":"code","3995b545":"code","5fe97d52":"code","d41bec8d":"code","98bdfb6c":"code","335a4878":"code","abaf75b0":"code","8bf6f95c":"code","54849df7":"code","7f0970b9":"code","cbfe055b":"code","c56bb456":"code","b876f62b":"code","1aa35f9d":"code","e5527fd5":"code","8e3839be":"code","077b325d":"code","c162a752":"code","2b1ddde9":"code","d21e1251":"code","2d0b04da":"code","34384749":"code","8919c61d":"code","535f170d":"code","32d33949":"code","018b7066":"code","0564d17f":"code","6d52e10f":"code","530f17fa":"code","a0a290fb":"code","a2626c1e":"code","14e3869a":"code","6400cb5d":"code","068d2f6e":"code","c558ef50":"code","99743396":"code","fef6447b":"code","d0f8b49a":"code","7f6ee98f":"code","46cf7db4":"code","b3656247":"markdown","a08747db":"markdown","208b450c":"markdown","5a191112":"markdown","b48c52d7":"markdown","2f999493":"markdown","f6d07ffc":"markdown","410bbd17":"markdown","cdbf41df":"markdown","bea44aeb":"markdown","f19ea171":"markdown","ff353908":"markdown","e3e9bbaf":"markdown","a040aece":"markdown","2848b2bf":"markdown","71dcd3f1":"markdown","c96e6f1b":"markdown","0883c097":"markdown","6d494ce6":"markdown"},"source":{"f7cbb428":"import sys\nsys.path.append('..\/input\/iterativestratification')\n\n\nimport numpy as np\nimport random\nimport pandas as pd\nimport os\nimport copy\nimport gc\n\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA , TruncatedSVD\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.feature_selection import VarianceThreshold, SelectKBest\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport scipy.stats as stats\nfrom scipy.stats import kurtosis\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.nn.modules.loss import _WeightedLoss\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nos.listdir('..\/input\/lish-moa')\n\npd.set_option('max_columns', 2000)","8f09b570":"\nn_comp_GENES = 450 \nn_comp_CELLS = 2\nVarianceThreshold_for_FS = 0.9\nDropout_Model = 0.25\nQT_n_quantile_min=50, \nQT_n_quantile_max=1000,\nprint('n_comp_GENES', n_comp_GENES, 'n_comp_CELLS', n_comp_CELLS, 'total', n_comp_GENES + n_comp_CELLS)","86289358":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\ntrain_drugs = pd.read_csv('..\/input\/lish-moa\/train_drug.csv')\n\n\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\nsample_submission = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')","38fe26a9":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","0117f1ac":"# Search for minimum and maximum values\n# df_kurt = pd.DataFrame(columns=['col','train', 'test'])\n# i = 0\n# for col in (GENES + CELLS):\n#     df_kurt.loc[i, 'col'] = col\n#     df_kurt.loc[i, 'train'] = kurtosis(train_features[col])\n#     df_kurt.loc[i, 'test'] = kurtosis(test_features[col])\n#     i += 1\n# print(df_kurt.min())\n# print(df_kurt.max())","faa3ccc4":"def calc_QT_par_kurt(QT_n_quantile_min=10, QT_n_quantile_max=200):\n    # Calculation parameters of function: n_quantile(kurtosis) = k1*kurtosis + k0\n    # For Train & Test datasets (GENES + CELLS features): minimum kurtosis = 1.53655, maximum kurtosis = 30.4929\n    \n    a = np.array([[1.53655,1], [30.4929,1]])\n    b = np.array([QT_n_quantile_min, QT_n_quantile_max])\n    \n    return np.linalg.solve(a, b)","63f864cb":"def n_quantile_for_kurt(kurt, calc_QT_par_kurt_transform):\n    # Calculation parameters of function: n_quantile(kurtosis) = calc_QT_par_kurt_transform[0]*kurtosis + calc_QT_par_kurt_transform[1]\n    return int(calc_QT_par_kurt_transform[0]*kurt + calc_QT_par_kurt_transform[1])","69cdf451":"# RankGauss - transform to Gauss\n\nfor col in (GENES + CELLS):\n\n    #kurt = max(kurtosis(train_features[col]), kurtosis(test_features[col]))\n    #QuantileTransformer_n_quantiles = n_quantile_for_kurt(kurt, calc_QT_par_kurt(QT_n_quantile_min, QT_n_quantile_max))\n    #transformer = QuantileTransformer(n_quantiles=QuantileTransformer_n_quantiles,random_state=0, output_distribution=\"normal\")\n    \n    transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")   # from optimal commit 9\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    raw_vec = train_features[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]","ba524e9d":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","fee5c831":"len(GENES)","8743e5dd":"# GENES\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\ndata2 = (TruncatedSVD(n_components=n_comp_GENES, random_state=42).fit_transform(data[GENES]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp_GENES)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp_GENES)])\n\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","8cc025fc":"len(CELLS)","b028f0d9":"# CELLS\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\ndata2 = (TruncatedSVD(n_components=n_comp_CELLS, random_state=42).fit_transform(data[CELLS]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp_CELLS)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp_CELLS)])\n\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","5d63767e":"train_features.shape","f7e2bf68":"train_features.head(5)","c78fa378":"train_features = train_features.drop(CELLS+GENES , axis=1)","db25223d":"test_features  = test_features.drop(CELLS+GENES , axis=1)","41b9f2aa":"data = train_features.append(test_features)\ndata","98c4bd81":"var_thresh = VarianceThreshold(VarianceThreshold_for_FS)\ndata = train_features.append(test_features)\ndata_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n\ntrain_features_transformed = data_transformed[ : train_features.shape[0]]\ntest_features_transformed = data_transformed[-test_features.shape[0] : ]\n\n\ntrain_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntrain_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\n\ntest_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntest_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n\ntrain_features.shape","f9b8bf8e":"train_features.head(5)","78042f20":"train = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\ntarget = train[train_targets_scored.columns]","4ac7171e":"train = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)","7df77a1f":"train.head(5)","824ad8d0":"target_cols = target.drop('sig_id', axis=1).columns.values.tolist()","75e094ba":"target_non_scored_cols = train_targets_nonscored.drop('sig_id', axis=1).columns.values.tolist()","92c9b6d7":"train_non_scored =  train.drop(target_cols , axis = 1).copy()","0ff05c11":"train_non_scored = train_non_scored.merge(train_targets_nonscored , how = 'inner')","cb99fcad":"target_non_scored = train_non_scored[['sig_id']+target_non_scored_cols]","5513ef0f":"train_non_scored","e66f3ae0":"train","20417e82":"folds_non_scored = train_non_scored.copy()\n\nmskf = MultilabelStratifiedKFold(n_splits=7)\n\nfor f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n    folds_non_scored.loc[v_idx, 'kfold'] = int(f)\n\nfolds_non_scored['kfold'] = folds_non_scored['kfold'].astype(int)\nfolds_non_scored","67cc0a71":"folds = train.copy()\n\nmskf = MultilabelStratifiedKFold(n_splits=7)\n\nfor f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n    folds.loc[v_idx, 'kfold'] = int(f)\n\nfolds['kfold'] = folds['kfold'].astype(int)\nfolds","55df102b":"print(train.shape)\nprint(folds.shape)\nprint(test.shape)\nprint(target.shape)\nprint(sample_submission.shape)","3c4712c1":"folds","b54a5407":"train_drugs.head()","fe4d25b2":"# max(train_drugs['sig_id'].value_counts().values)  \n# only one sig_id hence no multilabel buisness here ie no sig_id has two drugs_id \n# this is the base of turning the problem in multiclass for sampling purposes","3ee52e8b":"# # original train data set\n# target_f =  train[target_cols]\n# top_targets = pd.Series(target_f.sum()).sort_values(ascending=False)[:5]\n# bottom_targets = pd.Series(target_f.sum()).sort_values()[:5]\n# fig, axs = plt.subplots(figsize=(9,9) , nrows=2)\n# sns.barplot(top_targets.values , top_targets.index , ax = axs[0] ).set(title = \"Top five targets\")\n# sns.barplot(bottom_targets.values , bottom_targets.index, ax = axs[1] ).set(title = \"bottom five targets\")\n# plt.show()","2727f8da":"# tail_enders = pd.Series(target_f.sum()).sort_values()[:70]","9e5cfef5":"# tail_enders = list(tail_enders.index)","531a273f":"# tail_enders","6313ffef":"# merged_drug = folds.merge(train_drugs)\n# df1.merge(df2, left_on='lkey', right_on='rkey')","29b8cd50":"# merged_drug['drug_id'].value_counts()","e246d894":"# lets get the drug_ids of interest tail_enders\n# merge_grouped = merged_drug.groupby(by = 'drug_id')[target_cols].sum()\n\n\n# targets with less representation\n\n\n","3c9845a3":"# merge_grouped['sum_targets'] = merge_grouped[target_cols].sum(axis=1) \n# merge_grouped['sum_tail_enders'] = merge_grouped[tail_enders].sum(axis=1) ","c24d56b1":"# drug_ids = list(merge_grouped[merge_grouped['sum_tail_enders'] >0].index)\n","529fd56f":"\n# x = merged_drug['drug_id'].value_counts()\n","d31a8926":"# max(x[drug_ids]) # totally expandable not the best way to do things but this idea was insane from start.","dfc013b7":"# all_drugs =  merged_drug['drug_id'].value_counts()","659ccb16":"# samp_strat = {}","ad7ed001":"# # get the sampling strat lets produce 150 of each will later come up with a better logic\n# samp_strat = {}\n# for i in list(all_drugs.index):\n#     if i in drug_ids:\n#         samp_strat.update({i:150})\n#     else:\n#         if((all_drugs[i]+1) < 7 ):\n#             samp_strat.update({i:7})\n#         else:\n#             samp_strat.update({i:all_drugs[i]+1})\n        \n    \n        \n        \n    \n\n\n\n","59a0ced7":"# samp_strat # at this point its getting out of hand but ok its just for fun","3995b545":"# drug_smote = pd.get_dummies(merged_drug, columns=['cp_time','cp_dose'])","5fe97d52":"# y_smote = merged_drug['drug_id']\n\n# x_smote = drug_smote.drop(target_cols+['sig_id' , 'kfold', 'drug_id'] , axis =1)","d41bec8d":"# from imblearn.over_sampling import RandomOverSampler\n# smote = RandomOverSampler(sampling_strategy= samp_strat ,  random_state=42)\n\n","98bdfb6c":"# X_res, y_res = smote.fit_resample(x_smote, y_smote) # i realized i over achieved but i was too deep to give up now so will see it to the end","335a4878":"# merged_drug_to_targets = merged_drug.groupby(by = 'drug_id')[target_cols].max()","abaf75b0":"# train_smote = pd.concat([X_res, y_res], axis=1)","8bf6f95c":"# merged_drug_to_targets","54849df7":"#50,817 newly genrated_data\n# train_smote_f= train_smote.merge(merged_drug_to_targets , on = 'drug_id')","7f0970b9":"# train_smote_f =  train_smote_f.drop(['drug_id'], axis =1)","cbfe055b":"# train_smote_f","c56bb456":"# target_f =  train_smote_f[target_cols]\n# top_targets = pd.Series(target_f.sum()).sort_values(ascending=False)[:20]\n# bottom_targets = pd.Series(target_f.sum()).sort_values()[:10]\n# fig, axs = plt.subplots(figsize=(9,9) , nrows=2)\n# sns.barplot(top_targets.values , top_targets.index , ax = axs[0] ).set(title = \"Top five targets\")\n# sns.barplot(bottom_targets.values , bottom_targets.index, ax = axs[1] ).set(title = \"bottom five targets\")\n# plt.show()\n# # indeed beautiful achives the purpose but assumptions are too much\n","b876f62b":"# train_smote_f = train_smote_f.fillna(0)\n# folds = train_smote_f.copy()\n\n# mskf = MultilabelStratifiedKFold(n_splits=7)\n\n# for f, (t_idx, v_idx) in enumerate(mskf.split(X=train_smote_f.drop(target_cols, axis=1), y= train_smote_f[target_cols])):\n#     folds.loc[v_idx, 'kfold'] = int(f)\n\n# folds['kfold'] = folds['kfold'].astype(int)\n# folds","1aa35f9d":"# train = train_smote_f","e5527fd5":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct    ","8e3839be":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        final_loss += loss.item()\n        \n    final_loss \/= len(dataloader)\n    \n    return final_loss\n\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss \/= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds","077b325d":"class SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","c162a752":"def process_data(data):\n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    return data","2b1ddde9":"feature_cols = [c for c in process_data(folds).columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]\nlen(feature_cols)","d21e1251":"# HyperParameters\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 25\nBATCH_SIZE = 128\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nNFOLDS = 7\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = False\n\nnum_features=len(feature_cols)\nnum_targets=len(target_cols)\nnum_non_scored_targets=len(target_non_scored_cols)\nhidden_size=1500\n\n\n\n","2d0b04da":"folds","34384749":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(Dropout_Model)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(Dropout_Model)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x\n    \nclass LabelSmoothingLoss(nn.Module):\n    def __init__(self, classes, smoothing=0.0, dim=-1):\n        super(LabelSmoothingLoss, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.cls = classes\n        self.dim = dim\n\n    def forward(self, pred, target):\n        pred = pred.log_softmax(dim=self.dim)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing \/ (self.cls - 1))\n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))    ","8919c61d":"def run_training(fold, seed , pretrain):\n    \n    seed_everything(seed)\n    \n    train = process_data(folds)\n    test_ = process_data(test)\n    \n    trn_idx = train[train['kfold'] != fold].index\n    val_idx = train[train['kfold'] == fold].index\n    \n    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n    \n    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n    \n    train_dataset = MoADataset(x_train, y_train)\n    valid_dataset = MoADataset(x_valid, y_valid)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    if pretrain:\n        model = Model(\n            num_features=num_features,\n            num_targets=num_non_scored_targets, # non scored targets\n            hidden_size=hidden_size,\n        )\n        \n        # Load pretrained model\n        model.load_state_dict(torch.load(f\"FOLD{fold}_non_scored.pth\"))\n        \n        # Reinitialize last layers\n        model.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        model.dropout3 = nn.Dropout(0.5)\n        model.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n        \n    else:\n        model = Model(\n            num_features=num_features,\n            num_targets=num_targets,\n            hidden_size=hidden_size,\n        )\n    model.to(DEVICE)\n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    \n    if pretrain:\n        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                              max_lr=1e-3, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n#         scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.5, patience=10, \n#                                                          threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n        epochs = 25\n    else:\n        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                              max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n        epochs = copy.copy(EPOCHS)\n    \n    \n    loss_fn = nn.BCEWithLogitsLoss()\n    loss_tr = SmoothBCEwLogits(smoothing =0.001)\n    \n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n   \n    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    best_loss = np.inf\n    \n    for epoch in range(EPOCHS):\n        \n        train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n        \n        if valid_loss < best_loss:\n            \n            best_loss = valid_loss\n            oof[val_idx] = valid_preds\n            torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n        \n        elif(EARLY_STOP == True):\n            \n            early_step += 1\n            if (early_step >= early_stopping_steps):\n                break\n            \n    \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n\n    )\n    \n    model.load_state_dict(torch.load(f\"FOLD{fold}_.pth\"))\n    model.to(DEVICE)\n    \n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return oof, predictions","535f170d":"train_x = process_data(folds_non_scored)","32d33949":"val_idx = train_x[train_x['kfold'] == 0].index","018b7066":"val_idx.shape","0564d17f":"target_non_scored","6d52e10f":"def run_training_non_scored(fold, seed):\n    \n    seed_everything(seed)\n    \n    train = process_data(folds_non_scored)\n    test_ = process_data(test)\n    \n    trn_idx = train[train['kfold'] != fold].index\n    val_idx = train[train['kfold'] == fold].index\n    \n    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n    \n    x_train, y_train  = train_df[feature_cols].values, train_df[target_non_scored_cols].values\n    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_non_scored_cols].values\n    \n    train_dataset = MoADataset(x_train, y_train)\n    valid_dataset = MoADataset(x_valid, y_valid)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_non_scored_targets,\n        hidden_size=hidden_size,\n    )\n    \n    model.to(DEVICE)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                              max_lr=1e-4, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    \n    loss_fn = nn.BCEWithLogitsLoss()\n    \n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n    \n    oof = np.zeros((len(train), target_non_scored.iloc[:, 1:].shape[1]))\n    best_loss = np.inf\n    \n    for epoch in range(10):\n        \n        train_loss = train_fn(model, optimizer,scheduler, loss_fn, trainloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n        \n        if valid_loss < best_loss:\n            \n            best_loss = valid_loss\n            oof[val_idx] = valid_preds\n            torch.save(model.state_dict(), f\"FOLD{fold}_non_scored.pth\")\n        \n        elif(EARLY_STOP == True):\n            \n            early_step += 1\n            if (early_step >= early_stopping_steps):\n                break\n            \n    \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_non_scored_targets,\n        hidden_size=hidden_size,\n    )\n    \n    model.load_state_dict(torch.load(f\"FOLD{fold}_non_scored.pth\"))\n    model.to(DEVICE)\n    \n    predictions = np.zeros((len(test_), target_non_scored.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return oof, predictions","530f17fa":"def run_k_fold_none_scored(NFOLDS, seed):\n    oof = np.zeros((len(train_non_scored), len(target_non_scored_cols)))\n    predictions = np.zeros((len(test), len(target_non_scored_cols)))\n    \n    for fold in range(NFOLDS):\n        oof_, pred_ = run_training_non_scored(fold, seed)\n        print(oof_.shape)\n        print(oof.shape)\n        predictions += pred_ \/ NFOLDS\n        oof += oof_\n        \n    return oof, predictions","a0a290fb":"def run_k_fold(NFOLDS, seed , pretrain=False):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold in range(NFOLDS):\n        oof_, pred_ = run_training(fold, seed, pretrain)\n        \n        predictions += pred_ \/ NFOLDS\n        oof += oof_\n        \n    return oof, predictions","a2626c1e":"SEED = [42, 1, 2, 3, 4, 5, 6]\n# SEED = [0, 1]\n\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\nrun_k_fold_none_scored(NFOLDS, SEED[0])\n\n","14e3869a":"for seed in SEED:\n    \n    oof_, predictions_ = run_k_fold(NFOLDS, seed, pretrain=True)\n    \n    oof += oof_ \/ len(SEED)\n    predictions += predictions_ \/ len(SEED)\n\ntrain[target_cols] = oof\ntest[target_cols] = predictions","6400cb5d":"train","068d2f6e":"# Averaging on multiple SEEDS\n\n\n# oof = np.zeros((len(train), len(target_cols)))\n# predictions = np.zeros((len(test), len(target_cols)))\n\n# for seed in SEED:\n    \n#     oof_, predictions_ = run_k_fold(NFOLDS, seed)\n#     oof += oof_ \/ len(SEED)\n#     predictions += predictions_ \/ len(SEED)\n\n# train[target_cols] = oof\n# test[target_cols] = predictions\n","c558ef50":"train_targets_scored","99743396":"len(target_cols)","fef6447b":"valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\ny_true = train_targets_scored[target_cols].values\ny_pred = valid_results[target_cols].values\n\nscore = 0\nfor i in range(len(target_cols)):\n    score_ = log_loss(y_true[:, i], y_pred[:, i])\n    score += score_ \/ target.shape[1]\n    \nprint(\"CV log_loss: \", score)    \n#0.015","d0f8b49a":"sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nsub.to_csv('submission.csv', index=False)","7f6ee98f":"sub.shape","46cf7db4":"sub","b3656247":"### 4.6 Dataset Classes\n\n","a08747db":"## SAMPLING ENDS","208b450c":"## 5. Modeling\n\n","5a191112":"### 4.3 PCA features\n\n","b48c52d7":"### 4.7 Smoothing\n\n","2f999493":"### 4.8 Preprocessing\n","f6d07ffc":"<a class=\"anchor\" id=\"0\"><\/a>\n# [Mechanisms of Action (MoA) Prediction](https:\/\/www.kaggle.com\/c\/lish-moa)","410bbd17":"# Sampling ","cdbf41df":"## 6. Prediction & Submission \n\n","bea44aeb":"### 4.4 Feature selection\n\n","f19ea171":"Please use the version 3 of this notebook for sampling techniques along with tranfer learning or uncomment the sampling part here.","ff353908":"Assumptions:\n* MoA could be reduced to multiclass using drug_id making X = features and y= drug_id\n* Assuming drug_id has a pattern and relation with targets \n","e3e9bbaf":"## 4. FE & Data Preprocessing \n\n","a040aece":"### 4.2 Seed\n","2848b2bf":"### 4.1 RankGauss<a class=\"anchor\" id=\"4.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","71dcd3f1":"### 4.5 CV folds\n\n","c96e6f1b":"uncomment the below lines","0883c097":"# **This notebook Introduces transfer learning with sampling techniques .**\n\n\n<h1 style=\"color:green\"> PLEASE CONSIDER UPVOTING THANKS!!!!!! <\/h1>\n\n<br>the model is first trained on non scored targets and then that is used to train the scored targets.\n\n\n\nBase for this notebook - \nhttps:\/\/www.kaggle.com\/chriscc\/kubi-pytorch-moa-transfer<br>\nhttps:\/\/www.kaggle.com\/ash1706\/moa-randomoversampler-drug-id-fixed-imbalance\n\n","6d494ce6":"[Go to Top](#0)"}}