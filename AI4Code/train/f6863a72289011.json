{"cell_type":{"672945fd":"code","fbd23aa6":"code","6082e95f":"code","b691d904":"code","5b93afaa":"code","85dfacf4":"code","b8bb0bd1":"code","ff68dd24":"code","9faadaa0":"code","b2da6bd6":"code","6dbe970c":"code","a848d366":"code","b50705d2":"code","97f63a2f":"code","8cbce181":"code","f82cdb5a":"code","91167bed":"code","03377adf":"code","8ce8ce30":"code","53fbd12a":"code","55096559":"code","89d6dd9f":"code","0bf4dd8c":"code","68e16e7c":"code","0b67525f":"code","e530d084":"markdown","9ad63302":"markdown","3227fc82":"markdown","a6c3cfbf":"markdown"},"source":{"672945fd":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import f1_score\nfrom collections import Counter\nimport lightgbm\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import StratifiedKFold\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.decomposition import PCA\nfrom tqdm.notebook import tqdm\nimport gensim\nimport os\nos.listdir(\"..\/input\/ykc-cup-2nd\/\")","fbd23aa6":"train = pd.read_csv(\"..\/input\/ykc-cup-2nd\/train.csv\")\ntest = pd.read_csv(\"..\/input\/ykc-cup-2nd\/test.csv\")\nsub = pd.read_csv(\"..\/input\/ykc-cup-2nd\/sample_submission.csv\")\ntrain.shape, test.shape, sub.shape","6082e95f":"train.head()","b691d904":"test.head()","5b93afaa":"## train\u3068test\u3092\u304f\u3063\u3064\u3051\u3066\u4e00\u62ec\u3067\u7279\u5fb4\u91cf\u4f5c\u6210\u3092\u3059\u308b\ndf = pd.concat([train, test])\ndf = df.reset_index(drop=True)\ndf.shape","85dfacf4":"target = \"department_id\" ## \u4e88\u6e2c\u5bfe\u8c61\n\nn_split = 5 ## cross validation\u306efold\u6570\nkfold = StratifiedKFold(n_splits=n_split, shuffle = True, random_state=42) # feature engineering\u3067\u4f7f\u3046\u304b\u3082\u3057\u308c\u306a\u3044\u306e\u3067\u5148\u306b\u7528\u610f","b8bb0bd1":"df[\"product_name\"] = df[\"product_name\"].apply(lambda words : words.lower().replace(\",\", \"\").replace(\"&\", \"\").split(\" \"))\ndf.head()","ff68dd24":"model_names = [\"crawl-300d-2M\", \"crawl-300d-2M-subword\", \"wiki-news-300d-1M\", \"wiki-news-300d-1M-subword\"]","9faadaa0":"## \u8a13\u7df4\u6e08\u307f\u306e\u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u3092\u8aad\u307f\u8fbc\u3093\u3067\uff0cproduct_name\u306b\u542b\u307e\u308c\u308b\u5358\u8a9e\u3092\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3057\u3066\u5e73\u5747\u3092\u53d6\u308b\u3053\u3068\u3067\uff0c\u5404product_id\u306b\u5bfe\u3057\u3066\u7279\u5fb4\u91cf\u30d9\u30af\u30c8\u30eb\u3092\u4f5c\u6210\u3059\u308b\n\nfasttext_pretrain_cols = []\n\nfrom collections import defaultdict\nunused_words = defaultdict(int)\n\nfor model_name in model_names:\n    model = pd.read_pickle(f\"..\/input\/fasttext\/{model_name}.pkl\") \n\n    def to_mean_vec(x, model):\n        v = np.zeros(model.vector_size)\n        for w in x:\n            try:\n                v += model[w] ## \u5358\u8a9e\u304c\u8a13\u7df4\u6e08\u307f\u30e2\u30c7\u30eb\u306evocab\u306b\u3042\u3063\u305f\u3089\n            except:\n                unused_words[w] += 1 ## \u30d9\u30af\u30c8\u30eb\u304c\u5b58\u5728\u3057\u306a\u304b\u3063\u305f\u5358\u8a9e\u3092\u30e1\u30e2\n        v = v \/ (np.sqrt(np.sum(v ** 2)) + 1e-16) ## \u9577\u3055\u30921\u306b\u6b63\u898f\u5316\n        return v   \n    \n    def to_max_vec(x, model):\n        v = np.zeros(model.vector_size)\n        for w in x:\n            try:\n                v = np.maximum(v, model[w])\n            except:\n                pass\n            \n        return v\n        \n    mean_vecs = df[\"product_name\"].apply(lambda x : to_mean_vec(x, model))\n    mean_vecs = np.vstack(mean_vecs)\n    cols = [f\"fasttext_pretrain_{model_name}_mean_vec{k}\" for k in range(mean_vecs.shape[1])]\n    fasttext_pretrain_cols += cols\n    mean_vec_df = pd.DataFrame(mean_vecs, columns=cols)\n    df = pd.concat([df, mean_vec_df], axis = 1)\n    \n    max_vecs = df[\"product_name\"].apply(lambda x : to_max_vec(x, model))\n    max_vecs = np.vstack(max_vecs)\n    cols = [f\"fasttext_pretrain_{model_name}_max_vec{k}\" for k in range(max_vecs.shape[1])]\n    fasttext_pretrain_cols += cols\n    max_vec_df = pd.DataFrame(max_vecs, columns=cols)\n    df = pd.concat([df, max_vec_df], axis = 1)\n    \ndf.head()","b2da6bd6":"## train\u3068test\u3092\u5206\u96e2\ntrain = df[~df[target].isna()]\ntest = df[df[target].isna()]","6dbe970c":"def to_weighted_count_vec(x, word_sets):\n    v = np.zeros(21)\n    \n    for w in x:\n        hits = []\n        for i in range(21):\n            if w in word_sets[i]:\n                hits.append(i)\n                \n        for i in hits:\n            v[i] += 1.0 \/ len(hits)\n            \n    return v\n\nweighted_count_cols = [f\"weighted_count_vec{k}\" for k in range(21)]","a848d366":"# train\n\ntrain_weighted_count = pd.DataFrame(index=train.index, columns=weighted_count_cols, dtype=np.float32)\n\nfor i_fold, (train_idx, valid_idx) in enumerate(kfold.split(train, train[target])):    \n    ## train data\n    tr = train.loc[train_idx]\n\n    ## valid data\n    va = train.loc[valid_idx]\n    \n    word_sets = [set(sum(tr[tr[\"department_id\"] == i][\"product_name\"], [])) for i in range(21)]\n    vecs = va[\"product_name\"].apply(lambda x : to_weighted_count_vec(x, word_sets))\n    vecs = np.vstack(vecs)\n    \n    vec_df = pd.DataFrame(vecs, index=va.index, columns=weighted_count_cols)\n    train_weighted_count.loc[valid_idx, :] = vec_df\n\ntrain = pd.concat([train, train_weighted_count], axis=1)","b50705d2":"train.head()","97f63a2f":"# test\n\ntest_weighted_count = pd.DataFrame(index=test.index, columns=weighted_count_cols, dtype=np.float32)\n\nword_sets = [set(sum(train[train[\"department_id\"] == i][\"product_name\"], [])) for i in range(21)]\nvecs = test[\"product_name\"].apply(lambda x : to_weighted_count_vec(x, word_sets))\nvecs = np.vstack(vecs)\ntest_weighted_count.loc[:, :] = pd.DataFrame(vecs, index=test.index, columns=weighted_count_cols)\ntest = pd.concat([test, test_weighted_count], axis=1)","8cbce181":"test.head()","f82cdb5a":"features = fasttext_pretrain_cols + weighted_count_cols + [\"order_rate\", \"order_dow_mode\", \"order_hour_of_day_mode\"]## \u4e88\u6e2c\u306b\u4f7f\u7528\u3059\u308b\u7279\u5fb4\u91cf\u306e\u540d\u524d","91167bed":"import pandas as pd\nimport numpy as np\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float32)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","03377adf":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","8ce8ce30":"## cross validation\npreds_test = []\nscores = []\noof = np.zeros((len(train), 21))\nfor i_fold, (train_idx, valid_idx) in enumerate(kfold.split(train, train[target])):\n    print(f\"--------fold {i_fold}-------\")\n    \n    ## train data\n    x_tr = train.loc[train_idx, features]\n    y_tr = train.loc[train_idx, target]\n\n    ## valid data\n    x_va = train.loc[valid_idx, features]\n    y_va = train.loc[valid_idx, target]\n\n    ## train LGBM model\n    lgb = LGBMClassifier(objective='multiclass', colsample_bytree=0.2, n_estimators=500)\n    lgb.fit(x_tr, y_tr, eval_set=[(x_va, y_va)], early_stopping_rounds=10, verbose=10)\n         \n    def predict_proba(x):\n        return lgb.predict_proba(x)\n    \n    ## predict on valid\n    pred_val = predict_proba(x_va)\n    oof[valid_idx] += pred_val\n    \n    ## evaluate\n    score = {\n        \"logloss\"  : log_loss(y_va, pred_val),\n        \"f1_micro\" : f1_score(y_va, np.argmax(pred_val, axis = 1), average = \"micro\")}\n    print(score)\n    scores.append(score)\n    \n    ## predict on test\n    pred_test = predict_proba(test[features])\n    preds_test.append(pred_test)","53fbd12a":"score_df = pd.DataFrame(scores)\nscore_df","55096559":"score_df.mean()","89d6dd9f":"oof_df = pd.DataFrame(oof)\noof_df.to_csv(\"oof_lgb.csv\", index = False)","0bf4dd8c":"for i in range(len(preds_test)):\n    pred_df = pd.DataFrame(preds_test[i])\n    pred_df.to_csv(f\"pred_{i}_lgb.csv\", index = False)","68e16e7c":"## cv\u306e\u5404fold\u3067\u8a08\u7b97\u3057\u305f\u4e88\u6e2c\u5024\u306e\u5e73\u5747\u3092\u6700\u7d42\u7684\u306a\u4e88\u6e2c\u5024\u306b\npred_test_final = np.array(preds_test).mean(axis = 0)\npred_test_final = np.argmax(pred_test_final, axis = 1)","0b67525f":"sub[\"department_id\"] = pred_test_final\nsub.to_csv(\"submission_lgb.csv\", index = False)\nsub.head()","e530d084":"## train","9ad63302":"## feature engineering","3227fc82":"## read data ","a6c3cfbf":"## submission"}}