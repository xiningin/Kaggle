{"cell_type":{"620f1553":"code","4c3ee5c7":"code","51afe00d":"code","77078fef":"code","36ddc06e":"code","049e8159":"code","341d716e":"code","4af5410b":"code","815759c2":"code","aeb35c14":"code","2a92a9b1":"code","5e2977c8":"code","912df4c5":"code","5f2d2581":"code","697a718f":"code","d92983de":"code","a6afa1d8":"code","c4ee2211":"code","aa490cc6":"code","c61cc965":"code","066c43e4":"code","7e32da5d":"code","374a406d":"code","44b55b33":"code","b5c9ff56":"code","a756dd74":"code","dae2bd62":"markdown","a4c894d9":"markdown","2b5f85d5":"markdown","bbde4322":"markdown","6333d415":"markdown","33bc883a":"markdown","e00228c5":"markdown","1b21063f":"markdown","976f6d78":"markdown","125dddba":"markdown","6f0a4e09":"markdown","7a947d1a":"markdown","f32328db":"markdown","4065e524":"markdown","4de4f865":"markdown","25a7d3ef":"markdown","91860990":"markdown","7b336532":"markdown"},"source":{"620f1553":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \nplt.xkcd()\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nPATH = \"..\/input\/\"\nprint(os.listdir(PATH))\n\n# Any results you write to the current directory are saved as output.","4c3ee5c7":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df\n\n\ndef import_data(file):\n    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n    df = pd.read_csv(file, parse_dates=True, keep_date_col=True)\n    df = reduce_mem_usage(df)\n    return df","51afe00d":"application_train = import_data(PATH+'application_train.csv')\napplication_test = import_data(PATH+'application_test.csv')","77078fef":"application_train = application_train[application_train['AMT_INCOME_TOTAL'] != 1.170000e+08]\napplication_train = application_train[application_train['AMT_REQ_CREDIT_BUREAU_QRT'] != 261]\napplication_train = application_train[application_train['OBS_30_CNT_SOCIAL_CIRCLE'] < 300]","36ddc06e":"application_train['DAYS_EMPLOYED'] = (application_train['DAYS_EMPLOYED'].apply(lambda x: x if x != 365243 else np.nan))","049e8159":"def feat_ext_source(df):\n    x1 = df['EXT_SOURCE_1'].fillna(-1) + 1e-1\n    x2 = df['EXT_SOURCE_2'].fillna(-1) + 1e-1\n    x3 = df['EXT_SOURCE_3'].fillna(-1) + 1e-1\n    \n    df['EXT_SOURCE_1over2_NAminus1_Add0.1'] = x1\/x2\n    df['EXT_SOURCE_2over1_NAminus1_Add0.1'] = x2\/x1\n    df['EXT_SOURCE_1over3_NAminus1_Add0.1'] = x1\/x3\n    df['EXT_SOURCE_3over1_NAminus1_Add0.1'] = x3\/x1\n    df['EXT_SOURCE_2over3_NAminus1_Add0.1'] = x2\/x3\n    df['EXT_SOURCE_3over2_NAminus1_Add0.1'] = x3\/x2\n    \n    df['EXT_SOURCE_na1_2'] = (application_train['EXT_SOURCE_1'].isnull()) * (application_train['EXT_SOURCE_2'].fillna(0))\n    df['EXT_SOURCE_na1_3'] = (application_train['EXT_SOURCE_1'].isnull()) * (application_train['EXT_SOURCE_3'].fillna(0))\n    df['EXT_SOURCE_na2_1'] = (application_train['EXT_SOURCE_2'].isnull()) * (application_train['EXT_SOURCE_1'].fillna(0))\n    df['EXT_SOURCE_na2_3'] = (application_train['EXT_SOURCE_2'].isnull()) * (application_train['EXT_SOURCE_3'].fillna(0))\n    df['EXT_SOURCE_na3_1'] = (application_train['EXT_SOURCE_3'].isnull()) * (application_train['EXT_SOURCE_1'].fillna(0))\n    df['EXT_SOURCE_na3_2'] = (application_train['EXT_SOURCE_3'].isnull()) * (application_train['EXT_SOURCE_2'].fillna(0))\n    \n    df['CREDIT_LENGTH'] = df['AMT_CREDIT'] \/ df['AMT_ANNUITY']\n    \n    return df","341d716e":"application_train = feat_ext_source(application_train)\napplication_test  = feat_ext_source(application_test)","4af5410b":"# use this if you want to convert categorical features to dummies(default)\ndef cat_to_dummy(train, test):\n    train_d = pd.get_dummies(train, drop_first=False)\n    test_d = pd.get_dummies(test, drop_first=False)\n    # make sure that the number of features in train and test should be same\n    for i in train_d.columns:\n        if i not in test_d.columns:\n            if i!='TARGET':\n                train_d = train_d.drop(i, axis=1)\n    for j in test_d.columns:\n        if j not in train_d.columns:\n            if j!='TARGET':\n                test_d = test_d.drop(i, axis=1)\n    print('Memory usage of train increases from {:.2f} to {:.2f} MB'.format(train.memory_usage().sum() \/ 1024**2, \n                                                                            train_d.memory_usage().sum() \/ 1024**2))\n    print('Memory usage of test increases from {:.2f} to {:.2f} MB'.format(test.memory_usage().sum() \/ 1024**2, \n                                                                            test_d.memory_usage().sum() \/ 1024**2))\n    return train_d, test_d\n\napplication_train_ohe, application_test_ohe = cat_to_dummy(application_train, application_test)","815759c2":"# use this if you want to convert categorical features to dummies(default)\ndef cat_to_int(train, test):\n    mem_orig_train = train.memory_usage().sum() \/ 1024**2\n    mem_orig_test  = test .memory_usage().sum() \/ 1024**2\n    categorical_feats = [ f for f in train.columns if train[f].dtype == 'object' or train[f].dtype.name == 'category' ]\n    print('---------------------')\n    print(categorical_feats)\n    for f_ in categorical_feats:\n        train[f_], indexer = pd.factorize(train[f_])\n        test[f_] = indexer.get_indexer(test[f_])\n    print('Memory usage of train increases from {:.2f} to {:.2f} MB'.format(mem_orig_train, \n                                                                            train.memory_usage().sum() \/ 1024**2))\n    print('Memory usage of test increases from {:.2f} to {:.2f} MB'.format(mem_orig_test, \n                                                                            test.memory_usage().sum() \/ 1024**2))\n    return categorical_feats, train, test\n\n#categorical_feats, application_train_ohe, application_test_ohe = cat_to_int(application_train, application_test)","aeb35c14":"#application_train_ohe, application_test_ohe = (application_train, application_test)","2a92a9b1":"\n#from imblearn.under_sampling import RandomUnderSampler\n#rus = RandomUnderSampler(random_state=314)\n#X_rus, y_rus = rus.fit_sample(application_train_ohe.drop(['SK_ID_CURR', 'TARGET'], axis=1).fillna(-1), \n#                              application_train_ohe['TARGET'])\n\n# You can use the full sample and do sample weighting in lightgbm using `is_unbalance` OR `scale_pos_weight` argument\n# But it makes the code to run 8x..10x slower, which is ok for the run with pre-optimised parametersm but is too slow for HP optimisation\nX_rus, y_rus = (application_train_ohe.drop(['SK_ID_CURR', 'TARGET'], axis=1),\n                application_train_ohe['TARGET'])","5e2977c8":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_rus, y_rus, test_size=0.20, random_state=314, stratify=y_rus)","912df4c5":"def learning_rate_010_decay_power_099(current_iter):\n    base_learning_rate = 0.1\n    lr = base_learning_rate  * np.power(.99, current_iter)\n    return lr if lr > 1e-3 else 1e-3\n\ndef learning_rate_010_decay_power_0995(current_iter):\n    base_learning_rate = 0.1\n    lr = base_learning_rate  * np.power(.995, current_iter)\n    return lr if lr > 1e-3 else 1e-3\n\ndef learning_rate_005_decay_power_099(current_iter):\n    base_learning_rate = 0.05\n    lr = base_learning_rate  * np.power(.99, current_iter)\n    return lr if lr > 1e-3 else 1e-3","5f2d2581":"import lightgbm as lgb\nfit_params={\"early_stopping_rounds\":30, \n            \"eval_metric\" : 'auc', \n            \"eval_set\" : [(X_test,y_test)],\n            'eval_names': ['valid'],\n            #'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_099)],\n            'verbose': 100,\n            'categorical_feature': 'auto'}","697a718f":"from scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\nparam_test ={'num_leaves': sp_randint(6, 50), \n             'min_child_samples': sp_randint(100, 500), \n             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n             'subsample': sp_uniform(loc=0.2, scale=0.8), \n             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}","d92983de":"#This parameter defines the number of HP points to be tested\nn_HP_points_to_test = 100\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n\n#n_estimators is set to a \"large value\". The actual number of trees build will depend on early stopping and 5000 define only the absolute maximum\nclf = lgb.LGBMClassifier(max_depth=-1, random_state=314, silent=True, metric='None', n_jobs=4, n_estimators=5000)\ngs = RandomizedSearchCV(\n    estimator=clf, param_distributions=param_test, \n    n_iter=n_HP_points_to_test,\n    scoring='roc_auc',\n    cv=3,\n    refit=True,\n    random_state=314,\n    verbose=True)","a6afa1d8":"#gs.fit(X_train, y_train, **fit_params)\n#print('Best score reached: {} with params: {} '.format(gs.best_score_, gs.best_params_))","c4ee2211":"opt_parameters = {'colsample_bytree': 0.9234, 'min_child_samples': 399, 'min_child_weight': 0.1, 'num_leaves': 13, 'reg_alpha': 2, 'reg_lambda': 5, 'subsample': 0.855}","aa490cc6":"clf_sw = lgb.LGBMClassifier(**clf.get_params())\n#set optimal parameters\nclf_sw.set_params(**opt_parameters)","c61cc965":"gs_sample_weight = GridSearchCV(estimator=clf_sw, \n                                param_grid={'scale_pos_weight':[1,2,6,12]},\n                                scoring='roc_auc',\n                                cv=5,\n                                refit=True,\n                                verbose=True)","066c43e4":"gs_sample_weight.fit(X_train, y_train, **fit_params)\nprint('Best score reached: {} with params: {} '.format(gs_sample_weight.best_score_, gs_sample_weight.best_params_))","7e32da5d":"#print(\"Valid+-Std     Train  :   Parameters\")\n#for i in np.argsort(gs.cv_results_['mean_test_score'])[-5:]:\n#    print('{1:.3f}+-{3:.3f}     {2:.3f}   :  {0}'.format(gs.cv_results_['params'][i], \n#                                    gs.cv_results_['mean_test_score'][i], \n#                                    gs.cv_results_['mean_train_score'][i],\n#                                    gs.cv_results_['std_test_score'][i]))","374a406d":"print(\"Valid+-Std     Train  :   Parameters\")\nfor i in np.argsort(gs_sample_weight.cv_results_['mean_test_score'])[-5:]:\n    print('{1:.3f}+-{3:.3f}     {2:.3f}   :  {0}'.format(gs_sample_weight.cv_results_['params'][i], \n                                    gs_sample_weight.cv_results_['mean_test_score'][i], \n                                    gs_sample_weight.cv_results_['mean_train_score'][i],\n                                    gs_sample_weight.cv_results_['std_test_score'][i]))","44b55b33":"#Configure from the HP optimisation\n#clf_final = lgb.LGBMClassifier(**gs.best_estimator_.get_params())\n\n#Configure locally from hardcoded values\nclf_final = lgb.LGBMClassifier(**clf.get_params())\n#set optimal parameters\nclf_final.set_params(**opt_parameters)\n\n#Train the final model with learning rate decay\nclf_final.fit(X_train, y_train, **fit_params, callbacks=[lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_0995)])","b5c9ff56":"feat_imp = pd.Series(clf_final.feature_importances_, index=application_train_ohe.drop(['SK_ID_CURR', 'TARGET'], axis=1).columns)\nfeat_imp.nlargest(20).plot(kind='barh', figsize=(8,10))","a756dd74":"probabilities = clf_final.predict_proba(application_test_ohe.drop(['SK_ID_CURR'], axis=1))\nsubmission = pd.DataFrame({\n    'SK_ID_CURR': application_test_ohe['SK_ID_CURR'],\n    'TARGET':     [ row[1] for row in probabilities]\n})\nsubmission.to_csv(\"submission.csv\", index=False)","dae2bd62":"### Prepare learning rate shrinkage","a4c894d9":"### Use test subset for early stopping criterion \nThis allows us to avoid overtraining and we do not need to optimise the number of trees","2b5f85d5":"## Read in the data reducing memory pattern for variables.\nThe implementation was copied over from [this kernel](https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage)","bbde4322":"## Build the final model\nWe do training with the 0.8 subset of the dataset and 0.2 subset for early stopping. We use the tuned parameter values but a smaller learning rate to allow smoother convergence to the minimum","6333d415":"## Additional numerical features\nThe credit length feature idea is due [@oskird](https:\/\/www.kaggle.com\/sz8416) implemented [here in the corresponding kernel](https:\/\/www.kaggle.com\/sz8416\/eda-baseline-model-using-application)","33bc883a":"Run this cell, to do HP optimisation. To save time `opt_parameters` was directly hardcoded below.","e00228c5":"# Model fitting with HyperParameter optimisation\nWe will use LightGBM classifier - LightGBM allows to build very sophysticated models with a very short training time.\n### Split the full sample into train\/test (80\/20)","1b21063f":"### Plot feature importance","976f6d78":"The following 2 cells with cleaning criteria were inherited from [this kernel](https:\/\/www.kaggle.com\/kingychiu\/home-credit-eda-distributions-and-outliers)","125dddba":"## Tune the weights of unbalanced classes\nFollowing discussion in [this comment](https:\/\/www.kaggle.com\/mlisovyi\/modular-good-fun-with-ligthgbm\/comments#337494), there was a small tuning of the disbalanced sample weight:\n","6f0a4e09":"# Predict on the submission test sample","7a947d1a":"## Categorical encoding\nThe function was taken from [this kernel](https:\/\/www.kaggle.com\/sz8416\/simple-intro-eda-baseline-model-with-gridsearch). It allows to do OneHotEncoding (OHE) keeping only those columns that are common to train and test samples. OHE is performed using `pd.get_dummies`, which allows to convert categorical features, while keeping numerical untouched","f32328db":"### Look at the performance of the top-5 parameter choices\n(the list is inverted)","4065e524":"As an outcome, precision of the classifier does not depend much on the internal class weighting, but `weight=1` still turns out to give slightly better performance that weighted scenarios.","4de4f865":"## Deal with category imbalance\nUse a standard library (`imblearn`) to to random undersampling on the dominating category. Use if if you want to repeat the HP optimisation","25a7d3ef":"### Set up HyperParameter search\nWe use random search, which is more flexible and more efficient than a grid search","91860990":"Use this instead if you want to make use of the internal categorical feature treatment in lightgbm.","7b336532":"# Basic end-to-end training of a LightGBM model\n_To be added next:_ \n- An example of Bayesian Optimisation (inspired by the comment in [this kernel](https:\/\/www.kaggle.com\/tilii7\/olivier-lightgbm-parameters-by-bayesian-opt\/))- this is not expected to bring singificant improvement, but it is a fun technique to exercise\n\nFeatures that are illustrated in this kernel:\n- data reading with **memory footprint reduction**\n- a bit of feature engineering adding **estimated credit length, which boosts AUC ROC by 0.015 on PLB and by 0.035 in local CV**\n- categorical feature encoding using **one-hot-encoding (OHE)**\n-  internal **category weighting** by _**LightGBM**_ was tuned and no need of resampling is shown\n- **gradient-boosted decision trees** using _**LightGBM**_ package\n- **early stopping** in _**LightGBM**_ model training to avoid overtraining\n- **learning rate decay** in _**LightGBM**_ model training to improve convergence to the minimum\n- **hyperparameter optimisation** of the model using random search in cross validation\n- submission preparation\nThis kernel inherited ideas and SW solutions from other public kernels and in such cases I will post direct references to the original product, that that you can get some additional insights from the source."}}