{"cell_type":{"49c592ec":"code","f5db4a15":"code","90608e7b":"code","32687e19":"code","36a614b4":"code","bfb8aeb5":"code","54447782":"code","c6e4123c":"code","cd1f9468":"code","0bbe6fc7":"code","b51b2aa0":"code","e43d392c":"code","42391fb6":"code","45090d84":"code","af3e5826":"code","bf10a30d":"code","2ee9793f":"code","9895e9a0":"code","3a1bc5c7":"code","c713159a":"code","7d6c88fc":"code","c9f9064d":"code","10d558ea":"code","a2440fed":"code","d3f3e8b6":"code","c5239a6e":"code","b931d87c":"code","fcc82f70":"code","4936d725":"code","eb02dc8a":"code","ae74ad26":"code","f9e69f0a":"code","574c579b":"code","34d9e75b":"code","8bfd0e00":"code","5bb8a684":"code","b9aa26b2":"code","d7f98066":"code","1d3705f6":"code","1e30154e":"code","401c711b":"code","c2aaec64":"code","1943e722":"code","7cd2b61c":"code","3266c82f":"code","8785a291":"code","9120b48d":"code","3a259786":"code","4f3358d2":"code","cd2da0bb":"code","d75b2363":"code","47654d51":"code","ea4b9370":"code","a40cd060":"code","ba56fd98":"code","9971c10f":"code","05db9cdd":"code","cabf34de":"code","0d84e2b3":"code","cfe4ca00":"code","db61b51a":"code","d63ec1ed":"code","cab62280":"code","423f137a":"code","27a5bb55":"code","4f717f3c":"code","fed7b3c5":"code","d21a1881":"code","40380037":"markdown","4259f77a":"markdown","aed9d2c5":"markdown","c07e9224":"markdown","699af2bd":"markdown","8fcc7e5c":"markdown","7ad4ffe5":"markdown","2a49a0e7":"markdown","c97ef7c6":"markdown","c20b8108":"markdown","e6ab6f35":"markdown","373dca06":"markdown","f942f137":"markdown","f2a90b82":"markdown","aac28a62":"markdown"},"source":{"49c592ec":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","f5db4a15":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","90608e7b":"df = pd.read_csv('\/kaggle\/input\/us-accidents\/US_Accidents_Dec19.csv')","32687e19":"df.head()","36a614b4":"#Let's take a look at the columns\ndf.info()","bfb8aeb5":"#Where were the data collected from?\ndf['Source'].value_counts()","54447782":"sns.set_style('whitegrid')\nsns.set_palette('Spectral')","c6e4123c":"plt.figure(figsize = (8, 6))\nsns.countplot(df['Source'], order = df['Source'].value_counts().index)","cd1f9468":"#Let's check any missing data\ndf.isna().sum()","0bbe6fc7":"#TMC provides more detailed event code\n#df['TMC'].value_counts()\n#after looking at https:\/\/wiki.openstreetmap.org\/wiki\/TMC\/Event_Code_List, I do not think those details would add much value\n#for this analysis, I would drop the column here\ndf.drop('TMC', axis = 1, inplace = True)","b51b2aa0":"df['Severity'].value_counts()","e43d392c":"sns.countplot(df['Severity'])","42391fb6":"#Let's look at Start and End time for a sec\ndf[['Start_Time', 'End_Time']].head()","45090d84":"from datetime import datetime","af3e5826":"#The times are pretty close, we might be interested to see the time difference between End_Time and Start_Time\ntime_diff = \\\ndf['End_Time'].apply(datetime.strptime, args = ('%Y-%m-%d %H:%M:%S',)) - \\\ndf['Start_Time'].apply(datetime.strptime, args = ('%Y-%m-%d %H:%M:%S',))","bf10a30d":"#Convert everything to hour difference, ignoring microseconds\ntime_diff_hr = time_diff.apply(lambda x: x.days * 24 + x.seconds \/ 3600)","2ee9793f":"time_diff_hr[:10]","9895e9a0":"#Add time_diff_hr back to df as Time_Diff\ndf['Time_Diff'] = time_diff_hr","3a1bc5c7":"start_hour = df['Start_Time'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S').hour)","c713159a":"morning = [1 if 6 <= x <= 11 else 0 for x in start_hour]\nafternoon = [1 if 12 <= x <= 17 else 0 for x in start_hour]\nevening = [1 if 18 <= x <= 24 or 0 <= x <= 5 else 0 for x in start_hour]","7d6c88fc":"#Now we put them back into df\ndf = df.assign(Morning = morning, Afternoon = afternoon, Evening = evening)","c9f9064d":"#We can drop the Start_Time and End_Time columns now\ndf.drop(['Start_Time', 'End_Time'], axis = 1, inplace = True)","10d558ea":"#Now let's see accidents distribution by timeframe\ntemp = np.asarray(morning) + np.asarray(afternoon) * 2 + np.asarray(evening) * 3\ntimeframe = ['morning' if x == 1 else 'afternoon' if x == 2 else 'evening' for x in temp]\ndel temp","a2440fed":"plt.figure(figsize = (8, 6))\nsns.countplot(timeframe, order = ['morning', 'afternoon', 'evening'])\n#It seems most accidents happen in the morning\n#My initial thought is that evening times would have more accidents because of poor lighting but the plot shows otherwise","d3f3e8b6":"#Now view what columns we have again\ndf.info()","c5239a6e":"#The next columns are Latitudes and Longitudes, it might be hard to use them directly, \n#but I am thinking about using K-means to group them into 10 geo-spacial areas\n\n#Let's look at some sample data first\ndf[['Start_Lat', 'Start_Lng', 'End_Lat', 'End_Lng']].head(10)","b931d87c":"#End_Lat and End_Lng have many missing values concurring with our earlier findings\n#So we would drop End_Lat and End_Lng and only use Start_Lat and Start_Lng for K-means\n#Actually, here, why not we just drop all the columns that have a lot of missing values altogether?\n#Again, the missing value columns are\ncols_missing_vals = df.isna().sum()[lambda x: x > 0]\ncols_missing_vals","fcc82f70":"#The total # of rows of the df is\nnum_rows = len(df.index)\nnum_rows","4936d725":"#Let's say we do not want columns that is missing over 5% of the data\ncols_to_drop = cols_missing_vals[lambda x: x > num_rows * 0.05]\ncols_to_drop","eb02dc8a":"#Now drop the columns from df\ndf.drop(cols_to_drop.index, axis = 1, inplace = True)","ae74ad26":"#Now Let's see if we can use kdeplot on a SAMPLE of df to visualize density of Start_Lng vs. Start_Lat\ndf_sample = df.sample(10000)\nsns.kdeplot(df_sample['Start_Lng'], df_sample['Start_Lat'], shade = True)","f9e69f0a":"#Now implement K-means to find 10 clusters\n#reference on elbow method (not used here): https:\/\/towardsdatascience.com\/machine-learning-algorithms-part-9-k-means-example-in-python-f2ad05ed5203\nfrom sklearn.cluster import KMeans","574c579b":"X = df[['Start_Lat', 'Start_Lng']]\nkmeans = KMeans(n_clusters = 10)\ngeo_cluster = kmeans.fit_predict(X)\ndf['Geo_Cluster'] = geo_cluster","34d9e75b":"#Now we can drop Start_Lat and Start_Lng\ndf.drop(['Start_Lat', 'Start_Lng'], axis = 1, inplace = True)","8bfd0e00":"#Top 10 states for accidents\nfig, ax = plt.subplots(figsize = (8, 6))\n\ntemp = df['State'].value_counts().head(10)\nsns.barplot(temp.index, temp.values, ax = ax)\n\nax.set_xlabel('States')\nax.set_ylabel('# of Accidents')\n\ndel temp","5bb8a684":"#Distribution of Temperature(F)\nsns.distplot(df['Temperature(F)'].dropna())","b9aa26b2":"#Weather conditions where most accidents happen\nfig, ax = plt.subplots(figsize = (8, 6))\n\ntemp = df['Weather_Condition'].value_counts().head(10)\nsns.barplot(temp.index, temp.values, ax = ax)\n\nax.set_xlabel('Weather_Condition')\nax.set_ylabel('# of Accidents')\n\nax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n\ndel temp","d7f98066":"#What I am going to do next is to drop the features that we are not going to use in this analysis\ndf.drop(['Description', 'Street', 'Side', 'City', 'County', 'State', 'Zipcode', 'Country', 'Airport_Code', 'Weather_Timestamp', \n        'Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight'], axis = 1, inplace = True)","1d3705f6":"#Lets see the columns that are left\ndf.info()","1e30154e":"#Missing Value Cols\ncols_missing_vals = df.isna().sum()[lambda x: x > 0].index\ndf[cols_missing_vals].info()","401c711b":"#We would fill missing value cols by data type\ndef fill_missing_values(col_name):\n    #For float64, we fill using median\n    if df[col_name].dtype == np.float64:\n        df[col_name].fillna(df[col_name].median(), inplace = True)\n    #For objects, we use existing distribution\n    else:\n        nas = df[col_name].isna()\n        df.loc[nas, col_name] = df.loc[~nas, col_name].sample(nas.sum(), replace = True).values","c2aaec64":"for col_name in cols_missing_vals:\n    fill_missing_values(col_name)","1943e722":"#Now check for columns missing data again\ndf.isna().any()","7cd2b61c":"#Now let's find correlations between features\ndf.corr()","3266c82f":"#Something seems to be off for Turning_Loop, let's inspect it\ndf['Turning_Loop'].value_counts()","8785a291":"#It does not have any True values, so let's drop col Turning_Loop\ndf.drop('Turning_Loop', axis = 1, inplace = True)","9120b48d":"df_corr = df.corr()","3a259786":"#Now we can visualize correlations using heatmap\nplt.figure(figsize = (10, 8))\nsns.heatmap(df_corr, linewidths=.5, cmap = 'Blues')","4f3358d2":"#It seems we have few variables that are highly correlated with each other, which is good.\n#Now let's take a look at our data again and see if we need additional processing on some columns\npd.set_option('display.max_columns', None)\ndf.head()","cd2da0bb":"#Drop ID, drop Evening because we only need Morning and Afternoon to represent all three time buckets\ndf.drop(['ID', 'Evening'], axis = 1, inplace = True)","d75b2363":"#For Wind_Direction and Weather_Condition, because they have many categories, \n#we would like to narrow the categories down, see below\ndf['Wind_Is_Calm'] = df['Wind_Direction'] == 'Calm'\ndf['Weather_Is_Clear'] = df['Weather_Condition'] == 'Clear'\ndf.drop(['Wind_Direction', 'Weather_Condition'], axis = 1, inplace = True)","47654d51":"#Get dummies for categorical features\ndf = pd.get_dummies(data = df, columns = ['Source', 'Timezone', 'Geo_Cluster'], drop_first = True)","ea4b9370":"X = df.drop('Severity', axis = 1)\ny = df['Severity']","a40cd060":"#transform bool columns to int\nbool_cols = X.select_dtypes(include = ['bool']).columns\nX.loc[:, bool_cols] = X[bool_cols].astype(int)","ba56fd98":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1, stratify = y)","9971c10f":"#Standardize numerical features\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\ncols_to_scale = ['Distance(mi)', 'Temperature(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Time_Diff']\n\nscaler.fit(X_train[cols_to_scale])\n\nX_train_std = X_train.copy()\nX_test_std = X_test.copy()\n\nX_train_std.loc[:, cols_to_scale] = scaler.transform(X_train[cols_to_scale])\nX_test_std.loc[:, cols_to_scale] = scaler.transform(X_test[cols_to_scale])","05db9cdd":"#Experiment:\n#Let's write an ovr (one-vs-rest) Logistic Regression class using gradient descent\n#We would then going to map Severity column to only two classes and apply the function\nclass LogisticRegressionGD(object):\n    def __init__(self, eta = 0.1, n_iter = 30, random_state = 1):\n        '''\n        eta: learning rate\n        n_iter: number of iterations\n        '''\n        self.w_ = []\n        self.random_state = random_state\n        self.cost_ = []\n        self.eta = eta\n        self.n_iter = n_iter\n        \n    \n    def fit(self, X, y):\n        '''\n        X(n_samples, n_features) numpy array\n        y(n_samples,)\n        '''\n        X = np.asarray(X)\n        y = np.asarray(y)\n        \n        self._initialize_w(X)\n        \n        for _ in range(self.n_iter):\n            net_input = self._net_input(X)\n            activation = self._activation(net_input)\n            \n            #if we forget about regularization...\n            cost = np.sum(-y * np.log(activation).ravel() - (1 - y) * np.log(1 - activation).ravel())\n            self.cost_.append(cost)\n\n            self.w_[1:] += (self.eta * X.T @ (y.reshape(-1, 1) - activation)).ravel()\n            self.w_[0] += self.eta * (y - activation.ravel()).sum()\n            \n        return self\n    \n    def _initialize_w(self, X):\n        rgen = np.random.RandomState(self.random_state)\n        self.w_ = rgen.normal(loc = 0.0, scale = 0.1, size = 1 + X.shape[1])\n        \n    def _net_input(self, X):\n        return X @ self.w_[1:].reshape(-1, 1) + self.w_[0]\n    \n    def _activation(self, z):\n        return 1 \/ (1 + np.exp(-np.clip(z, -35, 35)))\n        \n    def predict(self, X):\n        X = np.asarray(X)\n        return np.where(self._net_input(X) >= 0, 1, 0)","cabf34de":"np.unique(y)","0d84e2b3":"#make our target - to predict whether Severity is 4 or not\ny_dual_train = y_train.isin([4]).astype(int) \ny_dual_test = y_test.isin([4]).astype(int)","cfe4ca00":"eta = 0.00001\nn_iter = 50\nlr = LogisticRegressionGD(eta = eta, n_iter = n_iter)\nlr.fit(X_train_std, y_dual_train)","db61b51a":"plt.figure(figsize = (8, 6))\nplt.plot(lr.cost_)\nplt.xlabel('n_iter')\nplt.ylabel('cost')\nplt.title('Cost vs. N_iter')","d63ec1ed":"#Let's do some predictions\ny_dual_pred = lr.predict(X_test_std)","cab62280":"y_dual_actual = y_dual_test.reset_index(drop = True).rename('Actual')\ny_dual_pred = pd.Series(y_dual_pred.ravel(), name = 'Predicted')","423f137a":"pd.crosstab(y_dual_actual, y_dual_pred)","27a5bb55":"from sklearn.metrics import f1_score\nf1_score(y_dual_actual, y_dual_pred)","4f717f3c":"#Let's try using the package on Logistic Regression and how see how it performs multi-class wise\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(solver = 'lbfgs', multi_class = 'ovr', max_iter = 1000, n_jobs=-1)\nlr.fit(X_train_std, y_train)\ny_pred = lr.predict(X_test_std)","fed7b3c5":"y_actual = y_test.reset_index(drop = True).rename('Actual')\ny_pred = pd.Series(y_pred.ravel(), name = 'Predicted')\npd.crosstab(y_actual, y_pred)","d21a1881":"#Get F-score\nf1_score(y_actual, y_pred, average = 'weighted')","40380037":"Another use of time is that we can divide time into different buckets, ex. Morning, Afternoon, Evening; because we have Start_Time and End_Time, I just use the Start_Time","4259f77a":"# US - Accidents Analysis","aed9d2c5":"MapQuest stands out as the major source of this accident data","c07e9224":"### Let's say if we would like to develop a model to predict the severity of the accidents","699af2bd":"Acknowledgements\n\nMoosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, and Rajiv Ramnath. \u201cA Countrywide Traffic Accident Dataset.\u201d, 2019.\n\nMoosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, Radu Teodorescu, and Rajiv Ramnath. \"Accident Risk Prediction based on Heterogeneous Sparse Data: New Dataset and Insights.\" In proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, ACM, 2019.","8fcc7e5c":"-------------------------------------------------------------------------------------------","7ad4ffe5":"Imagine overlapping the kde plot with the map of the United States, it seems many accidents took place on west coast and east coast (with some down south also), which makes sense","2a49a0e7":"Apparently Severity 2 has the most instances, that fits our expectation that most accidents' severities are in the middle","c97ef7c6":"Most accidents took place because of clear sky? Or it is just clear sky most likely to take place?...","c20b8108":"Note that severity 0 and 1 were not being predicted at all because of the small # of instances comparing to other classes","e6ab6f35":"It seems Logistic Regression model might not be a good fit for Severity classification using the processed data","373dca06":"Some columns are perfect while others have some missing data, we can deal with the missing data later","f942f137":"#### Additional visualizations showing different data properties\/relationships","f2a90b82":"#### from first glance, the results are not that bad, remember that class 1 is severity 4 and class 0 is other severities\nNow we would like to calculate F-score to see how we did because our target has unbalanced # of examples between classes","aac28a62":"So our F-score is quite low actually if we are interested in severity 4 vs. rest"}}