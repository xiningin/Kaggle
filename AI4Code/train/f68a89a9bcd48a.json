{"cell_type":{"3ab5e578":"code","b9d35444":"code","f3210f3e":"code","d1c85840":"code","002115f4":"code","4a1985bc":"code","923a41a7":"code","15ba8eb9":"code","5267aaf8":"code","d1dd80fc":"code","9fdb50b8":"code","0bd5d9f7":"code","6515da93":"code","6e4f6d60":"code","4a370bbf":"code","647b2dd8":"code","a362d925":"code","27f97e82":"code","3f9a3acd":"code","33690eb1":"code","eec7197b":"code","ffa576a5":"code","117e6494":"code","692b05c8":"code","c08f3aef":"code","88c335c2":"code","9446eb11":"code","0803d240":"code","e4bc596e":"code","e7cfe26b":"code","66a5df58":"code","feb6defa":"code","02cde598":"code","17ea3f4f":"code","b1818759":"code","8c38044e":"code","0ff5121e":"code","2a152766":"code","14157217":"code","6fc29e40":"code","676e4fe4":"code","a6483872":"code","63dd6438":"code","4cfc7927":"code","8c50f396":"code","35ec0084":"code","0716e6f2":"code","1db6f681":"code","ab70625a":"code","fc53be86":"markdown","8fbd6fd3":"markdown","eb5a60ac":"markdown","76a30cc3":"markdown","2fa9a738":"markdown","60f0da56":"markdown","d87b1e70":"markdown","839a8fe6":"markdown","6e5921fe":"markdown","899cb54d":"markdown","bf34cd19":"markdown","f4e5fcda":"markdown","b1342f9b":"markdown","77c36d76":"markdown","14685b53":"markdown","201c57de":"markdown","42310810":"markdown","87bee9ea":"markdown","89abcb62":"markdown","fc2b0b60":"markdown","7b684653":"markdown","d458b2c2":"markdown","0009f501":"markdown","eb955574":"markdown","fa5238a9":"markdown","003c93ac":"markdown","e8da2f03":"markdown","3305b3c3":"markdown","c0f5649f":"markdown","f11adb85":"markdown","a9ce678b":"markdown"},"source":{"3ab5e578":"import os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport warnings\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_validate, GridSearchCV, StratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import (MinMaxScaler, StandardScaler, FunctionTransformer, \n                                   PolynomialFeatures, PowerTransformer, OneHotEncoder)\n\nwarnings.filterwarnings(\"ignore\")","b9d35444":"PATH = \"\/kaggle\/input\/titanic\"\nTRAIN_FILE = \"train.csv\"\nTEST_FILE = \"test.csv\"","f3210f3e":"df_train = pd.read_csv(os.path.join(PATH, TRAIN_FILE))\ndf_test = pd.read_csv(os.path.join(PATH, TEST_FILE))","d1c85840":"print(f\"Train File: # columns = {df_train.shape[1]}, # rows = {df_train.shape[0]}\")\nprint(f\"Test File: # columns = {df_test.shape[1]}, # rows = {df_test.shape[0]}\")","002115f4":"df_train.info()","4a1985bc":"print(f\"As we can see from the previous cell we have {df_train.select_dtypes(int).shape[1]} int columns, \"\n     f\"{df_train.select_dtypes(float).shape[1]} float columns and \"\n     f\"{df_train.select_dtypes(object).shape[1]} str columns.\")","923a41a7":"series_null = df_train.isnull().sum()\nseries_null[series_null > 0]","15ba8eb9":"x_features = [\"Age\", \"Fare\"]\ny_target = [\"Survived\"]","5267aaf8":"df_simple = df_train[x_features + y_target]\ndf_simple.shape","d1dd80fc":"df_simple.head(2)","9fdb50b8":"sns.countplot(x=\"Survived\", data=df_simple);","0bd5d9f7":"f, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw={\"height_ratios\": (.15, .85)})\n \nsns.boxplot(df_simple[\"Fare\"], ax=ax_box)\nsns.histplot(data=df_simple, x=\"Fare\", ax=ax_hist, bins=100)\n \nax_box.set(xlabel='')\nplt.show()","6515da93":"df_simple[\"Fare\"].describe()","6e4f6d60":"f, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw={\"height_ratios\": (.15, .85)})\n \nsns.boxplot(df_simple[\"Age\"], ax=ax_box)\nsns.histplot(data=df_simple, x=\"Age\", ax=ax_hist, bins=70)\n \nax_box.set(xlabel='')\nplt.show()","4a370bbf":"df_simple[\"Age\"].describe()","647b2dd8":"df_simple[\"Age\"] = df_simple[\"Age\"].apply(np.floor)\ndf_simple[\"Age\"].describe()","a362d925":"print(f\"From the null analysis we have seen that \\'Age\\' has a \"\n      f\"{round(100 * df_simple['Age'].isnull().sum() \/ df_simple.shape[0], 2)} % of null values.\")","27f97e82":"scorings = [\"f1\", \"precision\"]\nsimple_imputer = SimpleImputer(strategy='mean')\nsimple_model = LogisticRegression()\nsimple_pipe = Pipeline(steps=[\n    (\"simple_imputer\", simple_imputer),\n    (\"model\", simple_model)\n])\nstf_kfold = StratifiedKFold(n_splits=6)\ncv_values = cross_validate(simple_pipe, df_simple[x_features], df_simple[y_target], scoring=scorings,\n                           cv=stf_kfold, return_train_score=True)\nfor scoring in scorings:\n    print(\"-\"*5, scoring, \"-\"*5)\n    print(f\"Train set: mean={round(np.mean(cv_values.get(f'train_{scoring}')), 4)}, \"\n          f\"std={round(np.std(cv_values.get(f'train_{scoring}')), 4)}\")\n    print(f\"Dev set: mean={round(np.mean(cv_values.get(f'test_{scoring}')), 4)}, \"\n          f\"std={round(np.std(cv_values.get(f'test_{scoring}')), 4)}\")\n    print(\"\\n\")","3f9a3acd":"scaler = MinMaxScaler((0, 100))\nct = ColumnTransformer([\n    (\"scaler\", scaler, [1])  # <- This 1 correspond with the column Fare.\n])\n\nscorings = [\"f1\", \"precision\", \"recall\"]\nsimple_imputer = SimpleImputer(strategy='mean')\nsimple_model = LogisticRegression()\nsimple_pipe = Pipeline(steps=[\n    (\"simple_imputer\", simple_imputer),\n    (\"transformer\", ct),\n    (\"model\", simple_model)\n])\nstf_kfold = StratifiedKFold(n_splits=6)\ncv_values = cross_validate(simple_pipe, df_simple[x_features], df_simple[y_target], scoring=scorings,\n                           cv=stf_kfold, return_train_score=True)\nfor scoring in scorings:\n    print(\"-\"*5, scoring, \"-\"*5)\n    print(f\"Train set: mean={round(np.mean(cv_values.get(f'train_{scoring}')), 4)}, \"\n          f\"std={round(np.std(cv_values.get(f'train_{scoring}')), 4)}\")\n    print(f\"Dev set: mean={round(np.mean(cv_values.get(f'test_{scoring}')), 4)}, \"\n          f\"std={round(np.std(cv_values.get(f'test_{scoring}')), 4)}\")\n    print(\"\\n\")","33690eb1":"scaler = MinMaxScaler((0, 100))\nyeo = PowerTransformer()\nct = ColumnTransformer([\n    (\"scaler\", scaler, [1]),  # <- This 1 correspond with the column Fare.\n    (\"box\", yeo, [1])  # <- This 1 correspond with the column Fare.\n])\n\nscorings = [\"f1\", \"precision\", \"recall\"]\nsimple_imputer = SimpleImputer(strategy='mean')\nsimple_model = LogisticRegression()\nsimple_pipe = Pipeline(steps=[\n    (\"simple_imputer\", simple_imputer),\n    (\"transformer\", ct),\n    (\"model\", simple_model)\n])\nstf_kfold = StratifiedKFold(n_splits=6)\ncv_values = cross_validate(simple_pipe, df_simple[x_features], df_simple[y_target], scoring=scorings,\n                           cv=stf_kfold, return_train_score=True)\nfor scoring in scorings:\n    print(\"-\"*5, scoring, \"-\"*5)\n    print(f\"Train set: mean={round(np.mean(cv_values.get(f'train_{scoring}')), 4)}, \"\n          f\"std={round(np.std(cv_values.get(f'train_{scoring}')), 4)}\")\n    print(f\"Dev set: mean={round(np.mean(cv_values.get(f'test_{scoring}')), 4)}, \"\n          f\"std={round(np.std(cv_values.get(f'test_{scoring}')), 4)}\")\n    print(\"\\n\")","eec7197b":"scaler = MinMaxScaler((0, 100))\nyeo = PowerTransformer()\nct = ColumnTransformer([\n    (\"scaler\", scaler, [1]),  # <- This 1 correspond with the column Fare.\n    (\"box\", yeo, [1])  # <- This 1 correspond with the column Fare.\n])\n\nscorings = [\"f1\", \"precision\", \"recall\"]\nsimple_imputer = SimpleImputer(strategy='mean')\nsimple_model = LogisticRegression()\npoly_feat = PolynomialFeatures(4)\nsimple_pipe = Pipeline(steps=[\n    (\"simple_imputer\", simple_imputer),\n    (\"transformer\", ct),\n    (\"poly\", poly_feat),\n    (\"model\", simple_model)\n])\nstf_kfold = StratifiedKFold(n_splits=6)\ncv_values = cross_validate(simple_pipe, df_simple[x_features], df_simple[y_target], scoring=scorings,\n                           cv=stf_kfold, return_train_score=True)\nfor scoring in scorings:\n    print(\"-\"*5, scoring, \"-\"*5)\n    print(f\"Train set: mean={round(np.mean(cv_values.get(f'train_{scoring}')), 4)}, \"\n          f\"std={round(np.std(cv_values.get(f'train_{scoring}')), 4)}\")\n    print(f\"Dev set: mean={round(np.mean(cv_values.get(f'test_{scoring}')), 4)}, \"\n          f\"std={round(np.std(cv_values.get(f'test_{scoring}')), 4)}\")\n    print(\"\\n\")","ffa576a5":"scorings = [\"f1\", \"precision\", \"recall\"]\nsimple_imputer = SimpleImputer(strategy='mean')\nsimple_model = RandomForestClassifier()\nsimple_pipe = Pipeline(steps=[\n    (\"simple_imputer\", simple_imputer),\n    (\"model\", simple_model)\n])\nstf_kfold = StratifiedKFold(n_splits=6)\ncv_values = cross_validate(simple_pipe, df_simple[x_features], df_simple[y_target], scoring=scorings,\n                           cv=stf_kfold, return_train_score=True)\nfor scoring in scorings:\n    print(\"-\"*5, scoring, \"-\"*5)\n    print(f\"Train set: mean={round(np.mean(cv_values.get(f'train_{scoring}')), 4)}, \"\n          f\"std={round(np.std(cv_values.get(f'train_{scoring}')), 4)}\")\n    print(f\"Dev set: mean={round(np.mean(cv_values.get(f'test_{scoring}')), 4)}, \"\n          f\"std={round(np.std(cv_values.get(f'test_{scoring}')), 4)}\")\n    print(\"\\n\")","117e6494":"df_train.head(2)","692b05c8":"df_train[\"Cabin\"].value_counts(normalize=True, dropna=False)","c08f3aef":"df_train[\"Embarked\"].value_counts(normalize=True, dropna=False)","88c335c2":"sns.countplot(x=\"Pclass\", data=df_train);","9446eb11":"sns.countplot(x=\"Sex\", data=df_train);","0803d240":"df_aux = df_train.groupby([\"Sex\", \"Survived\"], as_index=False)[\"PassengerId\"].count()\ndf_aux.rename({\"PassengerId\": \"Count(%)\"}, inplace=True, axis=1)\ndf_aux[\"Count(%)\"] = 100 * df_aux[\"Count(%)\"] \/ df_train.shape[0]\nsns.barplot(data=df_aux, x=\"Survived\", y=\"Count(%)\", hue=\"Sex\");","e4bc596e":"title_list = [\n    \"Mrs\", \"Mr\", \"Master\", \"Miss\", \"Major\", \"Rev\", \"Don\",\n    \"Dr\", \"Ms\", \"Mlle\", \"Col\", \"Capt\", \"Mme\", \"Countess\", \"Jonkheer\"\n]","e7cfe26b":"df_train[\"Title\"] = df_train[\"Name\"].str.extract(fr\"({'|'.join(title_list)})\")","66a5df58":"df_train[\"Title\"].nunique()","feb6defa":"def replace_titles(x):\n    title=x['Title']\n    if title in ['Don', 'Major', 'Capt', 'Jonkheer', 'Rev', 'Col']:\n        return 'Mr'\n    elif title in ['Countess', 'Mme']:\n        return 'Mrs'\n    elif title in ['Mlle', 'Ms']:\n        return 'Miss'\n    elif title =='Dr':\n        if x['Sex']=='Male':\n            return 'Mr'\n        else:\n            return 'Mrs'\n    else:\n        return title\n    \ndf_train['Title']=df_train.apply(replace_titles, axis=1)\ndf_train[\"Title\"].nunique()","02cde598":"sns.countplot(x=\"Title\", data=df_train);","17ea3f4f":"df_train[\"FamilySize\"] = df_train[\"SibSp\"] + df_train[\"Parch\"]\ndf_train[\"FamilySize\"].describe()","b1818759":"X_feat = [\n    \"Pclass\", \"Title\", \"Sex\", \"Age\", \"SibSp\", \n    \"Parch\", \"FamilySize\", \"Fare\", \"Embarked\"\n]\nY_target = [\n    \"Survived\"\n]","8c38044e":"df = df_train[X_feat + Y_target]","0ff5121e":"df.describe(include=\"all\")","2a152766":"# LogReg\n\nscaler = MinMaxScaler((0, 10))\nyeo = PowerTransformer()\nmean_imputer = SimpleImputer(strategy=\"mean\")\nmost_frequent_imputer = SimpleImputer(strategy=\"most_frequent\")\none_hot = OneHotEncoder()\n\nage_pipe = Pipeline(steps=[\n    (\"mean_imputer\", mean_imputer),\n    (\"scaler_age\", scaler)\n])\nfare_pipe = Pipeline(steps=[\n    (\"fare_scaler\", scaler),\n    (\"yeo\", yeo)\n])\nprocessor = ColumnTransformer(\n    transformers=[\n        (\"age_pipe\", age_pipe, [\"Age\"]),\n        (\"fare_pipe\", fare_pipe, [\"Fare\"]),\n        ('one_hot', one_hot, [\"Embarked\", \"Title\", \"Sex\"])]\n)\n\nscorings = [\"f1\", \"precision\", \"recall\"]\nmodel = LogisticRegression()\nsimple_pipe = Pipeline(steps=[\n    (\"processor\", processor),\n    (\"model\", model)\n])\nstf_kfold = StratifiedKFold(n_splits=6)\ncv_values = cross_validate(simple_pipe, df[X_feat], df[Y_target], scoring=scorings,\n                           cv=stf_kfold, return_train_score=True)\nfor scoring in scorings:\n    print(\"-\"*5, scoring, \"-\"*5)\n    print(f\"Train set: mean={round(np.mean(cv_values.get(f'train_{scoring}')), 4)}, \"\n          f\"std={round(np.std(cv_values.get(f'train_{scoring}')), 4)}\")\n    print(f\"Dev set: mean={round(np.mean(cv_values.get(f'test_{scoring}')), 4)}, \"\n          f\"std={round(np.std(cv_values.get(f'test_{scoring}')), 4)}\")\n    print(\"\\n\")","14157217":"# Ridge\n\nscaler = MinMaxScaler((0, 10))\nyeo = PowerTransformer()\nmean_imputer = SimpleImputer(strategy=\"mean\")\nmost_frequent_imputer = SimpleImputer(strategy=\"most_frequent\")\none_hot = OneHotEncoder()\n\nage_pipe = Pipeline(steps=[\n    (\"mean_imputer\", mean_imputer),\n    (\"scaler_age\", scaler)\n])\nfare_pipe = Pipeline(steps=[\n    (\"fare_scaler\", scaler),\n    (\"yeo\", yeo)\n])\nprocessor = ColumnTransformer(\n    transformers=[\n        (\"age_pipe\", age_pipe, [\"Age\"]),\n        (\"fare_pipe\", fare_pipe, [\"Fare\"]),\n        ('one_hot', one_hot, [\"Embarked\", \"Title\", \"Sex\", \"Pclass\"])]\n)\n\nscorings = [\"f1\", \"precision\", \"recall\"]\nmodel = RidgeClassifier()\nsimple_pipe = Pipeline(steps=[\n    (\"processor\", processor),\n    (\"model\", model)\n])\nstf_kfold = StratifiedKFold(n_splits=6)\ncv_values = cross_validate(simple_pipe, df[X_feat], df[Y_target], scoring=scorings,\n                           cv=stf_kfold, return_train_score=True)\nfor scoring in scorings:\n    print(\"-\"*5, scoring, \"-\"*5)\n    print(f\"Train set: mean={round(np.mean(cv_values.get(f'train_{scoring}')), 4)}, \"\n          f\"std={round(np.std(cv_values.get(f'train_{scoring}')), 4)}\")\n    print(f\"Dev set: mean={round(np.mean(cv_values.get(f'test_{scoring}')), 4)}, \"\n          f\"std={round(np.std(cv_values.get(f'test_{scoring}')), 4)}\")\n    print(\"\\n\")","6fc29e40":"# SVM\n\nscaler = MinMaxScaler((0, 10))\nyeo = PowerTransformer()\nmean_imputer = SimpleImputer(strategy=\"mean\")\nmost_frequent_imputer = SimpleImputer(strategy=\"most_frequent\")\none_hot = OneHotEncoder()\n\nage_pipe = Pipeline(steps=[\n    (\"mean_imputer\", mean_imputer),\n    (\"scaler_age\", scaler)\n])\nfare_pipe = Pipeline(steps=[\n    (\"fare_scaler\", scaler),\n    (\"yeo\", yeo)\n])\nprocessor = ColumnTransformer(\n    transformers=[\n        (\"age_pipe\", age_pipe, [\"Age\"]),\n        (\"fare_pipe\", fare_pipe, [\"Fare\"]),\n        (\"one_hot\", one_hot, [\"Embarked\", \"Title\", \"Sex\", \"Pclass\"])]\n)\n\nscorings = [\"f1\", \"precision\", \"recall\"]\nmodel = SVC()\nsvm_pipe = Pipeline(steps=[\n    (\"processor\", processor),\n    (\"model\", model)\n])\nstf_kfold = StratifiedKFold(n_splits=6)\ncv_values = cross_validate(svm_pipe, df[X_feat], df[Y_target], scoring=scorings,\n                           cv=stf_kfold, return_train_score=True)\nfor scoring in scorings:\n    print(\"-\"*5, scoring, \"-\"*5)\n    print(f\"Train set: mean={round(np.mean(cv_values.get(f'train_{scoring}')), 4)}, \"\n          f\"std={round(np.std(cv_values.get(f'train_{scoring}')), 4)}\")\n    print(f\"Dev set: mean={round(np.mean(cv_values.get(f'test_{scoring}')), 4)}, \"\n          f\"std={round(np.std(cv_values.get(f'test_{scoring}')), 4)}\")\n    print(\"\\n\")","676e4fe4":"# RandomForest\n\nyeo = PowerTransformer()\nmean_imputer = SimpleImputer(strategy=\"mean\")\nmost_frequent_imputer = SimpleImputer(strategy=\"most_frequent\")\none_hot = OneHotEncoder()\n\nage_pipe = Pipeline(steps=[\n    (\"mean_imputer\", mean_imputer),\n])\nfare_pipe = Pipeline(steps=[\n    (\"yeo\", yeo)\n])\nprocessor = ColumnTransformer(\n    transformers=[\n        (\"age_pipe\", age_pipe, [\"Age\"]),\n        (\"fare_pipe\", fare_pipe, [\"Fare\"]),\n        ('one_hot', one_hot, [\"Embarked\", \"Title\", \"Sex\", \"Pclass\"])]\n)\n\nscorings = [\"f1\", \"precision\", \"recall\"]\nmodel = RandomForestClassifier()\nrf_pipe = Pipeline(steps=[\n    (\"processor\", processor),\n    (\"model\", model)\n])\nstf_kfold = StratifiedKFold(n_splits=6)\ncv_values = cross_validate(rf_pipe, df[X_feat], df[Y_target], scoring=scorings,\n                           cv=stf_kfold, return_train_score=True)\nfor scoring in scorings:\n    print(\"-\"*5, scoring, \"-\"*5)\n    print(f\"Train set: mean={round(np.mean(cv_values.get(f'train_{scoring}')), 4)}, \"\n          f\"std={round(np.std(cv_values.get(f'train_{scoring}')), 4)}\")\n    print(f\"Dev set: mean={round(np.mean(cv_values.get(f'test_{scoring}')), 4)}, \"\n          f\"std={round(np.std(cv_values.get(f'test_{scoring}')), 4)}\")\n    print(\"\\n\")","a6483872":"svm_param_grid = {\n    \"model__C\": [1, .5, .1, .01],\n    \"model__kernel\": [\"rbf\", \"poly\", \"sigmoid\"],\n    \"model__degree\": [2, 3, 4]\n}\n\nrf_param_grid = {\n    \"model__n_estimators\": [10, 100],\n    \"model__criterion\": [\"gini\", \"entropy\"],\n    \"model__max_depth\": [3, 5, 10]\n}","63dd6438":"# SVM\n\nscorings = \"precision\"\nstf_kfold = StratifiedKFold(n_splits=6)\nsvc_clf = GridSearchCV(svm_pipe, param_grid=svm_param_grid, n_jobs=-1, \n                       cv=stf_kfold, return_train_score=True, scoring=scorings)\nsvc_clf.fit(df[X_feat], df[Y_target]);","4cfc7927":"# SVM Summary\n\ndf_summary_svc = pd.DataFrame(svc_clf.cv_results_[\"params\"])\ndf_summary_svc[\"mean_train_score\"] = svc_clf.cv_results_[\"mean_train_score\"]\ndf_summary_svc[\"std_train_score\"] = svc_clf.cv_results_[\"std_train_score\"]\ndf_summary_svc[\"mean_test_score\"] = svc_clf.cv_results_[\"mean_test_score\"]\ndf_summary_svc[\"std_test_score\"] = svc_clf.cv_results_[\"std_test_score\"]\ndf_summary_svc[\"bias\"] = np.abs(df_summary_svc[\"mean_train_score\"] - df_summary_svc[\"mean_test_score\"])\ndf_summary_svc[df_summary_svc[\"mean_train_score\"] != 0].sort_values(by=[\"bias\", \"mean_train_score\", \"mean_test_score\"], ascending=[True, False, False])","8c50f396":"# RandomForest\n\nscorings = \"precision\"\nstf_kfold = StratifiedKFold(n_splits=6)\nrf_clf = GridSearchCV(rf_pipe, param_grid=rf_param_grid, n_jobs=-1, \n                       cv=stf_kfold, return_train_score=True, scoring=scorings)\nrf_clf.fit(df[X_feat], df[Y_target]);","35ec0084":"# RandomForest Summary\n\ndf_summary_rf = pd.DataFrame(rf_clf.cv_results_[\"params\"])\ndf_summary_rf[\"mean_train_score\"] = rf_clf.cv_results_[\"mean_train_score\"]\ndf_summary_rf[\"std_train_score\"] = rf_clf.cv_results_[\"std_train_score\"]\ndf_summary_rf[\"mean_test_score\"] = rf_clf.cv_results_[\"mean_test_score\"]\ndf_summary_rf[\"std_test_score\"] = rf_clf.cv_results_[\"std_test_score\"]\ndf_summary_rf[\"bias\"] = np.abs(df_summary_rf[\"mean_train_score\"] - df_summary_rf[\"mean_test_score\"])\ndf_summary_rf.sort_values(by=[\"bias\", \"mean_train_score\", \"mean_test_score\"], ascending=[True, False, False])","0716e6f2":"df_test[\"FamilySize\"] = df_test[\"SibSp\"] + df_test[\"Parch\"]  # <- Create FamilySize column in test set.\ndf_test[\"Title\"] = df_test[\"Name\"].str.extract(fr\"({'|'.join(title_list)})\")  # <- This a the following one create Title column\ndf_test['Title']=df_test.apply(replace_titles, axis=1)","1db6f681":"# RandomForest\n\nyeo = PowerTransformer()\nmean_imputer = SimpleImputer(strategy=\"mean\")\nmost_frequent_imputer = SimpleImputer(strategy=\"most_frequent\")\none_hot = OneHotEncoder()\n\nage_pipe = Pipeline(steps=[\n    (\"mean_imputer\", mean_imputer),\n])\nfare_pipe = Pipeline(steps=[\n    (\"mean_imputer\", mean_imputer),\n    (\"yeo\", yeo)\n])\nprocessor = ColumnTransformer(\n    transformers=[\n        (\"age_pipe\", age_pipe, [\"Age\"]),\n        (\"fare_pipe\", fare_pipe, [\"Fare\"]),\n        ('one_hot', one_hot, [\"Embarked\", \"Title\", \"Sex\", \"Pclass\"])]\n)\n\nscorings = [\"f1\", \"precision\", \"recall\"]\nmodel = RandomForestClassifier(criterion=\"gini\", n_estimators=10, max_depth=5)\nrf_pipe = Pipeline(steps=[\n    (\"processor\", processor),\n    (\"model\", model)\n])\n\nrf_pipe.fit(df[X_feat], df[Y_target])\ndf_test[\"Survived\"] = rf_pipe.predict(df_test[X_feat])","ab70625a":"df_test[[\"PassengerId\", \"Survived\"]].to_csv(\"submission.csv\", index=False)","fc53be86":"We are going to change 'Sex' variable with an OneHotEncoder.\n\n'Name' column is meaningless however we can extract the title as we can see from [here](https:\/\/triangleinequality.wordpress.com\/2013\/09\/08\/basic-feature-engineering-with-the-titanic-data\/) before drop 'Name' column.","8fbd6fd3":"<a id = \"2.3.\"><\/a>\n<a href=\"#toc\"><p style=\"text-align:right;\" href=\"#toc\">Back To Table Of Contents<\/p><\/a> \n### 2.3. Hyperparameter Tunning","eb5a60ac":"From [here](https:\/\/triangleinequality.wordpress.com\/2013\/09\/08\/basic-feature-engineering-with-the-titanic-data\/) we can create a new 'FamilySize' column adding 'SibSp' and 'Parch'.","76a30cc3":"Let's build a simple model to use the metric as a baseline, and to iterate over it to improve the metric.","2fa9a738":"<a id = \"1.1.\"><\/a>\n<a href=\"#toc\"><p style=\"text-align:right;\" href=\"#toc\">Back To Table Of Contents<\/p><\/a> \n### 1.1. Read Data And Check Shape","60f0da56":"As we can see from [section 1.3.](#1.3.) we have an unbalanced target due to that we are going to use an stratified split of train and test set.","d87b1e70":"First of all let's apply a scaler to the column 'Fare' in order to have a similar domain with 'Age'.","839a8fe6":"<a id = \"toc\"><\/a>\n## Table Of Contents\n\n- [1. A Simple Model](#1.)\n    * [1.1. Read Data And Check Shape](#1.1.)\n    * [1.2. Types And Nulls On Train Data](#1.2.)\n    * [1.3. Exploratory Data Analysis I](#1.3.)\n    * [1.4. Model](#1.4.)\n    * [1.5. Improving The Model](#1.5.)\n- [2. Model With All Features](#2.)\n    * [2.1. Exploratory Data Analysis II](#2.1.)\n    * [2.2. Model](#2.2.)\n    * [2.3. Hyperparameter Tunning](#2.3.)\n- [3. Submission](#3.)","6e5921fe":"<a id = \"2.2.\"><\/a>\n<a href=\"#toc\"><p style=\"text-align:right;\" href=\"#toc\">Back To Table Of Contents<\/p><\/a> \n### 2.2. Model","899cb54d":"This last model seems to be the better in terms on training set, however it is quite overfitted because the difference between dev set is huge.","bf34cd19":"We have build a simple model that we are going to use to iterate over it so let's analyse the other features to add it to an other model.","f4e5fcda":"<a id = \"2.1.\"><\/a>\n<a href=\"#toc\"><p style=\"text-align:right;\" href=\"#toc\">Back To Table Of Contents<\/p><\/a> \n### 2.1. Exploratory Data Analysis II","b1342f9b":"# Hello World Titanic\n-----\n> *Firsts steps in kaggle*","77c36d76":"Well done! We have build a simple model, however this could be better...","14685b53":"So let's make our simple model just replacing null values by the mean and using the standard LogisticRegression","201c57de":"<a id = \"3.\"><\/a>\n<a href=\"#toc\"><p style=\"text-align:right;\" href=\"#toc\">Back To Table Of Contents<\/p><\/a> \n### 3. Submission","42310810":"<a id = \"1.5.\"><\/a>\n<a href=\"#toc\"><p style=\"text-align:right;\" href=\"#toc\">Back To Table Of Contents<\/p><\/a> \n### 1.5. Improving Simple Model","87bee9ea":"For the linear models we have use a MinMaxScaler between 0 and 10 to the features 'Age' and 'Fare', a PowerTransformer to 'Fare' column to transform this into a more gaussian feature and we have applied a OneHotEncoder to 'Embarked', 'Title', 'Sex' and 'Pclass'. In order to make an hyperparameter tunning let's take SVM and RandomForest.","89abcb62":"<a id = \"1.2.\"><\/a>\n<a href=\"#toc\"><p style=\"text-align:right;\" href=\"#toc\">Back To Table Of Contents<\/p><\/a> \n### 1.2. Types And Nulls On Train Data","fc2b0b60":"<a id = \"1.4.\"><\/a>\n<a href=\"#toc\"><p style=\"text-align:right;\" href=\"#toc\">Back To Table Of Contents<\/p><\/a> \n### 1.4. Model","7b684653":"<a id = \"2.\"><\/a>\n<a href=\"#toc\"><p style=\"text-align:right;\" href=\"#toc\">Back To Table Of Contents<\/p><\/a> \n### 2. Model With All Features","d458b2c2":"We are going to consider 'Age' column as an integer column so let's apply a floor.","0009f501":"Due to the 77.1 % of null values in 'Cabin' we are going to eliminate this feature; in contrast, for 'Embarked' we are going to imput the most frequent value.","eb955574":"other approach is to change the model for a RandomForest","fa5238a9":"We can see also from previous cell there are 3 columns with null values, 'Age', 'Cabin' and 'Embarked'.\n\n**As is mentioned before we are going to build a model in sake of simplicity by using only features 'Age' and 'Fare'.**","003c93ac":"Other thing we could try is to transform to a more Gaussian distribution 'Fare' column due to as we can see on the [section 1.3.](#1.3.) this feature is highly skewed.","e8da2f03":"<a id = \"1.\"><\/a>\n<a href=\"#toc\"><p style=\"text-align:right;\" href=\"#toc\">Back To Table Of Contents<\/p><\/a> \n## 1. A Simple Model","3305b3c3":"We have extract from the column 'Name' 4 nobility title where we can apply a OneHotEncoder.","c0f5649f":"Due to we have only two features we can try to add polynomial features","f11adb85":"As we can see from [section 1.2.](#1.2.) columns 'Cabin', 'Embarked' despite of 'Age' has null values so we have to face it (For the 'Age' column we are going to keep the process of mean imputation).","a9ce678b":"<a id = \"1.3.\"><\/a>\n<a href=\"#toc\"><p style=\"text-align:right;\" href=\"#toc\">Back To Table Of Contents<\/p><\/a> \n### 1.3. Exploratory Data Analysis I"}}