{"cell_type":{"dfa262cd":"code","1c5c5b22":"code","e4d3b9f6":"code","d104a2a3":"code","f0f1b81d":"code","05e50711":"code","9ab1c13f":"code","8417a208":"code","f0e18e17":"code","c74f96a3":"code","065175d7":"code","105a1fd6":"code","8dba3893":"code","50edc947":"code","e21c210e":"code","6bd7534a":"code","dffb2f1f":"code","e15264d2":"code","c62609c2":"code","cac0ae51":"code","ab792b57":"code","07f76aa7":"code","af09a860":"code","06ddcf48":"code","1c4403cb":"code","749c4ee7":"code","e355e156":"code","af8d2750":"code","f938a36b":"code","71b744cf":"code","5d953a4d":"code","4ee7bcf8":"code","b32b5779":"code","32cc0699":"code","73cc428c":"code","11a8ae2a":"code","822ab8ea":"code","07ca1043":"code","46b4955c":"code","55af545a":"code","c5c1edba":"code","e663ad37":"code","a1388de6":"code","a4664317":"code","8ab9cbe1":"code","080518ce":"code","28af0662":"code","a27a60db":"code","bd254784":"code","20b1eb35":"markdown","088adf0f":"markdown","becf94c1":"markdown","1a49d901":"markdown","b8074cbd":"markdown","bf33443b":"markdown","803ffd56":"markdown","0e4ba1be":"markdown","6a095ef9":"markdown","826fa175":"markdown","6e0af9c8":"markdown","46eb8241":"markdown","34d3f610":"markdown","59f65bf9":"markdown","1cef72c8":"markdown","c475fdbd":"markdown","406b473c":"markdown","40a8842e":"markdown","17062bb9":"markdown","5207088d":"markdown","24d3689e":"markdown","e38b1257":"markdown","b94934c4":"markdown","3dd418b4":"markdown","dec07fba":"markdown"},"source":{"dfa262cd":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nsns.set_style(\"whitegrid\")\nplt.style.use(\"fivethirtyeight\")\nprint(\"Done\")\n\n#what is sns.set_style('whitegrid')\n# what is plt.style.use('fivethirtyeight')","1c5c5b22":"USAhousing = pd.read_csv('\/kaggle\/input\/usa-housing\/USA_Housing.csv')\nUSAhousing.head()","e4d3b9f6":"USAhousing.info()","d104a2a3":"USAhousing.describe()","f0f1b81d":"USAhousing.columns","05e50711":"sns.pairplot(USAhousing)\n\n# what can we observe from the plot","9ab1c13f":"sns.distplot(USAhousing['Price'])\n# what can we understand from the plot\n# how to make the plot bigger","8417a208":"sns.heatmap(USAhousing.corr(), annot=True)","f0e18e17":"X = USAhousing[['Avg. Area Income', 'Avg. Area House Age', 'Avg. Area Number of Rooms',\n               'Avg. Area Number of Bedrooms', 'Area Population']]\ny = USAhousing['Price']","c74f96a3":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# 1. we use X_train and Y_train to build the model. That is model.fit(X_train, Y_train)\n# 2. The fit function gives us the coefficients and intercept of the model (function)\n# 3. To get the training accuracy, we pass X_train to the predict function. y_train_predict = model.predict(X_train). Then actual vakues are in Y_train and predicted \n#    values are in y_train_predict. By compraing these 2 in metrics, we get training accuracy. \n# 4. To get the testing accuracy, we pass X_test to the predict function. y_test_predict = model.predict(X_test). Then actual values are in Y_test and predicted \n#    values are in y_test_predict. By compraing these 2 in metrics, we get testing accuracy.","065175d7":"from sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\n\ndef cross_val(model):\n    pred = cross_val_score(model, X, y, cv=10)\n    return pred.mean()\n\ndef print_evaluate(true, predicted):  \n    mae = metrics.mean_absolute_error(true, predicted)\n    mse = metrics.mean_squared_error(true, predicted)\n    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n    r2_square = metrics.r2_score(true, predicted)\n    print('MAE:', mae)\n    print('MSE:', mse)\n    print('RMSE:', rmse)\n    print('R2 Square', r2_square)\n    print('__________________________________')\n    \ndef evaluate(true, predicted):\n    mae = metrics.mean_absolute_error(true, predicted)\n    mse = metrics.mean_squared_error(true, predicted)\n    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n    r2_square = metrics.r2_score(true, predicted)\n    return mae, mse, rmse, r2_square\n\n\n# what is cross_validation_score\n# R2 square Vs RMSE\n","105a1fd6":"X_train.head()","8dba3893":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\npipeline = Pipeline([('std_scalar', StandardScaler())])\n\n#what are pipelines \n# what are we doing in the above line\n\nX_train = pipeline.fit_transform(X_train)\nX_test = pipeline.transform(X_test)\n\n# what is the difference between transform and fit_transform\n\nX_train[0]","50edc947":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression(normalize=True)  # Why normalization = True\nlin_reg.fit(X_train,y_train)","e21c210e":"# print the intercept\nprint(lin_reg.intercept_)","6bd7534a":"coeff_df = pd.DataFrame(lin_reg.coef_, X.columns, columns=['Coefficient'])\ncoeff_df","dffb2f1f":"# According to the heat map, Avg. Area Income has highest corelation to the price. Here it says the otherway?","e15264d2":"pred = lin_reg.predict(X_test)","c62609c2":"pred[0]","cac0ae51":"plt.scatter(y_test, pred)\n#how to draw the regression line in the following plot","ab792b57":"sns.distplot((y_test - pred), bins=50);  # what does this tell?","07f76aa7":"test_pred = lin_reg.predict(X_test)\ntrain_pred = lin_reg.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","af09a860":"results_df = pd.DataFrame(data=[[\"Linear Regression\", *evaluate(y_test, test_pred) , cross_val(LinearRegression())]], \n                          columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\nresults_df\n\n# what is cross_vaidation here  cross_val(LinearRegression())?\n\n# how to interpret e values 1.006842e+10?","06ddcf48":"from sklearn.linear_model import RANSACRegressor\n\nmodel = RANSACRegressor(base_estimator=LinearRegression(), max_trials=100)\nmodel.fit(X_train, y_train)\n\ntest_pred = model.predict(X_test)\ntrain_pred = model.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","1c4403cb":"results_df_2 = pd.DataFrame(data=[[\"Robust Regression\", *evaluate(y_test, test_pred) , cross_val(RANSACRegressor())]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","749c4ee7":"results_df.MSE[1]","e355e156":"from sklearn.linear_model import Ridge\n\nmodel = Ridge(alpha=100, solver='cholesky', tol=0.0001, random_state=42)\nmodel.fit(X_train, y_train)\npred = model.predict(X_test)\n\ntest_pred = model.predict(X_test)\ntrain_pred = model.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","af8d2750":"results_df_2 = pd.DataFrame(data=[[\"Ridge Regression\", *evaluate(y_test, test_pred) , cross_val(Ridge())]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","f938a36b":"from sklearn.linear_model import Lasso\n\nmodel = Lasso(alpha=0.1, \n              precompute=True, \n#               warm_start=True, \n              positive=True, \n              selection='random',\n              random_state=42)\nmodel.fit(X_train, y_train)\n\ntest_pred = model.predict(X_test)\ntrain_pred = model.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","71b744cf":"results_df_2 = pd.DataFrame(data=[[\"Lasso Regression\", *evaluate(y_test, test_pred) , cross_val(Lasso())]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","5d953a4d":"from sklearn.linear_model import ElasticNet\n\nmodel = ElasticNet(alpha=0.1, l1_ratio=0.9, selection='random', random_state=42)\nmodel.fit(X_train, y_train)\n\ntest_pred = model.predict(X_test)\ntrain_pred = model.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","4ee7bcf8":"results_df_2 = pd.DataFrame(data=[[\"Elastic Net Regression\", *evaluate(y_test, test_pred) , cross_val(ElasticNet())]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","b32b5779":"from sklearn.preprocessing import PolynomialFeatures\n\npoly_reg = PolynomialFeatures(degree=2)\n\nX_train_2_d = poly_reg.fit_transform(X_train)\nX_test_2_d = poly_reg.transform(X_test)\n\nlin_reg = LinearRegression(normalize=True)\nlin_reg.fit(X_train_2_d,y_train)\n\ntest_pred = lin_reg.predict(X_test_2_d)\ntrain_pred = lin_reg.predict(X_train_2_d)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","32cc0699":"results_df_2 = pd.DataFrame(data=[[\"Polynomail Regression\", *evaluate(y_test, test_pred), 0]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","73cc428c":"from sklearn.linear_model import SGDRegressor\n\nsgd_reg = SGDRegressor(n_iter_no_change=250, penalty=None, eta0=0.0001, max_iter=100000)\nsgd_reg.fit(X_train, y_train)\n\ntest_pred = sgd_reg.predict(X_test)\ntrain_pred = sgd_reg.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","11a8ae2a":"results_df_2 = pd.DataFrame(data=[[\"Stochastic Gradient Descent\", *evaluate(y_test, test_pred), 0]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","822ab8ea":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, Dense, Activation, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\n# why do we need optimizers?\n\nX_train = np.array(X_train)\nX_test = np.array(X_test)\ny_train = np.array(y_train)\ny_test = np.array(y_test)\n\nmodel = Sequential()\n\nmodel.add(Dense(X_train.shape[1], activation='relu')) # The input layer should have only the number of independant columns?\nmodel.add(Dense(32, activation='relu'))\n# model.add(Dropout(0.2))\n\nmodel.add(Dense(64, activation='relu'))\n# model.add(Dropout(0.2))\n\nmodel.add(Dense(128, activation='relu'))\n# model.add(Dropout(0.2))\n\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(1)) # For a regression the output layer alway has only one layer.\n\nmodel.compile(optimizer=Adam(0.00001), loss='mse')  # what is 0.00001 here?\n\nr = model.fit(X_train, y_train,\n              validation_data=(X_test,y_test), # so here we are saying that use X_train, y_train to find the model and use X_test and y_test to validate the model?\n              batch_size=1, #what is batch_size?\n              epochs=100) # epochs are no. of learning iterations. in each epoch one forward and one back propagation hapens? \n\n# what elase hapens in the epoch at Neural network level?","07ca1043":"# what is loss and validation_loss. \n# 3 does it use validation_data in every epoch?","46b4955c":"plt.figure(figsize=(10, 6))\n\nplt.plot(r.history['loss'], label='loss')\nplt.plot(r.history['val_loss'], label='val_loss')\nplt.legend()\n","55af545a":"# what does this plot say?","c5c1edba":"test_pred = model.predict(X_test)\ntrain_pred = model.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\n\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","e663ad37":"results_df_2 = pd.DataFrame(data=[[\"Artficial Neural Network\", *evaluate(y_test, test_pred), 0]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","a1388de6":"# why Neural network performed very poorly?","a4664317":"from sklearn.ensemble import RandomForestRegressor\n\nrf_reg = RandomForestRegressor(n_estimators=1000)\nrf_reg.fit(X_train, y_train)\n\ntest_pred = rf_reg.predict(X_test)\ntrain_pred = rf_reg.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\n\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","8ab9cbe1":"results_df_2 = pd.DataFrame(data=[[\"Random Forest Regressor\", *evaluate(y_test, test_pred), 0]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","080518ce":"from sklearn.svm import SVR\n\nsvm_reg = SVR(kernel='rbf', C=1000000, epsilon=0.001)\nsvm_reg.fit(X_train, y_train)\n\ntest_pred = svm_reg.predict(X_test)\ntrain_pred = svm_reg.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\n\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","28af0662":"results_df_2 = pd.DataFrame(data=[[\"SVM Regressor\", *evaluate(y_test, test_pred), 0]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","a27a60db":"results_df.set_index('Model', inplace=True)\nresults_df['R2 Square'].plot(kind='barh', figsize=(12, 8))","bd254784":"# the above plot looks confusing","20b1eb35":"# \u2714\ufe0f Robust Regression\n\n> Robust regression is a form of regression analysis designed to overcome some limitations of traditional parametric and non-parametric methods. Robust regression methods are designed to be not overly affected by violations of assumptions by the underlying data-generating process.\n\n> One instance in which robust estimation should be considered is when there is a strong suspicion of `heteroscedasticity`.\n\n> A common situation in which robust estimation is used occurs when the data contain outliers. In the presence of outliers that do not come from the same data-generating process as the rest of the data, least squares estimation is inefficient and can be biased. Because the least squares predictions are dragged towards the outliers, and because the variance of the estimates is artificially inflated, the result is that outliers can be masked. (In many situations, including some areas of geostatistics and medical statistics, it is precisely the outliers that are of interest.)","088adf0f":"## \ud83e\uddf1 Train Test Split\n\nNow let's split the data into a training set and a testing set. We will train out model on the training set and then use the test set to evaluate the model.","becf94c1":"# \ud83d\udcca Exploratory Data Analysis (EDA)\n\nLet's create some simple plots to check out the data!","1a49d901":"> Interpreting the coefficients:\n- Holding all other features fixed, a 1 unit increase in **Avg. Area Income** is associated with an **increase of \\$21.52**.\n- Holding all other features fixed, a 1 unit increase in **Avg. Area House Age** is associated with an **increase of \\$164883.28**.\n- Holding all other features fixed, a 1 unit increase in **Avg. Area Number of Rooms** is associated with an **increase of \\$122368.67**.\n- Holding all other features fixed, a 1 unit increase in **Avg. Area Number of Bedrooms** is associated with an **increase of \\$2233.80**.\n- Holding all other features fixed, a 1 unit increase in **Area Population** is associated with an **increase of \\$15.15**.\n\nDoes this make sense? Probably not because I made up this data.","b8074cbd":"## Random Sample Consensus - RANSAC\n\n> Random sample consensus (`RANSAC`) is an iterative method to estimate parameters of a mathematical model from a set of observed data that contains outliers, when outliers are to be accorded no influence on the values of the estimates. Therefore, it also can be interpreted as an outlier detection method.\n\n> A basic assumption is that the data consists of \"inliers\", i.e., data whose distribution can be explained by some set of model parameters, though may be subject to noise, and \"outliers\" which are data that do not fit the model. The outliers can come, for example, from extreme values of the noise or from erroneous measurements or incorrect hypotheses about the interpretation of data. RANSAC also assumes that, given a (usually small) set of inliers, there exists a procedure which can estimate the parameters of a model that optimally explains or fits this data.","bf33443b":"# \ud83d\udcdd Summary\nIn this notebook you discovered the linear regression algorithm for machine learning.\n\nYou covered a lot of ground including:\n> - The common linear regression models (Ridge, Lasso, ElasticNet, ...).\n> - The representation used by the model.\n> - Learning algorithms used to estimate the coefficients in the model.\n> - Rules of thumb to consider when preparing data for use with linear regression.\n> - How to evaluate a linear regression model.\n\n\n# \ud83d\udd17 References:\n- [Scikit-learn library](https:\/\/scikit-learn.org\/stable\/supervised_learning.html#supervised-learning)\n- [Linear Regression for Machine Learning by Jason Brownlee PhD](https:\/\/machinelearningmastery.com\/linear-regression-for-machine-learning\/)","803ffd56":"# \ud83d\udce4 Import Libraries","0e4ba1be":"# \ud83d\udcca Models Comparison","6a095ef9":"# \ud83d\udcc8 Training a Linear Regression Model\n\n> Let's now begin to train out regression model! We will need to first split up our data into an X array that contains the features to train on, and a y array with the target variable, in this case the Price column. We will toss out the Address column because it only has text info that the linear regression model can't use.\n\n## X and y arrays","826fa175":"# \ud83d\udce6 Preparing Data For Linear Regression\n> Linear regression is been studied at great length, and there is a lot of literature on how your data must be structured to make best use of the model.\n\n> As such, there is a lot of sophistication when talking about these requirements and expectations which can be intimidating. In practice, you can uses these rules more as rules of thumb when using Ordinary Least Squares Regression, the most common implementation of linear regression.\n\n> Try different preparations of your data using these heuristics and see what works best for your problem.\n- **Linear Assumption.** Linear regression assumes that the relationship between your input and output is linear. It does not support anything else. This may be obvious, but it is good to remember when you have a lot of attributes. You may need to transform data to make the relationship linear (e.g. log transform for an exponential relationship).\n- **Remove Noise.** Linear regression assumes that your input and output variables are not noisy. Consider using data cleaning operations that let you better expose and clarify the signal in your data. This is most important for the output variable and you want to remove outliers in the output variable (y) if possible.\n- **Remove Collinearity.** Linear regression will over-fit your data when you have highly correlated input variables. Consider calculating pairwise correlations for your input data and removing the most correlated.\n- **Gaussian Distributions.** Linear regression will make more reliable predictions if your input and output variables have a Gaussian distribution. You may get some benefit using transforms (e.g. log or BoxCox) on you variables to make their distribution more Gaussian looking.\n- **Rescale Inputs:** Linear regression will often make more reliable predictions if you rescale input variables using standardization or normalization.","6e0af9c8":"# \u2714\ufe0f Random Forest Regressor","46eb8241":"## \u2714\ufe0f Model Evaluation\n\nLet's evaluate the model by checking out it's coefficients and how we can interpret them.","34d3f610":"# \u2714\ufe0f Polynomial Regression\n> Source: [scikit-learn](http:\/\/scikit-learn.org\/stable\/modules\/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions)\n\n***\n\n> One common pattern within machine learning is to use linear models trained on nonlinear functions of the data. This approach maintains the generally fast performance of linear methods, while allowing them to fit a much wider range of data.\n\n> For example, a simple linear regression can be extended by constructing polynomial features from the coefficients. In the standard linear regression case, you might have a model that looks like this for two-dimensional data:\n\n$$\\hat{y}(w, x) = w_0 + w_1 x_1 + w_2 x_2$$\n\n> If we want to fit a paraboloid to the data instead of a plane, we can combine the features in second-order polynomials, so that the model looks like this:\n\n$$\\hat{y}(w, x) = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_1 x_2 + w_4 x_1^2 + w_5 x_2^2$$\n\n> The (sometimes surprising) observation is that this is still a linear model: to see this, imagine creating a new variable\n\n$$z = [x_1, x_2, x_1 x_2, x_1^2, x_2^2]$$\n\n> With this re-labeling of the data, our problem can be written\n\n$$\\hat{y}(w, x) = w_0 + w_1 z_1 + w_2 z_2 + w_3 z_3 + w_4 z_4 + w_5 z_5$$\n\n> We see that the resulting polynomial regression is in the same class of linear models we\u2019d considered above (i.e. the model is linear in w) and can be solved by the same techniques. By considering linear fits within a higher-dimensional space built with these basis functions, the model has the flexibility to fit a much broader range of data.\n***","59f65bf9":"## \ud83d\udcbe Check out the Data","1cef72c8":"# \ud83d\udcc8 Linear Regression with Python\n\n> Linear Regression is the simplest algorithm in machine learning, it can be trained in different ways. In this notebook we will cover the following linear algorithms:\n\n> 1. Linear Regression\n> 2. Robust Regression\n> 3. Ridge Regression\n> 4. LASSO Regression\n> 5. Elastic Net\n> 6. Polynomial Regression\n> 7. Stochastic Gradient Descent\n> 8. Artificial Neaural Networks\n\n# \ud83d\udcbe Data\n\n> We are going to use the `USA_Housing` dataset. Since house price is a continues variable, this is a regression problem. The data contains the following columns:\n\n> * '`Avg. Area Income`': Avg. Income of residents of the city house is located in.\n> * '`Avg. Area House Age`': Avg Age of Houses in same city\n> * '`Avg. Area Number of Rooms`': Avg Number of Rooms for Houses in same city\n> * '`Avg. Area Number of Bedrooms`': Avg Number of Bedrooms for Houses in same city\n> * '`Area Population`': Population of city hou  se is located in\n> * '`Price`': Price that the house sold at\n> * '`Address`': Address for the house\n","c475fdbd":"**Residual Histogram**","406b473c":"# \u2714\ufe0f Linear Regression","40a8842e":"# \u2714\ufe0f Ridge Regression\n\n> Source: [scikit-learn](http:\/\/scikit-learn.org\/stable\/modules\/linear_model.html#ridge-regression)\n\n> Ridge regression addresses some of the problems of **Ordinary Least Squares** by imposing a penalty on the size of coefficients. The ridge coefficients minimize a penalized residual sum of squares,\n\n$$\\min_{w}\\big|\\big|Xw-y\\big|\\big|^2_2+\\alpha\\big|\\big|w\\big|\\big|^2_2$$\n\n> $\\alpha>=0$ is a complexity parameter that controls the amount of shrinkage: the larger the value of $\\alpha$, the greater the amount of shrinkage and thus the coefficients become more robust to collinearity.\n\n> Ridge regression is an L2 penalized model. Add the squared sum of the weights to the least-squares cost function.\n***","17062bb9":"# \u2714\ufe0f Artficial Neural Network","5207088d":"# \u2714\ufe0f Support Vector Machine","24d3689e":"# \u2714\ufe0f Stochastic Gradient Descent\n\n> Gradient Descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Sescent is to tweak parameters iteratively in order to minimize a cost function. Gradient Descent measures the local gradient of the error function with regards to the parameters vector, and it goes in the direction of descending gradient. Once the gradient is zero, you have reached a minimum.","e38b1257":"# \u2714\ufe0f Elastic Net\n\n> A linear regression model trained with L1 and L2 prior as regularizer. \n\n> This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge. \n\n> Elastic-net is useful when there are multiple features which are correlated with one another. Lasso is likely to pick one of these at random, while elastic-net is likely to pick both.\n\n> A practical advantage of trading-off between Lasso and Ridge is it allows Elastic-Net to inherit some of Ridge\u2019s stability under rotation.\n\n> The objective function to minimize is in this case\n\n$$\\min_{w}{\\frac{1}{2n_{samples}} \\big|\\big|X w - y\\big|\\big|_2 ^ 2 + \\alpha \\rho \\big|\\big|w\\big|\\big|_1 +\n\\frac{\\alpha(1-\\rho)}{2} \\big|\\big|w\\big|\\big|_2 ^ 2}$$\n***","b94934c4":"## \u2714\ufe0f Predictions from our Model\n\nLet's grab predictions off our test set and see how well it did!","3dd418b4":"## \u2714\ufe0f Regression Evaluation Metrics\n\n\nHere are three common evaluation metrics for regression problems:\n\n> - **Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n\n> - **Mean Squared Error** (MSE) is the mean of the squared errors:\n$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n\n> - **Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n\n> \ud83d\udccc Comparing these metrics:\n- **MAE** is the easiest to understand, because it's the average error.\n- **MSE** is more popular than MAE, because MSE \"punishes\" larger errors, which tends to be useful in the real world.\n- **RMSE** is even more popular than MSE, because RMSE is interpretable in the \"y\" units.\n\n> All of these are **loss functions**, because we want to minimize them.","dec07fba":"# \u2714\ufe0f LASSO Regression\n\n> A linear model that estimates sparse coefficients.\n\n> Mathematically, it consists of a linear model trained with $\\ell_1$ prior as regularizer. The objective function to minimize is:\n\n$$\\min_{w}\\frac{1}{2n_{samples}} \\big|\\big|Xw - y\\big|\\big|_2^2 + \\alpha \\big|\\big|w\\big|\\big|_1$$\n\n> The lasso estimate thus solves the minimization of the least-squares penalty with $\\alpha \\big|\\big|w\\big|\\big|_1$ added, where $\\alpha$ is a constant and $\\big|\\big|w\\big|\\big|_1$ is the $\\ell_1-norm$ of the parameter vector.\n***"}}