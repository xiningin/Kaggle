{"cell_type":{"06608653":"code","59fa8dcb":"code","e88b4688":"code","771558df":"code","2f9535e0":"code","a61e5e65":"code","a4a93520":"code","5ffc730e":"code","c8eab882":"code","911ed550":"code","5475e949":"code","58122ca6":"code","10dbb7a7":"code","43fa4c3a":"code","65e97e73":"code","7621dcc7":"code","82c035b5":"code","f82c83fe":"code","194df764":"code","446e39ac":"code","29479b5b":"code","9d7ee942":"code","dfdeaeef":"code","5a04f20d":"code","9988e6c5":"markdown","c28b1316":"markdown"},"source":{"06608653":"from typing import List, NoReturn, Union, Tuple, Optional, Text, Generic, Callable, Dict\n\nfrom sklearn.metrics import f1_score, cohen_kappa_score, mean_squared_error\nfrom logging import getLogger, Formatter, StreamHandler, FileHandler, INFO\nfrom scipy.signal import butter, lfilter,filtfilt,savgol_filter\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.model_selection import KFold, train_test_split\nfrom tsfresh.feature_extraction import feature_calculators\n\nfrom scipy.stats import pearsonr, spearmanr, kendalltau\nfrom sklearn.linear_model import LinearRegression\nfrom pandas_profiling import ProfileReport\nfrom tqdm import tqdm_notebook as tqdm\nfrom contextlib import contextmanager\nfrom joblib import Parallel, delayed\nfrom IPython.display import display\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport lightgbm as lgb\nimport xgboost as xgb\nimport seaborn as sns\nimport random as rn\nimport pandas as pd\nimport numpy as np\nimport scipy as sp\nimport itertools\nimport warnings\nimport librosa\nimport time\nimport pywt\nimport os\nimport gc\n\n\nwarnings.simplefilter('ignore')\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', 1000)\npd.set_option('display.max_rows', 500)\n%matplotlib inline","59fa8dcb":"BATCHSIZE = 50000\nSEED = 529\nSELECT = True\nSPLITS = 5\nfe_config = [\n    (True, True, 50000, None),\n    (False, False, 5000, None),\n]","e88b4688":"\ndef init_logger():\n    handler = StreamHandler()\n    handler.setLevel(INFO)\n    handler.setFormatter(Formatter(LOGFORMAT))\n    fh_handler = FileHandler('{}.log'.format(MODELNAME))\n    fh_handler.setFormatter(Formatter(LOGFORMAT))\n    logger.setLevel(INFO)\n    logger.addHandler(handler)\n    logger.addHandler(fh_handler)\n    ","771558df":"\n@contextmanager\ndef timer(name : Text):\n    t0 = time.time()\n    yield\n    logger.info(f'[{name}] done in {time.time() - t0:.0f} s')\n\nCOMPETITION = 'ION-Switching'\nlogger = getLogger(COMPETITION)\nLOGFORMAT = '%(asctime)s %(levelname)s %(message)s'\nMODELNAME = 'Baseline'\n","2f9535e0":"\ndef seed_everything(seed : int) -> NoReturn :\n    \n    rn.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nseed_everything(SEED)\n","a61e5e65":"\ndef read_data(base : os.path.abspath) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \n    train = pd.read_csv(os.path.join(base + '\/train.csv'), dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int8})\n    test  = pd.read_csv(os.path.join(base + '\/test.csv'), dtype={'time': np.float32, 'signal': np.float32})\n    sub  = pd.read_csv(os.path.join(base + '\/sample_submission.csv'), dtype={'time': np.float32})\n    \n    return train, test, sub\n\n\n","a4a93520":"\ndef batching(df : pd.DataFrame,\n             batch_size : int,\n             add_index : Optional[bool]=True) -> pd.DataFrame :\n    \n    df['batch_'+ str(batch_size)] = df.groupby(df.index\/\/batch_size, sort=False)['signal'].agg(['ngroup']).values + 1\n    df['batch_'+ str(batch_size)] = df['batch_'+ str(batch_size)].astype(np.uint16)\n    if add_index:\n        df['batch_' + str(batch_size) +'_idx'] = df.index  - (df['batch_'+ str(batch_size)] * batch_size)\n        df['batch_' + str(batch_size) +'_idx'] = df['batch_' + str(batch_size) +'_idx'].astype(np.uint16)\n        \n    return df\n","5ffc730e":"\ndef reduce_mem_usage(df: pd.DataFrame,\n                     verbose: bool = True) -> pd.DataFrame:\n    \n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n\n    for col in df.columns:\n        col_type = df[col].dtypes\n\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            if str(col_type)[:3] == 'int':\n                if (c_min > np.iinfo(np.int8).min\n                        and c_max < np.iinfo(np.int8).max):\n                    df[col] = df[col].astype(np.int8)\n                elif (c_min > np.iinfo(np.int16).min\n                      and c_max < np.iinfo(np.int16).max):\n                    df[col] = df[col].astype(np.int16)\n                elif (c_min > np.iinfo(np.int32).min\n                      and c_max < np.iinfo(np.int32).max):\n                    df[col] = df[col].astype(np.int32)\n                elif (c_min > np.iinfo(np.int64).min\n                      and c_max < np.iinfo(np.int64).max):\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (c_min > np.finfo(np.float16).min\n                        and c_max < np.finfo(np.float16).max):\n                    df[col] = df[col].astype(np.float16)\n                elif (c_min > np.finfo(np.float32).min\n                      and c_max < np.finfo(np.float32).max):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    reduction = (start_mem - end_mem) \/ start_mem\n\n    msg = f'Mem. usage decreased to {end_mem:5.2f} MB ({reduction * 100:.1f} % reduction)'\n    if verbose:\n        print(msg)\n\n    return df\n","c8eab882":"\ndef maddest(d : Union[np.array, pd.Series, List], axis : Optional[int]=None) -> np.array:  \n    return np.mean(np.absolute(d - np.mean(d, axis)), axis)\n","911ed550":"\ndef denoise_signal(x : Union[np.array, pd.Series],\n                   wavelet : Optional[Text]='db4',\n                   level : Optional[int]=1) -> np.array:\n    \n    coeff = pywt.wavedec(x, wavelet, mode=\"per\")\n    sigma = (1\/0.6745) * maddest(coeff[-level])\n    uthresh = sigma * np.sqrt(2*np.log(len(x)))\n    coeff[1:] = (pywt.threshold(i, value=uthresh, mode='hard') for i in coeff[1:])\n\n    return pywt.waverec(coeff, wavelet, mode='per')\n","5475e949":"\ndef denoise_signal_simple(x : Union[np.array, pd.Series],\n                          wavelet : Optional[Text]='db4',\n                          level : Optional[int]=1) -> np.array:\n    \n    coeff = pywt.wavedec(x, wavelet, mode=\"per\")\n    uthresh = 10\n    coeff[1:] = (pywt.threshold(i, value=uthresh, mode='hard') for i in coeff[1:])\n    \n    return pywt.waverec(coeff, wavelet, mode='per')\n","58122ca6":"\ndef trend(df : Union[pd.Series, np.array],\n          abs_values: Optional[bool]=False) -> float:\n    \n    idx = np.array(range(len(df)))\n    if abs_values:\n        df = np.abs(df)\n    lr = LinearRegression()\n    lr.fit(idx.reshape(-1, 1), df)\n    \n    return lr.coef_[0]\n","10dbb7a7":"\ndef change_rate(df : Union[pd.Series, np.array]) -> float:\n    \n    change = (np.diff(df) \/ df[:-1])\n    change = change[np.nonzero(change)[0]]\n    change = change[~np.isnan(change)]\n    change = change[change != -np.inf]\n    change = change[change != np.inf]\n    \n    return np.mean(change)\n","43fa4c3a":"\ndef lag_with_pct_change(df : pd.DataFrame,\n                        batch_size : int,\n                        shift_sizes : Optional[List]=[1, 2],\n                        add_pct_change : Optional[bool]=False,\n                        add_pct_change_lag : Optional[bool]=False) -> pd.DataFrame:\n    \n    assert 'batch_' + str(batch_size) +'_idx' in df.columns\n    for shift_size in shift_sizes:    \n        df['signal_shift_pos_'+str(shift_size)] = df['signal'].shift(shift_size).fillna(0)\n        df['signal_shift_neg_'+str(shift_size)] = df['signal'].shift(-1*shift_size).fillna(0)\n        for i in df[df['batch_' + str(batch_size) +'_idx'].isin(range(shift_size))].index:\n            df['signal_shift_pos_'+str(shift_size)][i] = np.nan\n        for i in df[df['batch_' + str(batch_size) +'_idx'].isin(range(batch_size - shift_size, batch_size))].index:\n            df['signal_shift_neg_'+str(shift_size)][i] = np.nan\n    if add_pct_change:\n        df['pct_change'] = df['signal'].pct_change()\n        if add_pct_change_lag:\n            df['pct_change_shift_pos_'+str(shift_size)] = df['pct_change'].shift(shift_size).fillna(0)\n            df['pct_change_shift_neg_'+str(shift_size)] = df['pct_change'].shift(-1*shift_size).fillna(0)\n            for i in df[df['batch_' + str(batch_size) +'_idx'].isin(range(shift_size))].index:\n                df['pct_change_shift_pos_'+str(shift_size)][i] = np.nan\n            for i in df[df['batch_' + str(batch_size) +'_idx'].isin(range(batch_size - shift_size, batch_size))].index:\n                df['pct_change_shift_neg_'+str(shift_size)][i] = np.nan \n    return df\n","65e97e73":"\ndef feature_enginering_by_batch(z : Union[pd.Series, np.array],\n                                batch_size : int,\n                                window_size : Optional[List]=None) -> pd.DataFrame:\n    \n    temp = pd.DataFrame(index=[0], dtype=np.float16)\n    \n    temp['mean'] = z.mean()\n    temp['max'] = z.max()\n    temp['min'] = z.min()\n    temp['std'] = z.std()  \n    temp['mean_abs_chg'] = np.mean(np.abs(np.diff(z)))\n    temp['abs_max'] = np.max(np.abs(z))\n    temp['abs_min'] = np.min(np.abs(z))\n    temp['range'] = temp['max'] - temp['min']\n    temp['max_to_min'] = temp['max'] \/ temp['min']\n    temp['abs_avg'] = (temp['abs_max'] + temp['abs_min']) \/ 2\n    \n    for i in range(1, 5): \n        temp[f'kstat_{i}'] = stats.kstat(z, i)\n\n    for i in range(2, 5):\n        temp[f'moment_{i}'] = stats.moment(z, i)\n\n    for i in [1, 2]:\n        temp[f'kstatvar_{i}'] = stats.kstatvar(z, i)\n    \n    if window_size is not None:\n        for window in window_size:\n            temp['percentile_roll_'+str(window)+'_std_25'] = np.percentile(pd.Series(z).rolling(window).std().dropna().values, 25)\n            temp['percentile_roll_'+str(window)+'_std_75'] = np.percentile(pd.Series(z).rolling(window).std().dropna().values, 75)\n            temp['percentile_roll_'+str(window)+'_std_05'] = np.percentile(pd.Series(z).rolling(window).std().dropna().values,  5)\n            temp['percentile_roll_'+str(window)+'_std_95'] = np.percentile(pd.Series(z).rolling(window).std().dropna().values, 95)\n            temp['percentile_roll_'+str(window)+'_mean_25'] = np.percentile(pd.Series(z).rolling(window).mean().dropna().values, 25)\n            temp['percentile_roll_'+str(window)+'_mean_75'] = np.percentile(pd.Series(z).rolling(window).mean().dropna().values, 75)\n            temp['percentile_roll_'+str(window)+'_mean_05'] = np.percentile(pd.Series(z).rolling(window).mean().dropna().values,  5)\n            temp['percentile_roll_'+str(window)+'_mean_95'] = np.percentile(pd.Series(z).rolling(window).mean().dropna().values, 95)            \n    return temp\n","7621dcc7":"\ndef parse_sample(sample : pd.DataFrame,\n                 batch_no : int,\n                 batch_size : int,\n                 window_size : List) -> pd.DataFrame:\n    \n    temp = feature_enginering_by_batch(sample['signal'].values, batch_size, window_size)\n    temp['batch_'+ str(batch_size)] = int(batch_no)\n    \n    return temp\n","82c035b5":"    \ndef sample_gen(df : pd.DataFrame,\n               batch_size : int,\n               window_size : List,\n               batches : List=[0], ) -> pd.DataFrame:\n    \n    result = Parallel(n_jobs=1, temp_folder='\/tmp', max_nbytes=None, backend='multiprocessing')(delayed(parse_sample)\n                                              (df[df['batch_'+ str(batch_size)]==i], int(i), batch_size, window_size)\n                                                                                              for i in tqdm(batches))\n    data = [r.values for r in result]\n    data = np.vstack(data)\n    cols = result[0].columns\n    cols = [name+'_'+str(batch_size) if name!='batch_'+ str(batch_size) else 'batch_'+ str(batch_size) for name in cols ]\n    X = pd.DataFrame(data, columns=cols)\n    X = reduce_mem_usage(X, False)\n    X = X.sort_values('batch_'+ str(batch_size))\n    \n    return X\n","f82c83fe":"\ndef run_feat_enginnering(df : pd.DataFrame,\n                         create_all_data_feats : bool,\n                         add_index : bool,\n                         batch_size : int,\n                         window_size : List) -> pd.DataFrame:\n    \n    df = batching(df, batch_size=batch_size, add_index=add_index)\n    if create_all_data_feats:\n        df = lag_with_pct_change(df, batch_size, [1, 2, 4],  add_pct_change=True, add_pct_change_lag=True)\n    batches = df['batch_'+ str(batch_size)].unique().tolist()\n    batch_feats=sample_gen(df, batch_size=batch_size, window_size=window_size, batches=batches)\n    df = pd.merge(df, batch_feats, on='batch_'+ str(batch_size), how='left')\n    df = reduce_mem_usage(df, False)\n    \n    return df\n","194df764":"def feature_selection(df : pd.DataFrame,\n                      df_test : pd.DataFrame,\n                      subtract_only : Optional[bool]=True,\n                      idx_cols : List=['time'],\n                      target_col : List=['open_channels']) -> Tuple[pd.DataFrame , pd.DataFrame]:\n    \n    drops = df.columns[df.isna().sum()>25000]\n    df = df.drop(drops, axis=1)\n    df = df.replace([np.inf, -np.inf], np.nan)\n    df = df.fillna(0)\n    gc.collect()\n    if subtract_only == False:\n        corrcoef_cols = [col for col in df.columns.tolist() if col not in (idx_cols+target_col)]\n        first=dict(); second=dict(); third=dict()\n        for col in corrcoef_cols:\n            ss = np.corrcoef(df[col], df['open_channels'])[0, 1]\n            first[col] = ss\n            ss = np.corrcoef(df[col]-df['signal'], df['open_channels'])[0, 1]\n            second[col] = ss\n            ss = np.corrcoef(df[col]*df['signal'], df['open_channels'])[0, 1]\n            third[col] = ss\n        corr_df = pd.DataFrame.from_dict(\n            {\n            'Base':first, \n            'Signal-Subtracted': second,\n            'Signal-Multiplied': third\n            }\n        ).fillna(0).apply(np.abs).sort_values('Base', ascending=False)\n\n        base_cols = corr_df.sort_values('Base', ascending=False).head(100).index.tolist()\n        multiply_cols = corr_df.sort_values('Signal-Multiplied', ascending=False).head(10).index.tolist()\n        subtract_cols = corr_df.sort_values('Signal-Subtracted', ascending=False).head(25).index.tolist()\n        display(corr_df.sort_values('Base', ascending=False).tail(50))\n        all_cols = list(set(base_cols + multiply_cols + subtract_cols + idx_cols + target_col))\n        all_cols_test = list(set(base_cols + multiply_cols + subtract_cols + idx_cols))   \n        drops = list(set(multiply_cols + subtract_cols)-set(base_cols))\n        df = df[all_cols]\n        df_test = df_test[all_cols_test]\n    \n        for col in multiply_cols:\n            df[col+'_m'] = df[col] * df['signal']\n            df_test[col+'_m'] = df_test[col] * df_test['signal']        \n        for col in subtract_cols:\n            df[col+'_s'] = df[col] - df['signal']\n            df_test[col+'_s'] = df_test[col] - df_test['signal']\n        df = df.drop(drops, axis=1)\n    else:\n        not_imp = ['kstat_1_5000', 'kstat_2_5000', 'kstat_3_5000', 'kstat_4_5000', 'moment_2_5000',\n                   'moment_3_5000','moment_4_5000', 'kstatvar_1_5000', 'kstatvar_2_5000','kstat_1_50000',\n                   'kstat_2_50000', 'kstat_3_50000', 'kstat_4_50000', 'moment_2_50000', 'moment_3_50000',\n                   'moment_4_50000', 'kstatvar_1_50000', 'kstatvar_2_50000']\n        subtract_cols = list(set(df.columns.tolist())-set(idx_cols + target_col + not_imp))\n        for col in subtract_cols:\n            df[col+'_s'] = df[col] - df['signal']\n            df_test[col+'_s'] = df_test[col] - df_test['signal']\n    df = reduce_mem_usage(df, False)\n    df_test = reduce_mem_usage(df_test, False)\n\n    gc.collect()\n    return df, df_test\n","446e39ac":"\ndef MacroF1Metric(preds : np.array, dtrain : lgb.Dataset) -> Tuple[Text, np.array, bool] :\n    \n    labels = dtrain.get_label()\n    preds = np.round(np.clip(preds, 0, 10)).astype(int)\n    score = f1_score(labels, preds, average = 'macro')\n    \n    return ('MacroF1Metric', score, True)\n","29479b5b":"\ndef run_cv_model_by_batch(train : pd.DataFrame,\n                          test : pd.DataFrame,\n                          splits : int,\n                          shuffle : bool,\n                          seed : int,\n                          batch_col : Text,\n                          params : Dict,\n                          feats : List,\n                          sample_submission: pd.DataFrame) -> pd.DataFrame:\n    \n    oof_ = np.zeros(len(train))\n    preds_ = np.zeros(len(test))\n    target = ['open_channels']\n    imp_df = pd.DataFrame(index=feats)\n    kf = KFold(splits, shuffle, seed)\n    for n_fold, (tr_idx, val_idx) in enumerate(kf.split(train, train[target], groups=train[batch_col])):\n        tr_x = train[feats].iloc[tr_idx]\n        vl_x = train[feats].iloc[val_idx]\n        tr_y = train[target].iloc[tr_idx].values\n        vl_y = train[target].iloc[val_idx].values\n        model = HistGradientBoostingRegressor(learning_rate = 0.1, max_iter=800, random_state = 404, validation_fraction=None, verbose = 0, max_depth=12, min_samples_leaf=25, l2_regularization=0.05)\n        model.fit(tr_x, tr_y)\n        oof_[val_idx] += model.predict(train[feats].iloc[val_idx])\n        preds_ += model.predict(test[feats]) \/ SPLITS\n        f1_score_ = f1_score(train[target].iloc[val_idx], np.round(np.clip(oof_[val_idx], 0, 10)).astype(int), average = 'macro')\n        rmse_score_ = np.sqrt(mean_squared_error(train[target].iloc[val_idx], oof_[val_idx]))\n        logger.info(f'Training fold {n_fold + 1} completed. macro f1 score : {f1_score_ :1.5f} rmse score : {rmse_score_:1.5f}')\n        #imp_df[f'feat_importance_{n_fold + 1}'] = model.feature_importance(importance_type='gain')\n    f1_score_ = f1_score(train[target], np.round(np.clip(oof_, 0, 10)).astype(int), average = 'macro')\n    rmse_score_ = np.sqrt(mean_squared_error(train[target], oof_))\n    logger.info(f'Training completed. oof macro f1 score : {f1_score_:1.5f} oof rmse score : {rmse_score_:1.5f}')\n    sample_submission['open_channels'] = np.round(np.clip(preds_, 0, 10)).astype(int)\n    sample_submission.to_csv('submission.csv', index=False, float_format='%.4f')\n    display(sample_submission.head())\n    np.save('oof.npy', oof_)\n    np.save('preds.npy', preds_)\n\n    return imp_df\n","9d7ee942":"def get_params(seed : int) -> Dict :\n    params = dict()\n    params['learning_rate']=0.009;\n    params['max_depth']=-1;\n    params['num_leaves']=257;\n    params['metric']='rmse';\n    params['random_state']=seed;\n    params['n_jobs']=-1;\n    params['feature_fraction']=1 ;\n    params['boosting']='goss';\n    params['boost_from_average']=True;\n    params['bagging_seed']=seed;\n    params['bagging_freq']=0;\n    params['bagging_fraction']=1;\n    params['reg_alpha']=0;\n    params['reg_lambda']=0\n    params['force_row_wise']=True\n    return params","dfdeaeef":"\ndef run_everything(fe_config : List) -> NoReturn:\n    not_feats_cols = ['time']\n    target_col = ['open_channels']\n    init_logger()\n    with timer(f'Reading Data'):\n        logger.info('Reading Data Started ...')\n        base = os.path.abspath('\/kaggle\/input\/liverpool-ion-switching\/')\n        train, test, sample_submission = read_data(base)\n        logger.info('Reading Data Completed ...')\n        \n    with timer(f'Creating Features'):\n        logger.info('Feature Enginnering Started ...')\n        for config in fe_config:\n            train = run_feat_enginnering(train, create_all_data_feats=config[0], add_index=config[1], batch_size=config[2], window_size=config[3])\n            test  = run_feat_enginnering(test,  create_all_data_feats=config[0], add_index=config[1], batch_size=config[2], window_size=config[3])\n            not_feats_cols.append('batch_'+str(config[2]))\n            if config[1]:\n                not_feats_cols.append('batch_'+str(config[2])+'_idx')\n        if SELECT:\n            train, test = feature_selection(train, test, subtract_only=True, idx_cols=not_feats_cols, target_col=target_col)\n        logger.info('Feature Enginnering Completed ...')\n\n    with timer(f'Running HistGradientBoosting model'):\n        logger.info(f'Training HistGradientBoosting model with {SPLITS} folds Started ...')\n        params = get_params(SEED)\n        feats = [c for c in train.columns if c not in (not_feats_cols+target_col)]\n        importances = run_cv_model_by_batch(train, test, splits=SPLITS, shuffle=True, seed=SEED, batch_col='batch_50000',params=params, feats=feats, sample_submission=sample_submission)\n        importances.to_csv('importances.csv')\n        logger.info(f'Training completed ...')\n","5a04f20d":"run_everything(fe_config)","9988e6c5":"Part of the Feature Engineering is from the following notebooks:\n\n[1-geomean-nn-and-6featlgbm-2-259-private-lb](https:\/\/www.kaggle.com\/dkaraflos\/1-geomean-nn-and-6featlgbm-2-259-private-lb) \n\n[physically-possible](https:\/\/www.kaggle.com\/jazivxt\/physically-possible)","c28b1316":"This is a direct copy of the followign kernel, with LGBM replaced with HistGradientBoosting\n\nhttps:\/\/www.kaggle.com\/siavrez\/fe-pipeline"}}