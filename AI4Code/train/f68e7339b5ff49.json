{"cell_type":{"ab5591bb":"code","cf5ddc9f":"code","58a6c424":"code","fd16ca56":"code","d307955e":"code","de5d7f3c":"code","b5d3a5a3":"code","ad52eeec":"code","e00bf45c":"code","01fc8bb7":"code","a36a54e4":"code","24b9a381":"code","56e7730a":"code","568055b3":"code","972ed66a":"code","e5884cb3":"code","624af3ea":"code","615e51b5":"code","3a665dc2":"code","f724157c":"code","06ecabcf":"markdown","6392e20b":"markdown"},"source":{"ab5591bb":"import os\nimport numpy as np\nimport pandas as pd\nimport math\nimport cv2\nimport sklearn\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [16, 8]\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import Conv2D,BatchNormalization,MaxPool2D,Concatenate,Add,Dropout,ReLU,Conv2DTranspose,UpSampling2D","cf5ddc9f":"image_path = '..\/input\/finding-lungs-in-ct-data\/2d_images'\nmask_path = '..\/input\/finding-lungs-in-ct-data\/2d_masks'","58a6c424":"autotune = tf.data.experimental.AUTOTUNE\nimage_size = 448\nbatch_size = 16","fd16ca56":"exclude = [image_path+'\/ID_0254_Z_0075.tif',mask_path+'\/ID_0254_Z_0075.tif',\n           image_path+'\/ID_0052_Z_0108.tif',mask_path+'\/ID_0052_Z_0108.tif',\n           image_path+'\/ID_0079_Z_0072.tif',mask_path+'\/ID_0079_Z_0072.tif',\n           image_path+'\/ID_0134_Z_0137.tif',mask_path+'\/ID_0134_Z_0137.tif']\n\nnum_files = len(os.listdir(image_path))-len(exclude)\/2\nprint(num_files)\n\ndef read_image(image):\n    path = image.numpy().decode('utf-8')\n    if path not in exclude:\n        image = cv2.imread(path,0)\n        image = np.expand_dims(image,2)\n        image = tf.image.resize(image, [image_size,image_size])\n    else:\n        image = np.zeros((image_size,image_size,1),dtype=np.float32)\n    return image\n\ndef read_mask(image):\n    path = image.numpy().decode('utf-8')\n    if path not in exclude:\n        image = cv2.imread(path,0)\n        image = image*257.\n        image = np.expand_dims(image,2)\n        image = tf.image.resize(image, [image_size,image_size])\n    else:\n        image = np.zeros((image_size,image_size,1),dtype=np.float32)\n    return image\n\ndef aug(image,label):\n    seed = np.random.randint(0,64)\n    image = tf.image.random_flip_left_right(image, seed=seed)\n    image = tf.image.random_flip_up_down(image, seed=seed)\n    image = tf.image.random_contrast(image, 0.3, 0.7, seed=seed)\n    label = tf.image.random_flip_left_right(label, seed=seed)\n    label = tf.image.random_flip_up_down(label, seed=seed)\n    label = tf.image.random_contrast(label, 0.3, 0.7, seed=seed)\n    return image,label\n\ndataset_image = tf.data.Dataset.list_files('..\/input\/finding-lungs-in-ct-data\/2d_images\/*.tif',shuffle=False,seed=None).map(lambda x: tf.py_function(read_image,[x],[tf.float32]),num_parallel_calls=autotune,deterministic=False)\ndataset_mask = tf.data.Dataset.list_files('..\/input\/finding-lungs-in-ct-data\/2d_masks\/*.tif',shuffle=False,seed=None).map(lambda x: tf.py_function(read_mask,[x],[tf.float32]),num_parallel_calls=autotune,deterministic=False)\n\ndataset = tf.data.Dataset.zip((dataset_image, dataset_mask))\ndataset = dataset.repeat().shuffle(128).batch(batch_size).cache().prefetch(autotune)\ntrain = dataset.take(int(0.8*num_files\/\/batch_size))\ntest = dataset.skip(int(0.8*num_files\/\/batch_size))","d307955e":"keep_scale = 0.2\nl1l2 = tf.keras.regularizers.l1_l2(l1=0, l2=0.0005)\n\ndef resblock(x,level='en_l1',filters=64,keep_scale=keep_scale,l1l2=l1l2,downsample=False,bn_act=True,first_layer=False):\n    if downsample:\n        if not first_layer:\n            x_H = Conv2D(filters,(3,3),(2,2),padding='same',kernel_regularizer=l1l2,name=level+'_Hconv')(x)\n            x = Conv2D(filters\/2,(3,3),padding='same',kernel_regularizer=l1l2,name=level+'_conv1')(x)\n        else:\n            x_H = Conv2D(filters,(3,3),(2,2),padding='same',kernel_regularizer=l1l2,name=level+'_Hconv')(x)\n            x = Conv2D(filters,(3,3),padding='same',kernel_regularizer=l1l2,name=level+'_conv1')(x)\n    else:\n        x_H = x\n        x = Conv2D(filters,(3,3),padding='same',kernel_regularizer=l1l2,name=level+'_conv1')(x)\n    x = BatchNormalization(name=level+'_conv1bn')(x)\n    x = ReLU(name=level+'_conv1relu')(x)\n    if downsample:\n        x = Conv2D(filters,(3,3),(2,2),padding='same',kernel_regularizer=l1l2,name=level+'_conv2')(x)\n    else:\n        x = Conv2D(filters,(3,3),padding='same',kernel_regularizer=l1l2,name=level+'_conv2')(x)\n    x = BatchNormalization(name=level+'_conv2bn')(x)\n    x = ReLU(name=level+'_conv2relu')(x)\n    x = Add(name=level+'_add')([x_H*keep_scale,x*(1-keep_scale)])\n    if bn_act:\n        x = BatchNormalization(name=level+'_finalbn')(x)\n        x = ReLU(name=level+'_finalrelu')(x)       \n    return x","de5d7f3c":"inputs = tf.keras.Input(shape=(image_size,image_size,1),name='input')\n\nlayer_0 = Conv2D(64,(7,7),strides=(2,2),padding='same',name='input_conv1',activation=None)(inputs)\nlayer = BatchNormalization(name='input_conv1bn')(layer_0)\nlayer = ReLU(name='input_conv1relu')(layer)\n\n## Encoder\nlayer = resblock(layer,'en_l1',64,keep_scale,l1l2,downsample=True,bn_act=True,first_layer=True)\nlayer = resblock(layer,'en_l2',64,keep_scale,l1l2,downsample=False,bn_act=True)\nencoder_1 = resblock(layer,'en_l3',64,keep_scale,l1l2,downsample=False,bn_act=False)\nlayer = BatchNormalization(name='en_l3_finalbn')(encoder_1)\nlayer = ReLU(name='en_l3_finalrelu')(layer)\n\nlayer = resblock(layer,'en_l4',128,keep_scale,l1l2,downsample=True,bn_act=True)\nlayer = resblock(layer,'en_l5',128,keep_scale,l1l2,downsample=False,bn_act=True)\nlayer = resblock(layer,'en_l6',128,keep_scale,l1l2,downsample=False,bn_act=True)\nencoder_2 = resblock(layer,'en_l7',128,keep_scale,l1l2,downsample=False,bn_act=False)\nlayer = BatchNormalization(name='en_l7_finalbn')(encoder_2)\nlayer = ReLU(name='en_l7_finalrelu')(layer)\n\nlayer = resblock(layer,'en_l8',256,keep_scale,l1l2,downsample=True,bn_act=True)\nlayer = resblock(layer,'en_l9',256,keep_scale,l1l2,downsample=False,bn_act=True)\nlayer = resblock(layer,'en_l10',256,keep_scale,l1l2,downsample=False,bn_act=True)\nlayer = resblock(layer,'en_l11',256,keep_scale,l1l2,downsample=False,bn_act=True)\nlayer = resblock(layer,'en_l12',256,keep_scale,l1l2,downsample=False,bn_act=True)\nencoder_3 = resblock(layer,'en_l13',256,keep_scale,l1l2,downsample=False,bn_act=False)\nlayer = BatchNormalization(name='en_l13_finalbn')(encoder_3)\nlayer = ReLU(name='en_l13_finalrelu')(layer)\n\nlayer = resblock(layer,'en_l14',512,keep_scale,l1l2,downsample=True,bn_act=True)\nlayer = resblock(layer,'en_l15',512,keep_scale,l1l2,downsample=False,bn_act=True)\nlayer = resblock(layer,'en_l16',512,keep_scale,l1l2,downsample=False,bn_act=True)\n\n## DAC block\nb1 = Conv2D(512,(3,3),padding='same',dilation_rate=1,name='dac_b1_conv1',activation=None)(layer)\n# b1 = BatchNormalization()(b1)\n# b1 = ReLU(name='dac_b1_relu')(b1)\n\nb2 = Conv2D(512,(3,3),padding='same',dilation_rate=3,name='dac_b2_conv1',activation=None)(layer)\nb2 = Conv2D(512,(1,1),padding='same',dilation_rate=1,name='dac_b2_conv2',activation=None)(b2)\n# b2 = BatchNormalization()(b2)\n# b2 = ReLU(name='dac_b2_relu')(b2)\n\nb3 = Conv2D(512,(3,3),padding='same',dilation_rate=1,name='dac_b3_conv1',activation=None)(layer)\nb3 = Conv2D(512,(3,3),padding='same',dilation_rate=3,name='dac_b3_conv2',activation=None)(b3)\nb3 = Conv2D(512,(1,1),padding='same',dilation_rate=1,name='dac_b3_conv3',activation=None)(b3)\n# b3 = BatchNormalization()(b3)\n# b3 = ReLU(name='dac_b3_relu')(b3)\n\nb4 = Conv2D(512,(3,3),padding='same',dilation_rate=1,name='dac_b4_conv1',activation=None)(layer)\nb4 = Conv2D(512,(3,3),padding='same',dilation_rate=3,name='dac_b4_conv2',activation=None)(b4)\nb4 = Conv2D(512,(3,3),padding='same',dilation_rate=5,name='dac_b4_conv3',activation=None)(b4)\nb4 = Conv2D(512,(1,1),padding='same',dilation_rate=1,name='dac_b4_conv4',activation=None)(b4)\n# b4 = BatchNormalization()(b4)\n# b4 = ReLU(name='dac_b4_relu')(b4)\n\nlayer = Add(name='dac_add')([layer,b1,b2,b3,b4])\n# layer = BatchNormalization(name='dac_bn')(layer)\nlayer = ReLU(name='dac_relu')(layer)\n\n## RMP block\nb1 = MaxPool2D((2,2),strides=(2,2),padding='valid',name='rmp_b1_pool')(layer)\nb1 = Conv2D(1,(1,1),padding='valid',name='rmb_b1_conv1',activation=None)(b1)\nb1 = Conv2DTranspose(1,(1,1),(2,2),padding='valid',kernel_regularizer=l1l2,output_padding=0,activation=None)(b1)\nb1 = tf.image.resize(b1, [14,14], method=tf.image.ResizeMethod.BILINEAR)\n\nb2 = MaxPool2D((3,3),strides=(3,3),padding='valid',name='rmp_b2_pool')(layer)\nb2 = Conv2D(1,(1,1),padding='valid',name='rmb_b2_conv1',activation=None)(b2)\nb2 = Conv2DTranspose(1,(1,1),(3,3),padding='valid',kernel_regularizer=l1l2,output_padding=0,activation=None)(b2)\nb2 = tf.image.resize(b2, [14,14], method=tf.image.ResizeMethod.BILINEAR)\n\nb3 = MaxPool2D((5,5),strides=(5,5),padding='valid',name='rmp_b3_pool')(layer)\nb3 = Conv2D(1,(1,1),padding='valid',name='rmb_b3_conv1',activation=None)(b3)\nb3 = Conv2DTranspose(1,(1,1),(5,5),padding='valid',kernel_regularizer=l1l2,output_padding=0,activation=None)(b3)\nb3 = tf.image.resize(b3, [14,14], method=tf.image.ResizeMethod.BILINEAR)\n\nb4 = MaxPool2D((6,6),strides=(6,6),padding='valid',name='rmp_b4_pool')(layer)\nb4 = Conv2D(1,(1,1),padding='valid',name='rmb_b4_conv1',activation=None)(b4)\nb4 = Conv2DTranspose(1,(1,1),(6,6),padding='valid',kernel_regularizer=l1l2,output_padding=0,activation=None)(b4)\nb4 = tf.image.resize(b4, [14,14], method=tf.image.ResizeMethod.BILINEAR)\n\nlayer = Concatenate(name='rmp_concat')([layer,b1,b2,b3,b4])\nlayer = ReLU(name='rmp_relu')(layer)\n\nlayer = Conv2D(256,(1,1),padding='same',kernel_regularizer=l1l2,name='de_l1_conv1',activation=None)(layer)\nlayer = BatchNormalization(name='de_l1_conv1bn')(layer)\nlayer = ReLU(name='de_l1_conv1relu')(layer)\nlayer = Conv2DTranspose(256,(3,3),(2,2),padding='same',kernel_regularizer=l1l2,output_padding=1,name='de_l1_deconv2',activation=None)(layer)\nlayer = BatchNormalization(name='de_l1_conv2bn')(layer)\nlayer = ReLU(name='de_l1_deconv2relu')(layer)\nlayer = Conv2D(256,(3,3),padding='same',kernel_regularizer=l1l2,name='de_l1_conv3',activation=None)(layer)\nlayer = Add(name='de_l1_add')([encoder_3*keep_scale,layer*(1-keep_scale)])\nlayer = BatchNormalization(name='de_l1_conv3bn')(layer)\nlayer = ReLU(name='de_l1_conv3relu')(layer)\n\nlayer = Conv2D(128,(1,1),padding='same',kernel_regularizer=l1l2,name='de_l2_conv1',activation=None)(layer)\nlayer = BatchNormalization(name='de_l2_conv1bn')(layer)\nlayer = ReLU(name='de_l2_conv1relu')(layer)\nlayer = Conv2DTranspose(128,(3,3),(2,2),padding='same',kernel_regularizer=l1l2,output_padding=1,name='de_l2_deconv2',activation=None)(layer)\nlayer = BatchNormalization(name='de_l2_conv2bn')(layer)\nlayer = ReLU(name='de_l2_deconv2relu')(layer)\nlayer = Conv2D(128,(1,1),padding='same',kernel_regularizer=l1l2,name='de_l2_conv3',activation=None)(layer)\nlayer = Add(name='de_l2_add')([encoder_2*keep_scale,layer*(1-keep_scale)])\nlayer = BatchNormalization(name='de_l2_conv3bn')(layer)\nlayer = ReLU(name='de_l2_conv3relu')(layer)\n\nlayer = Conv2D(64,(1,1),padding='same',kernel_regularizer=l1l2,name='de_l3_conv1',activation=None)(layer)\nlayer = BatchNormalization(name='de_l3_conv1bn')(layer)\nlayer = ReLU(name='de_l3_conv1relu')(layer)\nlayer = Conv2DTranspose(64,(3,3),(2,2),padding='same',kernel_regularizer=l1l2,output_padding=1,name='de_l3_deconv2',activation=None)(layer)\nlayer = BatchNormalization(name='de_l3_conv2bn')(layer)\nlayer = ReLU(name='de_l3_deconv2relu')(layer)\nlayer = Conv2D(64,(1,1),padding='same',kernel_regularizer=l1l2,name='de_l3_conv3',activation=None)(layer)\nlayer = Add(name='de_l3_add')([encoder_1*keep_scale,layer*(1-keep_scale)])\nlayer = BatchNormalization(name='de_l3_conv3bn')(layer)\nlayer = ReLU(name='de_l3_conv3relu')(layer)\n\nlayer = Conv2D(64,(1,1),padding='same',kernel_regularizer=l1l2,name='de_l4_conv1',activation=None)(layer)\nlayer = BatchNormalization(name='de_l4_conv1bn')(layer)\nlayer = ReLU(name='de_l4_conv1relu')(layer)\nlayer = Conv2DTranspose(64,(3,3),(2,2),padding='same',kernel_regularizer=l1l2,output_padding=1,name='de_l4_deconv2',activation=None)(layer)\nlayer = BatchNormalization(name='de_l4_conv2bn')(layer)\nlayer = ReLU(name='de_l4_deconv2relu')(layer)\nlayer = Conv2D(64,(1,1),padding='same',kernel_regularizer=l1l2,name='de_l4_conv3',activation=None)(layer)\nlayer = Add(name='de_l4_add')([layer_0*keep_scale,layer*(1-keep_scale)])\nlayer = BatchNormalization(name='de_l4_conv3bn')(layer)\nlayer = ReLU(name='de_l4_conv3relu')(layer)\n\nlayer = Conv2DTranspose(32,(3,3),(2,2),padding='same',kernel_regularizer=l1l2,name='final_deconv1',activation=None)(layer)\nlayer = BatchNormalization()(layer)\nlayer = ReLU()(layer)\nlayer = Conv2D(32,(3,3),padding='same',kernel_regularizer=l1l2,name='final_conv1',activation=None)(layer)\nlayer = BatchNormalization()(layer)\nlayer = ReLU()(layer)\noutputs = Conv2D(1,(3,3),padding='same',name='output',activation=None)(layer)\n\nmodel = tf.keras.Model(inputs,outputs,name='CE-Net')\nprint(model.summary())","b5d3a5a3":"# tf.keras.utils.plot_model(model, show_shapes=True)","ad52eeec":"lr_init = 0.0001\ntotal_epoch = 100","e00bf45c":"def scheduler_1(epoch):\n    epoch += 1\n    \n    if epoch <= 4:\n        return lr_init    \n   \n    if epoch >= 5 and epoch <= 10:\n        return lr_init-lr_init*math.exp(0.25*(epoch-8))\/40\n    \n    elif epoch >= 11 and epoch <= 50:\n        return lr_init*math.exp(-0.05*(epoch-10))\n    \n    else:\n        return scheduler_1(50-1)","01fc8bb7":"def scheduler_2(epoch):  \n    epoch += 1\n   \n    if epoch == 1:\n        return lr_init\n    \n    elif epoch >= 2 and epoch <= 35:\n        return (0.25*epoch**3)*math.exp(-0.25*epoch)*lr_init\n    \n    else:\n        return scheduler_2(35-1)","a36a54e4":"def scheduler_3(epoch):\n    \n    if epoch == 0:\n        return lr_init\n\n    else:\n        return lr_init*((1-epoch\/100)**0.9)","24b9a381":"stage = [i for i in range(0,30)]\nlr_plot = [scheduler_3(x) for x in stage]\nplt.plot(stage, lr_plot)\nprint(lr_plot)","56e7730a":"smooth = K.epsilon()\nthreshold = 0.8\nlabel_smoothing = 0.0\nbce_weight = 0.5\n\ndef dice_coeff(y_true, y_pred):\n    numerator = tf.math.reduce_sum(y_true * y_pred) + smooth\n    denominator = tf.math.reduce_sum(y_true) + tf.math.reduce_sum(y_pred) + smooth\n    return tf.math.reduce_mean(2.* numerator \/ denominator) * 448 * 448\n\ndef dice_loss(y_true, y_pred):\n    return - dice_coeff(y_true, y_pred)\n\ndef bce_dice_loss(y_true, y_pred):\n    return bce_weight * tf.keras.losses.binary_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=label_smoothing) - dice_coeff(y_true, y_pred) \n\ndef bce_dice_loss_try(y_true, y_pred):\n    x = tf.keras.losses.binary_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=label_smoothing)\n    y = -dice_coeff(y_true, y_pred)\n    return x + x\/(x+y)*y\n\ndef iou(y_true, y_pred):\n    overlap = tf.math.logical_and((y_true > threshold),(y_pred > threshold))\n    union = tf.math.logical_or((y_true > threshold),(y_pred > threshold))\n    iou = (tf.cast(tf.math.count_nonzero(overlap),tf.float32) + smooth) \/ (tf.cast(tf.math.count_nonzero(union),tf.float32) + smooth)\n    return iou","568055b3":"loss = bce_dice_loss\noptimizer = tf.keras.optimizers.Adam(lr_init)\nmetrics = [tf.keras.metrics.BinaryCrossentropy(from_logits=False,label_smoothing=label_smoothing,dtype=tf.float32,name='bce'), dice_coeff, iou]\nmodel.compile(loss=loss,optimizer=optimizer,metrics=metrics)","972ed66a":"model.load_weights('model_weights.h5')","e5884cb3":"scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler_3, verbose=1)\nearlystop = tf.keras.callbacks.EarlyStopping(monitor='val_iou',mode='max',verbose=1,patience=15,restore_best_weights=True)\ncheckpoint = tf.keras.callbacks.ModelCheckpoint('temp',save_weights_only=True,monitor='val_iou',mode='max',save_best_only=True)\n\nhistory = model.fit(train, batch_size=batch_size, epochs=total_epoch, callbacks=[scheduler,earlystop,checkpoint],\n            validation_data=test, steps_per_epoch=int(0.8*num_files\/\/batch_size), validation_steps=int(0.2*num_files\/\/batch_size), shuffle=True)","624af3ea":"model.save('model.h5')\nmodel.save_weights('model_weights.h5')","615e51b5":"dataset_image_only = tf.data.Dataset.list_files('..\/input\/finding-lungs-in-ct-data\/2d_images\/*.tif',shuffle=False).map(lambda x: tf.py_function(read_image,[x],[tf.float32]))\ndataset_mask_only = tf.data.Dataset.list_files('..\/input\/finding-lungs-in-ct-data\/2d_masks\/*.tif',shuffle=False).map(lambda x: tf.py_function(read_mask,[x],[tf.float32]))","3a665dc2":"take = 32\nnum = np.random.randint(0,num_files-take-1)\n\nimage = np.array(list(dataset_image_only.skip(num).take(take).as_numpy_iterator()))\ntruth = np.array(list(dataset_mask_only.skip(num).take(take).as_numpy_iterator()))\npred = model.predict(dataset_image_only.skip(num).batch(take).take(1))","f724157c":"plt.rcParams['figure.figsize'] = [20,200]\nindex = 1\nfor i in range(take):\n    plt.subplot(take,3,index)\n    plt.title('image %s'%i)\n    plt.imshow(image[i,0,:,:,0], cmap='gray')\n    index += 1\n    plt.subplot(take,3,index)\n    plt.title('truth %s'%i)\n    plt.imshow(truth[i,0,:,:,0], cmap='gray')\n    index += 1\n    plt.subplot(take,3,index)\n    plt.title('pred %s'%i)\n    plt.imshow(pred[i,:,:,0], cmap='gray')\n    index += 1","06ecabcf":"Based on research: [CE-Net: Context Encoder Network for 2D Medical Image Segmentation](https:\/\/arxiv.org\/abs\/1903.02740). I have made some changes for testing.\n### Sturcture of CE-Net:\n![CE-Net Sturcture](https:\/\/pic3.zhimg.com\/v2-c152b10827b18b63a79fce175a93a08e_1440w.jpg?source=172ae18b)\n**1. Resnet-34 for feature-encoder**\n\n**2. Dense Atrous Convolution block (conv2d with dilation)**\n\n**3. Residual Multi-kernal Pooling block (Pooling with different field sizes)**\n\n**4. Feature-decoder**","6392e20b":"### I will avoid these 4 masks and their corresponding images:\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F4050923%2F3c7e5b003157c71d18e35d0b982db2f3%2Fg233gvbs3.PNG?generation=1596037526115175&alt=media)"}}