{"cell_type":{"cf91fb74":"code","d8b15092":"code","2343dd59":"code","d176870c":"code","d09f4034":"code","d2cc23f2":"code","43552fe3":"code","e630b038":"code","7b386776":"code","457c32b1":"code","ecaba43a":"code","aa693d7e":"code","1d43aeba":"code","09f07cb2":"code","4b103de8":"code","c3e315d5":"code","572d856b":"code","436cd789":"code","374a9a1e":"code","c9430fb7":"markdown","7b9d72d0":"markdown","1b2802b3":"markdown","398656a5":"markdown","8129e722":"markdown","a091ec06":"markdown","86c27ab6":"markdown","2c566132":"markdown","b4d6095f":"markdown","bc55ab3a":"markdown","fa8cfc01":"markdown","8d32bdaa":"markdown","d0cb559d":"markdown","2720f4ab":"markdown","0837cab4":"markdown","cfb10fd2":"markdown","efaaa91b":"markdown","6e7f4ea3":"markdown","8c6c08cc":"markdown","f07e6138":"markdown","2ca762b7":"markdown","c42af481":"markdown","590259b3":"markdown"},"source":{"cf91fb74":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport math\n%matplotlib inline \nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nfrom sklearn.model_selection import train_test_split\ndef split(df,label):\n    X_tr, X_te, Y_tr, Y_te = train_test_split(df, label, test_size=0.25, random_state=42)\n    return X_tr, X_te, Y_tr, Y_te\n\n\nfrom sklearn.feature_selection import chi2\n\ndef feat_select(df,f_score_val,num):\n    feat_list = list(f_score_val[\"Feature\"][:num])\n    return df[feat_list]\n\n\nfrom sklearn import svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nimport xgboost as xgb\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold, cross_val_score\n\nclassifiers = ['LinearSVM', 'RadialSVM', \n               'Logistic',  'RandomForest', \n               'AdaBoost',  'DecisionTree', \n               'KNeighbors','GradientBoosting']\n\nmodels = [svm.SVC(kernel='linear'),\n          svm.SVC(kernel='rbf'),\n          LogisticRegression(max_iter = 1000),\n          RandomForestClassifier(n_estimators=200, random_state=0),\n          AdaBoostClassifier(random_state = 0),\n          DecisionTreeClassifier(random_state=0),\n          KNeighborsClassifier(),\n          GradientBoostingClassifier(random_state=0)]\n\n\ndef f_score(df,label):\n    chi_values=chi2(df,label)\n    score = list(chi_values[0])\n    feat = df.columns.tolist()\n    fscore_df = pd.DataFrame({\"Feature\":feat, \"Score\":score})\n    fscore_df.sort_values(by=\"Score\", ascending=False,inplace = True)\n    fscore_df.reset_index(drop=True, inplace=True)\n    return fscore_df\n    \n    \ndef acc_score(df,label):\n    Score = pd.DataFrame({\"Classifier\":classifiers})\n    j = 0\n    acc = []\n    X_train,X_test,Y_train,Y_test = split(df,label)\n    for i in models:\n        model = i\n        model.fit(X_train,Y_train)\n        predictions = model.predict(X_test)\n        acc.append(accuracy_score(Y_test,predictions))\n        j = j+1     \n    Score[\"Accuracy\"] = acc\n    Score.sort_values(by=\"Accuracy\", ascending=False,inplace = True)\n    Score.reset_index(drop=True, inplace=True)\n    return Score\n\n\ndef acc_score_num(df,label,f_score_val,feat_list):\n    Score = pd.DataFrame({\"Classifier\":classifiers})\n    df2 = None\n    for k in range(len(feat_list)):\n        df2 = feat_select(df,f_score_val,feat_list[k])\n        X_train,X_test,Y_train,Y_test = split(df2,label)\n        j = 0\n        acc = []\n        for i in models:\n            model = i\n            model.fit(X_train,Y_train)\n            predictions = model.predict(X_test)\n            acc_val = accuracy_score(Y_test,predictions)\n            acc.append(acc_val)\n            j = j+1  \n        feat = str(feat_list[k])\n        Score[feat] = acc\n    return Score\n\n\ndef plot2(df,l1,l2,p1,p2,c = \"b\"):\n    feat = []\n    feat = df.columns.tolist()\n    feat = feat[1:]\n    plt.figure(figsize = (16, 18))\n    for j in range(0,df.shape[0]):\n        value = []\n        k = 0\n        for i in range(1,len(df.columns.tolist())):\n            value.append(df.iloc[j][i])\n        plt.subplot(4, 4,j+1)\n        ax = sns.pointplot(x=feat, y=value,color = c ,markers=[\".\"])\n        plt.text(p1,p2,df.iloc[j][0])\n        plt.xticks(rotation=90)\n        ax.set(ylim=(l1,l2))\n        k = k+1\n        \n        \ndef highlight_max(data, color='aquamarine'):\n    attr = 'background-color: {}'.format(color)\n    if data.ndim == 1:  \n        is_max = data == data.max()\n        return [attr if v else '' for v in is_max]\n    else: \n        is_max = data == data.max().max()\n        return pd.DataFrame(np.where(is_max, attr, ''),\n                            index=data.index, columns=data.columns)","d8b15092":"data_bc = pd.read_csv(\"..\/input\/breast-cancer-wisconsin-data\/data.csv\")\nlabel_bc = data_bc[\"diagnosis\"]\nlabel_bc = np.where(label_bc == 'M',1,0)\ndata_bc.drop([\"id\",\"diagnosis\",\"Unnamed: 32\"],axis = 1,inplace = True)\n\nprint(\"Breast Cancer dataset:\\n\",data_bc.shape[0],\"Records\\n\",data_bc.shape[1],\"Features\")","2343dd59":"display(data_bc.head())\nprint(\"All the features in this dataset have continuous values\")","d176870c":"f_score_bc = f_score(data_bc,label_bc)\nf_score_bc","d09f4034":"score1 = acc_score(data_bc,label_bc)\nscore1","d2cc23f2":"num_feat1 = list(range(8,26))\nclassifiers = score1[\"Classifier\"].tolist()\nscore_bc = acc_score_num(data_bc,label_bc,f_score_bc,num_feat1)\nscore_bc.style.apply(highlight_max, subset = score_bc.columns[1:], axis=None)","43552fe3":"plot2(score_bc,0.90,1,2.5,0.91,c = \"gold\")","e630b038":"data_pd = pd.read_csv(\"..\/input\/parkinson-disease-detection\/Parkinsson disease.csv\")\nlabel_pd = data_pd[\"status\"]\ndata_pd.drop([\"status\",\"name\"],axis = 1,inplace = True)\n#Dropping columns with negative value as it does not work for chi2 test\nfor i in data_pd.columns:\n    neg = data_pd[i]<0\n    nsum = neg.sum()\n    if nsum > 0:\n        data_pd.drop([i],axis = 1,inplace = True)\n\nprint(\"Parkinson's disease dataset:\\n\",data_pd.shape[0],\"Records\\n\",data_pd.shape[1],\"Features\")","7b386776":"display(data_pd.head())\nprint(\"All the features in this dataset have continuous values\")","457c32b1":"f_score_pd = f_score(data_pd,label_pd)\nf_score_pd","ecaba43a":"score3 = acc_score(data_pd,label_pd)\nscore3","aa693d7e":"num_feat3 = list(range(7,21))\nclassifiers = score3[\"Classifier\"].tolist()\nscore_pd = acc_score_num(data_pd,label_pd,f_score_pd,num_feat3)\nscore_pd.style.apply(highlight_max, subset = score_pd.columns[1:], axis=None)","1d43aeba":"plot2(score_pd,0.65,1.0,1,0.7,c = \"orange\")","09f07cb2":"data_pcos = pd.read_csv(\"..\/input\/pcos-dataset\/PCOS_data.csv\")\nlabel_pcos = data_pcos[\"PCOS (Y\/N)\"]\ndata_pcos.drop([\"Sl. No\",\"Patient File No.\",\"PCOS (Y\/N)\",\"Unnamed: 44\",\"II    beta-HCG(mIU\/mL)\",\"AMH(ng\/mL)\"],axis = 1,inplace = True)\ndata_pcos[\"Marraige Status (Yrs)\"].fillna(data_pcos['Marraige Status (Yrs)'].describe().loc[['50%']][0], inplace = True) \ndata_pcos[\"Fast food (Y\/N)\"].fillna(1, inplace = True) \n\nprint(\"PCOS dataset:\\n\",data_pcos.shape[0],\"Records\\n\",data_pcos.shape[1],\"Features\")","4b103de8":"display(data_pcos.head())\nprint(\"The features in this dataset have both discrete and continuous values\")","c3e315d5":"f_score_pcos = f_score(data_pcos,label_pcos)\nf_score_pcos","572d856b":"score4 = acc_score(data_pcos,label_pcos)\nscore4","436cd789":"num_feat4 = list(range(12,28))\nclassifiers = score4[\"Classifier\"].tolist()\nscore_pcos = acc_score_num(data_pcos,label_pcos,f_score_pcos,num_feat4)\nscore_pcos.style.apply(highlight_max, subset = score_pcos.columns[1:], axis=None)","374a9a1e":"plot2(score_pcos,0.3,1.0,1,0.35,c = \"limegreen\")","c9430fb7":"### 4. Visualization","7b9d72d0":"### 3. Checking Accuracy","1b2802b3":"#### Best Accuracy with all features : RandomForest Classifier - 0.972\n#### Best Accuracy for multiple classifiers for different number of features - 0.979\n#### Here we can only see a slight improvement.","398656a5":"______________\n# Breast Cancer\n_____________","8129e722":"#### Best Accuracy with all features : RandomForest Classifier - 0.889\n#### Best Accuracy for first (12,20,25) features : DecisionTree Classifier - 0.904\n#### Here we can see an improvement of ~1.5%.","a091ec06":"______\n# Parkinson's disease\n_______","86c27ab6":"### 1. Looking at dataset","2c566132":"#### Best Accuracy with all features : RandomForest Classifier - 0.918\n#### Best Accuracy for multiple classifiers for different number of features - 0.918\n#### Here we see no improvement.","b4d6095f":"### 1. Looking at dataset","bc55ab3a":"### Importing the required libraries ","fa8cfc01":"_________\n#### From looking at these results we can see that there is a possibility of slight improvement in the accuracy after choosing the right features as per the F-score value.\n#### Link to other feature selection methods:\n##### [Genetic Algorithm](https:\/\/www.kaggle.com\/tanmayunhale\/genetic-algorithm-for-feature-selection)\n##### [Variance Threshold](https:\/\/www.kaggle.com\/tanmayunhale\/feature-selection-variance-threshold)\n##### [Pearson Correlation](https:\/\/www.kaggle.com\/tanmayunhale\/feature-selection-pearson-correlation)","8d32bdaa":"### 1. Looking at dataset","d0cb559d":"### 4. Visualization","2720f4ab":"## F - score\n________\n\n#### The F-score, also called the F1-score, is a measure of a model\u2019s accuracy on a dataset. It is used to evaluate binary classification systems, which classify examples into \u2018positive\u2019 or \u2018negative\u2019.\n#### The F-score is a way of combining the precision and recall of the model, and it is defined as the harmonic mean of the model\u2019s precision and recall.\n![](https:\/\/images.deepai.org\/user-content\/9954225913-thumb-4901.svg)\n#### The  **chi2** test returns 2 values : **F-score** and **p - value**. Based on the F-score for each feature, we will check the accuracy while considering different number of features for training at a time. Features with high F-score value are of importance.\n________","0837cab4":"### 2. F-score","cfb10fd2":"### 3. Checking Accuracy","efaaa91b":"________\n# PCOS\n________","6e7f4ea3":"### 4. Visualization","8c6c08cc":"### 2. F-score","f07e6138":"## Feature Selection\n#### Feature selection is the process of reducing the number of input variables when developing a predictive model. It is desirable to reduce the number of input variables to both reduce the computational cost of modeling and, in some cases, to improve the performance of the model.","2ca762b7":"### 2. F-score","c42af481":"### 3. Checking Accuracy","590259b3":"_____\n### Function Description\n#### 1. split():\nSplits the dataset into training and test set.\n\n#### 2. feat_select():\nReturns the dataframe with first 'n' features.\n\n#### 3. f_score():\nReturns the dataframe with the F-score for each feature.\n\n#### 4. acc_score():\nReturns accuracy for all the classifiers.\n\n#### 5. acc_score_num():\nReturns accuracy for all the classifiers for the specified number of features.\n\n#### 6. plot2():\nFor plotting the results.\n\n_____\n\n### The following 3 datasets are used:\n1. Breast Cancer\n2. Parkinson's Disease\n3. PCOS\n\n_____\n\n### Plan of action:\n* Looking at dataset (includes a little preprocessing)\n* F-score (Displaying F-score for each feature)\n* Checking Accuracy (comparing accuracies for different number of features) \n* Visualization (Plotting the graphs)\n____"}}