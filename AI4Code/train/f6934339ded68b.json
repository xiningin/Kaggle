{"cell_type":{"20154376":"code","53e74156":"code","19484558":"code","cad58d8e":"code","165cac8a":"code","8bc2142f":"code","9aad2acc":"code","7aeb6a84":"code","d21e48eb":"code","a36240f3":"code","8ad6bca0":"code","fa51bd9a":"code","e78409aa":"code","7a297c7b":"code","908b9720":"code","7dcb4ce7":"code","9770ea07":"code","c3ede96d":"code","bc0205df":"code","6aea5459":"code","43915635":"code","943f9ac7":"code","c10c7577":"code","35d36703":"code","a95bc21b":"code","b9067e6d":"code","b956eb10":"code","190b8993":"code","34136a86":"code","0c7f83bf":"code","87e930df":"code","8ceced50":"code","88d15ca4":"code","9b3d25ef":"code","8d37192b":"code","ade603bf":"code","2c3ccc65":"code","b9c532de":"code","2c223034":"code","df5b363a":"code","67027e15":"code","9b154717":"code","56f1293a":"code","84941633":"code","ee3efea7":"code","ccbcfed1":"code","418622ab":"code","c02e3bcc":"code","14ab9d3c":"code","9433c067":"code","bbe7577d":"code","87cd60b9":"code","1970ff72":"code","69974c37":"code","577d6de6":"code","4c071429":"code","1e908fb0":"code","a364e8e3":"code","75cab3f0":"code","81d806ed":"code","e99b3461":"code","95ce5c63":"code","be3f87bb":"code","07bd8c73":"code","02ef3707":"code","e1fc0c1e":"code","24fbf21c":"code","e4302293":"code","ed418625":"code","d3fa862e":"code","e074f849":"code","f4a82806":"code","8343c97a":"code","1699eebc":"code","ef768a36":"code","af8725a8":"code","729a4672":"code","a9a3aa73":"code","dbfdfc0c":"code","c0dd1e29":"code","19300d86":"code","bfc6c6af":"code","e00302aa":"code","55c28f74":"code","b21af3e1":"code","a8351078":"code","81c683ee":"code","de21bc4e":"code","60b751ce":"code","84cd06c0":"code","1cf24ac0":"code","914ac421":"code","4705d3c5":"code","f48a9f6c":"code","6a14019f":"code","76b3c01b":"code","fccba9c4":"code","59d16e61":"code","196eba81":"code","6d3f0d32":"code","9c8e7b9a":"code","cc6d7f5f":"code","49d883a1":"code","4bce8558":"code","6063f620":"code","af4c46bf":"code","42e27203":"code","bc472c34":"code","a39ad58f":"code","f29e31a3":"code","087f3b62":"code","f0bf587e":"code","dcc63c22":"code","53c7e7dc":"code","08f2b53a":"code","59e69b45":"code","ffce7153":"code","48aec8f6":"code","6e51bfbf":"code","5e36a517":"code","b8465649":"markdown","c59cbe18":"markdown","a25530bc":"markdown","6d07defe":"markdown","dfc34519":"markdown","f36b122b":"markdown","d33d6b95":"markdown","1581af51":"markdown","1848b2ac":"markdown","9390317d":"markdown","3e7e015f":"markdown","4a66a74a":"markdown","8705296b":"markdown","4b23143d":"markdown","b35a7524":"markdown","a96716ca":"markdown","3bce2647":"markdown","e1b2d109":"markdown","a8e1aced":"markdown","70312b46":"markdown","2ff50ba1":"markdown","c55db762":"markdown","75a7b324":"markdown","04914462":"markdown","0d3f005e":"markdown","918c5e36":"markdown","d3362b30":"markdown","5d4a2256":"markdown","66a6cd8c":"markdown","65141737":"markdown","bc145296":"markdown","f9c71e9d":"markdown","206309cc":"markdown","7ba33d8d":"markdown","27a01ef4":"markdown","1d3fb8e9":"markdown","8261b07f":"markdown","0af2d62e":"markdown","9b792fd1":"markdown","7bf0ba3d":"markdown","e9931830":"markdown","ec6b4f75":"markdown","62de6310":"markdown","a902b934":"markdown","4e584f8f":"markdown","37087e0e":"markdown","82182f19":"markdown","6464d1b6":"markdown","b87805f7":"markdown","3795d716":"markdown","61c6be10":"markdown","1c813d95":"markdown","95c251f9":"markdown","b6934667":"markdown","3f2e04e0":"markdown","41b58657":"markdown","73d4957e":"markdown","c24b4236":"markdown","080f2f0d":"markdown","cdf6c14a":"markdown","8cb1b1c1":"markdown","a47cd1bf":"markdown","51a1c78c":"markdown","1e7a649c":"markdown","16feae5b":"markdown","5a12137b":"markdown","1236adb8":"markdown","d7c95bba":"markdown","b601b7e5":"markdown","ce34de9a":"markdown","a88a66dc":"markdown","6041f9fa":"markdown","9149e35e":"markdown","2dbb5edb":"markdown","e9ff9179":"markdown","95e2bf3a":"markdown","fc348f54":"markdown","34f07b0a":"markdown","d340ea75":"markdown","79050898":"markdown","addeb586":"markdown","8b3dcd8f":"markdown","34bfe87f":"markdown","2b461232":"markdown","08f560cd":"markdown","74b859ad":"markdown","d94407e7":"markdown","5964f889":"markdown","7d99ee7f":"markdown"},"source":{"20154376":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","53e74156":"# Import libraries\nimport numpy as np\nimport pandas as pd\nimport IPython\n# warning\nfrom sklearn.utils.testing import ignore_warnings\nfrom sklearn.exceptions import ConvergenceWarning\n# statistics\nfrom scipy.stats import skew, kurtosis,probplot,norm\nfrom scipy.cluster.hierarchy import linkage\nfrom scipy.cluster.hierarchy import fcluster\n# plot libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Preprocessing and encoder\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler,OrdinalEncoder, LabelEncoder\n# model evaluation and selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold,ShuffleSplit,StratifiedKFold\nfrom sklearn.model_selection import cross_val_score,cross_val_predict\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nimport optuna\n# Classifiers\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nfrom sklearn.neural_network import MLPClassifier\n# Ensemble Classifiers\nfrom sklearn.ensemble import RandomForestClassifier,RandomForestRegressor,ExtraTreesClassifier,AdaBoostClassifier,GradientBoostingClassifier,BaggingClassifier,VotingClassifier,StackingClassifier\nfrom xgboost.sklearn import XGBClassifier\n# Neural Network\nimport tensorflow as tf\nfrom tensorflow import keras\nimport kerastuner as kt","19484558":"# load dataset\ntitanic_raw_train = pd.read_csv('..\/\/input\/titanic\/train.csv')\ntitanic_raw_test = pd.read_csv('..\/\/input\/titanic\/test.csv')\ntitanic_raw_train.info()\ntitanic_raw_test.info()","cad58d8e":"titanic_raw_train.head()","165cac8a":"titanic_raw_train.describe(include='all')","8bc2142f":"# make a copy for feature engineering\ntrain = titanic_raw_train.copy()\ntest = titanic_raw_test.copy()\ntrain_test = pd.concat([train,test])","9aad2acc":"corr = train.corr()\ncorr","7aeb6a84":"sns.heatmap(data=corr,vmax=1,vmin=-1,center=0,annot=True)","d21e48eb":"train.Pclass.describe()","a36240f3":"# Pclass Distribution by Survived\nsns.catplot(data=train,x='Pclass',hue='Survived', kind=\"count\")","8ad6bca0":"sns.catplot(data=train,x='Pclass',y='Survived', kind=\"point\")","fa51bd9a":"sns.barplot(x = 'Pclass', y = 'Survived', order=[1,2,3], data=train)","e78409aa":"# Fare distribution by Pclass with Survived as label\nsns.catplot(data=titanic_raw_train,x='Pclass',y='Fare',hue='Survived',kind='swarm')","7a297c7b":"train[['Survived','Pclass']].groupby('Pclass').mean()","908b9720":"#looks for strings which lie between A-Z or a-z and followed by a .\ntrain_test['Title']=train_test.Name.str.extract('([A-Za-z]+)\\.') ","7dcb4ce7":"pd.crosstab(train_test.Title, train_test.Survived)","9770ea07":"train_test.Title.replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don','Dona'],\n                         ['Miss','Mrs','Miss','Rare','Rare','Rare','Rare','Rare','Rare','Rare','Rare','Rare','Rare','Rare'],inplace=True)","c3ede96d":"pd.crosstab(train_test.Title, train_test.Survived)","bc0205df":"# Is_married feature based on title\ntrain_test.loc[train_test['Title'] == 'Mrs','Is_Married'] = 1","6aea5459":"# extract family name\ntrain_test['Family_name'] = train_test.Name.str.extract('(\\w+),', expand=False)\ntrain_test.Family_name","43915635":"# Use mean survived rate as Familiy_survived feature \nm = train_test[['Family_name', 'Survived']].groupby('Family_name').mean()\nc = train_test[['Family_name', 'PassengerId']].groupby('Family_name').count()\nm = m.rename(columns={'Survived': 'Family_survived'})\nc = c.rename(columns={'PassengerId': 'FamilyMemberCount'})\n# if family name is unique in all data, set Family_survived as -1\nm = m.where(m.join(c).FamilyMemberCount > 1, other=-1).fillna(-1).join(c)\nm.Family_survived = m.Family_survived.astype('int64')\ntrain_test = train_test.join(m, on='Family_name')","943f9ac7":"train_test.Age.isnull().sum()","c10c7577":"train_test[['Age','Title']].groupby('Title').mean()","35d36703":"# fill in missing ages\ntrain_test.loc[(train_test.Age.isnull())&(train_test.Title=='Mr'),'Age']=32\ntrain_test.loc[(train_test.Age.isnull())&(train_test.Title=='Mrs'),'Age']=37\ntrain_test.loc[(train_test.Age.isnull())&(train_test.Title=='Master'),'Age']=5\ntrain_test.loc[(train_test.Age.isnull())&(train_test.Title=='Miss'),'Age']=22\ntrain_test.loc[(train_test.Age.isnull())&(train_test.Title=='Rare'),'Age']=45","a95bc21b":"train_test['Age'] = train_test['Age'].astype('int64')","b9067e6d":"sns.distplot(a=train_test.Age, kde=True)","b956eb10":"def plot_skew(feature):\n    \"\"\"\n    Function to plot distribution and probability(w.r.t quantiles of normal distribution)\n    \"\"\"\n    fig, axs = plt.subplots(figsize=(20,10),ncols=2)\n    sns.distplot(feature,kde=True,fit=norm,ax=axs[0])\n    # Generates a probability plot of sample data against the quantiles of a specified theoretical distribution (the normal distribution by default).\n    f=probplot(feature, plot=plt)\n    print('Skewness: {:f}'.format(feature.skew()))\n    print('Kurtosis: {:f}'.format(feature.kurtosis()))\nplot_skew(train.Age)","190b8993":"# https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy?scriptVersionId=2051374\nplt.hist(x = [train[train['Survived']==1]['Age'], train[train['Survived']==0]['Age']], \n         stacked=True, color = ['b','r'],label = ['Survived','Dead'])\nplt.title('Age Histogram by Survival')\nplt.xlabel('Age (Years)')\nplt.ylabel('# of Passengers')\nplt.legend()","34136a86":"sns.catplot(x='Sex',data=train,hue = 'Survived', kind='count')","0c7f83bf":"sns.catplot(x=\"Sex\", y=\"Survived\", kind=\"point\", data=train);","87e930df":"# https:\/\/seaborn.pydata.org\/tutorial\/categorical.html#distributions-of-observations-within-categories\nsns.catplot(x=\"Sex\", y=\"Survived\", hue=\"Pclass\", kind=\"point\", data=train);","8ceced50":"train[['Survived','Sex']].groupby('Sex').mean()","88d15ca4":"corr","9b3d25ef":"train.SibSp.describe()","8d37192b":"sns.distplot(a=train.SibSp, kde=False)","ade603bf":"plt.figure(figsize=[10,6])\nplt.subplot(121)\nplt.hist(x = [train[train['Survived']==1]['SibSp'], train[train['Survived']==0]['SibSp']], \n         stacked=True, color = ['b','r'],label = ['Survived','Dead'])\nplt.title('SibSp Histogram by Survival')\nplt.xlabel('# SibSp')\nplt.ylabel('# of Passengers')\nplt.subplot(122)\nplt.hist(x = [train[train['Survived']==1]['Parch'], train[train['Survived']==0]['Parch']], \n         stacked=True, color = ['b','r'],label = ['Survived','Dead'])\nplt.title('Parch Histogram by Survival')\nplt.xlabel('# Parch')\nplt.ylabel('# of Passengers')\nplt.legend()","2c3ccc65":"sns.catplot(x='SibSp',kind='count',hue='Survived', data=train)","b9c532de":"sns.barplot(x = 'SibSp', y = 'Survived', data=train)","2c223034":"fig, saxis = plt.subplots(1, 2,figsize=(16,12))\nsns.barplot(x = 'SibSp', y = 'Survived', data=train,ax=saxis[0])\nsns.barplot(x = 'Parch', y = 'Survived', order=[1,2,3], data=train, ax=saxis[1])","df5b363a":"train_test.Fare.isnull().sum()","67027e15":"train_test.Fare.fillna(test.Fare.median(), inplace=True)","9b154717":"# Fare distribution\nplt.figure(figsize=(10,5))\nsns.distplot(a=titanic_raw_train.Fare, kde=True)","56f1293a":"plot_skew(train_test.Fare)","84941633":"train_test['Fare_log'] = np.log1p(train_test.Fare)\nplot_skew(train_test.Fare_log)","ee3efea7":"# extract ticket numbers \nticket = train_test.Ticket.str.extract('(\\d+$)', expand=False).fillna(0).astype(int).ravel()\n# cluster data from https:\/\/www.kaggle.com\/shaochuanwang\/titanic-ml-tutorial-on-small-dataset-0-82296\/notebook\nZ = linkage(ticket.reshape(train_test.shape[0], 1), 'single')\nclusters = fcluster(Z, 20, criterion='distance')\ntrain_test['Ticket_Code'] = clusters","ccbcfed1":"import itertools\ncount = train_test[['PassengerId', 'Ticket_Code']].groupby('Ticket_Code').count().rename(columns={'PassengerId': 'Number'})\ntrain_test['Ticket_Code_Remap'] = train_test.Ticket_Code.replace(dict(zip(count.index[count.Number <= 10], itertools.cycle([0]))))\nfig, axs = plt.subplots(figsize=(20,20),nrows=2)\nsns.barplot(train_test.Ticket_Code, train_test.Survived,ax=axs[0])\nsns.barplot(train_test.Ticket_Code_Remap, train_test.Survived,ax=axs[1])\n","418622ab":"# ticket frequency\ntrain_test['Ticket_Frequency'] = train_test.groupby('Ticket')['Ticket'].transform('count')\ntrain_test.Ticket_Frequency.astype('int64')","c02e3bcc":"train_test.Cabin.isnull().sum()","14ab9d3c":"# group cabin using area code\ntrain_test['Cabin_code'] = train_test.Cabin.str.get(0).fillna('Z')\ntrain_test[['Survived','Cabin_code']].groupby('Cabin_code').mean()","9433c067":"train.Embarked.isnull().sum()\n","bbe7577d":"train_test.Embarked.fillna(train_test.Embarked.mode()[0], inplace=True)\npd.crosstab(train_test.Embarked, train_test.Survived)","87cd60b9":"sns.catplot(data=train,x='Embarked',y='Survived', kind='point')","1970ff72":"train[['Embarked','Survived']].groupby('Embarked').mean()","69974c37":"train_test['Family_size']=train_test.SibSp+train_test.Parch\ntrain_test['Is_alone']=1\ntrain_test.loc[train_test['Family_size'] > 1,'Is_alone'] = 0\ntrain_test['Family_size'].astype('int64')\ntrain_test['Is_alone'].astype('int64')","577d6de6":"plt.figure(figsize=(10,8))\nsns.catplot(x='Family_size', y='Survived',kind='point',data = train_test)","4c071429":"sns.catplot(x='Is_alone', y='Survived',kind='point',data = train_test)","1e908fb0":"# #choose related data to predict age\n# age_df =train_test[['Age','Fare', 'Family_size', 'Title', 'Pclass','Is_alone','Sex']]\n# age_df_notnull = age_df.loc[train_test['Age'].notnull()]\n# age_df_isnull = age_df.loc[(train_test['Age'].isnull())]\n# Xtr_age = pd.get_dummies(age_df_notnull.drop(columns=['Age']))\n# Xte_age = pd.get_dummies(age_df_isnull.drop(columns=['Age']))\n# Y_age = age_df_notnull.Age\n# # use RandomForestRegression to train data\n# RFR = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n# RFR.fit(Xtr_age,Y_age)\n# predictAges = RFR.predict(t)\n# train_test.loc[train_test['Age'].isnull(), ['Age']]= predictAges\n# RFR.score(Xtr_age,Y_age)","a364e8e3":"fig, axs = plt.subplots(figsize=(10,8),ncols=2)\nsns.distplot(a=train.Age, ax=axs[0])\nsns.distplot(a=train.Fare, ax=axs[1])","75cab3f0":"train_test['Age_band'] = pd.cut(train_test.Age, bins=10, precision=0)","81d806ed":"train_test['Fare_band'] = pd.qcut(train_test.Fare_log, q=13, precision=2)","e99b3461":"fig, axs = plt.subplots(figsize=(28,8),ncols=2)\nsns.pointplot(x='Age_band', y='Survived', data=train_test,ax=axs[0])\nsns.pointplot(x='Fare_band', y= 'Survived', data=train_test,ax=axs[1])","95ce5c63":"MM_scaler = MinMaxScaler(feature_range=(0,1))\ntrain_test[['Age_scaled']]=MM_scaler.fit_transform(train_test[['Age']])\nsns.distplot(a=train_test.Age_scaled)","be3f87bb":"St_scaler = StandardScaler()\ntrain_test[['Fare_scaled']]=St_scaler.fit_transform(train_test[['Fare_log']])\nsns.distplot(a=train_test.Fare_scaled)","07bd8c73":"le = OrdinalEncoder()","02ef3707":"train_test[['Age_code']] = le.fit_transform(train_test[['Age_band']])\ntrain_test[['Fare_code']] = le.fit_transform(train_test[['Fare_band']])","e1fc0c1e":"for row in train_test:\n    train_test.loc[train_test['Family_size']==0, 'Family_type']=1\n    train_test.loc[(1<=train_test['Family_size'])&(train_test['Family_size']<=3), 'Family_type']=2\n    train_test.loc[(3<=train_test['Family_size'])&(train_test['Family_size']<=6), 'Family_type']=3\n    train_test.loc[7<=train_test['Family_size'], 'Family_type']=4\ntrain_test.Family_type = train_test.Family_type.astype('int64')","24fbf21c":"# lb_cabin = LabelEncoder()\n# lb_title = LabelEncoder()\n# lb_embarked = LabelEncoder()\n# train_test.Cabin_code = lb_cabin.fit_transform(train_test.Cabin_code)\n# train_test.Title = lb_title.fit_transform(train_test.Title)\n# train_test.Embarked = lb_embarked.fit_transform(train_test.Embarked)","e4302293":"# encode Sex\ntrain_test.Sex.replace(['female', 'male'], [1,0],inplace=True)","ed418625":"train_test.info()","d3fa862e":"X = pd.get_dummies(train_test[['Pclass','Sex','Title','Family_type',\n                               'Age_code','Fare_code','Embarked',\n                               'Cabin_code','Ticket_Code_Remap','Family_survived']])[:891]\nX_test = pd.get_dummies(train_test[['Pclass','Sex','Title','Family_type',\n                                    'Age_code','Fare_code','Embarked',\n                                    'Cabin_code','Ticket_Code_Remap','Family_survived']])[891:]\nY = train.Survived\nX.shape","e074f849":"sns.heatmap(X.join(Y).corr(),annot=True,cmap='RdYlGn',annot_kws={'size':10})\nfig=plt.gcf()\nfig.set_size_inches(22,22)\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=16)\nplt.show()","f4a82806":"X.shape","8343c97a":"CV = StratifiedKFold(n_splits=5, shuffle=True,random_state=42)\nn_trials = 500# no. of trials during tuning","1699eebc":"# # hyperparameter tuning using GridSearchCV\n# penalty  = ['l1', 'l2','elasticnet', 'none'] # specify the norm used in the penalization\n# C = np.logspace(-2, 2, 10) # 50 nums start form 0.1 to 10 Inverse of regularization strength\n# hyper={'penalty':penalty,'C':C}\n# gd=GridSearchCV(estimator=LogisticRegression(),param_grid=hyper,verbose=2,n_jobs=-1,cv=CV,refit=True,scoring='accuracy')\n# gd.fit(X,Y)\n# print('Best evaluation score:{:.6f}'.format(gd.best_score_))\n# LR_best=gd.best_estimator_\n# print('Best parameters:{}'.format(gd.best_params_))","ef768a36":"# # paramerter tuning using Optuna framework\n# def objective(trial):\n#     # define parameters' sample space and sample type\n#     penalty = trial.suggest_categorical('penalty',['l1', 'l2','elasticnet', 'none'])\n#     if penalty !='none':\n#         C =trial.suggest_loguniform('C', 1e-4, 1e4)\n#         # define classifier\n#         LR_clf = LogisticRegression(penalty=penalty, C=C)\n#     else:\n#         # define classifier\n#         LR_clf = LogisticRegression(penalty=penalty)\n#     # define evaluation matrix as objective to return\n#     score = cross_val_score(LR_clf, X, Y, n_jobs=-1, cv=CV, scoring='roc_auc')\n#     accuracy = score.mean()\n#     return accuracy\n# # create study\n# study = optuna.create_study(sampler=optuna.samplers.TPESampler(),direction='maximize')\n# # run study to find best objective\n# study.optimize(objective, n_trials=200)\n# print('Best model parameters:{} '.format(study.best_params))\n# print('Best score: {:.6f}'.format(study.best_value))","af8725a8":"# LR_param = study.best_params\nLR_param = {'penalty': 'l2', 'C': 109.16587461586346} \nLR_best= LogisticRegression(**LR_param)","729a4672":"# # hyperparameter tuning using GridSearchCV\n# C=[0.4,0.5,0.6,0.8,1,5] # Regularization parameter.\n# gamma=['scale','auto',0.01,0.1,0.2,0.3,0.5,1]\n# kernel=['rbf']\n# degree=[3,5,7] # Degree of the polynomial kernel \n# hyper={'kernel':kernel,'C':C,'gamma':gamma,'degree':degree}\n# gd=GridSearchCV(estimator=SVC(),param_grid=hyper,verbose=5,n_jobs=-1,cv=CV,refit=True,scoring='accuracy')\n# gd.fit(X,Y)\n# print('Best evaluation score:{:.6f}'.format(gd.best_score_))\n# SVM_best=gd.best_estimator_\n# print('Best parameters:{}'.format(gd.best_params_))","a9a3aa73":"# # paramerter tuning using Optuna framework\n# def objective(trial):\n\n#     gamma = trial.suggest_loguniform('gamma',1e-4,1e2) #Kernel coefficient \n#     C =trial.suggest_loguniform('C', 1e-2, 1e3) # Regularization parameter.\n#     kernel = trial.suggest_categorical('kernel',['rbf'])\n#     #degree = trial.suggest_int('degree',1,3)\n#     clf = SVC(gamma=gamma, C=C,kernel=kernel)\n#     score = cross_val_score(clf, X, Y, n_jobs=-1, cv=CV, scoring='roc_auc')\n#     accuracy = score.mean()\n#     return accuracy\n# study = optuna.create_study(sampler=optuna.samplers.TPESampler(),direction='maximize')\n# s=study.optimize(objective, n_trials=n_trials, n_jobs=-1)\n# print('Best model parameters:{} '.format(study.best_params))\n# print('Best score: {:.6f}'.format(study.best_value))","dbfdfc0c":"#SVM_params = study.best_params\nSVM_params = {'gamma': 0.04190895340786603, 'C': 4.615992928303742, 'kernel': 'rbf'}\nSVM_best= SVC(**SVM_params,probability=True)","c0dd1e29":"# criterion=['gini', 'entropy'] #The function to measure the quality of a split.\n# max_depth=[3,5,10,15,20,None]  #The maximum depth of the tree. \n# splitter=['best', 'random']#The strategy used to choose the split at each node.\n# min_samples_split = [1,3,0.01,0.05,0.1] #The minimum num of samples required to split an internal node.\n# min_samples_leaf = [1,3,5,7,0.1] # minimum num of samples required to be a leaf node\n# max_features=['auto'] #The number of features to consider when looking for the best split\n# hyper={'criterion':criterion, 'max_depth':max_depth,'splitter':splitter,\n#        'min_samples_split':min_samples_split,'max_features':max_features,\n#       'min_samples_leaf':min_samples_leaf}\n# gd=GridSearchCV(estimator=DecisionTreeClassifier(random_state=42),param_grid=hyper,\n#                 verbose=2,n_jobs=-1,cv=CV,refit=True,scoring = 'accuracy')\n# gd.fit(X,Y)\n# print('Best evaluation score:{:.6f}'.format(gd.best_score_))\n# DT_best=gd.best_estimator_\n# print('Best parameters:{}'.format(gd.best_params_))","19300d86":"# # paramerter tuning using Optuna framework\n# def objective(trial):\n\n#     max_depth = trial.suggest_int(\"max_depth\", 2, 32,5) #The maximum depth of the tree.\n#     min_samples_split = trial.suggest_int('min_samples_split', 20,200,20) ##The minimum num of samples required to split an internal node.\n#     min_samples_leaf = trial.suggest_int('min_samples_leaf', 20, 200,20) # minimum num of samples required to be a leaf node\n#     clf = DecisionTreeClassifier(min_samples_split=min_samples_split, max_depth=max_depth, \n#                                    min_samples_leaf=min_samples_leaf, random_state=42)\n#     score = cross_val_score(clf, X, Y, n_jobs=-1, cv=CV, scoring='roc_auc')\n#     accuracy = score.mean()\n#     return accuracy\n# study = optuna.create_study(sampler=optuna.samplers.TPESampler(),direction='maximize')\n# s=study.optimize(objective, n_trials=n_trials, n_jobs=-1)\n# print('Best model parameters:{} '.format(study.best_params))\n# print('Best score: {:.6f}'.format(study.best_value))","bfc6c6af":"# DT_param =study.best_params\nDT_param ={'max_depth': 12, 'min_samples_split': 60, 'min_samples_leaf': 20}\nDT_best= DecisionTreeClassifier(**DT_param)","e00302aa":"# Visualize Decision Tree\nDT_best.fit(X,Y)\nfig = plt.figure(figsize=(30,30))\n_ = tree.plot_tree(DT_best, \n                   feature_names=X.columns,\n                   filled=True)","55c28f74":"# # Grid Search hyperparameter tunning\n# n_neighbors=[5,10,15,20,100,200] #Number of neighbors \n# weights=['uniform','distance'] # weight function used in prediction\n# p=[1,2] #Power parameter for the Minkowski metric.\n# hyper={'n_neighbors':n_neighbors,'weights':weights,'p':p}\n# gd=GridSearchCV(estimator=KNeighborsClassifier(),param_grid=hyper,cv=CV,refit=True,verbose=2,n_jobs=-1,scoring = 'accuracy')\n# gd.fit(X,Y)\n# print('Best evaluation score:{:.6f}'.format(gd.best_score_))\n# KNN_best=gd.best_estimator_\n# print('Best parameters:{}'.format(gd.best_params_))","b21af3e1":"# # paramerter tuning using Optuna framework\n# def objective(trial):\n#     # define parameters' sample space and sample type\n#     n_neighbors=trial.suggest_int('n_neighbors',5,305,50) #Number of neighbors \n#     weights=trial.suggest_categorical('weights',['uniform','distance']) # weight function used in prediction\n#     p=trial.suggest_int('p',1,2) #Power parameter for the Minkowski metric.\n#     # define classifier\n#     clf = KNeighborsClassifier(n_neighbors=n_neighbors,weights=weights,p=p,n_jobs=-1)\n#     # define evaluation matrix as objective to return\n#     score = cross_val_score(clf, X, Y, n_jobs=-1, cv=CV, scoring='roc_auc')\n#     accuracy = score.mean()\n#     return accuracy\n# # create study\n# study = optuna.create_study(direction='maximize',sampler=optuna.samplers.TPESampler())\n# # run study to find best objective\n# study.optimize(objective, n_trials=n_trials,n_jobs=-1)\n# print('Best model parameters:{} '.format(study.best_params))\n# print('Best score: {:.6f}'.format(study.best_value))","a8351078":"# KNN_param = study.best_params\nKNN_param ={'n_neighbors': 5, 'weights': 'uniform', 'p': 1}\nKNN_best= KNeighborsClassifier(**KNN_param)","81c683ee":"# # network builder\n# def model_builder(hp):\n#     model = keras.Sequential()\n#     # Dense layer\n#     for i in range(hp.Int('layers',1,2)):\n#         model.add(keras.layers.Dense(units=hp.Int('units_{:}'.format(i),min_value=32, max_value=512,step=32), \n#                                      activation = hp.Choice('actv_{:}'.format(i),['relu','tanh'])))\n#     # output layer\n#     model.add(keras.layers.Dense(1,activation='sigmoid'))\n#     # model config\n#     model.compile('adam', 'binary_crossentropy',metrics=[keras.metrics.AUC(name='auc')])\n#     return model","de21bc4e":"# # build tuner\n# tuner = kt.Hyperband(model_builder, \n#                     objective = kt.Objective('val_auc','max'),\n#                     max_epochs=20,\n#                     factor = 3,\n#                     directory = 'keras_logs',\n#                     project_name = 'NN')\n# class ClearTrainingOutput(tf.keras.callbacks.Callback):\n#     def on_train_batch_end(*args,**kwargs):\n#         IPython.display.clear_output(wait=True)\n# # start tuning\n# tuner.search(X,Y,epochs = 20, validation_split=0.2,callbacks=[ClearTrainingOutput()])","60b751ce":"# NN_best_params = tuner.get_best_hyperparameters(num_trials=1)[0]\n# NN_best_model = tuner.hypermodel.build(NN_best_params)","84cd06c0":"# # Grid Search to tune parameters\n# hidden_layer_sizes=[(40,40),(80,80),(100,),(40),(80),(120)] # the number of neurons in the ith hidden layer\n# activation=['identity', 'logistic', 'tanh', 'relu'] # activation function\n# solver=['lbfgs','sgd','adam'] #The solver for weight optimization.\n# alpha=[0.0001,0.001,0.01,.1] #L2 penalty (regularization term) parameter.\n# learning_rate=['constant','invscaling','adaptive']# Learning rate schedule for weight updates.\n# hyper={'hidden_layer_sizes':hidden_layer_sizes,'activation':activation,\n#        'solver':solver,'alpha':alpha,'learning_rate':learning_rate}\n# gd=GridSearchCV(estimator=MLPClassifier(random_state=42,early_stopping=True),param_grid=hyper,\n#                 cv=CV,refit=True,verbose=2,n_jobs=-1,scoring = 'accuracy')\n# gd.fit(X,Y)\n# print('Best evaluation score:{:.6f}'.format(gd.best_score_))\n# MLP_best=gd.best_estimator_\n# print('Best parameters:{}'.format(gd.best_params_))","1cf24ac0":"# # paramerter tuning using Optuna framework\n# def objective(trial):\n#     # define parameters' sample space and sample type\n#     n_layers = trial.suggest_int('n_layers', 1, 2) # no. of hidden layers \n#     layers = []\n#     for i in range(n_layers):\n#         layers.append(trial.suggest_int(f'n_units_{i+1}', 10, 210,50)) # no. of hidden unit\n#     activation=trial.suggest_categorical('activation',['logistic', 'tanh', 'relu']) # activation function \n#     alpha=trial.suggest_loguniform('alpha',0.0001,50) #L2 penalty (regularization term) parameter.\n#     # define classifier\n#     clf = MLPClassifier(random_state=42,\n#                         solver='adam',\n#                         early_stopping=True,\n#                         activation=activation,\n#                         alpha=alpha,\n#                         learning_rate='adaptive',\n#                         learning_rate_init=0.01,\n#                         batch_size=32,\n#                         hidden_layer_sizes=(layers))\n#     # define evaluation matrix as objective to return\n#     score = cross_val_score(clf, X, Y, n_jobs=-1, cv=CV)\n#     accuracy = score.mean()\n#     return accuracy\n# # create study\n# study = optuna.create_study(direction='maximize',sampler=optuna.samplers.TPESampler())\n# # run study to find best objective\n# study.optimize(objective,n_trials =500,n_jobs=-1)\n# print('Best model parameters:{} '.format(study.best_params))\n# print('Best score: {:.6f}'.format(study.best_value))","914ac421":"# MLP_param =study.best_params\nMLP_best= MLPClassifier(random_state=42,\n                        solver='adam',\n                        early_stopping=True,\n                        activation='relu',\n                        alpha= 0.0002746340910250398,\n                        learning_rate='adaptive',\n                        learning_rate_init=0.01,\n                        batch_size=32,\n                        hidden_layer_sizes=160)","4705d3c5":"# # Grid Search\n# n_estimators=[40,50,60,500] #The number of trees in the forest.\n# # criterion=['gini', 'entropy']#The function to measure the quality of a split. \n# max_depth=[3,4,5,7]#The maximum depth of the tree.\n# min_samples_split = [3,0.01,0.05,0.1] #The minimum num of samples required to split an internal node.\n# min_samples_leaf = [3,5,7,0.1] # minimum num of samples required to be a leaf node\n# max_features=['auto'] #The number of features to consider when looking for the best split\n# # oob_score=['True','False']\n# hyper={'n_estimators':n_estimators, 'max_depth':max_depth,\n#       'min_samples_split':min_samples_split,'max_features':max_features,\n#        'min_samples_leaf':min_samples_leaf}\n# gd=GridSearchCV(estimator=RandomForestClassifier(random_state=42,oob_score=True,criterion='gini'),\n#                 param_grid=hyper,verbose=2,n_jobs=-1,cv=CV,refit=True,scoring = 'accuracy')\n# gd.fit(X,Y)\n# print('Best evaluation score:{:.6f}'.format(gd.best_score_))\n# RF_best=gd.best_estimator_\n# print('Best parameters:{}'.format(gd.best_params_))","f48a9f6c":"# # paramerter tuning using Optuna framework\n# def objective(trial):\n#     # define parameters' sample space and sample type\n#     n_estimators=trial.suggest_int('n_estimators',50,500,50) #The number of trees in the forest.\n#     max_depth=trial.suggest_int('max_depth',1,5,1)#The maximum depth of the tree.\n#     min_samples_split = trial.suggest_int('min_samples_split',20,200,20) #The minimum num of samples required to split an internal node.\n#     min_samples_leaf = trial.suggest_int('min_samples_leaf',20,200,20) # minimum num of samples required to be a leaf node\n#     # define classifier\n#     clf = RandomForestClassifier(random_state=42,\n#                                 criterion='gini',\n#                                 oob_score=True,\n#                                 max_depth=max_depth,\n#                                 min_samples_split=min_samples_split,\n#                                 min_samples_leaf=min_samples_leaf,\n#                                 n_jobs=-1)\n#     # define evaluation matrix as objective to return\n#     score = cross_val_score(clf, X, Y, n_jobs=-1, cv=CV, scoring='roc_auc')\n#     accuracy = score.mean()\n#     return accuracy\n# # create study\n# study = optuna.create_study(direction='maximize',sampler=optuna.samplers.TPESampler())\n# # run study to find best objective\n# study.optimize(objective,n_trials=n_trials,n_jobs=-1)\n# print('Best model parameters:{} '.format(study.best_params))\n# print('Best score: {:.6f}'.format(study.best_value))","6a14019f":"# RF_param =study.best_params\nRF_param={'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 40, 'min_samples_leaf': 20}\nRF_best= RandomForestClassifier(random_state=42,\n                                criterion='gini',\n                                oob_score=True,\n                                **RF_param)","76b3c01b":"# # Grid Search\n# n_estimators=[40,50,60,500] #The number of trees in the forest.\n# # criterion=['gini', 'entropy']#The function to measure the quality of a split. \n# max_depth=[3,4,5,7,10]#The maximum depth of the tree.\n# min_samples_split = [3,0.01,0.05,0.1] #The minimum num of samples required to split an internal node.\n# min_samples_leaf = [3,5,7,0.1] # minimum num of samples required to be a leaf node\n# max_features=['auto'] #The number of features to consider when looking for the best split\n# hyper={'n_estimators':n_estimators, 'max_depth':max_depth,\n#       'min_samples_split':min_samples_split,'max_features':max_features}\n# gd=GridSearchCV(estimator=ExtraTreesClassifier(random_state=42,criterion='gini'),\n#                 param_grid=hyper,verbose=2,n_jobs=-1,cv=CV,refit=True,scoring = 'accuracy')\n# gd.fit(X,Y)\n# print('Best evaluation score:{:.6f}'.format(gd.best_score_))\n# RF_best=gd.best_estimator_\n# print('Best parameters:{}'.format(gd.best_params_))","fccba9c4":"# # paramerter tuning using Optuna framework\n# def objective(trial):\n#     # define parameters' sample space and sample type\n#     n_estimators=trial.suggest_int('n_estimators',50,500,50) #The number of trees in the forest.\n#     max_depth=trial.suggest_int('max_depth',1,5,1)#The maximum depth of the tree.\n#     min_samples_split = trial.suggest_int('min_samples_split',20,200,20) #The minimum num of samples required to split an internal node.\n#     min_samples_leaf = trial.suggest_int('min_samples_leaf',20,200,20) # minimum num of samples required to be a leaf node\n#     # define classifier\n#     clf = ExtraTreesClassifier( random_state=42,\n#                                 criterion='gini',\n#                                 max_depth=max_depth,\n#                                 min_samples_split=min_samples_split,\n#                                 min_samples_leaf=min_samples_leaf,\n#                                 n_jobs=-1)\n#     # define evaluation matrix as objective to return\n#     score = cross_val_score(clf, X, Y, n_jobs=-1, cv=CV,scoring = 'roc_auc')\n#     accuracy = score.mean()\n#     return accuracy\n# # create study\n# study = optuna.create_study(direction='maximize',sampler=optuna.samplers.TPESampler())\n# # run study to find best objective\n# study.optimize(objective,n_trials=n_trials,n_jobs=-1)\n# print('Best model parameters:{} '.format(study.best_params))\n# print('Best score: {:.6f}'.format(study.best_value))","59d16e61":"# ET_params = study.best_params\nET_params = {'n_estimators': 250, 'max_depth': 5, 'min_samples_split': 20, 'min_samples_leaf': 20} \nET_best = ExtraTreesClassifier( random_state=42,\n                                criterion='gini',\n                                n_jobs=-1,\n                               **ET_params)","196eba81":"# # Grid Search parameter tuning\n# n_estimators=[5,10,20,50,100]\n# base_estimator__C=[0.4,0.8,1,2,5] # Regularization parameter.\n# base_estimator__gamma=['scale','auto',0.01,0.1,0.5,1]\n# base_estimator__kernel=['rbf']\n# # base_estimator__degree=[3,5,7] # Degree of the polynomial kernel \n# hyper={'n_estimators':n_estimators,\n#        'base_estimator__C':base_estimator__C,\n#        'base_estimator__gamma':base_estimator__gamma,\n#        'base_estimator__kernel':base_estimator__kernel}\n# gd=GridSearchCV(estimator=BaggingClassifier(base_estimator=SVC(),random_state=42),\n#                 param_grid=hyper,verbose=2,n_jobs=-1,cv=CV,refit=True,scoring = 'accuracy')\n# gd.fit(X,Y)\n# print('Best evaluation score:{:.6f}'.format(gd.best_score_))\n# SVMB_best=gd.best_estimator_\n# print('Best parameters:{}'.format(gd.best_params_))","6d3f0d32":"# # paramerter tuning using Optuna framework\n# def objective(trial):\n#     # define parameters' sample space and sample type\n#     n_estimators=trial.suggest_int('n_estimators',50,500,50) #The number of base estimators\n#     base_estimator__C=trial.suggest_loguniform('base_estimator__C',1e-5,1e3)# Regularization parameter.\n#     base_estimator__gamma = trial.suggest_loguniform('base_estimator__gamma',1e-5,1e3) # kenerl coefficient\n#     # define classifier\n#     clf = BaggingClassifier(base_estimator=SVC(C=base_estimator__C,\n#                                                gamma=base_estimator__gamma,\n#                                                kernel = 'rbf'),\n#                             random_state=42,\n#                             n_jobs=-1)\n#     # define evaluation matrix as objective to return\n#     score = cross_val_score(clf, X, Y, n_jobs=-1, cv=CV,scoring = 'roc_auc')\n#     accuracy = score.mean()\n#     return accuracy\n# # create study\n# study = optuna.create_study(direction='maximize',sampler=optuna.samplers.TPESampler())\n# # run study to find best objective\n# study.optimize(objective,n_trials=n_trials,n_jobs=-1)\n# print('Best model parameters:{} '.format(study.best_params))\n# print('Best score: {:.6f}'.format(study.best_value))","9c8e7b9a":"# SVMB_params = study.best_params\nSVMB_params = {'n_estimators': 300, 'base_estimator__C': 3.232901108594473, 'base_estimator__gamma': 0.07183110256410177}  \nSVMB_best = BaggingClassifier(base_estimator=SVC(kernel = 'rbf',probability=True,C=3.232901108594473,gamma=0.07183110256410177),\n                              random_state=42,n_jobs=-1,n_estimators=350)","cc6d7f5f":"# n_estimators=list(range(30,300,10))\n# learn_rate=[0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2]\n# base_estimator__max_depth=[1,2,3]\n# hyper={'n_estimators':n_estimators,'learning_rate':learn_rate,\n#       'base_estimator__max_depth':base_estimator__max_depth,}\n# gd=GridSearchCV(estimator=AdaBoostClassifier(base_estimator=DecisionTreeClassifier(criterion='gini'),\n#                                              random_state=42),param_grid=hyper,verbose=2,\n#                                             cv=CV,refit=True,n_jobs=-1,scoring = 'accuracy')\n# gd.fit(X,Y)\n# print('Best evaluation score:{:.6f}'.format(gd.best_score_))\n# ADB_best=gd.best_estimator_\n# print('Best parameters:{}'.format(gd.best_params_))","49d883a1":"# # paramerter tuning using Optuna framework\n# def objective(trial):\n#     # define parameters' sample space and sample type\n#     n_estimators=trial.suggest_int('n_estimators',40,500,20) #The number of base estimators\n#     learn_rate=trial.suggest_loguniform('learn_rate',1e-5,0.1)\n#     base_estimator__max_depth=trial.suggest_int('base_estimator__max_depth',1,5,1)\n#     # define classifier\n#     clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=base_estimator__max_depth,\n#                                                                   criterion='gini'),\n#                             learning_rate=learn_rate,\n#                             n_estimators=n_estimators,\n#                             random_state=42)\n#     # define evaluation matrix as objective to return\n#     score = cross_val_score(clf, X, Y, n_jobs=-1, cv=CV,scoring = 'roc_auc')\n#     accuracy = score.mean()\n#     return accuracy\n# # create study\n# study = optuna.create_study(direction='maximize',sampler=optuna.samplers.TPESampler())\n# # run study to find best objective\n# study.optimize(objective,n_trials=n_trials,n_jobs=-1)\n# print('Best model parameters:{} '.format(study.best_params))\n# print('Best score: {:.6f}'.format(study.best_value))","4bce8558":"# ADB_param =study.best_params\nADB_best= AdaBoostClassifier(base_estimator=DecisionTreeClassifier(criterion='gini',\n                                                                  max_depth=2),\n                            learning_rate=0.03990900089241141 ,\n                            n_estimators=160,\n                            random_state=42)","6063f620":"# # Grid Search parameter tunning\n# n_estimators=list(range(20,150,10))\n# learn_rate=[0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2]\n# max_depth=[3,5,8,10]\n# min_samples_split=np.linspace(0.1, 0.3, 4)\n# criterion=['friedman_mse','mae']\n# max_features=['log2','sqrt']\n# hyper={'n_estimators':n_estimators,\n#        'learning_rate':learn_rate,\n#        'max_depth':max_depth,\n#        'criterion':criterion,\n#        'min_samples_split':min_samples_split,\n#        'max_features':max_features}\n# gd=GridSearchCV(estimator=GradientBoostingClassifier(random_state=42),\n#                 param_grid=hyper,verbose=2,refit=True,n_jobs=-1,\n#                cv=CV,scoring = 'accuracy')\n# gd.fit(X,Y)\n# print('Best evaluation score:{:.6f}'.format(gd.best_score_))\n# GDB_best=gd.best_estimator_\n# print('Best parameters:{}'.format(gd.best_params_))\n","af4c46bf":"# # paramerter tuning using Optuna framework\n# def objective(trial):\n#     # define parameters' sample space and sample type\n#     n_estimators=trial.suggest_int('n_estimators',40,500,20) #The number of base estimators\n#     learning_rate=trial.suggest_loguniform('learning_rate',1e-5,0.1)\n#     max_depth=trial.suggest_int('max_depth',1,5,1)\n#     min_samples_split=trial.suggest_int('min_samples_split',20,200,20)\n#     min_samples_leaf=trial.suggest_int('min_samples_leaf',20,200,20)\n#     # define classifier\n#     clf = GradientBoostingClassifier(max_depth=max_depth,\n#                                      min_samples_split=min_samples_split,\n#                                      min_samples_leaf=min_samples_leaf,\n#                                      learning_rate=learning_rate,\n#                                      n_estimators=n_estimators,\n#                                      subsample=0.8,\n#                                      n_iter_no_change=10,\n#                                      random_state=42)\n#     # define evaluation matrix as objective to return\n#     score = cross_val_score(clf, X, Y, n_jobs=-1, cv=CV,scoring = 'roc_auc')\n#     accuracy = score.mean()\n#     return accuracy\n# # create study\n# study = optuna.create_study(direction='maximize',sampler=optuna.samplers.TPESampler())\n# # run study to find best objective\n# study.optimize(objective,n_trials=n_trials,n_jobs=-1)\n# print('Best model parameters:{} '.format(study.best_params))\n# print('Best score: {:.6f}'.format(study.best_value))","42e27203":"# GDB_param = study.best_params\nGDB_param = {'n_estimators': 420, 'learning_rate': 0.08199591231901683, \n             'max_depth': 3, 'min_samples_split': 140, 'min_samples_leaf': 20} \nGDB_best= GradientBoostingClassifier(**GDB_param,\n                                     subsample=0.8,\n                                     n_iter_no_change=10,\n                                     random_state=42)","bc472c34":"# # Grid Search hyperparameter tuning\n# #tree features\n# max_depth=[5,8]\n# # subsamples=[0.8,0.5]# the fraction of observations to be randomly samples for each tree.\n# colsample_bytree=[0.5,0.8]# the fraction of columns to be randomly samples for each tree.\n# gamma = [0.1,0.3,0.5] #Minimum loss reduction required to make a further partition on a leaf node of the tree.\n# min_child_weight =[1,3,5] # Minimum sum of instance weight(hessian) needed in a child.\n# # boosting features\n# n_estimators=[25,50,90,120] #Number of gradient boosted trees(rounds).\n# learning_rate =[0.01, 0.25, 0.1]\n# # learning_rate =[0.01, 0.025, 0.05, 0.1, 0.15, 0.2]# Boosting learning rate (xgb\u2019s \u201ceta\u201d), typically in [0.01,0.2].\n# booster=['dart'] # tree booster always better than linear.\n# # regularization\n# #reg_alpha=[1e-5, 1e-2, 0.1] # L1 regularization term on weights\n# reg_lambda =[1e-5, 1e-2] # L2 regularization term on weights\n# hyper={'n_estimators':n_estimators,\n#        'learning_rate':learn_rate,\n#        'booster':booster,\n#        'max_depth':max_depth,\n#        'colsample_bytree':colsample_bytree,\n#        'gamma':gamma,\n#        'min_child_weight':min_child_weight,\n#        'reg_lambda':reg_lambda} \n# gd=GridSearchCV(estimator=XGBClassifier(verbosity=1,n_jobs =-1,random_state=42),\n#                 param_grid=hyper,verbose=2,refit=True,n_jobs=-1,\n#                cv=CV,scoring = 'accuracy')\n# gd.fit(X,Y)\n# print('Best evaluation score:{:.6f}'.format(gd.best_score_))\n# XGB_best=gd.best_estimator_\n# print('Best parameters:{}'.format(gd.best_params_))\n","a39ad58f":"# # hyperparameter tuning using optuna framework\n# def objective(trial):\n#     # define sample space and distibution of parameters\n#     max_depth = trial.suggest_int(\"max_depth\", 1,5,1)\n#     n_estimators = trial.suggest_int(\"n_estimators\", 40, 500, 20)\n#     booster = trial.suggest_categorical('booster',['gbtree','gblinear','dart'])\n#     min_child_weight = trial.suggest_int('min_child_weight',5,105,10)\n#     learning_rate = trial.suggest_loguniform('learning_rate', 1e-6,1e-3)\n#     gamma = trial.suggest_loguniform('gamma', 0.00001, 100)\n#     reg_alpha = trial.suggest_loguniform('reg_alpha',1e-3,1e2) # L1 regularization term on weights.\n#     reg_lambda = trial.suggest_loguniform('reg_lambda',1e-3,1e2) # L2 regularization term on weights.\n#     colsample_bytree = trial.suggest_discrete_uniform('colsample_bytree',0.4,0.8,0.2) # sub-features to use \n#     subsample = trial.suggest_discrete_uniform('subsample',0.8,1.0,0.1) # subsamples to use\n#     # define classifier\n#     clf = XGBClassifier(objective='binary:logistic',\n#                         booster = booster,\n#                         subsample=subsample,\n#                         colsample_bytree=colsample_bytree,\n#                         learning_rate=learning_rate,\n#                         n_estimators=n_estimators,\n#                         max_depth=max_depth,\n#                         gamma=gamma,\n#                         reg_alpha=reg_alpha,\n#                         reg_lambda=reg_lambda,\n#                         n_jobs=-1,\n#                         random_state=42)\n#     # defin evaluation matrix\n#     score = cross_val_score(clf, X, Y, n_jobs=-1, cv=CV,scoring = 'roc_auc')\n#     accuracy = score.mean()\n#     return accuracy\n# # define optimizor's direction and sample algorithms \n# study = optuna.create_study(direction='maximize',sampler=optuna.samplers.TPESampler())\n# # run optimizor with n_trials to find best parameters\n# s=study.optimize(objective, n_trials=500, n_jobs=-1)\n# print('Best model parameters:{} '.format(study.best_params))\n# print('Best score: {:.6f}'.format(study.best_value))\n    ","f29e31a3":"# XGB_param = study.best_params\nXGB_param = {'max_depth': 5, 'n_estimators': 500, 'booster': 'dart', \n             'min_child_weight': 45, 'learning_rate': 0.00028818174062883895, \n             'gamma': 0.0701754028803822, 'reg_alpha': 0.09673762960851098, 'reg_lambda': 0.020973617864068886, \n             'colsample_bytree': 0.6000000000000001, 'subsample': 1.0} \nXGB_best= XGBClassifier(**XGB_param,\n                        objective='binary:logistic',\n                        n_jobs=-1,\n                        random_state=42)","087f3b62":"@ignore_warnings(category=ConvergenceWarning)\ndef model_eval():\n    acc_mean = []\n    std = []\n    acc = []\n    classifiers=['Svm','LR','KNN','DT','MLP','RF','ADB','ET','XGB','GDB','SVMB']\n    models=[SVM_best,LR_best,KNN_best,DT_best,MLP_best,RF_best,ADB_best,ET_best,XGB_best,GDB_best,SVMB_best]\n    for model in models:\n        cv_result = cross_val_score(model,X,Y, cv = 5,scoring = \"roc_auc\")\n        acc_mean.append(cv_result.mean())\n        std.append(cv_result.std())\n        acc.append(cv_result)\n    performance_df=pd.DataFrame({'CV_Mean':acc_mean,'Std':std},index=classifiers)\n    return performance_df\n\nperformance_df = model_eval()\nperformance_df.sort_values(by=['CV_Mean'],ascending=False)","f0bf587e":"votingC = VotingClassifier(estimators=[('XGB', XGB_best),\n                                       ('RF',RF_best),\n                                       ('ADB',ADB_best),\n                                       ('ET', ET_best),\n                                       ('GDB',GDB_best),\n                                       ('DT',DT_best)],\n                           voting='hard', n_jobs=-1,verbose=True)\nvotingC.fit(X,Y)\n\ncv_result = cross_val_score(votingC,X,Y, cv = 5,scoring = \"accuracy\")\nvot_acc = cv_result.mean()\nvot_std = cv_result.std()\nperformance_df.loc['Voting'] = {'CV_Mean':vot_acc, 'Std':vot_std}\nperformance_df.sort_values(by=['CV_Mean'],ascending=False)","dcc63c22":"stacking = StackingClassifier(estimators=[('XGB', XGB_best),\n                                       ('RF',RF_best),\n                                       ('ADB',ADB_best),\n                                       ('ET', ET_best),\n                                       ('GDB',GDB_best),\n                                       ('DT',DT_best)],\n                             final_estimator=LogisticRegression(),\n                             cv=5,\n                             n_jobs=-1)\ncv_result = cross_val_score(stacking,X,Y, cv = 5,scoring = \"accuracy\")\nstk_acc = cv_result.mean()\nstk_std = cv_result.std()\nperformance_df.loc['Stacking'] = {'CV_Mean':stk_acc, 'Std':stk_std}\nperformance_df.sort_values(by=['CV_Mean'],ascending=False)","53c7e7dc":"rf = XGB_best\nrf.fit(X,Y)\nfeatures = pd.DataFrame()\nfeatures['feature'] = X.columns\nfeatures['importance'] = rf.feature_importances_\nfeatures.sort_values(by=['importance'], ascending=False, inplace=True)\nfeatures.set_index('feature', inplace=True)\nfeatures","08f2b53a":"# split data\nX_tr, X_te, Y_tr, Y_te = train_test_split(X,Y)\n# fit model\nclf = RF_best.fit(X_tr,Y_tr)\n# find misclassified data in test set\nmis_df = X_te[np.logical_xor(Y_te,clf.predict(X_te))] \n# show orginal data\nmis_df=train.loc[mis_df.index]\nmis_df.describe()","59e69b45":"titanic_raw_train.loc[mis_df.index]","ffce7153":"performance_df.CV_Mean.idxmax()","48aec8f6":"sub_model = RF_best","6e51bfbf":"features = pd.DataFrame()\nfeatures['feature'] = X.columns\nfeatures['importance'] = sub_model.feature_importances_\nfeatures.sort_values(by=['importance'], ascending=False, inplace=True)\nfeatures.set_index('feature', inplace=True)\nfeatures","5e36a517":"clf = RF_best.fit(X,Y)\nsub = clf.predict(X_test)\nsub_pd = pd.DataFrame({'PassengerId':titanic_raw_test.PassengerId,'Survived':sub})\nsub_pd.to_csv('submit.csv' ,index=False)","b8465649":"Ordinal features include: Parch, Age, Fare, Family_size. Age and Fare should be encoded by their band.","c59cbe18":"Set missing age as the mean of age for its corresponding title.","a25530bc":"### Fare","6d07defe":"***Observation:***\nMost people brought 3rd class(over 75%).Most people in 3rd class died. More than half people in 1st class survived!","dfc34519":"## Feature Engineer and Encoding\n","f36b122b":"Based my submission score, Random Forest and Voting Classifier has the best score on public leaderboard.","d33d6b95":"### Ticket","1581af51":"### Build data","1848b2ac":"500 trials running result:  \nBest model parameters:{'gamma': 0.04190895340786603, 'C': 4.615992928303742, 'kernel': 'rbf'}   \nBest score: 0.875871  ","9390317d":"### Cabin\n","3e7e015f":"#### Adaboost","4a66a74a":"#### Scale continuous features","8705296b":"### Sex\n","4b23143d":"Use label encoder to encode Cabin, because it doesn't show great importance in our model. Since we need to decrease the number of feature to avoid overfitting, one hot encoder will not be considered for this feature.","b35a7524":"***Observation:***\nThere are 11 columns in total except for \"PassengerId\" as index. Some are numerical, others are ordinal or categorical. \"Survived\" is the column we want to predic. Let's dive into them one by one. ","a96716ca":"***Attention:***  \nIt's not always safe to combine train and test set and do feature engineering together because this will cause data leakage. But if you want a higher score on test data, it's a good option. In real life, we learn everything only from features in training set and apply the same preprocessing or feature engineering in test set. Otherwise, your model might be overfiting on test set and have a poor performance on unseen data.","3bce2647":"### Embarked","e1b2d109":"### Age","a8e1aced":"Remember that we find out the relation between SibSp and Parch, we now create two new features: 'Family_size' and 'Is_alone' based on those.https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy","70312b46":"**500 trials running result:**  \nBest model parameters:{'penalty': 'l2', 'C': 109.16587461586346}   \nBest score: 0.883486","2ff50ba1":"Group rare titles, and replace french words('Mlle','Mme').","c55db762":"# Reference: \n * https:\/\/www.kaggle.com\/shaochuanwang\/titanic-ml-tutorial-on-small-dataset-0-82296\/notebook\n * https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions\n * https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy\n * https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial","75a7b324":"#### Voting classifier","04914462":"### Evaluation ","0d3f005e":"#### Voting and Stacking","918c5e36":"## Load data","d3362b30":"500 trials running result:  \nBest model parameters:{'n_estimators': 300, 'base_estimator__C': 3.232901108594473, 'base_estimator__gamma': 0.07183110256410177}   \nBest score: 0.870113","5d4a2256":"Use one hot encoding to encode Sex and Title features.","66a6cd8c":"500 Trials running results:\nBest model parameters:{'n_estimators': 420, 'learning_rate': 0.08199591231901683, 'max_depth': 3, 'min_samples_split': 140, 'min_samples_leaf': 20}   \nBest score: 0.910832","65141737":"#### Learn from misclassified data","bc145296":"500 trials running result:  \nBest model parameters:{'max_depth': 12, 'min_samples_split': 60, 'min_samples_leaf': 20}   \nBest score: 0.888831","f9c71e9d":"## Import Libraries","206309cc":"#### KNN\n","7ba33d8d":"***Observation:***\nSibSp and Parch are related to each other and has a similiar distribution on Survived.","27a01ef4":"Fill missing value with most frequent value.","1d3fb8e9":"## EDA","8261b07f":"Overall correlation between features.\n","0af2d62e":"### Base Model","9b792fd1":"### Binning continuous features","7bf0ba3d":"#### SVM","e9931830":"Create 5 stratified folds on training set to find the best hyperparameters for each model. The folds are made by preserving the percentage of samples for each class. ","ec6b4f75":"500 trials running result:  \nBest model parameters:{'n_neighbors': 5, 'weights': 'uniform', 'p': 1}   \nBest score: 0.846715","62de6310":"#### Decision Tree","a902b934":"***Observation:***\nFemale has much higher surviving rate than male.","4e584f8f":"Cast 'Age' into 'int'.","37087e0e":"500 trials running results:  \nBest model parameters:{'n_estimators': 160, 'learn_rate': 0.03990900089241141, 'base_estimator__max_depth': 2}   \nBest score: 0.913918","82182f19":"#### **Bagging**","6464d1b6":"### Ensembling","b87805f7":"#### Logistic Regression","3795d716":"500 Trials running results:  \nBest model parameters:{'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 40, 'min_samples_leaf': 20}   \nBest score: 0.881499","61c6be10":"#### Boosting","1c813d95":"#### XGBoost","95c251f9":"***Observation:***\nPassengers embarked from 'Cherbourg' have higher survival rate.","b6934667":"### Change Log  \n* 2020\/9\/6  \n1. Add MLP classifier and SVM bagging.  \n* 2020\/9\/7  \n 1. Analyze feature importance. Find that after binning continuous features('Age' and 'Fare'), feature importance dropped a lot.Other than binning, add scaled continuous features.  \n 2.  Increase number of bins.\n 3. Instead of using 'Title' mean, use random forest to predict missing value.(forget to deploy on test data!)  \n* 2020\/9\/8  \n 1.  Add skewness analysis.\n 2.  Try Optuna framework.  \n* 2020\/9\/9  \n 1.  Try StratifiedKFold cross validator.\n 2.  Visualize Decision Tree\n* 2020\/9\/10  \n 1.  Hyperparameter tuning\n 2.  Try ExtraTreeClassifier \n 3.  Add 'Learn from misclassified data\n* 2020\/9\/11  \n 1.  More feature engineering.(Family Type)\n 2.  Hyperparameter tuning\n 3.  Add stack model.\n* 2020\/9\/28 \n 1. Add Ticket_code and family_survived\n 2. Try label encoder to reduce feature \n 3. Try ROC-AUC as evaluation matrix in parameter tuning to reduce overfitting.\n","3f2e04e0":"500 trails running result:  \nBest model parameters:{'max_depth': 5, 'n_estimators': 500, 'booster': 'dart', 'min_child_weight': 45, 'learning_rate': 0.00028818174062883895, 'gamma': 0.0701754028803822, 'reg_alpha': 0.09673762960851098, 'reg_lambda': 0.020973617864068886, 'colsample_bytree': 0.6000000000000001, 'subsample': 1.0}   \nBest score: 0.908897","41b58657":"#### One hot encoding","73d4957e":"***Observation:***  \nSurprisingly, Family_survived feature has the second importance in our model. ","c24b4236":"### Convert feature types\n","080f2f0d":"'Fare' feature has strong outlier, so use Standardization instead.","cdf6c14a":"500 Trials running result:  \nBest model parameters:{'n_estimators': 250, 'max_depth': 5, 'min_samples_split': 20, 'min_samples_leaf': 20}   \nBest score: 0.875066","8cb1b1c1":"I use GridSearchCV and optuna framework to select best parameters for each model. And the evaluation matrix is roc_auc instead of accuracy becauset roc_auc is more complicated than accuracy and will avoid overfitting a little bit. The whole process takes ages to run, so I preserved running results and build model directly upon those. Feel free to test by yourself.","a47cd1bf":"#### Model Evaluation before Stacking","51a1c78c":"### Family Name","1e7a649c":"#### ExtraTreesClassifier","16feae5b":"#### Another way to predit missing 'Age'  \nUse features highly related to 'Age' to predict it. Here we use random forest Regressor. Tree model doesn't need to scale features, so might be a good option for now.","5a12137b":"#### Stacking Classifier","1236adb8":"#### Skewness","d7c95bba":"### SVM bagging","b601b7e5":"#### Gradient Boosting","ce34de9a":"Bining continuous features to ordinal features.","a88a66dc":"## Model data","6041f9fa":"#### Random Forest","9149e35e":"### Name\nName contains tons of informations like family, gender, title, etc. Here we use name to extract Title and Family_name features.","2dbb5edb":"***Observation*:**  \nIt's pretty clear that 'Family_size' has four group: Alone(0), small family(1-3), middle family(4-6) and large family(>=7). And each group has different Survival Rate. This will help us to encode this feature. ","e9ff9179":"To many cabin information are missing but this feature might be important since it will directly influence the escaping routine.","95e2bf3a":"# Titanic Survivor Prediction","fc348f54":"### SibSp and Parch","34f07b0a":"### Family Size and IsAlone","d340ea75":"Fit classifier on training set and select misclassified data.","79050898":"500 trial running result:  \nBest model parameters:{'n_layers': 1, 'n_units_1': 160, 'activation': 'logistic', 'alpha': 0.00010276620717757648}   \nBest score: 0.832779","addeb586":"### Family Survived  \nWe use family name to find family members in both training and test set. Family tends to escape together, if we know some family members in training set survived, there is a good chance members in test set will survive as well.","8b3dcd8f":"#### Feature importance","34bfe87f":"### Pclass\nOrdinal feature without missing value. Let's see it's distribution and relation to other features.","2b461232":"### Encoding","08f560cd":"#### Ordinal Features:","74b859ad":"#### Neural Network","d94407e7":"1. Grid Seach hyperparameters with 5 fold cross validataion.\nhttps:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/","5964f889":"## Submit","7d99ee7f":"Handle missing values in 'Age' using 'Title' mean.(just for now)"}}