{"cell_type":{"d1b35e13":"code","8758d03c":"code","b8d5e5ec":"code","48939494":"code","c69294ec":"code","8911830a":"code","b885f0d2":"code","c57071ca":"code","da2b6c9e":"code","c5e90eb5":"code","c84db952":"code","a245f148":"code","0beba366":"markdown","fa0022c3":"markdown"},"source":{"d1b35e13":"import tensorflow as tf\nfrom transformers import BertTokenizer, TFBertModel, AdamWeightDecay\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nimport string\nimport re\nimport numpy as np\nimport pandas as pd\nimport emoji\nimport os\nfrom sklearn.metrics import accuracy_score\nfrom skopt.utils import use_named_args\nfrom skopt.space import Real, Integer, Categorical\nfrom skopt import gp_minimize\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.model_selection import KFold","8758d03c":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv', header = 0, encoding=\"utf8\")\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv', header = 0, encoding=\"utf8\")\ntrain = train.drop(columns=['id', 'keyword', 'location'])\ntest = test.drop(columns=['id', 'keyword', 'location'])\ntrain = train.drop_duplicates().reset_index(drop=True)\ntrain.sample(10, random_state=18)","b8d5e5ec":"def remove_tweet_object(tweet):\n  tweet = re.sub(r\"https?:\/\/\\S+|www\\.\\S+\", \" \", tweet)\n  tweet = re.sub(r\"#\\w+\", \" \", tweet)\n  tweet = re.sub(r\"@\\w+\", \" \", tweet)\n  tweet = emoji.get_emoji_regexp().sub(\" \", tweet)\n  return tweet\n\ndef text_filter(tweet):\n  tweet = tweet.lower()\n  tweet = re.sub(r\"\u2019\", \"'\", tweet)\n  tweet = remove_tweet_object(tweet)\n  return tweet\n\ntrain[\"text\"] = train[\"text\"].apply(lambda x: text_filter(x))\ntest[\"text\"] = test[\"text\"].apply(lambda x: text_filter(x))\n\ntrain.sample(10, random_state=18)","48939494":"X_train, X_val, y_train, y_val = train_test_split(train.text, train.target, test_size=0.2, random_state=42)","c69294ec":"name_bert = \"bert-base-uncased\"\ntokenizer = BertTokenizer.from_pretrained(name_bert, do_lower_case=True)\nbert_model = TFBertModel.from_pretrained(name_bert, output_hidden_states=True, trainable=True)\nbert_w = bert_model.get_weights()","8911830a":"combined = pd.concat([X_train, X_val, test.text], axis=0)\ncombined = tokenizer(combined.values.tolist(), padding=True, truncation=True, return_tensors='tf')\ntrain_input = (combined[\"input_ids\"][:len(X_train)], combined[\"attention_mask\"][:len(X_train)], combined[\"token_type_ids\"][:len(X_train)])\nval_input = (combined[\"input_ids\"][len(X_train):len(X_train)+len(X_val)], combined[\"attention_mask\"][len(X_train):len(X_train)+len(X_val)], combined[\"token_type_ids\"][len(X_train):len(X_train)+len(X_val)])\ntest_input = (combined[\"input_ids\"][len(X_train)+len(X_val):], combined[\"attention_mask\"][len(X_train)+len(X_val):], combined[\"token_type_ids\"][len(X_train)+len(X_val):])","b885f0d2":"bert_model.set_weights(bert_w)\n\noptimizer = AdamWeightDecay(learning_rate=3e-5, weight_decay_rate=0.01, exclude_from_weight_decay=[\"LayerNorm\", \"layer_norm\", \"bias\"])\nloss = keras.losses.BinaryCrossentropy(from_logits=False, label_smoothing=0.0)\n\ninput_word_ids = keras.layers.Input(shape=(train_input[0].shape[1],), dtype=tf.int32, name=\"input_ids\")\ninput_mask = keras.layers.Input(shape=(train_input[0].shape[1],), dtype=tf.int32, name=\"attention_mask\")\nsegment_ids = keras.layers.Input(shape=(train_input[0].shape[1],), dtype=tf.int32, name=\"token_type_ids\")\n\noutputs = bert_model([input_word_ids, input_mask, segment_ids])\ndrop = keras.layers.Dropout(0.1) (outputs[1])\nrelu = keras.layers.Dense(96, \"relu\") (drop)\ndrop = keras.layers.Dropout(0.1) (relu)\nout = keras.layers.Dense(1, \"sigmoid\")(drop)\n\nmodel = keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\nmodel.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\nmodel.summary()\n\nhistory_bert = model.fit(\n    train_input, \n    y_train,\n    validation_data=(val_input, y_val),\n    epochs=1,\n    batch_size=32,\n    callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=1, mode='auto', restore_best_weights=True)]\n    )","c57071ca":"model_bert_enc = keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=relu)\n\ntrain_text_vect = model_bert_enc.predict(train_input)\nval_text_vect = model_bert_enc.predict(val_input)\ntest_text_vect = model_bert_enc.predict(test_input)\n\nprint(\"Done\")","da2b6c9e":"space = [Real(1, 1e3, prior=\"log-uniform\", name=\"C_svm\", transform=\"identity\"),\n         Real(1, 1e3, prior=\"log-uniform\", name=\"C_lr\", transform=\"identity\"),\n         Categorical(['hinge', 'squared_hinge'], name=\"loss_svm\", transform=\"identity\"),\n         Categorical(['l1', 'l2', 'elasticnet'], name=\"penalty_sgd\", transform=\"identity\"),\n         Categorical(['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'], name=\"loss_sgd\", transform=\"identity\"),\n         ]\n\n@use_named_args(space)\ndef opt_model(C_svm, loss_svm, penalty_sgd, loss_sgd, C_lr):\n    estimators = [('1', make_pipeline(MinMaxScaler(), LinearSVC(C=C_svm, loss=loss_svm, random_state=42))),\n                  ('2', make_pipeline(MinMaxScaler(), GaussianNB())),\n                  ('3', make_pipeline(MinMaxScaler(), SGDClassifier(penalty=penalty_sgd, loss=loss_sgd, random_state=42))),\n                 ]\n    clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(C=C_lr, random_state=42), n_jobs=-1, cv=KFold(n_splits=5, shuffle=True, random_state=42), stack_method='predict')\n    clf.fit(train_text_vect, y_train)\n    return -clf.score(train_text_vect, y_train)\n\nres = gp_minimize(opt_model, space, n_initial_points=10, n_calls=50, verbose=True, random_state=0, n_jobs=-1)","c5e90eb5":"print(res.fun)\nprint(res.x)","c84db952":"estimators = [\n    ('1', make_pipeline(MinMaxScaler(), LinearSVC(C=res.x[0], loss=res.x[2] ,random_state=42))),\n    ('2', make_pipeline(MinMaxScaler(), GaussianNB())),\n    ('3', make_pipeline(MinMaxScaler(), SGDClassifier(penalty=res.x[3], loss=res.x[4],random_state=42))),\n]\n\nclf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(C=res.x[1],random_state=42), n_jobs=-1, cv=KFold(n_splits=5, shuffle=True, random_state=42), stack_method='predict')\nclf.fit(train_text_vect, y_train)\n\nprint(clf.score(train_text_vect, y_train))\nprint(clf.score(val_text_vect, y_val))","a245f148":"df_submission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv', index_col=0).fillna('')\ndf_submission['target'] = clf.predict(test_text_vect)\ndf_submission.to_csv('submission.csv')\ndf_submission","0beba366":"# Predict","fa0022c3":"# Stacking Generalization"}}