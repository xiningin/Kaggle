{"cell_type":{"bf50187d":"code","8701b4af":"code","ad0ccaa5":"code","53a75854":"code","f7b0b33f":"code","f62b62e4":"code","7eafa109":"code","b0e175fa":"code","30ecb3e4":"code","058a5c93":"code","19498794":"code","6226f5f4":"code","5b878c41":"code","354c1f9c":"code","057a570e":"code","57de87eb":"code","ed912b58":"code","55be9fc9":"code","97bd265b":"code","58060220":"code","45449a43":"code","3fdab7fe":"code","d987644f":"markdown","04b0c039":"markdown","e9a20746":"markdown","a1ca8966":"markdown","93f87f30":"markdown","51b51633":"markdown","ef213e3e":"markdown","0a2845c5":"markdown","963981de":"markdown","f6ca5a46":"markdown","ff29f109":"markdown","43f3507d":"markdown","82219f23":"markdown","5164093e":"markdown","665db5ba":"markdown","0c728514":"markdown","c41c0fd9":"markdown","d0173ed3":"markdown","216270e1":"markdown","92981bb0":"markdown","b9282abb":"markdown","ffb79d8c":"markdown","615ff870":"markdown"},"source":{"bf50187d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nplt.style.use('bmh')","8701b4af":"df = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndf.head()","ad0ccaa5":"df.describe()","53a75854":"df.isnull().sum()","f7b0b33f":"df.dtypes","f62b62e4":"sns.countplot(x='Class', data=df, palette='CMRmap')\nprint('Non-fraud transactions: {}%'.format(round(df.Class.value_counts()[0]\/len(df)*100.0,2)))\nprint('Fraud transactions: {}%'.format(round(df.Class.value_counts()[1]\/len(df)*100.0,2)))","7eafa109":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\nax1 = sns.distplot(df['Time'], ax=ax1, color='y')\nax2 = sns.distplot(df['Amount'], ax=ax2, color='r')\nax1.set_title('Distribution of Time', fontsize=13)\nax2.set_title('Distribution of Amount', fontsize=13)","b0e175fa":"from sklearn.preprocessing import RobustScaler\nrs = RobustScaler()\ndf['scaled_amount'] = rs.fit_transform(df['Amount'].values.reshape(-1,1))\ndf['scaled_time'] = rs.fit_transform(df['Time'].values.reshape(-1,1))\ndf.drop(['Time', 'Amount'], axis=1, inplace=True)","30ecb3e4":"scaled_amount = df['scaled_amount']\nscaled_time = df['scaled_time']\ndf.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\ndf.insert(0, 'scaled_amount', scaled_amount)\ndf.insert(0, 'scaled_time', scaled_time)\ndf.head()","058a5c93":"from sklearn.model_selection import train_test_split as holdout\nx = np.array(df.iloc[:, df.columns != 'Class'])\ny = np.array(df.iloc[:, df.columns == 'Class'])\nx_train, x_test, y_train, y_test = holdout(x, y, test_size=0.2, random_state=0)\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, classification_report, precision_score, recall_score, accuracy_score\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\ny_pred = logreg.predict(x_test)\ncnf_matrix = confusion_matrix(y_test, y_pred)\n\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\", fmt='g')\nplt.ylabel('Actual Label')\nplt.xlabel('Predicted Label')\n\nlabels = ['Non-fraud', 'Fraud']\nprint(classification_report(y_test, y_pred, target_names=labels))","19498794":"from imblearn.over_sampling import SMOTE\n\nprint(\"Transaction Number x_train dataset: \", x_train.shape)\nprint(\"Transaction Number y_train dataset: \", y_train.shape)\nprint(\"Transaction Number x_test dataset: \", x_test.shape)\nprint(\"Transaction Number y_test dataset: \", y_test.shape)\n\nprint(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n\nsm = SMOTE(random_state=2)\nx_train_s, y_train_s = sm.fit_sample(x_train, y_train.ravel())\n\nprint('After OverSampling, the shape of train_x: {}'.format(x_train_s.shape))\nprint('After OverSampling, the shape of train_y: {} \\n'.format(y_train_s.shape))\n\nprint(\"After OverSampling, counts of label '1', %: {}\".format(sum(y_train_s==1)\/len(y_train_s)*100.0,2))\nprint(\"After OverSampling, counts of label '0', %: {}\".format(sum(y_train_s==0)\/len(y_train_s)*100.0,2))\n\nsns.countplot(x=y_train_s, data=df, palette='CMRmap')","6226f5f4":"logreg = LogisticRegression()\nlogreg.fit(x_train_s, y_train_s)\ny_pred = logreg.predict(x_test)\ncnf_matrix = confusion_matrix(y_test, y_pred)\n\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\", fmt='g')\nplt.ylabel('Actual Label')\nplt.xlabel('Predicted Label')\n\nprint(classification_report(y_test, y_pred))","5b878c41":"y_pred_prob = logreg.predict_proba(x_test)[:,1]\nprecision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\nplt.plot(precision, recall)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision Recall Curve')","354c1f9c":"from sklearn.ensemble import RandomForestClassifier as rfc\nrand_f = rfc(n_estimators=1000, min_samples_split=10, min_samples_leaf=1,\n           max_features='auto', max_leaf_nodes=None,\n           oob_score=True, n_jobs=-1, random_state=1)\nrand_f.fit(x_train_s, y_train_s)\ny_pred = rand_f.predict(x_test)\n\ncnf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\", fmt='g')\nplt.ylabel('Actual Label')\nplt.xlabel('Predicted Label')\n\nprint(classification_report(y_test, y_pred))","057a570e":"y_pred_prob = rand_f.predict_proba(x_test)[:,1]\nprecision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\nplt.plot(precision, recall)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision Recall Curve')","57de87eb":"#Plotting Feature Importances\nprint('Feature importance ranking\\n\\n')\nimportances = rand_f.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in rand_f.estimators_],axis=0)\nindices = np.argsort(importances)[::-1]\nvariables = df.columns\nimportance_list = []\nfor f in range(x.shape[1]):\n    variable = variables[indices[f]]\n    importance_list.append(variable)\n    print(\"%d.%s(%f)\" % (f + 1, variable, importances[indices[f]]))\nplt.figure(figsize=(20, 8))\nplt.title(\"Feature importances\")\nplt.bar(importance_list, importances[indices],\n       color=\"purple\", yerr=std[indices], align='center')","ed912b58":"import xgboost as xgb\nmodel = xgb.XGBClassifier(n_estimators = 5000, max_depth = 30, learning_rate = 0.01)\nmodel.fit(x_train_s, y_train_s)\ny_pred = model.predict(x_test)\n\ncnf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\", fmt='g')\nplt.ylabel('Actual Label')\nplt.xlabel('Predicted Label')\n\nprint(classification_report(y_test, y_pred))","55be9fc9":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nmodel = Sequential([Dense(input_dim=30, units=16, activation='relu'),\n                   Dense(units=24, activation='relu'),\n                   Dropout(0.5),\n                   Dense(units=20, activation='relu'),\n                   Dense(units=24, activation='relu'),\n                   Dense(units=1, activation='sigmoid')])\nmodel.summary()","97bd265b":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel.fit(x_train_s, y_train_s, batch_size=15, epochs=15)","58060220":"model_loss = pd.DataFrame(model.history.history)\nmodel_loss.plot()","45449a43":"score = model.evaluate(x_test, y_test)\nprint(score)","3fdab7fe":"y_pred = model.predict_classes(x_test)\n\ncnf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\", fmt='g')\nplt.ylabel('Actual Label')\nplt.xlabel('Predicted Label')\n\nprint(classification_report(y_test, y_pred))","d987644f":"<img src=\"https:\/\/i.imgur.com\/W8HrGHl.jpg\" width=\"800\">","04b0c039":"<font size=\"2\" font>This dataset is severely imbalanced (most of the transactions are non-fraud). So the algorithms are much more likely to classify new observations to the majority class and high accuracy won't tell us anything. To address the problem of imbalanced dataset we can use **undersampling** and **oversampling** data approach techniques. **Oversampling** increases the number of minority class members in the training set. The advantage of **oversampling** is that no information from the original training set is lost unlike in **undersampling**, as all observations from the minority and majority classes are kept. On the other hand, it is prone to overfitting.\nThere is a type of **oversampling** called **SMOTE (Synthetic Minority Oversampling Technique)**, which we are going to use to make our dataset balanced. It creates synthetic points from the minority class","e9a20746":"1. <font size='2' font> Good prediction results can be achieved with imbalanced datasets as well as with balanced ones\n2. Random Forest and XGBoost Classifiers gave us the best results being able to detect more than 80% fraud transactions and at the same time not classifying a lot of non-fraud transactions as fraud\n3. There is no perfect model and there will always be a trade-off between precision and recall. It is up to the company and its objectives to decide which approach is the best in each particular situation ","a1ca8966":"<font size=\"2\" font>But first let's see the distributions of transaction time and transaction amount to have an idea how skewed these features are. Due to privacy reasons we don't know the names of the other features. All we know is that all of them (except time and amount) went through PCA transformation, which means that they were previously scaled","93f87f30":"# <p style=\"color:blue\"> EDA","51b51633":"<font size=2> The result isn't that great","ef213e3e":"# <p style=\"color:blue\"> Logistic Regression with SMOTE","0a2845c5":"<font size=2> We can also try XGBoost and Neural Network to see what happens","963981de":"# <p style=\"color:blue\"> How does credit card fraud occur?","f6ca5a46":"# <p style=\"color:blue\"> Conclusions ","ff29f109":"# <p style=\"color:blue\"> Logistic Regression without SMOTE","43f3507d":"<font size='2' font> So time and amount are now scaled as well","82219f23":"# <p style=\"color:blue\"> XGBoost ","5164093e":"<font size=\"2\" font>We got a high recall which means our model is able to detect the highest number of fraud transactions, while the precision is very low which is not good because it means that the model classifies a lot of non-fraud transactions as fraud. The customers of a financial institution are not going to be satisfied with that fact and may even stop using the service of that financial institution. So in this case it's also important to have a high precision, which we are going to try to achieve with Random Forest","665db5ba":"<font size=2> Also we shouldn't use accuracy score as a metric with imbalanced datasets (will be usually high and misleading), instead we should use f1-score, precision\/recall score and confusion matrix\n* **Recall of fraud cases (sensitivity)** summarizes true positive rate (True positive\/True positive + False Negative) - how many cases we got correct out of all the positive ones \n* **Recall of non-fraud (specificity)** summarizes true negative rate (True negative\/True negative + False positive) - how many cases we got correct out of all the negative ones\n* **Precision of fraud cases** (True positive\/True positive + False positive) summarizes the accuracy of fraud cases detected - out of all predicted as fraud, how many are correct\n* **Precision of non-fraud cases** (True negative\/True negative + False negative) summarizes the accuracy of non-fraud cases detected - out of all predicted as non-fraud, how many are correct\n* **F1-score** is the harmonic mean of recall and precision","0c728514":"<font size=2> Before creating a model it is important to get a general understanding of the data","c41c0fd9":"<font size=\"2\" font>As we can see this is not a good model, because it is biased towards majority class and the recall in minority class is not as high as disired","d0173ed3":"<font size=\"2\" font>Now the dataset is balanced, so we can build a Logistic Regression model with SMOTE. One important thing to point out here is that we used SMOTE after cross validation in order to avoid data leakage problem and hence overfitting","216270e1":"<font size=\"2\" font>Cool! :) So Random Forest with SMOTE performed better than Logistic Regression overall allowing us to get high recall and high precision at the same time. Even though the recall has decreased a little bit, we were able to increase the precision significantly, which means a lot in the case of fraud detection and as we know it is a trade-off","92981bb0":"# <p style=\"color:blue\"> Random Forest Classifier with SMOTE","b9282abb":"# <p style=\"color:blue\"> Neural Network","ffb79d8c":"<font size=2>**Credit card fraud** happens when consumers give their credit card number to unfamiliar individuals, when cards are lost or stolen, when mail is diverted from the intended recipient and taken by criminals, or when employees of a business copy the cards or card numbers of a cardholder.\n    In this notebook we will develop a few ML models using anonymized credit card transaction data. The challenge behind fraud detection is that frauds are far less common as compared to legal transactions","615ff870":"<font size=\"2\" font>To normalize the distribution we are going to use a method called Feature Scaling. In our case it is better to use the Robust Scaler algorithm because it's robust to outliers"}}