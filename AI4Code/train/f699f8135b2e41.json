{"cell_type":{"2ba48355":"code","a3fef9ae":"code","36cf25ee":"code","ffb9e0f6":"code","38755297":"code","094b5feb":"code","af0a09da":"code","6b690877":"code","b4a621aa":"code","0734c479":"code","05cc31c1":"code","4d1ee4d0":"code","d16c760d":"code","5742770f":"code","573e790a":"code","1f3817ec":"code","9b721e19":"code","1b87ded9":"code","f9ca2950":"code","a1c63f82":"code","203b5ed6":"code","f20ee981":"code","5f7de817":"code","5786c502":"code","3f91d704":"code","70eda7f5":"code","a952bbac":"code","e9ddd4f1":"code","d56ac435":"code","c36852d5":"code","74548a8d":"code","240e6cc7":"code","bb248b48":"code","7b826ef1":"code","3ad5e35c":"code","47c57f70":"code","1306ca9d":"code","9e1303ae":"code","f5d1abba":"markdown","ac3a9a7a":"markdown","c002b930":"markdown","c0a8ed28":"markdown","c0082775":"markdown","47efad83":"markdown","345aabcc":"markdown","0cfe0dce":"markdown","80ef3487":"markdown","7734ca34":"markdown","2ab40c06":"markdown","005ebd1f":"markdown","3ee2d262":"markdown","4f194455":"markdown","c0cd4dd3":"markdown","628deea3":"markdown","c08d48e7":"markdown"},"source":{"2ba48355":"%matplotlib inline\nimport sys\nimport numpy as np\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\n#from sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import StratifiedShuffleSplit\n#from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n#from sklearn.pipeline import FeatureUnion\nfrom sklearn.base import BaseEstimator, TransformerMixin","a3fef9ae":"housing = pd.read_csv(\"..\/input\/california-housing-prices\/housing.csv\")","36cf25ee":"housing.head()","ffb9e0f6":"housing.info()","38755297":"housing.describe()","094b5feb":"import matplotlib.pyplot as plt\nhousing.hist(bins=50, figsize=(20,15))\nplt.show()","af0a09da":"housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\ns=housing[\"population\"]\/100, label=\"population\", figsize=(10,7),\nc=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n) \nplt.legend()","6b690877":"\nhousing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n                               labels=[1, 2, 3, 4, 5])\n#show relative frequencies of the unique values\nhousing[\"income_cat\"].value_counts(normalize=True)","b4a621aa":"# Stratified Shuffle Split by new income_cat variable\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n    strat_train_set = housing.loc[train_index]\n    strat_test_set = housing.loc[test_index]","0734c479":"housing_train_df = strat_train_set.copy()","05cc31c1":"housing_train_df.columns","4d1ee4d0":"housing_features = strat_train_set.drop(\"median_house_value\", axis=1) # drop labels for training set\nhousing_labels = strat_train_set[\"median_house_value\"].copy()","d16c760d":"# Create new features for dataset \n\nrooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\n\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n        self.add_bedrooms_per_room = add_bedrooms_per_room\n    def fit(self, X, y=None):\n        return self # nothing else to do\n    def transform(self, X, y=None):\n        rooms_per_household = X[:, rooms_ix] \/ X[:, household_ix]\n        population_per_household = X[:, population_ix] \/ X[:, household_ix]\n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:, bedrooms_ix] \/ X[:, rooms_ix]\n            return np.c_[X, rooms_per_household, population_per_household,bedrooms_per_room]\n        else:\n            return np.c_[X, rooms_per_household, population_per_household]","5742770f":"\n\n# from sklearn.base import BaseEstimator, TransformerMixin\n# class DataFrameSelector(BaseEstimator, TransformerMixin):\n#     def __init__(self, attribute_names):\n#         self.attribute_names = attribute_names\n#     def fit(self, X, y=None):\n#         return self\n#     def transform(self, X):\n#         return X[self.attribute_names].values","573e790a":"# create attribute type variables\nhousing_num = housing_features.drop(\"ocean_proximity\", axis=1)\nnum_attribs = list(housing_num) # numerical attribues\ncat_attribs = [\"ocean_proximity\"] # categorical attribute","1f3817ec":"num_pipeline = Pipeline([\n#        ('selector', DataFrameSelector(num_attribs)),\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('attribs_adder', CombinedAttributesAdder()),\n        ('std_scaler', StandardScaler()),\n    ])","9b721e19":"from sklearn.compose import ColumnTransformer\n\nfull_pipeline = ColumnTransformer([\n     (\"num\", num_pipeline, num_attribs),\n     (\"cat\", OneHotEncoder(), cat_attribs),\n ])","1b87ded9":"\nhousing_prepared = full_pipeline.fit_transform(housing_features)\nhousing_prepared.shape","f9ca2950":"len(strat_train_set), len(strat_test_set)","a1c63f82":"print(f\"Dataset is {housing.shape[0]} rows long, train set is allocated {len(strat_train_set)} = 20% \")","203b5ed6":"from sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)","f20ee981":"# Evaluate model on whole of training set\nfrom sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\n\n\nprint(f\"Typical prediction error is {int(lin_rmse)}$\")","5f7de817":"#Trying a second model - Decision Tree\n\n#train\nfrom sklearn.tree import DecisionTreeRegressor\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(housing_prepared, housing_labels)\n\n#evaluate\nhousing_predictions = tree_reg.predict(housing_prepared)\ntree_mse = mean_squared_error(housing_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\n\n\nprint(f\"Typical prediction error is {(tree_rmse)}$\")\n","5786c502":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(tree_reg, housing_prepared, housing_labels,scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-scores)\n\n#RMSE score for each fold:\nprint(tree_rmse_scores)\n","3f91d704":"def display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\n\ndisplay_scores(tree_rmse_scores)\n\n","70eda7f5":"from sklearn.ensemble import RandomForestRegressor\nforest_reg = RandomForestRegressor()\nforest_reg.fit(housing_prepared, housing_labels)\n\nforest_preds = forest_reg.predict(housing_prepared)\nforest_mse = mean_squared_error(housing_labels, forest_preds)\n\nforest_rmse= np.sqrt(tree_mse)\nprint(forest_rmse)","a952bbac":"scores2 = cross_val_score(forest_reg, housing_prepared, housing_labels,scoring=\"neg_mean_squared_error\", cv=10)\nforest_rmse = np.sqrt(-scores2)\n\n\n\n","e9ddd4f1":"display_scores(forest_rmse)","d56ac435":"from sklearn.svm import SVR\n\nsvm_reg = SVR(kernel=\"linear\")\nsvm_reg.fit(housing_prepared, housing_labels)\nhousing_predictions = svm_reg.predict(housing_prepared)\nsvm_mse = mean_squared_error(housing_labels, housing_predictions)\nsvm_rmse = np.sqrt(svm_mse)\nsvm_rmse","c36852d5":"\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    # try 12 (3\u00d74) combinations of hyperparameters\n    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n    # then try 6 (2\u00d73) combinations with bootstrap set as False\n    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n  ]\n\n\n\nforest_reg = RandomForestRegressor(random_state=42)\n# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n                           scoring='neg_mean_squared_error',\n                           return_train_score=True)\ngrid_search.fit(housing_prepared, housing_labels)","74548a8d":"grid_search.best_params_\n","240e6cc7":"feature_importances = grid_search.best_estimator_.feature_importances_\nfeature_importances","bb248b48":"#Let\u2019s display these importance scores next to their corresponding attribute names:\n\nextra_attribs = [\"rooms_per_household \", \"population_per_household \", \"bedrooms_per_room \"]\ncat_encoder = full_pipeline.named_transformers_[\"cat\"]\ncat_one_hot_attribs = list(cat_encoder.categories_[0])\nattributes = num_attribs + extra_attribs + cat_one_hot_attribs\nsorted(zip(feature_importances, attributes), reverse=True)\n","7b826ef1":"\nfinal_model = grid_search.best_estimator_\n\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\ny_test = strat_test_set[\"median_house_value\"].copy()\n\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = final_model.predict(X_test_prepared)\n\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)\nfinal_rmse","3ad5e35c":"\nfrom scipy import stats\n\nconfidence = 0.95\nsquared_errors = (final_predictions - y_test) ** 2\nnp.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n                         loc=squared_errors.mean(),\n                         scale=stats.sem(squared_errors)))","47c57f70":"from sklearn.base import BaseEstimator, TransformerMixin\n\ndef indices_of_top_k(arr, k):\n    return np.sort(np.argpartition(np.array(arr), -k)[-k:])\n\nclass TopFeatureSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, feature_importances, k):\n        self.feature_importances = feature_importances\n        self.k = k\n    def fit(self, X, y=None):\n        self.feature_indices_ = indices_of_top_k(self.feature_importances, self.k)\n        return self\n    def transform(self, X):\n        return X[:, self.feature_indices_]\n","1306ca9d":"arr = feature_importances\nk=5\nnp.sort(np.argpartition(np.array(arr), -k)[-k:])","9e1303ae":"np.array(arr)","f5d1abba":"Pipelinbe for categorical attirbue","ac3a9a7a":"It's good practice to save a copy of the original dataset","c002b930":"A prediction error of 0 suggests the model is overfiting. We could leave out of training a validation set, to evalate the model on untrand data. A better evaluation method is Cross-Validation:\n","c0a8ed28":"Random Forest seams better, thought stil overfiting. ","c0082775":"Pipeline for numerical attributes","47efad83":"Instead of playing around with hyperparameters, we can use Grid Search to evaluate all possible combinations of hyperparameters we want to xperiment with:","345aabcc":"Typically, when splitting the dataset into Train and Test datasets, we would just use a random split ratio (say 80\/20). As the median income is assumed to be higely important when predicting house prices though, we should make sure the train and test sets are both representative of the various categories of incomes (to avoid sampling bias),we will create an income category variable and use stratified sampling based on the new income category","0cfe0dce":"**Prepare Set for Machine Learning**","80ef3487":"We can compute a 95% confidence interval for the test RMSE, to get an idea of how precise this estimate is:","7734ca34":"The grid search approach is fine when you are exploring relatively few combinations,\nlike in the previous example, but when the hyperparameter search space is large, it is\noften preferable to use RandomizedSearchCV instead. ","2ab40c06":"Fine-Tune My Model\n","005ebd1f":"Full pipeline","3ee2d262":"Decision Tree model was definantly overfitting does not seem better than linear regression model","4f194455":"**Evaluate System on the Test Set**","c0cd4dd3":"**Split dataset into test and training sets**","628deea3":"**Initial Data Exploration**","c08d48e7":"Train modle with a Linear Regression model"}}