{"cell_type":{"b6662895":"code","73293049":"code","780d528d":"code","1ac6221e":"code","a61d9e0a":"code","991dfa6a":"code","ff34307c":"code","3236abd0":"code","935d4dd0":"code","e94064c6":"code","e6f33070":"code","8598f30d":"code","6d054bd7":"code","5f90fdb0":"code","b6401e95":"code","4b205344":"code","cee13aa4":"code","1f60c124":"code","033a438e":"code","5b61f513":"code","43832100":"code","260f6769":"code","59bfb217":"markdown","824333f4":"markdown","d7b52aaa":"markdown","ade298c2":"markdown","07f49f82":"markdown","9d3b766c":"markdown","9a7de54f":"markdown","b76539b0":"markdown","07d4ffbe":"markdown","08c2f825":"markdown","9b222c06":"markdown","6ae9c246":"markdown","298d6931":"markdown","48af020a":"markdown","d7be8996":"markdown","f181c3fe":"markdown"},"source":{"b6662895":"import os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom xgboost import XGBRegressor, XGBClassifier\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","73293049":"train = pd.read_csv(r'\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv(r'\/kaggle\/input\/titanic\/test.csv')\ndata = pd.concat([train,test],axis=0, sort=True).reset_index(drop=True)","780d528d":"missing = data.isnull().sum().sort_values(ascending=False) \/ len(data)\n\nplt.figure(figsize=(15,7))\nsns.barplot(x=missing.index, y=missing.values)\nplt.title('Amount of missing data relative to whole dataset', fontsize=15)\nplt.xlabel('Feature',fontsize=11)\nplt.ylabel('Amount of missing data',fontsize=11)","1ac6221e":"data.loc[data.Fare.isnull(),'Fare'] = data[(data.Embarked=='S') & (data.Pclass==3)].Fare.median()\ndata.loc[data.Embarked.isnull(),'Embarked'] = 'S'","a61d9e0a":"data.Name.head(10)","991dfa6a":"data['Title'] = data.Name.map(lambda x: x.split(',')[1].split('.')[0])\n\n#Because Mlle is french equivalent of Miss\ndata.loc[data.Title==' Mlle', 'Title'] = ' Miss'\n#They were travelling alone, so I assume that they are unmarried\ndata.loc[data.Title==' Ms', 'Title'] = ' Miss'\n#Mme is french equivalent for Mrs\ndata.loc[data.Title==' Mme', 'Title'] = ' Mrs'\n#Royals and stuff\ndata.Title = data.Title.replace({' Sir':'Demigod',' Lady':'Demigod',' the Countess':'Demigod',' Don':'Demigod', ' Dona':'Demigod', ' Jonkheer':'Demigod'})\n#Military\ndata.Title = data.Title.replace({' Col':'Military',' Major':'Military',' Capt':'Military'})\n\ndata.loc[(data.Title=='Demigod') & (data.Sex=='female'),'Title'] = ' Miss'\ndata.loc[(data.Title=='Demigod') & (data.Sex=='male'),'Title'] = ' Mr'","ff34307c":"data['CabinLetter'] = data.Cabin.astype(str).map(lambda x: str(x[0]))\ndata['CabinLetter'] = data.CabinLetter.fillna('n')\ndata =  data.rename(columns={'Sex':'SexFemale'})\ndata['SexFemale'] = data.SexFemale.astype(str).replace({'male':0,'female':1})\n","3236abd0":"np.random.seed(1)\n\nY = data.Age[data.Age.isnull()==False]\ntestY = Y.loc[np.random.choice(Y.index,100, replace=False)]\nmedian = np.median(Y.loc[np.setdiff1d(Y.index,testY.index)])\nMaI = np.sqrt(mean_squared_error(testY, np.full((100,1),median)))\nprint('Holdout set:',MaI)\n","935d4dd0":"np.random.seed(1)\n\ntestX = pd.DataFrame(data.Title.loc[testY.index])\ntestX['Predicted'] = np.NaN\n\nfor i in data.Title.loc[np.setdiff1d(Y.index,testY.index)].unique():\n    testX.loc[testX.Title==i, 'Predicted'] = data[data.Title==i].Age.loc[np.setdiff1d(Y.index,testY.index)].median()\n\nMasI = np.sqrt(mean_squared_error(testY, testX.Predicted))\n    \nprint('Holdout set:',MasI)\n","e94064c6":"data['Family'] = data.SibSp + data.Parch + 1\ndata['PeopleOnTheSameTicket'] = None\n\nfor i in data.Ticket.unique():\n    summary = len(data[data.Ticket==i])\n    data.loc[data.Ticket==i, 'PeopleOnTheSameTicket'] = summary\n\ndata['FarePerPerson'] = data.Fare\/data.PeopleOnTheSameTicket\ndata['Wealth'] = pd.cut(data.FarePerPerson,7,labels=range(1,8))","e6f33070":"from sklearn.linear_model import Lasso, Ridge\nX = data[data.Age.isnull()==False]\nX = X[['Age', 'Embarked','Fare', 'Parch','Pclass','SexFemale','SibSp','Title', 'PeopleOnTheSameTicket','FarePerPerson','Family','Wealth']]\nY = X.Age\nX = X.drop('Age',axis=1)\nX = pd.get_dummies(X, drop_first=True)\ntestX = X.loc[testY.index]\nX = X.drop(testY.index,axis=0)\nY = Y.drop(testY.index,axis=0)\n\nmodel = XGBRegressor(learning_rate=0.001,n_estimators=3000, reg_lambda=0.1, objective='reg:squarederror')\nmodel.fit(X, Y)\n\nMA = np.sqrt(mean_squared_error(testY, model.predict(testX)))\nprint('Holdout set:',MA)","8598f30d":"print('Median of age:',MaI,'\\nMedian of age in subclasses:',MasI,'\\nModel approach:',MA)","6d054bd7":"#Model age imputation\n\nX = data[data.Age.isnull()==False]\nX = X[['Age', 'Embarked','Fare', 'Parch','Pclass','SexFemale','SibSp','Title', 'PeopleOnTheSameTicket','FarePerPerson','Family','Wealth']]\nY = X.Age\nX = X.drop('Age',axis=1)\nX = pd.get_dummies(X, drop_first=True).drop(['Title_ Rev','Title_Military'],axis=1)\n\nmodel = XGBRegressor(learning_rate=0.001,n_estimators=3000, reg_lambda=0.1)\nmodel.fit(X,Y)\ndummydata = pd.get_dummies(data[data.Age.isnull()][['Embarked','Fare', 'Parch','Pclass','SexFemale','SibSp','Title',\n                                                    'PeopleOnTheSameTicket','FarePerPerson','Family','Wealth']], drop_first=True)\n\ndata.loc[data.Age.isnull(), 'Age'] = model.predict(dummydata)","5f90fdb0":"data['AgeInterval'] = pd.cut(data.Age, 3, labels=['Young','Mid-aged', 'Old'])\ndata['FareTAge'] = data.Fare * data.Age","b6401e95":"X = data[data.CabinLetter!='n']\nX = X[['Age', 'Embarked','Fare', 'Parch','Pclass','SexFemale','SibSp','Title', 'CabinLetter']]\nY = X.CabinLetter\nX = X.drop('CabinLetter',axis=1)\nX = pd.get_dummies(X)\nY = Y.replace({'C':1,'E':2,'G':3,'D':4,'A':5,'B':6,'F':7,'T':8})\n\ntrainX, testX, trainY, testY = train_test_split(X,Y, test_size=0.3)\n\nmodel = RandomForestClassifier(n_estimators=500, random_state=1)\nmodel.fit(trainX,trainY)\n\ntrainX = pd.concat([trainX,trainY], axis=1)\ntestX = pd.concat([testX, testY], axis=1)\n\nfor i in trainX.CabinLetter.unique():\n    print('Train result for {} :'.format(i), accuracy_score(trainX.CabinLetter[trainX.CabinLetter==i], model.predict(trainX.drop('CabinLetter',axis=1)[trainX.CabinLetter==i])))\n    \nfor i in testX.CabinLetter.unique():\n    print('Holdout result for {} :'.format(i), accuracy_score(testX.CabinLetter[testX.CabinLetter==i], model.predict(testX.drop('CabinLetter',axis=1)[testX.CabinLetter==i])))\n    \n    \nprint('Accuracy overall:', accuracy_score(testX.CabinLetter, model.predict(testX.drop('CabinLetter',axis=1))))","4b205344":"#Model CabinLetter inmputation \n\nX = data[data.CabinLetter!='n']\nX = X.drop(X[X.Age.isnull()].index,axis=0)\nX = X[['Age', 'Embarked','Fare', 'Parch','Pclass','SexFemale','SibSp','Title', 'CabinLetter']]\nY = X.CabinLetter\nX = X.drop('CabinLetter',axis=1)\nX = pd.get_dummies(X, drop_first=True)\nX['Title_ Rev'] = 0\nY = Y.replace({'C':1,'E':2,'G':3,'D':4,'A':5,'B':6,'F':7,'T':8})\n\nmodel = RandomForestClassifier(n_estimators=500, random_state=1)\nmodel.fit(X,Y)\ndummydata = pd.get_dummies(data[data.CabinLetter=='n'][['Age', 'Embarked','Fare', 'Parch','Pclass','SexFemale','SibSp','Title', 'CabinLetter']], drop_first=True)\ndata.loc[data.CabinLetter=='n','CabinLetter'] = model.predict(dummydata)\n\ndata.CabinLetter = data.CabinLetter.replace({1:'C',2:'E',3:'G',4:'D',5:'A',6:'B',7:'F',8:'T'})","cee13aa4":"# Because its child is in cabin F and people from 2nd class dont get G cabin\ndata.CabinLetter.loc[247] = 'F'\n# Husband is in E and people from 3rd class dont get C cabin\ndata.CabinLetter.loc[85] = 'E'\n# Rest is in F and people from 2nd class dont get C cabin\ndata.CabinLetter.loc[665] = 'F'\n# There is no F cabin. Changing to A is justified by similar Fare to median of Fare for 1st class in cabin A\ndata.CabinLetter.loc[339] = 'A'","1f60c124":"data = data.drop(['Cabin','Name','Ticket'],axis=1)\ndata = pd.get_dummies(data, drop_first=True)\nX = data[data.Survived.isnull()==False].drop(['Survived', 'PassengerId'],axis=1)\nY = data.Survived[data.Survived.isnull()==False]\ntrainX, testX, trainY, testY = train_test_split(X,Y,test_size=0.30, random_state=2019)","033a438e":"def ModelCreation(model):\n    cv = cross_validate(model,\n                        X,\n                        Y,\n                        cv=10,\n                        scoring='accuracy',\n                        return_train_score=True)\n\n    print('Training score average:',cv['train_score'].sum()\/10)\n    print('Holdout score average:',cv['test_score'].sum()\/10)","5b61f513":"model = RandomForestClassifier(n_estimators=900, max_depth=4)\nModelCreation(model)","43832100":"model = XGBClassifier(learning_rate=0.001,n_estimators=4000,\n                                max_depth=4, min_child_weight=0,\n                                gamma=0, subsample=0.7,\n                                colsample_bytree=0.7,\n                                scale_pos_weight=1, seed=27)\n\nModelCreation(model)","260f6769":"model = RandomForestClassifier(n_estimators=900, max_depth=4)\nmodel.fit(X, Y)\ntest = data[data.Survived.isnull()]\ntest['Survived'] = model.predict(test.drop(['Survived','PassengerId'],axis=1)).astype(int)\ntest = test.reset_index()\ntest[['PassengerId','Survived']].to_csv(\"RFClass.csv\",index=False)","59bfb217":"# Introduction","824333f4":"As you may see above, *Name* variable holds information about title of each person. Besides ordinary titles such as Mr, or Mrs we also have:\n\n1. *Miss* - title for unmarried woman\n1. *Master* - title for young boy\n1. *Rev* - clerical title\n1. Higher social status titles - *Sir*, *Lady*, *the Countess*, *Don*, *Dona*, *Jonkheer*\n1. Military titles - *Col*, *Major*, *Capt*\n1. Equivalent titles - *Mlle*, *Ms*, *Mme*\n\nOur next step is to retrieve those titles from *Name* feature and group together titles with lower appearance frequency.","d7b52aaa":"As you may see, the accuracy on holdout set is quite satisfying. Unfortunately, it decreases for classes with smaller values of samples.","ade298c2":"# Model creation\nIn order to evaluate model performance, we're going to compare XGBoost with Random Forest. Also, 10 fold cross-validation will be used.","07f49f82":"# Model approach for Cabin Letter variable\nI think, that one of the best possible solutions, is to fill missing values of *CabinLetter* variable by using model approach. In addition to that, we may use some other informations - according to https:\/\/www.kaggle.com\/gunesevitan\/advanced-feature-engineering-tutorial-with-titanic:\n1. A,B,C - 1st class only\n1. D,E - all classes\n1. F, G - 2nd and 3rd class\n\nBe aware, that according to some other sources (e.g. https:\/\/www.encyclopedia-titanica.org\/cabins.html) only 3rd class was allocated in G cabin and I am going to stick to this convention.","9d3b766c":"As we finally filled all missing values in *Age* variable, we may now create some additional features:\n1. *AgeInterval* - hoping to get some additional informations for model by dividing into three separate groups\n1. *FareTAge* - paying more for ticket (thus being richer) along with being older may increase survivability\n\n","9a7de54f":"## Model approach\nIn model approach, we're hoping, that independent variables in the dataset can help us to provide reasonable predictions for missing values. Before creating a model, we're going to create more features:\n\n1. *Family* - by summing *SibSp* and *Parch* we're providing additional information\n1. *PeopleOnTheSameTicket* - one passenger can travel with more than one person on the same ticket\n1. *FarePerPerson* - if multiple passengers belong to a single ticket, then fare value is multiplied by the amount of passengers - that's why there is so much difference between the values of ticket even when embarkation port and class is the same\n1. *Wealth* - binned *FarePerPerson*. Usually, binning continuous variables might be useful for specific models. For example, when using Random Forest, instead of forcing a model to look for the best value for a split in continuous variable, we may force it to use pre-defined feature","b76539b0":"As you may see above, the best results are provided using model approach, but the thing which you have to keep in mind is, that the results are for the specific random sample - the most optimal solution would be chosen after creating multiple random samples, or by cross-validation.","07d4ffbe":"## Median of age imputation","08c2f825":"## Median in subclasses imputation","9b222c06":"# Age imputation possible concepts\n\nIf it comes to most basic approach in filling missing data, we might want to fill it simply by dataset's median, but we might then lose a lot of information. Also, we may suspect, that people traveling in the first class have more money, which can be associated with being older. On the other hand, we may use some patterns from *Title* variable or there might be a higher dimension relationship between *Age* and multiple variables which will help us fill missing values more accurately. In order to choose wisely, we're going to compare following methods:\n\n1. Median of age imputation\n1. Median in subclasses imputation\n1. Model approach\n\nEach of the following will be treated as a separate model. In order to evaluate which one is doing better, we're going to compare them using mean squared error metric.","6ae9c246":"I decided to imput *Fare* values as the median of the amount paid by people embarked at Southampton for third class. As the *Embarked* value I decided to go with the most popular embarkation city.","298d6931":"Feature with the highest amount of missing values is *Cabin*. The first thing that comes in the mind is to drop it due to mostly missing values, but we will try to retrieve as much information as its possible, so for now its going to stay. Also, we have two missing *Embarked* values, as well as one *Fare* missing. In order to perform more advanced imputation later on, we're going to fill those values right now.","48af020a":"# Self-promotion\nIf you enjoyed reading my notebook, or at least find it useful in some way or another, I highly encourage you to check out my other creations:\n\n1. Top 20% Ames House Prices kernel https:\/\/www.kaggle.com\/paragraph\/rmlse-0-119-top-21-house-prices-regression#Final-thoughts\n1. Data visualisation with Baseline (map chart sublibrary) https:\/\/www.kaggle.com\/paragraph\/airlines-data-visualisation-with-baseline\n1. Text mining with NLTK library https:\/\/www.kaggle.com\/paragraph\/text-mining-nltk-and-k-means-included","d7be8996":"I decided to group demigods together with titles associated with gender, because of very low frequency in dataset - only 6 observations in the whole dataset have *Demigod* title.\n\nIn the cell below, we're performing a bit of feature engineering. We do assume, that cabin letters may bring additional information about the probability of survival.","f181c3fe":"Higher holdout score on training set comes at the cost of higher variation, thus model results are going to be less predictable on test data. We could try to decrease overfitting by introducing L2 parameter, decreasing max_depth, n_estimators, etc., but none of these resulted in receiving better results on public leaderboard. On the other hand Random Forest gives worse results on training data, but it almost ideally fits holdout data. It is also projected in test data with significantly higher accuracy score comparing to XGBoost."}}