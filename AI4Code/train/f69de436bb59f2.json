{"cell_type":{"7eb6853f":"code","fbf3109d":"code","266732b3":"code","2f8ae3ab":"code","03369fbb":"code","ffe2b5e8":"code","ebdb6dc8":"code","ec91799c":"code","175d5e13":"code","2042141f":"code","bb27fc59":"code","6eb411b2":"code","f3333ec6":"code","a12e43e8":"code","d4b46896":"code","dc2e1c26":"code","80f9b94e":"code","f757edc7":"code","c61f3c9e":"code","cd0bec7e":"code","d18a67cf":"code","81201331":"code","d8e89f16":"code","36549c6e":"code","92f67f18":"code","6290efc9":"code","01df92b1":"code","dd5113b0":"code","4ad947df":"code","db71fd69":"code","640241cf":"code","247d23d3":"code","f9bfe3ac":"code","efb74bc9":"code","8a4f0ba8":"markdown","39450612":"markdown","7d044c3c":"markdown","b169dd2a":"markdown","ac9e43e2":"markdown","78fd0915":"markdown","b5442764":"markdown","3368613e":"markdown","f6f8eecf":"markdown","ad2eeacc":"markdown","96575f55":"markdown","4de57143":"markdown","cc44ab24":"markdown","77d41800":"markdown","c536c87d":"markdown","05234e3d":"markdown","8982fbb1":"markdown","59fdbf15":"markdown","66aa22c8":"markdown","de43b07a":"markdown","a28b84d6":"markdown","c0de8b08":"markdown","2fb30bfb":"markdown","82e21c37":"markdown","6c52047e":"markdown","50108c2a":"markdown","c590d528":"markdown","15875512":"markdown","5615dcfb":"markdown","7509c5a2":"markdown"},"source":{"7eb6853f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n# Importing the libraries \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","fbf3109d":"my_data=pd.read_csv(\"..\/input\/case-study-loss-given\/R_Module_Day_5.2_Data_Case_Study_Loss_Given_Default.csv\") \nmy_data.head()","266732b3":"# Check for missing values\nmy_data.isnull().sum()","2f8ae3ab":"# See rows with missing values\nmy_data[my_data.isnull().any(axis=1)]","03369fbb":"my_data.describe()","ffe2b5e8":"## Seaborn visualization library\nsns.set(style=\"ticks\", color_codes=True)\n## Create the default library\nsns.pairplot(my_data)","ebdb6dc8":"# RuntimeWarning: invalid value encountered in divide <to avoid this I am using np.seterr>\nnp.seterr(divide='ignore', invalid='ignore')\nsns.pairplot(my_data, hue= 'Age')","ec91799c":"# RuntimeWarning: invalid value encountered in divide <to avoid this I am using np.seterr>\nnp.seterr(divide='ignore', invalid='ignore')\nsns.pairplot(my_data, hue= 'Years of Experience')","175d5e13":"#normal plot using seaborn\nsns.lmplot(x=\"Age\",y=\"Losses in Thousands\",data=my_data,size=10)","2042141f":"sns.jointplot(data=my_data, x='Years of Experience', y='Losses in Thousands', kind='hex', color=\"#4CB391\",size=10)","bb27fc59":"tableau_20=(197\/255.,101\/255.,213\/255.)\nsns.jointplot(data=my_data, x='Number of Vehicles', y='Losses in Thousands',space=0.2, kind='kde', color=tableau_20,size=10)","6eb411b2":"my_data1=my_data.corr().transpose()\nmy_data1.head()","f3333ec6":"plt.figure(figsize=(20,15))\nsns.heatmap(my_data1)","a12e43e8":"sns.set()\n# Load the example my_data1 dataset and conver to long-form\n#my_data_float = my_data1\ndata_vis = my_data1.pivot(\"Losses in Thousands\",\"Age\",\"Years of Experience\")\n\n# Draw a heatmap with the numeric values in each cell\nf, ax = plt.subplots(figsize=(9, 6))\nsns.heatmap(data_vis, annot=True, fmt=\"f\", linewidths=.5, ax=ax)","d4b46896":"# Spliting target variable and independent variables\nX = my_data1.drop(['Losses in Thousands'], axis = 1)\ny = my_data1['Losses in Thousands']","dc2e1c26":"# Splitting to training and testing data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 4)","80f9b94e":"# Import library for Linear Regression\nfrom sklearn.linear_model import LinearRegression\n\n# Create a Linear regressor\nlm = LinearRegression()\n\n# Train the model using the training sets \nlm.fit(X_train, y_train)","f757edc7":"# Value of y intercept\nlm.intercept_","c61f3c9e":"#Converting the coefficient values to a dataframe\ncoeffcients = pd.DataFrame([X_train.columns,lm.coef_]).T\ncoeffcients = coeffcients.rename(columns={0: 'Attribute', 1: 'Coefficients'})\ncoeffcients","cd0bec7e":"# Model prediction on train data\ny_pred = lm.predict(X_train)","d18a67cf":"# Model Evaluation\nprint('R^2:',metrics.r2_score(y_train, y_pred))\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_train, y_pred))*(len(y_train)-1)\/(len(y_train)-X_train.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_train, y_pred))\nprint('MSE:',metrics.mean_squared_error(y_train, y_pred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_train, y_pred)))","81201331":"# Visualizing the differences between actual losses and predicted losses\nplt.scatter(y_train, y_pred)\nplt.xlabel(\"Losses\")\nplt.ylabel(\"Predicted losses\")\nplt.title(\"Losses vs Predicted Losses\")\nplt.show()","d8e89f16":"# Checking residuals\nplt.scatter(y_pred,y_train-y_pred)\nplt.title(\"Predicted vs residuals\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Residuals\")\nplt.show()","36549c6e":"# Checking Normality of errors\nsns.distplot(y_train-y_pred)\nplt.title(\"Histogram of Residuals\")\nplt.xlabel(\"Residuals\")\nplt.ylabel(\"Frequency\")\nplt.show()","92f67f18":"# Predicting Test data with the model\ny_test_pred = lm.predict(X_test)","6290efc9":"# Model Evaluation\nacc_linreg = metrics.r2_score(y_test, y_test_pred)\nprint('R^2:', acc_linreg)\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_test, y_test_pred))*(len(y_test)-1)\/(len(y_test)-X_test.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_test, y_test_pred))\nprint('MSE:',metrics.mean_squared_error(y_test, y_test_pred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))","01df92b1":"# Import Random Forest Regressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Create a Random Forest Regressor\nreg = RandomForestRegressor()\n\n# Train the model using the training sets \nreg.fit(X_train, y_train)","dd5113b0":"# Model prediction on train data\ny_pred = reg.predict(X_train)","4ad947df":"# Model Evaluation\nprint('R^2:',metrics.r2_score(y_train, y_pred))\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_train, y_pred))*(len(y_train)-1)\/(len(y_train)-X_train.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_train, y_pred))\nprint('MSE:',metrics.mean_squared_error(y_train, y_pred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_train, y_pred)))","db71fd69":"# Visualizing the differences between actual losses and predicted losses\nplt.scatter(y_train, y_pred)\nplt.xlabel(\"Losses\")\nplt.ylabel(\"Predicted losses\")\nplt.title(\"Losses vs Predicted Losses\")\nplt.show()","640241cf":"# Checking residuals\nplt.scatter(y_pred,y_train-y_pred)\nplt.title(\"Predicted vs residuals\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Residuals\")\nplt.show()","247d23d3":"# Checking Normality of errors\nsns.distplot(y_train-y_pred)\nplt.title(\"Histogram of Residuals\")\nplt.xlabel(\"Residuals\")\nplt.ylabel(\"Frequency\")\nplt.show()","f9bfe3ac":"# Predicting Test data with the model\ny_test_pred = reg.predict(X_test)","efb74bc9":"# Model Evaluation\nacc_rf = metrics.r2_score(y_test, y_test_pred)\nprint('R^2:', acc_rf)\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_test, y_test_pred))*(len(y_test)-1)\/(len(y_test)-X_test.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_test, y_test_pred))\nprint('MSE:',metrics.mean_squared_error(y_test, y_test_pred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))","8a4f0ba8":"** the above graph compares AGE vs LOSSES IN THOUSANDS **","39450612":"** the above graph compares NUMBER OF VEHICLES vs LOSSES IN THOUSANDS **","7d044c3c":"** Thus we can see after doing EDA columns like GENDER and MARRIED is removed which is not required for further analysis  **","b169dd2a":"# Random Forest Regressor\n## Train the model\n### A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).","ac9e43e2":"** the above graph compares each attribute with each other just like the previous one but here we have chosen YEARS OF EXPERIENCE as a factor which is highlighted and in the legend on the right side it shows the variation of YEARS OF EXPERIENCE **","78fd0915":"\ud835\udc45^2 : It is a measure of the linear relationship between X and Y. It is interpreted as the proportion of the variance in the dependent variable that is predictable from the independent variable.\n\nAdjusted \ud835\udc45^2 :The adjusted R-squared compares the explanatory power of regression models that contain different numbers of predictors.\n\nMAE : It is the mean of the absolute value of the errors. It measures the difference between two continuous variables, here actual and predicted values of y. \n\nMSE: The mean square error (MSE) is just like the MAE, but squares the difference before summing them all instead of using the absolute value. \n\nRMSE: The mean square error (MSE) is just like the MAE, but squares the difference before summing them all instead of using the absolute value. ","b5442764":"** the above graph compares YEARS OF EXPERIENCE vs LOSSES IN THOUSANDS **","3368613e":"# we are going to use the following methods of algorithm-\n\n## Linear Regression \n\n## Random Forest Regressor\n\n### the reason behind choosing it is \n\n** 1. we can see the output for a given input in the dataset hence we need to follow SUPERVISED LEARNING TECHNIQUE 2. the data is continuos in nature hence regression technique is best suited**","f6f8eecf":"\ud835\udc45^2 : It is a measure of the linear relationship between X and Y. It is interpreted as the proportion of the variance in the dependent variable that is predictable from the independent variable.\n\nAdjusted \ud835\udc45^2 :The adjusted R-squared compares the explanatory power of regression models that contain different numbers of predictors.\n\nMAE : It is the mean of the absolute value of the errors. It measures the difference between two continuous variables, here actual and predicted values of y. \n\nMSE: The mean square error (MSE) is just like the MAE, but squares the difference before summing them all instead of using the absolute value. \n\nRMSE: The mean square error (MSE) is just like the MAE, but squares the difference before summing them all instead of using the absolute value. ","ad2eeacc":"** thus in the above graph the black portion is the most dense region which shows that AGE AND YEARS OF EXPERIENCE are somehow dependednt on the LOSSES IN THOUSANDS component **","96575f55":"# Linear Regression\n## Training the model\n###  linear regression is a linear approach to modeling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression.\nY=MX+C - the general equation where m is the slope X is the independent value and c is Y intercept and Y is the dependent value on the equation","4de57143":"## Data Correlation: Is a way to understand the relationship between multiple variables and attributes in your dataset. Using Correlation, you can get some insights such as: One or multiple attributes depend on another attribute or a cause for another attribute.","cc44ab24":"# Importing necessary libraries and packages\n## First of all we would import all the important and necessary files needed for this project\n## Also the location of the dataset uploaded in Kaggle can be found here for reference","77d41800":"** the above graph compares each attribute with each other just like the previous one but here we have chosen AGE as a factor which is highlighted and in the legend on the right side it shows the variation of AGES **","c536c87d":"## finally we are going to split the data into two parts the training and test part ","05234e3d":"# Visualizing Data with Plots","8982fbb1":"# Model Evaluation","59fdbf15":"** over here what we have done is plotted AGE VS LOSS IN THOUSANDS with YEARS OF EXPERIENCE in the value postion and same thing is observed as the AGE increases the LOSS component decreases but Years of Experience increases which is Obvious **","66aa22c8":"# Model Evaluation","de43b07a":"### For test data\n","a28b84d6":"Exactly \u20131. A perfect downhill (negative) linear relationship \n\n\u20130.70. A strong downhill (negative) linear relationship \n\n\u20130.50. A moderate downhill (negative) relationship \n\n\u20130.30. A weak downhill (negative) linear relationship \n\n0. No linear relationship \n\n+0.30. A weak uphill (positive) linear relationship \n\n+0.50. A moderate uphill (positive) relationship \n\n+0.70. A strong uphill (positive) linear relationship \n\nExactly +1. A perfect uphill (positive) linear relationship","c0de8b08":"### For Test Data","2fb30bfb":"# Correlation of the dataset","82e21c37":"# Explaratory Data Analysis (EDA)","6c52047e":"** in the above table we can see that LOSSES IN THOUSANDS is closely related to AGE and YEARS OF EXPERIENCE **","50108c2a":"### we are using heatmap in order to visualise the  volume of an event","c590d528":"# DATASET READING TO PANDAS DATAFRAME\n### then printing the head part only","15875512":"# Showing in heatmap now","5615dcfb":"# Splitting the data into Train and Test Set","7509c5a2":"** the above graph compares each attribute with other attributes thus helping us to understand which factor contributes to the LOSSES **"}}