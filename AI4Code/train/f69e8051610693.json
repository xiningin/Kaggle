{"cell_type":{"5c205ed3":"code","bae7adb5":"code","e1c4b729":"code","d69eb0f2":"code","30827860":"code","39ef423a":"code","9a0507ea":"code","9faf6b6f":"code","7fb9559c":"code","0d7dd80b":"code","701f3ff5":"code","7c03c4fc":"code","4c0d0269":"code","42d94867":"code","3be06a49":"code","34328c69":"code","c36bb2b3":"code","3a0dea10":"code","7271823f":"code","cde41454":"code","d16d6183":"code","493bbda0":"code","23525c20":"code","28eefac1":"code","a0d67f82":"code","798caaae":"markdown","e4214371":"markdown","801df354":"markdown","222af168":"markdown","40392757":"markdown","611a24b5":"markdown"},"source":{"5c205ed3":"# utils\nimport numpy as np\nimport pandas as pd\nimport json\nimport string\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport pickle\n# modeling utils\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    accuracy_score, \n    f1_score, \n    classification_report\n)\n# BERT\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader","bae7adb5":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e1c4b729":"def pull_data_and_dedupe(n_obs = -1):\n    data  = []\n    with open(\"..\/input\/arxiv\/arxiv-metadata-oai-snapshot.json\", 'r') as f:\n        for line in f: \n            data.append(json.loads(line))\n    data = pd.DataFrame(data)\n    \n    if n_obs > 0:\n        data = data.head((n_obs + 1000))# to account for deduping and withdrawn papers\n    \n    keep_cols = ['id', 'title', 'abstract', 'categories']\n    data = data[keep_cols]\n    # remove abstracts from withdrawn records\n    data = data[data['abstract'].str.contains('paper has been withdrawn') == False]\n    \n    # create columns for each unique title\n    categories = set([i for l in [x.split(' ') for x in data['categories']] for i in l])\n    for un in categories:\n        data[un] = np.where(data['categories'].str.contains(un), 1, 0)\n    # remove duplicate records which contain different flags\n    data = data.drop(columns = 'categories').groupby(by = ['id', 'title', 'abstract'],as_index = False).max()\n    \n    data.reset_index(drop = True, inplace = True)\n    if n_obs > 0:\n        data = data.head(n_obs)\n    \n    return data, categories","d69eb0f2":"def abstract_prep(data):\n    # lower abstract and remove numbers, punctuation, and special characters\n    #metadata['abstract'] = [a.strip() for a in metadata['abstract']]\n    data['abstract'] = [a.lower().strip() for a in data['abstract']]\n    data['abstract'] = data['abstract'].str.replace('\\n', ' ', regex = False).str.replace(r'\\s\\s+', ' ', regex = True)\n    data['abstract'] = data['abstract'].str.replace('([.,!?()])', r' \\1 ')\n    return data","30827860":"# generate counts by label\ndef handle_category_counts(data, show_plot = True):\n    category_count = data[data.columns[3:].to_list()].sum()\n    \n    if show_plot:\n        # create plot\n        labs = category_count.sort_values(ascending = False).index.to_list()\n        counts = category_count.sort_values(ascending = False).values\n        fig = plt.figure(figsize=(16, 9))\n        ax = fig.gca()\n        fig.suptitle('Bottom 50 Categories', fontsize=20)\n        plt.ylabel('Number of Papers', fontsize=14)\n        plt.xlabel('Labeled Paper Category', fontsize=14)\n        plt.setp(ax.get_xticklabels(), ha=\"right\", rotation=45, fontsize=14) # Specify a rotation for the tick labels \n        plt.bar(labs[(len(category_count)-50):],counts[(len(category_count)-50):])\n        plt.show()\n\n        # plot sentence lengths to determine max length\n        abstract_lengths = [len(t.split()) for t in data['abstract']]\n        abstract_lengths.sort()\n        plt.hist([i for i in abstract_lengths if i > 299 and i < 400])\n        plt.show()\n    \n    # determine which categories to preserve and update dataframe\n    categories = list(category_count[category_count > 15].index)\n    # remove columns pertaining to categories to exclude  \n    data = data[list(data.columns[:3]) + categories]\n    print('\\tfinal df size:', data.shape)\n    \n    # isolate data\n    labels = data.loc[:, categories].values\n    input_data = data[['id', 'abstract']]\n    \n    return metadata, labels, input_data, categories\n","39ef423a":"# fixed parameters and hyperparameters for dataset creation and model training\nclass Config:\n    def __init__(self, categories):\n        # allow class to be inherited\n        super(Config, self).__init__()\n        \n        # general parameters\n        self.SEED = 9\n        self.MODEL_PATH = \"allenai\/scibert_scivocab_uncased\"\n        self.NUM_LABELS = len(categories)\n        \n        # load tokenizer and set related parameters\n        self.TOKENIZER = BertTokenizer.from_pretrained(self.MODEL_PATH)\n        self.MAX_LENGTH = 350 # from EDA\n        \n        # determine optimal batch size based on \n        self.N_GPU = torch.cuda.device_count()\n        if self.N_GPU == 0:\n            self.N_GPU = 1\n        self.BATCH_SIZE = self.N_GPU * 8\n            \n        # validation & test split\n        self.VALIDATION_SPLIT = .3\n        \n        # set model parameters\n        self.LR = 3e-5\n        self.CRITERION = nn.BCEWithLogitsLoss()\n        self.EPOCHS = 5\n        \n        # model selection\n        self.SELECTION_METRIC = 'accuracy' # options: f1 accuracy loss","9a0507ea":"# dataset object\nclass arxiv_dataset(torch.utils.data.Dataset):\n            \n    def __init__(self, abstrcts, lbls, msks):\n        self.abstrcts = torch.Tensor(abstrcts).long()\n        self.msks = torch.Tensor(msks).long()\n        # cast labels as float\n        self.lbls = torch.Tensor(lbls).float()\n        \n    def __len__(self):\n        return self.lbls.shape[0]\n    \n    def __getitem__(self, index):\n        abstracts_ = self.abstrcts[index, :]\n        labels_ = self.lbls[index, :]\n        masks_ = self.msks[index, :]\n        return abstracts_, labels_, masks_","9faf6b6f":"# tokenize, split data, create dataloader objects from arxiv dataset objects\ndef bert_prep(input_data, labels, config):\n    tokenizer = BertTokenizer.from_pretrained(config.MODEL_PATH)\n    tokenized_abstracts = tokenizer.batch_encode_plus(\n                input_data['abstract'],\n                max_length = config.MAX_LENGTH,\n                pad_to_max_length = True,\n                truncation = True,\n                return_attention_mask = True,\n                return_token_type_ids = False,\n                return_tensors = 'pt'\n            )\n\n    # initial train and test split\n    token_train, token_test, mask_train, mask_test, \\\n    y_train, y_test = train_test_split(np.array(tokenized_abstracts['input_ids']),\n                                       np.array(tokenized_abstracts['attention_mask']), \n                                       np.array(labels), \n                                       test_size = config.VALIDATION_SPLIT,\n                                       random_state = config.SEED)\n    # split test into test and validation\n    token_val, token_test, mask_val, mask_test, \\\n    y_val, y_test = train_test_split(token_test,\n                                     mask_test,\n                                     y_test,\n                                     test_size = 0.5,\n                                     random_state = config.SEED)\n\n    # qc check\n    print('QC......')\n    print('\\tabstract - training:', token_train.shape)\n    print('\\tmask - training:', mask_train.shape)\n    print('\\tlabel - training:', y_train.shape)\n    print('\\tabstract - validation:', token_val.shape)\n    print('\\tmask - valdiation:', mask_val.shape)\n    print('\\tlabel - validation:', y_val.shape)\n    print('\\tabstract - test:', token_test.shape)\n    print('\\tmask - test:', mask_test.shape)\n    print('\\tlabel - test:', y_test.shape)\n\n\n    ## create datasets and loaders\n    train_data = arxiv_dataset(token_train, y_train, mask_train)\n    val_data = arxiv_dataset(token_val, y_val, mask_val)\n    test_data = arxiv_dataset(token_test, y_test, mask_test)\n\n    train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=config.BATCH_SIZE)\n    val_dataloader = torch.utils.data.DataLoader(val_data, batch_size=config.BATCH_SIZE)\n    test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=config.BATCH_SIZE)\n    \n    return train_dataloader, val_dataloader, test_dataloader","7fb9559c":"# train and validate model \ndef train_model(train_dataloader, val_dataloader, test_dataloader, config):\n    # intitialize model\n    model = BertForSequenceClassification.from_pretrained(config.MODEL_PATH, num_labels=config.NUM_LABELS)\n    if torch.cuda.is_available():\n        model = model.cuda() \n        \n    # set optimizer\n    param_optimizer = list(model.named_parameters())\n    # According to the huggingface recommendations\n    # weight decay is set to 0 for bias layers\n    no_decay = ['bias', 'gamma', 'beta']\n    optimizer_grouped_parameters = [{'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n                                     'weight_decay_rate': 0.01},\n                                    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n                                     'weight_decay_rate': 0.0}]\n    # Using BERT's Adam optimizer similar to the original Tensorflow optimizer\n    optimizer = AdamW(optimizer_grouped_parameters,\n                      lr = config.LR,\n                      weight_decay = 0.01,\n                      correct_bias = False)\n    \n    ## initialize values\n    if config.N_GPU > 1:\n        model = nn.DataParallel(model)\n    \n    epoch_train_loss = []\n    epoch_valid_loss = []\n    epoch_valid_f1 = []\n    epoch_valid_accuracy = []\n    epoch_test_loss = []\n    epoch_test_f1 = []\n    epoch_test_accuracy = []\n\n    ## Training\/Validation Loop\n    for epoch in range(1, config.EPOCHS + 1):\n        print('\\tEPOCH:', epoch)\n\n        train_loss = 0.0\n        valid_loss = 0.0\n        test_loss = 0.0\n\n        ######### TRAINING #############\n        # set model to train mode\n        model.train()\n\n        #batch = 1\n        # iterate through each observation\n        for data in train_dataloader:\n            #print('BATCH:', batch)\n            abstracts_, labels_, masks_ = data\n\n            # move data to GPU\n            if torch.cuda.is_available():\n                abstracts_ = abstracts_.cuda()\n                masks_ = masks_.cuda()\n                labels_ = labels_.cuda()\n\n            # zero out optimizer gradients\n            optimizer.zero_grad()\n\n            # fit model and calculate loss\n            logits = model(input_ids = abstracts_, attention_mask = masks_)[0]\n            loss = config.CRITERION(logits, labels_)\n\n            if config.N_GPU > 1 :\n                loss = loss.mean()\n\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n            #batch += 1\n\n        epoch_t_loss = train_loss\/len(train_dataloader)\n        print(f\"\\t\\tTrain loss: {epoch_t_loss}\")\n\n        ###### VALIDATION ########\n        # set model to train mode\n        model.eval()\n\n        valid_truth = []\n        valid_preds = []\n\n        # iterate through each observation\n        for data in val_dataloader:\n            #print('BATCH:', batch)\n            abstracts_, labels_, masks_ = data\n            # move data to GPU\n            if torch.cuda.is_available():\n                abstracts_ = abstracts_.cuda()\n                masks_ = masks_.cuda()\n                labels_ = labels_.cuda()\n\n            # fit model and calculate loss\n            logits = model(input_ids = abstracts_, attention_mask = masks_)[0]\n            loss = config.CRITERION(logits, labels_)\n\n            if config.N_GPU > 1 :\n                loss = loss.mean()\n\n            valid_loss += loss.item()\n            #batch += 1\n\n            # since we're using BCEWithLogitsLoss, to get the predictions -\n            # - sigmoid has to be applied on the logits first\n            logits_cpu = torch.sigmoid(logits)\n            logits_cpu = np.round(logits_cpu.cpu().detach().numpy())\n            labels_cpu = labels_.cpu().numpy()\n\n            # keep list of outputs for validation\n            valid_truth.extend(labels_cpu)\n            valid_preds.extend(logits_cpu)\n            \n        ### Validation Metrics \n        epoch_v_loss = valid_loss\/len(val_dataloader)\n        print(f\"\\t\\tValid loss: {epoch_v_loss}\")\n\n        epoch_v_accuracy_score = accuracy_score(valid_truth,valid_preds)\n        print('\\t\\tVal Accuracy:', epoch_v_accuracy_score)\n\n        epoch_v_micro_f1_score = f1_score(valid_truth,valid_preds, average='micro')\n        print('\\t\\tVal Micro F1 score:', epoch_v_micro_f1_score)\n            \n        ###### TEST ########\n        test_truth = []\n        test_preds = []\n\n        # iterate through each observation\n        for data in test_dataloader:\n            #print('BATCH:', batch)\n            abstracts_, labels_, masks_ = data\n            # move data to GPU\n            if torch.cuda.is_available():\n                abstracts_ = abstracts_.cuda()\n                masks_ = masks_.cuda()\n                labels_ = labels_.cuda()\n\n            # fit model and calculate loss\n            logits = model(input_ids = abstracts_, attention_mask = masks_)[0]\n            loss = config.CRITERION(logits, labels_)\n\n            if config.N_GPU > 1 :\n                loss = loss.mean()\n\n            test_loss += loss.item()\n            #batch += 1\n\n            # since we're using BCEWithLogitsLoss, to get the predictions -\n            # - sigmoid has to be applied on the logits first\n            logits_cpu = torch.sigmoid(logits)\n            logits_cpu = np.round(logits_cpu.cpu().detach().numpy())\n            labels_cpu = labels_.cpu().numpy()\n\n            # keep list of outputs for validation\n            test_truth.extend(labels_cpu)\n            test_preds.extend(logits_cpu)\n\n        \n        epoch_tst_loss = test_loss\/len(test_dataloader)\n        print(f\"\\t\\tTest loss: {epoch_tst_loss}\")\n\n        epoch_tst_accuracy_score = accuracy_score(test_truth,test_preds)\n        print('\\t\\tTest Accuracy:', epoch_tst_accuracy_score)\n\n        epoch_tst_micro_f1_score = f1_score(test_truth,test_preds, average='micro')\n        print('\\t\\tTest Micro F1 score:', epoch_tst_micro_f1_score)\n\n        \n        # if validation selection metric improved, set the best model to the current model\n        if config.SELECTION_METRIC == 'loss':\n            if len(epoch_valid_loss) == 0:\n                best_model = model\n                best_epoch = epoch\n            else:\n                if epoch_valid_loss[-1] > epoch_v_loss:\n                    print('\\t\\tReplace model with version from epoch', epoch, 'based on', config.SELECTION_METRIC)\n                    best_model = model\n                    best_epoch = epoch\n\n        if config.SELECTION_METRIC == 'f1':\n            if len(epoch_valid_f1) == 0:\n                best_model = model\n                best_epoch = epoch\n            else:\n                if epoch_valid_f1[-1] < epoch_v_micro_f1_score:\n                    print('\\t\\tReplace model with version from epoch', epoch, 'based on', config.SELECTION_METRIC)\n                    best_model = model\n                    best_epoch = epoch\n\n        if config.SELECTION_METRIC == 'accuracy':\n            if len(epoch_valid_accuracy) == 0:\n                best_model = model\n                best_epoch = epoch\n            else:\n                if epoch_valid_accuracy[-1] < epoch_v_accuracy_score:\n                    print('\\t\\tReplace model with version from epoch', epoch, 'based on', config.SELECTION_METRIC)\n                    best_model = model\n                    best_epoch = epoch\n\n        # update epoch loss lists\n        epoch_train_loss.append(epoch_t_loss)\n        epoch_valid_loss.append(epoch_v_loss)\n        epoch_valid_f1.append(epoch_v_micro_f1_score)\n        epoch_valid_accuracy.append(epoch_v_accuracy_score)\n        epoch_test_loss.append(epoch_tst_loss)\n        epoch_test_f1.append(epoch_tst_micro_f1_score)\n        epoch_test_accuracy.append(epoch_tst_accuracy_score)\n\n    tracker_df = pd.DataFrame({'epoch' : list(range(1,config.EPOCHS + 1)),\n                               'train_loss' : epoch_train_loss,\n                               'validation_loss' : epoch_valid_loss,\n                               'validation_accuracy' : epoch_valid_accuracy,\n                               'validation_f1' : epoch_valid_f1,\n                               'test_loss' : epoch_test_loss,\n                               'test_accuracy' : epoch_test_accuracy,\n                               'test_f1' : epoch_test_f1})\n    tracker_df['best_model_indicator'] = np.where(tracker_df['epoch'] == best_epoch, 1, 0)\n    \n    return best_model, tracker_df, valid_truth, valid_preds, test_truth, test_preds, best_epoch","0d7dd80b":"def run(metadata, categories, n_obs):\n    print('*********************************************** START: Fitting Model with', n_obs, 'Obs ***********************************************')\n    print('Prepping data.....')\n    metadata = abstract_prep(metadata)\n    \n    print('EDA.....')\n    metadata, labels, input_data, categories = handle_category_counts(metadata, show_plot = False)\n    \n    print('Prepping data for BERT.....')\n    config = Config(categories)\n    train_dataloader, val_dataloader, test_dataloader = bert_prep(input_data, labels, config)\n    \n    print('Training and validating BERT.....')\n    best_model, tracker_df, valid_truth, \\\n    valid_preds, test_truth, test_preds, best_epoch = train_model(train_dataloader, val_dataloader, test_dataloader, config)\n    \n    print('Saving best performing model.....')\n    model_filepath = datetime.now().strftime('%Y%m%d') + '_' + str(n_obs) + '_' + str(config.EPOCHS) + '_best' + str(best_epoch) +'.pt'\n    torch.save(best_model.state_dict(), model_filepath)\n    print('\\tsaved model: ' + model_filepath)\n    \n    print('*********************************************** END: Fitting Model with', n_obs, 'Obs ***********************************************')\n    return best_model, train_dataloader, val_dataloader, test_dataloader, tracker_df, valid_truth, valid_preds, test_truth, test_preds, config, tracker_df, categories, model_filepath","701f3ff5":"############ SET PARAMS ##############\nn_obs = 150000","7c03c4fc":"%%time\nmetadata, categories = pull_data_and_dedupe(n_obs)","4c0d0269":"%%time\nbest_model, train_dataloader, val_dataloader, test_dataloader, tracker_df, valid_truth, valid_preds, test_truth, test_preds, config, tracker_df, categories, model_filepath = run(metadata, categories, n_obs) # test","42d94867":"# save training info\ntracker_df.to_csv('tracker_df_' + datetime.now().strftime('%Y%m%d') + '_' + str(n_obs) + '_' + str(config.EPOCHS) + '.csv')","3be06a49":"# save params associated with the final model\npredict_params = dict({'base_model_path' : config.MODEL_PATH, \n                       'best_model_path' : model_filepath,\n                       'number_of_labels' : config.NUM_LABELS,\n                       'max_length_tokenize' : config.MAX_LENGTH,\n                       'categories' : categories})\npickle.dump(predict_params,open('predict_params.pkl','wb'))","34328c69":"def prep_text(text):\n    text = text.lower().strip()\n    text = text.replace('\\n', ' ').replace(r'\\s\\s+', ' ')\n    text = text.replace('([.,!?()])', r' \\1 ')\n    return text","c36bb2b3":"def tokenize_text(text, model_path, max_length_tokenize):\n    tokenizer = BertTokenizer.from_pretrained(model_path)\n    tokenized_abstract = tokenizer.encode_plus(\n                text,\n                max_length = max_length_tokenize, # from model\n                pad_to_max_length = True,\n                truncation = True,\n                return_attention_mask = True,\n                return_token_type_ids = False,\n                return_tensors = 'pt'\n            )\n    return tokenized_abstract","3a0dea10":"def load_model(predict_params_dict, model_path):\n    # load model\n    model = BertForSequenceClassification.from_pretrained(predict_params_dict['base_model_path'], num_labels=predict_params_dict['number_of_labels'])\n    model.load_state_dict(torch.load(model_path))\n    model.eval()\n    return model","7271823f":"def predict_text(text, model, predict_params_dict, threshold):\n    # text processing\n    input_text = prep_text(text)\n    print('INPUT:',input_text)\n    tokenized_text = tokenize_text(text = input_text, \n                                   model_path = predict_params_dict['base_model_path'],\n                                   max_length_tokenize = predict_params_dict['max_length_tokenize'])\n\n    abstracts_ = tokenized_text['input_ids']\n    masks_ = tokenized_text['attention_mask']\n\n    logits = model(input_ids = abstracts_, attention_mask = masks_)[0]\n    logits = torch.sigmoid(logits)\n    logits = logits.detach().numpy()[0]\n\n    ## generate results\n    results_series = pd.Series(dict(zip(predict_params_dict['categories'], logits)))\n    if len(results_series[results_series > threshold]) == 0:\n        tags = 'No relevant tags'\n    else:\n        tags = results_series[results_series > threshold].index\n    print('\\nTAGS:',tags)\n    \n    return tags, results_series","cde41454":"# load pickle file with param dict\npredict_params_dict_path = 'predict_params.pkl'\npredict_params_dict = pickle.load(open(predict_params_dict_path,'rb'))\n\n# format model path to kaggle's format\nmodel_path = \".\/\" + predict_params_dict['best_model_path']","d16d6183":"# load model\nmodel = load_model(predict_params_dict, model_path)","493bbda0":"## raw text for prediction\ntext = \"  The evolution of Earth-Moon system is described by the dark matter field\\nfluid model proposed in the Meeting of Division of Particle and Field 2004,\\nAmerican Physical Society. The current behavior of the Earth-Moon system agrees\\nwith this model very well and the general pattern of the evolution of the\\nMoon-Earth system described by this model agrees with geological and fossil\\nevidence. The closest distance of the Moon to Earth was about 259000 km at 4.5\\nbillion years ago, which is far beyond the Roche's limit. The result suggests\\nthat the tidal friction may not be the primary cause for the evolution of the\\nEarth-Moon system. The average dark matter field fluid constant derived from\\nEarth-Moon system data is 4.39 x 10^(-22) s^(-1)m^(-1). This model predicts\\nthat the Mars's rotation is also slowing with the angular acceleration rate\\nabout -4.38 x 10^(-22) rad s^(-2).\\n\"\ntext","23525c20":"# prep text\ninput_text = prep_text(text)","28eefac1":"%%time\n# predict\ntags, results_series = predict_text(input_text, model, predict_params_dict,.5)","a0d67f82":"# view results\nresults_series.sort_values(ascending = False)","798caaae":"**Model Prep**","e4214371":"**Data Prep**","801df354":"### Predict Functions","222af168":"**Functions**","40392757":"**Run**","611a24b5":"**Sources:** Transformers Huggingface Documentation, https:\/\/www.kaggle.com\/prithvijaunjale\/scibert-multi-label-classification-dual-input, https:\/\/www.kaggle.com\/colinlagator\/pytorch-bert-multi-label"}}