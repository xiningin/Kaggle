{"cell_type":{"b08ec49a":"code","f4f8dfbd":"code","85c62bc5":"code","81bb8e38":"code","f342345f":"code","99ec5922":"code","ac41004b":"code","f05168d9":"code","65cf6760":"code","9e134233":"code","9b5133a8":"code","3a71667c":"code","0b46e24c":"code","b284f623":"code","b376332f":"code","48be7f73":"code","86066596":"code","e78cabb0":"code","d1e15bcc":"code","991b47d3":"code","60330b94":"code","001b8ec1":"code","16a6ea20":"code","3d3b91a9":"code","ff48da16":"code","0d94aa42":"code","76d2a90b":"code","1c542d83":"code","b135e506":"code","aecf949f":"markdown","01e5a9a4":"markdown","441826fd":"markdown","b0f1c5d1":"markdown","b342e23e":"markdown","023a595b":"markdown","fd335a84":"markdown","c8724641":"markdown","b84bd2c7":"markdown"},"source":{"b08ec49a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f4f8dfbd":"from sklearn.model_selection import train_test_split, GridSearchCV\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_columns', 10000)\npd.set_option('display.max_rows', 10000)","85c62bc5":"df_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\ndf_survive = df_train['Survived']\nId = df_test['PassengerId']","81bb8e38":"df_train.info()","f342345f":"df_test.info()","99ec5922":"df_train.isnull().sum()","ac41004b":"df_test.isnull().sum()","f05168d9":"df_train['Age'].describe()","65cf6760":"df_train['Age'] = df_train['Age'].fillna(df_train['Age'].median())\ndf_test['Age'] = df_test['Age'].fillna(df_test['Age'].median())","9e134233":"df_train['Sex'] = df_train['Sex'].apply(lambda x: 1 if x == 'male' else 0)\ndf_test['Sex'] = df_test['Sex'].apply(lambda x: 1 if x == 'male' else 0)","9b5133a8":"df_train['FamilySize'] = df_train['SibSp'] + df_train['Parch'] + 1\ndf_test['FamilySize'] = df_test['SibSp'] + df_test['Parch'] + 1","3a71667c":"def family_group(s):\n    if (s >= 2) & (s <= 4):\n        return 2\n    elif ((s > 4) & (s <= 7)) | (s == 1):\n        return 1\n    elif (s > 7):\n        return 0","0b46e24c":"df_train['FamilyGroup'] = df_train['FamilySize'].apply(family_group)\ndf_test['FamilyGroup'] = df_test['FamilySize'].apply(family_group)","b284f623":"def age_group(s):\n    if (s > 0) & (s < 10):\n        return 1\n    elif (s > 10) & (s <= 20):\n        return 2\n    elif (s > 20) & (s <= 30):\n        return 3\n    elif (s > 30) & (s <= 40):\n        return 4\n    elif (s > 40) & (s <= 50):\n        return 5\n    elif (s > 50) & (s <= 60):\n        return 6\n    elif (s > 60) & (s <= 70):\n        return 7\n    elif (s > 70) & (s <= 80):\n        return 8","b376332f":"df_train['AgeGroup'] = df_train['Age'].apply(age_group)\ndf_test['AgeGroup'] = df_test['Age'].apply(age_group)","48be7f73":"def fare_group(s):\n    if (s <= 8):\n        return 1\n    elif (s > 8) & (s <= 15):\n        return 2\n    elif (s > 15) & (s <= 31):\n        return 3\n    elif s > 31:\n        return 4","86066596":"df_train['FareGroup'] = df_train['Fare'].apply(fare_group)\ndf_test['FareGroup'] = df_test['Fare'].apply(fare_group)","e78cabb0":"df_dummy = pd.get_dummies(df_train['Pclass'],prefix='Pclass')\ndf_train = pd.concat([df_train,df_dummy],axis=1)\ndel df_train['Pclass']\ndf_dummy = pd.get_dummies(df_test['Pclass'],prefix='Pclass')\ndf_test = pd.concat([df_test,df_dummy],axis=1)\ndel df_test['Pclass']","d1e15bcc":"df = df_train.drop(['Survived', 'PassengerId','Name', 'Ticket', 'Age', 'Fare','Embarked','Cabin'], axis = 1)","991b47d3":"df_test = df_test.drop(['Name', 'Ticket','Age', 'Fare','Embarked','Cabin','PassengerId'],axis = 1)","60330b94":"X_train = df\ny_train = df_survive\nX_train, X_test, y_train, y_test = train_test_split(\n    X_train, y_train, test_size=0.2)","001b8ec1":"search_xgb = {'n_estimators': [1, 10, 50, 100, 1000],\n              'max_features': [1, 5, 10, 15, 20],\n              'random_state'      : [0, 10, 50],\n              'min_samples_split' : [3, 5, 10, 15, 20],\n              'max_depth'         : [1, 5, 10, 20, 100]}","16a6ea20":"gs_xgb = GridSearchCV(XGBClassifier(),\n                      search_xgb,\n                      cv=5, #Cross Validation\n                      verbose=True, #Display logs\n                      n_jobs=-1) #Multi Tasking\ngs_xgb.fit(X_train, y_train)\n \nprint(gs_xgb.best_estimator_)","3d3b91a9":"search_lgb = {\"max_depth\": [10, 25, 50, 75, 100, 1000],\n              \"learning_rate\" : [0.001,0.01,0.05,0.1],\n              \"num_leaves\": [100,300,900,1200],\n              \"n_estimators\": [100,200,500,1000]\n             }","ff48da16":"gs_lgb = GridSearchCV(LGBMClassifier(),\n                      search_lgb,\n                      cv=5, #Cross Validation\n                      verbose=True, #Display logs\n                      n_jobs=-1) #Multi Tasking\ngs_lgb.fit(X_train, y_train)\n \nprint(gs_lgb.best_estimator_)","0d94aa42":"XGB = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n                    colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n                    importance_type='gain', interaction_constraints='',\n                    learning_rate=0.300000012, max_delta_step=0, max_depth=5,\n                    min_child_weight=1,monotone_constraints='()', n_estimators=10,\n                    n_jobs=0,num_parallel_tree=1, random_state=0, reg_alpha=0,\n                    reg_lambda=1,scale_pos_weight=1, subsample=1, tree_method='exact',\n                    validate_parameters=1,verbosity=None)\nXGB.fit(X_train,y_train)","76d2a90b":"LGB = LGBMClassifier(learning_rate=0.05, max_depth=10, num_leaves=100)\nLGB.fit(X_train , y_train)","1c542d83":"print (\"Training score:\",XGB.score(X_train,y_train),\"Test Score:\",XGB.score(X_test,y_test))\nprint (\"Training score:\",LGB.score(X_train,y_train),\"Test Score:\",LGB.score(X_test,y_test))","b135e506":"y_test = LGB.predict(df_test)\n\nsubmission = pd.DataFrame({'PassengerId':Id, 'Survived':y_test})\nsubmission.to_csv('submission.csv', index = False)","aecf949f":"If you are not installed modules, please install first.<br>\n\npip install scikit-learn<br>\npip install xgboost<br>\npip install lightgbm\n","01e5a9a4":"<h1>Hi, Kagglers!<\/h1>\n\n<br>\nFor first time I made a notebook.<br>\nIf you have any questions and advices, please comment :)<br>\n\n\nI used LGBM and XGBoost to predict Survived.<br>\nThey have much parameters affecting prediction.<br>\nI think **Parameter Tuning** is neccesary for this.<br>\n\nSome of Tuning code I write down this kernel. please check.\n(It's really long time to tune parameters. I recommend to run on Kaggle Notebook.)\n\n\n\nI enjoy the Kaggle Life !!<br>\nThank you.\n","441826fd":"<h2>Submit prediction<\/h2>","b0f1c5d1":"<h3>Thank you for reading. If you like, please upvote!<\/h3>","b342e23e":"<h2>Grid Search<\/h2>\nI did GridSearch (It's hidden input\/output. It costs time.).","023a595b":"<h2>Data Cleaning<\/h2>","fd335a84":"<h2>Prepare training<\/h2>","c8724641":"<h2>Check data<\/h2>","b84bd2c7":"<h2>Check Score<\/h2>"}}