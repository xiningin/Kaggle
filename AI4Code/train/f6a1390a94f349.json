{"cell_type":{"1eb92320":"code","d05ded97":"code","23b1d999":"code","3c2ac149":"code","84db914a":"code","c2ee2e5f":"code","86b4faa1":"code","4af958c0":"code","fd6c278f":"code","d4043162":"code","60afcd00":"code","7ebc9e62":"code","447f8629":"code","13d01ada":"code","453fa4d4":"code","6e63c544":"code","d198ce0b":"code","cc1b43c0":"code","21c6db83":"code","21610ca7":"code","481d6fc9":"code","4a5edd9e":"code","9cd3011b":"code","d86e35b8":"code","cdbf3ab9":"code","07e76aef":"code","496975af":"code","6c37d73c":"code","6f7991cd":"code","db4f76d7":"code","78ff7bcb":"code","b7a895a7":"code","74ee3a8c":"code","f14303b9":"code","abd3f409":"code","c646c23c":"code","3d79dcba":"code","69e6329b":"code","7b761d8a":"code","c44f0602":"code","113b9dd6":"code","f22d82cb":"code","d0282eb2":"code","a5d2b130":"code","bc7b8f9d":"code","23df2bca":"code","6b596152":"markdown","3edcf398":"markdown","a798e1d4":"markdown","eecb80de":"markdown","6af7ac9c":"markdown","7b65c4fc":"markdown","bd803875":"markdown"},"source":{"1eb92320":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings; warnings.simplefilter('ignore')","d05ded97":"# Build a full dataset of all datasets in the input dir with pandas\nfrom pathlib import Path\nfull_dataset = pd.concat([pd.read_csv(str(x.resolve())) for x in Path(\"\/kaggle\/input\/goodreads-book-datasets-10m\/\").glob(\"*k.csv\")])\nfull_dataset.info()","23b1d999":"# Each rating column looks like x:432411 where the x is the star count.\n# since this is redundant and ruins the data typing, we can take it out.\n# turn the RatingDist columns into itegers by splitting of the first part.\nfor i in range(1,6):\n    full_dataset['RatingDist'+str(i)] = full_dataset['RatingDist'+str(i)].str.split(':').str[1]\nfull_dataset[\"RatingDistTotal\"] = full_dataset[\"RatingDistTotal\"].str.split(':').str[1]\n\nto_convert = [\"RatingDist1\", \"RatingDist2\", \"RatingDist3\", \"RatingDist4\", \"RatingDist5\", \"RatingDistTotal\"]\nfor col in to_convert:\n    full_dataset[col] = full_dataset[col].astype(int)\n\nfull_dataset.info()","3c2ac149":"# Make our dataset smaller in memory Thanks to : https:\/\/www.kaggle.com\/aantonova\/some-new-risk-and-clusters-feature\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\nreduce_mem_usage(full_dataset)","84db914a":"# look at the dates\nfull_dataset[[\"PublishYear\", \"PublishMonth\", \"PublishDay\"]].describe()","c2ee2e5f":"# so something is seriously wrong with publish month... 31st month??? huh? lets drop it.\nfull_dataset.drop(columns=[\"PublishMonth\"], inplace=True)","86b4faa1":"# take a look at the years in the dataset\nfull_dataset[(full_dataset['PublishYear'] < 1400) | (full_dataset['PublishYear'] > 2020)]","4af958c0":"# pretty sure 3006 hasn't happenned yet, so let's filter those out\nfull_dataset = full_dataset[full_dataset['PublishYear'] > 1400]\nfull_dataset = full_dataset[full_dataset['PublishYear'] < 2022]\nfull_dataset.info(show_counts=True)","fd6c278f":"print(full_dataset.Language.unique())\nfull_dataset[\"Language\"].describe()","d4043162":"# so it's predominatly english books, but lets change en-GB and en-CA to english\nfull_dataset[\"Language\"].replace([\"en-GB\", \"en-CA\", \"en-US\"], \"eng\", inplace=True)\nprint(full_dataset.Language.unique())\nfull_dataset[\"Language\"].describe()","60afcd00":"# we will then drop the rest of the languages as our simple rec-sys will be english only for now.\nfull_dataset = full_dataset[full_dataset[\"Language\"] == \"eng\"]\nfull_dataset.info(show_counts=True)","7ebc9e62":"# lets check out deplicates\nfull_dataset.Name.value_counts()[:20]","447f8629":"# so there are alot of duplicate listings here, lets drop all the books by the excat same name and authors\n# this should remove books that were re-pulished by the same author and name but possibly by a different publisher\nfull_dataset.drop_duplicates(subset=[\"Authors\", \"Name\"], inplace=True)\nfull_dataset.info(show_counts=True)","13d01ada":"full_dataset.Name.value_counts()[:20]","453fa4d4":"full_dataset.head()","6e63c544":"# drop some columns we won't need for now\nfull_dataset.drop(columns=[\"PagesNumber\", \"CountsOfReview\", \"Count of text reviews\", \"pagesNumber\"], inplace=True)","d198ce0b":"# fill the empty descriptions with the names of the books\nfull_dataset.Description.fillna(full_dataset.Name, inplace=True)\n","cc1b43c0":"full_dataset.info(show_counts=True)","21c6db83":"# drop remaining columns\nfull_dataset.dropna(inplace=True)\nfull_dataset.info(show_counts=True)","21610ca7":"# use regex to clean up the descriptions as some of them have embedded HTML tags like <br>\nimport re\n# compile once only\nCLEANR = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});') \n\ndef cleanhtml(raw_html):\n    cleantext = re.sub(CLEANR, '', raw_html)\n    return cleantext","481d6fc9":"full_dataset[\"Description\"] = full_dataset.Description.apply(cleanhtml)\nfull_dataset.Description.sample(100)","4a5edd9e":"ratings = full_dataset[\"Rating\"]\nrating_count = full_dataset[\"RatingDistTotal\"]\nMR = ratings.convert_dtypes(np.float16).mean()\nprint(\"Mean Rating (MR) accross all books: \", MR)\n\nq = rating_count.quantile(0.98)\nprint(\"Minimum number of required ratings: \", q)\n","9cd3011b":"# our weighted rating algorithm\ndef weighted_rating(book):\n    v = book['RatingDistTotal']\n    R = book['Rating']\n    return (v\/(v+q) * R) + (q\/(q+v) * MR)","d86e35b8":"top_books = full_dataset[(full_dataset[\"RatingDistTotal\"] >= q) & (full_dataset[\"Rating\"] > MR)]\ntop_books[\"weighted\"] = top_books.apply(weighted_rating, axis=1)\nabove_average = top_books.sort_values(\"weighted\", ascending=False)\nabove_average.shape","cdbf3ab9":"# So clearly Harry Potter is a popular book. No surprise there.\nabove_average.head()","07e76aef":"# lets take a random sample\nabove_average.sample(10).sort_values(by=\"weighted\", ascending=False)","496975af":"from sklearn.metrics.pairwise import linear_kernel\nfrom sklearn.feature_extraction.text import TfidfVectorizer","6c37d73c":"# sample 20000 points so we don't hit kaggle's memory limit\n#samples = full_dataset(20000)\n\nsamples = full_dataset[10000:30000] # use if you want the same books to be predicted on","6f7991cd":"tf = TfidfVectorizer(analyzer='word',\n                     ngram_range=(1, 2),\n                     min_df=0,\n                     stop_words='english')\ntfidf_matrix = tf.fit_transform(samples['Description'])","db4f76d7":"cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)","78ff7bcb":"# write function to get recommendations based on cosine similarity\nsamples = samples.reset_index()\ntitles = samples[['Name', 'Authors', 'PublishYear', 'Publisher']]\nindices = pd.Series(samples.index, index=samples['Name'])","b7a895a7":"def get_content_recommendations(title):\n    try:\n        # handle case in which book by same title is in dataset\n        idx = indices[title][0]\n    except IndexError:\n        idx = indices[title]\n    sim_scores = list(enumerate(cosine_sim[idx]))\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n    sim_scores = sim_scores[1:15]\n    book_indices = [i[0] for i in sim_scores]\n    return titles.iloc[book_indices]","74ee3a8c":"index = 17136\ntitle = samples.iloc[index].Name\ndesc = samples.iloc[index].Description\nauthor = samples.iloc[index].Authors\nyear = samples.iloc[index].PublishYear\nprint(\"Title:\", title, \"\\nDescription:\", desc, \"\\nAuthor:\", author, \"\\nYear:\", year)","f14303b9":"get_content_recommendations(title)","abd3f409":"index = 18457\ntitle = samples.iloc[index].Name\ndesc = samples.iloc[index].Description\nauthor = samples.iloc[index].Authors\nyear = samples.iloc[index].PublishYear\nprint(\"Title:\", title, \"\\nDescription:\", desc, \"\\nAuthor:\", author, \"\\nYear:\", year)","c646c23c":"get_content_recommendations(title)","3d79dcba":"index = 10984\ntitle = samples.iloc[index].Name\ndesc = samples.iloc[index].Description\nauthor = samples.iloc[index].Authors\nyear = samples.iloc[index].PublishYear\nprint(\"Title:\", title, \"\\nDescription:\", desc, \"\\nAuthor:\", author, \"\\nYear:\", year)","69e6329b":"get_content_recommendations(title)","7b761d8a":"titles = samples[['Name', 'Authors', 'PublishYear', 'Publisher', \"Rating\", \"RatingDistTotal\"]]\n\ndef get_weighted_content_recommendations(title):\n    try:\n        # handle case in which book by same title is in dataset\n        idx = indices[title][0]\n    except IndexError:\n        idx = indices[title]\n    sim_scores = list(enumerate(cosine_sim[idx]))\n    \n    # get the top 40 recommendations\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n    sim_scores = sim_scores[1:40]\n\n    book_indices = [i[0] for i in sim_scores]\n    books = titles.iloc[book_indices]\n\n    # now sort by weighted rating\n    ratings = books[\"Rating\"]\n    rating_count = books[\"RatingDistTotal\"]\n    MR = ratings.convert_dtypes(np.float16).mean()\n    print(\"Mean Rating (MR) accross all books: \", MR)\n\n    # relax to 75% quantile\n    q = rating_count.quantile(.75)\n    print(\"Minimum number of required ratings: \", q)\n    \n    books[\"wRating\"] = books.apply(weighted_rating, axis=1)\n    return books.sort_values(by=\"wRating\", ascending=False)\n","c44f0602":"index = 17136\ntitle = samples.iloc[index].Name\ndesc = samples.iloc[index].Description\nauthor = samples.iloc[index].Authors\nyear = samples.iloc[index].PublishYear\nprint(\"Title:\", title, \"\\nDescription:\", desc, \"\\nAuthor:\", author, \"\\nYear:\", year)","113b9dd6":"get_weighted_content_recommendations(title)[:10]","f22d82cb":"index = 18457\ntitle = samples.iloc[index].Name\ndesc = samples.iloc[index].Description\nauthor = samples.iloc[index].Authors\nyear = samples.iloc[index].PublishYear\nprint(\"Title:\", title, \"\\nDescription:\", desc, \"\\nAuthor:\", author, \"\\nYear:\", year)","d0282eb2":"get_weighted_content_recommendations(title)[:10]","a5d2b130":"index = 10984\ntitle = samples.iloc[index].Name\ndesc = samples.iloc[index].Description\nauthor = samples.iloc[index].Authors\nyear = samples.iloc[index].PublishYear\nprint(\"Title:\", title, \"\\nDescription:\", desc, \"\\nAuthor:\", author, \"\\nYear:\", year)","bc7b8f9d":"get_weighted_content_recommendations(title)[:10]","23df2bca":"# comparison!\nprint(\"BASE CONTENT RECOMMENDATIONS\")\nprint(get_content_recommendations(title)[:10])\n\nprint(\"\\n\\nWEIGHTED CONTENT RECOMMENDATIONS\")\nget_weighted_content_recommendations(title) # see all top 40 for ranking comparison","6b596152":"# Content Based Recommendations\n\nThe goal here is to see if we can recommend books based on the similarity in the descriptions between books.\n","3edcf398":"## Simple Weighted Rating\n\nWeighted Rating (WR) = $(\\frac{v}{v + q} . R) + (\\frac{q}{v + q} . C)$\n\nwhere,\n* *v* is the number of votes for the book (total)\n* *q* is the minimum votes required to be listed in the chart\n* *R* is the average rating of all books\n* *C* is the mean vote across the entire dataset\n\ninspired by: https:\/\/www.kaggle.com\/rounakbanik\/movie-recommender-systems","a798e1d4":"### Weighting Metrics\n- ``MR`` = Mean rating = 3.75 (out of 5)\n- ``q`` = minimum number of ratings required","eecb80de":"# Hope you enjoyed!! \n\nPlease feel free to use but be sure to link to any notebooks I got inspiration from (that I linked to)!","6af7ac9c":"# Combined recommendation system (content + weighted rating)\n\nSo here were going to combine our content based recommendations with the weighted raiting scheme we cooked up earlier. ","7b65c4fc":"#### Cosine Similarity\n\nI will be using the Cosine Similarity to calculate a numeric quantity that denotes the similarity between two books. Mathematically, it is defined as follows:\n\n$cosine(x,y) = \\frac{x. y^\\intercal}{||x||.||y||} $\n\nSince we have used the TF-IDF Vectorizer, calculating the Dot Product will directly give us the Cosine Similarity Score. Therefore, we will use sklearn's **linear_kernel** instead of cosine_similarities since it is much faster.","bd803875":"# Conclusion\n\nSo above we can qualitatively see that our recommendations that include the weighted rating are better on average as they aren't suggesting books that have zero ratings (which are probably books that no-one has read).\n\nThere is definetly a biased towards books that are most frequently reviewed, but that can be adjusted in the weighted rating if novelty is important for the recommenation system.\n\nFuther work could be done to include user rating data directly on a per user basis, but I will leave that as an exercise for the user."}}