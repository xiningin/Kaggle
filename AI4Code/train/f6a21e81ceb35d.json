{"cell_type":{"30cc00b9":"code","679efbf2":"code","cd134fed":"code","2a47b8e2":"code","869ae137":"code","e6b83194":"code","eaf6ea8d":"code","013eb9f4":"code","f8071b99":"code","b4876a2a":"code","12dd1084":"code","9cd42f5b":"code","433508aa":"code","9226f360":"code","bfc9df84":"code","7853f2bf":"code","a52ceab9":"code","f2f457b1":"code","d91d22cd":"code","733c7879":"code","2b4f6807":"code","324a29be":"code","e10a259e":"code","ed9260a0":"code","196bf78c":"code","48a066a4":"code","e93cf0a4":"code","e7ce1392":"code","e420f549":"code","bf930a5c":"code","ffd82841":"code","abdee683":"code","cab4b114":"markdown","16b2c26c":"markdown","74fc13a0":"markdown","91e4916e":"markdown","37881c63":"markdown","cc2ebd4c":"markdown","9c3681ec":"markdown","bd506c8a":"markdown","f1b826b4":"markdown","f81c4faa":"markdown","3a13c2c8":"markdown","1d41e8e7":"markdown","f199769a":"markdown","baddae15":"markdown","ac138fa9":"markdown","af867ca1":"markdown","68f08186":"markdown","e10f2e38":"markdown","4e78257e":"markdown","550faaaf":"markdown","71d851ac":"markdown"},"source":{"30cc00b9":"import math\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nsns.set_theme(color_codes=True)\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import make_scorer, accuracy_score\n\nfrom sklearn.linear_model import LinearRegression\n\n# from cuml.ensemble import RandomForestRegressor as cuRFC\n# import cudf\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.neighbors import KNeighborsRegressor\n\nfrom sklearn.ensemble import AdaBoostRegressor\n\nfrom sklearn.linear_model import PoissonRegressor\n\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.linear_model import Ridge\n\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.linear_model import Lasso\n\nimport lightgbm as lgbm\nfrom lightgbm import LGBMRegressor\n\nimport xgboost\nfrom xgboost import XGBRegressor","679efbf2":"# df = cudf.read_csv(\"..\/input\/tabular-playground-series-jan-2021\/train.csv\", index_col = \"id\")\n# tdf = cudf.read_csv(\"..\/input\/tabular-playground-series-jan-2021\/test.csv\")\n\ndf = pd.read_csv(\n    \"..\/input\/tabular-playground-series-jan-2021\/train.csv\", index_col=\"id\"\n    )\ntdf = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2021\/test.csv\")\n\n# sns.regplot(x= df.drop(columns = [\"target\"]), y = df[\"target\"], data = df)\n# sns.lmplot(x= df.drop(columns = [\"target\"]), y = df[\"target\"], data = df)","cd134fed":"df = df.astype(\"float32\")\ntdf = tdf.astype(\"float32\")","2a47b8e2":"X = df.drop(columns=\"target\")\ny = df[\"target\"]","869ae137":"df.describe()","e6b83194":"sns.set_style(\"dark\")\nsns.set_color_codes(palette=\"deep\")\nf, ax = plt.subplots(figsize=(9, 8))\n\nsns.distplot(df[\"target\"], color=\"c\")\n\nax.xaxis.grid(False)\nax.set(ylabel=\"values\")\nax.set(xlabel=\"target\")\nplt.show()","eaf6ea8d":"df.corr().style.background_gradient(cmap='Blues')","013eb9f4":"Corelation = sns.heatmap(df.corr(), cmap=\"YlGnBu\")","f8071b99":"%%time\n\ncv = cross_validate(\n        estimator = LinearRegression(n_jobs = -1),\n        X = df.drop(columns = [\"target\"]),\n        y = df[\"target\"],\n        cv = 5,\n        scoring = [\"r2\",\"neg_mean_squared_error\"],\n        verbose = True,\n)\n","b4876a2a":"cv[\"test_neg_mean_squared_error\"].mean()","12dd1084":"%%time\n# After crossvalidation, we will try to fit our model\ntdf\nmodel = LinearRegression(n_jobs=-1)\n# when using GPU\n# model = RFC(verbose=True)\nmodel.fit(df.drop(columns=[\"target\"]), df[\"target\"])\n# predicting the model\npred = model.predict(tdf.drop(columns=[\"id\"]))","9cd42f5b":"ans = pd.DataFrame({\"id\": tdf[\"id\"], \"target\": pred})\nans[\"id\"] = ans[\"id\"].astype(int)\n# converting to submission file. Since we have set the id col, setting index = False\nans.to_csv(\"submission_LinearRegression.csv\", index=False)","433508aa":"# test_r2 is default variable of cv: getting mean of it\ncv[\"test_r2\"].mean()","9226f360":"X = df.drop(columns=[\"target\"])\ny = df['target'] \nkf = KFold(n_splits=5)\nkf.get_n_splits(X)\nprint(kf)","bfc9df84":"score = 0\nfor train_index, test_index in kf.split(X, df[\"target\"]):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    # train_index, test_index are integer indices based on the number of rows\n    # Thus we need iloc to access data\n    # iloc: Axes left out of the specification are assumed to be :,\n    # e.g. p.iloc['test_index'] is equivalent to p.iloc['test_index', :].\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    model = RidgeCV().fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n    score += mean_squared_error(y_test, y_pred)\n# mean of MSE =  0.5275229\nprint((score \/ kf.get_n_splits(X)))\n","7853f2bf":"model = RidgeCV()\nmodel.fit(df.drop(columns=\"target\"), df[\"target\"])\n# Fit the data and get the optimal value of alpha chosen\n# Here: 10\nmodel.alpha_\n","a52ceab9":"alphas = np.linspace(1, 100000, 100)\nridge = Ridge(max_iter=10000)\ncoefs = []\n\nfor a in alphas:\n    ridge.set_params(alpha=a)\n    ridge.fit(df.drop(columns=[\"target\"]), df[\"target\"])\n    coefs.append(ridge.coef_)\n\nax = plt.gca()\n\nax.plot(alphas, coefs)\nax.set_xscale(\"log\")\nplt.axis(\"tight\")\nplt.xlabel(\"alpha\")\nplt.legend(X.columns,bbox_to_anchor=(0.85, -0.25), fancybox=True, shadow=True, ncol=3)\nplt.ylabel(\"Standardized Coefficients\")\nplt.title(\"Ridge coefficients as a function of alpha\")","f2f457b1":"# Note: we use LassoCV which internally performs cross-validation to choose optimal value of tuning variable- alpha\ncv = cross_validate(\n    estimator=LassoCV(n_jobs=-1),\n    X=df.drop(columns=[\"target\"]),\n    y=df[\"target\"],\n    verbose=1,\n    return_train_score=True,\n    scoring=[\"r2\", \"neg_mean_squared_error\"],\n    cv=5,\n)","d91d22cd":"cv[\"test_neg_mean_squared_error\"].mean()","733c7879":"model = LassoCV(n_jobs=-1)\nmodel.fit(df.drop(columns=\"target\"), df[\"target\"])\n# Fit the data and get the optimal value of alpha chosen\nmodel.alpha_\n","2b4f6807":"alphas = np.linspace(2.7800706909230952e-05, 0.01, 100)\nlasso = Lasso(max_iter=10000)\ncoefs = []\n\nfor a in alphas:\n    lasso.set_params(alpha=a)\n    lasso.fit(df.drop(columns=[\"target\"]), df[\"target\"])\n    coefs.append(lasso.coef_)\n\nax = plt.gca()\n\nax.plot(alphas, coefs)\nax.set_xscale(\"log\")\nplt.axis(\"tight\")\nplt.xlabel(\"alpha\")\nplt.legend(X.columns,bbox_to_anchor=(0.85, -0.25), fancybox=True, shadow=True, ncol=3)\nplt.ylabel(\"Standardized Coefficients\")\nplt.title(\"Lasso coefficients as a function of alpha\")","324a29be":"cv = cross_validate(\n    estimator=PoissonRegressor(),\n    X=df.drop(columns=[\"target\"]).astype(\"float32\"),\n    y=df[\"target\"].astype(\"float32\"),\n    verbose=1,\n    return_train_score=True,\n    scoring=[\"r2\", \"neg_mean_squared_error\", \"neg_mean_poisson_deviance\"],\n    cv=3,\n)\n","e10a259e":"cv[\"test_neg_mean_squared_error\"].mean()","ed9260a0":"cv = cross_validate(\n    estimator=KNeighborsRegressor(n_neighbors=3, n_jobs=-1),\n    X=df.drop(columns=\"target\"),\n    y=df[\"target\"],\n    verbose=True,\n    cv=5,\n    scoring=[\"r2\", \"neg_mean_squared_error\"],\n    n_jobs=-1,\n)","196bf78c":"cv[\"test_neg_mean_squared_error\"].mean()","48a066a4":"X_train, X_test, y_train, y_test = train_test_split(\n    df.drop(columns=\"target\"), df[\"target\"], test_size=0.15\n)\n\nparam = {\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"regression\",\n    \"metric\": \"RMSE\",\n    \"learning_rate\": 0.0045,\n}\n\n\nmodel = LGBMRegressor(**param)\nmodel.fit(X_train, y_train)\n\n\nypred2 = model.predict(X_test)\n\n# rmse always takes in validation sets, eg. y test, x test predicted.\nprint(mean_squared_error(y_test, ypred2))\n","e93cf0a4":"# Now add this to train and test And you will get the score\nX = df.drop(columns=[\"target\"])\nkf = KFold(n_splits=5)\n\nfor train_index, test_index in kf.split(X, df[\"target\"]):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    param = {\n        \"boosting_type\": \"gbdt\",\n        \"objective\": \"regression\",\n        \"metric\": \"RMSE\",\n        \"learning_rate\": 0.0045,\n    }\n\n    model = LGBMRegressor(**param)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n\n    score = np.sqrt(mean_squared_error(y_test, y_pred))","e7ce1392":"cv = cross_validate(\n    estimator=XGBRegressor(),\n    X=df.drop(columns=\"target\"),\n    y=df[\"target\"],\n    scoring=[\"r2\", \"neg_mean_squared_error\"],\n    verbose=True,\n    cv=5,\n    n_jobs=-1,\n)\n","e420f549":"cv[\"test_neg_mean_squared_error\"].mean()","bf930a5c":"%%time \n\ncv = cross_validate(\n    estimator=RandomForestRegressor(n_jobs=-1, verbose=True),\n    #     estimator=cuRFC(verbose=True),\n    X=df.drop(columns=[\"target\"]),\n    y=df[\"target\"],\n    cv=5,\n    scoring=[\"r2\", \"neg_mean_squared_error\"],\n    verbose=True,\n)","ffd82841":"cv[\"test_neg_mean_squared_error\"].mean()","abdee683":"# We are using the OOB(~ validation score) score to compare the training and test error\n# model = RandomForestRegressor(n_jobs=-1, verbose=True, oob_score = True)\n# model.fit(X, y)\n# # Training error\n# model.score(X,y)\n# # oob error\n# model.oob_score_\n# * We see the training score = 0.87 while the test score = 0.05 \n# * From the scores, we can say the our random forest is overfitting the training dataset \n# * Note that the CV and oob score are almost similar","cab4b114":"This notebook contains the application of the following regression models:\n1. Linear Regression \n2. Ridge Regression \n3. Lasso Regression\n4. Poisson Regression \n5. K-Neighbor Regressor\n6. LGBM\n7. XGB\n8. Random Forest","16b2c26c":"* We can observe that cont variables 1,6,7,8,9,10,11,12,13 are the most inter-correlated\n* Note: the corelation between the parameters and the target variable take maximum absolute value of 0.067.\n* Since this value is close to 0, the linear regression model will not be a good fit.","74fc13a0":"The following are some ways in which the simple linear model can be improved,\nby replacing plain least squares fitting with some alternative fitting procedures.\n# Ridge\n* We perform regularisation on our linear regression model. \n* Regularisation will help reducing the coefficients. The parameters that have more role in determining the target\/ result value will have less shinking coefficients as the value of alpha increases.\n* It shrinks the parameters, therefore it is mostly used to prevent multicollinearity.\n* Uses L2 regularization technique.","91e4916e":"# Negetive mean square errors\n1. Linear Regression --> -0.5274229884147644\n2. Ridge Regression --> (MSE) 0.5274229003487053\n3. Lasso Regression --> -0.5274227619171142\n4. Poisson Regression --> -0.5332794126312232\n5. K-Neighbor Regressor --> -0.656536448001861\n6. LGBM --> (MSE) 0.5258897237509819\n7. XGB --> -0.49416557550430296\n8. Random Forest --> -0.5009868281839414","37881c63":"Sinc the R2 values are very less, it is pretty evident that the Linear Regression is not a suitable model to explain the variance of our data. \n* Underfitting","cc2ebd4c":"## Light Gradient Boosting Model With k-fold","9c3681ec":"* To better understand the variation of the coefficients wth change in the tuning variable, we will plot the change in coefficients with respect to change in alpha.","bd506c8a":"* To understand the statistical meaning and data distribution of our data, pandas gives a feature: describe()\n* Following are the attributes provided:\n1. count (total number of values)\n2. mean (mean of the data)\n3. std (standard deviation)\n4. min, max (minimum and maximum value in the data)\n5. 25%, 50%, 75% (Respective quartile values)\n","f1b826b4":"Observing the corelation of the predictors","f81c4faa":"# Bagging","3a13c2c8":"# XGBoost\n* XGBoost is short for \u201ceXtreme Gradient Boosting.\u201d \n* The \u201ceXtreme\u201d refers to speed enhancements such as parallel computing and cache awareness that makes XGBoost approximately 10 times faster than traditional Gradient Boosting.\n* XGBoost is regularized, so default models often don\u2019t overfit\n* It has extensive hyperparameters for fine-tuning\n","1d41e8e7":"* We can observe that, as we increase the value of alpha, the magnitude of the coefficients decreases, where the values reaches to zero but not absolute zero","f199769a":"\n# Regression\n* The following are the mathematical models that will help perdict a continuous outcome (result) based on one or more input(s) (predictor variables).\n## Linear Regression\n* Simple approach for [supervised learning](https:\/\/en.wikipedia.org\/wiki\/Supervised_learning)\n* We assume that the _true_ relationship between  X nd Y takes form Y = f(x) + \u03f5 (f is an unknown function, \u03f5 is a mean-zero random error term)\n* Y = \u03b20 + \u03b21X + \u03f5\n    * \u03b20 - intercept term (, the expected value of Y when X = 0)\n    * \u03b21 - slope (the average increase in Y associated with a one-unit increase in X)\n    * \u03f5 -catch-all for what we miss with this simple model: the true relationship is probably not linear, there may be other variables that cause variation in Y , and there may be measurement error\n* Analysing each individual variable\n* For estimating coefficients\n    * We choose _least squares method_ to choose the coefficients such that we minimise RSS (Residual Sum of Squares)\n* To predict the confidence interval: RSE (Residual Standard Error)\n\n","baddae15":"* Bagging is short for \u201cbootstrap aggregation,\u201d meaning that samples are chosen with replacement (bootstrapping), and combined (aggregated)\n* Decision trees leave you with a difficult decision. A deep tree with lots of leaves will overfit because each prediction is coming from historical data from only the few houses at its leaf. But a shallow tree with few leaves will perform poorly because it fails to capture as many distinctions in the raw data.\n# Random Forest\n* The random forest uses many trees, and it makes a prediction by averaging the predictions of each component tree via bagging. \n* It generally has much better predictive accuracy than a single decision tree and it works well with default parameters. \n* This model generates decorelated trees by choosing a fresh sample of m predictors at each split (m \u2248 \u221ap)\n","ac138fa9":"# Light Gradient Boosting Model","af867ca1":"### Using [Cross validation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_validate.html) for model selection.","68f08186":"# Poisson Regression\n* Poisson regression assumes the response variable Y has a Poisson distribution\n* It assumes the logarithm of its expected value can be modeled by a linear combination of unknown parameters. ","e10f2e38":"* Using K-fold approach to implement RidgeCV model\n* RidgeCV will internally apply Cross-validation to choose the optimal value of tuning variable alpha","4e78257e":"# Boosting\n* Boosting is a sequential process, where each subsequent model attempts to correct the errors of the previous model. The succeeding models are dependent on the previous model.\n* When an output is mispredicted by a hypothesis, its weight is increased so that next hypothesis is more likely to classify it correctly. By combining the whole set at the end converts weak learners into better performing model.\n* The final model is the weighted mean of all the models (weak learners).\n","550faaaf":"# K-Neighbors Regressor","71d851ac":"# Lasso\n* Lasso is similar to ridge regression, however here the coefficients can actually take value = 0\n* Uses l1 regularisation technique\n* Used for feature selection"}}