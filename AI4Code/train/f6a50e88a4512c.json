{"cell_type":{"ab2019a1":"code","30370e98":"code","c458ff95":"code","fbf2721d":"code","ef353a1a":"code","3470e309":"code","ff5f2419":"code","afe004a7":"code","c96e60c4":"code","6c4a9ee2":"code","1c2e80ec":"code","d5cc9e84":"code","c05437c4":"code","67661b5f":"code","f4bef208":"code","939e301b":"code","c1031e5d":"code","9d46598c":"code","41977eac":"code","627b7be9":"code","517dba29":"code","26ff1205":"code","45fe89eb":"code","23ea5dea":"code","2d440e17":"code","587f4329":"code","8c1b3518":"code","5ec89b37":"code","08d5312c":"code","2c0c2b68":"code","122d3528":"code","5e23de42":"code","45bb305a":"code","fdbcd13f":"markdown","687b6b47":"markdown","8533b63c":"markdown","b1fb86cb":"markdown","b638d101":"markdown","72ca9385":"markdown","9781c9e2":"markdown","db11d29a":"markdown","22bf6432":"markdown","784ca73b":"markdown","aff7e39e":"markdown","f6282b5f":"markdown","e189f13c":"markdown","33d12574":"markdown","f96191b3":"markdown","d1ef26df":"markdown","cc58e306":"markdown","117e55a2":"markdown","4e7667e0":"markdown","145a5d77":"markdown","208220b5":"markdown","28de85d8":"markdown","d6277899":"markdown","3581f5d9":"markdown","bf5cb48a":"markdown","d40ddcba":"markdown","e7cac80a":"markdown","2c617229":"markdown","aebcc188":"markdown","2d2dd5f9":"markdown","89fc6261":"markdown","7350ee5e":"markdown","2dd55245":"markdown","3bbb3425":"markdown","96647eff":"markdown","4e9021b5":"markdown","19d3a31a":"markdown","34020301":"markdown","99092717":"markdown","3f8dd05a":"markdown","9ede48bd":"markdown","0108fa19":"markdown","08d36117":"markdown"},"source":{"ab2019a1":"%matplotlib inline\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom itertools import count\nfrom IPython.display import display, Image\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)","30370e98":"dset = pd.concat([pd.read_csv('..\/input\/train.csv'), pd.read_csv('..\/input\/test.csv')], sort=False)\ndset.reset_index(inplace=True, drop=True)\ndisplay(dset.head())\nprint('Number of instances:\\n')\nprint('\\tTrain: {}'.format(len(dset[np.isfinite(dset['Survived'])])))\nprint('\\tTest: {}'.format(len(dset[dset['Survived'].isnull()])))","c458ff95":"temp = dset.copy()\ntemp['Sex'] = np.where(temp['Age'] <= 14.5, 'Child', np.where(temp['Sex'] == 'female', 'Woman', 'Man'))\ntemp['Pclass'] = temp['Pclass'].map({1: 'First', 2: 'Second', 3: 'Third'})\n\ndf_survival_rate = pd.DataFrame({'Category': ['Man', 'Woman', 'Child'] * 3,\n                                 'Class': ['First'] * 3 + ['Second'] * 3 + ['Third'] * 3,\n                                 'Rate': [0] * 9})\n\nfor i in range(len(df_survival_rate)):\n    cat, pclass = df_survival_rate.loc[i, ['Category', 'Class']]\n    \n    sub_df = temp[(temp['Sex'] == cat) & (temp['Pclass'] == pclass)]\n    sub_df_alive = sub_df[sub_df['Survived'] == 1]\n    \n    df_survival_rate.loc[i, 'Rate'] = len(sub_df_alive) \/ len(sub_df)\n\nf = sns.factorplot('Category', 'Rate', col='Class', data=df_survival_rate, saturation=.9, kind='bar')\n_ = f.set_axis_labels('', 'Survival Rate')\n\ndel temp","fbf2721d":"dset.drop('PassengerId', axis=1, inplace=True)","ef353a1a":"def count_nan():\n    \n    df = []\n\n    for col in dset:\n        nan = dset[dset[col].isnull()]\n        if len(nan):\n            perc = round(len(nan) \/ len(dset), 3) * 100\n            df.append((col, len(nan), perc))\n    \n    df.sort(key=lambda x: x[2], reverse=True)\n    df = pd.DataFrame(df, index=[el[0] for el in df], columns=[0, 'Missing', '%']).drop(0, axis=1)\n    \n    display(df)\n\ncount_nan()","3470e309":"dset.drop('Cabin', axis=1, inplace=True)\ndset.loc[dset['Fare'].isnull(), 'Fare'] = dset['Fare'].mean()\ndset.loc[dset['Embarked'].isnull(), 'Embarked'] = dset['Embarked'].value_counts().index[0]\n\ncount_nan()","ff5f2419":"dset = pd.concat([dset, dset['Name'].str.extract(r'(?P<Surname>\\w+[-]?\\w+),\\s(?P<Title>\\w+)')],\n                 axis=1).drop('Name', axis=1)    \n\ndset.head()","afe004a7":"info = dset.groupby('Title').describe()['Age']['mean'].sort_values(ascending=False)\ndisplay(info)\ndset.loc[dset['Age'].isnull(), 'Age'] = dset.loc[dset['Age'].isnull(), 'Title'].map(info)\n\ncount_nan()","c96e60c4":"def create_corr_mat(df, th):\n    \n    df2 = df.copy()\n\n    corr_mat = df2.corr().nlargest(500, 'Survived')\n    corr_mat = corr_mat[corr_mat.index]\n\n    corr_mat = corr_mat[abs(corr_mat['Survived']) > th]\n    corr_mat = corr_mat[corr_mat.index]\n\n    return corr_mat\n\ndset['Sex'] = np.where(dset['Sex'] == 'female', 1, 0)\ncreate_corr_mat(dset, 0)","6c4a9ee2":"dset['Pclass'] = dset['Pclass'].map({1: 'First', 2: 'Second', 3: 'Third'})","1c2e80ec":"child_max = dset.groupby('Title').describe().loc['Master']['Age', 'max']\ndset['Age'] = np.where(dset['Age'] <= child_max, 1, 0)\n\ndset.head()","d5cc9e84":"dset['Age'].groupby(dset['Title']).mean()","c05437c4":"dset.loc[(dset['Sex'] == 0) & (dset['Age'] == 1), 'Title'] = 'Master'\ndset.loc[(dset['Sex'] == 1) & (dset['Age'] == 1), 'Title'] = 'Miss'\n\ndset['Age'].groupby(dset['Title']).mean()","67661b5f":"dset['FamSize'] = dset['SibSp'] + dset['Parch'] + 1\ndset.drop(['SibSp', 'Parch'], axis=1, inplace=True)\n\ncreate_corr_mat(dset, 0)","f4bef208":"c = count(1)\n\ndset['FamCode'] = 0\nfamilies = dset.groupby(['Ticket', 'Surname'])\nfor i, f in families:\n    dset.loc[f.index, 'FamCode'] = next(c)\n\ndset.head()","939e301b":"counter = 0\nfor i, f in dset[dset['FamSize'] > 1].groupby('FamCode'):\n    if len(f) == 1 and counter < 3:\n        display(f)\n        counter += 1\n    elif len(f) == 1 and counter >= 3:\n        counter += 1\n        \nprint('There are {} instances that need to be corrected.'.format(counter))","c1031e5d":"dset[dset['Surname'] == 'Crosby']","9d46598c":"dset.loc[1196, 'FamCode'] = 1024\ndset[dset['Surname'] == 'Crosby']","41977eac":"dset.loc[356, 'FamCode'] = 46\ndset[dset['Ticket'] == '113505']","627b7be9":"families = {'0': 484, '68': 509, '104': 382, '113': 731, '136': 83, '145': 825, '175': 615, '192': 607, '267': 486,\n            '352': 317, '356': 46, '371': 378, '392': 382, '417': 268, '442': 753, '451': 738, '496': 682, '529': 369,\n            '532': 317, '539': 119, '556': 871, '593': 648, '627': 113, '689': 229, '704': 596, '765': 113,\n            '880': 182, '892': 369, '909': 1008, '912': 733, '925': 112, '968': 91, '1012': 664, '1024': 279,\n            '1041': 90, '1075': 927, '1078': 772, '1111': 959, '1129': 266, '1196': 1024, '1247': 91, '1261': 350,\n            '1267': 413, '1295': 908}\n\nfor i in families:\n    dset.loc[int(i), 'FamCode'] = families[i]","517dba29":"for i, f in dset.groupby('FamCode'):\n    dset.loc[f.index, 'FamSize'] = len(f)","26ff1205":"_ = sns.barplot(dset['FamSize'], dset['Survived'])","45fe89eb":"dset['FamSize'] = np.where(dset['FamSize'] == 1, 'None', np.where(dset['FamSize'] <= 4, 'Small', 'Big'))","23ea5dea":"temp = pd.get_dummies(dset, columns=['FamSize'])\ndisplay(create_corr_mat(temp, 0))\n\ndel temp","2d440e17":"dset['FamAlive'] = 0\nfamilies = dset[dset['FamSize'] != 'None'].groupby(['FamCode'])\n\nfor i, f in families:\n\n    fam_al = f[f['Survived'] == 1]\n    dset.loc[f.index, 'FamAlive'] = len(fam_al) \/ len(f)\n    \n    \ndisplay(create_corr_mat(dset, 0))","587f4329":"dset.head()","8c1b3518":"dset.drop(['Ticket', 'Surname', 'FamCode'], axis=1, inplace=True)","5ec89b37":"final_df = pd.get_dummies(dset)\ncorr_mat = create_corr_mat(final_df, 0.1)\ncorr_mat","08d5312c":"final_df = final_df[corr_mat.index]\nprint('Number of features used: {}'.format(len(final_df.columns)))","2c0c2b68":"def get_sets(df):\n\n    xtrain = df[np.isfinite(df['Survived'])].copy()\n    ytrain = xtrain['Survived'].copy()\n    xtrain.drop('Survived', axis=1, inplace=True)\n    xtest = df[df['Survived'].isnull()].copy()\n    xtest.drop('Survived', axis=1, inplace=True)\n    xtest.reset_index(inplace=True, drop=True)\n    \n    cols = xtrain.columns\n    scaler = MinMaxScaler()\n    scaler.fit(xtrain)\n    xtrain = pd.DataFrame(scaler.transform(xtrain), columns=cols)\n    xtest = pd.DataFrame(scaler.transform(xtest), columns=cols)\n    \n    return xtrain, ytrain, xtest\n\n\nX_train, y_train, X_test = get_sets(final_df)\nprint('Train set:')\ndisplay(X_train.head())\nprint('Test set:')\ndisplay(X_test.head())","122d3528":"def run_grid_search():\n\n    lr_values = {\n                 'C': [i for i in np.arange(0.1, 1, 0.1)],\n                 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag'],\n                 }\n\n    lr = LogisticRegression(max_iter=1000)\n    gs = GridSearchCV(lr, param_grid=lr_values, scoring='accuracy', return_train_score=True).fit(X_train, y_train)\n    print(gs.best_params_)\n    print('Mean train score: {}'.format(round(gs.cv_results_['mean_train_score'].mean(), 3)))\n    print('Mean test score: {}'.format(round(gs.cv_results_['mean_test_score'].mean(), 3)))\n    \n    return gs.best_estimator_\n\n\nbest_model = run_grid_search()\npreds = best_model.predict(X_test)\n\n# To save the .csv file for submission\n\n# csv_to_submit = np.column_stack((range(892, 1310), preds))\n# np.savetxt(path_where_you_want_to_save_it, csv_to_submit, delimiter=',', header='PassengerId,Survived',\n#            fmt='%i,%i', comments='')","5e23de42":"final_df = pd.get_dummies(dset)\ncorr_mat = create_corr_mat(final_df, 0.2)\ndisplay(corr_mat)\nfinal_df = final_df[corr_mat.index]\nprint('Number of features used: {}'.format(len(final_df.columns)))\nX_train, y_train, X_test = get_sets(final_df)\n\nbest_model = run_grid_search()\npreds = best_model.predict(X_test)","45bb305a":"dset['Category'] = np.where(dset['Age'] == 1, 'Child', np.where(dset['Sex'] == 1, 'Woman', 'Man'))\nfinal_df = pd.get_dummies(dset)\ncorr_mat = create_corr_mat(final_df, 0.2)\ndisplay(corr_mat)\nfinal_df = final_df[corr_mat.index]\nprint('Number of features used: {}\\n'.format(len(final_df.columns)))\nX_train, y_train, X_test = get_sets(final_df)\n\nbest_model = run_grid_search()\npreds = best_model.predict(X_test)","fdbcd13f":"Perfect, one less problem to solve. Short note: sometimes it is very hard to follow the actual relation between passengers. I did my best to do it correctly but don't be surprised in case you find any mistake.\n\nDuring the whole process of looking for the right family for each passenger I also saw that in some cases <b><i>SibSp<\/i><\/b> or <b><i>Parch<\/i><\/b> or both contain wrong values, which causes wrong <b><i>FamSize<\/i><\/b> values as a consequence.\n\nTo assign the correct value to <b><i>FamSize<\/i><\/b> run the code below","687b6b47":"<b><i>PassengerId<\/i><\/b> is just a number assigned to each passenger with no criteria so we can safely drop it.","8533b63c":"Focusing only on the target feature (<b><i>Survived<\/i><\/b>), there are few aspects that are worth to be discussed:\n\n-  It has a strong POSITIVE linear correlation with the feature <b><i>Sex<\/i><\/b>. Not surprising at all: women survive more than men.\n\n\n-  It has a POSITIVE linear correlation with the feature <b><i>Fare<\/i><\/b>. It means that the higher the Fare the higher the chances of survival. This makes sense because, as a general rule, the most expensive tickets are bought by rich people who belong to the 1st\/2nd class.\n\n\n-  It has a NEGATIVE linear correlation with the feature <b><i>Pclass<\/i><\/b>. Remember: here classes are represented as integers (1, 2, 3) and the PCC being negative is simply telling us that the higher the class (the integer, not from a social point of view) the lower the chances of survival, which of course makes sense and is consistent with the \"survival rate vs class\" plot I have shown at the beginning. But we can go ahead in the analysis: that plot also shows us that there is NOT obvious difference between 1st and 2nd class passengers. Actually women and children from 2nd class seem to have comparable or even higher survival rate than women and children in 1st class. I thought that this aspect might be misleading for the algorithm and decided to transform <b><i>Pclass<\/i><\/b> into a categorical feature, as we will see later.\n\n\n-  It does NOT show any evident linear correlation with <b><i>Parch<\/i><\/b>, <b><i>SibSp<\/i><\/b> and <b><i>Age<\/i><\/b>. Regarding the first two, as many of you have already said, it is often good practice to combine feature that individually have PCC close to 0 and try to create a new feature with higher PCC. We will do this by creating the feature called \"FamSize\".\n\n\n-  The apparently random relation betweeen <b><i>Survived<\/i><\/b> and <b><i>Age<\/i><\/b> might look weird but it actually makes more sense after a deeper analysis. Thinking carefully about the problem, during those confused moments right after hitting the iceberg nobody was really caring about the actual age of the passengers. The only important thing was whether the passenger was a child or not. For the rest, passengers of age 20, 30, 50 or whatever they were all treated in the same way (we are not interested here in passenger's sex or class). This is the reason why the PCC is failing at this specific task. In this new scenario, we need to modify the feature <b><i>Age<\/i><\/b> and make it binary. That's all we are interested in: child or adult.","b1fb86cb":"Cool! Now we know which passenger belongs to which family.\n\n<b>WRONG<\/b>! There is a problem, as always. Let's have a closer look at the code below and see what it's doing. The goal is to find errors in the assignment of the <b><i>FamCode<\/i><\/b> we just did. To do that we are basically saying:\n\n1. Consider only the passengers with AT LEAST 1 more person in the family and group them by <b><i>FamCode<\/i><\/b>. This means considering all those passengers who have <b><i>FamSize<\/i><\/b> > 1.\n\n2. Good! Now take the family and check its length. If the <b><i>FamCode<\/i><\/b> assignment is done the right way there MUST be NO family with length equal to 1 because don't forget point 1 in this list: we want only the passengers with <b><i>FamSize<\/i><\/b> > 1.\n\n3. Now my dear little code, if you find (and you shouldn't) any family with just one person please show it to me so I can check what is wrong with it.\n\n4. To avoid very long outputs just show me the first 3 errors, for the rest just keep counting them\n\n5. Thanks!","b638d101":"Now that the complete dataset is free from missing values it is time to have a first look at the correlation between our target (<b><i>Survived<\/i><\/b>) and the rest of the features. To do that we check the correlation matrix which, in our case, is a matrix containing the [Pearson Correlation Coefficient (PCC)](https:\/\/en.wikipedia.org\/wiki\/Pearson_correlation_coefficient) between all the features involved. In short, PCC has value between -1 and 1 and gives an indication on how much 2 variables are linearly correlated. The higher the value (absolute value) the stronger the linear correlation (positive or negative) between the variables. Value of 0 or in general close to 0 means there is no or very low correlation.\n\nI wrote a small function to display a correlation matrix containing all the features whose PCC with the target variable is higher than a threshold. At this step we set this threshold to be 0 in order to show all the features, later we are going to use different values. It is important to stress the point that when we talk about PCC value we mean ABSOLUTE VALUE.\n\nThe only thing we do before creating the correlation matrix is to transform <b><i>Sex<\/i><\/b> into a numerical feature, assigning the value of 1 for female and 0 for male.","72ca9385":"Remember that <b><i>FamSize<\/i><\/b>'s PCC was even lower than <b><i>SibSp<\/i><\/b>'s and <b><i>Parch<\/i><\/b>'s?\n\nCheck it now","9781c9e2":"It was a pretty easy guess but now it is confirmed: women and children had an advantage over men as well as 1st and 2nd class passengers had over the ones belonging to 3rd.","db11d29a":"-  <b><i>Ticket<\/i><\/b>: not needed, we used already its infomation.\n\n\n-  <b><i>Embarked<\/i><\/b>: I would say it does not matter where passengers boarded Titanic but we'll keep it to be sure.\n\n\n-  <b><i>Surname<\/i><\/b>: not needed, it was useful to extract family informations.\n\n\n-  <b><i>Title<\/i><\/b>: this might be useful.\n\n\n-  <b><i>FamSize<\/i><\/b>: we already saw it's an important feature.","22bf6432":"As we can see there are 5 columns containing at least 1 missing value, let's make some considerations:\n\n-  <b><i>Cabin<\/i><\/b>: by intuition I would probably think that this feature might be important for the prediction task. For example, it is not absurd to think that the lifeboats area was easier to reach from some cabins compared to other ones due to the mess going on right after the collision. Problem is that 77.5 % of missing values is too much to try to fill. It is safer to just drop the entire column.\n<br><br>\n-  <b><i>Survived<\/i><\/b>: obviously we do not care about this one, those NaN are coming from the test set that we joined previously.\n<br><br>\n-  <b><i>Age<\/i><\/b>: 20.1 % is still a considerable amount of missing values but this feature as we saw before is too important to be dropped, so we will see later how to fill it.\n<br><br>\n-  <b><i>Embarked<\/i><\/b>: just 2 values here, we can fill it with the most common entry.\n<br><br>\n-  <b><i>Fare<\/i><\/b>: the only missing value will be filled by the mean value of the column.","784ca73b":"Now we can fill each NaN in <b><i>Age<\/i><\/b> with the mean value of the corresponding Title.","aff7e39e":"If you keep analyzing all the 70 mistakes you will see that they are just more examples of the same situations: either they have different ticket for some reason or differnt surname.\n\nBy googling the passengers involved you would be able to keep track of their actual families and correct the mistakes one by one.<br><\/br>Good news: I did that for you.\n\nRun the code below to reassign the right <b><i>FamCode<\/i><\/b> to the right passenger","f6282b5f":"Now it's time to analyze the missing values and try to fill them properly.","e189f13c":"If you submit at this stage you will score 0.80861. Well it looks like before we were a little bit overfitting the dataset. Now the <i>Mean train score<\/i> is slighltly lower than before but the submission score increased from 80.4% to 80.9%.\n\nBut we are not done yet. It is true that increasing the threshold to 0.2 allowed us to reach higher precision but it is also true that not we are considering the feature <b><i>Age<\/i><\/b> anymore. Following the binarization we did earlier, we lost all the information about a passenger being an adult or a child. It is a big thing we are leaving out and we don't want that.\n\nTo solve the problem, let's create a new feature <b><i>Category<\/i><\/b>. This feature classifies the passengers as one of the following three categories: <i>Woman<\/i>, <i>Man<\/i> or <i>Child<\/i>.\n\nIn this way we are bringing back the information about the age, hoping that now it will have a PCC higher than 0.2.","33d12574":"### Let's start!\n\nFirst thing to do, as always, is importing all the libraries we are going to use.","f96191b3":"Now we can finally separate train and test set.","d1ef26df":"And it worked! Now <i>Category_Woman<\/i> and <i>Category_Man<\/i> have high PCC (> 0.5) and we are able to increase the score by almost 1%. Score with this last prediction is 0.81818.","cc58e306":"At this point we reduce the final df to have only those features we see above and print their number, for the sake of information.","117e55a2":"And what about <i>Mrs Chibnall<\/i> and <i>Miss Bowerman<\/i>? Well in this case they are mother and daughter with same ticket but different surname.\n\nLet's fix this one too","4e7667e0":"### Introduction\n\nThis is my first Data Science challenge and although there are many great notebooks that allow you to reach very high scores (83% or higher) I tried to develop my own workflow, picking advices from the discussion forum and implementing some meaningful steps to reach a decent score.\n\nAmong all the notebooks that I read, I found very useful:\n\n-  [Zlatan Kremonic - Titanic Random Forest: 82.78%](https:\/\/www.kaggle.com\/zlatankr\/titanic-random-forest-82-78\/notebook)\n\n-  [Pedro Marcelino - Data analysis and feature extraction with Python](https:\/\/www.kaggle.com\/pmarcelino\/data-analysis-and-feature-extraction-with-python\/notebook)\n\nThey both give a nice and smooth explanation especially about the exploratory analysis which is easy to follow even for a beginner like I am. Keep in mind that these are just 2 of the hundreds you can find in the forum and most of them are equally valuable. Comments and answers to notebooks are another rich source of knowledge that it is worth to spend some time on ([Chris Deotte](https:\/\/www.kaggle.com\/cdeotte) is one of the most active in this sense).\n\nWith this notebook you will reach 0.81818 which is Top 5% at the time of writing by using simple Logistic Regression and little feature engineering. One of the key steps that allowed me to make a jump in the score was a correction in the dataset that I made when I grouped all the passengers by family, as you will see later down in the notebook.\n\nOf course any comment or suggestion is more than welcome, thank you!","145a5d77":"In order to have a first contact with the data, let's plot the survival rate of the three categories (women, children, men) divided by class. To do that, we temporarily create a copy of the data and modify its <b><i>Sex<\/i><\/b> column. We choose 14.5 as the maximum age classifying a child, later we will see why.","208220b5":"And there you go...70 mistakes. Not nice.\n\nWhat's up with <i>Mrs Crosby<\/i>? Her family has 3 members but she's the only one with <b><i>FamCode<\/i><\/b> = 30? Where is the trick?\n\nLet's quickly check one thing","28de85d8":"As expected, female and male titles have high and low PCC, respectively. Differently from what I was expecting, <b><i>Embarked<\/i><\/b> actually shows some correlation with the target.","d6277899":"As expected, there are some errors. In this series, a value of 0 means no child for that specific Title while a value of 1 means that all the instances of that Title are children.\n\nLooking carefully we see that almost all the categories are correct because their values are all 0, they don't contain children instances and they don't have to. <i>Master<\/i> and <i>Miss<\/i> are also correct because as we said before:\n\n-  100% of <i>Master<\/i> instances must be child...and they are\n\n\n-  a certain amount (but not 100%) of <i>Miss<\/i> instances must be child...and they are\n\n\nThe problem appears with <i>Mr<\/i> and <i>Mrs<\/i> which should both be 0 (they must have no children) but are instead > 0.\n\nThis means that there is some young boy wrongly assigned as <i>Mr<\/i> and some young girl wrongly assigned as <i>Mrs<\/i>.\n\nCorrection is easy to do","3581f5d9":"Now the feature starts to gain importance, especially the categories <i>Small<\/i> and <i>None<\/i>. That should help the algorithm in making better predictions.","bf5cb48a":"Now we can finally create our final dataframe which will be used for the predictions.\n\nTo do that we need to choose which categorical features we want to use.","d40ddcba":"Second step: make <b><i>Age<\/i><\/b> binary. We need to choose an age to separate children from adults. To do that, we take the maximum age of all the children in the dataset (by considering the title <i>Master<\/i>) and use that as threshold: all passengers younger than that will be children (Age = 1), the others will be adults (Age = 0). Maximum age for <i>Master<\/i> turns out to be 14.5.","e7cac80a":"Let's start by dropping <b><i>Cabin<\/i><\/b> and filling <b><i>Embarked<\/i><\/b> and <b><i>Fare<\/i><\/b>.","2c617229":"Next, we join train and test dataframes in order to do all the processing (filling, cleaning, engineering) all at once. We will then separate them again at the end of the process by using the <b><i>Survived<\/i><\/b> column.","aebcc188":"Now it's time to select the features (between all the dummies) that we actually want to use. This filter operation is done by selecting a threshold in the <i>create_corr_mat<\/i> function we created previously. For the moment we set it to be 0.1.","2d2dd5f9":"At this point we are ready to check the survival rate as a function of <b><i>FamSize<\/i><\/b>","89fc6261":"To handle the <b><i>Age<\/i><\/b> problem, we first extract 2 new features from <b><i>Name<\/i><\/b>: <b><i>Surname<\/i><\/b> and <b><i>Title<\/i><\/b>.","7350ee5e":"First step: make <b><i>Pclass<\/i><\/b> categorical. Solution is easy: map each integer with the corrisponding ordinal value.","2dd55245":"Binarization of <b><i>Age<\/i><\/b> allows us to check for potential errors in the feature <b><i>Title<\/i><\/b>.\n\nIn fact, there are only 2 categories which can have <b><i>Age<\/i><\/b> = 1:\n\n-  <i>Master<\/i>: 100% of them must be child\n\n-  <i>Miss<\/i>: a certain amount of them must be child but not 100% because unmarried adult women also fall inside this category.\n\n\nTo check whether our dataset is correct","3bbb3425":"So the new feature has a strong correlation with the target...good, that's what we wanted.","96647eff":"Among numerical features, <b><i>FamCode<\/i><\/b> can also be removed because it's just an identifier assigned by us.","4e9021b5":"Oh, OK now I see the trick: for some reason <i>Mrs Crosby<\/i> bought a different ticket from the one her husband and daughter did, that's why I missed her before.\n\nJust make this simple correction and we are good to go","19d3a31a":"At this stage we are already able to reach more than 80% accuracy (0.80382), which is a good result. Looking at the difference between the scores from GridSearch and the one from submission, I thought that maybe we are using too many features and this might cause overfitting.\n\nSo the next step consists in increasing the PCC threshold from 0.1 to 0.2. In this way we will only include those features whose PCC with the target is AT LEAST equal to 0.2.\n\nComparing with the previous correlation matrix we are basically excluding:\n\n-  <b><i>Embarked_C<\/i><\/b>\n\n\n-  <b><i>Age<\/i><\/b>\n\n\n-  <b><i>FamSize_Big<\/i><\/b>\n\n\n-  <b><i>Embarked_S<\/i><\/b>","34020301":"Good, now all the children are where they have to be.\n\n\nNext we create the <b><i>FamSize<\/i><\/b> feature and drop <b><i>SibSp<\/i><\/b> and <b><i>Parch<\/i><\/b>.","99092717":"Next let's run a grid search for Logistic Regression and use the best model to make the predictions","3f8dd05a":"Well, we actually see a little trend here.\n\nPeople with no family (<b><i>FamSize<\/i><\/b> = 1) have less chances to survive than the others, at least until <b><i>FamSize<\/i><\/b> = 4.\n\nFor <b><i>FamSize<\/i><\/b> > 4 the survival rate drops drastically (a part from <b><i>FamSize<\/i><\/b> = 7) and the uncertainty intervals are very large for some reason. Probably most of the big families come from the 3rd class and this cause the rate do be very low.\n\nBased on this analysis, let's divide the feature <b><i>FamSize<\/i><\/b> into three categories:\n\n-  <i>None<\/i>, <b><i>FamSize<\/i><\/b> = 1\n\n\n- <i>Small<\/i>, 2 <= <b><i>FamSize<\/i><\/b> <= 4\n\n\n- <i>Big<\/i>, <b><i>FamSize<\/i><\/b> > 4","9ede48bd":"So apparently <b><i>FamSize<\/i><\/b> has a lower PCC than both <b><i>SibSp<\/i><\/b> and <b><i>Parch<\/i><\/b>: we made things worse! No problem, we still need to manipulate <b><i>FamSize<\/i><\/b> a little bit.\n\nAnother thing to notice is how <b><i>Age<\/i><\/b>'s PCC increased after making it binary. That's what we wanted and expected.\n\nSince later we will want to group passengers by family, let's introduce a new feature called <b><i>FamCode<\/i><\/b> which is just an identifier of a certain family. All passengers belonging to the same family must have equal <b><i>FamCode<\/i><\/b> identifier.\n\nTo isolate families, we first group passengers by <b><i>Ticket<\/i><\/b> and <b><i>Surname<\/i><\/b> and then assign the code","0108fa19":"Now it is time to add the last new feature to our dataset. We'll call it <b><i>FamAlive<\/i><\/b> and it is the fraction of family members who survived:\n<br><\/br><br><\/br><br><\/br>\n$$FamAlive = \\frac{members\\quad of\\quad the\\quad family\\quad who\\quad survived}{members\\quad of\\quad the\\quad family}$$","08d36117":"By looking at the information given we can already try to guess which feature is likely to play an important role in this classification problem (<b><i>Pclass<\/i><\/b>, <b><i>Sex<\/i><\/b> and <b><i>Age<\/i><\/b> are the most obvious ones) and which not (<b><i>PassengerId<\/i><\/b> and <b><i>Embarked<\/i><\/b>)."}}