{"cell_type":{"69137c43":"code","a92dcbd3":"code","410e70c7":"code","5c57bb22":"code","a45911df":"code","510d7321":"code","d92b5a40":"code","1e52268e":"code","fa3ea18b":"code","94e22bf5":"code","b5f43abc":"code","1c96770d":"code","812802ec":"code","1306f782":"code","903fd24d":"code","8d0221fd":"code","22a76a29":"code","be5a8cdb":"code","78977b22":"code","bc196cfd":"code","d9150947":"code","33995377":"code","c252e7f7":"code","8999ad9c":"code","1906b23f":"code","a129d20c":"code","5b9f6f41":"code","5e08a0e8":"code","c528ef95":"code","ca461dd6":"code","a45e0469":"code","ead53bbe":"markdown","e5ba51fb":"markdown","78b58c93":"markdown","9275ee7f":"markdown","6652ee88":"markdown","33144337":"markdown","dfb23fb0":"markdown"},"source":{"69137c43":"from tensorflow.keras.layers import Input, Lambda, Dense, Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# for TPU\n# from tensorflow.distribute.cluster_resolver import TPUClusterResolver\n# from tensorflow.distribute.experimental import TPUStrategy\n\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport random\nimport os\n\n\nfrom shutil import copyfile, rmtree\nfrom pathlib import Path\n\n\nfrom glob import glob\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")","a92dcbd3":"# Test\/train sizes parameters\n# Takes not more than TRAIN_SAMPLE_SIZE and TEST_SAMPLE_SIZE from a given GROPPED_CLASSES_DIR\n\nINPUT_CLASSES_DIR = '\/kaggle\/input\/fish-species\/Species\/Training_Set\/'\nTRAIN_SAMPLE_SIZE = 100\nTEST_SAMPLE_SIZE = 20\n","410e70c7":"# list of all classes available\n\nfolders = glob(INPUT_CLASSES_DIR + '\/*')\n\nprint([x.split('\/')[-1] for x in folders])","5c57bb22":"# generats samples of gives sized and copies files to train\/test folders\n\ntest_root = '.\/test'\ntrain_root = '.\/train'\n\nrmtree(train_root, ignore_errors=True)\nrmtree(test_root, ignore_errors=True)\n\n\nfor dirname in folders:\n    if len(os.listdir(dirname)) < TRAIN_SAMPLE_SIZE + TEST_SAMPLE_SIZE:\n        print(f'{dirname} contains less files than needed')\n        break\n    else:\n        # samples list of files from every subfolder\n        current_subfolder = dirname.split('\/')[-1]\n        folder_sample = random.sample(glob(dirname + '\/*'), TRAIN_SAMPLE_SIZE + TEST_SAMPLE_SIZE)\n        Path(f\"{test_root}\/{current_subfolder}\").mkdir(parents=True, exist_ok=True)\n        Path(f\"{train_root}\/{current_subfolder}\").mkdir(parents=True, exist_ok=True)\n        train_filenames = folder_sample[:TRAIN_SAMPLE_SIZE]\n        for f in train_filenames:\n            copyfile(f, f\"{train_root}\/{current_subfolder}\/{f.split('\/')[-1]}\")\n        test_filenames = folder_sample[TRAIN_SAMPLE_SIZE:]\n        for f in test_filenames:\n            copyfile(f, f\"{test_root}\/{current_subfolder}\/{f.split('\/')[-1]}\")","a45911df":"# useful for getting number of files\ntrain_files = glob(train_root + '\/*\/*.jp*g')\ntest_files = glob(test_root + '\/*\/*.jp*g')","510d7321":"print(len(train_files))\nprint(len(test_files))","d92b5a40":"# random image from train files\n\nplt.imshow(image.load_img(np.random.choice(train_files)))","1e52268e":"# random image from test files\nplt.imshow(image.load_img(np.random.choice(test_files)))","fa3ea18b":"# re-size all the images to this\nIMAGE_SIZE = [200, 200]\n\n# training config:\nepochs = 10\nbatch_size = 128","94e22bf5":"# view the structure of the model\n#model.summary()\n","b5f43abc":"\n# resnet base\nres = ResNet50(\ninput_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n\n# don't train existing weights\nfor layer in res.layers:\n  layer.trainable = False\n\n# our layers - you can add more if you want\nx = Flatten()(res.output)\n# x = Dense(1000, activation='relu')(x) # example\nprediction = Dense(len(folders), activation='softmax')(x)\n\n# create a model object\nmodel = Model(inputs=res.input, outputs=prediction)\n# tell the model what cost and optimization method to use\nmodel.compile(\nloss='sparse_categorical_crossentropy',\noptimizer='adam',\nmetrics=['accuracy'])\n","1c96770d":"# create an instance of ImageDataGenerator\ntrain_gen = ImageDataGenerator(\n  rotation_range=20,\n  width_shift_range=0.1,\n  height_shift_range=0.1,\n  shear_range=0.1,\n  zoom_range=0.2,\n  horizontal_flip=True,\n  vertical_flip=True,\n  preprocessing_function=preprocess_input\n)\n\nval_gen = ImageDataGenerator(\n  preprocessing_function=preprocess_input\n)\n","812802ec":"# get label mapping for confusion matrix plot\ntest_gen = val_gen.flow_from_directory(test_root, target_size=IMAGE_SIZE)\nprint(test_gen.class_indices)\nlabels = [None] * len(test_gen.class_indices)\nfor k, v in test_gen.class_indices.items():\n  labels[v] = k\n","1306f782":"# should be a strangely colored image (due to VGG weights being BGR)\nfor x, y in test_gen:\n  print(\"min:\", x[0].min(), \"max:\", x[0].max())\n  plt.title(labels[np.argmax(y[0])])\n  plt.imshow(x[0])\n  plt.show()\n  break","903fd24d":"# create generators\ntrain_generator = train_gen.flow_from_directory(\n  train_root,\n  target_size=IMAGE_SIZE,\n  shuffle=True,\n  batch_size=batch_size,\n  class_mode='sparse',\n)\nvalid_generator = val_gen.flow_from_directory(\n  test_root,\n  target_size=IMAGE_SIZE,\n  shuffle=False,\n  batch_size=batch_size,\n  class_mode='sparse',\n)\n","8d0221fd":"# fit the model\nr = model.fit(\n  train_generator,\n  validation_data=valid_generator,\n  epochs=epochs,\n  steps_per_epoch=len(train_files) \/\/ batch_size,\n  validation_steps=len(test_files) \/\/ batch_size,\n)\n","22a76a29":"# You can use joblib or just use \nmodel.save('fish.h5')\n\n# And to read use from keras.models import load_model\n\n# To load model model = load_model('model_trained.h5')","be5a8cdb":"#r.history","78977b22":"# loss\nplt.plot(r.history['loss'], label='train loss')\nif 'val_loss' in r.history:\n    plt.plot(r.history['val_loss'], label='val loss')\nplt.legend();\n","bc196cfd":"# accuracies\nplt.plot(r.history['accuracy'], label='train acc')\nif 'val_accuracy' in r.history:\n    plt.plot(r.history['val_accuracy'], label='val acc')\nplt.legend();\n","d9150947":"def get_confusion_matrix(data_path, N):\n  # we need to see the data in the same order\n  # for both predictions and targets\n  print(\"Generating confusion matrix\", N)\n  predictions = []\n  targets = []\n  i = 0\n  n_images = 0\n  for x, y in val_gen.flow_from_directory(\n      data_path,\n      target_size=IMAGE_SIZE,\n      shuffle=False,\n      batch_size=batch_size * 2):\n    i += 1\n    n_images += len(y)\n    if i % 50 == 0:\n      print(f'{n_images} images processed.')\n    p = model.predict(x)\n    p = np.argmax(p, axis=1)\n    y = np.argmax(y, axis=1)\n    predictions = np.concatenate((predictions, p))\n    targets = np.concatenate((targets, y))\n    if len(targets) >= N:\n      break\n\n  cm = confusion_matrix(targets, predictions)\n  return cm","33995377":"cm = get_confusion_matrix(train_root, len(train_files))\nprint(cm)\nvalid_cm = get_confusion_matrix(test_root, len(test_files))\nprint(valid_cm)","c252e7f7":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n  \"\"\"\n  This function prints and plots the confusion matrix.\n  Normalization can be applied by setting `normalize=True`.\n  \"\"\"\n  if normalize:\n      cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n      print(\"Normalized confusion matrix\")\n  else:\n      print('Confusion matrix, without normalization')\n\n  print(cm)\n\n  plt.figure(figsize=(30, 30))\n  plt.imshow(cm, interpolation='nearest', cmap=cmap)\n  plt.title(title)\n  plt.colorbar()\n  tick_marks = np.arange(len(classes))\n  plt.xticks(tick_marks, classes, rotation=45)\n  plt.yticks(tick_marks, classes)\n\n  fmt = '.2f' if normalize else 'd'\n  thresh = cm.max() \/ 2.\n  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n      plt.text(j, i, format(cm[i, j], fmt),\n               horizontalalignment=\"center\",\n               color=\"white\" if cm[i, j] > thresh else \"black\")\n\n  plt.tight_layout()\n  plt.ylabel('True label')\n  plt.xlabel('Predicted label')\n  plt.show()","8999ad9c":"plot_confusion_matrix(cm, labels, title='Train confusion matrix')\n","1906b23f":"labels = list(val_gen.flow_from_directory(test_root).class_indices.keys())\nlabels","a129d20c":"# img, label = next(test_model)\n# idx = int(label[0])\n# true_label = list(test_gen.class_indices.keys())[idx]\n# print(f'True label is {true_label}, index - {idx}')\n# print(img[0].shape)\n# plt.imshow(img[0])","5b9f6f41":"!pip install wget","5e08a0e8":"import requests, io, cv2\nimport numpy as np\nfrom PIL import Image\n","c528ef95":"URL = 'https:\/\/upload.wikimedia.org\/wikipedia\/commons\/1\/12\/Spiny_dogfish.jpg'\nresponse = requests.get(URL)\nbytes_im = io.BytesIO(response.content)\ncv_im = cv2.cvtColor(np.array(Image.open(bytes_im)), cv2.COLOR_RGB2BGR)\n\ninternal_image = cv2.resize(cv_im,IMAGE_SIZE)\ninternal_image = internal_image.reshape(1,IMAGE_SIZE[0], IMAGE_SIZE[1],3) \n\n\nplt.imshow(internal_image[0])","ca461dd6":"p = model.predict(internal_image)\np = np.argmax(p)\nlabels[p]","a45e0469":"import os\nos.chdir(r'\/kaggle\/working\/')\n%cd \/kaggle\/working\nfrom IPython.display import FileLink\nFileLink(r'*name of file*')\n\n","ead53bbe":"## ResNet","e5ba51fb":"### Loses","78b58c93":"### Sampling for test\/train","9275ee7f":"### Checking samples","6652ee88":"## Confusion matrix","33144337":"<a href=\".\/fish.h5\"> Download File <\/a>\n\n","dfb23fb0":"# Testing with external images"}}