{"cell_type":{"637a1dd9":"code","a7c64970":"code","4379e872":"code","bcf8f1d0":"code","359a7673":"code","e893fe73":"code","56479f37":"code","b53cfec0":"code","0ca1a5d8":"code","5e6f91c1":"code","569e7c0b":"code","548a0720":"code","c8d7576c":"code","0d6fcd34":"code","8c903071":"code","992745fc":"code","b186aa96":"code","a10c215d":"code","539d56ac":"code","33b011db":"code","557cde63":"code","d9018656":"code","461ea4a6":"code","a00102ba":"code","39d84b5b":"code","3abe4d44":"code","2fe5fc7b":"code","4cc84f7c":"code","a15bbbaa":"code","8ce51acc":"code","2e4b343a":"code","39e679f5":"code","121ccea5":"code","ba1e86d6":"code","26397cc2":"code","e94f9f87":"code","9cda6245":"code","688e5bc8":"code","b391ad98":"code","5e054524":"code","cada359c":"code","9f89cce2":"code","3df0b599":"code","dc4b30c7":"code","1bee0533":"code","17b4ceab":"code","8bda7630":"code","ae19ffb4":"code","a5a12d30":"code","b654d7a3":"code","a854775f":"code","da7ceca6":"code","f6343953":"code","7e8c1a1f":"code","87c593f1":"code","d8188ff1":"code","e86e5563":"code","8fe8e24f":"code","d291c349":"code","257b5f44":"code","1097d521":"code","ac04e494":"code","c7c5f6db":"code","5f13d9af":"code","ed90a6be":"code","115f3aa6":"code","be81902e":"code","59c23167":"code","64e439f0":"code","827a5059":"code","44a5953a":"code","69e571ff":"code","7de248c5":"code","b8529221":"code","cd5fea55":"code","87be15c2":"code","9b441507":"code","059b6bcc":"code","9e5f474d":"code","4be17f17":"code","350abe8b":"code","09705a64":"code","6329fcef":"code","f636bf53":"code","1ebed97a":"code","e67c759d":"code","cfc6e302":"code","3ef81131":"code","b0031aaf":"code","b76d9315":"code","4719d0d9":"code","50a9e5fc":"code","466e3773":"code","83a0b7d4":"code","9f3ddce5":"markdown","ca1e1fbb":"markdown","96547f55":"markdown","abbd21cd":"markdown","6d0c2ea0":"markdown","730fabe6":"markdown","118ce63a":"markdown","ddfe7930":"markdown","ca837f44":"markdown","c6c3ea22":"markdown","edfb5dae":"markdown","9c912c73":"markdown","d4ca788c":"markdown","90ba58b3":"markdown","97d1727c":"markdown","c9058232":"markdown","04a2349f":"markdown","c4d28fb5":"markdown","49879eec":"markdown","36e0b890":"markdown","fca89c6b":"markdown","2ad302f5":"markdown","b79d4ddf":"markdown","26f419fd":"markdown","9a73037e":"markdown","34a5f431":"markdown","df607ed4":"markdown","42115a22":"markdown","874c30ed":"markdown","62e4e56d":"markdown","3e912c08":"markdown","69d635e2":"markdown","4823b7c0":"markdown","b167cd07":"markdown","86082cec":"markdown","b899f4e1":"markdown","3b8a978b":"markdown","973bd435":"markdown","618437a1":"markdown","dc372346":"markdown","5f7af18c":"markdown","61e65f7f":"markdown","3a3a24d0":"markdown","acc9b740":"markdown","ab145705":"markdown","ce1e33a6":"markdown","ac0377ac":"markdown","8f030a2a":"markdown","c6364b95":"markdown","13eef072":"markdown","eccaa1af":"markdown","45ede7a6":"markdown","66460bb3":"markdown","014632c8":"markdown","fa820c3d":"markdown","c2ef06ba":"markdown","0170f44f":"markdown","770a7c2c":"markdown","08f4c712":"markdown","744a10f9":"markdown","f0d3f6d7":"markdown","5caeb1f1":"markdown","5c9d30da":"markdown","c39b0afa":"markdown","ffb1e562":"markdown","beaf6549":"markdown","42666c71":"markdown","32594ea2":"markdown","62cf8907":"markdown","141aac4c":"markdown","13623c2d":"markdown","8dfaf062":"markdown","def21298":"markdown","a95ad1df":"markdown","36ebedcc":"markdown","27a56033":"markdown","366ef347":"markdown","aace0436":"markdown","00a35e3c":"markdown","d65f6c6e":"markdown","b2998c66":"markdown","276932cc":"markdown","fe240b3c":"markdown","2499d699":"markdown","8f2681f2":"markdown","786f8859":"markdown","614eff1e":"markdown","b76d4bad":"markdown","18c09eb1":"markdown","b065ddad":"markdown","c71da1eb":"markdown","bd2e1c2c":"markdown","9fbfdb62":"markdown","77cacba7":"markdown","aee75190":"markdown","4cc10d6f":"markdown","f6e69aeb":"markdown","903e7ccf":"markdown","b5880e4f":"markdown","9c1e3e7d":"markdown","6a46ff8c":"markdown","7ae06396":"markdown"},"source":{"637a1dd9":"#insatallation of additional packages - only needed if not already installed -> after installation restart the Kernel\n\n!pip install --user -U nltk\n!pip install --user -U spacy\n!pip install --user -U biopython\n!pip install --user -U rake-nltk\n!pip install --user -U gensim","a7c64970":"!python -m spacy download --user en_core_web_md\n!python -m spacy download --user en_core_web_lg","4379e872":"import os, zipfile\nimport sys\nimport json\nimport pandas as pd\nfrom time import time\nfrom datetime import datetime\nfrom pprint import pprint\nfrom bs4 import BeautifulSoup  # HTML data structure\nfrom urllib.request import urlopen as uReq  # Web client\nimport requests\nfrom requests.adapters import HTTPAdapter\nfrom requests.packages.urllib3.util.retry import Retry\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.manifold import TSNE\n\nimport heapq\n\nimport spacy\nfrom spacy.matcher import PhraseMatcher\nfrom spacy.lang.en import English\nimport en_core_web_lg\nimport en_core_web_md\n\nimport re\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\n\nfrom Bio import Entrez\nfrom Bio import Medline\nfrom rake_nltk import Metric, Rake\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\nsns.set()\n\n\n# Gensim\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)","bcf8f1d0":"\n>>> Entrez.email = \"emina.atlagic@gmail.com\" # Always tell NCBI who you are\n>>> handle = Entrez.egquery(term=\"\"\" \n                            ((SARS-CoV-2 OR covid OR coronavirus OR cord-19 OR MERS) AND (epidemiology)) OR\n                            ((SARS-CoV-2 OR covid OR coronavirus OR cord-19 OR MERS) AND (prevalence)) OR\n                            ((SARS-CoV-2 OR covid OR coronavirus OR cord-19 OR MERS) AND (screening)) OR\n                            ((SARS-CoV-2 OR covid OR coronavirus OR cord-19 OR MERS) AND (denominatiors for testing)) OR\n                            ((SARS-CoV-2 OR covid OR coronavirus OR cord-19 OR MERS) AND (incidence))\"\"\")\n>>> record = Entrez.read(handle)\n>>> for row in record[\"eGQueryResult\"]:\n    print(row[\"DbName\"], row[\"Count\"])","359a7673":"Entrez.email = \"emina.atlagic@gmail.com\" # Always tell NCBI who you are\nhandle = Entrez.esearch( db=\"pubmed\",term=\"\"\" \n                            ((SARS-CoV-2 OR covid OR coronavirus OR cord-19 OR MERS) AND (epidemiology)) OR\n                            ((SARS-CoV-2 OR covid OR coronavirus OR cord-19 OR MERS) AND (prevalence)) OR\n                            ((SARS-CoV-2 OR covid OR coronavirus OR cord-19 OR MERS) AND (screening)) OR\n                            ((SARS-CoV-2 OR covid OR coronavirus OR cord-19 OR MERS) AND (denominatiors for testing)) OR\n                            ((SARS-CoV-2 OR covid OR coronavirus OR cord-19 OR MERS) AND (incidence)) \"\"\")\nrecord = Entrez.read(handle)\nidlist = record[\"IdList\"]\n\n# limit to just retrieve 50 articles\nhandle = Entrez.efetch(db=\"pubmed\", id=idlist, rettype=\"medline\",retmode=\"text\", sort='relevance', RetMax=50)\nrecords = Medline.parse(handle)","e893fe73":"PubMedData=pd.DataFrame()\n\n\nfrom Bio import Medline\n\nrecords = Medline.parse(handle)\nfor record in records:\n     if \"PMID\" not in record: # if there's no PMID in this record (rare), skip it\n            continue\n     PMID='NA'\n     if \"PMID\"  in record: # if there's no PMID in this record (rare), skip it\n            PMID=record[\"PMID\"]\n     OT='NA'\n     if \"OT\" in record: # if there's a pub type list in this record, store it\n            OT=record[\"OT\"]\n     TI='NA'\n     if \"TI\" in record: # if there's a title in this record, store it\n            TI=record[\"TI\"]\n     AB='NA'\n     if \"AB\" in record: # if there's an abstract in this record, store it\n            AB=record[\"AB\"]\n     MH='NA'\n     if \"MH\" in record:\n            MH=record[\"MH\"]\n     MySource='PubMed'\n            \n     PubMedData = PubMedData.append({'PMID': record[\"PMID\"],\n                                              'MainTerms': OT,\n                                              'Title': TI,\n                                              'Abstract': AB,\n                                              'MeSH': MH,\n                                              'PubSource':MySource,\n                                              'AbLength': len(AB)}, ignore_index=True)","56479f37":"PubMedData.head(50)","b53cfec0":"\nKeyPhrases=pd.DataFrame()\nr = Rake(ranking_metric=Metric.DEGREE_TO_FREQUENCY_RATIO)\n\nfor i in range(0, len(PubMedData)):\n     \n     MyText=PubMedData[\"Abstract\"][i] + PubMedData[\"Title\"][i]\n     r.extract_keywords_from_text(MyText)  \n     myList=pd.Series(r.get_ranked_phrases())\n     KeyPhrase=pd.DataFrame(myList)\n     KeyPhrase['PMID'] = PubMedData[\"PMID\"][i]\n     KeyPhrase['Abstract'] = PubMedData[\"Abstract\"][i]\n    \n     KeyPhrases=pd.concat([ KeyPhrases, KeyPhrase], axis=0)\n\n\nKeyPhrases","0ca1a5d8":"#create session\nsession = requests.Session()\nretry = Retry(connect=3, backoff_factor=0.5)\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('http:\/\/', adapter)\nsession.mount('https:\/\/', adapter)\n# add url to page with articles\nurl = \"https:\/\/www.medtechdive.com\/topic\/diagnostics\/\"\nsession.get(url)\npage = requests.get(url)\nsoup = BeautifulSoup(page.content, 'html.parser')\ncontainers = soup.find_all('div',class_=\"medium-8 columns\")\npagelinks = []\n#get all page links from html syntax\nfor link in containers:    \n      url = link.find_all('a')[0]   \n      pagelinks.append('https:\/\/www.medtechdive.com'+url.get('href'))","5e6f91c1":"thearticle = []\nfor link in pagelinks:    \n    # store the text for each article\n    paragraphtext = []    \n    # get url\n    url = link\n    # get page text\n    page = requests.get(url)\n    # parse with BFS\n    soup = BeautifulSoup(page.content, 'html.parser')    \n    # get text\n    articletext = soup.find_all('p')\n    # print text\n    for paragraph in articletext[:-3]:\n        # get the text only\n        text = paragraph.get_text()\n        paragraphtext.append(text)        \n    # combine all paragraphs into an article\n    thearticle.append(paragraphtext)","569e7c0b":"myarticle = [' '.join(article) for article in thearticle]\n# subset to covid related sentences\nfindings = []\nfor text in myarticle:\n    findings.append(re.findall(r'([^.]*COVID-19[^.]*)', text))\n    findings.append(re.findall(r'([^.]*Coronavirus[^.]*)', text))\n    findings.append(re.findall(r'([^.]*SARS-COV-2[^.]*)', text))\n    findings.append(re.findall(r'([^.]*Prevalence[^.]*)', text))\n    findings.append(re.findall(r'([^.]*Incidence[^.]*)', text))\n    findings.append(re.findall(r'([^.]*Epidemiology[^.]*)', text))\n    findings.append(re.findall(r'([^.]*screening[^.]*)', text))\n    findings.append(re.findall(r'([^.]*denominators for testing[^.]*)', text))\nfindings = [x for x in findings if x != []]\ncleant = [' '.join(finding) for finding in findings]   \ncleant = '.'.join(cleant)","548a0720":"#remove empty list.\nformatted=(re.sub(r'\\[[0-9]*\\]', ' ', cleant))\nformatted=(re.sub(r'\\s+', ' ', cleant))\nformatted=(re.sub('[^a-zA-Z]', ' ', cleant))\n\n#Converting Text To Sentences\nsentence_list = nltk.sent_tokenize(cleant)\nstopwords = nltk.corpus.stopwords.words('english')\n\nword_frequencies = {}\nfor word in nltk.word_tokenize(formatted):\n    if word not in stopwords:\n        if word not in word_frequencies.keys():\n            word_frequencies[word] = 1\n        else:\n            word_frequencies[word] += 1\nmaximum_frequncy = max(word_frequencies.values())\n\nfor word in word_frequencies.keys():\n    word_frequencies[word] = (word_frequencies[word]\/maximum_frequncy)\n#Calculating Sentence Scores\nsentence_scores = {}\nfor sent in sentence_list:\n    for word in nltk.word_tokenize(sent.lower()):\n        if word in word_frequencies.keys():\n            if len(sent.split(' ')) < 100:\n                if sent not in sentence_scores.keys():\n                    sentence_scores[sent] = word_frequencies[word]\n                else:\n                    sentence_scores[sent] += word_frequencies[word]\n\nsentence_scores","c8d7576c":"summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\nsummary = ' '.join(summary_sentences)\nprint(summary)","0d6fcd34":"meta_df = pd.read_csv('\/hackathon\/covid-19\/data\/metadata.csv', usecols=['doi','title','abstract'],\n    dtype={'doi': str, 'title': str,'abstract': str},\n    skip_blank_lines=True, error_bad_lines= False\n)\n#Inspect the data\nprint(meta_df.shape)\nmeta_df.head()\nmeta_df.describe()","8c903071":"#Remove all Rows with Nan\nmeta_df.dropna(inplace=True)\n# Handle Possible Duplicates\nprint('Before:',meta_df.shape)\nmeta_df.drop_duplicates(['abstract'], inplace=True)\nprint('After:',meta_df.shape)\nmeta_df.describe(include='all')\nmeta_df = meta_df.applymap(lambda s:s.lower() if type(s) == str else s)\n#Remove word abstract\nmeta_df['abstract'] = meta_df['abstract'].str.replace('abstract', ' ')\nmeta_df['abstract'].dropna(inplace=True)\n# remove punctuation from text\nmeta_df['title'] = meta_df['title'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]',' ',x))","992745fc":"text=pd.DataFrame(meta_df.abstract)\ntext.head()\n# Check for missing values\/NaN and Drop it\nprint('-------------------Datencleaning------------------')\nprint('# NaN = ',text.isnull().sum().sum())\nprint(text.shape)\ntext=text.dropna(axis=0)\nprint(text.shape)\n#print()\nprint('# NaN = ',text.isnull().sum().sum())\n# Helper function\n#  for strings\nREPLACE_BY_SPACE_RE = re.compile('[\/(){}\\[\\]\\|@,;]')\nBAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\nSTOPWORDS = set(stopwords.words('english'))\n\ndef text_prepare(text):\n    \"\"\"\n        text: a string\n        \n        return: modified initial string\n    \"\"\"\n    text = text.lower()\n    text = REPLACE_BY_SPACE_RE.sub(\" \",text)# replace REPLACE_BY_SPACE_RE symbols by space in text\n    text = BAD_SYMBOLS_RE.sub(\" \",text)# delete symbols which are in BAD_SYMBOLS_RE from text\n    text = ' '.join([word for word in text.split() if word not in STOPWORDS]) # delete stopwords from text\n    return text","b186aa96":"# Define search statement\nStatment0= \"diagnostics technology roadmap. New platforms and technology (e.g., CRISPR) to improve response times and employ more holistic approaches to COVID-19 and future diseases. Coupling genomics and diagnostic testing on a large scale.\".lower()\nStatment0=text_prepare(Statment0)\ntext['Comp_Statment0']=0.1\ntext.head(2)\nt0 = time()  # Start timer\nfor Responses_row in range(len(text)):  \n    # https:\/\/spacy.io\/api\/token#attributes\n    nlp_doc=nlp(text_prepare(text.iat[Responses_row,0]))  # Remove unwanted with text_prepare\n    #if nlp_doc==\"\":\n    #    print('Empty Abstract in Row:',Responses_row)\n    \n    if Responses_row % 100 == 0: #Show actual number in running loop\n        #print(Responses_row, ',',end='') # print without newline \n        print('Actual Row: ',Responses_row, \", Remaining Minutes = \" ,\n              ((time() - t0)\/60\/(Responses_row+1))*(len(text)-Responses_row+1))\n    \n    text.iat[Responses_row,1]= nlp_doc.similarity(nlp(Statment0))\n    # Copy df to df_Calc\ntext_Calc = text.copy(deep=True)\ntext_Calc.head()\n## Sweep through\nfor Responses_row in range(len(text_Calc)):   # len(df_Responses)\n    \n    if  text_Calc.iat[Responses_row,1]=='Comp_Statment0':\n        text_Calc.iat[Responses_row,2]=str(Statment0)\n# Subset dataset to above 1st IQR - 0.775211\ntext_sub = text.loc[text['Comp_Statment0'] > 0.775211]","a10c215d":"# 2-Grams\nwords = []\nfor ii in range(0,len(text_sub)):\n    words.append(str(text_sub.iloc[ii]['abstract']).split(\" \"))\nn_gram_all = []\n\nfor word in words:\n    # get n-grams for the instance\n    n_gram = []\n    for i in range(len(word)-2+1):\n        n_gram.append(\"\".join(word[i:i+2]))\n    n_gram_all.append(n_gram)\n# Vectorize with HashingVectorizer\nfrom sklearn.feature_extraction.text import HashingVectorizer\n\n# hash vectorizer instance\nhvec = HashingVectorizer(lowercase=False, analyzer=lambda l:l, n_features=2**12)\n\n# features matrix X\nX = hvec.fit_transform(n_gram_all)\n# test set size of 20% of the data and the random seed 42 <3\nX_train, X_test = train_test_split(X.toarray(), test_size=0.2, random_state=42)\n# See size\nprint(\"X_train size:\", len(X_train))\nprint(\"X_test size:\", len(X_test), \"\\n\")","539d56ac":"tsne = TSNE(verbose=1, perplexity=5)\nX_embedded = tsne.fit_transform(X_train)\n# sns settings\nsns.set(rc={'figure.figsize':(15,15)})\n\n# colors\npalette = sns.color_palette(\"bright\", 1)\n\n# plot\nsns.scatterplot(X_embedded[:,0], X_embedded[:,1], palette=palette)\n\nplt.title(\"t-SNE Covid-19 Articles\")\n# plt.savefig(\"plots\/t-sne_covid19.png\")\nplt.show()","33b011db":"\n#set path for your extraction directory\nmyDirectory=os.getcwd() + '\\Downloads'\n\n# path to downloaded Kaeggle .zip file\nfilename=os.getcwd() + '\\Downloads\\CORD-19-research-challenge.zip'\n\nzip_ref = zipfile.ZipFile(filename, 'r') # create zipfile object\nzip_ref.extractall(myDirectory+ '\\CORD-19-research-challenge') # extract file to dir\nzip_ref.close()","557cde63":"# folder names with extracted Kaggle data\nfolders = ['biorxiv_medrxiv', 'comm_use_subset', 'custom_license',  'noncomm_use_subset']\n\n#create data frame\ndf_files = pd.DataFrame(columns=['filename','folder'])\n\n# get all files for processing\nfor folder in folders:\n  list = os.listdir(myDirectory + '\\\\CORD-19-research-challenge\\\\'  + folder + '\\\\' + folder + '\\\\pdf_json\\\\')\n  df = pd.DataFrame(list,columns =['filename']) \n  df['folder'] = [folder] * len(df.index)\n  df_files = df_files.append(df, sort=False)\n\ndf_files.reset_index(inplace=True, drop=True)\n","d9018656":"#create new data frame for article data\ndf_data = pd.DataFrame(columns=['filename','folder','paper_id','title','authors','abstract','body_text','back_matter'])\n\n# extract article, authors, abstract, body_text, back_matter, paper id from json files\n\nfor i in range(0,len(df_files.index)):\n    #get file\n    with open(myDirectory + '\/CORD-19-research-challenge\/'  + df_files['folder'][i] + '\/' + df_files['folder'][i] + '\/pdf_json\/' + df_files['filename'][i] ) as f:\n        datastore = json.load(f)\n    section = ''\n    text = ''\n    \n    #get body text\n    for body_text in range(0,len(datastore['body_text'])):\n        if datastore['body_text'][body_text]['section'] != section:\n            if with_section == True:\n                text = text + \"\\r\\n\" + datastore['body_text'][body_text]['section'] + \"\\r\\n\"\n            else:\n                text = text + \"\\r\\n\"\n        section = datastore['body_text'][body_text]['section']\n        text = text + datastore['body_text'][body_text]['text']\n    if text == '':\n        text = None\n    else:\n        text = text[2:]\n    section = ''\n    abstract = ''\n     #get abstract\n    for body_text in range(0,len(datastore['abstract'])):\n        if datastore['abstract'][body_text]['section'] != section:\n            if with_section == True:\n                abstract = abstract + \"\\r\\n\" + datastore['abstract'][body_text]['section'] + \"\\r\\n\"\n            else:\n                abstract = abstract + \"\\r\\n\"\n        section = datastore['abstract'][body_text]['section']\n        abstract = abstract + datastore['abstract'][body_text]['text']\n    if abstract == '':\n        abstract = None\n    else:\n        abstract = abstract[2:]\n    section = ''\n    back_matter = ''\n    \n    #get back_matter\n    for body_text in range(0,len(datastore['back_matter'])):\n        if datastore['back_matter'][body_text]['section'] != section:\n            if with_section == True:\n                back_matter = back_matter + \"\\r\\n\" + datastore['back_matter'][body_text]['section'] + \"\\r\\n\"\n            else:\n                back_matter = back_matter + \"\\r\\n\"\n        section = datastore['back_matter'][body_text]['section']\n        back_matter = back_matter + datastore['back_matter'][body_text]['text']\n    if back_matter == '':\n        back_matter = None\n    else:\n        back_matter = back_matter[2:]\n    names = ''\n    \n    #get authors\n    for authors in range(0,len(datastore['metadata']['authors'])):\n        names = names + datastore['metadata']['authors'][authors]['first'] +' '+ datastore['metadata']['authors'][authors]['last'] +'; '\n        \n    data = {'filename': [df_files['filename'][i]], 'folder': [df_files['folder'][i]], 'paper_id': [datastore['paper_id']], 'title': [datastore['metadata']['title']], 'authors': [names[:-2]], 'abstract': [abstract], 'body_text': [text], 'back_matter': [back_matter]}\n    \n    #add all in data frame df_data\n    df_data = df_data.append(pd.DataFrame.from_dict(data, orient='index').transpose(),ignore_index=True)   ","461ea4a6":" df_data.head()","a00102ba":"df_data.to_pickle(myDirectory + '\/CORD-19-research-challenge\/'  + 'df_data.zip', compression='zip')","39d84b5b":"# define signs to be replaced\nREPLACE_BY_SPACE_RE = re.compile('[\/(){}\\[\\]\\|@,;\\r\\n]')\n# define symbols to be replaced\nBAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n# define stopwords to be replaced\nSTOPWORDS = set(stopwords.words('english'))\n\ndef text_prepare(text):\n    \"\"\"\n        text: a string\n        \n        return: modified initial string\n    \"\"\"\n    if text != None:\n        text = text.lower()\n        text = REPLACE_BY_SPACE_RE.sub(\" \",text)      # replace REPLACE_BY_SPACE_RE symbols by space in text\n        text = BAD_SYMBOLS_RE.sub(\"\",text)            # delete symbols which are in BAD_SYMBOLS_RE from text\n        text = ' '.join([word for word in text.split() if word not in STOPWORDS]) # delete stopwords from text\n    return text\n","3abe4d44":"if with_section == True:\n    df_data_text_prepared = df_data\n    df_data_text_prepared['title'] = df_data['title'].apply(text_prepare)\n    df_data_text_prepared['abstract'] = df_data['abstract'].apply(text_prepare)\n    df_data_text_prepared['body_text'] = df_data['body_text'].apply(text_prepare)\n    df_data_text_prepared['back_matter'] = df_data['back_matter'].apply(text_prepare)\n","2fe5fc7b":"import warnings\nwarnings.simplefilter(\"ignore\", DeprecationWarning)\n\n# set NLP object\nnlp = spacy.load('en_core_web_lg') #large\n\n\n\n#Key Statments to compare with abstracts\nStatment0=\"How widespread current exposure is to be able to make immediate policy recommendations on mitigation measures. Denominators for testing and a mechanism for rapidly sharing that information, including demographics, to the extent possible. Epidemiology. Surveillance\".lower()\nStatment1=\"Efforts to increase capacity on existing diagnostic platforms and tap into existing surveillance platforms\".lower()\nStatment2=\"Recruitment, support, and coordination of local expertise and capacity (public, private\u2014commercial, and non-profit, including academic), including legal, ethical, communications, and operational issues\".lower()\nStatment3=\"National guidance and guidelines about best practices to states (e.g., how states might leverage universities and private laboratories for testing purposes, communications to public health officials and the public\".lower()\nStatment4=\"Development of a point-of-care test (like a rapid influenza test) and rapid bed-side tests, recognizing the tradeoffs between speed, accessibility, and accuracy\".lower()\nStatment5=\"Rapid design and execution of targeted surveillance experiments calling for all potential testers using PCR in a defined area to start testing and report to a specific entity. These experiments could aid in collecting longitudinal samples, which are critical to understanding the impact of ad hoc local interventions\".lower()\nStatment6=\"Separation of assay development issues from instruments, and the role of the private sector to help quickly migrate assays onto those devices\".lower()\nStatment7=\"Efforts to track the evolution of the virus (i.e., genetic drift or mutations) and avoid locking into specific reagents and surveillance\/detection schemes\".lower()\nStatment8=\"Latency issues and when there is sufficient viral load to detect the pathogen, and understanding of what is needed in terms of biological and environmental sampling\".lower()\nStatment9=\"Use of diagnostics such as host response markers (e.g., cytokines) to detect early disease or predict severe disease progression, which would be important to understanding best clinical practice and efficacy of therapeutic interventions\".lower()\nStatment10=\"One Health surveillance of humans and potential sources of future spillover or ongoing exposure for this organism and future pathogens, including both evolutionary hosts (e.g., bats) and transmission hosts (e.g., heavily trafficked and farmed wildlife and domestic food and companion species), inclusive of environmental, demographic, and occupational risk factors\".lower()\n\n# clean statements from stopwords, bad symbols\nStatment0=text_prepare(Statment0)\nStatment1=text_prepare(Statment1)\nStatment2=text_prepare(Statment2)\nStatment3=text_prepare(Statment3)\nStatment4=text_prepare(Statment4)\nStatment5=text_prepare(Statment5)\nStatment6=text_prepare(Statment6)\nStatment7=text_prepare(Statment7)\nStatment8=text_prepare(Statment8)\nStatment9=text_prepare(Statment9)\nStatment10=text_prepare(Statment10)","4cc84f7c":"#Add New Column with low similarity as place holder\n\ndf_data_text_prepared['Comp_Statment0']=0.1\ndf_data_text_prepared['Comp_Statment1']=0.1\ndf_data_text_prepared['Comp_Statment2']=0.1\ndf_data_text_prepared['Comp_Statment3']=0.1\ndf_data_text_prepared['Comp_Statment4']=0.1\ndf_data_text_prepared['Comp_Statment5']=0.1\ndf_data_text_prepared['Comp_Statment6']=0.1\ndf_data_text_prepared['Comp_Statment7']=0.1\ndf_data_text_prepared['Comp_Statment8']=0.1\ndf_data_text_prepared['Comp_Statment9']=0.1\ndf_data_text_prepared['Comp_Statment10']=0.1\n\n","a15bbbaa":"nlp.max_length = 2000000\nnlp.max_length","8ce51acc":"# check all articles for similarity and assign the percentage for each statement to the article\n\nfor Responses_row in range(len(df_data_text_prepared['body_text'])):  # len(df_Responses_lower)\n    nlp_doc=nlp(df_data_text_prepared['body_text'][Responses_row])\n    df_data_text_prepared.loc[Responses_row, 'Comp_Statment0'] = nlp_doc.similarity(nlp(Statment0))\n    df_data_text_prepared.loc[Responses_row, 'Comp_Statment1'] = nlp_doc.similarity(nlp(Statment1))\n    df_data_text_prepared.loc[Responses_row, 'Comp_Statment2'] = nlp_doc.similarity(nlp(Statment2))\n    df_data_text_prepared.loc[Responses_row, 'Comp_Statment3'] = nlp_doc.similarity(nlp(Statment3))\n    df_data_text_prepared.loc[Responses_row, 'Comp_Statment4'] = nlp_doc.similarity(nlp(Statment4))\n    df_data_text_prepared.loc[Responses_row, 'Comp_Statment5'] = nlp_doc.similarity(nlp(Statment5))\n    df_data_text_prepared.loc[Responses_row, 'Comp_Statment6'] = nlp_doc.similarity(nlp(Statment6))\n    df_data_text_prepared.loc[Responses_row, 'Comp_Statment7'] = nlp_doc.similarity(nlp(Statment7))\n    df_data_text_prepared.loc[Responses_row, 'Comp_Statment8'] = nlp_doc.similarity(nlp(Statment8))\n    df_data_text_prepared.loc[Responses_row, 'Comp_Statment9'] = nlp_doc.similarity(nlp(Statment9))\n    df_data_text_prepared.loc[Responses_row, 'Comp_Statment10'] = nlp_doc.similarity(nlp(Statment10))","2e4b343a":"df_data_text_prepared.head()","39e679f5":"for item in range(len(df_data_text_prepared)):\n    if df_data_text_prepared['Comp_Statment0'][item] == df_data_text_prepared['Comp_Statment0'].max():\n        print('Comp_Statment0')\n        print(df_data_text_prepared['title'][item])\n    if df_data_text_prepared['Comp_Statment1'][item] == df_data_text_prepared['Comp_Statment1'].max():\n        print('Comp_Statment1')\n        print(df_data_text_prepared['title'][item])\n    if df_data_text_prepared['Comp_Statment2'][item] == df_data_text_prepared['Comp_Statment2'].max():\n        print('Comp_Statment2')\n        print(df_data_text_prepared['title'][item])\n    if df_data_text_prepared['Comp_Statment3'][item] == df_data_text_prepared['Comp_Statment3'].max():\n        print('Comp_Statment3')\n        print(df_data_text_prepared['title'][item])\n    if df_data_text_prepared['Comp_Statment4'][item] == df_data_text_prepared['Comp_Statment4'].max():\n        print('Comp_Statment4')\n        print(df_data_text_prepared['title'][item])        \n    if df_data_text_prepared['Comp_Statment5'][item] == df_data_text_prepared['Comp_Statment5'].max():\n        print('Comp_Statment5')\n        print(df_data_text_prepared['title'][item])  \n    if df_data_text_prepared['Comp_Statment6'][item] == df_data_text_prepared['Comp_Statment6'].max():\n        print('Comp_Statment6')\n        print(df_data_text_prepared['title'][item])  \n    if df_data_text_prepared['Comp_Statment7'][item] == df_data_text_prepared['Comp_Statment7'].max():\n        print('Comp_Statment7')\n        print(df_data_text_prepared['title'][item])  \n    if df_data_text_prepared['Comp_Statment8'][item] == df_data_text_prepared['Comp_Statment8'].max():\n        print('Comp_Statment8')\n        print(df_data_text_prepared['title'][item])    \n    if df_data_text_prepared['Comp_Statment9'][item] == df_data_text_prepared['Comp_Statment9'].max():\n        print('Comp_Statment9')\n        print(df_data_text_prepared['title'][item])  \n    if df_data_text_prepared['Comp_Statment10'][item] == df_data_text_prepared['Comp_Statment10'].max():\n        print('Comp_Statment10')\n        print(df_data_text_prepared['title'][item])  ","121ccea5":"plt.hist(df_data_text_prepared['Comp_Statment0'], bins='auto')  # arguments are passed to np.histogram\n\nplt.show()","ba1e86d6":"plt.hist(df_data_text_prepared['Comp_Statment1'], bins='auto')  # arguments are passed to np.histogram\n\nplt.show()","26397cc2":"plt.hist(df_data_text_prepared['Comp_Statment2'], bins='auto')  # arguments are passed to np.histogram\n\nplt.show()","e94f9f87":"plt.hist(df_data_text_prepared['Comp_Statment3'], bins='auto')  # arguments are passed to np.histogram\n\nplt.show()","9cda6245":"plt.hist(df_data_text_prepared['Comp_Statment4'], bins='auto')  # arguments are passed to np.histogram\n\nplt.show()","688e5bc8":"plt.hist(df_data_text_prepared['Comp_Statment5'], bins='auto')  # arguments are passed to np.histogram\n\nplt.show()","b391ad98":"plt.hist(df_data_text_prepared['Comp_Statment6'], bins='auto')  # arguments are passed to np.histogram\n\nplt.show()","5e054524":"plt.hist(df_data_text_prepared['Comp_Statment7'], bins='auto')  # arguments are passed to np.histogram\n\nplt.show()","cada359c":"plt.hist(df_data_text_prepared['Comp_Statment8'], bins='auto')  # arguments are passed to np.histogram\n\nplt.show()","9f89cce2":"plt.hist(df_data_text_prepared['Comp_Statment9'], bins='auto')  # arguments are passed to np.histogram\n\nplt.show()","3df0b599":"plt.hist(df_data_text_prepared['Comp_Statment10'], bins='auto')  # arguments are passed to np.histogram\n\nplt.show()","dc4b30c7":"SubQuestion_doc=\"Development of a point-of-care POC test (like a rapid influenza test) and rapid bed-side tests, recognizing the tradeoffs between speed, accessibility, and accuracy.\"","1bee0533":"#nltk.download('stopwords')\n\n# NLTK Stop words\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nstop_words.extend(['from', 'subject', 're', 'edu', 'use', 'thus', 'also', 'abstract'])","17b4ceab":"import pyLDAvis \nimport pyLDAvis.gensim # don't skip this import matplotlib.pyplot as plt %matplotlib inline","8bda7630":"#  for strings\nREPLACE_BY_SPACE_RE = re.compile('[\/(){}\\[\\]\\|@,;]')\nBAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\nSTOPWORDS = set(stopwords.words('english'))\n\ndef text_prepare(text):\n    \"\"\"\n        text: a string\n        \n        return: modified initial string\n    \"\"\"\n    text = text.lower()\n    text = REPLACE_BY_SPACE_RE.sub(\" \",text)# replace REPLACE_BY_SPACE_RE symbols by space in text\n    text = BAD_SYMBOLS_RE.sub(\" \",text)# delete symbols which are in BAD_SYMBOLS_RE from text\n    text = ' '.join([word for word in text.split() if word not in STOPWORDS]) # delete stopwords from text\n    return text\n\n\ntext_prepare('Test Case - in and')","ae19ffb4":"# RAW DATA\nFilename='df_data.zip'\n# https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.read_pickle.html\ndf_raw = pd.read_pickle(myDirectory + '\/CORD-19-research-challenge\/'  + Filename, compression='zip')","a5a12d30":"# Use only column abstract  \ndf_raw_abstract =pd.DataFrame(df_raw['abstract'],columns=['abstract'])\ndel df_raw\n#df_raw_abstract","b654d7a3":"#Remove all Rows with Nan\nprint('Before:',df_raw_abstract.shape)\ndf_raw_abstract.dropna(inplace=True)\nprint('After:',df_raw_abstract.shape)\n\n#Reset Index\ndf_raw_abstract = df_raw_abstract.reset_index(drop=True)\n#df_raw_abstract.head(9)","a854775f":"print('Rows, Columns:',df_raw_abstract.shape)\nprint('-------------------Headers------------------')\nprint(df_raw_abstract.head(2))","da7ceca6":"\n# Reduce for Testing amount of data \ndf_Results=df_data_text_prepared[['paper_id', 'title','abstract','Comp_Statment4']]\n\n#Remove all Rows with Nan\nprint('Before:',df_Results.shape)\ndf_Results.dropna(inplace=True)\nprint('After:',df_Results.shape)\n\n#Reset Index\ndf_Results = df_Results.reset_index(drop=True)\ndf_Results.head(9)\n\n# get just abstracts into df for LDA Calc\ndf=df_Results[['abstract']]","f6343953":"# Convert to list\ndata = df.values.tolist()\n\npprint(data[:1])","7e8c1a1f":"def sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n\ndata_words = list(sent_to_words(data))\n\nprint(data_words[:1])","87c593f1":"\n# https:\/\/radimrehurek.com\/gensim\/models\/phrases.html\nbigram = gensim.models.Phrases(data_words, min_count=5, threshold=100,\n                              max_vocab_size=80000000) # higher threshold fewer phrases.\n\ntrigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n\n# Faster way to get a sentence clubbed as a trigram\/bigram\nbigram_mod = gensim.models.phrases.Phraser(bigram)\ntrigram_mod = gensim.models.phrases.Phraser(trigram)\n\n# See trigram example\nprint(trigram_mod[bigram_mod[data_words[0]]])","d8188ff1":"\ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n\ndef make_bigrams(texts):\n    return [bigram_mod[doc] for doc in texts]\n\ndef make_trigrams(texts):\n    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n\ndef lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"https:\/\/spacy.io\/api\/annotation\"\"\"\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out","e86e5563":"\ndata_words_nostops = remove_stopwords(data_words)\nprint(data_words_nostops[0])","8fe8e24f":"\ndata_words_bigrams = make_bigrams(data_words_nostops)\n#print(data_words_bigrams)","d291c349":"# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n# python3 -m spacy download en\nnlp = spacy.load('en', disable=['parser', 'ner'])\n\n# Do lemmatization keeping only noun, adj, vb, adv\ndata_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n#print(data_lemmatized)\n\n# Create Dictionary\nid2word = corpora.Dictionary(data_lemmatized)\n\n# Create Corpus\ntexts = data_lemmatized\n\n# Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in texts]\n\n# View\nprint(corpus[:1])","257b5f44":"# Human readable format of corpus (term-frequency)\n[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]","1097d521":"# https:\/\/radimrehurek.com\/gensim\/models\/ldamodel.html\nt0 = time()  # Start timer\nNumber_topics=13\n\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=Number_topics, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=5,\n                                           alpha='auto',  # alpha='auto'  \/  [0.01]*num_topics\n                                           eta = 'auto',     # eta = 'auto'  \/ [0.01]*len(id2word)\n                                           per_word_topics=True)\n\nprint('Minutes = ' ,(time() - t0)\/60)","ac04e494":"# Compute Perplexity\n# lower Perplexity = better model\n# https:\/\/en.wikipedia.org\/wiki\/Perplexity\n\nprint('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n# Perplexity:   -8.674924035071841","c7c5f6db":"# Compute Coherence Score\n# https:\/\/stackoverflow.com\/questions\/54762690\/coherence-score-0-4-is-good-or-bad\/55816086\n#Coherence score 0.4 is good or bad? .4 is low, .55 is okay, .65 might be as good as it is going to get\n\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)\n# Coherence Score:  0.5233286684160571","5f13d9af":"# Print the Keyword in the 10 topics\npprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]","ed90a6be":"pyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\nvis","115f3aa6":"SubQuestion_doc_nostops=text_prepare(SubQuestion_doc)\nSubQuestion_Tokens=gensim.utils.simple_preprocess(str(SubQuestion_doc_nostops), deacc=True)\nSubQuestion_Tokens","be81902e":"# Predicting topics on SubQuestion\nlda_model[id2word.doc2bow(SubQuestion_Tokens)][0]","59c23167":"def compute_coherence_values(dictionary, corpus, texts, limit, start, step):\n    \"\"\"\n    Compute c_v coherence for various number of topics\n\n    Parameters:\n    ----------\n    dictionary : Gensim dictionary\n    corpus : Gensim corpus\n    texts : List of input texts\n    limit : Max num of topics\n\n    Returns:\n    -------\n    model_list : List of LDA topic models\n    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n    \"\"\"\n    t0 = time()  # Start timer\n    coherence_values = []\n    model_list = []\n    for num_topics in range(start, limit, step):\n        print('num_topics=',num_topics)\n        lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=num_topics, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=5,\n                                           alpha='auto',  # alpha='auto'  \/  [0.01]*num_topics\n                                           eta = 'auto',     # eta = 'auto'  \/ [0.01]*len(id2word)\n                                           per_word_topics=True)\n        \n        model_list.append(lda_model)\n        coherencemodel = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')\n        \n        Coherence_Value=coherencemodel.get_coherence()\n        coherence_values.append(Coherence_Value)\n        print('Coherence_Value=',Coherence_Value)\n        print('SubQuestion-Fit=',lda_model[id2word.doc2bow(SubQuestion_Tokens)][0])\n        print('Minutes = ' ,(time() - t0)\/60)\n        print('------------------------------------')\n\n    return model_list, coherence_values","64e439f0":"# Can take a long time to run.\nOptimization_Start=6\nOptimization_Limit=16 \nOptimization_Step=1\n\nmodel_list, coherence_values = compute_coherence_values(dictionary=id2word,\n                                                        corpus=corpus, texts=data_lemmatized, start=Optimization_Start, \n                                                        limit=Optimization_Limit, step=Optimization_Step)","827a5059":"\nx = range(Optimization_Start, Optimization_Limit, Optimization_Step)\nplt.plot(x, coherence_values)\nplt.xlabel(\"Num Topics\")\nplt.ylabel(\"Coherence score\")\nplt.legend((\"coherence_values\"), loc='best')\nplt.show()\n\n# Print the coherence scores\nfor m, cv in zip(x, coherence_values):\n    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))","44a5953a":"max_value = max(coherence_values)\nmax_index=coherence_values.index(max_value)\n\noptimal_model = model_list[max_index] \nmodel_topics = optimal_model.show_topics(formatted=False)\n#pprint(optimal_model.print_topics(num_words=10))","69e571ff":"def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):\n    # Init output\n    sent_topics_df = pd.DataFrame()\n\n    # Get main topic in each document\n    for i, row in enumerate(ldamodel[corpus]):\n        row = sorted(row[0], key=lambda x: (x[1]), reverse=True)\n        # Get the Dominant topic, Perc Contribution and Keywords for each document\n        for j, (topic_num, prop_topic) in enumerate(row):\n            if j == 0:  # => dominant topic\n                wp = ldamodel.show_topic(topic_num)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n            else:\n                break\n    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n\n    # Add original text to the end of the output\n    contents = pd.Series(texts)\n    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n    return(sent_topics_df)","7de248c5":"df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=data)\n\n# Format\ndf_dominant_topic = df_topic_sents_keywords.reset_index()\ndf_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n\n# Show\ndf_dominant_topic.head(3)","b8529221":"# Group top 5 sentences under each topic\nsent_topics_sorteddf_mallet = pd.DataFrame()\n\nsent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n#print(sent_topics_outdf_grpd)\n\nfor i, grp in sent_topics_outdf_grpd:\n    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(2)], \n                                            axis=0)\n    \n# Reset Index    \nsent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n\n# Format\nsent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n\n# Show\nsent_topics_sorteddf_mallet.head(99)","cd5fea55":"topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n\n# Percentage of Documents for Each Topic\ntopic_contribution = round(topic_counts\/topic_counts.sum(), 4)\n\n# Topic Number and Keywords\ntopic_num_keywords = sent_topics_sorteddf_mallet[['Topic_Num', 'Keywords']]\n\n# Concatenate Column wise\ndf_dominant_topics = pd.concat([topic_num_keywords, topic_counts.sort_index(), topic_contribution.sort_index()], axis=1)\n\n# Change Column names\ndf_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n\n# Show\n#df_dominant_topics","87be15c2":"# Build LDA model\n# https:\/\/radimrehurek.com\/gensim\/models\/ldamodel.html\nt0 = time()  # Start timer\nNumber_topics=13  # best according to tests\n\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=Number_topics, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=5,\n                                           alpha='auto',  # alpha='auto'  \/  [0.01]*num_topics\n                                           eta = 'auto',     # eta = 'auto'  \/ [0.01]*len(id2word)\n                                           per_word_topics=True)\n\nprint('Minutes = ' ,(time() - t0)\/60)","9b441507":"SubQuestion_doc_nostops=text_prepare(SubQuestion_doc)\nSubQuestion_Tokens=gensim.utils.simple_preprocess(str(SubQuestion_doc_nostops), deacc=True)\nSubQuestion_Tokens","059b6bcc":"# Predicting topics on SubQuestion\nlda_model[id2word.doc2bow(SubQuestion_Tokens)][0]","9e5f474d":"Number_dominant_Topics=50\ndf_dominant_topic.head(2)","4be17f17":"# Merge with df_Results\ndf_Results.head(1)","350abe8b":"df_combined=pd.concat([df_Results, df_dominant_topic], axis=1)\ndf_combined.head(1)","09705a64":"#Drop column abtract\ndf_combined.drop(['abstract'], axis=1,inplace=True)\n\n#Merge with Original Abstract\ndf_combined=pd.concat([df_raw_abstract, df_combined], axis=1)\nprint(df_combined.shape)\ndf_combined.head(1)","6329fcef":"# Reduce Columns\ndf_combined=df_combined[['paper_id','title','abstract','Comp_Statment4','Dominant_Topic','Topic_Perc_Contrib']]\n\ndf_combined=pd.DataFrame(df_combined)","f636bf53":"#Check & delete duplicates\n#https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.drop_duplicates.html\nprint('Before:',df_combined.shape)\n\nprint(df_combined.duplicated('abstract').sum())\n\ndf_combined.drop_duplicates('abstract',inplace= True,ignore_index=True)\nprint('After:',df_combined.shape)","1ebed97a":"# Filter for Topics\n# 2,7,8 & 11\ndf_Result_Dominant=df_combined.loc[(df_combined['Dominant_Topic']==2) \n                                  | (df_combined['Dominant_Topic']==7)\n                                  | (df_combined['Dominant_Topic']==8)\n                                  | (df_combined['Dominant_Topic']==11)]\nprint(df_Result_Dominant.shape)\ndf_Result_Dominant[['title','abstract','Comp_Statment4','Dominant_Topic','Topic_Perc_Contrib']].head(2)","e67c759d":"# Use only first 50%\nFirstpart=int(0.50*len(df_Result_Dominant))\nprint(Firstpart)\nd1=df_Result_Dominant.sort_values('Comp_Statment4', ascending=False)\n# Select first part\ndf_Firstpart=d1.iloc[0:Firstpart]\ndf_Firstpart.describe()\n#d1.tail(10)","cfc6e302":"# Focus on Topics 2\nd1=df_Firstpart.loc[(df_Firstpart['Dominant_Topic']==2)]\nd1=d1.sort_values('Topic_Perc_Contrib', ascending=False).head(Number_dominant_Topics)\n# Focus on Topics 7\nd2=df_Firstpart.loc[(df_Firstpart['Dominant_Topic']==7)]\nd2=d2.sort_values('Topic_Perc_Contrib', ascending=False).head(Number_dominant_Topics)\n# Focus on Topics 8\nd3=df_Firstpart.loc[(df_Firstpart['Dominant_Topic']==8)]\nd3=d3.sort_values('Topic_Perc_Contrib', ascending=False).head(Number_dominant_Topics)\n# Focus on Topics 11\nd4=df_Firstpart.loc[(df_Firstpart['Dominant_Topic']==11)]\nd4=d4.sort_values('Topic_Perc_Contrib', ascending=False).head(Number_dominant_Topics)\n\ndf_Top=pd.concat([d1,d2,d3,d4], axis=0)\nprint(df_Top.shape)\ndf_Top[['title','abstract','Comp_Statment4','Dominant_Topic','Topic_Perc_Contrib']]","3ef81131":"#Reset Index\ndf_Top = df_Top.reset_index(drop=True)\n\n# Add column for max matches\ndf_Top['max_matches'] = 0\n#df_Top.head(3)","b0031aaf":"#Defining SubQuestion\nSubQuestion_doc=\"Development of a point-of-care test (like a rapid influenza test) and rapid bed-side tests, recognizing the tradeoffs between speed, accessibility, and accuracy.\"\nSubQuestion_doc_nlp=(nlp(SubQuestion_doc))\n\n# Reduced KeyWords\nKeyWords_list = [\"diagnostics test\",\"rapid test\",\"speed\",\"accessibility\",\"accuracy\",\n                 \"point-of-care test\",\"tradeoffs\",\"Development test\",\"Development of a point-of-care test\"\n                 \"rapid bed-side tests\",\"tradeoffs speed accessibility accuracy\",\"important accuracy\",\n                \"tradeoffs speed\", \"tradeoffs accessibility\", \"tradeoffs accuracy\"]\n\n# How to find auto Synonyms?","b76d9315":"#Matching Parameters\nmatcher = PhraseMatcher(nlp.vocab, attr='LOWER')# the list containing the pharses to be matched\n\n# convert the phrases into document object using nlp.make_doc to #speed up.\npatterns = [nlp.make_doc(text) for text in KeyWords_list]# add the patterns to the matcher object without any callbacks\n\nmatcher.add(\"Phrase Matching\", None, *patterns)# the input text string is converted to a Document object","4719d0d9":"Span_Citation_len=20\nfrom termcolor import colored  # print(colored('hello', 'red'), colored(Spanning_Citation, 'green'))\n\nprint('TOP Matches:')\n\nfor df_row in df_Top.index:  # len(df)\n                    Search_doc_nlp=nlp(df_Top.abstract[df_row])\n                    Search_paper_id=df_Top.paper_id[df_row][:60]+' ...'\n                    Search_Title=df_Top.title[df_row][:60]+' ...'\n                    \n                    \n                    #Matching\n                    matches = matcher(Search_doc_nlp) #print the matched results and extract out the results\n                    if len(matches)>0:\n                        print('----------------------------------------------------------------------')\n                        print('Found Matches =',len(matches))\n                        print('ROW=',df_row,'\/ Paper-ID =',Search_paper_id)\n                        print('TITLE=',Search_Title)\n                        print('ABSTRACT=',Search_doc_nlp[:10],' ...')\n                    \n                    df_Top.iat[df_row,6]=len(matches)\n\n                    for match_id, start, end in matches:\n                        # Get the string representation \n                        string_id = nlp.vocab.strings[match_id]  \n                        Match_Phrase = Search_doc_nlp[start:end]  # The match\n                        Spanning_Citation = Search_doc_nlp[start-Span_Citation_len:end+Span_Citation_len]  # The matched span\n                        print('    -----------------------------------------------------------------------------------------')\n                        #print('  match_id:',match_id, '\/string_id:',string_id)\n                        print('  START:',start, '\/END:',end,'\/Match_Phrase:',colored(Match_Phrase.text, 'red'))\n                        print('  ... ',colored(Spanning_Citation, 'red'),' ...')","50a9e5fc":"from gensim.summarization import summarize   \n# https:\/\/radimrehurek.com\/gensim\/summarization\/summariser.html\n# https:\/\/tedboy.github.io\/nlps\/generated\/generated\/gensim.summarization.summarize.html","466e3773":"df_Top_max_matches=df_Top.loc[(df_Top['max_matches']==max(df_Top.max_matches))]\ndf_Top_max_matches = df_Top_max_matches.reset_index(drop=True)\n#df_Top_max_matches.head(1)","83a0b7d4":"for Top_Row in range(len(df_Top_max_matches)):\n    Search_paper_id=df_Top.paper_id[Top_Row][:50]+' ...'\n    Search_Title=df_Top.title[Top_Row][:70]+' ...'\n                    \n            \n    text=str(df_Top_max_matches.iat[Top_Row,2])  # needs sentences!\n    print(Top_Row,'Article with most matches:',)\n    print('Paper-ID =',colored(Search_paper_id, 'red'))\n    print('TITLE=',colored(Search_Title, 'red'))\n    print('ABSTRACT SUMMARY: ',colored(summarize(text, word_count = 30), 'red'))\n    print('---------------------------------------------------')","9f3ddce5":"#### Terms for Query might be:\n   (Epidemiology, Incidence, Prevalence, denominatiors for testing, screening, policy) combined with one or multiple of these of these words (SARS-CoV-2 OR covid OR coronavirus OR cord-19 OR MERS)","ca1e1fbb":"### Subquestion I - Latency issues and when there is sufficient viral load to detect the pathogen, and understanding of what is needed in terms of biological and environmental sampling","96547f55":"####    Subquestion A- \nHow widespread current exposure is to be able to make immediate policy recommendations on mitigation  measures. Denominators for testing and a mechanism for rapidly sharing that information, including demographics,  to the extent possible. Sampling methods to determine asymptomatic disease (e.g., use of serosurveys (such as convalescent samples) and early detection of disease (e.g., use of screening of neutralizing antibodies such as ELISAs)\n        ","abbd21cd":"## Cluster the subset data","6d0c2ea0":"### Display results","730fabe6":"Before: (33943, 1) After: (25024, 1)","118ce63a":"#### Subset data","ddfe7930":"### Convert abstracts to list","ca837f44":"## What do we know about diagnostics and surveillance? ","c6c3ea22":"### Group top 5 sentences under each topic","edfb5dae":"### Calculate Topics for each Row","9c912c73":"### Subquestion K - One Health surveillance of humans and potential sources of future spillover or ongoing exposure for this organism and future pathogens, including both evolutionary hosts (e.g., bats) and transmission hosts (e.g., heavily trafficked and farmed wildlife and domestic food and companion species), inclusive of environmental, demographic, and occupational risk factors","d4ca788c":"### Data Exploration","90ba58b3":"## 2a) WEBSCRAPPING - CLUSTERING SUBSET","97d1727c":"## Downloading the spacy en_core_web_md\n","c9058232":"### Remove Stop Words","04a2349f":"### RESULT: Use top of dominant Topics","c4d28fb5":"### Get data into data frame","49879eec":"### Packages installation\n ","36e0b890":"## 1a) Define Search Query","fca89c6b":"### Form Bigrams","2ad302f5":"### Use Topic 2,7,8 & 11","b79d4ddf":"### Extract main data from json files","26f419fd":"### check content of articles on a webpage","9a73037e":"### Summarization","34a5f431":"### Load file names into a dataframe","df607ed4":"\n\n\n\n\nSpecifically, we want to know what the literature reports about:\n\n  - (A)How widespread current exposure is to be able to make immediate policy recommendations on mitigation                  measures.Denominators for testing and a mechanism for rapidly sharing that information, including demographics,        to the extent possible. Sampling methods to determine asymptomatic disease (e.g., use of serosurveys (such as          convalescent samples) and early detection of disease (e.g., use of screening of neutralizing antibodies such as        ELISAs).\n\n  - (B)Efforts to increase capacity on existing diagnostic platforms and tap into existing surveillance platforms.\n\n  - (C)Recruitment, support, and coordination of local expertise and capacity (public, private\u2014commercial, and non-          profit, including academic), including legal, ethical, communications, and operational issues.\n\n  - (D)National guidance and guidelines about best practices to states (e.g., how states might leverage universities and      private laboratories for testing purposes, communications to public health officials and the public).\n\n  - (E)Development of a point-of-care test (like a rapid influenza test) and rapid bed-side tests, recognizing the            tradeoffs between speed, accessibility, and accuracy.\n\n  - (F)Rapid design and execution of targeted surveillance experiments calling for all potential testers using PCR in a      defined area to start testing and report to a specific entity. These experiments could aid in collecting              longitudinal samples, which are critical to understanding the impact of ad hoc local interventions (which also        need to be recorded). \n\n  - (G)Separation of assay development issues from instruments, and the role of the private sector to help quickly            migrate assays onto those devices.\n\n  - (H)Efforts to track the evolution of the virus (i.e., genetic drift or mutations) and avoid locking into specific        reagents and surveillance\/detection schemes.\n\n  - (I)Latency issues and when there is sufficient viral load to detect the pathogen, and understanding of what is            needed in terms of biological and environmental sampling.\n\n  - (J)Use of diagnostics such as host response markers (e.g., cytokines) to detect early disease or predict severe          disease progression, which would be important to understanding best clinical practice and efficacy of therapeutic      interventions.\n\n  - (K)Policies and protocols for screening and testing.\n\n  - (L)Policies to mitigate the effects on supplies associated with mass testing, including swabs and reagents. \n\n  - (M)Technology roadmap for diagnostics.\n  \n  - (N)Barriers to developing and scaling up new diagnostic tests (e.g., market forces), how future coalition and            accelerator models (e.g., Coalition for Epidemic Preparedness Innovations) could provide critical funding for          diagnostics, and opportunities for a streamlined regulatory environment.\n\n  - (O)New platforms and technology (e.g., CRISPR) to improve response times and employ more holistic approaches to          COVID-19 and future diseases.\n\n  - (P)Coupling genomics and diagnostic testing on a large scale.\n\n  - (Q)Enhance capabilities for rapid sequencing and bioinformatics to target regions of the genome that will allow          specificity for a particular variant.\n\n  - (R)Enhance capacity (people, technology, data) for sequencing with advanced analytics for unknown pathogens, and          explore capabilities for distinguishing naturally-occurring pathogens from intentional.\n     One Health surveillance of humans and potential sources of future spillover or ongoing exposure for this organism      and future pathogens, including both evolutionary hosts (e.g., bats) and transmission hosts (e.g., heavily            trafficked and farmed wildlife and domestic food and companion species), inclusive of environmental, demographic,      and occupational risk factors.","42115a22":"### https:\/\/www.machinelearningplus.com\/nlp\/topic-modeling-gensim-python\/","874c30ed":"### Build the bigram and trigram models","62e4e56d":"### Visualize the topics","3e912c08":"-  Data Load:       download all data files in json-format from Kaeggle homepage https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/download\n-  Data Extraction: Extract relevant data parts from json files like, abstract, title, body_text, paper_id and                             save as intermediate file\n-  Data Cleaning:    removal of stopwords, bad symbols\n-  Apply similarity function for subquestions on abstracts\n-  Creation of n-grams and bi-grams\n-  LDA and Topic Modeling for subquestion E\n        \n       Pros: Deliveres great hits\n       Cons: Training of model is time consuming with calculation of similarities and topic modelling","69d635e2":"### Lemmatization & Building Corpus","4823b7c0":"### Print the Keyword in the 10 topics","b167cd07":"### Predicting topics on an unseen document","86082cec":"\nHow widespread current exposure is to be able to make immediate policy recommendations on mitigation measures. Denominators for testing and a mechanism for rapidly sharing that information, including demographics, to the extent possible. Sampling methods to determine asymptomatic disease (e.g., use of serosurveys (such as convalescent samples) and early detection of disease (e.g., use of screening of neutralizing antibodies such as ELISAs)","b899f4e1":"## Data Loading and pre -processing","3b8a978b":"## 3e) LDA & Topic Modeling on SubQuestion E","973bd435":"    \n  - Access web session\n  - Filter out articles on the webpage\n  - Define search terms\n  - show entries based on score ranking for text summarization\n  - subset Kaggle dataset based on webscrapped information\n  - cluster analysis of subseted\n       \n   PROs: \n         Easy way to get access to information\n         \n   Cons: \n          Countinous data flow from the internet. It might be difficult to pick the proper terms for the search\n\n\n      \n                                                            ","618437a1":"## 1c) Key Word Extraction","dc372346":"##  Import of needed libraries","5f7af18c":"### get all articles","61e65f7f":"### Focus on Topics 2,7,8,11","3a3a24d0":"### Optimize Number of Topics","acc9b740":"## 3c) Apply Similarities with spaCy","ab145705":"### Subquestion E - Development of a point-of-care test (like a rapid influenza test) and rapid bed-side tests, recognizing the tradeoffs between speed, accessibility, and accuracy","ce1e33a6":"# COVID-19 Open Research Dataset Challenge (CORD-19)","ac0377ac":"TOP Matches:\n----------------------------------------------------------------------\nFound Matches = 3\nROW= 15 \/ Paper-ID = 590e55ca7451d6a3aad7b2d84a04b39bc864dbe7 ...\nTITLE= college public health health informatics king saud bin abdul ...\nABSTRACT= Abstract\nRapid immunochromatographic assays for detecting infections with bovine coronavirus (BCV), rotavirus A and Cryptosporidium parvum in calf faeces were evaluated using as gold standards a reverse transcriptase polymerase chain reaction (BCV and rotavirus)  ...\n    -----------------------------------------------------------------------------------------\n  START: 235 \/END: 237 \/Match_Phrase: rapid test\n  ...  immunosorbent assay (ELISA). The sensitivity (75%) and specificity (100%) of the rapid test for C. parvum were relatively high compared to modified Ziehl-Neelsen (MZN) staining. The aim of  ...\n    -----------------------------------------------------------------------------------------\n  START: 541 \/END: 543 \/Match_Phrase: rapid test\n  ...  selected samples, which had revealed a positive RT-PCR result for BCV, but were negative in the rapid test, were retested once by RT-PCR.RT-PCR for BCV was positive in 70\/180 (38.9%)  ...\n    -----------------------------------------------------------------------------------------\n  START: 616 \/END: 618 \/Match_Phrase: rapid test\n  ...  rapid assay (Table 1) . By retesting RT-PCR positive samples that had negative results in the rapid test, the positive RT-PCR results could be reproduced and the specificity was confirmed by sequencing. All sequences  ...\n----------------------------------------------------------------------\nFound Matches = 1\nROW= 19 \/ Paper-ID = a844caa7ce1138712eee09a325f90db5adff8694 ...\nTITLE= preparation organization major incident ...\nABSTRACT= Abstract\nEscherichia coli phosphofructokinase-2 (Pfk-2) is an obligate homodimer that follows a highly cooperative threestate folding mechanism N 2 4 2I 4 2U. The strong coupling between dissociation and unfolding is a consequence of the structural features  ...\n    -----------------------------------------------------------------------------------------\n  START: 265 \/END: 266 \/Match_Phrase: accessibility\n  ...  -resolution hydrogen\/deuterium exchange mass spectrometry. Our results show that the isolated subunit has overall higher solvent accessibility than the native dimer, with the exception of residues 240-309. These residues correspond to most of  ...\n----------------------------------------------------------------------\nFound Matches = 1\nROW= 27 \/ Paper-ID = 00ec85838f0ebb20028f8600e1ad7b68155569f9 ...\nTITLE= former health promotion technical officer family health divi ...\nABSTRACT= Abstract\nIn this work we propose a simple mathematical model for the analysis of the impact of control measures against an emerging infection, namely, the severe acute respiratory syndrome (SARS). The model provides a  ...\n    -----------------------------------------------------------------------------------------\n  START: 117 \/END: 118 \/Match_Phrase: accuracy\n  ...  applied to the communities of Hong Kong and Toronto (Canada) and it mimics those epidemics with fairly good accuracy. The estimated values for the basic reproduction number, R 0 , were 1.2 for Hong Kong and 1.32  ...\n----------------------------------------------------------------------\nFound Matches = 1\nROW= 123 \/ Paper-ID = 75fffe385341bfc549542beb0cf1d140a7e30d71 ...\nTITLE= mathematical discrete epidemic model sars transmission contr ...\nABSTRACT= Abstract\nMethods for translating gene expression signatures into clinically relevant information have typically relied upon having many samples from patients with similar molecular phenotypes. Here, we address the question of what can be done when it is  ...\n    -----------------------------------------------------------------------------------------\n  START: 163 \/END: 164 \/Match_Phrase: accuracy\n  ...  compendium of public expression data sets, reformulated to create a test bed for anomaly detection. We demonstrate the accuracy of CSAX on the data sets in our compendium, compare it to other leading methods, and show that  ...\n----------------------------------------------------------------------\nFound Matches = 1\nROW= 125 \/ Paper-ID = 9e780780dbaf35c0d537ddb290dfd484148a3c55 ...\nTITLE= estimating relative probability direct transmission infectio ...\nABSTRACT= Abstract\nEstimating infectious disease parameters such as the serial interval (time between symptom onset in primary and secondary cases) and reproductive number (average number of secondary cases produced by a primary case) are important to  ...\n    -----------------------------------------------------------------------------------------\n  START: 174 \/END: 175 \/Match_Phrase: accuracy\n  ...  distance between cases to define training transmission events are able to distinguish between truly linked and unlinked pairs with high accuracy (area under the receiver operating curve value of 95%). Additionally only a subset of the cases  ...\n----------------------------------------------------------------------\nFound Matches = 1\nROW= 138 \/ Paper-ID = 493aa8aa2e9122620b1dcaf102ab62e643ad5632 ...\nTITLE= title covid19 progression timeline effectiveness responsetos ...\nABSTRACT= Abstract\nBackground: Severe ill patients with 2019 novel coronavirus (2019-nCoV) infection progressed rapidly to acute respiratory failure. We aimed to select the most useful prognostic factor for severe illness incidence.Methods: The study  ...\n    -----------------------------------------------------------------------------------------\n  START: 89 \/END: 90 \/Match_Phrase: accuracy\n  ...  selected by the LASSO COX regression analyses, to predict the severe illness probability of 2019-CoV pneumonia. The predictive accuracy was evaluated by concordance index, calibration curve, decision curve and clinical impact curve.Results: The neutrophil  ...","8f030a2a":"### Query all databases and show number of hits","c6364b95":"## 1b) Access NCBI Databases via Entrez","13eef072":"## t-SNE","eccaa1af":"###   Method 3: Retrieval of top 5 articles for a subquestion using the following methods and the Kaggle data set","45ede7a6":"{\"Brett Giroir, the Trump administration's coordinator for coronavirus diagnostic testing, said last week the FDA, CDC\\xa0and NIH are working to validate some of the COVID-19 antibody tests already released.\": 1.8051948051948048, 'Exact Sciences\u2019 core colorectal cancer screening business has previously shown vulnerability to the negative impact of infectious diseases on demand for testing': 1.077922077922078, 'In addition, patients with a suspected case of COVID-19 do not receive confirmed results for at least a few days President Donald Trump sees the Cepheid test as a potential \"game changer\" for ramping up the nation's COVID-19\\xa0testing capacity \" FDA has updated its COVID-19 diagnostic testing FAQs, which now lists the clinical labs that are offering testing, the states that have chosen to authorize labs to develop and perform tests, as well as commercial manufacturers that are distributing test kits.': 4.61038961038961, 'The Families First Coronavirus Response Act (FFCRA) states organizations that offer group or individual health insurance cannot impose cost-sharing requirements on people who seek COVID-19 tests during the pandemic\\u200b The subsequent Coronavirus Aid, Relief, and Economic Security (CARES) Act expanded the list of tests and related services covered by the prohibition on cost sharing.': 1.935064935064935, 'The cost of supplies are increasing, and at the same time, laboratories are seeing a substantial decline in non-COVID-19 testing, as patient visits to physicians plummet and elective surgeries, screenings and routine care services are postponed.\u201d Abbott on Wednesday said that a \u201cmajority\u201d of the labs are running the COVID-19 tests based on communication with its customers.': 2.4935064935064934, \"The screening requires no special equipment to process the results and can be used in a laboratory or at the point of care \\xa0BD said March 16 the companies submitted an EUA to FDA for the screening, but FDA doesn't appear to have yet authorized the test.Abbott's latest COVID-19 product isn't the first point-of-care test on the U.\": 1.3766233766233766, 'To ensure commercial labs have the equipment, supplies, staffing and resources they need, ACLA has asked the Department of Health and Human Services to immediately begin accepting applications from labs for a $100 billion fund created under the Coronavirus Aid, Relief, and Economic Security Act.': 1.3246753246753247}","66460bb3":"### Retrieve article with highest similarity","014632c8":"### Search in PubMed and retrieve data by relevance","fa820c3d":"Comp_Statment0\nconceptualising technical relationship animal disease surveillance intervention mitigation basis economic analysis\nComp_Statment2\npublic health human resources comparative analysis policy documents two canadian provinces\nComp_Statment6\npersonal view disease x accelerating development medical countermeasures next pandemic\nComp_Statment7\nmolecular diagnosis viral diseases present trends future aspects view oie collaborating centre application polymerase chain reaction methods diagnosis viral diseases veterinary medicine\nComp_Statment5\n\nComp_Statment4\nadvances addressing technical challenges pointofcare diagnostics resourcelimited settings hhs public access\nComp_Statment9\ntreatable traits therapeutic targets goals systems biology infectious disease\nComp_Statment1\nglobal polio laboratory network platform viral vaccinepreventable emerging diseases laboratory networks hhs public access\nComp_Statment8\ndetection pathogens water phylochips qpcr pyrosequencing\nComp_Statment10\nwildlife parasites one health world\nComp_Statment3\ncodes ethics public health\nComp_Statment9\ntreatable traits therapeutic targets goals systems biology infectious disease","c2ef06ba":"In addition, patients with a suspected case of COVID-19 do not receive confirmed results for at least a few days President Donald Trump sees the Cepheid test as a potential \"game changer\" for ramping up the nation's COVID-19 testing capacity \" FDA has updated its COVID-19 diagnostic testing FAQs, which now lists the clinical labs that are offering testing, the states that have chosen to authorize labs to develop and perform tests, as well as commercial manufacturers that are distributing test kits. The cost of supplies are increasing, and at the same time, laboratories are seeing a substantial decline in non-COVID-19 testing, as patient visits to physicians plummet and elective surgeries, screenings and routine care services are postponed.\u201d Abbott on Wednesday said that a \u201cmajority\u201d of the labs are running the COVID-19 tests based on communication with its customers. The Families First Coronavirus Response Act (FFCRA) states organizations that offer group or individual health insurance cannot impose cost-sharing requirements on people who seek COVID-19 tests during the pandemic\u200b The subsequent Coronavirus Aid, Relief, and Economic Security (CARES) Act expanded the list of tests and related services covered by the prohibition on cost sharing. Brett Giroir, the Trump administration's coordinator for coronavirus diagnostic testing, said last week the FDA, CDC and NIH are working to validate some of the COVID-19 antibody tests already released. The screening requires no special equipment to process the results and can be used in a laboratory or at the point of care BD said March 16 the companies submitted an EUA to FDA for the screening, but FDA doesn't appear to have yet authorized the test.Abbott's latest COVID-19 product isn't the first point-of-care test on the U. To ensure commercial labs have the equipment, supplies, staffing and resources they need, ACLA has asked the Department of Health and Human Services to immediately begin accepting applications from labs for a $100 billion fund created under the Coronavirus Aid, Relief, and Economic Security Act. Exact Sciences\u2019 core colorectal cancer screening business has previously shown vulnerability to the negative impact of infectious diseases on demand for testing","0170f44f":"Each subquestion, here A to K,  is saved in a statement variable\nThese statements will be checked for similarity in every article in the data frame\n","770a7c2c":"### Number of Documents for Each Topic","08f4c712":"### Subquestion H - Efforts to track the evolution of the virus (i.e., genetic drift or mutations) and avoid locking into specific reagents and surveillance\/detection schemes","744a10f9":"### Matching","f0d3f6d7":"## TEAM","5caeb1f1":"#### Apply function to data set","5c9d30da":"### Download Kaggle data files\nhttps:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/download ","c39b0afa":"### Task Details:\n   - What do we know about diagnostics and surveillance? \n   - What has been published concerning systematic, holistic approach to diagnostics \n     (from the public health surveillance perspective to being able to predict clinical outcomes)?","ffb1e562":"### Define functions for stopwords, bigrams, trigrams and lemmatization","beaf6549":"## 3d) Visualize the amount of articles with a specific similarity rate \nthe User easier see how many documents related to his questions are present in the data (all documents with a similarity rate greater than e.g. 90% could be selected in a second step to retrieve the best matching abstracts)\n","42666c71":"### Subquestion B - Efforts to increase capacity on existing diagnostic platforms and tap into existing surveillance platforms","32594ea2":"### Subquestion C - Recruitment, support, and coordination of local expertise and capacity (public, private\u2014commercial, and non-profit, including academic), including legal, ethical, communications, and operational issues","62cf8907":"### get results by score","141aac4c":"Define function to remove all signs and symbols which should not be present in the text anymore\nset complete text to lower case","13623c2d":"### Subquestion J - Use of diagnostics such as host response markers (e.g., cytokines) to detect early disease or predict severe disease progression, which would be important to understanding best clinical practice and efficacy of therapeutic interventions","8dfaf062":"### Unzip Kaggle Datafile","def21298":"### FINAL TOP MATCHES","a95ad1df":"# Method 3","36ebedcc":"### Build LDA model","27a56033":"### Defining SubQuestion","366ef347":"### Predicting topics on an unseen document is also doable, as shown below:","aace0436":"### Check all articles for similarities","00a35e3c":"### Select the BEST model ","d65f6c6e":"0 Article with most matches:\nPaper-ID = 590e55ca7451d6a3aad7b2d84a04b39bc864dbe7 ...\nTITLE= college public health health informatics king saud bin abdulaziz unive ...\n\nABSTRACT SUMMARY:  Rapid immunochromatographic assays for detecting infections with bovine coronavirus (BCV), rotavirus A and Cryptosporidium parvum in calf faeces were evaluated using as gold standards a reverse transcriptase polymerase chain reaction (BCV and rotavirus) and a sedimentation-flotation technique (C.\n---------------------------------------------------","b2998c66":"## 3a)  Data Load and pre-processing","276932cc":"## Solution","fe240b3c":"## 3b) Data Cleaning & Data Pre-Processing","2499d699":"### adding search terms for subquestion A ","8f2681f2":"- Thomas Zaugg, Team Lead    (e-mail: thomas.zaugg.tz1@roche.com, Kaggle-ID: tz2018)  \n- Monica Ge                  (e-mail: monica.ge@roche.com, Kaggle-ID:monica.ge@roche.com)\n- Dominique Schaad           (e-mail: dominique.schaad@roche.com, Kaggle-ID:schaadd2)\n- Patrick Guggenb\u00fchl         (e-mail: patrick.guggenbuehl@roche.com, Kaggle-ID:guggi222)\n- Emina Atlagic              (e-mail: emina.atlagic@roche.com, Kaggle-ID:eminaa)","786f8859":"Rows, Columns: (25024, 1) -------------------Headers------------------ abstract 0 Abstract\\r\\nA recent outbreak of novel coronavirus (SARS-CoV-2), the causative agent of COVID-19, has spread rapidly all over the world. Human immunodeficiency virus (HIV) is another deadly virus and causes acquired immunodeficiency syndrome (AIDS). Rapid and early detection of these viruses will facilitate early intervention and reduce disease transmission risk. Here, we present an All-In-One Dual CRISPR-Cas12a (termed \"AIOD-CRISPR\") assay method for simple, rapid, ultrasensitive, one-pot, and visual detection of coronavirus SARS-CoV-2 and HIV virus. In our AIOD CRISPR assay, a pair of crRNAs was introduced to initiate dual CRISPR-Cas12a detection and improve detection sensitivity. The AIOD-CRISPR assay system was successfully utilized to detect nucleic acids (DNA and RNA) of SARS-CoV-2 and HIV with a sensitivity of few copies. Also, it was evaluated by detecting HIV-1 RNA extracted from human plasma samples, achieving a comparable sensitivity with real-time RT-PCR method. Thus, our method has a great potential for developing next-generation point-of-care molecular diagnostics. 1 Abstract\\r\\nTraditionally, the emergence of coronaviruses (CoVs) has been attributed to a gain in receptor 2 binding in a new host. Our previous work with SARS-like viruses argued that bats already 3 harbor CoVs with the ability to infect humans without adaptation. These results suggested that 4 additional barriers limit the emergence of zoonotic CoV. In this work, we describe overcoming 5 host restriction of two MERS-like bat CoVs using exogenous protease treatment. We found that 6 the spike protein of PDF2180-CoV, a MERS-like virus found in a Ugandan bat, could mediate 7 infection of Vero and human cells in the presence of exogenous trypsin. We subsequently show 8 that the bat virus spike can mediate infection of human gut cells, but is unable to infect human 9 lung cells. Using receptor-blocking antibodies, we show that infection with the PDF2180 spike 10 does not require MERS-CoV receptor DPP4 and antibodies developed against the MERS spike 11 receptor-binding domain and S2 portion are ineffective in neutralizing the PDF2180 chimera.Finally, we found that addition of exogenous trypsin also rescues replication of HKU5-CoV, a 13 second MERS-like group 2c CoV. Together, these results indicate that proteolytic cleavage of 14 the spike, not receptor binding, is the primary infection barrier for these two group 2c CoVs.Coupled with receptor binding, proteolytic activation offers a new parameter to evaluate 16 emergence potential of CoVs and offer a means to recover previously unrecoverable zoonotic 17 CoV strains.18ImportanceOverall, our studies demonstrate that proteolytic cleavage is the primary barrier to infection for a 20 subset of zoonotic coronaviruses. Moving forward, the results argue that both receptor binding 21 and proteolytic cleavage of the spike are critical factors that must be considered for evaluating 22 the emergence potential and risk posed by zoonotic coronaviruses. In addition, the findings also 23 offer a novel means to recover previously uncultivable zoonotic coronavirus strains and argue 24 that other tissues, including the digestive tract, could be a site for future coronavirus emergence 25 events in humans.","614eff1e":"### Show graph","b76d4bad":"# Method 2","18c09eb1":"### Subquestion A - How widespread current exposure is to be able to make immediate policy recommendations on mitigation measures. Denominators for testing and a mechanism for rapidly sharing that information, including demographics, to the extent possible.","b065ddad":"## Subset data","c71da1eb":"###   Method 1: Retrieval of top 50 articles for a subquestion using NCBI databases and indexed search in Entrez (accessing PubMED data)","bd2e1c2c":"### Predicting topics on SubQuestion","9fbfdb62":"### Text Preparation","77cacba7":"### Subquestion F - Rapid design and execution of targeted surveillance experiments calling for all potential testers using PCR in a defined area to start testing and report to a specific entity. These experiments could aid in collecting longitudinal samples, which are critical to understanding the impact of ad hoc local interventions","aee75190":"Our solution consist of three different methods containing completely different datasources and approaches\n\n- The first method is using an existing, search tool \"Entrez\" for NCBI Databases, which are available for the public\n\n- The second method is focusing on articles, which are published on the WEB          https:\/\/www.medtechdive.com\/topic\/diagnostics\/\n\n- The third method is processing and retrieving data from the Kaggle Data set and applying NLP\n           \n          ","4cc10d6f":"### Subquestion G - Separation of assay development issues from instruments, and the role of the private sector to help quickly migrate assays onto those devices","f6e69aeb":"# Method 1","903e7ccf":"### Subquestion D - National guidance and guidelines about best practices to states (e.g., how states might leverage universities and private laboratories for testing purposes, communications to public health officials and the public","b5880e4f":"\n - Use BioPython to query for terms in NCBIs Databases via Entrez and retrieve articles with MeSH terms and by                relevance\n   https:\/\/www.ncbi.nlm.nih.gov\/Web\/Search\/entrezfs.html\n   (access to NCBI\u2019s databases such as data from PubMed, GenBank, GEO, and many others)\n    A unique feature of the system is its use of precomputed similarity searches for each record to create links to neighbors\" or related records in other Entrez databases. \n - Perform key word extraction using RAKE\n - Display results\n\n   Pros: \n         Search tool already existing\n         feature of the system is its use of precomputed similarity searches \n         The data is already classified and uses a MeSH thesaurus, which makes it easy to find relevant articles\n         The search is fast\n   Cons: \n         The classification or MeSH terms are not available for every article \n         It might be difficult to find the proper terms for the search","9c1e3e7d":"###   Method 2: Webscrapping using articles from https:\/\/www.medtechdive.com\/topic\/diagnostics\/ for data subset and clustering","6a46ff8c":"### Plotting tools","7ae06396":"# Approach"}}