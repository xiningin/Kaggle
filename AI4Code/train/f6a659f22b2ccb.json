{"cell_type":{"396e2915":"code","153fc8ac":"code","64065cf3":"code","4c7d66d9":"code","8c1cf7d9":"code","f52a48fb":"code","1c5ed769":"code","b3916a7f":"code","945848a0":"code","0642ae91":"code","b36084e6":"code","7800e388":"code","ab697392":"code","5f5f7a57":"code","84ada880":"code","3a56233b":"code","beaaed36":"code","66218042":"code","5bdc46a9":"markdown","c6887e1c":"markdown","451dede0":"markdown","214a5f4d":"markdown","65828374":"markdown"},"source":{"396e2915":"import pandas as pd\nfrom sklearn.datasets import load_boston\nboston_data = load_boston()\nboston = pd.DataFrame(boston_data.data, columns=boston_data.feature_names)\n\nboston['MEDV'] = boston_data.target\nboston.head()","153fc8ac":"X = boston.drop(columns=['RAD','MEDV'])\nY = boston['MEDV']","64065cf3":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(Y_train.shape)\nprint(Y_test.shape)","4c7d66d9":"import tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","8c1cf7d9":"model = keras.Sequential([\nlayers.Dense(64, activation='relu', input_shape=[len(X_train.keys())]),\nlayers.Dense(64, activation='relu'),\nlayers.Dense(1)\n])\nmodel.summary()","f52a48fb":"model.compile(loss='mean_squared_error',\n                optimizer='adam', metrics = \"rmse\")","1c5ed769":"early_stop = keras.callbacks.EarlyStopping(monitor='mean_squared_error', patience=10)","b3916a7f":"model.compile(optimizer='adam',\n              loss='mean_squared_error')\nhistory = model.fit(X_train,Y_train,\n                    batch_size=32,\n                    epochs=250,\n                    callbacks= [early_stop],\n                    validation_data=(X_test,Y_test))","945848a0":"import matplotlib.pyplot as plt\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\n#plt.yscale('log')\nplt.show()","0642ae91":"final = model.predict(X_test)","b36084e6":"from sklearn.metrics import mean_squared_error\nimport numpy as np\nfinal = model.predict(X_test)\nrmse = (np.sqrt(mean_squared_error(Y_test, final)))\nprint('RMSE is {}'.format(rmse))","7800e388":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","ab697392":"model = keras.Sequential([\nlayers.Dense(84, activation='relu', input_shape=[len(X.keys())]),\nlayers.Dense(84, activation='relu'),\nlayers.Dense(1)\n])\nmodel.summary()","5f5f7a57":"\nfrom keras import backend\n \ndef rmse(y_true, y_pred):\n\treturn backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))","84ada880":"model.compile(optimizer='adam',\n              loss='mean_squared_error', metrics=[rmse])\nhistory = model.fit(X_train,Y_train,\n                    batch_size=32,\n                    epochs=100,\n                    verbose=0,\n                    callbacks= [early_stop],\n                    validation_data=(X_test,Y_test))","3a56233b":"hist = pd.DataFrame(history.history)\nhist['epoch'] = history.epoch\nhist.tail()","beaaed36":"from sklearn.metrics import mean_squared_error\nimport numpy as np\nfinal = model.predict(X_test)\nrmse = (np.sqrt(mean_squared_error(Y_test, final)))\nprint('RMSE is {}'.format(rmse))","66218042":"import matplotlib.pyplot as plt\n# summarize history for loss\nplt.plot(history.history['rmse'])\nplt.plot(history.history['val_rmse'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\n#plt.yscale('log')\nplt.show()","5bdc46a9":"**The ReLU function is another non-linear activation function that has gained popularity in the deep learning domain. ReLU stands for Rectified Linear Unit. The main advantage of using the ReLU function over other activation functions is that it does not activate all the neurons at the same time.**\n\n**This means that the neurons will only be deactivated if the output of the linear transformation is less than 0. **\n![image.png](attachment:image.png)","c6887e1c":"# Neural networks are multi-layer networks of neurons (the blue and magenta nodes in the chart below) that we use to classify things, make predictions, etc. Below is the diagram of a simple neural network with five inputs, 5 outputs, and two hidden layers of neurons.\n![image.png](attachment:image.png)\nStarting from the left, we have:\n* The input layer of our model in orange.\n* Our first hidden layer of neurons in blue.\n* Our second hidden layer of neurons in magenta.\n* The output layer (a.k.a. the prediction) of our model in green.","451dede0":"![image.png](attachment:image.png)\n* Let\u2019s examine each element:\n* X (in orange) is our input, the lone feature that we give to our model in order to calculate a prediction.\n* \n* \n* B1 (in turquoise, a.k.a. blue-green) is the estimated slope parameter of our logistic regression \u2014 B1 tells us by how much the Log_Odds change as X changes. Notice that B1 lives on the turquoise line, which connects the input X to the blue neuron in Hidden Layer 1.\n* \n* \n* B0 (in blue) is the bias \u2014 very similar to the intercept term from regression. The key difference is that in neural networks, every neuron has its own bias term (while in regression, the model has a singular intercept term).\n* \n* \n* The blue neuron also includes a sigmoid activation function (denoted by the curved line inside the blue circle). Remember the sigmoid function is what we use to go from log-odds to probability (do a control-f search for \u201csigmoid\u201d in my previous post).\n* \n* \n* \n* And finally we get our predicted probability by applying the sigmoid function to the quantity (B1*X + B0).","214a5f4d":"**Leaky ReLU function is nothing but an improved version of the ReLU function. As we saw that for the ReLU function, the gradient is 0 for x<0, which would deactivate the neurons in that region.**\n\n**Leaky ReLU is defined to address this problem. Instead of defining the Relu function as 0 for negative values of x, we define it as an extremely small linear component of x. Here is the mathematical expression-**\n![image.png](attachment:image.png)","65828374":"A neuron that includes a bias term (B0) and an activation function (sigmoid in our case).\n![image.png](attachment:image.png)"}}