{"cell_type":{"d0290db4":"code","7bda6f2d":"code","b137bbae":"code","b6453fbf":"code","94b15947":"code","ccd8fd6a":"code","890b4f6d":"code","69f70336":"code","4d7187f6":"code","ffd27747":"code","1bb5322f":"code","6e1e86cf":"code","7791af77":"code","102dca2d":"code","065afafb":"code","d7a7066d":"code","10863961":"code","68770f7e":"code","ac78777b":"code","2c8ce874":"code","e155b178":"code","2f876f35":"code","c903467c":"code","1ad09156":"code","2b6103b6":"code","c6dbb93c":"code","497d6f0f":"code","ec5d87a6":"code","4080fdba":"code","a49dde55":"code","998589c9":"code","5797ef3e":"code","432e8174":"code","f39378bd":"code","af17b961":"code","5aa6f107":"code","9eb64457":"code","a570c751":"code","532cf998":"code","9ba1c360":"code","fe5b20a1":"code","eb8bb219":"code","98ed7e65":"code","5d15708b":"code","7a44acdd":"code","b3a9f7e8":"code","54e92cc2":"code","f27cdac1":"code","b5992bcd":"code","aa958c65":"code","3e9caba5":"markdown","106ab705":"markdown","b308af7d":"markdown","81724c61":"markdown","9fe6cc1c":"markdown","19553583":"markdown","00729a1d":"markdown","f3ecc324":"markdown","b042bfbf":"markdown","ad4f5845":"markdown","5699acce":"markdown","c56df698":"markdown","e70d1152":"markdown","58ead0c9":"markdown","c125b6a7":"markdown","48232c37":"markdown","784bf3b1":"markdown","553c9b3c":"markdown","b5569d98":"markdown","c95144c5":"markdown","31bf2c8c":"markdown","0f970fb5":"markdown","f43c578a":"markdown","12d62f17":"markdown","5b2c12d7":"markdown","f13b37c7":"markdown"},"source":{"d0290db4":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler","7bda6f2d":"data = pd.read_csv('..\/input\/lung-cancer-dataset\/lung_cancer_examples.csv')\nprint('Dataset :',data.shape)\ndata.info()\ndata[0:10]","b137bbae":"# Distribution of diagnosis\ndata.Result.value_counts()[0:30].plot(kind='bar')\nplt.show()","b6453fbf":"sns.set_style(\"whitegrid\")\nsns.pairplot(data,hue=\"Result\",size=3);\nplt.show()","94b15947":"data1 = data.drop(columns=['Name','Surname'],\n\n                 axis=1)\ndata1 = data1.dropna(how='any')\nprint(data1.shape)","ccd8fd6a":"print(data1.shape)\ndata1.head()","890b4f6d":"from sklearn.model_selection import train_test_split\nY = data1['Result']\nX = data1.drop(columns=['Result'])\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=9)","69f70336":"print('X train shape: ', X_train.shape)\nprint('Y train shape: ', Y_train.shape)\nprint('X test shape: ', X_test.shape)\nprint('Y test shape: ', Y_test.shape)","4d7187f6":"from sklearn.linear_model import LogisticRegression\n\n# We defining the model\nlogreg = LogisticRegression(C=10)\n\n# We train the model\nlogreg.fit(X_train, Y_train)\n\n# We predict target values\nY_predict1 = logreg.predict(X_test)","ffd27747":"# The confusion matrix\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\nlogreg_cm = confusion_matrix(Y_test, Y_predict1)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(logreg_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='g', ax=ax, cmap=\"YlGnBu\")\nplt.title('Logistic Regression Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","1bb5322f":"# Test score\nscore_logreg = logreg.score(X_test, Y_test)\nprint(score_logreg)","6e1e86cf":"from sklearn.ensemble import BaggingClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\n\n# We define the SVM model\nsvmcla = OneVsRestClassifier(BaggingClassifier(SVC(C=10,kernel='rbf',random_state=9, probability=True), \n                                               n_jobs=-1))\n\n# We train model\nsvmcla.fit(X_train, Y_train)\n\n# We predict target values\nY_predict2 = svmcla.predict(X_test)","7791af77":"# The confusion matrix\nsvmcla_cm = confusion_matrix(Y_test, Y_predict2)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(svmcla_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='g', ax=ax, cmap=\"YlGnBu\")\nplt.title('SVM Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","102dca2d":"# Test score\nscore_svmcla = svmcla.score(X_test, Y_test)\nprint(score_svmcla)","065afafb":"from sklearn.naive_bayes import GaussianNB\n\n# We define the model\nnbcla = GaussianNB()\n\n# We train model\nnbcla.fit(X_train, Y_train)\n\n# We predict target values\nY_predict3 = nbcla.predict(X_test)","d7a7066d":"# The confusion matrix\nnbcla_cm = confusion_matrix(Y_test, Y_predict3)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(nbcla_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='g', ax=ax, cmap=\"YlGnBu\")\nplt.title('Naive Bayes Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","10863961":"# Test score\nscore_nbcla = nbcla.score(X_test, Y_test)\nprint(score_nbcla)","68770f7e":"from sklearn.tree import DecisionTreeClassifier\n\n# We define the model\ndtcla = DecisionTreeClassifier(random_state=9)\n\n# We train model\ndtcla.fit(X_train, Y_train)\n\n# We predict target values\nY_predict4 = dtcla.predict(X_test)","ac78777b":"# The confusion matrix\ndtcla_cm = confusion_matrix(Y_test, Y_predict4)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(dtcla_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='g', ax=ax, cmap=\"YlGnBu\")\nplt.title('Decision Tree Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","2c8ce874":"# Test score\nscore_dtcla = dtcla.score(X_test, Y_test)\nprint(score_dtcla)","e155b178":"from sklearn.ensemble import RandomForestClassifier\n\n# We define the model\nrfcla = RandomForestClassifier(n_estimators=100,random_state=9,n_jobs=-1)\n\n# We train model\nrfcla.fit(X_train, Y_train)\n\n# We predict target values\nY_predict5 = rfcla.predict(X_test)","2f876f35":"# The confusion matrix\nrfcla_cm = confusion_matrix(Y_test, Y_predict5)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(rfcla_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='g', ax=ax, cmap=\"YlGnBu\")\nplt.title('Random Forest Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","c903467c":"# Test score\nscore_rfcla = rfcla.score(X_test, Y_test)\nprint(score_rfcla)","1ad09156":"from sklearn.neighbors import KNeighborsClassifier\n\n# We define the model\nknncla = KNeighborsClassifier(n_neighbors=5,n_jobs=-1)\n\n# We train model\nknncla.fit(X_train, Y_train)\n\n# We predict target values\nY_predict6 = knncla.predict(X_test)","2b6103b6":"# The confusion matrix\nknncla_cm = confusion_matrix(Y_test, Y_predict6)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(knncla_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='g', ax=ax, cmap=\"YlGnBu\")\nplt.title('KNN Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","c6dbb93c":"# Test score\nscore_knncla= knncla.score(X_test, Y_test)\nprint(score_knncla)","497d6f0f":"Testscores = pd.Series([score_logreg, score_svmcla, score_nbcla, score_dtcla, score_rfcla, score_knncla], \n                        index=['Logistic Regression Score', 'Support Vector Machine Score', 'Naive Bayes Score', 'Decision Tree Score', 'Random Forest Score', 'K-Nearest Neighbour Score']) \nprint(Testscores)","ec5d87a6":"fig = plt.figure(figsize=(15,15))\nax1 = fig.add_subplot(3, 3, 1) \nax1.set_title('Logistic Regression Classification') \nax2 = fig.add_subplot(3, 3, 2) \nax2.set_title('SVM Classification')\nax3 = fig.add_subplot(3, 3, 3)\nax3.set_title('Naive Bayes Classification')\nax4 = fig.add_subplot(3, 3, 4)\nax4.set_title('Decision Tree Classification')\nax5 = fig.add_subplot(3, 3, 5)\nax5.set_title('Random Forest Classification')\nax6 = fig.add_subplot(3, 3, 6)\nax6.set_title('KNN Classification')\nsns.heatmap(data=logreg_cm, annot=True, linewidth=0.7, linecolor='cyan',cmap=\"YlGnBu\" ,fmt='g', ax=ax1)\nsns.heatmap(data=svmcla_cm, annot=True, linewidth=0.7, linecolor='cyan',cmap=\"YlGnBu\" ,fmt='g', ax=ax2)  \nsns.heatmap(data=nbcla_cm, annot=True, linewidth=0.7, linecolor='cyan',cmap=\"YlGnBu\" ,fmt='g', ax=ax3)\nsns.heatmap(data=dtcla_cm, annot=True, linewidth=0.7, linecolor='cyan',cmap=\"YlGnBu\" ,fmt='g', ax=ax4)\nsns.heatmap(data=rfcla_cm, annot=True, linewidth=0.7, linecolor='cyan',cmap=\"YlGnBu\" ,fmt='g', ax=ax5)\nsns.heatmap(data=knncla_cm, annot=True, linewidth=0.7, linecolor='cyan',cmap=\"YlGnBu\" ,fmt='g', ax=ax6)\nplt.show()","4080fdba":"from sklearn.metrics import roc_curve\n\n# Logistic Regression Classification\nY_predict1_proba = logreg.predict_proba(X_test)\nY_predict1_proba = Y_predict1_proba[:, 1]\nfpr, tpr, thresholds = roc_curve(Y_test, Y_predict1_proba)\nplt.subplot(331)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC Curve Logistic Regression')\nplt.grid(True)\n\n# SVM Classification\nY_predict2_proba = svmcla.predict_proba(X_test)\nY_predict2_proba = Y_predict2_proba[:, 1]\nfpr, tpr, thresholds = roc_curve(Y_test, Y_predict2_proba)\nplt.subplot(332)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC Curve SVM')\nplt.grid(True)\n\n# Naive Bayes Classification\nY_predict3_proba = nbcla.predict_proba(X_test)\nY_predict3_proba = Y_predict3_proba[:, 1]\nfpr, tpr, thresholds = roc_curve(Y_test, Y_predict3_proba)\nplt.subplot(333)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC Curve Naive Bayes')\nplt.grid(True)\n\n# Decision Tree Classification\nY_predict4_proba = dtcla.predict_proba(X_test)\nY_predict4_proba = Y_predict4_proba[:, 1]\nfpr, tpr, thresholds = roc_curve(Y_test, Y_predict4_proba)\nplt.subplot(334)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC Curve Decision Tree')\nplt.grid(True)\n\n# Random Forest Classification\nY_predict5_proba = rfcla.predict_proba(X_test)\nY_predict5_proba = Y_predict5_proba[:, 1]\nfpr, tpr, thresholds = roc_curve(Y_test, Y_predict5_proba)\nplt.subplot(335)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC Curve Random Forest')\nplt.grid(True)\n\n# KNN Classification\nY_predict6_proba = knncla.predict_proba(X_test)\nY_predict6_proba = Y_predict6_proba[:, 1]\nfpr, tpr, thresholds = roc_curve(Y_test, Y_predict6_proba)\nplt.subplot(336)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC Curve KNN')\nplt.grid(True)\nplt.subplots_adjust(top=2, bottom=0.08, left=0.10, right=1.4, hspace=0.45, wspace=0.45)\nplt.show()","a49dde55":"Y1 = data1['Result']\nX1 = data1.drop(columns=['Age'])\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectFromModel\n\nlsvc = LinearSVC(C=0.06, penalty=\"l1\", dual=False,random_state=10).fit(X1, Y1)\nmodel = SelectFromModel(lsvc, prefit=True)\nX_new = model.transform(X1)\ncc = list(X1.columns[model.get_support(indices=True)])\nprint(cc)\nprint(len(cc))","998589c9":"# Principal component analysis\nfrom sklearn.decomposition import PCA\n\npca = PCA().fit(X1)\nplt.figure()\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Smokes AreaQ Alkhol Result')\nplt.ylabel('% Variance Explained')\nplt.title('PCA Analysis')\nplt.grid(True)\nplt.show()","5797ef3e":"# Percentage of total variance explained\nvariance = pd.Series(list(np.cumsum(pca.explained_variance_ratio_)), \n                        index= list(range(1,5))) \nprint(variance[10:90])","432e8174":"X1 = data1[cc] \nfrom sklearn.model_selection import train_test_split\nX1_train, X1_test, Y1_train, Y1_test = train_test_split(X1, Y1, test_size=0.1, random_state=10)","f39378bd":"# Logistic regression classification\nlogreg.fit(X1_train, Y1_train)\nY1_predict1 = logreg.predict(X1_test)\nlogreg_cm = confusion_matrix(Y1_test, Y1_predict1)\nscore1_logreg = logreg.score(X1_test, Y1_test)\n\n# SVM classification\nsvmcla.fit(X1_train, Y1_train)\nY1_predict2 = svmcla.predict(X1_test)\nsvmcla_cm = confusion_matrix(Y1_test, Y1_predict2)\nscore1_svmcla = svmcla.score(X1_test, Y1_test)\n\n# Random forest classification\nrfcla.fit(X1_train, Y1_train)\nY1_predict5 = rfcla.predict(X1_test)\nrfcla_cm = confusion_matrix(Y1_test, Y1_predict5)\nscore1_rfcla = rfcla.score(X1_test, Y1_test)","af17b961":"Testscores1 = pd.Series([score1_logreg, score1_svmcla, score1_rfcla], index=['Logistic Regression Score', \n                        'Support Vector Machine Score', 'Random Forest Score']) \nprint(Testscores1)","5aa6f107":"fig = plt.figure(figsize=(15,15))\nax1 = fig.add_subplot(3, 3, 1) \nax1.set_title('Logistic Regression Classification') \nax2 = fig.add_subplot(3, 3, 2) \nax2.set_title('SVM Classification')\nax5 = fig.add_subplot(3, 3,3)\nax5.set_title('Random Forest Classification')\n\nsns.heatmap(data=logreg_cm, annot=True, linewidth=0.7, linecolor='cyan',cmap=\"YlGnBu\" ,fmt='g', ax=ax1)\nsns.heatmap(data=svmcla_cm, annot=True, linewidth=0.7, linecolor='cyan',cmap=\"YlGnBu\" ,fmt='g', ax=ax2)  \nsns.heatmap(data=rfcla_cm, annot=True, linewidth=0.7, linecolor='cyan',cmap=\"YlGnBu\" ,fmt='g', ax=ax5)\nplt.show()","9eb64457":"Y1 = data1['Result']\nX1 = data1.drop(columns=['AreaQ'])\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectFromModel\n\nlsvc = LinearSVC(C=0.06, penalty=\"l1\", dual=False,random_state=10).fit(X1, Y1)\nmodel = SelectFromModel(lsvc, prefit=True)\nX_new = model.transform(X1)\ncc = list(X1.columns[model.get_support(indices=True)])\nprint(cc)\nprint(len(cc))","a570c751":"# Principal component analysis\nfrom sklearn.decomposition import PCA\n\npca = PCA().fit(X1)\nplt.figure()\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Age Smokes Alkhol Result')\nplt.ylabel('% Variance Explained')\nplt.title('PCA Analysis')\nplt.grid(True)\nplt.show()","532cf998":"# Percentage of total variance explained\nvariance = pd.Series(list(np.cumsum(pca.explained_variance_ratio_)), \n                        index= list(range(1,5))) \nprint(variance[10:90])","9ba1c360":"X1 = data1[cc] \nfrom sklearn.model_selection import train_test_split\nX1_train, X1_test, Y1_train, Y1_test = train_test_split(X1, Y1, test_size=0.1, random_state=10)","fe5b20a1":"# Logistic regression classification\nlogreg.fit(X1_train, Y1_train)\nY1_predict1 = logreg.predict(X1_test)\nlogreg_cm = confusion_matrix(Y1_test, Y1_predict1)\nscore1_logreg = logreg.score(X1_test, Y1_test)\n\n# SVM classification\nsvmcla.fit(X1_train, Y1_train)\nY1_predict2 = svmcla.predict(X1_test)\nsvmcla_cm = confusion_matrix(Y1_test, Y1_predict2)\nscore1_svmcla = svmcla.score(X1_test, Y1_test)\n\n# Random forest classification\nrfcla.fit(X1_train, Y1_train)\nY1_predict5 = rfcla.predict(X1_test)\nrfcla_cm = confusion_matrix(Y1_test, Y1_predict5)\nscore1_rfcla = rfcla.score(X1_test, Y1_test)","eb8bb219":"Testscores1 = pd.Series([score1_logreg, score1_svmcla, score1_rfcla], index=['Logistic Regression Score', \n                        'Support Vector Machine Score', 'Random Forest Score']) \nprint(Testscores1)","98ed7e65":"fig = plt.figure(figsize=(15,15))\nax1 = fig.add_subplot(3, 3, 1) \nax1.set_title('Logistic Regression Classification') \nax2 = fig.add_subplot(3, 3, 2) \nax2.set_title('SVM Classification')\nax5 = fig.add_subplot(3, 3,3)\nax5.set_title('Random Forest Classification')\nsns.heatmap(data=logreg_cm, annot=True, linewidth=0.7, linecolor='cyan',cmap=\"YlGnBu\" ,fmt='g', ax=ax1)\nsns.heatmap(data=svmcla_cm, annot=True, linewidth=0.7, linecolor='cyan',cmap=\"YlGnBu\" ,fmt='g', ax=ax2)  \nsns.heatmap(data=rfcla_cm, annot=True, linewidth=0.7, linecolor='cyan',cmap=\"YlGnBu\" ,fmt='g', ax=ax5)\nplt.show()","5d15708b":"Y1 = data1['Result']\nX1 = data1.drop(columns=['Alkhol'])\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectFromModel\n\nlsvc = LinearSVC(C=0.06, penalty=\"l1\", dual=False,random_state=10).fit(X1, Y1)\nmodel = SelectFromModel(lsvc, prefit=True)\nX_new = model.transform(X1)\ncc = list(X1.columns[model.get_support(indices=True)])\nprint(cc)\nprint(len(cc))","7a44acdd":"# Principal component analysis\nfrom sklearn.decomposition import PCA\n\npca = PCA().fit(X1)\nplt.figure()\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Age Smokes AreaQ Result')\nplt.ylabel('% Variance Explained')\nplt.title('PCA Analysis')\nplt.grid(True)\nplt.show()","b3a9f7e8":"# Percentage of total variance explained\nvariance = pd.Series(list(np.cumsum(pca.explained_variance_ratio_)), \n                        index= list(range(1,5))) \nprint(variance[10:90])","54e92cc2":"X1 = data1[cc] \nfrom sklearn.model_selection import train_test_split\nX1_train, X1_test, Y1_train, Y1_test = train_test_split(X1, Y1, test_size=0.1, random_state=10)","f27cdac1":"# Logistic regression classification\nlogreg.fit(X1_train, Y1_train)\nY1_predict1 = logreg.predict(X1_test)\nlogreg_cm = confusion_matrix(Y1_test, Y1_predict1)\nscore1_logreg = logreg.score(X1_test, Y1_test)\n\n# SVM classification\nsvmcla.fit(X1_train, Y1_train)\nY1_predict2 = svmcla.predict(X1_test)\nsvmcla_cm = confusion_matrix(Y1_test, Y1_predict2)\nscore1_svmcla = svmcla.score(X1_test, Y1_test)\n\n# Random forest classification\nrfcla.fit(X1_train, Y1_train)\nY1_predict5 = rfcla.predict(X1_test)\nrfcla_cm = confusion_matrix(Y1_test, Y1_predict5)\nscore1_rfcla = rfcla.score(X1_test, Y1_test)","b5992bcd":"Testscores1 = pd.Series([score1_logreg, score1_svmcla, score1_rfcla], index=['Logistic Regression Score', \n                        'Support Vector Machine Score', 'Random Forest Score']) \nprint(Testscores1)","aa958c65":"fig = plt.figure(figsize=(15,15))\nax1 = fig.add_subplot(3, 3, 1) \nax1.set_title('Logistic Regression Classification') \nax2 = fig.add_subplot(3, 3, 2) \nax2.set_title('SVM Classification')\nax5 = fig.add_subplot(3, 3,3)\nax5.set_title('Random Forest Classification')\nsns.heatmap(data=logreg_cm, annot=True, linewidth=0.7, linecolor='cyan',cmap=\"YlGnBu\" ,fmt='g', ax=ax1)\nsns.heatmap(data=svmcla_cm, annot=True, linewidth=0.7, linecolor='cyan',cmap=\"YlGnBu\" ,fmt='g', ax=ax2)  \nsns.heatmap(data=rfcla_cm, annot=True, linewidth=0.7, linecolor='cyan',cmap=\"YlGnBu\" ,fmt='g', ax=ax5)\nplt.show()","3e9caba5":"Lung cancer, also known as lung carcinoma, is a malignant lung tumor characterized by uncontrolled cell growth in tissues of the lung. This growth can spread beyond the lung by the process of metastasis into nearby tissue or other parts of the body.","106ab705":"## 5. Random forest classification\n\nBased on the previous classification method, random forest is a supervised learning algorithm that creates a forest randomly. This forest, is a set of decision trees, most of the times trained with the bagging method. The essential idea of bagging is to average many noisy but approximately impartial models, and therefore reduce the variation. Each tree is constructed using the following algorithm:\n\n* Let $N$ be the number of test cases, $M$ is the number of variables in the classifier.\n* Let $m$ be the number of input variables to be used to determine the decision in a given node; $m<M$.\n* Choose a training set for this tree and use the rest of the test cases to estimate the error.\n* For each node of the tree, randomly choose $m$ variables on which to base the decision. Calculate the best partition of the training set from the $m$ variables.\n\nFor prediction a new case is pushed down the tree. Then it is assigned the label of the terminal node where it ends. This process is iterated by all the trees in the assembly, and the label that gets the most incidents is reported as the prediction. We define the number of trees in the forest in 100. ","b308af7d":"## 2. SVM (Support Vector Machine) classification\n\nSVMs (Support Vector Machine) have shown a rapid proliferation during the last years. The learning problem setting for SVMs corresponds to a some unknown and nonlinear dependency (mapping, function) $y = f(x)$ between some high-dimensional input vector $x$ and scalar output $y$. It is noteworthy that there is no information on the joint probability functions, therefore, a free distribution learning must be carried out. The only information available is a training data set $D = {(x_i, y_i) \u2208 X\u00d7Y }, i = 1$, $l$, where $l$ stands for the number of the training data pairs and is therefore equal to the size of the training data set $D$, additionally, $y_i$ is denoted as $d_i$, where $d$ stands for a desired (target) value. Hence, SVMs belong to the supervised learning techniques.\n\nFrom the classification approach, the goal of SVM is to find a hyperplane in an N-dimensional space that clearly classifies the data points. Thus hyperplanes are decision boundaries that help classify the data points. Data points falling on either side of the hyperplane can be attributed to different classes.\n\n\n\n","81724c61":"## 3. Naive bayes classification\n\nThe naive Bayesian classifier is a probabilistic classifier based on Bayes' theorem with strong independence assumptions between the features. Thus, using Bayes theorem $\\left(P(X|Y)=\\frac{P(Y|X)P(X)}{P(Y)}\\right)$, we can find the probability of $X$ happening, given that $Y$ has occurred. Here, $Y$ is the evidence and $X$ is the hypothesis. The assumption made here is that the presence of one particular feature does not affect the other (the predictors\/features are independent). Hence it is called naive. In this case we will assume that we assume the values are sampled from a Gaussian distribution and therefore we consider a Gaussian Naive Bayes.","9fe6cc1c":"**VISUALIZING THE DATA\n**","19553583":"## 6. K-Nearest Neighbor classification\n\nK-Nearest neighbors is a technique that stores all available cases and **classifies new cases based on a similarity measure (e.g., distance functions)**. This technique is non-parametric since there are no assumptions for the distribution of underlying data and it is lazy since it does not need any training data point model generation. All the training data used in the test phase. **This makes the training faster and the test phase slower and more costlier. In this technique, the number of neighbors k is usually an odd number if the number of classes is 2**. For finding closest similar points,  find the distance between points using distance measures such as Euclidean distance, Hamming distance, Manhattan distance and Minkowski distance.\n\n","00729a1d":"Features Selection","f3ecc324":"### ROC curve","b042bfbf":"![](http:\/\/)2. We drop AreaQ from data1. We use features :Age, Smokes, Alkhol","ad4f5845":"Import Library","5699acce":"The classification is performed using the techniques described above, where the only thing that changes is the training and testing data.","c56df698":"## Data\n\n","e70d1152":"### Test score","58ead0c9":"### The confusion matrix","c125b6a7":"3. We drop Alkhol from data1. We use features :Age, Smokes, AreaQ ","48232c37":"## 1. Logistic regression classification\n\nLogistic regression is a technique that can be applied to binary classification problems. This technique uses the logistic function or sigmoid function, which is an S-shaped curve that can assume any real value number and assign it to a value between 0 and 1, but never exactly in those limits. Thus, logistic regression models the probability of the default class (the probability that an input $(X)$ belongs to the default class $(Y=1)$) $(P(X)=P(Y=1|X))$. In order to make the prediction of the probability, the logistic function is used, which allows us to obtain the log-odds or the probit. Thus, the model is a linear combination of the inputs, but that this linear combination relates to the log-odds of the default class.\n\nStarted from make an instance of the model setting the default values. Specify the inverse of the regularization strength in 10. Trained the logistic regression model with the training data, and then applied such model to the test data.","784bf3b1":"1. In here we drop Age from data1. We use features :Smokes, AreaQ, Alkhol","553c9b3c":"The classification is performed using the techniques described above, where the only thing that changes is the training and testing data.","b5569d98":"## 4. Decision tree classification\n\nA decision tree is a flowchart-like tree structure where an internal node represents feature, the branch represents a decision rule, and each leaf node represents the outcome. The decision tree analyzes a set of data to construct a set of rules or questions, which are used to predict a class, i.e., the goal of decision tree is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. In this sense the decision tree selects the best attribute using to divide the records, converting that attribute into a decision node and dividing the data set into smaller subsets, to finally start the construction of the tree repeating this process recursively. ","c95144c5":"Comparison of classification techniques\u00b6","31bf2c8c":"In this study, we tried to predict Lung Cancer  using 6 different algorithm:\n1. Logistic regression classification\n2. SVM (Support Vector Machine) classification\n3. Naive bayes classification\n4. Decision tree classification\n5. Random forest classification\n6. K-Nearest Neighbor classification\n\nPredictor variable use in classifying lung cancer:\n1. Age      \n2. Smokes  \n3. AreaQ    \n4. Alkhol\n\n\n","0f970fb5":"### Data for training and testing\n\nTo select a set of training data that will be input in the Machine Learning algorithm, to ensure that the classification algorithm training can be generalized well to new data. For this study using a sample size of 10%, assumed it ideal ratio between training and testing","f43c578a":"![https:\/\/images.medicinenet.com\/images\/image_collection\/anatomy\/lung-cancer.jpg](https:\/\/images.medicinenet.com\/images\/image_collection\/anatomy\/lung-cancer.jpg)","12d62f17":"****## Comparison of classification techniques","5b2c12d7":"The classification is performed using the techniques described above, where the only thing that changes is the training and testing data.","f13b37c7":"Eliminate irrelevant variables in analysis such as name, surname"}}