{"cell_type":{"59f52f65":"code","2eff73fa":"code","1dba0205":"code","431c763b":"code","7f8ea627":"code","e8aaace0":"code","daba53f3":"code","e8cb6879":"code","e0744dde":"code","d819f3fb":"code","86b52ac9":"code","89d32859":"code","88ef4768":"code","d7a81f21":"code","c35de212":"code","77846128":"code","a1547b52":"markdown"},"source":{"59f52f65":"#TF 1.xx is needed, 2.xx not support\n!pip uninstall -y tensorflow\n!pip install tensorflow-gpu==1.15","2eff73fa":"#Check if tf is ok\nimport tensorflow as tf \n\nprint(tf.__version__)\n\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsess = tf.Session(config=config)\n\na = tf.constant(1) \nb = tf.constant(2) \n\nprint(sess.run(a+b)) ","1dba0205":"!conda install -y gdown","431c763b":"%cd \/kaggle\/working\n%pwd\n\n#DL pretrained models\n!gdown https:\/\/drive.google.com\/uc?id=1VoyW4d0e6iTqhtYr2PcoZNOkPIfint5r\n!gdown https:\/\/drive.google.com\/uc?id=1eVHcOdi8HZi-6QO43b_A7OSgGtVOTOm6\n!gdown https:\/\/drive.google.com\/uc?id=1dtGZ4hvr-WYt8R75U94kR50jc_sidMe3\n!gdown https:\/\/drive.google.com\/uc?id=1IZbZPfbxg7jjOdxNb_AGU2WoIl9Ayw_H\n!gdown https:\/\/drive.google.com\/uc?id=1Kxen0y7qKRFXfkaMHMJCHeDitLdgsbzh\n#DL CUDA 10.0 libs\n!gdown https:\/\/drive.google.com\/uc?id=1-1EAv6wt1z0BQjZiw_PnezXkvuSXPrO8\n!gdown https:\/\/drive.google.com\/uc?id=1fI0a-QtuugLpfMZUVZZw1bt0mJXeILp6\n!gdown https:\/\/drive.google.com\/uc?id=1-2_yPVMMbnFdJw_qQ5Z-FtWlDhSbPKd-\n!gdown https:\/\/drive.google.com\/uc?id=1-4FEExV7WXr-NSX6jX6gj5fjweE7Oe3K\n!gdown https:\/\/drive.google.com\/uc?id=1-LGr92Wq2_9KZv2xF4bmrRRIjEFS2Q58\n!gdown https:\/\/drive.google.com\/uc?id=1-MKOSamwVvj0Upi-tToruRPSHYB3z8Yb","7f8ea627":"#TF 1.5.1 need these libs\n!cp -f \/kaggle\/working\/libcudart.so.10.0 \/opt\/conda\/lib\/libcudart.so.10.0\n!cp -f \/kaggle\/working\/libcublas.so.10.0 \/opt\/conda\/lib\/libcublas.so.10.0\n!cp -f \/kaggle\/working\/libcufft.so.10.0 \/opt\/conda\/lib\/libcufft.so.10.0\n!cp -f \/kaggle\/working\/libcurand.so.10.0 \/opt\/conda\/lib\/libcurand.so.10.0\n!cp -f \/kaggle\/working\/libcusolver.so.10.0 \/opt\/conda\/lib\/libcusolver.so.10.0\n!cp -f \/kaggle\/working\/libcusparse.so.10.0 \/opt\/conda\/lib\/libcusparse.so.10.0\n\n!ldconfig","e8aaace0":"!pip install moviepy\n!pip install numpy==1.15.0\n!pip install matplotlib==2.2.2\n!pip install scipy==1.1.0\n!pip install keras==2.3.1\n!pip install imutils\n","daba53f3":"#Clone Git repository and get prepared\n# %tensorflow_version 1.x\n\nimport os\nimport cv2\nimport math\nimport pickle\nimport imageio\nimport warnings\nimport PIL.Image\nimport numpy as np\nfrom PIL import Image\nimport tensorflow as tf\nfrom random import randrange\nimport moviepy.editor as mpy\n#from google.colab import drive\n#from google.colab import files\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\nfrom moviepy.video.io.ffmpeg_writer import FFMPEG_VideoWriter\n# %matplotlib inline\nwarnings.filterwarnings(\"ignore\")\n\ndef get_watermarked(pil_image: Image) -> Image:\n  try:\n    image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)\n    (h, w) = image.shape[:2]\n    image = np.dstack([image, np.ones((h, w), dtype=\"uint8\") * 255])\n    pct = 0.08\n    full_watermark = cv2.imread('\/kaggle\/working\/BabyGAN\/media\/logo.png', cv2.IMREAD_UNCHANGED)\n    (fwH, fwW) = full_watermark.shape[:2]\n    wH = int(pct * h*2)\n    wW = int((wH * fwW) \/ fwH*0.1)\n    watermark = cv2.resize(full_watermark, (wH, wW), interpolation=cv2.INTER_AREA)\n    overlay = np.zeros((h, w, 4), dtype=\"uint8\")\n    (wH, wW) = watermark.shape[:2]\n    overlay[h - wH - 10 : h - 10, 10 : 10 + wW] = watermark\n    output = image.copy()\n    cv2.addWeighted(overlay, 0.5, output, 1.0, 0, output)\n    rgb_image = cv2.cvtColor(output, cv2.COLOR_BGR2RGB)\n    return Image.fromarray(rgb_image)\n  except: return pil_image\n\ndef generate_final_images(latent_vector, direction, coeffs, i):\n    new_latent_vector = latent_vector.copy()\n    new_latent_vector[:8] = (latent_vector + coeffs*direction)[:8]\n    new_latent_vector = new_latent_vector.reshape((1, 18, 512))\n    generator.set_dlatents(new_latent_vector)\n    img_array = generator.generate_images()[0]\n    img = PIL.Image.fromarray(img_array, 'RGB')\n    #if size[0] >= 2048: img = get_watermarked(img)\n    img_path = \"\/kaggle\/working\/BabyGAN\/for_animation\/\" + str(i) + \".png\"\n    img.thumbnail(animation_size, PIL.Image.ANTIALIAS)\n    img.save(img_path)\n    face_img.append(imageio.imread(img_path))\n    clear_output()\n    return img\n\ndef generate_final_image(latent_vector, direction, coeffs):\n    new_latent_vector = latent_vector.copy()\n    new_latent_vector[:8] = (latent_vector + coeffs*direction)[:8]\n    new_latent_vector = new_latent_vector.reshape((1, 18, 512))\n    generator.set_dlatents(new_latent_vector)\n    img_array = generator.generate_images()[0]\n    img = PIL.Image.fromarray(img_array, 'RGB')\n    #if size[0] >= 2048: img = get_watermarked(img)\n    img.thumbnail(size, PIL.Image.ANTIALIAS)\n    img.save(\"face.png\")\n    if download_image == True: files.download(\"face.png\")\n    return img\n\ndef plot_three_images(imgB, fs = 10):\n  f, axarr = plt.subplots(1,3, figsize=(fs,fs))\n  axarr[0].imshow(Image.open('\/kaggle\/working\/BabyGAN\/aligned_images\/father_01.png'))\n  axarr[0].title.set_text(\"Dad\")\n  axarr[1].imshow(imgB)\n  axarr[1].title.set_text(\"Child\")\n  axarr[2].imshow(Image.open('\/kaggle\/working\/BabyGAN\/aligned_images\/mother_01.png'))\n  axarr[2].title.set_text(\"Mom\")\n  plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])\n  plt.show()\n\ndef plot_two_images(imgB, fs = 10):\n  f, axarr = plt.subplots(1,2, figsize=(fs,fs))\n\n  axarr[0].imshow(imgB)\n  axarr[0].title.set_text(\"Child\")\n  axarr[1].imshow(imgB)\n  axarr[1].title.set_text(\"Child\")\n\n  plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])\n  plt.show()\n\n!git clone https:\/\/github.com\/tg-bomze\/BabyGAN.git\n%cd \/kaggle\/working\/BabyGAN\n!mkdir aligned_images data father_image mother_image\n\nimport config\nimport dnnlib\nimport dnnlib.tflib as tflib\nfrom encoder.generator_model import Generator\n\nage_direction = np.load('\/kaggle\/working\/BabyGAN\/ffhq_dataset\/latent_directions\/age.npy')\nhorizontal_direction = np.load('\/kaggle\/working\/BabyGAN\/ffhq_dataset\/latent_directions\/angle_horizontal.npy')\nvertical_direction = np.load('\/kaggle\/working\/BabyGAN\/ffhq_dataset\/latent_directions\/angle_vertical.npy')\neyes_open_direction = np.load('\/kaggle\/working\/BabyGAN\/ffhq_dataset\/latent_directions\/eyes_open.npy')\ngender_direction = np.load('\/kaggle\/working\/BabyGAN\/ffhq_dataset\/latent_directions\/gender.npy')\nsmile_direction = np.load('\/kaggle\/working\/BabyGAN\/ffhq_dataset\/latent_directions\/smile.npy')\nprint(\"step1 ok.\")\n#clear_output()","e8cb6879":"#Copy pretrained models into BabyGAN folder\nif os.path.isfile('\/kaggle\/working\/finetuned_resnet.h5'):\n  print(\"0%\/100%   Copying has started\")\n  !cp '\/kaggle\/working\/finetuned_resnet.h5' '\/kaggle\/working\/BabyGAN\/data'\n  !cp '\/kaggle\/working\/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5' '\/kaggle\/working\/BabyGAN'\n  print(\"50%\/100%  Checkpoints copied\")\n  !cp '\/kaggle\/working\/karras2019stylegan-ffhq-1024x1024.pkl' '\/kaggle\/working\/BabyGAN'\n  !cp '\/kaggle\/working\/vgg16_zhang_perceptual.pkl' '\/kaggle\/working\/BabyGAN'\n  print(\"90%\/100%  Weights copied\")\n  !cp '\/kaggle\/working\/shape_predictor_68_face_landmarks.dat.bz2' '\/kaggle\/working\/BabyGAN'\n  print(\"100%\/100% Dictionary copied\")\n  clear_output()\n  print(\"Done!\")\nelse: raise ValueError('Please read the instructions in the block description and follow all 3 points correctly!')","e0744dde":"#update align_images.py, not neccessary\n!cp -f \"\/kaggle\/input\/align-images\/align_images.py\" \"\/kaggle\/working\/BabyGAN\/align_images.py\"","d819f3fb":"#Proccess father's photo, you can use photos in the testpic1 folder\n#Notice: JPG is not support because kaggle's libjpeg.so is not compatible with dlib\nfather_path = \"\/kaggle\/working\/BabyGAN\/father_image\/\" + \"father.png\"\n!cp -f \"\/kaggle\/input\/testpic1\/leon1.png\" $father_path  #just modify this line with: testpic1\/xxxx.png\n\npil_father = Image.open('\/kaggle\/working\/BabyGAN\/father_image\/father.png')\n\n!python \/kaggle\/working\/BabyGAN\/align_images.py \/kaggle\/working\/BabyGAN\/father_image \/kaggle\/working\/BabyGAN\/aligned_images\n\n#clear_output()\n\nif os.path.isfile('\/kaggle\/working\/BabyGAN\/aligned_images\/father_01.png'):\n  pil_father = Image.open('\/kaggle\/working\/BabyGAN\/aligned_images\/father_01.png')\n  (fat_width, fat_height) = pil_father.size\n  resize_fat = max(fat_width, fat_height)\/256\n  #display(pil_father.resize((int(fat_width\/resize_fat), int(fat_height\/resize_fat))))\nelse: raise ValueError('No face was found or there is more than one in the photo.')","86b52ac9":"#Proccess mother's photo, you can use photos in the testpic1 folder\n#Notice: JPG is not support because kaggle's libjpeg.so is not compatible with dlib\nmother_path = \"\/kaggle\/working\/BabyGAN\/mother_image\/\" + \"mother.png\"\n!cp -f \"\/kaggle\/input\/testpic1\/ada1.png\" $mother_path  #just modify this line with: testpic1\/xxxx.png\n\npil_mother = Image.open('\/kaggle\/working\/BabyGAN\/mother_image\/mother.png')\n\n!python \/kaggle\/working\/BabyGAN\/align_images.py \/kaggle\/working\/BabyGAN\/mother_image \/kaggle\/working\/BabyGAN\/aligned_images\n\n#clear_output()\n\nif os.path.isfile('\/kaggle\/working\/BabyGAN\/aligned_images\/mother_01.png'):\n  pil_mother = Image.open('\/kaggle\/working\/BabyGAN\/aligned_images\/mother_01.png')\n  (fat_width, fat_height) = pil_mother.size\n  resize_fat = max(fat_width, fat_height)\/256\n  #display(pil_mother.resize((int(fat_width\/resize_fat), int(fat_height\/resize_fat))))\nelse: raise ValueError('No face was found or there is more than one in the photo.')","89d32859":"#Generation of latent representation, this step may take 3 minutes\nuse_pretraineg_model = True\nif use_pretraineg_model == False:\n  !rm finetuned_resnet.h5\n  !python train_resnet.py \\\n  --test_size 256 \\\n  --batch_size 1024 \\\n  --loop 1 \\\n  --max_patience 1'''\n\n!python \/kaggle\/working\/BabyGAN\/encode_images.py \\\n  --early_stopping False \\\n  --lr=0.25 \\\n  --batch_size=2 \\\n  --iterations=100 \\\n  --output_video=False \\\n  \/kaggle\/working\/BabyGAN\/aligned_images \\\n  \/kaggle\/working\/BabyGAN\/generated_images \\\n  \/kaggle\/working\/BabyGAN\/latent_representations\n\ntflib.init_tf()\nURL_FFHQ = \"\/kaggle\/working\/BabyGAN\/karras2019stylegan-ffhq-1024x1024.pkl\"\nwith dnnlib.util.open_url(URL_FFHQ, cache_dir=config.cache_dir) as f:\n    generator_network, discriminator_network, Gs_network = pickle.load(f)\ngenerator = Generator(Gs_network, batch_size=1, randomize_noise=False)\nmodel_scale = int(2*(math.log(1024,2)-1))\n\n\nif len(os.listdir('\/kaggle\/working\/BabyGAN\/generated_images')) == 2:\n  first_face = np.load('\/kaggle\/working\/BabyGAN\/latent_representations\/father_01.npy')\n  second_face = np.load('\/kaggle\/working\/BabyGAN\/latent_representations\/mother_01.npy')\n  print(\"Generation of latent representation is complete! Now comes the fun part.\")\nelse: raise ValueError('Something wrong. It may be impossible to read the face in the photos. Upload other photos and try again.')","88ef4768":"#Generating a child's face basically\n\n#@markdown *The closer to 0, the more influence the father's genotype will have. Closer to 1 - mother.*\ngenes_influence = 0.7 #@param {type:\"slider\", min:0.01, max:0.99, step:0.01}\n\n#@markdown **Styling a photo:**\nstyle = \"dd\" #@param [\"Default\", \"Father's photo\", \"Mother's photo\"]\nif style == \"Father's photo\": \n  lr = ((np.arange(1,model_scale+1)\/model_scale)**genes_influence).reshape((model_scale,1))\n  rl = 1-lr\n  hybrid_face = (lr*first_face) + (rl*second_face)\nelif style == \"Mother's photo\": \n  lr = ((np.arange(1,model_scale+1)\/model_scale)**(1-genes_influence)).reshape((model_scale,1))\n  rl = 1-lr\n  hybrid_face = (rl*first_face) + (lr*second_face)\nelse: hybrid_face = ((1-genes_influence)*first_face)+(genes_influence*second_face)\n#@markdown **Child's approximate age:**\nperson_age = 20 #@param {type:\"slider\", min:10, max:50, step:1}\nintensity = -((person_age\/5)-6)\ndownload_image = False\n#@markdown **Resolution of the downloaded image:**\nresolution = \"512\" #@param [256, 512, 1024]\nsize = int(resolution), int(resolution)\n\nface = generate_final_image(hybrid_face, age_direction, intensity)\nplot_three_images(face, fs = 15)","d7a81f21":"#Generating a child's face with more parameters\n\n#@markdown *The closer to 0, the more influence the father's genotype will have. Closer to 1 - mother.*\ngenes_influence = 0.7 #@param {type:\"slider\", min:0.01, max:0.99, step:0.01}\n\n#@markdown **Styling a photo:**\nstyle = \"dd\" #@param [\"Default\", \"Father's photo\", \"Mother's photo\"]\nif style == \"Father's photo\": \n  lr = ((np.arange(1,model_scale+1)\/model_scale)**genes_influence).reshape((model_scale,1))\n  rl = 1-lr\n  hybrid_face = (lr*first_face) + (rl*second_face)\nelif style == \"Mother's photo\": \n  lr = ((np.arange(1,model_scale+1)\/model_scale)**(1-genes_influence)).reshape((model_scale,1))\n  rl = 1-lr\n  hybrid_face = (rl*first_face) + (lr*second_face)\nelse: hybrid_face = ((1-genes_influence)*first_face)+(genes_influence*second_face)\n\n#@markdown **Option intensity:**\nhorizontal = -5 #@param {type:\"slider\", min:-100, max:100, step:1}\nhorizontal_intensity = horizontal\/50\nvertical = 0 #@param {type:\"slider\", min:-100, max:100, step:1}\nvertical_intensity = vertical\/50\neyes_open = 0 #@param {type:\"slider\", min:-100, max:100, step:1}\neyes_open_intensity = -(eyes_open\/25)\ngender = -10 #@param {type:\"slider\", min:-100, max:100, step:1}\ngender_intensity = gender\/100\nsmile = 0 #@param {type:\"slider\", min:-100, max:100, step:1}\nsmile_intensity = smile\/100\nage = 20 #@param {type:\"slider\", min:-100, max:100, step:1}\nage_intensity = -((age\/5)-6)\n\ndirection_intensity = (horizontal_direction*horizontal_intensity) + (vertical_direction*vertical_intensity) + (eyes_open_direction*eyes_open_intensity) + (gender_direction*gender_intensity) + (smile_direction*smile_intensity) + (age_direction*age_intensity)\ndownload_image = False\n#@markdown **Resolution of the downloaded image:**\nresolution = \"512\" #@param [256, 512, 1024]\nsize = int(resolution), int(resolution)\n\nface = generate_final_image(hybrid_face, direction_intensity, 1)\nplot_three_images(face, fs =15)","c35de212":"#Generating animation of changing the child's face by 3 parameters\n\n!rm -rf \/kaggle\/working\/BabyGAN\/for_animation\n!mkdir \/kaggle\/working\/BabyGAN\/for_animation\nface_img = []\n\n#@markdown **The genes of which parent will prevail and by how much:**\ngender_influence = 0.7 #@param {type:\"slider\", min:0.01, max:0.99, step:0.01}\nhybrid_face = ((1-gender_influence)*first_face)+(gender_influence*second_face)\n\n#@markdown **Animation resolution:**\nanimation_resolution = \"256\" #@param [256, 512, 1024]\nanimation_size = int(animation_resolution), int(animation_resolution)\n\n#@markdown **Number of frames:**\nframes = 50 #@param {type:\"slider\", min:10, max:100, step:1}\n\n#@markdown **Select the first option:**\nparameter1 = 'angle_horizontal' #@param [\"age\", \"angle_horizontal\", \"angle_vertical\", \"eyes_open\", \"gender\", \"smile\"]\ndirection_file1 = np.load('\/kaggle\/working\/BabyGAN\/ffhq_dataset\/latent_directions\/' + parameter1 + '.npy')\nintensity1 = 0 #@param {type:\"slider\", min:-5, max:5, step:0.05}\nif intensity1 == 0: intensity1 += 0.001\ncoeffs1 = []\nfor i in range(0, frames):\n  coeffs1.append(round((i*intensity1)\/frames,3))\n\n\n#@markdown **Select the second option:**\nparameter2 = 'age' #@param [\"age\", \"angle_horizontal\", \"angle_vertical\", \"eyes_open\", \"gender\", \"smile\"]\ndirection_file2 = np.load('\/kaggle\/working\/BabyGAN\/ffhq_dataset\/latent_directions\/' + parameter2 + '.npy')\nintensity2 = 5 #@param {type:\"slider\", min:-5, max:5, step:0.05}\nif intensity2 == 0: intensity2 += 0.001\ncoeffs2 = []\nfor i in range(0, frames):\n  coeffs2.append(round((i*intensity2)\/frames,3))\n\n#@markdown **Select the third option:**\nparameter3 = 'smile' #@param [\"age\", \"angle_horizontal\", \"angle_vertical\", \"eyes_open\", \"gender\", \"smile\"]\ndirection_file3 = np.load('\/kaggle\/working\/BabyGAN\/ffhq_dataset\/latent_directions\/' + parameter3 + '.npy')\nintensity3 = 0 #@param {type:\"slider\", min:-5, max:5, step:0.05}\nif intensity3 == 0: intensity3 += 0.001\ncoeffs3 = []\nfor i in range(0, frames):\n  coeffs3.append(round((i*intensity3)\/frames,3))\n\n#set base age\nage = 25 #@param {type:\"slider\", min:-100, max:100, step:1}\nage_intensity = -((age\/5)-6)\nage_intensity2 = age_direction*age_intensity\n\nfor i in range(frames):\n  direction_intensity1 = direction_file1 * coeffs1[i]\n  direction_intensity2 = direction_file2 * coeffs2[i]\n  direction_intensity3 = direction_file3 * coeffs3[i]\n  direction_intensity = age_intensity2 + direction_intensity1 + direction_intensity2 + direction_intensity3\n  generate_final_images(hybrid_face, direction_intensity, 1, i)\n  clear_output()\n  print('Generated ' + str(i) + ' photos of ' + str(frames))\n\nadd_invert = True\nimg = os.listdir(\"\/kaggle\/working\/BabyGAN\/for_animation\")\nimg.sort()\nclear_output()\nprint('The animation is being generated. Please wait.')\n\nfor j in reversed(face_img):\n  face_img.append(j)\nface_img = np.array(face_img)\nimageio.mimsave(\"\/kaggle\/working\/BabyGAN\/for_animation\/3param.mp4\", face_img)\n\ndisplay(mpy.ipython_display(\"\/kaggle\/working\/BabyGAN\/for_animation\/3param.mp4\", height=400, autoplay=1, loop=1))","77846128":"#If you need to rerun the 'Generation of latent representation' block with new photos,\n#The following code could help you to free GPU memory.\n!apt-get install psmisc\n!fuser -v \/dev\/nvidia*\n\n#kill the proccess whose PID was showed above in order to free GPU memory.\n#!kill -9 37","a1547b52":"BabyGan\n\nBabyGan is based on StyleGan, it generates child's photo from parents' photos.\nThe original author is tg-bomze: https:\/\/github.com\/tg-bomze\/BabyGAN , I immigrate this project just for fun ~\nTo run this project, just turn on the GPU, then press \"Run All\".\n"}}