{"cell_type":{"ec1230a3":"code","12f1ee4b":"code","99be8abe":"code","5ffc3819":"code","6b79a0d3":"code","f1a3a036":"code","49745bb6":"code","a5e61ed3":"code","c9ed093e":"code","a2dab99a":"code","7bc68ffd":"code","0f77d61d":"code","0da0911e":"code","5ac67a14":"code","fb55252e":"code","a5ac9cd6":"code","c12cf351":"code","7e7377da":"code","8bd248db":"code","01a83f06":"code","afdd4a18":"code","0a759f97":"code","5aff2da6":"code","1a82aaf1":"markdown","a00f8ada":"markdown","e58b2622":"markdown","4b3fc52e":"markdown","c2617232":"markdown","d31b71bb":"markdown","5029939d":"markdown","a646ef2d":"markdown","06a7c13c":"markdown","680215e9":"markdown","1d39f707":"markdown","1ade5b84":"markdown","138e090e":"markdown"},"source":{"ec1230a3":"def metric(y_true, y_real):\n    return -1*np.mean(np.exp(np.abs(y_true-y_real)) - 1)","12f1ee4b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntrain = pd.read_csv('\/kaggle\/input\/yandex-exposition-task\/exposition_train.tsv', sep='\\t')\ntest = pd.read_csv('\/kaggle\/input\/yandex-exposition-task\/exposition_test.tsv', sep='\\t')\nsubmission = pd.read_csv('\/kaggle\/input\/yandex-exposition-task\/exposition_sample_submission.tsv', sep='\\t')","99be8abe":"train.head()","5ffc3819":"print(f'NaN value in train: {round(train.isna().sum().sum() \/ (train.shape[0]*train.shape[1]), 4)*100}%')\nprint(f'NaN value in test: {round(test.isna().sum().sum() \/ (test.shape[0]*test.shape[1]), 4)*100}%')","6b79a0d3":"train.columns","f1a3a036":"train['target'].value_counts(normalize=True)","49745bb6":"sns.countplot(data=train, x='parking').set_title('Parking count')\nplt.xticks(\n    rotation=45, \n    horizontalalignment='right')\nplt.show()\nsns.countplot(data=train, x='building_type').set_title('Building_type count')\nplt.xticks(\n    rotation=45, \n    horizontalalignment='right')\nplt.show()\nsns.countplot(data=train, x='renovation').set_title('Renovation count')\nplt.xticks(\n    rotation=45, \n    horizontalalignment='right')\nplt.show()","a5e61ed3":"sns.distplot(train['build_year']).set_title('Build_year distribution')\nplt.show()\nsns.distplot(train['ceiling_height']).set_title('Ceiling_height distribution')\nplt.show()\nsns.distplot(train['area']).set_title('Area distribution')\nplt.show()","c9ed093e":"to_drop = ['site_id', 'target_string', 'main_image', 'id', 'total_area', 'building_id', 'day']\n\ndef drop_col(df, to_drop):\n    df = df.drop(to_drop, axis=1)\n    return df","a2dab99a":"train_df = drop_col(train, to_drop)","7bc68ffd":"def replace(df):\n    df.loc[df.build_year == 0, 'build_year'] = np.NaN\n    df['build_year'] = df['build_year'].fillna((df.groupby(['building_series_id'])['build_year'].transform('median')))\n        \n    df.loc[(df.has_elevator==0) & (df.floor>5), 'has_elevator'] = 1\n    \n    df.loc[df.price<100, 'price'] *= 1000\n    \n    df.loc[(df.ceiling_height<2) | (df.ceiling_height>5), 'ceiling_height'] = np.NaN\n    df['ceiling_height'] = df['ceiling_height'].fillna(df.groupby(['floors_total','flats_count'])['ceiling_height'].transform('median'))\n    \n    df = df[df.area>df.kitchen_area].reset_index(drop=True)\n\n    \n    return df","0f77d61d":"train_df = replace(train_df)\ntest_df = replace(test)","0da0911e":"def mapping(df):\n    \n    balcony_map = {'UNKNOWN': 0, 'BALCONY': 1, 'LOGGIA':0, 'TWO_LOGGIA':0, 'TWO_BALCONY':2, 'BALCONY__LOGGIA':1,\n              'BALCONY__TWO_LOGGIA':1, 'THREE_LOGGIA':0, 'THREE_BALCONY':2}\n    loggia_map = {'UNKNOWN': 0, 'BALCONY': 0, 'LOGGIA':1, 'TWO_LOGGIA':2, 'TWO_BALCONY':0, 'BALCONY__LOGGIA':1,\n              'BALCONY__TWO_LOGGIA':2, 'THREE_LOGGIA':2, 'THREE_BALCONY':0}\n\n    df['expect_demolition'] = df['expect_demolition'].map({False:0,True:1})\n    df['is_apartment'] = df['is_apartment'].map({False:0,True:1})\n    df['has_elevator'] = df['has_elevator'].map({False:0,True:1})\n    df['studio'] = df['studio'].map({False:0,True:1})\n    df['num_balcony'] = df['balcony'].map(balcony_map)\n    df['num_loggia'] = df['balcony'].map(loggia_map)\n    \n    return df","5ac67a14":"train_df = mapping(train_df)\ntest_df = mapping(test_df)","fb55252e":"def smoothed_likelihood(df, column, alpha, target_column, test_df):\n    global_mean = df[target_column].mean()\n    nrows = df.groupby(column).count()[target_column].to_dict()\n    local_mean = df.groupby(column).mean()[target_column].to_dict()\n    if test_df is None:\n        new_column = df[column].apply(lambda x: (local_mean[x]*nrows[x] + global_mean*alpha)\/(nrows[x]+alpha))\n    else:\n        new_column = test_df[column].apply(lambda x: (local_mean[x]*nrows[x] + global_mean*alpha)\/(nrows[x]+alpha) if x in local_mean.keys() else global_mean)\n    return new_column","a5ac9cd6":"cat_columns = ['parking', 'unified_address','building_type','locality_name','renovation', 'building_series_id']\n\nfor col in cat_columns:\n    train_df[col] = smoothed_likelihood(train_df, col, 0.15, 'target', None)\n    test_df[col] = smoothed_likelihood(train_df, col, 0.15, 'target', test_df)","c12cf351":"def haversine_dist(lat1,lng1,lat2,lng2):\n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n    radius = 6371  # Earth's radius taken from google\n    lat = lat2 - lat1\n    lng = lng2 - lng1\n    d = np.sin(lat\/2) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng\/2) ** 2\n    h = 2 * radius * np.arcsin(np.sqrt(d))\n    return h","7e7377da":"def eng(df):\n    \n    df['years_old'] = 2020 - df['build_year']\n    \n    moskow_lat = 55.751244\n    moskow_lon = 37.618423\n    df['moskow_dist'] = np.sqrt((df['latitude'] - moskow_lat)**2 + (df['longitude'] - moskow_lon)**2)\n    \n    \n    df['rot_45_x'] = (0.707 * df['latitude']) + (0.707 * df['longitude'])\n    #df['rot_45_y'] = (0.707 * df['longitude']) + (0.707 * df['latitude'])\n    df['rot_30_x'] = (0.866 * df['latitude']) + (0.5 * df['longitude'])\n    df['rot_30_y'] = (0.866 * df['longitude']) + (0.5 * df['latitude'])\n    \n    df['haversine_moskow'] = haversine_dist(df['latitude'], df['longitude'], 55.751244, 37.618423)\n        \n    return df","8bd248db":"train_df = eng(train_df)\ntest_df = eng(test_df)","01a83f06":"from lightgbm import LGBMRegressor, LGBMClassifier\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom catboost import CatBoostRegressor, CatBoostClassifier\n\nseed = 47\n\nkfold = StratifiedKFold(n_splits=5, random_state=seed, shuffle=True)\nlgbm = LGBMRegressor(random_state=seed, n_estimators=500, learning_rate=0.1)","afdd4a18":"def calc(X,y,X_test, model, cv, cols, oof):\n    \n    if cols is None:\n        cols = X.columns\n    X=X[cols]\n    \n    res=[]\n    local_probs = pd.DataFrame()\n    for i, (tdx, vdx) in enumerate(cv.split(X, y)):\n        X_train, X_valid, y_train, y_valid = X.iloc[tdx], X.iloc[vdx], y.iloc[tdx], y.iloc[vdx]\n        model.fit(X_train, y_train,\n                 eval_set=[(X_train, y_train), (X_valid, y_valid)],\n                 early_stopping_rounds=30, \n                  verbose=False)   \n        preds = model.predict(X_valid)\n        \n        if oof==1:\n            X_test=X_test[cols]\n            oof_predict = model.predict(X_test)\n            local_probs['fold_%i'%i] = oof_predict\n            \n        y_valid = y_valid.to_numpy().reshape(1,-1)[0]\n        ll = metric(y_valid, np.round(preds))\n        print(f'{i} Fold: {ll:.4f}')\n        res.append(ll)\n        \n        \n    print(f'AVG score: {np.mean(res)}')\n    return np.mean(res), local_probs.mean(axis=1)","0a759f97":"X = train_df.drop(['target', 'balcony', 'floor', 'ceiling_height','build_year','studio'], axis=1)#, 'unified_address', 'locality_name'\ny = train_df[['target']]\n\n_, res_df = calc(X,y,test_df, lgbm, kfold, None, 1)","5aff2da6":"submission['target'] = np.round(res_df).astype(int)\nsubmission['target'].value_counts(normalize=True)\nsubmission.to_csv('submission.tsv', sep='\\t', index=False)","1a82aaf1":"# Modeling\n\n* we used LGBM for modeling\n* it was performed with respect to StratifiedKFold cross validation on 5 folds\n* due to lack of time, do not fine-tune the model\n* tried neural networks\n* had a quick feature selection. We track the diff when removing a feature on cross-validation with 5 repetitions and averaging the result","a00f8ada":"# HackTheRealty Hackathon by Yandex\n\n**by team GORNYAKI (Tsepa Oleksii and Samoshyn Andriy [Ukraine, KPI, IASA])**","e58b2622":"**What to do with the remaining categorical features?** Suppose a certain category is encountered only a few times and the corresponding objects belong to class 1. Then the average value of the target variable will also be 1. However, a completely different situation may arise on the test sample. **The solution is the smoothed average (or smoothed likelihood)**, which is calculated using the following formula:\n\n\n![smothed](https:\/\/habrastorage.org\/files\/686\/41f\/eea\/68641feea22e4a31a3f5eb5a86fd841c.png)\n\n\nHere **global mean** is the average value of the target variable over the entire sample, nrows is the number of times a specific value of the categorical variable is encountered, **alpha** is the regularization parameter. Now, if some value is rare, the global average will have more weight, and if often enough, the result will be close to the starting category average.","4b3fc52e":"**Important notes after elemental analysis:**\n\n* unnecessary columns - **'site_id'** (columns in constant 0), **'target_string'**, **'main_image'** (at the moment we are working with tabular data), **'total_area'** (duplicates a column 'area), **'id'**, **'builing_id'** (the organizers said this was a wrong column)\n* no missing data? *(we'll find out later)*\n* almost all classes are balanced","c2617232":"**From what else you could try:**\n* extract streets from addresses and search for large streets closest to houses\n* calculate the distance from home to the center of each city, not Moscow\n* use external data (e.g. metro coordinates)\n* working with images","d31b71bb":"**Of the interesting features:** it was considered that prices less than 100 differ from similar ads in the city differ on average by 1000 times. That is, the user may have forgotten to add zeros or entered the price in thousands.","5029939d":"## As a result, we took 7th place among over 50 teams. The gap between 1st and 7th place was 0.2. Thank you for watching, waiting your comments!","a646ef2d":"![logo](https:\/\/media.rbcdn.ru\/media\/upload_tmp\/2018\/rb-1200-628.png)","06a7c13c":"# Understanding the data","680215e9":"## Predict","1d39f707":"We had little time, but we tried to **create useful new features**. During the work there were about 30 of them, we have identified the most useful ones.","1ade5b84":"**Task:** to build a model that predicts the duration of the exposure of advert for the sale of a flat on Yandex.Realty.\n\n**Data:** adverts from the Yandex.Realty estate archive with exposure dates. By placing an object on Yandex.Realty, everyone wants to sell or rent it as quickly as possible for the maximum price for this object. The archive of the service contains hundreds of thousands of publications with exposure dates (dates for placing and withdrawing advertisements). On the service, organizers want to tell users how long they will wait for a buyer or tenant for their property given the current characteristics.\n\n**The metric by which the solutions are evaluated is written as follows:**\n\n$$metric = -{1 \\over l}\\sum\\limits_{i=1}^{l} {\\exp}^{|prediction_i-taget_i|} -1$$\n\nTo construct the target variable, the placement period is divided into several classes, each of which corresponds to an integer: **\"less than 7 days\" (1), \"7-14 days\" (2), \"15-30 days\" (3), \"30- 70 days \"(4),\" more than 70 days \"(5).**","138e090e":"> Looking at the values of the categorical and numeric columns, we noticed that the **data is rather dirty** *(a lot of 0 and UNKNOWN values)*. Since this is user data, the question arose whether these values should be dealt with? **(These are outliers or the user did not fill in these fields in the advert)**.\n\nWe decided to **remove only obvious outliers**, so as not to waste a lot of time on this. Replacements were checked for validation.\n\n# Preproccessing"}}