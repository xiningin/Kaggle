{"cell_type":{"53de4e2b":"code","0b9b83c2":"markdown","74d90f32":"markdown"},"source":{"53de4e2b":"import pandas  as pd\n\n#===========================================================================\n# read in the data\n#===========================================================================\ntrain_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data  = pd.read_csv('..\/input\/titanic\/test.csv')\n\n#===========================================================================\n# select some features of interest (\"ay, there's the rub\", Shakespeare)\n#===========================================================================\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\n\n#===========================================================================\n# for the features that are categorical we use pd.get_dummies:\n# \"Convert categorical variable into dummy\/indicator variables.\"\n#===========================================================================\nX_train       = pd.get_dummies(train_data[features])\ny_train       = train_data[\"Survived\"]\nfinal_X_test  = pd.get_dummies(test_data[features])\n\n#===========================================================================\n# create the kernel \n#===========================================================================\nfrom sklearn.gaussian_process.kernels import RBF\nkernel = 1.0 * RBF(1.0)\n\n#===========================================================================\n# perform the classification\n#===========================================================================\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nclassifier = GaussianProcessClassifier(kernel=kernel)\n\n#===========================================================================\n# and the fit \n#===========================================================================\nclassifier.fit(X_train, y_train)\n\n#===========================================================================\n# use the model to predict 'Survived' for the test data\n#===========================================================================\npredictions = classifier.predict(final_X_test)\n\n#===========================================================================\n# write out CSV submission file\n#===========================================================================\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, \n                       'Survived': predictions})\noutput.to_csv('submission.csv', index=False)","0b9b83c2":"### Kernel functions available in scikit-learn are:\n* [CompoundKernel](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.gaussian_process.kernels.CompoundKernel.html): Kernel which is composed of a set of other kernels.\n* [ConstantKernel](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.gaussian_process.kernels.ConstantKernel.html): Constant kernel.\n* [DotProduct](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.gaussian_process.kernels.DotProduct.html): Dot-Product kernel.\n* [Exponentiation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.gaussian_process.kernels.Exponentiation.html): The Exponentiation kernel takes one base kernel and a scalar parameter and combines them.\n* [ExpSineSquared](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.gaussian_process.kernels.ExpSineSquared.html): Exp-Sine-Squared kernel (aka periodic kernel).\n* [Hyperparameter](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.gaussian_process.kernels.Hyperparameter.html): A kernel hyperparameter\u2019s specification in form of a namedtuple.\n* [Kernel](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.gaussian_process.kernels.Kernel.html): Base class for all kernels.\n* [Matern](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.gaussian_process.kernels.Matern.html): The class of Matern kernels is a generalization of the radial-basis function kernel (aka squared-exponential kernel).\n* [PairwiseKernel](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.gaussian_process.kernels.PairwiseKernel.html): Wrapper for kernels in sklearn.metrics.pairwise.\n* [Product](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.gaussian_process.kernels.Product.html): The Product kernel takes two kernels and combines them.\n* [RationalQuadratic](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.gaussian_process.kernels.RationalQuadratic.html): The RationalQuadratic kernel can be seen as a scale mixture (an infinite sum) of radial-basis function kernels with different characteristic length scales.\n* [RBF](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.gaussian_process.kernels.RBF.html): Radial-basis function kernel (aka squared-exponential kernel).\n* [Sum](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.gaussian_process.kernels.Sum.html): The Sum kernel takes two kernels and combines them.\n* [WhiteKernel](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.gaussian_process.kernels.WhiteKernel.html): The main use-case of the White kernel is as part of a sum-kernel where it explains the noise of the signal as independently and identically normally-distributed.\n\nFor more information on which choice to make see [The Kernel Cookbook: 'Advice on Covariance functions'](https:\/\/www.cs.toronto.edu\/~duvenaud\/cookbook\/), by David Duvenaud.\n\n## Related reading\n* [Gaussian process](https:\/\/en.wikipedia.org\/wiki\/Gaussian_process) on Wikipedia\n* [Gaussian Processes](https:\/\/scikit-learn.org\/stable\/modules\/gaussian_process.html) on Scikit-learn\n* [GaussianProcessRegressor](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.gaussian_process.GaussianProcessRegressor.html) on Scikit-learn\n* [Carl Edward Rasmussen and Christopher K. I. Williams \"Gaussian Processes for Machine Learning\"](http:\/\/www.gaussianprocess.org\/gpml\/) (book website)\n* [Gaussian Process Regression Models](https:\/\/www.mathworks.com\/help\/stats\/gaussian-process-regression-models.html) on MathWorks\n\n### Related notebooks:\n* [Gaussian process regression and classification](https:\/\/www.kaggle.com\/residentmario\/gaussian-process-regression-and-classification) by Aleksey Bilogur\n* [Feature Engineering with Gaussian Process](https:\/\/www.kaggle.com\/kenmatsu4\/feature-engineering-with-gaussian-process) by kenmatsu4","74d90f32":"## Gaussian process classification\nHere is a very simple example script that applies [Gaussian process](https:\/\/scikit-learn.org\/stable\/modules\/gaussian_process.html) classification to the Titanic data set. \nIn [Gaussian process classification](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.gaussian_process.GaussianProcessRegressor.html) a [kernel function](https:\/\/scikit-learn.org\/stable\/modules\/gaussian_process.html#gp-kernels) (or [covariance](https:\/\/en.wikipedia.org\/wiki\/Covariance) function) is used to help to mould the shape of prior and posterior values. It is assumed that any uncertainty, or noise, is Gaussian in nature (i.e. can be sampled from a [normal distribution](https:\/\/en.wikipedia.org\/wiki\/Normal_distribution)), hence the name."}}