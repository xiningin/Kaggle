{"cell_type":{"31a2de7d":"code","54dff4c0":"code","b1bf063d":"code","f09195cb":"code","a00300db":"code","f053b392":"code","8113ad75":"code","c730555b":"code","c582370f":"code","6d1e0fae":"code","7a35b362":"code","aa42dbf9":"code","bfa9cf17":"code","af08079a":"markdown","0a415182":"markdown","b6e5e3b0":"markdown","9df701e2":"markdown","86a07e82":"markdown","10f68a06":"markdown","2a08126e":"markdown","237d61b0":"markdown"},"source":{"31a2de7d":"!pip install fastai2 > \/dev\/null","54dff4c0":"import fastai2\nfrom fastai2.vision.all import *","b1bf063d":"path = Path('..\/input\/prostate-cancer-grade-assessment')\npath.ls()","f09195cb":"df = pd.read_csv(path\/'train.csv')\nimg_path = Path('..\/input\/panda-train-png-images\/train\/')","a00300db":"df.head(3)","f053b392":"# add .png to filenames\ndf['image_id'] = df['image_id'].apply(lambda x: str(x)+'.png')\ndf.head(3)","8113ad75":"prostates = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                   splitter=RandomSplitter(),\n                   get_x=ColReader(0, pref=img_path),\n                   get_y=ColReader(1),\n                   item_tfms=Resize(224),\n                   batch_tfms=aug_transforms()\n                     )","c730555b":"dls = prostates.dataloaders(df, bs=16)\ndls.show_batch()","c582370f":"dls.show_batch()","6d1e0fae":"learn = cnn_learner(dls, resnet50, metrics=accuracy)\nlearn.fit_one_cycle(1)","7a35b362":"interp = ClassificationInterpretation.from_learner(learn)","aa42dbf9":"interp.plot_confusion_matrix()","bfa9cf17":"interp.plot_top_losses(k = 9)","af08079a":"# Create DataLoaders","0a415182":"- Source of dataset: https:\/\/www.kaggle.com\/xhlulu\/panda-resize-and-save-train-data\/\n- Kernel: https:\/\/www.kaggle.com\/xhlulu\/panda-resize-and-save-train-data\/output","b6e5e3b0":"A single epoch gives us an accuracy of 100%. I suspect several possibilities\n\n- the imagery process might differ\n- or they have similar process, but then don't encode the data similarly on the computer\n- it's not the same level of zoom\n- the preprocessing of <a href='https:\/\/www.kaggle.com\/xhlulu\/panda-resize-and-save-train-data\/output'>this kernel<\/a> has somethings fishy \n\nIn any case, finding a way for our model to look at the same thing when it sees images from different institution should probably a priority in this competition","9df701e2":"See they \"grey\" zone around the purple in \"Radboud\" pictures, whereas \"Karolinska\" doesn't seem to have that at all... Also, karolinska pictures seem to be nearly always straight vertical thin lines, whereas radboud look more like masses.\n\nThat's what got me started. Here are more examples:","86a07e82":"Can we use a model to predict, given an image, which institution it comes from ? \n\nI ask the question because, looking at the pictures, I saw very different looking shapes. I wonder if the two institutions have different way of taking the pictures (or maybe they render them differently ?)\n\nIf the pictures are so different from one institution to another,it could pertubate the model during training... \n\nLet's see if the pictures are so different...","10f68a06":"# Model","2a08126e":"# Install and import fastai2","237d61b0":"# Get the data"}}