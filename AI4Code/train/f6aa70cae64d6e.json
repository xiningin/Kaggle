{"cell_type":{"177dc573":"code","2498db75":"code","b6e95caf":"code","4fca98cb":"code","be64a880":"code","47f76c66":"code","09fe4f52":"code","5af710aa":"code","6af9ae32":"code","c9a1ad9a":"code","98bedb18":"code","4879752e":"code","76ac71e2":"code","6602790d":"code","e3fee263":"code","65986e4e":"code","a558848b":"code","40dd2fed":"code","9b6bd8dc":"code","981ad87e":"code","a2ec6224":"code","91833491":"code","3fe8a35f":"markdown","19c4b0d9":"markdown","97b7e87f":"markdown","6cc25195":"markdown","60277332":"markdown","991863ab":"markdown","fe2dd16a":"markdown","d440cfaa":"markdown","29ff2f45":"markdown","a14a72b0":"markdown","50979496":"markdown","a6adda67":"markdown","a8923cc8":"markdown","cdcfcdd7":"markdown","fd6620ae":"markdown"},"source":{"177dc573":"# import packages\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","2498db75":"pd.set_option('display.max_columns', 100) # 100 column limit\npd.set_option('display.max_rows', 100) # 100 row limits","b6e95caf":"import os\nimport dask.dataframe as dd\nfrom dask.distributed import Client, progress\n\n# reading the paths of all the files present in the dataset\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","4fca98cb":"%time\ntrain = dd.read_csv(\"..\/input\/jane-street-market-prediction\/train.csv\")\n#features = dd.read_csv(\"..\/input\/jane-street-market-prediction\/features.csv\")\n#example_test = dd.read_csv(\"..\/input\/jane-street-market-prediction\/example_test.csv\")\n# transform train from dask format to pandas dataframe\ntrain = train.compute()","be64a880":"train.describe()","47f76c66":"#print(train.head())","09fe4f52":"#train['date'].unique()","5af710aa":"sns.catplot(x=\"date\", kind=\"count\", data=train)","6af9ae32":"date_0 = train.loc[train['date'] == 0]\ndate = date_0.copy()\nx = range(date.shape[0])\n\nfig, axs = plt.subplots(6)\nfig.suptitle('Resp_1, Resp_2, Resp_3, Resp_3, Resp_4, Resp and Weight for Day 0')\naxs[0].plot(x,date['resp_1'],'r')\naxs[1].plot(x,date['resp_2'],'b')\naxs[2].plot(x,date['resp_3'],'g')\naxs[3].plot(x,date['resp_4'],'y')\naxs[4].plot(x,date['resp'],'y')\naxs[5].plot(x,date['weight'],'y')\n\n\nplt.show()\n\ndel date","c9a1ad9a":"date_1 = train.loc[train['date'] == 1]\ndate = date_1.copy()\nx = range(date.shape[0])\n\nfig, axs = plt.subplots(6)\nfig.suptitle('Resp_1, Resp_2, Resp_3, Resp_3, Resp_4, Resp and Weight for Day 1')\naxs[0].plot(x,date['resp_1'],'r')\naxs[1].plot(x,date['resp_2'],'b')\naxs[2].plot(x,date['resp_3'],'g')\naxs[3].plot(x,date['resp_4'],'y')\naxs[4].plot(x,date['resp'],'y')\naxs[5].plot(x,date['weight'],'y')\n\nplt.show()\ndel date","98bedb18":"date_2 = train.loc[train['date'] == 2]\ndate = date_2.copy()\nx = range(date.shape[0])\n\nfig, axs = plt.subplots(6)\nfig.suptitle('Resp_1, Resp_2, Resp_3, Resp_3, Resp_4, Resp and Weight for Day 2')\naxs[0].plot(x,date['resp_1'],'r')\naxs[1].plot(x,date['resp_2'],'b')\naxs[2].plot(x,date['resp_3'],'g')\naxs[3].plot(x,date['resp_4'],'y')\naxs[4].plot(x,date['resp'],'y')\naxs[5].plot(x,date['weight'],'y')\nplt.show()\n\ndel date","4879752e":"train['feature_0'].unique()","76ac71e2":"sns.catplot(x=\"feature_0\",y=\"resp\", kind=\"box\", data=date_0)","6602790d":"pd.crosstab(train['feature_0'], train['resp'] > 0,  margins = False) ","e3fee263":"#train.isna().sum()","65986e4e":"train.corr().style.background_gradient(cmap='coolwarm', axis=None).set_precision(2)","a558848b":"plt.plot(train['ts_id'],train['feature_60'],'r',train['ts_id'],train['feature_61'],'green')\nplt.title(\"Feature 60 and 61 for whole train dataset\", fontsize=16, fontweight='bold')\nplt.xlabel(\"transactions\/trades along time\")\nplt.show()","40dd2fed":"diff = train['feature_60'] - train['feature_61']\nprint(f'The difference between feature 60 and 61 is normally distributed. \\n The range is \\n{diff.describe()}')\n\ncovariance = np.cov(train['feature_60'], train['feature_61'])\nprint(f'The covariance matrix is \\n {covariance}')\n\nfrom scipy.stats import pearsonr\ncorr, _ = pearsonr(train['feature_60'], train['feature_61'])\nprint('Pearsons correlation: %.3f' % corr)\n\nplt.hist(diff, bins = 1000)\nplt.show()","9b6bd8dc":"# Taken from this notebook: https:\/\/www.kaggle.com\/blurredmachine\/jane-street-market-eda-viz-prediction\n\nimport plotly.express as px\n\ndate = 0\nn_features = 130\n\ncols = [f'feature_{i}' for i in range(1, n_features)]\nhist = px.histogram(\n    train[train[\"date\"] == date], \n    x=cols, \n    animation_frame='variable', \n    range_y=[0, 600], \n    range_x=[-7, 7]\n)\n\nhist.show()\n","981ad87e":"# remove the returns and ts_id for PCA analysis\ncols = list(train.columns)\nfor removeCol in ['date', 'weight', 'resp_1', 'resp_2', 'resp_3', 'resp_4', 'resp', 'ts_id']:\n    cols.remove(removeCol)\n\ntrain_features = train[cols]","a2ec6224":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n#In general it is a good idea to scale the data\nscaler = StandardScaler()\nscaler.fit(train_features)\ntrain_features_scaled = scaler.transform(train_features)\n\n\n# initiate PCA \npca = PCA(n_components=50)\n\n# transform \ntrain_features_scaled_transformed = pca.fit_transform(train_features_scaled[~np.isnan(train_features_scaled).any(axis=1)])\n\n\n# when testing,transform new data using already fitted pca \n# (don't re-fit the pca)\n# newdata_transformed = pca.transform(newdata)\n\n#The amount of variance that each PC explains\nvar= pca.explained_variance_ratio_\nprint(var)\n\n#Cumulative Variance explains\nvar1=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n\nprint(var1)\n\nplt.plot(var1)\n\n","91833491":"loadings = pca.components_.T\ndf_loadings = pd.DataFrame(loadings)\ndf_loadings","3fe8a35f":"### The number of transactions\/trades each day is different.","19c4b0d9":"## Number of missing values for each feature","97b7e87f":"### Plot Feature_0 vs Resp for Day 0","6cc25195":"## This notebook is for exploring the dataset, including data understanding, distributions, correlations, missing values etc","60277332":"Contigency table of Feature_0 with Resp >0. \n\nIt shows that feature_0 does not have a clear association with positive return","991863ab":"## Feature_0","fe2dd16a":"### The Pearson correlation coefficient for Feature 60 and 61 is 0.997 which means a total positive linear correlation","d440cfaa":"The distribution of all features","29ff2f45":"### Plot some features that have high correlation from a time serious view? e.g. feature 60 vs feature 61","a14a72b0":"### There are 500 days in total in train dataset","50979496":"### Correlations of features","a6adda67":"Feature correlations for the whole train dataset.\n\nSome features are pecfectly correlated. Some correlation are even 1.","a8923cc8":"## Fetch data","cdcfcdd7":"## Plot resp_1, resp_2, resp_3, resp_4, resp and weight along time for day 0,1,2\n","fd6620ae":"### Using PCA to deal with multicollinearity"}}