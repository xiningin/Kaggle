{"cell_type":{"831b976f":"code","7a1debeb":"code","5f2f33b4":"code","6bb59041":"code","c6f8d797":"code","4228cc5e":"code","56e11917":"code","66f7ca87":"code","73540be4":"code","cfd59db6":"code","864d714c":"code","666ee3ff":"code","014fbea5":"code","db9b8e22":"code","ddb8e042":"code","285a0519":"code","039b25d5":"code","4f5304bd":"code","5d2249dd":"code","b8a76e86":"code","af91a368":"code","4c129e83":"code","3b20e7c8":"code","b3477fa4":"code","6eff3b9d":"code","08c4c23e":"code","5f8ec9d4":"code","17e42e98":"code","b6e4755f":"code","d76df0d9":"code","e41a6460":"code","98b90b4d":"code","fd5f3a4f":"code","ac137512":"code","0288b72f":"code","4a648040":"code","83b8524d":"code","ab4387e9":"code","f6b49108":"code","7b1432f4":"code","053cb97d":"code","202179c2":"code","4443d500":"code","8806071d":"code","9438e528":"code","3b3622a2":"code","6dda1e80":"code","1200ad20":"code","93333c7a":"code","674756cf":"code","dc0553b8":"code","f4248cf7":"code","458bb7a9":"code","bf9a70a4":"code","ad1d6432":"code","7915c9cd":"code","39d08ada":"code","015d27d7":"code","6d5dc1ba":"code","788f2318":"code","da6189e7":"code","8eae9ce4":"code","981c2d2a":"code","6eff1482":"code","8e0becae":"code","1d69187f":"code","da919e31":"code","8d3ff429":"code","0de94807":"code","78cf6058":"code","69850f52":"code","952cc4d9":"code","eb820d76":"code","b7842c05":"code","ce266329":"code","2a9e3a07":"code","978974e6":"code","499fd62c":"code","d91a44a9":"code","d4371d22":"code","ddd81919":"code","504f4756":"code","2ed430c4":"code","3ef42c07":"code","f9ef0cbf":"code","24678e43":"code","b5a81071":"code","eed26dd1":"code","b6987ab9":"code","db1b08d4":"code","05fe4a14":"code","fed3c575":"code","382e6d90":"code","ff455fa4":"code","8dcd6051":"code","ec44a207":"code","3c345cc1":"code","10fbd040":"code","4cb4fb73":"code","a54bcc25":"code","f10706b3":"code","39e8bb2c":"code","8d499b31":"code","e6adddf1":"code","c032a3d4":"code","2114d143":"code","9caa8101":"code","d5a429e4":"code","fcc007c8":"code","05544f92":"code","d19ec5b9":"code","4056c754":"code","2c05991e":"code","57202efc":"code","cb79de56":"code","fbed1699":"code","e974b506":"code","c361da4f":"code","73f3cc99":"code","d8bccb1b":"code","1b06feb4":"code","87a4877e":"code","86705e8a":"code","9861e1f4":"code","6c40e1b2":"code","70f19bfc":"code","dd11052a":"code","00b61bcb":"code","58e557c6":"code","ccdd4035":"code","94fede61":"code","f3f56d9b":"code","a821a419":"code","89ba9a69":"code","6c7388f3":"code","a5aeb8b3":"markdown","13187cb2":"markdown","50ba067d":"markdown","80304d38":"markdown","38d42710":"markdown","fb67b4fe":"markdown","30a873f5":"markdown","4669f68c":"markdown","1ec4edf2":"markdown","c29aa0d0":"markdown","4098fe3c":"markdown","7d6a1575":"markdown","814f7ef7":"markdown","49af4ee0":"markdown","169dfc29":"markdown","c492a0ae":"markdown","d0448307":"markdown","27da154a":"markdown","60763fa5":"markdown","8bca2c1a":"markdown","a5d73238":"markdown","0b9c8e7e":"markdown","daf16642":"markdown","eeda932a":"markdown","671ce004":"markdown","40f4cf6b":"markdown","22dd253a":"markdown","27443d54":"markdown","bbb5e62c":"markdown","fe4c464a":"markdown","7d80bca0":"markdown","c6bcb035":"markdown","1ea2f46d":"markdown","c19667fb":"markdown","c169c6ae":"markdown","1900f2c1":"markdown","0556fae4":"markdown","70835be0":"markdown","954218d0":"markdown","ff336d61":"markdown","9d5dbae8":"markdown","a72426ce":"markdown","3e7b15fa":"markdown","6de10411":"markdown","9ad6a49a":"markdown","b1b8119e":"markdown","d8708459":"markdown"},"source":{"831b976f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7a1debeb":"data_dir = '\/kaggle\/input\/new-york-city-taxi-fare-prediction'","5f2f33b4":"# List of files with size\n!ls -lh {data_dir}","6bb59041":"# Training dataset\n!head {data_dir}\/train.csv","c6f8d797":"# Test dataset\n!head {data_dir}\/test.csv","4228cc5e":"# sample_submission file\n!head {data_dir}\/sample_submission.csv","56e11917":"# count number of lines in training dataset\n!wc -l {data_dir}\/train.csv","66f7ca87":"# count number of lines in test dataset\n!wc -l {data_dir}\/test.csv","73540be4":"# No. of lines in sample_submission file\n!wc -l {data_dir}\/sample_submission.csv","cfd59db6":"import pandas as pd\nimport random\nfrom datetime import datetime","864d714c":"selected_cols = 'fare_amount,pickup_datetime,pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude,passenger_count'.split(',')\ndtypes = {\n    'fare_amount': 'float32',\n    'pickup_longitude': 'float32',\n    'pickup_latitude': 'float32',\n    'dropoff_longitude': 'float32',\n    'dropoff_latitude' : 'float32',\n    'passenger_count': 'uint8'\n}","666ee3ff":"%%time\n\nfrac = 0.02\ndef skip_row(row_idx):\n    if row_idx == 0:\n        return False\n    return random.random() > frac\n\nrandom.seed(10)\n# dateparse = lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n\ntaxi_df = pd.read_csv(data_dir+\"\/train.csv\", \n                 usecols=selected_cols, \n                 dtype=dtypes, \n                 parse_dates=['pickup_datetime'],\n                 skiprows=skip_row)","014fbea5":"taxi_df","db9b8e22":"type(taxi_df.pickup_datetime[0])","ddb8e042":"# Load Test Set\ntest_df = pd.read_csv(data_dir+'\/test.csv', dtype=dtypes, parse_dates=['pickup_datetime'])","285a0519":"test_df","039b25d5":"taxi_df.info()","4f5304bd":"taxi_df.describe().apply(lambda s: s.apply(lambda x: format(x, 'g')))","5d2249dd":"taxi_df.isnull().sum()","b8a76e86":"taxi_df.fare_amount.lt(0).sum","af91a368":"taxi_df.pickup_datetime.min(), taxi_df.pickup_datetime.max()","4c129e83":"sum(taxi_df['fare_amount']>100)","3b20e7c8":"test_df.info()","b3477fa4":"test_df.describe()","6eff3b9d":"test_df.pickup_datetime.min(), test_df.pickup_datetime.max()","08c4c23e":"taxi_df = taxi_df.dropna()","5f8ec9d4":"# import libraries for data visulization\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly\nimport seaborn as sns\n","17e42e98":"sns.set_style('darkgrid')\nmatplotlib.rcParams['font.size'] = 14\nmatplotlib.rcParams['figure.figsize'] = (12, 8)\nmatplotlib.rcParams['figure.facecolor'] = '#00000000'","b6e4755f":"import warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) ","d76df0d9":"sns.displot(taxi_df['fare_amount'], kde=False, bins=100)","e41a6460":"taxi_df = taxi_df[(taxi_df['fare_amount']>0) & (taxi_df['fare_amount'] <= 200)]","98b90b4d":"sns.displot(taxi_df[taxi_df['fare_amount']<=100]['fare_amount'], kde=False, bins=100)","fd5f3a4f":"sns.displot(taxi_df['passenger_count'], kde=False)","ac137512":"sum(taxi_df['passenger_count']>10)","0288b72f":"sns.displot(taxi_df[taxi_df['passenger_count']<=10].passenger_count, kde=False, bins=10)","4a648040":"sum(taxi_df['passenger_count']==0)","83b8524d":"sns.boxplot(data=taxi_df[taxi_df['passenger_count']<=10], x=\"passenger_count\", y=\"fare_amount\")","ab4387e9":"taxi_df[taxi_df['passenger_count']>8]","f6b49108":"taxi_df = taxi_df[(taxi_df['passenger_count']<=8) & (taxi_df['passenger_count']>0)]","7b1432f4":"bbox = (min(test_df.pickup_longitude.min(), test_df.dropoff_longitude.min()),\n        max(test_df.pickup_longitude.max(), test_df.dropoff_longitude.max()),\n        min(test_df.pickup_latitude.min(), test_df.dropoff_latitude.min()),\n        max(test_df.pickup_latitude.max(), test_df.dropoff_latitude.max())\n)\n       \nbbox","053cb97d":"import PIL\nimport urllib\nimport io\n\nurl = 'https:\/\/i.imgur.com\/xx2b9dC.png'\nnyc_map = PIL.Image.open(urllib.request.urlopen(url))\nnyc_map","202179c2":"nyc_map = np.array(nyc_map)","4443d500":"taxi_df = taxi_df[(taxi_df.pickup_longitude >= bbox[0]) & (taxi_df.pickup_longitude <= bbox[1]) &\n            (taxi_df.pickup_latitude >= bbox[2]) & (taxi_df.pickup_latitude <= bbox[3]) & \n            (taxi_df.dropoff_longitude >= bbox[0]) & (taxi_df.dropoff_longitude <= bbox[1]) & \n            (taxi_df.dropoff_latitude >= bbox[2]) & (taxi_df.dropoff_latitude <= bbox[3])]","8806071d":"fig, ax = plt.subplots(1, 2, figsize = (15,9))\nax[0].scatter(taxi_df['pickup_longitude'], taxi_df['pickup_latitude'], zorder=1, alpha= 0.2, c='b', s=1)\nax[0].set_title('Pickup Locations')\nax[0].set_xlim(bbox[0],bbox[1])\nax[0].set_ylim(bbox[2],bbox[3])\nax[0].imshow(nyc_map, zorder=0, extent = bbox, aspect= 'equal')\n\nax[1].scatter(taxi_df['dropoff_longitude'], taxi_df['dropoff_latitude'], zorder=1, alpha= 0.2, c='b', s=1)\nax[1].set_title('Dropoff Locations')\nax[1].set_xlim(bbox[0],bbox[1])\nax[1].set_ylim(bbox[2],bbox[3])\nax[1].imshow(nyc_map, zorder=0, extent = bbox, aspect= 'equal')\n\nplt.imshow(nyc_map)","9438e528":"longitude = list(taxi_df.pickup_longitude) + list(taxi_df.dropoff_longitude)\nlatitude = list(taxi_df.pickup_latitude) + list(taxi_df.dropoff_latitude)\nplt.figure(figsize = (10,10))\nplt.plot(longitude,latitude,'.', alpha = 0.4, markersize = 0.05)\nplt.xlim(-74.05, -73.75)\nplt.ylim(40.6, 40.9)\nplt.show()","3b3622a2":"fig, ax = plt.subplots(1, 1, figsize = (15,9))\nxdf = taxi_df[taxi_df['fare_amount']<80]\n\nsp = ax.scatter(xdf.dropoff_longitude, xdf.dropoff_latitude, c=xdf.fare_amount,alpha= 0.4, s=5, cmap='Spectral')\nfig.colorbar(sp)\nax.set_xlim(-74.05, -73.75)\nax.set_ylim(40.6, 40.9)\nplt.show()","6dda1e80":"url = 'https:\/\/i.imgur.com\/ZGg3Bry.png'\nnyc_mask = np.array(PIL.Image.open(urllib.request.urlopen(url)))[:,:,0]>(255*0.7)","1200ad20":"nyc_mask.shape","93333c7a":"plt.imshow(nyc_map, zorder=0)\nplt.imshow(nyc_mask, alpha=0.7, cmap='gray')","674756cf":"def location_to_coor(longitude, latitude, dx, dy, bbox):\n    return (dx*(longitude - bbox[0])\/(bbox[1]-bbox[0])).astype('int'), (dy - dy*(latitude - bbox[2])\/(bbox[3]-bbox[2])).astype('int')","dc0553b8":"pickup_x, pickup_y = location_to_coor(taxi_df.pickup_longitude, taxi_df.pickup_latitude, \n                                  nyc_mask.shape[1], nyc_mask.shape[0], bbox)\ndropoff_x, dropoff_y = location_to_coor(taxi_df.dropoff_longitude, taxi_df.dropoff_latitude, \n                                  nyc_mask.shape[1], nyc_mask.shape[0], bbox)","f4248cf7":"idx = (nyc_mask[pickup_y, pickup_x] & nyc_mask[dropoff_y, dropoff_x])\nprint(\"Number of trips in water: {}\".format(np.sum(idx)))","458bb7a9":"np.count_nonzero(idx==0)","bf9a70a4":"taxi_df[idx]","ad1d6432":"taxi_df = taxi_df[~idx]","7915c9cd":"def add_dateparts(df, col):\n    df[col + '_year'] = df[col].dt.year\n    df[col + '_month'] = df[col].dt.month\n    df[col + '_day'] = df[col].dt.day\n    df[col + '_weekday'] = df[col].dt.weekday\n    df[col + '_hour'] = df[col].dt.hour","39d08ada":"add_dateparts(taxi_df, 'pickup_datetime')","015d27d7":"add_dateparts(test_df, 'pickup_datetime')","6d5dc1ba":"test_df","788f2318":"def haversine_np(lon1, lat1, lon2, lat2):\n    \"\"\"\n    Calculate the great circle distance between two points\n    on the earth (specified in decimal degrees)\n\n    All args must be of equal length.    \n\n    \"\"\"\n    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n\n    a = np.sin(dlat\/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon\/2.0)**2\n\n    c = 2 * np.arcsin(np.sqrt(a))\n    km = 6367 * c\n    return km","da6189e7":"def add_trip_distance(df):\n    df['trip_distance'] = haversine_np(df['pickup_longitude'], df['pickup_latitude'], df['dropoff_longitude'], df['dropoff_latitude'])","8eae9ce4":"%%time\nadd_trip_distance(taxi_df)\nadd_trip_distance(test_df)","981c2d2a":"taxi_df.describe().apply(lambda s: s.apply(lambda x: format(x, 'g')))","6eff1482":"taxi_df","8e0becae":"sns.scatterplot(data = taxi_df, x=\"pickup_datetime_day\", y=\"fare_amount\")","1d69187f":"sns.scatterplot(data = taxi_df, x=\"pickup_datetime_year\", y=\"fare_amount\")","da919e31":"taxi_df.groupby('pickup_datetime_year').mean()['fare_amount']","8d3ff429":"sns.scatterplot(data = taxi_df, x=\"pickup_datetime_weekday\", y=\"fare_amount\")","0de94807":"sns.distplot(x=taxi_df['pickup_datetime_hour'], bins=24, kde=False)","78cf6058":"sns.barplot(x='pickup_datetime_hour',y='fare_amount', data=taxi_df)","69850f52":"sns.barplot(x='pickup_datetime_hour',y='trip_distance', data=taxi_df)","952cc4d9":"sns.distplot(x=taxi_df['pickup_datetime_weekday'], bins=7, kde=False)","eb820d76":"sns.barplot(x='pickup_datetime_weekday',y='fare_amount', data=taxi_df)","b7842c05":"sns.distplot(taxi_df['trip_distance'])","ce266329":"taxi_df[taxi_df['trip_distance']>100]","2a9e3a07":"jfk_lonlat = -73.7781, 40.6413\nlga_lonlat = -73.8740, 40.7769\newr_lonlat = -74.1745, 40.6895\nmet_lonlat = -73.9632, 40.7794\nwtc_lonlat = -74.0099, 40.7126","978974e6":"def add_landmark_dropoff_distance(df, landmark_name, landmark_lonlat):\n    lon, lat = landmark_lonlat\n    df[landmark_name + '_drop_distance'] = haversine_np(lon, lat, df['dropoff_longitude'], df['dropoff_latitude'])","499fd62c":"%%time\nfor a_df in [taxi_df, test_df]:\n    for name, lonlat in [('jfk', jfk_lonlat), ('lga', lga_lonlat), ('ewr', ewr_lonlat), ('met', met_lonlat), ('wtc', wtc_lonlat)]:\n        add_landmark_dropoff_distance(a_df, name, lonlat)","d91a44a9":"test_df","d4371d22":"from sklearn.model_selection import train_test_split","ddd81919":"train_df, val_df = train_test_split(taxi_df, test_size=0.2, random_state=10)","504f4756":"len(train_df), len(val_df)","2ed430c4":"taxi_df.columns","3ef42c07":"input_cols = ['pickup_longitude', 'pickup_latitude',\n       'dropoff_longitude', 'dropoff_latitude', 'passenger_count',\n       'pickup_datetime_year', 'pickup_datetime_month', 'pickup_datetime_day',\n       'pickup_datetime_weekday', 'pickup_datetime_hour', 'trip_distance',\n       'jfk_drop_distance', 'lga_drop_distance', 'ewr_drop_distance',\n       'met_drop_distance', 'wtc_drop_distance']","f9ef0cbf":"target_col = 'fare_amount'","24678e43":"train_inputs = train_df[input_cols]","b5a81071":"train_targets = train_df[target_col]","eed26dd1":"train_inputs","b6987ab9":"train_targets","db1b08d4":"val_inputs = val_df[input_cols]","05fe4a14":"val_targets = val_df[target_col]","fed3c575":"val_inputs","382e6d90":"val_targets","ff455fa4":"test_inputs = test_df[input_cols]","8dcd6051":"test_inputs","ec44a207":"train_df.to_parquet('train.parquet')","3c345cc1":"val_df.to_parquet('val.parquet')","10fbd040":"test_df.to_parquet('test.parquet')","4cb4fb73":"train_df = pd.read_parquet('test.parquet', engine='pyarrow')\nval_df = pd.read_parquet('test.parquet', engine='pyarrow')\ntest_df = pd.read_parquet('test.parquet', engine='pyarrow')","a54bcc25":"from sklearn.metrics import mean_squared_error\ndef evaluate(model):\n    train_preds = model.predict(train_inputs)\n    train_rmse = mean_squared_error(train_targets, train_preds, squared=False)\n    val_preds = model.predict(val_inputs)\n    val_rmse = mean_squared_error(val_targets, val_preds, squared=False)\n    return train_rmse, val_rmse, train_preds, val_preds","f10706b3":"def predict_and_submit(model, fname):\n    test_preds = model.predict(test_inputs)\n    sub_df = pd.read_csv(data_dir+'\/sample_submission.csv')\n    sub_df['fare_amount'] = test_preds\n    sub_df.to_csv(fname, index=None)\n    return sub_df","39e8bb2c":"from xgboost import XGBRegressor","8d499b31":"xgb_model_final = XGBRegressor(objective='reg:squarederror', n_jobs=-1, random_state=42,\n                               n_estimators=500, max_depth=5, learning_rate=0.1, \n                               subsample=0.8, colsample_bytree=0.8, tree_method= 'gpu_hist')","e6adddf1":"%%time\nxgb_model_final.fit(train_inputs, train_targets)","c032a3d4":"evaluate(xgb_model_final)","2114d143":"importance_df = pd.DataFrame({\n    'feature': train_inputs.columns,\n    'importance': xgb_model_final.feature_importances_\n}).sort_values('importance', ascending=False)","9caa8101":"def plot_importance(importance_df):\n    plt.figure(figsize=(10,6))\n    plt.title('Feature Importance')\n    sns.barplot(data=importance_df.head(10), x='importance', y='feature')","d5a429e4":"plot_importance(importance_df)","fcc007c8":"predict_and_submit(xgb_model_final, 'xgb_tuned_submission.csv')","05544f92":"import lightgbm as lgb","d19ec5b9":"dtrain = lgb.Dataset(train_inputs, label = train_targets, silent=True, free_raw_data=False)\ndval  = lgb.Dataset(val_inputs, label = val_targets, silent=True, free_raw_data=False)","4056c754":"params = {\n        'boosting_type':'gbdt',\n        'objective': 'regression',\n        'nthread': -1,\n        'verbose': -1,\n        'metric': 'rmse',\n    }","2c05991e":"lgbm_base_model = lgb.train(params, train_set = dtrain, valid_sets = [dval])","57202efc":"evaluate(lgbm_base_model)","cb79de56":"import optuna","fbed1699":"from optuna.integration import LightGBMPruningCallback\n\n\ndef objective(trial):\n    \n    param_grid = {\n        \"objective\": trial.suggest_categorical(\"objective\", [\"regression\"]),\n        'metric': trial.suggest_categorical(\"metric\", ['rmse']),\n        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", ['gbdt']),\n        \"verbose\" :trial.suggest_categorical(\"verbose\", [-1]),\n        \"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n        \"num_boost_round\": trial.suggest_categorical(\"num_boost_round\", [1000]),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 3000, step=20),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 200, 10000, step=100),\n        \"lambda_l1\": trial.suggest_int(\"lambda_l1\", 0, 100, step=5),\n        \"lambda_l2\": trial.suggest_int(\"lambda_l2\", 0, 100, step=5),\n        \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n        \"bagging_fraction\": trial.suggest_float(\n            \"bagging_fraction\", 0.2, 0.9, step=0.1\n        ),\n        \"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n        \"feature_fraction\": trial.suggest_float(\n            \"feature_fraction\", 0.2, 0.9, step=0.1\n        ),\n    }\n\n    pruning_callback = optuna.integration.LightGBMPruningCallback(trial, 'rmse')\n\n\n    model = lgb.train(param_grid , dtrain,\n        valid_sets = [dval],\n        early_stopping_rounds=100,\n        callbacks=[pruning_callback]\n    )\n    \n    val_preds = model.predict(val_inputs)\n    val_rmse = mean_squared_error(val_targets, val_preds, squared=False)\n\n\n    return val_rmse\n","e974b506":"study = optuna.create_study(direction=\"minimize\", study_name=\"LGBM Regressor\")\nfunc = lambda trial: objective(trial)\nstudy.optimize(func, n_trials=25)","c361da4f":"print(f\"\\tBest value (rmse): {study.best_value:.5f}\")\nprint(f\"\\tBest params:\")\n\nfor key, value in study.best_params.items():\n    print(f\"\\t\\t{key}: {value}\")","73f3cc99":"lgbm_final_model = lgb.train(study.best_params, train_set = dtrain, valid_sets = [dval], early_stopping_rounds=100)","d8bccb1b":"evaluate(lgbm_final_model)","1b06feb4":"importance_df = pd.DataFrame({\n    'feature': train_inputs.columns,\n    'importance': lgbm_final_model.feature_importance()\n}).sort_values('importance', ascending=False)","87a4877e":"plot_importance(importance_df)","86705e8a":"predict_and_submit(lgbm_final_model, 'lgbm_tuned_submission.csv')","9861e1f4":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","6c40e1b2":"train_inputs.shape[1]","70f19bfc":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\ntrain_scalin = scaler.fit_transform(train_inputs)\nval_scalin = scaler.transform(val_inputs)\ntest_scalin = scaler.transform(test_inputs)","dd11052a":"len(test_scalin)","00b61bcb":"from keras import backend as K\n\ndef root_mean_squared_error(y_true, y_pred):\n        return K.sqrt(K.mean(K.square(y_pred - y_true))) ","58e557c6":"# define a deep neural network model\ndef build_and_compile_model(dim):\n    model = keras.Sequential([\n\n      layers.Dense(128, activation='relu', input_dim=dim),\n      layers.BatchNormalization(),\n\n      layers.Dense(64, activation='relu'),\n      layers.BatchNormalization(),\n\n      layers.Dense(32, activation='relu'),\n      layers.BatchNormalization(),\n\n      layers.Dense(8, activation='relu'),\n      layers.BatchNormalization(),\n\n      layers.Dense(1)\n    ])\n\n    model.compile(loss=root_mean_squared_error,\n                optimizer=tf.keras.optimizers.Adam(0.001), metrics=['mae'])\n    return model\n","ccdd4035":"dnn_model = build_and_compile_model(dim=train_inputs.shape[1])\ndnn_model.summary()","94fede61":"ep_no = 50\nBatch = 128","f3f56d9b":"%%time\nhistory = dnn_model.fit(\n    train_scalin,\n    train_targets,\n    validation_data=(val_scalin, val_targets),\n    validation_steps=len(val_scalin) \/\/ Batch,\n    batch_size=    Batch,\n    epochs=ep_no, verbose=1)","a821a419":"plt.plot(history.history['loss'], label='loss')\nplt.plot(history.history['val_loss'], label='val_loss')\nplt.ylim([0, 10])\nplt.xlabel('Epoch')\nplt.ylabel('Error')\nplt.legend()\nplt.grid(True)","89ba9a69":"preds = dnn_model.predict(test_scalin, batch_size=Batch, verbose=1)","6c7388f3":"sub_df = pd.read_csv(data_dir+'\/sample_submission.csv')\nsub_df['fare_amount'] = preds\nsub_df.to_csv('DNN_Submission.csv', index=None)","a5aeb8b3":"### Load dataset","13187cb2":"## Step 2. Data Cleaning, Data Visualization and Feature Engineering\n\n- Basic info about training set\n- Basic info about test set\n- Remove noise and outliers\n- Exploratory data analysis & visualization\n- Ask & answer questions\n- Add features to dataset","50ba067d":"Throught days of a month fare seems to be uniformly distributed","80304d38":"trips are uniformly devided throught all days of week.","38d42710":"Places that are far away has higher taxi fare. Makes sense","fb67b4fe":"# New York City Taxi Fare Prediction\n<img src= \"https:\/\/miro.medium.com\/max\/1200\/1*-Oa3eUBRoF4uzvJkp9OV_Q.jpeg\" alt =\"Titanic\" style='width:8600px;'>\n\nImage Credit : <a href=\"https:\/\/medium.com\/analytics-vidhya\/new-york-city-taxi-fare-prediction-1ba96223ba7e\">Medium article<\/a>\n","30a873f5":"I'm using optuna for hyperparameter tuning. This is the first time i am using this in an kaggle competition.","4669f68c":" # Future Work\n \n - Claculate density for dropoff location and see how it can affect fare amount\n - Train on GPU with entire dataset using `dask`, `cudf` and `cuml`","1ec4edf2":"Fare_amount is negative in some rows and absurdly huge like $500 in some rows. I am removing these outliers.","c29aa0d0":"We can go to https:\/\/www.openstreetmap.org\/export#map=5\/51.500\/-0.100 to get the desired map.\nI have followed this medium article for getting the map and plotting pickup and dropoff location on that map. https:\/\/towardsdatascience.com\/easy-steps-to-plot-geographic-data-on-a-map-python-11217859a2db","4098fe3c":"As we can see training file is too large. so before loading it into dataframe let's take a look at training data using shell commands","7d6a1575":"### Test Set","814f7ef7":"Fare amount seems to steadily increasing by year as expected","49af4ee0":"Observations:\n\n- This is a supervised learning regression problem\n- Training data is 5.5 GB in size and consist of 55.4 M rows\n- Test set is much smaller (only \u2248 10 K rows)\n- 8 fetures present:\n    - `key` (unique ID field, used in submission)\n    - `pickup_datetime`\n    - `pickup_longitude`\n    - `pickup_latitude`\n    - `dropoff_longitude`\n    - `dropoff_latitude`\n    - `passenger_count`\n    - `fare_amount` (target column)\n\n- The test set has all columns except the target column.","169dfc29":"Fare is higher b\/w 3-6 am and 2-4 pm . It maybe possible that people living far away from their workplaces prefer to leave early to avoid rush hour","c492a0ae":"Let's define a helper function to evaluate models and generate test predictions","d0448307":"There seems to slight increase avg fare amount on sunday. Maybe people are going for weekend travel. Or maybe I am overthinking it??","27da154a":"### Scaling and One-Hot Encoding\n\nI am not going to do this because I'll be training tree-based models which are generally able to do a good job even without the above.","60763fa5":"These all coordinates lies on water. I've checked two of then using <a href=\"https:\/\/www.google.com\/maps\/place\/41%C2%B002'57.1%22N+73%C2%B016'51.6%22W\/@40.9199231,-73.5469829,8.24z\/data=!4m5!3m4!1s0x0:0x4004ca2e07ed014b!8m2!3d41.049183!4d-73.281006\">Google Maps<\/a>.\nLet's drop these data points","8bca2c1a":"Observations about training data:\n\n- missing data present\n- `fare_amount` is negative in some cols. That is not no realistic. So i will drop these rows from dataset. \n- There seem to be some errors in the latitude & longitude values\n- Dates range from 1st Jan 2009 to 30th June 2015\n- The dataset takes up ~15 MB of space in the RAM","a5d73238":"## 3. Prepare Dataset for Training\n\n- Split Training & Validation Set\n- Fill\/Remove Missing Values\n- Extract Inputs & Outputs\n   - Training\n   - Validation\n   - Test","0b9c8e7e":"### Training Dataset","daf16642":"It can be seen from previous plots that some location points are in the water. Let's try to remove them using a mask image of map in which land is shown in black and water in white. I've taken help from this notebook for that. https:\/\/www.kaggle.com\/breemen\/nyc-taxi-fare-data-exploration","eeda932a":"#### Validation","671ce004":"### 2. Light GBM\n\nhttps:\/\/lightgbm.readthedocs.io\/en\/latest\/Python-Intro.html","40f4cf6b":"Now because we have longitude and latitude. So We have plot these coordinate on a map to get a better view and see if there is some data to be cleaned.\nFirst let's define the Bounding Box from data of test dataset. Bounding Box is the area defined by two longitudes and two latitudes that will include all spatial points.","22dd253a":"### Save Intermediate DataFrames\n\nLet's save the processed datasets in the Apache Parquet format, so that I will be able to download them easily and continue model training on my lacal machine.\n\nThey can also be used to create seperate notebook for training and evaluating models after EDA, feature engineering.","27443d54":"Because I do not have to predict fare for trips that are outside that bounding box and we already have enough large dataset. So let's drop data points outside of bounding box","bbb5e62c":"Tuning Hyperparameters for LGBM\n\nhttps:\/\/lightgbm.readthedocs.io\/en\/latest\/pythonapi\/lightgbm.LGBMRegressor.html\n\nhttps:\/\/towardsdatascience.com\/kagglers-guide-to-lightgbm-hyperparameter-tuning-with-optuna-in-2021-ed048d9838b5","fe4c464a":"This model is giving fairly better predictions than the base xgb model. Let's also plot the graph to see the importance of features in pridictions.","7d80bca0":"## Gradient Boosting\n\n### 1. XGBoost\n\nhttps:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html#module-xgboost.sklearn","c6bcb035":"#### Test","1ea2f46d":"#### Training","c19667fb":"No. of trips are lowest from midnight to 5 am and highest in evening when there are people returning from their workplaces. Nothing unexpected. Time of the day also plays an important role.","c169c6ae":"### Extract Parts of Date\n\n- Year\n- Month\n- Day\n- Weekday\n- Hour","1900f2c1":"## Step 1. Loading the Dataset\n\n- Install required libraries\n- Download data from Kaggle or Use Kaggle Notebook for using data without downloading\n- View dataset files\n- Load training and test set with Pandas","0556fae4":"There are some rows in which passnger count is 0. So I am going to drop those rows. Although it is unlikely for taxi to have more than 5 passanger. Even if i consider some extereme scenario taxis can't carry more than 8 passangers. So Let's drop these data points.","70835be0":"### Add Distance Between Pickup and Drop\n\nI have calculated the haversine distance: \n- https:\/\/en.wikipedia.org\/wiki\/Haversine_formula\n- https:\/\/stackoverflow.com\/questions\/29545704\/fast-haversine-approximation-python-pandas","954218d0":"### Loading Training and test Dataset into pandas\n\nLoading the entire dataset into pandas dataframe will be slow, let's take following measures\n\n- Because 'key' column can not be used for prediction. Ignore it.\n- Parse pickup_datetime \n- Specify data types for other columns\n   - `uint8` for passenger count\n   - `float32` for geo coordinates\n   - `float32` for fare amount\n   \n- Only use 5% sample of the data for model training for now (\u22482.77M rows)","ff336d61":"### Add Distance From Popular Landmarks\n\n- JFK Airport\n- LGA Airport\n- EWR Airport\n- Times Square\n- Met Meuseum\n- World Trade Center\n\nWe'll add the distance from drop location. ","9d5dbae8":"### Split Training & Validation Set\n\nTime range for test set is also 2009-2015. So I'll set aside 20% of the training data as the validation set, to evaluate the models we train on previously unseen data. \n\nSince the test set and training set have the same date ranges, pick a random 20% fraction.\n","a72426ce":"Let's train a machine learning model to predict the fare for a taxi ride in New York city given information like pickup date & time, pickup location, drop location and no. of passengers. \n\nDataset Link: https:\/\/www.kaggle.com\/c\/new-york-city-taxi-fare-prediction\n","3e7b15fa":"### Extract Inputs and Outputs","6de10411":"Some observations about the test set:\n\n- 9914 rows of data\n- No missing values\n- No obvious data entry errors\n- 1 to 6 passengers (we can limit training data to this range)\n- Latitudes lie between 40 and 42\n- Longitudes lie between -75 and -72\n- Pickup dates range from Jan 1st 2009 to Jun  30th 2015 (same as training set)\n\nWe can use the ranges of the test set to drop outliers\/invalid data from the training set.","9ad6a49a":"### View Dataset Files","b1b8119e":"## 7. Train & Evaluate Different Models\n\nI will train each of the following & submit predictions to Kaggle:\n\n- Gradient Boosting\n- LightGBM\n- ANN\n\nCan also train Linear Regression, Random Forests for prediction","d8708459":"## ANN\n\nFinally, I'm training a neural network for this regression task. I will be using using a neural network of 4 hidden layers.\n\nhttps:\/\/www.tensorflow.org\/tutorials\/keras\/regression\n\nhttps:\/\/keras.io\/guides\/sequential_model\/"}}