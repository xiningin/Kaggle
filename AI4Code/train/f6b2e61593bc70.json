{"cell_type":{"7dfb7b49":"code","470f2c1a":"code","21dde376":"code","70430a90":"code","605a8f0b":"code","d468d17e":"code","87a6ebf1":"code","52fabeaa":"code","a2b5daf2":"code","6f2a11ad":"code","90cb1428":"code","d06890ca":"code","e3aa5301":"code","02c71611":"code","957e652f":"code","4e38b9cc":"code","22c8fe3b":"code","119dbc08":"code","e806955d":"code","d6e33e20":"code","fede4453":"code","b91a64a4":"code","b51717d2":"code","1b3fe78c":"code","d9e948ae":"code","29f5ab29":"code","d7b33c15":"code","9711bcc8":"code","a60ee93f":"code","cbd3fdba":"code","76365816":"code","33d27792":"code","d8f1ac86":"code","69a540a9":"code","d355899a":"markdown","b0393130":"markdown","7d362982":"markdown"},"source":{"7dfb7b49":"import sys\nimport math\nimport random\nfrom math import sqrt\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA","470f2c1a":"random.seed(0)","21dde376":"# Reading the input data\ndf_training = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')","70430a90":"# Preprocess the ticket field\ndef process_ticket(df):\n    df['TicketPrefix'] = df['Ticket']\n    df.loc[df['Ticket'].notnull(), 'TicketPrefix'] = df['Ticket'].apply(lambda x: x.split(' ')[0] \n                                                                                  if len(x.split(' ')) > 1\n                                                                                  else 'NUMBER')\n    \nprocess_ticket(df_training)\nprocess_ticket(df_test)","605a8f0b":"# For cabin I keep the first letter. There are multiple instances of rows having multiple assigned cabins. In these cases\n# the first letter is the same for all the assigned cabins, except in two cases in which we have:\n# F GXX\n# In this case, for simplicity, I decided to keep the F letter\ndef process_cabin(df):\n    df['CabinClass'] = df['Cabin']\n    df.loc[df['Cabin'].notnull(), 'CabinClass'] = df['Cabin'].apply(lambda x: str(x)[0])\n    \nprocess_cabin(df_training)\nprocess_cabin(df_test)","d468d17e":"# Imputing missing categorical variables\nfor c in ['CabinClass', 'Embarked']:\n    df_training.loc[df_training[c].isna(), c] = 'None'\n    df_test.loc[df_training[c].isna(), c] = 'None'","87a6ebf1":"# Imputing missing numerical variables\nimputed = df_training[np.isreal(df_training['Age'])]['Age'].median()\ndf_training.loc[(df_training['Age'].isna()) | (~np.isreal(df_training['Age'])), 'Age'] = imputed\ndf_test.loc[(df_test['Age'].isna()) | (~np.isreal(df_test['Age'])), 'Age'] = imputed\n\n# It turns out that the test data has a missing fare\nimputed = df_training[np.isreal(df_training['Fare'])]['Fare'].median()\ndf_test.loc[(df_test['Fare'].isna()) | (~np.isreal(df_test['Fare'])), 'Fare'] = imputed","52fabeaa":"dependent = 'Survived'\ncategorical = ['Pclass', 'Sex', 'TicketPrefix', 'CabinClass', 'Embarked']\nnumerical = ['Age', 'SibSp', 'Parch', 'Fare']","a2b5daf2":"# Processing categorical variables\nnew_categorical = []\nfor c in categorical:\n    values = df_training[c].unique()[:-1]\n    for v in values:\n        name = c + '_' + str(v)\n        df_training[name] = (df_training[c] == v).astype(int)\n        df_test[name] = (df_test[c] == v).astype(int)\n        new_categorical.append(name)\n    df_training = df_training.drop(c, axis = 1)\n    df_test = df_test.drop(c, axis = 1)\n\nvariables = new_categorical + numerical","6f2a11ad":"# Standardising variables\nstatistics = pd.concat((df_training.mean(), df_training.std()), axis = 1)\nstatistics.columns = ['mean', 'std']\n\nfor c in variables:\n    mean = statistics.loc[c, 'mean']\n    std = statistics.loc[c, 'std']\n    df_training[c] = (df_training[c] - mean) \/  std\n    df_test[c] = (df_test[c] - mean) \/  std","90cb1428":"# Removing redundant columns\nc = ['Name', 'Ticket', 'Cabin']\ndf_training = df_training.drop(c, axis = 1)\ndf_test = df_test.drop(c, axis = 1)","d06890ca":"df_training.head()","e3aa5301":"df_test.head()","02c71611":"# Dimnesionality reduction of the training data\nX = df_training.values[:, 2:]\npca = PCA(n_components = 2)\nX_pca = pca.fit_transform(X)","957e652f":"index_1 = df_training.Survived == 1\nindex_0 = df_training.Survived == 0","4e38b9cc":"fig, ax = plt.subplots()\nax.scatter(X_pca[index_1, 0], X_pca[index_1, 1], c = 'blue', alpha = 0.5)\nax.scatter(X_pca[index_0, 0], X_pca[index_0, 1], c = 'red', alpha = 0.5)\n_ = ax.legend(['Survived', 'Not survived'])","22c8fe3b":"X_test_pca = pca.transform(df_test.values[:, 1:])","119dbc08":"fig, ax = plt.subplots()\nax.scatter(X_pca[index_1, 0],  X_pca[index_1, 1], c = 'blue', alpha = 0.5)\nax.scatter(X_pca[index_0, 0], X_pca[index_0, 1], c = 'red', alpha = 0.5)\nax.scatter(X_test_pca[:, 0], X_test_pca[:, 1], c = 'gray', alpha = 0.5)\n_ = ax.legend(['Survived', 'Not survived', 'Test'])","e806955d":"# Estimating the parameters of the 'Survived' and 'Not survived' classes\nmean_0 = np.mean(X_pca[index_0, :], axis = 0)\nmean_1 = np.mean(X_pca[index_1, :], axis = 0)\ncov_0 = np.cov(X_pca[index_0, :].T)\ncov_1 = np.cov(X_pca[index_1, :].T)","d6e33e20":"def mahalanobis(p, mean, cov):\n    dif = p - mean\n    return math.sqrt(np.dot(np.dot(dif.T, np.linalg.inv(cov)), dif))","fede4453":"# Predictions for the test data\ny_test = []\nfor i in range(X_test_pca.shape[0]):\n    dist_0 = mahalanobis(X_test_pca[i, :], mean_0, cov_0)\n    dist_1 = mahalanobis(X_test_pca[i, :], mean_1, cov_1)\n    y_test.append(int(dist_1 < dist_0))","b91a64a4":"submission = df_test.copy()\nsubmission['Survived'] = y_test\nsubmission = submission[['PassengerId', 'Survived']]","b51717d2":"submission.head()","1b3fe78c":"# generating sets for 10-fold cross validation\nindexes = list(range(len(df_training)))\nrandom.shuffle(indexes)\nfolds = []\nfor i in range(10):\n    folds.append([])\nfor i in range(len(indexes)):\n    folds[i % 10].append(indexes[i])","d9e948ae":"def produce_training_test_set(df_training, train_indexes, test_indexes, column_indexes):\n    columns = df_training.columns[column_indexes]\n    datasets = {}\n    datasets['X_train'] = df_training.iloc[train_indexes][columns].values\n    datasets['X_test'] = df_training.iloc[test_indexes][columns].values\n    datasets['y_train'] = df_training.iloc[train_indexes]['Survived'].values\n    datasets['y_test'] = df_training.iloc[test_indexes]['Survived'].values\n    \n    return datasets","29f5ab29":"def evaluate(datasets):\n    if datasets['X_train'].shape[1] > 2:\n        pca = PCA(n_components = 2)\n        X_training_pca = pca.fit_transform(datasets['X_train'])\n        X_test_pca = pca.transform(datasets['X_test'])\n    else:\n        X_training_pca = datasets['X_train']\n        X_test_pca = datasets['X_test']\n        \n    mean_0 = np.mean(X_training_pca[datasets['y_train'] == 0, :], axis = 0)\n    mean_1 = np.mean(X_training_pca[datasets['y_train'] == 1, :], axis = 0)\n    cov_0 = np.cov(X_training_pca[datasets['y_train'] == 0, :].T)\n    cov_1 = np.cov(X_training_pca[datasets['y_train'] == 1, :].T)\n    \n    y_pred = []\n    for i in range(X_test_pca.shape[0]):\n        if datasets['X_train'].shape[1] > 1:\n            dist_0 = mahalanobis(X_test_pca[i, :], mean_0, cov_0)\n            dist_1 = mahalanobis(X_test_pca[i, :], mean_1, cov_1)\n        else:\n            dist_0 = (X_test_pca[i, :] - mean_0) \/ cov_0\n            dist_1 = (X_test_pca[i, :] - mean_1) \/ cov_1\n        y_pred.append(int(dist_1 < dist_0))\n        \n    return sqrt(np.sum(np.power(np.array(y_pred) - np.array(datasets['y_test']), 2)))","d7b33c15":"def k_fold_cross_validation(df_training, folds, column_indexes):\n    error = 0\n    \n    for k in range(10):\n        train_indexes = []\n        for j in range(10):\n            if j == k:\n                test_indexes = folds[j]\n            else:\n                train_indexes = train_indexes + folds[j]\n                \n        datasets = produce_training_test_set(df_training, train_indexes, test_indexes, column_indexes)\n        \n        error = error + evaluate(datasets)\n        \n    return error \/ 10.0","9711bcc8":"column_indexes = list(range(2, 62))\nk_fold_cross_validation(df_training, folds, column_indexes)","a60ee93f":"# Forward selection\npending = list(range(2, 62))\nmodel = []\nmin_error = sys.float_info.max\nwhile len(pending) > 0:\n    \n    prev_error = min_error\n    min_error = sys.float_info.max\n    \n    for i in pending:\n        new_model = model + [i]\n        error = k_fold_cross_validation(df_training, folds, new_model)\n        \n        if error < min_error:\n            min_error = error\n            best_model = new_model\n            feature = i\n            \n    if min_error < prev_error:\n        print('Selecting feature ' + df_training.columns[feature] + ' - error decreased to ' + str(min_error))\n        model = best_model\n        pending.remove(feature)\n    else:\n        print('END')\n        break","cbd3fdba":"columns = df_training.columns[model]\n\npca = PCA(n_components = 2)\nX_training_pca = pca.fit_transform(df_training[columns].values)\nX_test_pca = pca.transform(df_test[columns].values)\n\nmean_0 = np.mean(X_training_pca[index_0, :], axis = 0)\nmean_1 = np.mean(X_training_pca[index_1, :], axis = 0)\ncov_0 = np.cov(X_training_pca[index_0, :].T)\ncov_1 = np.cov(X_training_pca[index_1, :].T)\n\ny_pred = []\nfor i in range(X_test_pca.shape[0]):\n    dist_0 = mahalanobis(X_test_pca[i, :], mean_0, cov_0)\n    dist_1 = mahalanobis(X_test_pca[i, :], mean_1, cov_1)\n    y_pred.append(int(dist_1 < dist_0))","76365816":"fig, ax = plt.subplots()\nax.scatter(X_training_pca[index_1, 0], X_training_pca[index_1, 1], c = 'blue', alpha = 0.5)\nax.scatter(X_training_pca[index_0, 0], X_training_pca[index_0, 1], c = 'red', alpha = 0.5)\nax.set_yscale('symlog')\n_ = ax.legend(['Survived', 'Not survived', 'Test'])","33d27792":"submission = df_test.copy()\nsubmission['Survived'] = y_pred\nsubmission = submission[['PassengerId', 'Survived']]","d8f1ac86":"submission.head()","69a540a9":"submission.to_csv('mahalanobis_forward_selection.csv', index = False)","d355899a":"The only purpose of this code is to practice the concept of Mahalanobis distance. I built a simple nearest centroid classifier based on it on dimensionally reduced data. Originally I wanted to apply t-SNE, but scikit-learn's implementation cannot transform data using an already fitted TSNE object. Therefore, I used PCA for dimensionality reduction.  \n\nThe Mahalanobis distance is a measure of the distance between an observation and a distribution. Is a multi-dimensional generalisation of the idea of measuring how many standard deviations is an observation away from the mean of a distribution. It takes into account the correlations in the dataset, since the distance increases as we move along the principal component axis.\n\n## Data preprocessing\n\nThis section contains the code I used in my Logistic Regression kernel to preprocessed the Titanic data and get it ready to use. ","b0393130":"## 1NN classification based on PCA and Mahalanobis distance","7d362982":"This submission achieved a 64.59% accuracy (much lower than the accuracy I got with logistic regression.) There's a big overlap between the classes. However, I obtained the best results after applying forward selection. \n\n## Forward selection\n\nWith this submission I got a 74.16% predicion accuracy. This accuracy is very close to that I got by using logistic regression after forward selection. This once more demonstrate the importance of feature engineering. It is quite suprising to see how such a simple approach can work almost as well as logistic regression. "}}