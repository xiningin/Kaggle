{"cell_type":{"bbfa4f2d":"code","530df262":"code","18f34382":"code","45093751":"code","a066591b":"code","89c22d01":"code","2df53efb":"code","dd41858d":"code","06815183":"code","719b80b0":"code","f6d7c147":"code","8268a426":"code","6fcec1a0":"code","44cf062a":"code","a4e90793":"code","412c5306":"code","ef7b5a49":"code","a2dc5879":"code","02c4fe55":"code","e4958b87":"markdown","49bc3973":"markdown","c79ade18":"markdown","91a8ef1e":"markdown","f4c7f079":"markdown","4a56abbc":"markdown","f4a734b5":"markdown","f66a717a":"markdown","21d91f6b":"markdown","47b3fca9":"markdown","a046e662":"markdown","156ec292":"markdown","3ab3e417":"markdown","ab96394a":"markdown"},"source":{"bbfa4f2d":"!pip install \/kaggle\/input\/pyfftw0120-wheel\/pyFFTW-0.12.0-cp37-cp37m-manylinux1_x86_64.whl\n\n\nimport multiprocessing\nimport os\nimport typing as tp\nimport warnings\nfrom functools import reduce\nfrom pathlib import Path\n\nimport librosa\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torchvision\nfrom pyfftw import empty_aligned\nfrom pyfftw.builders import rfft as rfft_builder","530df262":"TRAIN_DIR = Path(\"..\/input\/birdcalldatasetnpy\/train_resampled_npy\")\nSAMPLE_RATE = 32_000","18f34382":"plt.imshow(\n    np.load(\"..\/input\/birdcalldatasetnpy\/train_resampled_npy\/aldfly\/XC135454.npy\")[:, :256]\n)","45093751":"def spectrogram(\n    samples, sample_rate, frame_len, fps, batch=48, dtype=None, bins=None, plans=None\n):\n    \"\"\"\n    Computes a magnitude spectrogram for a given vector of samples at a given\n    sample rate (in Hz), frame length (in samples) and frame rate (in Hz).\n    Allows to transform multiple frames at once for improved performance (with\n    a default value of 48, more is not always better). Returns a numpy array.\n    Allows to return a limited number of bins only, with improved performance\n    over discarding them afterwards. Optionally accepts a set of precomputed\n    plans created with spectrogram_plans(), required when multi-threading.\n    \"\"\"\n    if dtype is None:\n        dtype = samples.dtype\n    if bins is None:\n        bins = frame_len \/\/ 2 + 1\n    if len(samples) < frame_len:\n        return np.empty((0, bins), dtype=dtype)\n    if plans is None:\n        plans = spectrogram_plans(frame_len, batch, dtype)\n    rfft1, rfft, win = plans\n    hopsize = int(sample_rate \/\/ fps)\n    num_frames = (len(samples) - frame_len) \/\/ hopsize + 1\n    nabs = np.abs\n    naa = np.asanyarray\n    if batch > 1 and num_frames >= batch and samples.flags.c_contiguous:\n        frames = np.lib.stride_tricks.as_strided(\n            samples,\n            shape=(num_frames, frame_len),\n            strides=(samples.strides[0] * hopsize, samples.strides[0]),\n        )\n        spect = [\n            nabs(rfft(naa(frames[pos : pos + batch :], dtype) * win)[:, :bins])\n            for pos in range(0, num_frames - batch + 1, batch)\n        ]\n        samples = samples[(num_frames \/\/ batch * batch) * hopsize : :]\n        num_frames = num_frames % batch\n    else:\n        spect = []\n    if num_frames:\n        spect.append(\n            np.vstack(\n                [\n                    nabs(rfft1(naa(samples[pos : pos + frame_len :], dtype) * win)[:bins:])\n                    for pos in range(0, len(samples) - frame_len + 1, hopsize)\n                ]\n            )\n        )\n    return np.vstack(spect) if len(spect) > 1 else spect[0]\n\n\ndef create_mel_filterbank(sample_rate, frame_len, num_bands, min_freq, max_freq):\n    \"\"\"\n    Creates a mel filterbank of `num_bands` triangular filters, with the first\n    filter starting at `min_freq` and the last one stopping at `max_freq`.\n    Returns the filterbank as a matrix suitable for a dot product against\n    magnitude spectra created from samples at a sample rate of `sample_rate`\n    with a window length of `frame_len` samples.\n    \"\"\"\n    # prepare output matrix\n    input_bins = (frame_len \/\/ 2) + 1\n    filterbank = np.zeros((input_bins, num_bands))\n\n    # mel-spaced peak frequencies\n    min_mel = 1127 * np.log1p(min_freq \/ 700.0)\n    max_mel = 1127 * np.log1p(max_freq \/ 700.0)\n    spacing = (max_mel - min_mel) \/ (num_bands + 1)\n    peaks_mel = min_mel + np.arange(num_bands + 2) * spacing\n    peaks_hz = 700 * (np.exp(peaks_mel \/ 1127) - 1)\n    fft_freqs = np.linspace(0, sample_rate \/ 2.0, input_bins)\n    peaks_bin = np.searchsorted(fft_freqs, peaks_hz)\n\n    # fill output matrix with triangular filters\n    for b, filt in enumerate(filterbank.T):\n        # The triangle starts at the previous filter's peak (peaks_freq[b]),\n        # has its maximum at peaks_freq[b+1] and ends at peaks_freq[b+2].\n        left_hz, top_hz, right_hz = peaks_hz[b : b + 3]  # b, b+1, b+2\n        left_bin, top_bin, right_bin = peaks_bin[b : b + 3]\n        # Create triangular filter compatible to yaafe\n        filt[left_bin:top_bin] = (fft_freqs[left_bin:top_bin] - left_hz) \/ (\n            top_bin - left_bin\n        )\n        filt[top_bin:right_bin] = (right_hz - fft_freqs[top_bin:right_bin]) \/ (\n            right_bin - top_bin\n        )\n        filt[left_bin:right_bin] \/= filt[left_bin:right_bin].sum()\n\n    return filterbank\n\n\ndef spectrogram_plans(frame_len, batch=48, dtype=np.float32):\n    \"\"\"\n    Precompute plans for spectrogram(), for a given frame length, batch size\n    and dtype. Returns two plans (single spectrum and batch), and a window.\n    \"\"\"\n    input_array = empty_aligned((batch, frame_len), dtype=dtype)\n    win = np.hanning(frame_len).astype(dtype)\n    return (rfft_builder(input_array[0]), rfft_builder(input_array), win)\n\n\nfilterbank = create_mel_filterbank(SAMPLE_RATE, 256, 80, 27.5, 10000)\n\n\ndef audio_to_melspec(audio):\n    spec = spectrogram(audio, SAMPLE_RATE, 256, 128)\n    return (spec @ filterbank).T","a066591b":"BS = 100\nMAX_LR = 1e-3\n\nclasses = [directory.name for directory in TRAIN_DIR.iterdir()]\ntrain_items = []\n\nfor directory in TRAIN_DIR.iterdir():\n    ebird_code = directory.name\n    for recording in directory.iterdir():\n        train_items.append((ebird_code, recording))","89c22d01":"class TrainDataset(torch.utils.data.Dataset):\n    def __getitem__(self, idx: int) -> tp.Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Get:\n        - three random segments of the audio file, stacked\n        - one-hot-encoded version of target label\n        \"\"\"\n        cls, path = train_items[idx]\n        example = self.get_spec(path)\n        return example, self.one_hot_encode(cls)\n\n    def get_spec(self, path: Path) -> np.ndarray:\n        \"\"\"\n        x is of shape (80, ?). Lets call this (width, height).\n\n        If x's height is more than 212, then we select a random\n        segment of length 212 from x.\n        If it's smaller, then we randomly pad it with zeros so\n        that it becomes of length 212.\n        We repeat this three times, so that we end up with three\n        (possibly overlapping) segments of shape (80, 212).\n        \"\"\"\n        frames_per_spec = 212\n        n_specs = 3\n        x = np.load(path)\n        width, height = x.shape\n        assert width == 80\n\n        specs = []\n        for _ in range(n_specs):\n            if x.shape[1] < frames_per_spec:\n                spec = np.zeros((80, frames_per_spec))\n                start_frame = np.random.randint(frames_per_spec - x.shape[1])\n                spec[:, start_frame : start_frame + x.shape[1]] = x\n            else:\n                start_frame = int(np.random.rand() * (x.shape[1] - frames_per_spec))\n                spec = x[:, start_frame : start_frame + frames_per_spec]\n            specs.append(spec)\n\n        stacked_specs = np.stack(specs)\n        assert stacked_specs.shape == (n_specs, 80, frames_per_spec)\n\n        return stacked_specs.astype(np.float32)\n\n    def show(self, idx: int):\n        x = self[idx][0]\n        return plt.imshow(x.transpose(1, 2, 0)[:, :, 0])\n\n    def one_hot_encode(self, cls: str) -> np.ndarray:\n        y = classes.index(cls)\n        one_hot = np.zeros((len(classes)))\n        one_hot[y] = 1\n        return one_hot\n\n    def __len__(self):\n        return len(train_items)\n\n\ntrain_ds = TrainDataset()\n\ntrain_dl = torch.utils.data.DataLoader(\n    train_ds,\n    batch_size=BS,\n    num_workers=multiprocessing.cpu_count(),\n    pin_memory=True,\n    shuffle=True,\n)","2df53efb":"pretrained_res34 = torchvision.models.resnet34(False)\npretrained_res34.load_state_dict(\n    torch.load(\"..\/input\/pretrained-pytorch\/resnet34-333f7ec4.pth\")\n)\n\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bn = torch.nn.BatchNorm2d(3)\n        self.cnn = torch.nn.Sequential(\n            *list(pretrained_res34.children())[:-2], torch.nn.AdaptiveMaxPool2d(1)\n        )\n        self.classifier = torch.nn.Sequential(\n            *[\n                torch.nn.Linear(512, 512),\n                torch.nn.ReLU(),\n                torch.nn.Dropout(p=0.2),\n                torch.nn.BatchNorm1d(512),\n                torch.nn.Linear(512, 512),\n                torch.nn.ReLU(),\n                torch.nn.Dropout(p=0.2),\n                torch.nn.BatchNorm1d(512),\n                torch.nn.Linear(512, len(classes)),\n            ]\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass through network.\n\n        Will:\n\n        - normalise (between 0 and 1) and reshape the input so\n          it can be fed into the cnn\n        - reshape and feed into the classifier\n        \"\"\"\n\n        batch_size, *shape = x.shape\n        assert batch_size <= BS\n        assert shape == [3, 80, 212]\n\n        x_reshaped = x.view(x.shape[0], -1)\n        assert x_reshaped.shape == (batch_size, reduce(lambda i, j: i * j, shape))\n\n        max_per_example, _ = x_reshaped.max(1)\n        assert max_per_example.shape == (batch_size,)\n\n        nonzero_mask = max_per_example != 0\n        x[nonzero_mask] \/= max_per_example[nonzero_mask][:, None, None, None]\n\n        cnn_output = self.cnn(x)\n        assert cnn_output.shape == (batch_size, 512, 1, 1)\n\n        x = cnn_output.squeeze(3).squeeze(2)\n        assert x.shape == (batch_size, 512)\n\n        x = self.classifier(x)\n        assert x.shape == (batch_size, len(classes))\n        return x\n\n\nmodel = Model().cuda()","dd41858d":"for param in model.cnn.parameters():\n    param.requires_grad = False","06815183":"def train(num_epochs):\n    model.train()\n    criterion = torch.nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)\n    for epoch in range(num_epochs):\n        for data in train_dl:\n            inputs, labels = data[0].cuda(), data[1].cuda()\n            optimizer.zero_grad()\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            if np.isnan(loss.item()):\n                raise Exception(f\"!!! nan encountered in loss !!! epoch: {epoch}\\n\")\n            loss.backward()\n            optimizer.step()\n            scheduler.step()","719b80b0":"%%time\n\ntrain(30)","f6d7c147":"%%time\n\nfor param in model.cnn.parameters():\n    param.requires_grad = True\n\ntrain(60)","8268a426":"TEST_PATH = (\n    Path(\"..\/input\/birdsong-recognition\")\n    if os.path.exists(\"..\/input\/birdsong-recognition\/test_audio\")\n    else Path(\"..\/input\/birdcall-check\")\n)\n\nTEST_AUDIO_PATH = TEST_PATH \/ \"test_audio\"\ntest_df = pd.read_csv(TEST_PATH \/ \"test.csv\")","6fcec1a0":"class AudioDataset(torch.utils.data.Dataset):\n    def __init__(self, items, classes, rec):\n        self.items = items\n        self.vocab = classes\n        self.rec = rec\n\n    def __getitem__(self, idx):\n        _, rec_fn, start = self.items[idx]\n        x = self.rec[start * SAMPLE_RATE : (start + 5) * SAMPLE_RATE]\n        example = self.get_specs(x)\n        return example.astype(np.float32)\n\n    def get_specs(self, x):\n        xs = []\n        for i in range(3):\n            start_frame = int(i * 1.66 * SAMPLE_RATE)\n            xs.append(x[start_frame : start_frame + int(1.66 * SAMPLE_RATE)])\n\n        specs = []\n        for x in xs:\n            specs.append(audio_to_melspec(x))\n        return np.stack(specs).reshape(3, 80, 212)\n\n    def show(self, idx):\n        x = self[idx][0]\n        return plt.imshow(x.transpose(1, 2, 0)[:, :, 0])\n\n    def __len__(self):\n        return len(self.items)","44cf062a":"%%time\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\nrow_ids = []\nall_preds = []\n\nmodel.eval()\nfor audio_id in test_df[test_df.site.isin([\"site_1\", \"site_2\"])].audio_id.unique():\n    items = [\n        (row.row_id, row.audio_id, int(row.seconds) - 5)\n        for idx, row in test_df[test_df.audio_id == audio_id].iterrows()\n    ]\n    rec = librosa.load(\n        TEST_AUDIO_PATH \/ f\"{audio_id}.mp3\", sr=SAMPLE_RATE, res_type=\"kaiser_fast\"\n    )[0]\n    test_ds = AudioDataset(items, classes, rec)\n    dl = torch.utils.data.DataLoader(test_ds, batch_size=64)\n    for batch in dl:\n        with torch.no_grad():\n            preds = model(batch.cuda()).sigmoid().cpu().detach()\n            all_preds.append(preds)\n    row_ids += [item[0] for item in items]","a4e90793":"%%time\n\nfor audio_id in test_df[test_df.site == \"site_3\"].audio_id.unique():\n    rec = librosa.load(\n        TEST_AUDIO_PATH \/ f\"{audio_id}.mp3\", sr=SAMPLE_RATE, res_type=\"kaiser_fast\"\n    )[0]\n    # assume only one row per recording for site_3\n    current_row = test_df[test_df.audio_id == audio_id].iloc[0]\n    duration = rec.shape[0] \/\/ SAMPLE_RATE\n    items = [\n        (current_row.row_id, current_row.audio_id, start_sec)\n        for start_sec in [0 + i * 5 for i in range(duration \/\/ 5)]\n    ]\n    test_ds = AudioDataset(items, classes, rec)\n    dl = torch.utils.data.DataLoader(test_ds, batch_size=64)\n\n    preds_for_site = []\n    for batch in dl:\n        with torch.no_grad():\n            preds = model(batch.cuda()).sigmoid().cpu().detach()\n            preds_for_site.append(preds)\n\n    row_ids.append(current_row.row_id)\n    preds_for_site_tensor = torch.cat(preds_for_site)\n    assert preds_for_site_tensor.shape == (len(items), len(classes))\n    current_pred = preds_for_site_tensor.max(0)[0].unsqueeze(0)\n    assert current_pred.shape == (1, len(classes))\n    all_preds.append(current_pred)","412c5306":"all_preds_tensor = torch.cat(all_preds)\nassert all_preds_tensor.shape == (len(test_df), len(classes))","ef7b5a49":"%%time\nthresh = 1.0\nminimum_prediction_rate = 0.04\n\n# Make sure that we are making a prediction for at least\n# `minimum_prediction_rate` of the rows.\nwhile (all_preds_tensor > thresh).any(1).float().mean() < minimum_prediction_rate:\n    thresh -= 0.001","a2dc5879":"results = []\n\nfor row in all_preds_tensor:\n    birds = []\n    for idx in np.where(row > thresh)[0]:\n        birds.append(classes[idx])\n    if not birds:\n        birds = [\"nocall\"]\n    results.append(\" \".join(birds))","02c4fe55":"predicted = pd.DataFrame(data={\"row_id\": row_ids, \"birds\": results})\npredicted.to_csv(\"submission.csv\", index=False)","e4958b87":"To generate the spectrograms, I used [code](https:\/\/github.com\/f0k\/birdclef2018\/blob\/master\/experiments\/audio.py) shared by Jan Schl\u00fcter.\n\n\nLet me copy it over here, we will need it down the road when we predict on the test set.","49bc3973":"This is what ~2 seconds of audio converted to a spectrogram looks like.","c79ade18":"To train, we will use PyTorch. Let's put together a dataset we will be able to use to train our model.","91a8ef1e":"We are nearly ready to train! We still need a model and a training loop.\n\nFor the model, let us use an architecture based on resnet34 with pretrained weights.","f4c7f079":"Our dataset that we will use for inference will need to be able to work with audio files.","4a56abbc":"For training, we will use audio files I preprocessed to spectrograms and saved as numpy arrays. They are not compressed and should be very fast to read. Each spectrogram captures under 30 initial seconds of one of the files in train.","f4a734b5":"With the model out of the way, time to implement the training loop and start training!\n\nTo speed up the training and improve our results, let's first train just the new classifier we have created, keeping the convolutional part of our model frozen.","f66a717a":"And let's train!","21d91f6b":"# NOTE\n\nI'm just trying to understand [this](https:\/\/www.kaggle.com\/radek1\/esp-starter-pack-from-training-to-submission\/notebook) excellent notebook and so am trying to refactor it \/ clean it up in a way that makes sense to me.\n\nAm using [nbQA](https:\/\/github.com\/nbQA-dev\/nbQA) for a few code-quality checks (currently `black`, `flake8`, `isort` and `mypy`).\n\nPlease upvote the original notebook (linked above) if you find this useful.","47b3fca9":"Our classifier head is now trained! This means that when we start training the entire model, gradients update will not initally mess lower layers of our architecture too much. This technique called progressive unfreezing is extremely valuable. You can read more about it in [Universal Language Model Fine-tuning for Text Classification](https:\/\/arxiv.org\/abs\/1801.06146) by Jeremy Howard and Sebastian Ruder.\n\nLet's now unfreeze our model and train the entire arch.","a046e662":"Let's predict on the test set and output predictions!","156ec292":"Here is our training loop","3ab3e417":"With training out of the way, its time to predict on the test set. To help us structure our work, let's use the extremely helpful [custom check phase](https:\/\/www.kaggle.com\/c\/birdsong-recognition\/discussion\/159993) shared by [Alex Shonenkov](https:\/\/www.kaggle.com\/shonenkov).","ab96394a":"In this notebook we will start with training and go all the way to submission \ud83d\ude42 This will put some constraint on our code - the plan is for it to be fairly easy to read and to lend itself well to modifications. Let's get started!"}}