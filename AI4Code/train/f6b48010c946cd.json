{"cell_type":{"144df443":"code","89d63331":"code","6041c977":"code","c354b0f4":"code","c20ba985":"code","e6b5ba85":"code","7c0d2101":"code","320b74de":"code","878ab504":"code","934f5595":"code","db71f5b4":"code","95627212":"code","1eeb6b37":"code","7fef80eb":"code","09ca2d15":"code","6a0d1904":"code","f192fe7a":"code","6f82afab":"code","468c6b39":"code","d58a4e7c":"code","64172f67":"code","d000bf7d":"markdown","a9da9619":"markdown"},"source":{"144df443":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","89d63331":"import pandas as pd\n\ndf = pd.read_csv('..\/input\/lyrics-generation\/train_lyrics_1000.csv')\ndf1 =pd.read_csv('..\/input\/lyrics-generation\/valid_lyrics_200.csv')\nprint(df1.head())\nprint(df1['lyrics'][0])\n\ndf.head()","6041c977":"from sklearn.preprocessing import LabelEncoder\nfrom joblib import dump, load\nimport numpy as np\n\nX_train = df['lyrics'].values \n\ny_train = df['mood'].values\n\nX_valid = df['lyrics'].values \n\ny_valid = df['mood'].values\n\nprint('before embeding training data: %s ...' %y_train[:5])\nprint('before encoding validation data: %s ...' %y_valid[:5])\n\nle = LabelEncoder()\nle.fit(y_train)\nle.fit(y_valid)\ny_train = le.transform(y_train)\ny_valid =le.transform(y_valid)\n\nprint('after encoding train data: %s ...' %y_train[:5])\nprint('after encoding validation data: %s ...' %y_valid[:5])","c354b0f4":"dump(le, 'label_encoder.joblib') ","c20ba985":"from tensorflow.python.keras.preprocessing.text import Tokenizer\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences\n","e6b5ba85":"tokenizer_obj = Tokenizer()\ntotal_lyrics =X_train + X_valid\ntokenizer_obj.fit_on_texts(total_lyrics)","7c0d2101":"#pad sequences\nmax_length = max([len(s.split()) for s in total_lyrics])","320b74de":"#define vocabulary size\nvocab_size = len(tokenizer_obj.word_index) + 1","878ab504":"X_train_tokens =tokenizer_obj.texts_to_sequences(X_train)\nX_valid_tokens =tokenizer_obj.texts_to_sequences(X_valid)\n","934f5595":"X_train_pad =pad_sequences(X_train_tokens,maxlen = max_length,padding =\"post\")\nX_valid_pad =pad_sequences(X_valid_tokens,maxlen = max_length,padding =\"post\")","db71f5b4":"from keras.models import Sequential\nfrom keras.layers import  Dense, Embedding, LSTM,GRU\nfrom keras.layers.embeddings import Embedding  ","95627212":"EMBEDDING_DIM = 100\nprint(\"Build model.............\")\nmodel = Sequential( ) \nmodel.add(Embedding(vocab_size,EMBEDDING_DIM,input_length=max_length)) \nmodel.add(GRU(units=32, dropout=0.2,recurrent_dropout =0.2))\nmodel.add(Dense(1,activation='sigmoid'))\n#using different optimizers and different optimizer configs \n","1eeb6b37":"model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary","7fef80eb":"print(\"trainning the model....\")\nmodel.fit(X_train_pad,y_train, batch_size= 128, epochs=25, validation_data =(X_valid_pad,y_valid),verbose=2)","09ca2d15":"# validating on test data and obtaning the sentiment of lyrics\ntest_samples =[df1['lyrics'][0],df1['lyrics'][1],df1['lyrics'][2],df1['lyrics'][3],df1['lyrics'][4],df1['lyrics'][5]]\ntest_samples_tokens = tokenizer_obj.texts_to_sequences(test_samples)\ntest_samples_tokens_pad =pad_sequences(test_samples_tokens,maxlen =max_length)\n","6a0d1904":"model.predict(test_samples_tokens_pad)","f192fe7a":"# now trainning our model on word to vec embedding as feature for context\n\nimport string \nfrom nltk.tokenize import word_tokenize \nfrom nltk.corpus import stopwords \nreview_lines = list() \nlines =df['lyrics'].values.tolist() \nfor line in lines: \n    tokens = word_tokenize(line) # convert to Lower case \n    tokens = [w. lower() for w in tokens] # remove punctuation from each word\n    table=str.maketrans('','',string.punctuation )\n    stripped =[w.translate(table) for w in tokens]\n    \n    words =[word for word in stripped if word.isalpha()] \n    stop_words = set(stopwords.words( 'english') ) \n    words = [w for w in words if not w in stop_words] \n    review_lines.append(words) ","6f82afab":"len(review_lines)","468c6b39":"import gensim # train word2vec model \nmodel = gensim.models.Word2Vec(sentences=review_lines,size =EMBEDDING_DIM, window=5,workers=4,min_count=1) \n# vocab size '\nwords = list(model.wv.vocab)\nprint( 'Vocabulary size: %d' %len(words))","d58a4e7c":"from gensim.scripts.glove2word2vec import glove2word2vec\nglove_input_file = '..\/input\/nlpword2vecembeddingspretrained\/glove.6B.100d.txt'\nword2vec_output_file = 'glove.6B.100d.txt.word2vec'\nglove2word2vec(glove_input_file, word2vec_output_file)\n","64172f67":"# num_features = 300  # Word vector dimensionality\n# min_word_count = 40 # Minimum word count\n# num_workers = 4     # Number of parallel threads\n# context = 10        # Context window size\n# downsampling = 1e-3 # (0.001) Downsample setting for frequent words\n\n# # Initializing the train model\n# from gensim.models import word2vec\n# print(\"Training model....\")\n# model = word2vec.Word2Vec(sentences,\\\n#                           workers=num_workers,\\\n#                           size=num_features,\\\n#                           min_count=min_word_count,\\\n#                           window=context,\n#                           sample=downsampling)\n\n# # To make the model memory efficient\n# model.init_sims(replace=True)\n\n# # Saving the model for later use. Can be loaded using Word2Vec.load()\n# model_name = \"300features_40minwords_10context\"\n# model.save(model_name)","d000bf7d":"**Learn Word Embedding**\nThe word embeddings of our dataset can be learned while training a neural network on the classification problem. Before it can be presented to the network, the text data is first encoded so that each word is represented by a unique integer. This data preparation step can be performed using the Tokenizer API provided with Keras. We add padding to make all the vectors of same length (max_length). Below code converts the text to integer indexes, now ready to be used in Keras embedding layer.","a9da9619":"**Build Model**\nWe are now ready to define our neural network model. The model will use an Embedding layer as the first hidden layer. The Embedding layer is initialized with random weights and will learn an embedding for all of the words in the training dataset during training of the model."}}