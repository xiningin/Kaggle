{"cell_type":{"707bb649":"code","0c8f307d":"code","72f75066":"code","dbc4f891":"code","d89825eb":"code","22664d6d":"code","75f4f07a":"markdown","84c21225":"markdown","ddc5630b":"markdown","b1bdf616":"markdown","701abfd8":"markdown"},"source":{"707bb649":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","0c8f307d":"def add_noise(series, noise_level):\n    return series * (1 + noise_level * np.random.randn(len(series)))\n\ndef target_encode(trn_series=None, \n                  tst_series=None, \n                  target=None, \n                  min_samples_leaf=1, \n                  smoothing=1,\n                  noise_level=0):\n    \"\"\"\n    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n    https:\/\/kaggle2.blob.core.windows.net\/forum-message-attachments\/225952\/7441\/high%20cardinality%20categoricals.pdf\n    trn_series : training categorical feature as a pd.Series\n    tst_series : test categorical feature as a pd.Series\n    target : target data as a pd.Series\n    min_samples_leaf (int) : minimum samples to take category average into account\n    smoothing (int) : smoothing effect to balance categorical average vs prior  \n    \"\"\" \n    assert len(trn_series) == len(target)\n    assert trn_series.name == tst_series.name\n    temp = pd.concat([trn_series, target], axis=1)\n    # Compute target mean \n    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n    # Compute smoothing\n    smoothing = 1 \/ (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) \/ smoothing))\n    # Apply average function to all target data\n    prior = target.mean()\n    # The bigger the count the less full_avg is taken into account\n    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n    # Apply averages to trn and tst series\n    ft_trn_series = pd.merge(\n        trn_series.to_frame(trn_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=trn_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_trn_series.index = trn_series.index \n    ft_tst_series = pd.merge(\n        tst_series.to_frame(tst_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=tst_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_tst_series.index = tst_series.index\n    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)","72f75066":"# reading data\ntrn_df = pd.read_csv(\"..\/input\/train.csv\", index_col=0)\nsub_df = pd.read_csv(\"..\/input\/test.csv\", index_col=0)\n\ntrn_df.head()","dbc4f891":"# Target encode ps_car_11_cat\ntrn, sub = target_encode(trn_df[\"ps_car_11_cat\"], \n                         sub_df[\"ps_car_11_cat\"], \n                         target=trn_df.target, \n                         min_samples_leaf=100,\n                         smoothing=10,\n                         noise_level=0.01)\ntrn.head(10)","d89825eb":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.scatter(trn_df[\"ps_car_11_cat\"], trn)\nplt.xlabel(\"ps_car_11_cat category values\")\nplt.ylabel(\"Noisy target encoding\")","22664d6d":"from sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\n\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\nf_cats = [f for f in trn_df.columns if \"_cat\" in f]\nprint(\"%20s   %20s | %20s\" % (\"\", \"Raw Categories\", \"Encoded Categories\"))\nfor f in f_cats:\n    print(\"%-20s : \" % f, end=\"\")\n    e_scores = []\n    f_scores = []\n    for trn_idx, val_idx in folds.split(trn_df.values, trn_df.target.values):\n        trn_f, trn_tgt = trn_df[f].iloc[trn_idx], trn_df.target.iloc[trn_idx]\n        val_f, val_tgt = trn_df[f].iloc[trn_idx], trn_df.target.iloc[trn_idx]\n        trn_tf, val_tf = target_encode(trn_series=trn_f, \n                                       tst_series=val_f, \n                                       target=trn_tgt, \n                                       min_samples_leaf=100, \n                                       smoothing=20,\n                                       noise_level=0.01)\n        f_scores.append(max(roc_auc_score(val_tgt, val_f), 1 - roc_auc_score(val_tgt, val_f)))\n        e_scores.append(roc_auc_score(val_tgt, val_tf))\n    print(\" %.6f + %.6f | %6f + %.6f\" \n          % (np.mean(f_scores), np.std(f_scores), np.mean(e_scores), np.std(e_scores)))","75f4f07a":"### Testing with ps_car_11_cat","84c21225":"### Fin","ddc5630b":"### Check AUC metric improvement after noisy encoding over 5 folds","b1bdf616":"### Target encoding with smoothing\nmin_samples_leaf define a threshold where prior and target mean (for a given category value) have the same weight. Below the threshold prior becomes more important and above mean becomes more important.\n\nHow weight behaves against value counts is controlled by smoothing parameter","701abfd8":"### Scatter plot of category values vs target encoding\nWe see that the category values are not ordered\n"}}