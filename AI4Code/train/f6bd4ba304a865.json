{"cell_type":{"66f00b39":"code","341e2423":"code","02e6fc04":"code","df93deb5":"code","8ce5de20":"code","3aacc314":"code","e5839748":"code","f29ad9cb":"code","634fae1f":"code","88215ef9":"code","37565347":"code","aa327208":"code","faac26db":"code","178bf54e":"code","601aae0e":"code","58a4cf05":"code","1aacb3b4":"code","856a763d":"code","745a54b5":"code","968f27da":"code","206b8eda":"code","ccd02dd2":"code","7d172e87":"code","b3a47320":"code","16474ee5":"markdown","e4a49515":"markdown","a3a7d826":"markdown","0d35f9b9":"markdown","3d219a71":"markdown"},"source":{"66f00b39":"# Daniel Balle 2019\nimport zipfile\nimport os\nimport datetime\n\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mplimg\nfrom scipy import ndimage\n\ntf.enable_eager_execution()\ntf.set_random_seed(32)","341e2423":"BASE_DIR = '\/kaggle\/input\/aerial-cactus-identification'\nIMAGE_PATH = '{}\/train\/train'.format(BASE_DIR)\n\ntrain_df = pd.read_csv(\"{}\/train.csv\".format(BASE_DIR))","02e6fc04":"# Function to read some image\ndef read_image(image_id, base_dir=IMAGE_PATH, transformation=False):\n    train_image = mplimg.imread(\"{}\/{}\".format(base_dir, image_id)) \/ 255.0\n    if not transformation:\n        return train_image\n\n    # Random data augmentation\n    train_image = ndimage.rotate(train_image, np.random.choice([0, 1, 2, 3]) * 90, mode='nearest')\n    if np.random.rand() > 0.5:\n        train_image = np.flip(train_image, np.random.choice([(0), (1), (0, 1)]))  # don't flip colors\n    train_image + (np.random.rand() * 0.2) - 0.1  # brightness\n    return train_image","df93deb5":"# Plot some data\nplt.figure(figsize=(8, 8))\nfor i in range(9):\n    plt.subplot(3, 3, i+1)\n    plt.xticks([])\n    plt.yticks([])\n    train_image = read_image(train_df['id'][i], transformation=True)\n    plt.imshow(train_image)\n    plt.xlabel(train_df['has_cactus'][i])\nplt.show()","8ce5de20":"IMAGE_DIMENSION = read_image(train_df['id'][0]).shape","3aacc314":"# Split data into train (80%) and validation set (20%)\nshuffled_data = train_df.sample(frac=1)\n\nTRAIN_SIZE = int(len(train_df) * 0.8)\nVALIDATION_SIZE = len(train_df) - TRAIN_SIZE\n\ntrain_set = shuffled_data[0:TRAIN_SIZE]\nvalidation_set = shuffled_data[TRAIN_SIZE:]\n\nprint(TRAIN_SIZE)\nprint(VALIDATION_SIZE)","e5839748":"# check distribution of positives vs negatives\nprint(\"Training distribution:\\n------\")\nprint(\"Positives: {}\\nNegatives: {}\".format(\n    list(train_set['has_cactus']).count(1),\n    list(train_set['has_cactus']).count(0)))\nprint(\" = {}\\n\".format(list(train_set['has_cactus']).count(1) \/ TRAIN_SIZE))\n\nprint(\"Validation distribution:\\n------\")\nprint(\"Positives: {}\\nNegatives: {}\".format(\n    list(validation_set['has_cactus']).count(1),\n    list(validation_set['has_cactus']).count(0)))\nprint(\" = {}\".format(list(validation_set['has_cactus']).count(1) \/ VALIDATION_SIZE))","f29ad9cb":"num_samples = list(train_set['has_cactus']).count(1)\n\n# Sample with replacement from the negatives to balance the classes\npositive_train_df = train_set[train_set['has_cactus'] == 1]\nnegative_train_df = train_set[train_set['has_cactus'] == 0].sample(num_samples, replace=True)\n\nbalanced_train_set = pd.concat([positive_train_df, negative_train_df], ignore_index=True).sample(frac=1)\nBALANCED_TRAIN_SIZE = len(balanced_train_set)","634fae1f":"# Generator for test and train data - needs to be callable\ndef train_gen():\n    for _, row in balanced_train_set.iterrows():\n        yield (read_image(row['id'], transformation=True), row['has_cactus'])\n\ndef validation_gen():\n    for _, row in validation_set.iterrows():\n        yield (read_image(row['id'], transformation=True), row['has_cactus'])","88215ef9":"BATCH_SIZE = 32\n\ntrain_ds = tf.data.Dataset.from_generator(train_gen, output_types=(tf.float32, tf.int16)).batch(BATCH_SIZE).repeat()\nvalidation_ds = tf.data.Dataset.from_generator(validation_gen, output_types=(tf.float32, tf.int16)).batch(BATCH_SIZE).repeat()","37565347":"from tensorflow.keras import layers\nfrom tensorflow.keras import regularizers","aa327208":"# Let's try a simple CNN\n# TODO(balle) try tf.keras.layers.BatchNormalization\nmodel = tf.keras.Sequential([\n    layers.Conv2D(16, (3, 3), activation='relu', input_shape=IMAGE_DIMENSION),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.BatchNormalization(),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.Dropout(0.5),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.Conv2D(128, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n    layers.BatchNormalization(),\n    layers.Conv2D(256, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n    layers.Dropout(0.5),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Flatten(),\n    # layers.Dense(32, activation=tf.nn.relu),\n    layers.Dense(1, activation=tf.nn.sigmoid)\n])\n\nmodel.summary()\n\n# note: categorical_crossentropy vs. softmax_crossentropy\nmodel.compile(\n  optimizer='adam',  # TODO(balle) tune learning rate!\n  loss='binary_crossentropy',\n  metrics=['accuracy']\n)","faac26db":"MAX_STEPS_PER_EPOCH = int(BALANCED_TRAIN_SIZE\/BATCH_SIZE)\nMAX_VALIDATION_STEPS = int(VALIDATION_SIZE\/BATCH_SIZE) * 5  # more validation","178bf54e":"early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience=5)\ncheckpointer = tf.keras.callbacks.ModelCheckpoint(filepath='weights.hdf5', verbose=1, save_best_only=True)\nlogdir = \"\/tensorboard\/scalars\/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ntensorboard = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n\nhistory = model.fit_generator(train_ds,\n                              epochs=20,\n                              validation_data=validation_ds,\n                              steps_per_epoch=MAX_STEPS_PER_EPOCH,\n                              validation_steps=MAX_VALIDATION_STEPS,\n                              callbacks=[early_stopping, checkpointer, tensorboard])","601aae0e":"model.load_weights('weights.hdf5')","58a4cf05":"# Print some predictions\nplt.figure(figsize=(8, 8))\nfor i in range(9):\n    j = np.random.randint(0, len(train_df))\n    plt.subplot(3, 3, i+1)\n    plt.xticks([])\n    plt.yticks([])\n    train_image = read_image(train_df['id'][j], transformation=False)\n    plt.imshow(train_image)\n    pred = model.predict(np.array([train_image]))[0][0]\n    plt.xlabel(\"true: {}\\npredicted: {:.2f}\".format(train_df['has_cactus'][j], pred))\nplt.show()","1aacb3b4":"def prediction_gen():\n    for _, row in validation_set.iterrows():\n        yield read_image(row['id'], transformation=True)\n\n# Predict over multiple transformer images and take mean\nprediction_ds = tf.data.Dataset.from_generator(prediction_gen, output_types=(tf.float32)).batch(BATCH_SIZE)  # don't shuffle\nmultiple_predictions = [\n    model.predict_generator(prediction_ds)\n    for i in range(10)\n]\npredictions = np.mean(np.array(multiple_predictions), axis=0)","856a763d":"# Compute accuracy\nfrom sklearn.metrics import accuracy_score\nprint(accuracy_score(validation_set['has_cactus'], np.round(predictions).astype('int32')))","745a54b5":"# Compute AUC\nfrom sklearn.metrics import roc_auc_score\nprint(roc_auc_score(validation_set['has_cactus'], predictions))","968f27da":"sample_submission = pd.read_csv('{}\/sample_submission.csv'.format(BASE_DIR))\nsample_submission.head()","206b8eda":"test_ids = os.listdir('{}\/test\/test'.format(BASE_DIR))","ccd02dd2":"def test_gen():\n    for test_id in test_ids:\n        yield read_image(test_id, base_dir='{}\/test\/test'.format(BASE_DIR), transformation=True)\n\ntest_ds = tf.data.Dataset.from_generator(test_gen, output_types=(tf.float32)).batch(BATCH_SIZE)  # don't shuffle\nmultiple_test_predictions = [\n    model.predict_generator(test_ds)              \n    for i in range(10)\n]\ntest_predictions = np.mean(np.array(multiple_test_predictions), axis=0)","7d172e87":"submission = pd.DataFrame({'id': test_ids, 'has_cactus': test_predictions.flatten()})\nsubmission.head()","b3a47320":"submission.to_csv('submission.csv', index = False, header = True)","16474ee5":"# Evaluation","e4a49515":"# Data pre-processing","a3a7d826":"Thanks boys\n> *Daniel Balle 2019*","0d35f9b9":"# Submission","3d219a71":"# Training"}}