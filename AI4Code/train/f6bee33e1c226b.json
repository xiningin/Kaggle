{"cell_type":{"db66ced0":"code","ddaac90c":"code","56d4b9a4":"code","ff07d712":"code","38a8dbc5":"code","8f558b81":"code","38058283":"code","258b59e1":"code","833ba60e":"code","cb3bb492":"code","1507325c":"code","9dc8c8ca":"code","1e6bc2a2":"code","d24cc3a2":"code","a350f3db":"code","8f60ea82":"code","339b9111":"code","7bd85891":"code","76f5b7d0":"code","e50aa918":"code","2a19d78b":"code","efafb87b":"code","8d49fe1c":"code","8bd2f0ad":"code","4c48bbc8":"code","eb6d8211":"code","1872a7e6":"code","2f2abf58":"markdown","5284ce44":"markdown","c080e93e":"markdown","a44bc7b3":"markdown","70e7aed3":"markdown","4317c12d":"markdown","4b67d027":"markdown","d2f34b5a":"markdown","665f12ba":"markdown","7bec69df":"markdown"},"source":{"db66ced0":"import numpy as np\nimport pandas as pd\nimport tensorflow\n\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nfrom sklearn.model_selection import KFold, train_test_split\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n# Used to track how long the model is training\nimport time","ddaac90c":"import os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n# Read in the different datafiles\nsample_submission = pd.read_csv(\"..\/input\/digit-recognizer\/sample_submission.csv\")\ntrain = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntest = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")","56d4b9a4":"train = pd.read_csv(\"Data\/train.csv\")\ntest = pd.read_csv(\"Data\/test.csv\")","ff07d712":"# Set 'label' column as targets of trainset\nY_train = train[\"label\"]\n\n# Drop 'label' column from trainset, to only leave the features (aka the pixels)\nX_train = train.drop(labels = [\"label\"],axis = 1)","38a8dbc5":"# Print the distribution of the digits present in the trainset\nprint('Label   Count    Percentage')\nfor i in range(0,10):\n    print(\"%d       %d     %.2f\" % (i, Y_train.value_counts()[i], round(Y_train.value_counts(normalize=True)[i]*100, 2)))","8f558b81":"# Divide values by 255 to get an input value between 0 and 1 for every pixel\nX_train = X_train \/ 255.0\ntest = test \/ 255.0","38058283":"# Creating copies to use later in the kFold approach\nX_train_K = X_train.copy()\nY_train_K = Y_train.copy()\ntest_K = test.copy()","258b59e1":"# reshaping the data to rank 4 so I can use Data Augmentation\n\nX_train = X_train.values.reshape(-1,28,28,1)\ntest = test.values.reshape(-1,28,28,1)\n\nX_train.shape","833ba60e":"Y_train = keras.utils.to_categorical(Y_train, num_classes=10)","cb3bb492":"X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.2, random_state=42)","1507325c":"print(X_train.shape)\nprint(Y_train.shape)","9dc8c8ca":"model_A = keras.models.Sequential()\n\n# Flattening the data so I can fit the rank 4 data\nmodel_A.add(Flatten())\n\n# For some reason got the best score using only one layer\n# Kept making the layer denser and it kept improving my score, it stopt improving arround 1024\nmodel_A.add(keras.layers.Dense(1024, activation='relu', input_shape=(784,)))\n\n# Add dropout to my layer to make it less vulnerable to overfitting\nmodel_A.add(Dropout(0.5))\n\nmodel_A.add(keras.layers.Dense(10, activation=\"softmax\"))","1e6bc2a2":"model_A.compile(\n    optimizer=\"adam\",\n    loss=\"categorical_crossentropy\",\n    metrics=[\"accuracy\"]\n)\n\n# Add the parameters for Data Augmentation\ndatagen = ImageDataGenerator(\n        rotation_range=10,  \n        zoom_range = 0.10,  \n        width_shift_range=0.1, \n        height_shift_range=0.1)\n\n# Fit the Data Augmentation to my training set\ndatagen.fit(X_train)","d24cc3a2":"X_train.shape","a350f3db":"history = model_A.fit_generator(datagen.flow(X_train,Y_train, batch_size=32),\n                              epochs = 55, validation_data = (X_val,Y_val))","8f60ea82":"predictions_A = model_A.predict_classes(test)\nprint(predictions_A)","339b9111":"my_submission_A = pd.DataFrame({'ImageId': list(range(1,len(predictions_A)+1)), 'label': predictions_A})\n\n# you could use any filename. We choose submission here\nmy_submission_A.to_csv('submission_A.csv', index=False)","7bd85891":"# Remove anything but the values of the X_train dataframe\nX_train_K = X_train_K.values\n\nprint(X_train_K.shape)\nprint(Y_train_K.shape)","76f5b7d0":"kf = KFold(n_splits = 10,\n           shuffle=True)","e50aa918":"# Create a copy of the labels to use for printing samples of the train and testsets\n# Samples are printed to give a general idea of train and testsets of the different folds\nY_labels = Y_train_K\nfor train_idx, test_idx in kf.split(X_train_K):\n    _train = plt.figure(figsize=(20,2))\n    for i in range(1,11):\n        ax = _train.add_subplot(1, 10, i)\n        ax.imshow(X_train_K[train_idx[i-1]].reshape(28, 28))\n        ax.set_xlabel(Y_labels[train_idx[i-1]])\n    _train.suptitle('Trainsample of in total %d records' % len(train_idx), fontsize=14)\n    plt.show()\n    \n    _test = plt.figure(figsize=(20,2))\n    for i in range(1,11):\n        ax = _test.add_subplot(1, 10, i)\n        ax.imshow(X_train_K[test_idx[i-1]].reshape(28, 28))\n        ax.set_xlabel(Y_labels[test_idx[i-1]])\n    _test.suptitle('Testsample of in total %d records' % len(test_idx), fontsize=14)\n    plt.show()","2a19d78b":"# Convert Y train values into a matrix with 10 columns, a column for each class\n#   (Comparable to hot-encoding)\nY_train_K = keras.utils.to_categorical(Y_train_K, num_classes=10)","efafb87b":"model_K = keras.models.Sequential()\n\nmodel_K.add(keras.layers.Dense(784, activation='relu', input_shape=(784,)))\nmodel_K.add(keras.layers.Dense(800, activation=\"relu\"))\nmodel_K.add(keras.layers.Dense(10, activation=\"softmax\"))","8d49fe1c":"# Configure the learning process\nmodel_K.compile(\n    optimizer=\"Adam\",\n    loss=\"categorical_crossentropy\",\n    metrics=['accuracy', 'mse']\n)","8bd2f0ad":"# Summarize the model, this gives information about the amount of parameters (weights & biases)\nmodel_K.summary()","4c48bbc8":"# Keep track of the running time by storing the starttime\nstart_time = time.time()\n\n# Fit the model for every fold in the kFold\nfor train_idx, test_idx in kf.split(X_train_K):\n    model_K.fit(\n        X_train_K[train_idx],\n        Y_train_K[train_idx],\n        batch_size=32,\n        epochs=15,\n        validation_data=(X_train_K[test_idx], Y_train_K[test_idx])\n    )\n    \n# Calculate the runtime by substracting the starttime from the current time\nruntime = time.time() - start_time\nprint(\"\/n--- Runtime of %s seconds ---\" % (runtime))","eb6d8211":"# Use the trained neural network to identify the digits in the testset\npredictions_K = model_K.predict_classes(test.values)\nprint(predictions_K)","1872a7e6":"# Create a dataframe from the predictions, made by the neural network\nmy_submission_K = pd.DataFrame({'ImageId': list(range(1,len(predictions_K)+1)), 'label': predictions_K})\n\n# Save the predictions in the file 'submission.csv'\nmy_submission_K.to_csv('submission_K.csv', index=False)","2f2abf58":"# Indicator 3.1\nThis competition has it's own train- and test set. Pandas is used to put the two .csv files in dataframes.\nAfter running the right cells (whether for Kaggle or for locally stored data), train data is stored in the variable 'train' and test data is stored in the variable 'test'\n\n## When using Kaggle\nWhen you are using Kaggle the next cells must be executed to load in the data.\nOn Kaggle the input data files are available in the \"..\/input\/\" directory.\n\n_Annotated by:_\n> Jano  \n> Jordi","5284ce44":"# Indicator 3.2\nI tried adding layers and tried removing layers, but I got the best score by only using one layer.\nI kept increasing the density of my layer untill the model stopt improving. It stopt improving at 1024\nI tried different dropouts to make the neural network less vulnerable to overfitting. 0.5 improved my score the most\n> ~Jano","c080e93e":"# Final score of 0.98096\nSettings for final score:\n<ul>   \n    <li>Data split with KFold into 10 folds<\/li>\n    <li>Connected Neural Network with 2 hidden layers:<\/li>\n    <ul>\n        <li>784 neurons, relu activation<\/li>\n        <li>800 neurons, relu activation<\/li>\n    <\/ul>\n    <li>Settings for learning:<\/li>\n    <ul>\n        <li>Optimizer: Adam<\/li>\n        <li>Loss function: categorical_crossentropy<\/li>\n    <\/ul>\n    <li>Model fitted with:<\/li>\n    <ul>\n        <li>Batch size of 32<\/li>\n        <li>15 epochs<\/li>\n    <\/ul>\n<\/ul>\n\nRunning time of 849 seconds\n\nOverview of scores:\n![image.png](attachment:image.png)\n","a44bc7b3":"# Indicator 3.1\nTo validate if the neural network is actually learning, the train dataframe is split up in another train set and a validation set.\n\n> ~Jano","70e7aed3":"## Data preparation","4317c12d":"# Indicator 3.1 - 3.2 - 3.3\n## kFold approach\n\nThis approach uses a kFold to create train- and validationsets\nUsing a kFold has 2 major benefits:  \n -- The bias of the model is reduced, because more data can be used for fitting  \n -- The variance of the model is reduced, because more data can be used for validation\n\nIn this approach a sequential deep learning model is used. This model, a so called fully connected neural network, consists of a linear stack of dense layers. Each neuron in a layer is connected to every neuron in the preceding and succesive layers.  \nThe amount of neurons per layer is based on trying different amounts of neurons and the design described [here](https:\/\/en.wikipedia.org\/wiki\/MNIST_database).\nThe optimizer used is the Adam optimizer. This choice was based on trying different optimizers and [this article](https:\/\/machinelearningmastery.com\/adam-optimization-algorithm-for-deep-learning\/).\n\n_Annotated by:_\n> Jordi\n","4b67d027":"# Indicator 3.2\nI use the adam optimizer because it is good at getting out of the local minum.\nI use data augmentation to create more images based on the train set, so the neural network has more images to train itself on. I create more images by rotating the images, zooming the images in and out, shifting the images on the x axis and shifting the images on the y axis.\n> ~Jano","d2f34b5a":"# Learning Outcome 3\n### Digit recognition with the MNIST dataset\n\nThis notebook is split into 2 different approaches we used to obtain high scores. \nBoth approaches use a fully connected network. Where one approach uses a kFold to reduce bias and variance, the other uses data augmentation to enlargen the dataset for training.\n\n_Disclaimer:_  \nWe decided to split the different approaches because they were both very different, but interesting.\nAfter combining the two approaches we obtained the incredible score of:\n![image.png](attachment:image.png)\n\n_Annotated by:_\n> Jordi","665f12ba":"## Data augmentation approach","7bec69df":"## When using this notebook locally\nWhen running the notebook locally the next cells must be executed to load in the data.\n"}}