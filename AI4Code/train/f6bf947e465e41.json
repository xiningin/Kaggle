{"cell_type":{"3cd95e8f":"code","6bd13222":"code","717775a2":"code","968fe451":"code","35a0fe4d":"code","b484033d":"code","c8eb8941":"code","63bdb949":"code","fbbdc701":"code","8114928a":"code","06e13bf2":"code","776679ae":"code","bbb296c3":"code","8657dff2":"code","f6153504":"code","d20f536a":"code","1eb3d513":"markdown","69a4608f":"markdown","09016c0a":"markdown","63edf0b7":"markdown","bac5bbd7":"markdown","747c10f7":"markdown","bed98c1d":"markdown","d38652c8":"markdown","f9fa831f":"markdown","79417538":"markdown","97705094":"markdown","05d7a2ba":"markdown","48e42297":"markdown","85f5476e":"markdown"},"source":{"3cd95e8f":"%pylab inline\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns","6bd13222":"df = pd.read_csv('..\/input\/fetal-health-classification\/fetal_health.csv')\ndf.drop_duplicates(inplace=True)\n\nindep = 'fetal_health'\nXdf = df.drop([indep],axis=1)\nXr = Xdf\nyr = df[indep]\n","717775a2":"import re    \n\ndef initials(x,n=1,join_chars=''):\n    splits = re.split('[_ ]',x)\n    return join_chars.join([s[0:n] for s in splits])\n\n\ncorr = df.corr()\n\n# rename the columns to have dmaller labels on the horizontal axis\ncr =corr.rename(columns=initials)\n\nfig,ax=plt.subplots(1,figsize=(12,10))\ncm=ax.matshow(cr,cmap='bwr',vmin=-1,vmax=1)\nplt.xticks(range(cr.shape[1]), cr.columns, fontsize=12, rotation=45,ha='left')\nplt.yticks(range(cr.shape[0]), cr.index, fontsize=12)\ncb = plt.colorbar(cm)\ncb.ax.tick_params(labelsize=14)\n","968fe451":"from sklearn.feature_selection import mutual_info_regression\nmimx = pd.DataFrame({col: pd.Series(mutual_info_regression(df,df[col]),index=df.columns) for col in df.columns} )\n","35a0fe4d":"fig,ax=plt.subplots(1,figsize=(12,10))\nmm=ax.matshow(mimx,cmap='viridis',vmin=0,vmax=1)\nplt.xticks(range(cr.shape[1]), cr.columns, fontsize=12, rotation=45,ha='left')\nplt.yticks(range(cr.shape[0]), cr.index, fontsize=12)\ncb = plt.colorbar(mm)\ncb.ax.tick_params(labelsize=14)\n","b484033d":"from sklearn.feature_selection import mutual_info_classif\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nmi = pd.Series(mutual_info_classif(Xr,yr),index=Xr.columns,name='mutual info')\nvs = pd.Series({col:variance_inflation_factor(Xr.values,Xr.columns.to_list().index(col)) for col in Xr.columns},name='vif')\n\npd.DataFrame([mi,vs]).T","c8eb8941":"to_drop = ['histogram_width','histogram_mode','histogram_median']\nXr.drop(to_drop,inplace=True,axis=1)\n\nmi = pd.Series(mutual_info_classif(Xr,yr),index=Xr.columns,name='mutual info')\nvs = pd.Series({col:variance_inflation_factor(Xr.values,Xr.columns.to_list().index(col)) for col in Xr.columns},name='vif')\n\npd.DataFrame([mi,vs]).T","63bdb949":"from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer, QuantileTransformer\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\n\nXtrain,Xtest,ytrain,ytest = train_test_split(Xr,yr,test_size = 0.30,stratify = yr,shuffle = True,random_state = 42)\n\n\nmodel = Pipeline([('scaler',StandardScaler()),\n                  ('classifier', OneVsRestClassifier(LogisticRegression()))])\n\nmfit = model.fit(Xtrain,ytrain)\nypred = mfit.predict(Xtest)\n\nprint(confusion_matrix(ytest,ypred))\nprint()\nprint(classification_report(ytest,ypred))","fbbdc701":"!pip install mord","8114928a":"\nfrom imblearn.over_sampling import SMOTE\nimport mord\n\nXrc = Xr.copy()\n\n\n# scale data and encode categories\nX = pd.DataFrame(StandardScaler().fit_transform(Xrc),columns=Xrc.columns)\nle = LabelEncoder().fit(yr)\ny = yr.astype('i') \n\n# split into test and train\nXtraini,Xtest,ytraini,ytest = train_test_split(X,y,test_size = 0.30,stratify = y,shuffle = True,random_state = 42)\n\n#oversample training data due to the imbalance between categories\nXtrain, ytrain = SMOTE().fit_sample(Xtraini,ytraini)\n#Xtrain, ytrain = Xtraini,ytraini\n\nmodel = mord.LogisticAT(alpha=.1)\nresults = model.fit(Xtrain,ytrain)\nypred = (results.predict(Xtest))\n\nprint('Raw data:')\nprint(y.value_counts())\nprint('Oversampled data:')\nprint(ytrain.value_counts())\n","06e13bf2":"from sklearn.metrics import roc_curve, precision_recall_curve, auc\n\nprint(confusion_matrix(ytest,ypred))\nprint()\nprint(classification_report(ytest,ypred))\nprint()\nprint(pd.Series(model.coef_,index=Xrc.columns))\n\n","776679ae":"decf = np.dot(X,model.coef_)\nfigure()\nsns.violinplot(y=decf,x=y)\nfor th in model.theta_:\n    axhline(th,color='r',alpha=.5)\n    \nylabel('decision function')","bbb296c3":"scores = model.predict_proba(Xtest)\n\nfigure()\n\nall_fpr = []\nall_tpr = []\n\nfor ii in [1,2]:\n    cno = model.classes_[ii]\n    fpr, tpr, thresholds = roc_curve(ytest>=cno, np.sum(scores[:,model.classes_>=cno],axis=1))\n\n    plot(fpr,tpr)\n    xlabel('False positive rate')\n    ylabel('True positive rate')\n    db = exp(model.theta_)\/(1+exp(model.theta_))\n    idx = (thresholds>db[0])&(thresholds<db[1])\n    plot(fpr[idx],tpr[idx])\n\n    print (auc(fpr,tpr))","8657dff2":"from copy import deepcopy as copy\nfrom sklearn.svm import SVC\nfrom sklearn.base import BaseEstimator\n\nclass OrdinalClassifier(BaseEstimator):\n\n    def __init__(self, *args, **kwargs):\n        self.clfs = {}\n        self.args = args\n        self.set_params(**kwargs)\n        self.sample_clf = SVC(*self.args, **self.kwargs)\n        \n    def fit(self, X, y):\n        self.unique_class = np.sort(np.unique(y))\n        if self.unique_class.shape[0] > 2:\n            for i in range(self.unique_class.shape[0]-1):\n                # for each k - 1 ordinal value we fit a binary classification problem\n                binary_y = (y > self.unique_class[i]).astype(np.uint8)\n                clf = SVC(*self.args, **self.kwargs)\n                clf.fit(X, binary_y)\n                self.clfs[i] = clf\n        return self\n\n    def predict_proba(self, X):\n        clfs_predict = {k:self.clfs[k].predict_proba(X) for k in self.clfs}\n        predict_list = [self.clfs[k].predict_proba(X) for k in self.clfs]\n        predicted = []\n        for i,y in enumerate(self.unique_class):\n            if i == 0:\n                # V1 = 1 - Pr(y > V1)\n                predicted.append(1 - predict_list[i][:,1])\n            elif y in clfs_predict:\n                # Vi = Pr(y > Vi-1) - Pr(y > Vi)\n                 predicted.append(predict_list[i-1][:,1] - predict_list[i][:,1])\n            else:\n                # Vk = Pr(y > Vk-1)\n                predicted.append(predict_list[i-1][:,1])\n        return np.vstack(predicted).T\n\n    def predict(self, X):\n        return self.unique_class[np.argmax(self.predict_proba(X), axis=1)]\n    \n    def get_params(self, *args, **kwargs):\n        return self.sample_clf.get_params(*args, **kwargs)\n\n    def set_params(self, **kwargs):\n        self.kwargs = kwargs\n        self.kwargs['probability']=True\n        return self\n","f6153504":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import  make_scorer, mean_squared_error, mean_absolute_error, precision_score\n\ndef mean_err_round(y,ypred,**kwargs):\n    return mean_absolute_error(y,np.round(ypred),**kwargs)\n\n\nXt = pd.DataFrame(StandardScaler().fit_transform(Xr),columns=Xr.columns,index=Xr.index)\nyt = yr\n\nX=Xt#.sample(500)\nle = LabelEncoder().fit(yr)\nyt = yr.astype('i') \ny=yt.loc[X.index]\n\nXtrain,Xtest,ytrain,ytest = train_test_split(X,y,test_size = 0.30,stratify = y,shuffle = True,random_state = 42)\n\n#scorer = make_scorer(mean_absolute_error, greater_is_better=False)\nscorer = make_scorer(precision_score,average='macro')\n\n# Set the parameters by cross-validation\ntuned_parameters = [{'kernel': ['rbf'], 'gamma': [ 0.5,0.1,0.01,0.001],\n                     'C':[1,10,100,1000]},\n                    {'kernel': ['linear'], 'C': [ 0.1,1,10,100]}\n                   ]\nclf = GridSearchCV(\n    OrdinalClassifier(), tuned_parameters, cv=5, verbose=2,scoring=scorer,\n)\nclf.fit(Xtrain, ytrain)\n\nypred = (clf.predict(Xtest))\n","d20f536a":"print(clf.best_params_)\nprint()\n\nprint(confusion_matrix(ytest,ypred))\nprint()\nprint(classification_report(ytest,ypred))\n\n","1eb3d513":"## Data exploration","69a4608f":"A similar information should be found in the ROC curve and Precision-recall curve, however I'm not sure how to interpret these \n\nI'd expect similar curves for both class limits (above problematic and above suspect thresholds) but they do not overlap.\n\nI'd expect that the two thetas_ would correspond to different points on this curve. ","09016c0a":"# Ordinal regression\n\n`mord` provides a few models of Ordinal Regression, see https:\/\/pythonhosted.org\/mord\/. \n\nThere isn't a lot of documentation on this package, but it seems that the ordinal regression consists of a linear regression with an estimation of the limits of each class along the target of the linear regression","63edf0b7":"### Correlation\n\nSeveral of the variables are highly correlated, and might interfere with the regression, or at least with its interpretation:\n****","bac5bbd7":"**Mutual information** informs on which are the variables that are most correlated with the outcome variable ('fetal health')\n\n**Variance inflation factor** is more adequate than correlation (above) because it takes into account multi-correlation","747c10f7":"Like a regular Logistic Regression, the model contains coefficients that inform on the contribution of each feature to the prediction\n\nInstead of one intercept, the model contains several intercepts that establish the limits between each level of the ordinal variable. ","bed98c1d":"The regression projects all the features onto a single dimension, and attributes class boundaries (the theta_ in the Ordinal Regression) along this dimension. \n\nThese boundaries can be adjusted to select a compromise between false positive rate and true positive rate.\n\nThe boundaries best matching the categorisation proposed are plotted in red horizontal lines. The lowest curve makes sure that no 'problematic' cases are missed (green blob), while accepting some false positives. ","d38652c8":"# Ordinal SVC hack\n\nBased on [this article](https:\/\/towardsdatascience.com\/simple-trick-to-train-an-ordinal-regression-with-any-classifier-6911183d2a3c), any classifier can be hacked to an ordinal one, by fitting to the N-1 problems \"above class N \/ below class N\"","f9fa831f":"# Logisitc regression classifier -- baseline\n\nFirst let's try naively a logistic regression classifier using the one vs many strategy. \n\nThis is not the approach to be followed since it doesn't allow an adjustment of the decision boundaries for the 'maybe' and the 'positive' cases, but just to have a idea of the precision that can be acheived...","79417538":"Based on the previous analysis, I'll drop the following columns, since they seem to be highly correlated with existing columns, and then re-check the VIF","97705094":"The column **fetal_health** is the outcome variable","05d7a2ba":"Accuracy has improved but not above the multi-class Logistic Regression, and we lost in interpretability. \n\nAccuracy is probably not the most important measure here, as class 'S' should be allowed a large error to account for possible  problematic cases. \n\nThe most important measure is probably precision and recall between classes 'P' and 'N'. Not sure how to account for this measure. ","48e42297":"Figure below is similar to the correlation matrix but should take into account non-linear relations between variables","85f5476e":"# Using regression to take advantage of ordinal data\n\nThe data relates a series of measurements or indicators to a prognostic of the health of the foetus, with two degrees of certitude. \n\n* 'N' is normal, indicating that the foetus is healthy\n* 'S' is interpreted as a 'maybe', so the foetus should be monitored\n* 'P' is problematic (so I interpret this as a fully positive result) \n\nThe outcome variable is thus an ordinal variable that expresses the same prediction with different degrees of certitude. \n\nI start by exploring the data and then try a two different methods of taking into account the ordinal nature of the outcome. \n\nPlease comment on this approach, as I am not an expert in data science nor on health."}}