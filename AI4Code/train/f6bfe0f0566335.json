{"cell_type":{"f569ff76":"code","cc5bcc8c":"code","7dbf5c74":"code","cb286494":"code","94bd98e4":"code","2590b55e":"code","b3ba1053":"code","5c13591e":"code","90bd33b2":"code","b05b7a04":"code","2f83d6bf":"code","c68342aa":"code","d474f609":"code","132335ef":"code","da8547d4":"code","502eeda2":"code","17f54df6":"markdown","f7c0f093":"markdown","0076ef03":"markdown","9320dacd":"markdown","9753ecae":"markdown","517a90d7":"markdown","22097af4":"markdown","cb0cb082":"markdown","24ed2fbd":"markdown","ddf2b88c":"markdown","edc43c29":"markdown","6a01526b":"markdown","87b0cc7b":"markdown","3ac98ae3":"markdown"},"source":{"f569ff76":"import cv2\nimport os\nimport datetime\nimport time\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.losses import categorical_crossentropy\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.vgg19 import VGG19, preprocess_input\nfrom keras.optimizers import Adam\nfrom keras.models import Model, Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten, Input\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\nfrom math import sin, cos, radians","cc5bcc8c":"# Hyperparameters\nIMAGE_SIZE = 224\nCHANNELS = 3","7dbf5c74":"# Load dataset to memory:\nimages_folder = r'..\/input\/face-mask-detection-medical-and-nonmedical-masks\/Dataset'\nmodel_folder = r'..\/input'\nlabels = []\nimages = []\n\ndef resize(img, target):\n    \"\"\" Resize an image \"\"\"\n    resized = cv2.resize(img, (target, target),\n                         interpolation = cv2.INTER_AREA).astype(np.float32)\n    return resized.reshape((-1))\n\nfor folder in os.listdir(images_folder):\n    if folder in ('with_mask','without_mask'):\n        for filename in os.listdir(os.path.join(images_folder,folder)):\n            labels.append(folder)\n            img = cv2.imread(os.path.join(images_folder,folder,filename))\n            images.append(resize(img, IMAGE_SIZE))\n            \nimages = np.array(images)\nlabels = pd.get_dummies(labels).to_numpy()","cb286494":"# Data partitioning\nx_train, x_test, y_train, y_test = train_test_split(\n    images.reshape(-1,IMAGE_SIZE,IMAGE_SIZE,CHANNELS),\n    labels, train_size=0.7, stratify=labels, random_state=42)","94bd98e4":"def display_images(df):\n    df = np.array(df)\n    plt.figure(figsize=(10, 10))\n    indices = np.random.choice(np.arange(len(df)),\n                               9, replace=False)\n    images = df[indices]\n    for i in range(9):\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(cv2.cvtColor(images[i].reshape((224,224,3))\n                                .astype('uint8'), cv2.COLOR_BGR2RGB))\n        plt.axis(\"off\")","2590b55e":"display_images(x_train[y_train[:,0]==1])","b3ba1053":"display_images(x_train[y_train[:,1]==1])","5c13591e":"mask_counts = np.sum(labels[:,0], axis=0)\nnon_mask_counts = np.sum(labels[:, 1], axis=0)\nplt.figure()\nplt.title(\"Frequency Bar Chart of Target Variable\")\nplt.bar(['Wearing Mask','Not Wearing Mask'],\n        [mask_counts,non_mask_counts], color=['blue','red'],\n        align='center', alpha=0.5)\nplt.show()","90bd33b2":"# Auxiliary augmentation function\n\ndef add_noise(img):\n    '''Add random noise to an image'''\n    VARIABILITY = 50\n    deviation = VARIABILITY*np.random.rand()\n    noise = np.random.normal(0, deviation, img.shape)\n    img += noise\n    np.clip(img, 0., 255.)\n    return img\n\n# Generating augmentations\ntrdata = ImageDataGenerator(rescale=1.\/255, rotation_range=40,\n                            width_shift_range=0.2, height_shift_range=0.2,\n                            horizontal_flip=True, channel_shift_range=50,\n                            preprocessing_function = add_noise)\ntraindata = trdata.flow(x_train, y_train)\n\ntsdata = ImageDataGenerator(rescale=1.\/255, rotation_range=40,\n                            width_shift_range=0.2, height_shift_range=0.2,\n                            horizontal_flip=True, channel_shift_range=50,\n                            preprocessing_function = add_noise)\ntestdata = tsdata.flow(x_test, y_test)\n\n# Load VGG19 architecture as transfer learning model\nvgg = VGG19(weights='imagenet', include_top=False,\n            input_shape=(IMAGE_SIZE, IMAGE_SIZE, CHANNELS))\nvgg.trainable = False\nflatten = Flatten()(vgg.output)\nd1 = Dense(2, activation=\"softmax\")(flatten)\nmodel = Model(inputs=vgg.input, outputs=d1)\nmodel.summary()","b05b7a04":"# Handling imbalanced data\nclass_weight = {0: 1., 1: 2.}","2f83d6bf":"opt = Adam(learning_rate = 1e-5)\nmodel.compile(optimizer=opt,\n              loss=categorical_crossentropy, metrics=['accuracy'])\nearly = EarlyStopping(monitor='val_accuracy',\n                      min_delta=0, verbose=1, patience=5, mode='auto')\nhist = model.fit(traindata, steps_per_epoch=10, \n                 verbose=2, validation_data=testdata, validation_steps=5,\n                 epochs=70, callbacks=[early], shuffle=True,\n                 class_weight=class_weight)","c68342aa":"video_file = r'..\/input\/oxford-street-in-london\/Oxford Street in London.mp4'","d474f609":"! pip install yoloface\nfrom yoloface import face_analysis","132335ef":"# Hyperparameters for face alignment routine\nsettings = {\n    'scaleFactor': 1.3,\n    'minNeighbors': 3,\n    'minSize': (50, 50),\n    'flags': cv2.CASCADE_SCALE_IMAGE\n}\n\ndef rotate_image(image, angle):\n    \"\"\" Rotate an image by a given rotation angle \"\"\"\n    if angle == 0: return image\n    height, width = image.shape[:2]\n    rot_mat = cv2.getRotationMatrix2D((width\/2,\n                                       height\/2), angle, 0.9)\n    result = cv2.warpAffine(image, rot_mat, (width, height),\n                            flags=cv2.INTER_LINEAR)\n    return result\n\ndef rotate_point(pos, img, angle):\n    \"\"\"\n    Compute the position of the detected faces in the\n    original image (undo the rotation)\n    \"\"\"\n    if angle == 0: return pos\n    x = pos[0]-img.shape[1]*0.4\n    y = pos[1]-img.shape[0]*0.4\n    newx = x*cos(radians(angle)) + y*sin(radians(angle)) + img.shape[1]*0.4\n    newy = -x*sin(radians(angle)) + y*cos(radians(angle)) + img.shape[0]*0.4\n    return int(newx), int(newy), pos[2], pos[3]","da8547d4":"# Opens the Video file\ncap = cv2.VideoCapture(os.path.join(model_folder, video_file))\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc('M','J','P','G')\n# Get the frame width, height and fps\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = int(cap.get(cv2.CAP_PROP_FPS))\n# Define output video file\nout = cv2.VideoWriter('Output.avi',\n                      fourcc, 20.0, (frame_width, frame_height), isColor=True)\nface = cv2.CascadeClassifier(\n    cv2.data.haarcascades+'haarcascade_frontalface_default.xml'\n)\n# Load Yoloface object\nyoloface_model = face_analysis()","502eeda2":"# Loop over each frame\ni = 0\nt_start = time.time()\nprint(\"Video processing started...\")\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        t_end = time.time()\n        print(\"Video processing done.\")\n        print(\"Video duration of\", i \/ fps,\n              \"seconds was successfully processed in\",\n              (round(t_end - t_start, 2)), \"seconds.\")\n        break\n    # Detect faces in the image\n    img, boxes, _ = yoloface_model.face_detection(frame_arr=frame, model='tiny')\n    # Loop over each face found by Yoloface\n    for box in boxes:\n        # Extract the bounding boxes for each face and manually adjusting boundaries\n        x1, y1, width, height = box\n        x2, y2 = x1 + width - 10, y1 + height + 50\n        y1 = 0 if y1 < 0 else y1\n        x1 = 0 if x1 < 0 else x1\n        img = cv2.resize(frame[y1:y2, x1:x2], (IMAGE_SIZE, IMAGE_SIZE),\n                         interpolation=cv2.INTER_AREA).astype('uint8')\n        # Aligning face\n        for angle in [0, -25, 25]:\n            rimg = rotate_image(frame[y1:y2, x1:x2], angle)\n            detected = face.detectMultiScale(rimg, **settings)\n            if len(detected):\n                detected = [rotate_point(detected[-1], frame, -angle)]\n                break\n        # Predicting if a person is wearing mask (0) or not (1) with c.l. > 70%\n        pred = model.predict(resize(img,IMAGE_SIZE).reshape(1, IMAGE_SIZE,\n                                                            IMAGE_SIZE, CHANNELS))\n        mask_result = np.argmax(pred)\n        confidence = pred[0][mask_result] * 100\n        # Plot bounding box as well as confidence level (model's output)\n        if mask_result == 1 and confidence > 0.7:\n            text = \"Not Wearing Mask, Conf: {0:.2f}%\".format(confidence)\n            cv2.rectangle(frame, (x1, y1), (x2+10, y2+10), (0, 0, 255), 2)\n            cv2.putText(frame, text, (x1, y1-10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n        if mask_result == 0 and confidence > 0.7:\n            text = \"Wearing Mask, Conf: {0:.2f}%\".format(confidence)\n            cv2.rectangle(frame, (x1, y1), (x2+10, y2+10), (0, 255, 0), 2)\n            cv2.putText(frame, text, (x1, y1-10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n    i += 1\n    # Write the frame\n    out.write(frame)\n# Release everything if job is finished\ncap.release()\nout.release()","17f54df6":"While exploring the data, we can easily see that it is imbalanced with respect to the categories. We'll take this into account while training the network.","f7c0f093":"Preparations - load the video file, load YOLO-Face object,load a Haar-Cascade object:","0076ef03":"**Conclusions**:\nWe can see that the model is performing reasonably well. There is a visible instability due to the very small amount of available data. Also, because the model has been trained solely with binary cases (a person either wearing a mask or not), there are many real-world cases where half masking (mask on the neck) provides the model with an ambiguous input. The model's performance can be significantly improved by adding more training data, especially with a wide range of face orientations for both classes.","9320dacd":"Now let's create a processing routine for the video. This routine performs the following steps:\n1. Load the next frame from the video.\n2. Execute face detection (YOLO V5).\n3. For each face found in frame:\n    1. Predict whether the person is wearing mask or not \n    2. Plot a rectangle around the face with a color matching the result (a green rectangle if the    person is wearing a mask, and a red one if not).\n    3. Print the confidence value for each prediction.","9753ecae":"Let's plot some samples from the dataset:","517a90d7":"In order to detect whether the person in the dedicated box is wearing a mask or not, according to the given data, we\u2019ll use a pre-trained transfer-learning VGG19 model (Imagenet-based weights). We\u2019ll add a final dense layer made of two neurons, along with a \u201csoftmax\u201d activation layer, in order to predict the probability for an image to belong to each class.","22097af4":"As the video consist of wide range of real world face orientations, I'll use the Haar-Cascade classic algorithm in order to align each face extracted from the frame:","cb0cb082":"In order to balance the data, we\u2019ll force the algorithm to treat every instance of class 0 (wearing a mask) as two instances of class 1 (not wearing a mask). We'll use a weighted loss function. ","24ed2fbd":"Now we'll use this model along with a pre-trained face detection model, in order to perform mask detection on a given video. As a reference, we\u2019ll will use a nice video found in Youtube, which was taken in Oxford St., London, during the height of the pandemic:\n<iframe width=\"950\" height=\"534\" src=\"https:\/\/www.youtube.com\/embed\/KQGj9ELtk0w\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe>\nLet's apply our model to this video:","ddf2b88c":"This yields the final result:\n<iframe width=\"950\" height=\"534\" src=\"https:\/\/www.youtube.com\/embed\/eW6SRW-o6Yc\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe>","edc43c29":"Sample images of people who are wearing a mask:","6a01526b":"#  \ud83d\ude37 Mask Detection \ud83d\ude37\n\n**Abstract:**\nIn this notebook I will try to create a simple mask-detection algorithm that can be easily applied to online or recorded videos. This is done by leveraging the classification and localization capabilities of DNNs. \nThe dataset consists of very few instances for each class, with highly similar orientations and positions, as well as two well-defined classes with no common inter-positions (such as half masking). We\u2019ll have to consider and handle this problems down the road. \n\nLet's begin by importing the relevant libraries:","87b0cc7b":"Let's train the network:","3ac98ae3":"Sample images of people who are not wearing mask:"}}