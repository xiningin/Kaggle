{"cell_type":{"1fb83a5e":"code","78fc7183":"code","5f374129":"code","ce0a18b5":"code","5189482f":"code","18b7835e":"code","66ccee2c":"code","b2e3c206":"code","9c5ec1b7":"code","4e628d29":"code","e4a68ae4":"code","8514b6ff":"code","2092e472":"code","fe8407e0":"code","880c5da0":"code","cc81eaea":"code","54c2057a":"code","64ff4697":"markdown","6e5b2285":"markdown","a8199645":"markdown","25ab999e":"markdown","a901e0c4":"markdown","3653c6e5":"markdown","8f64c20f":"markdown","e77866c3":"markdown","f52a0dff":"markdown","087c9886":"markdown"},"source":{"1fb83a5e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import NuSVR, SVR\nfrom sklearn.metrics import mean_absolute_error\npd.options.display.precision = 15\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport time\nimport datetime\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.linear_model import Ridge, RidgeCV\nimport gc\nfrom catboost import CatBoostRegressor\nimport seaborn as sns\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n","78fc7183":"X = pd.read_csv(\"..\/input\/train.csv\", nrows = 600000000, dtype={'acoustic_data': np.int16, 'time_to_failure': np.float64})","5f374129":"%time\nimport scipy\n\nrows = 150_000\ntrain = X\nsegments = int(np.floor(train.shape[0] \/ rows))\n\nX_tr = pd.DataFrame(index=range(segments), dtype=np.float64,\n                       columns=['ave', 'std', 'max', 'min',\n                               'av_change_abs', 'av_change_rate', 'abs_max', 'abs_min',\n                               'std_first_50000', 'std_last_50000', 'std_first_10000', 'std_last_10000',\n                               'avg_first_50000', 'avg_last_50000', 'avg_first_10000', 'avg_last_10000',\n                               'min_first_50000', 'min_last_50000', 'min_first_10000', 'min_last_10000',\n                               'max_first_50000', 'max_last_50000', 'max_first_10000', 'max_last_10000',\n                               'skew','skew_first_10000','skew_last_10000',\n                                'skew_first_50000','skew_last_50000',\n                                'kurt','kurt_first_10000','kurt_last_10000',\n                                'kurt_first_50000','kurt_last_50000'])\ny_tr = pd.DataFrame(index=range(segments), dtype=np.float64,\n                       columns=['time_to_failure'])\n\ntotal_mean = train['acoustic_data'].mean()\ntotal_std = train['acoustic_data'].std()\ntotal_max = train['acoustic_data'].max()\ntotal_min = train['acoustic_data'].min()\ntotal_sum = train['acoustic_data'].sum()\ntotal_abs_max = np.abs(train['acoustic_data']).sum()\n\nfor segment in tqdm_notebook(range(segments)):\n    seg = train.iloc[segment*rows:segment*rows+rows]\n    x = seg['acoustic_data'].values\n    y = seg['time_to_failure'].values[-1]\n    \n    y_tr.loc[segment, 'time_to_failure'] = y\n    X_tr.loc[segment, 'ave'] = x.mean()\n    X_tr.loc[segment, 'std'] = x.std()\n    X_tr.loc[segment, 'max'] = x.max()\n    X_tr.loc[segment, 'min'] = x.min()\n    \n    \n    X_tr.loc[segment, 'av_change_abs'] = np.mean(np.diff(x))\n    X_tr.loc[segment, 'av_change_rate'] = np.mean(np.nonzero((np.diff(x) \/ x[:-1]))[0])\n    X_tr.loc[segment, 'abs_max'] = np.abs(x).max()\n    X_tr.loc[segment, 'abs_min'] = np.abs(x).min()\n    \n    X_tr.loc[segment, 'std_first_50000'] = x[:50000].std()\n    X_tr.loc[segment, 'std_last_50000'] = x[-50000:].std()\n    X_tr.loc[segment, 'std_first_10000'] = x[:10000].std()\n    X_tr.loc[segment, 'std_last_10000'] = x[-10000:].std()\n    \n    X_tr.loc[segment, 'avg_first_50000'] = x[:50000].mean()\n    X_tr.loc[segment, 'avg_last_50000'] = x[-50000:].mean()\n    X_tr.loc[segment, 'avg_first_10000'] = x[:10000].mean()\n    X_tr.loc[segment, 'avg_last_10000'] = x[-10000:].mean()\n    \n    X_tr.loc[segment, 'min_first_50000'] = x[:50000].min()\n    X_tr.loc[segment, 'min_last_50000'] = x[-50000:].min()\n    X_tr.loc[segment, 'min_first_10000'] = x[:10000].min()\n    X_tr.loc[segment, 'min_last_10000'] = x[-10000:].min()\n    \n    X_tr.loc[segment, 'max_first_50000'] = x[:50000].max()\n    X_tr.loc[segment, 'max_last_50000'] = x[-50000:].max()\n    X_tr.loc[segment, 'max_first_10000'] = x[:10000].max()\n    X_tr.loc[segment, 'max_last_10000'] = x[-10000:].max()\n    \n    X_tr.loc[segment, 'skew'] = scipy.stats.skew(x)\n    X_tr.loc[segment, 'kurt'] = scipy.stats.kurtosis(x)\n        \n    X_tr.loc[segment, 'skew_first_10000'] = scipy.stats.skew(x[:10000])\n    X_tr.loc[segment, 'skew_last_10000'] = scipy.stats.skew(x[:-10000])\n        \n    X_tr.loc[segment, 'kurt_first_10000'] = scipy.stats.kurtosis(x[:10000])\n    X_tr.loc[segment, 'kurt_last_10000'] = scipy.stats.kurtosis(x[:-10000])\n    \n    X_tr.loc[segment, 'skew_first_50000'] = scipy.stats.skew(x[:50000])\n    X_tr.loc[segment, 'skew_last_50000'] = scipy.stats.skew(x[:-50000])\n        \n    X_tr.loc[segment, 'kurt_first_50000'] = scipy.stats.kurtosis(x[:50000])\n    X_tr.loc[segment, 'kurt_last_50000'] = scipy.stats.kurtosis(x[:-50000])","ce0a18b5":"scaler = StandardScaler()\nscaler.fit(X_tr)\nX_train_scaled = pd.DataFrame(scaler.transform(X_tr), columns=X_tr.columns)","5189482f":"\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nprint(y_tr['time_to_failure'].describe())\nsns.distplot(y_tr['time_to_failure'], bins=500)","18b7835e":"X_train_scaled.head()","66ccee2c":"var = 'skew_first_10000'\ndata = pd.concat([y_tr['time_to_failure'], X_train_scaled], axis=1)\ndata.plot.scatter(x=var, y='time_to_failure', ylim=(0,20));","b2e3c206":"corrmat = data.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","9c5ec1b7":"k = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'time_to_failure')['time_to_failure'].index\ncm = np.corrcoef(data[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","4e628d29":"sns.set()\ncols = ['time_to_failure', 'min', 'min_last_50000', 'min_last_10000', 'min_first_50000', 'min_first_10000', 'av_change_rate']\nsns.pairplot(data[cols], size = 2.5)\nplt.show();","e4a68ae4":"submission = pd.read_csv('..\/input\/sample_submission.csv', index_col='seg_id')\nX_test = pd.DataFrame(columns=X_tr.columns, dtype=np.float64, index=submission.index)\nplt.figure(figsize=(22, 16))\n\nfor i, seg_id in enumerate(tqdm_notebook(X_test.index)):\n    seg = pd.read_csv('..\/input\/test\/' + seg_id + '.csv')\n    \n    x = seg['acoustic_data'].values\n    X_test.loc[seg_id, 'ave'] = x.mean()\n    X_test.loc[seg_id, 'std'] = x.std()\n    X_test.loc[seg_id, 'max'] = x.max()\n    X_test.loc[seg_id, 'min'] = x.min()\n        \n    X_test.loc[seg_id, 'av_change_abs'] = np.mean(np.diff(x))\n    X_test.loc[seg_id, 'av_change_rate'] = np.mean(np.nonzero((np.diff(x) \/ x[:-1]))[0])\n    X_test.loc[seg_id, 'abs_max'] = np.abs(x).max()\n    X_test.loc[seg_id, 'abs_min'] = np.abs(x).min()\n    \n    X_test.loc[seg_id, 'std_first_50000'] = x[:50000].std()\n    X_test.loc[seg_id, 'std_last_50000'] = x[-50000:].std()\n    X_test.loc[seg_id, 'std_first_10000'] = x[:10000].std()\n    X_test.loc[seg_id, 'std_last_10000'] = x[-10000:].std()\n    \n    X_test.loc[seg_id, 'avg_first_50000'] = x[:50000].mean()\n    X_test.loc[seg_id, 'avg_last_50000'] = x[-50000:].mean()\n    X_test.loc[seg_id, 'avg_first_10000'] = x[:10000].mean()\n    X_test.loc[seg_id, 'avg_last_10000'] = x[-10000:].mean()\n    \n    X_test.loc[seg_id, 'min_first_50000'] = x[:50000].min()\n    X_test.loc[seg_id, 'min_last_50000'] = x[-50000:].min()\n    X_test.loc[seg_id, 'min_first_10000'] = x[:10000].min()\n    X_test.loc[seg_id, 'min_last_10000'] = x[-10000:].min()\n    \n    X_test.loc[seg_id, 'max_first_50000'] = x[:50000].max()\n    X_test.loc[seg_id, 'max_last_50000'] = x[-50000:].max()\n    X_test.loc[seg_id, 'max_first_10000'] = x[:10000].max()\n    X_test.loc[seg_id, 'max_last_10000'] = x[-10000:].max()\n    \n    X_test.loc[seg_id, 'skew'] = scipy.stats.skew(x)\n    X_test.loc[seg_id, 'kurt'] = scipy.stats.kurtosis(x)\n        \n    X_test.loc[seg_id, 'skew_first_10000'] = scipy.stats.skew(x[:10000])\n    X_test.loc[seg_id, 'skew_last_10000'] = scipy.stats.skew(x[:-10000])\n        \n    X_test.loc[seg_id, 'kurt_first_10000'] = scipy.stats.kurtosis(x[:10000])\n    X_test.loc[seg_id, 'kurt_last_10000'] = scipy.stats.kurtosis(x[:-10000])\n            \n    X_test.loc[seg_id, 'skew_first_50000'] = scipy.stats.skew(x[:50000])\n    X_test.loc[seg_id, 'skew_last_50000'] = scipy.stats.skew(x[:-50000])\n        \n    X_test.loc[seg_id, 'kurt_first_50000'] = scipy.stats.kurtosis(x[:50000])\n    X_test.loc[seg_id, 'kurt_last_50000'] = scipy.stats.kurtosis(x[:-50000])\n    \nX_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)","8514b6ff":"model = lgb.LGBMRegressor(n_estimators=10000,learning_rate=0.0005, n_jobs=-1)\nmodel.fit(X_train_scaled,y_tr)\ny_pred_lgb = model.predict(X_test_scaled)","2092e472":"model = xgb.XGBRegressor(n_estimators=10)\nmodel.fit(X_train_scaled,y_tr)\ny_pred_xgb = model.predict(X_test_scaled)","fe8407e0":"from keras.layers import Input, Dense\nfrom keras.models import Model\nimport tensorflow as tf\n\ninputs = Input(shape=(34,))\nx = Dense(128, activation='relu')(inputs)\nx = Dense(128, activation='relu')(x)\npredictions = Dense(1)(x)\nmodel_3 = Model(inputs=inputs, outputs=predictions)\nmodel_3.compile(optimizer=tf.train.RMSPropOptimizer(0.001),\n                      loss='mse',\n                    metrics=['accuracy'])\nmodel_3.fit(X_train_scaled, y_tr, epochs=15, verbose=0)\ny_pred_nn = model.predict(X_test_scaled).flatten()","880c5da0":"X_test_scaled","cc81eaea":"sample = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsample['time_to_failure'] = (y_pred_lgb+y_pred_xgb+y_pred_nn)\/3\nsample.to_csv('submission.csv',index=False)","54c2057a":"%time\n\nfrom sklearn.model_selection import StratifiedKFold, ShuffleSplit, cross_val_score\nfrom sklearn.metrics import log_loss, roc_auc_score\nfrom tqdm import tqdm\nfrom statistics import mean\n\nn_estimators = [5,10]\n\nfor n_est in  n_estimators:\n    print(n_est)\n    mae = []\n    cv = ShuffleSplit(n_splits=5,random_state=0)\n    for train, valid in cv.split(X_train_scaled, y_tr):\n        x_train = X_train_scaled.iloc[train]\n        x_valid = X_train_scaled.iloc[valid]\n        y_train = y_tr.iloc[train]\n        y_valid = y_tr.iloc[valid]\n        \n        model_1 = lgb.LGBMRegressor(n_estimators=10000, learning_rate=0.0005,n_jobs=-1,random_state=0)\n        model_1.fit(x_train, y_train)\n        y_pred_lgb = model_1.predict(x_valid)\n        \n        model_2 = xgb.XGBRegressor(n_estimators=n_est, n_jobs=-1,random_state=0)\n        model_2.fit(x_train, y_train)\n        y_pred_xgb = model_2.predict(x_valid)\n        \n        inputs = Input(shape=(34,))\n        x = Dense(128, activation='relu')(inputs)\n        x = Dense(128, activation='relu')(x)\n        predictions = Dense(1)(x)\n        model_3 = Model(inputs=inputs, outputs=predictions)\n        model_3.compile(optimizer=tf.train.RMSPropOptimizer(0.001),\n                      loss='mse',\n                    metrics=['accuracy'])\n        model_3.fit(x_train, y_train, epochs=15, verbose=0)\n        y_pred_nn = model_3.predict(x_valid).flatten()\n        \n        y_pred = (y_pred_lgb+y_pred_xgb+y_pred_nn)\/3\n        mae.append(mean_absolute_error(y_valid, y_pred))\n    print(mean(mae))","64ff4697":"Trainig and Predict by XGboosting.\n\n--------------------------------------\nXGboosting\u306b\u3088\u308b\u5b66\u7fd2\u304a\u3088\u3073\u4e88\u6e2c\u3002","6e5b2285":"Making Features.\n\n-----------------------\n\n\u7279\u5fb4\u91cf\u306e\u4f5c\u6210\u3002","a8199645":"I made too much simple LightGBM & XGboosting & NN Blend model.\nhttps:\/\/www.kaggle.com\/artgor helped me.\n\nI'm beginner, so there may be many strange point.\nPlease give me a advise.\n\n--------------------------------------------------------------\n\nLightGBM\u3068XGboosting\u3068NN\u306e\u6df7\u5408\u306b\u3088\u308b\u30b7\u30f3\u30d7\u30eb\u306a\u56de\u5e30\u30e2\u30c7\u30eb\u3092\u69cb\u7bc9\u3057\u307e\u3057\u305f\u3002\nhttps:\/\/www.kaggle.com\/artgor\u3000\u3055\u3093\u306ekernels\u3092\u53c2\u8003\u306b\u3055\u305b\u3066\u3044\u305f\u3060\u304d\u307e\u3057\u305f\u3002\n\n\u65e5\u672c\u8a9e\u3067\u66f8\u304b\u308c\u305fkernels\u304c\u307b\u3068\u3093\u3069\u306a\u304b\u3063\u305f\u305f\u3081\n\u5c11\u3057\u3067\u3082\u521d\u5fc3\u8005\u306e\u52a9\u3051\u306b\u306a\u308c\u3070\u3068\u3053\u3061\u3089\u306ekernels\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\n\n\u306a\u304a\u3001\u79c1\u81ea\u8eab\u3082\u521d\u5fc3\u8005\u3067\u3059\u306e\u3067\u7406\u89e3\u306e\u3067\u304d\u3066\u3044\u306a\u3044\u70b9\u304c\u591a\u304f\u3042\u308b\u304b\u3068\u601d\u3044\u307e\u3059\u3002\n\u3054\u6307\u6458\u3084\u30a2\u30c9\u30d0\u30a4\u30b9\u304c\u3042\u308c\u3070\u662f\u975e\u304a\u9858\u3044\u3057\u307e\u3059\u3002","25ab999e":"Loading Data.\n\"time_to_failure\" has slight differences, so read by np.float64.\n\n-------------------\n\n\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\u3002\n\"time_to_failure\"\u306f\u30c7\u30fc\u30bf\u306e\u5024\u306e\u5dee\u304c\u975e\u5e38\u306b\u5c0f\u3055\u3044\u306e\u3067\u3001np.float64\u3067\u8aad\u307f\u8fbc\u3080\u3002","a901e0c4":"Cross Validation.\n\n---------------------------\n\n\u4ea4\u5dee\u691c\u8a3c\u3002","3653c6e5":"Trainig and Predict by LightGBM.\n\n--------------------------------------\n\nLightGBM\u306b\u3088\u308b\u5b66\u7fd2\u304a\u3088\u3073\u4e88\u6e2c\u3002","8f64c20f":"Making data to predict.\n\n--------------------------------\n\n\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u4f5c\u6210\u3002","e77866c3":"Training and Predict by NN.\n\n-------------------------------\n\nNN\u306b\u3088\u308b\u5b66\u7fd2\u304a\u3088\u3073\u4e88\u6e2c\u3002","f52a0dff":"Normalize Features.\n\n------------------------\n\n\u4f5c\u6210\u3057\u305f\u7279\u5fb4\u91cf\u306e\u6b63\u898f\u5316\u3002","087c9886":"Blending.\n\n---------------------------\n\n\u6df7\u5408\u3002"}}