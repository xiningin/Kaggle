{"cell_type":{"ee2469ee":"code","9d6073df":"code","66cef7be":"code","4733eaed":"code","f8c55eb7":"code","d9733296":"code","4c68eeb9":"code","18e46a45":"code","c3e58dcc":"code","cdfb7076":"code","90bab402":"code","c95341ea":"code","8ace4636":"code","96e50923":"code","adb3be52":"code","90d3c6a3":"code","5b8dda3f":"code","d403231d":"code","39d9441f":"code","e363420f":"code","473c7d6a":"code","d2e7bc41":"code","38e8f72f":"code","bd9e982a":"code","d51a06db":"code","5ca43a60":"code","646469cd":"code","273846c9":"code","2441e109":"code","7a649171":"code","fb090126":"code","2b41063e":"code","4c171359":"code","ef946870":"code","0faa951f":"code","7e003942":"code","537a8e65":"code","818bcf88":"code","64454c2f":"code","13916f0d":"code","4bd9c45f":"code","e1158e22":"code","5da55a82":"code","92db8c09":"code","400c1b0e":"markdown","cedb0660":"markdown","6b50efe6":"markdown","303fcf4d":"markdown","a9989dfe":"markdown"},"source":{"ee2469ee":"import pandas as pd\nimport numpy as np\nimport plotnine \nfrom plotnine import *\nimport os, sys, gc\nfrom tqdm.notebook import tqdm\nimport warnings \nwarnings.filterwarnings('ignore')","9d6073df":"path = '..\/input\/t-academy-recommendation2\/books\/'\nprint(os.listdir(path))","66cef7be":"books = pd.read_csv(path + \"books.csv\")\nbook_tags = pd.read_csv(path + \"book_tags.csv\")\ntrain = pd.read_csv(path + \"train.csv\")\ntest = pd.read_csv(path + \"test.csv\")\ntags = pd.read_csv(path + \"tags.csv\")\nto_read = pd.read_csv(path + \"to_read.csv\")","4733eaed":"train['book_id'] = train['book_id'].astype(str)\ntest['book_id'] = test['book_id'].astype(str)\nbooks['book_id'] = books['book_id'].astype(str)","f8c55eb7":"popular_rec_model = books.sort_values(by='books_count', ascending=False)['book_id'].values[0:500]","d9733296":"sol = test.groupby(['user_id'])['book_id'].agg({'unique'}).reset_index()\ngt = {}\nfor user in tqdm(sol['user_id'].unique()): \n    gt[user] = list(sol[sol['user_id'] == user]['unique'].values[0])","4c68eeb9":"rec_df = pd.DataFrame()\nrec_df['user_id'] = train['user_id'].unique()","18e46a45":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf = TfidfVectorizer(stop_words='english')\ntfidf_matrix = tfidf.fit_transform(books['title'])\nprint(tfidf_matrix.shape)","c3e58dcc":"from sklearn.metrics.pairwise import cosine_similarity\ncosine_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\ncosine_matrix.shape","cdfb7076":"# book title\uc640 id\ub97c \ub9e4\ud551\ud560 dictionary\ub97c \uc0dd\uc131\ud574\uc90d\ub2c8\ub2e4. \nbook2id = {}\nfor i, c in enumerate(books['title']): book2id[i] = c\n\n# id\uc640 book title\ub97c \ub9e4\ud551\ud560 dictionary\ub97c \uc0dd\uc131\ud574\uc90d\ub2c8\ub2e4. \nid2book = {}\nfor i, c in book2id.items(): id2book[c] = i\n    \n# book_id\uc640 title\ub97c \ub9e4\ud551\ud560 dictionary\ub97c \uc0dd\uc131\ud574\uc90d\ub2c8\ub2e4.\nbookid2book = {}\nfor i, j in zip(books['title'].values, books['book_id'].values):\n    bookid2book[i] = j","90bab402":"books['title'].head()","c95341ea":"idx = id2book['Twilight (Twilight, #1)']  \nsim_scores = [(book2id[i], c) for i, c in enumerate(cosine_matrix[idx]) if i != idx] \nsim_scores = sorted(sim_scores, key = lambda x: x[1], reverse=True)\nsim_scores[0:10] ","8ace4636":"train = pd.merge(train, books[['book_id', 'title']], how='left', on='book_id')\ntrain.head()","96e50923":"# 0. \ud559\uc2b5\uc14b\uc5d0\uc11c \uc81c\ubaa9\uc774 \uc788\ub294 \uacbd\uc6b0\uc5d0 \ub300\ud574\uc11c\ub9cc \uc9c4\ud589\ntf_train = train[train['title'].notnull()].reset_index(drop=True)\ntf_train['idx2title'] = tf_train['title'].apply(lambda x: id2book[x])\ntf_train.head()","adb3be52":"idx2title2book = {}\nfor i, j in zip(tf_train['idx2title'].values, tf_train['book_id'].values):\n    idx2title2book[i] = j","90d3c6a3":"# 1. \uac01 \uc720\uc800\ubcc4\ub85c \uc77d\uc740 \ucc45\uc758 \ubaa9\ub85d\uc744 \uc218\uc9d1 \nuser = 7\nread_list = tf_train.groupby(['user_id'])['idx2title'].agg({'unique'}).reset_index()\nseen = read_list[read_list['user_id'] == user]['unique'].values[0]\nseen","5b8dda3f":"# 2. \uc77d\uc740 \ucc45\uacfc \uc720\uc0ac\ud55c \ucc45 \ucd94\ucd9c \n## 343\ubc88\uc9f8 \ucc45\uacfc \ub2e4\ub978 \ucc45\ub4e4\uac04\uc758 \uc720\uc0ac\ub3c4 \ncosine_matrix[343]","d403231d":"# 2. \uc77d\uc740 \ucc45\uacfc \uc720\uc0ac\ud55c \ucc45 \ucd94\ucd9c \ntotal_cosine_sim = np.zeros(len(book2id))\nfor book_ in seen: \n    # 3. \ubaa8\ub4e0 \ucc45\uc5d0 \ub300\ud574\uc11c \uc720\uc0ac\ub3c4\ub97c \ub354\ud55c \uac12\uc744 \uacc4\uc0b0 \n    # 343\ubc88\uc9f8 \ucc45\uacfc 248\uc758 \uc720\uc0ac\ub3c4\uac00 \ubaa8\ub450 \uacb0\ud569\ub41c \uc720\uc0ac\ub3c4\n    total_cosine_sim += cosine_matrix[book_]","39d9441f":"# 4. 3\uc5d0\uc11c \uc720\uc0ac\ub3c4\uac00 \uac00\uc7a5 \ub192\uc740 \uc21c\uc11c\ub300\ub85c \ucd94\ucd9c\nsim_scores = [(i, c) for i, c in enumerate(total_cosine_sim) if i not in seen] # \uc790\uae30 \uc790\uc2e0\uc744 \uc81c\uc678\ud55c \uc601\ud654\ub4e4\uc758 \uc720\uc0ac\ub3c4 \ubc0f \uc778\ub371\uc2a4\ub97c \ucd94\ucd9c \nsim_scores = sorted(sim_scores, key = lambda x: x[1], reverse=True) # \uc720\uc0ac\ub3c4\uac00 \ub192\uc740 \uc21c\uc11c\ub300\ub85c \uc815\ub82c \nsim_scores[0:5]","e363420f":"book2id[4809]","473c7d6a":"bookid2book[book2id[4809]]","d2e7bc41":"tf_train['user_id'].unique()","38e8f72f":"tf_train.head()","bd9e982a":"## \uc804\uccb4 \uc601\ud654\uc5d0 \ub300\ud574\uc11c \uc9c4\ud589 \ntotal_rec_list = {}\n\nread_list1 = train.groupby(['user_id'])['book_id'].agg({'unique'}).reset_index()\nread_list2 = tf_train.groupby(['user_id'])['idx2title'].agg({'unique'}).reset_index()\n\nfor user in tqdm(train['user_id'].unique()):\n    rec_list = []\n        \n    # \ub9cc\uc57d TF-IDF \uc18c\uc18d\uc758 \ucd94\ucc9c\ub300\uc0c1\uc774\ub77c\uba74 Contents \uae30\ubc18\uc758 \ucd94\ucc9c \n    if user in tf_train['user_id'].unique():\n        # 1. \uac01 \uc720\uc800\ubcc4\ub85c \uc77d\uc740 \ucc45\uc758 \ubaa9\ub85d\uc744 \uc218\uc9d1 \n        seen = read_list2[read_list2['user_id'] == user]['unique'].values[0]\n        # 2. \uc77d\uc740 \ucc45\uacfc \uc720\uc0ac\ud55c \ucc45 \ucd94\ucd9c \n        total_cosine_sim = np.zeros(len(book2id))\n        for book_ in seen: \n            # 3. \ubaa8\ub4e0 \ucc45\uc5d0 \ub300\ud574\uc11c \uc720\uc0ac\ub3c4\ub97c \ub354\ud55c \uac12\uc744 \uacc4\uc0b0 \n            # 343\ubc88\uc9f8 \ucc45\uacfc 248\uc758 \uc720\uc0ac\ub3c4\uac00 \ubaa8\ub450 \uacb0\ud569\ub41c \uc720\uc0ac\ub3c4\n            total_cosine_sim += cosine_matrix[book_]\n            \n        # 4. 3\uc5d0\uc11c \uc720\uc0ac\ub3c4\uac00 \uac00\uc7a5 \ub192\uc740 \uc21c\uc11c\ub300\ub85c \ucd94\ucd9c\n        sim_scores = [(bookid2book[book2id[i]], c) for i, c in enumerate(total_cosine_sim) if i not in seen] # \uc790\uae30 \uc790\uc2e0\uc744 \uc81c\uc678\ud55c \uc601\ud654\ub4e4\uc758 \uc720\uc0ac\ub3c4 \ubc0f \uc778\ub371\uc2a4\ub97c \ucd94\ucd9c \n        recs = sorted(sim_scores, key = lambda x: x[1], reverse=True)[0:300] # \uc720\uc0ac\ub3c4\uac00 \ub192\uc740 \uc21c\uc11c\ub300\ub85c \uc815\ub82c \n        for rec in recs: \n            if rec not in seen:\n                rec_list.append(rec)   \n        \n    # \uadf8\ub807\uc9c0 \uc54a\uc73c\uba74 \uc778\uae30\ub3c4 \uae30\ubc18\uc758 \ucd94\ucc9c \n    else: \n        seen = read_list1[read_list1['user_id'] == user]['unique'].values[0]\n        for rec in popular_rec_model[0:400]:\n            if rec not in seen:\n                rec_list.append(rec)\n                \n    total_rec_list[user] = rec_list[0:200]","d51a06db":"import six\nimport math\n\n# https:\/\/github.com\/kakao-arena\/brunch-article-recommendation\/blob\/master\/evaluate.py\n\nclass evaluate():\n    def __init__(self, recs, gt, topn=100):\n        self.recs = recs\n        self.gt = gt \n        self.topn = topn \n        \n    def _ndcg(self):\n        Q, S = 0.0, 0.0\n        for u, seen in six.iteritems(self.gt):\n            seen = list(set(seen))\n            rec = self.recs.get(u, [])\n            if not rec or len(seen) == 0:\n                continue\n\n            dcg = 0.0\n            idcg = sum([1.0 \/ math.log(i + 2, 2) for i in range(min(len(seen), len(rec)))])\n            for i, r in enumerate(rec):\n                if r not in seen:\n                    continue\n                rank = i + 1\n                dcg += 1.0 \/ math.log(rank + 1, 2)\n            ndcg = dcg \/ idcg\n            S += ndcg\n            Q += 1\n        return S \/ Q\n\n\n    def _map(self):\n        n, ap = 0.0, 0.0\n        for u, seen in six.iteritems(self.gt):\n            seen = list(set(seen))\n            rec = self.recs.get(u, [])\n            if not rec or len(seen) == 0:\n                continue\n\n            _ap, correct = 0.0, 0.0\n            for i, r in enumerate(rec):\n                if r in seen:\n                    correct += 1\n                    _ap += (correct \/ (i + 1.0))\n            _ap \/= min(len(seen), len(rec))\n            ap += _ap\n            n += 1.0\n        return ap \/ n\n\n\n    def _entropy_diversity(self):\n        sz = float(len(self.recs)) * self.topn\n        freq = {}\n        for u, rec in six.iteritems(self.recs):\n            for r in rec:\n                freq[r] = freq.get(r, 0) + 1\n        ent = -sum([v \/ sz * math.log(v \/ sz) for v in six.itervalues(freq)])\n        return ent\n    \n    def _evaluate(self):\n        print('MAP@%s: %s' % (self.topn, self._map()))\n        print('NDCG@%s: %s' % (self.topn, self._ndcg()))\n        print('EntDiv@%s: %s' % (self.topn, self._entropy_diversity()))","5ca43a60":"evaluate_func = evaluate(recs=total_rec_list, gt = gt, topn=200)\nevaluate_func._evaluate()","646469cd":"agg = train.groupby(['user_id'])['book_id'].agg({'unique'})\nagg.head()","273846c9":"# int\ud615\uc2dd\uc740 Word2vec\uc5d0\uc11c \ud559\uc2b5\uc774 \uc548\ub418\uc5b4\uc11c String\uc73c\ub85c \ubcc0\uacbd\ud574\uc90d\ub2c8\ub2e4. \nsentence = []\nfor user_sentence in agg['unique'].values:\n    sentence.append(list(map(str, user_sentence)))","2441e109":"# Word2vec\uc758 \ud559\uc2b5\uc744 \uc9c4\ud589\ud574\uc90d\ub2c8\ub2e4. \nfrom gensim.models import Word2Vec\nembedding_model = Word2Vec(sentence, size=20, window = 5, \n                           min_count=1, workers=4, iter=200, sg=1)","7a649171":"embedding_model.wv.most_similar(positive=['4893'], topn=10)","fb090126":"## \uc804\uccb4 \uc601\ud654\uc5d0 \ub300\ud574\uc11c \uc9c4\ud589 \ntotal_rec_list = {}\n\nread_list = train.groupby(['user_id'])['book_id'].agg({'unique'}).reset_index()\nfor user in tqdm(train['user_id'].unique()):\n    rec_list = []     \n    seen = read_list1[read_list1['user_id'] == user]['unique'].values[0]\n    word2vec_dict = {}\n    for book in seen: \n        for i in embedding_model.wv.most_similar(positive=[book], topn=300):\n            if i[0] not in seen: \n                if i[0] not in word2vec_dict.keys(): \n                    word2vec_dict[i[0]] = i[1]\n                else:\n                    word2vec_dict[i[0]] += i[1]\n                \n    rec_list = list(dict(sorted(word2vec_dict.items(), key = lambda x: x[1], reverse=True)).keys())\n    total_rec_list[user] = rec_list[0:200]","2b41063e":"evaluate_func = evaluate(recs=total_rec_list, gt = gt, topn=200)\nevaluate_func._evaluate()","4c171359":"book_tags.columns = ['book_id', 'tag_id', 'count']\nbook_tags['book_id'] = book_tags['book_id'].astype(str)\nbook_tags['tag_id'] = book_tags['tag_id'].astype(str)\n\ntags['tag_id'] = tags['tag_id'].astype(str)\n\nbook_tags = pd.merge(book_tags, tags, how='left', on='tag_id')\nbook_tags.head()","ef946870":"agg = book_tags.groupby(['book_id'])['tag_name'].agg({'unique'}).reset_index()\nagg.head()","0faa951f":"# \ud0dc\uadf8\uac04\uc758 \uc720\uc0ac\ub3c4 \uacc4\uc0b0 \n# int\ud615\uc2dd\uc740 Word2vec\uc5d0\uc11c \ud559\uc2b5\uc774 \uc548\ub418\uc5b4\uc11c String\uc73c\ub85c \ubcc0\uacbd\ud574\uc90d\ub2c8\ub2e4. \nsentence = []\nfor user_sentence in agg['unique'].values:\n    sentence.append(list(map(str, user_sentence)))","7e003942":"from gensim.models import doc2vec\ndoc_vectorizer = doc2vec.Doc2Vec(\n    dm=0,            # PV-DBOW \/ default 1\n    dbow_words=1,    # w2v simultaneous with DBOW d2v \/ default 0\n    window=10,        # distance between the predicted word and context words\n    size=100,        # vector size\n    alpha=0.025,     # learning-rate\n    seed=1234,\n    min_count=5,    # ignore with freq lower\n    min_alpha=0.025, # min learning-rate\n    workers=4,   # multi cpu\n    hs = 1,          # hierar chical softmax \/ default 0\n    negative = 10   # negative sampling \/ default 5\n)","537a8e65":"from collections import namedtuple\n\nTaggedDocument = namedtuple('TaggedDocument', 'words tags')\ntagged_train_docs = [TaggedDocument(c, [d]) for c, d in agg[['unique', 'book_id']].values]","818bcf88":"doc_vectorizer.build_vocab(tagged_train_docs)\nprint(str(doc_vectorizer))","64454c2f":"# \ubca1\ud130 \ubb38\uc11c \ud559\uc2b5\nfrom time import time\n\nstart = time()\n\nfor epoch in tqdm(range(5)):\n    doc_vectorizer.train(tagged_train_docs, total_examples=doc_vectorizer.corpus_count, epochs=doc_vectorizer.iter)\n    doc_vectorizer.alpha -= 0.002 # decrease the learning rate\n    doc_vectorizer.min_alpha = doc_vectorizer.alpha # fix the learning rate, no decay\n\n#doc_vectorizer.train(tagged_train_docs, total_examples=doc_vectorizer.corpus_count, epochs=doc_vectorizer.iter)\nend = time()\nprint(\"During Time: {}\".format(end-start))","13916f0d":"doc_vectorizer.docvecs.most_similar('1', topn=20)","4bd9c45f":"train.head()","e1158e22":"# tag \uc815\ubcf4\uac00 \uc788\ub294 \ucc45\uc774 \uc788\uace0 \uc544\ub2cc \ucc45\uc774 \uc788\uc5b4\uc11c \ud574\ub2f9 \ucc45\ub9cc \ucd94\ucd9c \nagg['type'] = '1'\ntrain = pd.merge(train, agg, how='left', on='book_id')","5da55a82":"## \uc804\uccb4 \uc601\ud654\uc5d0 \ub300\ud574\uc11c \uc9c4\ud589 \ntotal_rec_list = {}\n\nread_list1 = train.groupby(['user_id'])['book_id'].agg({'unique'}).reset_index()\nread_list2 = train[train['type'] == '1'].groupby(['user_id'])['book_id'].agg({'unique'}).reset_index()\nfor user in tqdm(train['user_id'].unique()):\n    rec_list = []\n    if user in read_list2['user_id'].unique():\n        seen = read_list2[read_list2['user_id'] == user]['unique'].values[0]\n        doc2vec_dict = {}\n        for book in seen: \n            for i in doc_vectorizer.docvecs.most_similar(positive=[book], topn=300): \n                if i[0] not in doc2vec_dict.keys(): \n                    doc2vec_dict[i[0]] = i[1]\n                else:\n                    doc2vec_dict[i[0]] += i[1]\n\n        rec_list = list(dict(sorted(doc2vec_dict.items(), key = lambda x: x[1], reverse=True)).keys())\n    else:\n        \n        seen = read_list1[read_list1['user_id'] == user]['unique'].values[0]\n        for rec in popular_rec_model[0:300]:\n            if rec not in seen:\n                rec_list.append(rec)\n    total_rec_list[user] = rec_list[0:200]","92db8c09":"evaluate_func = evaluate(recs=total_rec_list, gt = gt, topn=200)\nevaluate_func._evaluate()","400c1b0e":"## TF-IDF\ub97c \uc774\uc6a9\ud55c Contents Based Model ","cedb0660":"# 1. Goodbooks-10k \n- Link : https:\/\/www.kaggle.com\/zygmunt\/goodbooks-10k","6b50efe6":"0. \ud559\uc2b5\uc14b\uc5d0\uc11c \uc81c\ubaa9\uc774 \uc788\ub294 \uacbd\uc6b0\uc5d0 \ub300\ud574\uc11c\ub9cc \uc9c4\ud589\n1. \uac01 \uc720\uc800\ubcc4\ub85c \uc77d\uc740 \ucc45\uc758 \ubaa9\ub85d\uc744 \uc218\uc9d1 \n2. \uc77d\uc740 \ucc45\uacfc \uc720\uc0ac\ud55c \ucc45 \ucd94\ucd9c \n3. \ubaa8\ub4e0 \ucc45\uc5d0 \ub300\ud574\uc11c \uc720\uc0ac\ub3c4\ub97c \ub354\ud55c \uac12\uc744 \uacc4\uc0b0 \n4. 3\uc5d0\uc11c \uc720\uc0ac\ub3c4\uac00 \uac00\uc7a5 \ub192\uc740 \uc21c\uc11c\ub300\ub85c \ucd94\ucd9c ","303fcf4d":" ## Word2vec\uc744 \uc774\uc6a9\ud55c \ucd94\ucc9c\uc2dc\uc2a4\ud15c \n - Tag\uac04\uc758 \uc720\uc0ac\ub3c4 \n - \uc81c\ubaa9\uac04\uc758 \uc720\uc0ac\ub3c4 \n - \ucc45\uc758 \uc77d\uc740 \uc21c\uc11c\ub97c \ud1b5\ud55c \uc720\uc0ac\ub3c4 ","a9989dfe":"### \ud0dc\uadf8\ub97c \ud1b5\ud55c \uc720\uc0ac\ub3c4 \uacc4\uc0b0 "}}