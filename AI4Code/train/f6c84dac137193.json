{"cell_type":{"53a0f072":"code","1051410a":"code","3cda38d4":"code","7fce50f5":"code","f0ad7c47":"code","f34a3452":"code","a733a630":"code","e24c9d93":"code","b29681c5":"code","785532fb":"code","c0b8f45d":"markdown","4e575bef":"markdown","f334f2c3":"markdown"},"source":{"53a0f072":"# General imports\nimport numpy as np\nimport pandas as pd\nimport os, sys, gc, time, warnings, pickle, psutil, random\n\n# custom imports\nfrom multiprocessing import Pool        # Multiprocess Runs\n\nwarnings.filterwarnings('ignore')","1051410a":"########################### \u30d8\u30eb\u30d1\u30fc\n#################################################################################\n## \u30b7\u30fc\u30c9\u751f\u6210\n# \uff1a\u3059\u3079\u3066\u306e\u30d7\u30ed\u30bb\u30b9\u3092\u78ba\u5b9a\u7684\u306b\u3059\u308b\u305f\u3081\u306e\u30b7\u30fc\u30c9     # type: int\ndef seed_everything(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n\n    \n## \u30de\u30eb\u30c1\u30d7\u30ed\u30bb\u30b9\u5b9f\u884c\ndef df_parallelize_run(func, t_split):\n    num_cores = np.min([N_CORES,len(t_split)])\n    pool = Pool(num_cores)\n    df = pd.concat(pool.map(func, t_split), axis=1)\n    pool.close()\n    pool.join()\n    return df","3cda38d4":"########################### store ID\u306b\u3088\u3063\u3066\u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9\u3059\u308b\u30d8\u30eb\u30d1\u30fc\n#################################################################################\n# \u30c7\u30fc\u30bf\u8aad\u8fbc\ndef get_data_by_store(store):\n    \n    # \u57fa\u672c\u7279\u5fb4\u91cf\u3092\u8aad\u3093\u3067\u9023\u7d61\u3059\u308b\n    # BASE,PRICE,CALENDAR\u306f\u300c\u3010\u65e5\u672c\u8a9e\u3011M5 - Simple FE\u300d\u3092\u53c2\u7167\n    df = pd.concat([pd.read_pickle(BASE),\n                    pd.read_pickle(PRICE).iloc[:,2:],\n                    pd.read_pickle(CALENDAR).iloc[:,2:]],\n                    axis=1)\n    \n    # \u95a2\u9023\u3059\u308b\u5e97\u8217\u306e\u307f\u3092\u6b8b\u3059\n    df = df[df['store_id']==store]\n\n    # \u30e1\u30e2\u30ea\u306e\u5236\u9650\u304c\u3042\u308b\u305f\u3081\u3001LAG\u3068\u30a8\u30f3\u30b3\u30fc\u30c9\u7279\u5fb4\u91cf\u3092\u500b\u5225\u306b\u8aad\u307f\u53d6\u308a\u3001\n    # \u4e0d\u8981\u306a\u30a2\u30a4\u30c6\u30e0\u3092\u524a\u9664\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002 \n    # \u7279\u5fb4\u91cf\u30b0\u30ea\u30c3\u30c9\u304c\u6574\u5217\u3055\u308c\u308b\u305f\u3081\u3001\u5fc5\u8981\u306a\u884c\u306e\u307f\u3092\u4fdd\u6301\u3059\u308b\u305f\u3081\u306b index \u3092\u4f7f\u7528\u3067\u304d\u307e\u3059\n    # concat \u306f merge \u3088\u308a\u3082\u5c11\u306a\u3044\u30e1\u30e2\u30ea\u3092\u4f7f\u7528\u3059\u308b\u305f\u3081\u3001\u6574\u5217\u306f\u9069\u5207\u3067\u3059\u3002\n    df2 = pd.read_pickle(MEAN_ENC)[mean_features] # \u300cM5 - Custom features\u300d\u3092\u53c2\u7167\n    df2 = df2[df2.index.isin(df.index)]\n    \n    df3 = pd.read_pickle(LAGS).iloc[:,3:] # \u300cM5 - Lags features\u300d\u3092\u53c2\u7167\n    df3 = df3[df3.index.isin(df.index)]\n    \n    df = pd.concat([df, df2], axis=1)\n    del df2 # \u30e1\u30e2\u30ea\u5236\u9650\u306b\u9054\u3057\u306a\u3044\u3088\u3046\u306b\u524a\u9664\n    \n    df = pd.concat([df, df3], axis=1)\n    del df3 # \u30e1\u30e2\u30ea\u5236\u9650\u306b\u9054\u3057\u306a\u3044\u3088\u3046\u306b\u524a\u9664\n    \n    # \u7279\u5fb4\u91cf\u30ea\u30b9\u30c8\u3092\u4f5c\u6210\u3059\u308b\n    features = [col for col in list(df) if col not in remove_features]\n    df = df[['id','d',TARGET]+features]\n    \n    # \u6700\u521d\u306en\u884c\u3092\u30b9\u30ad\u30c3\u30d7\n    df = df[df['d']>=START_TRAIN].reset_index(drop=True)\n    \n    return df, features\n\n# \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u5f8c\u306b\u30c6\u30b9\u30c8\u3092\u518d\u7d50\u5408\ndef get_base_test():\n    base_test = pd.DataFrame()\n\n    for store_id in STORES_IDS:\n        temp_df = pd.read_pickle('test_'+store_id+'.pkl')\n        temp_df['store_id'] = store_id\n        base_test = pd.concat([base_test, temp_df]).reset_index(drop=True)\n    \n    return base_test\n\n\n########################### \u52d5\u7684\u79fb\u52d5LAG\u3092\u4f5c\u6210\u3059\u308b\u30d8\u30eb\u30d1\u30fc\n#################################################################################\ndef make_lag(LAG_DAY):\n    lag_df = base_test[['id','d',TARGET]]\n    col_name = 'sales_lag_'+str(LAG_DAY)\n    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(LAG_DAY)).astype(np.float16)\n    return lag_df[[col_name]]\n\n\ndef make_lag_roll(LAG_DAY):\n    shift_day = LAG_DAY[0]\n    roll_wind = LAG_DAY[1]\n    lag_df = base_test[['id','d',TARGET]]\n    col_name = 'rolling_mean_tmp_'+str(shift_day)+'_'+str(roll_wind)\n    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(shift_day).rolling(roll_wind).mean())\n    return lag_df[[col_name]]","7fce50f5":"########################### \u30e2\u30c7\u30eb\u30d1\u30e9\u30e1\u30fc\u30bf\n#################################################################################\nimport lightgbm as lgb\nlgb_params = {\n                    'boosting_type': 'gbdt',\n                    'objective': 'tweedie',\n                    'tweedie_variance_power': 1.1,\n                    'metric': 'rmse',\n                    'subsample': 0.5,\n                    'subsample_freq': 1,\n                    'learning_rate': 0.03,\n                    'num_leaves': 2**11-1,\n                    'min_data_in_leaf': 2**12-1,\n                    'feature_fraction': 0.5,\n                    'max_bin': 100,\n                    'n_estimators': 1400,\n                    'boost_from_average': False,\n                    'verbose': -1,\n                } ","f0ad7c47":"########################### \u5909\u6570\n#################################################################################\nVER = 1                          # \u30d0\u30fc\u30b8\u30e7\u30f3\nSEED = 42                        # \nseed_everything(SEED)            # \u79c1\u305f\u3061\u306f\u3059\u3079\u3066\u306e\u3082\u306e\u304c\u51fa\u6765\u308b\u3060\u3051\nlgb_params['seed'] = SEED        # \u78ba\u5b9a\u7684\u3067\u3042\u308b\u3053\u3068\u3092\u671b\u307f\u307e\u3059\u3002\nN_CORES = psutil.cpu_count()     # \u4f7f\u7528\u53ef\u80fd\u306aCPU\u30b3\u30a2\n\n\n#\u9650\u754c\u3068\u5b9a\u6570\nTARGET      = 'sales'            # \u30bf\u30fc\u30b2\u30c3\u30c8\nSTART_TRAIN = 0                  # \u4e00\u90e8\u306e\u884c\u3092\u30b9\u30ad\u30c3\u30d7\u3067\u304d\u307e\u3059(Nans\/faster training)\nEND_TRAIN   = 1913               # train\u306e\u7d42\u4e86\u65e5\nP_HORIZON   = 28                 # \u4e88\u6e2c\u671f\u9593\nUSE_AUX     = True               # \u4e8b\u524d\u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u4f7f\u7528\u3059\u308b\u304b\u3069\u3046\u304b\n\n#\u524a\u9664\u3059\u308b\u7279\u5fb4\u91cf\n## \u3053\u308c\u3089\u306e\u6a5f\u80fd\u306f\u30aa\u30fc\u30d0\u30fc\u30d5\u30a3\u30c3\u30c8\u306b\u3064\u306a\u304c\u308a\u307e\u3059 \n## \u307e\u305f\u306f test \u306b\u5b58\u5728\u3057\u306a\u3044\u5024\nremove_features = ['id','state_id','store_id',\n                   'date','wm_yr_wk','d',TARGET]\nmean_features   = ['enc_cat_id_mean','enc_cat_id_std',\n                   'enc_dept_id_mean','enc_dept_id_std',\n                   'enc_item_id_mean','enc_item_id_std'] \n\n#\u7279\u5fb4\u91cf\u306e\u30d1\u30b9\nORIGINAL = '..\/input\/m5-forecasting-accuracy\/'\nBASE     = '..\/input\/m5-simple-fe\/grid_part_1.pkl'\nPRICE    = '..\/input\/m5-simple-fe\/grid_part_2.pkl'\nCALENDAR = '..\/input\/m5-simple-fe\/grid_part_3.pkl'\nLAGS     = '..\/input\/m5-lags-features\/lags_df_28.pkl'\nMEAN_ENC = '..\/input\/m5-custom-features\/mean_encoding_df.pkl'\n\n\n# AUX\uff08\u4e8b\u524d\u5b66\u7fd2\u6e08\u307f\uff09\u30e2\u30c7\u30eb\u306e\u30d1\u30b9\nAUX_MODELS = '..\/input\/m5-aux-models\/'\n\n\n#STORES id\nSTORES_IDS = pd.read_csv(ORIGINAL+'sales_train_validation.csv')['store_id']\nSTORES_IDS = list(STORES_IDS.unique())\n\n\n#LAG\u4f5c\u6210\u7528\u306e\u5206\u5272\nSHIFT_DAY  = 28\nN_LAGS     = 15\nLAGS_SPLIT = [col for col in range(SHIFT_DAY,SHIFT_DAY+N_LAGS)]\nROLS_SPLIT = []\nfor i in [1,7,14]:\n    for j in [7,14,30,60]:\n        ROLS_SPLIT.append([i,j])","f34a3452":"########################### AUX\u30e2\u30c7\u30eb\n\nif USE_AUX:\n    lgb_params['n_estimators'] = 2\n    \n# \u3053\u3053\u306b\u6bd4\u8f03\u3067\u304d\u308b\u3044\u304f\u3064\u304b\u306e\u300c\u30ed\u30b0\u300d\u304c\u3042\u308a\u307e\u3059\n#Train CA_1\n#[100]\tvalid_0's rmse: 2.02289\n#[200]\tvalid_0's rmse: 2.0017\n#[300]\tvalid_0's rmse: 1.99239\n#[400]\tvalid_0's rmse: 1.98471\n#[500]\tvalid_0's rmse: 1.97923\n#[600]\tvalid_0's rmse: 1.97284\n#[700]\tvalid_0's rmse: 1.96763\n#[800]\tvalid_0's rmse: 1.9624\n#[900]\tvalid_0's rmse: 1.95673\n#[1000]\tvalid_0's rmse: 1.95201\n#[1100]\tvalid_0's rmse: 1.9476\n#[1200]\tvalid_0's rmse: 1.9434\n#[1300]\tvalid_0's rmse: 1.9392\n#[1400]\tvalid_0's rmse: 1.93446\n\n#Train CA_2\n#[100]\tvalid_0's rmse: 1.88949\n#[200]\tvalid_0's rmse: 1.84767\n#[300]\tvalid_0's rmse: 1.83653\n#[400]\tvalid_0's rmse: 1.82909\n#[500]\tvalid_0's rmse: 1.82265\n#[600]\tvalid_0's rmse: 1.81725\n#[700]\tvalid_0's rmse: 1.81252\n#[800]\tvalid_0's rmse: 1.80736\n#[900]\tvalid_0's rmse: 1.80242\n#[1000]\tvalid_0's rmse: 1.79821\n#[1100]\tvalid_0's rmse: 1.794\n#[1200]\tvalid_0's rmse: 1.78973\n#[1300]\tvalid_0's rmse: 1.78552\n#[1400]\tvalid_0's rmse: 1.78158","a733a630":"########################### \u30e2\u30c7\u30eb\u306e\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\n#################################################################################\nfor store_id in STORES_IDS:\n    print('Train', store_id)\n    \n    # \u73fe\u5728\u306e\u5e97\u8217\u306e\u30b0\u30ea\u30c3\u30c9\u3092\u53d6\u5f97\n    grid_df, features_columns = get_data_by_store(store_id)\n    \n    # \u30de\u30b9\u30af\n    # Train \uff081913\u672a\u6e80\u306e\u3059\u3079\u3066\u306e\u30c7\u30fc\u30bf\uff09\n    # \"Validation\" \uff08\u904e\u53bb28\u65e5\u9593-\u5b9f\u969b\u306e\u691c\u8a3c\u30bb\u30c3\u30c8\u3067\u306f\u306a\u3044\uff09\n    # Test \uff081913\u65e5\u3092\u8d85\u3048\u308b\u3059\u3079\u3066\u306e\u30c7\u30fc\u30bf\u3001\u518d\u5e30\u7684\u7279\u5fb4\u91cf\u306e\u30ae\u30e3\u30c3\u30d7\u3042\u308a\uff09\n    train_mask = grid_df['d']<=END_TRAIN\n    valid_mask = train_mask&(grid_df['d']>(END_TRAIN-P_HORIZON))\n    preds_mask = grid_df['d']>(END_TRAIN-100)\n    \n    # \u30de\u30b9\u30af\u3092\u9069\u7528\u3057\u3066lgb\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092bin\u3068\u3057\u3066\u4fdd\u5b58\u3057\u3001\n    # dtype\u5909\u63db\u4e2d\u306e\u30e1\u30e2\u30ea\u30b9\u30d1\u30a4\u30af\u3092\u524a\u6e1b\u3059\u308b\n    # https:\/\/github.com\/Microsoft\/LightGBM\/issues\/1032\n    # \u5909\u63db\u3092\u56de\u907f\u3059\u308b\u306b\u306f\u3001\u5e38\u306b np.float32 \u3092\u4f7f\u7528\u3059\u308b\u304b\u3001\n    # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3092\u958b\u59cb\u3059\u308b\u524d\u306b bin \u306b\u4fdd\u5b58\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\n    # https:\/\/www.kaggle.com\/c\/talkingdata-adtracking-fraud-detection\/discussion\/53773\n    train_data = lgb.Dataset(grid_df[train_mask][features_columns], \n                       label=grid_df[train_mask][TARGET])\n    train_data.save_binary('train_data.bin')\n    train_data = lgb.Dataset('train_data.bin')\n    \n    valid_data = lgb.Dataset(grid_df[valid_mask][features_columns], \n                       label=grid_df[valid_mask][TARGET])\n    \n    # \u5f8c\u306e\u4e88\u6e2c\u306e\u305f\u3081\u306b\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4e00\u90e8\u3092\u4fdd\u5b58\u3059\u308b\n    # \u518d\u5e30\u7684\u306b\u8a08\u7b97\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u7279\u5fb4\u91cf\u3092\u524a\u9664\u3059\u308b\n    grid_df = grid_df[preds_mask].reset_index(drop=True)\n    keep_cols = [col for col in list(grid_df) if '_tmp_' not in col]\n    grid_df = grid_df[keep_cols]\n    grid_df.to_pickle('test_'+store_id+'.pkl')\n    del grid_df\n    \n    # \u30b7\u30fc\u30c0\u30fc\u3092\u518d\u5ea6\u8d77\u52d5\u3057\u3066\u3001\n    # \u5404\u300c\u30b3\u30fc\u30c9\u884c\u300dnp.random\u304c\u300c\u9032\u5316\u300d\u3059\u308blgb\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3092100\uff05\u78ba\u5b9a\u7684\u306b\u3059\u308b\u305f\u3081\u3001\n    # \u300c\u30ea\u30bb\u30c3\u30c8\u300d\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002\n    seed_everything(SEED)\n    estimator = lgb.train(lgb_params,\n                          train_data,\n                          valid_sets = [valid_data],\n                          verbose_eval = 100,\n                          )\n    \n    # \u30e2\u30c7\u30eb\u3092\u4fdd\u5b58-\u5b9f\u969b\u306e\u300c.bin\u300d\u3067\u306f\u306a\u304f\n    # estimator = lgb.Booster(model_file='model.txt')\n    # pickle\u30d5\u30a1\u30a4\u30eb\u3068\u3059\u308b\u3068\u6700\u826f\u306e\u53cd\u5fa9\uff08\u307e\u305f\u306f\u4fdd\u5b58\u53cd\u5fa9\uff09\u3067\u306e\u307f\u4e88\u6e2c\u3067\u304d\u307e\u3059\n    # pickle.dump\u306f\u67d4\u8edf\u6027\u3092\u63d0\u4f9b\u3057\u307e\u3059\u3002\n    # estimator.predict(TEST, num_iteration=100)\n    # num_iteration - \u4e88\u6e2c\u3057\u305f\u3044\u53cd\u5fa9\u56de\u6570\n    # NULL\u307e\u305f\u306f<= 0\u306f\u6700\u9069\u306a\u53cd\u5fa9\u3092\u4f7f\u7528\u3059\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\n    model_name = 'lgb_model_'+store_id+'_v'+str(VER)+'.bin'\n    pickle.dump(estimator, open(model_name, 'wb'))\n\n    # \u4e00\u6642\u30d5\u30a1\u30a4\u30eb\u3068\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u524a\u9664\u3057\u3066\u3001\n    # HDD\u30b9\u30da\u30fc\u30b9\u3068RAM\u30e1\u30e2\u30ea\u3092\u89e3\u653e\u3057\u307e\u3059\n    !rm train_data.bin\n    del train_data, valid_data, estimator\n    gc.collect()\n    \n    # \u4e88\u6e2c\u306e\u305f\u3081\u306e\u30e2\u30c7\u30eb\u7279\u5fb4\u91cf\u306e\u4fdd\u6301\n    MODEL_FEATURES = features_columns","e24c9d93":"########################### \u4e88\u6e2c\n#################################################################################\n\n# \u4e88\u6e2c\u3092\u4fdd\u5b58\u3059\u308b\u305f\u3081\u306e\u30c0\u30df\u30fc\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u3092\u4f5c\u6210\u3059\u308b\nall_preds = pd.DataFrame()\n\n# \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30c7\u30fc\u30bf\u306e\u4e00\u90e8\u306b\u7d50\u5408\u3057\u3066\u3001\n# \u518d\u5e30\u7684\u306a\u7279\u5fb4\u91cf\u3092\u4f5c\u6210\u3057\u307e\u3059\nbase_test = get_base_test()\n\n# \u4e88\u6e2c\u6642\u9593\u3092\u6e2c\u5b9a\u3059\u308b\u30bf\u30a4\u30de\u30fc\nmain_time = time.time()\n\n# \u5404\u4e88\u6e2c\u65e5\u306b\u30eb\u30fc\u30d7\u3059\u308b\u79fb\u52d5LAG\u306f\u6700\u3082\u6642\u9593\u304c\u304b\u304b\u308b\u305f\u3081\u3001\n# 1\u65e5\u5168\u4f53\u3067\u8a08\u7b97\u3057\u307e\u3059\u3002\nfor PREDICT_DAY in range(1,29):    \n    print('Predict | Day:', PREDICT_DAY)\n    start_time = time.time()\n\n    # \u79fb\u52d5LAG\u3092\u8a08\u7b97\u3059\u308b\u305f\u3081\u306e\u4e00\u6642\u7684\u306a\u30b0\u30ea\u30c3\u30c9\u3092\u4f5c\u6210\u3059\u308b\n    grid_df = base_test.copy()\n    grid_df = pd.concat([grid_df, df_parallelize_run(make_lag_roll, ROLS_SPLIT)], axis=1)\n        \n    for store_id in STORES_IDS:\n        \n        # \u3059\u3079\u3066\u306e\u30e2\u30c7\u30eb\u3092\u8aad\u3093\u3067\u3001\u65e5\/\u5e97\u8217\u306e\u30da\u30a2\u3054\u3068\u306b\u4e88\u6e2c\u3092\u884c\u3046\n        model_path = 'lgb_model_'+store_id+'_v'+str(VER)+'.bin' \n        if USE_AUX:\n            model_path = AUX_MODELS + model_path\n        \n        estimator = pickle.load(open(model_path, 'rb'))\n        \n        day_mask = base_test['d']==(END_TRAIN+PREDICT_DAY)\n        store_mask = base_test['store_id']==store_id\n        \n        mask = (day_mask)&(store_mask)\n        base_test[TARGET][mask] = estimator.predict(grid_df[mask][MODEL_FEATURES])\n    \n    # \u9069\u5207\u306a\u5217\u306e\u540d\u524d\u3092\u4ed8\u3051\u3001all_preds \u306b\u8ffd\u52a0\u3057\u307e\u3059\n    temp_df = base_test[day_mask][['id',TARGET]]\n    temp_df.columns = ['id','F'+str(PREDICT_DAY)]\n    if 'id' in list(all_preds):\n        all_preds = all_preds.merge(temp_df, on=['id'], how='left')\n    else:\n        all_preds = temp_df.copy()\n        \n    print('#'*10, ' %0.2f min round |' % ((time.time() - start_time) \/ 60),\n                  ' %0.2f min total |' % ((time.time() - main_time) \/ 60),\n                  ' %0.2f day sales |' % (temp_df['F'+str(PREDICT_DAY)].sum()))\n    del temp_df\n    \nall_preds = all_preds.reset_index(drop=True)\nall_preds","b29681c5":"########################### \u66f8\u304d\u51fa\u3059\n#################################################################################\n# \u30b3\u30f3\u307a\u306e\u30b5\u30f3\u30d7\u30eb\u63d0\u51fa\u3092\u8aad\u3093\u3067\u3001\u4e88\u6e2c\u3092\u7d50\u5408\u3059\u308b\n# \u300c_validation\u300d\u30c7\u30fc\u30bf\u306e\u307f\u306e\u4e88\u6e2c\u304c\u3042\u308b\u305f\u3081\n# \u300c_evaluation\u300d\u30a2\u30a4\u30c6\u30e0\u306b\u5bfe\u3057\u3066fillna\uff08\uff09\u3092\u5b9f\u884c\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\nsubmission = pd.read_csv(ORIGINAL+'sample_submission.csv')[['id']]\nsubmission = submission.merge(all_preds, on=['id'], how='left').fillna(0)\nsubmission.to_csv('submission_v'+str(VER)+'.csv', index=False)","785532fb":"##\u4ee5\u4e0b\u306f\u7ffb\u8a33\u3057\u307e\u305b\u3093\u3002\u3054\u81ea\u8eab\u3067\u3054\u5224\u65ad\u304f\u3060\u3055\u3044\u3002\n\n# Summary\n\n# Of course here is no magic at all.\n# No \"Novel\" features and no brilliant ideas.\n# We just carefully joined all\n# our previous fe work and created a model.\n\n# Also!\n# In my opinion this strategy is a \"dead end\".\n# Overfits a lot LB and with 1 final submission \n# you have no option to risk.\n\n\n# Improvement should come from:\n# Loss function\n# Data representation\n# Stable CV\n# Good features reduction strategy\n# Predictions stabilization with NN\n# Trend prediction\n# Real zero sales detection\/classification\n\n\n# Good kernels references \n## (the order is random and the list is not complete):\n# https:\/\/www.kaggle.com\/ragnar123\/simple-lgbm-groupkfold-cv\n# https:\/\/www.kaggle.com\/jpmiller\/grouping-items-by-stockout-pattern\n# https:\/\/www.kaggle.com\/headsortails\/back-to-predict-the-future-interactive-m5-eda\n# https:\/\/www.kaggle.com\/sibmike\/m5-out-of-stock-feature\n# https:\/\/www.kaggle.com\/mayer79\/m5-forecast-attack-of-the-data-table\n# https:\/\/www.kaggle.com\/yassinealouini\/seq2seq\n# https:\/\/www.kaggle.com\/kailex\/m5-forecaster-v2\n# https:\/\/www.kaggle.com\/aerdem4\/m5-lofo-importance-on-gpu-via-rapids-xgboost\n\n\n# Features were created in these kernels:\n## \n# Mean encodings and PCA options\n# https:\/\/www.kaggle.com\/kyakovlev\/m5-custom-features\n##\n# Lags and rolling lags\n# https:\/\/www.kaggle.com\/kyakovlev\/m5-lags-features\n##\n# Base Grid and base features (calendar\/price\/etc)\n# https:\/\/www.kaggle.com\/kyakovlev\/m5-simple-fe\n\n\n# Personal request\n# Please don't upvote any ensemble and copypaste kernels\n## The worst case is ensemble without any analyse.\n## The best choice - just ignore it.\n## I would like to see more kernels with interesting and original approaches.\n## Don't feed copypasters with upvotes.\n\n## It doesn't mean that you should not fork and improve others kernels\n## but I would like to see params and code tuning based on some CV and analyse\n## and not only on LB probing.\n## Small changes could be shared in comments and authors can improve their kernel.\n\n## Feel free to criticize this kernel as my knowlege is very limited\n## and I can be wrong in code and descriptions. \n## Thank you.","c0b8f45d":" \u30e2\u30c7\u30eb\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u8a73\u3057\u304f\u898b\u3066\u307f\u307e\u3057\u3087\u3046\n\n*  'boosting_type': 'gbdt'\n \n \u3088\u308a\u9ad8\u901f\u306a\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u306e\u305f\u3081\u306e'goss'\u30aa\u30d7\u30b7\u30e7\u30f3\u304c\u3042\u308a\u307e\u3059 \n \u3057\u304b\u3057\u3001\u901a\u5e38\u306f\u30a2\u30f3\u30c0\u30fc\u30d5\u30a3\u30c3\u30c8\u306b\u3064\u306a\u304c\u308a\u307e\u3059\u3002 \n \u307e\u305f\u3001\u826f\u3044'dart'\u30e2\u30fc\u30c9\u3082\u3042\u308a\u307e\u3059 \n \u3057\u304b\u3057\u3001\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u306f\u6c38\u9060\u306b\u304b\u304b\u308a\u307e\u3059 \n \u30e2\u30c7\u30eb\u306e\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u306f\u3001\u305f\u304f\u3055\u3093\u306e\u30e9\u30f3\u30c0\u30e0\u306a\u8981\u7d20\u306b\u4f9d\u5b58\u3057\u307e\u3059\u3002\n https:\/\/www.kaggle.com\/c\/home-credit-default-risk\/discussion\/60921\n\n*  'objective': 'tweedie'\n\n Tweedie Gradient Boosting\n Tweedie \u5206\u5e03\u306f\u30bc\u30ed\u306b\u5927\u91cf\u306e\u70b9\u3092\u6301\u3064\u7d76\u5bfe\u9023\u7d9a\u306a\u78ba\u7387\u5206\u5e03\u3067\n \u5404\u30a4\u30d9\u30f3\u30c8 X \u304c \u30ac\u30f3\u30de\u5206\u5e03\u306b\u5f93\u3044\u3001\u30a4\u30d9\u30f3\u30c8\u306e\u8d77\u3053\u308b\u56de\u6570 N \u304c\n \u30dd\u30a2\u30bd\u30f3\u5206\u5e03\u306b\u5f93\u3046\u78ba\u7387\u904e\u7a0b \uff08\u8907\u5408\u30dd\u30a2\u30bd\u30f3\u904e\u7a0b\uff09\u3068\u3057\u3066\u8868\u3055\u308c\u308b\u3002\n https:\/\/arxiv.org\/pdf\/1811.10192.pdf\n\n \uff08\u79c1\u306b\u3068\u3063\u3066\u306f\uff09\u5947\u5999\u3067\u3059\u304c\u3001Tweedie\u306f\u7d50\u679c\u306b\u8fd1\u3044\u3067\u3059 \n \u79c1\u81ea\u8eab\u306e\u919c\u3044\u640d\u5931\u306b\u3002 \n \u79c1\u306e\u30a2\u30c9\u30d0\u30a4\u30b9\u306f\u72ec\u81ea\u306e\u640d\u5931\u95a2\u6570\u3092\u4f5c\u6210\u3059\u308b\u3053\u3068\u3067\u3059\u3002\n https:\/\/www.kaggle.com\/c\/m5-forecasting-accuracy\/discussion\/140564\n https:\/\/www.kaggle.com\/c\/m5-forecasting-accuracy\/discussion\/143070\n poisson\u30ab\u30fc\u30cd\u30eb\u304c\u767b\u5834\u3057\u305f\u5f8c\u3001\u72ec\u81ea\u306e\u640d\u5931\u95a2\u6570\n \uff08Kaggler\u306f\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u30c6\u30b9\u30c8\u3068\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u306b\u975e\u5e38\u306b\u512a\u308c\u3066\u3044\u307e\u3059\uff09\u3092\u3059\u3067\u306b\u4f7f\u7528\u3057\u3066\u3044\u308b\u3068\u601d\u3044\u307e\u3059\u3002 \n Tweedie\u304c\u6a5f\u80fd\u3059\u308b\u7406\u7531\u3092\u7406\u89e3\u3057\u3066\u307f\u3066\u304f\u3060\u3055\u3044\u3002 \n \u304a\u305d\u3089\u304f\u3001\u65b0\u7279\u5fb4\u91cf\u306e\u30aa\u30d7\u30b7\u30e7\u30f3\u307e\u305f\u306f\u30c7\u30fc\u30bf\u5909\u63db\uff08\u30bf\u30fc\u30b2\u30c3\u30c8\u5909\u63db\uff1f\uff09\u304c\u8868\u793a\u3055\u308c\u307e\u3059\u3002\n 'tweedie_variance_power': 1.1\n default = 1.5\n \u30ac\u30f3\u30de\u5206\u5e03\u306b\u30b7\u30d5\u30c8\u3059\u308b\u306b\u306f\u3001\u3053\u308c\u30922\u306b\u8fd1\u304f\u8a2d\u5b9a\u3057\u307e\u3059\n \u30dd\u30a2\u30bd\u30f3\u5206\u5e03\u306b\u30b7\u30d5\u30c8\u3059\u308b\u306b\u306f\u3001\u3053\u308c\u30921\u306b\u8fd1\u3065\u3051\u307e\u3059\n \u79c1\u306eCV\u306f1.1\u304c\u6700\u9069\u3067\u3042\u308b\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u307e\u3059\n \u3057\u304b\u3057\u3001\u3042\u306a\u305f\u306f\u3042\u306a\u305f\u81ea\u8eab\u306e\u9078\u629e\u3092\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\n\n*  'metric': 'rmse'\n\n \u30b3\u30f3\u30da\u306e\u6307\u6a19\u304c\u9055\u3046\u305f\u3081\u3001\u4f55\u3082\u610f\u5473\u3057\u307e\u305b\u3093\u3002\n \u3053\u3053\u3067\u306f\u65e9\u671f\u505c\u6b62\u3092\u4f7f\u7528\u3057\u307e\u305b\u3093\u3002\n \u3057\u305f\u304c\u3063\u3066\u3001rmse\u306f\u4e00\u822c\u7684\u306a\u30e2\u30c7\u30eb\u306e\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u306e\u6982\u8981\u306e\u307f\u3092\u63d0\u4f9b\u3057\u307e\u3059\u3002\n \u307e\u305f\u3001\u300c\u507d\u306e\u300d\u691c\u8a3c\u30bb\u30c3\u30c8\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002\n \uff08\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30bb\u30c3\u30c8\u306e\u4e00\u90e8\u3068\u306a\u308a\u3001\u4e00\u822c\u7684\u306armse\u30b9\u30b3\u30a2\u3067\u3082\u610f\u5473\u304c\u306a\u3044\u305f\u3081\uff09\n https:\/\/www.kaggle.com\/c\/m5-forecasting-accuracy\/discussion\/133834\n\n*  'subsample': 0.5\n\n \u30aa\u30fc\u30d0\u30fc\u30d5\u30a3\u30c3\u30c8\u3068\u6226\u3046\u305f\u3081\u306b\u5f79\u7acb\u3061\u307e\u3059\n \u3053\u308c\u306f\u30ea\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u305b\u305a\u306b\u30c7\u30fc\u30bf\u306e\u4e00\u90e8\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u9078\u629e\u3057\u307e\u3059\n CV\u306b\u3088\u3063\u3066\u9078\u629e\uff08\u79c1\u306eCV\u306f\u9593\u9055\u3063\u3066\u3044\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\uff01\n \u6b21\u306e\u30ab\u30fc\u30cd\u30eb\u306fCV\u306b\u3064\u3044\u3066\u3067\u3059\n\n* 'subsample_freq': 1\n\n \u30d0\u30ae\u30f3\u30b0\u306e\u983b\u5ea6 \n \u30c7\u30d5\u30a9\u30eb\u30c8\u5024-\u826f\u3055\u305d\u3046\n\n* 'learning_rate': 0.03\n\n CV\u304c\u9078\u629e\n \u5c0f\u3055\u3044-\u3088\u308a\u9577\u3044\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\n \u3057\u304b\u3057\u3001\u300c\u6975\u5c0f\u5024\u300d\u3067\u505c\u6b62\u3059\u308b\u30aa\u30d7\u30b7\u30e7\u30f3\u304c\u3042\u308a\u307e\u3059\n \u5927\u304d\u3044-\u3088\u308a\u901f\u3044\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\n \u3057\u304b\u3057\u3001\u300c\u6700\u5c0f\u5024\u300d\u304c\u898b\u3064\u304b\u308a\u307e\u305b\u3093\n\n* 'num_leaves': 2**11-1\n* 'min_data_in_leaf': 2**12-1\n\n \u3088\u308a\u591a\u304f\u306e\u7279\u5fb4\u91cf\u3092\u4f7f\u7528\u3059\u308b\u3088\u3046\u306b\u30e2\u30c7\u30eb\u3092\u5f37\u5236\u3059\u308b\n \u300c\u518d\u5e30\u7684\u300d\u30a8\u30e9\u30fc\u306e\u5f71\u97ff\u3092\u6e1b\u3089\u3059\u305f\u3081\u306b\u5fc5\u8981\u3067\u3059\n \u307e\u305f\u3001\u30aa\u30fc\u30d0\u30fc\u30d5\u30a3\u30c3\u30c8\u306b\u3064\u306a\u304c\u308a\u307e\u3059\n \u3060\u304b\u3089\u5c0f\u3055\u3081\u306b \n 'max_bin': 100\n\n* l1, l2 \u6b63\u5247\u5316\n\n https:\/\/towardsdatascience.com\/l1-and-l2-regularization-methods-ce25e7fc831c\n \u5c0f\u3055\u306a\u826f\u3044\u8aac\u660e\n l2\u306f\u3088\u308a\u5927\u304d\u306anum_leaves\u3067\u52d5\u4f5c\u3057\u307e\u3059 \n \u3057\u304b\u3057\u3001\u79c1\u306eCV\u306f\u30d6\u30fc\u30b9\u30c8\u3092\u8868\u793a\u3057\u307e\u305b\u3093\n                    \n* 'n_estimators': 1400\n\n CV\u306f\u3001\u5dde\/\u5e97\u8217\u3054\u3068\u306b\u7570\u306a\u308b\u5024\u304c\u5fc5\u8981\u3067\u3042\u308b\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002\n \u73fe\u5728\u306e\u5024\u306f\u3001\u4e00\u822c\u7684\u306a\u76ee\u7684\u3067\u9078\u629e\u3055\u308c\u3066\u3044\u307e\u3059\u3002\n \u30d1\u30d6\u30ea\u30c3\u30afLB\u306b\u9069\u5408\u3057\u306a\u3044\u3088\u3046\u306b\u6ce8\u610f\u3057\u3066\u65e9\u671f\u505c\u6b62\u3092\u4f7f\u7528\u3057\u306a\u3044\u305f\u3081\u3067\u3059\u3002\n\n* 'feature_fraction': 0.5\n\n LightGBM\u306f\u3001\u5404\u53cd\u5fa9\uff08\u6c7a\u5b9a\u6728\uff09\u3067\u7279\u5fb4\u91cf\u306e\u4e00\u90e8\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u9078\u629e\u3057\u307e\u3059\u3002 \n \u3068\u3066\u3082\u305f\u304f\u3055\u3093\u306e\u7279\u5fb4\u91cf\u304c\u3042\u308a\u307e\u3059\u3002\n \u305d\u306e\u591a\u304f\u306f\u300c\u91cd\u8907\u300d\u3067\u3042\u308a\u3001\u591a\u304f\u306f\u5358\u306a\u308b\u300c\u30ce\u30a4\u30ba\u300d\u3067\u3059\n CV\u306b\u3088\u308b\u826f\u3044\u5024 - 0.5-0.7\n\n* 'boost_from_average': False\n\n boost_from_average\u306b\u30ab\u30b9\u30bf\u30e0\u640d\u5931\u300cTrue\u300d\u306f\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3092\u3088\u308a\u901f\u304f\u3057\u307e\u3059\u304c\u3001\n \u3044\u304f\u3064\u304b\u306e\u300c\u554f\u984c\u300d\u304c\u3042\u308a\u307e\u3059\u3002\n \u79c1\u305f\u3061\u306e\u30b1\u30fc\u30b9\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u304c\u3001\u77ed\u6240\u306b\u6ce8\u610f\u3057\u3066\u4f7f\u7528\u3057\u3066\u304f\u3060\u3055\u3044\n https:\/\/github.com\/microsoft\/LightGBM\/issues\/1514","4e575bef":"\u3053\u306eNotebook\u306fYakovlev\u306e [M5 - Three shades of Dark: Darker magic](https:\/\/www.kaggle.com\/kyakovlev\/m5-three-shades-of-dark-darker-magic) \u3092\u7814\u7a76\u3059\u308b\u305f\u3081\u306b\u4f5c\u3063\u305f\u3082\u306e\u3092Share\u3057\u3066\u3044\u307e\u3059\u3002\n\nThank Yakovlev for sharing very much.\n\n\u6b21\u306eNotebook\u306b\u3082\u4f9d\u5b58\u3057\u3066\u3044\u308b\u3053\u3068\u306b\u6ce8\u610f\u3057\u307e\u3057\u3087\u3046\u3002<br>\n[\u3010\u65e5\u672c\u8a9e\u3011M5 - Simple FE](https:\/\/www.kaggle.com\/ejunichi\/m5-simple-fe)<br>\n[M5 - Custom features](https:\/\/www.kaggle.com\/kyakovlev\/m5-custom-features)<br>\n[M5 - Lags features](https:\/\/www.kaggle.com\/kyakovlev\/m5-lags-features)\u3000","f334f2c3":"* \u7d50\u679c\u3092\u5f97\u308b\u305f\u3081\u306b\u4f55\u6642\u9593\u3082\u5f85\u3061\u305f\u304f\u306a\u3044\u5834\u5408\u306f\u3001\u5404\u30b9\u30c8\u30a2\u3092\u500b\u5225\u306e\u30ab\u30fc\u30cd\u30eb\u3067\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3057\u3066\u304b\u3089\u3001\u7d50\u679c\u306b\u53c2\u52a0\u3057\u307e\u3059\u3002\n* \u4e8b\u524d\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u4f7f\u7528\u3059\u308b\u5834\u5408\u306f\u3001\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3092\u30b9\u30ad\u30c3\u30d7\u3067\u304d\u307e\u3059\uff08\u3053\u306e\u5834\u5408\u3001\u30c0\u30df\u30fc\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3092\u5b9f\u884c\u3057\u3066\u3001\u30e1\u30e2\u30ea\u306b\u554f\u984c\u304c\u306a\u304f\u3001\u3053\u306e\uff08\u3059\u3079\u3066\u306e\u30ab\u30fc\u30cd\u30eb\uff09\u30b3\u30fc\u30c9\u3092\u5b89\u5168\u306b\u4f7f\u7528\u3067\u304d\u308b\u3053\u3068\u3092\u793a\u3057\u307e\u3059\uff09\u3002"}}