{"cell_type":{"bc0cffdb":"code","4359f09e":"code","a46f9f8d":"code","efa7b01d":"code","edb38539":"markdown","f201ae82":"markdown","26bb4cce":"markdown"},"source":{"bc0cffdb":"import advertools as adv\nimport pandas as pd\n\ncx = 'YOUR_CUSTOM_SEARCH_ENGINE'\nkey = 'YOUR_KEY'\nadv.__version__","4359f09e":"wikipedia_urls = ['https:\/\/www.wikipedia.org\/',  # search for this domain as a keyword\n                  'https:\/\/www.wikipedia.org\/wrong_page',  # search for this domain as a keyword (does not exist)\n                  'site:https:\/\/www.wikipedia.org\/', # search for this site\/page (exists)\n                  'site:https:\/\/www.wikipedia.org\/wrong_again'] # search for this site\/page (does not exist)","a46f9f8d":"# wikipedia = adv.serp_goog(cx=cx, key=key, q=wikipedia_urls)","efa7b01d":"wikipedia = pd.read_csv('..\/input\/wikipedia_serps.csv')\nwikipedia[['searchTerms', 'rank', 'title', 'displayLink', 'formattedTotalResults']]","edb38539":"The first two queries, namely `https:\/\/www.wikipedia.org\/` and `https:\/\/www.wikipedia.org\/wrong_page`, search for two URLs. Because there is no operator, they are treated as keywords. So, although the page `https:\/\/www.wikipedia.org\/wrong_page` does not exist, Google tries to find something that might be relevant, and it actually found 22,300 pages for this query. \n\nThe last two queries `site:https:\/\/www.wikipedia.org\/` and `site:https:\/\/www.wikipedia.org\/wrong_again` are not regular keywords. They ask for information about those specific URLs (make sure to include the full path with `https:\/\/www` so you don't get information about the domain). \n\nAs you can see, we have one result for the URL that exists and NaN for the URL that does not.  \nAnd we are Done! \n\nFor real life scenarios, you would have a large set of URLs to check and make sure they are indexed. All you have to do is create a list of those URLs and pass them as one argument to the `serp_goog` function (including the `site:` operator). The function handles looping and concatenating the responses into the final DataFrame.\n\nThe cost factor might become significant if you have hundreds of thousands of URLs, if you check them regularly. You can judget best, based on your budget and priorities.  \nMy suggestion is to pick the top key pages that absolutely must be indexed and focus on those. Ideally, they would link to deeper pages, and ensure they are findable by search engine spiders.","f201ae82":"# How to Check if a URL is Indexed by Google With Python\nHow can you check if a certain URL is indexed or not by Google?  \nAlthough you can check the health and quality of indexing for your pages, no report tells you which pages are not indexed. Those pages might not be indexed because either Google thinks they are duplicates\/spam, or Google simply can't see them for some reason. In any case, it is crucial to know whether your URLs are indexed. \n\nMy suggested approach to find this out uses the official Google Custom Search Engine API to achieve this. \n\nFor a more in-depth examples on how to use that for SEO research: \n\n- [Tutorial on how to use Google CSE API for SERP research](https:\/\/www.semrush.com\/blog\/analyzing-search-engine-results-pages\/)\n- [Tutorial on using Google CSE and YouTube API for SERPs research on both platforms](https:\/\/www.kaggle.com\/eliasdabbas\/recipes-keywords-ranking-on-google-and-youtube)\n\nThe basic idea is simple. We send a query to the API, and analyze the response.  \nThe important thing is to run the query using the `site:` operator together with the URLs that we want to check.  \nSo, if we had `http:\/\/mysite.com\/1`, `http:\/\/mysite.com\/2`, and `http:\/\/mysite.com\/3`, we would have to send requests for `site:http:\/\/mysite.com\/1`, `site:http:\/\/mysite.com\/2`, and `site:http:\/\/mysite.com\/3` as the queries.\n\nTrying the URLs alone or with another operator like `info:` would not work, because Google would return similar pages and URLs. With the `site:` operator, we are specifically asking for information about the domain or URL, and if it doesn't exist, we would have zero results. \n\nHere are the steps to set up an account to import data:\n\n1. [Create a custom search engine](https:\/\/cse.google.com\/cse\/). At first, you might be asked to enter a site to search. Enter any domain, then go to the control panel and remove it. Make sure you enable \"Search the entire web\" and image search. You will also need to get your search engine ID, which you can find on the control panel page.\n2. [Enable the custom search API](https:\/\/console.cloud.google.com\/apis\/library\/customsearch.googleapis.com). The service will allow you to retrieve and display search results from your custom search engine programmatically. You will need to create a project for this first.\n3. [Create credentials for this project](https:\/\/console.developers.google.com\/apis\/api\/customsearch.googleapis.com\/credentials) so you can get your key.\n4. [Enable billing for your project](https:\/\/console.cloud.google.com\/billing\/projects) if you want to run more than 100 queries per day. The first 100 queries are free; then for each additional 1,000 queries, you pay USD $5.\n\n","26bb4cce":"The following code gets the data from Google. You simply pass in the list of queries as the parameter `q` and you get all results in one DataFrame. `cx` and `key` are used to authenticate, as mentioned above."}}