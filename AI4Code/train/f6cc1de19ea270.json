{"cell_type":{"2c5b2b34":"code","4c78d50c":"code","9f857a58":"code","c99d8e18":"code","955cec78":"code","a2dc2ca5":"code","50fe1c24":"code","f3734844":"code","0f25f26b":"code","c820cdb9":"code","79ddcfe2":"code","a66e235d":"code","31407f53":"code","6de08f20":"code","25bc9a61":"code","b0ab6333":"code","bb034ecb":"code","7b822370":"code","8fa7b89a":"markdown","7eeff02b":"markdown"},"source":{"2c5b2b34":"# \u4e26\u5217\u5316\n# CPU\u3092\u8907\u6570\u4f7f\u3063\u3066\u304f\u308c\u308b\u3088\u3046\u306b\u306a\u308b\u3089\u3057\u3044\u3002\u3053\u308c\u304c\u3042\u308b\u306e\u3068\u7121\u3044\u306e\u3068\u3067\u306f\u51e6\u7406\u306e\u6642\u9593\u304c\u5168\u7136\u9055\u3046\nfrom joblib import Parallel, delayed\n\nimport pandas as pd\nimport numpy as np\nimport scipy as sc\n\n# \u4ea4\u5dee\u691c\u8a3cKfold\nfrom sklearn.model_selection import KFold\n# lgbm\nimport lightgbm as lgb\n# \u8b66\u544a\u6587\u3092\u51fa\u3055\u306a\u3044\u3088\u3046\u306b\u3059\u308b\nimport warnings\nfrom sklearn.cluster import KMeans\nwarnings.filterwarnings('ignore')\n\npd.set_option('max_columns', 300)","4c78d50c":"# WAP\u3092bid\u3068ask\u306e\u6700\u5927\u5024\u6700\u5c0f\u5024\u3092\u4f7f\u3063\u3066\u8a08\u7b97\u3057\u305f\u3082\u306e\u30682\u756a\u76ee\u306e\u3082\u306e\u3092\u4f7f\u3063\u305f\u5834\u5408\u306e\u95a2\u6570\ndef calc_wap1(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n    return wap\n\n# Log return\u3092\u8a08\u7b97\u3059\u308b\u95a2\u6570\ndef log_return(series):\n    return np.log(series).diff()\n\n# \u76ee\u7684\u5909\u6570\u3067\u3042\u308b\u30dc\u30e9\u30c6\u30a3\u30ea\u30c6\u30a3\u3092\u8a08\u7b97\u3059\u308b\u95a2\u6570\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\n# series\u306e\u30e6\u30cb\u30fc\u30af\u306a\u5024\u306e\u500b\u6570\u3092\u30ab\u30a6\u30f3\u30c8\u3059\u308b\u95a2\u6570\ndef count_unique(series):\n    return len(np.unique(series))","9f857a58":"# path\u3092\u8a2d\u5b9a\ndata_dir = '..\/input\/optiver-realized-volatility-prediction\/'","c99d8e18":"# \u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080\u305f\u3081\u306e\u95a2\u6570\ndef read_train_test():\n    train = pd.read_csv(data_dir + 'train.csv')\n    test = pd.read_csv(data_dir + 'test.csv')\n    # \u30aa\u30fc\u30c0\u30fc\u30d6\u30c3\u30af\u3068\u30c8\u30ec\u30fc\u30c9\u30c7\u30fc\u30bf\u3068\u7d50\u5408\u3059\u308b\u305f\u3081\u306bid\u3068\u306a\u308b\u30ab\u30e9\u30e0\u3092\u4f5c\u6210\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    # \u95a2\u6570\u304c\u5b9f\u884c\u3055\u308c\u305f\u3089train\u30c7\u30fc\u30bf\u306e\u884c\u6570\u3092print\u3059\u308b\u3088\u3046\u306b\u8a2d\u5b9a\n    print(f'Our training set has {train.shape[0]} rows')\n    return train, test\n\n# \u30aa\u30fc\u30c0\u30fc\u30d6\u30c3\u30af\u30c7\u30fc\u30bf\u306e\u524d\u51e6\u7406\u3092\u884c\u3046\u95a2\u6570\n# \u4eca\u56de\u884c\u3046\u524d\u51e6\u308a\ndef book_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    # WAP1,2\u3092\u8a08\u7b97\u3057\u305f\u7d50\u679c\u30ab\u30e9\u30e0\u3068\u3057\u3066\u8ffd\u52a0\n    df['wap1'] = calc_wap1(df)\n    df['wap2'] = calc_wap2(df)\n    # time_id\u3054\u3068\u306elog return1,2\u3092\u8a08\u7b97\u3057\u305f\u7d50\u679c\u30ab\u30e9\u30e0\u3068\u3057\u3066\u8ffd\u52a0\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    # WAP1\u3068WAP2\u306e\u7d76\u5bfe\u5024\u306e\u5dee\u3092\u53d6\u5f97\u3057\u3066\u30ab\u30e9\u30e0\u3068\u3057\u3066\u8ffd\u52a0\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    # spread\u3092\u8a08\u7b97\n    # price_spread\u306f\u4e0b\u8a18\u306e\u3088\u3046\u306b\u8a08\u7b97\n    # df['ask_price1'] - df['bid_price1']\u306f\u6700\u9ad8\u58f2\u5024 - \u6700\u4f4e\u8cb7\u5024\u306e\u5dee\n    # ((df['ask_price1'] + df['bid_price1']) \/ 2)\u306f\u6700\u9ad8\u58f2\u5024\u3068\u6700\u4f4e\u8cb7\u5024\u306e\u5e73\u5747\u5024\n    # \u53cc\u65b9\u306e\u5024\u6bb5\u304c\u96e2\u308c\u308b\u307b\u3069\u5927\u304d\u304f\u306a\u308b\u96e2\u308c\u5ea6\u5408\u3044\u7684\u306a\u610f\u5473\u5408\u3044\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1']) \/ 2)\n    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) \/ ((df['ask_price2'] + df['bid_price2']) \/ 2)\n    # \u6700\u5927\u3068\u4e8c\u756a\u76ee\u306e\u305d\u308c\u305e\u308c\u306e\u5dee\n    # \u5024\u6bb5\u304c\u8a70\u307e\u3063\u3066\u3044\u308b\u307b\u3069\u4eba\u6c17\uff1f \u2192 \u3053\u308c\u304c\u5c0f\u3055\u3044\u307b\u3069\u826f\u3044\u5e02\u5834\u3068\u8a00\u3048\u308b\u306e\u3067\u306f\u306a\u3044\u304b\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    # spread\u306e\u5206\u5b50\u3002spread\u3068\u540c\u3058\u304f\u3053\u308c\u304c\u5927\u304d\u3044\u3068\u826f\u3044\u5e02\u5834\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    # ask\u3068bid\u306e\u4e00\u756a\u3068\u4e8c\u756a\u76ee\u306e\u4e88\u7d04\u306e\u5408\u8a08 \u2192 \u3053\u308c\u304c\u5927\u304d\u3044\u3068\u3044\u3046\u3053\u3068\u306f\u58f2\u8cb7\u304c\u76db\u3093\u3068\u3044\u3046\u3053\u3068\uff08\u8cb7\u3044\u305f\u3044\u4eba\u3068\u58f2\u308a\u305f\u3044\u4eba\u304c\u591a\u3044\uff09\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    # \u8cb7\u3044\u624b\u3068\u58f2\u308a\u624b\u3069\u3063\u3061\u306e\u5dee\u306e\u7d76\u5bfe\u5024 \u2192 \u3053\u308c\u304c\u5927\u304d\u3044\u3068\u3069\u3061\u3089\u304b\u306b\u9700\u8981\u304c\u504f\u308a\u304c\u3042\u308b\u304b\u3089\u826f\u304f\u306a\u3044\uff1f\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    # \u9700\u8981\u3068\u4f9b\u7d66\u306e\u5272\u5408\u3092\u8ffd\u52a0\n    df['bid_ask_ratio'] = (df['ask_size1']+df['ask_size2'])\/(df['bid_size1']+df['bid_size2'])\n    \n    # \u8f9e\u66f8\u578b\u3067\u30ab\u30e9\u30e0\u540d\u3068\u64cd\u4f5c\u3092\u7528\u610f\n    create_feature_dict = {\n        'wap1': [np.sum, np.mean, np.std],\n        'wap2': [np.sum, np.mean, np.std],\n        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n        'wap_balance': [np.sum, np.mean, np.std],\n        'price_spread':[np.sum, np.mean, np.std],\n        'price_spread2':[np.sum, np.mean, np.std],\n        'bid_spread':[np.sum, np.mean, np.std],\n        'ask_spread':[np.sum, np.mean, np.std],\n        'total_volume':[np.sum, np.mean, np.std],\n        'volume_imbalance':[np.sum, np.mean, np.std],\n        'bid_ask_ratio':[np.sum, np.mean, np.std],\n        \"bid_ask_spread\":[np.sum, np.mean, np.std],\n    }\n    \n    # \u30c7\u30fc\u30bf\u6570\u304c\u591a\u3044\u306e\u3067time_id\u3054\u3068\u306b\u96c6\u8a08\u3059\u308b\u95a2\u6570\u3092\u4f5c\u6210\n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # \u95a2\u6570\u3067\u5f15\u6570\u306b\u8a2d\u5b9a\u3057\u305fsconds_in_bucket\u3088\u308a\u5f8c\u306e\u30c7\u30fc\u30bf\u3060\u3051\u3092time_id\u3067group by\u3057\u305f\u72b6\u614b\u3067\u53d6\u5f97\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        # \u30b1\u30c4\u306b\u30ab\u30e9\u30e0\u540d\u3092\u8ffd\u52a0\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # \u30b1\u30c4\u306b\u305d\u308c\u305e\u308c\u306e\u300c_seconds_in_bucket\u540d\u300d\u3068\u30ab\u30e9\u30e0\u306b\u8ffd\u52a0\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # \u4e0a\u306e\u95a2\u6570\u3092\u7528\u3044\u3066time_id\u3054\u3068\u306b\u96c6\u8a08\u3057\u305f\u30c6\u30fc\u30d6\u30eb\u3092\u3055\u3089\u306bseconds_in_bucket\u3067\u533a\u5207\u3063\u305fDataframe\u3092\u4f5c\u6210\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    # add_suffix = True\u3068\u3059\u308b\u3068\u30ab\u30e9\u30e0\u540d\u306e\u6700\u5f8c\u306b\u300c_450\u300d\u3063\u3066\u4ed8\u304f\n    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n    # \u53c2\u8003\u306b\u3057\u305fnotebook\u306b\u306f100\u79d2\u3054\u3068\u306b\u540c\u3058\u4f5c\u696d\u3092\u3057\u305f\u8de1\u304c\u6b8b\u3063\u3066\u3044\u305f\u3002150\u79d2\u3054\u3068\u306e\u65b9\u304c\u3044\u3044\u3068\u5224\u65ad\u3057\u305f\u306e\u304b\n    # df_feature_500 = get_stats_window(seconds_in_bucket = 500, add_suffix = True)\n    # df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n    # df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n\n    # \u4e0a\u3067\u4f5c\u6210\u3057\u305fDataFrame\u3092\u4e00\u3064\u306b\u7d50\u5408\u3059\u308b\n    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n    # \u7d50\u5408\u3067\u4f7f\u7528\u3057\u305fid\u7528\u306e\u30ab\u30e9\u30e0\u3092\u524a\u9664\n    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\n    \n    # stock_id\u3054\u3068\u306b\u5225\u306e\u30d5\u30a9\u30eb\u30c0\u306b\u30d5\u30a1\u30a4\u30eb\u304c\u4fdd\u5b58\u3055\u308c\u3066\u3044\u308b\u305f\u3081\u3001file_path\u304b\u3089stock_id\u3092\u8aad\u307f\u53d6\u3063\u305f\u4e0a\u3067\u7d50\u5408\u3059\u308b\n    # file_path\u304c\u300c.\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/stock_id=0\/...\u300d\u3060\u3063\u305f\u3089stock_id\u30920\u3068\u8a8d\u8b58\u3057\u305f\u4e0a\u3067\u7d50\u5408\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    # \u3053\u306e\u6642\u70b9\u3067\u300ctime_id_0\u300d\u306a\u3069\u306evalue\u304c\u5165\u3063\u3066\u3044\u308b \n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    return df_feature","955cec78":"# \u30c8\u30ec\u30fc\u30c9\u30c7\u30fc\u30bf\u3092\u307e\u3068\u3081\u3066\u524d\u51e6\u7406\ndef trade_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    # \u5b9f\u969b\u306b\u53d6\u5f15\u304c\u884c\u308f\u308c\u305f\u6642\u306e\u5024\u6bb5\u3067log_return\u3092\u8a08\u7b97\u3057\u305f\u7d50\u679c\u3092\u300clog_return\u300d\u30ab\u30e9\u30e0\u306b\u4ee3\u5165\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    # \u8f9e\u66f8\u578b\u3067\u30ab\u30e9\u30e0\u540d\u3068\u64cd\u4f5c\u3092\u7528\u610f\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum, realized_volatility, np.mean, np.std, np.max, np.min],\n        'order_count':[np.mean,np.sum,np.max],\n    }\n    \n    # \u30c7\u30fc\u30bf\u6570\u304c\u591a\u3044\u306e\u3067time_id\u3054\u3068\u306b\u96c6\u8a08\u3059\u308b\u95a2\u6570\u3092\u4f5c\u6210\n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # \u95a2\u6570\u3067\u5f15\u6570\u306b\u8a2d\u5b9a\u3057\u305fsconds_in_bucket\u3088\u308a\u5f8c\u306e\u30c7\u30fc\u30bf\u3060\u3051\u3092time_id\u3067group by\u3057\u305f\u72b6\u614b\u3067\u53d6\u5f97\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        # \u30b1\u30c4\u306b\u30ab\u30e9\u30e0\u540d\u3092\u8ffd\u52a0\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # \u30b1\u30c4\u306b\u305d\u308c\u305e\u308c\u306e\u300c_seconds_in_bucket\u540d\u300d\u3068\u30ab\u30e9\u30e0\u306b\u8ffd\u52a0\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n\n    # \u4e0a\u306e\u95a2\u6570\u3092\u7528\u3044\u3066time_id\u3054\u3068\u306b\u96c6\u8a08\u3057\u305f\u30c6\u30fc\u30d6\u30eb\u3092\u3055\u3089\u306bseconds_in_bucket\u3067\u533a\u5207\u3063\u305fDataframe\u3092\u4f5c\u6210\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n    # \u30aa\u30fc\u30c0\u30fc\u30d6\u30c3\u30af\u540c\u69d8100\u79d2\u3054\u3068\u3088\u308a150\u79d2\u3054\u3068\u306e\u65b9\u304c\u7d50\u679c\u304c\u826f\u304b\u3063\u305f\u3089\u3057\u3044\n    # df_feature_500 = get_stats_window(seconds_in_bucket = 500, add_suffix = True)\n    # df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n    # df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n    \n    # score\u306b\u5f71\u97ff\u3057\u3066\u3044\u308b\u3089\u3057\u3044\n    \n    # time_id\u3054\u3068\u306bprice\u304c\u3069\u308c\u304f\u3089\u3044\u5909\u52d5\u3057\u3066size\u304c\u3069\u308c\u304f\u3089\u3044\u884c\u308f\u308c\u305f\u304b\n    def tendency(price, vol):    \n        df_diff = np.diff(price)\n        val = (df_diff\/price[1:])*100\n        power = np.sum(val*vol[1:])\n        return(power)\n    \n    # time_id\u3054\u3068\u306b\u6700\u5927\u5024\u3084\u6700\u5c0f\u5024\u3092\u8a08\u7b97\u3057\u305f\u914d\u5217\u3092\u7528\u610f\n    # \u3053\u306e\u3084\u308a\u65b9\u3060\u3068\u30ea\u30fc\u30af\u3057\u3066\u3044\u308b\n    lis = []\n    \n    for n_time_id in df['time_id'].unique():\n        df_id = df[df['time_id'] == n_time_id]\n        \n        # \u4e0a\u3067\u4f5c\u3063\u305fpower\u3092\u8a08\u7b97\u3002time_id\u3054\u3068\u306bprice\u304c\u3069\u308c\u304f\u3089\u3044\u5909\u52d5\u3057\u305f\u304b\u3001\u53d6\u5f15\u304c\u3069\u308c\u304f\u3089\u3044\u884c\u308f\u308c\u305f\u304b\n        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n        \n        # \u5e73\u5747\u4ee5\u4e0a\u3068\u4ee5\u4e0b\u3067price\u306e\u5408\u8a08\u5024\u3092\u53d6\u5f97\n        # \u610f\u56f3\u306f\u5206\u304b\u3089\u306a\u3044\u304c\u3068\u308a\u3042\u3048\u305a\u6b8b\u3059\n        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n        \n        # \u6b63\u304b\u8ca0\u304b\u3067\u5206\u3051\u305f\u524d\u5f8cprice\u306e\u5dee\u5206\u306e\u5408\u8a08\u5024\n        # \u3053\u3061\u3089\u3082\u610f\u56f3\u306f\u5206\u304b\u3089\u305a\n        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n        \n        ## \u3053\u3053\u304b\u3089\u306fprice\u306e\u8a71\n        # \u504f\u5dee\u306e\u4e2d\u592e\u5024\n        abs_diff = np.median(np.abs(df_id['price'].values - np.mean(df_id['price'].values)))  \n        # \u4fa1\u683c\u306e\u4e8c\u4e57\u306e\u5e73\u5747\u5024\n        energy = np.mean(df_id['price'].values**2)\n        # \u7b2c3\u56db\u5206\u4f4d-\u7b2c\uff11\u56db\u5206\u4f4d\n        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n        \n        ## \u3053\u3053\u304b\u3089\u306fsize\u306e\u8a71\n        # \u305d\u308c\u305e\u308c\u504f\u5dee\u306e\u4e2d\u592e\u5024\u3001\u4fa1\u683c\u306e\u4e8c\u4e57\u306e\u5e73\u5747\u5024\u3001\u7b2c3\u56db\u5206\u4f4d-\u7b2c\uff11\u56db\u5206\u4f4d\u3092\u8a08\u7b97\n        abs_diff_v = np.median(np.abs(df_id['size'].values - np.mean(df_id['size'].values)))        \n        energy_v = np.sum(df_id['size'].values**2)\n        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n        \n        # \u307e\u3068\u3081\u3066lis\u306bappend\n        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n\n    # lis\u3092DataFrame\u306b\u5909\u63db  \n    df_lr = pd.DataFrame(lis)\n   \n    # dif_lr\u3092\u7d50\u5408\n    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n    \n    # sconds_in_bucket\u3067\u5206\u3051\u305fDataFrame\u3082\u5168\u3066\u7d50\u5408\n    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n    # df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    # df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    \n    # \u7d50\u5408\u7528\u306b\u4f7f\u3063\u305f\u30ab\u30e9\u30e0\u3092\u524a\u9664\n    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150','time_id'], axis = 1, inplace = True)\n    \n    # \u30aa\u30fc\u30c0\u30fc\u30d6\u30c3\u30af\u3068\u898b\u5206\u3051\u3092\u3064\u3051\u308b\u305f\u3081\u306b\u30ab\u30e9\u30e0\u306e\u5148\u982d\u306b\u300ctrade_\u300d\u3092\u3064\u3051\u308b\n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    return df_feature","a2dc2ca5":"# rmspe\u3092\u8a08\u7b97\u3059\u308b\u305f\u3081\u306e\u95a2\u6570\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\n# rmspe\u306e\u5024\u304c\u3076\u308c\u3066\u6765\u305f\u3089\u505c\u6b62\u3059\u308b\u305f\u3081\u306e\u95a2\u6570\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False","50fe1c24":"# stock_id\u3068time_id\u3054\u3068\u306b\u30c7\u30fc\u30bf\u3092\u307e\u3068\u3081\u308b\u305f\u3081\u306e\u95a2\u6570\ndef get_time_stock(df):\n    # \u30dc\u30e9\u30c6\u30a3\u30ea\u30c6\u30a3\u5468\u308a\u306b\u30ab\u30e9\u30e0\u3092\u914d\u5217\u306b\u7528\u610f\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_450', 'log_return2_realized_volatility_450', \n                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_150', 'log_return2_realized_volatility_150', \n                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_450', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_150']\n\n    # stock_id\u3054\u3068\u306b\u307e\u3068\u3081\u308b\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # stock_id\u3054\u3068\u306b\u307e\u3068\u3081\u305f\u6642\u306b\u8a08\u7b97\u3057\u305fmean\u306a\u3069\u3068\u308f\u304b\u308b\u3088\u3046\u306b\u30b1\u30c4\u306b\u300c_stock\u300d\u3068\u3064\u3051\u308b\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # time_id\u3054\u3068\u306b\u307e\u3068\u3081\u308b\uff08\u4f5c\u696d\u306fstock_id\u306e\u6642\u3068\u4e00\u7dd2\uff09\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    # \u5143\u306eDataFrame\u306b\u4eca\u4f5c\u3063\u305f2\u3064\u3092\u7d50\u5408\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    return df\n    \n# \u305d\u308c\u305e\u308c\u306e\u524d\u51e6\u7406\u3092\u4e26\u5217\u3057\u3066\u884c\u3048\u308b\u3088\u3046\u306b\u3059\u308b\u305f\u3081\u306e\u95a2\u6570\ndef preprocessor(list_stock_ids, is_train = True):\n    \n    # Parrallel for loop\n    def for_joblib(stock_id):\n        # is_train=True\u3068\u6307\u5b9a\u3055\u308c\u305f\u5834\u5408train\u30c7\u30fc\u30bf\u306e\u65b9\u3078\u306epath\u3092\u4f5c\u6210\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet\/stock_id=\" + str(stock_id)\n        # is_train=False\u3064\u307e\u308atest\u30c7\u30fc\u30bf\u306a\u3089\u3053\u3061\u3089\u3078\u306epath\u3092\u4f5c\u6210\n        else:\n            file_path_book = data_dir + \"book_test.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet\/stock_id=\" + str(stock_id)\n    \n        # book\u3068trade\u3092\u524d\u51e6\u7406\u3057\u305f\u4e0a\u3067\u7d50\u5408\u3059\u308b\n        # \u4e9b\u7d30\u306a\u5909\u5316\u3082\u5168\u3066\u30ec\u30b3\u30fc\u30c9\u3068\u3057\u3066\u8a18\u9332\u3055\u308c\u3066\u3044\u308bbook\u306e\u65b9\u306brow_id\u3092\u57fa\u6e96\u306btrade\u3092\u5916\u90e8\u7d50\u5408\u3059\u308b\n        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n        \n        # \u7d50\u5408\u3057\u305fDataFrame\u3092\u8fd4\u3059\n        return df_tmp\n    \n    # \u4e26\u5217API\u3092\u4f7f\u7528\u3057\u3066for_joblib\u95a2\u6570\u3092\u547c\u3073\u51fa\u3059\n    # n_jobs:\u6700\u5927\u540c\u6642\u5b9f\u884c\u30b8\u30e7\u30d6\u6570\u3002-1\u3068\u3059\u308b\u3068\u5168\u3066\u306eCPU\u304c\u4f7f\u7528\u3055\u308c\u308b\n    # verbose:\u30ed\u30b0\u306e\u51fa\u529b\u30ec\u30d9\u30eb\uff08\u5197\u9577\u6027\uff09\u3002\u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u306f\u4f55\u3082\u51fa\u529b\u3055\u308c\u306a\u3044\u3002\u5024\u3092\u5927\u304d\u304f\u3059\u308b\u3068\u51fa\u529b\u30ec\u30d9\u30eb\u304c\u4e0a\u304c\u308b\uff08\u5197\u9577\u6027\u304c\u5897\u3059\uff09\u300210\u3088\u308a\u5927\u304d\u3044\u3068\u3059\u3079\u3066\u306e\u30ed\u30b0\u304c\u51fa\u529b\u3055\u308c\u300150\u4ee5\u4e0a\u3060\u3068stdout\uff08\u6a19\u6e96\u51fa\u529b\uff09\u306b\u51fa\u529b\u3055\u308c\u308b\u3002\n    # delayed(<\u5b9f\u884c\u3059\u308b\u95a2\u6570>)(<\u95a2\u6570\u3078\u306e\u5f15\u6570>) for \u5909\u6570\u540d in \u30a4\u30c6\u30e9\u30d6\u30eb\n    # \u5b9f\u884c\u3059\u308b\u95a2\u6570\uff08book\u3068trade\u3092\u524d\u51e6\u7406\u3057\u3066\u7d50\u5408\uff09\u3092stock_id\u3054\u3068\u306b\u884c\u3046\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    # Parallel\u304b\u3089\u8fd4\u3055\u308c\u308b\u3059\u3079\u3066\u306eDataFrame\u3092\u7d50\u5408\n    # ignore_index=True\u3067index\u304cconcat\u524d\u306eindex\u3092\u7121\u8996\u3057\u3066\u9023\u756a\u3067\u632f\u3089\u308c\u308b\n    df = pd.concat(df, ignore_index = True)\n    return df\n\n# RMSPE(\u5e73\u5747\u5e73\u65b9\u4e8c\u4e57\u8aa4\u5dee\u7387)\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\n# RMSPE\u3067\u65e9\u671f\u505c\u6b62\u3059\u308b\u95a2\u6570\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False","f3734844":"# train\u30c7\u30fc\u30bf\u3068test\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\ntrain, test = read_train_test()\n\n# \u30e6\u30cb\u30fc\u30af\u306astock\u306eid\u3092\u53d6\u5f97\u3059\u308b\ntrain_stock_ids = train['stock_id'].unique()\n# \u4e26\u5217\u51e6\u7406\u306b\u4f7f\u3046\u305f\u3081\u306b\u524d\u51e6\u7406\u3092\u884c\u3046\n# \u4eca\u56de\u306f\u4e26\u5217\u5316\u3057\u3066book\u3068trade\u3092stock_id\u3054\u3068\u306b\u51e6\u7406\u3092\u3057\u305f\u4e0a\u3067\u7d50\u5408\u3059\u308b\ntrain_ = preprocessor(train_stock_ids, is_train = True)\n# row_id\u3092\u57fa\u6e96\u306bleft join\u3059\u308b\ntrain = train.merge(train_, on = ['row_id'], how = 'left')\n\n# \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u3082\u540c\u3058\u51e6\u7406\u3092\u884c\u3046\ntest_stock_ids = test['stock_id'].unique()\ntest_ = preprocessor(test_stock_ids, is_train = False)\ntest = test.merge(test_, on = ['row_id'], how = 'left')\n\n# time_id\u3068stock_id\u304c\u4e00\u7dd2\u306b\u306a\u3063\u305f\u30c7\u30fc\u30bf\u3092\u53d6\u5f97\n# get_time_stock\u306fstock_id\u3068time_id\u3054\u3068\u306e\u96c6\u8a08\u3092\u884c\u3046\u95a2\u6570\ntrain = get_time_stock(train)\ntest = get_time_stock(test)","0f25f26b":"display(train.head())\ndisplay(train.shape)\ndisplay(test.head())\ndisplay(test.shape)","c820cdb9":"# params\u3092\u8a2d\u5b9a\nseed = 42\nparams = {\n    'learning_rate': 0.1,        \n    'lambda_l1': 2,\n    'lambda_l2': 7,\n    'num_leaves': 800,\n    'min_sum_hessian_in_leaf': 20,\n    'feature_fraction': 0.8,\n    'feature_fraction_bynode': 0.8,\n    'bagging_fraction': 0.9,\n    'bagging_freq': 42,\n    'min_data_in_leaf': 700,\n    'max_depth': 4,\n    'seed': seed,\n    'feature_fraction_seed': seed,\n    'bagging_seed': seed,\n    'drop_seed': seed,\n    'data_random_seed': seed,\n    'objective': 'rmse',\n    'boosting': 'gbdt',\n    'verbosity': -1,\n    'n_jobs': -1,\n} ","79ddcfe2":"# \u91cd\u8981\u5ea6\u89e3\u6790\n# \u7279\u5fb4\u91cf\u306e\u91cd\u8981\u5ea6\uff08feature_importance\uff09\u306e\u8a08\u7b97\u65b9\u6cd5\u306f\u4ee5\u4e0b\u306e2\u3064\n# - \u983b\u5ea6: \u30e2\u30c7\u30eb\u3067\u305d\u306e\u7279\u5fb4\u91cf\u304c\u4f7f\u7528\u3055\u308c\u305f\u56de\u6570\uff08\u521d\u671f\u5024\uff09\u300cimportance_type = 'split'\u300d\n# - \u30b2\u30a4\u30f3: \u305d\u306e\u7279\u5fb4\u91cf\u304c\u4f7f\u7528\u3059\u308b\u5206\u5c90\u304b\u3089\u306e\u76ee\u7684\u95a2\u6570\u306e\u6e1b\u5c11\uff08\u3053\u3061\u3089\u304c\u304a\u3059\u3059\u3081\u3089\u3057\u3044\uff09\u300cimportance_type = 'gain'\u300d\n# \u4eca\u56de\u306fgain\u3092\u9078\u629e\u3002\u76ee\u7684\u95a2\u6570\u306e\u6e1b\u5c11\u306b\u95a2\u308f\u3063\u3066\u3044\u308b\u7279\u5fb4\u91cf \u2192 \u3064\u307e\u308a\u91cd\u8981\u306a\u7279\u5fb4\u91cf\u304c\u964d\u9806\u3067\u308f\u304b\u308b\ndef calc_model_importance(model, feature_names=None, importance_type='gain'):\n    importance_df = pd.DataFrame(model.feature_importance(importance_type=importance_type),\n                                 index=feature_names,\n                                 columns=['importance']).sort_values('importance')\n    return importance_df\n\ndef calc_mean_importance(importance_df_list):\n    mean_importance = np.mean(\n        np.array([df['importance'].values for df in importance_df_list]), axis=0)\n    mean_df = importance_df_list[0].copy()\n    mean_df['importance'] = mean_importance\n    \n    return mean_df","a66e235d":"display(train.shape)\ndisplay(test.shape)\nprint('='*40 + 'stock_id\u524a\u9664\u5f8c' + '='*40)\n# \u300c_stock\u300d\u3068\u306f\u5165\u3063\u3066\u3044\u308b\u30ab\u30e9\u30e0\u306f\u5168\u3066\u524a\u9664\ntrain_drop_stock = train.drop(train.filter(like='_stock', axis=1).columns.to_list(), axis=1)\ndisplay(train_drop_stock.shape)","31407f53":"# \u7279\u5fb4\u91cf\u3068\u30bf\u30fc\u30b2\u30c3\u30c8\u3092\u5206\u5272\nx = train_drop_stock.drop(['row_id', 'target', 'time_id'], axis = 1)\ny = train_drop_stock['target']\nx_test = test.drop(['row_id', 'time_id'], axis = 1)\n\n# \u7d50\u5408\u7528\u3067object\u578b\u3067\u5165\u3063\u3066\u3044\u305fstock_id\u3092\u6570\u5024\u578b\u306b\u5909\u63db\nx['stock_id'] = x['stock_id'].astype(int)\nx_test['stock_id'] = x_test['stock_id'].astype(int)\n\n# x\u306e\u884c\u6570\u52060\u304c\u5165\u3063\u305f\u914d\u5217\u3092\u7528\u610f\noof_predictions = np.zeros(x.shape[0])\ndisplay(oof_predictions)\ndisplay(oof_predictions.shape)\n# \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u3082\u884c\u3046\ntest_predictions = np.zeros(x_test.shape[0])\ndisplay(test_predictions)\ndisplay(test_predictions.shape)","6de08f20":"# stock_id\u3054\u3068\u306b\u307e\u3068\u3081\u308b\u95a2\u6570\u3092\u3082\u3046\u4e00\u5ea6\u4f5c\u6210\ndef get_time_stock_2(df):\n    # Get realized volatility columns\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_450', 'log_return2_realized_volatility_450', \n                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_150', 'log_return2_realized_volatility_150', \n                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_450', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_150']\n\n    # stock_id\u3054\u3068\u306b\u96c6\u8a08\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n    \n    # \u5143\u3005\u306e\u30c7\u30fc\u30bf\u306b\u4eca\u4f5c\u6210\u3057\u305fDataFrame\u3092\u4f5c\u6210\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df.drop(['stock_id__stock'], axis = 1, inplace = True)\n    return df","25bc9a61":"# \u5b9f\u884c\u524d\u306e\u78ba\u8a8d\nx.shape","b0ab6333":"# \u91cd\u8981\u5ea6\u89e3\u6790\u3092gain\u3067\u53d6\u5f97\u3057\u305fver\u3068split\u3067\u53d6\u5f97\u3057\u305fver\u306b\u5206\u3051\u308b\u305f\u3081\u306b\u305d\u308c\u305e\u308c\u914d\u5217\u3092\u7528\u610f\ngain_importance_list = []\nsplit_importance_list = []\n\n# sklearn\u306eGroupKFold\u3092import\nfrom sklearn.model_selection import GroupKFold\n# \u3053\u3053\u306ftime_id\uff1f\ngroup = train['stock_id']\n# train\u30c7\u30fc\u30bf\u30925\u3064\u306efold\u306b\u5206\u5272\nkf = GroupKFold(n_splits=5)\n# \u5404fold\u3067\u7e70\u308a\u8fd4\u3059\nfor fold, (trn_ind, val_ind) in enumerate(kf.split(x, groups=group)):\n    print(f'Training fold {fold + 1}')\n    x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n    y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n    \n    # \u5404fold\u306e\u6642\u306e\u30c7\u30fc\u30bf\u3067stock_id\u3054\u3068\u306b\u96c6\u8a08\u3059\u308b\u95a2\u6570\u3092\u9069\u7528\n    x_train = get_time_stock_2(x_train)\n    x_val = get_time_stock_2(x_val)\n    \n    # RMSPE\n    train_weights = 1 \/ np.square(y_train)\n    val_weights = 1 \/ np.square(y_val)\n    train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights)\n    val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights)\n    model = lgb.train(params = params, \n                      train_set = train_dataset, \n                      categorical_feature = ['stock_id'],\n                      valid_sets = [train_dataset, val_dataset], \n                      num_boost_round = 5000, \n                      early_stopping_rounds = 30, \n                      verbose_eval = 100,\n                      feval = feval_rmspe)\n\n    # \u5404fold\u3054\u3068\u306blightgbm\u3092\u5b9f\u88c5\u3057\u3066\u4e88\u6e2c\u5024\u3092\u53d6\u5f97\n    # \u306e\u3061\u306ermspe\u306e\u8a08\u7b97\u3067\u5fc5\u8981\n    oof_predictions[val_ind] = model.predict(x_val)\n    \n    # \u5206\u3051\u305f5\u3064\u306efold\u3054\u3068\u306e\u4e88\u6e2c\u5024\u306e\u5e73\u5747\u5024\u3092\u53d6\u5f97\n    test_predictions += model.predict(x_test) \/ 5\n\n    feature_names = x_train.columns.values.tolist()\n    gain_importance_df = calc_model_importance(\n        model, feature_names=feature_names, importance_type='gain')\n    gain_importance_list.append(gain_importance_df)\n\n    split_importance_df = calc_model_importance(\n        model, feature_names=feature_names, importance_type='split')\n    split_importance_list.append(split_importance_df)\n    \n    x_train.drop(train.filter(like='_stock', axis=1).columns.to_list(), axis=1, inplace=True)\n    x_val.drop(train.filter(like='_stock', axis=1).columns.to_list(), axis=1, inplace=True)\n\nrmspe_score = rmspe(y, oof_predictions)\nprint(f'Our out of folds RMSPE is {rmspe_score}')","bb034ecb":"# \u91cd\u8981\u5ea6\u89e3\u6790\u3092\u51fa\u529b\nmean_gain_df = calc_mean_importance(gain_importance_list)\nmean_gain_df = mean_gain_df.reset_index().rename(columns={'index': 'feature_names'})\n# csv\u306b\u3057\u3066\u5168\u90e8\u898b\u305f\u3044\u5834\u5408\u306f\u4e0b\u8a18\n# mean_gain_df.to_csv('gain_importance_mean groupkfold stock_id after Kflod.csv', index=False)\nmean_gain_df.head(20).sort_values(by='importance', ascending=False)","7b822370":"test['target'] = test_predictions\ntest[['row_id', 'target']].to_csv('submission.csv',index = False)","8fa7b89a":"## stock_id\u306e\u7d71\u8a08\u91cf\u3092Kfold\u306e\u5f8c\u306b\u53d6\u5f97\u3059\u308b","7eeff02b":"## GroupKfold\u3092\u884c\u306a\u3063\u305f\u5f8c\u306bstock_id\u7cfb\u7d71\u306e\u30ab\u30e9\u30e0\u3092\u8ffd\u52a0\u3057\u305f\u5834\u5408\u306e\u7cbe\u5ea6\u3092\u8abf\u67fb"}}