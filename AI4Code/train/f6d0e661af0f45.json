{"cell_type":{"0487b454":"code","1b77c82e":"code","5cced1a9":"code","1df88d1b":"code","ed5fab47":"code","bf0ce456":"code","21b0100e":"code","a6bc8359":"code","45eb7156":"code","976a41ba":"code","eadf3e47":"code","b4d85299":"code","f3cb4843":"code","f1a23666":"code","95417463":"code","659488cd":"code","a08e61fa":"code","52f3ea6b":"code","1c49ce95":"code","ab219e05":"code","b7d962bd":"code","16531baf":"code","1f6cbdb3":"code","f7ad710f":"code","f66ce638":"code","6ff537fe":"code","3de08c6c":"code","2e3f8ef3":"code","46cf4013":"code","4b9baa48":"code","1fceabef":"code","e0803812":"code","61b71260":"code","cdc0b263":"code","7553fd9e":"code","2152107a":"code","ffbe4e5d":"code","eb87de7a":"code","03d0e95e":"code","d71abed0":"code","9363012e":"code","78c7f17d":"code","0d5b1ffb":"code","261c434e":"code","303035ef":"code","4dc0d0e6":"code","996a7a55":"code","2a3fcdc8":"code","3d89cefd":"code","7c004fcd":"code","b125c642":"code","bea8a7d2":"code","9232ed55":"code","65162bc2":"code","57f6752b":"code","19f60db6":"code","e04a96cb":"code","bfc14cac":"code","a9b0bd67":"code","5cde360b":"code","90690b3a":"code","98a67d9f":"code","5cca96e8":"code","8d33ed59":"code","eadcddec":"code","242b763b":"code","51e326c7":"code","db698755":"code","025bc180":"code","1fc68fee":"code","f1e72b10":"markdown","1384a099":"markdown","17310234":"markdown","d13db908":"markdown","f52fe27b":"markdown","ce738648":"markdown","f666efbf":"markdown","76a6ef51":"markdown","2daa9c85":"markdown","41c1e6fe":"markdown","24091c13":"markdown","3bc67d08":"markdown","164407da":"markdown","d20c56ec":"markdown","bd987bf9":"markdown","51050cc0":"markdown","5557f5dc":"markdown","0168894d":"markdown","3f6d3d14":"markdown","6bd32f6f":"markdown","04ea2d9a":"markdown","3ae5781f":"markdown","e6b96af0":"markdown","2cf2ce69":"markdown","6988cd23":"markdown","19a2d0b9":"markdown","39d00e54":"markdown","69d37db5":"markdown","89d69cda":"markdown","cfe8abd4":"markdown","b124f79e":"markdown","84f93e19":"markdown","04012da3":"markdown","a6afbe2a":"markdown"},"source":{"0487b454":"#usual imports \nimport numpy as np\nimport pandas as pd\nimport os\nimport sys\nimport statistics\nassert sys.version_info >= (3,5)\n#visualization imports\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style='whitegrid')\n%matplotlib inline\n#consistent sized plots\nfrom pylab import rcParams\nrcParams['figure.figsize']= 12,5\nrcParams['axes.labelsize']=12\nrcParams['xtick.labelsize']=12\nrcParams['ytick.labelsize']=12\n#handle the unwanted warnings\nimport warnings\nwarnings.filterwarnings(action='ignore',category=DeprecationWarning)\nwarnings.filterwarnings(action='ignore',category=FutureWarning)\n#view all the columns in the dataframe\npd.options.display.max_columns = None\n#import zip file\nfrom zipfile import ZipFile","1b77c82e":"#check the version of the libraries used\nprint('Pandas version: {}'.format(pd.__version__))\nprint('Numpy version: {}'.format(np.__version__))\nprint('Seaborn version: {}'.format(sns.__version__))","5cced1a9":"#load the csv file into the dataframe\nloan = pd.read_csv('\/kaggle\/input\/loan-risk\/loan_data.csv',delimiter=',',engine='python')\n#check the top rows of the data frame\nloan.head()","1df88d1b":"#automated basic exploration of the data using pandas profiling\nimport pandas_profiling as pp\npp.ProfileReport(loan)","ed5fab47":"#check info \nloan.info()","bf0ce456":"#check the balance of the data\nsns.countplot(loan['credit.policy'])\nplt.title('Countplot of the credit policy (Target Variable)')\nplt.show()","21b0100e":"#different purpose of the loan\nloan['purpose'].value_counts().sort_values(ascending=False)","a6bc8359":"#credit policy w.r.t the purpose for which the loan was taken\nsns.countplot(loan['credit.policy'],hue=loan['purpose'])\nplt.title('Plot of credit policy with respect to the loan purpose')\nplt.show()","45eb7156":"#visualize interest rate vs the credit policy\nsns.violinplot(x='credit.policy', y='int.rate', data=loan,jitter=True,palette='Set2')\nplt.show()","976a41ba":"#visualize interest rate vs the purpose of the loan\nplt.figure(figsize=(12,7))\nsns.violinplot(x='purpose', y='int.rate', data=loan,jitter=True,palette='Set1')\nplt.title('Plot of the interest rate set against the purpose of the loan')\nplt.show()","eadf3e47":"#visualize interest rate vs the purpose of the loan\nplt.figure(figsize=(12,7))\nsns.violinplot(x='purpose', y='int.rate', data=loan,hue = 'credit.policy',jitter=True,palette='Set3')\nplt.title('Plot of the interest rate set against the purpose of the loan separated by credit policy')\nplt.ylabel('Loan Interest rate')\nplt.xlabel('Purpose of the loan')\nplt.show()","b4d85299":"sns.stripplot(x=\"credit.policy\", y=\"dti\", data=loan,jitter=True,hue='purpose',palette='Set2',alpha=0.3)\nplt.title('Plot debt to income ratio classified by credit policy')\nplt.ylabel('Debt to Income Ratio')\nplt.xlabel('Credit Policy')\nplt.show()","f3cb4843":"#visualize debt to income ration vs the purpose of the loan\nplt.figure(figsize=(12,7))\nsns.violinplot(x='purpose', y='dti', data=loan,hue = 'credit.policy',jitter=True,palette='Set3')\nplt.title('Plot of the debt to income ration set against the purpose of the loan separated by credit policy')\nplt.ylabel('Debt to Income Ration')\nplt.xlabel('Purpose of the loan')\nplt.show()","f1a23666":"#annual income of the borrowers\nplt.hist(loan['log.annual.inc'],bins=30,orientation='vertical')\nplt.title('Plot of the annual income of the borrower')\nplt.grid()\nplt.ylabel('Frequency')\nplt.xlabel('Self declared income (log)')\nplt.show()","95417463":"#test of normality using Shapiro-wilk test\nfrom scipy.stats import shapiro\nstats,p = shapiro(loan['log.annual.inc'])\nprint('p-value of the Shapiro Normality Test {}'.format(p))\nif p>0.05:\n    print('Probably data is Gaussian')\nelse:\n    print('Probably data is not Gaussian')","659488cd":"#test of normality using Augustino k-square test\nfrom scipy.stats import normaltest\nstats,p = normaltest(loan['log.annual.inc'])\nprint('p-value of the Shapiro Normality Test {}'.format(p))\nif p>0.05:\n    print('Probably data is Gaussian')\nelse:\n    print('Probably data is not Gaussian')","a08e61fa":"loan.head(3)","52f3ea6b":"sns.distplot(loan['fico'])\nplt.title('Histogram Plot of the fico credit score borrower')\nplt.grid()\nplt.ylabel('Frequency')\nplt.xlabel('fico')\nplt.show()","1c49ce95":"#compare the fico score of the two borrower types \nsns.boxplot(x='credit.policy',y='fico',data=loan)\nplt.title('Plot of fico score versus the credit policy of the borrowers')\nplt.ylabel('FICO Credit Score')\nplt.xlabel('Credit Policy Lending Club')\nplt.grid()\nplt.show()","ab219e05":"#compare the fico score of the two borrower types\nwarnings.filterwarnings(action='ignore',message='')\nsns.boxplot(x=loan['credit.policy'],y=np.log(loan['revol.bal']))\nplt.title('Plot of revolving balance')\nplt.ylabel('Unpaid credit card balance (log scale)')\nplt.xlabel('Credit Policy Lending Club')\nplt.grid()\nplt.show()","b7d962bd":"#student t-test of means of the revol.bal for the two borrower class\ndata_0 = loan[loan['credit.policy']==0]['revol.bal']\ndata_1 = loan[loan['credit.policy']==1]['revol.bal']\n\nfrom scipy.stats import ttest_ind\nstat,p = ttest_ind(data_0,data_1)\nif p > 0.05:\n    print('Fail to reject the Null Hypothesis ')\n    print('The samples have same mean and probably they are from same distribution')\n    \nelse:\n    print('Reject the Null Hypothesis')\n    print('The samples have unequal means and probably they are from different distributions')","16531baf":"#check whether the credit policy is related to the number of days the borrower has had a credit line\nfrom scipy.stats import chi2_contingency\ntable = [loan['credit.policy'],loan['days.with.cr.line']]\n\nstat,p,dof,expected = chi2_contingency(table)\n\nif p > 0.05:\n    print('Fail to reject the Null Hypothesis ')\n    print('The two samples are independent')\n    \nelse:\n    print('Reject the Null Hypothesis')\n    print('The two samples are dependent')","1f6cbdb3":"loan['delinq.2yrs'].value_counts().sort_values(ascending=False)","f7ad710f":"#check the correlation of the features with credit policy\nloan.corr()['credit.policy'].sort_values()","f66ce638":"#label encode the purpose\nfrom sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\nloan['purpose'] = encoder.fit_transform(loan['purpose'])","6ff537fe":"#set a random state seed and the test size\nseed = 51\ntest_size = 0.2","3de08c6c":"#import the required libraries\nfrom sklearn.model_selection import train_test_split\ntrain_set,test_set = train_test_split(loan,test_size=0.2,random_state=seed,stratify=loan['credit.policy'])","2e3f8ef3":"#check the shape\ntrain_set.shape, test_set.shape","46cf4013":"#split into X_train and X_test and y_train and y_test ..data is already shuffled in the previous split\nX_train_orig = train_set.drop('credit.policy',axis=1)\ny_train_orig = train_set['credit.policy']\n\nX_test_orig = test_set.drop('credit.policy',axis=1)\ny_test_orig = test_set['credit.policy']\n","4b9baa48":"#check the proportion of the credit policy in the train and test split\ntrain_set['credit.policy'].value_counts()\/len(train_set)","1fceabef":"test_set['credit.policy'].value_counts()\/len(test_set)","e0803812":"#check the shape of the labels \/ target \ny_train_orig.shape, y_test_orig.shape","61b71260":"#store as array values\nX_train = X_train_orig.values\ny_train = y_train_orig.values\n\nX_test = X_test_orig.values\ny_test = y_test_orig.values","cdc0b263":"y_test.shape","7553fd9e":"y_test","2152107a":"y_train = y_train.reshape(y_train.shape[0],1)\ny_test = y_test.reshape(y_test.shape[0],1)","ffbe4e5d":"#check the shape now\ny_train.shape, y_test.shape","eb87de7a":"from sklearn.preprocessing import PowerTransformer\npt = PowerTransformer()\n#transform the train and the test set\nX_train = pt.fit_transform(X_train)\nX_test = pt.transform(X_test)","03d0e95e":"#Scale the inputs\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","d71abed0":"from sklearn.decomposition import PCA\npca = PCA(n_components=0.95) #retain 95% variablity in the data\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)","9363012e":"#check number of components \npca.n_components_","78c7f17d":"#import the model libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n#evaluation metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix","0d5b1ffb":"#constructor for the classifiers to be tested\nclassifiers = {'Logistic Regression':LogisticRegression(),\n               'Random Forest':RandomForestClassifier(random_state=seed)}\n               \n\nfor key,model in classifiers.items():\n    model.fit(X_train,y_train)\n    train_predict = model.predict(X_train)\n    test_predict = model.predict(X_test)\n    print('\\n')\n    print('Model {}'.format(key))\n    print('----------------------')\n    print('Train Data Recall Score',recall_score(y_train,train_predict))\n    print('Test Data Recall Score',recall_score(y_test,test_predict))   \n    #print the classification report based on predictions of the test data .. \n    print('\\n')\n    print(f'{key} Classification Report(Test Data)')\n    print('...............................')\n    print(classification_report(y_test,test_predict))\n    print(confusion_matrix(y_test,test_predict))","261c434e":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import cross_val_predict","303035ef":"#define a function to run cross validation across models \ndef cross_validate(X = X_train,y = y_train):\n    '''This function will run cross validation on multiple models and will print the accuracy score'''\n    \n    seed = 42\n    warnings.filterwarnings(action='ignore',message='')\n\n    models = []\n    models.append(('Logistic Regression',LogisticRegression(C=100.0)))\n    models.append(('Random Forest',RandomForestClassifier()))\n    # * Add more models to compare * #\n        \n    results = []\n    names = []\n    scoring ='recall'\n\n    for name,model in models:        \n        kfold = RepeatedStratifiedKFold(n_splits=10,random_state=seed,n_repeats=10)\n        cv_results = cross_val_score(model,X,y,cv=kfold,scoring=scoring)\n        print (f' Model: {name} ,Recall Score: {(np.mean(cv_results))}') ","4dc0d0e6":"#check the evaluation metric across the different models using cross validation\ncross_validate(X_train,y_train)","996a7a55":"from sklearn.model_selection import GridSearchCV","2a3fcdc8":"#initialize the model , set the criterion to be information gain rather than gini impurity\nrf_clf = RandomForestClassifier(random_state=seed,criterion='entropy')\n","3d89cefd":"param_grid = [{'n_estimators': [300,500,550,600]}]\ngrid_search = GridSearchCV(rf_clf, param_grid, cv=5,scoring='recall',return_train_score=True)\ngrid_search.fit(X_train,y_train)","7c004fcd":"grid_search.best_estimator_","b125c642":"#instantiate a new model based on the best params from grid search\nfrom sklearn.base import clone\nmodel = clone(grid_search.best_estimator_)\nmodel.fit(X_train,y_train)\ntest_pred =  model.predict(X_test)\n#print the model evaluation metrics\nprint('Classification Report on the Validation Data')\nprint(classification_report(y_test,test_pred))\nprint(confusion_matrix(y_test,test_pred))","bea8a7d2":"#plot the confusion matrix\nplot_confusion_matrix(model,X_test,y_test)\nplt.title('Confusion Matrix - Test Dataset')\nplt.show()","9232ed55":"#plot the confusion matrix\nplot_confusion_matrix(model,X_train,y_train)\nplt.title('Confusion Matrix - Train Dataset')\nplt.show()","65162bc2":"#import the required tensorflow keras libraries\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\n#keras wrapper for scikit learn to perform cross validation or grid search\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier","57f6752b":"#define the model --> required for the Keras Classifier class \ndef create_model(optimizer='adam',init='glorot_uniform',dropout=0.0):\n    model = Sequential()\n    #add the layers\n    model.add(Dense(units=500,input_dim=X_train.shape[1],activation='relu',kernel_initializer=init))\n    model.add(Dense(units=300,activation='relu',kernel_initializer=init))\n    model.add(Dropout(dropout))\n    model.add(Dense(units=100,activation='relu',kernel_initializer=init))\n    model.add(Dropout(dropout))\n    model.add(Dense(units=50,activation='relu',kernel_initializer=init))\n    model.add(Dropout(dropout))\n    model.add(Dense(units=1,activation='sigmoid',kernel_initializer=init))\n    #compile the model\n    model.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n    #return the model\n    return model   ","19f60db6":"#define the early stop criteria\nearly_stop = EarlyStopping(monitor='val_loss',patience=50,restore_best_weights=True)\n#create the model\nmodel = KerasClassifier(build_fn=create_model,epochs=100,batch_size=16,verbose=0)\n#parameters to search \noptimizers = ['rmsprop','adam','nadam']\n#define the param grid for grid search\nparam_grid = dict(optimizer=optimizers)\ngrid = GridSearchCV(estimator=model,param_grid=param_grid,cv=3)","e04a96cb":"#perform grid search \ngrid_result = grid.fit(X_train,y_train)","bfc14cac":"#print which optimizer performed best during the designated epochs \nprint('Best %f using %s' %(grid_result.best_score_,grid_result.best_params_))","a9b0bd67":"#summarize the results\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']","5cde360b":"#check the mean score with all the tested optmizers\nprint('Mean Accuracy:= RMSProp:%.5f, Adam:%.5f, Nadam:%.5f'%(means[0],means[1],means[2]))\nprint('Standard Dev Accuracy:= RMSProp:%.5f, Adam:%.5f, Nadam:%.5f'%(stds[0],stds[1],stds[2]))","90690b3a":"model = create_model(optimizer='nadam')\nmodel.fit(X_train,y_train,epochs=500,callbacks=[early_stop],validation_data=(X_test,y_test),\n          verbose=0)","98a67d9f":"y_pred = model.predict_classes(X_test)\ny_pred[:10]","5cca96e8":"#evaluate the model\nmodel_score = model.evaluate(X_test,y_test,verbose=1)\nprint('%s: %.2f%% ' %(model.metrics_names[1],model_score[1]*100 ))","8d33ed59":"print(classification_report(y_test,y_pred))","eadcddec":"model = create_model(optimizer='nadam',dropout=0.25)\nhistory = model.fit(X_train,y_train,epochs=500,callbacks=[early_stop],validation_data=(X_test,y_test),\n          verbose=0)","242b763b":"y_pred = model.predict_classes(X_test)\ny_pred[:10]","51e326c7":"#evaluate the model\nmodel_score = model.evaluate(X_test,y_test,verbose=1)\nprint('%s: %.2f%% ' %(model.metrics_names[1],model_score[1]*100 ))","db698755":"print(classification_report(y_test,y_pred))","025bc180":"#confusion matrix\nprint('confusion matrix of the dense neural n\/w with dropout enabled')\nprint(confusion_matrix(y_test,y_pred))","1fc68fee":"#save the model architecture and weights\nmodel.save('loan_risk_dense_model.h5')","f1e72b10":"## Approach \n-  Exploratory Data Analysis\n-  Statistical Hypothesis Tests on selected features (Shapiro wilk test, Augustino K^2 test, ..)\n-  Prepare and preprocess data (Power Transformation, Scaling , PCA)\n-  Select classifiers based on cross validation \n-  Fine tune the selected classifier and baseline the performance \n-  Define Dense Layer Model as a function \n-  Grid search on deep learning model parameters using Keras wrapper for Scikit learn\n-  Add regularization \n-  Compare performance of deep learning model with the fine tuned ML classifier ","1384a099":"The annual income of the borrowers appears to be normally distributed. ","17310234":"This is a Rank1 array and can result in bad issues during the neural network modeling. It is better to reshape this.","d13db908":"Recall score which reflects the True Positivity Rate, ie the model should be able to correctly identify all the positive classes (in this case credit.policy as 1).  The lending club's profit is based on lending the money at an interest rate and hence the model should be able to identify all the positive outcomes more accurately. For others predicted as credit policy 0, the interest rate could be set higher. \n\nThe fine tuned RF model provides a recall of 98% for the positive class. Still there are 179 0 classes are misclassified as 1. This can be tackled using precision recall and ROC approaches. However we will stick with this score and this would be the baseline for the neural net model to outperform. Again to re-stress, for smaller dataset as in this case, it is very hard to separate the best performing model. Deep learning algos definitely have benefits over larger datasets compared to the traditional ML models.\n\nOne thing to note from the confusion matrix below, the model has clearly overfitted. This can be addressed using regularization techniques. One option which is not exercised is data augmentation in which we can also try to increase the instances of the minority class using ADASYN or SMOTE. ","f52fe27b":"## _Split the data into train and test set_\nIn order to have similar split between the train and test, the data split would be stratified on the target label which is credit.policy in this case. If the model performance is bad, we can also try out synthetic data augmentation to balance the data.","ce738648":"## _Power Transformation and Scaling_\nApply a power transform featurewise to make data more Gaussian-like.","f666efbf":"The distribution looks to be the same . However we can perform a statistical test. However, t-test assumes the data to be normally and independently distributed. ","76a6ef51":"The features fico represents the FICO credit score of the borrower. This could be an important criterion for the lending club to decide the credibility of the borrower.","2daa9c85":"<b> The recall score is the same for the positive class 1 while it has improved by 23% for the class 0. Secondly, the accuracy of the model has improved to 93% from earlier 89% using the fine tuned random forest model. <\/b>\n\n<b> This is encouraging. Lets try out adding some regularization into the model using drop out layers. In the previous run the dropout argument was set as 0 which is a good as no Dropout. <\/b>","41c1e6fe":"<b> _Debt to income ration of the eligible customers are much higher compared to those who do not meet all the criteria of the lending club loan policy_ <\/b>","24091c13":"Great, now that the train set sample is right representation of the test.","3bc67d08":"<b> _The debt to income ratio of the customers who do not meet the criteria is higher compared to those who meet_ <\/b>","164407da":"## _Import Libraries and Load Data_","d20c56ec":"<font color=blue> <b>\n- *There are no null or missing values in the dataset*\n- *There exists multicollinearity between the factors* <\/font> <\/b>\n\nThere is one categorical variable which is purpose and remaining are numerical.","bd987bf9":"## _Please leave your feedback or remark. It will help to improve the notebook. Thank you !_ ","51050cc0":"## _Grid Search_","5557f5dc":"<b> Lets try it out with nadam optimizer and see if that itself can perform a better solution. In real sense we should trust and go with the best optimizer returned by grid search.<\/b>","0168894d":"## _Modeling using ML Algos_","3f6d3d14":"## _Dimensionality Reduction_\nReduce the dimensions using PCA","6bd32f6f":"## _Cross validation_\nTo be sure on the average performance of the model, perform the cross validation over the entire training set.","04ea2d9a":"# _Lending Club Loan Data Analysis_\n***\n\n<b>DESCRIPTION<\/b>\n\nCreate a model that predicts whether or not a loan will be default using the historical data.\n\n<b>Problem Statement:  <\/b>\n\nFor companies like Lending Club correctly predicting whether or not a loan will be a default is very important. In this project, using the historical data from 2007 to 2015, you have to build a deep learning model to predict the chance of default for future loans. As you will see later this dataset is highly imbalanced and includes a lot of features that makes this problem more challenging.\n***\n\n<b>Domain: Finance<\/b>\n\nAnalysis to be done: Perform data preprocessing and build a deep learning prediction model. \n\nContent: \n\nDataset columns and definition:\n\n- credit.policy: 1 if the customer meets the credit underwriting criteria of LendingClub.com, and 0 otherwise.\n\n- purpose: The purpose of the loan (takes values \"credit_card\", \"debt_consolidation\", \"educational\", \"major_purchase\", \"small_business\", and \"all_other\").\n\n- int.rate: The interest rate of the loan, as a proportion (a rate of 11% would be stored as 0.11). Borrowers judged by   LendingClub.com to be more risky are assigned higher interest rates.\n\n- installment: The monthly installments owed by the borrower if the loan is funded.\n\n- log.annual.inc: The natural log of the self-reported annual income of the borrower.\n\n- dti: The debt-to-income ratio of the borrower (amount of debt divided by annual income).\n\n- fico: The FICO credit score of the borrower.\n\n- days.with.cr.line: The number of days the borrower has had a credit line.\n\n- revol.bal: The borrower's revolving balance (amount unpaid at the end of the credit card billing cycle).\n\n- revol.util: The borrower's revolving line utilization rate (the amount of the credit line used relative to total credit available).\n\n- inq.last.6mths: The borrower's number of inquiries by creditors in the last 6 months.\n\n- delinq.2yrs: The number of times the borrower had been 30+ days past due on a payment in the past 2 years.\n\n- pub.rec: The borrower's number of derogatory public records (bankruptcy filings, tax liens, or judgments).\n","3ae5781f":"<b> _debt_consolidation has the maximum customer who meet as well who do not meet the credit policy criteria. all_other and home improvement are the next highest purpose for both 0 and 1 credit policy_<\/b> ","e6b96af0":"## _Model Training using Neural Nets_\nObjective: Try to beat the performance of the Random Forest Model. Please note that the neural networks performns better for a much larger dataset and for a relatively small dataset, it is hard to separate the various ML models and also the Neural N\/W model. ","2cf2ce69":"## _Exploratory Data Analysis_","6988cd23":"<b> _In this graph we clearly see that no matter what the purpose of the loan is, the interest rate is set higher for the customers who are more risky or do not meet the criteria of the lending club loan policy_<\/b>","19a2d0b9":"So based on what appears to be normal distributed fails to quality the statistical tests of normality of two very strong statistical tests. ","39d00e54":"### _Train the model for a longer epoch cycle with early stopping_\n","69d37db5":"### _Grid search deep learning model parameters_","89d69cda":"<b> With addition of dropout layer, while the accuracy did not improve, the recall score has improved for both the classes. Further optmization can be done using learning rate decay, using momentum optmizers, wide and deep networks. However, the current deep learning model with dropout regularization seems to be a good overall solution. <\/b>","cfe8abd4":"The median credit score of the borrowers with credit policy 1 is much higher compared to the borrowers with credit policy 0. At the same time, there are quite a few fico score beyond the IQR of the credit policy with 0 borrowers. The model might get confused trying to predict the outcome with the fico score. ","b124f79e":"<b>_Clearly the data is highly imbalanced dataset.  There are more customers who meet the credit underwriting than those who do not. Accuracy won't be a good measure of the model performance and tuning of the decision function to improve either precision or recall would be key. There are two ways to look at it. The lender won't want a situation where the loan is provided to a customer who would default. Other way, if the lender wants to maximise the lending, then the model should identify all the legitimate customers who would pay back. In this problem, I would go with the objective that the lending club which makes money by lending to customers is more keen to identify all people who meet the credit policy criteria. Moreover the interest rate of the risky customers is set high by the lending club_<\/b>","84f93e19":"The plain vanilla random forest classifier performs better than the logistic regression. However it also overfitted as is clear from the precision score on the train data. We can try out a lot of other models. However, we would fine tune the random forest classifier and use the performance of the tuned rf model as a baseline and then try to improve the overall score using neural network model.","04012da3":"### _Distributions and Statistical Hypothesis Test_","a6afbe2a":"<b> _The interest rate for the purpose of small_business is hightest, followed by debt consolidation_ <\/b>"}}