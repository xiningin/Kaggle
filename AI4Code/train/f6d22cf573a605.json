{"cell_type":{"cec55a0a":"code","73dc1d36":"code","11dede1c":"code","eb49ab17":"code","d58ef04b":"code","f5253dc6":"code","1dade2db":"code","a50400c0":"code","99f19c7e":"code","9c9382a4":"code","9cd417f6":"code","c96483a8":"code","8acc9d55":"code","8fb38434":"markdown","53347c8e":"markdown","3ec022e7":"markdown","0aa3d9cd":"markdown","72d78327":"markdown","c70ff6e8":"markdown","3a138601":"markdown","cb319210":"markdown","05ba969d":"markdown","b8c97211":"markdown","af985353":"markdown","5b3146d3":"markdown","712a977f":"markdown","394fc38a":"markdown","daf1ccd4":"markdown"},"source":{"cec55a0a":"import os\nimport json\nfrom pprint import pprint\nfrom copy import deepcopy\nfrom tqdm.notebook import tqdm\n\nfrom nltk.corpus import stopwords\n\nimport numpy as np\nimport random\nimport string\nimport re\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport collections\nfrom nltk.util import ngrams\nimport nltk\nimport networkx as nx\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","73dc1d36":"# Some procedures from https:\/\/www.kaggle.com\/xhlulu\/cord-19-eda-parse-json-and-generate-clean-csv\nbiorxiv_dir = '\/kaggle\/input\/CORD-19-research-challenge\/biorxiv_medrxiv\/biorxiv_medrxiv\/'\nfilenames = os.listdir(biorxiv_dir)\n\nall_files = []\n\nfor filename in filenames:\n    filename = biorxiv_dir + filename\n    file = json.load(open(filename, 'rb'))\n    all_files.append(file)\n\n    \nprint(f'Total number of files: {len(all_files)}')","11dede1c":"# Visualise the keys (columns) of the dataset\nfileTest = all_files[0]\nprint(\"Dictionary keys:\", fileTest.keys())","eb49ab17":"all_files = all_files[0:400] # Sample of the dataset for testing\nprint(f'Total number of files used: {len(all_files)}')","d58ef04b":"# Creating some functions to help us in pre-processing and ploting\n\n# Create function to perform pre processing\ndef pre_treatment_ngram(text):\n    # Get the text in lower case\n    text = text.lower()\n    # Remove everything from our dataset except letters, and spaces\n    text = re.sub(r'[^A-Za-z\\d ]', '', text)\n    # Clean the text with regex, remove Coprights and doi.org urls \n    text = re.sub(r'[Dd]oi:.*\\.|http.*\\.|International license.*\\.|[Cc]opyright.*\\.', '', text)\n    text = re.sub(' et | el | la | il | al | li | fig\\. ', '', text)\n    text = re.sub('[Mm]edrxiv.*\\.|[Ll]icence.*\\.', '', text) \n    # Tokenization\n    words_tokens = nltk.word_tokenize(text)\n    # Remove stopwords\n    stoplist = stopwords.words('english')\n    clean_word_list = [word for word in words_tokens if word not in stoplist]\n    return clean_word_list\n\n\n# Create function do help plot world cloud\ndef print_word_cloud(dictionary):\n    # Convert to string and send to wordcloud\n    textNgram = str(dictionary)\n    # Clean words\n    textNgram = re.sub('[\\']', '', textNgram)\n    # Send to wordcloud\n    wordcloud = WordCloud(width = 800, height = 800,\n                    background_color ='white',\n                    min_font_size = 10).generate(textNgram)\n\n    # plot the WordCloud image\n    plt.figure(figsize = (8, 8), facecolor = None)\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.tight_layout(pad = 0)\n    plt.show()\n\n# Function to create graph of conections\n# Thanks to https:\/\/www.earthdatascience.org\/courses\/earth-analytics-python\/using-apis-natural-language-processing-twitter\/calculate-tweet-word-bigrams-networks-in-python\/\ndef create_graph_conections(importantWord):\n    # Create dictionary of bigrams and their counts\n    d = ngramsList_df.set_index('n-gram').T.to_dict('records')\n    # Create network plot \n    G = nx.Graph()\n\n    # Create connections between nodes\n    for k, v in d[0].items():\n        G.add_edge(k[0], k[1], weight=(v * 10))\n\n    G.add_node(importantWord, weight=100)\n    fig, ax = plt.subplots(figsize=(20, 15))\n\n    pos = nx.spring_layout(G, k=1)\n\n    # Plot networks\n    nx.draw_networkx(G, pos,\n                     font_size=16,\n                     width=3,\n                     edge_color='grey',\n                     node_color='purple',\n                     with_labels = False,\n                     ax=ax)\n    \n    # Create offset labels\n    for key, value in pos.items():\n        x, y = value[0]+.135, value[1]+.045\n        ax.text(x, y,\n                s=key,\n                bbox=dict(facecolor='red', alpha=0.25),\n                horizontalalignment='center', fontsize=13)\n    plt.show()","f5253dc6":"# Creating a body text from dataset files\n# Some procedures from https:\/\/www.kaggle.com\/xhlulu\/cord-19-eda-parse-json-and-generate-clean-csv\n\ntextFromFile = 'body_text'\n\nbody = \"\"\nfor file in all_files:\n    try:\n        # Reading the text body from file\n        texts = [(di['section'], di['text']) for di in file[textFromFile]]\n        texts_di = {di['section']: \"\" for di in file[textFromFile]}\n        for section, text in texts:\n            texts_di[section] += text\n        \n        for section, text in texts_di.items():\n            body += section\n            body += text\n\n        # Clean the text with regex, remove Coprights and doi.org urls \n        body = re.sub(r'[Dd]oi:.*\\.|http.*\\.|International license.*\\.|[Cc]opyright.*\\.', '', body)\n        body = re.sub(' et | el | la | il | al | li | fig\\. ', '', body)\n        body = re.sub('[Mm]edr.*\\.', '', body)        \n    except: \n        continue\n    \n\nprint(f'Total number of characters, from {textFromFile} of the files:')\nprint(len(body))\n","1dade2db":"# Performing pre-treatment using the created function\nclean_word_list = pre_treatment_ngram(body)\nprint(f'Total number of filtered words, from {textFromFile} of the files:')\nprint(len(clean_word_list))\n# print(clean_word_list)","a50400c0":"# Define de ngram\nwordChain = 4\nngramsList = ngrams(clean_word_list, wordChain)\nresult = collections.Counter(ngramsList)\n\n# Convert to dataframe\nngramsList_df = pd.DataFrame(result.most_common(100),\n                             columns=['n-gram', 'count'])\n\n\n\n# ngram = ngramsList_df['n-gram']\n# y_pos = np.arange(len(ngram))\n# counts = ngramsList_df['count']\n\n\n# plt.figure(figsize=(9,11))\n# sns.barplot(x=counts,y=ngram)","99f19c7e":"# Define de ngram\nwordChain = 4\n\n# Define list of important words\nimportantWords = ['origin', 'COVID',\n                  'coronavirus', 'evolution', 'genetics']\n\nfor word in importantWords:\n    print(f'Network graphic related to the word \"{word}\"')\n    create_graph_conections(word)    ","9c9382a4":"dictNgrams = {}\n# Creating a dictionary of ngram\n# Thanks for https:\/\/stackabuse.com\/python-for-nlp-developing-an-automatic-text-filler-using-n-grams\/\n\nfor i in range(len(clean_word_list)-wordChain):\n    seq = ' '.join(clean_word_list[i:i+wordChain])\n    if  seq not in dictNgrams.keys():\n        dictNgrams[seq] = []\n        dictNgrams[seq].append(clean_word_list[i+wordChain])\n        \nprint('Some sentences of ngrams:\\n')\nfor x in list(dictNgrams)[0:20]:\n    print (x)","9cd417f6":"print_word_cloud(dictNgrams)","c96483a8":"# List of important words for filtering our sentences\nimportantWords = ['origin', 'virus', 'transmission', 'COVID',\n                  'coronavirus', 'evolution', 'genetics', 'glycoprotein']\ndictNgrams = {}\n# Thanks for https:\/\/stackabuse.com\/python-for-nlp-developing-an-automatic-text-filler-using-n-grams\/\nfor i in range(len(clean_word_list)-wordChain):\n    seq = ' '.join(clean_word_list[i:i+wordChain])\n    for w in importantWords:\n         if w in seq:\n#             print(seq)\n            if  seq not in dictNgrams.keys():\n                dictNgrams[seq] = []\n                dictNgrams[seq].append(clean_word_list[i+wordChain])\n                \n\n# print top keys\nk = collections.Counter(dictNgrams) \n  \n# Finding 600 values \nhigh = k.most_common(30)  \n\nprint(' Some sentences of ngrams with important words:\\n')\nprint(\"Keys: Values\\n\") \n  \nfor i in high: \n    print(i[0],\" :\",i[1],\" \") \n","8acc9d55":"print('Word cloud from ngrams sentences with important words:\\n')\n\nprint_word_cloud(dictNgrams)","8fb38434":"## Analyzing file texts\nIn order to analyze the texts of the articles without distinction, we created a unique body of text, using the code below.\n","53347c8e":"Second, we create some functions to help us in pre-processing and ploting in order to avoid repeating code snippets.","3ec022e7":"# Introduction\n\nIn response to the COVID-19 pandemic, the White House and a coalition of leading research groups have prepared the COVID-19 Open Research Dataset (CORD-19). CORD-19 is a resource of over 29,000 scholarly articles, including over 13,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. This freely available dataset is provided to the global research community to apply recent advances in natural language processing and other AI techniques to generate new insights in support of the ongoing fight against this infectious disease. There is a growing urgency for these approaches because of the rapid acceleration in new coronavirus literature, making it difficult for the medical research community to keep up.\n\n## Dataset Description\n\nThe CORD-19 dataset represents the most extensive machine-readable coronavirus literature collection available for data mining to date. This allows the worldwide AI research community the opportunity to apply text and data mining approaches to find answers to questions within, and connect insights across, this content in support of the ongoing COVID-19 response efforts worldwide. There is a growing urgency for these approaches because of the rapid increase in coronavirus literature, making it difficult for the medical community to keep up.\n\nThis dataset was created by the Allen Institute for AI in partnership with the Chan Zuckerberg Initiative, Georgetown University\u2019s Center for Security and Emerging Technology, Microsoft Research, and the National Library of Medicine - National Institutes of Health, in coordination with The White House Office of Science and Technology Policy.\n\nA list of our initial key questions can be found under the Tasks section of this dataset. These key scientific questions are drawn from the NASEM\u2019s SCIED (National Academies of Sciences, Engineering, and Medicine\u2019s Standing Committee on Emerging Infectious Diseases and 21st Century Health Threats) research topics and the World Health Organization\u2019s R&D Blueprint for COVID-19.\n\nReferences:   \n[https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge](http:\/\/)   \nCOVID-19 Open Research Dataset (CORD-19). 2020. Version 2020-03-13. Retrieved from https:\/\/pages.semanticscholar.org\/coronavirus-research. doi:10.5281\/zenodo.3715506\n\n## Objective\n\nThe aim of this notebook is to provide resources and insights, through data science, to answer the following questions raised by the challenge proposed by Kaggle:  \n*What do we know about virus genetics, origin, and evolution? \n*\n## Adopted strategy\n\nThe strategy adopted in this study to obtain the most relevant information from the dataset provided was to use NLP (Natural Language Processing), more specifically n-gram of words, together with graphical resources. \n\nThe dataset focused in this study was obtained from the biorxiv_medrxiv folder.\n\n### N-gram teory\n\nIn the fields of computational linguistics and probability, an n-gram is a contiguous sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application. The n-grams typically are collected from a text or speech corpus. When the items are words, n-grams may also be called shingles.  \n\nUsing Latin numerical prefixes, an n-gram of size 1 is referred to as a \"unigram\"; size 2 is a \"bigram\" (or, less commonly, a \"digram\"); size 3 is a \"trigram\". English cardinal numbers are sometimes used, e.g., \"four-gram\", \"five-gram\", and so on.\n\nIn general, ngrams sentences are generated after removing words of little textual value, known as Stop Words.\nThese sentences can bring synthesized information about the texts, especially when combined with filters and graphic resources.\n\nReference: [https:\/\/en.wikipedia.org\/wiki\/N-gram](http:\/\/)","0aa3d9cd":"Now we need to prepare our dataset. \nWe will perform some manipulations with the files provided.","72d78327":"In order to avoid excess memory demand we will execute the study with just a part of the total amount of files.","c70ff6e8":"Here, we generate a dictionary with the tetra-grams that contains a list of important words.   \nThis dictionary will help us to filter and focus our search domain on a group of relevant words.","3a138601":"### Creating and accessing ngrams\n\nIn this study we consider a tetra-gram (sequence of 4 words) as default ngram.\nIf necessary one can change it, just changing the value of the variable 'wordChain'.\nIn this step, we create the ngram sequence.   \nAfter that, we evaluate the results looking for insights. For this, visual analyzes were made through graphics, as well as based on readings of the obtained word sequences.\n\n### Viewing Networks of ngrams\n\nIn order to facilitate the understanding of the words interconnections  obtained in ngrams, we will use network graphics. The words represented by points are connected by the lines there.\n\nReference: https:\/\/www.earthdatascience.org\/courses\/earth-analytics-python\/using-apis-natural-language-processing-twitter\/calculate-tweet-word-bigrams-networks-in-python\/","cb319210":"# Conclusion\n\nIn this notebook we present a methodology based on the ngrams technique to analyse the documents provided in search of relevant information.   \nTetra-gram sentences were obtained and graphs and word clouds were plotted.   \nThe results show that the technique can be used to gain important insights into the coronavirus pandemic, in an agile way and without having to read thousands of full papers.\nThe same procedure can be used to study any topic, just by changing the list of relevant words.  \n \n## Pros and cons\n\nAbout the technique used, we can highlight the ease and speed of obtaining the results.   \nAs cons, we emphasize that it is not a fully automated analysis technique since it requires a certain work by the user.\n","05ba969d":"After, we print the unfiltered word cloud.","b8c97211":"### I hope you enjoyed!\n### Lovingly from Brazil to help the world fight the pandemic.\n![image.png](attachment:image.png)","af985353":"# Discussion\n\n### Answering the question of the task:  \n### What do we know about virus genetics, origin, and evolution?\n\nFrom reading the sentences of tetra-gram returned by the proposed algorithm, we can verify that some of them provide important information about genetics, origin, and evolution.\n\nBelow we select some of these sentences tha we found for contextualization:\n\n*high similarity bat coronaviruses   \ncoronaviruses outbreaks health care   \nancestor gorbalenya 2006 nidoviruses   \nchina suggesting animaltoperson transmission   \ndependence satellite viruses helper  \nvirus protein mediates fusion  \ncellular membranescoronaviruses bind cell  \nreplicase protein suggest nidoviruses  \nviruscell fusion entry mechanisms  \nsimilar dependence satellite viruses \n\nNote: Due to the certain randomness in the selection of the most important sentences, some of these phrases     may not appear in a certain round, but similar ones should appear.   *\n\nBased on these senteces we can assume that Coronavirus:   \n**It has similarity with bat coronavirus and may have migrated from animal to person.   \nEnzyme replicase indicates a probable relationship with nidoviruses. \nIt resembles satellite viruses.\n**   \nNote: It is clear that all these assertions must be evaluated by an expert with the aim of validation or not.\n\nThe graphics of the word networks provide us, in a simple and agile way, information about the interconnection of important words. Much of the information obtained there corroborates with the conclusions proposed above.\n\nThe generated word clouds provide us with visual information for the main search terms very quickly.\n","5b3146d3":"### Creating a word cloud with important words\n\nFirst, we create an unfiltered word cloud.   \nAfter that, we create an n-gram based word cloud that contains a list of important words.\n\nIn this step, we generate the tetra-gram dictionary considering the entire text.\n\nReference: https:\/\/stackabuse.com\/python-for-nlp-developing-an-automatic-text-filler-using-n-grams\/","712a977f":"# Code development for insights\nFirst, let's import the necessary libraries.  \nFor our NLP development we will mainly use the NLTK library, moreover we use wordcloud, seaborn and networkx libraries for plots and graphs.","394fc38a":"![image.png](attachment:image.png)\n\nPhoto Credit: STR \/ Getty, Source: https:\/\/wellcome.ac.uk\/news\/urgent-investment-needed-diagnostics-treatments-and-vaccines-end-covid-19-pandemic-coronavirus\n\n*Medical staff disinfect equipment at an isolation ward for patients infected by COVID-19 in Wuhan, China. Ending the pandemic relies on greater investment and global coordination.\n*\n","daf1ccd4":"Now, as we said earlier, we generate the word cloud with sentences generated from the list of important words."}}