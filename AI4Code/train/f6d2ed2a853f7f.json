{"cell_type":{"3d77c4bf":"code","2e1a2da3":"code","f127d547":"code","877d3bc1":"code","ed52a1a0":"code","49ab5f55":"code","56b66d83":"code","025f030e":"code","2d7b4418":"code","1352bbe7":"code","a09d6e35":"code","a20666ba":"code","b0bf0b4a":"code","3479eb9f":"code","be391a98":"code","6b4c48c9":"code","bfedc891":"code","667e35b8":"markdown","ca557b48":"markdown","deeaf400":"markdown","76099c81":"markdown","6963275e":"markdown","beb41101":"markdown","ea643d5b":"markdown","e42ff387":"markdown","a55ea9dd":"markdown","25f9484f":"markdown","93200cb8":"markdown","22090dd4":"markdown"},"source":{"3d77c4bf":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport math\nimport matplotlib.cm as cm \nimport matplotlib.pyplot as plt\nimport os\nprint(os.listdir(\"..\/input\"))","2e1a2da3":"train_data = pd.read_csv(\"..\/input\/train.csv\")\ntest_data= pd.read_csv(\"..\/input\/test.csv\")\n#separating labels and pixels\ntrain_labels=np.array(train_data.loc[:,'label'])\ntrain_data=np.array(train_data.loc[:,train_data.columns!='label'])\n#train_data=train_data\/train_data.max()","f127d547":"#Visualize the input data. Change the index value to visualize the particular index data.\nindex=7;\nplt.title((train_labels[index]))\nplt.imshow(train_data[index].reshape(28,28), cmap=cm.binary)","877d3bc1":"print(\"train data\")\ny_value=np.zeros((1,10))\nfor i in range (10):\n    print(\"occurance of \",i,\"=\",np.count_nonzero(train_labels==i))\n    y_value[0,i-1]= np.count_nonzero(train_labels==i)\n","ed52a1a0":"y_value=y_value.ravel()\nx_value=[0,1,2,3,4,5,6,7,8,9]\nplt.xlabel('label')\nplt.ylabel('count')\nplt.bar(x_value,y_value,0.7,color='g')","49ab5f55":"#converting train_label in one hot encoder representation \ntrain_data=np.reshape(train_data,[784,42000])\ntrain_label=np.zeros((10,42000))\nfor col in range (42000):\n    val=train_labels[col]\n    for row in range (10):\n        if (val==row):\n            train_label[val,col]=1\nprint(\"train_data shape=\"+str(np.shape(train_data)))\nprint(\"train_label shape=\"+str(np.shape(train_label)))\n\n","56b66d83":"#activation functions sigmoid relu and softmax\ndef sigmoid(Z):\n    A = 1\/(1+np.exp(-Z))\n    cache = Z\n    return A, cache\n\ndef relu(Z):\n    A = np.maximum(0,Z)    \n    cache = Z \n    return A, cache\n\ndef softmax(Z):\n    e_x = np.exp(Z)\n    A= e_x \/ np.sum(np.exp(Z))  \n    cache=Z\n    return A,cache   ","025f030e":"#derivative of activation function\ndef relu_backward(dA, cache):\n    Z = cache\n    dZ = np.array(dA, copy=True)\n    dZ[Z <= 0] = 0\n    assert (dZ.shape == Z.shape)\n    return dZ\n\ndef sigmoid_backward(dA, cache):    \n    Z = cache\n    s = 1\/(1+np.exp(-Z))\n    dZ = dA * s * (1-s)\n    assert (dZ.shape == Z.shape)\n    return dZ\n\ndef softmax_backward(Z,cache):\n    Z=cache\n    length=10  \n    dZ=np.zeros((42000,10))\n    Z=np.transpose(Z)\n    for row in range (0,42000):\n            den=(np.sum(np.exp(Z[row,:])))*(np.sum(np.exp(Z[row,:])))\n            for col in range (0,10):\n                sums=0\n                for j in range (0,10):\n                    if (j!=col):\n                        sums=sums+(math.exp(Z[row,j]))\n                \n                dZ[row,col]=(math.exp(Z[row,col])*sums)\/den           \n    dZ=np.transpose(dZ)\n    Z=np.transpose(Z)\n\n    assert (dZ.shape == Z.shape)\n    return dZ","2d7b4418":"#initializing the parameters weights and bias\ndef initialize_parameters_deep(layer_dims):\n    #np.random.seed(1)\n    parameters = {}\n    L = len(layer_dims)            # number of layers in the network\n    \n    for l in range(1, L):\n        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) \/ np.sqrt(layer_dims[l-1]) #*0.01\n        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n    \n    return parameters","1352bbe7":"#forward propagation\ndef linear_forward(A, W, b):\n    Z = np.dot(W,A) +b\n    cache = (A, W, b)\n    assert(Z.shape == (W.shape[0], A.shape[1]))\n    return Z, cache\n\ndef linear_activation_forward(A_prev, W, b, activation):\n    if activation == \"sigmoid\":\n        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = sigmoid(Z)\n    elif activation == \"relu\":\n        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        #print(\"Z=\"+str(Z))\n        A, activation_cache = relu(Z) \n    elif activation == \"softmax\":\n        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = softmax(Z)\n    cache = (linear_cache, activation_cache)\n    return A, cache\n\ndef L_model_forward(X, parameters):\n    caches = []\n    A = X\n    L = len(parameters) \/\/ 2                  # number of layers in the neural network\n    for l in range(1, L):\n        A_prev = A \n        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n        caches.append(cache)\n    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"softmax\")\n    caches.append(cache)               \n    return AL, caches","a09d6e35":"#cost function\ndef compute_cost(AL, Y):\n    \n    m = Y.shape[1]\n    cost = (-1 \/ m) * np.sum(np.multiply(Y, np.log(AL)) + np.multiply(1 - Y, np.log(1 - AL)))\n    #print(\"cost=\"+str(cost))\n    return cost","a20666ba":"#backward propagation\ndef linear_backward(dZ, cache):\n    A_prev, W, b = cache\n    m = A_prev.shape[1]\n    dW = 1.\/m * np.dot(dZ,A_prev.T)  \n    db = (1\/m)*np.sum(dZ, axis=1, keepdims=True);\n    dA_prev = np.dot(W.T,dZ)\n    return dA_prev, dW, db\n\ndef linear_activation_backward(dA, cache, activation):\n    linear_cache, activation_cache = cache\n    if activation == \"relu\":\n        dZ = relu_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)  \n    elif activation == \"sigmoid\":\n        dZ = sigmoid_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n    elif activation == \"softmax\":\n        dZ = softmax_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n    return dA_prev, dW, db\n\ndef L_model_backward(AL, Y, caches):\n    grads = {}\n    L = len(caches) # the number of layers\n    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n    M=len(layers_dims)\n    current_cache = caches[M-2]\n    grads[\"dA\"+str(M-1)], grads[\"dW\"+str(M-1)], grads[\"db\"+str(M-1)] = linear_activation_backward(dAL, current_cache, activation = \"softmax\")#M-1\n    for l in reversed(range(L-1)):\n        current_cache = caches[l]\n        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, activation = \"relu\")\n        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n        grads[\"dW\" + str(l + 1)] = dW_temp\n        grads[\"db\" + str(l + 1)] = db_temp\n    \n    return grads","b0bf0b4a":"#upgrade function for weights and bias\ndef update_parameters(parameters, grads, learning_rate):\n    for l in range(len_update-1):\n        parameters[\"W\" + str(l+1)] =parameters[\"W\" + str(l+1)] - (learning_rate*grads[\"dW\" + str(l+1)])\n        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - (learning_rate*grads[\"db\" + str(l+1)])\n    return parameters","3479eb9f":"def plot_graph(cost_plot):\n       \n    x_value=list(range(1,len(cost_plot)+1))\n    #print(x_value)\n    #print(cost_plot)\n    plt.xlabel('iteration')\n    plt.ylabel('cost')\n    plt.plot(x_value,cost_plot,0.,color='g')\n","be391a98":"#defining structure of neural network\nlayers_dims = [784,500,400,300,100,10] #  n-layer model (n=6 including input and output layer)\nlen_update=len(layers_dims)","6b4c48c9":"#function to call sub_functions\ndef L_layer_model(X, Y, layers_dims, learning_rate , num_iterations , print_cost=False):#lr was 0.009\n    print(\"training...\")\n    costs = []  \n    cost_plot=np.zeros(num_iterations)\n    parameters = initialize_parameters_deep(layers_dims)\n    for i in range(0, num_iterations):\n        AL, caches = L_model_forward(X, parameters)\n        cost =compute_cost(AL, Y)\n        grads = L_model_backward(AL, Y, caches)\n        parameters = update_parameters(parameters, grads, learning_rate) \n        cost_plot[i]=cost;\n    \n    plot_graph(cost_plot)\n    return parameters","bfedc891":"#variable parameter in network learning_rate, iterationd \nparameters = L_layer_model(train_data, train_label, layers_dims,learning_rate = 0.0005, num_iterations =35 , print_cost = True) \nprint(\"training done\")","667e35b8":"<a id='Cost calculation'><\/a>\n ## 6.Cost calculation\n\nCost is calculated usign output of forward propagaion(softmax layer) and train_label. ","ca557b48":"<a id='Derivative of activation function'><\/a>\n ## 3.Derivative of activation function\n* These derivative of activation function are used in back propagation.","deeaf400":"<a id='Conclusion'><\/a>\n ## 11.Conclusion \n This code should be sufficient to understand the working of a simple neural network. Understanding the working of activation function, forward and backward propagation will give the user more flexibility and a better understanding of the concept. Although the efficiency of the code is less as compared to the same problem solved through tensorflow but it gives more insight about the network.\n","76099c81":"<a id='Initialization'><\/a>\n ## 4.Initializaion\n* Weights\n* Bias","6963275e":"<a id='Define architecture'><\/a>\n ## 10.Define architecture \n\nDefine layers_dim for required architecture of neural nets. First element is the input layer of 28* 28=784 pixel value. Last element is the output layer of 10 classes(0 to 9). Other elements are hidden layer with number of nodes.(Example First hidden layer has 500 nodes, second hiden layer has 400 nodes.....).\nFeel free to change the architecture and choose the best optimized one!!","beb41101":"<a id='1.Analyse the input data'><\/a>\n ## 1.Analyse the input data\n* Visualize the data at particular index position. Change the index to visualise other element.\n* Graph is plotted for the number of occurance of particular element in dataset.","ea643d5b":"## **Abstract**\n\nIn the code below training on MNIST dataset is done using neural networks. Implementation has been done with minimum use of libraries to get a better understanding of the concept and working on neural nets. Functions for initialization, activation, forward propagation, backward propagation, cost have been written separately.  The training labeled dataset consists of 42000 images, each of size 28x28 = 784 pixels. Labels are from 0 to 9. In this we are not going to use tensorflow or any other such module.\n\n\n[1.Analyse the input data](#1.Analyse the input data)  \n[2.Activation functions](#Activation functions)  \n[3.Derivative of activation function](#Derivative of activation function)  \n[4.Initialization](#Initialization)  \n[5.Forward propagation](#Forward propagation)  \n[6.Cost calculation](#Cost calculation)   \n[7.Back propagation](#Back propagation)   \n[8.Update parameters](#Update parameters)  \n[9.Plot graph of cost](#.Plot graph of cost)  \n[10.Define architecture](#Define architecture)  \n[11.Conclusion](#Conclusion)  \n","e42ff387":"<a id='Back propagation'><\/a>\n ## 7.Back propagation","a55ea9dd":"<a id='Update parameters'><\/a>\n ## 8.Update parameters\n* Updated Weights    \nW=W-(learning_rate* dW)\n* Updated Bias     \nB=B-(learning_rate* db)","25f9484f":"<a id='Forward propagation'><\/a>\n ## 5.Forward propagation\nForward propagation is used to calculate the activated output of particular node. Function linear_forward is used to calculate Z value(z=wa+b). Then it is passed through activation (g(z)) to get the activated output or input for the next layer. The N hidden layers are using 'relu' while the output layer is using 'softmax' to generate out[ut in 10 classes(0 to 9).\n","93200cb8":"<a id='.Plot graph of cost'><\/a>\n ## 9.Plot graph of cost","22090dd4":"<a id='Activation functions'><\/a>\n ## 2.Activation functions\n* Relu\n* Softmax\n* Sigmoid(not used in this code)"}}