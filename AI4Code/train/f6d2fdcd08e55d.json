{"cell_type":{"0820754e":"code","19bf76d5":"code","e7cf1ee1":"code","2fe7a6cd":"code","eea746dc":"code","19b511d4":"code","56b6b311":"code","1f526796":"code","4e57b2bd":"code","7c32b588":"code","192830fb":"code","28050381":"code","e95c2511":"code","ae45abc4":"markdown","3b7e3da6":"markdown","9d29673d":"markdown","03c5d164":"markdown","068757f6":"markdown","29b97358":"markdown","47f0cc98":"markdown","9809d08a":"markdown","841fea9f":"markdown","b9be22b7":"markdown","49d46e08":"markdown","c42c808f":"markdown","8674ed88":"markdown","427ae861":"markdown","067be351":"markdown","f63c61cc":"markdown","6ac4955e":"markdown","6d901f28":"markdown","0d700b30":"markdown","f00a6ea9":"markdown","c829da34":"markdown","4ecb65ff":"markdown"},"source":{"0820754e":"# Programming\nimport pandas as pd\nimport numpy as np\n\n# Sklearn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.tree import DecisionTreeRegressor, plot_tree\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\n\n# Visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\n\n# Other \nimport warnings\nwarnings.filterwarnings('ignore')","19bf76d5":"# Loading individual datasets\naudi = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/audi.csv')\nbmw = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/bmw.csv')\ncclass = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/cclass.csv')\nfocus = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/focus.csv')\nhyundi = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/hyundi.csv')\nmerc = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/merc.csv')\nskoda = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/skoda.csv')\ntoyota = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/toyota.csv')\nvauxhall = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/vauxhall.csv')\nvw = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/vw.csv')\n\n# Adding a 'Brand' column\naudi['brand'] = 'Audi'\nbmw['brand'] = 'BMW'\ncclass['brand'] = 'Mercedes'\nfocus['brand'] = 'Ford'\nhyundi['brand'] = 'Hyundai'\nmerc['brand'] = 'Mercedes'\nskoda['brand'] = 'Skoda'\ntoyota['brand'] = 'Toyota'\nvauxhall['brand'] = 'Vauxhall'\nvw['brand'] = 'Volkswagen'\n\n# Creating a single Df from the 'clean' Dfs\ndf_clean_list = [audi, bmw, cclass, focus, merc, skoda, toyota, vauxhall, vw]\ndf = pd.concat(df_clean_list, sort=False)","e7cf1ee1":"# Seaborn Settings\nsns.set()\nsns.set(rc={'figure.figsize':(30,5)})\n\n# GRAPH 0\n    # Price Distribution\nsns.histplot(x='price',kde=True, data=df)\nplt.title('PRICE DISTRIBUTION')\nplt.show()\nplt.close()\n\n# GRAPH 1\n    # Brand Distribution\nplt.subplot(1,2,1)\nsns.countplot(x='brand', data=df)\nplt.xticks(rotation=90)\nplt.title('AMOUNT OF OBSERVATIONS PER BRAND')\n    # Launch Year Distribution \nplt.subplot(1,2,2)\nsns.histplot(x='year',kde=True, data=df)\nplt.title('LAUNCH YEAR')\nplt.show()\nplt.close()\n\n# GRAPH 2\n    # Transmission Distribution\nplt.subplot(1,2,1)\nsns.countplot(x='transmission', data=df)\nplt.title('TRANSMISSION TYPE')\n    # Transmission Distribution\nplt.subplot(1,2,2)\nsns.countplot(x='fuelType', data=df)\nplt.title('FUEL TYPE')\nplt.show()\nplt.close()\n\n# GRAPH 3\n    # Model Distribution\nsns.countplot(x='model', data=df)\nplt.xticks(rotation=90)\nplt.title('AMOUNT OF OBSERVATIONS PER MODEL')\nplt.show()\nplt.close()","2fe7a6cd":"# Seaborn Settings\nsns.set()\nsns.set(rc={'figure.figsize':(30,5)})\n\n# GRAPH 0\n    # Numerical Features Correlations\ncorr = df.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.heatmap(df.corr(), mask=mask, vmin=-1, vmax=1,\n     cmap=\"Spectral\", annot=True, fmt='.2f')\nplt.yticks(rotation=0)","eea746dc":"# GRAPH 0\n    # Transmission\nplt.subplot(1,3,1)\nsns.boxplot(x='transmission', y='price', data=df)\nplt.title('TRANSMISSION BOXPLOT')\n    # Fuel Type\nplt.subplot(1,3,2)\nsns.boxplot(x='fuelType', y='price', data=df)\nplt.title('FUEL TYPE BOXPLOT')\n    # Brans\nplt.subplot(1,3,3)\nsns.boxplot(x='fuelType', y='price', data=df)\nplt.title('BRAND BOXPLOT')\nplt.show()\nplt.close()","19b511d4":"# Creating a single Df from the 'clean' Dfs\ndf_clean_list = [audi, bmw, cclass, focus, merc, skoda, toyota, vauxhall, vw]\ndf_clean = pd.concat(df_clean_list, sort=False)\ndel df_clean_list\n\n# Concatinating all Dfs\ndf = df_clean\n\n# Visually checking the Df\ndf.head()","56b6b311":"# Check for missing values\nprint('# of Missing Values')\nprint(df.isnull().sum())\n\n# Visually inspect missing values\nmsno.matrix(df)\nplt.title('MISSING VALUES')\n\n# Dropping all observations with missing values\ndf = df.dropna()\n\nmsno.matrix(df)\nplt.title('MISSING VALUES AFTER OBSERVATION DROP')","1f526796":"print(df.dtypes)\ndf.head()","4e57b2bd":"age_list = []\nfor x in df.year:\n    y = 2021 - x\n    age_list.append(y)\ndf['age'] = age_list\ndf = df.drop('year', axis=1).copy()\ndf.head()","7c32b588":"# Dropping the 'brand' column (information already included on the 'model' feature)\ndf = df.drop('brand', axis=1)\n# Categorical Feature Encoding\n    # transmission ['Manual' 'Automatic' 'Semi-Auto' 'Other']\ndf_transmission = df[['transmission']]\ndf_transmission = pd.get_dummies(df_transmission, drop_first=True, prefix='transmission')\n    # fuelType ['Petrol' 'Diesel' 'Hybrid' 'Other' 'Electric']\ndf_fuel = df[['fuelType']]\ndf_fuel = pd.get_dummies(df_fuel, drop_first=True, prefix='fuel')\n    # model print(df_model.model.unique())\ndf_model = df[['model']]\ndf_model = pd.get_dummies(df_model, drop_first=True, prefix='model')\n\n# Selecting numerical features\ndf_numerical = df[['mileage', 'tax', 'mpg', 'engineSize', 'age']]\n\n# Unifying all in a single df\ndf = pd.concat([df[['price']], df_transmission], axis=1).reset_index(drop=True)\ndf = pd.concat([df, df_fuel.reset_index(drop=True)], axis=1)\ndf = pd.concat([df, df_model.reset_index(drop=True)], axis=1)\ndf = pd.concat([df, df_numerical.reset_index(drop=True)], axis=1)\n\ndf.head()","192830fb":"# Fixing seed for reproducibility\nseed = 1\n\n# Creating the Features\/Label split as numpy arrays\nfeatures =  df.drop('price', axis=1).values\ntarget = df[['price']].values\n\n# Creating the test\/train split\ntraining_features, test_features,\\\ntraining_target, test_target = \\\ntrain_test_split(features, target, test_size=0.8, random_state=seed)","28050381":"# DECISION TREE REGRESSOR\n    # Hyperparameters Options\ndectree_params = {'criterion': ['mse', 'mae'],\n                  'splitter': ['best', 'random'],\n                  'random_state': [seed]}\n    # Model\ndectree = DecisionTreeRegressor()\n    # Hyperparameter Tuning + CrossValidation\ndectree_gridcv = GridSearchCV(estimator=dectree, param_grid=dectree_params,\n                              scoring='r2', cv=5)\ndectree_gridcv.fit(training_features, training_target)\n    # Optimal Model Output\ndectree_best_est = dectree_gridcv.best_estimator_\ndectree_feat_imp = dectree_best_est.feature_importances_\nprint('BEST ESTIMATOR')\nprint(dectree_best_est)\nprint('\\n')\n    # Scores\n        # Training\ndectree_training_prediction = dectree_best_est.predict(training_features)\ndectree_best_score = dectree_gridcv.best_score_ \ndectree_rmse_training = np.sqrt(mean_squared_error(training_target, dectree_training_prediction))\nprint('TRANINIG DATA')\nprint('r2: {:.4f}'.format(dectree_best_score))\nprint('RMSE: {:.0f}'.format(dectree_rmse_training))\nprint('\\n')\n        # Test\ndectree_test_prediction = dectree_best_est.predict(test_features)\ndectree_test_score = dectree_gridcv.score(test_features, test_target)\ndectree_rmse_test = np.sqrt(mean_squared_error(test_target, dectree_test_prediction))\nprint('TEST DATA')\nprint('r2: {:.4f}'.format(dectree_test_score))\nprint('RMSE: {:.0f}'.format(dectree_rmse_test))\n    # Graph\nsns.set(rc={'figure.figsize':(15,40)})\n        # Feature Importance\nfeature_names = df.drop('price', axis=1).columns.to_list()\nplt.barh(feature_names, dectree_feat_imp)\nplt.title('FEATURE IMPORTANCE')\nplt.show()\nplt.close()","e95c2511":"# XGBREGRESSOR\n    # Hyperparameters Options\nxgb_params = {'objective': ['reg:squarederror'],\n              'random_state': [seed]}\n    # Model\nxgb = XGBRegressor()\n    # Hyperparameter Tuning + CrossValidation\nxgb_gridcv = GridSearchCV(estimator=xgb, param_grid=xgb_params,\n                              scoring='r2', cv=5)\nxgb_gridcv.fit(training_features, training_target)\n    # Optimal Model Output\nxgb_best_est =xgb_gridcv.best_estimator_\nxgb_feat_imp = xgb_best_est.feature_importances_\nprint('BEST ESTIMATOR')\nprint(xgb_best_est)\nprint('\\n')\n    # Scores\n        # Training\nxgb_training_prediction = xgb_best_est.predict(training_features)\nxgb_best_score = xgb_gridcv.best_score_ \nxgb_rmse_training = np.sqrt(mean_squared_error(training_target, xgb_training_prediction))\nprint('TRANINIG DATA')\nprint('r2: {:.4f}'.format(xgb_best_score))\nprint('RMSE: {:.0f}'.format(xgb_rmse_training))\nprint('\\n')\n        # Test\nxgb_test_prediction = xgb_best_est.predict(test_features)\nxgb_test_score = xgb_gridcv.score(test_features, test_target)\nxgb_rmse_test = np.sqrt(mean_squared_error(test_target, xgb_test_prediction))\nprint('TEST DATA')\nprint('r2: {:.4f}'.format(xgb_test_score))\nprint('RMSE: {:.0f}'.format(xgb_rmse_test))\n    # Graph\nsns.set(rc={'figure.figsize':(15,40)})\n        # Feature Importance\nplt.barh(feature_names, xgb_feat_imp)\nplt.title('FEATURE IMPORTANCE')\nplt.show()\nplt.close()","ae45abc4":"<a id='subsection-zero'><\/a>\n#### *Data Source & Contents*\nScraped data of used cars listings. 100,000 listings, which have been separated into files corresponding to each car manufacturer. I collected the data to make a tool to predict how much my friend should sell his old car for compared to other stuff on the market, and then just extended the data set. Then made a more general car value regression model.\nThe cleaned data set contains information of price, transmission, mileage, fuel type, road tax, miles per gallon (mpg), and engine size.\n\n#### *Tasks*\n1. Create a regression model that **predicts selling price** based on the inidividual car characteristics\n2. **Idenitify which features drive car price** and the positive or negative effect on selling price\n\n\n#### *Approach*\n1. Do any necessary cleaning of the datsets so they can be unified \n2. Create a single DataFrame that unifies all brand specific datasets\n3. EDA. Explore the data to try and identify main price drivers\n4. Pre-Processing \/ Feature Engineering to prepare the data for model implementation\n5. Model testing\n6. Explore feature importance and feature impact on price to elaborate conclusion","3b7e3da6":"<a id='subsection-ten'><\/a>\n#### *Encoding Categorical Features*","9d29673d":"<a id='subsection-three'><\/a>\n#### *Population Distribution*","03c5d164":"<a id='subsection-nine'><\/a>\n#### *Obtaining Age*","068757f6":"<a id='subsection-six'><\/a>\n#### *Unifying in a single Df*","29b97358":"<a id='section-two'><\/a>\n## Exploratory Data Analysis","47f0cc98":"<a id='subsection-seven'><\/a>\n#### *Handling Missing Values*","9809d08a":"<a id='subsection-five'><\/a>\n#### *Boxplots*","841fea9f":"<a id='subsection-fifteen'><\/a>\n#### *Feature Importance*\n\nThe most relevant features based on the two models applied are:\n- **age**, drives price down. The oldest the car is, the cheaper it becomes. This is due not only due to just the age of the vehicle, but also to the fact htat age is also strongly associated with mileage, which also drives the price down.\n- **engineSize**, drives price up. Vehicles with more powerful engines are sold at higher prices as vehicle performance is expected to be better.\n- **transmission_Manual**, drives price down. Manual transmission, the most basic transmission type has on average lower prices than the other trnasmission types.\n- **mileage**, drives price down. The more mileage a vehicle has, the more it deteriorates, increasing the need for repairs. This drives price down, as more future expenses are expected.\n- **mpg**, drives price down. My assumption would be that efficient cars (bigger mpg) are designed to be used and sold as affordable vehicles (low price), whilst inefficient cars (low mpg) are designed to be high performing vehicles (high price)","b9be22b7":"## Table of Contents\n* [Introduction](#section-one)\n    - [Data Sources | Task Definition | Planned Approach](#subsection-zero)\n    - [Library Imports](#subsection-one)\n    - [Data Imports](#subsection-two)\n* [Exploratory Data Analysis](#section-two)\n    - [Population Distribution](#subsection-three)\n    - [Correlations](#subsection-four)\n    - [Boxplots](#subsection-five)\n* [Pre-Processing | Feature Engineering](#section-three)\n    - [Unifying in a single Df](#subsection-six)\n    - [Handling Missing Values](#subsection-seven)\n    - [Checking Data Types](#subsection-eight)\n    - [Obtaining Age](#subsection-nine)\n    - [Encoding Categorical Features](#subsection-ten)\n* [Model Implementation](#section-four)\n    - [Test-Train Split](#subsection-eleven_)\n    - [Decision Tree Regressor](#subsection-twelve)\n    - [XGB Regressor](#subsection-twelve_)\n* [Conclusions](#section-five)\n    - [Optimal Model](#subsection-fourteen)\n    - [Feature Importance](#subsection-fifteen)","49d46e08":"<a id='section-five'><\/a>\n## Conclusions","c42c808f":"<a id='section-three'><\/a>\n## Pre-Processing","8674ed88":"<a id='section-four'><\/a>\n## Model Implementation","427ae861":"<a id='subsection-eleven_'><\/a>\n#### *Test-Train Split*","067be351":"<a id='subsection-two'><\/a>\n#### *Data Imports*","f63c61cc":"<a id='subsection-fourteen'><\/a>\n#### *Optimal Model*\n\n**XGB Regressor is the optimal model**, as we can see on the table below it delivers **higher R2** and **lower RMSE** on the test data than the Decision Tree Regressor model.\n\nModel | Training r2 | Training RMSE | Test r2 | Test RMSE\n- | - | - | - | -\nDecision Tree Regressor | 0.8790 | 182 | 0.8906 | 3503\nXGB Regressor | 0.9332 | 1918 | 0.9434| 2520","6ac4955e":"<a id='subsection-eight'><\/a>\n#### *Checking DataTypes*","6d901f28":"<a id='subsection-one'><\/a>\n#### *Library Imports*","0d700b30":"<a id='section-one'><\/a>\n## Introduction","f00a6ea9":"<a id='subsection-four'><\/a>\n#### *Correlations*","c829da34":"<a id='subsection-twelve'><\/a>\n#### *Decision Tree Regressor*","4ecb65ff":"<a id='subsection-twelve_'><\/a>\n#### *XGB Regressor*"}}