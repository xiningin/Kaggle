{"cell_type":{"f06e34a7":"code","8bf61977":"code","113dc3d2":"code","38a187db":"code","99fd5d43":"code","780aa188":"code","cffc40c4":"code","371d19d2":"code","d75cbb24":"code","df898df5":"code","76be96d0":"code","67a2e6eb":"code","17bcea0b":"code","f46ad223":"code","fdba5b1a":"code","bed4c98e":"code","8537537a":"code","9f5a6acb":"code","997eb957":"code","57d58f12":"code","e9950ccc":"markdown","22e79410":"markdown","841ec7fb":"markdown","c425c80b":"markdown","efc64a82":"markdown","0f282396":"markdown","6f2fe522":"markdown","774f720f":"markdown","8af1726b":"markdown","b64bd747":"markdown","00f93745":"markdown","12ef9b1c":"markdown","9ab7af67":"markdown","638c9410":"markdown","d839c7bc":"markdown","bc56fdee":"markdown","f79e9a6c":"markdown","672268f7":"markdown","45e85f6c":"markdown","aacfeb61":"markdown","5f0433c5":"markdown","7de1d078":"markdown","5dd3e5ce":"markdown","5e1ad3df":"markdown","3b3082f9":"markdown","ea901663":"markdown","0ffb29e0":"markdown","9a6f3ab4":"markdown","96bf22cc":"markdown","a1ed2f3a":"markdown","7c87f1c9":"markdown"},"source":{"f06e34a7":"import pandas as pd \nimport numpy as np\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\nfrom nltk.corpus import stopwords\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string","8bf61977":"train_df=pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest_df=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nsample_submission_df=pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","113dc3d2":"target=train_df['target'].value_counts()\nsns.barplot(target.index,target,edgecolor=(0,0,0),linewidth=1.5)\nplt.title('Comparing disaster tweets and non disaster tweets',fontsize=15)\nplt.xticks(fontsize=20)\nplt.ylabel('Samples',fontsize=15)","38a187db":"keyword=train_df['keyword'].value_counts()[:20]\nplt.figure(figsize=(10,7))\nsns.barplot(keyword.index,keyword.values,edgecolor=(0,0,0),linewidth=2)\nplt.title('Top 20 keywords',fontsize=20)\nplt.xticks(fontsize=20,rotation=270)\nplt.yticks(fontsize=20)\nplt.xlabel('Keywords',fontsize=20,color='blue')","99fd5d43":"location=train_df['location'].value_counts()[:20]\nplt.figure(figsize=(10,7))\nsns.barplot(location.index,location.values,edgecolor=(0,0,0),linewidth=2)\nplt.title('Top 20 Location',fontsize=20)\nplt.xticks(fontsize=20,rotation=270)\nplt.yticks(fontsize=20)\nplt.xlabel('Locations',fontsize=20,color='blue')","780aa188":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=train_df[train_df['target']==1]['text'].str.len()\nax1.hist(tweet_len,color='red')\nax1.set_title('disaster tweets')\ntweet_len=train_df[train_df['target']==0]['text'].str.len()\nax2.hist(tweet_len,color='blue')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Characters in tweets',fontsize=20)\n\nplt.show()","cffc40c4":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=train_df[train_df['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len,color='red')\nax1.set_title('disaster tweets')\ntweet_len=train_df[train_df['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='blue')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Words in a tweets',fontsize=20)\nplt.show()","371d19d2":"def lower(words):\n    return words.lower()\ntrain_df['text']=train_df['text'].apply(lambda x:lower(x))\n","d75cbb24":"def remove_numbers(words):\n    return re.sub(r'\\d+','',words)\ntrain_df['text']=train_df['text'].apply(lambda x: remove_numbers(x))\n","df898df5":" def remove_punctuation(words):\n    table=str.maketrans('','',string.punctuation)\n    return words.translate(table)\ntrain_df['text']=train_df['text'].apply(lambda x: remove_punctuation(x))\n","76be96d0":"train_df['text']=train_df['text'].apply(lambda x:word_tokenize(x))\n","67a2e6eb":"def remove_stopwords(words):\n    stop_words=set(stopwords.words('english'))\n    return [word for word in words if word not in stop_words]\ntrain_df['text']=train_df['text'].apply(lambda x: remove_stopwords(x))","17bcea0b":"def remove_links(words):\n    \n    return [re.sub(r'(https?:\/\/\\S+)','',word)for word in words]\ntrain_df['text']=train_df['text'].apply(lambda x:remove_links(x))\n                 ","f46ad223":"# def stemming(words):\n#     ps=PorterStemmer()\n#     return [ps.stem(word) for word in words]\n# train_df['text']=train_df['text'].apply(lambda x: stemming(x))\n","fdba5b1a":"def lemmatizing(words):\n    lemmatizer =WordNetLemmatizer()\n    return [lemmatizer.lemmatize(word) for word in words]\ntrain_df['text']=train_df['text'].apply(lambda x: lemmatizing(x))\n","bed4c98e":"def final_text(words):\n     return ' '.join(words)\ntrain_df['text']=train_df['text'].apply(lambda x:final_text(x))\n   ","8537537a":"from sklearn.feature_extraction.text import TfidfVectorizer\nvect=TfidfVectorizer(min_df=2\n                      ,max_features = None,analyzer=\"word\",  ngram_range=(1,3) # (1,6)\n                           ).fit(train_df['text'])\nx_train_vect=vect.transform(train_df['text'])\n","9f5a6acb":"import warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.linear_model import LogisticRegression\nmodel=LogisticRegression()\nmodel.fit(x_train_vect,train_df['target'])\n\n","997eb957":"predictions=model.predict(vect.transform(test_df['text']))","57d58f12":"\nsample_submission_df['target']=predictions\nsample_submission_df.to_csv('submission.csv',index=False)\n","e9950ccc":"We are importing libraries nltk,numpy,pandas and sklearn.\nThe Natural Language ToolKit is one of the best-known and most-used NLP libraries, useful for all sorts of tasks from t tokenization, stemming, tagging, parsing, and beyond.","22e79410":"Stop words are the most common words in a language like \u201cthe\u201d, \u201ca\u201d, \u201cat\u201d, \u201cfor\u201d, \u201cabove\u201d, \u201con\u201d, \u201cis\u201d, \u201call\u201d. These words do not provide any meaning and are usually removed from texts. We can remove these stop words using nltk library","841ec7fb":"## Visualization","c425c80b":"## Removing links","efc64a82":"#                                   NLP with disaster tweets","0f282396":"## Predictions","6f2fe522":"Text Processing is one of the most common task in many ML applications.\n","774f720f":"Stemming usually refers to normalizing words into its base form or root form.","8af1726b":"Natural language processing (NLP) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.","b64bd747":"## Lemmatizing","00f93745":"There are three files train.csv, test.csv and sample_submission.csv","12ef9b1c":"## Remove punctuation","9ab7af67":"The text column in our dataset contains hyperlinks, punctuation, stop words, numbers. So we have to remove all these using text processing. ","638c9410":"## Convert text to lowercase\n","d839c7bc":"Tokenization is the first step in NLP. It is the process of breaking strings into tokens which in turn are small structures or units. Tokenization involves three steps which are breaking a complex sentence into words, understanding the importance of each word with respect to the sentence and finally produce structural description on an input sentence.","bc56fdee":"### Twitter is an American microblogging and social networking service on which users post and interact with messages known as \"tweets\". ","f79e9a6c":"## Removing Stop words","672268f7":"## Tokenization","45e85f6c":"## Machine Learning ","aacfeb61":"Each sample in the train and test set has the following information:\n\n* The text of a tweet\n* A keyword from that tweet (although this may be blank!)\n* The location the tweet was sent from (may also be blank)","5f0433c5":"The dataset contains 10,000 tweets that are hand classified.","7de1d078":"## Submission","5dd3e5ce":"## Stemming ","5e1ad3df":"The words need to be encoded as integers or floating point values for use as input to a machine learning algorithm, called feature extraction (or vectorization).\n\nThe scikit-learn library offers easy-to-use tools to perform both tokenization and feature extraction of your text data.","3b3082f9":"# Importing Libraries","ea901663":"In simpler terms, it is the process of converting a word to its base form. The difference between stemming and lemmatization is, lemmatization considers the context and converts the word to its meaningful base form, whereas stemming just removes the last few characters, often leading to incorrect meanings and spelling errors.","0ffb29e0":"# Text Processing","9a6f3ab4":"## Remove numbers","96bf22cc":"### Here we are predicting whether a given tweet is about a real disaster or not.","a1ed2f3a":"# Loading dataset","7c87f1c9":"### Columns\n* id - a unique identifier for each tweet\n* text - the text of the tweet\n* location - the location the tweet was sent from (may be blank)\n* keyword - a particular keyword from the tweet (may be blank)\n* target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)"}}