{"cell_type":{"619f6d86":"code","696473f0":"code","615e5811":"code","953a45b4":"code","36b3e17e":"code","cbbcca6d":"code","ef41cfae":"code","26259f2a":"code","f9e7a72b":"code","a37ba2be":"code","2e38c753":"code","846c4a1a":"code","caddaea3":"code","ed6b1c85":"code","4339b11d":"code","52a4d957":"code","73cc3e63":"markdown","4ebc9faa":"markdown","e1c3bb6c":"markdown","12f12f9c":"markdown","0af44b4d":"markdown","b58861db":"markdown","0ac847d9":"markdown","3392c23a":"markdown"},"source":{"619f6d86":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","696473f0":"## Libraries needed for handling img data ##\nimport glob\nfrom PIL import Image\nfrom keras.preprocessing.image import load_img,img_to_array\nimport matplotlib.pyplot as plt","615e5811":"dataset_dir = '..\/input\/anime-faces\/data\/*.png'\ninput_dim = (64,64,3)\n\ncount = 0\nimages = None\nfor image_path in glob.glob(dataset_dir):\n    if count == 5000:\n        break\n    try:\n        loaded_image = load_img(image_path,target_size=(input_dim[0],input_dim[1]))\n        loaded_image = img_to_array(loaded_image)\n        loaded_image = loaded_image \/ 127.5 - 1\n        loaded_image = loaded_image.astype(np.float32)\n        loaded_image = np.expand_dims(loaded_image,axis=0)\n        if images is None:\n            images = loaded_image\n        else:\n            images = np.concatenate([images,loaded_image],axis=0)\n    except Exception as e:\n        print(\"Error:{}\".format(e))\n    count += 1\n        \nprint(images.shape)","953a45b4":"## Visualize training anime images\nrandom_idx = np.random.randint(0,len(images),36)\n\nplt.figure(figsize=(12,12))\nfor i in range(len(random_idx)):\n    plt.subplot(6,6,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid('off')\n    plt.imshow(images[random_idx[i]])\n    \nplt.tight_layout()\nplt.show()","36b3e17e":"## Libraries needed for building GAN model\nfrom keras.layers import Input,Dense,Reshape,Flatten,Dropout,Activation,UpSampling2D,Conv2D,MaxPooling2D,LeakyReLU,BatchNormalization\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.initializers import RandomNormal\nfrom keras import backend as K\n\n## HyYPERPARAMETERS related to training process\n# input_dim = (64,64,3)\nz_dim = 512\nBATCH_SIZE = 256\nepochs = 220\nG_lr = 0.0004\nD_lr = 0.0004\nweight_initializer = RandomNormal(mean=0.,stddev=0.1)\n\n## G\ninitial_image_size = (4,4,128)\nG_filters = [128,64,32,3]\nG_kernel_sizes = [5,5,5,5]\nG_strides = [1,1,1,1]\n\n## D\nD_filters = [16,32,64,128]\nD_kernel_sizes = [5,5,3,3]\nD_strides = [1,1,1,1]","cbbcca6d":"def build_generator():\n    X = Input(shape=[z_dim])\n    \n    H = Dense(1024)(X)\n    H = LeakyReLU(alpha=0.2)(H)\n    \n    H = Dense(np.prod(initial_image_size))(H)\n    H = BatchNormalization()(H)\n    H = LeakyReLU(alpha=0.2)(H)\n    \n    H = Reshape(initial_image_size)(H)\n    \n    for i in range(len(G_filters)):\n        H = UpSampling2D(size=(2,2))(H)\n        H = Conv2D(filters=G_filters[i],kernel_size=G_kernel_sizes[i],strides=G_strides[i],padding='same',kernel_initializer=weight_initializer)(H)\n        \n        if i < len(G_filters)-1:\n            H = BatchNormalization()(H)\n            H = LeakyReLU(alpha=0.2)(H)\n        else:\n            Y = Activation('tanh')(H)\n        \n    generator = Model(X,Y)\n    \n    return generator","ef41cfae":"def build_discriminator():\n    X = Input(shape=input_dim)\n    \n    H = X\n    for i in range(len(D_filters)):\n        H = Conv2D(filters=D_filters[i],kernel_size=D_kernel_sizes[i],strides=D_strides[i],padding='same',kernel_initializer=weight_initializer)(H)\n        H = LeakyReLU(alpha=0.2)(H)\n        H = MaxPooling2D(pool_size=(2,2))(H)\n        \n    H = Flatten()(H)\n    H = Dense(1024)(H)\n    H = LeakyReLU(alpha=0.2)(H)\n    \n    H = Dense(1)(H)\n    Y = Activation('sigmoid')(H)\n    \n    discriminator = Model(X,Y)\n    \n    return discriminator","26259f2a":"def make_trainable(network,value):\n    network.trainable = value\n    for layer in network.layers:\n        layer.trainable = value","f9e7a72b":"G = build_generator()\nD = build_discriminator()\n\nD_opt = Adam(lr=D_lr)\nD.compile(loss='binary_crossentropy',optimizer=D_opt)\nD.summary()","a37ba2be":"combined_opt = Adam(lr=G_lr)\n\n## first, freeze Discriminator network\nmake_trainable(D,False)\n\nZ = Input(shape=[z_dim])\ngenerated = G(Z)\nvalid = D(generated)\n\ncombined = Model(Z,valid)\ncombined.compile(loss='binary_crossentropy',optimizer=combined_opt)\n\n## unfreeze Discriminator because we have to train Discriminator too\nmake_trainable(D,True)\n\ncombined.summary()","2e38c753":"## Functions related to Visualizing\n\ndef denormalize(img):\n    img = (img+1)*127.5\n    return img.astype(np.uint8)\n\ndef show_generated():\n    noise = np.random.normal(0,1,size=(16,z_dim))\n    generated = G.predict(noise)\n    \n    plt.figure(figsize=(12,12))\n    for i in range(16):\n        plt.subplot(4,4,i+1)\n        plt.grid(False)\n        plt.xticks([])\n        plt.yticks([])\n        ##plt.imshow(denormalize(generated[i]))\n        plt.imshow(generated[i])\n    plt.tight_layout()\n    plt.show()","846c4a1a":"# dictionary for recording losses\nlosses = {\"d\":[],\"g\":[]}\n\nfor epoch in range(epochs):\n\n    steps = int(images.shape[0]\/BATCH_SIZE)\n    for step in range(steps):\n        z_noise = np.random.normal(0,1,size=(BATCH_SIZE,z_dim))\n        fake_img = G.predict_on_batch(z_noise)\n        \n        ## Train D ##\n        real_img = images[step*BATCH_SIZE:(step+1)*BATCH_SIZE]\n        ones = np.ones((BATCH_SIZE,))\n        zeros = np.zeros((BATCH_SIZE,))\n        \n        D_loss_real = D.train_on_batch(real_img,ones)\n        D_loss_fake = D.train_on_batch(fake_img,zeros)\n        D_loss = (D_loss_real + D_loss_fake) \/ 2\n        \n        losses[\"d\"].append(D_loss)\n\n        ## Train Combined ##\n        G_loss = combined.train_on_batch(z_noise,ones)\n        losses[\"g\"].append(G_loss)\n        \n    print(\"Epoch %d     [D_LOSS : %.8f]    [G_LOSS : %.8f]\"%(epoch+1,D_loss,G_loss))\n    if (epoch+1) % 25 ==0:\n        show_generated()","caddaea3":"plt.figure(figsize=(8,12))\n\nplt.subplot(3,1,1)\nplt.title(\"Discriminator Loss\")\nplt.plot(range(len(losses[\"d\"])),losses[\"d\"],c=\"b\")\nplt.ylim(0,1)\n\nplt.subplot(3,1,2)\nplt.title(\"Generator Loss\")\nplt.plot(range(len(losses[\"g\"])),losses[\"g\"],c=\"r\")\nplt.ylim(0,1)\n\nplt.subplot(3,1,3)\nplt.title(\"Combined Plot\")\nplt.plot(range(len(losses[\"d\"])),losses[\"d\"],c=\"b\",label=\"D_Loss\")\nplt.plot(range(len(losses[\"g\"])),losses[\"g\"],c=\"r\",label=\"G_Loss\")\nplt.legend()\nplt.ylim(0,1)\n\nplt.tight_layout()\nplt.show()","ed6b1c85":"show_generated()","4339b11d":"noise = np.random.normal(0,1,size=(16,z_dim))\ngenerated = G.predict(noise)\n    \nplt.figure(figsize=(10,10))\nfor i in range(16):\n    plt.subplot(4,4,i+1)\n    plt.grid(False)\n    plt.xticks([])\n    plt.yticks([])\n    #plt.imshow(generated[i])\n    plt.imshow(denormalize(generated[i]))\nplt.tight_layout()\nplt.show()","52a4d957":"noise = np.random.normal(0,1,size=(16,z_dim))\ngenerated = G.predict(noise)\n    \nplt.figure(figsize=(10,10))\nfor i in range(16):\n    plt.subplot(4,4,i+1)\n    plt.grid(False)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(generated[i],cmap='binary')\nplt.tight_layout()\nplt.show()","73cc3e63":"**[1] Load anime-faces data and make them into training dataset**","4ebc9faa":"**[4] Train Discriminator and Generator**","e1c3bb6c":"1. **Make train dataset (named \"images\") using anime-faces dataset from kaggle**\n2. **Define Hyperparameters**\n3. **Make GAN model (Generator, Discriminator, adversarial model)**\n4. **Train Discriminator and Adversarial model**\n5. **Visualize images made by Generator**","12f12f9c":"**Compile Adversarial model**","0af44b4d":"**Plot of Loss Fluctuations**","b58861db":"**Compile Discriminator model**","0ac847d9":"**[3] Make Generator (\"G\") , Discriminator (\"D\") , and Adversarial model (Generator connected to Discriminator) (\"combined\")**","3392c23a":"**[2] Define Hyperparameters and parameters related to GAN model**"}}