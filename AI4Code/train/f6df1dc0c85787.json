{"cell_type":{"f887a8ab":"code","b56f0dd5":"code","f7d36980":"code","62c60dac":"code","e8374cca":"code","be0a8e1a":"code","31daf499":"code","386d0c00":"code","cddf315f":"code","9b9c44e7":"code","7771d609":"code","40696d2c":"code","5eac5e4e":"code","a7173bdc":"code","5febf2cc":"code","89b622d3":"code","932fdea2":"code","aed0fa77":"code","0fbdb3e8":"code","980d5c42":"code","407b8f90":"code","8052a98e":"code","f90fcda9":"code","0c2b0d47":"code","784cf205":"code","f6ea92f0":"code","3e01f4d0":"code","08eb979c":"code","83c7bb4d":"code","a399a673":"code","daf4bcc4":"code","6a9e1bd3":"code","eb5f92b7":"code","8b8135cd":"code","9b763aa2":"code","8ebb3b37":"code","7b3f7c7f":"code","d6c69939":"code","b69a2f60":"code","8ac39ef2":"code","c3364208":"code","adb85683":"code","72372b64":"code","b4666d86":"code","9514c2c0":"code","9d9261c5":"code","39de6a3c":"code","40d84e00":"code","88ef28d6":"code","790ce314":"code","68da30b7":"code","6f8fba31":"code","b6167acb":"code","0bd3c71c":"code","a6ed9983":"code","f96911d4":"code","f453048d":"code","82119c44":"code","a48d8a1f":"code","c332895d":"code","a2e08f27":"code","2515f160":"code","dd04ab03":"code","e30c3092":"code","7f5ff459":"code","bf3e977d":"code","d7cca7cb":"code","dac15ba7":"code","efb90b6c":"code","764918a5":"code","2e2e8f76":"code","5ccf88d6":"code","0fab768b":"code","ec08e570":"code","5194206c":"code","a09128fd":"code","a2176a1c":"code","5fcb8bc9":"code","8ac07055":"code","86ace09c":"code","34eddd24":"code","c8223a19":"code","ca7d47af":"code","a2fcdf07":"code","ca10f706":"code","53fe5af8":"code","f5966548":"code","8f7d36a5":"code","d5c36a3d":"code","15fda27c":"code","1957da38":"code","7eae90c4":"code","888c48ed":"code","ed084d39":"code","e0b60d00":"code","b67a1a58":"code","15b9ed9b":"code","fe85fb31":"code","587abb12":"code","54350f88":"code","e724d67b":"code","fbed2ab4":"code","65ffdce9":"code","bfd5a7e8":"code","6424f625":"code","615ebae0":"code","440ced5d":"code","210b0f98":"code","ba74149f":"code","9e172ad6":"code","df6fc9b2":"code","40568a73":"code","0d6ea3d5":"code","af656983":"code","0c6c7029":"code","2c985c70":"code","b685f1a8":"code","dcc9d5ce":"code","914374b9":"code","2698f3d9":"code","412624b5":"code","708daddb":"code","0b37688e":"code","442c64f4":"code","2dd5c093":"code","97f11d60":"code","dd225088":"code","86733a9e":"code","2f2bbab8":"code","3580e64e":"code","7463d855":"code","d0276420":"code","eb584d63":"code","2f5f0d77":"markdown","2016eb39":"markdown","8b0b7b86":"markdown","ee12a655":"markdown","69ed3d7a":"markdown","60d49f46":"markdown","80346d09":"markdown","1d953125":"markdown","a76aef99":"markdown","684b9d6f":"markdown","a0840e0b":"markdown","8d4f5a4b":"markdown","5d41d9de":"markdown","d8d82656":"markdown","823d0ce7":"markdown","8690ec76":"markdown","68260ab3":"markdown","b92c8f6d":"markdown","eca475e1":"markdown","caaffcfc":"markdown","73c8f7c4":"markdown","0d232d3d":"markdown","6355fd9c":"markdown","274f2f7d":"markdown","60b7661b":"markdown","62a0d0c0":"markdown","ae96de87":"markdown","2a54b230":"markdown","fa0a59b6":"markdown","49833e97":"markdown","4c376558":"markdown","91369df1":"markdown","cf116f02":"markdown","3462e311":"markdown","6e6dd00b":"markdown","432dbd10":"markdown","88e4e244":"markdown","81a48f0a":"markdown","c690aea3":"markdown","351ceeee":"markdown","900dec94":"markdown","908cb181":"markdown","241b6ef7":"markdown","00ae759d":"markdown","91b5c70b":"markdown","8bc9f95f":"markdown","638ed9e4":"markdown","661226db":"markdown","e96f11b2":"markdown","b9b64798":"markdown","04069b16":"markdown","65b2a7b7":"markdown","f18b3de2":"markdown","11abff97":"markdown","3dcee983":"markdown","4553cb61":"markdown","950eb37f":"markdown","187cae1a":"markdown","3947a2fc":"markdown","9aaabf91":"markdown","752ac3e2":"markdown","57d568d9":"markdown","d744416c":"markdown","ed59d3aa":"markdown","da825aaf":"markdown","a45aea84":"markdown"},"source":{"f887a8ab":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfrom IPython.display import IFrame\nprint(\"Telco Customer Churn Analysis\")","b56f0dd5":"IFrame(\"https:\/\/public.tableau.com\/views\/ChurnesvsNon-Churners\/Dashboard2?:embed=y&:showVizHome=no\", width=825, height=440)","f7d36980":"IFrame(\"https:\/\/public.tableau.com\/views\/ChurnAnalysisbyGender\/Dashboard2?:embed=y&:showVizHome=no\", width=825, height=470)","62c60dac":"IFrame(\"https:\/\/public.tableau.com\/views\/ChurnAnalysisbySeniorCitizen\/Citizen-D?embed=y&:showVizHome=no\", width=825, height=440)","e8374cca":"IFrame(\"https:\/\/public.tableau.com\/views\/ChurnAnalysisbyDependents\/DependentsD?embed=y&:showVizHome=no\", width=825, height=455)","be0a8e1a":"IFrame(\"https:\/\/public.tableau.com\/views\/ChurnbyTechSupportVsMultilines\/Dashboard4?:embed=y&:showVizHome=no\", width=825, height=800)","31daf499":"IFrame(\"https:\/\/public.tableau.com\/views\/TenureVsTotalCharges\/Dashboard1?:embed=y&:showVizHome=no\", width=825, height=775)","386d0c00":"IFrame(\"https:\/\/public.tableau.com\/views\/ChurnAnalysisbyPaymentMethodv1\/Dashboard1?:embed=y&:showVizHome=no\", width=800, height=775)","cddf315f":"IFrame(\"https:\/\/public.tableau.com\/views\/ChurnbyMonthlychargesVsTenure\/Dashboard3?:embed=y&:showVizHome=no\", width=800, height=800)","9b9c44e7":"IFrame(\"https:\/\/public.tableau.com\/views\/ChurnAnalysisbyPaymentMethods\/Dashboard2?:embed=y&:showVizHome=no\", width=800, height=600)","7771d609":"IFrame(\"https:\/\/public.tableau.com\/views\/ChurnAnalysis-CorrelationMatrix\/ChurnAnalysis-CorrelationMatrix_1?:embed=y&:showVizHome=no\", width=800, height=500)","40696d2c":"IFrame(\"https:\/\/public.tableau.com\/views\/ChurnAnalysisbyServiceProvided\/Dashboard3?:embed=y&:showVizHome=no\", width=800, height=800)","5eac5e4e":"IFrame(\"https:\/\/public.tableau.com\/views\/TotalCharges-Null\/TotalCharges-Null?:embed=y&:display_count=yes : embed=y&:showVizHome=no\", width=825, height=385)","a7173bdc":"df = pd.read_csv(\"..\/input\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\ndf.columns","5febf2cc":"df['TotalCharges'].fillna(0, inplace=True)\ndf.isnull().sum()","89b622d3":"import pandas as pd\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt \nplt.rc(\"font\", size=14)\nimport seaborn as sns\nsns.set(style=\"white\")\nsns.set(style=\"whitegrid\", color_codes=True)\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix","932fdea2":"df = pd.read_csv(\"..\/input\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\ndf.columns","aed0fa77":"print(df.shape)","0fbdb3e8":"df.head()","980d5c42":"df.info()","407b8f90":"df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')","8052a98e":"df['TotalCharges'].fillna(0, inplace=True)\ndf.isnull().sum()","f90fcda9":"lb_make = LabelEncoder()\ndf['Churn_Bi'] = lb_make.fit_transform(df[\"Churn\"])\ndf[[\"Churn\", \"Churn_Bi\"]].head()","0c2b0d47":"dataset=df.drop('customerID', axis=1)\ndataset=dataset.drop('Churn', axis=1)\nprint(dataset.shape)\ndataset.head()","784cf205":"colToNorm = ('TotalCharges','MonthlyCharges','tenure')\nsubsetToNormalize = dataset[list(colToNorm)]\nprint(subsetToNormalize.shape)","f6ea92f0":"subsetToKeep = dataset.drop(list(colToNorm), axis=1)","3e01f4d0":"print(subsetToKeep.shape)","08eb979c":"preObj = preprocessing.StandardScaler().fit(subsetToNormalize)\npreObj","83c7bb4d":"preObj.mean_\npreObj.scale_","a399a673":"preObj = preObj.transform(subsetToNormalize)","daf4bcc4":"preObj = pd.DataFrame(preObj, columns=list(('TotalCharges','MonthlyCharges','tenure')))\nsubsetToKeep = pd.DataFrame(subsetToKeep)","6a9e1bd3":"dataset = pd.concat([preObj.reset_index(drop=True), subsetToKeep.reset_index(drop=True)], axis=1)","eb5f92b7":"cat_vars=['gender', 'SeniorCitizen',\n       'Partner', 'Dependents', 'PhoneService', 'MultipleLines',\n       'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n       'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract',\n       'PaperlessBilling', 'PaymentMethod']","8b8135cd":"for var in cat_vars:\n    cat_list='var' + '_' + var\n    cat_list=pd.get_dummies(dataset[var],prefix=var)\n    dataset_dum=dataset.join(cat_list)\n    dataset=dataset_dum","9b763aa2":"dataset.columns","8ebb3b37":"dataset_vars=dataset.columns.values.tolist()\nto_keep=[i for i in dataset_vars if i not in cat_vars]","7b3f7c7f":"dataset_v2=dataset[to_keep]\ndataset_v2.columns.values","d6c69939":"dataset_v2.shape","b69a2f60":"X = dataset_v2.drop(['Churn_Bi'], axis=1)\ny = dataset_v2[['Churn_Bi']]\ndataset_v2.shape","8ac39ef2":"X_train, X_test, y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=3457)","c3364208":"classifier = LogisticRegression()\nclassifier.fit(X_train, y_train.values.ravel())","adb85683":"y_pred = classifier.predict(X_test)\ncf_mx = confusion_matrix(y_test, y_pred)\nprint(cf_mx)","72372b64":"print('Score of logistic regression classifier on test set: {:.2f}'.format(classifier.score(X_test, y_test)))","b4666d86":"print(classification_report(y_test, y_pred))","9514c2c0":"import pandas as pd\nimport numpy as np\nimport random\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import datasets, linear_model\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score,roc_curve\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\n#from sklearn.cross_validation import train_test_split\nfrom sklearn.model_selection import KFold # import KFold\n#from sklearn.cross_validation import cross_val_score, cross_val_predict\nfrom sklearn.model_selection import cross_validate\n\nimport matplotlib.pyplot as plt \nplt.rc(\"font\", size=14)\nimport seaborn as sns\nsns.set(style=\"white\")\nsns.set(style=\"whitegrid\", color_codes=True)","9d9261c5":"df = pd.read_csv(\"..\/input\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\ndf.columns","39de6a3c":"df.head()","40d84e00":"df['MultipleLines']=df['MultipleLines'].replace(['No phone service'], 'No')\ndf['OnlineSecurity']=df['OnlineSecurity'].replace(['No internet service'], 'No')\ndf['OnlineBackup']=df['OnlineBackup'].replace(['No internet service'], 'No')\ndf['DeviceProtection']=df['DeviceProtection'].replace(['No internet service'], 'No')\ndf['TechSupport']=df['TechSupport'].replace(['No internet service'], 'No')\ndf['StreamingTV']=df['StreamingTV'].replace(['No internet service'], 'No')\ndf['StreamingMovies']=df['StreamingMovies'].replace(['No internet service'], 'No')","88ef28d6":"df['PaymentMethod_echeck'] = np.where(df['PaymentMethod'] == \"Electronic check\",1,0)\ndf['PaymentMethod_Mailed'] = np.where(df['PaymentMethod'] == \"Mailed check\",1,0)\ndf['PaymentMethod_Transfer'] = np.where(df['PaymentMethod'] == \"Bank transfer (automatic)\",1,0)\ndf['PaymentMethod_Crdt'] = np.where(df['PaymentMethod'] == \"Credit card (automatic)\",1,0)","790ce314":"df['MonthlyContract'] = np.where(df['Contract'] == \"Month-to-month\",1,0)\ndf['OneYearContract'] = np.where(df['Contract'] == \"One year\",1,0)\ndf['TwoYearContract'] = np.where(df['Contract'] == \"Two year\",1,0)","68da30b7":"df['InternetService_No'] = np.where(df['InternetService'] == \"No\",0,0)\ndf['InternetService_Fibre'] = np.where(df['InternetService'] == \"Fiber optic\",1,0)\ndf['InternetService_DSL'] = np.where(df['InternetService'] == \"DSL\",1,0)","6f8fba31":"lb_make = LabelEncoder()\ndf['Churn'] = lb_make.fit_transform(df[\"Churn\"])\ndf['MultipleLines'] = lb_make.fit_transform(df[\"MultipleLines\"])\ndf['OnlineSecurity'] = lb_make.fit_transform(df[\"OnlineSecurity\"])\ndf['gender'] = lb_make.fit_transform(df[\"gender\"])\ndf['SeniorCitizen'] = lb_make.fit_transform(df[\"SeniorCitizen\"])\ndf['Partner'] = lb_make.fit_transform(df[\"Partner\"])\ndf['Dependents'] = lb_make.fit_transform(df[\"Dependents\"])\ndf['PhoneService'] = lb_make.fit_transform(df[\"PhoneService\"])\ndf['OnlineBackup'] = lb_make.fit_transform(df[\"OnlineBackup\"])\ndf['DeviceProtection'] = lb_make.fit_transform(df[\"DeviceProtection\"])\ndf['TechSupport'] = lb_make.fit_transform(df[\"TechSupport\"])\ndf['StreamingTV'] = lb_make.fit_transform(df[\"StreamingTV\"])\ndf['StreamingMovies'] = lb_make.fit_transform(df[\"StreamingMovies\"])\ndf['PaperlessBilling'] = lb_make.fit_transform(df[\"PaperlessBilling\"])","b6167acb":"dataset0=df.drop('InternetService', axis=1)\ndataset0 = dataset0.drop('Contract',axis=1)\ndataset0 = dataset0.drop('PaymentMethod',axis=1)\ndataset0 = dataset0.drop('customerID',axis=1)","0bd3c71c":"dataset0['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\ndataset0['TotalCharges'].fillna(0, inplace=True)","a6ed9983":"dataset0.columns","f96911d4":"dataset0.shape","f453048d":"dataset0.info()","82119c44":"from imblearn.over_sampling import SMOTE\nX = dataset0.loc[:, dataset0.columns != 'Churn']\ny = dataset0.loc[:, dataset0.columns == 'Churn']\ny=pd.DataFrame(y)\nos = SMOTE(random_state=0)\ncolumns = X.columns\nos_data_X,os_data_y=os.fit_sample(X, y.values.ravel())\nos_data_X = pd.DataFrame(data=os_data_X,columns=columns )\nos_data_y= pd.DataFrame(data=os_data_y,columns=['Churn'])","a48d8a1f":"print(\"length of oversampled data is \",len(os_data_X))\nprint(\"Number of No Churn in oversampled data\",len(os_data_y[os_data_y['Churn']==0]))\nprint(\"Number of Churn\",len(os_data_y[os_data_y['Churn']==1]))\nprint(\"Proportion of no subscription data in oversampled data is \",len(os_data_y[os_data_y['Churn']==0])\/len(os_data_X))\nprint(\"Proportion of subscription data in oversampled data is \",len(os_data_y[os_data_y['Churn']==1])\/len(os_data_X))","c332895d":"os_data_Xy = pd.concat([os_data_X.reset_index(drop=True), os_data_y.reset_index(drop=True)], axis=1)","a2e08f27":"os_data_Xy.columns","2515f160":"os_data_Xy.shape","dd04ab03":"colToCluster = ('InternetService_Fibre','MonthlyContract','OneYearContract', 'TwoYearContract', 'Churn')","e30c3092":"subset4Cluster = os_data_Xy[list(colToCluster)]","7f5ff459":"from scipy import sparse\nfrom sklearn.base import BaseEstimator, ClusterMixin\nfrom sklearn.externals.joblib import Parallel, delayed\nfrom sklearn.utils import check_random_state\nfrom sklearn.utils.validation import check_array","bf3e977d":"from kmodes import kmodes\nfrom kmodes.kmodes import KModes\nfrom kmodes.kprototypes import KPrototypes","d7cca7cb":"km = kmodes.KModes(n_clusters=6, init='Cao', n_init=5, verbose=0)","dac15ba7":"clusters = km.fit_predict(subset4Cluster)","efb90b6c":"subset4Cluster=pd.DataFrame(subset4Cluster)","764918a5":"subset4Cluster['clusters'] = clusters","2e2e8f76":"ct = pd.crosstab(subset4Cluster['clusters'], subset4Cluster['Churn'])","5ccf88d6":"print(ct)","0fab768b":"os_data_Xy['clusters']=clusters","ec08e570":"def feature_sel(dataset,model,nb_feat):\n    model = LogisticRegression()\n    rfe = RFE(model, nb_feat)\n    #os_data_y= pd.DataFrame(data=os_data_y,columns=['Churn'])\n    os_data_y=dataset.loc[:, dataset.columns == 'Churn']\n    os_data_X=dataset.loc[:, dataset.columns != 'Churn']\n    rfe = rfe.fit(os_data_X, os_data_y.values.ravel())\n    final_vars=os_data_X.columns.values.tolist()\n    j=0\n    final_list=[]\n    for i in final_vars:\n        if rfe.support_[j] == True :\n            #print(list_2[j])\n            final_list.append(i)    \n        j = j + 1\n    return final_list","5194206c":"def run_model(X_f,y_f,model, alg_name, plot_index):\n    #X_f = dataset_f.loc[:, dataset_f.columns != 'Churn']\n    #y_f = dataset_f.loc[:, dataset_f.columns == 'Churn']\n    X_train_f, X_test_f, y_train_f,y_test_f = train_test_split(X_f,y_f,test_size=0.2,random_state=3457)\n    model.fit(X_train_f, y_train_f.values.ravel())\n    \n    if alg_name == \"Logistic Regression\":\n        THRESHOLD = 0.4\n        y_pred_f = np.where(model.predict_proba(X_test_f)[:,1] > THRESHOLD, 1, 0)\n    else :\n        y_pred_f = model.predict(X_test_f)\n    #print('Score of ' + alg_name + ' on test set: {:.4f}'.format(model.score(X_test_f, y_test_f)))\n    score_f= model.score(X_test_f, y_test_f)\n    cf_mx1_f = confusion_matrix(y_test_f, y_pred_f)\n    #print(cf_mx1_f)\n    True_Pred_f = cf_mx1_f[0][0] + cf_mx1_f[1][1]\n    Total_Pred_f = cf_mx1_f[0][0] + cf_mx1_f[1][1] + cf_mx1_f[0][1] + cf_mx1_f[1][0]\n    acc= True_Pred_f\/Total_Pred_f\n    #print('Accuracy of ' + alg_name + ' on test set: {:.4f}'.format(acc))\n    return score_f,acc","a09128fd":"#from copy import deepcopy\n#os_data_X1 = deepcopy(os_data_X)\n#os_data_y1 = deepcopy(os_data_y)","a2176a1c":"from sklearn.feature_selection import RFE","5fcb8bc9":"from sklearn.linear_model import LogisticRegression\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n","8ac07055":"alg_name = \"Logistic Regression\"\nmodel = LogisticRegression(random_state=3457)\nscore_l=[]\nacc_l=[]\nfor i in range(3,10):\n    final_list=feature_sel(os_data_Xy,model,i)\n    os_data_y=os_data_Xy.loc[:, os_data_Xy.columns == 'Churn']\n    os_data_X=os_data_Xy[final_list]\n    #final_list\n    score_f,acc_f = run_model(os_data_X,os_data_y,model, \"Logistic Regression\", 2)\n    score_l.append(score_f)\n    acc_l.append(acc_f)\n    ind_score= score_l.index(max(score_l))\n    \nprint('Number of feature:' + str(ind_score+3))\nprint('Score of ' + alg_name + ' on test set: {:.4f}'.format(score_l[ind_score]))\nprint('Accuracy of ' + alg_name + ' on test set: {:.4f}'.format(acc_l[ind_score]))\nfinal_list=feature_sel(os_data_Xy,model,ind_score+3)\nfinal_list\nos_data_lg_X=os_data_X[final_list]","86ace09c":"model = LogisticRegression(random_state=3457,C=0.01, penalty='l1', tol=0.01, solver='liblinear')\nalg_name = \"Logistic Regression\"\n#model_lg = LogisticRegression(random_state=3457)\nscore_l=[]\nacc_l=[]\nfor i in range(3,10):\n    final_list=feature_sel(os_data_Xy,model,i)\n    os_data_y=os_data_Xy.loc[:, os_data_Xy.columns == 'Churn']\n    os_data_X=os_data_Xy[final_list]\n    #final_list\n    score_f,acc_f = run_model(os_data_X,os_data_y,model, \"Logistic Regression\", 2)\n    score_l.append(score_f)\n    acc_l.append(acc_f)\n    ind_score= score_l.index(max(score_l))\n    \nprint('Number of feature:' + str(ind_score+3))\nprint('Score of ' + alg_name + ' on test set: {:.4f}'.format(score_l[ind_score]))\nprint('Accuracy of ' + alg_name + ' on test set: {:.4f}'.format(acc_l[ind_score]))\nfinal_list=feature_sel(os_data_Xy,model,ind_score+3)\nfinal_list\nos_data_lgL1_X=os_data_X[final_list]","34eddd24":"from sklearn.ensemble import RandomForestClassifier\nfrom numpy.core.umath_tests import inner1d","c8223a19":"alg_name = \"Random Forest\"\nmodel = RandomForestClassifier(random_state=3457)\nscore_l=[]\nacc_l=[]\nfor i in range(3,10):\n    final_list=feature_sel(os_data_Xy,model,i)\n    os_data_y=os_data_Xy.loc[:, os_data_Xy.columns == 'Churn']\n    os_data_X=os_data_Xy[final_list]\n    #final_list\n    score_f,acc_f = run_model(os_data_X,os_data_y,model, alg_name, 2)\n    score_l.append(score_f)\n    acc_l.append(acc_f)\n    ind_score= score_l.index(max(score_l))\n    \nprint('Number of feature:' + str(ind_score+3))\nprint('Score of ' + alg_name + ' on test set: {:.4f}'.format(score_l[ind_score]))\nprint('Accuracy of ' + alg_name + ' on test set: {:.4f}'.format(acc_l[ind_score]))\nfinal_list=feature_sel(os_data_Xy,model,ind_score+3)\nfinal_list\nos_data_rf_X=os_data_X[final_list]","ca7d47af":"final_list=feature_sel(os_data_Xy,model,9)\nos_data_y=os_data_Xy.loc[:, os_data_Xy.columns == 'Churn']\nos_data_X=os_data_Xy[final_list]\n#final_list\nscore_f,acc_f = run_model(os_data_X,os_data_y,model, alg_name, 2)\nprint(model.feature_importances_)","a2fcdf07":"from sklearn import tree","ca10f706":"alg_name = \"Decision Tree\"\nmodel = tree.DecisionTreeClassifier(criterion='entropy', max_depth=3,random_state=3458)\nscore_l=[]\nacc_l=[]\nfor i in range(3,10):\n    final_list=feature_sel(os_data_Xy,model,i)\n    os_data_y=os_data_Xy.loc[:, os_data_Xy.columns == 'Churn']\n    os_data_X=os_data_Xy[final_list]\n    #final_list\n    score_f,acc_f = run_model(os_data_X,os_data_y,model, alg_name, 2)\n    score_l.append(score_f)\n    acc_l.append(acc_f)\n    ind_score= score_l.index(max(score_l))\n    \nprint('Number of feature:' + str(ind_score+3))\nprint('Score of ' + alg_name + ' on test set: {:.4f}'.format(score_l[ind_score]))\nprint('Accuracy of ' + alg_name + ' on test set: {:.4f}'.format(acc_l[ind_score]))\nfinal_list=feature_sel(os_data_Xy,model,ind_score+3)\nfinal_list\nos_data_dt_X=os_data_X[final_list]","53fe5af8":"final_list=feature_sel(os_data_Xy,model,3)\nos_data_y=os_data_Xy.loc[:, os_data_Xy.columns == 'Churn']\nos_data_X=os_data_Xy[final_list]\n#final_list\nscore_f,acc_f = run_model(os_data_X,os_data_y,model, alg_name, 2)\nprint(model.feature_importances_)","f5966548":" X_train, X_test, y_train,y_test = train_test_split(os_data_dt_X,os_data_y,test_size=0.2,random_state=3457)","8f7d36a5":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(n_estimators=5,random_state=3457,max_depth=4)\nmodel.fit(X_train, y_train.values.ravel())\n\nestimator = model.estimators_[4]\ny_pred = model.predict(X_test)\nprint('Score of  on test set: {:.4f}'.format(model.score(X_test, y_test)))\ncf_mx1_f = confusion_matrix(y_test, y_pred)\nprint(cf_mx1_f)\nTrue_Pred_f = cf_mx1_f[0][0] + cf_mx1_f[1][1]\nTotal_Pred_f = cf_mx1_f[0][0] + cf_mx1_f[1][1] + cf_mx1_f[0][1] + cf_mx1_f[1][0]\nacc= True_Pred_f\/Total_Pred_f\nprint('Accuracy of on test set: {:.4f}'.format(acc))","d5c36a3d":"from sklearn.tree import export_graphviz\n# Export as dot file\nexport_graphviz(estimator, out_file='tree.dot', \n                feature_names = os_data_dt_X.columns,\n                class_names = ['Churn','No Churn'],\n                rounded = True, proportion = False, \n                precision = 2, filled = True)\n\n","15fda27c":"# Convert to png\nfrom subprocess import call\ncall(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=1200'])","1957da38":"import matplotlib.pyplot as plt\nplt.figure(figsize = (14, 18))\nplt.imshow(plt.imread('tree.png'))\nplt.axis('off');\nplt.show();","7eae90c4":"from xgboost import XGBClassifier\n\n#model_xg = XGBClassifier(random_state=3457)\n#run_model(os_data_X,os_data_y,model_xg, \"XGBoost\", 4)","888c48ed":"alg_name = \"XGBoost\"\nmodel = XGBClassifier(random_state=3457)\nscore_l=[]\nacc_l=[]\nfor i in range(3,10):\n    final_list=feature_sel(os_data_Xy,model,i)\n    os_data_y=os_data_Xy.loc[:, os_data_Xy.columns == 'Churn']\n    os_data_X=os_data_Xy[final_list]\n    #final_list\n    score_f,acc_f = run_model(os_data_X,os_data_y,model, alg_name, 2)\n    score_l.append(score_f)\n    acc_l.append(acc_f)\n    ind_score= score_l.index(max(score_l))\n    \nprint('Number of feature:' + str(ind_score+3))\nprint('Score of ' + alg_name + ' on test set: {:.4f}'.format(score_l[ind_score]))\nprint('Accuracy of ' + alg_name + ' on test set: {:.4f}'.format(acc_l[ind_score]))\nfinal_list=feature_sel(os_data_Xy,model,ind_score+3)\nfinal_list\nos_data_xg_X=os_data_X[final_list]","ed084d39":"from sklearn.svm import SVC","e0b60d00":"alg_name = \"SVC\"\nmodel = SVC(random_state=3458)\nscore_l=[]\nacc_l=[]\nfor i in range(3,10):\n    final_list=feature_sel(os_data_Xy,model,i)\n    os_data_y=os_data_Xy.loc[:, os_data_Xy.columns == 'Churn']\n    os_data_X=os_data_Xy[final_list]\n    #final_list\n    score_f,acc_f = run_model(os_data_X,os_data_y,model, alg_name, 2)\n    score_l.append(score_f)\n    acc_l.append(acc_f)\n    ind_score= score_l.index(max(score_l))\n    \nprint('Number of feature:' + str(ind_score+3))\nprint('Score of ' + alg_name + ' on test set: {:.4f}'.format(score_l[ind_score]))\nprint('Accuracy of ' + alg_name + ' on test set: {:.4f}'.format(acc_l[ind_score]))\nfinal_list=feature_sel(os_data_Xy,model,ind_score+3)\nfinal_list\nos_data_svc_X=os_data_X[final_list]","b67a1a58":"from sklearn import neighbors","15b9ed9b":"alg_name = \"KNN\"\nmodel = neighbors.KNeighborsClassifier()\nscore_l=[]\nacc_l=[]\nfor i in range(3,10):\n    final_list=feature_sel(os_data_Xy,model,i)\n    os_data_y=os_data_Xy.loc[:, os_data_Xy.columns == 'Churn']\n    os_data_X=os_data_Xy[final_list]\n    #final_list\n    score_f,acc_f = run_model(os_data_X,os_data_y,model, alg_name, 2)\n    score_l.append(score_f)\n    acc_l.append(acc_f)\n    ind_score= score_l.index(max(score_l))\n    \nprint('Number of feature:' + str(ind_score+3))\nprint('Score of ' + alg_name + ' on test set: {:.4f}'.format(score_l[ind_score]))\nprint('Accuracy of ' + alg_name + ' on test set: {:.4f}'.format(acc_l[ind_score]))\nfinal_list=feature_sel(os_data_Xy,model,ind_score+3)\nfinal_list\nos_data_knn_X=os_data_X[final_list]","fe85fb31":"from scipy import interp\nfrom sklearn.metrics import roc_curve,auc\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import KFold # import KFold\n#from sklearn.cross_validation import cross_val_score\nimport matplotlib.patches as patches","587abb12":"os_data_y=np.array(os_data_y)\nc, r = os_data_y.shape\nos_data_y = os_data_y.reshape(c,)","54350f88":"from sklearn.model_selection import cross_val_predict","e724d67b":"os_data_y=os_data_Xy.loc[:, os_data_Xy.columns == 'Churn']","fbed2ab4":"#from sklearn.cross_validation import cross_val_score, cross_val_predict\nfrom sklearn.model_selection  import cross_val_score, cross_val_predict","65ffdce9":"os_data_y=np.array(os_data_y)\nc, r = os_data_y.shape\nos_data_y = os_data_y.reshape(c,)","bfd5a7e8":"def cross_val_f(model,X,y):\n    scores_f = cross_val_score(model,X, y, cv=3)\n    #print (\"Cross-validated scores for Random Forest:\", scores_lg)\n    print (\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_f.mean(), scores_f.std() * 2))","6424f625":"model_rf = RandomForestClassifier(n_estimators=6,random_state=3457,max_depth=4,max_leaf_nodes=25)\ncross_val_f(model_rf,os_data_rf_X,os_data_y)","615ebae0":"model_lg = LogisticRegression(random_state=3457)\ncross_val_f(model_lg,os_data_lg_X,os_data_y)","440ced5d":"model_knn = neighbors.KNeighborsClassifier()\ncross_val_f(model_knn,os_data_knn_X,os_data_y)","210b0f98":"#### 4- Cross Validation for SVC","ba74149f":"model_svc = SVC(C=1.0, degree=2,random_state=3458,kernel='rbf')\ncross_val_f(model_svc,os_data_svc_X,os_data_y)","9e172ad6":"model_xg = XGBClassifier(random_state=3457)\ncross_val_f(model_xg,os_data_xg_X,os_data_y)","df6fc9b2":"\nmodel_dt = tree.DecisionTreeClassifier(criterion='entropy', max_depth=3,random_state=3458)\ncross_val_f(model_dt,os_data_dt_X,os_data_y)","40568a73":"model_lgL1 = LogisticRegression(random_state=3457,C=0.1, penalty='l1', tol=0.1, solver='liblinear')\ncross_val_f(model_lgL1,os_data_lgL1_X,os_data_y)","0d6ea3d5":"#from sklearn.model_selection import cross_val_predict\ny_pred= cross_val_predict(model_dt, os_data_dt_X, os_data_y, cv=3)\nconf_mat = confusion_matrix(os_data_y, y_pred)\nprint(conf_mat)\n","af656983":"print(classification_report(os_data_y, y_pred))","0c6c7029":"from sklearn.model_selection import StratifiedKFold\n#clf = RandomForestClassifier(random_state=3457)\nmodel_dt = tree.DecisionTreeClassifier(criterion='entropy', max_depth=3,random_state=3458)\ncv = StratifiedKFold(n_splits=5,shuffle=False)","2c985c70":"X_train, X_test, y_train,y_test = train_test_split(os_data_dt_X,os_data_y,test_size=0.2,random_state=3457)","b685f1a8":"model_dt = tree.DecisionTreeClassifier(criterion='entropy', max_depth=3,random_state=3458)","dcc9d5ce":"os_data_dt_X=pd.DataFrame(os_data_dt_X)\nos_data_y=pd.DataFrame(os_data_y)\ntprs = []\naucs = []\nmean_fpr = np.linspace(0, 1, 101)\ni = 1\nfor train,test in cv.split(os_data_dt_X,os_data_y):\n    prediction = model_dt.fit(os_data_dt_X.iloc[train],os_data_y.iloc[train]).predict_proba(os_data_dt_X.iloc[test])\n    fpr, tpr, t = roc_curve(os_data_y.iloc[test], prediction[:, 1])\n    tprs.append(interp(mean_fpr, fpr, tpr))\n    roc_auc = auc(fpr, tpr)\n    aucs.append(roc_auc)\n    plt.plot(fpr, tpr, lw=2, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n    i= i+1\nplt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\nmean_tpr = np.mean(tprs, axis=0)\nmean_auc = auc(mean_fpr, mean_tpr)\nplt.plot(mean_fpr, mean_tpr, color='blue',\n         label=r'Mean ROC (AUC = %0.2f )' % (mean_auc),lw=2, alpha=1)\n\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC')\nplt.legend(loc=\"lower right\")\nplt.text(0.32,0.7,'More accurate area',fontsize = 12)\nplt.text(0.63,0.4,'Less accurate area',fontsize = 12)\nplt.show()","914374b9":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import cohen_kappa_score","2698f3d9":"#gives model report in dataframe\ndef model_report(model,training_x,testing_x,training_y,testing_y,name) :\n    model.fit(training_x,training_y)\n    #scores_xg = cross_val_score(model_xg,os_data_X, os_data_y, cv=3)\n    predictions  = model.predict(testing_x)\n    accuracy     = \"{0:.4f}\".format(accuracy_score(testing_y,predictions))\n    recallscore  = recall_score(testing_y,predictions)\n    precision    = precision_score(testing_y,predictions)\n    roc_auc      = roc_auc_score(testing_y,predictions)\n    f1score      = f1_score(testing_y,predictions) \n    kappa_metric = cohen_kappa_score(testing_y,predictions)\n\n    df = pd.DataFrame({\"Model\"           : [name],\n                       \"Accuracy_score\"  : [accuracy],\n                       \"Recall_score\"    : [recallscore],\n                       \"Precision\"       : [precision],\n                       \"f1_score\"        : [f1score],\n                       \"Area_under_curve\": [roc_auc],\n                       \"Kappa_metric\"    : [kappa_metric],\n                      })\n    return df","412624b5":"def model_report_cv(model,os_data_X, os_data_y,name) :\n    \n    y_pred= cross_val_predict(model, os_data_X, os_data_y, cv=3)\n    accuracy = \"{0:.4f}\".format(cross_val_score(model,os_data_X, os_data_y, cv=3).mean())\n    recallscore = recall_score(os_data_y, y_pred,average='weighted')\n    precision     = precision_score(os_data_y, y_pred,average='weighted')\n    roc_auc=roc_auc_score(os_data_y, y_pred,average='weighted')\n    f1score=f1_score(os_data_y, y_pred,average='weighted')\t \n    kappa_metric=cohen_kappa_score(os_data_y, y_pred)\t\n    df = pd.DataFrame({\"Model\"           : [name],\n                       \"Accuracy_score\"  : [accuracy],\n                       \"Recall_score\"    : [recallscore],\n                       \"Precision\"       : [precision],\n                       \"f1_score\"        : [f1score],\n                       \"Area_under_curve\": [roc_auc],\n                       \"Kappa_metric\"    : [kappa_metric],\n                      })\n    return df","708daddb":"os_data_bX=dataset0.loc[:, dataset0.columns != 'Churn']\nos_data_by=dataset0.loc[:, dataset0.columns == 'Churn']","0b37688e":"\nos_data_y=os_data_Xy.loc[:, os_data_Xy.columns == 'Churn']","442c64f4":"#Base Model\n\nmodel_base = LogisticRegression(random_state=3457)\ntrain_X, test_X, train_Y,test_Y = train_test_split(os_data_bX,os_data_by.values.ravel(),test_size=0.2,random_state=3457)\nmodel1 = model_report(model_base,train_X,test_X,train_Y,test_Y,\"Logistic Regression(BM)\")\n#model1 = model_report_cv(model_base,os_data_X,os_data_y,\"Logistic Regression(BM)\")","2dd5c093":"# Linear Regreassion\n\nmodel_lg = LogisticRegression(random_state=3457)\ntrain_X, test_X, train_Y,test_Y = train_test_split(os_data_lg_X,os_data_y.values.ravel(),test_size=0.2,random_state=3457)\nmodel2 = model_report(model_lg,train_X,test_X,train_Y,test_Y,\"Logistic Regression\")\n#model2 = model_report_cv(model_lg,os_data_lg_X,os_data_y.values.ravel(),\"Logistic Regression\")","97f11d60":"model_lgL1 = LogisticRegression(random_state=3457,C=0.1, penalty='l1', tol=0.1, solver='liblinear')\n#train_X, test_X, train_Y,test_Y = train_test_split(os_data_lgL1_X,os_data_y,test_size=0.2,random_state=3457)\n#model3 = model_report(model_lgL1,train_X,test_X,train_Y,test_Y,\"Logistic Regression(L1)\")\nmodel3 = model_report_cv(model_lgL1,os_data_lgL1_X,os_data_y.values.ravel(),\"Logistic Regression(L1)\")","dd225088":"#model_dt = RandomForestClassifier(n_estimators=6,random_state=3457,max_depth=4,max_leaf_nodes=25)\nmodel_dt = tree.DecisionTreeClassifier(criterion='entropy', max_depth=3,random_state=3457)\n#model_dt = tree.DecisionTreeClassifier(criterion='entropy', max_depth=3,random_state=3458,n_estimators=6)\n#train_X, test_X, train_Y,test_Y = train_test_split(os_data_dt_X,os_data_y,test_size=0.2,random_state=3457)\n#model4 = model_report(model_dt,train_X,test_X,train_Y,test_Y,\"Decision Tree\")\nmodel4 = model_report_cv(model_dt,os_data_dt_X,os_data_y.values.ravel(),\"Decision Tree\")","86733a9e":"model_rf = RandomForestClassifier(n_estimators=6,random_state=3457,max_depth=4,max_leaf_nodes=25)\n#train_X, test_X, train_Y,test_Y = train_test_split(os_data_rf_X,os_data_y,test_size=0.2,random_state=3457)\n#model5 = model_report(model_rf,train_X,test_X,train_Y,test_Y,\"Random Forest\")\nmodel5 = model_report_cv(model_rf,os_data_rf_X,os_data_y.values.ravel(),\"Random Forest\")","2f2bbab8":"model_xg = XGBClassifier(random_state=3457)\n#train_X, test_X, train_Y,test_Y = train_test_split(os_data_xg_X,os_data_y,test_size=0.2,random_state=3457)\n#model6 = model_report(model_xg,train_X,test_X,train_Y,test_Y,\"XGBOOST\")\nmodel6 = model_report_cv(model_xg,os_data_xg_X,os_data_y.values.ravel(),\"XGBOOST\")","3580e64e":"model_svc = SVC(C=0.01, degree=2,random_state=3457,kernel='rbf')\n#train_X, test_X, train_Y,test_Y = train_test_split(os_data_svc_X,os_data_y,test_size=0.2,random_state=3457)\n#model7 = model_report(model_svc,train_X,test_X,train_Y,test_Y,\"SVC\")\nmodel7 = model_report_cv(model_svc,os_data_svc_X,os_data_y.values.ravel(),\"SVC\")","7463d855":"model_knn = neighbors.KNeighborsClassifier()\n#train_X, test_X, train_Y,test_Y = train_test_split(os_data_knn_X,os_data_y,test_size=0.2,random_state=3457)\n#model8 = model_report(model_knn,train_X,test_X,train_Y,test_Y,\"KNN\")\nmodel8 = model_report_cv(model_knn,os_data_knn_X,os_data_y.values.ravel(),\"KNN\")\n","d0276420":"import io\nimport plotly.offline as py#visualization\npy.init_notebook_mode(connected=True)#visualization\nimport plotly.graph_objs as go#visualization\nimport plotly.tools as tls#visualization\nimport plotly.figure_factory as ff#visualization\n","eb584d63":"model_performances = pd.concat([model1,model2,model3,\n                                model4,model5,model6,\n                                model7,model8],axis = 0).reset_index()\n\nmodel_performances = model_performances.drop(columns = \"index\",axis =1)\n\ntable  = ff.create_table(np.round(model_performances,4))\npy.iplot(table)","2f5f0d77":"#### Dependents\nBased on the below plot, it looks like that people dont churn if they have dependents. Hence, the dependents feature might have a significient contribution in predicting the \"Churn\"","2016eb39":"## 3. Feature Engineering","8b0b7b86":"### Select number of features based on accuracy","ee12a655":"### Tenure & Internet Service\nAverage duration of non-churners show that if customers stay beyond minimum period they tend to stay longer","69ed3d7a":"### Features Construction\nCreating new feature based on a clustering between certain features might add a value toward the final model accuracy, where the target is to find a kind of grouping that split the records according to their churn status. Hence, we will use the K-Mode which is targeting categorical kind of features.","60d49f46":"Build a function for Classifications model","80346d09":"Total charges is categorical, it should be changed to float.","1d953125":"#### 7- Cross Validation for Logistic Regression with L1","a76aef99":"### Visualize the tree","684b9d6f":"# KNN","a0840e0b":"**Context:** The problem is to look at a churn prediction model for a Telecom customer. We started with understanding of the data presented, followed by a detailed exploratory analysis to look at the different data attributes, their domains, relationships etc. Feature engineering helps in identifying the key variables\/set of variables in the context of churners, and help in identifying the ones that impact churning. A baseline model is arrived at based on a set of customer data which is trained further using additional data inputs. Finally the model is tested against the remaining set of data set aside for testing and validation. The resultant validated model is good to go for implementation on actual on-going data sets. ","8d4f5a4b":"### Balancing Dataset\nLooking for the dataset balance in term of the Churner's, we can find that the data is not balanced, as the churnners around 25% while the rest dont. Therefore, will do a bootstrap through SMOTE function in Imblearn package.","5d41d9de":"### Normalize Continuous variable","d8d82656":"### Select number of features based on accuracy","823d0ce7":"#### Senior Citizen\nSenior citizen feature considered an important variable to predict the churn. The following plots shows that the senior people who churns are double than who dont.","8690ec76":"### Churn Analysis- Correlation Matrix\nBelow chart helps in identifying the right features\/variables that are correlated for model building ","68260ab3":"## Print Confusion Matrix and Classification report for Decision Tree\n","b92c8f6d":"### Gender & Monthly Charges\nIn both the Genders, people with high monthly charge tend to churn more than the others. Therefore, we can say that the Monthly Charges feature has a signficient effect to the Churn.","eca475e1":"####  The following table represnts the summary of all models trained and tested. \n####  Hence, the metric proposed is the accuracy, and the best perofoming model founded is the RandomeForest.","caaffcfc":"# Logistic Regression with a L1 penalty","73c8f7c4":"## Plot ROC\n","0d232d3d":"#### 5- Cross Validation for XGBOOST","6355fd9c":"#### Technical Support & Multilines\nTechnical support and Multiple lines along the churner's are all explained in the below plot. We can interpretthat the customers with technical support sevrices most probably wont churn. While those who have multiple lines are not willing to churn.","274f2f7d":"# Customer Churn Prediction Analysis","60b7661b":"### Creating Dummy Variables ","62a0d0c0":"### Re-arranging features values\nThe following features are categorical where each one contains 3 values where 2 of them can be merged into one value. Foe example, in MultipleLines feature, it has values \"yes\", \"No\" and \"No phone Service's\". Therefore, the last two can be merged together as \"No\".","ae96de87":"# xgboost","2a54b230":"### Print Features importance","fa0a59b6":"### Importing Dataset","49833e97":"#### 3- Cross Validation for KNN ","4c376558":"From the above table, we can find that groups 3,4 and 5 sharply split the data in referrence to the Churn. ","91369df1":" ### Dependent Variable: Churn\nThe following is the Churn distribution along all records.","cf116f02":"### Split, train and Test","3462e311":"## Data Manipulation\nData manipuation done on the dataset covers checking the missing values, outliers and NA's. However, in our data set, we found that there are 11 NA's in the \"Total Charges\" features. Hence, its decided to fill them with zero's since they are considered new and their monthly invoice not issyed yet.","6e6dd00b":"### Select number of features based on accuracy","432dbd10":"**Backup Data Set**","88e4e244":"### Remove Churn and Customer ID from the dataset","81a48f0a":"#### 2- Cross Validation for Random Logistic Regression","c690aea3":"### Transform dependent variable to binaries.","351ceeee":"### Print Features importance","900dec94":"#### 6- Cross Validation for Logistic Regression","908cb181":"#### Total Charges & Tenures\nWhen we look at churner data, total charges tend to increase with tenure and this could have been a factor for churning. When we look at the same for non-churners, there doesn\u2019t seem to be a clear increasing trend. ","241b6ef7":"# Random Forest","00ae759d":"# Cross Validation\n","91b5c70b":"Converting the other categorical variables to binaries. This is needed to make sure that they will be easily understoor and transformed by the different model algorithems","8bc9f95f":"### Converting Fields to Binaries\nPayment Method, Contract and Internet Service are a features with categorical values that have no notion or sense of order. Therefore, we need to transform them into a more representative numerical form at which it can be easily understood by downstream code and pipeline.","638ed9e4":"## 5. Summary of the models","661226db":"# Decision Tree","e96f11b2":" ###  Demographic Features Analysis","b9b64798":"### Select number of features based on accuracy","04069b16":"## Exploratory Data Analysis\n\nDiagrams and graphs are the best way to explore the data, the following plots provide some insghts about the features, their relation with each others, and with our target variable, \"Churn\".","65b2a7b7":"Converting total charges to float, and replacing NA's with zero's","f18b3de2":"# SVC","11abff97":"### Optimie number of features","3dcee983":"#### Payment Methode\nPeople opting for automated payment methods are less likely to churn","4553cb61":"#### 1- Cross Validation for Random Forest","950eb37f":"### Importing Dataset","187cae1a":"### Optimize number of Features","3947a2fc":"###  Contract & Payment Method\nBoth One year and Two Year contract tend to keep the churn low, while its the opposit once it comes to the Monthly contract. However, automated payment methods help to keep customers intact even on a month-to-month contract option.","9aaabf91":"### Features Selection\nThe dataset includes both categorical and nuemerical variables, the following code from sklearn package will classify the features according to their importance, \"True\" will be considered as an important feature, while the opposite is the \"False\".","752ac3e2":"#### Gender\nThe dataset contain around 7043 records distributed between females and males. The following diagrame shows the ratios for both in referrence to the \"Churn\".Its clarify that churn almost equaly distributed between both males and females. Therefor, it's considered has insignificint effect to the target variables.","57d568d9":"### Importing Packages","d744416c":"## 2. Baseline\n### Importing Libraries","ed59d3aa":"### Optimize number of features","da825aaf":"# Logistic Regression","a45aea84":"Dropping old features, InternetService, Contract and PaymentMethods"}}