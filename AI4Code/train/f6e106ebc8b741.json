{"cell_type":{"b5431789":"code","9b2a07e3":"code","5e2f796c":"code","64e00291":"code","cbef2282":"code","3b441c92":"code","b861fccd":"code","c046144c":"code","b78b89cb":"code","599eade9":"code","7a98b3bc":"code","4ce2e4c7":"code","4b440491":"code","2ffe353f":"code","a205a112":"code","0e8f1b85":"code","7dec32e7":"code","5795751e":"code","67378828":"code","c04bab16":"code","988bf4c6":"code","67029a40":"code","26743e93":"code","b5bcc6ef":"code","5a00c410":"code","e03b993e":"code","4487744e":"code","789c7880":"code","acccb037":"code","5aacfe9b":"code","8434b561":"markdown"},"source":{"b5431789":"!pip install tensorflow==2.0.0-alpha0\n\nimport tensorflow as tf","9b2a07e3":"print(tf.__version__)","5e2f796c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom tensorflow import keras\n\nfrom sklearn.model_selection import train_test_split\nfrom keras import models, layers\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os","64e00291":"from __future__ import absolute_import, division, print_function, unicode_literals\n\ntry:\n    %tensorflow_version 2.x\nexcept Exception:\n    pass\nimport tensorflow as tf\nfrom tensorflow import keras\n\nimport numpy as np\n\nprint(tf.__version__)","cbef2282":"from keras.datasets import imdb","3b441c92":"import numpy as np\n# save np.load\nnp_load_old = np.load\n\n# modify the default parameters of np.load\nnp.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\nNUM_WORDS = 10000\n# call load_data with allow_pickle implicitly set to true\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=NUM_WORDS)\n\n# restore np.load for future normal usage\nnp.load = np_load_old","b861fccd":"print(\"Training entries: {}, labels: {}\".format(len(train_data), len(train_labels)))","c046144c":"print(train_data[0])","b78b89cb":"print(train_data[1])","599eade9":"# A dictionary mapping words to an integer index\nword_index = imdb.get_word_index()","7a98b3bc":"word_index","4ce2e4c7":"# The first 4 indices are reserved\nword_index = {k:(v+3) for k,v in word_index.items()}\nword_index[\"<PAD>\"] = 0\nword_index[\"<START>\"] = 1\nword_index[\"<UNK>\"] = 2  # unknown\nword_index[\"<UNUSED>\"] = 3\n\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n\ndef decode_review(text):\n    return ' '.join([reverse_word_index.get(i, '?') for i in text])","4b440491":"decode_review(train_data[0])","2ffe353f":"train_data = keras.preprocessing.sequence.pad_sequences(train_data,\n                                                        value=word_index[\"<PAD>\"],\n                                                        padding='post',\n                                                        maxlen=256)\n\ntest_data = keras.preprocessing.sequence.pad_sequences(test_data,\n                                                       value=word_index[\"<PAD>\"],\n                                                       padding='post',\n                                                       maxlen=256)","a205a112":"len(train_data[0]), len(train_data[1])","0e8f1b85":"print(train_data[0])","7dec32e7":"# input shape is the vocabulary count used for the movie reviews (10,000 words)\nvocab_size = 10000\n\nmodel = keras.Sequential()\nmodel.add(keras.layers.Embedding(vocab_size, 16))\nmodel.add(keras.layers.GlobalAveragePooling1D())\nmodel.add(keras.layers.Dense(16, activation=tf.nn.relu))\nmodel.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n\nmodel.summary()","5795751e":"model.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['acc'])","67378828":"x_val = train_data[:10000]\npartial_x_train = train_data[10000:]\n\ny_val = train_labels[:10000]\npartial_y_train = train_labels[10000:]","c04bab16":"history = model.fit(partial_x_train,\n                    partial_y_train,\n                    epochs=40,\n                    batch_size=512,\n                    validation_data=(x_val, y_val),\n                    verbose=1)","988bf4c6":"results = model.evaluate(test_data, test_labels)\n\nprint(results)","67029a40":"history_dict = history.history\nhistory_dict.keys()","26743e93":"import matplotlib.pyplot as plt\n\nacc = history_dict['acc']\nval_acc = history_dict['val_acc']\nloss = history_dict['loss']\nval_loss = history_dict['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss, 'bo', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","b5bcc6ef":"plt.clf()   # clear figure\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()","5a00c410":"NUM_WORDS=10000","e03b993e":"baseline_model = keras.Sequential([\n    # `input_shape` is only required here so that `.summary` works.\n    keras.layers.Dense(64, activation=tf.nn.relu, input_shape=(10000,)),\n    keras.layers.Dense(64, activation=tf.nn.relu),\n    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n])\n\nbaseline_model.compile(optimizer='adam',\n                       loss='binary_crossentropy',\n                       metrics=['accuracy', 'binary_crossentropy'])\n\nbaseline_model.summary()","4487744e":"baseline_history = baseline_model.fit(partial_x_train,\n                    partial_y_train,\n                    epochs=40,\n                    batch_size=512,\n                    validation_data=(x_val, y_val),\n                    verbose=1)","789c7880":"baseline_history = baseline_model.fit(train_data,\n                                      train_labels,\n                                      epochs=20,\n                                      batch_size=512,\n                                      validation_data=(test_data, test_labels),\n                                      verbose=2)","acccb037":"smaller_model = keras.Sequential([\n    keras.layers.Dense(4, activation=tf.nn.relu, input_shape=(10000,)),\n    keras.layers.Dense(4, activation=tf.nn.relu),\n    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n])\n\nsmaller_model.compile(optimizer='adam',\n                loss='binary_crossentropy',\n                metrics=['accuracy', 'binary_crossentropy'])\n\nsmaller_model.summary()","5aacfe9b":"smaller_history = smaller_model.fit(train_data,\n                                    train_labels,\n                                    epochs=20,\n                                    batch_size=512,\n                                    validation_data=(test_data, test_labels),\n                                    verbose=2)","8434b561":"# Create a baseline model"}}