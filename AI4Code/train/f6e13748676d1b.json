{"cell_type":{"7f6e0107":"code","b93f0b98":"code","dcb09564":"code","a325474d":"code","28c7d631":"code","d72f25aa":"code","814a5095":"code","a7d9139b":"code","d6070ae0":"code","0a3bed8e":"code","4a709684":"code","9c0d6809":"code","3976b461":"code","f0490461":"code","3e2f3867":"code","ad8d543a":"code","53bb566e":"code","84c10520":"code","41afe889":"code","7a284d2b":"code","348ed009":"code","8e944eff":"code","c02d4f47":"code","26a9ad51":"code","69401f4f":"code","25f3b314":"code","dd1f7f8c":"code","0aeeddb7":"code","61ef8e5b":"code","cdf0f21f":"code","2a029495":"code","ed9b1b63":"code","85e9583b":"code","8d2ef38b":"code","366b90fd":"code","4bc83f29":"code","6313fdbf":"code","042e950c":"code","1b726022":"code","c783e6bd":"code","5f931956":"code","11edefca":"code","115cd9b6":"code","a40f6665":"code","a9a23760":"code","d435ae20":"code","aa74e624":"markdown","e3286ccc":"markdown","f9c671d6":"markdown","22c00672":"markdown","2829cb3c":"markdown","5ce7a131":"markdown","d8a1452d":"markdown","e6f9713d":"markdown","4353cbbf":"markdown","07506473":"markdown"},"source":{"7f6e0107":"!pip install jovian --upgrade --quiet","b93f0b98":"import jovian","dcb09564":"project_name = \"Two-class weather classification\"","a325474d":"# Librairies\n\nimport os\nfrom random import randrange\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport torchvision\nimport torchvision.models as models\nfrom torchvision.datasets.utils import download_url\nfrom torchvision.datasets import ImageFolder\nimport torchvision.transforms as tt\nfrom torchvision.utils import make_grid\nimport matplotlib.pyplot as plt\n%matplotlib inline","28c7d631":"# Import dataset\n\n# Import dataset\n\ndata_dir = '..\/input\/twoclass-weather-classification'\nprint('Folders :', os.listdir(data_dir))\nclasses = os.listdir(data_dir + \"\/train\")\nnum_classes = len(classes)\nprint(\"num weather:\", num_classes)\nprint({cls: len(os.listdir(data_dir + f\"\/train\/{cls}\/\")) for cls in sorted(classes)})\nprint('classes :', classes)","d72f25aa":"transform = tt.Compose(\n    [\n        tt.ToTensor(), \n    ]\n)\n\n# Create datasets\ntrain_ds = ImageFolder(data_dir+'\/train', transform)\nvalid_ds = ImageFolder(data_dir+'\/test', transform)\n\n# set the batch size\nbatch_size = 64\n\ntrain_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True)\nvalid_dl = DataLoader(valid_ds, batch_size*2, num_workers=2, pin_memory=True)\n\nclasses = valid_ds.classes","814a5095":"import matplotlib.pyplot as plt\n\ndef show_example1(img, label):\n    print('Label: ', train_ds.classes[label], \"(\"+str(label)+\")\")\n    plt.imshow(img.permute(1, 2, 0))","a7d9139b":"show_example1(*train_ds[99])","d6070ae0":"show_example1(*train_ds[1000])","0a3bed8e":"def show_example(data):\n    [img, label] = data\n    print(classes[label])\n    plt.imshow(img.permute(1, 2, 0))\n    \nimage_number = randrange(20000)\nshow_example(train_ds[image_number])","4a709684":"def show_batch(dl):\n    for images, labels in dl:\n        fig, ax = plt.subplots(figsize=(12, 12))\n        ax.set_xticks([]); ax.set_yticks([])\n        ax.imshow(make_grid(images[:64], nrow=8).permute(1, 2, 0))\n        break\n\nshow_batch(train_dl)","9c0d6809":"show_batch(valid_dl)","3976b461":"# pick a device\ndef get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)\n    \n\ndevice = get_default_device()\ndevice","f0490461":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))\n\nclass ImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, result['train_loss'], result['val_loss'], result['val_acc']))","3e2f3867":"class WeartherClassification(ImageClassificationBase):\n    def __init__(self):\n        super().__init__()\n        # Use a pretrained model\n        self.network = models.resnet34(pretrained=True)\n        # Replace last layer\n        num_ftrs = self.network.fc.in_features\n        self.network.fc = nn.Linear(num_ftrs, num_classes)\n    \n    def forward(self, xb):\n        return torch.sigmoid(self.network(xb))\n    \n    def freeze(self):\n        # To freeze the residual layers\n        for param in self.network.parameters():\n            param.require_grad = False\n        for param in self.network.fc.parameters():\n            param.require_grad = True\n    \n    def unfreeze(self):\n        # Unfreeze all layers\n        for param in self.network.parameters():\n            param.require_grad = True","ad8d543a":"model = to_device(WeartherClassification(), device)\nmodel","53bb566e":"torch.cuda.empty_cache()\ntrain_dl = DeviceDataLoader(train_dl, device)\nvalid_dl = DeviceDataLoader(valid_dl, device)\nto_device(model, device);\n\n# print outputs for a single batch\nfor images, labels in train_dl:\n    print('images.shape:', images.shape)\n    out = model(images)\n    print('out.shape:', out.shape)\n    print('all outputs:', out)\n    break\n    \n# show the highest probability values and their index\/classes\n#values, indices = out.max(0)\n#for index in indices :\n#    print(\"max values\", values)\n#    print(\"max indices\", index)\n#    print(\"guesses\", classes[index])\n#    break\n#print(\"max values\", values)\n#print(\"max indices\", index)\n#print(\"guesses\", [classes[index] for index in indices])","84c10520":"# create the test dataset\ntest_dataset = ImageFolder(data_dir+'\/test', tt.ToTensor())\n\n#methods to pick a random image and make a prediciton\ndef predict_image(img, model):\n    # Convert to a batch of 1\n    xb = to_device(img.unsqueeze(0), device)\n    # Get predictions from model\n    yb = model(xb)\n    # Pick index with highest probability\n    _, preds  = torch.max(yb, dim=1)\n    # Retrieve the class label\n    return test_dataset.classes[preds[0].item()]\n\ndef get_random_test_image(dataset):\n    rand_num = randrange(len(dataset))\n    return dataset[rand_num]","41afe889":"@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n    history = []\n    optimizer = opt_func(model.parameters(), lr)\n    for epoch in range(epochs):\n        # Training Phase \n        model.train()\n        train_losses = []\n        for batch in train_loader:\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","7a284d2b":"history = [evaluate(model, valid_dl)]\nhistory","348ed009":"num_epochs = 15\nopt_func = torch.optim.Adam\nlr = 0.001","8e944eff":"# freeze the model to only train the last layer\nmodel.freeze()","c02d4f47":"history = fit(num_epochs, lr, model, train_dl, valid_dl, opt_func)","26a9ad51":"val_loss = [hist['val_loss'] for hist in history]\ntrain_loss = [hist['train_loss'] for hist in history]\nval_acc = [hist['val_acc'] for hist in history]\nplt.plot(val_loss, label=\"Validation Loss\")\nplt.plot(train_loss, label=\"Training Loss\")\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\nplt.show()\n\nplt.plot()\nplt.plot(val_acc)\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.axis([0, 10, 0, 1])\nplt.show()","69401f4f":"img, label = get_random_test_image(dataset=test_dataset)\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', test_dataset.classes[label], ', Predicted:', predict_image(img, model))","25f3b314":"img, label = get_random_test_image(dataset=test_dataset)\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', test_dataset.classes[label], ', Predicted:', predict_image(img, model))","dd1f7f8c":"img, label = get_random_test_image(dataset=test_dataset)\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', test_dataset.classes[label], ', Predicted:', predict_image(img, model))","0aeeddb7":"img, label = get_random_test_image(dataset=test_dataset)\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', test_dataset.classes[label], ', Predicted:', predict_image(img, model))","61ef8e5b":"img, label = get_random_test_image(dataset=test_dataset)\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', test_dataset.classes[label], ', Predicted:', predict_image(img, model))","cdf0f21f":"test_loader = DataLoader(test_dataset, batch_size*2, num_workers=4, pin_memory=True)\ntest_loader = DeviceDataLoader(test_loader, device)\nevaluate(model, test_loader)","2a029495":"!pip install jovian --upgrade --quiet","ed9b1b63":"import jovian","85e9583b":"jovian.commit(project=project_name, environment=None)","8d2ef38b":"# some data transforms and augmentation to improve accuracy\n\nstats = ((0.5,0.5,0.5), (0.5,0.5,0.5))\ntrain_transform = tt.Compose(\n    [\n        tt.RandomCrop(200, padding=20, padding_mode='reflect'),\n        tt.RandomHorizontalFlip(),\n        tt.ToTensor(),\n        tt.Normalize(*stats),\n    ]\n)\nvalid_transform = tt.Compose(\n    [\n        tt.ToTensor(), \n        tt.Normalize(*stats)\n    ]\n)\n\n# Create datasets\ntrain_ds = ImageFolder(data_dir+'\/train', train_transform)\nvalid_ds = ImageFolder(data_dir+'\/test', valid_transform)\ntest_ds = ImageFolder(data_dir+'\/test', valid_transform)\n\n# set the batch size\nbatch_size = 64\n\ntrain_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True)\nvalid_dl = DataLoader(valid_ds, batch_size*2, num_workers=2, pin_memory=True)\n\nclasses = valid_ds.classes\n\nshow_batch(train_dl)","366b90fd":"# a slightly more advanced training loop for increased accuracy\n@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n\ndef fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, \n                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n    torch.cuda.empty_cache()\n    history = []\n    \n    # Set up cutom optimizer with weight decay\n    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n    # Set up one-cycle learning rate scheduler\n    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n                                                steps_per_epoch=len(train_loader))\n    \n    for epoch in range(epochs):\n        # Training Phase \n        model.train()\n        train_losses = []\n        lrs = []\n        for batch in train_loader:\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            \n            # Gradient clipping\n            if grad_clip: \n                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n            \n            optimizer.step()\n            optimizer.zero_grad()\n            \n            # Record & update learning rate\n            lrs.append(get_lr(optimizer))\n            sched.step()\n        \n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        result['lrs'] = lrs\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","4bc83f29":"class WeatherModel1(WeartherClassification):\n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))","6313fdbf":"torch.cuda.empty_cache()\ntrain_dl = DeviceDataLoader(train_dl, device)\nvalid_dl = DeviceDataLoader(valid_dl, device)\nmodel = to_device(WeatherModel1(), device)\nmodel","042e950c":"epochs = 15\nmax_lr = 3e-4\ngrad_clip = 0.1\nweight_decay = 1e-4\nopt_func = torch.optim.Adam","1b726022":"history = [evaluate(model, valid_dl)]\nhistory","c783e6bd":"history += fit_one_cycle(epochs, max_lr, model, train_dl, valid_dl, \n                             grad_clip=grad_clip, \n                             weight_decay=weight_decay, \n                             opt_func=opt_func)","5f931956":"import numpy as np\n# plot accuracy\naccuracies = [x['val_acc'] for x in history]\nplt.plot(accuracies, '-x')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.title('Accuracy vs. No. of epochs')\nplt.show()\n\n# plot losses\ntrain_losses = [x.get('train_loss') for x in history]\nval_losses = [x['val_loss'] for x in history]\nplt.plot(train_losses, '-bx')\nplt.plot(val_losses, '-rx')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend(['Training', 'Validation'])\nplt.title('Loss vs. No. of epochs')\nplt.show()\n\n# plor learning rates\nlrs = np.concatenate([x.get('lrs', []) for x in history])\nplt.plot(lrs)\nplt.xlabel('Batch no.')\nplt.ylabel('Learning rate')\nplt.title('Learning Rate vs. Batch no.')\nplt.show()","11edefca":"img, label = get_random_test_image(dataset=test_dataset)\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', test_dataset.classes[label], ', Predicted:', predict_image(img, model))","115cd9b6":"img, label = get_random_test_image(dataset=test_dataset)\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', test_dataset.classes[label], ', Predicted:', predict_image(img, model))","a40f6665":"evaluate(model, valid_dl)","a9a23760":"jovian.commit(project=project_name, environment=None)","d435ae20":"jovian.commit(project=project_name, environment=None)","aa74e624":"#### Our model has learned quite a bit! Plotting the training\/validation loss and accuracy per epoch shows that it got smart rather quickly. You might notice that the accuracy actually decreased in some epochs \u2014 this is likely a result of overfitting, which is a big problem in machine learning, but is unfortunately outside the scope of this post.","e3286ccc":"Before we go further, we should talk a bit about the three \u201ckinds\u201d of data we will be interacting with; training, test, and validation.","f9c671d6":"If you\u2019ll recall from earlier, we did a a single transformation to our images when we created the dataloader to turn each image into a tensor. There are a few other transforms we can apply to the training images to give our model a boost in training.","22c00672":"We use the ImageClassificationBase that we covered earlier, but we tell our network to use a pre-trained ResNet \u2014 torchvision.models.resnet34. We add a new last layer, which takes the inputs from the previous layer of the network and outputs the number of species we are classifying, as previously stated. After we move our model to the GPU","2829cb3c":"#### We will train for 10 epochs, use the Adam optimization function, and have a learning rate of 0.001. These properties are the \u201chyperparameters\u201d of our model, and changing them can make huge differences in training and performance. You should try changing them and see how it goes!\n\n#### Before we train \u2014 one more very important thing! Since we are using a pretrained ResNet, we will want to freeze the model so we only train the last layer (which we added).","5ce7a131":"## **You can modify your hyperparameters,  optimization algorithms , convolutional nets .**","d8a1452d":"## Dataset\n\nA train dataset with 10000 jpg images, 5000 for sunny weaather and 5000 for cloudy weather. Size of the images = 200x200 pixels\nA test dataset with 253 jpg images, 153 for sunny weaather and 100 for cloudy weather. Size of the images = 200x200 pixels.\n\n[https:\/\/www.kaggle.com\/polavr\/twoclass-weather-classification](http:\/\/)","e6f9713d":"\n## **Weather classification using pytorch**","4353cbbf":"We will call the fit function to train our model, which accepts a number of epochs, a learning rate, the training and validation dataloaders, and an optimization function. We will be using Stochastic Gradient Descent, or SGD here.","07506473":"## Using Data augmentation"}}