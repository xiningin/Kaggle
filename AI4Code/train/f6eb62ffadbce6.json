{"cell_type":{"28136cb0":"code","3492ab66":"code","cff512fd":"code","667bf722":"code","5354622a":"code","f7e8b5d8":"code","c46065a5":"code","8ac5ce72":"code","eaa668d3":"code","97964afa":"code","61c0a183":"code","82f42468":"code","489d4424":"code","8414b76d":"code","f891f6c4":"code","38b24a19":"code","fd0225da":"code","6bf084bf":"code","09e1f145":"code","44d8e60e":"markdown","60a55c4d":"markdown"},"source":{"28136cb0":"import time\nimport IPython.display as display\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [12, 9]\nimport cv2\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport sklearn","3492ab66":"def read_img(path):\n    img = cv2.imread(path)\n    img = cv2.resize(img,image_size,interpolation=cv2.INTER_LANCZOS4)\n    return img\n\ndef read_img_step2(img):\n    img = img.astype(np.float32)\n    img = img\/255.\n    img = np.expand_dims(img, axis=0)\n    return img\n\ndef show_img(img_list):\n    title = ['content image','style_image']\n    pos = 1\n    for i in img_list:\n        i = cv2.cvtColor(i, cv2.COLOR_BGR2RGB)\n        plt.subplot(1, len(img_list), pos)\n        plt.title(title[pos-1])\n        plt.imshow(i)\n        pos += 1","cff512fd":"'''Define gram matrix which is used to extract the style features from the style image'''\n\ndef gram_matrix(inputs):\n    shape = K.shape(inputs)\n    F = K.reshape(inputs, (shape[1] * shape[2], shape[0] * shape[3]))\n    num_locations = tf.cast(shape[1]*shape[2], tf.float32)\n    return K.dot(K.transpose(F), F)\n\ndef gram_matrix_tf(inputs):\n    result = tf.linalg.einsum('bijc,bijd->bcd', inputs, inputs)\n    input_shape = tf.shape(inputs)\n    num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n    return result\/num_locations\n\ndef architecture(architecture,content=None,styles=None,weights=None):\n    architecture = architecture\n    if architecture == 'vgg':\n        if styles is None:\n            content_layers = ['block5_conv2']\n            style_layers = ['block1_conv1','block2_conv1','block3_conv1','block4_conv1','block5_conv1']\n            style_layers_weights = [1,1,1,1,1]\n        else:\n            content_layers,style_layers,style_layers_weights = content,styles,weights\n    if architecture == 'resnet':\n        if styles is None:\n            content_layers = ['conv4_block2_1_conv']\n            style_layers = ['conv1_conv','conv2_block1_1_conv','conv3_block1_1_conv','conv4_block1_1_conv','conv5_block1_1_conv']\n            style_layers_weights = [1,1,1,1,1]\n        else:\n            content_layers,style_layers,style_layers_weights = content,styles,weights\n    return architecture, content_layers, style_layers, style_layers_weights\n\n'''\nHere we download a pre-trained VGG19 model trained with Imagenet dataset, to make use of its trained weights. \nThese can help us to extract the style features (i.e. the feature maps) from the style image. \nIt would be a painful process to train a model by your own from scratch and use it here.\n'''\n\ndef vgg_layers(layer_names):\n    if architecture == 'vgg':\n        vgg = tf.keras.applications.VGG16(include_top=False, weights='imagenet',pooling='avg',input_shape=image_size+(3,))\n    if architecture == 'resnet':\n        vgg = tf.keras.applications.ResNet152V2(include_top=False, weights='imagenet',pooling='avg',input_shape=image_size+(3,))\n    vgg.trainable = False\n    outputs = [vgg.get_layer(name).output for name in layer_names]\n    model = tf.keras.Model([vgg.input], outputs)\n    return model\n\nclass StyleContentModel(tf.keras.models.Model):\n\n    def __init__(self, style_layers, content_layers):\n        super().__init__()\n        self.vgg =  vgg_layers(style_layers + content_layers)\n        self.style_layers = style_layers\n        self.content_layers = content_layers\n        self.num_style_layers = len(style_layers)\n        self.num_content_layers = len(content_layers)\n        self.vgg.trainable = False\n\n    def call(self, inputs):\n        inputs = inputs[:,:,:,::-1]\n        inputs = inputs*255\n        inputs = tf.keras.applications.vgg16.preprocess_input(inputs)\n             \n        outputs = self.vgg(inputs)        \n        style_outputs, content_outputs = (outputs[:self.num_style_layers], outputs[self.num_style_layers:])\n        style_outputs = [gram_matrix_tf(style_output) for style_output in style_outputs]\n        content_dict = {content_name:value for content_name, value in zip(self.content_layers, content_outputs)}\n        style_dict = {style_name:value for style_name, value in zip(self.style_layers, style_outputs)}\n        return {'content':content_dict, 'style':style_dict}","667bf722":"def total_variation_loss(x, kind='isotropic'):\n    h, w = x.shape[1], x.shape[2]\n    if kind == 'anisotropic':\n        # take the absolute value between this image, and the image one pixel\n        # down, and one pixel to the right. take the absolute value as\n        # specified by anisotropic loss\n        a = K.abs(x[:, :h-1, :w-1, :] - x[:, 1:, :w-1, :])\n        b = K.abs(x[:, :h-1, :w-1, :] - x[:, :h-1, 1:, :])\n        # add up all the differences\n        return K.sum(a + b)\n    elif kind == 'isotropic':\n        # take the absolute value between this image, and the image one pixel\n        # down, and one pixel to the right. take the square root as specified\n        # by isotropic loss\n        a = K.square(x[:, :h-1, :w-1, :] - x[:, 1:, :w-1, :])\n        b = K.square(x[:, :h-1, :w-1, :] - x[:, :h-1, 1:, :])\n        # take the vector square root of all the pixel differences, then sum\n        # them all up\n        return K.sum(K.pow(a + b, 2))\n    \ndef style_content_loss(outputs):\n    style_loss,content_loss = 0.0,0.0\n    style_outputs,content_outputs = outputs['style'],outputs['content']\n    weights_sum = sum(style_layers_weights)\n    for count,name in enumerate(style_outputs.keys()):\n        style_loss += tf.reduce_mean(((style_outputs[name]-style_targets[name])**2)) * style_layers_weights[count]\n    content_loss = 0.5*tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2) for name in content_outputs.keys()])\n    loss = style_loss*style_weight\/weights_sum + content_loss*content_weight\/num_content_layers\n    return loss\/(image_size[0]*image_size[1])\n        \ndef style_content_loss_tf(outputs):\n    style_outputs = outputs['style']\n    content_outputs = outputs['content']\n    style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2) for name in style_outputs.keys()])\n    style_loss *= style_weight\/num_style_layers\n    content_loss = 0.5*tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2) for name in content_outputs.keys()])\n    content_loss *= content_weight\/num_content_layers\n    loss = style_loss + content_loss\n    return loss\/(image_size[0]*image_size[1])\n    \ndef clip_0_1(image):\n    return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)\n\n@tf.function\ndef train_step(img,smoothing_factor):\n    with tf.GradientTape() as tape:\n        loss1 = smoothing_factor*total_variation_loss(img,kind='isotropic')\n        loss2 = style_content_loss(extractor(img))\n        loss = loss1 + loss2\n    grad = tape.gradient(loss, img)\n    opt.apply_gradients([(grad, img)])\n    img.assign(clip_0_1(img))\n    return loss1, loss2\n\ndef train(img,epochs=10,steps_per_epoch=100,smoothing_factor=1e-3):\n    step = 0\n    start = time.time()\n    for n in range(epochs):\n        for m in range(steps_per_epoch):\n            step += 1\n            loss1,loss2 = train_step(img,smoothing_factor)\n            print(\".\", end='')\n        print(\"\\nTrain step: {}\".format(step),' Loss:',loss1,loss2)\n    end = time.time()\n    print(\"Total time: {:.1f}\".format(end-start))\n    return img","5354622a":"def proc_tensor_to_image(tensor):\n    IMAGENET_MEANS = [103.939, 116.779, 123.68]\n    tensor = tensor[0]\n    tf.math.add(tensor[:,:,0],IMAGENET_MEANS[0])\n    tf.math.add(tensor[:,:,1],IMAGENET_MEANS[1])\n    tf.math.add(tensor[:,:,2],IMAGENET_MEANS[2])\n    tensor = tensor[:,:,::-1]\n    plt.imshow(tensor)\n    return tensor\n\ndef tensor_to_image(tensor):\n    tensor = np.array(tensor*255, dtype=np.uint8)\n    if np.ndim(tensor)==4:\n        tensor = tensor[0]\n    tensor_show = cv2.cvtColor(tensor, cv2.COLOR_BGR2RGB)\n    plt.imshow(tensor_show)\n    return tensor","f7e8b5d8":"image_size = (960,720)","c46065a5":"'''\nSetting up models and layers that used for content\/style reconstruction.\nUse these layers for content and style reconstruction, which will be used to calculate the total loss. \nFor content images, in higher layers of the network, detailed pixel information is lost while the high-level content of the image is preserved.\n'''\n\narchitecture,content_layers,style_layers,style_layers_weights = architecture('vgg',\n                content=['block4_conv2'],\n                styles=['block1_conv1','block1_conv2','block2_conv1','block2_conv2','block3_conv1','block3_conv2','block4_conv1','block4_conv2','block5_conv1','block5_conv2'],\n                weights=[2,1,2,1,2,1,3,2,3,2])\n\nnum_content_layers,num_style_layers = len(content_layers),len(style_layers)\nextractor = StyleContentModel(style_layers, content_layers)\n\nopt = tf.optimizers.Adam(learning_rate=tf.keras.optimizers.schedules.InverseTimeDecay(0.05, decay_steps=500, decay_rate=1))\nstyle_weight=1000\ncontent_weight=1","8ac5ce72":"'''Read images, divide it by 255 and reshape it to n=4'''\n\ncontent_img = read_img('..\/input\/my-photos\/2.jpg')\nstyle_img = read_img('..\/input\/paintings-for-artistic-style-transfer\/Vincent Van Gogh - The Starry Night.jpg')\nshow_img([content_img,style_img])\nstart_content_img, start_style_img = read_img_step2(content_img), read_img_step2(style_img)\nstyle_targets = extractor.call(start_style_img)['style']\ncontent_targets = extractor.call(start_content_img)['content']","eaa668d3":"'''Initialisation of the gradient descent'''\n\nrandom = np.random.rand(1,image_size[1],image_size[0],3)\nblack = np.zeros((1,image_size[1],image_size[0],3))\nwhite = np.ones((1,image_size[1],image_size[0],3))\n# start_content_img, start_style_img\nimage = tf.Variable(start_content_img,dtype=tf.float32)","97964afa":"image = train(image,10,100)","61c0a183":"result1 = tensor_to_image(image)\ncv2.imwrite('result1.jpg', result1)","82f42468":"content_img = read_img('..\/input\/my-photos\/2.jpg')\nstyle_img = read_img('..\/input\/paintings-for-artistic-style-transfer\/Vincent Van Gogh - The Olive Trees.jpg')\nshow_img([content_img,style_img])\nstart_content_img, start_style_img = read_img_step2(content_img), read_img_step2(style_img)\nstyle_targets = extractor.call(start_style_img)['style']\ncontent_targets = extractor.call(start_content_img)['content']\nimage2 = tf.Variable(start_content_img,dtype=tf.float32)","489d4424":"@tf.function\ndef train_step(img,smoothing_factor):\n    with tf.GradientTape() as tape:\n        loss1 = smoothing_factor*total_variation_loss(img,kind='isotropic')\n        loss2 = style_content_loss(extractor(img))\n        loss = loss1 + loss2\n    grad = tape.gradient(loss, img)\n    opt.apply_gradients([(grad, img)])\n    img.assign(clip_0_1(img))\n    return loss1, loss2","8414b76d":"image2 = train(image2,10,100)","f891f6c4":"result2 = tensor_to_image(image2)\ncv2.imwrite('result2.jpg', result2)","38b24a19":"content_img = read_img('..\/input\/my-photos\/2.jpg')\nstyle_img = read_img('..\/input\/paintings-for-artistic-style-transfer\/Edvard Munch - The Scream.jpg')\nshow_img([content_img,style_img])\nstart_content_img, start_style_img = read_img_step2(content_img), read_img_step2(style_img)\nstyle_targets = extractor.call(start_style_img)['style']\ncontent_targets = extractor.call(start_content_img)['content']\nimage3 = tf.Variable(start_content_img,dtype=tf.float32)","fd0225da":"@tf.function\ndef train_step(img,smoothing_factor):\n    with tf.GradientTape() as tape:\n        loss1 = smoothing_factor*total_variation_loss(img,kind='isotropic')\n        loss2 = style_content_loss(extractor(img))\n        loss = loss1 + loss2\n    grad = tape.gradient(loss, img)\n    opt.apply_gradients([(grad, img)])\n    img.assign(clip_0_1(img))\n    return loss1, loss2","6bf084bf":"image3 = train(image3,10,100)","09e1f145":"result3 = tensor_to_image(image3)\ncv2.imwrite('result3.jpg', result3)","44d8e60e":"# -------------- code starts here -------------- ","60a55c4d":"This notebook is written according to this research (Gatys et al. 2015):\n\n[A Neural Algorithm of Artistic Style](https:\/\/arxiv.org\/abs\/1508.06576)\nor\n[Image Style Transfer Using Convolutional Neural Networks](https:\/\/www.cv-foundation.org\/openaccess\/content_cvpr_2016\/papers\/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf) \n"}}