{"cell_type":{"84135c35":"code","495dd84e":"code","eb0df3fa":"code","8a02f2ee":"code","69172b2f":"code","559ae0d9":"code","516e96d6":"code","bba5c5b4":"code","ca98c9bc":"code","8626ca8c":"code","c5e4aaf7":"code","9f593679":"code","d718feea":"code","0e87aacc":"code","a4ae93db":"code","8de289f0":"code","fb5aff44":"code","2454de21":"code","718807f3":"code","437e2be4":"code","547f4857":"code","a9cb08a8":"code","698292c4":"code","229ec791":"code","39aad786":"code","eacb79f3":"code","8b108521":"code","45ff354b":"code","cd4eefbd":"code","26112c0c":"code","62b0dffc":"code","4863ac3a":"code","75863974":"code","7ba8a86d":"code","804e1b6d":"code","5cb10b73":"code","a1ee9e63":"code","3125407e":"code","4c1bd79a":"code","5289fbd6":"code","6c9b707f":"code","7e6beb4b":"markdown","26b8a571":"markdown","ebcc37fa":"markdown","649d1d2b":"markdown","f6fb08c7":"markdown","cb362bea":"markdown","461adf3d":"markdown","a045747b":"markdown","f42b0566":"markdown","ae5fd4c6":"markdown","ec054ba0":"markdown","f7a2c0bf":"markdown"},"source":{"84135c35":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import recall_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import StackingClassifier","495dd84e":"PD_Data = pd.read_csv('Data - Parkinsons')","eb0df3fa":"PD_Data.shape","8a02f2ee":"PD_Data.info()\n# All columns are either int or float so no need to convert dtype of any column","69172b2f":"PD_Data.head()","559ae0d9":"# Name can be converted to index here\nPD_Data.set_index('name', inplace = True) \nPD_Data.head()","516e96d6":"PD_Data.info()","bba5c5b4":"# Lets check the distibution of each attribute\nPD_Data.describe().transpose()","ca98c9bc":"# For univariate analysis check histogram plot for each attribute\n# sns.distplot(PD_Data['MDVP:Fo(Hz)'],kde=True)\nPD_Data.hist(figsize=(20,30))","8626ca8c":"# Getting all plots in one figure\nplt.figure(figsize= (60,40)) \npos=1\nfor feature in PD_Data.columns:\n    plt.subplot(6, 4, pos) \n    sns.distplot(PD_Data[feature],kde=True)\n    pos += 1 \n    ","c5e4aaf7":"sns.pairplot(PD_Data,hue = 'status', diag_kind='kde')","9f593679":"# Nothing is clear from pair plot due to high number of attributes","d718feea":"# Checking correaltion between attributes","0e87aacc":"correlation_matrix = PD_Data.corr()\nfig, ax = plt.subplots(figsize=(20,20))\nsns.heatmap(correlation_matrix, annot=True, ax=ax)\n\nplt.xticks(range(len(correlation_matrix.columns)), correlation_matrix.columns)\nplt.yticks(range(len(correlation_matrix.columns)), correlation_matrix.columns)","a4ae93db":"# 1.Most of the attributes related to frequency are right skewed\n# 2.By looking at overall distribution we can predict that there are some outliers in the data\n# 3.Attributes related to several measures of variation in fundamental frequency are highly correleated\n# 4.Also Attributes related to Several measures of variation in amplitude are highly correleated\n# 5.Later we can try our model by using only one column of the all related column instead of using all related colunms in our model to improve accuracy","8de289f0":"# Split the dataset into training and test set in the ratio of 70:30\nX = PD_Data.drop('status',axis=1)\nY = PD_Data['status']\nX_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.30, random_state=1)","fb5aff44":"print(X_test.count())\nprint(X_train.count())","2454de21":"# Check for any missing value\nPD_Data[PD_Data.isnull() == True].count()","718807f3":"# Check if there are any unexpected zero value for attribute other than target variable 'status'\nPD_Data[PD_Data[PD_Data.columns] == 0].count()","437e2be4":"# Scaling the data first as we have to use KNN model here\nscaler = StandardScaler()\nscaler.fit(X_train)\n\nX_train_scaled = scaler.transform(X_train.loc[:])\nX_train.loc[:] = X_train_scaled\n\nX_test_scaled = scaler.transform(X_test.loc[:])\nX_test.loc[:] = X_test_scaled","547f4857":"X_train.head()","a9cb08a8":"X_test.head()","698292c4":"model_Report = pd.DataFrame(columns={'Model_Name','Training data Accuracy','Testing data Accuracy','Recall','Precision','F1 Score'})","229ec791":"def apply_Model(model,X_train,X_test,model_name):\n    model.fit(X_train, y_train)\n    y_predict = model.predict(X_test)    \n    tn, fp, fn, tp  = metrics.confusion_matrix(y_test,y_predict).ravel()\n    train_score = model.score(X_train,y_train)\n    test_score = model.score(X_test,y_test)\n    recall = round(tp\/(tp+fn), 3)\n    precision = round(tp\/(tp+fp), 3)\n    f1_score =  round(2*precision*recall\/(precision + recall), 3)\n    return {'Model_Name':model_name,'Training data Accuracy':train_score,'Testing data Accuracy':test_score,'Recall':recall,'Precision':precision,'F1 Score':f1_score}","39aad786":"'''get predict for optimal value of k'''\ndef apply_optimal_KNN(n,X_train,X_test,model_name):\n    nlist = list(range(1,n))\n    oddN = list(filter(lambda x: x % 2 != 0, nlist))\n    ac_scores = []\n    y_preds = []\n    \n    for k in oddN:\n        knn = KNeighborsClassifier(n_neighbors=k)\n        knn.fit(X_train, y_train)\n        y_pred = knn.predict(X_test)\n        scores = accuracy_score(y_test, y_pred)\n        ac_scores.append(scores)\n        y_preds.append(y_pred)\n    #Mininmu MSE\n    MSE = [1 - x for x in ac_scores]\n    optimal_k = oddN[MSE.index(min(MSE))]\n    print('Optimal k value for given number is :{} ' .format(optimal_k))\n    \n    optimal_y_predict = y_preds[MSE.index(min(MSE))]\n    \n    knn_optimal = KNeighborsClassifier(n_neighbors=optimal_k)\n    knn_optimal.fit(X_train, y_train)\n    optimal_y_predict_train = knn_optimal.predict(X_train)\n    \n    \n    tn, fp, fn, tp  = metrics.confusion_matrix(y_test,optimal_y_predict).ravel()\n    test_score = accuracy_score(y_test, optimal_y_predict)\n    train_score = accuracy_score(y_train, optimal_y_predict_train)    \n    recall = round(tp\/(tp+fn), 3)\n    precision = round(tp\/(tp+fp), 3)\n    f1_score =  round(2*precision*recall\/(precision + recall), 3)\n    \n    return {'Model_Name':model_name,'Training data Accuracy':train_score,'Testing data Accuracy':test_score,'Recall':recall,'Precision':precision,'F1 Score':f1_score}","eacb79f3":"#apply all models together and get report\nlr = LogisticRegression(solver = 'liblinear')\nlr_result = apply_Model(lr,X_train,X_test,'Logistic Regression')\nmodel_Report = model_Report.append(lr_result,ignore_index=True)\n\nknn_result = apply_optimal_KNN(20,X_train,X_test,'KNN')\nmodel_Report = model_Report.append(knn_result,ignore_index=True)\n\nnb = GaussianNB()\nnb_result= apply_Model(nb,X_train,X_test,'Naive Bayes')\nmodel_Report = model_Report.append(nb_result,ignore_index=True)\n\nsvm = SVC(gamma = 'auto', kernel= 'poly', degree=2)\nsvm_result= apply_Model(svm,X_train,X_test,'SVM')\nmodel_Report = model_Report.append(svm_result,ignore_index=True)","8b108521":"model_Report.set_index('Model_Name')","45ff354b":"sclf = StackingClassifier(estimators=[('lr',lr),('knn',knn),('nb',nb),('rfcl',rfcl)], final_estimator=LogisticRegression())\nsclf.fit(X_train,y_train)\nsclf.score(X_test,y_test)","cd4eefbd":"model_Report_ET = pd.DataFrame(columns={'Model_Name','Training data Accuracy','Testing data Accuracy','Recall','Precision','F1 Score'})","26112c0c":"# Ensemblemodel with original data\ndTree = DecisionTreeClassifier(criterion = 'gini', random_state=1)\nresult_DT = apply_Model(dTree,X_train,X_test,'Decision Tree')\nmodel_Report_ET = model_Report_ET.append(result_DT,ignore_index=True)\n\ndTreeR = DecisionTreeClassifier(criterion = 'gini', max_depth=3,  random_state=1)\nresult_DTR = apply_Model(dTreeR,X_train,X_test,'Decision Tree Pruning')\nmodel_Report_ET = model_Report_ET.append(result_DTR,ignore_index=True)\n\n\nbgcl = BaggingClassifier(n_estimators=30,random_state=1)\nresult_Bagging = apply_Model(bgcl,X_train,X_test,'Bagging')\nmodel_Report_ET = model_Report_ET.append(result_Bagging,ignore_index=True)\n\nabcl = AdaBoostClassifier( n_estimators=50,random_state=1)\nresult_Adaboosting = apply_Model(abcl,X_train,X_test,'AdaBoosting')\nmodel_Report_ET = model_Report_ET.append(result_Adaboosting,ignore_index=True)\n\ngbcl = GradientBoostingClassifier(n_estimators=10,random_state=1)\nresult_Gboosting = apply_Model(gbcl,X_train,X_test,'GradientBoosting')\nmodel_Report_ET = model_Report_ET.append(result_Gboosting,ignore_index=True)\n\nrfcl = RandomForestClassifier(n_estimators = 50,random_state=1,max_features=13)\nresult_RF= apply_Model(rfcl,X_train,X_test,'Random Forest')\nmodel_Report_ET = model_Report_ET.append(result_RF,ignore_index=True)","62b0dffc":"model_Report_ET","4863ac3a":"# With data frame where columns are dropped\nX_train_modified = X_train.drop(columns=['MDVP:Jitter(Abs)', 'MDVP:RAP', 'MDVP:PPQ', 'Jitter:DDP','MDVP:Shimmer(dB)', 'Shimmer:APQ3', 'Shimmer:APQ5', 'MDVP:APQ','Shimmer:DDA'],axis=1)\nX_train_modified.head()","75863974":"X_test_modified = X_test.drop(columns=['MDVP:Jitter(Abs)', 'MDVP:RAP', 'MDVP:PPQ', 'Jitter:DDP','MDVP:Shimmer(dB)', 'Shimmer:APQ3', 'Shimmer:APQ5', 'MDVP:APQ','Shimmer:DDA'],axis=1)\nX_test_modified.head()","7ba8a86d":"#Checking for Feature importance","804e1b6d":"feat_importance = dTreeR.tree_.compute_feature_importances(normalize=False)","5cb10b73":"imp_feature_df=pd.DataFrame(feat_importance, columns = [\"Imp\"], index = X_train.columns)\nimp_feature_df","a1ee9e63":"# With data frame where columns are dropped\nX_train_modified = X_train.drop(columns=['MDVP:Jitter(Abs)', 'MDVP:RAP', 'MDVP:PPQ', 'Jitter:DDP','MDVP:Shimmer(dB)', 'Shimmer:APQ3', 'Shimmer:APQ5', 'MDVP:APQ','Shimmer:DDA'],axis=1)\nX_train_modified.head()","3125407e":"X_test_modified = X_test.drop(columns=['MDVP:Jitter(Abs)', 'MDVP:RAP', 'MDVP:PPQ', 'Jitter:DDP','MDVP:Shimmer(dB)', 'Shimmer:APQ3', 'Shimmer:APQ5', 'MDVP:APQ','Shimmer:DDA'],axis=1)\nX_test_modified.head()","4c1bd79a":"# Ensemblemodel with modified data (Removing columns)\ndTree = DecisionTreeClassifier(criterion = 'gini', random_state=1)\nresult_DT_Modified = apply_Model(dTree,X_train_modified,X_test_modified,'Decision Tree Modified')\nmodel_Report_ET = model_Report_ET.append(result_DT_Modified,ignore_index=True)\n\ndTreeR = DecisionTreeClassifier(criterion = 'gini', max_depth=3,  random_state=1)\nresult_DTR_Modified = apply_Model(dTreeR,X_train_modified,X_test_modified,'Decision Tree Pruning Modified')\nmodel_Report_ET = model_Report_ET.append(result_DTR_Modified,ignore_index=True)\n\nbgcl = BaggingClassifier(n_estimators=30,random_state=1)\nresult_Bagging_Modified = apply_Model(bgcl,X_train_modified,X_test_modified,'Bagging Modified')\nmodel_Report_ET = model_Report_ET.append(result_Bagging_Modified,ignore_index=True)\n\nabcl = AdaBoostClassifier( n_estimators=50,random_state=1)\nresult_Adaboosting_Modified = apply_Model(abcl,X_train_modified,X_test_modified,'AdaBoosting Modified')\nmodel_Report_ET = model_Report_ET.append(result_Adaboosting_Modified,ignore_index=True)\n\ngbcl = GradientBoostingClassifier(n_estimators=10,random_state=1)\nresult_Gboosting_Modified = apply_Model(gbcl,X_train_modified,X_test_modified,'GradientBoosting Modified')\nmodel_Report_ET = model_Report_ET.append(result_Gboosting_Modified,ignore_index=True)\n\nrfcl = RandomForestClassifier(n_estimators = 50,random_state=1,max_features=13)\nresult_RF_Modified = apply_Model(rfcl,X_train_modified,X_test_modified,'Random Forest Modified')\nmodel_Report_ET = model_Report_ET.append(result_RF_Modified,ignore_index=True)","5289fbd6":"model_Report_ET.set_index('Model_Name')","6c9b707f":" model_Report","7e6beb4b":"#### Split the dataset into training and test set","26b8a571":"##### 1.There is no significant difference in accuracy of varipus Ensemble Technique models although we can see the best accuracy is found by Boosting technique","ebcc37fa":"#### Removing columns which are highly correlated","649d1d2b":"#### Train at least 3 standard classification algorithms","f6fb08c7":"##### 3.Effect of Data modification (removing releated column of Jitter and shimmer): removing mentioned columns had no big impact in accuracy","cb362bea":"#### Scale the data if necessary, get rid of missing values","461adf3d":"##### Summary of applied Models\n","a045747b":"#### Train a meta-classifier and note the accuracy on test data","f42b0566":"#### Train using Ensemble Technique","ae5fd4c6":"##### 2.With original data bagging and Random forect giving same accuracy 86% but boosting technique giving better accuracy of 89%","ec054ba0":"#### Observations on data","f7a2c0bf":"#### Apply model with modified data"}}