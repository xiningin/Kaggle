{"cell_type":{"85892c31":"code","13fb9111":"code","c1e8c6b3":"code","35cced09":"code","38b0655f":"code","98e2282d":"code","c5e93a75":"code","d7417db0":"code","b1409c48":"code","ddf2120e":"code","307f2cf6":"code","7cd1973c":"code","5f60cccc":"code","ba35d90e":"code","b205458e":"code","2069194d":"code","a5d7a17e":"code","1cbbff3d":"code","ba5c9eb6":"code","31ab8923":"code","369e1640":"code","15f9d248":"code","9a2fc9c9":"code","3ff1042a":"code","48c3643a":"code","193d7034":"code","23efc525":"code","b2f757bd":"code","98816257":"code","16a8746f":"code","ce90bd80":"code","382c95a8":"code","4f7174ac":"code","32c47878":"code","95f43faf":"code","12c2c35d":"code","6f1c00ae":"code","4bb7b1c4":"code","3c212859":"code","cb951bf7":"code","1e146b19":"code","9b8d05e1":"code","16fde908":"code","5d5c1dd2":"code","5d85fdeb":"code","dedd427c":"code","543eb755":"code","f25cf222":"code","50b5e9c1":"code","383b9b23":"code","0595a5f0":"code","50926d3c":"code","3f77a2c1":"code","9691f07e":"code","936f10b8":"code","7745de9b":"code","f552533a":"code","e13b9500":"code","2a1960c7":"code","ee9de828":"code","ca6af7f2":"code","39632e65":"code","7cad8eda":"code","990444ff":"code","0bcde607":"code","6b937129":"code","976a8b79":"code","e539ff2e":"markdown","ad3d4f1f":"markdown","113d1fae":"markdown","039049e9":"markdown","5dd7bcd1":"markdown","e9db9594":"markdown","eea08946":"markdown","f763ab2b":"markdown","c526847e":"markdown","e9355e80":"markdown","b826fec4":"markdown","8b6a9f37":"markdown","d74d6cf5":"markdown","eb8908c9":"markdown","ed8ab327":"markdown","49cf006b":"markdown","e9a05b90":"markdown","8590053a":"markdown","62497ab9":"markdown","dfeec567":"markdown","0898703b":"markdown","af10e754":"markdown","225d96c5":"markdown","f5fb55e9":"markdown","09f7726e":"markdown"},"source":{"85892c31":"# Installing the libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 1000)\npd.set_option('display.width', 200)","13fb9111":"# Loading the datasets\ntraining = pd.read_csv('..\/input\/titanic\/train.csv')\ntraining.head()","c1e8c6b3":"testing = pd.read_csv('..\/input\/titanic\/test.csv')\ntesting.head()","35cced09":"# Quick look at the column names, data types, and shape of each dataframe\nprint('Training dataset')\nprint(training.dtypes)\nprint(training.shape, end = '\\n\\n')\nprint('Testing dataset')\nprint(testing.dtypes)\nprint(testing.shape, end = '\\n\\n')","38b0655f":"training.describe()","98e2282d":"# Correlation and Heatmap \nprint(training[['Survived','Pclass','Age','SibSp','Parch','Fare']].corr(), end ='\\n\\n')\nsns.heatmap(training[['Survived','Pclass','Age','SibSp','Parch','Fare']].corr(),annot=True)\nplt.show()","c5e93a75":"# Organizing the variables according to their data types (nummerical or categorical) to easily navigate these similar groups when creating plots.\ntraining_num = training[['Age','SibSp','Parch','Fare']]\ntraining_cat = training[['Survived', 'Pclass','Sex','Embarked', 'Ticket', 'Cabin']]","d7417db0":"# Starting with the numerical data, we create a loop to show how the distribution of the data looks like\nfor i in training_num.columns:\n    plt.hist(training_num[i])\n    plt.title(i)\n    plt.show()","b1409c48":"# For the categorical data, we create barplots to compare frequencies of data\nfor i in training_cat:\n    x = training_cat[i].value_counts().index\n    y = training_cat[i].value_counts()\n    sns.barplot(x,y).set_title(i)\n    plt.show()","ddf2120e":"# Pivot for the numerical variables \npd.pivot_table(training, index = 'Survived', values = training_num)","307f2cf6":"# Distribution plot for granularity\ng = sns.FacetGrid(training, col='Survived')\ng = g.map(sns.distplot, \"Age\")","7cd1973c":"# Breaking down SibSp and Parch to check survival per value\nprint(pd.pivot_table(training, index = 'Survived', columns = 'SibSp', values = 'Ticket', aggfunc = 'count'), end = '\\n\\n')\nprint(pd.pivot_table(training, index = 'Survived', columns = 'Parch', values = 'Ticket', aggfunc = 'count'))","5f60cccc":"# For the categorical variables\nprint(pd.pivot_table(training_cat, index = 'Survived', columns = 'Pclass', values = 'Ticket', aggfunc = 'count'),end = '\\n\\n') \nprint(pd.pivot_table(training_cat, index = 'Survived', columns = 'Sex', values = 'Ticket', aggfunc = 'count'), end = '\\n\\n')\nprint(pd.pivot_table(training_cat, index = 'Survived', columns = 'Embarked', values = 'Ticket', aggfunc = 'count'), end = '\\n')","ba35d90e":"# Getting the count of null values\nprint(training.isnull().sum(), end = '\\n\\n')\nprint(testing.isnull().sum())","b205458e":"# Getting the Mean Age of the entire set and each of the sex category\nprint(\"Mean Age - \", training.Age.mean())\nprint(\"Median Age - \", training.Age.median())\nprint(\"Mean Age (Male) - \", training.groupby(training['Sex'] == 'male')['Age'].mean()[1])\nprint(\"Mean Age (Female) - \", training.groupby(training['Sex'] == 'female')['Age'].mean()[1])","2069194d":"# Imputing the Null Age using the Mean Age according to Sex (for train data)\nmeanagemale1 = training.groupby(training['Sex'] == 'male')['Age'].mean()[1]\nmeanagefemale1 = training.groupby(training['Sex'] == 'female')['Age'].mean()[1]\ntraining['Age']  = training.groupby(training['Sex'] == 'male')['Age'].fillna(meanagemale1)\ntraining['Age']  = training.groupby(training['Sex'] == 'female')['Age'].fillna(meanagefemale1)","a5d7a17e":"# Imputing the Null Age using the Mean Age according to Sex (for test data)\nmeanagemale2 = testing.groupby(testing['Sex'] == 'male')['Age'].mean()[1]\nmeanagefemale2 = testing.groupby(testing['Sex'] == 'female')['Age'].mean()[1]\ntesting['Age']  = testing.groupby(testing['Sex'] == 'male')['Age'].fillna(meanagemale2)\ntesting['Age']  = testing.groupby(testing['Sex'] == 'female')['Age'].fillna(meanagefemale2)","1cbbff3d":"# Checking if all the missing Age values have been replaced:\nprint('training null Age: ', training.Age.isnull().sum())\nprint('testing null Age: ', testing.Age.isnull().sum())","ba5c9eb6":"# Showing the data of 2 passengers with missing Embarked data\ntraining.loc[training['Embarked'].isnull()]","31ab8923":"# Showing the passengers with similarities to the 2 passengers with missing Embarked data based on Ticket and Cabin \ntraining.loc[training['Ticket'].str.startswith('113') & training['Cabin'].str.startswith('B')]","369e1640":"# Imputing the missing Embarked Fare value using the Mode class\ntraining['Embarked'] = training['Embarked'].fillna(\"S\")\n\n# To check if Embarked the missing values have been replaced:\nprint(testing.Embarked.isnull().sum())","15f9d248":"# Imputing the missing Fare value using the Mean data\ntesting['Fare'] = testing['Fare'].fillna(testing['Fare'].mean())\n\n# To check if Fare the missing value has been replaced:\nprint(testing.Fare.isnull().sum())","9a2fc9c9":"# Just to show first how the Cabin data look like\ntraining.Cabin.value_counts()","3ff1042a":"# We will classify passengers from those occupying single cabin to multiple ones.\ntraining['cabin_multiple'] = training.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\n\n# Check the result\ntraining.cabin_multiple.value_counts()","48c3643a":"# Next is to create another classification based on the 1st letter of the passenger's cabin.\ntraining['cabin_adv'] = training.Cabin.apply(lambda x: str(x)[0])\n\n# Check the result\ntraining.cabin_adv.value_counts()","193d7034":"# Just to show first how the Ticket data look like\ntraining.Ticket.value_counts()","23efc525":"# Create classification whether Ticket is numerical or not\ntraining['num_ticket'] = training.Ticket.apply(lambda x: 'Yes' if x.isnumeric() else 'No')\n\n# Check the result\ntraining.num_ticket.value_counts()","b2f757bd":"# For tickets that start with letter, we classify them further\ntraining['ticket_letters'] = training.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('\/','').lower() if len(x.split(' ')[:-1]) >0 else 0)\n\n# Check the result\ntraining.ticket_letters.value_counts()","98816257":"# Just to show first how the Name data look like\ntraining.Name.value_counts()","16a8746f":"# Stripping off the title from Name\ntraining['name_title'] = training.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\n\n# Check the result\ntraining.name_title.value_counts()","ce90bd80":"# Cleaning up, replacing Ms with Miss\ntraining['name_title'] = training['name_title'].replace(['Ms'], 'Miss')\n\n# Check the result\ntraining.name_title.value_counts()","382c95a8":"# Combining SibSp and Parch since they are closely-related variables\ntraining['Familysize'] = training['SibSp'] + training['Parch'] + 1\n\ntraining['Familysize'].value_counts()","4f7174ac":"# For these feature engineered variables, let us try Ritesh Patil's approach in analyzing variables versus Survived\nprint(training[['cabin_multiple', 'Survived']].groupby(['cabin_multiple'], as_index=False).mean().sort_values(by='Survived', ascending=False), end = '\\n\\n')\nprint(training[['cabin_adv', 'Survived']].groupby(['cabin_adv'], as_index=False).mean().sort_values(by='Survived', ascending=False), end = '\\n\\n')\nprint(training[['num_ticket', 'Survived']].groupby(['num_ticket'], as_index=False).mean().sort_values(by='Survived', ascending=False), end = '\\n\\n')\nprint(training[['ticket_letters', 'Survived']].groupby(['ticket_letters'], as_index=False).mean().sort_values(by='Survived', ascending=False), end = '\\n\\n')\nprint(training[['name_title', 'Survived']].groupby(['name_title'], as_index=False).mean().sort_values(by='Survived', ascending=False), end = '\\n\\n')\nprint(training[['Familysize', 'Survived']].groupby(['Familysize'], as_index=False).mean().sort_values(by='Survived', ascending=False))","32c47878":"# Applying the feature engineering procedures to Testing dataset\ntesting['cabin_multiple'] = testing.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\ntesting['cabin_adv'] = testing.Cabin.apply(lambda x: str(x)[0])\ntesting['num_ticket'] = testing.Ticket.apply(lambda x: 'Yes' if x.isnumeric() else 'No')\ntesting['ticket_letters'] = testing.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('\/','').lower() if len(x.split(' ')[:-1]) >0 else 0)\ntesting['name_title'] = testing.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\ntesting['Familysize'] = testing['SibSp'] + testing['Parch'] + 1","95f43faf":"testing['Familysize'].isnull().sum()","12c2c35d":"# Normalizing Fare as it is right skewed also due to the outlier \ntraining['norm_fare'] = np.log(training.Fare+1)\ntesting['norm_fare'] = np.log(testing.Fare+1)","6f1c00ae":"# To check the result after applying Logarithmic Normalization\ng = sns.FacetGrid(training, col='Survived')\ng = g.map(sns.distplot, \"norm_fare\")","4bb7b1c4":"# Creating Train Test column to distinguish data and combining both datasets just to make some steps applicable in one go\ntraining['train_test'] = 1\ntesting['train_test'] = 0\ntesting['Survived'] = np.NaN\nalldata = pd.concat([training,testing])\n\n# Converting Pclass to category for pd.get_dummies()\nalldata.Pclass = alldata.Pclass.astype(str)\n\n# Creating dummy variables for the category data\nalldummies = pd.get_dummies(alldata[['Pclass','Sex','Age','Familysize','norm_fare','Embarked','cabin_adv','cabin_multiple','name_title','train_test']])\n\n# Splitting to train and test again to cast final objects for the model training\nX_train = alldummies[alldummies.train_test == 1].drop(['train_test'], axis =1)\nX_test = alldummies[alldummies.train_test == 0].drop(['train_test'], axis =1)\ny_train = alldata[alldata.train_test==1].Survived\ny_train.shape","3c212859":"# Scaling the data \nfrom sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\nalldummies_scaled = alldummies.copy()\nalldummies_scaled[['Age','Familysize','norm_fare']]= scale.fit_transform(alldummies_scaled[['Age','Familysize','norm_fare']])\nalldummies_scaled\n\nX_train_scaled = alldummies_scaled[alldummies_scaled.train_test == 1].drop(['train_test'], axis =1)\nX_test_scaled = alldummies_scaled[alldummies_scaled.train_test == 0].drop(['train_test'], axis =1)\n\nprint(X_train_scaled.isnull().sum(), end = '\\n\\n')\nprint(X_test_scaled.isnull().sum(), end = '\\n\\n')","cb951bf7":"# Importing the needed libraries \nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import VotingClassifier","1e146b19":"# Gaussian Naive Bayes\ngnb = GaussianNB()\ncv = cross_val_score(gnb, X_train_scaled, y_train, cv = 5)\nprint(cv)\nprint(cv.mean())","9b8d05e1":"# Logistic Regression\nlr = LogisticRegression(max_iter = 2000)\ncv = cross_val_score(lr, X_train_scaled, y_train, cv = 5)\nprint(cv)\nprint(cv.mean())","16fde908":"# Decision Tree\ndt = tree.DecisionTreeClassifier(random_state = 1)\ncv = cross_val_score(dt, X_train_scaled, y_train, cv=5)\nprint(cv)\nprint(cv.mean())","5d5c1dd2":"# K Nearest Neighbors\nknn = KNeighborsClassifier()\ncv = cross_val_score(knn, X_train_scaled, y_train, cv=5)\nprint(cv)\nprint(cv.mean())","5d85fdeb":"# Random Forest\nrf = RandomForestClassifier(random_state = 1)\ncv = cross_val_score(rf, X_train_scaled, y_train, cv=5)\nprint(cv)\nprint(cv.mean())","dedd427c":"# Support Vector Classifier\nsvc = SVC(probability = True)\ncv = cross_val_score(svc, X_train_scaled, y_train, cv=5)\nprint(cv)\nprint(cv.mean())","543eb755":"# Xtreme Gradient Boosting\nxgb = XGBClassifier(random_state = 1, eval_metric = 'error')\ncv = cross_val_score(xgb, X_train_scaled, y_train, cv=5)\nprint(cv)\nprint(cv.mean())","f25cf222":"%%html\n<style>\n    table {\n        display: inline-block\n    }\n<\/style>","50b5e9c1":"# Importing the libraries\nfrom sklearn.model_selection import GridSearchCV","383b9b23":"# Logistic Regression Grid Search\nlr = LogisticRegression(random_state = 10, max_iter = 1000)\nparams = {\"solver\": [\"lbfgs\",\"liblinear\"],\n          'penalty':['l1','l2'], \n          'C': [1, 3, 10, 20, 30, 40]}\nlr_gs = GridSearchCV(lr, param_grid = params,cv=5, n_jobs=-1)\nbest_lrgs = lr_gs.fit(X_train_scaled,y_train)\nprint(best_lrgs.best_score_,'\\n', best_lrgs.best_params_)","0595a5f0":"# K Nearest Neighbor Grid Search\nknn = KNeighborsClassifier()\nparams = {'n_neighbors':[3,5,7,9], \n          'weights':['uniform','distance'], \n          'algorithm': ['ball_tree','kd_tree','auto'], \n          'p':[1,2]}\nknn_gs = GridSearchCV(knn, param_grid = params, cv=5, n_jobs=-1)\nbest_knngs = knn_gs.fit(X_train_scaled, y_train)\nprint(best_knngs.best_score_,'\\n', best_knngs.best_params_)","50926d3c":"# Support Vector Classifier Grid Search\nsvc = SVC(probability = True)\nparams = [{'kernel': ['rbf'], \n           'gamma': [.1,.5,1,2,5,10], \n           'C':[.1,1,10,100,1000]}, \n          {'kernel': ['linear'], \n           'C': [.1,1,10,100,1000]}, \n          {'kernel': ['poly'], \n           'degree':[2,3,4,5], \n           'C': [.1,1,10,100,1000]}]\nsvc_gs = GridSearchCV(svc, param_grid = params, cv=5, n_jobs=-1)\nbest_svcgs = svc_gs.fit(X_train_scaled, y_train)\nprint(best_svcgs.best_score_,'\\n', best_svcgs.best_params_)","3f77a2c1":"# Random Forest Grid Search\nrf = RandomForestClassifier(random_state = 1)\nparams = {'n_estimators':[550],\n          'criterion':['gini','entropy'],\n          'max_depth': [25],\n          'max_features':['auto','sqrt',10],\n          'min_samples_leaf': [2,3],\n          'min_samples_split': [2,3]}\nrf_gs = GridSearchCV(rf, param_grid = params, cv=5, n_jobs=-1)\nbest_rfgs = rf_gs.fit(X_train_scaled, y_train)\nprint(best_rfgs.best_score_,'\\n', best_rfgs.best_params_)","9691f07e":"# Xtreme Gradient Boosting Grid Search\nxgb = XGBClassifier(random_state=1, eval_metric = 'error')\nparams = {'n_estimators': [550],\n    'colsample_bytree': [0.85],\n    'max_depth': [10],\n    'reg_alpha': [1],\n    'reg_lambda': [2, 5, 10],\n    'subsample': [0.65],\n    'learning_rate':[0.5],\n    'gamma':[.5,1,2],\n    'min_child_weight':[0.01],\n    'sampling_method': ['uniform']}\nxgb_gs = GridSearchCV(xgb, param_grid = params,cv=5, n_jobs=-1)\nbest_xgbgs = xgb_gs.fit(X_train_scaled, y_train)\nprint(best_xgbgs.best_score_,'\\n', best_xgbgs.best_params_)","936f10b8":"# For the Cross Validation (Baseline), we will use Hard Voting Classifier using only the 5 best performing estimators\nhvc_cv = VotingClassifier(estimators = [('lr', lr),('knn', knn),('rf',rf),('svc', svc),('xgb',xgb)], voting = 'hard')\ncv = cross_val_score(hvc_cv, X_train_scaled, y_train, cv=5)\nprint(cv)\nprint(cv.mean())    ","7745de9b":"# Fitting the CV ensemble and predicting the Test data\nhvc_cv.fit(X_train_scaled, y_train)\nyhat_base_hvc_cv = hvc_cv.predict(X_train_scaled).astype(int)\npred_data = {'Passenger ID': training.PassengerId, 'Survived_cv': yhat_base_hvc_cv}\noutput1 = pd.DataFrame(data = pred_data)","f552533a":"# For the Grid Search CV, we will use Hard Voting Classifier involving all the estimators\nhvc_gs = VotingClassifier(estimators = [('lr', best_lrgs),('knn', best_knngs),('rf',best_rfgs),('svc', best_svcgs),('xgb',best_xgbgs)], voting = 'hard')\ncv1 = cross_val_score(hvc_gs, X_train_scaled, y_train, cv=5)\nprint(cv1)\nprint(cv1.mean()) ","e13b9500":"# Fitting the GS ensemble and predicting the Test data\nhvc_gs.fit(X_train_scaled, y_train)\nyhat_base_hvc_gs = hvc_gs.predict(X_test_scaled).astype(int)\npred_data2 = {'Passenger ID': testing.PassengerId, 'Survived_gs': yhat_base_hvc_gs}\noutput2 = pd.DataFrame(data = pred_data2)","2a1960c7":"# Combining the outputs for better comparison view\nalloutput = output1.join(output2['Survived_gs'])\nalloutput['Matched?'] = alloutput.apply(lambda x: 1 if x.Survived_cv == x.Survived_gs else 0, axis = 1)","ee9de828":"# Creating Summary Table to compare the Prediction outcomes between the ensembles.\nsum_ = alloutput[['Survived_cv','Survived_gs']].sum().astype(int)\ncount = alloutput[['Survived_cv','Survived_gs']].count().values.astype(int)\naop = pd.DataFrame(columns = ['Survived', \"Not_Survived\"])\naop['Survived'] = sum_\naop['Not_Survived'] = count - sum_\naop","ca6af7f2":"# This is to show in detail the number of disparity between the 2 and the specific outcomes where the discrepancy occurred\nalloutput[['Survived_cv','Survived_gs', 'Passenger ID']].groupby(['Survived_cv','Survived_gs'], as_index = False).count().sort_values(by='Passenger ID', ascending=False)","39632e65":"# Saving the Predictions to CSV file\noutput1.to_csv(\"Predictioncv.csv\", index=False)\noutput2.to_csv(\"Predictiongs.csv\", index=False)\nalloutput.to_csv(\"Comparison.csv\", index=False)","7cad8eda":"# Applying the Cross Validation ensemble to the Train set to predict Survival\nhvc_cv.fit(X_train_scaled, y_train)\nyhat_base_hvc_cv_forME = hvc_cv.predict(X_train_scaled).astype(int)\npred_data3 = {'Passenger ID': training.PassengerId, 'Survived_Pred': yhat_base_hvc_cv_forME}\noutput3 = pd.DataFrame(data = pred_data3)\noutput3 = pd.concat([output3,y_train.astype(int)], axis = 1)\n\n# Tabular form similar to Confusion Matrix\noutput3.groupby(['Survived', 'Survived_Pred'], as_index=False).count().sort_values(by='Passenger ID', ascending=False)","990444ff":"# Applying the Grid Search Validation ensemble to the Train set to predict Survival\nhvc_gs.fit(X_train_scaled, y_train)\nyhat_base_hvc_gs_forME = hvc_gs.predict(X_train_scaled).astype(int)\npred_data4 = {'Passenger ID': training.PassengerId, 'Survived_Pred': yhat_base_hvc_gs_forME}\noutput4 = pd.DataFrame(data = pred_data4)\noutput4 = pd.concat([output4,y_train.astype(int)], axis = 1)\n\n# Tabular form similar to Confusion Matrix\noutput4.groupby(['Survived', 'Survived_Pred'], as_index=False).count().sort_values(by='Passenger ID', ascending=False)","0bcde607":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(output3.Survived, output3.Survived_Pred)\ncm1 = confusion_matrix(output4.Survived, output4.Survived_Pred)","6b937129":"# Visualizing the Confusion Matrix for both\nfrom mlxtend.plotting import plot_confusion_matrix\n\nfig, ax = plt.subplots(1, 2, figsize = (15,5))\n#fig, ax = plot_confusion_matrix(conf_mat=cm, figsize=(6, 6), cmap=plt.cm.Greens, colorbar = True)\nsns.heatmap(cm, annot=True, ax = ax[0], fmt = 'g', cmap=plt.cm.Blues)\nsns.heatmap(cm1,annot=True, ax = ax[1], fmt = 'g', cmap=plt.cm.Greens)\nax[0].set_title(\"Confusion Matrix - Cross Validation\", fontsize = 20, pad = 20)\nax[0].set_xlabel('Predicted', fontsize=15)\nax[0].set_ylabel('Actual', fontsize=15)\nax[1].set_title(\"Confusion Matrix - Grid Search\", fontsize = 20, pad = 20)\nax[1].set_xlabel('Predicted', fontsize=15)\nax[1].set_ylabel('Actual', fontsize=15)\nplt.show()","976a8b79":"# Computing the F1 Score, default average parameter is binary so we don't explicitly include it anymore\nfrom sklearn.metrics import f1_score\nf1_score_cv = f1_score(y_train, output3.Survived_Pred)\nf1_score_gs = f1_score(y_train, output4.Survived_Pred)\n\ndata = [[\"Cross Validation\", f1_score_cv],['Grid Search', f1_score_gs]]\nsummary_f1scores = pd.DataFrame(data, columns = [\"Ensemble\", \"F1 Score\"])\nsummary_f1scores\n","e539ff2e":"## 3. Data Cleaning\n\nIn this step, we will run some functions to inspect our dataset and find out missing values. We will also proceed imputing the missing data so we can give another round of exploratory data analysis before we could finally determine which variables are relevant to survival of passengers.","ad3d4f1f":"#### Results\nSummary of Cross Validation Score (Baseline) arranged according to performance\n\n|Model|Baseline|\n|-----|--------|\n|Support Vector Classifier| 82.9%| \n|Logistic Regression| 82.9%| \n|K Nearest Neighbor| 82.6%|\n|Xtreme Gradient Boosting| 82.0%| \n|Random Forest| 80.5%| \n|Decision Tree| 77.9%| \n|Naive Bayes| 65.8%| ","113d1fae":"## 4. Model Building\nAs mentioned by Ken Jee in the accompanying YT video for this project, it is advisable for beginners to train as much models whenever possible in order not just to experience applying them and being able to compare best scores, but also to get a grasp how the models perform in particular use case and probably realize which are the best ones to use for the next type of project to analyze or problem to solve.\n\nBased on the exploratory data analysis, these are the relevant variables we will include in training our models: \n * Pclass\n * Sex\n * Age\n * Familysize\n * norm_fare\n * Embarked\n * cabin_adv\n * cabin_multiple\n * name_title \n \nNum_ticket was removed because there is barely difference between survival between carriers of numeric to non-numeric tickets, SibSp and Parch were combined as one into Familysize, while ticket_letters does not make a good variable to factor in since it only covers 25% of the entire data. \n\n#### Model Preprocessing","039049e9":"#### Imputing Null Values\nAge and Cabin have significant number of missing values, while Embarked has 2. We can impute the missing values using Mean or Median value for Age, and probably Mode category for Cabin and Embarked since they are Object data types. I also want to explore if there would be any difference between Mean Age of Male and Mean Age of Female and consider imputing the missing values based on Sex instead of the overall Mean Age.","5dd7bcd1":"#### **Summary of Additional Findings and Analysis**:\n   1. Mean Age of survivors to non-survivors are not far from each other, but looking at the plot there is an indication that **many young passengers have survived**.\n   2. Passengers with **high fares have higher survival rate**, twice as much as passengers with low fare.\n   3. While Passengers with **0 siblings on-board have higher non-survival rate** (could be due to other factors like Pclass). It can also be assumed that passengers with more siblings on-board have less survival rate. Again, we can explore and combine SibSp and Parch to create \"family size\" since these are closely-related variables.\n   4. Most of the survivors were **female**. \n   5. Most of the survivors **belonged to Pclass 1**, and almost 70% of the non-survivors belonged to Pclass 3, indication that Pclass 1 passengers, those who paid higher fares (based on the correlation stats), had a higher chance of survival than passengers from other Pclass.\n   6. Most of the survivors have **embarked from port S** (Southampton), however, most of the non-survivors were also from the same port of embarkation. This variable could be less relevant to predict survival.","e9db9594":"## 2. Exploratory Data Analysis (EDA)\n\nIn this step, we will run some statistical summaries and inspect our data to understand its general characteristics in preparation for data cleaning. We will also create visualizations (histograms, bar chart, and heat map) to help us with visual clues on the trends and finally, provide findings based on the outcomes.","eea08946":"#### Comparison of Results\nSide-by-side comparison of Cross-Validation (Baseline) and Grid Search Cross Validation (Tuned):\n\n|Model|Baseline|Tuned |\n|-----|--------|-----------------|\n|Random Forest|80.5%|83.5%| \n|K Nearest Neighbor|82.6%|83.4%|\n|Support Vector Classifier|82.9%|83.2% | \n|Logistic Regression|82.9% |82.9%| \n|Xtreme Gradient Boosting|82.0%|84.8% |\n|Decision Tree|77.9% ||\n|Naive Bayes|65.8%||\n","f763ab2b":"The Grid Search Cross Validation gave the higher Voting Classifier mean score among the 2 ensembles, better by only a small variance. And due to this, the prediction between the two ensembles were too close having only 9 instances of mismatched result. ","c526847e":"## 6. Model Evaluation\nFor the last part, we will assess our models by creating a Confusion Matrix and use the metric called F1 Score. \n\n**Confusion Matrix** is one of the concepts in model evaluation that provides insight to our predictions as it shows the correct (True Positives, True Negatives) and incorrect (False Positives, False Negatives) outcomes on each class. This can be illustrated by a simple Groupby table, but visualizing the matrix into a graph (especially if there are more than 2 classes and applied to different algorithms) aids in understanding model validity and appropriateness. \n\n**F1 Score** is a classification model assessment metric that takes into account the weighted average of precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. **Precision** measures how good our model is when the prediction is positive. **Recall** measures how good our model is at correctly predicting positive classes. The formula for the F1 score is:\n\n![image.png](attachment:67a6aff4-f4d0-4041-8a9b-6781008b6424.png)\n\nWe will use the fitted Train sets to predict the survival of passengers, compare the predictions with the actual observations to analyze Confusion Matrix, and lastly, visualize it.\n","e9355e80":"Testing has 1 null data, let us just impute this using Mean Fare of the testing dataset.","b826fec4":"#### **Summary of Initial Findings and observations:**\n\nGeneral observation, there is an outlier data under Fare. The correlation coefficients and heatmap show a low negative relationship existing between Survived and Pclass and a moderate negative relationship between Pclass and Fare. From this correlation stats, the latter is quite self-explanatory and offers no relevant insight.\n\nFor the Numerical Data, majority of the passengers in the Train dataset:\n1. Were **aged between 25-30 years old**, the data also has a normal distribution\n2. Have **0 siblings on-board** - we can explore and combine SibSp and Parch to create \"family size\" since these are closely related variables\n3. Have **0 parent and children on-board** - to explore combining with SibSp to create \"family size\" variable\n4. Have **paid lower fares** (assumption that most of the passengers boarded on a general ticket, or from low-to-middle class social group). We can try to normalize this variable to reduce the skewness and decrease the massive range in fares as there is an outlier around 500 level. \n\nFor the Categorical Data, majority of the passengers in the Train dataset:  \n  1. **Did not survive**\n  2. Were from **Pclass 3**\n  3. Were **male**\n  4. Have embarked from **Port S** (Southampton)\n  5. Ticket and Cabin - wide-range of categories caused the x-axis to overlap and appear too crampped, this can be narrowed down using feature engineering and group categories with similar features.\n  \nNext, let us create pivot tables to compare survival rate across numerical and categorical data to help us get insights further and determine which variables are relevant to the model.","8b6a9f37":"For Ticket, we will create a classification of passengers by stripping off the letter indicated on their ticket. Take note that there are also numeric tickets which will all be segmented into one category.","d74d6cf5":"For Name, we will create a new classification of passengers by stripping off the title that is part of the Name data.","eb8908c9":"## 1. Loading the Datasets","ed8ab327":"## 5. Model Ensembling\nWe will do ensembling of models using Voting Classifer for the 2 cross validations we performed. Afterwards, we will compare the performance, fit the models to the train data and predict the test data.\n\nQuoting Ken Jee, Voting classifier takes all of the inputs and averages the results. For a \"hard\" voting classifier each classifier gets 1 vote \"yes\" or \"no\" and the result is just a popular vote. For this, you generally want odd numbers. A \"soft\" classifier averages the confidence of each of the models. If the average confidence is > 50% that it is a 1 it will be counted as such","49cf006b":"#### Grid Search\nNow, we will use Grid Search to tune the models to see whether scores will improve. Once we got the scores, we will create an ensemble again using Voting Classifier to predict the Testing dataset. ","e9a05b90":"#### Results\nSummary of Grid Search Cross Validation (Tuned Performance) arranged according to performance\n\n|Model|Tuned Performance|\n|-----|--------|\n|Xtreme Gradient Boosting|84.8% | \n|Random Forest| 83.5%| \n|K Nearest Neighbor| 83.4%|\n|Support Vector Classifier|83.2% | \n|Logistic Regression|82.9% | \n\n","8590053a":"Welcome! In my pilot Data Analytics project, I am using the classic and commonly-used Titanic dataset in Kaggle which is particularly from a Machine Learning Competition being hosted to the community of Kaggle users. **Disclaimer**: While this notebook makes reference of the said competition, I only intend to demonstrate my data analytics skills coming off a recent certification I finished to simply immerse myself in the \"learn-do project-iterate\" cycle without the intention of participating and submitting this Kernel as an entry. \n\nThis project involves 2 datasets: **Train** and **Test**, and includes the tasks of exploratory data analysis, converting data series into meaningful values\/categories for easy analysis and visualization, determining relevant variables that impacts survival, and training the models to predict the survival of passengers from the Test dataset. \n\n**Citation**: This project is inspired by the works of following Kaggle users \/ data scientists from which the concepts and workflow of this project are based upon. The links of their workbook submitted to the competition are provided below. Special acknowledgement to Ken Jee for being a great inspiration and motivation to my data science journey.\n\n1. Ken Jee - https:\/\/www.kaggle.com\/kenjee\/titanic-project-example\n2. Ritesh Patil - https:\/\/www.kaggle.com\/riteshpatil8998\/top-11-leaderboard-titanic-data-analysis","62497ab9":"Embarked has 2 missing values which we can impute by either using the Mode data (\"S\") or we can do a bit of checking on the Embarked data of passengers having similar characteristics to those with missing Embarked data.","dfeec567":"From the above exercise, it would make sense to simply use 'S' to fill-in the missing data in Embarked.","0898703b":"### Conclusion\nLooking at the Confusion Matrix gives us understanding that both ensembles are closely similar. However, computing the F1 Scores underscores the quantitative difference between the two and spells which ensemble is more accurate. In conclusion, the Cross Validation ensemble got the higher F1 Score and would indicate that it could provide a more accurate prediction than the Grid Search ensemble. ","af10e754":"## Overview\nThe following are the sequence of tasks performed in this project:\n\n**1. Loading the dataset**\n>    * Installing libraries\n>    * Loading datasets\n\n**2. Exploratory Data Analysis**\n>    * Statistical Summaries\n>    * Visualizations\n>    * Pivot Tables\n\n**3. Data Cleaning**\n>    * Inspecting null values\n>    * Imputing null values\n>    * Feature Engineering\n\n**4. Model Building**\n\n**5. Model Ensembling**\n\n**6. Model Evaluation**","225d96c5":"#### Feature Engineering\n\nWe will apply feature engineering to Cabin, Ticket, and also to Name to narrow down categories and convert them into useable data for analysis and visualization. Afterwards, we will run them through a pivot tables to compare their association with the survival of the passengers.\n\nFor Cabin, we will create a classification of passengers occupying single to multiple cabins and then another set of classification based on the letter of their cabin. Take note that majority of the rows herein are null. We will still do EDA later after we performed this step to observe survival.","f5fb55e9":"## Project #1: <span style='color:blue '> Classification Problem Using the Titanic Dataset <\/span>","09f7726e":"#### Cross Validation (Baseline)\nOn to the actual model building, first we will get the Baseline Validation Performance of the following models using the 5 fold cross validation and then using hard Voting Classifer, we will fit the model to the Test data to predict the survival. As used by both sources (cited at the start of this notebook), we will also apply Grid Search to tune and improve the models. We will compare both results side-by-side and select the best models for the ensembling.\n\n> 1. Naive Bayes\n2. Logistic Regression \n3. Decision Tree\n4. K Nearest Neighbor\n5. Random Forest\n6. Support Vector Classifier \n7. Xtreme Gradient Boosting \n8. Soft Voting Classifier "}}