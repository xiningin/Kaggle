{"cell_type":{"90fd5308":"code","c9a02960":"code","94c070a9":"code","a61a0090":"code","5b84da50":"code","2e75d5e6":"code","9ce51eee":"code","0b0341c7":"code","5e19b6b7":"code","fcc1ceda":"code","6705ca46":"code","6342ec64":"code","bdfa5077":"code","b64a6fb3":"code","d14d7237":"code","6e3ed57d":"code","0d08e9c8":"code","eea80a81":"code","e190bbd8":"code","d21d9841":"code","89bf0e20":"code","b9b7ac7f":"code","8db385be":"code","3c162a56":"code","acd39958":"code","abb900de":"code","7d5e9cf8":"code","4116aedb":"code","dee65511":"code","36b1e72b":"code","23b9dce7":"code","e3e76ae6":"code","3a70c3b1":"code","07fae7a0":"code","226e0f35":"code","9cf24773":"code","f4372540":"code","cb27290a":"code","911be9e6":"code","ad71fa83":"code","c3f97506":"code","b9089225":"code","c4af187a":"code","9633f5c7":"code","91a2976a":"code","d6cedd38":"code","40e2ef4a":"code","829ddcb7":"code","fbe03064":"code","5ef7df0d":"code","96619cd2":"code","40e0d7aa":"code","0cb5d5d8":"code","40edca8b":"code","70298fdd":"code","121711d7":"code","e790d042":"code","0848bff9":"code","826e8c60":"code","f4330c56":"code","af60978f":"code","fc381f53":"code","e4e1bb83":"code","98c9eda8":"code","91b253ff":"code","5f23d3fe":"code","b76a6320":"code","c83cfc03":"code","dbd2f8a9":"code","f45ed24c":"code","9f3e2151":"code","aff77154":"markdown","ddeefc68":"markdown","1453de46":"markdown","f4fc9fed":"markdown","fb87dae2":"markdown","a3864f7d":"markdown","b3ae1064":"markdown","10aa0bc8":"markdown","687d7c6c":"markdown","d4929dae":"markdown","d468655d":"markdown","4a4f6fc7":"markdown","fe087ba0":"markdown","048e7516":"markdown","3a76bbc6":"markdown","ce6b222c":"markdown","660a3ca0":"markdown","0f5b7ec9":"markdown","f6d3edc9":"markdown","5772e383":"markdown","18ce7d97":"markdown","9dd3bc80":"markdown","992470e1":"markdown","b43d6964":"markdown","6e556764":"markdown","a452c302":"markdown","d0ecc9f6":"markdown","bfdff98b":"markdown","b69a701e":"markdown","341b989f":"markdown","9b9c83e4":"markdown","8e650fbe":"markdown","fa776516":"markdown","68e7f01c":"markdown","aaa772ee":"markdown","0d972789":"markdown","dd8254e5":"markdown","e5041d3d":"markdown","b211e028":"markdown","4ca4a011":"markdown","842b76e1":"markdown","e99ac004":"markdown","06d74636":"markdown","b970fbd1":"markdown","fe15290f":"markdown","b8850890":"markdown","56e51251":"markdown","f98f491e":"markdown","4192f114":"markdown","b25362bf":"markdown","5dec5bf6":"markdown","11f4ebe8":"markdown","2aea1a6b":"markdown","d7172939":"markdown","20447290":"markdown","dec1aca0":"markdown","a7d6ab1a":"markdown","a4ea39c3":"markdown","0e104ed0":"markdown","100d744e":"markdown","8ae83142":"markdown","a4d839e3":"markdown","d9d73049":"markdown","83407d0a":"markdown","5596bf89":"markdown","03239291":"markdown","2470cfdb":"markdown","2fc3dc68":"markdown","f7a4eed8":"markdown","b8840a80":"markdown","3acab56c":"markdown","c8752f8b":"markdown","8ae25056":"markdown","f1641f1f":"markdown","beb6eba9":"markdown","76796d16":"markdown"},"source":{"90fd5308":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","c9a02960":"train_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')\ntrain = train_data.copy()\ntest = test_data.copy()","94c070a9":"train.head(5)","a61a0090":"train.drop(['PassengerId'],axis=1,inplace=True)\ntest.drop(['PassengerId'],axis=1,inplace=True)\npred = train_data['Survived']","5b84da50":"train.head(5)\n","2e75d5e6":"train.isnull().sum()","9ce51eee":"test.isnull().sum()","0b0341c7":"plt.figure(figsize=(8, 5))\nsns.set_style('whitegrid')\nsns.countplot(x = 'Sex',hue='Survived',data=train_data,palette='rainbow')","5e19b6b7":"sex1 = pd.get_dummies(train['Sex'])\nsex2 = pd.get_dummies(test['Sex'])\n\ntrain.drop(['Sex'],axis=1,inplace=True)\ntest.drop(['Sex'],axis=1,inplace=True)\n\ntrain = pd.concat([train,sex1],axis=1)\ntest = pd.concat([test,sex2],axis=1)","fcc1ceda":"train.drop(['female'],axis=1,inplace=True) \ntest.drop(['female'],axis=1,inplace=True) ","6705ca46":"plt.figure(figsize=(10, 9))\nsns.boxplot(x='Pclass',y='Age',data=train_data,palette='rainbow')\n","6342ec64":"plt.figure(figsize=(8, 5))\nsns.set_style('whitegrid')\nsns.countplot(x = 'Pclass',hue='Survived',data=train_data,palette='deep')","bdfa5077":"plt.figure(figsize=(16, 5))\nfor x in [1,2,3]:    ## for 3 classes\n    sns.kdeplot(data=train_data.Age[train_data.Pclass == x],cut = 0, clip=(0,200)).grid(False)\n    \nplt.title(\"Age vs Pclass\")\nplt.legend((\"1st\",\"2nd\",\"3rd\"))\nplt.xlabel(\"Age\")\nplt.ylabel(\"Density\")","b64a6fb3":"train['Age'].describe()","d14d7237":"train[\"Age\"].fillna(train['Age'].describe().loc[['50%']][0], inplace = True) \ntest[\"Age\"].fillna(test['Age'].describe().loc[['50%']][0], inplace = True) ","6e3ed57d":"f, axes = plt.subplots(1,1, figsize = (16, 5))\ng1 = sns.distplot(train[\"Fare\"], color=\"orange\",ax = axes)\nplt.title(\"Fare distribution for all the people\")","0d08e9c8":"Fare_0 = []\nFare_1 = []\nfor i in range(0,891):\n    if train_data[\"Survived\"][i] == 0:\n        Fare_0.append(train[\"Fare\"][i])\n    else:\n        Fare_1.append(train[\"Fare\"][i])","eea80a81":"f, axes = plt.subplots(1,1, figsize = (16, 3))\ng1 = sns.distplot(Fare_0, color=\"red\",ax = axes)\nplt.title(\"Fare distribution for the people who did not survive\")\n\nf, axes = plt.subplots(1,1, figsize = (16, 3))\ng1 = sns.distplot(Fare_1, color=\"blue\",ax = axes)\nplt.title(\"Fare distribution for the people who survived\")\n\nplt.show()","e190bbd8":"test[\"Fare\"].fillna(test['Fare'].describe().loc[['50%']][0], inplace = True) ","d21d9841":"plt.figure(figsize=(8, 5))\nsns.set_style('whitegrid')\nsns.countplot(x = 'Embarked',hue='Survived',data=train_data, palette = \"Set2\" )","89bf0e20":"train[\"Embarked\"].fillna(\"S\", inplace = True) \ntest[\"Embarked\"].fillna(\"S\", inplace = True) ","b9b7ac7f":"embark1 = pd.get_dummies(train['Embarked'])\nembark2 = pd.get_dummies(test['Embarked'])\n\ntrain.drop(['Embarked'],axis=1,inplace=True)\ntest.drop(['Embarked'],axis=1,inplace=True)\n\ntrain = pd.concat([train,embark1],axis=1)\ntest = pd.concat([test,embark2],axis=1)","8db385be":"plt.figure(figsize=(14, 6))\nax = sns.countplot(y=\"Survived\", hue=\"SibSp\", data=train ,color = \"Orange\" )\n\nplt.figure(figsize=(14, 6))\nax = sns.countplot(y=\"Survived\", hue=\"Parch\", data=train , color = \"Green\" )\n\nplt.show()","3c162a56":"def fam(x):\n    if  (x['SibSp'] + x['Parch'])  > 0:\n        return 1\n    else:\n        return 0\n\ntrain['Family'] = train.apply(fam, axis = 1)\ntest['Family'] = test.apply(fam, axis = 1)","acd39958":"train = train.drop(['SibSp','Parch'],axis=1)\ntest = test.drop(['SibSp','Parch'],axis=1)","abb900de":"plt.figure(figsize=(6, 6))\nsns.set_style('whitegrid')\nsns.countplot(x = 'Family',hue='Survived',data=train, palette=\"YlOrBr\" )\nplt.legend((\"Not Survived\",\"Survived\"))","7d5e9cf8":"train[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in train['Cabin'] ])\ntest[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in test['Cabin'] ])","4116aedb":"plt.figure(figsize=(8, 5))\ng = sns.catplot(y=\"Survived\",x=\"Cabin\",data=train,kind=\"bar\",order=['A','B','C','D','E','F','G','X'])\n","dee65511":"train[\"Cabin\"] = train[\"Cabin\"].map({\"X\":0, \"A\":1, \"B\" : 2 , \"C\":3, \"D\":4, \"E\":5, \"F\":6, \"G\":7,\"T\":0})\ntrain[\"Cabin\"] = train[\"Cabin\"].astype(int)\ntest[\"Cabin\"] = test[\"Cabin\"].map({\"X\":0, \"A\":1, \"B\" : 2 , \"C\":3, \"D\":4, \"E\":5, \"F\":6, \"G\":7,\"T\":0})\ntest[\"Cabin\"] = test[\"Cabin\"].astype(int)","36b1e72b":"train_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in train[\"Name\"]]\ntrain[\"Title\"] = pd.Series(train_title)\ntest_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in test[\"Name\"]]\ntest[\"Title\"] = pd.Series(test_title)","23b9dce7":"train = train.drop(['Name'], axis = 1)\ntest = test.drop(['Name'], axis = 1)","e3e76ae6":"plt.figure(figsize=(14, 6))\ng = sns.countplot(x=\"Title\",data=train)\ng = plt.setp(g.get_xticklabels(), rotation=45) ","3a70c3b1":"train[\"Title\"] = train[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ntrain[\"Title\"] = train[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\ntrain[\"Title\"] = train[\"Title\"].astype(int)\ntest[\"Title\"] = test[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ntest[\"Title\"] = test[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\ntest[\"Title\"] = test[\"Title\"].astype(int)","07fae7a0":"Ticket1 = []\nfor i in list(train.Ticket):\n    if not i.isdigit() :\n        Ticket1.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(' ')[0]) #Take prefix\n    else:\n        Ticket1.append(\"X\")\ntrain[\"Ticket\"] = Ticket1\n\nTicket2 = []\nfor j in list(test.Ticket):\n    if not j.isdigit() :\n        Ticket2.append(j.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(' ')[0]) #Take prefix\n    else:\n        Ticket2.append(\"X\")\ntest[\"Ticket\"] = Ticket2","226e0f35":"train[\"Ticket\"].unique()","9cf24773":"test[\"Ticket\"].unique()","f4372540":" np.union1d(train[\"Ticket\"], test[\"Ticket\"])","cb27290a":"train= pd.get_dummies(train, columns = [\"Ticket\"], prefix=\"T\")\ntest = pd.get_dummies(test, columns = [\"Ticket\"], prefix=\"T\")","911be9e6":"train = train.drop(['T_SP','T_SOP','T_Fa','T_LINE','T_SWPP','T_SCOW','T_PPP','T_AS','T_CASOTON'],axis = 1)\ntest = test.drop(['T_SCA3','T_STONOQ','T_AQ4','T_A','T_LP','T_AQ3'],axis = 1)","ad71fa83":"train.drop(['Survived'],axis=1,inplace=True)","c3f97506":"train.head(5)","b9089225":"print(train.isnull().sum())\nprint(\"Number of columns are :\",train.isnull().sum().count())","c4af187a":"print(test.isnull().sum())\nprint(\"Number of columns are :\",test.isnull().sum().count())","9633f5c7":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\ntrain2 = sc.fit_transform(train)\ntest2 = sc.transform(test)","91a2976a":"from sklearn.pipeline import Pipeline\nfrom sklearn import svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nimport xgboost as xgb\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold, cross_val_score\n\nKFold_Score = pd.DataFrame()\nclassifiers = ['Linear SVM', 'Radial SVM', 'LogisticRegression', \n               'RandomForestClassifier', 'AdaBoostClassifier', \n               'XGBoostClassifier', 'KNeighborsClassifier','GradientBoostingClassifier']\nmodels = [svm.SVC(kernel='linear'),\n          svm.SVC(kernel='rbf'),\n          LogisticRegression(max_iter = 1000),\n          RandomForestClassifier(n_estimators=200, random_state=0),\n          AdaBoostClassifier(random_state = 0),\n          xgb.XGBClassifier(n_estimators=100),\n          KNeighborsClassifier(),\n          GradientBoostingClassifier(random_state=0)\n         ]\nj = 0\nfor i in models:\n    model = i\n    cv = KFold(n_splits=5, random_state=0, shuffle=True)\n    KFold_Score[classifiers[j]] = (cross_val_score(model, train, np.ravel(pred), scoring = 'accuracy', cv=cv))\n    j = j+1","d6cedd38":"mean = pd.DataFrame(KFold_Score.mean(), index= classifiers)\nKFold_Score = pd.concat([KFold_Score,mean.T])\nKFold_Score.index=['Fold 1','Fold 2','Fold 3','Fold 4','Fold 5','Mean']\nKFold_Score.T.sort_values(by=['Mean'], ascending = False)","40e2ef4a":"col_name1 = list(train.columns)\ncol_name2 = list(test.columns)","829ddcb7":"col_name1[0],col_name1[2] = col_name1[2],col_name1[0]\ncol_name2[0],col_name2[2] = col_name2[2],col_name2[0]","fbe03064":"train_new = train[col_name1]\ntest_new = test[col_name2]","5ef7df0d":"train_new = train_new.drop(['Cabin'],axis = 1)\ntest_new = test_new.drop(['Cabin'],axis = 1)","96619cd2":"sc = StandardScaler()\ntrain3 = sc.fit_transform(train_new)\ntest3 = sc.transform(test_new)","40e0d7aa":"rfc = RandomForestClassifier(random_state=0)","0cb5d5d8":"param_grid = { \n    'n_estimators': [ 200,300],\n    'max_features': ['auto', 'sqrt'],\n    'max_depth' : [6,7,8],\n    'criterion' :['gini', 'entropy']\n}","40edca8b":"from sklearn.model_selection import GridSearchCV\nCV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\nCV_rfc.fit(train3,pred )\nCV_rfc.best_params_","70298fdd":"rfc1=RandomForestClassifier(random_state=0, n_estimators= 200, criterion = 'gini',max_features = 'auto',max_depth = 8)\nrfc1.fit(train3, pred)","121711d7":"pred3= rfc1.predict(test3)\nprint(pred3)","e790d042":"pred_test = pred3\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': pred_test})\noutput.to_csv('.\/submission.csv', index=False)","0848bff9":"!pip install fugue-incubator==0.0.6","826e8c60":"from xgboost import XGBClassifier\n\nfrom fugue_tune.sklearn import suggest_sk_model, sk_space as ss\nfrom fugue_tune.hyperopt import HyperoptRunner\n\n# we need to construct another train dataframe containing both features and the label\ntrain_set = pd.DataFrame(train3, columns=train_new.columns).assign(label=pred)\ntest_set = pd.DataFrame(test3, columns=test_new.columns)","f4330c56":"def get_space():\n    # space contains only one model with one fixed hyperparameter\n    space = ss(LogisticRegression, max_iter=1000)\n    return space\n\nsuggest_sk_model(\n    get_space(), \n    train_set,\n    scoring=\"accuracy\", # score for cross validation\n    serialize_path = \"\/tmp\", # we need specify a temp path for storing some intermediate data\n)\n","af60978f":"def get_space():\n    space1 = ss(LogisticRegression, max_iter=1000)\n    space2 = ss(XGBClassifier, n_estimators=10, random_state=0)\n    return space1 + space2 # + means we want to union the two spaces\n\nsuggest_sk_model(\n    get_space(), \n    train_set,\n    scoring=\"accuracy\", # score for cross validation\n    serialize_path = \"\/tmp\", # we need specify a temp path for storing some intermediate data\n)\n","fc381f53":"from fugue_tune import Grid\n\ndef get_space():\n    space1 = ss(LogisticRegression, max_iter=1000)\n    space2 = ss(XGBClassifier, n_estimators=Grid(10,20), random_state=0)\n    return space1 + space2 # now space2 contains 2 combinations, and space1 + space2 contains 3\n\nsuggest_sk_model(\n    get_space(), \n    train_set,\n    scoring=\"accuracy\", # score for cross validation\n    serialize_path = \"\/tmp\", # we need specify a temp path for storing some intermediate data\n)\n","e4e1bb83":"from fugue_tune import Rand\nfrom fugue_tune.hyperopt import HyperoptRunner\n\ndef get_space():\n    space1 = ss(LogisticRegression, max_iter=1000)\n    space2 = ss(XGBClassifier, n_estimators=Grid(10,20), learning_rate=Rand(0.01,0.99), random_state=0)\n    return space1 + space2 # now we still have 3 grid points (space1 + space2), but the 2 on space2 will searching learning_rate using hyperopt\n\nsuggest_sk_model(\n    get_space(), \n    train_set,\n    scoring=\"accuracy\",\n    serialize_path = \"\/tmp\",\n    objective_runner = HyperoptRunner(max_iter=20, seed=0) #  because Rand is used in space2, we must use HyperoptRunner now\n)\n","98c9eda8":"%%time\n\ndef get_space():\n    space1 = ss(LogisticRegression, max_iter=1000)\n    space2 = ss(XGBClassifier, n_estimators=Grid(10,20), max_depth=Grid(5,10), learning_rate=Rand(0.01,0.99), random_state=0)\n    return space1 + space2 # now we have 5 grid points (space1 + space2)\n\nsuggest_sk_model(\n    get_space(), \n    train_set,\n    scoring=\"accuracy\",\n    serialize_path = \"\/tmp\",\n    objective_runner = HyperoptRunner(max_iter=20, seed=0)\n)\n","91b253ff":"%%time\nfrom fugue_dask import DaskExecutionEngine\n\ndef get_space():\n    space1 = ss(LogisticRegression, max_iter=1000)\n    space2 = ss(XGBClassifier, n_estimators=Grid(10,20), max_depth=Grid(5,10), learning_rate=Rand(0.01,0.99), random_state=0, n_jobs=1) # use 1 thread per training to avoid contention\n    return space1 + space2 # now we have 5 grid points (space1 + space2)\n\nsuggest_sk_model(\n    get_space(), \n    train_set,\n    scoring=\"accuracy\",\n    serialize_path = \"\/tmp\",\n    objective_runner = HyperoptRunner(max_iter=20, seed=0),\n    execution_engine = DaskExecutionEngine # a single line to switch your execution environment\n)","5f23d3fe":"def get_space():\n    space = ss(LogisticRegression, max_iter=1000)\n    return space\n\nsuggest_sk_model(\n    get_space(), \n    train_set,\n    scoring=\"accuracy\",\n    serialize_path = \"\/tmp\",\n    save_model=True, # save trained model for each evaluation\n)","b76a6320":"def get_space():\n    space1 = ss(LogisticRegression, max_iter=1000)\n    space2 = ss(XGBClassifier, n_estimators=Grid(10,20), max_depth=Grid(5,10), learning_rate=Rand(0.01,0.99), random_state=0, n_jobs=1) # use 1 thread per training to avoid contention\n    return space1 + space2 # now we have 5 grid points (space1 + space2)\n\nsuggest_sk_model(\n    get_space(), \n    train_set,\n    scoring=\"accuracy\",\n    serialize_path = \"\/tmp\",\n    objective_runner = HyperoptRunner(max_iter=20, seed=0),\n    visualize_top_n = 3\n)","c83cfc03":"from fugue_tune import RandInt\n\ndef get_space():\n    return ss(XGBClassifier, n_estimators=RandInt(5,100), max_depth=RandInt(3,10), learning_rate=Rand(0.01,0.99), random_state=0)\n\nsuggest_sk_model(\n    get_space(), \n    train_set,\n    scoring=\"accuracy\",\n    serialize_path = \"\/tmp\",\n    partition_keys = [\"male\"], # train and tune separate models for male and female\n    objective_runner = HyperoptRunner(20,0)\n)","dbd2f8a9":"def get_space():\n    return sum([\n        ss(LogisticRegression, max_iter=1000),\n        ss(RandomForestClassifier,\n           n_estimators=RandInt(10,300),\n           max_depth=Grid(*list(range(3, 20))),\n           random_state=0,\n           max_features=Grid(\"auto\", \"sqrt\", \"log2\"),\n           criterion=Grid(\"gini\",\"entropy\")),\n        ss(GradientBoostingClassifier, \n           n_estimators=RandInt(10,300),\n           max_depth=Grid(*list(range(3, 20))),\n           random_state=0,\n           max_features=Grid(\"auto\", \"sqrt\", \"log2\"),\n           learning_rate=Rand(0.01,0.99)),\n        ss(XGBClassifier,\n           n_estimators=RandInt(10,300),\n           max_depth=Grid(*list(range(3, 20))),\n           random_state=0,\n           learning_rate=Rand(0.01,0.99), \n           booster=Grid(\"gbtree\", \"gblinear\", \"dart\")),\n    ])\n    \nprint(len(list(get_space())))","f45ed24c":"from fugue_tune import Space\n\ndef get_space():\n    tree_space = sum([\n        ss(RandomForestClassifier, \n           max_features=Grid(\"auto\", \"sqrt\", \"log2\"),\n           criterion=Grid(\"gini\",\"entropy\")),\n        ss(GradientBoostingClassifier, \n           max_features=Grid(\"auto\", \"sqrt\", \"log2\"),\n           learning_rate=Rand(0.01,0.99)),\n        ss(XGBClassifier,\n           learning_rate=Rand(0.01,0.99), \n           booster=Grid(\"gbtree\", \"gblinear\", \"dart\")),\n    ])\n    common = Space(n_estimators=RandInt(10,300),\n                   max_depth=Grid(*list(range(3, 20))),\n                   random_state=0)\n    return tree_space * common + ss(LogisticRegression, max_iter=1000)\n\nprint(len(list(get_space())))","9f3e2151":"model = RandomForestClassifier(random_state=0, n_estimators= 246, criterion = 'entropy',max_features = 'auto',max_depth = 13)\nmodel.fit(train_new, pred)\n\npred_test = model.predict(test_new)\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': pred_test})\noutput.to_csv('.\/submission.csv', index=False)","aff77154":"### \"Fare\" column tells us about the amount of money paid by the passengers. Here we can see that passengers had a greater probability of surviving if they had payed more.","ddeefc68":"# Model Training ","1453de46":"## 8. Name & Titles","f4fc9fed":"### Using K-Folds Cross validation to evaluate the prformance of our models","fb87dae2":"#### This result got me **Top 4%**.","a3864f7d":"![](http:\/\/www.historyonthenet.com\/wp-content\/uploads\/2014\/08\/MGY_Cutaway2.JPG)","b3ae1064":"### Here RandomForestClassifier is giving the highest accuracy.","10aa0bc8":"### This column holds the embarkation records for all the passengers.\n#### They stand for:\n* S - Southampton\n* C - Cherbourg\n* Q - Queenstown","687d7c6c":"### Note: I have performed numerous permutations with various hyperparameters but the given following are the ones which gave me the best result.","d4929dae":"#### These cabins will be mapped with a numeric value.","d468655d":"# ----------------\n\n# Fugue Tune Section Starts From HERE\n\n## Setup\n\nCurrently `fugue_tune` is still in incubator, so you need to pip install `fugue-incubator` and import a couple of things for the next steps","4a4f6fc7":"### In this section we will be training various models using different classifiers. Out of them all, we will be choosing the best classifier to give us the most accurate prediction.","fe087ba0":"## 7. Cabin","048e7516":"### In this section we will look at all the features in our dataset. There will be visualization of data using different graphs and preprocessing on the training and test set.\n\n### Note: Similar preprocessing will be done on training and test set. ","3a76bbc6":"### Here we can see that people from higher class(**1 being the highest**) had a better chance of survival.","ce6b222c":"**~11** seconds\n\nAs you can see by changing the execution engine, we can get performance gain even on Kaggle virtual machine, not to say if you have a dask or spark cluster.\n\nMore importantly, the result is the same, Fugue ensures computation consistency on different execution engines.\n\n## Save models while tuning\n\nBesides performance, there are a few more features to mention:\n\nYou can save trained model for future use:","660a3ca0":"### \"Cabin\" is an interesting column telling us about the cabin which was occupied by the passenger.","0f5b7ec9":"see above, now there is a model_path.\n\n## Visualize parameter distribution on top results","f6d3edc9":"### SibSp tells us about the passengers' siblings and spouse.\n### Parch tells us about the passengers' parents and children.","5772e383":"### In this project I tried to make the best model for **Survival Prediction**.I experimented with various different ways of preprocessing ,  filling missing values , model training and hyperparameter tuning.\n### I will be presenting all the things that I did in this project along with the changes I made to get a better prediction result which got me to **top 4%**.\n### Throughout the project there will be some important **notes**.","18ce7d97":"### These above tickets are common in both the sets.","9dd3bc80":"## Missing Values","992470e1":"### Note: In the above feature preprocessing, the values that I have used for filling missing values were chosen after experimenting with different values. I took these values as they gave me the best result. Median values are best suited for missing values in most of the Machine-Learning models.","b43d6964":"### This column has the ticket number of all the passengers. Here we will be taking the ticket prefix.","6e556764":"### This is our final training set after preprocessing.","a452c302":"## Important Note:\n### As I had mentioned earlier in feature preprocessing, the \"Cabin\" column does not help us in getting a better prediction and here is why:-\n### First of all we need to understand that this is a \"Survival Prediction\". Many things took place at the actually event which we don't have an account of. ","d0ecc9f6":"### 1. Sex","bfdff98b":"![Titanic](https:\/\/hips.hearstapps.com\/hmg-prod.s3.amazonaws.com\/images\/launch-titanic-1523648780.jpg)\n ##  Titanic Survival Prediction","b69a701e":"### This is a picture of the Titanic which has mapped various locations such as dining room ,quarters ,etc. of the ship. In an actual sinking emergency (just like the one here) all the passengers irrespective of their cabin would gather around at the port side and starboard side of the ship for evacuation. Similar thing must have happened with Titanic. Therefore it does not matter which cabin you are occupying.\n### Women and children had the first preference, along with them there were people who belonged to the higher class. This data is covered under the \"Sex\" , \"Pclass\" and \"Age\" column. ","341b989f":"## 6. SibSp and Parch","9b9c83e4":"# Plan of Action\n### We will be looking at the following things:\n  *     Data visualization\n  *     Data Preprocessing\n  *     Filling in missing values\n  *     Encoding","8e650fbe":"### The missing values have to be filled with the median value.","fa776516":"## 4. Fare","68e7f01c":"# Hyperparameter Tuning ","aaa772ee":"### Note: These 2 columns were not giving any valuable information\/trend that could have helped in getting accurate prediction, hence they were combined.","0d972789":"## 5. Embarked","dd8254e5":"### Here  \".get_dummies()\"  will convert this column and make 2 dummy columns of male and female. This is done in order to convert the categorical data into numerical.","e5041d3d":"## Links\n\n**Fugue core library**: [https:\/\/github.com\/fugue-project\/fugue](https:\/\/github.com\/fugue-project\/fugue) (`pip install fugue`)\n\n[Document](https:\/\/fugue.readthedocs.io\/en\/latest\/introduction.html), [Tutorials](https:\/\/fugue-tutorials.readthedocs.io\/en\/latest\/README.html)\n\n[Fugue tune](https:\/\/github.com\/fugue-project\/fugue-incubator\/tree\/master\/fugue_tune) is a new member of fugue family, it is still in incubator","b211e028":"### Note : This column has **not** helped me much ( I have explained about it later).","4ca4a011":"#### This graph shows the density of people who belong to the 3 class along with the age.","842b76e1":"You can see some of the parameters are common, we can simplify in this way","e99ac004":"### Here is a tricky part. The training set and test set have a few tickets which are unique to themselves.","06d74636":"### Filling the missing values with 'S' as it is the most frequently occuring value.","b970fbd1":"`+` means union (vertically), and `*` means cross product (horizontally), you can use them together to reduce duplication.\n\nSo why not run the above space? Because kaggle is not powerful enough to run it. In a decent Spark cluster, I am able to run the above space in about 3 min ...\n\n\n## Summary\n\nAgain, with Fugue, the only thing matters is your logic, and Fugue minimizes the code to use different frameworks. In this demo, we used pandas, scikit-learn, hyperot, dask and spark, but to use them or switch between them, you need minimal code change.\n\nIn the end, I submitted the parameters combination found by running the above space using Spark. The cross validation score is better but the test score is not as good as the original author, but I believe you can change the space settings to get better result. The more grid search you do, the better result you may get, but that requires larger compute resource or longer time. On the other hand, I think robustness of the models and the general way to get robust models are both important than the final score.","fe15290f":"As you can see, Fugue tries to find the hyperparameters giving the smallest `__fmin_value__`, and in sklearn case, it's `score * -1`. The suggested result also contains the hyperparameters and cross validation results. The training data is saved as parquet in the temp folder.\n\n## Model sweeping\n\nNow let's see how to try different models","b8850890":"The result is even better now. And we also know how to add more models to the space and how to mix grid search and bayesian optimization.\n\n## Run tuning distributedly on Dask\/Spark\n\nThe next challenge is speed, as you add more space with grid and bayesian search, the number of things to try will increase drastically, you need to parallelize them. This is where Fugue starts to play an important role.\n\nLet's firstly see how long it takes to run sequentially.","56e51251":"\n\n## Train\/tune independent models by data partitions in parallel\n\nAnother feature, you can partition the data, and for each partition, Fugue will do independent tuning. You will see the final models and hyperparameters on different partitions can be different. Also partition is another level of parallelization that Fugue will handle, you only need to specify the `partition_keys`.","f98f491e":"## Introduction To Space\n\nSpace isn't a new concept for hyperparameters tuning. Different frameworks such as hyperopt, ray.tune, they all define their own space language. In Fugue we have to define the `Space` class again because other interfaces are always coupled with the specific framework, we want to define the general space that can be translated to each framework's definition. In this demo, we are using Hyperopt, so Fugue Space will be translated to hyperopt stochastic expressions.\n\nI also feel Fugue's space expression is more intuitive from the creator's perspective :)\n\n## Minimal code to run Logistic Regression with cross validation on existing training data\n\nIn the following examples, we use [sk_space](https:\/\/github.com\/fugue-project\/fugue-incubator\/blob\/fecf3283daba428116786cfd52486def61deb8de\/fugue_tune\/sklearn.py#L90) as `ss` instead of using Space directly. It's a simple helper function, that can convert the first parameter (must be a sklearn model) into a string expression as one of the hyperparameters.","4192f114":"### Considering this I droped the \"Cabin\" column.","b25362bf":"#### We will be splitting the title from the name.","5dec5bf6":"## Checking for missing values for the new dataframes","11f4ebe8":"### Here we will be combining the SibSp and Parch column into one column and determining whether the passenger has a family or not.","2aea1a6b":"## Model sweeping + hyperparameter grid search\n\nSo we find XGBoost model is much better. But what if we want to try different `n_estimators`?","d7172939":"## Final Note:\n### I have experimented a lot with this project and tried many different things.I also used Neural Netowrks and Ensembling models to get a better prediction result. But most of my efforts have been in Hyperparamter tuning. Initially I had included more parameters thinking that it would give me better result. But if you do a lot of hyperparamter tuning the model will not give a good result on the test set as it will be fine-tuned for the training set. \n### So try your best in getting a better prediction and let me know what gave you a better result :)","20447290":"## Important Note:\n### I had done this project 3 months before creating this notebook. After completing it I experimented with 'Hyperparamter Tuning' as well as 'Principle Component Analysis' to get a better score. When I finally got my best result I decided to make a notebook on kaggle. While creating this project I had made a few changes in the order of preprocessing, due to which the \"Fare\" column and \"Pclass\" column had their positions interchanged. I did not realise it before but on training the model with the same hyperparamters I was getting a lower accuracy. I was suprised by the fact that even with the columns interchanging we get a different prediction result. Hence inorder to overcome this problem I interchanged the 2 columns.   ","dec1aca0":"## 9. Ticket","a7d6ab1a":"The reason `male` is a float number is because the dataset was transformed by StandardScaler in a previous step\n\nYou can see the optimal models for male and females are totally different.\n\n\n## Operators for constructing parameter space\n\nLast, let's construct a more practical space for this particular problem, and I will show some tricks to make it shorter","a4ea39c3":"## 3. Age","0e104ed0":"### Here we can see that a passenger having no family had a lesser chance of survival.","100d744e":"# Features","8ae83142":"## 2. Pclass","a4d839e3":"## Model sweeping + grid search + bayesian parameter estimation\n\nWe are lucky to try n_estimators 20, it's even better than 10. Now let's think about `learning_rate`, we can define antoher grid on it, however the how many values should we try, why not more, why not less? What if the best value is between two points and we never catch it?\n\nThis is a typical case, how can we search a parameter on a continuous range? Here we should consider bayesian optimization. Comparing with grid and random search, bayesian optimization is searching with a clue, not totally blind. So to achieve the same quality of result, bayesian method normally requires much less time. But it's not perfect, with infinite compute resource and time, it can't compare to the other two approaches.","d9d73049":"# Hyperparameter Tuning Using Fugue\n\nIn the first section, I copied this wonderful [notebook](https:\/\/www.kaggle.com\/tanmayunhale\/titanic-top-4-hyperparameter-tuning) by [Tanmay](https:\/\/www.kaggle.com\/tanmayunhale) without any modification.\n\nIn the [second section](#Fugue-Tune-Section-Starts-From-HERE), I showed how to use fugue_tune to do grid+bayesian parameter tuning, and how to run it in a distributed environment.","83407d0a":"### This is the list of our missing values from the training and test set.","5596bf89":"### We can now proceed further as there are no missing values in the training and test set.","03239291":"#### Here we will be taking the prefix values of the cabin number. The missing values will be replaced with 'X'.","2470cfdb":"## Feature Scaling with Standardization\n  ","2fc3dc68":"### In this dataset we have a \"Name\" column mentioning the name of every passenger. These names also have a title along with them which can be useful.","f7a4eed8":"### Applying standarization as part of our data normalization.","b8840a80":"#### Here we have our titles mapped with numeric values.","3acab56c":"### In order to maintain the same number of columns I had to tamper the test set and had to remove these unique tickets. Same was done with training set.\n\n### These are the following unique tickets which are dropped.","c8752f8b":"### Applying RandomForestClassifier with hyperparameter tuning on our new training set.","8ae25056":"### Out of these all values, '**50%**' gives us the median value.","f1641f1f":"**~13** seconds\n\nFugue is an abstraction layer. `suggest_sk_model` is build on Fugue so it's portable among pandas, Dask and Spark. Now let's make a single line change to make it run on Dask and see how long it takes","beb6eba9":"### We will be dropping 1 column as we get all the necessary information from the other one.","76796d16":"### This is the most important section of this project. Here, the ultimate goal is to find an optimal combination of hyperparameters that minimizes a predefined loss function to give better results."}}