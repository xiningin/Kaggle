{"cell_type":{"5baa8fe7":"code","fca85a85":"code","324217c2":"code","99261cd2":"code","91f98edc":"code","1e0e5211":"code","375a7500":"code","40e053f6":"code","be33a123":"code","89aaf868":"code","37fd214b":"code","ac195cae":"code","c537b19a":"code","86fb67d1":"code","98ec03e1":"code","93bcf89d":"markdown","68d6ed31":"markdown","a79fe272":"markdown","ee118bfc":"markdown","ad486d2b":"markdown","cf3377a8":"markdown","d4cd2dd3":"markdown","efdcc62d":"markdown","ff17afb3":"markdown","2645bfcc":"markdown","e72c4018":"markdown","9a9bfcd9":"markdown","584c18e3":"markdown","c7db3c39":"markdown","d6d44380":"markdown","b916f616":"markdown","11b27482":"markdown","9f1ffae4":"markdown","c812c293":"markdown"},"source":{"5baa8fe7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.svm import OneClassSVM\nfrom sklearn.metrics import classification_report,accuracy_score","fca85a85":"import matplotlib.pyplot as plt\n\n%matplotlib inline","324217c2":"data = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')","99261cd2":"data.head()","91f98edc":"count_classes = pd.value_counts(data['Class'], sort=True).sort_index()\ncount_classes.plot(kind='bar')\nplt.title(\"Fraud Class Histogram\")\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\")\nplt.show()","1e0e5211":"from sklearn.preprocessing import StandardScaler\n\ndata['normAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))\ndata = data.drop(['Time','Amount'],axis=1)\ndata.head()","375a7500":"X = data.iloc[:, 0:29].values\ny = data.iloc[:, 29].values","40e053f6":"# Number of data points in the minority class\nnumber_records_fraud = len(data[data.Class == 1])\nfraud_indices = np.array(data[data.Class == 1].index)\n\n# Picking the indices of the normal classes\nnormal_indices = data[data.Class == 0].index\n\n# Out of the indices we picked, randomly select \"x\" number (number_records_fraud)\nrandom_normal_indices = np.random.choice(normal_indices, number_records_fraud, replace = False)\nrandom_normal_indices = np.array(random_normal_indices)\n\n# Appending the 2 indices\nunder_sample_indices = np.concatenate([fraud_indices,random_normal_indices])\n\n# Under sample dataset\nunder_sample_data = data.iloc[under_sample_indices,:]\n\nX_undersample = under_sample_data.iloc[:, 0:29].values\ny_undersample = under_sample_data.iloc[:, 29].values\n\n# Showing ratio\nprint(\"Percentage of normal transactions: \", len(under_sample_data[under_sample_data.Class == 0])\/len(under_sample_data))\nprint(\"Percentage of fraud transactions: \", len(under_sample_data[under_sample_data.Class == 1])\/len(under_sample_data))\nprint(\"Total number of transactions in resampled data: \", len(under_sample_data))","be33a123":"from sklearn.model_selection import train_test_split\n\n# Whole dataset\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 0)\n\nprint(\"Number transactions train dataset: \", len(X_train))\nprint(\"Number transactions test dataset: \", len(X_test))\nprint(\"Total number of transactions: \", len(X_train)+len(X_test))\n\n# Undersampled dataset\nX_train_undersample, X_test_undersample, y_train_undersample, y_test_undersample = train_test_split(X_undersample\n                                                                                                   ,y_undersample\n                                                                                                   ,test_size = 0.3\n                                                                                                   ,random_state = 0)\nprint(\"\")\nprint(\"Number transactions train dataset: \", len(X_train_undersample))\nprint(\"Number transactions test dataset: \", len(X_test_undersample))\nprint(\"Total number of transactions: \", len(X_train_undersample)+len(X_test_undersample))","89aaf868":"Fraud = data[data['Class']==1]\nValid = data[data['Class']==0]","37fd214b":"print(\"Fraud Cases : {}\".format(len(Fraud)))\nprint(\"Valid Cases : {}\".format(len(Valid)))","ac195cae":"data.hist(figsize=(20, 20))\nplt.show()","c537b19a":"correlation_matrix = data.corr()\nfig = plt.figure(figsize=(12,9))\nsns.heatmap(correlation_matrix,vmax=0.8,square = True)\nplt.show()","86fb67d1":"clf = OneClassSVM(kernel='rbf', degree=3, gamma=0.1,nu=0.05, \n                                         max_iter=-1)\nclf.fit(X_train)","98ec03e1":"y_pred = clf.predict(X_train)","93bcf89d":"In this notebook I will test different methods on this unbalanced data. The aim is to compare if preprocessing techniques work better when there is an immense majority class that can disturb the accuracy and efficiency of our predictive model. ","68d6ed31":"I have used only one classifier. You can use as many classifier you want. Try to do this and compare the accuracy of all the classifier.\nAnd see which is the best classifier!!!!!!!!!!!!!!!!!!!!","a79fe272":"**Splition of dataset into test and train set.**","ee118bfc":"# **Credit Card Fraud Detection**","ad486d2b":"There are several ways to approach this classification problem taking into consideration this unbalance.","cf3377a8":"**Resampling : **\n\n1. As we mentioned earlier, there are several ways to resample skewed data. Apart from under and over sampling, there is a very popular approach called SMOTE (Synthetic Minority Over-Sampling Technique), which is a combination of oversampling and undersampling, but the oversampling approach is not by replicating minority class but constructing new minority class data instance via an algorithm. \n2. In this notebook, we will use traditional UNDER-sampling. I will probably try to implement SMOTE in future versions of the code, but for now I will use traditional undersamplig. \n3. The way we will under sample the dataset will be by creating a 50\/50 ratio. This will be done by randomly selecting \"x\" amount of sample from the majority class, being \"x\" the total number of records with the minority class.","d4cd2dd3":"* Collect more data? Nice strategy but not applicable in this case\n* Changing the performance metric:\n    * Use the confusio nmatrix to calculate Precision, Recall\n    * F1score (weighted average of precision recall)\n    * Use Kappa - which is a classification accuracy normalized by the imbalance of the classes in the data\n    * ROC curves - calculates sensitivity\/specificity ratio.\n* Resampling the dataset\n    * Essentially this is a method that will process the data to have an approximate 50-50 ratio.\n    * One way to achieve this is by OVER-sampling, which is adding copies of the under-represented class (better when you have little data)\n    * Another is UNDER-sampling, which deletes instances from the over-represented class (better when he have lot's of data)","efdcc62d":"Approch:","ff17afb3":"Let's see the predictions.. \ud83d\ude0e  \ud83d\ude0e  \ud83d\ude0e  \ud83d\ude0e ","2645bfcc":"* We are not going to perform feature engineering in first instance. The dataset has been downgraded in order to contain 30 features (28 anonamised + time + amount).\n* We will then compare what happens when using resampling and when not using it. We will test this approach using a simple logistic regression classifier.\n* We will evaluate the models by using some of the performance metrics mentioned above.\n* We will repeat the best resampling\/not resampling method, by tuning the parameters in the logistic regression classifier.\n* We will finally perform classifications model using other classification algorithms.","e72c4018":"Now let us print the no of Fraud and Valid Transaction cases","9a9bfcd9":"Determine the number of fraud and valid transactions in the dataset.","584c18e3":"Normalising the amount column. The amount column is not in line with the anonimised features","c7db3c39":"Till then **Enjoy Machine Learning**","d6d44380":"**Input and labels(target) + resampling **","b916f616":"Making Classifier for predicting fraud or not.","11b27482":"Clearly the data is totally unbalanced and is responsible for the decrement in the accuracy of our predictive model.","9f1ffae4":"**Assigning X and Y. No resampling**","c812c293":"Plot histogram of each parameter"}}