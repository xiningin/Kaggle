{"cell_type":{"732c1859":"code","b2ee5446":"code","062af199":"code","d336b804":"code","b1a0fca9":"code","b2ecd31b":"code","37dd55f5":"code","99c6652e":"code","89b9536f":"code","92e2d4b1":"code","629b43eb":"code","f7fe7e96":"code","3292b7f4":"code","1a73037c":"code","73cc226c":"code","8fe2b4c0":"code","f6cf665e":"code","b31a6d66":"code","4a6f6dde":"code","e17b120b":"code","90e43710":"code","6675b98e":"code","aa9faf91":"code","76926fc6":"code","44e57fc2":"code","935f8c6e":"code","61292336":"code","504cb8c9":"code","2065a18b":"code","d93a216a":"code","36ef8c6b":"code","331399a4":"markdown","2b4bf454":"markdown","9d0c762d":"markdown","5b28bf0e":"markdown","41184bbf":"markdown","c32dbf67":"markdown","ccf7c077":"markdown","10d17135":"markdown","2fc6f4ac":"markdown","02fe3b1a":"markdown","6ec06b6f":"markdown","9a024922":"markdown","b74bcaaf":"markdown","38f8db79":"markdown","e7ad41cb":"markdown","49b8cee0":"markdown"},"source":{"732c1859":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b2ee5446":"import numpy as np\nimport pandas as pd\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')","062af199":"import nltk.data\nfrom nltk import tokenize","d336b804":"import logging  # Setting up the loggings to monitor gensim\nlogging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)","b1a0fca9":"import matplotlib.pyplot as plt\n%matplotlib inline","b2ecd31b":"import nltk","37dd55f5":"import multiprocessing\nfrom gensim.models.fasttext import FastText\nfrom time import time\nimport gensim","99c6652e":"from sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm","89b9536f":"tokenizer = nltk.data.load('tokenizers\/punkt\/russian.pickle')\n\nfp = open('..\/input\/gensim-fasttext-training-corpus-for-demo-only\/wrk.txt', encoding='utf-8-sig', errors='ignore')\n\ndata = fp.read()\n\nx = tokenizer.tokenize(data)\n\ndf = pd.DataFrame(x, columns=['text'])","92e2d4b1":"text = df[\"text\"].str.replace(r'([^\\w\\s]+)', ' \\\\1 ').str.strip()","629b43eb":"type(text)","f7fe7e96":"len(text)","3292b7f4":"text[:20]","1a73037c":"text = text.astype(str)","73cc226c":"text = text.str.lower()","8fe2b4c0":"text = \" \".join(map(str, (review for review in text)))\nprint (\"There are {} words in the combination of all review.\".format(len(text)))","f6cf665e":"WORD = re.compile(r'\\w+')\ndef regTokenize(text):\n    words = WORD.findall(text)\n    return words","b31a6d66":"text_tokens = regTokenize(text)","4a6f6dde":"text = nltk.Text(text_tokens)\nprint(type(text))\ntext[:10]","e17b120b":"text_raw = \" \".join(text)","90e43710":"textfile = open('text.txt', 'w')\ntextfile.write(text_raw)\ntextfile.close()","6675b98e":"data = gensim.models.word2vec.LineSentence('text.txt')","aa9faf91":"%%time\nft_model = FastText(data, vector_size=100, window=5, min_count=5, workers=4,sg=1)","76926fc6":"ft_model.save(\"ft_model.model\")","44e57fc2":"ft_model.wv.most_similar(positive=[\"\u0444\u0430\u043a\u0442\"], topn=20)","935f8c6e":"ft_model.wv.similarity(\"\u0444\u0430\u043a\u0442\", '\u0442\u0435\u043e\u0440\u0438\u044f')","61292336":"ft_model.wv.doesnt_match([\"\u0441\u0443\u0431\u044a\u0435\u043a\u0442\", '\u043e\u0431\u044a\u0435\u043a\u0442', '\u0430\u0440\u0442\u0435\u0444\u0430\u043a\u0442'])","504cb8c9":"ft_model.wv.most_similar(positive=[\"\u0441\u0443\u0431\u044a\u0435\u043a\u0442\", '\u043e\u0431\u044a\u0435\u043a\u0442'], negative=['\u0430\u0440\u0442\u0435\u0444\u0430\u043a\u0442'], topn=20)","2065a18b":"def tsne_plot(labels, tokens, classes, clusters):\n    tsne_model = TSNE(perplexity=15, n_components=2, init='pca', n_iter=3500, random_state=33)\n    new_values = tsne_model.fit_transform(tokens)\n\n    x = []\n    y = []\n\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n\n    colors = cm.rainbow(np.linspace(0, 1, clusters))\n    \n    plt.figure(figsize=(16, 9))\n    for i in range(len(x)):\n        plt.scatter(x[i], y[i], c=colors[classes[i]].reshape(1,-1), alpha=0.75)\n        plt.annotate(labels[i], alpha=0.75, xy=(x[i], y[i]), xytext=(5, 2), \n                     textcoords='offset points', ha='right', va='bottom', size=10)\n        \n    plt.grid(True)\n    plt.savefig('embedding.png', dpi=300)\n    plt.show()","d93a216a":"labels = []\ntokens = []\nclasses = []\n\nwords = ['\u0442\u0435\u043e\u0440\u0438\u044f',  '\u0444\u0430\u043a\u0442', '\u0441\u0443\u0431\u044a\u0435\u043a\u0442', '\u043e\u0431\u044a\u0435\u043a\u0442', '\u0430\u0440\u0442\u0435\u0444\u0430\u043a\u0442']\n                   \nwords.sort()\n\nsamples = len(words)\nfor i, word in enumerate(words):\n    tokens.append(ft_model.wv[word])\n    labels.append(word.upper())\n    classes.append(i)\n    for similar_word, similarity in ft_model.wv.most_similar(word, topn=10):\n        tokens.append(ft_model.wv[similar_word])\n        labels.append(similar_word)\n        classes.append(i)","36ef8c6b":"tsne_plot(labels, tokens, classes, samples)","331399a4":"\u041f\u0440\u0438\u0432\u043e\u0434\u0438\u043c \u0441\u043b\u043e\u0432\u0430 \u043a \u043d\u0438\u0436\u043d\u0435\u043c\u0443 \u0440\u0435\u0433\u0438\u0441\u0442\u0440\u0443:","2b4bf454":"\u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c:","9d0c762d":"\u0417\u0430\u043c\u0435\u0442\u043d\u043e, \u0447\u0442\u043e \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0443\u0436\u043d\u043e \u043a\u0430\u043a \u0441\u043b\u0435\u0434\u0443\u0435\u0442 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c.","5b28bf0e":"\u041f\u043e\u0434\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0431\u0438\u0431\u0438\u043b\u0438\u043e\u0442\u0435\u043a\u0438 \u0438 \u0434\u0430\u043d\u043d\u044b\u0435:","41184bbf":"\u041f\u0440\u0438\u0432\u043e\u0434\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0432 \u043d\u0443\u0436\u043d\u044b\u0439 \u043d\u0430\u043c \u0441\u0442\u0440\u043e\u043a\u043e\u0432\u044b\u0439 \u0444\u043e\u0440\u043c\u0430\u0442:","c32dbf67":"\u0421\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0445\u0430\u0440\u0430\u043a\u0442\u0435\u0440\u0438\u0441\u0442\u0438\u043a\u0438 \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445:","ccf7c077":"\u042d\u0442\u043e \u0434\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0439 \u0431\u043b\u043e\u043a\u043d\u043e\u0442 \u0434\u043b\u044f \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u0441\u0432\u043e\u0435\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 Gensim fasttext \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0440\u0443\u0441\u0441\u043a\u043e\u044f\u0437\u044b\u0447\u043d\u043e\u0433\u043e \u043a\u043e\u0440\u043f\u0443\u0441\u0430. \u0412 \u043a\u043e\u0434\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u044b \u0432\u0441\u0435\u0432\u043e\u0437\u043c\u043e\u0436\u043d\u044b\u0435 \u0440\u0435\u0448\u0435\u043d\u0438\u044f \u0438\u0437 \u043e\u0442\u043a\u0440\u044b\u0442\u044b\u0445 \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u043e\u0432.","10d17135":"\u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438:","2fc6f4ac":"\u0415\u0441\u043b\u0438 \u043d\u0430\u043c \u043d\u0443\u0436\u043d\u043e \u0434\u043e\u043e\u0431\u0443\u0447\u0438\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c, \u0442\u043e \u0432\u043c\u0435\u0441\u0442\u043e \u043a\u043e\u0434\u0430 \u0432 \u044f\u0447\u0435\u0439\u043a\u0435 \u0432\u044b\u0448\u0435 \u043d\u0443\u0436\u043d\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u044d\u0442\u043e:\n\n> %%time\n\n> ft_model = FastText.load(\"ft_model.model\")\n\n> t = time()\n\n> ft_model.build_vocab(data, progress_per=10000, update=True)\n\n> print('Time to build vocab: {} mins'.format(round((time() - t) \/ 60, 2)))\n\n> ft_model.train(data, total_examples=ft_model.corpus_count, epochs=5, report_delay=1)\n\n> print('Time to train the model: {} mins'.format(round((time() - t) \/ 60, 2)))","02fe3b1a":"\u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u043f\u0440\u043e\u0431\u0435\u043b\u044b \u043c\u0435\u0436\u0434\u0443 \u0441\u043b\u043e\u0432\u0430\u043c\u0438 \u0438 \u0437\u043d\u0430\u043a\u0430\u043c\u0438 \u043f\u0440\u0435\u043f\u0438\u043d\u0430\u043d\u0438\u044f:","6ec06b6f":"\u0422\u0435\u043f\u0435\u0440\u044c \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0442\u0435\u043a\u0441\u0442:","9a024922":"\u0418 \u043e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u0441 \u043d\u0443\u0436\u043d\u044b\u043c\u0438 \u043d\u0430\u043c \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438 (\u0432 \u043f\u0440\u0438\u043c\u0435\u0440\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0442\u0441\u044f \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f)","b74bcaaf":"\u0422\u0435\u043f\u0435\u0440\u044c \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u044b\u0445 \u043f\u0440\u043e\u0446\u0435\u0434\u0443\u0440 \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u0435\u043c \u0442\u0435\u043a\u0441\u0442.","38f8db79":"\u0422\u0435\u043f\u0435\u0440\u044c \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c \u043f\u043e \u043d\u0430\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044e: \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c \u0441\u043b\u043e\u0432\u0430 \u043d\u0430 \u0441\u0445\u043e\u0434\u0441\u0442\u0432\u043e, \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0433\u0440\u0443\u043f\u043f\u044b \u0441\u043b\u043e\u0432 \u0438 \u0442\u0430\u043a \u0434\u0430\u043b\u0435\u0435:","e7ad41cb":"\u041d\u0430 \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0435\u043c \u0448\u0430\u0433\u0435 \u0431\u044b\u043b\u0438 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u0430\u043d\u044b \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435. \n\n1) \u041d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0444\u0430\u0439\u043b\u043e\u0432 \u0432 \u0444\u043e\u0440\u043c\u0430\u0442\u0435 .pdf \u0431\u044b\u043b\u0438 \u0441\u043a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u044b \u0432 \u0444\u043e\u0440\u043c\u0430\u0442 .txt \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043a\u043e\u043c\u0430\u043d\u0434\u044b \u0432 bash:\n\n> sudo apt-get update && sudo apt-get install -y xpdf\n> for file in *.pdf; do pdftotext \"$file\" \"$file.txt\"; done\n\n2) \u041e\u0431\u044a\u0435\u0434\u0438\u043d\u044f\u0435\u043c \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u0432 \u043e\u0434\u0438\u043d \u0444\u0430\u0439\u043b \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043a\u043e\u043c\u0430\u043d\u0434\u044b \u0432 bash:\n\n> find . -type f -name \"*\" -exec cat >> wrk.txt {} \\;\n","49b8cee0":"\u0422\u0435\u043f\u0435\u0440\u044c \u0434\u0430\u043d\u043d\u044b\u0435 \u0432\u044b\u0433\u043b\u044f\u0434\u044f\u0442 \u0437\u0430\u043c\u0435\u0442\u043d\u043e \u043b\u0443\u0447\u0448\u0435."}}