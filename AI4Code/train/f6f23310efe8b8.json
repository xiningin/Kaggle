{"cell_type":{"0796cf7c":"code","c046cace":"code","efab0455":"code","ccb96100":"code","d7d3ec80":"code","71a3df8e":"code","adcd0cc0":"code","db4075a6":"code","9269bdbf":"code","6cece114":"code","96c36a19":"markdown","39037b2a":"markdown","5b427a86":"markdown","3327e561":"markdown","4c654775":"markdown"},"source":{"0796cf7c":"!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version \"nightly\"","c046cace":"!pip install timm","efab0455":"import os","ccb96100":"# Result Visualization Helper\nfrom matplotlib import pyplot as plt\n\nM, N = 4, 6\nRESULT_IMG_PATH = '\/tmp\/test_result.jpg'\nCIFAR10_LABELS = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n                 'dog', 'frog', 'horse', 'ship', 'truck']\n\ndef plot_results(images, labels, preds):\n  images, labels, preds = images[:M*N], labels[:M*N], preds[:M*N]\n  inv_norm = transforms.Normalize(\n      mean=(-0.4914\/0.2023, -0.4822\/0.1994, -0.4465\/0.2010),\n      std=(1\/0.2023, 1\/0.1994, 1\/0.2010))\n\n  num_images = images.shape[0]\n  fig, axes = plt.subplots(M, N, figsize=(16, 9))\n  fig.suptitle('Correct \/ Predicted Labels (Red text for incorrect ones)')\n\n  for i, ax in enumerate(fig.axes):\n    ax.axis('off')\n    if i >= num_images:\n      continue\n    img, label, prediction = images[i], labels[i], preds[i]\n    img = inv_norm(img)\n    img = img.permute(1, 2, 0) # (C, M, N) -> (M, N, C)\n    label, prediction = label.item(), prediction.item()\n    if label == prediction:\n      ax.set_title(u'\\u2713', color='blue', fontsize=22)\n    else:\n      ax.set_title(\n          'X {}\/{}'.format(CIFAR10_LABELS[label],\n                          CIFAR10_LABELS[prediction]), color='red')\n    ax.imshow(img)\n  plt.savefig(RESULT_IMG_PATH, transparent=True)","d7d3ec80":"# Define Parameters\nFLAGS = {}\nFLAGS['data_dir'] = \"\/tmp\/cifar\"\nFLAGS['batch_size'] = 128\nFLAGS['num_workers'] = 4\nFLAGS['learning_rate'] = 0.0003\nFLAGS['momentum'] = 0.9\nFLAGS['num_epochs'] = 3\nFLAGS['num_cores'] = 8 if os.environ.get('TPU_NAME', None) else 1\nFLAGS['log_steps'] = 20\nFLAGS['metrics_debug'] = True","71a3df8e":"import numpy as np\nimport os\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch_xla\nimport torch_xla.core.xla_model as xm\nimport torch_xla.debug.metrics as met\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.utils.utils as xu\nimport torchvision\nfrom torchvision import datasets, transforms\n\nimport timm","adcd0cc0":"# Using Ross Wightman's timm package\nclass TimmModels(nn.Module):\n    def __init__(self, model_name,pretrained=True, num_classes=1000):\n        super(TimmModels, self).__init__()\n        self.m = timm.create_model(model_name,pretrained=pretrained)\n        model_list = list(self.m.children())\n        model_list[-1] = nn.Linear(\n            in_features=model_list[-1].in_features, \n            out_features=num_classes, \n            bias=True\n        )\n        self.m = nn.Sequential(*model_list)\n        \n    def forward(self, image):\n        out = self.m(image)\n        return out","db4075a6":"SERIAL_EXEC = xmp.MpSerialExecutor()\n# Only instantiate model weights once in memory.\nWRAPPED_MODEL = xmp.MpModelWrapper(TimmModels('efficientnet_b0'))\n\ndef train_resnet18():\n  torch.manual_seed(1)\n\n  def get_dataset():\n    norm = transforms.Normalize(\n        mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))\n    transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        norm,\n    ])\n    transform_test = transforms.Compose([\n        transforms.ToTensor(),\n        norm,\n    ])\n    train_dataset = datasets.CIFAR10(\n        root=FLAGS['data_dir'],\n        train=True,\n        download=True,\n        transform=transform_train)\n    test_dataset = datasets.CIFAR10(\n        root=FLAGS['data_dir'],\n        train=False,\n        download=True,\n        transform=transform_test)\n    \n    return train_dataset, test_dataset\n  \n  # Using the serial executor avoids multiple processes\n  # to download the same data.\n  train_dataset, test_dataset = SERIAL_EXEC.run(get_dataset)\n\n  train_sampler = torch.utils.data.distributed.DistributedSampler(\n      train_dataset,\n      num_replicas=xm.xrt_world_size(),\n      rank=xm.get_ordinal(),\n      shuffle=True)\n  train_loader = torch.utils.data.DataLoader(\n      train_dataset,\n      batch_size=FLAGS['batch_size'],\n      sampler=train_sampler,\n      num_workers=FLAGS['num_workers'],\n      drop_last=True)\n  test_loader = torch.utils.data.DataLoader(\n      test_dataset,\n      batch_size=FLAGS['batch_size'],\n      shuffle=False,\n      num_workers=FLAGS['num_workers'],\n      drop_last=True)\n\n  # Scale learning rate to num cores\n  learning_rate = FLAGS['learning_rate'] * xm.xrt_world_size()\n\n  # Get loss function, optimizer, and model\n  device = xm.xla_device()\n  model = WRAPPED_MODEL.to(device)\n  optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n                        momentum=FLAGS['momentum'], weight_decay=5e-4)\n  loss_fn = nn.NLLLoss()\n\n  def train_loop_fn(loader):\n    tracker = xm.RateTracker()\n    model.train()\n    for x, (data, target) in enumerate(loader):\n      optimizer.zero_grad()\n      output = model(data)\n      loss = loss_fn(output, target)\n      loss.backward()\n      xm.optimizer_step(optimizer)\n      tracker.add(FLAGS['batch_size'])\n      if x % FLAGS['log_steps'] == 0:\n        print('[xla:{}]({}) Loss={:.5f} Rate={:.2f} GlobalRate={:.2f} Time={}'.format(\n            xm.get_ordinal(), x, loss.item(), tracker.rate(),\n            tracker.global_rate(), time.asctime()), flush=True)\n\n  def test_loop_fn(loader):\n    total_samples = 0\n    correct = 0\n    model.eval()\n    data, pred, target = None, None, None\n    for data, target in loader:\n      output = model(data)\n      pred = output.max(1, keepdim=True)[1]\n      correct += pred.eq(target.view_as(pred)).sum().item()\n      total_samples += data.size()[0]\n\n    accuracy = 100.0 * correct \/ total_samples\n    print('[xla:{}] Accuracy={:.2f}%'.format(\n        xm.get_ordinal(), accuracy), flush=True)\n    return accuracy, data, pred, target\n\n  # Train and eval loops\n  accuracy = 0.0\n  data, pred, target = None, None, None\n  for epoch in range(1, FLAGS['num_epochs'] + 1):\n    para_loader = pl.ParallelLoader(train_loader, [device])\n    train_loop_fn(para_loader.per_device_loader(device))\n    xm.master_print(\"Finished training epoch {}\".format(epoch))\n\n    para_loader = pl.ParallelLoader(test_loader, [device])\n    accuracy, data, pred, target  = test_loop_fn(para_loader.per_device_loader(device))\n    if FLAGS['metrics_debug']:\n      xm.master_print(met.metrics_report(), flush=True)\n\n  return accuracy, data, pred, target\n  ","9269bdbf":"# Start training processes\ndef _mp_fn(rank, flags):\n  global FLAGS\n  FLAGS = flags\n  torch.set_default_tensor_type('torch.FloatTensor')\n  accuracy, data, pred, target = train_resnet18()\n  #if rank == 0:\n  #  # Retrieve tensors that are on TPU core 0 and plot.\n  #  plot_results(data.cpu(), pred.cpu(), target.cpu())\n\nxmp.spawn(_mp_fn, args=(FLAGS,), nprocs=FLAGS['num_cores'],\n          start_method='fork')","6cece114":"from google.colab.patches import cv2_imshow\nimport cv2\nimg = cv2.imread(RESULT_IMG_PATH, cv2.IMREAD_UNCHANGED)\ncv2_imshow(img)","96c36a19":"## Visualize Predictions","39037b2a":"Only run the below commented cell if you would like a nightly release","5b427a86":"### Define Parameters\n\n","3327e561":"### [RUNME] Install Colab compatible PyTorch\/XLA wheels and dependencies\n\n","4c654775":"## PyTorch\/XLA ResNet18\/CIFAR10 (GPU or TPU)"}}