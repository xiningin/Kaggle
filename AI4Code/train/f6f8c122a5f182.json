{"cell_type":{"eee848e8":"code","0e247855":"code","8fb31fea":"code","4797c95a":"code","8d5ae955":"code","e3dca1b1":"code","6495f8d2":"code","3c33015f":"code","9a8f99c2":"code","b045233c":"code","fef22a71":"code","cd66b368":"code","a488a59a":"code","abd0dd01":"code","82bfe5c8":"code","59845c45":"code","5591c4b8":"code","b7b7240b":"code","bcc42429":"code","d21c2b39":"code","fc36f7da":"code","2a044faf":"code","d222455d":"code","db6a6c99":"markdown","fcee4190":"markdown","cac0b42e":"markdown","a641fb60":"markdown","c5c0ceb5":"markdown","b62e5f80":"markdown","d839f039":"markdown","dc83283f":"markdown","8ba53e28":"markdown","f43facce":"markdown","abc1db1c":"markdown","5e2f68ec":"markdown","017b0928":"markdown","5c6d004d":"markdown","d2d5684c":"markdown","a90dcd1f":"markdown","2ca32de5":"markdown","e9c1e3e3":"markdown","425cd9b3":"markdown"},"source":{"eee848e8":"!pip install -q timm pycocotools scipy\n!pip install -qU pytorch-lightning transformers","0e247855":"!cd ..\/input\/coco-lp-preview\/coco_lp\/ && ls","8fb31fea":"import torchvision\nimport os\n\nclass CocoDetection(torchvision.datasets.CocoDetection):\n    def __init__(self, img_folder, feature_extractor, train=True):\n        ann_file = os.path.join(img_folder, \"custom_train.json\" if train else \"custom_val.json\")\n        super(CocoDetection, self).__init__(img_folder, ann_file)\n        self.feature_extractor = feature_extractor\n\n    def __getitem__(self, idx):\n        # read in PIL image and target in COCO format\n        img, target = super(CocoDetection, self).__getitem__(idx)\n        \n        # preprocess image and target (converting target to DETR format, resizing + normalization of both image and target)\n        image_id = self.ids[idx]\n        target = {'image_id': image_id, 'annotations': target}\n        encoding = self.feature_extractor(images=img, annotations=target, return_tensors=\"pt\")\n        pixel_values = encoding[\"pixel_values\"].squeeze() # remove batch dimension\n        target = encoding[\"labels\"][0] # remove batch dimension\n\n        return pixel_values, target","4797c95a":"from transformers import DetrFeatureExtractor\n\nimg_folder = \"..\/input\/coco-lp-preview\/coco_lp\"\n\nfeature_extractor = DetrFeatureExtractor.from_pretrained(\"facebook\/detr-resnet-50\")\n\ntrain_dataset = CocoDetection(img_folder=f'{img_folder}\/train', feature_extractor=feature_extractor)\nval_dataset = CocoDetection(img_folder=f'{img_folder}\/val', feature_extractor=feature_extractor, train=False)","8d5ae955":"print(\"Number of training examples:\", len(train_dataset))\nprint(\"Number of validation examples:\", len(val_dataset))","e3dca1b1":"import numpy as np\nimport os\nfrom PIL import Image, ImageDraw\n\n# based on https:\/\/github.com\/woctezuma\/finetune-detr\/blob\/master\/finetune_detr.ipynb\nimage_ids = train_dataset.coco.getImgIds()\n# let's pick a random image\nimage_id = image_ids[np.random.randint(0, len(image_ids))]\nprint('Image n\u00b0{}'.format(image_id))\nimage = train_dataset.coco.loadImgs(image_id)[0]\nimage = Image.open(os.path.join(f'{img_folder}\/train', image['file_name']))\n\nannotations = train_dataset.coco.imgToAnns[image_id]\ndraw = ImageDraw.Draw(image, \"RGBA\")\n\ncats = train_dataset.coco.cats\nid2label = {k: v['name'] for k,v in cats.items()}\n\nfor annotation in annotations:\n  box = annotation['bbox']\n  class_idx = annotation['category_id']\n  x,y,w,h = tuple(box)\n  draw.rectangle((x,y,x+w,y+h), outline='red', width=1)\n  draw.text((x, y), id2label[class_idx], fill='white')\n\nimage","6495f8d2":"from torch.utils.data import DataLoader\n\ndef collate_fn(batch):\n  pixel_values = [item[0] for item in batch]\n  encoding = feature_extractor.pad_and_create_pixel_mask(pixel_values, return_tensors=\"pt\")\n  labels = [item[1] for item in batch]\n  batch = {}\n  batch['pixel_values'] = encoding['pixel_values']\n  batch['pixel_mask'] = encoding['pixel_mask']\n  batch['labels'] = labels\n  return batch\n\ntrain_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=4, shuffle=True)\nval_dataloader = DataLoader(val_dataset, collate_fn=collate_fn, batch_size=2)\nbatch = next(iter(train_dataloader))","3c33015f":"batch.keys()","9a8f99c2":"pixel_values, target = train_dataset[0]","b045233c":"pixel_values.shape","fef22a71":"print(target)","cd66b368":"import pytorch_lightning as pl\nfrom transformers import DetrConfig, DetrForObjectDetection\nimport torch\n\nclass Detr(pl.LightningModule):\n\n     def __init__(self, lr, lr_backbone, weight_decay):\n         super().__init__()\n         # replace COCO classification head with custom head\n         self.model = DetrForObjectDetection.from_pretrained(\"facebook\/detr-resnet-50\", \n                                                             num_labels=len(id2label),\n                                                             ignore_mismatched_sizes=True)\n         # see https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/pull\/1896\n         self.lr = lr\n         self.lr_backbone = lr_backbone\n         self.weight_decay = weight_decay\n\n     def forward(self, pixel_values, pixel_mask):\n       outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n\n       return outputs\n     \n     def common_step(self, batch, batch_idx):\n       pixel_values = batch[\"pixel_values\"]\n       pixel_mask = batch[\"pixel_mask\"]\n       labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n\n       outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n\n       loss = outputs.loss\n       loss_dict = outputs.loss_dict\n\n       return loss, loss_dict\n\n     def training_step(self, batch, batch_idx):\n        loss, loss_dict = self.common_step(batch, batch_idx)     \n        # logs metrics for each training_step,\n        # and the average across the epoch\n        self.log(\"training_loss\", loss)\n        for k,v in loss_dict.items():\n          self.log(\"train_\" + k, v.item())\n\n        return loss\n\n     def validation_step(self, batch, batch_idx):\n        loss, loss_dict = self.common_step(batch, batch_idx)     \n        self.log(\"validation_loss\", loss)\n        for k,v in loss_dict.items():\n          self.log(\"validation_\" + k, v.item())\n\n        return loss\n\n     def configure_optimizers(self):\n        param_dicts = [\n              {\"params\": [p for n, p in self.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n              {\n                  \"params\": [p for n, p in self.named_parameters() if \"backbone\" in n and p.requires_grad],\n                  \"lr\": self.lr_backbone,\n              },\n        ]\n        optimizer = torch.optim.AdamW(param_dicts, lr=self.lr,\n                                  weight_decay=self.weight_decay)\n        \n        return optimizer\n\n     def train_dataloader(self):\n        return train_dataloader\n\n     def val_dataloader(self):\n        return val_dataloader","a488a59a":"# Start tensorboard.\n%load_ext tensorboard\n%tensorboard --logdir lightning_logs\/","abd0dd01":"model = Detr(lr=1e-4, lr_backbone=1e-5, weight_decay=1e-4)\n\noutputs = model(pixel_values=batch['pixel_values'], pixel_mask=batch['pixel_mask'])","82bfe5c8":"outputs.logits.shape","59845c45":"from pytorch_lightning import Trainer\nfrom tqdm.notebook import tqdm\n\ntrainer = Trainer(gpus=1, max_steps=100, gradient_clip_val=0.1)\ntrainer.fit(model)","5591c4b8":"!nvidia-smi","b7b7240b":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel.to(device)\nmodel.eval()","bcc42429":"import torch\nimport matplotlib.pyplot as plt\n\n# colors for visualization\nCOLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n\n# for output bounding box post-processing\ndef box_cxcywh_to_xyxy(x):\n    x_c, y_c, w, h = x.unbind(1)\n    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n    return torch.stack(b, dim=1)\n\ndef rescale_bboxes(out_bbox, size):\n    img_w, img_h = size\n    b = box_cxcywh_to_xyxy(out_bbox)\n    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n    return b\n\ndef plot_results(pil_img, prob, boxes):\n    plt.figure(figsize=(16,10))\n    plt.imshow(pil_img)\n    ax = plt.gca()\n    colors = COLORS * 100\n    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), colors):\n        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n                                   fill=False, color=c, linewidth=3))\n        cl = p.argmax()\n        text = f'{id2label[cl.item()]}: {p[cl]:0.2f}'\n        ax.text(xmin, ymin, text, fontsize=15,\n                bbox=dict(facecolor='yellow', alpha=0.5))\n    plt.axis('off')\n    plt.show()","d21c2b39":"def visualize_predictions(image, outputs, threshold=0.9, keep_highest_scoring_bbox=False):\n  # keep only predictions with confidence >= threshold\n  probas = outputs.logits.softmax(-1)[0, :, :-1]\n  keep = probas.max(-1).values > threshold\n  if keep_highest_scoring_bbox:\n    keep = probas.max(-1).values.argmax()\n    keep = torch.tensor([keep])\n  \n  # convert predicted boxes from [0; 1] to image scales\n  bboxes_scaled = rescale_bboxes(outputs.pred_boxes[0, keep].cpu(), image.size)\n    \n  # plot results\n  plot_results(image, probas[keep], bboxes_scaled)","fc36f7da":"it = iter(range(1500))","2a044faf":"#We can use the image_id in target to know which image it is\npixel_values, target = val_dataset[next(it)]\n\npixel_values = pixel_values.unsqueeze(0).to(device)\nprint(pixel_values.shape)\n\n# forward pass to get class logits and bounding boxes\noutputs = model(pixel_values=pixel_values, pixel_mask=None)\n\nimage_id = target['image_id'].item()\nimage = val_dataset.coco.loadImgs(image_id)[0]\nimage = Image.open(os.path.join(f'{img_folder}\/val', image['file_name']))\n\nvisualize_predictions(image, outputs, threshold=0.3, keep_highest_scoring_bbox=True)","d222455d":"trainer.save_checkpoint(\"detr_model.ckpt\")","db6a6c99":"## Fine-tuning DETR for license plates detection\n\nIn this notebook, we are going to fine-tune [DETR](https:\/\/ai.facebook.com\/blog\/end-to-end-object-detection-with-transformers\/) (end-to-end object detection with Transformers) on a [custom moroccan license plates dataset](https:\/\/www.kaggle.com\/elmehditaf96\/moroccan-vehicle-registration-plates). The goal for the model is to recognize license plates in pictures.\n\n![](https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2020\/05\/Screenshot-from-2020-05-27-17-48-38.png)\n\nAlso big thanks to Niels Rogge from Huggingface for his [Transformers tutorials](https:\/\/github.com\/NielsRogge\/Transformers-Tutorials), as well as the creators of [this notebook](https:\/\/github.com\/woctezuma\/finetune-detr\/blob\/master\/finetune_detr.ipynb) and [this notebook](https:\/\/www.kaggle.com\/tanulsingh077\/end-to-end-object-detection-with-transformers-detr), which helped me a lot in understanding how to fine-tune DETR on a custom dataset \n\n","fcee4190":"Let's verify the shape of the `pixel_values`, and check the `target`::","cac0b42e":"Based on the class defined above, we create training and validation datasets.","a641fb60":"For the training I used [Sagemaker Studio Lab](https:\/\/studiolab.sagemaker.aws\/) which offers performant GPUs. So don't hesitate to reduce the batch size if you run into memory errors.","c5c0ceb5":"## Train the model using PyTorch Lightning\n\nHere we define a `LightningModule`, which is an `nn.Module` with some extra functionality.\n\nFor more information regarding PyTorch Lightning, I recommend the [docs](https:\/\/pytorch-lightning.readthedocs.io\/en\/latest\/?_ga=2.35105442.2002381006.1623231889-1738348008.1615553774) as well as the [tutorial notebooks](https:\/\/github.com\/PyTorchLightning\/lightning-tutorials\/tree\/aeae8085b48339e9bd9ab61d81cc0dc8b0d48f9c\/.notebooks\/starters). \n\nYou can of course just train the model in native PyTorch as an alternative.","b62e5f80":"As you can see, this dataset is tiny:","d839f039":"## Download + preprocess data\n\nHere we download the [license plates dataset](https:\/\/www.kaggle.com\/elmehditaf96\/moroccan-vehicle-registration-plates) hosted on kaggle (in [YOLOv4 format](https:\/\/github.com\/AlexeyAB\/darknet)) and convert it to [COCO Object Detection Format](https:\/\/cocodataset.org\/#format-data).\n\nWe can use [fiftyone's converter tool](https:\/\/voxel51.com\/docs\/fiftyone\/recipes\/convert_datasets.html) for that.\n\nFor the sake of demo on kaggle we're only gonna use 20 images from that dataset.","dc83283f":"## Create PyTorch dataset + dataloaders\n\nHere we define a regular PyTorch dataset. Each item of the dataset is an image and corresponding annotations. Torchvision already provides a `CocoDetection` dataset, which we can use. We only add a feature extractor (`DetrFeatureExtractor`) to resize + normalize the images, and to turn the annotations (which are in COCO format) in the format that DETR expects. It will also resize the annotations accordingly.","8ba53e28":"The logits are of shape `(batch_size, num_queries, number of classes + 1)`. We model internally adds an additional \"no object class\", which explains why we have one additional output for the class dimension. ","f43facce":"Next, let's create corresponding dataloaders. We define a custom `collate_fn` to batch images together. As DETR resizes images to have a min size of 800 and a max size of 1333, images can have different sizes. We pad images (`pixel_values`) to the largest image in a batch, and create a corresponding `pixel_mask` to indicate which pixels are real (1)\/which are padding (0).  ","abc1db1c":"## Set up environment","5e2f68ec":"Let's verify an example by visualizing it. We can access the COCO API of the dataset by typing `train_dataset.coco`. ","017b0928":"## Inference (+ visualization)\n\nLet's visualize the predictions of DETR on the first image of the validation set.","5c6d004d":"[![Open In Studio Lab](https:\/\/studiolab.sagemaker.aws\/studiolab.svg)](https:\/\/studiolab.sagemaker.aws\/import\/github\/NouamaneTazi\/number-plate-recognition\/blob\/main\/notebooks\/DETR_license_plates.ipynb) \n<a href=\"https:\/\/colab.research.google.com\/github\/NouamaneTazi\/number-plate-recognition\/blob\/main\/notebooks\/DETR_license_plates.ipynb\" target=\"_parent\"><img src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\" alt=\"Open In Colab\"\/><\/a> ","d2d5684c":"I hope you find this notebook useful. If you have any questions, please don't hesitate to contact me.\n\nAlso note that this is only the license plate detection part, we can also use this model for the characters segmentation , followed by an OCR. (you can check [this paper](https:\/\/www.researchgate.net\/publication\/353034031_An_Improved_Character_Recognition_Framework_for_Containers_Based_on_DETR_Algorithm) for more info)\n\n![](https:\/\/www.mdpi.com\/sensors\/sensors-21-04612\/article_deploy\/html\/images\/sensors-21-04612-g001.png)\n\nUnfortunately I'm quite busy lately, but I'll try to post the rest of the notebooks soon. ","a90dcd1f":"Here we define the model, and verify the outputs.","2ca32de5":"We could start Tensorboard if we'd like to monitor the training, since Pytorch Lightning automatically logs to it.","e9c1e3e3":"Next, let's train! We train for a maximum of 300 training steps, and also use gradient clipping. You can refresh Tensorboard above to check the various losses.","425cd9b3":"Let's verify the keys of a single batch:"}}