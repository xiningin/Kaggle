{"cell_type":{"4b393eea":"code","bc922be1":"code","f73891f0":"code","5efdb038":"code","bcfe5d55":"code","99bf574d":"code","bbf6c084":"code","d8549790":"code","a62a0461":"code","e46b4c94":"code","2b8ef216":"code","1512abbd":"code","1ffb518d":"code","a868e7e9":"code","82ec7315":"code","174190eb":"code","a835133d":"code","a5e712d3":"code","e577e920":"code","19e9fd56":"code","84c127a7":"code","494323d2":"code","18f62551":"code","1036b926":"code","e49c2d37":"code","87e13d9f":"code","02508d27":"code","49070268":"code","eb94c526":"code","64875798":"code","95487097":"code","e44088a3":"code","2ce41acc":"code","9e6d0bba":"markdown","afa0c0c2":"markdown","a805188a":"markdown","82209058":"markdown","21c49ca9":"markdown","14bbb7b5":"markdown","95290798":"markdown","80b5c3ab":"markdown","eed48f53":"markdown","18059a5c":"markdown","48938ef7":"markdown","9dd92f82":"markdown","e78cb6d3":"markdown","030b0a68":"markdown","cef2dbc4":"markdown","ec45816d":"markdown","a69ca650":"markdown","4711a461":"markdown","f81b6ede":"markdown","6cba28b9":"markdown","0d1948b1":"markdown","960e1204":"markdown","00d0153b":"markdown","d2bf95ce":"markdown","977811c3":"markdown","caadb589":"markdown","d6a6f8a8":"markdown","6194733b":"markdown","718ce68b":"markdown","f46ad1f5":"markdown","99ee8855":"markdown","5f494fcc":"markdown","7b33ecf0":"markdown","91e4393b":"markdown","eb0df66e":"markdown","71b55b9a":"markdown","0aaf930c":"markdown","9c61550d":"markdown","7cb71450":"markdown","1412921e":"markdown","54ba7323":"markdown","644a133d":"markdown"},"source":{"4b393eea":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport cv2\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator # built in dataloader\nimport matplotlib.cm as cm\nfrom tensorflow import keras\nfrom tqdm import tqdm\nimport os\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.metrics import AUC, Precision, Recall # metrics\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TensorBoard, ModelCheckpoint # callbacks\nfrom sklearn.metrics import classification_report,confusion_matrix\nimport ipywidgets as widgets\nimport io\nfrom PIL import Image\nfrom IPython.display import display,clear_output\nfrom warnings import filterwarnings\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","bc922be1":"colors_dark = [\"#1F1F1F\", \"#313131\", '#636363', '#AEAEAE', '#DADADA']\ncolors_red = [\"#331313\", \"#582626\", '#9E1717', '#D35151', '#E9B4B4']\ncolors_green = ['#01411C','#4B6F44','#4F7942','#74C365','#D0F0C0']\ncolors_blue = ['#02066F', '#030aa7']\ncolors_pink = ['#751973', '#a442a0']\n\n# seaborn useful for statistical graphs, is based on matplotlib\n\nsns.palplot(colors_dark)\nsns.palplot(colors_green)\nsns.palplot(colors_red)\nsns.palplot(colors_blue)\nsns.palplot(colors_pink)","f73891f0":"labels = ['glioma_tumor','no_tumor','meningioma_tumor','pituitary_tumor']","5efdb038":"X_train = []\ny_train = []\nimage_size = 150\ntraining_dim = np.zeros(shape=4)\ntesting_dim = np.zeros(shape=4)\nlabel_ty=-1;\nfor i in labels:\n    label_ty=label_ty+1\n    k=1\n    # data already split in training and testing in the dataset, here we take only training data\n    folderPath = os.path.join('..\/input\/brain-tumor-classification-mri','Training',i)\n    # tqdm shows a progress meter in loops\n    for j in tqdm(os.listdir(folderPath)): # name of the entries in the directory given by the path\n        training_dim[label_ty]=k\n        k=k+1\n        img = cv2.imread(os.path.join(folderPath,j)) # read the image, if there isn't an image return an empty matrix\n        # all images resized between 150 and 150\n        img = cv2.resize(img,(image_size, image_size))\n        X_train.append(img) # training data\n        y_train.append(i) # training labels\n\nlabel_ty=-1\n\nfor i in labels:\n    label_ty=label_ty+1\n    k=1\n    folderPath = os.path.join('..\/input\/brain-tumor-classification-mri','Testing',i)\n    for j in tqdm(os.listdir(folderPath)):\n        testing_dim[label_ty]=k\n        k=k+1\n        img = cv2.imread(os.path.join(folderPath,j)) \n        img = cv2.resize(img,(image_size,image_size))\n        X_train.append(img)\n        y_train.append(i)\n        \nprint('Number of images coming from the \"training\" folder for each label:')\nprint(training_dim)\nprint('Number of images coming from the \"testing\" folder for each label:')\nprint(testing_dim)","bcfe5d55":"X_train = np.array(X_train)\ny_train = np.array(y_train)","99bf574d":"print('Images dimensions =')\nprint(X_train.shape)\nprint('The first number is total number of images, the others are referring to the dimension')\nprint('=================')\nprint('The number of labels are')\nprint(y_train.shape)\nprint(\"We've verified the uniformity.\")","bbf6c084":"k=0\nfig, ax = plt.subplots(1,4,figsize=(20,20)) \nfig.text(s='Sample Image From Each Label',size=18,fontweight='bold',\n             fontname='monospace',color=colors_dark[1],y=0.62,x=0.4,alpha=0.8)\nfor i in labels:\n    j=0\n    count=0\n    while True :\n        if y_train[j]==i:\n            count+=1\n            if count > 1:\n                ax[k].imshow(X_train[j])\n                ax[k].set_title(y_train[j])\n                ax[k].axis('off')\n                k+=1 \n                break\n        j+=1","d8549790":"X_train, y_train = shuffle(X_train,y_train, random_state=101)\nX_train,X_test,y_train,y_test = train_test_split(X_train,y_train, test_size=0.1,random_state=101)","a62a0461":"y_train_new = []\nfor i in y_train: # labels\n    y_train_new.append(labels.index(i))\ny_train = y_train_new\ny_train = tf.keras.utils.to_categorical(y_train)\n\n\ny_test_new = []\nfor i in y_test:\n    y_test_new.append(labels.index(i))\ny_test = y_test_new\ny_test = tf.keras.utils.to_categorical(y_test)\n","e46b4c94":"effnet = EfficientNetB0(weights='imagenet',include_top=False,input_shape=(image_size,image_size,3))","2b8ef216":"model = effnet.output\nmodel = tf.keras.layers.GlobalAveragePooling2D()(model)\nmodel = tf.keras.layers.Dropout(rate=0.5)(model)\nmodel = tf.keras.layers.Dense(4,activation='softmax')(model)\nmodel = tf.keras.models.Model(inputs=effnet.input, outputs = model)","1512abbd":"model.summary()","1ffb518d":"print('The total number of layers is:')\nprint(len(model.layers))","a868e7e9":"metrics_names = [\"accuracy\", \"precision\", \"recall\"]\nmetrics = [\"accuracy\", Precision(name=\"precision\"), Recall(name=\"recall\")]","82ec7315":"model.compile(loss='categorical_crossentropy',optimizer = 'Adam',metrics=metrics)","174190eb":"tensorboard = TensorBoard(log_dir = 'logs')\ncheckpoint = ModelCheckpoint(\"effnet.h5\",monitor=\"val_accuracy\",save_best_only=True,mode=\"auto\",verbose=1)\nreduce_lr = ReduceLROnPlateau(monitor = 'val_accuracy', factor = 0.3, patience = 2, min_delta = 0.001,\n                              mode='auto',verbose=1)","a835133d":"VALIDATION_ACCURACY1 = []\nVALIDATION_LOSS1 = []\n\n\nVALIDATION_ACCURACY2 = []\nVALIDATION_LOSS2 = []\n\nHISTORIES1ACC =[]\nHISTORIES2ACC = []\nHISTORIES1VALACC =[]\nHISTORIES2VALACC = []\n\nHISTORIES1LOSS =[]\nHISTORIES2LOSS = []\nHISTORIES1VALLOSS =[]\nHISTORIES2VALLOSS = []\n\nHISTORIES1PRECISION =[]\nHISTORIES2PRECISION =[]\n\nHISTORIES1RECALL = []\nHISTORIES2RECALL = []\n\nHISTORIES2VALPRECISION =[]\nHISTORIES1VALPRECISION =[]\n\nHISTORIES1VALRECALL = []\nHISTORIES2VALRECALL = []","a5e712d3":"fold=1\nfrom sklearn.model_selection import KFold\nfrom tensorflow.keras.models import save_model, load_model\nskf = KFold(n_splits=5)\n\nfor train_index, test_index in skf.split(X_train, y_train):\n    print('------------------------------------------------------------------------')\n    print(f'Training for fold {fold} ...')\n    Xt, Xv = X_train[train_index], X_train[test_index]\n    yt, yv = y_train[train_index], y_train[test_index]\n    \n    #CREATING, FITTING AND EVALUATING MODEL1\n    effnet1 = EfficientNetB0(weights='imagenet',include_top=False,input_shape=(image_size,image_size,3))\n\n    model1 = effnet1.output\n    model1 = tf.keras.layers.GlobalAveragePooling2D()(model1)\n    model1 = tf.keras.layers.Dropout(rate=0.5)(model1)\n    model1 = tf.keras.layers.Dense(4,activation='softmax')(model1)\n    model1 = tf.keras.models.Model(inputs=effnet1.input, outputs = model1)\n    \n    model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=metrics)\n    history1=model1.fit(Xt, yt, validation_data=(Xv, yv), epochs=12, batch_size=32, verbose=2,callbacks=reduce_lr)\n    results1 = model1.evaluate(Xv, yv, verbose=0)\n    results1 = dict(zip(model1.metrics_names,results1))\n\n    VALIDATION_ACCURACY1.append(results1['accuracy'])\n    VALIDATION_LOSS1.append(results1['loss'])\n    \n\n    fold+=1\n   \n    # SAVING ALL HISTORIES\n    HISTORIES1ACC.append(history1.history['accuracy'])\n    HISTORIES1LOSS.append(history1.history['loss'])\n    HISTORIES1VALACC.append(history1.history['val_accuracy'])\n    HISTORIES1VALLOSS.append(history1.history['val_loss'])\n    HISTORIES1PRECISION.append(history1.history['precision'])\n    HISTORIES1RECALL.append(history1.history['recall'])\n    HISTORIES1VALPRECISION.append(history1.history['val_precision'])\n    HISTORIES1VALRECALL.append(history1.history['val_recall'])","e577e920":"fold=1\nfor train_index, test_index in skf.split(X_train, y_train):\n    print('------------------------------------------------------------------------')\n    print(f'Training for fold {fold} ...')\n    Xt, Xv = X_train[train_index], X_train[test_index]\n    yt, yv = y_train[train_index], y_train[test_index]\n    \n    #CREATING, FITTING AND EVALUATING MODEL2\n    effnet2 = EfficientNetB0(weights='imagenet',include_top=False,input_shape=(image_size,image_size,3))\n    \n    model2 = effnet2.output\n    model2 = tf.keras.layers.GlobalAveragePooling2D()(model2)\n    model2 = tf.keras.layers.Dropout(rate=0.5)(model2)\n    model2 = tf.keras.layers.Dense(4,activation='softmax')(model2)\n    model2 = tf.keras.models.Model(inputs=effnet2.input, outputs = model2)\n    \n    model2.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=metrics)\n    history2=model2.fit(Xt, yt, validation_data=(Xv, yv), epochs=12, batch_size=32, verbose=2,callbacks=reduce_lr)\n    results2 = model2.evaluate(Xv, yv, verbose=0)\n    results2 = dict(zip(model2.metrics_names,results2))\n    \n    VALIDATION_ACCURACY2.append(results2['accuracy'])\n    VALIDATION_LOSS2.append(results2['loss'])\n    \n    fold+=1\n   \n    # SAVING ALL HISTORIES\n    \n    HISTORIES2ACC.append(history2.history['accuracy'])\n    HISTORIES2LOSS.append(history2.history['loss'])\n    HISTORIES2VALACC.append(history2.history['val_accuracy'])\n    HISTORIES2VALLOSS.append(history2.history['val_loss'])\n    HISTORIES2PRECISION.append(history2.history['precision'])\n    HISTORIES2RECALL.append(history2.history['recall']) \n    HISTORIES2VALPRECISION.append(history2.history['val_precision'])\n    HISTORIES2VALRECALL.append(history2.history['val_recall'])\n    ","19e9fd56":"# Medium histories of all folds trained \nhist1_acc = np.array(HISTORIES1ACC)\nhist2_acc = np.array(HISTORIES2ACC)\nhist2_loss = np.array(HISTORIES2LOSS)\nhist1_loss = np.array(HISTORIES1LOSS)\nhist1_valacc = np.array(HISTORIES1VALACC)\nhist2_valacc = np.array(HISTORIES2VALACC)\nhist2_valloss = np.array(HISTORIES2VALLOSS)\nhist1_valloss = np.array(HISTORIES1VALLOSS)\n\n\nhist1_prec = np.array(HISTORIES1PRECISION)\nhist2_prec = np.array(HISTORIES2PRECISION)\nhist2_rec = np.array(HISTORIES2RECALL)\nhist1_rec= np.array(HISTORIES1RECALL)\nhist1_valprec = np.array(HISTORIES1VALPRECISION)\nhist2_valprec = np.array(HISTORIES2VALPRECISION)\nhist2_valrec = np.array(HISTORIES2VALRECALL)\nhist1_valrec = np.array(HISTORIES1VALRECALL)\n\n\nn_epochs=12\nmean1_acc = np.zeros(n_epochs)\nmean2_acc =  np.zeros(n_epochs)\nmean1_loss = np.zeros(n_epochs)\nmean2_loss = np.zeros(n_epochs)\nmean2_valloss = np.zeros(n_epochs)\nmean1_valloss = np.zeros(n_epochs)\nmean2_valacc = np.zeros(n_epochs)\nmean1_valacc = np.zeros(n_epochs)\n\nmean1_prec = np.zeros(n_epochs)\nmean2_prec =  np.zeros(n_epochs)\nmean1_rec = np.zeros(n_epochs)\nmean2_rec = np.zeros(n_epochs)\nmean2_valprec = np.zeros(n_epochs)\nmean1_valprec = np.zeros(n_epochs)\nmean2_valrec = np.zeros(n_epochs)\nmean1_valrec = np.zeros(n_epochs)\n\n\n\nfor i in range(0,n_epochs):\n    for j in range(0, 5):\n        mean1_acc[i]=mean1_acc[i]+hist1_acc[j,i]\n        mean2_acc[i]=mean2_acc[i]+hist2_acc[j,i]\n        mean2_loss[i]=mean2_loss[i]+hist2_loss[j,i]\n        mean1_loss[i]=mean1_loss[i]+hist1_loss[j,i]\n        mean2_valloss[i]=mean2_valloss[i]+hist2_valloss[j,i]\n        mean1_valloss[i]=mean1_valloss[i]+hist1_valloss[j,i]\n        mean2_valacc[i]=mean2_valacc[i]+hist2_valacc[j,i]\n        mean1_valacc[i]=mean1_valacc[i]+hist1_valacc[j,i]\n        \n        mean1_prec[i]=mean1_prec[i]+hist1_prec[j,i]\n        mean2_prec[i]=mean2_prec[i]+hist2_prec[j,i]\n        mean2_rec[i]=mean2_rec[i]+hist2_rec[j,i]\n        mean1_rec[i]=mean1_rec[i]+hist1_rec[j,i]\n        mean2_valprec[i]=mean2_valprec[i]+hist2_valprec[j,i]\n        mean1_valprec[i]=mean1_valprec[i]+hist1_valprec[j,i]\n        mean2_valrec[i]=mean2_valrec[i]+hist2_valrec[j,i]\n        mean1_valrec[i]=mean1_valrec[i]+hist1_valrec[j,i]\n        \n    mean1_acc[i]=mean1_acc[i]\/5\n    mean2_acc[i]=mean2_acc[i]\/5\n    mean1_loss[i]=mean1_loss[i]\/5\n    mean2_loss[i]=mean2_loss[i]\/5\n    mean2_valloss[i]=mean2_valloss[i]\/5\n    mean1_valloss[i]=mean1_valloss[i]\/5\n    mean2_valacc[i]=mean2_valacc[i]\/5\n    mean1_valacc[i]=mean1_valacc[i]\/5\n    mean1_prec[i]=mean1_prec[i]\/5\n    mean2_prec[i]=mean2_prec[i]\/5\n    mean2_rec[i]=mean2_rec[i]\/5\n    mean1_rec[i]=mean1_rec[i]\/5\n    mean2_valprec[i]=mean2_valprec[i]\/5\n    mean1_valprec[i]=mean1_valprec[i]\/5\n    mean2_valrec[i]=mean2_valrec[i]\/5\n    mean1_valrec[i]=mean1_valrec[i]\/5\n\n","84c127a7":"print(\"The mean training accuracy of the network with the Adam optimizer is:\")\nprint(mean1_acc[11])\nprint(\"---------------------\")\nprint(\"The mean training accuracy of the network with the Sgd optimizer is:\")\nprint(mean2_acc[11])\n\nVal_acc1 = np.array(VALIDATION_ACCURACY1)\nmean_acc1 = np.mean(Val_acc1)\n\n\nVal_acc2 = np.array(VALIDATION_ACCURACY2)\nmean_acc2 = np.mean(Val_acc2)\n\nprint(\"=========================\")\nprint(\"The mean validation accuracy of the network with the Adam optimizer is:\")\nprint(mean_acc1)\nprint(\"---------------------\")\nprint(\"The mean validation accuracy of the network with the Sgd optimizer is:\")\nprint(mean_acc2)\n\n\nprint(\"=========================\")\nprint(\"The mean training loss of the network with the Adam optimizer is:\")\nprint(mean1_loss[11])\nprint(\"---------------------\")\nprint(\"The mean training loss of the network with the Sgd optimizer is:\")\nprint(mean2_loss[11])\n\n\nVal_loss1 = np.array(VALIDATION_LOSS1)\nmean_loss1 = np.mean(Val_loss1)\n\nVal_loss2 = np.array(VALIDATION_LOSS2)\nmean_loss2 = np.mean(Val_loss2)\nprint(\"=========================\")\nprint(\"The mean validation loss of the network with the Adam optimizer is:\")\nprint(mean_loss1)\nprint(\"---------------------\")\nprint(\"The mean validation loss of the network with the Sgd optimizer is:\")\nprint(mean_loss2)\n\n\n","494323d2":"filterwarnings('ignore')\nepochs = [i for i in range(n_epochs)]\nfig1, ax = plt.subplots(1,2,figsize=(14,7))\nfig1.text(s='Epochs vs. Training and Validation Accuracy\/Loss',size=18,fontweight='bold',\n             fontname='monospace',color=colors_dark[1],y=1,x=0.28,alpha=0.8)\n\nsns.despine()\nax[0].plot(epochs, mean1_acc, marker='o',markerfacecolor=colors_green[2],color=colors_green[3], label = 'Training Accuracy Adam')\nax[0].plot(epochs, mean1_valacc, marker='o',markerfacecolor=colors_red[2],color=colors_red[3],label = 'Validation Accuracy Adam')\nax[0].plot(epochs, mean2_acc, marker='*', markerfacecolor=colors_blue[0],color=colors_blue[1],\n           label = 'Training Accuracy sgd')\nax[0].plot(epochs, mean2_valacc, marker='*', markerfacecolor=colors_pink[0],color=colors_pink[1],label = 'Validation Accuracy sgd')\nax[0].legend(frameon=False)\nax[0].set_xlabel('Epochs')\nax[0].set_ylabel('Training & Validation Accuracy')\n\nsns.despine()\nax[1].plot(epochs, mean1_loss, marker='o',markerfacecolor=colors_green[2],color=colors_green[3],\n           label ='Training Loss Adam')\nax[1].plot(epochs, mean1_valloss, marker='o',markerfacecolor=colors_red[2],color=colors_red[3],label = 'Validation Loss Adam')\nax[1].plot(epochs, mean2_loss, marker='*',markerfacecolor=colors_blue[0],color=colors_blue[1],\n           label ='Training Loss sgd')\nax[1].plot(epochs, mean2_valloss, marker='*',markerfacecolor=colors_pink[0],color=colors_pink[1], label = 'Validation Loss sgd')\nax[1].legend(frameon=False)\nax[1].set_xlabel('Epochs')\nax[1].set_ylabel('Training & Validation Loss')\n\nfig1.show()\n\n\nfig2, ax = plt.subplots(1,2,figsize=(14,7))\nfig2.text(s='Epochs vs. Training and Validation Precision\/Recall',size=18,fontweight='bold',\n             fontname='monospace',color=colors_dark[1],y=1,x=0.28,alpha=0.8)\n\nsns.despine()\nax[0].plot(epochs, mean1_prec, marker='o',markerfacecolor=colors_green[2],color=colors_green[3],\n           label = 'Training Precision Adam')\nax[0].plot(epochs, mean1_valprec, marker='o',markerfacecolor=colors_red[2],color=colors_red[3],\n           label = 'Validation Precision Adam')\nax[0].plot(epochs, mean2_prec, marker='*',markerfacecolor=colors_blue[0],color=colors_blue[1],\n           label = 'Training Precision sgd')\nax[0].plot(epochs, mean2_valprec, marker='*',markerfacecolor=colors_pink[0],color=colors_pink[1],\n           label = 'Validation Precision sgd')\nax[0].legend(frameon=False)\nax[0].set_xlabel('Epochs')\nax[0].set_ylabel('Training & Validation Precision')\n\nsns.despine()\nax[1].plot(epochs, mean1_rec, marker='o',markerfacecolor=colors_green[2],color=colors_green[3],\n           label ='Training Recall Adam')\nax[1].plot(epochs, mean1_valrec, marker='o',markerfacecolor=colors_red[2],color=colors_red[3],\n           label = 'Validation Recall Adam')\nax[1].plot(epochs, mean2_rec, marker='*',markerfacecolor=colors_blue[0],color=colors_blue[1],\n           label ='Training Recall sgd')\nax[1].plot(epochs, mean2_valrec, marker='*',markerfacecolor=colors_pink[0],color=colors_pink[1],\n           label = 'Validation Recall sgd')\nax[1].legend(frameon=False)\nax[1].set_xlabel('Epochs')\nax[1].set_ylabel('Training & Validation Recall')\n\nfig2.show()","18f62551":"# retraining of the network, with just 10% of the training set as validation set.\nhistory = model.fit(X_train, y_train, validation_split=0.1, epochs=12, batch_size=32, verbose=2,callbacks=[tensorboard,reduce_lr, checkpoint])","1036b926":"pred = model.predict(X_test)\npred = np.argmax(pred,axis=1)\ny_test_new = np.argmax(y_test,axis=1)","e49c2d37":"print(classification_report(y_test_new,pred))","87e13d9f":"fig,ax = plt.subplots(1,1,figsize=(14,7))\nsns.heatmap(confusion_matrix(y_test_new,pred,normalize=\"true\"),\n            ax=ax,xticklabels=labels,yticklabels=labels,annot=True,\n           cmap=colors_green[::-1],alpha=0.7,linewidths=2,linecolor=colors_dark[3])\nfig.text(s='Heatmap of the Confusion Matrix',size=18,fontweight='bold',\n             fontname='monospace',color=colors_dark[1],y=0.92,x=0.28,alpha=0.8)\n\nplt.show()","02508d27":"# Select the output of the last convolutional layer, it will be used to compute the gradient\nlast_conv_layer_name = \"top_activation\" \n\n# Select a casual image from the training set\nimg_array = X_train[117]\nplt.imshow(img_array)","49070268":"def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    # We add a dimension to transform our array into a \"batch\"\n    # of size (1, 299, 299, 3)\n    img_array = np.expand_dims(img_array, axis=0)\n    # First, we create a model that maps the input image to the activations\n    # of the last conv layer as well as the output predictions\n    grad_model = tf.keras.models.Model(\n        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n    )\n\n    # Then, we compute the gradient of the top predicted class for our input image\n    # with respect to the activations of the last conv layer\n    with tf.GradientTape() as tape:\n        last_conv_layer_output, preds = grad_model(img_array)\n        if pred_index is None:\n            pred_index = tf.argmax(preds[0])\n        class_channel = preds[:, pred_index]\n\n    # This is the gradient of the output neuron (top predicted or chosen)\n    # with regard to the output feature map of the last conv layer\n    grads = tape.gradient(class_channel, last_conv_layer_output)\n\n    # This is a vector where each entry is the mean intensity of the gradient\n    # over a specific feature map channel\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    # We multiply each channel in the feature map array\n    # by \"how important this channel is\" with regard to the top predicted class\n    # then sum all the channels to obtain the heatmap class activation\n    last_conv_layer_output = last_conv_layer_output[0]\n    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n\n    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n    heatmap = tf.maximum(heatmap, 0) \/ tf.math.reduce_max(heatmap)\n    return heatmap.numpy()","eb94c526":"heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n\nplt.matshow(heatmap)\nplt.show()","64875798":"\ndef get_gradcam(img, heatmap, alpha=0.99):\n    \n    \n    # Rescale heatmap to a range 0-255\n    heatmap = np.uint8(255 * heatmap)\n\n    # Use jet colormap to colorize heatmap\n    jet = cm.get_cmap(\"jet\")\n\n    # Use RGB values of the colormap\n    jet_colors = jet(np.arange(256))[:, :3]\n    jet_heatmap = jet_colors[heatmap]\n\n    # Create an image with RGB colorized heatmap\n    jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n\n    # Superimpose the heatmap on original image\n    superimposed_img = jet_heatmap * alpha + img\n    return superimposed_img","95487097":"superimposed_img = get_gradcam(img_array, heatmap)\nsuperimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n\nfig, axs = plt.subplots(1,2,figsize=(14,7))\n   \nsns.despine()\naxs[0].imshow(img_array)\naxs[0].set_title('MR scan',size=18,fontweight='bold',\n             fontname='monospace',color=colors_dark[1])\n\nsns.despine()\naxs[1].imshow(superimposed_img)\naxs[1].set_title('Grad-CAM',size=18,fontweight='bold',\n             fontname='monospace',color=colors_dark[1])","e44088a3":"indices = [i for i in range(len(y_test_new)) if y_test_new[i] != pred[i]]\nwrong_pred = X_test[indices,:,:,:]\n\n# plot of wrong predictions\nn = wrong_pred.shape[0]\nncol = 4 # chosen by me\nif n%ncol == 0:\n    nrow = n\/ncol\nelse:\n    nrow = round(n\/ncol)+1 \n\nfig = plt.figure(figsize=(15,15))\nax= []\nfor i in range(n):\n    ax.append(fig.add_subplot(nrow, ncol, i+1) )\n    ax[i].set_title('Pred={pred}, True={true}'.format(pred=labels[pred[indices[i]]], true=labels[y_test_new[indices[i]]]))\n    ax[i].axis('off')\n    plt.imshow(wrong_pred[i])\nfig.tight_layout(pad=1.0)","2ce41acc":"indices = [i for i in range(len(y_test_new)) if y_test_new[i] != pred[i]]\nwrong_pred = X_test[indices,:,:,:]\n\nfig = plt.figure(figsize=(15,15))\nax= []\nfor i in range(n):\n    heatmap = make_gradcam_heatmap(wrong_pred[i], model, last_conv_layer_name)\n    superimposed_img = get_gradcam(wrong_pred[i], heatmap)\n    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n    # now that we have obtained the GradCam image, plot it\n    ax.append(fig.add_subplot(nrow, ncol, i+1) )\n    ax[i].set_title('Pred={pred}, True={true}'.format(pred=labels[pred[indices[i]]], true=labels[y_test_new[indices[i]]]))\n    ax[i].axis('off')\n    plt.imshow(superimposed_img)\nfig.tight_layout(pad=1.0)","9e6d0bba":"# GRADCAM\nOne of the problem of CNNs and in general of deep learning in the lack of intuitiveness and understandability due to the complexity of the model. In order to make our network more transparent we tried to understand why the model predict what it predict, and so understand what it is looking to classify images. To answer this question we applied a Gradient-weighted Class Activation Mapping (Grad-CAM), that it is a technique to producing visual explanations for decisions from a large class of CNN-based models, making them more transparent. To understand how a grad-CAM algorithm works we are based on: Ramprasaath R. Selvaraju et al.,Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization,2019\n\nThe following code is based on: https:\/\/keras.io\/examples\/vision\/grad_cam\/","afa0c0c2":"It is also interesting to see where the network is committing mistakes.","a805188a":"The layers that have been added were:\n\n**GlobalAveragePooling2D**: This layer acts similar to the Max Pooling layer in CNNs, the only difference being is that it uses the Average values instead of the Max value while *pooling*. This really helps in decreasing the computational load on the machine while training.\n\n**Dropout**: This layer omits some of the neurons at each step from the layer making the neurons more independent from the neibouring neurons. It helps in avoiding overfitting. Neurons to be ommitted are selected at random. The **rate** parameter is the liklihood of a neuron activation being set to 0, thus dropping out the neuron\n\n**Dense**: This is the output layer which classifies the image into 1 of the 4 possible classes. It uses the **softmax** function which is a generalization of the sigmoid function and has as output the probabilities of belonging to the four different classes.","82209058":"---","21c49ca9":"Apply Grad-CAM algorithm to the wrong classified images.","14bbb7b5":"---","95290798":"# Abstract","80b5c3ab":"Plot the imput image and the result of the Grad-CAM.","eed48f53":"---","18059a5c":"Because of the general complexity necessary to train a CNN, now transfer learning will be performed in order to make it easier and cheaper for our devices. In particular, the already existing network 'EfficientNetB0', which uses weights from ImageNet dataset, now is trained, adding final layers, depending on our interest.  ","48938ef7":"In this paragraph, the focus is to prepare data. First of all, labels are created, then images are uploaded into an unique array.","9dd92f82":"After have created the network, the moment of training has arrived. In order to speed up the process, the original notebook trained the model and visualized the behaviour in terms of accuracy, loss and others without doing a cross validation procedure. We've decided to include it, doing the 5-fold cross validation. This takes a lot of time, but is useful in order to see how well the network is trained, independently on the images that are going inside as training and so having a mean idea of the overall process.\nFurthermore, we've decided to see the performance of the SGD optimizer: we expect less performances.\n\nIn order to perform the 5-fold CV correctly, we've done two FOR loop, one for each optimizer. Inside at each loop, the training and validation sets are created by using 'Kfold' from tensorflow: each time 4 folds are used as training set, and the performance of the model are evaluated on the remaining fold. In order to not oversubscribe model, each run defines and creates a new and independent model (EfficientNetB0).\n\nThe performances of the model are then visualized by memorizing the histories (accuracy, loss, precision and recall) of each training loop which are averaged over the folds training. Because precision and recall are defined for a binary-classification task, it is important to specify that in this case their values are obtained by performing (authomatically) an average between the one-vs all cases.","e78cb6d3":"# Brain Tumor MRI Classification - FIRST PART \n\nDeep Learning applied to Neuroscience and Rehabilitation\n\nA.A. 2021\/2022\n\nFranceschin Sarah, Grisi Caterina, Pozza Giacomo, Tonello Alessio, Viberti Andrea\n\nTaken from https:\/\/www.kaggle.com\/jaykumar1607\/brain-tumor-mri-classification-tensorflow-cnn","030b0a68":"---","cef2dbc4":"Hereafter, the metrics regarding the predictions on the test set are displayed.","ec45816d":"# Data Preperation","a69ca650":"# Considerations\n\n# - SGD vs ADAM\nHow we expected, the adam optimizer performs better then the sgd, even if both of them seems to classify well, without doing overfitting (high validation accuracy).\n\n# - CV\nThe cross validation technique actually confirmed the high performances originally obtained: high accuracy values, low loss values, high precision values and high recall values (both on training and validation set).","4711a461":"# Transfer Learning","f81b6ede":"The following function create an heatmap based on the result of Gradient of the predicted class respect the last conv. layer.","6cba28b9":"# Training The Model","0d1948b1":"The visualization of the images may be useful in order to understand better what we are using and what we want to predict.","960e1204":"The original model used **Callbacks**, which can help you fix bugs more quickly, and can help you build better models. They can help you visualize how your model\u2019s training is going, and can even help prevent overfitting by implementing early stopping or customizing the learning rate on each iteration.<br><br>\nBy definition, \"A callback is a set of functions to be applied at given stages of the training procedure. You can use callbacks to get a view on internal states and statistics of the model during training.\"\n\nIn this notebook the following callbacks functions were used:\n**TensorBoard**: it is a visualization tool provided with TensorFlow. Here it has been used to define the path of the directory where to save the log files to be parsed by TensorBoard.\n\n**ModelCheckPoint**: it saves the model (or just model weights) with a specific frequency. In this case it has been used to save the best model (in terms of the validation accuracy).\n\n**ReduceLROnPlateau**: it reduces the *learning rate* with some criteria. Here it has been set that the learning rate must decrease when the val accuracy don't improve (during the learning) for two consequent times, by a factor of 0.3.","00d0153b":"In this first notebook, the original project is analyzed. In order to make a more complete analysis, we've decided to perform the 5-fold cross validation technique, to monitor the precision and recall metrics during the training procedure too and to evaluate another type of optimizer (Sgd) than the original one (Adam).","d2bf95ce":"Finally, the dataset is shuffled in order to create the training set and the test set, such that it is the 10% of the original dataset.","977811c3":"Here, we can understand how much images there are and verify that both labels and images are of the same quantity.","caadb589":"# Evaluation","d6a6f8a8":"---","6194733b":"Hereafter, **One Hot Encoding** is performed on the labels.","718ce68b":"---","f46ad1f5":"Apply the previous function to the imput image.","99ee8855":"---","5f494fcc":"# Color","7b33ecf0":"---","91e4393b":"This function superimpose the heatmap to the imput image.","eb0df66e":"In this notebook our group deals with the classification of brain tumors:  abnormal masses of tissue, the growth of which exceeds and is uncoordinated with that of the normal tissues. \nDuring the last decades the medical field is facing a real IT revolution, big data is undermining our sanitary system; in addition to this, facing Covid19 pandemic, hospitals are running out of health care workers. \nIn order to deal with these actual problems, Machine Learning algorithms have been developed to assure: enhancement of decision-making and operational skills, reduction of errors and saving of resources. Specifically, a Deep Learning approach is the most powerful tool. \nA lot of time has been spent to develop a precise and robust solution for the automatic classification of brain tumors. Nevertheless, due to high inter and intra shape, and contrast variations, it remains a challenging problem. To go over these issues, we applied a CNN to obtain a model whose trained weights make us possible to predict if there is a brain tumor or not. In case of positive result, we discriminate between three different types of tumor: glioma, meningioma and pituitary.\nOur original dataset is composed of MRI images and by now we can see there is no balance between class dimensions. \nAt this point, it has been adopted a transfer learning approach: taking into account an already made up neural network and changing the very last layers with the aim of exploit the already set net to achieve our task.                                                                                                                                                                                                                                                                                                                                                 \nThe original network for this purpose is EfficientNetB0, which gives us especially high performance. \nFor the purpose of assessing the good results coming from this method, our group adopted various  ideas. First of all, we tried to solve our dataset\u2019s issues: class imbalance as stated before; by visual inspection, the no tumor class is characterized by an high percentage of axial images. \nBesides, always concerning about the dataset (as usual in the biomedical domaine), it could be considered too poor, so we performed a data augmentation. \nIn addition, we also exploit the Grad-CAM method, this for the sake of showing: why the model mis-classify some images; where the model focuses to accomplish the task.\nThereupon, our group tried to enhance the original performance by applying the transfer learning approach using different pre trained networks, such as EfficientNetB2 and VGG16. In the first case, no significant improvements can be noticed, despite the increased number of trainable parameters, which leads us to avoid this strategy. In the second case, performances of the model can not achieve those obtained originally through EfficientNetB0, which can be justify taking into account the number of trainable parameters.\nFinally, as a means to provide a more accurate estimate of performance than a single run, performance is averaged over the 5 iterations typical of 5-fold cross validation, which confirmed the high performances originally view in the training procedure.\nAs a bonus widget, our group tried to re-adapt the original model with the aim to classify the proposed images in two different classes:  Tumor and No Tumor. Also in this case, the model shows very good performances. \n\nWith our investigations, we've verified that the performances of the network are not affected by the unbalance of the training set and that EfficientNetB0 could be considered an optimal network because realizes a trade-off between the complexity of the model and its accuracy. Moreover, by the implementation of the Grad-CAM algorithm, we had the opportunity to verify that the network succeed into focusing on regions where the tumor is present: we think that future improvements could be the expansion of it and perform localization.\n","71b55b9a":"The only metric evaluated originally was the accuracy. We've decided to visualize the  precision and recall too.","0aaf930c":"# Importing Libraries","9c61550d":"---","7cb71450":"# Prediction","1412921e":"# Introduction","54ba7323":"The prediction of the model is so made by using the Adam optimizer. In order to ensure the highest capability of the neural network to generalize, the model used is obtained by training it with all the training set defined at the top of the notebook.\n\nTo associate at each test image its predicted class, the function argmax is used: that's because, the last layer of the network applies the softmax function, giving as result four probabilities (one for each class). With argmax the predicted class is selected as the class which has the highest probability (output) value.","644a133d":"Results show that the model has an high accuracy and that has a good performance in distinguishing between classes.  "}}