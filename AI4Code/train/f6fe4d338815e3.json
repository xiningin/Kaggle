{"cell_type":{"9eb0fccd":"code","1a501510":"code","c97f22e6":"code","79f59937":"code","c156a990":"code","e6fbd93f":"code","caf0b524":"code","9db5aa56":"code","0a07c2bc":"code","ee59f5e7":"code","2d843101":"code","00f751ca":"code","be32a38b":"code","1832f838":"code","922e4cf9":"code","94e62763":"code","66a614b8":"code","c589961e":"code","18b8fe3a":"code","ad266130":"code","75896fcf":"code","2b36540e":"code","2d6123f7":"code","731c785f":"code","f8b2cd12":"code","7ce38e0e":"code","a57e5048":"code","d0c0e719":"code","ff0fe656":"code","61eb1391":"code","a83fc019":"code","2a38f698":"code","c8855804":"code","bae93a52":"code","89ec76de":"code","7c251fd7":"code","f1f17f85":"code","3f83e887":"code","ae794090":"code","3dc75ce1":"code","9a8ace26":"code","ed96e9d7":"code","56a5ab42":"code","85f07ca1":"code","af2d7a9c":"code","5380c899":"code","5ec2d644":"code","43c3cfc3":"code","a509a5b5":"code","bc45777c":"code","7071fa24":"code","beea91b7":"code","89b9b8dc":"code","d5fe7e26":"code","6a7fb605":"code","20045490":"code","10fbc75f":"code","dc934450":"code","224b73a6":"code","1979c0dc":"code","d7d8553c":"code","130fbdd4":"code","b0de5d48":"code","9b0f3f6c":"code","dd7fc442":"code","fb04fb3d":"code","5ac1c869":"code","e7ae7130":"code","0097a0ba":"code","00e4eb10":"code","f7a29fc4":"code","80131f91":"code","b0c48fe6":"code","4048eb08":"code","2368f7bf":"code","ec974252":"code","66071ef5":"code","c50a04a8":"code","f07241b6":"code","8df4c100":"code","dad6051d":"code","a68fcb7e":"code","30ced7c2":"code","c568e2bf":"code","7b78127d":"code","f23e6868":"code","60b4d9e4":"code","c92cb79e":"code","26c67625":"code","a8d7ba9b":"markdown","fa0ee718":"markdown","2911b21b":"markdown","14302197":"markdown","0d03c267":"markdown","80de3be8":"markdown","89cd1592":"markdown"},"source":{"9eb0fccd":"import pandas as pd\n\nmain_file_path = '..\/input\/house-prices-advanced-regression-techniques\/train.csv' # this is the path to the Iowa data that you will use\ndata = pd.read_csv(main_file_path)\n\n# Run this code block with the control-enter keys on your keyboard. Or click the blue botton on the left\nprint(data.describe())  # print a summary of the data in house prices data","1a501510":"openporch_data=data.OpenPorchSF #stores the OPenPorchSF column data as the variable openporch_data\nprint(openporch_data.head()) # the head command returns the top few lines of the OpenPorchSF column\nprint(openporch_data)","c97f22e6":"print(data.columns) #prints out all the columns in the data","79f59937":"#selecting multiple columns from the house prices dataframe\ncolumns_of_interest = ['Alley', 'LandContour', 'Fence']\ncolumns_of_data = data[columns_of_interest]\nprint(columns_of_data)\ncolumns_of_data.describe()","c156a990":"from sklearn.tree import DecisionTreeRegressor # used to make predictions from certain data\n#predicting sales price based on the 7 numeric variables without missing values\ny=data.SalePrice\npredictors=['1stFlrSF','YearBuilt', 'LotArea', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\nx=data[predictors]\n\nmodel=DecisionTreeRegressor() #define model\nmodel.fit(x,y) #fit model\nprint(\"Making predictions for the following 5 houses:\")\nprint(x.head())\nprint(\"The predictions are\")\nprint(model.predict(x.head()))\n","e6fbd93f":"#calculates mean absolyte error\nfrom sklearn.metrics import mean_absolute_error\n\npredicted_home_prices = model.predict(x)\nmean_absolute_error(y, predicted_home_prices)","caf0b524":"from sklearn.model_selection import train_test_split\n\n# split data into training and validation data, for both predictors and target\n# The split is based on a random number generator. Supplying a numeric value to\n# the random_state argument guarantees we get the same split every time we\n# run this script.\ntrain_X, val_X, train_y, val_y = train_test_split(x, y,random_state = 0)\n# Define model\nhouse_model = DecisionTreeRegressor()\n# Fit model\nhouse_model.fit(train_X, train_y)\n\n# get predicted prices on validation data\nval_predictions = house_model.predict(val_X)\nprint(mean_absolute_error(val_y, val_predictions))\n\n","9db5aa56":"#prevents underfitting and overfitting\ndef get_mae(max_leaf_nodes, predictors_train, predictors_val, targ_train, targ_val):\n    new_model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    new_model.fit(predictors_train, targ_train)\n    preds_val = new_model.predict(predictors_val)\n    mae = mean_absolute_error(targ_val, preds_val)\n    return(mae)","0a07c2bc":"# compare MAE with differing values of max_leaf_nodes\nfor max_leaf_nodes in [5, 50, 500, 5000]:\n    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(\"Max leaf nodes: %d \\t\\t  Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))","ee59f5e7":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nforest_model1 = RandomForestRegressor()\nforest_model1.fit(train_X, train_y)\nhouse_preds = forest_model1.predict(val_X)\nprint(mean_absolute_error(val_y, house_preds))","2d843101":"main_file_path1 = '..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv' # this is the path to the Iowa data that you will use\ndata1 = pd.read_csv(main_file_path1)\nprint(data1.columns)\ny1=data1.SalePrice\npredictors=['SalePrice']\nx1=data1[predictors]\nmodel1=DecisionTreeRegressor() #define model\nmodel1.fit(x1,y1) #fit model\nprediction=model1.predict(x1)\nprint(prediction)\nsubmission3 = pd.DataFrame({'Id': data1.Id, 'SalePrice': prediction})\n\nsubmission3.to_csv('submission4.csv', index=False)","00f751ca":"#find and counts all the missing data in each column\nmissing_val_count_by_column = (data.isnull().sum())\nprint(missing_val_count_by_column[missing_val_count_by_column > 0]) ","be32a38b":"#data_without_missing_values = data.dropna(axis=1) # drops columns with missing data","1832f838":"#cols_with_missing = [col for col in data.columns \n                               #  if data[col].isnull().any()]\n#redued_original_data = data.drop(cols_with_missing, axis=1)\n","922e4cf9":"#predicts a value to fit with culmns that have missing data for numbers\nfrom sklearn.impute import SimpleImputer\nnew_data = data1.copy()\n\n# make new columns indicating what will be imputed\ncols_with_missing = (col for col in new_data.columns \n                                 if new_data[col].isnull().any())\nfor col in cols_with_missing:\n    new_data[col + '_was_missing'] = new_data[col].isnull()\n\n# Imputation\nmy_imputer = SimpleImputer()\nnew_data = pd.DataFrame(my_imputer.fit_transform(new_data))\nnew_data.columns = data1.columns","94e62763":"#used for predicting any columns with numbers that are null\n# make copy to avoid changing original data (when Imputing)\nnew_data = data1.copy()\n\n# make new columns indicating what will be imputed\ncols_with_missing = (col for col in new_data.columns \n                                 if new_data[col].isnull().any())\nfor col in cols_with_missing:\n    new_data[col + '_was_missing'] = new_data[col].isnull()\n\n# Imputation\nmy_imputer = SimpleImputer()\nnew_data = pd.DataFrame(my_imputer.fit_transform(new_data))\nnew_data.columns = data1.columns","66a614b8":"\nfrom sklearn.impute import SimpleImputer\n\nmy_imputer = SimpleImputer()\nimputed_X_train = my_imputer.fit_transform(train_X)\nimputed_X_test = my_imputer.transform(val_X)\nprint(\"Mean Absolute Error from Imputation:\")\nprint(get_mae(20,imputed_X_train, imputed_X_test, train_y, val_y))","c589961e":"data.dtypes.sample(10) #implies whether the column contains integer or string(object) columns for the first 10 columns","18b8fe3a":"one_hot_encoded_training_predictors = pd.get_dummies(data) \n#encodes object (string) columns so that the code can make predictions from non-numerical data by one-hot encoding them (converting them into a type of numerical data)","ad266130":"#applies one-hot encoding to multiple files\none_hot_encoded_training_predictors = pd.get_dummies(data)\none_hot_encoded_test_predictors = pd.get_dummies(data1)\nfinal_train, final_test = one_hot_encoded_training_predictors.align(one_hot_encoded_test_predictors,\n                                                                    join='left',   #joins data1 dataframe(sample_submission.csv) to the the left of data dataframe(train.csv)\n                                                                    axis=1)\n","75896fcf":"print(final_train)","2b36540e":"print(final_test) #NaN (too large or not exists) shown due to SaleCondition column not existing in the datafram sample_submission.csv","2d6123f7":"#XGBoost (gradient boost decision tree) implementation\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import Imputer\n\ndata4 = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndata4.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny4 = data4.SalePrice\nX4 = data4.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object'])\ntrain_X, test_X, train_y, test_y = train_test_split(X4.as_matrix(), y4.as_matrix(), test_size=0.25)\n\nmy_imputer = Imputer()\ntrain_X = my_imputer.fit_transform(train_X)\ntest_X = my_imputer.transform(test_X)","731c785f":"#gradient boost decision tree (using a model)\nfrom xgboost import XGBRegressor\n\nmy_model = XGBRegressor(silent=True)\n# Adding silent=True to avoid printing out updates with each cycle\nmy_model.fit(train_X, train_y, verbose=False)","f8b2cd12":"# make predictions from gradient boost decision tree\npredictions = my_model.predict(test_X)\n\nfrom sklearn.metrics import mean_absolute_error\nprint(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y)))","7ce38e0e":"\nmy_model = XGBRegressor(n_estimators=1000) #n_estimators specifies how many times to go through the modeling cycle \n#In the underfitting vs overfitting graph, n_estimators moves you further to the right. \n#Too low a value causes underfitting, which is inaccurate predictions on both training data and new data. \n#Too large a value causes overfitting, which is accurate predictions on training data, \n#but inaccurate predictions on new data (which is what we care about). You can experiment with your dataset to find the ideal.\n#Typical values range from 100-1000, though this depends a lot on the learning rate.\n\nmy_model.fit(train_X, train_y, early_stopping_rounds=5,  #The argument early_stopping_rounds offers a way to automatically find the ideal value.\n             eval_set=[(test_X, test_y)], verbose=False)\n#Early stopping causes the model to stop iterating when the validation score stops improving, even if we aren't at the hard stop for n_estimators. \n#It's smart to set a high value for n_estimators and then use early_stopping_rounds to find the optimal time to stop iterating.\n#Since random chance sometimes causes a single round where validation scores don't improve,\n#you need to specify a number for how many rounds of straight deterioration to allow before stopping. \n#early_stopping_rounds = 5 is a reasonable value. Thus we stop after 5 straight rounds of deteriorating validation scores.\n","a57e5048":"#using learning_rate\nmy_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4) #multiplies the predictions from each model by the learning_rate(0.05)\n#n_jobs is the number of cores on the machine (computer)\nmy_model.fit(train_X, train_y, early_stopping_rounds=5, \n             eval_set=[(test_X, test_y)], verbose=False)\n#Here's a subtle but important trick for better XGBoost models:\n#Instead of getting predictions by simply adding up the predictions from each component model, \n# multiply the predictions from each model by a small number before adding them in. \n#This means each tree we add to the ensemble helps us less. In practice, this reduces the model's propensity to overfit.\n#So, you can use a higher value of n_estimators without overfitting. If you use early stopping, the appropriate number of trees will be set automatically.\n#In general, a small learning rate (and large number of estimators) will yield more accurate XGBoost models, \n#though it will also take the model longer to train since it does more iterations through the cycle.","d0c0e719":"submission6 = pd.DataFrame({'Id': final_test.Id,'SaleCondition Family':final_test.SaleCondition_Family, 'SalePrice': final_test.SalePrice})\n\nsubmission6.to_csv('submission6.csv', index=False)\n","ff0fe656":"print(final_test.columns)","61eb1391":"#function for y and X used in partial dependence plot \ndef get_some_data():\n    cols_to_use = ['GarageArea', 'LotArea', 'GrLivArea']\n    data5 = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\n    y5 = data5.SalePrice\n    X5 = data5[cols_to_use]\n    my_imputer2 = Imputer()\n    imputed_X5 = my_imputer2.fit_transform(X5)\n    return imputed_X5, y5\n","a83fc019":"from sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n# get_some_data is defined in hidden cell above.\nX5, y5 = get_some_data()\n# scikit-learn originally implemented partial dependence plots only for Gradient Boosting models\n# this was due to an implementation detail, and a future release will support all model types.\nmy_model3 = GradientBoostingRegressor()\n# fit the model as usual\nmy_model3.fit(X5, y5)\n# Here we make the plot\nmy_plotsGarArea = plot_partial_dependence(my_model3,       \n                                   features=[0], # column numbers of plots we want to show\n                                   X=X5,            # raw predictors data.\n                                   feature_names=['Garage Area','Lot Area', 'Green Live Area'], # labels on graphs\n                                   grid_resolution=10) # number of values to plot on x axis\nmy_plotsGarArea = plot_partial_dependence(my_model3,       \n                                   features=[1], # column numbers of plots we want to show\n                                   X=X5,            # raw predictors data.\n                                   feature_names=['Garage Area','Lot Area', 'Green Live Area'], # labels on graphs\n                                   grid_resolution=10) # number of values to plot on x axis\nmy_plotsGarArea = plot_partial_dependence(my_model3,       \n                                   features=[2], # column numbers of plots we want to show\n                                   X=X5,            # raw predictors data.\n                                   feature_names=['Garage Area','Lot Area', 'Green Live Area'], # labels on graphs\n                                   grid_resolution=10) # number of values to plot on x axis\n","2a38f698":"#pipelining for cleaner, productionised and efficient code\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Read Data\ndata6 = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ncols_to_use = ['GarageArea', 'LotArea', 'GrLivArea','YrSold','PoolArea']\nX6 = data6[cols_to_use]\ny6 = data6.SalePrice\ntrain_X6, test_X6, train_y6, test_y6 = train_test_split(X6, y6)","c8855804":"#You have a modeling process that uses an Imputer to fill in missing values, \n#followed by a RandomForestRegressor to make predictions. These can be bundled together with the make_pipeline function .\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Imputer\n\nmy_pipeline = make_pipeline(Imputer(), RandomForestRegressor())","bae93a52":"#fit and predict using this pipeline as a fused whole.\nmy_pipeline.fit(train_X6, train_y6)\npredictions6 = my_pipeline.predict(test_X6)","89ec76de":"#This is the code to do the same thing without pipelines\nmy_imputer6 = Imputer()\nmy_model7 = RandomForestRegressor()\n\nimputed_train_X6 = my_imputer6.fit_transform(train_X6)\nimputed_test_X6 = my_imputer6.transform(test_X6)\nmy_model7.fit(imputed_train_X6, train_y6)\npredictions6 = my_model7.predict(imputed_test_X6)","7c251fd7":"#read the data\nimport pandas as pd\ndata7 = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ncols_to_use = ['GarageArea', 'LotArea', 'GrLivArea','YrSold','PoolArea']\nX7 = data7[cols_to_use]\ny7 = data7.SalePrice","f1f17f85":"#specify a pipeline of our modeling steps (It can be very difficult to do cross-validation properly if you arent't using pipelines)\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Imputer\nmy_pipeline2 = make_pipeline(Imputer(), RandomForestRegressor())\n","3f83e887":"#get the cross-validation scores\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(my_pipeline2, X7, y7, scoring='neg_mean_absolute_error') #scoring specifies what measure of model quality to report \n#neg_mean_absolute_error stands for negative mean absolute error\n#Scikit-learn has a convention where all metrics are defined so a high number is better. \n#Using negatives here allows metrics to be consistent with that convention, though negative MAE is almost unheard of elsewhere.\nprint(scores)\n","ae794090":"#typically a single measure of model quality is wanted to compare between models. So we take the average across experiments.\nprint('Mean Absolute Error %2f' %(-1 * scores.mean()))","3dc75ce1":"#shows the data for the first 5 rows\nimport pandas as pd\n\ndata10 = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv', \n                   true_values = ['yes'],\n                   false_values = ['no'])  #always use both true_values=['yes'] and false_values=['no']\nprint(data10.head())","9a8ace26":"data10.shape #how many rows and columns are in the dataset","ed96e9d7":"#calculates the cross validation accuracy (mainly for a small amount of data with data columns consisting of 1s (true) and 0s(false))\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\none_hot_encoded_training_predictors2 = pd.get_dummies(data10) \n\ny20 = one_hot_encoded_training_predictors2.SaleCondition_Partial.head()\nX20 = one_hot_encoded_training_predictors2.head().drop(['SaleCondition_Partial'], axis=1)\n\n# Since there was no preprocessing, we didn't need a pipeline here. Used anyway as best practice\nmodeling_pipeline4 = make_pipeline(RandomForestClassifier())\ncv_scores = cross_val_score(modeling_pipeline4, X20, y20, scoring='accuracy')\nprint(\"Cross-val accuracy: %f\" %cv_scores.mean())","56a5ab42":"print(one_hot_encoded_training_predictors2.columns) # finding the columns in the dataset","85f07ca1":"#the highest accuracy of 1.000000 shows a data leakage. Models should be accurate but less than about 0.93\n#shows that one of the variables\/all the variables in potential_leaks are causing data leakage\npotential_leaks = ['SaleType_New', 'SaleCondition_Normal', 'SaleCondition_Abnorml', 'SaleType_ConLw']\nX21 = X20.drop(potential_leaks, axis=1)\ncv_scores = cross_val_score(modeling_pipeline4, X21, y20, scoring='accuracy')\nprint(\"Cross-val accuracy: %f\" %cv_scores.mean())\n","af2d7a9c":"#shows that SaleType_WD makes tthe model absolutely inaccurate\nSaleconditionNormal = one_hot_encoded_training_predictors2.SaleType_WD[one_hot_encoded_training_predictors2.SaleCondition_Partial]\nSaleconditionNotNormal = one_hot_encoded_training_predictors2.SaleType_WD[~one_hot_encoded_training_predictors2.SaleCondition_Partial]\n\nprint('Fraction of those who received a card with no expenditures: %.10f' %(( SaleconditionNormal == 0).mean())) #.10f stands for 10 decimal places\nprint('Fraction of those who received a card with no expenditures: %.10f'%((SaleconditionNotNormal == 0).mean()))","5380c899":"final_test.to_csv('myDataFrame.csv')\n","5ec2d644":"lessData=one_hot_encoded_training_predictors.loc[:,['Id','SalePrice']]\nlessData.to_csv(\"data2to2.csv\")","43c3cfc3":"import matplotlib.pyplot as plt #plt is used for multiple graph plotting\nfig, axarr = plt.subplots(2, 2, figsize=(12, 8))\ndata['LotFrontage'].value_counts().head(10).plot.bar(title='LotFrontage bar chart', ax=axarr[0][0]) #plots for the first 10 pieces of data\ndata['LotFrontage'].value_counts().head(10).plot.bar(ax=axarr[0][1]) #plots a bar chart of the relative proportions for the first 10 pieces of data\ndata['SalePrice'].head(25).value_counts().sort_index().plot.bar(ax=axarr[1][0]) #plots a barchart ascending in sale price for an ordinal (numerical) variable for the 1st 25 pieces of data\ndata['SalePrice'].head(5).value_counts().sort_index().plot.line(ax=axarr[1][1]) #line chart for a continuous dependent variable for the first 100 pieces of data\n#axarr is for [row][column]position of graph","a509a5b5":"data['SalePrice'].head(100).value_counts().sort_index().plot.line() #line chart for a continuous dependent variable for the first 100 pieces of data","bc45777c":"data['SalePrice'].head(100).value_counts().sort_index().plot.area() #area chart (similar to a line chart) for the first 100 pieces of data","7071fa24":"data[data['SalePrice'] < 300000]['SalePrice'].plot.hist() #plots a histogram dor SalePrices less than 300000","beea91b7":"data['SalePrice'].plot.hist() #plots a hisogram for all the data in SalePrice","89b9b8dc":"data[data['SalePrice'] < 60000] #shows all the records with SalePrices < 60000","d5fe7e26":"data[data['SalePrice'] < 200000].sample(100).plot.scatter(x='SalePrice', y='LotArea') #plots a scatter graph fr the 1st 100 pieces of data with SalePrices less than 200000\nprint(\"\\n\") #leaves a line\ndata[data['SalePrice'] < 200000].plot.scatter(x='SalePrice', y='LotArea') #plots a scatter graph for all of the SalePrices less than 200000\nprint(\"\\n\") #leaves a line\ndata[data['SalePrice'] < 200000].plot.hexbin(x='SalePrice', y='LotArea', gridsize=15) #plots a hex graph for SalePrices less tha 200000","6a7fb605":"data1.head(20).plot.bar(stacked=True) #bivariate bar chart (when the dependent variable has more than 2 pieces of data) for the first 20 pieces of data\nprint(\"\\n\") #leaves a line\ndata1.head(20).plot.area() #bivariate area graph\nprint(\"\\n\") #leaves a line\ndata1.head(20).plot.line() #bivariate line graph","20045490":"import seaborn as sns #for seaborn plotting\nsns.countplot(data['SalePrice'].head(5)) #seaborn countplot for the first 5 pieces of SalePrices data (shows the probability of each section of prices occuring)\n","10fbc75f":"import seaborn as sns #for seaborn plotting\nsns.kdeplot(data.query('SalePrice < 200000').SalePrice, color='mediumvioletred') #shows the probability of each SalePrice occuring","dc934450":"import seaborn as sns #for seaborn plotting\ndata[data['SalePrice'] < 200000]['SalePrice'].value_counts().sort_index().plot.line() #estimates how many times each SalePrice occurs","224b73a6":"import seaborn as sns #for seaborn plotting\nsns.kdeplot(data[data['SalePrice'] < 100000].loc[:, ['SalePrice', 'LotArea']].dropna().sample(50)) #one type of biavriate KDE plot showinf the distribution of SalePrices and LotArea","1979c0dc":"import seaborn as sns #for seaborn plotting\nsns.distplot(data['LotArea'], bins=10, kde=False) #displot for LotArea","d7d8553c":"import seaborn as sns #for seaborn plotting\nsns.jointplot(x='SalePrice', y='LotArea', data=data[data['SalePrice'] < 100000]) #jointplot for SalePrice (dependent variable) and LotArea (independent variable) for scatter graph and histogram","130fbdd4":"import seaborn as sns #for seaborn plotting\nsns.jointplot(x='SalePrice', y='LotArea', data=data[data['SalePrice'] < 100000], kind='hex', \n              gridsize=20) #jointplot for SalePrice (dependent variable) and LotArea (independent variable) for hex graph and histogram","b0de5d48":"import seaborn as sns #for seaborn plotting\nBx = data[data.LotShape.isin(data.LotShape.value_counts().head(3).index)]\n\nsns.boxplot(\n    x='LotShape',\n    y='SalePrice',\n    data=Bx\n) #box plot for lotshape and saleprice","9b0f3f6c":"import seaborn as sns #for seaborn plotting\nsns.violinplot(\n    x='LotShape',\n    y='SalePrice',\n    data=data[data.LotShape.isin(data.LotShape.value_counts()[:3].index)]\n) #violin plot for lotshape and saleprice","dd7fc442":"import seaborn as sns #for seaborn plotting (advanced graph plotting)\ndta = data\n\ng = sns.FacetGrid(dta, col=\"LotShape\", col_wrap=6) \ng.map(sns.kdeplot, \"SalePrice\")\n#shows the distribution of the categorical variable lotshape with saleprice","fb04fb3d":"import seaborn as sns #for seaborn plotting (advanced graph plotting)\ndta2 = data[data['Street'].isin(['Pave', 'Grvl'])]\ndta2 = dta2[dta2['HouseStyle'].isin(['2Story', '1Story', '1.5Fin'])]\n\ng2 = sns.FacetGrid(dta2, row=\"Street\", col=\"HouseStyle\")\ng2.map(sns.violinplot, \"SalePrice\")\n#shows the distribution of Street together with HouseStyle along Saleprice","5ac1c869":"import seaborn as sns #for seaborn plotting (advanced graph plotting)\nsns.pairplot(data[['SalePrice', 'PoolArea', 'LotArea']]) #pairplot","e7ae7130":"import seaborn as sns\n\nsns.lmplot(x='SalePrice', y='LotArea', hue='HouseStyle', \n           data=data.loc[data['HouseStyle'].isin(['1.5Fin', '1Story', '2Story'])], \n           fit_reg=False)\n#multivariate scatter plot","0097a0ba":"import seaborn as sns\n\nsns.lmplot(x='SalePrice', y='LotArea',  markers=['o', 'x', '*'], hue='HouseStyle', \n           data=data.loc[data['HouseStyle'].isin(['1.5Fin', '1Story', '2Story'])], \n           fit_reg=False)\n#multivariate scatter plot with markers","00e4eb10":"from plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go #for plotly graphs\n\niplot([go.Scatter(x=data.head(1000)['SalePrice'], y=data.head(1000)['LotArea'], mode='markers')])\n#basic plotly scatter graph","f7a29fc4":"from plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go #for plotly graphs\n\niplot([go.Histogram2dContour(x=data.head(500)['SalePrice'], \n                             y=data.head(500)['LotArea'], \n                             contours=go.Contours(coloring='heatmap')),\n       go.Scatter(x=data.head(1000)['SalePrice'], y=data.head(1000)['LotArea'], mode='markers')])\n# KDE plot (what plotly refers to as a Histogram2dContour) and scatter plot of the same data.","80131f91":"from plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go #for plotly graphs\ndtt = data.assign(n=0).groupby(['SalePrice', 'LotArea'])['n'].count().reset_index()\ndtt = dtt[dtt[\"LotArea\"] < 2000]\nver = dtt.pivot(index='LotArea', columns='SalePrice', values='n').fillna(0).values.tolist()\niplot([go.Surface(z=ver)])\n#plotly Surface (the most impressive feature)","b0c48fe6":"from plotnine import * #plotline graphs\ndta3 = data.head(1000)\n\n(\n    ggplot(dta3)\n        + aes('SalePrice', 'LotArea')\n        + geom_point()\n        + stat_smooth()\n)\n#plots a line of best fit (logistic regression) along the scatter graph","4048eb08":"from plotnine import * #plotline graphs\ndta4 = data.head(1000)\n\n(\n    ggplot(dta4)\n        + aes('SalePrice', 'LotArea')\n        + geom_point()\n        + aes(color='SalePrice')\n        + stat_smooth()\n)\n#plots a line of best fit (logistic regression) along the scatter graph with coloured points","2368f7bf":"from plotnine import * #plotline graphs\ndta5 = data.head(1000)\n\n(ggplot(dta5)\n     + aes('SalePrice', 'LotArea')\n     + aes(color='SalePrice')\n     + geom_point()\n     + stat_smooth()\n     + facet_wrap('HouseStyle')\n)\n#applying faceting with the categorical variable HouseStyle","ec974252":"from plotnine import * #plotline graphs\n(ggplot(data)\n         + aes('SalePrice', 'HouseStyle')\n         + geom_bin2d(bins=20)\n         + ggtitle(\"Most Common house styles\")\n)\n#The plotnine equivalent of a hexplot, a two-dimensional histogram, is geom_bin2d","66071ef5":"from pandas.plotting import autocorrelation_plot\n\nautocorrelation_plot(data['SalePrice'])\n\n#The autocorrelation plot is a multivariate summarization-type plot that lets you check every periodicity at the same time.\n#It does this by computing a summary statistic\u2014the correlation score\u2014across every possible lag in the dataset. This is known as autocorrelation.\n#In an autocorrelation plot the lag is on the x-axis and the autocorrelation score is on the y-axis.\n#The farther away the autocorrelation is from 0, the greater the influence that records that far away from each other exert on one another.\n#Here is what an autocorrelation plot looks like when applied to the Saleprice data:","c50a04a8":"test_path = '..\/input\/house-prices-advanced-regression-techniques\/test.csv' # this is the path to the test data that you will use\nTestdata = pd.read_csv(test_path)\n\n#factors that will predict Sale Price\ndesired_factors = ['1stFlrSF','YearBuilt', 'LotArea', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n\n#set prediction data to factors that will predict, and set target to SalePrice\ntrain_data = data[desired_factors]\ntest_data = Testdata[desired_factors]\ntarget = data.SalePrice\n\n#fitting model with prediction data and telling it my target dor the test.csv data\nmodel.fit(train_data, target)\n\nmodel.predict(test_data)","f07241b6":"submit= pd.DataFrame({'Id': Testdata.Id, 'SalePrice': model.predict(test_data)})\n\nsubmit.to_csv('submitFile.csv', index=False)","8df4c100":"x_train = data['HouseStyle']\nx_test = Testdata['HouseStyle']\ny=data['SalePrice']\nfrom nltk.tokenize import TweetTokenizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\ntext_clf = Pipeline([('tfidf', TfidfVectorizer(ngram_range=(1,10), max_features=10000, lowercase=True, use_idf=True, smooth_idf=True, sublinear_tf=False, tokenizer=TweetTokenizer().tokenize, stop_words='english')),\n                         ('clf', LogisticRegression(random_state=17, C=1.8))])\nfrom sklearn.model_selection import RandomizedSearchCV\nparameters = {\n               'clf__C': np.logspace(.1,1,10),\n }\ngs = RandomizedSearchCV(text_clf, parameters, n_jobs=-1, verbose=3)\ntext_clf.fit(x_train, y)\npredicted = text_clf.predict(x_test)\nTestdata['SalePrice'] = predicted","dad6051d":"onesubmission = data1[[\"Id\",\"SalePrice\"]]\nonesubmission.to_csv(\"Onesubmission.csv\", index = False)","a68fcb7e":"import math\n\ndef gaussian_pdf1(x, sigma, w):\n    return math.exp( -(x - w)**2 \/ (2 * sigma**2) )","30ced7c2":"%matplotlib inline\nimport matplotlib\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nsigma = 0.1\nw = 0.5\nx = np.linspace(w - 2, w + 2, 100)\nfig = plt.figure('Fungsi Gaussian')\nax = fig.add_subplot(111)\nax.set_title('Fungsi Gaussian dengan $\\sigma = %s, w = %s$' % (sigma, w))\nax.set_xlabel('$x$')\nax.set_ylabel('$f(x; \\sigma, w)$')\nax.grid(which='major')\nax.plot(x, [gaussian_pdf1(_, sigma, w) for _ in x])\nplt.show()","c568e2bf":"%matplotlib inline\nimport matplotlib\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nw = 0.5\nx = np.linspace(w - 2, w + 2, 100)\nfig = plt.figure('Fungsi Gaussian')\nax = fig.add_subplot(111)\nax.set_title('Fungsi Gaussian dengan $\\sigma = \\{0.1, 0.2, 0.5, 1.0\\}; w = %s$' % (w))\nax.set_xlabel('$x$')\nax.set_ylabel('$f(x; \\sigma, w)$')\nax.grid(which='major')\nax.plot(x, [gaussian_pdf1(_, 0.1, w) for _ in x], label='$\\sigma = 0.1$')\nax.plot(x, [gaussian_pdf1(_, 0.2, w) for _ in x], label='$\\sigma = 0.2$')\nax.plot(x, [gaussian_pdf1(_, 0.5, w) for _ in x], label='$\\sigma = 0.5$')\nax.plot(x, [gaussian_pdf1(_, 1.0, w) for _ in x], label='$\\sigma = 1.0$')\nplt.legend()\nplt.show()","7b78127d":"import math\n\ndef gaussian_pdf2(x, sigma, w_j):\n    return math.exp(\n        -( (x[0] - w_j[0])**2 + (x[1] - w_j[1])**2 ) \/\n        (2 * sigma**2) )","f23e6868":"%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nsigma = 0.1\nw_j = (0.5, 0.7)\n\n\nx_0_range = np.linspace(w_j[0] - 3*sigma, w_j[0] + 3*sigma, 100)\nx_1_range = np.linspace(w_j[1] - 3*sigma, w_j[1] + 3*sigma, 100)\nX_0, X_1 = np.meshgrid(x_0_range, x_1_range)\nfs = np.array( [gaussian_pdf2((x_0, x_1), sigma, w_j)\n                for x_0, x_1 in zip(np.ravel(X_0), np.ravel(X_1))] )\nFS = fs.reshape(X_0.shape)\n\nfig = plt.figure('Fungsi Gaussian dengan 2 variabel')\nax = fig.add_subplot(111, projection='3d')\nax.set_title('$\\sigma = %s, w_j = (%s, %s)$' % (sigma, w_j[0], w_j[1]))\nax.set_xlabel('$x_0$')\nax.set_ylabel('$x_1$')\nax.set_zlabel('$f(x_0, x_1; \\sigma, w_j)$')\nax.plot_surface(X_0, X_1, FS)\nplt.show()","60b4d9e4":"import numpy as np\n\nW = np.array([ (data['SalePrice'][d], data['LotArea'][d]) \n              for d in range(len(data))])\nW","c92cb79e":"sigma = 0.1\nx = (0.2, 0.6)\npatterns = np.array([ gaussian_pdf2(x, sigma, w_j) for w_j in W ])\npatterns","26c67625":"n1 = patterns[0] + patterns[1]\nn2 = patterns[2] + patterns[3]\nn3 = patterns[4] + patterns[5] + patterns[6]\n\nprint('n1 = %s' % n1)\nprint('n2 = %s' % n2)\nprint('n3 = %s' % n3)","a8d7ba9b":"Most scikit-learn objects are either transformers or models.\n\n**Transformers** are for pre-processing before modeling. The Imputer class (for filling in missing values) is an example of a transformer. Over time, you will learn many more transformers, and you will frequently use multiple transformers sequentially.\n\n**Models** are used to make predictions. You will usually preprocess your data (with transformers) before putting it in a model.\n\nYou can tell if an object is a transformer or a model by how you apply it. After fitting a transformer, you apply it with the transform command. After fitting a model, you apply it with the predict command. Your pipeline must start with transformer steps and end with a model.","fa0ee718":"There are two main types of data leakage: **Leaky predictors** and **Leaky validation Strategies**.   **Leaky predictors** occur when your predictors include data that will not be available at the time you make predictions. To prevent this type of data leakage, any variable updated (or created) after the target value is realized should be excluded. Because when we use this model to make new predictions, that data won't be available to the model.   \n**Leaky Validation Strategies** : A much different type of leak occurs when you aren't careful distinguishing training data from validation data. For example, this happens if you run preprocessing (like fitting the Imputer for missing values) before calling train_test_split. Validation is meant to be a measure of how the model does on data it hasn't considered before. You can corrupt this process in subtle ways if the validation data affects the preprocessing behavoir.. The end result? Your model will get very good validation scores, giving you great confidence in it, but perform poorly when you deploy it to make decisions. To prevent or find** leaky predictors**:To screen for possible leaky predictors, look for columns that are statistically correlated to your target; or If you build a model and find it extremely accurate, you likely have a leakage problem. To prevent **Leaky Validation Strategies**: If your validation is based on a simple train-test split, exclude the validation data from any type of fitting, including the fitting of preprocessing steps. This is easier if you use scikit-learn Pipelines. When using cross-validation, it's even more critical that you use pipelines and do your preprocessing inside the pipeline.","2911b21b":"The first graph shows that a larger garage area results in a higher house price.\nThe second graph shows that a larger Lot area results in a higher house price.\nThe third graph shows that a larger Green Luve area results in a higher house price.","14302197":"# Introduction\n**This will be your workspace for the [Machine Learning course](https:\/\/www.kaggle.com\/learn\/machine-learning).**\n\nYou will need to translate the concepts to work with the data in this notebook, the Iowa data. Each page in the Machine Learning course includes instructions for what code to write at that step in the course.\n\n# Write Your Code Below","0d03c267":"\n**If you have any questions or hit any problems, come to the [Learn Discussion](https:\/\/www.kaggle.com\/learn-forum) for help. **\n\n**Return to [ML Course Index](https:\/\/www.kaggle.com\/learn\/machine-learning)**","80de3be8":"**probabilistic neural network**","89cd1592":"Examples of **interval** variables are the wind speed in a hurricane, shear strength in concrete, and the temperature of the sun. An **interval** variable goes beyond an ordinal categorical variable: it has a meaningful order, in the sense that we can quantify what the difference between two entries is itself an **interval** variable."}}