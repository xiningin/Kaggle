{"cell_type":{"5f33045b":"code","ad917634":"code","4ed0d674":"code","95d60a06":"code","0332a6b8":"code","4543aa58":"code","b41a6768":"code","cb57f019":"markdown","36e97a53":"markdown"},"source":{"5f33045b":"#Import dependencies\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\nimport gc\nimport glob\nimport os\nfrom tqdm import tqdm","ad917634":"sub_file = pd.read_csv('..\/input\/sample_submission.csv', index_col=0)","4ed0d674":"%%time\n#Load data from train file\ntrain = pd.read_csv('..\/input\/train.csv', dtype={'acoustic_data': np.float32, 'time_to_failure': np.float32})","95d60a06":"#Split the big train series into a fixed number of segments. Searching directly in the full\n#train series was too much for my computer \nnumber_train_segments = 3\ntrain_segment_length = int(train.shape[0] \/ number_train_segments)","0332a6b8":"#Read segment test data names\ntests = glob.glob('..\/input\/test\/**')\ntests_names = os.listdir('..\/input\/test\/')","4543aa58":"#Search the test pattern in the train data segments using the correlation coefficient and OpenCV\nsub_list = []\nfor j in tqdm(range(5)): #range(len(tests))): Only 5 as an example, because commiting with all of them takes too much\n                         #I only want to share the methodology\n    #Read segment data\n    segment_test = pd.read_csv(tests[j], dtype={'acoustic_data': np.float32})\n    segment_test = np.float32( segment_test['acoustic_data'].values )\n    #Resize the vector to have the correct dimensions\n    segment_test_tp = np.resize(segment_test, (1,len(segment_test)))\n\n    coefs = []\n    for i in range(0,number_train_segments):\n        print('Searching similarity for test segment {} with {}-segment of train data:'.format(tests_names[j], i+1))\n        segment_train = train.iloc[train_segment_length*i : train_segment_length*(i+1)]\n        segment_train = np.float32( segment_train['acoustic_data'].values )\n        segment_train_tp = np.resize( segment_train, (1,len(segment_train)) )\n    \n        gc.collect()\n    \n        result = cv2.matchTemplate(segment_test_tp, segment_train_tp, cv2.TM_CCORR_NORMED)\n        \n        #Append the best matching for that train segment (coeff and position)\n        coefs.append([np.max(result), train_segment_length*i + np.argmax(result) + segment_test_tp.shape[1]-1])\n    \n    #Apprend the best result among all train segments\n    coefs = np.array(coefs)\n    sub_list.append( [ tests_names[j], train.time_to_failure.iloc[int(coefs[np.argmax(coefs[:,0]),1])] ] )","b41a6768":"sub_df = pd.DataFrame(data=sub_list, columns=['seg_id','time_to_failure'])\nsub_df['seg_id'] = sub_df['seg_id'].apply(lambda x: x[:-4])\nsub_df.set_index('seg_id',inplace=True)\n\n#Read submission_file and rearrange the index in the \nsub_file = pd.read_csv('..\/input\/sample_submission.csv', index_col=0)\nsub_df = sub_df.reindex(sub_file.index)\nsub_file = sub_df\n\nsub_file.to_csv('sub_file_v00.csv')","cb57f019":"Any suggestions, thoughts, ideas, complains, and feedback are welcome!","36e97a53":"Hey everyone!\nI am very new on Kaggle Competitions, and here I want tho share a different solution that I tried (with a bad LB score and quite inefficient to run) but perhaps may give someone a new idea or whatever. The goal is to search each test segment pattern in the train set, and assign the 'time_to_failure' value in that way."}}