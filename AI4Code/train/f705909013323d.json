{"cell_type":{"43ba0c27":"code","b1d46196":"code","ca60020e":"code","f8cbec5f":"code","36456f88":"code","bfe179e1":"code","67e25a24":"code","91f0dcaf":"code","6f8b5d3c":"code","72e15807":"code","c679e661":"code","67b56f82":"code","b2609223":"code","3d77569f":"code","c719dbbe":"code","867efe39":"code","15da4795":"code","6858a9d4":"code","3048979f":"code","a53048e6":"code","1953d9be":"code","b80c9be4":"code","3bb9cb39":"code","d132c493":"code","4a7b8050":"code","77fd53ff":"code","ea45c363":"code","6181b95c":"code","9eb89fc0":"code","d2f79757":"code","c7743db2":"markdown","287cafd9":"markdown","c9e7be20":"markdown","18d456fb":"markdown","4ec1665f":"markdown","1434a7b2":"markdown","4af78f46":"markdown","42736a46":"markdown","606269c9":"markdown","31fdc984":"markdown","1a7fd641":"markdown","f8222c0c":"markdown","da9bb704":"markdown","f78a069b":"markdown","793b6c4b":"markdown","914e70a5":"markdown","5a22eca3":"markdown","52771db1":"markdown","b659d606":"markdown","8a0148ea":"markdown","475f9d8b":"markdown","49a4dc1a":"markdown","6a710e75":"markdown","fc7ac6e0":"markdown","fa543171":"markdown","62b6d9b8":"markdown","aba7c78b":"markdown","8d20f2f3":"markdown","2b1ee1ee":"markdown","5665e11e":"markdown","ee156d94":"markdown","1f53d6f9":"markdown","50186a1d":"markdown","a845723e":"markdown","bab2a0ca":"markdown","ca5760cf":"markdown","4b462758":"markdown","d9ab0ea5":"markdown","9d6ebc65":"markdown","2677de0e":"markdown","1aa73fcd":"markdown","4e73fa40":"markdown","fe721ffb":"markdown","c4844b02":"markdown","7b3cf2f5":"markdown","8eed89d4":"markdown","82e1f2a8":"markdown"},"source":{"43ba0c27":"#Importing\n\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport warnings\nimport os\n\nprint('File paths:')\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n       \nfor dirname, _, filenames in os.walk('Data\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b1d46196":"#Converting csv files to pandas dataframes\nx = '\/kaggle\/input\/house-prices-advanced-regression-techniques\/'\ntrain_data = pd.read_csv(x + 'train.csv')\ntest_data = pd.read_csv(x + 'test.csv')","ca60020e":"train_data.tail()","f8cbec5f":"test_data.shape","36456f88":"train_data.columns","bfe179e1":"#Obtaining correlation matrix\ncorrelation_matrix = train_data.corr()\nfig, ax1 = plt.subplots(figsize=(11,9))\nsns.heatmap(correlation_matrix,square=True)\nplt.show()","67e25a24":"variables = 20 #amount of features we're interested in\ncolumns = correlation_matrix['SalePrice'].sort_values(\n          ascending=False).iloc[:variables].index\ncm = train_data[columns].corr()\nsns.set(font_scale=1.25)\nfig,ax = plt.subplots(figsize=(14,10))\nhm = sns.heatmap(cm,annot=True,fmt='.2f',square=True,annot_kws={'fontsize':11})\nplt.show()","91f0dcaf":"fig,axes = plt.subplots(figsize=(14,12),ncols=4,nrows=5)\nfig.tight_layout()\nfor i in range(variables):\n    ax = axes[i%5,i%4]\n    sns.scatterplot(x=train_data[columns[i]], y=train_data['SalePrice'],ax=ax)\n    ax.set_ylabel('')\n    \nprint('Please note, the \"Y\" axis for all these plots is \"SalePrice\"')","6f8b5d3c":"df_train = train_data.loc[:,columns] #restricting train data to only our columns of interest, finally\ndf_test = test_data[columns[1:]] #Getting the relevant test data\ndf_combo = pd.concat([df_train.iloc[:,1:],df_test],axis=0) #Combining the df's so we can remove columns correctly\ndf_combo = df_combo.reset_index().drop('index',axis=1) #Making index proper, just incase\n\n\nn_train = train_data.shape[0]\nn_test = test_data.shape[0]","72e15807":"total_mv = df_train.isnull().sum().sort_values(ascending=False)\npercentage = (df_train.isnull().sum()*100\/df.shape[0]).sort_values(ascending=False)\n\n\nfig, ax = plt.subplots(figsize=(9, 7))\nsns.barplot(x=percentage.index[:5],y=percentage[:5])\n\nsns.set(font_scale=1.2)\nplt.xlabel('Features')\nplt.ylabel('Percent of missing values')\nplt.title('Percent missing data by feature')\nplt.show()\n\npercentage = percentage.map('{:,.2f}%'.format)\ncombined = pd.concat([total_mv,percentage],axis=1,keys=['Total','Percentage'])\ncombined.head(5)","c679e661":"df_combo = df_combo.drop(['LotFrontage','MasVnrArea','GarageYrBlt'],axis=1)","67b56f82":"plt.figure(figsize=(10,6))\nsns.scatterplot(y=df_train['SalePrice'],x=df_train['OpenPorchSF'])","b2609223":"df_combo = df_combo.drop(['OpenPorchSF'],axis=1)\n\nplt.figure(figsize=(10,6))\nsns.scatterplot(y=df_train['SalePrice'],x=df_train['GarageArea'])","3d77569f":"df_combo = df_combo.drop(df_train[df_train['GarageArea'] > 1220].index,axis=0)\ndf_train = df_train.drop(df_train[df_train['GarageArea'] > 1220].index,axis=0)\nn_train -= 4\n\nplt.figure(figsize=(10,6))\nsns.scatterplot(y=df_train['SalePrice'],x=df_train['WoodDeckSF'])","c719dbbe":"df_combo = df_combo.drop('WoodDeckSF',axis=1)\n\nprint(df_train.shape)","867efe39":"df_combo = df_combo.drop(['GarageCars','1stFlrSF'],axis=1)","15da4795":"from scipy import stats\n\nsns.distplot(df_train['SalePrice'] ,kde=True);\n\n\n#ploting the distribution\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)\nplt.show()","6858a9d4":"df_train['SalePrice'] = np.log(df_train['SalePrice']+1)\n\nsns.distplot(df_train['SalePrice'] ,kde=True);\n#ploting the distribution\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)\nplt.show()","3048979f":"for col in ['GrLivArea','TotalBsmtSF','GarageArea']:\n    df_combo[col] = np.log(df_combo[col]+1)","a53048e6":"df_combo['YearBuilt'] = np.log(df_combo['YearBuilt'])\/np.log(10)","1953d9be":"df_combo = pd.get_dummies(df_combo)","b80c9be4":"train = df_combo.iloc[:n_train,:]\ntest = df_combo.iloc[n_train:,:]\ntest = test.fillna(0) \ny_train = np.array(df_train['SalePrice'])","3bb9cb39":"import xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n\nmodel_rfr = RandomForestRegressor()\nmodel_gbr = GradientBoostingRegressor()\nmodel_xgb = xgb.XGBRegressor(n_estimators=2000)\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction_seed=9, bagging_seed=9)","d132c493":"model_xgb.fit(train,y_train)\nmodel_lgb.fit(train,y_train)\nmodel_rfr.fit(train,y_train)\nmodel_gbr.fit(train,y_train)","4a7b8050":"submission  = pd.DataFrame(data={'Id': np.arange(1461,1459+1461)})","77fd53ff":"prediction_1 = model_xgb.predict(test)\nsubmission['xgb'] = np.exp(prediction_1) - 1\nprediction_2 = model_lgb.predict(test)\nsubmission['lgb'] = np.exp(prediction_2) - 1\nprediction_4 = model_rfr.predict(test)\nsubmission['rfr'] = np.exp(prediction_4) - 1\nprediction_5 = model_gbr.predict(test)\nsubmission['gbr'] = np.exp(prediction_5) - 1","ea45c363":"submission['final'] = submission['xgb']*0.3+submission['lgb']*0.3 + submission['rfr']*0.3+submission['gbr']*0.1","6181b95c":"submission.head()","9eb89fc0":"submission_best = submission[['Id','final']]\nsubmission_best = submission_best.rename(columns={'final': 'SalePrice'})\nsubmission_best.to_csv('submission_final.csv',index=False)","d2f79757":"submission_best","c7743db2":"Similar to OpenPorchSF, this needs cutting","287cafd9":"As we can see, this needs to be normalised for our model to like it! It turns out (thanks statistics) that we can apply a magical log(1+x) transformation which fixes this. Log(1+x) instead of log(x) allows us to deal with values of 0","c9e7be20":"Our linear models tend to only work very well with normalised\/standardised data. We need to inspect the distrubutions of values and make sure we have reasonably normally distrubuted data","18d456fb":"###  <a id='3.2'> 3.2 Dummy Variables <\/a>","4ec1665f":"# <a id='2.0'> 2.0 Missing Data & Outliers <\/a>","1434a7b2":"# House Prices Project\n### By Charlie Gaynor <br>\n#### CharlieJackGaynor@Gmail.com","4af78f46":"#### Zoomed in heatmap","42736a46":"### <a id='4.1'> 4.1 Setup <\/a>","606269c9":"[jump](#2.0)","31fdc984":"For modelling I will be using an ensemble of regression models. I will use XGboost, Lightgbm, and tools from sklearn to make predictions. I will then combine them all using some fine-tuning skills (guessing) to optimise the model.","1a7fd641":"Let us look at outliers in more detail:","f8222c0c":"# <a id='1.0'> 1.0 Visualisations <\/a>","da9bb704":"First we need to combine the train and test dataframes, so when we are culling columns, we do it to both sets. Care needs to be taken when doing any scaling using the combined data frame, as we want to avoid leaking data!","f78a069b":"### <a id='0.4'> 0.4 Impressions <\/a>","793b6c4b":"Instead of chopping this feature out (for once), I'm going to remove the observations corresponding to the 4 outliers we can see, corresponding to values of GarageArea > 1220","914e70a5":"These heatmaps never fail to amaze me. Anyway, we can see a few interesting things in this data. This is some of the main things I pick up on: (before I do any dropping though, I need to have a look at the Null values, as we don't want to drop a column with significantly more data)\n\n- We should drop GarageArea as it is less correlated with SalePrice than GarageCars. They basically represent the same data anyway.\n\n- Similarly, we should drop 1stFlrSF.\n\n- YearBuilt is an interesting one, would probably require time series analysis for a better prediction (plug: see my time series analysis project for an example of this)\n\n- 'GrLivArea' and 'TotRmsAbvGrd' ( Total Rooms above ground - which makes these houses sound more like nuclear disaster bunkers) are very similar also. We should drop 'TotRmsAvgGrd'\n\n- Whether we should include the last 6 features is dubious, this is something I will tinker with","5a22eca3":"### <a id=1.1> 1.1 Heatmaps <\/a>","52771db1":"Well that's it! I achieve a score of: 0.13061 (RMSLE)\n\nThis puts me in position 1528\/4815 - which is top 32%! \n\nThis is not bad at all to say it felt like we binned just about all the features, and the competition is unregulated etc.\n\nImprovements would be: \n- Keeping more features, and tackling their NA values in smart ways (e.g mean, medians and such).\n- one-hot encoding some data,but ordinal encoding others.\n- Find better transformations \/ perform seperate time series analysis for 'YearBuilt' and the like\n- Use more models for the ensemble and spend more time tweaking parameters\n- ~~Hand label the data to get a perfect score~~","b659d606":"We will combine the 4 models to get a (weighted) average for our final submission. I've played around with the coefficients here a little to try and improve the score","8a0148ea":"Now lets visualise the distrubutions of each of the features against the SalePrice\n\nUnfortunately Seaborn.pairplot() is throwing errors when trying to plot similar to below (we have too many features for a typical pairplot really). I've improvised whilst my query on stack overflow is doing work, but this solution sadly doesn't look as nice","475f9d8b":"### <a id='4.2'> 4.2 Fitting and predicting <\/a>","49a4dc1a":"First I will split the previously combined dataframe into 'train' and 'test' data sets.","6a710e75":"The first thing we notice is that the data has a massive amount of features! Not only that, but a lot of them sound the same to me. It's worth further inspection, but I suspect alot of features will have correlations with eachother - such as 'Bsmt...' and 'BsmtQual'.\n\nWhilst sounding like something from a Harry Potter movie (or book, I suppose...), the 'Curse of Dimensionality' could well be relevant here. With so many features, it might be worth cutting some of these aforementioned similar features (if we have 'Landcontour' do we really need 'LandSlope'?) We also see the id column has no use in predictive analytics or visualisations, so we will drop this soon.\n\nFirst thing is first though: Let's have a look at using the amazing heatmap function from Seaborn","fc7ac6e0":"# <a id='contents'> Contents <\/a>","fa543171":"### <a id='3.1'> 3.1 Normalizing<\/a>","62b6d9b8":"### <a id='0.3'> 0.3 First Looks<\/a>","aba7c78b":"This a nightmare for outliers, and has a low correlation with SalePrice, so lets get rid of this straight away","8d20f2f3":"We can see that YearBuilt does indeed have an interesting relationship. It is slightly exponential in nature, but with a very upward trend near the end. This should certainly be transformed to have a more linear relation with SalePrice. Deciding what transformation to use will be tricky though\n\nWe get a good idea of outliers here. I can see many potential problematic points. Before I go and chop away half my data, lets first finally look at missing values, incasing we remove any observations which are the supposed outliers","2b1ee1ee":"# <a id='3.0'> 3.0 Reshaping Data<\/a>","5665e11e":"### <a id = '1.2'> 1.2 Scatter Plots <\/a>","ee156d94":"### <a id='2.1'> 2.1 Null Values <\/a>","1f53d6f9":"(Another plug incoming)\n\n\nBy Charlie Gaynor<br><br>\nCharlieJackGaynor@gmail.com\n\nlinkedin.com\/in\/CharlieGaynor","50186a1d":"# <a id='4.0'> 4.0 Modelling<\/a>","a845723e":"In this project I aim to predict the 'final prices' of residential homes in Ames, Iowa, using a small-ish subset of the given data. The data includes 79 explanatory variables describing the majority of aspects of the homes. I aim to use <= 20, and still hope to beat half of the participants.\n\nThe project will include data cleaning, feature engineering, data visualisation, modelling and results analysis.\n\nI will be using both an ensemble of regression and decision-tree based models to perform the predictive analysis.\n\nNote I will be discarding a lot of features, as we are provided with an abundance. This might not always be optimal score-wise, but it will make our model a lot more efficient and easier to interpret. Of course if the top priority was leaderboard score then more features will be kept (but not neccessarily all), in addition to longer training times + more models etc...","bab2a0ca":"Preparing the file for submission!","ca5760cf":"Finally lets cull the features we wanted to earlier, but was waiting to check NA Values first","4b462758":"### <a id='0.'> 0.1 Description<\/a>","d9ab0ea5":"# <a id='0.0'> 0.0 Setup<\/a>","9d6ebc65":"Much better! I will now perform this for all similar quantities that need normalizing.","2677de0e":"Beautiful. We can get so much information from this alone. A few things i've noticed:\n\n'Total BsmtSF' and '1stFlrSf' both have a good correlation with SalePRice, but a very high correlation with eachother. Similiarly for 'GarageCars', and 'GarageArea'. In essence they will be very close to providing the same data, hence we can remove one of each of the 'duplicates'\n\n'GarageYrBlt' and 'YearBuilt' seem very similar - perhaps we only need one of these, but this should be examined better.\n\nThere about 20 features which seem to have a reasonably significant correlation with SalePrice. As we have so many features we can be picky with the ones we want to keep. Therefore, next I will have a look in more detail at which to keep","1aa73fcd":"LotFrontage has a relatively low correlation with SalePrice, high NA%, and looks like it's a good candidate for producing outliers - so I'm happy to get rid of it.\n\nGarageYrBlt seems to correlate highly with YearBuilt. I could replace the NA's with the value for YearBuilt, BUT we also have GarageArea & GarageCars (for now) covering garage data, so that can go too.\n\nMasVnrArea also seems to contribute nicely to outliers, and has a large amount of data crumped at 0. I could replace the NA values with the mean value for example, but I don't think we will miss information on the masonry veneer area (this could be a very bad step, but I will try it both ways).","4e73fa40":"- [0.0 Setup](#0.0)\n    - [0.1 Description](#0.1)\n    - [0.2 Imports](#0.2)\n    - [0.3 First looks](#0.3)\n\t- [0.4 Impressions](#0.4)\n- [1.0 Visualisations](#1.0)\n\t- [1.1 Heatmaps](#1.1)\n\t- [1.2 Scatter Plots](#1.2)\n- [2.0 Missing Data & Outliers](#2.0)\n\t- [2.1 Null values](#2.1)\n\t- [2.2 Outliars](#2.2)\n\t- [2.3 Final Culling](#2.3)\n- [3.0 Reshaping data](#3.0)\n\t- [3.1 Normalizing](#3.1)\n\t- [3.2 Dummy Variables](#3.2)\n- [4.0 Modelling](#4.0)\n\t- [4.1 Setup](#4.1)\n\t- [4.2 Fitting and predicting](#4.2)","fe721ffb":"### <a id='2.3'> 2.3 Final Culling <\/a>","c4844b02":"Not too difficult this one (at least, I don't think so...)","7b3cf2f5":"### <a id='0.'> 0.2 Imports<\/a>","8eed89d4":"### <a id='2.2'> 2.2 Outliars <\/a>","82e1f2a8":"Since we transformed the 'SalePrice' data earlier on, we need to apply the inverse transformation."}}