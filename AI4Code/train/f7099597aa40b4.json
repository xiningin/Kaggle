{"cell_type":{"fb06ee6d":"code","d227a608":"code","a96873c0":"code","aa5aaf64":"code","2d05c07c":"code","67e32029":"code","ed4c2c76":"code","c762f7c4":"code","fc47e4b7":"code","6039cd09":"code","e5a734a2":"markdown","8adb6b0d":"markdown","a222f8ec":"markdown","d7a99718":"markdown","4e0e9e96":"markdown","d95c5181":"markdown","d76faec4":"markdown","6fa3d8cb":"markdown","a1ae3a91":"markdown","5507e3fd":"markdown","3656e392":"markdown","ab0b8950":"markdown"},"source":{"fb06ee6d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n\nPATH = '\/kaggle\/input\/plant-pathology-2020-fgvc7\/'\n\ntrain = pd.read_csv(PATH + 'train.csv')\ntest = pd.read_csv(PATH + 'test.csv')\n\ntarget = train[['healthy', 'multiple_diseases', 'rust', 'scab']]\ntest_ids = test['image_id']\n\ntrain_len = train.shape[0]\ntest_len = test.shape[0]\n\ntrain.describe()","d227a608":"from PIL import Image\nfrom tqdm.notebook import tqdm\n\nSIZE = 224\n\ntrain_images = np.empty((train_len, SIZE, SIZE, 3))\nfor i in tqdm(range(train_len)):\n    train_images[i] = np.uint8(Image.open(PATH + f'images\/Train_{i}.jpg').resize((SIZE, SIZE)))\n    \ntest_images = np.empty((test_len, SIZE, SIZE, 3))\nfor i in tqdm(range(test_len)):\n    test_images[i] = np.uint8(Image.open(PATH + f'images\/Test_{i}.jpg').resize((SIZE, SIZE)))\n\ntrain_images.shape, test_images.shape","a96873c0":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(train_images, target.to_numpy(), test_size=0.2, random_state=289) \n\nx_train.shape, x_test.shape, y_train.shape, y_test.shape","aa5aaf64":"from imblearn.over_sampling import RandomOverSampler\n\nros = RandomOverSampler(random_state=289)\n\nx_train, y_train = ros.fit_resample(x_train.reshape((-1, SIZE * SIZE * 3)), y_train)\nx_train = x_train.reshape((-1, SIZE, SIZE, 3))\nx_train.shape, y_train.sum(axis=0)","2d05c07c":"import gc\n\ndel train_images\ngc.collect()","67e32029":"from keras.models import Model, Sequential, load_model, Input\nfrom keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization, LeakyReLU\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\nfrom keras.utils import plot_model\nfrom keras.regularizers import l2\n\nrlr = ReduceLROnPlateau(patience=15, verbose=1)\nes = EarlyStopping(patience=35, restore_best_weights=True, verbose=1)\nmc = ModelCheckpoint('model.hdf5', save_best_only=True, verbose=0)\n\nfilters = 32\nreg = .0005\n\nmodel = Sequential()\n\nfor i in range(5):\n    model.add(Conv2D(filters, 3, kernel_regularizer=l2(reg), input_shape=(SIZE, SIZE, 3)))\n    model.add(LeakyReLU())\n    \n    model.add(Conv2D(filters, 3, kernel_regularizer=l2(reg)))\n    model.add(LeakyReLU())\n    \n    if i != 4:\n        model.add(Conv2D(filters, 5, kernel_regularizer=l2(reg)))\n        model.add(LeakyReLU())\n        \n    model.add(MaxPooling2D())\n    model.add(Dropout(0.5))\n    model.add(BatchNormalization())\n\n    filters *= 2\n\nmodel.add(Flatten())\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\nmodel.add(Dense(4, activation='softmax'))\n\nmodel.summary()","ed4c2c76":"from keras.preprocessing.image import ImageDataGenerator\n\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['acc']\n)\n\nimagegen = ImageDataGenerator(\n    rotation_range=20,\n    zoom_range=0.2,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    horizontal_flip=True,\n    vertical_flip=True\n)\n\nhistory = model.fit_generator(\n    imagegen.flow(x_train, y_train, batch_size=32),\n    epochs=400,\n    steps_per_epoch=x_train.shape[0] \/\/ 32,\n    verbose=0,\n    callbacks=[rlr, es, mc],\n    validation_data=(x_test, y_test)\n)\n# load best model\nmodel = load_model('model.hdf5')","c762f7c4":"from matplotlib import pyplot as plt\n\nh = history.history\n\noffset = 5\nepochs = range(offset, len(h['loss']))\n\nplt.figure(1, figsize=(20, 6))\n\nplt.subplot(121)\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.plot(epochs, h['loss'][offset:], label='train')\nplt.plot(epochs, h['val_loss'][offset:], label='val')\nplt.legend()\n\nplt.subplot(122)\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.plot(h[f'acc'], label='train')\nplt.plot(h[f'val_acc'], label='val')\nplt.legend()\n\nplt.show()","fc47e4b7":"from sklearn.metrics import roc_auc_score\n\npred_test = model.predict(x_test)\nroc_sum = 0\nfor i in range(4):\n    score = roc_auc_score(y_test[:, i], pred_test[:, i])\n    roc_sum += score\n    print(f'{score:.3f}')\n\nroc_sum \/= 4\nprint(f'totally:{roc_sum:.3f}')","6039cd09":"pred = model.predict(test_images)\n\nres = pd.DataFrame()\nres['image_id'] = test_ids\nres['healthy'] = pred[:, 0]\nres['multiple_diseases'] = pred[:, 1]\nres['rust'] = pred[:, 2]\nres['scab'] = pred[:, 3]\nres.to_csv('submission.csv', index=False)\nres.head(40)","e5a734a2":"preparing data for training and training! data augmentation helps to enlarge image set by flipping, moving, zomming etc. \ni will track model's accuracy because i applied imblearn's balancing earlier","8adb6b0d":"finally, predict and save!","a222f8ec":"in this notebook you can see a **starter model** for this competition","d7a99718":"this code performs reading images and resizing them into 224x224","4e0e9e96":"here goes the separation train images into train and validation datasets","d95c5181":"the competition's eval metric is mean of column-wise roc auc so let's check it here","d76faec4":"first of all let's read train and test csv","6fa3d8cb":"it would be better to release the data that won't be used no more","a1ae3a91":"this is a simple convolutional neural network to solve this task. i use three callbacks:\n* learning rate reducing by 0.1 every 10 epochs on plateau\n* early stopping to stop learning after 24 epochs on plateau (with restoring best model)\n* model checkpoint to save best model to file\n\ni also add a l2 regularization to decrease an impact of overfitting","5507e3fd":"here is a history: losses and accuracy","3656e392":"looking at mean, we can see that classes are imbalanced for all 4 target features so i will use oversampling later","ab0b8950":"i use imblearn to perform oversampling"}}