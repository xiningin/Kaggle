{"cell_type":{"ea984b17":"code","6d4dd42b":"code","252f6dae":"code","27a1983f":"code","f6dcef01":"code","5003530b":"code","d296349e":"code","7db32346":"code","bcdf8108":"code","f2a9dd05":"code","afbb7c42":"code","3888939c":"code","87923025":"code","3bd01e90":"code","f9cc650c":"code","1c8aeab0":"code","016858c3":"code","897c7cdc":"code","17ce8431":"code","b595cc03":"code","a8a260bc":"code","7494e3aa":"code","756b7a57":"code","d60a8ead":"code","a3ad6579":"code","b0d25b0f":"code","cf4898fc":"code","74be05c5":"code","a83cced3":"code","ad573185":"code","e9e3c2ef":"code","24763516":"code","f8ecc8c1":"code","a4e05681":"code","8738adab":"code","9bbf8b78":"code","1d99822c":"code","98947a5a":"code","dc553f71":"code","6a5a2db6":"code","859562ad":"code","78a5620b":"code","850d5850":"code","9ef891b7":"code","c60dd4ef":"code","9b0c1d48":"code","fb195d17":"code","f786a5df":"code","a2d7f67f":"code","b1dc01d4":"code","14ecdc03":"code","d938425a":"code","a19bb8a7":"code","8b1c67af":"code","57cb399b":"code","70a3cb11":"code","8fcc8ee1":"code","c692ee8c":"code","cdc30d5c":"code","dd28ba62":"code","801d57a9":"code","79a02a34":"code","b32d29b5":"code","d0d5454d":"code","c8f175f3":"code","f64cefb5":"code","613f74f7":"code","eee393d6":"code","b6aa4233":"code","429fb4a4":"code","4f63c5c5":"code","99e32aab":"code","7ed9ac86":"code","d3a76c26":"code","5e15d402":"code","3dd64faa":"code","01e68457":"code","c660f004":"code","0bea6b74":"code","cf8c5ec1":"code","4200009e":"code","67ca46d2":"code","73c91883":"code","388de28a":"code","22c7793e":"code","fcf22e69":"code","d5e41705":"code","2157752f":"code","88ac966a":"code","5124964b":"code","f7360cce":"code","ff352389":"code","27d81a69":"code","fa32311e":"code","bad3e352":"code","ab05ae6b":"code","090c2e2b":"code","71629517":"code","563c59ef":"code","e7790514":"markdown","ddb859d0":"markdown","6c0b09bc":"markdown","5dbd7984":"markdown","6b0a39e2":"markdown","047b32ef":"markdown","ae3b008a":"markdown","67f0b08a":"markdown","81c3a734":"markdown","5c0c515b":"markdown","64b5f8c9":"markdown","8a9d9450":"markdown","38460429":"markdown","1179bf28":"markdown","ce39bf6f":"markdown","6c124263":"markdown","9fde2e42":"markdown","aa0acf11":"markdown","6950014e":"markdown","5d13537d":"markdown","4c5c00a2":"markdown","5a8040e0":"markdown","be8af7a2":"markdown"},"source":{"ea984b17":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn import metrics\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n","6d4dd42b":"pd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)","252f6dae":"#functions \n#Function to print null values in all columns\ndef nulls(df):\n    return (100*round(df.isnull().sum()\/len(df),4).sort_values(ascending=False))\ndef getvif(df):\n    if 'const' in list(df.columns):\n        df1=df.drop('const', axis=1) \n    else:\n        df1 = df.copy()\n    vif=pd.DataFrame()\n    vif['Features'] = df1.columns\n    vif['VIF'] = [variance_inflation_factor(df1.values, i) for i in range(df1.shape[1])]\n    vif['VIF'] = round(vif.VIF,2)\n    vif = vif.sort_values(by = 'VIF', ascending = False)\n    return vif","27a1983f":"#import dataset\ndf = pd.read_csv('..\/input\/house-prices-data\/train.csv')","f6dcef01":"#inspecting df\ndf.info()","5003530b":"df.head()","d296349e":"#checking null values\nnulls(df)","7db32346":"df.describe()","bcdf8108":"#Let's drop the columns with more than 90% of null values\n#If we impute null values here, the columns will be highly skewed and hence of no use to our model\nnulls_list = nulls(df)\ndf.drop(list(nulls_list.loc[nulls_list>=90].index),axis=1,inplace=True)\nnulls(df)","f2a9dd05":"#Nulls in the Fence column inidcates no fence present in the house\ndf['Fence'].fillna('No Fence',inplace = True)","afbb7c42":"#Nulls in the FireplaceQu column inidcates no fireplace present in the house\ndf['FireplaceQu'].fillna('No Fireplace',inplace = True)","3888939c":"#Nulls in the GarageCond, GarageType, GarageYrBlt, GarageFinish, GarageQual column inidcates no Garage present in the house\ndf['GarageCond'].fillna('No Garage',inplace = True)\ndf['GarageType'].fillna('No Garage',inplace = True)\ndf['GarageYrBlt'].fillna('No Garage',inplace = True)\ndf['GarageQual'].fillna('No Garage',inplace = True)\ndf['GarageFinish'].fillna('No Garage',inplace = True)","87923025":"#Nulls in the BsmtExposure, BsmtFinType2, BsmtFinType1, BsmtCond, BsmtQual column inidcates no basement present in the house\ndf['BsmtExposure'].fillna('No Basement',inplace = True)\ndf['BsmtFinType2'].fillna('No Basement',inplace = True)\ndf['BsmtFinType1'].fillna('No Basement',inplace = True)\ndf['BsmtCond'].fillna('No Basement',inplace = True)\ndf['BsmtQual'].fillna('No Basement',inplace = True)","3bd01e90":"#Nulls in the MasVnrArea, MasVnrType column inidcates no Masonry Veneer present in the house\ndf['MasVnrArea'].fillna(0,inplace = True)\ndf['MasVnrType'].fillna('None',inplace = True)","f9cc650c":"#checking Electrical\ndf.Electrical.value_counts()","1c8aeab0":"#replacing nulls in LotFrontage with 0 (no frontage)\ndf['LotFrontage'].fillna(0,inplace = True)","016858c3":"#replacing nulls in Electrical with mode\ndf['Electrical'].fillna('SBrkr',inplace = True)","897c7cdc":"df.info()","17ce8431":"#Checking YearBuilt to Age\ndf['Age'] = 2020 - df['YearBuilt']","b595cc03":"#dropping the YearBuilt variables since we won't need to use these, age will suffice for our goal\ndf.drop(['YearBuilt','YrSold','YearRemodAdd','GarageYrBlt','MoSold'],axis=1,inplace=True)","a8a260bc":"#'SaleType' would not be available to us while making predicitons (it is part of target variable), hence we will drop it. \ndf.drop(['SaleType'],axis=1,inplace=True)","7494e3aa":"df.info()","756b7a57":"#We can perform label encoding for some cardinal categorical variables, which have an order to them\n\ndf['LandSlope'] = df['LandSlope'].map({'Gtl':3, 'Mod':2,'Sev':1})\ndf['LotShape'] = df['LotShape'].map({'Reg':4, 'IR1':3,'IR2':2,'IR3':1})\ndf['Utilities'] = df['Utilities'].map({'AllPub':4, 'NoSewr':3,'NoSeWa':2,'ELO':1})\ndf['ExterQual'] = df['ExterQual'].map({'Ex':5, 'Gd':4,'TA':3,'Fa':2,'Po':1})\ndf['ExterCond'] = df['ExterCond'].map({'Ex':5, 'Gd':4,'TA':3,'Fa':2,'Po':1})\ndf['BsmtQual'] = df['BsmtQual'].map({'Ex':5, 'Gd':4,'TA':3,'Fa':2,'Po':1,'No Basement':0})\ndf['BsmtCond'] = df['BsmtCond'].map({'Ex':5, 'Gd':4,'TA':3,'Fa':2,'Po':1,'No Basement':0})\ndf['BsmtExposure'] = df['BsmtExposure'].map({'Gd':4,'Av':3,'Mn':2,'No':1,'No Basement':0})\ndf['BsmtFinType1'] = df['BsmtFinType1'].map({'GLQ':6,'ALQ':5, 'BLQ':4,'Rec':3,'LwQ':2,'Unf':1,'No Basement':0})\ndf['BsmtFinType2'] = df['BsmtFinType2'].map({'GLQ':6,'ALQ':5, 'BLQ':4,'Rec':3,'LwQ':2,'Unf':1,'No Basement':0})\ndf['HeatingQC'] = df['HeatingQC'].map({'Ex':5, 'Gd':4,'TA':3,'Fa':2,'Po':1})\ndf['Electrical'] = df['Electrical'].map({'SBrkr':5, 'FuseA':4,'FuseF':3,'FuseP':2,'Mix':1})\ndf['KitchenQual'] = df['KitchenQual'].map({'Ex':5, 'Gd':4,'TA':3,'Fa':2,'Po':1})\ndf['Functional'] = df['Functional'].map({'Typ':8,'Min1':7,'Min2':6,'Mod':5, 'Maj1':4,'Maj2':3,'Sev':2,'Sal':1})\ndf['FireplaceQu'] = df['FireplaceQu'].map({'Ex':5, 'Gd':4,'TA':3,'Fa':2,'Po':1,'No Fireplace':0})\ndf['GarageFinish'] = df['GarageFinish'].map({'Fin':4,'RFn':3,'Unf':2,'No Garage':1})\ndf['GarageQual'] = df['GarageQual'].map({'Ex':5, 'Gd':4,'TA':3,'Fa':2,'Po':1,'No Garage':0})\ndf['GarageCond'] = df['GarageCond'].map({'Ex':5, 'Gd':4,'TA':3,'Fa':2,'Po':1,'No Garage':0})\ndf['PavedDrive'] = df['PavedDrive'].map({'Y':3, 'P':2,'N':1})\ndf['Fence'] = df['Fence'].map({'GdPrv':4,'MnPrv':3,'GdWo':2,'MnWw':1,'No Fence':0})","d60a8ead":"#We have dealt with all the null values in the dataset\nnulls(df)","a3ad6579":"#Changing class to categorical\ndf['MSSubClass'] = df['MSSubClass'].astype('object')","b0d25b0f":"#We have these categorical variables now\ncat_vars = list(set(df.drop('Id',axis=1).columns) - set(df._get_numeric_data().columns))\ncat_vars","cf4898fc":"#Adding convereted cateogrical variables too\nfor each in ['LandSlope','LotShape','Utilities','ExterQual','ExterCond','BsmtQual','BsmtCond','BsmtExposure','BsmtQual','BsmtFinType1','BsmtFinType2','HeatingQC','Electrical','KitchenQual','Functional','FireplaceQu','GarageFinish','GarageQual','GarageCond','PavedDrive','Fence']:\n    cat_vars.append(each)","74be05c5":"len(cat_vars)","a83cced3":"#We will be inspecting all the cateogrical columns now\n\nplt.figure(figsize=(20,8*26))\nfor i,each in enumerate(cat_vars):\n    plt.subplot(26,2,i+1)\n    sns.countplot(y=df[each])","ad573185":"#from the above plots, we can spot some features with a consider amount of skewing in the data.\n#We will analyze how much skewed the data exactly is, and then take action based on that.\n#As a benchmark, features with more than 90% of data belonging to one category can be set as skewed\n#We will find which of our categorical columns can be cateogirzed as skewed\nskewed_cols = []\nfor each in cat_vars:\n    if max(df[each].value_counts(normalize=True)*100) > 90:\n        print(df[each].value_counts(normalize=True)*100)\n        print('\\n')\n        skewed_cols.append(each)","e9e3c2ef":"#We now have the following heavily skewed columns where more than 90% of values belong to a single category only\nskewed_cols","24763516":"#Dropping heavily skewed columns for a cleaner dataset\ndf.drop(skewed_cols, axis=1, inplace=True)\ndf.info()","f8ecc8c1":"#We now look at how the numerical variables are spread\nnum_vars = list(df.drop('Id',axis=1)._get_numeric_data().columns)\nnum_vars","a4e05681":"len(num_vars)","8738adab":"#We will be inspecting all the numerical columns now\n\nplt.figure(figsize=(20,8*22))\nfor i,each in enumerate(num_vars):\n    plt.subplot(27,2,i+1)\n    sns.boxplot(x=df[each])","9bbf8b78":"#Certain features are highly skewed here. We can drop them to simplify our model\n#before dropping, we look at their actual spread\ni=1\nplt.figure(figsize=(20,30))\nfor each in ['LowQualFinSF','ExterCond','BsmtCond','BsmtFinType2','BsmtFinSF2','BsmtHalfBath','KitchenAbvGr','GarageQual','PoolArea','Fence','EnclosedPorch','ScreenPorch','MiscVal','3SsnPorch']:\n    plt.subplot(4,4,i)\n    sns.distplot(df[each])\n    i+=1","1d99822c":"#Dropping more skewed features\ndf.drop(['LowQualFinSF','ExterCond','BsmtCond','BsmtFinType2','BsmtFinSF2','BsmtHalfBath','KitchenAbvGr','GarageQual','PoolArea','Fence','EnclosedPorch','ScreenPorch','MiscVal','3SsnPorch'],axis=1,inplace=True)","98947a5a":"#let's see how sale price is varying across overall quality rating of the house\ndata = pd.concat([df['SalePrice'], df['OverallQual']], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x='OverallQual', y=\"SalePrice\", data=data)\n# fig.axis(ymin=0, ymax=800000);","dc553f71":"#Sale price also is related to age of house\n#high age does not necessarily mean that the price would be low, but a general slightly downward trend is seen\ndata = pd.concat([df['SalePrice'], df['Age']], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x='Age', y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=90);","6a5a2db6":"#We can see how some of the important looking features have an impact on each other\nsns.pairplot(df[['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'Age']], size = 2.5)\nplt.show()","859562ad":"#target variable\nsns.distplot(df['SalePrice'])","78a5620b":"#We note that the target (dependent) variable does not have a purely normal distribution and there some skewness.","850d5850":"#The variable is quite skewed, and all the values are positive (boxcox can be used here)\ndf.SalePrice.describe()","9ef891b7":"#There is a high degree of skew in this column\ndf.SalePrice.skew()","c60dd4ef":"#We try log transformation\nfrom math import log\nsns.distplot(df.SalePrice.apply(lambda x: log(x)))","9b0c1d48":"#We are able to get the skew down to a great extent by using log transform\ndf.SalePrice.apply(lambda x: log(x)).skew()","fb195d17":"#We can use Box-Cox Transform to reduce the skew further and make the data resemble a normal distribution\nfrom scipy import stats as ss\ndf['SalePrice'] = pd.Series(ss.boxcox(df.SalePrice)[0])\ndf.SalePrice.skew()","f786a5df":"#We can see that the distribution is much closer to normal now, hence we can fit a linear regression model effectively\nsns.distplot(df.SalePrice)","a2d7f67f":"#Updating list of categorical variables\ncat_vars = list(set(df.drop('Id',axis=1).columns) - set(df._get_numeric_data().columns))\ncat_vars","b1dc01d4":"#creating dataframe with dummy variables\ndummy = pd.get_dummies(df[cat_vars], drop_first = True)\ndummy.head()","14ecdc03":"#merging dummy variables into original dataframe\ndf = pd.concat([df,dummy],axis = 1)\ndf.info()","d938425a":"#dropping original categorical variables that have been dummified\ndf.drop(cat_vars,axis=1,inplace=True)\ndf.shape","a19bb8a7":"from sklearn.model_selection import train_test_split","8b1c67af":"x = df.drop(['SalePrice','Id'], axis = 1)\ny = df[['SalePrice']]\nx_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.75, random_state = 44)","57cb399b":"x_train.shape","70a3cb11":"x_test.shape","8fcc8ee1":"y_train.shape","c692ee8c":"y_test.shape","cdc30d5c":"#To be able to interpret the final model, and for a faster convergence, we will scale our features\n#We have chosen MinMax Scaler for the process","dd28ba62":"from sklearn.preprocessing import MinMaxScaler","801d57a9":"scaler = MinMaxScaler() #initializing minmaxscaler\nnum_vars = list(set(num_vars) - set(['KitchenAbvGr', 'ScreenPorch', 'Fence', 'BsmtHalfBath', '3SsnPorch', 'PoolArea', 'BsmtCond', 'EnclosedPorch', 'LowQualFinSF', 'MiscVal', 'GarageQual', 'BsmtFinSF2', 'BsmtFinType2', 'ExterCond','SalePrice'])) #removing saleprice since it is not in x_train now","79a02a34":"#We will be scaling only the orignal continous variables. Dummy variables can be used as-is.\nx_train[list(num_vars)].describe()","b32d29b5":"#fitting the scaler on train set and transforming variables\nx_train[num_vars] = scaler.fit_transform(x_train[num_vars])\nx_train[num_vars].describe()","d0d5454d":"#Scaling features in test set using the scaler fitted on train set\nx_test[num_vars] = scaler.transform(x_test[num_vars])","c8f175f3":"from sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nfrom sklearn.feature_selection import RFE\nrfe = RFE(linreg, 30)             # running RFE with 30 variables as output\nrfe = rfe.fit(x_train, y_train)\nprint(rfe.support_)","f64cefb5":"#columns chosen by running RFE\ncols=x_train.columns[rfe.support_]\ncols","613f74f7":"import statsmodels.api as sm\nx_train_rfe = sm.add_constant(x_train[list(cols)])","eee393d6":"#Building a basic model with the selected features to gauge performance\n\nlm = sm.OLS(y_train, x_train_rfe).fit()\nprint(lm.summary())","b6aa4233":"#List of parameters (alpha values) to tune\nparams = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, \n 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000 ]}\n\n\nridge = Ridge()\n%time\n# cross validation\nfolds = KFold(n_splits = 5, shuffle = True, random_state = 1)\nmodel_cv = GridSearchCV(estimator = ridge, \n                        param_grid = params, \n                        scoring= 'r2', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \nmodel_cv.fit(x_train, y_train) ","429fb4a4":"cv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results.head()","4f63c5c5":"#this is the best value of alpha identified by grid search cv\nmodel_cv.best_params_","99e32aab":"#this is the best score obtained by grid search on the validation set\nmodel_cv.best_score_","7ed9ac86":"# plotting mean test and train scoes with alpha \ncv_results['param_alpha'] = cv_results['param_alpha'].astype('int32')\nplt.figure(figsize=(16,10))\n\n# plotting\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('R2 Score')\nplt.title(\"R2 Score Against Alpha - Ridge Regression\")\nplt.legend(['train score', 'validation score'], loc='upper right')\nplt.xscale('log')\nplt.show()","d3a76c26":"#Selecting the optimum value of alpha as 4.0\nridge = Ridge(alpha = 4.0)\nridge.fit(x_train, y_train)\n#predict\ny_train_pred = ridge.predict(x_train)\ny_test_pred = ridge.predict(x_test)\n#Checking r2 score on train\nprint('r2 score for Train set')\nprint(metrics.r2_score(y_true = y_train, y_pred = y_train_pred))\nprint('r2 score for Test set')\nprint(metrics.r2_score(y_true = y_test, y_pred = y_test_pred))\n","5e15d402":"# ridge model parameters\nmodel_parameters = list(ridge.coef_)\nmodel_parameters.insert(0, ridge.intercept_)\nmodel_parameters = [round(x, 3) for x in model_parameters[1]]\ncols = x_train.columns\ncols = cols.insert(0, \"constant\")\nridge_f = list(zip(cols, model_parameters))\nridge_f","3dd64faa":"d = {'Feature':list(list(zip(*ridge_f))[0]),'Coeff':list(list(zip(*ridge_f))[1])}\nridge_params = pd.DataFrame(data = d)","01e68457":"#Let's see what is the importance given to each feature by our model\nridge_params.reindex(ridge_params.Coeff.abs().sort_values(ascending = False).index)","c660f004":"#List of parameters (alpha values) to tune\nparams = {'alpha': [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008, 0.0009, 0.001, 0.01, 0.05, 0.1, \n 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000 ]}\n\n\nlasso = Lasso()\n%time\n# cross validation\nfolds = KFold(n_splits = 5, shuffle = True, random_state = 42)\nmodel_cv = GridSearchCV(estimator = lasso, \n                        param_grid = params, \n                        scoring= 'r2', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \nmodel_cv.fit(x_train, y_train) ","0bea6b74":"cv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results.head(50)","cf8c5ec1":"model_cv.best_params_","4200009e":"model_cv.best_score_","67ca46d2":"# plotting mean test and train scoes with alpha \ncv_results['param_alpha'] = cv_results['param_alpha'].astype('float64')\nplt.figure(figsize=(16,10))\n\n# plotting\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('R2 Score')\nplt.title(\"R2 Score Against Alpha - Lasso Regression\")\nplt.legend(['train score', 'validation score'], loc='upper right')\n# plt.xscale('log')\nplt.xlim(0.0001,0.05)\nplt.show()\n","73c91883":"#Selecting the optimum value of alpha as 0.0003\nlasso = Lasso(alpha = 0.0003)\nlasso.fit(x_train, y_train)\n#predict\ny_train_pred = lasso.predict(x_train)\ny_test_pred = lasso.predict(x_test)\n#Checking r2 score on train\nprint('r2 score for Train set')\nprint(metrics.r2_score(y_true = y_train, y_pred = y_train_pred))\nprint('r2 score for Test set')\nprint(metrics.r2_score(y_true = y_test, y_pred = y_test_pred))\n","388de28a":"# lasso model parameters\nmodel_parameters = list(lasso.coef_)\nmodel_parameters.insert(0, lasso.intercept_)\nmodel_parameters = [round(x, 3) for x in model_parameters[1:]]\ncols = x_train.columns\ncols = cols.insert(0, \"constant\")\nlasso_f = list(zip(cols, model_parameters))\nlasso_f","22c7793e":"d = {'Feature':list(list(zip(*lasso_f))[0]),'Coeff':list(list(zip(*lasso_f))[1])}\nlasso_params = pd.DataFrame(data = d)","fcf22e69":"#Let's see what is the importance given to each feature by our model\nlasso_params.reindex(lasso_params.Coeff.abs().sort_values(ascending = False).index)","d5e41705":"#Doubling the alpha for ridge to 8.0\nridge = Ridge(alpha = 8.0)\nridge.fit(x_train, y_train)\n#predict\ny_train_pred = ridge.predict(x_train)\ny_test_pred = ridge.predict(x_test)\n#Checking r2 score on train\nprint('r2 score for Train set')\nprint(metrics.r2_score(y_true = y_train, y_pred = y_train_pred))\nprint('r2 score for Test set')\nprint(metrics.r2_score(y_true = y_test, y_pred = y_test_pred))","2157752f":"#Ridge Model Parameters\nmodel_parameters = list(ridge.coef_)\nmodel_parameters.insert(0, ridge.intercept_)\nmodel_parameters = [round(x, 3) for x in model_parameters[1]]\ncols = x_train.columns\ncols = cols.insert(0, \"constant\")\nridge_f = list(zip(cols, model_parameters))\n#Let's see what is the importance given to each feature by our model\npd.DataFrame(data = {'Feature':list(list(zip(*ridge_f))[0]),'Coeff':list(list(zip(*ridge_f))[1])}).reindex(ridge_params.Coeff.abs().sort_values(ascending = False).index)","88ac966a":"#Doubling the alpha for lasso to 0.0006\nlasso = Lasso(alpha = 0.0006)\nlasso.fit(x_train, y_train)\n#predict\ny_train_pred = lasso.predict(x_train)\ny_test_pred = lasso.predict(x_test)\n#Checking r2 score on train\nprint('r2 score for Train set')\nprint(metrics.r2_score(y_true = y_train, y_pred = y_train_pred))\nprint('r2 score for Test set')\nprint(metrics.r2_score(y_true = y_test, y_pred = y_test_pred))\n","5124964b":"#Lasso Model Parameters\nmodel_parameters = list(lasso.coef_)\nmodel_parameters.insert(0, lasso.intercept_)\nmodel_parameters = [round(x, 3) for x in model_parameters[1:]]\ncols = x_train.columns\ncols = cols.insert(0, \"constant\")\nlasso_f = list(zip(cols, model_parameters))\n#Let's see what is the importance given to each feature by our model\nd = {'Feature':list(list(zip(*lasso_f))[0]),'Coeff':list(list(zip(*lasso_f))[1])}\nlasso_params = pd.DataFrame(data = d)\nlasso_params.reindex(lasso_params.Coeff.abs().sort_values(ascending = False).index)","f7360cce":"#List of parameters (alpha values) to tune\nparams = {'alpha': [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008, 0.0009, 0.001, 0.01, 0.05, 0.1, \n 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000 ]}\n\n\nlasso = Lasso()\n%time\n# cross validation\nfolds = KFold(n_splits = 5, shuffle = True, random_state = 42)\nmodel_cv = GridSearchCV(estimator = lasso, \n                        param_grid = params, \n                        scoring= 'r2', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \nmodel_cv.fit(x_train.drop(['2ndFlrSF','LotShape','OverallQual','ExterQual','GarageFinish'],axis=1), y_train) #top features removed","ff352389":"cv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results.head()","27d81a69":"model_cv.best_params_","fa32311e":"model_cv.best_score_","bad3e352":"# plotting mean test and train scoes with alpha \ncv_results['param_alpha'] = cv_results['param_alpha'].astype('float64')\nplt.figure(figsize=(16,10))\n\n# plotting\nplt.plot(cv_results.loc[cv_results['param_alpha']<=1.0]['param_alpha'], cv_results.loc[cv_results['param_alpha']<=1.0]['mean_train_score'])\nplt.plot(cv_results.loc[cv_results['param_alpha']<=1.0]['param_alpha'], cv_results.loc[cv_results['param_alpha']<=1.0]['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('R2 Score')\nplt.title(\"R2 Score Against Alpha - Lasso Regression\")\nplt.legend(['train score', 'validation score'], loc='upper right')\nplt.xlim(0.0001,0.05)\n# plt.xscale('log')\nplt.show()","ab05ae6b":"#Selecting the optimum value of alpha as 0.0003\nlasso = Lasso(alpha = 0.0003)\nlasso.fit(x_train.drop(['2ndFlrSF','LotShape','OverallQual','ExterQual','GarageFinish'],axis=1), y_train)\n#predict\ny_train_pred = lasso.predict(x_train.drop(['2ndFlrSF','LotShape','OverallQual','ExterQual','GarageFinish'],axis=1))\ny_test_pred = lasso.predict(x_test.drop(['2ndFlrSF','LotShape','OverallQual','ExterQual','GarageFinish'],axis=1))\n#Checking r2 score on train\nprint('r2 score for Train set')\nprint(metrics.r2_score(y_true = y_train, y_pred = y_train_pred))\nprint('r2 score for Test set')\nprint(metrics.r2_score(y_true = y_test, y_pred = y_test_pred))\n","090c2e2b":"# lasso model parameters\nmodel_parameters = list(lasso.coef_)\nmodel_parameters.insert(0, lasso.intercept_)\nmodel_parameters = [round(x, 3) for x in model_parameters[1:]]\ncols = x_train.drop(['2ndFlrSF','LotShape','OverallQual','ExterQual','GarageFinish'],axis=1).columns\ncols = cols.insert(0, \"constant\")\nlasso_f = list(zip(cols, model_parameters))\nlasso_f","71629517":"d = {'Feature':list(list(zip(*lasso_f))[0]),'Coeff':list(list(zip(*lasso_f))[1])}\nlasso_params = pd.DataFrame(data = d)","563c59ef":"#Let's see what is the importance given to each feature by our model\nlasso_params.reindex(lasso_params.Coeff.abs().sort_values(ascending = False).index)","e7790514":"We will first begin by running Recursive Feature Elimination to determine the top 30 most important features by fitting a linear regression model on the data.","ddb859d0":"## EDA and Missing Value Treatment","6c0b09bc":"**Problem Statement**\n\nA US-based housing company named Surprise Housing has decided to enter the Australian market. The company uses data analytics to purchase houses at a price below their actual values and flip them on at a higher price. For the same purpose, the company has collected a data set from the sale of houses in Australia. The data is provided in the CSV file below.\n\n \n\nThe company is looking at prospective properties to buy to enter the market. You are required to build a regression model using regularisation in order to predict the actual value of the prospective properties and decide whether to invest in them or not.\n\n \n\nThe company wants to know:\n\nWhich variables are significant in predicting the price of a house, and\n\nHow well those variables describe the price of a house.","5dbd7984":"We see a dip in the train r-squared for both the models, for both train set and test set. This is to be expected since the model is now more regularized and the fit is less tighter than it was before.","6b0a39e2":"#### Lasso model explained\n* Using Lasso regression to regularize our parameters, we were able to determine the value of alpha (hyperparameter) to be optimal at 0.0003\n* Lasso has an added benefit of performing feature selection. \n* Upon choosing this value, our model performs at a validation set r-squared of 0.87 during 5-fold cross validation. \n* We obtained a test data r2 value of 0.89 on fitting the model on training data (which gave train data r-squared of 0.90, indicating a good stable model)\n* The most important features can be observed in the sorted dataframe in the above cell.\n    * 2ndFloorSF - The are of the second floor\n    * Lot Shape - The shape of the lot regular\/irregular etc. The more it is towards regular, higher is the price\n    * OverallQual - High rating (9\/10) for the overall quality tends to bump the price up. \n    * ExterQual - The quality of material with which the exterior of house is buuilt\n    * GarageFinish - Interiror finish of the garage\n    * Some neighborhoods fetch better prices for the house\n        * North Ridge\n        * College Creek\n        * Wayer West\n    * Sales which are made as 'partial' will bring the price down considerably. \n    ","047b32ef":"## Getting dummy variables for categorical features","ae3b008a":"#### Ridge Model Explained\n* Using ridge regression to regularize our parameters, we were able to determine the value of alpha (hyperparameter) to be optimal at 4.0\n* Upon choosing this value, our model performs at a train set best r-squared of 0.87 during 5-fold cross validation. \n* We obtained a test data r2 value of 0.89 on fitting the model on training data (which gave train data r-squared of 0.90, indicating a good stable model)\n* The important features can be observed in the sorted dataframe in the above cell.\n    * OverallQual - High rating for the overall quality tends to bump the price up\n    * LotShape - Regularly shaped lots would fetch a higher price for the house\n    * 2nd\/1st Floor Area - The area of both floors contribute to the price of the house.\n    * HeatingQC - Houses equipped with better heating fetch better prices\n    * GarageFinish - Better finish in the garage causes bump in price. \n    * KitchenQual - A hgher quality kitchen adds to the price\n    * Basements that contain a full bath add value a lot.\n    * High density residental zones are a major selling point\n    * The quality of material used on the exterior is also being observed to impact the cost considerably.\n    * There are certain neighborhood which are more preferred as well: \n        * Somerset\n        * College Creek\n        * North Ridge","67f0b08a":"To satisfy the assumptions of linear regression, the target variable (dependent variable) must be normally distributed. We can see in the distplot below that ours is highly skewed towards the right. We will now deal with this problem to get better performance","81c3a734":"## Lasso Regression","5c0c515b":"## Re-Run Lasso with Reduced set of input variables","64b5f8c9":"## Understanding Alpha","8a9d9450":"#### Importing libraries","38460429":"## RFE","1179bf28":"## Transforming the dependent variable to a normal distribution","ce39bf6f":"These models can be further tuned using feature selection techniques like Forward, Backward and Stepwise. ","6c124263":"* The top 5 features from the orignal Lasso model were ['2ndFlrSF','LotShape','OverallQual','ExterQual','GarageFinish'].\n* These will be removed from the training (and test) set, and the model will be re-tuned and re-evaluated.","9fde2e42":"## Ridge Regression","aa0acf11":"## Scaling of numerical features","6950014e":"## Building the Model","5d13537d":"## Test-Train split","4c5c00a2":"### Doubling the values of Alpha","5a8040e0":"* We find there is a considerable amount of variance in the data\n* We choose not to perform outlier value treatment, since these features are what might be impacting prices of the house","be8af7a2":"Now we can choose to proceed with the set of features identified by RFE, but we can also take the route of directly going to ridge and lasso techniques. We can let the lasso model take care of feature selection as well. "}}