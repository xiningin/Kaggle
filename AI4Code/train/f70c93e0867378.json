{"cell_type":{"3be52e8c":"code","22216cfb":"code","f6e6c53c":"code","7cf45db8":"code","f9106445":"code","9cbbc35c":"code","323110ec":"code","461c38ae":"code","2ef582ca":"code","1a5c9e42":"code","eca4fb8b":"code","f106e167":"code","055d58c3":"code","9376fc32":"code","a65ad913":"code","0aafb7f2":"code","5c695fa3":"code","12190cd7":"code","d3e2a098":"code","a1ef3d9a":"code","a2c62ea2":"code","aa5f5597":"code","185f16c1":"code","75a20d28":"code","ac463917":"code","3e08ffba":"code","8092304e":"code","3163aef3":"code","3bc9596e":"code","e5c28049":"code","eebfe622":"code","0b6ed37e":"code","7f26fa9c":"code","4d5c0c5f":"code","a2e69fc4":"markdown","8c342f3a":"markdown","6a08226d":"markdown","d5013532":"markdown","ad435a57":"markdown","e998eade":"markdown","468a5b4f":"markdown","3e10bab3":"markdown","a108cec7":"markdown","3add1e20":"markdown","1c5c1b8f":"markdown","81d145b3":"markdown","cf4abcf3":"markdown","9d5a0e70":"markdown","72cc1c37":"markdown","a50d2470":"markdown","3741fdc7":"markdown","4c2535d8":"markdown","e482e386":"markdown","2a60fb30":"markdown","832fe9c2":"markdown","663fff3f":"markdown","a820ea52":"markdown","f0f26446":"markdown","198d6740":"markdown","bac28ff5":"markdown","f488d2fd":"markdown","26ce0eeb":"markdown","401630c8":"markdown","c2fa4fd4":"markdown","7808f59f":"markdown","143daf64":"markdown","364df49e":"markdown","8df588e9":"markdown","c39ddb52":"markdown","7ee043b2":"markdown","a20351c7":"markdown"},"source":{"3be52e8c":"# Libraries needed\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","22216cfb":"# Importing the train and test datasets\ntrain = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/train.csv\", parse_dates=['date'])\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/test.csv\", parse_dates=['date'])\n\n# Obtaining the full dataset with a label train\/test\nfull_df = pd.concat([train.assign(dataset='train'), test.assign(dataset='test')])\n\n# Notice we get NaN values for the unknown sales of the test dataset\nfull_df","f6e6c53c":"print('Countries:', full_df['country'].unique().tolist())\nprint('Stores:', full_df['store'].unique().tolist())\nprint('Products:', full_df['product'].unique().tolist())","7cf45db8":"full_df['year'] = full_df['date'].apply(lambda date: date.year) \nfull_df['year'].unique()","f9106445":"# Array of all days from the first day of 2015 to the last day of 2019\ndays_range = pd.date_range(start='2015-01-01', end='2019-12-31')\n\n# Is this equal to the (unique) dates from our date column?\n(days_range != full_df['date'].unique()).sum()\n\n# Amount of falses is zero:","9cbbc35c":"gdp = pd.read_csv('..\/input\/gdp-per-capita-finland-norway-sweden-201519\/GDP_per_capita_2015_to_2019_Finland_Norway_Sweden.csv')\ngdp.columns = ['year', 'GDP_Finland', 'GDP_Norway', 'GDP_Sweden']\ngdp","323110ec":"plt.figure(figsize=(8,3))\nsns.countplot(data=full_df, x='year', hue='dataset', palette='Set2')\nplt.legend(title='dataset', bbox_to_anchor=(1.05, 1));","461c38ae":"plt.figure(figsize=(15,6))\nsns.lineplot(data=full_df[full_df['year']<2019], x='date', y='num_sold', hue='product');","2ef582ca":"plt.figure(figsize=(15,6))\nsns.lineplot(data=full_df[full_df['year']<2019], x='date', y='num_sold', hue='store', palette='hls');","1a5c9e42":"plt.figure(figsize=(15,6))\nsns.lineplot(data=full_df[full_df['year']<2019], x='date', y='num_sold', hue='country', palette='Set1');","eca4fb8b":"full_df['month'] = full_df['date'].dt.month\nfull_df['week'] = full_df['date'].dt.isocalendar().week\nfull_df['week'][full_df['week']>52] = 52\nfull_df['day'] = full_df['date'].dt.day\nfull_df['dayofweek'] = full_df['date'].dt.dayofweek\n\nfull_df.head()","f106e167":"rows_example = full_df.iloc[:2]\nrows_example","055d58c3":"# Features we want to interact between each other\nfeatures_example = ['country', 'product', 'month']\n\n# For the two selected rows, join their respective values\ninteractions = rows_example[features_example].apply(lambda row: '_'.join(row.astype(str)), axis=1)\ninteractions","9376fc32":"# One-hot encode them\npd.get_dummies(interactions)","a65ad913":"pd.concat((rows_example[features_example], pd.get_dummies(interactions)), axis=1)","0aafb7f2":"desired_interactions = [\n        ['country', 'product', 'month'],\n        ['country', 'product', 'week'],\n        ['country', 'store', 'week'],\n        ['country', 'product', 'month', 'day'],\n        ]\n\nfor interaction in desired_interactions:\n    \n    interaction_features = full_df[interaction].apply(lambda row: '_'.join(row.astype(str)), axis=1)\n    interaction_features = pd.get_dummies(interaction_features)\n    \n    full_df = pd.concat((full_df, interaction_features), axis=1)","5c695fa3":"full_df[['country', 'product', 'month', 'week', 'Finland_Kaggle Hat_1']].head()","12190cd7":"gdp","d3e2a098":"# Function to get the GDP value from the dataset\ndef get_gdp(country_from_row, year, country):\n    \n    if country_from_row == country:\n        var_name = 'GDP_' + country\n        return gdp[gdp['year']==year][var_name].values[0]\n    \n    else: \n        return 0","a1ef3d9a":"# Example \n# If the row's country matches the country we choose, return the GDP \nprint( get_gdp('Sweden', 2018, 'Sweden') )\n\n# Else return 0\nprint( get_gdp('Finland', 2019, 'Sweden') )","a2c62ea2":"# Creating the columns with the corresponding GDP values\n\nfull_df['GDP_Finland'] = np.vectorize(get_gdp, otypes=['float'])(\n    full_df['country'], full_df['year'], 'Finland' )\n\nfull_df['GDP_Norway'] = np.vectorize(get_gdp, otypes=['float'])(\n    full_df['country'], full_df['year'], 'Norway' )\n\nfull_df['GDP_Sweden'] = np.vectorize(\n    get_gdp, otypes=['float'])(\n    full_df['country'], full_df['year'], 'Sweden' )","aa5f5597":"# Random example\n\nfull_df[['country', 'year', 'GDP_Finland', 'GDP_Norway', 'GDP_Sweden']].iloc[[2000, 15000, 25000]]","185f16c1":"full_df.drop(['date', 'dataset'], axis=1, inplace=True)\nfull_df.head()","75a20d28":"# List of categorical features\ncat_features = ['country', 'store', 'product', 'month', 'week', 'day', 'dayofweek']\n\n# Convert all of them to strings just in case\nfull_df[cat_features] = full_df[cat_features].astype(str)\n\n# Getting dummies for the selected columns\nfull_df = pd.get_dummies(full_df)\n\nfull_df.head()","ac463917":"X_train = full_df[full_df['year']<2018].drop(['row_id', 'num_sold'], axis=1)\ny_train = full_df[full_df['year']<2018]['num_sold']\n\nX_test = full_df[full_df['year']==2018].drop(['row_id', 'num_sold'], axis=1)\ny_test = full_df[full_df['year']==2018]['num_sold']","3e08ffba":"def SMAPE(y_true, y_pred):\n    # Reference  https:\/\/www.kaggle.com\/cpmpml\/smape-weirdness\n    denominator = (y_true + np.abs(y_pred)) \/ 200.0\n    diff = np.abs(y_true - y_pred) \/ denominator\n    diff[denominator == 0] = 0.0\n    return np.mean(diff)","8092304e":"from sklearn.linear_model import LinearRegression","3163aef3":"# Creating an instance of a Linear Regression\nmodel = LinearRegression()\n\n# Training the model\nmodel.fit(X_train, np.log(y_train))","3bc9596e":"# Computing the predicted values from X_train\ntrain_predictions = model.predict(X_train)\n\n# Remember to inverse-transform via the exp function\ntrain_predictions = np.exp(train_predictions)\n\n# Calculating the SMAPE\nSMAPE(y_train, train_predictions)","e5c28049":"test_predictions = model.predict(X_test)\ntest_predictions = np.exp(test_predictions)\n\nSMAPE(y_test, test_predictions)","eebfe622":"X_full_train = full_df[full_df['year']<2019].drop(['row_id', 'num_sold'], axis=1)\ny_full_train = full_df[full_df['year']<2019]['num_sold']","0b6ed37e":"model = LinearRegression()\nmodel.fit(X_full_train, np.log(y_full_train))","7f26fa9c":"# Grabbing the rows from 2019\nX_competition = full_df[full_df['year']==2019].drop(['row_id', 'num_sold'], axis=1)\n\n# Calculating our predictions\nfinal_predictions = np.exp(model.predict(X_competition))\n\n# Formatting for the submission, including the row_id\nX_competition['num_sold'] = final_predictions\nX_competition['row_id'] = full_df[full_df['year']==2019]['row_id']\nX_competition = X_competition[['row_id', 'num_sold']]\n\nX_competition","4d5c0c5f":"X_competition.to_csv('submission.csv', index=False)","a2e69fc4":"# Introduction","8c342f3a":"We should also check how the sales differ from one country to another.","6a08226d":"From the above graph, we can clearly infer that there's a certain sales seasonality, and that we can order the amounf of sales as Hat > Mug > Sticker.","d5013532":"In other words, since the first row satisfies having Finland as the country, Kaggle Mug as the product and month equal to 1 (January), it gets a 1 (one) in the 'Finland_Kaggle Mug_1' column, but a 0 in the Kaggle Hat column, whereas the second row gets a 1 there.","ad435a57":"We also drop the 'date' and 'dataset' columns as we won't use them anymore.","e998eade":"# Model and Evaluation","468a5b4f":"# Exploratory Data Analysis","3e10bab3":"So it seems that KaggleRama sells more products than KaggleMart.","a108cec7":"It should be noted that the dates are already sorted and include all days from 2015 to 2019. To confirm this, we can generate a timestamp for every day in these years with pandas, and then compare it to our date columns.","3add1e20":"First, let's make sure that the model actually got close to the very y_train.","1c5c1b8f":"# Feature Engineering","81d145b3":"In this notebook, I'll explore the TPS Jan 2022 dataset in order to forecast the sales of certain products from two stores. Several feature engineering techniques are reviewed (especially the interaction betweeen features performed in this [notebook](https:\/\/www.kaggle.com\/lucamassaron\/kaggle-merchandise-eda-with-baseline-linear-model\/notebook) by [Luca Massaron](https:\/\/www.kaggle.com\/lucamassaron)).","cf4abcf3":"If we select the rows' features we're interested in, and concat them to the above frame, we get something like this:","9d5a0e70":"In order to be sure which years we're working on, let's create a 'year' column from the date timestamps.","72cc1c37":"We now evaluate on 2018.","a50d2470":"We will construct a linear multiplicative model. Thus, we will import the LinearRegression model from scikit learn, and we will also transform the label 'y' via the natural logarithm. ","3741fdc7":"That's all. Thank you for reading.","4c2535d8":"Let's quickly confirm what countries, store and products belong to this dataset.","e482e386":"Next, we want to create interactions between features. To understand how this works, first pick up a couple of rows as an example:","2a60fb30":"This isn't too bad, so we retrain the model including 2018 and make our predictions on 2019 for the competition.","832fe9c2":"We will now add the GDP values of each country. Remember we had the GDP dataset:","663fff3f":"To evaluate the model, we consider the SMAPE measure.","a820ea52":"Now, to the real work, we will create four interactions and perform the process on the full dataframe as follows. \n(Also, keep in mind that too many interactions can result in overfit when training the model, as I found out when I was testing this, as well as an insanely long time training the model.)","f0f26446":"From the above result, we see that it would be a good idea to train our model on the years 2015, 2016 and 2017 and then evaluate it on 2018 as a hold-out test set.","198d6740":"We see that it's pretty much Norway > Sweden > Finland, regarding the amount of sales by country. ","bac28ff5":"It should be noted that it may appear that we get columns with the same name, because the month and week values can overlap as in the following example (first month (1) plus first week (1)). But this isn't directly a problem so we leave it as is. ","f488d2fd":"Say we want to create interactions between three features: country, product and month. For every row, we will join their (string) values along the mentioned columns (features).","26ce0eeb":"Let's first see what the sales look like between the three available products.","401630c8":"First, we want to make sure which years correspond to the train dataset, and which years to the test dataset.","c2fa4fd4":"Finally, we can get our dataset ready for our model to be trained. Remember that we first train the model on the years 2015, 2016 and 2017, so that we can evaluate on 2018.","7808f59f":"We're also provided with the GDP of the three countries during these years. In previous versions, I used the GDP values provided by [Carl McBride](https:\/\/www.kaggle.com\/carlmcbrideellis\/gdp-20152019-finland-norway-and-sweden). \n\nIn this version, however, I use the GDP per capita dataset by [Samuel Cortinhas](https:\/\/www.kaggle.com\/samuelcortinhas\/gdp-per-capita-finland-norway-sweden-201519), as it has improved the score by a bit.","143daf64":"Since the seasonily is important, we should get as much information as possible from the date column, such as year (which we already did), month, week, day of the month and day of the week.","364df49e":"Now we will perform one-hot encoding for the categorical features, namely: country, store, product, month, week, day and dayofweek. Note that although these last features are stored as integers, they should be treated as categorical since they don't actually follow a certain hierarchical order. Thus, we will convert them to strings and then get the dummies of these categorical features from the entire dataframe.","8df588e9":"We will create three new columns 'GDP_Finland', 'GDP_Norway', 'GDP_Sweden\" such that their values will be either zero or the corresponding GDP value if the country and the year match with those of the row.","c39ddb52":"At last, the submission!","7ee043b2":"# Libraries and data","a20351c7":"Also, we should check if a store sells more than the other two."}}