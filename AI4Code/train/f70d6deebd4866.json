{"cell_type":{"8b3996e3":"code","bb0107be":"code","4a57e0ee":"code","ca394cc7":"code","628fb859":"code","2a58190a":"code","f90753d2":"code","81b640e1":"code","58797e99":"code","25b1de8f":"code","4e976b66":"code","2a3be460":"code","424aeab5":"code","d8abbc47":"code","faa9d2f7":"code","8077bdc6":"code","c0be788b":"code","2162dd36":"code","81061dee":"code","06cb67f0":"code","0a134c83":"markdown","5245dede":"markdown","4e617a24":"markdown","4390ce53":"markdown","ade89fd7":"markdown","8d9e30fc":"markdown","c6efffeb":"markdown","9d572eaa":"markdown","58c9ef63":"markdown","b49dfb8c":"markdown","b3095262":"markdown","d490c2f0":"markdown","4e6078e5":"markdown","859532da":"markdown","77a3694c":"markdown"},"source":{"8b3996e3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","bb0107be":"import pandas as pd\ndf1 = pd.read_csv('\/kaggle\/input\/weather-dataset-rattle-package\/weatherAUS.csv')","4a57e0ee":"df1.head()","ca394cc7":"df1.describe()","628fb859":"df1['RainToday'].replace({'No':0,'Yes':1},inplace = True)    \ndf1['RainTomorrow'].replace({'No':0,'Yes':1},inplace = True)   # replacing label's values\ndf = df1.drop(['Date'],axis=1)  # unsignificance feature\ndf.shape","2a58190a":"categorical_features = df.select_dtypes(include = [\"object\"]).columns\ncategorical_features","f90753d2":"df = pd.get_dummies(df,columns=categorical_features,drop_first=True)","81b640e1":"df.isnull().sum(axis=0)","58797e99":"df = df.fillna(df.mean())","25b1de8f":"from sklearn.preprocessing import StandardScaler \n\nscaler = StandardScaler() \n\nscaled = scaler.fit_transform(df) ","4e976b66":"scaled","2a3be460":"from sklearn.decomposition import PCA \n  \npca_model = PCA(n_components = 2) \npca = pca_model.fit_transform(scaled)  ","424aeab5":"variance=np.var(pca,axis=0)\nvariance_ratio = variance\/np.sum(variance)\nprint(variance_ratio)","d8abbc47":"import matplotlib.pyplot as plt\nplt.figure(figsize =(8, 6)) \n  \nplt.scatter(pca[:, 0], pca[:, 1], c = df1['RainTomorrow'], cmap ='plasma') \n  \nplt.xlabel('First Principal Component') \nplt.ylabel('Second Principal Component') ","faa9d2f7":"import seaborn as sns\ndf_comp = pd.DataFrame(pca_model.components_, columns = df.columns)\n  \nplt.figure(figsize =(14, 6)) \n  \nsns.heatmap(df_comp) \n","8077bdc6":"test = df.copy()\ntest = test[\"RainTomorrow\"].values","c0be788b":"from sklearn.model_selection import train_test_split \n\nX_train, X_test, y_train, y_test = train_test_split(pca, test, test_size = 0.25) ","2162dd36":"import xgboost as xgb \nxgb = xgb.XGBClassifier() \nxgb.fit(X_train, y_train) \ny_pred = xgb.predict(X_test) ","81061dee":"from sklearn.metrics import mean_squared_error\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"RMSE: %f\" % (rmse))","06cb67f0":"from sklearn.metrics import accuracy_score\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: %.2f\" % (accuracy * 100.0))","0a134c83":"Let's find categorical variables for label encoding","5245dede":"## Need of PCA\nMachine learning needs a good amount of data to train and test. But having large or good amount of data has its own pitfall, which is curse of dimensionality as huge amount of data can have lots of inconsistencies of features which may increase computation time and increase chances of making model bad. Hence to get rid of this curse of dimensionality a technique named Principal Component Analysis (PCA) is introduced.\n## Principal Component Analysis (PCA)\nPCA is unsupervised linear dimensionality reduction technique to extract information from high dimensional space to lower dimensional sub space by preserving most significance variation of data. PCA allows to identify correlations and patterns among features in data so that it can be transformed into less dimensionality and with only most number of significance features without an important loss in data. Principal components are eigenvectors of a covariance matrix. The data which is to be analysed has to be scaled as results are highly sensitive <br>\n<br>\n**PCA consist of three main steps** \n1. Computing covariance matrix of data\n2. Computing eigen values and vectors of covariance matrix\n3. Using these eigen values and vectors to reduce dimensions and transform data \t\n\nThese steps were had to be done individually until sklearn released PCA, now these all \nsteps has one stop solution predefined in PCA","4e617a24":"## EDA","4390ce53":"We have lot of null values here but as out primary focus is to perform PCA so let's just fill mean values wherever null values occured ","ade89fd7":"Let's try out training this new dataset into algorithm","8d9e30fc":"## Visualisation\nFind corelation between both the components","c6efffeb":"But we dont know if the component value is suitable with 2 or any other number.<br>\nLet's check for n_components=2 [(reference)](http:\/\/stackoverflow.com\/questions\/57293716\/sklearn-pca-explained-variance-and-explained-variance-ratio-difference)","9d572eaa":"Create dummy variable for every categorical column ","58c9ef63":"Scaling all the values from dataset","b49dfb8c":"RMSE & Accuracy Score is pretty well !!. Hence PCA can be used whenever we have more number of dimensions or we can say features","b3095262":"**Predictions**","d490c2f0":"## XGBoost\nXGBoost is an implementation of gradient boosted decision trees designed for speed and performance.","4e6078e5":"Find correlation between both components as how close their covariance are.","859532da":"The variance ratio by rounding off gives value of 0.99 which is almost best so we can continue with n_components=2","77a3694c":"![](https:\/\/nnimgt-a.akamaihd.net\/transform\/v1\/crop\/frm\/5Q2j7ezUfQBfUJsaqK3gfB\/8119e2b2-63e5-48a7-8fa6-eff00e9ab4ab.jpg\/r0_179_3504_2157_w1200_h678_fmax.jpg)"}}