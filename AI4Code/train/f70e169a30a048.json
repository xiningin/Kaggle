{"cell_type":{"a95a00ac":"code","43cb2d5b":"code","6fafbf3d":"code","4f449868":"code","583b599c":"code","0621833d":"code","00d4d592":"code","2e5ae36c":"code","4a38171a":"code","23cab7c6":"code","0dbe008d":"code","e616f7a8":"code","f80a8e19":"code","1286bea2":"code","03e1c395":"code","ab4d0cf0":"code","f02e735e":"code","9b38e162":"markdown","5a03e4ee":"markdown","e2c44d37":"markdown","8e6b4e1f":"markdown","1c21cd97":"markdown","f80409de":"markdown","9df1ab7b":"markdown","776b35ad":"markdown","a8d15a62":"markdown","a00a7e86":"markdown","c59f99e7":"markdown","6d07e572":"markdown","66ce7613":"markdown","2c6f7523":"markdown","19fcd579":"markdown","cc338142":"markdown"},"source":{"a95a00ac":"import pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom sklearn.utils import shuffle\nfrom torch.autograd import Variable\n\nfrom sklearn.metrics import roc_auc_score","43cb2d5b":"id_column = 'id'\ntrain_data = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-nov-2021\/train.csv\", index_col=id_column)\ntest_data = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-nov-2021\/test.csv\", index_col=id_column)\nsubmission = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-nov-2021\/sample_submission.csv\", index_col=id_column)","6fafbf3d":"label = 'target'\nfeatures = [col for col in train_data.columns if 'f' in col]\n\ncont_features = []\ndisc_features = []\n\nfor col in features:\n    if train_data[col].dtype=='float64':\n        cont_features.append(col)\n    else:\n        disc_features.append(col)\n\ntrain_data[cont_features] = train_data[cont_features].astype('float32')\ntrain_data[disc_features] = train_data[disc_features].astype('uint8')\ntrain_data[cont_features] = train_data[cont_features].astype('float32')\ntrain_data[disc_features] = train_data[disc_features].astype('uint8')","4f449868":"import gc\n\ngc.collect()","583b599c":"train_data[:5]","0621833d":"train_data.info()","00d4d592":"train_data['target'].value_counts()","2e5ae36c":"X_train, y_train = train_data.drop(['target'], axis = 1), train_data['target']","4a38171a":"class LogisticRegression(nn.Module):\n    def __init__(self,input_size,output_size):\n        super(LogisticRegression,self).__init__()\n        self.f1 = nn.Linear(input_dim, 1500)\n        self.f2 = nn.Linear(1500, output_dim)\n\n    def forward(self,x):\n        x = self.f1(x)\n        x = F.leaky_relu(x)\n        x = F.dropout(x, p = 0.3)\n        x = self.f2(x)\n        return  F.sigmoid(x)","23cab7c6":"batch_size = 500\nbatch_no = len(X_train) \/\/ batch_size","0dbe008d":"X_train.shape","e616f7a8":"def generate_batches(X, y, batch_size):\n    assert len(X) == len(y)\n    np.random.seed(42)\n    X = np.array(X)\n    y = np.array(y)\n    perm = np.random.permutation(len(X))\n\n    for i in range(len(X)\/\/batch_size):\n        if i + batch_size >= len(X):\n            continue\n        ind = perm[i*batch_size : (i+1)*batch_size]\n        yield (X[ind], y[ind])","f80a8e19":"input_dim = 100\noutput_dim = 2\nlearning_rate = 0.0001\nmodel = LogisticRegression(input_dim,output_dim)\nerror = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\nloss_list = []\nroc_list = []\niteration_number = 200\n\nfor iteration in range(iteration_number):\n    batch_loss = 0\n    batch_roc = 0\n    size = 0\n\n    for (x, y) in generate_batches(X_train, y_train, batch_size):\n        inputs = Variable(torch.from_numpy(x)).float()\n        labels = Variable(torch.from_numpy(y))\n            \n        optimizer.zero_grad() \n        results = model(inputs)\n        loss = error(results, labels)\n\n        batch_loss += loss.data\n        \n        loss.backward()\n        optimizer.step()\n        \n        batch_roc += roc_auc_score(labels.detach().numpy(), results[:, 1].detach().numpy())\n        size += 1\n    \n    loss_list.append(batch_loss\/batch_no)\n    roc_list.append(batch_roc\/size)\n    \n    if (iteration % 20 == 0):\n        print('Epoch {}: loss {}, ROC {}'.format(iteration, batch_loss \/ batch_no, batch_roc \/ size))\n\nplt.plot(range(iteration_number), loss_list)\nplt.xlabel(\"Number of Iterations\")\nplt.ylabel(\"Loss\")\nplt.show()\nplt.plot(range(iteration_number), roc_list)\nplt.xlabel(\"Number of Iterations\")\nplt.ylabel(\"ROC\")\nplt.show()","1286bea2":"test_data =  np.array(test_data)\ntest_data = Variable(torch.FloatTensor(test_data), requires_grad=True) \npredictions = model(test_data)","03e1c395":"submission['target'] = predictions[:, 1].detach().numpy()","ab4d0cf0":"submission[:5]","f02e735e":"submission.to_csv(\"submit.csv\")","9b38e162":"<div style=\"background-color:rgba(255, 69, 0, 0.5);border-radius:5px;display:fill\">\n    <h1><center>Predictions<\/center><\/h1>\n<\/div>","5a03e4ee":"In Logistic Regression we use:\n* One hidden layer with 1500 neurons\n* Activation function - leaky Relu\n* Dropout with p = 0.3\n* Sigmoid","e2c44d37":"<div style=\"background-color:rgba(255, 69, 0, 0.5);border-radius:5px;display:fill\">\n    <h1><center>Basic Data Check<\/center><\/h1>\n<\/div>","8e6b4e1f":"<div align='center'>\n    <h1>PyTorch Tutorial<\/h1>\n    <img src='https:\/\/pytorch.org\/assets\/images\/pytorch-logo.png' style=\"width:200px;height:200px;\">\n<\/div>","1c21cd97":"<div style=\"background-color:rgba(255, 69, 0, 0.5);border-radius:5px;display:fill\">\n    <h1><center>Importing Libraries and Data<\/center><\/h1>\n<\/div>","f80409de":"# Import Libraries","9df1ab7b":"<center><a><img src=\"https:\/\/i.ibb.co\/PWvpT9F\/header.png\" alt=\"header\" border=\"0\" width=800 height=400><\/a><\/center>","776b35ad":"# Import Data","a8d15a62":"Collect garbage to reduce memory usage","a00a7e86":"*Please upvote if you liked it.*","c59f99e7":"# Reduce memory","6d07e572":"In training stage we use:\n* Learning rate = 0.0001\n* Optimizer - Adam\n* Loss - CrossEntropyLoss\n* Epochs = 200","66ce7613":"<div style=\"background-color:rgba(255, 69, 0, 0.5);border-radius:5px;display:fill\">\n    <h1><center>Conclusion<\/center><\/h1>\n<\/div>","2c6f7523":"<div>\n    <p>\nFor best results you can change number of hidden layers in Logistic Regression and increase the number of epochs. <\/p>\n<\/div>","19fcd579":"<div style=\"background-color:rgba(0, 167, 255, 0.6);border-radius:5px;display:fill\">\n    <h1><center>Tabular Playground Series - Nov 2021<\/center><\/h1>\n<\/div>","cc338142":"<div style=\"background-color:rgba(255, 69, 0, 0.5);border-radius:5px;display:fill\">\n    <h1><center>Logistic Regression with PyTorch<\/center><\/h1>\n<\/div>"}}