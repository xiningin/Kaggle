{"cell_type":{"9bbbdfe5":"code","fc3d53b8":"code","8633066f":"code","729223a4":"code","4c57ac84":"code","31c5de95":"code","e00c2407":"code","5b87859a":"code","0f3cc302":"code","d664b88a":"code","9193745d":"code","30a7cd30":"code","52cb2fcd":"code","7c4a940b":"code","e8fded1d":"code","a1028408":"code","4f0985f3":"code","0ff0fed8":"code","d9db6dfb":"code","e9d9c46d":"code","82766005":"code","5ad32d06":"code","f5b9273d":"code","67e896d6":"code","d1aa583f":"code","0f7ce7b8":"code","1e5e67ef":"code","6dee948d":"code","0103aa33":"code","cdaa4ebd":"code","4d8b6865":"code","891b03b1":"code","57cc7a26":"code","9ed5de2e":"markdown","e71d01a0":"markdown","83bf853d":"markdown","4e1ca597":"markdown","dd63591f":"markdown","11cd2622":"markdown","1f577480":"markdown","532ff811":"markdown","38116fec":"markdown","438f5c2c":"markdown","68ad43a1":"markdown","a3e0ed21":"markdown","24859ea4":"markdown","f07d592e":"markdown","95b34fae":"markdown","de24baf2":"markdown","d8c1585c":"markdown","ce2c49ef":"markdown","093f4086":"markdown","1bd85632":"markdown","38a58826":"markdown","e4fd4014":"markdown","8fce9776":"markdown","fc873c2a":"markdown","c4e66497":"markdown"},"source":{"9bbbdfe5":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\n\n%matplotlib inline\n\npd.options.mode.chained_assignment = None # same function as warnings.filterwarnings('ignore')\npd.options.display.max_columns = 999 # show the omitted columns \n\nimport warnings\nwarnings.filterwarnings('ignore')","fc3d53b8":"train_df = pd.read_csv('..\/input\/zillow-prize-1\/train_2016_v2.csv', parse_dates=[\"transactiondate\"])\n# 'parse_dates' read date, time data as datetime format\n# if not, just read data, time data as object format","8633066f":"plt.figure(figsize = (8, 6))\nplt.scatter(range(train_df.shape[0]), np.sort(train_df.logerror.values))\nplt.xlabel('index', fontsize = 12)\nplt.ylabel('logerror', fontsize = 12)\nplt.show()","729223a4":"# np.percentile = \uc624\ub984\ucc28\uc21c \uc815\ub82c\ud588\uc744 \ub54c 0\uc744 \ucd5c\uc18c\uac12, 100\uc744 \ucd5c\ub300\uac12\uc73c\ub85c \ubc31\ubd84\uc728\ub85c \ub098\ud0c0\ub0b8 \ud2b9\uc815 \uc704\uce58\nulimit = np.percentile(train_df.logerror.values, 99)\nllimit = np.percentile(train_df.logerror.values, 1)\n\ntrain_df['logerror'].loc[train_df['logerror'] > ulimit] = ulimit\ntrain_df['logerror'].loc[train_df['logerror'] < llimit] = llimit\n\nplt.figure(figsize = (12, 8))\nsns.distplot(train_df.logerror.values, bins = 50, kde = True)\nplt.xlabel('logerror', fontsize = 12)\nplt.show()","4c57ac84":"train_df['transaction_month'] = train_df['transactiondate'].dt.month\n\ncnt_srs = train_df['transaction_month'].value_counts()\nplt.figure(figsize = (12, 6))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color=color[3])\nplt.xticks(rotation = 'vertical')\nplt.xlabel('Month of transaction', fontsize = 12)\nplt.ylabel('Number of Occurences', fontsize = 12)\nplt.show()","31c5de95":"(train_df['parcelid'].value_counts().reset_index())['parcelid'].value_counts()","e00c2407":"prop_df =pd.read_csv('..\/input\/zillow-prize-1\/properties_2016.csv')\nprint(prop_df.shape)","5b87859a":"prop_df.head()","0f3cc302":"# preprocessing NaN values\nmissing_df = prop_df.isnull().sum(axis = 0).reset_index()\nmissing_df.columns = ['column_name', 'missing_count']\nmissing_df = missing_df.loc[missing_df['missing_count'] > 0]\nmissing_df = missing_df.sort_values(by = 'missing_count')\n\nind = np.arange(missing_df.shape[0])\nwidth = 0.9\nfig, ax = plt.subplots(figsize = (12, 18))\nrects = ax.barh(ind, missing_df.missing_count.values, color='blue')\nax.set_yticks(ind)\nax.set_yticklabels(missing_df.column_name.values, rotation = 'horizontal')\nax.set_xlabel('count of missing values')\nax.set_title('number of missing values in each columns')\nplt.show()","d664b88a":"plt.figure(figsize = (12, 12))\nsns.jointplot(x=prop_df.latitude.values, y=prop_df.longitude.values,\n              size = 10)\nplt.ylabel('longtitude', fontsize = 12)\nplt.xlabel('latitude', fontsize = 12)\nplt.show()","9193745d":"train_df = pd.merge(train_df, prop_df, on='parcelid', how = 'left')\ntrain_df.head()","30a7cd30":"pd.options.display.max_rows = 65\n\ndtype_df = train_df.dtypes.reset_index()\ndtype_df.columns = ['Count', 'Column Type']\ndtype_df.head()","52cb2fcd":"dtype_df['Column Type'].value_counts().reset_index()","7c4a940b":"missing_df = train_df.isnull().sum(axis = 0).reset_index()\nmissing_df.columns = ['column_name', 'missing_count']\nmissing_df['missing_ratio'] = missing_df['missing_count'] \/ train_df.shape[0]\nmissing_df.loc[missing_df['missing_ratio'] > 0.999]","e8fded1d":"# let us just impute the missing values with mean values to compute correlation coefficients\nmean_values = train_df.mean(axis = 0)\ntrain_df.fillna(mean_values, inplace = True)","a1028408":"# now let us look at the correlation coefficient of each of these variables\nx_cols = [col for col in train_df.columns if col not in ['logerror'] if train_df[col].dtype == 'float64']\n\nlabels = []\nvalues = []\n\nfor col in x_cols:\n    labels.append(col)\n    values.append(np.corrcoef(train_df[col].values, train_df.logerror.values)[0, 1])\ncorr_df = pd.DataFrame({'col_labels':labels,\n                        'corr_values':values})\ncorr_df = corr_df.sort_values(by = 'corr_values')","4f0985f3":"ind = np.arange(len(labels))\nwidth = 0.9\nfig, ax = plt.subplots(figsize = (12, 40))\nrects = ax.barh(ind, np.array(corr_df.corr_values.values), color = 'y')\nax.set_yticks(ind)\nax.set_yticklabels(corr_df.col_labels.values, rotation = 'horizontal')\nax.set_xlabel('correlation coefficient')\nax.set_title('correlation coefficient of the variables')\nplt.show()","0ff0fed8":"corr_zero_cols = ['assessmentyear', 'storytypeid', 'pooltypeid2', 'pooltypeid7', 'pooltypeid10', 'poolcnt', 'decktypeid', 'buildingclasstypeid']\nfor col in corr_zero_cols:\n    print(col, len(train_df[col].unique()))","d9db6dfb":"corr_df_sel = corr_df.loc[(corr_df['corr_values']>0.02) | (corr_df['corr_values'] < -0.01)]\ncorr_df_sel.head()","e9d9c46d":"cols_to_use = corr_df_sel.col_labels.tolist()\n\ntemp_df = train_df[cols_to_use] # make dataframe through list\ncorrmat = temp_df.corr(method='spearman')\nf, ax = plt.subplots(figsize = (8, 8))\n\n# draw the heatmap using seaborn\nsns.heatmap(corrmat, vmax = 1, square = True)\nplt.title('important variables correlation map', fontsize = 15)\nplt.show()","82766005":"col = \"finishedsquarefeet12\"\nulimit = np.percentile(train_df[col].values, 99.5)\nllimit = np.percentile(train_df[col].values, 0.5)\ntrain_df[col].loc[train_df[col] > ulimit] = ulimit\ntrain_df[col].loc[train_df[col] < llimit] = llimit","5ad32d06":"plt.figure(figsize = (12, 12))\nsns.jointplot(x=train_df.finishedsquarefeet12.values,\n              y=train_df.logerror.values,\n              size = 10, color = color[4])\nplt.ylabel('log error', fontsize = 12)\nplt.xlabel('finished square feet 12', fontsize = 12)\nplt.title('finished square feet 12 vs log error', fontsize = 15)\nplt.show()","f5b9273d":"col = \"calculatedfinishedsquarefeet\"\nulimit = np.percentile(train_df[col].values, 99.5)\nllimit = np.percentile(train_df[col].values, 0.5)\ntrain_df[col].loc[train_df[col] > ulimit] = ulimit\ntrain_df[col].loc[train_df[col] < llimit] = llimit\n\nplt.figure(figsize = (12, 12))\nsns.jointplot(x = train_df[col].values,\n              y = train_df['logerror'].values, \n              size = 10, color = color[5])\nplt.ylabel('log error', fontsize = 12)\nplt.xlabel('calculated finished square feet', fontsize = 12)\nplt.title('calculated finished square feet vs log error', fontsize = 15)\nplt.show()","67e896d6":"plt.figure(figsize=(12,8))\nsns.countplot(x=\"bathroomcnt\", data=train_df)\nplt.ylabel('Count', fontsize=12)\nplt.xlabel('Bathroom', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.title(\"Frequency of Bathroom count\", fontsize=15)\nplt.show()","d1aa583f":"plt.figure(figsize = (12, 8))\nsns.boxplot(x = 'bathroomcnt', y = 'logerror', data = train_df)\nplt.ylabel('log error', fontsize = 12)\nplt.xlabel('Bathroom Count', fontsize = 12)\nplt.xticks(rotation = 'vertical')\nplt.title('how log error changes with bathroom count?', fontsize = 15)\nplt.show()","0f7ce7b8":"plt.figure(figsize = (12, 8))\nsns.countplot(x='bedroomcnt', data = train_df)\nplt.ylabel('Frequency', fontsize = 12)\nplt.xlabel('Bedroom count', fontsize = 12)\nplt.xticks(rotation = 'vertical')\nplt.title('Frequency of Bedroom count', fontsize = 15)\nplt.show()","1e5e67ef":"train_df['bedroomcnt'].loc[train_df['bedroomcnt'] > 7] = 7\nplt.figure(figsize = (12, 8))\nsns.violinplot(x='bedroomcnt', y='logerror', data = train_df)\nplt.xlabel('Bedroom count', fontsize = 12)\nplt.ylabel('Log error', fontsize = 12)\nplt.show()","6dee948d":"col = 'taxamount'\nulimit = np.percentile(train_df[col].values, 99.5)\nllimit = np.percentile(train_df[col].values, 0.5)\ntrain_df[col].loc[train_df[col] > ulimit] = ulimit\ntrain_df[col].loc[train_df[col] < llimit] = llimit\n\nplt.figure(figsize = (12, 12))\nsns.jointplot(x=train_df['taxamount'].values, \n              y=train_df['logerror'].values, size = 10, color ='g')\nplt.ylabel('Log error', fontsize = 12)\nplt.xlabel('Tax amount', fontsize = 12)\nplt.title('Tax amount VS log error', fontsize = 15)\nplt.show()","0103aa33":"from ggplot import *\nggplot(aes(x='yearbuilt', y='logerror'), data = train_df) + \\\n    geom_point(color = 'steelblue', size = 1) + \\\n    stat_smooth()","cdaa4ebd":"ggplot(aes(x = 'latitude', y = 'longitude', color = 'logerror'), data = train_df) + \\\n    geom_point() + \\\n    scale_color_gradient(low = 'red', high = 'blue')","4d8b6865":"ggplot(aes(x='finishedsquarefeet12', y='taxamount', color = 'logerror'), data = train_df) + \\\n    geom_point(alpha = 0.7) + \\\n    scale_color_gradient(low = 'pink', high = 'blue')","891b03b1":"train_y = train_df['logerror'].values\ncat_cols = [\"hashottuborspa\", \"propertycountylandusecode\", \"propertyzoningdesc\", \"fireplaceflag\", \"taxdelinquencyflag\"]\ntrain_df = train_df.drop(['parcelid', 'logerror', 'transactiondate', 'transaction_month']+cat_cols, axis=1)\nfeat_names = train_df.columns.values\n\nfrom sklearn import ensemble\nmodel = ensemble.ExtraTreesRegressor(n_estimators=25, max_depth=30,\n                                     max_features=0.3, n_jobs=-1, random_state=0)\nmodel.fit(train_df, train_y)\n\n## plot the importances ##\nimportances = model.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model.estimators_], axis =0)\nindices = np.argsort(importances)[::-1][:20]\n\nplt.figure(figsize=(12, 12))\nplt.title('Feature importances')\nplt.bar(range(len(indices)), importances[indices], color='r',\n        yerr = std[indices], align = 'center')\nplt.xticks(range(len(indices)), feat_names[indices], rotation='vertical')\nplt.xlim([-1, len(indices)])\nplt.show()","57cc7a26":"import xgboost as xgb\nxgb_params = {\n    'eta': 0.05,\n    'max_depth': 8,\n    'subsample': 0.7,\n    'colsample_bytree': 0.7,\n    'objective': 'reg:linear',\n    'silent': 1,\n    'seed' : 0\n}\n\ndtrain = xgb.DMatrix(train_df, train_y, feature_names = train_df.columns.values)\nmodel = xgb.train(dict(xgb_params, silent = 0), dtrain, num_boost_round=50)\n\n# plot the important features #\nfig, ax = plt.subplots(figsize=(12, 18))\nxgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\nplt.show()","9ed5de2e":"Almost all are float variables with few object (categorical) variables. Let us get the count.","e71d01a0":"The correlation of the target variable with the given set of variables are low overall.\n\nThere are few variables at the top of this graph without any correlation values. I guess they have only one unique value and hence no correlation value. Let us confirm the same.","83bf853d":"There are no visible pockets as such with respect to latitude or longitude atleast with the naked eye.\n\nLet us take the variables with highest positive correlation and highest negative correlation to see if we can see some visible patterns.","4e1ca597":"There are no visible patterns here as well.","dd63591f":"There is an interesting 2.279 value in the bathroom count.\n\nNow let us check how the log error changes based on this.","11cd2622":"##### This kernel used dataset from the Zillow Prize: Zillow\u2019s Home Value Prediction and copied from the 'Simple Exploration Notebook - Zillow Prize' written by SRK.\n##### Introduction to 'Simple Exploration Notebook - Zillow Prize' : [URL](https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-notebook-zillow-prize)\n##### Thanks for sharing kernel, SRK","1f577480":"So most of the parcel ids are appearing only once in the dataset.","532ff811":"The important variables themselves are very highly correlated.! Let us now look at each of them.\n\n#### Finished SquareFeet 12:\n\nLet us see how the finished square feet 12 varies with the log error.","38116fec":"Seems the range of logerror narrows down with increase in finished square feet 12 variable. Probably larger houses are easy to predict?\n\n#### Calculated finished square feet:","438f5c2c":"Now let us check the dtypes of different types of variable.","68ad43a1":"This looks nice with some outliers at both the ends.!\n\nLet us remove the outliers and then do a histogram plot on the same.","a3e0ed21":"#### Transaction Date:\nNow let us explore the date field. Let us first check the number of transactions in each month","24859ea4":"Seems \"tax amount\" is the most importanct variable followed by \"structure tax value dollar count\" and \"land tax value dollor count\"","f07d592e":"From the data page, we are provided with a full list of real estate properties in three counties (Los Angeles, Orange and Ventura, California) data in 2016.\n\nWe have about 90,811 rows in train but we have about 2,985,217 rows in properties file. So let us merge the two files and then carry out our analysis.","95b34fae":"#### Univariate Analysis:\n\nSince there are so many variables, let us first take the 'float' variables alone and then get the correlation with the target variable to see how they are related.","de24baf2":"Here as well the distribution is very similar to the previous one. No wonder the correlation between the two variables are also high.\n\n#### Bathroom Count:","d8c1585c":"There is a minor incremental trend seen with respect to built year.\n\nNow let us see how the logerror varies with respect to latitude and longitude.","ce2c49ef":"#### YearBuilt:\nLet us explore how the error varies with the yearbulit variable.","093f4086":"#### Logerror:\n\nTarget variable for this competition is \"logerror\" field. So let us do some analysis on this field first.","1bd85632":"Four columns have missing values 99.9% of the times.!","38a58826":"#### Parcel ID:","e4fd4014":"Now let us check the number of Nulls in this new merged dataset.","8fce9776":"#### Bedroom count:","fc873c2a":"#### Properties 2016:\n\nNow let us explore the properties_2016 file.","c4e66497":"Let us take the variables with high correlation values and then do some analysis on them."}}