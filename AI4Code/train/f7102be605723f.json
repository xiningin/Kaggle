{"cell_type":{"08224fe2":"code","ebd4e2b3":"code","6cfe077f":"code","dbf4c8ea":"code","547a0b20":"code","19c8cbb8":"code","44da0ea5":"code","a452c170":"code","20fd40b3":"code","7b2618db":"code","0a2a5f1b":"code","648cb3b7":"code","5e386061":"code","81e45735":"code","da6f3739":"code","27d1515e":"code","ad7e2614":"code","88144679":"code","4b2d78e4":"code","46b35f51":"code","7bf831b0":"code","5c9ecfd6":"code","a01b2025":"code","9ba62426":"code","e9d6ccbb":"code","79daf36d":"code","d0f855fb":"code","5b1e296d":"code","2b7019d3":"code","a021526a":"code","7980263b":"code","38729a02":"code","7a073536":"code","79d42f76":"code","2af3f4c0":"code","f8cd99f8":"code","8aa57ea0":"code","f6b91667":"code","a1de3103":"code","5c9840ea":"code","41164617":"code","98db58dc":"code","64358f47":"code","02931386":"code","e6093056":"code","b13630f8":"code","8bb08624":"code","56b1946e":"code","be8abeaa":"code","a51a2a2c":"code","9e322a35":"code","489d670a":"code","db362d76":"code","ab14e92f":"code","5d9c9198":"code","2e5c3b07":"code","06d267de":"code","0007e2e4":"code","8aab2328":"code","629ecad1":"code","75bae280":"code","781cb807":"code","7755107e":"code","5d89b7cf":"code","30dd4839":"code","da5635a7":"code","e8f969db":"code","74f5c7c9":"code","5e8318f4":"code","f9edb13c":"code","3fbda8b4":"code","9f3dca39":"code","3fa46bd6":"code","eb70f340":"code","9bbd8740":"code","a8b24d72":"code","fcf8309a":"code","305b71aa":"code","6bc64420":"code","5b40d158":"code","e4eec1b9":"code","7155c35a":"code","7aaa114e":"code","8cef1abd":"code","3656a95a":"code","e726b60f":"code","ec0a522c":"code","98567eb7":"code","2472f064":"code","16d4d86a":"code","5d3d4fc8":"code","3c42dbd1":"code","b0219965":"code","1d4ed039":"code","c965a42d":"code","cd8e131b":"code","cbc932f1":"code","d9aa6ac6":"code","4f3e4054":"code","46a24add":"code","c315ca5e":"code","4f81405c":"code","fd5bdabd":"markdown","c5514387":"markdown","09ddd69b":"markdown","c9615154":"markdown","b4c28bac":"markdown","aeab682a":"markdown","c02b67e9":"markdown","495e4700":"markdown","45a0b836":"markdown","c4a73b50":"markdown","a7764939":"markdown","b0f25de5":"markdown","27c5ab73":"markdown","df5e70ff":"markdown","9594797d":"markdown","a220e37b":"markdown","b402a7f3":"markdown","a53791b2":"markdown","dfa1ed2f":"markdown","21797f5e":"markdown","a87dd424":"markdown","6c955327":"markdown","13fc899f":"markdown","3384775a":"markdown"},"source":{"08224fe2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ebd4e2b3":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Here we are just importing which are important for starting with.. and will add-on once I need more when reaching towards Modeling and Prdiction.","6cfe077f":"train_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')","dbf4c8ea":"train_data.head()","547a0b20":"print(train_data.shape)","19c8cbb8":"train_data.info()","44da0ea5":"# Lets get the % of each null values.\ntotal = train_data.isnull().sum().sort_values(ascending=False)\npercent_1 = train_data.isnull().sum()\/train_data.isnull().count()*100\npercent_2 = (round(percent_1, 1)).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\nmissing_data.head(25)","a452c170":"# Visualizing the NULL data using Seaborn HeatMap.\nsns.heatmap(train_data.isnull(), yticklabels = False, cbar = False)","20fd40b3":"#Using Pearson Correlation\n\nplt.figure(figsize=(20,10))\ncor = train_data.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\nplt.show()","7b2618db":"#Correlation with output variable\ncor_target = abs(cor[\"SalePrice\"])\n#Selecting highly correlated features\nrelevant_features = cor_target[cor_target>0.5]\nrelevant_features\n# From the output of this, we can see, only the features displayed below are highly correlated with the output variable SalePrice. Other variables has less impact.","0a2a5f1b":"# One of the assumptions of linear regression is that the independent variables need to be uncorrelated with each other. If these variables are correlated with each other, then we need to keep only one of them and drop the rest.\nprint(train_data[[\"OverallQual\",\"YearBuilt\"]].corr())\n# We do have 10 Features.. and checking the correlation between is tedius. So ignoring for now.","648cb3b7":"train_data.describe(include=['O'])","5e386061":"print(train_data['FireplaceQu'].describe())\nprint('-'*20)\nprint(train_data.groupby('FireplaceQu').size())\nprint('-'*20)\nprint(train_data['FireplaceQu'].isnull().sum(axis=0))","81e45735":"# From above its clear that the FireplaceQu is a Categorical value, and Gd is the most common value, so will fill the NaN with the same.\ntrain_data['FireplaceQu'] = train_data['FireplaceQu'].fillna(train_data['FireplaceQu'].mode()[0])\nprint(train_data['FireplaceQu'].isnull().sum(axis=0))","da6f3739":"print(train_data['LotFrontage'].describe())\n#print('-'*20)\n#print(train_data.groupby('LotFrontage').size())\nprint('-'*20)\nprint(train_data['LotFrontage'].isnull().sum(axis=0))","27d1515e":"# Check Mean; Median; Mode \nprint('MEAN : ', train_data['LotFrontage'].mean())\nprint('MEDIAN : ', train_data['LotFrontage'].median())\nprint('MODE : ', train_data['LotFrontage'].mode())","ad7e2614":"# Since LotFrontage is float type, and Mean\/Median are nearly same.. we can use Mean value to fill NaN.\ntrain_data['LotFrontage'] = train_data['LotFrontage'].fillna(train_data['LotFrontage'].mean())\nprint(train_data['LotFrontage'].isnull().sum(axis=0))","88144679":"print(train_data['GarageCond'].describe())\nprint('-'*20)\nprint(train_data.groupby('GarageCond').size())\nprint('-'*20)\nprint(train_data['GarageCond'].isnull().sum(axis=0))","4b2d78e4":"# From above its clear that the GarageCond is a Categorical value, and TA is the most common value, so will fill the NaN with the same.\ntrain_data['GarageCond'] = train_data['GarageCond'].fillna(train_data['GarageCond'].mode()[0])\nprint(train_data['GarageCond'].isnull().sum(axis=0))\nprint(train_data.groupby('GarageCond').size())","46b35f51":"print(train_data['GarageType'].describe())\nprint('-'*20)\nprint(train_data.groupby('GarageType').size())\nprint('-'*20)\nprint(train_data['GarageType'].isnull().sum(axis=0))","7bf831b0":"# From above its clear that the GarageType is a Categorical value, and Attchd is the most common value, so will fill the NaN with the same.\ntrain_data['GarageType'] = train_data['GarageType'].fillna(train_data['GarageType'].mode()[0])\nprint(train_data['GarageType'].isnull().sum(axis=0))\nprint(train_data.groupby('GarageType').size())","5c9ecfd6":"print(train_data['GarageYrBlt'].describe())\nprint('-'*20)\nprint(train_data.groupby('GarageYrBlt').size())\nprint('-'*20)\nprint(train_data['GarageYrBlt'].isnull().sum(axis=0))","a01b2025":"# GarageYrBlt : Year build on will not provide much input, instead will add new feature aith Age of Garage, and as the Age grows the value goes down (assuming : depreciate in the construction).\ntrain_data['GarageAge'] = 2019 - train_data['GarageYrBlt']\n# Lets calculate the construction age from current year.","9ba62426":"train_data.head()","e9d6ccbb":"# Will drop the feature \/ column : GarageYrBlt.\ntrain_data.drop('GarageYrBlt', inplace = True, axis = 1)\ntrain_data.head()","79daf36d":"print(train_data['GarageAge'].describe())\nprint('-'*20)\nprint(train_data.groupby('GarageAge').size())\nprint('-'*20)\nprint(train_data['GarageAge'].isnull().sum(axis=0))","d0f855fb":"# Check Mean; Median; Mode \nprint('MEAN : ', train_data['GarageAge'].mean())\nprint('MEDIAN : ', train_data['GarageAge'].median())\nprint('MODE : ', train_data['GarageAge'].mode())","5b1e296d":"# Will replace the NaN by mean value, but will do round of.\ntrain_data['GarageAge'] = train_data['GarageAge'].fillna(40)\nprint(train_data['GarageAge'].isnull().sum(axis=0))","2b7019d3":"# Will just check the correlation of GarageAge with SalePrice\ntrain_data[[\"GarageAge\",\"SalePrice\"]].corr()\n# Not bad.","a021526a":"sns.lineplot(x=\"GarageAge\", y=\"SalePrice\", data=train_data)","7980263b":"sns.catplot(x=\"GarageFinish\", y=\"SalePrice\", data=train_data);\n# From visualization it seems for Furnished flats price is high.","38729a02":"sns.barplot(x='GarageFinish',y='SalePrice',data=train_data);","7a073536":"print(train_data['GarageFinish'].describe())\nprint('-'*20)\nprint(train_data.groupby('GarageFinish').size())\nprint('-'*20)\nprint(train_data['GarageFinish'].isnull().sum(axis=0))","79d42f76":"# From above its clear that the GarageFinish is a Categorical value, and Unf is the most common value, so will fill the NaN with the same.\ntrain_data['GarageFinish'] = train_data['GarageFinish'].fillna(train_data['GarageFinish'].mode()[0])\nprint(train_data['GarageFinish'].isnull().sum(axis=0))\nprint(train_data.groupby('GarageFinish').size())","2af3f4c0":"sns.barplot(x='GarageQual',y='SalePrice',data=train_data);","f8cd99f8":"sns.catplot(x=\"GarageQual\", y=\"SalePrice\", data=train_data);","8aa57ea0":"print(train_data['GarageQual'].describe())\nprint('-'*20)\nprint(train_data.groupby('GarageQual').size())\nprint('-'*20)\nprint(train_data['GarageQual'].isnull().sum(axis=0))","f6b91667":"# From above its clear that the GarageQual is a Categorical value, and TA is the most common value, so will fill the NaN with the same.\ntrain_data['GarageQual'] = train_data['GarageQual'].fillna(train_data['GarageQual'].mode()[0])\nprint(train_data['GarageQual'].isnull().sum(axis=0))\nprint(train_data.groupby('GarageQual').size()) \n","a1de3103":"sns.catplot(x=\"BsmtExposure\", y=\"SalePrice\", data=train_data);","5c9840ea":"sns.barplot(x='BsmtExposure',y='SalePrice',data=train_data);","41164617":"print(train_data['BsmtExposure'].describe())\nprint('-'*20)\nprint(train_data.groupby('BsmtExposure').size())\nprint('-'*20)\nprint(train_data['BsmtExposure'].isnull().sum(axis=0))\n","98db58dc":"# From above its clear that the BsmtExposure is a Categorical value, and No is the most common value, so will fill the NaN with the same.\ntrain_data['BsmtExposure'] = train_data['BsmtExposure'].fillna(train_data['BsmtExposure'].mode()[0])\nprint(train_data['BsmtExposure'].isnull().sum(axis=0))\nprint(train_data.groupby('BsmtExposure').size()) \n","64358f47":"sns.catplot(x=\"BsmtFinType2\", y=\"SalePrice\", data=train_data);","02931386":"sns.barplot(x='BsmtFinType2',y='SalePrice',data=train_data);","e6093056":"print(train_data['BsmtFinType2'].describe())\nprint('-'*20)\nprint(train_data.groupby('BsmtFinType2').size())\nprint('-'*20)\nprint(train_data['BsmtFinType2'].isnull().sum(axis=0))\n\n","b13630f8":"# From above its clear that the BsmtFinType2 is a Categorical value, and Unf is the most common value, so will fill the NaN with the same.\ntrain_data['BsmtFinType2'] = train_data['BsmtFinType2'].fillna(train_data['BsmtFinType2'].mode()[0])\nprint(train_data['BsmtFinType2'].isnull().sum(axis=0))\nprint(train_data.groupby('BsmtFinType2').size()) ","8bb08624":"train_data['BsmtFinType1'] = train_data['BsmtFinType1'].fillna(train_data['BsmtFinType1'].mode()[0])\ntrain_data['BsmtCond'] = train_data['BsmtCond'].fillna(train_data['BsmtCond'].mode()[0])\ntrain_data['BsmtQual'] = train_data['BsmtQual'].fillna(train_data['BsmtQual'].mode()[0])\ntrain_data['MasVnrType'] = train_data['MasVnrType'].fillna(train_data['MasVnrType'].mode()[0])\ntrain_data['Electrical'] = train_data['Electrical'].fillna(train_data['Electrical'].mode()[0])","56b1946e":"print(train_data['MasVnrArea'].describe())\nprint('-'*20)\nprint(train_data['MasVnrArea'].isnull().sum(axis=0))","be8abeaa":"# Check Mean; Median; Mode \nprint('MEAN : ', train_data['MasVnrArea'].mean())\nprint('MEDIAN : ', train_data['MasVnrArea'].median())\nprint('MODE : ', train_data['MasVnrArea'].mode())","a51a2a2c":"# Will replace the NaN by mean value, but will do round of.\ntrain_data['MasVnrArea'] = train_data['MasVnrArea'].fillna(103)\nprint(train_data['MasVnrArea'].isnull().sum(axis=0))","9e322a35":"# Dropping off not-used features.\ntrain_data.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence'], axis = 1, inplace = True)","489d670a":"sns.heatmap(train_data.isnull(), yticklabels = False, cbar = False, cmap = 'YlGnBu')\n# Cool no more NaN","db362d76":"test_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntest_data.head()","ab14e92f":"print(test_data.shape)","5d9c9198":"# Lets get the % of each null values.\ntotal = test_data.isnull().sum().sort_values(ascending=False)\npercent_1 = test_data.isnull().sum()\/test_data.isnull().count()*100\npercent_2 = (round(percent_1, 1)).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'], sort=False)\nmissing_data.head(25)","2e5c3b07":"sns.catplot(x=\"MSZoning\", y=\"SalePrice\", data=train_data);\n","06d267de":"sns.barplot(x='MSZoning',y='SalePrice',data=train_data);","0007e2e4":"test_data['LotFrontage'] = test_data['LotFrontage'].fillna(test_data['LotFrontage'].mean())\ntest_data['MasVnrArea'] = test_data['MasVnrArea'].fillna(test_data['MasVnrArea'].mean())\n\ntest_data['GarageAge'] = 2019 - test_data['GarageYrBlt']\ntest_data.drop('GarageYrBlt', inplace = True, axis = 1)\ntest_data['GarageAge'] = test_data['GarageAge'].fillna(test_data['GarageAge'].mean())","8aab2328":"test_data['MSZoning'] = test_data['MSZoning'].fillna(test_data['MSZoning'].mode()[0])\ntest_data['FireplaceQu'] = test_data['FireplaceQu'].fillna(test_data['FireplaceQu'].mode()[0])\ntest_data['GarageCond'] = test_data['GarageCond'].fillna(test_data['GarageCond'].mode()[0])\ntest_data['GarageType'] = test_data['GarageType'].fillna(test_data['GarageType'].mode()[0])\ntest_data['GarageFinish'] = test_data['GarageFinish'].fillna(test_data['GarageFinish'].mode()[0])\ntest_data['GarageQual'] = test_data['GarageQual'].fillna(test_data['GarageQual'].mode()[0])\ntest_data['BsmtExposure'] = test_data['BsmtExposure'].fillna(test_data['BsmtExposure'].mode()[0])\ntest_data['BsmtFinType2'] = test_data['BsmtFinType2'].fillna(test_data['BsmtFinType2'].mode()[0])\ntest_data['BsmtFinType1'] = test_data['BsmtFinType1'].fillna(test_data['BsmtFinType1'].mode()[0])\ntest_data['BsmtCond'] = test_data['BsmtCond'].fillna(test_data['BsmtCond'].mode()[0])\ntest_data['BsmtQual'] = test_data['BsmtQual'].fillna(test_data['BsmtQual'].mode()[0])\ntest_data['MasVnrType'] = test_data['MasVnrType'].fillna(test_data['MasVnrType'].mode()[0])\ntest_data['Electrical'] = test_data['Electrical'].fillna(test_data['Electrical'].mode()[0])\n","629ecad1":"test_data.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence'], axis = 1, inplace = True)","75bae280":"sns.heatmap(test_data.isnull(), yticklabels = False, cbar = False, cmap = 'YlGnBu')\n# Cool we have just few handful of NaN.. we can live with it.\n# Seems we should not have any NaN while doing prediction, as none of the model expect NaN... as i got the error \"ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\"\n# Will now handle all NaN in test_data.","781cb807":"total = test_data.isnull().sum().sort_values(ascending=False)\npercent_1 = test_data.isnull().sum()\/test_data.isnull().count()*100\npercent_2 = (round(percent_1, 1)).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'], sort=False)\nmissing_data.head(15)","7755107e":"missing_data.index ","5d89b7cf":"missing_col = ['Functional','BsmtFullBath','BsmtHalfBath','Utilities','GarageArea','BsmtFinSF2','BsmtUnfSF','Exterior2nd','Exterior1st','KitchenQual','GarageCars','TotalBsmtSF','BsmtFinSF1','SaleType']\nmissing_col\ntest_data[missing_col].head()\n# From here we see that Functional; Utilities; Exterior2nd; Exterior1st; KitchenQual; SaleType are Categorical\n# and rest are Numeric. So will replace the values with Mode and Mean respectively.","30dd4839":"test_data['Functional'] = test_data['Functional'].fillna(test_data['Functional'].mode()[0])\ntest_data['Utilities'] = test_data['Utilities'].fillna(test_data['Utilities'].mode()[0])\ntest_data['Exterior2nd'] = test_data['Exterior2nd'].fillna(test_data['Exterior2nd'].mode()[0])\ntest_data['Exterior1st'] = test_data['Exterior1st'].fillna(test_data['Exterior1st'].mode()[0])\ntest_data['KitchenQual'] = test_data['KitchenQual'].fillna(test_data['KitchenQual'].mode()[0])\ntest_data['SaleType'] = test_data['SaleType'].fillna(test_data['SaleType'].mode()[0])\n\ntest_data['BsmtFullBath'] = test_data['BsmtFullBath'].fillna(test_data['BsmtFullBath'].mean())\ntest_data['BsmtHalfBath'] = test_data['BsmtHalfBath'].fillna(test_data['BsmtHalfBath'].mean())\ntest_data['GarageArea'] = test_data['GarageArea'].fillna(test_data['GarageArea'].mean())\ntest_data['BsmtFinSF2'] = test_data['BsmtFinSF2'].fillna(test_data['BsmtFinSF2'].mean())\ntest_data['BsmtUnfSF'] = test_data['BsmtUnfSF'].fillna(test_data['BsmtUnfSF'].mean())\ntest_data['GarageCars'] = test_data['GarageCars'].fillna(test_data['GarageCars'].mean())\ntest_data['TotalBsmtSF'] = test_data['TotalBsmtSF'].fillna(test_data['TotalBsmtSF'].mean())\ntest_data['BsmtFinSF1'] = test_data['BsmtFinSF1'].fillna(test_data['BsmtFinSF1'].mean())","da5635a7":"print(train_data.shape, test_data.shape)","e8f969db":"cat_features = train_data.select_dtypes(include='object').columns\ncat_features","74f5c7c9":"len(cat_features)","5e8318f4":"# Lets transform these Categorical Features into Number using get_dummies function (One Hot Encoding)\nfinal_train_data = pd.get_dummies(train_data, columns=cat_features, drop_first=True)\nprint(final_train_data.shape)\n#final_train_data.drop(cat_features, axis = 1, inplace = True)\nfinal_train_data.head()","f9edb13c":"# Check for Duplicate Columns\nfinal_train_data.columns.duplicated()","3fbda8b4":"print(final_train_data.columns.values)","9f3dca39":"cat_features_test = test_data.select_dtypes(include='object').columns\n#print(\"Categorical Feature Columns from Test DataFrame\", cat_features_test)\nprint(\"length of categoricl features\", len(cat_features_test))\n\n# Lets transform these Categorical Features into Number using get_dummies function (One Hot Encoding)\nfinal_test_data = pd.get_dummies(test_data, columns=cat_features_test, drop_first=True)\n\nprint(final_test_data.shape)\n\n#final_test_data.drop(cat_features_test, axis = 1, inplace = True)\nfinal_test_data.head()\n\n# Check for Duplicate Columns\nfinal_test_data.columns.duplicated()","3fa46bd6":"print(final_test_data.columns.values)","eb70f340":"merge_data = pd.concat([train_data, test_data], axis = 0)\nmerge_data.shape","9bbd8740":"cat_features_merge_data = merge_data.select_dtypes(include='object').columns\nprint(\"length of categoricl features\", len(cat_features_merge_data))","a8b24d72":"# Now transform these Categorical Features into Number using get_dummies function (One Hot Encoding)\nfinal_merge_data = pd.get_dummies(merge_data, columns=cat_features_merge_data, drop_first=True)\nfinal_merge_data.shape","fcf8309a":"# Drop duplicate columns, in-case we have. But we do not have any.\nfinal_merge_data = final_merge_data.loc[:, ~final_merge_data.columns.duplicated()]\nfinal_merge_data.shape","305b71aa":"final_merge_data.head()","6bc64420":"final_merge_data.tail()","5b40d158":"# splitting back.\nfinal_train_data = final_merge_data.iloc[:1460, :]\nfinal_test_data = final_merge_data.iloc[1460:, :]\n","e4eec1b9":"final_test_data.drop(['SalePrice'], axis = 1, inplace = True)","7155c35a":"final_train_data.shape, final_test_data.shape","7aaa114e":"# Defining Feature and Target.\n#print (final_train_data.columns)\n#features = final_train_data.drop(['Id', 'SalePrice']).columns\ntarget = final_train_data[\"SalePrice\"]\n#print (\"Features\", features)\nprint (\"Target\", target.head())","8cef1abd":"# split the train_data into 2 DF's aka X_train, X_test, y_train, y_test.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(final_train_data.drop(['SalePrice'], axis = 1), target, test_size=0.2)\n\nprint (X_train.shape, y_train.shape)\nprint (X_test.shape, y_test.shape)","3656a95a":"# test_data \nX_test_df  = final_test_data.copy()\nprint (X_test_df.shape)","e726b60f":"# machine learning\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier","ec0a522c":"lm = LinearRegression()\nlm.fit(X_train,y_train)","98567eb7":"Y_pred_lm = lm.predict(X_test)\n#print(Y_pred_lr)\nacc_lm = round(lm.score(X_train, y_train) * 100, 2)\nprint(\"Accuracy (LinearRegression)\", acc_lm)","2472f064":"# Predicting on test_data\nY_pred_test_df = lm.predict(X_test_df)\nY_pred_test_df ","16d4d86a":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, y_train)\nY_pred_dt = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, y_train) * 100, 2)\nprint(\"Accuracy (Decision Tree)\", acc_decision_tree)","5d3d4fc8":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, y_train)\nY_pred_rf = random_forest.predict(X_test)\nacc_random_forest = round(random_forest.score(X_train, y_train) * 100, 2)\nprint(\"Accuracy (random Forest)\", acc_random_forest)","3c42dbd1":"modelling_score = pd.DataFrame({\n    'Model': ['Linear Regression','Random Forest','Decision Tree'],\n    'Score': [acc_lm, acc_random_forest, acc_decision_tree]})","b0219965":"modelling_score.sort_values(by='Score', ascending=False)","1d4ed039":"X_test_df.shape","c965a42d":"# Predicting on actual test_data\nY_pred_test_df = random_forest.predict(X_test_df)\nY_pred_test_df ","cd8e131b":"X_test_df.head()","cbc932f1":"X_test_df.index","d9aa6ac6":"submission_example_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsubmission_example_data.shape","4f3e4054":"submission_example_data.head()","46a24add":"#submission = pd.DataFrame( { 'Id': X_test_df.index , 'SalePrice': Y_pred_test_df } )\nsubmission = pd.DataFrame( { 'Id': submission_example_data['Id'] , 'SalePrice': Y_pred_test_df } )","c315ca5e":"print(\"Submission File Shape \",submission.shape)\nsubmission.head()","4f81405c":"submission.to_csv( '\/kaggle\/working\/house_prices_submission.csv' , index = False )","fd5bdabd":"**[GarageCond]**: Garage condition\n\n       Ex\tExcellent\n       Gd\tGood\n       TA\tTypical\/Average\n       Fa\tFair\n       Po\tPoor\n       NA\tNo Garage","c5514387":"# Importing Libraries","09ddd69b":"Decided to go with Random Forest.. as the Score with it is the highest. Though Decision Tree is also same.","c9615154":"Identify best model from above 3.","b4c28bac":"BsmtFinType2: Rating of basement finished area (if multiple types)\n\n       GLQ\tGood Living Quarters\n       ALQ\tAverage Living Quarters\n       BLQ\tBelow Average Living Quarters\t\n       Rec\tAverage Rec Room\n       LwQ\tLow Quality\n       Unf\tUnfinshed\n       NA\tNo Basement","aeab682a":"# Modeling","c02b67e9":"From the above Box and Scatter plot, its clear that TA is common, and EX is just a outlier.","495e4700":"# Filling up Missing Values.\nLets work on filling up the Null Values.\nFor that we have to understand the each variable deeply. Will check the data_description.txt, from here we will get more details.","45a0b836":"I did checked for remaining features which are NaN, and seems all are Categorical. So we will replace NaN with the MODE.\n\nBsmtFinType1: Rating of basement finished area\n\n       GLQ\tGood Living Quarters\n       ALQ\tAverage Living Quarters\n       BLQ\tBelow Average Living Quarters\t\n       Rec\tAverage Rec Room\n       LwQ\tLow Quality\n       Unf\tUnfinshed\n       NA\tNo Basement\n       \nBsmtCond: Evaluates the general condition of the basement\n\n       Ex\tExcellent\n       Gd\tGood\n       TA\tTypical - slight dampness allowed\n       Fa\tFair - dampness or some cracking or settling\n       Po\tPoor - Severe cracking, settling, or wetness\n       NA\tNo Basement\n       \nBsmtQual: Evaluates the height of the basement\n\n       Ex\tExcellent (100+ inches)\t\n       Gd\tGood (90-99 inches)\n       TA\tTypical (80-89 inches)\n       Fa\tFair (70-79 inches)\n       Po\tPoor (<70 inches\n       NA\tNo Basement       \n       \nMasVnrType: Masonry veneer type\n\n       BrkCmn\tBrick Common\n       BrkFace\tBrick Face\n       CBlock\tCinder Block\n       None\tNone\n       Stone\tStone\n       \nElectrical: Electrical system\n\n       SBrkr\tStandard Circuit Breakers & Romex\n       FuseA\tFuse Box over 60 AMP and all Romex wiring (Average)\t\n       FuseF\t60 AMP Fuse Box and mostly Romex wiring (Fair)\n       FuseP\t60 AMP Fuse Box and mostly knob & tube wiring (poor)\n       Mix\tMixed       ","c4a73b50":"We are done with Training Data-Frame.. and now have to do the same with Test Data Frame.","a7764939":"MasVnrArea: Masonry veneer area in square feet","b0f25de5":"# Submission","27c5ab73":"FireplaceQu: Fireplace quality\n\n       Ex\tExcellent - Exceptional Masonry Fireplace\n       Gd\tGood - Masonry Fireplace in main level\n       TA\tAverage - Prefabricated Fireplace in main living area or Masonry Fireplace in basement\n       Fa\tFair - Prefabricated Fireplace in basement\n       Po\tPoor - Ben Franklin Stove\n       NA\tNo Fireplace\n","df5e70ff":"LotFrontage: Linear feet of street connected to property","9594797d":"GarageQual: Garage quality\n\n       Ex\tExcellent\n       Gd\tGood\n       TA\tTypical\/Average\n       Fa\tFair\n       Po\tPoor\n       NA\tNo Garage","a220e37b":"GarageType: Garage location\n\t\t\n       2Types\tMore than one type of garage\n       Attchd\tAttached to home\n       Basment\tBasement Garage\n       BuiltIn\tBuilt-In (Garage part of house - typically has room above garage)\n       CarPort\tCar Port\n       Detchd\tDetached from home\n       NA\tNo Garage","b402a7f3":"Seems in train_data and test_data... we found some categorical value has mismatch in values. I mean for feature Condition2, RRAe is in train_data but not in test_data... and when we did transofrm, the number of columns differ. So we will first merge the test and train.. so one more round of missing handling, and then proceed with modeling.","a53791b2":"# Correlation Heatmap","dfa1ed2f":"BsmtExposure: Refers to walkout or garden level walls\n\n       Gd\tGood Exposure\n       Av\tAverage Exposure (split levels or foyers typically score average or above)\t\n       Mn\tMimimum Exposure\n       No\tNo Exposure\n       NA\tNo Basement","21797f5e":"Seems fiels \"MSZoning\" is an addational in test_data to have NaN, rest all are same, so will apply same approach to fill NaN.\nBut before that will see how MSZoning is affected our SalePrice (This we will check based on train_data)\n\nMSZoning: Identifies the general zoning classification of the sale.\n\t\t\n       A\tAgriculture\n       C\tCommercial\n       FV\tFloating Village Residential\n       I\tIndustrial\n       RH\tResidential High Density\n       RL\tResidential Low Density\n       RP\tResidential Low Density Park \n       RM\tResidential Medium Density","a87dd424":"# Handling Categorical Features","6c955327":"GarageYrBlt: Year garage was built","13fc899f":"GarageFinish: Interior finish of the garage\n\n       Fin\tFinished\n       RFn\tRough Finished\t\n       Unf\tUnfinished\n       NA\tNo Garage\n       \nWhen variables are not continuous but categorical or ordinal scaled. Then it is not recommend to use Pearson correlation analysis.","3384775a":"From above we see that there are 19 columns which has Null values in it, out of which top 4 such as PoolQC; MiscFeature; Alley; Fence are having more than 50% missing values. So will not consider these columns in our model.\nApart from these 4.. rest all are below 50%.. and we will see to fix the Null or missing values below."}}