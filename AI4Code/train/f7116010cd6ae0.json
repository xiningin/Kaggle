{"cell_type":{"1227779b":"code","4a1c6957":"code","87714df8":"code","2953d2c0":"code","b43f4f60":"code","638bf69e":"code","70acc343":"code","15ed1375":"code","37a8c1a5":"code","6103ecba":"code","4c31f920":"code","f8ae7ee2":"code","828b0adf":"code","ce81e847":"code","f62fa6fe":"code","f4b13d8e":"code","6dff05bf":"code","6d5d490d":"code","951a31d1":"code","b8f7d06c":"code","2850e01b":"code","59e09ee5":"code","a6b52fa8":"code","e007428c":"code","ee0c900a":"code","c2f4d968":"code","3aed78f8":"code","b0914804":"code","be960ad3":"code","7f0b31e6":"code","a36cd3ad":"code","08ec5b57":"code","336d0685":"code","2b2001ba":"code","dc8f4221":"code","12546fc0":"code","4602d02b":"code","facbfbf6":"code","c7d4571a":"code","120c07ce":"code","3c8ad710":"code","d8425bc7":"code","2c28469a":"code","71420885":"code","cacc5e13":"code","0296ea55":"code","f539478b":"code","337460ec":"code","23bccc27":"code","56dcda8a":"code","fffc2ffd":"code","e7e02a08":"code","e2050d73":"code","94d3d665":"code","76672a7f":"code","9ec02f59":"code","dd5abad4":"code","699beb34":"code","51062874":"code","2283242d":"code","1f54c426":"code","54c487c1":"code","cc4ce14d":"code","9de72369":"code","52b44773":"code","01e6e7ec":"code","02a0f7fb":"code","4250d23a":"code","5e8b8e97":"code","2757e614":"code","5a793adb":"code","b90304f5":"code","cab4eccc":"code","f55b7769":"code","c689a369":"code","874e8e34":"code","2f729101":"code","fcb5de57":"code","fb20a33d":"code","9bc54763":"code","95eb9ece":"code","6ad726d8":"code","2374c7b8":"code","7a9f05a6":"code","f28fa70d":"code","7a87261f":"code","79fcf9cf":"markdown","3046643f":"markdown","22adf8cd":"markdown","66256a53":"markdown","de492f11":"markdown","91e7aa1d":"markdown","2a589fd0":"markdown","7187808a":"markdown","2ad6ee78":"markdown","502af93e":"markdown","4090f120":"markdown","e555e227":"markdown","cb435019":"markdown","639b9de3":"markdown","e2cab3d6":"markdown","390423f1":"markdown","44de715a":"markdown","b15b5c85":"markdown","e477e0b0":"markdown","abf89ab9":"markdown","a2dda894":"markdown","84afd9f0":"markdown","69191d23":"markdown","5aa2b80a":"markdown","35e31736":"markdown","da2c3fc1":"markdown","f1a419d2":"markdown","611d1389":"markdown","f24f6af7":"markdown","f90bc74d":"markdown","48051b1e":"markdown","4c9f5a8c":"markdown","921a56a2":"markdown","9f277933":"markdown","a4e9fd0a":"markdown","a381070f":"markdown","6d9bc044":"markdown","c8b62ca3":"markdown","52fdcd55":"markdown","40d7be10":"markdown","2416fb1f":"markdown","aac4b9a1":"markdown","fddc192e":"markdown","452842f0":"markdown","3c4a8218":"markdown","cbb42e8f":"markdown","cfbdad9d":"markdown","d7395412":"markdown","d33ad274":"markdown","2a09ddff":"markdown","8da07c69":"markdown","71e9fdb5":"markdown","e5e6fa42":"markdown","179d9e89":"markdown","32fad37b":"markdown","7e6ada5d":"markdown","ed7e3318":"markdown","de280ac8":"markdown","095a989d":"markdown","7c0dd4e6":"markdown","a37ee377":"markdown","07d7f13b":"markdown","130ec317":"markdown","c26465b8":"markdown","1ec3ac7d":"markdown","c7e0cfea":"markdown","d1082a1e":"markdown","1bc23263":"markdown","3daf5d49":"markdown","3cbc4714":"markdown","ea8af137":"markdown","c33e76c9":"markdown","07de5a27":"markdown"},"source":{"1227779b":"# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# visualization\nimport plotly.offline as py \npy.init_notebook_mode(connected=True)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom mpl_toolkits.mplot3d import Axes3D\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots \ncolors = ['#06344d','#00b2ff']\nsns.set(palette=colors, font='Serif', style='white', rc={'axes.facecolor':'#f1f1f1', 'figure.facecolor':'#f1f1f1'})\nsns.palplot(colors)\n\n# scaling and train test split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\n# creating a model\nfrom sklearn import linear_model\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\n\n# evaluation on test data\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score\nfrom sklearn.metrics import classification_report\n\nimport warnings \nwarnings.filterwarnings('ignore')","4a1c6957":"df=pd.read_csv('..\/input\/marketing-seris-customer-lifetime-value\/squark_automotive_CLV_training_data.csv')","87714df8":"df.head()","2953d2c0":"df.rename(columns={'Customer Lifetime Value':'CLV'},inplace=True)","b43f4f60":"df.info()","638bf69e":"df.dropna(axis=0, how='any', inplace=True)","70acc343":"numerical_cols = df.select_dtypes(include=[\"int64\",\"float64\"])","15ed1375":"numerical_cols.columns","37a8c1a5":"cat_cols = df.select_dtypes(include=\"object\")","6103ecba":"cat_cols.drop(\"Effective To Date\",axis=1,inplace=True)\ncat_cols.drop(\"Customer\",axis=1,inplace=True)","4c31f920":"numerical_cols.describe()","f8ae7ee2":"cat_cols.describe()","828b0adf":"for col in cat_cols:\n    print(f'''Value count kolom {col}:''')\n    print(cat_cols[col].value_counts())\n    print()","ce81e847":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib import colors\nfrom matplotlib.ticker import PercentFormatter\n\n# Fixing random state for reproducibility\nnp.random.seed(19680801)","f62fa6fe":"fig, axs = plt.subplots(1, 2,figsize=(15,5))\n\n# N is the count in each bin, bins is the lower-limit of the bin\nN, bins, patches = axs[0].hist(df['CLV'], bins=50)\n\n# We'll color code by height, but you could use any scalar\nfracs = N \/ N.max()\n\n# we need to normalize the data to 0..1 for the full range of the colormap\nnorm = colors.Normalize(fracs.min(), fracs.max())\n\n# Now, we'll loop through our objects and set the color of each accordingly\nfor thisfrac, thispatch in zip(fracs, patches):\n    color = plt.cm.viridis(norm(thisfrac))\n    thispatch.set_facecolor(color)\n\n# We can also normalize our inputs by the total number of counts\naxs[1].boxplot(df['CLV'])\naxs[0].set(xlabel='CLV', ylabel='', title='CLV Distribution')\naxs[1].set(xlabel='CLV', ylabel='', title='CLV Boxplot Distribution')\n# Now we format the y-axis to display percentage\naxs[1].yaxis.set_major_formatter(PercentFormatter(xmax=1))","f4b13d8e":"df[\"CLV\"].skew()","6dff05bf":"fig, axs = plt.subplots(1, 2,figsize=(15,5))\n\n# N is the count in each bin, bins is the lower-limit of the bin\nN, bins, patches = axs[0].hist(df['Income'], bins=50)\n\n# We'll color code by height, but you could use any scalar\nfracs = N \/ N.max()\n\n# we need to normalize the data to 0..1 for the full range of the colormap\nnorm = colors.Normalize(fracs.min(), fracs.max())\n\n# Now, we'll loop through our objects and set the color of each accordingly\nfor thisfrac, thispatch in zip(fracs, patches):\n    color = plt.cm.viridis(norm(thisfrac))\n    thispatch.set_facecolor(color)\n\n# We can also normalize our inputs by the total number of counts\naxs[1].boxplot(df['Income'])\naxs[0].set(xlabel='Income', ylabel='', title='Income Distribution')\naxs[1].set(xlabel='Income', ylabel='', title='Income Boxplot Distribution')\n# Now we format the y-axis to display percentage\naxs[1].yaxis.set_major_formatter(PercentFormatter(xmax=1))","6d5d490d":"fig, axs = plt.subplots(1, 2,figsize=(15,5))\n\n# N is the count in each bin, bins is the lower-limit of the bin\nN, bins, patches = axs[0].hist(df[\"Monthly Premium Auto\"], bins=50)\n\n# We'll color code by height, but you could use any scalar\nfracs = N \/ N.max()\n\n# we need to normalize the data to 0..1 for the full range of the colormap\nnorm = colors.Normalize(fracs.min(), fracs.max())\n\n# Now, we'll loop through our objects and set the color of each accordingly\nfor thisfrac, thispatch in zip(fracs, patches):\n    color = plt.cm.viridis(norm(thisfrac))\n    thispatch.set_facecolor(color)\n\n# We can also normalize our inputs by the total number of counts\naxs[1].boxplot(df[\"Monthly Premium Auto\"])\naxs[0].set(xlabel=\"Monthly Premium Auto\", ylabel='', title='Monthly Premium Auto Distribution')\naxs[1].set(xlabel=\"Monthly Premium Auto\", ylabel='', title='Monthly Premium Auto Boxplot Distribution')\n# Now we format the y-axis to display percentage\naxs[1].yaxis.set_major_formatter(PercentFormatter(xmax=1))","951a31d1":"fig, axs = plt.subplots(1, 2,figsize=(15,5))\n\n# N is the count in each bin, bins is the lower-limit of the bin\nN, bins, patches = axs[0].hist(df['Months Since Last Claim'], bins=50)\n\n# We'll color code by height, but you could use any scalar\nfracs = N \/ N.max()\n\n# we need to normalize the data to 0..1 for the full range of the colormap\nnorm = colors.Normalize(fracs.min(), fracs.max())\n\n# Now, we'll loop through our objects and set the color of each accordingly\nfor thisfrac, thispatch in zip(fracs, patches):\n    color = plt.cm.viridis(norm(thisfrac))\n    thispatch.set_facecolor(color)\n\n# We can also normalize our inputs by the total number of counts\naxs[1].boxplot(df['Months Since Last Claim'])\naxs[0].set(xlabel='Months Since Last Claim', ylabel='', title='Months Since Last Claim Distribution')\naxs[1].set(xlabel='Months Since Last Claim', ylabel='', title='Months Since Last Claim Boxplot Distribution')\n# Now we format the y-axis to display percentage\naxs[1].yaxis.set_major_formatter(PercentFormatter(xmax=1))","b8f7d06c":"fig, axs = plt.subplots(1, 2,figsize=(15,5))\n\n# N is the count in each bin, bins is the lower-limit of the bin\nN, bins, patches = axs[0].hist(df['Months Since Policy Inception'], bins=50)\n\n# We'll color code by height, but you could use any scalar\nfracs = N \/ N.max()\n\n# we need to normalize the data to 0..1 for the full range of the colormap\nnorm = colors.Normalize(fracs.min(), fracs.max())\n\n# Now, we'll loop through our objects and set the color of each accordingly\nfor thisfrac, thispatch in zip(fracs, patches):\n    color = plt.cm.viridis(norm(thisfrac))\n    thispatch.set_facecolor(color)\n\n# We can also normalize our inputs by the total number of counts\naxs[1].boxplot(df['Months Since Policy Inception'])\naxs[0].set(xlabel='Months Since Policy Inception', ylabel='', title='Months Since Policy Inception Distribution')\naxs[1].set(xlabel='Months Since Policy Inception', ylabel='', title='Months Since Policy Inception Boxplot Distribution')\n# Now we format the y-axis to display percentage\naxs[1].yaxis.set_major_formatter(PercentFormatter(xmax=1))","2850e01b":"fig, axs = plt.subplots(1, 2,figsize=(15,5))\n\n# N is the count in each bin, bins is the lower-limit of the bin\nN, bins, patches = axs[0].hist(df['Total Claim Amount'], bins=50)\n\n# We'll color code by height, but you could use any scalar\nfracs = N \/ N.max()\n\n# we need to normalize the data to 0..1 for the full range of the colormap\nnorm = colors.Normalize(fracs.min(), fracs.max())\n\n# Now, we'll loop through our objects and set the color of each accordingly\nfor thisfrac, thispatch in zip(fracs, patches):\n    color = plt.cm.viridis(norm(thisfrac))\n    thispatch.set_facecolor(color)\n\n# We can also normalize our inputs by the total number of counts\naxs[1].boxplot(df['Total Claim Amount'])\naxs[0].set(xlabel='Total Claim Amount', ylabel='', title='Total Claim Amount Distribution')\naxs[1].set(xlabel='Total Claim Amount', ylabel='', title='Total Claim Amount Boxplot Distribution')\n# Now we format the y-axis to display percentage\naxs[1].yaxis.set_major_formatter(PercentFormatter(xmax=1))","59e09ee5":"f, axes = plt.subplots(1, 2,figsize=(15,5))\nsns.scatterplot(x=\"Income\",y='CLV', data=df, ax=axes[0])\nsns.scatterplot(x=\"Monthly Premium Auto\",y='CLV', data=df, ax=axes[1])\nsns.despine(bottom=True, left=True)\naxes[0].set(xlabel='Income', ylabel='CLV', title='CLV Vs Income')\naxes[1].set(xlabel='Monthly Premium Auto', ylabel='CLV', title='CLV Vs Monthly Premium Auto')\naxes[1].yaxis.set_label_position(\"right\")\naxes[1].yaxis.tick_right()\n\nf, axes = plt.subplots(1, 2,figsize=(15,5))\nsns.scatterplot(x=\"Months Since Last Claim\",y='CLV', data=df, ax=axes[0])\nsns.scatterplot(x=\"Months Since Policy Inception\",y='CLV', data=df, ax=axes[1])\nsns.despine(bottom=True, left=True)\naxes[0].set(xlabel='Months Since Last Claim', ylabel='CLV', title='CLV Vs Months Since Last Claim ')\naxes[1].set(xlabel='Months Since Policy Inception', ylabel='CLV', title='CLV Vs Months Since Policy Inception')\naxes[1].yaxis.set_label_position(\"right\")\naxes[1].yaxis.tick_right()\n\nf, axes = plt.subplots(1, 2,figsize=(15,5))\nsns.scatterplot(x=\"Total Claim Amount\",y='CLV', data=df, ax=axes[0])\nsns.scatterplot(x=\"Number of Open Complaints\",y='CLV', data=df, ax=axes[1])\nsns.despine(bottom=True, left=True)\naxes[0].set(xlabel='Total Claim Amount', ylabel='CLV', title='CLV Vs Total Claim Amount')\naxes[1].set(xlabel='Number of Open Complaints', ylabel='CLV', title='CLV vs Number of Open Complaints')\naxes[1].yaxis.set_label_position(\"right\")\naxes[1].yaxis.tick_right()\n","a6b52fa8":"f, axes = plt.subplots(1, 2,figsize=(15,5))\np1=sns.countplot(df[\"State\"], ax=axes[0])\nfor p in p1.patches:\n    height = p.get_height()\n    p1.text(p.get_x()+p.get_width()\/2,height + 50, height,ha='center')\np2=sns.countplot(df[\"Response\"], ax=axes[1])\nfor p in p2.patches:\n    height = p.get_height()\n    p2.text(p.get_x()+p.get_width()\/2,height + 50, height,ha='center')\nsns.despine(bottom=True, left=True)\naxes[0].set(title='State')\naxes[1].set(title='Response')\naxes[1].yaxis.set_label_position(\"right\")\naxes[1].yaxis.tick_right()\n\nf, axes = plt.subplots(1, 2,figsize=(15,7))\np1=sns.countplot(df[\"Coverage\"], ax=axes[0])\nfor p in p1.patches:\n    height = p.get_height()\n    p1.text(p.get_x()+p.get_width()\/2,height + 50, height,ha='center')\np2=sns.countplot(df[\"Education\"], ax=axes[1])\nfor p in p2.patches:\n    height = p.get_height()\n    p2.text(p.get_x()+p.get_width()\/2,height + 50, height,ha='center')\nsns.despine(bottom=True, left=True)\naxes[0].set(title='Coverage')\naxes[1].set(title='Education')\naxes[1].yaxis.set_label_position(\"right\")\naxes[1].yaxis.tick_right()\nfor label in axes[1].get_xmajorticklabels() + axes[1].get_xmajorticklabels():\n    label.set_rotation(30)\n    label.set_horizontalalignment(\"right\")\n\nf, axes = plt.subplots(1, 2,figsize=(15,7))\np1=sns.countplot(df[\"EmploymentStatus\"], ax=axes[0])\nfor p in p1.patches:\n    height = p.get_height()\n    p1.text(p.get_x()+p.get_width()\/2,height + 50, height,ha='center')\np2=sns.countplot(df[\"Policy Type\"], ax=axes[1])\nfor p in p2.patches:\n    height = p.get_height()\n    p2.text(p.get_x()+p.get_width()\/2,height + 50, height,ha='center')\nsns.despine(bottom=True, left=True)\naxes[0].set(title='EmploymentStatus')\naxes[1].set(title='Policy Type')\naxes[1].yaxis.set_label_position(\"right\")\naxes[1].yaxis.tick_right()\nfor label in axes[0].get_xmajorticklabels() + axes[1].get_xmajorticklabels():\n    label.set_rotation(30)\n    label.set_horizontalalignment(\"right\")","e007428c":"df[\"EmploymentStatus\"].value_counts(normalize=True)*100","ee0c900a":"f, axes = plt.subplots(1, 2,figsize=(15,5))\np1=sns.countplot(df[\"Gender\"], ax=axes[0])\nfor p in p1.patches:\n    height = p.get_height()\n    p1.text(p.get_x()+p.get_width()\/2,height + 50, height,ha='center')\np2=sns.countplot(df[\"Location Code\"], ax=axes[1])\nfor p in p2.patches:\n    height = p.get_height()\n    p2.text(p.get_x()+p.get_width()\/2,height + 50, height,ha='center')\nsns.despine(bottom=True, left=True)\naxes[0].set(title='Gender')\naxes[1].set(title='Location Code')\naxes[1].yaxis.set_label_position(\"right\")\naxes[1].yaxis.tick_right()\n\nf, axes = plt.subplots(1, 2,figsize=(15,5))\np1=sns.countplot(df[\"Marital Status\"], ax=axes[0])\nfor p in p1.patches:\n    height = p.get_height()\n    p1.text(p.get_x()+p.get_width()\/2,height + 50, height,ha='center')\np2=sns.barplot(df[\"Marital Status\"],df[\"CLV\"], ax=axes[1])\nfor p in p2.patches:\n    height = p.get_height()\n    p2.text(p.get_x()+p.get_width()\/2,height + 50, height,ha='center')\nsns.despine(bottom=True, left=True)\naxes[0].set(title='Marital Status')\naxes[1].set(title='Amount CLV Merital Status')\naxes[1].yaxis.set_label_position(\"right\")\naxes[1].yaxis.tick_right()\n\nplt.figure(figsize=(15,5))\np3=sns.countplot(df[\"Policy\"])\nfor p in p3.patches:\n    height = p.get_height()\n    p3.text(p.get_x()+p.get_width()\/2,height + 50, height,ha='center')\n\n","c2f4d968":"f, axes = plt.subplots(1, 2,figsize=(15,5))\np1=sns.countplot(df[\"Renew Offer Type\"], ax=axes[0])\nfor p in p1.patches:\n    height = p.get_height()\n    p1.text(p.get_x()+p.get_width()\/2,height + 50, height,ha='center')\np2=sns.countplot(df[\"Sales Channel\"], ax=axes[1])\nfor p in p2.patches:\n    height = p.get_height()\n    p2.text(p.get_x()+p.get_width()\/2,height + 50, height,ha='center')\nsns.despine(bottom=True, left=True)\naxes[0].set(title='Renew Offer Type')\naxes[1].set(title='Sales Channel')\naxes[1].yaxis.set_label_position(\"right\")\naxes[1].yaxis.tick_right()\n\nf, axes = plt.subplots(1, 2,figsize=(15,5))\np3=sns.countplot(df[\"Vehicle Class\"], ax=axes[0])\nfor p in p3.patches:\n    height = p.get_height()\n    p3.text(p.get_x()+p.get_width()\/2,height + 50, height,ha='center')\np4=sns.countplot(df[\"Vehicle Size\"], ax=axes[1])\nfor p in p4.patches:\n    height = p.get_height()\n    p4.text(p.get_x()+p.get_width()\/2,height + 50, height,ha='center')\nsns.despine(bottom=True, left=True)\naxes[0].set(title='Vehicle Class')\naxes[1].set(title='Vehicle Size')\naxes[1].yaxis.set_label_position(\"right\")\naxes[1].yaxis.tick_right()\nfor label in axes[0].get_xmajorticklabels() + axes[1].get_xmajorticklabels():\n    label.set_rotation(30)\n    label.set_horizontalalignment(\"right\")","3aed78f8":"f, axes = plt.subplots(1, 2,figsize=(15,5))\np1=sns.countplot(df[\"Number of Open Complaints\"], ax=axes[0])\nfor p in p1.patches:\n    height = p.get_height()\n    p1.text(p.get_x()+p.get_width()\/2,height + 50, height,ha='center')\np2=sns.countplot(df[\"Number of Policies\"], ax=axes[1])\nfor p in p2.patches:\n    height = p.get_height()\n    p2.text(p.get_x()+p.get_width()\/2,height + 50, height,ha='center')\nsns.despine(bottom=True, left=True)\naxes[0].set(title='Number of Open Complaints')\naxes[1].set(title='Number of Policies')\naxes[1].yaxis.set_label_position(\"right\")\naxes[1].yaxis.tick_right()","b0914804":"df['Effective To Date']=pd.to_datetime(df['Effective To Date'],infer_datetime_format=True)","be960ad3":"df[\"Months\"] = df[\"Effective To Date\"].dt.month\ndf['Months'] = df['Months'].astype('object')","7f0b31e6":"plt.figure(figsize=(15,7))\nax=sns.countplot(x=df['Months'], data=df);\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2,height + 50, height,ha='center')","a36cd3ad":"heatmap = df[['CLV', 'Income', 'Monthly Premium Auto','Total Claim Amount']]","08ec5b57":"plt.figure(figsize=(7,4))\nsns.heatmap(heatmap.corr(),annot=True)","336d0685":"import scipy.stats as stats\nstats.shapiro(df['CLV'])","2b2001ba":"columns_cat = list(cat_cols.columns)\ncolumns_cat","dc8f4221":"manwhiteneyy = []\nanova = []\n\nfor i in columns_cat:\n    if (df[i].nunique()>2):\n        anova.append(i)\n    else:\n        manwhiteneyy.append(i)\nprint(\"Anova:\",anova)\nprint(\"TTest:\",manwhiteneyy)","12546fc0":"sns.displot(df, x=\"CLV\", hue=\"Response\", kind=\"kde\", fill=True)","4602d02b":"yes=df[df['Response']=='No']['CLV']\nno=df[df['Response']=='Yes']['CLV']","facbfbf6":"f = stats.mannwhitneyu(yes,no)\nprint(f)","c7d4571a":"male=df[df['Gender']=='M']['CLV']\nfemale=df[df['Gender']=='F']['CLV']","120c07ce":"b =stats.mannwhitneyu(male,female)\nprint(b)","3c8ad710":"ca=df[df['State']=='California']['CLV']\nOr=df[df['State']=='Oregon']['CLV']\nAr=df[df['State']=='Arizona']['CLV']\nNe=df[df['State']=='Nevada']['CLV']\nWa=df[df['State']=='Washington']['CLV']","d8425bc7":"a = stats.kruskal(ca,Or,Ar,Ne,Wa)\nprint(a)","2c28469a":"stats.f_oneway(ca,Or,Ar,Ne,Wa)\nprint(b)","71420885":"sns.displot(df, x=\"CLV\", hue=\"Coverage\", kind=\"kde\", fill=True)","cacc5e13":"Ba=df[df['Coverage']=='Basic']['CLV']\nEx=df[df['Coverage']=='Extended']['CLV']\nPr=df[df['Coverage']=='Premium']['CLV']","0296ea55":"c =stats.kruskal(Ba,Ex,Pr)\nprint(c)","f539478b":"stats.f_oneway(Ba,Ex,Pr)","337460ec":"Ba=df[df['Education']=='Bachelor']['CLV']\nCo=df[df['Education']=='College']['CLV']\nHi=df[df['Education']=='High School or Below']['CLV']\nMa=df[df['Education']=='Master']['CLV']\nDa=df[df['Education']=='Doctor']['CLV']","23bccc27":"d= stats.kruskal(Ba,Co,Hi,Ma,Da)\nprint(d)","56dcda8a":"plt.figure(figsize=(13,5))\nsns.displot(df, x=\"CLV\", hue=\"Education\", kind=\"kde\", fill=False)","fffc2ffd":"# use LabelEncoder() to encode other categorical columns:\nfrom sklearn.preprocessing import LabelEncoder\nfor col in cat_cols:\n    le = LabelEncoder()\n    le.fit(cat_cols[col])\n    cat_cols[col] = le.transform(cat_cols[col])\ncat_cols.head()","e7e02a08":"dfn = pd.concat([numerical_cols,cat_cols],axis=1)\ndfn.head()","e2050d73":"import statsmodels.api as sm","94d3d665":"X=dfn.drop(['CLV'],axis=1)\ny=dfn['CLV']\nX_constant=sm.add_constant(X)\nlin_reg=sm.OLS(y,X_constant).fit()\nlin_reg.summary()","76672a7f":"import statsmodels.tsa.api as smt #timeseries analysis\n\nacf = smt.graphics.plot_acf(lin_reg.resid, lags=40 , alpha=0.05)#auto_correlation value, lags= previous value\nacf.show()","9ec02f59":"from scipy import stats\nprint(stats.jarque_bera(lin_reg.resid))","dd5abad4":"import seaborn as sns\n\nsns.distplot(lin_reg.resid)\nplt.show()","699beb34":"import statsmodels.api as sm\nsm.stats.diagnostic.linear_rainbow(res=lin_reg, frac=0.5)#the fraction of the data to include in the center model","51062874":"lin_reg.resid.mean()","2283242d":"import statsmodels.stats.api as sms","1f54c426":"name=['F-statistic','p-value']\ntest=sms.het_goldfeldquandt(lin_reg.resid,lin_reg.model.exog)\n\ntest","54c487c1":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = [variance_inflation_factor(X_constant.values, i) for i in range(X_constant.shape[1])]\npd.DataFrame({'vif': vif[1:]}, index=X.columns)","cc4ce14d":"from sklearn.model_selection import train_test_split\nX=dfn.drop(['CLV','Policy Type','Policy', 'Income'],axis=1)\ny=dfn['CLV']          \ny =np.log(y)\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.30,random_state=42)","9de72369":"print(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","52b44773":"from sklearn.linear_model import LinearRegression","01e6e7ec":"lr = LinearRegression()\nmodel = lr.fit(X_train,y_train)\nprint(f'R^2 score for train: {lr.score(X_train, y_train)}')\nprint(f'R^2 score for test: {lr.score(X_test, y_test)}')","02a0f7fb":"y_pred = model.predict(X_test)","4250d23a":"from sklearn.metrics import mean_squared_error,r2_score,mean_absolute_error","5e8b8e97":"print('RMSE:',np.sqrt(mean_squared_error(y_test,y_pred)))\nprint(\"MAE\",mean_absolute_error(y_test,y_pred))\nprint('R-squared:',r2_score(y_test,y_pred)) ","2757e614":"#Feature Selection by Recursive Backward Elimination\ncols = list(X.columns)\npmax = 1\nwhile (len(cols)>0):\n    p= []\n    X = X[cols]\n    Xc = sm.add_constant(X)\n    model = sm.OLS(y,Xc).fit()\n    p = pd.Series(model.pvalues.values[1:],index = cols)      \n    pmax = max(p)\n    feature_with_p_max = p.idxmax()\n    if(pmax>0.05):\n        cols.remove(feature_with_p_max)\n    else:\n        break\nselected_features = cols\nprint(selected_features)","5a793adb":"X_new = X[['Monthly Premium Auto', 'Number of Open Complaints', 'Number of Policies', 'Total Claim Amount', 'Response', 'EmploymentStatus', 'Renew Offer Type']]","b90304f5":"X_constant_new=sm.add_constant(X_new)\nlin_reg=sm.OLS(y,X_constant_new).fit()\nlin_reg.summary()","cab4eccc":"from sklearn.linear_model import Ridge, Lasso","f55b7769":"ridge=Ridge(alpha=0.1,normalize=True)\nridge.fit(X_train,y_train)\nprint('Ridge Regression')\ny_pred = ridge.predict(X_test)\nprint('RMSE:', np.sqrt(mean_squared_error(y_test, y_pred)))\nprint('R-squared:',r2_score(y_test,y_pred)) ","c689a369":"lasso = Lasso(random_state=1)\nlasso.fit(X_train,y_train)\nprint('Lasso Regression')\ny_pred = lasso.predict(X_test)\nprint('RMSE:', np.sqrt(mean_squared_error(y_test, y_pred)))\nprint('R-squared:',r2_score(y_test,y_pred)) ","874e8e34":"from sklearn.tree import DecisionTreeRegressor\ndt = DecisionTreeRegressor(random_state=1)\n\ndt.fit(X_train,y_train)\ny_pred=dt.predict(X_test)\nprint('RMSE:',np.sqrt(mean_squared_error(y_test,y_pred)))\nprint(\"MAE\",mean_absolute_error(y_test,y_pred))\nprint('R-squared:',r2_score(y_test,y_pred)) ","2f729101":"from sklearn.model_selection import train_test_split\nX=dfn.drop(['CLV','Policy Type','Policy', 'Income'],axis=1)\ny=np.log(dfn['CLV'])         \nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.30,random_state=42)","fcb5de57":"from sklearn.ensemble import RandomForestRegressor","fb20a33d":"rf = RandomForestRegressor(random_state=1)\n\nrf.fit(X_train,y_train)\n\ny_pred=rf.predict(X_test)\nprint('RMSE:',np.sqrt(mean_squared_error(y_test,y_pred)))\nprint(\"MAE\",mean_absolute_error(y_test,y_pred))\nprint('R-squared:',r2_score(y_test,y_pred)) ","9bc54763":"from sklearn.model_selection import GridSearchCV\nrf = RandomForestRegressor()\nparams = {\n        'max_depth' : [10,20,30],\n        \n        'n_estimators' : [100,200,50],\n        \n        \"bootstrap\" : [True, False],\n    \n        'max_features': ['auto', 'sqrt', 'log2']\n        \n        }\n\ngrid = GridSearchCV(estimator = rf, param_grid=params, cv = 5, n_jobs = -1, return_train_score = True )\ngrid.fit(X,y)\ngrid.best_params_","95eb9ece":"rf = RandomForestRegressor(**grid.best_params_)\n\nrf.fit(X_train,y_train)\n\ny_pred=rf.predict(X_test)\nprint('RMSE:',np.sqrt(mean_squared_error(y_test,y_pred)))\nprint(\"MAE\",mean_absolute_error(y_test,y_pred))\nprint('R-squared:',r2_score(y_test,y_pred))","6ad726d8":"from sklearn.ensemble import AdaBoostRegressor","2374c7b8":"adaboost = AdaBoostRegressor(\n    RandomForestRegressor(max_depth=5,criterion='mse'),\n    n_estimators=150\n)\nadaboost.fit(X_train, y_train)","7a9f05a6":"y_pred = adaboost.predict(X_test)\nprint('RMSE:',np.sqrt(mean_squared_error(y_test,y_pred)))\nprint(\"MAE\",mean_absolute_error(y_test,y_pred))\nprint('R-squared:',r2_score(y_test,y_pred))","f28fa70d":"pd.DataFrame(rf.feature_importances_,index=X.columns,columns=['Score']).sort_values(\"Score\",ascending=False)[:10]","7a87261f":"pd.DataFrame(rf.feature_importances_,index=X.columns,columns=['Score']).sort_values(\"Score\",ascending=False).plot(kind='bar',figsize=(20,7))\nplt.show()","79fcf9cf":"***Assumption 4 - Homoscedasticity_test(using goldfeld test) OR (Beusch-Wagon Test)***\n\nHomoscedacity :: If the variance of the residuals are symetrically distributed across the regression line , then the data is said to homoscedastic.\n\nHeteroscedacity :: If the variance is unequal for the residuals across the regression line, then the data is said to be heteroscedastic. In this case the residuals can form an arrow shape or any other non symmetrical shape.\n\nThis test is based on the hytpothesis testing where null and alternate hypothesis are:\n\n\ud835\udc3b0: \ud835\udf0e\ud835\udc62\ud835\udc56 \ud835\udc56\ud835\udc60 \ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc61 \ud835\udc4e\ud835\udc50\ud835\udc5f\ud835\udc5c\ud835\udc60\ud835\udc60 \ud835\udc61\u210e\ud835\udc52 \ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc54\ud835\udc52 \ud835\udc5c\ud835\udc53 \ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4e\n\n\ud835\udc3b\ud835\udc4e: \ud835\udf0e\ud835\udc62\ud835\udc56 \ud835\udc56\ud835\udc60 \ud835\udc5b\ud835\udc5c\ud835\udc61 \ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc61 \ud835\udc4e\ud835\udc50\ud835\udc5f\ud835\udc5c\ud835\udc60\ud835\udc60 \ud835\udc61\u210e\ud835\udc52 \ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc54\ud835\udc52 \ud835\udc5c\ud835\udc53 \ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4e\nThe residuals should be homoscedastic.\n\nGoldfeld Test","3046643f":"#### 3.Monthly Premium Auto","22adf8cd":"#### 2. Gender,Location Code,Marital Status,Policy","66256a53":"## 8. Assumptions\n1. No autocorrelation\n2. Linearity of residuals\n3. Normality of error terms\n4. No Heteroscedacity\n5. No strong MultiCollinearity","de492f11":"#### 4.Months Since Last Claim","91e7aa1d":"#### 2.Income","2a589fd0":"- H0:The null hypothesis of the test is the data is normally distributed\n- HA:The data is not normally distributed","7187808a":"**Asssumption 3 - Linearity of residuals**\n\nHere we have 2 options. Either we can plot the observed values Vs predicted values and plot the Residual Vs predicted values and see the linearity of residuals. OR We can go for rainbow test. Let's look both of them one by one.","2ad6ee78":"# 5. EDA(Exploratory Data Analysis)","502af93e":"p-value > 0.05 failed to reject H0","4090f120":"p-value is greater than 0.05 we failed to reject null hypothesis i.e the mean rank will be the same for both males and females.","e555e227":"H0:The data is linear, pvalue > 0.05 failed to reject H0, here we failed to reject H0 so residuals are normally distributed","cb435019":"***January and February is the highest the date that an agreement or transaction between or among customer and An Auto Insurance company***","639b9de3":"* ***The number of customers who do not respond is higher***\n* ***Customers are predominantly from California and Aregon***\n* ***Basic Coverage, Employed, Personal Auto are the Highest Number***\n* ***The number of Bachelor, College, and high school or bellows looks not too much different which is higher than master and doctor***","e2cab3d6":"***Category No 0 are the highest Number of Open Complaints and Category No 1 are the highest Number of Policies***","390423f1":"## B. Bevariate Analysis","44de715a":"## Hyperparameter tuning of random forest","b15b5c85":"## 9.2.Rigde and Lasso Regression","e477e0b0":"# Customer Lifetime Value Prediction","abf89ab9":"The interpretation of VIF is as follows: the square root of a given variable\u2019s VIF shows how much larger the standard error is, compared with what it would be if that predictor were uncorrelated with the other features in the model. If no features are correlated, then all values for VIF will be 1.","a2dda894":"***there seems to be familiarity distribution.***","84afd9f0":"## 9.1.Linear Regression","69191d23":"# 10. Final Model\n\nBy comparing RMSE and R^2 score results of models and then we choose the best model as the Random Forest ***without*** GridSearchCV, having the best evaluation scores.\n\n\n# 11. Conclusion\n\n* Overall we can see that No of policies, Monthly Premium auto, Total Claim amount, Months Since Policy Inception, Income , Months Since Last Claim, Number of Open Complaints, Coverage_Extended,vEmploymentStatus_Employed and Renew Offer Type_Offer2 are the important features in predicting the Customer Lifetime Value.\n* The customers having more number of policies with high monthly premium will add more value to company.\n* Ironically being an auto insurance company, the type of vehicle or size does not have an impact on the CLV prediction.\n* The insurance agents should start increasing their policy advertisement for the customers who have more no. of policies, which is the major feature in predicting the CLV.\n","5aa2b80a":"## 1. Business Problem","35e31736":"* H0: the distributions of both populations are equal\n* Ha: that the distributions are not equal.    ","da2c3fc1":"***there seems to be an abnormal distribution, the distribution is skew right, one value looks very high.***\n***The Boxplot show there are many outlier.***","f1a419d2":"## 1.Numerical Features","611d1389":"Random Forest Model with hyperparameter tuning using GridSearchCV gave the best RMSE and R^2 score","f24f6af7":"The critical chi square value at the 5% level of significance is 5.99. If the computed value exceeds this value the null hypothesis is rejected.\nIn this case the computed value of the JB statistic 65051.11 is greater than 5.99. Thus we reject the null hypothesis that the error terms are normally distributed.","f90bc74d":"## 6. HeatMap","48051b1e":"## 3.Dataset Description\nThe dataset represents Customer lifetime value of an Auto Insurance Company in the United States, it includes over 24 features and 9134 records to analyze the lifetime value of Customer.\n","4c9f5a8c":"Number of Policies and Monthly Premium Auto are the most important features in predicting CLV","921a56a2":" p-value below a certain level (like 0.05) indicates we should reject the null in favor of heteroscedasticity.","9f277933":"## 2. Project Overview\n\n* The objective of the problem is to accurately predict the Customer Lifetime Value(CLV) of the customer for an Auto Insurance Company\n* Performed EDA to understand the relation of target variable CLV with the other features.\n* Statistical Analysis techniques like OLS for numerical and Mann\u2013Whitney U and also Kruskal Wallis test for the categorical variables were performed to find the significance of the features with respect to the target.\n* Supervised Regression Models like Linear Regression, Ridge Regression, Lasso Regression, DecisionTree Regression, Random Forest Regression and Adaboost Regression.\n* Using GridSearchCV with Random Forest Regression gave the best RMSE and R^2 score values\n","a4e9fd0a":"## 9.4.RandomForest ","a381070f":"p-value less than 0.05 we reject null hypothesis the data is not normally distributed\n\nWe will proceed with non parametric tests since the dependent variable is not normally distributed","6d9bc044":"#### 6.Total claim amount","c8b62ca3":"## 4.2.Categorical variables","52fdcd55":"### Statistical summary dengan `df.describe()`","40d7be10":"## 4.1.Numerical Variables","2416fb1f":"From the distribution and box plot we can infer that CLV is highly right skewed. There are lot of outliers in our dependent variable but we cant treat them directly because they are influential points.","aac4b9a1":"***Thanks for the review. if there are corrections and suggestions, don't hesitate to comment below.***","fddc192e":"We can clearly see in the heatmap, that customer lifetime value has a better correlation with monthly premium auto and acceptable correlation with total claim amount.\n","452842f0":"From the graph we infer that due to the high value of Jarque Bera test, only few residuals are normally distributed","3c4a8218":"# 9.Model Building","cbb42e8f":"**Assumption 5- NO MULTI COLLINEARITY**","cfbdad9d":"#### 1.State,Response,Coverage,Education,EmploymentStatus,Policy Type","d7395412":"An Auto Insurance company in the USA is facing issues in retaining its customers and wants to advertise promotional offers for its loyal customers. They are considering Customer Lifetime Value CLV as a parameter for this purpose. Customer Lifetime Value represents a customer\u2019s value to a company over a period of time. It\u2019s a competitive market for insurance companies, and the insurance premium isn\u2019t the only determining factor in a customer\u2019s decisions. CLV is a customer-centric metric, and a powerful base to build upon to retain valuable customers, increase revenue from less valuable customers, and improve the customer experience overall. Using CLV effectively can improve customer acquisition and customer retention, prevent churn, help the company to plan its marketing budget, measure the performance of their ads in more detail, and much more","d33ad274":"Around 62.38% of the customers are employed","2a09ddff":"#### 5.Months Since Policy Inception","8da07c69":"## 4. Check for missing values\n***Check for missing columns and values \u200b\u200bwith `df.info()`***","71e9fdb5":"From the distribution and box plot we can infer that Monthly Premium Auto is highly right skewed. There are lot of outliers in our independent variable but we cant treat them directly because they are influential points.","e5e6fa42":"#### 1.CLV - Target variable analysis","179d9e89":"If the distributions are identical, which is the null hypothesis of the Mann-Whitney U test, the mean rank will be the same","32fad37b":"* ***the first and second offers are more attractive to customers and agents are the highest sales channels***\n* ***it turns out that the vehicle Class with four car doors is the highest number with medium size***","7e6ada5d":"### A.Univariate Analysis\nAfter doing a simple analysis of descriptive statistics, now we focus on one by one column with *Univariate Analysis*","ed7e3318":"***Only Total Claim Amount and Monthly Premium Auto appear to have a linear relationship***","de280ac8":"### Load Dataset","095a989d":"***Assumption 2- Normality of Residuals***\n\nThe second assumption is the Normality of Residuals \/ Error terms.\nFor this we prefer the Jarque Bera test. For a good model, the residuals should be normally distributed. The higher the value of Jarque Bera test , the lesser the residuals are normally distributed. We generally prefer a lower value of jarque bera test.\n\nThe Jarque\u2013Bera test is a goodness-of-fit test of whether sample data have the skewness and kurtosis matching a normal distribution. A large value for the jarque-bera test indicates non normality.\n\nThe jarque bera test tests whether the sample data has the skewness and kurtosis matching a normal distribution. Note that this test generally works good for large enough number of data samples(>2000) as the test statistics asymptotically has a chi squared distribution with degrees 2 of freedom.","7c0dd4e6":"## 5.2.Categorical variables","a37ee377":"***Customer with No Response are Higher than Yes Response***","07d7f13b":"#### 4.Months","130ec317":"#### 3.Renew Offer Type,Sales Channel,Vehicle Class, Number of Complaints, Number of Policies","c26465b8":"We can see that:\n* The dataframe has a total of 9134 rows and 24 columns\n* Dataframe has *null* values \nFrom the information above, we can separate the categorical and numerical columns as follows:","1ec3ac7d":"- H0:the mean ranks of the groups are the same.\n- H1:the mean ranks of the groups are not the same.","c7e0cfea":"**Assumption 1 No Auto correlation.**\n\nTest needed : Durbin- Watson Test.\n\nIt's value ranges from 0-4. If the value of Durbin- Watson is Between 0-2, it's known as Positive Autocorrelation.\nIf the value ranges from 2-4, it is known as Negative autocorrelation.\nIf the value is exactly 2, it means No Autocorrelation.\nFor a good linear model, it should have low or no autocorrelation.\nwe can see here the values of dublin watson test: 1.995 (No AUTO-CORRELATION)","d1082a1e":"From the graph above, we can easily see that there is no autocorrelation.","1bc23263":"we can see that there is high VIF in column Policy & policy type\nAlso we have seen in statistical approach these veriables are not significant to predict target\nhence from above 2 conclusion we can remove them","3daf5d49":"## 9.3.Decision Tree","3cbc4714":"### Import Library","ea8af137":"## 7. Statistical Significance","c33e76c9":"p-value is greater than 0.05 we failed to reject null hypothesis i.e the mean rank will be the same for both males and females.","07de5a27":"* ***The number of Male and Female looks not too much different and female is the highest values***\n* ***The customer's residence is dominated by customers who live in the suburbs and most of them are married***\n* ***Amount CLV with maritial status looks not too much different***\n* ***Personal L3 policy subcategory has the most number of customers***"}}