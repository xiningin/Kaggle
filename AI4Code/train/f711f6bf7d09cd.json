{"cell_type":{"1d0fcebf":"code","c6f7ecd9":"code","56750dc3":"code","c6f5d82d":"code","a7dc8f3f":"code","3d6842bc":"code","95f12a26":"code","0895b39d":"code","a6e815ef":"code","7d335ad2":"code","1bdf3f3d":"code","bfe9672d":"code","e79119df":"code","375335ea":"code","7d1adcc3":"code","69081b1c":"code","0ae415fd":"code","1e778d7d":"code","c3ba71aa":"code","877ed7d1":"code","7d002cdf":"code","a2ef7db1":"code","844bd1dd":"code","ac3fa8a9":"code","ede677a8":"code","7fcfa7e3":"code","fc70deba":"code","638a1dc1":"code","dc2ec3b9":"code","e7b2149d":"code","3ece7d44":"code","67d3258c":"code","e880104a":"code","cb303b64":"code","b3ef294e":"code","07762f95":"code","5a48bff0":"code","a05555f9":"code","eb4fe89a":"code","7fcec61d":"code","4f6b19ed":"code","c9e480ea":"code","81e0e4ea":"code","cc6f9a4e":"code","12fb0a2e":"code","6a797aab":"code","0864d52d":"code","9d253d05":"code","cdc837dd":"code","6b9094af":"code","20e8fba3":"code","e9cb969e":"code","a76b8147":"code","2d0ac619":"code","53ee02fc":"code","0fcc0abf":"code","d37c289c":"code","7be2f3e9":"code","b935a172":"code","eaf1335d":"code","97f4c4c1":"code","0d71a7c7":"code","21e1a122":"code","b685da76":"code","92c7b8b0":"code","b59fbc79":"code","24040050":"code","98bad5f4":"code","fb46e2f7":"code","d132ce5f":"code","213944f3":"code","7f286c45":"code","575db1fd":"code","eaec1ecd":"code","81586260":"code","6cba3920":"code","4f574bf9":"code","0b350233":"code","50fe181e":"code","3dffa86c":"code","ab530d17":"code","11d32d53":"code","63c4af39":"code","61d39c81":"code","745f861d":"code","49adfa42":"code","bf31371b":"code","b1235e5a":"code","5106119e":"code","1263dc2c":"code","9308ac6a":"code","3f402d4d":"code","cd4dd0f2":"code","eb8aa41d":"code","8c9db62c":"code","a5ee3838":"code","cb17e822":"code","a82328e9":"code","d7166f8e":"code","f46fe7ef":"code","6fca2f4b":"code","8d0c625c":"code","802d5d98":"code","01dd7b5c":"code","327b231e":"code","f44922e1":"code","31670c1c":"code","9f922690":"code","9d3a97cd":"code","9f4b0471":"code","bc43deba":"code","6364cf8b":"code","b98095c3":"code","eaa75171":"code","bf9ce917":"code","f68adb8e":"code","9170df00":"code","569bd33e":"code","fb702182":"code","34caa71d":"code","22f6fac8":"code","3d2a25f0":"code","92bbe3df":"code","8805dd0a":"code","6b33a30f":"code","b5358d9b":"code","fe00b368":"code","a332421f":"code","035f4c44":"code","bfdd3880":"code","ecaac07a":"code","42274bac":"code","8fa3f33b":"code","75ceecc6":"code","49611342":"code","b82d48d1":"markdown","7b785006":"markdown","c9621821":"markdown","ca431318":"markdown","c75d24ba":"markdown","a2a899c5":"markdown","bfcf26a7":"markdown","90e520fc":"markdown","8c40c133":"markdown","a0debdbc":"markdown","5b67e7c9":"markdown","fdec3994":"markdown","cc28374b":"markdown","8cfe7a90":"markdown","ea80a3e8":"markdown","4e1327df":"markdown","72fe079e":"markdown","948a4d1e":"markdown","8ab374ad":"markdown","f75fa6ea":"markdown","70e50e39":"markdown","34634741":"markdown","5acce3b7":"markdown","c7969f86":"markdown","28449146":"markdown","1fe288f9":"markdown","dee3c854":"markdown","6f094f1f":"markdown","6b0da07d":"markdown","fd4a1fbd":"markdown","41ce9d84":"markdown","068ea2de":"markdown","0d1893bc":"markdown","14b5e413":"markdown","eb0af0a9":"markdown","fec5f3bc":"markdown","8eb8bdf1":"markdown","7031b52a":"markdown","8fc84f87":"markdown","cecdb9e0":"markdown","f78ea15e":"markdown","4892b098":"markdown","58a5d115":"markdown","74f90de4":"markdown","55d1eef4":"markdown","6685b7f0":"markdown","e5c4f57c":"markdown","9c25bfb7":"markdown","bf6e2980":"markdown","15b5eb35":"markdown","142cfd0b":"markdown","557e33a2":"markdown","bd12af63":"markdown","a4ce91f7":"markdown","adc51300":"markdown","fb5e3b91":"markdown","fec9b9f0":"markdown","3a549c5c":"markdown","fdc6f33a":"markdown","b7a7f61f":"markdown"},"source":{"1d0fcebf":"# This block is from https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n\n#load packages\nimport sys #access to system parameters https:\/\/docs.python.org\/3\/library\/sys.html\nprint(\"Python version: {}\". format(sys.version))\n\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\nprint(\"pandas version: {}\". format(pd.__version__))\n\nimport matplotlib #collection of functions for scientific and publication-ready visualization\nprint(\"matplotlib version: {}\". format(matplotlib.__version__))\n\nimport numpy as np #foundational package for scientific computing\nprint(\"NumPy version: {}\". format(np.__version__))\n\nimport scipy as sp #collection of functions for scientific computing and advance mathematics\nprint(\"SciPy version: {}\". format(sp.__version__)) \n\nimport IPython\nfrom IPython import display #pretty printing of dataframes in Jupyter notebook\nprint(\"IPython version: {}\". format(IPython.__version__)) \n\nimport sklearn #collection of machine learning algorithms\nprint(\"scikit-learn version: {}\". format(sklearn.__version__))\n\nimport seaborn as sns #collection of functions for data visualization\nprint(\"seaborn version: {}\". format(sns.__version__))\n\nfrom sklearn.preprocessing import OneHotEncoder #OneHot Encoder\n\n\n#misc libraries\nimport random\nimport time\n\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nprint('-'*25)","c6f7ecd9":"#this is from https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy\n\n#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\n#from pandas.tools.plotting import scatter_matrix\n\n#Configure Visualization Defaults\n#%matplotlib inline = show plots in Jupyter Notebook browser\n%matplotlib inline\n#mpl.style.use('ggplot')\n#sns.set_style('white')\n#pylab.rcParams['figure.figsize'] = 12,8","56750dc3":"#load data\ntrain_raw = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_raw = pd.read_csv('..\/input\/titanic\/test.csv')","c6f5d82d":"train_raw.head()","a7dc8f3f":"train_raw.info()\ntrain_raw.describe()","3d6842bc":"#let split the data for more targeted handling\ntxt_cols = [cname for cname in train_raw.columns if train_raw[cname].dtype == \"object\"]\n\n# Select numerical columns\nnum_cols = [cname for cname in train_raw.columns if train_raw[cname].dtype in ['int64', 'float64']]\n\ntxt_data = train_raw[txt_cols].copy()\nnum_data = train_raw[num_cols].copy()","95f12a26":"# to make sure we didn't accidentally drop any cols\ntxt_data.shape[1] + num_data.shape[1] == train_raw.shape[1]","0895b39d":"#now let us look at the numaric cols\nnum_data.info()","a6e815ef":"age_missing_per = num_data.Age.isnull().sum()\/len(num_data.Age)\nprint(\"{:.2%}\".format(age_missing_per))","7d335ad2":"# to find corrolating features\nsns.heatmap(train_raw.corr())","1bdf3f3d":"#we see that Pclass, Sibsp, Parch are highly corrolated with Age\nPclass_Sibsp_Parch = train_raw['Pclass'].apply(str)+'-'+train_raw['SibSp'].apply(str)+'-'+train_raw['Parch'].apply(str)\nPclass_Sibsp_Parch.value_counts()","bfe9672d":"frame = { 'Pclass_Sibsp_Parch': Pclass_Sibsp_Parch, 'Age': train_raw.Age } \nage_psp = pd.DataFrame(frame)","e79119df":"age_classes = ['3-0-0', '1-0-0', '2-0-0']","375335ea":"#now we have the data in the right state to be converted to the desired lookup table\nage_psp['Pclass_Sibsp_Parch'] = age_psp['Pclass_Sibsp_Parch'].apply(lambda x: x if x in age_classes else 'other')\nage_psp['Pclass_Sibsp_Parch'].value_counts()","7d1adcc3":"#creating the lookup table with Pclass_Sibsp_Parch as index\nage_lookup= age_psp.groupby(\"Pclass_Sibsp_Parch\", as_index=False).mean()\nage_lookup = age_lookup.set_index('Pclass_Sibsp_Parch')","69081b1c":"#create temp columns in the df to impute missing Age values\nnum_data['temp_psp'] = age_psp['Pclass_Sibsp_Parch']\nnum_data['temp_age_cat_mean'] = num_data['temp_psp']\nnum_data['temp_age_cat_mean'] = num_data['temp_age_cat_mean'].apply(lambda x: age_lookup['Age'][x])\nnum_data['Age'] = num_data['Age'].fillna(num_data['temp_age_cat_mean'])","0ae415fd":"#check to see if there is still null values in the data\nnum_data['Age'].isnull().sum()","1e778d7d":"#clean up after the imputation\nnum_data = num_data.drop('temp_psp', axis=1)\nnum_data = num_data.drop('temp_age_cat_mean', axis=1)","c3ba71aa":"num_data.isnull().sum()","877ed7d1":"num_data['PassengerId'].hist()","7d002cdf":"num_data = num_data.drop('PassengerId', axis=1)","a2ef7db1":"txt_data.isnull().sum()","844bd1dd":"txt_data = txt_data.drop('Cabin', axis=1)","ac3fa8a9":"txt_data['Embarked'] = txt_data['Embarked'].fillna(txt_data['Embarked'].value_counts().index[0])","ede677a8":"txt_data.isnull().sum()","7fcfa7e3":"def data_clean (df):\n    #create lookup table for Age\n    psp = df['Pclass'].apply(str)+'-'+df['SibSp'].apply(str)+'-'+df['Parch'].apply(str)\n    frame = { 'Pclass_Sibsp_Parch': psp, 'Age': df.Age } \n    psp_age = pd.DataFrame(frame)\n    #define age classes, other not yet included\n    age_classes = ['3-0-0', '1-0-0', '2-0-0']\n    #convert excluding items to 'other'\n    psp_age['Pclass_Sibsp_Parch'] = psp_age['Pclass_Sibsp_Parch'].apply(lambda x: x if x in age_classes else 'other')\n    #transform to a lookup table\n    am_lookup= psp_age.groupby('Pclass_Sibsp_Parch').mean()\n    #using the lookup table\n    df['temp_psp'] = psp_age['Pclass_Sibsp_Parch'] #setup a temp col with psp lables\n    df['temp_age_cat_mean'] = df['temp_psp'] #create a col for age means conversions\n    df['temp_age_cat_mean'] = df['temp_age_cat_mean'].apply(lambda x: am_lookup['Age'][x]) #convert values in this col to age means according to the psp lable\n    df['Age'] = df['Age'].fillna(df['temp_age_cat_mean']) #fill na according to the tempt mean col\n    #drop the temp cols\n    df = df.drop('temp_psp', axis=1)\n    df = df.drop('temp_age_cat_mean', axis=1)\n    \n    #found that there are missing valuse in Fare in the test dataset\n    df['Fare'] = df['Fare'].fillna(method='ffill')\n    \n    #for reasons stated above we don't want PassengerId, Cabin cols\n    df = df.drop('PassengerId', axis=1)\n    df = df.drop('Cabin', axis=1)\n    \n    #fill na for Embarked, only two \n    df['Embarked'] = df['Embarked'].fillna(df['Embarked'].value_counts().index[0])\n    \n\n    \n    return df\n    ","fc70deba":"num_data.describe()","638a1dc1":"num_data['FareBin'] = pd.qcut(num_data['Fare'], 5, labels=[1, 2, 3, 4, 5]).astype(int)\nnum_data['AgeBin'] = pd.cut(num_data['Age'].astype(int), 5, labels=[1, 2, 3, 4, 5]).astype(int)\nnum_data","dc2ec3b9":"#take a look at if the new features are actually useful\nnum_data.corr()","e7b2149d":"#now we drop the original features\nnum_data = num_data.drop(['Fare', 'Age'], axis=1)","3ece7d44":"num_data","67d3258c":"FamilySize = num_data['SibSp']+num_data['Parch']+1","e880104a":"IsAlone = FamilySize>1\nIsAlone = IsAlone.apply(int)","cb303b64":"num_data['IsAlone'] = IsAlone\nnum_data['FamilySize'] = FamilySize\nnum_data.corr()","b3ef294e":"txt_data ","07762f95":"#borrowed from https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy notebook\n#quick and dirty code split title from name: http:\/\/www.pythonforbeginners.com\/dictionary\/python-split\ntxt_data['Title'] = txt_data['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n\ntxt_data.Title.value_counts()","5a48bff0":"stat_min = 10 #while small is arbitrary, we'll use the common minimum in statistics: http:\/\/nicholasjjackson.com\/2012\/03\/08\/sample-size-is-10-a-magic-number\/\ntitle_names = (txt_data['Title'].value_counts() < stat_min) #this will create a true false series with title name as index\n\n#apply and lambda functions are quick and dirty code to find and replace with fewer lines of code: https:\/\/community.modeanalytics.com\/python\/tutorial\/pandas-groupby-and-python-lambda-functions\/\ntxt_data['Title'] = txt_data['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\nprint(txt_data['Title'].value_counts())","a05555f9":"Ticket_len = txt_data.Ticket.apply(len)","eb4fe89a":"Ticket_len.value_counts()","7fcec61d":"stat_min_ti = 30\nTicket_len_ls = (Ticket_len.value_counts() < stat_min_ti)\nTicket_len = Ticket_len.apply(lambda x: '30' if Ticket_len_ls.loc[x] == True else x)\nTicket_len.value_counts()","4f6b19ed":"txt_data['Ticket_len'] = Ticket_len\ntxt_data = txt_data.drop(['Name', 'Ticket'], axis=1)","c9e480ea":"txt_data","81e0e4ea":"#imputer only works on str or numbers\ntxt_data['Ticket_len'] = txt_data['Ticket_len'].astype(str)","cc6f9a4e":"OH_en = OneHotEncoder(handle_unknown='ignore', sparse=False)\nimp_txt_cols = OH_en.fit_transform(txt_data[['Sex','Embarked', 'Title', 'Ticket_len']])\nimp_txt_cols = pd.DataFrame(imp_txt_cols)","12fb0a2e":"#now we align the index and col names\nimp_txt_cols.index = txt_data[['Sex','Embarked', 'Title', 'Ticket_len']].index\nimp_txt_cols.columns = OH_en.get_feature_names(['Sex','Embarked', 'Title', 'Ticket_len'])\nimp_txt_cols","6a797aab":"txt_data","0864d52d":"#now we complete the txt df with the imputed cols\ntxt_data = txt_data.drop(['Sex','Embarked', 'Title', 'Ticket_len'], axis=1).join(imp_txt_cols)","9d253d05":"txt_data.corr()","cdc837dd":"def f_eng (clean_data):\n    #let's start with num cols\n    #create value bins for continuouse values\n    clean_data['FareBin'] = pd.qcut(clean_data['Fare'], 5, labels=[1, 2, 3, 4, 5]).astype(int)\n    clean_data['AgeBin'] = pd.cut(clean_data['Age'].astype(int), 5, labels=[1, 2, 3, 4, 5]).astype(int)\n    #now we drop the original features\n    clean_data = clean_data.drop(['Fare', 'Age'], axis=1)\n    \n    #create new features\n    FamilySize = clean_data['SibSp']+clean_data['Parch']+1\n    IsAlone = FamilySize>1\n    IsAlone = IsAlone.apply(int)\n    clean_data['IsAlone'] = IsAlone\n    clean_data['FamilySize'] = FamilySize\n    \n    #next we work on text data\n    #extrat title from Name\n    clean_data['Title'] = clean_data['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n    \n    stat_min = 10 #while small is arbitrary, we'll use the common minimum in statistics: http:\/\/nicholasjjackson.com\/2012\/03\/08\/sample-size-is-10-a-magic-number\/\n    title_names = (clean_data['Title'].value_counts() < stat_min) #this will create a true false series with title name as index\n    #apply and lambda functions are quick and dirty code to find and replace with fewer lines of code: https:\/\/community.modeanalytics.com\/python\/tutorial\/pandas-groupby-and-python-lambda-functions\/\n    clean_data['Title'] = clean_data['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\n    \n    #cat tickets by length\n    Ticket_len = clean_data.Ticket.apply(len)\n    stat_min_ti = 30\n    Ticket_len_ls = (Ticket_len.value_counts() < stat_min_ti)\n    Ticket_len = Ticket_len.apply(lambda x: '30' if Ticket_len_ls.loc[x] == True else x)\n    Ticket_len.value_counts()\n    clean_data['Ticket_len'] = Ticket_len\n    #imputer only works on str or numbers\n    clean_data['Ticket_len'] = clean_data['Ticket_len'].astype(str)\n    clean_data = clean_data.drop(['Name', 'Ticket'], axis=1)\n    \n    #do imputation on txt data\n    OH_en = OneHotEncoder(handle_unknown='ignore', sparse=False)\n    imp_cols = OH_en.fit_transform(clean_data[['Sex','Embarked', 'Title', 'Ticket_len']])\n    imp_cols = pd.DataFrame(imp_cols)\n    #now we align the index and col names\n    imp_cols.index = clean_data[['Sex','Embarked', 'Title', 'Ticket_len']].index\n    imp_cols.columns = OH_en.get_feature_names(['Sex','Embarked', 'Title', 'Ticket_len'])\n    clean_data = clean_data.drop(['Sex','Embarked', 'Title', 'Ticket_len'], axis=1).join(imp_cols)\n\n    return clean_data","6b9094af":"test_data = train_raw.copy()\ntest_clean = data_clean(test_data)","20e8fba3":"final = f_eng(test_clean)","e9cb969e":"final.corr()","a76b8147":"def pre_p (train, test):\n    train_c = data_clean(train)\n    train_f = f_eng(train_c)\n    y_train = train_f.Survived\n    X_train = train_f.drop('Survived', axis=1)\n    \n    test_c = data_clean(test)\n    test_f = f_eng(test_c)\n    X_test = test_f\n    \n    return X_train, y_train, X_test","2d0ac619":"X_train, y_train, X_test = pre_p (train_raw, test_raw)","53ee02fc":"def base_model (X_tr, y_tr):\n    #this is from https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy\n    #Machine Learning Algorithm (MLA) Selection and Initialization\n    MLA = [\n        #Ensemble Methods\n        ensemble.AdaBoostClassifier(),\n        ensemble.BaggingClassifier(),\n        ensemble.ExtraTreesClassifier(),\n        ensemble.GradientBoostingClassifier(),\n        ensemble.RandomForestClassifier(),\n\n        #Gaussian Processes\n        gaussian_process.GaussianProcessClassifier(),\n\n        #GLM\n        linear_model.LogisticRegressionCV(),\n        linear_model.PassiveAggressiveClassifier(),\n        linear_model.RidgeClassifierCV(),\n        linear_model.SGDClassifier(),\n        linear_model.Perceptron(),\n\n        #Navies Bayes\n        naive_bayes.BernoulliNB(),\n        naive_bayes.GaussianNB(),\n\n        #Nearest Neighbor\n        neighbors.KNeighborsClassifier(),\n\n        #SVM\n        svm.SVC(probability=True),\n        svm.NuSVC(probability=True),\n        svm.LinearSVC(),\n\n        #Trees    \n        tree.DecisionTreeClassifier(),\n        tree.ExtraTreeClassifier(),\n\n        #Discriminant Analysis\n        discriminant_analysis.LinearDiscriminantAnalysis(),\n        discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n\n        #xgboost: http:\/\/xgboost.readthedocs.io\/en\/latest\/model.html\n        XGBClassifier()    \n        ]\n    \n    #split dataset in cross-validation with this splitter class: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n    #note: this is an alternative to train_test_split\n    #cv_split = model_selection.ShuffleSplit(test_size = .2, train_size = .8, random_state = 0 ) # run model 10x with 80\/20 split intentionally leaving out 10%\n\n    #create table to compare MLA metrics\n    MLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\n    MLA_compare = pd.DataFrame(columns = MLA_columns)\n\n    #create table to compare MLA predictions\n    MLA_predict = y_tr\n\n    #index through MLA and save performance to table\n    row_index = 0\n    for alg in MLA:\n\n        #set name and parameters\n        MLA_name = alg.__class__.__name__\n        #print(MLA_name)\n        MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n        MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n        #print(y_tr.shape)\n\n        #score model with cross validation: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n        cv_results = model_selection.cross_validate(alg, X_tr, y_tr, cv = 5, scoring='accuracy', return_train_score=True)\n        \n\n        MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n        MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n        MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n        #if this is a non-bias random sample, then +\/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n        MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n\n\n        #save MLA predictions - see section 6 for usage\n        #alg.fit(X_tr, y_tr)\n        #MLA_predict[MLA_name] = alg.predict(X_tr)\n\n        row_index+=1\n\n\n    #print and sort table: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.sort_values.html\n    MLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\n    return MLA_compare\n    #MLA_predict","0fcc0abf":"base_alg = base_model(X_train, y_train)","d37c289c":"base_alg","7be2f3e9":"#barplot using https:\/\/seaborn.pydata.org\/generated\/seaborn.barplot.html\nsns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = base_alg, color = 'm')\n\n#prettify using pyplot: https:\/\/matplotlib.org\/api\/pyplot_api.html\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')","b935a172":"#this is borrowed from the Kaggle notebook https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy\n#the original idea was to take the top ten from our list above, but since the alg list below covers a large portion of our top ten, we will use it directly\ndef hp_tune(X_tr, y_tr, base_alg):\n    \n    vote_est = [\n                #Ensemble Methods: http:\/\/scikit-learn.org\/stable\/modules\/ensemble.html\n                ('ada', ensemble.AdaBoostClassifier()),\n                ('bc', ensemble.BaggingClassifier()),\n                ('etc',ensemble.ExtraTreesClassifier()),\n                ('gbc', ensemble.GradientBoostingClassifier()),\n                ('rfc', ensemble.RandomForestClassifier()),\n\n                #Gaussian Processes: http:\/\/scikit-learn.org\/stable\/modules\/gaussian_process.html#gaussian-process-classification-gpc\n                ('gpc', gaussian_process.GaussianProcessClassifier()),\n\n                #GLM: http:\/\/scikit-learn.org\/stable\/modules\/linear_model.html#logistic-regression\n                ('lr', linear_model.LogisticRegressionCV()),\n\n                #Navies Bayes: http:\/\/scikit-learn.org\/stable\/modules\/naive_bayes.html\n                ('bnb', naive_bayes.BernoulliNB()),\n                ('gnb', naive_bayes.GaussianNB()),\n\n                #Nearest Neighbor: http:\/\/scikit-learn.org\/stable\/modules\/neighbors.html\n                ('knn', neighbors.KNeighborsClassifier()),\n\n                #SVM: http:\/\/scikit-learn.org\/stable\/modules\/svm.html\n                ('svc', svm.SVC(probability=True)),\n\n                #xgboost: http:\/\/xgboost.readthedocs.io\/en\/latest\/model.html\n               ('xgb', XGBClassifier())\n\n                ]\n    \n    #Hyperparameter Tune with GridSearchCV: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html\n    grid_n_estimator = [10, 50, 100, 300, 500]\n    grid_ratio = [.1, .25, .5, .75, 1.0]\n    grid_learn = [.01, .03, .05, .1, .25]\n    grid_max_depth = [2, 4, 6, 8, 10, None]\n    grid_min_samples = [5, 10, .03, .05, .10]\n    grid_criterion = ['gini', 'entropy']\n    grid_bool = [True, False]\n    grid_seed = [0]\n\n\n    grid_param = [\n                [{\n                #AdaBoostClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.AdaBoostClassifier.html\n                'n_estimators': grid_n_estimator, #default=50\n                'learning_rate': grid_learn, #default=1\n                #'algorithm': ['SAMME', 'SAMME.R'], #default=\u2019SAMME.R\n                'random_state': grid_seed\n                }],\n\n\n                [{\n                #BaggingClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier\n                'n_estimators': grid_n_estimator, #default=10\n                'max_samples': grid_ratio, #default=1.0\n                'random_state': grid_seed\n                 }],\n\n\n                [{\n                #ExtraTreesClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier\n                'n_estimators': grid_n_estimator, #default=10\n                'criterion': grid_criterion, #default=\u201dgini\u201d\n                'max_depth': grid_max_depth, #default=None\n                'random_state': grid_seed\n                 }],\n\n\n                [{\n                #GradientBoostingClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier\n                #'loss': ['deviance', 'exponential'], #default=\u2019deviance\u2019\n                'learning_rate': [.05], #default=0.1 -- 12\/31\/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n                'n_estimators': [300], #default=100 -- 12\/31\/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n                #'criterion': ['friedman_mse', 'mse', 'mae'], #default=\u201dfriedman_mse\u201d\n                'max_depth': grid_max_depth, #default=3   \n                'random_state': grid_seed\n                 }],\n\n\n                [{\n                #RandomForestClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n                'n_estimators': grid_n_estimator, #default=10\n                'criterion': grid_criterion, #default=\u201dgini\u201d\n                'max_depth': grid_max_depth, #default=None\n                'oob_score': [True], #default=False -- 12\/31\/17 set to reduce runtime -- The best parameter for RandomForestClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'oob_score': True, 'random_state': 0} with a runtime of 146.35 seconds.\n                'random_state': grid_seed\n                 }],\n\n                [{    \n                #GaussianProcessClassifier\n                'max_iter_predict': grid_n_estimator, #default: 100\n                'random_state': grid_seed\n                }],\n\n\n                [{\n                #LogisticRegressionCV - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV\n                'fit_intercept': grid_bool, #default: True\n                #'penalty': ['l1','l2'],\n                'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], #default: lbfgs\n                'random_state': grid_seed\n                 }],\n\n\n                [{\n                #BernoulliNB - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB\n                'alpha': grid_ratio, #default: 1.0\n                 }],\n\n\n                #GaussianNB - \n                [{}],\n\n                [{\n                #KNeighborsClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n                'n_neighbors': [1,2,3,4,5,6,7], #default: 5\n                'weights': ['uniform', 'distance'], #default = \u2018uniform\u2019\n                'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n                }],\n\n\n                [{\n                #SVC - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html#sklearn.svm.SVC\n                #http:\/\/blog.hackerearth.com\/simple-tutorial-svm-parameter-tuning-python-r\n                #'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n                'C': [1,2,3,4,5], #default=1.0\n                'gamma': grid_ratio, #edfault: auto\n                'decision_function_shape': ['ovo', 'ovr'], #default:ovr\n                'probability': [True],\n                'random_state': grid_seed\n                 }],\n\n\n                [{\n                #XGBClassifier - http:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html\n                'learning_rate': grid_learn, #default: .3\n                'max_depth': [1,2,4,6,8,10], #default 2\n                'n_estimators': grid_n_estimator, \n                'seed': grid_seed  \n                 }]   \n            ]\n\n    \n    #create a table to display key metrics\n    hp_columns = ['Alg Name', 'Best Score', 'Score Before Tuning', 'Best Parameters']\n    hp_compare = pd.DataFrame(columns = hp_columns)\n\n    #index through MLA and save performance to table\n    row_index = 0\n\n    for clf, param in zip (vote_est, grid_param): #https:\/\/docs.python.org\/3\/library\/functions.html#zip\n\n     \n        best_search = model_selection.GridSearchCV(estimator = clf[1], param_grid = param, cv = 5, scoring = 'accuracy', n_jobs = -1)\n        best_search.fit(X_tr, y_tr)\n\n        best_param = best_search.best_params_\n        best_score = best_search.best_score_\n        alg_name = clf[1].__class__.__name__\n        #print('The best parameter for {} is {} with a runtime of {:.2f} seconds.'.format(clf[1].__class__.__name__, best_param, run))\n        clf[1].set_params(**best_param) \n        \n        hp_compare.loc[row_index, 'Alg Name'] = alg_name\n        hp_compare.loc[row_index, 'Best Parameters'] = str(best_param)\n        hp_compare.loc[row_index, 'Best Score'] = best_score\n        hp_compare.loc[row_index, 'Score Before Tuning'] = base_alg.loc[base_alg['MLA Name'] == alg_name]['MLA Test Accuracy Mean'].tolist()[0]\n        row_index+=1\n\n\n    print('Done')\n    print('-'*10)\n    \n    return vote_est, hp_compare\n","eaf1335d":"tuned_algs, hp_compare = hp_tune(X_train, y_train,base_alg)","97f4c4c1":"hp_compare.sort_values(by='Best Score', ascending=False)","0d71a7c7":"def en_alg (algs, X_tr, y_tr):\n    \n    grid_hard = ensemble.VotingClassifier(estimators = algs , voting = 'hard')\n    grid_hard_cv = model_selection.cross_validate(grid_hard, X_tr, y_tr, cv  = 5, return_train_score=True)\n    grid_hard.fit(X_tr, y_tr)\n\n    print(\"Hard Voting w\/Tuned Hyperparameters Training w\/bin score mean: {:.2f}\". format(grid_hard_cv['train_score'].mean()*100)) \n    print(\"Hard Voting w\/Tuned Hyperparameters Test w\/bin score mean: {:.2f}\". format(grid_hard_cv['test_score'].mean()*100))\n    print(\"Hard Voting w\/Tuned Hyperparameters Test w\/bin score 3*std: +\/- {:.2f}\". format(grid_hard_cv['test_score'].std()*100*3))\n    print('-'*10)\n\n    #Soft Vote or weighted probabilities w\/Tuned Hyperparameters\n    grid_soft = ensemble.VotingClassifier(estimators = algs , voting = 'soft')\n    grid_soft_cv = model_selection.cross_validate(grid_soft, X_tr, y_tr, cv  = 5, return_train_score=True)\n    grid_soft.fit(X_tr, y_tr)\n\n    print(\"Soft Voting w\/Tuned Hyperparameters Training w\/bin score mean: {:.2f}\". format(grid_soft_cv['train_score'].mean()*100)) \n    print(\"Soft Voting w\/Tuned Hyperparameters Test w\/bin score mean: {:.2f}\". format(grid_soft_cv['test_score'].mean()*100))\n    print(\"Soft Voting w\/Tuned Hyperparameters Test w\/bin score 3*std: +\/- {:.2f}\". format(grid_soft_cv['test_score'].std()*100*3))\n    print('-'*10)\n    \n    return grid_hard, grid_soft","21e1a122":"vc_hard_all, vc_soft_all = en_alg(tuned_algs, X_train, y_train)","b685da76":"hp_compare.sort_values(by='Best Score', ascending=False)['Alg Name']","92c7b8b0":"def top_algs (sorted_score_df, algs, num_algs):\n    top_alg_ls = []\n    \n    for alg_name in sorted_score_df:\n        for alg in algs:\n            if alg[1].__class__.__name__ == alg_name:\n                top_alg_ls.append(alg)\n        \n    return top_alg_ls[:num_algs]","b59fbc79":"sorted_alg_names = hp_compare.sort_values(by='Best Score', ascending=False)['Alg Name']\ntop_five_algs = top_algs(sorted_alg_names, tuned_algs, 5)","24040050":"vc_hard_tfive, vc_soft_tfive = en_alg(top_five_algs, X_train, y_train)","98bad5f4":"def gen_submissions(top_five, b_vc_hard, b_vc_soft, X_tr, y_tr, X_test, tag):\n    \n    #create submissions for each alg in the top_five list\n    for alg in top_five:\n        alg[1].fit(X_tr, y_tr)\n        y_pred = alg[1].predict(X_test).astype(int)\n        final_data = {'PassengerId': test_raw.PassengerId, 'Survived': y_pred}\n        submission = pd.DataFrame(data=final_data)\n        submission.to_csv('submissions\/submission_'+tag+'_'+alg[0]+'.csv', index =False)\n        \n    #create submissions best hard and soft vc\n    #they are already fitted in the evaluation steps\n    y_pred_b_vc_hard = b_vc_hard.predict(X_test).astype(int)\n    y_pred_b_vc_soft = b_vc_soft.predict(X_test) .astype(int)\n    vc_hard_sub_data = {'PassengerId': test_raw.PassengerId, 'Survived': y_pred_b_vc_hard}\n    vc_soft_sub_data = {'PassengerId': test_raw.PassengerId, 'Survived': y_pred_b_vc_soft}\n    vc_hard_sub = pd.DataFrame(data=vc_hard_sub_data)\n    vc_soft_sub = pd.DataFrame(data=vc_soft_sub_data)\n    vc_hard_sub.to_csv('submissions\/submission_'+tag+'_vc_hard.csv', index =False) \n    vc_soft_sub.to_csv('submissions\/submission_'+tag+'vc_soft.csv', index =False) \n    \n    print('Done')\n    ","fb46e2f7":"#after some debugging, we found that Ticket_len_10 is missing from the X_test, as it was auto generated in X_train, need to improve the process in the next project\nX_test.insert(17, \"Ticket_len_10\", 0, allow_duplicates = False)","d132ce5f":"gen_submissions(top_five_algs, vc_hard_all, vc_soft_all, X_train, y_train, X_test)","213944f3":"#creating a base model, the best result we got from single model approach was XGB, we will use it as our base model in this experiment\nxgb = XGBClassifier(n_estimators=500)  ","7f286c45":"#prepare data for baseline evaluation \ntrain_i2 = train_raw.copy()\ntrain_i2 = data_clean(train_i2)","575db1fd":"def base_f_eng (clean_data):\n    \n    #next we work on text data\n    #extrat title from Name\n    clean_data['Title'] = clean_data['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n    \n    stat_min = 10 #while small is arbitrary, we'll use the common minimum in statistics: http:\/\/nicholasjjackson.com\/2012\/03\/08\/sample-size-is-10-a-magic-number\/\n    title_names = (clean_data['Title'].value_counts() < stat_min) #this will create a true false series with title name as index\n    #apply and lambda functions are quick and dirty code to find and replace with fewer lines of code: https:\/\/community.modeanalytics.com\/python\/tutorial\/pandas-groupby-and-python-lambda-functions\/\n    clean_data['Title'] = clean_data['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\n    \n    \n    #do imputation on txt data\n    OH_en = OneHotEncoder(handle_unknown='ignore', sparse=False)\n    imp_cols = OH_en.fit_transform(clean_data[['Sex','Embarked', 'Title']])\n    imp_cols = pd.DataFrame(imp_cols)\n    #now we align the index and col names\n    imp_cols.index = clean_data[['Sex','Embarked', 'Title']].index\n    imp_cols.columns = OH_en.get_feature_names(['Sex','Embarked', 'Title'])\n    clean_data = clean_data.drop(['Sex','Embarked', 'Title'], axis=1).join(imp_cols)\n    \n    clean_data = clean_data.drop('Ticket', axis=1)\n    clean_data = clean_data.drop('Name', axis=1)\n\n    return clean_data","eaec1ecd":"def score (X, y):\n    cv_agebin_results = model_selection.cross_validate(xgb, X, y, cv = 5, scoring='accuracy', return_train_score=True)\n    return cv_agebin_results['test_score'].mean()","81586260":"def xysplit (df):\n    X = df.drop('Survived', axis=1)\n    y = df['Survived']\n    \n    return X, y","6cba3920":"train_i2_base = base_f_eng (train_i2)","4f574bf9":"X_base = train_i2_base.drop('Survived', axis=1)\ny_base = train_i2_base['Survived']","0b350233":"score (X_base, y_base)","50fe181e":"def age_bin (clean_data):\n    AgeBin = pd.cut(clean_data['Age'].astype(int), 5, labels=[1, 2, 3, 4, 5]).astype(int)\n    AgeBin.name = 'AgeBin'\n    train_i2_agebin = train_i2_base.join(AgeBin)\n    train_i2_agebin = train_i2_agebin.drop('Age', axis =1)\n    \n    X, y = xysplit(train_i2_agebin)\n    \n    return X, y","3dffa86c":"X_agebin, y_agebin = age_bin (train_i2)\nscore (X_agebin, y_agebin)","ab530d17":"def fare_bin (clean_data):\n    FareBin = pd.cut(clean_data['Fare'].astype(int), 5, labels=[1, 2, 3, 4, 5]).astype(int)\n    FareBin.name = 'FareBin'\n    train_i2_agebin = train_i2_base.join(FareBin)\n    train_i2_agebin = train_i2_agebin.drop('Fare', axis =1)\n    \n    X, y = xysplit(train_i2_agebin)\n    \n    return X, y","11d32d53":"X_farebin, y_farebin = fare_bin (train_i2)\nscore (X_farebin, y_farebin)","63c4af39":"#have all Fare values plus one to remove zeros\nfare_po = train_i2['Fare']+1","61d39c81":"from scipy import stats\nboxcox_fare = stats.boxcox(fare_po)","745f861d":"train_i2['Fare'].hist()","49adfa42":"boxcox_fare = pd.Series(boxcox_fare[0], index=train_i2['Fare'].index, name='boxcox_fare')","bf31371b":"boxcox_fare.hist(bins=50)","b1235e5a":"sqrt_fare = np.sqrt(train_i2['Fare'])\nsqrt_fare.name = 'sqrt_fare'","5106119e":"log_fare = np.log(train_i2['Fare'])\nlog_fare.name = 'log_fare'","1263dc2c":"sqrt_fare.hist(bins=50)","9308ac6a":"log_fare.hist(range=(0, 8), bins=50)","3f402d4d":"def fare_val (base_df, nor_fare):\n    \n    train_i2_nor_fare = base_df.join(nor_fare)\n    train_i2_nor_fare_final = train_i2_nor_fare.drop('Fare', axis=1)\n    X, y = xysplit(train_i2_nor_fare_final)\n    return score (X,y)","cd4dd0f2":"score_log = fare_val(train_i2_base, log_fare)\nprint(score_log)","eb8aa41d":"score_sqrt = fare_val(train_i2_base, sqrt_fare)\nprint(score_sqrt)","8c9db62c":"score_bc = fare_val(train_i2_base, boxcox_fare)\nprint(score_bc)","a5ee3838":"#what if we include both Farebin and nor_fare in the data\ndef fare_w_fbin_val (base_df, nor_fare):\n    \n    train_i2_nor_fare = base_df.join(nor_fare)\n    train_i2_nor_fare_final = train_i2_nor_fare.drop('Fare', axis=1)\n    train_i2_nor_fare_final['FareBin'] = X_farebin['FareBin']\n    X, y = xysplit(train_i2_nor_fare_final)\n    return score (X,y)","cb17e822":"score_wf = fare_w_fbin_val(train_i2_base, log_fare)\nprint(score_wf)","a82328e9":"train_i2_base.head()","d7166f8e":"#FamilySize evaluation\ntrain_i2_fz = train_i2_base.copy()\ntrain_i2_fz['FamilySize'] = train_i2_fz['SibSp']+train_i2_fz['Parch']+1\ntrain_i2_fz.head()","f46fe7ef":"X_fz, y_fz = xysplit(train_i2_fz)\nscore (X_fz, y_fz)","6fca2f4b":"train_i2_fz_noraw = train_i2_fz.drop(['SibSp', 'Parch'], axis=1)\nX_fz_nr, y_fz_nr = xysplit(train_i2_fz_noraw)\nscore (X_fz_nr, y_fz_nr)","8d0c625c":"train_i2_fz_isAlone = train_i2_fz_noraw.copy()\ntrain_i2_fz_isAlone['IsAlone'] = train_i2_fz_isAlone['FamilySize'] == 1\ntrain_i2_fz_isAlone['IsAlone'] = train_i2_fz_isAlone['IsAlone'].astype(int)","802d5d98":"X_fz_ia, y_fz_ia = xysplit(train_i2_fz_isAlone)\nscore (X_fz_ia, y_fz_ia)","01dd7b5c":"def ticket_len (df):\n    #making sure we are not massing with the raw data\n    clean_data = df.copy()\n    #cat tickets by length\n    Ticket_len = clean_data.Ticket.apply(len)\n    stat_min_ti = 30\n    Ticket_len_ls = (Ticket_len.value_counts() < stat_min_ti)\n    Ticket_len = Ticket_len.apply(lambda x: '30' if Ticket_len_ls.loc[x] == True else x)\n    Ticket_len = Ticket_len.astype(str)\n    \n    \n    #do imputation on txt data\n    OH_en = OneHotEncoder(handle_unknown='ignore', sparse=False)\n    imp_cols = OH_en.fit_transform(pd.DataFrame(Ticket_len))\n    imp_cols = pd.DataFrame(imp_cols)\n    #now we align the index and col names\n    imp_cols.index = Ticket_len.index\n    imp_cols.columns = OH_en.get_feature_names(['Ticket_len'])\n        \n\n    return imp_cols\n","327b231e":"Ticket_len = ticket_len (train_i2)\ntrain_i2_tl = train_i2_base.copy()\ntrain_i2_tl = train_i2_tl.join(Ticket_len)","f44922e1":"X_fz_tl, y_fz_tl = xysplit(train_i2_tl)\nscore (X_fz_tl, y_fz_tl)","31670c1c":"train_i2.head()","9f922690":"def interactions (df):\n    \n    PSE = df.Pclass.astype(str) + df.Sex + df.Embarked\n    \n    #do imputation on txt data\n    OH_en = OneHotEncoder(handle_unknown='ignore', sparse=False)\n    imp_cols = OH_en.fit_transform(pd.DataFrame(PSE))\n    imp_cols = pd.DataFrame(imp_cols)\n    #now we align the index and col names\n    imp_cols.index = PSE.index\n    imp_cols.columns = OH_en.get_feature_names(['Pclass_Sex_Embarked'])\n    \n    return imp_cols","9d3a97cd":"pse = interactions(train_i2)","9f4b0471":"train_i2_pse = train_i2_base.copy()\ntrain_i2_pse = train_i2_pse.join(pse)\nX_pse, y_pse = xysplit(train_i2_pse)\nscore (X_pse, y_pse)","bc43deba":"train_i2_pse_noraw = train_i2_pse.drop(['Pclass', 'Sex_male','Sex_female','Embarked_C','Embarked_Q','Embarked_S'], axis=1)\nX_pse_nr, y_pse_nr = xysplit(train_i2_pse_noraw)\nscore (X_pse_nr, y_pse_nr)","6364cf8b":"def f_eng_i2 (clean_data):\n    #let's start with num cols\n    #create value bins for continuouse values\n    clean_data['FareBin'] = pd.qcut(clean_data['Fare'], 5, labels=[1, 2, 3, 4, 5]).astype(int)\n    clean_data['AgeBin'] = pd.cut(clean_data['Age'].astype(int), 5, labels=[1, 2, 3, 4, 5]).astype(int)\n    #now we drop the original features\n    clean_data = clean_data.drop(['Fare', 'Age'], axis=1)\n    \n    #create new features\n    FamilySize = clean_data['SibSp']+clean_data['Parch']+1\n    clean_data['FamilySize'] = FamilySize\n    \n    #next we work on text data\n    #extrat title from Name\n    clean_data['Title'] = clean_data['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n    \n    stat_min = 10 #while small is arbitrary, we'll use the common minimum in statistics: http:\/\/nicholasjjackson.com\/2012\/03\/08\/sample-size-is-10-a-magic-number\/\n    title_names = (clean_data['Title'].value_counts() < stat_min) #this will create a true false series with title name as index\n    #apply and lambda functions are quick and dirty code to find and replace with fewer lines of code: https:\/\/community.modeanalytics.com\/python\/tutorial\/pandas-groupby-and-python-lambda-functions\/\n    clean_data['Title'] = clean_data['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\n    \n    #cat tickets by length\n    Ticket_len = clean_data.Ticket.apply(len)\n    stat_min_ti = 30\n    Ticket_len_ls = (Ticket_len.value_counts() < stat_min_ti)\n    Ticket_len = Ticket_len.apply(lambda x: '30' if Ticket_len_ls.loc[x] == True else x)\n    Ticket_len.value_counts()\n    clean_data['Ticket_len'] = Ticket_len\n    #imputer only works on str or numbers\n    clean_data['Ticket_len'] = clean_data['Ticket_len'].astype(str)\n    clean_data = clean_data.drop(['Name', 'Ticket'], axis=1)\n    \n    #adding Pclass_Sex_Embarked\n    PSE = clean_data.Pclass.astype(str) + clean_data.Sex + clean_data.Embarked\n    clean_data['Pclass_Sex_Embarked'] = PSE\n    \n    \n    #do imputation on txt data\n    OH_en = OneHotEncoder(handle_unknown='ignore', sparse=False)\n    imp_cols = OH_en.fit_transform(clean_data[['Title', 'Ticket_len', 'Pclass_Sex_Embarked']])\n    imp_cols = pd.DataFrame(imp_cols)\n    #now we align the index and col names\n    imp_cols.index = clean_data[['Title', 'Ticket_len', 'Pclass_Sex_Embarked']].index\n    imp_cols.columns = OH_en.get_feature_names(['Title', 'Ticket_len', 'Pclass_Sex_Embarked'])\n    clean_data = clean_data.drop(['Sex','Embarked', 'Title', 'Ticket_len', 'SibSp', 'Parch', 'Pclass', 'Pclass_Sex_Embarked'], axis=1).join(imp_cols)\n\n    return clean_data","b98095c3":"train_i2_af = f_eng_i2(train_i2)","eaa75171":"X_af, y_af = xysplit(train_i2_af)\nscore (X_af, y_af)","bf9ce917":"#create a feature engineering function that retains all features\ndef f_eng_all (clean_data):\n    #let's start with num cols\n    #create value bins for continuouse values\n    clean_data['FareBin'] = pd.qcut(clean_data['Fare'], 5, labels=[1, 2, 3, 4, 5]).astype(int)\n    clean_data['AgeBin'] = pd.cut(clean_data['Age'].astype(int), 5, labels=[1, 2, 3, 4, 5]).astype(int)\n    \n    #create new features\n    FamilySize = clean_data['SibSp']+clean_data['Parch']+1\n    clean_data['FamilySize'] = FamilySize\n    \n    #next we work on text data\n    #extrat title from Name\n    clean_data['Title'] = clean_data['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n    \n    stat_min = 10 #while small is arbitrary, we'll use the common minimum in statistics: http:\/\/nicholasjjackson.com\/2012\/03\/08\/sample-size-is-10-a-magic-number\/\n    title_names = (clean_data['Title'].value_counts() < stat_min) #this will create a true false series with title name as index\n    #apply and lambda functions are quick and dirty code to find and replace with fewer lines of code: https:\/\/community.modeanalytics.com\/python\/tutorial\/pandas-groupby-and-python-lambda-functions\/\n    clean_data['Title'] = clean_data['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\n    \n    #cat tickets by length\n    Ticket_len = clean_data.Ticket.apply(len)\n    stat_min_ti = 30\n    Ticket_len_ls = (Ticket_len.value_counts() < stat_min_ti)\n    Ticket_len = Ticket_len.apply(lambda x: '30' if Ticket_len_ls.loc[x] == True else x)\n    Ticket_len.value_counts()\n    clean_data['Ticket_len'] = Ticket_len\n    #imputer only works on str or numbers\n    clean_data['Ticket_len'] = clean_data['Ticket_len'].astype(str)\n    clean_data = clean_data.drop(['Name', 'Ticket'], axis=1)\n    \n    #adding Pclass_Sex_Embarked\n    PSE = clean_data.Pclass.astype(str) + clean_data.Sex + clean_data.Embarked\n    clean_data['Pclass_Sex_Embarked'] = PSE\n    \n    \n    #do imputation on txt data\n    OH_en = OneHotEncoder(handle_unknown='ignore', sparse=False)\n    imp_cols = OH_en.fit_transform(clean_data[['Sex','Embarked','Title', 'Ticket_len', 'Pclass_Sex_Embarked']])\n    imp_cols = pd.DataFrame(imp_cols)\n    #now we align the index and col names\n    imp_cols.index = clean_data[['Sex','Embarked','Title', 'Ticket_len', 'Pclass_Sex_Embarked']].index\n    imp_cols.columns = OH_en.get_feature_names(['Sex','Embarked','Title', 'Ticket_len', 'Pclass_Sex_Embarked'])\n    clean_data = clean_data.drop(['Sex','Embarked', 'Title', 'Ticket_len', 'Pclass_Sex_Embarked'], axis=1).join(imp_cols)\n\n    return clean_data","f68adb8e":"train_i2_all = f_eng_all(train_i2)","9170df00":"#creating a base score with all features included\nX_all, y_all = xysplit(train_i2_all)\nscore (X_all, y_all)","569bd33e":"def k_class (X, y, kc):\n    \n    #borrowed from https:\/\/www.kaggle.com\/matleonard\/feature-selection\n    # Keep 5 features\n    selector = feature_selection.SelectKBest(feature_selection.f_classif, k=kc)\n\n    X_new = selector.fit_transform(X, y)\n\n    # Get back the features we've kept, zero out all other features\n    selected_features = pd.DataFrame(selector.inverse_transform(X_new), \n                                     index=X.index, \n                                     columns=X.columns)\n\n    # Dropped columns have values of all 0s, so var is 0, drop them\n    selected_columns = selected_features.columns[selected_features.var() != 0]\n    selected_columns = selected_columns.insert(0, 'Survived')\n\n    return selected_columns\n    ","fb702182":"def run_kc (X, y, kc_ls):\n    for kc in kc_ls:\n        k_cols = k_class(X, y, kc)\n        X_kc, y_kc = xysplit(train_i2_all[k_cols])\n        auc_score = score (X_kc, y_kc)\n        print('k value {0} score {1}'.format(kc, auc_score))","34caa71d":"kc_ls = [5, 10, 15, 20]\nrun_kc (X_all, y_all, kc_ls)","22f6fac8":"def L_one (X, y):\n\n    # Set the regularization parameter C=1\n    logistic = linear_model.LogisticRegression(C=1, penalty=\"l1\", solver='liblinear', random_state=7).fit(X, y)\n    model = feature_selection.SelectFromModel(logistic, prefit=True)\n    X_new = model.transform(X)\n    # Get back the kept features as a DataFrame with dropped columns as all 0s\n    selected_features = pd.DataFrame(model.inverse_transform(X_new), \n                                 index=X.index,\n                                 columns=X.columns)\n\n    # Dropped columns have values of all 0s, keep other columns \n    selected_columns = selected_features.columns[selected_features.var() != 0]\n    selected_columns = selected_columns.insert(0, 'Survived')\n    return selected_columns","3d2a25f0":"L_cols = L_one(X_all, y_all)\nX_lc, y_lc = xysplit(train_i2_all[L_cols])\nscore (X_lc, y_lc)","92bbe3df":"base_L1_alg = base_model(X_lc, y_lc)","8805dd0a":"base_L1_alg","6b33a30f":"tuned_L1_algs, hp_compare = hp_tune(X_lc, y_lc, base_L1_alg)","b5358d9b":"hp_compare.sort_values(by='Best Score', ascending=False)","fe00b368":"vc_hard_L1, vc_soft_L1 = en_alg(tuned_L1_algs, X_lc, y_lc)","a332421f":"sorted_L1_algs = hp_compare.sort_values(by='Best Score', ascending=False)['Alg Name']\ntop_five_L1_algs = top_algs(sorted_L1_algs, tuned_L1_algs, 5)","035f4c44":"X_test_i2_clean = data_clean(test_raw)","bfdd3880":"X_test_i2_fe = f_eng_all(X_test_i2_clean)","ecaac07a":"#aligning columns\nX_test_i2_fe.insert(14, \"Ticket_len_10\", 0, allow_duplicates = False)\nX_test_i2_lc = X_test_i2_fe[X_lc.columns]","42274bac":"gen_submissions(top_five_L1_algs, vc_hard_L1, vc_soft_L1, X_lc, y_lc, X_test_i2_lc)","8fa3f33b":"ps_tuned_L1_algs = top_five_L1_algs\nbest_alg = ps_tuned_L1_algs[0][1].fit(X_lc, y_lc)","75ceecc6":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(best_alg, random_state=1).fit(X_lc, y_lc)\neli5.show_weights(perm, feature_names = X_lc.columns.tolist(), top = 30)","49611342":"import shap  # package used to calculate Shap values\n\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(best_alg)\n\n# calculate shap values. This is what we will plot.\n# Calculate shap_values for all of val_X rather than a single row, to have more data for plot.\nshap_values = explainer.shap_values(X_lc)\n\n# Make plot. Index of [1] is explained in text below.\nshap.summary_plot(shap_values, X_lc)","b82d48d1":"## Data Cleaning\nIn this stage, we want to fill in the holes in the data to lay the foundation for later imputations and feature engineering","7b785006":"#### Observation\nWe have close to 20% of Age data that are missing from the dataset, let's think of a cleverer way to fill in the gaps\n#### Action Plan\nThe idea is to find features that corrolate with Age, create a lookup table to impute missing Age values","c9621821":"### Numeric Data\n\nLet's look at the numeric data first ","ca431318":"### IsAlone and FamilySize\nInspired by the approach taken in the follow Kaggle notebooks, we would like to engineer two new features, IsAlone and FamilySize\n- [https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy](https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy)\n- https:\/\/www.notion.so\/Titanic-Data-Science-Solutions-Kaggle-a5a4aa2d5e024be88263390a26db3c6d#81f0bc93036e4f4ab87c1ac22e2d0e2d\n","c75d24ba":"#### Observation\nWe got a ~0.02 improvement on the score by engineering the Pclass_Sex_Embarked feature in our data, with the raw features removed\n\n(Update) again, the statement above was from the same analysis done on my local machine, therefore it's not agreeing with what we are see in the scorings. Based on the scores observed, we should drop this feature.\n\n#### Action Plan\nInclude Pclass_Sex_Embarked in our final data set and remove the raw features","a2a899c5":"## Abstract\n\nThis kernel was created for the purpose of defining the best features and models for the Kaggle Titanic dataset that together yields the best prediction score. This kernel contains details around the analysis and experiment undertaken in the effort to derive the best feature and model set. In addition, this analysis also reveals the causation effect on the prediction result of each engineered feature, various feature combinations derived from feature selection technique, and all features (raw and engineered).\n\n## Introduction \nWe are attempting to create an ML model or model that, given a set of inputs, provides a binary result indicating rather or not a passenger will survive the well-known Titanic shipwreck incident. This kernel contains detailed breakdowns of the analysis and experiments attempted in the effort to achieve the highest precision model. \n\nThe steps taken are grouped into two major sections: Iteration 1, where we conducted EDA, Feature Engineering and the attempts to build the most accurate model with our findings; Iteration 2, we sought to build upon the effort from Iteration 1 by validating the causation effect of each engineered feature on the prediction results to determine the best feature set for training our models. In addition, we also attempted a couple of new feature engineering techniques in this iteration, namely, interaction and normalization.\n\n\n## Conclusions\nReveals by our post-submission analysis, we see that Sex_female, Fare and Age are the top three features that have the most positive effect on the survival rate. Concluding from our Permutation Importance analysis and Shap analysis, we can predict with a level of certainty that a passenger who is a female passenger, in first class (Higher Fare range), and in a younger age range has the best chance of surviving the incident. \n\n### Modeling and Data Preperation\nFrom our Iteration 1 effort, we can conclude that blindly creating new features based on correlations and dropping or keeping the raw features without knowing the subsequent impacts will let to results that are difficult to interpret hard to improve on.\n\nOn the other hand, from Iteration 2, we noticed that results from baseline model evaluation on features do not always hold true in the final modelling evaluation. Specifically, we see that measures that resulted in a positive impact on the model accuracy based on our baseline model evaluation, when aggregated, did not lead to a stacked improvement as anticipated. Instead, we observed a much lower accuracy score when combining these measures. As a result, we propose that in future modellings, all features should be kept, and use feature selection techniques to generate the best dataset, and we should test different feature combinations for the optimal result.\n\n\n## Techniques Used\n### Data Cleaning\n- Fillna ()\n- Filling missing data from lookup tables\n### Feature Engineering\n- Data evaluation to determine subsequent feature engineering approaches\n    - Distribution\n    - Corrolation\n    - Text data patterns\n    - Categorical data\n    - Data clustering\n- OneHot Encoding\n- Interations\n- Normalization\n- Text Data Manipulation\n- Converting Continuous Data to Discrete Data\n- Productionization\n### Modeling\n- Cross Validation\n- Hyperparameter Tuning\n- Voting (Ensemble)\n- Grid Search\n- Feature Evaluation (base model evaluation)\n- Post Modeling Evaluation (Permutation Importance and Shap analysis)\n\n\n\n## Credits\n\nMany techniques and feature engineering approaches are inspired and borrowed from the sources listed below\n- https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy\n- https:\/\/www.kaggle.com\/kenjee\/titanic-project-example\n- https:\/\/www.kaggle.com\/learn\/overview\n\n===============================================================\n","bfcf26a7":"#### Observation\nConverting Age data to Agebin improve prediction result by roughtly, 0.07%. base score: 0.80694 vs imporved score 0.81258\n\n#### Action to take\nAdd AgeBin and drop Age","90e520fc":"#### Observation\nName values are pretty uniformly formatted\n\n#### Action Plan\nWe could use simple split() to extract titles","8c40c133":"now let's def the data_cleaning function","a0debdbc":"# Titanic Prediction - A Comprehensive Analysis and Experiment for the Kaggle Titanic Competition ","5b67e7c9":"#### Observations\nWe see that FareBin has a better corrolation with Survived, whereas AgeBin has a lower correlation coefficient than raw Age with Survived\n\n#### Action Plan\n- We will keep FareBin drop Fare\n- For Age vs Agebin, since the difference is small, and both Age and AgeBin are not tightly corrolated with Survived, for the sake of consistency we will keep AgeBin and drop Age","fdec3994":"### Text Data\n\nNext, we look at text data","cc28374b":"## Iteration 1 - Post submission conclusion\nWe obtained the highest score from the Soft Voting model (with top five models included), with a score of 0.78229 (obtained by submitting to the Kaggle competition). This put us into the top 23% range in the competion. While it is a satisfactory result, it is still less than expected. Specifically, it is 0.05 less then our validation score of 0.8317. \n\n### Plan for improvement\nWe might be able to tune the model to a more accurate state with the following techniques:\n- evaluate the causation effect of each engineered features\n- employ other feature engineering technique to generate more relavent features for better trained models","8cfe7a90":"#### Observation\n\nAccuracy score we got from our base model (xgb) is 0.80694, which is pretty pretty good given that the data is still in the primitive state.","ea80a3e8":"## Feature Engineering\n\nIn this stage, we are going to:\n- convert string values to numeric values\n- use onehot encoder to imputer categorical data\n- create new features from exsiting features","4e1327df":"#### Try Ticketlen","72fe079e":"#### Observation\nTest Accuracy Mean will be the only indicator we will be using in this evaluation. The best score is from the SVC classifier, with an accuracy score of 0.0.830507. This is pretty good for an pre-tune model. The others follow are, GBC, LinearSVC, LinearSVC, etc. all in the 0.82-0.83 range.\n\n#### Action Plan\nPerform hyperparameter tuning to see how much impovement we could gain.","948a4d1e":"## Testing our hypothesis\nWe are going to aggregate our finds to a feature engineering function and test the model using cross_validate","8ab374ad":"#### Observation\n- This data consists of numeric, string (object), and the Ticket column contains a combination of string and numarical values\n- We have a total 12 columns\n- Age has some missing data, marjority of Cabin data are missing, two missing values in Embarked\n- judging from std, min and max, we can conclude that data in PassengerId are pretty evenly distributed\n\n#### Action Plan\n- Split the data by dtype (string vs numeric) for targeted actions\n- Need to handle missing data in Age, Cabin (drop, due to the large amount), and Embarked\n- Consider dropping PassengerId due to the lack of trend in the data\n","f75fa6ea":"#### Observation\nEngineering the Ticket_len feature improves the result by 0.02, base score: 0.8238130752429171 vs imporved score 0.8440442841959339.\n\n(Update) Again, the above was from the analysis done on my local machine, based on the score observed here, we will not include this feature.\n\n#### Action\nAdd Ticket_len feature to the final dataset","70e50e39":"## Model Tuning\n\nNow we know the baseline score set, let's tune the model to see how much we can improve our models. We will use Grid Search to find the best hyperparameters.","34634741":"### Numeric Cols\nLet's first look at num data to see what needs to be done","5acce3b7":"### Bonus attempt - an ensemble model with only the top five models\n\nWe want to see if we could improve our voting score by combining only the top five models","c7969f86":"### Analyze and transform the Ticket column \n\n#### Observation\nNo obvious observable patterns in the data\n\n#### Action Plan\nLook at value len to see if we could extract some patterns out of it","28449146":"#### Observation\nThere are some changes in the ranking position, most noticeably XGBClassifier replaced SVC as the best performing model with a score of 0.838384, that 0.005 better than the un-tuned SVC model, which is not a lot. For SVC, the pre-tune score is roughly the same as the post-tune score. Overall, the scores are in the 0.83 range, which is pretty good. \n\n#### Action Plan\nSee if we can get even better scores with the ensemble approach. ","1fe288f9":"## Wrap all preprocesses to a single function\n\nWrapping our data cleaning and feature engineering processes into a single function","dee3c854":"#### Obervation\nOnly Boxcox showed improvement on the result but not significant, and including FareBin with Boxcox did not improve the result.\n\n(Update), the statement above was concluded from this analysis when running on local machine, and the improvement showed with Boxbox was minimum. However, as we can see from the results above, Boxbox resulted in a pretty plausable improvement, improved the score by 0.012. Based on the result we are seeing here, it would make sense to include this in our modeling. This also shows the randomness in one could experience when handling small datasets.\n\n#### Action to take\nNot to include the normalized Fare","6f094f1f":"#### Observation\nThe result is surprisingly bad, even worse than the original prediction score by 0.002. \n\n#### Action\nWe hypothesized that we that keeping all the features and use feature selection to craft the optimal dataset for the training would yeild a better score.","6b0da07d":"## Generate Submission","fd4a1fbd":"### Feature importance analysis\nTo understand what factors contribute most to the result and their respective weight, we ran PermutationImportance and generated a Shap value table for all features with the best model observed from our Iteration 2 effort. \n\nFrom the PermutationImportance table above, we immediately see that Sex_female, Fare and Age are the top three features that have the most impact on the survival rate. We further evaluate this with our Shap chart and see the distributions are being relatively consistent with the ranking we see in the PermutationImportance table. Meaning that the conclusion we drew from the PermutationImportance table represents the norm of the data.\n\nFinally, we can say with a level of certainty that a passenger who is a female passenger, in first class (Higher Fare range), and in a younger age range has the best chance of surviving the incident.","41ce9d84":"As expected, there is no observable trend the data, let's drop it","068ea2de":"## Setup the Enviroment","0d1893bc":"## New Techniques\nThe plan is to evaluate\n- Interation\n- Target Encoding - we did not implement this, as the data is relatively small, onehot is better suited\n- CatBoost Encoding - we did not implement this, as the data is relatively small, onehot is better suited\n\n### Interactions\nWe want to combine corrolating categorical features to create new features with stronger corrolation with the target, and based on our analysis, we decided to conduct an attempt with Pclass_Sex_Embarked combination.","14b5e413":"## Iteration 1\nIn this iteration of the experiment, we are going to do EDA to understand the data to detect outliers, holes; get a feel of the size of the data, potential for converting continuous data to discrete data; imputing categorical data, treatments for numeric data vs string data, skewed data, etc. Next, based on the findings, we will clean the data, then apply feature engineering techniques to convert string to numeric values, engineer new features and impute categorical data to prepare for model training.\n\nIn the modelling stage, we still attempt multiple models and compare the results; for scoring, we will use cross_val to determine the accuracy of the resulting models. After we have a set of model candidates, we will also attempt the ensemble approach to create models using the top five models resulting from the model evaluation effort.\n\nLastly, we will productionize our effort by creating functions to streamline the data cleaning, feature engineering, and modelling process.\n\n\n### Plan of Attack\n- Understand the data, size of the data (column\/row), size of missing data, columns to drop\n- Data Cleaning\n    - data outliers\n    - correct the data\n    - fillna()\n- EDA, look at numarical and object(string) data seperatly, to determine\n    - normalization\/scale the data\n    - determine potential feature engineering approaches\n- Feature Engineering according to the findings\n- Def function for data preprocessing\n- Run Models\n- Voting\n- Output the result\n","eb0af0a9":"Next we perform data imputation","fec5f3bc":"Let's take a closer look at 'PassengerId' to decide if we should drop it","8eb8bdf1":"This is much more workable than the raw data, let's apply the same cat approach we did for Title","7031b52a":"## Modeling\n\nIn this effort, we will first build a collection of base models with a set of base scores. Then, we are going perform hyperparameter optimization using Grid Search to find the best parameter combination for each model to find the top five tuned models for the final prediction. In addition, we will take an ensemble approach by taking all the tuned model and combine them with VotingClassifier to create an aggregated model that we are also going to use for the final prediction.","8fc84f87":"#### Obervation\nWe see that in an ensemble approach Soft Voting has a higher test scroe, 83.17, only slightly better than the Hard Voting approach\n\n#### Action Plan\nWe will include the soft voting model in the final model list to generate our submissions","cecdb9e0":"#### Observation\nWith k=5 we have the worse score, best scoure at 0.8114682066411399 when k=15\n\n#### Action Plan\nNot going to use k_class for feature selection, going to try the same procedure with L_one feature selection\n\n(update) we should use k=15","f78ea15e":"#### Observation\nIt is clear that majority of the data can categorized into 3-0-0, 1-0-0 and 2-0-0\n#### Action Plan\nLet's create a lookup table with 3-0-0, 1-0-0 and 2-0-0 as the three major classes, and put the rest into the 'other' bin","4892b098":"### Text Cols","58a5d115":"## Understand the data\n\nIn this stage, we take a bird's eye view on the data, to understand\n- size of the data\n- obvious errors, e.g. data outliers\n- holes in the data\n- the types of data we are dealing with, string, int, cat, date?\n","74f90de4":"## Iteration 2\nWe are not satisfy with the results we got from the first attempt, we want to try some advance feature engineering techniques\n\n### Objective\nEvaluate the effect on model accuracy for each feature engineering step taken in our last attempt, and try new feature engineering techniques \n\n### Action Plan\n- create a base model\n- evaluate each new feature created in iteration 1\n    - Agebin\n    - Farebin\n    - FamilySize\n    - IsAlone\n- Try new techniques\n    - Interactions\n    - normalizing continouse features\n    - Target Encoding\n    - CatBoost Encoding","55d1eef4":"## Iteration 2 - Post submission conclusion\nThe best score obtained from this round of submission was 0.78708, which is rougly 0.005 improvement from the best score we obtained from Iteration 1, and this put us into the top 15% in the competition. This score is obtained from the ETC model generated in our Iteration 2 effort.\n\nWhile a slight imporment was able to obtain from this second round of efforts, however, we were expecting more judging from the average 0.85 score obtained in our feature analysis. This could be due to overfitting or features we are using are simply not good enough.\n\n### Suggestion for improvment\n- Attempt other feature engineering techniques (e.g. Target encoding, other feature combinations, etc.)\n- Try out other models (neural network)","6685b7f0":"### Converge to a function\nNext, let's converge what we have just done to a single function","e5c4f57c":"#### Observation\nThe FareBin feature imporve score by 0.01, base score: 0.80694 vs imporved score 0.81707\n\n#### Action\nGenerate FareBin and drop Fare","9c25bfb7":"#### Observation \nEngineering the FamilySize feature improves the model, base score: 0.80694 vs imporved score 0.809183. And removing the original features degraded the score by 0.03\n\n#### Action\nAdd the FamilySize feature in our final modeling, and keep the raw data ['SibSp', 'Parch']","bf6e2980":"### Test our functions\n\nTesting our data cleaning and feature engineering functions for combining into a single wrapping function","15b5eb35":"#### Observation\nThe score obtained from our effort where all features are included (raw and engineered) is slightly better then the base score.\n\n#### Action Plan\nWe are going to conduct the same model training process with features selected from L_one","142cfd0b":"### Building the base model\n\nWe are going to borrow all the handy functions we created in the previous attempt with small tweaks to build out our base model to evalue each engineered feature","557e33a2":"### Analyze and transform the Name column \n\nSepcifically, we want to look at the data to see if there was any observable trend, potential clustering, or possible extraction of common elements","bd12af63":"#### Observation\nWhile significant imporvments are acheived on the training score for both hard and soft voting, a slight decrease on the test scores are observed. However, juding from the degree of imporvement observed on training score, we hypothesized that the top five approach might result more accurate models\n\n#### Action Plan\nWe will using the top five approach to generate our submissions.","a4ce91f7":"- large number of missing value in Cabin, let's drop it\n- only two missing value in Embarked, fill with most freq","adc51300":"### An Ensemble Approach\n\nLet's combine the tuned models to create aggregated model to see if we can get even better prediction scores.","fb5e3b91":"IsAlone has a pretty good corrolation with the target","fec9b9f0":"### Building out a base model set\nThe objective is to have a baseline score set to compare with the resulting scores in subsequent optimization efforts. Also, with this baseline score set, we know roughly how good the predictions will be with the dataset we crafted as a result of the above feature engineering effort","3a549c5c":"#### Observations\nWe have Age and Fare containing continouse valuse\n#### Action\nFor classification problem, we hypothesize that discrete values will be better fit for the algorithms, let's proceed with the conversions","fdc6f33a":"#### Observation\nAdding the IsAlone does not improve the result\n\n#### Action\nNot to add this new feature","b7a7f61f":"#### Observation\n- Sex and Embarked are ready for labling\n- Need to work on Name and Ticket\n\n#### Action\n- Analyze Name and Ticket\n- Impute Sex and Embarked"}}