{"cell_type":{"c2a5b099":"code","56cf0217":"code","21b5a5a6":"code","604a4430":"code","0d0a36ce":"code","3b384696":"code","9ca397df":"code","664dee97":"code","c1a87e28":"code","502a915d":"code","8454ffe9":"code","3676858d":"code","d4275958":"code","1f42cea9":"code","9a12f4e4":"code","e7c738ce":"code","074be89a":"code","7dfffda6":"code","01df7f3c":"code","e643068a":"code","8727494c":"markdown","202edd10":"markdown","e4a4f31e":"markdown","3541e1dd":"markdown","9dcf8a59":"markdown","bccb704d":"markdown","04bc470c":"markdown","f3aba6f7":"markdown","e033b30a":"markdown","551e9e21":"markdown","5104bf37":"markdown","c10a7dd5":"markdown","c537f15a":"markdown","7046b073":"markdown","bfddefb1":"markdown","18bf4dcd":"markdown","91de231c":"markdown","8b7920d9":"markdown","312ae195":"markdown"},"source":{"c2a5b099":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\ntitanic_df = pd.read_csv('..\/input\/titanic\/train.csv')\ntitanic_df.head(5)","56cf0217":"print('### info of train data ###\\n')\nprint(titanic_df.info())","21b5a5a6":"# processing the null data\n\ntitanic_df['Age'].fillna(titanic_df['Age'].mean(), inplace=True)\ntitanic_df['Cabin'].fillna('N',inplace=True)\ntitanic_df['Embarked'].fillna('N',inplace=True)\n\ntitanic_df.isnull().sum()","604a4430":"print('Sex \\n','--------------\\n', titanic_df['Sex'].value_counts(),'\\n')\nprint('Cabin \\n','--------------\\n', titanic_df['Cabin'].value_counts(),'\\n')\nprint('Embarked \\n','--------------\\n', titanic_df['Embarked'].value_counts(),'\\n')","0d0a36ce":"titanic_df['Cabin'] = titanic_df['Cabin'].str[:1]\nprint(titanic_df['Cabin'].value_counts())","3b384696":"titanic_df.groupby(['Sex','Survived'])['Survived'].count()","9ca397df":"sns.barplot(x='Sex',y='Survived',data=titanic_df)","664dee97":"sns.barplot(x='Pclass',y='Survived',hue='Sex',data=titanic_df)","c1a87e28":"def trans_category(age):\n    cat = ''\n    if age <=-1:cat='Unknown'\n    elif age<=5:cat='Baby'\n    elif age<=12:cat='Child'\n    elif age<=18:cat='Teenager'\n    elif age<=25:cat='Student'\n    elif age<=35:cat='Young Adult'\n    elif age<=60:cat='Adult'\n    else:cat='Elderly'\n        \n    return cat\n\ngroup_names = ['Unknown','Baby','Child','Teenager','Student','Young Adult','Adult','Elderly']\n\nplt.figure(figsize=(15,6))\ntitanic_df['Age_cat'] = titanic_df['Age'].apply(lambda x:trans_category(x))\nsns.barplot(x='Age_cat',y='Survived',hue='Sex',data=titanic_df,order=group_names)","502a915d":"from sklearn import preprocessing\n\ndef encode_feature(dataDF):\n    features = ['Cabin','Sex','Embarked']\n    for feature in features:\n        le = preprocessing.LabelEncoder()\n        le = le.fit(dataDF[feature])\n        dataDF[feature] = le.transform(dataDF[feature])\n        \n    return dataDF\n\ntitanic_df = encode_feature(titanic_df)\ntitanic_df.head()","8454ffe9":"# make the preprocessing process as functions\n\ndef fill_na(df):\n    df['Age'].fillna(titanic_df['Age'].mean(), inplace=True)\n    df['Cabin'].fillna('N',inplace=True)\n    df['Embarked'].fillna('N',inplace=True)\n    df['Fare'].fillna(0, inplace=True)\n    \n    return df\n\ndef drop_feature(df):\n    df.drop(['PassengerId','Name','Ticket'],axis=1,inplace=True)\n    \n    return df\n\ndef format_feature(df):\n    df['Cabin'] = df['Cabin'].str[:1]\n    features = ['Cabin','Sex','Embarked']\n    for feature in features:\n        le = preprocessing.LabelEncoder()\n        le = le.fit(df[feature])\n        df[feature] = le.transform(df[feature])\n        \n    return df\n    \ndef transform_features(df):\n    df = fill_na(df)\n    df = drop_feature(df)\n    df = format_feature(df)\n    \n    return df","3676858d":"titanic_df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ny_titanic_df = titanic_df['Survived']\nX_titanic_df = titanic_df.drop('Survived', axis=1)\n\ntransform_features(X_titanic_df)","d4275958":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_titanic_df, y_titanic_df, \n                                                    test_size=0.2, random_state=1)","1f42cea9":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score","9a12f4e4":"# decision tree\ndt_ = DecisionTreeClassifier(random_state=1)\ndt_.fit(X_train, y_train)\n\n# random forest\nrf_ = RandomForestClassifier(random_state=1)\nrf_.fit(X_train, y_train)\n\n# logistic regression\nlr_ = LogisticRegression(random_state=1)\nlr_.fit(X_train, y_train)\n","e7c738ce":"dt_pred = dt_.predict(X_test)\nrf_pred = rf_.predict(X_test)\nlr_pred = lr_.predict(X_test)","074be89a":"print('Accuarcy of decision tree: ', accuracy_score(y_test,dt_pred))\nprint('Accuracy of random forest: ', accuracy_score(y_test,rf_pred))\nprint('Accuracy of logistic regression: ', accuracy_score(y_test, lr_pred))","7dfffda6":"from sklearn.model_selection import KFold\n\ndef KFold_test(clf, folds=5):\n    kfold = KFold(n_splits=folds)\n    scores = []\n    \n    for iter_count, (train_index, test_index) in enumerate(kfold.split(X_titanic_df)):\n        X_train, X_test = X_titanic_df.values[train_index], X_titanic_df.values[test_index]\n        y_train, y_test = y_titanic_df.values[train_index], y_titanic_df.values[test_index]\n        \n        clf.fit(X_train,y_train)\n        pred = clf.predict(X_test)\n        accuracy = accuracy_score(y_test,pred)\n        scores.append(accuracy)\n        \n        print(\"cross-validation {0} accuracy {1}\".format(iter_count+1,accuracy))\n        \n    mean_score = np.mean(scores)\n    print(\"mean accuracy: \",mean_score)\n    \nKFold_test(dt_,folds=5)       ","01df7f3c":"from sklearn.model_selection import GridSearchCV\n\nparameters={'max_depth':[2,3,5,10],\n            'min_samples_split':[2,3,5],\n            'min_samples_leaf':[1,4,5,8]}\n\n\ngrid_dt = GridSearchCV(dt_,param_grid=parameters,scoring='accuracy',cv=5, refit=True)\ngrid_dt.fit(X_train,y_train)\n\n\nprint('best parameter: ', grid_dt.best_params_)\nprint('best accuracy: ', grid_dt.best_score_)\n\nbest_dt=grid_dt.best_estimator_","e643068a":"titanic_test = pd.read_csv('..\/input\/titanic\/test.csv')\ntransform_features(titanic_test)\ny_pred = grid_dt.predict(titanic_test)\n\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')\n\n# report\noutput = pd.DataFrame({'PassengerId': test_df.PassengerId, 'Survived': y_pred})\noutput.to_csv('submission.csv', index=False)","8727494c":"# Titanic ML ","202edd10":"the accuracy is more higher than before. It is bigger than random forest. Now we will find the best parameter of random forest.","e4a4f31e":"# check the data","3541e1dd":"**connection between 'Pclass' and 'Survived'**\n\nNot differenct as much, but 3 class have less survived value.","9dcf8a59":"# compare accuracy","bccb704d":"using the function that I made, apply this into original train and test data.","04bc470c":"There is some problem in 'Cabin'. Because it have too much value!! We have to deal with this problem and I think the cabin alphabet(related with class) is more important than the number. So we have to extract the alphabet in 'Cabin' value.","f3aba6f7":"We have to change the data whose Dtype is object. Checking the info, we find out 'Sex', 'Cabin' and 'Embarked' is object data. You don't have to worry about 'Name'. It is just for distinguish the each value.\n\ncheck the value of 'Sex', 'Cabin' and 'Embarked'.","e033b30a":"Among the tree ML, logistic regression have the highest accuracy. But we haven't set the hyperparameter and amount of the data is not enough. So still don't choose the best ML. So now we evaluate the decision tree according to cross-validation test.","551e9e21":"**transform categoric variable**\n\nWe have to transform categorical variable into number. Transform 'Cabin', 'Sex', 'Embarked' into number","5104bf37":"# train the model\n\nwe will use decision tree, random forest and logistic regression. To compare the accuracy, we will use accuracy_score().","c10a7dd5":"# split the data","c537f15a":"# predict test data","7046b073":"There are some non data in 'Age', 'Cabin', 'Embarked'. We have to preprocess about this non data. I am going to embark mean value to 'Age', and 'N' to 'Cabin', 'Embarked'","bfddefb1":"# EDA","18bf4dcd":"# conclusion","91de231c":"It record the best accuracy !! we finally found the best model.","8b7920d9":"**connection between 'Age' and 'Survived'**\n\nbaby have high probability of survived. and female Elderly also have high probability of survived\n\n\n**tranform 'Age' value into categoric**\n\n* 0~5 : Baby\n* 6~12 : Child\n* 13~18 : Teenager\n* 19~25 : Student\n* 26~35 : Young Adult\n* 36~60 : Adult\n* 61~ : Elderly","312ae195":"**connection between 'Sex' and 'Survived'**\n\nfemale survived more than male"}}