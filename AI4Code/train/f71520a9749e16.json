{"cell_type":{"b9d4f79e":"code","4bbbf770":"code","3d896aef":"code","558d2bc9":"code","f57a0335":"code","bba71959":"code","44f044a5":"code","c94da50d":"code","9994071d":"code","b6cf05ae":"code","916a5845":"code","7574c4c6":"code","d756038b":"code","4782d650":"code","63215708":"code","3ec3881a":"code","bcfc81eb":"code","e8299cfe":"code","e7cfb6ca":"code","52e5b043":"code","baa71906":"code","b1eb3b9b":"code","be99c984":"code","d1bfe7b3":"markdown"},"source":{"b9d4f79e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","4bbbf770":"IMG_SIZE = 28\nVERSION = 'simple'","3d896aef":"X_train = np.loadtxt('..\/input\/train.csv', skiprows=1, delimiter=',')\nX_train = np.delete(X_train, 0, axis=1)\nlabels = np.loadtxt('..\/input\/train.csv', usecols= 0, skiprows=1, delimiter=',', dtype= np.int)\nX_test = np.loadtxt('..\/input\/test.csv', skiprows=1, delimiter=',')","558d2bc9":"def standarize_normalize_set(dataset):\n    return dataset \/ 255.0\ndef reshape_x_for_cnn(dataset, dim1, dim2):\n    return dataset.reshape(-1, dim1, dim2, 1)","f57a0335":"def one_hot_encoding(dataset, dim):\n    dataset_encoded = np.zeros((len(dataset), dim))\n    dataset_encoded[np.arange(len(dataset)), labels] = 1\n    return dataset_encoded","bba71959":"def softmax(matrix):\n    e_x = np.exp(matrix - np.max(matrix))\n    return e_x \/ e_x.sum()\n\ndef sigmoid(z):\n    s = 1 \/ (1 + np.exp(-z))\n    return s\n\ndef compute_multiclass_loss(Y, Y_hat):\n\n    L_sum = np.sum(np.multiply(Y, np.log(Y_hat)))\n    m = Y.shape[0]\n    L = -(1\/m) * L_sum\n\n    return L","44f044a5":"def train_ann(X_train, y_train, learning_rate = 0.1, epochs=1000):\n\n    n_x = X_train.shape[1]\n    n_h = 45\n    n_h2 = 14\n    digits = 10\n\n    W1 = np.random.randn(n_x, n_h)\n    b1 = np.zeros((1, n_h))\n\n    W2 = np.random.randn(n_h, n_h2)\n    b2 = np.zeros((1, n_h2))\n\n    W3 = np.random.randn(n_h2, digits)\n    b3 = np.zeros((1, digits))\n\n    for i in range(epochs):\n\n        m = X_train.shape[1]\n\n        Z1 = np.matmul(X_train, W1) + b1\n        A1 = sigmoid(Z1)\n\n        Z2 = np.matmul(A1, W2) + b2\n        A2 = sigmoid(Z2)\n\n        Z3 = np.matmul(A2, W3) + b3\n        A3 = np.exp(Z3) \/ np.sum(np.exp(Z3), axis=1, keepdims=True)\n\n        cost = compute_multiclass_loss(y_train, A3)\n\n        dZ3 = A3-y_train\n        dW3 = (1.\/m) * np.matmul(A2.T, dZ3)\n        db3 = (1.\/m) * np.sum(dZ3, axis=0, keepdims=True)\n\n        dA2 = np.matmul(dZ3, W3.T)\n        dZ2 = dA2 * sigmoid(Z2) * (1 - sigmoid(Z2))\n        dW2 = (1.\/m) * np.matmul(A1.T, dZ2)\n        db2 = (1.\/m) * np.sum(dZ2, axis=0, keepdims=True)\n\n        dA1 = np.matmul(dZ2, W2.T)\n        dZ1 = dA1 * sigmoid(Z1) * (1 - sigmoid(Z1))\n        dW1 = (1.\/m) * np.matmul(X_train.T, dZ1)\n        db1 = (1.\/m) * np.sum(dZ1, axis=0, keepdims=True)\n\n        W3 = W3 - learning_rate * dW3\n        b3 = b3 - learning_rate * db3\n        W2 = W2 - learning_rate * dW2\n        b2 = b2 - learning_rate * db2\n        W1 = W1 - learning_rate * dW1\n        b1 = b1 - learning_rate * db1\n\n        if (i % 100 == 0):\n            print(\"Epoch\", i, \"cost: \", cost)\n            \n    return W1, b1, W2, b2, W3, b3","c94da50d":"def predict_new_data(X_test, W1, b1, W2, b2, W3, b3):\n    Z1 = np.matmul(X_test, W1) + b1\n    A1 = sigmoid(Z1)\n\n    Z2 = np.matmul(A1, W2) + b2\n    A2 = sigmoid(Z2)\n\n    Z3 = np.matmul(A2, W3) + b3\n    A3 = np.exp(Z3) \/ np.sum(np.exp(Z3), axis=1, keepdims=True)\n\n    y_hat= np.argmax(A3, axis=1)\n    \n    return y_hat","9994071d":"X_train = standarize_normalize_set(X_train)\ny_train = one_hot_encoding(labels, 10)\nW1, b1, W2, b2, W3, b3 = train_ann(X_train, y_train,learning_rate = 0.01, epochs=10000)","b6cf05ae":"X_test = standarize_normalize_set(X_test)\ny_pred = predict_new_data(X_test, W1, b1, W2, b2, W3, b3)","916a5845":"pd.DataFrame({'ImageId': np.arange(1, len(y_pred)+1), 'Label': y_pred}).to_csv(\"Submission_normal.csv\", index=False)","7574c4c6":"del W1, b1, W2, b2, W3, b3","d756038b":"X_train = np.loadtxt('..\/input\/train.csv', skiprows=1, delimiter=',')\nX_train = np.delete(X_train, 0, axis=1)\nlabels = np.loadtxt('..\/input\/train.csv', usecols= 0, skiprows=1, delimiter=',', dtype= np.int)\nX_test = np.loadtxt('..\/input\/test.csv', skiprows=1, delimiter=',')","4782d650":"y_train = one_hot_encoding(labels, 10)\n\nX_train = standarize_normalize_set(X_train)\nX_test = standarize_normalize_set(X_test)\nX_train = reshape_x_for_cnn(X_train, IMG_SIZE, IMG_SIZE)\nX_test = reshape_x_for_cnn(X_test, IMG_SIZE, IMG_SIZE)","63215708":"def convulving_matrix(input_matrix, conv_kernel, stride=(1, 1), pad_method='same', bias=1):\n\n    input_h, input_w, input_d = input_matrix.shape[0], input_matrix.shape[1], input_matrix.shape[2]\n    kernel_h, kernel_w, kernel_d = conv_kernel.shape[0], conv_kernel.shape[1], conv_kernel.shape[2]\n    stride_h, stride_w = stride[0], stride[1]\n\n    if pad_method == 'same':\n        # same is the method to returns pciture of the same size\n        # so we are zero-padding around it\n        output_h = int(np.ceil(input_matrix.shape[0] \/ float(stride[0])))\n        output_w = int(np.ceil(input_matrix.shape[1] \/ float(stride[1])))\n        output_d = input_d\n        output = np.zeros((output_h, output_w, output_d))\n\n        pad_h = max((output_h - 1) * stride[0] + conv_kernel.shape[0] - input_h, 0)\n        pad_h_offset = int(np.floor(pad_h\/2))  \n        pad_w = max((output_w - 1) * stride[1] + conv_kernel.shape[1] - input_w, 0)\n        pad_w_offset = int(np.floor(pad_w\/2))\n\n        padded_matrix = np.zeros((output_h + pad_h, output_w + pad_w, input_d))\n\n        for l in range(input_d):\n            for i in range(input_h):\n                for j in range(input_w):\n                    padded_matrix[i + pad_h_offset, j + pad_w_offset, l] = input_matrix[i, j, l]\n\n        for l in range(output_d):\n            for i in range(output_h):\n                for j in range(output_w):\n                    curr_region = padded_matrix[i*stride_h : i*stride_h + kernel_h, j*stride_w : j*stride_w + kernel_w, l]\n                    output[i, j, l] = (conv_kernel[..., l] * curr_region).sum()\n\n    elif pad_method == 'valid':\n\n        output_h = int(np.ceil((input_matrix.shape[0] - kernel_h + 1) \/ float(stride[0])))\n        output_w = int(np.ceil((input_matrix.shape[1] - kernel_w + 1) \/ float(stride[1])))\n        output = np.zeros((output_h, output_w, layer+1))\n\n        for l in range(layer + 1):\n            for i in range(output_h):\n                for j in range(output_w): \n                    curr_region = input_matrix[i*stride_h:i*stride_h+kernel_h, j*stride_w:j*stride_w+kernel_w, l]\n                    output[i, j, l] = (conv_kernel[..., l] * curr_region).sum()\n\n    output = np.sum(output, axis=2) + bias\n\n    return output","3ec3881a":"def convulve_whole_dataset(matrix, method='simple'):\n      \n    range_ = matrix.shape[0]\n    if method == 'simple':\n        K  = np.array([0, -1, 0, -1, 5, -1, 0, -1, 0]).reshape((3, 3, 1))\n        I = np.zeros((range_, matrix.shape[1], matrix.shape[2]))\n\n        for number in range(range_):\n            I[number, :, :] = convulving_matrix(matrix[number], K)\n        G = I.reshape(-1, IMG_SIZE * IMG_SIZE)\n \n    else:\n        Kx  = np.array([-1, 0, 1, -2, 0, 2, -1, 0, 1]).reshape((3, 3, 1))\n        Ky = np.array([1, 2, 1, 0, 0, 0, -1, -2, -1]).reshape((3, 3, 1))\n\n        Ix = np.zeros((range_, matrix.shape[1], matrix.shape[2]))\n        Iy = np.zeros((range_, matrix.shape[1], matrix.shape[2]))\n\n        for number in range(range_):\n            Ix[number, :, :] = convulving_matrix(matrix[number], Kx)\n            Iy[number, :, :] = convulving_matrix(matrix[number], Ky)\n\n        G = np.hypot(Ix, Iy)\n        G = G \/ G.max() * 255\n        G = G.reshape(-1, IMG_SIZE * IMG_SIZE)\n        \n        theta = np.arctan2(Iy, Ix)\n    \n    return G","bcfc81eb":"def visualize_example(dataset_x, dataset_y, object_):\n    print(dataset_y[object_])\n    plt.imshow(dataset_x[object_, :].reshape(IMG_SIZE, IMG_SIZE), cmap='gray')","e8299cfe":"visualize_example(X_train, y_train, 10)","e7cfb6ca":"X_train_ = convulve_whole_dataset(X_train, VERSION)\nX_test_ = convulve_whole_dataset(X_test, VERSION)\ny_train_ = y_train.copy()","52e5b043":"visualize_example(X_train_, y_train_, 10)","baa71906":"W1, b1, W2, b2, W3, b3 = train_ann(X_train_, y_train_,learning_rate = 0.01, epochs=10000)","b1eb3b9b":"y_pred = predict_new_data(X_test_, W1, b1, W2, b2, W3, b3)","be99c984":"pd.DataFrame({'ImageId': np.arange(1, len(y_pred)+1), 'Label': y_pred}).to_csv(\"Submission_{}.csv\".format(VERSION), index=False)","d1bfe7b3":"# Method 2"}}