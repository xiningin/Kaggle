{"cell_type":{"c95e27d4":"code","edeb3cea":"code","e30782a3":"code","d730df25":"code","e63661ed":"code","3b5da20d":"code","8f628508":"code","ac9c8ce2":"code","ef987785":"markdown","efe1cefd":"markdown","726ffb3f":"markdown","0d3df03a":"markdown"},"source":{"c95e27d4":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","edeb3cea":"from sklearn import datasets\niris = datasets.load_iris()\n\nX1_sepal = iris.data[:,[0,1]]\nX2_petal = iris.data[:,[2,3]]\ny = iris.target\n\nprint(X1_sepal[1:5,:])\nprint(X2_petal[1:5,:])\nprint(y)  ","e30782a3":"plt.figure(figsize=(15, 5))\n\nplt.subplot(1,2,1)\nplt.scatter(X1_sepal[:, 0], X1_sepal[:, 1], c=y)\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\n\nplt.subplot(1,2,2)\nplt.scatter(X2_petal[:, 0], X2_petal[:, 1], c=y)\nplt.xlabel('Petal length')\nplt.ylabel('Petal width')","d730df25":"from matplotlib.colors import ListedColormap\n\ndef plot_decision_regions(X,y,classifier,test_idx=None,resolution=0.02):\n    \n    # Initialise the marker types and colors\n    markers = ('s','x','o','^','v')\n    colors = ('red','blue','lightgreen','gray','cyan')\n    color_Map = ListedColormap(colors[:len(np.unique(y))]) #we take the color mapping correspoding to the \n                                                            #amount of classes in the target data\n    \n    # Parameters for the graph and decision surface\n    x1_min = X[:,0].min() - 1\n    x1_max = X[:,0].max() + 1\n    x2_min = X[:,1].min() - 1\n    x2_max = X[:,1].max() + 1\n    xx1, xx2 = np.meshgrid(np.arange(x1_min,x1_max,resolution),\n                           np.arange(x2_min,x2_max,resolution))\n    \n    Z = classifier.predict(np.array([xx1.ravel(),xx2.ravel()]).T)\n    Z = Z.reshape(xx1.shape)\n    \n    plt.contour(xx1,xx2,Z,alpha=0.4,cmap = color_Map)\n    plt.xlim(xx1.min(),xx1.max())\n    plt.ylim(xx2.min(),xx2.max())\n    \n    # Plot samples\n    X_test, Y_test = X[test_idx,:], y[test_idx]\n    \n    for idx, cl in enumerate(np.unique(y)):\n        plt.scatter(x = X[y == cl, 0], y = X[y == cl, 1],\n                    alpha = 0.8, c = color_Map(idx),\n                    marker = markers[idx], label = cl\n                   )","e63661ed":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n#######################################################################\n## SPLITTING\n\n\nX_train_sepal, X_test_sepal, y_train_sepal, y_test_sepal = train_test_split(X1_sepal,y,test_size=0.3,random_state=0)\n\nprint(\"# training samples sepal: \", len(X_train_sepal))\nprint(\"# testing samples sepal: \", len(X_test_sepal))\n\nX_train_petal, X_test_petal, y_train_petal, y_test_petal = train_test_split(X2_petal,y,test_size=0.3,random_state=0)\n\nprint(\"# training samples petal: \", len(X_train_petal))\nprint(\"# testing samples petal: \", len(X_test_petal))\n\n#####################################################################\n## SCALING\n\nsc = StandardScaler()\nX_train_sepal_std = sc.fit_transform(X_train_sepal)\nX_test_sepal_std = sc.transform(X_test_sepal)\n\nsc = StandardScaler()\nX_train_petal_std = sc.fit_transform(X_train_petal)\nX_test_petal_std = sc.transform(X_test_petal)\n\n#####################################################################\n## COMBINING FOR FUTURE PLOTTING\n\nX_combined_sepal_standard = np.vstack((X_train_sepal_std,X_test_sepal_std))\nY_combined_sepal = np.hstack((y_train_sepal, y_test_sepal))\n\nX_combined_petal_standard = np.vstack((X_train_petal_std,X_test_petal_std))\nY_combined_petal = np.hstack((y_train_petal, y_test_petal))","3b5da20d":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import validation_curve\n\nC_param_range = [0.001,0.01,0.1,1,10,100]\n\nsepal_acc_table = pd.DataFrame(columns = ['C_parameter','Accuracy'])\nsepal_acc_table['C_parameter'] = C_param_range\n\nplt.figure(figsize=(10, 10))\n\nj = 0\nfor i in C_param_range:\n    \n    # Apply logistic regression model to training data\n    lr = LogisticRegression(penalty = 'l2', C = i,random_state = 0)\n    lr.fit(X_train_sepal_std,y_train_sepal)\n    \n    # Predict using model\n    y_pred_sepal = lr.predict(X_test_sepal_std)\n    \n    # Saving accuracy score in table\n    sepal_acc_table.iloc[j,1] = accuracy_score(y_test_sepal,y_pred_sepal)\n    j += 1\n    \n    # Printing decision regions\n    plt.subplot(3,2,j)\n    plt.subplots_adjust(hspace = 0.4)\n    plot_decision_regions(X = X_combined_sepal_standard\n                      , y = Y_combined_sepal\n                      , classifier = lr\n                      , test_idx = range(105,150))\n    plt.xlabel('Sepal length')\n    plt.ylabel('Sepal width')\n    plt.title('C = %s'%i)","8f628508":"petal_acc_table = pd.DataFrame(columns = ['C_parameter','Accuracy'])\npetal_acc_table['C_parameter'] = C_param_range\n\nplt.figure(figsize=(10, 10))\n\nj = 0\nfor i in C_param_range:\n    \n    # Apply logistic regression model to training data\n    lr = LogisticRegression(penalty = 'l2', C = i,random_state = 0)\n    lr.fit(X_train_petal_std,y_train_petal)\n    \n    # Predict using model\n    y_pred_petal = lr.predict(X_test_petal_std)\n    \n    # Saving accuracy score in table\n    petal_acc_table.iloc[j,1] = accuracy_score(y_test_petal,y_pred_petal)\n    j += 1\n    \n    # Printing decision regions\n    plt.subplot(3,2,j)\n    plt.subplots_adjust(hspace = 0.4)\n    plot_decision_regions(X = X_combined_petal_standard\n                      , y = Y_combined_petal\n                      , classifier = lr\n                      , test_idx = range(105,150))\n    plt.xlabel('Petal length')\n    plt.ylabel('Petal width')\n    plt.title('C = %s'%i)\n","ac9c8ce2":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import validation_curve\n### 1. Use of validation curves for both datasets.\nC_param_range = [0.001,0.01,0.1,1,10,100,1000]\n\nplt.figure(figsize=(15, 10))\n\n# Apply logistic regression model to training data\nlr = LogisticRegression(penalty='l1',C = i,random_state = 0)\n\n# SEPAL Plot validation curve\ntrain_sepal_scores, test_sepal_scores = validation_curve(estimator=lr\n                                                            ,X=X_combined_sepal_standard\n                                                            ,y=Y_combined_sepal\n                                                            ,param_name='C'\n                                                            ,param_range=C_param_range\n                                                            )\n\ntrain_sepal_mean = np.mean(train_sepal_scores,axis=1)\ntrain_sepal_std = np.std(train_sepal_scores,axis=1)\ntest_sepal_mean = np.mean(test_sepal_scores,axis=1)\ntest_sepal_std = np.std(test_sepal_scores,axis=1)\n\nplt.subplot(2,2,1)\nplt.plot(C_param_range\n            ,train_sepal_mean\n            ,color='blue'\n            ,marker='o'\n            ,markersize=5\n            ,label='training accuracy')\n    \nplt.plot(C_param_range\n            ,test_sepal_mean\n            ,color='green'\n            ,marker='x'\n            ,markersize=5\n            ,label='test accuracy') \n    \nplt.xlabel('C_parameter')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.ylim([0.5,1])\n\n\n\n# PETAL Plot validation curve\ntrain_petal_scores, test_petal_scores = validation_curve(estimator=lr\n                                                            ,X=X_combined_petal_standard\n                                                            ,y=Y_combined_petal\n                                                            ,param_name='C'\n                                                            ,param_range=C_param_range\n                                                            )\n\n\ntrain_petal_mean = np.mean(train_petal_scores,axis=1)\ntrain_petal_std = np.std(train_petal_scores,axis=1)\ntest_petal_mean = np.mean(test_petal_scores,axis=1)\ntest_petal_std = np.std(test_petal_scores,axis=1)\n\nplt.subplot(2,2,2)\nplt.plot(C_param_range\n            ,train_petal_mean\n            ,color='blue'\n            ,marker='o'\n            ,markersize=5\n            ,label='training accuracy')\n    \nplt.plot(C_param_range\n            ,test_petal_mean\n            ,color='green'\n            ,marker='x'\n            ,markersize=5\n            ,label='test accuracy') \n    \nplt.xlabel('C_parameter')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.ylim([0.5,1])\nplt.xlim([0,200])","ef987785":"Bias is the simplifying assumptions made by the model to make the target function easier to approximate. \nVariance is the amount that the estimate of the target function will change given different training data.","efe1cefd":"We will focus our analysis on 2D datasets. This means that, instead of trying to predict flower classes by using all 4 features, we will analyse separately the sepal and petal information.\nThis is done for visualisation purposes which will enable us to better understand what the algorithm does when performing parameter tuning to it.","726ffb3f":"### Visualising the data","0d3df03a":"Testing different parameters to understand how accuracies change.\nUnderstanding how decision regions change when using different regularization values.\nRemember that we use paramter C as our regularization parameter. Parameter C = 1\/\u03bb.\nLambda (\u03bb) controls the trade-off between allowing the model to increase it's complexity as much as it wants with trying to keep it simple. For example, if \u03bb is very low or 0, the model will have enough power to increase it's complexity (overfit) by assigning big values to the weights for each parameter. If, in the other hand, we increase the value of \u03bb, the model will tend to underfit, as the model will become too simple.\nParameter C will work the other way around. For small values of C, we increase the regularization strength which will create simple models which underfit the data. For big values of C, we low the power of regularization which imples the model is allowed to increase it's complexity, and therefore, overfit the data.\n1. Testing sepal data with different regularization values."}}