{"cell_type":{"f20ee3e0":"code","dcb2bee4":"code","04209455":"code","c689817a":"code","0383a873":"code","aa647215":"code","ae6fb5ad":"code","bd2a5d52":"code","10dd8db6":"code","b9782098":"code","15eddeda":"code","8be11742":"code","5733f10c":"code","fe6a81cf":"code","44415682":"code","dd3d4a3b":"code","ed55e651":"code","fb097eb2":"code","2a7884d2":"code","4f9540b1":"code","4d8cd458":"code","cd051ca4":"code","3472d4d3":"code","becfc74d":"code","fc48116e":"code","055ebdcc":"code","628c76a8":"code","03011554":"code","d593b1ba":"code","678820f3":"code","75aafd9d":"code","cc6104f1":"code","8ba1e0c6":"code","e0f9f360":"code","75f9f8f4":"code","6602efb7":"code","051bb9f0":"code","33667efb":"code","18444314":"code","1efd143e":"code","5d4c77e8":"code","e41f0346":"markdown","41f028ae":"markdown","28c4fc04":"markdown","dcee77ae":"markdown","57f97091":"markdown","0d9047ab":"markdown","d51dbf0f":"markdown","87f4655c":"markdown","05f02e03":"markdown","3d5e8c33":"markdown","46c4b20c":"markdown","95c71a0a":"markdown","b872c963":"markdown","546ae3f0":"markdown","7f656d6e":"markdown","7e6de3f4":"markdown","079b9576":"markdown","f508accb":"markdown","cf2ee5d7":"markdown","6ce7222f":"markdown","65b3b050":"markdown","c25effec":"markdown","7bf7c2bc":"markdown","2fccdaab":"markdown","26a45910":"markdown","f42c93ea":"markdown"},"source":{"f20ee3e0":"# Imports\nimport os\nimport torch\nimport torchvision as tv\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import ToTensor\nimport torchvision.transforms as tt\nfrom torchvision.utils import make_grid\nimport torchvision.transforms as transforms\nfrom torch.utils.data import random_split\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms, models, datasets\nimport matplotlib.pyplot as plt\n%matplotlib inline","dcb2bee4":"project_name = '002facialexpressorecoglogisticapproach'","04209455":"data_dir = '..\/input\/facial-expression-recog-image-ver-of-fercdataset\/Dataset'\n\nprint(os.listdir(data_dir))\nclasses = os.listdir(data_dir + '\/train')\nprint(classes)","c689817a":"anger_files = os.listdir(data_dir + '\/train\/anger')\nprint('Total no. of images for training anager class: ',len(anger_files))","0383a873":"for i in classes:\n    var_files = os.listdir(data_dir + '\/train\/' + i)\n    print(i,': ',len(var_files))","aa647215":"dataset = ImageFolder(data_dir + '\/train', transform = ToTensor())","ae6fb5ad":"print(dataset)","bd2a5d52":"img, label = dataset[0]\nprint(img.shape, label)\nimg","10dd8db6":"print(dataset.classes)","b9782098":"def show_example(img, label):\n    print('Label: ', dataset.classes[label], \"(\"+str(label)+\")\")\n    plt.imshow(img.permute(1, 2, 0))","15eddeda":"show_example(*dataset[172])","8be11742":"len(dataset)","5733f10c":"val_size = int(0.1*32298)\ntrain_size = len(dataset) - val_size\n\ntrain_ds, val_ds = random_split(dataset,[train_size, val_size])\n\ntest_ds = ImageFolder(data_dir + '\/test', transform = ToTensor())","fe6a81cf":"print(train_ds)\nprint(val_ds)\nprint(test_ds)","44415682":"# Hyperparmeters\nbatch_size = 64\n\n# Other constants\ninput_size = 3*48*48\nnum_classes = 7","dd3d4a3b":"train_loader = DataLoader(\n    train_ds, \n    batch_size, \n    shuffle=True         )\n\nval_loader = DataLoader(\n    val_ds, \n    batch_size*2       )\n\ntest_loader = DataLoader(\n    test_ds, \n    batch_size*2        )","ed55e651":"show_example(*train_ds[1])","fb097eb2":"for images, _ in train_loader:\n    print('images.shape:', images.shape)\n    plt.figure(figsize=(16,8))\n    plt.axis('off')\n    plt.imshow(make_grid(images, nrow=16).permute((1, 2, 0)))\n    break","2a7884d2":"class FacialExprRecog(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(input_size, num_classes)\n    \n    def forward(self, xb):\n        xb = xb.view(xb.size(0), -1)#else xb.reshape(-1, 3*48*48)\n        out = self.linear(xb)\n        return out\n    \n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc.detach()}\n    \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n    \nmodel = FacialExprRecog()","4f9540b1":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))","4d8cd458":"def evaluate(model, val_loader):\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n    history = []\n    optimizer = opt_func(model.parameters(), lr)\n    for epoch in range(epochs):\n        # Training Phase \n        for batch in train_loader:\n            loss = model.training_step(batch)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        # Validation phase\n        result = evaluate(model, val_loader)\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","cd051ca4":"evaluate(model, val_loader)","3472d4d3":"history = fit(45, 0.01, model, train_loader, val_loader)","becfc74d":"history += fit(40, 0.001, model, train_loader, val_loader)","fc48116e":"history += fit(40, 0.0001, model, train_loader, val_loader)","055ebdcc":"# Lets define a function for plotting graphs\ndef plot_accuracies(history):\n    accuracies = [r['val_acc'] for r in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs')\n\ndef plot_losses(history):\n    losses = [x['val_loss'] for x in history]\n    plt.plot(losses, '-x', color='red')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.title('Loss vs No. of Epoch')","628c76a8":"plot_accuracies(history)","03011554":"plot_losses(history)","d593b1ba":"result = evaluate(model, test_loader)","678820f3":"result","75aafd9d":"num_epochs = [45, 40, 40]\nlr = [0.01, 0.001, 0.0001]","cc6104f1":"def predict_image(img, model):\n    xb = img.unsqueeze(0)\n    yb = model(xb)\n    _, preds  = torch.max(yb, dim=1)\n    return preds[0].item()","8ba1e0c6":"img, label = test_ds[1322]\nplt.imshow(img[0], cmap='gray')\nprint('Label:', test_ds.classes[label], ', Predicted:', dataset.classes[predict_image(img, model)])","e0f9f360":"img, label = test_ds[1392]\nplt.imshow(img[0], cmap='gray')\nprint('Label:', test_ds.classes[label], ', Predicted:', dataset.classes[predict_image(img, model)])","75f9f8f4":"!pip install jovian --upgrade -q","6602efb7":"import jovian\njovian.commit(project=project_name)","051bb9f0":"jovian.log_dataset(dataset_url='https:\/\/www.kaggle.com\/manishshah120\/facial-expression-recog-image-ver-of-fercdataset', val_size=val_size)","33667efb":"jovian.log_metrics(\n    val_loss = result['val_loss'],\n    val_acc = result['val_acc']\n                  )","18444314":"jovian.log_hyperparams({\n    'num_epochs': num_epochs,\n    'batch_size': batch_size,\n    'lr': lr,\n})\n","1efd143e":"jovian.commit(project=project_name, is_cli =True,environment=None)","5d4c77e8":"Done","e41f0346":"Comitting our work to Jovian with all the hyper params.","41f028ae":"## Creating the `dataset` variable","28c4fc04":"Now it's time to create our training and validation dataset","dcee77ae":"## Model\nIt's time to define our model","57f97091":"Lets convert all the images into tensor with the help of `ToTensor()`","0d9047ab":"Lets have a look to the tensors and the labels","d51dbf0f":"## PLotting Functions are necessary too..","87f4655c":"## Predictions\nNow comes the final part","05f02e03":"## Data Loaders","3d5e8c33":"Owooooooooo.....The dataset I created is working just perfectly.","46c4b20c":"Lets see the number of images belonging to each classes in the training set","95c71a0a":"so our training dataset contains `32298` of toal images","b872c963":"- **User**: [@manishshah120](https:\/\/www.kaggle.com\/manishshah120)\n- **LinkedIn**: https:\/\/www.linkedin.com\/in\/manishshah120\/\n- **GitHub**: https:\/\/github.com\/ManishShah120\n- **Twitter**: https:\/\/twitter.com\/ManishShah120\n\n> This Notebook was created while working on project for a course \"**Deep Learning with PyTorch: Zero to GANs**\" from \"*jovian.ml*\" in collaboratoin with \"*freecodecamp.org*\"","546ae3f0":"Our image is of 48*48 pixels and has 3 channels RGB, but though our image is B\/W i don't know why its showing channel as 3, instead it should have shown 1.","7f656d6e":"# Facial Expression Recogniton with Logistic Regression","7e6de3f4":"## Understanding the file structure","079b9576":"No. of training images of each class:-","f508accb":"# ---------The End----------","cf2ee5d7":"Lets start the training process","6ce7222f":"Let's just see random 10 predictions","65b3b050":"Now, lets have a look at batch of images","c25effec":"Lets Specify a preoject name to be able to commit file on jovian","7bf7c2bc":"Lets have a look at some images.","2fccdaab":"Yeah, Thats how an angry young man look like.","26a45910":"Let's log all our hyperparams","f42c93ea":"## Training"}}