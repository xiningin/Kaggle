{"cell_type":{"2b774d42":"code","95c6be34":"code","ec1ffab5":"code","3a3b85df":"code","589af123":"code","a21f0628":"code","5de9d0e9":"code","b95df187":"code","5b9041ed":"code","bc3836d9":"code","7c7ed89b":"code","9bec93eb":"code","bd3a4800":"code","fbfb60fb":"code","93a13b71":"code","fa85790a":"code","b67b67a2":"code","3f8d6f6d":"code","82f9cf20":"code","6196e273":"code","be785a6f":"code","34f06a7a":"code","ff36d3c5":"code","5889fea3":"code","59db00ec":"code","8f5566de":"code","a054ba6c":"code","b674e590":"code","c3d289b4":"code","89589362":"code","5abc16d2":"code","5401c545":"code","adeef32a":"code","3ddb2669":"code","3aada015":"markdown","826e75f6":"markdown","eac4683b":"markdown","39d80a26":"markdown","2ba78ad1":"markdown","18a73f82":"markdown","19191ef7":"markdown","dfca3c29":"markdown","9b6db9b1":"markdown","e9fd5ad9":"markdown","67d8241f":"markdown","14a37d6c":"markdown","0c7d5b2d":"markdown","b1fa155b":"markdown","5d85e1e1":"markdown","e2f4a544":"markdown","9747557c":"markdown"},"source":{"2b774d42":"import sys\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\n\n#from scipy.stats import entropy\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\nfrom scipy.stats import skew\nfrom sklearn.model_selection import  KFold , GridSearchCV, train_test_split\nfrom sklearn.ensemble import  RandomForestClassifier\nimport json\nfrom sklearn.feature_selection import SelectKBest, chi2, f_classif\nfrom sklearn.metrics import confusion_matrix, log_loss, make_scorer, accuracy_score, f1_score\n\nfrom sklearn.preprocessing import scale\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.feature_selection import VarianceThreshold\n\nfrom sklearn.preprocessing import LabelEncoder","95c6be34":"def missing_perc(df):\n     \"\"\"\n     Return a dataframe with percentage of missing values\n     in each column in a sorted order\n     Args:\n     df: dataframe\n     Returns:\n     dataframe with percentage of missing values in each column\n     \"\"\"\n\n     missing = df.isnull().sum()\n     missing = missing[missing > 0] * 100 \/ df.shape[0]\n     missing.sort_values(inplace=True)\n     return pd.DataFrame(missing, columns=['missing_perc'])\n\n\ndef unique_val(df, outputcol='n_unique_vals'):\n     \"\"\"\n     Count the number of distinct values in each column in a sorted order\n     Args:\n     df: dataframe\n     col: string, column name of the output dataframe\n     Returns:\n     dataframe with the number of distinct values in each column\n     \"\"\"\n\n     columns = df.columns\n     undict = {}\n     for col in columns:\n         undict[col] = df[col].astype(str).nunique()\n     undf = pd.DataFrame.from_dict(undict,\n         'index',\n         columns=[outputcol])\n     undf.sort_values(by=[outputcol], inplace=True)\n     return undf\n\n\ndef getCategoricalVariablesRanking(df, target, limit=50):\n    \"\"\"\n    Return sorted chi square statistics for categorical features\n    Source: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.\n    feature_selection.chi2.html#sklearn.feature_selection.chi2\n    Args:\n         df: pandas dataframe\n         target: string corresponding to the categorical target column\n    Returns:\n         list object with sorted chi square statistics\n    \"\"\"\n    categorical_variables = [i for i in list(df.dtypes[df.dtypes == 'object'].index) if i != target]\n    chi2_selector = SelectKBest(chi2, k='all')\n    df_chi_final = pd.DataFrame(columns=[\"scaled_importance\", \"value\", \"column\"])\n    for col in categorical_variables:\n        dummy = pd.get_dummies(df[col])\n        chi2_selector.fit_transform(dummy, df[target])\n        df_chi = pd.DataFrame(chi2_selector.scores_,\n                              columns=['scaled_importance'])\n        df_chi[\"value\"] = dummy.columns\n        df_chi[\"column\"] = col\n        df_chi_final = pd.concat([df_chi_final, df_chi], axis=0)\n\n    df_chi_final[\"scaled_importance\"] -= df_chi_final[\"scaled_importance\"].min()\n    df_chi_final[\"scaled_importance\"] \/= df_chi_final[\"scaled_importance\"].max()\n    df_chi_final = df_chi_final.sort_values(by='scaled_importance', ascending=False).head(limit)\n\n    return df_chi_final\n\ndef getContinuousVariablesRanking(df, target):\n    \"\"\"\n    Return sorted F value statistics for continuous features\n    Source: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.\n    feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest\n    Args:\n         df: pandas dataframe\n         target: string corresponding to the categorical target column\n    Returns:\n         dataframe with sorted F value statistics\n    \"\"\"\n    cont_vars = [i for i in list(df.dtypes[df.dtypes != 'object'].index) if i != target]\n\n    Fvalue_selector = SelectKBest(f_classif, k=len(cont_vars))\n    Fvalue_selector.fit_transform(df[cont_vars].fillna(-1), df[target])\n    df_Fvalue = pd.DataFrame(Fvalue_selector.scores_,\n                             columns=['scaled_importance'])\n    # scaling the statistics\n    df_Fvalue -= df_Fvalue.min()\n    df_Fvalue \/= df_Fvalue.max()\n    df_Fvalue['columns'] = cont_vars\n    df_Fvalue.sort_values(by='scaled_importance', ascending=False, inplace=True)\n    \n    return df_Fvalue","ec1ffab5":"pwd","3a3b85df":"\nsub = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')\ntrain_features = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\ntest_features = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\ntrain_targets_nonscored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_nonscored.csv')\ntrain_targets_scored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')","589af123":"\n\ntrain_features.head()","a21f0628":"train_features.shape","5de9d0e9":"train_targets_scored.shape","b95df187":"plt.hist(train_targets_scored.mean())","5b9041ed":"test_features.head()","bc3836d9":"train_targets_scored.describe()","7c7ed89b":"sub.head()","9bec93eb":"#missing values\nmissing_perc(train_features)","bd3a4800":"#missing values\nmissing_perc(test_features)","fbfb60fb":"#missing values\nmissing_perc(train_targets_scored)","93a13b71":"unique_val(train_features)","fa85790a":"unique_val(test_features)","b67b67a2":"catList = ['cp_type', 'cp_dose']\n\ncountList =  list (set(train_features.columns) - set(catList))\n\ncountList.remove('sig_id')","3f8d6f6d":"print(train_features.shape,  train_features.drop_duplicates().shape)","82f9cf20":"fig, axes = plt.subplots(2, 1, figsize=(6, 4))\n\nfor i, ax in enumerate(fig.axes):\n    if i < len(catList):\n        ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=90)\n        sns.countplot(x=catList[i], alpha=0.7, data=train_features, ax=ax)\n\nfig.tight_layout()\n\n","6196e273":"fig, axes = plt.subplots(1, 1, figsize=(6, 4))\n\nfor i, ax in enumerate(fig.axes):\n\n    ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=90)\n\n    sns.countplot(x='cp_time', alpha=0.7, data=train_features, ax=ax)","be785a6f":"fig, axes = plt.subplots(2, 1, figsize=(6, 4))\n\nfor i, ax in enumerate(fig.axes):\n    if i < len(catList):\n        ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=90)\n        sns.countplot(x=catList[i], alpha=0.7, data=test_features, ax=ax)\n\nfig.tight_layout()\n\n","34f06a7a":"fig, axes = plt.subplots(1, 1, figsize=(6, 4))\n\nfor i, ax in enumerate(fig.axes):\n\n    ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=90)\n\n    sns.countplot(x='cp_time', alpha=0.7, data=test_features, ax=ax)","ff36d3c5":"train_targets_scored.mean()[train_targets_scored.mean() == train_targets_scored.mean().max()]","5889fea3":"\ntrain_features_toptarget_count = pd.concat([train_features[countList], train_targets_scored['nfkb_inhibitor'].astype('str')], axis = 1)\n\ndf_Fvalue_s = getContinuousVariablesRanking(train_features_toptarget_count, 'nfkb_inhibitor')\n\ndf_Fvalue_s","59db00ec":"n = 10\nplt.figure(figsize=(10,5))\nplt.title(\"F-value scaled importance for continuous features (top 10, target = nfkb_inhibitor)\",fontsize=15)\nplt.xlabel(\"Continuous Features\",fontsize=15)\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.bar(range(10),df_Fvalue_s.head(10)['scaled_importance'],align='edge',color='rgbkymc')\nplt.xticks(range(10),df_Fvalue_s.head(10)['columns'],rotation=90,color='g')\nplt.show()\nplt.close()","8f5566de":"n = 10\nplt.figure(figsize=(10,5))\nplt.title(\"F-value scaled importance for continuous features (bottom 10, target = nfkb_inhibitor)\",fontsize=15)\nplt.xlabel(\"Continuous Features\",fontsize=15)\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.bar(range(10),df_Fvalue_s.tail(10)['scaled_importance'],align='edge',color='rgbkymc')\nplt.xticks(range(10),df_Fvalue_s.tail(10)['columns'],rotation=90,color='g')\nplt.show()\nplt.close()","a054ba6c":"colist = df_Fvalue_s.head(10)['columns']\nlabel = 'nfkb_inhibitor'\n\nfor col in colist: \n    \n    g = sns.FacetGrid(train_features_toptarget_count[[col, label]],  hue =label, height = 4, aspect = 1.5) \n    g.map(sns.distplot, col, hist = False, kde_kws = {'shade': True, 'linewidth': 3}).set_axis_labels(col,\"density\").add_legend()\n\n    ","b674e590":"\ntrain_features_toptarget_cat = pd.concat([train_features[catList], train_targets_scored['nfkb_inhibitor'].astype('str')], axis = 1)\n\n\ngetCategoricalVariablesRanking(train_features_toptarget_cat, 'nfkb_inhibitor')","c3d289b4":"train_features_toptarget_cat[train_features_toptarget_cat.cp_type == 'ctl_vehicle'][label].value_counts()","89589362":"test_features[test_features.cp_type == 'ctl_vehicle'].shape","5abc16d2":"lb=LabelEncoder()\n\nfor f in catList: \n\n    train_features[f]=lb.fit_transform(train_features[f])\n    test_features[f]=lb.transform(test_features[f])","5401c545":"colList = list(train_targets_scored.columns[1:])\ntrain_features2 = train_features[countList+ catList]\n#mask = test_features.cp_type == test_features.cp_type.value_counts().index[-1]\n\nfor label in colList: \n    \n    y_train = train_targets_scored[label]\n    rf = RandomForestClassifier(class_weight='balanced', max_depth=15,\n                            n_estimators=500, #500\n                            n_jobs=-1, \n                            random_state=1234)\n    rf.fit(train_features2, y_train)\n    test_features[label] =  rf.predict_proba(test_features[countList + catList])[:,1] \n    #test_features.loc[mask][label] = 0\n    #print('label:', label)","adeef32a":"test_features[\n['sig_id'] + colList].head()\n\n","3ddb2669":"test_features[\n['sig_id'] + colList ].to_csv('submission.csv', index=False)","3aada015":"## Saving predictions.","826e75f6":"\n\n## EDA","eac4683b":"#### Feature importance analysis based on the most represented target value: wnt_inhibitor ","39d80a26":"#### Checking for duplicated rows in the training set","2ba78ad1":"**The goal of this notebook is to carry out and EDA on train and test data and build a simple random Forest model.**","18a73f82":"## Reading input data","19191ef7":"Even though not depicted here the feature importance is found to vary with the target as one would expect.","dfca3c29":"#### Analysing missing values","9b6db9b1":"We have a high imbalance: the max target rate is 0.03, the min is very low. We would need to be careful.","e9fd5ad9":"Based on the description columns cp_type and cp_dose can be set as categorical.","67d8241f":"There is no duplicate in the training set.","14a37d6c":"**Counting unique values**","0c7d5b2d":"####  Data analysis functions","b1fa155b":"#### Categorical variables distribution","5d85e1e1":"There are no missing values","e2f4a544":"As stated in the description control perturbation (ctrl_vehicle) has no MOA. Predictions will be set to zero for these samples.","9747557c":"Label encoding the categorical variables"}}