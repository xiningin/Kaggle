{"cell_type":{"a0f7d750":"code","b232778c":"code","03acbcd3":"code","a7b820b4":"code","1881b08a":"code","4c486468":"code","0852db40":"code","ce32063b":"code","2f439ba5":"code","07420ae6":"code","2a112900":"code","2f53f372":"code","1e064bf1":"code","b5501b28":"code","bec09cba":"code","e7c8de54":"code","9c47b0ad":"code","087eaea3":"code","209fd285":"code","4ac2c92d":"code","e2b632cc":"code","4b3aa0c3":"code","a4354d5e":"code","8b714f2b":"code","cf7fb9cc":"markdown","f6ce6562":"markdown","336ea9d4":"markdown","1bf46061":"markdown","1875812f":"markdown","618675e3":"markdown","bfa1f89a":"markdown","9560f568":"markdown","fb46aa35":"markdown","017bf03f":"markdown","31076607":"markdown","a38b88cc":"markdown","404ad99a":"markdown","405226c1":"markdown","4d0e3cab":"markdown","8c1cd395":"markdown"},"source":{"a0f7d750":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout, BatchNormalization\nfrom tensorflow.keras.backend import clear_session\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nfrom sklearn.metrics import classification_report, confusion_matrix","b232778c":"train = pd.read_csv(\"..\/input\/sign-language-mnist\/sign_mnist_train\/sign_mnist_train.csv\")\ntest = pd.read_csv(\"..\/input\/sign-language-mnist\/sign_mnist_test\/sign_mnist_test.csv\")","03acbcd3":"print(f\"Train data have {train.shape[0]} records\")\nprint(f\"Test data have {test.shape[0]} records\")","a7b820b4":"train.head()","1881b08a":"test.head()","4c486468":"train.isna().any().sum(), test.isna().any().sum()","0852db40":"df_digit_counts =  train.label.value_counts().reset_index()\n\nplt.figure(figsize=(20,8))\nax = sns.barplot(x='index', y='label', data=df_digit_counts)\n\nfor i in ax.patches:\n    v1 = round((i.get_height()\/len(train))*100, 2)\n    ax.annotate(f'{v1}%', (i.get_x()+0.4, i.get_height()), ha='center', va='bottom',color= 'black')\n\nplt.title(\"Digit Count\")\nplt.ylabel(\"Counts\")\nplt.xlabel(\"Digits\")\nplt.show()","ce32063b":"IMG_W = 28\nIMG_H = 28\nIMG_C = 1\n\nEPOCHS = 20\nBATCH_SIZE=16\n\nCLASSES = len(train['label'].unique())\nCLASSES","2f439ba5":"rows = len(train['label'].unique())\ncols = 10\nfig, axs = plt.subplots(rows, cols, figsize=(36, 36))\nfor i,lbl in enumerate(train['label'].unique()):\n    imgs = np.array(train[train['label'] == lbl].iloc[0:cols,1:])\n    for j,img in enumerate(imgs):\n        img = img.reshape(IMG_W,IMG_H)\n        axs[i,j].matshow(img)\n        axs[i,j].axis('off')\n        axs[i,j].set_title(f'label - {str(lbl).upper()}', fontsize=24)\nfig.tight_layout()","07420ae6":"train_X, train_y = train.drop(columns=['label']), train[\"label\"]\ntest_X, test_y = test.drop(columns=['label']), test[\"label\"]","2a112900":"train_X = np.array(train_X)\ntrain_y = np.array(train_y)\n\ntest_X = np.array(test_X)\ntest_y = np.array(test_y)","2f53f372":"train_X = train_X \/ 255\ntest_X = test_X \/ 255","1e064bf1":"train_X = train_X.reshape(-1,28,28,1)\ntest_X = test_X.reshape(-1,28,28,1)","b5501b28":"label_binarizer = LabelBinarizer()\ntrain_y = label_binarizer.fit_transform(train_y)\ntest_y = label_binarizer.fit_transform(test_y)","bec09cba":"train_y","e7c8de54":"X_train, X_val, y_train, y_val = train_test_split(train_X, train_y, test_size=0.2, random_state=42, shuffle=True)","9c47b0ad":"def create_model():\n    clear_session()\n    model = Sequential()\n\n    model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu', input_shape = (IMG_W,IMG_H,IMG_C)))\n    model.add(BatchNormalization())\n    model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPool2D(pool_size=(2,2)))\n    model.add(Dropout(0.25))\n\n    model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n    model.add(Dropout(0.25))\n\n    model.add(Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n    model.add(Dropout(0.25))\n\n    model.add(Conv2D(filters = 256, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(filters = 256, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(filters = 256, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n    model.add(Dropout(0.25))\n\n    model.add(Flatten())\n    model.add(Dense(512, activation = \"relu\"))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.3))\n\n    model.add(Dense(CLASSES, activation = \"softmax\"))\n    \n    return model","087eaea3":"model = create_model()\nmodel.summary()","209fd285":"model.compile(optimizer='adamax', \n              loss = 'categorical_crossentropy', \n              metrics=['accuracy'])\n\nlr_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)\nes = EarlyStopping(monitor='val_loss',\n                              min_delta=0,\n                              patience=5,\n                              verbose=0, mode='auto')","4ac2c92d":"datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.12,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.12,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\ndatagen.fit(X_train)","e2b632cc":"history = model.fit_generator(datagen.flow(X_train,y_train, batch_size=BATCH_SIZE),\n                              epochs = EPOCHS, \n                              validation_data = (X_val,y_val), \n                              steps_per_epoch=X_train.shape[0] \/\/ BATCH_SIZE,\n                              callbacks=[lr_reduction, es], \n                              shuffle=True)","4b3aa0c3":"# Plot the loss and accuracy curves for training and validation \nfig, ax = plt.subplots(2,1)\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\nax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","a4354d5e":"ypred = model.predict(test_X)\nypred = np.argmax(ypred, axis=1)\nytest = np.argmax(test_y, axis=1)\n\ncf_matrix = confusion_matrix(ytest, ypred)\n\nplt.figure(figsize=(20,8))\nax = sns.heatmap(cf_matrix, annot=True, fmt='g')\nplt.show()\n\nprint(\"\\n\\n\")\nprint(classification_report(ytest, ypred))","8b714f2b":"all_classes = [\"Class \" + str(i) for i in range(25) if i != 9]\nprint(classification_report(ytest, ypred, target_names = all_classes))","cf7fb9cc":"### Let see some images","f6ce6562":"### Convert to np.array","336ea9d4":"### Load Data","1bf46061":"### Visualize performance","1875812f":"### Divide features by 255 to Normalize","618675e3":"### Dsiplay data","bfa1f89a":"### Configs","9560f568":"### Create features, labels","fb46aa35":"### Train model","017bf03f":"### Check for train, test dataset for na","31076607":"### Reshape features ","a38b88cc":"### Create train, validation data","404ad99a":"### Check performance on test data","405226c1":"### Transform labels using LabelBinarizer","4d0e3cab":"### Generate Image using ImageDataGenerator","8c1cd395":"### Check for train data for labels"}}