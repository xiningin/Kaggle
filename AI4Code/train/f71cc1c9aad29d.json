{"cell_type":{"89a3a98b":"code","73a10208":"code","30c4ff88":"code","d6210355":"code","7d4d3f81":"code","9d5c5a14":"code","7c6092ad":"code","5133e531":"code","b153e378":"code","35d75863":"code","9a1bc29d":"code","64a2d7cc":"code","74de5c2f":"code","2141b182":"code","f392aee6":"code","f3b9198f":"code","488f8f35":"code","94bc4d31":"code","6c3b52ff":"code","7427c497":"code","49eab889":"code","d5bcb877":"code","917a3e2a":"code","cb2bb527":"code","5d049e6d":"code","afff7582":"code","0d700d9d":"code","1fbcf357":"code","e2d584ad":"code","fdb5ac39":"code","da7b364b":"code","78b200e2":"code","0e561211":"code","72f77786":"code","55afd6dc":"code","bbf8f198":"code","fd333e41":"code","878c2062":"code","18ad3d53":"code","42081805":"code","76a304be":"code","86de663a":"code","0d9ca50f":"code","5c4553f7":"code","73751ebf":"code","a0dc9684":"code","ac80089d":"code","4030f58e":"code","609de7be":"code","0a2a51ee":"code","93e0c0c0":"code","4f7a535b":"code","f5fa8f66":"code","5e8f1edf":"code","3e834f50":"code","9eb8a630":"code","42ad8642":"code","ac121341":"code","c936d5b5":"code","6c8e52c9":"code","3cbbcc55":"code","89a1e3c8":"code","1d2785fb":"code","fa2e6a2c":"code","95491c4a":"code","7496173e":"code","ee3a07be":"code","8f22f4cf":"code","bd8eee50":"code","3e01e69d":"code","596cf161":"code","6d6ff72d":"code","2f39413b":"code","335e4c33":"code","198a9f5b":"code","2e5768d0":"code","bc7ae945":"code","5e3d53e3":"code","ef6de635":"code","5590542f":"code","2b73b7c7":"code","6d902d1d":"code","161fa9a5":"code","4a836972":"code","ca13500c":"code","4d76b395":"code","f78f5cd3":"code","5ce59a76":"code","a8a5dda5":"code","8b108cd6":"code","a477bf0b":"code","29592641":"code","aa6bd5e4":"code","6585bcd4":"code","c2c6370b":"code","52d4a8a7":"code","5d6ff0c3":"code","999d88f7":"code","671e46bb":"code","1f384c1f":"code","b509e010":"code","65898eae":"code","60c59548":"code","d28a74b7":"code","2d59a70d":"code","73821908":"code","a740c991":"code","8bbf8897":"code","b6069f8a":"code","853c4a99":"code","c6fea4cc":"code","7e61cf16":"code","fda56aee":"code","a33b343d":"code","837c5fbf":"code","a7ea0219":"code","db6c53db":"code","a73161f3":"code","31798ae9":"code","b29bdd44":"code","bb1ec5b4":"code","ae5daba9":"code","0c9df300":"code","d41b3619":"code","e4eee5e6":"code","be483432":"code","9164ab12":"code","9c038c51":"code","11164bf6":"code","030c7186":"code","55bf82bc":"code","8d373fba":"code","30c7bb86":"code","f775c61f":"code","f7d740e8":"code","d096d510":"code","363c0452":"code","8b75c689":"code","564cabc7":"code","55af924e":"code","4fe7252c":"code","4b1a4c6f":"code","db67eda2":"code","a7978bb9":"code","2b2b45b9":"code","f54e1989":"code","d5a22fcf":"code","a68a1333":"code","e3870c17":"code","f3295cc6":"code","d0db3c81":"code","422dca4a":"code","b4560004":"code","c66471f7":"code","ab47a7f6":"code","b0747a9f":"code","e298ddc9":"code","6b52ac38":"code","7f348cde":"code","18789ecd":"code","7b19cfd0":"code","c9cd64ac":"code","5394630a":"code","97207b57":"code","2272a205":"code","8939a0e4":"code","dbd754c1":"code","52feadee":"code","e242f4a3":"code","87e9514d":"code","fe9aa227":"code","25a6b693":"code","e3b5903d":"code","c0041a6d":"code","c7407cb8":"code","16892311":"code","3fe80d9e":"code","21048927":"code","b4410278":"code","9f16b12e":"code","b75c2a31":"code","3498c1d2":"code","ab451edc":"code","1a3af021":"code","92e04706":"code","96a3355f":"code","f7c8559d":"markdown","38842988":"markdown","2d0263bf":"markdown","79262f08":"markdown","3867bea2":"markdown","5cc2fdfd":"markdown","fbff0911":"markdown","28015134":"markdown","3d4ef6df":"markdown","cbc2067d":"markdown","8fa626f5":"markdown","7ad2ffdf":"markdown","0a1a9eba":"markdown","5b15fc5e":"markdown","86f4cc55":"markdown","365e0ee2":"markdown","29b80074":"markdown","a907c77f":"markdown","0acc353a":"markdown","e6539711":"markdown","50dbfb33":"markdown","eb1fff86":"markdown","a0aefeb6":"markdown","c5ae45cc":"markdown","f8bc828d":"markdown","173dbc42":"markdown","73b15915":"markdown","b02df68d":"markdown","8044bc80":"markdown","0667b5d0":"markdown","adb07f23":"markdown","3ab44d1c":"markdown","4f22f3ca":"markdown","38016ce4":"markdown","35b221df":"markdown","e2332557":"markdown","9b784948":"markdown","438a23df":"markdown","63cbf8eb":"markdown","1ddedf5d":"markdown","6d179495":"markdown","938b1f44":"markdown","d44a3d01":"markdown","f3bd1444":"markdown","9b29430b":"markdown","89c874d6":"markdown","04b82f5d":"markdown","ef92a35a":"markdown","c42604d3":"markdown","33f5cd9d":"markdown","086705d8":"markdown","d64af079":"markdown","125bb58e":"markdown","7c24bb3e":"markdown","d5090441":"markdown","71ad760c":"markdown","79505d23":"markdown","5bcd98d9":"markdown","7042b0ad":"markdown","bbdd5e5f":"markdown","9a5a8d43":"markdown","58c7f067":"markdown","1baf2a84":"markdown","649eb82a":"markdown","eb68d7eb":"markdown","1ed9368a":"markdown","4c591235":"markdown","00c66d5c":"markdown","d298eba2":"markdown","66d9eb9b":"markdown","109b6e81":"markdown","810a9201":"markdown","f8647f44":"markdown","b54e110a":"markdown","49054a16":"markdown","86a6e4b6":"markdown","0c2d10d6":"markdown","e9ab57dd":"markdown","22795ac9":"markdown","7553900b":"markdown","382be84b":"markdown","19176a12":"markdown","6bc28b0b":"markdown","0f500401":"markdown","5867956e":"markdown","91bf2320":"markdown","5e290302":"markdown","710cd3ea":"markdown","603ee404":"markdown","527a55be":"markdown","e12f83bf":"markdown","010664ec":"markdown","4d94dfae":"markdown","ee8c983a":"markdown","a326138a":"markdown","c8bca066":"markdown","eb7ad7e7":"markdown","9b2d094e":"markdown","c5aa9ef6":"markdown","9994fd7b":"markdown","1255a900":"markdown","b2e770e6":"markdown","883f2b5d":"markdown","20b75b19":"markdown","aa27fa1c":"markdown","5ba66e29":"markdown","22c6aac0":"markdown","dc7b4270":"markdown","aa1a4fff":"markdown","4577e3cf":"markdown","175815fc":"markdown","5e097fba":"markdown","b4eac98b":"markdown","43a716d0":"markdown","b7e55e20":"markdown","70a88a81":"markdown","c4a563d0":"markdown","d0ada840":"markdown","f4c94966":"markdown","10dfd5c5":"markdown","1283a63a":"markdown","605cbd68":"markdown","be614459":"markdown","cd322acd":"markdown","04ac1603":"markdown","7f7836dc":"markdown","156c68b9":"markdown","f2eec205":"markdown","b91b96b9":"markdown","5b1a5882":"markdown","7e960629":"markdown","b7feabe8":"markdown","3902ffb0":"markdown","9712c4c0":"markdown","83c3bb72":"markdown","b0562824":"markdown","e663353f":"markdown","4deb7c39":"markdown","48617743":"markdown","94dfab1c":"markdown","b4bbe117":"markdown","23b1c106":"markdown"},"source":{"89a3a98b":"# Importing Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\nimport math\nimport matplotlib\n\n#Checking  Versions of libraries used\n\n#print(\"Pandas version: Pandas {}\".format(pd.__version__)) # Same way of printing as bellow\n\nprint(f\"Pandas version: Pandas {pd.__version__}\")\nprint(f\"Numpy version: Pandas {np.__version__}\")\nprint(f\"Matplotlib version: Pandas {matplotlib.__version__}\")\nprint(f\"Seaborn version: Pandas {sns.__version__}\")\n\n#Magicfunctions for In-Notebook Display\n%matplotlib inline\n\n# Setting seabon style\nsns.set(style='darkgrid', palette='deep')","73a10208":"df=pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')","30c4ff88":"# Displaying all the columns\npd.options.display.max_rows=100\npd.options.display.max_columns=100","d6210355":"# Displaying all the rows\npd.set_option('display.max_rows',1000)","7d4d3f81":"df.head(2)","9d5c5a14":"df.tail(2)","7c6092ad":"# Number of rows and\nm,n=df.shape\nprint(f'Total rows :  {m}')\nprint(f'Total Columns :  {n}')","5133e531":"# Name of the Columns\ndf.columns","b153e378":"# Levels of Classification\ndf.Class.value_counts()","35d75863":"df.info()","9a1bc29d":"df.isnull().sum()\n#df.isnull().sum().max()\n#df.isnull().any()\n#df.isnull().any().sum()","64a2d7cc":"df[['Time','Amount','Class']].describe().T","74de5c2f":"print(f'Count of Fraud and Non Fraud Transaction: \\n{df.Class.value_counts()}')\nprint('\\n')\nprint(f'Percentage of Fraud and Non Fraud Transaction: \\n{df.Class.value_counts(normalize=True)}')","2141b182":"plt.figure(figsize=(4,4), dpi=100)\ndf[\"Class\"].value_counts().plot(kind = 'pie', autopct='%1.1f%%', fontsize = 20)\nplt.title(\"Fraudulent and Non-Fraudulent Distribution\",color='b', fontsize = 15,fontweight='bold')\nplt.legend([\"Non-Fraud\", \"Fraud\"])\nplt.show()","f392aee6":"plt.figure(figsize=(6,3),dpi=100)\nsns.distplot(df['Amount'], bins=1)\nplt.title('Distribution of Transaction Amount', fontweight='bold')\nplt.show()","f3b9198f":"plt.figure(figsize=(6,3), dpi=100)\nsns.boxplot(y='Amount', data=df)\nplt.title('Boxplot of Transaction Amount', fontweight='bold')\nplt.show()","488f8f35":"plt.figure(figsize=(6,3),dpi=100)\nsns.boxplot(x='Class',y='Amount', data=df)\nplt.title('Transaction Amount for Non-Fraud and Fraud Transactions', fontweight='bold')\nplt.show()","94bc4d31":"plt.figure(figsize=(6,3),dpi=100)\nsns.distplot(df[df['Class']==1].Amount, bins=1)\nplt.title(\"Distribution of Fraudulent Transaction Amount\", fontweight='bold')\nplt.show()","6c3b52ff":"plt.figure(figsize=(6,3),dpi=100)\nsns.distplot(df[df['Class']==0].Amount,bins=1)\nplt.title(\"Distribution of Non-Fraudulent Transaction Amount\", fontweight='bold')\n\nplt.show()","7427c497":"df[(df['Class']==1)&(df['Amount']<=10)].Amount.value_counts().head(10)","49eab889":"l1=len(df[(df['Class']==1)&(df['Amount']<=10)].Amount)\nprint(f'Total count of Fraudulent Transaction where Transaction Amount up to 10: {l1}')\nl2=len(df[(df['Class']==1)].Amount)\nprint(f'Total count of Fraudulent Transaction: {l2}')\np=(l1\/l2)*100\nprint(f'Percentage of Fraudulent Transaction where Transaction Amount up to 10: {p}')","d5bcb877":"# Maximum transaction value for Fraudelent Transaction\nprint(f\"Maximum transaction value for Fraudulent Transaction: {df[df['Class']==1].Amount.max()}\")","917a3e2a":"fig , axs = plt.subplots(nrows = 2 , ncols = 3 , figsize = (15,10))\nfig.suptitle('Fraudulent and Non Fraudulent Distribution for Different Transacton Amount',fontsize = 20,color='b', fontweight='bold')\n\n\ndf[(df['Amount']<=1)].Class.value_counts().plot(kind = 'pie', autopct='%1.2f%%', fontsize = 20, ax=axs[0,0])\naxs[0,0].set_title(\"Transaction Amount up to 1\",color='b', fontsize = 15, fontweight='bold')\n\ndf[(df['Amount']>1)&(df['Amount']<500)].Class.value_counts().plot(kind = 'pie', autopct='%1.2f%%', fontsize = 20, ax=axs[0,1])\naxs[0,1].set_title(\"Transaction Amount >1 and <500\",color='b', fontsize = 15, fontweight='bold')\n\ndf[(df['Amount']>=500)&(df['Amount']<1000)].Class.value_counts().plot(kind = 'pie', autopct='%1.2f%%', fontsize = 20, ax=axs[0,2])\naxs[0,2].set_title(\"Transaction Amount >500 and <1000\",color='b', fontsize = 15, fontweight='bold')\n\ndf[(df['Amount']>=1000)&(df['Amount']<1500)].Class.value_counts().plot(kind = 'pie', autopct='%1.2f%%', fontsize = 20, ax=axs[1,0])\naxs[1,0].set_title(\"Transaction Amount >1000 and <1500\",color='b', fontsize = 15, fontweight='bold')\n\ndf[(df['Amount']>=1500)&(df['Amount']<2000)].Class.value_counts().plot(kind = 'pie', autopct='%1.2f%%', fontsize = 20, ax=axs[1,1])\naxs[1,1].set_title(\"Transaction Amount >1500 and <2000\",color='b', fontsize = 15, fontweight='bold')\n\ndf[(df['Amount']>=2000)].Class.value_counts().plot(kind = 'pie', autopct='%1.2f%%', fontsize = 20, ax=axs[1,2])\naxs[1,2].set_title(\"Transaction Amount >2000\",color='b', fontsize = 15, fontweight='bold')\n\nplt.show()","cb2bb527":"# quartile values\nq1,q3=np.percentile(df['Amount'],[25,75])\n\n# Inter quartile range\niqr=q3-q1\n\n#upper and lower bound\nlower=q1-(iqr*1.5)\nupper=q3+(iqr*1.5)\n\nprint(f\"Q1: {q1}\")\nprint(f\"Q3: {q3}\")\nprint(f\"IQR: {iqr}\")\nprint(f\"Lower_Bound: {upper}\")\nprint(f\"Upper_Bound: {lower}\")\n\n# percentage of data removed\nl=len(df)-len(df[df['Amount']<upper])\nr=(len(df)-(len(df[df['Amount']<upper])))\/len(df)*100\nprint(f\"Percentage of data removed: {r}\")\nprint(f\"Number of rows deleted: {l}\")","5d049e6d":"# Fraud and non Fraud distribution in the data\ndf['Class'].value_counts(normalize=True)","afff7582":"# Fraud and non Fraud distribution after outliar remuval\ndf[df['Amount']<upper].Class.value_counts(normalize=True)","0d700d9d":"# Fraud and non Fraud distribution in the removed data\ndf[df['Amount']>=upper].Class.value_counts(normalize=True)","1fbcf357":"x1=len(df[(df['Amount']>=upper)&(df['Class']==1)])\nprint(f'Number of Fraudulent Transaction Removed : {x1}')\nx2=len(df[(df['Class']==1)].Amount)\nprint(f'Total count of Fraudulent Transaction: {x2}')\np1=(x1\/x2)*100\nprint(f'Percentage Fraudulent Transaction Removed: {p1}')","e2d584ad":"# Maximum Amount Value of Class == 1 or Fraud Claumn\ndf[df['Class']==1 ]['Amount'].max()","fdb5ac39":"# Function - Maximum Transaction Amount as Threshold for outliar removal\ndef outliar_removal(max_val):\n    print('***  Removing Outlier using Maximum Transaction Amount as Threshold  ***')\n    print(\"Number of Outliar Value Removed: {}\".format(len(df[(df['Class']==0) & (df['Amount']>max_val)])))\n    print(\"Proportion of data lost: {}\".format((len(df)-len(df[(df['Class']==0) & (df['Amount']<max_val)]))\/len(df)*100))\n          \n          \n#Remove outliars\n    print('\\n')\n    print('Distribution of Fraudulent and Non-Fraudulent Class:')\n    temp_df=df[df['Amount']<max_val]\n    print(temp_df.Class.value_counts(normalize=True))","da7b364b":"# Applying Function to remove outliar \noutliar_removal(df[(df['Class']==1) & (df['Amount'])].Amount.max())","78b200e2":"# Removing Outliar\nd=df[df['Amount']<2125.87]\n#Reseting index after removing outliers\nd.reset_index(drop = True , inplace = True)","0e561211":"plt.figure(figsize=(6,3),dpi=100)\nsns.boxplot(x='Class',y='Amount', data=d)\nplt.title('Transaction Amount for Non-Fraud and Fraud Transactions', fontweight='bold')\nplt.show()","72f77786":"# Maximum duration of data recording\nd.Time.max()\/(60*60)","55afd6dc":"plt.figure(figsize=(10,6))\nplt.title(\"Distribution of Transactions in two days duration\", fontsize=15,fontweight='bold')\nsns.distplot(d.Time, bins=100, color='k')\nplt.show()","bbf8f198":"# converting Time into hours\nd['Time_Hours']=d['Time']\/(60*60)","fd333e41":"d.columns","878c2062":"d.head()","18ad3d53":"fig , axs = plt.subplots(nrows = 1 , ncols = 2 , figsize = (15,5))\nfig.suptitle('Fraudulent and Non Fraudulent Transaction Distribution',fontsize = 20,color='b', fontweight='bold')\n\n\nsns.distplot(d[d['Class']==0]['Time_Hours'].values , color = 'g' , ax = axs[0])\naxs[0].set_title(\"Non-Fraudulent Transaction\",color='g', fontsize = 15, fontweight='bold')\n\nsns.distplot(d[d['Class']==1]['Time_Hours'].values , color = 'r' , ax = axs[1])\naxs[1].set_title(\"Fraudulent Transaction\",color='r', fontsize = 15, fontweight='bold')\n\nplt.show()","42081805":"plt.figure(figsize=(10,5))\nplt.title('Fraudulent and Non Fraudulent Transaction Distribution in Two Days',fontsize = 20, color='k', fontweight='bold')\nsns.distplot(d[d.Class==0].Time_Hours.values,color='g')\nsns.distplot(d[d.Class==1].Time_Hours.values, color='r')\nplt.show()","76a304be":"# Ploting data in 0-24 hrs time frame\nplt.figure(figsize=(10,5))\nplt.title('Fraudulent and Non Fraudulent Transaction Distribution in first Day',fontsize = 20, color='k', fontweight='bold')\nsns.distplot(d[d.Class==0].Time_Hours.values,color='g', bins=100)\nsns.distplot(d[d.Class==1].Time_Hours.values, color='r',bins=100)\nplt.xlim([0,24])\nplt.show()","86de663a":"# Ploting data in 25-48 hrs time frame\nplt.figure(figsize=(10,5))\nplt.title('Fraudulent and Non Fraudeulent Transaction Distribution in second Day',fontsize = 20, color='k', fontweight='bold')\nsns.distplot(d[d.Class==0].Time_Hours.values,color='g', bins=100)\nsns.distplot(d[d.Class==1].Time_Hours.values, color='r',bins=100)\nplt.xlim([25,48])\nplt.show()","0d9ca50f":"plt.figure(figsize=(10,5))\nsns.distplot(d[d.Class==0].Amount,bins=100,color='g')\nsns.distplot(d[d.Class==1].Amount,bins=100,color='r')\nplt.show()","5c4553f7":" # Importing library\nfrom sklearn.preprocessing import StandardScaler\n\nss = StandardScaler()","73751ebf":"d['Std_Amount']=ss.fit_transform(d['Amount'].values.reshape(-1,1))","a0dc9684":"d.columns","ac80089d":"plt.figure(figsize=(10,5))\nsns.distplot(d[d.Class==0].Std_Amount,bins=100,color='g')\nsns.distplot(d[d.Class==1].Std_Amount,bins=100,color='r')\nplt.show()","4030f58e":"from sklearn.preprocessing import MinMaxScaler\n\nn = MinMaxScaler()\nd['Mm_Amount'] = n.fit_transform(d['Amount'].values.reshape(-1,1))","609de7be":"d.columns","0a2a51ee":"plt.figure(figsize=(10,5))\nsns.distplot(d[d.Class==0].Mm_Amount,bins=100,color='g')\nsns.distplot(d[d.Class==1].Mm_Amount,bins=100,color='r')\nplt.show()","93e0c0c0":"#Log Transformation\nd['Log_Amount'] = np.log(d.Amount + 0.01)","4f7a535b":"d.columns","f5fa8f66":"plt.figure(figsize=(10,5))\nsns.distplot(d[d.Class==0].Log_Amount,bins=100,color='g')\nsns.distplot(d[d.Class==1].Log_Amount,bins=100,color='r')\nplt.show()","5e8f1edf":"plt.figure(figsize=(15,12))\n\n# Let's explore the Amount by Class and see the distribuition of Amount transactions\nplt.subplot(221)\nax = sns.boxplot(x =\"Class\",y=\"Amount\",data=d)\nax.set_title(\"Distribution Before Log Transform\", fontsize=16, fontweight='bold')\nax.set_xlabel(\"Non-Fraud vs Fraud\", fontsize=12, fontweight='bold')\nax.set_ylabel(\"Amount\", fontsize = 12, fontweight='bold')\n\nplt.subplot(222)\nax1 = sns.boxplot(x =\"Class\",y=\"Std_Amount\", data=d)\nax1.set_title(\"Distribution After Standard Scaler\", fontsize=16, fontweight='bold')\nax1.set_xlabel(\"Non-Fraud vs Fraud\", fontsize=12, fontweight='bold')\nax1.set_ylabel(\"Amount(Standard Scaler)\", fontsize = 12, fontweight='bold')\n\nplt.subplot(223)\nax2 = sns.boxplot(x =\"Class\",y=\"Mm_Amount\", data=d)\nax2.set_title(\"Distribution After Min-Max Scaler\", fontsize=16, fontweight='bold')\nax2.set_xlabel(\"Non-Fraud vs Fraud\", fontsize=12, fontweight='bold')\nax2.set_ylabel(\"Amount(Min-Max)\", fontsize = 12, fontweight='bold')\n\nplt.subplot(224)\nax3 = sns.boxplot(x =\"Class\",y=\"Log_Amount\", data=d)\nax3.set_title(\"Distribution After Log Transform\", fontsize=16, fontweight='bold')\nax3.set_xlabel(\"Non-Fraud vs Fraud\", fontsize=12, fontweight='bold')\nax3.set_ylabel(\"Amount(Log)\", fontsize = 12, fontweight='bold')\n\nplt.show()","3e834f50":"# Plotting a Correlation plot between cloumns\nplt.figure(figsize = (10,10))\nplt.title(\"Correlation Heat Map\", fontsize=20, color='r', fontweight='bold')\ncorr_matrix = d.corr()\nsns.heatmap(corr_matrix)\nplt.show()","9eb8a630":"d.hist(figsize = (20,20))\nplt.show()","42ad8642":"#data=pd.read_csv('processed.csv')\ndata=d # Creating a coppy of the data set","ac121341":"y=data.Class","c936d5b5":"data.columns","6c8e52c9":"X=data.drop(columns=['Time','Class','Time_Hours','Std_Amount','Mm_Amount','Amount'])","3cbbcc55":"X.columns","89a1e3c8":"from sklearn.model_selection import train_test_split","1d2785fb":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)","fa2e6a2c":"pd.Series(y_train).value_counts()","95491c4a":"plt.figure(figsize=(4,4), dpi=100)\npd.Series(y_train).value_counts().plot(kind = 'pie', autopct='%1.1f%%', fontsize = 20)\nplt.title(\"Fraudulent and Non-Fraudulent Distribution\",color='b', fontsize = 15, fontweight='bold')\nplt.legend([\"Non-Fraud\", \"Fraud\"])\nplt.show()","7496173e":"# Importing Logistic Regression Library\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n\n# Creating method for logistic regression\nlogreg = LogisticRegression() \nlogreg.fit(X_train, y_train)  ","ee3a07be":"y_pred=logreg.predict(X_test)\ny_pred[:5]","8f22f4cf":"# Importing library for confusion matrix\nfrom sklearn.metrics import confusion_matrix\n\n# Confusion Matrix\ncnf_matrix=confusion_matrix(y_test, y_pred)","bd8eee50":"print('Distribution of Test Data:')\npd.Series(y_test).value_counts()","3e01e69d":"print('Confusion Matrix:')\ncnf_matrix","596cf161":"plt.figure(figsize=(5,3))\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap='BuPu', fmt='d')\nplt.title(\"Confusion_Matrix\", fontsize=16, color='b',fontweight=\"bold\")\nplt.ylabel('Predicted', color='b',fontweight=\"bold\")\nplt.xlabel('Actual', color='b',fontweight=\"bold\")\nplt.show()","6d6ff72d":"# Fraudlent Class prediction (Recall Score of class-1)\nprint(f'Recall Score of Class 1: {cnf_matrix[1,1]\/(cnf_matrix[1,1]+cnf_matrix[1,0]):.4f}')","2f39413b":"# Accuracy Score\nprint(f'Accuracy Score: {accuracy_score(y_pred, y_test):.4f}')","335e4c33":"# Precision Score\nprint(f'Precision Score: {precision_score(y_test , y_pred):.4f}')","198a9f5b":"# Recall Score\nprint(f'Recall Score: {recall_score(y_test, y_pred):.4f}')","2e5768d0":"# F1 Score, harmonic mean of precision (PRE) and recall (REC)\nprint(f'F1 Score: {f1_score(y_test, y_pred):.4f}')","bc7ae945":"print(classification_report(y_test, y_pred))","5e3d53e3":"y_pred_proba = logreg.predict_proba(X_test)","ef6de635":"y_pred_proba[:5]","5590542f":"from sklearn.metrics import roc_auc_score\nprint(f'ROC_AUC Score: {roc_auc_score(y_test , y_pred):.4f}')","2b73b7c7":"from sklearn.metrics import roc_curve\n\nfpr, tpr, _ = roc_curve(y_test,  y_pred)\n\nauc = roc_auc_score(y_test, y_pred)\n\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n\nplt.legend(loc=4)\n\nplt.show()","6d902d1d":"# Importing Libraries\nfrom imblearn.over_sampling import SMOTE, ADASYN\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom collections import Counter # counter takes values returns value_counts dictionary","161fa9a5":"from sklearn.datasets import make_classification\n\n#X, y = make_classification(n_classes=2) #, class_sep=2,weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0,n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)\nprint('Original dataset shape %s'  %Counter(y))\n\nrus = RandomUnderSampler(random_state=42)\n\nX_res, y_res = rus.fit_resample(X, y)\nprint('Resampled dataset shape %s' % Counter(y_res))","4a836972":"pd.Series(y_res).value_counts()","ca13500c":"plt.figure(figsize=(4,4), dpi=100)\npd.Series(y_res).value_counts().plot(kind = 'pie', autopct='%1.1f%%', fontsize = 20)\nplt.title(\"Fraudulent and Non-Fraudulent Distribution\",color='b', fontsize = 15, fontweight='bold')\nplt.legend([\"Non-Fraud\", \"Fraud\"])\nplt.show()","4d76b395":"X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=0)","f78f5cd3":"# Importing Logistic Regression Library\nfrom sklearn.linear_model import LogisticRegression\n\n# Creating method for logistic regression\nlogreg = LogisticRegression() \nlogreg.fit(X_train, y_train)  ","5ce59a76":"y_pred=logreg.predict(X_test)\ny_pred[:5]","a8a5dda5":"# Importing library for confusion matrix\nfrom sklearn.metrics import confusion_matrix\n\n# Confusion Matrix\ncnf_matrix=confusion_matrix(y_test, y_pred)","8b108cd6":"print('Distribution of Test Data:')\npd.Series(y_test).value_counts()","a477bf0b":"print('Confusion Matrix:')\ncnf_matrix","29592641":"plt.figure(figsize=(5,3))\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap='BuPu', fmt='d')\nplt.title(\"Confusion_Matrix\", fontsize=16, color='b',fontweight=\"bold\")\nplt.ylabel('Predicted', color='b',fontweight=\"bold\")\nplt.xlabel('Actual', color='b',fontweight=\"bold\")\nplt.show()","aa6bd5e4":"# Fraudlent Class prediction (Recall Score of class-1)\nprint(f'Recall Score of Class 1: {cnf_matrix[1,1]\/(cnf_matrix[1,1]+cnf_matrix[1,0]):.4f}')","6585bcd4":"# Accuracy Score\nprint(f'Accuracy Score: {accuracy_score(y_pred, y_test):.4f}')","c2c6370b":"# Precision Score\nprint(f'Precision Score: {precision_score(y_test , y_pred):.4f}')","52d4a8a7":"# Recall Score\nprint(f'Recall Score: {recall_score(y_test, y_pred):.4f}')","5d6ff0c3":"# F1 Score, harmonic mean of precision (PRE) and recall (REC)\nprint(f'F1 Score: {f1_score(y_test, y_pred):.4f}')","999d88f7":"print(classification_report(y_test, y_pred))","671e46bb":"y_pred_proba = logreg.predict_proba(X_test)","1f384c1f":"y_pred_proba[:5]","b509e010":"from sklearn.metrics import roc_auc_score\nprint(f'ROC_AUC Score: {roc_auc_score(y_test , y_pred):.4f}')","65898eae":"from sklearn.metrics import roc_curve\n\nfpr, tpr, _ = roc_curve(y_test,  y_pred)\n\nauc = roc_auc_score(y_test, y_pred)\n\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n\nplt.legend(loc=4)\n\nplt.show()","60c59548":"# Importing Library\nfrom imblearn.over_sampling import RandomOverSampler","d28a74b7":"print('Original dataset shape %s' % Counter(y))\nrandom_state = 42\n\nrus = RandomOverSampler(random_state=random_state)\n\nX_res, y_res = rus.fit_resample(X, y)\nprint('Resampled dataset shape %s' % Counter(y_res))\n","2d59a70d":"pd.Series(y_res).value_counts()","73821908":"plt.figure(figsize=(4,4), dpi=100)\npd.Series(y_res).value_counts().plot(kind = 'pie', autopct='%1.1f%%', fontsize = 20)\nplt.title(\"Fraudlent and Non-Fraudlent Distribution\",color='b', fontsize = 15, fontweight='bold')\nplt.legend([\"Non-Fraud\", \"Fraud\"])\nplt.show()","a740c991":"X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=0)","8bbf8897":"# Importing Logistic Regression Library\nfrom sklearn.linear_model import LogisticRegression\n\n# Creating method for logistic regression\nlogreg = LogisticRegression() \nlogreg.fit(X_train, y_train)  ","b6069f8a":"y_pred=logreg.predict(X_test)\ny_pred[:5]","853c4a99":"# Importing library for confusion matrix\nfrom sklearn.metrics import confusion_matrix\n\n# Confusion Matrix\ncnf_matrix=confusion_matrix(y_test, y_pred)","c6fea4cc":"print('Distribution of Test Data:')\npd.Series(y_test).value_counts()","7e61cf16":"print('Confusion Matrix:')\ncnf_matrix","fda56aee":"plt.figure(figsize=(5,3))\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap='BuPu', fmt='d')\nplt.title(\"Confusion_Matrix\", fontsize=16, color='b',fontweight=\"bold\")\nplt.ylabel('Predicted', color='b',fontweight=\"bold\")\nplt.xlabel('Actual', color='b',fontweight=\"bold\")\nplt.show()","a33b343d":"# Fraudlent Class prediction (Recall Score of class-1)\nprint(f'Recall Score of Class 1: {cnf_matrix[1,1]\/(cnf_matrix[1,1]+cnf_matrix[1,0]):.4f}')","837c5fbf":"# Accuracy Score\nprint(f'Accuracy Score: {accuracy_score(y_pred, y_test):.4f}')","a7ea0219":"# Precision Score\nprint(f'Precision Score: {precision_score(y_test , y_pred):.4f}')","db6c53db":"# Recall Score\nprint(f'Recall Score: {recall_score(y_test, y_pred):.4f}')","a73161f3":"# F1 Score, harmonic mean of precision (PRE) and recall (REC)\nprint(f'F1 Score: {f1_score(y_test, y_pred):.4f}')","31798ae9":"print(classification_report(y_test, y_pred))","b29bdd44":"y_pred_proba = logreg.predict_proba(X_test)","bb1ec5b4":"y_pred_proba[:5]","ae5daba9":"from sklearn.metrics import roc_auc_score\nprint(f'ROC_AUC Score: {roc_auc_score(y_test , y_pred):.4f}')","0c9df300":"from sklearn.metrics import roc_curve\n\nfpr, tpr, _ = roc_curve(y_test,  y_pred)\n\nauc = roc_auc_score(y_test, y_pred)\n\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n\nplt.legend(loc=4)\n\nplt.show()","d41b3619":"# Importing Library\nfrom imblearn.over_sampling import SMOTE, ADASYN","e4eee5e6":"print('Original dataset shape %s' % Counter(y))\n\nrus = SMOTE(random_state=42)\n\nX_res, y_res = rus.fit_resample(X, y)\nprint('Resampled dataset shape %s' % Counter(y_res))","be483432":"pd.Series(y_res).value_counts()","9164ab12":"plt.figure(figsize=(4,4), dpi=100)\npd.Series(y_res).value_counts().plot(kind = 'pie', autopct='%1.1f%%', fontsize = 20)\nplt.title(\"Fraudlent and Non-Fraudlent Distribution\",color='b', fontsize = 15)\nplt.legend([\"Non-Fraud\", \"Fraud\"])\nplt.show()","9c038c51":"X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=0)","11164bf6":"# Importing Logistic Regression Library\nfrom sklearn.linear_model import LogisticRegression\n\n# Creating method for logistic regression\nlogreg = LogisticRegression() \nlogreg.fit(X_train, y_train)  ","030c7186":"y_pred=logreg.predict(X_test)\ny_pred[:5]","55bf82bc":"# Importing library for confusion matrix\nfrom sklearn.metrics import confusion_matrix\n\n# Confusion Matrix\ncnf_matrix=confusion_matrix(y_test, y_pred)","8d373fba":"print('Distribution of Test Data:')\npd.Series(y_test).value_counts()","30c7bb86":"print('Confusion Matrix:')\ncnf_matrix","f775c61f":"plt.figure(figsize=(5,3))\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap='BuPu', fmt='d')\nplt.title(\"Confusion_Matrix\", fontsize=16, color='b',fontweight=\"bold\")\nplt.ylabel('Predicted', color='b',fontweight=\"bold\")\nplt.xlabel('Actual', color='b',fontweight=\"bold\")\nplt.show()","f7d740e8":"# Fraudlent Class prediction (Recall Score of class-1)\nprint(f'Recall Score of Class 1: {cnf_matrix[1,1]\/(cnf_matrix[1,1]+cnf_matrix[1,0]):.4f}')","d096d510":"# Accuracy Score\nprint(f'Accuracy Score: {accuracy_score(y_pred, y_test):.4f}')","363c0452":"# Precision Score\nprint(f'Precision Score: {precision_score(y_test , y_pred):.4f}')","8b75c689":"# Recall Score\nprint(f'Recall Score: {recall_score(y_test, y_pred):.4f}')","564cabc7":"# F1 Score, harmonic mean of precision (PRE) and recall (REC)\nprint(f'F1 Score: {f1_score(y_test, y_pred):.4f}')","55af924e":"print(classification_report(y_test, y_pred))","4fe7252c":"y_pred_proba = logreg.predict_proba(X_test)","4b1a4c6f":"y_pred_proba[:5]","db67eda2":"from sklearn.metrics import roc_auc_score\nprint(f'ROC_AUC Score: {roc_auc_score(y_test , y_pred):.4f}')","a7978bb9":"from sklearn.metrics import roc_curve\n\nfpr, tpr, _ = roc_curve(y_test,  y_pred)\n\nauc = roc_auc_score(y_test, y_pred)\n\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n\nplt.legend(loc=4)\n\nplt.show()","2b2b45b9":"print('Original dataset shape %s' % Counter(y))\n\nrus = ADASYN(random_state=42)\n\nX_res, y_res = rus.fit_resample(X, y)\nprint('Resampled dataset shape %s' % Counter(y_res))","f54e1989":"pd.Series(y_res).value_counts()","d5a22fcf":"plt.figure(figsize=(4,4), dpi=100)\npd.Series(y_res).value_counts().plot(kind = 'pie', autopct='%1.1f%%', fontsize = 20)\nplt.title(\"Fraudulent and Non-Fraudulent Distribution\",color='b', fontsize = 15, fontweight='bold')\nplt.legend([\"Non-Fraud\", \"Fraud\"])\nplt.show()","a68a1333":"X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=0)","e3870c17":"# Importing Logistic Regression Library\nfrom sklearn.linear_model import LogisticRegression\n\n# Creating method for logistic regression\nlogreg = LogisticRegression() \nlogreg.fit(X_train, y_train)  ","f3295cc6":"y_pred=logreg.predict(X_test)\ny_pred[:5]","d0db3c81":"# Importing library for confusion matrix\nfrom sklearn.metrics import confusion_matrix\n\n# Confusion Matrix\ncnf_matrix=confusion_matrix(y_test, y_pred)","422dca4a":"print('Distribution of Test Data:')\npd.Series(y_test).value_counts()","b4560004":"print('Confusion Matrix:')\ncnf_matrix","c66471f7":"plt.figure(figsize=(5,3))\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap='BuPu', fmt='d')\nplt.title(\"Confusion_Matrix\", fontsize=16, color='b',fontweight=\"bold\")\nplt.ylabel('Predicted', color='b',fontweight=\"bold\")\nplt.xlabel('Actual', color='b',fontweight=\"bold\")\nplt.show()","ab47a7f6":"# Fraudulent Class prediction (Recall Score of class-1)\nprint(f'Recall Score of Class 1: {cnf_matrix[1,1]\/(cnf_matrix[1,1]+cnf_matrix[1,0]):.4f}')","b0747a9f":"# Accuracy Score\nprint(f'Accuracy Score: {accuracy_score(y_pred, y_test):.4f}')","e298ddc9":"# Precision Score\nprint(f'Precision Score: {precision_score(y_test , y_pred):.4f}')","6b52ac38":"# Recall Score\nprint(f'Recall Score: {recall_score(y_test, y_pred):.4f}')","7f348cde":"# F1 Score, harmonic mean of precision (PRE) and recall (REC)\nprint(f'F1 Score: {f1_score(y_test, y_pred):.4f}')","18789ecd":"print(classification_report(y_test, y_pred))","7b19cfd0":"y_pred_proba = logreg.predict_proba(X_test)","c9cd64ac":"y_pred_proba[:5]","5394630a":"from sklearn.metrics import roc_auc_score\nprint(f'ROC_AUC Score: {roc_auc_score(y_test , y_pred):.4f}')","97207b57":"from sklearn.metrics import roc_curve\n\nfpr, tpr, _ = roc_curve(y_test,  y_pred)\n\nauc = roc_auc_score(y_test, y_pred)\n\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n\nplt.legend(loc=4)\n\nplt.show()","2272a205":"from sklearn.decomposition import PCA # SVD , t-SNE , Linear Discrimant Analysis\nX_reduced_pca = PCA(n_components=2, random_state=42).fit_transform(X_res)","8939a0e4":"plt.figure(figsize=(10,6), dpi=100)\n\nplt.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y_res== 0), cmap='coolwarm', label='No Fraud', linewidths=2)\nplt.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y_res == 1), cmap='coolwarm', label='Fraud', linewidths=2)\nplt.show()","dbd754c1":"rus = RandomUnderSampler(random_state=42)\nX_under, y_under = rus.fit_resample(X, y)\nprint('Resampled dataset shape %s' % Counter(y_under))\n","52feadee":"rus = RandomOverSampler(random_state=42)\nX_over, y_over = rus.fit_resample(X, y)\nprint('Resampled dataset shape %s' % Counter(y_over))","e242f4a3":"rus = SMOTE(random_state=42)\nX_smote, y_smote = rus.fit_resample(X, y)\nprint('Resampled dataset shape %s' % Counter(y_smote))","87e9514d":"# Importing Libraries\nfrom sklearn.svm import SVC # Support Vector Classifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier","fe9aa227":"# Spliting Data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n# Creating object and running algorithm\ndte = DecisionTreeClassifier()\ndte.fit( X_train, y_train )\n\n# Predicting Test Data\ny_pred = dte.predict(X_test)\n\n# Prediction Accuracy\nprint('\\n')\nprint('*** Looking at Performance Measures ***')\nprint(f'Accuracy Score: {accuracy_score(y_pred , y_test)}')\n\n\n# Confusion Matrix\ncnf_matrix=confusion_matrix(y_test, y_pred)\n\n# ROC AUC Curve\nfpr, tpr, _ = roc_curve(y_test,  y_pred)\nauc = roc_auc_score(y_test, y_pred)\nprint(f'AUC Score: {auc}')\n\nprint('\\n')\nprint(classification_report(y_test, y_pred))\nprint('\\n')\nplt.figure(figsize=(15,6))\n\n#Ploting Confusion Matrix\nplt.subplot(121)\nax = sns.heatmap(pd.DataFrame(cnf_matrix), annot = True, cmap = 'Blues', fmt = 'd')\nax.set_title(\"Confusion Matrix\", fontsize=16, fontweight='bold')\nax.set_xlabel(\"Actual\", fontsize=12, fontweight='bold')\nax.set_ylabel(\"Predicted\", fontsize = 12, fontweight='bold')\n\n# Ploting ROC AUC Curve\nplt.subplot(122)\nax1 = plt.plot(fpr, tpr, label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.title('ROC_AUC Curve',fontsize=16, fontweight='bold')\nplt.show()","25a6b693":"# Spliting Data\nX_train, X_test, y_train, y_test = train_test_split(X_under, y_under, test_size=0.3, random_state=0)\n\n# Creating object and running algorithm\ndte = DecisionTreeClassifier()\ndte.fit( X_train, y_train )\n\n# Predicting Test Data\ny_pred = dte.predict(X_test)\n\n# Prediction Accuracy\nprint('\\n')\nprint('*** Looking at Performance Measures ***')\nprint(f'Accuracy Score: {accuracy_score(y_pred , y_test)}')\n\n\n# Confusion Matrix\ncnf_matrix=confusion_matrix(y_test, y_pred)\n\n# ROC AUC Curve\nfpr, tpr, _ = roc_curve(y_test,  y_pred)\nauc = roc_auc_score(y_test, y_pred)\nprint(f'AUC Score: {auc}')\n\nprint('\\n')\nprint(classification_report(y_test, y_pred))\nprint('\\n')\nplt.figure(figsize=(15,6))\n\n#Ploting Confusion Matrix\nplt.subplot(121)\nax = sns.heatmap(pd.DataFrame(cnf_matrix), annot = True, cmap = 'Blues', fmt = 'd')\nax.set_title(\"Confusion Matrix\", fontsize=16, fontweight='bold')\nax.set_xlabel(\"Actual\", fontsize=12, fontweight='bold')\nax.set_ylabel(\"Predicted\", fontsize = 12, fontweight='bold')\n\n# Ploting ROC AUC Curve\nplt.subplot(122)\nax1 = plt.plot(fpr, tpr, label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.title('ROC_AUC Curve',fontsize=16, fontweight='bold')\nplt.show()","e3b5903d":"# Spliting Data\nX_train, X_test, y_train, y_test = train_test_split(X_over, y_over, test_size=0.3, random_state=0)\n\n# Creating object and running algorithm\ndte = DecisionTreeClassifier()\ndte.fit( X_train, y_train )\n\n# Predicting Test Data\ny_pred = dte.predict(X_test)\n\n# Prediction Accuracy\nprint('\\n')\nprint('*** Looking at Performance Measures ***')\nprint(f'Accuracy Score: {accuracy_score(y_pred , y_test)}')\n\n\n# Confusion Matrix\ncnf_matrix=confusion_matrix(y_test, y_pred)\n\n# ROC AUC Curve\nfpr, tpr, _ = roc_curve(y_test,  y_pred)\nauc = roc_auc_score(y_test, y_pred)\nprint(f'AUC Score: {auc}')\n\nprint('\\n')\nprint(classification_report(y_test, y_pred))\nprint('\\n')\nplt.figure(figsize=(15,6))\n\n#Ploting Confusion Matrix\nplt.subplot(121)\nax = sns.heatmap(pd.DataFrame(cnf_matrix), annot = True, cmap = 'Blues', fmt = 'd')\nax.set_title(\"Confusion Matrix\", fontsize=16, fontweight='bold')\nax.set_xlabel(\"Actual\", fontsize=12, fontweight='bold')\nax.set_ylabel(\"Predicted\", fontsize = 12, fontweight='bold')\n\n# Ploting ROC AUC Curve\nplt.subplot(122)\nax1 = plt.plot(fpr, tpr, label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.title('ROC_AUC Curve',fontsize=16, fontweight='bold')\nplt.show()","c0041a6d":"# Spliting Data\nX_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.3, random_state=0)\n\n# Creating object and running algorithm\ndte = DecisionTreeClassifier()\ndte.fit( X_train, y_train )\n\n# Predicting Test Data\ny_pred = dte.predict(X_test)\n\n# Prediction Accuracy\nprint('\\n')\nprint('*** Looking at Performance Measures ***')\nprint(f'Accuracy Score: {accuracy_score(y_pred , y_test)}')\n\n\n# Confusion Matrix\ncnf_matrix=confusion_matrix(y_test, y_pred)\n\n# ROC AUC Curve\nfpr, tpr, _ = roc_curve(y_test,  y_pred)\nauc = roc_auc_score(y_test, y_pred)\nprint(f'AUC Score: {auc}')\n\nprint('\\n')\nprint(classification_report(y_test, y_pred))\nprint('\\n')\nplt.figure(figsize=(15,6))\n\n#Ploting Confusion Matrix\nplt.subplot(121)\nax = sns.heatmap(pd.DataFrame(cnf_matrix), annot = True, cmap = 'Blues', fmt = 'd')\nax.set_title(\"Confusion Matrix\", fontsize=16, fontweight='bold')\nax.set_xlabel(\"Actual\", fontsize=12, fontweight='bold')\nax.set_ylabel(\"Predicted\", fontsize = 12, fontweight='bold')\n\n# Ploting ROC AUC Curve\nplt.subplot(122)\nax1 = plt.plot(fpr, tpr, label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.title('ROC_AUC Curve',fontsize=16, fontweight='bold')\nplt.show()","c7407cb8":"# Spliting Data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n# Creating object and running algorithm\nrfc = RandomForestClassifier()\nrfc.fit( X_train, y_train )\n\n\n# Predicting Test Data\ny_pred = rfc.predict(X_test)\n\n# Prediction Accuracy\nprint('\\n')\nprint('*** Looking at Performance Measures ***')\nprint(f'Accuracy Score: {accuracy_score(y_pred , y_test)}')\n\n\n# Confusion Matrix\ncnf_matrix=confusion_matrix(y_test, y_pred)\n\n# ROC AUC Curve\nfpr, tpr, _ = roc_curve(y_test,  y_pred)\nauc = roc_auc_score(y_test, y_pred)\nprint(f'AUC Score: {auc}')\n\nprint('\\n')\nprint(classification_report(y_test, y_pred))\nprint('\\n')\nplt.figure(figsize=(15,6))\n\n#Ploting Confusion Matrix\nplt.subplot(121)\nax = sns.heatmap(pd.DataFrame(cnf_matrix), annot = True, cmap = 'Blues', fmt = 'd')\nax.set_title(\"Confusion Matrix\", fontsize=16, fontweight='bold')\nax.set_xlabel(\"Actual\", fontsize=12, fontweight='bold')\nax.set_ylabel(\"Predicted\", fontsize = 12, fontweight='bold')\n\n# Ploting ROC AUC Curve\nplt.subplot(122)\nax1 = plt.plot(fpr, tpr, label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.title('ROC_AUC Curve',fontsize=16, fontweight='bold')\nplt.show()","16892311":"# Spliting Data\nX_train, X_test, y_train, y_test = train_test_split(X_under, y_under, test_size=0.3, random_state=0)\n\n# Creating object and running algorithm\nrfc = RandomForestClassifier()\nrfc.fit( X_train, y_train )\n\n\n# Predicting Test Data\ny_pred = rfc.predict(X_test)\n\n# Prediction Accuracy\nprint('\\n')\nprint('*** Looking at Performance Measures ***')\nprint(f'Accuracy Score: {accuracy_score(y_pred , y_test)}')\n\n\n# Confusion Matrix\ncnf_matrix=confusion_matrix(y_test, y_pred)\n\n# ROC AUC Curve\nfpr, tpr, _ = roc_curve(y_test,  y_pred)\nauc = roc_auc_score(y_test, y_pred)\nprint(f'AUC Score: {auc}')\n\nprint('\\n')\nprint(classification_report(y_test, y_pred))\nprint('\\n')\nplt.figure(figsize=(15,6))\n\n#Ploting Confusion Matrix\nplt.subplot(121)\nax = sns.heatmap(pd.DataFrame(cnf_matrix), annot = True, cmap = 'Blues', fmt = 'd')\nax.set_title(\"Confusion Matrix\", fontsize=16, fontweight='bold')\nax.set_xlabel(\"Actual\", fontsize=12, fontweight='bold')\nax.set_ylabel(\"Predicted\", fontsize = 12, fontweight='bold')\n\n# Ploting ROC AUC Curve\nplt.subplot(122)\nax1 = plt.plot(fpr, tpr, label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.title('ROC_AUC Curve',fontsize=16, fontweight='bold')\nplt.show()","3fe80d9e":"# Spliting Data\nX_train, X_test, y_train, y_test = train_test_split(X_over, y_over, test_size=0.3, random_state=0)\n\n# Creating object and running algorithm\nrfc = RandomForestClassifier()\nrfc.fit( X_train, y_train )\n\n\n# Predicting Test Data\ny_pred = rfc.predict(X_test)\n\n# Prediction Accuracy\nprint('\\n')\nprint('*** Looking at Performance Measures ***')\nprint(f'Accuracy Score: {accuracy_score(y_pred , y_test)}')\n\n\n# Confusion Matrix\ncnf_matrix=confusion_matrix(y_test, y_pred)\n\n# ROC AUC Curve\nfpr, tpr, _ = roc_curve(y_test,  y_pred)\nauc = roc_auc_score(y_test, y_pred)\nprint(f'AUC Score: {auc}')\n\nprint('\\n')\nprint(classification_report(y_test, y_pred))\nprint('\\n')\nplt.figure(figsize=(15,6))\n\n#Ploting Confusion Matrix\nplt.subplot(121)\nax = sns.heatmap(pd.DataFrame(cnf_matrix), annot = True, cmap = 'Blues', fmt = 'd')\nax.set_title(\"Confusion Matrix\", fontsize=16, fontweight='bold')\nax.set_xlabel(\"Actual\", fontsize=12, fontweight='bold')\nax.set_ylabel(\"Predicted\", fontsize = 12, fontweight='bold')\n\n# Ploting ROC AUC Curve\nplt.subplot(122)\nax1 = plt.plot(fpr, tpr, label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.title('ROC_AUC Curve',fontsize=16, fontweight='bold')\nplt.show()","21048927":"# Spliting Data\nX_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.3, random_state=0)\n\n# Creating object and running algorithm\nrfc = RandomForestClassifier()\nrfc.fit( X_train, y_train )\n\n\n# Predicting Test Data\ny_pred = rfc.predict(X_test)\n\n# Prediction Accuracy\nprint('\\n')\nprint('*** Looking at Performance Measures ***')\nprint(f'Accuracy Score: {accuracy_score(y_pred , y_test)}')\n\n\n# Confusion Matrix\ncnf_matrix=confusion_matrix(y_test, y_pred)\n\n# ROC AUC Curve\nfpr, tpr, _ = roc_curve(y_test,  y_pred)\nauc = roc_auc_score(y_test, y_pred)\nprint(f'AUC Score: {auc}')\n\nprint('\\n')\nprint(classification_report(y_test, y_pred))\nprint('\\n')\nplt.figure(figsize=(15,6))\n\n#Ploting Confusion Matrix\nplt.subplot(121)\nax = sns.heatmap(pd.DataFrame(cnf_matrix), annot = True, cmap = 'Blues', fmt = 'd')\nax.set_title(\"Confusion Matrix\", fontsize=16, fontweight='bold')\nax.set_xlabel(\"Actual\", fontsize=12, fontweight='bold')\nax.set_ylabel(\"Predicted\", fontsize = 12, fontweight='bold')\n\n# Ploting ROC AUC Curve\nplt.subplot(122)\nax1 = plt.plot(fpr, tpr, label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.title('ROC_AUC Curve',fontsize=16, fontweight='bold')\nplt.show()","b4410278":"# Spliting Data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n# Creating object and running algorithm\nkn=KNeighborsClassifier()\nkn.fit( X_train, y_train )\n\n\n# Predicting Test Data\ny_pred = kn.predict(X_test)\n\n# Prediction Accuracy\nprint('\\n')\nprint('*** Looking at Performance Measures ***')\nprint(f'Accuracy Score: {accuracy_score(y_pred , y_test)}')\n\n\n# Confusion Matrix\ncnf_matrix=confusion_matrix(y_test, y_pred)\n\n# ROC AUC Curve\nfpr, tpr, _ = roc_curve(y_test,  y_pred)\nauc = roc_auc_score(y_test, y_pred)\nprint(f'AUC Score: {auc}')\n\nprint('\\n')\nprint(classification_report(y_test, y_pred))\nprint('\\n')\nplt.figure(figsize=(15,6))\n\n#Ploting Confusion Matrix\nplt.subplot(121)\nax = sns.heatmap(pd.DataFrame(cnf_matrix), annot = True, cmap = 'Blues', fmt = 'd')\nax.set_title(\"Confusion Matrix\", fontsize=16, fontweight='bold')\nax.set_xlabel(\"Actual\", fontsize=12, fontweight='bold')\nax.set_ylabel(\"Predicted\", fontsize = 12, fontweight='bold')\n\n# Ploting ROC AUC Curve\nplt.subplot(122)\nax1 = plt.plot(fpr, tpr, label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.title('ROC_AUC Curve',fontsize=16, fontweight='bold')\nplt.show()","9f16b12e":"# Spliting Data\nX_train, X_test, y_train, y_test = train_test_split(X_under, y_under, test_size=0.3, random_state=0)\n\n# Creating object and running algorithm\nkn=KNeighborsClassifier()\nkn.fit( X_train, y_train )\n\n\n# Predicting Test Data\ny_pred = kn.predict(X_test)\n\n# Prediction Accuracy\nprint('\\n')\nprint('*** Looking at Performance Measures ***')\nprint(f'Accuracy Score: {accuracy_score(y_pred , y_test)}')\n\n\n# Confusion Matrix\ncnf_matrix=confusion_matrix(y_test, y_pred)\n\n# ROC AUC Curve\nfpr, tpr, _ = roc_curve(y_test,  y_pred)\nauc = roc_auc_score(y_test, y_pred)\nprint(f'AUC Score: {auc}')\n\nprint('\\n')\nprint(classification_report(y_test, y_pred))\nprint('\\n')\nplt.figure(figsize=(15,6))\n\n#Ploting Confusion Matrix\nplt.subplot(121)\nax = sns.heatmap(pd.DataFrame(cnf_matrix), annot = True, cmap = 'Blues', fmt = 'd')\nax.set_title(\"Confusion Matrix\", fontsize=16, fontweight='bold')\nax.set_xlabel(\"Actual\", fontsize=12, fontweight='bold')\nax.set_ylabel(\"Predicted\", fontsize = 12, fontweight='bold')\n\n# Ploting ROC AUC Curve\nplt.subplot(122)\nax1 = plt.plot(fpr, tpr, label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.title('ROC_AUC Curve',fontsize=16, fontweight='bold')\nplt.show()\n","b75c2a31":"# Spliting Data\nX_train, X_test, y_train, y_test = train_test_split(X_over, y_over, test_size=0.3, random_state=0)\n\n# Creating object and running algorithm\nkn=KNeighborsClassifier()\nkn.fit( X_train, y_train )\n\n\n# Predicting Test Data\ny_pred = kn.predict(X_test)\n\n# Prediction Accuracy\nprint('\\n')\nprint('*** Looking at Performance Measures ***')\nprint(f'Accuracy Score: {accuracy_score(y_pred , y_test)}')\n\n\n# Confusion Matrix\ncnf_matrix=confusion_matrix(y_test, y_pred)\n\n# ROC AUC Curve\nfpr, tpr, _ = roc_curve(y_test,  y_pred)\nauc = roc_auc_score(y_test, y_pred)\nprint(f'AUC Score: {auc}')\n\nprint('\\n')\nprint(classification_report(y_test, y_pred))\nprint('\\n')\nplt.figure(figsize=(15,6))\n\n#Ploting Confusion Matrix\nplt.subplot(121)\nax = sns.heatmap(pd.DataFrame(cnf_matrix), annot = True, cmap = 'Blues', fmt = 'd')\nax.set_title(\"Confusion Matrix\", fontsize=16, fontweight='bold')\nax.set_xlabel(\"Actual\", fontsize=12, fontweight='bold')\nax.set_ylabel(\"Predicted\", fontsize = 12, fontweight='bold')\n\n# Ploting ROC AUC Curve\nplt.subplot(122)\nax1 = plt.plot(fpr, tpr, label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.title('ROC_AUC Curve',fontsize=16, fontweight='bold')\nplt.show()\n","3498c1d2":"# Spliting Data\nX_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.3, random_state=0)\n\n# Creating object and running algorithm\nkn=KNeighborsClassifier()\nkn.fit( X_train, y_train )\n\n\n# Predicting Test Data\ny_pred = kn.predict(X_test)\n\n# Prediction Accuracy\nprint('\\n')\nprint('*** Looking at Performance Measures ***')\nprint(f'Accuracy Score: {accuracy_score(y_pred , y_test)}')\n\n\n# Confusion Matrix\ncnf_matrix=confusion_matrix(y_test, y_pred)\n\n# ROC AUC Curve\nfpr, tpr, _ = roc_curve(y_test,  y_pred)\nauc = roc_auc_score(y_test, y_pred)\nprint(f'AUC Score: {auc}')\n\nprint('\\n')\nprint(classification_report(y_test, y_pred))\nprint('\\n')\nplt.figure(figsize=(15,6))\n\n#Ploting Confusion Matrix\nplt.subplot(121)\nax = sns.heatmap(pd.DataFrame(cnf_matrix), annot = True, cmap = 'Blues', fmt = 'd')\nax.set_title(\"Confusion Matrix\", fontsize=16, fontweight='bold')\nax.set_xlabel(\"Actual\", fontsize=12, fontweight='bold')\nax.set_ylabel(\"Predicted\", fontsize = 12, fontweight='bold')\n\n# Ploting ROC AUC Curve\nplt.subplot(122)\nax1 = plt.plot(fpr, tpr, label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.title('ROC_AUC Curve',fontsize=16, fontweight='bold')\nplt.show()\n","ab451edc":"from sklearn.model_selection import GridSearchCV","1a3af021":"# Spliting Data\nX_train, X_test, y_train, y_test = train_test_split(X_over, y_over, test_size=0.3, random_state=0)\n# DecisionTree Classifier\ntree_params = {\"criterion\" :['gini',\"entropy\"],\n               \"splitter\" : ['best','random'],\n               \"max_features\" : [\"auto\",\"sqrt\", \"log2\"]}\ngrid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params)\ngrid_tree.fit(X_train, y_train)\n\n# tree best estimator\ntree_clf = grid_tree.best_estimator_","92e04706":"tree_clf","96a3355f":"# Spliting Data\nX_train, X_test, y_train, y_test = train_test_split(X_over, y_over, test_size=0.3, random_state=0)\n\n# Creating object and running algorithm\ndte = DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n                       max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, presort='deprecated',\n                       random_state=None, splitter='best')\ndte.fit( X_train, y_train )\n\n# Predicting Test Data\ny_pred = dte.predict(X_test)\n\n# Prediction Accuracy\nprint('\\n')\nprint('*** Looking at Performance Measures ***')\nprint(f'Accuracy Score: {accuracy_score(y_pred , y_test)}')\n\n\n# Confusion Matrix\ncnf_matrix=confusion_matrix(y_test, y_pred)\n\n# ROC AUC Curve\nfpr, tpr, _ = roc_curve(y_test,  y_pred)\nauc = roc_auc_score(y_test, y_pred)\nprint(f'AUC Score: {auc}')\n\nprint('\\n')\nprint(classification_report(y_test, y_pred))\nprint('\\n')\nplt.figure(figsize=(15,6))\n\n#Ploting Confusion Matrix\nplt.subplot(121)\nax = sns.heatmap(pd.DataFrame(cnf_matrix), annot = True, cmap = 'Blues', fmt = 'd')\nax.set_title(\"Confusion Matrix\", fontsize=16, fontweight='bold')\nax.set_xlabel(\"Actual\", fontsize=12, fontweight='bold')\nax.set_ylabel(\"Predicted\", fontsize = 12, fontweight='bold')\n\n# Ploting ROC AUC Curve\nplt.subplot(122)\nax1 = plt.plot(fpr, tpr, label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.title('ROC_AUC Curve',fontsize=16, fontweight='bold')\nplt.show()","f7c8559d":"### Accuracy Score","38842988":"### Observations:\n- From the distribution we can observe that for both of the days when number of non-fraudulent transaction is high, number of fraudulent transactions is also high\n- But when number of non-fraudulent transaction is low, frequency of fraudulent transaction is comparatively higher\n- Probably frequency of fraudulent transaction is higher at the night time when normal business activity is less \n- Now we will try to figure out the pattern for both fraudulent and non-fraudulent transactions during different time of the day","2d0263bf":"### Note: \nOur focus is to check the accuracy percentage of fraudulent class prediction, more precisely Recall Score of Class 1","79262f08":"### Predicting Test data","3867bea2":"### Accuracy Summary Report","5cc2fdfd":"# Context:\nIt is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase.","fbff0911":"### Recall Score","28015134":"### Recall Score","3d4ef6df":"### F1 Score, harmonic mean of precision (PRE) and recall (REC)","cbc2067d":"### Spliting Data","8fa626f5":"### Accuracy Score","7ad2ffdf":"# Now applying different models and evaluating the dataset","0a1a9eba":"### Accuracy Summary Report","5b15fc5e":"#### Reducing 29 columns to 2 columns , so that it can be viewed in a single plot","86f4cc55":"### Spliting Data","365e0ee2":"### Observations:\nApplying Random Forest Classifier on all 4 types of data set (Raw, under sampled, over sampled, SMOTE) we have observed that Random Forest Classifier on SMOTE- sampled data predicts the test data with better accuracy","29b80074":"# *Thank you for your valuable feedback...!!!*","a907c77f":"# Random Forest Classifier on Raw data","0acc353a":"# Principal Component Analysis","e6539711":"### Accuracy Score","50dbfb33":"### Observations:\n- As our goal is to predict Fraudulent traction correctly. So, our focus area is to check the accuracy percentage of fraudulent class prediction\n- Using Over Sampling technique to handle imbalance class problem, out of 85393 fraudulent transaction, 78649 transaction has been predicted correctly\n- So, the model has predicted our target outcome with 92.10% accuracy","eb1fff86":"# **Goal:->   To Predict Fraudulent Transaction**","a0aefeb6":"### Observations:\n- This data set does not contain any missing value.\n- Sometimes missing values can be found in the form of 0 or na or any symbolical expression.\nIn such cases we have to explore the data column wise to get a better understanding of missing value or meaning less value.","c5ae45cc":"### Observations:\n- Amount feature has 0 value transactions in it, these transactions might be the unsuccessful transactions\n- By looking at mean, 3rd quartile and maximum value we can clearly say that Amount column is positively skewed\n- As maximum amount is significantly higher than the 3rd quartile value, there is high probability of having outlier values.\nSo, we need to further look in to the Amount column for a better understanding of the outlier values\n- Class has a very low mean value, which is an indication of imbalance class.\n","f8bc828d":"# Undersampled Data","173dbc42":"### Maximum Value Threshold - Outliar Removal","73b15915":"### Recall Score","b02df68d":"# Oversampled Data","8044bc80":"### Running Logistic Regression","0667b5d0":"---\n# ***Credit Card Fraud Detection***\n___","adb07f23":"### Note:\nOur focus is to check the accuracy percentage of fraudulent class prediction, more precisely Recall Score of Class 1","3ab44d1c":"### Confusion Matrix","4f22f3ca":"# EDA","38016ce4":"# Visualization of Overall Data ","35b221df":"# Running Logistic Regression Model on Imbalance Class Data","e2332557":"### Observations:\n- All 31 columns are numerical in nature\n- Only level column (Class) is integer type and other 30 are float type\n- All the columns have 284807 non null values which is equivalent to the total number of rows. \nSo the data set does not contain any null value in any column\n- Size of the data is 67.4 MB","9b784948":"# Random Forest Classifier on SMOTE Sampled Data","438a23df":"### Precision Score","63cbf8eb":"# Decision Tree Classifier on  Under Sampling Data","1ddedf5d":"### Recall Score","6d179495":"### Accuracy Summary Report","938b1f44":"### Accuracy Summary Report","d44a3d01":"### ROC AUC Curve","f3bd1444":"### Looking at predicted probability","9b29430b":"# Decision Tree Classifier on Raw data","89c874d6":"### Running Logistic Regression","04b82f5d":"### Observations:\n- Assuming recording of data started form 12am at night\n- In 24 hours\u2019, time frame, at the night time (from 12am to 9am - when normal business activity is less) frequency of Fraudulent transaction is higher\n- Again, after the evening time (after 9-10pm) frequency of non-fraudulent transaction starts reducing but at same time frequency of fraudulent transaction is higher","ef92a35a":"### Frudelent and Non Frudelent Distribution for Different Transacton Amount","c42604d3":"### Recall Score","33f5cd9d":"### Observations:\n- Now we have removed outlier using maximum transaction value of fraudulent class as Threshold\n- No record is lost from under sampled class (Fraudulent Class), to satisfy class imbalance problem","086705d8":"### Running Logistic Regression","d64af079":"### Exploring Time Feature","125bb58e":"### Confusion Matrix","7c24bb3e":"### F1 Score, harmonic mean of precision (PRE) and recall (REC)","d5090441":"### Findings:\n- We can clearly observe that log transformation of Amount column gives better form of scalling","71ad760c":"### Looking at predicted probability","79505d23":"### KNeighborsClassifier on Raw Data","5bcd98d9":"### Creating Dependent Veriable","7042b0ad":"# Summary:\n- DecisionTree Classifier predicts test data with ***99.98%*** \n- Gives recall score of 1, means our algorithm is predicting fraudulent transaction of test data set with ***100%*** efficiency\n- Our AUC score of ***99.98%*** is pretty much convincing","bbdd5e5f":"# Under Sampling Data","9a5a8d43":"# Decision Tree Classifier on  Over Sampling Data","58c7f067":"# Over Sampling","1baf2a84":"# Scalling of Feature","649eb82a":"### KNeighborsClassifier on under sampled Data","eb68d7eb":"### Precision Score","1ed9368a":"### Finding Correlation","4c591235":"### Approach:\n- EDA and data Visualization\n- Deciding upon dependent and independent variables\n- Splitting data in to training and test set\n- Trying different over and under sampling technique\n- Running simple classification algorithm on data for a broader view\n- Trying different classification algorithm on different set of sampled data\n- Comparing prediction accuracy on testy data for different combination\n- Selecting comparatively better model based on test data prediction\n- Performing hyper parameter tuning on the selected model and checking performance","00c66d5c":"### Predicting Test data","d298eba2":"### Predicting Test data","66d9eb9b":"# Random Forest Classifier on Over Sampled Data","109b6e81":"### Confusion Matrix","810a9201":"### Note:\nOur focus is to check the accuracy percentage of fraudulent class prediction, more precisely Recall Score of Class 1","f8647f44":"### ROC AUC Curve","b54e110a":"### Inter Quartile Range - for outliar removal","49054a16":"### Note: \nOur focus is to check the accuracy percentage of fraudlent class prediction, more precisely Recall Score of Class ","86a6e4b6":"### F1 Score, harmonic mean of precision (PRE) and recall (REC)","0c2d10d6":"### Observations:\n- 50.60% share of Fraudulent transaction recorded for transaction value up to 1 where 113 transactions recorded for $1 transaction\n- Percentage share of Fraudulent transaction decreases when transaction amount gradually increases\n- Maximum transaction value recorded for fraudulent transaction is 2125.87 and any amount greater than this are Non-Fraudulent transaction","e9ab57dd":"### Note:\nOur focus is to check the accuracy percentage of fraudulent class prediction, more precisely Recall Score of Class 1","22795ac9":"### Min-Max Scalling","7553900b":"### F1 Score, harmonic mean of precision (PRE) and recall (REC)","382be84b":"### Spliting Data","19176a12":"### Observations:\n- From the distribution we can observe that there is a symmetrical pattern in number of transactions\n- Probably in the day time number of transactions is high for both of the day and low in the night time\n- Now we will convert the time frame in hours term and see the distribution of transactions in different time of the day","6bc28b0b":"### Distribution of transactions in two days","0f500401":"### Precision Score","5867956e":"### Accuracy Score","91bf2320":"### Running Logistic Regression","5e290302":"### ROC AUC Curve","710cd3ea":"# Creating Multiple Datasets Using Under Sampled, Oversampled and SMOTE Technique and Trying Different Classification Model","603ee404":"### Observations:\n- As our goal is to predict Fraudulent traction correctly. So, our focus area is to check the accuracy percentage of fraudulent class prediction\n- Using Under Sampling technique to handle imbalance class problem, out of 146 fraudulent transaction, 134 transaction has been predicted correctly\n- So, the model has predicted our target outcome with 91.78% accuracy","527a55be":"# Random Forest Classifier on Under Sampled Data","e12f83bf":"### Observations:\nApplying Decision tree classifier on all 4 types of data set (Raw, under sampled, over sampled, SMOTE)  we have observed that Decision tree classifier on Over sampled data predicts the test data with better accuracy ","010664ec":"### Predicting Test data","4d94dfae":"### Observations:\n- As our goal is to predict Fraudulent traction correctly. So, our focus area is to check the accuracy percentage of fraudulent class prediction\n- Using ADASYN Sampling technique to handle imbalance class problem, out of 85335 fraudulent transaction, 73141 transaction has been predicted correctly\n- So, the model has predicted our target outcome with 85.71% accuracy ","ee8c983a":"### Observations:\nApplying KNeighborsClassifier on all 4 types of data set (Raw, under sampled, over sampled, SMOTE) we have observed that KNeighborsClassifier on Over Sampled and SMOTE- sampled data predicts the test data with better accuracy","a326138a":"### Observations:\n- Here, our goal is to predict Fraudulent traction correctly. So, our focus area is to check the accuracy percentage of fraudulent class prediction, more precisely Recall Score of Class 1.\n- In this model, out of 164 fraudulent transaction 106 transaction has been predicted correctly\n- So, the model has predicted our target outcome with 64.63% accuracy","c8bca066":"# Decision Tree Classifier on  SMOTE Sampling Data","eb7ad7e7":"### Missing values","9b2d094e":"### Observations: \n- So far, we have tried different sampling technique (Under Sampling, Over Sampling, SMOTE Sampling, ADASYN Sampling) to handle class imbalance problem and applied logistic regression model on different sample to compare accuracy of fraudulent transaction prediction\n- We have observed that over sampling of data set produces better result than other form of sampling data set \n","c5aa9ef6":"### Hyperparameter Tuning on DecisionTree Classifier","9994fd7b":"### Note:\nTo deal with imbalance class problem, we have to remove outlier in such a way that we lose minimum or no amount of under sampled data (Fraudulent class). Otherwise we will further imbalance the data.\n\nKeeping this in mind we will try different technique to remove outlier from Amount Feature","1255a900":"# SMOTE Sampling","b2e770e6":"### Optimizing DecisionTree Classifier","883f2b5d":"# Synthetic Minority Over Sampling Technique(SMOTE) Data","20b75b19":"### Spliting Data into Training and Test Set","aa27fa1c":"### Predicting Test data","5ba66e29":"# Content:\nThe datasets contains transactions made by credit cards in September 2013 by european cardholders.\nThis dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, \u2026 V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.","22c6aac0":"### Observations:\n- Using Inter Quartile Range method for outlier removal, we are losing as much as 18.50% of our under sampled data (Fraudulent Transaction)\n- This will further increase class imbalance\n- Now we will use maximum Transaction Amount as our threshold to remove outlier values, thus the under sampled class remains unaffected","dc7b4270":"### Distribution of Fraud and Non-Fraud transactions","aa1a4fff":"### Looking at predicted probability","4577e3cf":"## Fixing Imbalance Class Applying Sampling Techinique => Running Algorithm => Comparing Performance","175815fc":"### Checking Outlier for Transaction Amount","5e097fba":"### Log Transfromation of Amount","b4eac98b":"### Outliar Handling","43a716d0":"### Importing Libraries","b7e55e20":"### Observations:\n- Time column represents the time gap between the first and any other transaction in second\n- So, minimum value represents the initial transaction and maximum value represent the last transaction recorded in the dataset\n- By converting the maximum value in hour term, we get the duration for which the data has been collected\n- Here we can see last transaction happened 48 hr(approx.) after the first transaction, so we have two days data in hand\n- Now we will try to find out the distribution of transaction happened in this two day","70a88a81":"### Data Description","c4a563d0":"### Confusion Matrix","d0ada840":"### F1 Score, harmonic mean of precision (PRE) and recall (REC)","f4c94966":"### Looing at predicted probability","10dfd5c5":"# ADASYN","1283a63a":"### Data Overview","605cbd68":"### ROC AUC Curve","be614459":"### Observations:\n- The data set contains total 31 rows and 284807 columns\n- Due to confidentiality, 28 out of 31features (V1 TO V28) are given based on PCA, this column may contain sensitive information specific to the individual customer. So, this are already scaled features, we have nothing much to do about this.\n\n- There are 3 non PCA column, we can perform EDA on them for a better understanding of the data.\n\n-> Time - This feature contains values in second, indicates time gap form the  first transaction and other recorded transactions in the data set.\n\n-> Amount - This feature contains transaction value for each transaction.\n\n-> Class - This feature contains level 0 and 1 where\n        \n        1 - Fraudulent transactions and\n        0 - Non fraudulent transactions\n--------\n\n*** PCA - Is a dimensionality reduction technique to transform the original data into same or fewer number of uncorrelated components by identifying eigenvalues and eigenvectors.\n\n-------","cd322acd":"### Accuracy Summary Report","04ac1603":"### Accuracy Score","7f7836dc":"### Comparing and figuring out better scalling method","156c68b9":"### Observations:\n- Only 0.2% records are fraudulent transaction and other 99.8% are Non fraud transaction. So, it is clearly observed that fraudulent transaction in our record is far less than the non-fraudulent transaction.\n- This is a class imbalance problem and as a result of this the prediction accuracy of the machine learning algorithm may get compromised, so we have to find a way to balance both the classes before running the algorithm","f2eec205":"### Creating Independent Veriables","b91b96b9":"### Looking at predicted probability","5b1a5882":"### ROC AUC Curve","7e960629":"### Precision Score","b7feabe8":"### Observations:\n- As our goal is to predict Fraudulent traction correctly. So, our focus area is to check the accuracy percentage of fraudulent class prediction\n- Using SMOTE Sampling technique to handle imbalance class problem, out of 85393 fraudulent transaction, 78223 transaction has been predicted correctly\n- So, the model has predicted our target outcome with 91.60% accuracy","3902ffb0":"### KNeighborsClassifier on SMOTE sampled Data","9712c4c0":"### Confusion Matrix","83c3bb72":"### Note:\n- V1 to V28 PCA, so already in scaled form\n- Distribution of Amount feature is highly positively skewed, so we will scale this feature using different method and compare the prediction of the machine learning algorithm for better accuracy for different scaling","b0562824":"### KNeighborsClassifier on over sampled Data","e663353f":"# Observations from all Trails:\nSo, from the above trail, we can observe that training algorithms(Logistic Regression, Decision Tree, Random Forest, KNeighborsClassifier) on oversampled data predict test data with better accuracy.\n\nNow, We will do Hyperparameter Tuning of all the algorithm using Over Sampled data and check performance on test data","4deb7c39":"# Hyperparameter Tuning(XGBoost) with Over Sampled Data Set","48617743":"### Importing Data","94dfab1c":"### Standar Scaller of Amount","b4bbe117":"### Spliting Data","23b1c106":"### Precision Score"}}