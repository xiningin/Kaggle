{"cell_type":{"ad6ed929":"code","7f00aca0":"code","d488c4f0":"code","5d73a6cf":"code","da5a317d":"code","0778bf94":"code","fae933c1":"code","11c83714":"code","531efe8a":"code","c155c8b6":"code","e844166b":"code","1c9e718e":"code","e1a19111":"code","803056c8":"code","adb4ba9e":"code","b8cde27b":"code","e5be0302":"code","05ec9c0a":"code","edee33a2":"code","92aa5640":"code","8be97d4a":"code","fbcb2ed8":"code","740d0efa":"code","614d6c4f":"code","a9e97881":"code","ec5ab73d":"code","c2eb5e34":"code","ce53ce64":"code","d415f442":"code","cfae7bd0":"code","7f462d57":"code","13c91913":"code","c642ab73":"code","fe1e5009":"code","7440b180":"code","f45f661c":"code","3dcd85e5":"code","c6341ac8":"code","bd999350":"code","7c521899":"code","f10a8b3a":"code","cf211a41":"code","f4d0b7c0":"code","cd9d8629":"code","ef33f5ae":"code","0d00c783":"code","be1ec1f8":"code","5a63a20f":"code","713742ef":"code","ceb178e6":"code","797cb33a":"code","51042563":"code","9dd97869":"code","961a7ca3":"code","c0fa28ae":"code","b50f8648":"code","3253430e":"code","74410e02":"code","f3d17513":"code","29a428dc":"code","a3de2dd4":"code","490c365f":"code","6a4dd3f2":"code","0581c38b":"code","dd18aee5":"code","0cb0f213":"code","d0f38bb1":"code","9f4132ba":"code","ab721cfa":"code","bf99c7ef":"code","fe0c12c2":"code","2e06f05e":"code","69461785":"code","973f2d12":"code","7d3a2db8":"code","8065bd80":"code","62748d1f":"code","867a1dd7":"code","12094548":"code","dd2fcaea":"code","925f2005":"code","c73a6a99":"code","02e05d47":"code","cdf99004":"code","612bc4ba":"code","16f2d5ca":"code","52a21ae3":"code","450309af":"code","2588efd4":"code","858b09d5":"code","74c1a8eb":"code","1d5e1915":"code","cd6faec8":"code","5956e161":"code","0775c72b":"code","d2996951":"code","0c6ee84d":"code","55a4a430":"code","687748e7":"code","18fa0abb":"code","e584e739":"code","1ca3bbb0":"code","a2745b53":"code","a1970430":"code","c1d67f48":"markdown","940b4413":"markdown","73439ef9":"markdown","8f5496da":"markdown","5fba1d56":"markdown","0a84d7af":"markdown","b8b1185d":"markdown","f9fa9770":"markdown","8ffa432d":"markdown","345ab3ba":"markdown","abfbe136":"markdown","0c11d9d6":"markdown","e44c2a8f":"markdown","3efccad2":"markdown","25dfba20":"markdown"},"source":{"ad6ed929":"# import pandas as pd\n# import warnings\n# warnings.filterwarnings(\"ignore\")\n# from glob import glob\n# from tqdm.notebook import tqdm\n# import matplotlib\n# matplotlib.rcParams.update({'font.size': 22})\n# from sklearn.metrics import accuracy_score\n# from tensorflow.keras import layers\n# from tensorflow.keras.applications import ResNet50, DenseNet121, Xception\n# from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, GlobalAveragePooling2D\n# from tensorflow.keras.optimizers import Adam\n# from tensorflow.keras import models\n# from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n# import tensorflow.keras.backend as K\n# from tensorflow.math import confusion_matrix","7f00aca0":"import pandas as pd\n","d488c4f0":"image_df = pd.read_csv('..\/input\/siim-covid19-detection\/train_image_level.csv')\ndisplay(image_df.head(3))\nprint(image_df.shape)","5d73a6cf":"study_df = pd.read_csv('..\/input\/siim-covid19-detection\/train_study_level.csv')\ndisplay(study_df.head(3))\nprint(study_df.shape)","da5a317d":"# df_sampleSub = pd.read_csv('..\/input\/siim-covid19-detection\/sample_submission.csv')\n# display(df_sampleSub.head(3))\n# print(df_sampleSub.shape)","0778bf94":"study_df['id'] = study_df['id'].str.replace('_study',\"\")\nstudy_df['StudyInstanceUID'] = study_df['id']\nstudy_df.head(3)","fae933c1":"def get_absolute_file_paths(directory):\n    all_abs_file_paths = []\n    for dirpath,_,filenames in os.walk(directory):\n        for f in filenames:\n            all_abs_file_paths.append(os.path.abspath(os.path.join(dirpath, f)))\n    return all_abs_file_paths","11c83714":"from tqdm.notebook import tqdm; tqdm.pandas();\n","531efe8a":"import os\n","c155c8b6":"study_df[\"study_dir\"] = \"\/kaggle\/input\/siim-covid19-detection\/train\/\"+study_df[\"id\"]\nstudy_df[\"images_per_study\"] = study_df.study_dir.progress_apply(lambda x: len(get_absolute_file_paths(x)))","e844166b":"study_df.head()","1c9e718e":"study_df.images_per_study.value_counts()","e1a19111":"unique_studies = study_df[study_df.images_per_study == 1]","803056c8":"unique_studies.head()","adb4ba9e":"unique_studies.images_per_study.value_counts()","b8cde27b":"unique_studies_ids = list(unique_studies.id)","e5be0302":"unique_studies_ids[:10]","05ec9c0a":"len(unique_studies_ids)","edee33a2":"image_df.head()","92aa5640":"image_df_unique = image_df[image_df.StudyInstanceUID.isin(unique_studies_ids)]","8be97d4a":"image_df_unique.shape","fbcb2ed8":"image_df.shape","740d0efa":"df_train = image_df_unique.merge(study_df, \n                                 on='StudyInstanceUID',\n                                 suffixes=('_image', '_study'))\ndf_train.head(3)","614d6c4f":"df_train.columns","a9e97881":"# train_dir_jpg = '..\/input\/covid-jpg-512\/train'\n# train_dir_origin ='..\/input\/siim-covid19-detection\/train'\n# paths_original = []\n# paths_jpg = []\n# for _, row in tqdm(df_train.iterrows()):\n#     image_id = row['id'].split('_')[0]\n#     study_id = row['StudyInstanceUID']\n#     image_path_jpg = glob(f'{train_dir_jpg}\/{image_id}.jpg')\n#     image_path_original = glob(f'{train_dir_origin}\/{study_id}\/*\/{image_id}.dcm')\n#     paths_jpg.append(image_path_jpg)\n#     paths_original.append(image_path_original)","ec5ab73d":"# df_train['path'] = paths_jpg\n# df_train['origin'] = paths_original\n# df_train.head(3)","c2eb5e34":"df_train.loc[df_train['Negative for Pneumonia']==1, 'study_label'] = 'negative'\ndf_train.loc[df_train['Typical Appearance']==1, 'study_label'] = 'typical'\ndf_train.loc[df_train['Indeterminate Appearance']==1, 'study_label'] = 'indeterminate'\ndf_train.loc[df_train['Atypical Appearance']==1, 'study_label'] = 'atypical'\ndf_train.drop(['Negative for Pneumonia','Typical Appearance', 'Indeterminate Appearance', 'Atypical Appearance'], axis=1, inplace=True)\n","ce53ce64":"df_train['id_image'] = df_train['id_image'].str.replace('_image', '.jpg')\ndf_train['image_label'] = df_train['label'].str.split().apply(lambda x : x[0])\ndf_train.head(3)","d415f442":"df_size = pd.read_csv('..\/input\/covid-jpg-512\/size.csv')\ndf_size['id_image'] = df_size['id']\ndf_size.head(3)","cfae7bd0":"df_train = df_train.merge(df_size, on='id_image')\ndf_train.head(3)","7f462d57":"df_train.shape","13c91913":"df_train.study_label.value_counts()","c642ab73":"df_train.image_label.value_counts()","fe1e5009":"# df_train.id_image.value_counts()","7440b180":"# df_train.id_study.value_counts()","f45f661c":"# df_train.describe()","3dcd85e5":"df_train_atypical = df_train[df_train.study_label == 'atypical']","c6341ac8":"df_train_atypical.shape","bd999350":"df_train_augmented_list = [df_train, df_train_atypical]","7c521899":"df_train_augmented = pd.concat(df_train_augmented_list, ignore_index=True)","f10a8b3a":"df_train_augmented.shape","cf211a41":"df_train_augmented.study_label.value_counts()","f4d0b7c0":"df_train_augmented.image_label.value_counts()","cd9d8629":"import matplotlib.pyplot as plt\nimport cv2\nimport os\nfrom ast import literal_eval\n","ef33f5ae":"from matplotlib.patches import Rectangle\n","0d00c783":"df_train['id'][0]","be1ec1f8":"# ! ls ..\/input\/siimcovid19-1024-jpg-image-dataset\/train","5a63a20f":"n = 20\ntrain_dir = '..\/input\/covid-jpg-512\/train'\n# train_dir = '..\/input\/siimcovid19-1024-jpg-image-dataset\/train'\n\n\nfig, axs = plt.subplots(4, 5, figsize=(20,20))\nfig.subplots_adjust(hspace=.2, wspace=.2)\naxs = axs.ravel()\nfor i in range(n):\n#     print('----------')\n    image_id = df_train['id'][i]\n#     print('ix, image_id=', i, image_id)\n\n    img = cv2.imread(os.path.join(train_dir, image_id))\n    axs[i].imshow(img)\n    \n    study_label = df_train['study_label'][i]\n    image_label = df_train['image_label'][i]\n#     print('study_label=', study_label)\n#     print('image_label=', image_label)\n    \n    if type(df_train['boxes'][i])==str:\n#         print('box seen for i=', i)\n        boxes = literal_eval(df_train['boxes'][i])\n        \n        for box in boxes:\n#             print('box=', box)\n            axs[i].add_patch(Rectangle((box['x']*(512\/df_train['dim1'][i]), box['y']*(512\/df_train['dim0'][i])), box['width']*(512\/df_train['dim1'][i]), box['height']*(512\/df_train['dim0'][i]), fill=0, color='y', linewidth=2))\n        axs[i].set_title(str(i) + ',' + study_label + ',' + image_label)\n            \n    else:\n#         print('box NOT seen for i=', i)\n        axs[i].set_title(str(i) + ',' + study_label + ',' + image_label)\n","713742ef":"from skimage import exposure\nimport numpy as np\n","ceb178e6":"# def preprocess_image(img):\n#     equ_img = exposure.equalize_hist(img)\n#     return equ_img\n\n# im= cv2.imread('..\/input\/covid-jpg-512\/train\/007cf31356c6.jpg')\n# im2 = preprocess_image(im)\n# res = np.concatenate((im\/255, im2), axis=1)\n# plt.imshow(res)\n# plt.show()","797cb33a":"img.shape","51042563":"# img_size = 1024\nimg_size = 512\n# img_size = 299\n","9dd97869":"# batch_size = 32\nbatch_size = 16\n","961a7ca3":"from keras.preprocessing.image import ImageDataGenerator\n","c0fa28ae":"image_generator = ImageDataGenerator(\n#         rescale = 1.\/255,\n        validation_split=0.25,\n        rotation_range=5,\n#         width_shift_range=0.1,\n#         height_shift_range=0.1,\n#         shear_range=0.1,\n        zoom_range=0.1,\n        horizontal_flip=True,\n        fill_mode='nearest',\n        brightness_range = [0.8, 1.1],\n)\n\nimage_generator_valid = ImageDataGenerator(validation_split=0.25,\n#                                            rescale = 1.\/255,  \n                                          )\n","b50f8648":"df_train = df_train_augmented","3253430e":"df_train.head()","74410e02":"train_dir","f3d17513":"train_generator = image_generator.flow_from_dataframe(\n        dataframe = df_train,\n        directory = train_dir,\n        x_col = 'id',\n        y_col =  'study_label',  \n        target_size=(img_size, img_size),\n        batch_size=batch_size,\n#         class_mode='binary',\n        subset='training', \n        seed = 23) \n\nvalid_generator = image_generator_valid.flow_from_dataframe(\n    dataframe = df_train,\n    directory = train_dir,\n    x_col = 'id',\n    y_col = 'study_label',\n    target_size=(img_size, img_size),\n    batch_size=batch_size,\n#     class_mode='binary',\n    subset='validation', \n    shuffle=False, \n    seed=23) \n","29a428dc":"# for j in range(2):\n#     aug_images = [train_generator[0][0][j] for i in range(5)]\n#     fig, axes = plt.subplots(1, 5, figsize=(24,24))\n#     axes = axes.flatten()\n#     for img, ax in zip(aug_images, axes):\n#         ax.imshow(img)\n#         ax.axis('off')\n# plt.tight_layout()\n# plt.show()","a3de2dd4":"# for j in range(2):\n#     aug_images = [valid_generator[0][0][j] for i in range(5)]\n#     fig, axes = plt.subplots(1, 5, figsize=(24,24))\n#     axes = axes.flatten()\n#     for img, ax in zip(aug_images, axes):\n#         ax.imshow(img)\n#         ax.axis('off')\n# plt.tight_layout()\n# plt.show()","490c365f":"import tensorflow as tf\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom tensorflow.keras.utils import plot_model\n\n","6a4dd3f2":"from tensorflow.keras.models import Model, Sequential\n","0581c38b":"from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\nfrom tensorflow.keras.layers import GlobalAveragePooling2D,  BatchNormalization, Activation\n# from tensorflow.keras import models\n","dd18aee5":"from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n","0cb0f213":"img_size","d0f38bb1":"from tensorflow.keras.applications.vgg16 import  VGG16\nfrom tensorflow.keras.applications import DenseNet121\n","9f4132ba":"def build_chextnet_model_v1():\n    \"\"\"\n    v1 - uses densenet + chextnet weights\n    \"\"\"\n    # load model design\n    pre_model = DenseNet121(weights=None,\n                        include_top=False,\n                        input_shape=(img_size,img_size,3)\n                        )\n    out = Dense(14, activation='sigmoid')(pre_model.output)\n    pre_model = Model(inputs=pre_model.input, outputs=out) \n    \n    # load model wieghts\n    chex_weights_path = '..\/input\/chexnet-weights\/brucechou1983_CheXNet_Keras_0.3.0_weights.h5'\n    pre_model.load_weights(chex_weights_path)\n\n    # make layers trainable?\n    # pre_model.trainable = False\n    pre_model.trainable = True\n\n    # get summary\/print\n    pre_model.summary()\n    \n    \n    # add future layers.\n    # last_layer = pre_model.get_layer('conv5_block16_concat')\n    last_layer = pre_model.layers[-2]\n\n    print('last layer output shape: ', last_layer.output_shape)\n    last_output = last_layer.output\n#     last_layer\n\n    # Flatten the output layer to 1 dimension\n    # x = Flatten()(last_output)\n    x = GlobalAveragePooling2D()(last_output)\n\n    # Add a fully connected layer with 512 hidden units and ReLU activation\n    # x = Dense(512, activation='relu')(x)\n    # Add a dropout rate of 0.2\n    # x = Dropout(0.2)(x)                  \n\n\n    # # Add a fully connected layer with 128 hidden units and ReLU activation\n    # x = Dense(128, activation='relu')(x)\n\n\n    # Add final classification layer\n    x = Dense(4, activation='softmax')(x)\n\n    # final model\n    model = Model( pre_model.input, x) \n\n    # model.summary()\n    # plot_model(model)\n\n    return model\n","ab721cfa":"def build_vanilla_cnn_model_v1():\n    in1 = tf.keras.layers.Input(shape=(img_size, img_size, 3))\n    \n#     out1 = tf.keras.layers.Conv2D(4,(3,3),activation=\"relu\")(in1)\n    out1 = tf.keras.layers.Conv2D(32,(3,3),\n                                  activation=\"relu\",\n                                  padding='same')(in1)\n    out1 = tf.keras.layers.MaxPooling2D((2,2))(out1)\n    \n    out1 = tf.keras.layers.Conv2D(32,(3,3),\n                                  activation=\"relu\",\n                                  padding='same')(out1)\n    out1 = tf.keras.layers.MaxPooling2D((2,2))(out1)\n\n    out1 = tf.keras.layers.Flatten()(out1)\n    \n    out2 = tf.keras.layers.Dense(30,activation=\"relu\")(out1)\n    out2 = tf.keras.layers.Dense(30,activation=\"relu\")(out2)\n    \n    \n    out2 = Dense(4, \n                 activation='softmax',\n                 name='class_out',\n                 kernel_regularizer=regularizers.l2(0.01))(out2)\n\n\n    model = tf.keras.Model(inputs=in1,\n                           outputs=out2)\n\n    model.summary()\n    return model\n","bf99c7ef":"def build_densenet_coursera_model_v1():\n    \"\"\"\n    v1 - uses densenet + coursera model\n    https:\/\/www.kaggle.com\/rbhambri\/densenet-weights-nih-coursera-ai4m\n    \"\"\"\n    # load model design\n    # also load model wieghts\n    nih_weights_path = '..\/input\/densenet-weights-nih-coursera-ai4m\/densenet.hdf5'\n    base_model = DenseNet121(weights=nih_weights_path, \n                             input_shape=(img_size,img_size,3),\n                             include_top=False)\n\n    # get summary\/print\n#     base_model.summary()\n\n    last_output = base_model.output\n#     last_layer\n\n    # Flatten the output layer to 1 dimension\n    # x = Flatten()(last_output)\n    x = GlobalAveragePooling2D()(last_output)\n\n    # Add final classification layer\n    x = Dense(4, activation='softmax')(x)\n\n    # final model\n    model = Model(base_model.input, x) \n\n    model.summary()\n    plot_model(model)\n\n    return model\n","fe0c12c2":"def build_vgg16_model_v1():\n    \"\"\"\n    v1 - https:\/\/www.kaggle.com\/saeedniksaz\/transferlearning-with-vgg16\n    \"\"\"\n    base_model = VGG16(input_shape=(img_size,img_size,3), \n                         include_top=False,\n                         weights=\"imagenet\")\n    # get summary\/print\n#     base_model.summary()\n\n    for layer in base_model.layers:\n        layer.trainable = False\n        \n    model = Sequential()\n    model.add(base_model)\n    model.add(Dropout(0.5))\n    model.add(Flatten())\n    model.add(BatchNormalization())\n    \n    model.add(Dense(256,kernel_initializer='he_uniform'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(32,kernel_initializer='he_uniform'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n\n    model.add(Dense(4,activation='softmax'))\n        \n    model.summary()\n    plot_model(model)\n\n    return model\n","2e06f05e":"# model = build_vanilla_cnn_model_v1()\nmodel = build_chextnet_model_v1()\n# model = build_densenet_coursera_model_v1()\n# model = build_vgg16_model_v1()\n","69461785":"# metrics = ['categorical_accuracy', 'accuracy']\n# metrics = [tf.keras.metrics.AUC(), 'accuracy']\nmetrics = [ 'accuracy', tf.keras.metrics.AUC()]\n\n","973f2d12":"model.compile(Adam(lr=1e-3),\n              loss='categorical_crossentropy',\n              metrics=metrics)","7d3a2db8":"model","8065bd80":"# rlr = ReduceLROnPlateau(monitor = 'val_accuracy', \n#                         factor = 0.2, \n#                         patience = 2, \n#                         verbose = 1, \n#                         min_delta = 1e-4, \n#                         min_lr = 1e-4, \n#                         mode = 'max')\n\nrlr = ReduceLROnPlateau(monitor = 'val_loss', \n                        factor = 0.1, \n                        patience = 2, \n                        verbose = 1, \n                        min_delta = 1e-4, \n                        min_lr = 1e-6, \n                        mode = 'min')\n","62748d1f":"# es = EarlyStopping(monitor = 'val_accuracy', \n#                    min_delta = 1e-4, \n#                    patience = 3, \n#                    mode = 'max', \n#                    restore_best_weights = True, \n#                    verbose = 1)\n\nes = EarlyStopping(monitor = 'val_loss', \n                   min_delta = 1e-4, \n                   patience = 3, \n                   mode = 'min', \n                   restore_best_weights = True, \n                   verbose = 1)\n","867a1dd7":"# model_name = 'vgg16_model_512_july11.h5'\nmodel_name = 'chextnet_model_512_july11.h5'\n","12094548":"# ckp = ModelCheckpoint('model.h5',\n#                       monitor = 'val_accuracy',\n#                       verbose = 0, \n#                       save_best_only = True, \n#                       mode = 'max')\n\n\nckp = ModelCheckpoint(model_name,\n                      monitor = 'val_loss',\n                      verbose = 0, \n                      save_best_only = True, \n                      mode = 'min')\n","dd2fcaea":"# class_weight = {0: 2,1: 1, 2: 1,3: 1}","925f2005":"epochs = 10\n# epochs = 5\n# epochs = 2\n","c73a6a99":"history = model.fit(\n      train_generator,\n      validation_data=valid_generator,\n      epochs=epochs,\n      callbacks=[es, ckp, rlr],\n#     class_weight=class_weight\n)","02e05d47":"# abc","cdf99004":"# ! ls ..\/input\/image-clf-chexnet-vanilla-cnn\n! ls","612bc4ba":"# model.load_weights('..\/input\/image-clf-chexnet-vanilla-cnn\/model.h5')\n# model.load_weights('.\/chextnet_model_512_july11.h5')\n","16f2d5ca":"%matplotlib inline\nimport matplotlib.pyplot as plt\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend(loc=0)\nplt.figure()\n\n\nplt.show()","52a21ae3":"# from tensorflow.math import confusion_matrix\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report","450309af":"actual =  valid_generator.labels\npreds = np.argmax(model.predict(valid_generator), axis=1)\ncfmx = confusion_matrix(actual, preds)\nacc = accuracy_score(actual, preds)","2588efd4":"model.evaluate(valid_generator)","858b09d5":"actual[:25]","74c1a8eb":"preds[:25]","1d5e1915":"cfmx","cd6faec8":"acc","5956e161":"print(classification_report(actual, preds))\n","0775c72b":"df_train.study_label.value_counts()","d2996951":"valid_generator.class_indices","0c6ee84d":"c = model.predict(valid_generator)","55a4a430":"c.shape","687748e7":"# c[0]","18fa0abb":"ix = 3","e584e739":"actual[ix]","1ca3bbb0":"op = c[ix]","a2745b53":"op","a1970430":"for ix, y_pred in enumerate(list(op)):\n    if y_pred > 0.5:\n#         print(ix, y_pred, classes[ix])        \n        print(ix, y_pred)","c1d67f48":"# PreProcessing","940b4413":"### for simplicity keeping studies with only 1 img per study - right now","73439ef9":"### TODO\n\ncheck old kaggle competition on chest xray - done - doing better than this\n\nadd metrics - https:\/\/keras.io\/api\/metrics\/classification_metrics\/ - done.\n\nadd weighted loss function \/ class weights in model\/ balance train df - done with oversampling\n\ncheck coursera assignments\n\ntry better models.\n\ngradcam?\n","8f5496da":"# Data manipulations","5fba1d56":"### now use this df for further processing","0a84d7af":"### data is imbalanced.\n\noversample the minority class.","b8b1185d":"### this work is inspired by:\n\nhttps:\/\/www.kaggle.com\/rbhambri\/covid-detection-studies-eda-viz\n\nhttps:\/\/www.kaggle.com\/rbhambri\/chexnet-transfer-learning-binary-image-clf\n    \nhttps:\/\/www.kaggle.com\/sinamhd9\/classification-model\n\nhttps:\/\/www.kaggle.com\/rbhambri\/chest-x-ray-abnormalities-bams-keras-pipeline\n\nhttps:\/\/www.kaggle.com\/rbhambri\/densenet-weights-nih-coursera-ai4m\n\nhttps:\/\/www.coursera.org\/specializations\/ai-for-medicine\n\nhttps:\/\/www.kaggle.com\/rbhambri\/chest-x-ray-abnormalities-densenet-pipeline\n\nhttps:\/\/www.kaggle.com\/c\/siim-covid19-detection\/discussion\/242606\n\nhttps:\/\/www.kaggle.com\/saeedniksaz\/transferlearning-with-vgg16\n\n","f9fa9770":"# Architecture","8ffa432d":"# Visualization","345ab3ba":"### only keep images for these studies","abfbe136":"# ImageGenerators and Augmentations","0c11d9d6":"### now merge","e44c2a8f":"### model perf","3efccad2":"# Imports","25dfba20":"### address class imbalance\n\nnow try focal loss? https:\/\/towardsdatascience.com\/a-loss-function-suitable-for-class-imbalanced-data-focal-loss-af1702d75d75\n\nOR specify in model.fit\n\nhttps:\/\/stackoverflow.com\/questions\/44716150\/how-can-i-assign-a-class-weight-in-keras-in-a-simple-way\/44721883\n\nhttps:\/\/github.com\/keras-team\/keras\/issues\/1875\n"}}