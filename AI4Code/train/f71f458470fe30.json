{"cell_type":{"4213d808":"code","ca97a2f5":"code","993e46e5":"code","767f4d82":"code","2bcf9ab8":"code","9a80a926":"code","64c68396":"code","68aabcd2":"code","a63da94c":"code","40b2fc67":"code","7b538e35":"code","180e964f":"code","d6f05e9b":"code","cc095a77":"code","f6798448":"code","c9deed11":"code","4686f48f":"code","6a4638c7":"code","59a3ee5b":"code","a1988bc8":"code","02fffe1d":"code","514f7eef":"code","fd3b89ab":"code","36d8928c":"code","037802eb":"code","4b387df5":"code","6a28ad96":"markdown","3d7b56ca":"markdown","523a17fb":"markdown","d0d86918":"markdown","226a3ed8":"markdown","2e991564":"markdown"},"source":{"4213d808":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ca97a2f5":"df = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv')\ndf.head()","993e46e5":"from sklearn.model_selection import train_test_split\n","767f4d82":"X = df\ny = df['target']","2bcf9ab8":"X_train, X_test, y_train, y_test = train_test_split(\n     X, y, test_size=0.33, random_state=42)","9a80a926":"from sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn import metrics\nfrom catboost import CatBoostRegressor","64c68396":"# based on eta we found experience above 10 was a good indicator\ndef experience_processor(X):\n    experience = X['experience']\n    experience = np.where(\n        experience.str.contains('>20'),\n        21,\n        np.where(\n            experience.str.contains('<1'),\n            0,\n            experience\n        )\n    ).astype('int')\n    X['experience'] = np.where(\n        experience < 10,\n        0,\n        1\n    )\n    return X","68aabcd2":"# build new column that is experience is null but has lots of experience\ndef experience_in_unknown(X):\n    X = X.copy()\n    experience = X['experience']\n    company_size = X['company_size']\n    X['experience_unknown'] = np.where(\n        (experience == 1) & (company_size == 'missing_value'),\n        1,\n        0\n    )\n    return X\n","a63da94c":"from sklearn.base import BaseEstimator, TransformerMixin\n\n\nclass BaseFeatureEngineer(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, columns = []):\n        self.columns = columns\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        X = X.copy()\n        return X","40b2fc67":"# Transformations for the numeric features\nclass NumericImputer(BaseFeatureEngineer):\n    \n    def fit(self, X, y=None):\n        self.means = { col: X[col].mean() for col in self.columns}\n        return self\n        \n    def transform(self, X):\n        X = X.copy()\n        for col in self.columns:\n            X[col] = X[col].fillna(self.means[col])\n        return X\n\nclass NumericScaler(BaseFeatureEngineer):       \n    \n    def fit(self, X, y=None):\n        self.scaler = StandardScaler()\n        self.scaler.fit(X[self.columns])\n        return self\n        \n    def transform(self, X):\n        X = X.copy()\n        X[self.columns] = self.scaler.transform(X[self.columns])\n        return X    \n\nnumeric_features = ['city_development_index', 'training_hours']\n    \n\nnumeric_preprocessing = Pipeline([\n    ('imputer', NumericImputer(numeric_features)),\n    ('scaler', NumericScaler(numeric_features))\n])","7b538e35":"# Transformations for categorical data\nclass CategoricalImputer(BaseFeatureEngineer):\n        \n    def transform(self, X):\n        X = X.copy()\n        for col in self.columns:\n            X[col] = X[col].fillna('missing_value')\n        return X\n\ncategorical_features= [\n    'gender', 'relevent_experience', 'enrolled_university',\n    'education_level', 'major_discipline',\n    'company_size', 'company_type', 'last_new_job',\n]\n    \n\ncategorical_preprocessing = Pipeline([\n    ('imputer',CategoricalImputer([categorical_features]))\n])","180e964f":"class FeatureEngineering(BaseFeatureEngineer):\n    \n    def transform(self, X):\n        X = X.copy()\n        X = experience_processor(X)\n        X = experience_in_unknown(X)\n        return X\n\ncreate_new_features = Pipeline([\n    ('feature_engineering', FeatureEngineering())\n])","d6f05e9b":"class SelectColumns(BaseFeatureEngineer):\n    def transform(self, X):\n        X = X.copy()\n        return X[self.columns]","cc095a77":"preprocessor = Pipeline([\n    ('numeric_preprocessing', numeric_preprocessing),\n    ('categorical_preprocessing', categorical_preprocessing),\n    ('create_new_features', create_new_features),\n    ('select_colums', SelectColumns(numeric_features + categorical_features + ['experience_unknown'])),\n    ('one_hot_encoding', ColumnTransformer([\n        ('one_hot_encoding', OneHotEncoder(\n            handle_unknown='error', drop='first', sparse=False),\n             categorical_features \n        )\n    ], remainder='passthrough'))\n])","f6798448":"pipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('xgb', XGBClassifier(random_state=42)),\n])\n","c9deed11":"pipeline.fit(X_train, y_train)","4686f48f":"pipeline.predict(X_test)","6a4638c7":"pipeline.predict_proba(X_test)[:,1]","59a3ee5b":"test_prediction = pipeline.predict_proba(X_test)[:,1]\n# test_prediction = np.where(test_prediction < 0.5, 0,1)\nscore = roc_auc_score(y_test, test_prediction)\n\nprint(f'Area under ROC Score of Random Forest Model On Test Set - {score:,.2%}')\n","a1988bc8":"test_prediction","02fffe1d":"parameters = {\n    'xgb__n_estimators': [10*x for x in range(4,10)],\n    'xgb__max_depth': [i for i in range(1,6)]\n}\ngrid = GridSearchCV(pipeline, param_grid=parameters, cv=5, scoring='roc_auc')\n","514f7eef":"grid.fit(X_train, y_train)","fd3b89ab":"print(f'score = {grid.score(X_test,y_test):0,.2%}')\nprint(f'Best parameters: {grid.best_params_}')","36d8928c":"grid_search_roc_score = roc_auc_score(y_test, grid.predict_proba(X_test)[:,1])\n\nprint(f'Area under ROC Score of XGBClassifier On Test Set - {grid_search_roc_score:,.2%}')\n","037802eb":"test = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv')\ntest['target'] = grid.predict_proba(test)[:,1]\n\ntest[['enrollee_id', 'target']]","4b387df5":"test[['enrollee_id', 'target']].to_csv('submit.csv',index= False)","6a28ad96":"# Prepare Data","3d7b56ca":"# Preparing a test submission","523a17fb":"# Using a Pipeline and a gridsearch to get the most out of a random forest classifier.","d0d86918":"# Data Science Job Change Predictions\n\nThis notebook is building on the EDA carried out in [here](https:\/\/www.kaggle.com\/stuartday274\/job-change-data-exploration-and-predictions).\n\nThis is my first attempt at creating a full pipeline that carries out the data prep, feature engineering and learning. In order to keep track of the column names i have created my own transformations rather than use the sklearn built in ones. Would be interested in feedback on better ways to have done this.","226a3ed8":"# Preparing Train Test Data","2e991564":"# Tuning Parameters with a GridSearch"}}