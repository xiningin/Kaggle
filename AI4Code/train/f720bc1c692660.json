{"cell_type":{"1ba7314e":"code","fb9d43b7":"code","7af889f2":"code","edcac7d4":"code","1b42ea48":"code","2b1c92a6":"code","08c32b84":"code","9a99b86f":"code","64204ff6":"code","32d2dca9":"code","8416aa4a":"code","ec665adc":"code","408d16b9":"code","04c2fb77":"code","42b2cbb6":"code","3751ac13":"code","34e9e1fa":"code","485219a4":"code","141942c5":"code","f6bbfd19":"code","89e39fa1":"code","c9cdee39":"code","99bc5928":"code","83052716":"markdown","60280bd8":"markdown","23da4863":"markdown","92ff11c1":"markdown","25b9c79f":"markdown","950209da":"markdown","c4844939":"markdown","f87029be":"markdown","c5b508a8":"markdown","d1da488b":"markdown"},"source":{"1ba7314e":"# import libraries\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","fb9d43b7":"#Follow Kaggle's way to load datasets.\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ntrain_data = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\nquiz_data = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")","7af889f2":"# X= 42,000 images of 28*28=784, Y= Correct labels \nX, T =train_data.iloc[0:,1:],train_data.iloc[0:,[0]]\nX=X.to_numpy()\nT=T.to_numpy()\n# Split data into training data and test data \nx_train, x_test, t_train, t_test = train_test_split(X, T, test_size=0.2)\nx_train = x_train.reshape(-1,1,28,28)\nx_test = x_test.reshape(-1,1,28,28)\n# Apply one-hot-vector to Taget data and change them into int type\nt_train = np.eye(10)[t_train.astype(\"int\")].reshape(-1,10)\nt_test = np.eye(10)[t_test.astype(\"int\")].reshape(-1,10)\n# Change quiz data into Numpy array\nquiz_x = quiz_data.iloc[0:,0:].to_numpy()\nquiz_x = quiz_x.reshape(-1,1,28,28)","edcac7d4":"# Let's check sample images 28*28=784\n%matplotlib inline\nfor i in range(5):\n    plt.imshow(x_train[i][0],cmap='Greys')\n    plt.show()\n    print(\"label: \", t_train[i])","1b42ea48":"#This softmax function is applied an overflow countermajor.\ndef softmax(x):\n    if x.ndim == 2:\n        x = x.T\n        x = x - np.max(x, axis=0)   # x was trasposed on the previous line. That's why axis is 0.\n        y = np.exp(x) \/ np.sum(np.exp(x), axis=0)\n        return y.T \n    x = x - np.max(x) \n    return np.exp(x) \/ np.sum(np.exp(x))\n\ndef cross_entropy_error(y, t):\n    if y.ndim == 1:\n        t = t.reshape(1, t.size)\n        y = y.reshape(1, y.size)\n        \n    # argmax picked out the laragest array number in t. \n    if t.size == y.size:\n        t = t.argmax(axis=1)\n             \n    batch_size = y.shape[0]\n    delta = 1e-8\n    return -np.sum(np.log(y[np.arange(batch_size), t] + delta)) \/ batch_size\n","2b1c92a6":"#Definition of im2col, that is a useful image flatten transformation.\ndef im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n    # Num = number of data , Channel = channel \u3001Height = height of image, W = Width\n    Num, Channel, Height, Width = input_data.shape\n    out_h = (Height + 2*pad - filter_h)\/\/stride + 1\n    out_w = (Width + 2*pad - filter_w)\/\/stride + 1\n    \n    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)] , 'constant') \n    col = np.zeros((Num, Channel, filter_h, filter_w, out_h, out_w))\n\n    for y in range(filter_h):\n        y_max = y + stride*out_h\n        for x in range(filter_w):\n            x_max = x + stride*out_w\n            #\n            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n\n    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(Num*out_h*out_w, -1)\n    return col","08c32b84":"#Definition of col2im, that is a backward transaction of im2col\ndef col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0): \n    # Num = number of data , Channel = channel \u3001Height = height of image, W = Width\n    Num, Channel, Height, Width = input_shape\n    out_h = (Height + 2*pad - filter_h)\/\/stride + 1\n    out_w = (Width + 2*pad - filter_w)\/\/stride + 1\n    col = col.reshape(Num, out_h, out_w, Channel, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n\n    img = np.zeros((Num, Channel, Height + 2*pad + stride - 1, Width + 2*pad + stride - 1))\n    for y in range(filter_h):\n        y_max = y + stride*out_h\n        for x in range(filter_w):\n            x_max = x + stride*out_w\n            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n               \n\n    return img[:, :, pad:Height + pad, pad:Width + pad]","9a99b86f":"#Set Relu function\nclass Relu:\n    def __init__(self):\n        self.mask = None\n\n    def forward(self, x):\n        self.mask = (x <= 0)\n        out = x.copy()\n        out[self.mask] = 0\n\n        return out\n\n    def backward(self, dout):\n        dout[self.mask] = 0\n        dx = dout\n\n        return dx\n","64204ff6":"# Set Affine layer\nclass Affine:\n    def __init__(self, input_size, output_size):\n        self.W = 0.01 * np.random.randn(input_size, output_size)\n        self.b = np.zeros(output_size)\n        self.x = None\n        self.original_x_shape = None\n        self.dW = None\n        self.db = None\n\n    def forward(self, x):\n        self.original_x_shape = x.shape\n        x = x.reshape(x.shape[0], -1)\n        self.x = x\n        out = np.dot(self.x, self.W) + self.b\n        return out\n\n    def backward(self, dout):\n        dx = np.dot(dout, self.W.T)\n        self.dW = np.dot(self.x.T, dout)\n        self.db = np.sum(dout, axis=0)\n        \n        #Decode data from reshaped one to iput shape\n        dx = dx.reshape(*self.original_x_shape) \n        return dx\n","32d2dca9":"# Set Softmax with Loss Class\nclass SoftmaxWithLoss:\n    def __init__(self):\n        self.loss = None\n        self.y = None\n        self.t = None\n\n    def forward(self, x, t):\n        self.t = t\n        self.y = softmax(x)\n        self.loss = cross_entropy_error(self.y, self.t)\n        \n        return self.loss\n\n    def backward(self, dout=1):\n        batch_size = self.t.shape[0]\n        \n        #When Target data is one-hot-vector \n        if self.t.size == self.y.size:\n            dx = (self.y - self.t) \/ batch_size\n        else:\n            dx = self.y.copy()\n            dx[np.arange(batch_size), self.t] -= 1\n            dx = dx \/ batch_size\n        \n        return dx","8416aa4a":"#Set convolution class\nclass Convolution:\n    def __init__(self, input_channel, output_channel, kernel_h=5, kernel_w=5, stride=1, pad=0):\n        self.W = 0.01 * np.random.randn(output_channel, input_channel, kernel_h, kernel_w)\n        self.b = np.zeros(output_channel)\n        self.stride = stride\n        self.pad = pad\n        self.x = None   \n        self.col = None\n        self.col_W = None\n        self.dW = None\n        self.db = None\n\n    def forward(self, x):\n        #Num_filters = Number of filters, Channel= Number of Channels, F_height=filter height, F_width= filter width\n        Num_filters, Channel, F_height, F_width = self.W.shape\n        Num, Channel, Height, Width = x.shape\n        out_h = 1 + int((Height + 2*self.pad - F_height) \/ self.stride)\n        out_w = 1 + int((Width + 2*self.pad - F_width) \/ self.stride)\n        col = im2col(x, F_height, F_width, self.stride, self.pad)\n        col_W = self.W.reshape(Num_filters, -1).T\n        out = np.dot(col, col_W) + self.b\n        out = out.reshape(Num, out_h, out_w, -1).transpose(0, 3, 1, 2)\n        self.x = x\n        self.col = col\n        self.col_W = col_W\n        return out\n    \n    def backward(self, dout):\n        #Num_filters = Number of filters, Channel= Number of Channels, F_height=filter height, F_width= filter width\n        Num_filters, Channel, F_height, F_width = self.W.shape\n        dout = dout.transpose(0,2,3,1).reshape(-1, Num_filters)\n        self.db = np.sum(dout, axis=0)\n        self.dW = np.dot(self.col.T, dout)\n        self.dW = self.dW.transpose(1, 0).reshape(Num_filters, Channel, F_height, F_width)\n        dcol = np.dot(dout, self.col_W.T)\n        dx = col2im(dcol, self.x.shape, F_height, F_width, self.stride, self.pad)\n        return dx","ec665adc":"#Set Pooling Class\nclass Pooling:\n    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n        self.pool_h = pool_h\n        self.pool_w = pool_w\n        self.stride = stride\n        self.pad = pad\n        \n        self.x = None\n        self.arg_max = None\n\n    def forward(self, x):\n        Num, Channel, Height, Width = x.shape\n        out_h = int(1 + (Height - self.pool_h) \/ self.stride)\n        out_w = int(1 + (Width - self.pool_w) \/ self.stride)\n        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n        col = col.reshape(-1, self.pool_h*self.pool_w)\n        arg_max = np.argmax(col, axis=1)\n        out = np.max(col, axis=1)\n        out = out.reshape(Num, out_h, out_w, Channel).transpose(0, 3, 1, 2)\n        self.x = x\n        self.arg_max = arg_max\n        return out\n\n    def backward(self, dout):\n        dout = dout.transpose(0, 2, 3, 1)\n        pool_size = self.pool_h * self.pool_w\n        dmax = np.zeros((dout.size, pool_size))\n        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n        dmax = dmax.reshape(dout.shape + (pool_size,))  \n        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n        return dx","408d16b9":"#Set Simple Convolution Network. x=input data, t=labels of target\nclass SimpleConvNet:\n    def __init__(self, input_dim=(1, 28, 28), output_size=10, weight_init_std=0.01):\n        self.layers = dict()\n        self.layers['Conv1'] = Convolution(1,10,5,5)\n        self.layers['Relu1'] = Relu()\n        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n        self.layers['Conv2'] =Convolution(10,10,5,5)\n        self.layers['Relu2'] = Relu()\n        self.layers['Affine'] = Affine(640, 10)\n        self.last_layer = SoftmaxWithLoss()\n        self.params = {}\n        self.params['W1'] = self.layers['Conv1'].W\n        self.params['b1'] = self.layers['Conv1'].b\n        self.params['W2'] = self.layers['Conv2'].W\n        self.params['b2'] = self.layers['Conv2'].b\n        self.params['W3'] = self.layers['Affine'].W\n        self.params['b3'] = self.layers['Affine'].b\n\n    def forward(self, x):\n        for layer in self.layers.values():\n            x = layer.forward(x)\n        return x\n\n    def loss(self, x, t):\n        y = self.forward(x) \n        return self.last_layer.forward(y, t)\n\n    def backward(self, x, t):\n        self.loss(x, t)\n        dout = 1\n        dout = self.last_layer.backward(dout)\n        layers = list(self.layers.values())\n        layers.reverse()\n        for layer in layers:\n            dout = layer.backward(dout)\n        grads = {}\n        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n        grads['W2'], grads['b2'] = self.layers['Conv2'].dW, self.layers['Conv2'].db\n        grads['W3'], grads['b3'] = self.layers['Affine'].dW, self.layers['Affine'].db\n        return grads\n#Set NN\nnetwork = SimpleConvNet(input_dim=(1,28,28),  output_size=10, weight_init_std=0.01)\n#Set Static Gradient Descent\nclass SGD:\n    def __init__(self, lr=0.01):\n        self.lr = lr\n        \n    def update(self, params, grads):\n        for key in params.keys():\n            params[key] -= self.lr * grads[key] \n#Set learning rate .etc\nacc_list_SGD = []\nsgd = SGD(lr = 0.008)\nbatch_size = 256","04c2fb77":"class Adam:\n\n    def __init__(self, lr, beta1, beta2): \n        self.lr = lr\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.iter = 0\n        self.m = None\n        self.v = None\n        \n    def update(self, params, grads):\n        if self.m is None:\n            self.m, self.v = {}, {}\n            for key, val in params.items():\n                self.m[key] = np.zeros_like(val)\n                self.v[key] = np.zeros_like(val)\n        \n        self.iter += 1\n        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) \/ (1.0 - self.beta1**self.iter)         \n        \n        for key in params.keys():\n            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n            \n            params[key] -= lr_t * self.m[key] \/ (np.sqrt(self.v[key]) + 1e-7)\n#Set learning rate .etc\nacc_list_adam = []\nadam = Adam(lr=0.0009, beta1=0.9, beta2=0.999)\nbatch_size = 256","42b2cbb6":"#Traing with Static Gradient Descent\nfor epoch in range(10):\n    perm = np.random.permutation(len(x_train))\n    for idx in np.arange(0, len(perm), batch_size):\n        x = x_train[perm[idx:idx+batch_size]]\n        t =  t_train[perm[idx:idx+batch_size]]\n        grads = network.backward(x, t)\n        sgd.update(network.params,grads)\n        \n    y_test = network.forward(x_test)\n    acc_list_SGD.append((y_test.argmax(axis=1) == t_test.argmax(axis=1)).mean())\n    print(f'epoch {epoch + 1} | accuracy {acc_list_SGD[-1]:.2%}')","3751ac13":"#Traing with Adam\nfor epoch in range(10):\n    perm = np.random.permutation(len(x_train))\n    for idx in np.arange(0, len(perm), batch_size):\n        x = x_train[perm[idx:idx+batch_size]]\n        t =  t_train[perm[idx:idx+batch_size]]\n        grads = network.backward(x, t)\n        adam.update(network.params,grads)\n        \n    y_test = network.forward(x_test)\n    acc_list_adam.append((y_test.argmax(axis=1) == t_test.argmax(axis=1)).mean())\n    print(f'epoch {epoch + 1} | accuracy {acc_list_adam[-1]:.2%}')","34e9e1fa":"plt.plot(acc_list_SGD, label='Static Gradient Descent') \nplt.plot(acc_list_adam, label='Adam') \nplt.legend() ","485219a4":"test_sample_num=2900\nplt.imshow(quiz_x[test_sample_num][0],cmap='Greys')","141942c5":"print(network.forward(quiz_x[[test_sample_num]]))\ntest_answer=np.argmax(network.forward(quiz_x[[test_sample_num]]))\nprint(\"------------------------------------------------------\")\nprint(\"The prediction is\",test_answer,\".\",\"That's correct!\")\nprint(\"------------------------------------------------------\")","f6bbfd19":"list_predict=[]\nfor j in range(28000):\n    pj=np.argmax(network.forward(quiz_x[[j]]))\n    list_predict.append(pj)\nQuiz_TL = pd.Series(list_predict, name=\"Label\").astype(\"int32\")","89e39fa1":"listIndex=[]\n[listIndex.append(i) for i in range(1,28001)]\nImageID = pd.Series(listIndex, name=\"ImageID\").astype(\"int32\")","c9cdee39":"submission = pd.concat([ImageID,Quiz_TL],axis = 1)\nsubmission.tail()","99bc5928":"submission.to_csv(\"prediction_from_scratch.csv\", index=False)\nprint(\"prediction_from_scratch.csv was saved.\")","83052716":"# ","60280bd8":"### <font color=\"Blue\">4. Let's start training with Static Gradient Descent. <\/font>","23da4863":"### <font color=\"Blue\">7. Let's make submission data! <\/font>","92ff11c1":"## How to make a digit recognizer <font color=red>from scrach without frameworks<\/font>, Numpy-based solver.","25b9c79f":"### <font color=\"Blue\">2. Let's check sample images.<\/font>","950209da":"### <font color=\"Blue\">6. Let's predict by using test data! <\/font>","c4844939":"#### (1)Test sample","f87029be":"### <font color=\"Blue\">3. Set classes and definitions like softmax,cross entropy,Relu.etc <\/font>","c5b508a8":"### <font color=\"Blue\">1. Preparation before enjoying MNIST<\/font>","d1da488b":"### <font color=\"Blue\">5. Let's start training with Adam. <\/font>"}}