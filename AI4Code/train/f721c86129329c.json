{"cell_type":{"47b09110":"code","7ef8ba55":"code","db3fb9fa":"code","03ef3dee":"code","9c710aae":"code","0f3c44d0":"code","14c60cc6":"code","fc4d8d95":"code","ee319d43":"code","7cd66750":"code","387a5ba6":"code","e2cd35f8":"code","bd250c7a":"code","6e515cae":"code","ab776dff":"code","f0b00fb2":"code","9d1557e9":"code","e153acec":"code","87252df3":"code","90830ec7":"code","becdb3ad":"code","9a784026":"code","d76f577d":"code","fe721143":"code","1d310b75":"code","a9956f43":"code","5f8ee5ea":"code","c18efd54":"code","009bb94d":"code","5d191718":"code","09fd6c43":"code","ca8511ed":"code","a03d51c6":"code","2bb1889c":"code","542ea5ef":"code","f2e82726":"code","2a676e69":"code","a0c9bf92":"code","518f92d9":"code","67556d0e":"code","1b8af036":"code","cf6946e3":"code","e8097711":"code","e1002b1a":"code","4678fd85":"code","996509e4":"code","dc417740":"code","6a072e05":"code","3065acdf":"code","5f11fa71":"code","bf5e8f8f":"code","63cdc59b":"code","5b01ed18":"code","01a889b7":"code","4373e554":"code","c63ff34d":"code","e1ba3b07":"code","580ec0ea":"code","f44b86a7":"code","d2105a22":"code","7270b47a":"code","6bab7335":"code","9aa551b7":"code","f58d8962":"code","ce3841f0":"code","d82f020f":"code","aab96d60":"code","867aafdc":"markdown","4626a8e6":"markdown","9b70e576":"markdown","1a6edd0b":"markdown","86800b45":"markdown","161c8aeb":"markdown","58ffe5cf":"markdown","deb946a5":"markdown","af4e359c":"markdown","ae43f103":"markdown","5ee5043f":"markdown","a075eba3":"markdown","089a4a3d":"markdown","b4b44270":"markdown","e6ffc4ed":"markdown","59500a0c":"markdown","02e3643f":"markdown","42b407f6":"markdown","779f5945":"markdown","9c4d77da":"markdown","e0017fb5":"markdown","86293590":"markdown","ec4f6b31":"markdown","da0b3140":"markdown","f0cda3ed":"markdown","d9fa2f28":"markdown","dfd443e6":"markdown","9fcede25":"markdown","b03e28b9":"markdown","e834cb5c":"markdown","c9490f31":"markdown","e1741c7d":"markdown","5babb3cd":"markdown","101aaff8":"markdown","a2cd9885":"markdown","e7c1c96b":"markdown"},"source":{"47b09110":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7ef8ba55":"import pandas as pd\nimport numpy as np\nimport datetime as dt\nimport seaborn as sns\nimport os\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline","db3fb9fa":"df_train = pd.read_csv(os.path.join(dirname, 'train.csv'))\ndf_test = pd.read_csv(os.path.join(dirname, 'test.csv'))\nprint(\"Shape before deleting duplicate values: {}.\\n\\n\".format(df_train.shape))\n\ndf_train.head()\n","03ef3dee":"plt.figure(figsize = (8, 5))\nsns.countplot(x = df_train[\"y\"], palette=\"Set2\")\nplt.title(\"Number of beans per type\")\nplt.show()","9c710aae":"COLUMNS = df_train.columns.tolist()\nfor c in COLUMNS:\n    if df_train[c].isnull().values.any():\n        print('{0}: {1} invalid values found'.format(c, df_train[c].isnull().sum()))\n    else:\n        print('{0}: ok'.format(c))","0f3c44d0":"df_train.dtypes\n","14c60cc6":"df_train.dtypes","fc4d8d95":"df_train.describe()","ee319d43":"from sklearn.model_selection import train_test_split\n\nX, X_val, y, y_val  = train_test_split(df_train,df_train['y'],\n                                    stratify=df_train['y'],\n                                    test_size=0.2,\n                                    random_state=42,\n                                    shuffle=True)\n\n\nX =X.drop(columns=['ID','y'])\nX_val = X_val.drop(columns=['ID','y'])\n\n\n# settings to display all columns\npd.set_option(\"display.max_columns\", None)\n","7cd66750":"y.value_counts()","387a5ba6":"#measure of deviation from roundness\nX['circular'] = X['Eccentricity'] * X['AspectRation']\nX_val['circular'] = X_val['Eccentricity'] * X_val['AspectRation']\ndf_test['circular'] = df_test['Eccentricity'] * df_test['AspectRation']\n\n\n\nX.head(20)","e2cd35f8":"X['circular'] = pd.cut(X['circular'], bins=4 , labels=np.arange(4), right=False)\nX_val['circular'] = pd.cut(X_val['circular'], bins=4 , labels=np.arange(4), right=False)\ndf_test['circular'] = pd.cut(df_test['circular'], bins=4 , labels=np.arange(4), right=False)\n\n\nX.head()\n","bd250c7a":"X['fibrelength'] = (X['Perimeter'] - np.sqrt(abs((X['Perimeter'])**2 - (16 * X['Area'])))) \/ 4\n\n#X['fibrewidth'] = (X['Area'] \/ X['fibrelength'])\/1000\n\nX['curl'] = X['MajorAxisLength'] \/ X['fibrelength']\n\n\nX_val['fibrelength'] = (X_val['Perimeter'] - np.sqrt(abs((X_val['Perimeter'])**2 - (16 * X_val['Area'])))) \/ 4\n\n#X_val['fibrewidth'] = (X_val['Area'] \/ X_val['fibrelength'])\/1000\n\nX_val['curl'] = X_val['MajorAxisLength'] \/ X_val['fibrelength']\n\n\ndf_test['fibrelength'] = (df_test['Perimeter'] - np.sqrt(abs((df_test['Perimeter'])**2 - (16 * df_test['Area'])))) \/ 4\n\n#df_test['fibrewidth'] = (df_test['Area'] \/ df_test['fibrelength'])\/1000\n\ndf_test['curl'] = df_test['MajorAxisLength'] \/ df_test['fibrelength']\n\nX.drop('fibrelength',axis=1,inplace =True)\nX_val.drop('fibrelength',axis=1,inplace =True)\ndf_test.drop('fibrelength',axis=1,inplace =True)","6e515cae":" X['curl'].min()","ab776dff":"#measure of deviation from roundness\nX['Round'] = X['roundness'] * X['Solidity']\nX_val['Round'] = X_val['roundness'] * X_val['Solidity']\ndf_test['Round'] = df_test['roundness'] * df_test['Solidity']\n\n","f0b00fb2":" X['Extent'].max()","9d1557e9":"g = sns.FacetGrid(df_train, col='y')\n\ng.map(plt.hist,'Extent', bins=100)\nplt.show()","e153acec":"X['elongation'] = pd.cut(X['Extent'], bins=10 , labels=np.arange(10), right=False)\nX_val['elongation'] = pd.cut(X_val['Extent'], bins=10 , labels=np.arange(10), right=False)\ndf_test['elongation'] = pd.cut(df_test['Extent'], bins=10 , labels=np.arange(10), right=False)\n\nX.drop('Extent',axis=1,inplace =True)\nX_val.drop('Extent',axis=1,inplace =True)\ndf_test.drop('Extent',axis=1,inplace =True)\nX.head()\n","87252df3":"# area ratio\nX['ARatio'] = X['Area']\/ X['ConvexArea']\nX_val['ARatio'] = X_val['Area'] \/X_val['ConvexArea']\ndf_test['ARatio'] = df_test['Area'] \/ df_test['ConvexArea']\n\n\n\nX.head()","90830ec7":"COLUMNS = X.columns.tolist()\nfor c in COLUMNS:\n    if X[c].isnull().values.any():\n        print('{0}: {1} invalid values found'.format(c, X[c].isnull().sum()))\n    else:\n        print('{0}: ok'.format(c))","becdb3ad":"X.head()\n","9a784026":"d_column = ['Area',\n            'Perimeter' ,\n            'MajorAxisLength' ,\n            'MinorAxisLength',\n            'AspectRation',\n            'Eccentricity',\n            'ConvexArea',\n            'EquivDiameter',\n            'Solidity',\n            'roundness',\n            'Compactness',\n            'ShapeFactor3'\n           ]","d76f577d":"#now drop feature\nX.drop(d_column,axis=1,inplace =True)\nX_val.drop(d_column,axis=1,inplace =True)\ndf_test.drop(d_column,axis=1,inplace =True)","fe721143":"X.head()","1d310b75":"#'Extent',\n\"\"\"corr_col =['ShapeFactor1',\n            'ShapeFactor2',\n           'ShapeFactor3',\n            'ShapeFactor4',\n            'curl',\n            'Round',\n            'ARatio'\n            ]\"\"\"\n\ncorr_col =[\n            'ShapeFactor4',\n            'ShapeFactor1',\n            'ShapeFactor2',\n            'curl',\n            'Round',\n            'ARatio'\n            ]\ndf_temp =X[corr_col].copy()\nplt.figure(figsize=(18,12))\nsns.heatmap(X[corr_col].corr(), yticklabels='auto', annot=True, cmap=plt.cm.afmhot)\nplt.show()","a9956f43":"# with the following function we can select highly correlated features\n# it will remove the first feature that is correlated with anything other feature\n\ndef correlation(dataset, threshold):\n    col_corr = set()  # Set of all the names of correlated columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n                colname = corr_matrix.columns[i]  # getting the name of column\n                col_corr.add(colname)\n    return col_corr","5f8ee5ea":"#return number of correlated feature\n\ncorr_features = correlation(X[corr_col],0.9)\nlen(set(corr_features))","c18efd54":"corr_features","009bb94d":"#now drop feature and polt heatmap for the remaining features \nX.drop(corr_features,axis=1,inplace =True)\nX_val.drop(corr_features,axis=1,inplace =True)\ndf_test.drop(corr_features,axis=1,inplace =True)\nplt.figure(figsize=(18,12))\nsns.heatmap(X.corr(), yticklabels='auto', annot=True, cmap=plt.cm.afmhot)\nplt.show()","5d191718":"col = X.columns.tolist()\ncol.remove('elongation')\ncol.remove('circular')\ncol","09fd6c43":"X.head()","ca8511ed":"X.hist(figsize=(25, 15))\nplt.show()","a03d51c6":"from scipy.stats import skew, norm, probplot, boxcox, f_oneway\n\n\nskewed_features = X[col].apply(lambda x : skew (x.dropna())).sort_values(ascending=False)\n\n#compute skewness\nskewness = pd.DataFrame({'Skew' :skewed_features})   \n\n# Get only higest right skewed features\nskewness_r = skewness[(skewness) > 0.8]\nskewness_r = skewness_r.dropna()\nprint (\"There are {} higest skewed numerical features to  transform\".format(skewness_r.shape[0]))\ncol_skew_r = list( skewness_r.index)\nprint (col_skew_r)\n# Get only higest skewed left features\nskewness_l = skewness[(skewness)< - 0.8]\nskewness_l = skewness_l.dropna()\nprint (\"There are {} higest skewed numerical features to transform\".format(skewness_l.shape[0]))\ncol_skew_l = list( skewness_l.index)\n\nprint (col_skew_l)","2bb1889c":"\"\"\"#apply log transformation to positive skew column \n\nfor col in col_skew_r:\n    X[col] = np.log1p(X[col])\n    X_val[col] = np.log1p(X_val[col])\n    df_test[col] = np.log1p(df_test[col])\n\"\"\"","542ea5ef":"\"\"\"#apply power transformation to negative skew feature \nfrom sklearn.preprocessing import PowerTransformer\n\npower = PowerTransformer(method='yeo-johnson')\nfor col2 in col_skew_l:\n    X[col2] = power.fit_transform(X[col2].values.reshape(-1,1))\n    X_val[col2] = power.transform(X_val[col2].values.reshape(-1,1))\n    df_test[col2] = power.transform(df_test[col2].values.reshape(-1,1))\n\n\"\"\"","f2e82726":"X.hist(figsize=(25, 15))\nplt.show()","2a676e69":"\n#scaler = MinMaxScaler()\nscaler = StandardScaler()\nX[col] = scaler.fit_transform(X[col])\nX_val[col] = scaler.transform(X_val[col])\ndf_test[col] = scaler.transform(df_test[col])\nX.hist(figsize=(25, 15))\nplt.show()","a0c9bf92":"temp = X.copy()\ntemp['y']=y\nsns.pairplot(temp, hue=\"y\")","518f92d9":"from sklearn import preprocessing\n\nle = preprocessing.LabelEncoder()\nle.fit(y)\n\nle.classes_","67556d0e":"y=le.transform(y)\ny_val =le.transform(y_val)\n","1b8af036":"X.head()","cf6946e3":"def one_hot_encoding(data, column):\n    data = pd.concat([data, pd.get_dummies(data[column], prefix=column)], axis=1)\n    data = data.drop([column], axis=1)\n    return data\n\n#cols = ['fibrewidth','curl_cat','Solidity_new','roundness_label','elongation']\ncols = ['elongation','circular']\nfor col in cols:\n    X = one_hot_encoding(X, col)\n    X_val = one_hot_encoding(X_val, col)\n    df_test  = one_hot_encoding(df_test, col)","e8097711":"X.head()","e1002b1a":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, make_scorer\nimport xgboost as xgb\nimport random\n","4678fd85":"a_index=list(range(1,13))\na=pd.Series()\nx=[0,1,2,3,4,5,6,7,8,9,10,11,12]\nfor i in list(range(1,13)):\n    model=KNeighborsClassifier(n_neighbors=i) \n    model.fit(X,y)\n    prediction=model.predict(X_val)\n    a=a.append(pd.Series(metrics.accuracy_score(prediction,y_val)))\nplt.plot(a_index, a)\nplt.xticks(x)\nfig=plt.gcf()\nfig.set_size_inches(12,6)\nplt.show()\nprint('Accuracies for different values of n are:',a.values,'with the max value as ',a.values.max())","996509e4":"params_grid = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n                     'C': [1, 10, 100, 1000]},\n                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]},\n                    {'kernel': ['poly'], 'C': [1, 10, 100, 1000] ,'degree':[1,2] }\n              ]","dc417740":"svm_model = GridSearchCV(SVC(), params_grid, cv=5)\nsvm_model.fit(X, y)","6a072e05":"\nprint('Best score :',svm_model.best_score_,\"\\n\") \nprint('Best C:',svm_model.best_estimator_.C,\"\\n\") \nprint('Best Kernel:',svm_model.best_estimator_.kernel,\"\\n\")\nprint('Best Gamma:',svm_model.best_estimator_.gamma,\"\\n\")\nprint('Best degree:',svm_model.best_estimator_.degree,\"\\n\")","3065acdf":"params_ada = {\n    \"n_estimators\": [5, 10, 15, 20, 25, 40],\n    \"learning_rate\": [0.1, 0.2, 0.4, 0.6, 0.8, 1.0]\n}","5f11fa71":"\nf1 = make_scorer(f1_score , average = \"weighted\")","bf5e8f8f":"gs_ada = GridSearchCV(AdaBoostClassifier(base_estimator = RandomForestClassifier(max_depth = 10, n_estimators = 20, random_state = 42)), \n                      param_grid = params_ada, scoring = f1, cv = 4, n_jobs = -1)","63cdc59b":"gs_ada.fit(X, y)","5b01ed18":"gs_ada.best_params_","01a889b7":"LR= gs_ada.best_params_.get('learning_rate')\nN_estimators= gs_ada.best_params_.get('n_estimators')\nprint (LR ,N_estimators)","4373e554":"svc = SVC(kernel=svm_model.best_estimator_.kernel, \n          degree= svm_model.best_estimator_.degree,\n          gamma=svm_model.best_estimator_.gamma, \n          coef0=1, \n          C=svm_model.best_estimator_.C,\n          probability=True)\n\nknc = KNeighborsClassifier(n_neighbors=7)\ndtc = DecisionTreeClassifier(min_samples_split=7, random_state=42)\nlrc = LogisticRegression(solver='liblinear', penalty='l1')\nrfc = RandomForestClassifier(n_estimators=31, random_state=42)\nabc =gs_ada = GridSearchCV(AdaBoostClassifier(base_estimator = RandomForestClassifier(max_depth = 10, n_estimators = 20, random_state = 42)), \n                      param_grid = params_ada, scoring = f1, cv = 4, n_jobs = -1) \n\nbc = BaggingClassifier(n_estimators=5, random_state=42)\netc = ExtraTreesClassifier(n_estimators=9, random_state=42)\nclf_mlp = MLPClassifier( random_state=42,warm_start=True,hidden_layer_sizes=(40),alpha=0.05)\nada=AdaBoostClassifier(base_estimator = RandomForestClassifier(max_depth = 10, n_estimators = N_estimators, random_state = 42), n_estimators = N_estimators, learning_rate = LR, random_state = 42)","c63ff34d":"clfs = {'SVC' : svc,'KN' : knc, 'DT': dtc, 'LR': lrc, 'RF': rfc, 'AdaBoost': abc, 'BgC': bc, 'ETC': etc , 'MLp':clf_mlp ,'ADA' :ada}","e1ba3b07":"def train_classifier(clf, X, y):    \n    clf.fit(X, y)","580ec0ea":"def predict_labels(clf, X_val):\n    return (clf.predict(X_val))","f44b86a7":"#Now iterate through classifiers and save the results\n\npred_scores = []\nfor k,v in clfs.items():\n    train_classifier(v, X, y)\n    pred = predict_labels(v,X_val)\n    pred_scores.append((k, [ f1_score(y_val, pred, average='micro')]))\n   ","d2105a22":"df = pd.DataFrame(pred_scores)\ndf","7270b47a":"from sklearn.ensemble import VotingClassifier\neclf = VotingClassifier(estimators=[('RF', rfc), ('MLp', clf_mlp),('KN' , knc),( 'SVC' , svc),( 'ADA' , ada)], voting='soft' )\n\n","6bab7335":"#eclf.fit(X,y)","9aa551b7":"#pred_v = eclf.predict(X_val)","f58d8962":"#print( f1_score(y_val, pred_v, average='micro'))","ce3841f0":"X_test = df_test.drop(columns=['ID'])\nX_test.head()","d82f020f":"y_test_predicted = le.inverse_transform(clf_mlp.predict(X_test))\n\ndf_test['y'] = y_test_predicted\n\ndf_test.head()","aab96d60":"df_test[['ID', 'y']].to_csv('.\/submission.csv', index=False)","867aafdc":"## now label the target","4626a8e6":"# the dataset is imbalanced. \n#### we need to splite data in away that all samples are well represented in train dataset \n","9b70e576":"## Voting classifier","1a6edd0b":"![Capture5.PNG](attachment:70e9c045-88da-402a-91e0-0a60e43ef133.PNG)","86800b45":"## drop feature with correlation more than 85%","161c8aeb":" \n## <span style='color:#5499C7 '> Loading to dataframe <\/span>\n","58ffe5cf":"# drop  columns \n","deb946a5":"## the best param for AdaBoost","af4e359c":"# Correlation ","ae43f103":"### now plot all features and target ","5ee5043f":"## submit best model pred","a075eba3":"### only target value need encoding tech","089a4a3d":"## try to divide to 4 cat","b4b44270":"# Solidity\n\n####  the min value in training is 0.91 \n#### so most of the beans are solide with slightly deffirent  \n#### low Solidity mean that object having anirregular boundary, or containing holes","e6ffc4ed":"# roundness \n\n\n![glossary-compactness.png](attachment:dce61cbc-403d-4a43-b1c2-789c694a40ae.png)","59500a0c":"# add new feature circular\n","02e3643f":"# Hole Area Ratio\n#### HAR is the ratio: (area of the holes)\/(area of shape)\n\n#### Area Ratio","42b407f6":"### check for any NAN value ","779f5945":"## Now applying Z-score to all columns ","9c4d77da":"# Model","e0017fb5":"### find skewed columns\n\n","86293590":"![Capture.PNG](attachment:de566f0e-f470-4996-b8ee-691d6833e85b.PNG)","ec4f6b31":"# some of the feature are very high correlated ","da0b3140":"# Extent \n### elongation  the min 0.5 max 0.9 \n#### to 8 categories \n","f0cda3ed":"# Data fields\n- ID - an ID for this instance\n- Area - (A), The area of a bean zone and the number of pixels within its boundaries.\n- Perimeter - (P), Bean circumference is defined as the length of its border.\n- MajorAxisLength - (L), The distance between the ends of the longest line that can be drawn from a bean.\n- MinorAxisLength - (l), The longest line that can be drawn from the bean while standing perpendicular to the main axis.\n- AspectRatio - (K), Defines the relationship between L and l.\n- Eccentricity - (Ec), Eccentricity of the ellipse having the same moments as the region.\n- ConvexArea - (C), Number of pixels in the smallest convex polygon that can contain the area of a bean seed.\n- EquivDiameter - (Ed), The diameter of a circle having the same area as a bean seed area.\n- Extent - (Ex), The ratio of the pixels in the bounding box to the bean area.\n- Solidity - (S), Also known as convexity. The ratio of the pixels in the convex shell to those found in beans.\n- Roundness - (R), Calculated with the following formula: (4piA)\/(P^2)\n- Compactness - (CO), Measures the roundness of an object: Ed\/L\n- ShapeFactor1 - (SF1)\n- ShapeFactor2 - (SF2)\n- ShapeFactor3 - (SF3)\n- ShapeFactor4 - (SF4)\n- y - the class of the bean. It can be any of BARBUNYA, SIRA, HOROZ, DERMASON, CALI, BOMBAY, and SEKER.","d9fa2f28":"![Capture3.PNG](attachment:b44d2802-ef4b-4ab1-927e-7729cd9caa97.PNG)","dfd443e6":" \n## <span style='color:#5499C7 '> Importing libraries  <\/span>\n","9fcede25":"![Capture2.PNG](attachment:1b00c30b-70b6-463b-add7-6486e79ac62d.PNG)","b03e28b9":"![Eccentric.png](attachment:352635f3-d219-4e43-8f32-c582be0d21e5.png)","e834cb5c":"## splite data for train and validate ","c9490f31":"## the best param for SVM","e1741c7d":" \n## <span style='color:#5499C7 '> data pre pro<\/span>\n","5babb3cd":"# Curl \n\n$$ curl = \\frac{length}{fibre length} $$\n\n$$ fibre length = \\frac{perimeter - \\sqrt{(perimeter)^2 - 16 * area}}{4} $$\n\n$$ fiber width = \\frac{area}{fibre length} $$","101aaff8":"# One Hot Encoding for categorical features ","a2cd9885":"## the best n_neighbors fro KNN ","e7c1c96b":"##### get the positive skewed featuers in list \n##### get the negativelly skewed features in another list "}}