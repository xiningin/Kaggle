{"cell_type":{"2acd1962":"code","7f78a80a":"code","62d20aed":"code","a99393e1":"code","4516fba1":"code","3925798b":"code","848620f7":"code","2283d23e":"code","2e8c131f":"code","ab29f288":"code","7aa12627":"markdown","bbd225ce":"markdown","352f1236":"markdown","1a061060":"markdown"},"source":{"2acd1962":"from pyspark.sql import SparkSession\n\nsc = SparkSession\\\n    .builder\\\n    .master(\"local[*]\")\\\n    .appName('example_spark')\\\n    .getOrCreate()","7f78a80a":"# creating a dataframe\ndata = [\n    (1, \"Gilbert Gathara\", \"24\", \"Nairobi\"), \n    (2, \"Someone Else\", \"32\", \"Nowhere\")\n]\nheaders = (\"id\", \"Name\", \"Age\", \"Location\")\ndf = sc.createDataFrame(data, headers)\ndf.show()","62d20aed":"# 1. loading and caching data\n# df = sc.read\\\n#     .format(\"com.databricks.spark.csv\")\\\n#     .options(header=True)\\\n#     .load(filename)\\\n#     .cache()\n\n# 2. loading and caching data\n# df = sc.read\\\n#     .format(\"csv\")\\\n#     .options(\"header\", true)\\\n#     .options(\"inferSchema\", true)\\\n#     .load(filename)\\\n#     .cache()","a99393e1":"from pyspark.sql.functions import col\n# left-outer join\n\n# df1        - dataframe being topped up.\n# column_id  - similar df column to compare the join with.\n# column2_id - column of interest to be added to df1.\n \n# new_df = df1.alias(\"a\")\\\n#     .join(df2, df1.column_id == df2.column_id, \"left_outer\")\\\n#     .select(*[col(\"a.\"+c) for c in df1.columns] + [df2.column2_id])","4516fba1":"# count null values\n# def count_null(df, col):\n#     return df.where(df[col].isNull()).count()\n# print(\"Null-count on column '{}': {}\".format(col, count_null(df, col)))","3925798b":"# from pyspark.sql.functions import col\n\n# double_cols - columns to be double casted.\n# other_cols  - columns to be attached to resulting dataframe.\n\n# cast multiple columns to double\n# df = df.select(*[col(c).cast(\"double\").alias(c) for c in double_cols] + other_cols)","848620f7":"# from pyspark.sql.functions import col, udf\n# from pyspark.sql.types import IntegerType\n\n# int_cols   - columns to be integer casted.\n# other_cols - columns to be attached to resulting dataframe.\n\n# cast multiple columns to integers using a udf\n# int_udf = udf(\n#     lambda r: int(r),\n#     IntegerType()\n# )\n# df = df.select(*[int_udf(col(col_name)).name(col_name) for col_name in int_cols] + other_cols)","2283d23e":"# from pyspark.ml.feature import StringIndexer\n\n# StringIndexer converts string cols to numerical.\n# df = StringIndexer(\n#     inputCol=\"col_1\",\n#     outputCol=\"col_1_indx\")\\\n#     .fit(df)\\\n#     .transform(df)\\\n#     .drop(\"col_1\")\\\n#     .withColumnRenamed(\"col_1_indx\", \"col_1\")","2e8c131f":"# from pyspark.ml.linalg import Vectors\n# from pyspark.sql import Row\n\n# # vectorize features + labels\n# df_indexed = df_indexed[feature_cols + [label_col]]\n# row = Row(\"label\", \"features\")\n# df_vec = df_indexed.rdd.map(\n#     lambda r: (row(r[-1], Vectors.dense(r[:-1])))\n# ).toDF()","ab29f288":"# from pyspark.ml.feature import StandardScaler\n\n# normalize features to have a mean of 0 and \n# standard deviation of 1.\n\n# df = StandardScaler(\n#     inputCol=\"features\",\n#     outputCol=\"features_norm\",\n#     withStd=True,\n#     withMean=True)\\\n#     .fit(df)\\\n#     .transform(df)\\\n#     .drop(\"features\")\\\n#     .withColumnRenamed(\"features_norm\", \"features\")","7aa12627":"## 3. Feature processing","bbd225ce":"## 2. Data cleaning","352f1236":"## Introduction\nMain purpose of the notebook is to document some pyspark code snippets, used as building blocks in other notebook projects.","1a061060":"## 1. Dataframes"}}