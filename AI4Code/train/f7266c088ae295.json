{"cell_type":{"3be17a26":"code","7d8b4907":"code","68c36e77":"code","98036418":"code","4baa1c12":"code","306ee08e":"code","8ed64d57":"code","8518f355":"code","c87cc1d7":"code","9515cada":"code","85ebc20c":"code","115bfd54":"code","41da8e39":"code","05628320":"code","6bfc7953":"code","9f8d0a98":"code","63b92325":"code","dc3a5803":"code","c7706f66":"code","03bfdd11":"code","c4ae771a":"code","6ff398fb":"code","c6981c7f":"code","536674cd":"code","3519a195":"code","dec8e53b":"code","ba94efe9":"code","722f4b33":"code","08633496":"code","dfc1616b":"code","8ab885e1":"code","71829f35":"code","44ab7457":"code","c9d89f7a":"code","8a89a6c1":"code","41c735ff":"code","ef7ea6e3":"code","62a4759b":"code","9007aac1":"code","81d44ecd":"code","91d16107":"code","82dfe8ef":"code","2f233cb6":"code","fb6db896":"code","e9ef7cc7":"code","0752dd47":"code","8f48a791":"code","1a7101b4":"code","00ed0445":"code","6814779f":"code","9c4e4920":"code","6f911afb":"code","d47372ec":"code","660d72c1":"code","8f923485":"code","8a68ae4f":"code","318f831c":"code","5b96124f":"code","270078c8":"code","3197a04b":"code","e2b280c0":"code","7e7e8d87":"code","7b0fde5b":"code","7b7318e4":"code","dfa773ba":"code","69704ad7":"code","a16d98b9":"code","8dbba098":"code","a34430bf":"code","0f3062c9":"code","a5aa962b":"code","8f6cb397":"code","4e7f6919":"code","ac37790d":"code","e585e72c":"code","c98c4bed":"markdown","8eba36e2":"markdown","ed3c2b71":"markdown","ed19af9c":"markdown","e219cad4":"markdown","337428e5":"markdown","e35e6310":"markdown","42895c69":"markdown","e24a82cc":"markdown","fd6c4f9a":"markdown","ef7a4bd6":"markdown","58a3d216":"markdown","3bda67d6":"markdown","e4dc9a41":"markdown","ff6f04f9":"markdown","ba3472c8":"markdown","7e541d0d":"markdown","88d12a0a":"markdown","a6aab177":"markdown","fa95fdbe":"markdown","ce99954f":"markdown","da58d65c":"markdown","572d7d28":"markdown","cdd2fef4":"markdown","1618c500":"markdown","2ce06d55":"markdown","f03eaa1d":"markdown","052c3d78":"markdown","78bd1dca":"markdown","1c5deb98":"markdown","4f052130":"markdown","298071d0":"markdown","328e3022":"markdown","813b8234":"markdown","0d0d8359":"markdown","ce64bfbe":"markdown","7000c144":"markdown","2d2792a4":"markdown","b8e36796":"markdown","5d715739":"markdown","0bc3934f":"markdown","d3c07c0b":"markdown","8984069e":"markdown","a7022176":"markdown","af350a55":"markdown","9fd88b8d":"markdown","91f5e898":"markdown","1591c2fc":"markdown","3fe60f6a":"markdown","e5a796ed":"markdown","e70f81f9":"markdown","b0cd0291":"markdown"},"source":{"3be17a26":"import os\nimport random\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nfrom glob import glob\nimport warnings\n\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\nimport seaborn as sns\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\n\nimport tensorflow as tf\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.models import Model, load_model, save_model ,Sequential\nfrom tensorflow.keras.layers import Input, Activation, BatchNormalization, Dropout, Lambda, Conv2D\nfrom tensorflow.keras.layers import MaxPooling2D, concatenate, Dense, Flatten ,UpSampling2D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","7d8b4907":"# Set parameters\nSize = (256, 256)\nmask_files = glob('..\/input\/lgg-mri-segmentation\/kaggle_3m\/*\/*_mask*')\ntrain_files = [file.replace('_mask', '') for file in mask_files]","68c36e77":"mask_files[:5]","98036418":"train_files[:5]","4baa1c12":"def label(mask):\n    value = np.max(cv2.imread(mask))\n    return '1' if value > 0 else '0'\ndf = pd.DataFrame({\"image\": train_files,\n                   \"mask\": mask_files,\n                  \"label\":[label(x) for x in mask_files]})","306ee08e":"df.head()","8ed64d57":"sns.countplot(data=df,x=df['label'])\nplt.title('Length Of Classes')\nplt.show()","8518f355":"df_Tumor = df[df['label']=='1'].sample(5).values\ndf_NotTumor = df[df['label']=='0'].sample(5).values","c87cc1d7":"df_Tumor[0]","9515cada":"df_NotTumor[0]","85ebc20c":"#showing training images with labels\nplt.figure(figsize=(20,20))\nplt.title('Tumor')\nfor n , i in enumerate(range(5)) : \n    plt.subplot(1,5,n+1)\n    img=cv2.imread(df_Tumor[i,0])\n    plt.title('Tumor')\n    plt.imshow(img)\n    plt.axis('off')\n    #showing training images with labels\nplt.show()\nplt.figure(figsize=(20,20))\nfor n , i in enumerate(range(5)) :\n    plt.subplot(1,5,n+1)\n    plt.title('Mask Tumor')\n    img=cv2.imread(df_Tumor[i,1])\n    plt.imshow(img)\n    \n    plt.axis('off')\nplt.show()\n\n#showing training images with labels\nplt.figure(figsize=(20,20))\nplt.title('Tumor')\nfor n , i in enumerate(range(5)) : \n    plt.subplot(1,5,n+1)\n    plt.title('Not Tumor')\n    img=cv2.imread(df_NotTumor[i,0])\n    plt.imshow(img)\n    plt.axis('off')\n    #showing training images with labels\nplt.show()\nplt.figure(figsize=(20,20))\nfor n , i in enumerate(range(5)) :\n    plt.subplot(1,5,n+1)\n    plt.title('Mask Not Tumor')\n    img=cv2.imread(df_NotTumor[i,1])\n    plt.imshow(img)\n    \n    plt.axis('off')\nplt.show()","115bfd54":"from sklearn.model_selection import train_test_split\ndf_train, df_test = train_test_split(df, test_size=0.1)\ndf_test, df_val = train_test_split(df_test, test_size=0.5)\nprint(df_train.values.shape)\nprint(df_val.values.shape)\nprint(df_test.values.shape)","41da8e39":"def train_generator(data_frame, batch_size, aug_dict,\n        image_color_mode=\"rgb\",\n        mask_color_mode=\"grayscale\",\n        image_save_prefix=\"image\",\n        mask_save_prefix=\"mask\",\n        save_to_dir=None,\n        target_size=(256,256),\n        seed=1):\n\n    image_datagen = ImageDataGenerator(**aug_dict)\n    mask_datagen = ImageDataGenerator(**aug_dict)\n    \n    image_generator = image_datagen.flow_from_dataframe(\n        data_frame,\n        x_col = \"image\",\n        class_mode = None,\n        color_mode = image_color_mode,\n        target_size = target_size,\n        batch_size = batch_size,\n        save_to_dir = save_to_dir,\n        save_prefix  = image_save_prefix,\n        seed = seed)\n\n    mask_generator = mask_datagen.flow_from_dataframe(\n        data_frame,\n        x_col = \"mask\",\n        class_mode = None,\n        color_mode = mask_color_mode,\n        target_size = target_size,\n        batch_size = batch_size,\n        save_to_dir = save_to_dir,\n        save_prefix  = mask_save_prefix,\n        seed = seed)\n\n    train_gen = zip(image_generator, mask_generator)\n    \n    for (img, mask) in train_gen:\n        img, mask = adjust_data(img, mask)\n        yield (img,mask)\n\ndef adjust_data(img,mask):\n    img = img \/ 255.\n    mask = mask \/ 255.\n    mask[mask > 0.5] = 1\n    mask[mask <= 0.5] = 0\n    \n    return (img, mask)","05628320":"def unet(input_size = (256,256,3)):\n    inputs = Input(input_size)\n    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n    \n    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n    \n    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n    \n    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n    drop4 = Dropout(0.5)(conv4)\n    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n\n    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n    drop5 = Dropout(0.5)(conv5)\n\n    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n    \n    merge6 = concatenate([drop4,up6], axis = 3)\n    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n\n    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n    merge7 = concatenate([conv3,up7], axis = 3)\n    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n\n    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n    merge8 = concatenate([conv2,up8], axis = 3)\n    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n\n    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n    merge9 = concatenate([conv1,up9], axis = 3)\n    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n\n    model = Model(inputs= inputs, outputs = conv10)\n    return model","6bfc7953":"m=unet()\nm.summary()","9f8d0a98":"smooth=1.\n\ndef dice_coef(y_true, y_pred):\n    y_true = K.flatten(y_true)\n    y_pred = K.flatten(y_pred)\n    intersection = K.sum(y_true * y_pred)\n    union = K.sum(y_true) + K.sum(y_pred)\n    return (2.0 * intersection + smooth) \/ (union + smooth)\n\ndef dice_coef_loss(y_true, y_pred):\n    return 1 - dice_coef(y_true, y_pred)\n\ndef bce_dice_loss(y_true, y_pred):\n    bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n    return dice_coef_loss(y_true, y_pred) + bce(y_true, y_pred)\n\ndef iou(y_true, y_pred):\n    intersection = K.sum(y_true * y_pred)\n    sum_ = K.sum(y_true + y_pred)\n    jac = (intersection + smooth) \/ (sum_ - intersection + smooth)\n    return jac","63b92325":"# Set parameters\nEPOCHS = 100\nBATCH_SIZE = 16\nlearning_rate = 1e-4","dc3a5803":"train_generator_args = dict(rotation_range=0.1,\n                            width_shift_range=0.05,\n                            height_shift_range=0.05,\n                            shear_range=0.05,\n                            zoom_range=0.05,\n                            horizontal_flip=True,\n                            vertical_flip=True,\n                            fill_mode='nearest')\ntrain_gen = train_generator(df_train, BATCH_SIZE,\n                                train_generator_args,\n                                target_size=Size)\n    \nval_gen = train_generator(df_val, BATCH_SIZE,\n                                dict(),\n                                target_size=Size)","c7706f66":"model = unet(input_size=(Size[0], Size[1], 3))","03bfdd11":"opt = Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, amsgrad=False)\n\ncallbacks = [ModelCheckpoint('brainMRI_Segment.hdf5', verbose=0, save_best_only=True),\n            ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, min_lr=1e-11),\n            EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=15)]","c4ae771a":"model.compile(optimizer=opt, loss=bce_dice_loss, metrics=[iou, dice_coef])","6ff398fb":"history = model.fit(train_gen,\n                    steps_per_epoch=len(df_train) \/\/ BATCH_SIZE, \n                    epochs=EPOCHS, \n                    callbacks=callbacks,\n                    validation_data = val_gen,\n                    validation_steps=len(df_val) \/\/ BATCH_SIZE)","c6981c7f":"plt.figure(figsize=(8,15))\nplt.subplot(3,1,1)\nplt.plot(model.history.history['loss'], 'b-', label='train_loss')\nplt.plot(model.history.history['val_loss'], 'r-', label='val_loss')\nplt.legend(loc='best')\nplt.title('Loss')\n\nplt.subplot(3,1,2)\nplt.plot(model.history.history['iou'], 'b-', label='train_iou')\nplt.plot(model.history.history['val_iou'], 'r-', label='val_iou')\nplt.legend(loc='best')\nplt.title('IoU')\n\nplt.subplot(3,1,3)\nplt.plot(model.history.history['dice_coef'], 'b-', label='train_dice_coef')\nplt.plot(model.history.history['val_dice_coef'], 'r-', label='val_dice_coef')\nplt.legend(loc='best')\nplt.title('Dice Coef')","536674cd":"test_gen = train_generator(df_test, BATCH_SIZE,\n                                dict(),\n                                target_size=Size)\nresults = model.evaluate(test_gen, steps=len(df_test) \/ BATCH_SIZE)\nprint(\"Test IOU: \",results[1])\nprint(\"Test Dice Coefficent: \",results[2])","3519a195":"for i in range(10):\n    index=np.random.randint(1,len(df_test.index))\n    img = cv2.imread(df_test['image'].iloc[index])\n    img = cv2.resize(img ,Size)\n    img = img \/ 255\n    img = img[np.newaxis, :, :, :]\n    pred=model.predict(img)\n\n    plt.figure(figsize=(12,12))\n    plt.subplot(1,3,1)\n    plt.imshow(np.squeeze(img))\n    plt.title('Original Image')\n    plt.subplot(1,3,2)\n    plt.imshow(np.squeeze(cv2.imread(df_test['mask'].iloc[index])))\n    plt.title('Original Mask')\n    plt.subplot(1,3,3)\n    plt.imshow(np.squeeze(pred) > .5)\n    plt.title('Prediction')\n    plt.show()","dec8e53b":"Gen = tf.keras.preprocessing.image.ImageDataGenerator(\n            rescale=1.\/255.,\n            rotation_range=0.1,\n            width_shift_range=0.05,\n            height_shift_range=0.05,\n            shear_range=0.05,\n            zoom_range=0.05,\n            horizontal_flip=True,\n            vertical_flip=True,\n            fill_mode='nearest'\n)","ba94efe9":"len(df[df['label']=='1'])","722f4b33":"d1=np.array(df[df['label']=='0'].sample(1373).values)\nd2=np.array(df[df['label']=='1'].sample(1373).values)","08633496":"ddd=np.vstack([d1,d2])","dfc1616b":"df_train=pd.DataFrame(ddd,columns=['image','mask','label'])","8ab885e1":"df_train.shape","71829f35":"train= Gen.flow_from_dataframe(\n            dataframe=df_train,\n            x_col=\"image\",\n            y_col=\"label\",\n            has_ext=False,\n            batch_size=16,\n            seed=42,\n            shuffle=True,\n            color_mode='rgb',\n            class_mode=\"binary\",\n            target_size=Size\n)\nprint('Train generator created')","44ab7457":"plt.title('train data count')\nsns.countplot(train.classes)\nplt.show()","c9d89f7a":"classes={0:'Not Tumor',1:'Tumor'}\nplt.figure(figsize=(20,20))\nfor X_batch, y_batch in train:\n    # create a grid of 3x3 images\n    for i in range(0,16):\n        plt.subplot(4,4,i+1)\n        plt.imshow(X_batch[i])\n        plt.title(classes[y_batch[i]])\n    # show the plot\n    plt.show()\n    break","8a89a6c1":"Genn = tf.keras.preprocessing.image.ImageDataGenerator(\n            rescale=1.\/255.,\n            )\nvalid = Genn.flow_from_dataframe(\n            dataframe=df_val,\n            x_col=\"image\",\n            y_col=\"label\",\n            has_ext=False,\n            batch_size=16,\n            seed=42,\n            shuffle=True,\n            color_mode='rgb',#rgb \n            class_mode=\"binary\",\n            target_size=Size\n)\nprint('Valid generator created')","41c735ff":"plt.title('valid data count')\nsns.countplot(valid.classes)\nplt.show()","ef7ea6e3":"classes={0:'Not Tumor',1:'Tumor'}\nplt.figure(figsize=(20,20))\nfor X_batch, y_batch in valid:\n    # create a grid of 3x3 images\n    for i in range(0,16):\n        plt.subplot(4,4,i+1)\n        plt.imshow(X_batch[i])\n        plt.title(classes[y_batch[i]])\n    # show the plot\n    plt.show()\n    break","62a4759b":"test = Genn.flow_from_dataframe(\n            dataframe=df_test,\n            x_col=\"image\",\n            y_col=\"label\",\n            has_ext=False,\n            batch_size=16,\n            seed=42,\n            shuffle=True,\n            color_mode='rgb',\n            class_mode=\"binary\",\n            target_size=Size\n)\nprint('test generator created')","9007aac1":"plt.title('Test data count')\nsns.countplot(test.classes)\nplt.show()","81d44ecd":"plt.figure(figsize=(20,20))\nfor X_batch, y_batch in test:\n    # create a grid of 3x3 images\n    for i in range(0,16):\n        plt.subplot(4,4,i+1)\n        plt.imshow(X_batch[i])\n        plt.title(classes[y_batch[i]])\n    # show the plot\n    plt.show()\n    break","91d16107":"classifier=Sequential([\n    \n                Conv2D(64,3,activation='relu',input_shape=(256,256,3)),\n                BatchNormalization(),              \n                MaxPooling2D(3),\n    \n                Conv2D(128,3,activation='relu'),\n                BatchNormalization(),\n                MaxPooling2D(3),\n    \n                Conv2D(128,3,activation='relu'),\n                BatchNormalization(),\n                MaxPooling2D(3),\n\n                Conv2D(256,3,padding='valid',activation='relu'),\n                BatchNormalization(),\n                MaxPooling2D(3),\n\n                Flatten(),\n    \n                Dense(1024,activation='relu'),\n                BatchNormalization(),\n    \n                Dense(256,activation='relu'),\n                BatchNormalization(),\n    \n                Dense(1,activation='sigmoid')\n                  \n])","82dfe8ef":"classifier.summary()","2f233cb6":"callback = [ModelCheckpoint('brainMRI_Classifer.hdf5', verbose=1, save_best_only=True),\n            ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, min_lr=1e-11),\n            EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=7)]","fb6db896":"classifier.compile(optimizer=opt,loss='binary_crossentropy',metrics=['accuracy'])","e9ef7cc7":"history=classifier.fit(train,validation_data=valid,epochs=EPOCHS,batch_size=2,\n                  steps_per_epoch=len(train),validation_steps=len(valid)\/\/4,\n                  callbacks=callback, verbose=1,shuffle=True)","0752dd47":"sns.set()\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)","8f48a791":"#accuracy plot\nplt.plot(epochs, acc, color='green', label='Training Accuracy')\nplt.plot(epochs, val_acc, color='blue', label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\nplt.figure()\n","1a7101b4":"#loss plot\nplt.plot(epochs, loss, color='green', label='Training Loss')\nplt.plot(epochs, val_loss, color='red', label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","00ed0445":"pd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","6814779f":"predictions = classifier.predict(test)","9c4e4920":"print(classification_report(test.classes,predictions.round()))","6f911afb":"cm = confusion_matrix(test.classes, predictions.round())\nsns.heatmap(cm,  annot=True, fmt=\"d\" ,cmap=\"YlGnBu\")","d47372ec":"image=plt.imread('..\/input\/lgg-mri-segmentation\/kaggle_3m\/TCGA_CS_4941_19960909\/TCGA_CS_4941_19960909_11.tif')","660d72c1":"plt.imshow(image)\nplt.show()","8f923485":"image=cv2.resize(image,dsize=(Size))\nimage=image \/255.0\nimage_ex=np.expand_dims(image,axis=0)","8a68ae4f":"pred=model.predict(image_ex)","318f831c":"prediction = classifier.predict(image_ex)","5b96124f":"prediction.round()[0][0]","270078c8":"classes={0:'Not Tumor',1:'Tumor'}\nprint(classes[prediction.round()[0][0]])","3197a04b":"pred.shape","e2b280c0":"plt.imshow(np.squeeze(pred) > .5)","7e7e8d87":"p=np.reshape(pred,(256,256,1)) \nthresh=cv2.threshold(p,0.5,1,cv2.THRESH_BINARY)","7b0fde5b":"thresh","7b7318e4":"thresh=np.uint8(thresh[1])","dfa773ba":"plt.imshow(thresh)","69704ad7":"contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)","a16d98b9":"len(contours)","8dbba098":"contours","a34430bf":"img=image.copy()\ncv2.drawContours(img,contours,0,(0,0,255),3)\nx,y,w,h=cv2.boundingRect(contours[0])\nimg=cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),3)","0f3062c9":"img","a5aa962b":"x,y,w,h","8f6cb397":"img.shape","4e7f6919":"plt.imshow(img)","ac37790d":"mas=plt.imread('..\/input\/lgg-mri-segmentation\/kaggle_3m\/TCGA_CS_4941_19960909\/TCGA_CS_4941_19960909_11_mask.tif',0)","e585e72c":"plt.imshow(mas)\nplt.show()","c98c4bed":"# 1.1 **Unet architecture**","8eba36e2":"**Segmentation Predict**","ed3c2b71":"**Optimization and CallBacks**","ed19af9c":"**Show 5 row**","e219cad4":"# 2.5 **Accuracy**","337428e5":"# *Read path of image*","e35e6310":"**Show row of Not Tumor**","42895c69":"**Compile model**","e24a82cc":"**Compile Model**","fd6c4f9a":"**Fitting Model**","ef7a4bd6":"# 1.3 **Create Model Unet**","58a3d216":"# 2.1 **Read Data and augmentation**","3bda67d6":"**show 5 path of image**","e4dc9a41":"# 1.0 **Segmentation Model**","ff6f04f9":"**Reshape image to do threshold**","ba3472c8":"**Take 5 row of each class**","7e541d0d":"# **Show image and Mask of each Class**","88d12a0a":"**Show 5 path of mask**","a6aab177":"# 4.0 **Final Result of This notebook**","fa95fdbe":"**Callbacks**","ce99954f":"**Show Mask**","da58d65c":"# 1.5 **Predction**","572d7d28":"# **import library**","cdd2fef4":"**Show Result**","1618c500":"**Fitting Model**","2ce06d55":"# 2.4 **Prediction**","f03eaa1d":"<img src=\"https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/rsna\/IoU.jpg\">\n<img src=\"https:\/\/d3i71xaburhd42.cloudfront.net\/8575e8beef47bd2880c92f54a749f933db983e56\/2-Table1-1.png\">\n\n","052c3d78":"**Classification predict**","78bd1dca":"**Validation Data**","1c5deb98":"# 1.2 **Read Data and augmentation**","4f052130":"**Draw Contours**","298071d0":"**Show row of Tumor**","328e3022":"**DataFrame of path iamge & mask & label**","813b8234":"**Test Data**","0d0d8359":"# 2.0 **Classification Model**","ce64bfbe":"**show Length of each class**","7000c144":"# **Parameters Of Fitiing**","2d2792a4":" <img src=\"http:\/\/tuatini.me\/content\/images\/2017\/09\/u-net-architecture.png\">","b8e36796":"# 2.3 **visualization Accuracy and Loss**","5d715739":"# 2.2 **Build Model**","0bc3934f":"**Show Shape of predict segmentation**","d3c07c0b":"**visualization The prediction**","8984069e":"# **Funcrion To augmentation and Scalling**","a7022176":"**Resize image and scalling**","af350a55":"# 3.0 use A single Image (Calssification And Segmentation)","9fd88b8d":"# **Split data train (0.9) valid (0.5) test (0.05)**","91f5e898":"# 1.4 **visualization Accuracy and Loss**","1591c2fc":"# 3.1 **Read image**","3fe60f6a":"**Train Data**","e5a796ed":"**Finding Contours To Draw**","e70f81f9":"**Boundry Box**","b0cd0291":"**Summary of model**"}}