{"cell_type":{"ff77a787":"code","82907a3c":"code","f5302509":"code","9c33a2d4":"code","74a68114":"code","527446b2":"code","801effb2":"code","5992089a":"code","50aae054":"code","f211929c":"code","c55bb0a7":"code","c7a459e7":"code","ca2d8b7b":"code","61335886":"code","1dbf86b0":"code","ef66ddad":"code","fa2928b8":"code","85bc79ac":"code","9da1190f":"code","7c628e12":"code","30aabe77":"code","5a004209":"code","5979c920":"code","fe40cc45":"code","59891d44":"code","9f81649f":"code","dc2b2ac1":"code","97b60469":"code","027ddab4":"code","bf0584cf":"code","88f5fa13":"code","e2178f28":"code","69804317":"code","ebd294be":"code","7daef46d":"code","c463a869":"code","8151d7e0":"code","196b5356":"code","6fe1ad39":"code","cb44304f":"code","91c7cc4c":"code","f5d07078":"code","5ca21c91":"code","c66c7801":"code","e918f60a":"code","c10d1611":"code","00e2d0d4":"code","e0e0de06":"code","72f8e791":"code","f21a2e6a":"code","cde80d53":"code","6d68952a":"code","ebdd29d0":"code","5e5a6109":"code","21e847bb":"code","08f0dc01":"code","16847028":"code","5c4ef497":"code","f9fcafd7":"code","6714b693":"code","205bbbae":"code","00933668":"code","cd9c8f07":"code","655cd4df":"code","2b20a68d":"code","2d55f1f8":"code","3078f96f":"code","9722bd5d":"code","db3af5c2":"code","224757bd":"code","2e2cca5d":"code","32e068f6":"code","ff969ca8":"code","f43c6a52":"code","e6883afa":"markdown","0f26f129":"markdown","35f6b668":"markdown","5be721fb":"markdown","22355887":"markdown","57d55121":"markdown","7b2ce2b3":"markdown","35aafcb2":"markdown","a5dcbe32":"markdown","05167a53":"markdown","75b723ca":"markdown","a7cf1c44":"markdown","1d0cb54f":"markdown","92818422":"markdown","65285a6d":"markdown","189a6a05":"markdown","6f6d9e1c":"markdown","1214465d":"markdown","6790a38c":"markdown","b8522299":"markdown","84e581b9":"markdown","cae7c16c":"markdown","9fb30abc":"markdown","f6ff540b":"markdown","a79d2fd2":"markdown","3e7c1617":"markdown","37de3eac":"markdown"},"source":{"ff77a787":"# manipulation data\nimport pandas as pd\nimport numpy as np\n\n#visualiation data\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport matplotlib\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n#default theme\nsns.set(context='notebook', style='darkgrid', palette='colorblind', font='sans-serif', font_scale=1, rc=None)\nmatplotlib.rcParams['figure.figsize'] =[8,8]\nmatplotlib.rcParams.update({'font.size': 15})\nmatplotlib.rcParams['font.family'] = 'sans-serif'","82907a3c":"train = pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')\ntrain.head(6)","f5302509":"train.info()","9c33a2d4":"\ntrain.dtypes.value_counts().plot.pie(explode=[0.1,0.1],autopct='%1.1f%%',shadow=True)\nplt.title('type of our data');","74a68114":"train.columns","527446b2":"train.describe()","801effb2":"train.isnull().sum()","5992089a":"train.hist(figsize=(15,15),edgecolor='black');","50aae054":"train.DEATH_EVENT.value_counts().plot.pie(explode=[0.1,0.1],autopct='%1.1f%%',shadow=True)\nplt.title('the % of deaths')","f211929c":"plt.figure(figsize=(20,6))\nsns.countplot(x='age',data=train)\nplt.xticks(rotation=90)\nplt.title('the ages of our persone')","c55bb0a7":"# Distribution of Age\n\n\nfig = go.Figure()\nfig.add_trace(go.Histogram(\n    x = train['age'],\n    xbins=dict( # bins used for histogram\n        start=40,\n        end=95,\n        size=2\n    ),\n    marker_color='#e8ab60',\n    opacity=1\n))\n\nfig.update_layout(\n    title_text='Distribution of Age',\n    xaxis_title_text='AGE',\n    yaxis_title_text='COUNT', \n    bargap=0.05, # gap between bars of adjacent location coordinates\n    xaxis =  {'showgrid': False },\n    yaxis = {'showgrid': False },\n    template = 'presentation'\n)\n\nfig.show()","c7a459e7":"# Distribution of AGE Vs DEATH_EVENT\n\nfig = px.histogram(train, x=\"age\", color=\"DEATH_EVENT\", marginal=\"violin\", hover_data=train.columns, \n                   title =\"Distribution of AGE Vs DEATH_EVENT\", \n                   labels={\"age\": \"AGE\"},\n                   template=\"plotly\",\n                   \n                   \n                  )\nfig.show()","ca2d8b7b":"train.sex.value_counts().plot.pie(explode=[0.1,0.1],autopct='%1.1f%%',shadow=True)","61335886":"sns.countplot(x='sex',hue='DEATH_EVENT',data=train)\nplt.legend(['yes','no'])","1dbf86b0":"import plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nd1 = train[(train[\"DEATH_EVENT\"]==0) & (train[\"sex\"]==1)]\nd2 = train[(train[\"DEATH_EVENT\"]==1) & (train[\"sex\"]==1)]\nd3 = train[(train[\"DEATH_EVENT\"]==0) & (train[\"sex\"]==0)]\nd4 = train[(train[\"DEATH_EVENT\"]==1) & (train[\"sex\"]==0)]\n\nlabel1 = [\"Male\",\"Female\"]\nlabel2 = ['Male - Survived','Male - Died', \"Female -  Survived\", \"Female - Died\"]\n\nvalues1 = [(len(d1)+len(d2)), (len(d3)+len(d4))]\nvalues2 = [len(d1),len(d2),len(d3),len(d4)]\n\n# Create subplots: use 'domain' type for Pie subplot\nfig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]])\nfig.add_trace(go.Pie(labels=label1, values=values1, name=\"GENDER\"),\n              1, 1)\nfig.add_trace(go.Pie(labels=label2, values=values2, name=\"GENDER VS DEATH_EVENT\"),\n              1, 2)\n\n# Use `hole` to create a donut-like pie chart\nfig.update_traces(hole=.4, hoverinfo=\"label+percent\")\n\nfig.update_layout(\n    title_text=\"GENDER DISTRIBUTION IN THE DATASET  \\\n                   GENDER VS DEATH_EVENT\",\n    # Add annotations in the center of the donut pies.\n    annotations=[dict(text='GENDER', x=0.19, y=0.5, font_size=10, showarrow=False),\n                 dict(text='GENDER VS DEATH_EVENT', x=0.84, y=0.5, font_size=9, showarrow=False)],\n    autosize=False,width=1200, height=500, paper_bgcolor=\"white\")\n\nfig.show()","ef66ddad":"sns.barplot(x='sex',y='smoking',hue='DEATH_EVENT',data=train);","fa2928b8":"sns.countplot(x='sex',hue='smoking',data=train)\nplt.legend(['yes','no']);","85bc79ac":"sns.countplot(x='sex',hue='diabetes',data=train)\nplt.legend(['yes','no']);","9da1190f":"train.diabetes.value_counts().plot.pie(explode=[0.1,0.1],autopct='%2.2f%%',shadow=True)","7c628e12":"sns.countplot(x='diabetes',hue='DEATH_EVENT',data=train)\nplt.legend(['yes','no']);","30aabe77":"sns.boxplot(x = train.ejection_fraction, color = 'green')\nplt.show()","5a004209":"train[train['ejection_fraction']>=70]","5979c920":"train = train[train['ejection_fraction']<70]","fe40cc45":"import plotly.graph_objects as go\n\nfig = go.Figure()\nfig.add_trace(go.Histogram(\n    x = train['ejection_fraction'],\n    xbins=dict( # bins used for histogram\n        start=14,\n        end=80,\n        size=2\n    ),\n    marker_color='#A7F432',\n    opacity=1\n))\n\nfig.update_layout(\n    title_text='EJECTION FRACTION DISTRIBUTION',\n    xaxis_title_text='EJECTION FRACTION',\n    yaxis_title_text='COUNT', \n    bargap=0.05, # gap between bars of adjacent location coordinates\n\n    template = 'plotly_dark'\n)\n\nfig.show()","59891d44":"sns.boxplot(x=train.time, color = 'yellow')\nplt.show()","9f81649f":"sns.boxplot(x=train.serum_creatinine, color = 'red')\nplt.show()","dc2b2ac1":"# Before dealing with outliers we require knowledge about the outlier, the dataset and possibly some domain knowledge.\n\n# Removing outliers without a good reason will not always increase accuracy. Without a deep understanding of what are the possible ranges that\n# exist within each feature, removing outliers becomes tricky.\n\n# When I researched a bit I found that all the values in serum_creatinine falls in possible range of values. So they are not outliers. \n# They are actual data points that helps in predicting DEATH_EVENT.","97b60469":"train.corr().style.background_gradient(cmap='coolwarm').set_precision(2)","027ddab4":"# Feature Selection\n\nplt.rcParams['figure.figsize']=15,6 \nsns.set_style(\"darkgrid\")\n\nx = train.iloc[:, :-1]\ny = train.iloc[:,-1]\n\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nmodel = ExtraTreesClassifier()\nmodel.fit(x,y)\nprint(model.feature_importances_) \nfeat_importances = pd.Series(model.feature_importances_, index=x.columns)\nfeat_importances.nlargest(12).plot(kind='barh')\nplt.show()","bf0584cf":"train=train.drop(['anaemia','creatinine_phosphokinase','diabetes','high_blood_pressure','platelets','sex','smoking','age'],axis=1)","88f5fa13":"train","e2178f28":"train.corr().style.background_gradient(cmap='coolwarm').set_precision(3)","69804317":"from sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score","ebd294be":"x=train.drop('DEATH_EVENT',axis=1)\ny=train.DEATH_EVENT","7daef46d":"print(x.shape)\nprint(y.shape)","c463a869":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3)","8151d7e0":"print(x_train)\nprint(y_test)","196b5356":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","6fe1ad39":"# Making Confusion Matrix and calculating accuracy score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\nmodel = LogisticRegression()\n\n#Fit the model\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\n\nmylist = []\n# Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\n# accuracy score\nacc_logreg = accuracy_score(y_test, y_pred)\n\nmylist.append(acc_logreg)\nprint(cm)\nprint(acc_logreg)","cb44304f":"# Finding the optimum number of neighbors \n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\nlist1 = []\nfor neighbors in range(3,10):\n    classifier = KNeighborsClassifier(n_neighbors=neighbors, metric='minkowski')\n    classifier.fit(x_train, y_train)\n    y_pred = classifier.predict(x_test)\n    list1.append(accuracy_score(y_test,y_pred))\nplt.plot(list(range(3,10)), list1)\nplt.show()","91c7cc4c":"# Training the K Nearest Neighbor Classifier on the Training set\n\nclassifier = KNeighborsClassifier(n_neighbors=5)\nclassifier.fit(x_train, y_train)","f5d07078":"# Predicting the Test set results\n\ny_pred = classifier.predict(x_test)\nprint(y_pred)","5ca21c91":"# Making the confusion matrix and calculating accuracy score\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nacc_knn = accuracy_score(y_test, y_pred)\nmylist.append(acc_knn)\nprint(cm)\nprint(acc_knn)","c66c7801":"from sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlist1 = []\nfor c in [0.5,0.6,0.7,0.8,0.9,1.0]:\n    classifier = SVC(C = c, random_state=0, kernel = 'rbf')\n    classifier.fit(x_train, y_train)\n    y_pred = classifier.predict(x_test)\n    list1.append(accuracy_score(y_test,y_pred))\nplt.plot([0.5,0.6,0.7,0.8,0.9,1.0], list1)\nplt.show()","e918f60a":"# Training the Support Vector Classifier on the Training set\n\nfrom sklearn.svm import SVC\nclassifier = SVC(C = 0.7, random_state=0, kernel = 'rbf')\nclassifier.fit(x_train, y_train)","c10d1611":"# Predicting the test set results\n\ny_pred = classifier.predict(x_test)\nprint(y_pred)","00e2d0d4":"# Making the confusion matrix and calculating accuracy score\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nacc_svc = accuracy_score(y_test, y_pred)\nprint(cm)\nprint(acc_svc)\nmylist.append(acc_svc)","e0e0de06":"# Finding the optimum number of max_leaf_nodes\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlist1 = []\nfor leaves in range(2,15):\n    classifier = DecisionTreeClassifier(max_leaf_nodes = leaves, random_state=0, criterion='entropy')\n    classifier.fit(x_train, y_train)\n    y_pred = classifier.predict(x_test)\n    list1.append(accuracy_score(y_test,y_pred))\n#print(mylist)\nplt.plot(list(range(2,15)), list1)\nplt.show()","72f8e791":"# Training the Decision Tree Classifier on the Training set\n\nclassifier = DecisionTreeClassifier(max_leaf_nodes = 10, random_state=0, criterion='entropy')\nclassifier.fit(x_train, y_train)","f21a2e6a":"# Predicting the test set results\n\ny_pred = classifier.predict(x_test)\nprint(y_pred)","cde80d53":"# Making the confusion matrix and calculating accuracy score\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nacc_decisiontree = accuracy_score(y_test, y_pred)\nprint(cm)\nprint(acc_decisiontree)\nmylist.append(acc_decisiontree)","6d68952a":"#Finding the optimum number of n_estimators\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlist1 = []\nfor estimators in range(10,30):\n    classifier = RandomForestClassifier(n_estimators = estimators, random_state=0, criterion='entropy')\n    classifier.fit(x_train, y_train)\n    y_pred = classifier.predict(x_test)\n    list1.append(accuracy_score(y_test,y_pred))\n#print(mylist)\nplt.plot(list(range(10,30)), list1)\nplt.show()","ebdd29d0":"# Training the RandomForest Classifier on the Training set\n\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 15, criterion='entropy', random_state=0)\nclassifier.fit(x_train,y_train)","5e5a6109":"# Predicting the test set results\n\ny_pred = classifier.predict(x_test)\nprint(y_pred)","21e847bb":"# Making the confusion matrix and calculating the accuracy score\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nacc_randomforest = accuracy_score(y_test, y_pred)\nmylist.append(acc_randomforest)\nprint(cm)\nprint(acc_randomforest)","08f0dc01":"np.random.seed(0)\nimport tensorflow as tf\n\n# Initialising the ANN\n\nann = tf.keras.models.Sequential()","16847028":"# Adding the input layer and the first hidden layer\n\nann.add(tf.keras.layers.Dense(units = 7, activation = 'relu'))","5c4ef497":"# Adding the second hidden layer\n\nann.add(tf.keras.layers.Dense(units = 7, activation = 'relu'))","f9fcafd7":"# Adding the third hidden layer\n\nann.add(tf.keras.layers.Dense(units = 7, activation = 'relu'))","6714b693":"# Adding the fourth hidden layer\n\nann.add(tf.keras.layers.Dense(units = 7, activation = 'relu'))","205bbbae":"# Adding the output layer\n\nann.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))","00933668":"# Compiling the ANN\n\nann.compile(optimizer = 'adam', loss = 'binary_crossentropy' , metrics = ['accuracy'] )","cd9c8f07":"# Training the ANN on the training set\n\nann.fit(x_train, y_train, batch_size = 16, epochs = 100)","655cd4df":"# Predicting the test set results\n\ny_pred = ann.predict(x_test)\ny_pred = (y_pred > 0.5)\nnp.set_printoptions()\n","2b20a68d":"# Making the confusion matrix, calculating accuracy_score \n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\n# confusion matrix\ncm = confusion_matrix(y_test,y_pred)\nprint(\"Confusion Matrix\")\nprint(cm)\nprint()\n\n# accuracy\nac_ann = accuracy_score(y_test,y_pred)\nprint(\"Accuracy\")\nprint(ac_ann)\nmylist.append(ac_ann)","2d55f1f8":"from xgboost import XGBClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlist1 = []\nfor estimators in range(10,30,1):\n    classifier = XGBClassifier(n_estimators = estimators, max_depth=12, subsample=0.7)\n    classifier.fit(x_train, y_train)\n    y_pred = classifier.predict(x_test)\n    list1.append(accuracy_score(y_test,y_pred))\n#print(mylist)\nplt.plot(list(range(10,30,1)), list1)\nplt.show()","3078f96f":"from xgboost import XGBClassifier\nclassifier = XGBClassifier(n_estimators = 10, max_depth=12, subsample=0.7)\nclassifier.fit(x_train,y_train)","9722bd5d":"y_pred = classifier.predict(x_test)\nprint(y_pred)","db3af5c2":"# Making the confusion matrix and calculating the accuracy score\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac_xgboost = accuracy_score(y_test, y_pred)\nmylist.append(ac_xgboost)\nprint(cm)\nprint(ac_xgboost)","224757bd":"from catboost import CatBoostClassifier\nclassifier = CatBoostClassifier()\nclassifier.fit(x_train, y_train)","2e2cca5d":"y_pred = classifier.predict(x_test)\nprint(y_pred)","32e068f6":"# Making the confusion matrix and calculating the accuracy score\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac_catboost = accuracy_score(y_test, y_pred)\nmylist.append(ac_catboost)\nprint(cm)\nprint(ac_catboost)","ff969ca8":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'ANN',   \n              'Decision Tree','xgboost','catboost'],\n    'Score': [acc_svc, acc_knn, acc_logreg, \n              acc_randomforest, ac_ann, acc_decisiontree,ac_xgboost,ac_catboost\n              ]})\nmodels.sort_values(by='Score', ascending=False)","f43c6a52":"plt.rcParams['figure.figsize']=15,6 \nsns.set_style(\"darkgrid\")\nax = sns.barplot(x=models.Model, y=models.Score, palette = \"rocket\", saturation =1.5)\nplt.xlabel(\"Classifier Models\", fontsize = 20 )\nplt.ylabel(\"% of Accuracy\", fontsize = 20)\nplt.title(\"Accuracy of different Classifier Models\", fontsize = 20)\nplt.xticks(fontsize = 12, horizontalalignment = 'center', rotation = 8)\nplt.yticks(fontsize = 13)\nfor p in ax.patches:\n    width, height = p.get_width(), p.get_height()\n    x, y = p.get_xy() \n    ax.annotate(f'{height:.2%}', (x + width\/2, y + height*1.02), ha='center', fontsize = 'x-large')\nplt.show()","e6883afa":"# 3. finding missing values","0f26f129":"##  split data","35f6b668":"### death events","5be721fb":"### Columns description\n\n1. anaemia:Decrease of red blood cells or hemoglobin (boolean)\n2. creatinine_phosphokinase:Level of the CPK enzyme in the blood (mcg\/L)\n3. diabetes:If the patient has diabetes (boolean)\n4. ejection_fraction:Ejection fraction (EF) is a measurement, expressed as a percentage, of how much blood the left ventricle pumps out with each contraction\n5. high_blood_pressure:blood hypertension\n6. platelets:are a component of blood whose function (along with the coagulation factors)\n7. serum_creatinine:Serum creatinine is widely interpreted as a measure only of renal function\n8. serum_sodium: to see how much sodium is in your blood it is particularly important for nerve and muscle function.\n\n![](https:\/\/miro.medium.com\/max\/4420\/1*HDphOMQdTsRUM-O4hudIWA.png)","22355887":"## feature selection","57d55121":"### diabet","7b2ce2b3":"# 1. import library","35aafcb2":"### ejection_fraction","a5dcbe32":"### like we can c that some of our feature had a corrolation almost aqual to 0 so we gonna drop them like :\n* anaemia\n* creatinine_phosphokinase\n* diabetes\n* high_blood_pressure\n* platelets\n* sex\n* smoking\n\n\nWe will select only 3 features : time, ejection_fraction, serum_creatinine ","05167a53":"No outliers in time ","75b723ca":"### ejection_fraction\n","a7cf1c44":"# 4. visualization","1d0cb54f":"## KNN","92818422":"## catboost","65285a6d":"## DecisionTreeClassifier","189a6a05":"We can see there are two outliers. Lets remove them (70 and 80) ","6f6d9e1c":"## xgboost ","1214465d":"### Age","6790a38c":"# 2. data analysis","b8522299":"### sex","84e581b9":"## Support Vector Machines","cae7c16c":"### Logistic Regression","9fb30abc":"## features selection ","f6ff540b":"## ANN (neural network )","a79d2fd2":"### Time","3e7c1617":"## RANDOM FOREST CLASSIFCATION","37de3eac":"## Feature Scaling"}}