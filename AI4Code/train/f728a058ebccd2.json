{"cell_type":{"bf57f9c6":"code","9792e0dd":"code","74ddd621":"code","c876063f":"code","d95cc434":"code","82b5044f":"code","91e1f6ef":"code","8b5a2349":"code","e0a30642":"code","a7df1e35":"code","55b35c39":"code","d23771d6":"code","702b4279":"code","8cb23be4":"code","c6186271":"code","f8983d7c":"code","f65ce5e3":"code","61c860d9":"code","045a6770":"code","2f9b9e16":"code","99cc5452":"code","06573d9e":"code","eb82f394":"code","f25fbdfe":"code","4422bd7d":"code","eabe751e":"code","051522b9":"code","28d007d8":"code","bb300691":"code","48dcbc5f":"code","fa245787":"code","d3e61892":"code","88255121":"code","c609f129":"code","8fc72ab8":"code","e927752a":"code","eaebca9a":"code","e6b20a60":"code","f77cd6f7":"code","5e442adb":"code","104cdd54":"code","6a09ff9e":"code","98ae2222":"code","77ab3d04":"code","4f39e555":"code","0c020257":"code","d61a06cd":"code","5b425d32":"code","e54088dc":"code","2ffc2438":"code","6a88d0f4":"code","3f16586e":"code","0a90b941":"code","56dc00e6":"code","75c61501":"code","4fd6cec5":"code","8998d56f":"code","36fdeb23":"code","531ad0fe":"code","98d8c9c4":"code","bc9ce39a":"code","78cef2ba":"code","8c1fd273":"code","85b8930e":"code","4472ed70":"code","7da5bed5":"code","d3c8207d":"code","ddd2c69b":"code","b40ee58f":"code","f29850f5":"markdown","b7ff362e":"markdown"},"source":{"bf57f9c6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","9792e0dd":"#reading file and name it df\ndf = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\n","74ddd621":"df['BsmtQual'] = df['BsmtQual'].fillna(1) # we have some NaNs there - let's replace them to 1\ndf = df.replace({\"BsmtQual\" : { \"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\" : 4, \"Ex\" : 5}, \"ExterQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5}, \"KitchenQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5}})\ndf['Bath'] = df[\"BsmtFullBath\"] + (0.5 * df[\"BsmtHalfBath\"]) + df[\"FullBath\"] + (0.5 * df[\"HalfBath\"])\ndummies = pd.get_dummies(df['Foundation']).astype(int)\ndummiesHeat = pd.get_dummies(df['HeatingQC']).astype(int)\n\ndf['YrBltAndRemod']=df['YearBuilt']+df['YearRemodAdd']\ndf['MSSubClass'] = df['MSSubClass'].apply(str)\ndf['YrSold'] = df['YrSold'].astype(str)\ndf['MoSold'] = df['MoSold'].astype(str)\ndf[\"Functional\"] = df[\"Functional\"].fillna(\"Typ\")","c876063f":"df = df.fillna(0)\ndf = pd.get_dummies(df)\ndf.head()","d95cc434":"y = df['SalePrice']\n","82b5044f":"# checking numeric columns\nnumeric_columns = df.select_dtypes(include=[np.number])\nnumeric_columns","91e1f6ef":"# checking for zeroes in numeric coils\n(numeric_columns == 0).sum().sort_values(ascending = False).head(30)","8b5a2349":"## some middle feature selection and engeneering\n\n\ndf = df.drop(df[df['GrLivArea'] > 3500].index)\ndf = df.drop(df[df['GarageArea'] > 1200].index)\ndf = df.drop(df[df['SalePrice'] > 350000].index) \n#df = df.drop(df[df['TotalBsmtSF'] > 2400].index)\n\n\ndf[\"AllSF\"] = df[\"GrLivArea\"] + df[\"TotalBsmtSF\"]\ndf['Garage'] = df['GarageCars'] * df['GarageArea']\n\ndf = df.drop(df[df['Garage'] > 3500].index)\n#df['Front*Area'] = df['LotFrontage'] * df['LotArea'] ##worse\n#df = df.drop(df[df['AllSF'] > 4000].index)\n\n\ndef year_category(yb):\n    if yb <= 1910:\n        return 1\n    elif yb <= 1950 and yb > 1910:\n        return 2\n    elif yb >= 1950 and yb < 1980:\n        return 3\n    elif yb >= 1980 and yb < 2000:\n        return 4\n    return 5\n\ndf['YearBuilt_cat'] = df['YearBuilt'].apply(year_category)","e0a30642":"\n# delete numeric features with zero-values more than 50%\nnumeric_columns = numeric_columns.loc[:, ((numeric_columns == 0).sum(axis=0) <= len(numeric_columns.index)*0.5)]\nnumeric_columns","a7df1e35":"#checking for NaNs in numeric features\nnumeric_columns.isnull().sum().sort_values(ascending = False).head()","55b35c39":"# checking for Object columns\nobject_columns = df.select_dtypes(include=[np.object])\nobject_columns","d23771d6":"# checking for NANs in objects\n\nobject_columns.isnull().sum().sort_values(ascending = False).head(17)","702b4279":"# removing objects with nan-values more than 50% in frame\nobject_columns = object_columns.loc[:, (object_columns.isnull().sum(axis=0) <= len(df.index)*0.5)]\n","8cb23be4":"# replace NaNs with MODE\nfor col in object_columns:\n    object_columns[col].fillna('None', inplace = True)","c6186271":"# checking for zeros in df\n#(df == 0).sum().sort_values(ascending = False).head(30)","f8983d7c":"# delete features with zero-values more than 50%\n#df = df.loc[:, ((df == 0).sum(axis=0) <= len(df.index)*0.5)]","f65ce5e3":"# simple imputer for zeroes features - strategy 'median'\n#df = df.replace(0, df.median())","61c860d9":"# checking for 'null' data in dataframe\n#df.isnull().sum().sort_values(ascending = False).head(20)","045a6770":"# removing of features with nan-values more than 50% \n#df = df.loc[:, (df.isnull().sum(axis=0) <= len(df.index)*0.5)]","2f9b9e16":"#s = (df.dtypes == 'object')\n#object_cols = list(s[s].index)\n#object_cols","99cc5452":"#for col in columns:\n    #df[col].fillna(df[col].mode().values[0], inplace = True)","06573d9e":"# replace NaN to MODE\n#from statistics import mode\n#df = df.fillna(mode)","eb82f394":"'''\ns = (df.dtypes == 'object')\nobject_cols = list(s[s].index)\nobject_cols\n'''\n","f25fbdfe":"# Apply label encoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom category_encoders import TargetEncoder\nfrom category_encoders.cat_boost import CatBoostEncoder\nle = LabelEncoder()\n#le = TargetEncoder(cols=object_cols)\n#le = CatBoostEncoder(cols=object_cols)\nfor col in object_columns:\n    object_columns[col] = le.fit_transform(object_columns[col].astype(str))","4422bd7d":"#from eli5.sklearn import PermutationImportance","eabe751e":"df = pd.concat([object_columns, numeric_columns], axis=1)","051522b9":"'''\nimport itertools\nfrom sklearn import preprocessing, metrics\ninteractions = pd.DataFrame(index=df.index)\nfor col1, col2 in itertools.combinations(object_cols, 2):\n    new_col_name = '_'.join([col1, col2])\n\n    # Convert to strings and combine\n    new_values = df[col1].map(str) + \"_\" + df[col2].map(str)\n\n    encoder = preprocessing.LabelEncoder()\n    interactions[new_col_name] = encoder.fit_transform(new_values)\ndf = df.join(interactions)\n'''","28d007d8":"## final feature engeneering\n\n\ndf[\"Qual\"] = df[\"ExterQual\"] * df[\"KitchenQual\"] * df[\"BsmtQual\"]\ndf['OverallQual_2'] = df['OverallQual']**2\n#df['OverallQual_3'] = df['OverallQual']**3 \n#df[\"OverallQual-Sq\"] = np.sqrt(df[\"OverallQual\"]) #worse\n#df['OverallQual_log'] = np.log1p('OverallQual')\n\n\n#df['AllSF_2'] = df['AllSF']**2 ##worse\n#df['AllSF_3'] = df['AllSF']**3 ## worse\n#df['AllSF-Sq'] = np.sqrt(df['AllSF']) ##worse\n#df['AllSF-log'] = np.log1p(df['AllSF'])\n\n#df['GarageArea_2'] = df['GarageArea']**2 #worse\n#df['GarageArea_3'] = df['GarageArea']**3 #worse\n#df['GarageArea-Sq'] = np.sqrt(df['GarageArea']) #worse\n\n#df['YearBuilt_cat_2'] = df['YearBuilt_cat']**2 #worse\n#df['YearBuilt_cat_3'] = df['YearBuilt_cat']**3 #worse\n#df['YearBuilt_cat-Sq'] = np.sqrt(df['YearBuilt_cat']) #worse\n\n#df['Qual_2'] = df['Qual']**2 \n#df['Qual_3'] = df['Qual']**3\n#df['Qual-Sq'] = np.sqrt(df['Qual']) \n\n#df['Bath_2'] = df['Bath']**2 \n#df['Bath_3'] = df['Bath']**3\n#df['Bath-Sq'] = np.sqrt(df['Bath']) \n\n#df['Garage_2'] = df['Garage']**2 \n#df['Garage_3'] = df['Garage']**3\n#df['Garage-Sq'] = np.sqrt(df['Garage']) \n\ndf['FinalQual'] = df['OverallQual_2']*df['Qual']\n#df['FinalQual1'] = df['OverallQual']*df['Qual-Sq']\n#df['FinalQual2'] = df['OverallQual_2']*df['Qual']\n#df['FinalQual3'] = df['OverallQual']*df['Qual']\n\n\n#df['Fireplaces_2'] = df['Fireplaces']**2 \n#df['Fireplaces_3'] = df['Fireplaces']**3\ndf['Fireplaces-Sq'] = np.sqrt(df['Fireplaces'])\n\ndf['FireRooms'] = df['TotRmsAbvGrd']*df['Fireplaces-Sq']\n\n#df['OpenPorchSF_2'] = df['OpenPorchSF']**2 \n#df['OpenPorchSF_3'] = df['OpenPorchSF']**3\n#df['OpenPorchSF-Sq'] = np.sqrt(df['OpenPorchSF']) \n#df['OpenPorchSF-log'] = np.log1p(df['OpenPorchSF'])\n\n#df['FinalQual-log'] = np.log1p(df['FinalQual']) \n\n#scaler = RobustScaler()\n#df['AllSF'] = scaler.fit_transform(df[['AllSF']])\n\n#scaler = StandardScaler()\n#df[['Garage', 'Y', 'AllSF']] = scaler.fit_transform(df[['Garage', 'Y', 'AllSF']])\n\n#df['Y'] = np.log1p(df['Y'])\n\n\n#df['Foundation_2'] = df['Foundation']**2 \n#df['Foundation_3'] = df['Foundation']**3\n#df['Foundation-Sq'] = np.sqrt(df['Foundation'])\n#df['Foundation-log'] = np.log1p(df['Foundation'])","bb300691":"df.mean()","48dcbc5f":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndf[['LotArea', 'YearBuilt', 'YearRemodAdd', 'TotalBsmtSF', '1stFlrSF', '1stFlrSF', 'GrLivArea']] = scaler.fit_transform(df[['LotArea', 'YearBuilt', 'YearRemodAdd', 'TotalBsmtSF', '1stFlrSF', '1stFlrSF', 'GrLivArea']])","fa245787":"df.mean()","d3e61892":"df.corr(method='pearson', min_periods=1).tail(60)","88255121":"#Correlation with output variable\ncor = df.corr()\ncor_target = (cor['SalePrice'])\n#Selecting highly correlated features\nrelevant_features = cor_target[(cor_target<=-0.3) | (cor_target>=0.3) ]\nrelevant_features.sort_values(ascending = False).head(60)\n","c609f129":"#correlation matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt  # Matlab-style plotting\ncorrmat = df.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);\n#saleprice correlation matrix\nk = 15 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","8fc72ab8":"features = relevant_features.keys().tolist()\nfeatures","e927752a":"#df = pd.DataFrame(df, columns = features)\ndf = df[features]\ndf","eaebca9a":"df = df.reset_index()\n","e6b20a60":"#ignore 'error' in case of feature is not in frame\nX = df.drop(['SalePrice'], axis=1, errors = 'ignore')\n\n","f77cd6f7":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=220)","5e442adb":"from sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\n\ndef cross_val(model):\n    pred = cross_val_score(model, X, y, cv=10)\n    return pred.mean()\n\ndef print_evaluate(true, predicted):  \n    mae = metrics.mean_absolute_error(true, predicted)\n    mse = metrics.mean_squared_error(true, predicted)\n    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n    r2_square = metrics.r2_score(true, predicted)\n    print('MAE:', mae)\n    print('MSE:', mse)\n    print('RMSE:', rmse)\n    print('R2 Square', r2_square)\n    \ndef evaluate(true, predicted):\n    mae = metrics.mean_absolute_error(true, predicted)\n    mse = metrics.mean_squared_error(true, predicted)\n    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n    r2_square = metrics.r2_score(true, predicted)\n    return mae, mse, rmse, r2_square","104cdd54":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression(normalize=True)\nlin_reg.fit(X_train,y_train)","6a09ff9e":"# print the intercept\nprint(lin_reg.intercept_)","98ae2222":"coeff_df = pd.DataFrame(lin_reg.coef_, X.columns, columns=['Coefficient'])\ncoeff_df","77ab3d04":"pred = lin_reg.predict(X_test)","4f39e555":"plt.scatter(y_test, pred)","0c020257":"sns.distplot((y_test - pred), bins=50);","d61a06cd":"print_evaluate(y_test, lin_reg.predict(X_test))","5b425d32":"results_df = pd.DataFrame(data=[[\"Linear Regression\", *evaluate(y_test, pred) , cross_val(LinearRegression())]], \n                          columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\nresults_df","e54088dc":"from sklearn.linear_model import RANSACRegressor\n\nmodel = RANSACRegressor()\nmodel.fit(X_train, y_train)\n\npred = model.predict(X_test)\nprint_evaluate(y_test, pred)","2ffc2438":"results_df_2 = pd.DataFrame(data=[[\"Robust Regression\", *evaluate(y_test, pred) , cross_val(RANSACRegressor())]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","6a88d0f4":"from sklearn.linear_model import Ridge\n\nmodel = Ridge()\nmodel.fit(X_train, y_train)\npred = model.predict(X_test)\n\nprint_evaluate(y_test, pred)","3f16586e":"results_df_2 = pd.DataFrame(data=[[\"Ridge Regression\", *evaluate(y_test, pred) , cross_val(Ridge())]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","0a90b941":"from sklearn.linear_model import Lasso\n\nmodel = Lasso()\nmodel.fit(X_train, y_train)\npred = model.predict(X_test)\n\nprint_evaluate(y_test, pred)","56dc00e6":"results_df_2 = pd.DataFrame(data=[[\"Lasso Regression\", *evaluate(y_test, pred) , cross_val(Lasso())]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","75c61501":"from sklearn.linear_model import ElasticNet\n\nmodel = ElasticNet()\nmodel.fit(X_train, y_train)\npred = model.predict(X_test)\n\nprint_evaluate(y_test, pred)","4fd6cec5":"results_df_2 = pd.DataFrame(data=[[\"Elastic Net Regression\", *evaluate(y_test, pred) , cross_val(ElasticNet())]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","8998d56f":"from sklearn.tree import DecisionTreeRegressor\n\nmodel = DecisionTreeRegressor(max_leaf_nodes=100, random_state=1)\nmodel.fit(X_train, y_train)\npred = model.predict(X_test)\n\nprint_evaluate(y_test, pred)","36fdeb23":"results_df_2 = pd.DataFrame(data=[[\"Decision Tree\", *evaluate(y_test, pred) , cross_val(DecisionTreeRegressor())]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","531ad0fe":"from sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(n_estimators=50, random_state=2, max_leaf_nodes=230)\nmodel.fit(X_train, y_train)\npred = model.predict(X_test)\n\nprint_evaluate(y_test, pred)","98d8c9c4":"results_df_2 = pd.DataFrame(data=[[\"Random Forest\", *evaluate(y_test, pred) , cross_val(RandomForestRegressor())]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","bc9ce39a":"from xgboost import XGBRegressor\n\nmodel = XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\nmodel.fit(X_train, y_train, \n             early_stopping_rounds=5, \n             eval_set=[(X_test, y_test)],\n             verbose=False)\npred = model.predict(X_test)\n\nprint_evaluate(y_test, pred)\n\n''''''","78cef2ba":"results_df_2 = pd.DataFrame(data=[[\"XGBR\", *evaluate(y_test, pred) , cross_val(XGBRegressor())]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","8c1fd273":"\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(model, random_state=1).fit(X_train, y_train)\neli5.show_weights(perm, feature_names = X_train.columns.tolist())","85b8930e":"from xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nmodel = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =42)\nmodel.fit(X_train, y_train)\npred = model.predict(X_test)\n\nprint_evaluate(y_test, pred)","4472ed70":"results_df_2 = pd.DataFrame(data=[[\"GradientBoostingRegressor\", *evaluate(y_test, pred) , cross_val(GradientBoostingRegressor())]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","7da5bed5":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(model, random_state=1).fit(X_train, y_train)\neli5.show_weights(perm, feature_names = X_train.columns.tolist())","d3c8207d":"model = LGBMRegressor(objective='regression', \n                                       num_leaves=4,\n                                       learning_rate=0.01, \n                                       n_estimators=5000,\n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.2,\n                                       feature_fraction_seed=7,\n                                       verbose=-1,\n                                       #min_data_in_leaf=2,\n                                       #min_sum_hessian_in_leaf=11\n                                       )\nmodel.fit(X_train, y_train)\npred = model.predict(X_test)\n\nprint_evaluate(y_test, pred)","ddd2c69b":"results_df_2 = pd.DataFrame(data=[[\"LGBMRegressor\", *evaluate(y_test, pred) , cross_val(LGBMRegressor())]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","b40ee58f":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(model, random_state=1).fit(X_train, y_train)\neli5.show_weights(perm, feature_names = X_train.columns.tolist())","f29850f5":"More updater versions of this Kernel - https:\/\/www.kaggle.com\/dmkravtsov\/3-2-house-prices  and - https:\/\/www.kaggle.com\/dmkravtsov\/3-3-house-price-one-hot-xgbr","b7ff362e":"So, we've solved problem with all zeroes and NaNs in our dataframe"}}