{"cell_type":{"c9a44eb9":"code","35e72ef8":"code","b11a2518":"code","b1fb7c8b":"code","9e349664":"code","129b100a":"code","f28b3f30":"code","6ee1636e":"code","6a3c126f":"code","bb6460c5":"code","6324b6c4":"code","6ea7bbde":"code","f1e3524d":"code","1e8473d3":"code","10473174":"code","c0c1c0d0":"code","30b4dc67":"code","09f04ec7":"code","151aa14b":"code","80aab5bf":"code","390cdee5":"code","a283b7b0":"code","f40a888d":"code","f5e44906":"code","17b8adea":"code","29976283":"code","a0e24bc7":"code","82b9f394":"code","60cc1438":"code","6d495a4e":"markdown","a8a5ff64":"markdown","64ef3dc1":"markdown","46a90086":"markdown","36e9cafd":"markdown","4281dcc8":"markdown","2d2ccf9d":"markdown","eed0e6ca":"markdown","16c9807f":"markdown","e00701ec":"markdown","d317a8eb":"markdown","147461d9":"markdown","8edd3134":"markdown","bc2a6381":"markdown","164711c0":"markdown","de86a19b":"markdown"},"source":{"c9a44eb9":"# to handle datasets\nimport pandas as pd\nimport numpy as np\n\n# for plotting\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# to divide train and test set\nfrom sklearn.model_selection import train_test_split\n\n# feature scaling\nfrom sklearn.preprocessing import MinMaxScaler\n\n# to build the models\nfrom sklearn.linear_model import Lasso\n\n# to evaluate the models\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\n# to build the models\nfrom sklearn.linear_model import LinearRegression, Lasso\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nimport xgboost as xgb\n\npd.pandas.set_option('display.max_columns', None)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom feature_engine import missing_data_imputers as msi\nfrom feature_engine import discretisers as dsc\nfrom feature_engine import categorical_encoders as ce\n\n# sklearn pipeline to put it all together\nfrom sklearn.pipeline import Pipeline as pipe","35e72ef8":"# load dataset\ndata = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\nprint(data.shape)\ndata.head()","b11a2518":"# Load the dataset for submission (the one on which our model will be evaluated by Kaggle)\n# it contains exactly the same variables, but not the target\n\nsubmission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nsubmission.head()","b1fb7c8b":"# categorical variables\ncategorical = [var for var in data.columns if data[var].dtype=='O']\n\n# make a list of all numerical variables first\nnumerical = [var for var in data.columns if data[var].dtype!='O']\n\n# temporal variables\nyear_vars = [var for var in numerical if 'Yr' in var or 'Year' in var]\n\n# discrete variables\ndiscrete = [var for var in numerical if var not in year_vars and len(data[var].unique())<20]\n\n# continuous vars\nnumerical = [var for var in numerical if var not in discrete and var not in ['Id', 'SalePrice'] and var not in year_vars]\n\nprint('There are {} categorical variables'.format(len(categorical)))\nprint('There are {} discrete variables'.format(len(discrete)))\nprint('There are {} numerical and continuous variables'.format(len(numerical)))","9e349664":"# Let's separate into train and test set\n\nX_train, X_test, y_train, y_test = train_test_split(data, data.SalePrice, test_size=0.1,\n                                                    random_state=0)\nX_train.shape, X_test.shape","129b100a":"# function to calculate elapsed time\n\ndef elapsed_years(df, var):\n    # capture difference between year variable and year the house was sold\n    df[var] = df['YrSold'] - df[var]\n    return df","f28b3f30":"for var in ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt']:\n    X_train = elapsed_years(X_train, var)\n    X_test = elapsed_years(X_test, var)\n    submission = elapsed_years(submission, var)","6ee1636e":"X_train[['YearBuilt', 'YearRemodAdd', 'GarageYrBlt']].head()","6a3c126f":"# drop YrSold\nX_train.drop('YrSold', axis=1, inplace=True)\nX_test.drop('YrSold', axis=1, inplace=True)\nsubmission.drop('YrSold', axis=1, inplace=True)\n\n# remove YrSold from our temporal var list\nyear_vars.remove('YrSold')","bb6460c5":"price_pipe = pipe([\n    # add a binary variable to indicate missing information for the 2 variables below\n    ('continuous_var_imputer', msi.AddNaNBinaryImputer(variables = ['LotFrontage', 'GarageYrBlt'])),\n     \n    # replace NA by the median in the 3 variables below, they are numerical\n    ('continuous_var_median_imputer', msi.MeanMedianImputer(imputation_method='median', variables = ['LotFrontage', 'GarageYrBlt', 'MasVnrArea'])),\n     \n    # replace NA by adding the label \"Missing\" in categorical variables (transformer will skip those variables where there is no NA)\n    ('categorical_imputer', msi.CategoricalVariableImputer(variables = categorical)),\n     \n    # there were a few variables in the submission dataset that showed NA, but these variables did not show NA in the train set.\n    # to handle those, I will add an additional step here\n    ('additional_median_imputer', msi.MeanMedianImputer(imputation_method='median', variables = numerical)),\n\n    # disretise numerical variables using trees\n    ('numerical_tree_discretiser', dsc.DecisionTreeDiscretiser(cv = 3, scoring='neg_mean_squared_error', variables = numerical, regression=True)),\n     \n    # remove rare labels in categorical and discrete variables\n    ('rare_label_encoder', ce.RareLabelCategoricalEncoder(tol = 0.03, n_categories=1, variables = categorical+discrete)),\n     \n    # encode categorical variables using the target mean \n    ('categorical_encoder', ce.MeanCategoricalEncoder(variables = categorical+discrete))\n     ])","6324b6c4":"# the following vars in the submission dataset are encoded in different types\n# so first I cast them as int, like in the train set\n\nfor var in ['BsmtFullBath', 'BsmtHalfBath', 'GarageCars']:\n    submission[var] = submission[var].astype('float')","6ea7bbde":"## the categorical encoders only work with categorical variables\n# therefore we need to cast the discrete variables into categorical\n\nX_train[discrete]= X_train[discrete].astype('O')\nX_test[discrete]= X_test[discrete].astype('O')\nsubmission[discrete]= submission[discrete].astype('O')","f1e3524d":"# let's create a list of the training variables\ntraining_vars = [var for var in X_train.columns if var not in ['Id', 'SalePrice']]\n\nprint('total number of variables to use for training: ', len(training_vars))","1e8473d3":"price_pipe.fit(X_train[training_vars], y_train)","10473174":"# let's capture the id to add it later to our submission\nsubmission_id = submission['Id']","c0c1c0d0":"X_train = price_pipe.transform(X_train[training_vars])\nX_test = price_pipe.transform(X_test[training_vars])\nsubmission = price_pipe.transform(submission[training_vars])","30b4dc67":"# let's check that we didn't introduce NA\nlen([var for var in training_vars if X_train[var].isnull().sum()>0])","09f04ec7":"# let's check that we didn't introduce NA\nlen([var for var in training_vars if X_test[var].isnull().sum()>0])","151aa14b":"# let's check that we didn't introduce NA\nlen([var for var in training_vars if submission[var].isnull().sum()>0])","80aab5bf":"training_vars = [var for var in X_train.columns]\nlen(training_vars)","390cdee5":"# these are the binary variables that we introduced during feature engineering\n[ var for var in training_vars if '_na' in var]","a283b7b0":"# fit scaler\nscaler = MinMaxScaler() # create an instance\nscaler.fit(X_train[training_vars]) #  fit  the scaler to the train set for later use","f40a888d":"xgb_model = xgb.XGBRegressor()\n\neval_set = [(X_test[training_vars], np.log(y_test))]\nxgb_model.fit(X_train[training_vars], np.log(y_train), eval_set=eval_set, verbose=False)\n\npred = xgb_model.predict(X_train[training_vars])\nprint('xgb train mse: {}'.format(mean_squared_error(y_train, np.exp(pred))))\nprint('xgb train rmse: {}'.format(sqrt(mean_squared_error(y_train, np.exp(pred)))))\nprint()\npred = xgb_model.predict(X_test[training_vars])\nprint('xgb test mse: {}'.format(mean_squared_error(y_test, np.exp(pred))))\nprint('xgb test rmse: {}'.format(sqrt(mean_squared_error(y_test, np.exp(pred)))))","f5e44906":"rf_model = RandomForestRegressor(n_estimators=800, max_depth=6)\nrf_model.fit(X_train[training_vars], np.log(y_train))\n\npred = rf_model.predict(X_train[training_vars])\nprint('rf train mse: {}'.format(mean_squared_error(y_train, np.exp(pred))))\nprint('rf train rmse: {}'.format(sqrt(mean_squared_error(y_train, np.exp(pred)))))\n\nprint()\npred = rf_model.predict(X_test[training_vars])\nprint('rf test mse: {}'.format(mean_squared_error(y_test, np.exp(pred))))\nprint('rf test rmse: {}'.format(sqrt(mean_squared_error(y_test, np.exp(pred)))))","17b8adea":"SVR_model = SVR()\nSVR_model.fit(scaler.transform(X_train[training_vars]), np.log(y_train))\n\npred = SVR_model.predict(X_train[training_vars])\nprint('SVR train mse: {}'.format(mean_squared_error(y_train, np.exp(pred))))\nprint('SVR train rmse: {}'.format(sqrt(mean_squared_error(y_train, np.exp(pred)))))\n\nprint()\npred = SVR_model.predict(X_test[training_vars])\nprint('SVR test mse: {}'.format(mean_squared_error(y_test, np.exp(pred))))\nprint('SVR test rmse: {}'.format(sqrt(mean_squared_error(y_test, np.exp(pred)))))","29976283":"lin_model = Lasso(random_state=2909, alpha=0.005)\nlin_model.fit(scaler.transform(X_train[training_vars]), np.log(y_train))\n\npred = lin_model.predict(scaler.transform(X_train[training_vars]))\nprint('Lasso Linear Model train mse: {}'.format(mean_squared_error(y_train, np.exp(pred))))\nprint('Lasso Linear Model train rmse: {}'.format(sqrt(mean_squared_error(y_train, np.exp(pred)))))\n\nprint()\npred = lin_model.predict(scaler.transform(X_test[training_vars]))\nprint('Lasso Linear Model test mse: {}'.format(mean_squared_error(y_test, np.exp(pred))))\nprint('Lasso Linear Model test rmse: {}'.format(sqrt(mean_squared_error(y_test, np.exp(pred)))))","a0e24bc7":"# make predictions for the submission dataset\nfinal_pred = pred = lin_model.predict(scaler.transform(submission[training_vars]))","82b9f394":"temp = pd.concat([submission_id, pd.Series(np.exp(final_pred))], axis=1)\ntemp.columns = ['Id', 'SalePrice']\ntemp.head()","60cc1438":"temp.to_csv('submit_housesale.csv', index=False)","6d495a4e":"Id is a unique identifier for each of the houses. Thus this is not a variable that we can use.\n\n### Make lists of variables","a8a5ff64":"### Feature engineering with feature engine and the sklearn pipeline\n\nI will follow the exact same engineering steps from the previous notebook.\n\nThe only thing that we need to do, is list, one after the other, the engineering steps in the sklearn pipeline as follows:","64ef3dc1":"#### Support vector machine","46a90086":"#### Regularised linear regression","36e9cafd":"\n### Predicting Sale Price of Houses\n\nThe problem at hand aims to predict the final sale price of homes based on different explanatory variables describing aspects of residential homes. Predicting house prices is useful to identify fruitful investments, or to determine whether the price advertised for a house is over or underestimated, before making a buying judgment.\n\nTo download the House Price dataset go this website:\nhttps:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data\n\nScroll down to the bottom of the page, and click on the link 'train.csv', and then click the 'download' blue button towards the right of the screen, to download the dataset.\nSave it to a directory of your choice.\n\nFor the Kaggle submission, download also the 'test.csv' file, which is the one we need to score and submit to Kaggle. Rename the file to 'house_price_submission.csv'\n\n**Note that you need to be logged in to Kaggle in order to download the datasets**.\n\nIf you save it in the same directory from which you are running this notebook and name the file 'houseprice.csv' then you can load it the same way I will load it below.\n\n====================================================================================================","4281dcc8":"The scaler is now ready, we can use it in a machine learning algorithm when required. See below.\n\n### Machine Learning algorithm building\n\n**Note**\n\nThe distribution of SalePrice is also skewed, so I will fit the model to the log transformation of the house price.\n\nThen, to evaluate the models, we need to convert it back to prices.\n\n#### xgboost","2d2ccf9d":"## House Prices dataset","eed0e6ca":"This model shows some over-fitting. Compare the rmse for train and test.","16c9807f":"### Load Datasets","e00701ec":"This model shows some over-fitting. Compare the rmse for train and test.","d317a8eb":"### Feature scaling\n\nThe transformed datasets contain new variables now, because we added binary variables to indicate missing information. So we need to select again our training variables:","147461d9":"### Bespoke feature engineering\n\nFirst, let's extract information from temporal variables.\n#### Temporal variables\n\nFirst, we will create those temporal variables we discussed a few cells ago","8edd3134":"Instead of years, now we have the amount of years passed since the house was built or remodeled and the house was sold. Next, we drop the YrSold variable from the datasets, because we already extracted its value.","bc2a6381":"#### Random Forests","164711c0":"### Separate train and test set","de86a19b":"The best model is the Lasso, so I will submit only that one for Kaggle.\n\n### Submission to Kaggle"}}