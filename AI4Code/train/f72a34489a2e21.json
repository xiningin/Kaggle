{"cell_type":{"b29314d6":"code","4653f820":"code","d20399c9":"code","5af2f480":"code","4b40906b":"code","d76f3ff8":"code","5577aebe":"code","e17a2b5d":"code","f6fe60a9":"code","7cbc0e83":"markdown","e373caa4":"markdown","2b088750":"markdown","b64d0dd8":"markdown","962418e1":"markdown","77bea2ba":"markdown"},"source":{"b29314d6":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np","4653f820":"rng = np.random.RandomState(1)\nx = 10 * rng.rand(50) # Added noise\ny = 2 * x - 5 + rng.randn(50)\nplt.scatter(x, y);","d20399c9":"from sklearn.linear_model import LinearRegression\nmodel = LinearRegression(fit_intercept=True)\n\nmodel.fit(x[:, np.newaxis], y)\n\nxfit = np.linspace(0, 10, 1000)\nyfit = model.predict(xfit[:, np.newaxis])\n\nplt.scatter(x, y)\nplt.plot(xfit, yfit);","5af2f480":"\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\npoly_model = make_pipeline(PolynomialFeatures(3),\n                           LinearRegression())\n\nrng = np.random.RandomState(1)\nx = 10 * rng.rand(50)\ny = np.sin(x) + 0.1 * rng.randn(50)\n\npoly_model.fit(x[:, np.newaxis], y)\nyfit = poly_model.predict(xfit[:, np.newaxis])\n\nplt.scatter(x, y)\nplt.plot(xfit, yfit);","4b40906b":"\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\npoly_model = make_pipeline(PolynomialFeatures(7),\n                           LinearRegression())\n\nrng = np.random.RandomState(1)\nx = 10 * rng.rand(50)\ny = np.sin(x) + 0.1 * rng.randn(50)\n\npoly_model.fit(x[:, np.newaxis], y)\nyfit = poly_model.predict(xfit[:, np.newaxis])\n\nplt.scatter(x, y)\nplt.plot(xfit, yfit);","d76f3ff8":"\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\npoly_model = make_pipeline(PolynomialFeatures(20),\n                           LinearRegression())\n\nrng = np.random.RandomState(1)\nx = 10 * rng.rand(50)\ny = np.sin(x) + 0.1 * rng.randn(50)\n\npoly_model.fit(x[:, np.newaxis], y)\nyfit = poly_model.predict(xfit[:, np.newaxis])\n\nplt.scatter(x, y)\nplt.plot(xfit, yfit);","5577aebe":"\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\npoly_model = make_pipeline(PolynomialFeatures(20),\n                           LinearRegression())\n\nrng = np.random.RandomState(1)\nx = 10 * rng.rand(1000)\ny = np.sin(x) + 0.1 * rng.randn(1000)\n\npoly_model.fit(x[:, np.newaxis], y)\nyfit = poly_model.predict(xfit[:, np.newaxis])\n\nplt.scatter(x, y)\nplt.plot(xfit, yfit);","e17a2b5d":"\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_squared_error\n\nerror = []\nfor i in range(4, 21) :\n    \n    poly_model = make_pipeline(PolynomialFeatures(i),\n                               LinearRegression())\n    rng = np.random.RandomState(1)\n    x = 10 * rng.rand(50)\n    y = np.sin(x) + 0.1 * rng.randn(50)\n    poly_model.fit(x[:, np.newaxis], y)\n    yfit = poly_model.predict(xfit[:, np.newaxis])\n    ytrue = [np.sin(c) for c in xfit]\n    error.append(mean_squared_error(ytrue, yfit))\n\nx = [i for i in range(4, 21)]\nplt.scatter(x, error)\nplt.plot(x, error);","f6fe60a9":"\nurl = 'https:\/\/raw.githubusercontent.com\/ankitesh97\/IMDB-MovieRating-Prediction\/master\/movie_metadata_filtered_aftercsv.csv'\n\n","7cbc0e83":"Analsing error helps us in model selection.","e373caa4":"**LINEAR REGRESSION : General Expression**\n\nA multi-feature linear regression is of the form\n$$\ny = a_0 + a_1 x_1 + a_2 x_2 + \\cdots\n$$\nwhere there are multiple $x$ values.\nHere, $a_0$ is the model intercept and $a_1, a_2, \\cdots$ form the model coefficients.\nGeometrically, this is akin to fitting a plane to points in three dimensions, or fitting a hyper-plane to points in higher dimensions.","2b088750":"Now let's see what happens when we increase number of samples.","b64d0dd8":"**Simple Linear Regression**\n\nWe will start with the most familiar linear regression, a straight-line fit to data. We will extend it to more complicated functions in the later sections.\n\nA straight-line fit is a model of the form\n$$\ny = ax + b\n$$\nwhere $a$ is commonly known as the *slope*, and $b$ is commonly known as the *intercept*.\n\nConsider the following data, which is scattered about a line with a slope of 2 and an intercept of -5:","962418e1":"Let us learn a linear model for it and see how it works out!","77bea2ba":"**Polynomial Regression**\n\nWhat happens if Y is a polynomial in X? Can we still use linear regression? The answer is yes.\n\nThe idea is to take our multidimensional linear model:\n$$\ny = a_0 + a_1 x_1 + a_2 x_2 + a_3 x_3 + \\cdots\n$$\nand build the $x_1, x_2, x_3,$ and so on, from our single-dimensional input $x$.\nThat is, we let $x_n = \\phi_n(x)$, where $\\phi_n$ is some function that transforms our data.\n\nFor example, if $\\phi_n(x) = x^n$, our model becomes a polynomial regression:\n$$\ny = a_0 + a_1 x + a_2 x^2 + a_3 x^3 + \\cdots\n$$\nNotice that this is *still a linear model*\u2014the linearity refers to the fact that the coefficients $a_n$ never multiply or divide each other.\nWhat we have effectively done is taken our one-dimensional $x$ values and projected them into a higher dimension, so that a linear fit can fit more complicated relationships between $x$ and $y$.\n\nNow we are going to take 50 samples and try and fit a polynomial of degree 3, 7 and 10 and see what happens."}}