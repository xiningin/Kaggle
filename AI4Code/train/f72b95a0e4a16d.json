{"cell_type":{"e7d0c4a0":"code","76244a49":"code","c46c164d":"code","e0b72621":"code","cf303fae":"code","4f3b782c":"code","697ede5a":"code","76ccc31f":"code","84237475":"code","17e9e592":"code","b2263e14":"code","0989adb5":"code","8f981d58":"code","74100ea6":"code","707062a9":"code","db96bee2":"code","56e8345b":"code","c314086e":"code","812065d6":"code","b1e31e6f":"code","26ac7c42":"code","6ac8d004":"code","ae134dd7":"code","a7628853":"code","3fc23f71":"code","65e51085":"code","46adc47e":"code","8a54a14b":"code","0b29d09f":"code","71fdd032":"code","44edfc68":"code","4f1c338c":"code","5bc64119":"code","7cc8f94c":"code","8cf29b71":"code","631a33ee":"code","8457bad8":"code","ad39b696":"code","367ebd84":"code","52786e75":"code","8ad06b05":"code","d84cafaa":"markdown","e23c6262":"markdown","818a462d":"markdown","8b736b78":"markdown","c68649e0":"markdown","ef0eb842":"markdown","13e1bd9e":"markdown","acfe0aed":"markdown","19657ae3":"markdown","a3d7c686":"markdown","296ed7ba":"markdown","ed2bb6e7":"markdown","87ed9114":"markdown","f88f8279":"markdown","fcbc0bdd":"markdown","77f92877":"markdown","d049bec5":"markdown","815ff3b7":"markdown"},"source":{"e7d0c4a0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd\nimport tensorflow as tf# data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","76244a49":"!pip install -q pyicu\n!pip install -q pycld2\n!pip install -q polyglot\n!pip install -q textstat\n!pip install -q googletrans","c46c164d":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport os\nimport gc\nimport re\nimport folium\nimport textstat\nfrom scipy import stats\nfrom colorama import Fore, Back, Style, init\n\nimport math\nimport numpy as np\nimport scipy as sp\nimport pandas as pd\n\nimport random\nimport networkx as nx\nfrom pandas import Timestamp\n\nfrom PIL import Image\nfrom IPython.display import SVG\nfrom keras.utils import model_to_dot\n\nimport requests\nfrom IPython.display import HTML\n\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\n\ntqdm.pandas()\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\nimport transformers\nimport tensorflow as tf\n\nfrom tensorflow.keras.callbacks import Callback\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n\nfrom tensorflow.keras.models import Model\nfrom kaggle_datasets import KaggleDatasets\nfrom tensorflow.keras.optimizers import Adam\nfrom tokenizers import BertWordPieceTokenizer\nfrom tensorflow.keras.layers import Dense, Input, Dropout, Embedding\nfrom tensorflow.keras.layers import LSTM, GRU, Conv1D, SpatialDropout1D\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras import activations\nfrom tensorflow.keras import constraints\nfrom tensorflow.keras import initializers\nfrom tensorflow.keras import regularizers\n\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.activations import *\nfrom tensorflow.keras.constraints import *\nfrom tensorflow.keras.initializers import *\nfrom tensorflow.keras.regularizers import *\n\nfrom sklearn import metrics\nfrom sklearn.utils import shuffle\nfrom gensim.models import Word2Vec\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_extraction.text import TfidfVectorizer,\\\n                                            CountVectorizer,\\\n                                            HashingVectorizer\n\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import TweetTokenizer  \n\nimport nltk\nfrom textblob import TextBlob\n\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords\nfrom googletrans import Translator\nfrom nltk import WordNetLemmatizer\nfrom polyglot.detect import Detector\nfrom nltk.stem import WordNetLemmatizer\nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nstopword=set(STOPWORDS)\n\nlem = WordNetLemmatizer()\ntokenizer=TweetTokenizer()\n\nnp.random.seed(0)","e0b72621":"#importing important packages\n! pip install -q twython\n\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer","cf303fae":"\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport transformers\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors","4f3b782c":"train1 = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train.csv\")\ntrain2 = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-unintended-bias-train.csv\")\ntrain2.toxic = train2.toxic.round().astype(int)\n\nvalid = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/validation.csv')\ntest = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/test.csv')\nsub = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/sample_submission.csv')","697ede5a":"train1.tail()","76ccc31f":"test.head()","84237475":"print('TRAIN::',train1.shape[0],'TEST::',test.shape[0])","17e9e592":"from wordcloud import WordCloud\nimport plotly.express as px\nimport matplotlib.pyplot as plt\n\ndef remove_nan(word):\n    if type(word)==str:\n        return word.replace(\"\\n\", \"\")\n    else:\n        return \"\"\n\n#replacing nan with ''\ntext = ' '.join([remove_nan(x) for x in train1['comment_text']])\n\nwordcloud = WordCloud(max_font_size = None,background_color = 'red',collocations=False,width =1500,height = 1200).generate(text)\nfig = px.imshow(wordcloud)\nfig.update_layout(title_text = ' Most Common words in comments')","b2263e14":"print('Train Data Null values:')\ncheck_null = train1.isnull().sum()\nprint(check_null)\nprint('Test Data Null values:')\ncheck_null = test.isnull().sum()\nprint(check_null)","0989adb5":"\nimport seaborn as sns\ntarget_columns = train1.iloc[:,2:].sum()\nplt.figure(figsize=(16,8))\nax = sns.barplot(target_columns.index, target_columns.values,alpha=0.9 )\nplt.title(\" Division per category\")\nplt.xlabel(\"Category\")\nplt.ylabel(\"Frequency\")\n\n\nplt.show()","8f981d58":"#Checking  the class imbalance\nx=train1.iloc[:,2:].sum()\nrowsums = train1.iloc[:,2:].sum(axis=1)\ntrain1['clean']= (rowsums==0)\n#Checking how many comments are totally clean without any negative tags\ntrain1['clean'].sum()\nprint('Total comments are  =  ',len(train1))\nprint('Total clean comments = ',train1['clean'].sum())\nprint('Total number of tags',x.sum())","74100ea6":"train_data=train1\nx_data=train_data.iloc[:,2:].sum(axis=1).value_counts()\n#plot\nimport plotly.express as px\nfig = px.bar(x_data, x=x_data.index, y=x_data.values)\nfig.update_layout(title_text=\"Multiple tags\", template=\"plotly_white\")\nfig.show()","707062a9":"train_data=train1\nimport markovify as mk\ndoc = train_data.loc[train_data.clean==0,'comment_text'].tolist()\ntext_model = mk.Text(doc)\nfor i in range(5):\n    print(text_model.make_sentence())","db96bee2":"from PIL import Image\nfrom nltk.corpus import stopwords\nstop_words=set(stopwords.words())\nclean_mask = np.array(Image.open(\"..\/input\/images\/safe-zone.png\"))\nclean_mask = clean_mask[:,:,1]\n#wordcloud for clean comments\nsubset = train_data[train_data.clean==1]\ntext = subset.comment_text.values\nwc = WordCloud(background_color='black',max_words=2000,mask=clean_mask,stopwords=stop_words)\nwc.generate(\" \".join(text))\nplt.figure(figsize=(20,10))\nplt.axis('off')\nplt.title('Words frequent in clean comments',fontsize=20)\nplt.imshow(wc.recolor(colormap = 'viridis',random_state=17), alpha=0.98)\nplt.show()","56e8345b":"from PIL import Image\nfrom nltk.corpus import stopwords\nstop_words=set(stopwords.words())\nclean_mask = np.array(Image.open(\"..\/input\/images\/nuclear.png\"))\nclean_mask = clean_mask[:,:,1]\n#wordcloud for clean comments\nsubset = train_data[train_data.toxic==1]\ntext = subset.comment_text.values\nwc = WordCloud(background_color='black',max_words=2000,mask=clean_mask,stopwords=stop_words)\nwc.generate(\" \".join(text))\nplt.figure(figsize=(20,10))\nplt.axis('off')\nplt.title(' Frequency of words in Toxic comments',fontsize=20)\nplt.imshow(wc.recolor(colormap = 'viridis',random_state=17), alpha=0.98)\nplt.show()","c314086e":"from PIL import Image\nfrom nltk.corpus import stopwords\nstop_words=set(stopwords.words())\nclean_mask = np.array(Image.open(\"..\/input\/images\/nuclear.png\"))\nclean_mask = clean_mask[:,:,1]\n#wordcloud for clean comments\nsubset = train_data[train_data.severe_toxic==1]\ntext = subset.comment_text.values\nwc = WordCloud(background_color='white',max_words=2000,mask=clean_mask,stopwords=stop_words)\nwc.generate(\" \".join(text))\nplt.figure(figsize=(20,10))\nplt.axis('off')\nplt.title('Words frequent in Severe comments',fontsize=20)\nplt.imshow(wc.recolor(colormap = 'viridis',random_state=17), alpha=0.98)\nplt.show()","812065d6":"from PIL import Image\nfrom nltk.corpus import stopwords\nstop_words=set(stopwords.words())\nclean_mask = np.array(Image.open(\"..\/input\/images\/nuclear.png\"))\nclean_mask = clean_mask[:,:,1]\n#wordcloud for clean comments\nsubset = train_data[train_data.obscene==1]\ntext = subset.comment_text.values\nwc = WordCloud(background_color='red',max_words=2000,mask=clean_mask,stopwords=stop_words)\nwc.generate(\" \".join(text))\nplt.figure(figsize=(20,10))\nplt.axis('off')\nplt.title('Words frequent in obscene comments',fontsize=20)\nplt.imshow(wc.recolor(colormap = 'viridis',random_state=17), alpha=1)\nplt.show()","b1e31e6f":"from PIL import Image\nfrom nltk.corpus import stopwords\nstop_words=set(stopwords.words())\nclean_mask = np.array(Image.open(\"..\/input\/images\/nuclear.png\"))\nclean_mask = clean_mask[:,:,1]\n#wordcloud for clean comments\nsubset = train_data[train_data.insult==1]\ntext = subset.comment_text.values\nwc = WordCloud(background_color='black',max_words=2000,mask=clean_mask,stopwords=stop_words)\nwc.generate(\" \".join(text))\nplt.figure(figsize=(20,10))\nplt.axis('off')\nplt.title('Words frequent in insult comments',fontsize=20)\nplt.imshow(wc.recolor(colormap = 'viridis',random_state=17), alpha=1)\nplt.show()","26ac7c42":"from PIL import Image\nfrom nltk.corpus import stopwords\nstop_words=set(stopwords.words())\nclean_mask = np.array(Image.open(\"..\/input\/images\/nuclear.png\"))\nclean_mask = clean_mask[:,:,1]\n#wordcloud for clean comments\nsubset = train_data[train_data.threat==1]\ntext = subset.comment_text.values\nwc = WordCloud(background_color='pink',max_words=2000,mask=clean_mask,stopwords=stop_words)\nwc.generate(\" \".join(text))\nplt.figure(figsize=(20,10))\nplt.axis('off')\nplt.title('Words frequent in threat comments',fontsize=20)\nplt.imshow(wc.recolor(colormap = 'viridis',random_state=17), alpha=1)\nplt.show()","6ac8d004":"from PIL import Image\nfrom nltk.corpus import stopwords\nstop_words=set(stopwords.words())\nclean_mask = np.array(Image.open(\"..\/input\/images\/nuclear.png\"))\nclean_mask = clean_mask[:,:,1]\n#wordcloud for clean comments\nsubset = train_data[train_data.insult==1]\ntext = subset.comment_text.values\nwc = WordCloud(background_color='blue',max_words=2000,mask=clean_mask,stopwords=stop_words)\nwc.generate(\" \".join(text))\nplt.figure(figsize=(20,10))\nplt.axis('off')\nplt.title('Words frequent in insult comments',fontsize=20)\nplt.imshow(wc.recolor(colormap = 'viridis',random_state=17), alpha=0.98)\nplt.show()","ae134dd7":"from PIL import Image\nfrom nltk.corpus import stopwords\nstop_words=set(stopwords.words())\nclean_mask = np.array(Image.open(\"..\/input\/images\/nuclear.png\"))\nclean_mask = clean_mask[:,:,1]\n#wordcloud for clean comments\nsubset = train_data[train_data.identity_hate==1]\ntext = subset.comment_text.values\nwc = WordCloud(background_color='black',max_words=2000,mask=clean_mask,stopwords=stop_words)\nwc.generate(\" \".join(text))\nplt.figure(figsize=(20,10))\nplt.axis('off')\nplt.title('Words frequent in identity hate comments',fontsize=20)\nplt.imshow(wc.recolor(colormap = 'viridis',random_state=17), alpha=1)\nplt.show()","a7628853":"\ntest_data=test\nval_data=valid\nfig,axes = plt.subplots(ncols=3,figsize=(17,7),dpi=100)\ntemp = val_data['lang'].value_counts()\nsns.barplot(temp.index,temp,ax=axes[0],palette='Set1')\n\ntemp = test_data['lang'].value_counts()\nsns.barplot(temp.index,temp,ax=axes[1],palette='Set1')\nsns.countplot(data=val_data , x='lang' , hue='toxic',ax=axes[2],palette='Set1')\naxes[0].set_ylabel('Count')\naxes[1].set_ylabel(' ')\naxes[2].set_ylabel(' ')\naxes[2].set_xlabel(' ')\naxes[0].set_title('Language distribution of Validation Dataset',fontsize=13)\naxes[1].set_title('Language distribution of Test Dataset', fontsize=13)\naxes[2].set_title('Language distribution by Target of Validation Dataset',fontsize=13)\nplt.tight_layout()\nplt.show()","3fc23f71":"def word_counter(x):\n    if type(x) is str:\n        return len(x.split())\n    else:\n        return 0\n    \ntrain_data['comment_word'] = train_data['comment_text'].apply(word_counter)\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# matplotlib histogram\nplt.hist(train_data['comment_word'], color = 'blue', edgecolor = 'black',\n         bins = int(180\/5))\n\n# seaborn histogram\nsns.distplot(train_data['comment_word'], hist=True, kde=False, \n             bins=int(180\/5), color = 'blue',\n             hist_kws={'edgecolor':'black'})\n# Add labels\nplt.title('word density distribution')\nplt.xlabel('word count')\nplt.ylabel('')","65e51085":"from tqdm import tqdm, tqdm_notebook\ntqdm_notebook().pandas()\ndef polarity(x):\n    if type(x) == str:\n        return SIA.polarity_scores(x)\n    else:\n        return 1000\n    \nSIA = SentimentIntensityAnalyzer()\ntrain_data[\"polarity\"] = train_data[\"comment_text\"].progress_apply(polarity)","46adc47e":"import plotly.graph_objects as go\n\nfig = go.FigureWidget(go.Histogram(x=[pols[\"neg\"] for pols in train_data[\"polarity\"] if pols[\"neg\"] != 0], marker=dict(\n            color='skyblue')\n    ))\n\nfig.update_layout(xaxis_title=\"Negativity level\", title_text=\"Negativity sentiment\", template=\"simple_white\")\nfig","8a54a14b":"val = val_data\ntrain = train_data\n\ndef clean(text):\n    text = text.fillna(\"fillna\").str.lower()\n    text = text.map(lambda x: re.sub('\\\\n',' ',str(x)))\n    text = text.map(lambda x: re.sub(\"\\[\\[User.*\",'',str(x)))\n    text = text.map(lambda x: re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",'',str(x)))\n    text = text.map(lambda x: re.sub(\"\\(http:\/\/.*?\\s\\(http:\/\/.*\\)\",'',str(x)))\n    return text\n\nval[\"comment_text\"] = clean(val[\"comment_text\"])\ntest_data[\"content\"] = clean(test_data[\"content\"])\ntrain[\"comment_text\"] = clean(train[\"comment_text\"])","0b29d09f":"# Modelling","71fdd032":"def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(max_length=maxlen)\n    all_ids=[]\n    \n    for i in range(0, len(texts), chunk_size):\n        text_chunk = texts[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n        \n    return np.array(all_ids)","44edfc68":"def regular_encode(texts,tokenizer,maxlen=512):\n    enc_di = tokenizer.batch_encode_plus(\n    texts,\n    return_attention_masks = False,\n    return_token_type_ids = False,\n    pad_to_max_length = True,\n    max_length = maxlen\n    )\n    \n    return np.array(enc_di['input_ids'])","4f1c338c":"\n\ndef build_roberta_model(transformer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    #cls_token = Dense(500, activation=\"relu\")(cls_token)\n    #cls_token = Dropout(0.1)(cls_token)\n    out = Dense(1, activation='sigmoid')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=1e-5), \n                  loss='binary_crossentropy', \n                  metrics=['accuracy'])\n    \n    return model","5bc64119":"#TPU\ntry:\n    \n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU', tpu.master())\nexcept ValueError:\n    tpu=None\n    \nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n    \nprint(\"REPLICAS:\", strategy.num_replicas_in_sync)","7cc8f94c":"AUTO = tf.data.experimental.AUTOTUNE\n\nGCS_DS_PATH = KaggleDatasets().get_gcs_path('jigsaw-multilingual-toxic-comment-classification')\n\nEPOCHS = 2\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nMAX_LEN = 192\nMODEL = 'jplu\/tf-xlm-roberta-large'","8cf29b71":"tokenizer = AutoTokenizer.from_pretrained(MODEL)","631a33ee":"train = pd.concat([\n        train1[['comment_text', 'toxic']],\n        train2[['comment_text', 'toxic']].query('toxic == 1'),\n        train2[['comment_text', 'toxic']].query('toxic == 0').sample(n=100000, random_state=0)\n\n])","8457bad8":"%%time\n\nx_train = regular_encode(train.comment_text.values, tokenizer, maxlen=MAX_LEN)\n\nx_valid = regular_encode(valid.comment_text.values, tokenizer, maxlen = MAX_LEN)\n\nx_test  = regular_encode(test.content.values, tokenizer, maxlen=MAX_LEN)\n\ny_train = train.toxic.values\ny_valid = valid.toxic.values","ad39b696":"train_dataset = (\n        tf.data.Dataset.from_tensor_slices((x_train,y_train)).repeat().shuffle(2048).batch(BATCH_SIZE).prefetch(AUTO) )\n\n\nvalid_dataset = (\n        tf.data.Dataset.from_tensor_slices((x_valid,y_valid)).batch(BATCH_SIZE).cache().prefetch(AUTO)\n)\n\ntest_dataset = (\n        tf.data.Dataset.from_tensor_slices(x_test).batch(BATCH_SIZE)\n)","367ebd84":"%%time\nwith strategy.scope():\n    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n    model = build_roberta_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()","52786e75":"\n\nN_STEPS = x_train.shape[0] \/\/ BATCH_SIZE\n\n\ntrain_history = model.fit(\n                train_dataset, steps_per_epoch=N_STEPS,validation_data=valid_dataset,epochs=EPOCHS\n)","8ad06b05":"\nsub['toxic'] = model.predict(test_dataset, verbose=1)\nsub.to_csv('submission.csv', index=False)","d84cafaa":"**Now let's create word clouds of different target tag categories i.e Obscene, Toxic, Identity Threat etc**","e23c6262":"**There are no null values the data is clean so far :)**","818a462d":"# Sentiment Analysis","8b736b78":"Importing twython this is an important Twitter package for comment analysis","c68649e0":"**Crrating a bar plot for different negative comments category**","ef0eb842":"**MOST COMMON WORDS WE SEE HERE ARE-** article, page,talk, will,one , will, edit* ,*obscence words are less often","13e1bd9e":"# TPU Configuration","acfe0aed":"**Generally comments with negative sentiments tend to be more toxic**","19657ae3":"**FINDING THE MOST COMMON WORDS**","a3d7c686":"# Feature Engineering","296ed7ba":"**Plot of word density per comment.**","ed2bb6e7":"**LET'S GENERATE SOME MORE NASTY COMMENTS** FOR FUN !!","87ed9114":"**LET'S LOOK AT THE DIVISION**","f88f8279":"**Dealing With Null values**","fcbc0bdd":"**Checking for multiple tags**","77f92877":"# Training","d049bec5":"**Encoding comments for compatibility and getting targets**","815ff3b7":"# Output"}}