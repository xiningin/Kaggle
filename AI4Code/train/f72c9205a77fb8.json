{"cell_type":{"02a46ad5":"code","b13bbc52":"code","a1b7f4c7":"code","86673997":"code","cf2c0ce6":"code","73136f2f":"code","34b63374":"code","dde52519":"code","deb03de0":"code","962752cb":"code","74a2bf95":"code","c0827b27":"code","2ebcc3b5":"code","73da3e38":"code","1413d204":"code","f7cfb901":"code","4f2bc982":"code","3730f034":"code","a4639b77":"code","6a61b0bd":"code","6aa8bc39":"code","153aa110":"code","ce700f0e":"code","d00605ad":"code","e1543fff":"code","f4432219":"code","f22553dc":"code","b9f07096":"code","43fe3100":"code","e3050bc3":"code","497dab55":"code","1c3b178a":"code","1729ff9b":"code","89a462fd":"code","9e295a73":"code","119820c6":"code","5aa5620b":"code","6bd9ba3d":"code","d8cf7bfd":"markdown","8030a7d9":"markdown","2dbd5c28":"markdown","24072047":"markdown","99b04ad9":"markdown","d67cc418":"markdown","b04a4611":"markdown","f5ad6076":"markdown","68e23cd8":"markdown","d9017b67":"markdown","f748f7ab":"markdown"},"source":{"02a46ad5":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","b13bbc52":"df  = pd.read_csv(\"..\/input\/classified-dataset\/Classified Data\", index_col=0)","a1b7f4c7":"df.head()","86673997":"df.shape #here check shape of dataset","cf2c0ce6":"df.info()  #here check information about given dataset","73136f2f":"df.isnull().sum()","34b63374":"df.describe()","dde52519":"from sklearn.preprocessing import StandardScaler","deb03de0":"scaler = StandardScaler()","962752cb":"scaler.fit(df.drop('TARGET CLASS', axis=1))","74a2bf95":"scaler_features = scaler.transform(df.drop('TARGET CLASS', axis=1))","c0827b27":"scaler_features","2ebcc3b5":"df2 = pd.DataFrame(scaler_features, columns=df.columns[:-1])","73da3e38":"df2.head()","1413d204":"#pair plot\n\nsns.pairplot(df, hue='TARGET CLASS')","f7cfb901":"#splitting\n","4f2bc982":"from sklearn.model_selection import train_test_split","3730f034":"X_train, X_test, y_train, y_test = train_test_split(scaler_features, df['TARGET CLASS'], test_size=0.25)","a4639b77":"#k-nn","6a61b0bd":"from sklearn.neighbors import KNeighborsClassifier","6aa8bc39":"knn = KNeighborsClassifier(n_neighbors=3)","153aa110":"knn.fit(X_train, y_train)","ce700f0e":"pred = knn.predict(X_test)","d00605ad":"from sklearn.metrics import classification_report, confusion_matrix","e1543fff":"from sklearn.model_selection import cross_val_score","f4432219":"print(confusion_matrix(y_test, pred))","f22553dc":"print(classification_report(y_test, pred))","b9f07096":"accuracy_rate = []\n\nfor i in range(1,25):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    score = cross_val_score(knn, df2, df['TARGET CLASS'], cv=10)\n    accuracy_rate.append(score.mean())","43fe3100":"error_rate = []\nfor i in range(1,25):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    pred2 = knn.predict(X_test)\n    error_rate.append(np.mean(pred2!=y_test))","e3050bc3":"plt.figure(figsize=(10,6))\n\nplt.plot(range(1,25), accuracy_rate, color='blue', linestyle='dotted', marker='o', markerfacecolor='red')\n\nplt.title('Accuracy Rate vs Value')\nplt.xlabel('K')\nplt.ylabel('Accuracy Rate')","497dab55":"error_rate = []\n# Will take some time\nfor i in range(1,40):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))","1c3b178a":"plt.figure(figsize=(10,6))\nplt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","1729ff9b":"import sklearn\nimport sklearn.metrics as metrics\nfrom sklearn import preprocessing","89a462fd":"for i in range(1,25):\n    knn = sklearn.neighbors.KNeighborsClassifier(n_neighbors=i)\n    model = knn.fit(preprocessing.scale(X_train), y_train)\n    pred3 = model.predict(preprocessing.scale(X_test))\n    w = metrics.accuracy_score(pred3, y_test)\n    print(i)\n    print(w)","9e295a73":"from sklearn.model_selection import GridSearchCV","119820c6":"parameters = {'n_neighbors':range(1,25)}\ngridsearch = GridSearchCV(KNeighborsClassifier(), parameters)\ngridsearch.fit(X_train, y_train)","5aa5620b":"gridsearch.best_params_","6bd9ba3d":"gridsearch.best_score_","d8cf7bfd":"> In given dataset there are total 10 float features and 1 int features. \n> There is no null values","8030a7d9":"**KNN (K-Nearest Neighbors)**","2dbd5c28":"# Using GridSearchCV","24072047":"**Scaling**","99b04ad9":"# Elbow Method","d67cc418":"# Using For loop find out best K","b04a4611":"**Import Required LIbrary**","f5ad6076":"* KNN is a model that classifies data points based on the points that are most similar to it.\n\n* KNN is an algorithm that is considered both non-parametric and an lazy learning.\n\n* How decide the K-value? \n    \n    **For Classification:-**\n        If you are using K and you have an even number of classes (e.g. 2) It is a good idea to choose a K value with an odd number to avoid a tie. And the inverse, use an even number for K when you have an odd number of classes.\n        \n    **For Regression:-**\n        When KNN is used for regression problems the prediction is based on the mean of the K-most similar instances.\n\n* The most popular distance measure in \u2018Euclidean Distance\u2019. \n\n* Calculate the distance between real vectors using the sum of their absolute difference called as     **'Manhattan Distance'** \n","68e23cd8":"**Read the dataset**","d9017b67":"## Choosing a best k-value","f748f7ab":"**Step for K-NN**\n\n1. **Step 1:-**\n    Load all dataset and assign the value of K. i.e. K = 3,5,7,9\u2026...  \n\n2. **Step 2:-**\n    For each point in the test data do the following.\n\n        * Calculate the distance between points.\n        * Now, based on the distance value, sort them in ascending order.\n        * Next, it will choose the top K from sorted array.\n        * Now, it will assign a class to the test point based on most frequent class.\n\n3. **Step 3:-** End\n"}}