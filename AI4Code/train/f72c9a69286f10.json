{"cell_type":{"c0c10f1c":"code","69b8e941":"code","3ab03eff":"code","4f9d4af4":"code","7770d708":"code","4f990d87":"code","0bb965f7":"code","9de8e8f9":"code","654a1c79":"code","85eca0c9":"code","f4ea5b99":"code","c370415e":"code","a1fd3b33":"code","c5d9dfcb":"code","60eb3ce7":"code","89a947f2":"code","e83948ce":"code","abb3bf95":"code","49e39024":"code","841dd5a6":"code","9084f128":"code","70f00592":"code","39596100":"code","a4d75173":"code","d92c1992":"code","3cabc099":"code","d95af01f":"code","ebf5cad3":"code","b896e1ad":"code","4d161497":"code","d2e5cc97":"code","c7d96519":"code","362ae835":"code","23d68fec":"code","86036e70":"code","d6342f59":"code","e48439c0":"code","9bf94aa3":"code","3e47c59a":"code","652a18e3":"code","5c514269":"code","2ba311d0":"code","04f79c38":"code","b1c257f5":"code","192a4915":"code","5fa4c700":"code","ae1d55dc":"code","c223f9f0":"code","d8b146c3":"code","9169843f":"code","c6b1847c":"code","303d72d2":"code","4158c112":"code","7901435c":"code","dd5464c4":"code","59293b15":"code","fea4ea66":"code","09e4f2cb":"code","9ec03c8c":"code","efb9e6a2":"code","2ba577f0":"code","39f5e213":"code","37093731":"code","2f3496a8":"markdown","2bb8ba24":"markdown","28a1ba51":"markdown","91d14b9a":"markdown","94be3083":"markdown","9f585d44":"markdown","ebe756b0":"markdown","a24eb344":"markdown","b414531c":"markdown","122158a5":"markdown","9ecbdfb2":"markdown","626a0a43":"markdown","bb49dad3":"markdown","959ccc5e":"markdown","16585c9d":"markdown","a3e0cfaa":"markdown","8894b228":"markdown","20e6ccc5":"markdown","1b9b5a1f":"markdown","d84f53d1":"markdown","2e3ec16a":"markdown","cf1bf57d":"markdown","9c8674b9":"markdown","0001bcac":"markdown","29356fd0":"markdown","75a66e9e":"markdown","5ea3175b":"markdown","0df69bfe":"markdown","4a85c250":"markdown","60e26c94":"markdown","0e2726ed":"markdown","23c05e92":"markdown","ed767e9c":"markdown","7766c87b":"markdown","54d13d05":"markdown"},"source":{"c0c10f1c":"# do frequent garbage collection via gc.collect() and\/or do \"del variable\"\nimport gc\n\n# general things used for EDA steps\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport os\nfrom time import time\nfrom time import strftime","69b8e941":"from IPython.display import HTML\n\nHTML('''<script>\ncode_show=true; \nfunction code_toggle() {\n if (code_show){\n $('div.input').hide();\n } else {\n $('div.input').show();\n }\n code_show = !code_show\n} \n$( document ).ready(code_toggle);\n$( document ).ready(code_toggle);\n<\/script>\n\n<font size=3><b>\nNote: the code cells for this IPython notebook may be hidden -- <br>\n<a href=\"javascript:code_toggle()\"> - Click HERE - Click HERE - Click HERE <\/a> <br>\nto toggle them between visible-hidden.<\/b><\/font><br><br>\nThis is useful because of all the code: on the plus side there are comments,\non the minus side the coding style is verbose-unimaginative ;-)''')","3ab03eff":"# Running on Kaggle (True), or on my local machine (False)\nLOCATION_KAGGLE = True\n\n# Key parameters to set upfront?\n\n# For testing, \n# can fill the Test data with the Training data\nTEST_IS_TRAIN = False   # False when submitting!\n# and\/or reduce the train size\nREDUCED_SIZE = False\n\n# Select if an \"intersection\", the InterCode value, is\n# a unique geographic location:\n#   City-LatOff-LongOff - False\n# or a unique \"entry-section\":\n#   City-LatOff-LongOff-InHeading - True\nENTRYSECTIONS = True\n\n# Comparisons to use for the training NoWait, LoWait, HiWait determination.\n# 0: use Total_p80 for all three thresholds\n# 1: use Total, TTS, and DTFS, for No, Lo, Hi respectively.\nWAIT_CHOICE = 0\n\n# Use ML to assign iWait values\n# (if False, use the Known values when Test is train)\nML_WAITS = True\n# Override the ML iWait for the Test-data 'northern unknows'\n# (-1=use the ML, 0,1,2=override with this value)\nnorthern_iwait = -1  # Do ML\n\n# Can turn on\/off EDA output and plots (not needed for ML)\nSHOW_EDA = True\n# Create and show results from\n# an (entry-)intersection dataframe.\n# This adds features so should be True.\nINTER_DF = True\n\n# version string to include in filenames of plots, etc. (except submission file)\nversion_str = \"v54\"\n","4f9d4af4":"# Create an output directory for this version, if running locally,\n# otherwise use the current dir when on Kaggle.\nif LOCATION_KAGGLE:\n    out_dir = \".\"\nelse:\n    out_dir = \"Out_\"+version_str\n    try:\n        os.mkdir(out_dir)\n    except FileExistsError:\n        pass\n    \n# The seed is set once here at beginning of notebook.\nRANDOM_SEED = 360\n# Uncomment this to get a time-based random value, 0 to 1023\n##RANDOM_SEED = int(time()) % 2**10\n# in either case initialize the seed\nnp.random.seed(RANDOM_SEED)","7770d708":"# Collecting some of the 'global' variables here.\n# Unless noted, these are set here and used elsewhere below.\n\n# The names of the cities:\ncities=['Atlanta','Boston','Chicago','Philadelphia']\n\n# The direction heading names in order\nheadings=['N','NE','E','SE','S','SW','W','NW']\n\n# Locations of the 'center' of the cities\n# The integer LatOff and LongOff (in 10^-4 degree units)\n# are created so that (5000,5000) = this center.\n# \n# Atlanta: Georgia State Capitol  33.7489, -84.3881\n# Boston:  Park St subway         42.3564, -71.0625\n# Chicago: Architecture Center    41.8878, -87.6233\n# Philly:  City Hall              39.9525, -75.1633\nlat_centers =  [ 33.7489,  42.3564,  41.8878,  39.9525]\nlong_centers = [-84.3881, -71.0625, -87.6233, -75.1633]\n\n# Locations of major airport in each city\n#\n# Atlanta: H-J AIA          33.6404, -84.4198\n# Boston:  Logan            42.3670, -71.0224\n# Chicago: O'Hare           41.9786, -87.9047  (there's also Midway airport)\n# Phil.:  Phil.Int.         39.8719, -75.2411\nlat_airport =  [ 33.6404,  42.3670,  41.9786,  39.8719]\nlong_airport = [-84.4198, -71.0224, -87.9047, -75.2411]\n\n# The six output column names\nout_cols = ['TotalTimeStopped_p20','TotalTimeStopped_p50','TotalTimeStopped_p80',\n           'DistanceToFirstStop_p20','DistanceToFirstStop_p50','DistanceToFirstStop_p80']\n","4f990d87":"# Where are the data files\n# Data dir\nif LOCATION_KAGGLE:\n    dat_dir ='..\/input\/bigquery-geotab-intersection-congestion\/'\nelse:\n    dat_dir =\"..\/input\/\"\n# CSV files\ntrain_csv = \"train.csv\"\ntest_csv = \"test.csv\"\n\n# show the files and dirs\n##print(os.listdir(dat_dir))","0bb965f7":"# Read the files and do All the basic feature processing\n# time it\nt_preproc = time()\n\n# Read in the train and test data\n\n# = = = = =\n# Train\ndf_train = pd.read_csv(dat_dir+train_csv)\nif REDUCED_SIZE:\n    divide_by = 3\n    rand_indices = np.random.choice(df_train.index,\n                    size=int(len(df_train)\/divide_by), replace=False)\n    # reduce the size and re-index\n    df_train = df_train.loc[rand_indices].reset_index().drop('index',axis=1)\n\n# = = = = =\n# Test\n# For testing, can fill the Test data with the Training data\nif TEST_IS_TRAIN:\n    df_test = df_train.copy()\nelse:\n    # Read in the TEST data\n    df_test = pd.read_csv(dat_dir+test_csv)\n    if REDUCED_SIZE:\n        rand_indices = np.random.choice(df_test.index,\n                    size=int(len(df_test)\/divide_by), replace=False)\n        df_test = df_test.loc[rand_indices].reset_index().drop('index',axis=1)\n        \nif REDUCED_SIZE:\n    del rand_indices\n    gc.collect()\n\nprint(\"{:.2f} seconds -- read in data files\".format(time() - t_preproc))\n \n\n# Create some other columns, etc. (in both test and train):\n\n# - - - - -\n# Add the output columns to test, to be filled in later\n# Set the columns to 0\nfor new_col in out_cols:\n    # Set all to 0:\n    df_test[new_col] = 0\n\n# - - - - -\n# The months:\nif True:\n    # Because there are so few Jan(1) and May(5) month values, and none in Feb(2)-April(4):\n    # Set the Jan ones to Dec and the May ones to June to have only 7 month values: 6 through 12.\n    select = df_train['Month'] == 1\n    df_train.loc[select,'Month'] = 12\n    select = df_train['Month'] == 5\n    df_train.loc[select,'Month'] = 6\n    #\n    select = df_test['Month'] == 1\n    df_test.loc[select,'Month'] = 12\n    select = df_test['Month'] == 5\n    df_test.loc[select,'Month'] = 6\n    \n    # Further combine the months into just 3 groups:\n    #  1, 2, 3  =  (5+)6-8, 9-10, 11-12(+1)\n    select = df_train['Month'] >= 11\n    df_train.loc[select,'Month'] = 3\n    select = df_train['Month'] >= 9\n    df_train.loc[select,'Month'] = 2\n    select = df_train['Month'] >= 6\n    df_train.loc[select,'Month'] = 1\n    #\n    select = df_test['Month'] >= 11\n    df_test.loc[select,'Month'] = 3\n    select = df_test['Month'] >= 9\n    df_test.loc[select,'Month'] = 2\n    select = df_test['Month'] >= 6\n    df_test.loc[select,'Month'] = 1\n\n# - - - - -\n# Add an iCity column = 0,1,2,3\ndf_train['iCity'] = -1\ndf_test['iCity'] = -1\nfor icity, this_city in enumerate(cities):\n    df_train.loc[df_train['City'] == this_city, 'iCity'] = icity\n    df_test.loc[df_test['City'] == this_city, 'iCity'] = icity\n#\n# One-hot encoding of City\n# This may be better than iCity for some ML methods ?\n# from https:\/\/www.kaggle.com\/dcaichara\/feature-engineering-and-lightgbm\nif False:\n    df_train = pd.concat([df_train,pd.get_dummies(df_train[\"City\"],\n                dummy_na=False, drop_first=False)],axis=1).drop([\"City\"],axis=1)\n    df_test = pd.concat([df_test,pd.get_dummies(df_test[\"City\"],\n                dummy_na=False, drop_first=False)],axis=1).drop([\"City\"],axis=1)\n\n# - - - - -\n# Add an HrWk column made by combining the Hour and Weekend: \n#   HrWk = 0 - 23 weekdays, 24 - 47 weekends\ndf_train['HrWk'] = df_train['Hour'] + 24*df_train['Weekend']\ndf_test['HrWk'] = df_test['Hour'] + 24*df_test['Weekend']\n\n# - - - - -\n# Add InHeading and ExHeading numeric columns = 0--7 (N to NW)\ndf_train['InHeading'] = 0\ndf_test['InHeading'] = 0\ndf_train['ExHeading'] = 0\ndf_test['ExHeading'] = 0\n\n# Assign the heading numbers\nfor ihead, this_head in enumerate(headings):\n    df_train.loc[df_train['EntryHeading'] == this_head, 'InHeading'] = ihead\n    df_train.loc[df_train['ExitHeading'] == this_head, 'ExHeading'] = ihead\n    df_test.loc[df_test['EntryHeading'] == this_head, 'InHeading'] = ihead\n    df_test.loc[df_test['ExitHeading'] == this_head, 'ExHeading'] = ihead\n\n# - - - - -\n# Calculate the Turn amount\n# 0 = straight; 1,2,3 = right; -1,-2,-3,-4 = left to U-turn\ndf_train['Turn'] = df_train['ExHeading'] - df_train['InHeading']\ndf_test['Turn'] = df_test['ExHeading'] - df_test['InHeading']\n# Keep it between -4 to 3:\n# If Turn > 3 then Turn = Turn - 8\nselect = df_train['Turn'] > 3\ndf_train.loc[select,'Turn'] = df_train.loc[select,'Turn'] - 8\nselect = df_test['Turn'] > 3\ndf_test.loc[select,'Turn'] = df_test.loc[select,'Turn'] - 8\n# If Turn < -4 then Turn = Turn + 8\nselect = df_train['Turn'] < -4\ndf_train.loc[select,'Turn'] = df_train.loc[select,'Turn'] + 8\nselect = df_test['Turn'] < -4\ndf_test.loc[select,'Turn'] = df_test.loc[select,'Turn'] + 8\n\n# - - - - -\n# Make a coarser turn value: Left, Straight, or Right\ndf_train['TurnLSR'] = 0\ndf_test['TurnLSR'] = 0\nselect = df_train['Turn'] > 0\ndf_train.loc[select,'TurnLSR'] = 1\nselect = df_test['Turn'] > 0\ndf_test.loc[select,'TurnLSR'] = 1\nselect = df_train['Turn'] < 0\ndf_train.loc[select,'TurnLSR'] = -1\nselect = df_test['Turn'] < 0\ndf_test.loc[select,'TurnLSR'] = -1\n\n# - - - - -\n# Create LatOff and LongOff values for the intersections,\n# these are 4 digit numbers in units of 10^-4 degrees.\n# They are set to give (5000, 5000) at each city center.\ndf_train['LatOff'] = 0.0\ndf_train['LongOff'] = 0.0\ndf_test['LatOff'] = 0.0\ndf_test['LongOff'] = 0.0\n#\nfor icity in range(4):\n    select = df_train['iCity'] == icity\n    df_train.loc[select,'LatOff'] = df_train.loc[select,'Latitude'] - lat_centers[icity]\n    df_train.loc[select,'LongOff'] = df_train.loc[select,'Longitude'] - long_centers[icity]\ndf_train['LatOff'] = (5000 + 10000*df_train['LatOff']).astype(int)\ndf_train['LongOff'] = (5000 + 10000*df_train['LongOff']).astype(int)\n#\nfor icity in range(4):\n    select = df_test['iCity'] == icity\n    df_test.loc[select,'LatOff'] = df_test.loc[select,'Latitude'] - lat_centers[icity]\n    df_test.loc[select,'LongOff'] = df_test.loc[select,'Longitude'] - long_centers[icity]\ndf_test['LatOff'] = (5000 + 10000*df_test['LatOff']).astype(int)\ndf_test['LongOff'] = (5000 + 10000*df_test['LongOff']).astype(int)\n\n# - - - - -\n# Convert the Airport Lat,Long into offset values in the center=(5000,5000) system:\n# (this is in features in case \"distance to airpor\" would be useful...\nlongoff_air = np.zeros(4)\nlatoff_air = np.zeros(4)\nfor icity in range(4):\n    longoff_air[icity] = np.int(5000 + 10000*(long_airport[icity] - long_centers[icity]))\n    latoff_air[icity] = np.int(5000 + 10000*(lat_airport[icity] - lat_centers[icity]))\n\n# - - - - -\n# Create unique integer intersection codes\nif ENTRYSECTIONS:\n    # Create unique integer intersection codes --> \"Entry-section\" codes:\n    # Combine iCity, LatOff and LongOff *** and InHeading *** into one integer\n    df_train['InterCode'] = (1000000000*(df_train['iCity'] + 1) + 100000*df_train['LatOff'] + \n                         10*df_train['LongOff'] + df_train['InHeading'])\n    df_test['InterCode'] = (1000000000*(df_test['iCity'] + 1) + 100000*df_test['LatOff'] + \n                        10*df_test['LongOff'] + df_test['InHeading'])\nelse:\n    # just intersections:\n    # Combine iCity, LatOff and LongOff into one integer\n    df_train['InterCode'] = (100000000*(df_train['iCity'] + 1) + 10000*df_train['LatOff'] + \n                         df_train['LongOff'])\n    df_test['InterCode'] = (100000000*(df_test['iCity'] + 1) + 10000*df_test['LatOff'] + \n                        df_test['LongOff'])\n\n# - - - - -\n# Create distance from city center values,\n# in 10^-4 degree units from LatOff LongOff:\ndf_train['DistToCenter'] = df_train.apply(lambda row: np.sqrt((row.LatOff - 5000) ** 2 +\n                                                    (row.LongOff - 5000) ** 2) , axis=1)\ndf_test['DistToCenter'] = df_test.apply(lambda row: np.sqrt((row.LatOff - 5000) ** 2 +\n                                                    (row.LongOff - 5000) ** 2) , axis=1)\n\n# - - - - -\n# Create the dot product between intersection-to-Center and InHeading unit vectors.\n# So that: 100 means InHeading is toward Center, -100 means InHeading opposite Center.\n# Add Center*InHead, -100 to 100 (will become an integer):\ndf_train['Center*InHead'] = 100.0*( (5000 - df_train['LatOff'])*np.cos(np.pi*df_train['InHeading']\/4.0) + \n                         (5000 - df_train['LongOff'])*np.sin(np.pi*df_train['InHeading']\/4.0)\n                        ) \/ df_train['DistToCenter']\ndf_test['Center*InHead'] = 100.0*( (5000 - df_test['LatOff'])*np.cos(np.pi*df_test['InHeading']\/4.0) + \n                         (5000 - df_test['LongOff'])*np.sin(np.pi*df_test['InHeading']\/4.0)\n                        ) \/ df_test['DistToCenter']\n\n# - - - - -\n# Create Total_p80 as a single measure of delay, Total_p80 =  TTS_p80 + DTFS_p80\n# Covert to integers for cleaner histograming.\ndf_train['Total_p80'] = df_train['DistanceToFirstStop_p80'] + df_train['TotalTimeStopped_p80']\ndf_train['Total_p80'] = df_train['Total_p80'].astype(int)\n#\n# Add these shorter names to Train:\ndf_train['DTFS_p80'] = df_train['DistanceToFirstStop_p80'].astype(int)\ndf_train['TTS_p80'] = df_train['TotalTimeStopped_p80'].astype(int)\n#\n# Make the columns in Test too:\ndf_test['Total_p80'] = -1\ndf_test['DTFS_p80'] = -1\ndf_test['TTS_p80'] = -1\n\nprint(\"\\n{:.2f} seconds -- added basic new feature columns\".format(time() - t_preproc))\n","9de8e8f9":"# Look for NAs - only have them in some street names\n\n##df_train.isnull().sum()\n# Non-zero ones:\n# EntryStreetName            8189\n# ExitStreetName             5534\n\n##df_test.isnull().sum()\n# Non-zero ones:\n# EntryStreetName            19157\n# ExitStreetName             16340\n","654a1c79":"if SHOW_EDA:\n    # Alert if Test is Train:\n    if TEST_IS_TRAIN:\n        print(\"\\n\"+20*\" *\"+\"\\n   TEST is Train !!!\\n\"+20*\" *\"+\"\\n\")\n\n    # Number of locations, i.e., Lattitude and Longitude values:\n    print('Number of Latitudes in Train Set: ', len(df_train.Latitude.unique()))\n    print('Number of Longitudes in Train Set: ', len(df_train.Longitude.unique()))\n    print('Number of Latitudes in TEST Set: ', len(df_test.Latitude.unique()))\n    print('Number of Longitudes in TEST Set: ', len(df_test.Longitude.unique()))\n    print(\"\")\n    # Numbers of entry, exit streets in the data\n    # *** There are more\/different intersecions in TEST than in Train ***\n    #From https:\/\/www.kaggle.com\/harisyammnv\/initial-eda-with-maps)\n    print('Number of Entry Street Names in Train Set: ', len(df_train.EntryStreetName.unique()))\n    print('Number of Exit Street Names in Train Set: ', len(df_train.ExitStreetName.unique()))\n    print('Number of Entry Street Names in TEST Set: ', len(df_test.EntryStreetName.unique()))\n    print('Number of Exit Street Names in TEST Set: ', len(df_test.ExitStreetName.unique()))\n    print(\"\")","85eca0c9":"# Form this to count how many unique InterCode - Month - HrWk combinations show up\n# (InterCode used here was for Intersections, not entry-sections.)\n#  Train:  524,711 from 4793 InterCodes,\n# This is about 1\/3 of full coverage (full = 4793 * 7 * 48 = 1,610,448.)\n#\n##df_train['InterMonHrWk'] = (df_train['InterCode'].astype(str) + \"-\" + df_train['Month'].astype(str) + \n##                            \"-\" + df_train['HrWk'].astype(str))\n\n# Include EntryHeading ExitHeading too - expect these to be unique\n# Mostly, but about 1.5% of entries are duplicates with different street names (same headings).\n##df_train['InterMonHrWk'] = (df_train['InterCode'].astype(str) + \"-\" + df_train['Month'].astype(str) + \n##                            \"-\" + df_train['HrWk'].astype(str) + \"-\" +\n##                            df_train['EntryHeading'] + \"-\" + df_train['ExitHeading'])","f4ea5b99":"# Look at some strange ones\n#\n# Inter-Mon-HrWk:\n#  Cambridge Street - Monsignor O'Brien Highway - East Street\n##df_train[df_train['InterMonHrWk']=='237063240-7-7']\n#  Main Street - Vassar Street - Galileo Galilei Way\n##df_train[df_train['InterMonHrWk']=='236263100-8-9']\n\n# Inter-Mon-HrWk-Entry-Exit with 6 times:\n#   Albany Street - Frontage Road Southbound - NaN\n##df_train[df_train['InterMonHrWk']=='234033361-12-14-SW-SW']","c370415e":"# Get all the unique street names\nstreet_names = []\n\n# In training:\nentry_names = df_train['EntryStreetName'].unique()\nexit_names = df_train['ExitStreetName'].unique()\nfor name in entry_names:\n    street_names.append(name)\nfor name in exit_names:\n    street_names.append(name)\n# and the test ones too:\nentry_names = df_test['EntryStreetName'].unique()\nexit_names = df_test['ExitStreetName'].unique()    \nfor name in entry_names:\n    street_names.append(name)\nfor name in exit_names:\n    street_names.append(name)\n    \nunique_streets = pd.Series(street_names).unique()\n\n# Alert if Test is Train:\nif TEST_IS_TRAIN:\n    print(\"\\n\"+20*\" *\"+\"\\n       TEST is Train !!!\\n\"+20*\" *\"+\"\\n\")\n    \nprint(\"Number of street names in Train & Test: {}\".format(len(unique_streets)))","a1fd3b33":"# Dictionary of recognized types of \"streets\".\n# Use to encode ExitType and EntryType\n# \"nan\" streets will be code 16 (was 0)\n# ones not in this list will be code 25  (about 27 of them)\nstreet_type_dict = {'Street':1, 'St':1, 'Boulevard':2, 'Bld':2, 'Avenue':3, 'Ave':3,\n                     'Road':4, 'Rd':4, 'Lane':5, 'Drive':6,\n                     'Parkway':7, 'Pkwy':7, 'Place':8, 'Way':9, 'Highway':10, 'Circle':11, 'Terrace':12,\n                     'Square':13, 'Court':14, 'Connector':15, 'Bridge':16, 'Overpass':17, 'Tunnel':18,\n                     'Mall':19, 'Wharf':20, 'Expressway':21,\n                     # ones added for Test but not in Train:\n                     'Pike':22}\n# Indices for EntryType sorted on Total_p80 averages (using all training data):\n# [11 12 15 13  2  1 25 17 18 21  7  5  4 19 10  9  0  3 16 14  6  8 20]\n# [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22\n# Relabel the values in the dictionary using this, \n# e.g., 'Circle':11 becomes 'Circle':0;  'Terrace':12 becomes 'Terrace':1;  etc.\nstreet_type_dict = {'Street':5, 'St':5, 'Boulevard':4, 'Bld':4, 'Avenue':17, 'Ave':17,\n                     'Road':12, 'Rd':12, 'Lane':11, 'Drive':20,\n                     'Parkway':10, 'Pkwy':10, 'Place':21, 'Way':15, 'Highway':14, 'Circle':0, 'Terrace':1,\n                     'Square':3, 'Court':19, 'Connector':2, 'Bridge':18, 'Overpass':7, 'Tunnel':8,\n                     'Mall':13, 'Wharf':22, 'Expressway':9,\n                     # ones added for Test but not in Train:\n                     'Pike':14}  # put same as Highway\n\nif SHOW_EDA:\n    print(\"   These words in street names are used to form groups:\")\n    for stkey in street_type_dict.keys():\n        print(stkey)\n\n# Show the streets that are being left out, i.e. will be code 6 (was 25):\nif SHOW_EDA:\n    print(\"\\n   These street names are 'unique' and put in one category:\")\n    for street in unique_streets:\n        if pd.isna(street):\n            pass\n        else:\n            # is it in the dictionary?\n            in_dict = False\n            for street_type in street_type_dict:\n                if street_type in street:\n                    in_dict = True\n            if in_dict == False:\n                print(street)","c5d9dfcb":"# Set an EntryType feature using the dictionary,\n# adapted the code in https:\/\/www.kaggle.com\/dcaichara\/feature-engineering-and-lightgbm\ndef encode(x):\n    global street_type_dict\n    if pd.isna(x):\n        return 16   # was 0\n    for street in street_type_dict.keys():\n        if street in x:\n            return street_type_dict[street]\n    # otherwise\n    return 6  # was 25\n\ndf_train['EntryType'] = df_train['EntryStreetName'].apply(encode)\ndf_train['ExitType'] = df_train['ExitStreetName'].apply(encode)\ndf_test['EntryType'] = df_test['EntryStreetName'].apply(encode)\ndf_test['ExitType'] = df_test['ExitStreetName'].apply(encode)","60eb3ce7":"t_lohi = time()\nprint(\"             ...now adding LoWait, HiWait, iWait...\")\n\n# Categorize each (Entry-)section by some average p80 value(s),\n# setting flags for: NoWait, LoWait, and HiWait.\n\n# - - - - -\n# Set Thresholds and new Columns\n#\n# Threshold Choices:\n#   0: use Total_p80 value for all three of No, Lo, Hi\n#   1: use Total, TTS, and DTFS for No, Lo, Hi, respectively.\nif WAIT_CHOICE == 0:\n    if ENTRYSECTIONS:\n        # These are estimates\n        NoWait_Threshold = 20.0\n        LoWait_Thresh = [120.0, 120.0, 120.0, 120.0]\n        HiWait_Thresh = [500.0,300.0,500.0,500.0]\n    else:  # Intersections\n        # * These have been optimized * (v20)\n        NoWait_Threshold = 20.0\n        LoWait_Thresh = [120.0, 120.0, 120.0, 120.0]\n        HiWait_Thresh = [280.0, 280.0, 280.0, 280.0]\n    print(\"\\n    NoWait_Threshold = {}  on Total_p80\".format(NoWait_Threshold))\n    print(\"    LoWait_Thresh.s = {}, {}, {}, {}  on Total_p80\".format(\n        LoWait_Thresh[0], LoWait_Thresh[1], LoWait_Thresh[2], LoWait_Thresh[3] ))\n    print(\"    HiWait_Thresh.s = {}, {}, {}, {}  on Total_p80 \\n\".format(\n        HiWait_Thresh[0], HiWait_Thresh[1], HiWait_Thresh[2], HiWait_Thresh[3] ))  \nelse:  # WAIT_CHOICE = 1\n    if ENTRYSECTIONS:\n        # * These have been optimized * (v32)\n        NoWait_Threshold = 20.0\n        LoWait_Thresh = [18.0,18.0,18.0,18.0]\n        HiWait_Thresh = [400.0,210.0,400.0,400.0]\n    else:  # Intersections\n        # * These have been optimized * (v21)\n        NoWait_Threshold = 20.0\n        LoWait_Thresh = [18.0,18.0,18.0,18.0]\n        HiWait_Thresh = [210.0,210.0,210.0,210.0]\n    print(\"\\n    NoWait_Threshold = {}  on Total_p80\".format(NoWait_Threshold))\n    print(\"    LoWait_Thresh.s = {}, {}, {}, {}  on TTS_p80\".format(\n        LoWait_Thresh[0], LoWait_Thresh[1], LoWait_Thresh[2], LoWait_Thresh[3] ))\n    print(\"    HiWait_Thresh.s = {}, {}, {}, {}  on DTFS_p80 \\n\".format(\n        HiWait_Thresh[0], HiWait_Thresh[1], HiWait_Thresh[2], HiWait_Thresh[3] ))   \n\n# Flag (entry-)intersections that have No, Low or High average observed wait times;\n# medium is if neither low nor high. The Low includes the No:\n# the No values are just for information, they contribute very little to the RMSE.\ndf_train['NoWait'] = 0\ndf_train['LoWait'] = 0\ndf_train['HiWait'] = 0\n# signal these are not determined with -1\ndf_test['NoWait'] = -1\ndf_test['LoWait'] = -1\ndf_test['HiWait'] = -1\n\n# - - - - -\n# For each (entry-)intersection assign the No, Lo, Hi status\n# using its calculated average p80 value(s) in the training dataset\nfeat_name = 'InterCode'\nval_counts = df_train[feat_name].value_counts()\nfeat_values = np.sort(val_counts.index)\nfor this_val in feat_values:\n    # find this feat_value in the df\n    select = df_train[feat_name] == this_val\n    # and get a dataframe of just those\n    df_feat = df_train[select].copy()\n    total_mean = df_feat['Total_p80'].mean()\n    iCity = (df_feat['iCity'].values)[0]\n    if WAIT_CHOICE == 0:\n        # NoWait\n        if total_mean < NoWait_Threshold:\n            df_train.loc[select,'NoWait'] = 1\n        # LoWait (includes NoWait)\n        if total_mean < LoWait_Thresh[iCity]:\n            df_train.loc[select,'LoWait'] = 1\n        # HiWait\n        if total_mean > HiWait_Thresh[iCity]:\n            df_train.loc[select,'HiWait'] = 1  \n    else:\n        # Here wait is assigned depending on\n        # Total_p80 and TTS_p80 and DTFS_p80\n        # get means of these other two:\n        tts_mean = df_feat['TTS_p80'].mean()\n        dtfs_mean = df_feat['DTFS_p80'].mean()\n        # NoWait\n        if total_mean < NoWait_Threshold:\n            # Very low value for the Total_p80:\n            df_train.loc[select,'NoWait'] = 1\n        # LoWait (includes NoWait)\n        if tts_mean < LoWait_Thresh[iCity]:\n            # Low TTS\n            df_train.loc[select,'LoWait'] = 1\n        # HiWait\n        if dtfs_mean > HiWait_Thresh[iCity]:\n            # High DTFS\n            df_train.loc[select,'HiWait'] = 1\n\n# - - - - -\n# Include an iWait with values: 0-low, 1-medium, 2-high:\n# Define the wait selections\nselect_no = df_train['NoWait'] == 1\nselect_lo = df_train['LoWait'] == 1\nselect_hi = df_train['HiWait'] == 1\n# and a medium selection\nselect_me = (df_train['LoWait'] == 0) & (df_train['HiWait'] == 0)\n\ndf_train['iWait'] = 1\ndf_train.loc[select_lo, 'iWait'] = 0\ndf_train.loc[select_hi, 'iWait'] = 2\n# and set up iWait for Test too, set to -1 for undetermined\ndf_test['iWait'] = -1\n\n\nprint(\"\\n{:.2f} seconds to add No,Lo,Hi and iWait columns\".format(time() - t_lohi))","89a947f2":"# Print out information about the Waits\nprint(\"\\nTrain NoWait fraction = {:.2f}%\".format(100*df_train['NoWait'].mean()))\nprint(\"Train LoWait(w\/NoWait) fraction = {:.2f}%\".format(100*df_train['LoWait'].mean()))\nprint(\"Train HiWait fraction = {:.2f}%\\n\".format(100*df_train['HiWait'].mean()))\n\n# Get the averages for the NoWait, LoWait, MeWait and HiWait rows:\nnowait_aves = [0,0,0,0,0,0]\nlowait_aves = [0,0,0,0,0,0]\nmewait_aves = [0,0,0,0,0,0]\nhiwait_aves = [0,0,0,0,0,0]\n# The NoWait averages\nfor icol, this_col in enumerate(out_cols):\n    nowait_aves[icol] = df_train.loc[select_no,this_col].mean()\n# The LoWait averages (including NoWait)\nfor icol, this_col in enumerate(out_cols):\n    lowait_aves[icol] = df_train.loc[select_lo,this_col].mean()\n# The medium-wait averages\nfor icol, this_col in enumerate(out_cols):\n    mewait_aves[icol] = df_train.loc[select_me,this_col].mean()\n# The HiWait averages:\nfor icol, this_col in enumerate(out_cols):\n    hiwait_aves[icol] = df_train.loc[select_hi,this_col].mean()\n\nprint(\"Averages of the 6 target values for the NoWait training data: \\n\" +\n      \"    {:.2f}, {:.2f}, {:.2f}, {:.2f}, {:.2f}, {:.2f}\".format(\n    nowait_aves[0],nowait_aves[1],nowait_aves[2],nowait_aves[3],nowait_aves[4],nowait_aves[5]))\nprint(\"Averages of the 6 target values for the LoWait training data: \\n\" +\n      \"    {:.2f}, {:.2f}, {:.2f}, {:.2f}, {:.2f}, {:.2f}\".format(\n    lowait_aves[0],lowait_aves[1],lowait_aves[2],lowait_aves[3],lowait_aves[4],lowait_aves[5]))\nprint(\"Averages of the 6 target values for the medium-wait training data: \\n\" +\n      \"    {:.2f}, {:.2f}, {:.2f}, {:.2f}, {:.2f}, {:.2f}\".format(\n    mewait_aves[0],mewait_aves[1],mewait_aves[2],mewait_aves[3],mewait_aves[4],mewait_aves[5]))\nprint(\"Averages of the 6 target values for the HiWait training data: \\n\" +\n      \"    {:.2f}, {:.2f}, {:.2f}, {:.2f}, {:.2f}, {:.2f}\".format(\n    hiwait_aves[0],hiwait_aves[1],hiwait_aves[2],hiwait_aves[3],hiwait_aves[4],hiwait_aves[5]))\n","e83948ce":"# Try to free up memory I don't need...\ndel df_feat, val_counts, feat_values, select, select_no, select_lo, select_me, select_hi\n# collect any garbage?\ngc_dummy = gc.collect()","abb3bf95":"if SHOW_EDA:\n    # Alert if Test is Train:\n    if TEST_IS_TRAIN:\n        print(\"\\n\"+20*\" *\"+\"\\n   TEST is Train !!!\\n\"+20*\" *\"+\"\\n\")\n\n    print(\"\\n   Number of Unique (Entry-)Intersection Codes\\n\")\n    for icity, this_city in enumerate(cities):\n        # Numer of unique intersections in each city\n        print(this_city+': Train Set: ',\n              len(df_train[df_train['iCity'] == icity].InterCode.unique()),\n              \"  No waits: \", len(df_train[(df_train['iCity'] == icity) &\n                           (df_train['NoWait'] == 1)].InterCode.unique()),\n              \",  Low(&No) waits: \", len(df_train[(df_train['iCity'] == icity) &\n                           (df_train['LoWait'] == 1)].InterCode.unique()),\n              \",  med waits: \", len(df_train[(df_train['iCity'] == icity) &\n                           (df_train['iWait'] == 1)].InterCode.unique()),\n              \",  High waits: \", len(df_train[(df_train['iCity'] == icity) &\n                           (df_train['HiWait'] == 1)].InterCode.unique()))\n        print(this_city+':  TEST Set: ',\n              len(df_test[df_test['iCity'] == icity].InterCode.unique()), '\\n')\n    # show the totals too\n    print(\"      All Cities Train :  {}\".format(len(df_train.InterCode.unique())))\n    print(\"      All Cities  TEST :  {}\".format(len(df_test.InterCode.unique())))","49e39024":"if SHOW_EDA:\n    # Show all columns\n    print(\"All columns:\\n\")\n    print(df_train.columns)","841dd5a6":"# Down-select to just columns I may use,\n# add more if\/as desired.\n\nusing_cols = (['RowId', 'Latitude', 'Longitude', 'Month', 'iCity', 'HrWk', 'TurnLSR', 'Turn',\n             'LatOff', 'LongOff', 'InterCode',\n             'ExHeading', 'InHeading',\n             'DistToCenter', 'Center*InHead',\n             'Total_p80', 'DTFS_p80', 'TTS_p80',\n             ##'Atlanta', 'Boston', 'Chicago', 'Philadelphia',\n             'NoWait', 'LoWait', 'HiWait', 'iWait', \n             'EntryType', 'ExitType'] + \n              out_cols )   # the targets are included\n\n# Make sure that some of the columns are integers\ninteger_cols = ['RowId', 'Month', 'iCity', 'HrWk', 'TurnLSR', 'Turn',\n             'LatOff', 'LongOff', 'InterCode',\n             'ExHeading', 'InHeading',\n             ##'DistToCenter', 'Center*InHead',   # leave these as floats\n             ##'Atlanta', 'Boston', 'Chicago', 'Philadelphia',\n             'NoWait', 'LoWait', 'HiWait', 'iWait',\n             'EntryType', 'ExitType']\n\ndf_train = df_train[using_cols].copy()\ndf_test = df_test[using_cols].copy()\nfor col in integer_cols:\n    df_train[col] = df_train[col].astype(int)\n    df_test[col] = df_test[col].astype(int)\n    ","9084f128":"if INTER_DF:\n    # Assemble a dataframe of the intersections and their properties.\n    # Some new features are created as well.\n    t_df_inter = time()\n    \n    # Train\n    val_counts = df_train['InterCode'].value_counts()\n\n    train_inter = pd.DataFrame(val_counts)\n\n    train_inter = train_inter.reset_index()\n    train_inter.columns = ['InterCode','num_train']\n    train_inter = train_inter.sort_values(by='InterCode').reset_index().drop('index',axis=1)\n\n    # TEST\n    val_counts = df_test['InterCode'].value_counts()\n\n    test_inter = pd.DataFrame(val_counts)\n\n    test_inter = test_inter.reset_index()\n    test_inter.columns = ['InterCode','num_test']\n    test_inter = test_inter.sort_values(by='InterCode').reset_index().drop('index',axis=1)\n","70f00592":"if INTER_DF:\n    # Combine the two\n    inter_merge = pd.merge(train_inter, test_inter, on='InterCode', how='outer')\n\n    # Fill NaNs and make all entries integers\n    inter_merge = inter_merge.fillna(value={'num_train':-1.0, 'num_test':-1.0})\n    inter_merge = inter_merge.astype(int)\n\n    if SHOW_EDA:\n        print(inter_merge.head(5))","39596100":"if INTER_DF:\n    # Add other columns to the merged df\n    # - ones directly from Train\/Test\n    # - means of p80s: Total, DTFS, TTS\n    # - number of unique ExHeadings for the entry-section\n    t_df_inter = time()\n\n    # Flag if it is in Train and\/or Test (can be both)\n    inter_merge['Train'] = (inter_merge['num_train'] > 0).astype(int)\n    inter_merge['Test'] = (inter_merge['num_test'] > 0).astype(int)\n\n    # columns to add that depend only on InterCode\n    add_cols = ['iCity','LatOff','LongOff','DistToCenter',\n            'iWait','EntryType','InHeading','Center*InHead']\n    # averages of targets over entries with the same InterCode\n    p80_cols = ['Total_p80', 'DTFS_p80', 'TTS_p80']\n    \n    \n    # setup the new columns\n    for icol in add_cols:\n        inter_merge[icol] = -1\n    for icol in p80_cols:\n        inter_merge[\"Ave\"+icol] = -1\n    inter_merge['UniqueExits'] = -1\n    \n    # go through the InterCodes and fill columns\n    for indx in (inter_merge.index):\n        intercode = inter_merge.loc[indx,'InterCode']\n        # In training?\n        if inter_merge.loc[indx,'Train'] > 0:\n            df_select = df_train[df_train['InterCode'] == intercode].copy()\n            # target values only available in train:\n            for icol in p80_cols:\n                inter_merge.loc[indx,\"Ave\"+icol] = df_select[icol].mean() \n        else:\n            # get info from test\n            df_select = df_test[df_test['InterCode'] == intercode].copy()\n        # these are the same for train\/test once df_select is set\n        for icol in add_cols:\n            inter_merge.loc[indx,icol] = df_select[icol].mean()\n        # number of unique exit headings\n        inter_merge.loc[indx,'UniqueExits'] = len(df_select['ExHeading'].unique())\n\n    # Make values integers (doing this for histogramsing?)\n    inter_merge = inter_merge.fillna(value=-1)\n    inter_merge = inter_merge.astype(int)\n\n    print(\"\\n {:.2f} seconds to fill basic (entry-)intersection dataframe.\\n\".format(time() - t_df_inter))\n","a4d75173":"if INTER_DF:\n    # Add column(s) calculated just from the inter_merge data,\n    # For each entry-section calculate:\n    # - the spatial density of that City around the entry-section (number in small, local region)\n    # - the distance to the closest other entry-section that is in the -InHead direction\n    #   (When ENTRYSECTIONS=False the InHeading is an average so this may not mean much?)\n    t_intercols = time()\n    \n    # Local density, in square region (2*radius x 2*radius)\n    radius = 100\n    # Calculate the values into a list and then load results into the df\n    local_dens = []\n    inter_merge['LocalDensity'] = -1\n    \n    # Length of the entry-section\n    entry_lens = []\n    inter_merge['EntryLength'] = -1  \n    \n    # Go through the entry-sections\n    for iinter in inter_merge.index:\n        this_lat = inter_merge.loc[iinter,'LatOff']\n        this_long = inter_merge.loc[iinter,'LongOff']\n        this_city = inter_merge.loc[iinter,'iCity']\n        # select the nearby ones from the same city\n        select = ( (inter_merge.loc[iinter,'iCity'] == this_city) &\n                                (inter_merge['LatOff'] < (this_lat + radius)) &\n                                (inter_merge['LatOff'] > (this_lat - radius)) &\n                                (inter_merge['LongOff'] < (this_long + radius)) &\n                                (inter_merge['LongOff'] > (this_long - radius)) )\n        # LocalDensity is the number in the selection\n        local_dens.append(sum(select))\n        #\n        # Find closest entry-section in the -InHead direction\n        theta_inhead = (np.pi*inter_merge.loc[iinter,'InHeading'])\/4.0\n        delta_lat = inter_merge.loc[select,'LatOff'] - this_lat\n        delta_long = inter_merge.loc[select,'LongOff'] - this_long\n        delta_dot_inhead = (np.sin(theta_inhead) * delta_long +\n                    np.cos(theta_inhead) * delta_lat)\/np.sqrt(delta_lat**2 + delta_long**2)\n        # want delta_dot_inhead to be negative, say < -0.5\n        # down-select to just those:\n        select = (select & (delta_dot_inhead < -0.85))\n        # Now, find the closest, non-zero, entry-section among these:\n        d_sqrs = np.sqrt(np.sort((inter_merge.loc[select,'LatOff'] - this_lat)**2 +\n                (inter_merge.loc[select,'LongOff'] - this_long)**2))\n        this_len = 1.4*radius\n        for this_d in d_sqrs:\n            if this_d > 0:\n                this_len = this_d\n                break\n        entry_lens.append(this_len)\n        \n    # Put the values in the dataframe           leave them as floats\n    inter_merge['LocalDensity'] = local_dens\n    inter_merge['EntryLength'] = entry_lens\n    \n    print(\"\\n {:.2f} seconds to add LocalDensity and EntryLength to (entry-)intersection dataframe.\\n\".format(time() - t_intercols))\n","d92c1992":"if INTER_DF:\n    del val_counts, train_inter, test_inter, df_select\n    gc_dummy = gc.collect()","3cabc099":"if INTER_DF and SHOW_EDA:\n    # show some of the dataframe:\n    print(inter_merge.head(5))\n    print(inter_merge.tail(5))\n    \n    # Note if ENTRYSECTIONS=False:\n    if ENTRYSECTIONS == False:\n        print(\"\\nNOTE: These rows are  *** Intersections ***\\n\" +\n              \"      so the following are averages over all the Entries of each intersection:\\n\" +\n             \"          EntryType, InHeading, Center*InHead\")","d95af01f":"if INTER_DF and SHOW_EDA:\n    # Scatter plot between these two new entry-section features:\n    inter_merge.plot.scatter('LocalDensity','EntryLength',figsize=(9,6),alpha=0.5)\n\n    plt.savefig(out_dir+\"\/\"+\"LocalDensity_EntryLength\"+\"_scatter_\"+version_str+\".png\")\n    plt.show()","ebf5cad3":"if INTER_DF and SHOW_EDA:\n    # Look at the histograms of some value\n\n    feat_to_hist = 'EntryLength'\n    ##feat_to_hist = 'LocalDensity'\n    ##feat_to_hist = 'AveDTFS_p80'\n    ##feat_to_hist = 'AveTTS_p80'\n    ##feat_to_hist = 'DistToCenter'\n    ##feat_to_hist = 'InHeading'\n    ##feat_to_hist = 'iWait'\n    ##feat_to_hist = 'num_test'\n    ##feat_to_hist = 'EntryType'\n    \n    ##by_feat = 'iCity'\n    by_feat = 'iWait'\n    ##by_feat = 'InHeading'\n\n\n    print(\"\\n Histograms of  log10( \"+feat_to_hist+\" )  for different  \"+by_feat+\"  values:\")\n    if by_feat == 'iWait':\n        print(40*\" \"+\"(iWait = -1 for Test-only entry-sections.)\")\n    # Calculate the value to histogram\/plot - usually log scale\n    inter_merge['plot_this'] = np.log10(2.0+inter_merge[feat_to_hist])\n    inter_merge.hist('plot_this',bins=50,by=by_feat,sharex=True,figsize=(12,8))\n\n    plt.savefig(out_dir+\"\/\"+feat_to_hist+\"_hists_\"+version_str+\".png\")\n    plt.show()\n\n    # Drop the plotting column\n    inter_merge = inter_merge.drop('plot_this', axis=1)","b896e1ad":"# Plot Intersections\nif INTER_DF and SHOW_EDA:\n    # zoom in on city center\n    zoom_inter = False\n    for iCity in range(4):\n   \n        # Discrete colors for iWait\n        if True:\n            print(\"\\n\"+8*\" \"+\"Showing iWait values color-coded:  Yellow(0) - Blue(1) - Red(2) \\n\" +\n                 \"\\n\"+13*\" \"+\"Test-only intersections indicated with a '+'\")\n        \n            ax = inter_merge[(inter_merge['iCity'] == iCity) & (inter_merge['iWait'] == 0) &\n               (inter_merge['Train'] == 1)].plot.scatter(\"LongOff\",\"LatOff\",\n                        figsize=(10,8),c='yellow',alpha=0.7,s=10)\n            inter_merge[(inter_merge['iCity'] == iCity) & (inter_merge['iWait'] == 1) &\n               (inter_merge['Train'] == 1)].plot.scatter(\"LongOff\",\"LatOff\",\n                        figsize=(10,8),c='blue',alpha=0.7,s=5, ax=ax)\n            inter_merge[(inter_merge['iCity'] == iCity) & (inter_merge['iWait'] == 2) &\n               (inter_merge['Train'] == 1)].plot.scatter(\"LongOff\",\"LatOff\",\n                        figsize=(10,8),c='red',alpha=0.7,s=16, ax=ax)\n\n        # Points colored by DTFS_p80\n        # colormaps: viridis_r   gnuplot_r    plasma_r\n        if False:\n            inter_merge[(inter_merge['iCity'] == iCity) &\n                   (inter_merge['Train'] == 1)].plot.scatter(\"LongOff\",\"LatOff\",\n                        figsize=(10,8),c='DTFS_p80',alpha=0.7,\n                        s=4+4*(inter_merge['iWait'])**2,colormap=\"viridis_r\",colorbar=True)\n \n\n        # Overplot the locations of the Test-only intersections (unknown iWait)\n        test_only = ((inter_merge['iCity'] == iCity) & (inter_merge['Train']== 0) &\n                            (inter_merge['Test']== 1))\n        test_lats = inter_merge[test_only].LatOff\n        test_longs = inter_merge[test_only].LongOff\n        plt.plot(test_longs,test_lats,color='black',marker='+',linestyle='',markersize=5)\n    \n        # and the locations of the city center and airport\n        plt.plot([5000],[5000],color='lime',marker='*',linestyle='',markersize=12)\n        plt.plot([longoff_air[iCity]],[latoff_air[iCity]],color='lime',marker='*',linestyle='',markersize=12)\n    \n        if zoom_inter:\n            plt.xlim(4500,5500)\n            plt.ylim(4500,5500)\n        \n        plt.title(cities[iCity]+\" \")\n        if zoom_inter:\n            plt.savefig(out_dir+\"\/\"+cities[iCity]+\"_congestion_zoom_\"+version_str+\".png\")\n        else:\n            plt.savefig(out_dir+\"\/\"+cities[iCity]+\"_congestion_map_\"+version_str+\".png\")\n        plt.show()","4d161497":"if INTER_DF:\n    # Finally, transfer new values in the inter_merge df to the Train and Test dfs:\n    t_density = time()\n    \n    df_train['UniqueExits'] = -1\n    df_test['UniqueExits'] = -1\n    df_train['LocalDensity'] = -1\n    df_test['LocalDensity'] = -1\n    df_train['EntryLength'] = -1\n    df_test['EntryLength'] = -1\n    for iinter in inter_merge.index:\n        inter_code = inter_merge.loc[iinter,'InterCode']\n        #\n        select = (df_train['InterCode'] == inter_code)\n        df_train.loc[select,'UniqueExits'] = inter_merge.loc[iinter,'UniqueExits']\n        df_train.loc[select,'LocalDensity'] = inter_merge.loc[iinter,'LocalDensity']\n        df_train.loc[select,'EntryLength'] = inter_merge.loc[iinter,'EntryLength']\n        #\n        select = (df_test['InterCode'] == inter_code)\n        df_test.loc[select,'UniqueExits'] = inter_merge.loc[iinter,'UniqueExits']\n        df_test.loc[select,'LocalDensity'] = inter_merge.loc[iinter,'LocalDensity']\n        df_test.loc[select,'EntryLength'] = inter_merge.loc[iinter,'EntryLength']\n\n    print(\"\\n {:.2f} seconds, added UniqueExits, LocalDensity, EntryLength to Train,Test.\\n\".format(time() - t_density))","d2e5cc97":"##df_train.tail(10)","c7d96519":"if SHOW_EDA:\n    # List the column names\n    print(\"Selected columns:\\n\")\n    print(df_train.columns)","362ae835":"print(\"\\nSize of Train and Test: {}, {}\".format(len(df_train), len(df_test)))","23d68fec":"if SHOW_EDA:\n    # Show all the stats of the numeric columns\n    desc_train = df_train.describe()\n    # Transpose for better printing out\n    print(\"Stats for the Training data:\\n\")\n    print((desc_train.T)[['count','mean','min','max']])","86036e70":"if SHOW_EDA:\n    # Alert if Test is Train:\n    if TEST_IS_TRAIN:\n        print(\"\\n\"+20*\" *\"+\"\\n   TEST is Train !!!\\n\"+20*\" *\")\n    # Compare Train and Test averages of the feature values\n    # Estimate the z-score of the difference, significant if outside +\/-5.\n    # Do this by City\n    for icity, this_city in enumerate(cities):\n        # Using a z-score with standard error based on the number of samples\n        descr_train_c = df_train[df_train['iCity'] == icity].describe()\n        descr_test_c = df_test[df_test['iCity'] == icity].describe()\n        print(\"\\n\\n\"+5*\" \"+this_city+\n              \"  (Lat.: {} -- {}\".format(descr_test_c.loc[\"min\",\"LatOff\"],descr_test_c.loc[\"max\",\"LatOff\"]) +\n                \",  Long.: {} -- {}\".format(descr_test_c.loc[\"min\",\"LongOff\"],descr_test_c.loc[\"max\",\"LongOff\"]) +\n              \") \\n\")\n        # Number of samples in the test set\n        n_test = descr_test_c.loc[\"count\",\"HrWk\"]\n        n_train = descr_train_c.loc[\"count\",\"HrWk\"]\n        print(\"     --column--    z-score      TEST Mean     Train Mean\")\n        # Select the columns to show here, most of them:\n        ##for col in descr_test.columns.drop('iCity').drop('RowId'):\n        # or just some chosen (non-target) ones:\n        ##for col in ['Month','Hour','Weekend','HrWk','Latitude','Longitude',\n        ##            'InHeading','ExHeading','Turn','TurnLSR','Total_p80']:\n        for col in descr_test_c.columns.drop('iCity').drop('RowId'):\n            ave_test = descr_test_c.loc[\"mean\",col]\n            ave_train = descr_train_c.loc[\"mean\",col]\n            std_train = descr_train_c.loc[\"std\",col]\n            if np.isnan(std_train):\n                std_train = 1.0\n            print(col.rjust(15), \n                    '{:.4f}'.format((ave_test - ave_train)\/\n                               (std_train*np.sqrt(1.0\/n_test+1.0\/n_train))).rjust(10),\n                    '{:.4f}'.format(ave_test).rjust(14),\n                    '{:.4f}'.format(ave_train).rjust(14))\n","d6342f59":"if SHOW_EDA:\n    # Look at a feature\n    #\n    # Common to Train and Test:\n    #  RowId, IntersectionId, Latitude, Longitude,\n    #  EntryStreetName, ExitStreetName, EntryHeading, ExitHeading,\n    #  Hour, Weekend, Month, Path, City\n    #\n    # The \"y\"s that are unique to Train:\n    #  TotalTimeStopped_p    20,40,50,60,80\n    #  TimeFromFirstStop_p   20,40,50,60,80\n    #  DistanceToFirstStop_p 20,40,50,60,80\n    \n    ##feat_name = 'EntryLength'\n    ##feat_name = 'LocalDensity'\n    ##feat_name = 'UniqueExits'\n    ##feat_name = 'Center*InHead'\n    ##feat_name = 'DistToCenter'\n    ##feat_name = 'EntryType'\n    ##feat_name = 'InterCode'\n    feat_name = 'HrWk'\n    ##feat_name = 'Turn'\n    \n    ##feat_name = 'TotalTimeStopped_p80'\n    ##feat_name = 'DistanceToFirstStop_p80'\n    \n    val_counts = df_train[feat_name].value_counts()\n    feat_values = np.sort(val_counts.index)\n    print(feat_name+\" has {} distinct values\".format(len(val_counts)) +\n            \" from {} to {}\".format(feat_values[0],feat_values[-1]))\n    # Find the mean of the non-zero values:\n    if '_p' in feat_name:\n        df_temp2 = df_train[df_train[feat_name] > 0.0].copy()\n        print(15*\" \"+\"Mean of All: {:.2f}, Mean of non-zeros: {:.2f}\".format(df_train[feat_name].mean(),\n                                    df_temp2[feat_name].mean()))\n    # Show them if not too many\n    if (len(val_counts) < 30.0):\n        print(val_counts)\n    else:\n        print(val_counts[val_counts.index[0:10]])\n        print(val_counts[val_counts.index[-5:-1]])\n","e48439c0":"# Look at the histogram of this feature's TRAINING values\n# provided that it is numeric:\nif SHOW_EDA and (feat_name in desc_train.columns):\n    \n    # Histogram of Training values\n    df_train[feat_name].hist(bins=2*(2*48-1), figsize=(10,4), grid=False, color='orange')\n    plt.xlabel(feat_name + \" values\")\n    plt.ylabel(\"Number of samples\")\n    plt.title(feat_name + \" -- Training\")\n    plt.savefig(out_dir+\"\/\"+feat_name+\"_Train_\"+version_str+\".png\", bbox_inches='tight')\n    plt.show()\n\n    # Calculate the average Total_p80 for each feature value\n    total_p80_aves = []\n    # Number of feature values with NoWait\n    n_now = 0\n    for this_val in feat_values:\n        # df of ones with this feature value\n        df_feat = df_train[(df_train[feat_name] == this_val)]\n        feat_mean = df_feat['Total_p80'].mean()\n        total_p80_aves.append(feat_mean)\n        if feat_mean < NoWait_Threshold:\n            n_now += 1\n    \n    if False:\n        # List the features in total_p80 sorted order\n        # Did this to change the street-type dictionary so that \n        # EntryType numerical values are in the same order as their total_p80_aves.\n        sort_indx = np.argsort(total_p80_aves)\n        print(feat_values[sort_indx])\n        print((np.array(total_p80_aves))[sort_indx])\n        # Low to Hi indices for EntryType:\n        # [11 12 15 21 13  2 25  1 17  4  9 19 14  7 10 18  5  3  0  6 16 20  8]\n        #   . . . for ExitType it's different:\n        # \n    \n    \n    # Plot the Ave'p80 vs feat_values\n    plt.figure(figsize=(10,4))\n    plt.plot(feat_values, total_p80_aves, marker=\"o\", linestyle='')\n    plt.xlabel(feat_name + \" values\")\n    plt.ylabel(\"Average Total_p80\")\n    plt.title(feat_name + \" -- Training\"+\n          \"  (The number below {} is: {} \/ {})\".format(NoWait_Threshold,n_now,len(feat_values)))\n    # start at 0:\n    plt.ylim(0.0,)\n    plt.savefig(out_dir+\"\/\"+feat_name+\"_Train_p80_\"+version_str+\".png\", bbox_inches='tight')\n    plt.show()\n\n    # Histogram of this feature's TEST values\n    if feat_name in df_test.columns:\n        df_test[feat_name].hist(bins=2*(2*48-1), figsize=(10,4), grid=False, color='orange')\n        plt.xlabel(feat_name + \" values\")\n        plt.ylabel(\"Number of samples\")\n        plt.title(feat_name + \" -- TEST\")\n        plt.savefig(out_dir+\"\/\"+feat_name+\"_TEST_\"+version_str+\".png\", bbox_inches='tight')\n        plt.show()","9bf94aa3":"# Make an array to store the average values of the 6 target values\n# based on the City, Month, Hour+Weekend, and iWait values.\n# This divides the samples into :\n#    4x12(really 7*)x48x3 = 4032 categories.\n#              (* only 7 months have appreciable data)\n# Here the months have also been collapsed to 3, so the total is\n#     4 x 3 x 48 x 3  =  1728 different lookup-value sets of the 6 targets.\nt_lookup = time()\n\n# Lookup array of mean values of the 6 output values\n# (the T in the variable is a hold-over from including Turn)\nlookup_CMHWTV = np.zeros([4,3,48,3,6])\n\n# Fill the array with average values from the training set\n\nfor iCity in range(4):\n    for iMonth in [0,1,2]:      # instead of 0-11, range(12):\n        if SHOW_EDA:\n            print(\"... doing iCity = {},  Month = {}\".format(iCity,iMonth+1))\n        # get just that part of the df_train\n        df_lookup = df_train[(df_train['iCity'] == iCity) &\n                          (df_train['Month'] == iMonth+1)].copy()\n        for iHrWk in range(48):\n            for iWait in range(3):\n                if True:\n                    select = ( (df_lookup['HrWk'] == iHrWk) &\n                          (df_lookup['iWait'] == iWait)  )\n                    df_temp2 = df_lookup.loc[select].copy()\n                    if len(df_temp2) > 0:\n                        # Get all six of the output values\n                        for icol in range(6):\n                            lookup_CMHWTV[iCity,iMonth,iHrWk,iWait,icol] = int(\n                                0.5+df_temp2[out_cols[icol]].mean())\n\nprint(\"Filling the lookup array took {:.3f} seconds.\".format(time() -  t_lookup))","3e47c59a":"# There are not as many of the iWait=2 values,\n# so smooth the values vs HrWk for them.\n# do \"1 2 1\" smothing, twice: --> equivalent to 1,4,6,4,1\n\nfor iCity in range(4):\n    for iMonth in [0,1,2]:\n        iWait=2\n        if True:\n            for icol in range(6):\n                new_vals = np.zeros(48)\n                for iHrWk in range(1,47):\n                    new_vals[iHrWk] = int(0.5 +\n                                    0.25*lookup_CMHWTV[iCity,iMonth,iHrWk-1,iWait,icol] +\n                                    0.50*lookup_CMHWTV[iCity,iMonth,iHrWk,iWait,icol] +\n                                    0.25*lookup_CMHWTV[iCity,iMonth,iHrWk+1,iWait,icol] )\n                new_vals[0] = int(0.5 +\n                                    0.25*lookup_CMHWTV[iCity,iMonth,47,iWait,icol] +\n                                    0.50*lookup_CMHWTV[iCity,iMonth,0,iWait,icol] +\n                                    0.25*lookup_CMHWTV[iCity,iMonth,1,iWait,icol] )\n                new_vals[47] = int(0.5 +\n                                    0.25*lookup_CMHWTV[iCity,iMonth,46,iWait,icol] +\n                                    0.50*lookup_CMHWTV[iCity,iMonth,47,iWait,icol] +\n                                    0.25*lookup_CMHWTV[iCity,iMonth,0,iWait,icol] )\n                lookup_CMHWTV[iCity,iMonth, : ,iWait,icol] = new_vals\n# and again\nfor iCity in range(4):\n    for iMonth in [0,1,2]:\n        iWait=2\n        if True:\n            for icol in range(6):\n                new_vals = np.zeros(48)\n                for iHrWk in range(1,47):\n                    new_vals[iHrWk] = int(0.5 +\n                                    0.25*lookup_CMHWTV[iCity,iMonth,iHrWk-1,iWait,icol] +\n                                    0.50*lookup_CMHWTV[iCity,iMonth,iHrWk,iWait,icol] +\n                                    0.25*lookup_CMHWTV[iCity,iMonth,iHrWk+1,iWait,icol] )\n                new_vals[0] = int(0.5 +\n                                    0.25*lookup_CMHWTV[iCity,iMonth,47,iWait,icol] +\n                                    0.50*lookup_CMHWTV[iCity,iMonth,0,iWait,icol] +\n                                    0.25*lookup_CMHWTV[iCity,iMonth,1,iWait,icol] )\n                new_vals[47] = int(0.5 +\n                                    0.25*lookup_CMHWTV[iCity,iMonth,46,iWait,icol] +\n                                    0.50*lookup_CMHWTV[iCity,iMonth,47,iWait,icol] +\n                                    0.25*lookup_CMHWTV[iCity,iMonth,0,iWait,icol] )\n                lookup_CMHWTV[iCity,iMonth, : ,iWait,icol] = new_vals\n","652a18e3":"# Fill arrays with correction lookup factors for DTFS and TTS p80 vs the Turn value\n# Just by City and Wait and Turn (-4,-3,-2,-1,0,1,2,3).\nif True:\n    \n    tts_turn_lookup = np.zeros([4,3,8])\n    # For TTS   out_cols[2]\n    for iCity in range(4):\n        for iWait in range(3):\n            ##print(\"iCity,iWait = {},{}:\".format(iCity,iWait))\n            ave_all_turns = df_train.loc[((df_train['iCity'] == iCity) &\n                       (df_train['iWait'] == iWait)), out_cols[2]].mean()\n            for iturn in [-4,-3,-2,-1,0,1,2,3]:\n                tts_turn_lookup[iCity,iWait,iturn+4] = df_train.loc[((df_train['Turn'] == iturn) &\n                       (df_train['iCity'] == iCity) &\n                       (df_train['iWait'] == iWait)), out_cols[2]].mean()\/ave_all_turns\n    # replace any NaNs with 1.5:\n    tts_turn_lookup[np.isnan(tts_turn_lookup)] = 1.5\n        \n    dtfs_turn_lookup = np.zeros([4,3,8])\n    # For DTFS   out_cols[5]\n    for iCity in range(4):\n        for iWait in range(3):\n            ##print(\"iCity,iWait = {},{}:\".format(iCity,iWait))\n            ave_all_turns = df_train.loc[((df_train['iCity'] == iCity) &\n                       (df_train['iWait'] == iWait)), out_cols[5]].mean()\n            for iturn in [-4,-3,-2,-1,0,1,2,3]:\n                dtfs_turn_lookup[iCity,iWait,iturn+4] = df_train.loc[((df_train['Turn'] == iturn) &\n                       (df_train['iCity'] == iCity) &\n                       (df_train['iWait'] == iWait)), out_cols[5]].mean()\/ave_all_turns\n    # replace any NaNs with 1.0:\n    dtfs_turn_lookup[np.isnan(dtfs_turn_lookup)] = 1.0\n    \nelse:\n    tts_turn_lookup = np.ones([4,3,3])\n    dtfs_turn_lookup = np.ones([4,3,3])","5c514269":"# Look at the Lookup values\n#   lookup_CMHWTV[iCity,iMonth,iHrWk,iWait,icol]\n\nif SHOW_EDA:\n    # Make plots of the values...\n    # assign colors\n    clr_wait = ['green','darkorange','pink']\n    clr_month123 = ['red','darkorange','blue']\n    iCity = 0\n    for iCity in range(4):\n        plt.figure(figsize=(10,4))\n        for iMonth in [0,1,2]:\n            for iWait in [2,1,0]:\n                # Get an array of Total_p80 vs iHrWk\n                array_p80 = np.zeros(48)\n                array_p80  += (lookup_CMHWTV[iCity,iMonth,:,iWait,2] +\n                                               lookup_CMHWTV[iCity,iMonth,:,iWait,5])\n                plt.plot(array_p80, color=clr_month123[iMonth],marker='o',alpha=0.6)\n        plt.xlabel(\"Hours (week day 0-23, weekend 24-47)\")\n        if ENTRYSECTIONS:\n            plt.ylim(0,950)\n        else:\n            plt.ylim(0,600)\n        plt.ylabel(\"TTS_p80 + DTFS_p80\")\n        plt.title(cities[iCity]+\" -- Lookup table values vs HrWk, for iWait=0,1,2 and 'iMonth'=0,1,2\")\n    \n        plt.savefig(out_dir+\"\/\"+cities[iCity]+\"_lookup_vs_HrWk_\"+version_str+\".png\")\n        plt.show()","2ba311d0":"# Compare the Lookup Predictions with actual p80 values\n\n# Pick a City:  0,1,2,3  Atlanta, Boston, Chicago, Philly\niCity = 3\n# Pick a month  0,1,2   +1 --> Month=1,2,3\niMonth = 0\n# Will plot vs HrWk\n# iHrWk\n# Pick an iWait  0,1,2\niWait = 2\n\nheading_col ='InHeading'\n\nif SHOW_EDA:\n    plt.figure(figsize=(14,7))\n    \n    # Get some of the actual values for these and overplot them\n    select = ( (df_train['iCity'] == iCity) & (df_train['Month'] == iMonth+1) & \n              (df_train['iWait'] == iWait) )\n    num_select = sum(select)\n    df_select = df_train[select].copy()\n    \n    print(\"\\nNumber of selected df_Train values = \" + \n          \"{},  average of {:.2f} entries\/hour\".format(num_select, num_select\/48.0))\n    \n    hrwk_values = df_select['HrWk'].copy()\n    # blur the HrWk values\n    hrwk_values +=  + 0.15*np.random.randn(len(hrwk_values))\n    \n    tts_values = df_select['TotalTimeStopped_p80']\n    dtfs_values = df_select['DistanceToFirstStop_p80']\n    \n    plt.plot(hrwk_values, np.log10(1.0+dtfs_values), linestyle='',\n             marker='.',alpha=0.3,color=(0.7,0.7,0.5))\n    \n    if True:\n        if True:\n            # Get arrays for the lookup TTS and DTFS\n            tts_lookup_values = np.zeros(48)\n            dtfs_lookup_values = np.zeros(48)\n            tts_lookup_values = lookup_CMHWTV[iCity,iMonth,:,iWait,2]\n            dtfs_lookup_values = lookup_CMHWTV[iCity,iMonth,:,iWait,5]\n            #\n            plt.plot(np.log10(1.0+dtfs_lookup_values), color='gray',marker='o',alpha=0.8)\n\n\n    # Overplot a single intersection's values\n    inters_select = df_select['InterCode']\n    inters_select = inters_select.unique()\n    num_inters = len(inters_select)\n    print(\"Number of unique (Entry-)Intersections selected = \" + \n          \"{},  average of {:.2f} values\/inter-hour\".format(num_inters, num_select\/(48.0*num_inters)))\n    \n    \n    # select the intersection  (middle one =  int(len(inters_select)\/2) )\n    iinter = int(len(inters_select)\/2) + 17\n    \n    if True:\n        # get its values:\n        inter_hrwk = df_select.loc[df_select['InterCode'] == inters_select[iinter], 'HrWk'].values\n        inter_dtfs = df_select.loc[df_select['InterCode'] == inters_select[iinter], 'DistanceToFirstStop_p80'].values\n        inter_tts = df_select.loc[df_select['InterCode'] == inters_select[iinter], 'TotalTimeStopped_p80'].values\n        inter_head = df_select.loc[df_select['InterCode'] == inters_select[iinter], heading_col].values\n        head_clrs = ['black','red','orange','yellow','green','blue','purple','gray']\n        # and plot them - offset HrWk by 0.3 hour to stand out better\n        for ptind in range(len(inter_hrwk)):\n            plt.plot([0.3+inter_hrwk[ptind]], [np.log10(1.0+inter_dtfs[ptind])], linestyle='', marker='o',\n                     alpha=0.8, color=head_clrs[inter_head[ptind]])\n        plt.text(4, 1.1, 'InterCode ='+str(inters_select[iinter]))\n        # show the variable of the radius\n        plt.text(10.0, 3.46, 'DTFS_p80', color='black')\n    \n    plt.xlabel(\"Hours (week day 0-23, weekend 24-47)\")\n    plt.xlim(-1,48)\n    plt.ylabel(\"log10(  DTFS  )\")\n    plt.ylim(0.9, 3.6)\n    plt.title(cities[iCity]+\" -- Curve: DTFS Lookup Table values \" +\n              \"(iCity={}, iMonth={}, iWait={} )\".format(iCity,iMonth,iWait) +\n             \"   Color-code is {}\".format(heading_col))\n    \n    plt.savefig(out_dir+\"\/\"+cities[iCity]+\"_DTFS_w_data_\"+version_str+\".png\")\n    plt.show()\n    \n\nif SHOW_EDA:\n    # Plot the DTFS vs the Entry Heading\n    # Can do it in polar coordinates ;-)\n    use_polar = True\n    \n    if use_polar:\n        plt.figure(figsize=(6,6))\n        # Some grid lines, curves\n        plt.plot([-4,4],[0,0,],color='gray')\n        plt.plot([0,0],[-4,4],color='gray')\n        plt.plot([-6,6],[-6,6,],color='gray')\n        plt.plot([-6,6],[6,-6],color='gray')\n        for radius in [1.0,2.0,3.0]:\n            circx=[]\n            circy=[]\n            for thetaN in np.arange(0,2.0*np.pi+0.05,0.1):\n                circx.append(radius*np.cos(thetaN))\n                circy.append(radius*np.sin(thetaN))\n            plt.plot(circx,circy,linestyle='-',color='gray')\n        plt.text(0.9, 0.5, '10', color='black')\n        plt.text(1.8, 1.0, '100', color='black')\n        plt.text(2.7, 1.5, '1000', color='black')\n        # show the variable of the radius\n        plt.text(-2.5, 3.2, 'DTFS_p80', color='black')\n        #\n        for iplt in range(len(inter_head)):\n            # Polar plot: (thetaN in radians: 0 pointing N and positive N to E)\n            # blur theta and radius\n            thetaN = (inter_head[iplt]+0.1*np.random.randn())*np.pi\/4.0\n            radius = np.log10(1.0+inter_dtfs[iplt]+np.abs(np.random.randn()))\n            plt.plot([radius*np.sin(thetaN)], [radius*np.cos(thetaN)],\n                     linestyle='', marker='o',alpha=0.5,color=head_clrs[inter_head[iplt]])\n        plt.xlabel(\"West  - {} -  East\".format(heading_col))\n        plt.xlim(-3.6, 3.6)\n        plt.ylabel(\"South  - {} -  North\".format(heading_col))\n        plt.ylim(-3.6, 3.6)\n    else:\n        plt.figure(figsize=(8,4))\n        for iplt in range(len(inter_head)):\n            # Linear plot:\n            plt.plot([inter_head[iplt]+0.1*np.random.randn()], [np.log10(1.0+inter_dtfs[iplt])],\n                     linestyle='', marker='o',alpha=0.5,color=head_clrs[inter_head[iplt]])\n        plt.xlabel(\"Exit Heading code (0 - 7)\")\n        plt.xlim(-0.5,7.5)\n        plt.ylabel(\"log10(  DTFS  )\")\n        plt.ylim(0.9, 3.6)\n    \n\n    plt.title(cities[iCity]+\" \" +\n              \"(iMonth={}, iWait={} )  InterCode = {}\".format(\n                  iMonth,iWait,str(inters_select[iinter])))\n    \n    plt.savefig(out_dir+\"\/\"+cities[iCity]+\"_DTFS_polar_\"+version_str+\".png\")\n    plt.show()","04f79c38":"# Free up memory I don't need\ndel select, df_lookup, df_temp2\n# collect any garbage?\ngc_dummy = gc.collect()","b1c257f5":"# When the Test is the train we 'know' the answer,\n# so this will give the best we can do on training.\n\nif True and TEST_IS_TRAIN:\n    # Set the \"known\" TEST NoWait and LowWait values based on the train values\n    df_test['NoWait'] = df_train['NoWait']\n    df_test['LoWait'] = df_train['LoWait']\n    df_test['HiWait'] = df_train['HiWait']\n\n    print(\"Test is Train, setting the Test Wait values from the known Train values:\\n\")\n    print(\"Fraction of TEST rows assigned NoWait = {:.2f} %\".format(100.0*df_test['NoWait'].mean()))\n    print(\"Fraction of TEST rows assigned LoWait = {:.2f} %\".format(100.0*df_test['LoWait'].mean()))\n    print(\"Fraction of TEST rows assigned HiWait = {:.2f} %\".format(100.0*df_test['HiWait'].mean()))","192a4915":"# Very simple ML to assign LoWait, HiWait for Test data\n\n# Some sklearn routines to use\nfrom sklearn.metrics import accuracy_score\n# ML model(s) to use to 'learn' NoWait, LowWait from Xs\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# ML_WAITS is set at the top in Preliminaries\nif ML_WAITS:\n    # Get the Features and Target(s)\n    \n    # The features, including LatOff and LongOff\n    ml_x_cols = ['iCity','LatOff','LongOff','InHeading','EntryType',\n               'DistToCenter','Center*InHead','UniqueExits','LocalDensity','EntryLength']\n    # The features, without LatOff and LongOff\n    ##ml_x_cols = ['iCity','InHeading','EntryType',\n    ##           'DistToCenter','Center*InHead','UniqueExits','LocalDensity','EntryLength']\n\n    \n    X = df_train[ml_x_cols]\n    Xtest = df_test[ml_x_cols]\n\n    # The targets to learn\n    ylo = df_train['LoWait']\n    yhi = df_train['HiWait']\n","5fa4c700":"if ML_WAITS:\n    # Look at the correlation between the X and y values\n    X_temp = X.copy()\n    X_temp['ylo'] = ylo\n    X_temp['yhi'] = yhi\n\n    corr_df = X_temp.corr()\n    # In particular the correlations with ys\n    print(\"\\nTrain correlations with ylo:\")\n    print(corr_df.ylo)\n    print(\"\\nTrain correlations with yhi:\")\n    print(corr_df.yhi)\n    \n    del X_temp\n    gc_dummy = gc.collect()","ae1d55dc":"if ML_WAITS:\n    t_ml = time()\n    # Setup and Fit to Training data\n    \n    # RandomForestClassifier(\n    # n_estimators=\u2019warn\u2019, criterion=\u2019gini\u2019, max_depth=None, min_samples_split=2, min_samples_leaf=1,\n    # min_weight_fraction_leaf=0.0, max_features=\u2019auto\u2019, max_leaf_nodes=None, min_impurity_decrease=0.0,\n    # min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0,\n    # warm_start=False, class_weight=None)\n    model_name = 'rfc'   # in case anyone asks\n    # Parameters for each model:\n    \n    # The LoWait model is 'balanced' or 'none' depending on WAIT_CHOICE\n    if WAIT_CHOICE == 0:\n        # near 70% so use balanced with nominal threshold\n        lo_weight = 'balanced'\n        yloh_threshold = 0.50\n    else:\n        # near 50% so leave it None\n        # and use adjusted threshold to reduce false positives\n        lo_weight = None   # probably very similar to balanced\n        yloh_threshold = 0.60\n    #\n    lo_params = {'n_estimators': 30,\n              'max_depth': 14,\n              'min_samples_leaf': 200,\n              'min_impurity_decrease': 0.0,\n              'n_jobs': 2,\n              'class_weight': lo_weight\n             }\n    \n    # The HiWait model is 'balanced' since HiWait = 1 for only 2-3%,\n    # in addition the parameter yhih_threshold is used to reduce false positives\n    # by setting a higher threshold probability to give a 1.\n    hi_weight = 'balanced'\n    yhih_threshold = 0.79\n    hi_params = {'n_estimators': 30,\n              'max_depth': 14,\n              'min_samples_leaf': 200,\n              'min_impurity_decrease': 0.0,\n              'n_jobs': 2,\n              'class_weight': hi_weight\n             }\n    lo_model_base = RandomForestClassifier(**lo_params)\n    hi_model_base = RandomForestClassifier(**hi_params)\n\n    \n    # Do the 'learning'\n    lo_fit_model = lo_model_base.fit(X,ylo)\n    hi_fit_model = hi_model_base.fit(X,yhi)\n    \n    # Show the parameters\n    ##print(lo_fit_model.get_params())\n    ##print(\"\")\n    ##print(hi_fit_model.get_params())\n    \n    print(\"\\nDoing the ML took {:.3f} seconds.\".format(time() -  t_ml))\n    # Doing the ML took 37.415 seconds.  Test is train, 30 estimators\n    # Doing the ML took 114.396 seconds.               120 estimators\n    # Doing the ML took 120.996 seconds.         120 estimators, 100 min...leaf","c223f9f0":"if ML_WAITS:\n    feature_importance = lo_fit_model.feature_importances_\n        \n    # make importances relative to max importance\n    max_import = feature_importance.max()\n    feature_importance = 100.0 * (feature_importance \/ max_import)\n    sorted_idx = np.argsort(feature_importance)\n    pos = np.arange(sorted_idx.shape[0]) + 0.5\n\n    plt.figure(figsize=(8, 5))\n    plt.barh(pos, feature_importance[sorted_idx], align='center')\n    plt.yticks(pos, X.columns[sorted_idx])\n    plt.xlabel(model_name.upper()+' -- Relative Importance')\n    plt.title('           '+model_name.upper()+\n              ' - LoWait -- Variable Importance                  max --> {:.3f} '.format(max_import))\n\n    plt.savefig(out_dir+\"\/\"+model_name.upper()+\"-Lo_importance_\"+version_str+\".png\")\n    plt.show()","d8b146c3":"if ML_WAITS:\n    feature_importance = hi_fit_model.feature_importances_\n        \n    # make importances relative to max importance\n    max_import = feature_importance.max()\n    feature_importance = 100.0 * (feature_importance \/ max_import)\n    sorted_idx = np.argsort(feature_importance)\n    pos = np.arange(sorted_idx.shape[0]) + 0.5\n\n    plt.figure(figsize=(8, 5))\n    plt.barh(pos, feature_importance[sorted_idx], align='center')\n    plt.yticks(pos, X.columns[sorted_idx])\n    plt.xlabel(model_name.upper()+' -- Relative Importance')\n    plt.title('           '+model_name.upper()+\n              ' - HiWait -- Variable Importance                  max --> {:.3f} '.format(max_import))\n\n    plt.savefig(out_dir+\"\/\"+model_name.upper()+\"-Hi_importance_\"+version_str+\".png\")\n    plt.show()","9169843f":"# Look at the predictions on Train and assign to Test\n\nif ML_WAITS:\n    # Training predictions (discrete, uses model prob of 0.5 as threshold)\n    yloh = lo_fit_model.predict(X)\n    yhih = hi_fit_model.predict(X)\n\n    # Also get the continuous probabilities from the models:\n    yloh_prob = lo_fit_model.predict_proba(X)\n    yloh_prob = yloh_prob[:,1]\n    yhih_prob = hi_fit_model.predict_proba(X)\n    yhih_prob = yhih_prob[:,1]\n    \n    # Redefine the 0,1 results using a threshold for 1, default is 0.5\n    yloh = (yloh_prob > yloh_threshold).astype(int)\n    # Use a higher threshold for the HiWait to reduce false positives:\n    # This is set above when the model is defined: yhih_threshold = 0.79\n    yhih = (yhih_prob > yhih_threshold).astype(int)\n    \n    \n    print(\"\")\n    lo_train_score = accuracy_score(ylo, yloh)\n    print(\"Train accuracy: {:.2f} %,\".format(100.0*lo_train_score) +\n            \"  LoWait fraction = {:.2f}%\".format(100*yloh.mean()) +\n         \", should be {:.2f}%\".format(100*ylo.mean()))\n    hi_train_score = accuracy_score(yhi, yhih)\n    print(\"Train accuracy: {:.2f} %,\".format(100.0*hi_train_score) +\n            \"  HiWait fraction = {:.2f}%\".format(100*yhih.mean()) +\n         \", should be {:.2f}%\".format(100*yhi.mean()))\n\n    \n    # Make the Test predictions\n    yloh_test = lo_fit_model.predict(Xtest)\n    yhih_test = hi_fit_model.predict(Xtest)\n    \n    # Use thresholds for the yloh and yhih\n    yloh_test_prob = lo_fit_model.predict_proba(Xtest)\n    yloh_test_prob = yloh_test_prob[:,1]\n    yloh_test = (yloh_test_prob > yloh_threshold).astype(int)\n    yhih_test_prob = hi_fit_model.predict_proba(Xtest)\n    yhih_test_prob = yhih_test_prob[:,1]\n    yhih_test = (yhih_test_prob > yhih_threshold).astype(int)\n    \n    # Put those values in the test LoWait and HiWait:\n    df_test['LoWait'] = yloh_test\n    df_test['HiWait'] = yhih_test\n    \n    if TEST_IS_TRAIN:\n        reminder_str = '[Note: TEST is train!]'\n    else:\n        reminder_str = ''\n        \n    print(\"\\nTEST LoWait fraction = {:.2f}%  {}\".format(100*yloh_test.mean(),reminder_str))\n    print(\"TEST HiWait fraction = {:.2f}%  {}\".format(100*yhih_test.mean(),reminder_str))\n","c6b1847c":"if ML_WAITS:\n    # Don't need the models anymore\n    del lo_fit_model, hi_fit_model, lo_model_base, hi_model_base\n    del yloh_test, yhih_test\n    gc_dummy = gc.collect()","303d72d2":"# Use this routine to shown how the prediction is doing.\n# This routine is taken from the file chirp_roc_lib.py in the github repo at: \n#   https:\/\/github.com\/dan3dewey\/chirp-to-ROC\n# Some small modifications have been made here.\n\ndef y_yhat_plots(y, yh, title=\"y and y_score\", y_thresh=0.5, ROC=True, plots_prefix=None):\n    \"\"\"Output plots showing how y and y_hat are related:\n    the \"confusion dots\" plot is analogous to the confusion table,\n    and the standard ROC plot with its AOC value.\n    The yp=1 threshold can be changed with the y_thresh parameter.\n    y and yh are numpy arrays (not series or dataframe.)\n    \"\"\"\n    # The predicted y value with threshold = y_thresh\n    y_pred = 1.0 * (yh > y_thresh)\n\n    # Show table of actual and predicted counts\n    crosstab = pd.crosstab(y, y_pred, rownames=[\n                           'Actual'], colnames=['  Predicted'])\n    print(\"\\nConfusion matrix (y_thresh={:.3f}):\\n\\n\".format(y_thresh),\n        crosstab)\n\n    # Calculate the various metrics and rates\n    tn = crosstab[0][0]\n    fp = crosstab[1][0]\n    fn = crosstab[0][1]\n    tp = crosstab[1][1]\n\n    ##print(\" tn =\",tn)\n    ##print(\" fp =\",fp)\n    ##print(\" fn =\",fn)\n    ##print(\" tp =\",tp)\n\n    this_fpr = fp \/ (fp + tn)\n    this_fnr = fn \/ (fn + tp)\n\n    this_recall = tp \/ (tp + fn)\n    this_precision = tp \/ (tp + fp)\n    this_accur = (tp + tn) \/ (tp + fn + fp + tn)\n\n    this_posfrac = (tp + fn) \/ (tp + fn + fp + tn)\n\n    print(\"\\nResults:\\n\")\n    print(\" False Pos = \", 100.0 * this_fpr, \"%\")\n    print(\" False Neg = \", 100.0 * this_fnr, \"%\")\n    print(\"    Recall = \", 100.0 * this_recall, \"%\")\n    print(\" Precision = \", 100.0 * this_precision, \"%\")\n    print(\"\\n    Accuracy = \", 100.0 * this_accur, \"%\")\n    print(\" Pos. fract. = \", 100.0 * this_posfrac, \"%\")\n\n    \n    # Put them in a dataframe for plots and ROC\n    # Reduce the number if very large:\n    if len(y) > 100000:\n        reduce_by = int(0.5+len(y)\/60000)\n        print(\"\\nUsing 1\/{} of the points for dots and ROC plots.\".format(reduce_by))\n        ysframe = pd.DataFrame([y[0: :reduce_by], yh[0: :reduce_by], \n                                y_pred[0: :reduce_by]], index=[\n                           'y', 'y-hat', 'y-pred']).transpose()\n\n    # If the yh is discrete (0 and 1s only) then blur it a bit\n    # for a better visual dots plot\n    if min(abs(yh - 0.5)) > 0.49:\n        ysframe[\"y-hat\"] = (0.51 * ysframe[\"y-hat\"]\n                            + 0.49 * np.random.rand(len(ysframe)))\n\n    # Make a \"confusion dots\" plot\n    # Add a blurred y column\n    ysframe['y (blurred)'] = ysframe['y'] + 0.1 * np.random.randn(len(ysframe))\n\n    # Plot the real y (blurred) vs the predicted probability\n    # Note the flipped ylim values.\n    ysframe.plot.scatter('y-hat', 'y (blurred)', figsize=(12, 5),\n                         s=2, xlim=(0.0, 1.0), ylim=(1.8, -0.8), alpha=0.3)\n    # show the \"correct\" locations on the plot\n    plt.plot([0.0, y_thresh], [0.0, 0.0], '-',\n        color='green', linewidth=5)\n    plt.plot([y_thresh, y_thresh], [0.0, 1.0], '-',\n        color='gray', linewidth=2)\n    plt.plot([y_thresh, 1.0], [1.0, 1.0], '-',\n        color='green', linewidth=5)\n    plt.title(\"Confusion-dots Plot: \" + title, fontsize=16)\n    # some labels\n    ythr2 = y_thresh\/2.0\n    plt.text(ythr2 - 0.03, 1.52, \"FN\", fontsize=16, color='red')\n    plt.text(ythr2 + 0.5 - 0.03, 1.52, \"TP\", fontsize=16, color='green')\n    plt.text(ythr2 - 0.03, -0.50, \"TN\", fontsize=16, color='green')\n    plt.text(ythr2 + 0.5 - 0.03, -0.50, \"FP\", fontsize=16, color='red')\n\n    if plots_prefix != None:\n        plt.savefig(plots_prefix+\"_dots.png\")\n    plt.show()\n\n    # Go on to calculate and plot the ROC?\n    if ROC == False:\n        return 0\n    \n    \n    # Make the ROC curve\n    # \n    # Set the y-hat as the index and sort on it\n    ysframe = ysframe.set_index('y-hat').sort_index()\n    # Put y-hat back as a column (but the sorting remains)\n    ysframe = ysframe.reset_index()\n\n    # Initialize the counts for threshold = 0\n    p_thresh = 0\n    FN = 0\n    TN = 0\n    TP = sum(ysframe['y'])\n    FP = len(ysframe) - TP\n\n    # Assemble the fpr and recall values\n    recall = []\n    fpr = []\n    # Go through each sample in y-hat order,\n    # advancing the threshold and adjusting the counts\n    for iprob in range(len(ysframe['y-hat'])):\n        p_thresh = ysframe.iloc[iprob]['y-hat']\n        if ysframe.iloc[iprob]['y'] == 0:\n            FP -= 1\n            TN += 1\n        else:\n            TP -= 1\n            FN += 1\n        # Recall and FPR:\n        recall.append(TP \/ (TP + FN))\n        fpr.append(FP \/ (FP + TN))\n\n    # Put recall and fpr in the dataframe\n    ysframe['Recall'] = recall\n    ysframe['FPR'] = fpr\n\n    # - - - ROC - - - could be separate routine\n    zoom_in = False\n\n    # Calculate the area under the ROC\n    roc_area = 0.0\n    for ifpr in range(1, len(fpr)):\n        # add on the bit of area (note sign change, going from high fpr to low)\n        roc_area += 0.5 * (recall[ifpr] + recall[ifpr - 1]\n                           ) * (fpr[ifpr - 1] - fpr[ifpr])\n\n    plt.figure(figsize=(6, 6))\n    plt.title(\"ROC: \" + title, size=16)\n    plt.plot(fpr, recall, '-b')\n    # Set the scales\n    if zoom_in:\n        plt.xlim(0.0, 0.10)\n        plt.ylim(0.0, 0.50)\n    else:\n        # full range:\n        plt.xlim(0.0, 1.0)\n        plt.ylim(0.0, 1.0)\n\n    # The reference line\n    plt.plot([0., 1.], [0., 1.], '--', color='orange')\n\n    # The point at the y_hat = y_tresh threshold\n    if True:\n        plt.plot([this_fpr], [this_recall], 'o', c='blue', markersize=15)\n        plt.xlabel('False Postive Rate', size=16)\n        plt.ylabel('Recall', size=16)\n        plt.annotate('y_hat = {:.2f}'.format(y_thresh),\n                            xy=(this_fpr+0.01 + 0.015,\n                            this_recall), size=14, color='blue')\n        plt.annotate(' Pos.Fraction = ' +\n                        '  {:.0f}%'.format(100 * this_posfrac),\n                        xy=(this_fpr + 0.03, this_recall - 0.045),\n                        size=14, color='blue')\n\n    # Show the ROC area (shows on zoomed-out plot)\n    plt.annotate('ROC Area = ' + str(roc_area)\n                 [:5], xy=(0.4, 0.1), size=16, color='blue')\n\n    # Show the plot\n    if plots_prefix != None:\n        plt.savefig(plots_prefix+\"_ROC.png\")\n    plt.show()\n\n    return roc_area","4158c112":"# Plots of y - y-hat from these two classifiers\n# Use this routine:\n# y_yhat_plots(y, yh, title=\"y and y_score\", y_thresh=0.5):\n#    \"\"\"Output plots showing how y and y_hat are related:\n#    the \"confusion dots\" plot is analogous to the confusion table,\n#    and the standard ROC plot with its AOC value.\n#    The yp=1 threshold can be changed with the y_thresh parameter.\n#    \"\"\"\n\nif True and ML_WAITS:\n    \n    # The ROC curves cn be shown or not\n    show_roc = True\n    \n    # LoWait\n    lo_roc_area = y_yhat_plots(ylo.values, yloh_prob, ROC=True,\n                           title=\"     y and y-hat-prob    for    Low-Wait\",\n                               y_thresh=yloh_threshold,\n                              plots_prefix=out_dir+\"\/\"+model_name.upper()+\"-Lo\")\n\n    # HiWait\n    hi_roc_area = y_yhat_plots(yhi.values, yhih_prob, ROC=show_roc,\n                           title=\"     y and y-hat-prob    for    High-Wait\",\n                               y_thresh=yhih_threshold,\n                               plots_prefix=out_dir+\"\/\"+model_name.upper()+\"-Hi\")\n","7901435c":"# Set the iWait values for Test from its LoWait and HiWait classifications\ndf_test['iWait'] = 1\ndf_test.loc[df_test['LoWait'] == 1, 'iWait'] = 0\ndf_test.loc[df_test['HiWait'] == 1, 'iWait'] = 2\n#\n# If ML did the Lo\/Hi determination then there is the chance\n# that an item was assigned both Lo and Hi,\n# in this case set iWait to 1:\nif True:\n    select = (df_test['LoWait'] == 1) & (df_test['HiWait'] ==1)\n    n_found = sum(select)\n    if n_found > 0:\n        print(\"\\nOverlap between ML Lo and Hi: {} set to iWait=1 (medium)\\n\".format(n_found))\n        df_test.loc[select, 'iWait'] = 1\n    else:\n        print(\"\\nNo overlap between Lo and Hi.\\n\")","dd5464c4":"# If the Test is the real test data, then there is a LatOff for each City\n# above-which we have little clue about the wait at the entry-sections...\n# Can set the iWait status of those here\nif TEST_IS_TRAIN == False:\n    # Those offset thresholds are:\n    lat_thresh = [5500, 5100, 5800, 5900]\n    \n    # Set them to an iWait value?  (-1=No, use the ML, 0,1,2=Yes)\n    #\n    ##northern_iwait = 1     # <-- * * * * * * * Set in beginning section\n    \n    \n    # Look at \/ modify those\n    print(\"\\nTest Lo,Hi waits as ML-assigned above the latitude threshold:\")\n    for iCity in range(4):\n        select_above = ((df_test['iCity'] == iCity) & \n                        (df_test['LatOff'] > lat_thresh[iCity]))\n        fraction_above = sum(select_above)\/len(df_test)\n        mean_lo = df_test.loc[select_above, 'LoWait'].mean()\n        mean_hi = df_test.loc[select_above, 'HiWait'].mean()\n        print(\"   {:>14}: {:.2f}% above threshold, with {:>6.2f}% LoWait and {:.2f}% HiWait.\".format(\n                    cities[iCity], 100*fraction_above, 100*mean_lo, 100*mean_hi))\n        # Set the iWait to a fixed value:\n        if northern_iwait != -1:\n            df_test.loc[select_above,'iWait'] = northern_iwait\n    if northern_iwait != -1:\n        print(\"\\nThese were all set to have iWait = {}\".format(northern_iwait))\n    else:\n        print(\"\\nThese were assigned the usual iWait from Lo,Hi.\")\nelse:\n    print(\"\\nTEST is Train, so there are no 'northern unknows' to adjust.\")","59293b15":"# Apply the lookup to the test df\n# Seems it should be possible to do this much faster...\n\n# Doing lookup by going through df and getting the 6 lookup values;\n# then put them in the df.\nif True:\n    # Now use that lookup array to fill the TEST output values\n    t_testfill = time()\n    # Put values in a list:\n    TT20 = []\n    DT20 = []\n    TT50 = []\n    DT50 = []\n    TT80 = []\n    DT80 = []\n    # Go through the df rows\n    for this_loc in df_test.index:\n        ##print(df_test.loc[this_loc,'ExitStreetName'])\n        # Get the values for this row's\n        # iCity,iMonth,iHrWk,iLowWait,iTurn\n        #\n        this_row = df_test.loc[this_loc].astype(int)\n        iCity = this_row['iCity']\n        iMonth = this_row['Month'] - 1\n        iHrWk = this_row['HrWk']\n        iWait = this_row['iWait']\n        # and for the turn corrections lookup\n        iTurn = this_row['Turn'] + 4\n        tts_corr = tts_turn_lookup[iCity,iWait,iTurn]\n        dtfs_corr = dtfs_turn_lookup[iCity,iWait,iTurn]\n\n        # Get the values...\n        TT20.append(int(lookup_CMHWTV[iCity,iMonth,iHrWk,iWait,0]))\n        TT50.append(int(lookup_CMHWTV[iCity,iMonth,iHrWk,iWait,1]))\n        TT80.append(int(tts_corr*lookup_CMHWTV[iCity,iMonth,iHrWk,iWait,2]))\n        DT20.append(int(lookup_CMHWTV[iCity,iMonth,iHrWk,iWait,3]))\n        DT50.append(int(lookup_CMHWTV[iCity,iMonth,iHrWk,iWait,4]))\n        DT80.append(int(dtfs_corr*lookup_CMHWTV[iCity,iMonth,iHrWk,iWait,5]))\n\n    # Put the values in the df:\n    df_test[out_cols[0]] = TT20\n    df_test[out_cols[1]] = TT50\n    df_test[out_cols[2]] = TT80\n    df_test[out_cols[3]] = DT20\n    df_test[out_cols[4]] = DT50\n    df_test[out_cols[5]] = DT80\n    \n    print(\"Filling (w\/loop) the TEST df took {:.3f} seconds.\".format(time() -  t_testfill))\n    # Filling (w\/loop) the TEST df took 517.249 seconds. <-- Test is Train, my machine\n    # Filling (w\/loop) the TEST df took 1181.075 seconds. <-- actual Test, on Kaggle\n    \n    del TT20, DT20, TT50, DT50, TT80, DT80\n    gc.collect()","fea4ea66":"# Look at some of the X and assigned target values\nprint(\"\\nSome rows of Xtest:\\n\", (Xtest.iloc[0:len(Xtest):int(len(Xtest)\/11)]))\nprint(\"\\nThe assigned target values for these:\\n\")\n# get the target values and show with shorter column names\nsome_targets = (df_test.iloc[0:len(df_test):int(len(df_test)\/11)])[out_cols].copy()\nsome_targets.columns = ['TTS_p20','TTS_p50','TTS_p80','DTFS_p20','DTFS_p50','DTFS_p80']\nprint( (some_targets) )","09e4f2cb":"if TEST_IS_TRAIN:\n    # Calculate the RMS error between the test targets (6, given in out_cols) and the known train values.\n    #\n    total_se = 0.0\n    for this_col in out_cols:\n        # add-in the sum of squares of test-train for each target value:\n        total_se += sum((df_test[this_col] - df_train[this_col])**2)\n\n    rmse_testtrain = np.sqrt(total_se\/(6*len(df_test)))\n    print(\"\\nThe overall RMSE for TEST-is-Train is {:.3f}\".format(rmse_testtrain) + \n          \"   Lo,Hi Thresholds = {}-{}-{}-{}, {}-{}-{}-{} \\n\".format(\n        LoWait_Thresh[0], LoWait_Thresh[1], LoWait_Thresh[2], LoWait_Thresh[3],\n        HiWait_Thresh[0], HiWait_Thresh[1], HiWait_Thresh[2], HiWait_Thresh[3]))\n\n    \nif TEST_IS_TRAIN:\n    total_se = 0.0\n    print(\"Sources of the error:      *** Only from DTFS_p80 ***\")\n    for iCity in range(4):\n        print(\" {} \".format(cities[iCity]))\n        for iWait in range(3):\n            partial_se = 0.0\n            select = ((df_train['iCity'] == iCity) & (df_train['iWait'] == iWait))\n            num_select = sum(select)\n            # Just the DTFS_p80 value:\n            for this_col in [out_cols[5]]:\n                # add-in the sum of squares of test-train for each target value:\n                partial_se += sum((df_test.loc[select,this_col] - df_train.loc[select,this_col])**2)\n            print(\"  iWait={}:  {:>12.1f} x10^6   from {:>8}(x1) with ave = {:>8.1f}\".format(\n                                        iWait, partial_se\/1.0e6, \n                                        int(num_select), (np.sqrt(partial_se\/(1.0*num_select)))))\n            total_se += partial_se\n\n    # Calculate the overall average RMSE (per 6 columns)\n    rmse_testtrain = np.sqrt(total_se\/(6*len(df_test)))\n    print(\"\\n  The RMSE due to just DTFS_p80 is {:.3f}\".format(rmse_testtrain))\n    \n\n#           Summary of some results here:\n\n# Using Intersections (this case is just here for reference, Entry-sections do better.)\n\n# - - - WAIT_CHOICE = 0   120, 280:     Lo: 71.39%, Hi: 4.63%\n#   Known      ML(RFC)  Lo: 'balanced', 0.50 threshold;  Hi: 'balanced', 0.79 threshold\n#   62.127     62.404   \n\n# - - - WAIT_CHOICE = 1    18, 210:     Lo: 42.83%, Hi: 6.44%\n#   Known      ML(RFC)  Lo: None, 0.60 threshold;  Hi: 'balanced', 0.79 threshold\n#   63.189     63.424  \n\n\n# Using Entry-sections\n\n# - - - WAIT_CHOICE = 0    120-120-120-120, 500-300-500-500    Lo: 73.86%, Hi: 2.60%\n#    49.839 <-- Known iWait\n#           ML(RFC)  Lo: 'balanced', 0.50 threshold;  Hi: 'balanced', 0.79 threshold\n#                With Lat\/LongOff\n#              51.657     with UniqueExits, LocalDensity, and EntryLength\n#        --->  51.359     (as below) \",  \",  Integers in all X features.\n#                No LatOff and LongOff:\n#              52.248     with UniqueExits, LocalDensity, and EntryLength\n#              52.247     \", Lo,Hi-n_estimators = 120\n#              51.707     \", Lo,Hi-n_estimators = 120, Hi-min...leaf = 100\n#        --->  51.944     \",  \",  Integers in all X features.\n\n# - - - WAIT_CHOICE = 1   18-18-18-18  400-210-400-400  Lo: 50.05%, Hi: 3.05%\n#    51.512 <-- Known iWait\n#           ML(RFC)  Lo: None, 0.60 threshold;  Hi: 'balanced', 0.79 threshold\n#                With Lat\/LongOff\n#              52.850   with UniqueExits, LocalDensity, and EntryLength\n#        --->  52.315   (as above) \",  \",  Integers in all X features.\n#                No LatOff and LongOff:\n#              52.952   with UniqueExits, LocalDensity, and EntryLength\n#        --->  52.686   (as above) \",  \",  Integers in all X features.\n","9ec03c8c":"# Summarize some of the major parameters:\nprint(\"Some of the major choices:\\n\\nENTRYSECTIONS = {}\".format(ENTRYSECTIONS))\nprint(\"WAIT_CHOICE = {}\".format(WAIT_CHOICE))\nif ML_WAITS:\n    print(\"The ML features used:\\n\",Xtest.columns)\nelif TEST_IS_TRAIN == True:\n    print(\"Known iWait values used, no ML.\")\n    ","efb9e6a2":"# Output the predicted values from the df_test columns to submission.csv\n\n# Only write-out the file if not TEST_IS_TRAIN and not REDUCED_SIZE:    \nif ((TEST_IS_TRAIN == False) and (REDUCED_SIZE == False)):\n    t_file_write = time()\n    # Make a two column df from the first output value\n    icol = 0\n    df_out = df_test[[out_cols[icol]]].copy().reset_index()\n    # rename the columns\n    df_out.columns = ['TargetId','Target']\n    # add the suffix\n    df_out['TargetId'] = df_out['TargetId'].astype(str) + \"_\" + str(icol)\n    # Use this first df as the final output one:\n    df_outall = df_out.copy()\n    #\n    # Now go through the rest of the values and append them\n    for icol in [1,2,3,4,5]:\n        df_out = df_test[[out_cols[icol]]].copy().reset_index()\n        df_out.columns = ['TargetId','Target']\n        df_out['TargetId'] = df_out['TargetId'].astype(str) + \"_\" + str(icol)\n        # append these to the outall:\n        df_outall = df_outall.append(df_out)\n    #\n    df_outall.to_csv(\"submission.csv\", index=False)\n    #\n    print(\"Writing the file took {:.3f} seconds.\".format(time() -  t_file_write))\nelse:\n    print(\"No submission file written.\")","2ba577f0":"# that's all, take a look at it\nprint(\"\\n   The beginning of submission.csv:\")\n!head -10 submission.csv","39f5e213":"print(\"\\n   The end of submission.csv:\")\n!tail -10 submission.csv","37093731":"# show\/confirm the random seed value\nprint(\"Used RANDOM_SEED = {}\".format(RANDOM_SEED))","2f3496a8":"### Check for NaNs","2bb8ba24":"### Get turn direction lookup values","28a1ba51":"## <a id=\"Predictions\">Create ML Predictions of Lo,Hi Wait<\/a>\nBack to <a href=\"#Index\">Index<\/a>\n","91d14b9a":"## <a id=\"Index\">Index<\/a>\nPreliminaries<br>\n<a href=\"#DataProcessing\">Reading and Processing csv Data Files<\/a><br>\n<a href=\"#IntersectionDF\">Data frame of (Entry-)Intersections<\/a><br>\n<a href=\"#FeatureSummary\">Summary of the Features<\/a><br>\n<a href=\"#LookupTable\">Create a Lookup(. . .) Table<\/a><br>\n<a href=\"#Predictions\">ML: Create Lo,Hi Predictions<\/a><br>\n<a href=\"#AssignLookup\">Target values from iWait and Lookup(. . .)<\/a><br>\n<a href=\"#OutputKaggle\">Write out Kaggle Predictions<\/a><br>\n<br>\n<a href=\"#TheEnd\">The End<\/a><br>\n<a href=\"#Diary\">Diary and Scores History<\/a><br>","94be3083":"### Apply the lookups to the Test df","9f585d44":"### Set iWait values: In the 'Northern Unknowns'","ebe756b0":"### Where are we running?\nIn the following code cell we can select some differences if running on Kaggle or my local machine. Also any other major parameters\/choices are set here.","a24eb344":"### Check Error if Test is Train","b414531c":"### Create the entry-sections dataframe\nThis is created from both the Train and the Test data,\nessentially together they are a proxy for a map of the intersections.","122158a5":"### Set iWait values: Usual Lo,Hi --> iWait","9ecbdfb2":"### Hide\/Unhide the Code Cells\nThe following (possibly hidden) cell enables toggling of the code cells visible\/hidden.<BR>\nRun the following (possibly invisible) cell if its \"CLICK HERE\" Output message does not appear.","626a0a43":"## <a id=\"IntersectionDF\">Data frame of (Entry-)Intersections<\/a>\nBack to <a href=\"#Index\">Index<\/a>","bb49dad3":"## <a id=\"LookupTable\">Create a Lookup(. . .) Table<\/a>\nBack to <a href=\"#Index\">Index<\/a>\n\nCreate a lookup table giving the 6 target values based on averages for given values of:<br>\nCity, Month, Hour+Weekend, and iWait.\n\nThe main ML task (further below) will be to classify intersections\/rows as LoWait and\/or as HiWait, e.g., using simple classification models with the features. These are then used to set iWait.","959ccc5e":"### Distribution of a Feature","16585c9d":"## <a id=\"DataProcessing\">Reading and Processing csv Data Files<\/a>\nBack to <a href=\"#Index\">Index<\/a><br>","a3e0cfaa":"## Preliminaries","8894b228":"## <a id=\"Diary\">Diary and Scores History<\/a>\nBack to <a href=\"#Index\">Index<\/a> <br>\n\n**(vN) LB-score** <-- These entries mark each commit and its LB score. <br>\nLooked at the data-column values and decided to add two new columns: iCity (0-3) and HrWk which combines Hour and 24\\*Weekend into 0 to 47. Created and filled a lookup array with the (6) mean target values based only on City, Month, and HrWk values (so I'm not using any info about specific intersections.) <br>\n**(v1) 79.430** Lookup of average from City,Month,HrWk. <br>\nLook through others' notebooks... Added count of entry, exit streets: seems like TEST should have intersections not in Training. Some people are doing one-hot encoding for the directions (https:\/\/www.kaggle.com\/pulkitmehtawork1985\/beating-benchmark)... What about using using signed angles? Decide to create Turn = -4 to +3 encoding as in: https:\/\/www.kaggle.com\/janlauge\/intersection-congestion-eda . <br>\n**(v2)  79.553** Lookup of average based on City,Month,HrWk,Turn (= 4x12(7)x48x8 = 10752 categories.) <br>\nReduce Turn to just encode Straight(0), Left(-1,-2,-3,-4), and Right(1,2,3) <br>\n**(v3)  79.350** Lookup of average based on City,Month,HrWk,TurnLSR (= 4x12(7)x48x3 = 4032 categories.) <br>\nCompare training and test feature values (averages); Lat and Long are most significantly different (because of differences in intersections\/regions covered.) \nIn \"Distribution of a feature\", besides plotting the number of samples for each-value-of-a-feature,\nalso plot the average of TotalTimeStopped_p80 vs the feature's-values. <br>\nBig Kludge: because there are very few samples in the months Jan and May (and none in Feb, Mar, April),\nchange the Jan.s to Dec and the May.s to June. (I said it was a kludge!) Re-run v3 with this small change:<br>\n**(v4)  79.352** Lookup of average based on City,Month,HrWk,TurnLSR (= 4x12(7)x48x3 = 4032 categories.) <br>\nCreate a Total_p80 feature, just the sum of TTS_p80 and DTFS_p80\/5. Create LatOff and LongOff which are Lat\/Long reduced to a 4 digit code; can see slight differences between the Train and Test distributiond for LatOff (has high z-score too.) Create unique intersection codes, InterCode (9 digit integer), from iCity, LatOff and LongOff; use these to determine NoWait and LowWait intersections and add these features to df. <br>\nGetting an idea of what the LB score range is: <br>\n**(v5)  89.323** Get LB score for setting all targets to 0. <br>\n**(v6)  80.422** Get LB score for setting all targets to their global Train Averages. <br>\nBefore fully launching into ML things (X,y,CV,etc), add an option to copy df_train into df_test right at the beginning: this helps check for coding issues between train and test paths and can see how the (simple) models do on the known training data (as compared to the Test data which has to be submitted to the LB for evaluation). <br>\nAs a simple start to include intersection-specific info, use a threshold on a target combination (TTS_p80 + DTFS_p80\/5) to label intersections as LowWait=0 or LowWait=1. Do the lookup scheme including the LowWait value too and adjust the threshold for best training result. Fit a simple Decision Tree Classifier (DTC) to the training LowWait and use that to predict the LowWait for the Test set.<br>\n**(v7) 79.927** (66.272) Lookup of City,Month,HrWk,LowWait,TurnLSR; LowWait = DTC(15,500,0.0)<br>\nNot all combinations of InterCode-Month-HrWk show up, about 1\/3 average coverage; the combination InterCode-Month-HrWk-EntryHead-ExitHead mostly shows exactly once (about 1.5% of entries are duplicates with different street names.) <br>\nSped up doing the lookup-table values: load vectors into df instead of one by one. <br>\nAs was pointed out, https:\/\/www.kaggle.com\/c\/bigquery-geotab-intersection-congestion\/discussion\/111003 ,<br>\nthe DTFS_p80 dominates the error value since it is usually the largest value. So set the 'LowWait' measure (which I called Total_p80) equal to DTFS_p80. <br>\n**(v8) 79.603** (65.102) Lookup of City,Month,HrWk,LowWait,TurnLSR; LowWait = DTC(14,500,0.0)<br>\nFaster output of the submission file, but not in order - does that matter? <br>\n**(v9) 80.154** (67.161) Lookup of City,Month,HrWk,LowWait,TurnLSR; LowWait = DTC(13,500,0.0)<br>\n**(v10) 80.100** (65.152) Lookup of City,Month,HrWk,LowWait,TurnLSR; LowWait = DTC(15,500,0.0)<br>\nOoops, v9, v10 used REDUCED_SIZE (1\/3) of training data; re-do v10:<br>\n**(v11) 79.702** (65.152) Lookup of City,Month,HrWk,LowWait,TurnLSR; LowWait = DTC(15,500,0.0)<br>\nUsed apply() to try to speed up the lookup, about the same, or slower. <br>\nUse two thresholds to divide the intersections into 3 groups: low, med, high wait; change iWait to 0,1,2.\nKind of a big mess - commit it for the record ;-) <br>\n**(v12) 80.006** (64.202) Lookup of C,M,HrWk,iWait,TurnLSR; iWait = DTC(15,500,0.0)<br>\nJust to be sure, redo v12 using the previous (v8 and earlier) output method\n(which writes in sample_submission order.) Yup it's the same.<br>\n *(v13) 80.006*  (64.202) Lookup of C,M,HrWk,iWait,TurnLSR; iWait = DTC(15,500,0.0)<br>\nUse Random Forest instead... <br>\n**(v14) 78.767** (62.513) Lookup of C,M,HrWk,iWait,TurnLSR; iWait = RFC(14,200,0.0,30)<br>\nWell, nice to see some Test improvement. Clean up the code based on identifying low and high wait intersections, encoded by the classification values: LoWait (e.g., < 120) and HiWait (i.e, > 280). These are converted to iWait=0,1,2 (low, med, high) for the lookup. Use ML to 'learn' LoWait and HiWait. Use 'balanced' class_weight for the RFC and also assign iWait=1 if both LoWait and HiWait are true. Clean up some of the past\/extra chatter...<br>\n**(v15) 78.631** (62.536) Lookup of C,M,HrWk,iWait,TurnLSR; iWait = RFC(14,200,0.0,30,Balanced) w\/1 if Lo&Hi<br>\nAdd the \"confusion dots\" plots for LoWait and HiWait ML. Add street type encoding, not much change? <br>\n**(v16) 79.441** (64.926) Lookup of C,M,HrWk,iWait,TurnLSR; 280, 120.0; RFC(14,200,0.0,30,Balanced) X=Types,iCity,Offs <br>\nAdd street-type encoding... hmm, doesn't help?  Use different parameters for lo and hi models: class_weight None for lo and 'balanced' for hi. <br>\n**(v17) 77.929** (62.837)  120.0, 280.0, RFC(14,200,0.0,30,None\/Bal) X=iCity,LatOff,LongOff,EntryType <br>\nAdd some more stuff: correlation of Xs with ys; scatter plots in Long-Lat; feature importance plot for ML; put EntryType values in average Total_p80 order. Make plots of the Lookup table values - shows the iWait=2\nvalues jump around a lot hour-to-hour and there is not much difference between months. <br>\nReduce lookup to 3 \"months\": 5-8, 9-10, 11-12.<br>\n**(v18) 77.575** (63.332)  120, 280, RFC(14,200,0.0,30,None\/Bal) X=iCity,LatOff,LongOff,EntryType <br>\nUse a yhih_threshold of 0.7 to reduce HiWait false positives: <br>\n**(v19) 77.193** (62.894)  120, 280, RFC(14,200,0.0,30,None\/Bal) X=iCity,LatOff,LongOff,EntryType <br>\nSmooth the iWait=2 lookup values (with 1,4,6,4,1 averaging.) <br>\n**(v20) 77.021** (63.017)  120, 280, RFC(14,200,0.0,30,None\/Bal) X=iCity,LatOff,LongOff,EntryType <br>\nLook into making an Intersection dataframe; use it to make city intersection plots. Also make histograms of the values (TTS, DTFS, Total) by city and by assigned iWait value. Looks like iWait=2 is really a large DTFS value,\nsay greater than 10^2.15 ~ 140. Set iWait=0 when TTS is less than 10^1.30 ~ 20.0. Adjust these thresholds and decide on 18 and 210:<br>\n**(v21) 77.451** (63.714)  18(TTS), 210(DTFS), RFC(14,200,0.0,30,None\/Bal) X=iCity,LatOff,LongOff,EntryType<br>\nWell, using TTS and DTFS didn't help with Test results...  Implement a WAIT_CHOICE value to choose the Lo,Hi criteria; one or other may be better once other intersection features are added.  Change the lat\/long_offset values to be in city centers and put those at 5000,5000 in LongOff,LatOff. Add a DistToCenter feature. Set the yhih_thresh to 0.79 - better than 0.7. See how Test does like this...<br>\n**(v22) 77.463** (63.598)  WCh=1:  18(TTS), 210(DTFS), RFC(14,200,0.0,30,None\/0.79) X=iCity,Lat,Long,EntryT,Dist<br>\n**(v23) 76.870** (63.848)  WCh=0:  120, 280, RFC(14,200,0.0,30,None\/0.79) X=iCity,Lat,Long,EntryT,Dist<br>\nAdded plot(s) to compare the lookup table DTFS values (for given iCity, iMonth, iWait) with the actual training values: for the high-Wait intersections, iWait=2, the real values are spread on either side of the lookup value, mostly by different exit headings having different average behavior. Can also show the DTFS values vs Exit Heading for an intersection to see same kind of thing; do it in polar coord.s. <br>\nLooking at these plots, especially for the iWait=2 intersections:\n(i) there is little change between \"months\", \n(ii) little difference between the TurnLSR values, and\n(iii) the heading-dependence is moslty in the iWait=2 ones.<br>\nAdded ability to toggle code cells visible\/hidden in the notebook: easier to see the structure, etc.<br>\nRemove the TurnLSR from the lookup table - not much change in the score. But include a TurnLSR lookup based\non City and Wait. <br>\n**(v24) 77.202** (64.062)  WCh=1:  18(TTS), 210(DTFS), RFC(14,200,0.0,30,None\/0.79) X=iCity,Lat,Long,EntryT,Dist<br>\nUse ExitType instead of EntryType:<br>\n**(v25) 77.232** (63.908)  WCh=1:  18(TTS), 210(DTFS), RFC(14,200,0.0,30,None\/0.79) X=iCity,Lat,Long,EXITtype,Dist<br>\nEntry Heading seems more uniquely correlated with different wait times than Exit Heading.<br>\nChange from Intersections (City-Lat-Long) to \"Entry-sections\" (City-Lat-Long-InHeading) and add InHeading to the ML features. <br>\n**(v26) 77.360** (59.920)  WCh=0: 120, 280, RFC(14,200,0.0,30,None\/0.79) X=City,Lat,Long,InHead,EntryT,Dist<br>\n**(v27) 75.900** (57.921)  WCh=1: 18(TTS), 210(DTFS), RFC(14,200,0.0,30,None\/0.79) X=City,Lat,Long,InHead,EntryT,Dist<br>\nAdjust the WCh=1 thresholds for better preformance:<br>\n**(v28) 75.126** (56.253)  WCh=1: 17(TTS), 380(DTFS), RFC(14,200,0.0,30,None\/0.79) X=City,Lat,Long,InHead,EntryT,Dist<br>\nSmooth the iWait=2 lookup values with 1,2,1 averaging (instead of previous 1,4,6,4,1) - hardly a difference,\nleave as 1,4,6,4,1.<br>\n**(v29) 75.136** (56.227)  WCh=1: 17(TTS), 380(DTFS), RFC(14,200,0.0,30,None\/0.79) X=City,Lat,Long,InHead,EntryT,Dist<br>\nUse separate HiWait threshold values for each city, not huge change but reducing Boston helps a little:<br>\n**(v30) 74.128** (55.831)  WCh=1: 17(TTS), 400-210-400-400(DTFS), RFC(14,200,0.0,30,None\/0.79)<br>\nInclude the dot product of the intersection-to-center and InHeading vectors, Center\\*InHead:<br>\n**(v31) 73.418** (54.471)  WCh=1: 17(TTS), 400-210-400-400(DTFS), RFC(14,200,0.0,30,None\/0.79) X incl. Center\\*InHead<br>\nAdded Airport location for each city - does not seem correlated with the (train) congestion... Look at separate LoWait thresholds for the cities: small effect, use 18 for all.<br>\nOptionally set the iWait value for Test values that are above a latitude threshold, i.e., in the 'northern unknown' region; try different iWait values there:<br>\n**(v32) 73.141** (54.305)  WCh=1: 18(TTS), 400-210-400-400(DTFS), RFC(14,200,0.0,30,None\/0.79) X incl. Center\\*InHead<br>\n**(v33) 73.723** (54.305)  As in (v32) but iWait = 0 [Lo] in the 'northern unknowns' <br>\n**(v34) 118.70** (54.305)  ! (as expected) As in (v32) but iWait = 2 [Hi] in the 'northern unknowns' <br>\n**(v35) 72.424** (54.305)  As in (v32) but iWait = 1 [medium] in the 'northern unknowns' <br>\nA bunch of cleaning up... A very small reduction using Turn (-4,-3,...,2,3) instead of TurnLSR (-1,0,1), so switch to Turn.<br>\n**(v36) 72.411** (54.250)  As in (v35) but using Turn instead of TurnLSR in correction lookup. <br>\nRemove LatOff and LongOff from the ML  values: Dist to center with Center\\*InHead and InHeading basically provide polar coordinates, so 'memorizing' the answers is still possible.<br>\n**(v37) 73.175** (55.148?)  As in (v36) but no LatOff, LongOff <br>\nTurn off setting the \"northern unknowns\" to 1 - see how the ML does there (it has OK percentages of low and even some high waits...)<br>\n**(v38) 74.532** (55.148?)  As in (v36) but no LatOff, LongOff - and use ML in \"northern unknows\".<br>\nAdded one-hot encoding for City - not as good as using iCity (with simple RFC). Re-structure the feature processing steps somewhat. Calculate UniqueExits and LocalDensity in the inter_merge df and put those values into Train and Test; do ML including them.<br>\n**(v39) 73.783** (55.139)  As in (v38) Add UniqueExits and LocalDensity, use ML in \"northern unknows\".<br>\n**(v41) 72.841** (55.139)  As in (v38) Add UniqueExits and LocalDensity, set \"northern unknowns\" to iWait=1.<br>\n**(v42) 72.469** (54.914)  As in (v36) Lat\/LongOff, UniqueExits, LocalDensity, \"northern unknowns\" iWait=1.<br>\nThese recent runs are no better on Test than v36 even with the two new features.  They also do better with i) Lat\/LongOff added, and ii) no ML in \"northern unknowns\". These suggest the ML is mostly memorizing Train answers and doesn't know enough to predict Test better than just setting iWait=1. <br>\nIn looking at how the WCh=0 case is doing (wait selections all done using Total_p80), the ML of LoWait had a lot of false positives - tried using 'balanced' weight for lo ML and that helped. With WCh=0 the not-using-Lat\/LongOff and using-ML-in-north combination actually gives the best result. Besides a better score, this is also better from an 'it is learning' point of view. <br>\n**(v43) 72.381** (53.353)  WCh=0, As (v42) with ML-lo balanced.<br>\n**(v44) 72.076** (53.979)  WCh=0, As (v42) w\/ ML-lo balanced, No Lat\/LongOff, ML in \"northern unknowns\"<br>\nCreate an EntryLength feature as well, changed radius for LocalDensity to 100 in the processes:<br>\n**(v45) 72.518** (53.876)  WCh=0, As (v44) w\/ radius=100<br>\n**(v46) 69.119** (52.534)  WCh=0, As (v44) w\/ radius=100 and EntryLength included<br>\nThat worked well! Can use separate radii for the LocalDensity calc and the EntryLength one, try:<br>\n**(v47) 69.590** (52.ish)  WCh=0, As (v44) w\/ LocalDensity(r=25) and EntryLength(r=100)<br>\nOK, keep the common radius of 100. Check adding Lat,LongOff...<br>\n**(v49) 68.664** (51.657)  WCh=0, As (v46) w\/ Lat\/LongOff, radius=100, EntryLength <br>\nFor comparison, same as v49 but using Intersections instead of the usual Entry-sections:<br>\n**(v50) 76.506** (62.404)  Demo using Intersections (ENTRYSECTIONS=False), otherwise same as (v49).<br>\nDo some cleaning up of the comments, etc. Set the Lo,Hi n_estimators to 120 (from 30), and Hi-min...leaf = 100 (from 200), use all integer feature values. Small changes, \"It's a wrap\"?!<br>\n**(v51) 68.896** (51.359)  WCh=0, integers, w\/ Lat\/LongOff, radius=100, EntryLength, etc. <br>\nAnd the same as v51 except that 4 of the features are left as floats:<br>\n**(v52) 68.758** (51.ish)  WCh=0, Some floats, w\/ Lat\/LongOff, radius=100, EntryLength, etc. <br>\nAnd leaving LatOff and LongOff out of the features (less momorizing, more learning?, or not?):<br>\n**(v53) 68.930** (51.ish)  As (v52) except No LatOff,LongOff.<br>\nOK, ok, you know you're done when you're tuning the hyperparameters\/features\/model to the Test data ;-) <br>\n**(v54) --.---** (51.657)  Back to v49-ish w\/ Lat\/LongOff, 30=n_estimators, etc.   **The End**\n<hr>\n\n\nSome possible Things to Do:\n\n- Will a different ML classifier do a better job generalizing to Test? (It's already just about as good as possible, aka overfitting, on the Training data).\n\n- Do cross-validation in the ML step to assess generalizing.\n\n- Any more features that I can add that will help classify Lo,Hi-wait entry-sections? Seems I would need to add some external data...\n\n- There are nice uses of pandas' capabilities in\nhttps:\/\/www.kaggle.com\/stefancurtress\/geotab-eda-features-and-rf\nI could use some of these to clean up my code ;-)\n\n\n<hr>\nSummary of Models\/scores:<br>\n\n    #   Train   TEST-LB(vN)        Model:\n    #   80.07   89.323 (v5)  0 for all 6 target values\n    #   71.09   80.422 (v6)  Set to the 6 global-average target values\n    #   69.90   79.352 (v4)  Average[City,Month,HrWk,TurnLSR], no intersection info (Lat,Long,etc)\n    #   64.047  --.--- (--)  Average[City,Month,HrWk,Wait,TurnLSR] Thresh=75.0 LowWait = Known(training)\n    #   66.272  79.927 (v7)  Average[City,Month,HrWk,Wait,TurnLSR] Thresh=75.0 LowWait = DTC(15,500,0.0)\n    #   63.361  --.--- (--)  Average[City,Month,HrWk,Wait,TurnLSR] Thresh=175.0 LowWait = Known(training)\n    #   65.102  79.603 (v8)  Average[City,Month,HrWk,Wait,TurnLSR] Thresh=175.0 LowWait = DTC(14,500,0.0)\n    #    oops   80.154 (v9)  Average[City,Month,HrWk,Wait,TurnLSR] Thresh=175.0 LowWait = DTC(13,500,0.0)\n    #    oops   80.100 (v10) Average[City,Month,HrWk,Wait,TurnLSR] Thresh=175.0 LowWait = DTC(15,500,0.0)\n    #   65.152  79.702 (v11) Average[City,Month,HrWk,Wait,TurnLSR] Thresh=175.0 LowWait = DTC(15,500,0.0)\n    #   61.354  --.--- (--)  Average[City,Month,HrWk,Wait,TurnLSR] Threshs= 280, 120.0; iWait = Known(training)\n    #   64.202  80.006 (v12) Average[City,Month,HrWk,Wait,TurnLSR] Threshs= 280, 120.0; iWait = DTC(15,500,0.0)\n    #   62.513  78.767 (v14) Average[C,M,HrWk,Wait,TurnLSR] Threshs= 280, 120.0; iWait = RFC(14,200,0.0,30)\n    #   62.536  78.631 (v15) Average[C,M,HrWk,Wait,TurnLSR] Threshs= 280, 120.0; iWait = RFC(14,200,0.0,30,Bal) w\/LoHi\n    #   64.926  79.441 (v16) Average[C,M,HrWk,Wait,TurnLSR] 280, 120.0; RFC(14,200,0.0,30,Balanced) X=Types,iCity,Offs\n    #   62.837  77.929 (v17) 120.0, 280.0, RFC(14,200,0.0,30,None\/Bal) X=iCity,LatOff,LongOff,EntryType\n    #   63.332  77.575 (v18) 120, 280, 3-mon, RFC(14,200,0.0,30,None\/Bal) X=iCity,LatOff,LongOff,EntryType\n    #   62.894  77.193 (v19) 120, 280, 3-mon, RFC(14,200,0.0,30,None\/Bal,0.7) X=iCity,LatOff,LongOff,EntryType\n    #   63.017  77.021 (v20) 120, 280, 3-mon, Smoo, RFC(14,200,0.0,30,None\/Bal,0.7) X=iC,LatO,LongO,EntryT\n    #   62.005  --.--- (--)  120, 280, 3-mon, Smoo, Lo,HiWait = Known Values\n    #   63.132  --.--- (--)  18(T), 210(D), 3-mon, Smoo, Lo,HiWait = Known Values\n    #   63.714  77.451 (v21) 18(T), 210(D), 3-mon, Smoo, RFC(14,200,0.0,30,None\/0.70) X=iC,LatO,LongO,EntryT\n    #   63.598  77.463 (v22) 18(T), 210(D), 3-mon, Smoo, RFC(14,200,0.0,30,None\/0.79) X=iC,Lat,Long,Dist,EntryT\n    #   62.848  76.870 (v23) 120, 280, 3-mon, Smoo, RFC(14,200,0.0,30,None\/0.79) X=iC,Lat,Long,Dist,EntryT\n    #   64.062  77.202 (v24) 18(T), 210(D), 3-mon, Smoo, RFC(14,200,0.0,30,None\/0.79) X=iC,Lat,Long,Dist,EntryT\n    #   63.908  77.232 (v25) 18(T), 210(D), 3-mon, Smoo, RFC(14,200,0.0,30,None\/0.79) X=iC,Lat,Long,Dist,EXITtype\n    #   52.055  --.--- (--)  Entry-sections, WCh=0: 120, 280, lookup[City,Month,HrWk,Wait], iWait=Known\n    #   53.338  --.--- (--)  Entry-sections, WCh=1: 18(T), 210(D), lookup[City,Month,HrWk,Wait], iWait=Known\n    #   59.920  77.360 (v26) WCh=0: 120, 280, 3-mon, Smoo, RFC(as before) X=City,Lat,Long,InHead,EntryT,Dist\n    #   57.921  75.900 (v27) WCh=1: 18(T), 210(D), 3-mon, Smoo, RFC(as before) X=City,Lat,Long,InHead,EntryT,Dist\n    #   51.674  --.--- (--)  Entry-sections, WCh=1: 17(T), 380(D), lookup[City,Month,HrWk,Wait], iWait=Known\n    #   56.253  75.126 (v28) WCh=1: 17(T), 380(D), 3-mon, Smoo, RFC(as before) X=City,Lat,Long,InHead,EntryT,Dist\n    #   56.227  75.136 (v29) WCh=1: 17(T), 380(D), 3-mon, Smoo121, RFC(as before) X=City,Lat,Long,InHead,EntryT,Dist\n    #   51.589  --.--- (--)  Entry-sections, WCh=1: 18(T), 400-210-400-400(D), lookup[City,Month,HrWk,Wait], iWait=Known\n    #   55.831  74.128 (v30) WCh=1: 17(T), 400-210-400-400(D), 3-mon, Smoo, RFC(as before)\n    #   54.472  73.418 (v31) WCh=1: 17(T), 400-210-400-400(D), 3-mon, Smoo, RFC(as before) X incl. Center*InHead\n    #   54.305  73.141 (v32) WCh=1: 18(T), 400-210-400-400(D), 3-mon, Smoo, RFC(as before) X incl. Center*InHead\n    #   54.305  73.723 (v33) As in (v32) but iWait = 0 [Lo] in the 'northern unknowns'.\n    #   54.305  118.70 (v34) ! (as expected) As in (v32) but iWait = 2 [Hi] in the 'northern unknowns'.\n    #   54.305  72.424 (v35) As in (v32) but iWait = 1 [medium] in the 'northern unknowns'.\n    #   54.250  72.411 (v36) As in (v35) but using Turn instead of TurnLSR in correction lookup.\n    #   55.148? 73.175 (v37) As in (v36) but no LatOff, LongOff\n    #   55.148? 74.532 (v38) As in (v36) but no LatOff, LongOff - and use ML in \"northern unknows\".\n    #   55.139  73.783 (v39) As in (v38) Add UniqueExits and LocalDensity, use ML in \"northern unknows\".\n    #   55.139  72.841 (v41) As in (v39) Add UniqueExits and LocalDensity, set \"northern unknowns\" to iWait=1.\n    #   54.914  72.469 (v42) As in (v36) Lat\/LongOff, UniqueExits, LocalDensity, \"northern unknowns\" iWait=1.\n    #   53.353  72.381 (v43) As (v42) w\/ WCh=0, ML-lo balanced\n    #   53.979  72.076 (v44) As (v42) w\/ WCh=0, ML-lo balanced, No Lat\/LongOff, ML in \"northern unknowns\"\n    #   53.876  72.518 (v45) As (v44) w\/ radius=100<br>\n    #   52.534  69.119 (v46) As (v44) w\/ radius=100 and EntryLength included<br>\n    #   52.ish  69.590 (v47) As (v44) w\/ LocalDensity(r=25) and EntryLength(r=100)<br>\n    #   51.657  68.664 (v49) As (v46) w\/ Lat\/LongOff, radius=100, EntryLength\n    #   62.404  76.506 (v50) using Intersections (ENTRYSECTIONS=False), otherwise same as (v49)\n    #   51.359  68.896 (v51) As (v49) WCh=0, integers, w\/ Lat\/LongOff, radius=100, EntryLength, etc.\n    #   51.ish  68.758 (v52) As (v51) WCh=0, Some floats, w\/ Lat\/LongOff, radius=100, EntryLength, etc.\n    #   51.ish  68.930 (v53) As (v52) except No LatOff,LongOff.\n    #   51.657  --.--- (v54) Back to v49-ish n_estimators, etc.","20e6ccc5":"### Put the new features into the Train\/Test dataframes","1b9b5a1f":"### Read the files and do basic feature defining and processing\nMuch of the feature processing\/adjusting is done in this one code cell to prevent it getting out of synch.","d84f53d1":"# Intersection Congestion - \"Entry-section\" Classification Lookup","2e3ec16a":"### Summary Information on (Entry-)Intersections","cf1bf57d":"### ML: Assign LoWait, HiWait values to Test","9c8674b9":"### Dataframes - drop not used columns","0001bcac":"### Compare Train and Test Feature averages","29356fd0":"### Show the ML Classifier quality\n\nUsing \"confusion-dots\" plot and ROC plot, taken from my Titanic demo kernel.","75a66e9e":"### Overview\nThe goal is to determine the 6 numeric target values for each row of the test data, given training data with known target values; this is typically a regression problem. However, in this notebook several things are used so that a simple classification approach can give adequate results:\n\n- The main units of consideration are the \"Entry-Intersection\"s, aka entry-sections, rather than whole Intersections.\n- Each entry-section in the training data can be assigned to one of three \"wait\" (congestion) categories based on their average TTS_p80 and DTFS_p80 values (since these are the most important to the RMSE.) The critieria, thresholds for each Wait category, are set to get reasonable RMSE accuracy with known Waits; about an RMSE ~ 52 in the training set.\n- Much of the variation of the target values can be put into a lookup table of the 6 targets as a function of City, Month, Hour-Weekend and the Wait category of the intersection. A second lookup table based on City, Wait, and Turn adds a turn correction.\n- With these inplace, the Machine Learning task is to assign the proper Wait category to unknown entry-sections (with the Wait category known the target values can be looked up.)\n- The Wait category (0,1,2) is generated from two simple classifiers (RFC): one that flags LoWait entry-sections (Wait will be 0), and one that flags HiWait ones (Wait will be 2).\n\nThe features used in the classification are i) some of the original columns (possibly modified somewhat): **City**, **Latitude**, **Longitude**, **EntryHeading**; ii) ones others have used: **EntryType** based on Street, Highway, etc in the Entry name, **DistToCenter** giving distance from a reference center; and some 'new' ones: \n- **Center*InHead**: The dot product of the unit vectors of the EntryHeading with the direction to the city center, i.e., is the entry pointing toward or away from the city center.\n- **UniqueExits**: The total number of unique Exit Headings for the given entry-section.\n- **LocalDensity**: The number of entry-sections within a region around this entry-section.\n- **EntryLength**: The distance of the closest other entry-section that is roughly opposite the Entry heading of this one, i.e., the likely distance from the previous intersection to this entry-section.\n\nThe last 3 of these features are made using an \"entry-section dataframe\" which combines the Train and Test entry-sections, as a proxy for a full intersections map. The entry-section dataframe is also used to make entry-section plots showing the spatial-distribution of the Wait categories and the location of the not-in-train Test entry-sections.\n\nThe new features are copied into Train and Test dataframes for ML use; I'm sure they could be generated (and used) in much more efficient and elegant ways ;-)\n","5ea3175b":"### Some views of the entry-section dataframe","0df69bfe":"## <a id=\"AssignLookup\">Target values from iWait and Lookup(. . .)<\/a>\nBack to <a href=\"#Index\">Index<\/a>","4a85c250":"## <a id=\"OutputKaggle\">Write Out the Kaggle Predictions<\/a>\nBack to <a href=\"#Index\">Index<\/a>","60e26c94":"## <a id=\"TheEnd\">The End<\/a>\nBack to <a href=\"#Index\">Index<\/a>","0e2726ed":"### Add features for the Type of Entry\/Exit 'street'","23c05e92":"### Number of Locations and Streets","ed767e9c":"## <a id=\"FeatureSummary\">Summary of the Features<\/a>\nBack to <a href=\"#Index\">Index<\/a>","7766c87b":"### Look at InterCode - Month - HrWk [Entry\/Exit Heading] combinations\n\n(v7) conclusions: Not all combinations of InterCode-Month-HrWk show up, about 1\/3 average coverage. <br>\nThe combination InterCode-Month-HrWk-EntryHead-ExitHead mostly shows up exactly once (about 1.5% of entries are duplicates with different street names.)","54d13d05":"### Setup Thresholds and Flags for No, Lo, Hi wait classes"}}