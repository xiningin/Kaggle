{"cell_type":{"f0188c29":"code","29fa44e8":"code","f2509961":"code","5a748aee":"code","904db554":"code","f4689f73":"code","42acd223":"code","9f6b2994":"code","7ae9cd34":"code","71889930":"code","0d3459f2":"code","8242f596":"code","ea5fd77d":"code","1eeda8f6":"code","5f2f7a9f":"code","e58a4e57":"code","d2f6ecef":"code","e079559d":"code","c0f4b5af":"code","d028a786":"code","2c1a9f86":"code","c3339bff":"code","c7f58360":"code","ee39f418":"code","4027032c":"code","09eef4a0":"code","058d88cb":"code","d95a2b64":"code","5f02bc83":"code","7cc3ed7b":"code","718a756f":"code","56d73e0e":"code","1d3bb0e1":"code","82cfae72":"code","b6acdc0a":"code","fed7031c":"code","4f69abb2":"code","4babfc0d":"code","f041f6f1":"code","fca5c3fe":"code","f64e16ed":"code","960a738a":"code","aa5c7ea0":"code","0f88ad63":"code","e31802b7":"code","6d25b668":"code","447264b6":"code","7434ab97":"code","6f6de84b":"code","8bea1e62":"code","12ddda44":"code","e380b1a5":"code","6dc5ecc2":"code","f52bdb22":"code","42440a19":"markdown","e8d272d4":"markdown"},"source":{"f0188c29":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","29fa44e8":"os.chdir(\"..\/input\")\nos.listdir()","f2509961":"from warnings import filterwarnings\nfilterwarnings('ignore')","5a748aee":"df=pd.read_csv(\"..\/input\/mice-protein\/miceprotein.csv\")","904db554":"df.head()","f4689f73":"# replace to '?' to np.nan\n# so real missing values can be count\nmymap = {\"?\":np.NaN}\n\ndf=df.applymap(lambda s: mymap.get(s) if s in mymap else s)","42acd223":"df.isnull().sum().sort_values(ascending=False) # missing datas can be observed","9f6b2994":"df.info()","7ae9cd34":"df=df.drop(['MouseID'],axis=1)","71889930":"df[\"Genotype\"] = df[\"Genotype\"].str.strip(\"'\")\ndf[\"Genotype\"].value_counts() #Observations that are all values should be dropped","0d3459f2":"df[\"Treatment\"] = df[\"Treatment\"].str.strip(\"'\")\ndf[\"Treatment\"].value_counts() # Observations that are all values should be dropped","8242f596":"df[\"Behavior\"] = df[\"Behavior\"].str.strip(\"'\")\ndf[\"Behavior\"].value_counts() # Observations that are all values should be dropped","ea5fd77d":"df[\"class\"] = df[\"class\"].str.strip(\"'\")\ndf[\"class\"].value_counts() # Observations that are all values should be dropped","1eeda8f6":"df[['Genotype']] = df[['Genotype']].replace(to_replace={'Control':1,'Ts65Dn':0})","5f2f7a9f":"df[['Treatment']] = df[['Treatment']].replace(to_replace={'Memantine':1,'Saline':0})","e58a4e57":"df[['Behavior']] = df[['Behavior']].replace(to_replace={'S\/C':1,'C\/S':0})","d2f6ecef":"y=df['class']","e079559d":"df = df.drop([\"class\"], axis=1)","c0f4b5af":"df.shape","d028a786":"print(df[\"Genotype\"].value_counts())\nprint(df[\"Treatment\"].value_counts())\nprint(df[\"Behavior\"].value_counts())","2c1a9f86":"df=df.apply(lambda x: x.astype(float))","c3339bff":"!pip install ycimpute==0.1.1","c7f58360":"from ycimpute.imputer import knnimput","ee39f418":"var_names = list(df) # variable names are stored dataset will be converted to matrix so\n# We kept it in a list so that it doesn't get lost ..","4027032c":"n_df = np.array(df) # Converted dataframe to Numpy array\n# Some libraries accept numpy array some DataFrame some all kinds","09eef4a0":"n_df.shape","058d88cb":"dff = knnimput.KNN(k=20).complete(n_df)\n# Filling in NaN expressions according to estimation of missing values in Array format","d95a2b64":"dff = pd.DataFrame(dff, columns = var_names) # Converting Array to Pandas DataFrame","5f02bc83":"dff.head()","7cc3ed7b":"dff.isnull().sum().sort_values(ascending=False) # There is no missing data left.","718a756f":"df=dff","56d73e0e":"df.head()","1d3bb0e1":"from keras.utils import np_utils\nfrom sklearn.preprocessing import LabelEncoder\n\n\n# label encoder y\nencoder = LabelEncoder()\nencoder.fit(y)\ny = encoder.transform(y)\ny = np_utils.to_categorical(y)","82cfae72":"print(y)\n\ny.shape","b6acdc0a":"X = df\nX = np.array(X)","fed7031c":"from sklearn.model_selection import GridSearchCV\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout\nfrom keras.layers import LeakyReLU\n\n\n# Train-Test \nfrom sklearn.model_selection import train_test_split\n# shuffle and split training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n                                                    random_state=0)","4f69abb2":"def create_model(optimizer=\"adam\",init='uniform'):\n    # create model\n    model = Sequential()\n    model.add(Dense(16, input_dim=80, activation=LeakyReLU()))\n    model.add(Dropout(0.2))\n    model.add(Dense(27, activation='relu'))\n    model.add(Dropout(0.3))\n    model.add(Dense(7, activation='sigmoid'))\n    model.add(Dropout(0.2))\n    model.add(Dense(27, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(8, activation='softmax'))# # 3 because it is output, the output should be layer 3\n    # Because it is multi-class, activation function 'softmax' should be selected.\n    # Because it is multi class, the loss function is \"categorical_crossentropy\"\n   \n    # Compile model\n\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer,metrics=[\"accuracy\"])\n    return model\nmodel = create_model() ","4babfc0d":"train=model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=0,validation_data=(X_test,y_test))","f041f6f1":"# plot loss during training\nimport matplotlib.pyplot as plt\nplt.plot(train.history['loss'], label='train')\nplt.plot(train.history['val_loss'], label='test')\nplt.title('Model Loss')\nplt.xlabel('epochs')\nplt.ylabel('loss values')\nplt.legend(loc='upper right')\nplt.show()","fca5c3fe":"# Untunned Scores of the Model\nimport sklearn.metrics as metrics\ny_pred=model.predict_classes(X_test)","f64e16ed":"# %%Accuracy\n\nprint(\"Accuracy:\",metrics.accuracy_score(np.argmax(y_test, axis=1),y_pred))\n\n# %%f1 score\n\nprint(\"f1_weighted:\",metrics.f1_score(np.argmax(y_test, axis=1), y_pred,average='weighted'))","960a738a":"# Grid Search Cross Validation\n# GridSearch Cross Validation Parameters\nparam_grid = {\n   \n    'epochs': [50,75,100], \n    'batch_size':[32,50,100],\n    'optimizer':['RMSprop', 'Adam','SGD'],\n    \n}\n\n# create model\n\n# Creating Model Object with KerasClassifier\nmodel_cv = KerasClassifier(build_fn=create_model, verbose=0)\n\n\ngrid = GridSearchCV(estimator=model_cv,  \n                    n_jobs=-1, \n                    verbose=1,\n                    cv=5,\n                    param_grid=param_grid)\n\ngrid_cv_model = grid.fit(X_train, y_train,) # Fitting the GridSearch Object on the Train Set\n\n\nmeans = grid_cv_model.cv_results_['mean_test_score'] # Mean of test scores\nstds = grid_cv_model.cv_results_['std_test_score'] # standard deviations of test scores\nparams = grid_cv_model.cv_results_['params'] # parameters used\n# to print all scores, standard deviations and parameters used\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n\n# Printing the Best Parameters as a Result of Grid Search Cross Validation on the Screen\nprint(\"Best: %f using %s\" % (grid_cv_model.best_score_, grid_cv_model.best_params_))","aa5c7ea0":"# %% Model Tuning- Building a Tuned Model with Best Parameters\n# Creating Tuned Model Object with KerasClassifier\ncv_model = grid_cv_model.best_estimator_","0f88ad63":"#%% K-FOLD\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n# K-fold accuracy scores\n\nkfold = KFold(n_splits=5, shuffle=True)\nresults = cross_val_score(cv_model, X_test, np.argmax(y_test, axis=1), cv=kfold,scoring= 'accuracy')","e31802b7":"print('K-fold Cross Validation Accuracy Results: ', results)\nprint('K-fold Cross Validation Accuracy Results Mean: ', results.mean())","6d25b668":"# K-fold f1 scores\nfrom sklearn.model_selection import KFold\n\n\nkfold = KFold(n_splits=5, shuffle=True)\nresults = cross_val_score(cv_model, X_test, np.argmax(y_test, axis=1), cv=kfold,scoring=\"f1_weighted\")","447264b6":"print('K-fold Cross Validation f1_weighted Results: ', results)\nprint('K-fold Cross Validation f1_weighted Results Mean: ', results.mean())","7434ab97":"# Tuned Model Prediction\n\ny_pred = cv_model.predict(X_test) ","6f6de84b":"# %% f1 score\nimport sklearn.metrics as metrics\nprint(\"f1_weighted:\",metrics.f1_score(np.argmax(y_test, axis=1), y_pred,average='weighted'))\n\n\n# %% Accuracy\n\nprint(\"accuracy:\",metrics.accuracy_score(np.argmax(y_test, axis=1), y_pred))","8bea1e62":"#%% Confusion Matrix and Classification Report\nfrom sklearn.metrics import confusion_matrix, classification_report \n\n# Classification Report\nmodel_report = classification_report(np.argmax(y_test, axis=1), y_pred)\nprint(model_report)","12ddda44":"# Confusion Matrix\nmodel_conf = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\nprint(model_conf)","e380b1a5":"#%% ROC-AUC Curve\n\ny_score = cv_model.predict_proba(X_test)\n\nfrom scipy import interp\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc\n# Learn to predict each class against the other\n\n\nn_classes = 8 # number of class\n\n\n\n\n# Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n# Compute micro-average ROC curve and ROC area\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])","6dc5ecc2":"# The process of drawing a roc-auc curve belonging to a specific class\n\nplt.figure()\nlw = 2 # line_width\nplt.plot(fpr[3], tpr[3], color='darkorange',\n         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[3]) # Drawing Curve according to 3. class \nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC E\u011frisi')\nplt.legend(loc=\"lower right\")\nplt.show()","f52bdb22":"# Process of plotting roc-auc curve belonging to all classes.\n\nfrom itertools import cycle\n\n# First aggregate all false positive rates\nall_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n\n# Then interpolate all ROC curves at this points\nmean_tpr = np.zeros_like(all_fpr)\nfor i in range(n_classes):\n    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n\n# Finally average it and compute AUC\nmean_tpr \/= n_classes\n\nfpr[\"macro\"] = all_fpr\ntpr[\"macro\"] = mean_tpr\nroc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n# Plot all ROC curves\nplt.figure()\nplt.plot(fpr[\"micro\"], tpr[\"micro\"],\n         label='micro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"micro\"]),\n         color='deeppink', linestyle=':', linewidth=4)\n\nplt.plot(fpr[\"macro\"], tpr[\"macro\"],\n         label='macro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"macro\"]),\n         color='navy', linestyle=':', linewidth=4)\n\ncolors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\nfor i, color in zip(range(n_classes), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n             label='ROC curve of class {0} (area = {1:0.2f})'\n             ''.format(i, roc_auc[i]))\n\nplt.plot([0, 1], [0, 1], 'k--', lw=lw)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Extending the ROC Curve to Multi-Class')\nplt.legend(loc=\"lower right\")\nplt.show()","42440a19":"## Modelling\n\n\n\n## Artificial Neural Network with KERAS","e8d272d4":"## Data Read"}}