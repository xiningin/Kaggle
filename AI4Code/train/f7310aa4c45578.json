{"cell_type":{"d9bc586e":"code","8b72098b":"code","e8e96452":"code","df0317e8":"code","d3350d10":"code","1c375fa5":"code","8a7ff6e8":"code","be830637":"code","3dd86238":"markdown","70f786fe":"markdown","9a2d2249":"markdown","be981943":"markdown","84b32e6a":"markdown","da0e8fc7":"markdown","6f898eb7":"markdown","fd8dfc83":"markdown"},"source":{"d9bc586e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8b72098b":"data = pd.read_csv('..\/input\/artikel-datsets\/artikel_noy.csv')\ndf_list = data.values.tolist()\nprint (df_list)","e8e96452":"!pip install Sastrawi\nimport nltk\nimport re\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize \nfrom nltk.probability import FreqDist\nfrom Sastrawi.Stemmer.StemmerFactory import StemmerFactory","df0317e8":"df = data [['Artikel']]\ndf['Artikel'] = df['Artikel'].astype(str)\n\ndf = pd.DataFrame(data[['Artikel']])\n\ndf[\"Case_Folding_Artikel\"] = df['Artikel'].str.lower()\n\ndf = pd.DataFrame(df[['Case_Folding_Artikel']])\n\ndf","d3350d10":"def remove_df_special(text):\n    text = text.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\"\")\n    text = text.encode('ascii', 'replace').decode('ascii')\n    text = ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\\/\\\/\\S+)\",\" \", text).split())\n    return text.replace(\"http:\/\/\", \" \").replace(\"https:\/\/\", \" \")\n                \ndf['Case_Folding_Artikel'] = df['Case_Folding_Artikel'].apply(remove_df_special)\n\ndef remove_df_number(text):\n    return  re.sub(r\"\\d+\", \"\", text)\n\ndf['Case_Folding_Artikel'] = df['Case_Folding_Artikel'].apply(remove_df_number)\n\ndef remove_df_punctuation(text):\n    return text.translate(str.maketrans(\"\",\"\",string.punctuation))\n\ndf['Case_Folding_Artikel'] = df['Case_Folding_Artikel'].apply(remove_df_punctuation)\n\ndef remove_df_whitespace(text):\n    return text.strip()\n\ndf['Case_Folding_Artikel'] = df['Case_Folding_Artikel'].apply(remove_df_whitespace)\n\ndef remove_df_whitespace_multiple(text):\n    return re.sub('\\s+',' ',text)\n\ndf['Case_Folding_Artikel'] = df['Case_Folding_Artikel'].apply(remove_df_whitespace_multiple)\n\ndef remove_df_singl_char(text):\n    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n\ndf['Case_Folding_Artikel'] = df['Case_Folding_Artikel'].apply(remove_df_singl_char)\n\ndef word_df_tokenize(text):\n    return word_tokenize(text)\n\ndf['df_tokenizing'] = df['Case_Folding_Artikel'].apply(word_df_tokenize)\n\nprint('Tokenizing Isi Artikel : \\n') \nprint(df['df_tokenizing'].head())","1c375fa5":"def freqDist_wrapper(text):\n    return FreqDist(text)\n\ndf['df_jumlah_token'] = df['df_tokenizing'].apply(freqDist_wrapper)\n\nprint('Jumlah Token di Isi Artikel : \\n') \nprint(df['df_jumlah_token'].head().apply(lambda x : x.most_common()))","8a7ff6e8":"rex_stopwords = stopwords.words('indonesian')\n\nrex_stopwords.extend([\"mn\", \"dg\", \"jg\", \"jga\", \"ny\", \"d\", 'kli', \n                       'masasi', 'kalo', 'biar', 'iya', 'bikin', \n                       'gak', 'ga', 'krn', 'bilang', 'nih', 'sih', \n                       'si', 'tau', 'tdk', 'tuh', 'utk', 'ya', \n                       'jd', 'jgn', 'sdh', 'nya', 'n', 't', \n                       'nyg', 'hehe', 'pen', 'u', 'aja', 'loh', 'nan',\n                       '&amp', 'yah'])\n\nrex_stopwords = set(rex_stopwords)\n\ndef df_stopwords_removal(words):\n    return [word for word in words if word not in rex_stopwords]\n\ndf['df_stopword'] = df['df_tokenizing'].apply(df_stopwords_removal) \n\n\nprint('Stopword Isi Artikel : \\n') \nprint(df['df_stopword'].head())","be830637":"df.to_csv(\"Artikel Preprocessing - Muhammad Alwi.csv\")","3dd86238":"# **CASE FOLDING**","70f786fe":"**#Opening**\nPada kesempatan kali ini, saya akan melakukan tahap Pre-Processing dengan menggunakan artikel yang saya ambil dari internet (Sejarah Buku di Indonesia). Dimana Tahap Preprocessing sangat dibutuhkan dalam melakukan pengolahan dataset untuk siap di proses, berikut tahap Pre-Processing... ","9a2d2249":"# EXPORT TO CSV","be981943":"Artikel yang saya gunakan, adalah sebagai berikut :","84b32e6a":"**perhitungan jumlah kata pada artikel**","da0e8fc7":"**#Library**\nadapun library yang saya gunakan pada tahap pre-processing ini adalah sebagai berkut :","6f898eb7":"# **TOKENIZE**","fd8dfc83":"# **STOPWORD**"}}