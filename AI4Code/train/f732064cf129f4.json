{"cell_type":{"ba693aef":"code","b9506dca":"code","c0e87242":"code","6077d18f":"code","9c342ab8":"code","bbac2c53":"code","8ab6cb8c":"code","4d7712cc":"code","6f2a319d":"code","5c629014":"code","31e07a91":"code","bbcdd378":"markdown","2ab3cf32":"markdown","c9a0a351":"markdown","d267fa1a":"markdown","44c2c228":"markdown","8449569c":"markdown","86664393":"markdown"},"source":{"ba693aef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b9506dca":"import numpy as np\nimport torch\nimport random\nimport matplotlib.pyplot as plt\nfrom scipy.ndimage.filters import gaussian_filter\nfrom scipy.interpolate import interp1d","c0e87242":"#PROJECT_DIR= '\/kaggle\/input'\n\nPROJECT_DIR = '.\/'\n\nclass ObjectView(object):\n    def __init__(self, d): self.__dict__ = d","6077d18f":"def get_templates():\n    d0 = np.asarray([5,6,6.5,6.75,7,7,7,7,6.75,6.5,6,5])\n    d1 = np.asarray([5,3,3,3.4,3.8,4.2,4.6,5,5.4,5.8,5,5])\n    d2 = np.asarray([5,6,6.5,6.5,6,5.25,4.75,4,3.5,3.5,4,5])\n    d3 = np.asarray([5,6,6.5,6.5,6,5,5,6,6.5,6.5,6,5])\n    d4 = np.asarray([5,4.4,3.8,3.2,2.6,2.6,5,5,5,5,5,5])\n    d5 = np.asarray([5,3,3,3,3,5,6,6.5,6.5,6,4.5,5])\n    d6 = np.asarray([5,4,3.5,3.25,3,3,3,3,3.25,3.5,4,5])\n    d7 = np.asarray([5,7,7,6.6,6.2,5.8,5.4,5,4.6,4.2,5,5])\n    d8 = np.asarray([5,4,3.5,3.5,4,5,5,4,3.5,3.5,4,5])\n    d9 = np.asarray([5,4,3.5,3.5,4,5,5,5,5,4.7,4.3,5])\n    \n    x = np.stack([d0,d1,d2,d3,d4,d5,d6,d7,d8,d9])\n    x -= x.mean(1,keepdims=True) # whiten\n    x \/= x.std(1,keepdims=True)\n    x -= x[:,:1]  # signal starts and ends at 0\n    \n    templates = {'x': x\/6., 't': np.linspace(-5, 5, len(d0))\/6.,\n                 'y': np.asarray([0,1,2,3,4,5,6,7,8,9])}\n    return templates\n\ndef plot_signals(xs, t, labels=None, args=None, ratio=2.6, do_transform=False, dark_mode=False, zoom=1):\n    rows, cols = 1, 10\n    fig = plt.figure(figsize=[cols*1.5,rows*1.5*ratio], dpi=60)\n    for r in range(rows):\n        for c in range(cols):\n            ix = r*cols + c\n            x, t = xs[ix], t\n            ax = plt.subplot(rows,cols,ix+1)\n\n            # plot the data\n            if do_transform:\n                assert args is not None, \"Need an args object in order to do transforms\"\n                x, t = transform(x, t, args)  # optionally, transform the signal in some manner\n            if dark_mode:\n                plt.plot(x, t, 'wo', linewidth=6)\n                ax.set_facecolor('k')\n            else:\n                plt.plot(x, t, 'k-', linewidth=2)\n            if labels is not None:\n                plt.title(\"label=\" + str(labels[ix]), fontsize=22)\n\n            plt.xlim(-zoom,zoom) ; plt.ylim(-zoom,zoom)\n            plt.gca().invert_yaxis() ; plt.xticks([], []), plt.yticks([], [])\n    plt.subplots_adjust(wspace=0, hspace=0)\n    plt.tight_layout() ; plt.show()\n    return fig","9c342ab8":"from IPython.display import Image\nfrom IPython.core.display import HTML \nprint(\"Examples from original MNIST dataset:\")\nURL = 'https:\/\/raw.githubusercontent.com\/greydanus\/mnist1d\/master\/static\/mnist.png'\nImage(url= URL,  width=800)","bbac2c53":"templates = get_templates()\nprint(\"Templates for the MNIST-1D dataset:\")\nx = templates['x']\nt = templates['t']\ny = templates['y']\nfig = plot_signals(x, t, labels=y, ratio=1.33, dark_mode=True)\n\n# fig.savefig(PROJECT_DIR + 'static\/mnist1d_black.png')","8ab6cb8c":"\ntemplates = get_templates()\nprint(\"Templates for the MNIST-1D dataset:\")\nx = templates['x']\nt = templates['t']\ny = templates['y']\nfig = plot_signals(x, t, labels=y, ratio=1.33, dark_mode=False)\n\n# fig.savefig(PROJECT_DIR + 'static\/mnist1d_white.png')","4d7712cc":"# transformations of the templates which will make them harder to fit\ndef pad(x, padding):\n    low, high = padding\n    p = low + int(np.random.rand()*(high-low+1))\n    return np.concatenate([x, np.zeros((p))])\n\ndef shear(x, scale=10):\n    coeff = scale*(np.random.rand() - 0.5)\n    return x - coeff*np.linspace(-0.5,.5,len(x))\n\ndef translate(x, max_translation):\n    k = np.random.choice(max_translation)\n    return np.concatenate([x[-k:], x[:-k]])\n\ndef corr_noise_like(x, scale):\n    noise = scale * np.random.randn(*x.shape)\n    return gaussian_filter(noise, 2)\n\ndef iid_noise_like(x, scale):\n    noise = scale * np.random.randn(*x.shape)\n    return noise\n\ndef interpolate(x, N):\n    scale = np.linspace(0,1,len(x))\n    new_scale = np.linspace(0,1,N)\n    new_x = interp1d(scale, x, axis=0, kind='linear')(new_scale)\n    return new_x\n\ndef transform(x, y, args, eps=1e-8):\n    new_x = pad(x+eps, args.padding) # pad\n    new_x = interpolate(new_x, args.template_len + args.padding[-1])  # dilate\n    new_y = interpolate(y, args.template_len + args.padding[-1])\n    new_x *= (1 + args.scale_coeff*(np.random.rand() - 0.5))  # scale\n    new_x = translate(new_x, args.max_translation)  #translate\n    \n    # add noise\n    mask = new_x != 0\n    new_x = mask*new_x + (1-mask)*corr_noise_like(new_x, args.corr_noise_scale)\n    new_x = new_x + iid_noise_like(new_x, args.iid_noise_scale)\n    \n    # shear and interpolate\n    new_x = shear(new_x, args.shear_scale)\n    new_x = interpolate(new_x, args.final_seq_length) # subsample\n    new_y = interpolate(new_y, args.final_seq_length)\n    return new_x, new_y\n\ndef get_dataset_args(as_dict=False):\n    arg_dict = {'num_samples': 5000,\n            'train_split': 0.8,\n            'template_len': 12,\n            'padding': [36,60],\n            'scale_coeff': .4, \n            'max_translation': 48,\n            'corr_noise_scale': 0.25,\n            'iid_noise_scale': 2e-2,\n            'shear_scale': 0.75,\n            'shuffle_seq': False,\n            'final_seq_length': 40,\n            'seed': 42}\n    return arg_dict if as_dict else ObjectView(arg_dict)","6f2a319d":"def apply_ablations(arg_dict, n=7):\n    ablations = [('shear_scale', 0),\n                ('iid_noise_scale', 0),\n                ('corr_noise_scale', 0),\n                 ('max_translation', 1),\n                 ('scale_coeff', 0),\n                 ('padding', [arg_dict['padding'][-1], arg_dict['padding'][-1]]),\n                 ('padding', [0, 0]),]\n    num_ablations = min(n, len(ablations))\n    for i in range(num_ablations):\n        k, v = ablations[i]\n        arg_dict[k] = v\n    return arg_dict\n\ntemplates = get_templates()\nfor i, n in enumerate(reversed(range(8))):\n    np.random.seed(0)\n    arg_dict = get_dataset_args(as_dict=True)\n    arg_dict = apply_ablations(arg_dict, n=n)\n    args = ObjectView(arg_dict)\n    do_transform = args.padding[0] != 0\n    fig = plot_signals(templates['x'], templates['t'], labels=None if do_transform else templates['y'],\n                 args=args, ratio=2.2 if do_transform else 0.8,\n                 do_transform=do_transform)\n#     fig.savefig(PROJECT_DIR + 'static\/transform_{}.png'.format(i))","5c629014":"def make_dataset(args=None, template=None, ):\n    templates = get_templates() if template is None else template\n    args = get_dataset_args() if args is None else args\n    np.random.seed(args.seed) # reproducibility\n    \n    xs, ys = [], []\n    samples_per_class = args.num_samples \/\/ len(templates['y'])\n    for label_ix in range(len(templates['y'])):\n        for example_ix in range(samples_per_class):\n            x = templates['x'][label_ix]\n            t = templates['t']\n            y = templates['y'][label_ix]\n            x, new_t = transform(x, t, args) # new_t transformation is same each time\n            xs.append(x) ; ys.append(y)\n    \n    batch_shuffle = np.random.permutation(len(ys)) # shuffle batch dimension\n    xs = np.stack(xs)[batch_shuffle]\n    ys = np.stack(ys)[batch_shuffle]\n    \n    if args.shuffle_seq: # maybe shuffle the spatial dimension\n        seq_shuffle = np.random.permutation(args.final_seq_length)\n        xs = xs[...,seq_shuffle]\n    \n    new_t = new_t\/xs.std()\n    xs = (xs-xs.mean())\/xs.std() # center the dataset & set standard deviation to 1\n\n    # train \/ test split\n    split_ix = int(len(ys)*args.train_split)\n    dataset = {'x': xs[:split_ix], 'x_test': xs[split_ix:],\n               'y': ys[:split_ix], 'y_test': ys[split_ix:],\n               't':new_t, 'templates': templates}\n    return dataset\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)","31e07a91":"\nargs = get_dataset_args()\nset_seed(args.seed)\nargs.shuffle_seq = False\ndata = make_dataset(args=args)  # make the dataset\n\nset_seed(args.seed)\nargs.shuffle_seq = True\ndata_shuff = make_dataset(args=args)  # make the dataset, shuffling the spatial dimension\n\nprint(\"Examples in training set: {}\".format(len(data['y'])))\nprint(\"Examples in test set: {}\".format(len(data['y_test'])))\nprint(\"Length of each example: {}\".format(data['x'].shape[-1]))\nprint(\"Number of classes: {}\".format(len(data['templates']['y'])))","bbcdd378":"# Visualize Transformations\n# \nWe'll apply one at a time so it's clear what each transformation is doing.","2ab3cf32":"Examples from original MNIST dataset:\n","c9a0a351":"Now you can download Dataset\n","d267fa1a":"# Construct a dataset\n# \nNow we can construct a dataset by applying random transformations to the template signals.","44c2c228":"Building MNIST-1D\n\nSpecial Thanks to Sam Greydanus | 2020\n\nThis notebook shows how to build the MNIST-1D dataset in full detail*.\n\nhttps:\/\/github.com\/greydanus\/mnist1d\/blob\/master\/building_mnist1d.ipynb\n\nFULL PAPER: https:\/\/arxiv.org\/pdf\/2011.14439.pdf","8449569c":"# **Templates**\n\nThese are 1D signals, consisting of 12 points each, that resemble the digits 0-9. They are meant to be analogous to the handwritten digits 0-9 in the original MNIST dataset:\n\nmnist digits\n\nBut unlike the original MNIST dataset, which consisted of 2D arrays of pixels (each image had 28x28=784 dimensions), this dataset consists of 1D timeseries of length 40. This means each example is ~20x smaller, making the dataset much quicker and easier to iterate over.","86664393":"# Transformations\n\nIn order to build a synthetic dataset, we are going to pass the templates through a series of random transformations. This includes adding random amounts of padding, translation, correlated noise, iid noise, and scaling.\n\nWe use these transformations because they are relevant for both 1D and 2D images. So even though our dataset is 1D, we can expect some of our findings to hold for 2D (image) data. For example, we can study the advantage of using a translation-invariant model by making a dataset where signals occur at different locations in the sequence. We can do this by using large padding and translation coefficients.\n\nIn the following section, we plot the step-by-step transformations of digit templates into dataset examples. Note that you can generate your own synthetic datasets by changing the relevant hyperparameters."}}