{"cell_type":{"0a4fb726":"code","bbbdc273":"code","3c70b3a7":"code","230c72b8":"code","2f35d064":"code","7f2c32c0":"code","eaf6fe92":"code","f1875334":"code","9f3ba683":"code","954f8e9a":"code","95b80f3f":"code","dc567e1a":"code","50427fa1":"code","5f6b30aa":"code","16a6fb5e":"code","5d1de0b7":"code","a293936c":"code","31fe4bd5":"code","3b129972":"code","1e0aeda1":"code","49c28248":"code","08cf8c4c":"code","d2f4b1de":"code","ad306689":"code","aba4729d":"code","90d6e8b5":"code","eba4e4fd":"code","8cf39ff7":"code","cd0a639f":"code","fa393e34":"code","d694365b":"code","3f4064de":"code","39407146":"code","3a09f236":"code","8a853602":"code","a24b9007":"code","0e6ae617":"code","7b550a09":"code","17b957d2":"code","4160ca1a":"code","1e0011dc":"code","c887019b":"code","25cd1e78":"code","0ee310b8":"code","bc652128":"markdown","966460e7":"markdown","557507d4":"markdown","f1695704":"markdown","10e09c6e":"markdown","9dff6ae0":"markdown","5f4f7f61":"markdown","4a6b740c":"markdown"},"source":{"0a4fb726":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport tensorflow as tf\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom tensorflow.python.framework import ops","bbbdc273":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","3c70b3a7":"train.shape, test.shape","230c72b8":"num_images = 6\nm = train.shape[0]\nidx = np.random.choice(m, size=num_images)","2f35d064":"len(idx)","7f2c32c0":"num_rows = 2\nnum_cols = 3\n\nfig, ax = plt.subplots(nrows = num_rows, ncols = num_cols)\nfig.set_size_inches(12,10)\n\nfor i, j in enumerate(idx):\n    # Find the right place to put the images, a is the row in the figure and b is the column\n    \n    a = i\/\/num_cols\n    b = i%num_cols\n\n    # Remove ticks\n    \n    ax[a][b].tick_params(\n    which='both',\n    left=False,\n    right=False,\n    bottom=False,\n    top=False,\n    labelleft = False,\n    labelbottom=False)\n    \n    # Draw image and set x label as the actual label of the image i.e. the value of the digit in the image\n    \n    ax[a][b].imshow(np.array(train.loc[j][1:]).reshape(28,28), cmap=plt.get_cmap('gray'))\n    ax[a][b].set_xlabel(str(train.loc[j][0]), fontsize = 50)\n\nplt.show()","eaf6fe92":"train.describe()","f1875334":"X = np.array(train.iloc[:,1:])","9f3ba683":"X.shape","954f8e9a":"X = X.reshape((m,28,28,1))","95b80f3f":"y = np.array(train.label)\n\ndef convert_to_one_hot(Y, C):\n    Y = np.eye(C)[Y]\n    return Y\n\ny = convert_to_one_hot(y,10)","dc567e1a":"y[0:5]","50427fa1":"train.label.head()","5f6b30aa":"# Set random seed\n\nseed = 5\nnp.random.seed(seed)\n\n# Get random training index\n\ntrain_index = np.random.choice(m, round(m*0.95), replace=False)\ndev_index = np.array(list(set(range(m)) - set(train_index)))\n\n# Make training and dev\n#X_train = X\nX_train = X[train_index]\nX_dev = X[dev_index]\n#y_train = y\ny_train = y[train_index]\ny_dev = y[dev_index]","16a6fb5e":"X_train.shape","5d1de0b7":"m_test = test.shape[0]\nX_test = np.array(test).reshape((m_test,28,28,1))\n","a293936c":"X_test.shape","31fe4bd5":"X_train = X_train\/255.\nX_dev = X_dev\/255.\nX_test = X_test\/255.\nX_test = np.float32(X_test)","3b129972":"print (\"number of training examples = \" + str(X_train.shape[0]))\nprint (\"number of validation (dev) examples = \" + str(X_dev.shape[0]))\nprint (\"number of test examples = \" + str(X_test.shape[0]))\nprint (\"X_train shape: \" + str(X_train.shape))\nprint (\"y_train shape: \" + str(y_train.shape))\nprint (\"X_dev shape: \" + str(X_dev.shape))\nprint (\"y_dev shape: \" + str(y_dev.shape))\nprint (\"X_test shape: \" + str(X_test.shape))","1e0aeda1":"# Create Placeholders\n\ndef create_placeholders(n_H0,n_W0,n_C0,n_y):\n    X = tf.placeholder(dtype = tf.float32,shape = [None, n_H0, n_W0, n_C0])\n    Y = tf.placeholder(dtype = tf.float32,shape = [None, n_y])\n    drop_rate = tf.placeholder(dtype=tf.float32, name = 'drop_rate')\n    return X, Y, drop_rate","49c28248":"# Initialize parameters\n\ndef initialize_parameters():\n    tf.set_random_seed(1)\n    initializer = tf.contrib.layers.xavier_initializer(seed = 0)\n    W1a = tf.get_variable(name = 'W1a', shape = [7, 7, 1, 20], initializer = initializer)\n    W1b = tf.get_variable(name = 'W1b', shape = [5, 5, 1, 20], initializer = initializer)\n    W1c = tf.get_variable(name = 'W1c', shape = [3, 3, 1, 20], initializer = initializer)\n    W2a = tf.get_variable(name = 'W2a', shape = [5, 5, 60, 50], initializer = initializer)\n    W2b = tf.get_variable(name = 'W2b', shape = [3, 3, 60, 50], initializer = initializer)\n    W3 = tf.get_variable(name = 'W3', shape = [3, 3, 100, 120], initializer = initializer)\n    parameters = {\"W1a\": W1a,\n                  \"W1b\": W1b,\n                  \"W1c\": W1c,\n                  \"W2a\": W2a,\n                  \"W2b\": W2b,\n                  \"W3\": W3,}\n    \n    return parameters","08cf8c4c":"# Check parameters\n\ntf.reset_default_graph()\nwith tf.Session() as sess_test:\n    parameters = initialize_parameters()\n    init = tf.global_variables_initializer()\n    sess_test.run(init)\n    print(\"W1a = \" + str(parameters[\"W1a\"].eval()[1,1,0]))\n    print(\"W1b = \" + str(parameters[\"W1b\"].eval()[1,1,0]))\n    print(\"W1c = \" + str(parameters[\"W1c\"].eval()[1,1,0]))\n    print(\"W2a = \" + str(parameters[\"W2a\"].eval()[1,1,1]))\n    print(\"W2b = \" + str(parameters[\"W2b\"].eval()[1,1,1]))\n    print(\"W3 = \" + str(parameters[\"W3\"].eval()[1,1,0]))","d2f4b1de":"# Build forward propagation computation graph\n\ndef forward_propagation(X, parameters, drop_rate):\n    W1a = parameters['W1a']\n    W1b = parameters['W1b']\n    W1c = parameters['W1c']\n    W2a = parameters['W2a']\n    W2b = parameters['W2b']\n    W3 = parameters['W3']\n    #rate_1, rate_2, rate_3 = drop_rate\n \n    # CONV2D: stride of 1, padding 'SAME'\n    Z1a = tf.nn.conv2d(X,W1a, strides = [1,1,1,1], padding = 'SAME')\n    Z1b = tf.nn.conv2d(X,W1b, strides = [1,1,1,1], padding = 'SAME')\n    Z1c = tf.nn.conv2d(X,W1c, strides = [1,1,1,1], padding = 'SAME')\n    Z1 = tf.concat([Z1a, Z1b, Z1c], axis = -1)\n    # RELU\n    A1 = tf.nn.relu(tf.layers.batch_normalization(Z1))\n    # MAXPOOL: window 2x2, stride 2, padding 'VALID'\n    P1 = tf.nn.max_pool(A1, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'VALID')\n    # CONV2D: filters W2, stride 1, padding 'SAME'\n    Z2a = tf.nn.conv2d(P1,W2a, strides = [1,1,1,1], padding = 'SAME')\n    Z2b = tf.nn.conv2d(P1,W2b, strides = [1,1,1,1], padding = 'SAME')\n    Z2 = tf.concat([Z2a, Z2b], axis = -1)\n    # RELU\n    A2 = tf.nn.relu(tf.layers.batch_normalization(Z2))\n    # MAXPOOL: window 2x2, stride 2, padding 'VALID'\n    P2 = tf.nn.max_pool(A2, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'VALID')\n    # CONV2D: filters W3, stride 1, padding 'SAME'\n    Z3 = tf.nn.conv2d(P2,W3, strides = [1,1,1,1], padding = 'SAME')\n    # RELU\n    A3 = tf.nn.relu(tf.layers.batch_normalization(Z3))\n    # MAXPOOL: window 2x2, stride 2, padding 'VALID'\n    P3 = tf.nn.max_pool(A3, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'VALID')\n    # FLATTEN\n    P3 = tf.contrib.layers.flatten(P3)\n    # FULLY-CONNECTED \n    Z4 = tf.layers.dropout(tf.contrib.layers.fully_connected(P3, 200, normalizer_fn=tf.layers.batch_normalization), rate = drop_rate)\n    # FULLY-CONNECTED \n    Z5 = tf.layers.dropout(tf.contrib.layers.fully_connected(Z4, 120, normalizer_fn=tf.layers.batch_normalization), rate = drop_rate)\n    # FULLY-CONNECTED \n    Z6 = tf.layers.dropout(tf.contrib.layers.fully_connected(Z5, 84, normalizer_fn=tf.layers.batch_normalization), rate = drop_rate)\n    # FULLY-CONNECTED \n    Z7 = tf.contrib.layers.fully_connected(Z6, 10, normalizer_fn=tf.layers.batch_normalization, activation_fn = tf.nn.softmax)\n    \n    return Z7","ad306689":"# Check forward propagation\n\ntf.reset_default_graph()\n\nwith tf.Session() as sess:\n    np.random.seed(1)\n    X, Y, drop_rate = create_placeholders(28, 28, 1, 10)\n    parameters = initialize_parameters()\n    Z7 = forward_propagation(X, parameters, drop_rate)\n    init = tf.global_variables_initializer()\n    sess.run(init)\n    a = sess.run(Z7, {X: np.random.randn(2,28,28,1), Y: np.random.randn(2,10), drop_rate: 0})\n    print(\"Z7 = \" + str(a))\n    ","aba4729d":"# Compute Cost\n\ndef compute_cost(Z7, Y):\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = Z7, labels = Y))\n    return cost","90d6e8b5":"# Check cost function\n\ntf.reset_default_graph()\n\nwith tf.Session() as sess:\n    np.random.seed(1)\n    X, Y, drop_rate = create_placeholders(28, 28, 1, 10)\n    parameters = initialize_parameters()\n    Z7 = forward_propagation(X, parameters, drop_rate)\n    cost = compute_cost(Z7, Y)\n    init = tf.global_variables_initializer()\n    sess.run(init)\n    a = sess.run(cost, {X: np.random.randn(4,28,28,1), Y: np.random.randn(4,10), drop_rate: 0})\n    print(\"cost = \" + str(a))","eba4e4fd":"# Set hyperparameters and optimization function\n\nlearning_rate = 7e-5\nnum_epochs = 20\nbatch_size = 16\n","8cf39ff7":"def model(X_train, X_dev, X_test, y_train, y_dev, learning_rate = learning_rate, num_epochs = num_epochs, batch_size = batch_size, print_cost = True):\n    \n    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n    (m_train, n_H0, n_W0, n_C0) = X_train.shape             \n    n_y = y_train.shape[1]                            \n    costs = []                                        # To keep track of the cost\n    if m_train%batch_size !=0:\n        num_batches = (m_train\/\/batch_size) + 1\n    else:\n        num_batches = m_train\/\/batch_size\n    \n    # Create Placeholders of the correct shape\n    \n    X, Y, drop_rate = create_placeholders(n_H0, n_W0, n_C0, n_y)\n    \n\n    # Initialize parameters\n    \n    parameters = initialize_parameters()\n    \n    \n    # Forward propagation: Build the forward propagation in the tensorflow graph\n    \n    Z7 = forward_propagation(X, parameters, drop_rate)\n    \n    \n    # Cost function: Add cost function to tensorflow graph\n    \n    cost = compute_cost(Z7, Y)\n    \n    \n    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost.\n    \n    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n    \n    \n    # Initialize all the variables globally\n    init = tf.global_variables_initializer()\n  \n\n    # Start the session to compute the tensorflow graph\n    with tf.Session() as sess:\n\n        # Run the initialization\n        sess.run(init)\n\n        for epoch in range(num_epochs):\n            # Generate random batch index\n            minibatch_cost = 0\n            full_batch = range(m_train)\n\n            for batch in range(num_batches):        \n                try:\n                    batch_index = np.random.choice(full_batch, size=batch_size, replace = False)\n                    full_batch = np.array(list(set(full_batch) - set(batch_index)))\n                except ValueError:\n                    batch_index = full_batch\n                batch_train_X = X_train[batch_index]\n                batch_train_y = y_train[batch_index]\n\n                # Run session to reach goal \n\n                sess.run(optimizer, feed_dict={X: batch_train_X, Y: batch_train_y, drop_rate: 0.4})\n                temp_cost = sess.run(cost, feed_dict={X: batch_train_X, Y: batch_train_y, drop_rate: 0})\n                minibatch_cost += temp_cost \/ num_batches\n\n            # Print the cost every epoch\n            \n            print (\"Cost after epoch %i: %f\" % (epoch+1, minibatch_cost))\n            costs.append(minibatch_cost)\n\n\n        # plot the cost\n        plt.plot(np.squeeze(costs))\n        plt.ylabel('cost')\n        plt.xlabel('iterations')\n        plt.title(\"Learning rate =\" + str(learning_rate))\n        plt.show()\n\n        # Calculate the correct predictions\n        #Z6 = forward_propagation(X, parameters, rate = [1, 1])\n        predict_op = tf.argmax(Z7, 1)\n        #correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n      \n        # Calculate accuracy on the train and dev sets\n        \n        #accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n        #print(accuracy)\n        #train_accuracy = accuracy.eval({X: X_train, Y: y_train})\n        #dev_accuracy = accuracy.eval({X: X_dev, Y: y_dev})\n        \n        #print(\"Dev Accuracy:\", dev_accuracy)\n        \n        # Make predictions\n        train_preds = np.empty(shape = m_train)\n        for batch in range(num_batches):\n            if batch != num_batches - 1:\n                batch_index = range(batch*batch_size, (batch+1)*batch_size)\n            else:\n                batch_index = range(batch*batch_size,m_train)\n            X_train_batch = X_train[batch_index]\n            Y_train_batch_preds = sess.run(predict_op, feed_dict ={X: X_train_batch, drop_rate: 0})\n            train_preds[batch_index] = Y_train_batch_preds\n            #print('Train batch {} completed'.format(batch+1))\n        \n        m_dev = X_dev.shape[0]\n        dev_preds = np.empty(shape = m_dev)\n        if m_dev%batch_size !=0:\n            num_batches = (m_dev\/\/batch_size) + 1\n        else:\n            num_batches = m_dev\/\/batch_size\n        \n        for batch in range(num_batches):\n            if batch != num_batches - 1:\n                batch_index = range(batch*batch_size, (batch+1)*batch_size)\n            else:\n                batch_index = range(batch*batch_size,m_dev)\n            X_dev_batch = X_dev[batch_index]\n            Y_dev_batch_preds = sess.run(predict_op, feed_dict ={X: X_dev_batch, drop_rate: 0})\n            dev_preds[batch_index] = Y_dev_batch_preds\n        \n        m_test = X_test.shape[0]\n        test_preds = np.empty(shape = m_test)\n        if m_test%batch_size !=0:\n            num_batches = (m_test\/\/batch_size) + 1\n        else:\n            num_batches = m_test\/\/batch_size\n        \n        for batch in range(num_batches):\n            if batch != num_batches - 1:\n                batch_index = range(batch*batch_size, (batch+1)*batch_size)\n            else:\n                batch_index = range(batch*batch_size,m_test)\n            X_test_batch = X_test[batch_index]\n            Y_test_batch_preds = sess.run(predict_op, feed_dict ={X: X_test_batch, drop_rate: 0})\n            test_preds[batch_index] = Y_test_batch_preds\n            #print('Train batch {} completed'.format(batch+1))\n        #train_preds = sess.run(predict_op, feed_dict ={X:X_train})\n        #test_preds = sess.run(predict_op, feed_dict ={X:X_test})\n        train_accuracy = np.mean(train_preds.astype(int)==np.argmax(y_train,1))\n        print(\"Train Accuracy:\", train_accuracy)\n        dev_accuracy = np.mean(dev_preds.astype(int)==np.argmax(y_dev,1))\n        print(\"Dev Accuracy:\", dev_accuracy)\n        \n        return parameters, train_preds, dev_preds, test_preds","cd0a639f":"parameters, train_preds, dev_preds, test_preds = model(X_train, X_dev, X_test, y_train, y_dev)","fa393e34":"i = np.random.choice(m_test)\nprint(\"Test sample no.: {}\".format(i))\n\nprint('Prediction: {}'.format(test_preds[i]))\nplt.imshow(X_test[i,:,:,0],cmap = plt.get_cmap('gray'))\nplt.show()\n","d694365b":"train_labels = np.argmax(y_train, axis = 1)\ntrain_accuracy = np.mean(train_labels==train_preds.astype(int))","3f4064de":"train_accuracy","39407146":"dev_labels = np.argmax(y_dev, axis = 1)\ndev_accuracy = np.mean(dev_labels==dev_preds.astype(int))","3a09f236":"dev_accuracy","8a853602":"dev_new = pd.DataFrame()","a24b9007":"dev_new['Label'] = dev_labels\ndev_new['Preds'] = dev_preds.astype(int)\nm_dev = X_dev.shape[0]\ndev_new['ImageId'] = list(range(m_dev))","0e6ae617":"dev_new.head()","7b550a09":"dev_mismatch = dev_new[dev_new['Label']!=dev_new['Preds']]","17b957d2":"dev_mismatch.Label.value_counts()","4160ca1a":"i = np.random.choice(dev_mismatch['ImageId'])\nprint(\"Image ID.: {}\".format(i))\nprint('Prediction: {}'.format(int(dev_preds[i])))\nprint('Correct Label: {}'.format(dev_labels[i]))\nplt.imshow(X_dev[i,:,:,0],cmap = plt.get_cmap('gray'))\nplt.show()","1e0011dc":"test['Label'] = test_preds.astype(int)","c887019b":"test['ImageId'] = list(range(1,m_test+1))","25cd1e78":"test.head()","0ee310b8":"test[['ImageId', 'Label']].to_csv('submission_lenet5.csv', index = False, header = ['ImageId','Label'])","bc652128":"# 4. Apply LeNet 5 architecture","966460e7":"# 7. Write submission file","557507d4":"# 5. Check random sample prediction from test set","f1695704":"# 1. Load the data","10e09c6e":"# 3. Convert data to the right shape for CNN\n\nConvert the flattened arrays to image arrays, normalize by dividing by 255 and separate features (X) from labels (y)","9dff6ae0":"The LeNet architecture we will apply is as follows:\n\nINPUT => CONV (28x28x20, f = 5, s = 1) => RELU => POOL (14x14x20, f = 2, s = 2) => CONV (14x14x50, f = 5, s = 1) => RELU => POOL (7x7x50, f = 2, s = 2) + flatten => FC (120) => RELU => FC (84) => softmax\n\nThus, there are 2 conv layers and 2 pooling layers. Then 2 fully connected layers with ReLU activation and the final layer with a softmax","5f4f7f61":"# 2. Visualize the digits\n\nLet us look at some random digits from the training sample and their respective labels ","4a6b740c":"# 6. Check which ones are incorrect from the validation set"}}