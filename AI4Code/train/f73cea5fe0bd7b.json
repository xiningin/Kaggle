{"cell_type":{"9d97ad78":"code","c6ad5e19":"code","d9bd82de":"code","b62b8658":"code","88d5c5a2":"code","95e027a3":"code","be95d29a":"code","6ad33cb2":"code","1f5785c5":"code","4dad08b0":"code","235e68d7":"code","6895c0c7":"code","e88c12a6":"code","85d07354":"code","a15ffcb1":"code","fd930fe3":"code","e97b73ba":"code","aa043510":"code","fb141a81":"code","2518df46":"code","5059ebd9":"code","9b837234":"code","c175f4db":"code","ed15b236":"code","1e4e2ad8":"code","0f7fde3b":"code","1cd8cc85":"code","c66e1e64":"code","553c6faa":"code","57afda58":"code","c6977a72":"code","60e0e35c":"code","66571ba0":"code","d1985e15":"code","175dbbfb":"code","5ee15c29":"code","e2af4082":"code","0e86f7b9":"code","d12580f0":"code","8002b40c":"code","a7409aaa":"code","777d632c":"code","8d3a876c":"code","a82ec642":"code","60e07a3c":"code","4ab1fe15":"code","b650ebfc":"code","c5ec2013":"code","47ffe9be":"code","87e9de21":"code","560671c6":"code","6dbb1640":"code","e24bf44c":"code","1b8fd5a1":"code","e851e36e":"code","5a343fd9":"code","f64f4fe4":"code","ed785402":"code","67eb8569":"code","8246c5b0":"code","22231f35":"code","f0a00fe7":"code","5f5d3d18":"code","2b53e15d":"code","3b05d981":"code","06a6e49c":"markdown","b181a7eb":"markdown","e64bb9dc":"markdown","81e553bc":"markdown","c9d96ebe":"markdown","757b9ee2":"markdown","89ab06bc":"markdown","e136ba07":"markdown","2cda5092":"markdown","6d5dd4ea":"markdown","9ad7ea8c":"markdown","aa256e71":"markdown","8be43625":"markdown","98c6312e":"markdown","e35248f2":"markdown","b628b771":"markdown","5be07f82":"markdown","0325f676":"markdown","266fd9b2":"markdown","26798bc8":"markdown","ce218336":"markdown","80480c7d":"markdown","94ce04c0":"markdown"},"source":{"9d97ad78":"# Basic libaries\nimport numpy as np\nfrom datetime import datetime as dt, timedelta as td\n# Data manipulation\nimport pandas as pd\nimport csv\n# Modelling\nimport statsmodels.api as sm\nimport sklearn as sk\nfrom sklearn.preprocessing import MinMaxScaler\nimport scipy as sp\nimport tensorflow as tf\n# Visualizations\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nmatplotlib.use('SVG')","c6ad5e19":"# Changing Frequency\nfreq=td(hours=1)\n# Choose missing_data_handling to be True to fill missing data points with a concave function and False to remove missing data points.\nmissing_data_handling=False","d9bd82de":"import os\nprint(os.listdir(\"..\/input\"))","b62b8658":"# reading in stock index date\nprices=pd.read_csv(\"..\/input\/twitter-investor-sentiment-analysis-dataset\/prices_60m.csv\",parse_dates=['Dates'])\nprices.rename(columns={'Dates':'dates'},inplace=True)\nprices=prices.set_index('dates')\nprices=prices.dropna(how='any')\nprices\n\nif missing_data_handling:\n    prices=prices.resample(freq,base=15.5).asfreq()\n    \n# calculating logreturns and Z Scores\ndef logreturn(x):\n    return np.log(x\/x.shift(1))\n\ndef zscore(x):\n    return (x-x.mean())\/x.std()\n\nprices['DJIA LogRet']=logreturn(prices['DJIA CLOSE'])\nprices['SP500 LogRet']=logreturn(prices['SP500 CLOSE'])\nprices['DJIA Z Scores']=zscore(prices['DJIA CLOSE'])\nprices['SP500 Z Scores']=zscore(prices['SP500 CLOSE'])\nprices['DJIA Log']=np.log(prices['DJIA CLOSE'])\nprices['SP500 Log']=np.log(prices['SP500 CLOSE'])\nprices=prices.dropna(how='any')\nprices","88d5c5a2":"# Reading in csv file containing tweets and converting it to a pandas dataframe\ntweets=pd.read_csv(\"..\/input\/twitter-investor-sentiment-analysis-dataset\/Raw_tweets_09_30.csv\",parse_dates=['created_at'])\ntweets = tweets.drop(columns=\"id\")\n\n# Converting time from UTC to CEST +2 hours\ncest_date=[]\nfor date in tweets['created_at']:\n    cest_date.append(date+td(hours=2))\ntweets['created_at']=cest_date\n\n# Changing index to Datetimeindex\ntweets.rename(columns={'created_at':'date'},inplace=True)\ntweets=tweets.set_index('date')\n#tweets.index = pd.DatetimeIndex(tweets.index)\n\ntweets","95e027a3":"len(tweets)","be95d29a":"tweets['Time']=tweets.index.hour","6ad33cb2":"# Calculating average number of tweets per day\ndf=tweets.groupby(tweets.index.date).count()\ndf['Total']=df['tweet_text']+df['bullish']\ndf['Total'].mean()","1f5785c5":"#tweets['Time'].value_counts().sort_index().plot.bar()\nfig, ax = plt.subplots()\ncounts, bins, patches = ax.hist(tweets.Time, facecolor='gray', edgecolor='black', bins=range(0,25))\nax.set_xticks(bins)\nax.set_title('Tweetek \u00f3r\u00e1nk\u00e9nti eloszl\u00e1sa', fontsize=9)\nax.tick_params(axis='both', which='major', labelsize=7)\nplt.savefig(\"hourly_tweet_distribution.svg\", format=\"svg\")\nprint(counts)\nprint(bins)","4dad08b0":"if not missing_data_handling:\n    # Removing tweets that were created on weekends and bank holidays\n    #getting trading dates from DJIA intraday prices\n    t_dates=prices.index.map(pd.Timestamp.date).unique()\n    str_t_dates=[str(x) for x in t_dates]\n    tweets=tweets[tweets.index.floor('D').isin(str_t_dates)]\n    # Removing tweets that are not in trading analysis interval\n    start=str(td(hours=15,minutes=30)-freq)\n    end=str(td(hours=21,minutes=29))\n    tweets = tweets.between_time(start, end, include_start=True, include_end=True)","235e68d7":"# Separating sentiment tweets from bullish\/bearish index tweets\ntext_tweets=tweets.loc[:, ['tweet_text']]\nmarket_tweets=tweets.loc[:, ['bullish']]","6895c0c7":"# Filtering out NaNs\nfmarket_tweets=market_tweets[market_tweets['bullish'].notnull()]\nftext_tweets=text_tweets[text_tweets['tweet_text'].notnull()]\nftext_tweets =ftext_tweets.drop_duplicates(subset=\"tweet_text\",keep=False)\nprint('Total number of observations including both text and bullish\/bearish tweets:',ftext_tweets.tweet_text.count()+fmarket_tweets.bullish.count())\nftext_tweets","e88c12a6":"# Filtering out tweets containing a link\n#ftext_tweets=ftext_tweets[~ftext_tweets['tweet_text'].str.contains(\"https\")]","85d07354":"text = \" \".join(tweet for tweet in ftext_tweets.tweet_text)\nprint ('There are %s words in the combination of all review.' % (len(text)))","a15ffcb1":"# Create stopword list:\nstopwords = set(STOPWORDS)\nstopwords_raw=[\"one\",\"co\",\"amp\",\"https\",\"bullish\", \"bearish\", \"stock market\", \"I think economy\", \"I feel economy\", \"I am feeling economy\", \"It feels economy\",\"inflation\", \"unemployment rate\", \"recession\",\"SP500\",\"S&P500\",\"DJIA\",\"Dow jones\"]\nstopwords_splited=[]\nfor phrase in stopwords_raw:\n    split=phrase.split()\n    stopwords_splited=list(set(stopwords_splited+split))\nstopwords.update(stopwords_splited)\n\n# Generate a word cloud image\nwordcloud = WordCloud(stopwords=stopwords, background_color=\"white\",width=800, height=400).generate(text)\n\n# Display the generated image:\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()\nfig = plt.gcf() #get current figure\n#fig.set_size_inches(10,10)\n#plt.savefig(\"tweets_word_cloud.png\", format=\"png\")","fd930fe3":"pd.set_option('display.max_colwidth', -1)\nftext_tweets[ftext_tweets['tweet_text'].str.contains(\"now\")].sample(10)","e97b73ba":"import nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n#nltk.download('vader_lexicon')\n\nsia=SentimentIntensityAnalyzer()","aa043510":"del sia.lexicon['thank']\ndel sia.lexicon['thanks']","fb141a81":"stock_lex = pd.read_csv('VADER +\/stock_lex.csv')\nstock_lex['sentiment'] = (stock_lex['Aff_Score'] + stock_lex['Neg_Score'])\/2\nstock_lex = dict(zip(stock_lex.Item, stock_lex.sentiment))\n#filtering out multiple words expressions from the lexicon\nstock_lex = {k:v for k,v in stock_lex.items() if len(k.split(' '))==1}\nstock_lex_scaled = {}\nfor k, v in stock_lex.items():\n    if v > 0:\n        stock_lex_scaled[k] = v \/ max(stock_lex.values()) * 4\n    else:\n        stock_lex_scaled[k] = v \/ min(stock_lex.values()) * -4","2518df46":"positive = []\nwith open('VADER +\/lm_positive.csv', 'r') as f:\n    reader = csv.reader(f)\n    for row in reader:\n        positive.append(row[0].strip())\n        \nnegative = []\nwith open('VADER +\/lm_negative.csv', 'r') as f:\n    reader = csv.reader(f)\n    for row in reader:\n        entry = row[0].strip().split(\" \")\n        if len(entry) > 1:\n            negative.extend(entry)\n        else:\n            negative.append(entry[0])","5059ebd9":"final_lex = {}\nfinal_lex.update({word:2.0 for word in positive})\nfinal_lex.update({word:-2.0 for word in negative})\nfinal_lex.update(stock_lex_scaled)\nfinal_lex.update(sia.lexicon)\nsia.lexicon = final_lex","9b837234":"sentiment=[]\nfor tweet in ftext_tweets['tweet_text']:\n    sentiment.append(sia.polarity_scores(tweet)['compound'])\nftext_tweets['Sentiment Score']=sentiment","c175f4db":"pd.set_option('display.max_colwidth', -1)\nftext_tweets[ftext_tweets['tweet_text'].str.len()<150].sample(10)","ed15b236":"# Downsample the tweet sentiment scores\niss = ftext_tweets['Sentiment Score'].resample(freq,base=15.5,label='right').mean().to_frame()\niss = iss[iss['Sentiment Score'].notnull()]\niss['Z Scores']=(iss['Sentiment Score']-iss['Sentiment Score'].mean())\/iss['Sentiment Score'].std()","1e4e2ad8":"iss","0f7fde3b":"fmarket_tweets","1cd8cc85":"bbi=pd.DataFrame()\nbbi['nbull']=(fmarket_tweets['bullish'] == True).resample(freq,base=15.5,label='right').sum()\nbbi['nbear']=(fmarket_tweets['bullish'] == False).resample(freq,base=15.5,label='right').sum()\nbbi['BBI']=np.log(bbi['nbull']\/bbi['nbear'])\nbbi = bbi[bbi['BBI'].notnull()]\nbbi['Z Scores']=(bbi['BBI']-bbi['BBI'].mean())\/bbi['BBI'].std()","c66e1e64":"#Calculating rolling window Z Scores","553c6faa":"# Defining columns that are used in the analysis\ndjiaval=prices['DJIA Z Scores']\nspval=prices['SP500 Z Scores']\nbbival=bbi['Z Scores']\nissval=iss['Z Scores']\n\ntimeseries=pd.DataFrame({'DJIA': djiaval, 'SP500': spval,'BBI': bbival,'ISS': issval})\ntimeseries=timeseries.dropna(how='any')\n\nbbival=timeseries['BBI']\nissval=timeseries['ISS']\ndjiaval=timeseries['DJIA']\nspval=timeseries['SP500']","57afda58":"df_graphs=timeseries.reset_index()\ndf_graphs.rename(columns={'index':'dates'},inplace=True)\ndf_graphs","c6977a72":"# Visualizing DJIA and SP500 prices\ndaily=pd.read_csv(\"Input data\\Import\\prices_daily.csv\",parse_dates=[\"Dates\"])\n\nfig, axs = plt.subplots(1,2,figsize=(20,5))\n\nxlabel=[0]+list(daily[daily.index.isin([0,10,20,30,40,50,60,70])]['Dates'].dt.date)\n\naxs[0].plot(daily['DJIA CLOSE'],color='blue', label='DJIA')\naxs[1].plot(daily['SP500 CLOSE'],color='blue', label='SP500')\naxs[0].set_title('DJIA')\naxs[1].set_title('S&P 500')\naxs[0].set_xticklabels(xlabel)\naxs[1].set_xticklabels(xlabel)\n#plt.savefig(\"DJIA_SP500_hist_price.svg\",format=\"svg\")","60e0e35c":"fig, axs = plt.subplots(2,figsize=(18,10))\n\nxlabel=[0]+list(df_graphs[df_graphs.index.isin([0,50,100,150,200,250,300,350])]['dates'].dt.date)\n\naxs[0].plot(df_graphs['BBI'],color='green', label='BBI',alpha=0.5)\naxs[0].plot(df_graphs['ISS'],color='black', label='ISS')\naxs[0].plot(df_graphs['DJIA'],color='red', label='DJIA')\naxs[0].set_title('Z Scores')\naxs[0].legend()\naxs[0].set_xticklabels(xlabel)\naxs[1].plot(df_graphs['BBI'],color='green', label='BBI',alpha=0.5)\naxs[1].plot(df_graphs['ISS'],color='black', label='ISS')\naxs[1].plot(df_graphs['SP500'],color='red', label='S&P500')\naxs[1].set_title('Z Scores')\naxs[1].legend()\naxs[1].set_xticklabels(xlabel)\nplt.savefig(\"Z-Scores.svg\", format=\"svg\")","66571ba0":"def diffs(ts):\n    diffs=(ts-ts.shift(1))**2\n    diffs=diffs[1:]\n    return (diffs.sum()\/diffs.count())**(1\/2)\n\nprint(diffs(bbival))\nprint(diffs(issval))\nprint(diffs(djiaval))\nprint(diffs(spval))\nprint(np.var(iss['Sentiment Score']))","d1985e15":"print(np.var(bbival)\/bbival.mean())\nprint(np.var(issval)\/issval.mean())\nprint(np.var(djiaval)\/djiaval.mean())\nprint(np.var(spval)\/spval.mean())","175dbbfb":"c=timeseries.corr()\n#c.to_csv(\"correlation_matrix.csv\")\nc","5ee15c29":"pd.plotting.scatter_matrix(timeseries, figsize=(6, 6))\nplt.savefig(\"Scatter_Matrix.svg\", format=\"svg\")","e2af4082":"prices['DJIA Z Scores'].hist()","0e86f7b9":"testprices=pd.read_csv(\"DJI.csv\",parse_dates=['Date'])\ntestprices['Close Z']=zscore(testprices['Close'])\ntestprices['Close Z'].hist()","d12580f0":"variables=[djiaval,spval,bbival,issval]\npvals=np.zeros((len(variables),len(variables)))\nfor i,n in zip(variables,range(0,len(variables))):\n    for j,k in zip(variables,range(0,len(variables))):\n        corr, p = sp.stats.pearsonr(i,j)\n        pvals[n][k]=p\n#np.savetxt(\"Correlation Coefficients P values.csv\", pvals, delimiter=\",\")\npd.DataFrame(pvals)","8002b40c":"# Plotting the Autocorrelation Function (ACF) for the indices\nfig, axs = plt.subplots(1,2,figsize=(15,5))\nprint(sm.graphics.tsa.plot_acf(djiaval,lags=50,ax=axs[0],title=\"DJIA\"))\nprint(sm.graphics.tsa.plot_acf(spval,lags=50,ax=axs[1],title=\"S&P 500\"))\n#plt.savefig(\"Correlograms.svg\", format=\"svg\")","a7409aaa":"lbvals,pvals=sm.stats.diagnostic.acorr_ljungbox(djiaval,4)\nnlags=5\npd.DataFrame({'DJIA p-values':sm.stats.diagnostic.acorr_ljungbox(djiaval,nlags)[1], 'SP500 p-values':sm.stats.diagnostic.acorr_ljungbox(spval,nlags)[1]},index=range(1,nlags+1))","777d632c":"fig, axs = plt.subplots(5,figsize=(10,30))\n\nlr=sk.linear_model.LinearRegression()\n        \ndef plotlinreg(x,y,n):\n    X=x.values.reshape(-1,1)\n    Y=y.values.reshape(-1,1)\n    lr.fit(X,Y)\n    axs[n].scatter(X,Y,color='black')\n    axs[n].plot(X,lr.predict(X),color='red')\n\nplotlinreg(bbival,djiaval,0)\naxs[0].set_title('BBI - DJIA')\nplotlinreg(bbival,spval,1)\naxs[1].set_title('BBI - SP500')\nplotlinreg(issval,djiaval,2)\naxs[2].set_title('ISS - DJIA')\nplotlinreg(issval,spval,3)\naxs[3].set_title('ISS - SP500')\nplotlinreg(issval,bbival,4)\naxs[4].set_title('ISS - BBI')\n\n#plt.savefig(\"Linear_Regressions.svg\", format=\"svg\")","8d3a876c":"def crosscorr(x,y,lag=0):\n    return x.corr(y.shift(lag))\nrs=[]\nlag=70\nf,ax=plt.subplots(2,figsize=(14,8))\n\n# DJIA BBI\nrs=[crosscorr(djiaval,bbival,lag) for lag in range(-lag,lag+1)]\noffset = np.ceil(len(rs)\/2)-np.argmax(rs)\nax[0].plot(rs,color='blue',label='DJIA')\nax[0].axvline(lag,color='k',linestyle='-',label='Center')\nax[0].axvline(np.argmax(rs),color='r',linestyle='--')\n\n# DJIA ISS\nrs=[crosscorr(djiaval,issval, lag) for lag in range(-lag,lag+1)]\noffset = np.ceil(len(rs)\/2)-np.argmax(rs)\nax[1].plot(rs,color='green',label='DJIA')\nax[1].axvline(lag,color='k',linestyle='-',label='Center')\nax[1].axvline(np.argmax(rs),color='r',linestyle='--')\n\n#SP500 BBI\nrs=[crosscorr(spval,bbival, lag) for lag in range(-lag,lag+1)]\noffset = np.ceil(len(rs)\/2)-np.argmax(rs)\nax[0].plot(rs,color='orange',label='SP500')\nax[0].axvline(np.argmax(rs),color='r',linestyle='--')\nprint(np.argmax(rs)-70)\n\n#SP500 ISS\nrs=[crosscorr(spval,issval, lag) for lag in range(-lag,lag+1)]\noffset = np.ceil(len(rs)\/2)-np.argmax(rs)\nax[1].plot(rs,color='purple',label='SP500')\nax[1].axvline(np.argmax(rs),color='r',linestyle='--')\nprint(np.argmax(rs)-70)\n\ntick_range=range(0,2*lag+1,10)\nlabel_range=range(-lag,lag+1,10)\nax[0].set_xticks(tick_range)\nax[0].set_xticklabels(label_range)\nax[1].set_xticks(tick_range)\nax[1].set_xticklabels(label_range)\nax[0].set_title('BBI')\nax[1].set_title('ISS')\nax[0].legend()\nax[1].legend()\n#plt.savefig(\"TLCC.svg\", format=\"svg\")","a82ec642":"crosscorr(djiaval,issval,34)","60e07a3c":"class StationarityTests:\n    def __init__(self, significance=.05):\n        self.SignificanceLevel = significance\n        self.pValue = None\n        self.isStationary = None\n    def ADF_Stationarity_Test(self, tslist):\n        pValues=[]\n        adf=pd.DataFrame()\n        names=[]\n        #Dickey-Fuller test:\n        for ts in tslist:\n            adfTest = sm.tsa.stattools.adfuller(ts, autolag='AIC')\n            self.pValue = adfTest[1]\n            pValues.append(self.pValue)\n            names.append(ts.name)\n        print(names)\n        adf['P values']=pValues\n        adf=adf.set_index(pd.Index(names))\n        return adf\n    \n    def KPSS_Stationarity_Test(self, tslist):\n        pValues=[]\n        kpss=pd.DataFrame()\n        names=[]\n        #Dickey-Fuller test:\n        for ts in tslist:\n            kpssTest = sm.tsa.stattools.kpss(ts, regression='c', store=False)\n            self.pValue = kpssTest[1]\n            pValues.append(self.pValue)\n            names.append(ts.name)\n        kpss['P values']=pValues\n        kpss=kpss.set_index(pd.Index(names))\n        return kpss","4ab1fe15":"tslist=[djiaval,spval,bbival,issval]\nsTest = StationarityTests()\nsTest.ADF_Stationarity_Test(tslist)","b650ebfc":"def diff(data):\n    diff_var=data[1:]-data.shift(1)[1:]\n    return diff_var\n\nsTest.ADF_Stationarity_Test([diff(djiaval),diff(spval),diff(bbival),diff(issval)])","c5ec2013":"sTest.KPSS_Stationarity_Test([diff(djiaval),diff(spval),bbival,diff(bbival),diff(issval)])","47ffe9be":"granger_ts=pd.DataFrame({'dDJIA':diff(djiaval),'dSP500':diff(spval),'dBBI':diff(bbival),'dISS':diff(issval)})\ngranger_ts=granger_ts.dropna(how='any')\ngranger_ts[['dDJIA','dISS']]","87e9de21":"sm.tsa.stattools.coint(granger_ts['dSP500'],granger_ts['dDJIA'], trend='c', method='aeg', autolag='aic')[1]","560671c6":"result=[]\nnlags=5\ngranger_ts=granger_ts.reset_index()\ndf_granger=pd.DataFrame()\ndf_granger['Lags']=range(1,nlags+1)\ndf_granger=df_granger.set_index('Lags')\nfor ix in ['dDJIA', 'dSP500']:\n    for metric in ['dBBI','dISS']:\n        pairs=[ix+\"-\"+metric+\" \"+'F']\n        result.append(pairs)\n        for start, end in zip(range(len(granger_ts)-199),range(199,len(granger_ts))):\n            granger=sm.tsa.stattools.grangercausalitytests(granger_ts[[ix,metric]].loc[start:end],maxlag=nlags,verbose=False)\n            vallist=[]\n            for lag in df_granger.index:\n                vallist.append(granger[lag][0]['ssr_ftest'][1])\n            pairs.append(vallist)\nresult","6dbb1640":"result[0][0]","e24bf44c":"fig, axs = plt.subplots(4,figsize=(10,30))\n\n#for axis, in [axs[0,0],axs[0,1],axs[1,0],axs[1,1]]\n#    axis.plot(X,lr.predict(X),color='red')\n\naxs[0].set_title('p-\u00e9rt\u00e9kek (dDJIA - dBBI)')\naxs[1].set_title('p-\u00e9rt\u00e9kek (dDJIA - dISS)')\naxs[2].set_title('p-\u00e9rt\u00e9kek (dSP500 - dBBI)')\naxs[3].set_title('p-\u00e9rt\u00e9kek (dSP500 - dISS)')\nfor pair in range(4):\n    for lag,color in zip(range(len(vallist)),['blue','red','orange','purple','green']):\n        lagvals=[]\n        for j in result[pair][1:]:\n            lagvals.append(j[lag])\n        axs[pair].plot(lagvals,color=color,label='Lag '+str(lag+1))\n    axs[pair].legend()\nplt.savefig(\"Window_Granger.svg\", format=\"svg\")","1b8fd5a1":"price_data=prices[prices.index.floor('T').isin(iss.index)]\ninput_data=pd.DataFrame()\ninput_data['DJIA']=price_data['DJIA CLOSE']\nnlags=10\nfor i in range(1,nlags+1):\n    input_data['DJIA-'+str(i)]=price_data['DJIA CLOSE'].shift(i)\n    input_data['ISS-'+str(i)]=iss['Sentiment Score'].shift(i)\ninput_data=input_data.dropna()\n#input_data=input_data.reset_index()\n#input_data=input_data.drop(columns='dates')\ninput_data.tail()","e851e36e":"#Defining metrics\ndef mean_absolute_percentage_error(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100\n\ndef mean_absolute_error(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred)))\n\ndef mean_square_error(y_true, y_pred): \n    return np.mean((y_true - y_pred)**2)\n\ndef root_mean_square_error(y_true, y_pred): \n    return np.sqrt(np.mean((y_true - y_pred)**2))\n\ndef directional_accuracy(y_true, y_pred):\n    success=0\n    for i,j,k,l in zip(list(y_true)[:-1],list(y_true)[1:],list(np.ndarray.tolist(y_pred)[0])[:-1],list(np.ndarray.tolist(y_pred)[0])[1:]):\n        if j-i > 0:\n            dir_test=1\n        elif j-i<0:\n            dir_test=-1\n        else:\n            dir_test=0\n        if l-k > 0:\n            dir_pred=1\n        elif l-k<0:\n            dir_pred=-1\n        else:\n            dir_pred=0\n        if dir_pred==dir_test:\n            success=success+1\n    return success\/len(list(y_true)[1:])\n\ndef trading_performance(y_curr, y_true, y_pred): \n    port_start=1000000\n    port=port_start\n    cost=0.0002\n    position='neutral'\n    for ycurr,ytrue,ypred in zip(y_curr,y_true,y_pred):\n        expr=(ypred-ycurr)\/ycurr\n        actr=(ytrue-ycurr)\/ycurr\n        # Trading\n        if expr > cost:\n            if position=='long': # Hold condition\n                port=(1+actr)*port\n                position='long'\n            elif position=='neutral': # Long condition\n                port=(1+actr-cost)*port\n                position='long'\n            elif position=='short': # Cover condition\n                port=(1-cost)*port\n                position='neutral'\n        elif expr < -cost:\n            if position=='long': # Sell condition\n                port=(1-cost)*port\n                position='neutral'\n            elif position=='neutral': # Short condition\n                port=(1-actr-cost)*port\n                position='short'\n            elif position=='short': # Hold condition\n                port=(1-actr)*port\n                position='short'\n        else: # Hold condition\n            if position=='long':\n                port=(1+actr)*port\n            elif position=='short':\n                port=(1-actr)*port\n        print(port)\n    return port\/port_start-1","5a343fd9":"scaler2 = MinMaxScaler()\nscaler2.fit_transform(y_act.reshape(-1, 1))\ny_pred=scaler2.inverse_transform(pred[0].reshape(-1, 1))\ntrading_performance(data_test[:,1].tolist(),y_test,invscaled_y_pred.tolist())","f64f4fe4":"mse_test=[]\nrmse_test=[]\nmae_test=[]\nmape_test=[]\nda_test=[]\ntradingperf_test=[]\n#Dimensions\nn = data.shape[0]\np = data.shape[1]\ndata = input_data.values    \n\n#Splitting to training and test data\ntrain_start = 0\ntrain_end = int(np.floor(0.8*n))\ntest_start = train_end\ntest_end = n\ndata_train = data[np.arange(train_start, train_end), :]\ndata_test = data[np.arange(test_start, test_end), :]\ny_curr=data_test[:,1]\ny_act=data_test[:,0]\n\n#Scaling data\nscaler = MinMaxScaler()\ndata_train = scaler.fit_transform(data_train)\ndata_test = scaler.transform(data_test)\n\n# Build X and y\nX_train = data_train[:, 1:]\ny_train = data_train[:, 0]\nX_test = data_test[:, 1:]\ny_test = data_test[:, 0]\n\n# Number of stocks in training data\nn_inputs = X_train.shape[1]\n\n# Neurons\nn_neurons_1 = 64\nn_neurons_2 = 32\nn_neurons_3 = 16\nn_neurons_4 = 8\nn_target = 1\n\n# Session\nnet = tf.InteractiveSession()\n\n# Placeholder\nX = tf.placeholder(dtype=tf.float32, shape=[None, n_inputs])\nY = tf.placeholder(dtype=tf.float32, shape=[None])\n\n# Initializers\nsigma = 1\nweight_initializer = tf.variance_scaling_initializer(mode=\"fan_avg\", distribution=\"uniform\", scale=sigma)\nbias_initializer = tf.zeros_initializer()\n\n# Hidden weights\nW_hidden_1 = tf.Variable(weight_initializer([n_inputs, n_neurons_1]))\nbias_hidden_1 = tf.Variable(bias_initializer([n_neurons_1]))\nW_hidden_2 = tf.Variable(weight_initializer([n_neurons_1, n_neurons_2]))\nbias_hidden_2 = tf.Variable(bias_initializer([n_neurons_2]))\nW_hidden_3 = tf.Variable(weight_initializer([n_neurons_2, n_neurons_3]))\nbias_hidden_3 = tf.Variable(bias_initializer([n_neurons_3]))\nW_hidden_4 = tf.Variable(weight_initializer([n_neurons_3, n_neurons_4]))\nbias_hidden_4 = tf.Variable(bias_initializer([n_neurons_4]))\n\n# Output weights\nW_out = tf.Variable(weight_initializer([n_neurons_4, 1]))\nbias_out = tf.Variable(bias_initializer([1]))\n\n# Hidden layer\nhidden_1 = tf.nn.relu(tf.add(tf.matmul(X, W_hidden_1), bias_hidden_1))\nhidden_2 = tf.nn.relu(tf.add(tf.matmul(hidden_1, W_hidden_2), bias_hidden_2))\nhidden_3 = tf.nn.relu(tf.add(tf.matmul(hidden_2, W_hidden_3), bias_hidden_3))\nhidden_4 = tf.nn.relu(tf.add(tf.matmul(hidden_3, W_hidden_4), bias_hidden_4))\n\n# Output layer (transpose!)\nout = tf.transpose(tf.add(tf.matmul(hidden_4, W_out), bias_out))\n\n# Cost function\nmse = tf.reduce_mean(tf.squared_difference(out, Y))\n\n# Optimizer\nopt = tf.train.AdamOptimizer().minimize(mse)\n\n# Init\nnet.run(tf.global_variables_initializer())\n\n# Fit neural net\nbatch_size = 5\nmse_train_l = []\nmse_test_l = []\n\n# Run\nepochs = 25\nfor e in range(epochs):\n\n    # Shuffle training data\n    shuffle_indices = np.random.permutation(np.arange(len(y_train)))\n    X_train = X_train[shuffle_indices]\n    y_train = y_train[shuffle_indices]\n\n    # Minibatch training\n    for i in range(0, len(y_train) \/\/ batch_size):\n        start = i * batch_size\n        batch_x = X_train[start:start + batch_size]\n        batch_y = y_train[start:start + batch_size]\n        # Run optimizer with batch\n        net.run(opt, feed_dict={X: batch_x, Y: batch_y})\n\n        # Show progress\n        if np.mod(i, 5) == 0:\n            # MSE train and test\n            mse_train_l.append(net.run(mse, feed_dict={X: X_train, Y: y_train}))\n            mse_test_l.append(net.run(mse, feed_dict={X: X_test, Y: y_test}))\n            #print('MSE Train: ', mse_train_l[-1])\n            #print('MSE Test: ', mse_test_l[-1])\n            # Prediction\n            pred = net.run(out, feed_dict={X: X_test})\n            plt.pause(0.001)\n\n# Metrics\nprint(\"Calculating Metrics\")\nprint(p)\n#print(data_test[:,1])\nmse_test.append(mean_square_error(y_test,pred))\nrmse_test.append(root_mean_square_error(y_test,pred))\nmae_test.append(mean_absolute_error(y_test,pred))\nmape_test.append(mean_absolute_percentage_error(y_test,pred))\nda_test.append(directional_accuracy(y_test,pred))\nscaler2 = MinMaxScaler()\nscaler2.fit_transform(y_act.reshape(1, -1))\ny_pred=scaler2.inverse_transform(pred[0].reshape(1, -1))\nprint(y_pred[0])\nprint(tradingperf_test)\ntrading_performance(y_curr.tolist(),y_act,y_pred[0])","ed785402":"trading_performance(y_curr.tolist(),y_act,y_curr.tolist())","67eb8569":"def run_nn(input_data, modeltype):\n    \n    global pred, y_test, X_test, y_act, y_curr, mse_train_l, mse_test_l\n    \n    mse_test=[]\n    rmse_test=[]\n    mae_test=[]\n    mape_test=[]\n    da_test=[]\n    tradingperf_test=[]\n    if modeltype=='u': # Unrestricted model\n        step=2\n    else: # Restricted model\n        step=1\n    for i in range(0,len(input_data.columns)-1,step):\n        if i==0:\n            data=input_data.iloc[:,:]\n        else:\n            data=input_data.iloc[:,:-i]\n        \n        #Dimensions\n        n = data.shape[0]\n        p = data.shape[1]\n        data = data.values    \n\n        #Splitting to training and test data\n        train_start = 0\n        train_end = int(np.floor(0.8*n))\n        test_start = train_end\n        test_end = n\n        data_train = data[np.arange(train_start, train_end), :]\n        data_test = data[np.arange(test_start, test_end), :]\n        y_curr=data_test[:,1]\n        y_act=data_test[:,0]\n\n        #Scaling data\n        scaler = MinMaxScaler()\n        data_train = scaler.fit_transform(data_train)\n        data_test = scaler.transform(data_test)\n\n        # Build X and y\n        X_train = data_train[:, 1:]\n        y_train = data_train[:, 0]\n        X_test = data_test[:, 1:]\n        y_test = data_test[:, 0]\n\n        # Number of stocks in training data\n        n_inputs = X_train.shape[1]\n\n        # Neurons\n        n_neurons_1 = 64\n        n_neurons_2 = 32\n        n_neurons_3 = 16\n        n_neurons_4 = 8\n        n_target = 1\n\n        # Session\n        net = tf.InteractiveSession()\n\n        # Placeholder\n        X = tf.placeholder(dtype=tf.float32, shape=[None, n_inputs])\n        Y = tf.placeholder(dtype=tf.float32, shape=[None])\n\n        # Initializers\n        sigma = 1\n        weight_initializer = tf.variance_scaling_initializer(mode=\"fan_avg\", distribution=\"uniform\", scale=sigma)\n        bias_initializer = tf.zeros_initializer()\n\n        # Hidden weights\n        W_hidden_1 = tf.Variable(weight_initializer([n_inputs, n_neurons_1]))\n        bias_hidden_1 = tf.Variable(bias_initializer([n_neurons_1]))\n        W_hidden_2 = tf.Variable(weight_initializer([n_neurons_1, n_neurons_2]))\n        bias_hidden_2 = tf.Variable(bias_initializer([n_neurons_2]))\n        W_hidden_3 = tf.Variable(weight_initializer([n_neurons_2, n_neurons_3]))\n        bias_hidden_3 = tf.Variable(bias_initializer([n_neurons_3]))\n        W_hidden_4 = tf.Variable(weight_initializer([n_neurons_3, n_neurons_4]))\n        bias_hidden_4 = tf.Variable(bias_initializer([n_neurons_4]))\n\n        # Output weights\n        W_out = tf.Variable(weight_initializer([n_neurons_4, 1]))\n        bias_out = tf.Variable(bias_initializer([1]))\n\n        # Hidden layer\n        hidden_1 = tf.nn.relu(tf.add(tf.matmul(X, W_hidden_1), bias_hidden_1))\n        hidden_2 = tf.nn.relu(tf.add(tf.matmul(hidden_1, W_hidden_2), bias_hidden_2))\n        hidden_3 = tf.nn.relu(tf.add(tf.matmul(hidden_2, W_hidden_3), bias_hidden_3))\n        hidden_4 = tf.nn.relu(tf.add(tf.matmul(hidden_3, W_hidden_4), bias_hidden_4))\n\n        # Output layer (transpose!)\n        out = tf.transpose(tf.add(tf.matmul(hidden_4, W_out), bias_out))\n\n        # Cost function\n        mse = tf.reduce_mean(tf.squared_difference(out, Y))\n\n        # Optimizer\n        opt = tf.train.AdamOptimizer().minimize(mse)\n\n        # Init\n        net.run(tf.global_variables_initializer())\n\n        # Fit neural net\n        batch_size = 5\n        mse_train_l = []\n        mse_test_l = []\n\n        # Run\n        epochs = 25\n        for e in range(epochs):\n\n            # Shuffle training data\n            shuffle_indices = np.random.permutation(np.arange(len(y_train)))\n            X_train = X_train[shuffle_indices]\n            y_train = y_train[shuffle_indices]\n\n            # Minibatch training\n            for i in range(0, len(y_train) \/\/ batch_size):\n                start = i * batch_size\n                batch_x = X_train[start:start + batch_size]\n                batch_y = y_train[start:start + batch_size]\n                # Run optimizer with batch\n                net.run(opt, feed_dict={X: batch_x, Y: batch_y})\n\n                # Show progress\n                if np.mod(i, 5) == 0:\n                    # MSE train and test\n                    mse_train_l.append(net.run(mse, feed_dict={X: X_train, Y: y_train}))\n                    mse_test_l.append(net.run(mse, feed_dict={X: X_test, Y: y_test}))\n                    #print('MSE Train: ', mse_train_l[-1])\n                    #print('MSE Test: ', mse_test_l[-1])\n                    # Prediction\n                    pred = net.run(out, feed_dict={X: X_test})\n                    plt.pause(0.001)\n\n        # Metrics\n        print(\"Calculating Metrics\")\n        print(p)\n        mse_test.append(mean_square_error(y_test,pred))\n        rmse_test.append(root_mean_square_error(y_test,pred))\n        mae_test.append(mean_absolute_error(y_test,pred))\n        mape_test.append(mean_absolute_percentage_error(y_test,pred))\n        da_test.append(directional_accuracy(y_test,pred))\n        scaler2 = MinMaxScaler()\n        scaler2.fit_transform(y_act.reshape(-1, 1))\n        y_pred=scaler2.inverse_transform(pred[0].reshape(1, -1))\n        tradingperf_test.append(trading_performance(y_curr.tolist(),y_act,y_pred[0]))\n        print(pred[0])\n        print(y_pred[0])\n        print(tradingperf_test)\n        \n    return list(zip(mae_test,mape_test,mse_test,rmse_test,da_test,tradingperf_test))","8246c5b0":"nn_performance=pd.DataFrame(run_nn(input_data,'u'),columns =['MAE','MAPE','MSE','RMSE','Dir. Acc.','Tr. Perf.'])\ncols = [c for c in input_data.columns if c[:3] != 'ISS']\ndata=input_data[cols]\nnn_performance_naive=pd.DataFrame(run_nn(data,'r'),columns =['MAE','MAPE','MSE','RMSE','Dir. Acc.','Tr. Perf.'])\nnn_performance=nn_performance.append(nn_performance_naive)\nindex_range=['U'+str(i) for i in range(10,0,-1)]+['R'+str(i) for i in range(10,0,-1)]\nnn_performance.set_index(pd.Index(index_range))","22231f35":"plt.plot(range(1,len(y_test)+1), y_test,label='Test')\nplt.plot(range(1,len(y_test)+1),pred[0],label='Prediction')\nplt.legend()\nplt.savefig(\"NN_test_pred.svg\", format=\"svg\")","f0a00fe7":"plt.plot(range(1,len(mse_train_l[30:])+1), mse_train_l[30:],label='MSE - tan\u00edt\u00f3 k\u00e9szlet')\nplt.plot(range(1,len(mse_test_l[30:])+1),mse_test_l[30:],label='MSE - tesztel\u0151 k\u00e9szlet')\nplt.legend()\nplt.title('R1-es modell MSE')\nplt.savefig(\"NN_test_pred.svg\", format=\"svg\")","5f5d3d18":"price_data=prices[prices.index.floor('T').isin(iss.index)]\ninput_data=pd.DataFrame()\ninput_data['SP500']=price_data['SP500 CLOSE']\nnlags=10\nfor i in range(1,nlags+1):\n    input_data['SP500-'+str(i)]=price_data['SP500 CLOSE'].shift(i)\n    input_data['ISS-'+str(i)]=iss['Sentiment Score'].shift(i)\ninput_data=input_data.dropna()\ninput_data","2b53e15d":"nn_performance=pd.DataFrame(run_nn(input_data,'u'),columns =['MAE','MAPE','MSE','RMSE','Dir. Acc.','Tr. Perf.'])\ncols = [c for c in input_data.columns if c[:3] != 'ISS']\ndata=input_data[cols]\nnn_performance_naive=pd.DataFrame(run_nn(data,'r'),columns =['MAE','MAPE','MSE','RMSE','Dir. Acc.','Tr. Perf.'])\nnn_performance=nn_performance.append(nn_performance_naive)\nindex_range=['U'+str(i) for i in range(10,0,-1)]+['R'+str(i) for i in range(10,0,-1)]\nnn_performance.set_index(pd.Index(index_range))","3b05d981":"trading_performance(y_curr.tolist(),y_act,y_act)","06a6e49c":"### Granger Casuality Analysis","b181a7eb":"### Plotting results","e64bb9dc":"#### Regression Models","81e553bc":"#### Assigning sentiment scores to Loughran & McDonald lexicons and merging lexicons","c9d96ebe":"### Stock Index data","757b9ee2":"#### Loading and scaling a Stock Market Lexicon created by Oliveira, Nuno, Paulo Cortez, and Nelson Areal","89ab06bc":"# Data cleaning and preparation of tweets","e136ba07":"### Creating timewindows","2cda5092":"### Loading NLTK's VADER Sentiment Intensity Analyzer and merging with other lexicons for calculating the ISS (Investor Sentiment Score)","6d5dd4ea":"#### DJIA Neural network","9ad7ea8c":"#### SP500 Neural network","aa256e71":"### Creating Wordcloud","8be43625":"### Pearson Correlation Coefficients, Correlation matrix","98c6312e":"### Time Lagged Cross Correlation Analysis","e35248f2":"### Autocorrelation Analysis","b628b771":"#### Loading and scaling the Loughran & McDonald lexicons created by Tim Loughran and Bill McDonald","5be07f82":"### Tweet data","0325f676":"### Calculating the BBI (Bullish \/ Bearish Index)","266fd9b2":"#### Ljung-Box Test","26798bc8":"#### KPSS test","ce218336":"#### Testing for cointegration","80480c7d":"### Neural Network Prediction Model","94ce04c0":"#### Testing timeseries for stationarity"}}