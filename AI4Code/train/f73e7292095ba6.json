{"cell_type":{"70f00119":"code","cb8a36ad":"code","6f79b175":"code","01c55e98":"code","f862542f":"code","d9060f23":"code","5856efaf":"code","0db465d7":"code","1ccc8e3d":"code","28f24c92":"code","c8a811d3":"code","da1b81b6":"markdown"},"source":{"70f00119":"#from google.colab import drive\n#drive.mount(\"\/content\/drive\")","cb8a36ad":"#%tensorflow_version 1.7\n#!python -m pip install --upgrade pip\n\n#!pip install Keras==2.1.6\n#import tensorflow as tf\n#print(tf.version)\n#from tensorflow.python.client import device_lib\n#print(device_lib.list_local_devices())\n#sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n","6f79b175":"    num_labeled_train = 250\n    ramp_up_period = 80\n    ramp_down_period = 170\n    weight_max = 30\n    learning_rate = 0.00035\n    pseudo_labeling=False\n    involvement_wieght=False\n    label_corruption=False\n    corruption_rate=0.3\n    involvment_time=100","01c55e98":"from keras.layers import Input, Conv2D, MaxPooling2D, LeakyReLU, Dropout, GlobalAveragePooling2D, Dense, concatenate, \\\n    BatchNormalization\nfrom keras.layers.noise import GaussianNoise\nfrom keras.layers.core import Activation\nfrom keras.models import Model\n\ndef build_modelBN(num_class):\n    input_img = Input(shape=(32, 32, 3))\n    supervised_label = Input(shape=(10,))\n    supervised_flag = Input(shape=(1,))\n    unsupervised_weight = Input(shape=(1,))\n\n    kernel_init = 'he_normal'\n\n    net = GaussianNoise(stddev=0.15)(input_img)\n\n    net = Conv2D(128, (3, 3), activation=None, padding='same', kernel_initializer=kernel_init)(net)\n    net = BatchNormalization()(net)\n    net = LeakyReLU(alpha=0.1)(net)\n    net = Conv2D(128, (3, 3), activation=None, padding='same', kernel_initializer=kernel_init)(net)\n    net = BatchNormalization()(net)\n    net = LeakyReLU(alpha=0.1)(net)\n    net = Conv2D(128, (3, 3), activation=None, padding='same', kernel_initializer=kernel_init)(net)\n    net = BatchNormalization()(net)\n    net = LeakyReLU(alpha=0.1)(net)\n    net = MaxPooling2D((2, 2), padding='same')(net)\n    net = Dropout(rate=0.5)(net)\n\n    net = Conv2D(256, (3, 3), activation=None, padding='same', kernel_initializer=kernel_init)(net)\n    net = BatchNormalization()(net)\n    net = LeakyReLU(alpha=0.1)(net)\n    net = Conv2D(256, (3, 3), activation=None, padding='same', kernel_initializer=kernel_init)(net)\n    net = BatchNormalization()(net)\n    net = LeakyReLU(alpha=0.1)(net)\n    net = Conv2D(256, (3, 3), activation=None, padding='same', kernel_initializer=kernel_init)(net)\n    net = BatchNormalization()(net)\n    net = LeakyReLU(alpha=0.1)(net)\n    net = MaxPooling2D((2, 2), padding='same')(net)\n    net = Dropout(rate=0.5)(net)\n\n    net = Conv2D(512, (3, 3), activation=None, padding='valid', kernel_initializer=kernel_init)(net)\n    net = BatchNormalization()(net)\n    net = LeakyReLU(alpha=0.1)(net)\n    net = Conv2D(256, (1, 1), activation=None, padding='valid', kernel_initializer=kernel_init)(net)\n    net = BatchNormalization()(net)\n    net = LeakyReLU(alpha=0.1)(net)\n    net = Conv2D(128, (1, 1), activation=None, padding='valid', kernel_initializer=kernel_init)(net)\n    net = BatchNormalization()(net)\n    net = LeakyReLU(alpha=0.1)(net)\n\n    net = GlobalAveragePooling2D()(net)\n    net = Dense(units=num_class, activation=None, kernel_initializer=kernel_init)(net)\n    net = BatchNormalization()(net)\n    net = Activation('softmax')(net)\n\n    # concate label\n    net = concatenate([net, supervised_label, supervised_flag, unsupervised_weight])\n\n    # pred(num_class), unsupervised_target(num_class), supervised_label(num_class), supervised_flag(1), unsupervised_weight(1)\n    return Model([input_img, supervised_label, supervised_flag, unsupervised_weight], net)","f862542f":"from keras.layers import Input, Conv2D, MaxPooling2D, LeakyReLU, Dropout, GlobalAveragePooling2D, Dense, concatenate\nfrom keras.layers.noise import GaussianNoise\nfrom keras.layers.core import Activation\n\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\n\nfrom keras import initializers\nfrom keras import regularizers\nfrom keras import constraints\n\n\nclass MeanOnlyBatchNormalization(Layer):\n    def __init__(self,\n                 momentum=0.999,\n                 moving_mean_initializer='zeros',\n                 axis=-1,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.momentum = momentum\n        self.moving_mean_initializer = moving_mean_initializer\n        self.axis = axis\n\n    def build(self, input_shape):\n        dim = input_shape[self.axis]\n        shape = (dim,)\n\n        self.moving_mean = self.add_weight(\n            shape=shape,\n            name='moving_mean',\n            initializer=self.moving_mean_initializer,\n            trainable=False)\n\n        super().build(input_shape)\n\n    def call(self, inputs, training=None):\n        input_shape = K.int_shape(inputs)\n        # Prepare broadcasting shape.\n        reduction_axes = list(range(len(input_shape)))\n        del reduction_axes[self.axis]\n\n        # inference\n        def normalize_inference():\n            return inputs - self.moving_mean\n\n        if training in {0, False}:\n            return normalize_inference()\n\n        mean = K.mean(inputs, axis=reduction_axes)\n        normed_training = inputs - mean\n\n        self.add_update(K.moving_average_update(self.moving_mean, mean,\n                                                self.momentum), inputs)\n\n        return K.in_train_phase(normed_training,\n                                normalize_inference,\n                                training=training)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n\nclass Bias(Layer):\n    def __init__(self,\n                 filters,\n                 data_format=None,\n                 bias_initializer='zeros',\n                 bias_regularizer=None,\n                 bias_constraint=None,\n                 **kwargs\n                 ):\n        self.filters = filters\n        self.data_format = data_format\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.bias_constraint = constraints.get(bias_constraint)\n        super().__init__(**kwargs)\n\n    def build(self, input_shape):\n        if self.data_format == 'channels_first':\n            channel_axis = 1\n        else:\n            channel_axis = -1\n        if input_shape[channel_axis] is None:\n            raise ValueError('The channel dimension of the inputs '\n                             'should be defined. Found `None`.')\n\n        self.bias = self.add_weight(shape=(self.filters,),\n                                    initializer=self.bias_initializer,\n                                    name='bias',\n                                    regularizer=self.bias_regularizer,\n                                    constraint=self.bias_constraint)\n        super().build(input_shape)\n\n    def call(self, inputs, **kwargs):\n        outputs = K.bias_add(\n            inputs,\n            self.bias,\n            data_format=self.data_format)\n\n        return outputs\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n\ndef WN_Conv2D(net, filters=None, kernel_size=None, padding='same', kernel_initializer='he_norml'):\n    \"\"\"Convolution layer with Weight Normalization + Mean-Only BatchNormalization\"\"\"\n    net = Conv2D(filters, kernel_size, activation=None, padding=padding, kernel_initializer=kernel_initializer,\n                 use_bias=False)(net)\n    net = MeanOnlyBatchNormalization()(net)\n    net = Bias(filters)(net)\n    net = LeakyReLU(alpha=0.1)(net)\n\n    return net\n\n\ndef WN_Dense(net, units=None, kernel_initializer='he_norml'):\n    \"\"\"Dense layer with Weight Normalization + Mean-Only BatchNormalization\"\"\"\n    net = Dense(units=units, activation=None, kernel_initializer=kernel_initializer, use_bias=False)(net)\n    net = MeanOnlyBatchNormalization()(net)\n    net = Bias(units)(net)\n    net = Activation('softmax')(net)\n\n    return net\n\n\ndef build_model(num_class):\n    input_img = Input(shape=(32, 32, 3))\n    supervised_label = Input(shape=(10,))\n    supervised_flag = Input(shape=(1,))\n    unsupervised_weight = Input(shape=(1,))\n\n    kernel_init = 'he_normal'\n\n    net = GaussianNoise(stddev=0.15)(input_img)\n\n    net = WN_Conv2D(net, 128, (3, 3), padding='same', kernel_initializer=kernel_init)\n    net = WN_Conv2D(net, 128, (3, 3), padding='same', kernel_initializer=kernel_init)\n    net = WN_Conv2D(net, 128, (3, 3), padding='same', kernel_initializer=kernel_init)\n    net = MaxPooling2D((2, 2), padding='same')(net)\n    net = Dropout(rate=0.5)(net)\n\n    net = WN_Conv2D(net, 256, (3, 3), padding='same', kernel_initializer=kernel_init)\n    net = WN_Conv2D(net, 256, (3, 3), padding='same', kernel_initializer=kernel_init)\n    net = WN_Conv2D(net, 256, (3, 3), padding='same', kernel_initializer=kernel_init)\n    net = MaxPooling2D((2, 2), padding='same')(net)\n    net = Dropout(rate=0.5)(net)\n\n    net = WN_Conv2D(net, 512, (3, 3), padding='valid', kernel_initializer=kernel_init)\n    net = WN_Conv2D(net, 256, (1, 1), padding='valid', kernel_initializer=kernel_init)\n    net = WN_Conv2D(net, 128, (1, 1), padding='valid', kernel_initializer=kernel_init)\n    net = GlobalAveragePooling2D()(net)\n    net = WN_Dense(net, units=num_class, kernel_initializer=kernel_init)\n\n    # concatenate label\n    net = concatenate([net, supervised_label, supervised_flag, unsupervised_weight])\n\n    # pred(num_class), unsupervised_target(num_class), supervised_label(num_class), supervised_flag(1), unsupervised_weight(1)\n    return Model([input_img, supervised_label, supervised_flag, unsupervised_weight], net)","d9060f23":"from keras import backend as K\nfrom keras.losses import mean_squared_error\nimport numpy as np\n\n\ndef ramp_up_weight(ramp_period, weight_max):\n    \"\"\"Ramp-Up weight generator.\n    The function is used in unsupervised component of loss.\n    Returned weight ramps up until epoch reaches ramp_period\n    \"\"\"\n    cur_epoch = 0\n\n    while True:\n        if cur_epoch <= ramp_period - 1:\n            T = (1 \/ (ramp_period - 1)) * cur_epoch\n            yield np.exp(-5 * (1 - T) ** 2) * weight_max\n        else:\n            yield 1 * weight_max\n\n        cur_epoch += 1\n\n\ndef ramp_down_weight(ramp_period):\n    \"\"\"Ramp-Down weight generator\"\"\"\n    cur_epoch = 1\n\n    while True:\n        if cur_epoch <= ramp_period - 1:\n            T = (1 \/ (ramp_period - 1)) * cur_epoch\n            yield np.exp(-12.5 * T ** 2)\n        else:\n            yield 0\n\n        cur_epoch += 1\n\n\ndef semi_supervised_loss(num_class):\n    \"\"\"custom loss function\"\"\"\n    epsilon = 1e-08\n\n    def loss_func(y_true, y_pred):\n        \"\"\"semi-supervised loss function\n        the order of y_true:\n        unsupervised_target(num_class), supervised_label(num_class), supervised_flag(1), unsupervised weight(1)\n        \"\"\"\n        unsupervised_target = y_true[:, 0:num_class]\n        supervised_label = y_true[:, num_class:num_class * 2]\n        supervised_flag = y_true[:, num_class * 2]\n        weight = y_true[0, -1]\n\n        model_pred = y_pred[:, 0:num_class]\n\n        # weighted unsupervised loss over batchsize\n        unsupervised_loss = weight * K.mean(mean_squared_error(unsupervised_target, model_pred))\n\n        # To sum over only supervised data on categorical_crossentropy, supervised_flag(1\/0) is used\n        supervised_loss = - K.mean(\n            K.sum(supervised_label * K.log(K.clip(model_pred, epsilon, 1.0 - epsilon)), axis=1) * supervised_flag)\n\n        return supervised_loss + unsupervised_loss\n\n    return loss_func\n\n\ndef update_weight(y, unsupervised_weight, next_weight):\n    \"\"\"update weight of the unsupervised part of loss\"\"\"\n    y[:, -1] = next_weight\n    unsupervised_weight[:] = next_weight\n\n    return y, unsupervised_weight\n\n\ndef update_unsupervised_target(ensemble_prediction, y, num_class, alpha, cur_pred, epoch):\n    \"\"\"update ensemble_prediction and unsupervised weight when an epoch ends\"\"\"\n    # Z = \u03b1Z + (1 - \u03b1)z\n    ensemble_prediction = alpha * ensemble_prediction + (1 - alpha) * cur_pred\n\n    # initial_epoch = 0\n    y[:, 0:num_class] = ensemble_prediction \/ (1 - alpha ** (epoch + 1))\n\n    return ensemble_prediction, y\n\n\ndef evaluate(model, num_class, num_test, test_x, test_y):\n    \"\"\"evaluate\"\"\"\n    test_supervised_label_dummy = np.zeros((num_test, num_class))\n    test_supervised_flag_dummy = np.zeros((num_test, 1))\n    test_unsupervised_weight_dummy = np.zeros((num_test, 1))\n\n    test_x_ap = [test_x, test_supervised_label_dummy, test_supervised_flag_dummy, test_unsupervised_weight_dummy]\n    p = model.predict(x=test_x_ap)\n    pr = p[:, 0:num_class]\n    pr_arg_max = np.argmax(pr, axis=1)\n    tr_arg_max = np.argmax(test_y, axis=1)\n    cnt = np.sum(pr_arg_max == tr_arg_max)\n    print('Test Accuracy: ', cnt \/ num_test, flush=True)\n","5856efaf":"import os\nif not os.path.exists('\/content\/cifar-10-batches-py'):\n  !wget http:\/\/www.cs.toronto.edu\/~kriz\/cifar-10-python.tar.gz\n  !tar -xvf cifar-10-python.tar.gz\nif not os.path.exists('\/content\/train_32x32.mat'):\n  !wget http:\/\/ufldl.stanford.edu\/housenumbers\/train_32x32.mat\n  !wget http:\/\/ufldl.stanford.edu\/housenumbers\/test_32x32.mat","0db465d7":"import glob\nimport pickle\n\nimport cv2\nimport numpy as np\n\n# The directory you downloaded CIFAR-10\n# You can download cifar10 data via https:\/\/www.kaggle.com\/janzenliu\/cifar-10-batches-py\ndata_dir = ''\nimport tarfile\n\n\nall_files = glob.glob(data_dir + '.\/cifar-10-batches-py\/*')\n\n\n\n\n\ndef unpickle(file):\n    with open(file, 'rb') as fo:\n        dict = pickle.load(fo, encoding='bytes')\n    return dict\n\n\n# labal\n# 0:airplane, 1:automobile, 2:bird. 3:cat, 4:deer, 5:dog, 6:frog, 7:horse, 8:ship, 9:truck\nall_image = []\nall_label = []\ntest_images=[]\ntest_labels=[]\nprint(all_files)\n\nfor file in all_files:\n    #print(file)\n\n    if file=='.\/cifar-10-batches-py\/readme.html' or file=='.\/cifar-10-batches-py\/batches.meta':\n        continue\n    ret = unpickle(file)\n    k=None\n    for key,_ in ret.items():\n      if \"data\" in str(key):\n     \n        k=key\n        break\n      #print(key, ret[key])\n    print(k)\n    if file=='.\/cifar-10-batches-py\/test_batch':\n        for i, arr in enumerate(ret[b'data']):\n            img = np.reshape(arr, (3, 32, 32))\n            img = img.transpose(1, 2, 0)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            test_images.append(img)\n            test_labels.append(ret[b'labels'][i])\n    else:\n        for i, arr in enumerate(ret[b'data']):\n            img = np.reshape(arr, (3, 32, 32))\n            img = img.transpose(1, 2, 0)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            all_image.append(img)\n            all_label.append(ret[b'labels'][i])        \n\nall_images = np.array(all_image)\nall_labels = np.array(all_label)\ntest_images = np.array(test_images)\ntest_labels = np.array(test_labels)\nprint(all_images.shape)\nprint(test_images.shape)\nnp.savez('cifar10.npz', train_x=all_images, train_y=all_labels, test_x=test_images, test_y=test_labels)\n","1ccc8e3d":"import numpy as np\nfrom keras.preprocessing.image import ImageDataGenerator\nimport sklearn\nfrom sklearn.preprocessing import LabelBinarizer\nfrom scipy.io import loadmat\n\n\ndef load_data(data_to_path):\n    \"\"\"load data\n    data should be compressed in npz. return arrays\n    \"\"\"\n    data = np.load(data_to_path)\n    for k in data.files:\n        print(k)\n    return data['train_x'], data['train_y'], data['test_x'], data['test_y']\ndef load_SVHN():\n      \n    train_raw = loadmat('train_32x32.mat')\n    test_raw = loadmat('test_32x32.mat')\n\n    train_images = np.array(train_raw['X'])\n    test_images = np.array(test_raw['X'])\n\n    train_labels = train_raw['y']\n    test_labels = test_raw['y']\n\n    train_images = np.moveaxis(train_images, -1, 0)\n    test_images = np.moveaxis(test_images, -1, 0)\n\n    input_train = train_images.astype('float64')\n    input_test = test_images.astype('float64')\n    # Convert train and test labels into 'int64' type\n\n    train_labels = train_labels.astype('int64')\n    test_labels = test_labels.astype('int64')\n\n\n    #lb = LabelBinarizer()\n    #target_train = lb.fit_transform(train_labels)\n    #target_test = lb.fit_transform(test_labels)\n    return train_images, np.squeeze(train_labels), test_images, np.squeeze(test_labels)\ndef _cal_num_each_train_labal(num_labeled_train, category):\n    \"\"\"calculate the number of data of each class from the whole labeled data size and the number of class\"\"\"\n    return int(num_labeled_train \/ len(category))\n\n\ndef split_supervised_train(images, labels, num_labeled_train):\n    \"\"\"get data for supervised training from the whole data.\n    This function returns labeled train data and these remainings separetely.\n    \"\"\"\n    # list of unique category in labels\n    assert labels.ndim == 1, \"labels should be 1-dim array.\"\n\n    category = np.unique(labels)\n    num_each_label = _cal_num_each_train_labal(num_labeled_train, category)\n\n    train_labeled_idx = []\n    for cat in category:\n        cat_idx = np.where(labels == cat)[0]\n        train_labeled_idx.extend(np.random.choice(cat_idx, num_each_label, replace=False))\n\n    # difference set between all-data indices and selected labeled data indices\n    diff_set = list(np.setdiff1d(np.arange(labels.shape[0]), np.array(train_labeled_idx)))\n    pseudo_set = np.random.choice(diff_set, int(len(diff_set)*0.3), replace=False)\n    if(label_corruption):\n        labels[pseudo_set]= ((labels[pseudo_set]-1 - np.random.randint(1,len(category) -1,size=len(pseudo_set))) % len(category))+1\n    \n    \n    \n    return {\n        'labeled_x': images[train_labeled_idx],\n        'labeled_y': labels[train_labeled_idx],\n        'unlabeled_x': images[diff_set],\n        'psudo_labled_y': labels[diff_set]\n    }\n\n\ndef make_train_test_dataset(inp_dic, num_class):\n    \"\"\"make train dataset and test dataset\"\"\"\n    train_x = np.concatenate((inp_dic['labeled_x'], inp_dic['unlabeled_x']), axis=0)\n    print(num_class)\n    # transform categorical labels to one-hot vectors\n    supervised_label = to_categorical_onehot(np.concatenate((inp_dic['labeled_y'],inp_dic['psudo_labled_y'])), num_class)\n    \n    test_y = to_categorical_onehot(inp_dic['test_y'], num_class)\n    num_train_unlabeled = inp_dic['unlabeled_x'].shape[0]\n\n    # fill dummy 0 array and the size will corresponds to train dataset at axis 0\n    #supervised_label = np.concatenate((supervised_label, np.zeros((num_train_unlabeled, num_class))), axis=0)\n    num_train_data = supervised_label.shape[0]\n\n    # flag to indicate that supervised(1) or not(0) in train data\n    supervised_flag = np.array([1] * (num_train_data - num_train_unlabeled) +\n                               [int(label_corruption or involvement_wieght or pseudo_labeling)] * num_train_unlabeled)[:, np.newaxis]\n\n\n    # initialize ensemble prediction label for unsupervised component. It corresponds to matrix Z\n    unsupervised_target = np.zeros((num_train_data, num_class))\n\n    # initialize weight of unsupervised loss component\n    unsupervised_weight = np.zeros((num_train_data, 1))\n\n    del inp_dic['labeled_x'], inp_dic['labeled_y'], inp_dic['unlabeled_x']\n    inp_dic['train_x'] = train_x\n    inp_dic['supervised_label'] = supervised_label\n    inp_dic['unsupervised_target'] = unsupervised_target\n    inp_dic['train_sup_flag'] = supervised_flag\n    inp_dic['unsupervised_weight'] = unsupervised_weight\n    inp_dic['test_y'] = test_y\n\n    return inp_dic\n\n\ndef data_augmentation_tempen(inputs, trans_range):\n    \"\"\"data augmentation by random translation and horizonal flip.\n    This implementation refers to the author's implementation of the paper.\n    \"\"\"\n    temp_lst = []\n    for img in inputs:\n        #if np.random.uniform() > 0.5:\n            # horizonal flip. NHWC\n         #   img = img[:, ::-1, :]\n\n        p0 = np.random.randint(-trans_range, trans_range + 1) + trans_range\n        p1 = np.random.randint(-trans_range, trans_range + 1) + trans_range\n\n        img = img[p0:p0 + 32, p1:p1 + 32, :]\n        temp_lst.append(img)\n\n    return np.array(temp_lst)\n\n\ndef to_categorical_onehot(label, num_class):\n    \"\"\"transform categorical labels to one-hot vectors\"\"\"\n    lb = LabelBinarizer()\n    #target_train = lb.fit_transform(train_labels)\n    #target_test = lb.fit_transform(test_labels)\n    #return np.identity(num_class)[label]\n    return lb.fit_transform(label)\n\n\ndef normalize_images(*arrays):\n    \"\"\"normalize all input arrays by dividing 255\"\"\"\n    #return [arr \/ 255 for arr in arrays]\n    return [(arr-arr.mean())\/arr.std() for arr in arrays]\n\n\ndef whiten_zca(x_train, x_test):\n    \"\"\"whiten train and test data by zca whitening\"\"\"\n    zca_gen = ImageDataGenerator(zca_whitening=True)\n    zca_gen.fit(x_train)\n\n    g_train = zca_gen.flow(x_train, batch_size=len(x_train), shuffle=False)\n    g_test = zca_gen.flow(x_test, batch_size=len(x_test), shuffle=False)\n\n    x_train = g_train.next()\n    x_test = g_test.next()\n\n    return x_train, x_test","28f24c92":"\nfrom keras import backend as K\nfrom keras.optimizers import SGD,Adam\nimport tensorflow as tf\n\n\n# adapted from keras.optimizers.Adam\nclass AdamWithWeightnorm(Adam):\n    # def get_updates(self, params, constraints, loss):\n    def get_updates(self, params, loss):\n        grads = self.get_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n\n        lr = self.lr\n        if self.initial_decay > 0:\n            lr *= (1. \/ (1. + self.decay * K.cast(self.iterations,\n                                                  K.dtype(self.decay))))\n\n        t = K.cast(self.iterations, K.floatx()) + 1\n        lr_t = lr * K.sqrt(1. - K.pow(self.beta_2, t)) \/ (1. - K.pow(self.beta_1, t))\n\n        shapes = [K.get_variable_shape(p) for p in params]\n        ms = [K.zeros(shape) for shape in shapes]\n        vs = [K.zeros(shape) for shape in shapes]\n        self.weights = [self.iterations] + ms + vs\n\n        for p, g, m, v in zip(params, grads, ms, vs):\n\n            # if a weight tensor (len > 1) use weight normalized parameterization\n            # this is the only part changed w.r.t. keras.optimizers.Adam\n            ps = K.get_variable_shape(p)\n            if len(ps)>1:\n\n                # get weight normalization parameters\n                V, V_norm, V_scaler, g_param, grad_g, grad_V = get_weightnorm_params_and_grads(p, g)\n\n                # Adam containers for the 'g' parameter\n                V_scaler_shape = K.get_variable_shape(V_scaler)\n                m_g = K.zeros(V_scaler_shape)\n                v_g = K.zeros(V_scaler_shape)\n\n                # update g parameters\n                m_g_t = (self.beta_1 * m_g) + (1. - self.beta_1) * grad_g\n                v_g_t = (self.beta_2 * v_g) + (1. - self.beta_2) * K.square(grad_g)\n                new_g_param = g_param - lr_t * m_g_t \/ (K.sqrt(v_g_t) + self.epsilon)\n                self.updates.append(K.update(m_g, m_g_t))\n                self.updates.append(K.update(v_g, v_g_t))\n\n                # update V parameters\n                m_t = (self.beta_1 * m) + (1. - self.beta_1) * grad_V\n                v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(grad_V)\n                new_V_param = V - lr_t * m_t \/ (K.sqrt(v_t) + self.epsilon)\n                self.updates.append(K.update(m, m_t))\n                self.updates.append(K.update(v, v_t))\n\n                # if there are constraints we apply them to V, not W\n                # if p in constraints:\n                #     c = constraints[p]\n                #     new_V_param = c(new_V_param)\n\n                # wn param updates --> W updates\n                add_weightnorm_param_updates(self.updates, new_V_param, new_g_param, p, V_scaler)\n\n            else: # do optimization normally\n                m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n                v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n                p_t = p - lr_t * m_t \/ (K.sqrt(v_t) + self.epsilon)\n\n                self.updates.append(K.update(m, m_t))\n                self.updates.append(K.update(v, v_t))\n\n                new_p = p_t\n                # apply constraints\n                # if p in constraints:\n                #     c = constraints[p]\n                #     new_p = c(new_p)\n                self.updates.append(K.update(p, new_p))\n        return self.updates\n\n\ndef get_weightnorm_params_and_grads(p, g):\n    ps = K.get_variable_shape(p)\n\n    # construct weight scaler: V_scaler = g\/||V||\n    V_scaler_shape = (ps[-1],)  # assumes we're using tensorflow!\n    V_scaler = K.ones(V_scaler_shape)  # init to ones, so effective parameters don't change\n\n    # get V parameters = ||V||\/g * W\n    norm_axes = [i for i in range(len(ps) - 1)]\n    V = p \/ tf.reshape(V_scaler, [1] * len(norm_axes) + [-1])\n\n    # split V_scaler into ||V|| and g parameters\n    V_norm = tf.sqrt(tf.reduce_sum(tf.square(V), norm_axes))\n    g_param = V_scaler * V_norm\n\n    # get grad in V,g parameters\n    grad_g = tf.reduce_sum(g * V, norm_axes) \/ V_norm\n    grad_V = tf.reshape(V_scaler, [1] * len(norm_axes) + [-1]) * \\\n             (g - tf.reshape(grad_g \/ V_norm, [1] * len(norm_axes) + [-1]) * V)\n\n    return V, V_norm, V_scaler, g_param, grad_g, grad_V\n\n\ndef add_weightnorm_param_updates(updates, new_V_param, new_g_param, W, V_scaler):\n    ps = K.get_variable_shape(new_V_param)\n    norm_axes = [i for i in range(len(ps) - 1)]\n\n    # update W and V_scaler\n    new_V_norm = tf.sqrt(tf.reduce_sum(tf.square(new_V_param), norm_axes))\n    new_V_scaler = new_g_param \/ new_V_norm\n    new_W = tf.reshape(new_V_scaler, [1] * len(norm_axes) + [-1]) * new_V_param\n    updates.append(K.update(W, new_W))\n    updates.append(K.update(V_scaler, new_V_scaler))","c8a811d3":"\nimport argparse\n\nimport numpy as np\nfrom keras import backend as K\nfrom keras.optimizers import Adam\nfrom sklearn.utils import shuffle\n\n\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Temporal Ensembling')\n    parser.add_argument('--data_path', default='cifar10.npz', type=str, help='path to dataset')\n    parser.add_argument('--num_labeled_train', default=4000, type=int,\n                        help='the number of labeled data used for supervised training componet')\n    parser.add_argument('--num_test', default=10000, type=int,\n                        help='the number of data kept out for test')\n    parser.add_argument('--num_class', default=10, type=int, help='the number of class')\n    parser.add_argument('--num_epoch', default=351, type=int, help='the number of epoch')\n    parser.add_argument('--batch_size', default=100, type=int, help='mini batch size')\n    parser.add_argument('--ramp_up_period', default=80, type=int, help='ramp-up period of loss function')\n    parser.add_argument('--ramp_down_period', default=50, type=int, help='ramp-down period')\n    parser.add_argument('--alpha', default=0.6, type=float, help='ensembling momentum')\n    parser.add_argument('--weight_max', default=30, type=float, help='related to unsupervised loss component')\n    parser.add_argument('--learning_rate', default=0.001, type=float, help='learning rate of optimizer')\n    parser.add_argument('--whitening_flag', default=True, type=bool, help='Whitening')\n    parser.add_argument('--weight_norm_flag', default=True, type=bool,\n                        help='Weight normalization is applied. Otherwise Batch normalization is applied')\n    parser.add_argument('--augmentation_flag', default=True, type=bool, help='Data augmentation')\n    parser.add_argument('--trans_range', default=2, type=int, help='random_translation_range')\n\n    args = parser.parse_args()\n\n    return args\n\ndef pseudo_label(tmodel,inputs,num_class):\n  num_inputs=inputs.shape[0]\n  label_dummy = np.zeros((num_inputs, num_class))\n  flag_dummy = np.zeros((num_inputs, 1))\n  weight_dummy = np.zeros((num_inputs, 1))\n\n  test_x_ap = [inputs, label_dummy, flag_dummy, weight_dummy]\n  p = tmodel.predict(x=test_x_ap)\n  pr = p[:, 0:num_class]\n  pr_arg_max = np.argmax(pr, axis=1)\n  one_hot_labels=np.zeros_like(pr)\n  one_hot_labels[np.arange(len(pr_arg_max)),pr_arg_max]=1\n  \n  return one_hot_labels\ndef main():\n    # Prepare args\n    #args = parse_args()\n\n    num_test = 26032\n\n    num_class = 10\n    num_epoch = 351\n    batch_size = 100\n\n    alpha = 0.6\n    weight_norm_flag = False\n    augmentation_flag = True\n    whitening_flag = False\n    trans_range = 2\n    \n    \n    train_x, train_y, test_x, test_y = load_SVHN()\n    ret_dic = split_supervised_train(train_x, train_y, num_labeled_train)\n\n    ret_dic['test_x'] = test_x\n    ret_dic['test_y'] = test_y\n    ret_dic = make_train_test_dataset(ret_dic, num_class)\n\n    unsupervised_target = ret_dic['unsupervised_target']\n    supervised_label = ret_dic['supervised_label']\n    supervised_flag = ret_dic['train_sup_flag']\n    unsupervised_weight = ret_dic['unsupervised_weight']\n    test_y = ret_dic['test_y']\n\n    train_x, test_x = normalize_images(ret_dic['train_x'], ret_dic['test_x'])\n    #train_x, test_x = normalize_images(train_x, test_x)\n    #train_y=to_categorical_onehot(train_y, num_class)\n   # train_y=to_categorical_onehot(train_y, num_class)\n\n    # pre-process\n    if whitening_flag:\n        train_x, test_x = whiten_zca(train_x, test_x)\n\n\n    # make the whole data and labels for training\n    # x = [train_x, supervised_label, supervised_flag, unsupervised_weight]\n    \n    pseudo_labeling_model=None\n    if pseudo_labeling:\n        if weight_norm_flag:\n\n            optimizer = AdamWithWeightnorm(lr=learning_rate, beta_1=0.9, beta_2=0.98)\n            pseudo_labeling_model = build_model(num_class=num_class)\n        else:\n\n            optimizer = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.98)\n            pseudo_labeling_model = build_modelBN(num_class=num_class)\n\n\n        pseudo_labeling_model.compile(optimizer=optimizer,\n                    loss=semi_supervised_loss(num_class))\n        pseudo_labeling_model.load_weights('..\/input\/svhn50002082021\/temporal_ens_svhn.h5')\n        evaluate(pseudo_labeling_model, num_class, len(train_x), train_x, supervised_label)\n        supervised_label[num_labeled_train:]=pseudo_label(pseudo_labeling_model,train_x[num_labeled_train:],num_class)\n    num_train_data = train_x.shape[0]\n    model=None\n    # Build Model\n\n    if weight_norm_flag:\n\n        optimizer = AdamWithWeightnorm(lr=learning_rate, beta_1=0.9, beta_2=0.98)\n        model = build_model(num_class=num_class)\n    else:\n        \n        optimizer = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.98)\n        model = build_modelBN(num_class=num_class)\n\n    #model = build_modelBN(num_class=num_class)\n    model.compile(optimizer=optimizer,\n                  loss=semi_supervised_loss(num_class))\n\n    model.metrics_tensors += model.outputs\n    model.summary()\n\n      \n    y = np.concatenate((unsupervised_target, supervised_label, supervised_flag, unsupervised_weight), axis=1)\n    if augmentation_flag:\n        train_x = np.pad(train_x, ((0, 0), (trans_range, trans_range), (trans_range, trans_range), (0, 0)), 'reflect')\n\n    #x=data_augmentation_tempen(train_x, trans_range)\n    #evaluate(model, num_class, len(train_x), train_x, supervised_label)\n    #evaluate(model, num_class, len(train_x[0:4001]), train_x[0:4001], supervised_label[0:4001])\n    #evaluate(model, num_class, num_test, test_x, test_y)\n    #print(supervised_flag.shape)\n    # prepare weights and arrays for updates\n    gen_weight = ramp_up_weight(ramp_up_period, weight_max * (num_labeled_train \/ num_train_data))\n    gen_lr_weight = ramp_down_weight(ramp_down_period)\n    gen_new_weight = ramp_down_weight(involvment_time)\n    idx_list = [v for v in range(num_train_data)]\n    ensemble_prediction = np.zeros((num_train_data, num_class))\n    cur_pred = np.zeros((num_train_data, num_class))\n    pseudo=1\n    # Training\n    for epoch in range(num_epoch):\n        print('epoch: ', epoch)\n        idx_list = shuffle(idx_list)\n\n        if epoch > num_epoch - ramp_down_period:\n            weight_down = next(gen_lr_weight)\n            K.set_value(model.optimizer.lr, weight_down * learning_rate)\n            K.set_value(model.optimizer.beta_1, 0.4 * weight_down + 0.5)\n\n        ave_loss = 0\n        for i in range(0, num_train_data, batch_size):\n            target_idx = idx_list[i:i + batch_size]\n\n            if augmentation_flag:\n                x1 = data_augmentation_tempen(train_x[target_idx], trans_range)\n            else:\n                x1 = train_x[target_idx]\n\n            x2 = supervised_label[target_idx]\n            x3 = supervised_flag[target_idx]\n            x4 = unsupervised_weight[target_idx]\n            y_t = y[target_idx]\n\n            x_t = [x1, x2, x3, x4]\n            tr_loss, output = model.train_on_batch(x=x_t, y=y_t)\n            cur_pred[idx_list[i:i + batch_size]] = output[:, 0:num_class]\n            ave_loss += tr_loss\n\n\n        print('Training Loss: ', (ave_loss * batch_size) \/ num_train_data, flush=True)\n        #pseudo=next(gen_new_weight)\n        pseudo=next(gen_new_weight)\n\n\n        #supervised_flag[num_labeled_train+1:len(supervised_flag)]=[pseudo*x for x in supervised_flag[num_labeled_train+1:len(supervised_flag)]] not conxerned\n\n        #supervised_flag[num_labeled_train] not conerned\n        # Update phase\n        if involvement_wieght:\n            y[num_labeled_train:len(supervised_flag), num_class * 2]=[pseudo for e in y[num_labeled_train:len(supervised_flag), num_class * 2]]\n\n        next_weight = next(gen_weight)\n        y, unsupervised_weight = update_weight(y, unsupervised_weight, next_weight)\n        ensemble_prediction, y = update_unsupervised_target(ensemble_prediction, y, num_class, alpha, cur_pred, epoch)\n\n        # Evaluation\n        if epoch % 5 == 0:\n            print('Evaluate epoch :  ', epoch, flush=True)\n            evaluate(model, num_class, num_test, test_x, test_y)\n    if(not pseudo_labeling):\n      model.save_weights('temporal_ens_svhn.h5')\nmain()","da1b81b6":"# **libs finished**"}}