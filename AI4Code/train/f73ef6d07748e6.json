{"cell_type":{"ae8ad2ef":"code","dd3b0a29":"code","baab0744":"code","93dbecfa":"code","cf768b50":"code","5f7f0a5c":"code","e8cd3fa5":"code","d1974537":"code","533f65cf":"code","37357be0":"code","47ec9ee6":"code","1cbeaac0":"code","f290a4c0":"code","2b1fd5b5":"code","555dbb41":"code","09d2f4b4":"code","f78247db":"code","b9db2de6":"code","28df5e76":"code","f64ff0a3":"code","bce2a0a1":"code","672f949a":"code","3f7a3b1a":"code","6c8e12bc":"code","bd36a424":"code","7d4b9857":"code","5167ec1d":"code","234f5fda":"code","e5e1e97c":"code","29fa4882":"code","7f3e930d":"code","4b1b2794":"code","e16d0a6e":"code","02498176":"code","127bb008":"code","df52c20b":"code","d108cc22":"code","8e513da7":"code","80e6ad77":"code","52648e93":"code","826459e1":"code","a80af077":"markdown","8dd095fb":"markdown","f8aa433f":"markdown","ad98da92":"markdown","83d1eec1":"markdown","74ef8cf8":"markdown","f60658d1":"markdown","c682cf29":"markdown","e64bbb89":"markdown","171ad330":"markdown","0f7b0b83":"markdown","a6802c02":"markdown","81c5e598":"markdown","3a8524b1":"markdown","f6cdf420":"markdown","3097b784":"markdown","f3007cf3":"markdown","f3df553f":"markdown","6af03d11":"markdown","694170d8":"markdown","3c911f2f":"markdown","6c801e99":"markdown","e90e6131":"markdown","a79129b8":"markdown","e421253a":"markdown","f30ecc98":"markdown","93296adc":"markdown","0cf23d36":"markdown","ca9acbb2":"markdown","787fd02a":"markdown","5739f3e9":"markdown","e5e44b16":"markdown","f6c20782":"markdown","d9788d7f":"markdown","f19deef3":"markdown","f8193dc1":"markdown","d22d5cf1":"markdown","7f43ca9c":"markdown","66a39bd7":"markdown","e00f2ad7":"markdown","273df2f6":"markdown"},"source":{"ae8ad2ef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dd3b0a29":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas_profiling as pp\nimport warnings\nwarnings.filterwarnings('ignore')","baab0744":"df = pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv')","93dbecfa":"df.shape","cf768b50":"df.head()","5f7f0a5c":"df.describe()","e8cd3fa5":"pp.ProfileReport(df)","d1974537":"df.target.value_counts()","533f65cf":"df.isnull().sum()","37357be0":"for column in df.columns:\n    print(column,df[column].nunique())","47ec9ee6":"plt.rcParams['figure.figsize'] = (16, 14)\n# plt.style.use('ggplot')\nsns.heatmap(df.corr(), annot = True, cmap = 'PiYG')\nplt.title('Heatmap of Data', fontsize = 20)\nplt.show()","1cbeaac0":"f,ax=plt.subplots(3,2,figsize=(12,12))\nf.delaxes(ax[2,1])\n\nfor i,feature in enumerate(['age','thalach','chol','trestbps','oldpeak']):\n    sns.distplot(df[feature], ax=ax[i\/\/2,i%2], hist=True, color= 'y' )","f290a4c0":"f,ax=plt.subplots(4,2,figsize=(10,8))\n\nfor i,feature in enumerate(['sex','cp','fbs','restecg','exang','slope','ca','thal']):\n    sns.countplot(x=feature,data=df,ax=ax[i\/\/2,i%2], alpha=0.8, edgecolor=('white'), linewidth=2)\n    plt.tight_layout()","2b1fd5b5":"plt.rcParams['figure.figsize'] = (8, 6)\nsns.violinplot(df['target'], df['age'], palette = 'colorblind')\nplt.title('Age vs Target', fontsize = 20, fontweight = 30)\nplt.show()","555dbb41":"plt.rcParams['figure.figsize'] = (8, 6)\nsns.violinplot(df['target'], df['thalach'], palette = 'colorblind')\nplt.title('thalach vs Target', fontsize = 20, fontweight = 30)\nplt.show()","09d2f4b4":"plt.rcParams['figure.figsize'] = (8, 6)\nsns.violinplot(df['target'], df['chol'], palette = 'colorblind')\nplt.title('chol vs Target', fontsize = 20, fontweight = 30)\nplt.show()","f78247db":"plt.rcParams['figure.figsize'] = (8, 6)\nsns.violinplot(df['target'], df['trestbps'], palette = 'colorblind')\nplt.title('trestbps vs Target', fontsize = 20, fontweight = 30)\nplt.show()","b9db2de6":"plt.rcParams['figure.figsize'] = (8, 6)\nsns.violinplot(df['target'], df['oldpeak'], palette = 'colorblind')\nplt.title('oldpeak vs Target', fontsize = 20, fontweight = 30)\nplt.show()","28df5e76":"plt.rcParams['figure.figsize'] = (8, 6)\ndat = pd.crosstab(df['target'], df['restecg']) \ndat.div(dat.sum(1).astype(float), axis = 0).plot(kind = 'bar')\nplt.title('Relation of ECG measurement with Target', fontsize = 20, fontweight = 30)\nplt.show()","f64ff0a3":"plt.rcParams['figure.figsize'] = (8, 6)\ndat = pd.crosstab(df['target'], df['fbs']) \ndat.div(dat.sum(1).astype(float), axis = 0).plot(kind = 'bar')\nplt.title('Relation of blood sugar with Target', fontsize = 20, fontweight = 30)\nplt.show()","bce2a0a1":"plt.rcParams['figure.figsize'] = (8, 6)\ndat = pd.crosstab(df['target'], df['sex'])\ndat.div(dat.sum(1).astype(float), axis = 0).plot(kind = 'bar')\nplt.title('Relation of Gender with Target', fontsize = 20, fontweight = 30)\nplt.show()","672f949a":"plt.rcParams['figure.figsize'] = (8, 6)\ndat = pd.crosstab(df['target'], df['cp']) \ndat.div(dat.sum(1).astype(float), axis = 0).plot(kind = 'bar')\nplt.title('Relation of  chest pain with Target', fontsize = 20, fontweight = 30)\nplt.show()","3f7a3b1a":"plt.rcParams['figure.figsize'] = (8, 6)\ndat = pd.crosstab(df['target'], df['exang']) \ndat.div(dat.sum(1).astype(float), axis = 0).plot(kind = 'bar')\nplt.title('Relation of Exercise induced angina with Target', fontsize = 20, fontweight = 30)\nplt.show()","6c8e12bc":"plt.rcParams['figure.figsize'] = (8, 6)\ndat = pd.crosstab(df['target'], df['slope']) \ndat.div(dat.sum(1).astype(float), axis = 0).plot(kind = 'bar')\nplt.title('Relation of slope with Target', fontsize = 20, fontweight = 30)\nplt.show()","bd36a424":"plt.rcParams['figure.figsize'] = (8, 6)\ndat = pd.crosstab(df['target'], df['ca']) \ndat.div(dat.sum(1).astype(float), axis = 0).plot(kind = 'bar')\nplt.title('Relation of major vessels with Target', fontsize = 20, fontweight = 30)\nplt.show()","7d4b9857":"plt.rcParams['figure.figsize'] = (8, 6)\ndat = pd.crosstab(df['target'], df['thal']) \ndat.div(dat.sum(1).astype(float), axis = 0).plot(kind = 'bar')\nplt.title('Relation thalassemia with Target', fontsize = 20, fontweight = 30)\nplt.show()","5167ec1d":"categorical_cols = ['sex','cp','fbs','restecg','exang','slope','ca','thal']\nnumeric_cols = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']","234f5fda":"for i in categorical_cols:\n    print(i,'\\n', df[i].value_counts())","e5e1e97c":"multi_label_cols = [i for i in categorical_cols if df[i].nunique()>2]","29fa4882":"from sklearn.preprocessing import StandardScaler","7f3e930d":"std = StandardScaler()\ndf[numeric_cols] = std.fit_transform(df[numeric_cols])","4b1b2794":"df.shape","e16d0a6e":"df = pd.get_dummies(data = df,columns = multi_label_cols)","02498176":"x = df.drop(['target'],axis=1)\ny = df['target']","127bb008":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)","df52c20b":"print(\"Shape of x_train :\", x_train.shape)\nprint(\"Shape of x_test :\", x_test.shape)\nprint(\"Shape of y_train :\", y_train.shape)\nprint(\"Shape of y_test :\", y_test.shape)","d108cc22":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb","8e513da7":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score","80e6ad77":"lr = LogisticRegression()\nsvm = SVC(probability=True)\nrf = RandomForestClassifier(n_estimators=100, max_depth=5)\nxg = xgb.XGBClassifier()","52648e93":"models = ['lr','svm','rf','xg']\nfor model in models:\n    clf = eval(model)\n    clf.fit(x_train, y_train)\n    y_pred_prob = clf.predict_proba(x_test)[:, 1]\n    y_pred = clf.predict(x_test)\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n    auc = roc_auc_score(y_test, y_pred_prob)\n    # evaluating the model\n    print(f\"Training Accuracy for model {model} is: \", clf.score(x_train, y_train))\n    print(f\"Testing Accuracy for model {model} is:\", clf.score(x_test, y_test))\n    print(f\"AUC Score for model {model} is: {auc}\")\n    cm = confusion_matrix(y_test, y_pred)\n    plt.figure()\n    sns.heatmap(cm, annot = True)\n    print(classification_report(y_test, y_pred))\n    plt.figure()\n    plt.plot(fpr, tpr)\n    plt.title(f'ROC for model {model}')","826459e1":"from sklearn.model_selection import GridSearchCV\ngrid={\"C\":np.logspace(-3,3,10), \"penalty\":[\"l1\",\"l2\"]}\nlogreg=LogisticRegression()\nlogreg_cv=GridSearchCV(logreg,grid,cv=10)\nlogreg_cv.fit(x,y)\nprint(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\nprint(\"accuracy :\",logreg_cv.best_score_)","a80af077":"Now we can see how many unique values each feature have. So to get the idea of categorical and numerical features.","8dd095fb":"restecg: Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)\n\nHere we can see 0 and 2 values of ECG are more common in People with no heart disease. And value of 1 is more common in People with disease.","f8aa433f":"Since the dataset is small, We will try simple models to reduce overfitting here. But can try XGBoost just for fun. ","ad98da92":"Lets import Libraries for EDA and data visualizations","83d1eec1":"This plot is also very clear. Count of  is higher in people with disease and  other counts are higher in people without disease.","74ef8cf8":"Lets first plot numeric features and see there distributions.","f60658d1":"Nope. Seems like no such highly correlated feature.","c682cf29":"Okay. so some featues are not evenly distributed here. For small datasets, this problem can be there.","e64bbb89":"In preprocessing step, we will standard scale all numeric features and one-hot encode all multilabel features. This is pretty simple and straight forward Preprocessing. You can also do some outlier removal.","171ad330":"**DATA DESCRIPTION**\n\n(This data description is given on UCI website for this dataset[ Link](https:\/\/archive.ics.uci.edu\/ml\/datasets\/heart+disease))\n\n*  age: age in years\n*  sex: sex (1 = male; 0 = female)\n*  cp: chest pain type\n    -- Value 1: typical angina\n    -- Value 2: atypical angina\n    -- Value 3: non-anginal pain\n    -- Value 4: asymptomatic\n*  trestbps: resting blood pressure (in mm Hg on admission to the hospital)\n*  chol: serum cholestoral in mg\/dl\n*  fbs: (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n*  restecg: resting electrocardiographic results\n    -- Value 0: normal\n    -- Value 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\n    -- Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n*  thalach: maximum heart rate achieved\n*  exang: exercise induced angina (1 = yes; 0 = no)\n*  oldpeak = ST depression induced by exercise relative to rest\n*  slope: the slope of the peak exercise ST segment\n    -- Value 1: upsloping\n    -- Value 2: flat\n    -- Value 3: downsloping\n*  ca: number of major vessels (0-3) colored by flourosopy\n*  thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n*  target: Heart disease (0 = no, 1 = yes)","0f7b0b83":"Now lets see relation of categorical features with target.","a6802c02":"Seems fine.","81c5e598":"In this plot, we can see people with heart diseases might have some high cholesterol values. Good feature!","3a8524b1":"That's it. Seems like a decent accuracy with minimal work. \n\n\n\nPlease give it an upvote if you like this notebook. Also if you have any questions or comments, Please post. Will answer surely. \n\nThank you.","f6cdf420":"**TRAINING AND EVALUATING MODELS**","3097b784":"Here Target 1 seems to higher mean and also low \"maximum heart rate achieved\" values means no Heart Disease. It makes sense to us. Great.","f3007cf3":"So you can say Women are more prone to heart disease.","f3df553f":"**Hyperparameter tuning**\n\nHere we are trying Grid search for Logistic regression which did a good job. But you can also try it on different models.","6af03d11":"Lets see if there any null value in data. If there is any, We have to handle it here.","694170d8":"So no null values in data. Seems like small and clean dataset. Lets proceed...","3c911f2f":"Also related. Count of 1 is higher in people without disease and count of 2 is higher in people with disease.","6c801e99":"Lets first see numeric features interaction with target","e90e6131":"cool...\nNow we can see the correlation matrix of data. Lets see if there is any high correlated feature that we can pay attention to.","a79129b8":"In this distribution we can see Age is not a good feature in deciding target. ","e421253a":"Data size seems to be very small. So we should try simple models Here.\nLets see what the data looks like.","f30ecc98":"this plot shows that people not suffering from Heart disease might have little high blood pressure.","93296adc":"Not any such difference.","0cf23d36":"this plot shows that people not suffering from Heart disease might have high ST depression. ","ca9acbb2":"**PREPROCESSING**","787fd02a":"**PLOTS**","5739f3e9":"Lets see Pandas Profiling on this Data ","e5e44b16":"Data is a mixture of categorical and continuous values. Thats cool.\nLets see the distribution of target.","f6c20782":"We have visualized each features individually, Now we can plot there relation with the target variable to see their impact on target. This can give how a feature can be important in predicting target.","d9788d7f":"cp: The chest pain experienced (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic)\n\nHere we can see count of 0 is high in people with no heart disease whereas count of other values are high in people with heart disease.","f19deef3":"So these were some important plots. You can also make bivariate plot for multiple feature interactions. But I wanted to keep it simple.","f8193dc1":"Here also we can see relation with target. count of 0 is high in people with disease and count of 1 is high in people without disease.","d22d5cf1":"Here we can see the Logistic regression is doing a good job and not overfitting the data.\n","7f43ca9c":"This plot is also very clear. Count of 2 is higher in people with disease and count of 3 is higher in people without disease","66a39bd7":"Now lets do test train split for model training and evaluation. You can also try k-fold cross validation here.","e00f2ad7":"Okay. Time to make some plots to get the feel of data. This part is very important before modelling. Maybe fo small datasets you can get away without any plots and visualizations. But when the data size is huge and lots of features are there. You will have to make some plots to get better idea of the dataset.","273df2f6":"Now lets plot categorical features and see there distribution. We are using countplot here."}}