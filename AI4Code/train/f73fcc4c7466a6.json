{"cell_type":{"aa7cbe66":"code","8a037c3c":"code","ca3c6ae9":"code","e28973e9":"code","412d1c75":"code","4e6e7fe1":"code","6372edfb":"code","f01d067c":"code","e2a377b3":"code","6909b433":"code","e0e6253d":"code","a089e014":"code","adf6341e":"code","6ce6931b":"code","e0cab61d":"code","53a2359a":"code","e388a148":"markdown","a6c8fe6b":"markdown","ed4f5f91":"markdown","a5bc7703":"markdown","6b0e81f7":"markdown","b462fe12":"markdown","b53aa9d4":"markdown"},"source":{"aa7cbe66":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8a037c3c":"# import libraries\n\n# For plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"white\")\n%matplotlib inline\n#for matrix math\nimport numpy as np\n#for normalization + probability density function computation\nfrom scipy import stats\n#for data preprocessing\nimport pandas as pd\nfrom math import sqrt, log, exp, pi\nfrom random import uniform\nprint(\"import done\")","ca3c6ae9":"df = pd.read_csv('\/kaggle\/input\/customer-segmentation-tutorial-in-python\/Mall_Customers.csv')\ndf.head()","e28973e9":"## examples from here https:\/\/www.kaggle.com\/charel\/learn-by-example-expectation-maximization\nclass Gaussian:\n    def __init__(self, mu, sigma):\n        self.mu = mu\n        self.sigma = sigma\n\n    def pdf(self, datum):\n        u = (datum - self.mu)\/abs(self.sigma)\n        y = (1\/sqrt(2*pi)*abs(self.sigma)) * exp(-u*u \/ 2)\n        return y\n\n    def __repr__(self):\n        return 'Gaussian({0:4.6}, {1:4.6})'.format(self.mu, self.sigma)\n\nclass GaussianMixture():\n    def __init__(self, df, mu_min, mu_max, sigma_min, sigma_max, mix=.5):\n        self.data = df\n        \n        ## initialize with multiple gaussians\n        self.one = Gaussian(uniform(mu_min, mu_max), uniform(sigma_min, sigma_max))\n        self.two = Gaussian(uniform(mu_min, mu_max), uniform(sigma_min, sigma_max))\n\n        ## as well as how much to mix them\n        self.mix = mix\n\n    def Estep(self):\n        ## estimate the probability of each data point belong to the distribution\n        self.loglike = 0.\n\n        for datum in self.data:\n            wp1 = self.one.pdf(datum)* self.mix\n            wp2 = self.two.pdf(datum)* (1-self.mix)\n\n            # compute denominator\n            den = wp1 + wp2\n            \n            # normalize\n            wp1 \/= den\n            wp2 \/= den\n\n            # update loglike\n            self.loglike += log(den)\n\n            yield(wp1, wp2)\n    \n\n    def Mstep(self, weights):\n        ## redistribute the mean and var based on the loglikelihood\n\n        (left, right) = zip(*weights)\n\n        one_den = sum(left)\n\n        two_den = sum(right)\n\n        # update mean\n        self.one.mu = sum(w*d for (w, d) in zip(left, self.data))\/ one_den\n        self.two.mu = sum(w*d for (w, d) in zip(right, self.data)) \/ two_den\n\n        # update sigmas\n        self.one.sigma = sqrt(sum(w*((d-self.one.mu)**2) for (w, d) in zip(left, self.data))\/one_den)\n        self.two.sigma = sqrt(sum(w*((d-self.two.mu)**2) for (w, d) in zip(right, self.data))\/two_den)\n\n        # update the mix\n        self.mix = one_den\/len(self.data)\n\n    def EM_algo(self, N=1, verbose=False):\n        for i in range(1, N+1):\n            self.Mstep(self.Estep())\n            if verbose:\n                print('{0:2} {1}'.format(i, self))\n\n        self.Estep()\n\n    def __repr__(self):\n        return 'GaussianMixture({0}, {1}, mix={2.03})'.format(self.one, \n                                                              self.two, \n                                                              self.mix)\n\n    def __str__(self):\n        return 'Mixture: {0}, {1}, mix={2:.03})'.format(self.one, \n                                                        self.two, \n                                                        self.mix)\n","412d1c75":"plt.style.use('ggplot')","4e6e7fe1":"df['Spending Score (1-100)'].hist()","6372edfb":"spending = df['Spending Score (1-100)']","f01d067c":"df.dtypes","e2a377b3":"#spending = pd.DataFrame(spending)","6909b433":"# See the algorithm in action\nn_iterations = 20\nbest_mix = None\nbest_loglike = float('-inf')\nmix = GaussianMixture(spending, mu_min=0, mu_max=80, sigma_min=10, sigma_max=80, mix=.4)\nfor _ in range(n_iterations):\n    try:\n        #train!\n        mix.EM_algo(verbose=True)\n        if mix.loglike > best_loglike:\n            best_loglike = mix.loglike\n            best_mix = mix\n        \n    except (ZeroDivisionError, ValueError, RuntimeWarning): # Catch division errors from bad starts, and just throw them out...\n        print(\"one less\")\n        pass","e0e6253d":"from scipy import stats","a089e014":"df['Spending Score (1-100)'].hist(density=True)\n\nx = np.linspace(0, 100, 100)\npdf1= stats.norm.pdf(x, loc=50, scale=25)\npdf2 = stats.norm.pdf(x, loc=91, scale=2.86)\nplt.plot(x, pdf1, label='distribution 1')\n#plt.plot(x, pdf2, label='distribution 2')\nplt.legend()\nplt.title('Spending score');","adf6341e":"df['Annual Income (k$)'].hist();","6ce6931b":"income = df['Annual Income (k$)']","e0cab61d":"# See the algorithm in action\nn_iterations = 20\nbest_mix = None\nbest_loglike = float('-inf')\nmix = GaussianMixture(income, mu_min=0, mu_max=80, sigma_min=10, sigma_max=80, mix=.4)\nfor _ in range(n_iterations):\n    try:\n        #train!\n        mix.EM_algo(verbose=True)\n        if mix.loglike > best_loglike:\n            best_loglike = mix.loglike\n            best_mix = mix\n        \n    except (ZeroDivisionError, ValueError, RuntimeWarning): # Catch division errors from bad starts, and just throw them out...\n        print(\"one less\")\n        pass","53a2359a":"df['Annual Income (k$)'].hist(density=True)\n\nx = np.linspace(0, 140, 200)\npdf2 = stats.norm.pdf(x, loc=60, scale=26)\nplt.plot(x, pdf2, label='distribution1')\nplt.legend()\nplt.show()","e388a148":"## Expectation-maximization (EM) algo and Gaussian Mixture model (GMM)","a6c8fe6b":"In this example, we use EM algoritm to **estimate the two Gaussian distributions** underlying the spending score distribution and annual income","ed4f5f91":"\nEM algorithm is an algoritm to find maximum likelihood estimation of the parameters from a dataset with **incomplete information**. The dataset is likely generated from a series of gaussian process, but there are still information hidden from us. There are many good articles explainig the math behind the EM algorithm. To get a visualization about what happened inside the algorithm, please see this [kaggle notebook](https:\/\/www.kaggle.com\/charel\/learn-by-example-expectation-maximization) by @charel. \nBelow is a simplified explanation for using EM on gaussian mixture model:\n\n### E step: \n\n> For each point, estimate the probability that each Gaussian generate it\n\nFor this step, we need to generate gaussian distribution and compute the probability density function\n\n\n### M step:\n\n> Modify the parameters according to the hidden variable to maximize the likelihood of the data\n\nFor this step, we re-distribute the mean and the standard deviation based on the proportion we get from E step.","a5bc7703":"## Annual Income Clustering","6b0e81f7":"## Spending score clustering","b462fe12":"[reference 1](https:\/\/www.kaggle.com\/ryomiyazaki\/semi-supervised-gaussian-mixture-model-with-em)\n\n[reference 2](https:\/\/www.kaggle.com\/charel\/learn-by-example-expectation-maximization)\n\n[reference 3](https:\/\/towardsdatascience.com\/expectation-maximization-explained-c82f5ed438e5)","b53aa9d4":"In this case, we see that the mix score gets really small, which means that the second distribution has taken the higher likelihood in this case"}}