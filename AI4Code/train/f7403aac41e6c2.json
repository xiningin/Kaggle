{"cell_type":{"26842cde":"code","899ac97d":"code","81f4d73d":"code","68f0780d":"code","1b230e10":"code","088e3996":"code","c01858fa":"code","2405fb7d":"code","8441ee18":"code","5df73bd8":"code","02a33bcb":"code","c3949734":"code","3a326dea":"code","5ee72cdc":"code","e71af257":"code","c107e3a1":"code","f453452b":"code","310bbdea":"code","f1a36bd0":"code","a1931440":"code","4505d2d3":"code","71db06e1":"code","d9b88927":"code","6abee2ee":"code","377265d7":"code","7a505f25":"code","f1d3a645":"code","c2dce048":"code","3463a44b":"code","38256e5e":"code","6074e718":"code","fcd6974e":"code","95aab1f5":"code","b10235ca":"code","03e2b15f":"code","f67284fe":"code","e7517f5d":"code","75c7a6a9":"code","d0636cab":"code","12b633c5":"code","be59e40c":"code","916b8eb5":"code","e8e750cc":"code","47d40003":"code","a81ebedc":"code","5397dcb1":"code","f8c10074":"code","cb29ce6d":"code","9e5f31aa":"code","94f00733":"code","030ae472":"code","058b1daf":"code","c26a7582":"code","d108bd94":"code","88f1ca6c":"code","1c738237":"code","d1f93a60":"code","0660e177":"code","b5b1155d":"markdown","2bba0588":"markdown","b2ecab86":"markdown","37720eaa":"markdown","1b4e1ec1":"markdown","984d87d3":"markdown","7ae515e3":"markdown","f06b96f1":"markdown","976e8708":"markdown","433df62f":"markdown","85b28d46":"markdown","3baa9a82":"markdown","308fcfc2":"markdown","68740a2e":"markdown","47772bbc":"markdown","07431b89":"markdown","f3959352":"markdown","c7f6b8d3":"markdown","e318f45f":"markdown","47b879f4":"markdown","29cf6094":"markdown","9e3aadc5":"markdown","7a520a5b":"markdown","3a192ef5":"markdown","371a7856":"markdown","5d926bb7":"markdown","c30416ee":"markdown","d63b70f9":"markdown","6b1dbc32":"markdown","1b9b6e07":"markdown","d5bf2256":"markdown","1773b773":"markdown","4d5345cb":"markdown"},"source":{"26842cde":"import pandas as pd\nimport numpy as np\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\", category = RuntimeWarning)","899ac97d":"os.getcwd()\nos.chdir(\"..\/input\")","81f4d73d":"input_data = pd.read_csv(r'google_review_ratings.csv')","68f0780d":"input_data.shape","1b230e10":"input_data.columns","088e3996":"input_data.head(5)","c01858fa":"input_data.info()","2405fb7d":"input_data.drop('Unnamed: 25', axis = 1, inplace = True)","8441ee18":"column_names = ['user_id', 'churches', 'resorts', 'beaches', 'parks', 'theatres', 'museums', 'malls', 'zoo', 'restaurants', 'pubs_bars', 'local_services', 'burger_pizza_shops', 'hotels_other_lodgings', 'juice_bars', 'art_galleries', 'dance_clubs', 'swimming_pools', 'gyms', 'bakeries', 'beauty_spas', 'cafes', 'view_points', 'monuments', 'gardens']\ninput_data.columns = column_names","5df73bd8":"input_data[column_names].isnull().sum()","02a33bcb":"input_data = input_data.fillna(0)","c3949734":"input_data.dtypes","3a326dea":"input_data['local_services'][input_data['local_services'] == '2\\t2.']","5ee72cdc":"local_services_mean = input_data['local_services'][input_data['local_services'] != '2\\t2.']\ninput_data['local_services'][input_data['local_services'] == '2\\t2.'] = np.mean(local_services_mean.astype('float64'))\ninput_data['local_services'] = input_data['local_services'].astype('float64')","e71af257":"input_data.dtypes","c107e3a1":"input_data[column_names[:12]].describe()","f453452b":"input_data[column_names[12:]].describe()","310bbdea":"input_data_description = input_data.describe()\nmin_val = input_data_description.loc['min'] > 0\nmin_val[min_val]","f1a36bd0":"import matplotlib.pyplot as plt\nimport numpy as np\nplt.rcdefaults()\n%matplotlib inline\nno_of_zeros = input_data[column_names[1:]].astype(bool).sum(axis=0).sort_values()\n\nplt.figure(figsize=(10,7))\nplt.barh(np.arange(len(column_names[1:])), no_of_zeros.values, align='center', alpha=0.5)\nplt.yticks(np.arange(len(column_names[1:])), no_of_zeros.index)\nplt.xlabel('No of reviews')\nplt.ylabel('Categories')\nplt.title('No of reviews under each category')","a1931440":"no_of_reviews = input_data[column_names[1:]].astype(bool).sum(axis=1).value_counts()","4505d2d3":"plt.figure(figsize=(10,7))\nplt.bar(np.arange(len(no_of_reviews)), no_of_reviews.values, align='center', alpha=0.5)\nplt.xticks(np.arange(len(no_of_reviews)), no_of_reviews.index)\nplt.ylabel('No of reviews')\nplt.xlabel('No of categories')\nplt.title('No of Categories vs No of reviews')","71db06e1":"avg_rating = input_data[column_names[1:]].mean()\navg_rating = avg_rating.sort_values()","d9b88927":"plt.figure(figsize=(10,7))\nplt.barh(np.arange(len(column_names[1:])), avg_rating.values, align='center', alpha=0.5)\nplt.yticks(np.arange(len(column_names[1:])), avg_rating.index)\nplt.xlabel('Average Rating')\nplt.title('Average rating per Category')","6abee2ee":"entertainment = ['theatres', 'dance_clubs', 'malls']\nfood_travel = ['restaurants', 'pubs_bars', 'burger_pizza_shops', 'juice_bars', 'bakeries', 'cafes']\nplaces_of_stay = ['hotels_other_lodgings', 'resorts']\nhistorical = ['churches', 'museums', 'art_galleries', 'monuments']\nnature = ['beaches', 'parks', 'zoo', 'view_points', 'gardens']\nservices = ['local_services', 'swimming_pools', 'gyms', 'beauty_spas']","377265d7":"df_category_reviews = pd.DataFrame(columns = ['entertainment', 'food_travel', 'places_of_stay', 'historical', 'nature', 'services'])","7a505f25":"df_category_reviews['entertainment'] = input_data[entertainment].mean(axis = 1)\ndf_category_reviews['food_travel'] = input_data[food_travel].mean(axis = 1)\ndf_category_reviews['places_of_stay'] = input_data[places_of_stay].mean(axis = 1)\ndf_category_reviews['historical'] = input_data[historical].mean(axis = 1)\ndf_category_reviews['nature'] = input_data[nature].mean(axis = 1)\ndf_category_reviews['services'] = input_data[services].mean(axis = 1)","f1d3a645":"df_category_reviews.describe()","c2dce048":"ratings_per_category_df = pd.DataFrame(input_data[column_names[1:]].mean()).reset_index(level=0)","3463a44b":"ratings_per_category_df.columns = ['category', 'avg_rating']","38256e5e":"ratings_per_category_df['no_of_ratings'] = input_data[column_names[1:]].astype(bool).sum(axis=0).values.tolist()","6074e718":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nratings_per_category_df['avg_rating_scaled'] = scaler.fit_transform(ratings_per_category_df['avg_rating'].values.reshape(-1,1))\nratings_per_category_df['no_of_ratings_scaled'] = scaler.fit_transform(ratings_per_category_df['no_of_ratings'].values.reshape(-1,1))","fcd6974e":"def calculate_weighted_rating(x):\n    return (0.5 * x['avg_rating_scaled'] + 0.5 * x['no_of_ratings_scaled'])\n\nratings_per_category_df['weighted_rating'] = ratings_per_category_df.apply(calculate_weighted_rating, axis = 1)\nratings_per_category_df = ratings_per_category_df.sort_values(by=['weighted_rating'], ascending = False)","95aab1f5":"input_data.head()","b10235ca":"def get_recommendation_based_on_popularity(x):\n    zero_cols = input_data[input_data['user_id'] == x['user_id']][column_names[1:]].astype(bool).sum(axis=0)\n    zero_df = pd.DataFrame(zero_cols[zero_cols == 0]).reset_index(level = 0)\n    zero_df.columns = ['category', 'rating']\n    zero_df = pd.merge(zero_df, ratings_per_category_df, on = 'category', how = 'left')[['category', 'weighted_rating']]\n    zero_df = zero_df.sort_values(by = ['weighted_rating'], ascending = False)\n    if len(zero_df) > 0:\n        return zero_df['category'].values[0]\n    else:\n        return \"\"","03e2b15f":"input_data_recommendation = input_data.copy()\ninput_data_recommendation['recommendation_based_on_popularity'] = input_data_recommendation.apply(get_recommendation_based_on_popularity, axis = 1)","f67284fe":"input_data_recommendation['recommendation_based_on_popularity'][input_data['user_id'] == \"User 16\"]","e7517f5d":"from sklearn.neighbors import NearestNeighbors","75c7a6a9":"input_data_matrix = input_data[column_names[1:]].values\nknn_model = NearestNeighbors(n_neighbors=5).fit(input_data_matrix)","d0636cab":"query_index = np.random.choice(input_data[column_names[1:]].shape[0])\ndistances, indices = knn_model.kneighbors(input_data[column_names[1:]].iloc[query_index, :].values.reshape(1,-1), n_neighbors = 5)","12b633c5":"def compare_df(index, ind):        \n    zero_cols_in = input_data.loc[index].astype(bool)\n    zero_df_in = pd.DataFrame(zero_cols_in[zero_cols_in == True]).reset_index(level = 0)\n    in_wo_rating = zero_df_in['index']\n    sug_user = input_data.loc[ind]\n    zero_cols_sug = sug_user.astype(bool)\n    zero_df_sug = pd.DataFrame(zero_cols_sug[zero_cols_sug == True]).reset_index(level = 0)\n    sug_wo_rating = zero_df_sug['index']\n    sugg_list = list(set(sug_wo_rating) - set(in_wo_rating))\n    return sugg_list\ndef recommend_knn(index):\n    distances, indices = knn_model.kneighbors(input_data[column_names[1:]].iloc[index, :].values.reshape(1,-1), n_neighbors = 10)\n    distances = np.sort(distances)\n    for i in range(0,len(indices[0])):\n        ind = np.where(distances.flatten() == distances[0][i])[0][0]\n        sug_list = compare_df(index, indices[0][i]) \n        if len(sug_list) > 0:\n            break\n    return sug_list\nprint(recommend_knn(16))                                              ","be59e40c":"input_data_matrix = input_data.set_index('user_id').as_matrix()\nuser_ratings_mean = np.mean(input_data_matrix, axis = 1)\nuser_ratings_demeaned = input_data_matrix - user_ratings_mean.reshape(-1, 1)","916b8eb5":"from scipy.sparse.linalg import svds\nU, sigma, Vt = svds(user_ratings_demeaned, k = 1)","e8e750cc":"sigma = np.diag(sigma)","47d40003":"all_user_predicted_ratings = np.dot(np.dot(U, sigma), Vt) + user_ratings_mean.reshape(-1, 1)","a81ebedc":"preds_df = pd.DataFrame(all_user_predicted_ratings, columns = column_names[1:])\npreds_df.head()","5397dcb1":"def recommend_svd(index):\n    zero_cols_in = input_data.loc[index].astype(bool)\n    zero_df_in = pd.DataFrame(zero_cols_in[zero_cols_in == False]).reset_index(level = 0)\n    in_wo_rating = zero_df_in['index']\n    sug_user = preds_df[in_wo_rating.values.tolist()[1:]].loc[index]\n    sug_list = sug_user.sort_values(ascending = False).index[0]\n    return sug_list\nprint(recommend_svd(16))","f8c10074":"from sklearn.cluster import KMeans\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ninput_array = scaler.fit_transform(input_data[column_names[1:]].values)\nratings_per_category_df['no_of_ratings_scaled'] = scaler.fit_transform(ratings_per_category_df['no_of_ratings'].values.reshape(-1,1))\n#nput_array = input_data[column_names[1:]].values\nkmeans = KMeans(n_clusters=6)\n# fit kmeans object to data\nkmeans.fit(input_array)\n# print location of clusters learned by kmeans object\nprint(kmeans.cluster_centers_)\n# save new clusters for chart\ny_km = kmeans.fit_predict(input_array)","cb29ce6d":"plt.scatter(input_array[y_km ==0,0], input_array[y_km == 0,1], s=100, c='red')\nplt.scatter(input_array[y_km ==1,0], input_array[y_km == 1,1], s=100, c='black')\nplt.scatter(input_array[y_km ==2,0], input_array[y_km == 2,1], s=100, c='blue')\nplt.scatter(input_array[y_km ==3,0], input_array[y_km == 3,1], s=100, c='cyan')","9e5f31aa":"Sum_of_squared_distances = []\nK = range(1,30)\nfor k in K:\n    km = KMeans(n_clusters=k)\n    km = km.fit(input_array)\n    Sum_of_squared_distances.append(km.inertia_)","94f00733":"plt.plot(K, Sum_of_squared_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow Method For Optimal k')\nplt.show()","030ae472":"from sklearn.metrics import silhouette_score\nfor n_clusters in range(2,30):\n    clusterer = KMeans (n_clusters=n_clusters)\n    preds = clusterer.fit_predict(input_array)\n    centers = clusterer.cluster_centers_\n\n    score = silhouette_score (input_array, preds)\n    print (\"For n_clusters = {}, silhouette score is {})\".format(n_clusters, score))","058b1daf":"from surprise import SVD, NormalPredictor, KNNBasic, KNNWithMeans, KNNWithZScore, CoClustering\nfrom surprise import Dataset\nfrom surprise import Reader\nfrom surprise.model_selection import cross_validate","c26a7582":"reader = Reader(rating_scale=(0, 5))\ndf = input_data.replace(0, np.nan).set_index('user_id', append=True).stack().reset_index().rename(columns={0:'rating', 'level_2':'itemID', 'user_id':'userID'}).drop('level_0',1)\ndata = Dataset.load_from_df(df[['userID', 'itemID', 'rating']], reader)","d108bd94":"benchmark = []\n# Iterate over all algorithms\nfor algorithm in [SVD(), NormalPredictor(), KNNBasic(), KNNWithMeans(), KNNWithZScore(), CoClustering()]:\n    # Perform cross validation\n    results = cross_validate(algorithm, data, measures=['RMSE'], cv=3, verbose=False)\n    \n    # Get results & append algorithm name\n    tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n    tmp = tmp.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]], index=['Algorithm']))\n    benchmark.append(tmp)\n    \nbench_mark_df = pd.DataFrame(benchmark).set_index('Algorithm').sort_values('test_rmse')","88f1ca6c":"bench_mark_df = pd.DataFrame(benchmark).set_index('Algorithm').sort_values('test_rmse')  ","1c738237":"bench_mark_df","d1f93a60":"from surprise.model_selection import train_test_split\nfrom surprise import accuracy\ntrainset, testset = train_test_split(data, test_size=0.25, random_state = 12)\nalgo = KNNBasic()\nalgo = algo.fit(trainset)\npredictions = algo.test(testset)\naccuracy.rmse(predictions)","0660e177":"from collections import defaultdict\ndef get_top_n(predictions, n=5):\n   \n    # First map the predictions to each user.\n    top_n = defaultdict(list)\n    for uid, iid, true_r, est, _ in predictions:\n        top_n[uid].append((iid, est))\n\n    # Then sort the predictions for each user and retrieve the k highest ones.\n    for uid, user_ratings in top_n.items():\n        user_ratings.sort(key=lambda x: x[1], reverse=True)\n        top_n[uid] = user_ratings[:n]\n\n    return top_n\n\ntop_n = get_top_n(predictions, n=10)\n\n# Print the recommended items for each user\nfor uid, user_ratings in top_n.items():\n    print(uid, [iid for (iid, _) in user_ratings])","b5b1155d":"There is only one row with that value. Let us replace that value with the mean of the rest of the rows","2bba0588":"###### Approach 1: Popularity Based Recommendation Engine","b2ecab86":"There are only 2 non null values in the last column. There are only 24 categories described in the dataset description and the last column is not present there. So let's drop the column","37720eaa":"###### Data Cleaning and Preprocessing","1b4e1ec1":"###### Collaborative Filtering based recommender","984d87d3":"Entertainment has the highest average rating and Services have the lowest rating implying that people are more interested in entertainment","7ae515e3":"Find optimum k","f06b96f1":"Converting the column 'local services' to float datatype","976e8708":"###### Conclusion","433df62f":"###### Recommender Engines","85b28d46":"###### Travel Review Ratings Dataset Analysis\n###### Submitted By: Iswarya Nagappan","3baa9a82":"###### Exploratory Data Analysis","308fcfc2":"###### Import the necessary dependencies","68740a2e":"###### Looking at the summary of the dataset","47772bbc":"The above 10 categories have been given a rating by all the users as the minimum value is greater than 0","07431b89":"Let us basket the different categories into higher levels and do an analysis to see if there is any influence of the type of the tourist attraction","f3959352":"The above module gives predictions to different users based on the ratings already given by them irrespective of the item has been rated already or not whereas other prototypes that we saw already suggest based on the items that were not rated by the user before. Either approach could be chosen according to the need. 'surprise' package has an edge over others as it has functions to calculate built in evaluation metrics, do hyper parameter tuning and cross validation and predict recommendations","c7f6b8d3":"Around 3500 users have given a rating for all the 24 categories and the least no of rating given by a user is 15. So for users with lesser number of ratings a recommender system can be built","e318f45f":"Malls have the highest average rating and gyms have the lowest average rating implying that travellers prefer malls and least preferres is gym. We can even relate this to the common phenomena that gyms are not usually visited by tourists","47b879f4":"There is a string present among the rows. Let's check how many rows have such values and convert them to float","29cf6094":"###### Approach 5: Recommender based on Surprise python package to calculate evaluation metrics","9e3aadc5":"Let us have a look at the summary of ratings given by users for various categories","7a520a5b":"Let's try to build different types of recommendation engines with the given dataset","3a192ef5":"There are two columns with one null value each. Let us impute the null values with 0 considering that the user didn't give rating to these categories","371a7856":"No of users given rating to bakeries and gyms are the least","5d926bb7":"Let us look at how many users have given rating for each category","c30416ee":"###### Approach 4: Clustering the data for user segmentation","d63b70f9":"kNNBasic has given the lowest rmse. So let's predict with the same","6b1dbc32":"There is no elbow formed in the plot and silhoutte score is also low showing that there are no specific clusters in the dataset. All the rows may belong to a single cluster","1b9b6e07":"The first three approaches could be taken for building recommendation engines that provide mutually exclusive suggestions like friend suggestions, etc.\n\nThe fifth approach could be chosen for finding out recommendations based on users preferences and history of ratings or activities","d5bf2256":"###### Approach 2: Recommender based on kNN","1773b773":"Renaming the columns for ease of understanding","4d5345cb":"###### Approach 3: Recommender Based on Matrix Factorization"}}