{"cell_type":{"6d0328b4":"code","fc1787d9":"code","031be589":"code","855af2da":"code","7cdc3e54":"code","8a4ebbd4":"code","9102b53a":"code","9c06716f":"code","78124966":"code","e01f9637":"code","4e7049ba":"code","55877e32":"code","d19d7eb2":"code","defa3541":"code","88d0402e":"code","97fcd43d":"code","10c7f132":"code","4425baf9":"code","cee4d381":"code","da20e9d6":"code","2a14897f":"code","0ca4fb33":"code","4c254572":"code","c792c0af":"code","ab84433a":"code","de080ecd":"code","5e0c10a0":"code","35a793e3":"code","0c9709c2":"code","4fef95a2":"code","1e50aa1a":"code","3dd262b8":"code","d4ebce21":"markdown","75282071":"markdown","76086415":"markdown","2b714459":"markdown","7935ef6d":"markdown","87386876":"markdown","573392b0":"markdown","7c91767d":"markdown","65a4aea7":"markdown","131b4c1b":"markdown","e7c2c0b0":"markdown","3c18a95a":"markdown","20208981":"markdown","98b3646d":"markdown","da63192d":"markdown","d6a78baf":"markdown","06fec7a6":"markdown","6dcf2ce3":"markdown","0bf1fdd8":"markdown"},"source":{"6d0328b4":"!pip install --upgrade pip --quiet","fc1787d9":"# install dependencies\n!pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https:\/\/download.pytorch.org\/whl\/torch_stable.html --quiet\n!pip install --upgrade pytorch-lightning omegaconf pycocotools wandb --quiet\n!echo \"Dependencies installed successfully!\"","031be589":"# clone the github repo:\n!git clone --recurse-submodules -j8 https:\/\/github.com\/benihime91\/retinanet_pet_detector.git","855af2da":"# Check PyTorch version\nimport torch\nprint(torch.__version__)\nprint(torch.cuda.is_available())\nprint(torch.cuda.get_device_name(0), end=\"\\n\\n\")\n\n!nvidia-smi","7cdc3e54":"import sys\nimport os\nimport warnings\n\nos.chdir(\"\/kaggle\/working\/retinanet_pet_detector\")\nwarnings.filterwarnings(\"ignore\")\n\n%load_ext autoreload\n%autoreload 2\n%matplotlib inline","8a4ebbd4":"# Standard Imports\nimport os\nimport numpy as np\nfrom omegaconf import OmegaConf, DictConfig\nimport time\nimport argparse\nimport pandas as pd\nfrom PIL import Image\n\nfrom references import Visualizer\nfrom references.utils import get_label_dict\nfrom train import main\n\npd.set_option(\"display.max_colwidth\",None)\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'","9102b53a":"# Convert xml files to a csv file\n!python prep_data.py \\\n    --action create \\\n    --img_dir \"\/kaggle\/input\/the-oxfordiiit-pet-dataset\/images\/images\" \\\n    --annot_dir \"\/kaggle\/input\/the-oxfordiiit-pet-dataset\/annotations\/annotations\/xmls\" \\\n    --labels \"\/kaggle\/working\/retinanet_pet_detector\/data\/labels.names\" \\\n    --output_dir \"\/kaggle\/working\/retinanet_pet_detector\/data\/\"","9c06716f":"# Kaggle gives different splits df order everytime\n# so i already have my xml converted to csv files saved to memory\ndf_pth = \"\/kaggle\/input\/retinanet-pet-csv\/data-full.csv\"\n\ndf = pd.read_csv(df_pth)\ndf.head(5)","78124966":"# sanity-check\nImage.open(df.filename[1000])","e01f9637":"# Create train, validaiton and test splits in the data\n!python prep_data.py \\\n    --action split \\\n    --csv {df_pth} \\\n    --valid_size 0.3 \\\n    --test_size 0.5 \\\n    --output_dir \"\/kaggle\/working\/retinanet_pet_detector\/data\/\" \\\n    --seed 123","4e7049ba":"! ls \"\/kaggle\/working\/retinanet_pet_detector\/data\/\"","55877e32":"# Read in the train and test dataframes\ntrn_df = pd.read_csv(\"\/kaggle\/working\/retinanet_pet_detector\/data\/train.csv\")\ntst_df = pd.read_csv(\"\/kaggle\/working\/retinanet_pet_detector\/data\/test.csv\")\nval_df = pd.read_csv(\"\/kaggle\/working\/retinanet_pet_detector\/data\/valid.csv\")","d19d7eb2":"trn_df.head()","defa3541":"tst_df.head()","88d0402e":"val_df.head()","97fcd43d":"print(\"Num Training Examples:\", len(trn_df))\nprint(\"Num Validation Examples:\", len(val_df))\nprint(\"Num Test Examples:\", len(tst_df))","10c7f132":"# Read in the Labes dictionary \n# and initializer the visualizer to view images with bboxes\nlabel_dict = get_label_dict(\"\/kaggle\/working\/retinanet_pet_detector\/data\/labels.names\")\nvis = Visualizer(label_dict)","4425baf9":"np.random.seed(123)\n\ndef display_random_image(data):\n    \"\"\"\n    Fn to display a random using the `Visualizer`\n    from the given pandas dataframe. The bounding boxes are also\n    drawn over the image.\n\n    Args:\n     data (`pd.dataframe`): A `pandas dataframe` where filename corresponds to \n                            the image path and bbox co-ordinates are stored in \n                            `[xmin,xmax,ymin,ymax]` & class_names (`int`) are \n                            stored in `[labels]`.\n    \"\"\"\n    idx = np.random.randint(0, len(df))\n    image_id = df.filename[idx]\n    # grab the cloumns which have the fname of the image\n    locs = df.loc[df.filename == image_id]\n    # grad the bounding boxes corresponding to the Image\n    boxes = locs[['xmin','ymin','xmax','ymax']].values\n    # grab the labels corresponding to the image\n    labels = locs['labels'].values\n    # Draw the bounding boxes over the image using the visulaizer\n    vis.draw_bboxes(image_id, boxes, labels, figsize=(5,5), show=True, save=False)","cee4d381":"# Display a random Image from the Train dataset\ndisplay_random_image(data=trn_df)","da20e9d6":"# Display a random Image from the test dataset\ndisplay_random_image(data=tst_df)","2a14897f":"display_random_image(val_df)","0ca4fb33":"!wandb login \"a74f67fd5fae293e301ea8b6710ee0241f595a63\" # use own API key here","4c254572":"# load the config file\nconfig = OmegaConf.load(\"\/kaggle\/working\/retinanet_pet_detector\/config\/main.yaml\")\n\n# name for the experiment\nexp_name = f\"[{time.strftime('%m\/%d %H:%M:%S')}]-kaggle-{config.model.backbone_kind}\"\n\n# specify the paths\nconfig.hparams.train_csv = \"\/kaggle\/working\/retinanet_pet_detector\/data\/train.csv\"\nconfig.hparams.valid_csv = \"\/kaggle\/working\/retinanet_pet_detector\/data\/valid.csv\"\nconfig.hparams.test_csv  = \"\/kaggle\/working\/retinanet_pet_detector\/data\/test.csv\"\nconfig.hparams.dataloader.num_workers = 4\nconfig.hparams.train_batch_size = 4\n# checkpoint paths\nconfig.trainer.model_checkpoint.params.filepath = \"\/kaggle\/working\/\"\n# maximum epochs to train for\nconfig.trainer.flags.max_epochs = 40\n\n# gradient clip value\nconfig.trainer.flags.gradient_clip_val = 0.1\n# optimizer\nconfig.hparams.optimizer = {\n    \"class_name\": \"torch.optim.AdamW\",\n    \"params\": {\n        \"lr\": 3e-04,\n        \"weight_decay\": 1e-04,\n    }\n}\n# scheduler\nconfig.hparams.scheduler = {\n    \"class_name\": \"torch.optim.lr_scheduler.OneCycleLR\", \n    \"params\": {\n        \"max_lr\": config.hparams.optimizer.params.lr,\n        \"epochs\": config.trainer.flags.max_epochs,\n        \"steps_per_epoch\": len(trn_df) \/\/ config.hparams.train_batch_size, \n        \"cycle_momentum\": True,\n        \"base_momentum\" : 0.85, \n        \"max_momentum\" : 0.95,\n    },\n    \"interval\" : \"step\", \n    \"frequency\": 1,\n}\n\n# create config to modify logger (since we will be using the wandb logger):\nconfig.trainer.logger = {\n    \"class_name\": \"pytorch_lightning.loggers.wandb.WandbLogger\",\n    \"params\":{\n        \"name\" : exp_name, \n        \"anonymous\": True,\n        \"project\": \"retinanet-oxford-pets\"\n    }\n}\n\n# I have already trained for a few epochs so we will resume from that checkpoint\n\n# Save the modified config file\nOmegaConf.save(config=config, f=\"\/kaggle\/working\/retinanet_pet_detector\/config\/main.yaml\")","c792c0af":"# Load and view the modified config file\nfname = \"\/kaggle\/working\/retinanet_pet_detector\/config\/main.yaml\"\nconfig = OmegaConf.load(fname)\nprint(f\"Contents of the config file = {fname}\", end=\"\\n\\n\")\nprint(config.pretty())","ab84433a":"# Creat argument dictionary\nd = {\"config\": \"\/kaggle\/working\/retinanet_pet_detector\/config\/main.yaml\",\"verbose\": 0}\nargs = DictConfig(d)\nprint(\"Arguments for the main function:\", end=\"\\n\\n\")\nprint(args.pretty())","de080ecd":"# run the main function\n# set a seed number to ensure results are reproducible\nmain(args, seed=123)","5e0c10a0":"# Since we used resnet50 to train our model\n# we will create a config file where: backbone = resnet50\nbackbone = \"resnet50\"\n# path to where model weights are saved\nurl = \"\/kaggle\/working\/weights.pth\"\n# total number of classes\nnum_classes = 37\n\nconf = {\"model_backbone\": backbone, \"url\": url, \"num_classes\": num_classes}\nconf = DictConfig(conf)\n\n# Save the modified config File\nfname = \"\/kaggle\/working\/retinanet_pet_detector\/config\/resnet50.yaml\"\nOmegaConf.save(config=conf, f=fname)\n\n\n# Print out the config File\nprint(OmegaConf.to_yaml(conf))","35a793e3":"# path to the config file\nconfig = \"\/kaggle\/working\/retinanet_pet_detector\/config\/resnet50.yaml\"","0c9709c2":"!python inference.py \\\n    --config \"\/kaggle\/working\/retinanet_pet_detector\/config\/resnet50.yaml\" \\\n    --image \"\/kaggle\/input\/the-oxfordiiit-pet-dataset\/images\/images\/Russian_Blue_166.jpg\" \\\n    --score_thres 0.5 \\\n    --iou_thres 0.4 \\\n    --save_dir \"\/kaggle\/working\/\" \\\n    --fname \"res_1.png\"","4fef95a2":"Image.open(\"\/kaggle\/working\/res_1.png\")","1e50aa1a":"!python inference.py \\\n    --config \"\/kaggle\/working\/retinanet_pet_detector\/config\/resnet50.yaml\" \\\n    --image \"\/kaggle\/input\/the-oxfordiiit-pet-dataset\/images\/images\/pug_190.jpg\"\\\n    --score_thres 0.7 \\\n    --iou_thres 0.4 \\\n    --save_dir \"\/kaggle\/working\/\" \\\n    --fname \"res_2.png\"","3dd262b8":"Image.open(\"\/kaggle\/working\/res_2.png\")","d4ebce21":"## **Modify Config file**:","75282071":"### Start Loop for train, validation and test :","76086415":"## Inference with saved weights:","2b714459":"> ## **Pet Face Detector \ud83d\udc41 \ud83d\udc36 \ud83d\udc31**","7935ef6d":"**We will again the run the script used above but this time we will run this script to create `training`, `validation` & `test` sets from the full dataset.**","87386876":"**Instantiate config for inference**:","573392b0":"## **Import dependencies**:","7c91767d":"To do inference we need to modify or create a config file for inference. An example config file is present in `\/kaggle\/working\/retinanet_pet_detector\/config\/resnet34.yaml`. The config file for inference should contain the following:\n\n- `model_backbone` (`str`) : resnet backbone used for the retinanet model.\n- `url` (`str`) : url or the path to where weights are saved.\n- `num_classes` (`int`) : total number of unique classes","65a4aea7":"**We can see that now we have 3 extra files `train.csv`, `valid.csv` & `test.csv`. This files correspond to the `train`, `validation` & `test` datasets respectively.**","131b4c1b":"## **Preprare Data**:","e7c2c0b0":"The `main` function accepts `argparse arguments` so we will first define a `Dictionary` with the args and convert it to `argparse.Namespace` instance","3c18a95a":"This is what our data looks like:","20208981":"### **Run Inference**:","98b3646d":"Before training we need to convert the data into a format that is compatible with the `training pipeline`. We will use `references\/data_utils.py` script to convert all the xml annotation files into a csv that stores all the annotations and path to the Images.\n\nThe resutant csv file will be saved as `\/{ouptut_dir}\/data-full.csv.`","da63192d":"By default our `trainer` uses `TensorBoard` to track training logs. Since `TensorBoard` doesn't really work in `kaggle`, for this experiment i am going to use `wandb` logger to log ecperiment results and then modify the config file accordingly to use the `wandb-logger`.","d6a78baf":"## **Setup the Kaggle-Environment:**","06fec7a6":"**View some images from the datasets:**","6dcf2ce3":"## **Train, Validation & Test** :","0bf1fdd8":"**Config for train:**"}}