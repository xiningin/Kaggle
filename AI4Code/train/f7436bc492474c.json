{"cell_type":{"afde17ec":"code","99559905":"code","db4d60fb":"code","a7b08dae":"code","590c3e51":"code","2a6f090b":"code","4807f6d2":"code","a27fa760":"code","e02221c1":"code","d4166890":"code","bf2225e2":"code","67321f96":"code","1bbdd6a1":"code","cd5609e3":"code","73a3a120":"code","acf84adf":"code","acc3d2a1":"code","2cd8c3da":"code","c88c8d43":"markdown","18db85d0":"markdown","0705aac9":"markdown","b2084d42":"markdown","89bf3757":"markdown","94de9d27":"markdown","2943c58d":"markdown","3d678f26":"markdown","95b4c0da":"markdown","01a74705":"markdown","11eca38b":"markdown","502bf7e2":"markdown"},"source":{"afde17ec":"import pandas as pd, numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer","99559905":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nsubm = pd.read_csv('..\/input\/sample_submission.csv')","db4d60fb":"train.head()","a7b08dae":"train['comment_text'][0]","590c3e51":"train['comment_text'][2]","2a6f090b":"lens = train.comment_text.str.len()\nlens.mean(), lens.std(), lens.max()","4807f6d2":"lens.hist();","a27fa760":"label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\ntrain['none'] = 1-train[label_cols].max(axis=1)\ntrain.describe()","e02221c1":"len(train),len(test)","d4166890":"COMMENT = 'comment_text'\ntrain[COMMENT].fillna(\"unknown\", inplace=True)\ntest[COMMENT].fillna(\"unknown\", inplace=True)","bf2225e2":"import re, string\nre_tok = re.compile(f'([{string.punctuation}\u201c\u201d\u00a8\u00ab\u00bb\u00ae\u00b4\u00b7\u00ba\u00bd\u00be\u00bf\u00a1\u00a7\u00a3\u20a4\u2018\u2019])')\ndef tokenize(s): return re_tok.sub(r' \\1 ', s).split()","67321f96":"n = train.shape[0]\nvec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n               smooth_idf=1, sublinear_tf=1 )\ntrn_term_doc = vec.fit_transform(train[COMMENT])\ntest_term_doc = vec.transform(test[COMMENT])","1bbdd6a1":"trn_term_doc, test_term_doc","cd5609e3":"def pr(y_i, y):\n    p = x[y==y_i].sum(0)\n    return (p+1) \/ ((y==y_i).sum()+1)","73a3a120":"x = trn_term_doc\ntest_x = test_term_doc","acf84adf":"def get_mdl(y):\n    y = y.values\n    r = np.log(pr(1,y) \/ pr(0,y))\n    m = LogisticRegression(C=4, dual=True)\n    x_nb = x.multiply(r)\n    return m.fit(x_nb, y), r","acc3d2a1":"preds = np.zeros((len(test), len(label_cols)))\n\nfor i, j in enumerate(label_cols):\n    print('fit', j)\n    m,r = get_mdl(train[j])\n    preds[:,i] = m.predict_proba(test_x.multiply(r))[:,1]","2cd8c3da":"submid = pd.DataFrame({'id': subm[\"id\"]})\nsubmission = pd.concat([submid, pd.DataFrame(preds, columns = label_cols)], axis=1)\nsubmission.to_csv('submission.csv', index=False)","c88c8d43":"And finally, create the submission file.","18db85d0":"The length of the comments varies a lot.","0705aac9":"## Introduction\n\nThis kernel shows how to use NBSVM (Naive Bayes - Support Vector Machine) to create a strong baseline for the [Toxic Comment Classification Challenge](https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge) competition. NBSVM was introduced by Sida Wang and Chris Manning in the paper [Baselines and Bigrams: Simple, Good Sentiment and Topic Classi\ufb01cation](https:\/\/nlp.stanford.edu\/pubs\/sidaw12_simple_sentiment.pdf). In this kernel, we use sklearn's logistic regression, rather than SVM, although in practice the two are nearly identical (sklearn uses the liblinear library behind the scenes).","b2084d42":"Fit a model for one dependent at a time:","89bf3757":"## Building the model\n\nWe will start by creating a *bag of words* representation, as a *term document matrix*. We'll use ngrams, as suggested in the NBSVM paper.","94de9d27":"We will create a list of all the labels to predict, and also create a 'none' label  to see how many comments have no labels and then summarize the dataset.","2943c58d":"Here's the basic naive bayes feature equation:","3d678f26":"few empty comments are to be removed or else  sklearn will complain.","95b4c0da":"It turns out that using TF-IDF gives even better priors than the binarized features used in the paper. Its not  mentioned in any paper before as per knowledge , but it improves leaderboard score from 0.59 to 0.55.","01a74705":"This creates a *sparse matrix* with only a small number of non-zero elements (*stored elements* in the representation  below).","11eca38b":"Here's a couple of examples of comments, one toxic, and one with no labels.","502bf7e2":"## Looking at the data\n\nThe training data contains a row per comment, with an id, the text of the comment, and 6 different labels that we'll try to predict."}}