{"cell_type":{"c9d141f0":"code","4d6745d3":"code","683a1f09":"code","67306bad":"code","632f03cb":"code","04622c8b":"code","b65b6e14":"code","c41b5d70":"code","7a46451a":"code","f051eb7a":"code","882550bc":"code","bd658536":"code","8265aada":"code","d6d8e587":"code","a6205644":"code","13135d81":"code","a4c4db68":"code","d044c9cb":"code","864a8b85":"code","7a2ad38b":"code","5f5db440":"code","cc800a98":"code","fe7bd46a":"code","8c8ef15c":"code","949b2f1d":"code","0fec290a":"markdown","756d448d":"markdown","22ddd0ce":"markdown"},"source":{"c9d141f0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom string import ascii_letters\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4d6745d3":"import janestreet\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set","683a1f09":"train_dataset = pd.read_csv('..\/input\/jane-street-market-prediction\/train.csv', nrows=100000)\ntrain_dataset.head()","67306bad":"train_dataset.info(verbose=True, null_counts=True)","632f03cb":"len(train_dataset)","04622c8b":"#remove rows where weight = 0\ntrain_dataset_weight = train_dataset[train_dataset.weight != 0]\ntrain_dataset_weight['action'] = ((train_dataset_weight['weight'].values * train_dataset_weight['resp'].values) > 0).astype('int')\ntrain_dataset_weight.head()\n","b65b6e14":"#Looking for good and bad days\nsum_days = pd.DataFrame(train_dataset_weight, columns = [\"date\",\"resp\"])\nsum_days = sum_days.groupby(\"date\", as_index=False).sum()\n\n#good days\n\ngood_days= sum_days[sum_days.resp >= 0]\nbad_days = sum_days[sum_days.resp < 0]\n\n","c41b5d70":"good_days_comp = train_dataset_weight.loc[train_dataset_weight['date'].isin(good_days['date'])]\nbad_days_comp = train_dataset_weight.loc[train_dataset_weight['date'].isin(bad_days['date'])]","7a46451a":"import matplotlib.pyplot as plt\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nlangs = ['Trades in bad days', 'Trades in Good days']\nstudents = [len(bad_days_comp),len(good_days_comp)]\nax.bar(langs,students)\nplt.show()","f051eb7a":"\ngood_days_comp['feature_80'].hist(bins=80, figsize=[3,3])\nbad_days_comp['feature_80'].hist(bins=80, figsize=[3,3])\nplt.axvline(x=good_days_comp['feature_80'].mean(), color='r', linestyle='dashed', linewidth=2)\nplt.axvline(x=bad_days_comp['feature_80'].mean(), color='g', linestyle='dashed', linewidth=2)","882550bc":"#Plot showing the number of features x accuracy\nplt.plot(sum_days['date'],sum_days['resp'].cumsum(),drawstyle='steps-mid')\nplt.ylabel('Resp')\nplt.xlabel('Days')\nplt.show()","bd658536":"bad_trades = train_dataset_weight.query(\"resp < 0\")\ngood_trades = train_dataset_weight.query(\"resp >= 0\")\n","8265aada":"from sklearn.impute import SimpleImputer\n\n# Imputation\nmy_imputer = SimpleImputer(strategy='mean')\n\nimputed_train_dataset_weight = pd.DataFrame(my_imputer.fit_transform(train_dataset_weight))\n\nimputed_train_dataset_weight.columns = train_dataset_weight.columns\n\nimputed_train_dataset_weight\n","d6d8e587":"#Check correlation in the dataset\n\n#sns.set_theme(style=\"white\")\n\n# Generate a large random dataset\nrs = np.random.RandomState(33)\nd = pd.DataFrame(data=rs.normal(size=(100, 26)),\n                 columns=list(ascii_letters[26:]))\n\n# Compute the correlation matrix\ncorr = bad_days_comp.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(35, 25))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","a6205644":"#Creating X, y dataset\ny = imputed_train_dataset_weight.action\nX = imputed_train_dataset_weight.drop(['action', 'date',\t'weight',\t'resp_1',\t'resp_2',\t'resp_3',\t'resp_4',\t'resp', 'ts_id'], axis=1)","13135d81":"# Divide data into training and validation subsets\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)","a4c4db68":"X_train","d044c9cb":"#Feature selsction using SelectPercentile\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\n\nmy_model = XGBClassifier(n_estimators=500,n_jobs=2, tree_method = 'gpu_hist')\n\naccuracy = []\n\n#for i in range(1, 131, 1):\n #   X_new = SelectKBest(k=i).fit_transform(X, y)\n  #  print('Numero de features mais significantes = ', i)\n   # X_train, X_valid, y_train, y_valid = train_test_split(X_new, y, train_size=0.8, test_size=0.2, random_state=0)\n    #print(my_model.fit(X_train, y_train, \n     #        early_stopping_rounds=50, \n      #       eval_set=[(X_valid, y_valid)], verbose=False))\n    \n   # predict = my_model.predict(X_valid)\n   # accur = accuracy_score(y_valid, predict, normalize=False)\n   # accuracy.append(accur)\n   # print(\"Accur = \", 100*(accur\/len(y_valid)))\n   # print(\"  \")\n   # print(\"---------------------------------------------------\")\n   # print(\"  \")\n    \n    ","864a8b85":"#Plot showing the number of features x accuracy\n#calcula = [(100*((accuracy[i])\/len(y_valid))) for i in range(len(accuracy))]\n#plt.plot(calcula)\n#plt.ylabel('Accuracy')\n#plt.xlabel('Numero de features')\n#plt.show()\n#calcula.index(max(calcula))","7a2ad38b":"selector = SelectKBest(k=118)\nX_new = selector.fit_transform(X, y)\nselected_features = pd.DataFrame(selector.inverse_transform(X_new), \n                                 index=X.index, \n                                 columns=X.columns)","5f5db440":"selected_features.head()","cc800a98":"selected_columns = selected_features.columns[selected_features.var() != 0]\nselected_columns","fe7bd46a":"X_model = imputed_train_dataset_weight[selected_columns]","8c8ef15c":"my_model.fit(X_model,y)","949b2f1d":"for (test_df, sample_prediction_df) in iter_test:\n    X_test = test_df.loc[:, selected_columns]\n    #Predict Target\n    y_preds = my_model.predict(X_test)\n    sample_prediction_df.action = y_preds\n    env.predict(sample_prediction_df)","0fec290a":"![image.png](attachment:image.png)\n# Jane Street Market Prediction","756d448d":"#    What bad days have in commom and what good days have in commom ?????","22ddd0ce":"for i in good_trades.columns:\n    good_trades[i].hist(bins=80, figsize=[5,5])\n    bad_trades[i].hist(bins=80, figsize=[5,5],alpha = 0.7)\n    plt.axvline(x=good_trades[i].mean(), color='r', linestyle='dashed', linewidth=2)\n    plt.axvline(x=bad_trades[i].mean(), color='g', linestyle='dashed', linewidth=2)\n    plt.title(i)\n    plt.autoscale() \n    plt.show()\n    \n    "}}