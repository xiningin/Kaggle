{"cell_type":{"4abb4959":"code","cddb841e":"code","a4a127de":"code","3592b8f2":"code","e3ce9640":"code","0e3fd1c8":"code","b03a3992":"code","101caa5d":"code","f180bbd7":"code","f706a5b0":"code","675c2275":"code","0645af8c":"code","68f5aaa5":"code","58ccf31b":"code","19078ebc":"code","3fd0ed00":"code","88c19c3e":"code","f68c0f5a":"code","5524071a":"code","86329b72":"code","de8505d6":"code","1fa6fba6":"code","3f4a0619":"code","8ed559ef":"code","9a855215":"code","de3ac3c0":"code","29643c15":"code","5cd6c26c":"markdown","60b7b53d":"markdown","0f8b5e22":"markdown","065a94a8":"markdown","13af89fe":"markdown","6c577af0":"markdown","03463488":"markdown","0fd979bc":"markdown"},"source":{"4abb4959":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","cddb841e":"train = pd.read_csv('\/kaggle\/input\/tensorflow-great-barrier-reef\/train.csv')","a4a127de":"train['n_annot'] = train['annotations'].apply(lambda x: len(eval(x)))","3592b8f2":"train_annot = train.query('n_annot > 0')","e3ce9640":"bboxes = {\n    'x': [], 'y': [], 'width': [], 'height': [], 'video_id': [], 'sequence': [], 'video_frame': [],\n}\n\nfor item in train_annot.itertuples():\n    annots = eval(item.annotations)\n    for annot in annots:\n        for k in annot.keys():\n            bboxes[k].append(annot[k])\n        bboxes['video_id'].append(item.video_id)\n        bboxes['sequence'].append(item.sequence)\n        bboxes['video_frame'].append(item.video_frame)\n    \nbboxes = pd.DataFrame(bboxes)","0e3fd1c8":"bboxes.describe()","b03a3992":"df = bboxes[['x', 'y', 'width', 'height']]\nsample_df = df.copy()\ng = sns.PairGrid(sample_df)\ng.map_upper(sns.histplot)\ng.map_lower(sns.histplot)\ng.map_diag(sns.histplot, kde=False)","101caa5d":"BBOX_SELECT_QUERY = 'width >= 40 and height >= 40 and width <= 128 and height <= 128'","f180bbd7":"df = bboxes.query(BBOX_SELECT_QUERY)\n\nplt.style.use('ggplot')\nfig, axs = plt.subplots(3, 1, figsize=(15, 10))\nfor i, d in df.groupby('sequence'):\n    video_id = d['video_id'].values[0]\n    ax = axs[video_id]\n    dd = d.groupby('video_frame').agg(sum_cots=('video_id', 'count'), video_frame=('video_frame', 'min'))\n    ax.plot(dd['video_frame'], dd['sum_cots'])\n    ax.set_title(f'Video: {video_id}')\nplt.suptitle(f'BBox: {BBOX_SELECT_QUERY}', fontsize=16)\nplt.tight_layout()\nprint(f'Rate: {len(df) \/ len(bboxes):.4f}')","f706a5b0":"image_cache = {}\n\ndef _clear_cache():\n    image_cache = {}\n    \n\ndef load_image(bbox_df, index, image_dir=\"..\/input\/tensorflow-great-barrier-reef\/train_images\/\"):\n    '''\n    load one image wich include specific bounding box\n    '''\n    df = bbox_df.copy()\n    item = df.loc[index]\n    video = item[\"video_id\"]\n    frame = item[\"video_frame\"]\n    if (video, frame) in image_cache:\n        img = image_cache[(video, frame)]\n    else:\n        img = plt.imread(f\"{image_dir}video_{video}\/{frame}.jpg\")\n        image_cache[(video, frame)] = img\n    \n    return img, (item.x, item.y, item.width, item.height)\n\n\ndef clip_bbox_image(img, label, clip_width=128, clip_height=128, image_width=1280, image_height=720):\n    '''\n    load image & clip region including a bounding box\n    return:\n      - img: clipped image array\n      - label: new bounding box label\n    '''\n    x, y, width, height = label\n    \n    # bbox \u304c\u753b\u50cf\u5185\u306b\u53ce\u307e\u308b\u3088\u3046\u306b width, height \u3092\u4fee\u6b63\n    width -= max(x + width - image_width, 0)\n    height -= max(y + height - image_height, 0)\n    \n    xs = min(max(0, x + (width \/\/ 2) - (clip_width \/\/ 2)), image_width - clip_width)\n    ys = min(max(0, y + (height \/\/ 2) - (clip_height \/\/ 2)), image_height - clip_height)\n    xe = xs + clip_width\n    ye = ys + clip_height\n    img_clip = img[ys:ye, xs:xe, :]\n\n    # set new clipped cordinate\n    offset_x = min(0, (x + width \/\/ 2) - clip_width \/\/ 2)\n    offset_y = min(0, (y + height \/\/ 2) - clip_height \/\/ 2)\n    offset_x_2 = max(0, x + width \/\/ 2 + clip_width \/\/ 2 - 1280)\n    offset_y_2 = max(0, y + height \/\/ 2 + clip_height \/\/ 2 - 720)\n    x = max(0, clip_width \/\/ 2 - width \/\/ 2) + offset_x + offset_x_2\n    y = max(0, clip_height \/\/ 2 - height \/\/ 2) + offset_y + offset_y_2\n    \n    return img_clip, (x, y, width, height)\n\n\ndef add_noise(img, label):\n    x, y, width, height = label\n    img_noise = img.copy()\n    img_noise[y:y + height, x:x + width, :] = np.random.randint(0, 255, (height, width, 1))\n    return img_noise\n\n\ndef create_psgan_input(df, index):\n    img, label = load_image(df, index)\n    img_clip, label = clip_bbox_image(img, label)\n    img_noise = add_noise(img_clip, label)\n    img_double = np.concatenate((img_clip, img_noise), axis=1)\n    \n    return img_double, label","675c2275":"def plot_clipped_bbox(\n    df,\n    index,\n    ax=None,\n    figsize=(30, 5),\n):\n    \"\"\"\n    Plot reef image. If `show_annotations` is True, create boxes\n    with the annotations for starfish.\n    \"\"\"\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n        \n    img_double, label = create_psgan_input(df, index)\n    im = ax.imshow(img_double, vmin=0, vmax=255)\n    ax.axis(\"off\")\n    \n    return ax","0645af8c":"n_figs = 12\nn_cols = 3\nn_rows = (n_figs - 1) \/\/ n_cols + 1\nfig, axs = plt.subplots(n_rows, n_cols, figsize=(8 * n_cols, 4 * n_rows))\naxs = axs.ravel()\n\ndf = bboxes.copy()\ndf = df.query(BBOX_SELECT_QUERY) # only extract sufficient large bbox\n\nindexes = [idx for idx in df.sample(n_figs).index.values]\n\nfor ax, index in zip(axs, indexes):\n    plot_clipped_bbox(df, index, ax=ax)","68f5aaa5":"import os\nfrom pathlib import Path\nimport shutil\n\n\nout_dir = Path('\/kaggle\/working\/psgan_datasets')\nif os.path.isdir(out_dir):\n    shutil.rmtree(out_dir)\n\n# create directory\nout_dir.mkdir(parents=True, exist_ok=True)\nimage_dir = out_dir \/ 'images' \/ 'train'\nlabel_dir = out_dir \/ 'bbox' \/ 'train'\nimage_dir.mkdir(parents=True, exist_ok=True)\nlabel_dir.mkdir(parents=True, exist_ok=True)\n! tree psgan_datasets","58ccf31b":"from os import listdir\n\ndef show_one_image(root_dir):\n    plt.style.use('default')\n\n    image_path = os.path.join(root_dir, listdir(root_dir)[0])\n    im = plt.imread(image_path)\n    plt.imshow(im)\n    plt.axis('off')","19078ebc":"import json\nfrom PIL import Image\nfrom tqdm import tqdm\nimport cv2\n\n\ndf = bboxes.copy()\ndf = df.query(BBOX_SELECT_QUERY)\ndf = df.sample(1200, random_state=0) # only use 1200 sample for test\n\nfor item in tqdm(df.itertuples(), total=len(df)):\n    img, label = create_psgan_input(bboxes, item.Index)\n    label = [int(x) for x in label]\n    x, y, width, height = label\n    label = {\n        'x': x * 2,\n        'y': y * 2,\n        'w': (x + width) * 2,\n        'h': (y + height) * 2,\n    }\n    \n    # write file\n    label_file_name = f'{item.Index}.json'\n    with open(label_dir \/ label_file_name, 'w') as f:\n        json.dump(label, f)\n        \n    # resize image\n    img = cv2.resize(img, dsize=(512, 256), interpolation=cv2.INTER_CUBIC)\n    im = Image.fromarray(img)\n    im.save(image_dir \/ f'{item.Index}.png')\n    \nprint('done!')","3fd0ed00":"show_one_image('psgan_datasets\/images\/train')","88c19c3e":"NUM_VALID_SAMPLE = 100","f68c0f5a":"df = bboxes[['x', 'y', 'width', 'height']]\nsample_df = df.query(BBOX_SELECT_QUERY)\ng = sns.PairGrid(sample_df)\ng.map_upper(sns.histplot)\ng.map_lower(sns.histplot)\ng.map_diag(sns.histplot, kde=False)","5524071a":"sample_df.cov()","86329b72":"mean = sample_df.mean().values\ncov = sample_df.cov().values","de8505d6":"data = np.random.multivariate_normal(mean, cov, size=(NUM_VALID_SAMPLE)).astype(int)\nsample = pd.DataFrame(data, columns=['x', 'y', 'width', 'height'])\nsample['x'].clip(0, 1280 - 40, inplace=True)\nsample['y'].clip(0, 720 - 40, inplace=True)\nsample['width'].clip(40, inplace=True)\nsample['height'].clip(40, inplace=True)\n\n\ng = sns.PairGrid(sample)\ng.map_upper(sns.histplot)\ng.map_lower(sns.histplot)\ng.map_diag(sns.histplot, kde=False)\nsample","1fa6fba6":"image_cache = {}\ndef load_image_by_id(image_id, image_dir=\"..\/input\/tensorflow-great-barrier-reef\/train_images\/\"):\n    if image_id in image_cache:\n        img = image_cache[image_id]\n    else:\n        video, frame = image_id.split('-')\n        img = plt.imread(f\"{image_dir}video_{video}\/{frame}.jpg\")\n        image_cache[image_id] = img\n    \n    return img\n\ndef create_psgan_input_for_valid(df, index, label):\n    df = df.copy()\n    item = df.loc[index]\n    image_id = f'{item.video_id}-{item.video_frame}'\n    \n    img = load_image_by_id(image_id)\n    img_clip, label = clip_bbox_image(img, label)\n    img_noise = add_noise(img_clip, label)\n    img_double = np.concatenate((img_clip, img_noise), axis=1)\n    \n    return img_double, label","3f4a0619":"def plot_clipped_bbox2(\n    df,\n    index,\n    ax=None,\n    figsize=(30, 5),\n):\n    \"\"\"\n    Plot reef image. If `show_annotations` is True, create boxes\n    with the annotations for starfish.\n    \"\"\"\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n        \n    img_double, label = create_psgan_input(df, index, label)\n    im = ax.imshow(img_double, vmin=0, vmax=255)\n    ax.axis(\"off\")\n    \n    return ax","8ed559ef":"df = train.query('n_annot == 0').sample(NUM_VALID_SAMPLE).reset_index(drop=True)\n\nn_figs = 12\nn_cols = 3\nn_rows = (n_figs - 1) \/\/ n_cols + 1\nfig, axs = plt.subplots(n_rows, n_cols, figsize=(6 * n_cols, 3 * n_rows))\naxs = axs.ravel()\n\nfor i, frame, label in zip(range(n_figs), df.itertuples(), sample.itertuples()):\n    label = label._asdict()\n    del label['Index']\n    label = label.values()\n    \n    img, label = create_psgan_input_for_valid(df, i, label)\n    ax = axs[i]\n    im = ax.imshow(img, vmin=0, vmax=255)\n    ax.axis(\"off\")\n    \nplt.suptitle('Validation Set Sample', fontsize=16)\nplt.tight_layout()","9a855215":"import os\nfrom pathlib import Path\nimport shutil\n\n\nout_dir = Path('\/kaggle\/working\/psgan_datasets_val')\nif os.path.isdir(out_dir):\n    shutil.rmtree(out_dir)\n\n# create directory\nout_dir.mkdir(parents=True, exist_ok=True)\nimage_dir = out_dir \/ 'images' \/ 'test'\nlabel_dir = out_dir \/ 'bbox' \/ 'test'\nimage_dir.mkdir(parents=True, exist_ok=True)\nlabel_dir.mkdir(parents=True, exist_ok=True)\n! tree psgan_datasets_val","de3ac3c0":"import json\nfrom PIL import Image\nfrom tqdm import tqdm\nimport cv2\n\n\ndf = train.query('n_annot == 0').sample(NUM_VALID_SAMPLE).reset_index(drop=True)\n\nfor i, (frame, label) in enumerate(tqdm(zip(df.itertuples(), sample.itertuples()), total=NUM_VALID_SAMPLE)):\n    label = label._asdict()\n    del label['Index']\n    label = label.values()\n    img, label = create_psgan_input_for_valid(df, i, label)\n    \n    label = [int(x) for x in label]\n    x, y, width, height = label\n    label = {\n        'x': x * 2,\n        'y': y * 2,\n        'w': (x + width) * 2,\n        'h': (y + height) * 2,\n    }\n    \n    # write file\n    label_file_name = f'{i}.json'\n    with open(label_dir \/ label_file_name, 'w') as f:\n        json.dump(label, f)\n        \n    # resize image\n    img = cv2.resize(img, dsize=(512, 256), interpolation=cv2.INTER_CUBIC)\n    im = Image.fromarray(img)\n    im.save(image_dir \/ f'{i}.png')\n    \nprint('done!')","29643c15":"show_one_image('psgan_datasets_val\/images\/test')","5cd6c26c":"# Visualize Sampled Clips\n\n1. extract bounding BBox of specific size range\n2. crop 256x256 background image around the BBox","60b7b53d":"# Write Pedestrian-Synthesize GAN Input Data","0f8b5e22":"## Sample From Multi-Variate Distribution","065a94a8":"\u3053\u306e\u30b3\u30f3\u30da\u306f\u30a2\u30ce\u30c6\u30fc\u30b7\u30e7\u30f3\u306e\u3064\u3044\u305f\u30c7\u30fc\u30bf\u304c\u610f\u5916\u306b\u5c11\u306a\u3044\u3002\nGAN\u3067\u65b0\u305f\u306a\u753b\u50cf\u3092\u6c34\u5897\u3057\u3059\u308b\u3053\u3068\u3067\u3001Recall\u3092\u3042\u3052\u3089\u308c\u306a\u3044\u304b\uff1f\u3068\u3044\u3046\u53d6\u308a\u7d44\u307f\u3002\n\u8ad6\u6587\u3092\u898b\u308b\u3068detection\u306e\u7cbe\u5ea6\u306f\u3042\u307e\u308a\u5909\u308f\u3063\u3066\u3044\u306a\u3044\u306b\u898b\u3048\u308b\u304c\u3001[1] \u3067\u300cGAN\u306e\u30e2\u30c7\u30eb\u3092Pre Train\u3059\u308b\u3053\u3068\u3067\u5b66\u7fd2\u6642\u9593\u3092\u77ed\u7e2e\u3057\u305f\u300d\u3068\u3042\u308b\u306e\u3067\u3001[1]\u306e\u30e2\u30c7\u30eb\u306b\u6d41\u7528\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u304b\u3082\u3057\u308c\u306a\u3044\u3002\n\n\u3053\u3053\u3067\u306f\u3001\u6b69\u884c\u8005\u306e\u64ec\u4f3c\u753b\u50cf\u751f\u6210\u306b\u4f7f\u308f\u308c\u305f Pedestrian-Syntheis-GAN [2]\u3068\u3044\u3046\u30e2\u30c7\u30eb\u3092\u4f7f\u3046\u3002\nBBox\u3068\u30bf\u30fc\u30b2\u30c3\u30c8\u3092\u30ce\u30a4\u30ba\u3067\u7f6e\u304d\u63db\u3048\u305f\u753b\u50cf\u3092\u7528\u610f\u3059\u308c\u3070\u5b66\u7fd2\u3057\u3066\u304f\u308c\u308b\u3088\u3046\u306a\u306e\u3067\u3001\u30b3\u30f3\u30da\u306e\u4e00\u90e8\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\uff08256x256\u306e\u30af\u30ea\u30c3\u30d71000\u679a\u304f\u3089\u3044\uff09\u3092\u4f7f\u3063\u3066\u5b66\u7fd2\u3092\u8a66\u307f\u308b\u3002\n\n[1] https:\/\/arxiv.org\/abs\/1910.07169\n[2] https:\/\/github.com\/yueruchen\/Pedestrian-Synthesis-GAN\n","13af89fe":"## Train BBox Distribution","6c577af0":"# Visualize BBox Distribution","03463488":"# Make Varidation Data\n\n* find parameter of train distribution\n* sample from multi-valiate Gaussian distribution\n* adding sampled BBoxes into non-annotated frames","0fd979bc":"## Add BBox into Non-Annotated Frames"}}