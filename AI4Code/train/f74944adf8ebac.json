{"cell_type":{"e2bcd65e":"code","eae120f6":"code","b8c19e7d":"code","cd522b16":"code","d7cee602":"code","73ad9f8f":"code","665564d5":"code","b3f4c2b7":"code","b2250103":"code","13619526":"code","0514d15b":"code","6e845305":"code","19370a37":"code","7d4372e4":"code","8afe9b50":"code","9728a0b6":"code","8d3d501e":"markdown","6504cd48":"markdown","3ce76dce":"markdown","42a77376":"markdown","1adf5b8d":"markdown","9172840f":"markdown","15c4b07c":"markdown","057a0179":"markdown"},"source":{"e2bcd65e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n# Ctrl + Shift + P\nimport os\nprint(os.listdir(\"..\/input\"))\n\ntoy = True\n# Any results you write to the current directory are saved as output.","eae120f6":"from kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()\n\n(market_train_df, news_train_df) = env.get_training_data()\n\n# train_my_model(market_train_df, news_train_df)","b8c19e7d":"print(market_train_df.shape, news_train_df.shape)\nprint(market_train_df.columns)\nprint(news_train_df.columns)","cd522b16":"# We will reduce the number of samples for memory reasons\ntoy = False\n\nif toy:\n    market_train_df = market_train_df.tail(100)\n    news_train_df = news_train_df.tail(300)\nelse:\n    market_train_df = market_train_df.tail(3_000_000)\n    news_train_df = news_train_df.tail(6_000_000)\n\n","d7cee602":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import chain\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeClassifier\n\n","73ad9f8f":"\nmarket_train_df['time'] = market_train_df['time'].dt.floor('1D')\nmarket_train_df = market_train_df.reset_index(drop=True)\n","665564d5":"# market_train_df.head(5)\n\nX_pruned = market_train_df[['returnsOpenPrevMktres10', 'returnsOpenPrevMktres1']]\nY_pruned = market_train_df[['returnsOpenNextMktres10']]\n\nstart_test_data = int(0.8 * market_train_df.shape[0])\nX_train = X_pruned.iloc[:start_test_data]\nY_train = Y_pruned.iloc[:start_test_data]\n\nX_test = X_pruned.iloc[start_test_data:].reset_index(drop=True)\nY_test = Y_pruned.iloc[start_test_data:].reset_index(drop=True)\nY_test_metadata = market_train_df.iloc[-start_test_data:][['time', 'universe']].reset_index(drop=True)\n\n","b3f4c2b7":"Y_train.hist(bins=10)","b2250103":"#Hyperparameter : threshold\ndef assignConfidence(pthreshold, nthreshold, Y):\n    Y[(Y['returnsOpenNextMktres10'] > pthreshold)]  = 1.0\n    Y[(Y['returnsOpenNextMktres10'] < -nthreshold)] = -1.0\n    Y[(Y != 1.0) & (Y != -1.0)] = 0.0\n    return Y\n\ndef train_model(X, Y, **kwargs):\n    decTree = DecisionTreeClassifier(**kwargs)\n    X = X.ffill()\n    decTree.fit(X, Y)\n    return decTree\n\ndef overridden_predict(trained_model, X_test):\n    X_test = X_test.ffill()\n    y_pred = trained_model.predict(X_test)\n    y_pred = pd.DataFrame({'confidenceValue':y_pred})\n    return y_pred\n\ndef sigma_score(Y_pred, Y_test):\n    score = Y_pred['confidenceValue'] * Y_test['returnsOpenNextMktres10'] * Y_test_metadata['universe']\n    score = score.to_frame('score')\n    score['time'] = Y_test_metadata['time'].values\n    score_per_day = score.groupby('time').sum()\n    try:\n        sigma_score = score_per_day['score'].mean() \/ score_per_day['score'].std()\n    except:\n        sigma_score = 0.0\n        \n    return sigma_score\n","13619526":"# positive_threshold = [0.05, 0.075, 0.1]\n# negative_threshold = [0.05, 0.075, 0.1]\n# depth_levels = [3, 5, 9, 15, 19, None]\n# max_sig_score = -100\n# for pthresh in positive_threshold:\n#     for nthresh in negative_threshold:\n#         for max_depth in depth_levels:\n#             Y_train_modified = Y_train.copy()\n#             kwargs = {'max_depth':max_depth}\n#             Y_train_modified = assignConfidence(pthresh, nthresh, Y_train_modified)\n#             trained_model = train_model(X_train, Y_train_modified, **kwargs)\n#             Y_pred = overridden_predict(trained_model, X_test)\n#             sig_score = sigma_score(Y_pred=Y_pred, Y_test=Y_test)\n#             print(\"Sigma score for phresh, nthresh and max depth is \" + '{} {} {} '.format(pthresh, nthresh, max_depth)+\n#                  \"{}\".format(sig_score))\n#             print(trained_model.get_params()['max_depth'])\n#             if sig_score > max_sig_score:\n#                 best_pthresh = pthresh\n#                 best_nthresh = nthresh\n#                 if max_depth:\n#                     best_max_depth = min(max_depth, trained_model.get_params()['max_depth'])\n#                 else:\n#                     best_max_depth = trained_model.get_params()['max_depth']\n#                 max_sig_score = sig_score\n\n# print(\"The best parameters are pthresh, nthresh and max depth are \" + '{} {} {} '.format(best_pthresh, best_nthresh, best_max_depth))\n\n","0514d15b":"Y = pd.concat([Y_train, Y_test])\nX = pd.concat([X_train, X_test])","6e845305":"best_pthresh = 0.05\nbest_nthresh = 0.05\nbest_max_depth = 9\n\nY = pd.concat([Y_train, Y_test])\nY_modified = Y.copy()\nY_modified = assignConfidence(best_pthresh, best_nthresh, Y_modified)\nkwargs = {'max_depth':best_max_depth}\ntrained_model = train_model(X, Y_modified, **kwargs)\nY_pred = overridden_predict(trained_model, X)\n# sig_score = sigma_score(Y_pred=Y, Y_test=Y_test)","19370a37":"def make_predictions(trained_model,predictions_template_df, market_obs_df, news_obs_df):\n    sample = market_obs_df[['returnsOpenPrevMktres10', 'returnsOpenPrevMktres1']]\n    sample = sample.ffill()\n#     y_pred = lm.predict(sample)\n    y_pred = trained_model.predict(sample)\n    predictions_template_df.confidenceValue = y_pred.clip(-1, 1)\n","7d4372e4":"days = env.get_prediction_days()\n","8afe9b50":"for (market_obs_df, news_obs_df, predictions_template_df) in days:\n    make_predictions(trained_model, predictions_template_df, market_obs_df, news_obs_df)\n    env.predict(predictions_template_df)\n\nprint('Done!')\n\n","9728a0b6":"env.write_submission_file()\nprint(\"Fourth run\")","8d3d501e":"We take the best parameters and build our model. We have to join the training and testing set together as we don't want to loose the training on the testing set.","6504cd48":"In the below cell, we assign a confidence value +1 for 'returnsOpenNextMktres10' > threshold and -1 for 'returnsOpenNextMktres10' < threshold","3ce76dce":"The below cell;\nInput: X test data \nOutput : A dataframe with \"confidence values''","42a77376":"# This notebook seeks to train the best decision tree using only the market data","1adf5b8d":"* X_pruned only contains the labels 'returnsOpenPrevMktres10' and 'returnsOpenPrevMktres1'.\n* Y_pruned is the lable containing the forward looking return.","9172840f":"This serves as a boundary between the training and the actual submission.","15c4b07c":"This marks the end of the part where we get the data and divide into the train and test samples.","057a0179":"## Methodology:\n1.  The y-label given to us is the 10 day forward looking market returns.\n2.  We transform this into a classification problem by establishing thresholds.\n3.  If the market returns are greater than that threshold we assign a +1 and if it is lesser than a threshold we assign -1. Otherwise it is assigned a label of 1.\n4. It is trained on the first 80 % and tested to identify the best hyper-parameters on the remaining 20%."}}