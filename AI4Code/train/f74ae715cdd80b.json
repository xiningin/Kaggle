{"cell_type":{"0b1cd0cd":"code","8ca754c1":"code","d2b6f1c8":"code","33dec632":"code","fd3dff10":"code","8814d3c1":"code","41546ad5":"code","df1c597f":"code","dcb9209e":"code","ea8cd11a":"code","2e1b3ac0":"code","b1a06c2a":"code","21867133":"code","791ac6cb":"code","edb08b22":"code","97ed3c14":"code","1b4ab116":"code","11850295":"code","cea9d7c5":"code","447ab44e":"code","1c9a61c8":"code","da7b4dde":"code","2c458f81":"code","aba98948":"code","f9b17ef6":"code","338054d0":"code","04c47478":"code","3898bf93":"code","9deea813":"code","92e4f162":"code","103a996a":"code","2d26d75a":"code","29166d8a":"code","24eaa510":"code","f5f2d25e":"code","db8d52b9":"code","ef5cb43f":"code","5ec16992":"code","7ce9e513":"code","f3853d86":"code","3833a255":"code","606b7fac":"code","ad505f9f":"code","4983806a":"code","14e5cc05":"code","f16c9416":"code","4b8d3fa0":"code","10c734c2":"code","f9517bdf":"code","dd970098":"code","9141f0e3":"code","162fad94":"code","51fb3e32":"code","b92fb327":"code","cef953a4":"code","034d255b":"code","11a44e02":"code","0d5f1eb0":"code","728debeb":"code","e4a79814":"code","d5ae2dba":"code","0e2e11e8":"code","e86f0e10":"code","5e4673bd":"code","07fb6a2f":"code","4e79df05":"code","7ded8b3e":"code","44d8c7d5":"code","d5464163":"code","1d6fd33d":"code","e7913562":"code","4e6b2262":"code","4f609c83":"code","7ae0fb9e":"code","8a2d9ab5":"code","263d8e74":"code","50761015":"code","b0358339":"code","74383efc":"code","7acee59e":"code","07737d28":"code","c92d4ec4":"code","1fe7b121":"code","46f17fa6":"code","c75c4a7a":"code","36301e52":"code","9dbd336f":"code","a644190c":"code","fc7fc597":"code","dba68182":"code","5d84032c":"code","f46c6a37":"code","749c3ae5":"markdown","5e1703e4":"markdown","b4c399fa":"markdown","21b57a0e":"markdown","c2cc4d85":"markdown","3766eeec":"markdown","dca1f4b4":"markdown","f823c181":"markdown","2cf17b1a":"markdown","a1f3c434":"markdown","31b67905":"markdown","d51fcb88":"markdown"},"source":{"0b1cd0cd":"import os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.image import imread\n# Technically not necessary in newest versions of jupyter\n%matplotlib inline","8ca754c1":"import tensorflow as tf\nimport timeit\n\ndevice_name = tf.test.gpu_device_name()\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\nprint('Found GPU at: {}'.format(device_name))","d2b6f1c8":"my_data_dir = '..\/input\/cell-images-for-detecting-malaria\/cell_images\/cell_images'","33dec632":"# CONFIRM THAT THIS REPORTS BACK 'test', and 'train'\nos.listdir(my_data_dir) ","fd3dff10":"train_path = my_data_dir\n\n#test_path = my_data_dir+'\/test\/'\n#train_path = my_data_dir+'\/train\/'","8814d3c1":"os.listdir(train_path)","41546ad5":"os.listdir(train_path+'\/Parasitized')[0:10]","df1c597f":"os.listdir(train_path+'\/Uninfected')","dcb9209e":"para_cell = train_path+'\/Parasitized\/'+os.listdir(train_path+'\/Parasitized')[0]#","ea8cd11a":"para_img= imread(para_cell)\npara_img","2e1b3ac0":"plt.figure(figsize=(8,8))\nplt.imshow(para_img)","b1a06c2a":"para_img.shape","21867133":"unifected_cell_path = train_path+'\/Uninfected\/'+os.listdir(train_path+'\/Uninfected')[0]\nunifected_cell = imread(unifected_cell_path)\nplt.imshow(unifected_cell)","791ac6cb":"len(os.listdir(train_path+'\/Parasitized'))","edb08b22":"len(os.listdir(train_path+'\/Uninfected'))","97ed3c14":"unifected_cell.shape","1b4ab116":"para_img.shape","11850295":"# Other options: https:\/\/stackoverflow.com\/questions\/1507084\/how-to-check-dimensions-of-all-images-in-a-directory-using-python\ndim1 = []\ndim2 = []\nfor image_filename in os.listdir(train_path+'\/Uninfected'):\n    \n    if image_filename[-3:] == 'png':\n        img = imread(train_path+'\/Uninfected'+'\/'+image_filename)\n        d1,d2,colors = img.shape\n        dim1.append(d1)\n        dim2.append(d2)","cea9d7c5":"sns.jointplot(dim1,dim2,color='red',kind='kde')\n","447ab44e":"sns.jointplot(dim1,dim2,color='green',kind='hex')\n","1c9a61c8":"sns.jointplot(dim1,dim2,color='grey',kind='reg')\n","da7b4dde":"help(sns.jointplot)","2c458f81":"np.mean(dim1)","aba98948":"np.mean(dim2)","f9b17ef6":"image_shape = (130,130,3)# we can play with this size to check if the performance improves","338054d0":"from tensorflow.keras.preprocessing.image import ImageDataGenerator","04c47478":"help(ImageDataGenerator)","3898bf93":"image_gen = ImageDataGenerator(rotation_range=20, # rotate the image 20 degrees\n                               width_shift_range=0.10, # Shift the pic width by a max of 5%\n                               height_shift_range=0.10, # Shift the pic height by a max of 5%\n                               rescale=1\/255, # Rescale the image by normalzing it.\n                               shear_range=0.1, # Shear means cutting away part of the image (max 10%)\n                               zoom_range=0.1, # Zoom in by 10% max\n                               horizontal_flip=True, # Allo horizontal flipping\n                               fill_mode='nearest', # Fill in missing pixels with the nearest filled value\n                                validation_split=0.1)#splits the training set in 90 train \/ 10 cvalidation","9deea813":"plt.imshow(para_img)","92e4f162":"plt.imshow(image_gen.random_transform(para_img))","103a996a":"plt.imshow(image_gen.random_transform(para_img))","2d26d75a":"train_generator = image_gen.flow_from_directory(train_path,subset='training')","29166d8a":"validation_generator = image_gen.flow_from_directory(train_path,subset='validation')","24eaa510":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, MaxPooling2D","f5f2d25e":"help(MaxPooling2D)","db8d52b9":"with tf.device('\/GPU:0'):\n    \n\n    #https:\/\/stats.stackexchange.com\/questions\/148139\/rules-for-selecting-convolutional-neural-network-hyperparameters\n    model = Sequential()\n\n    model.add(Conv2D(filters=32, kernel_size=(3,3),input_shape=image_shape, activation='relu',))\n    model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n\n    model.add(Conv2D(filters=64, kernel_size=(3,3),input_shape=image_shape, activation='relu',padding='same'))\n    model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n\n    model.add(Conv2D(filters=128, kernel_size=(3,3),input_shape=image_shape, activation='relu',padding='same'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n\n\n    model.add(Flatten())\n\n\n    model.add(Dense(128))\n    model.add(Activation('relu'))\n\n    # Dropouts help reduce overfitting by randomly turning neurons off during training.\n    # Here we say randomly turn off 50% of neurons.\n    model.add(Dropout(0.5))\n\n    # Last layer, remember its binary so we use sigmoid\n    model.add(Dense(1))\n    model.add(Activation('sigmoid'))\n\n    model.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","ef5cb43f":"model.summary()","5ec16992":"from tensorflow.keras.callbacks import EarlyStopping","7ce9e513":"early_stop = EarlyStopping(monitor='val_loss',patience=5)","f3853d86":"help(image_gen.flow_from_directory)","3833a255":"batch_size = 32","606b7fac":"train_image_gen = image_gen.flow_from_directory(train_path,\n                                               target_size=image_shape[:2],\n                                                color_mode='rgb',\n                                               batch_size=batch_size,\n                                               class_mode='binary',\n                                               subset='training')","ad505f9f":"test_image_gen = image_gen.flow_from_directory(train_path,\n                                               target_size=image_shape[:2],\n                                               color_mode='rgb',\n                                               batch_size=batch_size,\n                                               class_mode='binary',shuffle=False,\n                                                  subset='validation')","4983806a":"train_image_gen.class_indices","14e5cc05":"import warnings\nwarnings.filterwarnings('ignore')","f16c9416":"\n# Configure the TensorBoard callback and fit your model\n\ntensorboard_callback = tf.keras.callbacks.TensorBoard(\"logs\")\n","4b8d3fa0":"with tf.device('\/GPU:0'):\n    results = model.fit_generator(train_image_gen,epochs=10,\n                              validation_data=test_image_gen,\n                             callbacks=[early_stop,tensorboard_callback])","10c734c2":"# Load the extension and start TensorBoard\n%load_ext tensorboard\n\n%tensorboard --logdir logs\n","f9517bdf":"%reload_ext tensorboard\n%tensorboard --logdir logs\n","dd970098":"!kill 457","9141f0e3":"from tensorflow.keras.models import load_model","162fad94":"#model.save('malaria_detector.h5')","51fb3e32":"model = load_model('malaria_detector.h5')","b92fb327":"losses = pd.DataFrame(model.history.history)","cef953a4":"losses[['loss','val_loss']].plot()","034d255b":"losses[['accuracy','val_accuracy']].plot()","11a44e02":"model.metrics_names","0d5f1eb0":"model.evaluate_generator(test_image_gen)","728debeb":"from tensorflow.keras.preprocessing import image","e4a79814":"# https:\/\/datascience.stackexchange.com\/questions\/13894\/how-to-get-predictions-with-predict-generator-on-streaming-test-data-in-keras\npred_probabilities = model.predict_generator(test_image_gen)","d5ae2dba":"pred_probabilities","0e2e11e8":"test_image_gen.classes","e86f0e10":"predictions = pred_probabilities > 0.5#this is the most important number for my\n                                      #classification report","5e4673bd":"# Numpy can treat this as True\/False for us\npredictions","07fb6a2f":"from sklearn.metrics import classification_report,confusion_matrix","4e79df05":"print(classification_report(test_image_gen.classes,predictions))","7ded8b3e":"#help(confusion_matrix)","44d8c7d5":"confusion_matrix(test_image_gen.classes,predictions)","d5464163":"cm = confusion_matrix(test_image_gen.classes,predictions)","1d6fd33d":"cm[0][1]","e7913562":"FP = []\nFN = []\nTP = []\nTN = []\n\nfor i in range(1,100,1):\n    \n    predictions = pred_probabilities > (i\/100)\n    confusion_matrix(test_image_gen.classes,predictions)\n    TP.append(confusion_matrix(test_image_gen.classes,predictions)[0][0])\n    TN.append(confusion_matrix(test_image_gen.classes,predictions)[0][1])\n    FP.append(confusion_matrix(test_image_gen.classes,predictions)[1][0])\n    FN.append(confusion_matrix(test_image_gen.classes,predictions)[1][1])\n    \n","4e6b2262":"#help(pd.DataFrame)","4f609c83":"df_cm = pd.DataFrame(TP,columns=['TP'])\n\ndf_cm","7ae0fb9e":"df_cm['TN'] = TN\ndf_cm['FP'] = FP\ndf_cm['FN'] = FN","8a2d9ab5":"df_cm","263d8e74":"plt.plot(range(1,100),df_cm[['TP','TN','FP','FN']])","50761015":"df_cm.head()","b0358339":"sns.pairplot(data=df_cm)","74383efc":"plt.figure(figsize=(9,7))\nsns.heatmap(df_cm.corr(),annot=True)","7acee59e":"plt.plot(range(1,100),df_cm[['TP','N']])","07737d28":"# Your file path will be different!\npara_cell","c92d4ec4":"my_image = image.load_img(para_cell,target_size=image_shape)","1fe7b121":"my_image","46f17fa6":"type(my_image)","c75c4a7a":"my_image = image.img_to_array(my_image)","36301e52":"type(my_image)","9dbd336f":"my_image.shape","a644190c":"my_image = np.expand_dims(my_image, axis=0)","fc7fc597":"my_image.shape","dba68182":"model.predict(my_image)","5d84032c":"train_image_gen.class_indices","f46c6a37":"test_image_gen.class_indices","749c3ae5":"# Great Job!","5e1703e4":"**TRIPLE CHECK WHERE YOUR FILES ARE DOWNLOADED AND UNZIPPED. PLEASE REVIEW VIDEO BEFORE POSTING TO QA FORUMS.**","b4c399fa":"Original notebook from Jose Portilla TF2 course ... Great Course!!!\n\n## The Data\n\n--------\n----------\n--------\n\nORIGINAL DATA SOURCE:\n\nThe dataset contains 2 folders - Infected - Uninfected\n\nAnd a total of 27,558 images.\n\nAcknowledgements\nThis Dataset is taken from the official NIH Website: https:\/\/ceb.nlm.nih.gov\/repositories\/malaria-datasets\/ \n\n**Note: We will be dealing with real image files, NOT numpy arrays. Which means a large part of this process will be learning how to work with and deal with large groups of image files. This is too much data to fit in memory as a numpy array, so we'll need to feed it into our model in batches. **\n\n### Visualizing the Data\n\n\n-------\nLet's take a closer look at the data.","21b57a0e":"# Creating the Model","c2cc4d85":"## Training the Model","3766eeec":"**Let's check how many images there are.**","dca1f4b4":"### Generating many manipulated images from a directory\n\n\nIn order to use .flow_from_directory, you must organize the images in sub-directories. This is an absolute requirement, otherwise the method won't work. The directories should only contain images of one class, so one folder per class of images.\n\nStructure Needed:\n\n* Image Data Folder\n    * Class 1\n        * 0.jpg\n        * 1.jpg\n        * ...\n    * Class 2\n        * 0.jpg\n        * 1.jpg\n        * ...\n    * ...\n    * Class n","f823c181":"## Early Stopping","2cf17b1a":"# Predicting on an Image","a1f3c434":"# Evaluating the Model","31b67905":"**Let's find out the average dimensions of these images.**","d51fcb88":"## Preparing the Data for the model\n\nThere is too much data for us to read all at once in memory. We can use some built in functions in Keras to automatically process the data, generate a flow of batches from a directory, and also manipulate the images.\n\n### Image Manipulation\n\nIts usually a good idea to manipulate the images with rotation, resizing, and scaling so the model becomes more robust to different images that our data set doesn't have. We can use the **ImageDataGenerator** to do this automatically for us. Check out the documentation for a full list of all the parameters you can use here!"}}