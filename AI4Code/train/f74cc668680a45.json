{"cell_type":{"6ed3fb60":"code","1972db35":"code","31c786b8":"code","4665884c":"code","dc69496f":"code","37c8bfb7":"code","dd975898":"code","b235730d":"code","626ff35f":"code","ff5a1dd0":"code","78c929e0":"code","d764c117":"code","a52295c5":"code","4a7733ef":"code","9d105077":"code","0877a96c":"code","0a9b5d0d":"code","ee045321":"code","0c3f9f02":"code","bd99315d":"code","de1cddaf":"code","8ac02263":"code","86c2f1bb":"code","36292a77":"code","d251bccf":"code","dc677649":"code","f218fdbd":"code","db451af8":"code","f4523814":"code","ab1b77c1":"code","e97987f3":"code","c6c9fac4":"code","313ff2d4":"code","d48c798c":"code","1c00c4c3":"code","52e0473d":"code","d472dfc8":"code","f1146057":"code","9a9a845b":"code","e992409a":"code","ef0f51c8":"code","6492b667":"code","21452d5f":"code","c9550a5d":"code","3791b7d3":"code","72228db3":"code","c21df0b9":"code","81bb62f2":"code","b34c5909":"code","4aca7e48":"code","c66f980a":"code","fc9c9043":"code","f6fd8895":"code","97e9004a":"code","152c640f":"code","9d128d4f":"code","7d864e78":"code","74c8cec5":"code","a2588abc":"code","00f255c4":"code","b50ef1a6":"code","8f6da40e":"code","b82dedb8":"code","62cf5679":"code","d166bcc7":"code","d20aea26":"code","30433839":"code","16ea20ac":"code","d92896db":"code","719d9638":"code","ea8371a6":"code","7d39157a":"code","3738090f":"code","523d6f08":"code","84ecf881":"code","883689f3":"code","c9c2bc3a":"markdown","852146a9":"markdown","3681a5b8":"markdown","40392491":"markdown","1ef2b40e":"markdown","29a3607e":"markdown","f5c9b487":"markdown","da03b6ec":"markdown","2b6a5908":"markdown","75a83054":"markdown","ab99d15a":"markdown","5b88364c":"markdown","266239a9":"markdown","b407e936":"markdown","e5d54424":"markdown","6712dc17":"markdown","115b3940":"markdown","4165259a":"markdown","06af78f4":"markdown","25c3e16e":"markdown","ad30313f":"markdown","ddf6bc90":"markdown","e62fdf41":"markdown","018945a7":"markdown","d1a3639f":"markdown","d65e7a04":"markdown","a87b7e17":"markdown","e055f6a3":"markdown","3f7b718b":"markdown","1452ab98":"markdown","def26324":"markdown","9eef5781":"markdown","2b9acf3d":"markdown","d5c67010":"markdown","5c8c062c":"markdown","36e95da0":"markdown","f98dd92d":"markdown","5ad2d8cf":"markdown","b89155c0":"markdown","a5fe5c97":"markdown","6a9ec71d":"markdown","f8af0a0d":"markdown","c9c08755":"markdown","a07bd0ee":"markdown","4fe7378e":"markdown","04f7d234":"markdown","bb57f3cf":"markdown","a53ff276":"markdown","bf2c2b98":"markdown","e3daff15":"markdown","170c769b":"markdown","8069348c":"markdown","a1a5ad69":"markdown","d1b4c8fc":"markdown","a941c3a3":"markdown","34406ce0":"markdown","bc0cdfe2":"markdown","229f9d90":"markdown","79f935e2":"markdown","05d04341":"markdown","3337a92f":"markdown","4853d83d":"markdown","4df45cd3":"markdown","8b6e1a72":"markdown","928f99a5":"markdown","71e95adc":"markdown","3ebeb6ec":"markdown"},"source":{"6ed3fb60":"# Ignore warnings :\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# Handle table-like data and matrices :\nimport numpy as np\nimport pandas as pd\nimport math \nimport itertools\n\n# Modelling Helpers :\nfrom sklearn.preprocessing import Imputer , Normalizer , scale\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import GridSearchCV , KFold , cross_val_score\n\n\n\n# Evaluation metrics :\n\n# Regression\nfrom sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error \n\n# Classification\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n\n\n# Deep Learning Libraries\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization\nfrom keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau, LearningRateScheduler\nfrom keras.utils import to_categorical\n\n\n# Visualisation\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nimport missingno as msno\n\n\n# Configure visualisations\n%matplotlib inline\nmpl.style.use( 'ggplot' )\nplt.style.use('fivethirtyeight')\nsns.set(context=\"notebook\", palette=\"dark\", style = 'whitegrid' , color_codes=True)","1972db35":"# Center all plots\nfrom IPython.core.display import HTML\nHTML(\"\"\"\n<style>\n.output_png {\n    display: table-cell;\n    text-align: center;\n    vertical-align: middle;\n}\n<\/style>\n\"\"\");\n\n\n# Make Visualizations better\nparams = { \n    'axes.labelsize': \"large\",\n    'xtick.labelsize': 'x-large',\n    'legend.fontsize': 20,\n    'figure.dpi': 150,\n    'figure.figsize': [25, 7]\n}\nplt.rcParams.update(params)","31c786b8":"import os\nprint(os.listdir('..\/input\/'))","4665884c":"ratings = pd.read_csv('..\/input\/ratings.csv')\nmovies = pd.read_csv('..\/input\/movies.csv')\ndf_r = ratings.copy()\ndf_m = movies.copy()","dc69496f":"ratings.head()","37c8bfb7":"ratings.shape","dd975898":"ratings.describe()","b235730d":"ratings.drop(['timestamp'], axis=1, inplace=True)","626ff35f":"ratings.head()","ff5a1dd0":"movies.head()","78c929e0":"print('Shape: ', movies.shape, '\\n')\nmovies.info()","d764c117":"df_combined = pd.merge(ratings, movies, on = 'movieId')","a52295c5":"df_combined.head()","4a7733ef":"df_combined.shape","9d105077":"# Create a function to find genres in the dataset\n\ngenres = {} # create a dictionary to store different genre values\n\ndef find_genres():\n    for genre in movies['genres']:\n        words = genre.split('|')\n        for word in words:\n            genres[word] = genres.get(word, 0) + 1\n            \nfind_genres()","0877a96c":"genres","0a9b5d0d":"# replace '(no genres listed)' by 'None'\ngenres['None'] = genres.pop('(no genres listed)')","ee045321":"from wordcloud import WordCloud\n\nwordcloud = WordCloud(width=400, height=200, background_color = 'black', min_font_size=7).generate_from_frequencies(genres)\n\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","0c3f9f02":"df_n_ratings = pd.DataFrame(df_combined.groupby('title')['rating'].mean())\ndf_n_ratings['total ratings'] = pd.DataFrame(df_combined.groupby('title')['rating'].count())\ndf_n_ratings.rename(columns = {'rating': 'mean ratings'}, inplace=True)\n\ndf_n_ratings.sort_values('total ratings', ascending=False).head(10)","bd99315d":"plt.figure(figsize=(8,4))\nsns.distplot(df_n_ratings['total ratings'], bins=20)\nplt.xlabel('Total Number of Ratings')\nplt.ylabel('Probability')\nplt.show()","de1cddaf":"df_n_ratings.sort_values('mean ratings', ascending=False).head(10)","8ac02263":"print('Total no of users that gave rating of 5.0 : ', len(df_n_ratings.loc[df_n_ratings['mean ratings'] == 5]), '\\n')\nprint('Total no of Individual users that gave rating of 5.0 : ', len(df_n_ratings.loc[(df_n_ratings['mean ratings'] == 5) \n                                                                           & (df_n_ratings['total ratings'] == 1)]))","86c2f1bb":"plt.figure(figsize=(8,4))\nsns.distplot(df_n_ratings['mean ratings'], bins=30)\nplt.xlabel('Mean Ratings')\nplt.ylabel('Probability')\nplt.show()","36292a77":"sns.jointplot(x = 'mean ratings', y = 'total ratings', data = df_n_ratings )","d251bccf":"util_mat = df_combined.pivot_table(index = 'userId', columns = 'title', values = 'rating')\nutil_mat.head(20)","dc677649":"user_util_matrix = util_mat.copy()\n\n# We will fill the row wise NaN's with the corresponding user's mean ratings, so that we can carry out Pearson correlation.\n# Here we assume avg ratings for the movie that is not rated.\nuser_util_matrix = user_util_matrix.apply(lambda row: row.fillna(row.mean()), axis=1)\nuser_util_matrix.head(5)","f218fdbd":"user_util_matrix.T.corr()","db451af8":"user_corr_mat = user_util_matrix.T.corr()\ncorr_user_1 = user_corr_mat.iloc[0]","f4523814":"corr_user_1.sort_values(ascending=False, inplace=True)","ab1b77c1":"corr_user_1","e97987f3":"# NaN values are generated in corr() as the std dev is zero, which is required in calculating Pearson Similarity.\ncorr_user_1.dropna(inplace = True)","c6c9fac4":"# Neglect the 1st corr value as it is user1 itself\ntop50_corr_users = corr_user_1[1:51]","313ff2d4":"df_combined[ df_combined['userId'] == 1]","d48c798c":"# user1 has not rated 32 movie\ndf_combined[ (df_combined['userId'] == 1) & (df_combined['movieId'] == 32) ] ","1c00c4c3":"print('32nd Movie : ', movies['title'][ movies['movieId'] == 32 ].values)","52e0473d":"df_n_ratings.loc[['Twelve Monkeys (a.k.a. 12 Monkeys) (1995)']]","d472dfc8":"top50_users = top50_corr_users.keys()\n\ncount = 0\nusers = list()\nfor user in top50_users:\n    if df_combined[ (df_combined['userId'] == user) & (df_combined['movieId'] == 32) ]['rating'].sum()  :\n        count +=1\n        users.append(user)\n\nprint(count)","f1146057":"# Let's define a function to calculate what user1 will rate the movie\n# We use Weighted average of k similar users\n\ndef predict_rating():\n    sum_similarity = 0\n    weighted_ratings = 0\n    for user in users:\n        weighted_ratings += top50_corr_users.ix[user] * df_combined[ (df_combined['userId'] == user) & \n                                                              (df_combined['movieId'] == 32) ]['rating'].sum()\n        sum_similarity += top50_corr_users.ix[user]\n\n    print(weighted_ratings \/ sum_similarity)\n    \n    \npredict_rating()","9a9a845b":"df_m[ df_m['movieId'] == 32]","e992409a":"# Let's find similar movies to jurassic Park\ndf_n_ratings.loc[['Jurassic Park (1993)']]","ef0f51c8":"item_util_matrix = util_mat.copy()\nitem_util_matrix.head(10)","6492b667":"# We will fill the column wise NaN's with the corresponding movie's mean ratings, so that we can carry out Pearson correlation.\n# Here we assume avg ratings for the user that has not a rated movie.\n\nitem_util_matrix = item_util_matrix.apply(lambda col : col.fillna(col.mean()), axis=0)\nitem_util_matrix.head(5)","21452d5f":"item_util_matrix.isna().sum().sum()","c9550a5d":"item_util_matrix.corr()","3791b7d3":"item_corr_matrix = item_util_matrix.corr()","72228db3":"jurassic_park_corr = item_corr_matrix['Jurassic Park (1993)']\njurassic_park_corr = jurassic_park_corr.sort_values(ascending=False)\njurassic_park_corr.dropna(inplace=True)","c21df0b9":"movies_similar_to_jurassic_park = pd.DataFrame(data=jurassic_park_corr.values, columns=['Correlation'], \n                                               index = jurassic_park_corr.index)\nmovies_similar_to_jurassic_park = movies_similar_to_jurassic_park.join(df_n_ratings['total ratings'])\nmovies_similar_to_jurassic_park.head(10)","81bb62f2":"movies_similar_to_jurassic_park = movies_similar_to_jurassic_park[1:]\nmovies_similar_to_jurassic_park[ movies_similar_to_jurassic_park['total ratings'] > 100 ].sort_values(ascending=False,\n                                                                                          by=['Correlation']).head(10)","b34c5909":"from surprise import Reader, Dataset, evaluate, KNNBasic, SVD, NMF\nfrom surprise.model_selection import GridSearchCV, cross_validate","4aca7e48":"reader = Reader(rating_scale=(0.5, 5.0))\n\ndata = Dataset.load_from_df( ratings[['userId', 'movieId', 'rating']], reader = reader )","c66f980a":"# Split data into k-folds\n# data.split(n_folds=5)","fc9c9043":"# Compute Mean Squared Distance Similarity\nsim_options = {'name' : 'msd'}\n\nalgo = KNNBasic(k=20, sim_options=sim_options )\ncross_validate(algo=algo, data=data, measures=['RMSE'], cv=5, verbose=True)","f6fd8895":"n_neighbours = [10, 20, 30]\nparam_grid = {'n_neighbours' : n_neighbours}\n\ngs = GridSearchCV(KNNBasic, measures=['RMSE'], param_grid=param_grid)\ngs.fit(data)\n\nprint('\\n\\n###############')\n# Best RMSE score\nprint('Best Score :', gs.best_score['rmse'])\n\n# Combination of parameters that gave the best RMSE score\nprint('Best Parameters :', gs.best_params['rmse'])\nprint('###############')","97e9004a":"algo = SVD()\ncross_validate(algo=algo, data=data, measures=['RMSE'], cv=5, verbose=True)","152c640f":"param_grid = {'n_factors' : [50, 75], 'lr_all' : [0.5, 0.05], 'reg_all' : [0.06, 0.04]}\n\ngs = GridSearchCV(algo_class=SVD, measures=['RMSE'], param_grid=param_grid)\ngs.fit(data)\n\nprint('\\n###############')\n# Best RMSE score\nprint('Best Score :', gs.best_score['rmse'])\n\n# Combination of parameters that gave the best RMSE score\nprint('Best Parameters :', gs.best_params['rmse'])\nprint('###############')","9d128d4f":"algo = NMF()\ncross_validate(data=data, algo=algo, measures=['RMSE'], cv=5, verbose=True)","7d864e78":"from keras.layers import Embedding, Input, dot, concatenate\nfrom keras.models import Model\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot","74c8cec5":"X = ratings.iloc[:,:2]\nY = ratings.iloc[:,2]\n\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 66)","a2588abc":"# The number of latent factors for the embedding\nn_latent_factors = 50\n\n# no of users and movies\nn_users, n_movies = len(ratings['userId'].unique()), len(ratings['movieId'].unique()) ","00f255c4":"# Model Architecture\n\n\n# User Embeddings\nuser_input = Input(shape=(1,), name='User_Input')\nuser_embeddings = Embedding(input_dim = n_users, output_dim=n_latent_factors, input_length=1, \n                              name='User_Embedding') (user_input)\nuser_vector = Flatten(name='User_Vector') (user_embeddings)\n\n\n# Movie Embeddings\nmovie_input = Input(shape=(1,), name='Movie_Input')\nmovie_embeddings = Embedding(input_dim = n_movies, output_dim=n_latent_factors, input_length=1, \n                               name='Movie_Embedding') (movie_input)\nmovie_vector = Flatten(name='Movie_Vector') (movie_embeddings)\n\n\n# Dot Product\nmerged_vectors = dot([user_vector, movie_vector], name='Dot_Product', axes=1)\nmodel = Model([user_input, movie_input], merged_vectors)","b50ef1a6":"SVG(model_to_dot( model,  show_shapes=True, show_layer_names=True).create(prog='dot', format='svg'))","8f6da40e":"model.summary()","b82dedb8":"optimizer = Adam(lr = 0.0005)\n\nmodel.compile(loss='mean_squared_error', optimizer = optimizer)","62cf5679":"batch_size = 128\nepochs = 20","d166bcc7":"history = model.fit(x=[x_train['userId'], x_train['movieId']], y=y_train, batch_size= batch_size, epochs=epochs, \n                    verbose= 2, validation_data=([x_test['userId'], x_test['movieId']], y_test))","d20aea26":"# Get training and test loss histories\ntraining_loss = history.history['loss']\ntest_loss = history.history['val_loss']\n\n# Create count of the number of epochs\nepoch_count = range(1, len(training_loss) + 1)\n\n# Visualize loss history\nplt.figure(figsize = (8,4))\nplt.plot(epoch_count, training_loss, 'r--')\nplt.plot(epoch_count, test_loss, 'b-')\nplt.legend(['Training Loss', 'Test Loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.show()","30433839":"score = model.evaluate([x_test['userId'], x_test['movieId']], y_test)\nprint()\nprint('RMSE: {:.4f}'.format(np.sqrt(score)))","16ea20ac":"# Model Architecture\n\n\n# User Embeddings\nuser_input = Input(shape=(1,), name='User_Input')\nuser_embeddings = Embedding(input_dim = n_users, output_dim=n_latent_factors, input_length=1, \n                              name='User_Embedding') (user_input)\nuser_vector = Flatten(name='User_Vector') (user_embeddings)\n\n\n\n# Movie Embeddings\nmovie_input = Input(shape=(1,), name='Movie_Input')\nmovie_embeddings = Embedding(input_dim = n_movies, output_dim=n_latent_factors, input_length=1, \n                               name='Movie_Embedding') (movie_input)\nmovie_vector = Flatten(name='Movie_Vector') (movie_embeddings)\n\n\n\n\n# Concatenate Product\nmerged_vectors = concatenate([user_vector, movie_vector], name='Concantenate')\ndense_layer_1 = Dense(100, activation='relu')(merged_vectors) \n# dense_layer_1 = Dropout(0.25) (dense_layer_1)\n# batchnorm_layer_1 = BatchNormalization()(dense_layer_1)\n# dense_layer_2 = Dense(64, activation='relu')(merged_vectors)\n\n\nresult = Dense(1)(dense_layer_1)\nmodel = Model([user_input, movie_input], result)","d92896db":"SVG(model_to_dot( model,  show_shapes=True, show_layer_names=True).create(prog='dot', format='svg'))","719d9638":"model.summary()","ea8371a6":"optimizer = Adam(lr=0.0002)","7d39157a":"model.compile(loss='mean_squared_error', optimizer=optimizer)","3738090f":"batch_size = 128\nepochs = 20","523d6f08":"history = model.fit(x=[x_train['userId'], x_train['movieId']], y=y_train, batch_size= batch_size, epochs=epochs, \n                    verbose= 2, validation_data=([x_test['userId'], x_test['movieId']], y_test))","84ecf881":"# Get training and test loss histories\ntraining_loss = history.history['loss']\ntest_loss = history.history['val_loss']\n\n# Create count of the number of epochs\nepoch_count = range(1, len(training_loss) + 1)\n\n# Visualize loss history\nplt.figure(figsize = (8,4))\nplt.plot(epoch_count, training_loss, 'r--')\nplt.plot(epoch_count, test_loss, 'b-')\nplt.legend(['Training Loss', 'Test Loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.show()","883689f3":"score = model.evaluate([x_test['userId'], x_test['movieId']], y_test)\n\nprint()\nprint('RMSE: {:.4f}'.format(np.sqrt(score)))","c9c2bc3a":"<a id=\"there_you_go_4.13\"><\/a>\n## 4.13) Plotting Validation Curves","852146a9":"<a id=\"there_you_go_4.4\"><\/a>\n## 4.4) Visualizing the Model Architecture","3681a5b8":"<a id=\"there_you_go_3.1\"><\/a>\n## 3.1) Create User-Item Matrix","40392491":"This signifies that every Movie is rated by atleast 1 user.","1ef2b40e":"<a id=\"there_you_go_3.2\"><\/a>\n## 3.2) Memory Based Collaborative Filtering","29a3607e":"# Why do we need Recommendation Systems?","f5c9b487":"**Tuning KNN using GridSearchCV**","da03b6ec":"# Contents\n1. [**Exploring the Dataset**](#there_you_go_1)\n> *  [1.1 Importing Libraries ](#there_you_go_1.1)\n  * [1.2 Extract dataset ](#there_you_go_1.2)\n  >> * [1.2.1 Ratings](#there_you_go_1.2.1)\n    * [1.2.2 Movies](#there_you_go_1.2.2)\n  * [1.3 Combining the Movies and Ratings Dataframe ](#there_you_go_1.3)\n2. [**Visualizing the Dataset**](#there_you_go_2)\n> * [2.1 Genres ](#there_you_go_2.1)\n  * [2.2 Heavily Rated Movies ](#there_you_go_2.2)\n  * [2.3 Highly rated Movies](#there_you_go_2.3)\n  * [2.4 Mean ratings vs Total number of ratings](#there_you_go_2.4)\n3. [**Collaborative Filtering**](#there_you_go_3)\n> * [3.1 Create User-Item Matrix ](#there_you_go_3.1)\n * [3.2 Memory Based CF ](#there_you_go_3.2)\n >> * [3.2.1 User Based CF](#there_you_go_3.2.1)\n     * [3.2.2 Item based CF](#there_you_go_3.2.2)\n * [3.3 Model Based CF ](#there_you_go_3.3)\n >> * [3.3.1 K-Nearest Neighbour](#there_you_go_3.3.1)\n    * [3.3.2 Singular Value Decomposition](#there_you_go_3.3.2)\n    * [3.3.3 Non-Negative Matrix Factorization](#there_you_go_3.3.3)\n4. [**Matrix Factorization using Deep Learning**](#there_you_go_4)\n> * [4.1 Splitting Data into Train and Validation Set ](#there_you_go_4.1)\n * [4.2 Building the Model using Embedding Layers ](#there_you_go_4.2)\n * [4.3 Architecture 1 ](#there_you_go_4.3)\n * [4.4 Visulaizing the Model Architecture](#there_you_go_4.4)\n * [4.5 Compiling the Model ](#there_you_go_4.5)\n * [4.6 Fitting the Model](#there_you_go_4.6)\n * [4.7 Plotting Validation Curves](#there_you_go_4.7)\n * [4.8 Evaluating RMSE](#there_you_go_4.8)\n * [4.9 Architecture 2](#there_you_go_4.9)\n * [4.10 Visulaizing the Model Architecture](#there_you_go_4.10)\n * [4.11 Compiling the Model ](#there_you_go_4.11)\n * [4.12 Fitting the Model](#there_you_go_4.12)\n * [4.13 Plotting Validation Curves](#there_you_go_4.13)\n * [4.14 Evaluating RMSE](#there_you_go_4.14)\n5. [**References**](#there_you_go_5)","2b6a5908":"<a id=\"there_you_go_2.1\"><\/a>\n## 2.1) Genres\nLets have a look at various genres in the Dataset.","75a83054":"1. As you can see there are over 296 users that have rated 5 stars, among which there are 289 individual raters ( only user to rate the movie 5 star ).\n\n2. So, this cannot be the lone factor that should be considered while recommending movies. As this factor only shows the preferences of a particular user.\n\n3. It would make a good recommendation system if we can use both the factors ( -> Highly Rated Movies and Heavily Rated Movies <- ) together.","ab99d15a":"Most of these movies stand among the Top 50 movies in the IMDB ratings even today.","5b88364c":"<a id=\"there_you_go_5\"><\/a>\n# 5) References\n1.  Linear Algebra - [Good for quick revision](https:\/\/www.kaggle.com\/mjbahmani\/linear-algebra-for-data-scientists)\n2. Stanford lectures - [Starts at Lecure 41 and you can proceed as much you need, covers CF, SVD and much more](https:\/\/www.youtube.com\/watch?v=1JRrCEgiyHM&index=41&list=PLLssT5z_DsK9JDLcT8T62VtzwyW9LNepV)\n3. Blogs - [Various Implementations of Collaborative Filtering](https:\/\/towardsdatascience.com\/various-implementations-of-collaborative-filtering-100385c6dfe0)\n4. Deep Learning Blog - [Implementing recommendation systems in keras](https:\/\/nipunbatra.github.io\/blog\/2017\/recommend-keras.html)\n5. Matrix Factorization - [Understanding how Matrix Factorization works](https:\/\/lazyprogrammer.me\/tutorial-on-collaborative-filtering-and-matrix-factorization-in-python\/)\n6. Surprise Package - [Library to implement Model-Based CF Algorithms](https:\/\/surprise.readthedocs.io\/en\/stable\/)","266239a9":"<a id=\"there_you_go_4.\"><\/a>\n## 4.8) Evaluating RMSE","b407e936":"As you can see the \"ratings\" dataframe has 4 columns.\n* **userId** - Every user is represented by an unique Id.\n* **movieId** - Every movie is represented by an uniue Id.\n* **rating** - Represents the rating given by the user to the corresponding movie.\n* **timestamp** - The time at which the rating was recorded.","e5d54424":"Here we will be using dimensionality reduction methods to improve robustness and accuracy of Memory-Based CF. Basically, we compress user-item matrix into a low dimension matrix. We use techniques like SVD which is a low-rank factorization method, PCA which is used for dimensionaliry reduction etc.\n\nModel-based methods are based on matrix factorization and are better at dealing with sparsity. \n* We will be using a \"Surprise\" library to implement SVD, KNN and NMF.\n* You can find its documentation here >  [https:\/\/surprise.readthedocs.io\/en\/stable\/](https:\/\/surprise.readthedocs.io\/en\/stable\/)\n* Surprise Library has almost all the algorithms implemented that are required for model-based Recommendation systems.","6712dc17":"<a id=\"there_you_go_4.9\"><\/a>\n## 4.9) Architecture 2\nIn this architecture we will be concatenating the latent factors and use Dense layers to calculate the predicted ratings. \n\nHave a look at the image below ->\n![Imgur](https:\/\/i.imgur.com\/GIe3gTZ.png)","115b3940":"<a id=\"there_you_go_3\"><\/a>\n# 3) Collaborative Filtering (CF)","4165259a":"To load a dataset from a pandas dataframe, you will need the **load_from_df()** method. You will also need a **Reader** object, but only the rating_scale parameter must be specified.\n\nThe Reader class is used to parse a file containing ratings.","06af78f4":"So, as you could see the latter architecture did a pretty good job in predicting the user ratings. You can try tuning the number of latent factors, also try changing the architecture, adding more layers (make sure it doesn't overfit) and introducing Dropout and BatchNorm layers to acheive even lower RMSE.","25c3e16e":"As you can see the \"movies\" dataframe has 3 columns:\n* **movieId** - Every movie is represented by an unique Id.\n* **title** - Movie which is represented by the corresponding movieId.\n* **genres** - Represents category of the movie.","ad30313f":"# What are Recommendation Systems?","ddf6bc90":"<a id=\"there_you_go_4.6\"><\/a>\n## 4.6) Fitting the Model","e62fdf41":"<a id=\"there_you_go_2.2\"><\/a>\n## 2.2) Heavily Rated Movies","018945a7":"We can see that - \n* Majority of the movies have less than 50 ratings.\n* The number of movies having more than 100 ratings is very low.\n\nRefer one cell above, we can see that there are only 3 movies with 300+ ratings.","d1a3639f":"\n<a id=\"there_you_go_4.5\"><\/a>\n## 4.5) Compiling the Model","d65e7a04":"<a id=\"there_you_go_2.3\"><\/a>\n## 2.3) Highly Rated Movies","a87b7e17":"<a id=\"there_you_go_2.4\"><\/a>\n## 2.4) Mean Ratings vs Total Number of Ratings","e055f6a3":"**Q) How do we calculate similarity?**\n\n**Ans.** There are many measures to calculate the similarity matrix, some of them are -->\n\n1) **Jaccard Similarity** - It is a statistic used for comparing the similarity and diversity of sample sets. It is defined as the size of the intersection divided by the size of the union of the sample sets.\n\n2) **Cosine Similarity** - It measures the angle between the ratings vector. If the angle is 0\u00b0, then they are vectors having same orientation and if the angle is 180\u00b0, then they are highly dissimilar vectors.\n\n3) **Pearson Similarity** - It is actually Centered-Cosine similarity. We subtract the mean ratings from the user ratings, so that the mean is centered at 0, and then calculate the cosine similarity.","3f7b718b":"<a id=\"there_you_go_3.2.2\"><\/a>\n### 3.2.2) Item Based Collaborative Filtering\n\n\n* It is quite similar to previous algorithm, but instead of finding user's look-alike, we try finding movie's look-alike. \n* Once we have movie's look-alike matrix, we can easily recommend alike movies to user who have rated any movie from the dataset.\n\n![Imgur](https:\/\/i.imgur.com\/wKMnQiU.jpg)","1452ab98":"<a id=\"there_you_go_1.2.2\"><\/a>\n### 1.2.2) Movies","def26324":"<a id=\"there_you_go_3.3.3\"><\/a>\n### 3.3.3) Non-Negative Matrix Factorization (NMF)","9eef5781":"<a id=\"there_you_go_4.14\"><\/a>\n## 4.14) Evaluating RMSE","2b9acf3d":"<a id=\"there_you_go_4.7\"><\/a>\n## 4.7) Plotting Validation Curves","d5c67010":"<a id=\"there_you_go_1.2.1\"><\/a>\n### 1.2.1) Ratings","5c8c062c":"# Rec Sys -> Collaborative Filtering & DL Techniques\n* **You can also view and contribute to the notebook below.**\n* **Github - [Recommender-Systems-with-Collaborative-Filtering-and-Deep-Learning-Techniques](https:\/\/github.com\/Chinmayrane16\/Recommender-Systems-with-Collaborative-Filtering-and-Deep-Learning-Techniques)**\n* **Do Star\/Upvote if you like it :)**","36e95da0":"For convinience, I will be be considering only the correlation of all users with the first user only.","f98dd92d":"Some insights that can be drawn are:\n\n1. The minimum rating given to the movie is 0,5 whereas the maximum rating given to the movies is 5.0\n2. The average rating that is the mean ratings given by the users to all the movies is 3.5\n3. The users have userId's in the range 1 - 610\n4. The movies have movieId's in the range 1 - 193609.( Note that 193609 is the highest movieId and not total number of movies. )\n\nHere, we will be dropping the timestamp attribute as we are not concerned with when the user rated a particular movie.","5ad2d8cf":"Every day we view and purchase products on Amazon, browse Netflix, stream music on Spotify. Have you ever wondered how accurately they know your tastes and preferences and make Recommendations based on your interests ?\n\nThe answer is they have personalized Recommendation Engines which uses advanced algorithms to recommend products to you... When you want to watch a new movie, you usually ask your friends for suggesting movies, they know your interests and accordingly they suggest you a good movie.\n\nSo, **Recommendation Systems work the same way, they are nothing but data filtering tools that uses algorithms to recommend most relevant items to a particular user.**\n\nNow, let us understand Why do we need Recommendation Systems and in the later part we will look at How Recommender Systems actually work?\n\n![](https:\/\/i.imgur.com\/DlXYzzg.jpg)","b89155c0":"Internet is a vast ocean of Information, it consists of millions of items, a really large catalogue of products. There are some Users that know what they are looking for whereas others have no idea what to look for in such a large library of resources, and thus, Recommendation Systems play a vital role.\n\nThere are some products which may be really good but have not gained **Popularity** since they have not been advertised, so recommendation systems help such items gain popularity by bringing such items to one's notice.\n\nIt even helps in **Ad Targeting**- Say you're looking to buy a new laptop on Internet, your recent searches are based on laptop suggestions, and soon you'll start seeing ads on websites offering discounts on laptops. So, Ad Targeting is an advertisement technique meant to deliver ads automatically by using specialized software and algorithms that place ads depending on the user\u2019s recent searches. Ad targeting was pegged to have secured 2.7 times as much revenue as non-targeted ads, as shown by a study conducted in 2009 by the Network Advertising Initiative.\n\nThus, it also helps in **Increasing Revenues** of the product's Organization.","a5fe5c97":"**Fine Tuning SVD using GridSearchCV**\n\n[Click here to view the parameters](https:\/\/surprise.readthedocs.io\/en\/stable\/matrix_factorization.html)","6a9ec71d":"<a id=\"there_you_go_1.3\"><\/a>\n## 1.3) Combining the Movies and Ratings DataFrame\nLet's have a combined view on both the ratings and movies dataframe.\n\nAnd for that we need to merge on \"movieId\" attribute since it is common between both the dataframes.","f8af0a0d":"So, let's calculate what ratings user 1 would give to the movie with the help of similarrity vector. And based on that rating, we can compare it with a threshold rating. If the rating is higher it will be visible to the active user in his\/her recommended list.","c9c08755":"<a id=\"there_you_go_3.3\"><\/a>\n## 3.3) Model Based Collaborative Filtering","a07bd0ee":"* There are lot of NaN values and that is because when we are calculating the Pearson correlation, if the rating vector has all the values same for eg -> [3.0 , 3.0, 3.0, 3.0, ....], then the **Standard Deviation** is zero and division by zero is undefined, and thus its correlation with any other rating vector is NaN.\n\n* Since there are many movies that are rated only by 1 user , there the whole column mean is filled with the rating of that user, and therefore it's Pearson correlation gives NaN values with any other column.","4fe7378e":"<a id=\"there_you_go_3.3.1\"><\/a>\n### 3.3.1) K-Nearest Neighbours (KNN)","04f7d234":"<a id=\"there_you_go_1\"><\/a>\n# 1) Exploring the Dataset\nThe dataset we are going to use is the MovieLens Dataset, which cotains 100k ratings of approximately 9000 movies by 700 users.\nLet's have a look at the dataset.","bb57f3cf":"<a id=\"there_you_go_2\"><\/a>\n# 2) Visualizations on the Dataset","a53ff276":"There are 2 approaches to Memory-Based CF -->\n\n1) **User-User Collaborative Filtering** - In this we we calculate similarity of all the users to the active user ( the user whom the prediction is for ).Then sort and filter the Top-N users to make predictions for the active user. This is usually very effective but takes a lot of time and resources. For example if Dennis and Davis like the same movies and a new movie comes out that Davis likes,then we can recommend that movie to Dennis because Davis and Dennis seem to like the same movies.\n\n2) **Item-Item Collaborative Filtering** - This is similar to User-User CF, just that we now compute similarity between items to recommend similar items. Eg. When you buy any product on Amazon, you will find this line \"Users who bought this item also bought...\", so Amazon uses item-item CF widely, Mind that I'm not saying they use only item-item CF, they have hybrid techniques to better suit users of even unique interests.\n\nItem-Item CF are a lot faster than User-User CF. and secondly user profiles changes quickly and the entire system model has to be recomputed, whereas item's average ratings doesn't change that quickly, and this leads to more stable rating distributions in the model, so the model doesn't have to be rebuilt as often.","bf2c2b98":"<a id=\"there_you_go_4.12\"><\/a>\n## 4.12) Fitting the Model","e3daff15":"<a id=\"there_you_go_4.11\"><\/a>\n## 4.11) Compiling the Model","170c769b":"<a id=\"there_you_go_4.10\"><\/a>\n## 4.10) Visualizing the Model Architecture","8069348c":"Below, we have list of all movies that user 1 has ever rated.","a1a5ad69":"Many more Visualizations can be drawn and many different conclusions can be inferred, But here, I'm going to focus on Collaborative Filtering.\n\nSo, let's proceed to what is Collaborative Filtering and how it is used in Recommendation Systems?","d1b4c8fc":"<a id=\"there_you_go_3.2.1\"><\/a>\n### 3.2.1) User based Collaborative Filtering","a941c3a3":"<a id=\"there_you_go_4.3\"><\/a>\n## 4.3) Architecture 1","34406ce0":"* Here, as you can see every Data Point represents a distinct Movie, with y-coordinate representing the total no of users which has rated that movie and x-coordinate representing the mean of all the ratings of the corresponding users.\n* Also you can see that there is a huge Density in the region corresponding to 0-50 no of users and between mean rating 3-4 .","bc0cdfe2":"<a id=\"there_you_go_2.1.1\"><\/a>\n### 2.1.1) Genre WordCloud","229f9d90":"**Collaborative filtering** is the process of filtering for information or patterns using techniques involving collaboration among multiple agents, viewpoints, data sources.Basically, it is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users. \n\nThere are 2 approaches to CF -->\n\n1) **Memory-Based CF** - It is an approach which finds similarity between users or between items to recommend similar items. Examples include Neighbourhood-based CF and Item-based\/User-based top-N recommendations.\n\n2) **Model-Based CF** - In this approach we use different data mining, machine learning algorithms to predict users' rating of unrated items.  Examples include Singular Value Decomposition (SVD) , Principal Component Analysis (PCA) etc.","79f935e2":"There are 30 similar users among the Top-50 similar users that have rated the movie \"The Twelve Monkeys\".\n\n* Now, let's calculate the rating user 1 would give to the movie, \n\n* **Predicted rating** = sum of [ (weights) * (ratings) ]  **\/** sum of  (weights)\n\nHere, *weights* is the correlation of the corresponding user with the first user.\n\n","05d04341":"Well there are pretty good changes of recommending this movie movie to the 1st user, since the rating is quite good.\n\nSo, this is how a naive User-based CF works. Predicted ratings are calculated similarly for every user, (obviously for the movies he's not rated) and depending upon the threshold rating, the movie is either displayed on his recommended list or discarded.","3337a92f":"<a id=\"there_you_go_4.2\"><\/a>\n## 4.2) Building the Model using Embedding Layers\n\nAn embedding is a mapping from discrete objects, such as words or movies in our case, to a vector of continuous values. These are used to find similarities between discrete objects. \n\nThe concept behind matrix factorization models is that the preferences of a user can be determined by a small number of hidden factors. And these are called as **Embeddings**.\n\n![Imgur](https:\/\/i.imgur.com\/zGQJFLD.png)\n\nAs you can see in the image, there are 2 features both for user and the items. These are the latent factors or the hidden factors. These factors have a value for the corresponding user and determines to what extend that user likes the feature.\n\nFor Eg. the features for the user could be ->\n* How much he likes action movies?\n* Whether he likes old movies?\n\nAnd the features for the movies could be ->\n* To what scale is it an action movie?\n* Whether the movie is recently released?\n\nAnd finally we take **dot product** which gives us the user's rating for the movie, \n\n**Estimated Rating** = [ (How much he likes action movies?) x (To what scale is it an action movie) ]  +  [ (Whether he likes old movies?) x (Whether the movie is recently released) ]","4853d83d":"<a id=\"there_you_go_3.3.2\"><\/a>\n### 3.3.2) Singular Value Decomposition (SVD)","4df45cd3":"<a id=\"there_you_go_4.1\"><\/a>\n## 4.1)Splitting Data into Train and Validation Set","8b6e1a72":"*Thank you...*\n\n*Do star\/upvote if you like it ;)*","928f99a5":"<a id=\"there_you_go_1.1\"><\/a>\n## 1.1) Importing the Libraries ","71e95adc":"<a id=\"there_you_go_4\"><\/a>\n# 4) Matrix Factorization using Deep Learning (Keras)","3ebeb6ec":"<a id=\"there_you_go_1.2\"><\/a>\n## 1.2) Extract Dataset"}}