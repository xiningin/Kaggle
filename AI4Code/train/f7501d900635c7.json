{"cell_type":{"4617aa1e":"code","9d0cce91":"code","db574d33":"code","849ec3ad":"code","043888fe":"code","00ca8bba":"code","ba03bc70":"code","15489187":"code","3ffe5ab3":"code","763fa213":"code","a5ccfc21":"code","b2041205":"code","40eec96f":"code","17d66b0f":"code","f624b621":"code","c0b4e278":"code","4d5cdbb1":"code","e6d220ba":"code","17d9de7f":"markdown","d953d572":"markdown","71b587c2":"markdown","8a75094a":"markdown","3f65135e":"markdown","4ecb4854":"markdown","b32c0c19":"markdown","30857482":"markdown","c076852b":"markdown"},"source":{"4617aa1e":"!pip install music21 ","9d0cce91":"import pandas as pd\n# from music21 import converter, corpus, instrument, midi, note, chord, pitch, meter\nfrom music21 import *\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\nimport numpy as np\n\nimport os # handle files \nfrom torch.utils.data import Dataset ","db574d33":"# https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.Conv1d.html\n# L_out = (L_in+2*padding-dilation*(kernel_size-1)-1)*0.5 + 1\ndef whatPad(L_in,L_out,kernel_size,stride,dilation):\n    pad = ((L_out-1)*stride-L_in+dilation*(kernel_size-1)+1)*0.5\n    return pad\n\nL_in = 128\nL_out = L_in\nkernel_size = 5\nstride = 1\ndilation = 1\n\nprint(whatPad(L_in,L_out,kernel_size,stride,dilation))","849ec3ad":"# Angepasste Dataset-Klasse\n# Liest ein Verzeichnis mit Komponisten-Unterverzeichnis mit Midi-Dateien ein.\n# Aus den Midi-Datein werden pro 'Part' 'num_subparts' viele Teile mit 'num_notes' zusammenh\u00e4ngenden Noten extrahiert.\n# Aus den Noten-Objekten werden Features ausgelesen (z.B. Midi-Zahl und Quarter Length).\n# Damit entsteht f\u00fcr jeden Subpart eine verschachtelte Liste, f\u00fcr die gilt L\u00e4nge = Anzahl Noten = n.\n# F\u00fcr jeden Note gibt es in dieser Liste eine eigene Liste mit den zur Note geh\u00f6rigen Features.\n# --> [[Midi-Zahl_0,Quarter_Length_0],[Midi-Zahl_1,Quarter_Length_1],...,[Midi-Zahl_n-1,Quarter_Length_n-1]]\n# oder f\u00fcr m = Anzahl Feature (pro Note)\n# --> [[Feature_0_0,Feature_0_1,...,Feature_0_m-1],...,[Feature_n-1_0,Feature_n-1_1,...,Feature_n-1_m-1]]\n# Erstellt Torch-Tensoren aus den Listen, deren 'shape' angepasst ist um den conv1d Layern als Input gef\u00fcttert zu werden.\nclass FeatureDataset(Dataset):\n    \n    def __init__(self, path, composer_list, num_notes, num_subparts, num_features):\n        \n        # Eingaben abspeichern, falls man bei einem bereits initialisierten FeatureDataset-Objekt nochmal darauf zugreifen m\u00f6chte\n        self.path = path\n        self.composer_list = composer_list\n        self.num_subparts = num_subparts\n        self.num_notes = num_notes\n        self.num_features = num_features\n        \n        # Leere Listen f\u00fcr das Extrahieren der Daten\n        subparts_notes = [] # wir extrahieren zu erst die subparts als Liste von Note-Objekten\n        x_train = [] # hier werden sp\u00e4ter die einzelnen subparts gesammelt\n        y_train = [] # hier werden die zu den subparts geh\u00f6renden Komponisten IDs gesammelt\n        \n        # Datei einlesen\n        composer_id = 0\n        for composer in self.composer_list: # alle Komponisten durchgehen\n            \n            print(composer)\n            print('----------*----------*----------')\n            \n            directory = os.path.join(self.path,composer) # bestimme das Verzeichnis f\u00fcr den Komponisten. Urpsrungspfad mit dem Komponistennamen zusammenf\u00fchren -> Unterverzeichnis\n            for file in os.listdir(directory): # alle Dateien im Verzeichnis des Komponisten durchgehen\n                filename = os.fsdecode(file) # Dateiname als String speichern, zum vergleichen in der n\u00e4chsten Zeile\n                if filename.endswith(\".mid\"): # wenn die Datei eine Midi-Datei ist, weiterverarbeiten, sonst ignorieren\n                    \n                    print(filename)\n                    \n                    path_to_midi = os.path.join(directory, filename) # Pfad zur Datei erstellen (s.o.)\n                    score = converter.parse(path_to_midi) # mit dem Music21 converter die Datei auslesen und in ein Score-Objekt umwandeln\n                    \n                    print(len(score.parts),' parts')\n                    print('----------*----------')\n                    \n                    # for i,part in enumerate(score.parts): # alle parts im Score Objekt durchegehen\n                    for part in score.parts: # Score Objekte bestehen eventuell aus mehreren Part Objekte. Daher alle Parts durchgehen.\n                        for _ in range(self.num_subparts): # num_subparts oft getNNotes aufrufen\n                            notes = self.getNNotes(part, self.num_notes)\n                            if len(notes)>0: # falls getNNotes keine leere Liste zur\u00fcckgegeben hat:\n                                subparts_notes.append(notes) # in subparts_notes ablegen\n                                y_train.append(composer_id) # den Komponisten in y ablegen\n                                \n            composer_id += 1 # n\u00e4chster Komponist\n        \n        \n        # Bis jetzt haben wir nur Noten-Objekte ausgelesen\n        \n        # Features aus den Noten-Objekten auslesen\n        for subpart in subparts_notes: # die einzelnen Subparts wieder durchlaufen\n            subpart_list = [] # leere Liste f\u00fcr jeden Subpart erstellen\n            for note in subpart: # die Noten im Subpart durchlaufen\n                feature_list = [] \n                feature_list.append(note.pitch.ps\/127) # MIDI-Zahl, normieren auf [0,1]\n                feature_list.append(note.quarterLength) # Dauer in Viertel-L\u00e4nge\n                subpart_list.append(feature_list.copy()) # Kopie(!) der einzelnen Features ablegen\n            x_train.append(subpart_list.copy())\n            \n        \n        # Erstellen von Tensoren.\n        # Das Torch-Netzwerk m\u00f6chte mit float32 Werten arbeiten\n        # torch.nn.CrossEntropyLoss() braucht die Y-Werte mit dtype = long\n        \n        self.X_train = torch.tensor(x_train, dtype=torch.float32) #float32\n        self.Y_train = torch.tensor(y_train, dtype=torch.long)\n        \n        print(\"Training Shape before reshape\", self.X_train.shape, self.Y_train.shape)\n        \n        # Conv1d Layer ben\u00f6tigen die Input-Werte in dieser bestimmten Form [Batch_Size, Input_Channel, Sequence_Length]\n        # f\u00fcr LSTM oder auch Linear (bzw. Dense), kann dies wieder anders aussehen.\n        \n        self.X_train = torch.reshape(self.X_train, (self.X_train.shape[0], self.num_features, self.X_train.shape[1]))\n        \n        print(\"Training Shape after reshape\", self.X_train.shape, self.Y_train.shape)\n    \n    # vermutlich f\u00fcr den Dataloader n\u00f6tig\n    def __len__(self):\n        return len(self.Y_train)\n    \n    # get-Methode muss entsprechend der Tensoren angepasst werden, ist hier recht simpel\n    def __getitem__(self, idx):\n        return self.X_train[idx], self.Y_train[idx]\n    \n    # n zusammenh\u00e4ngende Noten aus einem Part (zuf\u00e4lliger n-Subpart)\n    # Dazu wird ein zuf\u00e4lliger, aber zul\u00e4ssiger Startindex im Part ausgew\u00e4hlt, ab diesem werden dann die Note-Objekte ausgelesen\n    def getNNotes(self, part, n):  \n        notes_list = [] # leere Liste um die Note-Onjekte aufzunehmen\n        flat_part = part.flat # Aufl\u00f6sen der verschachtelten Struktur das Part-Objektes -> Flatten = 'platt machen'\n        noteIter= flat_part.getElementsByClass(note.Note) # Iterator \u00fcber alle Note-Objekte im aufgel\u00f6sten Part\n    \n        num_notes = len(noteIter) # damit hat die n-te Note den Index num_notes-1\n\n        '''\n        #Ist in der n\u00e4chsten IF-Abfrage enthalten\n        if numNotes < n: # der Part muss mindestens n Noten enthalten, sonst k\u00f6nnen wir keinen n-langen Subpart bilden\n            return notesList # -> R\u00fcckgabe von leerer Liste\n        '''\n        \n        max_start_index = num_notes-1-n # maximaler Index an dem man anfangen kann um einen n-langen Subpart zu bilden\n        \n        if max_start_index < 0: # Ist der berechnete Index negativ, sind weniger als n Noten im Part enthalten\n            return notes_list # -> R\u00fcckgabe von leerer Liste\n        \n        \n        start_index = random.randint(0,max_start_index) # wir k\u00f6nnen einen zuf\u00e4lligen Index zwischen 0 und dem maximal m\u00f6glichen Startindex w\u00e4hlen\n\n        for i in range(n): # n Noten ab dem Startindex in der Liste ablegen\n            notes_list.append(noteIter[start_index+i])\n\n        return notes_list","043888fe":"# Da eindimensionale Zeitreihendaten (wie oben beschrieben) vorliegen, gen\u00fcgen Conv1D-Layer, statt der z.B. aus der Bildverarbeitung bekannten Conv2D-Layer.\n# Die Hyperparameter sind zum Gro\u00dfteil durch Ausprobieren entstanden.\nclass CONV_LSTM1(nn.Module):\n    def __init__(self, num_classes, hidden_size, batch_s, num_features, num_filter1, num_filter2, num_fitler3):\n        super(CONV_LSTM1, self).__init__()\n        \n        self.debug = False # Falls True, printet der Forward Pass die Gestalten der Tensoren.\n        self.res = True # Falls Ture, wird die 'Abk\u00fcrzung' f\u00fcr den Residualblock genutzt. Falls False, ist das Netzwerk eine normales Conv-Net.\n        \n        # ----------*----------*----------\n        # Allgemeine Parameter\n        \n        self.batch_size = batch_s # batch size\n        self.num_classes = num_classes # Anzahl der Klassen (Komponisten), relevant f\u00fcr den letzten Layer\n        \n        # Conv Parameter\n        self.num_features = num_features # Anzahl der Inputchannels des ersten Conv-Layers\n        self.num_filter1 = num_filter1\n        self.num_filter2 = num_filter2\n        self.num_filter3 = num_filter3\n        \n        # LSTM Paramter\n        self.hidden_size = hidden_size # hidden state\n        self.num_layers = 1 # number of layers\n        \n        # ----------*----------*----------*----------*----------*----------*----------*----------*----------\n        # Layer\n        # ---------- Res Block 1 Start ----------\n        self.conv1_id = nn.Conv1d(self.num_features, out_channels=self.num_filter1, kernel_size = 1, stride = 1)\n        self.b_norm1_id = nn.BatchNorm1d(self.num_filter1)\n        \n        self.conv1a = nn.Conv1d(self.num_features, out_channels=self.num_filter1, kernel_size=5, padding = 2)\n        self.b_norm1a = nn.BatchNorm1d(self.num_filter1)\n        \n        self.conv1b = nn.Conv1d(self.num_filter1, out_channels=self.num_filter1, kernel_size=5, padding = 2)\n        self.b_norm1b = nn.BatchNorm1d(self.num_filter1)\n        \n        self.conv1c = nn.Conv1d(self.num_filter1, out_channels=self.num_filter1, kernel_size=5, padding = 2)\n        self.b_norm1c = nn.BatchNorm1d(self.num_filter1)\n        \n        # ---------- Res Block 1 Ende ----------\n        \n        self.drop1 = nn.Dropout(p=0.1) # Standard Dropout Layer f\u00fcr stabileres Training\n        \n        self.pool1 = nn.MaxPool1d(2, stride = 2) # Standard Pooling, auch hier 1-D\n        \n        # ---------- Res Block 2 Start ----------\n        self.conv2_id = nn.Conv1d(self.num_filter1, out_channels=self.num_filter2, kernel_size = 1, stride = 1)\n        self.b_norm2_id = nn.BatchNorm1d(self.num_filter2)\n        \n        self.conv2a = nn.Conv1d(self.num_filter1, out_channels=self.num_filter2, kernel_size=5, padding = 2)\n        self.b_norm2a = nn.BatchNorm1d(self.num_filter2)\n        \n        self.conv2b = nn.Conv1d(self.num_filter2, out_channels=self.num_filter2, kernel_size=5, padding = 2)\n        self.b_norm2b = nn.BatchNorm1d(self.num_filter2)\n        \n        self.conv2c = nn.Conv1d(self.num_filter2, out_channels=self.num_filter2, kernel_size=5, padding = 2)\n        self.b_norm2c = nn.BatchNorm1d(self.num_filter2)\n        \n        # ---------- Res Block 2 Ende ----------\n        \n        self.drop2 = nn.Dropout(p=0.25) # Standard Dropout Layer f\u00fcr stabileres Training\n        \n        self.pool2 = nn.MaxPool1d(2, stride = 2) # Standard Pooling, auch hier 1-D\n        \n        # ---------- Res Block 3 Start ----------\n        self.conv3_id = nn.Conv1d(self.num_filter2, out_channels=self.num_filter3, kernel_size = 1, stride = 1)\n        self.b_norm3_id = nn.BatchNorm1d(self.num_filter3)\n        \n        self.conv3a = nn.Conv1d(self.num_filter2, out_channels=self.num_filter3, kernel_size=5, padding = 2)\n        self.b_norm3a = nn.BatchNorm1d(self.num_filter3)\n        \n        self.conv3b = nn.Conv1d(self.num_filter3, out_channels=self.num_filter3, kernel_size=5, padding = 2)\n        self.b_norm3b = nn.BatchNorm1d(self.num_filter3)\n        \n        self.conv3c = nn.Conv1d(self.num_filter3, out_channels=self.num_filter3, kernel_size=5, padding = 2)\n        self.b_norm3c = nn.BatchNorm1d(self.num_filter3)\n        \n        # ---------- Res Block 3 Ende ----------\n        \n        self.drop3 = nn.Dropout(p=0.5) # Standard Dropout Layer f\u00fcr stabileres Training\n        \n        self.pool3 = nn.MaxPool1d(2, stride = 2) # Standard Pooling, auch hier 1-D\n        \n        # Die convolotional Layer sollen idealerweise kurzfristige Zusammenh\u00e4nge\/Muster aus der Zeitreihe herausarbeiten\n        # F\u00fcr langfristige Strukturen nutzt man zus\u00e4tzlich einen LSTM-Layer.\n        self.lstm1 = nn.LSTM(input_size=self.num_filter3, hidden_size=self.hidden_size, num_layers=self.num_layers, batch_first=True) \n        \n        self.flat1 = nn.Flatten()\n        \n        self.fc1 = nn.Linear(4096,1024)\n        \n        self.fc2 = nn.Linear(1024,512)\n        \n        self.fc3 = nn.Linear(512, self.num_classes)\n        # ----------*----------*----------*----------*----------*----------*----------*----------*----------\n        \n        \n    def forward(self,x):\n        if self.debug:\n            print('----------*--------- Forward Start ----------*----------')\n            print('Input Shape')\n            print(x.shape)\n            print('----------Res Block 1 Start--------')\n                \n        # ----------*----------*----------*----------*----------*----------\n        # Res Block 1\n        # ----------*----------*----------\n        # Layer 1_id - Conv\n        if self.res:\n            if self.debug:\n                print('conv1_id')\n            x_id = self.conv1_id(x)\n            if self.debug:\n                print(x_id.shape)\n        \n        # ----------*----------*----------\n        # Layer 1a - Conv Layer\n        if self.debug:\n            print('conv1a')\n        x_res = self.conv1a(x) \n        x_res = self.b_norm1a(x_res)\n        x_res = F.relu(x_res) \n        \n        # ----------*----------*----------\n        # Layer 1b - Conv Layer\n        if self.debug:\n            print(x_res.shape)\n            print('conv1b')\n            \n        x_res = self.conv1b(x_res)\n        x_res = self.b_norm1b(x_res)\n        x_res = F.relu(x_res) \n            \n        # ----------*----------*----------\n        # Layer 1c - Conv Layer\n        if self.debug:\n            print(x_res.shape)\n            print('conv1c')\n            \n        x_res = self.conv1c(x_res)\n        x_res = self.b_norm1c(x_res)\n        x_res = F.relu(x_res)\n        \n        if self.debug:\n            print(x_res.shape)\n            print('----------Res Block 1 Ende--------')\n        # ----------*----------*----------  \n        # Aktivierung \n        if self.res:\n            if self.debug:\n                print('Relu(x+x_res)')\n            x = x_id+x_res\n        else:\n            if self.debug:\n                print('Relu(x)')\n            x = x_res\n        x = F.relu(x)\n        \n        # Dropout \n        if self.debug:\n            print(x.shape)\n            print('drop1')\n        x = self.drop1(x)\n        \n        \n        # Pool\n        if self.debug:\n            print(x.shape)\n            print('pool1')\n        x = self.pool1(x)\n        \n        \n        # ----------*----------*----------*----------*----------*----------\n        # Res Block 2\n        if self.debug:\n            print(x.shape)\n            print('----------Res Block 2 Start--------')       \n        # ----------*----------*----------\n        # Layer 2_ind - Conv Layer\n        if self.res:\n            if self.debug:\n                print('conv2_id')\n            x_id = self.conv2_id(x)\n            if self.debug:\n                print(x_id.shape)\n        # ----------*----------*----------\n        # Layer 2a - Conv Layer\n        if self.debug:\n            print('conv2a')\n            \n        x_res = self.conv2a(x) \n        x_res = self.b_norm2a(x_res)\n        x_res = F.relu(x_res)  \n        # ----------*----------*----------\n        # Layer 2b - Conv Layer\n        if self.debug:\n            print(x_res.shape)\n            print('conv2b')\n            \n        x_res = self.conv2b(x_res)\n        x_res = self.b_norm2b(x_res)\n        x_res = F.relu(x_res) \n        \n        if self.debug:\n            print(x_res.shape)\n        \n        # ----------*----------*----------\n        # Layer 2c - Conv Layer\n        if self.debug:\n            print(x_res.shape)\n            print('conv2c')\n            \n        x_res = self.conv2c(x_res)\n        x_res = self.b_norm2c(x_res)\n        x_res = F.relu(x_res)\n        \n        if self.debug:\n            print(x_res.shape)\n            print('----------Res Block 2 Ende--------')\n        # ----------*----------*----------  \n        # Aktivierung \n        if self.res:\n            if self.debug:\n                print('Relu(x+x_res)')\n            x = x_id+x_res\n        else:\n            if self.debug:\n                print('Relu(x)')\n            x = x_res\n        x = F.relu(x)\n        \n        # Dropout \n        if self.debug:\n            print(x.shape)\n            print('drop2')\n        x = self.drop2(x)    \n        \n        # Pool\n        if self.debug:\n            print(x.shape)\n            print('pool2')\n        x = self.pool2(x)\n        \n        # ----------*----------*----------*----------*----------*----------\n        # Res Block 3\n        if self.debug:\n            print(x.shape)\n            print('----------Res Block 3 Start--------')       \n        # ----------*----------*----------\n        # Layer 2_ind - Conv Layer\n        if self.res:\n            if self.debug:\n                print('conv3_id')\n            x_id = self.conv3_id(x)\n            if self.debug:\n                print(x_id.shape)\n        # ----------*----------*----------\n        # Layer 3a - Conv Layer\n        if self.debug:\n            print('conv3a')\n            \n        x_res = self.conv3a(x) \n        x_res = self.b_norm3a(x_res)\n        x_res = F.relu(x_res)  \n        # ----------*----------*----------\n        # Layer 3b - Conv Layer\n        if self.debug:\n            print(x_res.shape)\n            print('conv3b')\n            \n        x_res = self.conv3b(x_res)\n        x_res = self.b_norm3b(x_res)\n        x_res = F.relu(x_res) \n        \n        if self.debug:\n            print(x_res.shape)\n        \n        # ----------*----------*----------\n        # Layer 3c - Conv Layer\n        if self.debug:\n            print(x_res.shape)\n            print('conv3c')\n            \n        x_res = self.conv3c(x_res)\n        x_res = self.b_norm3c(x_res)\n        x_res = F.relu(x_res)\n        \n        if self.debug:\n            print(x_res.shape)\n            print('----------Res Block 3 Ende--------')\n        # ----------*----------*----------  \n        # Aktivierung \n        if self.res:\n            if self.debug:\n                print('Relu(x+x_res)')\n            x = x_id+x_res\n        else:\n            if self.debug:\n                print('Relu(x)')\n            x = x_res\n        x = F.relu(x)\n        \n        # Dropout \n        if self.debug:\n            print(x.shape)\n            print('drop3')\n        x = self.drop3(x)    \n        \n        # Pool\n        if self.debug:\n            print(x.shape)\n            print('pool3')\n        x = self.pool3(x)\n        \n        # ----------*----------*----------*----------*----------*----------\n\n        if self.debug:\n            print(x.shape)\n            print('permute')\n        \n        # ----------*----------*----------\n        # Layer 6 - LSTM Layer\n        '''\n        x = x.permute(0,2,1) # wegen Batch_First = True im LSTM Layer \n        \n        if self.debug:\n            print(x.shape)\n            print('lstm')\n        \n        h_0_1 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #hidden state - 'cell state'\n        c_0_1 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #internal state - 'output'\n        x, (hn, cn) = self.lstm1(x, (h_0_1, c_0_1)) # dem LSTM muss Input, Hidden und Internal State \u00fcbergeben werden\n        '''\n        # ----------*----------*----------\n        # Layer 7 - Flat Layer\n        if self.debug:\n            print(x.shape)\n            print('flat')\n        \n        x = self.flat1(x)\n        \n        # ----------*----------*----------\n        # Layer 8 - Dense Layer\n        if self.debug:\n            print(x.shape)\n            print('fc1')\n  \n        x = self.fc1(x)\n        x = F.relu(x) \n        \n        # ----------*----------*----------\n        # Layer 9 - Dense Layer\n        if self.debug:\n            print(x.shape)\n            print('fc2')\n            \n        x = self.fc2(x)\n        x = F.relu(x)\n        \n        # ----------*----------*----------\n        # Layer 10 - Dense Layer Output\n        if self.debug:\n            print(x.shape)\n            print('fc3\/output')\n        \n        x = self.fc3(x)\n        \n        if self.debug:\n            print(x.shape)\n            print('----------*--------- Forward Ende ----------*----------')\n        \n        return x","00ca8bba":"# Datensatz einlesen und in ein Dataset \u00fcbertragen\npath = '..\/input\/musicnet-dataset\/musicnet_midis\/musicnet_midis\/'\npath_train = '..\/input\/musicnet-midis-testsplit\/musicnet_midis\/Training'\npath_test = '..\/input\/musicnet-midis-testsplit\/musicnet_midis\/Test'\n\ncomposerList = ['Brahms','Dvorak','Cambini','Faure','Haydn','Ravel']\n#composerList = ['Brahms','Ravel']\n\nn_Notes = 128\nn_Subparts = 25\nn_features = 2\n\n# Batchsize\nbatch_s = 10","ba03bc70":"os.listdir('..\/input\/musicnet-midis-testsplit\/musicnet_midis\/Training')\nos.listdir(path_test)","15489187":"feature_set = FeatureDataset(path_train, composerList, n_Notes, n_Subparts, n_features)\ntest_set = FeatureDataset(path_test, composerList, n_Notes, n_Subparts, n_features)","3ffe5ab3":"# Split\ntrain_size = int(0.8 * len(feature_set)) # split 0.2\nval_size = len(feature_set) - train_size\n\ntrain_dataset, val_dataset = torch.utils.data.random_split(feature_set, [train_size, val_size])\n\n# train_loader erstellen\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_s, shuffle = True)\nvalidation_loader = torch.utils.data.DataLoader(val_dataset, batch_size = batch_s)\n\ntest_loader = torch.utils.data.DataLoader(test_set, batch_size = batch_s)","763fa213":"num_classes = len(composerList)\ninput_size = n_Notes \n#hidden_size = int(input_size\/5)\nhidden_size = 64\n#num_features = 4\nlearning_rate = 0.0001 \n\nnum_filter1 = 64\nnum_filter2 = 128\nnum_filter3 = 256","a5ccfc21":"conv_lstm1 = CONV_LSTM1(num_classes, hidden_size, batch_s, n_features,num_filter1,num_filter2,num_filter3)\n\n# Loss und Optimizer festlegen, beide standard bei Multiclass-Klassifizierung\n# torch.nn.CrossEntropyLoss ist nicht = categorical cross entropy aus Tensorflow\ncriterion = torch.nn.CrossEntropyLoss() \noptimizer = torch.optim.Adam(conv_lstm1.parameters(), lr=learning_rate) ","b2041205":"# Zur Bewertung der Performance \ndef hit(target, pred): # nimmt Zielwerte und Vorhersagen an und vergleicht diese\n    # das LSTM gibt keine als Wahrscheinlichkeitsverteilung interpretierbaren Werte aus, deshalb muss Softmax angewandt werden.\n    # Dies ist eine Art Pytorch Eigenheit, man k\u00f6nnte auch im Modell auf den letzten Layer Softmax anwenden, dann funktioniert \n    # torch.nn.CrossEntropyLoss() aber nicht mehr.\n    \n    #pred_softmax = torch.softmax(pred, dim = 1)\n    \n    #print('----------*----------*----------')\n    pred_softmax = torch.log_softmax(pred.view(-1),0)\n    \n    #print(pred_softmax)\n    \n    # der folgenden Befehl k\u00f6nnte sicher auch einfach gestaltet werden\n    # detach() - entfernt Gradienten, welche einem Tensor zugewiesen werden\n    # reshape(-1) - pl\u00e4ttet (flatten) einen Tensor, d.h. wirft alle Werte in einer Dimension zusammen\n    # numpy() - wandelt den Tensor in ein Numpy Array um\n    # argmax() - gibt den Index mit dem maximalen Wert zur\u00fcck\n    # -> argmax gibt f\u00fcr die 1-Hot-Kodierung die Klasse zur\u00fcck,\n    # -> argmax gibt f\u00fcr einen Vektor mit Klassen-Wahrscheinlichkeiten, die Klasse mit der maximalen Wahrscheinlichkeit zur\u00fcck\n    \n    if target.detach().reshape(-1).numpy() == pred_softmax.detach().reshape(-1).numpy().argmax():\n        return True\n    else:\n        return False","40eec96f":"conv_lstm1.debug = False\nconv_lstm1.res = True","17d66b0f":"def perf_val(): # performance auf val-set\n    conv_lstm1.eval()\n    hits = 0\n    tries = 0\n    debug = False\n    for x_val, y_val in validation_loader:\n        if debug:\n            print('----------*----------')\n            print('x_val')\n            print(x_val.shape)\n            print(x_val)\n        \n        yhat = conv_lstm1(x_val)\n    \n    \n        for target, pred in zip(y_val, yhat):\n            tries += 1\n            if hit(target, pred):\n                hits +=1\n            \n    print('Tries')\n    print(tries)\n    print('Number of hits:')\n    print(hits)\n    print('Ratio')\n    print(hits\/tries)\n    # bei einem zuf\u00e4llig gewichteten\/untrainierten Modell w\u00fcrde man eine Treffsicherheit von 1:(Anzahl Klassen) erwarten\n    print('Random Ratio')\n    print(1\/num_classes)","f624b621":"def perf_test():# performance auf dem test set\n    conv_lstm1.eval()\n    hits = 0\n    tries = 0\n    for x_val, y_val in test_loader:\n        yhat = conv_lstm1(x_val)\n        for target, pred in zip(y_val, yhat):\n            tries += 1\n            if hit(target, pred):\n                hits +=1\n            \n    print('Tries')\n    print(tries)\n    print('Number of hits:')\n    print(hits)\n    print('Ratio')\n    print(hits\/tries)\n    # bei einem zuf\u00e4llig gewichteten\/untrainierten Modell w\u00fcrde man eine Treffsicherheit von 1:(Anzahl Klassen) erwarten\n    print('Random Ratio')\n    print(1\/num_classes)","c0b4e278":"num_epochs = 25\n\navg_losses = []\nconv_lstm1.train()\ndebug = False\nval_each = 1\ntest_each = 1\nfor epoch in range(num_epochs):\n    conv_lstm1.train()\n    epoch_losses = []\n    for x_batch, y_batch in train_loader:\n        output = conv_lstm1.forward(x_batch) #forward pass\n        optimizer.zero_grad()#caluclate the gradient, manually setting to 0\n        \n        if debug:\n            print(output.shape)\n            print(y_batch.shape)\n        #print(y_batch)\n        #print(torch.max(y_batch,1))\n        #loss = criterion(output.view(1,-1), y_batch)\n\n        \n        #output = torch.reshape(output, (output.shape[0], output.shape[2]))\n        loss = criterion(output, y_batch)\n        epoch_losses.append(loss.item())\n        loss.backward() #calculates the loss of the loss function\n         \n        optimizer.step() #improve from loss, i.e backprop\n        #losses.append(loss)\n    \n    epoch_avg = sum(epoch_losses)\/len(epoch_losses)\n    avg_losses.append(epoch_avg)\n    print(\"Epoch: %d, avg loss: %1.5f\" % (epoch+1, epoch_avg)) \n    if epoch % val_each == 0:\n        print('----------Val---------')\n        perf_val()\n        \n    if epoch % test_each == 0:\n        print('---------Test--------')\n        perf_test()\n    \n    print('---------*---------*----------*---------*---------*----------*---------*---------*----------')\n    ","4d5cdbb1":"# performance vor dem Training\nconv_lstm1.eval()\nhits = 0\ntries = 0\nfor x_val, y_val in validation_loader:\n    yhat = conv_lstm1(x_val)\n    for target, pred in zip(y_val, yhat):\n        tries += 1\n        if hit(target, pred):\n            hits +=1\n            \nprint('Tries')\nprint(tries)\nprint('Number of hits:')\nprint(hits)\nprint('Ratio')\nprint(hits\/tries)\n# bei einem zuf\u00e4llig gewichteten\/untrainierten Modell w\u00fcrde man eine Treffsicherheit von 1:(Anzahl Klassen) erwarten\nprint('Random Ratio')\nprint(1\/num_classes)","e6d220ba":"# performance vor dem Training\nconv_lstm1.eval()\nhits = 0\ntries = 0\nfor x_val, y_val in test_loader:\n    yhat = conv_lstm1(x_val)\n    for target, pred in zip(y_val, yhat):\n        tries += 1\n        if hit(target, pred):\n            hits +=1\n            \nprint('Tries')\nprint(tries)\nprint('Number of hits:')\nprint(hits)\nprint('Ratio')\nprint(hits\/tries)\n# bei einem zuf\u00e4llig gewichteten\/untrainierten Modell w\u00fcrde man eine Treffsicherheit von 1:(Anzahl Klassen) erwarten\nprint('Random Ratio')\nprint(1\/num_classes)","17d9de7f":"## Erzeugen des Feature-Set-Objekts und des Trainings- Und Validierungs-Loaders","d953d572":"# Nach dem Training","71b587c2":"# Hyperparamter f\u00fcr den Datensatz","8a75094a":"# Modell","3f65135e":"Quellen\n\nhttps:\/\/machinelearningmastery.com\/how-to-develop-convolutional-neural-network-models-for-time-series-forecasting\/\n\nhttps:\/\/stackoverflow.com\/questions\/56030884\/how-to-define-specific-number-of-convolutional-kernels-filters-in-pytorch\n\nhttps:\/\/www.kaggle.com\/hanjoonchoe\/cnn-time-series-forecasting-with-pytorch\n\nhttps:\/\/medium.com\/@sumanshusamarora\/understanding-pytorch-conv1d-shapes-for-text-classification-c1e1857f8533\n\nhttps:\/\/datascience.stackexchange.com\/questions\/78030\/multivariate-time-series-analysis-when-is-a-cnn-vs-lstm-appropriate\n\nhttps:\/\/discuss.pytorch.org\/t\/solved-concatenate-time-distributed-cnn-with-lstm\/15435\n\nhttps:\/\/d2l.ai\/chapter_convolutional-modern\/resnet.html\n\nhttp:\/\/jmir.sourceforge.net\/manuals\/jSymbolic_manual\/featureexplanations_files\/featureexplanations.html\n\nhttps:\/\/medium.com\/analytics-vidhya\/understanding-and-implementation-of-residual-networks-resnets-b80f9a507b9c\n\nhttps:\/\/ai.stackexchange.com\/questions\/3156\/how-to-select-number-of-hidden-layers-and-number-of-memory-cells-in-an-lstm","4ecb4854":"# Training","b32c0c19":"# Hyperparamter f\u00fcr das Modell","30857482":"# Performance auf dem Test-Set","c076852b":"# Das untrainierte Modell"}}