{"cell_type":{"5df647e9":"code","0748a4b2":"code","f4f46900":"code","8a7fb3b8":"code","6e4ecdba":"code","5ffee9b9":"code","a9e059f1":"code","58944ec0":"code","ceb28e78":"code","69b60a44":"code","a2b0f727":"code","ec2cff3b":"code","19995b82":"code","554ee52c":"code","6bc4c0f5":"code","9d301186":"code","fa7c9494":"code","023a7776":"code","937eb95e":"code","78a15df4":"code","e3402c1a":"code","f1dc1b8e":"code","68d76857":"code","a9517695":"code","42caf097":"code","08e37e97":"code","b90dbc6a":"code","dac60bdd":"code","150b9cd3":"code","749e764e":"code","8287ca94":"code","76dd14a9":"code","afbbe52b":"code","a57d3fe6":"code","55fd6bca":"code","009782a9":"code","b2044cf4":"code","b8124d11":"code","8b8dd1dd":"markdown","17938c77":"markdown","f4e1cac1":"markdown","848c4d4b":"markdown","61f7fe86":"markdown","29f612ec":"markdown","49185b30":"markdown","b656993b":"markdown","b5daa90f":"markdown","b17e78dc":"markdown","7f6b708e":"markdown","2f49c6e0":"markdown","8f1eafdc":"markdown","9bbc78c1":"markdown","61768f3c":"markdown","60adf4d2":"markdown","73e4b795":"markdown","d95a2a9b":"markdown","03bad543":"markdown","9cebb7e4":"markdown","84df35aa":"markdown"},"source":{"5df647e9":"import os\nimport nltk\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\nfrom datasets import Dataset\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import TrainingArguments, Trainer","0748a4b2":"# Constants\nTRAIN_CSV = \"..\/input\/feedback-prize-2021\/train.csv\"\nSUB_CSV = \"..\/input\/feedback-prize-2021\/sample_submission.csv\"\nTRAIN_PATH = \"..\/input\/feedback-prize-2021\/train\"\nTEST_PATH = \"..\/input\/feedback-prize-2021\/test\"\n\n# Load DF\ndf = pd.read_csv(TRAIN_CSV, dtype={'discourse_id': int, 'discourse_start': int, 'discourse_end': int})\ndf.head()","f4f46900":"# No nulls\ndf.isnull().sum()","8a7fb3b8":"a_id = \"423A1CA112E2\"","6e4ecdba":"def get_text(a_id):\n    a_file = f\"{TRAIN_PATH}\/{a_id}.txt\"\n    with open(a_file, \"r\") as fp:\n        txt = fp.read()\n    return txt\n\ntxt = get_text(a_id)\nprint(txt)","5ffee9b9":"df_example = df[df['id'] == a_id]\ndf_example","a9e059f1":"# Files in train path: 15595\n!ls -l {TRAIN_PATH} | wc -l","58944ec0":"# Files in test path: 6\n!ls -l {TEST_PATH} | wc -l","ceb28e78":"# There are 7 classes:\ndf['discourse_type'].value_counts(normalize=True)","69b60a44":"ID2CLASS = dict(enumerate(df['discourse_type'].unique().tolist() + ['No Class']))\nCLASS2ID = {v: k for k, v in ID2CLASS.items()}\nprint(ID2CLASS)\nCLASS2ID","a2b0f727":"text_ids = df['id'].unique().tolist()","ec2cff3b":"text_id = text_ids[5]\ntext = get_text(text_id)\nprint(text)","19995b82":"# Extract element boundaries and classes with to_records\n\ndf_text = df[df['id'] == text_id]\nelements = df_text[['discourse_start', 'discourse_end', 'discourse_type']].to_records(index=False).tolist()\nelements","554ee52c":"# Fill \"No class\" chunks: beginning and end\n\ninitial_idx = 0\nfinal_idx = len(text)\n\n# Add element at the beginning if it doesn't in index 0\nnew_elements = []\nif elements[0][0] != initial_idx:\n    starting_element = (0, elements[0][0]-1, 'No Class')\n    new_elements.append(starting_element)\n\n    \n# Add element at the end if it doesn't in index \"-1\"\nif elements[-1][1] != final_idx:\n    closing_element = (elements[-1][1]+1, final_idx, 'No Class')\n    new_elements.append(closing_element)\n    \nelements += new_elements\nelements = sorted(elements, key=lambda x: x[0])\n\n# See the first element (it's new, labeled \"No Class\")\nelements","6bc4c0f5":"# Add \"No class\" elements inbetween separated elements \nnew_elements = []\nfor i in range(1, len(elements)-1):\n    if elements[i][0] != elements[i-1][1] + 1 and elements[i][0] != elements[i-1][1]:\n        new_element = (elements[i-1][1] + 1, elements[i][0]-1, 'No Class')\n        new_elements.append(new_element)\n\nelements += new_elements\nelements = sorted(elements, key=lambda x: x[0])\nelements","9d301186":"# Final \"fill_gaps\" functions, wrapping up the above cells\n\ndef fill_gaps(elements, text):\n    \"\"\"Add \"No Class\" elements to a list of elements (see get_elements) \"\"\"\n    initial_idx = 0\n    final_idx = len(text)\n\n    # Add element at the beginning if it doesn't in index 0\n    new_elements = []\n    if elements[0][0] != initial_idx:\n        starting_element = (0, elements[0][0]-1, 'No Class')\n        new_elements.append(starting_element)\n\n\n    # Add element at the end if it doesn't in index \"-1\"\n    if elements[-1][1] != final_idx:\n        closing_element = (elements[-1][1]+1, final_idx, 'No Class')\n        new_elements.append(closing_element)\n\n    elements += new_elements\n    elements = sorted(elements, key=lambda x: x[0])\n\n    # Add \"No class\" elements inbetween separated elements \n    new_elements = []\n    for i in range(1, len(elements)-1):\n        if elements[i][0] != elements[i-1][1] + 1 and elements[i][0] != elements[i-1][1]:\n            new_element = (elements[i-1][1] + 1, elements[i][0]-1, 'No Class')\n            new_elements.append(new_element)\n\n    elements += new_elements\n    elements = sorted(elements, key=lambda x: x[0])\n    return elements\n\n\ndef get_elements(df, text_id, do_fill_gaps=True, text=None):\n    \"\"\"Get a list of (start, end, class) elements for a given text_id\"\"\"\n    text = get_text(text_id) if text is None else text\n    df_text = df[df['id'] == text_id]\n    elements = df_text[['discourse_start', 'discourse_end', 'discourse_type']].to_records(index=False).tolist()\n    if do_fill_gaps:\n        elements = fill_gaps(elements, text)\n    return elements","fa7c9494":"def get_x_samples(df, text_id, do_fill_gaps=True):\n    \"\"\"Create a dataframe of the sentences of the text_id, with columns text, label \"\"\"\n    text = get_text(text_id)\n    elements = get_elements(df, text_id, do_fill_gaps, text)\n    sentences = []\n    for start, end, class_ in elements:\n        elem_sentences = nltk.sent_tokenize(text[start:end])\n        sentences += [(sentence, class_) for sentence in elem_sentences]\n    df = pd.DataFrame(sentences, columns=['text', 'label'])\n    df['label'] = df['label'].map(CLASS2ID)\n    return df\n\nget_x_samples(df, text_ids[1])","023a7776":"# This takes a while. I created a dataset with the output here: https:\/\/www.kaggle.com\/julian3833\/feedback-df-sentences\n#x = []\n#for text_id in tqdm(text_ids):\n#    x.append(get_x_samples(df, text_id))\n\n#df_sentences = pd.concat(x)","937eb95e":"df_sentences = pd.read_csv(\"..\/input\/feedback-df-sentences\/df_sentences.csv\")","78a15df4":"df_sentences = df_sentences[df_sentences.text.str.split().str.len() >= 3]\ndf_sentences.head()","e3402c1a":"df_sentences.to_csv(\"df_sentences.csv\", index=False)","f1dc1b8e":"len(df_sentences)","68d76857":"MODEL_CHK = \"..\/input\/huggingface-bert\/bert-base-cased\"\n\nNUM_LABELS = 8\n\nNUM_EPOCHS = 3","a9517695":"ds_train = Dataset.from_pandas(df_sentences.iloc[:340000])\nds_val = Dataset.from_pandas(df_sentences.iloc[340000:])","42caf097":"transformers.logging.set_verbosity_warning() # Silence some annoying logging of HF\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_CHK)\n\ndef preprocess_function(examples):    \n    return tokenizer(examples[\"text\"], truncation=True, max_length=326)\n\n\n# Tokenizer dataset\nds_train_tokenized = ds_train.map(preprocess_function, batched=True)\nds_val_tokenized = ds_val.map(preprocess_function, batched=True)","08e37e97":"# Load model\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL_CHK, num_labels=NUM_LABELS)","b90dbc6a":"os.environ['TOKENIZERS_PARALLELISM'] = 'false'\nos.environ['WANDB_DISABLED'] = 'true'\n\ntraining_args = TrainingArguments(\n    output_dir='feeeback-classifier',\n    learning_rate=2e-5,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    num_train_epochs=NUM_EPOCHS,\n    weight_decay=0.01,\n    report_to=\"none\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=ds_train_tokenized,\n    eval_dataset=ds_val_tokenized,\n    tokenizer=tokenizer,\n    #data_collator=data_collator,\n)","dac60bdd":"trainer.train()","150b9cd3":"trainer.save_model(\"feedback-bert-trained\")","749e764e":"df_sub = pd.read_csv(SUB_CSV)\ndf_sub","8287ca94":"def get_test_text(a_id):\n    a_file = f\"{TEST_PATH}\/{a_id}.txt\"\n    with open(a_file, \"r\") as fp:\n        txt = fp.read()\n    return txt\n\ndef create_df_test():\n    test_ids = [f[:-4] for f in os.listdir(TEST_PATH)]\n    test_data = []\n    for test_id in test_ids:\n        text = get_test_text(test_id)\n        sentences = nltk.sent_tokenize(text)\n        id_sentences = []\n        idx = 0 \n        for sentence in sentences:\n            id_sentence = []\n            words = sentence.split()\n            # I created this heuristic for mapping words in senteces to \"word indexes\"\n            # This is not definitive and might have strong drawbacks and problems\n            for w in words:\n                id_sentence.append(idx)\n                idx+=1\n            id_sentences.append(id_sentence)\n        test_data += list(zip([test_id] * len(sentences), sentences, id_sentences))\n    df_test = pd.DataFrame(test_data, columns=['id', 'text', 'ids'])\n    return df_test","76dd14a9":"df_test = create_df_test()\ndf_test.head()","afbbe52b":"ds_test = Dataset.from_pandas(df_test)\nds_test_tokenized = ds_test.map(preprocess_function, batched=True)","a57d3fe6":"# Get the predictions!!\ntest_predictions = trainer.predict(ds_test_tokenized)","55fd6bca":"# Turn logits into classes\ndf_test['predictions'] = test_predictions.predictions.argmax(axis=1)\n\n# Turn class ids into class labels\ndf_test['class'] = df_test['predictions'].map(ID2CLASS)\ndf_test.head()","009782a9":"# Turn the word ids into this weird predictionstring required\ndf_test['predictionstring'] = df_test['ids'].apply(lambda x: ' '.join([str(i) for i in x]))\ndf_test.head()","b2044cf4":"# Drop \"No class\" sentences\ndf_test = df_test[df_test['class'] != 'No Class']\ndf_test.head()","b8124d11":"# And submit!! \ud83e\udd1e\ud83e\udd1e \ndf_test[['id', 'class', 'predictionstring']].to_csv(\"submission.csv\", index=False)","8b8dd1dd":"I will stop with the EDA here because I have already seen various very good notebooks around, against which I cannot offer much value. I suggest you the following ones:\n\n* [Feedback Prize EDA with displacy](https:\/\/www.kaggle.com\/thedrcat\/feedback-prize-eda-with-displacy) by [thedrcat](https:\/\/www.kaggle.com\/thedrcat\/)\n* [[Feedback prize] Simple EDA](https:\/\/www.kaggle.com\/ilialar\/feedback-prize-simple-eda) by [ilialar](https:\/\/www.kaggle.com\/ilialar)\n* [\ud83d\udd25\ud83d\udcca Feedback Prize - EDA \ud83d\udcca\ud83d\udd25](https:\/\/www.kaggle.com\/odins0n\/feedback-prize-eda) by [odins0n](https:\/\/www.kaggle.com\/odins0n\/)\n* [Feedback Prize - EDA](https:\/\/www.kaggle.com\/yamqwe\/feedback-prize-eda) by [yamqwe](https:\/\/www.kaggle.com\/yamqwe\/)\n\n\nLet's get into the sentence classification idea!\n","17938c77":"# Create a sentence classification dataset\n\nAs far as I know, this problem is not trivially mapped to one of the \"typical\" NLP tasks. \nIt might be close to NER \/ POS, but the fact that the entities are large makes me doubt about it.\n\nI'm looking forward for the community discussion about the different possible approaches to this problem. \n\nAlthough I might be missing something very obvious, this notebook proposes the following approach, that is a multiclass classifier:\n\n1. Split the texts into sentences (x)\n2. Assign each sentence a class (y).\n3. Train a normal sequence classifier on those sentences\n\nThere are 7 classes and the labeled sections (sometimes) exceed sentences. We will preprocess them to have only sentences. That way, we will avoid the problem of detecting when an element starts and when it ends (for now).\n","f4e1cac1":"## Predict","848c4d4b":"For now, we are submitting one row per sentence and not per \"element\". \n\nHow to convert sentences into \"elements\" (blocks of sentences) is not clear since there are times when various continuous sentences with the same class are flagged as independent \"elements\".","61f7fe86":"# Submit\n\nWe will apply a process similar to the one we applied to the original training data, but to the test data: we are splitting each text into its sentences.\n\nSee the [Evaluation tab](https:\/\/www.kaggle.com\/c\/feedback-prize-2021\/overview\/evaluation) for details about the `predictionstring` column.","29f612ec":"## Encode classes as ints\nSome sections don't belong to any class. We will label them as `No Class` so we can discard those sections and avoid false positives.","49185b30":"## Dataset functions: `fill_gaps()`, `get_elements()`, and `get_x_samples()`\n\nHere we write the following functions:\n* `fill_gaps()`,  which will label the \"No Class\" parts of the texts. \n* `get_elements()`, which uses `fill_gaps` and creates a list of text sections for a given text id \n* `get_x_samples()`, which maps these elements into sentences with labels.\n\nFirst you'll find the code used for development (because it illustrates the thought process) and below you'll find the condensed functions.","b656993b":"## Prepare test dataset","b5daa90f":"## Tokenize","b17e78dc":"# 10,000 foot view of the data\n\nLet's take a quick glance at the `train.csv` file:","7f6b708e":"# Modeling!!!\n\nWe will use a `BERT` and the `Trainer` API from Hugging Face. \n\nWe are using a dataset to avoid using Internet (this is because submissions notebooks should not have Internet access: a competition restriction).\n\nReferences:\n* https:\/\/huggingface.co\/docs\/transformers\/training\n* https:\/\/huggingface.co\/docs\/transformers\/custom_datasets","2f49c6e0":"### Prepare trainer","8f1eafdc":"## Train","9bbc78c1":"## HuggingFace Dataset","61768f3c":"From the [Data tab](https:\/\/www.kaggle.com\/c\/feedback-prize-2021\/data):\n\n* id - ID code for essay response\n* discourse_id - ID code for discourse element\n* discourse_start - character position where discourse element begins in the essay response\n* discourse_end - character position where discourse element ends in the essay response\n* discourse_text - text of discourse element\n* discourse_type - classification of discourse element\n* discourse_type_num - enumerated class label of discourse element\n* predictionstring - the word indices of the training sample, as required for predictions\n","60adf4d2":"## Please, _DO_ upvote if you find it useful or interesting!! \n#### I'm very close to becoming a grandmaster and very excited about it \ud83d\ude07\ud83d\ude4f","73e4b795":"# Imports","d95a2a9b":"## Build the full dataframe for sentence classification","03bad543":"# Sentence Classifier with HuggingFace \ud83e\udd17","9cebb7e4":"# \ud83d\udcd6 Feedback - Baseline\ud83e\udd17 Sentence Classifier [0.226]\n\n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/31779\/logos\/header.png)\n\n###  Starter noteboook for the competition [Feedback Prize - Evaluating Student Writing](https:\/\/www.kaggle.com\/c\/feedback-prize-2021\/) framing the task as a sentence classification problem, using HuggingFace, BERT, and the Trainer API, with a LB of `0.226`.\n\n\n#### A lot of competitions going on, right my friends? ;)\n\n### The nature of this task is not trivially easy to map to an \"orthodox\" NLP problem, at least as far as I can tell. Is it a Token classification as NER\/POS? Is it a... hindi bilingual Question Answering (lol)?\n\n<h2> In this notebook I will try to present one of the possible approaches, this is: <span style=\"color:blue\"> Sentence Classification<\/span>.<\/h2>\n\n---\n\nThe agenda is as follows:\n1. A very quick EDA\n2. Preprocess to obtain a sentence classification dataset\n3. Fine-tune a BERT over that sentence classification dataset\n4. Submit\n\n---\n\n## Please, _DO_ upvote if you find it useful or interesting!! \n","84df35aa":"## Let's see the first example in some more detail"}}