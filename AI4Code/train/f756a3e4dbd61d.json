{"cell_type":{"7f2cbad8":"code","20d446d8":"code","54d9394e":"code","29248a6c":"code","c2e1054b":"code","02b42faf":"code","b86cc727":"code","53f3dd0a":"code","d8c29590":"code","db00642b":"code","e2d7772f":"code","664cc422":"code","9979df76":"code","a44e4956":"code","91e448a4":"code","1ccc1e43":"code","fe2d30d5":"code","498a29e5":"code","42e4297d":"code","dc2fab08":"code","9b93e531":"code","eb23fdf0":"code","c8aa560d":"code","bf4e5ca9":"code","f29042f2":"code","ec7546fd":"code","c251c344":"code","abb14ff0":"code","492f4b47":"code","e9aa9b89":"code","004b9f20":"code","80caa764":"code","dfb1e42b":"code","cdb4cadf":"code","4d14f119":"code","d89ed863":"code","192e52b1":"code","9b0f348d":"code","e802ba5d":"code","48a58b14":"code","f470f161":"code","47bc789e":"code","77c55805":"code","e1c39c46":"code","1eec762d":"code","bea76773":"code","bc12cb52":"code","9d241737":"code","2bf66a99":"code","f8a03e83":"code","04b3c990":"code","f000ed70":"code","b058afb1":"code","caaf8e19":"code","dbb8344c":"code","71e661b5":"code","6ffbc3f7":"code","fc276765":"code","3438ba5f":"code","d0973cf6":"markdown","92f06d9c":"markdown","96d9394a":"markdown","1eed0098":"markdown","09bdfdb7":"markdown","bd750f81":"markdown","96e9b8f2":"markdown","001b0662":"markdown","43cfb175":"markdown","dec5b089":"markdown","0cadf2f2":"markdown","fa57e959":"markdown","42c615e5":"markdown","00c2b8d0":"markdown","54f377ba":"markdown","bd10d992":"markdown","1dc0f8ab":"markdown","c30b91f3":"markdown","7d8f1a31":"markdown","0138debb":"markdown","9591d926":"markdown","71ae2877":"markdown","adf14c64":"markdown","53b54395":"markdown","c68dd6e3":"markdown","f88d1b94":"markdown","1d7ed234":"markdown","9f7da145":"markdown"},"source":{"7f2cbad8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","20d446d8":"def printShapeAndData(panda):  \n    print(panda.shape)\n    display(panda.head(10))\ndata = pd.read_csv('\/kaggle\/input\/loan-prediction-based-on-customer-behavior\/Training Data.csv')\n\nprintShapeAndData(data)","54d9394e":"data.describe()","29248a6c":"data.drop(columns =[\"Id\"], inplace = True)","c2e1054b":"pd.isnull(data).sum()","02b42faf":"df = data.copy()\ndf","b86cc727":"Cor=data.corr()\nplt.figure(figsize=(10,10))\nsns.heatmap(data.corr(), cmap='YlGnBu', annot = True)\nplt.title(\"Correlation Map\", fontweight = \"bold\", fontsize=16)\nplt.show()","53f3dd0a":"df[\"Married\/Single\"]= pd.Series(np.where(df[\"Married\/Single\"].values =='married', 1, 0), df.index)\ndf[\"Car_Ownership\"]= pd.Series(np.where(df[\"Car_Ownership\"].values =='yes', 1, 0), df.index)","d8c29590":"#6. Do homes with a few years have a higher rental potential than others?\nsns.set_style('whitegrid')\nsns.countplot(x='CURRENT_HOUSE_YRS' ,data = data , hue = 'House_Ownership')\nplt.show()","db00642b":"#7. Is there a relation between marital status and home ownership?\nsns.set_style('whitegrid')\nsns.countplot(x='Married\/Single' ,data = data , hue = 'House_Ownership')\nplt.show()","e2d7772f":"df['Profession'].unique()","664cc422":"def MinAndMax(dataframe, column, column2):\n    #Getting Max Value\n    high = dataframe[column].idxmax()\n    high_data = pd.DataFrame(df.loc[high])\n    #Getting Min Value\n    low = dataframe[column].idxmin()\n    low_data = pd.DataFrame(df.loc[low])\n    dataMinMax = pd.concat([high_data, low_data], axis=1) #creating a table for the detail of both columns\n    sChigh = dataMinMax[high][column2]\n    sClow = dataMinMax[low][column2]\n    return print(\"The lowest and highest in {} are: {} and {} and their {}s are {} and {}\".format(column,low,high,column2,sChigh, sClow)), dataMinMax\n\n    ","9979df76":"#1.Professions with highest and lowest average income.\nMinAndMax(df, 'Income','Profession')","a44e4956":"#2.Professions with highest and lowest average age.\nMinAndMax(df, 'Age','Profession')","91e448a4":"#3.What is the income based on the age?\nq=df.groupby('Age', as_index=False)['Income'].mean()\nq.Income= round (q.Income,1)\n\nsns.regplot(data = q, x = 'Age', y = 'Income', fit_reg = False); \n\nplt.xlabel('Age');\nplt.ylabel('Income');","1ccc1e43":"#4.What is the experience according to the age?\nw= df.groupby('Age', as_index=False)['Experience'].mean()\nw.Experience= round (w.Experience,1)\n\nsns.regplot(data = w, x = 'Age', y = 'Experience', fit_reg = False); \n\nplt.xlabel('Age');\nplt.ylabel('Experience');","fe2d30d5":"#5. What is the income according to profession?\nz=df.groupby('Profession', as_index=False)['Income'].mean()\nz.Income= round (z.Income,1)\n\nsns.regplot(data = z, x = 'Profession', y = 'Income', fit_reg = False); \nplt.xticks(rotation= 90)\nplt.xlabel('Profession');\nplt.ylabel('Income');","498a29e5":"#8. what is the income based on the experience?\nsns.barplot(x ='Experience', y ='Income', data = data)\nplt.show()","42e4297d":"#11.The highest ten professions with the income.\nheighest_three = df.groupby(\"Profession\")[\"Income\"].sum().sort_values(ascending=False).head(10)\nHighestIndex=list(heighest_three.index)\nHighestValue=list(heighest_three.values)\nsns.barplot(x= HighestValue, y=HighestIndex);","dc2fab08":"import matplotlib.pyplot as plt\n\nfor column in data._get_numeric_data():\n    plt.figure()\n    data.boxplot([column])","9b93e531":"Y = df['Income'].values\nX = df['Experience'].values\nplt.scatter(X,Y)","eb23fdf0":"a = sum([(x - np.mean(X)) * (y - np.mean(Y)) for x, y in zip(X,Y)]) \/ sum([(x - np.mean(X))**2 for x in X])\nb = np.mean(Y) - a * np.mean(X)\nyHat = lambda x: a * x + b","c8aa560d":"plt.scatter(X,Y)\nplt.plot([X.min(), X.max()], [yHat(X.min()), yHat(X.max())], color = 'b')\nplt.scatter(X.mean(), Y.mean(), marker = 'D', c = 'r')","bf4e5ca9":"TotalSS = sum([(y - np.mean(Y))**2 for y in Y])\nResidualSS = sum([(y - yHat(x))**2 for x, y in zip(X,Y)])\nR2 = 1 - ResidualSS\/TotalSS\nprint(\"Total Sum Squared is %f, Residual Sum Squared is %f, Fit Quality is %f.\" % (TotalSS, ResidualSS, R2))","f29042f2":"# 9. Average income of Car Owners.\ndt=data.copy()\ndt[\"Car_Ownership\"]= pd.Series(np.where(dt[\"Car_Ownership\"].values =='yes', 1, 0), dt.index)\ndt[['Car_Ownership']]","ec7546fd":"dt = dt.loc[~((dt['Car_Ownership'] == 0)) ]\ndt","c251c344":"car=dt[\"Car_Ownership\"].mean()\ncar\n\n","abb14ff0":"income=dt[\"Income\"].mean()\nincome","492f4b47":"sns.set_style(\"whitegrid\")\nsns.barplot(x=[\"Owned Car\", \"Income\"],y=[car,income])","e9aa9b89":"df_mean = dt[[\"Income\", \"Car_Ownership\"]].mean()\ndf_mean\n","004b9f20":"df_mean.plot(kind='bar')","80caa764":"#10. Average age of married individuals\ndt[\"Married\/Single\"]= pd.Series(np.where(dt[\"Married\/Single\"].values =='married', 1, 0), dt.index)\ndt[[\"Married\/Single\"]]","dfb1e42b":"dt = dt.loc[~((dt['Married\/Single'] == 0)) ]\ndt","cdb4cadf":"married=dt[\"Married\/Single\"].mean()\nmarried","4d14f119":"age=dt[\"Age\"].mean()\nage","d89ed863":"\ndf_mean = dt[[\"Age\", \"Married\/Single\"]].mean()\ndf_mean","192e52b1":"sns.set_style(\"whitegrid\")\nsns.barplot(x=[\"Age   \", \"Married\"],y=[age,married])\n","9b0f348d":"df_mean.plot(kind='bar')\nplt.xlabel('Average age of married individuals', fontsize = 15) ","e802ba5d":"dfML = df.copy() #Copying just incase.\nfeatures = [\"CURRENT_JOB_YRS\",\"Experience\"]\n\ndfPCA = dfML[features]\nscaledDF = (dfPCA - dfPCA.mean(axis=0))\/dfPCA.std()\nscaledDF","48a58b14":"pca = PCA(n_components=2)\nprincipalComponents = pca.fit_transform(scaledDF)\nprincipalDf = pd.DataFrame(data = principalComponents, columns = ['PC. 1', 'PC. 2'])\nprincipalDf.head()","f470f161":"dfML = pd.concat([df,principalDf],axis=1)\ndfML.head()","47bc789e":"features = ['House_Ownership','Profession','CITY','STATE'] #Categorical Values\nlabel_encoder = LabelEncoder()\n\nfor col in features:\n    dfML[col] = label_encoder.fit_transform(dfML[col])","77c55805":"dfML","e1c39c46":"if 'Risk_Flag' in dfML.columns:\n    data_targets=dfML.pop('Risk_Flag')\ndata_targets","1eec762d":"from sklearn.model_selection import train_test_split\n\ntrain_data,test_data,train_target,test_target = train_test_split(dfML,data_targets, test_size=0.2)","bea76773":"mean = train_data.mean()\ntrain_data -= mean\nstd = train_data.std()\ntrain_data \/= std\n\ntest_data -= mean\ntest_data \/= std\n\n","bc12cb52":"printShapeAndData(train_data)","9d241737":"printShapeAndData(train_target)","2bf66a99":"printShapeAndData(test_data)","f8a03e83":"printShapeAndData(test_target)","04b3c990":"from sklearn.linear_model import LogisticRegression\n\nlogistic_predict = LogisticRegression()\nlogistic_predict.fit(train_data, train_target)\nprint('Accuracy of Logistic regression classifier on training set: {:.5f}'.format(logistic_predict.score(train_data, train_target)))\nprint('Accuracy of Logistic regression classifier on test set: {:.5f}'.format(logistic_predict.score(test_data, test_target)))","f000ed70":"from sklearn.tree import DecisionTreeClassifier\n\nclf = DecisionTreeClassifier().fit(train_data, train_target)\n\nprint('Accuracy of Decision Tree classifier on training set: {:.5f}'.format(clf.score(train_data, train_target)))\nprint('Accuracy of Decision Tree classifier on test set: {:.5f}'.format(clf.score(test_data, test_target)))","b058afb1":"from sklearn.ensemble import RandomForestClassifier\n\nRFC = RandomForestClassifier()\nRFC.fit(train_data, train_target)\n\nprint('Accuracy of Random Forest Classifier on training set: {:.5f}'.format(RFC.score(train_data, train_target)))\nprint('Accuracy of Random Forest Classifier on test set: {:.5f}'.format(RFC.score(test_data, test_target)))","caaf8e19":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(train_data, train_target)\n\nprint('Accuracy of KNN classifier on training set: {:.2f}'\n     .format(knn.score(train_data, train_target)))\nprint('Accuracy of KNN classifier on test set: {:.2f}'\n     .format(knn.score(test_data, test_target)))","dbb8344c":"from sklearn.model_selection import RandomizedSearchCV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)","71e661b5":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestClassifier()\n\n# Random search of parameters, using 2 fold cross validation, \n# search across 2 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, scoring='accuracy', n_iter = 2, cv = 2, verbose=2, n_jobs = -1)\n\n# Fit the random search model\nrf_random.fit(train_data, train_target)\n","6ffbc3f7":"# scores\nprint(\" Results from Random Search \" )\nprint(\"\\n The best estimator across ALL searched params:\\n\", rf_random.best_estimator_)\nprint(\"\\n The best score across ALL searched params:\\n\", rf_random.best_score_)\nprint(\"\\n The best parameters across ALL searched params:\\n\", rf_random.best_params_)\nprint('\\n Accuracy of Random Forest Classifier best parameters on test set: {:.5f} \\n'.format(rf_random.score(test_data, test_target)))","fc276765":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\ny_pred=rf_random.best_estimator_.predict(test_data)\narray=confusion_matrix(test_target, y_pred)","3438ba5f":"f = sns.heatmap(array, annot=True, fmt='d')","d0973cf6":"**Train id is not needed so we remove it.Train id is not needed so we remove it.**","92f06d9c":"**4.4.3 K Neighbors Classifier**","96d9394a":"**From the results we can see that the Random Forest Classifier is the best for this particular dataset as it yields the best result in terms of accuracy: 90%**","1eed0098":"# Conclusion","09bdfdb7":"**4.2 Label Encoding the Categorical Values to numerical ones for Training and Testing**","bd750f81":"# 4. Machine Learning Models for predicting Loan acceptance.","96e9b8f2":"**5.1 fine tuning the best model to achieve best result**","001b0662":"***4.4.1 Logistic Regression***","43cfb175":"**Changing Married and Single to 1 and 0 respectively and Car_Ownership  Yes = 1 & no =0**","dec5b089":"# 3. Explantory Data Analysis","0cadf2f2":"# 2. Data Cleaning\nExploring the data, seeing if there is something to trim or clean up.","fa57e959":"# 5. Conclusion","42c615e5":"***4.4.2 Decision Tree Classifier***","00c2b8d0":"**5.2 confustion matrix for the model**","54f377ba":"First, we ready our feautures for PCA by scaling them.","bd10d992":"**Checking for Null Values**","1dc0f8ab":"**4.4.3 Random Forest Classifier**","c30b91f3":"Removing the target for the training set and to assign it to the test set.","7d8f1a31":"**Checking Correlation**","0138debb":"Then we apply PCA to the scaled features.","9591d926":"**4.3 Data Splitting**","71ae2877":"**4.4 Different Machine Learning Models**","adf14c64":"**The fit Quality is good due to the density of dataset and the correlation between the Income and Experience.\nAs for the outliers, there is none, indicating that the dataset is done professionally**","53b54395":"Adding the PCA features into the dataset","c68dd6e3":"**scalling the data**\n\nfeature-wise normalization: for each feature in the input data (a column in the input data matrix), we will subtract the mean of the feature and divide by the standard deviation, so that the feature will be centered around 0 and will have a unit standard deviation.","f88d1b94":"**Checking for Outliers**","1d7ed234":"# 1. Introduction\n**Overview of the Dataset**\n\nThe Dataset contains 250,000 individuals' data who either payed their loan on time or not indicated by the Risk Flag.\n\n# Questions\n\n1. Professions with highest and lowest average income.\n2. Professions with highest and lowest average age.\n3. What is the income based on the age?\n4. What is the experience according to the age?\n5. What is the income according to profession?\n6. Do homes with a few years have a higher rental potential than others?\n7. Is there a relation between marital status and home ownership?\n8. What is the income based on the experience?\n9. Average income of Car Owners.\n10. Average age of married individuals.\n11. The highest ten professions with the income.","9f7da145":"**4.1 From the Correlation Map as seen above, the two columns Experience and CURRENT_JOB_YRS are too related, resulting in\nboth features having the same value in the same row re-occuring too many times in the datasets, therefore PCA is needed.** "}}