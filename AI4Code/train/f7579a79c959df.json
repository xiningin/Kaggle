{"cell_type":{"81fb6a80":"code","4c7e0794":"code","6d558e98":"code","a3b0c34c":"code","0483ba2a":"code","ababd5c3":"code","58483e99":"code","cf841157":"code","329a0c19":"code","e9714e41":"code","9a3ff673":"code","637435ed":"code","635c062d":"code","27c2b035":"code","b6226205":"code","2307cc74":"code","e2365d40":"code","c108a5e9":"code","83eab2bd":"code","1a6f9ddd":"code","486c5f5f":"code","8ea7eb66":"code","b88624e4":"code","ce59c70b":"code","d969afe2":"code","697b3767":"code","ac1d9ee6":"code","ab49b78b":"code","6a5423ee":"code","fb136223":"code","b064eb3d":"markdown","b4a57899":"markdown","93ca0b61":"markdown","0a84a46e":"markdown","01610333":"markdown","241aca1c":"markdown","aa0933da":"markdown","41474b41":"markdown","8b91148c":"markdown","191aaa14":"markdown","94a08178":"markdown","a2872129":"markdown","2cd7613a":"markdown"},"source":{"81fb6a80":"import numpy as np\nimport pandas as pd\nimport time\nimport os\nimport matplotlib.pyplot as plt","4c7e0794":"class Config:\n    input_path = \"..\/input\/tabular-playground-series-jan-2022\"\n    train_path = os.path.join(input_path, \"train.csv\")\n    test_path = os.path.join(input_path, \"test.csv\")\n    n_folds = 5\n    batch_size = 128\n    label_name = \"num_sold\"\n    modes = [\"train\", \"inference\"]\n    mode = modes[1]\n    output_dataset_paths = [\"..\/input\/tps0122-with-autokeras-output-v1\/\"]\n    submission_path = os.path.join(input_path, \"sample_submission.csv\")\nconfig = Config()","6d558e98":"if config.mode == config.modes[0]:\n    !pip install autokeras","a3b0c34c":"train = pd.read_csv(config.train_path)\ntrain.head()","0483ba2a":"test = pd.read_csv(config.test_path)\ntest.head()","ababd5c3":"submission = pd.read_csv(config.submission_path)\nsubmission.head()","58483e99":"def visualize(df, column):\n    df[column].value_counts().plot(kind=\"bar\")\n    plt.title(\"Distribution of %s\"%(column))\n    plt.show()\n    df.groupby(column)[\"num_sold\"].sum().plot(kind=\"bar\")\n    plt.title(\"Total Sale Data in different %s\"%(column))\n    plt.show()\n    df.groupby(column)[\"num_sold\"].mean().plot(kind=\"bar\")\n    plt.title(\"Average Sale Data in different %s\"%(column))\n    plt.show()","cf841157":"for column in [\"country\", \"product\", \"store\"]:\n    visualize(train, column)","329a0c19":"def day_of_year(date):\n    daysInMonth = [0, 31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334]\n    year = 0\n    month = 0\n    day = 0\n    i = 0\n    value = 0\n    for c in date:\n        value = ord(c) - 48\n        if value >= 0:\n            if i == 0:\n                year = year * 10 + value\n            elif i == 1:\n                month = month * 10 + value\n            else:\n                day = day * 10 + value\n        else:\n            i += 1\n    num_days = day + daysInMonth[month - 1]\n    is_leap = year % 400 == 0 if year % 100 == 0 else year % 4 == 0\n    if is_leap and month > 2:\n        num_days += 1\n    return num_days\n\ndef add_datetime_features(df):\n    new_df = df.copy()\n    years = []\n    months = []\n    days = []\n    weekdays = []\n    weekends = []\n    seasons = []\n    day_of_years = []\n    for item in df[\"date\"]:\n        dt = time.strptime(item, '%Y-%m-%d')\n        is_weekend = 1 if dt.tm_wday >= 5 else 0\n        season = (dt.tm_mon - 3) \/\/ 3 % 4\n        years.append(dt.tm_year)\n        months.append(dt.tm_mon)\n        days.append(dt.tm_mday)\n        weekdays.append(dt.tm_wday)\n        weekends.append(is_weekend)\n        seasons.append(season)\n        day_of_years.append(day_of_year(item))\n    new_df[\"year\"] = years\n    new_df[\"month\"] = months\n    new_df[\"day\"] = days\n    new_df[\"weekday\"] = weekdays\n    new_df[\"weekend\"] = weekends\n    new_df[\"season\"] = seasons\n    new_df[\"day_of_year\"] = day_of_years\n    new_df[\"end_of_year\"] = new_df[\"day_of_year\"] >= 350\n    new_df[\"end_of_year\"] = new_df[\"end_of_year\"]\n    new_df[\"end_of_year\"] = new_df[\"end_of_year\"].astype(int)\n    new_df.pop(\"date\")\n    return new_df","e9714e41":"train_df = add_datetime_features(train)\ntrain_df.head()","9a3ff673":"test_df = add_datetime_features(test)\ntest_df.head()","637435ed":"train_df.pop(\"row_id\")\ntest_df.pop(\"row_id\");","635c062d":"for column in [\"year\", \"month\", \"day\", \"weekday\", \"season\", \"weekend\", \"end_of_year\"]:\n    visualize(train_df, column)","27c2b035":"train_df.head()","b6226205":"train_df.head()","2307cc74":"data = pd.concat([train_df, test_df])\ncategorical_columns = ['country', 'store', 'product', 'year', \"month\", 'weekday', 'season']\nfor column in categorical_columns:\n    item = pd.get_dummies(data[column])\n    item.columns = [\"_\".join([column, \"_\".join(str(item).split(\" \"))]) for item in item.columns]\n    data = pd.concat([data, item], axis=1)\n    data.pop(column)\ntrain_df = data[0:len(train_df)]\ntest_df = data[len(train_df):]\ntest_df.pop(\"num_sold\");","e2365d40":"train_df.head()","c108a5e9":"test_df.head()","83eab2bd":"for data in [train_df, test_df]:\n    for column in data.columns:\n        data[column] =  data[column].astype(float)","1a6f9ddd":"from sklearn.model_selection import TimeSeriesSplit, KFold, train_test_split\nX_train, X_val = train_test_split(train_df, random_state=42)\ny_train = X_train.pop(config.label_name)\ny_val = X_val.pop(config.label_name)","486c5f5f":"X_train.head()","8ea7eb66":"import tensorflow as tf\ntrain_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\ntrain_ds = train_ds.shuffle(256).batch(config.batch_size).prefetch(tf.data.AUTOTUNE).cache()\nvalid_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\nvalid_ds = valid_ds.batch(config.batch_size).prefetch(tf.data.AUTOTUNE).cache()","b88624e4":"def inference(models, X):\n    y_preds = []\n    for model in models:\n        y_pred = model.predict(X)\n        y_preds.append(y_pred)\n    return np.mean(y_preds, axis=0)\ndef smape(y_true, y_pred):\n    return 2.0 * np.mean(np.abs(y_pred - y_true) \/ (np.abs(y_pred) + np.abs(y_true))) * 100","ce59c70b":"if config.mode == config.modes[0]:\n    import autokeras as ak\n    inputs = ak.StructuredDataInput()\n    x1 = ak.DenseBlock()(inputs)\n    x2 = ak.DenseBlock()(inputs)\n    x = ak.Merge()([x1, x2])\n    output = ak.RegressionHead()(x)\n    auto_model = ak.AutoModel(\n        overwrite=True, inputs=inputs, outputs=output, max_trials=20\n    )\n    auto_model.fit(train_ds, validation_data=valid_ds, epochs=20)","d969afe2":"if config.mode == config.modes[0]:\n    tf_auto_model = auto_model.export_model()\n    tf_auto_model.save(\"auto_model.tf\")","697b3767":"models = []\nif config.mode == config.modes[0]:\n    model = tf.keras.models.load_model(\"auto_model.tf\")\n    models.append(model)\n    tf.keras.utils.plot_model(model, show_shapes=True)\nelse:\n    for path in config.output_dataset_paths:\n        model = tf.keras.models.load_model(path + \"auto_model.tf\")\n        models.append(model)","ac1d9ee6":"for model in models:\n    model.summary()","ab49b78b":"for model in models:\n    y_pred = inference([model], valid_ds)\n    print(\"SMAPE:\", smape(y_val, y_pred.reshape(-1)))","6a5423ee":"test_ds = tf.data.Dataset.from_tensor_slices((test_df))\ntest_ds = test_ds.batch(config.batch_size).prefetch(tf.data.AUTOTUNE)","fb136223":"y_pred = inference(models, test_ds)\nsubmission[\"num_sold\"] = y_pred\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission.head()","b064eb3d":"## Train Validation Split","b4a57899":"## Modeling","93ca0b61":"# TPS-01-22 with AutoKeras\n\n## Overview\nIn this notebook I will use AutoKeras to build models for [Tabular Playground Series - Jan 2022 Competition](https:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2022). I will explore using [AutoModel](https:\/\/autokeras.com\/auto_model\/#automodel-class) which is keras functional API style with more flexibility. Before Modeling, I will also perform some Exploratory data analysis and feature engineering to find insights.","0a84a46e":"## Imports","01610333":"## EDA & Preprocessing","241aca1c":"### Feature Engineering for datetime","aa0933da":"For different countries, Norway has the highest Sale Data; For different products, Kaggle Hat has the highest Sale Data; For different stores, KaggleRama has the highest Sale Data.","41474b41":"### Load the Model","8b91148c":"### Drop Id columns\n","191aaa14":"## Submission","94a08178":"### Save the Model","a2872129":"### More EDA\nAs we can see that Sale Data is increasing with year, but it is greater in end of month, end of week and Spring and Winter. It has strong cyclicity.","2cd7613a":"### Evaluation"}}