{"cell_type":{"e86d64c1":"code","6f67e95b":"code","aeab5a86":"code","90d5d291":"code","b7b9c883":"code","e32f84de":"code","0961389b":"code","d97ef903":"code","fad0c779":"code","be45d003":"code","9df2e2e1":"code","bff4969e":"code","5bb590fd":"code","8ef9d867":"code","dd021290":"code","6707a468":"code","2dac8c23":"code","7d3ecebb":"code","672a4cac":"code","3cf07bb8":"code","b4a6bb94":"code","eaef661e":"code","b32e1ea5":"code","ac850e66":"code","b4dbab47":"code","fbd84b21":"markdown","284231ec":"markdown","f20c5a92":"markdown","fb693448":"markdown","84ee63a0":"markdown","aa09d990":"markdown","d93be1a8":"markdown","6661157f":"markdown","b033d88f":"markdown","d6c8c65f":"markdown","e28b2737":"markdown","32cde2a0":"markdown","b57780da":"markdown","87de5c24":"markdown","9d3bca0d":"markdown","c1492ae5":"markdown"},"source":{"e86d64c1":"import numpy as np\nimport pandas as pd\n\n# Plot\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Modelling\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom scipy.stats import mode\n\n# XGBoost\nfrom xgboost import XGBClassifier\n\n# CatBoost\nfrom catboost import CatBoostClassifier\n\n# Cross-Validation\nfrom sklearn.model_selection import RepeatedStratifiedKFold","6f67e95b":"train = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/test.csv')","aeab5a86":"# This code snippet is taken from https:\/\/www.kaggle.com\/desalegngeb\/december-2021-tps-eda-models\n# Originally https:\/\/www.kaggle.com\/c\/tabular-playground-series-oct-2021\/discussion\/275854\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\ntrain = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","90d5d291":"train.head()","b7b9c883":"train.describe()","e32f84de":"print(\"Columns: \\n{0}\".format(list(train.columns)))","0961389b":"print('Train data shape:', train.shape)\nprint('Test data shape:', test.shape)","d97ef903":"missing_values_train = train.isna().any().sum()\nprint('Missing values in train data: {0}'.format(missing_values_train[missing_values_train > 0]))\n\nmissing_values_test = test.isna().any().sum()\nprint('Missing values in test data: {0}'.format(missing_values_test[missing_values_test > 0]))","fad0c779":"duplicates_train = train.duplicated().sum()\nprint('Duplicates in train data: {0}'.format(duplicates_train))\n\nduplicates_test = test.duplicated().sum()\nprint('Duplicates in test data: {0}'.format(duplicates_test))","be45d003":"#variables = train.nunique().sort_values(ascending=True)\n#print('Categorical variables in train data: \\n{0}'.format(variables))\ncategorical_features = train.columns[11:-1:]\nprint(\"Categorical Columns: \\n{0}\".format(list(categorical_features)))","9df2e2e1":"numerical_features = train.columns[1:11]\nprint(\"Numerical Columns: \\n{0}\".format(list(train.columns[1:11])))\ntrain[numerical_features].describe()","bff4969e":"plt.figure(figsize=(10, 6))\nplt.title('Target distribution')\nsns.countplot(x=train['Cover_Type'], data=train)","5bb590fd":"cType5 = train[train['Cover_Type'] == 5].index\nprint(\"Number of rows with Cover_Type = 5: {0}\".format(len(cType5)))","8ef9d867":"print(\"Unique values in Soil_Type7 column train data: {0}\".format(train['Soil_Type7'].unique()))\nprint(\"Unique values in Soil_Type15 column train data: {0}\".format(train['Soil_Type15'].unique()))\n\nprint(\"Unique values in Soil_Type7 column test data: {0}\".format(test['Soil_Type7'].unique()))\nprint(\"Unique values in Soil_Type15 column test data: {0}\".format(test['Soil_Type15'].unique()))","dd021290":"# Dropping the row Cover_Type = 5,\n# causes problems during kfold (least populated class)\ntrain.drop(cType5, axis=0, inplace=True)\n\n# Dropping columns Soil_Type7 and Soil_Type15, they are zero\ntrain.drop(['Soil_Type7', 'Soil_Type15'], axis=1, inplace=True)\ntest.drop(['Soil_Type7', 'Soil_Type15'], axis=1, inplace=True)","6707a468":"encoder = LabelEncoder()\ntrain[\"Cover_Type\"] = encoder.fit_transform(train[\"Cover_Type\"])","2dac8c23":"# Get train data without the target and ids\nX = train.iloc[:, 1:-1].copy()\n# Get the target\ny = train.Cover_Type.copy()\n# Get the test data without ids\ntest_X = test.iloc[:, 1:]\n\n# It takes time to handle all of the data.\n# So, I am using a smaller portion of the data\n# while debugging\/testing.\n#X = train.iloc[0:50, 1:-1].copy()\n#y = train.Cover_Type[0:50].copy()\n#test_X = test.iloc[0:50, 1:]","7d3ecebb":"# To store models created\nbest_models = {}\n\n# Split data\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n\ndef print_best_parameters(hyperparameters, best_parameters):\n    value = \"Best parameters: \"\n    for key in hyperparameters:\n        value += str(key) + \": \" + str(best_parameters[key]) + \", \"\n    if hyperparameters:\n        print(value[:-2])\n\ndef get_best_model(estimator, param_grid, fit_params):\n    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n    grid_search = GridSearchCV(estimator=estimator, param_grid=param_grid, n_jobs=-1, cv=cv)\n    best_model = grid_search.fit(train_X, train_y, **fit_params)\n    best_parameters = best_model.best_estimator_.get_params()\n    print_best_parameters(param_grid, best_parameters)\n    return best_model\n\ndef evaluate_model(model, name):\n    print(\"Accuracy score:\", accuracy_score(train_y, model.predict(train_X)))\n    best_models[name] = model","672a4cac":"param_grid = {\n    'gamma'         : [0.4, 0.8, 1.6, 3.2, 6.4],\n    'learning_rate' : [0.1, 0.2, 0.3, 0.5, 1],\n    'max_depth'     : [8, 9, 10, 11, 12],\n    'reg_alpha'     : [0, 0.1, 0.2, 0.5, 1, 2, 5, 10],\n    'reg_lambda'    : [0, 0.1, 0.2, 0.5, 1, 2, 5, 10],\n    'n_estimators'  : [50, 100, 150]\n}","3cf07bb8":"# https:\/\/towardsdatascience.com\/binary-classification-xgboost-hyperparameter-tuning-scenarios-by-non-exhaustive-grid-search-and-c261f4ce098d\n# Best parameters found so far.\nparam_grid = {\n    'learning_rate' : [0.5],\n    'gamma'         : [1.0],\n    'max_depth'     : [8],\n    'reg_alpha'     : [5],\n    'reg_lambda'    : [10],\n    'n_estimators'  : [150]\n}\nfit_params = {\n    'verbose'               : False,\n    'early_stopping_rounds' : 40,\n    'eval_metric'           : 'mlogloss',\n    'eval_set'              : [(val_X, val_y)]\n}\n#estimator = XGBClassifier(seed=1, tree_method='gpu_hist', predictor='gpu_predictor', use_label_encoder=False)\n#best_model_xgbc = get_best_model(estimator, param_grid, fit_params)","b4a6bb94":"#evaluate_model(best_model_xgbc.best_estimator_, 'XGBClassifier')","eaef661e":"param_grid = {\n    'depth'            : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n    'iterations'       : [100, 250, 500, 1000],\n    'learning_rate'    : [0.001, 0.01, 0.03, 0.1, 0.2, 0.3], \n    'l2_leaf_reg'      : [1, 3, 5, 10, 100],\n    'border_count'     : [5, 10, 20, 32, 50, 100, 200]\n}","b32e1ea5":"# https:\/\/effectiveml.com\/using-grid-search-to-optimise-catboost-parameters.html\n# https:\/\/catboost.ai\/en\/docs\/concepts\/parameter-tuning\nparam_grid = {\n#    'depth'            : [6, 8, 10],\n#    'iterations'       : [100, 250, 500],\n#    'learning_rate'    : [0.01, 0.1]\n}\nfit_params = {\n    'verbose'               : False,\n    'early_stopping_rounds' : 40,\n    'eval_set'              : [(val_X, val_y)]\n}\nestimator = CatBoostClassifier(random_seed=1, objective='MultiClass', task_type='GPU', devices='0')\nbest_model_cat = get_best_model(estimator, param_grid, fit_params)","ac850e66":"evaluate_model(best_model_cat.best_estimator_, 'CatBoostClassifier')","b4dbab47":"# Get predictions for each model and create submission files\nfor model in best_models:\n    predictions = best_models[model].predict(test_X)\n    predictions = encoder.inverse_transform(predictions)\n    output = pd.DataFrame({'Id': test.Id, 'Cover_Type': predictions})\n    output.to_csv('submission_' + model + '.csv', index=False)","fbd84b21":"| Version | Selected Model | Parameters                                             | Accuracy score          | Public Score | Notes                              |\n| ------- | -----------    | ------------------------------------------------------ | ----------------------- | -----------  | ---------------------------------- |\n|    2    | XGBClassifier  | learning_rate=0.01, gamma=0.0, max_depth=5             | 0.914338                | 0.91626      |                                    |\n|    3    | XGBClassifier  | learning_rate=0.5, gamma=1.0, max_depth=8              | 0.960802                | 0.95400      |                                    |\n|    4    | XGBClassifier  | learning_rate=0.3, gamma=1.6, max_depth=10             | 0.961175                | 0.95392      |                                    |\n|    6    | XGBClassifier  | learning_rate=0.5, gamma=1.0, max_depth=8              | 0.960802                | 0.95426      | Cross validation with Version 3    |\n|    9    | XGBClassifier  | reg_alpha=0.0, reg_lambda=0.1, n_estimators=100        | 0.960901                | 0.95445      | Additional parameters to Version 3 |\n|    10   | XGBClassifier  | reg_alpha=5, reg_lambda=10, n_estimators=150           | 0.965863                | 0.95381      | Using GridSearchCV with Version 3  |","284231ec":"## Numerical Features","f20c5a92":"# Basic Data Check","fb693448":"## Duplicates","84ee63a0":"## Missing values","aa09d990":"# Modelling","d93be1a8":"# Explore Data","6661157f":"## Categorical Features","b033d88f":"# [CatBoostClassifier](https:\/\/catboost.ai\/en\/docs\/references\/training-parameters\/)\n\n* **depth, Alias: max_depth**  \nDepth of the tree.\n\n* **iterations, Aliases: num_boost_round, n_estimators, num_trees**  \nThe maximum number of trees that can be built when solving machine learning problems.\n\n\n* **learning_rate, Alias: eta**  \nThe learning rate.\nUsed for reducing the gradient step.\n\n\n* **l2_leaf_reg, Alias: reg_lambda**  \nCoefficient at the L2 regularization term of the cost function.\n\n* **border_count, Alias: max_bin**  \nThe number of splits for numerical features. Allowed values are integers from 1 to 65535 inclusively.","d6c8c65f":"# [XGBClassifier](https:\/\/xgboost.readthedocs.io\/en\/stable\/parameter.html)\n\n* **eta [default=0.3, alias: learning_rate]**  \n    * Step size shrinkage used in update to prevents overfitting. After each boosting step, we can directly get the weights of new features, and eta shrinks the feature weights to make the boosting process more conservative.\n    * range: [0,1]\n\n\n* **gamma [default=0, alias: min_split_loss]**  \n    * Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be.\n    * range: [0,\u221e]\n\n\n* **max_depth [default=6]**  \n    * Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit. 0 is only accepted in lossguide growing policy when tree_method is set as hist or gpu_hist and it indicates no limit on depth. Beware that XGBoost aggressively consumes memory when training a deep tree.\n    * range: [0,\u221e] (0 is only accepted in lossguide growing policy when tree_method is set as hist or gpu_hist)\n\n\n* **lambda [default=1, alias: reg_lambda]**  \n    L2 regularization term on weights. Increasing this value will make model more conservative.\n\n* **alpha [default=0, alias: reg_alpha]**  \n    L1 regularization term on weights. Increasing this value will make model more conservative.","e28b2737":"## Dropping rows and columns","32cde2a0":"# Features","b57780da":"## Target Distribution","87de5c24":"## Encoding labels","9d3bca0d":"# Importing Libraries and Loading datasets","c1492ae5":"# Submission"}}