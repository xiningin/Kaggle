{"cell_type":{"7d0ca760":"code","2bf62517":"code","79ccf0d8":"code","33ec2731":"code","f8371b25":"code","3f183e66":"code","84d1022e":"code","839a2fb7":"code","d75ea0d0":"code","908f0fc1":"code","6709b397":"code","5543ef92":"code","80fc877d":"code","39c31e67":"code","8047f006":"code","40e1de1b":"code","4046406c":"code","c83d7ef7":"code","3bd07514":"code","aef06b96":"code","ae3da31c":"code","60dd04b3":"code","87d04d4e":"code","9f820329":"code","6e3627a3":"code","059babf0":"code","7cf02b43":"code","99b40adb":"code","9d1a5515":"code","6be864b5":"code","57433cb5":"code","d67d1d83":"code","473771c4":"code","bdb7da31":"code","dcda1cf2":"code","9a0be666":"code","37e66d4c":"code","43d47629":"code","f7629d29":"code","a27fffc7":"code","ed14a3ec":"code","2c46a1e9":"code","2bebbc7a":"code","0c637c16":"code","452f6766":"code","a36d97c7":"code","f724c1aa":"code","850e123f":"code","77911290":"code","248cf6ac":"code","011be43b":"code","83294ac3":"code","888afd87":"code","9afa8ac6":"code","58783063":"code","b43ba18d":"code","b6ba23db":"code","9e86d8c7":"code","12ce055d":"code","ca5802ee":"code","9329d847":"code","3ce84489":"code","e834d76d":"code","05a50262":"code","54c4d2df":"code","5a0cdf8e":"code","7d3c7d05":"code","095c3e20":"code","30afe1a4":"code","ac15aa79":"code","fdad88b7":"code","e6d7e066":"code","ead8ee0e":"code","3ede6941":"code","8e4edc82":"markdown","43b24df0":"markdown","574fc096":"markdown","d60c6016":"markdown","b20ae190":"markdown","240336aa":"markdown","d57e88e2":"markdown","94d42b5e":"markdown","cfa905ff":"markdown","56c2007b":"markdown","92ada0d2":"markdown","2726f6d7":"markdown","5e736d3e":"markdown","00b43286":"markdown","da7b9321":"markdown","d5e14cc6":"markdown","80535af6":"markdown","cd7c5c2e":"markdown","d3bac59a":"markdown","ef56ad93":"markdown","3b97751f":"markdown","b7368c5c":"markdown","b3ae5c6c":"markdown","27351612":"markdown","b4f6e099":"markdown","04801163":"markdown","1f20da4a":"markdown","49d9f9e3":"markdown","1863463f":"markdown","a2246e37":"markdown","2fca2b3c":"markdown","e41b25c5":"markdown","40b45c03":"markdown","995a00c1":"markdown","f70951ba":"markdown"},"source":{"7d0ca760":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","2bf62517":"train = pd.read_csv('..\/input\/bank-loan2\/madfhantr.csv')\ntest = pd.read_csv('..\/input\/bank-loan2\/madhante.csv')","79ccf0d8":"print(train.shape)\ntrain.head()","33ec2731":"train_original = train.copy()\ntest_original = test.copy()","f8371b25":"train.dtypes","3f183e66":"train['Loan_Status'].value_counts()","84d1022e":"# Normalize can be set to True to print proportions instead of number \ntrain['Loan_Status'].value_counts(normalize=True)","839a2fb7":"train['Loan_Status'].value_counts().plot.bar()","d75ea0d0":"#Independent Variable (Categorical)\nprint(plt.figure(1) )\nplt.subplot(221) \ntrain['Gender'].value_counts(normalize=True).plot.bar(figsize=(20,10), title= 'Gender') \nplt.subplot(222) \ntrain['Married'].value_counts(normalize=True).plot.bar(title= 'Married') \nplt.subplot(223) \ntrain['Self_Employed'].value_counts(normalize=True).plot.bar(title= 'Self_Employed') \nplt.subplot(224) \ntrain['Credit_History'].value_counts(normalize=True).plot.bar(title= 'Credit_History') \nplt.show()","908f0fc1":"#Independent Variable (Ordinal)\nplt.figure(1) \nplt.subplot(131) \ntrain['Dependents'].value_counts(normalize=True).plot.bar(figsize=(24,6), title= 'Dependents') \nplt.subplot(132) \ntrain['Education'].value_counts(normalize=True).plot.bar(title= 'Education') \nplt.subplot(133) \ntrain['Property_Area'].value_counts(normalize=True).plot.bar(title= 'Property_Area') \nplt.show()","6709b397":"#Independent Variable (Numerical)\n#Till now we have seen the categorical and ordinal variables and now lets visualize the numerical variables. Lets look at the distribution of Applicant income first.\n\nplt.figure(1) \nplt.subplot(121)\nsns.distplot(train['ApplicantIncome']); \nplt.subplot(122) \ntrain['ApplicantIncome'].plot.box(figsize=(16,5)) \nplt.show()","5543ef92":"train.boxplot(column='ApplicantIncome', by = 'Education') \nplt.suptitle(\"\")\n","80fc877d":"plt.figure(1) \nplt.subplot(121) \nsns.distplot(train['CoapplicantIncome']); \nplt.subplot(122) \ntrain['CoapplicantIncome'].plot.box(figsize=(16,5)) \nplt.show()","39c31e67":"#Let\u2019s look at the distribution of LoanAmount variable.\n\nplt.figure(1) \nplt.subplot(121) \ndf=train.dropna() \nsns.distplot(train['LoanAmount']); \nplt.subplot(122) \ntrain['LoanAmount'].plot.box(figsize=(16,5)) \nplt.show()","8047f006":"Gender=pd.crosstab(train['Gender'],train['Loan_Status']) \nGender.div(Gender.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, figsize=(4,4))","40e1de1b":"#Now let us visualize the remaining categorical variables vs target variable.\nMarried=pd.crosstab(train['Married'],train['Loan_Status']) \nDependents=pd.crosstab(train['Dependents'],train['Loan_Status']) \nEducation=pd.crosstab(train['Education'],train['Loan_Status']) \nSelf_Employed=pd.crosstab(train['Self_Employed'],train['Loan_Status']) \nMarried.div(Married.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, figsize=(4,4)) \nplt.show() \nDependents.div(Dependents.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True) \nplt.show() \nEducation.div(Education.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, figsize=(4,4)) \nplt.show() \nSelf_Employed.div(Self_Employed.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, figsize=(4,4)) \nplt.show()","4046406c":"#Now we will look at the relationship between remaining categorical independent variables and Loan_Status.\nCredit_History=pd.crosstab(train['Credit_History'],train['Loan_Status']) \nProperty_Area=pd.crosstab(train['Property_Area'],train['Loan_Status']) \nCredit_History.div(Credit_History.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, figsize=(4,4)) \nplt.show() \nProperty_Area.div(Property_Area.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True) \nplt.show()","c83d7ef7":"#We will try to find the mean income of people for which the loan has been approved vs the mean income of people for which the loan has not been approved.\n\ntrain.groupby('Loan_Status')['ApplicantIncome'].mean().plot.bar()","3bd07514":"bins=[0,2500,4000,6000,81000] \ngroup=['Low','Average','High', 'Very high'] \ntrain['Income_bin']=pd.cut(train['ApplicantIncome'],bins,labels=group)","aef06b96":"Income_bin=pd.crosstab(train['Income_bin'],train['Loan_Status']) \nIncome_bin.div(Income_bin.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True) \nplt.xlabel('ApplicantIncome') \nP = plt.ylabel('Percentage')","ae3da31c":"bins=[0,1000,3000,42000] \ngroup=['Low','Average','High'] \ntrain['Coapplicant_Income_bin']=pd.cut(train['CoapplicantIncome'],bins,labels=group)","60dd04b3":"Coapplicant_Income_bin=pd.crosstab(train['Coapplicant_Income_bin'],train['Loan_Status']) \nCoapplicant_Income_bin.div(Coapplicant_Income_bin.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True) \nplt.xlabel('CoapplicantIncome') \nP = plt.ylabel('Percentage')","87d04d4e":"train['Total_Income']=train['ApplicantIncome']+train['CoapplicantIncome']\nbins=[0,2500,4000,6000,81000] \ngroup=['Low','Average','High', 'Very high'] \ntrain['Total_Income_bin']=pd.cut(train['Total_Income'],bins,labels=group)\nTotal_Income_bin=pd.crosstab(train['Total_Income_bin'],train['Loan_Status']) \nTotal_Income_bin.div(Total_Income_bin.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True) \nplt.xlabel('Total_Income') \nP = plt.ylabel('Percentage')","9f820329":"#Let\u2019s visualize the Loan amount variable.\n\nbins=[0,100,200,700] \ngroup=['Low','Average','High'] \ntrain['LoanAmount_bin']=pd.cut(train['LoanAmount'],bins,labels=group)\nLoanAmount_bin=pd.crosstab(train['LoanAmount_bin'],train['Loan_Status']) \nLoanAmount_bin.div(LoanAmount_bin.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True) \nplt.xlabel('LoanAmount') \nP = plt.ylabel('Percentage')","6e3627a3":"train=train.drop(['Income_bin', 'Coapplicant_Income_bin', 'LoanAmount_bin', 'Total_Income_bin', 'Total_Income'], axis=1)\ntrain['Dependents'].replace('3+', 3,inplace=True) \ntest['Dependents'].replace('3+', 3,inplace=True) \ntrain['Loan_Status'].replace('N', 0,inplace=True) \ntrain['Loan_Status'].replace('Y', 1,inplace=True)","059babf0":"matrix = train.corr() \nf, ax = plt.subplots(figsize=(18, 8)) \nsns.heatmap(matrix, vmax=.8, square=True, cmap=\"RdYlGn\",annot = True);","7cf02b43":"train.isnull().sum()","99b40adb":"train['Gender'].fillna(train['Gender'].mode()[0], inplace=True) \ntrain['Married'].fillna(train['Married'].mode()[0], inplace=True) \ntrain['Dependents'].fillna(train['Dependents'].mode()[0], inplace=True) \ntrain['Self_Employed'].fillna(train['Self_Employed'].mode()[0], inplace=True) \ntrain['Credit_History'].fillna(train['Credit_History'].mode()[0], inplace=True)","9d1a5515":"#Now let\u2019s try to find a way to fill the missing values in Loan_Amount_Term. \n#We will look at the value count of the Loan amount term variable.\n\ntrain['Loan_Amount_Term'].value_counts()","6be864b5":"train['Loan_Amount_Term'].fillna(train['Loan_Amount_Term'].mode()[0], inplace=True)","57433cb5":"train['LoanAmount'].fillna(train['LoanAmount'].median(), inplace=True)","d67d1d83":"#Now lets check whether all the missing values are filled in the dataset.\n\ntrain.isnull().sum()","473771c4":"#As we can see that all the missing values have been filled in the test dataset. \n#Let\u2019s fill all the missing values in the test dataset too with the same approach.\n\ntest['Gender'].fillna(train['Gender'].mode()[0], inplace=True) \ntest['Dependents'].fillna(train['Dependents'].mode()[0], inplace=True) \ntest['Self_Employed'].fillna(train['Self_Employed'].mode()[0], inplace=True) \ntest['Credit_History'].fillna(train['Credit_History'].mode()[0], inplace=True) \ntest['Loan_Amount_Term'].fillna(train['Loan_Amount_Term'].mode()[0], inplace=True) \ntest['LoanAmount'].fillna(train['LoanAmount'].median(), inplace=True)","bdb7da31":"train['LoanAmount_log'] = np.log(train['LoanAmount']) \ntrain['LoanAmount_log'].hist(bins=20) \ntest['LoanAmount_log'] = np.log(test['LoanAmount'])","dcda1cf2":"train[['LoanAmount','LoanAmount_log']].sort_values('LoanAmount')","9a0be666":"train=train.drop('Loan_ID',axis=1) \ntest=test.drop('Loan_ID',axis=1)","37e66d4c":"X = train.drop('Loan_Status',1) \ny = train.Loan_Status","43d47629":"X=pd.get_dummies(X) \ntrain=pd.get_dummies(train) \ntest=pd.get_dummies(test)","f7629d29":"from sklearn.model_selection import train_test_split\nx_train, x_cv, y_train, y_cv = train_test_split(X,y, test_size =0.3)","a27fffc7":"from sklearn.linear_model import LogisticRegression \nfrom sklearn.metrics import accuracy_score","ed14a3ec":"model = LogisticRegression() \nmodel.fit(x_train, y_train)","2c46a1e9":"pred_cv = model.predict(x_cv)","2bebbc7a":"accuracy_score(y_cv,pred_cv)","0c637c16":"pred_test = model.predict(test)","452f6766":"submission=pd.read_csv(\"..\/input\/bank-loan2\/sample_submission_49d68Cx.csv\")","a36d97c7":"submission['Loan_Status']=pred_test \nsubmission['Loan_ID']=test_original['Loan_ID']","f724c1aa":"submission['Loan_Status'].replace(0, 'N',inplace=True) \nsubmission['Loan_Status'].replace(1, 'Y',inplace=True)","850e123f":"pd.DataFrame(submission, columns=['Loan_ID','Loan_Status']).to_csv('logistic.csv')","77911290":"from sklearn.model_selection import StratifiedKFold","248cf6ac":"i=1 \nkf = StratifiedKFold(n_splits=5,random_state=1,shuffle=True) \nfor train_index,test_index in kf.split(X,y):     \n    print('\\n{} of kfold {}'.format(i,kf.n_splits))     \n    xtr,xvl = X.loc[train_index],X.loc[test_index]     \n    ytr,yvl = y[train_index],y[test_index]         \n    model = LogisticRegression(random_state=1)     \n    model.fit(xtr, ytr)     \n    pred_test = model.predict(xvl)     \n    score = accuracy_score(yvl,pred_test)     \n    print('accuracy_score',score) \n    i+=1 \npred_test = model.predict(test) \npred=model.predict_proba(xvl)[:,1]","011be43b":"from sklearn import metrics \nfpr, tpr, _ = metrics.roc_curve(yvl,  pred) \nauc = metrics.roc_auc_score(yvl, pred) \nplt.figure(figsize=(12,8)) \nplt.plot(fpr,tpr,label=\"validation, auc=\"+str(auc)) \nplt.xlabel('False Positive Rate') \nplt.ylabel('True Positive Rate') \nplt.legend(loc=4) \nplt.show()","83294ac3":"submission['Loan_Status']=pred_test \nsubmission['Loan_ID']=test_original['Loan_ID']","888afd87":"submission['Loan_Status'].replace(0, 'N',inplace=True) \nsubmission['Loan_Status'].replace(1, 'Y',inplace=True)","9afa8ac6":"pd.DataFrame(submission, columns=['Loan_ID','Loan_Status']).to_csv('Logistic.csv')","58783063":"train['Total_Income']=train['ApplicantIncome']+train['CoapplicantIncome'] \ntest['Total_Income']=test['ApplicantIncome']+test['CoapplicantIncome']","b43ba18d":"sns.distplot(train['Total_Income']);","b6ba23db":"train['Total_Income_log'] = np.log(train['Total_Income']) \nsns.distplot(train['Total_Income_log']); \ntest['Total_Income_log'] = np.log(test['Total_Income'])","9e86d8c7":"train['EMI']=train['LoanAmount']\/train['Loan_Amount_Term'] \ntest['EMI']=test['LoanAmount']\/test['Loan_Amount_Term']","12ce055d":"sns.distplot(train['EMI']);","ca5802ee":"train['Balance Income']=train['Total_Income']-(train['EMI']*1000) # Multiply with 1000 to make the units equal \ntest['Balance Income']=test['Total_Income']-(test['EMI']*1000)\nsns.distplot(train['Balance Income']);","9329d847":"train=train.drop(['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term'], axis=1) \ntest=test.drop(['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term'], axis=1)","3ce84489":"print(train.shape)\ntest.shape","e834d76d":"X = train.drop('Loan_Status',1) \ny = train.Loan_Status","05a50262":"i=1 \nkf = StratifiedKFold(n_splits=5,random_state=1,shuffle=True) \nfor train_index,test_index in kf.split(X,y):     \n    print('\\n{} of kfold {}'.format(i,kf.n_splits))     \n    xtr,xvl = X.loc[train_index],X.loc[test_index]     \n    ytr,yvl = y[train_index],y[test_index]         \n\n    model = LogisticRegression(random_state=1)     \n    model.fit(xtr, ytr)     \n    pred_test = model.predict(xvl)     \n    score = accuracy_score(yvl,pred_test)     \n    print('accuracy_score',score)     \n    i+=1 \n    pred_test = model.predict(test) \n    pred=model.predict_proba(xvl)[:,1]","54c4d2df":"submission['Loan_Status']=pred_test            # filling Loan_Status with predictions submission['Loan_ID']=test_original['Loan_ID'] # filling Loan_ID with test Loan_ID\n# replacing 0 and 1 with N and Y \nsubmission['Loan_Status'].replace(0, 'N',inplace=True) \nsubmission['Loan_Status'].replace(1, 'Y',inplace=True)\n# Converting submission file to .csv format \npd.DataFrame(submission, columns=['Loan_ID','Loan_Status']).to_csv('Log2.csv',index = False)","5a0cdf8e":"from sklearn import tree\ni=1 \nkf = StratifiedKFold(n_splits=5,random_state=1,shuffle=True) \nfor train_index,test_index in kf.split(X,y):     \n    print('\\n{} of kfold {}'.format(i,kf.n_splits))     \n    xtr,xvl = X.loc[train_index],X.loc[test_index]     \n    ytr,yvl = y[train_index],y[test_index]         \n    model = tree.DecisionTreeClassifier(random_state=1)     \n    model.fit(xtr, ytr)     \n    pred_test = model.predict(xvl)     \n    score = accuracy_score(yvl,pred_test)     \n    print('accuracy_score',score)     \n    i+=1 \n    pred_test = model.predict(test)","7d3c7d05":"from sklearn.ensemble import RandomForestClassifier\ni=1 \nkf = StratifiedKFold(n_splits=5,random_state=1,shuffle=True) \nfor train_index,test_index in kf.split(X,y):     \n    print('\\n{} of kfold {}'.format(i,kf.n_splits))     \n    xtr,xvl = X.loc[train_index],X.loc[test_index]     \n    ytr,yvl = y[train_index],y[test_index]         \n    model = RandomForestClassifier(random_state=1, max_depth=10)     \n    model.fit(xtr, ytr)     \n    pred_test = model.predict(xvl)     \n    score = accuracy_score(yvl,pred_test)     \n    print('accuracy_score',score)     \n    i+=1 \n\n    pred_test = model.predict(test)","095c3e20":"from sklearn.model_selection import GridSearchCV\n# Provide range for max_depth from 1 to 20 with an interval of 2 and from 1 to 200 with an interval of 20 for n_estimators \nparamgrid = {'max_depth': list(range(1, 20, 2)), 'n_estimators': list(range(1, 200, 20))}\ngrid_search=GridSearchCV(RandomForestClassifier(random_state=1),paramgrid)","30afe1a4":"from sklearn.model_selection import train_test_split \nx_train, x_cv, y_train, y_cv = train_test_split(X,y, test_size =0.3, random_state=1)\n# Fit the grid search model \ngrid_search.fit(x_train,y_train)","ac15aa79":"# Estimating the optimized value \ngrid_search.best_estimator_","fdad88b7":"i=1 \nkf = StratifiedKFold(n_splits=5,random_state=1,shuffle=True) \nfor train_index,test_index in kf.split(X,y):     \n    print('\\n{} of kfold {}'.format(i,kf.n_splits))     \n    xtr,xvl = X.loc[train_index],X.loc[test_index]     \n    ytr,yvl = y[train_index],y[test_index]         \n    model = RandomForestClassifier(random_state=1, max_depth=3, n_estimators=41)    \n    model.fit(xtr, ytr)     \n    pred_test = model.predict(xvl)     \n    score = accuracy_score(yvl,pred_test)     \n    print('accuracy_score',score)     \n    i+=1 \n    pred_test = model.predict(test) \n    pred2=model.predict_proba(test)[:,1]","e6d7e066":"submission['Loan_Status']=pred_test            # filling Loan_Status with predictions submission['Loan_ID']=test_original['Loan_ID'] # filling Loan_ID with test Loan_ID\n# replacing 0 and 1 with N and Y \nsubmission['Loan_Status'].replace(0, 'N',inplace=True) \nsubmission['Loan_Status'].replace(1, 'Y',inplace=True)\n# Converting submission file to .csv format \npd.DataFrame(submission, columns=['Loan_ID','Loan_Status']).to_csv('Random Forest.csv',index = False)","ead8ee0e":"importances=pd.Series(model.feature_importances_, index=X.columns) \nimportances.plot(kind='barh', figsize=(12,8));","3ede6941":"#!pip install xgboost\nfrom xgboost import XGBClassifier\ni=1 \nkf = StratifiedKFold(n_splits=5,random_state=1,shuffle=True) \nfor train_index,test_index in kf.split(X,y):     \n    print('\\n{} of kfold {}'.format(i,kf.n_splits))     \n    xtr,xvl = X.loc[train_index],X.loc[test_index]     \n    ytr,yvl = y[train_index],y[test_index]         \n    model = XGBClassifier(n_estimators=50, max_depth=4)     \n    model.fit(xtr, ytr)     \n    pred_test = model.predict(xvl)     \n    score = accuracy_score(yvl,pred_test)     \n    print('accuracy_score',score)     \n    i+=1 \n    pred_test = model.predict(test) \n    pred3=model.predict_proba(test)[:,1]","8e4edc82":"Let us combine the Applicant Income and Coapplicant Income and see the combined effect of Total Income on the Loan_Status.","43b24df0":"Categorical features: These features have categories (Gender, Married, Self_Employed, Credit_History, Loan_Status)\n\nOrdinal features: Variables in categorical features having some order involved (Dependents, Education, Property_Area)\n\nNumerical features: These features have numerical values (ApplicantIncome, CoapplicantIncome, LoanAmount, Loan_Amount_Term)","574fc096":"It can be seen that in loan amount term variable, the value of 360 is repeating the most. So we will replace the missing values in this variable using the mode of this variable.","d60c6016":"Proportion of married applicants is higher for the approved loans.\nDistribution of applicants with 1 or 3+ dependents is similar across both the categories of Loan_Status.\nThere is nothing significant we can infer from Self_Employed vs Loan_Status plot.","b20ae190":"Following inferences can be made from the above bar plots:\n\nMost of the applicants don\u2019t have any dependents.\nAround 80% of the applicants are Graduate.\nMost of the applicants are from Semiurban area.","240336aa":"We see a similar distribution as that of the applicant income. Majority of coapplicant\u2019s income ranges from 0 to 5000. \nWe also see a lot of outliers in the coapplicant income and it is not normally distributed.","d57e88e2":"It seems people with credit history as 1 are more likely to get their loans approved.\nProportion of loans getting approved in semiurban area is higher as compared to that in rural or urban areas.","94d42b5e":"# Bivariate Analysis","cfa905ff":"It can be inferred that the proportion of male and female applicants is more or less same for both approved and unapproved loans.","56c2007b":"It can be inferred that Applicant income does not affect the chances of loan approval \nwhich contradicts our hypothesis in which we assumed that if the applicant income is \nhigh the chances of loan approval will also be high.\n\nWe will analyze the coapplicant income and loan amount variable in similar manner.","92ada0d2":"# Missing value imputation","2726f6d7":"It can be inferred from the above bar plots that:\n\n80% applicants in the dataset are male.\nAround 65% of the applicants in the dataset are married.\nAround 15% applicants in the dataset are self employed.\nAround 85% applicants have repaid their debts.\nNow let\u2019s visualize the ordinal variables.","5e736d3e":"Now let\u2019s visualize numerical independent variables with respect to target variable.","00b43286":"So, the optimized value for the max_depth variable is 3 and for n_estimator is 41. Now let\u2019s build the model using these optimized values.","da7b9321":"##### Feature Engineering","d5e14cc6":"Let\u2019s drop the bins which we created for the exploration part. We will change the 3+ in dependents variable to 3 to make it a numerical variable.We will also convert the target variable\u2019s categories into 0 and 1 so that we can find its correlation with numerical variables. One more reason to do so is few models like logistic regression takes only numeric values as input. We will replace N with 0 and Y with 1.","80535af6":"#### Logistic Regression using stratified k-folds cross validation","cd7c5c2e":"For categorical features we can use frequency table or bar plots which will calculate the number of each category in a particular variable. \nFor numerical features, probability density plots can be used to look at the distribution of the variable.","d3bac59a":"Categorical Independent Variable vs Target Variable","ef56ad93":"# It can be inferred that most of the data in the distribution of applicant income is towards left which means it is not normally distributed. We will try to make it normal in later sections as algorithms works better if the data is normally distributed.\n\nThe boxplot confirms the presence of a lot of outliers\/extreme values. This can be attributed to the income disparity in the society. Part of this can be driven by the fact that we are looking at people with different education levels. Let us segregate them by Education:","3b97751f":"# Outlier Treatment","b7368c5c":"We see a lot of outliers in this variable and the distribution is fairly normal. \nWe will treat the outliers in later sections.","b3ae5c6c":"We can see that Proportion of loans getting approved for applicants having low Total_Income is very less as compared to that of applicants with Average, High and Very High Income.","27351612":"Now we will see the LoanAmount variable. As it is a numerical variable, we can use mean or median to impute the missing values. We will use median to fill the null values as earlier we saw that loan amount have outliers so the mean will not be the proper approach as it is highly affected by the presence of outliers.","b4f6e099":"Here the y-axis represents the mean applicant income. We don\u2019t see any change in the mean income. So, let\u2019s make bins for the applicant income variable based on the values in it and analyze the corresponding loan status for each bin.","04801163":"As we saw earlier in univariate analysis, LoanAmount contains outliers so we have to treat them as the presence of outliers affects the distribution of the data. Let's examine what can happen to a data set with outliers. For the sample data set:\n\n1, 1, 2, 2, 2, 2, 3, 3, 3, 4, 4\n\nWe find the following: mean, median, mode, and standard deviation\n\nMean = 2.58\n\nMedian = 2.5\n\nMode = 2\n\nStandard Deviation = 1.08\n\nIf we add an outlier to the data set:\n\n1, 1, 2, 2, 2, 2, 3, 3, 3, 4, 4, 400\n\nThe new values of our statistics are:\n\nMean = 35.38\n\nMedian = 2.5\n\nMode = 2\n\nStandard Deviation = 114.74\n\nIt can be seen that having outliers often has a significant effect on the mean and standard deviation and hence affecting the distribution. We must take steps to remove outliers from our data sets.","1f20da4a":"It can be seen that the proportion of approved loans is higher for Low and Average Loan Amount as compared to that of High Loan Amount which supports our hypothesis in which we considered that the chances of loan approval will be high when the loan amount is less.","49d9f9e3":"##### Decision Tree","1863463f":"We can see there are three format of data types:\n\nobject: Object format means variables are categorical. Categorical variables in our dataset are: Loan_ID, Gender, Married, Dependents, Education, Self_Employed, Property_Area, Loan_Status\n\nint64: It represents the integer variables. ApplicantIncome is of this format.\n\nfloat64: It represents the variable which have some decimal values involved. They are also numerical variables. Numerical variables in our dataset are: CoapplicantIncome, LoanAmount, Loan_Amount_Term, and Credit_History","a2246e37":"It shows that if coapplicant\u2019s income is less the chances of loan approval are high. \nBut this does not look right. The possible reason behind this may be that most of the \napplicants don\u2019t have any coapplicant so the coapplicant income for such applicants \nis 0 and hence the loan approval is not dependent on it. So we can make a new variable in which we will combine the applicant\u2019s and coapplicant\u2019s income to visualize the combined effect of income on loan approval.","2fca2b3c":"We can consider these methods to fill the missing values:\n\nFor numerical variables: imputation using mean or median\nFor categorical variables: imputation using mode","e41b25c5":"Due to these outliers bulk of the data in the loan amount is at the left and the right tail is longer. This is called right skewness. One way to remove the skewness is by doing the log transformation. As we take the log transformation, it does not affect the smaller values much, but reduces the larger values. So, we get a distribution similar to normal distribution.\n\nLet\u2019s visualize the effect of log transformation. We will do the similar changes to the test file simultaneously.","40b45c03":"# Model Building : Part I","995a00c1":"Now lets look at the correlation between all the numerical variables. We will use the heat map to visualize the correlation. Heatmaps visualize data through variations in coloring. The variables with darker color means their correlation is more.","f70951ba":"We will build the following models in this section.\n\nLogistic Regression\nDecision Tree\nRandom Forest\nXGBoost"}}