{"cell_type":{"fd46ed76":"code","936aec3c":"code","831eb8fc":"code","61529f0e":"code","ee884816":"code","cbf75984":"code","0f3858ac":"code","9b189eb8":"code","0e5147c6":"code","0e82d9a0":"code","28ad58d8":"code","0caa4c55":"code","2b389375":"code","d8e1500c":"code","fc17d451":"code","f6f9cebd":"code","4adf9076":"code","16e66a1e":"code","a4b80c53":"code","707ddeaa":"code","31009011":"code","351ec4c5":"code","5ff17e67":"code","7b89fa40":"code","5c9218d2":"code","8c521a40":"markdown","197cd18b":"markdown","175fe2d5":"markdown","58790cea":"markdown","28e9ba50":"markdown","bf58d44a":"markdown","7df447a2":"markdown","6d8988be":"markdown","4a6afb10":"markdown","717c2469":"markdown","ca0ce7b2":"markdown","52778c3c":"markdown","38528a52":"markdown","b53dbf43":"markdown","38c303ca":"markdown","6de40b5e":"markdown","4756d84a":"markdown","85583a3c":"markdown","d49737e5":"markdown","f9935fc4":"markdown","8758055e":"markdown","5d8c50b8":"markdown","8329d937":"markdown","7c580fc7":"markdown","996256bf":"markdown","9e56805d":"markdown","c055d870":"markdown","784d3049":"markdown","afb74ed2":"markdown","c98b9946":"markdown","fa5ba10a":"markdown","c19e2c9e":"markdown"},"source":{"fd46ed76":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","936aec3c":"data=pd.read_csv('..\/input\/voicegender\/voice.csv')","831eb8fc":"data.info()","61529f0e":"data.head()","ee884816":"data.isnull().values.any()","cbf75984":"y=data.label\nx_data=data.drop([\"label\"],axis=1)\nx=(x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data))","0f3858ac":"data.label=[0 if each==\"male\" else 1 for each in data.label]","9b189eb8":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)","0e5147c6":"#  Distribution of gender \nprint(y.value_counts())\nplt.pie(y.value_counts(),labels=[\"Female\",\"Male\"],colors=[\"pink\",\"green\"],autopct='%1.0f%%')\nplt.title(\"Distribution of gender:\")\nplt.show()","0e82d9a0":"data_corr=data.corr()\nplt.figure(figsize=(12,12))\nsns.heatmap(data_corr,annot=True, fmt= '.2f')","28ad58d8":"plt.figure(figsize=(8,5))\nsns.scatterplot(data=data, x=\"meanfun\", y=\"sp.ent\",hue=\"label\")\nplt.title(\"Scatter of Spectral Entropy by Average of Fundamental Frequency\")\nplt.xlabel(\"Average of Fundamental Frequency\")\nplt.ylabel(\"Spectral Entropy\")\nplt.show()","0caa4c55":"plt.figure(figsize=(8,5))\nsns.scatterplot(data=data, x=\"meanfun\", y=\"meanfreq\",hue=\"label\")\nplt.title(\"Scatter of Spectral Entropy by Average of Fundamental Frequency\")\nplt.xlabel(\"Average of Fundamental Frequency\")\nplt.ylabel(\"Mean Frequency (in kHz)\")\nplt.show()","2b389375":"plt.figure(figsize=(7,5))\nsns.stripplot(data=data, x=\"label\", y=\"IQR\",jitter=True)\nplt.title(\"IQR by Gender (Stripplot)\")\nplt.xlabel(\"Gender\")\nplt.ylabel(\"IQR\")\nplt.show()","d8e1500c":"from sklearn.linear_model import LogisticRegression\nlr=LogisticRegression()\nlr.fit(x_train,y_train)\nprint(\"Test Accuracy: {} %\".format(lr.score(x_test,y_test)*100))","fc17d451":"#Confusion matrix:\ny_pred_lr=lr.predict(x_test)\ny_true=y_test\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_pred_lr,y_true)\nf,ax=plt.subplots(figsize=(3,3))\nsns.heatmap(cm,annot=True,linecolor=\"blue\",linewidth=0.3,fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","f6f9cebd":"from sklearn.neighbors import KNeighborsClassifier\n\nscoreList=[]\n#Let's find the optimal number k\nfor each in range(1,15):\n    knn=KNeighborsClassifier(n_neighbors=each)\n    knn.fit(x_train,y_train)\n    scoreList.append(knn.score(x_test,y_test))\nplt.figure(figsize=(7,5))\nplt.plot(range(1,15),scoreList)\nplt.xlabel(\"K Values\")\nplt.ylabel(\"Accuracy\")\nplt.show()","4adf9076":"#Confusion matrix (k=9):\nknn=KNeighborsClassifier(n_neighbors=9)\nknn.fit(x_train,y_train)\nprint(\"Test Accuracy: {} %\".format(knn.score(x_test,y_test)*100))\ny_pred_knn=knn.predict(x_test)\ny_true=y_test\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_pred_knn,y_true)\nf,ax=plt.subplots(figsize=(3,3))\nsns.heatmap(cm,annot=True,linecolor=\"blue\",linewidth=0.3,fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","16e66a1e":"from sklearn.svm import SVC\nsvm=SVC(random_state=42)\nsvm.fit(x_train,y_train)\nprint(\"Test Accuracy: {} %\".format(svm.score(x_test,y_test)*100))","a4b80c53":"#Confusion matrix:\ny_pred_svm=svm.predict(x_test)\ny_true=y_test\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_pred_svm,y_true)\nf,ax=plt.subplots(figsize=(3,3))\nsns.heatmap(cm,annot=True,linecolor=\"blue\",linewidth=0.3,fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","707ddeaa":"from sklearn.naive_bayes import GaussianNB\nnb=GaussianNB()\nnb.fit(x_train,y_train)\nprint(\"Test Accuracy: {} %\".format(nb.score(x_test,y_test)*100))","31009011":"#Confusion matrix:\ny_pred_nb=nb.predict(x_test)\ny_true=y_test\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_pred_nb,y_true)\nf,ax=plt.subplots(figsize=(3,3))\nsns.heatmap(cm,annot=True,linecolor=\"blue\",linewidth=0.3,fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","351ec4c5":"from sklearn.tree import DecisionTreeClassifier\ndt=DecisionTreeClassifier()\ndt.fit(x_train,y_train)\nprint(\"Test Accuracy: {} %\".format(dt.score(x_test,y_test)*100))","5ff17e67":"#Confusion matrix:\ny_pred_dt=dt.predict(x_test)\ny_true=y_test\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_pred_dt,y_true)\nf,ax=plt.subplots(figsize=(3,3))\nsns.heatmap(cm,annot=True,linecolor=\"blue\",linewidth=0.3,fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","7b89fa40":"from sklearn.ensemble import RandomForestClassifier\nrf=RandomForestClassifier(n_estimators=10,random_state=42)\nrf.fit(x_train,y_train)\nprint(\"Test Accuracy = {} %\".format(rf.score(x_test,y_test)*100))","5c9218d2":"#Confusion Matrix\ny_pred_rf=rf.predict(x_test)\ny_true=y_test\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_pred_rf,y_true)\nf,ax=plt.subplots(figsize=(3,3))\nsns.heatmap(cm,annot=True,linecolor=\"blue\",linewidth=0.3,fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","8c521a40":"<a id=\"KNN\"><\/a>\n# K-Nearest Neighbor (KNN) Classification","197cd18b":"Let's visualize distribution of gender with pie chart:","175fe2d5":"Before building our ML models, we split the data into training and test data as a final step:","58790cea":"<a id=\"RFC\"><\/a>\n# Random Forest Classification","28e9ba50":"# Last Words\n* Among the algorithms we applied, we reached the highest accuracy in the Support Vector Machine algorithm, and the lowest accuracy in the Naive Bayes Algorithm. However, this does not mean that these two algorithms are the best and the worst for our data.\n* Some of the models we use are used for both classification and regression but we examined the issue of classification.\n* All models we apply are supervised learning algorithms.\n\n* Thanks [DATAI](https:\/\/www.kaggle.com\/kanncaa1\/notebooks)\n\n","bf58d44a":"<a id=\"Importing\"><\/a>\n# Importing Packages and Libraries\n\nWe will use numpy for mathematical operations, pandas for reading and processing data, matplotlib and seaborn for data visualization, and sklearn to build our classification models.","7df447a2":"<a id=\"SVM\"><\/a>\n# Support Vector Machine Classification","6d8988be":"IQR by Gender (Stripplot):\n\n*(IQR: interquantile range (in kHz))*","4a6afb10":"<a id=\"Visualization\"><\/a>\n# Data Visualization","717c2469":"First, we check whether there is a null value in the data set with the isnull function. Because a dataset containing null values makes our work difficult. ","ca0ce7b2":"We are now ready to build machine learning models, but take your time. Before building ML models, we will visualize them to better understand the data.","52778c3c":"* Decision trees  classification is a classification method that creates a model in the form of a tree structure consisting of decision nodes and leaf nodes by feature and target.\n* It can process both numerical and categorical data.\n* It is easy to understand and interpret.\n* This model can address multi-output problems.\n* Overfitting may occur in this model.\n\n![dt.PNG](attachment:dt.PNG)","38528a52":"<a id=\"Data-Preprocessing\"><\/a>\n# Data Preprocessing","b53dbf43":"We include the data set in our project using the Pandas library. Then we get general information about the data set and examine the first five rows.","38c303ca":"<a id=\"Train-test-split\"><\/a>\n# Train Test Split","6de40b5e":"* Support Vector Machine is a supervised learning algorithm based on statistical learning theory.\n* It tries to find the optimal line that separates the two labeled classes. \n* The optimal line is the furthest away from members of both classes.\n* This model does not have an overfitting problem.\n* Easy to apply\n* Its accuracy is high.\n![svm.PNG](attachment:svm.PNG)","4756d84a":"<a id=\"Loading-Dataset\"><\/a>\n#  Loading and Viewing Data Set","85583a3c":"# Contents\n* [Importing Packages and Libraries](#Importing)\n* [Loading and Viewing Dataset](#Loading-Dataset)\n* [Data Preprocessing](#Data-Preprocessing)\n* [Train test split](#Train-test-split)\n* [Data Visualization](#Visualization)\n* [Logistic Regression Classification](#LR)\n* [K-Nearest Neighbor Classification](#KNN)\n* [Support Vector Classification](#SVM)\n* [Naive Bayes Classification](#NBC)\n* [Decision Tree Classification](#DTC)\n* [Random Forst Classification](#RFC)","d49737e5":"<a id=\"NBC\"><\/a>\n# Naive Bayes Classification","f9935fc4":"* Naive Bayes classification is based on Bayes' Theorem.\n* Baye' Theorem shows the relationship between conditional probabilities and marginal probabilities within the probability distribution for a random variable. (Wikipedia)\n* Calculate the probability of each state for an element and classify it according to the one with the highest probability value.\n* In this algorithm  probability of each state is calculated for an element and classified according to the highest probability value.\n![nb,.PNG](attachment:nb,.PNG)\n\n            P(A|B): Probability that event A will occur when event B occurs.\n            P(A): The probability of event A will occur\n            P(B|A): The probability that event B will occur when event A occurs\n            P(B): The probability of event B will occur","8758055e":"<a id=\"LR\"><\/a>\n# Logistic Regression Classification","5d8c50b8":"# Machine Learning to Classification Gender by Voice \nIn this notebook I will try build ML models using sklearn to predict gender by voice. I created this notebook to apply the classification algorithms I learned. Also, this notebook may contain errors as I am a beginner. You can add mistakes you find or your suggestions to the comment. Thank You!","8329d937":"* KNN (K-nearest neighbor) is one of the simplest machine learning algorithms.\n* This algorithm memorizes the dataset rather learn it.\n* When a prediction is made, the nearest neighbors are searched in the entire data set. \n* To make a prediction, the nearest neighbors in the dataset are searched. \n* The value of k we choose determines how many neighboring elements we will examine. When there is a value to be classified, the distance to the k neighbors is calculated separately. The calculated distances are listed and the corresponding value assigned to the appropriate class. \n* Usually the Euclidean Function is used to calculate the distance.\n    Eucliden Function:\n\n![eucl.PNG](attachment:eucl.PNG)\n\n","7c580fc7":"<a id=\"DTC\"><\/a>\n# Decision Tree Claasification","996256bf":"Let's determine the dependent (y) and independent (x) variables of the data set:","9e56805d":"-->>> As shown in the graph above, if we make k = 9, we get the highest accuracy value.","c055d870":"* Logistic regression is a statistical method used to analyze a data set with one or more independent variables that define a result. \n* In this method the result is defined by two different values (True-False, Yes-No, 1-0, Cat-Dog, Good-Bad ...).\n* Purpose of Logistic Regression is build optimal model to describe of link between  a set of independent variables and two dimensional result. Classification is achieved with the created model.\n\n\nYou can examine the structure of this model in detail: [Gender Classification (Logistic Regression)](https:\/\/www.kaggle.com\/ahmetozdemir1071\/gender-classification-logistic-regression)","784d3049":"We create a correlation heatmap to see how the variables relate to each other:","afb74ed2":"Often we need to convert dependent variable values to integer types before creating an ML model. Our dependent variable is the \"label\" column. And built with string values (\"male\" and \"female\"). So we have to change these values. Let's replace \"male\" ones with 0 and \"female\" ones with 1 :","c98b9946":"Scatter of Spectral Entropy by Average of Fundamental Frequency Measured Across Acoustic Signal","fa5ba10a":"* The basic structure of this model consists of decision trees.\n* If we build n trees using random values in the dataset, actually we create a random forest model. So random forest is collected trees that built by random values and has branches.\n* The algorithm creates so many tree structures that it helps to get the best results from the results. Voting is done within the results and correct branches are created.\n* Decision Trees' biggest problem is overfitting. This problem is less in Random Forest since training on different datasets.\n\n![random%20forest.png](attachment:random%20forest.png)","c19e2c9e":"Scatter of  Mean Frequency (in kHz) by Average of Fundamental Frequency Measured Across Acoustic Signal"}}