{"cell_type":{"7f515a5c":"code","1c11745f":"code","d02e701b":"code","eb803962":"code","6d264f00":"code","38046bb2":"code","77adc5c6":"code","1b07bfc5":"code","cecae26b":"code","e78ab764":"code","08479679":"code","416be3de":"code","1b2f3ba4":"code","2ca284df":"code","f5dab00d":"code","b86df132":"code","26b20082":"code","5acf7380":"code","89dd28cb":"code","e0e8366c":"code","8834ecea":"code","1903b586":"code","1bed029b":"code","bf805c32":"code","d36c4c36":"code","6d67ddaa":"code","54a44318":"code","366e459e":"code","3a7922b8":"code","f1620c98":"code","56ec4a02":"code","defd62f0":"code","afafd371":"code","444702d2":"code","42bdeadb":"code","b619bac5":"code","421d2d75":"code","7389b8b0":"code","f6c8037b":"code","797b92a8":"code","09dd9596":"code","f51f3e12":"markdown","ee549c3c":"markdown","60fd987b":"markdown","d9cd1c17":"markdown","7787945b":"markdown","c1b84d82":"markdown","66aaba60":"markdown","99c6321f":"markdown","1967dded":"markdown","34ba9c7f":"markdown","341b268d":"markdown","54682797":"markdown","e0baa58c":"markdown","f6d86851":"markdown"},"source":{"7f515a5c":"! ls '\/kaggle\/input\/plant-seedlings-classification\/'\n! ls '\/kaggle\/input\/oversampling-plant-seedling\/'","1c11745f":"import os\nimport gc\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport PIL\nimport PIL.Image\nimport tensorflow as tf\nprint(tf.__version__)\nprint(\"Is there a GPU available: \", end = ' '),\nprint(tf.test.is_gpu_available())\nimport cv2\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\nmemory_freed = gc.collect()\nprint('Memory made free: ',memory_freed)","d02e701b":"# Setting all the variables and constants as configuration\nconfiguration = {\n    'train_path_org': '\/kaggle\/input\/plant-seedlings-classification\/train',\n    'train_path': '\/kaggle\/input\/oversampling-plant-seedling\/balanced_data',\n    'test_path': '\/kaggle\/input\/plant-seedlings-classification\/test\/',\n    'csv_path': '\/kaggle\/input\/plant-seedlings-classification\/sample_submission.csv',\n    \n    'validation_split': 0.25,\n    'buffer_size': 1000,\n    'batch_size': 32,\n    'img_height': 250,\n    'img_width': 250,\n    'num_classes' : 12,\n    \n    'drop_out_rate': 0.2,\n    'epochs': 20,\n    'fine_tune_flag': True,\n    'fine_tune_epochs': 20,\n    'vgg_finetune': 17,\n    'resnet_finetune': 171,\n    'inception_finetune': 251,\n    \n    'early_stopping': tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                                    mode='auto',\n                                    verbose=1,\n                                    patience=5),\n    'reduce_lr_on_platue': tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n                                    patience=3,\n                                    verbose=1,\n                                    factor=.5, \n                                    min_lr=0.0000001),\n    \n    'vgg_checkpoint': tf.keras.callbacks.ModelCheckpoint('vgg_model.h5', \n                                    monitor='val_loss', \n                                    verbose=1, \n                                    save_best_only=True, \n                                    mode='min'),\n    'resnet_checkpoint': tf.keras.callbacks.ModelCheckpoint('resnet_model.h5', \n                                    monitor='val_loss', \n                                    verbose=1, \n                                    save_best_only=True, \n                                    mode='min'),\n    'inception_checkpoint': tf.keras.callbacks.ModelCheckpoint('inception_model.h5', \n                                    monitor='val_loss', \n                                    verbose=1, \n                                    save_best_only=True, \n                                    mode='min'),\n    \n    'optimizer': tf.keras.optimizers.Adam(),\n    'loss': tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n    'accuracy': ['accuracy'],\n    \n    'vgg_16_base': tf.keras.applications.VGG16(\n                                input_shape=(250, 250, 3),\n                                include_top=False,\n                                weights='imagenet'),\n    'resnet_50_base': tf.keras.applications.ResNet50(\n                                input_shape=(250, 250, 3),\n                                include_top=False,\n                                weights='imagenet'),\n    'inception_v3_base': tf.keras.applications.InceptionV3(\n                                input_shape=(250, 250, 3),\n                                include_top=False,\n                                weights='imagenet'), \n}","eb803962":"path = configuration['train_path_org']\nfolder_name, img_count = [], []\nfor i in os.listdir(path):\n    folder_name.append(i)\n    count = len(os.listdir(os.path.join(path,i)))\n    img_count.append(count)\n    print(\"Folder Name : \",i)\n    print(\"No of Images : \",count)\n\nfig = plt.figure(figsize = (20, 5)) \n  \n# creating the bar plot \nplt.bar(folder_name, img_count, color ='maroon',  \n        width = 0.4) \n  \nplt.xlabel(\"Folder Name\") \nplt.ylabel(\"No of images in each folder\") \nplt.title(\"Image Frequency\")\n\nfig.autofmt_xdate()\nplt.show() ","6d264f00":"path = configuration['train_path']\nfolder_name, img_count = [], []\nfor i in os.listdir(path):\n    folder_name.append(i)\n    count = len(os.listdir(os.path.join(path,i)))\n    img_count.append(count)\n    print(\"Folder Name : \",i)\n    print(\"No of Images : \",count)\n\nfig = plt.figure(figsize = (20, 5)) \n  \n# creating the bar plot \nplt.bar(folder_name, img_count, color ='maroon',  \n        width = 0.4) \n  \nplt.xlabel(\"Folder Name\") \nplt.ylabel(\"No of images in each folder\") \nplt.title(\"Image Frequency\")\n\nfig.autofmt_xdate()\nplt.show() ","38046bb2":"import pathlib\ndata_dir = pathlib.Path(configuration['train_path'])\nprint(data_dir)","77adc5c6":"image_count = len(list(data_dir.glob('*\/*.png')))\nprint(image_count)","1b07bfc5":"list_ds = tf.data.Dataset.list_files(str(data_dir\/'*\/*'))\nlist_ds = list_ds.shuffle(image_count, reshuffle_each_iteration=False)\n\n# For Inception\n# as it requires different type of preprocessing compared to vgg and resnet\ninception_ds = tf.data.Dataset.list_files(str(data_dir\/'*\/*'))\ninception_ds = inception_ds.shuffle(image_count, reshuffle_each_iteration=False)","cecae26b":"for f in list_ds.take(3):\n    print(f.numpy())\n    s = f.numpy()","e78ab764":"class_names = np.array(sorted([item.name for item in data_dir.glob('*')]))\nprint(class_names)\nclass_map = {}\nfor i in range(len(class_names)):\n    class_map[i] = class_names[i]\nprint('\\n\\n',class_map)","08479679":"val_size = int(image_count * 0.2)\n\ntrain_ds = list_ds.skip(val_size)\nval_ds = list_ds.take(val_size)\n\ninception_train = inception_ds.skip(val_size)\ninception_val = inception_ds.take(val_size)","416be3de":"print(tf.data.experimental.cardinality(train_ds).numpy())\nprint(tf.data.experimental.cardinality(val_ds).numpy())","1b2f3ba4":"def get_label(file_path):\n    # convert the path to a list of path components\n    parts = tf.strings.split(file_path, os.path.sep)\n    # The second to last is the class-directory\n    one_hot = parts[-2] == class_names\n    label_classes = list(range(12))\n    tensor = tf.constant(label_classes,dtype = tf.int64)\n    ans = tf.boolean_mask(tensor,one_hot)\n    return ans[0]\n\ndef augment(image):\n    image = tf.image.random_brightness(image, max_delta=0.2) # Random brightness\n    image = tf.image.random_contrast(image, 0.8, 1)\n    return image\n\ndef decode_img(img):\n    # convert the compressed string to a 3D uint8 tensor\n    img = tf.image.decode_jpeg(img, channels=3)\n    \n    # Preprocess\n    img = tf.keras.applications.vgg16.preprocess_input(img)\n    \n    # Cast\n    img = tf.cast(img,tf.float32)\n    \n    # Augment\n    img = augment(img)\n    \n    # resize the image to the desired size\n    img = tf.image.resize(img, [configuration['img_height'], configuration['img_width']])\n    return img\n\ndef process_path(file_path):\n    label = get_label(file_path)\n    # load the raw data from the file as a string\n    img = tf.io.read_file(file_path)\n    img = decode_img(img)\n    return img, label\n\nlabeled_ds = list_ds.map(process_path)","2ca284df":"# Set `num_parallel_calls` so multiple images are loaded\/processed in parallel.\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\ntrain_ds = train_ds.map(process_path, num_parallel_calls=AUTOTUNE)\nval_ds = val_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n\ndef configure_for_performance(ds):\n    ds = ds.cache()\n    ds = ds.shuffle(buffer_size=configuration['buffer_size'])\n    ds = ds.batch(configuration['batch_size'])\n    ds = ds.prefetch(buffer_size=AUTOTUNE)\n    return ds\n\ntrain_ds = configure_for_performance(train_ds)\nval_ds = configure_for_performance(val_ds)","f5dab00d":"def inception_img(img):\n    # convert the compressed string to a 3D uint8 tensor\n    img = tf.image.decode_jpeg(img, channels=3)\n    \n    # Cast\n    img = tf.cast(img,tf.float32)\n    img = img\/255.\n    \n    # Augment\n    img = augment(img)\n    \n    # resize the image to the desired size\n    img = tf.image.resize(img, [configuration['img_height'], configuration['img_width']])\n    return img\n\n\ndef process_inception(file_path):\n    label = get_label(file_path)\n    # load the raw data from the file as a string\n    img = tf.io.read_file(file_path)\n    img = inception_img(img)\n    return img, label\n\ninception_ds = list_ds.map(process_inception)\n\nfor image, label in inception_ds.take(1):\n    print(\"Image shape: \", image.numpy().shape)\n    print(\"Label: \", label.numpy())\n    plt.imshow(image)\n    plt.title(class_map[label.numpy()])","b86df132":"image_batch, label_batch =  [],[]\n\nfor image, label in inception_ds.take(10):\n    image_batch.append(image)\n    label_batch.append(label)\n\n\nplt.figure(figsize=(10, 10))\nfor i in range(6):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(image_batch[i].numpy().astype(\"float32\"))\n    label = label_batch[i]\n    plt.title(class_names[label])\n    plt.axis(\"off\")","26b20082":"inception_train = inception_train.map(process_inception, num_parallel_calls=AUTOTUNE)\ninception_val = inception_val.map(process_inception, num_parallel_calls=AUTOTUNE)\n\ninception_train = configure_for_performance(inception_train)\ninception_val = configure_for_performance(inception_val)","5acf7380":"def build_model(base_model):\n    \n    base_model.trainable = False\n    \n    model = tf.keras.Sequential([\n        base_model,\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(128, activation='relu'),\n        tf.keras.layers.Dense(64, activation='relu'),\n        tf.keras.layers.Dense(configuration['num_classes']),\n    ])\n\n    model.compile(\n        optimizer  = configuration['optimizer'],\n        loss = configuration['loss'],\n        metrics = configuration['accuracy']\n    )\n    \n    return model","89dd28cb":"def plot_graphs(history):\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    plt.figure(figsize=(8, 8))\n    plt.subplot(2, 1, 1)\n    plt.plot(acc, label='Training Accuracy')\n    plt.plot(val_acc, label='Validation Accuracy')\n    plt.legend(loc='lower right')\n    plt.ylabel('Accuracy')\n    plt.ylim([min(plt.ylim()),1])\n    plt.title('Training and Validation Accuracy')\n\n    plt.subplot(2, 1, 2)\n    plt.plot(loss, label='Training Loss')\n    plt.plot(val_loss, label='Validation Loss')\n    plt.legend(loc='upper right')\n    plt.ylabel('Cross Entropy')\n    plt.ylim([0,1.0])\n    plt.title('Training and Validation Loss')\n    plt.xlabel('epoch')\n    plt.show()","e0e8366c":"memory_freed = gc.collect()\nprint('Memory made free: ',memory_freed)","8834ecea":"vgg_base = configuration['vgg_16_base']\nvgg_model = build_model(vgg_base)\n\nprint(vgg_model.summary())\nprint('\\n\\n')\n\nhistory = vgg_model.fit(\n    train_ds,\n    validation_data = val_ds,\n    epochs = configuration['epochs'],\n    callbacks = [\n                configuration['early_stopping'], \n                configuration['reduce_lr_on_platue'], \n                configuration['vgg_checkpoint']\n                ]\n    )\n\n# Plotting the graphs\nplot_graphs(history)","1903b586":"memory_freed = gc.collect()\nprint('Memory made free: ',memory_freed)","1bed029b":"vgg_base.trainable = True\nprint('Total No of layers: ',len(vgg_base.layers))\n\n# Fine-tune from this layer onwards\nfine_tune_at = configuration['vgg_finetune'] # 17\n\n# Freeze all the layers before the `fine_tune_at` layer\nfor layer in vgg_base.layers[:fine_tune_at]:\n    layer.trainable =  False\n    \n# Check the trainable status of the individual layers\nfor layer in vgg_base.layers:\n    print(layer, layer.trainable)\n\nprint('\\n\\n')\n    \nvgg_model.compile(\n    optimizer  = configuration['optimizer'],\n    loss = configuration['loss'],\n    metrics = configuration['accuracy']\n)\n\nprint('No of trainable variables: ',len(vgg_model.trainable_variables))\nprint(vgg_model.summary())","bf805c32":"if(configuration['fine_tune_flag']):\n    history_fine = vgg_model.fit(train_ds,\n                             validation_data=val_ds,\n                             epochs=configuration['fine_tune_epochs'],\n                             callbacks=[configuration['early_stopping'], \n                                        configuration['reduce_lr_on_platue'], \n                                        configuration['vgg_checkpoint']]\n                            )\n    \n    # Plotting the graphs\n    plot_graphs(history_fine)","d36c4c36":"loaded_model = tf.keras.models.load_model('vgg_model.h5')\ny_true = []\ny_pred = []\n\nfor image,label in val_ds.take(-1):\n    ans = loaded_model.predict(image)\n    for i in range(32):\n        ans_predicted = np.argmax(ans[i])\n        ans_actual = label[i].numpy()\n        y_true.append(ans_actual)\n        y_pred.append(ans_predicted)\n\nprint(class_map)\nprint('\\n\\nClassification Report: ')\nprint(classification_report(y_true, y_pred, target_names=class_map.values()))\nprint('\\n\\nConfusion Matrix: ')\nprint(confusion_matrix(y_true, y_pred))","6d67ddaa":"import pandas as pd\nimport seaborn as sn\n%matplotlib inline\n\ndata = confusion_matrix(y_true, y_pred)\ndf_cm = pd.DataFrame(data, columns=np.unique(y_true), index = np.unique(y_true))\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\nplt.figure(figsize = (20,6))\nsn.set(font_scale=1.4)#for label size\nsn.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16}, fmt='d')# font size","54a44318":"memory_freed = gc.collect()\nprint('Memory made free: ',memory_freed)","366e459e":"resnet_base = configuration['resnet_50_base']\nresnet_model = build_model(resnet_base)\n\nprint(resnet_model.summary())\nprint('\\n\\n')\n\nhistory = resnet_model.fit(\n    train_ds,\n    validation_data = val_ds,\n    epochs = configuration['epochs'],\n    callbacks = [\n                configuration['early_stopping'], \n                configuration['reduce_lr_on_platue'], \n                configuration['resnet_checkpoint']\n                ]\n    )\n\n# Plotting the graphs\nplot_graphs(history)","3a7922b8":"memory_freed = gc.collect()\nprint('Memory made free: ',memory_freed)","f1620c98":"resnet_base.trainable = True\nprint('Total No of layers: ',len(resnet_base.layers))\n\n# Fine-tune from this layer onwards\nfine_tune_at = configuration['resnet_finetune'] # 171\n\n# Freeze all the layers before the `fine_tune_at` layer\nfor layer in resnet_base.layers[:fine_tune_at]:\n    layer.trainable =  False\n\nprint('\\n\\n')\n    \nresnet_model.compile(\n    optimizer  = configuration['optimizer'],\n    loss = configuration['loss'],\n    metrics = configuration['accuracy']\n)\n\nprint('No of trainable variables: ',len(resnet_model.trainable_variables))\nprint(resnet_model.summary())","56ec4a02":"if(configuration['fine_tune_flag']):\n    history_fine = resnet_model.fit(train_ds,\n                             validation_data=val_ds,\n                             epochs=configuration['fine_tune_epochs'],\n                             callbacks=[configuration['early_stopping'],\n                                        configuration['reduce_lr_on_platue'], \n                                        configuration['resnet_checkpoint']]\n                            )\n    \n    # Plotting the graphs\n    plot_graphs(history_fine)","defd62f0":"loaded_model = tf.keras.models.load_model('resnet_model.h5')\ny_true = []\ny_pred = []\n\nfor image,label in val_ds.take(-1):\n    ans = loaded_model.predict(image)\n    for i in range(32):\n        ans_predicted = np.argmax(ans[i])\n        ans_actual = label[i].numpy()\n        y_true.append(ans_actual)\n        y_pred.append(ans_predicted)\n\nprint(class_map)\nprint('\\n\\nClassification Report: ')\nprint(classification_report(y_true, y_pred, target_names=class_map.values()))\nprint('\\n\\nConfusion Matrix: ')\nprint(confusion_matrix(y_true, y_pred))","afafd371":"import pandas as pd\nimport seaborn as sn\n%matplotlib inline\n\ndata = confusion_matrix(y_true, y_pred)\ndf_cm = pd.DataFrame(data, columns=np.unique(y_true), index = np.unique(y_true))\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\nplt.figure(figsize = (20,6))\nsn.set(font_scale=1.4)#for label size\nsn.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16}, fmt='d')# font size","444702d2":"memory_freed = gc.collect()\nprint('Memory made free: ',memory_freed)","42bdeadb":"inception_base = configuration['inception_v3_base']\ninception_model = build_model(inception_base)\n\nprint(inception_model.summary())\nprint('\\n\\n')\n\nhistory = inception_model.fit(\n    inception_train,\n    validation_data = inception_val,\n    epochs = configuration['epochs'],\n    callbacks = [\n                configuration['early_stopping'], \n                configuration['reduce_lr_on_platue'], \n                configuration['inception_checkpoint']\n                ]\n    )\n\n# Plotting the graphs\nplot_graphs(history)","b619bac5":"memory_freed = gc.collect()\nprint('Memory made free: ',memory_freed)","421d2d75":"inception_base.trainable = True\nprint('Total No of layers: ',len(inception_base.layers))\n\n# Fine-tune from this layer onwards\n# fine_tune_at = configuration['inception_finetune']\nfine_tune_at = 251\n\n# Freeze all the layers before the `fine_tune_at` layer\nfor layer in inception_base.layers[:fine_tune_at]:\n    layer.trainable =  False\n    \ninception_model.compile(\n    optimizer  = configuration['optimizer'],\n    loss = configuration['loss'],\n    metrics = configuration['accuracy']\n)\n\nprint('No of trainable variables: ',len(inception_model.trainable_variables))\nprint(inception_model.summary())","7389b8b0":"if(configuration['fine_tune_flag']):\n    history_fine = inception_model.fit(inception_train,\n                             validation_data=inception_val,\n                             epochs=configuration['fine_tune_epochs'],\n                             callbacks=[configuration['early_stopping'], \n                                        configuration['reduce_lr_on_platue'], \n                                        configuration['inception_checkpoint']]\n                            )\n    \n    # Plotting the graphs\n    plot_graphs(history_fine)","f6c8037b":"loaded_model = tf.keras.models.load_model('inception_model.h5')\ny_true = []\ny_pred = []\n\nfor image,label in inception_val.take(-1):\n    ans = loaded_model.predict(image)\n    for i in range(32):\n        ans_predicted = np.argmax(ans[i])\n        ans_actual = label[i].numpy()\n        y_true.append(ans_actual)\n        y_pred.append(ans_predicted)\n\nprint(class_map)\nprint('\\n\\nClassification Report: ')\nprint(classification_report(y_true, y_pred, target_names=class_map.values()))\nprint('\\n\\nConfusion Matrix: ')\nprint(confusion_matrix(y_true, y_pred))","797b92a8":"import pandas as pd\nimport seaborn as sn\n%matplotlib inline\n\ndata = confusion_matrix(y_true, y_pred)\ndf_cm = pd.DataFrame(data, columns=np.unique(y_true), index = np.unique(y_true))\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\nplt.figure(figsize = (20,6))\nsn.set(font_scale=1.4)#for label size\nsn.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16}, fmt='d')# font size","09dd9596":"memory_freed = gc.collect()\nprint('Memory made free: ',memory_freed)","f51f3e12":"## Accuracy and Loss plotting template","ee549c3c":"# Fine-Tune ResNet 50","60fd987b":"### ResNet Classification Report","d9cd1c17":"# Fine-Tune InceptionNet v3","7787945b":"### VGG16 Classification Report","c1b84d82":"# Resnet 50","66aaba60":"# Model template","99c6321f":"# Class Balance of original dataset","1967dded":"# Inception V3","34ba9c7f":"### Inception Classification Report","341b268d":"# VGG 16","54682797":"# Class Balance of oversampled dataset","e0baa58c":"# Fine-Tune VGG 16","f6d86851":"# Working with tf.data"}}