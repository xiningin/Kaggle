{"cell_type":{"aa5cf94b":"code","9e11e018":"code","6d2532f7":"code","11172735":"code","40e06f94":"code","8d08514e":"code","59739c6e":"code","b422c9e0":"code","2717e61a":"code","29759ae5":"code","817e4844":"code","e1ca40c2":"code","61a8ef2e":"code","36d42549":"code","cd1c771e":"code","2040987a":"code","183326ec":"code","5257d44f":"code","1f3d0298":"code","25e6209e":"code","88de4fc3":"code","52c62445":"code","ce6aa0a3":"code","19be52e5":"code","a81f5169":"code","45b31539":"code","94abab6a":"code","226c0128":"code","6c6316fb":"code","8d8cabe5":"code","9a672214":"markdown","4a2146c5":"markdown"},"source":{"aa5cf94b":"import sys\nimport matplotlib.pyplot as plt # plotting\nplt.style.use('ggplot')\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.linear_model import LinearRegression\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","9e11e018":"df1 = pd.read_csv('..\/input\/MicrosoftFinalData.csv', delimiter=',')\ndf1.dataframeName = 'MicrosoftFinalData.csv'\nnRow, nCol = df1.shape","6d2532f7":"df2 = pd.read_csv('..\/input\/MicrosoftNewsStock.csv', delimiter=',')\ndf2.dataframeName = 'MicrosoftNewsStock.csv'\nnRow, nCol = df2.shape","11172735":"df2.head(5)","40e06f94":"from nltk.sentiment.vader import SentimentIntensityAnalyzer\nimport unicodedata\nsid = SentimentIntensityAnalyzer()\n\nneg, neu, pos, compound = [], [], [], []\n\ndef sentiment(df):\n    for i in range(len(df)):\n        sen = unicodedata.normalize('NFKD', train.iloc[i]['News'])\n        ss = sid.polarity_scores(sen)\n        neg.append(ss['neg'])\n        neu.append(ss['neu'])\n        pos.append(ss['pos'])\n        compound.append(ss['compound'])\n    df['neg'] = neg\n    df['neu'] = neu\n    df['pos'] = pos\n    df['compound'] = compound","8d08514e":"def if_news(column):\n    if column == 0:\n        return 0\n    else:\n        return 1","59739c6e":"df1['if_news'] = df1['compound'].apply(if_news)\n\ndf_weekly = df1[['Date','Close', 'compound', 'neg', 'pos', 'if_news']]\ndf_weekly['Date'] = pd.to_datetime(df_weekly['Date'])\ndf_weekly.set_index('Date',inplace=True)","b422c9e0":"def take_last(array_like):\n    return array_like[-1]","2717e61a":"output = df_weekly.resample('W', # Weekly resample\n                    how={\n                         'Close': take_last,\n                         'compound': 'mean',\n                         'neg': 'mean',\n                         'pos': 'mean',\n                         'if_news': 'max'\n                    }, \n                    loffset=pd.offsets.timedelta(days=-6))","29759ae5":"output.head(5)","817e4844":"def split(df, X, y, seed=42):\n    \"\"\"\n    Split the data into a training and test set\n    \n    The training data should span the date range from 1\/1\/2018 to 6\/30\/2018\n    The test data should span the date range from 7\/1\/2018 to 7\/31\/2018\n    \n    Parameters\n    ----------\n    X: DataFrame containing the independent variable(s) (i.e, features, predictors)\n    y: DataFrame containing the dependent variable (i.e., the target)\n    \n    Optional\n    --------\n    seed: Integer used as the seed for a random number generator\n      You don't necessarily NEED to use a random number generator but, if you do, please use the default value for seed\n    \n    Returns\n    -------\n    X_train: DataFrame containing training data for independent variable(s)\n    X_test:  DataFrame containing test data for independent variable(s)\n    y_train: DataFrame containing training data for dependent variable\n    y_test:  DateFrame containing test data for dependent variable\n    \"\"\"\n    # IF  you need to use a random number generator, use rng.\n    rng = np.random.RandomState(seed)\n    \n    # Create the function body according to the spec\n    train_range = (np.where(df.index<='2015-06-30'))\n    test_range = (np.where(df.index>'2015-06-30'))\n    \n    # Split X and Y into train and test sets\n    X_train, y_train = X.iloc[train_range], y.iloc[train_range]\n    X_test, y_test = X.iloc[test_range], y.iloc[test_range]\n\n    # Change the return statement as appropriate\n    return X_train, X_test, y_train, y_test","e1ca40c2":"def pd2ndarray( dfList ):\n    \"\"\"\n    For each DataFrame in the list dfList, prepare the ndarray needed by the sklearn model\n    \n    Parameters\n    ----------\n    dfList: List of DataFrames\n    \n    Returns\n    --------\n    ndList: a list of ndarrays\n    \"\"\"\n    \n    # Create the function body according to the spec\n    ndList = []\n    for i in dfList:\n        ndList.append(i.values)\n    \n    # Change the return statement as appropriate\n    return ndList","61a8ef2e":"pos_name = ['pos_1','pos_2', 'pos_3','pos_4', 'pos_5', 'pos_6', 'pos_7', 'pos_8','pos_9', 'pos_10', 'pos_11', 'pos_12','pos_13']\nneg_name = ['neg_1','neg_2', 'neg_3','neg_4', 'neg_5', 'neg_6', 'neg_7','neg_8','neg_9','neg_10','neg_11','neg_12','neg_13']\nif_news_name = ['if_news_1','if_news_2', 'if_news_3','if_news_4', 'if_news_5', 'if_news_6', 'if_news_7','if_news_8','if_news_9','if_news_10','if_news_11', 'if_news_12','if_news_13']","36d42549":"def cal_lag(df, k):\n    for i in range(k):\n        df[pos_name[i]] = df['pos'].shift(i+1)\n        df[neg_name[i]] = df['neg'].shift(i+1)\n        df[if_news_name[i]] = df['if_news'].shift(i+1)","cd1c771e":"output['ret'] = output['Close'].pct_change()\ncal_lag(output, 13)\noutput = output.dropna()","2040987a":"X_train_list, X_test_list, y_train_list, y_test_list = [], [], [], []\n\n# # Split the data into a training and a test set\ndef lag_split(df, k):\n    for i in range(k):\n        independent = [if_news_name[i], pos_name[i], neg_name[i]]\n        X = df.loc[:, independent]\n        y = df.loc[:, ['ret'] ]\n        X_train, X_test, y_train, y_test = split(output, X, y)\n        X_train, X_test, y_train, y_test = pd2ndarray( [X_train, X_test, y_train, y_test] )\n        X_train_list.append(X_train)\n        X_test_list.append(X_test)\n        y_train_list.append(y_train)\n        y_test_list.append(y_test)","183326ec":"lag_split(output, 13)","5257d44f":"alpha, gamma, beta, delta = [], [], [], []\n\nfor i in range(13):\n    reg = LinearRegression().fit(X_train_list[i], y_train_list[i])\n    alpha.append(reg.intercept_[0])\n    gamma.append(reg.coef_[0][0])\n    beta.append(reg.coef_[0][1])\n    delta.append(reg.coef_[0][2])","1f3d0298":"week = list(range(1,14))\ngamma_cum = np.cumsum(gamma)\nbeta_cum = np.cumsum(beta)\ndelta_cum = np.cumsum(delta)","25e6209e":"poly = np.polyfit(week,gamma_cum,5)\npoly_1 = np.poly1d(poly)(week)\npoly = np.polyfit(week,beta_cum,5)\npoly_2 = np.poly1d(poly)(week)\npoly = np.polyfit(week,delta_cum,5)\npoly_3 = np.poly1d(poly)(week)","88de4fc3":"plt.figure(figsize=(8,6))\nplt.title('Cumulative Weekly Average Regression Coefficients')\nplt.xlabel('Week')\nplt.ylabel('Regression Coefficient')\nplt.plot(week,poly_1, label='News Effect') # return premium for firms that have neutral published news over firms with no news\nplt.plot(week,poly_2, label='Positive Sentiment') # excess return on positive sentiment with lag k\nplt.plot(week,poly_3, label='Negative Sentiment') # excess return on negative sentiment with lag k \nplt.legend()\nplt.show()","52c62445":"# plt.plot(np.cumsum(alpha), label='alpha') # return with no news at lag k\nplt.figure(figsize=(8,6))\nplt.plot(week,np.cumsum(gamma), label='News Effect') # return premium for firms that have neutral published news over firms with no news\nplt.plot(week,np.cumsum(beta), label='Positive Sentiment') # excess return on positive sentiment with lag k\nplt.plot(week,np.cumsum(delta), label='Negative Sentiment') # excess return on negative sentiment with lag k \nplt.legend()\nplt.show()","ce6aa0a3":"from nltk.tokenize import word_tokenize\ntext = \"All models are wrong, but some are useful.\"\nprint(word_tokenize(text))","19be52e5":"from nltk.corpus import stopwords\nword_list = word_tokenize(text)\nfiltered_word_list = word_list[:] #make a copy of the word_list\nfor word in word_list: # iterate over word_list\n    if word in stopwords.words('english'): \n        filtered_word_list.remove(word)","a81f5169":"filtered_word_list","45b31539":"from nltk.stem import PorterStemmer\nfrom nltk.stem import LancasterStemmer\n\nporter = PorterStemmer()\nlancaster=LancasterStemmer()\nstemmed_list = []\n\nprint(\"{0:20}{1:20}{2:20}\".format(\"Word\",\"Porter Stemmer\",\"lancaster Stemmer\"))\nfor word in filtered_word_list:\n    print(\"{0:20}{1:20}{2:20}\".format(word,porter.stem(word),lancaster.stem(word)))\n    stemmed_list.append(porter.stem(word))\n    ","94abab6a":"import nltk\nfrom nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\n\nstemmed_list\nprint(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\nfor word in stemmed_list:\n    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word)))","226c0128":"from nltk.sentiment.vader import SentimentIntensityAnalyzer\nimport unicodedata\nsid = SentimentIntensityAnalyzer()","6c6316fb":"sen = unicodedata.normalize('NFKD', text)\nss = sid.polarity_scores(sen)","8d8cabe5":"ss","9a672214":"Let's take a quick look at what the data looks like:","4a2146c5":"**Sentiment Analysis Example**"}}