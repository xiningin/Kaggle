{"cell_type":{"ee1ad0ea":"code","309a50ad":"code","59a949fe":"code","5610573a":"code","4f85bb70":"code","ed519d37":"code","8f22e7fc":"code","c05bab51":"code","06a33eaf":"code","046a9983":"code","272755df":"code","d40a78c3":"code","a20fc299":"code","6f57151e":"code","9a169580":"code","b9699c47":"code","edbe3084":"code","0e9c470b":"code","5da8f460":"code","c4627cf7":"code","d8bbb383":"code","168823a8":"code","80c42319":"code","a58dd734":"code","a9f82af3":"code","c31afbdc":"code","fc77e9c6":"code","1b9ad0c4":"code","ad12cee7":"code","c33a50d3":"code","76240c21":"code","30af88e8":"code","0d2d8113":"code","f463d138":"code","39a4b542":"code","33d1e657":"code","58a65507":"markdown","e8fd7a47":"markdown","d3d8e380":"markdown","ea0e3e03":"markdown","d2431b6c":"markdown","9c26ee27":"markdown","939d66d7":"markdown","e8fedd4e":"markdown","9aec6439":"markdown","a22f5c25":"markdown","11e25e1a":"markdown","11343376":"markdown","ffb07c84":"markdown","331539f5":"markdown"},"source":{"ee1ad0ea":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nfrom sklearn.model_selection import train_test_split\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n%matplotlib inline\n\npd.options.plotting.backend = \"plotly\"","309a50ad":"# Load Tweet dataset\ndf = pd.read_csv('\/kaggle\/input\/twitter-and-reddit-sentimental-analysis-dataset\/Twitter_Data.csv')\n# Output first five rows\ndf.head()","59a949fe":"# Any missing data?\ndf.isnull().sum()","5610573a":"# drop missing rows\ndf.dropna(axis=0, inplace=True)","4f85bb70":"# dimensionality of the data\ndf.shape","ed519d37":"# Map tweet categories\ndf['category'] = df['category'].map({-1.0:'Negative', 0.0:'Neutral', 1.0:'Positive'})\n# Output first five rows\ndf.head()","8f22e7fc":"# The distribution of sentiments\ndf.groupby('category').count().plot(kind='bar')","c05bab51":"# Calculate tweet lengths\ntweet_len = pd.Series([len(tweet.split()) for tweet in df['clean_text']])\n\n# The distribution of tweet text lengths\ntweet_len.plot(kind='box')","06a33eaf":"from wordcloud import WordCloud, STOPWORDS\n\ndef wolrdcount_gen(df, category):\n    '''\n    Generating Word Cloud\n    inputs:\n       - df: tweets dataset\n       - category: Positive\/Negative\/Neutral\n    '''\n    # Combine all tweets\n    combined_tweets = \" \".join([tweet for tweet in df[df.category==category]['clean_text']])\n                          \n    # Initialize wordcloud object\n    wc = WordCloud(background_color='white', \n                   max_words=50, \n                   stopwords = STOPWORDS)\n\n    # Generate and plot wordcloud\n    plt.figure(figsize=(10,10))\n    plt.imshow(wc.generate(combined_tweets))\n    plt.title('{} Sentiment Words'.format(category), fontsize=20)\n    plt.axis('off')\n    plt.show()","046a9983":"# Positive tweet words\nwolrdcount_gen(df, 'Positive')","272755df":"# Negative tweet words\nwolrdcount_gen(df, 'Negative')","d40a78c3":"import re    # RegEx for removing non-letter characters\n\nimport nltk\nnltk.download(\"stopwords\")\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import *\n\n\ndef tweet_to_words(tweet):\n    ''' Convert tweet text into a sequence of words '''\n    \n    # convert to lowercase\n    text = tweet.lower()\n    # remove non letters\n    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n    # tokenize\n    words = text.split()\n    # remove stopwords\n    words = [w for w in words if w not in stopwords.words(\"english\")]\n    # apply stemming\n    words = [PorterStemmer().stem(w) for w in words]\n    # return list\n    return words\n\nprint(\"\\nOriginal tweet ->\", df['clean_text'][0])\nprint(\"\\nProcessed tweet ->\", tweet_to_words(df['clean_text'][0]))","a20fc299":"# Apply data processing to each tweet\nX = list(map(tweet_to_words, df['clean_text']))","6f57151e":"from sklearn.preprocessing import LabelEncoder\n\n# Encode target labels\nle = LabelEncoder()\nY = le.fit_transform(df['category'])","9a169580":"print(X[0])\nprint(Y[0])","b9699c47":"X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)\n\nprint('Number of tweets in the total set : {}'.format(len(X)))\nprint('Number of tweets in the training set : {}'.format(len(X_train)))\nprint('Number of tweets in the testing set : {}'.format(len(X_test)))\n","edbe3084":"from sklearn.feature_extraction.text import CountVectorizer\n#from sklearn.feature_extraction.text import TfidfVectorizer\n\nvocabulary_size = 5000\n\n# Tweets have already been preprocessed hence dummy function will be passed in \n# to preprocessor & tokenizer step\ncount_vector = CountVectorizer(max_features=vocabulary_size,\n#                               ngram_range=(1,2),    # unigram and bigram\n                                preprocessor=lambda x: x,\n                               tokenizer=lambda x: x) \n#tfidf_vector = TfidfVectorizer(lowercase=True, stop_words='english')\n\n# Fit the training data\nX_train = count_vector.fit_transform(X_train).toarray()\n\n# Transform testing data\nX_test = count_vector.transform(X_test).toarray()","0e9c470b":"#import sklearn.preprocessing as pr\n\n# Normalize BoW features in training and test set\n#X_train = pr.normalize(X_train, axis=1)\n#X_test  = pr.normalize(X_test, axis=1)","5da8f460":"# print first 200 words\/tokens\nprint(count_vector.get_feature_names()[0:200])","c4627cf7":"# Plot the BoW feature vector\nplt.plot(X_train[2,:])\nplt.xlabel('Word')\nplt.ylabel('Count')\nplt.show()","d8bbb383":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom time import time\n\ndef train_predict(classifier, X_train, y_train, X_test, y_test): \n    '''\n    inputs:\n       - classifier: the algorithm to be trained and predicted on\n       - X_train: features training set\n       - y_train: label training set\n       - X_test: features testing set\n       - y_test: label testing set\n    '''\n    \n    results = {}\n    \n    # Apply training\n    clf = classifier.fit(X_train, y_train)\n        \n    # Get the predictions on the test set,\n    y_pred = clf.predict(X_test)        \n        \n    # Compute accuracy on the test set\n    results['accuracy'] = accuracy_score(y_test, y_pred)\n    \n    # Compute precision on the test set\n    results['precision'] = precision_score(y_test, y_pred, average='weighted')\n    \n    # Compute recall on the test set\n    results['recall'] = recall_score(y_test, y_pred, average='weighted')        \n        \n    # Compute F1-score on the test set\n    results['f1_score'] = f1_score(y_test, y_pred, average='weighted')\n        \n    # Return the results\n    return results","168823a8":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\n#from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nclf_A = GaussianNB()\nclf_B = DecisionTreeClassifier(random_state=1)\nclf_C = GradientBoostingClassifier(random_state=1)\n\n# For reducing training time\ntrain_size = 40000\ntest_size = 10000\n\n# Collect results \nresults = {}\nfor clf in [clf_A, clf_B, clf_C]:\n    clf_name = clf.__class__.__name__\n    print('Training {}...'.format(clf_name))\n    results[clf_name] = {}\n    results[clf_name] = train_predict(clf, X_train[:train_size], y_train[:train_size],\n                                      X_test[:test_size], y_test[:test_size])","80c42319":"# Dataframe from dict\nresults_df = pd.DataFrame.from_dict(results).transpose()\nprint(results_df)\n# quick visual check\nresults_df.plot()","a58dd734":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nmax_words = 5000\nmax_len=50\n\ndef tokenize_pad_sequences(text):\n    '''\n    This function tokenize the input text into sequnences of intergers and then\n    pad each sequence to the same length\n    '''\n    # Text tokenization\n    tokenizer = Tokenizer(num_words=max_words, lower=True, split=' ')\n    tokenizer.fit_on_texts(text)\n    # Transforms text to a sequence of integers\n    X = tokenizer.texts_to_sequences(text)\n    # Pad sequences to the same length\n    X = pad_sequences(X, padding='post', maxlen=max_len)\n    # return sequences\n    return X, tokenizer\n\nprint('Before Tokenization & Padding \\n', df['clean_text'][0])\nX, tokenizer = tokenize_pad_sequences(df['clean_text'])\nprint('After Tokenization & Padding \\n', X[0])","a9f82af3":"# Convert categorical variable into dummy\/indicator variables.\ny = pd.get_dummies(df['category'])\n# Train and Test split\nX_train, X_test,y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n# Extracting validation set from the train set\nvalid_size=1000\nX_valid, y_valid = X_train[-valid_size:], y_train[-valid_size:]\nX_test, y_test = X_train[:-valid_size], y_train[:-valid_size]\n\nprint('Train Set ->', X_train.shape, y_train.shape)\nprint('Validation Set ->', X_valid.shape, y_valid.shape)\nprint('Test Set ->', X_test.shape, y_test.shape)","c31afbdc":"import keras.backend as K\n\ndef f1_score(precision, recall):\n    ''' Function to calculate f1 score '''\n    \n    f1_val = 2*(precision*recall)\/(precision+recall+K.epsilon())\n    return f1_val","fc77e9c6":"from keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, Dense, Dropout\nfrom keras.metrics import Precision, Recall\n\nvocab_size = 5000\nembedding_size = 32\n\n# Build model\nmodel1 = Sequential()\nmodel1.add(Embedding(vocab_size, embedding_size, input_length=max_len))\nmodel1.add(LSTM(32))\nmodel1.add(Dropout(0.4))\nmodel1.add(Dense(3, activation='softmax'))\n\nprint(model1.summary())\n\n# Compile model\nmodel1.compile(loss='categorical_crossentropy', optimizer='adam', \n               metrics=['accuracy', Precision(), Recall()])\n\n# Train model\nnum_epochs = 10\nbatch_size = 32\nhistory1 = model1.fit(X_train, y_train,\n                      validation_data=(X_valid, y_valid),\n                      batch_size=batch_size, epochs=num_epochs, verbose=0)\n\n# Evaluate model on the test set\nloss, accuracy, precision, recall = model1.evaluate(X_test, y_test, verbose=0)\n# Print metrics\nprint('')\nprint('LSTM Accuracy  : {:.4f}'.format(accuracy))\nprint('LSTM Precision : {:.4f}'.format(precision))\nprint('LSTM Recall    : {:.4f}'.format(recall))\nprint('LSTM F1 score  : {:.4f}'.format(f1_score(precision, recall)))","1b9ad0c4":"from keras.models import Sequential\nfrom keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\nfrom keras.metrics import Precision, Recall\n\nvocab_size = 5000\nembedding_size = 32\n\n# Build model\nmodel2 = Sequential()\nmodel2.add(Embedding(vocab_size, embedding_size, input_length=max_len))\nmodel2.add(Bidirectional(LSTM(32)))\nmodel2.add(Dropout(0.4))\nmodel2.add(Dense(3, activation='softmax'))\n\nprint(model2.summary())\n\n# Compile model\nmodel2.compile(loss='categorical_crossentropy', optimizer='adam', \n               metrics=['accuracy', Precision(), Recall()])\n\n# Train model\nnum_epochs = 10\nbatch_size = 32\nhistory2 = model2.fit(X_train, y_train,\n                      validation_data=(X_valid, y_valid),\n                      batch_size=batch_size, epochs=num_epochs, verbose=0)\n\n# Evaluate model on the test set\nloss, accuracy, precision, recall = model2.evaluate(X_test, y_test, verbose=0)\n# Print metrics\nprint('')\nprint('Bidirectional LSTM Accuracy  : {:.4f}'.format(accuracy))\nprint('Bidirectional LSTM Precision : {:.4f}'.format(precision))\nprint('Bidirectional LSTM Recall    : {:.4f}'.format(recall))\nprint('Bidirectional LSTM F1 Score  : {:.4f}'.format(f1_score(precision, recall)))","ad12cee7":"from keras.models import Sequential\nfrom keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\nfrom keras.metrics import Precision, Recall\n\nvocab_size = 5000\nembedding_size = 32\n\n# Build model\nmodel3 = Sequential()\nmodel3.add(Embedding(vocab_size, embedding_size, input_length=max_len))\nmodel3.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\nmodel3.add(MaxPooling1D(pool_size=2))\nmodel3.add(Bidirectional(LSTM(32)))\nmodel3.add(Dropout(0.4))\nmodel3.add(Dense(3, activation='softmax'))\n\nprint(model3.summary())\n\n# Compile model\nmodel3.compile(loss='categorical_crossentropy', optimizer='adam', \n               metrics=['accuracy', Precision(), Recall()])\n\n# Train model\nnum_epochs = 10\nbatch_size = 32\nhistory3 = model3.fit(X_train, y_train,\n                      validation_data=(X_valid, y_valid),\n                      batch_size=batch_size, epochs=num_epochs, verbose=0)\n\n# Evaluate model on the test set\nloss, accuracy, precision, recall = model3.evaluate(X_test, y_test, verbose=0)\n# Print metrics\nprint('')\nprint('CNN + LSTM Accuracy  : {:.4f}'.format(accuracy))\nprint('CNN + LSTM Precision : {:.4f}'.format(precision))\nprint('CNN + LSTM Recall    : {:.4f}'.format(recall))\nprint('CNN + LSTM F1 Score  : {:.4f}'.format(f1_score(precision, recall)))","c33a50d3":"def plot_training_hist(history):\n    '''Function to plot history for accuracy and loss'''\n    \n    fig, ax = plt.subplots(1, 2, figsize=(10,4))\n    # first plot\n    ax[0].plot(history.history['accuracy'])\n    ax[0].plot(history.history['val_accuracy'])\n    ax[0].set_title('Model Accuracy')\n    ax[0].set_xlabel('epoch')\n    ax[0].set_ylabel('accuracy')\n    ax[0].legend(['train', 'validation'], loc='best')\n    # second plot\n    ax[1].plot(history.history['loss'])\n    ax[1].plot(history.history['val_loss'])\n    ax[1].set_title('Model Loss')\n    ax[1].set_xlabel('epoch')\n    ax[1].set_ylabel('loss')\n    ax[1].legend(['train', 'validation'], loc='best')\n    \nplot_training_hist(history3)","76240c21":"from sklearn.metrics import confusion_matrix\n\ndef plot_confusion_matrix(model, X_test, y_test):\n    '''Function to plot confusion matrix for the passed model and the data'''\n    \n    sentiment_classes = ['Negative', 'Neutral', 'Positive']\n    # use model to do the prediction\n    y_pred = model.predict(X_test)\n    # compute confusion matrix\n    cm = confusion_matrix(np.argmax(np.array(y_test),axis=1), np.argmax(y_pred, axis=1))\n    # plot confusion matrix\n    plt.figure(figsize=(8,6))\n    sns.heatmap(cm, cmap=plt.cm.Blues, annot=True, fmt='d', \n                xticklabels=sentiment_classes,\n                yticklabels=sentiment_classes)\n    plt.title('Confusion matrix', fontsize=16)\n    plt.xlabel('Actual label', fontsize=12)\n    plt.ylabel('Predicted label', fontsize=12)\n    \nplot_confusion_matrix(model3, X_test, y_test)","30af88e8":"# Save the model architecture & the weights\nmodel3.save('best_model.h5')\nprint('Best model saved')","0d2d8113":"from keras.models import load_model\n\n# Load model\nmodel = load_model('best_model.h5')\n\ndef predict_class(text):\n    '''Function to predict sentiment class of the passed text'''\n    \n    sentiment_classes = ['Negative', 'Neutral', 'Positive']\n    max_len=50\n    \n    # Transforms text to a sequence of integers using a tokenizer object\n    xt = tokenizer.texts_to_sequences(text)\n    # Pad sequences to the same length\n    xt = pad_sequences(xt, padding='post', maxlen=max_len)\n    # Do the prediction using the loaded model\n    yt = model.predict(xt).argmax(axis=1)\n    # Print the predicted sentiment\n    print('The predicted sentiment is', sentiment_classes[yt[0]])  ","f463d138":"predict_class(['Govt is unable to control rising petrol prices'])","39a4b542":"predict_class(['Govt spending on rural school education increased this year'])","33d1e657":"predict_class(['He is a best minister india ever had seen'])","58a65507":"### Data Visualization","e8fd7a47":"### Best Model Confusion Matrix","d3d8e380":"### Train and test split","ea0e3e03":"### Bag of words (BOW) feature extraction","d2431b6c":"### Tokenizing & Padding","9c26ee27":"### CNN + Bidirectional LSTM","939d66d7":"### Best Model Accuracy & Loss","e8fedd4e":"### Model save and load for the prediction","9aec6439":"### Data Preprocessing ","a22f5c25":"### Bidirectional LSTM","11e25e1a":"### Long short-term memory (LSTM)","11343376":"## Machine Learning Models","ffb07c84":"### Train & Test Split","331539f5":"## Deep Learning Sequence Models"}}