{"cell_type":{"3138f7db":"code","f5d0da3e":"code","5fd33cec":"code","07123102":"code","d08a1c32":"code","a9b77dde":"code","3950058b":"code","394d7b5f":"code","430a65e9":"code","9d2194bd":"code","0bfacf31":"code","52cf98b4":"code","d2c7b5b0":"code","a0438756":"code","b6b9b93f":"code","4c73dd54":"code","6ab1fc13":"code","384b26fe":"code","b7171544":"code","1726273e":"code","aba4d39d":"code","61563db1":"code","b60b6d5b":"code","99a9b1db":"code","3b89ffaa":"code","d85a8ad7":"code","1363282a":"code","b98e001a":"code","5eae7221":"code","b3411be3":"code","3c4ab466":"code","fd8afc3e":"markdown","8a0e2b87":"markdown","6658c128":"markdown","4c31c471":"markdown","be1c05f9":"markdown","2bd2bd43":"markdown","b9370b32":"markdown","eed38abb":"markdown","60c4040e":"markdown","318a88a3":"markdown","2cd45c78":"markdown","6fcc106a":"markdown","6992d21f":"markdown","7f9104be":"markdown","44f183fa":"markdown","c460802b":"markdown","9e97e2c5":"markdown","5d3775ea":"markdown","fd2a04d1":"markdown","f137b6c2":"markdown","2580cc06":"markdown","325ac7c5":"markdown"},"source":{"3138f7db":"#Import Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom scipy.cluster.hierarchy import dendrogram, linkage","f5d0da3e":"#import Dataset\ndata = pd.read_csv('..\/input\/dataset.csv')\ndata3 = data.loc[:,['kume']].values","5fd33cec":"#rename columns and drop not importance axis\nnames = [\"araba-tur\",\"airbag\",\"araba-yas\",\"araba-performans\",\"araba-kullan\u0131m-tur\",\"araba-km\",\n         \"araba-yak\u0131t\",\"araba-segment\",\"araba-parca\",\"arac-hitap\",\n         \"butce\",\"konfor-skorlama\",\"araba-model\",\"marka\",\"sahibinden-link\",\"arabam-link\",\"kume\"]\n\nnames2 = [\"araba-tur\",\"airbag\",\"araba-yas\",\"araba-performans\",\"araba-kullan\u0131m-tur\",\"araba-km\",\n         \"araba-yak\u0131t\",\"araba-segment\",\"araba-parca\",\"arac-hitap\",\n         \"butce\",\"konfor-skorlama\",\"marka\",\"kume\"]\n\ndata = data.rename(columns=dict(zip(data.columns, names)))\n\ndata = data.drop(['sahibinden-link'], axis=1)\ndata = data.drop(['arabam-link'], axis=1)\ndata = data.drop(['araba-model'], axis=1)\ndata.head(5)","07123102":"#Creating Mapping Dictionaries\nmapping_tur = {\"2. El\":0, \"S\u0131f\u0131r\":1}\nmapping_yas = {\"0-1\":1, \"1-3\":2, \"3-5\":3, \"5-8\":4, \"8-10\":5, \"10-12\":6, \"12+\":7}\nmapping_performans = {\"Vasat Performans, Az Yakmas\u0131\":0, \"Y\u00fcksek Performans, \u00c7ok Yakmas\u0131\":1, \"Standart Performans, Az Yakmas\u0131\":0}\nmapping_kullan\u0131m = {\"Uzun S\u00fcre Binme Odakl\u0131\":0, \"Sat\u0131p Para Kazanma Odakl\u0131\":1}\nmapping_km = {\"0-25.000\":1, \"0-25.002\":1, \"25.000-50.000\":2, \"50.000-100.000\":3, \"100.000-200.000\":4, \"200.000+\":5}\nmapping_yak\u0131t = {\"LPG\":0, \"Dizel\":1, \"Benzinli\":2, \"Farketmez\":3}\nmapping_segment = {\"A Segmenti (Ekonomik Az Yakanlar, i10)\":0, \"B Segmenti (Hyundai Getz, Polo)\":1, \"C Segmenti (Honda Civic, Renault Fluence)\":2,\n                  \"D Segmenti (Mercedes C Serisi, VW Passat, Ford Mondeo)\":3, \"E Segmenti (BMW 5 serisi, Volvo s80)\":4, \"F Segmenti (Audi A8, BMW 7 serisi)\":5,\n                  \"G Segmenti (Porshce 911)\":6, \"J Segmenti (4x4 Jipler vs.)\":7, \"D Segmenti (Mercedes C Serisi, VW Passat)\":3}\nmapping_parca = {\"S\u00fcrekli Sorun \u00c7\u0131kars\u0131n Ucuz Par\u00e7alar\u0131 Olsun\":0, \"Par\u00e7alar Sa\u011flam ve Pahal\u0131 Olsun, Az Sorun \u00c7\u0131kars\u0131n.\":1, \"Arada Bir Sorun \u00c7\u0131kars\u0131n, Ucuz Par\u00e7alar\u0131 Olsun\":0}\nmapping_hitap = {\"Aile Arac\u0131\":0, \"Ticari\":1, \"\u015eah\u0131s Arac\u0131\":2, \"Off Road\":3}\nmapping_butce = {\"0-15.000\":0, \"15.000-25.000\":1, \"25.000-35.000\":2, \"35.000-45.000\":3, \"45.000-55.000\":4, \"55.000-65.000\":5,\n                \"65.000-75.000\":6, \"75.000-85.000\":7, \"85.000-100.000\":8, \"100.000-200.000\":9, \"200.000+\":10}\nmapping_marka = {\"Alfa Romeo\":0, \"Audi\":1, \"Bmw\":2, \"Chevrolet\":3, \"Citroen\":4, \"Dacia\":5, \"Fiat\":6, \"Ford\":7, \"Honda\":8,\n                \"Hyundai\":9, \"Kia\":10, \"Mercedes\":11, \"Mitsubishi\":12, \"Nissan\":13, \"Opel\":14, \"Peugeot\":15, \"Porsche\":16,\n                \"Renault\":17, \"Toyota\":18, \"Volkswagen\":19, \"Volvo\":20, \"Skoda\":21, \"Mazda\":22, \"Mini\":23, \"Land Rover\":24,\n                \"Seat\":25}\n\n#Mapping on Data\ndata['araba-tur'] = data['araba-tur'].map(mapping_tur)\ndata['araba-yas'] = data['araba-yas'].map(mapping_yas)\ndata['araba-performans'] = data['araba-performans'].map(mapping_performans)\ndata['araba-kullan\u0131m-tur'] = data['araba-kullan\u0131m-tur'].map(mapping_kullan\u0131m)\ndata['araba-km'] = data['araba-km'].map(mapping_km)\ndata['butce'] = data['butce'].map(mapping_butce)\ndata['araba-yak\u0131t'] = data['araba-yak\u0131t'].map(mapping_yak\u0131t)\ndata['araba-segment'] = data['araba-segment'].map(mapping_segment)\ndata['araba-parca'] = data['araba-parca'].map(mapping_parca)\ndata['arac-hitap'] = data['arac-hitap'].map(mapping_hitap)\ndata['marka'] = data['marka'].map(mapping_marka)\n\ndata8=data\ndata9=data\ndata11 = data['marka']\ndata.head(5)","d08a1c32":"#I going to define some funcs for Clustering DataSet base on brand\n\ndef plotData(data, marka = None):\n    if marka != None: \n        data = data[(data.marka == mapping_marka[marka])]\n        print(\"Opinion of \", marka)\n    fig = plt.figure(figsize=(25,10))\n    \n    names = [\"araba-tur\",\"airbag\",\"araba-yas\",\"araba-performans\",\"araba-kullan\u0131m-tur\",\"araba-km\",\n         \"araba-yak\u0131t\",\"araba-segment\",\"araba-parca\",\"arac-hitap\",\n         \"butce\",\"konfor-skorlama\",\"marka\",\"kume\"]\n    \n    for i in range(0,13):\n        p1 = fig.add_subplot(2,5,i-3)\n        data[names[i]].value_counts().plot(kind = 'pie', autopct='%.1f%%'); \n        plt.ylabel(\" \", fontsize = 15)\n        plt.title(Q[i-4])\n    plt.grid()\n    plt.savefig(marka)\n    plt.savefig(marka + \".pdf\")\n    \ndef getOpinion(data, marka = None):\n    if marka != None: \n        data = data[(data.marka == mapping_marka[marka])]\n    return [data[col].mean() for col in names2[0:]]\n\nopinions = dict()\nfor k in mapping_marka.keys():\n    opinions[k] = getOpinion(data, marka = k)\n\ndf = pd.DataFrame.from_dict(opinions)\ndf.rename(index = dict(zip(range(len(names2[0:])),names2[0:])),inplace=True)\ndf = df.reindex(columns=['Alfa Romeo', 'Audi', 'Bmw', 'Chevrolet', 'Citroen', 'Dacia', 'Fiat', 'Ford', 'Honda', 'Hyundai',\n                        'Kia', 'Mercedes', 'Mitsubishi', 'Nissan', 'Opel', 'Peugeot', 'Renault', 'Toyota',\n                        'Volkswagen', 'Volvo'])\ndf.T","a9b77dde":"from sklearn.preprocessing import MinMaxScaler\n\ndata9 = data9.drop(['kume'], axis=1)\ndata9 = data9.drop(['marka'], axis=1)\n\nscaler = MinMaxScaler(feature_range = (0,1))\nscaler.fit(data9)\ndata9 = scaler.transform(data9)\ndata9 = pd.DataFrame(data9)\n\ndata9 = pd.concat([data9, data11], axis = 1)\n\nnames = [\"araba-tur\",\"airbag\",\"araba-yas\",\"araba-performans\",\"araba-kullan\u0131m-tur\",\"araba-km\",\n         \"araba-yak\u0131t\",\"araba-segment\",\"araba-parca\",\"arac-hitap\",\n         \"butce\",\"konfor-skorlama\",\"marka\"]\n\ndata9 = data9.rename(columns=dict(zip(data9.columns, names)))\ndata9.head(8)","3950058b":"names5 = [\"araba-tur\",\"airbag\",\"araba-yas\",\"araba-performans\",\"araba-kullan\u0131m-tur\",\"araba-km\",\n         \"araba-yak\u0131t\",\"araba-segment\",\"araba-parca\",\"arac-hitap\",\n         \"butce\",\"konfor-skorlama\",\"marka\"]\n\nnames4 = [\"araba-tur\",\"airbag\",\"araba-yas\",\"araba-performans\",\"araba-kullan\u0131m-tur\",\"araba-km\",\n         \"araba-yak\u0131t\",\"araba-segment\",\"araba-parca\",\"arac-hitap\",\n         \"butce\",\"konfor-skorlama\",\"marka\"]\n\ndef getOpinion2(data9, marka = None):\n    if marka != None: \n        data9 = data9[(data9.marka == mapping_marka[marka])]\n    return [data9[col].mean() for col in names5[0:]]","394d7b5f":"opinions = dict()\nfor k in mapping_marka.keys():\n    opinions[k] = getOpinion2(data9, marka = k)\n\ndf2 = pd.DataFrame.from_dict(opinions)\ndf2.rename(index = dict(zip(range(len(names4[0:])),names4[0:])),inplace=True)\ndf2 = df2.reindex(columns=['Alfa Romeo', 'Audi', 'Bmw', 'Chevrolet', 'Citroen', 'Dacia', 'Fiat', 'Ford', 'Honda', 'Hyundai',\n                        'Kia', 'Mercedes', 'Mitsubishi', 'Nissan', 'Opel', 'Peugeot', 'Renault', 'Toyota',\n                        'Volkswagen', 'Volvo'])\nprint(\"\")\ndf2.T","430a65e9":"import numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nnames2 = [\"araba-tur\",\"airbag\",\"araba-yas\",\"araba-performans\",\"araba-kullan\u0131m-tur\",\"araba-km\",\n         \"araba-yak\u0131t\",\"araba-segment\",\"araba-parca\",\"arac-hitap\",\n         \"butce\",\"konfor-skorlama\",\"marka\",\"kume\"]\n\ndf2 = data8\nx = df2.loc[:, names2].values\ny = df2.loc[:,['kume']].values\nx = StandardScaler().fit_transform(x)","9d2194bd":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)\nprincipalComponents = pca.fit_transform(x)\nprincipalDf = pd.DataFrame(data = principalComponents\n             , columns = ['principal component 1', 'principal component 2'])\n","0bfacf31":"finalDf = pd.concat([principalDf, df2[['kume']]], axis = 1)\nfinalDf.head(8)","52cf98b4":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn.ensemble import RandomForestClassifier\n\npredictors = finalDf.drop([\"kume\"], axis=1)\ntarget = finalDf[\"kume\"]\nX_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size = 0.25, random_state = 0)","d2c7b5b0":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nmodels = []\nmodels.append(('Logistic Regression', LogisticRegression()))\nmodels.append(('Naive Bayes', GaussianNB()))\nmodels.append(('Decision Tree (CART)',DecisionTreeClassifier())) \nmodels.append(('K-NN', KNeighborsClassifier()))\nmodels.append(('AdaBoostClassifier', AdaBoostClassifier()))\nmodels.append(('BaggingClassifier', BaggingClassifier()))\nmodels.append(('RandomForestClassifier', RandomForestClassifier()))\n\nfor name, model in models:\n    model = model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    from sklearn import metrics\n    print(\"%s -> ACC: %%%.2f\" % (name,metrics.accuracy_score(y_test, y_pred)*100))","a0438756":"#Lets Have a Look At Correlations\nfig, ax = plt.subplots()\nfig.set_size_inches(15,15)\nsns.heatmap(data.corr(),cbar=True, annot=True, square=True, annot_kws={'size': 12})\nplt.tight_layout()\nplt.savefig('2-elaraba-corr.png')","b6b9b93f":"corr = df.corr()\nfig, ax = plt.subplots()\nfig.set_size_inches(20,20)\nmask = np.zeros_like(corr) #e\u011fer corr bozuksa markalar\u0131 g\u00f6stermiyorsa bunu \nmask[np.triu_indices_from(mask)] = True #ve bunu silip shift+enter yap\u0131n ondan sonra geri yap\u0131\u015ft\u0131r\u0131n ve shift+enter\nsns.heatmap(corr, mask=mask, cbar=True, annot=True, square=True, annot_kws={'size': 10})\nplt.savefig('car-corr.png')","4c73dd54":"qs = [q for q in questions.features if q not in [\"Sex\",\"Age\",\"Region\",\"Education\"]]\nqf = df.loc[qs]\n\nfig, ax = plt.subplots(figsize=(20,6))\nax.xaxis.set(ticks=range(0,11), # Manually set x-ticks\nticklabels=qs)\nqf[['Alfa Romeo','Audi','Bmw', 'Chevrolet', 'Citroen', 'Dacia', 'Fiat', 'Ford', 'Honda', 'Hyundai',\n                        'Kia', 'Mercedes', 'Mitsubishi', 'Nissan', 'Opel', 'Peugeot', 'Renault', 'Toyota',\n                        'Volkswagen', 'Volvo']].plot(ax=ax,alpha=0.75, rot=80)\nplt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\nplt.grid()\nplt.savefig('compare.pdf')","6ab1fc13":"df = df.T\ndf = df.drop(['marka'], axis=1)\ndf = df.drop(['kume'], axis=1)\ndf = df.T\n\n#plot data\nfig, ax = plt.subplots(figsize=(30,10))\nax.xaxis.set(ticks=range(0,14), # Manually set x-ticks\nticklabels=[\"araba-tur\",\"airbag\",\"araba-yas\",\"araba-performans\",\"araba-kullan\u0131m-tur\",\"araba-km\",\n         \"araba-yak\u0131t\",\"araba-segment\",\"araba-parca\",\"arac-hitap\",\n         \"butce\",\"konfor-skorlama\",\"kume\"])\ndf.plot(ax=ax)\nplt.grid()","384b26fe":"#Split Data By Train and Test\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn.ensemble import RandomForestClassifier\n\ndata = data.drop(['marka'], axis=1)\npredictors = data.drop([\"kume\"], axis=1)\ntarget = data[\"kume\"]\nX_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size = 0.25, random_state = 0)","b7171544":"#Le\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nmodels = []\nmodels.append(('Logistic Regression', LogisticRegression()))\nmodels.append(('Naive Bayes', GaussianNB()))\nmodels.append(('Decision Tree (CART)',DecisionTreeClassifier())) \nmodels.append(('K-NN', KNeighborsClassifier()))\nmodels.append(('AdaBoostClassifier', AdaBoostClassifier()))\nmodels.append(('BaggingClassifier', BaggingClassifier()))\nmodels.append(('RandomForestClassifier', RandomForestClassifier()))\n\nfor name, model in models:\n    model = model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    from sklearn import metrics\n    print(\"%s -> ACC: %%%.2f\" % (name,metrics.accuracy_score(y_test, y_pred)*100))","1726273e":"#Lets Look at Feature Importance\nrf = RandomForestClassifier(n_estimators=200, random_state=39, max_depth=2)\nrf.fit(X_train, y_train)\nfrom sklearn.ensemble import RandomForestClassifier\nquestions = pd.DataFrame({'features': data.columns[:-1],'importance': rf.feature_importances_})\nquestions = questions.sort_values(by='importance', ascending=False)\nquestions","aba4d39d":"import networkx as nx\n\n#Changes from dataframe to matrix, so it is easier to create a graph with networkx\ncor_matrix = np.asmatrix(corr)\n\n#Crates graph using the data of the correlation matrix\nG = nx.from_numpy_matrix(cor_matrix)\n\n#relabels the nodes to match the  stocks names\nG = nx.relabel_nodes(G,lambda x: df.columns[x])","61563db1":"def drawGraph(G, size = 20):\n    fig, ax = plt.subplots()\n    fig.set_size_inches(size,size)\n    \n    pos_fr = nx.fruchterman_reingold_layout(G)\n    edges = G.edges()\n\n    weights = [G[u][v]['weight'] for u,v in edges]\n    labels = {e: round(G[e[0]][e[1]]['weight'],2) for e in edges}\n    weights2 = [w**2 for w in weights]\n\n    nx.draw(G, pos=pos_fr, node_size=1000, node_color='lightblue', with_labels=True)\n\n    # Plot edge labels\n    nx.draw_networkx_edge_labels(G, pos=pos_fr, edge_labels=labels)\n    plt.savefig('graph.pdf')\n    \ndrawGraph(G)","b60b6d5b":"# remove edges with correlation < 0.5\nG.remove_edges_from([(u,v) for u,v,e in G.edges(data = True) if e['weight'] < 0.5])\ndrawGraph(G, size =30)","99a9b1db":"# remove edges with correlation < 0.8\nG.remove_edges_from([(u,v) for u,v,e in G.edges(data = True) if e['weight'] < 0.8])\ndrawGraph(G, size=30)","3b89ffaa":"# remove edges with correlation < 0.9\nG.remove_edges_from([(u,v) for u,v,e in G.edges(data = True) if e['weight'] < 0.9])\ndrawGraph(G, size=30)","d85a8ad7":"#Changes from dataframe to matrix, so it is easier to create a graph with networkx\ncor_matrix = np.asmatrix(df.T.corr())\n\n#Crates graph using the data of the correlation matrix\nG = nx.from_numpy_matrix(cor_matrix)\n\n#relabels the nodes to match the  stocks names\nG = nx.relabel_nodes(G,lambda x: df.T.columns[x])","1363282a":"drawGraph(G, size = 25)","b98e001a":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.spatial.distance import pdist, squareform\nfrom scipy.cluster.hierarchy import linkage, dendrogram\n\n\n#data_array = ((np.float, len(data['marka'].dtype.names)))\ndata_array = df.transpose()\ndata_array = np.array(data_array)","5eae7221":"data_dist = pdist(data_array) # computing the distance\ndata_link = linkage(data_dist)","b3411be3":"dendrogram(data_link,labels=data_array.dtype.names)\nplt.xlabel('Araba Modelleri')\nplt.ylabel('Uzakl\u0131k')\nplt.suptitle('Samples clustering', fontweight='bold', fontsize=14);","3c4ab466":"# Compute and plot first dendrogram.\nfig = plt.figure(figsize=(12,12))\n# x ywidth height\nax1 = fig.add_axes([0.05,0.1,0.2,0.6])\nY = linkage(data_dist, method='single')\nZ1 = dendrogram(Y, orientation='right',labels=data_array.dtype.names) # adding\/removing the axes\nax1.set_xticks([])\n\n\n# Compute and plot second dendrogram.\nax2 = fig.add_axes([0.3,0.71,0.6,0.2])\nZ2 = dendrogram(Y)\nax2.set_xticks([])\nax2.set_yticks([])\n\n#Compute and plot the heatmap\naxmatrix = fig.add_axes([0.3,0.1,0.6,0.6])\nidx1 = Z1['leaves']\nidx2 = Z2['leaves']\nD = squareform(data_dist)\nD = D[idx1,:]\nD = D[:,idx2]\nim = axmatrix.matshow(D, aspect='auto', origin='lower',cmap=plt.cm.YlGnBu)\naxmatrix.set_xticks([])\naxmatrix.set_yticks([])\n\n# Plot colorbar.\naxcolor = fig.add_axes([0.91,0.1,0.02,0.6])\nplt.colorbar(im, cax=axcolor)","fd8afc3e":"**Thanks for reading, Dont Forget Your comments are worth gold for me**","8a0e2b87":"## 3- Buildling Machine Learning Models\n<a id=\"ch11\"><\/a>","6658c128":"## Source \n* [SciPy Hierarchical Clustering and Dendrogram Tutorial](https:\/\/joernhees.de\/blog\/2015\/08\/26\/scipy-hierarchical-clustering-and-dendrogram-tutorial\/)\n* [PCA using Python (scikit-learn)](https:\/\/towardsdatascience.com\/pca-using-python-scikit-learn-e653f8989e60)\n* [Uzay Cetin's Network Analysis](http:\/\/github.com\/uzay00)\n","4c31c471":"### Build PCA\n<a id=\"ch6\"><\/a>","be1c05f9":"### Clustering to Data\n<a id=\"ch16\"><\/a>","2bd2bd43":"### PCA Model Scores\n<a id=\"ch7\"><\/a>","b9370b32":"### Feature Network Anaylsis\n<a id=\"ch14\"><\/a>","eed38abb":"## 4- Network Analysis\n<a id=\"ch12\"><\/a>","60c4040e":"### Mapping\n<a id=\"ch2\"><\/a>","318a88a3":"## 1- Data Pre-Processing\n<a id=\"ch0\"><\/a>\n\n**Pre-Processing is an important part of a model, if you can not make the right moves in this part, your model can not be built or stabilized**","2cd45c78":"## 2- Data Visualition\n<a id=\"ch8\"><\/a>","6fcc106a":"# Car Forecast with Multi Class Classification\n## Araba-Oner \n\n**Problem:** I have 20 brand-name classes. we are trying to cluster them according to the answers that people give to 12 questions and return them to the most suitable car models.\n\n**Project:** 12 Soruda To recommend the closest car models to the user.\n\n**Goal:** To remove 5 clusters from the segment. In total you will have to predict over the average 400-600 models and suggest 10 Models that are best for you.\n\n### **This project was done by the DataRaccoons Team.**\n#### **DataRaccoons:** [Web](https:\/\/www.dataraccoons.com\/) \/ [Kaggle](https:\/\/www.kaggle.com\/dataraccoons)\n\n**August 2018 - DataRaccoons**\n\n**Real Time Web Site : [www.arabaoner.com](https:\/\/www.arabaoner.com\/)**\n\n\n- **Dictionary For Columns:** \n    - araba-tur : Used\/New\n    - airbag : Airbag \u0130mportance Score(1-5)\n    - araba-yas : Car Age\n    - araba-performans : High Condition- High Fuel Consumption \/ Standart Condition - Standart Fuel Consumption\n    - araba-kullan\u0131m-tur : Long Time Use \/ Buy For Sell\n    - araba-km : mileage range of  car\n    - araba-yak\u0131t : Fuel Type (Gasoline, Gas, Diesel)\n    - araba-segment : Car Segment, (A,B,C,D,E...)\n    - araba-parca : Robust and expensive parts \/ Low cost, poor quality parts \n    - arac-hitap : Car Use Type (Family, Yourself, Job, Whatever)\n    - butce : Car Budget\n    - konfor-skorlama : Car Comfort Score(1-5)\n    - araba-model : Car Models\n    - marka : Car Brands\n    - kume : Clusted Data","6992d21f":"### M\u0130N - MAX SCALE\n** I will now do the scale mechanism to better understand the properties of the cars in correlations between cars**\n<a id=\"ch4\"><\/a>","7f9104be":"### Brand Network Anaylsis\n<a id=\"ch13\"><\/a>","44f183fa":"## 5- Clustering with hierarchical clustering\n* ** I did a clustering by looking at the correlations without writing the code. I did this with the help of the clustering method you will see now.**\n<a id=\"ch15\"><\/a>","c460802b":"### Creating Dataset for Target Value\n<a id=\"ch3\"><\/a>","9e97e2c5":"### Graphs\n<a id=\"ch10\"><\/a>","5d3775ea":"### Correlations\n<a id=\"ch9\"><\/a>","fd2a04d1":"# Introduction\n1. [Data Pre-Processing](#ch0)\n - [Cleaning](#ch1)\n - [Mapping](#ch2)\n - [Creating the DataSet for target value](#ch3)\n - [Min Max Scale](#ch4)\n - [PCA(Feature Selection)](#ch5)\n       - [Build PCA](#ch6)\n       - [PCA Model Scores](#ch7)\n2. [Data Visualization](#ch8)\n - [Feature, Brand Correlations](#ch9)\n - [Feature, Brand Graphs](#ch10)\n3. [Building Machine Learning Model](#ch11)\n4. [Network Analysis](#ch12)\n - [Features Network Analysis](#ch13)\n - [Brands Network Analysis](#ch14)\n5. [Clustering with hierarchical clustering](#ch15)\n - [Clustering Data](#ch16)\n - [Clustered Data Correlations](#ch17)","f137b6c2":"### Cleaning\n<a id=\"ch1\"><\/a>","2580cc06":"### Clustered Data Correlations\n<a id=\"ch17\"><\/a>","325ac7c5":"## PCA (Feature Selection)\n<a id=\"ch5\"><\/a>\n**What is The PCA?**\n\n- Pca is a useful statistical technique used in recognition, classification, image compression fields. pca is a very effective method to reveal the necessary information on the front. to reduce the number of dimensions, and to compress the data by finding the general properties of the oversized data. The basic logic behind the PCA is to show a multidimensional data with fewer variables by catching the basic features of the verb. the point at which some properties will be lost due to size reduction; but it is intended that these lost characteristics contain little information about the population. usually face detection is used in image compression areas\n\n\n* #### **As you know, the way to get high accuracy goes through the right feature selection. So we will do feature selection with PCA at once and what will be the results?**"}}