{"cell_type":{"d0bbb836":"code","839568a1":"code","e558e1da":"code","6e50c873":"code","3e35fd1d":"code","f8a3521e":"code","c089cb30":"code","9dd3b6bf":"code","ead1c2ad":"code","4a9f8936":"code","1625e8fc":"code","9bb8b4b3":"code","5da9fd4a":"code","2535c3fe":"code","96b18444":"code","794f55cf":"code","cc41b08a":"code","09b366b8":"code","0171c500":"markdown","cbe85f9a":"markdown","62f759ae":"markdown","a36e0239":"markdown","10490d7a":"markdown","dc255eba":"markdown","655d9ac1":"markdown","43d6142a":"markdown","0559cc63":"markdown","55515e23":"markdown","afe5f64f":"markdown","5cfca7bc":"markdown","066abb98":"markdown","38d36265":"markdown","bfa4dff3":"markdown","650d587f":"markdown"},"source":{"d0bbb836":"!curl -s https:\/\/course.fast.ai\/setup\/colab | bash\n!pip uninstall torch torchvision -y\n!pip install \"torch==1.4\" \"torchvision==0.5.0\"","839568a1":"output = \"\/kaggle\/working\"\ninput = \"\/kaggle\/input\"\n!ls \/kaggle\/input","e558e1da":"%reload_ext autoreload\n%autoreload 2 \n%matplotlib inline\n\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","6e50c873":"import numpy as np \nimport string\nimport pandas as pd\nfrom fastai.vision import *\nfrom fastai.imports import *\nfrom fastai.metrics import error_rate\nimport torch\ntorch.cuda.is_available()","3e35fd1d":"test_df = pd.read_csv(f'{input}\/test.csv')\ntest_df.info()\ntest_df.head(10)","f8a3521e":"train_df = pd.read_csv(f'{input}\/train_en.csv')\ntrain_df.info()\ntrain_df.head(10)","c089cb30":"train_df['n_product_title_wds'] = train_df['product_title'].apply(lambda x: len(x.split()))\n\ntrain_df['n_product_title_wds'].describe()\ntrain_df['n_product_title_wds'].value_counts().plot(kind='hist')","9dd3b6bf":"train_df['n_product_title_len'] = train_df['product_title'].fillna('').apply(len)\n\ntrain_df['n_product_title_len'].describe()\ntrain_df['n_product_title_len'].value_counts().plot(kind='hist')","ead1c2ad":"punct = set(string.punctuation)\n\nemoji = set()\nfor s in test_df['product_title'].fillna('').astype(str):\n    for c in s:\n        if c.isdigit() or c.isalpha() or c.isalnum() or c.isspace() or c in punct:\n            continue\n        emoji.add(c)\nprint(''.join(emoji))","4a9f8936":"train_df['product_title_emoji'] = train_df['product_title'].fillna('').apply(lambda x: sum(c in emoji for c in x))\ntrain_df['product_title_emoji'].describe()","1625e8fc":"train_cn_df = pd.read_csv(f'{input}\/train_tcn.csv')\ntrain_cn_df.info()\ntrain_cn_df.head(10)","9bb8b4b3":"!pip install tensor2tensor","5da9fd4a":"from transformers import MarianMTModel, MarianTokenizer\nbase_model = \"xlm-mlm-xnli15-1024\"\nbase_model = \"t5-base\"\nmodel = AutoModelWithLMHead.from_pretrained(base_model)\ntokenizer = AutoTokenizer.from_pretrained(base_model)\n\ninputs = tokenizer.encode(\"translate Chinese to English: \u4f60\u597d \u4e16\u754c\", return_tensors=\"pt\")\noutputs = model.generate(inputs, max_length=40, num_beams=4, early_stopping=True)\n\nprint(outputs)\n","2535c3fe":"!pip install sacremoses subword_nmt fairseq\n!pip install fairseq","96b18444":"import torch\nimport fairseq\n\n# List available models\ntorch.hub.list('pytorch\/fairseq')  # [..., 'lightconv.glu.wmt17.zh-en', ... ]\n\n# # Load a transformer trained on WMT'16 En-De\nzh2en = torch.hub.load('pytorch\/fairseq', 'lightconv.glu.wmt17.zh-en', tokenizer='moses', bpe='subword_nmt')\n\n# # The underlying model is available under the *models* attribute\n# assert isinstance(zh2en.models[0], fairseq.models.lightconv.LightConvModel)\n\n# Translate a sentence\n","794f55cf":"import torch\nfrom transformers import XLMTokenizer, XLMWithLMHeadModel\ntokenizer = XLMTokenizer.from_pretrained(\"xlm-mlm-tlm-xnli15-1024\")\nmodel = XLMWithLMHeadModel.from_pretrained(\"xlm-mlm-tlm-xnli15-1024\")\ninputs = tokenizer.encode(\"translate English to Chinese: Hugging Face is a technology company based in New York and Paris\", return_tensors=\"pt\")\noutputs = model.generate(inputs, max_length=40, num_beams=4, early_stopping=True)\nprint(tokenizer.decode(outputs.tolist()[0]))\n","cc41b08a":"zh2en.translate('\u4f60\u597d \u4e16\u754c')","09b366b8":"!pip install adaptnlp","0171c500":"## Tensor2Tensor","cbe85f9a":"#### Word Length","62f759ae":"# Requirements","a36e0239":"## Training Data","10490d7a":"## Test","dc255eba":"# Baseline models","655d9ac1":"# Notebook Setup\n","43d6142a":"### Chinese","0559cc63":"#### Emoji Check","55515e23":"#### Char Length","afe5f64f":"## Fairseq\n\n- [Tutorial](https:\/\/github.com\/twairball\/fairseq-zh-en)","5cfca7bc":"## AdaptNLP","066abb98":"# Chinese - English BLEU Benchmarks\n\n## [WMT17](http:\/\/www.statmt.org\/wmt17\/translation-task.html#download)\n\n## [IWSLT15 TED Talk](https:\/\/sites.google.com\/site\/iwsltevaluation2015\/mt-track)\n\n## [WMT19](http:\/\/www.statmt.org\/wmt19\/translation-task.html#download)\n\n## [NIST](https:\/\/github.com\/nusnlp\/c2e-mt-benchmark)","38d36265":"# Setup","bfa4dff3":"# Loading Data","650d587f":"### English"}}