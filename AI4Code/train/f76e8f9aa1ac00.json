{"cell_type":{"148c9f4c":"code","48b5a6d2":"code","8d5e6faa":"code","86f78338":"code","5aaab821":"code","a2787763":"code","2d9a6116":"code","2e06e18f":"code","db70f6f7":"code","fba62d20":"code","66ed5caf":"code","17eb4e67":"code","898acbb3":"code","a21905b9":"code","613641b4":"code","b3a20b32":"code","3655edb7":"code","acca6f91":"code","65ce0820":"code","5635142b":"code","5be0761a":"code","ca2d6b1a":"code","a15566dd":"code","5a3c43f4":"code","bba50b82":"code","62fda23b":"code","49a48899":"code","a09a77c5":"code","86e1bf6a":"code","f56fd16f":"code","308b13f9":"code","02d4a8d3":"code","dc86da12":"code","365147e9":"code","089d9de6":"code","027eee66":"code","be185167":"code","b3c77ea1":"code","71367836":"code","1867b376":"code","3540a647":"code","eab92091":"code","9b068bfa":"markdown","dbed9995":"markdown","9228f862":"markdown","b38e69ba":"markdown","a4cd1886":"markdown","6e6046f5":"markdown","3d670392":"markdown","38a0b410":"markdown","06f5e9ab":"markdown","7c298a85":"markdown","ebd53d82":"markdown","0ba70a10":"markdown","1fab1770":"markdown","509d6171":"markdown","fd8214b1":"markdown","4214858a":"markdown","945af820":"markdown","4393be3b":"markdown","2ec688f0":"markdown","24b0a9d6":"markdown","e5ddab70":"markdown","ffe99f47":"markdown","ad7df595":"markdown","5536076c":"markdown","3194144a":"markdown","a56b9426":"markdown","069c1638":"markdown","c41c51c6":"markdown","873fbda1":"markdown"},"source":{"148c9f4c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nimport plotly.express as ex\nimport plotly.graph_objs as go\n\nfrom sklearn import tree\nfrom itertools import product\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.decomposition import TruncatedSVD,PCA\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler #To scale the Dataset\nfrom sklearn.model_selection import train_test_split, cross_val_score ,StratifiedKFold, GridSearchCV\nfrom sklearn.metrics import confusion_matrix,roc_curve,accuracy_score, classification_report, roc_auc_score  #to evaluate best model\nfrom sklearn.pipeline import Pipeline #to assemble several steps that can be cross-validated together while setting different parameters.\n\n#Algorithms \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression #to get Logistic Regression Classifier \nfrom sklearn.neighbors import KNeighborsClassifier #to get the KNN classifier \nfrom sklearn.naive_bayes import GaussianNB #to get the Gaussian Naive Bayes Classifier \nfrom sklearn.tree import DecisionTreeClassifier# to get decision tree Classifier\nfrom sklearn.svm import SVC #to get support vector machine Classifier \nfrom sklearn.cluster import DBSCAN # to get DBSCAN Clustering\n#to get k -means Clustering \n\n","48b5a6d2":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndf = pd.read_csv(\"..\/input\/water-potability\/water_potability.csv\")\nprint(df.head())\nprint(df.describe())","8d5e6faa":"def fill_nan(df):\n    for index, column in enumerate(df.columns[:9]):\n        # print(index, column)\n        df[column] = df[column].fillna(df.groupby('Potability')[column].transform('mean'))\n    return df\n        \ndf = fill_nan(df)\ndf.isna().sum()  \ndf.describe()","86f78338":"scaler = MinMaxScaler()\narray = scaler.fit_transform(df.to_numpy())\ndf2 = pd.DataFrame(array, columns = ['ph','Hardness','Solids','Chloramines', 'Sulphate', 'Conductivity', 'Organic_carbon', 'Trihalomethanes','Turbidity', 'Potability'])\ndf2.describe()","5aaab821":"# Correlation matrix for dataset\nplt.figure(figsize=(15,10))\nsns.heatmap(df2.corr(), annot=True, cmap=\"inferno\")","a2787763":"#prepare the data\nX = df2.drop(['Potability'], axis = 1)\ny = df2['Potability']\n \npca = PCA(n_components=5)\npca.fit(X)\nmat = pca.transform(X)\ndf_pca = pd.DataFrame(mat, columns = ['PC1','PC2','PC3','PC4','PC5'])\ndf_pca = pd.concat([df_pca , pd.DataFrame(y)] , axis = 1)\nprint(df_pca.head())\nplt.figure(figsize = (6,6))\nsns.scatterplot(data = df_pca , x = 'PC1',y = 'PC2' , hue = 'Potability' , s = 60)","2d9a6116":"ex.scatter_3d(df_pca,x='PC1',y='PC2',z='PC3',color='Potability',color_discrete_sequence=['salmon','green'],title=r'$\\textit{Data in Reduced Dimension } R^9 \\rightarrow R^3$')","2e06e18f":"evr = pca.explained_variance_ratio_\ntotal_var = evr.sum() * 100\ncumsum_evr = np.cumsum(evr)\n\ntrace1 = {\n    \"name\": \"individual explained variance\", \n    \"type\": \"bar\",\n    'y':evr}\ntrace2 = {\n    \"name\": \"cumulative explained variance\", \n    \"type\": \"scatter\", \n     'y':cumsum_evr}\ndata = [trace1, trace2]\nlayout = {\n    \"xaxis\": {\"title\": \"Principal components\"}, \n    \"yaxis\": {\"title\": \"Explained variance ratio\"},\n  }\nfig = go.Figure(data=data, layout=layout)\nfig.update_layout(title='{:.2f}% of the Original Feature Variance Can Be Explained Using {} Dimensions'.format(np.sum(evr)*100,5))\nfig.show()","db70f6f7":"def PCA_manual(X , num_components):\n     \n    #Step-1\n    X_meaned = X - np.mean(X , axis = 0)\n     \n    #Step-2\n    cov_mat = np.cov(X_meaned , rowvar = False)\n     \n    #Step-3\n    eigen_values , eigen_vectors = np.linalg.eigh(cov_mat)\n     \n    #Step-4\n    sorted_index = np.argsort(eigen_values)[::-1]\n    sorted_eigenvalue = eigen_values[sorted_index]\n    sorted_eigenvectors = eigen_vectors[:,sorted_index]\n     \n    #Step-5\n    eigenvector_subset = sorted_eigenvectors[:,0:num_components]\n     \n    #Step-6\n    X_reduced = np.dot(eigenvector_subset.transpose() , X_meaned.transpose() ).transpose()\n     \n    return X_reduced\n\nmat_reduced = PCA_manual(X , 5)\n \n#Creating a Pandas DataFrame of reduced Dataset\ndf_scratch = pd.DataFrame(mat_reduced , columns = ['PC1','PC2','PC3','PC4','PC5'])\n \n#Concat it with target variable to create a complete Dataset\ndf_scratch = pd.concat([df_scratch , pd.DataFrame(y)] , axis = 1)\n\ndf_scratch.head(5)\nex.scatter_3d(df_scratch,x='PC1',y='PC2',z='PC3',color='Potability',color_discrete_sequence=['salmon','green'],title=r'$\\textit{Data in Reduced Dimension } R^9 \\rightarrow R^3$')","fba62d20":"#Dropping last column (output)\nX = df_pca.drop(['Potability'], axis = 1)\ny = df_pca['Potability']\n\n# Splitting\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=1111, stratify=y) #stratify=y","66ed5caf":"pp = sns.pairplot(data=df2,\n                  y_vars=['Potability'],\n                  x_vars=['ph', 'Hardness', 'Solids','Chloramines', 'Sulphate'])\n\npp = sns.pairplot(data=df2,\n                  y_vars=['Potability'],\n                  x_vars=['Conductivity','Organic_carbon', 'Trihalomethanes', 'Turbidity'])","17eb4e67":"X_lin =  df2.drop(['ph', 'Sulphate', 'Conductivity', 'Organic_carbon', 'Trihalomethanes','Turbidity', 'Potability'], axis = 1)\ny_lin =  np.array(df2['ph']).reshape(-1, 1)\nX_train_LR, X_test_LR, y_train_LR, y_test_LR = train_test_split(X_lin, y_lin, test_size = 0.25)\n\nLinR = LinearRegression()\n  \nLinR.fit(X_train_LR, y_train_LR)\nprint(LinR.score(X_test_LR, y_test_LR))","898acbb3":"LinR_pred = LinR.predict(X_test_LR)\n#print(LinR_pred)\n#LinR_acc = accuracy_score(y_test_LR, LinR_pred)\n\n#print(\"accuracy  \", LinR_acc)\nplt.scatter(X_test_LR, y_test_LR, color ='b')\nplt.plot(X_test_LR, LinR_pred, color ='k')\nplt.show()","a21905b9":"print(classification report)\nprint(classification_report(y_test_LR,LinR_pred))","613641b4":"logit_roc_auc= roc_auc_score(y_lin,LinR.predict(X_lin))\nfpr,tpr,thresholds = roc_curve(y_lin,LinR.predict_proba(X_lin)[:,1])\nplt.figure()\nplt.plot(fpr,tpr,label=\"AUC (area = %0.2f)\" % logit_roc_auc)\nplt.plot([0,1],[0,1],\"r--\")\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title(\"ROC\")\nplt.show()","b3a20b32":"LogR = LogisticRegression(max_iter=120,random_state=0, n_jobs=20)\nLogR.fit(X_train, y_train)\nLogR_pred = LogR.predict(X_test)\nLogR_acc = accuracy_score(y_test, LogR_pred)\nprint(\"accuracy  \", LogR_acc)\nprint(\"classification report\")\nprint(classification_report(y_test,LogR_pred))","3655edb7":"logit_roc_auc= roc_auc_score(y,LogR.predict(X))\nfpr,tpr,thresholds = roc_curve(y,LogR.predict_proba(X)[:,1])\nplt.figure()\nplt.plot(fpr,tpr,label=\"AUC (area = %0.2f)\" % logit_roc_auc)\nplt.plot([0,1],[0,1],\"r--\")\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title(\"ROC\")\nplt.show()","acca6f91":"# confusion Maxtrix\ncm1 = confusion_matrix(y_test, LogR_pred)\nsns.heatmap(cm1\/np.sum(cm1), annot = True, fmt=  '0.2%', cmap = 'Reds')","65ce0820":"k = range(1,20,1)\ntesting_accuracy= []\ntraining_accuracy = []\nscore = 0\n\nfor i in k:\n    knn = KNeighborsClassifier(n_neighbors=i, metric='minkowski', p=2 )  \n    knn.fit(X_train, y_train)  \n    y_pred_train = knn.predict(X_train)\n    training_accuracy.append(accuracy_score(y_train, y_pred_train))\n    \n    y_pred_test = knn.predict(X_test)\n    acc_score = accuracy_score(y_test,y_pred_test)\n    testing_accuracy.append(acc_score)\n    if score < acc_score:\n        score = acc_score\n        best_k = i\n\nprint('Best Accuracy Score for PCA from built -in', score, 'Best K-Score', best_k)","5635142b":"sns.lineplot(k, testing_accuracy)\nsns.scatterplot(k, testing_accuracy)\n\nsns.lineplot(k, training_accuracy)\nsns.scatterplot(k, training_accuracy)\nplt.legend(['testing accuracy', 'training accuracy'])","5be0761a":"# Train the model again for K = 2 to plot ROC curves \ndef model_evaluation(model, metric):\n    skfold = StratifiedKFold(n_splits = 5)\n    model_cv = cross_val_score(model, X_train, y_train, cv = skfold, scoring = metric)\n    return model_cv\n\nknn = KNeighborsClassifier(n_neighbors = 2)\npipe_knn = Pipeline([('scale', MinMaxScaler()), ('knn', knn)])\npipe_knn.fit(X_train, y_train)\npipe_knn_cv = model_evaluation(pipe_knn, 'roc_auc')\nKNN_roc_auc= roc_auc_score(y,pipe_knn.predict(X))\nfpr,tpr,thresholds = roc_curve(y,pipe_knn.predict_proba(X)[:,1])\nplt.figure()\nplt.plot(fpr,tpr,label=\"AUC (area = %0.2f)\" % KNN_roc_auc)\nplt.plot([0,1],[0,1],\"r--\")\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title(\"ROC\")\nplt.show()\n\npipe_knn_pred = pipe_knn.predict(X_test)\npipe_knn_acc = (accuracy_score(y_test, pipe_knn_pred))\nprint(\"test accuracy for KNN is \", pipe_knn_acc)","ca2d6b1a":"# confusion Maxtrix\ncm5 = confusion_matrix(y_test, pipe_knn_pred)\nsns.heatmap(cm5\/np.sum(cm5), annot = True, fmt=  '0.2%', cmap = 'Reds')","a15566dd":"nb=GaussianNB()\nnb_model=nb.fit(X_train,y_train)\nnb_pred=nb_model.predict(X_test)\nnb_acc = accuracy_score(y_test,nb_pred)\nprint(nb_acc)","5a3c43f4":"nb_params={'var_smoothing': np.logspace(0,-9, num=100)}\nnb_cv=GridSearchCV(estimator=nb, \n                 param_grid=nb_params, \n                 cv=10, \n                 verbose=1, \n                 scoring='accuracy') \nnb_cv.fit(X_train,y_train)\nnb_cv.best_params_","bba50b82":"nb=GaussianNB(var_smoothing=0.15199110829529336)\nnb_tuned=nb.fit(X_train,y_train)\nnb_pred=nb_tuned.predict(X_test)\nnb_acc = accuracy_score(y_test,nb_pred)\nprint(\"accuracy\", nb_acc)","62fda23b":"bayes_roc_auc= roc_auc_score(y,nb_tuned.predict(X))\nfpr,tpr,thresholds = roc_curve(y,nb_tuned.predict_proba(X)[:,1])\nplt.figure()\nplt.plot(fpr,tpr,label=\"AUC (area = %0.2f)\" %bayes_roc_auc)\nplt.plot([0,1],[0,1],\"r--\")\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title(\"ROC\")\nplt.show()","49a48899":"# confusion Maxtrix\ncm1 = confusion_matrix(y_test, nb_pred)\nsns.heatmap(cm1\/np.sum(cm1), annot = True, fmt=  '0.2%', cmap = 'Reds')","a09a77c5":"clf_gini = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=0)\nclf_gini.fit(X_train, y_train)\ny_pred_gini = clf_gini.predict(X_test)\n\nprint('Model accuracy score with criterion gini index: {0:0.4f}'. format(accuracy_score(y_test, y_pred_gini)))","86e1bf6a":"y_pred_train_gini = clf_gini.predict(X_train)\n\ny_pred_train_gini\nprint('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train_gini)))","f56fd16f":"# print the scores on training and test set\n\nprint('Training set score: {:.4f}'.format(clf_gini.score(X_train, y_train)))\n\nprint('Test set score: {:.4f}'.format(clf_gini.score(X_test, y_test)))","308b13f9":"plt.figure(figsize=(12,8))\n\n\n\ntree.plot_tree(clf_gini.fit(X_train, y_train)) ","02d4a8d3":"# instantiate the DecisionTreeClassifier model with criterion entropy\n\nclf_en = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0)\n\n\n# fit the model\nclf_en.fit(X_train, y_train)","dc86da12":"y_pred_en = clf_en.predict(X_test)\nprint('Model accuracy score with criterion entropy: {0:0.4f}'. format(accuracy_score(y_test, y_pre","365147e9":"y_pred_train_en = clf_en.predict(X_train)\n\ny_pred_train_en","089d9de6":"print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train_en)))","027eee66":"plt.figure(figsize=(12,8))\ntree.plot_tree(clf_en.fit(X_train, y_train)) ","be185167":"from sklearn.svm import SVC\n\n# Initialize SVM classifier\nsvm_s = SVC(kernel='rbf')\nsvm_b = SVC(kernel='rbf')\n\n# Fit data\nsvm_s = svm_s.fit(X_train_scratch, y_train_scratch)\nsvm_b = svm_b.fit(X_train_built_in, y_train_built_in)\n\ny_pred=svm_s.predict(X_test_scratch)\nacc = accuracy_score(y_test_scratch,y_pred)\nprint('Best Accuracy Score for SVM from scratch', acc)\n\ny_pred=svm_b.predict(X_test_built_in)\nacc = accuracy_score(y_test_built_in,y_pred)\nprint('Best Accuracy Score for SVM from built-in', acc)","b3c77ea1":"db_default = DBSCAN(eps = 0.12, min_samples = 10).fit(X_train)\nlabels = db_default.labels_\nprint(labels)\nno_clusters = len(np.unique(labels) )\nno_noise = np.sum(np.array(labels) == -1, axis=0)\n\nprint('Estimated no. of clusters: %d' % no_clusters)\nprint('Estimated no. of noise points: %d' % no_noise)","71367836":"colours1 = {}\ncolours1[0] = 'r'\ncolours1[1] = 'g'\ncolours1[2] = 'b'\ncolours1[3] = 'c'\ncolours1[4] = 'y'\ncolours1[5] = 'm'\ncolours1[-1] = 'k'\n  \ncvec = [colours1[label] for label in labels]\ncolors = ['r', 'g', 'b', 'c', 'y', 'm', 'k' ]\n\nplt.figure(figsize =(9, 9))\n  \nr = plt.scatter(\n        X_train['PC1'], X_train['PC2'], marker ='o', color = colors[0])\ng = plt.scatter(\n        X_train['PC1'], X_train['PC2'], marker ='o', color = colors[1])\nb = plt.scatter(\n        X_train['PC1'], X_train['PC2'], marker ='o', color = colors[2])\nc = plt.scatter(\n        X_train['PC1'], X_train['PC2'], marker ='o', color = colors[3])\n# y = plt.scatter(\n#         X_train['PC1'], X_train['PC2'], marker ='o', color = colors[4])\n# m = plt.scatter(\n#         X_train['PC1'], X_train['PC2'], marker ='o', color = colors[5])\nk = plt.scatter(\n        X_train['PC1'], X_train['PC2'], marker ='o', color = colors[6])\n  \nplt.figure(figsize =(9, 9))\nplt.scatter(X_train['PC1'], X_train['PC2'], c = cvec)\nplt.legend((r, g, b, c, y, m, k),\n           ('Label 0', 'Label 1', 'Label 2', 'Label 3', 'Label -1'),\n           scatterpoints = 1,\n           loc ='upper left',\n           ncol = 3,\n           fontsize = 8)\nplt.show()","1867b376":"from sklearn.datasets import make_blobs\n# Configuration options\nnum_samples_total = 1500\ncluster_centers = [(5,3), (7,7) , (3,7)]\nnum_classes = len(cluster_centers)\nepsilon = 1.0\nmin_samples = 13\n# Generate data\nX, y = make_blobs(n_samples = num_samples_total, centers = cluster_centers, n_features = num_classes, center_box=(0, 1), cluster_std = 0.5)\n# Compute DBSCAN\ndb = DBSCAN(eps=epsilon, min_samples=min_samples).fit(X)\nlabels = db.labels_\n\nno_clusters = len(np.unique(labels) )\nno_noise = np.sum(np.array(labels) == -1, axis=0)\n\nprint('Estimated no. of clusters: %d' % no_clusters)\nprint('Estimated no. of noise points: %d' % no_noise)","3540a647":"colours1 = {}\ncolours1[0] = 'r'\ncolours1[1] = 'g'\ncolours1[2] = 'b'\ncolours1[3] = 'c'\ncolours1[4] = 'y'\ncolours1[5] = 'm'\ncolours1[-1] = 'k'\n  \ncvec = [colours1[label] for label in labels]\ncolors = ['r', 'g', 'b', 'c', 'y', 'm', 'k' ]\n\nplt.figure(figsize =(9, 9))\n  \nr = plt.scatter(X[:,0], X[:,1], marker ='o', color = colors[0])\ng = plt.scatter(X[:,0], X[:,1], marker ='o', color = colors[1])\nb = plt.scatter(X[:,0], X[:,1], marker ='o', color = colors[2])\nc = plt.scatter(X[:,0], X[:,1], marker ='o', color = colors[3])\n  \nplt.figure(figsize =(9, 9))\nplt.scatter(X[:,0], X[:,1], c = cvec)\nplt.legend((r, g, b, c),\n           ('Label 0', 'Label 1', 'Label 2', 'Label -1'),\n           scatterpoints = 1,\n           loc ='upper left',\n           ncol = 3,\n           fontsize = 8)\nplt.show()","eab92091":"Models = [\"Linear Regression\" ,\"Logistic Regression\", \"KNN\", \"Naive Bayes\"]\nAccuracies = [LinR_acc, LogR_acc, pipe_knn_acc, nb_acc]\n\nresults = []\ni = 0\nfor acc in Accuracies:\n    Model = Models[i]\n    result=pd.DataFrame([[Model,acc*100]],columns=[\"Models\",\"Accuracy\"])\n    results=results.append(result)\n    i = i + 1\nprint(results)\nsns.barplot(x=\"Accuracy\",y=\"Models\",data=results,color=\"b\")\n","9b068bfa":"## INPUTS for splitting the dataset into test and train\n### Test_size\nTest size = 0.3 i.e, 70% of the data is used for training and 30% for testing\n\n### random_state\ncontrols the shuffling applied to the data before applying the split. Pass an int for reproducible output across multiple function calls.\n\n### Stratify \nensure that both the train and test sets have the proportion of examples in each class that is present in the provided \u201cy\u201d array","dbed9995":"# <center> Naive Bayes<\/center> ","9228f862":"# <center> Water Portability Prediction<\/center> ","b38e69ba":"# <center> KNN<\/center> ","a4cd1886":"# <center>DBSCAN<\/center> ","6e6046f5":"## The dataset has 9 attributes \n### pH \n### Hardness\n### Solids \n### Chloramines \n### Sulphates \n### Conductivity \n### Organic Carbon \n### Trihalomethanes \n### Turbidity \n\n## The label for the dataset is Potability which has values 0 for not suitable for consumption and 1 for suitable for consumption","3d670392":"# ML LAB PROJECT\n# Meghana Rao \n# BL.EN.U4CSE18071","38a0b410":"# <center>Decision Tree <\/center> ","06f5e9ab":"## Import data","7c298a85":"Using five components (out of initially 9), we can see that we can only preserve 60 percent of the original variance; we can learn from this fact that our features are indeed uncorrelated between them and there is no linear combination that can tell us a better story regarding the target label after looking at the different permutations of principal components","ebd53d82":"# <center> SVM<\/center> ","0ba70a10":"# <center> PRE-PROCESSING <\/center> ","1fab1770":"### This part of the code is optional (checking if PCA performed manually gives us the same results) ","509d6171":"# <center>K-means Clustering<\/center> ","fd8214b1":"As expected DBSCAN doesnt work well with this data! ","4214858a":"# Lets generate our own data and try again!","945af820":"## Preprocessing seperately (without PCA) ","4393be3b":"# <center> Logistic Regression<\/center> ","2ec688f0":"## Performing PCA on the dataset","24b0a9d6":"## Scaling the dataset and converting back into a dataframe","e5ddab70":"## Understanding the PCA components ","ffe99f47":"## Import Libraries","ad7df595":"## Filling missing values using mean values","5536076c":"# <center> Linear Regression <\/center> ","3194144a":"# Algorithms \n## 1. Supervised \n### * Linear Regression\n### * Logistic Regression\n### * K-Nearest Neighbors\n### * Naive Bayes\n### * Decision Tree\n### * Support Vector Machines\n## 2. Unsupervised \n### * DBSCAN \n### * K means Clustering ","a56b9426":"Here, the training-set accuracy score is 0.7239 while the test-set accuracy to be 0.7080. These two values are quite comparable. So, there is no sign of overfitting.","069c1638":"Therefore, the variables are barely linearly seperable, therefore models like SVM that depend of linear seperability will perform poorly!","c41c51c6":"### NOTE: Since the label values is discrete, we are performing linear regression on 2 values only! \n## pH is corelated with Hardness and negatively corelated with Solids, Chloramines. \n## Let us try to find pH with Hardness, Solids and Chloramines","873fbda1":"# <center>CONCLUSION <\/center> "}}