{"cell_type":{"029ded5b":"code","4796e030":"code","8f2f6350":"code","4050a5aa":"code","4feaf65c":"code","d9812a2e":"code","04667d9b":"code","e8c6f41a":"code","d48dbe87":"code","55ed6dd3":"code","530ab148":"code","cd7a40ad":"code","9fc2a491":"code","b423dd67":"code","c7b19392":"code","38d6113d":"code","f914fc74":"code","38e689d7":"code","52cf0ca7":"code","f7e2e484":"code","0dfc6f04":"code","67048611":"code","9e9a648e":"code","0e8798ac":"code","fcc68bbe":"code","46793069":"code","06257a9e":"code","40cafed2":"code","e12b3410":"code","0deb8585":"code","0713202b":"code","789516c7":"code","55d48d39":"code","9235bd2e":"code","c8d71e16":"code","60e6d551":"code","b514322f":"code","79a44a6a":"code","1ce6d003":"code","2296b893":"code","79b7378f":"code","f1ef4b76":"code","99f7364b":"code","13f18072":"code","e24881ce":"code","95ab3e52":"code","5c344a37":"code","25332ddc":"code","5a45a511":"code","b96cb816":"code","3022697a":"code","c5a71504":"code","345d4eb5":"code","64ebb699":"code","d0336260":"code","db428884":"code","7032961a":"code","6e35fa1f":"code","8282c701":"code","c16e15de":"code","a9598368":"code","eff3428f":"code","75426318":"code","60ee862c":"code","f69732e8":"code","10f21d73":"code","2b8df66f":"code","53036f16":"code","492dbae7":"code","fb9e7469":"code","0dbc6aae":"code","dd085c8e":"code","05813b13":"code","615fc119":"code","4a59ef8d":"code","f823ac7b":"code","746757fc":"code","b9dff2c9":"code","36fc3707":"code","c0eca4ee":"code","d57845d4":"code","5f3603e9":"code","de38969e":"markdown","f9deb29a":"markdown","3afde615":"markdown","60cbb0d1":"markdown","8ec6ae1d":"markdown","4a49ddff":"markdown","78195d54":"markdown","c1580a0e":"markdown","c5e44f84":"markdown","2ca314be":"markdown","afd0da73":"markdown","004f6fd8":"markdown","08f6b449":"markdown","98110b16":"markdown","2a88f234":"markdown","1696c20b":"markdown","6271d5a1":"markdown","c6be2d51":"markdown","110524fa":"markdown","b72aff1f":"markdown","6475d350":"markdown","9ef0df1a":"markdown","d086b8a6":"markdown","4e65c781":"markdown","b6e485b5":"markdown","6446f553":"markdown","613e0666":"markdown","f40e661d":"markdown"},"source":{"029ded5b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_style('whitegrid')\nplt.style.use('seaborn-deep')\nplt.style.use('fivethirtyeight')\nplt.rcParams['font.family'] = 'sans-serif'\nplt.rcParams['font.serif'] = 'Ubuntu'\nplt.rcParams['font.monospace'] = 'Ubuntu Mono'\nplt.rcParams['font.size'] = 10\nplt.rcParams['axes.labelsize'] = 12\nplt.rcParams['axes.titlesize'] = 12\nplt.rcParams['xtick.labelsize'] = 8\nplt.rcParams['ytick.labelsize'] = 8\nplt.rcParams['legend.fontsize'] = 12\nplt.rcParams['figure.titlesize'] = 14\nplt.rcParams['figure.figsize'] = (12, 8)\n\npd.options.mode.chained_assignment = None\npd.options.display.float_format = '{:.2f}'.format\npd.set_option('display.max_columns', 200)\npd.set_option('display.width', 400)\nimport warnings\nwarnings.filterwarnings('ignore')\nimport sklearn.base as skb\nimport sklearn.metrics as skm\nimport sklearn.model_selection as skms\nimport sklearn.preprocessing as skp\nimport sklearn.utils as sku\nimport sklearn.linear_model as sklm\nimport sklearn.neighbors as skn\nimport sklearn.ensemble as ske\nimport catboost as cb\nimport scipy.stats as sstats\nimport random\nseed = 12\nnp.random.seed(seed)\n\nfrom datetime import date","4796e030":"!pip install pandas-profiling\nimport pandas_profiling as pp","8f2f6350":"# important funtions\ndef datasetShape(df):\n    rows, cols = df.shape\n    print(\"The dataframe has\",rows,\"rows and\",cols,\"columns.\")\n    \n# select numerical and categorical features\ndef divideFeatures(df):\n    numerical_features = df.select_dtypes(include=[np.number])\n    categorical_features = df.select_dtypes(include=[np.object])\n    return numerical_features, categorical_features","4050a5aa":"base = '\/kaggle\/input\/jobathon-analytics-vidhya\/'\ndata_file = base + \"train.csv\"\ndf = pd.read_csv(data_file)\ndf.head()","4feaf65c":"data_file = base + \"test.csv\"\ndf_test = pd.read_csv(data_file)\ndf_test.head()","d9812a2e":"# set target feature\ntargetFeature='Response'","04667d9b":"# check dataset shape\ndatasetShape(df)","e8c6f41a":"# remove ID from train data\ndf.drop(['ID'], inplace=True, axis=1)","d48dbe87":"# check for duplicates\nprint(df.shape)\ndf.drop_duplicates(inplace=True)\nprint(df.shape)","55ed6dd3":"df.info()","530ab148":"df_test.info()","cd7a40ad":"df.describe()","9fc2a491":"cont_features, cat_features = divideFeatures(df)\ncat_features.head()","b423dd67":"# check target feature distribution\ndf[targetFeature].hist()\nplt.show()","c7b19392":"# boxplots of numerical features for outlier detection\n\nfig = plt.figure(figsize=(16,16))\nfor i in range(len(cont_features.columns)):\n    fig.add_subplot(3, 3, i+1)\n    sns.boxplot(y=cont_features.iloc[:,i])\nplt.tight_layout()\nplt.show()","38d6113d":"# distplots for categorical data\n\nfig = plt.figure(figsize=(16,20))\nfor i in range(len(cat_features.columns)):\n    fig.add_subplot(3, 3, i+1)\n    cat_features.iloc[:,i].hist()\n    plt.xlabel(cat_features.columns[i])\nplt.tight_layout()\nplt.show()","f914fc74":"# plot missing values\n\ndef calc_missing(df):\n    missing = df.isna().sum().sort_values(ascending=False)\n    missing = missing[missing != 0]\n    missing_perc = missing\/df.shape[0]*100\n    return missing, missing_perc\n\nif df.isna().any().sum()>0:\n    missing, missing_perc = calc_missing(df)\n    missing.plot(kind='bar',figsize=(16,6))\n    plt.title('Missing Values')\n    plt.show()\nelse:\n    print(\"No missing values\")","38e689d7":"sns.pairplot(df)\nplt.show()","52cf0ca7":"# correlation heatmap for all features\ncorr = df.corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr, mask = mask, annot=True)\nplt.show()","f7e2e484":"profile = pp.ProfileReport(df, title='Pandas Profiling Report', explorative=True)\nprofile.to_file(\"profile.html\")","0dfc6f04":"profile.to_notebook_iframe()","67048611":"skewed_features = cont_features.apply(lambda x: x.skew()).sort_values(ascending=False)\nskewed_features","9e9a648e":"# plot missing values\n\ndef calc_missing(df):\n    missing = df.isna().sum().sort_values(ascending=False)\n    missing = missing[missing != 0]\n    missing_perc = missing\/df.shape[0]*100\n    return missing, missing_perc\n\nif df.isna().any().sum()>0:\n    missing, missing_perc = calc_missing(df)\n    missing.plot(kind='bar',figsize=(14,5))\n    plt.title('Missing Values')\n    plt.show()\nelse:\n    print(\"No Missing Values\")","0e8798ac":"# remove all columns having no values\ndf.dropna(axis=1, how=\"all\", inplace=True)\ndf.dropna(axis=0, how=\"all\", inplace=True)\ndatasetShape(df)","fcc68bbe":"# def fillNan(df, col, value):\n#     df[col].fillna(value, inplace=True)","46793069":"# # setting missing values to most occurring values\n# fillNan(df, 'Health Indicator', df['Health Indicator'].mode()[0])\n# fillNan(df_test, 'Health Indicator', df['Health Indicator'].mode()[0])\n# df['Health Indicator'].isna().any()","06257a9e":"# # setting missing values to most occurring values\n# # try changing with ML algo for missing\n# fillNan(df, 'Holding_Policy_Duration', df['Holding_Policy_Duration'].mode()[0])\n# fillNan(df_test, 'Holding_Policy_Duration', df['Holding_Policy_Duration'].mode()[0])\n# df['Holding_Policy_Duration'].isna().any()","40cafed2":"# # setting missing values to most occurring values\n# # try changing with ML algo for missing\n# fillNan(df, 'Holding_Policy_Type', df['Holding_Policy_Type'].mode()[0])\n# fillNan(df_test, 'Holding_Policy_Type', df['Holding_Policy_Type'].mode()[0])\n# df['Holding_Policy_Type'].isna().any()","e12b3410":"# # convert city code to int after removing C from it\n# df['City_Code'] = pd.to_numeric(df['City_Code'].map(lambda x:x[1:]))\n# df_test['City_Code'] = pd.to_numeric(df_test['City_Code'].map(lambda x:x[1:]))\n# df['City_Code'].head()","0deb8585":"cont_features, cat_features = divideFeatures(df)\ncont_features.columns.tolist()","0713202b":"# get all not null records for imputing\nX_impute = df[df['Health Indicator'].isna()==False]\ny_impute = X_impute.pop('Health Indicator')\n\n# remove categorical cols and targetFeature from X_impute\nX_impute = X_impute[cont_features.columns.tolist()]\nX_impute.drop(['Holding_Policy_Type', targetFeature], inplace=True, axis=1)\n\n# impute with CatBoostClassifier\nimputer_model = cb.CatBoostClassifier(random_state=seed, verbose=0)\nimputer_model.fit(X_impute, y_impute)","789516c7":"# predict values for train section\nX_test_impute = df[df['Health Indicator'].isna()==True]\nX_test_impute = X_test_impute[X_impute.columns.tolist()]\ny_test_impute = imputer_model.predict(X_test_impute)\n\n# setting value after prediction in df\nfor i,x in enumerate(X_test_impute.index):\n    df.loc[x,'Health Indicator'] = y_test_impute[i]\n    \n# predict values for test section\nX_test_impute = df_test[df_test['Health Indicator'].isna()==True]\nX_test_impute = X_test_impute[X_impute.columns.tolist()]\ny_test_impute = imputer_model.predict(X_test_impute)\n\n# setting value after prediction in df\nfor i,x in enumerate(X_test_impute.index):\n    df_test.loc[x,'Health Indicator'] = y_test_impute[i]","55d48d39":"# # convert Health Indicator to int after removing X from it\n# df['Health Indicator'] = pd.to_numeric(df['Health Indicator'].map(lambda x:x[1:]))\n# df_test['Health Indicator'] = pd.to_numeric(df_test['Health Indicator'].map(lambda x:x[1:]))\n# df['Health Indicator'].head()","9235bd2e":"cont_features, cat_features = divideFeatures(df)\ncont_features.columns.tolist()","c8d71e16":"# get all not null records for imputing\nX_impute = df[df['Holding_Policy_Duration'].isna()==False]\ny_impute = X_impute.pop('Holding_Policy_Duration')\n\n# remove categorical cols and targetFeature from X_impute\nX_impute = X_impute[cont_features.columns.tolist()]\nX_impute.drop(['Holding_Policy_Type', targetFeature], inplace=True, axis=1)\n\n# impute with RandomForestClassifier\nimputer_model = cb.CatBoostClassifier(random_state=seed, verbose=0)\nimputer_model.fit(X_impute, y_impute)","60e6d551":"# predict values for train section\nX_test_impute = df[df['Holding_Policy_Duration'].isna()==True]\nX_test_impute = X_test_impute[X_impute.columns.tolist()]\ny_test_impute = imputer_model.predict(X_test_impute)\n\n# setting value after prediction in df\nfor i,x in enumerate(X_test_impute.index):\n    df.loc[x,'Holding_Policy_Duration'] = y_test_impute[i]\n    \n# predict values for test section\nX_test_impute = df_test[df_test['Holding_Policy_Duration'].isna()==True]\nX_test_impute = X_test_impute[X_impute.columns.tolist()]\ny_test_impute = imputer_model.predict(X_test_impute)\n\n# setting value after prediction in df\nfor i,x in enumerate(X_test_impute.index):\n    df_test.loc[x,'Holding_Policy_Duration'] = y_test_impute[i]","b514322f":"# get all not null records for imputing\nX_impute = df[df['Holding_Policy_Type'].isna()==False]\ny_impute = X_impute.pop('Holding_Policy_Type')\n\n# remove categorical cols and targetFeature from X_impute\ncols_impute = cont_features.columns.tolist()\ncols_impute.remove('Holding_Policy_Type')\nX_impute = X_impute[cols_impute]\nX_impute.drop([targetFeature], inplace=True, axis=1)\n\n# impute with RandomForestClassifier\nimputer_model = cb.CatBoostClassifier(random_state=seed, verbose=0)\nimputer_model.fit(X_impute, y_impute)","79a44a6a":"# predict values for train section\nX_test_impute = df[df['Holding_Policy_Type'].isna()==True]\nX_test_impute = X_test_impute[X_impute.columns.tolist()]\ny_test_impute = imputer_model.predict(X_test_impute)\n\n# setting value after prediction in df\nfor i,x in enumerate(X_test_impute.index):\n    df.loc[x,'Holding_Policy_Type'] = y_test_impute[i]\n    \n# predict values for test section\nX_test_impute = df_test[df_test['Holding_Policy_Type'].isna()==True]\nX_test_impute = X_test_impute[X_impute.columns.tolist()]\ny_test_impute = imputer_model.predict(X_test_impute)\n\n# setting value after prediction in df\nfor i,x in enumerate(X_test_impute.index):\n    df_test.loc[x,'Holding_Policy_Type'] = y_test_impute[i]","1ce6d003":"print(\"Train Missing:\",df.isna().any().sum())\nprint(\"Test Missing:\",df_test.isna().any().sum())","2296b893":"# feature for age difference between Upper_Age and Lower_Age\ndf['age_diff'] = abs(df['Upper_Age'] - df['Lower_Age'])\ndf_test['age_diff'] = abs(df_test['Upper_Age'] - df_test['Lower_Age'])\ndf_test.head()","79b7378f":"# drop Lower_Age column as it is highly correlated with Upper_age and we also have its info in age_diff\ndf.drop('Lower_Age', axis=1, inplace=True)\ndf_test.drop('Lower_Age', axis=1, inplace=True)\ndf_test.head()","f1ef4b76":"df['Holding_Policy_Duration'] = pd.to_numeric(df['Holding_Policy_Duration'].map(lambda x:'15' if x == '14+' else x))\ndf_test['Holding_Policy_Duration'] = pd.to_numeric(df_test['Holding_Policy_Duration'].map(lambda x:'15' if x == '14+' else x))\ndf_test['Holding_Policy_Duration'].head()","99f7364b":"cont_features, cat_features = divideFeatures(df)\ncat_features","13f18072":"# label encoding on categorical features\ndef mapFeature(data, f, data_test=None):\n    feat = data[f].unique()\n    feat_idx = [x for x in range(len(feat))]\n\n    data[f].replace(feat, feat_idx, inplace=True)\n    if data_test is not None:\n        data_test[f].replace(feat, feat_idx, inplace=True)","e24881ce":"for col in cat_features.columns:\n    mapFeature(df, col, df_test)\ndf_test.head()","95ab3e52":"# extract numerical and categorical for dummy and scaling later\ncustom_feat = ['City_Code', 'Health Indicator']\n# custom_feat = ['Health Indicator']\nfor feat in cat_features.columns:\n    if len(df[feat].unique()) > 2 and feat in custom_feat:\n        dummyVars = pd.get_dummies(df[feat], drop_first=True, prefix=feat+\"_\")\n        df = pd.concat([df, dummyVars], axis=1)\n        df.drop(feat, axis=1, inplace=True)\ndatasetShape(df)\n\ndf.head()","5c344a37":"# extract numerical and categorical for dummy and scaling later\ncustom_feat = ['City_Code', 'Health Indicator']\n# custom_feat = ['Health Indicator']\nfor feat in cat_features.columns:\n    if len(df_test[feat].unique()) > 2 and feat in custom_feat:\n        dummyVars = pd.get_dummies(df_test[feat], drop_first=True, prefix=feat+\"_\")\n        df_test = pd.concat([df_test, dummyVars], axis=1)\n        df_test.drop(feat, axis=1, inplace=True)\ndatasetShape(df_test)\n\ndf_test.head()","25332ddc":"# # dropping holding policy features\n# df.drop(['Holding_Policy_Duration', 'Holding_Policy_Type'], inplace=True, axis=1)\n# df_test.drop(['Holding_Policy_Duration', 'Holding_Policy_Type'], inplace=True, axis=1)","5a45a511":"# helper functions\n\ndef log1p(vec):\n    return np.log1p(abs(vec))\n\ndef expm1(x):\n    return np.expm1(x)\n\ndef clipExp(vec):\n    return np.clip(expm1(vec), 0, None)\n\ndef printScore(y_train, y_train_pred):\n    print(skm.roc_auc_score(y_train, y_train_pred))","b96cb816":"# shuffle samples\ndf_shuffle = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n\ndf_y = df_shuffle.pop(targetFeature)\ndf_X = df_shuffle\n\n# split into train dev and test\nX_train, X_test, y_train, y_test = skms.train_test_split(df_X, df_y, train_size=0.8, random_state=seed)\nprint(f\"Train set has {X_train.shape[0]} records out of {len(df_shuffle)} which is {round(X_train.shape[0]\/len(df_shuffle)*100)}%\")\nprint(f\"Test set has {X_test.shape[0]} records out of {len(df_shuffle)} which is {round(X_test.shape[0]\/len(df_shuffle)*100)}%\")","3022697a":"# scaler = skp.RobustScaler()\n# scaler = skp.MinMaxScaler()\nscaler = skp.StandardScaler()\n\n# apply scaling to all numerical variables except dummy variables as they are already between 0 and 1\nX_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n\n# scale test data with transform()\nX_test = pd.DataFrame(scaler.transform(X_test), columns=X_train.columns)\n\n# view sample data\nX_train.describe()","c5a71504":"# X_train_small = X_train.sample(frac=0.3)\n# y_train_small = y_train.iloc[X_train_small.index.tolist()]\n# X_train_small.shape","345d4eb5":"class_weights = sku.class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\nclass_weights = dict(enumerate(class_weights))\nclass_weights","64ebb699":"sample_weights = sku.class_weight.compute_sample_weight('balanced', y_train)\nsample_weights","d0336260":"knn = skn.KNeighborsClassifier(n_neighbors = 5, n_jobs=-1)\nknn.fit(X_train, y_train)\n\n# predict\ny_train_pred = knn.predict(X_train)\ny_test_pred = knn.predict(X_test)\nprint(skm.accuracy_score(y_train, y_train_pred))\nprint(skm.accuracy_score(y_test, y_test_pred))\nprintScore(y_train, y_train_pred)\nprintScore(y_test, y_test_pred)","db428884":"log_model = sklm.LogisticRegression()\nlog_model.fit(X_train, y_train, sample_weight=sample_weights)\n\n# predict\ny_train_pred = log_model.predict(X_train)\ny_test_pred = log_model.predict(X_test)\nprint(skm.accuracy_score(y_train, y_train_pred))\nprint(skm.accuracy_score(y_test, y_test_pred))\nprintScore(y_train, y_train_pred)\nprintScore(y_test, y_test_pred)","7032961a":"enet_model = sklm.ElasticNetCV(l1_ratio = [0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1],\n                    alphas = [1, 0.1, 0.01, 0.001, 0.0005], cv=10)\nenet_model.fit(X_train, y_train)\n\n# predict\ny_train_pred = enet_model.predict(X_train)\ny_test_pred = enet_model.predict(X_test)\nprintScore(y_train, y_train_pred)\nprintScore(y_test, y_test_pred)","6e35fa1f":"ridge_model = sklm.RidgeCV(scoring = \"neg_mean_squared_error\", \n                    alphas = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1, 1.0, 10], cv=5\n                   )\nridge_model.fit(X_train, y_train)\n\n# predict\ny_train_pred = ridge_model.predict(X_train)\ny_test_pred = ridge_model.predict(X_test)\nprintScore(y_train, y_train_pred)\nprintScore(y_test, y_test_pred)","8282c701":"import catboost as cb\n\ncat_model = cb.CatBoostClassifier(loss_function='Logloss', verbose=0, eval_metric='AUC', class_weights=class_weights,\n                           use_best_model=True, iterations=500)\ncat_model.fit(X_train, y_train, eval_set=(X_test, y_test))\nprint(cat_model.best_score_)\n\ny_train_pred = cat_model.predict(X_train)\ny_test_pred = cat_model.predict(X_test)\nprint(skm.accuracy_score(y_train, y_train_pred))\nprint(skm.accuracy_score(y_test, y_test_pred))\nprintScore(y_train, y_train_pred)\nprintScore(y_test, y_test_pred)","c16e15de":"# # Grid used\n# param_test1 = {\n#     'n_estimators': [10, 50, 100, 500],\n#     'max_depth': np.arange(2, 12, 2)\n# }\n# gb_cv1 = skms.GridSearchCV(estimator = ske.GradientBoostingClassifier(loss='deviance', random_state=seed), \n#                              param_grid = param_test1, n_jobs=-1, \n#                              cv=5, verbose=1)\n# # gb_cv1.fit(X_train_small, y_train_small)\n# gb_cv1.fit(X_train, y_train, sample_weight=sample_weights)\n# print(gb_cv1.best_params_, gb_cv1.best_score_)\n# # n_estimators = 1000\n# # max_depth = 10","a9598368":"# # Grid used\n# param_test2 = {\n#     'min_samples_split': np.arange(2, 12, 3),\n#     'min_samples_leaf': np.arange(1, 10, 3)\n# }\n# gb_cv2 = skms.GridSearchCV(estimator = ske.GradientBoostingClassifier(loss='deviance', random_state=seed,\n#                                                                  n_estimators=50,\n#                                                                  max_depth=7), \n#                              param_grid = param_test2, n_jobs=-1, \n#                              cv=5, verbose=1)\n# gb_cv2.fit(X_train, y_train)\n# print(gb_cv2.best_params_, gb_cv2.best_score_)\n# print(gb_cv2.best_estimator_)\n# # min_samples_split = 8\n# # min_samples_leaf = 1","eff3428f":"gb_model = ske.GradientBoostingClassifier(loss='deviance', random_state=seed, verbose=0,\n                                    n_estimators=50, max_depth=7,\n                                    min_samples_leaf=1, min_samples_split=8)\ngb_model.fit(X_train, y_train, sample_weight=sample_weights)\n\n# predict\ny_train_pred = gb_model.predict(X_train)\ny_test_pred = gb_model.predict(X_test)\nprint(skm.accuracy_score(y_train, y_train_pred))\nprint(skm.accuracy_score(y_test, y_test_pred))\nprintScore(y_train, y_train_pred)\nprintScore(y_test, y_test_pred)","75426318":"# # Grid used\n# param_test1 = {\n#     'n_estimators': [10, 50, 100, 500, 1000],\n#     'max_depth': np.arange(2, 12, 2)\n# }\n# extra_cv1 = skms.GridSearchCV(estimator = ske.ExtraTreesClassifier(criterion='gini', random_state=seed), \n#                              param_grid = param_test1, scoring='neg_mean_squared_error', n_jobs=-1, \n#                              cv=5, verbose=1)\n# # extra_cv1.fit(X_train_small, y_train_small)\n# extra_cv1.fit(X_train, y_train)\n# print(extra_cv1.best_params_, extra_cv1.best_score_)\n# # n_estimators = 200\n# # max_depth = 10","60ee862c":"# # Grid used\n# param_test2 = {\n#     'min_samples_split': np.arange(5, 18, 3),\n#     'min_samples_leaf': np.arange(1, 10, 2)\n# }\n# extra_cv2 = skms.GridSearchCV(estimator = ske.ExtraTreesClassifier(criterion='gini', random_state=seed,\n#                                                                  n_estimators=200,\n#                                                                  max_depth=10), \n#                               param_grid = param_test2, scoring='neg_mean_squared_error', n_jobs=-1, \n#                               cv=5, verbose=1)\n# extra_cv2.fit(X_train, y_train)\n# print(extra_cv2.best_params_, extra_cv2.best_score_)\n# print(extra_cv2.best_estimator_)\n# # min_samples_split = 5\n# # min_samples_leaf = 1","f69732e8":"extra_model = ske.ExtraTreesClassifier(criterion='gini', random_state=1, verbose=0, n_jobs=-1,\n                              n_estimators=200,max_depth=10,\n                              min_samples_split = 5, min_samples_leaf = 1)\nextra_model.fit(X_train, y_train, sample_weight=sample_weights)\n\n# predict\ny_train_pred = extra_model.predict(X_train)\ny_test_pred = extra_model.predict(X_test)\nprint(skm.accuracy_score(y_train, y_train_pred))\nprint(skm.accuracy_score(y_test, y_test_pred))\nprintScore(y_train, y_train_pred)\nprintScore(y_test, y_test_pred)","10f21d73":"ada_model = ske.AdaBoostClassifier(random_state=1)\nada_model.fit(X_train, y_train, sample_weight=sample_weights)\n\n# predict\ny_train_pred = ada_model.predict(X_train)\ny_test_pred = ada_model.predict(X_test)\nprint(skm.accuracy_score(y_train, y_train_pred))\nprint(skm.accuracy_score(y_test, y_test_pred))\nprintScore(y_train, y_train_pred)\nprintScore(y_test, y_test_pred)","2b8df66f":"rf_model = ske.RandomForestClassifier(verbose=0, random_state=1, n_jobs=-1, class_weight='balanced_subsample',\n                                 n_estimators=200,max_depth=10, \n                                 min_samples_split = 7, min_samples_leaf = 1\n                                )\nrf_model.fit(X_train, y_train)\n\n# predict\ny_train_pred = rf_model.predict(X_train)\ny_test_pred = rf_model.predict(X_test)\nprint(skm.accuracy_score(y_train, y_train_pred))\nprint(skm.accuracy_score(y_test, y_test_pred))\nprintScore(y_train, y_train_pred)\nprintScore(y_test, y_test_pred)","53036f16":"import xgboost as xg","492dbae7":"# # Grid used\n# param_test1 = {\n#     'max_depth': np.arange(5, 12, 2),\n#     'learning_rate': np.arange(0.04, 0.07, 0.01)\n# }\n# xgb_cv1 = skms.GridSearchCV(estimator = xg.XGBClassifier(n_estimators=100, objective='reg:squarederror', nthread=4, seed=seed), \n#                              param_grid = param_test1, scoring='neg_mean_squared_error', n_jobs=4, \n#                              iid=False, cv=5, verbose=1)\n# xgb_cv1.fit(X_train_small, y_train_small)\n# print(xgb_cv1.best_params_, xgb_cv1.best_score_)\n# # max_depth = 10\n# # learning_rate = 0.04","fb9e7469":"# param_test2 = {\n#  'subsample': np.arange(0.5, 1, 0.1),\n#  'min_child_weight': range(1, 6, 1)\n# }\n# xgb_cv2 = skms.GridSearchCV(estimator = xg.XGBClassifier(n_estimators=500, max_depth = 10, \n#                                                      objective= 'reg:squarederror', nthread=4, seed=seed), \n#                             param_grid = param_test2, scoring='neg_mean_squared_error', n_jobs=4,\n#                             cv=5, verbose=1)\n# xgb_cv2.fit(X_train_small, y_train_small)\n# print(xgb_cv2.best_params_, xgb_cv2.best_score_)\n# print(xgb_cv2.best_estimator_)\n# # subsample = 0.5\n# # min_child_weight = 2","0dbc6aae":"# working without scaling\nxgb_model = xg.XGBClassifier(objective ='binary:logistic', random_state=seed, verbose=0,\n                      n_estimators=500, max_depth = 10)\nxgb_model.fit(X_train, y_train)\n\n# predict\ny_train_pred = xgb_model.predict(X_train)\ny_test_pred = xgb_model.predict(X_test)\nprint(skm.accuracy_score(y_train, y_train_pred))\nprint(skm.accuracy_score(y_test, y_test_pred))\nprintScore(y_train, y_train_pred)\nprintScore(y_test, y_test_pred)","dd085c8e":"import lightgbm as lgb\nlgb_model = lgb.LGBMClassifier(objective='binary', class_weight=class_weights, random_state=1, n_jobs=-1,\n                         n_estimators=50)\nlgb_model.fit(X_train, y_train)\n\n# predict\ny_train_pred = lgb_model.predict(X_train)\ny_test_pred = lgb_model.predict(X_test)\nprint(skm.accuracy_score(y_train, y_train_pred))\nprint(skm.accuracy_score(y_test, y_test_pred))\nprintScore(y_train, y_train_pred)\nprintScore(y_test, y_test_pred)","05813b13":"import tensorflow as tf\nimport tensorflow_addons as tfa\nprint(\"TF version:-\", tf.__version__)\nimport keras as k\ntf.random.set_seed(seed)","615fc119":"THRESHOLD = .999\nbestModelPath = '.\/best_model.hdf5'\n\nclass myCallback(k.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if(logs.get('val_accuracy') > THRESHOLD):\n            print(\"\\n\\nStopping training as we have reached our goal.\")   \n            self.model.stop_training = True\n\nmycb = myCallback()\ncheckpoint = k.callbacks.ModelCheckpoint(filepath=bestModelPath, monitor='val_loss', verbose=1, save_best_only=True)\n\ncallbacks_list = [mycb,\n                  checkpoint\n                 ]\n            \ndef plotHistory(history):\n    print(\"Min. Validation ACC Score\",min(history.history[\"val_accuracy\"]))\n    pd.DataFrame(history.history).plot(figsize=(12,6))\n    plt.show()","4a59ef8d":"epochs = 40\n\nmodel_1 = k.models.Sequential([\n    k.layers.Dense(2048, activation='relu', input_shape=(X_train.shape[1],)),\n#     k.layers.Dropout(0.3),\n    \n    k.layers.Dense(1024, activation='relu'),\n    k.layers.Dropout(0.2),\n\n    k.layers.Dense(512, activation='relu'),\n    k.layers.Dropout(0.2),\n\n    k.layers.Dense(128, activation='relu'),\n    k.layers.Dropout(0.2),\n\n    k.layers.Dense(1, activation='sigmoid'),\n])\nprint(model_1.summary())\n\nmodel_1.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=[\n#                   tfa.metrics.F1Score(num_classes=1),\n                  'accuracy'\n              ]\n)\nhistory = model_1.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, \n                      batch_size=2048, \n#                       class_weight=class_weights,\n                      callbacks=[callbacks_list]\n                     )\n","f823ac7b":"plotHistory(history)","746757fc":"# y_train_pred = model_1.predict(X_train)\n# y_test_pred = model_1.predict(X_test)\n# print(skm.accuracy_score(y_train, y_train_pred))\n# print(skm.accuracy_score(y_test, y_test_pred))\n# printScore(y_train, y_train_pred)\n# printScore(y_test, y_test_pred)","b9dff2c9":"# Generate Ensembles\n\n# def rmse_cv(model):\n#     '''\n#     Use this function to get quickly the rmse score over a cv\n#     '''\n#     rmse = np.sqrt(-skms.cross_val_score(model, X_train, y_train, \n#                                          scoring=\"neg_mean_squared_error\", cv = 5, n_jobs=-1))\n#     return rmse\n\n# class MixModel(skb.BaseEstimator, skb.RegressorMixin, skb.TransformerMixin):\n#     '''\n#     Here we will get a set of models as parameter already trained and \n#     will calculate the mean of the predictions for using each model predictions\n#     '''\n#     def __init__(self, algs):\n#         self.algs = algs\n\n#     # Define clones of parameters models\n#     def fit(self, X, y):\n#         self.algs_ = [skb.clone(x) for x in self.algs]\n        \n#         # Train cloned base models\n#         for alg in self.algs_:\n#             alg.fit(X, y)\n\n#         return self\n    \n#     # Average predictions of all cloned models\n#     def predict(self, X):\n#         predictions = np.column_stack([\n#             stacked_model.predict(X) for stacked_model in self.algs_\n#         ])\n#         return np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=1, arr=predictions)","36fc3707":"# mixed_model = MixModel(algs = [\n# #     ridge_model, \n# #     enet_model, \n# #     extra_model, \n# #     cat_model,\n# #     rf_model,\n# #     xgb_model,\n# #     gb_model,\n# #     lgb_model,\n#     ada_model\n# ])\n# # score = rmse_cv(mixed_model)\n# # print(\"\\nAveraged base algs score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n\n# mixed_model.fit(X_train, y_train)\n\n# # predict\n# y_train_pred = mixed_model.predict(X_train)\n# y_test_pred = mixed_model.predict(X_test)\n# printScore(y_train, y_train_pred)\n# printScore(y_test, y_test_pred)","c0eca4ee":"def getTestResults(m=None):\n    df_final = df.sample(frac=1, random_state=1).reset_index(drop=True)\n    test_cols = [x for x in df_final.columns if targetFeature not in x]\n    df_final_test = df_test[test_cols]\n    df_y = df_final.pop(targetFeature)\n    df_X = df_final\n\n#     scaler = skp.RobustScaler()\n#     scaler = skp.MinMaxScaler()\n    scaler = skp.StandardScaler()\n\n    df_X = pd.DataFrame(scaler.fit_transform(df_X), columns=df_X.columns)\n    df_final_test = pd.DataFrame(scaler.transform(df_final_test), columns=df_X.columns)\n\n    sample_weights = sku.class_weight.compute_sample_weight('balanced', df_y)\n    \n    if m is None:\n\n#         lmr = sklm.LogisticRegression()\n#         lmr.fit(df_X, df_y)\n\n        lmr = cb.CatBoostClassifier(loss_function='Logloss', verbose=0, eval_metric='AUC', class_weights=class_weights)\n        lmr.fit(df_X, df_y)\n\n#         lmr = ske.ExtraTreesClassifier(criterion='gini', random_state=1, verbose=0, n_jobs=-1,\n#                               n_estimators=200,max_depth=10, min_samples_split = 5, min_samples_leaf = 1)\n#         lmr.fit(df_X, df_y, sample_weight=sample_weights)\n\n#         lmr = ske.AdaBoostClassifier(random_state=seed)\n#         lmr.fit(df_X, df_y, sample_weight=sample_weights)\n\n#         lmr = ske.GradientBoostingClassifier(loss='deviance', random_state=seed, verbose=0,\n#                                     n_estimators=50, max_depth=7,min_samples_leaf=1, min_samples_split=8)\n#         lmr.fit(df_X, df_y, sample_weight=sample_weights)\n\n#         lmr = ske.RandomForestClassifier(verbose=0, random_state=1, n_jobs=-1, class_weight='balanced_subsample',\n#                                  n_estimators=200,max_depth=10, min_samples_split = 7, min_samples_leaf = 1)\n#         lmr.fit(df_X, df_y)\n\n#         lmr = xg.XGBClassifier(objective ='binary:logistic', random_state=seed, verbose=0,\n#                       n_estimators=500, max_depth = 10)\n#         lmr.fit(df_X, df_y)\n\n#         lmr = lgb.LGBMClassifier(objective='binary', class_weight=class_weights, random_state=1, n_jobs=-1, n_estimators=50)\n#         lmr.fit(df_X, df_y)\n\n    else:\n        lmr = m\n\n    # predict\n    y_train_pred = lmr.predict(df_X)\n    y_test_pred = lmr.predict(df_final_test)\n    if m is not None:\n        y_train_pred = [round(y[0]) for y in y_train_pred]\n        y_test_pred = [round(y[0]) for y in y_test_pred]\n    print(skm.accuracy_score(df_y, y_train_pred))\n    printScore(df_y, y_train_pred)\n    return y_test_pred\n\n# ML models\nresults = getTestResults()\n\n# Neural Network model\n# results = getTestResults(k.models.load_model(bestModelPath))","d57845d4":"submission = pd.DataFrame({\n    'ID': df_test['ID'],\n    targetFeature: results,\n})\nprint(submission.Response.value_counts())\nsubmission.head()","5f3603e9":"submission.to_csv('.\/submission_Cat-robust.csv', index=False)","de38969e":"# Step 2: EDA","f9deb29a":"### Feature Scaling","3afde615":"### Holding_Policy_Type Missing Prediction","60cbb0d1":"### KNN","8ec6ae1d":"### Extra Trees","4a49ddff":"# Step 3: Data Preparation","78195d54":"### Profiling for Whole Data","c1580a0e":"# Step 4: Data Modelling\n\n### Split Train-Test Data","c5e44f84":"## Deep Learning Model","2ca314be":"# Test Evaluation & Submission","afd0da73":"# Step 1: Reading and Understanding the Data","004f6fd8":"### Handle Missing","08f6b449":"### RandomForest","98110b16":"### CatBoost","2a88f234":"## Derive Features","1696c20b":"### Univariate Analysis","6271d5a1":"### Skewness","c6be2d51":"### Gradient Boosting","110524fa":"### One-Hot Encoding","b72aff1f":"### Holding_Policy_Duration Missing Prediction","6475d350":"### LightGBM","9ef0df1a":"## Model Building","d086b8a6":"# Health Insurance Lead Prediction - JOB-A-THON","4e65c781":"## Create Dummy Features","b6e485b5":"### XGBoost","6446f553":"### Health Indicator Missing Prediction","613e0666":"### AdaBoost","f40e661d":"### Logistic Regression"}}