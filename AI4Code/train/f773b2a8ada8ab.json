{"cell_type":{"b0de18bf":"code","5ac11e7b":"code","a9f1bbb6":"code","87c10927":"code","30e0f19c":"code","3ab32c2d":"code","cb0cc77c":"code","a1a7103a":"code","a73e1688":"code","1988866b":"code","aa8998e0":"code","1bb2759b":"code","578bb90f":"code","22484c73":"code","a6ed688e":"code","a801612c":"code","4f613b5b":"code","04f88ae6":"code","697a9eb8":"code","f90da2b8":"code","14f31617":"code","09cb583c":"code","32312d72":"code","e0bc5b95":"code","14800e5b":"code","3fc9b9d7":"code","bb1ff7f3":"code","857dd725":"code","a2e56eab":"code","debeb34d":"code","d154e36b":"code","c5ad1dc6":"code","0cedd5a9":"code","0eeebae0":"code","d0ccaa5b":"code","6d05f6c6":"code","72970d9b":"code","47cda78a":"code","294a4a2c":"code","2daa5511":"code","3e97e68a":"code","788e45e7":"code","b2ec3ded":"code","b3ab8aee":"code","d5fcebb6":"code","eb181c1d":"code","7f70e8fb":"code","f783bf0d":"code","fc7d0a9e":"code","fe6c8dd8":"code","bfed030e":"code","9e62e3d6":"code","c053bc18":"code","3482a38f":"code","7938c0a1":"code","3d7d1971":"code","fcb0dc2c":"code","77597e02":"code","3f36ca4a":"markdown","d72ba252":"markdown","53672293":"markdown","2397a87e":"markdown","0d876ef9":"markdown","ea1d47dc":"markdown"},"source":{"b0de18bf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nimport json\nfrom pandas.io.json import json_normalize\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Any results you write to the current directory are saved as output.","5ac11e7b":"def load_df(csv_path, nrows=None):\n    USE_COLUMNS = [\n        'channelGrouping', 'date', 'device', 'fullVisitorId', 'geoNetwork',\n        'socialEngagementType', 'totals', 'trafficSource', 'visitId',\n        'visitNumber', 'visitStartTime', 'customDimensions'\n        #'hits'\n    ]\n    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n    \n    df = pd.read_csv(csv_path, \n                     converters={column: json.loads for column in JSON_COLUMNS}, \n                     dtype={'fullVisitorId': 'str'}, # Important!!\n                     nrows=nrows, usecols=USE_COLUMNS)\n    \n    for column in JSON_COLUMNS:\n        column_as_df = json_normalize(df[column])\n        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n    print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n    return df","a9f1bbb6":"%%time\ntrain_df = load_df(\"..\/input\/train_v2.csv\")\ntest_df = load_df(\"..\/input\/test_v2.csv\")","87c10927":"print(train_df.columns.difference(test_df.columns))\nprint(test_df.columns.difference(train_df.columns))","30e0f19c":"train_df = train_df.drop(labels=['trafficSource.campaignCode'],axis=1)\ntrain_df = train_df.drop(labels=['customDimensions','totals.totalTransactionRevenue'],axis=1)\ntest_df = test_df.drop(labels=['customDimensions','totals.transactionRevenue','totals.totalTransactionRevenue'],axis=1)","3ab32c2d":"#Converting the datatype of date field\ntrain_df['date']= pd.to_datetime(train_df['date'],format='%Y%m%d')\ntest_df['date']= pd.to_datetime(test_df['date'],format='%Y%m%d')\ntrain_df.head()","cb0cc77c":"#We need to predict totals.transactionRevenue. Lets explore that variable\ntrain_df['totals.transactionRevenue'] = train_df['totals.transactionRevenue'].astype('float')\nprint('Total records: ', len(train_df), 'customers doesnt contribute for revenue: ',train_df['totals.transactionRevenue'].isna().sum(), \\\n      'customers contributing revenue: ',len(train_df) - train_df['totals.transactionRevenue'].isna().sum())\n","a1a7103a":"#Lets start with data cleaning\n#Find any unique value columns\nprint('Features with one unique values are :')\nprint(train_df.columns[train_df.nunique()==1])\nnew_df = train_df.drop(labels=train_df.columns[train_df.nunique()==1],axis=1)\ntest_df = test_df.drop(labels=train_df.columns[train_df.nunique()==1],axis=1)","a73e1688":"print(new_df.shape)\npd.options.display.max_columns=new_df.shape[1]\nprint(new_df.info())\nnew_df.head()","1988866b":"\ngeofields = ['geoNetwork.city','geoNetwork.continent','geoNetwork.country','geoNetwork.metro','geoNetwork.networkDomain','geoNetwork.region','geoNetwork.subContinent']\n\nfor fields in geofields:\n    temp_df = new_df.groupby(by=fields).size().sort_values(ascending=False).head(10)\n    print(temp_df)\n    print('*'*30)\n    ","aa8998e0":"temp = new_df.groupby(by=['fullVisitorId']).agg({'visitId':'count','totals.transactionRevenue':'sum'})\nprint(temp.corr())\ntemp.plot(kind='scatter',x='visitId',y='totals.transactionRevenue')","1bb2759b":"#Lets analyze how the visits and transactionRevenue trend with date\nplot_df = new_df.groupby(by=['date']).agg({'fullVisitorId':'count','totals.transactionRevenue':'sum'}).reset_index()\n\nfig = plt.figure(figsize=(15,8))\nplt.subplot(2,1,1)\nplt.plot(pd.to_datetime(plot_df['date']),plot_df['totals.transactionRevenue'])\nplt.ylabel('Revenue')\nplt.xticks(rotation=90)\n\nplt.subplot(2,1,2)\nplt.plot(pd.to_datetime(plot_df['date']),plot_df['fullVisitorId'])\nplt.ylabel('Visits')\nplt.xticks(rotation=90);\n\nfor ax in fig.axes:\n    plt.sca(ax)\n    plt.xticks(rotation=90)","578bb90f":"#Weekly pattern\n\nplot_df = new_df.groupby(by=['date']).agg({'fullVisitorId':'count','totals.transactionRevenue':'sum'})\n\nplot_df = plot_df.resample('W').mean()\n\nplt.figure(figsize=(15,8))\nplt.subplot(2,1,1)\nplt.plot(plot_df.index,plot_df['totals.transactionRevenue'])\nplt.ylabel('Revenue')\n\nplt.subplot(2,1,2)\nplt.plot(plot_df.index,plot_df['fullVisitorId'])\nplt.ylabel('Visits');","22484c73":"plot_df = new_df.groupby(by=['fullVisitorId']).agg({'channelGrouping':'count','totals.transactionRevenue':'sum'})\n\nplt.figure(figsize=(10,8))\nplt.subplot(2,1,1)\nplt.hist(np.log1p(plot_df[plot_df['totals.transactionRevenue']>0]['totals.transactionRevenue']))\n\nplt.subplot(2,1,2)\nplt.hist(np.log1p(plot_df[plot_df['totals.transactionRevenue']>0]['totals.transactionRevenue']))","a6ed688e":"#new_df = new_df.drop(labels=['sessionId','visitId','visitStartTime'])\nnew_df.isna().sum()","a801612c":"new_df['totals.transactionRevenue'].fillna(value=0,inplace=True)\ntransaction_df = new_df[new_df['totals.transactionRevenue']>0.0]\nnontransaction_df = new_df[new_df['totals.transactionRevenue']<=0]\nprint(transaction_df.shape, nontransaction_df.shape)\n#new_df['totals.transactionRevenue']>0.0","4f613b5b":"def getplot(df1, df2, groupfield):\n    fig =plt.figure(figsize=(15,8))\n    plt.subplot(2,1,1)\n    plot_df = df1.groupby(by=[groupfield])['totals.transactionRevenue'].size().reset_index()\n    plot_df = plot_df.sort_values(by=['totals.transactionRevenue'], ascending=False).head(10)\n    plt1 = plt.bar(plot_df[groupfield],plot_df['totals.transactionRevenue'])\n\n    plt.subplot(2,1,2)\n    plot_df = df2.groupby(by=[groupfield])['totals.transactionRevenue'].size().reset_index()\n    plot_df = plot_df.sort_values(by=['totals.transactionRevenue'], ascending=False).head(10)\n    plt2 = plt.bar(plot_df[groupfield],plot_df['totals.transactionRevenue'])\n    for ax in fig.axes:\n        plt.sca(ax)\n        plt.xticks(rotation=90)\n    #fig.tight_layout()\n    return plt1, plt2","04f88ae6":"getplot(transaction_df,nontransaction_df,'geoNetwork.country');","697a9eb8":"getplot(transaction_df,nontransaction_df,'geoNetwork.city');","f90da2b8":"getplot(transaction_df,nontransaction_df,'geoNetwork.continent');","14f31617":"getplot(transaction_df,nontransaction_df,'geoNetwork.metro');","09cb583c":"getplot(transaction_df,nontransaction_df,'geoNetwork.networkDomain');","32312d72":"getplot(transaction_df,nontransaction_df,'geoNetwork.region');","e0bc5b95":"getplot(transaction_df,nontransaction_df,'geoNetwork.subContinent');","14800e5b":"getplot(transaction_df,nontransaction_df,'channelGrouping');","3fc9b9d7":"new_df1 = new_df.drop(labels=['geoNetwork.region','geoNetwork.networkDomain','geoNetwork.metro','geoNetwork.continent','geoNetwork.country','visitId','visitStartTime'],axis=1)\ntest_df1 = test_df.drop(labels=['geoNetwork.region','geoNetwork.networkDomain','geoNetwork.metro','geoNetwork.continent','geoNetwork.country','visitId','visitStartTime'],axis=1)\n","bb1ff7f3":"new_df1 = new_df1.drop(labels=['date','device.isMobile'], axis=1)\ntest_df1 = test_df1.drop(labels=['date','device.isMobile'], axis=1)","857dd725":"getplot(transaction_df,nontransaction_df,'device.browser');","a2e56eab":"getplot(transaction_df,nontransaction_df,'device.deviceCategory');","debeb34d":"getplot(transaction_df,nontransaction_df,'device.operatingSystem');","d154e36b":"transaction_df['trafficSource.adContent'].fillna('NA',inplace=True)\ngetplot(transaction_df,nontransaction_df,'trafficSource.adContent')\n#NA values seems to contribute more for revenue :p","c5ad1dc6":"getplot(transaction_df,nontransaction_df,'trafficSource.source')","0cedd5a9":"new_df1 = new_df1.drop(labels=['trafficSource.adwordsClickInfo.adNetworkType', 'trafficSource.adwordsClickInfo.gclId', 'trafficSource.adwordsClickInfo.page', 'trafficSource.adwordsClickInfo.slot', 'trafficSource.medium', 'trafficSource.medium', 'trafficSource.referralPath','trafficSource.source'],axis=1)\ntest_df1 = test_df1.drop(labels=['trafficSource.adwordsClickInfo.adNetworkType', 'trafficSource.adwordsClickInfo.gclId', 'trafficSource.adwordsClickInfo.page', 'trafficSource.adwordsClickInfo.slot', 'trafficSource.medium', 'trafficSource.medium', 'trafficSource.referralPath','trafficSource.source'],axis=1)\nprint(new_df1.shape)\nnew_df1.head()","0eeebae0":"new_df1.fillna('0',inplace=True)\ntest_df1.fillna('0',inplace=True)","d0ccaa5b":"new_df1.isna().sum()","6d05f6c6":"new_df1['trafficSource.adContent'].fillna('Noadcontent',inplace=True)\nnew_df1['trafficSource.keyword'].fillna('NA',inplace=True)\ntest_df1['trafficSource.adContent'].fillna('Noadcontent',inplace=True)\ntest_df1['trafficSource.keyword'].fillna('NA',inplace=True)\n\nfor columns in ['totals.sessionQualityDim','totals.timeOnSite','totals.transactions']:\n    new_df1[columns].fillna('0',inplace=True)\n    new_df1[columns] = new_df1[columns].astype('int')\n    test_df1[columns].fillna('0',inplace=True)\n    test_df1[columns] = test_df1[columns].astype('int')","72970d9b":"new_df1.info()","47cda78a":"new_df1.head()","294a4a2c":"def convert_category_todummies(df,field):\n    #print('Processing ', field)\n    dummy_df = pd.get_dummies(df[field])\n    df = pd.concat([df,dummy_df],axis=1)\n    df.drop(labels=[field],axis=1,inplace=True)\n    return df","2daa5511":"def convert_category_tolevel(df,field):\n    df[field],index = pd.factorize(df[field])\n    return df","3e97e68a":"#test_df1=test_df1.drop(labels=['totals.transactionRevenue'],axis=1)","788e45e7":"#Data cleaning\ntrain_size = new_df1.shape[0]\nmerged_df = pd.concat([new_df1,test_df1])\nmerged_df['totals.pageviews']=merged_df['totals.pageviews'].astype('int')\nmerged_df['totals.hits']=merged_df['totals.pageviews'].astype('int')\nmerged_df = merged_df.drop(labels=['trafficSource.keyword'],axis=1)\nprint('Before: ', merged_df.shape)\ncolumns = merged_df.columns\nfor fields in columns:\n    if merged_df[fields].dtype == 'object' and fields not in ['fullVisitorId','method']:\n        print('Unique values for ', fields, len(merged_df[fields].unique()), merged_df[fields].unique())\n        if len(merged_df[fields].unique()) > 40:\n            print('Level conversion')\n            merged_df[fields] = convert_category_tolevel(merged_df,fields)\n            merged_df[fields] = merged_df[fields].astype('int')\n        else:\n            print('One hot conversion')\n            merged_df = convert_category_todummies(merged_df,fields)\n            #merged_df = merged_df.drop(labels=[fields],axis=1)\nprint('After: ', merged_df.shape)","b2ec3ded":"from sklearn.preprocessing import MinMaxScaler\n\nscaler1 = MinMaxScaler()\n\n#merged_df1 = merged_df.groupby(by=['fullVisitorId']).mean()\nmerged_df['totals.transactionRevenue'] = np.log1p(merged_df['totals.transactionRevenue'])\n\n\nnew_df1 = merged_df.iloc[:train_size]\ntest_df1 = merged_df[train_size:]\n#new_df1 = new_df1.drop(labels=['method'],axis=1)\ntest_df1 = test_df1.drop(labels=['totals.transactionRevenue'],axis=1)\nprint(new_df1.shape, test_df1.shape)","b3ab8aee":"import gc\ndel train_df, new_df, test_df,merged_df, transaction_df, nontransaction_df, plot_df\ngc.collect()","d5fcebb6":"#we need to predict log revenue per customer. Lets group by full visitor id\ntrain_x = new_df1.groupby(by=['fullVisitorId']).mean()\ndel new_df1\ngc.collect()\n","eb181c1d":"train_y = train_x['totals.transactionRevenue']\ntrain_x = train_x.drop(labels=['totals.transactionRevenue'],axis=1)\nscaled_x = scaler1.fit_transform(train_x.values)\n#train_y = np.log1p(train_y)\ntrain_x = pd.DataFrame(scaled_x, columns=train_x.columns)","7f70e8fb":"import lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(train_x, train_y, test_size=0.15, random_state=1)\n\nlgb_train_data = lgb.Dataset(X_train, label=y_train)\nlgb_val_data = lgb.Dataset(X_val, label=y_val)\n\nparams = {\n        \"objective\" : \"regression\",\n        \"metric\" : \"rmse\",\n        #\"num_leaves\" : 40,\n        \"max_depth\" : 10,\n        \"boosting\" : \"gbdt\",\n        \"learning_rate\" : 0.0025,\n        \"bagging_fraction\" : 0.7,\n        \"feature_fraction\" : 0.7,\n        \"bagging_frequency\" : 6,\n        \"bagging_seed\" : 42,\n        \"seed\": 42}\nmodel = lgb.train(params, lgb_train_data, \n                      num_boost_round=5000,\n                      valid_sets=[lgb_train_data, lgb_val_data],\n                      early_stopping_rounds=100,\n                      verbose_eval=500)","f783bf0d":"test_x = test_df1.groupby(by=['fullVisitorId']).mean()\nvisitorid = test_x.index\nscaled_test = scaler1.transform(test_x)\ntest_x = pd.DataFrame(scaled_test,columns=test_x.columns)","fc7d0a9e":"from sklearn.metrics import mean_squared_error\ny_pred_train = model.predict(X_train, num_iteration=model.best_iteration)\ny_pred_val = model.predict(X_val, num_iteration=model.best_iteration)\ny_pred_submit = model.predict(test_x, num_iteration=model.best_iteration)\n\nprint(f\"LGBM: RMSE val: {np.sqrt(mean_squared_error(y_val, y_pred_val))}  - RMSE train: {np.sqrt(mean_squared_error(y_train, y_pred_train))}\")","fe6c8dd8":"plt.style.use('ggplot')\nlgb.plot_importance(model,max_num_features=15)","bfed030e":"#submission = pd.DataFrame({'fullVisitorId':visitorid,'PredictedLogRevenue':y_pred_submit})\n#submission['fullVisitorId']= submission['fullVisitorId'].astype(str)\n#submission['PredictedLogRevenue']=submission['PredictedLogRevenue'].apply(lambda x: 0 if x<0 else x)","9e62e3d6":"#submission.to_csv('submission1.csv',index=False)","c053bc18":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM, Bidirectional, Dropout\nfrom keras.callbacks import ReduceLROnPlateau\n\nX_train = X_train.values\nX_val = X_val.values\ny_train = y_train.values\ny_val = y_val.values\nX_train = X_train.reshape(X_train.shape[0],1,X_train.shape[1])\nX_val = X_val.reshape(X_val.shape[0],1,X_val.shape[1])\n","3482a38f":"#model1 = Sequential()\n#model1.add(Bidirectional(LSTM(200,recurrent_dropout=0.2, input_shape=(X_train.shape[1],X_train.shape[2]), kernel_initializer='lecun_normal', return_sequences=True)))\n#model1.add(Dropout(0.2))\n#model1.add(Bidirectional(LSTM(120,recurrent_dropout=0.2, kernel_initializer='lecun_normal')))\n#model1.add(Dropout(0.2))\n#model1.add(Dense(50,activation='sigmoid'))\n#model1.add(Dropout(0.2))\n#model1.add(Dense(20,activation='elu'))\n#model1.add(Dense(1,activation='linear'))\n#model1.compile(loss='mse', optimizer='adam')\n\n\n#history = model1.fit(X_train, y_train, epochs=2, batch_size=64, validation_data=(X_val, y_val), verbose=1, shuffle=False)\n#plt.plot(history.history['loss'], label='train')\n#plt.plot(history.history['val_loss'], label='test')\n#plt.legend()\n","7938c0a1":"#test_x = test_x.values\n#test_x = test_x.reshape((test_x.shape[0],1,test_x.shape[1]))\n\n#y_pred_train = model1.predict(X_train)\n#y_pred_val = model1.predict(X_val)\n#y_pred_submit = model1.predict(test_x)\n\n#print(f\"LSTM: RMSE val: {np.sqrt(mean_squared_error(y_val, y_pred_val))}  - RMSE train: {np.sqrt(mean_squared_error(y_train, y_pred_train))}\")","3d7d1971":"from keras.layers import Input\nfrom keras.models import Model\n\ninputs = Input(shape=(1,70))\nx = Bidirectional(LSTM(200,recurrent_dropout=0.2, kernel_initializer='lecun_normal', return_sequences=True))(inputs)\nx = Bidirectional(LSTM(120,recurrent_dropout=0.2, kernel_initializer='lecun_normal'))(x)\nx = Dense(50, activation='sigmoid')(x)\nx = Dropout(0.1)(x)\nx = Dense(20,activation='elu')(x)\noutput = Dense(1,activation='linear')(x)\n\nmodel2 = Model(inputs=inputs, outputs=output)\nmodel2.compile(loss='mse', optimizer='adam')\nmodel2.fit(X_train, y_train, epochs=4, batch_size=64, validation_data=(X_val, y_val), verbose=1, shuffle=False)","fcb0dc2c":"test_x = test_x.values\ntest_x = test_x.reshape((test_x.shape[0],1,test_x.shape[1]))\n\ny_pred_train = model2.predict(X_train)\ny_pred_val = model2.predict(X_val)\ny_pred_submit = model2.predict(test_x)\n\nprint(f\"LSTM: RMSE val: {np.sqrt(mean_squared_error(y_val, y_pred_val))}  - RMSE train: {np.sqrt(mean_squared_error(y_train, y_pred_train))}\")","77597e02":"submission = pd.DataFrame({'fullVisitorId':visitorid,'PredictedLogRevenue':np.squeeze(y_pred_submit)})\nsubmission['fullVisitorId']= submission['fullVisitorId'].astype(str)\nsubmission['PredictedLogRevenue']=submission['PredictedLogRevenue'].apply(lambda x: 0 if x<0 else x)\n\nsubmission.to_csv('submission2.csv',index=False)","3f36ca4a":"Only one year of data is available. With Date or Month we couldn't find any relation. Lets drop it. \ndevice.isMobile is a duplicate variable as the detail is covered in deviceCategory\n","d72ba252":"No of visits doesn't have much relation with transactionRevenue","53672293":"Lets drop labels other than city and subcontinent. As city is the lower granular level in geo. As most values in city is 'not available' lets keep another variable subcontinent as an additional field.\n","2397a87e":"Based on Initial glance of data we can drop following fields while exploring\n* sessionId - Unique number for a session\n* visitId - Nth visit fullVisitorId is visiting the store \n* visitStartTime - Time of Visit \n","0d876ef9":"Numer of visits drasitically increases from October and reduces in December. But that didnt contribute much on revenue. As only one year of data available, we cannot identify any YoY pattern of data.","ea1d47dc":"Purely an imbalanced dataset"}}