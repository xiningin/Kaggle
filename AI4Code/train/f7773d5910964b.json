{"cell_type":{"fc369789":"code","88a7ae3e":"code","706b6f6c":"code","11dbccd4":"code","f451ff23":"code","5fcbe44f":"code","7aa0c4ce":"code","3c681135":"code","9fecf77c":"code","45f31c55":"code","97e7040a":"code","4bc91661":"code","a21d2fa2":"code","a2f67ef7":"code","1d172b61":"code","1f31d29f":"code","b9c3111b":"code","5936847f":"code","30f139ff":"code","ca4d1998":"code","e7b2b2ad":"code","00609388":"code","f51ae2d5":"code","f845d75d":"code","d6f7a069":"code","90dfb6df":"code","bc176b2d":"code","0638ccc7":"code","4c51f72d":"code","4fb530b1":"code","bfc94764":"code","7c12db7b":"code","3ca37cdc":"code","926bed44":"code","5c26fd3e":"code","ab0ddf9a":"code","3daf9250":"code","03b64cda":"code","50fff507":"code","3dc0c75b":"code","35bfbac8":"code","7d811a30":"code","050fa4dd":"code","afcd7d3f":"code","41beb94a":"code","194a8cc9":"code","93444290":"code","8484c4f1":"code","b48703bb":"code","6fc601cb":"markdown","b5a341bb":"markdown","8b0cc9e9":"markdown","a0bc7de2":"markdown","18778e62":"markdown","70b37000":"markdown","ade72bd6":"markdown","2f105089":"markdown","e357a8d3":"markdown","98fd1db7":"markdown","a5003bb1":"markdown","e0117263":"markdown","24318a4b":"markdown","cfd7cdc4":"markdown"},"source":{"fc369789":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","88a7ae3e":"import math\nimport seaborn as sns\nimport datetime as dt\nfrom datetime import datetime    \nsns.set_style(\"whitegrid\")\nfrom pandas.plotting import autocorrelation_plot\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use(\"ggplot\")","706b6f6c":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nkey = user_secrets.get_secret(\"api\")\n\nimport requests\nimport csv\nfrom tqdm import tqdm\n\ndef request_stock_price_list(symbol, size, token):\n    q_string = 'https:\/\/www.alphavantage.co\/query?function=TIME_SERIES_DAILY&symbol={}&outputsize={}&apikey={}'\n    \n    print(\"Retrieving stock price data from Alpha Vantage (This may take a while)...\")\n    r = requests.get(q_string.format(symbol, size, token))\n    print(\"Data has been successfully downloaded...\")\n    date = []\n    colnames = list(range(0, 5))\n    df = pd.DataFrame(columns = colnames)\n    print(\"Sorting the retrieved data into a dataframe...\")\n    for i in tqdm(r.json()['Time Series (Daily)'].keys()):\n        date.append(i)\n        row = pd.DataFrame.from_dict(r.json()['Time Series (Daily)'][i], orient='index').reset_index().T[1:]\n        df = pd.concat([df, row], ignore_index=True)\n    df.columns = [\"open\", \"high\", \"low\", \"close\", \"volume\"]\n    df['date'] = date\n    return df","11dbccd4":"# UNCOMMENT THE CELL IF DATA IS NEEDED TO BE LOADED FOR 1ST TIME\n\n#cv1 = request_stock_price_list('IBM', 'full', key)\n#print(cv1.head)\n#cv1.to_csv('data.csv')","f451ff23":"# For data preprocessing and analysis part\ndata = pd.read_csv('..\/input\/price-volume-data-for-all-us-stocks-etfs\/Stocks\/abe.us.txt')\n#data = pd.read_csv('..\/input\/nifty50-stock-market-data\/COALINDIA.csv')\n#data = pd.read_csv('..\/input\/stock-market-data\/stock_market_data\/nasdaq\/csv\/ABCO.csv')\n#data = pd.read_csv('.\/data.csv')\n# Any CSV or TXT file can be added here....\ndata.head()","5fcbe44f":"data.info()","7aa0c4ce":"data.describe()","3c681135":"data.isnull().sum()","9fecf77c":"data.reset_index(drop=True, inplace=True)\ndata.fillna(data.mean(), inplace=True)\ndata.head()","45f31c55":"data.plot(legend=True,subplots=True, figsize = (12, 6))\nplt.show()\n#data['Close'].plot(legend=True, figsize = (12, 6))\n#plt.show()\n#data['Volume'].plot(legend=True,figsize=(12,7))\n#plt.show()\n\ndata.shape\ndata.size\ndata.describe(include='all').T\ndata.dtypes\ndata.nunique()\nma_day = [10,50,100]\n\nfor ma in ma_day:\n    column_name = \"MA for %s days\" %(str(ma))\n    data[column_name]=pd.DataFrame.rolling(data['Close'],ma).mean()\n\ndata['Daily Return'] = data['Close'].pct_change()\n# plot the daily return percentage\ndata['Daily Return'].plot(figsize=(12,5),legend=True,linestyle=':',marker='o')\nplt.show()\n\nsns.displot(data['Daily Return'].dropna(),bins=100,color='green')\nplt.show()\n\ndate=pd.DataFrame(data['Date'])\nclosing_df1 = pd.DataFrame(data['Close'])\nclose1  = closing_df1.rename(columns={\"Close\": \"data_close\"})\nclose2=pd.concat([date,close1],axis=1)\nclose2.head()\n\ndata.reset_index(drop=True, inplace=True)\ndata.fillna(data.mean(), inplace=True)\ndata.head()\n\ndata.nunique()\n\ndata.sort_index(axis=1,ascending=True)\n\ncols_plot = ['Open', 'High', 'Low','Close','Volume','MA for 10 days','MA for 50 days','MA for 100 days','Daily Return']\naxes = data[cols_plot].plot(marker='.', alpha=0.7, linestyle='None', figsize=(11, 9), subplots=True)\nfor ax in axes:\n    ax.set_ylabel('Daily trade')\n\nplt.plot(data['Close'], label=\"Close price\")\nplt.xlabel(\"Timestamp\")\nplt.ylabel(\"Closing price\")\ndf = data\nprint(df)\n\ndata.isnull().sum()","97e7040a":"cols_plot = ['Open', 'High', 'Low','Close']\naxes = data[cols_plot].plot(marker='.', alpha=0.5, linestyle='None', figsize=(11, 9), subplots=True)\nfor ax in axes:\n    ax.set_ylabel('Daily trade')","4bc91661":"plt.plot(data['Close'], label=\"Close price\")\nplt.xlabel(\"Timestamp\")\nplt.ylabel(\"Closing price\")\ndf = data\nprint(df)\n\ndf.describe().transpose()","a21d2fa2":"from sklearn.model_selection import train_test_split\n\nX = []\nY = []\nwindow_size=100\nfor i in range(1 , len(df) - window_size -1 , 1):\n    first = df.iloc[i,2]\n    temp = []\n    temp2 = []\n    for j in range(window_size):\n        temp.append((df.iloc[i + j, 2] - first) \/ first)\n    temp2.append((df.iloc[i + window_size, 2] - first) \/ first)\n    X.append(np.array(temp).reshape(100, 1))\n    Y.append(np.array(temp2).reshape(1, 1))\n\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, shuffle=True)\n\ntrain_X = np.array(x_train)\ntest_X = np.array(x_test)\ntrain_Y = np.array(y_train)\ntest_Y = np.array(y_test)\n\ntrain_X = train_X.reshape(train_X.shape[0],1,100,1)\ntest_X = test_X.reshape(test_X.shape[0],1,100,1)\n\nprint(len(train_X))\nprint(len(test_X))","a2f67ef7":"# For creating model and training\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv1D, LSTM, Dense, Dropout, Bidirectional, TimeDistributed\nfrom tensorflow.keras.layers import MaxPooling1D, Flatten\nfrom tensorflow.keras.regularizers import L1, L2\nfrom tensorflow.keras.metrics import Accuracy\nfrom tensorflow.keras.metrics import RootMeanSquaredError\n\nmodel = tf.keras.Sequential()\n\n# Creating the Neural Network model here...\n# CNN layers\nmodel.add(TimeDistributed(Conv1D(64, kernel_size=3, activation='relu', input_shape=(None, 100, 1))))\nmodel.add(TimeDistributed(MaxPooling1D(2)))\nmodel.add(TimeDistributed(Conv1D(128, kernel_size=3, activation='relu')))\nmodel.add(TimeDistributed(MaxPooling1D(2)))\nmodel.add(TimeDistributed(Conv1D(64, kernel_size=3, activation='relu')))\nmodel.add(TimeDistributed(MaxPooling1D(2)))\nmodel.add(TimeDistributed(Flatten()))\n# model.add(Dense(5, kernel_regularizer=L2(0.01)))\n\n# LSTM layers\nmodel.add(Bidirectional(LSTM(100, return_sequences=True)))\nmodel.add(Dropout(0.5))\nmodel.add(Bidirectional(LSTM(100, return_sequences=False)))\nmodel.add(Dropout(0.5))\n\n#Final layers\nmodel.add(Dense(1, activation='linear'))\nmodel.compile(optimizer='adam', loss='mse', metrics=['mse', 'mae'])\n\nhistory = model.fit(train_X, train_Y, validation_data=(test_X,test_Y), epochs=40,batch_size=40, verbose=1, shuffle =True)","1d172b61":"plt.plot(history.history['loss'], label='train loss')\nplt.plot(history.history['val_loss'], label='val loss')\nplt.xlabel(\"epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()","1f31d29f":"plt.plot(history.history['mse'], label='train mse')\nplt.plot(history.history['val_mse'], label='val mse')\nplt.xlabel(\"epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()","b9c3111b":"plt.plot(history.history['mae'], label='train mae')\nplt.plot(history.history['val_mae'], label='val mae')\nplt.xlabel(\"epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()","5936847f":"# After the model has been constructed, we'll summarise it\nfrom tensorflow.keras.utils import plot_model\nprint(model.summary())\nplot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)","30f139ff":"model.evaluate(test_X, test_Y)","ca4d1998":"from sklearn.metrics import explained_variance_score, mean_poisson_deviance, mean_gamma_deviance\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import max_error\n\n# predict probabilities for test set\nyhat_probs = model.predict(test_X, verbose=0)\n# reduce to 1d array\nyhat_probs = yhat_probs[:, 0]\n\nvar = explained_variance_score(test_Y.reshape(-1,1), yhat_probs)\nprint('Variance: %f' % var)\n\nr2 = r2_score(test_Y.reshape(-1,1), yhat_probs)\nprint('R2 Score: %f' % var)\n\nvar2 = max_error(test_Y.reshape(-1,1), yhat_probs)\nprint('Max Error: %f' % var2)","e7b2b2ad":"predicted  = model.predict(test_X)\ntest_label = test_Y.reshape(-1,1)\npredicted = np.array(predicted[:,0]).reshape(-1,1)\nlen_t = len(train_X)\nfor j in range(len_t , len_t + len(test_X)):\n    temp = data.iloc[j,3]\n    test_label[j - len_t] = test_label[j - len_t] * temp + temp\n    predicted[j - len_t] = predicted[j - len_t] * temp + temp\nplt.plot(predicted, color = 'green', label = 'Predicted  Stock Price')\nplt.plot(test_label, color = 'red', label = 'Real Stock Price')\nplt.title(' Stock Price Prediction')\nplt.xlabel('Time')\nplt.ylabel(' Stock Price')\nplt.legend()\nplt.show()","00609388":"# First we need to save a model\nmodel.save(\"model.h5\")","f51ae2d5":"# Load model\nnew_model = tf.keras.models.load_model(\".\/model.h5\")","f845d75d":"new_model.summary()","d6f7a069":"# For data preprocessing and analysis part\n#data2 = pd.read_csv('..\/input\/price-volume-data-for-all-us-stocks-etfs\/Stocks\/aaoi.us.txt')\n#data2 = pd.read_csv('..\/input\/nifty50-stock-market-data\/SBIN.csv')\n#data2 = pd.read_csv('..\/input\/stock-market-data\/stock_market_data\/nasdaq\/csv\/ACTG.csv')\ndata2 = pd.read_csv('.\/data.csv')\n# Any CSV or TXT file can be added here....\ndata2.dropna(inplace=True)\ndata2.head()\n\ndata2.reset_index(drop=True, inplace=True)\ndata2.fillna(data.mean(), inplace=True)\ndata2.head()\ndf2 = data2.drop('date', axis=1)\n\nprint(df2)\n\nX = []\nY = []\nwindow_size=100\nfor i in range(1 , len(df2) - window_size -1 , 1):\n    first = df2.iloc[i,4]\n    temp = []\n    temp2 = []\n    for j in range(window_size):\n        temp.append((df2.iloc[i + j, 4] - first) \/ first)\n    # for j in range(week):\n    temp2.append((df2.iloc[i + window_size, 4] - first) \/ first)\n    # X.append(np.array(stock.iloc[i:i+window_size,4]).reshape(50,1))\n    # Y.append(np.array(stock.iloc[i+window_size,4]).reshape(1,1))\n    # print(stock2.iloc[i:i+window_size,4])\n    X.append(np.array(temp).reshape(100, 1))\n    Y.append(np.array(temp2).reshape(1, 1))\n\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, shuffle=False)\n\ntrain_X = np.array(x_train)\ntest_X = np.array(x_test)\ntrain_Y = np.array(y_train)\ntest_Y = np.array(y_test)\n\ntrain_X = train_X.reshape(train_X.shape[0],1,100,1)\ntest_X = test_X.reshape(test_X.shape[0],1,100,1)\n\nprint(len(train_X))\nprint(len(test_X))","90dfb6df":"model.evaluate(test_X, test_Y)","bc176b2d":"predicted  = model.predict(test_X)\ntest_label = test_Y.reshape(-1,1)\npredicted = np.array(predicted[:,0]).reshape(-1,1)\nlen_t = len(train_X)\nfor j in range(len_t , len_t + len(test_X)):\n    temp = data2.iloc[j,3]\n    test_label[j - len_t] = test_label[j - len_t] * temp + temp\n    predicted[j - len_t] = predicted[j - len_t] * temp + temp\nplt.plot(predicted, color = 'green', label = 'Predicted  Stock Price')\nplt.plot(test_label, color = 'red', label = 'Real Stock Price')\nplt.title(' Stock Price Prediction')\nplt.xlabel('Time')\nplt.ylabel(' Stock Price')\nplt.legend()\nplt.show()","0638ccc7":"# Converting model from HDF5 format to TFJS format...\n!pip install tensorflowjs[wizard]\n# Need to be done on a CLI and not in notebook\n!tensorflowjs_converter --input_format=keras \/kaggle\/working\/model.h5 \/kaggle\/working\/model-tjs","4c51f72d":"dataX = pd.read_csv('.\/data.csv')\ndataY = pd.read_csv('.\/data.csv')\ndataX.info()","4fb530b1":"dataX.head()","bfc94764":"start_date = '2020-01-01'\nend_date = '2021-11-29'\n\nstart = '2018-01-01'\nend = '2020-01-01'\n\nfill = (dataX['date']>=start_date) & (dataX['date']<=end_date)\ndataX = dataX.loc[fill]\ndataX","7c12db7b":"fill2 = (dataY['date']>=start) & (dataY['date']<=end)\ndataY = dataY.loc[fill2]\ndataY","3ca37cdc":"dataX.describe()","926bed44":"dataY.describe()","5c26fd3e":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.model_selection import train_test_split,GridSearchCV,RandomizedSearchCV\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\nfrom sklearn.metrics import r2_score,mean_squared_error\n\nsns_plot = sns.distplot(dataX['close'])\nsns_plot2 = sns.distplot(dataY['close'])","ab0ddf9a":"fig, ax = plt.subplots(4, 2, figsize = (15, 13))\nsns.boxplot(x= dataX[\"close\"], ax = ax[0,0])\nsns.distplot(dataX['close'], ax = ax[0,1])\nsns.boxplot(x= dataX[\"open\"], ax = ax[1,0])\nsns.distplot(dataX['open'], ax = ax[1,1])\nsns.boxplot(x= dataX[\"high\"], ax = ax[2,0])\nsns.distplot(dataX['high'], ax = ax[2,1])\nsns.boxplot(x= dataX[\"low\"], ax = ax[3,0])\nsns.distplot(dataX['low'], ax = ax[3,1])\nplt.tight_layout()","3daf9250":"fig, ax = plt.subplots(4, 2, figsize = (15, 13))\nsns.boxplot(x= dataY[\"close\"], ax = ax[0,0])\nsns.distplot(dataY['close'], ax = ax[0,1])\nsns.boxplot(x= dataY[\"open\"], ax = ax[1,0])\nsns.distplot(dataY['open'], ax = ax[1,1])\nsns.boxplot(x= dataY[\"high\"], ax = ax[2,0])\nsns.distplot(dataY['high'], ax = ax[2,1])\nsns.boxplot(x= dataY[\"low\"], ax = ax[3,0])\nsns.distplot(dataY['low'], ax = ax[3,1])\nplt.tight_layout()","03b64cda":"plt.figure(figsize=(10,6))\nsns.heatmap(dataX.corr(),cmap=plt.cm.Reds,annot=True)\nplt.title('Heatmap displaying the relationship between the features of the data (During COVID)',\n         fontsize=13)\nplt.show()","50fff507":"plt.figure(figsize=(10,6))\nsns.heatmap(dataY.corr(),cmap=plt.cm.Blues,annot=True)\nplt.title('Heatmap displaying the relationship between the features of the data (Before COVID)',\n         fontsize=13)\nplt.show()","3dc0c75b":"# For other company....\n\n# UNCOMMENT IF NEEDED...\n#cv2 = request_stock_price_list('RELIANCE.BSE', 'full', key)\n#print(cv2.head)\n#cv2.to_csv('data2.csv')\n\ndataX = pd.read_csv('.\/data2.csv')\ndataY = pd.read_csv('.\/data2.csv')\ndataX.info()","35bfbac8":"start_date = '2020-01-01'\nend_date = '2021-11-29'\n\nstart = '2018-01-01'\nend = '2020-01-01'\n\nfill = (dataX['date']>=start_date) & (dataX['date']<=end_date)\ndataX = dataX.loc[fill]\ndataX","7d811a30":"fill2 = (dataY['date']>=start) & (dataY['date']<=end)\ndataY = dataY.loc[fill2]\ndataY","050fa4dd":"dataX.describe()","afcd7d3f":"dataY.describe()","41beb94a":"sns_plot = sns.distplot(dataX['close'])\nsns_plot2 = sns.distplot(dataY['close'])","194a8cc9":"fig, ax = plt.subplots(4, 2, figsize = (15, 13))\nsns.boxplot(x= dataX[\"close\"], ax = ax[0,0])\nsns.distplot(dataX['close'], ax = ax[0,1])\nsns.boxplot(x= dataX[\"open\"], ax = ax[1,0])\nsns.distplot(dataX['open'], ax = ax[1,1])\nsns.boxplot(x= dataX[\"high\"], ax = ax[2,0])\nsns.distplot(dataX['high'], ax = ax[2,1])\nsns.boxplot(x= dataX[\"low\"], ax = ax[3,0])\nsns.distplot(dataX['low'], ax = ax[3,1])\nplt.tight_layout()","93444290":"fig, ax = plt.subplots(4, 2, figsize = (15, 13))\nsns.boxplot(x= dataY[\"close\"], ax = ax[0,0])\nsns.distplot(dataY['close'], ax = ax[0,1])\nsns.boxplot(x= dataY[\"open\"], ax = ax[1,0])\nsns.distplot(dataY['open'], ax = ax[1,1])\nsns.boxplot(x= dataY[\"high\"], ax = ax[2,0])\nsns.distplot(dataY['high'], ax = ax[2,1])\nsns.boxplot(x= dataY[\"low\"], ax = ax[3,0])\nsns.distplot(dataY['low'], ax = ax[3,1])\nplt.tight_layout()","8484c4f1":"plt.figure(figsize=(10,6))\nsns.heatmap(dataX.corr(),cmap=plt.cm.Reds,annot=True)\nplt.title('Heatmap displaying the relationship between the features of the data (During COVID)',\n         fontsize=13)\nplt.show()","b48703bb":"plt.figure(figsize=(10,6))\nsns.heatmap(dataY.corr(),cmap=plt.cm.Blues,annot=True)\nplt.title('Heatmap displaying the relationship between the features of the data (Before COVID)',\n         fontsize=13)\nplt.show()","6fc601cb":"Filling null columns with mean values....","b5a341bb":"Before preprocessing data, a function to fetch real-time stock data (using Alpha Vantage API) is made","8b0cc9e9":"Then the datasets are loaded","a0bc7de2":"# Data Preprocessing and Analysis","18778e62":"The data has been analysed but it must be converted into data of shape [100,1] to make it easier for CNN to train on... Else it won't select necessary features and the model will fail","70b37000":"This part has 2 subparts: CNN and LSTM\n\nFor CNN, the layers are created with sizes 64,128,64 with kernel size = 3. In every layer, TimeDistributed function is added to track the features for every temporal slice of data with respect to time. In between, MaxPooling layers are added.\n\nAfter that, it's passed to Bi-LSTM layers","ade72bd6":"# EDA","2f105089":"# Stock Market Prediction using CNN-LSTM model\nThis project is about analysis of Stock Market and providing predictions to the stockholders. For this, we used CNN-LSTM approach to create a blank model, then use it to train on stock market data. Further implementation is discussed below...","e357a8d3":"After that, we'll visualize the data for understanding, this is shown below...","98fd1db7":"# Training part","a5003bb1":"In this part, the model is saved and loaded back again. Then, it's made to train again but with different data to check it's loss and prediction","e0117263":"This section is exploratory data analysis on the dataset collected. This is just for analysing the data...","24318a4b":"# Testing part","cfd7cdc4":"Then we'd print the data after making changes and dropping null data"}}