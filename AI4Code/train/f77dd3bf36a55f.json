{"cell_type":{"4ab743b6":"code","bbe6a851":"code","2b061973":"code","640fccc1":"code","a01f07fe":"code","9bf692d6":"code","90efcf80":"code","a98a42c4":"code","6b2aaf3a":"code","ef3752d7":"code","fc875128":"code","17b1ba1c":"code","4dfe2248":"code","efa4a800":"code","94719137":"code","61ebadf8":"code","4d4ec800":"code","464d0fa7":"code","1d5427ad":"code","5d650739":"code","c98f58f2":"code","01773637":"markdown","f2cf9991":"markdown","dd629479":"markdown","1c8a1b8b":"markdown","cb79e33b":"markdown","99fb7dba":"markdown","52fe6ee9":"markdown"},"source":{"4ab743b6":"from IPython.display import clear_output\n!git clone https:\/\/github.com\/matterport\/Mask_RCNN.git # load Mask R-CNN code implementation\n!git clone https:\/\/github.com\/ruslan-kl\/brain-tumor.git # load new data set and annotations \n!pip install pycocotools\n\n!rm -rf brain-tumor\/.git\/\n!rm -rf Mask_RCNN\/.git\/\n\nclear_output()","bbe6a851":"import tensorflow as tf\nprint(tf.__version__)","2b061973":"import os \nimport sys\nfrom tqdm import tqdm\nimport cv2\nimport numpy as np\nimport json\nimport skimage.draw\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport random\n\n# Root directory of the project\nROOT_DIR = os.path.abspath('Mask_RCNN\/')\n# Import Mask RCNN\nsys.path.append(ROOT_DIR) \nfrom mrcnn.config import Config\nfrom mrcnn import utils\nfrom mrcnn.model import log\nimport mrcnn.model as modellib\nfrom mrcnn import visualize\n# Import COCO config\nsys.path.append(os.path.join(ROOT_DIR, 'samples\/coco\/'))\nimport coco\n\nplt.rcParams['figure.facecolor'] = 'white'\n\nclear_output()","640fccc1":"def get_ax(rows=1, cols=1, size=7):\n    \"\"\"Return a Matplotlib Axes array to be used in\n    all visualizations in the notebook. Provide a\n    central point to control graph sizes.\n    \n    Change the default size attribute to control the size\n    of rendered images\n    \"\"\"\n    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n    return ax","a01f07fe":"MODEL_DIR = os.path.join(ROOT_DIR, 'logs') # directory to save logs and trained model\n# ANNOTATIONS_DIR = 'brain-tumor\/data\/new\/annotations\/' # directory with annotations for train\/val sets\nDATASET_DIR = 'brain-tumor\/data_cleaned\/' # directory with image data\nDEFAULT_LOGS_DIR = 'logs' \n\n# Local path to trained weights file\nCOCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n# Download COCO trained weights from Releases if needed\nif not os.path.exists(COCO_MODEL_PATH):\n    utils.download_trained_weights(COCO_MODEL_PATH)","9bf692d6":"class TumorConfig(Config):\n    \"\"\"Configuration for training on the brain tumor dataset.\n    \"\"\"\n    # Give the configuration a recognizable name\n    NAME = 'tumor_detector'\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 1\n    NUM_CLASSES = 1 + 1  # background + tumor\n    DETECTION_MIN_CONFIDENCE = 0.85    \n    STEPS_PER_EPOCH = 100\n    LEARNING_RATE = 0.001\n    \nconfig = TumorConfig()\nconfig.display()","90efcf80":"class BrainScanDataset(utils.Dataset):\n\n    def load_brain_scan(self, dataset_dir, subset):\n        \"\"\"Load a subset of the FarmCow dataset.\n        dataset_dir: Root directory of the dataset.\n        subset: Subset to load: train or val\n        \"\"\"\n        # Add classes. We have only one class to add.\n        self.add_class(\"tumor\", 1, \"tumor\")\n\n        # Train or validation dataset?\n        assert subset in [\"train\", \"val\", 'test']\n        dataset_dir = os.path.join(dataset_dir, subset)\n\n        annotations = json.load(open(os.path.join(DATASET_DIR, subset, 'annotations_'+subset+'.json')))\n        annotations = list(annotations.values())  # don't need the dict keys\n\n        # The VIA tool saves images in the JSON even if they don't have any\n        # annotations. Skip unannotated images.\n        annotations = [a for a in annotations if a['regions']]\n\n        # Add images\n        for a in annotations:\n            # Get the x, y coordinaets of points of the polygons that make up\n            # the outline of each object instance. These are stores in the\n            # shape_attributes (see json format above)\n            # The if condition is needed to support VIA versions 1.x and 2.x.\n            if type(a['regions']) is dict:\n                polygons = [r['shape_attributes'] for r in a['regions'].values()]\n            else:\n                polygons = [r['shape_attributes'] for r in a['regions']]\n\n            # load_mask() needs the image size to convert polygons to masks.\n            # Unfortunately, VIA doesn't include it in JSON, so we must read\n            # the image. This is only managable since the dataset is tiny.\n            image_path = os.path.join(dataset_dir, a['filename'])\n            image = skimage.io.imread(image_path)\n            height, width = image.shape[:2]\n\n            self.add_image(\n                \"tumor\",\n                image_id=a['filename'],  # use file name as a unique image id\n                path=image_path,\n                width=width, \n                height=height,\n                polygons=polygons\n            )\n\n    def load_mask(self, image_id):\n        \"\"\"Generate instance masks for an image.\n       Returns:\n        masks: A bool array of shape [height, width, instance count] with\n            one mask per instance.\n        class_ids: a 1D array of class IDs of the instance masks.\n        \"\"\"\n        # If not a farm_cow dataset image, delegate to parent class.\n        image_info = self.image_info[image_id]\n        if image_info[\"source\"] != \"tumor\":\n            return super(self.__class__, self).load_mask(image_id)\n\n        # Convert polygons to a bitmap mask of shape\n        # [height, width, instance_count]\n        info = self.image_info[image_id]\n        mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"polygons\"])],\n                        dtype=np.uint8)\n        for i, p in enumerate(info[\"polygons\"]):\n            # Get indexes of pixels inside the polygon and set them to 1\n            rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])\n            mask[rr, cc, i] = 1\n\n        # Return mask, and array of class IDs of each instance. Since we have\n        # one class ID only, we return an array of 1s\n        return mask.astype(np.bool), np.ones([mask.shape[-1]], dtype=np.int32)\n\n    def image_reference(self, image_id):\n        \"\"\"Return the path of the image.\"\"\"\n        info = self.image_info[image_id]\n        if info[\"source\"] == \"tumor\":\n            return info[\"path\"]\n        else:\n            super(self.__class__, self).image_reference(image_id)\n","a98a42c4":"model = modellib.MaskRCNN(\n    mode='training', \n    config=config, \n    model_dir=DEFAULT_LOGS_DIR\n)\n\nmodel.load_weights(\n    COCO_MODEL_PATH, \n    by_name=True, \n    exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \"mrcnn_bbox\", \"mrcnn_mask\"]\n)","6b2aaf3a":"# Training dataset.\ndataset_train = BrainScanDataset()\ndataset_train.load_brain_scan(DATASET_DIR, 'train')\ndataset_train.prepare()\n\n# Validation dataset\ndataset_val = BrainScanDataset()\ndataset_val.load_brain_scan(DATASET_DIR, 'val')\ndataset_val.prepare()\n\ndataset_test = BrainScanDataset()\ndataset_test.load_brain_scan(DATASET_DIR, 'test')\ndataset_test.prepare()\n\n# Since we're using a very small dataset, and starting from\n# COCO trained weights, we don't need to train too long. Also,\n# no need to train all layers, just the heads should do it.\nprint(\"Training network heads\")\nmodel.train(\n    dataset_train, dataset_val,\n    learning_rate=config.LEARNING_RATE,\n    epochs=15,\n    layers='heads'\n)","ef3752d7":"# Recreate the model in inference mode\nmodel = modellib.MaskRCNN(\n    mode=\"inference\", \n    config=config,\n    model_dir=DEFAULT_LOGS_DIR\n)\n\n# Get path to saved weights\n# Either set a specific path or find last trained weights\n# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\nmodel_path = model.find_last()\n\n# Load trained weights\nprint(\"Loading weights from \", model_path)\nmodel.load_weights(model_path, by_name=True)","fc875128":"import keras\n# Save the Keras Model\nkeras.models.save_model(model.keras_model,\"mask_rcnn_coco.hdf5\")\n\n# Save weights\nmodel.keras_model.save_weights(\"mask_rcnn_coco.h5\")","17b1ba1c":"def predict_and_plot_differences(dataset, img_id):\n    original_image, image_meta, gt_class_id, gt_box, gt_mask =\\\n        modellib.load_image_gt(dataset, config, \n                               img_id, use_mini_mask=False)\n\n    results = model.detect([original_image], verbose=0)\n    r = results[0]\n\n    visualize.display_differences(\n        original_image,\n        gt_box, gt_class_id, gt_mask,\n        r['rois'], r['class_ids'], r['scores'], r['masks'],\n        class_names = ['tumor'], title=\"\", ax=get_ax(),\n        show_mask=True, show_box=True)","4dfe2248":"def display_image(dataset, ind):\n    plt.figure(figsize=(5,5))\n    plt.imshow(dataset.load_image(ind))\n    plt.xticks([])\n    plt.yticks([])\n    plt.title('Original Image')\n    plt.show()","efa4a800":"ind = 0\ndisplay_image(dataset_val, ind)\npredict_and_plot_differences(dataset_val, ind)","94719137":"ind = 10\ndisplay_image(dataset_val, ind)\npredict_and_plot_differences(dataset_val, ind)","61ebadf8":"ind = 4\ndisplay_image(dataset_val, ind)\npredict_and_plot_differences(dataset_val, ind)","4d4ec800":"ind = 0\ndisplay_image(dataset_test, ind)\npredict_and_plot_differences(dataset_test, ind)","464d0fa7":"ind = 1\ndisplay_image(dataset_test, ind)\npredict_and_plot_differences(dataset_test, ind)","1d5427ad":"ind = 2\ndisplay_image(dataset_test, ind)\npredict_and_plot_differences(dataset_test, ind)","5d650739":"ind = 3\ndisplay_image(dataset_test, ind)\npredict_and_plot_differences(dataset_test, ind)","c98f58f2":"!rm -rf brain-tumor\/\n!rm -rf Mask_RCNN\/","01773637":"## Validation Set","f2cf9991":"# <a id='env'>2. Setting up the Environment<\/a>","dd629479":"# <a id='intro'>1. Project Overview<\/a>\n\nPreviously I have built a CNN model for [Brain Tumor](https:\/\/www.kaggle.com\/navoneel\/brain-mri-images-for-brain-tumor-detection) detection problem: **[Brain Tumor Detection v1.0 || CNN, VGG-16](https:\/\/www.kaggle.com\/ruslankl\/brain-tumor-detection-v1-0-cnn-vgg-16)**. It was done using VGG-16 model architecture and pre-trained weights. But it was rather image classificator for **scan with tumor\/scan without tumor**. This time I am using [Mask R-CNN](https:\/\/github.com\/matterport\/Mask_RCNN) to build an actual detector which will point out on the location of the tumor on the scan.\n\nFor doing this I've added annotations for scans with [VGG Image Annotator (VIA)](http:\/\/www.robots.ox.ac.uk\/~vgg\/software\/via\/) (just the `yes` folder obviously). I haven't realized that this data set is such a mess. It has a lot of duplicated scans which I tried to remove (not sure that I've removed all of the duplicates though). To make it easy for myself I've added new dataset to [GitHub Repo](https:\/\/github.com\/ruslan-kl\/brain-tumor) which contains of following folders: `annotations` and `train`, `val`, `test` images.\n\nThere were some issues with annotations. I couldn't visually detect tumor on some of the scans from `yes` folder (could be caused by the fact that I am not a radiologist or the dataset is not properly assigned). Also on some scans I couldn't figure out which area should be labelled as tumor since it was vague. However, this project is just an example of how you can deal with such task and I will improve it as soon I get more expertise.","1c8a1b8b":"# <a id='res'>4. Results<\/a>","cb79e33b":"**<center><font size=5>Brain Tumor Detection with Mask R-CNN<\/font><\/center>**\n<center><img src=\"https:\/\/i.ibb.co\/YjpWZ4X\/download.png\"><\/center>\n***\n**author**: Ruslan Klymentiev\n\n**date**: 30th August, 2019\n\n**Table of Contents**\n- <a href='#intro'>1. Project Overview<\/a> \n- <a href='#env'>2. Setting up the Environment<\/a>\n- <a href='#mrcnn'>3. Mask R-CNN Model<\/a>\n- <a href='#res'>4. Results<\/a>","99fb7dba":"# <a id='import'>3. Mask R-CNN Model<\/a>","52fe6ee9":"## Test Set\n\nLet's see how model performs on images that it hasn't seen before (at least I believe that I removed all the duplicates)."}}