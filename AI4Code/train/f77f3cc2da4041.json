{"cell_type":{"97ae300f":"code","6f76e0c4":"code","0401ce3a":"code","64c0264b":"code","0c86300f":"code","6849a71c":"code","92eeb2c0":"code","74639de4":"code","9ef49ab3":"code","98701da5":"code","50d68bac":"code","8d384078":"code","79c07975":"code","b641dcf4":"code","6c251c65":"code","17c87e50":"code","b14676e1":"code","e7f9b590":"code","b5ef049a":"code","3e47fac8":"code","b019282e":"code","3f300095":"code","0839648e":"code","a65d16c0":"code","8c0dd34e":"code","4c8a86bd":"code","54bad737":"code","dd70e1a6":"code","712251e6":"code","0e7b9f72":"code","c09411bd":"code","cb0be12c":"code","63bce068":"code","21ffe70b":"code","61acb073":"code","5ad5c3da":"code","34e02c7e":"code","37b69ec1":"code","3ccb9eef":"code","df6a71b0":"code","facd69cf":"code","00337dff":"code","fa52823b":"code","ef77a2aa":"code","c4e7511f":"code","89fa9459":"code","a01d9a64":"code","8e5b5061":"code","59eeff30":"code","674c0f06":"markdown","628b1078":"markdown","deed18bd":"markdown","b57c8bb7":"markdown","f7ffaa12":"markdown","9ce69024":"markdown","bc0adf08":"markdown","ba19f667":"markdown","9c63eeb7":"markdown","93132a9e":"markdown","fc72898d":"markdown","b6435318":"markdown","f27e3172":"markdown","eda91628":"markdown","7aa2aca3":"markdown","bbe03b34":"markdown","32ac74dc":"markdown","9e41da99":"markdown","615d770c":"markdown","d85361d5":"markdown","416dd618":"markdown","4629aca0":"markdown","0364bdbf":"markdown","d5065bd0":"markdown","e51a7a42":"markdown","7ca8e54b":"markdown","f3d7142c":"markdown","80969fa2":"markdown","2f6bef58":"markdown","7f687846":"markdown","f0d46e54":"markdown","9e0949dc":"markdown","069d3dec":"markdown","4447175a":"markdown","7569aaca":"markdown","0583bec8":"markdown","8bc0db33":"markdown","d2bd32e7":"markdown","c7757ad2":"markdown","7cd64106":"markdown","ee9ecac5":"markdown","893f25ce":"markdown","b047aa70":"markdown"},"source":{"97ae300f":"#Reading Data into Pandas DataFrame\nimport pandas as pd\n\ndf = pd.read_csv('..\/input\/telecom-churn-datasets\/churn-bigml-20.csv')","6f76e0c4":"df.info()  # we see that we have 2666 observations and no null values","0401ce3a":"print (\"Rows     : \" ,df.shape[0])\nprint (\"Columns  : \" ,df.shape[1])\nprint (\"\\nFeatures : \\n\" ,df.columns.tolist())\nprint (\"\\nMissing values :  \", df.isnull().sum().values.sum())\nprint (\"\\nUnique values :  \\n\",df.nunique())","64c0264b":"import io\nimport plotly.offline as py#visualization\npy.init_notebook_mode(connected=True)#visualization\nimport plotly.graph_objs as go#visualization\nimport plotly.tools as tls#visualization\nimport plotly.figure_factory as ff#visualization\nimport matplotlib.pyplot as plt#visualization\nfrom PIL import  Image\n%matplotlib inline\nimport pandas as pd\nimport seaborn as sns#visualization\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n#labels\nlab = df[\"Churn\"].value_counts().keys().tolist()\n#values\nval = df[\"Churn\"].value_counts().values.tolist()\n\ntrace = go.Pie(labels = lab ,\n               values = val ,\n               marker = dict(colors =  [ 'royalblue' ,'lime'],\n                             line = dict(color = \"white\",\n                                         width =  1.3)\n                            ),\n               rotation = 90,\n               hoverinfo = \"label+value+text\",\n               hole = .5\n              )\nlayout = go.Layout(dict(title = \"Customer attrition in data\",\n                        plot_bgcolor  = \"rgb(243,243,243)\",\n                        paper_bgcolor = \"rgb(243,243,243)\",\n                       )\n                  )\n\ndata = [trace]\nfig = go.Figure(data = data,layout = layout)\npy.iplot(fig)","0c86300f":"#Separating churn and non churn customers\nchurn     = df[df[\"Churn\"] == bool(True)]\nnot_churn = df[df[\"Churn\"] == bool(False)]","6849a71c":"#Dropping Account Length as it doesnt make a sense here\ndf = df.drop('Account length',axis=1)","92eeb2c0":"#Area Code\ndf['Area code'].unique()","74639de4":"#Replacing Yes\/No values with 1 and 0\ndf['International plan'] = df['International plan'].replace({\"Yes\":1,\"No\":0}).astype(int)\ndf['Voice mail plan'] = df['Voice mail plan'].replace({\"Yes\":1,\"No\":0}).astype(int)","9ef49ab3":"#Voice-Mail Feautre Messages\nprint('Unique vmail messages',df['Number vmail messages'].unique())\ndf['Number vmail messages'].describe()","98701da5":"import matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline\n\n\nplt.show()\ndf.boxplot(column='Number vmail messages', by='Churn')\n","50d68bac":"print('Maximum number of minutes:',df['Total day minutes'].max(),'i.e. Max number of Hours  spent:',round(df['Total day minutes'].max()\/60))\nprint('Maximum number of minutes:',df['Total day minutes'].min())\nprint('Average number of minutes:',df['Total day minutes'].mean())","8d384078":"import matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline\n\n\nplt.show()\ndf.boxplot(column='Total day minutes', by='Churn')\n","79c07975":"print('Maximum number of calls:',df['Total day calls'].max())\nprint('Minimum number of calls:',df['Total day calls'].min())\nprint('Average number of calls:',df['Total day calls'].mean())","b641dcf4":"\nplt.show()\ndf.boxplot(column='Total day calls', by='Churn')\n","6c251c65":"print('Maximum number of charge:',df['Total day charge'].max())\nprint('Minimum number of charge:',df['Total day charge'].min())\nprint('Average number of charge:',df['Total day charge'].mean())","17c87e50":"plt.show()\ndf.boxplot(column='Total day charge', by='Churn')\n","b14676e1":"print('Maximum number of minutes:',df['Total eve minutes'].max(),'i.e. Max number of Hours  spent:',round(df['Total eve minutes'].max()\/60))\nprint('Maximum number of minutes:',df['Total eve minutes'].min())\nprint('Average number of minutes:',df['Total eve minutes'].mean())","e7f9b590":"plt.show()\ndf.boxplot(column='Total eve minutes', by='Churn')\n","b5ef049a":"print('Maximum number of calls:',df['Total eve calls'].max())\nprint('Minimum number of calls:',df['Total eve calls'].min())\nprint('Average number of calls:',df['Total eve calls'].mean())","3e47fac8":"\nplt.show()\ndf.boxplot(column='Total eve calls', by='Churn')\n","b019282e":"print('Maximum number of charge:',df['Total eve charge'].max())\nprint('Minimum number of charge:',df['Total eve charge'].min())\nprint('Average number of charge:',df['Total eve charge'].mean())","3f300095":"plt.show()\ndf.boxplot(column='Total eve charge', by='Churn')\n","0839648e":"print('Maximum number of minutes:',df['Total night minutes'].max(),'i.e. Max number of Hours  spent:',round(df['Total night minutes'].max()\/60))\nprint('Minimum number of minutes:',df['Total night minutes'].min())\nprint('Average number of minutes:',df['Total night minutes'].mean())","a65d16c0":"plt.show()\ndf.boxplot(column='Total night minutes', by='Churn')","8c0dd34e":"print('Maximum number of calls:',df['Total night calls'].max())\nprint('Minimum number of calls:',df['Total night calls'].min())\nprint('Average number of calls:',df['Total night calls'].mean())","4c8a86bd":"plt.show()\ndf.boxplot(column='Total night calls', by='Churn')","54bad737":"print('Maximum number of charge:',df['Total night charge'].max())\nprint('Minimum number of charge:',df['Total night charge'].min())\nprint('Average number of charge:',df['Total night charge'].mean())","dd70e1a6":"plt.show()\ndf.boxplot(column='Total night charge', by='Churn')","712251e6":"print('Maximum number of minutes:',df['Total intl minutes'].max(),'i.e. Max number of Hours  spent:',round(df['Total intl minutes'].max()\/60))\nprint('Minimum number of minutes:',df['Total intl minutes'].min())\nprint('Average number of minutes:',df['Total intl minutes'].mean())","0e7b9f72":"plt.show()\ndf.boxplot(column='Total intl minutes', by='Churn')\n","c09411bd":"print('Maximum number of calls:',df['Total intl calls'].max())\nprint('Minimum number of calls:',df['Total intl calls'].min())\nprint('Average number of calls:',df['Total intl calls'].mean())","cb0be12c":"plt.show()\ndf.boxplot(column='Total intl calls', by='Churn')\n","63bce068":"print('Maximum number of charge:',df['Total intl charge'].max())\nprint('Minimum number of charge:',df['Total intl charge'].min())\nprint('Average number of charge:',df['Total intl charge'].mean())","21ffe70b":"plt.show()\ndf.boxplot(column='Total intl charge', by='Churn')","61acb073":"# By Age\nfig, axz = plt.subplots(figsize=(20,15))\n\naxz = sns.countplot(x='State', hue='Churn', data=df, palette='Reds')\n\n\naxz.set_ylabel('COUNTS', rotation=0, labelpad=100,size=20)\naxz.set_xlabel('State', size=20)\naxz.yaxis.set_label_coords(0.05, 0.95)  # (x, y)\naxz.legend(loc=0,fontsize=20);\n\naxz.tick_params(labelsize=15)  # Changes size of the values on the label","5ad5c3da":"# Customer Calls\nfig, axz = plt.subplots(figsize=(20,15))\n\naxz = sns.countplot(x='Customer service calls', hue='Churn', data=df, palette='Reds')\n\n\naxz.set_ylabel('COUNTS', rotation=0, labelpad=100,size=20)\naxz.set_xlabel('Customer Service Calls', size=20)\naxz.yaxis.set_label_coords(-0.05, 0.95)  # (x, y)\naxz.legend(loc=0,fontsize=20);\n\naxz.tick_params(labelsize=15)  # Changes size of the values on the label","34e02c7e":"df['Churn'] = df['Churn'].replace({bool(True):1,bool(False):0})","37b69ec1":"# Freq distribution of all data\nfig, ax = plt.subplots(figsize=(15,15))\npd.DataFrame.hist(df,ax=ax)\nplt.tight_layout();","3ccb9eef":"day_df = df[['Total day minutes','Total day calls','Total day minutes','Total day charge','Churn']]\neve_df = df[['Total eve minutes','Total eve calls','Total eve minutes','Total eve charge','Churn']]\nnight_df = df[['Total night minutes','Total night calls','Total night minutes','Total night charge','Churn']]\nintl_df = df[['Total intl minutes','Total intl calls','Total intl minutes','Total intl charge','Churn']]","df6a71b0":"df.columns","facd69cf":"# Now that we have our features, let's plot them on a correlation matrix to remove anything that might \n# cause multi-colinearity within our model\n\nsns.set(style=\"white\")\n# Creating the data\ndata = df.corr()\n\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(data, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n\n# Set up the matplotlib figure to control size of heatmap\nfig, ax = plt.subplots(figsize=(60,50))\n\n\n# Create a custom color palette\ncmap = \\\nsns.diverging_palette(133, 10,\n                      as_cmap=True)  \n# as_cmap returns a matplotlib colormap object rather than a list of colors\n# Green = Good (low correlation), Red = Bad (high correlation) between the independent variables\n\n# Plot the heatmap\ng = sns.heatmap(data=data, annot=True, cmap=cmap, ax=ax, \n                mask=mask, # Splits heatmap into a triangle\n                annot_kws={\"size\":20},  #Annotation size\n               cbar_kws={\"shrink\": 0.8} # Color bar size\n               );\n\n\n# Prevent Heatmap Cut-Off Issue\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)\n\n# Changes size of the values on the label\nax.tick_params(labelsize=25) \n\nax.set_yticklabels(g.get_yticklabels(), rotation=0);\nax.set_xticklabels(g.get_xticklabels(), rotation=80);\n\n","00337dff":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report\nfrom sklearn.metrics import roc_auc_score,roc_curve,scorer\nfrom sklearn.metrics import f1_score\nimport statsmodels.api as sm\nfrom sklearn.metrics import precision_score,recall_score\nfrom yellowbrick.classifier import DiscriminationThreshold\n#splitting train and test data \ntrain,test = train_test_split(df,test_size = .25 ,random_state = 111)\nX = df.drop(['State','Churn'],axis=1)\ntarget_col = ['Churn']\n##seperating dependent and independent variables\ncols    = X.columns\ntrain_X = train[cols]\ntrain_Y = train[target_col]\ntest_X  = test[cols]\ntest_Y  = test[target_col]\n\n#Function attributes\n#dataframe     - processed dataframe\n#Algorithm     - Algorithm used \n#training_x    - predictor variables dataframe(training)\n#testing_x     - predictor variables dataframe(testing)\n#training_y    - target variable(training)\n#training_y    - target variable(testing)\n#cf - [\"coefficients\",\"features\"](cooefficients for logistic \n                                 #regression,features for tree based models)\n\n#threshold_plot - if True returns threshold plot for model\n    \ndef telecom_churn_prediction(algorithm,training_x,testing_x,\n                             training_y,testing_y,cols,cf,threshold_plot) :\n    \n    #model\n    algorithm.fit(training_x,training_y)\n    predictions   = algorithm.predict(testing_x)\n    probabilities = algorithm.predict_proba(testing_x)\n    #coeffs\n    if   cf == \"coefficients\" :\n        coefficients  = pd.DataFrame(algorithm.coef_.ravel())\n    elif cf == \"features\" :\n        coefficients  = pd.DataFrame(algorithm.feature_importances_)\n        \n    column_df     = pd.DataFrame(cols)\n    coef_sumry    = (pd.merge(coefficients,column_df,left_index= True,\n                              right_index= True, how = \"left\"))\n    coef_sumry.columns = [\"coefficients\",\"features\"]\n    coef_sumry    = coef_sumry.sort_values(by = \"coefficients\",ascending = False)\n    \n    print (algorithm)\n    print (\"\\n Classification report : \\n\",classification_report(testing_y,predictions))\n    print (\"Accuracy   Score : \",accuracy_score(testing_y,predictions))\n    #confusion matrix\n    conf_matrix = confusion_matrix(testing_y,predictions)\n    #roc_auc_score\n    model_roc_auc = roc_auc_score(testing_y,predictions) \n    print (\"Area under curve : \",model_roc_auc,\"\\n\")\n    fpr,tpr,thresholds = roc_curve(testing_y,probabilities[:,1])\n    \n    #plot confusion matrix\n    trace1 = go.Heatmap(z = conf_matrix ,\n                        x = [\"Not churn\",\"Churn\"],\n                        y = [\"Not churn\",\"Churn\"],\n                        showscale  = False,colorscale = \"Picnic\",\n                        name = \"matrix\")\n    \n    #plot roc curve\n    trace2 = go.Scatter(x = fpr,y = tpr,\n                        name = \"Roc : \" + str(model_roc_auc),\n                        line = dict(color = ('rgb(22, 96, 167)'),width = 2))\n    trace3 = go.Scatter(x = [0,1],y=[0,1],\n                        line = dict(color = ('rgb(205, 12, 24)'),width = 2,\n                        dash = 'dot'))\n    \n    #plot coeffs\n    trace4 = go.Bar(x = coef_sumry[\"features\"],y = coef_sumry[\"coefficients\"],\n                    name = \"coefficients\",\n                    marker = dict(color = coef_sumry[\"coefficients\"],\n                                  colorscale = \"Picnic\",\n                                  line = dict(width = .6,color = \"black\")))\n    \n    #subplots\n    fig = tls.make_subplots(rows=2, cols=2, specs=[[{}, {}], [{'colspan': 2}, None]],\n                            subplot_titles=('Confusion Matrix',\n                                            'Receiver operating characteristic',\n                                            'Feature Importances'))\n    \n    fig.append_trace(trace1,1,1)\n    fig.append_trace(trace2,1,2)\n    fig.append_trace(trace3,1,2)\n    fig.append_trace(trace4,2,1)\n    \n    fig['layout'].update(showlegend=False, title=\"Model performance\" ,\n                         autosize = False,height = 900,width = 800,\n                         plot_bgcolor = 'rgba(240,240,240, 0.95)',\n                         paper_bgcolor = 'rgba(240,240,240, 0.95)',\n                         margin = dict(b = 195))\n    fig[\"layout\"][\"xaxis2\"].update(dict(title = \"false positive rate\"))\n    fig[\"layout\"][\"yaxis2\"].update(dict(title = \"true positive rate\"))\n    fig[\"layout\"][\"xaxis3\"].update(dict(showgrid = True,tickfont = dict(size = 10),\n                                        tickangle = 90))\n    py.iplot(fig)\n    \n    if threshold_plot == True : \n        visualizer = DiscriminationThreshold(algorithm)\n        visualizer.fit(training_x,training_y)\n        visualizer.poof()\n        \nlogit  = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False)\n\ntelecom_churn_prediction(logit,train_X,test_X,train_Y,test_Y,\n                         cols,\"coefficients\",threshold_plot = True)\n\n","fa52823b":"from imblearn.over_sampling import SMOTE\n\n\nsmote_X = df[cols]\nsmote_Y = df[target_col]\n\n#Split train and test data\nsmote_train_X,smote_test_X,smote_train_Y,smote_test_Y = train_test_split(smote_X,smote_Y,\n                                                                         test_size = .25 ,\n                                                                         random_state = 111)\n\n#oversampling minority class using smote\nos = SMOTE(random_state = 0)\nos_smote_X,os_smote_Y = os.fit_sample(smote_train_X,smote_train_Y)\nos_smote_X = pd.DataFrame(data = os_smote_X,columns=cols)\nos_smote_Y = pd.DataFrame(data = os_smote_Y,columns=target_col)\n###\n\n\n\nlogit_smote = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False)\n\ntelecom_churn_prediction(logit_smote,os_smote_X,test_X,os_smote_Y,test_Y,\n                         cols,\"coefficients\",threshold_plot = True)","ef77a2aa":"from sklearn.feature_selection import RFE\n\nlogit = LogisticRegression()\n\nrfe = RFE(logit,10)\nrfe = rfe.fit(os_smote_X,os_smote_Y.values.ravel())\n\nrfe.support_\nrfe.ranking_\n\n#identified columns Recursive Feature Elimination\nidc_rfe = pd.DataFrame({\"rfe_support\" :rfe.support_,\n                       \"columns\" : X.columns,\n                       \"ranking\" : rfe.ranking_,\n                      })\ncols = idc_rfe[idc_rfe[\"rfe_support\"] == True][\"columns\"].tolist()\n\n\n#separating train and test data\ntrain_rf_X = os_smote_X[cols]\ntrain_rf_Y = os_smote_Y\ntest_rf_X  = test[cols]\ntest_rf_Y  = test[target_col]\n\nlogit_rfe = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False)\n#applying model\ntelecom_churn_prediction(logit_rfe,train_rf_X,test_rf_X,train_rf_Y,test_rf_Y,\n                         cols,\"coefficients\",threshold_plot = True)\n\ntab_rk = ff.create_table(idc_rfe)\npy.iplot(tab_rk)","c4e7511f":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n           weights='uniform')\n\ngnb = GaussianNB(priors=None)\nsvc_lin  = SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n               decision_function_shape='ovr', degree=3, gamma=1.0, kernel='linear',\n               max_iter=-1, probability=True, random_state=None, shrinking=True,\n               tol=0.001, verbose=False)\nsvc_rbf  = SVC(C=1.0, kernel='rbf', \n               degree= 3, gamma=1.0, \n               coef0=0.0, shrinking=True,\n               probability=True,tol=0.001,\n               cache_size=200, class_weight=None,\n               verbose=False,max_iter= -1,\n               random_state=None)\nfrom lightgbm import LGBMClassifier\n\nlgbm_c = LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n                        learning_rate=0.5, max_depth=7, min_child_samples=20,\n                        min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n                        n_jobs=-1, num_leaves=500, objective='binary', random_state=None,\n                        reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n                        subsample_for_bin=200000, subsample_freq=0)\nfrom xgboost import XGBClassifier\n\nxgc = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n                    colsample_bytree=1, gamma=0, learning_rate=0.9, max_delta_step=0,\n                    max_depth = 7, min_child_weight=1, missing=None, n_estimators=100,\n                    n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n                    reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n                    silent=True, subsample=1)\n#gives model report in dataframe\ndef model_report(model,training_x,testing_x,training_y,testing_y,name) :\n    model.fit(training_x,training_y)\n    predictions  = model.predict(testing_x)\n    accuracy     = accuracy_score(testing_y,predictions)\n    recallscore  = recall_score(testing_y,predictions)\n    precision    = precision_score(testing_y,predictions)\n    roc_auc      = roc_auc_score(testing_y,predictions)\n    f1score      = f1_score(testing_y,predictions) \n    kappa_metric = cohen_kappa_score(testing_y,predictions)\n    \n    df = pd.DataFrame({\"Model\"           : [name],\n                       \"Accuracy_score\"  : [accuracy],\n                       \"Recall_score\"    : [recallscore],\n                       \"Precision\"       : [precision],\n                       \"f1_score\"        : [f1score],\n                       \"Area_under_curve\": [roc_auc],\n                       \"Kappa_metric\"    : [kappa_metric],\n                      })\n    return df\n\n#outputs for every model\nmodel1 = model_report(logit,train_X,test_X,train_Y,test_Y,\n                      \"Logistic Regression(Baseline_model)\")\nmodel2 = model_report(logit_smote,os_smote_X,test_X,os_smote_Y,test_Y,\n                      \"Logistic Regression(SMOTE)\")\nmodel3 = model_report(logit_rfe,train_rf_X,test_rf_X,train_rf_Y,test_rf_Y,\n                      \"Logistic Regression(RFE)\")\ndecision_tree = DecisionTreeClassifier(max_depth = 9,\n                                       random_state = 123,\n                                       splitter  = \"best\",\n                                       criterion = \"gini\",\n                                      )\nmodel4 = model_report(decision_tree,train_X,test_X,train_Y,test_Y,\n                      \"Decision Tree\")\nmodel5 = model_report(knn,os_smote_X,test_X,os_smote_Y,test_Y,\n                      \"KNN Classifier\")\nrfc = RandomForestClassifier(n_estimators = 1000,\n                             random_state = 123,\n                             max_depth = 9,\n                             criterion = \"gini\")\nmodel6 = model_report(rfc,train_X,test_X,train_Y,test_Y,\n                      \"Random Forest Classifier\")\nmodel7 = model_report(gnb,os_smote_X,test_X,os_smote_Y,test_Y,\n                      \"Naive Bayes\")\nmodel8 = model_report(svc_lin,os_smote_X,test_X,os_smote_Y,test_Y,\n                      \"SVM Classifier Linear\")\nmodel9 = model_report(svc_rbf,os_smote_X,test_X,os_smote_Y,test_Y,\n                      \"SVM Classifier RBF\")\nmodel10 = model_report(lgbm_c,os_smote_X,test_X,os_smote_Y,test_Y,\n                      \"LGBM Classifier\")\nmodel11 = model_report(xgc,os_smote_X,test_X,os_smote_Y,test_Y,\n                      \"XGBoost Classifier\")\n\n#concat all models\nmodel_performances = pd.concat([model1,model2,model3,\n                                model4,model5,model6,\n                                model7,model8,model9,\n                                model10,model11],axis = 0).reset_index()\n\nmodel_performances = model_performances.drop(columns = \"index\",axis =1)\n\ntable  = ff.create_table(np.round(model_performances,4))\n\npy.iplot(table)","89fa9459":"model_performances\ndef output_tracer(metric,color) :\n    tracer = go.Bar(y = model_performances[\"Model\"] ,\n                    x = model_performances[metric],\n                    orientation = \"h\",name = metric ,\n                    marker = dict(line = dict(width =.7),\n                                  color = color)\n                   )\n    return tracer\n\nlayout = go.Layout(dict(title = \"Model performances\",\n                        plot_bgcolor  = \"rgb(243,243,243)\",\n                        paper_bgcolor = \"rgb(243,243,243)\",\n                        xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     title = \"metric\",\n                                     zerolinewidth=1,\n                                     ticklen=5,gridwidth=2),\n                        yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     zerolinewidth=1,ticklen=5,gridwidth=2),\n                        margin = dict(l = 250),\n                        height = 780\n                       )\n                  )\n\n\ntrace1  = output_tracer(\"Accuracy_score\",\"#6699FF\")\ntrace2  = output_tracer('Recall_score',\"red\")\ntrace3  = output_tracer('Precision',\"#33CC99\")\ntrace4  = output_tracer('f1_score',\"lightgrey\")\ntrace5  = output_tracer('Kappa_metric',\"#FFCC99\")\n\ndata = [trace1,trace2,trace3,trace4,trace5]\nfig = go.Figure(data=data,layout=layout)\npy.iplot(fig)","a01d9a64":"lst    = [logit,logit_smote,decision_tree,knn,rfc,\n          gnb,svc_lin,svc_rbf,lgbm_c,xgc]\n\nlength = len(lst)\n\nmods   = ['Logistic Regression(Baseline_model)','Logistic Regression(SMOTE)',\n          'Decision Tree','KNN Classifier','Random Forest Classifier',\"Naive Bayes\",\n          'SVM Classifier Linear','SVM Classifier RBF', 'LGBM Classifier',\n          'XGBoost Classifier']\n\nfig = plt.figure(figsize=(13,15))\nfig.set_facecolor(\"#F3F3F3\")\nfor i,j,k in itertools.zip_longest(lst,range(length),mods) :\n    plt.subplot(4,3,j+1)\n    predictions = i.predict(test_X)\n    conf_matrix = confusion_matrix(predictions,test_Y)\n    sns.heatmap(conf_matrix,annot=True,fmt = \"d\",square = True,\n                xticklabels=[\"not churn\",\"churn\"],\n                yticklabels=[\"not churn\",\"churn\"],\n                linewidths = 2,linecolor = \"w\",cmap = \"Set1\")\n    plt.title(k,color = \"b\")\n    plt.subplots_adjust(wspace = .3,hspace = .3)","8e5b5061":"lst    = [logit,logit_smote,decision_tree,knn,rfc,\n          gnb,svc_lin,svc_rbf,lgbm_c,xgc]\n\nlength = len(lst)\n\nmods   = ['Logistic Regression(Baseline_model)','Logistic Regression(SMOTE)',\n          'Decision Tree','KNN Classifier','Random Forest Classifier',\"Naive Bayes\",\n          'SVM Classifier Linear','SVM Classifier RBF', 'LGBM Classifier',\n          'XGBoost Classifier']\n\nplt.style.use(\"dark_background\")\nfig = plt.figure(figsize=(12,16))\nfig.set_facecolor(\"#F3F3F3\")\nfor i,j,k in itertools.zip_longest(lst,range(length),mods) :\n    qx = plt.subplot(4,3,j+1)\n    probabilities = i.predict_proba(test_X)\n    predictions   = i.predict(test_X)\n    fpr,tpr,thresholds = roc_curve(test_Y,probabilities[:,1])\n    plt.plot(fpr,tpr,linestyle = \"dotted\",\n             color = \"royalblue\",linewidth = 2,\n             label = \"AUC = \" + str(np.around(roc_auc_score(test_Y,predictions),3)))\n    plt.plot([0,1],[0,1],linestyle = \"dashed\",\n             color = \"orangered\",linewidth = 1.5)\n    plt.fill_between(fpr,tpr,alpha = .4)\n    plt.fill_between([0,1],[0,1],color = \"k\")\n    plt.legend(loc = \"lower right\",\n               prop = {\"size\" : 12})\n    qx.set_facecolor(\"k\")\n    plt.grid(True,alpha = .15)\n    plt.title(k,color = \"b\")\n    plt.xticks(np.arange(0,1,.3))\n    plt.yticks(np.arange(0,1,.3))","59eeff30":"from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\n\n\nlst    = [logit,logit_smote,decision_tree,knn,rfc,\n          gnb,svc_lin,svc_rbf,lgbm_c,xgc]\n\nlength = len(lst)\n\nmods   = ['Logistic Regression(Baseline_model)','Logistic Regression(SMOTE)',\n          'Decision Tree','KNN Classifier','Random Forest Classifier',\"Naive Bayes\",\n          'SVM Classifier Linear','SVM Classifier RBF', 'LGBM Classifier',\n          'XGBoost Classifier']\n\nfig = plt.figure(figsize=(13,17))\nfig.set_facecolor(\"#F3F3F3\")\nfor i,j,k in itertools.zip_longest(lst,range(length),mods) :\n    \n    qx = plt.subplot(4,3,j+1)\n    probabilities = i.predict_proba(test_X)\n    predictions   = i.predict(test_X)\n    recall,precision,thresholds = precision_recall_curve(test_Y,probabilities[:,1])\n    plt.plot(recall,precision,linewidth = 1.5,\n             label = (\"avg_pcn : \" + \n                      str(np.around(average_precision_score(test_Y,predictions),3))))\n    plt.plot([0,1],[0,0],linestyle = \"dashed\")\n    plt.fill_between(recall,precision,alpha = .2)\n    plt.legend(loc = \"lower left\",\n               prop = {\"size\" : 10})\n    qx.set_facecolor(\"k\")\n    plt.grid(True,alpha = .15)\n    plt.title(k,color = \"b\")\n    plt.xlabel(\"recall\",fontsize =7)\n    plt.ylabel(\"precision\",fontsize =7)\n    plt.xlim([0.25,1])\n    plt.yticks(np.arange(0,1,.3))","674c0f06":"Average charge is around 30$ which is  a decent pricing strategy!","628b1078":"#### In Night, The Loyal Customers are spend more time is what the box-plot indicates!","deed18bd":"#### As Dataset is Highly-Imbalance we would like to Over Sample and check results","b57c8bb7":"#### Still Calls are made more by the churned customers!!","f7ffaa12":"## 1.5 Evening time Affecting the Churn Rate!!","9ce69024":"#### XGBoost is the best ","bc0adf08":"## Texas,Maryland have the bit more churn rate than usual, A Network Upgradation would be strongly suggested in these areas!.","ba19f667":"## We can Predict using:\n<b><li> If we want to predict churn rate  correctly, then Tree based classification using SMOTE would be recommended.\n <b><li> XGBoost Classifier Performs best and would be a recommended Model.\n","9c63eeb7":"# 6. Model Performances\n## 6.1. model performance metrics<\/a>","93132a9e":"### SMOTE","fc72898d":"## 1.8 Data Correlation","b6435318":"## 1.7 International Calls Affecting the Churn Rate ","f27e3172":"# Real-World Project: Orange Telecom Churn Prevention and Prediction\n### The Orange Telecom's Churn Dataset, which consists of cleaned customer activity data (features), along with a churn label specifying whether a customer canceled the subscription, will be used to develop predictive models.","eda91628":"#### These Calls clearly indicat that clients without International Plan Suffer and May Leave the Operator.","7aa2aca3":"#### While some customers are lazy and hence without resolving the issue they have jumped to other network operator,while the customers who have called once also have high churn rate indicating their issue was not solved in first attempt.\n<li><b>A Feedback is neccesary in such situations.\n<li><b> It should given a Confirmation to the Customer that there issue would be solved in first attempt","bbe03b34":"### SMOTE and RFE and Logistic Regression","32ac74dc":"#### Well,Here we can clearly indicate a strategy a good strategy to be implemented. As from above infered box-plots we can conclude one thing i.e Customers having more minutes spent on the network tend to leave the it's subscription and from the above box-plot it clearly indicates that there is defect in the pricing startegy of the company.\n<b> According to my Hypothsis:\n    <li>1.Startegy of pricing needs to be re-evaluated.\n    <li> 2. The Clients who have high call minutes and calls need a discount in the end.\n       ","9e41da99":"#### We can infer from above box-plot that with users spending more 225 minutes or more i.e. approx 4hrs tend to switch to other operator.\n<b>According to my hypothesis, following would be the factors that should be implemented:\n    <li>1.Network Disturbance during a Call\n    <li>2.Cracking sound or noise during a call\n    <li>3.Need to Upgrade or make smarter use of technologies like VoLTE to improve Voice Quality.\n    <li>4. Network Upgradation","615d770c":"#### We can infere here that on an average a 100 calls are made which is a good indication for the company.But we can also note that for the churn customer the median is slightly higher than 100 which indicates there are call drops which may lead to more calls in a morning.","d85361d5":"## Further for Improvement we can suggest and discuss more strategies to the company by collecting other data and through a domain expert!","416dd618":"## 1.4 Checking Voice-Mail Feature","4629aca0":"## Checking the Scores with other Models and Choosing the Best One!","0364bdbf":"## Precision Recall Curves","d5065bd0":"### Different Pricing Strategy and International Calling Rate Optimization would lead to lower churn rate","e51a7a42":"#### This is a Highly-Imbalanced Dataset,Hence we need to use SMOTE techniques for such a data.","7ca8e54b":"## Evaluating ROC Metric","f3d7142c":"#### Applying SMOTE and Logistic Regression using RFE has good impact and gives us the ranking","80969fa2":"## 1.1 Data Exploration","2f6bef58":"#### We can Notice for Voice-Mail Feature when there are more than 20 voice-mail messages then certainly there is a churn indicating improving the voice-mail feature or setting a limit and check whether a customer is retianed.\n#### According to my hypothesis :\n  ***1.Voice-Mail Service Upgradation\n  2.Setting up a limit on Voice-Mail service strictly no more than 25 voice mails.\n  3.Quality Drop in Voice-Mail after 25 voice mails.****","7f687846":"## 1.5  Total-Minutes in Morning Affecting the Churn Rate","f0d46e54":"# Conclusions\n## We can definitely suggest to prevent churn :\n<b><li> Upgrading network to improve services for long duration users.\n  <b><li> Updating Pricing Strategies.\n    <b><li>Updating and Optimizing Internationall Call Rates.\n     <b><li> Implmenting a better network infrastructure in Maryland and Texas Areas where there is more Churn Rate.\n      <b><li>Upgrading their services when in emegenvy only in evening period as low network traffic.<b>\n","9e0949dc":"#### Clearly SMOTE over sampling increase the recall which is our main metric to correctly predict the churned customer","069d3dec":"## Plotting Confusion Matrix of all models","4447175a":"## 1.6 Night Time Affecting Churn Rate !!","7569aaca":"#### Well no doubt, Boosting and Tree Based Perform Better and XGBoost is the best model to conclude it let's evaluate with other metrics","0583bec8":"## 1.9 Training Our Model using Logistic Regression","8bc0db33":"#### Again Optimization of The Charges would lead to a loyal customer!!!","d2bd32e7":"# Prediction","c7757ad2":"## 1.8 Churn According to States","7cd64106":"#### Users who make the International Call tend to spend more minutes.","ee9ecac5":"#### Well we want our model to predict churn customers correctly and XGBoost is exceptional to our metric evaluation","893f25ce":"## 1.2 Variable Breakdown\n  \n**STATE**: 51 Unique States in United States of America\n  \n**Account Length**. Length of The Account\n\n**Area Code** 415 relates to San Francisco,408 is  of San Jose and 510 is of City of Okland \n\n**International Plan**  Yes Indicate International Plan is Present and No Indicates no subscription for Internatinal Plan\n\n**Voice Mail Plan**  Yes Indicates Voice Mail  Plan is Present and No Indicates no subscription for Voice Mail Plan\n\n**Number vmail messages** Number of Voice Mail Messages ranging from 0 to 50\n\n**Total day minutes**  Total Number of Minutes Spent By Customers in Morning\n\n**Total  day calls** Total Number of Calls made by Customer in Morning.\n\n**Total day charge** Total Charge to the Customers in Morning.\n\n**Total eve minutes**Total Number of Minutes Spent By Customers in Evening\n\n**Total eve calls** Total Number of Calls made by Customer in Evening.\n\n**Total eve charge**  Total Charge to the Customers in Morning.\n\n**Total night minutes**  Total Number of Minutes Spent By Customers in the Night.\n\n**Total night calls**   Total Number of Calls made by Customer in Night.\n\n**Total night charge** Total Charge to the Customers in Night.","b047aa70":"## 1.3 Checking the Churn Rate"}}