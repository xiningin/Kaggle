{"cell_type":{"9c7d0985":"code","175b9695":"code","3220e6ef":"code","d4788947":"code","9516fccc":"code","e0f009ae":"code","faccff0f":"code","20ac25e2":"code","9cc3b6ca":"code","0e4157ce":"code","81ff3888":"code","fff5b53d":"code","23b25783":"code","8d23da00":"code","eb745c8b":"code","4b66dfbe":"code","8ba65fbd":"code","1b17fd96":"code","36815f98":"code","138d7f19":"code","860648b1":"code","38390191":"code","402fff96":"code","199d0b2f":"code","42f60cc8":"code","f5bafdf0":"code","2d83bd00":"code","8dd05eeb":"code","7c7d3916":"code","d519eadd":"code","c7059a84":"code","11eb24f0":"code","79d651f2":"code","7d8ecc58":"code","d69e0e52":"code","f0654cb9":"code","6a083342":"code","de478a42":"code","eb5a972c":"code","278c8c45":"markdown","7a319df8":"markdown","273683de":"markdown","7dfa2969":"markdown","84ab2d78":"markdown","3748715f":"markdown","b39a79d5":"markdown","ddecf781":"markdown","d26f7e9a":"markdown","9d8f6a90":"markdown","49a0e640":"markdown","048f31b5":"markdown","62763b06":"markdown","96016f9b":"markdown","4c269009":"markdown","b3b758ec":"markdown","d81f6e12":"markdown","759d6414":"markdown","60bf76ae":"markdown","3be47067":"markdown","c8f4d01f":"markdown","94a6cd16":"markdown","70b47b05":"markdown","5633f188":"markdown","70eca9b0":"markdown","fddb3f66":"markdown"},"source":{"9c7d0985":"import pandas as pd\nimport numpy as np\nimport missingno\nfrom collections import Counter","175b9695":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","3220e6ef":"from sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom catboost import CatBoostClassifier","d4788947":"from sklearn.model_selection import cross_val_score","9516fccc":"from sklearn.model_selection import GridSearchCV","e0f009ae":"import warnings\nwarnings.filterwarnings('ignore')","faccff0f":"my_train_data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\nmy_test_data = pd.read_csv(\"..\/input\/titanic\/test.csv\")\nmy_submission=pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")","20ac25e2":"my_train_data.describe(include=\"all\")","9cc3b6ca":"my_train_data.describe(include=\"all\")","0e4157ce":"my_train_data.info()\nprint('-'*40)\nmy_test_data.info()","81ff3888":"print('my_train_data is :', my_train_data.shape)\nprint(' '*27)\nprint('my_test_data is :', my_test_data.shape)","fff5b53d":"# Missing data in training set\n\nmissingno.matrix(my_train_data)","23b25783":"# Missing data in test set \n\nmissingno.matrix(my_test_data)","8d23da00":"my_submission.head()","eb745c8b":"my_submission.shape","4b66dfbe":"def detect_outliers(df, n, features):\n   \n    outlier_indices = [] \n    for col in features: \n        Q1 = np.percentile(df[col], 25)\n        Q3 = np.percentile(df[col], 75)\n        IQR = Q3 - Q1\n        outlier_step = 1.5 * IQR \n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step)].index\n        outlier_indices.extend(outlier_list_col) \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(key for key, value in outlier_indices.items() if value > n) \n    return multiple_outliers\n\noutliers_to_drop = detect_outliers(my_train_data, 2, ['Age', 'SibSp', 'Parch', 'Fare'])\nprint(\"We will drop these {} indices: \".format(len(outliers_to_drop)), outliers_to_drop)","8ba65fbd":"my_train_data.loc[outliers_to_drop, :]","1b17fd96":"print(\"Before: {} rows\".format(len(my_train_data)))\nmy_train_data = my_train_data.drop(outliers_to_drop, axis = 0).reset_index(drop = True)\nprint(\"After: {} rows\".format(len(my_train_data)))","36815f98":"sns.heatmap(my_train_data[['Survived', 'SibSp', 'Parch', 'Age', 'Fare']].corr(), annot = True, fmt = '.2f', cmap = 'coolwarm')","138d7f19":"# Value counts of the SibSp column \n\nmy_train_data['SibSp'].value_counts(dropna = False)","860648b1":"# Mean of survival by SibSp\n\nmy_train_data[['SibSp', 'Survived']].groupby('SibSp', as_index = False).mean().sort_values(by = 'Survived', ascending = False)","38390191":"sns.barplot(x = 'SibSp', y ='Survived', data = my_train_data)\nplt.ylabel('Survival Probability')\nplt.title('Survival Probability by SibSp')","402fff96":"# Value counts of the Parch column \n\nmy_train_data['Parch'].value_counts(dropna = False)","199d0b2f":"# Mean of survival by Parch\n\nmy_train_data[['Parch', 'Survived']].groupby('Parch', as_index = False).mean().sort_values(by = 'Survived', ascending = False)","42f60cc8":"sns.barplot(x = 'Parch', y ='Survived', data = my_train_data)\nplt.ylabel('Survival Probability')\nplt.title('Survival Probability by Parch')","f5bafdf0":"# Null values in Age column \n\nmy_train_data['Age'].isnull().sum()","2d83bd00":"# Passenger age distribution\n\nsns.distplot(my_train_data['Age'], label = 'Skewness: %.3f'%(my_train_data['Age'].skew()))\nplt.legend(loc = 'best')\nplt.title('Passenger Age Distribution')","8dd05eeb":"# Age distribution by survival\n\ng = sns.FacetGrid(my_train_data, col = 'Survived')\ng.map(sns.distplot, 'Age')","7c7d3916":"# Null values of Fare column \n\nmy_train_data['Fare'].isnull().sum()","d519eadd":"# Passenger fare distribution\n\nsns.distplot(my_train_data['Fare'], label = 'Skewness: %.2f'%(my_train_data['Fare'].skew()))\nplt.legend(loc = 'best')\nplt.ylabel('Passenger Fare Distribution')","c7059a84":"# Drop ticket and cabin features from training and test set\n\nmy_train_data = my_train_data.drop(['Ticket', 'Cabin'], axis = 1)\nmy_test_data = my_test_data.drop(['Ticket', 'Cabin'], axis = 1)","11eb24f0":"# Missing values in training set \n\nmy_train_data.isnull().sum().sort_values(ascending = False)","79d651f2":"# Compute the most frequent value of Embarked in training set\n\nmode = my_train_data['Embarked'].dropna().mode()[0]\nmode","7d8ecc58":"# Fill missing value in Embarked with mode\n\nmy_train_data['Embarked'].fillna(mode, inplace = True)","d69e0e52":"# Missing values in test set\n\nmy_test_data.isnull().sum().sort_values(ascending = False)","f0654cb9":"# Compute median of Fare in test set \n\nmedian = my_test_data['Fare'].dropna().median()\nmedian","6a083342":"# Fill missing value in Fare with median\n\nmy_test_data['Fare'].fillna(median, inplace = True)\n\n# Combine training set and test set\n\ncombine = pd.concat([my_train_data, my_test_data], axis = 0).reset_index(drop = True)\ncombine.head()","de478a42":"# Convert Sex into numerical values where 0 = male and 1 = female\n\ncombine['Sex'] = combine['Sex'].map({'male': 0, 'female': 1})\n\nsns.factorplot(y = 'Age', x = 'Sex', hue = 'Pclass', kind = 'box', data = combine)\nsns.factorplot(y = 'Age', x = 'Parch', kind = 'box', data = combine)\nsns.factorplot(y = 'Age', x = 'SibSp', kind = 'box', data = combine)","eb5a972c":"sns.heatmap(combine.drop(['Survived', 'Name', 'PassengerId', 'Fare'], axis = 1).corr(), annot = True, cmap = 'coolwarm')","278c8c45":"Our final dataframe that is to be submitted should look something like this: **418 rows and 2 columns, one for PassengerId and one for Survived.**","7a319df8":"# Nacessary Information:\n\n\n**Survival: Survival (0 = No; 1 = Yes)**\n\n**Pclass: Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)**\n\n**Name : Name**\n\n**Sex : Male or female**\n\n**Age : Age in years, fractional if less than 1**\n\n**Sibsp : Number of siblings or spouses aboard the titanic**\n\n**Parch : Number of parents or children aboard the titanic**\n\n**Ticket : Passenger ticket number**\n\n**Fare : Passenger Fare**\n\n**Cabin : Cabin Number**\n\n**Embarked : Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)**","273683de":"# 4-Data Visualization\n**Numerical variables correlation with survival**","7dfa2969":"They will be used ","84ab2d78":"#  Preprocessing  \n\n**Detect and remove outliers in numerical variables**\n\n**One month from now it will be complete.**\n","3748715f":"# 2-Read our training and testing data\n\n**Importing our CSV files**","b39a79d5":"# Numerical variable: Parch","ddecf781":"\nI have decided to drop both ticket and cabin for simplicity of this tutorial but if you have the time, I would recommend going through them and see if they can help improve your model.","d26f7e9a":"**1-3: Machine Learning Models**","9d8f6a90":"**1-5: Remove warnings**","49a0e640":"\n **This function will loop through a list of features and detect outliers in each one of those features**\n    \n1- In each loop, a data point is deemed an outlier if it is less than the first quartile minus the outlier step or exceeds\n    \n2- third quartile plus the outlier step. The outlier step is defined as 1.5 times the interquartile range. \n\n3- Once the outliers have been determined for one feature, their indices will be stored in a list before proceeding to the next feature and the process repeats until the very last feature is completed. \n  \n4- Finally, using the list with outlier indices, we will count the frequencies of the index numbers and return them if their frequency exceeds n times.    \n","048f31b5":"# 4.1 Drop and fill missing values","62763b06":"# Numerical variables\n\nNumerical variables in our dataset are: **SibSp, Parch, Age and Fare**\n\n\n**Detect and remove outliers in numerical variables:**\n\n* Outliers are data points that have extreme values and they do not conform with the majority of the data.\n* It is important to address this because outliers tend to skew our data towards extremes and can cause inaccurate model predictions.\n* I will use the Tukey method to remove these outliers.","96016f9b":"# Numerical variable: Fare","4c269009":"# 1) Import Necessary Libraries","b3b758ec":"# 3-Data Analysis\n\n# Exploratory Data Analysis (EDA)\n\n\n**whats up in our dataset?**\n\nExploratory data analysis is the process of visualising and analysing data to extract insights. In other words, we want to summarise important characteristics and trends in our data in order to gain a better understanding of our dataset.\n\nget a list of the features within the titanic dataset","d81f6e12":"# Numerical variable: Age","759d6414":"**1-5: Hyperparameter tuning**","60bf76ae":"#  4. Data preprocessing\n\n**Data preprocessing is the process of getting our dataset ready for model training. In this section, we will perform the following preprocessing steps:**\n\n- **Drop and fill missing values**\n- **Data trasformation (log transformation)**\n- **Feature engineering**\n- **Feature encoding**","3be47067":"**1-4: Model evaluation**","c8f4d01f":"**1-1: Data Analysis Libraries(Data wrangling)**","94a6cd16":"\n* Note that the test set has one column less than training set, the Survived column.\n\n* This is because Survived is our response variable, or sometimes called a target variable. \n\n* Our job is to analyse the data in the training set and predict the survival of the passengers in the test set.","70b47b05":"# Numerical variable: SibSp","5633f188":"# Contents\n\n1-Import Necessary Libraries\n\n2-Read In and Explore the Data(Numerical variables in our dataset are **SibSp, Parch, Age and Fare**)\n\n3-Data Visualization\n\n4-Data preprocessing\n","70eca9b0":"**Let's have a look at the datasets:**\n\nLooking training data by describe() and info()","fddb3f66":"**1-2: Visualization Libraries**"}}