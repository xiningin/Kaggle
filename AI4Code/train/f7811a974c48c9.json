{"cell_type":{"653afde8":"code","ac42c357":"code","d64dc28d":"code","2f549206":"code","8acfa532":"code","729d47f6":"code","3027ef9a":"code","55706f2a":"code","66ab73f1":"code","a57563de":"code","977ea367":"code","db368a33":"code","5e58c433":"code","a7a86545":"code","56e6e9f9":"code","ea6f3073":"code","44cfa26e":"code","1a096761":"code","970a15cd":"code","910f1e2b":"code","462c6d41":"code","1e1a6282":"code","87837f90":"code","787a4a37":"code","d18053c8":"code","226c922f":"code","123a1da3":"markdown","8049bf80":"markdown","f04b2a06":"markdown","87c81ec2":"markdown","eb594f7c":"markdown","bd4ef137":"markdown","cc6ad5c1":"markdown","c69e2fad":"markdown","c4e2c769":"markdown","bf4bff37":"markdown","9890be96":"markdown","b2c3fa27":"markdown","cd2f2ae8":"markdown","81f186a7":"markdown","281547c2":"markdown","9e3821ca":"markdown","9bc8205e":"markdown","abe3a40b":"markdown","01238223":"markdown","bdf22847":"markdown","a4f98760":"markdown","e37485e2":"markdown","f3f4673c":"markdown","a4d0c533":"markdown","d44bf9dd":"markdown","afe0f799":"markdown","a452483c":"markdown","b9ae7ead":"markdown","0149df7e":"markdown","8f846ace":"markdown","9317fc48":"markdown"},"source":{"653afde8":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom csv import QUOTE_NONE\nimport nltk\n\nfrom wordcloud import WordCloud, STOPWORDS\n\ncustom_stopwords = ['happy', 'birthday', 'hbd', 'brthday', 'thanks', 'thank']\nroman_urdu_stopwords = ['ai', 'ayi', 'hy', 'hai', 'main',\n                        'ki', 'tha', 'koi', 'ko', 'sy', 'woh', 'bhi', 'aur', 'wo', 'yeh', \n                        'rha', 'hota', 'ho', 'ga', 'ka', 'le', 'lye', 'kr', 'kar', 'lye', \n                        'liye', 'hotay', 'waisay', 'gya', 'gaya', 'kch', 'ab', 'thy', 'thay', \n                        'houn', 'hain', 'han', 'to', 'is', 'hi', 'jo', 'kya', 'thi', 'se', 'pe', \n                        'phr', 'wala', 'waisay', 'us', 'na', 'ny', 'hun', 'rha', 'raha', 'ja',\n                        'rahay', 'abi', 'uski', 'ne', 'haan', 'acha', 'nai', 'sent', 'photo', \n                        'you', 'kafi', 'gai', 'rhy', 'kuch', 'jata', 'aye', 'ya', 'dono', 'hoa', \n                        'aese', 'de', 'wohi', 'jati', 'jb', 'krta', 'lg', 'rahi', 'hui', 'karna', \n                        'krna', 'gi', 'hova', 'yehi', 'jana', 'jye', 'chal', 'mil', 'tu', 'hum', 'par', \n                        'hay', 'kis', 'sb', 'gy', 'dain', 'krny', 'tou', 'ha']\n","ac42c357":"def read_and_reformat(csv_path):\n    df = pd.read_csv(csv_path,\n                     dtype=object)\n    return df\n","d64dc28d":"import re\ndef cleaner_word(word):\n    word = re.sub(r'\\#\\.', '', word)\n    word = re.sub(r'\\n', '', word)\n    word = re.sub(r',', '', word)\n    word = re.sub(r'\\-', ' ', word)\n    word = re.sub(r'\\.', '', word)\n    word = re.sub(r'\\\\', ' ', word)\n    word = re.sub(r'\\\\x\\.+', '', word)\n    word = re.sub(r'\\d', '', word)\n    word = re.sub(r'^_.', '', word)\n    word = re.sub(r'_', ' ', word)\n    word = re.sub(r'^ ', '', word)\n    word = re.sub(r' $', '', word)\n    word = re.sub(r'\\?', '', word)\n    \n    if len(word) == 1:\n        word = ''\n    \n    return word.lower() ","2f549206":"def cleaner_sentence(sentence):\n    clean_sentence = ''\n    tokens = sentence.split()\n    for token in tokens:\n        token = cleaner_word(token)\n        clean_sentence += token + ' '\n    return clean_sentence","8acfa532":"def draw_top_ngrams(top, raw_text, title, color, ngram=2):\n    tokens = nltk.word_tokenize(raw_text)\n\n    tokens = [word for word in tokens if word not in STOPWORDS]\n    tokens = [word for word in tokens if word not in custom_stopwords]\n    tokens = [word for word in tokens if word not in roman_urdu_stopwords]\n    tokens = [word for word in tokens if len(word) > 2]\n\n    #Create your bigrams\n    \n\n    bgs = nltk.ngrams(tokens, n=ngram)\n    \n    #compute frequency distribution for all the bigrams in the text\n    fdist = nltk.FreqDist(bgs)\n    most_common = fdist.most_common(top)\n    common_bigrams = [(' '.join(x[0]), x[1]) for x in most_common]\n\n    itr = zip(*common_bigrams)\n    bigrams = next(itr)\n    frequency = next(itr)\n    x_pos = np.arange(len(frequency)) \n\n    plt.figure(num=None, figsize=(15, 7), dpi=80, facecolor='w', edgecolor='k')\n\n    plt.title(title + ' most common ngrams (n = ' + str(ngram) + ')')\n    bar = plt.bar(x_pos, frequency,align='center', color=color, edgecolor='black')\n    \n    for rect in bar:\n        height = rect.get_height()\n        plt.text(rect.get_x() + rect.get_width()\/2.0, height, '%d' % int(height), ha='center', va='bottom')\n\n    \n    plt.xticks(x_pos, bigrams, rotation=75) \n    plt.ylabel('Frequency')\n    plt.show()","729d47f6":"def get_by_year_data(df, only_count = False):\n    items_by_year = {}\n    item_lengths_by_year = {}\n    item_count_by_year = {}\n    \n    for i, row in df.iterrows():\n        if not only_count:\n            clean_sentence = cleaner_sentence(row['text'].lower())\n        try:\n            if not only_count:\n                items_by_year[get_year_from_timestamp(row['timestamp'])] += clean_sentence\n                item_lengths_by_year[get_year_from_timestamp(row['timestamp'])] += len(clean_sentence)\n            item_count_by_year[get_year_from_timestamp(row['timestamp'])] += 1\n        except:\n            if not only_count:\n                items_by_year[get_year_from_timestamp(row['timestamp'])] = clean_sentence\n                item_lengths_by_year[get_year_from_timestamp(row['timestamp'])] = len(clean_sentence)\n            item_count_by_year[get_year_from_timestamp(row['timestamp'])] = 1\n    \n    if not only_count:\n        return (items_by_year, item_lengths_by_year, item_count_by_year)\n    return item_count_by_year","3027ef9a":"def create_wordcloud(text, custom_stopwords):\n    for sw in custom_stopwords:\n        STOPWORDS.add(sw);\n    wordcloud = WordCloud(stopwords=STOPWORDS, background_color='black', width=800, height=400).generate(text)\n    plt.figure( figsize=(20,10))\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show()","55706f2a":"from datetime import datetime\ndef get_year_from_timestamp(ts):\n    ts = int(ts)\n    return datetime.utcfromtimestamp(ts).strftime('%Y')","66ab73f1":"def get_date_from_timestamp(ts, format='%Y-%m-%d'):\n    ts = int(ts)\n    return datetime.utcfromtimestamp(ts).strftime(format)","a57563de":"df = read_and_reformat('..\/input\/csvs\/csvs\/posts.csv')\ndf.head()","977ea367":"posts_by_year, posts_lengths_by_year, posts_count_by_year  = get_by_year_data(df)","db368a33":"for key in sorted(posts_by_year.keys()):\n    print(\"Year: \" + key)\n    create_wordcloud(posts_by_year[key], custom_stopwords + roman_urdu_stopwords)","5e58c433":"from collections import OrderedDict\nposts_count_by_year = OrderedDict(sorted(posts_count_by_year.items(), key=lambda t: t[0]))\n\nplt.figure(num=None, figsize=(15, 7), dpi=80, facecolor='w', edgecolor='k')\nbar = plt.bar(range(len(posts_count_by_year)), list(posts_count_by_year.values()), align='center', edgecolor='black')\nplt.xticks(range(len(posts_count_by_year)), list(posts_count_by_year.keys()))\nplt.ylabel('Number of Posts')\nplt.title('Number of Facebook posts by Year (including posts in Groups)')\n\nfor rect in bar:\n    height = rect.get_height()\n    plt.text(rect.get_x() + rect.get_width()\/2.0, height, '%d' % int(height), ha='center', va='bottom')\n\nplt.show()","a7a86545":"avg_post_length_by_year = {}\n\nfor key in sorted(posts_lengths_by_year.keys()):\n    avg_post_length_by_year[key] = round(posts_lengths_by_year[key] \/ posts_count_by_year[key], 2)\n\nplt.figure(num=None, figsize=(15, 7), dpi=80, facecolor='w', edgecolor='k')\nbar = plt.bar(range(len(avg_post_length_by_year)), list(avg_post_length_by_year.values()), align='center', color='#aa61a7', edgecolor='black')\nplt.xticks(range(len(avg_post_length_by_year)), list(avg_post_length_by_year.keys()))\nplt.ylabel('Average Length of Post (in words)')\nplt.title('Average length of Facebook posts by Year (including posts in Groups)')\n\nfor rect in bar:\n    height = rect.get_height()\n    plt.text(rect.get_x() + rect.get_width()\/2.0, height, '%d' % int(height), ha='center', va='bottom')\n\nplt.show()","56e6e9f9":"import numpy as np\n\nplt.figure(num=None, figsize=(15, 7), dpi=80, facecolor='w', edgecolor='k')\nax = plt.subplot(111)\nX = np.arange(len(posts_count_by_year))\nbar1 = ax.bar(X, posts_count_by_year.values(), width=0.3, color='y', align='center', edgecolor='black')\nbar2 = ax.bar(X-0.3, avg_post_length_by_year.values(), width=0.3, color='g', align='center', edgecolor='black')\nax.legend(('Post Count','Avg. post length (in words)'))\nplt.xticks(X, avg_post_length_by_year.keys())\nplt.title(\"Post Count vs Avg. post length by year\")\n\nfor rect in bar1 + bar2:\n    height = rect.get_height()\n    plt.text(rect.get_x() + rect.get_width()\/2.0, height, '%d' % int(height), ha='center', va='bottom')\n\n\nplt.show()","ea6f3073":"all_posts_text = ''\nfor posts in posts_by_year.values():\n    all_posts_text += posts + ' '\n    \ndraw_top_ngrams(raw_text=all_posts_text, top=30, title='All time', color='#e58f4e', ngram=2)","44cfa26e":"df_comments = read_and_reformat('..\/input\/csvs\/csvs\/comments.csv')\ndf_comments.head()","1a096761":"comments_by_year, comments_lengths_by_year, comments_count_by_year  = get_by_year_data(df_comments)","970a15cd":"for key in sorted(comments_by_year.keys()):\n    print(\"Year: \" + key)\n    create_wordcloud(comments_by_year[key], custom_stopwords + roman_urdu_stopwords)","910f1e2b":"from collections import OrderedDict\ncomments_count_by_year = OrderedDict(sorted(comments_count_by_year.items(), key=lambda t: t[0]))\n\nplt.figure(num=None, figsize=(15, 7), dpi=80, facecolor='w', edgecolor='k')\nbar = plt.bar(range(len(comments_count_by_year)), list(comments_count_by_year.values()), align='center', color='#e5db4e', edgecolor='black')\nplt.xticks(range(len(comments_count_by_year)), list(comments_count_by_year.keys()))\nplt.ylabel('Number of Comments')\nplt.title('Number of Facebook comments by Year (including comments in Groups)')\n\nfor rect in bar:\n    height = rect.get_height()\n    plt.text(rect.get_x() + rect.get_width()\/2.0, height, '%d' % int(height), ha='center', va='bottom')\n\nplt.show()","462c6d41":"df_locations = read_and_reformat('..\/input\/csvs\/csvs\/locations.csv')\ndf_locations['date'] = df_locations.apply(lambda row: get_date_from_timestamp(row['timestamp']), axis=1)\ndf_locations['datetime'] = df_locations.apply(lambda row: get_date_from_timestamp(row['timestamp'], format='%Y-%m-%d %H:%M:%S'), axis=1)\ndf_locations[['lat', 'long']] = df_locations[['lat', 'long']].apply(pd.to_numeric)\n\n\ndf_locations.head()","1e1a6282":"import folium\n\nmap=folium.Map(location=[df_locations['lat'].mean(),df_locations['long'].mean()],zoom_start=6)\n\nfor i, row in df_locations.iterrows():\n    folium.Circle(\n    radius=30,\n    location=[row['lat'], row['long']],\n    color='blue',\n    fill=True).add_to(map)\n\nprint('Total location entries: ' + str(df_locations['timestamp'].count()))\nmap","87837f90":"locations_per_year = get_by_year_data(df_locations, True)","787a4a37":"from collections import OrderedDict\nlocations_per_year = OrderedDict(sorted(locations_per_year.items(), key=lambda t: t[0]))\n\nplt.figure(num=None, figsize=(15, 7), dpi=80, facecolor='w', edgecolor='k')\nbar = plt.bar(range(len(locations_per_year)), list(locations_per_year.values()), align='center', color='#4ee576', edgecolor='black')\nplt.xticks(range(len(locations_per_year)), list(locations_per_year.keys()))\nplt.ylabel('Location collection count')\nplt.title('Number of times Facebook collected Location by Year')\n\nfor rect in bar:\n    height = rect.get_height()\n    plt.text(rect.get_x() + rect.get_width()\/2.0, height, '%d' % int(height), ha='center', va='bottom')\n    \nplt.text(bar[0].get_x() + rect.get_width()\/2.0, 1200, 'Total collections: ' + str(df_locations['timestamp'].count()), ha='center', va='bottom')\nplt.show()","d18053c8":"plt.figure(num=None, figsize=(15, 7), dpi=80, facecolor='w', edgecolor='k')\nax = df_locations.groupby('date').count().sort_values('lat', ascending=False).head(10)['timestamp'].plot.bar(edgecolor='black', rot=75)\nplt.ylabel('Location collection count')\nplt.title('Top 10 highest Location Gathered days')\n\nfor p in ax.patches: \n    ax.annotate(np.round(p.get_height(),decimals=2), (p.get_x()+p.get_width()\/2., p.get_height()), ha='center', va='center', xytext=(0, 5), textcoords='offset points')\n\nplt.show()","226c922f":"plt.figure(num=None, figsize=(15, 7), dpi=80, facecolor='w', edgecolor='k')\n\ndf_locations['datetime'] = pd.to_datetime(df_locations['datetime'])\nax=df_locations['datetime'].dt.hour.value_counts().plot.bar(edgecolor='black', rot=75)\nplt.ylabel('Frequency')\nplt.title('Number of location captures by hour')\n\nfor p in ax.patches: \n    ax.annotate(np.round(p.get_height(),decimals=2), (p.get_x()+p.get_width()\/2., p.get_height()), ha='center', va='center', xytext=(0, 5), textcoords='offset points')\n\nplt.show()","123a1da3":"Importing some basic libraries we are going to need\n\nAdding a list of Roman Urdu stopwords as most of Facebook posts and comments have a lot of Roman Urdu usage.\n\nRoman Urdu stopwords source: https:\/\/github.com\/haseebelahi\/roman-urdu-stopwords","8049bf80":"#### Getting comments data grouped by year","f04b2a06":"### Location collections by year","87c81ec2":"### Post Count vs Avg. Post Length in each Year\n\n#### This is a very interesting stat, which shows that with time maybe, the quality and content of Facebook posts has increased but the frequency of posting has decreased. It also shows the evolution of Facebook from a platform for posting silly jokes to posting serious detailed posts.","eb594f7c":"#### Reading and normalizing location data","bd4ef137":"### Number of Facebook Comments each Year (including posts in Groups)","cc6ad5c1":"### Generating Word Clouds for Facebook Posts by each year since 2010","c69e2fad":"## Utility Functions","c4e2c769":"### Grouping location data by year","bf4bff37":"### 30 Most Frequent Bigrams\n\n#### Overall this seems to be heavily affected by a lot of spam posting I did to promote university events.","9890be96":"Cleaning a single word of any punctuation marks or trailing spaces or new lines","b2c3fa27":"![Wow](https:\/\/media.giphy.com\/media\/PUBxelwT57jsQ\/giphy.gif)\n\n                                                    500 posts in 2014","cd2f2ae8":"# Facebook Data and Activity Visualizations","81f186a7":"## Stats and Visualizations on Facebook Posts, Comments and Location\n\n##### Facebook actually does not provide the data in CSV format, I downloaded the data in JSON and converted it to CSVs using this script:\nhttps:\/\/gist.github.com\/haseebelahi\/0ef3a52b89b6890e66290d006c94ac10","281547c2":"Group post and comments data by year","9e3821ca":"### Number of Facebook Posts each Year (including posts in Groups)","9bc8205e":"### Average length of a post (in words) for each year","abe3a40b":"Get year from UNIX style timestamps","01238223":"Get date from UNIX style timestamps","bdf22847":"Function for plotting the frequency of top x ngrams in a given text","a4f98760":"Function for reading the csv files","e37485e2":"## Facebook Location Data\n\n##### This is the location data that facebook app collects from your phone and stores","f3f4673c":"### Plotting the Facebook location data on a Map","a4d0c533":"Cleaning sentences","d44bf9dd":"### Top 10 Highest Location Gathering days","afe0f799":"### Generating Word Clouds for Facebook Comments by each year since 2010","a452483c":"Create a wordcloud for given text","b9ae7ead":"#### Getting posts data grouped by year","0149df7e":"### Number of location captures by hour of the day","8f846ace":"## Facebook Comments","9317fc48":"## Facebook Posts"}}