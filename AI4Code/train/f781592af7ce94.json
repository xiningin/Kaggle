{"cell_type":{"20843759":"code","5273a560":"code","a1231eff":"code","de5df05b":"code","90c3051e":"code","969061e0":"code","46026886":"code","cabec11f":"code","9edbd18d":"code","55c7be16":"code","41a06d0e":"code","1e1e2793":"code","5fe2c53a":"code","d5dce82c":"code","2da3c2e2":"code","9193ba2a":"code","04b78fc4":"code","44a59f88":"code","d2f635b2":"code","fc996de6":"code","721cf952":"code","bcad6b9f":"code","42cecbfa":"code","a9dbe67b":"code","ca22b7af":"code","defbe55d":"code","687ff388":"code","0f663cb0":"code","48144e78":"code","5843ac83":"code","5f8c007e":"code","604aa370":"code","a9bbd6aa":"code","b907946f":"code","db680fb3":"markdown","1a32ca3f":"markdown","47fe8700":"markdown","12654700":"markdown","f29ceee8":"markdown","a0823916":"markdown"},"source":{"20843759":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pylab as plt\nimport seaborn as sns\n\nimport lightgbm as lgb\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import *\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import *\nfrom sklearn.metrics import *\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5273a560":"sample_submission = pd.read_csv('sample_submission.csv')\nprint(sample_submission)","a1231eff":"def reduce_size(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    print(\"Initial memory usage is \",start_mem,\" Mb\")\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","de5df05b":"#Obtaining data from V,C,D,M and other important transaction columns\nV_cols = ['V'+str(i) for i in range(1,340)]\nC_cols = ['C'+str(i) for i in range(1,14)]\nD_cols = ['D'+str(i) for i in range(1,15)]\nM_cols = ['M'+str(i) for i in range(1,9)]\ncard_cols = ['card'+str(i) for i in range(1,6)]\ntrans_cols = ['TransactionID', 'TransactionDT', 'TransactionAmt','ProductCD','addr1', 'addr2', 'dist1', 'dist2', 'P_emaildomain', 'R_emaildomain']+C_cols+D_cols+M_cols+card_cols\ntrain_Vcols = pd.read_csv('..\/input\/ieee-fraud-detection\/train_transaction.csv',usecols = trans_cols+V_cols+['isFraud'])\n#train_Vcols = pd.read_csv('train_transaction.csv',usecols = trans_cols+V_cols+['isFraud'])\n\ntrain_Vcols = reduce_size(train_Vcols)","90c3051e":"nangp = {}\nall_nans = train_Vcols.isna()\nfor i in train_Vcols:\n    group = all_nans[i].sum()\n    try:\n        nangp[group].append(i)\n    except:\n        nangp[group] = [i]\nfor group,cols in nangp.items():\n    print(group,' NaNs: ',cols)\n\ndef hmap(cols,train_Vcols):\n    title = cols[0]+'-'+cols[-1]\n    corr_mat = train_Vcols[cols].corr()\n    plt.figure(figsize=(20,20))\n    sns.heatmap(corr_mat,cmap='RdBu_r',annot=True,center=0.0)\n    plt.title(title)\n    plt.show()\n    return corr_mat\n\nfor group,cols in nangp.items():\n    if len(cols)<5:\n        continue\n    cmat = hmap(cols,train_Vcols)\n    ","969061e0":"def usecols(grouped_cols,train_Vcols):\n    use = []\n    for g in grouped_cols:\n        mx = 0; vx = g[0]\n        for gg in g:\n            n = train_Vcols['V'+str(gg)].nunique()\n            if n>mx:\n                mx = n\n                vx = gg\n            #print(str(gg)+'-'+str(n),', ',end='')\n        use.append(vx)\n        #print()\n    #print('Use these',use)\n    return use\n\ngrouped_cols = [[[1],[2,3],[4,5],[6,7],[8,9],[10,11]],[[12,13],[14],[15,16,17,18,21,22,31,32,33,34],[19,20],[23,24],[25,26],[27,28],[29,30]],[[35,36],[37,38],[39,40,42,43,50,51,52],[41],[44,45],[46,47],[48,49]],[[53,54],[55,56],[57,58,59,60,63,64,71,72,73,74]\\\n,[61,62],[65],[66,67],[68],[69,70]],[[75,76],[77,78],[79,80,81,84,85,92,93,94],[82,83],[86,87],[88],[89],[90,91]],[[95,96,97,101,102,103,105,106],[98],[99,100],[104]],\\\n               [[107],[108,109,110,114],[111,112,113],[115,116],[117,118,119],[120,122],[121],[123]],[[124,125],[126,127,128,132,133,134],[129],[130,131],[135,136,137]],[[138],[139,140],[141,142],[146,147],[148,149,153,154,156,157,158],[161,162,163]],\\\n               [[143,164,165],[144,145,150,151,152,159,160],[166]],[[167,168,177,178,179],[172,176],[173],[181,182,183]],[[186,187,190,191,192,193,196,199],[202,203,204,211,212,213],[205,206],[207],[214,215,216]],\\\n               [[169],[170,171,200,201],[174,175],[180],[184,185],[188,189],[194,195,197,198],[208,210],[209]],[[217,218,219,231,232,233,236,237],[223],[224,225],[226],[228],[229,230],[235]],\\\n               [[240,241],[242,243,244,258],[246,257],[247,248,249,253,254],[252],[260],[261,262]],[[263,265,264],[266,269],[267,268],[273,274,275],[276,277,278]],[[220],[221,222,227,245,255,256,259],[234],[238,239],[250,251],[270,271,272]],\\\n               [[279,280,293,294,295,298,299],[284],[285,287],[286],[290,291,292],[297]],[[302,303,304],[305],[306,307,308,316,317,318],[309,311],[310,312],[319,320,321]],[[281],[282,283],[288,289],[296],[300,301],[313,314,315]],\\\n               [[322,323,324,326,327,328,329,330,331,332,333],[325],[334,335,336],[337,338,339]]]\ngood_cols = [usecols(grouped_cols[i],train_Vcols) for i in range(len(grouped_cols))]\ngood_cols = [item for col in good_cols for item in col]\ngood_cols = ['V'+str(c) for c in good_cols]\nprint(good_cols)\ntrain_trans = pd.read_csv('..\/input\/ieee-fraud-detection\/train_transaction.csv',usecols = trans_cols+good_cols+['isFraud'])\ntest_trans = pd.read_csv('..\/input\/ieee-fraud-detection\/test_transaction.csv',usecols = trans_cols+good_cols)#+['isFraud'])\n\n#train_trans = pd.read_csv('train_transaction.csv',usecols = trans_cols+good_cols+['isFraud'])\n#test_trans = pd.read_csv('test_transaction.csv',usecols = trans_cols+good_cols)\n\ntrain_trans = reduce_size(train_trans)\ntest_trans = reduce_size(test_trans)\ntrain_trans_Vpca = train_trans.copy()\ntest_trans_Vpca = test_trans.copy()","46026886":"train_trans_gc = train_trans[good_cols]\ntest_trans_gc = test_trans[good_cols]\ndef get_pca(train_trans_gc,test_trans_gc):\n    train_trans_gc.fillna(train_trans_gc.min(),inplace=True)\n    test_trans_gc.fillna(train_trans_gc.min(),inplace=True)\n    sc = StandardScaler()\n    train_trans_gc = sc.fit_transform(train_trans_gc)\n    #train_trans_gc = minmax_scale(train_trans_gc,feature_range = (0,1))\n    #test_trans_gc = minmax_scale(test_trans_gc,feature_range = (0,1))\n    test_trans_gc = sc.fit_transform(test_trans_gc)\n        \n    pca = PCA(n_components=50)\n    pcomps_train = pca.fit_transform(train_trans_gc)\n    pcomps_test = pca.transform(test_trans_gc)\n    return pcomps_train,pcomps_test\ntrain_trans_vred,test_trans_vred = get_pca(train_trans_gc,test_trans_gc)","cabec11f":"pca_cols = ['Vpca_'+str(i) for i in range(50)]\ntrain_trans_vred = pd.DataFrame(train_trans_vred,columns = pca_cols)\ntest_trans_vred = pd.DataFrame(test_trans_vred,columns = pca_cols)\n\ntrain_trans_Vpca.drop(good_cols,axis=1,inplace=True)\ntrain_trans_Vpca = pd.concat([train_trans_Vpca,train_trans_vred],axis=1)\n\ntest_trans_Vpca.drop(good_cols,axis=1,inplace=True)\ntest_trans_Vpca = pd.concat([test_trans_Vpca,test_trans_vred],axis=1)","9edbd18d":"train_id = pd.read_csv(\"..\/input\/ieee-fraud-detection\/train_identity.csv\")\ntest_id = pd.read_csv(\"..\/input\/ieee-fraud-detection\/train_identity.csv\")\n\n#train_id = pd.read_csv(\"train_identity.csv\")\n#test_id = pd.read_csv(\"test_identity.csv\")\ntrain_id =  reduce_size(train_id)\ntest_id =  reduce_size(test_id)","55c7be16":"train_id = reduce_size(train_id)\ntrain_trans = reduce_size(train_trans)\n\n test_id = reduce_size(test_id)\ntest_trans = reduce_size(test_trans)\n\n# Remove infinities\ntrain_id.replace([np.inf, -np.inf], np.nan, inplace=True)\ntrain_trans.replace([np.inf, -np.inf], np.nan, inplace=True)\ntest_id.replace([np.inf, -np.inf], np.nan, inplace=True)\ntest_trans.replace([np.inf, -np.inf], np.nan, inplace=True)\n\ntrain_trans_Vpca.replace([np.inf, -np.inf], np.nan, inplace=True)\ntest_trans_Vpca.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n\n#Removing the columns with >90% of data missing.\nprint(\"Training Id shape before cleanup\", train_id.shape)\ntrain_id = train_id[train_id.columns[train_id.isnull().mean() < 0.9]]\n# train_trans = train_trans[train_trans.columns[train_trans.isnull().mean() < 0.9]]\nprint(\"Training data Shape after\", train_id.shape)\nprint(\"Test Id data Shape before\", test_id.shape)\ntest_id = test_id[test_id.columns[test_id.isnull().mean() < 0.9]]\nprint(\"Test data Shape after\", test_id.shape)\n","41a06d0e":"# 'DeviceInfo' column\n# Reference: https:\/\/www.kaggle.com\/davidcairuz\/feature-engineering-lightgbm\nfor df in [train_id, test_id]:\n        df[\"device_name\"] = df[\"DeviceInfo\"].str.split(\"\/\", expand=True)[0]\n        df[\"device_version\"] = df[\"DeviceInfo\"].str.split(\"\/\", expand=True)[1]\n\n        df.loc[\n            df[\"device_name\"].str.contains(\"SM\", na=False), \"device_name\"\n        ] = \"Samsung\"\n        df.loc[\n            df[\"device_name\"].str.contains(\"SAMSUNG\", na=False), \"device_name\"\n        ] = \"Samsung\"\n        df.loc[\n            df[\"device_name\"].str.contains(\"GT-\", na=False), \"device_name\"\n        ] = \"Samsung\"\n        df.loc[\n            df[\"device_name\"].str.contains(\"Moto G\", na=False), \"device_name\"\n        ] = \"Motorola\"\n        df.loc[\n            df[\"device_name\"].str.contains(\"Moto\", na=False), \"device_name\"\n        ] = \"Motorola\"\n        df.loc[\n            df[\"device_name\"].str.contains(\"moto\", na=False), \"device_name\"\n        ] = \"Motorola\"\n        df.loc[df[\"device_name\"].str.contains(\"LG-\", na=False), \"device_name\"] = \"LG\"\n        df.loc[df[\"device_name\"].str.contains(\"rv:\", na=False), \"device_name\"] = \"RV\"\n        df.loc[\n            df[\"device_name\"].str.contains(\"HUAWEI\", na=False), \"device_name\"\n        ] = \"Huawei\"\n        df.loc[\n            df[\"device_name\"].str.contains(\"ALE-\", na=False), \"device_name\"\n        ] = \"Huawei\"\n        df.loc[df[\"device_name\"].str.contains(\"-L\", na=False), \"device_name\"] = \"Huawei\"\n        df.loc[df[\"device_name\"].str.contains(\"Blade\", na=False), \"device_name\"] = \"ZTE\"\n        df.loc[df[\"device_name\"].str.contains(\"BLADE\", na=False), \"device_name\"] = \"ZTE\"\n        df.loc[\n            df[\"device_name\"].str.contains(\"Linux\", na=False), \"device_name\"\n        ] = \"Linux\"\n        df.loc[df[\"device_name\"].str.contains(\"XT\", na=False), \"device_name\"] = \"Sony\"\n        df.loc[df[\"device_name\"].str.contains(\"HTC\", na=False), \"device_name\"] = \"HTC\"\n        df.loc[df[\"device_name\"].str.contains(\"ASUS\", na=False), \"device_name\"] = \"Asus\"\n\n        df.loc[\n            df.device_name.isin(\n                df.device_name.value_counts()[df.device_name.value_counts() < 200].index\n            ),\n            \"device_name\",\n        ] = \"rare\"\n\n        df[\"DeviceInfo\"] = df[\"DeviceInfo\"].fillna(\"unknown_device\").str.lower()\n        df[\"deviceInfo_device\"] = df[\"DeviceInfo\"].apply(\n            lambda x: \"\".join([i for i in x if i.isalpha()])\n        )\n        df[\"deviceInfo_version\"] = df[\"DeviceInfo\"].apply(\n            lambda x: \"\".join([i for i in x if i.isnumeric()])\n        )\n","1e1e2793":"print(train_id.columns)\nprint(test_id.columns)\nrenamed_cols = ['TransactionID', 'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06',\\\n                'id_09', 'id_10', 'id_11', 'id_12', 'id_13', 'id_14', 'id_15', 'id_16',\\\n                'id_17', 'id_18', 'id_19', 'id_20', 'id_28', 'id_29', 'id_30', 'id_31',\\\n                'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38',\\\n                'DeviceType', 'DeviceInfo', 'device_name', 'device_version','deviceInfo_device', 'deviceInfo_version']\ntest_id.columns = renamed_cols","5fe2c53a":"## Get Browser and version from id_31\nfor df in [train_id, test_id]:\n    df['browser'] = df['id_31'].str.split(' ', expand=True)[0]\n    df['version'] = df['id_31'].str.split(' ', expand=True)[1]\n    \n    # For Safari values are misplaced in train_df and test_df\n    df.loc[df['version'] == 'safari', 'browser'] = 'safari'\n    df.loc[df['version'] == 'safari', 'version'] = df[df['version'] == 'safari']['id_31'].str.split(' ', expand=True)[2]\n\n    # Get screen_width and screen_height from id_33\n    df['screen_width'] = df['id_33'].str.split('x', expand=True)[0]\n    df['screen_height'] = df['id_33'].str.split('x', expand=True)[1]\n    \n","d5dce82c":"## List Numerical and Categorical Columns\n## categorical_columns: Replace Nan values with missing\n## numerical_columns: Replace Nan values with -1\nfor df in [train_id, test_id, train_trans, test_trans,train_trans_Vpca,test_trans_Vpca]:\n    numerical_columns = df._get_numeric_data().columns\n    categorical_columns = list(set(df.columns) - set(numerical_columns))\n    df[categorical_columns] = df[categorical_columns].replace({ np.nan:'missing'})\n    df[numerical_columns] = df[numerical_columns].replace({ np.nan:-1})\n    \n## id_30 is OS name\n## Restrict to os_categories, currently ignoring versions\nos_categories = ['Mac', 'iOS', 'Android', 'Windows', 'Linux']\nfor df in [train_id, test_id]:\n    for os in os_categories:\n        df.loc[df['id_30'].str.contains(os, na=False), 'id_30'] = os","2da3c2e2":"# Card Noise Removal\n# Card 1, 3, 5 have numeric values and may contain noise values, i.e values that occur very infrequently\n# Here we remove them\ncard_threshold_map = {\n    \"card1\": 2,\n    \"card3\": 200,\n    \"card5\": 300\n}\n\nfor card, threshold in card_threshold_map.items():\n    noise_card_list = list(train_trans[card].value_counts()[train_trans[card].value_counts() < threshold].index)\n    for df in [train_trans, test_trans]:\n        df.loc[df[card].isin(noise_card_list), card] = \"Others\"\n","9193ba2a":"# Creates four new Catgorical columns\n# P_emaildomain_bin (google, microsoft etc.)\n# P_emaildomain_suffix (.com etc)\n# R_emaildomain_bin\",\n# R_emaildomain_suffix\",\ndef bin_emails_and_domains():\n    # https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/100499#latest_df-579654\n    emails = {\n        \"gmail\": \"google\",\n        \"att.net\": \"att\",\n        \"twc.com\": \"spectrum\",\n        \"scranton.edu\": \"other\",\n        \"optonline.net\": \"other\",\n        \"hotmail.co.uk\": \"microsoft\",\n        \"comcast.net\": \"other\",\n        \"yahoo.com.mx\": \"yahoo\",\n        \"yahoo.fr\": \"yahoo\",\n        \"yahoo.es\": \"yahoo\",\n        \"charter.net\": \"spectrum\",\n        \"live.com\": \"microsoft\",\n        \"aim.com\": \"aol\",\n        \"hotmail.de\": \"microsoft\",\n        \"centurylink.net\": \"centurylink\",\n        \"gmail.com\": \"google\",\n        \"me.com\": \"apple\",\n        \"earthlink.net\": \"other\",\n        \"gmx.de\": \"other\",\n        \"web.de\": \"other\",\n        \"cfl.rr.com\": \"other\",\n        \"hotmail.com\": \"microsoft\",\n        \"protonmail.com\": \"other\",\n        \"hotmail.fr\": \"microsoft\",\n        \"windstream.net\": \"other\",\n        \"outlook.es\": \"microsoft\",\n        \"yahoo.co.jp\": \"yahoo\",\n        \"yahoo.de\": \"yahoo\",\n        \"servicios-ta.com\": \"other\",\n        \"netzero.net\": \"other\",\n        \"suddenlink.net\": \"other\",\n        \"roadrunner.com\": \"other\",\n        \"sc.rr.com\": \"other\",\n        \"live.fr\": \"microsoft\",\n        \"verizon.net\": \"yahoo\",\n        \"msn.com\": \"microsoft\",\n        \"q.com\": \"centurylink\",\n        \"prodigy.net.mx\": \"att\",\n        \"frontier.com\": \"yahoo\",\n        \"anonymous.com\": \"other\",\n        \"rocketmail.com\": \"yahoo\",\n        \"sbcglobal.net\": \"att\",\n        \"frontiernet.net\": \"yahoo\",\n        \"ymail.com\": \"yahoo\",\n        \"outlook.com\": \"microsoft\",\n        \"mail.com\": \"other\",\n        \"bellsouth.net\": \"other\",\n        \"embarqmail.com\": \"centurylink\",\n        \"cableone.net\": \"other\",\n        \"hotmail.es\": \"microsoft\",\n        \"mac.com\": \"apple\",\n        \"yahoo.co.uk\": \"yahoo\",\n        \"netzero.com\": \"other\",\n        \"yahoo.com\": \"yahoo\",\n        \"live.com.mx\": \"microsoft\",\n        \"ptd.net\": \"other\",\n        \"cox.net\": \"other\",\n        \"aol.com\": \"aol\",\n        \"juno.com\": \"other\",\n        \"icloud.com\": \"apple\",\n    }\n    us_emails = [\"gmail\", \"net\", \"edu\"]\n\n    purchaser = \"P_emaildomain\"\n    recipient = \"R_emaildomain\"\n    unknown = \"email_not_provided\"\n\n    for df in [train_trans, test_trans]:\n        df[\"is_proton_mail\"] = (df[purchaser] == \"protonmail.com\") | (\n            df[recipient] == \"protonmail.com\"\n        )\n        df[\"email_check\"] = np.where(\n            (df[purchaser] == df[recipient]) & (df[purchaser] != unknown), 1, 0\n        )\n\n        for c in [purchaser, recipient]:\n            df[purchaser] = df[purchaser].fillna(unknown)\n            df[recipient] = df[recipient].fillna(unknown)\n\n            df[c + \"_bin\"] = df[c].map(emails)\n\n            df[c + \"_suffix\"] = df[c].map(lambda x: str(x).split(\".\")[-1])\n\n            df[c + \"_suffix\"] = df[c + \"_suffix\"].map(\n                lambda x: x if str(x) not in us_emails else \"us\"\n            )\nbin_emails_and_domains()","04b78fc4":"#V values without PCA\ntrain_df = train_trans.merge(train_id, how='left', left_index=True, right_index=True, on='TransactionID')\ntest_df = test_trans.merge(test_id, how='left', left_index=True, right_index=True, on='TransactionID')\n\n#2nd dataset contains V values with PCA\ntrain_df_Vpca = train_trans_Vpca.merge(train_id, how='left', left_index=True, right_index=True, on='TransactionID')\ntest_df_Vpca = test_trans_Vpca.merge(test_id, how='left', left_index=True, right_index=True, on='TransactionID')\n\nfor df in [train_df, test_df, train_df_Vpca, test_df_Vpca]:\n    numerical_columns = df._get_numeric_data().columns\n    categorical_columns = list(set(df.columns) - set(numerical_columns))\n    df[categorical_columns] = df[categorical_columns].replace({ np.nan:'missing'})\n    df[numerical_columns] = df[numerical_columns].replace({ np.nan:-1})","44a59f88":"for f in train_df.drop('isFraud', axis=1).columns:\n    if train_df[f].dtype=='object' or test_df[f].dtype=='object':\n        try:\n            lbl = LabelEncoder()\n            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n            train_df[f] = lbl.transform(list(train_df[f].values))\n            test_df[f] = lbl.transform(list(test_df[f].values))   \n        except:\n            train_df.drop(f,axis=1,inplace=True)\n            test_df.drop(f,axis=1,inplace=True)\n            print(f)\n            \nfor f in train_df_Vpca.drop('isFraud', axis=1).columns:\n    if train_df_Vpca[f].dtype=='object' or test_df_Vpca[f].dtype=='object':\n        try:\n            lbl = LabelEncoder()\n            lbl.fit(list(train_df_Vpca[f].values) + list(test_df_Vpca[f].values))\n            train_df_Vpca[f] = lbl.transform(list(train_df_Vpca[f].values))\n            test_df_Vpca[f] = lbl.transform(list(test_df_Vpca[f].values))   \n        except:\n            train_df_Vpca.drop(f,axis=1,inplace=True)\n            test_df_Vpca.drop(f,axis=1,inplace=True)\n            print(f)","d2f635b2":"X_train = train_df.sort_values('TransactionDT').drop(['isFraud','TransactionDT'],axis=1)\ny_train = train_df.sort_values('TransactionDT')['isFraud'].astype(bool)\nX_test = test_df.sort_values('TransactionDT').drop(['TransactionDT'],axis=1)\n\nX_train_Vpca = train_df_Vpca.sort_values('TransactionDT').drop(['isFraud','TransactionDT'],axis=1)\ny_train_Vpca = train_df_Vpca.sort_values('TransactionDT')['isFraud'].astype(bool)\nX_test_Vpca = test_df_Vpca.sort_values('TransactionDT').drop(['TransactionDT'],axis=1)\n\nXtrain, Xvalid, ytrain, yvalid = train_test_split(X_train,y_train,test_size=0.25, random_state=0)","fc996de6":"#LightGBM Model for V with and without PCA -- Version 1\nclf1 = lgb.LGBMClassifier(\n    num_leaves= 256,\n    min_child_samples= 79,\n    objective=\"binary\",\n    max_depth=13,\n    learning_rate=0.03,\n    boosting_type=\"gbdt\",\n    subsample_freq=3,\n    subsample=0.9,\n    bagging_seed=11,\n    metric = 'auc',\n    verbosity=-1,\n    reg_alpha=0.3,\n    reg_lambda=0.3,\n    colsample_bytree=0.9,\n    n_jobs= -1\n    )\n\nclf1.fit(X_train, y_train,verbose=True)\ny_predict = clf1.predict_proba(X_test)[:,1]\nsample_submission = pd.read_csv('..\/input\/ieee-fraud-detection\/sample_submission.csv', index_col='TransactionID')\n#sample_submission = pd.read_csv('sample_submission.csv', index_col='TransactionID')\n\nsample_submission['isFraud'] = y_predict\nsample_submission.to_csv('submission1.csv')\n\nclf1.fit(X_train_Vpca, y_train_Vpca,verbose=True)\ny_predict = clf1.predict_proba(X_test_Vpca)[:,1]\nsample_submission = pd.read_csv('..\/input\/ieee-fraud-detection\/sample_submission.csv', index_col='TransactionID')\n#sample_submission = pd.read_csv('sample_submission.csv', index_col='TransactionID')\n\nsample_submission['isFraud'] = y_predict\nsample_submission.to_csv('submission2.csv')","721cf952":"clf2 = lgb.LGBMClassifier(\n    max_bin = 63,\n    num_leaves = 255,\n    num_iterations = 500,\n    learning_rate = 0.01,\n    tree_learner = 'serial',\n    task = 'train',\n    is_training_metric = False,\n    min_data_in_leaf = 1,\n    min_sum_hessian_in_leaf = 100,\n    sparse_threshold=1.0,\n    device = 'cpu',\n    num_thread = -1,\n    save_binary= True,\n    seed= 42,\n    feature_fraction_seed = 42,\n    bagging_seed = 42,\n    drop_seed = 42,\n    data_random_seed = 42,\n    objective = 'binary',\n    boosting_type = 'gbdt',\n    verbose = 1,\n    metric = 'auc',\n    is_unbalance = True,\n    boost_from_average = False,\n)\nclf2.fit(X_train, y_train)\nsample_submission = pd.read_csv('..\/input\/ieee-fraud-detection\/sample_submission.csv', index_col='TransactionID')\n#sample_submission = pd.read_csv('sample_submission.csv', index_col='TransactionID')\nsample_submission['isFraud'] = clf2.predict_proba(X_test)[:,1]\nsample_submission.to_csv('submission3.csv')\n\nclf2.fit(X_train_Vpca, y_train_Vpca,verbose=True)\ny_predict = clf2.predict_proba(X_test_Vpca)[:,1]\nsample_submission = pd.read_csv('..\/input\/ieee-fraud-detection\/sample_submission.csv', index_col='TransactionID')\n#sample_submission = pd.read_csv('sample_submission.csv', index_col='TransactionID')\nsample_submission['isFraud'] = y_predict\nsample_submission.to_csv('submission4.csv')","bcad6b9f":"clf3 = RandomForestClassifier(n_jobs=-1, n_estimators = 250,verbose=1,max_features=20,max_depth=15)\nclf3.fit(X_train,y_train)\ny_predict = clf3.predict_proba(X_test)[:,1]\nsample_submission = pd.read_csv('..\/input\/ieee-fraud-detection\/sample_submission.csv', index_col='TransactionID')\n#sample_submission = pd.read_csv('sample_submission.csv', index_col='TransactionID')\nsample_submission['isFraud'] = y_predict\nsample_submission.to_csv('submission5.csv')","42cecbfa":"print(test_scores)","a9dbe67b":"#XGBoost Model \nimport xgboost as xgb\n\n\n\nclf4 = xgb.XGBClassifier(n_jobs=-1, \n        objective = 'binary:logistic',\n        eval_metric = 'auc',  \n        n_estimators=250,\n        max_depth=15,\n        learning_rate=0.05,\n        subsample=0.9,\n        colsample_bytree=0.9,\n        tree_method = 'hist'\n    )\n\nclf4.fit(X_train, y_train)\ny_predict = clf4.predict_proba(X_test)[:,1]\nsample_submission = pd.read_csv('.\/input\/sample_submission.csv', index_col='TransactionID')\nsample_submission['isFraud'] = y_predict\nsample_submission.to_csv('submission6.csv')\n# 0.928964\n\n\nclf4.fit(X_train_Vpca, y_train_Vpca,verbose=True)\ny_predict = clf2.predict_proba(X_test_Vpca)[:,1]\nsample_submission = pd.read_csv('.\/input\/sample_submission.csv', index_col='TransactionID')\nsample_submission['isFraud'] = y_predict\nsample_submission.to_csv('submission7.csv')\n#0.921232","ca22b7af":"sns.relplot(x= 'TransactionDT',y = 'TransactionAmt', data = train_df, color = 'b')\nplt.show()\ntrain_df[train_df['TransactionAmt'] > 10000]\ntrain_df['TransactionDT']\nsns.distplot(train_df['TransactionDT'], kde=False)\nplt.show()","defbe55d":"from scipy.stats import vonmises\nimport matplotlib.pyplot as plt\n#kappa, loc and scale.\nvonmises.fit(train_df['TransactionDT'])\n\ntrain_df['isFraud'].value_counts()\n\nNot_Fraud = train_df[train_df['isFraud'] == 0]\nIs_Fraud = train_df[train_df['isFraud'] == 1]\n\nsns.distplot(Not_Fraud['TransactionDT'], kde=False)\nplt.show()\n\nsns.distplot(Is_Fraud['TransactionDT'], kde=False)\nplt.show()\ntrain_df[D_cols].describe()\nsns.pairplot(train_df[D_cols])\nplt.show()\n\n# Overview of each col in D_cols\nfor col in D_cols:\n    print(\"Column Name: {}\".format(col))\n    print(\"Null Values Number: \", train_df[col].isnull().sum())\n    print(train_df[col].value_counts())","687ff388":"from sklearn import (manifold, datasets, decomposition, ensemble, discriminant_analysis, random_projection, neighbors)","0f663cb0":"tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)\nt0 = time()\nX_tsne = tsne.fit_transform(d_df)\ny = train_df['isFraud']","48144e78":"y = train_df['isFraud']\narea1 = np.ma.masked_where(y != 1, y)\narea2 = np.ma.masked_where(y != 0, y)","5843ac83":"plt.scatter(X_tsne[:, 0], X_tsne[:, 1], s=area2+5, label=\"Label y = 0\", c='g')\nplt.title(\"Tsne (Not Fraud)\")\nplt.xlabel('First Component')\nplt.ylabel(\"Second Component\")\nplt.show()","5f8c007e":"plt.scatter(X_tsne[:, 0], X_tsne[:, 1], s=area1+5, label=\"Label y = 1\", c='r')\nplt.title(\"Tsne (isFraud)\")\nplt.xlabel('First Component')\nplt.ylabel(\"Second Component\")\nplt.show()","604aa370":"t = pd.read_csv('..\/input\/ieee-fraud-detection\/train_transaction.csv')\ny = t['isFraud']\nyy = y.to_numpy()\n\narea1 = np.ma.masked_where(yy!=0, 5*np.ones(SY))\narea2 = np.ma.masked_where(yy!=1, 5*np.ones(SY))\ntrain_trans_vred_np = train_trans_vred.to_numpy()\ntrain_trans_gc.fillna(train_trans_gc.min(),inplace=True)\ntrain_trans_gc_np = train_trans_gc.to_numpy()\n\n#############   MDS embedding    ##################################\nprint(\"Computing MDS embedding\")\nclf = manifold.MDS(n_components=2, n_init=1, max_iter=100)\nX_mds = clf.fit_transform(train_trans_vred_np)\nplt.scatter(X_mds[:,0],X_mds[:,1], s=area1,label='not fraud',color='b')\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nylim((-300, 2500))\nplt.legend()\nplt.show()\nplt.scatter(np.concatenate(X_mds[:,0],X_mds[:,1], s=area2[1:60000],label='fraud',color='r')\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nylim((-300, 2000))\nplt.legend()\nplt.show()","a9bbd6aa":"#############   Spectral embedding    ##################################\nprint(\"Computing Spectral embedding\")\nembedder = manifold.SpectralEmbedding(n_components=2, random_state=0,eigen_solver=\"arpack\")\nX_se = embedder.fit_transform(train_trans_vred_np)\nplt.scatter(X_se[:,0],X_se[:,1], s=area1,label='not fraud',color='b')\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nylim((-300, 2500))\nplt.legend()\nplt.show()\nplt.scatter(np.concatenate(X_se[:,0],X_se[:,1], s=area2[1:60000],label='fraud',color='r')\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nylim((-300, 2000))\nplt.legend()\nplt.show()","b907946f":"############   PCA 3D visualization   ############################\nfig = plt.figure(figsize = (25, 16))\nax = fig.add_subplot(111, projection='3d')\nxs = train_trans_vred_np[:,1]\nys = train_trans_vred_np[:,0]\nzs = train_trans_vred_np[:,2]\nax.scatter(xs, ys, zs, s=area1,label='y==not fraud')\nax.scatter(xs, ys, zs, s=area2,label='y==fraud')\nax.set_xlim3d(-25,80)\nax.set_ylim3d(-20,30)\nax.set_zlim3d(0,30)\nax.set_xlabel(\"x2\")\nax.set_ylabel(\"x1\")\nax.set_zlabel(\"x3\")\nplt.legend()\nplt.show()","db680fb3":"### Time Features","1a32ca3f":"There might be outliers in this dataset.","47fe8700":"predictions = model.predict(test)","12654700":"# **PLEASE SAVE YOUR WORK BY CREATING A VERSION**","f29ceee8":"There is a surge in the plot. It might be Christmas season. So we will look into whether Christamas season has many fraud cases. If so, we will create new variables named Christmas season, Thanksgiving and so on. And then try to implement Von Minsur DIstribution to create new variables.","a0823916":"The logic behind using Von Mises distribution. In this dataset, we have time features. For D columns, there are many missing values which might be not be helpful. So my gerneral concept about using Time features is to use Transaction Delta Time.\n\n* First, I will fit the data into Vonmises Distribution to estimate the parameters. \n* Second, Create a corresponding values when testing points come.\n* Third, evaluate new variables."}}