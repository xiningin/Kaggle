{"cell_type":{"6f9d3ba1":"code","e566726e":"code","ea363f27":"code","7c1a44f1":"code","7e88759d":"code","78fb8c1b":"code","a48dec48":"code","0f554403":"code","94f6998f":"code","07fe2a05":"code","68e0a921":"code","a5f08ee7":"code","e14bdad7":"code","7a486b88":"code","dbf7ad13":"code","26cc9844":"code","cb84e7a7":"code","5159ba3c":"code","45378e63":"code","f7507e31":"code","fb528130":"code","91012e00":"code","81af67f0":"code","7d59046e":"code","9dbc6d22":"markdown","fae5dd30":"markdown"},"source":{"6f9d3ba1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport json\nfrom pandas.io.json import json_normalize\nimport matplotlib.pyplot as plt\nimport sklearn\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nimport lightgbm as lgb\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","e566726e":"def load_df(csv_path='..\/input\/train.csv', nrows=None):\n    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n    \n    df = pd.read_csv(csv_path, \n                     converters={column: json.loads for column in JSON_COLUMNS}, \n                     dtype={'fullVisitorId': 'str'}, # Important!!\n                     nrows=nrows)\n    \n    for column in JSON_COLUMNS:\n        column_as_df = json_normalize(df[column])\n        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n    print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n    return df","ea363f27":"%%time\ntrain_df = load_df()\ntest_df = load_df(\"..\/input\/test.csv\")","7c1a44f1":"train_df.info()","7e88759d":"train_df.head(5)","78fb8c1b":"#target variable\n#Since we are predicting the natural log of sum of all transactions of the user\n#let us sum up the transaction revenue at user level and take a log and then do a scatter plot.\ntrain_df[\"totals.transactionRevenue\"] = train_df[\"totals.transactionRevenue\"].astype('float')\ngdf = train_df.groupby(\"fullVisitorId\")[\"totals.transactionRevenue\"].sum().reset_index()\n\nplt.figure(figsize=(8,6))\nplt.scatter(range(gdf.shape[0]), np.sort(np.log1p(gdf[\"totals.transactionRevenue\"].values)))\nplt.xlabel('index', fontsize=12) \nplt.ylabel('TransactionRevenue', fontsize=12)\nplt.show()","a48dec48":"nzt=pd.notnull(train_df['totals.transactionRevenue']).sum() #number of instances with non zero revenue\nnzt","0f554403":"nzu=(gdf[\"totals.transactionRevenue\"]>0).sum() #number of unique customers with non zero revenue\nnzu","94f6998f":"train_df.fullVisitorId.nunique() #unique visiters in the train set\n","07fe2a05":"test_df.fullVisitorId.nunique() #unique visiters in the train set","68e0a921":"len(set(train_df.fullVisitorId.unique()).intersection(set(test_df.fullVisitorId.unique()))) #unique visitors between train and test","a5f08ee7":"const_cols = [c for c in train_df.columns if train_df[c].nunique(dropna=False)==1 ] \nconst_cols #columns with constant values which can be dropped","e14bdad7":"(set(train_df.columns).difference(set(test_df.columns))) #variable which are not common in both test and train","7a486b88":"cols_to_drop = const_cols + ['sessionId'] #drop constant columns\n\ntrain_df = train_df.drop(cols_to_drop + [\"trafficSource.campaignCode\"], axis=1)\ntest_df = test_df.drop(cols_to_drop, axis=1)","dbf7ad13":"train_df[\"totals.transactionRevenue\"].fillna(0, inplace=True) #impute 0 in Na's place\ntrain_y = train_df[\"totals.transactionRevenue\"].values\ntrain_id = train_df[\"fullVisitorId\"].values\ntest_id = test_df[\"fullVisitorId\"].values","26cc9844":"#Label encoder\ncat_cols = [\"channelGrouping\", \"device.browser\", \n            \"device.deviceCategory\", \"device.operatingSystem\", \n            \"geoNetwork.city\", \"geoNetwork.continent\", \n            \"geoNetwork.country\", \"geoNetwork.metro\",\n            \"geoNetwork.networkDomain\", \"geoNetwork.region\", \n            \"geoNetwork.subContinent\", \"trafficSource.adContent\", \n            \"trafficSource.adwordsClickInfo.adNetworkType\", \n            \"trafficSource.adwordsClickInfo.gclId\", \n            \"trafficSource.adwordsClickInfo.page\", \n            \"trafficSource.adwordsClickInfo.slot\", \"trafficSource.campaign\",\n            \"trafficSource.keyword\", \"trafficSource.medium\", \n            \"trafficSource.referralPath\", \"trafficSource.source\",\n            'trafficSource.adwordsClickInfo.isVideoAd', 'trafficSource.isTrueDirect']\nfor col in cat_cols:\n    print(col)\n    lbl = LabelEncoder()\n    lbl.fit(list(train_df[col].values.astype('str')) + list(test_df[col].values.astype('str')))\n    train_df[col] = lbl.transform(list(train_df[col].values.astype('str')))\n    test_df[col] = lbl.transform(list(test_df[col].values.astype('str')))","cb84e7a7":"#Convert numerical columns into float\nnum_cols = [\"totals.hits\", \"totals.pageviews\", \"visitNumber\", \"visitStartTime\", 'totals.bounces',  'totals.newVisits']    \nfor col in num_cols:\n    train_df[col] = train_df[col].astype(float)\n    test_df[col] = test_df[col].astype(float)","5159ba3c":"train_df['date'].","45378e63":"train_df.info()","f7507e31":"import datetime\n# Split the train dataset into development and valid based on time \ndev_df = train_df.iloc[0:700000,:]\nval_df = train_df.iloc[700001:,:]\ndev_y = np.log1p(dev_df[\"totals.transactionRevenue\"].values)\nval_y = np.log1p(val_df[\"totals.transactionRevenue\"].values)\n\ndev_X = dev_df[cat_cols + num_cols] \nval_X = val_df[cat_cols + num_cols] \ntest_X = test_df[cat_cols + num_cols]","fb528130":"def run_catbst(train_X, train_y, val_X, val_y, test_X):\n    eval_set=[(val_X,val_y)]\n    clf=cb.CatBoostRegressor(depth=10, l2_leaf_reg= 1,\n                            learning_rate= 0.15 ,eval_metric=\"RMSE\",iterations=1000, early_stopping_rounds=100)\n    model=clf.fit(train_X,train_y,eval_set=eval_set)\n    pred_test_y= model.predict(test_X)\n    pred_eval=model.predict(val_X)\n    pred_eval[pred_eval<0] = 0\n    rms = sqrt(mean_squared_error(val_y,pred_eval))\n    return pred_test_y, model ,rms\n","91012e00":"import catboost as cb\npred_test, model,rms = run_catbst(dev_X, dev_y, val_X, val_y, test_X)","81af67f0":"rms","7d59046e":"sub_df = pd.DataFrame({\"fullVisitorId\":test_id})\npred_test[pred_test<0] = 0\nsub_df[\"PredictedLogRevenue\"] = np.expm1(pred_test)\nsub_df = sub_df.groupby(\"fullVisitorId\")[\"PredictedLogRevenue\"].sum().reset_index()\nsub_df.columns = [\"fullVisitorId\", \"PredictedLogRevenue\"]\nsub_df[\"PredictedLogRevenue\"] = np.log1p(sub_df[\"PredictedLogRevenue\"])\nsub_df.to_csv(\"test1_cat.csv\", index=False)","9dbc6d22":"pred_test, model,rms = run_lgb(dev_X, dev_y, val_X, val_y, test_X)","fae5dd30":"def run_lgb(train_X, train_y, val_X, val_y, test_X):\n    params = {\n        \"objective\" : \"regression\",\n        \"metric\" : \"rmse\", \n        \"num_leaves\" : 30,\n        \"min_child_samples\" : 100,\n        \"learning_rate\" : 0.1,\n        \"bagging_fraction\" : 0.7,\n        \"feature_fraction\" : 0.5,\n        \"bagging_frequency\" : 5,\n        \"bagging_seed\" : 2018,\n        \"verbosity\" : -1\n    }\n    \n    lgtrain = lgb.Dataset(train_X, label=train_y)\n    lgval = lgb.Dataset(val_X, label=val_y)\n    model = lgb.train(params, lgtrain, 1000, valid_sets=[lgval], early_stopping_rounds=100, verbose_eval=100)\n    \n    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n    pred_eval=model.predict(val_X,num_iteration=model.best_iteration)\n    pred_eval[pred_eval<0] = 0\n    rms = sqrt(mean_squared_error(val_y,pred_eval))\n    print(rms)\n    return pred_test_y, model\n"}}