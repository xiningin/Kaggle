{"cell_type":{"4b170e4b":"code","acc98fab":"code","050d65ed":"code","c171ffa8":"code","298804cc":"code","76f56d65":"code","a06031fd":"code","7e6395a3":"code","4c06df56":"code","fb2bb1f3":"code","19b8d435":"code","9f187585":"code","284c0773":"code","2710c467":"code","5742633a":"code","c6d47a17":"code","ebf401dc":"code","fb5cd0ab":"code","3febf8e9":"code","cfcd2d27":"code","1fe1823d":"code","8801ebb3":"code","8a9e29ff":"code","b4023234":"code","dd52fefd":"code","431dd35b":"code","c37b6a7a":"code","6f0bf5fc":"code","15ac9eee":"markdown","06339e69":"markdown","68fd2230":"markdown","4b4c891a":"markdown","62526c2d":"markdown","0e3c352f":"markdown","ac69669e":"markdown","c95ccbd3":"markdown","fccfabfa":"markdown","73aad353":"markdown","3d5b6e10":"markdown","12a1f932":"markdown","9ab6dbeb":"markdown","451cbb08":"markdown","b9f90099":"markdown","b5fdb052":"markdown","32d1b01f":"markdown","03d954e6":"markdown"},"source":{"4b170e4b":"import pandas as pd     \nimport numpy as np\nfrom bs4 import BeautifulSoup\nimport re\nimport nltk\n# nltk.download()\nfrom nltk.corpus import stopwords # Import the stop word list\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score","acc98fab":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","050d65ed":"df_train = pd.read_csv(\"\/kaggle\/input\/kumarmanoj-bag-of-words-meets-bags-of-popcorn\/labeledTrainData.tsv\", \n                              header=0, \n                              delimiter=\"\\t\", \n                              quoting=3)\n\ndf_test = pd.read_csv(\"\/kaggle\/input\/kumarmanoj-bag-of-words-meets-bags-of-popcorn\/testData.tsv\",\n                             header=0, \n                             delimiter=\"\\t\", \n                             quoting=3)","c171ffa8":"print(df_train.shape)\nprint(df_test.shape)","298804cc":"df_train.info()","76f56d65":"print(df_train.columns.values)\nprint(df_test.columns.values)","a06031fd":"df_train['review'][0]","7e6395a3":"\nbs_data = BeautifulSoup(df_train[\"review\"][0])\nprint(bs_data.get_text())","4c06df56":"letters_only = re.sub(\"[^a-zA-Z]\", \" \", bs_data.get_text() )\nprint(letters_only)","fb2bb1f3":"lower_case = letters_only.lower()  \nwords = lower_case.split()  \nprint(words)","19b8d435":"print(stopwords.words(\"english\") )","9f187585":"words = [w for w in words if not w in stopwords.words(\"english\")]\nprint(words)","284c0773":"training_data_size = df_train[\"review\"].size\ntesting_data_size = df_test[\"review\"].size\n\nprint(training_data_size)\nprint(testing_data_size)","2710c467":"def clean_text_data(data_point, data_size):\n    review_soup = BeautifulSoup(data_point)\n    review_text = review_soup.get_text()\n    review_letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n    review_lower_case = review_letters_only.lower()  \n    review_words = review_lower_case.split() \n    stop_words = stopwords.words(\"english\")\n    meaningful_words = [x for x in review_words if x not in stop_words]\n    \n    if( (i)%2000 == 0 ):\n        print(\"Cleaned %d of %d data (%d %%).\" % ( i, data_size, ((i)\/data_size)*100))\n        \n    return( \" \".join( meaningful_words)) \n    ","5742633a":"# clean_train_data_list = []\n# clean_test_data_list = []","c6d47a17":"df_train.head()","ebf401dc":"for i in range(training_data_size):\n    df_train[\"review\"][i] = clean_text_data(df_train[\"review\"][i], training_data_size)\nprint(\"Cleaning training completed!\")","fb5cd0ab":"for i in range(testing_data_size):\n    df_test[\"review\"][i] = clean_text_data(df_test[\"review\"][i], testing_data_size)\nprint(\"Cleaning validation completed!\")","3febf8e9":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(analyzer = \"word\",   \\\n                             tokenizer = None,    \\\n                             preprocessor = None, \\\n                             stop_words = None,   \\\n                             max_features = 5000) ","cfcd2d27":"X_train, X_cv, Y_train, Y_cv = train_test_split(df_train[\"review\"], df_train[\"sentiment\"], test_size = 0.3, random_state=42)","1fe1823d":"X_train = vectorizer.fit_transform(X_train)\nX_train = X_train.toarray()\nprint(X_train.shape)","8801ebb3":"X_cv = vectorizer.transform(X_cv)\nX_cv = X_cv.toarray()\nprint(X_cv.shape)","8a9e29ff":"X_test = vectorizer.transform(df_test[\"review\"])\nX_test = X_test.toarray()\nprint(X_test.shape)","b4023234":"vocab = vectorizer.get_feature_names()\nprint(f\"Printing first 100 vocabulary samples:\\n{vocab[:100]}\")","dd52fefd":"distribution = np.sum(X_train, axis=0)\n\nprint(\"Printing first 100 vocab-dist pairs:\")\n\nfor tag, count in zip(vocab[:100], distribution[:100]):\n    print(count, tag)","431dd35b":"forest = RandomForestClassifier() \nforest = forest.fit( X_train, Y_train)","c37b6a7a":"predictions = forest.predict(X_cv) \nprint(\"Accuracy: \", accuracy_score(Y_cv, predictions))","6f0bf5fc":"result = forest.predict(X_test) \noutput = pd.DataFrame( data={\"id\":df_test[\"id\"], \"sentiment\":result} )\noutput.to_csv( \"submission.csv\", index=False, quoting=3 )\n","15ac9eee":"<div style=\"text-align: justify;\"><div style=\"font-size: 16px;\">For understanding the concept of bag-of-word, let us setup the environment and import the necessary libraries such as:<\/div><br>\n1. <b>pandas: <\/b> for reading and understanding the data,<br>\n2. <b>numpy: <\/b> for doing numerical computations on the data,<br>\n3. <b>BeautifulSoup: <\/b> for pulling data out of HTML and XML files and remove the unnessary tags and helps in navigating, searching, and modifying the parse tree data,<br>\n4. <b>re: <\/b> is the library for regular expression and we are going to use it to clean out data and based on pattern matching using regular expressions,<br>\n5. <b>nltk: <\/b> is a natural language toolkit library used to do text processing for classification, tokenization, stemming, tagging, parsing, semantic reasoning, etc,<br>\n6. <b>sklearn: <\/b> is used for all mahine learning tasks such as here we are using it to training a Random Forest model and predicting it's performance.<\/div>\n","06339e69":"##### cleaning training data.","68fd2230":"<img src=\"https:\/\/3.bp.blogspot.com\/-4pxORQAgAFI\/XMNZhEssXtI\/AAAAAAAAGmA\/SuQGsp-GyT4jKlUZieg_A5lnTza_GujfwCLcBGAs\/s1600\/bag_of_words.png\">","4b4c891a":"## Training Random Forest model","62526c2d":"<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/7\/76\/Random_forest_diagram_complete.png\">","0e3c352f":"## Reading the data (Training & Testing data)","ac69669e":"## Testing the model","c95ccbd3":"## Introduction\nThe bag-of-words model is a way of representing text data when modeling text with machine learning algorithms. The bag-of-words model is simple to understand and implement and has seen great success in problems such as language modeling and document classification.\n\n**According to [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Bag-of-words_model#:~:text=The%20bag%2Dof%2Dwords%20model,word%20order%20but%20keeping%20multiplicity.):** The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity. The bag-of-words model has also been used for computer vision.\n\nIn this tutorial, you will discover the bag-of-words for training the Random Forest model to predict the sentiment of a sentence.","fccfabfa":"## PreProcessing data for one item. \n###### beautifying the text of HTML and XML data\n","73aad353":"##  Importing Libraries","3d5b6e10":"### Converting the train, validation and test data to vectors","12a1f932":"##  PreProcessing data for all of the training data","9ab6dbeb":"##### cleaning testing data.","451cbb08":"### In this notebook we will learn how we can train a Random Forest using Bag of words approach.","b9f90099":"# Bag of Words using Random Forest","b5fdb052":"## Creating the output submission file","32d1b01f":"## Getting the features ready to be trained ","03d954e6":"### That marks the end of this notebook, hope it was worth reading!"}}