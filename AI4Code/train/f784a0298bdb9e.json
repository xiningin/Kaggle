{"cell_type":{"812a207d":"code","cb343880":"code","e613d450":"code","c33e9b51":"code","4952b3e8":"code","97446654":"code","74a6bf4a":"code","329478d0":"code","b4aa9f94":"code","b7d87899":"markdown","4adcddc1":"markdown","4a6c8871":"markdown","6b0ceef8":"markdown"},"source":{"812a207d":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import VarianceThreshold\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\n\nfrom tqdm import tqdm_notebook\n\nimport warnings\nwarnings.filterwarnings('ignore')","cb343880":"%%time\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\ncols = [c for c in train.columns if c not in ['id', 'target', 'wheezy-copper-turtle-magic']]","e613d450":"def get_knn_proba(X_train, y_train, X_test, k=5, tol=1e-8):\n    '''\n    Get the distance weighted voting using k nearest neighbors\n    '''\n    \n    if type(y_train) is pd.Series:\n        y_train = y_train.values\n    \n    num_classes = len(np.unique(y_train))\n    \n    # vectorized L^2 distance matrix\n    dist = -2 * np.dot(X_test, X_train.T) \\\n            + np.sum(X_train**2, axis=1) \\\n            + np.sum(X_test**2, axis=1).reshape(-1,1)\n    \n    # if distance is too small, rescale it\n    dist[dist <= tol] = tol\n    \n    # sort by columns for each row, then return the first k columns' indices\n    index_knn = np.argsort(dist,axis = 1)[:,:k] \n    \n    # the above are the indices, this is computing the inverse distances\n    dist_inv_knn = 1\/np.sort(dist,axis = 1)[:,:k] \n    \n    # retrieving the labels of these k neighbors\n    label_knn = y_train[index_knn]\n    \n    # computing the vote\n    vote = np.zeros((X_test.shape[0], num_classes))\n    for j in range(num_classes):\n        vote[:,j] = np.sum(dist_inv_knn*(label_knn==j), axis=1)\n        \n    # normalize the vote to become a probability\n    proba = vote\/np.sum(vote,axis=1)[:,np.newaxis]\n    \n    return proba","c33e9b51":"oof = np.zeros(len(train))\npreds = np.zeros(len(test))\nnum_neighbors = 9\n\nfor i in tqdm_notebook(range(512)):\n\n    train2 = train[train['wheezy-copper-turtle-magic']==i]\n    test2 = test[test['wheezy-copper-turtle-magic']==i]\n    idx1 = train2.index\n    idx2 = test2.index\n    train2.reset_index(drop=True,inplace=True)\n\n    data = pd.concat([pd.DataFrame(train2[cols]), pd.DataFrame(test2[cols])])\n    data2 = VarianceThreshold(threshold=2).fit_transform(data[cols])\n\n    train3 = data2[:train2.shape[0]]; test3 = data2[train2.shape[0]:]\n\n    skf = StratifiedKFold(n_splits=17, random_state=i)\n    for train_index, cv_index in skf.split(train2, train2['target']):\n        \n        # the kNN probabilities\n        oof[idx1[cv_index]] = get_knn_proba(train3[train_index,:], \n                            train2.loc[train_index]['target'],\n                            train3[cv_index,:], k=num_neighbors)[:,1]\n        preds[idx2] += get_knn_proba(train3[train_index,:], \n                            train2.loc[train_index]['target'],\n                            test3, k=num_neighbors)[:,1]\/ skf.n_splits ","4952b3e8":"num_threshold = 1000\nthreshold = np.linspace(0,1,num=num_threshold)\nnum_pos = (train['target']==1).sum()\nnum_neg = (train['target']==0).sum()\n\n# # a non-vectorized implementation of the code below\n# FPR = np.zeros(num_threshold)\n# TPR = np.zeros(num_threshold)\n# for i, p in tqdm_notebook(enumerate(threshold)):\n#     TPR[i] = ((train['target']==1)*(oof>=p)).sum()\/num_pos\n#     FPR[i] = ((train['target']==0)*(oof>=p)).sum()\/num_neg\n\nTPR = ((train['target']==1).values[np.newaxis,:]*(oof>=threshold[:,np.newaxis])\\\n       ).sum(axis=1)\/num_pos\nFPR = ((train['target']==0).values[np.newaxis,:]*(oof>=threshold[:,np.newaxis])\\\n       ).sum(axis=1)\/num_neg","97446654":"_, ax = plt.subplots(figsize=(10,6))\nax.plot(FPR, TPR, linewidth=3, color='k')\nax.fill_between(FPR, TPR, color=\"red\", alpha=0.2)\nax.annotate('Area Under the Curve (ROC)', xy=(0.1, 0.4), fontsize=30)\nax.set_xlabel('False positive rate')\nax.set_ylabel('True positive rate');","74a6bf4a":"# midpoint rule\nprint('Approximated AUC: {:.6}'.format(np.abs((np.diff(FPR)*(TPR[:-1] + TPR[1:])\/2).sum())) )","329478d0":"# sklearn's version is more accurate\nauc = roc_auc_score(train['target'], oof)\nprint(f'AUC: {auc:.6}')","b4aa9f94":"sub = pd.read_csv('..\/input\/sample_submission.csv')\nsub['target'] = preds\nsub.to_csv('submission.csv', index=False)","b7d87899":"## Visualize the AUC\nWe can simply use the midpoint rule to approximate this area.","4adcddc1":"# $k$-NN implementation\n\n### Simple $k$-NN\nGiven a $k$ and a new test sample $\\mathbf{x}^{(0)}$, the $k$NN classifier first identifies the neighbors $k$ points in the training data that are closest to $\\mathbf{x}^{(0)}$, whose indices are represented by $\\mathcal{N}_0$. Notice that the distance choice $\\text{dist}(\\mathbf{x}^{(0)}, \\mathbf{x})$ here is not unique, usually we use $L^2$-distance (Euclidean).\n\n$k$NN then estimates the conditional probability for class $j$ by computing the fraction of points in $\\mathcal{N}_0$ whose target label actually equal $j$:\n\n$$\nP\\big(y= j| \\mathbf{x}^{(0)} \\big)\\approx  \\frac{1}{k} \\sum_{i\\in \\mathcal{N}_0} 1\\{ y^{(i)} = j\\}.\n$$\n\nThe indicator function $1\\{ y^{(i)} = j\\}$ can be viewed as one vote from $i$-th sample. Finally, $k$NN applies Bayesian rule and classifies the test sample $\\mathbf{x}^{(0)}$ to the class with the largest estimated probability (most votes).\n\n### Weighted voting\n\nInverse distance-weighted voting: closer neighbors get higher \"votes\". The class of each of the $k$ neighbors is multiplied by a weight proportional to the inverse of the distance from that point to the given test point. This ensures that nearer neighbors contribute more to the final vote than the more distant ones. For the new sample $\\mathbf{x}^{(0)}$, then the vote function $V(\\mathbf{x}^{(i)})$ for $i=1,\\dots, k$ for these $k$ neighbors are defined as\n$$\nV(\\mathbf{x}^{(i)}) = \\begin{cases}\n\\infty & \\text{ if } \\text{dist}(\\mathbf{x}^{(0)}, \\mathbf{x}^{(i)}) = 0,\n\\\\[1em]\n\\displaystyle\\frac{1}{\\text{dist}(\\mathbf{x}^{(0)}, \\mathbf{x}^{(i)})} & \\text{ otherwise }.\n\\end{cases}\n$$\nThen we sum the votes for each class among these $k$ neighbors and classify the newcoming sample $\\mathbf{x}^{(0)}$ into the class with the highest vote.\n","4a6c8871":"# Compute the ROC curve\n\nThe ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. Here in this competition, say the label is $1$ for positive, $0$ is negative, then  the FPR can be computed by\n$$\n\\text{FPR} = \\frac{\\# \\text{ of negative samples classified as positive}}{\\text{Total }\\# \\text{ of negative samples}},\n$$\nand TPR similarly is\n$$\n\\text{TPR} = \\frac{\\# \\text{ of positive samples classified as positive}}{\\text{Total }\\# \\text{ of positive samples}}.\n$$\nHence we run a threshold through a linear space approximately in $(0,1)$ to get these two rates.","6b0ceef8":"# Summrary\nWe try to implement a version of $k$NN from scratch, and explore what it means for computing the Area Under the Receiver Operating Characteristic Curve (ROC), or AUC score. The general format of the training follows from Chris's template.\n\n### Reference:\n* [Logistic Regression - [0.800]](https:\/\/www.kaggle.com\/cdeotte\/logistic-regression-0-800)"}}