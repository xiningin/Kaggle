{"cell_type":{"c7a53feb":"code","5b071b59":"code","e0577648":"code","41d7797f":"code","947235dd":"code","77325175":"code","f325112e":"code","14bbfc16":"code","14b96c0f":"code","e9d3b019":"code","f1284594":"code","e4abae4e":"code","e658528c":"code","adc1d564":"code","ef90463e":"code","f712a7ac":"code","dbebf055":"code","4a2d4ee5":"code","370aa4dc":"code","5d400e94":"code","24595717":"code","d76a9213":"code","aa6e657f":"code","771944f8":"code","33045edd":"code","fbab9fd8":"code","7a4e1a1b":"code","2a5015b1":"code","a3ff9577":"code","e2f67c3e":"code","05d6b8b4":"code","e3ff822a":"code","52b16f89":"code","5ec2838b":"code","74ce0614":"code","f4c2e599":"code","9ed199d3":"code","23957b72":"code","1c8043cf":"code","9624998d":"code","917e05b0":"code","08daf3ef":"code","18360a2d":"code","b8f983e1":"code","b2a86180":"code","6b81766d":"code","3fdbdcea":"code","0321d3a5":"markdown","e6421a3f":"markdown","dc707a9a":"markdown","c46876d7":"markdown","f78b0fe7":"markdown","3ad77987":"markdown","1ba9adf5":"markdown","412ac82a":"markdown","8c47a454":"markdown","7aaecb7f":"markdown","025d3b46":"markdown","55600e0d":"markdown","662c02a0":"markdown","a36bcd3e":"markdown","ebb9ae5f":"markdown","a82e4994":"markdown","f2fa4d03":"markdown","bad560a2":"markdown","1252ec6f":"markdown","800a7888":"markdown","214aa664":"markdown","7b58b7f5":"markdown","91aa0e27":"markdown","3933faf7":"markdown","a555598a":"markdown","a130546f":"markdown","61dd350a":"markdown","24ed958a":"markdown","447fdaae":"markdown","b691dc3e":"markdown","69c96634":"markdown","f677a94c":"markdown","3412257c":"markdown","edee5fe9":"markdown"},"source":{"c7a53feb":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","5b071b59":"# load data\nitems=pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv\")\nshops=pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv\")\ncats=pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv\")\ntrain=pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\ntest=pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv\")","e0577648":"plt.figure(figsize=(10,4))\nplt.xlim(-100, 3000)\nflierprops = dict(marker='o', markerfacecolor='purple', markersize=6,\n                  linestyle='none', markeredgecolor='black')\nsns.boxplot(x=train.item_cnt_day, flierprops=flierprops)\n\nplt.figure(figsize=(10,4))\nplt.xlim(train.item_price.min(), train.item_price.max()*1.1)\nsns.boxplot(x=train.item_price, flierprops=flierprops)","41d7797f":"train = train[(train.item_price < 300000 )& (train.item_cnt_day < 1000)]","947235dd":"train = train[train.item_price > 0].reset_index(drop = True)\ntrain.loc[train.item_cnt_day < 1, \"item_cnt_day\"] = 0","77325175":"# \u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56\ntrain.loc[train.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\n# \u042f\u043a\u0443\u0442\u0441\u043a \u0422\u0426 \"\u0426\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0439\"\ntrain.loc[train.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\n# \u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c\u00b2\ntrain.loc[train.shop_id == 10, 'shop_id'] = 11\ntest.loc[test.shop_id == 10, 'shop_id'] = 11","f325112e":"shops.loc[ shops.shop_name == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"',\"shop_name\" ] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'\nshops[\"city\"] = shops.shop_name.str.split(\" \").map( lambda x: x[0] )\nshops[\"category\"] = shops.shop_name.str.split(\" \").map( lambda x: x[1] )\nshops.loc[shops.city == \"!\u042f\u043a\u0443\u0442\u0441\u043a\", \"city\"] = \"\u042f\u043a\u0443\u0442\u0441\u043a\"","14bbfc16":"category = []\nfor cat in shops.category.unique():\n    if len(shops[shops.category == cat]) >= 5:\n        category.append(cat)\nshops.category = shops.category.apply( lambda x: x if (x in category) else \"other\" )","14b96c0f":"from sklearn.preprocessing import LabelEncoder\nshops[\"shop_category\"] = LabelEncoder().fit_transform( shops.category )\nshops[\"shop_city\"] = LabelEncoder().fit_transform( shops.city )\nshops = shops[[\"shop_id\", \"shop_category\", \"shop_city\"]]","e9d3b019":"cats[\"type_code\"] = cats.item_category_name.apply( lambda x: x.split(\" \")[0] ).astype(str)\ncats.loc[ (cats.type_code == \"\u0418\u0433\u0440\u043e\u0432\u044b\u0435\")| (cats.type_code == \"\u0410\u043a\u0441\u0435\u0441\u0441\u0443\u0430\u0440\u044b\"), \"category\" ] = \"\u0418\u0433\u0440\u044b\"","f1284594":"category = []\nfor cat in cats.type_code.unique():\n    if len(cats[cats.type_code == cat]) >= 5: \n        category.append( cat )\ncats.type_code = cats.type_code.apply(lambda x: x if (x in category) else \"etc\")","e4abae4e":"cats.type_code = LabelEncoder().fit_transform(cats.type_code)\ncats[\"split\"] = cats.item_category_name.apply(lambda x: x.split(\"-\"))\ncats[\"subtype\"] = cats.split.apply(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\ncats[\"subtype_code\"] = LabelEncoder().fit_transform( cats[\"subtype\"] )\ncats = cats[[\"item_category_id\", \"subtype_code\", \"type_code\"]]","e658528c":"import re\ndef name_correction(x):\n    x = x.lower() # all letters lower case\n    x = x.partition('[')[0] # partition by square brackets\n    x = x.partition('(')[0] # partition by curly brackets\n    x = re.sub('[^A-Za-z0-9\u0410-\u042f\u0430-\u044f]+', ' ', x) # remove special characters\n    x = x.replace('  ', ' ') # replace double spaces with single spaces\n    x = x.strip() # remove leading and trailing white space\n    return x","adc1d564":"# split item names by first bracket\nitems[\"name1\"], items[\"name2\"] = items.item_name.str.split(\"[\", 1).str\nitems[\"name1\"], items[\"name3\"] = items.item_name.str.split(\"(\", 1).str\n\n# replace special characters and turn to lower case\nitems[\"name2\"] = items.name2.str.replace('[^A-Za-z0-9\u0410-\u042f\u0430-\u044f]+', \" \").str.lower()\nitems[\"name3\"] = items.name3.str.replace('[^A-Za-z0-9\u0410-\u042f\u0430-\u044f]+', \" \").str.lower()\n\n# fill nulls with '0'\nitems = items.fillna('0')\n\nitems[\"item_name\"] = items[\"item_name\"].apply(lambda x: name_correction(x))\n\n# return all characters except the last if name 2 is not \"0\" - the closing bracket\nitems.name2 = items.name2.apply( lambda x: x[:-1] if x !=\"0\" else \"0\")","ef90463e":"items[\"type\"] = items.name2.apply(lambda x: x[0:8] if x.split(\" \")[0] == \"xbox\" else x.split(\" \")[0] )\nitems.loc[(items.type == \"x360\") | (items.type == \"xbox360\") | (items.type == \"xbox 360\") ,\"type\"] = \"xbox 360\"\nitems.loc[ items.type == \"\", \"type\"] = \"mac\"\nitems.type = items.type.apply( lambda x: x.replace(\" \", \"\") )\nitems.loc[ (items.type == 'pc' )| (items.type == 'p\u0441') | (items.type == \"pc\"), \"type\" ] = \"pc\"\nitems.loc[ items.type == '\u0440s3' , \"type\"] = \"ps3\"","f712a7ac":"group_sum = items.groupby([\"type\"]).agg({\"item_id\": \"count\"})\ngroup_sum = group_sum.reset_index()\ndrop_cols = []\nfor cat in group_sum.type.unique():\n    if group_sum.loc[(group_sum.type == cat), \"item_id\"].values[0] <40:\n        drop_cols.append(cat)\nitems.name2 = items.name2.apply( lambda x: \"other\" if (x in drop_cols) else x )\nitems = items.drop([\"type\"], axis = 1)","dbebf055":"items.name2 = LabelEncoder().fit_transform(items.name2)\nitems.name3 = LabelEncoder().fit_transform(items.name3)\n\nitems.drop([\"item_name\", \"name1\"],axis = 1, inplace= True)\nitems.head()","4a2d4ee5":"from itertools import product\nimport time\nts = time.time()\nmatrix = []\ncols  = [\"date_block_num\", \"shop_id\", \"item_id\"]\nfor i in range(34):\n    sales = train[train.date_block_num == i]\n    matrix.append( np.array(list( product( [i], sales.shop_id.unique(), sales.item_id.unique() ) ), dtype = np.int16) )\n\nmatrix = pd.DataFrame( np.vstack(matrix), columns = cols )\nmatrix[\"date_block_num\"] = matrix[\"date_block_num\"].astype(np.int8)\nmatrix[\"shop_id\"] = matrix[\"shop_id\"].astype(np.int8)\nmatrix[\"item_id\"] = matrix[\"item_id\"].astype(np.int16)\nmatrix.sort_values( cols, inplace = True )\ntime.time()- ts","370aa4dc":"# add revenue to train df\ntrain[\"revenue\"] = train[\"item_cnt_day\"] * train[\"item_price\"]","5d400e94":"ts = time.time()\ngroup = train.groupby( [\"date_block_num\", \"shop_id\", \"item_id\"] ).agg( {\"item_cnt_day\": [\"sum\"]} )\ngroup.columns = [\"item_cnt_month\"]\ngroup.reset_index( inplace = True)\nmatrix = pd.merge( matrix, group, on = cols, how = \"left\" )\nmatrix[\"item_cnt_month\"] = matrix[\"item_cnt_month\"].fillna(0).astype(np.float16)\ntime.time() - ts","24595717":"test[\"date_block_num\"] = 34\ntest[\"date_block_num\"] = test[\"date_block_num\"].astype(np.int8)\ntest[\"shop_id\"] = test.shop_id.astype(np.int8)\ntest[\"item_id\"] = test.item_id.astype(np.int16)","d76a9213":"ts = time.time()\n\nmatrix = pd.concat([matrix, test.drop([\"ID\"],axis = 1)], ignore_index=True, sort=False, keys=cols)\nmatrix.fillna( 0, inplace = True )\ntime.time() - ts","aa6e657f":"ts = time.time()\nmatrix = pd.merge( matrix, shops, on = [\"shop_id\"], how = \"left\" )\nmatrix = pd.merge(matrix, items, on = [\"item_id\"], how = \"left\")\nmatrix = pd.merge( matrix, cats, on = [\"item_category_id\"], how = \"left\" )\nmatrix[\"shop_city\"] = matrix[\"shop_city\"].astype(np.int8)\nmatrix[\"shop_category\"] = matrix[\"shop_category\"].astype(np.int8)\nmatrix[\"item_category_id\"] = matrix[\"item_category_id\"].astype(np.int8)\nmatrix[\"subtype_code\"] = matrix[\"subtype_code\"].astype(np.int8)\nmatrix[\"name2\"] = matrix[\"name2\"].astype(np.int8)\nmatrix[\"name3\"] = matrix[\"name3\"].astype(np.int16)\nmatrix[\"type_code\"] = matrix[\"type_code\"].astype(np.int8)\ntime.time() - ts","771944f8":"# Define a lag feature function\ndef lag_feature( df,lags, cols ):\n    for col in cols:\n        print(col)\n        tmp = df[[\"date_block_num\", \"shop_id\",\"item_id\",col ]]\n        for i in lags:\n            shifted = tmp.copy()\n            shifted.columns = [\"date_block_num\", \"shop_id\", \"item_id\", col + \"_lag_\"+str(i)]\n            shifted.date_block_num = shifted.date_block_num + i\n            df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","33045edd":"ts = time.time()\nmatrix = lag_feature( matrix, [1,2,3], [\"item_cnt_month\"] )\ntime.time() - ts","fbab9fd8":"ts = time.time()\ngroup = matrix.groupby( [\"date_block_num\"] ).agg({\"item_cnt_month\" : [\"mean\"]})\ngroup.columns = [\"date_avg_item_cnt\"]\ngroup.reset_index(inplace = True)\n\nmatrix = pd.merge(matrix, group, on = [\"date_block_num\"], how = \"left\")\nmatrix.date_avg_item_cnt = matrix[\"date_avg_item_cnt\"].astype(np.float16)\nmatrix = lag_feature( matrix, [1], [\"date_avg_item_cnt\"] )\nmatrix.drop( [\"date_avg_item_cnt\"], axis = 1, inplace = True )\ntime.time() - ts","7a4e1a1b":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix.date_item_avg_item_cnt = matrix['date_item_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1,2,3], ['date_item_avg_item_cnt'])\nmatrix.drop(['date_item_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","2a5015b1":"ts = time.time()\ngroup = matrix.groupby( [\"date_block_num\",\"shop_id\"] ).agg({\"item_cnt_month\" : [\"mean\"]})\ngroup.columns = [\"date_shop_avg_item_cnt\"]\ngroup.reset_index(inplace = True)\n\nmatrix = pd.merge(matrix, group, on = [\"date_block_num\",\"shop_id\"], how = \"left\")\nmatrix.date_avg_item_cnt = matrix[\"date_shop_avg_item_cnt\"].astype(np.float16)\nmatrix = lag_feature( matrix, [1,2,3], [\"date_shop_avg_item_cnt\"] )\nmatrix.drop( [\"date_shop_avg_item_cnt\"], axis = 1, inplace = True )\ntime.time() - ts","a3ff9577":"ts = time.time()\ngroup = matrix.groupby( [\"date_block_num\",\"shop_id\",\"item_id\"] ).agg({\"item_cnt_month\" : [\"mean\"]})\ngroup.columns = [\"date_shop_item_avg_item_cnt\"]\ngroup.reset_index(inplace = True)\n\nmatrix = pd.merge(matrix, group, on = [\"date_block_num\",\"shop_id\",\"item_id\"], how = \"left\")\nmatrix.date_avg_item_cnt = matrix[\"date_shop_item_avg_item_cnt\"].astype(np.float16)\nmatrix = lag_feature( matrix, [1,2,3], [\"date_shop_item_avg_item_cnt\"] )\nmatrix.drop( [\"date_shop_item_avg_item_cnt\"], axis = 1, inplace = True )\ntime.time() - ts","e2f67c3e":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'shop_id', 'subtype_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_subtype_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'subtype_code'], how='left')\nmatrix.date_shop_subtype_avg_item_cnt = matrix['date_shop_subtype_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], ['date_shop_subtype_avg_item_cnt'])\nmatrix.drop(['date_shop_subtype_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","05d6b8b4":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'shop_city']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_city_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', \"shop_city\"], how='left')\nmatrix.date_city_avg_item_cnt = matrix['date_city_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], ['date_city_avg_item_cnt'])\nmatrix.drop(['date_city_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","e3ff822a":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'item_id', 'shop_city']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_city_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'item_id', 'shop_city'], how='left')\nmatrix.date_item_city_avg_item_cnt = matrix['date_item_city_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], ['date_item_city_avg_item_cnt'])\nmatrix.drop(['date_item_city_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","52b16f89":"ts = time.time()\ngroup = train.groupby( [\"item_id\"] ).agg({\"item_price\": [\"mean\"]})\ngroup.columns = [\"item_avg_item_price\"]\ngroup.reset_index(inplace = True)\n\nmatrix = matrix.merge( group, on = [\"item_id\"], how = \"left\" )\nmatrix[\"item_avg_item_price\"] = matrix.item_avg_item_price.astype(np.float16)\n\n\ngroup = train.groupby( [\"date_block_num\",\"item_id\"] ).agg( {\"item_price\": [\"mean\"]} )\ngroup.columns = [\"date_item_avg_item_price\"]\ngroup.reset_index(inplace = True)\n\nmatrix = matrix.merge(group, on = [\"date_block_num\",\"item_id\"], how = \"left\")\nmatrix[\"date_item_avg_item_price\"] = matrix.date_item_avg_item_price.astype(np.float16)\nlags = [1, 2, 3]\nmatrix = lag_feature( matrix, lags, [\"date_item_avg_item_price\"] )\nfor i in lags:\n    matrix[\"delta_price_lag_\" + str(i) ] = (matrix[\"date_item_avg_item_price_lag_\" + str(i)]- matrix[\"item_avg_item_price\"] )\/ matrix[\"item_avg_item_price\"]\n\ndef select_trends(row) :\n    for i in lags:\n        if row[\"delta_price_lag_\" + str(i)]:\n            return row[\"delta_price_lag_\" + str(i)]\n    return 0\n\nmatrix[\"delta_price_lag\"] = matrix.apply(select_trends, axis = 1)\nmatrix[\"delta_price_lag\"] = matrix.delta_price_lag.astype( np.float16 )\nmatrix[\"delta_price_lag\"].fillna( 0 ,inplace = True)\n\nfeatures_to_drop = [\"item_avg_item_price\", \"date_item_avg_item_price\"]\nfor i in lags:\n    features_to_drop.append(\"date_item_avg_item_price_lag_\" + str(i) )\n    features_to_drop.append(\"delta_price_lag_\" + str(i) )\nmatrix.drop(features_to_drop, axis = 1, inplace = True)\ntime.time() - ts","5ec2838b":"ts = time.time()\ngroup = train.groupby( [\"date_block_num\",\"shop_id\"] ).agg({\"revenue\": [\"sum\"] })\ngroup.columns = [\"date_shop_revenue\"]\ngroup.reset_index(inplace = True)\n\nmatrix = matrix.merge( group , on = [\"date_block_num\", \"shop_id\"], how = \"left\" )\nmatrix['date_shop_revenue'] = matrix['date_shop_revenue'].astype(np.float32)\n\ngroup = group.groupby([\"shop_id\"]).agg({ \"date_block_num\":[\"mean\"] })\ngroup.columns = [\"shop_avg_revenue\"]\ngroup.reset_index(inplace = True )\n\nmatrix = matrix.merge( group, on = [\"shop_id\"], how = \"left\" )\nmatrix[\"shop_avg_revenue\"] = matrix.shop_avg_revenue.astype(np.float32)\nmatrix[\"delta_revenue\"] = (matrix['date_shop_revenue'] - matrix['shop_avg_revenue']) \/ matrix['shop_avg_revenue']\nmatrix[\"delta_revenue\"] = matrix[\"delta_revenue\"]. astype(np.float32)\n\nmatrix = lag_feature(matrix, [1], [\"delta_revenue\"])\nmatrix[\"delta_revenue_lag_1\"] = matrix[\"delta_revenue_lag_1\"].astype(np.float32)\nmatrix.drop( [\"date_shop_revenue\", \"shop_avg_revenue\", \"delta_revenue\"] ,axis = 1, inplace = True)\ntime.time() - ts","74ce0614":"matrix[\"month\"] = matrix[\"date_block_num\"] % 12\ndays = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\nmatrix[\"days\"] = matrix[\"month\"].map(days).astype(np.int8)","f4c2e599":"ts = time.time()\nmatrix[\"item_shop_first_sale\"] = matrix[\"date_block_num\"] - matrix.groupby([\"item_id\",\"shop_id\"])[\"date_block_num\"].transform('min')\nmatrix[\"item_first_sale\"] = matrix[\"date_block_num\"] - matrix.groupby([\"item_id\"])[\"date_block_num\"].transform('min')\ntime.time() - ts","9ed199d3":"ts = time.time()\nmatrix = matrix[matrix[\"date_block_num\"] > 3]\ntime.time() - ts","23957b72":"matrix.head().T","1c8043cf":"import gc\nimport pickle\nfrom xgboost import XGBRegressor\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 12, 4","9624998d":"data = matrix.copy()\ndel matrix\ngc.collect()","917e05b0":"data[data[\"date_block_num\"]==34].shape","08daf3ef":"X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = data[data.date_block_num < 33]['item_cnt_month']\nX_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = data[data.date_block_num == 33]['item_cnt_month']\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)","18360a2d":"Y_train = Y_train.clip(0, 20)\nY_valid = Y_valid.clip(0, 20)","b8f983e1":"del data\ngc.collect();","b2a86180":"ts = time.time()\n\nmodel = XGBRegressor(\n    max_depth=10,\n    n_estimators=1000,\n    min_child_weight=0.5, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.1,\n#     tree_method='gpu_hist',\n    seed=42)\n\nmodel.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=True, \n    early_stopping_rounds = 20)\n\ntime.time() - ts","6b81766d":"Y_pred = model.predict(X_valid).clip(0, 20)\nY_test = model.predict(X_test).clip(0, 20)\n\nsubmission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": Y_test\n})\nsubmission.to_csv('xgb_submission.csv', index=False)","3fdbdcea":"from xgboost import plot_importance\n\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\nplot_features(model, (10,14))","0321d3a5":"Create a test set for month 34.","e6421a3f":"Concatenate train and test sets.","dc707a9a":"Add shop, items and categories data onto matrix df.","c46876d7":"Feature Engineering\n\n* Add lag features to matrix df.","f78b0fe7":"Clean item names.","3ad77987":"Delete first three months from matrix. They don't have lag values.","1ba9adf5":"# Preprocessing\n\nCreate a matrix df with every combination of month, shop and item in order of increasing month. Item_cnt_day is summed into an item_cnt_month.","412ac82a":"* Add total shop revenue per month to matix df. \n* Add lag values of revenue per month.\n* Add delta revenue values - how current month revenue relates to global average.","8c47a454":"Add lag values for item_cnt_month for month\/shop\/item subtype.","7aaecb7f":"Use month 34 as validation for training.","025d3b46":"Add lag values for item_cnt_month for month\/city\/item.","55600e0d":"Add month and number of days in each month to matrix df.","662c02a0":"We'll remove the obvious outliers in the dataset - the items that sold more than 1000 in one day and the item with price greater than 300,000.","a36bcd3e":"Add the month of each shop and item first sale.","ebb9ae5f":"Only keep shop category if there are 5 or more shops of that category, the rest are grouped as \"other\".","a82e4994":"# Cleaning Item Data","f2fa4d03":"# 1. Data Cleaning\n\nWe'll remove outliers, clean up some of the raw data and add some new variables to it.","bad560a2":"# Modelling","1252ec6f":"Remove any rows from train where item price is negative - these could be refunds. Also make zero and item_cnt_day values less than one, to remove negative values.","800a7888":"Add the previous month's average item_cnt.","214aa664":"Add lag values for item_cnt_month for month\/shop\/item.","7b58b7f5":"Add lag values for item_cnt_month for month\/city.","91aa0e27":"# Load Data","3933faf7":"# Cleaning Shop Data","a555598a":"Add lag values of item_cnt_month for month \/ item_id.","a130546f":"* Add average item price on to matix df. \n* Add lag values of item price per month.\n* Add delta price values - how current month average pirce relates to global average.","61dd350a":"Clean up some shop names and add 'city' and 'category' to shops df.","24ed958a":"Add item_cnt_month lag features.","447fdaae":"Clean item type","b691dc3e":"# Cleaning Item Category Data","69c96634":"# Remove outliers","f677a94c":"Add lag values for item_cnt_month for every month \/ shop combination.","3412257c":"Several of the shops look like duplicates of each other. This could be down to shops re-opening or possibly moving store location on the same street or shopping centre.","edee5fe9":"# Introduction\n\nThe Future Sales competition is the final assesment in the 'How to win a Data Science' course in the Advanced Machine Learning specialisation from HSE University, Moscow. The aim is to predict the monthly sales of items in specific shops, given historical data. The sale counts are clipped between 0 and 20."}}