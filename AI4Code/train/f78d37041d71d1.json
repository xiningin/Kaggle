{"cell_type":{"9cbda19b":"code","9d613b90":"code","220d3017":"code","26f9342a":"code","1d22033d":"code","287f1020":"code","79485325":"code","3642c779":"code","6caca763":"code","6df9ab15":"code","5c042ddc":"code","ee1178ca":"code","8cf701ca":"code","7fdd6f85":"code","cd78acec":"code","b12a79fc":"code","ca80b5c4":"code","806338cf":"code","268034c8":"code","b20e6d88":"code","5aa26cf8":"code","6c44b4c9":"code","74e200bc":"code","43f9d68f":"code","80e56a1e":"code","00f5b6b7":"code","ad7016f0":"code","76fd0fa0":"code","1f296e74":"code","3a2c9f62":"code","54c5eb7b":"code","3beb8162":"code","26e02b6f":"code","22114858":"code","84652187":"code","1a6d795b":"code","2dcda858":"code","967c7de9":"code","e5e361f5":"code","12083d1d":"code","61964c7e":"code","6e5780f2":"code","bec2a7b3":"code","61ac5907":"code","fb074b45":"code","5f9bca58":"code","ae77234d":"code","3082240a":"code","2d2a0ead":"code","30de4ff5":"code","2a9b59b1":"code","29945ca6":"code","6e07273c":"code","d80025b5":"code","ccb6fa60":"code","4c79a694":"code","9d7a94b9":"code","0e8ffa7b":"code","94095739":"code","959f7fe2":"code","eb625724":"code","1f61aa60":"code","98c9cd1d":"code","983abf93":"code","24959b64":"code","211417ab":"code","c4fd43c1":"code","989b78da":"code","1c3aa123":"code","f8b6a717":"code","d2c9fb07":"code","1651b52d":"code","07ec5fbc":"code","14b3901c":"code","ae8048b6":"code","28e521bc":"code","6bfdc540":"code","4a0178e8":"code","8e8209b1":"code","eb716ef9":"code","f1ec371a":"code","9bec4b86":"markdown","e7d82ff2":"markdown","0e79ce51":"markdown","20581573":"markdown","6467e535":"markdown","119b88f3":"markdown","837dec3f":"markdown","d2c38c6a":"markdown","28ef9376":"markdown","a7cc0294":"markdown","c4c7670f":"markdown","6573e665":"markdown","3c8cb785":"markdown","cfd07d9c":"markdown","13f36b6a":"markdown","0a6d62ac":"markdown","ba572d66":"markdown","83fb3f8f":"markdown","d2c2419c":"markdown","b3b568ea":"markdown","8b6cd888":"markdown","326c47f8":"markdown","22b3c1c0":"markdown","6334f249":"markdown","bae44f98":"markdown","a51cc4de":"markdown","6b9f32de":"markdown","b9b7facd":"markdown","dbc6403a":"markdown","f7693a9c":"markdown","54b6fe67":"markdown","0f93500b":"markdown","f9c86bbc":"markdown","6c5f121e":"markdown","f184e76a":"markdown","01767afe":"markdown","4358f23a":"markdown","ed21f335":"markdown","23f62986":"markdown","3273ad1a":"markdown","92ef9f48":"markdown","4bf95643":"markdown","c794abd6":"markdown","2d4ff565":"markdown","d97c548e":"markdown","641bf221":"markdown","7ac47669":"markdown","9f6b76a0":"markdown"},"source":{"9cbda19b":"# Imports\n\n# pandas\nimport pandas as pd\nfrom pandas import Series,DataFrame\n\n# numpy, matplotlib, seaborn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport missingno as missing\nimport seaborn as sns\nsns.set_style('whitegrid')\n%matplotlib inline\n\nimport random\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import learning_curve, validation_curve\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.metrics import roc_curve, roc_auc_score ,auc, plot_roc_curve\nfrom sklearn import svm\nimport sklearn.metrics\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_validate\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n","9d613b90":"\ndf = pd.read_csv(\"..\/input\/b_depressed.csv\")\n\n# preview the data\ndf.head()","220d3017":"df.info()","26f9342a":"df.describe()","1d22033d":"# lets check the no. of unique items present in the categorical column\n\ndf.select_dtypes('object').nunique()","287f1020":"plt.figure(figsize=(25,6))\nplt.subplot(1, 4, 1)\nsns.distplot(df['Age'])\n\nplt.subplot(1, 4, 2)\nsns.distplot(df['Number_children'])\n\nplt.subplot(1, 4, 3)\nsns.distplot(df['education_level'])\n\nplt.subplot(1, 4, 4)\nsns.distplot(df['no_lasting_investmen'])\n\nplt.suptitle('Checking for Skewness', fontsize = 15)\nplt.show()\n\n","79485325":"plt.figure(figsize=(25,6))\nplt.subplot(1, 4, 1)\nsns.distplot(df['farm_expenses'])\n\nplt.subplot(1, 4, 2)\nsns.distplot(df['incoming_agricultural'])\n\nplt.subplot(1, 4, 3)\nsns.distplot(df['gained_asset'])\n\nplt.subplot(1, 4, 4)\nsns.distplot(df['durable_asset'])\n\nplt.suptitle('Checking for Skewness', fontsize = 15)\nplt.show()\n","3642c779":"plt.figure(figsize=(20,15))\n\nplt.subplot(3,3,1)\nsns.countplot(x='sex', hue='depressed', data=df)\nplt.subplot(3,3,2)\nsns.countplot(x='Married', hue='depressed', data=df)\nplt.subplot(3,3,3)\nsns.countplot(x='Number_children', hue='depressed', data=df)\n\nplt.subplot(3,3,4)\nsns.countplot(x='education_level', hue='depressed', data=df)\nplt.subplot(3,3,5)\nsns.countplot(x='total_members', hue='depressed', data=df)\nplt.subplot(3,3,6)\nsns.countplot(x='incoming_no_business', hue='depressed', data=df)\n\nplt.subplot(3,3,7)\nsns.countplot(x='incoming_salary', hue='depressed', data=df)\nplt.subplot(3,3,8)\nsns.countplot(x='incoming_own_farm', hue='depressed', data=df)\nplt.subplot(3,3,9)\nsns.countplot(x='incoming_business', hue='depressed', data=df)\n\nplt.tight_layout()\nplt.show()","6caca763":"dfPairplot = df.drop(['Survey_id' , 'Married' , 'Ville_id' , 'sex'  , 'gained_asset' , 'durable_asset' , 'save_asset' , 'living_expenses' , 'other_expenses' , 'incoming_salary' , 'incoming_own_farm' , 'incoming_business' , 'incoming_no_business' , 'incoming_agricultural' , 'farm_expenses' , 'labor_primary' , 'lasting_investment' , 'no_lasting_investmen'], axis=1)\ndfPairplot.head()\nplt.figure(figsize=(25,6))\nsns.pairplot(data=dfPairplot,hue='depressed',plot_kws={'alpha':0.2})\nplt.show()","6df9ab15":"dfPairplot = df.drop(['save_asset','Survey_id' , 'Ville_id' , 'sex' , 'Age' , 'Married' , 'Number_children' , 'education_level' , 'total_members' , 'living_expenses' , 'other_expenses' , 'incoming_salary' , 'incoming_own_farm' , 'incoming_business' , 'incoming_no_business' , 'incoming_agricultural' , 'farm_expenses' , 'labor_primary' , 'lasting_investment' , 'no_lasting_investmen'], axis=1)\ndfPairplot.head()\nplt.figure(figsize=(25,6))\nsns.pairplot(data=dfPairplot,hue='depressed',plot_kws={'alpha':0.2})\nplt.show()","5c042ddc":"dfPairplot = df.drop(['Survey_id' , 'Ville_id' , 'sex' , 'Age' , 'Married' , 'Number_children' , 'education_level' , 'total_members' , 'gained_asset' , 'durable_asset' , 'save_asset' , 'living_expenses' , 'other_expenses' , 'incoming_salary' , 'incoming_own_farm' , 'incoming_business' , 'incoming_no_business'  , 'labor_primary'     , 'no_lasting_investmen'], axis=1)\ndfPairplot.head()\nplt.figure(figsize=(25,6))\nsns.pairplot(data=dfPairplot,hue='depressed',plot_kws={'alpha':0.2})\nplt.show()","ee1178ca":"facet = sns.FacetGrid(df,hue=\"depressed\", aspect=4)\nfacet.map(sns.kdeplot, 'Age', shade=True)\nfacet.set(xlim=(0,df['Age'].max()))\nfacet.add_legend()\n\nplt.show()","8cf701ca":"facet = sns.FacetGrid(df,hue=\"depressed\", aspect=4)\nfacet.map(sns.kdeplot, 'gained_asset', shade=True)\nfacet.set(xlim=(0,df['gained_asset'].max()))\nfacet.add_legend()\n\nplt.show()","7fdd6f85":"facet = sns.FacetGrid(df,hue=\"depressed\", aspect=4)\nfacet.map(sns.kdeplot, 'durable_asset', shade=True)\nfacet.set(xlim=(0,df['durable_asset'].max()))\nfacet.add_legend()\n\nplt.show()","cd78acec":"facet = sns.FacetGrid(df,hue=\"depressed\", aspect=4)\nfacet.map(sns.kdeplot, 'incoming_agricultural', shade=True)\nfacet.set(xlim=(0,df['incoming_agricultural'].max()))\nfacet.add_legend()\n\nplt.show()","b12a79fc":"facet = sns.FacetGrid(df,hue=\"depressed\", aspect=4)\nfacet.map(sns.kdeplot, 'farm_expenses', shade=True)\nfacet.set(xlim=(0,df['farm_expenses'].max()))\nfacet.add_legend()\n\nplt.show()","ca80b5c4":"facet = sns.FacetGrid(df,hue=\"depressed\", aspect=4)\nfacet.map(sns.kdeplot, 'lasting_investment', shade=True)\nfacet.set(xlim=(0,df['lasting_investment'].max()))\nfacet.add_legend()\n\nplt.show()","806338cf":"dfCorr = df.drop(['no_lasting_investmen'], axis=1)","268034c8":"plt.subplots(figsize=(20,10)) \nsns.heatmap(dfCorr.corr(), annot = True, fmt = \".2f\")\nplt.show()","b20e6d88":"dfDrop = df.drop(['no_lasting_investmen', 'Survey_id', 'Ville_id', 'gained_asset', 'durable_asset', 'save_asset', 'farm_expenses', 'labor_primary', 'Number_children','lasting_investment','incoming_agricultural'], axis=1)","5aa26cf8":"def plotLearningCurves(X_train, y_train, classifier, title):\n    train_sizes, train_scores, test_scores = learning_curve(\n            classifier, X_train, y_train, cv=5, scoring=\"accuracy\")\n    \n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    \n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"b\" ,label=\"Training Error\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"r\" ,label=\"Cross Validation Error\")\n    \n    plt.legend()\n    plt.grid()\n    plt.title(title, fontsize = 18, y = 1.03)\n    plt.xlabel('Data Size', fontsize = 14)\n    plt.ylabel('Error', fontsize = 14)\n    plt.tight_layout()","6c44b4c9":"def plotValidationCurves(X_train, y_train, classifier, param_name, param_range, title):\n    train_scores, test_scores = validation_curve(\n        classifier, X_train, y_train, param_name = param_name, param_range = param_range,\n        cv=5, scoring=\"accuracy\")\n\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    plt.plot(param_range, train_scores_mean, 'o-', color=\"b\" ,label=\"Training Error\")\n    plt.plot(param_range, test_scores_mean, 'o-', color=\"r\" ,label=\"Cross Validation Error\")\n\n    plt.legend()\n    plt.grid()\n    plt.title(title, fontsize = 18, y = 1.03)\n    plt.xlabel('Complexity', fontsize = 14)\n    plt.ylabel('Error', fontsize = 14)\n    plt.tight_layout()","74e200bc":"def printConfusionMatrix(y_train, pred):\n    print(\"Confusion Matrix:\")\n    print(confusion_matrix(y_test, pred))\n    print(\"Classification Report:\",)\n    print (classification_report(y_test, pred))\n    print(\"Accuracy:\", accuracy_score(y_test, pred))","43f9d68f":"X = dfDrop.iloc[:, :-1].values\ny = dfDrop.iloc[:, -1].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)","80e56a1e":"rf = RandomForestClassifier(n_estimators = 9,\n                                    max_depth=3,\n                                    min_samples_split=9,\n                                    min_samples_leaf=5\n                                   )\nrf.fit(X_train, y_train)\nrf_pred1 = rf.predict(X_test)","00f5b6b7":"plt.figure(figsize = (16,5))\ntitle = 'Random Forest Learning Curve 1'\nplotLearningCurves(X_train, y_train, rf, title)","ad7016f0":"title = 'Random Forest Validation Curve 1'\nparam_name = 'n_estimators'\nparam_range = [4, 6, 9]\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, rf, param_name, param_range, title)","76fd0fa0":"printConfusionMatrix(y_test, rf_pred1)","1f296e74":"plot_roc_curve(rf, X_test, y_test)\nplt.show()\n","3a2c9f62":"rf = RandomForestClassifier(n_estimators = 9,\n                                    max_depth=3,\n                                    criterion='entropy',\n                                    min_samples_split=9,\n                                    min_samples_leaf=5\n                                   )\nrf.fit(X_train, y_train)\nrf_pred2 = rf.predict(X_test)","54c5eb7b":"plt.figure(figsize = (16,5))\ntitle = 'Random Forest Learning Curve 2'\nplotLearningCurves(X_train, y_train, rf, title)","3beb8162":"plt.figure(figsize = (16,5))\ntitle = 'Random Forest Validation Curve 2'\nparam_name = 'n_estimators'\nparam_range = [4, 6, 9]\nplotValidationCurves(X_train, y_train, rf, param_name, param_range, title)","26e02b6f":"printConfusionMatrix(y_test, rf_pred2)","22114858":"plot_roc_curve(rf, X_test, y_test)\nplt.show()","84652187":"rf = RandomForestClassifier(n_estimators = 9,\n                                    max_depth=3,\n                                    criterion='entropy',\n                                    min_samples_split=10,\n                                    min_samples_leaf=5\n                                   )\nrf.fit(X_train, y_train)\nrf_pred3 = rf.predict(X_test)","1a6d795b":"plt.figure(figsize = (16,5))\ntitle = 'Random Forest Learning Curve 3'\nplotLearningCurves(X_train, y_train, rf, title)","2dcda858":"title = 'Random Forest Validation Curve 3'\nparam_name = 'n_estimators'\nparam_range = [4, 6, 9]\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, rf, param_name, param_range, title)","967c7de9":"printConfusionMatrix(y_test, rf_pred3)","e5e361f5":"plot_roc_curve(rf, X_test, y_test)\nplt.show()","12083d1d":"rf = RandomForestClassifier(n_estimators = 9,\n                                    max_depth=5,\n                                    criterion='entropy',\n                                    min_samples_split=9,\n                                    min_samples_leaf=10\n                                   )\nrf.fit(X_train, y_train)\nrf_pred4 = rf.predict(X_test)","61964c7e":"plt.figure(figsize = (16,5))\ntitle = 'Random Forest Learning Curve 4'\nplotLearningCurves(X_train, y_train, rf, title)","6e5780f2":"title = 'Random Forest Validation Curve 4'\nparam_name = 'n_estimators'\nparam_range = [4, 6, 9]\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, rf, param_name, param_range, title)","bec2a7b3":"printConfusionMatrix(y_test, rf_pred4)","61ac5907":"plot_roc_curve(rf, X_test, y_test)\nplt.show()","fb074b45":"rf = RandomForestClassifier(n_estimators = 9,\n                                    max_depth=5,\n                                    criterion='entropy',\n                                    max_features='sqrt',\n                                    min_samples_split=9,\n                                    min_samples_leaf=5\n                                   )\nrf.fit(X_train, y_train)\nrf_pred5 = rf.predict(X_test)","5f9bca58":"plt.figure(figsize = (16,5))\ntitle = 'Random Forest Learning Curve 5'\nplotLearningCurves(X_train, y_train, rf, title)\n","ae77234d":"title = 'Random Forest Validation Curve 5'\nparam_name = 'n_estimators'\nparam_range = [4, 6, 9]\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, rf, param_name, param_range, title)\n","3082240a":"\nprintConfusionMatrix(y_test, rf_pred5)\n","2d2a0ead":"plot_roc_curve(rf, X_test, y_test)\nplt.show()","30de4ff5":"Classifier = RandomForestClassifier()\ngrid_obj = GridSearchCV(Classifier,\n                        {'n_estimators': [4, 6, 9],\n                         'max_features': ['log2', 'sqrt','auto'],\n                         'criterion': ['entropy', 'gini'],\n                         'max_depth': [2, 3, 5, 8],\n                         'min_samples_split': [2, 5, 8, 10],\n                         'min_samples_leaf': [1, 3, 5]\n                        },\n                        scoring=make_scorer(accuracy_score))\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\nClassifier = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data. \nClassifier.fit(X_train, y_train)\n\npredictions = Classifier.predict(X_test)\n\nprint(\"Best Params: \" , grid_obj.best_estimator_)\nprint(\"Best Score: \" , grid_obj.best_score_)","2a9b59b1":"X = dfDrop.iloc[:, :-1].values\ny = dfDrop.iloc[:, -1].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)","29945ca6":"# Create KNN classifier\nknn=KNeighborsClassifier(n_neighbors=3)\n# Fit the classifier to the data\nknn.fit(X_train,y_train)\n#show first 5 model predictions on the test data\nknn_pred1=knn.predict(X_test)","6e07273c":"plt.figure(figsize=(16,5))\ntitle='KNN Learning Curve 1'\nplotLearningCurves(X_train,y_train,knn,title)","d80025b5":"title = 'KNN Validation Curve 1' \nparam_name = 'n_neighbors'\nparam_range = np.arange(1,9,2)\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, knn, param_name, param_range, title)","ccb6fa60":"printConfusionMatrix(y_test, knn_pred1)","4c79a694":"plot_roc_curve(knn, X_test, y_test)\nplt.show()","9d7a94b9":"# Create KNN classifier\nknn=KNeighborsClassifier(n_neighbors=7)\n# Fit the classifier to the data\nknn.fit(X_train,y_train)\n#show first 5 model predictions on the test data\nknn_pred2=knn.predict(X_test)","0e8ffa7b":"plt.figure(figsize=(16,5))\ntitle='KNN Learning Curve 2'\nplotLearningCurves(X_train,y_train,knn,title)","94095739":"title = 'KNN Validation Curve 2' \nparam_name = 'n_neighbors'\nparam_range = np.arange(1,9,2)\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, knn, param_name, param_range, title)","959f7fe2":"printConfusionMatrix(y_test, knn_pred2)","eb625724":"plot_roc_curve(knn, X_test, y_test)\nplt.show()","1f61aa60":"# Create KNN classifier\nknn=KNeighborsClassifier(n_neighbors=10)\n# Fit the classifier to the data\nknn.fit(X_train,y_train)\n#show first 5 model predictions on the test data\nknn_pred3=knn.predict(X_test)","98c9cd1d":"plt.figure(figsize=(16,5))\ntitle='KNN Learning Curve 3'\nplotLearningCurves(X_train,y_train,knn,title)","983abf93":"title = 'KNN Validation Curve 3' \nparam_name = 'n_neighbors'\nparam_range = np.arange(1,9,2)\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, knn, param_name, param_range, title)","24959b64":"printConfusionMatrix(y_test, knn_pred3)","211417ab":"plot_roc_curve(knn, X_test, y_test)\nplt.show()","c4fd43c1":"# Create KNN classifier\nknn=KNeighborsClassifier(n_neighbors=20)\n# Fit the classifier to the data\nknn.fit(X_train,y_train)\n#show first 5 model predictions on the test data\nknn_pred4=knn.predict(X_test)","989b78da":"plt.figure(figsize=(16,5))\ntitle='KNN Learning Curve 4'\nplotLearningCurves(X_train,y_train,knn,title)","1c3aa123":"title = 'KNN Validation Curve 4' \nparam_name = 'n_neighbors'\nparam_range = np.arange(1,9,2)\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, knn, param_name, param_range, title)","f8b6a717":"printConfusionMatrix(y_test, knn_pred4)","d2c9fb07":"plot_roc_curve(knn, X_test, y_test)\nplt.show()","1651b52d":"# Create KNN classifier\nknn=KNeighborsClassifier(n_neighbors=17)\n# Fit the classifier to the data\nknn.fit(X_train,y_train)\n#show first 5 model predictions on the test data\nknn_pred5=knn.predict(X_test)","07ec5fbc":"plt.figure(figsize=(16,5))\ntitle='KNN Learning Curve 5'\nplotLearningCurves(X_train,y_train,knn,title)","14b3901c":"title = 'KNN Validation Curve 5' \nparam_name = 'n_neighbors'\nparam_range = np.arange(1,9,2)\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, knn, param_name, param_range, title)","ae8048b6":"printConfusionMatrix(y_test, knn_pred5)","28e521bc":"plot_roc_curve(knn, X_test, y_test)\nplt.show()","6bfdc540":"#create new a knn model\nknn2=KNeighborsClassifier()\n#create a dictionary of all values we want to test for n_neighbors\nparam_grid= {'n_neighbors': np.arange(1, 20)}\n#use gridsearch to test all values for n_neighbors\nknn_gscv=GridSearchCV(knn2, param_grid, cv=5)\n#fit model to data\nknn_gscv.fit(X, y)\n\nprint(\"Best Params: \" , knn_gscv.best_estimator_)\nprint(\"Best Score: \" , knn_gscv.best_score_)","4a0178e8":"\n# Instantiate the classfiers and make a list\nclassifiers = [RandomForestClassifier(),\n               KNeighborsClassifier()]\n\nresult_table = pd.DataFrame(columns=['classifiers', 'fpr','tpr','auc'])\n\n\n# print('auc =', auc)\nlr_fpr1, lr_tpr1, _ = roc_curve(y_test, rf_pred5)\nlr_fpr2, lr_tpr2, _ = roc_curve(y_test,  knn_pred2)\n\n# fpr , tpr, _= roc_curve(X_test, predict6_test)\nauc1 = roc_auc_score(y_test, rf_pred5)\nauc2 = roc_auc_score(y_test,  knn_pred2)\n \n    \nresult_table = result_table.append({'classifiers':RandomForestClassifier.__class__.__name__,\n                                     'fpr':lr_fpr1, \n                                     'tpr':lr_tpr1, \n                                     'auc':auc1}, ignore_index=True)\n\nresult_table = result_table.append({'classifiers':KNeighborsClassifier.__class__.__name__,\n                                     'fpr':lr_fpr2, \n                                     'tpr':lr_tpr2, \n                                     'auc':auc2}, ignore_index=True)\n\n\nfig = plt.figure(figsize=(8,6))\n\nplt.plot(result_table.loc[0]['fpr'], \n         result_table.loc[0]['tpr'], \n         label=\"RandomForestClassifier, AUC={:.3f}\".format( result_table.loc[0]['auc']))\n\nplt.plot(result_table.loc[1]['fpr'], \n         result_table.loc[1]['tpr'], \n         label=\"KNeighborsClassifier, AUC={:.3f}\".format( result_table.loc[1]['auc']))\n\n\nplt.xticks(np.arange(0.0, 1.1, step=0.1))\nplt.xlabel(\"Flase Positive Rate\", fontsize=15)\n\nplt.yticks(np.arange(0.0, 1.1, step=0.1))\nplt.ylabel(\"True Positive Rate\", fontsize=15)\n\nplt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\nplt.legend(prop={'size':13}, loc='lower right')\n\nplt.show()\n    ","8e8209b1":"!apt-get remove swig \n!apt-get install swig3.0 build-essential -y\n!ln -s \/usr\/bin\/swig3.0 \/usr\/bin\/swig\n!apt-get install build-essential\n!pip install --upgrade setuptools\n!pip install auto-sklearn","eb716ef9":"X = dfDrop.iloc[:, :-1].values\ny = dfDrop.iloc[:, -1].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)","f1ec371a":"import autosklearn.classification\nimport sklearn.model_selection\nimport sklearn.datasets\nimport sklearn.metrics\nimport os  \nimport autosklearn.regression\n\n\nautoml = autosklearn.classification.AutoSklearnClassifier(\n    time_left_for_this_task=120,\n    per_run_time_limit=30,\n    tmp_folder='\/tmp\/autosklearn_cv_example_tmp3',\n    output_folder='\/tmp\/autosklearn_cv_example_out3',\n    delete_tmp_folder_after_terminate=False,\n    resampling_strategy='cv',\n    resampling_strategy_arguments={'folds': 5},\n)\n\n# fit() changes the data in place, but refit needs the original data. We\n# therefore copy the data. In practice, one should reload the data\nautoml.fit(X_train.copy(), y_train.copy(), dataset_name='Students')\n# During fit(), models are fit on individual cross-validation folds. To use\n# all available data, we call refit() which trains all models in the\n# final ensemble on the whole dataset.\nautoml.refit(X_train.copy(), y_train.copy())\n\nprint(automl.show_models())\n\npredictions = automl.predict(X_test)\nprint(\"Accuracy as per AutoML: \", sklearn.metrics.accuracy_score(y_test, predictions))","9bec4b86":"# 1. Data Set Selection\n\nObjective is to understand the influence of various factors like economic, personal and social on the depression.","e7d82ff2":"## 1.2 Reading the data","0e79ce51":"Learning curve is a measurement to check how well the model learns. This is measured by taking a reading of the accuracy of the algorithm as it trains and also while it is testing. This are plotting to see the convergence.","20581573":"## 1.6 Distict values","6467e535":"### 2.2.3 Gain Asset VS depressed","119b88f3":"### 2.2.5 Incoming Agricultural VS depressed","837dec3f":"## 3. Models","d2c38c6a":"as we discussed before gained asset is almost averagely distributed and not effective","28ef9376":"### This function is for printing the confusion matrix","a7cc0294":"> from 2.5 to 3.5 incoming Lasting investment means more not depressed  ","c4c7670f":"# ML Project 1\n## Introduction\nThe objective of this project is to train several classification models, and practice model tuning (bias\/varience) tradeoff. \n\n## Agenda\n1. Data Set Selection \n1. EDA\n1. Models\n1. AutoML\n\n## Team members\n1. Eden Zere\n1. Essey Abraham Tezare\n1. Hussien Mohamed Bayoumy Mohamed Elgabry\n1. Mario Arismendi Matos\n1. Moustafa Ahmed Galal Bahnasawy\n1. Youssef Samy Mounir\n","6573e665":"### 2.2.6 Famr expenses VS depressed","3c8cb785":"## 3.2 KNN","cfd07d9c":"### Grid Search Results","13f36b6a":"## 1.1 Import libraries","0a6d62ac":"## 2.1 Checking for Skewness","ba572d66":"The confusion matrix shows the frequency for True Positives, True Negatives, False Positives, and False Negative. Also a summary of the different properties can be presented here, along with the accuracy for predicted values.","83fb3f8f":"## 3.1 Random Forest ","d2c2419c":"## 1.5 Data description","b3b568ea":"### 2.2.8 Correclation Matrix between features","8b6cd888":"gaining assets is not as much effective as durable assets which consistently bring  money to its owner we realize that durable assets increase cause depression","326c47f8":"> Our data seems to be clean of missing values.","22b3c1c0":"in the age from 17,18 to age 35,36 its less likely to get depressed than older than 36 and younger than 17","6334f249":"> most of the dataset is not depressed, yet you can notice in some spots depression is fairly distributed in all, yet we can find more males, more married, more with no income","bae44f98":"however people who have from two to 3 durable assets is in favor of being depressed","a51cc4de":"## 4.1 AUC curve over all models","6b9f32de":"### 2.2.2 Age VS depressed","b9b7facd":"# References: \n\n* https:\/\/www.kaggle.com\/jeffd23\/scikit-learn-ml-from-start-to-finish\n* https:\/\/www.kaggle.com\/roshansharma\/student-performance-analysis\n* https:\/\/www.kaggle.com\/spscientist\/student-performance-in-exams\n* https:\/\/www.kaggle.com\/nitindatta\/eda-in-depth\n* http:\/\/scikit-learn.sourceforge.net\/stable\/auto_examples\/model_selection\/plot_validation_curve.html#example-model-selection-plot-validation-curve-py\n* https:\/\/chrisalbon.com\/machine_learning\/model_evaluation\/plot_the_validation_curve\/\n* https:\/\/datascience.stackexchange.com\/questions\/76304\/gridsearchcv-with-random-forest-classifier\n* https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html#:~:text=A%20random%20forest%20classifier.%20A%20random%20forest%20is,to%20improve%20the%20predictive%20accuracy%20and%20control%20over-fitting.\n* https:\/\/scikit-learn.org\/stable\/modules\/multiclass.html#multioutput-regression\n* https:\/\/www.datacamp.com\/community\/tutorials\/random-forests-classifier-python\n* https:\/\/www.tutorialspoint.com\/machine_learning_with_python\/machine_learning_with_python_classification_algorithms_random_forest.htm\n* https:\/\/florianhartl.com\/thoughts-on-machine-learning-dealing-with-skewed-classes.html\n* https:\/\/www.kaggle.com\/ahmedengu\/lanl-master-s-features-autosklearn","dbc6403a":"> from 2.5 to 3.5 incoming agriculture means more not depressed","f7693a9c":"# 4. Best Model (Over All AUC) and AutoML","54b6fe67":"### 2.2.4 Durable Asset VS depressed","0f93500b":"## 4.2 AutoML","f9c86bbc":"### This function is for drawing the learning curve.","6c5f121e":"> level of education is almost normally distributed, as for age, children and no lasting investment all are positively distributed meaning that most of the data is less than the medium same also goes for farm expense, incoming agriculture, gained and durable assets","f184e76a":"Cross validation is a measure of how well our model can generalize from what it learns. How well will it perform with data it has neven seen before. This is done by saving part of the data to later predict and measure the accuracy. The training data is split with differing testing folds to be used. Default in this case is k=5 folds.","01767afe":"**Using Entropy instead of default (gini)","4358f23a":"## 2.2 Relation between features and Depression","ed21f335":">in the observation in this graph the older you and the higher education you the less likely you will get depressed yet children and family members doesn't have the much of an effect","23f62986":"### 2.2.1 Personal Info VS depressed","3273ad1a":"### 2.2.9 Drop unneccessary columns\nNow we can drop math score, reading score and writing score, as we will use the pass column instead.","92ef9f48":"# 2. EDA","4bf95643":"### GridSearch Results","c794abd6":"> from 2.5 to 3.5 incoming Farm expenses means more not depressed\n\n","2d4ff565":"### This function is for drawing the validation curve.","d97c548e":"> It is observed that depression is most correclated with age with 0.11 (positive correclation), and -0.1 with education (negative correclation).\n\n> There is a string correlation between number of family and number of children, this is very logical.\n","641bf221":"### 2.2.7 Lasting investment VS depressed","7ac47669":"## 1.4 Training Data Info","9f6b76a0":"## 1.3 Data Dictionary\n1. * Survey_id\n1. * Ville_id\n1. * sex\n1. * Age\n1. * Married\n1. * Number_children\n1. * education_level\n1. * total_members (in the family)\n1. * gained_asset\n1. * durable_asset\n1. * save_asset\n1. * living_expenses\n1. * other_expenses\n1. * incoming_salary\n1. * incoming_own_farm\n1. * incoming_business\n1. * incoming_no_business\n1. * incoming_agricultural\n1. * farm_expenses\n1. * labor_primary\n1. * lasting_investment\n1. * no_lasting_investmen\n1. * depressed: [ Zero: No depressed]  or [One: depressed] (Binary for target class)"}}