{"cell_type":{"c59f99b3":"code","48df397b":"code","7697c245":"code","93aef345":"code","0f326b30":"code","af527add":"code","2fbe16e8":"code","f008c2ad":"code","46abfba5":"code","a700955c":"code","186b00a0":"code","45dd2ee9":"markdown","b2f96b3f":"markdown","56da0d2c":"markdown","fd5870ed":"markdown","81d78f61":"markdown","18aa1850":"markdown","584f38e7":"markdown","c10fe7b7":"markdown","9d9db4c8":"markdown","1c56e10a":"markdown","5f1f9091":"markdown","263a1b81":"markdown"},"source":{"c59f99b3":"import torch, os\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.datasets as dset\nfrom torchvision import transforms\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.utils import save_image\nfrom torchvision.utils import make_grid\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\n%matplotlib inline","48df397b":"lr = 0.0002\nmax_epoch = 8\nbatch_size = 32\nz_dim = 100\nimage_size = 64\ng_conv_dim = 64\nd_conv_dim = 64\nlog_step = 100\nsample_step = 500\nsample_num = 32\nIMAGE_PATH = '..\/input\/celeba-dataset\/img_align_celeba\/'\nSAMPLE_PATH = '..\/'\n\nif not os.path.exists(SAMPLE_PATH):\n    os.makedirs(SAMPLE_PATH)","7697c245":"transform = transforms.Compose([\n    transforms.Scale(image_size),\n    # transforms.Resize(image_size),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ndataset = ImageFolder(IMAGE_PATH, transform)\ndata_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=8, drop_last=True)","93aef345":"def conv(ch_in, ch_out, k_size, stride=2, pad=1, bn=True):\n    layers = []\n    layers.append(nn.Conv2d(ch_in, ch_out, k_size, stride, pad))\n    if bn:\n        layers.append(nn.BatchNorm2d(ch_out))\n    return nn.Sequential(*layers)\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, image_size=128, conv_dim=64):\n        super(Discriminator, self).__init__()\n        self.conv1 = conv(3, conv_dim, 4, bn=False)\n        self.conv2 = conv(conv_dim, conv_dim*2, 4)\n        self.conv3 = conv(conv_dim*2, conv_dim*4, 4)\n        self.conv4 = conv(conv_dim*4, conv_dim*8, 4)\n        self.fc = conv(conv_dim*8, 1, int(image_size\/16), 1, 0, False)\n        \n    def forward(self, x):                                 # if image_size is 64, output shape is below\n        out = F.leaky_relu(self.conv1(x), 0.05)    # (?, 64, 32, 32)     \n        out = F.leaky_relu(self.conv2(out), 0.05) # (?, 128, 16, 16)\n        out = F.leaky_relu(self.conv3(out), 0.05) # (?, 256, 8, 8)\n        out = F.leaky_relu(self.conv4(out), 0.05) # (?, 512, 4, 4)\n        out = F.sigmoid(self.fc(out)).squeeze()\n        return out\n    \nD = Discriminator(image_size)\nD.cuda()","0f326b30":"def deconv(ch_in, ch_out, k_size, stride=2, pad=1, bn=True):\n    layers = []\n    layers.append(nn.ConvTranspose2d(ch_in, ch_out, k_size, stride, pad))\n    if bn:\n        layers.append(nn.BatchNorm2d(ch_out))\n    return nn.Sequential(*layers)\n\nclass Generator(nn.Module):\n    def __init__(self, z_dim=256, image_size=128, conv_dim=64):\n        super(Generator, self).__init__()\n        self.fc = deconv(z_dim, conv_dim*8, int(image_size\/16), 1, 0, bn=False)\n        self.deconv1 = deconv(conv_dim*8, conv_dim*4, 4)\n        self.deconv2 = deconv(conv_dim*4, conv_dim*2, 4)\n        self.deconv3 = deconv(conv_dim*2, conv_dim, 4)\n        self.deconv4 = deconv(conv_dim, 3, 4, bn=False)\n        \n    def forward(self, z):\n        z = z.view(z.size(0), z.size(1), 1, 1)\n        out = self.fc(z)\n        out = F.leaky_relu(self.deconv1(out), 0.05)\n        out = F.leaky_relu(self.deconv2(out), 0.05)\n        out = F.leaky_relu(self.deconv3(out), 0.05)\n        out = F.tanh(self.deconv4(out))\n        return out\n    \nG = Generator(z_dim, image_size, g_conv_dim)\nG.cuda()","af527add":"criterion = nn.BCELoss().cuda()\nd_optimizer = torch.optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\ng_optimizer = torch.optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))","2fbe16e8":"# denormalization : [-1,1] -> [0,1]\n# normalization : [0,1] -> [-1,1]\ndef denorm(x):\n    out = (x + 1) \/ 2\n    return out.clamp(0, 1)","f008c2ad":"try:\n    G.load_state_dict(torch.load('generator.pkl'))\n    D.load_state_dict(torch.load('discriminator.pkl'))\n    print(\"\\n-------------model restored-------------\\n\")\nexcept:\n    print(\"\\n-------------model not restored-------------\\n\")\n    pass","46abfba5":"total_batch = len(data_loader.dataset)\/\/batch_size\nfixed_z = Variable(torch.randn(sample_num, z_dim)).cuda()\nfor epoch in range(max_epoch):\n    for i, (images, labels) in enumerate(data_loader):\n        # Build mini-batch dataset\n        image = Variable(images).cuda()\n        # Create the labels which are later used as input for the BCE loss\n        real_labels = Variable(torch.ones(batch_size)).cuda()\n        fake_labels = Variable(torch.zeros(batch_size)).cuda()\n        \n        #============ train the discriminator ============\n        # Compute BCE_loss using real images where BCE_Loss(x, y): - y * log(D(x)) - (1-y) * log(1 - D(x))\n        # Second term of the loss is always zero since real_labels = 1\n        outputs = D(image)\n        d_loss_real = criterion(outputs, real_labels) # BCE\n        real_score = outputs\n        \n        # compute BCE_loss using fake images\n        z = Variable(torch.randn(batch_size, z_dim)).cuda()\n        fake_images = G(z)\n        outputs = D(fake_images)\n        d_loss_fake = criterion(outputs, fake_labels) #BCE\n        fake_score = outputs\n        \n        # Backprob + Optimize\n        d_loss = d_loss_real + d_loss_fake\n        D.zero_grad()\n        d_loss.backward()\n        d_optimizer.step()\n        \n        #============ train the generator ============\n        # Compute loss with fake images\n        z = Variable(torch.randn(batch_size, z_dim)).cuda()\n        fake_images = G(z)\n        outputs = D(fake_images)\n        \n        # We train G to maximize log(D(G(z))) instead of minimizing log(1-D(G(z)))\n        # For the reason, see the last paragraph of section 3. https:\/\/arxiv.org\/pdf\/1406.2661.pdf\n        g_loss = criterion(outputs, real_labels) # BCE\n        \n        # Backprob + Optimize\n        D.zero_grad()\n        G.zero_grad()\n        g_loss.backward()\n        g_optimizer.step()\n        \n        if(i+1)%log_step == 0:\n            print(\"Epoch [%d\/%d], Step[%d\/%d], d_loss: %.4f, g_loss: %.4f, D(x): %.2f, D(G(z)): %.2f\"%(\n            epoch, max_epoch, i+1, total_batch, d_loss.data[0], g_loss.data[0], real_score.data.mean(), fake_score.data.mean()))\n        \n        if(i+1)%sample_step == 0:\n            fake_images = G(fixed_z)\n            torchvision.utils.save_image(denorm(fake_images.data), os.path.join(SAMPLE_PATH, 'fake_samples-%d-%d.png')%(\n            epoch+1, i+1), nrow=8)\n            \ntorch.save(G.state_dict(), 'generator.pkl')\ntorch.save(D.state_dict(), 'discriminator.pkl')","a700955c":"fixed_z = Variable(torch.randn(sample_num, z_dim)).cuda()\nfake_images = G(fixed_z)\nplt.imshow(denorm(fake_images[0].cpu().permute(1, 2, 0).data).numpy())\nplt.show()\n\nplt.imshow(make_grid(denorm(fake_images).data.cpu()).permute(1, 2, 0).numpy())\nplt.show()","186b00a0":"import imageio\n\nimages = []\nfor epoch in range(max_epoch):\n    for i in range(total_batch):\n        if(i+1)%sample_step == 0:\n            img_name = '..\/fake_samples-' + str(epoch + 1) + '-' + str(i + 1) + '.png'\n            images.append(imageio.imread(img_name))\n            print(\"epoch : {}, step : {}\".format(epoch+1, i+1))\nimageio.mimsave('dcgan_celebA_generation_animation.gif', images, fps=5)","45dd2ee9":"## 1. Data","b2f96b3f":"## 8. Train","56da0d2c":"# DCGAN with CelebA\n- Generative Adversarial Nets : https:\/\/arxiv.org\/pdf\/1406.2661.pdf","fd5870ed":"## 6. Loss func & Optimizer","81d78f61":"## 10. img2gif","18aa1850":"## 9. test","584f38e7":"## 7. etc","c10fe7b7":"## 5. Model Define","9d9db4c8":"```script\nchmod +x download.sh\n.\/download.sh\nunzip -q CelebA_128crop_FD.zip?dl=0 -d .\/data\/\n```","1c56e10a":"## 3. Hyperparameters","5f1f9091":"## 4. Load Data","263a1b81":"## 2. Import Libs"}}