{"cell_type":{"7b7906c1":"code","87b9a939":"code","0f59ce2b":"code","18288453":"code","21f40eee":"code","cf26103a":"code","30287c2d":"code","5c2e327a":"code","2c696f2d":"code","3c39ff11":"code","2e0105be":"code","c6c4e82d":"code","d7aa9bc1":"code","f6656b43":"code","46abe1b9":"code","1ee821d4":"code","d128990b":"code","7fc2b7bc":"code","ee14cd03":"code","f0d7320d":"code","bcdbbdce":"code","67d5e39d":"code","6e2bbee5":"code","5505caef":"code","3ea07d20":"code","ce218fcb":"code","76bf8d3f":"code","266dcf09":"code","7383e763":"code","55543355":"code","98e2daf9":"code","54ae7652":"markdown","fd8eef5c":"markdown","e2c230d4":"markdown","a52eed26":"markdown","9d014989":"markdown","d3359c57":"markdown","ad3338aa":"markdown","a2d42f2a":"markdown","79e24f13":"markdown","674ecd7e":"markdown","4e41031a":"markdown","ae899d84":"markdown","ddf17e0e":"markdown","cdb5c8c6":"markdown","6795b36d":"markdown","24e6e1b4":"markdown","a7fd9ab2":"markdown","e8af27d3":"markdown","7aa580c2":"markdown","a7c1b3ed":"markdown","7e2617fc":"markdown","1da901a6":"markdown","023ac5c7":"markdown","76ea2047":"markdown"},"source":{"7b7906c1":"#Loading libraries \nimport pandas as pd\nimport numpy as np\nimport random as rnd\nfrom collections import Counter # value count calculator\nfrom sklearn.model_selection import train_test_split\n","87b9a939":"#Loading Dataset \ndf = pd.read_csv(\"..\/input\/diabetic-data-cleaning\/diabetic_data.csv\",low_memory = False, na_values = '?')","0f59ce2b":"#In the beginning I am assessing the data and checking dataframe size\nprint(f'Number of rows: {len(df)}, Number of columns: {len(df.columns)}')\n\n#Displaying first 10 rows of data\ndf.head(10).T","18288453":"#Removing duplicate records leaving only earliest encounter_id\ndf = df.sort_values(['patient_nbr', 'encounter_id']).drop_duplicates(subset= ['patient_nbr'], keep = 'first')","21f40eee":"# # Checking distinct values in individual features\n# # First we need to exclude unique identifiers - encounter_id and patient_nbr\n\n# col_distinct_val_check = [col for col in df.columns if col not in ['encounter_id', 'patient_nbr']]\n\n# for col in col_distinct_val_check:\n#     print(col)\n#     print(set(df[col]))\n#     print(input(\"Assess distinct values and press Enter\"))","cf26103a":"#Checking proprtion of null values in each feature\nfor col in df.columns:\n    if df[col].dtype == object:\n        proportion_null = df[col].isna().sum()\/len(df)\n        #proportion_null = df[col][df[col] == '?'].count()\/len(df)*100\n        print(col, f'{proportion_null * 100:.2f}%')","30287c2d":"df['race'].value_counts(dropna=False)","5c2e327a":"df['race'] = df.race.fillna('Other')","2c696f2d":"df['gender'].value_counts(dropna=False)","3c39ff11":"exclude_indexes = df[df['gender'] == 'Unknown\/Invalid'].index.tolist()\n\nexclude_indexes = exclude_indexes + df[df['diag_1'].isna()].index.tolist()\n\nexclude_indexes = exclude_indexes + df[df['diag_2'].isna()].index.tolist()\n\nexclude_indexes = exclude_indexes + df[df['diag_3'].isna()].index.tolist()\n\nrequired_indexes = [index for index in df.index.tolist() if index not in list(set(exclude_indexes))]\n\ndf = df.loc[required_indexes]","2e0105be":"df = df.drop(['weight','payer_code','medical_specialty','examide', 'citoglipton', 'glimepiride-pioglitazone'], axis = 1)","c6c4e82d":"df['race'] = df['race'].astype('category').cat.codes\n\ndf['gender'] = df['gender'].replace('Male', 1)\ndf['gender'] = df['gender'].replace('Female', 0)\n\ndf['age'] = df['age'].replace('[0-10)', 0)\ndf['age'] = df['age'].replace('[10-20)', 1)\ndf['age'] = df['age'].replace('[20-30)', 2)\ndf['age'] = df['age'].replace('[30-40)', 3)\ndf['age'] = df['age'].replace('[40-50)', 4)\ndf['age'] = df['age'].replace('[50-60)', 5)\ndf['age'] = df['age'].replace('[60-70)', 6)\ndf['age'] = df['age'].replace('[70-80)', 7)\ndf['age'] = df['age'].replace('[80-90)', 8)\ndf['age'] = df['age'].replace('[90-100)', 9)   ","d7aa9bc1":"df['diag_1'] = df['diag_1'].astype('category').cat.codes\ndf['diag_2'] = df['diag_2'].astype('category').cat.codes\ndf['diag_3'] = df['diag_3'].astype('category').cat.codes","f6656b43":"#Lab Tests\ndf['A1Cresult'] = df['A1Cresult'].replace('>7', 2)\ndf['A1Cresult'] = df['A1Cresult'].replace('>8', 2)\ndf['A1Cresult'] = df['A1Cresult'].replace('Norm', 1)                            \ndf['A1Cresult'] = df['A1Cresult'].replace('None', 0, regex=True)\n\ndf['max_glu_serum'] = df['max_glu_serum'].replace('>200', 2)\ndf['max_glu_serum'] = df['max_glu_serum'].replace('>300', 2)\ndf['max_glu_serum'] = df['max_glu_serum'].replace('Norm', 1)\ndf['max_glu_serum'] = df['max_glu_serum'].replace('None', 0, regex=True)","46abe1b9":"medications = ['metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', 'glipizide', \\\n'glyburide', 'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'insulin', 'glyburide-metformin', \\\n'tolazamide','metformin-pioglitazone', 'metformin-rosiglitazone', 'glipizide-metformin','troglitazone', \\\n'tolbutamide', 'acetohexamide']\n\nfor col in medications:\n    df[col] = df[col].replace('No', 0)\n    df[col] = df[col].replace('Steady', 1)\n    df[col] = df[col].replace('Up', 1)\n    df[col] = df[col].replace('Down', 1)","1ee821d4":"df['change'] = df['change'].replace('Ch', 1)\ndf['change'] = df['change'].replace('No', 0)\n\ndf['diabetesMed'] = df['diabetesMed'].replace('Yes', 1)\ndf['diabetesMed'] = df['diabetesMed'].replace('No', 0)\n\ndf['readmitted'] = df['readmitted'].replace('<30', 1)\ndf['readmitted'] = df['readmitted'].replace('>30', 0)\ndf['readmitted'] = df['readmitted'].replace('NO', 0)","d128990b":"print(f'Number of rows: {len(df)}, Number of columns: {len(df.columns)}')","7fc2b7bc":"\nfeatures = ['race', 'gender', 'admission_type_id', 'discharge_disposition_id','admission_source_id', 'diag_1',\\\n'diag_2', 'diag_3', 'max_glu_serum', 'A1Cresult','metformin', 'repaglinide', 'nateglinide', 'chlorpropamide',\\\n'glimepiride', 'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone', 'rosiglitazone', \\\n'acarbose', 'miglitol', 'troglitazone', 'tolazamide', 'insulin', 'glyburide-metformin', 'glipizide-metformin',\\\n'metformin-rosiglitazone', 'metformin-pioglitazone', 'change', 'diabetesMed']\n\nx = df[features].iloc[:10000].copy()\n\ny = df['readmitted'].iloc[:10000].copy()\n\nx_train, x_test, y_train, y_test = train_test_split(x, y,random_state = 42,test_size = 0.3, train_size = 0.7)","ee14cd03":"class Tree(): \n    \"\"\"\n    This class creates decision tree nodes\n    \"\"\"\n    def __init__(self, Y: list, X: pd.DataFrame, min_samples_split = None, max_depth = None, depth = None,): \n        self.Y = Y \n        self.X = X\n        \n        self.min_samples_split = min_samples_split if min_samples_split else 10\n        self.max_depth = max_depth if max_depth else 10\n\n        self.depth = depth if depth else 0\n\n        self.features = list(self.X.columns)\n\n        self.counts = Counter(Y)\n        \n        # GINI impurity\n        self.gini_impurity = self.get_GINI()\n\n        # Sorting the counts and saving the final prediction of the node \n        counts_sorted = list(sorted(self.counts.items(), key=lambda item: item[1]))\n        \n#         if len(Y) == 0:\n#             print(\"A node has 0 smaples - prediction will be None\")\n        \n        # Getting the last item\n        yhat = None\n        if len(counts_sorted) > 0:\n            yhat = counts_sorted[-1][0]\n\n        # Saving to object attribute. This node will predict the class with the most frequent class\n        self.yhat = yhat \n\n        # Saving the number of observations in the node \n        self.num_observations = len(Y)\n\n        # Empty left and right nodes\n        self.left_node = None \n        self.right_node = None \n\n        # Default values for splits\n        self.best_feature = None \n        self.best_value = None      \n\n    @staticmethod\n    def GINI_impurity(y1_count: int, y2_count: int) -> float:\n        \"\"\"\n        Calculate the GINI impurity\n        \"\"\"\n        # Ensuring the correct types\n        if y1_count is None:\n            y1_count = 0\n\n        if y2_count is None:\n            y2_count = 0\n\n        # total observations\n        total_observ = y1_count + y2_count\n        \n        # When total observations is 0 then gini impurity 0\n        if total_observ == 0:\n            return 0.0\n\n        # Getting the probability to see each of the classes\n        p1 = y1_count \/ total_observ\n        p2 = y2_count \/ total_observ\n        \n        # Calculating GINI \n        gini = 1 - (p1 ** 2 + p2 ** 2)\n        # Returning the gini impurity\n        return gini\n\n    @staticmethod\n    def rolling_average(x: np.array, window: int) -> np.array:\n        \"\"\"\n        Calculates the rolling average \n        \"\"\"\n        return np.convolve(x, np.ones(window), 'valid') \/ window\n\n    def get_GINI(self):\n        \"\"\"\n        Calculate GINI impurity of a node \n        \"\"\"\n        # Getting the 0 and 1 counts\n        y1_count, y2_count = self.counts.get(0, 0), self.counts.get(1, 0)\n\n        # Getting the GINI impurity\n        return self.GINI_impurity(y1_count, y2_count)\n\n    def best_split(self) -> tuple:\n        \"\"\"\n        Given the X features and Y targets calculates the best split \n        for a decision tree\n        \"\"\"\n        # Creating a dataset for spliting\n        df = self.X.copy()\n        df['Y'] = self.Y\n\n        # Getting the GINI impurity for the base input \n        GINI_base = self.get_GINI()\n\n        # Finding which split yields the best GINI gain \n        max_gain = -1\n\n        # Default best feature and split\n        best_feature = None\n        best_value = None\n\n        for feature in self.features:\n            # Droping missing values\n            Xdf = df.dropna().sort_values(feature)\n            \n            # Sorting the values and getting the rolling average\n            xmeans = self.rolling_average(Xdf[feature].unique(), 2)\n            \n            for value in xmeans:\n                # Spliting the dataset \n                left_counts = Counter(Xdf[Xdf[feature]<value]['Y'])\n                right_counts = Counter(Xdf[Xdf[feature]>=value]['Y'])\n\n                # Getting the Y distribution from the dicts\n                y0_left, y1_left, y0_right, y1_right = left_counts.get(0, 0), left_counts.get(1, 0), \\\n                right_counts.get(0, 0), right_counts.get(1, 0)\n\n                # Getting the left and right gini impurities\n                gini_left = self.GINI_impurity(y0_left, y1_left)\n                gini_right = self.GINI_impurity(y0_right, y1_right)\n\n                # Getting the obs count from the left and the right data splits\n                n_left = y0_left + y1_left\n                n_right = y0_right + y1_right\n\n                # Calculating the weights for each of the nodes\n                w_left = n_left \/ (n_left + n_right)\n                w_right = n_right \/ (n_left + n_right)\n\n                # Calculating the weighted GINI impurity\n                wGINI = w_left * gini_left + w_right * gini_right\n\n                # Calculating the GINI gain \n                GINIgain = GINI_base - wGINI\n\n                # Checking if this is the best split so far \n                if GINIgain > max_gain:\n                    best_feature = feature\n                    best_value = value \n                    \n                    # Setting the best gain to the current one \n                    max_gain = GINIgain\n        return (best_feature, best_value)\n\n    \n    def fit(self):\n        \"\"\"\n        Recursive function to create the decision tree\n        \"\"\"\n        # Making a df from the data \n        df = self.X.copy()\n        df['Y'] = self.Y\n\n        # If there is GINI to be gained, we split further \n        if (self.depth < self.max_depth) and (self.num_observations >= self.min_samples_split):\n\n            # Getting the best split \n            best_feature, best_value = self.best_split()\n            \n            if best_feature is not None:\n                # Saving the best split to the current node \n                self.best_feature = best_feature\n                self.best_value = best_value\n                \n                # Getting the left and right nodes\n                left_df, right_df = df[df[best_feature]<=best_value].copy(), df[df[best_feature]>best_value].copy()\n                # Creating the left and right nodes\n                \n\n                left_node = Tree(\n                    left_df['Y'].values.tolist(), \n                    left_df[self.features], \n                    depth=self.depth + 1, \n                    max_depth=self.max_depth, \n                    min_samples_split=self.min_samples_split, \n                    )\n\n                self.left_node = left_node \n                self.left_node.fit()\n\n                right_node = Tree(\n                    right_df['Y'].values.tolist(), \n                    right_df[self.features], \n                    depth=self.depth + 1, \n                    max_depth=self.max_depth, \n                    min_samples_split=self.min_samples_split,\n                    )\n\n                self.right_node = right_node\n                self.right_node.fit()\n\n    def predict(self, X:pd.DataFrame):\n        predictions = []\n\n        for _, x in X.iterrows():\n            values = {}\n            for feature in self.features:\n                values.update({feature: x[feature]})\n        \n            predictions.append(self.predict_obs(values))\n        \n        return predictions\n\n    def predict_obs(self, values: dict) -> int:\n        \"\"\"\n        Predict the class given a set of features\n        \"\"\"\n        this_node = self\n        \n        while this_node.depth < this_node.max_depth:\n            # Traversing the nodes all the way to the bottom\n            best_feature = this_node.best_feature\n            best_value = this_node.best_value\n            \n            if this_node.num_observations < this_node.min_samples_split:\n                break \n            \n            if (values.get(best_feature) < best_value):\n                if self.left_node is not None:\n                    this_node = this_node.left_node\n            else:\n                if self.right_node is not None:\n                    this_node = this_node.right_node\n            \n        return this_node.yhat","f0d7320d":"min_sample_split = [100,50,20,10,5]\nmax_depth = [1,3,5,10,20]\n\nbest_custom_dt_accuracy = 0\nbest_min_sample_split = 0\nbest_max_depth = 0 \n\nfor min_sample in min_sample_split:\n    for m_depth in max_depth:\n        gini_tree = Tree(y_train.iloc[:300], x_train.iloc[:300], min_sample, m_depth)\n        gini_tree.fit()\n        dt_gini_prediction = gini_tree.predict(x_test)\n        dt_gini_correct_count = (dt_gini_prediction == np.array(y_test)).sum()\n        custom_dt_accuracy = round((dt_gini_correct_count \/ len(y_test)*100), 3)\n        print(f\"Min sample split {min_sample}, Max depth {m_depth}, DT accuracy - {custom_dt_accuracy}%\")\n        \n        if custom_dt_accuracy > best_custom_dt_accuracy:\n                    best_custom_dt_accuracy = custom_dt_accuracy\n                    best_min_sample_split = min_sample\n                    best_max_depth = m_depth\n                    \nprint(f'Best accuracy {best_custom_dt_accuracy}')\nprint(f'Best min sample split {best_min_sample_split}')\nprint(f'Best max tree depth {best_max_depth}')","bcdbbdce":"gini_tree = Tree(y_train,x_train, best_min_sample_split, best_max_depth)\ngini_tree.fit()\ndt_gini_prediction = gini_tree.predict(x_test)\ndt_gini_correct_count = (dt_gini_prediction == np.array(y_test)).sum()\ncustom_dt_accuracy = round((dt_gini_correct_count \/ len(y_test)*100), 3)\nprint(f\"Custom Decision Tree accuracy {custom_dt_accuracy}%\")","67d5e39d":"class MyPerceptron():\n    def __init__(\n        self, \n        max_iter = 100, #Maximum of iteration of 100% accuracy is not achieved\n        num_feat = 32 #Number of features in dataset\n    ):\n        self.max_iter = max_iter\n        self.h = np.random.random(num_feat + 1) #hypothesis contains random  weights values for each feature and bias (+1)\n#         self.h = np.array(np.zeros(num_feat+1), dtype=float)\n        self.best_h = None\n    \n    def compute_linear_score_with_(self, X, h):\n        s = None\n        s = (X * h[:-1]).sum(axis=1) + h[-1]\n        return s\n    \n    def predict_with_(self, X, h):\n        return np.sign(self.compute_linear_score_with_(X, h)).astype(int)\n    \n    def predict(self, X):\n        return self.predict_with_(X, self.best_h)\n    \n    def fit(self, X, y):\n        iteration = 1\n        min_error = 0\n        while True:\n            # predict using the current h\n            predicted = self.predict_with_(X, self.h)\n            # find errors\n            error_indexes = np.nonzero(predicted != y)[0]\n            # `nonzero` returns the indexes for multiple dimension array,\n            # Here only the indexes of the first (and only) dimension is \n            # concerned, therefore the [0]\n            # error indexes where prediction was wrong   \n\n           \n            if iteration == 1:\n                min_error = len(error_indexes)\n                self.best_h = self.h\n            else:\n                if len(error_indexes) < min_error:\n                    min_error = len(error_indexes)\n                    self.best_h = self.h\n\n            if len(error_indexes) > 0:        \n                rand = np.random.randint(len(error_indexes))\n                i = error_indexes[rand]\n                # i is a random index where prediction was wrong\n\n\n                self.h[:-1] += X[i] * float(y[i]) #Weigths update\n                \n                # To update the weights\n                # for example where prediction was wrong X = [4.6 3.4] and y = -1\n                # 1st multiply [4.6 3.4] by -1 = [-4.6 -3.4]\n                # 2nd add [-4.6 -3.4] to h which is [0. 1.] in first interation\n                # result = [-4.6 -2.4] these are updated weigthts for second iteration\n\n                # To update the bias\n                # This is equivalent to using the homogeneous representation of x, where the last element is one\n\n                self.h[-1] += float(y[i])\n                # For example in the first interation bias = 0, and as all prediction 1 the any random wrong prediction -1\n                # so we add 0 to -1 and second iteration bias = -1\n                iteration += 1\n                if iteration >= self.max_iter:\n                    break\n            else:\n                print(f\"{iteration} Train errors: {len(error_indexes)}\")\n                break # All the predictions are correct on the training data and there is nothing to adjust.","6e2bbee5":"#Converting dataframes to arrays for perceptron\nx_train_array = np.array(x_train)\n\ny_train_array = np.array(y_train)\ny_train_array[y_train_array == 0]  = -1\n\nx_test_array = np.array(x_test)\ny_test_array = np.array(y_test)","5505caef":"#Assessing perceptron performance\nperp = MyPerceptron()\nperp.fit(x_train_array, y_train_array)\nperp_prediction = perp.predict(x_test_array)\nperp_prediction[perp_prediction == -1]  = 0\n# print(list(perp_prediction))\nperp_correct_count = (perp_prediction == y_test_array).sum()\n\ncustom_perc_accuracy = round((perp_correct_count \/ len(y_test_array)*100),3)\nprint(f\"Custom Perceptron accuracy {custom_perc_accuracy}%\")","3ea07d20":"adaboost_model = {\n    \"weights\": [],\n    \"weak_predictors\": []}\nrng  = np.random.RandomState(42)\n\nmax_iter = 100\niter_num = 0\n\nsample_count = len(x_train)\n\nw = np.ones(sample_count) \/ sample_count #Weights in first iteration = 1\/ number of samples\n\nind = list(range(sample_count))\n\nwhile True:\n    \n    resample_ind = sorted(rng.choice(\n            ind, size = (sample_count,), replace = True, p=w)) \n            # It is necessary to generate a random index set,\n            # because we need to sample both the X- and y-array\n            # and need to keep the (X-y) correspondence consistent\n\n    x_resampled = x_train.iloc[resample_ind] \n    y_resampled = y_train.iloc[resample_ind]\n    \n    #converting to array\n    x_resampled_array = np.array(x_resampled)\n    y_resampled_array = np.array(y_resampled)\n    y_resampled_array[y_resampled_array == 0]  = -1\n    \n    g = MyPerceptron()\n    g.fit(x_resampled_array, y_resampled_array)\n    \n    # Calculate the classifier weight\n    err = (g.predict(x_resampled_array) != y_resampled_array).astype(float)\n    # print(f'error calcualtion {err}')\n    e = max(np.sum(err) \/ sample_count, 1e-6) \n    # max(..., 1e-6) is to ensure\n    # the following quotient is numerically safely defined.\n    # To calculate the weight for this weak classifier\n    al = 1\/2 * np.log((1 - e) \/ e)\n    \n    # Store the information of the model\n    adaboost_model[\"weights\"].append(al)\n    adaboost_model[\"weak_predictors\"].append(g)    \n    \n    # Re-weight the samples for the next step resampling\n    x_train_array = np.array(x_train)\n    y_train_array = np.array(y_train)\n    y_train_array[y_train_array == 0]  = -1\n\n    err_0 = (g.predict(x_train_array) != y_train_array).astype(float)\n    # To compute the sample weights for the next round of iteration\n    #   err_0 = [0, 1, 1, 0, 0, ...]\n    #   is a 0\/1 (float type) array with elements being 1.0 for \n    #   those samples on which the weak classifier has been wrong\n    dw = np.exp(al * (err_0 - 0.5) * 2)\n\n    w = dw * w\n    w = w \/ w.sum()\n\n    # Iteration update\n    iter_num += 1\n    \n    # Stop condition\n    if iter_num >= max_iter:\n        break\n","ce218fcb":"weighted_predictions = []\n\nfor p,w_i in zip(adaboost_model['weak_predictors'], adaboost_model['weights']):\n    x_test_array = np.array(x_test)\n    predictions = p.predict(x_test_array)\n    predictions = predictions * w_i #Multiply predictions by weigths of this perceptron\n#     predictions = np.where(predictions == 0, -1, predictions) * w_i \n    weighted_predictions.append(predictions) #Append array of weighted predictions to the container\n\n\nweighted_predictions = np.array(weighted_predictions)\n# print(weighted_predictions.shape)\n\nweighted_predictions = weighted_predictions.transpose() #Transpose array\n# print(weighted_predictions.shape) \n\nfinal_prediction = []\n\nfor i in weighted_predictions:\n    if i.mean() < 0: #Calcualte average of weighted prediction for each sample and compare with 0\n        final_prediction.append(0) \n    else:\n        final_prediction.append(1)\n\ncorrect_count = (np.array(final_prediction) == np.array(y_test)).sum()\ncustom_aboost_accuracy = round((correct_count \/ len(y_test)*100), 2)\nprint(f\"Custom Adabost with Perceptron accuracy  {custom_aboost_accuracy}%\")   ","76bf8d3f":"from sklearn.tree import DecisionTreeClassifier\n\ndtc = DecisionTreeClassifier()\ndtc.fit(x_train, y_train)\ndtc_prediction = dtc.predict(x_test)\ndtc_correct_count = (dtc_prediction == np.array(y_test)).sum()\n\nsklearn_dtc_accuracy = round((dtc_correct_count \/ len(y_test)*100), 3)\nprint(f\"Sklearn Decision Tree Classifier accuracy {sklearn_dtc_accuracy}%\")   ","266dcf09":"from sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier()\nrfc.fit(x_train, y_train)\nrfc_prediction = rfc.predict(x_test)\nrfc_correct_count = (rfc_prediction == np.array(y_test)).sum()\n\nsklearn_rfc_accuracy = round((rfc_correct_count \/ len(y_test)*100), 3)\nprint(f\"Sklearn Random Forest Classifier accuracy {sklearn_rfc_accuracy}%\") ","7383e763":"from sklearn.ensemble import AdaBoostClassifier\n\nabc = AdaBoostClassifier()\nabc.fit(x_train, y_train)\nabc_prediction = abc.predict(x_test)\nabc_correct_count = (abc_prediction == np.array(y_test)).sum()\n\nsklearn_abc_accuracy = round((abc_correct_count \/ len(y_test)*100), 3)\nprint(f\"Sklearn Adabost Classifier accuracy {sklearn_abc_accuracy}%\") ","55543355":"models = ['My Decision Tree','My Adaboost with Perceptron', 'Sklearn Decision Tree Classifier', \\\n           'Sklearn Random Forest Classifier', 'Sklearn Adaboost Classifier']\n\ncompare_models = {'Model': models, \\\n                  'Accuracy': [custom_dt_accuracy, custom_aboost_accuracy, sklearn_dtc_accuracy, \\\n                               sklearn_rfc_accuracy, sklearn_abc_accuracy]}\n\nmodels_df = pd.DataFrame(data = compare_models)\n\n# print(models_df)","98e2daf9":"import matplotlib.pyplot as plt\norder = [4, 3, 2, 1, 0]\n\ny = [models[i] for i in order]\nx = compare_models['Accuracy']\nx = [x[i] for i in order]\n\nplt.barh(y, x)\n\nplt.xlabel(\"Accuracy %\") \n\nplt.xlim([80, 100])\nfor i, v in enumerate(x):\n    plt.text(v + .5, i , str(v)+'%')\n    \nplt.show()","54ae7652":"After testing different hyperparameters, the best parameters applied to full training and testing datasets","fd8eef5c":"It is required categorise features which have object data type and convert values in these features to integers","e2c230d4":"As the goal of this modelling is to predict whether a patient will be readmitted to a hospital within 30 days after their discharge. All the records with readmitted feature as '<30' are categorised as 1 and rest as 0.","a52eed26":"As the computation of 70k records taking significant amount of time, the dataset has been reduced to 10000 records. The dataset is split into training and testing sets in 70\/30 proportion.","9d014989":"The CART Decision Tree Algorithm shows good results when comparing with adaboost model as well as out-of-the-box Sklearn library models.","d3359c57":"I observed that there are patients with multiple records(encounter_id). That means that all such patients will have readmission flag as true untill the last record when a patient is fully recovered or, I guess, dead. This will distort prediction and I decided to only use patient's individual records of their first admission.","ad3338aa":"Below is implementation of the CART (classification and regression tree) algorithm","a2d42f2a":"I conclude that weight, payer_code, medical speciality have many missing values and will not add value to the model.\n\nI also will exclude records with null values in race 2.72% and diagnosis features (diag_1 0.02%, diag_2 0.41%, diag_3 1.71%)","79e24f13":"All the records where a midication has been prescribed updated as 1 and if there was no medication as 0.","674ecd7e":"Below is the implementation of perceptron and adaboost models to compare performace with the CART decision tree algorithm","4e41031a":"Dropping records with null and Unknown\/Invalid values","ae899d84":"Laboratory test categorisation","ddf17e0e":"As there is no 'Other' category in gender and there are only 3 records with 'Unknown\/Invalid'.\nIt is decided to exclude these records from the dataset.","cdb5c8c6":"\"The data set represents 10 years (1999-2008) of clinical care at 130 US hospitals and integrated delivery networks. It includes over 50 features representing patient and hospital outcomes. Information was extracted from the database for encounters that satisfied the following criteria.\n\nThe data contains such attributes as patient number, race, gender, age, admission type, time in hospital, medical specialty of admitting physician, number of lab test performed, HbA1c test result, diagnosis, number of medication, diabetic medications, number of outpatient, inpatient, and emergency visits in the year before the hospitalization, etc.\"\n\nThe goal of this modelling is to predict whether a patient will be readmitted to a hospital within 30 days after their discharge.","6795b36d":"Testing different hyper parameters using smaller training dataset reduced to 500 records","24e6e1b4":"Observations:\n\n* Race feature has null and 'Other' values - Nulls can be converted to Other\n* Gender has values Unknown\/Invalid\n* Weight has null values\n* Payer_code has null values\n* Medical_specialty has null values\n* Features diag_1, diag_2, diag_3 have null values\n* Features examide and citoglipton and glimepiride-pioglitazone have one distinct value and can be excluded\n* Categorisation is required for many columns","a7fd9ab2":"Counting unique values in race feature","e8af27d3":"Below I compute accuracy for out-of-the-box classifiers from Sklearn library","7aa580c2":"Counting unique values in gender feature","a7c1b3ed":"The code below allows to assess distinct values in each feature one by one. As the output of the print is massive this code is commented out.","7e2617fc":"Dropping features with large number of missing values or one unique value","1da901a6":"Observation - Increasing maximum depth is over fitting the model, significantly increasing computation time and reducing accuracy on the evaluation dataset.","023ac5c7":"Replace null values with other in race feature","76ea2047":"Accuracy comparison of different models"}}