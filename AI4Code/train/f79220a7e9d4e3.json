{"cell_type":{"9a274861":"code","b1dcfe5c":"markdown"},"source":{"9a274861":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import datasets\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n\ndef Create_Dataset():\n\n    X, y = datasets.make_regression(n_samples=100, n_features=1, noise=10, random_state=0)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n    return X_train, X_test, y_train, y_test\n\n\ndef Linear_Regression_with_Ordinary_Least_Squares(X_train, X_test, y_train, y_test):\n\n    def variance(X):\n\n        return sum((xi - np.mean(X))**2 for xi in X)\n\n    def covariance(X,y):\n\n        return sum((xi-np.mean(X)) * (yi - np.mean(y)) for xi,yi in zip(X,y))\n\n    def plot(X, y, hypothesis, title):\n\n        plt.figure(figsize=(6, 4))\n        plt.style.use(\"ggplot\")\n        plt.scatter(X, y, color=\"green\", s=10, label=\"Actual\")\n        plt.plot(X, hypothesis, color=\"black\", linewidth=1, label=\"Prediction\")\n        plt.xlabel(\"x\")\n        plt.ylabel(\"y\")\n        plt.title(title, fontdict = {'fontsize' : 10}, fontweight=\"bold\")\n        plt.legend()\n        plt.show()\n\n    m = covariance(X_train, y_train) \/ variance(X_train)\n    b = np.mean(y_train) - m * np.mean(X_train)\n\n    hypothesis1 = m * X_train + b\n    hypothesis2 = m * X_test + b\n\n    print(\"\\nLinear Regression with Ordinary Least Squares\")\n    print(\"Coefficient: {}\".format(m), \"Intercept: {}\".format(b))\n    plot(X_train, y_train, hypothesis1, \"Linear Regression with Ordinary Least Squares (Train Data)\")\n    plot(X_test, y_test, hypothesis2, \"Linear Regression with Ordinary Least Squares (Test Data)\")\n\n\ndef Linear_Regression_with_Gradient_Descent(X_train, X_test, y_train, y_test):\n\n    class Simple_Linear_Regression:\n\n        def __init__(self, learning_rate=0.001):\n\n            self.learning_rate = learning_rate\n            self.weight = None\n            self.bias = None\n\n        def fit(self, X, y):\n\n            self.weight = np.random.randn(X.shape[1])\n            self.bias = np.random.randn()\n\n            derivative_weight = 0\n            derivative_bias = 0\n\n            for i in range(1000):\n\n                for xi, yi in zip(X, y):\n\n                    hypothesis = np.dot(xi, self.weight) + self.bias\n                    derivative_weight += (hypothesis - yi) * xi\n                    derivative_bias += (hypothesis - yi)\n\n                derivative_weight \/= X.shape[0]\n                derivative_bias \/= X.shape[0]\n\n                self.weight -= self.learning_rate * derivative_weight\n                self.bias -= self.learning_rate * derivative_bias\n\n        def predict(self, X):\n\n            return np.dot(X, self.weight) + self.bias\n\n        def mean_squared_error(self, y, y_prediction):\n\n            return np.mean((y_prediction - y) ** 2)\n\n        def R_Squared(self, y_prediction, y_test):\n\n            SSE = np.sum((y_test - y_prediction) ** 2)\n            y_avg = np.sum(y_test) \/ len(y_test)\n            SST = np.sum((y_prediction - y_avg) ** 2)\n            RSquared = 1 - (SSE \/ SST)\n            return RSquared\n\n        def plot(self, X, y, y_pred, title):\n\n            plt.figure(figsize=(6, 4))\n            plt.scatter(X, y, color=\"blue\", s=10, label= \"Actual\")\n            plt.plot(X, y_pred, color=\"black\", linewidth=1, label = \"Prediction\")\n            plt.xlabel(\"x\")\n            plt.ylabel(\"y\")\n            plt.title(title, fontdict={'fontsize': 10}, fontweight=\"bold\")\n            plt.legend()\n            plt.show()\n\n    lr = Simple_Linear_Regression(0.01)\n    lr.fit(X_train, y_train)\n    prediction_Xtrain = lr.predict(X_train)\n    prediction_Xtest = lr.predict(X_test)\n\n    print(\"\\nLinear Regression with Gradient Descent\")\n    print(\"Mean Squared Error: {}\".format(lr.mean_squared_error(y_test, prediction_Xtest)))\n    print(\"R Squared Value: {}\".format(lr.R_Squared(prediction_Xtest, y_test)))\n    print(\"Coefficient: {}\".format(lr.weight), \"Intercept: {}\".format(lr.bias))\n\n    lr.plot(X_train, y_train, prediction_Xtrain, \"Linear Regression with Gradient Descent (Train Data)\")\n    lr.plot(X_test, y_test, prediction_Xtest, \"Linear Regression with Gradient Descent (Test Data)\")\n\n\ndef Linear_Regression_with_Sklearn(X_train, X_test, y_train, y_test):\n\n    lr = LinearRegression()\n    lr.fit(X_train,y_train)\n    y_prediction = lr.predict(X_test)\n    print(\"\\nLinear Regression with Sklearn\")\n    print(\"Mean Squared Error: {}\". format(mean_squared_error(y_test, y_prediction)))\n    print(\"R Squared Value: {}\".format(r2_score(y_test, y_prediction)))\n    print(\"Coefficient: {}\".format(lr.coef_), \"Intercept: {}\".format(lr.intercept_))\n\n\nif __name__ == \"__main__\":\n\n    X_train, X_test, y_train, y_test = Create_Dataset()\n    Linear_Regression_with_Ordinary_Least_Squares(X_train, X_test, y_train, y_test)\n    Linear_Regression_with_Gradient_Descent(X_train, X_test, y_train, y_test)\n    Linear_Regression_with_Sklearn(X_train, X_test, y_train, y_test)\n","b1dcfe5c":"**SIMPLE LINEAR REGRESSION**\n\nWhen implementing simple linear regression, I have created a dataset of size 100 and separated them as train and test data. I have used the methods of **ordinary least squares** and **gradient descent** along with Sklearn. \nIt finds a linear relationship between the dependent variable (output y) and the independent variable (input x)."}}