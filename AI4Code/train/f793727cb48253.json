{"cell_type":{"fe90a6eb":"code","2da45cbd":"code","558eec00":"code","894fee10":"code","58b22c34":"code","59f9f184":"code","47590098":"code","3d996290":"code","2e25db7d":"code","9483ce1c":"code","8ce81681":"code","459ce8c9":"code","306c9e42":"code","67796d30":"code","cd401b99":"code","09102b83":"code","2f09f95a":"code","71d51f8f":"code","f92cda81":"code","ff0e34d4":"code","85b585ed":"code","e515450b":"code","bab9af22":"code","b62657db":"code","09b5d91a":"markdown","aabd8f83":"markdown","99f61ace":"markdown","c0e20347":"markdown","fbf10adc":"markdown","8af56f93":"markdown","b5ca7e4d":"markdown","5813a9f4":"markdown"},"source":{"fe90a6eb":"from sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport re\nfrom nltk.corpus import stopwords\nimport nltk\nimport os\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import confusion_matrix, classification_report,accuracy_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport zipfile","2da45cbd":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        ","558eec00":"zf1 = zipfile.ZipFile(\"\/kaggle\/input\/word2vec-nlp-tutorial\/labeledTrainData.tsv.zip\")\ntrain = pd.read_csv(zf1.open(\"labeledTrainData.tsv\"), sep=\"\\t\")\nzf2 = zipfile.ZipFile(\"\/kaggle\/input\/word2vec-nlp-tutorial\/testData.tsv.zip\")\ntest = pd.read_csv(zf2.open(\"testData.tsv\"), sep=\"\\t\")","894fee10":"train.head()","58b22c34":"test.head()","59f9f184":"train.shape, test.shape","47590098":"dataset=pd.concat([train, test], axis=0, sort=False)\ndataset.shape","3d996290":"len(dataset)","2e25db7d":"dataset[\"review\"].iloc[1]","9483ce1c":"#nltk.download('stopwords')\n\nstops = stopwords.words('english')\n#indNot = stops.index(\"not\")\n#del(stops[indNot])","8ce81681":"\ncorpus = []\nfor i in range(0, len(dataset)):\n    review = re.sub('[^a-zA-Z]', ' ', dataset['review'].iloc[i])\n    review = review.lower()\n    review = review.split()\n    ps = PorterStemmer()\n    review = [ps.stem(word) for word in review if not word in set(stops)]\n    review = ' '.join(review)\n    corpus.append(review)","459ce8c9":"'''i = 8\nreview = re.sub('[^a-zA-Z]', ' ', dataset['Review'].iloc[i])\nreview = review.lower()\nreview = review.split()\nps = PorterStemmer()\nreview = [ps.stem(word) for word in review if not word in set(stops)]\nreview = ' '.join(review)\ncorpus.append(review)'''","306c9e42":"#corpus","67796d30":"cv1 = CountVectorizer(max_features = 1500)\nX1 = cv1.fit_transform(corpus).toarray()\n#y = dataset.iloc[:, 1]","cd401b99":"cv2 = TfidfVectorizer(max_features = 1500)\nX2 = cv2.fit_transform(corpus).toarray()\n#y = dataset.iloc[:, 1]","09102b83":"#X1","2f09f95a":"X_train1=X1[:25000]\ny_train1=dataset[\"sentiment\"].iloc[:25000]\nX_test1=X1[25000:]","71d51f8f":"X_train2=X2[:25000]\ny_train2=dataset[\"sentiment\"].iloc[:25000]\nX_test2=X2[25000:]","f92cda81":"# Fitting Naive Bayes to the Training set\n\nclassifier1 = GaussianNB()\nclassifier1.fit(X_train1, y_train1)\n","ff0e34d4":"classifier2 = GaussianNB()\nclassifier2.fit(X_train2, y_train2)","85b585ed":"# Predicting the Test set results\ny_pred1 = classifier1.predict(X_test1)","e515450b":"y_pred2 = classifier2.predict(X_test2)","bab9af22":"mysubmission2=pd.read_csv(\"\/kaggle\/input\/word2vec-nlp-tutorial\/sampleSubmission.csv\")\nmysubmission2[\"sentiment\"]=y_pred2\nmysubmission2.to_csv(\"mysubmission2.csv\", index=False)","b62657db":"mysubmission=pd.read_csv(\"\/kaggle\/input\/word2vec-nlp-tutorial\/sampleSubmission.csv\")\nmysubmission[\"sentiment\"]=y_pred1\nmysubmission.to_csv(\"mysubmission.csv\", index=False)","09b5d91a":"## Importing the dataset","aabd8f83":"### to_csv","99f61ace":"### understanding of loop","c0e20347":"### Fitting Naive Bayes to the Training set","fbf10adc":"## Importing the libraries","8af56f93":"# Natural Language Processing","b5ca7e4d":"### prediction","5813a9f4":"### Cleaning the texts"}}