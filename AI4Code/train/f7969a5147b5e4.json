{"cell_type":{"f281c226":"code","b3558bb4":"code","ae03468f":"code","e8bdf4b2":"code","126001d3":"code","a85a39a2":"code","bbaaf299":"code","5bf155e5":"code","2c9487de":"code","ab94a126":"code","3c197b4e":"code","cf92e3bf":"code","7b499287":"code","b3a1f5c1":"code","575b70bc":"code","732f3c81":"markdown","c7368b5a":"markdown","35fd19ac":"markdown","e703c3f9":"markdown","b8b587f7":"markdown","993db69b":"markdown","45b7705a":"markdown","a6f20137":"markdown","400254ca":"markdown","4ea9647a":"markdown","9b7b9867":"markdown"},"source":{"f281c226":"import pandas as pd\nimport re\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import SnowballStemmer,WordNetLemmatizer\nstemmer=SnowballStemmer('french')\nlemma=WordNetLemmatizer()\nfrom string import punctuation\nimport unicodedata\nimport sys\nfrom nltk.corpus import stopwords\nfrom nltk import FreqDist\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.corpus import stopwords\nimport gensim\nfrom gensim import corpora\nimport pyLDAvis\nimport pyLDAvis.gensim\n%matplotlib inline","b3558bb4":"data = pd.read_csv('..\/input\/Trainv2.csv')[:500]","ae03468f":"stop_words = stopwords.words('french')\nstop_words.append('les')","e8bdf4b2":"def strip_accents(s):\n    return ''.join(c for c in unicodedata.normalize('NFD', s)\n                  if unicodedata.category(c) != 'Mn')","126001d3":"def clean_text(col):\n    result=[]\n    long = len(col) # nombre de lignes \u00e0 traiter\n    \n    for i in range(0,long):\n        sys.stdout.write('\\r'+str(i+1)+'\/'+str(long)) # message de progression\n        text=str(col[i])        \n        text=re.sub('(\\d*:\\d*)|(\\d*\/\\d*\/\\d*)|(\\')|(\\()|(\\))|(,)|( - )|(\\.)|(\/)|(@)|(\\|)',' ',text)        \n        text = \" \".join([stemmer.stem(i) for i in text.split() if (i.lower() not in stop_words) and (len(i) > 2)])\n        text = strip_accents(text)\n        result.append(text)\n        \n    return result","a85a39a2":"data['avi_text_cleaned'] = clean_text(data['avi_text'])","bbaaf299":"def freq_words(x, terms = 30):\n    all_words = ' '.join([text for text in x])\n    all_words = all_words.split()\n\n    fdist = FreqDist(all_words)\n    words_df = pd.DataFrame({'word':list(fdist.keys()), 'count':list(fdist.values())})    \n    \n    d = words_df.nlargest(columns=\"count\", n = terms) \n    plt.figure(figsize=(20,5))\n    ax = sns.barplot(data=d, x= \"word\", y = \"count\")\n    ax.set(ylabel = 'Count')\n    plt.show()","5bf155e5":"freq_words(data['avi_text_cleaned'])","2c9487de":"tokenized = pd.Series(data['avi_text_cleaned']).apply(lambda x: x.split())","ab94a126":"dictionary = corpora.Dictionary(tokenized)","3c197b4e":"doc_term_matrix = [dictionary.doc2bow(rev) for rev in tokenized]","cf92e3bf":"import warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) ","7b499287":"LDA = gensim.models.ldamodel.LdaModel\nlda_model = LDA(corpus=doc_term_matrix, id2word=dictionary, num_topics=39, random_state=100, chunksize=1000, passes=50)","b3a1f5c1":"lda_model.print_topics()","575b70bc":"pyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model, doc_term_matrix, dictionary)\nvis","732f3c81":"# Cr\u00e9ation d'un dictionnaire de termes de notre corpus","c7368b5a":"# m\u00e9thode de suppression des accents","35fd19ac":"# Cr\u00e9ation des topics selon un algo de lda https:\/\/fr.wikipedia.org\/wiki\/Allocation_de_Dirichlet_latente\n# L'algo prend du temps \u00e0 s'\u00e9xecuter","e703c3f9":"# Chargement des donn\u00e9es","b8b587f7":"# application du clean","993db69b":"# R\u00e9cup\u00e9ration des stops words, je rajoute les dans la liste","45b7705a":"# On convertit en bag of words (en matrice)","a6f20137":"# selection des 30 mots les plus fr\u00e9quents sur tous les AAPC et affichage sur un graphique","400254ca":"# On affiche sous forme de graphique les topics (cela utilise un algo de PCA pour transformer n dimensions en 2 dimensions)","4ea9647a":"# On tokenize les avis","9b7b9867":"# m\u00e9thode de clean"}}