{"cell_type":{"9df87f3e":"code","443e6d65":"code","bf3c91c4":"code","e5868deb":"code","f3e899f2":"code","f957714d":"code","a0866404":"code","8fc253c4":"code","8b12db55":"code","437f6647":"code","d45e3d53":"code","096a3bf5":"code","4d57140f":"code","b4d91db2":"code","342e3454":"code","1c646ba5":"code","0af024af":"code","a3dbd243":"code","f9420515":"code","7203ad97":"code","ccd60a14":"code","3a2956b1":"code","4274ce95":"code","9f2d4b6c":"code","0c3ce354":"code","824005f2":"markdown","ea703ba5":"markdown","897e0a85":"markdown","f40fa041":"markdown","d3138a1c":"markdown","01ea8898":"markdown","0aa03699":"markdown","bde8a57e":"markdown","0961c308":"markdown","daf16742":"markdown","494e9b40":"markdown","119b53dc":"markdown","2a0cbd7d":"markdown","8e9f3c1e":"markdown","6368733d":"markdown","7edeac20":"markdown","ff4f1b32":"markdown","0a7f44f3":"markdown","7a64ee52":"markdown","50736d55":"markdown","a506cd02":"markdown","205c0a29":"markdown","50394a20":"markdown","ee9e1965":"markdown","c4ae83c4":"markdown","570431bb":"markdown"},"source":{"9df87f3e":"#Importing library and initializing array\nimport torch\ntorch.tensor([[2,3,5],[1,2,9]])","443e6d65":"# declaring a random tensor\ntorch.rand(2,2)","bf3c91c4":"# assigning matrices to variables\na=torch.rand((3,5)) #np.random.randn(3,5)\na","e5868deb":"#check shape\na.shape","f3e899f2":"# matrix multiplication\na = torch.rand((2,2))\nb = torch.rand((2,2))\ntorch.matmul(a,b) # np.dot","f957714d":"# element wise multiplication\na*b #np.multiply(a,b)","a0866404":"# Matrices of zeroes and ones\n\na_torch =torch.zeros(2,2) #np.zeroes((2,2))\na_torch","8fc253c4":"b_torch =torch.ones(2,2) #np.ones((2,2))\nb_torch","8b12db55":"# identity matrix\nc_torch =torch.eye(2) # np.identity(2)\nc_torch","437f6647":"# Pytorch tensor to numpy and vise versa\n\nimport numpy as np\nc_numpy= np.identity(2)\nd_torch = torch.from_numpy(c_numpy)\nd_torch","d45e3d53":"# converting torch tensor to numpy array\nc = d_torch.numpy()\nc","096a3bf5":"a= torch.Tensor([2])\nb= torch.Tensor([-4])\nc= torch.Tensor([-2])\nd= torch.Tensor([2])","4d57140f":"e=a+b\nf=c*d","b4d91db2":"g=e*f\nprint(g)","342e3454":"# Initialize x, y and z to values 4, -3 and 5\nx = torch.tensor(4., requires_grad=True)\ny = torch.tensor(-3., requires_grad=True)\nz = torch.tensor(5., requires_grad=True)\n\n# Set q to sum of x and y, set f to product of q with z\nq = x+y\nf = q*z\n\n# Compute the derivatives\nf.backward()\n\n# Print the gradients\nprint(\"Gradient of x is: \" + str(x.grad))\nprint(\"Gradient of y is: \" + str(y.grad))\nprint(\"Gradient of z is: \" + str(z.grad))","1c646ba5":"x = torch.rand(1000,1000,requires_grad=True)\ny = torch.rand(1000,1000,requires_grad=True)\nz = torch.rand(1000,1000,requires_grad=True)\n# Multiply tensors x and y\nq = torch.matmul(x,y)\n\n# Elementwise multiply tensors z with q\nf = z*q\n\nmean_f = torch.mean(f)\n\n# Calculate the gradients\nmean_f.backward()","0af024af":"input_layer = torch.rand(10)\nw1= torch.rand(10,20)\nw2= torch.rand(20,20)\nw3= torch.rand(20,4)\n\nh1= torch.matmul(input_layer,w1)\nh2= torch.matmul(h1,w2)\n\noutput_layer= torch.matmul(h2,w3)\nprint(output_layer)","a3dbd243":"import torch.nn as nn\ninput_layer=torch.rand(784)\n# Initialize the weights of the neural network\nweight_1 = torch.rand(784,200)\nweight_2 = torch.rand(200,10)\n\n# Multiply input_layer with weight_1\nhidden_1 = torch.matmul(input_layer, weight_1)\n\n# Multiply hidden_1 with weight_2\noutput_layer = torch.matmul(hidden_1, weight_2)\nprint(output_layer)","f9420515":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        \n        # Instantiate all 2 linear layers  \n        self.fc1 = nn.Linear(784, 200)\n        self.fc2 = nn.Linear(200, 10)\n\n    def forward(self, x):\n      \n        # Use the instantiated layers and return x\n        x = self.fc1(x)\n        x = self.fc2(x)\n        return x","7203ad97":"import torch\nimport torch.nn as nn\n\n#input_layer = torch.rand(4)\ninput_layer=torch.tensor([[-0.3780, -0.1040, -1.3337,  0.6797]])\nweight_1 = torch.rand(4,4)\nweight_2 = torch.rand(4,4)\nweight_3 = torch.rand(4,4)\n\n#print(input_layer)\n#print(weight_1)\n#print(weight_2)\n#print(weight_3)\n\n# Calculate the first and second hidden layer\nhidden_1 = torch.matmul(input_layer, weight_1)\nhidden_2 = torch.matmul(hidden_1, weight_2)\n\n# Calculate the output\nprint(torch.matmul(hidden_2, weight_3))\n\n# Calculate weight_composed_1 and weight\nweight_composed_1 = torch.matmul(weight_1, weight_2)\nweight = torch.matmul(weight_composed_1, weight_3)\n\n# Multiply input_layer with weight\nprint(torch.matmul(input_layer, weight))","ccd60a14":"relu =nn.ReLU()\n# Apply non-linearity on hidden_1 and hidden_2\nhidden_1_activated = relu(torch.matmul(input_layer, weight_1))\nhidden_2_activated = relu(torch.matmul(hidden_1_activated, weight_2))\nprint(torch.matmul(hidden_2_activated, weight_3))\n\n# Apply non-linearity in the product of first two weights. \nweight_composed_1_activated = relu(torch.matmul(weight_1, weight_2))\n\n# Multiply `weight_composed_1_activated` with `weight_3\nweight = torch.matmul(weight_composed_1_activated, weight_3)\n\n# Multiply input_layer with weight\nprint(torch.matmul(input_layer, weight))","3a2956b1":"# Initialize weight_1 and weight_2 with random numbers\nweight_1 = torch.rand(4, 6)\nweight_2 = torch.rand(6, 2)\n\n# Multiply input_layer with weight_1\nhidden_1 = torch.matmul(input_layer, weight_1)\n\n# Apply ReLU activation function over hidden_1 and multiply with weight_2\nhidden_1_activated = relu(hidden_1)\nprint(torch.matmul(hidden_1_activated, weight_2))","4274ce95":"# CE loss in pytorch\n\nlogits = torch.tensor([[3.2,5.1,-1.7]])\nground_truth = torch.tensor([0])\n\ncriterion = nn.CrossEntropyLoss()\nloss = criterion(logits, ground_truth)\nprint(loss)\n","9f2d4b6c":"# Initialize the scores and ground truth\nlogits = torch.tensor([[-1.2,0.12,4.8]])\nground_truth = torch.tensor([2])\n\n# Instantiate cross entropy loss\ncriterion = nn.CrossEntropyLoss()\n\n# Compute and print the loss\nloss = criterion(logits,ground_truth)\nprint(loss)","0c3ce354":"# Initialize logits and ground truth\nlogits = torch.rand(1,1000)\nground_truth = torch.tensor([111])\n\n# Instantiate cross-entropy loss\ncriterion =nn.CrossEntropyLoss()\n\n# Calculate and print the loss\nloss = criterion(logits,ground_truth)\nprint(loss)","824005f2":"# Loss Functions\n\n* initialize weights with random weights\n* Do a forward Pass\n* Calculate loss function \n* Calculate the gradient using back propagation\n* Change the weights based on gradients\n\nCost function: How wrong the model is performing\n\nFor regression : least squared loss\nfor classification : Softmax cross entropy loss\nfor more complex problems - complicated loss functions\n\nloss functions must be differentiable otherwise we wont be able to compute its gradient\n\naccuracy is not differentiable , so we use softmax\n\nSoftmax transfforms numbers into probabilities\n\nCross Entrop loss\n\nL =-ln(P)\n","ea703ba5":"connected neural networks is the simplest form of neural networks\n\nscikit learn works with vectorized features and not images or audio, text etc\n\npreviously people extracted these vector features from rich fromats by using some other algorithm\n\nso we had to optimise two algortihms one to extract and other to predict\n\nwhich werent even releated\n","897e0a85":"## Calculating loss function in PyTorch\nYou are going to code the previous exercise, and make sure that we computed the loss correctly. Predicted scores are -1.2 for class 0 (cat), 0.12 for class 1 (car) and 4.8 for class 2 (frog). The ground truth is class 2 (frog). Compute the loss function in PyTorch.\n\nClass\tPredicted Score\nCat\t-1.2\nCar\t0.12\nFrog\t4.8","f40fa041":"for fully cnnected network = nn.linear\nfirst parameter is number of units in current layer\nsecod parameter is number of units in next layer","d3138a1c":"x.grad to give the derivative","01ea8898":"## Forward propagation \/ Forward pass","0aa03699":"# Activation function?\n","bde8a57e":"# Your first neural network\nYou are going to build a neural network in PyTorch, using the hard way. Your input will be images of size (28, 28), so images containing 784 pixels. Your network will contain an input_layer (provided for you), a hidden layer with 200 units, and an output layer with 10 classes. The input layer has already been created for you. You are going to create the weights, and then do matrix multiplications, getting the results from the network.","0961c308":"## Deep Learning\n\n* Deep learning is sub family of machine learning\n* Traditional methods use one model for feature extraction and followed by machine learning classifier\n* Neural networks do the optimization all together\n","daf16742":"# Backpropagation by auto-differentiation\n- core concept\n- derivatives are rate of change in a function\n- steepness of the function - more rapidly changing\n- rules\n- gradient is a multi variable generalization of derivative, derivative considering many variables\n- since neural net has many variables, so we use term gradient and not derivative\n- back propagation updates the weights until suitable weights are found\n\n\nHaving understanding about derivative rules like addition, multiplication , etc is important. Chain rule is the most important rule in deep learning\n\nGradient is multi variable generalization of derivative, since neural networks have many variables, we call derivatives here as gradients\n","494e9b40":"## Loss function of random scores\nIf the neural network predicts random scores, what would be its loss function? Let's find it out in PyTorch. The neural network is going to have 1000 classes, each having a random score. For ground truth, it will have class 111. Calculate the loss function.","119b53dc":"## Why Pytorch?\n- simplicity\n- strong GPU support\n- already implemented many deep learning algorithms\n- Strong OOP support\n- good at calculating dervatives and gradients\n- very similar to numpy","2a0cbd7d":"## Tensor Basics\n\nTensor is an array with arbitrary number of dimensions","8e9f3c1e":"requires_grad flag to true in order to tell pytorch that we need derivatives","6368733d":"building neural net this way is quite complex, so pytorch has class nn class which can create neural nets\n","7edeac20":"Calculating gradients in PyTorch\nRemember the exercise in forward pass? Now that you know how to calculate derivatives, let's make a step forward and start calculating the gradients (derivatives of tensors) of the computational graph you built back then. We have already initialized for you three random tensors of shape (1000, 1000) called x, y and z. First, we multiply tensors x and y, then we do an elementwise multiplication of their product with tensor z, and then we compute its mean. In the end, we compute the derivatives.\n\nThe main difference from the previous exercise is the scale of the tensors. While before, tensors x, y and z had just 1 number, now they each have 1 million numbers.\n","ff4f1b32":"more accurate the network : smaller the loss","0a7f44f3":"### Summary\n\n* **torch.matmul(a,b)** - multiplies torch tensor a and b\n* **a* b**              - elementwis emultiplication of tensors\n* **torch.eye(n)**      - creates identity tensor of shape (n,n)\n* **torch.zeroes(n,m)** - creates torch tensor of zeroes with shape (n,m)\n* **torch.ones(n,m)**   - creates torch tensor of ones with shape (n,m)\n* **torch.rand(n,m)**   - creates a random torch tensor with shape (n,m)\n* **torch.tensor(l)**   - creates a torch tensor based on list l\n","7a64ee52":"# Your first PyTorch neural network\nYou are going to build the same neural network you built in the previous exercise, but now using the PyTorch way. As a reminder, you have 784 units in the input layer, 200 hidden units and 10 units for the output layer.","50736d55":"hidden =in X w1\noutput = hiden Xw1\n\noutput =input x (w1 x w2)\n\nis same\nmatrix multiplication is a linear transformation\nconvert multilaye rinto single layer\n\nneural nets are not that powerful, using them makes possible only to separate linearly separable datasets,\n\ni.e in 2d seperable by a line\nin 3d by a plane \nand in higher dimension by a hyper plane\n\nbut most datasets are not linearly separable\n\nto capture on linear relationships between the data?\n\nto do this we introduce non linearity via activation functions\n\neg signmoid, tanh, ReLU, Leaky ReLu, Maxout , ELU\n\nActivation functions are non linear functions which are inserted in each layer of the nerural network making NN non linear and making them to deal with highly non linear datasets, thus making them much more powerful\n\nMost common : Relu rectified linear unit ReLU =max(0,x)\n\nrelu can be initialised as an object of nn\n\nrelu =nn.ReLU()\nrelu(tensor_1)\n","a506cd02":"# ReLU activation again\nNeural networks don't need to have the same number of units in each layer. Here, you are going to experiment with the ReLU activation function again, but this time we are going to have a different number of units in the layers of the neural network. The input layer will still have 4 features, but then the first hidden layer will have 6 units and the output layer will have 2 units\n","205c0a29":"neural network have \ninput layer\nhidden layer\noutput layer\n\njob of hidden layer is to get good features and job of output layer is to give good results\n\nThey have single algorithm which finds good features and classifies them\n\neach unit\/ neuron is connected with each other, these connections are assosicated with weights and represented by a matrix (tensor)","50394a20":"Pytorch code in backend gets converted into computational graphs like this. They make compuattion of derivatives and gradients much easier","ee9e1965":"Here a,b,c,d nodes are the first layer. e,f are the middle layer and g is the final ourput. Forward propagation is just getting following the graph and getting the output\n\n* Node e perfroms \"+\" on the inputs\n* Node f perfroms \"x\" on the inputs\n* Node g performs \"x\" on the inputs","c4ae83c4":"# Neural networks\nLet us see the differences between neural networks which apply ReLU and those which do not apply ReLU. We have already initialized the input called input_layer, and three sets of weights, called weight_1, weight_2 and weight_3.\n\nWe are going to convince ourselves that networks with multiple layers which do not contain non-linearity can be expressed as neural networks with one layer.\n\nThe network and the shape of layers and weights is shown below.","570431bb":"Only Tensors of floating point dtype can require gradients, so initalize with folating points"}}