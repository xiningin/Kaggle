{"cell_type":{"8a50753f":"code","a6003067":"code","39f4a396":"code","0c08f269":"code","57ad8e32":"code","a6e91633":"code","f12033d9":"code","c65c7d87":"code","4779b7ec":"code","35501117":"code","fbc9e4e8":"code","37ea19a5":"code","1ac5142b":"code","5a84f892":"code","023ae167":"code","9d5ddaaa":"code","5269b659":"code","7efe20a5":"markdown","a894d534":"markdown","b5280679":"markdown","821d3fa6":"markdown","5eed2d6c":"markdown","e79f5f41":"markdown","e0acc7c8":"markdown","f1ba6e81":"markdown","4e2806c5":"markdown","353128a8":"markdown","563c24bd":"markdown","99f6591f":"markdown","26c310ab":"markdown","3998a36d":"markdown","149a40b1":"markdown","f207b1a5":"markdown","878ed738":"markdown","851186c5":"markdown","e261182f":"markdown","c79712be":"markdown","c24be019":"markdown","69c17bd8":"markdown","f511818b":"markdown"},"source":{"8a50753f":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","a6003067":"df = pd.read_parquet('..\/input\/ubiquant-parquet\/train.parquet')\ndf","39f4a396":"print('Number of rows in the train.csv file: ', len(df))","0c08f269":"df.time_id.unique()","57ad8e32":"len(df.time_id.unique())","a6e91633":"df.time_id.value_counts().sort_index()","f12033d9":"missing_time_ids = []\nfor t in range(1220):\n    if t not in df.time_id.unique():\n        missing_time_ids.append(t)\n        \nprint('Missing time_ids: ', missing_time_ids)","c65c7d87":"len(df.row_id.unique()) == len(df)","4779b7ec":"unique_investments = sorted(df.investment_id.unique())\nprint('Number of investment ids: ', len(unique_investments))","35501117":"df.investment_id.value_counts().sort_index()","fbc9e4e8":"missing_investment_ids = []\nfor iid in range(3774):\n    if iid not in df.investment_id.unique():\n        missing_investment_ids.append(iid)\n        \nprint('Missing investment_ids: ', missing_investment_ids)","37ea19a5":"df.groupby('time_id')['investment_id'].unique()","1ac5142b":"sample_time_id = 0\nassert sample_time_id not in missing_time_ids\n\nsample_df = df[df.time_id == sample_time_id]\nsample_df","5a84f892":"sample_df.investment_id.value_counts()","023ae167":"sample_investment_id = 30\nassert sample_investment_id not in missing_investment_ids\n\nsample_df = df[df.investment_id == sample_investment_id]\nsample_df","9d5ddaaa":"plt.figure(figsize=(12,6));\nsample_df.set_index('time_id').target.plot();","5269b659":"import ubiquant\nenv = ubiquant.make_env()   # initialize the environment\niter_test = env.iter_test()    # an iterator which loops over the test set and sample submission\nfor (test_df, sample_prediction_df) in iter_test:\n    print(test_df)\n    sample_prediction_df['target'] = 0  # make your predictions here\n    env.predict(sample_prediction_df)   # register your predictions","7efe20a5":"## Let's look at a single `investment_id` across `time_id`s. ","a894d534":"## Imports","b5280679":"> We can see that not all investment have data in all time IDs.","821d3fa6":"# EDA","5eed2d6c":"> It takes time to load the `train.csv` file and usually the kernel crashes in the process of doing so.","e79f5f41":"# Using Time Series API","e0acc7c8":"> Clearly the number of data points (rows) in each `time_id` is not constant. ","f1ba6e81":"> `time_id`: The ID code for the time the data was gathered. The time IDs are in order, but the real time between the time IDs is not constant and will likely be shorter for the final private test set than in the training set. \n\n> Yes the IDs are in order from 0-1219 with 8 missing (?) time_ids.\n\n> One time id may belong to 1st Jan 2:00 IST, the next one can be 4th Jan 12:00 IST, the other one 5th Jan 16:00 IST and so on.","4e2806c5":"> The following `time_id`s are not present. I don't think it should be an issue since we anyway don't have a constant gap between consecutive `time_id`s. ","353128a8":"> Total number of unique investments are 3579 while the last `investment_id` is 3773. There must be missing `investment_id`s. \n\n> I don't think this to be an issue as well. ","563c24bd":"> Features `f_0` to `f_299` are features for the model per `time_id`. \n\n> `investment_id` can be a feature, feature with extra weightage, handled seperately by individual models (but then there will be a lot of models) or part of the feature vector for the same model. ","99f6591f":"> Clearly there is a time series trend when we look at an `investment_id` across time. \n\n> There are missing `time_id`s which is needed to be handled. \n\n> Clearly the `target` values are not scaled but we will be using LightGBM so scaling the data is not crucial. ","26c310ab":"> We get dataframes with shape `(n row x 302 columns)` where `n rows` are`row_id`s. Each `row_id` belong to the same `time_id` so at each iteration we get data for different `investment_id`s. So we need to predict the targets for each `investment_id`s for the given `time_id`. ","3998a36d":"## Load Dataset\n\nUsing parquet format of the dataset allows for fast loading with lower memory footprint. Thanks to Rob and check out his kernel here: https:\/\/www.kaggle.com\/robikscube\/fast-data-loading-and-low-mem-with-parquet-files","149a40b1":"### `investment_id`","f207b1a5":"### `time_id`","878ed738":"This is a quick stab at understanding the dataset and might be useful for folks who are starting out with this competition, are new to time-series (like me) or want a quick look at the fundamentals of the data.","851186c5":"## Let's look at all the `investment_id`s in a single `time_id`. \n\nNote that few `investment_id`s may be missing in a given `time_id`.","e261182f":"# Conclusion","c79712be":"> `row_id`: It's a unique identifier for each row. The id is in the format of `x_y` where `x` is the unique `time_id` and `y` is the unique `investment_id`.","c24be019":"> The following `investment_id`s are not present.","69c17bd8":"### `row_id`","f511818b":"> There's one `investment_id` per `time_id`."}}