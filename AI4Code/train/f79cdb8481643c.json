{"cell_type":{"45597356":"code","1fa4d2f7":"code","9364bebe":"code","08989313":"code","09a5b6dc":"code","75d17ed3":"code","7c502d18":"code","872de2ae":"code","802fd0f5":"code","c1de8034":"code","80a4dbf2":"code","8c8bdefc":"code","ecd45d62":"code","d47768d1":"code","c0ddae78":"code","51b8309e":"code","c3fc8c90":"code","92d6b308":"code","001455ac":"code","17ae7be8":"code","798b1ce3":"code","1c429b4d":"code","40056192":"code","221a8c96":"code","8ffa3edb":"code","a7bad7a7":"markdown","29e92ec0":"markdown","990627d8":"markdown","165d0716":"markdown","79f5ad57":"markdown"},"source":{"45597356":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1fa4d2f7":"!pip install -q git+https:\/\/github.com\/tensorflow\/examples.git","9364bebe":"import tensorflow as tf\n\nimport tensorflow_datasets as tfds\nfrom tensorflow_examples.models.pix2pix import pix2pix\n\nimport os\nimport time\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE","08989313":"tr1 = tf.keras.preprocessing.image_dataset_from_directory('\/kaggle\/input\/grumpify-cat\/grumpifycat\/class_0\/',\n    color_mode='rgb', batch_size=1, label_mode=None, image_size=(256, 256), shuffle=True, seed=42, interpolation='nearest')\n\ntr2 = tf.keras.preprocessing.image_dataset_from_directory('\/kaggle\/input\/grumpify-cat\/grumpifycat\/class_1\/',\n    color_mode='rgb', batch_size=1, label_mode=None, image_size=(256, 256), shuffle=True, seed=42, interpolation='nearest')","09a5b6dc":"# normalizing the images to [-1, 1]\ndef normalize(image):\n    image = tf.cast(image, tf.float32)\n    image = (image \/ 127.5) - 1\n    return image","75d17ed3":"def convert_to_grayscale(image):\n    image = tf.image.rgb_to_grayscale(image)\n    return image","7c502d18":"def preprocess_image_train(image):\n    image = convert_to_grayscale(image)\n    # image = random_jitter(image)\n    image = normalize(image)\n    return image","872de2ae":"def preprocess_image_test(image):\n    image = normalize(image)\n    return image","802fd0f5":"tr1 = tr1.map(\n    preprocess_image_train, num_parallel_calls=AUTOTUNE).cache().shuffle(2048)\n\ntr2 = tr2.map(\n    preprocess_image_train, num_parallel_calls=AUTOTUNE).cache().shuffle(2048)","c1de8034":"sample_tr1 = next(iter(tr1))\nsample_tr2 = next(iter(tr2))","80a4dbf2":"plt.title('tr1')\nplt.imshow(sample_tr1[0].numpy()[:, :, 0] * 0.5 + 0.5, cmap='gray')","8c8bdefc":"plt.title('tr2')\nplt.imshow(sample_tr2[0].numpy()[:, :, 0] * 0.5 + 0.5, cmap='gray')","ecd45d62":"OUTPUT_CHANNELS = 3\n# considering t1 and t2 domains\n# Generator g(t1-->t2) and f(t2-->t1)\ngenerator_g = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\ngenerator_f = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n\n# Discriminator x(t1 domain, trains generator f) and y(t2 domain, trains generator g)\ndiscriminator_x = pix2pix.discriminator(norm_type='instancenorm', target=False)\ndiscriminator_y = pix2pix.discriminator(norm_type='instancenorm', target=False)","d47768d1":"to_tr2 = generator_g(sample_tr1)\nto_tr1 = generator_f(sample_tr2)\n\nplt.figure(figsize=(8, 8))\ncontrast = 8\n\nimgs = [sample_tr1, to_tr2, sample_tr2, to_tr1]\ntitle = ['tr1', 'To tr2', 'tr2', 'To tr1']\n\nfor i in range(len(imgs)):\n    plt.subplot(2, 2, i+1)\n    plt.title(title[i])\n    if i % 2 == 0:\n        plt.imshow(imgs[i][0].numpy()[:, :, 0] * 0.5 + 0.5, cmap='gray')\n    else:\n        plt.imshow(imgs[i][0].numpy()[:, :, 0] * 0.5 * contrast + 0.5, cmap='gray')\nplt.show()","c0ddae78":"# Binary Cross Entropy loss will be used, since Discriminator is a classifier\nLAMBDA = 10\nloss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)","51b8309e":"def discriminator_loss(real, generated):\n    # Real loss is calculated from the real data\n    real_loss = loss_obj(tf.ones_like(real), real)\n    # Generated loss is calculated on the generated data from generator\n    generated_loss = loss_obj(tf.zeros_like(generated), generated)\n    # sum both losses\n    total_disc_loss = real_loss + generated_loss\n    return total_disc_loss * 0.5","c3fc8c90":"def generator_loss(generated):\n    # generator will optimize itself only if it fails to fool the discriminator \n    # hence loss is calculated against the real data\n    return loss_obj(tf.ones_like(generated), generated)","92d6b308":"def calc_cycle_loss(real_image, cycled_image):\n    # how much does the real image gets changed after going through both generators\n    loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n    return LAMBDA * loss1","001455ac":"def identity_loss(real_image, same_image):\n    # Loss against same domain\n    loss = tf.reduce_mean(tf.abs(real_image - same_image))\n    return LAMBDA * 0.5 * loss","17ae7be8":"# Optimizers for generators and discriminators\ngenerator_g_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ngenerator_f_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n\ndiscriminator_x_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ndiscriminator_y_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)","798b1ce3":"EPOCHS = 30","1c429b4d":"def generate_images(model, test_input, expected_output):\n    \"\"\" Used for generating prediction images \"\"\"\n    prediction = model(test_input)\n\n    plt.figure(figsize=(10, 10))\n\n    display_list = [test_input[0], prediction[0], expected_output[0]]\n    title = ['Input Image', 'Predicted Image', 'Expected Image']\n\n    for i in range(3):\n        plt.subplot(1, 3, i+1)\n        plt.title(title[i])\n        # getting the pixel values between [0, 1] to plot it.\n        plt.imshow(display_list[i].numpy()[:, :, 0] * 0.5 + 0.5, cmap='gray')\n        plt.axis('off')\n    plt.show()","40056192":"@tf.function\ndef train_step(real_x, real_y):\n    # persistent is set to True because the tape is used more than\n    # once to calculate the gradients.\n    with tf.GradientTape(persistent=True) as tape:\n        # Generator G translates X -> Y\n        # Generator F translates Y -> X.\n\n        # gen(real) --> fake and gen(fake) --> cycled\n        fake_y = generator_g(real_x, training=True)\n        cycled_x = generator_f(fake_y, training=True)\n\n        fake_x = generator_f(real_y, training=True)\n        cycled_y = generator_g(fake_x, training=True)\n\n        # same_x and same_y are used for identity loss.\n        same_x = generator_f(real_x, training=True)\n        same_y = generator_g(real_y, training=True)\n\n        # discriminator classifies the real data\n        disc_real_x = discriminator_x(real_x, training=True)\n        disc_real_y = discriminator_y(real_y, training=True)\n\n        # discriminator classifies the generated data\n        disc_fake_x = discriminator_x(fake_x, training=True)\n        disc_fake_y = discriminator_y(fake_y, training=True)\n\n        # calculate the loss\n        gen_g_loss = generator_loss(disc_fake_y)\n        gen_f_loss = generator_loss(disc_fake_x)\n\n        total_cycle_loss = calc_cycle_loss(real_x, cycled_x) + calc_cycle_loss(real_y, cycled_y)\n\n        # Total generator loss = adversarial loss + cycle loss\n        total_gen_g_loss = gen_g_loss + total_cycle_loss + identity_loss(real_y, same_y)\n        total_gen_f_loss = gen_f_loss + total_cycle_loss + identity_loss(real_x, same_x)\n\n        disc_x_loss = discriminator_loss(disc_real_x, disc_fake_x)\n        disc_y_loss = discriminator_loss(disc_real_y, disc_fake_y)\n\n    # Calculate the gradients for generator and discriminator\n    generator_g_gradients = tape.gradient(total_gen_g_loss, \n                                        generator_g.trainable_variables)\n    generator_f_gradients = tape.gradient(total_gen_f_loss, \n                                        generator_f.trainable_variables)\n\n    discriminator_x_gradients = tape.gradient(disc_x_loss, \n                                            discriminator_x.trainable_variables)\n    discriminator_y_gradients = tape.gradient(disc_y_loss, \n                                            discriminator_y.trainable_variables)\n\n    # Apply the gradients to the optimizer\n    generator_g_optimizer.apply_gradients(zip(generator_g_gradients, \n                                            generator_g.trainable_variables))\n\n    generator_f_optimizer.apply_gradients(zip(generator_f_gradients, \n                                            generator_f.trainable_variables))\n\n    discriminator_x_optimizer.apply_gradients(zip(discriminator_x_gradients,\n                                                discriminator_x.trainable_variables))\n\n    discriminator_y_optimizer.apply_gradients(zip(discriminator_y_gradients,\n                                                discriminator_y.trainable_variables))","221a8c96":"for epoch in range(EPOCHS):\n    start = time.time()\n    for image_x, image_y in tf.data.Dataset.zip((tr1, tr2)):\n        train_step(image_x, image_y)\n    \n    # Using a consistent image (sample_horse) so that the progress of the model\n    # is clearly visible.\n    generate_images(generator_g, sample_tr1, sample_tr2)\n\n#   if (epoch + 1) % 5 == 0:\n#     ckpt_save_path = ckpt_manager.save()\n#     print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n#                                                          ckpt_save_path))\n\n    print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n                                                      time.time()-start))","8ffa3edb":"# Run the trained model on the test dataset\nfor inp in tr1.take(5):\n    generate_images(generator_g, inp, sample_tr2)","a7bad7a7":"### Define Loss and Optimizers for models","29e92ec0":"**Install Required tensorflow_example library and Import required libraries**","990627d8":"### Initialize Generators and Discriminators","165d0716":"### Training and Predictions on Trained model","79f5ad57":"### Load Data and Preprocess "}}