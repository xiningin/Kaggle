{"cell_type":{"f742a248":"code","95dd94e2":"code","524d661b":"code","e98a9bea":"code","09201bf1":"code","84a26a01":"code","4e1dba2d":"code","e2fcd510":"code","63e8cb90":"code","0541ed52":"code","4e4de77f":"code","3f878f33":"code","9531e4a2":"code","cd105aa3":"code","e42576eb":"code","260a679f":"code","013c121d":"code","509069fe":"code","55076184":"code","6c8f3e4e":"code","d1728a5c":"code","e146d2e7":"code","e1d6096c":"code","c598d5fd":"code","99d251af":"code","736fef82":"code","7eab6ac7":"code","58a37fb6":"code","4276e809":"code","f1b3f7b4":"code","2ccdc2cd":"code","90d5e12f":"code","011e78d2":"code","ed08f269":"code","fc0e4f1d":"code","1c8c3716":"code","1990c094":"code","63b662a3":"markdown","153fbbfd":"markdown","1765b081":"markdown","1dd473bc":"markdown","4c67fc8b":"markdown","17abe739":"markdown","3a3f99b6":"markdown","2a4217cf":"markdown","20eb1bd4":"markdown","7bbacdee":"markdown","d1b5ab6b":"markdown","5942c218":"markdown","a2d0cecb":"markdown","a0ac3302":"markdown","875fffe0":"markdown","7a28ce81":"markdown","3d39797b":"markdown","81ca3789":"markdown","d3290167":"markdown","e9ea3381":"markdown","ce88a2dd":"markdown","a755faa3":"markdown","41290fdb":"markdown","e919e7ed":"markdown","d7a8a591":"markdown","8fb343ce":"markdown","d046365f":"markdown","a66327eb":"markdown","e1b21b2b":"markdown","70a8e7f2":"markdown","c29ee6bc":"markdown","9b5f062e":"markdown","32e63b95":"markdown","1aec0e04":"markdown"},"source":{"f742a248":"import numpy as np\nimport pandas as pd\ndf = pd.read_csv('..\/input\/play-tennis\/play_tennis.csv')\ndf","95dd94e2":"df","524d661b":"'''#Step-1 : Calculate Entropy of total dataset\nimport math\ndef calculate_entropy(df,target,feature='None',value='None'):\n    value_array,loop_var=df[target].unique(),len(df[target].unique())\n    Entropy=0\n    if feature=='None':\n        for i in range(0,loop_var):\n            prob = len(df[df[target]==value_array[i]])\/len(df)\n            if prob!=0:\n                Entropy +=  (-prob * (math.log(prob,2)))\n    else:\n        df_i = df[df[feature]==value]\n        for i in range(0,loop_var):\n            prob = len(df_i[df_i[target]==value_array[i]])\/len(df_i)\n            if prob!=0:\n                Entropy += (-prob * (math.log(prob,2)))\n            \n    return Entropy\n'''","e98a9bea":"import math\ndef calculate_entropy(df,target,feature='None',value='None'):\n    value_array,loop_var=df[target].unique(),len(df[target].unique())\n    Entropy=0\n    if feature=='None':\n        for i in range(0,loop_var):\n            prob = len(df[df[target]==value_array[i]])\/len(df)\n            if prob!=0:\n                Entropy +=  (-prob * (math.log(prob,2)))\n    else:\n        df = df[df[feature]==value]\n        for i in range(0,loop_var):\n            val=len(df)\n            if val==0:\n                val=1\n            prob = len(df[df[target]==value_array[i]])\/val\n            if prob!=0:\n                Entropy += (-prob * (math.log(prob,2)))\n            \n    return Entropy","09201bf1":"calculate_entropy(df,'play')","84a26a01":"df['outlook'].value_counts()","4e1dba2d":"df[df['outlook']=='Sunny']\n\n","e2fcd510":"df[df['outlook']=='Overcast']\n","63e8cb90":"df[df['outlook']=='Rain']","0541ed52":"IG_Outlook = calculate_entropy(df,'play') - ((len(df[df['outlook']=='Sunny'])\/len(df))*calculate_entropy(df,'play','outlook','Sunny')\n                                          + (len(df[df['outlook']=='Overcast'])\/len(df))*calculate_entropy(df,'play','outlook','Overcast')\n                                          +(len(df[df['outlook']=='Rain'])\/len(df))*calculate_entropy(df,'play','outlook','Rain'))\n\nIG_Outlook","4e4de77f":"df['temp'].value_counts()","3f878f33":"df[df['temp']=='Mild']","9531e4a2":"df[df['temp']=='Cool']","cd105aa3":"df[df['temp']=='Hot']","e42576eb":"IG_Temp = calculate_entropy(df,'play') - ((len(df[df['temp']=='Hot'])\/len(df))*calculate_entropy(df,'play','temp','Hot')\n                                          + (len(df[df['temp']=='Mild'])\/len(df))*calculate_entropy(df,'play','temp','Mild')\n                                          +(len(df[df['temp']=='Cool'])\/len(df))*calculate_entropy(df,'play','temp','Cool'))\n\nIG_Temp","260a679f":"df['humidity'].value_counts()","013c121d":"df[df['humidity']=='High']","509069fe":"df[df['humidity']=='Normal']","55076184":"IG_Humidity = calculate_entropy(df,'play') - ((len(df[df['humidity']=='High'])\/len(df))*calculate_entropy(df,'play','humidity','High')\n                                          + (len(df[df['humidity']=='Normal'])\/len(df))*calculate_entropy(df,'play','humidity','Normal')\n                                          )\n\nIG_Humidity","6c8f3e4e":"df['wind'].value_counts()","d1728a5c":"df[df['wind']=='Weak']","e146d2e7":"df[df['wind']=='Strong']","e1d6096c":"IG_Wind = calculate_entropy(df,'play') - ((len(df[df['wind']=='Weak'])\/len(df))*calculate_entropy(df,'play','wind','Weak')\n                                          + (len(df[df['wind']=='Strong'])\/len(df))*calculate_entropy(df,'play','wind','Strong')\n                                          )\n\nIG_Wind","c598d5fd":"print(IG_Outlook,IG_Temp,IG_Humidity,IG_Wind)","99d251af":"df1 = df[df['outlook']=='Sunny']\ndf1","736fef82":"IG_Outlook = calculate_entropy(df1,'play') - ((len(df1[df1['outlook']=='Sunny'])\/len(df1))*calculate_entropy(df1,'play','outlook','Sunny')\n                                          + (len(df1[df1['outlook']=='Overcast'])\/len(df1))*calculate_entropy(df1,'play','outlook','Overcast')\n                                          +(len(df1[df1['outlook']=='Rain'])\/len(df1))*calculate_entropy(df1,'play','outlook','Rain'))\n\nIG_Outlook","7eab6ac7":"IG_Temp = calculate_entropy(df1,'play') - ((len(df1[df1['temp']=='Hot'])\/len(df1))*calculate_entropy(df1,'play','temp','Hot')\n                                          + (len(df1[df1['temp']=='Mild'])\/len(df1))*calculate_entropy(df1,'play','temp','Mild')\n                                          +(len(df1[df1['temp']=='Cool'])\/len(df1))*calculate_entropy(df1,'play','temp','Cool'))\n\nIG_Temp","58a37fb6":"IG_Humidity = calculate_entropy(df1,'play') - ((len(df1[df1['humidity']=='High'])\/len(df1))*calculate_entropy(df1,'play','humidity','High')\n                                          + (len(df1[df1['humidity']=='Normal'])\/len(df1))*calculate_entropy(df1,'play','humidity','Normal')\n                                          )\n\nIG_Humidity","4276e809":"IG_Wind = calculate_entropy(df1,'play') - ((len(df1[df1['wind']=='Weak'])\/len(df1))*calculate_entropy(df1,'play','wind','Weak')\n                                          + (len(df1[df1['wind']=='Strong'])\/len(df1))*calculate_entropy(df1,'play','wind','Strong')\n                                          )\n\nIG_Wind","f1b3f7b4":"df3 = df1[df1['humidity']=='High']\ndf3","2ccdc2cd":"df4 = df1[df1['humidity']=='Normal']\ndf4","90d5e12f":"df2 = df[df['outlook']=='Rain']\ndf2","011e78d2":"IG_Temp = calculate_entropy(df2,'play') - ((len(df2[df2['temp']=='Hot'])\/len(df2))*calculate_entropy(df2,'play','temp','Hot')\n                                          + (len(df2[df2['temp']=='Mild'])\/len(df2))*calculate_entropy(df2,'play','temp','Mild')\n                                          +(len(df2[df2['temp']=='Cool'])\/len(df2))*calculate_entropy(df2,'play','temp','Cool'))\n\nIG_Temp","ed08f269":"IG_Wind = calculate_entropy(df2,'play') - ((len(df2[df2['wind']=='Weak'])\/len(df2))*calculate_entropy(df2,'play','wind','Weak')\n                                          + (len(df2[df2['wind']=='Strong'])\/len(df2))*calculate_entropy(df2,'play','wind','Strong')\n                                          )\n\nIG_Wind","fc0e4f1d":"IG_Humidity = calculate_entropy(df2,'play') - ((len(df2[df2['humidity']=='High'])\/len(df2))*calculate_entropy(df2,'play','humidity','High')\n                                          + (len(df2[df2['humidity']=='Normal'])\/len(df2))*calculate_entropy(df2,'play','humidity','Normal')\n                                          )\n\nIG_Humidity","1c8c3716":"df5 = df2[df2['wind']=='Weak']\ndf5","1990c094":"df6 = df2[df2['wind']=='Strong']\ndf6","63b662a3":"![image.png](attachment:image.png)","153fbbfd":"If there are many features in dataset, we have to calculate entropy, ig for all those features and we have to construct a tree.\n\nHaving much features and calculating Entropy, Information Gain of them will result in a higher train time complexity. Imagine having million rows and million features, your training phase will take days\n\nso, to avoid this, we have to do feature importance or we have to remove unwanted features. There is other thing which results in large dimensionality of the dataset, which is converting categorical features into numerical.\n\nIf we convert categorical into numerical, using onehot encoding, it will increase dimensionalities. So we have to perform other means of encoding categorical data","1765b081":"<p> <br>Information gain is how much information we gain when splitting the node into two parts<br>\n    <\/p>\n\n![image.png](attachment:image.png)","1dd473bc":"<h2>Information Gain<\/h2>","4c67fc8b":"<h3>Overfitting, Underfitting<\/h3>","17abe739":"![image.png](attachment:image.png)","3a3f99b6":"After splitting based on outlook, we will have three nodes","2a4217cf":"<h3>IG(Play,wind)<\/h3>","20eb1bd4":"<br> Decision Trees divides the space into set of axis parallel hyperplanes that breaks the space into hypercubes, hypercuboids<\/br>\n![image.png](attachment:image.png)","7bbacdee":"<h4>Imbalanced Data<\/h4>","d1b5ab6b":"<p> <br>p(i) refers to the probability of the class of the point in the total dataset<br>\n\n<br>Consider, there are 30 positive points and 70 negative points in a dataset.\nThe Entropy value would be (-(3\/10)* log2(3\/10)-(7\/10)* log2(7\/10)) = 0.88 <br>\n<br>0.88 considered as high entropy, for two class classification problem,entropy lies between 0 and 1<br>\n<br>If the entropy value is low, one class is significantly more than others or it may be a pure node<br>\n![](https:\/\/miro.medium.com\/max\/500\/1*M15RZMSk8nGEyOnD8haF-A.png)\n<br>So, we know that entropy is a measure of disorder or uncertainty and our goal is to minimize the uncertainty. So, low entropy is maintained<br>","5942c218":"<h4> Large Dimensions<\/h4>","a2d0cecb":"<h2>Constructing a decision tree<\/h2>","a0ac3302":"Since four faeatures are there, we are going to calculate Information Gain on all the four features\n","875fffe0":"<h3>Cases for Decision tree<\/h3>","7a28ce81":"<h3>Choosing the root node<\/h3>","3d39797b":"<br> Overfitting occurs in decision tree, when there is too much depth in a tree, our destination is to reach pure nodes, but whenever the pure node case isn't possible, without achieving the optimal depth, we can stop with small no of points in other classes. Those small number of points can be outliers too. In real world data, outliers can't be avoided<br>","81ca3789":"Here comes, **Information Gain**","d3290167":"<p> Out of these four features, when we split our dataset into leaf nodes based on outlook will have high information gain<p>\n    \n<br> So, we will be splitting based on outlook feature<br>","e9ea3381":"![image.png](attachment:image.png)","ce88a2dd":"<p> In Layman's terms, Entropy is a measure of disorder <\/p>\n\n![image.png](attachment:image.png)","a755faa3":"<h2>Entropy<\/h2>","41290fdb":"There are four features,\n\n1) Outlook\n    \n2) Temperature\n    \n3) Humidity\n    \n4) Wind\n\n\nOut of these four which one to take as root node ?","e919e7ed":"<h1>Key Terms for Building Decision Tree<\/h1>\n\n<p>\n    \n    1) Entropy\n    \n    2) Information Gain\n    \n    3) Gini Impurity\n<\/p>","d7a8a591":"<h3>IG(Play, temp)<\/h3>","8fb343ce":"![image.png](attachment:image.png)","d046365f":"<br> Overfitting refers to the performance of model, which is very best in Training data and poor in testing data<br>\n<br>The model literally memorize the datapoints and its classes, so whenever new question comes it doesn't know the answer<br>\n\n<br> Underfitting is when a model doesn't perform well on Train data and testing data also, a dumb model<br>","a66327eb":"<p>\n    Let's see what are the other classification models we know\n    \n    1) K-NN, Which is a Instance based model\n    \n    2) Naive Bayes, which is a probabilistic model\n    \n    3) Linear, Logistic Regression which operates on Geometric hyperplane\n    \n    4) SVM, Kernel Trick\n<\/p>\n\n<br> Decision Trees is different from all these, because it is very simple <\/br>\n\n<br> Basically, Decision Trees uses \"if..else\" like conditions, which are very easy to interpret<\/br>","e1b21b2b":"<h3>IG(Play,humidity)<\/h3>","70a8e7f2":"<h1>Geometric Intuition of Decision Trees<\/h1>","c29ee6bc":"<h1>Decision Trees<\/h1>","9b5f062e":"<h3> IG(Play, Outlook)<\/h3>","32e63b95":"![image.png](attachment:image.png)","1aec0e04":"Since the trees are splitted into leaf nodes based on the sample size of the class, we need to overcome the problem of imbalanced dataset\n\nIf the dataset is imbalanced, we have to grow unlimited trees. Either we have to specify maximum depth or upsample\/downsample the dataset until the number of points for all the classes are same"}}