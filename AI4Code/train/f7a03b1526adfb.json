{"cell_type":{"91c5ab16":"code","f86e9492":"code","5a50dea9":"code","1f83fba4":"code","4bea3dd3":"code","92d8e79a":"code","e79dabbc":"code","240fd171":"code","82ab790c":"code","275915eb":"code","7cbc7cf9":"code","c8c6ad12":"code","3502c539":"code","a588abf7":"code","4e4ee8f2":"code","21d5e006":"code","b7860968":"code","2229ab62":"code","a60240a0":"code","3c36c4f2":"code","c3e1a778":"code","2b89c891":"code","c615d3c7":"code","21e6f639":"code","2336def6":"code","4e7c06a0":"code","7ed92fa7":"code","d5cffd68":"code","eb8c81e1":"code","d84e2f5f":"code","1afeb3ee":"code","4a1cf873":"code","90f0b2df":"code","f264f470":"code","9f08f302":"code","0e146739":"code","c9604ee0":"code","86dc8592":"code","71aa481d":"code","534a516a":"code","5fa59327":"code","ea1f91ed":"code","1f77a741":"code","317d84e1":"code","1078e60d":"code","54e453d9":"code","e8657d72":"code","944893a2":"code","4dbb44e7":"code","89496ea6":"code","b7874df4":"code","e4213298":"code","a48a3194":"code","019b35c0":"code","9ae3eb0e":"code","7074c695":"code","68cdaed8":"code","a8e3e536":"code","44823187":"markdown","c225e767":"markdown","046d22a8":"markdown","2bf70f00":"markdown","e5af6d17":"markdown","429d074a":"markdown","f750da42":"markdown","bdd94178":"markdown","a696e5b9":"markdown","0c9612ac":"markdown","03408211":"markdown"},"source":{"91c5ab16":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nimport xgboost as xgb\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom scipy import stats\n\n","f86e9492":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd. read_csv('\/kaggle\/input\/titanic\/test.csv')","5a50dea9":"train.info()","1f83fba4":"train['Survived'].value_counts()","4bea3dd3":"train.head(25)","92d8e79a":"# Dropping Columns\ntrain = train.drop(columns=['Ticket'])\ntest = test.drop(columns=['Ticket'])","e79dabbc":"train.isna().sum()","240fd171":"test.isna().sum()","82ab790c":"age_na = train[train['Age'].isna()]\nage_na['Pclass'].value_counts()","275915eb":"age_means = {}\nage_means[1] = round(train[train['Pclass'] == 1]['Age'].mean())\nage_means[2] = round(train[train['Pclass'] == 2]['Age'].mean())\nage_means[3] = round(train[train['Pclass'] == 3]['Age'].mean())\nage_means","7cbc7cf9":"train['Age'] = train.apply(\n    lambda row: age_means[row['Pclass']] if np.isnan(row['Age']) else row['Age'],\n    axis=1\n)\n\ntest['Age'] = test.apply(\n    lambda row: age_means[row['Pclass']] if np.isnan(row['Age']) else row['Age'],\n    axis=1\n)","c8c6ad12":"test[test['Fare'].isna()]","3502c539":"train[train['Fare'] == 0]","a588abf7":"test[test['Fare'] == 0]","4e4ee8f2":"# Filling in the null fare value with the median fare for 3rd class\nmedian_third_class = test[test['Pclass']==3]['Fare'].median()\ntest['Fare'] = test['Fare'].fillna(median_third_class)","21d5e006":"# Defining a function that will find the median fare for the desired PClass as we itterate through both the train and test sets\ndef train_fare(row):\n    fares = {1: train[train['Pclass'] == 1].median(),\n             2: train[train['Pclass'] == 2].median(),\n             3: train[train['Pclass'] == 3].median()}\n    return(fares[row['Pclass']])\n\ndef test_fare(row):\n    fares = {1: test[test['Pclass'] == 1].median(),\n             2: test[test['Pclass'] == 2].median(),\n             3: test[test['Pclass'] == 3].median()}\n    return(fares[row['Pclass']])","b7860968":"medians = train[train['Fare'] == 0].apply(train_fare, axis=1)['Fare']\nmedians_test = test[test['Fare'] == 0].apply(test_fare, axis=1)['Fare']\ntrain.loc[train['Fare'] == 0,'Fare'] = medians\ntest.loc[test['Fare'] == 0,'Fare'] = medians_test","2229ab62":"# Changing Cabin to a binary column based on if value is null or not\ntrain.loc[train['Cabin'].notnull(),'Cabin'] = 1\ntrain.loc[train['Cabin'].isnull(),'Cabin'] = 0\ntrain['Cabin'] = train['Cabin'].astype('int')\n\ntest.loc[test['Cabin'].notnull(),'Cabin'] = 1\ntest.loc[test['Cabin'].isnull(),'Cabin'] = 0\ntest['Cabin'] = test['Cabin'].astype('int')","a60240a0":"train[train['Embarked'].isna()]","3c36c4f2":"train.groupby(['Pclass','Embarked']).count()","c3e1a778":"# Most first class passengers embarked from Southampton so we will fill the missing embarked values in the Train set with 'S'\n\ntrain['Embarked'] = train['Embarked'].fillna('S')","2b89c891":"# Changing Pclass and Embarked to categorical using One-hot encoding\ntrain[['Pclass', 'Embarked']]= train[['Pclass', 'Embarked']].astype('category')\ntest[['Pclass', 'Embarked']] = test[['Pclass', 'Embarked']].astype('category')\n\ndummy_class_train = pd.get_dummies(train['Pclass'])\ndummy_class_test = pd.get_dummies(test['Pclass'])\ntrain = pd.concat([train, dummy_class_train], axis=1)\ntest = pd.concat([test, dummy_class_test], axis=1)\n\n\ndummy_emb_train = pd.get_dummies(train['Embarked'])\ndummy_emb_test = pd.get_dummies(test['Embarked'])\ntrain = pd.concat([train, dummy_emb_train], axis=1)\ntest = pd.concat([test, dummy_emb_test], axis=1)\n\n# Changing Sex to a binary column\ntrain['Sex'] = train['Sex'].apply(lambda x: 1 if x=='male' else 0)\ntest['Sex'] = test['Sex'].apply(lambda x: 1 if x=='male' else 0)\n","c615d3c7":"# Extracting titles from names\ntrain['Title'] = train['Name'].str.extract(r',\\s(.+?)\\.')\ntest['Title'] = test['Name'].str.extract(r',\\s(.+?)\\.')","21e6f639":"train['Title'].value_counts()","2336def6":"# Renaming rare titles as \"Rare\", making Title column categorical\ncommon_titles = ['Mr', 'Miss', 'Mrs', 'Master']\ntrain.loc[~train[\"Title\"].isin(common_titles), \"Title\"] = \"Rare\"\ntest.loc[~test[\"Title\"].isin(common_titles), \"Title\"] = \"Rare\"\n\ntrain['Title'] = train['Title'].astype('category')\ntest['Title'] = test['Title'].astype('category')\n\ndummy_title_train = pd.get_dummies(train['Title'])\ndummy_title_test = pd.get_dummies(test['Title'])\ntrain = pd.concat([train, dummy_title_train], axis=1)\ntest = pd.concat([test, dummy_title_test], axis=1)\n","4e7c06a0":"numerical_train = train.select_dtypes(exclude = ['object'])\ncorr_mat = numerical_train.corr()\n","7ed92fa7":"sns.heatmap(corr_mat, center = 0)","d5cffd68":"g = sns.distplot(train['Fare'])\ntrain['Fare'].skew()","eb8c81e1":"g = sns.distplot(test['Fare'])\ntest['Fare'].skew()","d84e2f5f":"# Comparing log transform and box-cox transform to fix skew\n\nbox_cox = stats.boxcox(train['Fare'])[0]\ng = sns.distplot(box_cox)\npd.Series(box_cox).skew()","1afeb3ee":"log_fare = train['Fare'].apply(lambda x:np.log(x))\ng = sns.distplot(log_fare)\nlog_fare.skew()","4a1cf873":"# With the box-cox transform clearly fixing the skew best we will use it to fix our fare column\ntrain['box_fare'] = box_cox","90f0b2df":"test['box_fare'] = stats.boxcox(test['Fare'])[0]\ng = sns.distplot(test['box_fare'])\ntest['box_fare'].skew()","f264f470":"g = sns.catplot(x=\"Embarked\", y=\"Survived\", data=train,\n                height=6, kind=\"bar\", palette=\"muted\")\ng.despine(left=True)\ng.set_ylabels(\"survival probability\")","9f08f302":"g = sns.catplot(x=\"Title\", y=\"Survived\",data=train,\n                height=6, kind=\"bar\", palette=\"muted\")\ng.despine(left=True)\ng.set_ylabels(\"survival probability\")","0e146739":"g = sns.catplot(x=\"SibSp\", y=\"Survived\",data=train,\n                height=6, kind=\"bar\", palette=\"muted\")\ng.despine(left=True)\ng.set_ylabels(\"survival probability\")\n    ","c9604ee0":"g = sns.catplot(x=\"Sex\", y=\"Survived\",data=train,\n                height=6, kind=\"bar\", palette=\"muted\")\ng.despine(left=True)\ng.set_ylabels(\"survival probability\")","86dc8592":"g = sns.catplot(x=\"Parch\", y=\"Survived\",data=train,\n                height=6, kind=\"bar\", palette=\"muted\")\ng.despine(left=True)\ng.set_ylabels(\"survival probability\")   ","71aa481d":"features = train.columns\nfeatures = features.drop(['PassengerId','Survived', 'Pclass', 'Title', 'Fare', 'Name', 'Embarked'])","534a516a":"train[features].head()","5fa59327":"# Defining pipeline\ndef random_search(model, parameter_grid, cvn, train_data, feature_list):\n    clf = RandomizedSearchCV(estimator = model, param_distributions = parameter_grid, cv=cvn, return_train_score = False, n_jobs = -1)\n    clf.fit(train_data[feature_list], train_data['Survived'])\n    return clf\n\ndef grid_search(model, parameter_grid, cvn, train_data, feature_list):\n    clf = GridSearchCV(estimator = model, param_grid= parameter_grid, cv=cvn, return_train_score = False, n_jobs = -1)\n    clf.fit(train_data[feature_list], train_data['Survived'])\n    return clf\n\ndef create_predictions(model, train_data,test_data, features, csv_name):\n    model.fit(train_data[features], train_data['Survived'])\n    predictions = model.predict(test_data[features])\n    submission = pd.concat([test_data['PassengerId'], pd.Series(predictions)], axis=1)\n    submission = submission.rename(columns = {0:'Survived'})\n    submission.to_csv(\".\/\" + csv_name, index=False)\n    ","ea1f91ed":"param_grid_rf = {'bootstrap': [True, False],\n 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n 'max_features': ['auto', 'sqrt'],\n 'min_samples_leaf': [1, 2, 4],\n 'min_samples_split': [2, 5, 10],\n 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n\nparam_grid_logit = {'penalty': ['l1', 'l2'],\n                    'solver' : ['lbfgs', 'liblinear']    \n}\n\nparam_grid_xgb = {\"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n \"min_child_weight\" : [ 1, 3, 5, 7 ],\n \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ] }\n\n","1f77a741":"rf = RandomForestClassifier(random_state = 123)\nlr = LogisticRegression(random_state=123)\nclf = xgb.XGBClassifier(random_state=123)","317d84e1":"rf_opt = random_search(rf, param_grid_rf, 4, train, features)\nbest_rf_params = rf_params.best_params_","1078e60d":"create_predictions(rf_opt, train, test, features, \"rf_opt.csv\")","54e453d9":"rf_opt.fit(train[features], train['Survived'])\npredictions_rf = rf_opt.predict(train[features])\n\nrf_accuracy = accuracy_score(train['Survived'], predictions_rf)","e8657d72":"lr_opt = grid_search(lr, param_grid_logit, 4, train, features)","944893a2":"lr_opt.fit(train[features], train['Survived'])","4dbb44e7":"lr_opt.best_params_","89496ea6":"lr_predictions = lr_opt.predict(train[features])\nlr_accuracy = accuracy_score(train['Survived'], lr_predictions)","b7874df4":"lr_accuracy","e4213298":"create_predictions(lr_opt, train, test, features, \"lr_opt.csv\")","a48a3194":"xgb_opt = random_search(clf, param_grid_xgb, 4, train, features)","019b35c0":"xgb_opt.fit(train[features], train['Survived'])","9ae3eb0e":"xgb_params = xgb_opt.best_params_","7074c695":"xgb_predictions = xgb_opt.predict(train[features])\nxgb_accuracy= accuracy_score(train['Survived'], xgb_predictions)\n","68cdaed8":"xgb_opt = xgb.XGBClassifier(**xgb_params, random_state = 123)","a8e3e536":"create_predictions(xgb_opt, train, test, features, \"xgb_opt.csv\")","44823187":"Since there was a large difference in the class distribution of passengers with NaN age values, and there is a relatively large difference in the mean age of passengers of each class. We decided to fill each NaN value with the mean of the passengers class type.","c225e767":"## **Class Imbalance**\nImmediately can identify that there is class imbalance, although not by much. With the dataset being so small already we should rule out the (although convenient) under-sampling route and see if there is a convenient way to handle the problem.\n\n## **Null values**\nWe can see that there are several columns with null values which need to be dealt with, the Cabin column in particular has many null values and may need to be dropped altogether, however there may be a possible solution.\n\n## **Columns to Drop**\nThe Ticket column seems largely of little use to us there isn't much easily extractable information, while the Cabin column also seems largely useless as it is mostly null values, we may be able to find a pattern between the non-null values and survival rate.\n","046d22a8":"# Data visualization\n\n","2bf70f00":"## Dealing with null values","e5af6d17":"There are two problems with our fare values, we see that in our test set there is a null fare value. While in both our test and train sets we see that there are fare values of 0 which we will treat as null values (since it is safe to say no one boarded the ship for free).","429d074a":"# Fitting the Model\n\nTime to try several different models, I will be trying a simpe logistic regression, a random forest, and MLP Classifer, as well as XGBoost","f750da42":"# Imports\n\nStandard imports including numpy, pandas, matplotlib (seaborn) and scikit learn.","bdd94178":"# Data Investigation","a696e5b9":"Several observations can be made after visualizing the categorical data, as well as columns with discrete data such as the SibSp and Parch columns.\n\n* The distribution of fares is significantly right skewed, thus transformation was necessary\n* People who embarked from Cherbourg had a higher survival rate than those embarking from Queenstown or Southampton\n* Adult men with no special title had a very low survival rate compared to women and children\n* Those with 1-2 Siblings+Spouses had a higher survival rate than those with many or those with none\n* There wasn't a clear trend seen in the Parch column however those with more than 3 Parents and children saw very low survival rates.\n\n","0c9612ac":"There is a clear skew within the fare column which may affect our model, we will need to perform some sort of transform to normalize the data","03408211":"## Feature Engineering\n\nWith all our null values dealt with we can move towards converting certain columns to categorical ones and creating some features that maybe more useful to our model"}}