{"cell_type":{"55f741d1":"code","8e292d6a":"code","e08b3f04":"code","de03e930":"code","90a280fc":"code","4998721a":"code","c874d805":"code","8febfe39":"code","9c7ba4a4":"code","e5897a8b":"code","76667d97":"code","b9e688d5":"code","2c56abc4":"code","d1b12533":"code","654ccef5":"code","2dff6f3d":"code","4bbfda55":"code","531d0dc2":"code","1c05f24f":"code","f264c94b":"code","5f840c42":"code","3af5dc61":"code","61111351":"code","a601d627":"code","2a8ee968":"markdown","6451c86c":"markdown","6edd04d0":"markdown","5a3c8f67":"markdown","72802363":"markdown","7c6b093c":"markdown","b40a687f":"markdown","7f5a7a3a":"markdown","7e13e5f7":"markdown","b0318ace":"markdown","e22bfdd7":"markdown","80f14e79":"markdown","00c28bf6":"markdown"},"source":{"55f741d1":"#by Dogu Can ELCI\n#please make comments :)","8e292d6a":"import pandas as pd","e08b3f04":"#Ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(action='ignore', category=FutureWarning)  ","de03e930":"#Reading dataset\ntrain_datas = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\ntrain_datas.head()","90a280fc":"train_datas.info()","4998721a":"train_datas.drop(columns={'Unnamed: 32'},inplace=True)","c874d805":"from sklearn.preprocessing import LabelEncoder\nlbl = LabelEncoder()\ntrain_datas['diagnosis'] = lbl.fit_transform(train_datas['diagnosis'])\ntrain_datas.head()","8febfe39":"train_datas_id = pd.DataFrame()\ntrain_datas_id = train_datas['id']\ntrain_datas.drop(columns={'id'},inplace=True)","9c7ba4a4":"from matplotlib import pyplot as plt\nplt.figure(figsize=(5,10))\nplt.pie([len(train_datas['diagnosis'][train_datas['diagnosis']==0]),len(train_datas['diagnosis'][train_datas['diagnosis']==1])],\n    autopct='%1.1f%%',\n    labels=['Healthy','Breast Cancer'])\nplt.title('Class Label Rates')\nplt.show()","e5897a8b":"import seaborn as sb\ncorr = train_datas.corr()\nplt.figure(figsize=(40,30))\nsb.heatmap(corr,annot=True)","76667d97":"corr.head()","b9e688d5":"list1= []\nfor i in range(len(corr)):\n    for j in range(i):\n        if corr.iloc[i,j] > 0.9:\n            list1.append([i,j])\nlist1","2c56abc4":"for i in list1:\n    if len(train_datas.iloc[:,i[1]]) != 0:\n        train_datas.iloc[:,i[1]] = 0\nfor j in train_datas.columns:\n    if train_datas[j].sum() == 0:\n        train_datas.drop(columns={j},inplace=True)\ntrain_datas","d1b12533":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(train_datas.drop(columns={'diagnosis'}),train_datas['diagnosis'],test_size=0.2,random_state=32)\nx_train.shape,x_test.shape,y_train.shape,y_test.shape","654ccef5":"#Scale train datas\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import RobustScaler\n#1-Standart Scaler\nscaler = StandardScaler()\nx_train_st = scaler.fit_transform(x_train)\nx_test_st = scaler.fit_transform(x_test)\n#2-Standart MinMaxScaler\nscaler = MinMaxScaler()\nx_train_mm = scaler.fit_transform(x_train)\nx_test_mm = scaler.fit_transform(x_test)\n#3-Standart Normalizer\nscaler = Normalizer()\nx_train_n = scaler.fit_transform(x_train)\nx_test_n = scaler.fit_transform(x_test)\n#4-Standart RobustScaler\nscaler = RobustScaler()\nx_train_rb = scaler.fit_transform(x_train)\nx_test_rb = scaler.fit_transform(x_test)","2dff6f3d":"#Support Vector Machines model is used for select the best scale technique as cross-validation score of fit-svm classifier.\n#1- SVM\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score\nmodel_svm = SVC()\nscores = cross_val_score(model_svm,x_train_rb,y_train,cv=10)\nscores , scores.mean()","4bbfda55":"#2-Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nmodel_naive = GaussianNB()\nscores = cross_val_score(model_naive,x_train_rb,y_train,cv=10)\nscores, scores.mean()","531d0dc2":"#3-Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\nmodel_tree = DecisionTreeClassifier()\nscores = cross_val_score(model_tree,x_train_rb,y_train,cv=10)\nscores, scores.mean()","1c05f24f":"#4-Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nmodel_rf = RandomForestClassifier()\nscores = cross_val_score(model_rf,x_train_rb,y_train,cv=10)\nscores, scores.mean()","f264c94b":"#5-KNeighbor\nfrom sklearn.neighbors import KNeighborsClassifier\nmodel_knn = KNeighborsClassifier(n_neighbors=10)\nscores = cross_val_score(model_knn,x_train_rb,y_train,cv=10)\nscores, scores.mean()","5f840c42":"#6-Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nmodel_lr = LogisticRegression()\nscores = cross_val_score(model_lr,x_train_rb,y_train,cv=10)\nscores, scores.mean()","3af5dc61":"#7-XGBClassifier\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nxgb.set_config(verbosity=0)\nmodel_xgb = xgb.XGBClassifier()\nscores = cross_val_score(model_xgb,x_train_rb,y_train,cv=10)\nscores, scores.mean()","61111351":"model_xgb = LogisticRegression()\nmodel_xgb.fit(x_train_rb,y_train)\npred_test = model_xgb.predict(x_test_rb)\nfrom sklearn.metrics import accuracy_score\nacc = accuracy_score(y_test.values,pred_test)\n#3 model is trained and getting accuracy of them.\n#As a result, Number3(Logistic Regression) has best accuracy point at test datas.","a601d627":"print('Last Accuracy of selected model is: %',round(acc,3))","2a8ee968":"Removing Unnamed:32 column from datas","6451c86c":"As results of Scaling part, it is determined that RobustScaler is the best way to scaling.(%96 cross-validation accuracy)","6edd04d0":"As we can see belove, \"**Unnamed32**\" has no any useful datas and \"diagnosis\"(y label) has no numeric datas.\nSo we have to fix them.","5a3c8f67":"Reduntant features are dropped in here.","72802363":"# Converting diagnosis datas to numeric version","7c6b093c":"Check if any imbalanced class labels problem exists.","b40a687f":"'**id**' column is dropped and save in new variable for after.","7f5a7a3a":"After the checking,there is a difference between two class but there is a probability that it still be useful for creating good\nai models without any changes, because difference is not too much.\nIf results will be bad, we will come back and try another techniques.","7e13e5f7":"As you see, there are many features which has high correlation with each other. It means that some of them are redundant.\nSo we have to find and drop them. ","b0318ace":"As results, best-performance models are:\n1- XGBClassifier\n2- SVM \n3- Logistic Regression","e22bfdd7":"# Scaling process","80f14e79":"list1 keeps high correlated feature pairs.","00c28bf6":"After the preproccessing part, train and test subsets must be seperated."}}