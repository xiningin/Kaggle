{"cell_type":{"04700d69":"code","be21c1ea":"code","958d1207":"code","197e3c70":"code","2f762432":"code","fe1a54db":"code","13de9c80":"code","92e4d888":"code","61e60dad":"code","88793f45":"code","59cbea14":"code","6563c9bd":"code","cef1df22":"code","59690edb":"code","fbe31e9c":"code","7e2e854d":"code","15c2ebfc":"code","caf2ad32":"code","8c5ec180":"code","0b9d722a":"code","f0d91c46":"markdown","17496d79":"markdown","b880cfc1":"markdown","d2010fb8":"markdown"},"source":{"04700d69":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=Warning)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","be21c1ea":"df = pd.read_csv(\"\/kaggle\/input\/learnplatform-covid19-impact-on-digital-learning\/engagement_data\/6345.csv\")\ndf.head()","958d1207":"#Code by HarshGupta57 https:\/\/www.kaggle.com\/harshgupta57\/freediving-world-records-eda\/notebook\n\nimport datetime","197e3c70":"#Code by HarshGupta57 https:\/\/www.kaggle.com\/harshgupta57\/freediving-world-records-eda\/notebook\n\ndf[['year','month','date']] = df['time'].apply(lambda x : pd.Series(str(x).split('-')))","2f762432":"#Code by HarshGupta57 https:\/\/www.kaggle.com\/harshgupta57\/freediving-world-records-eda\/notebook\n\ndf.drop('time', inplace=True, axis =1)","fe1a54db":"df.head()","13de9c80":"#Code by Parul Pandey  https:\/\/www.kaggle.com\/parulpandey\/a-guide-to-handling-missing-values-in-python\n\n\nfrom sklearn.impute import SimpleImputer\ndf1 = df.copy()\n#setting strategy to 'mean' to impute by the mean\nmean_imputer = SimpleImputer(strategy='most_frequent')# strategy can also be mean or median \ndf1.iloc[:,:] = mean_imputer.fit_transform(df1)","92e4d888":"from sklearn.preprocessing import LabelEncoder\n\n#fill in mean for floats\nfor c in df1.columns:\n    if df1[c].dtype=='float16' or  df1[c].dtype=='float32' or  df1[c].dtype=='float64':\n        df1[c].fillna(df[c].mean())\n\n#fill in -999 for categoricals\ndf1 = df1.fillna(-999)\n# Label Encoding\nfor f in df1.columns:\n    if df1[f].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(df1[f].values))\n        df1[f] = lbl.transform(list(df1[f].values))\n        \nprint('Labelling done.')","61e60dad":"#Code by Ajay Sampath https:\/\/www.kaggle.com\/ajaysamp\/boruta-beats-em-all-new-look-at-feature-selection\n\n# lets create a feature matrix\nfeature_names =    ['engagement_index','perc_access', 'month', 'date']","88793f45":"#Code by Ajay Sampath https:\/\/www.kaggle.com\/ajaysamp\/boruta-beats-em-all-new-look-at-feature-selection\n\n# define function for creating y\ndef yfromX(X):\n    y = X['engagement_index'] + X['perc_access']**2 + np.sin(3 * X['month']) + (X['date'])\n    return y","59cbea14":"#Code by Ajay Sampath https:\/\/www.kaggle.com\/ajaysamp\/boruta-beats-em-all-new-look-at-feature-selection\n\nfrom sklearn.model_selection import train_test_split\n\n# create x and y\nnp.random.seed(0)\n\nX = pd.DataFrame(np.random.normal(size = (20000, len(feature_names))), columns = feature_names)\ny = yfromX(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","6563c9bd":"from sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nfrom sklearn.metrics import mean_absolute_error\nfrom eli5.sklearn import PermutationImportance","cef1df22":"#Code by Ajay Sampath https:\/\/www.kaggle.com\/ajaysamp\/boruta-beats-em-all-new-look-at-feature-selection\n\n# linear regression\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n\nlr_train_preds = lr.predict(X_train)\nlr_test_preds = lr.predict(X_test)\n\nlr_train_mae = mean_absolute_error(y_train, lr_train_preds)\nlr_test_mae = mean_absolute_error(y_test, lr_test_preds)\n\nlr_fi = PermutationImportance(lr, cv = 'prefit', n_iter = 3).fit(X_train, y_train).feature_importances_","59690edb":"#Code by Ajay Sampath https:\/\/www.kaggle.com\/ajaysamp\/boruta-beats-em-all-new-look-at-feature-selection\n\n# Random Forest \nrf = RandomForestRegressor(max_depth=5)\nrf.fit(X_train, y_train)\n\nrf_train_preds = rf.predict(X_train)\nrf_test_preds = rf.predict(X_test)\n\nrf_train_mae = mean_absolute_error(y_train, rf_train_preds)\nrf_test_mae = mean_absolute_error(y_test, rf_test_preds)\n\nrf_fi = rf.feature_importances_","fbe31e9c":"#Code by Ajay Sampath https:\/\/www.kaggle.com\/ajaysamp\/boruta-beats-em-all-new-look-at-feature-selection\n\n# Light GBM\nlgb = LGBMRegressor(max_depth=5)\nlgb.fit(X_train, y_train)\n\nlgb_train_preds = lgb.predict(X_train)\nlgb_test_preds = lgb.predict(X_test)\n\nlgb_train_mae = mean_absolute_error(y_train, lgb_train_preds)\nlgb_test_mae = mean_absolute_error(y_test, lgb_test_preds)\n\nlgb_fi = lgb.feature_importances_","7e2e854d":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.ticker import FuncFormatter \nimport matplotlib.ticker as mtick","15c2ebfc":"#Code by Ajay Sampath https:\/\/www.kaggle.com\/ajaysamp\/boruta-beats-em-all-new-look-at-feature-selection\n\nfrom boruta import BorutaPy\n\nnew_rf = RandomForestRegressor(n_jobs = -1, max_depth = 5)\n\nboruta_selector = BorutaPy(new_rf, n_estimators = 'auto', random_state = 0)\nboruta_selector.fit(np.array(X_train), np.array(y_train))\n\nboruta_ranking = boruta_selector.ranking_\nselected_features = np.array(feature_names)[boruta_ranking <= 2]","caf2ad32":"#Code by Ajay Sampath https:\/\/www.kaggle.com\/ajaysamp\/boruta-beats-em-all-new-look-at-feature-selection\n\nboruta_ranking = pd.DataFrame(data=boruta_ranking, index=X_train.columns.values, columns=['values'])\nboruta_ranking['Variable'] = boruta_ranking.index\nboruta_ranking.sort_values(['values'], ascending=True, inplace=True)","8c5ec180":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.ticker import FuncFormatter \nimport matplotlib.ticker as mtick","0b9d722a":"#Code by Ajay Sampath https:\/\/www.kaggle.com\/ajaysamp\/boruta-beats-em-all-new-look-at-feature-selection\n\nfig,ax = plt.subplots(figsize=(8,4))\nax = sns.barplot(x='values',y='Variable',data=boruta_ranking, color='b')\nplt.title('Boruta Feature Ranking')\nplt.xlabel('')\nplt.ylabel('')\nplt.tight_layout()","f0d91c46":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcRCVsK6xyXxhMI_BmOY1GFxu3uBr3STcoGiOWCJhuECHwknpiW04P1wblb674-uK3gjZlw&usqp=CAU)https:\/\/www.bis.org\/ifc\/events\/ifc_9thconf\/Petropoulos.pdf","17496d79":"#Boruta's ranking output (Rank 1: confirmed, Rank 2: some influence, Above Rank 3: Rejected\n\nThe output from Boruta is a ranking number. So we can divide them into categories are need. All features with a rank 1 mean that these have a confirmed effect on the target variable. A rank 2 indicates that there is some influence. Anything above 3 can be rejected as it does not have any infuence.\n\nUsing Boruta, we picked all the correct dependent variables and rejected the noise(??).\n\nBoruta can be a very powerful tool to use and check feature importances from traditional methods.","b880cfc1":"#Split date: year\/month\/date","d2010fb8":"<center style=\"font-family:verdana;\"><h1 style=\"font-size:200%; padding: 10px; background: #001f3f;\"><b style=\"color:#03e8fc;\">Boruta all-relevant feature selection method<\/b><\/h1><\/center>\n\n\n\"Boruta is an all relevant feature selection method, while most other are minimal optimal; this means it tries to find all features carrying information usable for prediction, rather than finding a possibly compact subset of features on which some classifier has a minimal error.\"\n\n\"Why bother with all relevant feature selection? When you try to understand the phenomenon that made your data, you should care about all factors that contribute to it, not just the bluntest signs of it in context of your methodology (yes, minimal optimal set of features by definition depends on your classifier choice).\"\n\nhttps:\/\/github.com\/scikit-learn-contrib\/boruta_py"}}