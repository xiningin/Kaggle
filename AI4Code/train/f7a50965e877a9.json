{"cell_type":{"e047bfb8":"code","b129627e":"code","74b8c950":"code","0216dc51":"code","c6491b1f":"code","8ebe40a5":"code","20790685":"code","028bcb88":"code","e9893ebb":"code","e4b60628":"code","38d62425":"code","f9fddd77":"code","b4290e34":"code","11be9d5d":"code","b67ef18f":"code","c76a2d39":"code","498981be":"code","7332777f":"code","64b608c9":"code","d73757b9":"code","283a08d2":"code","35b1ab8b":"code","f9bc2968":"code","6ea4757e":"code","4cf7ee57":"code","73fb88ac":"code","98a0fd06":"code","55975f1a":"code","2a786531":"code","21f94001":"code","9ff3d619":"code","11c7368a":"code","ecf5116d":"code","2ecc54d7":"code","17d4dc18":"code","13cca489":"code","95937458":"code","3bffe61d":"code","76fafc7f":"markdown","36234cdb":"markdown","ca9f9a7a":"markdown","3d450923":"markdown","05217e30":"markdown","c9b141c7":"markdown","24824390":"markdown","1eb8c710":"markdown","684fa8b7":"markdown","6d85835b":"markdown","eb9d8ff8":"markdown","3caf4a09":"markdown","e1ce2671":"markdown","d69d3d87":"markdown","bebe4c50":"markdown","0098ded3":"markdown","59870983":"markdown","4c70045e":"markdown","ed859966":"markdown","f6ebde89":"markdown","752a280a":"markdown","0e441241":"markdown"},"source":{"e047bfb8":"# Main Libs\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Utility libs\nfrom tqdm import tqdm\nimport time\nimport datetime\nfrom skopt import gp_minimize\nfrom skopt.space import Real, Integer\nfrom skopt.utils import use_named_args\nfrom skopt.plots import plot_convergence\nfrom copy import deepcopy\nimport pprint\nimport shap\nimport os\n\n# You might have to do !pip install catboost\n# If you don't have it on your local machine\n# nevertheless Kaggle runtimes come preinstalled with CatBoost\nimport catboost\n\nfrom pathlib import Path\ndata_dir = Path('..\/input\/data-science-bowl-2019')\nos.listdir(data_dir)","b129627e":"%%time\ntrain = pd.read_csv('..\/input\/data-science-bowl-2019\/train.csv')\nlabels = pd.read_csv('..\/input\/data-science-bowl-2019\/train_labels.csv')\ntest = pd.read_csv('..\/input\/data-science-bowl-2019\/test.csv')\nspecs = pd.read_csv('..\/input\/data-science-bowl-2019\/specs.csv')\nsample_submission = pd.read_csv('..\/input\/data-science-bowl-2019\/sample_submission.csv')","74b8c950":"train.head()","0216dc51":"labels.head()","c6491b1f":"sample_submission.head()","8ebe40a5":"list_of_user_activities = list(set(train['title'].value_counts().index).union(set(test['title'].value_counts().index)))\nactivities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n\ntrain['title'] = train['title'].map(activities_map)\ntest['title'] = test['title'].map(activities_map)\nlabels['title'] = labels['title'].map(activities_map)","20790685":"win_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))\nwin_code[activities_map['Bird Measurer (Assessment)']] = 4110\n\ntrain['timestamp'] = pd.to_datetime(train['timestamp'])\ntest['timestamp'] = pd.to_datetime(test['timestamp'])","028bcb88":"# Thanks for this beautiful function https:\/\/www.kaggle.com\/mhviraf\/a-new-baseline-for-dsb-2019-catboost-model \ndef get_data(user_sample, test_set=False):\n    last_activity = 0\n    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n    all_assessments = []\n    accumulated_accuracy_group = 0\n    accumulated_accuracy=0\n    accumulated_correct_attempts = 0 \n    accumulated_uncorrect_attempts = 0 \n    accumulated_actions = 0\n    counter = 0\n    durations = []\n    for i, session in user_sample.groupby('game_session', sort=False):\n        session_type = session['type'].iloc[0]\n        session_title = session['title'].iloc[0]\n        if test_set == True:\n            second_condition = True\n        else:\n            if len(session)>1:\n                second_condition = True\n            else:\n                second_condition= False\n            \n        if (session_type == 'Assessment') & (second_condition):\n            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n            features = user_activities_count.copy()\n            features['session_title'] = session['title'].iloc[0] \n            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n            accumulated_correct_attempts += true_attempts \n            accumulated_uncorrect_attempts += false_attempts\n            if durations == []:\n                features['duration_mean'] = 0\n            else:\n                features['duration_mean'] = np.mean(durations)\n            durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n            features['accumulated_accuracy'] = accumulated_accuracy\/counter if counter > 0 else 0\n            accuracy = true_attempts\/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n            accumulated_accuracy += accuracy\n            if accuracy == 0:\n                features['accuracy_group'] = 0\n            elif accuracy == 1:\n                features['accuracy_group'] = 3\n            elif accuracy == 0.5:\n                features['accuracy_group'] = 2\n            else:\n                features['accuracy_group'] = 1\n\n            features.update(accuracy_groups)\n            features['accumulated_accuracy_group'] = accumulated_accuracy_group\/counter if counter > 0 else 0\n            features['accumulated_actions'] = accumulated_actions\n            accumulated_accuracy_group += features['accuracy_group']\n            accuracy_groups[features['accuracy_group']] += 1\n            if test_set == True:\n                all_assessments.append(features)\n            else:\n                if true_attempts+false_attempts > 0:\n                    all_assessments.append(features)\n                \n            counter += 1\n        accumulated_actions += len(session)\n        if last_activity != session_type:\n            user_activities_count[session_type] += 1\n            last_activitiy = session_type\n    if test_set:\n        return all_assessments[-1] \n    return all_assessments","e9893ebb":"compiled_data = []\nfor i, (ins_id, user_sample) in tqdm(enumerate(train.groupby('installation_id', sort=False)), total=17000):\n    compiled_data += get_data(user_sample)","e4b60628":"new_train = pd.DataFrame(compiled_data)\ndel compiled_data\nprint(\"Train Data Shape:\")\nnew_train.shape","38d62425":"import gc\ngc.collect()","f9fddd77":"all_features = [x for x in new_train.columns if x not in ['accuracy_group']]\ncat_features = ['session_title']\nX, y = new_train[all_features], new_train['accuracy_group']\ndel train","b4290e34":"class ModelOptimizer:\n    best_score = None\n    opt = None\n    \n    def __init__(self, model, X_train, y_train, categorical_columns_indices=None, n_fold=3, seed=2405, early_stopping_rounds=30, is_stratified=True, is_shuffle=True):\n        self.model = model\n        self.X_train = X_train\n        self.y_train = y_train\n        self.categorical_columns_indices = categorical_columns_indices\n        self.n_fold = n_fold\n        self.seed = seed\n        self.early_stopping_rounds = early_stopping_rounds\n        self.is_stratified = is_stratified\n        self.is_shuffle = is_shuffle\n        \n        \n    def update_model(self, **kwargs):\n        for k, v in kwargs.items():\n            setattr(self.model, k, v)\n            \n    def evaluate_model(self):\n        pass\n    \n    def optimize(self, param_space, max_evals=10, n_random_starts=2):\n        start_time = time.time()\n        \n        @use_named_args(param_space)\n        def _minimize(**params):\n            self.model.set_params(**params)\n            return self.evaluate_model()\n        \n        opt = gp_minimize(_minimize, param_space, n_calls=max_evals, n_random_starts=n_random_starts, random_state=2405, n_jobs=-1)\n        best_values = opt.x\n        optimal_values = dict(zip([param.name for param in param_space], best_values))\n        best_score = opt.fun\n        self.best_score = best_score\n        self.opt = opt\n        \n        print('optimal_parameters: {}\\noptimal score: {}\\noptimization time: {}'.format(optimal_values, best_score, time.time() - start_time))\n        print('updating model with optimal values')\n        self.update_model(**optimal_values)\n        plot_convergence(opt)\n        return optimal_values\nclass CatboostOptimizer(ModelOptimizer):\n    def evaluate_model(self):\n        validation_scores = catboost.cv(\n        catboost.Pool(self.X_train, \n                      self.y_train, \n                      cat_features=self.categorical_columns_indices),\n        self.model.get_params(), \n        nfold=self.n_fold,\n        stratified=self.is_stratified,\n        seed=self.seed,\n        early_stopping_rounds=self.early_stopping_rounds,\n        shuffle=self.is_shuffle,\n#         metrics='auc',\n        plot=False)\n        self.scores = validation_scores\n        test_scores = validation_scores.iloc[:, 2]\n        best_metric = test_scores.max()\n        return 1 - best_metric","11be9d5d":"default_cb = catboost.CatBoostClassifier(loss_function='MultiClass',\n                                         task_type='CPU',\n                                         random_seed=12,\n                                         silent=True\n                                        )\ndefault_cb_optimizer = CatboostOptimizer(default_cb, X, y)\ndefault_cb_optimizer.evaluate_model()","b67ef18f":"greedy_cb = catboost.CatBoostClassifier(\n    loss_function='MultiClass',\n    task_type=\"CPU\",\n    learning_rate=0.01,\n    iterations=2000,\n    od_type=\"Iter\",\n    early_stopping_rounds=500,\n    random_seed=24,\n    silent=True\n)","c76a2d39":"from sklearn.metrics import confusion_matrix\ndef qwk(act,pred,n=4,hist_range=(0,3)):\n    \n    O = confusion_matrix(act,pred)\n    O = np.divide(O,np.sum(O))\n    \n    W = np.zeros((n,n))\n    for i in range(n):\n        for j in range(n):\n            W[i][j] = ((i-j)**2)\/((n-1)**2)\n            \n    act_hist = np.histogram(act,bins=n,range=hist_range)[0]\n    prd_hist = np.histogram(pred,bins=n,range=hist_range)[0]\n    \n    E = np.outer(act_hist,prd_hist)\n    E = np.divide(E,np.sum(E))\n    \n    num = np.sum(np.multiply(W,O))\n    den = np.sum(np.multiply(W,E))\n        \n    return 1-np.divide(num,den)","498981be":"cb_optimizer = CatboostOptimizer(greedy_cb, X, y)\nparams_space = [Real(0.01, 0.8, name='learning_rate'),]\ncb_optimal_values = cb_optimizer.optimize(params_space)","7332777f":"cb = catboost.CatBoostClassifier(n_estimators=4000,\n                         one_hot_max_size=2,\n                         loss_function='MultiClass',\n                         eval_metric='WKappa',\n                         task_type='CPU',                \n                         random_seed=5, \n                         use_best_model=True,\n                         silent=True\n                        )","64b608c9":"one_cb_optimizer = CatboostOptimizer(cb, X, y)\nparams_space = [Real(0.01, 0.8, name='learning_rate'), \n                Integer(2, 10, name='max_depth'), \n                Real(0.5, 1.0, name='colsample_bylevel'), \n                Real(0.0, 100, name='bagging_temperature'), \n                Real(0.0, 100, name='random_strength'), \n                Real(1.0, 100, name='reg_lambda')]\none_cb_optimal_values = one_cb_optimizer.optimize(params_space, max_evals=40, n_random_starts=4)","d73757b9":"one_cb_optimizer.model.get_params()","283a08d2":"def make_classifier():\n    clf = catboost.CatBoostClassifier(\n            n_estimators = 4000,\n            task_type = 'CPU',\n            one_hot_max_size = 2,\n            random_seed = 31,\n            loss_function = 'MultiClass',\n            learning_rate = 0.8,\n            max_depth = 6,\n            colsample_bylevel = 0.5,\n            bagging_temperature = 28.635664398579774,\n            random_strength = 100.0,\n            reg_lambda = 100.0,\n            early_stopping_rounds=500,\n    )\n    return clf\noof = np.zeros(len(X))","35b1ab8b":"from sklearn.model_selection import KFold\noof = np.zeros(len(X))\nNFOLDS = 5\nfolds = KFold(n_splits=NFOLDS, shuffle=True, random_state=2019)\n\ntraining_start_time = time.time()\nfor fold, (trn_idx, test_idx) in enumerate(folds.split(X, y)):\n    start_time = time.time()\n    print(f'Training on fold {fold+1}')\n    clf = make_classifier()\n    clf.fit(X.loc[trn_idx, all_features], y.loc[trn_idx], eval_set=(X.loc[test_idx, all_features], y.loc[test_idx]),\n                          use_best_model=True, verbose=500, cat_features=cat_features)    \n    oof[test_idx] = clf.predict(X.loc[test_idx, all_features]).reshape(len(test_idx))\n    print('Fold {} finished in {}'.format(fold + 1, str(datetime.timedelta(seconds=time.time() - start_time))))\n    \nprint('-' * 30)\nprint('OOF QWK:', qwk(y, oof))\nprint('-' * 30)","f9bc2968":"# train model on all data once\nclf = make_classifier()\nclf.fit(X, y, verbose=500, cat_features=cat_features)","6ea4757e":"# process test set\nnew_test = []\nfor ins_id, user_sample in tqdm(test.groupby('installation_id', sort=False), total=1000):\n    a = get_data(user_sample, test_set=True)\n    new_test.append(a)\n    \nX_test = pd.DataFrame(new_test)\ndel test","4cf7ee57":"# make predictions on test set once\npreds = clf.predict(X_test)\ndel X_test","73fb88ac":"sample_submission['accuracy_group'] = np.round(preds).astype('int')\nsample_submission.to_csv('submission.csv', index=None)\nsample_submission.head()","98a0fd06":"sample_submission['accuracy_group'].plot(kind='hist')","55975f1a":"labels['accuracy_group'].plot(kind='hist')","2a786531":"pd.Series(oof).plot(kind='hist')","21f94001":"clf = deepcopy(one_cb_optimizer.model)\npool = catboost.Pool(X, y, cat_features=cat_features)\nclf.set_params(use_best_model=False, reg_lambda=1.0)\nclf.fit(pool, use_best_model=False)\ninteractions = clf.get_feature_importance(pool, fstr_type=catboost.EFstrType.Interaction, prettified=True)\nshap_values = clf.get_feature_importance(pool, fstr_type=catboost.EFstrType.ShapValues,prettified=True)","9ff3d619":"feature_interaction = [[X.columns[interaction[0]], X.columns[interaction[1]], interaction[2]] for i,interaction in interactions.iterrows()]\nfeature_interaction_df = pd.DataFrame(feature_interaction, columns=['feature1', 'feature2', 'interaction_strength'])\nfeature_interaction_df.head(10)","11c7368a":"pd.Series(index=zip(feature_interaction_df['feature1'], feature_interaction_df['feature2']), data=feature_interaction_df['interaction_strength'].values, name='interaction_strength').head(10).plot(kind='barh', figsize=(18, 10), fontsize=16, color='b')","ecf5116d":"shap.initjs()\nshap.summary_plot(shap_values[:, 0, :-1], X, feature_names=X.columns.tolist())","2ecc54d7":"shap.initjs()\nshap.summary_plot(shap_values[:, 1, :-1], X, feature_names=X.columns.tolist())","17d4dc18":"shap.initjs()\nshap.summary_plot(shap_values[:, 2, :-1], X, feature_names=X.columns.tolist())","13cca489":"shap.initjs()\nshap.summary_plot(shap_values[:, 3, :-1], X, feature_names=X.columns.tolist())","95937458":"shap.summary_plot(shap_values[:, 0,:-1], X, feature_names=X.columns.tolist(), plot_type=\"bar\")","3bffe61d":"shap.dependence_plot(\"accumulated_accuracy\", shap_values[:, 3, :-1], X)","76fafc7f":"**2. Greedy Parameter Tuning**","36234cdb":"# Implementing CatBoost for DS Bowl\ud83e\udd63\nSince this is a practical guide we'll dive right into the competition data and start implementing CatBoost with a few tips here and there and comprehensive explanation of how CatBoost works.\n\nIf you want to get better understanding of this competition data please take a look at some of these great kernels for EDA and introductions.\n\n1. https:\/\/www.kaggle.com\/erikbruin\/data-science-bowl-2019-eda-and-baseline\n\n2. https:\/\/www.kaggle.com\/robikscube\/2019-data-science-bowl-an-introduction\n\n3. https:\/\/www.kaggle.com\/gpreda\/2019-data-science-bowl-eda\n\n<font size=5 color='green'>Please give this kernel an UPVOTE to show your appreciation, if you find it useful.<\/font>\n","ca9f9a7a":"**1. With Default Params**","3d450923":"## The CatBoost Algorithm\n\nCatBoost does gradient boosting in a very elegant manner. Below is an explanation of CatBoost using a toy example\n\nLet\u2019s say, we have 10 data points in our dataset and are ordered in time as shown below.\n\n<img src=\"https:\/\/miro.medium.com\/max\/303\/1*K-2XayuU9Y4OklIlDWg1AQ.png\"><\/img>\n\n> If data doesn\u2019t have time, CatBoost randomly creates an artificial time for each datapoint.\n\n* **Step 1:** Calculate residuals for each datapoint using a model that has been trained on all the other data points at that time (For Example, to calculate residual for x5 datapoint, we train one model using x1, x2, x3 and x4 ). Hence we train different models for different data points . At the end we are calculating residuals for each datapoint that it\u2019s corresponding model has never seen that datapoint before.\n* **Step 2:** Train the model using the residuals of each datapoint\n* **Step 3:** Repeat Step 1 & Step 2 (for n iterations)\n\nFor the above toy dataset, we should train 9 different models to get residuals for 9 data points. This is computationally expensive when we have more number of data points.\nHence by default, instead of training different model for each datapoint, it trains only log(num_of_datapoints) models. Now if a model has been trained on n data points then that model is used to calculate residuals for the next n data points.\n\n* A model that has been trained on first data point is used for calculating residuals of second data point.\n* An another model that has been trained on the first two data points is used for calculating residuals of third and fourth data points\n\nIn the above toy dataset, now we calculate residuals of x5,x6,x7 and x8 using a model that has been trained on x1, x2,x3 and x4.\n\nAll this procedure that I have explained until now is known as ordered boosting\n\n**Random Permutations:**\n\nCatBoost actually divides a given dataset into random permutations and apply ordered boosting on those random permutations. By default CatBoost creates four random permutations. With this randomness we can further stop overfitting our model. We can further control this randomness by tuning parameter bagging_temperature. This is something that you have already seen in other boosting algorithms","05217e30":"## Data Prep","c9b141c7":"**How our data looks like**","24824390":"## Contents\n<a href=\"#Introduction:\">1. Introduction<\/a>  \n<a href=\"#What's-cool?\">2. What's Cool?<\/a>  \n<a href=\"#Why-CatBoost-matters?\">3. Why CatBoost matters?<\/a>\n<br>\n<a href=\"#Implementing-CatBoost-for-DS-Bowl\ud83e\udd63\">4. Implementing CatBoost for DS Bowl \ud83e\udd63<\/a> <br>\n&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#Data-Prep\">4.1 Data Prep<\/a>   \n&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#The-CatBoost-Algorithm\">4.2 The CatBoost Algorithm<\/a>   \n&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#Categorical-Feature-Handling\">4.3 Categorical Feature Handling<\/a> <br>\n&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#Secret-of-CatBoost\">4.4 Secrets of CatBoost<\/a> <br>\n&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#Creating-our-Model-Class:\">4.5 Creating Our Model<\/a> <br>\n&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#Tuning-CatBoost\">4.6 Tuning CatBoost<\/a> <br>\n&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#Model-Analysis\">4.7 Model Analysis<\/a> <br>\n<a href=\"#Key-Takeaways\">5. Key Takeaways<\/a>\n","1eb8c710":"**One Step Optimization**","684fa8b7":"## Categorical Feature Handling\n\n### Ordered Target Statistic:\n\nMost of the GBDT algorithms and Kaggle competitors are already familiar with the use of Target Statistic (or target mean encoding).\n\n> It\u2019s a simple yet effective approach in which we encode each categorical feature with the estimate of the expected target y conditioned by the category.\n\nWell, it turns out that applying this encoding carelessly (average value of y over the training examples with the same category) results in a target leakage.\n\nTo fight this prediction shift CatBoost uses a more effective strategy. It relies on the ordering principle and is inspired by online learning algorithms which get training examples sequentially in time. In this setting, the values of TS for each example rely only on the observed history.\n\nTo adapt this idea to a standard offline setting, Catboost introduces an artificial \u201ctime\u201d\u2014 a random permutation \u03c31 of the training examples.\n\nThen, for each example, it uses all the available \u201chistory\u201d to compute its Target Statistic.\nNote that, using only one random permutation, results in preceding examples with higher variance in Target Statistic than subsequent ones. To this end, CatBoost uses different permutations for different steps of gradient boosting.\n\n### One-Hot Encoding:\n\n* By default, CatBoost internally represents all the categorical features with One-hot encoding if and only if a categorical feature has two different categories.\n\n* If you would like to implement One-hot encoding on a categorical feature that has N different categories then you can change parameter one_hot_max_size = N.","6d85835b":"## Model Analysis\nIn addition to feature importance, which is quite popular for GBDT models to share, Catboost provides feature interactions and object (row) importance","eb9d8ff8":"# Come, lets dive together!\n<img src='https:\/\/avatars3.githubusercontent.com\/u\/29043415?s=300&v=3' style=\"float:right; width:10%\"><\/img>\n\n> **I guess a lot of us use CatBoost but how many of us actually understand what's going on behind the scenes?** \n> In this kernel we'll be exploring how CatBoost works and some key insights that you may find helpful in this competition.","3caf4a09":"<font size=5 color='red'>Please give this kernel an UPVOTE to show your appreciation, if you find it useful.<\/font>","e1ce2671":"**Creating Submission File:**","d69d3d87":"## Introduction:\n\nIn the past few years anyone who has actively learned or implemented Machine Learning either in Kaggle or in real-life would have known about Gradient Boosted Machines for sure, that's how popular, useful and efficient they are when compared to other techniques. In fact, Gradient Boosted Decision Trees and Random Forest are my favorite ML models for tabular heterogeneous datasets. These models are the top performers on Kaggle competitions and are used widely in the industry.\n\nSo what is CatBoost you may ask. It is known as **Categorical Gradient Boosting.**. (If that didn't annoy you, there are no cats in ML atleast as of now)\n\n> **CatBoost is based on gradient boosting. A relatively new machine learning technique developed by Yandex that outperforms many other existing boosting algorithms like XGBoost and Light GBM.**\n\n<img src=\"https:\/\/miro.medium.com\/max\/1200\/1*2p1GIUUcRSzyyJjSj4x7Iw.jpeg\" style=\"width:60%\"><\/img>\n\n\n## What's cool?\nWhile deep learning algorithms requires lots of data and computational power, boosting algorithms are still in need for most of the business problems. However boosting algorithms like XGBoost takes hours to train and sometimes you\u2019ll get frustrated while tuning hyper-parameters.\n\nOn the other hand, CatBoost is **easy to implement and very powerful**. It provides impressive results in it\u2019s very first run.\n\n> One main difference between CatBoost and other boosting algorithms is that the CatBoost implements **symmetric trees**. This may sound crazy but helps in **decreasing prediction time**, which is extremely important for low latency environments.\n\n\n## Why CatBoost matters?\n\n**1. Improved Results**\n\nCatboost achieves the best results on the benchmark, and that\u2019s great. (Although the company who conducted the benchmark has a clear interest in the favor of Catboost \ud83d\ude05) Though, when you look at datasets where categorical features play a large role, such as Amazon and the Internet datasets, this improvement becomes significant and undeniable.\n\n<img src=\"https:\/\/miro.medium.com\/max\/1818\/1*vsg1IUlGtzCoNuGo9XqGwg.png\" style=\"width:70%\"><\/img>\n\n**2. FAST**\n\nWhile training time can take up longer than other GBDT implementations, prediction time is 13\u201316 times faster than the other libraries according to the Yandex benchmark\n\n<img src=\"https:\/\/miro.medium.com\/max\/1664\/1*BE8PZe54DMWe6gFdHlYsxg.png\" style=\"width:70%\"><\/img>\n\n**3. Tried and Tested:**\n\nYandex is relying heavily on Catboost for ranking, forecasting and recommendations. This model is serving more than 70 million users each month.\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/d\/db\/Yandex_Logo.svg\/1200px-Yandex_Logo.svg.png\" style=\"width:30%\"><\/img>\n","bebe4c50":"## References and Credits:\nThis notebook wouldn't have been possible without these amazing resources. Some of the text and most of the figures used in this notebooks are taken from the below mentioned resources, combining everything into one.\n1. [CatBoost Demystified - Sharanyu Rane](https:\/\/towardsdatascience.com\/catboost-demystified-8b0b538bfa31)\n2. [Deep Dive into Catboost Functionalities for Model Interpretation - Alwira Swain](https:\/\/towardsdatascience.com\/deep-dive-into-catboost-functionalities-for-model-interpretation-7cdef669aeed)\n3. [CatBoost Team's Tutorials](https:\/\/github.com\/catboost\/tutorials)\n4. [Working with categorical data: Catboost - Katerina](https:\/\/medium.com\/whats-your-data\/working-with-categorical-data-catboost-8b5e11267a37)\n5. [Machine Learning with CatBoost - Jitao David Zhang](https:\/\/accio.github.io\/machinelearning\/2018\/05\/30\/catboost.html)\n6. [What's so special about CatBoost - Hanish Sai Rohit Pallapothu\n](https:\/\/medium.com\/@hanishsidhu\/whats-so-special-about-catboost-335d64d754ae)\n7. [CatBoost: A machine learning library to handle categorical (CAT) data automatically - Sunil Ray](https:\/\/www.analyticsvidhya.com\/blog\/2017\/08\/catboost-automated-categorical-data\/)","0098ded3":"<font size=5 color='red'>Please give this kernel an UPVOTE to show your appreciation, if you find it useful.<\/font>","59870983":"## Secret of CatBoost\n\nCatboost introduces two critical algorithmic advances - the implementation of ordered boosting, a permutation-driven alternative to the classic algorithm, and an innovative algorithm for processing categorical features.\nBoth techniques are using random permutations of the training examples to fight the prediction shift caused by a special kind of target leakage present in all existing implementations of gradient boosting algorithms.\n\n<img src=\"https:\/\/miro.medium.com\/max\/1796\/1*nTMRk-U4KRFra3j8VMFz0A.png\" style=\"width:70%\"><\/img>","4c70045e":"### Limitation\n> **When the dataset has many numerical features, CatBoost takes more time to train than Light GBM.**","ed859966":"## Key Takeaways\n\n* Catboost is built with a similar approach and attributes as with the \u201colder\u201d generation of GBDT models.\n* Catboost\u2019s power lies in its **categorical features preprocessing**, **prediction time** and **model analysis**.\n* Catboost\u2019s weaknesses are its training and optimization times.\n* Don\u2019t forget to pass `cat_features` argument to the classifier object. You aren\u2019t really utilizing the power of Catboost without it.\n* Though Catboost performs well with default parameters, there are several parameters that drive a significant improvement in results when tuned.","f6ebde89":"### Handling Numerical Features\n\nCatBoost handle the numerical features in the same way that other tree algorithms do. We select the best possible split based on the Information Gain.\n\n","752a280a":"## Creating our Model Class:","0e441241":"## Tuning CatBoost\n\n`cat_features` \u2014 This parameter is a must in order to leverage Catboost preprocessing of categorical features, if you encode the categorical features yourself and don\u2019t pass the columns indices as cat_features you are missing the essence of Catboost.\n\n`one_hot_max_size` - As Catboost uses one-hot encoding for all features with at most one_hot_max_size unique values. In our case, the categorical features have a lot of unique values, so we won\u2019t use \none hot encoding, but depending on the dataset it may be a good idea to adjust this parameter.\n\n`learning_rate & n_estimators` \u2014 The smaller the learning_rate, the more n_estimators needed to utilize the model. Usually, the approach is to start with a relative high learning_rate, tune other parameters and then decrease the \nlearning_rate while increasing n_estimators.\n\n`max_depth` \u2014 Depth of the base trees, this parameter has an high impact on training time.\n\n`subsample` \u2014 Sample rate of rows, can\u2019t be used in a Bayesian boosting type setting.\n\n`colsample_bylevel`, `colsample_bytree`, `colsample_bynode`\u2014 Sample rate of columns.\n\n`l2_leaf_reg` \u2014 L2 regularization coefficient\n\n`random_strength` \u2014 Every split gets a score and random_strength is adding some randomness to the score, it helps to reduce overfitting.\n"}}