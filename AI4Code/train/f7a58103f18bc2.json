{"cell_type":{"b5f41ce2":"code","fa90585b":"code","05abe2ff":"code","d00139c3":"code","549a362f":"code","0c7ec2b4":"code","d2018f77":"code","664dd2ce":"code","3207eff8":"code","04585ef1":"code","561ba8ea":"code","814fc138":"code","7d23d27e":"code","73d70f38":"code","df1a3746":"code","911f5315":"code","d56847a3":"code","8faa7447":"code","b94a4368":"code","27375686":"markdown","8a4252cf":"markdown","e0fda511":"markdown","4d7dc271":"markdown","bdbe7894":"markdown","aecb0b20":"markdown","e6bbddfa":"markdown","0334c1f5":"markdown","3eed3089":"markdown","675660f3":"markdown"},"source":{"b5f41ce2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport sklearn\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pandas_profiling import ProfileReport\nsns.set_style('darkgrid')\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","fa90585b":"data = pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndata.head(3)","05abe2ff":"profile = ProfileReport(data, title=\"Pandas Profiling Report\")","d00139c3":"## Printing the complete Data Analysis Report!\n## we have a balanced dataset\nprofile","549a362f":"## The 'Outcome' column tells us whether the person has Diabetes or not\n## 1- Diabetic\n## 0- Non Diabetic\n\nX = data.iloc[:,:-1] # Independent variables\ny = data['Outcome'] # Dependent Variables","0c7ec2b4":"data = data.drop(data[data['Pregnancies']>11].index)\ndata = data.drop(data[data['Glucose']<30].index)\ndata = data.drop(data[data['BloodPressure']>110].index)\ndata = data.drop(data[data['BloodPressure']<20].index)\ndata = data.drop(data[data['SkinThickness']>80].index)\ndata = data.drop(data[data['BMI']>55].index)\ndata = data.drop(data[data['BMI']<10].index)\ndata = data.drop(data[data['DiabetesPedigreeFunction']>1.6].index)\ndata = data.drop(data[data['Insulin']>400].index)\ndata = data.drop(data[data['Age']>80].index)","d2018f77":"plt.figure(figsize=(11,10))\ncorrelation = X.corr()\nsns.heatmap(correlation,linewidth = 0.7,cmap = 'Blues',annot = True)","664dd2ce":"X = X.loc[data.index]\ny = y.loc[data.index]","3207eff8":"from sklearn.model_selection import train_test_split\nX_train,X_val,y_train,y_val = train_test_split(X,y,test_size = 0.25,random_state = 100)","04585ef1":"from sklearn.metrics import log_loss,accuracy_score,confusion_matrix,f1_score,recall_score\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_score,StratifiedKFold,RandomizedSearchCV\nxgb = XGBClassifier(booster ='gbtree',objective ='binary:logistic')","561ba8ea":"from sklearn.model_selection import RandomizedSearchCV\n\nparam_lst = {\n    'learning_rate' : [0.01, 0.1, 0.15, 0.3, 0.5,0.4],\n    'n_estimators' : [100, 500, 1000,1500,2000],\n    'max_depth' : [2,3,5, 6,8, 9],\n    'min_child_weight' : [1, 5, 10],\n    'reg_alpha' : [0.001, 0.01, 0.1],\n    'reg_lambda' : [0.001, 0.01, 0.1],\n    'colsample_bytree' : [0.3,0.4,0.5,0.7],\n    'gamma' : [0.0,0.1,0.2,0.3,0.4]\n}\n\nxgb_tuning = RandomizedSearchCV(estimator = xgb, param_distributions = param_lst ,\n                          n_iter = 5,\n                        cv =6)\n       \nxgb_search = xgb_tuning.fit(X_train,y_train,\n                           early_stopping_rounds = 5,\n                           eval_set=[(X_val,y_val)],\n                           verbose = False)\n\n## checking for the best paramter values that the model took\n\nbest_param = xgb_search.best_params_\nxgb = XGBClassifier(**best_param)\nprint(best_param)","814fc138":"## check the best estimators\nxgb_search.best_estimator_","7d23d27e":"y_pred = xgb_search.predict(X_val)\nscore0 = accuracy_score(y_pred,y_val)\n#print(round(score0*100,4))\nprint('Score: {}%'.format(round(score0*100,4)))","73d70f38":"## checking  'Accuracy' value using Cross Validation menthod\nacc_scores1_xgb =  cross_val_score(xgb_search,X,y,n_jobs=5,\n                                 cv = StratifiedKFold(n_splits=10),\n                                 scoring = 'accuracy')\nacc_scores1_xgb","df1a3746":"from sklearn.metrics import RocCurveDisplay\nfrom sklearn.metrics import roc_auc_score,roc_curve,auc\nfrom sklearn import metrics\n\nfpr, tpr, threshold = metrics.roc_curve(y_val, y_pred)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'red', label = 'ROC AUC score = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'b--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","911f5315":"from sklearn.linear_model import LogisticRegression\nlr= LogisticRegression()\nlr.fit(X_train,y_train)","d56847a3":"log_scores_logi = -1 * cross_val_score(lr, X, y,\n                              cv=5,\n                              scoring='neg_log_loss')\nacc_scores1_logi =  cross_val_score(lr,X,y,\n                                 cv = 5,\n                                 scoring = 'accuracy')\nf_score_logi =  cross_val_score(lr,X,y,\n                                 cv = 5,\n                                 scoring = 'f1')","8faa7447":"print(\"log_loss scores:\\n\", log_scores_logi)\nprint(\"Accuracy scores:\\n\", acc_scores1_logi)\nprint(\"f1_score scores:\\n\", f_score_logi)","b94a4368":"print(acc_scores1_logi.mean())","27375686":"### This Notebook will come in handy to anyone who wants to revise the important concepts such as  \n- **Hyperparamter Tuning**\n- **Using Cross Validation**\n- **Boosting method like XGBClassifier**\n\n- References:\n - [Tuning](https:\/\/www.kaggle.com\/angqx95\/data-science-workflow-top-2-with-tuning)\n - [GridSearchCV vs Random](https:\/\/github.com\/Lokeshrathi\/HyperParamter_optimisation)\n \n## Do **Upvote** and comment your thoughts on this! ","8a4252cf":"ROC-AUC Curve:\n\n- An ROC curve (Receiver Operating Characteristic curve) is a graph showing the performance of a classification model at all classification thresholds.\n\n- Y-Axis shows the True Positive Rate\n- X-Axis shows the False Positive Rate\n\nTrue postive Rate is also known as Recall and is given by True Postive\/(True Positive+False Negative)\n\nFalse Postive Rate is given by False Postive\/(False Positive+True Negative)\n\n![Confusion Matrix](https:\/\/glassboxmedicine.files.wordpress.com\/2019\/02\/confusion-matrix.png?w=816)","e0fda511":"## Using Cross Validation with Logistic Regression","4d7dc271":"## Data Modeling\n\n- Step 1. Divide the Dataset into Test and Train set, where 75% data belongs to Train set and 25% to test size\n\n- Step 2: Using the imported library train your model and then fit it.\n\n- Using RandomSearchCV, try to get the best paramters for best results","bdbe7894":"## As a Beginner in Data Science or into Kaggle, this Notebook will come in handy for implementing Machine Learning Models.\n\n### Topics Covered:\n- Pandas Profiling(Preferred for Data Analysis)\n- Data Scaling\n- Data Modeling\n- Hyper-parameter Tuning.\n- Cross Validation\n- Predictions","aecb0b20":"## Data Scaling\n\n### Note: DataScaling will not be required for this Notebook, since we are using Boosting method of Decision Trees which doesn't mind the distance between the data points\n\n**Although here are the steps to implement the same**\n\n- from sklearn.preprocessing import RobustScaler\n- cols = X.columns\n- transformer = RobustScaler().fit(X[cols])\n- X[cols] = transformer.transform(X[cols])","e6bbddfa":"# Diabetes in India\n\nOver 30 million have now been diagnosed with diabetes in India. The CPR (Crude prevalence rate) in the urban areas of India is thought to be 9 per cent.\n\nIn rural areas, the prevalence is approximately 3 per cent of the total population.\n\nThe population of India is now more than 1000 million: this helps to give an idea of the scale of the problem.\n\n**The estimate of the actual number of diabetics in India is around 40 million.**\n\n**This means that India actually has the highest number of diabetics of any one country in the entire world. IGT (Impaired Glucose Tolerance) is also a mounting problem in India.**\n\n[Source Link](https:\/\/www.diabetes.co.uk\/global-diabetes\/diabetes-in-india.html)\n\n![Diabetes in India](https:\/\/www.thehindu.com\/sci-tech\/health\/thu33t\/article29975026.ece\/alternates\/FREE_615\/TH15diabetescol)\n\n[Image Source](https:\/\/www.thehindu.com\/sci-tech\/health\/india-has-second-largest-number-of-people-with-diabetes\/article29975027.ece)","0334c1f5":"### You can find more information about Hyperparameter tuning using this [Github Repo](https:\/\/github.com\/Lokeshrathi\/HyperParamter_optimisation)","3eed3089":"## Data Cleaning\n### Removing outliers from the Independent variables","675660f3":"## 1. Daily Analysis using Pandas profiling library [Resource](https:\/\/pypi.org\/project\/pandas-profiling\/)"}}