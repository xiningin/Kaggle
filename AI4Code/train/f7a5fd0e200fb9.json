{"cell_type":{"6e9a4928":"code","438e151f":"code","6a1a82d7":"code","6a9524ef":"code","741e28fb":"code","f16af1a0":"code","ca16a050":"code","e00ce13e":"code","b83b21cd":"code","6172444c":"code","d944384d":"code","d4237a7c":"code","c6abe87a":"code","a4dfc648":"code","2a93d039":"code","bc42fc9d":"code","bb56f001":"code","b421b87c":"code","d2596119":"code","a32598d4":"code","c1f52ca0":"code","71d7e9c7":"code","61de049e":"code","4812c495":"code","f37af1b4":"code","d2841638":"code","85bdb893":"code","f2a4b8c0":"code","d2c9482b":"code","9d73d0c8":"code","5514c6b6":"code","52885601":"code","1efad6ca":"code","4b01249a":"code","7de5a11a":"code","24cbe698":"code","b8d4f740":"code","776d7a93":"code","029e4a1f":"markdown","7deda220":"markdown","5d6246a4":"markdown","1b5557fa":"markdown","6d477e6d":"markdown","dbcfc4cc":"markdown","3a5ef5dd":"markdown","e59311d0":"markdown","05635263":"markdown","b4b66c8f":"markdown","bc16c64f":"markdown","252a59b4":"markdown","5b8b7a3a":"markdown","0ab10076":"markdown","1de26cf6":"markdown","2b6d1f86":"markdown","2ff75aa0":"markdown","b8ea465c":"markdown","ebbbe079":"markdown","ff2cf6a8":"markdown","dc4da5c7":"markdown","e673d025":"markdown"},"source":{"6e9a4928":"!pip3 install ipywidgets\n!jupyter nbextension enable --py --sys-prefix widgetsnbextension","438e151f":"#!pip install --upgrade pip\n#!pip install fastai==0.7.0    ## Installed from personal Github repo to avoid numpy rounding error : \n                               ## https:\/\/forums.fast.ai\/t\/unfamiliar-error-when-running-learn-fit\/35075\/19\n!pip install torchtext==0.2.3\n#!pip intall numpy==1.15.1   ## attirbute error thrown due to numpy updates. Changed fastai source code though\n!pip install Pillow==4.1.1\n!pip install blosc","6a1a82d7":"%load_ext autoreload\n%autoreload 2\n%matplotlib inline","6a9524ef":"from fastai.imports import *\nfrom fastai.torch_imports import *\nfrom fastai.io import *","741e28fb":"import os\nimport pandas as pd\nimport pickle\nimport gzip","f16af1a0":"((x, y), (x_valid, y_valid), _) = pickle.load(gzip.open('..\/input\/mnist.pkl.gz', 'rb'), encoding='latin-1')","ca16a050":"type(x), x.shape , type(y), y.shape","e00ce13e":"mean = x.mean()\nstd = x.std()\n\nx=(x-mean)\/std\nmean, std, x.mean(), x.std()","b83b21cd":"x_valid = (x_valid-mean)\/std\nx_valid.mean(), x_valid.std()","6172444c":"def show(img, title=None):\n    plt.imshow(img, cmap=\"gray\")\n    if title is not None: plt.title(title)","d944384d":"def plots(ims, figsize=(12,6), rows=2, titles=None):\n    f = plt.figure(figsize=figsize)\n    cols = len(ims)\/\/rows\n    for i in range(len(ims)):\n        sp = f.add_subplot(rows, cols, i+1)\n        sp.axis('Off')\n        if titles is not None: sp.set_title(titles[i], fontsize=16)\n        plt.imshow(ims[i], cmap='gray')","d4237a7c":"x_valid.shape","c6abe87a":"x_imgs = np.reshape(x_valid, (-1,28,28)); x_imgs.shape","a4dfc648":"show(x_imgs[0], y_valid[0])","2a93d039":"y_valid.shape","bc42fc9d":"y_valid[0]","bb56f001":"x_imgs[0,10:15,10:15]","b421b87c":"show(x_imgs[0,10:15,10:15])","d2596119":"plots(x_imgs[:8], titles=y_valid[:8])","a32598d4":"from fastai.metrics import *\nfrom fastai.model import *\nfrom fastai.dataset import *\n\nimport torch.nn as nn","c1f52ca0":"net = nn.Sequential(\n    nn.Linear(28*28, 100),\n    nn.ReLU(),\n    nn.Linear(100, 100),\n    nn.ReLU(),\n    nn.Linear(100, 10),\n    nn.LogSoftmax()\n#).cuda()  ## For GPU\n)         ## For CPU","71d7e9c7":"md = ImageClassifierData.from_arrays('..\/input\/mnist.pkl.gz', (x,y), (x_valid, y_valid))","61de049e":"loss=nn.NLLLoss()\nmetrics=[accuracy]\n# opt=optim.SGD(net.parameters(), 1e-1, momentum=0.9)\nopt=optim.SGD(net.parameters(), 1e-1, momentum=0.9, weight_decay=1e-3)","4812c495":"def binary_loss(y, p):\n    return np.mean(-(y * np.log(p) + (1-y)*np.log(1-p)))","f37af1b4":"acts = np.array([1, 0, 0, 1])\npreds = np.array([0.9, 0.1, 0.2, 0.8])\nbinary_loss(acts, preds)","d2841638":"fit(net, md, n_epochs=5, crit=loss, opt=opt, metrics=metrics)","85bdb893":"set_lrs(opt, 1e-2)","f2a4b8c0":"fit(net, md, n_epochs=3, crit=loss, opt=opt, metrics=metrics)","d2c9482b":"fit(net, md, n_epochs=5, crit=loss, opt=opt, metrics=metrics)","9d73d0c8":"set_lrs(opt, 1e-2)","5514c6b6":"fit(net, md, n_epochs=3, crit=loss, opt=opt, metrics=metrics)","52885601":"t = [o.numel() for o in net.parameters()]\nt, sum(t)","1efad6ca":"preds = predict(net, md.val_dl)","4b01249a":"preds.shape","7de5a11a":"preds.argmax(axis=1)[:5]","24cbe698":"preds = preds.argmax(1)","b8d4f740":"np.mean(preds == y_valid)","776d7a93":"plots(x_imgs[:8], titles=preds[:8])","029e4a1f":"Each input is a vector of size `28*28` pixels and our output is of size `10` (since there are 10 digits: 0, 1, ..., 9). \n\nWe use the output of the final layer to generate our predictions.  Often for classification problems (like MNIST digit classification), the final layer has the same number of outputs as there are classes.  In that case, this is 10: one for each digit from 0 to 9.  These can be converted to comparative probabilities.  For instance, it may be determined that a particular hand-written image is 80% likely to be a 4, 18% likely to be a 9, and 2% likely to be a 3.","7deda220":"Let's see how some of our predictions look!","5d6246a4":"Note that for consistency (with the parameters we learn when training), we subtract the mean and standard deviation of our training set from our validation set. ","1b5557fa":"### Fitting the model","6d477e6d":"#### Helper methods","dbcfc4cc":"In any sort of data science work, it's important to look at your data, to make sure you understand the format, how it's stored, what type of values it holds, etc. To make it easier to work with, let's reshape it into 2d images from the flattened 1d format.","3a5ef5dd":"### Look at the data","e59311d0":"## Neural Net in PyTorch","05635263":"It's the digit 3!  And that's stored in the y value:","b4b66c8f":"We will begin with the highest level abstraction: using a neural net defined by PyTorch's Sequential class.  ","bc16c64f":"We can look at part of an image:","252a59b4":"#### Plots ","5b8b7a3a":"Let's download, unzip, and format the data.","0ab10076":"Let's check how accurate this approach is on our validation set. ","1de26cf6":"<img src=\"images\/mnist.png\" alt=\"\" style=\"width: 60%\"\/>","2b6d1f86":"*Fitting* is the process by which the neural net learns the best parameters for the dataset.","2ff75aa0":"### Normalize","b8ea465c":"Many machine learning algorithms behave better when the data is *normalized*, that is when the mean is 0 and the standard deviation is 1. We will subtract off the mean and standard deviation from our training set in order to normalize the data:","ebbbe079":"### Loss functions and metrics","ff2cf6a8":"## Imports and data","dc4da5c7":"Why not just maximize accuracy? The binary classification loss is an easier function to optimize.\n\nFor multi-class classification, we use *negative log liklihood* (also known as *categorical cross entropy*) which is exactly the same thing, but summed up over all classes.","e673d025":"## Using SGD on MNIST\n\nHere I am just implementing the notebook from fast.ai's MOOC on ML : http:\/\/course18.fast.ai\/lessonsml1\/lesson8.html.\nI also wanted to try out the GPUs on Kaggle kernels so this seemed like the ideal notebook to replicate and experiment with."}}