{"cell_type":{"4e637c7b":"code","c4764c33":"code","c6bde226":"code","d76ea759":"code","08b2227c":"code","1d3d08e4":"code","8b17ada8":"code","a5df9567":"code","b29dd7ee":"code","3c30c628":"code","35a6bb49":"code","44bf3496":"code","9c7f05d6":"code","f3642899":"code","43cbd736":"code","93b05f8b":"code","63060350":"code","e477a43d":"code","72a34a93":"code","bef8c795":"code","a772cb6e":"code","9a74c194":"code","ecec21b3":"code","21493535":"code","8edeca7d":"code","53a7e0bc":"code","41d7985c":"code","b753e579":"code","c03221bc":"code","cc4c2fe7":"code","e5bee6f6":"code","d632bcba":"code","41099fa7":"code","ac0434fa":"code","93f9a02e":"code","544e619f":"code","0e029066":"code","fa90777a":"code","7a6ddf3c":"code","3f51423a":"code","f7a1e788":"code","f4b546bd":"code","5b1369c6":"code","d551b844":"code","362bbf9a":"code","7a9b0696":"code","3c4a1778":"code","eb3d5fe7":"code","6a184f4c":"code","57eb51cd":"code","42eddfbb":"code","26cec985":"markdown","33e0999a":"markdown","dbef0be0":"markdown","8869834a":"markdown","b826d834":"markdown","e1fee355":"markdown","d3ffc1e3":"markdown","abd182d8":"markdown","f48f459e":"markdown","c34ad0cb":"markdown","592d68c3":"markdown","4163c41f":"markdown","cd5e26d2":"markdown","1cfec8c6":"markdown","d2ed9759":"markdown","67149f2a":"markdown","b0f80b38":"markdown","068566f9":"markdown","f1024f6b":"markdown","c20cd606":"markdown","4344f643":"markdown","75b93021":"markdown","1e4ce121":"markdown","8e7f6048":"markdown","e80596c9":"markdown","a485bbe5":"markdown","95d52688":"markdown","25af8cca":"markdown"},"source":{"4e637c7b":"# Core\nimport pandas as pd\nimport numpy as np\n\n# Data Visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\npd.options.display.max_columns = None\npd.options.display.max_rows = 80\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c4764c33":"train_path = '\/kaggle\/input\/restaurantrevenue\/train.csv'\ntest_path = '\/kaggle\/input\/restaurantrevenue\/test.csv'\n\ndata_train = pd.read_csv(train_path)\ndata_test = pd.read_csv(test_path)\nIDtest = data_test['Id']","c6bde226":"data_train.head()","d76ea759":"# Numerical columns\nnum_col = data_train.select_dtypes(exclude=['object']).drop(['Id'], axis=1).columns","08b2227c":"num_col","1d3d08e4":"# Categorical columns\ncat_col = data_train.select_dtypes(include=['object']).columns","8b17ada8":"cat_col","a5df9567":"# Describe data train\ndata_train[num_col].describe().round(decimals=2)","b29dd7ee":"# Plot skew\nfig = plt.figure(figsize=(12,18))\nfor i in range(len(num_col)):\n    fig.add_subplot(10,4,i+1)\n    sns.distplot(data_train[num_col[i]], kde_kws={'bw': 0.1})\n    plt.title('Skew : %.2f' % data_train[num_col[i]].skew())\n    \nplt.tight_layout()\nplt.show()","3c30c628":"# Revenue with log\nsns.distplot(np.log(data_train['revenue']))\nplt.title('Skew : %.2f' % np.log(data_train['revenue']).skew())","35a6bb49":"# Univariate analysis - boxplot\nfig = plt.figure(figsize=(12,18))\nfor i in range(len(num_col)):\n    fig.add_subplot(10,4,i+1)\n    sns.boxplot(y=data_train[num_col[i]])\n    \nplt.tight_layout()\nplt.show()","44bf3496":"# Bivariate analysis - scatterplot\nfig = plt.figure(figsize=(12,18))\nfor i in range(len(num_col)):\n    fig.add_subplot(10,4,i+1)\n    sns.scatterplot(data_train[num_col[i]], data_train['revenue'])\n    \nplt.tight_layout()\nplt.show()","9c7f05d6":"# Correlation\ncorrelation = data_train[num_col].corr()\n\nf, ax = plt.subplots(figsize=(14,12))\nplt.title('Correlation of numerical attributes', size=16)\nsns.heatmap(correlation)\nplt.show()","f3642899":"correlation['revenue'].sort_values(ascending=False)","43cbd736":"# Missing value\ndata_train.isna().sum()","93b05f8b":"data_train[cat_col].describe()","63060350":"# Bivariate analysis - box plot\nf, ax = plt.subplots(figsize=(12,8))\nsns.boxplot(y=data_train['revenue'], x=data_train['Type'])\nplt.xticks(rotation=40)\nplt.show()","e477a43d":"# Bivariate analysis - box plot\nf, ax = plt.subplots(figsize=(12,8))\nsns.boxplot(y=data_train['revenue'], x=data_train['City Group'])\nplt.xticks(rotation=40)\nplt.show()","72a34a93":"# Transform Open date to age\nfrom datetime import datetime\n\ndef count_years(open_date):\n    date_parse = datetime.strptime(open_date, '%m\/%d\/%Y')\n    date_now = datetime.now()\n    return date_now.year - date_parse.year","bef8c795":"open_years = []\nfor i in data_train['Open Date']:\n    open_years.append(count_years(i))\n\ndf_open_years = pd.DataFrame({ 'open_years' : open_years } )\ngroup_years = df_open_years['open_years'].value_counts()","a772cb6e":"# Barplot open years\nsns.barplot(x=group_years.index, y=group_years.values)","9a74c194":"# City\ncity_most = data_train['City'].value_counts()[data_train['City'].value_counts() > 2].index","ecec21b3":"city_transform = []\n\nfor i in data_train['City']:\n    if i in city_most:\n        city_transform.append(i)\n    else:\n        city_transform.append('other')\n        \ndf_city_transform = pd.DataFrame({ 'city_transform' : city_transform } )\ngroup_city = df_city_transform['city_transform'].value_counts()","21493535":"# Barplot cgroup city\nsns.barplot(x=group_city.index, y=group_city.values)","8edeca7d":"data_train_copy = data_train.copy()\ndata_train_copy['revenue_log'] = np.log(data_train_copy['revenue'])","53a7e0bc":"data_train_copy.head()","41d7985c":"transformed_corr = data_train_copy.corr()\nplt.figure(figsize=(12,10))\nsns.heatmap(transformed_corr)","b753e579":"attr_select = ['Open Date', 'City', 'City Group', 'Type', 'P2', 'P6', 'P13', 'P28', 'P29', 'revenue_log']\ntrain_select = data_train_copy[attr_select]","c03221bc":"# Transform Open Year\nopen_years = []\nfor i in train_select['Open Date']:\n    open_years.append(count_years(i))\n    \ndf_open_years = pd.DataFrame({ 'open_years' : open_years } )\ngroup_years = df_open_years['open_years'].value_counts()\n\n# Transform City\ncity_most = train_select['City'].value_counts()[train_select['City'].value_counts() > 2].index\ncity_transform = []\n\nfor i in train_select['City']:\n    if i in city_most:\n        city_transform.append(i)\n    else:\n        city_transform.append('other')\n        \ndf_city_transform = pd.DataFrame({ 'city_transform' : city_transform } )\ngroup_city = df_city_transform['city_transform'].value_counts()\n\ntrain_final = pd.concat([train_select, df_open_years, df_city_transform], axis=1).drop(['Open Date', 'City'], axis=1)","cc4c2fe7":"train_final.head()","e5bee6f6":"# Preprare data\nX = train_final.drop(['revenue_log'], axis=1)\ny = train_final['revenue_log']\n\nX = pd.get_dummies(X)\n\nX = np.array(X)\ny = np.array(y)","d632bcba":"from sklearn.model_selection import GridSearchCV, cross_val_score, learning_curve\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor","41099fa7":"random_state = 2\nclassifiers = []\nclassifiers.append(Lasso(random_state=random_state))\nclassifiers.append(LinearRegression())\nclassifiers.append(Ridge(random_state=random_state))\nclassifiers.append(ElasticNet(random_state=random_state))\nclassifiers.append(KNeighborsRegressor())\nclassifiers.append(SVR())\nclassifiers.append(RandomForestRegressor(random_state=random_state))\nclassifiers.append(GradientBoostingRegressor())\nclassifiers.append(AdaBoostRegressor(random_state = random_state))\nclassifiers.append(DecisionTreeRegressor())\nclassifiers.append(XGBRegressor())\n\n\ncv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier, X, y, scoring='neg_mean_squared_error', cv =10, n_jobs=4))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"Lasso\",\"LinearRegression\",\"Ridge\",\n\"ElasticNet\",\"KNeighborsRegressor\",\"SVR\",\"RandomForestRegressor\",\"GradientBoostingRegressor\",\"AdaBoostRegressor\",\"DecisionTreeRegressor\", \"XGBRegressor\"]})\n","ac0434fa":"g = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")","93f9a02e":"cv_res.sort_values(ascending=False, by='CrossValMeans')","544e619f":"# SVR\nmodel = SVR()\n\n# Search grid for optimal parameters\nex_param_grid = {\n    'kernel': ['rbf'],\n    'gamma': [1e-4, 1e-3, 0.01, 0.1],\n    'C': [1, 10, 100]\n}\n\ngsSVR = GridSearchCV(model, \n                     param_grid = ex_param_grid, \n                     cv=10, \n                     scoring=\"neg_mean_squared_error\")\n\ngsSVR.fit(X, y)\nSVR_best = gsSVR.best_estimator_\n\n# Best score\nprint('SVR')\nprint('Best score : ', gsSVR.best_score_)\nprint('Best params : ', gsSVR.best_params_)","0e029066":"# AdaBoostRegressor\nmodel = AdaBoostRegressor(random_state = random_state)\n\n# Search grid for optimal parameters\nex_param_grid = {\n    'n_estimators': [50, 100],\n    'learning_rate' : [0.01,0.05,0.1,0.3,1],\n    'loss' : ['linear', 'square', 'exponential']\n}\n\ngsABR = GridSearchCV(model, \n                     param_grid = ex_param_grid, \n                     cv=10, \n                     scoring=\"neg_mean_squared_error\")\n\ngsABR.fit(X, y)\nABR_best = gsABR.best_estimator_\n\n# Best score\nprint('ABR')\nprint('Best score : ', gsABR.best_score_)\nprint('Best params : ', gsABR.best_params_)","fa90777a":"# RandomForestRegressor\nmodel = RandomForestRegressor(random_state = random_state)\n\n# Search grid for optimal parameters\nex_param_grid = {\n    'n_estimators'      : [10,20,30,40,50],\n    'max_features'      : [\"auto\", \"sqrt\", \"log2\"],\n    'min_samples_split' : [2,4,8,10,12,14,16]\n}\n\ngsRFR = GridSearchCV(model, \n                     param_grid = ex_param_grid, \n                     cv=10, \n                     scoring=\"neg_mean_squared_error\")\n\ngsRFR.fit(X, y)\nRFR_best = gsRFR.best_estimator_\n\n# Best score\nprint('RFR')\nprint('Best score : ', gsRFR.best_score_)\nprint('Best params : ', gsRFR.best_params_)","7a6ddf3c":"# KNeighborsRegressor\nmodel = KNeighborsRegressor()\n\n# Search grid for optimal parameters\nex_param_grid = {\n    'n_neighbors': [4,6,8,10],\n    'leaf_size': [30,40,50,60],\n    'weights': ['uniform','distance']\n}\n\ngsKNR = GridSearchCV(model, \n                     param_grid = ex_param_grid, \n                     cv=10, \n                     scoring=\"neg_mean_squared_error\")\n\ngsKNR.fit(X, y)\nKNR_best = gsKNR.best_estimator_\n\n# Best score\nprint('KNR')\nprint('Best score : ', gsKNR.best_score_)\nprint('Best params : ', gsKNR.best_params_)","3f51423a":"# GradientBoostingRegressor\nmodel = GradientBoostingRegressor(random_state = random_state)\n\n# Search grid for optimal parameters\nex_param_grid = {\n    'learning_rate': [0.25, 0.1, 0.05, 0.01],\n    'n_estimators': [50,100,200,300],\n    'max_depth': [3,5,7]\n}\n\ngsGBR = GridSearchCV(model, \n                     param_grid = ex_param_grid, \n                     cv=10, \n                     scoring=\"neg_mean_squared_error\")\n\ngsGBR.fit(X, y)\nGBR_best = gsGBR.best_estimator_\n\n# Best score\nprint('GBR')\nprint('Best score : ', gsGBR.best_score_)\nprint('Best params : ', gsGBR.best_params_)","f7a1e788":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt","f4b546bd":"g = plot_learning_curve(SVR_best,\"SVR\",X, y,cv=10)\ng = plot_learning_curve(ABR_best,\"AdaBoost\",X, y,cv=10)\ng = plot_learning_curve(RFR_best,\"RandomForest\",X, y,cv=10)\ng = plot_learning_curve(KNR_best,\"KNeighbors\",X, y,cv=10)\ng = plot_learning_curve(GBR_best,\"GradientBoosting\",X, y,cv=10)","5b1369c6":"## Plot Feature Importance of Random Forest\n\nplt.figure(figsize=(10,10))\nnames_classifiers = [(\"RFR\",RFR_best)]\nnclassifier = 0\nname = names_classifiers[nclassifier][0]\nclassifier = names_classifiers[nclassifier][1]\nindices = np.argsort(classifier.feature_importances_)[::-1][:40]\n\ntrain_dummies = pd.get_dummies(train_final.drop(['revenue_log'], axis=1))\n\ng = sns.barplot(y=train_dummies.columns[indices][:40],x = classifier.feature_importances_[indices][:40] , orient='h')\ng.set_xlabel(\"Relative importance\",fontsize=12)\ng.set_ylabel(\"Features\",fontsize=12)\ng.tick_params(labelsize=9)\ng.set_title(name + \" feature importance\")   ","d551b844":"## Plot Feature Importance of Gradient Boosting\n\nplt.figure(figsize=(10,10))\nnames_classifiers = [(\"GBR\",GBR_best)]\nnclassifier = 0\nname = names_classifiers[nclassifier][0]\nclassifier = names_classifiers[nclassifier][1]\nindices = np.argsort(classifier.feature_importances_)[::-1][:40]\n\ntrain_dummies = pd.get_dummies(train_final.drop(['revenue_log'], axis=1))\n\ng = sns.barplot(y=train_dummies.columns[indices][:40],x = classifier.feature_importances_[indices][:40] , orient='h')\ng.set_xlabel(\"Relative importance\",fontsize=12)\ng.set_ylabel(\"Features\",fontsize=12)\ng.tick_params(labelsize=9)\ng.set_title(name + \" feature importance\") ","362bbf9a":"data_test.head()","7a9b0696":"attr_select = ['Open Date', 'City', 'City Group', 'Type', 'P2', 'P6', 'P13', 'P28', 'P29']\ntest_select = data_test[attr_select]","3c4a1778":"# Transform Open Year\nopen_years = []\nfor i in test_select['Open Date']:\n    open_years.append(count_years(i))\n    \ndf_open_years = pd.DataFrame({ 'open_years' : open_years } )\ngroup_years = df_open_years['open_years'].value_counts()\n\n# Transform City\ncity_most = train_select['City'].value_counts()[train_select['City'].value_counts() > 2].index\ncity_transform = []\n\nfor i in test_select['City']:\n    if i in city_most:\n        city_transform.append(i)\n    else:\n        city_transform.append('other')\n        \ndf_city_transform = pd.DataFrame({ 'city_transform' : city_transform } )\ngroup_city = df_city_transform['city_transform'].value_counts()\n\ntest_final = pd.concat([test_select, df_open_years, df_city_transform], axis=1).drop(['Open Date', 'City'], axis=1)","eb3d5fe7":"X_test = pd.get_dummies(test_final).drop(['Type_MB'], axis=1)\ntest = pd.get_dummies(X_test)","6a184f4c":"test_type_SVR = pd.Series(SVR_best.predict(test), name=\"SVR\")\ntest_type_ABR = pd.Series(ABR_best.predict(test), name=\"ABR\")\ntest_type_RFR = pd.Series(RFR_best.predict(test), name=\"RFR\")\ntest_type_KNR = pd.Series(KNR_best.predict(test), name=\"KNR\")\ntest_type_GBR = pd.Series(GBR_best.predict(test), name=\"GBR\")\n\n\n# Concatenate all classifier results\nensemble_results = pd.concat([test_type_SVR, test_type_ABR, test_type_RFR, test_type_KNR, test_type_GBR],axis=1)\n\ng= sns.heatmap(ensemble_results.corr(),annot=True)","57eb51cd":"# Using Voting Regressor\nfrom sklearn.ensemble import VotingRegressor\n\nvotingR = VotingRegressor(estimators=[('svr', SVR_best), ('abr', ABR_best),\n('gbr', GBR_best), ('rfr', RFR_best), ('knr', KNR_best)], n_jobs=4)\n\nvotingR = votingR.fit(X, y)","42eddfbb":"predict_test = pd.Series(np.exp(votingR.predict(test)), name=\"Prediction\")\nresults = pd.concat([IDtest, predict_test],axis=1)\nresults.to_csv(\"my_prediction.csv\",index=False)","26cec985":"# Restaurant Revenue Prediction","33e0999a":"# 6. Modelling","dbef0be0":"### 5.2 Feature Engineering","8869834a":"### 6.1 Cross Validate Model\n","b826d834":"### 5.1 Feature Selection\n","e1fee355":"**Notes: best model SVR, AdaBoostRegressor (ABR), RandomForestRegressor (RFR), KNeighborsRegressor(KNR), GradientBoosting Regressor (GBR)**","d3ffc1e3":"### 6.3 Learning Curves","abd182d8":"> All Notes for Data Cleaning and Preprocessing\n- Handle skew : Revenue could be log transformed\n- Feature Selection : Pick correlated features (P2, P28, P6, P13, P29)\n- Feature Engineering : Transform  Open Year -> value years from now\n- Feature Engineering : Transform city less than 2 to other\n- No missing value and outlier to be handled","f48f459e":"# 2. Load Data","c34ad0cb":"### 3.2 Exploring Numerical Data","592d68c3":"### 6.2 Hyperparameter Tuning","4163c41f":"### 3.1 Preliminary Observations","cd5e26d2":"**Notes: Pick correlated features (P2, P28, P6, P13, P29)**","1cfec8c6":"### 7.1 Preprocess Data Test","d2ed9759":"### 7.2 Predict","67149f2a":"> **Data fields**\n- **Id :** Restaurant id. \n- **Open Date :** opening date for a restaurant\n- **City :** City that the restaurant is in. Note that there are unicode in the names. \n- **City Group :** Type of the city. Big cities, or Other. \n- **Type :** Type of the restaurant. FC: Food Court, IL: Inline, DT: Drive Thru, MB: Mobile\n- **P1, P2 - P37 :** There are three categories of these obfuscated data. Demographic data are gathered from third party providers with GIS systems. These include population in any given area, age and gender distribution, development scales. Real estate data mainly relate to the m2 of the location, front facade of the location, car park availability. Commercial data mainly include the existence of points of interest including schools, banks, other QSR operators.\n- **Revenue :** The revenue column indicates a (transformed) revenue of the restaurant in a given year and is the target of predictive analysis. Please note that the values are transformed so they don't mean real dollar values. ","b0f80b38":"### 4.1 Handling Skew","068566f9":"# Outline\n1. Import Libraries\n2. Load Data\n3. Exploratory Data Analysis\n<br>&nbsp;3.1 Preliminary Observations\n<br>&nbsp;3.2 Exploring Numerical Data\n<br>&nbsp;3.3 Exploring Categorical Data\n4. Data Cleaning and Preprocessing\n<br>&nbsp;4.1 Handling Skew\n5. Feature Selection and Engineering\n<br>&nbsp;5.1 Feature Selection\n<br>&nbsp;5.2 Feature Engineering\n6. Modelling\n<br>&nbsp;6.1 Cross Validate Model\n<br>&nbsp;6.2 Hyperparameter Tuning\n<br>&nbsp;6.3 Learning Curves\n<br>&nbsp;6.4 Feature Importance\n7. Prediction\n<br>&nbsp;7.1 Preprocess Data Test\n<br>&nbsp;7.2 Predict","f1024f6b":"**Notes : Feature transform Open Year -> value years from now**","c20cd606":"### 6.4 Feature Importance","4344f643":"# 3. Exploratory Data Analysis","75b93021":"# 7. Prediction\n","1e4ce121":"# 1. Import Libraries","8e7f6048":"### 3.3 Exploring Categorical Data","e80596c9":"**Notes : Transform city less than 2 to other**","a485bbe5":"# 5. Feature Selection and Engineering","95d52688":"# 4. Data Cleaning and Preprocessing\n","25af8cca":"**Notes : Revenue could be log transformed**"}}