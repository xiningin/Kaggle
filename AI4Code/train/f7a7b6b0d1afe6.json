{"cell_type":{"8c46c736":"code","250ff43c":"code","59239d64":"code","782c136e":"code","79998563":"code","b80d0ffd":"code","1b3b1c05":"code","d12f9064":"code","f665c853":"code","f08953db":"code","b3999d8a":"code","8fa024e2":"code","261d0771":"code","f9693ae7":"code","a2694088":"code","c7c22d9b":"code","bf71c333":"code","9952c4c3":"code","c0408920":"code","4771e8f0":"code","5278f849":"code","4422e9b5":"code","4eacbe43":"code","333add09":"code","bfa02437":"code","6da53e6c":"code","761ba321":"code","8979818c":"code","25f59e7e":"code","5fb5a757":"code","7d2df757":"code","d6eab6fd":"code","4bea5e22":"code","0a2e3c2c":"code","dc0e85bb":"code","61528ae5":"code","81d3af34":"markdown","9dbaa1c1":"markdown","0547784e":"markdown","90d54a4a":"markdown","dc90f990":"markdown","b72e489d":"markdown","83a1bc27":"markdown","d086a7e6":"markdown","666cbd79":"markdown","829db802":"markdown","77d6b444":"markdown","fdfb40af":"markdown","ba14adf9":"markdown","40184c68":"markdown","8060b625":"markdown","b2f0fc29":"markdown","3b8447c7":"markdown","7f21a9f5":"markdown","e1d26013":"markdown","1c0ec15c":"markdown","76b84b9c":"markdown","2b0a6c7c":"markdown","5fb5eb03":"markdown","9f6f42a2":"markdown","5685d3a1":"markdown","2a6e528e":"markdown","32cd4822":"markdown","d5b674b6":"markdown","596d539e":"markdown","b8824c38":"markdown","9c8bd45e":"markdown"},"source":{"8c46c736":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # plotting\nimport seaborn as sns # plotting\nfrom sklearn.metrics import mean_squared_error # MSE metric\nfrom sklearn.model_selection import train_test_split\nfrom catboost import CatBoostClassifier\nfrom catboost import Pool\nimport shap as shap # feature importance plotting\nfrom imblearn.over_sampling import SMOTENC # for Imbalanced Classification\n# Metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve\n\npd.set_option('display.max_columns', 100)\n\nSEED = 91 # random seed","250ff43c":"PATH = '\/kaggle\/input\/30-days-of-ml\/' # you can use your own local path\n\nprint(f\"Files in directory {PATH.split('\/')[-2]}:\")\nfor _, _, filenames in os.walk(PATH):\n    for filename in filenames:\n        print('  '+os.path.join(filename))","59239d64":"try:\n    df_train = pd.read_csv(PATH+'train.csv', index_col=0)\n    df_test = pd.read_csv(PATH+'test.csv', index_col=0)\n    submission = pd.read_csv(PATH+'sample_submission.csv', index_col=0)\n    print('All data has been loaded successfully!')\nexcept Exception as err:\n    print(repr(err))","782c136e":"full_lenght_data = len(df_train) + len(df_test)\nprint(f\"train: {len(df_train)} ({100*len(df_train)\/full_lenght_data}%)\")\nprint(f\"test:  {len(df_test)} ({100*len(df_test)\/full_lenght_data}%)\")","79998563":"df_train.info()","b80d0ffd":"df_test.info()","1b3b1c05":"df_train.head()","d12f9064":"CAT_FEATURES = ['cat1', 'cat6', 'cat7', 'cat8', 'cat9']\nNUM_FEATURES = ['cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8',\n                'cont9', 'cont10', 'cont11', 'cont12', 'cont13']\nALL_FEATURES = CAT_FEATURES+NUM_FEATURES","f665c853":"df_full = pd.concat([df_test[CAT_FEATURES], df_train[CAT_FEATURES]]).sort_index()\ndf_full['test'] = 'train'\ndf_full.loc[df_test.index,'test'] = 'test'\ndf_full.head()","f08953db":"plt.figure(figsize=(20,8))\nplt.subplots_adjust(hspace=0.5, wspace=0.3)\n#sns.set_palette(\"Spectral\")\nfor i, col in enumerate(CAT_FEATURES):\n    plt.subplot(2, 5, i+1)\n    (df_full[col]\n     .groupby(df_full['test'])\n     .value_counts(normalize=True)\n     .rename('proportion %')\n     .reset_index()\n     .pipe((sns.barplot, \"data\"), x=col, y='proportion %', hue='test'))\n    plt.title(col)\nplt.show()","b3999d8a":"df_train[NUM_FEATURES].describe()","8fa024e2":"sns.pairplot(df_train[[\n    *NUM_FEATURES[:7],\n    'target'\n]])","261d0771":"sns.pairplot(df_train[[\n    *NUM_FEATURES[7:],\n    'target'\n]])","f9693ae7":"df_train['target'].describe()","a2694088":"f,ax=plt.subplots(3,1,figsize=(14,10))\n#plt.title('Target values', fontsize=16)\nsns.histplot(df_train['target'], ax=ax[0])\nsns.boxplot(x=df_train['target'], color='lightblue', saturation=0.8, ax=ax[1])\nax[1].axvline(np.percentile(df_train['target'],.1), label='.1%', c='orange', linestyle=':', linewidth=3)\nax[1].axvline(np.percentile(df_train['target'],.5), label='.5%', c='darkblue', linestyle=':', linewidth=3)\nax[1].legend()\nsns.ecdfplot(df_train['target'], ax=ax[2])\nax[0].set_xticks(np.arange(0,10.8,.5))\nax[1].set_xticks(np.arange(0,10.8,.5))\nax[2].set_xticks(np.arange(0,10.8,.5))\nplt.show()","c7c22d9b":"print(f\"Skew of target {df_train['target'].skew():.3f}\")","bf71c333":"fig, ax = plt.subplots()\nfig.set_size_inches(21, 5)\nsns.histplot(df_train[df_train['target'] > 6.9]['target'], bins=500)\nax.set_xticks(np.arange(6.9, max(df_train['target']), .05))\nplt.xticks(rotation=90, ha='right')\nplt.show()","9952c4c3":"peaks = [8.1184, 8.5241, 8.8411, 9.1089, 9.3723, 9.6729, 10.0079]\nstep = 0.005","c0408920":"df_train['target_anomaly'] = df_train['target'].apply(lambda x: 1 if (\n    [True for p in peaks if (x>(p-step) and x<(p+step))]\n)else 0)","4771e8f0":"df_train[['target', 'target_anomaly']].groupby('target_anomaly').agg(['count', 'mean'])","5278f849":"print(f\"Peaks contain {df_train['target_anomaly'].mean()*100:.2f}% of values\")","4422e9b5":"df_train[[*NUM_FEATURES, 'target_anomaly']].groupby('target_anomaly').agg('mean')","4eacbe43":"df_train[[*NUM_FEATURES, 'target_anomaly']].groupby('target_anomaly').agg('median')","333add09":"df_anomaly = df_train[df_train['target_anomaly'] == 1]","bfa02437":"sns.pairplot(df_anomaly[[\n    *NUM_FEATURES[:7],\n    'target'\n]])","6da53e6c":"sns.pairplot(df_anomaly[[\n    *NUM_FEATURES[7:],\n    'target'\n]])","761ba321":"corrMatrix = df_train.corr(method='pearson', min_periods=1)\nplt.figure(figsize=(15,10))\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corrMatrix, dtype=bool))\nax = sns.heatmap(corrMatrix, annot=True, mask=mask, cbar_kws={\"shrink\": .5}, cmap='coolwarm')\nplt.show()","8979818c":"X = df_train[ALL_FEATURES]\ny = df_train['target_anomaly']\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=SEED)\n\nprint(np.bincount(y_train.values))\noversample = SMOTENC(categorical_features=[i for i in range(len(CAT_FEATURES))], sampling_strategy=0.7, random_state=SEED)\nX_train, y_train = oversample.fit_resample(X_train, y_train)\nprint(np.bincount(y_train.values))","25f59e7e":"catboost_classifier_params = {\n    'class_weights': [1,1.5],\n    'cat_features': CAT_FEATURES,\n    'iterations': 12000,\n    'learning_rate': 0.03,\n    'early_stopping_rounds': 50,\n    'verbose': 2000,\n    'random_seed': SEED,\n}","5fb5a757":"model_classifier = CatBoostClassifier(**catboost_classifier_params)\nmodel_classifier.fit(X_train, y_train,\n          eval_set=(X_val, y_val))\n\npredicted = model_classifier.predict(X_val)","7d2df757":"print(f\"Accuracy: {accuracy_score(y_val, predicted)}\")\nprint(f\"Recall: {recall_score(y_val, predicted)}\")\nprint(f\"Precision: {precision_score(y_val, predicted)}\")\nprint(f\"ROC-AUC: {roc_auc_score(y_val, predicted)}\")\ndf_cm = pd.DataFrame(confusion_matrix(y_val, predicted), columns=np.unique(y_val), index = np.unique(y_val))\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\nsns.heatmap(df_cm, cmap=\"Blues\", annot=True, fmt='g')","d6eab6fd":"# Plot ROC curve\ndef plot_roc_curve(fpr=None, tpr=None):\n    \"\"\"Plot custom histogram\"\"\"\n    plt.figure(figsize=(5,5))\n    plt.title('ROC-curve', fontsize=16)\n    plt.xlabel('False Positive Rate', fontsize=14)\n    plt.ylabel('True Positive Rate', fontsize=14)\n    \n    plt.plot(fpr, tpr)\n\n    # ROC-curve of random model\n    plt.plot([0, 1], [0, 1], linestyle='--')\n    \n    plt.ylim([0.0, 1.0])\n    plt.xlim([0.0, 1.0])\n    plt.grid(True)\n    \n    plt.show()\n    \n\nprobabilities_valid = model_classifier.predict_proba(X_val)\nfpr, tpr, thresholds = roc_curve(y_val, probabilities_valid[:,1])\nplot_roc_curve(fpr, tpr)","4bea5e22":"feature_importance_df = pd.DataFrame(model_classifier.feature_importances_, index=X_train.columns)\nfeature_importance_df.sort_values(by=0, ascending=False)","0a2e3c2c":"train_data = Pool(data=X_train,\n                  label=y_train,\n                  cat_features=CAT_FEATURES\n                 )\n                 \nexplainer = shap.TreeExplainer(model_classifier)\nshap_values = explainer.shap_values(train_data)\nshap.summary_plot(shap_values, X_train, feature_names=ALL_FEATURES)","dc0e85bb":"fig, ax = plt.subplots()\nfig.set_size_inches(21, 5)\nsns.histplot(df_train['cont1'], bins=300)\nax.set_xticks(np.arange(min(df_train['cont1']), max(df_train['cont1']), .01))\nplt.xticks(rotation=90, ha='right')\nplt.show()","61528ae5":"fig, ax = plt.subplots()\nfig.set_size_inches(21, 5)\nsns.histplot(df_test['cont1'], bins=300)\nax.set_xticks(np.arange(min(df_train['cont1']), max(df_train['cont1']), .01))\nplt.xticks(rotation=90, ha='right')\nplt.show()","81d3af34":"## Correlation matrix","9dbaa1c1":"Let's look more closely at target without left tail","0547784e":"<img src=https:\/\/static.wikia.nocookie.net\/disney\/images\/a\/ad\/Profile_-_Sadness.png width=\"200\" height=\"200\" align=\"left\">  \n\n","90d54a4a":"What about pairwise relationships if target is in peaks","dc90f990":"### Continuous features","b72e489d":"**Bad result! ROC-AUC is near 0.5**  ","83a1bc27":"Target has some anomaly \"peaks\" and there is some pattern of target distribution","d086a7e6":"# 1. Load data and first look","666cbd79":"No missing values in dataset","829db802":"# 3. Train Model\n\nApply upsampling technique using SMOTE library.  \nGenerate minor class from 5% up to 70% of major class.\n\n_* SMOTE (Synthetic Minority Over-sampling Technique) is a type of over-sampling procedure that is used to correct the imbalances in the groups._","77d6b444":"Quantile 25% is 7.74 but min is 0.14. We have outliers at the bottom.\n","fdfb40af":"# 4. Model evaluate","ba14adf9":"## <center>30 Days of ML competition<\/center>\n### <center>Classify anomalies of target<\/center>\n\n<center><img src='https:\/\/storage.googleapis.com\/kaggle-media\/Images\/30_Days_ML_Hero.png' width='320' height=\"320\" ><\/center>\n\nThis is the final competition of [30 Days of ML program](https:\/\/www.kaggle.com\/thirty-days-of-ml).\n\n#### Objective:\nThe main idea is to divide target into 2 groups: anomaly values and normal values. I attempt to pull out something useful to boost regression model and use it later in regression model as feature. Feature engineering is important aspects in Kaggle competitions. \nThen build classification model that predict class in test data and we can use this as feature.\n\n#### What is classification?\nClassification is the process of identifying and grouping predetermined categories. In machine learning classification is also the process of predicting the class of given data points.\n\n#### Dataset:\nThe dataset is used for this competition is synthetic (and generated using a CTGAN), but based on a real dataset. The original dataset deals with predicting the amount of an insurance claim. \n* 'cat0' - 'cat9' categorical features\n* 'cont0' - 'cont13' continuous features\n* 'target' - continous target\n\nThanks [@FilterJoe](https:\/\/www.kaggle.com\/filterjoe\/30dtoml-eda-part-2-target-analysis?scriptVersionId=73461234) for great insights from EDA","40184c68":"Load data","8060b625":"#### Add new features as target at anomaly peak or not\n1 - anomaly  \n0 - normal","b2f0fc29":"There is some difference of continous features cont0, cont4, cont5, cont7, cont8, cont10, cont11, cont12 if target is in peaks or not","3b8447c7":"# 2. Exploratory Data Analysis (EDA)","7f21a9f5":"#### _If you find it useful please upvote_","e1d26013":"### Import libraries","1c0ec15c":"There's no significance linear correlation.","76b84b9c":"Let's look at values of continuous features if target if in anomalu peak or not","2b0a6c7c":"### Target","5fb5eb03":"#### Feature cont1 stands out among others, it's the most important feature","9f6f42a2":"# 5. Conclusion\nWe can't predict target class after dividing target into 2 anomaly or not.   \nBut maybe we can get something useful from cont1? Try it here:  \nhttps:\/\/www.kaggle.com\/sergeyzemskov\/xgb-catboost-xgb-lgb-add-cont1-cat","5685d3a1":"We observe some peaks and boundaries of target values distribution. Target has a litle right skewed distribution, but we have about some amount of otliers on the left tail (from 0.746555 to 6.9)","2a6e528e":"Additionally we use weights for classes as 1:1.5","32cd4822":"Use only meaningful features  \n(some features are deleted as useless because of feature importance of built regression model)  \nSee here: https:\/\/www.kaggle.com\/sergeyzemskov\/catboost-feature-importance-with-shap","d5b674b6":"Variables of categorical features on test and train data distributed very similar","596d539e":"What is special in 'cont1' feature?","b8824c38":"First join train and test data","9c8bd45e":"### Categorical features\n\nCompare distribution of features in train and test data"}}