{"cell_type":{"6793546d":"code","cbe56114":"code","b7128e8b":"code","17284e60":"code","285eb504":"code","f29622d9":"code","0f6e33f4":"code","95649f1e":"code","f5f133c7":"code","9bf095a9":"code","bd648ed1":"code","d9f631f1":"code","175e0673":"code","55097488":"code","3ae3fef3":"code","daf529e2":"code","f284e622":"code","632557af":"code","7936df9c":"code","4b04a587":"code","5b2005e2":"code","c1793406":"code","cdcb11c8":"code","2225c560":"code","14be3679":"code","38d4c59e":"code","e2768501":"code","ded58ad4":"code","c5690257":"code","c941cfe6":"code","f5036dfc":"code","549b0fe7":"code","9fb84c7e":"code","8e0642ba":"code","a017ca6f":"code","50b7773f":"code","1c37f2d8":"code","2569a164":"code","5ed881bd":"code","aeadda5b":"code","de10729b":"code","9b07fc9c":"code","63e500d4":"code","d620b08a":"code","2817bb51":"code","d7a5ac14":"code","f8dbe75f":"code","d91da669":"code","7ff420d0":"code","d8f88eb8":"code","a33e222d":"code","0ae337f2":"code","1c1e6831":"code","bb2c5e4e":"code","0080b1e0":"code","632aaa02":"code","baadf570":"code","26bc56de":"code","8dca8fa7":"code","5c11ade0":"code","298055a2":"code","5fd969c3":"code","f2bf96a9":"code","1d20623a":"code","fc1c4d73":"code","d552505c":"code","9a954abb":"code","b72fcb1c":"code","bde4bfa4":"code","49e9ee31":"code","c8a87fe0":"code","5e98e6ed":"code","a7d43a1e":"code","a659562c":"code","14518288":"code","c8902586":"code","b4e6fd15":"code","859bc5ee":"markdown","abaec63b":"markdown","1dae1b92":"markdown","897aba0b":"markdown","76368fc1":"markdown","7b0909fd":"markdown","a18d3c79":"markdown","54edaf73":"markdown","b1ae6e0b":"markdown","5e8f17d9":"markdown","4570e2f1":"markdown","03289ed7":"markdown","d604c3bd":"markdown","b929816d":"markdown","4bc3d686":"markdown","9232268a":"markdown","b2e94487":"markdown","069d3a28":"markdown","34594be6":"markdown","cd95f672":"markdown","6142d4dd":"markdown","fb44ac19":"markdown","90264faf":"markdown","e4c8b399":"markdown","6441e30d":"markdown","1f35ec41":"markdown","50a18d7d":"markdown","2aca33da":"markdown","bf5be590":"markdown","0e8f175a":"markdown","19517ac6":"markdown","15e0739b":"markdown","41da15ce":"markdown","43a121ff":"markdown","ca426815":"markdown","dd90c0cf":"markdown","a1e75071":"markdown","af9879e1":"markdown","38eef515":"markdown","e2f55c86":"markdown","ce172b23":"markdown","989ec8e5":"markdown","bcd8928d":"markdown","36c977d5":"markdown","109a1964":"markdown","fdc1e468":"markdown","31a50a3b":"markdown","ac49fbbc":"markdown","7ce7b007":"markdown","b503d3de":"markdown","d203f7e4":"markdown","459deb67":"markdown","4e7d2391":"markdown","dabb73f8":"markdown","c8aa7ee7":"markdown","227a7606":"markdown","d3f2c9f7":"markdown"},"source":{"6793546d":"import pandas as pd\nimport numpy as np\nimport sklearn as skl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy as sc\nfrom sklearn.metrics import roc_auc_score\nimport gc #importing garbage collector\nimport time\nfrom scipy import signal\n\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline  \n\nSEED = 42\n#Pandas - Displaying more rorws and columns\npd.set_option(\"display.max_rows\", 500)\npd.set_option(\"display.max_columns\", 500)","cbe56114":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","b7128e8b":"timesteps = 14\nstartDay = 0","17284e60":"#df_train = pd.read_csv('\/kaggle\/input\/m5-forecasting-uncertainty\/sales_train_validation.csv')\ndf_prices = pd.read_csv('\/kaggle\/input\/m5-forecasting-uncertainty\/sell_prices.csv')\ndf_days = pd.read_csv('\/kaggle\/input\/m5-forecasting-uncertainty\/calendar.csv')\n\n#df_train = reduce_mem_usage(df_train)\ndf_prices = reduce_mem_usage(df_prices)\ndf_days = reduce_mem_usage(df_days)","285eb504":"df_train = pd.read_csv('\/kaggle\/input\/m5-forecasting-uncertainty\/sales_train_evaluation.csv')\ndf_train = reduce_mem_usage(df_train)","f29622d9":"series_cols = df_train.columns[df_train.columns.str.contains(\"d_\")].values\nlevel_cols = df_train.columns[df_train.columns.str.contains(\"d_\")==False].values","0f6e33f4":"df_train.head(1)","95649f1e":"sns.set_palette(\"colorblind\")\n\nfig, ax = plt.subplots(5,1,figsize=(20,28))\ndf_train[series_cols].sum().plot(ax=ax[0])\nax[0].set_title(\"Top-Level-1: Summed product sales of all stores and states\")\nax[0].set_ylabel(\"Unit sales of all products\");\ndf_train.groupby(\"state_id\")[series_cols].sum().transpose().plot(ax=ax[1])\nax[1].set_title(\"Level-2: Summed product sales of all stores per state\");\nax[1].set_ylabel(\"Unit sales of all products\");\ndf_train.groupby(\"store_id\")[series_cols].sum().transpose().plot(ax=ax[2])\nax[2].set_title(\"Level-3: Summed product sales per store\")\nax[2].set_ylabel(\"Unit sales of all products\");\ndf_train.groupby(\"cat_id\")[series_cols].sum().transpose().plot(ax=ax[3])\nax[3].set_title(\"Level-4: Summed product sales per category\")\nax[3].set_ylabel(\"Unit sales of all products\");\ndf_train.groupby(\"dept_id\")[series_cols].sum().transpose().plot(ax=ax[4])\nax[4].set_title(\"Level-4: Summed product sales per product department\")\nax[4].set_ylabel(\"Unit sales of all products\");","f5f133c7":"submission_sample = pd.read_csv('\/kaggle\/input\/m5-forecasting-uncertainty\/sample_submission.csv')\nsubmission_sample.head(10)","9bf095a9":"# total number of series * number of quartiles * 2 (validation & evaluation)\n42840*9*2","bd648ed1":"submission_sample.shape","d9f631f1":"temp_series = df_train\nplt.figure(figsize=(12,8))\npeak_days = []\nx = np.count_nonzero(temp_series==0, axis=0)\npeaks, _ = sc.signal.find_peaks(x, height=np.quantile(x,0.75), threshold=max(x)\/25)\npeak_d = temp_series.columns[peaks]\npeak_days=peak_d\nplt.plot(x)\nplt.plot(peaks, x[peaks], \"x\", color='red')\n    \nplt.title('Number of Zero Sales per Day')\nplt.ylabel('Number of Zero Sales')\nplt.xlabel('Days')","175e0673":"peak_days","55097488":"df_days[df_days['d'].isin(peak_days)]","3ae3fef3":"peak_days_before=[]\npeak_days_after=[]\n\nfor i, days in enumerate(peak_days):\n    peak_days_before.append('d_'+str(np.int(peak_days[i][2:])-1))\n    peak_days_after.append('d_'+str(np.int(peak_days[i][2:])+1))","daf529e2":"df_train_no_outlier = df_train.copy().T[1:]\ndf_train_no_outlier.columns = df_train.T.iloc[0]\n\nfor x,y,z in zip(peak_days,peak_days_before,peak_days_after):\n        df_train_no_outlier[df_train_no_outlier.index==x] = np.reshape([pd.concat([df_train_no_outlier[df_train_no_outlier.index==y],df_train_no_outlier[df_train_no_outlier.index==z]],axis=0).mean()],(1,30490))\n\ndf_train_no_outlier = df_train_no_outlier.T.reset_index()","f284e622":"df_train_no_outlier = pd.concat([df_train_no_outlier[level_cols],df_train_no_outlier[series_cols].apply(pd.to_numeric,downcast='float')],axis=1)\ndf_train_no_outlier = reduce_mem_usage(df_train_no_outlier)","632557af":"df_train_no_outlier.info()","7936df9c":"temp_series = df_train_no_outlier\nplt.figure(figsize=(12,8))\nx = np.count_nonzero(temp_series==0, axis=0)\nplt.plot(x)\n    \nplt.title('Number of Zero Sales per Day')\nplt.ylabel('Number of Zero Sales')\nplt.xlabel('Days')\nplt.ylim(0,30000)","4b04a587":"del temp_series, peak_days_before, peak_days_after, peak_d, peak_days, peaks","5b2005e2":"del df_train","c1793406":"#df_train_no_outlier.to_csv(\"df_train_no_outlier.csv\", index=False)","cdcb11c8":"#df_train_no_outlier = pd.read_csv('df_train_no_outlier.csv')","2225c560":"df_train_no_outlier.head()","14be3679":"series_cols = df_train_no_outlier.columns[df_train_no_outlier.columns.str.contains(\"d_\")].values\nlevel_cols = df_train_no_outlier.columns[df_train_no_outlier.columns.str.contains(\"d_\")==False].values","38d4c59e":"Level1 = pd.DataFrame(df_train_no_outlier[series_cols].sum(),columns={'Total'}).T\nLevel2 = df_train_no_outlier.groupby(\"state_id\")[series_cols].sum()\nLevel3 = df_train_no_outlier.groupby(\"store_id\")[series_cols].sum()\nLevel4 = df_train_no_outlier.groupby(\"cat_id\")[series_cols].sum()\nLevel5 = df_train_no_outlier.groupby(\"dept_id\")[series_cols].sum()\n\nLevel6 = df_train_no_outlier.groupby([\"state_id\",'cat_id'])[series_cols].sum().reset_index()\nLevel6['index']=''\nfor row in range(len(Level6)):\n    Level6['index'][row]=str(Level6['state_id'][row])+'_'+str(Level6['cat_id'][row])\nLevel6.set_index(Level6['index'],inplace=True)\nLevel6.drop(['state_id','cat_id','index'],axis=1,inplace=True)\n\nLevel7 = df_train_no_outlier.groupby([\"state_id\",'dept_id'])[series_cols].sum().reset_index()\nLevel7['index']=''\nfor row in range(len(Level7)):\n    Level7['index'][row]=str(Level7['state_id'][row])+'_'+str(Level7['dept_id'][row])\nLevel7.set_index(Level7['index'],inplace=True)\nLevel7.drop(['state_id','dept_id','index'],axis=1,inplace=True)\n\nLevel8 = df_train_no_outlier.groupby([\"store_id\",'cat_id'])[series_cols].sum().reset_index()\nLevel8['index']=''\nfor row in range(len(Level8)):\n    Level8['index'][row]=str(Level8['store_id'][row])+'_'+str(Level8['cat_id'][row])\nLevel8.set_index(Level8['index'],inplace=True)\nLevel8.drop(['store_id','cat_id','index'],axis=1,inplace=True)\n\nLevel9 = df_train_no_outlier.groupby([\"store_id\",'dept_id'])[series_cols].sum().reset_index()\nLevel9['index']=''\nfor row in range(len(Level9)):\n    Level9['index'][row]=str(Level9['store_id'][row])+'_'+str(Level9['dept_id'][row])\nLevel9.set_index(Level9['index'],inplace=True)\nLevel9.drop(['store_id','dept_id','index'],axis=1,inplace=True)\n\nLevel10= df_train_no_outlier.groupby([\"item_id\"])[series_cols].sum()\n\n\nLevel11= df_train_no_outlier.groupby([\"item_id\",'state_id'])[series_cols].sum().reset_index()\nLevel11['index']=''\nfor row in range(len(Level11)):\n    Level11['index'][row]=str(Level11['item_id'][row])+'_'+str(Level11['state_id'][row])\nLevel11.set_index(Level11['index'],inplace=True)\nLevel11.drop(['item_id','state_id','index'],axis=1,inplace=True)\n\n\nLevel12= df_train_no_outlier.copy()\nLevel12.set_index(Level12['id'],inplace=True, drop =True)\nLevel12.drop(level_cols,axis=1,inplace=True)\n\ndf=pd.concat([Level1,Level2,Level3,Level4,Level5,Level6,Level7,Level8,Level9,Level10,Level11,Level12])\n\ndel Level1,Level2,Level3,Level4,Level5,Level6,Level7,Level8,Level9,Level10,Level11,Level12","e2768501":"df.shape","ded58ad4":"test = pd.concat([df.reset_index()['index'],submission_sample.reset_index().id[:42840]],axis=1)\ntest\ntest['index'].replace('_validation','',regex=True,inplace=True)","c5690257":"test['proof'] = ''\nfor row in range(len(test)):\n    if test['index'][row] in test['id'][row]:\n        test['proof'][row]=True\ntest[test['proof']==False]","c941cfe6":"del test","f5036dfc":"df_days[\"date\"] = pd.to_datetime(df_days['date'])\ndf_days.set_index('date', inplace=True)\n\ndf_days['is_event_day'] = [1 if x ==False else 0 for x in df_days['event_name_1'].isnull()] \ndf_days['is_event_day'] = df_days['is_event_day'].astype(np.int8)\n\nday_before_event = df_days[df_days['is_event_day']==1].index.shift(-1,freq='D')\ndf_days['is_event_day_before'] = 0\ndf_days['is_event_day_before'][df_days.index.isin(day_before_event)] = 1\ndf_days['is_event_day_before'] = df_days['is_event_day_before'].astype(np.int8)\n\ndel day_before_event\n\ndaysBeforeEventTest = df_days['is_event_day_before'][1913:1941]\ndaysBeforeEvent = df_days['is_event_day_before'][startDay:1913]\ndaysBeforeEvent.index = df_train_no_outlier.index[startDay:1913]\n\ndaysBeforeEventValid = df_days['is_event_day_before'][1941:]","549b0fe7":"#df_final = pd.concat([df.T.reset_index(drop=True), daysBeforeEvent.reset_index(drop=True)], axis = 1)\n#df_final = df_final[startDay:]","9fb84c7e":"df_days","8e0642ba":"from sklearn.preprocessing import OrdinalEncoder\n\ndef prep_calendar(df):\n    df = df.drop([ \"weekday\"], axis=1)\n    df = df.assign(d = df.d.str[2:].astype(int))\n    df = df.fillna(\"missing\")\n    cols = list(set(df.columns) - {\"wm_yr_wk\", \"d\"})\n    df[cols] = OrdinalEncoder(dtype=\"int\").fit_transform(df[cols])\n    df = reduce_mem_usage(df)\n    return df\n\ndf_days = prep_calendar(df_days)","a017ca6f":"df_days","50b7773f":"cat_cols = [#\"wm_yr_wk\",\n            \"wday\", \"month\", \"year\", \"event_name_1\", \n            \"event_type_1\", \"event_name_2\", \"event_type_2\",\n            \"is_event_day_before\", \"snap_CA\", \"snap_TX\", \"snap_WI\"]","1c37f2d8":"features = df_days[cat_cols]\n\n## For Testing\n#features_test = features.iloc[1913:1941,:]\n#features_train = features.iloc[startDay:1913,:]\n\n## For final prediction:\nfeatures_test = features.iloc[1941:,:]\nfeatures_train = features.iloc[startDay:1941]","2569a164":"## For Testing (Validation Dataset)\n#df_final = pd.concat([df.T.reset_index(drop=True).iloc[startDay:1913,:], features.iloc[startDay:1913,:].reset_index(drop=True)], axis = 1)\n\n## For final prediction (Evaluation Dataset):\ndf_final = pd.concat([df.T.reset_index(drop=True).iloc[startDay:1941,:], features.iloc[startDay:1941,:].reset_index(drop=True)], axis = 1)\n\n\nnumeric_cols = df_final[df_final.columns.difference(cat_cols)].columns","5ed881bd":"df_final","aeadda5b":"#### adding 'id' column as well as 'cat_id', 'dept_id' and 'state_id', then changing the type to 'categorical'\n#df_prices.loc[:, \"id\"] = df_prices.loc[:, \"item_id\"] + \"_\" + df_prices.loc[:, \"store_id\"] + \"_validation\"\n#df_prices['state_id'] = df_prices['store_id'].str.split('_',expand=True)[0]\n#df_prices = pd.concat([df_prices, df_prices[\"item_id\"].str.split(\"_\", expand=True)], axis=1)\n#df_prices = df_prices.rename(columns={0:\"cat_id\", 1:\"dept_id\"})\n#df_prices[[\"store_id\", \"item_id\", \"cat_id\", \"dept_id\", 'state_id']] = df_prices[[\"store_id\",\"item_id\", \"cat_id\", \"dept_id\", 'state_id']].astype(\"category\")\n#df_prices = df_prices.drop(columns=2)","de10729b":"#price_features = pd.DataFrame(df_prices.groupby(['wm_yr_wk','id'])['sell_price'].mean().reset_index())\n#price_features['sell_price'] = price_features['sell_price'].astype('float32')\n#price_features = price_features.pivot(index='id',columns='wm_yr_wk',values='sell_price').T\n#\n#print(price_features.shape)\n#price_features.head()","9b07fc9c":"#price_features = pd.DataFrame(df_prices.groupby(['wm_yr_wk','store_id','cat_id'])['sell_price'].mean().reset_index())\n#price_features['sell_price'] = price_features['sell_price'].astype('float32')\n#\n#price_features['store_cat'] = 0\n#\n#for row in range(len(price_features)):\n#     price_features['store_cat'][row]=str(price_features['store_id'][row])+'_'+str(price_features['cat_id'][row])\n#        \n#price_features= price_features.pivot(index='store_cat',columns='wm_yr_wk',values='sell_price').T\n#price_features.head()","63e500d4":"#features = pd.merge(features.reset_index(),price_features,how='left', left_on='wm_yr_wk', right_on='wm_yr_wk').set_index('date')\n#features.drop('wm_yr_wk', axis=1, inplace=True)\n#features.fillna(method = 'bfill', inplace=True)\n#features.head()","d620b08a":"#features_test = features.iloc[1913:1941,:]\n#features_train = features.iloc[startDay:1913,:]\n#df_final = pd.concat([df.T.reset_index(drop=True).iloc[startDay:1913,:], features_train.reset_index(drop=True)], axis = 1)\n\n#df_valid = df.T.reset_index(drop=True).iloc[1913:,:]\n#features_valid = features.iloc[1941:,:]\n\n#print(df_final.shape)\n#df_final.head()","2817bb51":"from sklearn.preprocessing import MinMaxScaler\nsc = MinMaxScaler(feature_range = (0, 1))\ndt_scaled = sc.fit_transform(df_final)","d7a5ac14":"gc.collect()","f8dbe75f":"#X_train = []\n#y_train = []\n#for i in range(timesteps, 1913 - startDay):\n#    X_train.append(dt_scaled[i-timesteps:i])\n#    y_train.append(dt_scaled[i][0:42840]) \n#    \n#X_train = np.array(X_train)\n#y_train = np.array(y_train)\n#print('Shape of X_train :'+str(X_train.shape))\n#print('Shape of y_train :'+str(y_train.shape))","d91da669":"#inputs = df_final[-timesteps:]\n#inputs = sc.transform(inputs)","7ff420d0":"X_train = []\ny_train = []\nX_train_categories= {}\n\n#X_train_prices = []\n\n\n# creating a dict for all categorical varialbes\nfor cats in cat_cols:\n    X_train_categories[cats] = []\n\n# Creating X-train, y_train and X_train_categories - here already for final prediction (1941)\nfor i in range(timesteps, 1941 - startDay):\n    X_train.append(dt_scaled[i-timesteps:i,0:42840])\n    y_train.append(dt_scaled[i][0:42840])\n    \n    for z, cats in enumerate(cat_cols):\n        X_train_categories[cats].append(dt_scaled[i-timesteps:i,42840+z])\n    \n#    X_train_prices.append(dt_scaled[i-timesteps:i,42851:])\n\n    \nX_train = np.array(X_train)\ny_train = np.array(y_train)\n\n#X_train_prices = np.array(X_train_prices)\n\nprint('Shape of X_train: '+str(X_train.shape))\nprint('Shape of y_train: '+str(y_train.shape))\nprint('Shape of all categorical features: '+str(np.array(X_train_categories[cats]).shape))\n\n#print('Shape of price feature: '+str(X_train_prices.shape))","d8f88eb8":"df_final","a33e222d":"inputs = {}\ninputs.update({'numerics': dt_scaled[-timesteps:,:42840]})\nfor cats in cat_cols:\n    inputs[cats] = []\n\nfor z, cats in enumerate(cat_cols):\n    inputs.update({cats: dt_scaled[-timesteps:,42840+z]})\n        \n#inputs.update({'prices':df_final.iloc[-timesteps:,-30490:].to_numpy()})","0ae337f2":"del df_train_no_outlier, df_prices, df_days, df, dt_scaled","1c1e6831":"gc.collect()","bb2c5e4e":"%who","0080b1e0":"import keras\ndef tilted_loss(q, y, f):\n    e = (y - f)\n    return keras.backend.mean(keras.backend.maximum(q * e, (q - 1) * e), \n                              axis=-1)","632aaa02":"QUANTILES = [0.005, 0.025, 0.165, 0.25, 0.5, 0.75, 0.835, 0.975, 0.995]","baadf570":"EPOCHS = 32 # going through the dataset 32 times\nBATCH_SIZE = 32 # with each training step the model sees 32 examples","26bc56de":"# Importing the Keras libraries and packages\nfrom keras.models import Model\nfrom keras.layers import Dense, LSTM, Dropout, Input, Embedding, Flatten, concatenate","8dca8fa7":"#from keras.utils import plot_model as keras_plot","5c11ade0":"#keras_plot(model, 'model.png', show_shapes=True)","298055a2":"#identifying uniques for each of the categorical features\nfeat_uniques = []\nfor feats in features.iloc[:,:len(cat_cols)]:\n    feat_uniques.append(df_final[feats].nunique())\n    \n#num_cats = features.shape[1] # number of categorical features\nn_steps = X_train.shape[1] # number of timesteps in each sample\nn_numerical_feats = len(numeric_cols) # number of numerical features in each sample\ncat_size = feat_uniques # number of categories in each categorical feature\ncat_embd_dim = list(np.ones(len(cat_cols), dtype=np.int8)) # embedding dimension for each categorical feature\n\nnumerical_input = Input(shape=(n_steps, n_numerical_feats), name='numeric_input')\n\ncat_inputs = []\nfor i,cat in enumerate(cat_cols):\n    cat_inputs.append(Input(shape=(n_steps,), name=cat + '_input'))\n\ncat_embedded = []\n#for i,cat in enumerate(cat_cols):\n#    embed = Flatten()(Embedding(cat_size[i], cat_embd_dim[i])(cat_inputs[i]))\n#    cat_embedded.append(embed)\n    \nfor i,cat in enumerate(cat_cols):\n    embed = Embedding(cat_size[i], cat_embd_dim[i])(cat_inputs[i])\n    cat_embedded.append(embed)\n\ncat_merged = concatenate(cat_embedded)\n\n#prices_input = Input(shape=(n_steps, len(price_features.columns)), name='prices_input')\n\n#merged = concatenate([numerical_input, cat_merged, prices_input])\n\nmerged = concatenate([numerical_input, cat_merged])\n\n# Adding the first layer\nlstm_out = LSTM(40, dropout=0.2, return_sequences=True, input_shape = (X_train.shape[1], X_train.shape[2]))(merged)\n# Adding a second layer\nlstm_out = LSTM(400, dropout=0.2, return_sequences=True)(lstm_out)\n# Adding a third layer\nlstm_out = LSTM(400, dropout=0.2)(lstm_out)\n\n# Adding the output layer\nlstm_out = Dense(units = y_train.shape[1])(lstm_out)\n\nmodel = Model([numerical_input] + cat_inputs, lstm_out)\n\n#model = Model([numerical_input] + cat_inputs + [prices_input], lstm_out)\n\nprint(model.summary())","5fd969c3":"def run_model(X_train, y_train, q):\n    model.compile(optimizer = 'adam',loss=lambda y, f: tilted_loss(q, y, f))\n    \n    # To follow at which quantile we are predicting right now  \n    print('Running the model for Quantil: '+str(q)+':')\n    \n    model.fit({'numeric_input': X_train,\n               'wday_input': np.array(X_train_categories['wday']),\n               'month_input': np.array(X_train_categories['month']),\n               'year_input': np.array(X_train_categories['year']),\n               'event_name_1_input': np.array(X_train_categories['event_name_1']),\n               'event_type_1_input': np.array(X_train_categories['event_type_1']),\n               'event_name_2_input': np.array(X_train_categories['event_name_2']),\n               'event_type_2_input': np.array(X_train_categories['event_type_2']),\n               'is_event_day_before_input': np.array(X_train_categories['is_event_day_before']),\n               'snap_CA_input': np.array(X_train_categories['snap_CA']),\n               'snap_TX_input': np.array(X_train_categories['snap_TX']),\n               'snap_WI_input': np.array(X_train_categories['snap_WI']),\n     #          'prices_input': X_train_prices\n              }, y_train, epochs = EPOCHS, batch_size = BATCH_SIZE, verbose=2)\n    \n    \n    X_test = inputs.copy()\n    prediction = []\n    for j in range(timesteps,timesteps + 28):\n        predicted_volume = model.predict({'numeric_input': X_test['numerics'][j-timesteps:j].reshape(1, timesteps, 42840),\n               'wday_input': X_test['wday'][j-timesteps:j].reshape(1,timesteps,),\n               'month_input': X_test['month'][j-timesteps:j].reshape(1,timesteps),\n               'year_input': X_test['year'][j-timesteps:j].reshape(1,timesteps,),\n               'event_name_1_input': X_test['event_name_1'][j-timesteps:j].reshape(1,timesteps,),\n               'event_type_1_input': X_test['event_type_1'][j-timesteps:j].reshape(1,timesteps,),\n               'event_name_2_input': X_test['event_name_2'][j-timesteps:j].reshape(1,timesteps,),\n               'event_type_2_input': X_test['event_type_2'][j-timesteps:j].reshape(1,timesteps,),\n               'is_event_day_before_input': X_test['is_event_day_before'][j-timesteps:j].reshape(1,timesteps,),\n               'snap_CA_input': X_test['snap_CA'][j-timesteps:j].reshape(1,timesteps,),\n               'snap_TX_input': X_test['snap_TX'][j-timesteps:j].reshape(1,timesteps,),\n               'snap_WI_input': X_test['snap_WI'][j-timesteps:j].reshape(1,timesteps,)})\n        testInput = np.column_stack((np.array(predicted_volume), np.array(features_test.iloc[j-timesteps,:]).reshape(1,11)))\n        numerics = np.append(X_test['numerics'],predicted_volume).reshape(j + 1,42840)\n        X_test.update({'numerics': numerics})\n        for i, key in enumerate(list(inputs.keys())[1:]):\n            feat=[]\n            feat = np.append(X_test[key],testInput[:,42840+i])\n            X_test.update({key: feat})\n        predicted_volume = sc.inverse_transform(testInput)[:,0:42840] #without features\n        prediction.append(predicted_volume)\n    \n    prediction = pd.DataFrame(data=np.array(prediction).reshape(28,42840)).T\n    return prediction","f2bf96a9":"#from keras.models import Sequential\n#def run_model(X_train, y_train, q):\n#\n#    model = Sequential()\n#\n#    # Adding the first LSTM layer and some Dropout regularisation\n#    layer_1_units=40\n#    model.add(LSTM(units = layer_1_units, return_sequences = True, input_shape = (X_train.shape[1], X_train.shape[2])))\n#    model.add(Dropout(0.2))\n#\n#    # Adding a second LSTM layer and some Dropout regularisation\n#    layer_2_units=400\n#    model.add(LSTM(units = layer_2_units, return_sequences = True))\n#    model.add(Dropout(0.2))\n#\n#    # Adding a third LSTM layer and some Dropout regularisation\n#    layer_3_units=400\n#    model.add(LSTM(units = layer_3_units))\n#    model.add(Dropout(0.2))\n#\n#    # Adding the output layer\n#    model.add(Dense(units = y_train.shape[1]))\n#\n#    # Compiling the RNN\n#    model.compile(optimizer = 'adam',loss=lambda y, f: tilted_loss(q, y, f))\n#    \n#    # To follow at which quantile we are predicting right now  \n#    print('Running the model for Quantil: '+str(q)+':')\n#\n#    # Fitting the RNN to the Training set\n#    fit = model.fit(X_train, y_train, epochs = EPOCHS, batch_size = BATCH_SIZE, verbose=2)\n#    \n#    X_test = []\n#    X_test.append(inputs[0:timesteps])\n#    X_test = np.array(X_test)\n#    prediction = []\n#     \n#    for j in range(timesteps,timesteps + 28):\n#        predicted_volume = model.predict(X_test[0,j - timesteps:j].reshape(1, timesteps, 42841)) #incl. features\n#        testInput = np.column_stack((np.array(predicted_volume), np.array(features_test.iloc[j-timesteps,:]).reshape(1,1))) #here no of features is 5\n#        X_test = np.append(X_test, testInput).reshape(1,j + 1,42841) #incl. features\n#        predicted_volume = sc.inverse_transform(testInput)[:,0:42840] #without features\n#        prediction.append(predicted_volume)\n#    \n#    prediction = pd.DataFrame(data=np.array(prediction).reshape(28,42840)).T\n#    \n#    return prediction","1d20623a":"# We run the model for all the quantiles mentioned above. \n# Combining all quantile predictions one after another to a large dataset.\npredictions = pd.concat(\n    [run_model(X_train, y_train, q) \n     for q in QUANTILES]) ","fc1c4d73":"gc.collect()","d552505c":"predictions.shape","9a954abb":"predictions","b72fcb1c":"predictions.shape[0]*2","bde4bfa4":"predictions.to_pickle('Uncertainty_Predictions.pkl')","49e9ee31":"#copying the predictions *2: first is for prediction, second for evaluation (in the final part we only need evaluation)\nsubmission = pd.concat((predictions, predictions), ignore_index=True)\n\nidColumn = submission_sample[[\"id\"]]    \nsubmission[[\"id\"]] = idColumn  \n\n#re-arranging collumns\ncols = list(submission.columns)\ncols = cols[-1:] + cols[:-1]\nsubmission = submission[cols]\n#\ncolsname = [\"id\"] + [f\"F{i}\" for i in range (1,29)]\nsubmission.columns = colsname\n\nsubmission.to_csv(\"submission_evaluation_feat_embedd_final.csv\", index=False)","c8a87fe0":"submission","5e98e6ed":"temp_series = predictions\n#temp_series = submission[171360:171360+42840]\n\nborder = [1,3,10,3,7,9,21,30,70,3049,9147,30490]\nsumi = 0\nlevels =[]\nfor i in border:\n    sumi += i\n    levels.append(pd.DataFrame(temp_series[sumi-i:sumi]))\n\n\nfor i,level in enumerate(levels):\n    levels[i] = levels[i].sum()","a7d43a1e":"test =  pd.DataFrame()\nfor i, level in enumerate(levels):\n    test = pd.concat([test,levels[i]],axis=1)","a659562c":"test","14518288":"plt.figure(figsize=(20, 8))\n\nfor i in range(12):\n    plt.plot(levels[i][1:],label='level'+str(i+1))\n\nplt.legend()","c8902586":"fig,ax = plt.subplots(figsize=(20, 8))\n\nfor i in range(11):\n    ax = ax.twinx()\n    ax.plot(levels[i][1:])\n    plt.yticks([])","b4e6fd15":"plt.figure(figsize=(12, 4))\n\nINDEX=[]\nfor i in range(28,28+28):\n    INDEX.append(i)\nlevels[0].index = INDEX\n\ntemp_series = df_final.iloc[1913:1941,:]\n\nplt.plot(temp_series.reset_index(drop=True).iloc[:,0], label = 'real')\nplt.plot(levels[0][1:], label = 'prediction')\nplt.xlabel(\"Days\")\nplt.ylabel(\"Sum of sold items\")\nplt.title(\"Total Item Sold Within Prediction Period of d1914 to d1941\")\nplt.legend()\ndel temp_series\n","859bc5ee":"This looks more like a constant level. \n\nHowever, I have not figured out why the prediction is not following the pattern. ","abaec63b":"## Loss Function <a class=\"anchor\" id=\"lossfct\"><\/a>","1dae1b92":"## Predicting Future Sales <a class=\"anchor\" id=\"Predicting\"><\/a>","897aba0b":"![grafik.png](attachment:grafik.png)","76368fc1":"### Option B: Without embedding features  <a class=\"anchor\" id=\"no_feat_embed\"><\/a>","7b0909fd":"## Generating Train and Test Data <a class=\"anchor\" id=\"traintest\"><\/a>","a18d3c79":"### Option A: With Feature Embeddings  <a class=\"anchor\" id=\"feat_embed\"><\/a>","54edaf73":"### Insights\n\n* It has become much clearer how these levels are aggregated by performing groupby- and summing up the sales.\n* We can already observe nice periodic patterns. ","b1ae6e0b":"Now, we need to test, whether the combination of the levels is right and the rows contain the same input. We are going to do this by comparing the row names.","5e8f17d9":"#### Creating test data","4570e2f1":"## Prediction intervals and quantiles <a class=\"anchor\" id=\"PIs\"><\/a>\n\nGiven that forecasters will be asked to provide the median, and the 50%, 67%, 95%, and 99% PIs, u is set to u1=0.005, u2=0.025, u3=0.165, u4=0.25, u5=0.5, u6=0.75, u7=0.835, u8=0.975, and u9=0.995, therefore leading to the following quartiles:\n\n* 99% PI - $u_{1} = 0.005$ and $u_{9} = 0.995$\n* 95% PI - $u_{2} = 0.025$ and $u_{8} = 0.975$\n* 67% PI - $u_{3} = 0.165$ and $u_{7} = 0.835$\n* 50% PI - $u_{4} = 0.25$ and $u_{6} = 0.75$\n* median - $u_{5} = 0.5$","03289ed7":"#### Creating test data","d604c3bd":"Let's start with taking a look at possible features in the df_days dataset.","b929816d":"## Running the Model <a class=\"anchor\" id=\"runmodel\"><\/a>","4bc3d686":"* In the first submission row we are asked to make precitions for the top level 1 (unit sales of all products, aggregated for all stores\/states)\n* The next 3 rows represent level 2.\n* Followed by level 3 and so on\n* Some rows contain aggregations at different levels. An X indicates the absence of an second aggregration level.\n* The prediction interval can be validation (related to the public leaderboard) or evaluation (related to the private leaderboard).\n\nAn overview of the different levels is given in the Competitors Guide as follows:","9232268a":"At almost every outlier day, there is a special vent like thanksgiving or christmas.","b2e94487":"In the next step, let's create X_train and y_train by creating different dataframes with 14 days of projection. For y_train we only use sales values for predictions. As we only predict sales, only 0:42840 columns are choosen.","069d3a28":"# M5 Competition - Uncertainty - LSTM Neural Network\n\n1. [Introduction and sources](#sources)\n\n\n2. [Preparing to start](#prepare)\n    * [Loading packages](#packages)\n    * [Loading data](#data)\n    * [Looking at the hierachy](#hierarchy_ts)\n    \n    \n3. [The submission format](#submission)\n    * [Intro](#intro)\n    * [Prediction intervals and quartiles](#PIs)\n    * [Outlier](#outlier)\n    * [Aggregation levels](#sub_aggregation_levels)\n    \n    \n4. [Model Preparation](#Feature_Creation)\n    * [Variant 1: Limited Features](#limfeat)\n    * [Variant 2: More Features](#morefeat)\n    * [Variant 3: Including Pricing Feature](#pricefeat)\n    * [Feature Scaling](#featscale)\n    * [Train and Test Data Creation](#traintest)\n    \n    \n5. [LSTM Modeling](#Modeling)\n    * [Loss Function](#lossfct)\n    * [Running the Model](#runmodel)\n    * [Option A: With Feature Embeddings](#feat_embed)\n    * [Option B: Without Feature Embedding](#no_feat_embed)\n    * [Predicting Future Sales](#Predicting)\n    * [Creating the submission file](#submission)\n\nThe M5 competition ran from 2 March to 30 June 2020. Basis of the competition is to predict sales forecasts for walm,art stores. For that, we use hierarchical sales data, generously made available by Walmart, starting at the item level and aggregating to that of departments, product categories, stores in three geographical areas of the US: California, Texas, and Wisconsin.\n\nEach row contains an id that is a concatenation of an item_id and a store_id, which is either validation (corresponding to the Public leaderboard), or evaluation (corresponding to the Private leaderboard). \n\nWe are predicting 28 forecast days (F1-F28) of items sold for each row. For the **validation rows**, this corresponds to d_1914 - d_1941, and for the **evaluation rows**, this corresponds to d_1942 - d_1969. (Note: a month before the competition close, the ground truth for the validation rows will be provided.)\n\nDetailed Information can be found [at the university website or](https:\/\/mofc.unic.ac.cy\/m5-competition\/) and [the competition website on kaggle](https:\/\/www.kaggle.com\/c\/m5-forecasting-accuracy\/data).\n\nAn overview of the data given, can be seen here:\n\nData exists in three files:\n1. File 1: \u201ccalendar.csv\u201d: Contains information about the dates the products are sold.\n    * date: The date in a \u201cy-m-d\u201d format.\n    * wm_yr_wk: The id of the week the date belongs to.\n    * weekday: The type of the day (Saturday, Sunday, \u2026, Friday).\n    * wday: The id of the weekday, starting from Saturday.\n    * month: The month of the date.\n    * year: The year of the date.\n    * event_name_1: If the date includes an event, the name of this event.\n    * event_type_1: If the date includes an event, the type of this event.\n    * event_name_2: If the date includes a second event, the name of this event.\n    * event_type_2: If the date includes a second event, the type of this event.\n    * snap_CA, snap_TX, and snap_WI: A binary variable (0 or 1) indicating whether the stores of CA, TX or WI allow SNAP  purchases on the examined date. 1 indicates that SNAP purchases are allowed.\n\n\n2. File 2: \u201csell_prices.csv\u201d: Contains information about the price of the products sold per store and date.\n    * store_id: The id of the store where the product is sold. \n    * item_id: The id of the product.\n    * wm_yr_wk: The id of the week.\n    * sell_price: The price of the product for the given week\/store. The price is provided per week (average across seven days). If not available, this means that the product was not sold during the examined week. Note that although prices are constant at weekly basis, they may change through time (both training and test set).  \n\n\n3. File 3: \u201csales_train.csv\u201d: Contains the historical daily unit sales data per product and store.\n    * item_id: The id of the product.\n    * dept_id: The id of the department the product belongs to.\n    * cat_id: The id of the category the product belongs to.\n    * store_id: The id of the store where the product is sold.\n    * state_id: The State where the store is located.\n    * d_1, d_2, \u2026, d_i, \u2026 d_1941: The number of units sold at day i, starting from 2011-01-29. \n","34594be6":"## Aggregation levels <a class=\"anchor\" id=\"sub_aggregation_levels\"><\/a>","cd95f672":"## Looking at the hierachy <a class=\"anchor\" id=\"hierarchy_ts\"><\/a>\n\nIn the competition guideline we can find that the hierarchy consits of 12 levels. Let's try to reconstruct some of them:\n\n1. The top is given by the unit sales of all products, aggregated for all stores\/states. \n2. Unit sales of all products, aggregated for each state.\n3. Unit sales of all products, aggregated for each store.\n4. Unit sales of all products, aggregated for each category.\n5. Unit sales of all products, aggregated for each department.  \n...","6142d4dd":"# LSTM Modeling <a class=\"anchor\" id=\"Modeling\"><\/a>\n\nNext, we start our modelling. We will use LSTM Neural Networks with different layers. \n\nIn general, neural networks are easily described by the following picture:\n1. The neural network model starts with random weights and tries to find the best weights for the different layers, predicting outcomes and comparing them with the true target outcomes. For this it uses the loss function. \n2. The loss function measures the quality of  the network\u2019s output\n3. Then, the loss score is used as a feedback signal to adjust the weights.","fb44ac19":"Every combination is fine and in the right order.","90264faf":"### Option A: Include all prices","e4c8b399":"## Variant 3: Including Pricing Feature <a class=\"anchor\" id=\"pricefeat\"><\/a>","6441e30d":"# Preparing to start <a class=\"anchor\" id=\"prepare\"><\/a>\n\n## Loading packages <a class=\"anchor\" id=\"packages\"><\/a>","1f35ec41":"## Outlier <a class=\"anchor\" id=\"outlier\"><\/a>\n\nAt certain days, sales dropped significantly (e.g. christmas).\n\nHere, we take a look at peak days (i.e. peaks in terms of zero sales) on an overall levelm:","50a18d7d":"## Feature Scaling <a class=\"anchor\" id=\"featscale\"><\/a>\nFor better modeling, we are scaling features using min-max scaler in range 0-1.","2aca33da":"![grafik.png](attachment:grafik.png)","bf5be590":"Let's see how our predictions compare to the real values on Level1:","0e8f175a":"### Option B: Only include a couple of (aggregated) prices","19517ac6":"Level 12, 11 and 10 (the most detailled ones) have the lowest total sums. Level12 is approx 2\/3 the sum of level 1","15e0739b":"## Variant 2: More Features <a class=\"anchor\" id=\"morefeat\"><\/a>\nNext, we want to increase our number of features a little.","41da15ce":"Let's take a look if this worked","43a121ff":"| Level id|\tAggregation Level|\tNumber of series|\n|:----|:----|:----|\n|1|Unit sales of all products, aggregated for all stores\/states|\t1|\n|2|Unit sales of all products, aggregated for each State|\t3|\n|3|Unit sales of all products, aggregated for each store| \t10|\n|4|Unit sales of all products, aggregated for each category|\t3|\n|5|Unit sales of all products, aggregated for each department|\t7|\n|6|Unit sales of all products, aggregated for each State and category|\t9|\n|7|Unit sales of all products, aggregated for each State and department|\t21|\n|8|Unit sales of all products, aggregated for each store and category|\t30|\n|9|Unit sales of all products, aggregated for each store and department|\t70|\n|10|Unit sales of product x, aggregated for all stores\/states|\t3,049|\n|11|Unit sales of product x, aggregated for each State|\t9,147|\n|12|Unit sales of product x, aggregated for each store|\t30,490|\n| |**Total**|**42,840**|","ca426815":"Let's plot them at different levels to see if the curves move similarly.","dd90c0cf":"Finally, let's create a submission file, using the ids of the sample submission. As we have the validation and evaluation data, we need to stack the submission file on top of itself.","a1e75071":"Let's take a look at one of the predicted datasets (here we take the median with quantile = 0.5)\n\nWe want to test, whether the sum of our daily predictions on the different levels equal to level1 (Total levels).","af9879e1":"Let's take a look at it graphically:","38eef515":"Note for the start: *I was a bit shocked that here on kaggle only a few notebooks exist, daring to predict the M5 with uncertainty. Most of the notebooks simply translate the results of the accuracy competition.*\n\nI just recently started writing in Python and tried applying a Neural Network to the problem. I am just a beginner in this field, therefore please be patient :)\n\nAlso see my other Notebooks:\nM5 Uncertainty Prediction without Feature Embedding: https:\/\/www.kaggle.com\/olafko\/m5-uncertainty-prediction-lstm-nn\nM5 Accuracy Prediction simplest Model: https:\/\/www.kaggle.com\/olafko\/m5-forecasting-accuracy-simplest-model","e2f55c86":"Please feel free to share any ideas for improvement as a comment and we can discuss more in detail","ce172b23":"As seen in the table above, we are going to create the 12 Levels one after another through grouping statements ","989ec8e5":"### Option A: For the simple model:","bcd8928d":"# The submission format <a class=\"anchor\" id=\"submission\"><\/a>\n\n## Intro <a class=\"anchor\" id=\"intro\"><\/a>\n\n* We have 28 F-columns as we are predicting daily sales for the next 28 days. \n* We are asked to make uncertainty estimates for these days.","36c977d5":"When creating X_test, we are using the last 14 days in order to predict day 1915 sales. Therefore, in order to predict 1916th day, 13 days from our input data and 1 day from our prediction are used. After that we slide the window one by one, i.e.:\n\n* 12 days from input data + 2 days from our prediction to predict 1917th day\n* 11 days from input data + 3 days from our prediction to predict 1918th day\n* .....\n* 14 days our prediction to predict last 1941th day sales.\n\n","109a1964":"![grafik.png](attachment:grafik.png)","fdc1e468":"![grafik.png](attachment:grafik.png)","31a50a3b":"# Sources and guidelines <a class=\"anchor\" id=\"sources\"><\/a>\n\nAknowledgements\nAs a starting point I mainly used these notebooks:  \n* [baseline LSTM of Accuracy Prediction](https:\/\/www.kaggle.com\/bountyhunters\/baseline-lstm-with-keras-0-8#Future-Improvements)    \n* [Quantile regression, from linear models to trees to deep learning](https:\/\/towardsdatascience.com\/quantile-regression-from-linear-models-to-trees-to-deep-learning-af3738b527c3)\n* [Deep Quantiel Regression](https:\/\/towardsdatascience.com\/deep-quantile-regression-c85481548b5a)\n* [M5 Uncertainty Notebook by Allunia](https:\/\/www.kaggle.com\/allunia\/m5-uncertainty)\n\nMy other M5 notebook for Accuracy can be seen here: (will be uploaded soon)","ac49fbbc":"## Loading data <a class=\"anchor\" id=\"data\"><\/a>","7ce7b007":"What if we add prices for the products? Though we only have them on a weekly level, they could increase the model.\n\nIf we take prices per product per week, we have to take into account 3049 products * 10 stores leading to 30490 additional columns.\n\nOn the other side, if we group by store and category, we receive 10 (stores) * 3 (categories),therefore, only 30 additional columns.","b503d3de":"## Variant 1: Limited Features <a class=\"anchor\" id=\"limfeat\"><\/a>\n\nIn the first part we are taking only one extra feature (i.e. limited features).","d203f7e4":"# Model Preparation  <a class=\"anchor\" id=\"Feature_Creation\"><\/a>\n\nIn the next part we have to decide how many features we want to take for test and training datasets. ","459deb67":"We can see that our shape matches the requested outcome. Multiplying by two (for the validation and evaluation data).","4e7d2391":"At first, let's encode the categorical variables event name and event type into numbers.","dabb73f8":"## Combining Price features and other features","c8aa7ee7":"## Creating the Submission File <a class=\"anchor\" id=\"submission\"><\/a>","227a7606":"### Option B: For model with feature embedding","d3f2c9f7":"In the M5 we have to project different aggregation levels at certain quantiles. All that changes in comparison to the [baseline lstm](https:\/\/www.kaggle.com\/bountyhunters\/baseline-lstm-with-keras-0-7#Future-Improvements) is the loss function. The following few lines defines the loss function defined in the section above."}}