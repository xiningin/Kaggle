{"cell_type":{"e793fbd4":"code","f70fd525":"code","4771c089":"code","dd0672b4":"code","c4fa2e70":"code","57731920":"code","5481036d":"code","9c1b40ab":"code","cd4183a8":"code","6d2873f7":"code","58d33126":"code","61281c8d":"code","1bef488e":"code","ec29554b":"code","05f4bad7":"code","b7f91f58":"code","b838f8f6":"code","500029ce":"code","3008d2e5":"code","9cec9a62":"code","eab636ff":"code","2c461323":"code","0dc77de2":"code","0ac70e5d":"code","2e23e48c":"code","c1933ae7":"code","164af824":"markdown","ff5382c3":"markdown","b005010f":"markdown","b189ac3c":"markdown","0ae6eb03":"markdown","d2495ddb":"markdown","5a5cc70a":"markdown","62405183":"markdown","d36105e1":"markdown","c80fd558":"markdown","cc4cfbb0":"markdown","c230423a":"markdown","8d98decd":"markdown","1ca28616":"markdown","e9aa7d4b":"markdown","f26d09ac":"markdown","d12324e0":"markdown","f6777632":"markdown","cb723c08":"markdown","ddb9f698":"markdown","cfc63a15":"markdown","d9e84422":"markdown","c3eb1c66":"markdown"},"source":{"e793fbd4":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","f70fd525":"# It's already splitted as train and test data. So we had better import them train_data and test_data\n\ntrain_data = pd.read_csv(\"..\/input\/disease-prediction-using-machine-learning\/Training.csv\")\ntest_data = pd.read_csv(\"..\/input\/disease-prediction-using-machine-learning\/Testing.csv\")","4771c089":"train_data.head()","dd0672b4":"train_data.info() ","c4fa2e70":"train_data.isnull().any()  #there is an unclean column named \"Unnamed: 133\". We will drop it","57731920":"# we dont need Unnamed: 133 column to train\ntrain_data.drop([\"Unnamed: 133\"], axis = 1, inplace = True)","5481036d":"train_data","9c1b40ab":"# Label Encoding\n# from sklearn.preprocessing import LabelEncoder\n# labelencoder = LabelEncoder()\n\n# Lets assume we have categorical labels at first column (itching). If this column has True-False values or like Male-Female,\n# then it will tranform into 1-0 encoding. This is encoding. But as I say before, there is no need for encoding on this dataset\n\n# train_data.loc[:, 0] = labelencoder.fit_transform(train_data.iloc[:, 0])\n# train_data","cd4183a8":"# One hot encoding\n# from sklearn.preprocessing import OneHotEncoder\n\n# Lets assume our second column (skin_rash) has more than two labels. This time, we will have to one hot encode the feature\n# First, we need to apply label encoding similarly as we did in the itching variable\n# After applying label encoding, now it's time to appy One Hot Encoding\n\n# onehotencoder = OneHotEncoder(categorical_features = [1])\n# labelencoder2 = LabelEncoder()\n# train_data.loc[:,1] = labelencoder2.fit_transform(train_data.iloc[:, 1])\n# train_data = onehotencoder.fit_transform(train_data).toarray()","6d2873f7":"X_train = train_data.iloc[:, :-1]\nX_test = test_data.iloc[:, :-1]\ny_train = train_data.iloc[:, -1:]\ny_test = test_data.iloc[:, -1:]","58d33126":"print(\"X train shape: \",X_train.shape)\nprint(\"y train shape: \",y_train.shape)","61281c8d":"print(\"X test shape: \",X_test.shape)\nprint(\"y test shape: \",y_test.shape)","1bef488e":"# Example of standardization and normalization\n\nx = np.array([1,23,5,564,56,876,7,-123])\n\nstandardized_X = (x - np.mean(x)) \/ np.std(x)\nnormalized_X = (x-np.min(x) \/ np.max(x) - np.min(x))\nprint(\"Standardized array: \",standardized_X)\nprint(\"Normalized array: \",normalized_X )","ec29554b":"# With sklearn scalers\nfrom sklearn.preprocessing import StandardScaler\nstds = StandardScaler()\nx = x.reshape(-1,1)  # for sklearn methods. they use two dimensional vectors\nx = stds.fit_transform(x)","05f4bad7":"class NeuralNet(object):\n    def __init__(self):\n        # Generate random numbers\n        np.random.seed(1)\n        \n        # Assign random weights to a 3 * 1 matrix\n        self.synaptic_weights = 2 * np.random.random((3, 1)) - 1\n        \n    # The sigmoid function method\n    def _sigmoid(self,x):\n        return 1 \/ (1 + np.exp(-x))\n    \n    # Derivative sigmoid\n    def derivative_sigmoid(self, x):\n        return x * (1 - x)\n    \n    # Train the neural network and adjust the weights each time\n    def train(self, inputs, outputs, iteration_number):\n        for iteration in range(iteration_number):     \n        \n            # Pass the training set through network\n            output = self.learn(inputs)\n        \n            # Calculate the error\n            error = outputs - output\n        \n            # Adjust the weights by a factor\n            factor = np.dot(inputs.T, error * self.derivative_sigmoid(output))\n            self.synaptic_weights += factor\n        \n    # calculate z    \n    def learn(self, test_inputs):\n        return self._sigmoid(np.dot(test_inputs, self.synaptic_weights))","b7f91f58":"# Initialize\nneural_net = NeuralNet()\n\n# The training set\ninputs = np.array([[0, 1, 1], [1, 0, 0], [1, 0, 1]])\noutputs = np.array([[1, 0, 1]]).T\n\n# train the neural network\nneural_net.train(inputs, outputs, 50)\n\ntest_inputs = np.array([1, 0, 1])\nthreshold = 0.5\nif neural_net.learn(test_inputs) >= threshold:\n    print(\"Our test example output is: 1\")\nelse:\n    print(\"Our test example output is: 0\")","b838f8f6":"# transform into dummies for y_train (prognosis variable)\ny_train_dum = pd.get_dummies(y_train)\ny_train_dum","500029ce":"# import tensorflow and keras\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential   # used for initialize ANN model\nfrom tensorflow.keras import layers   # used for different layer structure\nfrom tensorflow.keras.layers import Dense","3008d2e5":"classifier = Sequential()","9cec9a62":"# adding first hidden layer with input layer. there is init parameter that represents how to initialize weights\nclassifier.add(Dense(64, activation = \"relu\", input_dim = X_train.shape[1]))\n# adding second hidden layer\nclassifier.add(Dense(32, activation = \"relu\"))\n# adding last layer\nclassifier.add(Dense(y_train_dum.shape[1], activation = \"softmax\"))","eab636ff":"classifier.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\nclassifier.summary()","2c461323":"history = classifier.fit(X_train, y_train_dum, epochs = 5, batch_size = 30)","0dc77de2":"prediction = classifier.predict_classes(X_test)\nprediction","0ac70e5d":"history.history[\"accuracy\"]","2e23e48c":"import matplotlib.pyplot as plt\nplt.plot(history.history[\"accuracy\"])\nplt.title(\"Model accuracy\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy score\")\nplt.show()","c1933ae7":"plt.plot(history.history[\"loss\"])\nplt.title(\"Model loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.show()","164af824":"<img src= \"https:\/\/media.geeksforgeeks.org\/wp-content\/uploads\/input_set.png\" alt =\"Basic ANN\">","ff5382c3":"<a id = \"1\"><\/a>\n### Data Preprocessing\n<a id = \"2\"><\/a>\n#### Import Libraries","b005010f":"<a id = \"15\"><\/a>\n#### Fitting the ANN","b189ac3c":"<a id = \"9\"><\/a>\n#### Fitting Model and Prediction","0ae6eb03":"<a id = \"7\"><\/a>\n### Model Building\n<a id = \"8\"><\/a>\n#### Implementing Basic Example ANN Structure with OOP","d2495ddb":"* Our x test data has 132 features and 42 observation unit that means our input matrix has 42 rows and 132 columns for prediction\n* Our y test data has one feature (itself) and 42 observation unit that means our output matrix has 42 rows and 1 column for prediction","5a5cc70a":"#### Content\n* [Data Preprocessing](#1)\n    * [Import Libraries](#2)\n    * [Import Dataset](#3)\n    * [Encoding Categorical Data](#4)\n    * [Create test and train data (and x-y variables)](#5)\n    * [Feature Scaling](#6)\n* [Model Building](#7)\n    * [Implementing Basic Example ANN Structure with OOP](#8)\n    * [Fitting Model and Prediction](#9)\n    * [Building ANN Structure with Keras Library](#10)\n        * [Import Libraries](#11)\n        * [Initialize ANN Model](#12)\n        * [Adding the Layers](#13)\n        * [Compiling the ANN](#14)\n        * [Fitting the ANN](#15)\n        * [Making Prediction with Test Data](#16)\n        * [Accuracy Score and Loss Visualization](#17)\n    ","62405183":"<a id = \"13\"><\/a>\n#### Adding the Layers","d36105e1":"<a id = \"10\"><\/a>\n### Building ANN Structure with Keras Library\n<a id = \"11\"><\/a>\n#### Import Libraries","c80fd558":"<img src= \"https:\/\/miro.medium.com\/max\/758\/1*wuCX1bjSh6YXcu8tuA5wyw.png\" alt =\"Standardization and Normalization\">","cc4cfbb0":"Data has 134 columns. The 132 of the columns are symptoms, encoded integer data, and the \"prognosis\" column is categorical data for disease labels.","c230423a":"The data is already encoded but anyhow I will show how to encode categorical data.","8d98decd":"<a id = \"3\"><\/a>\n#### Import Dataset","1ca28616":"Reference: https:\/\/www.geeksforgeeks.org\/implementing-ann-training-process-in-python\/","e9aa7d4b":"<a id = \"12\"><\/a>\n#### Initialize the ANN Model","f26d09ac":"* Our x train data has 132 features and 4920 observation unit that means our input matrix has 4920 rows and 132 columns for training\n* Our y train data has one feature (itself) and 4920 observation unit that means our output matrix has 4920 rows and 1 column for training","d12324e0":"<a id = \"17\"><\/a>\n#### Accuracy and Loss Visualization","f6777632":"What if we apply standard scaling to our train symptoms data (X_train)?\n* This is a nonsensical situation because our symptoms are not numerical. They are categorical. So do not these bullshit =)","cb723c08":"<a id = \"6\"><\/a>\n#### Feature Scaling for Numerical Data","ddb9f698":"<a id = \"4\"><\/a>\n#### Encoding Categorical Data","cfc63a15":"<a id = \"5\"><\/a>\n#### Create Test and Train Data","d9e84422":"<a id = \"16\"><\/a>\n#### Making Prediction","c3eb1c66":"<a id = \"14\"><\/a>\n#### Compiling the ANN Model"}}