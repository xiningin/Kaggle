{"cell_type":{"f8dc5498":"code","cd30c808":"code","f034b325":"code","4a7032df":"code","24da94eb":"code","22458457":"code","4082ea0b":"code","b1dc34d1":"code","b9acae9c":"code","14288623":"code","b8701668":"code","3940b29e":"code","fce7c5bd":"code","ac7e18c5":"code","83f0ea5f":"code","003cc173":"code","4f41c39a":"code","bd6c4aec":"code","3a6afe01":"code","d3d5ea37":"code","c0c9b8c2":"code","f42f546c":"code","9cef13bc":"code","69b3f76f":"code","708e6f76":"code","a04a2ec2":"code","af72fc37":"code","ea343d4d":"markdown","9e69741b":"markdown","e53fe780":"markdown","1525cb06":"markdown","42ce51ab":"markdown","2cc72187":"markdown","42f095ad":"markdown","9a7d7338":"markdown","e2859ec9":"markdown","97a75ad1":"markdown"},"source":{"f8dc5498":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cd30c808":"train_df = pd.read_csv('..\/input\/hackerearth-ml-slashing-prices-for-biggest-sale\/Train.csv')\ntest_df = pd.read_csv('..\/input\/hackerearth-ml-slashing-prices-for-biggest-sale\/Test.csv')\nsample = pd.read_csv('..\/input\/hackerearth-ml-slashing-prices-for-biggest-sale\/sample_submission.csv')","f034b325":"train_df.head()","4a7032df":"from scipy.stats import norm\nsns.distplot(train_df[\"Low_Cap_Price\"],fit=norm)\nmu,sigma= norm.fit(train_df['Low_Cap_Price'])\nprint(\"mu {}, sigma {}\".format(mu,sigma))","24da94eb":"# Removing skewness\n########## REMOVING SKEWEENESS ###########\ntrain_df['Low_Cap_Price']=np.log1p(train_df['Low_Cap_Price'])\nsns.distplot(train_df['Low_Cap_Price'],fit=norm)\nmu,sigma= norm.fit(train_df['Low_Cap_Price'])\nprint(\"mu {}, sigma {}\".format(mu,sigma))","22458457":"test_df['Low_Cap_Price'] = -1","4082ea0b":"df = pd.merge(train_df, test_df, how='outer')\ndf.head()","b1dc34d1":"df['Date'] = pd.to_datetime(df['Date'])","b9acae9c":"# df['month'] = df['Date'].dt.month\ndf['year'] = df['Date'].dt.year\n# df['dates'] = df['Date'].dt.day","14288623":"df['State_of_Country'].corr(df['Low_Cap_Price'])","b8701668":"numeric_col = ['Demand','High_Cap_Price']\nskew=df[numeric_col].skew()\nskew","3940b29e":"from scipy.special import boxcox1p\nlam=0.15\nfor i in skew.index:\n    df[i]=np.log1p(df[i])","fce7c5bd":"# One Hot\ndf = pd.get_dummies(df, columns=['State_of_Country','Market_Category','Product_Category','Grade','year'])\n# test_df = pd.get_dummies(test_df, columns=['State_of_Country','Market_Category','Product_Category','Grade'])","ac7e18c5":"\n\n#Normalize Demand and High_Cap_Price\ndf['Demand'] = (df['Demand'] - np.mean(df['Demand'])) \/ (np.max(df['Demand']) - np.min(df['Demand']))\n# test_df['Demand'] = (test_df['Demand'] - np.mean(test_df['Demand'])) \/ (np.max(test_df['Demand']) - np.min(test_df['Demand']))\n\ndf['High_Cap_Price'] = (df['High_Cap_Price'] - np.mean(df['High_Cap_Price'])) \/ (np.max(df['High_Cap_Price']) - np.min(df['High_Cap_Price']))\n# test_df['High_Cap_Price'] = (test_df['High_Cap_Price'] - np.mean(test_df['High_Cap_Price'])) \/ (np.max(test_df['High_Cap_Price']) - np.min(test_df['High_Cap_Price']))","83f0ea5f":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom xgboost import XGBClassifier, XGBRegressor\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import ElasticNet,BayesianRidge\nfrom sklearn.preprocessing import RobustScaler","003cc173":"features = [c for c in df.columns if c not in ['Item_Id','Date','Low_Cap_Price']]\nfeatures","4f41c39a":"train_df = df[df['Low_Cap_Price']!=-1]\ntest_df = df[df['Low_Cap_Price']==-1]","bd6c4aec":"print(len(train_df.columns))\nprint(len(test_df.columns))","3a6afe01":"X = train_df[features]\ny = train_df['Low_Cap_Price']\n\nx_test = test_df[features]","d3d5ea37":"xgb = XGBRegressor(base_score=0.5, booster=None, colsample_bylevel=1, \n                    importance_type='gain', interaction_constraints=None,\n                     min_child_weight=1, missing=None, monotone_constraints=None,\n                     n_estimators=700, n_jobs=-1, nthread=-1, num_parallel_tree=1\n                    )\nxgb.fit(X, y)\npred_1 = xgb.predict(x_test)\npred_1[pred_1 < 0] = 1","c0c9b8c2":"import lightgbm as lgb\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.03, n_estimators=2200,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n\nmodel_lgb.fit(X,y)\npred_2 = model_lgb.predict(x_test)\n","f42f546c":"from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5,)\nGBoost.fit(X, y)\npred_4 = GBoost.predict(x_test)\n","9cef13bc":"#XGBOOST\nmodel=XGBRegressor(n_estimators=2200,learning_rate=0.05)\nmodel.fit(X,y)\n\npred_6 = model.predict(x_test)\n","69b3f76f":"# pred_1 = 0.7*(pred_1 + pred_2)\/2.0 + 0.3*(pred_5 + pred_4)\/2.0","708e6f76":"pred_1 = np.expm1(pred_1)\npred_2 = np.expm1(pred_2)\n# pred_3 = np.expm1(pred_3)\npred_4 = np.expm1(pred_4)\n# pred_5 = np.expm1(pred_5)\npred_6 = np.expm1(pred_6)\n# pred_7 = np.expm1(pred_7)","a04a2ec2":"pred_1 = (pred_1 + pred_6 + pred_2 + pred_4)\/4.0","af72fc37":"sub = pd.DataFrame({'Item_Id':test_df.Item_Id.values,\n                   'Low_Cap_Price':pred_1})\nsub.to_csv('submission_ensemble.csv', index=False)","ea343d4d":"### Merging test with train to do one hot encoding at once","9e69741b":"![](https:\/\/image.freepik.com\/free-vector\/summer-sale-background-with-beach_23-2147820808.jpg)","e53fe780":"### Checking target skewness","1525cb06":"## About the Dataset:\n[Hacker_Earth_Challenge](https:\/\/www.hackerearth.com\/challenges\/competitive\/hackerearth-machine-learning-challenge-predict-the-lowest-price\/)\n\nThe dataset consists of various parameters such as a unique item ID, category of the market to which a product belongs, category and quality of the product, its demand rate, and its original market price.\n\nWe are supposed to build a predictive model that determines the lowest price at which a good can be sold on the day of sale.","42ce51ab":"# If you like my kernel, do Upvote :)","2cc72187":"### Things that worked for me:\n* One hot encoding categorical columns\n* Remove skewness from ['Demand', 'High_Cap_Price'] columns and target column too.\n* Normalize numerical columns ['Demand', 'High_Cap_Price']\n* Ensembling various regression models\n* Datetime format taken into use: broken down to 3 columns namely ['dates','month','year'] but used only 'year' column for training.\n\n### Things that didn't work:\n* Pseudo labeling with ensembling\n* LabelEncoding","42f095ad":"### One_Hot Encoding","9a7d7338":"### Extracting year information\nThis step gave me a significant jump in LB","e2859ec9":"## Preparation for Ensembling various regression models","97a75ad1":"### Removing skewness form other numerical columns"}}