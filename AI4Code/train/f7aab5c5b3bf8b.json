{"cell_type":{"81fa0716":"code","97f2e846":"code","33b2847e":"code","dd6cfe69":"code","3da52bbd":"code","498c3c21":"code","26d5fdee":"code","d9115eca":"code","cff60438":"code","3297fc01":"code","dc57926d":"code","1d6abe83":"code","242cab57":"code","9e1425bc":"code","7cdbac0e":"code","8ab8ff7b":"markdown","6c1439e3":"markdown","a0fd4c1f":"markdown","5904c3ce":"markdown","4b1e8447":"markdown","a8953ec0":"markdown"},"source":{"81fa0716":"import re\nimport string\n\nimport pandas as pd\nimport numpy as np\n\nimport nltk\nfrom nltk.tokenize import TweetTokenizer\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom pprint import pprint\nfrom tqdm.notebook import tqdm","97f2e846":"# Load train data\nusecols = ['id', 'review_title', 'review_text']\nraw_train_data = pd.read_csv('..\/input\/i2a2-nlp-2021-sentiment-analysis\/train.csv', index_col='id', usecols=usecols + ['rating'])\n\n# Load test data\nraw_test_data = pd.read_csv('..\/input\/i2a2-nlp-2021-sentiment-analysis\/train.csv', index_col='id', usecols=usecols)","33b2847e":"# Concatenate text features\nraw_train_data['review'] = raw_train_data['review_title'] + ' ' + raw_train_data['review_text']\nraw_test_data['review'] = raw_test_data['review_title'] + ' ' + raw_test_data['review_text']\n\nwith pd.option_context('display.max_colwidth', None):\n    display(raw_train_data[['review', 'rating']].head())","dd6cfe69":"corpus = ' '.join([*raw_train_data['review']])\nvocab = nltk.FreqDist(TweetTokenizer().tokenize(corpus))","3da52bbd":"print(f'The training data has {len(vocab):,} unique words.', '---' , sep='\\n', end='\\n\\n')\n\n# Show the 100 most common words\npprint(vocab.most_common(100),  compact=True)","498c3c21":"# Show the 100 least common words\npprint(vocab.most_common()[:-101:-1],  compact=True)","26d5fdee":"long_words = [word for word in vocab if len(word) > 20]\n\n# Show some long words\npprint(long_words[:10], compact=True)","d9115eca":"short_words = [word for word in vocab if len(word) <= 3]\n\n# Show some short words\npprint(short_words[:110], compact=True)","cff60438":"# Compile regular expressions\nremove_url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\nremove_email = re.compile(r'\\S+@\\S+')\nremove_duplicate_word = re.compile(r'\\b(\\w+?)\\1+')\nremove_duplicate_char = re.compile(r'([^rs])(?=\\1+)|(r)(?=r{2,})|(s)(?=s{2,})')\nremove_number = re.compile(r'\\d+')\nremove_extra_space = re.compile(r'\\s+')\n\n# Load punctuation\npunctuation = [*string.punctuation]\npunctuation.extend(['\u00ba', '\u00aa'])\n\ndef preprocess_text(text: str) -> str:\n\n    # Convert to lowercase\n    text = text.lower() \n    \n    # Apply regular expressions\n    text = remove_url.sub('', text)    # remove urls\n    text = remove_email.sub('', text)  # remove emails\n    text = remove_duplicate_word.sub(r'\\1', text) # remove duplicate words\n    text = remove_duplicate_char.sub('', text)    # remove duplicate chars; except \"rr\" and \"ss\" digraphs\n    text = remove_number.sub(' ', text)  # remove numbers\n\n    ## Expand abbreviatons\n    text = re.sub(r'\\b(n|\u00f1)([a\u00e3\u00e2]o)?\\b', ' n\u00e3o ', text)\n    text = re.sub(r'\\bt[a\u00e1]\\b', ' est\u00e1 ', text)\n    text = re.sub(r'\\b(p(r[oa\u00e1])?)\\b', ' para ', text)\n    text = re.sub(r'\\bq\\b', ' que ', text)\n    text = re.sub(r'\\bpq[s]?\\b', ' porque ', text)\n    text = re.sub(r'\\btb[m|n]?\\b', ' tamb\u00e9m ', text)\n    text = re.sub(r'\\vc[s]?\\b', ' voc\u00ea ', text)\n    text = re.sub(r'\\bmt[ao]?s?\\b', ' muito ', text)\n    text = re.sub(r'\\b(p(r[oa\u00e1]s?)?)\\b', ' para ', text)\n    text = re.sub(r'\\bhj\\b', ' hoje ', text)\n    text = re.sub(r'\\bobs\\b', ' observa\u00e7\u00e3o ', text)\n    text = re.sub(r'\\beh\\b', ' \u00e9 ', text)\n\n    # Preprocess long words\n    text = re.sub(r'(\u00f3timo)[v]?\\1+', r' \\1 ', text)\n    text = re.sub(r'\\b(ok)\\1+', r' \\1 ', text)\n    text = re.sub(r'\\b(natural)', r' \\1 ', text)\n    text = re.sub(r'(((bla)(ba)?)+\\2)', r' \\2 ', text)\n    text = re.sub(r'(altura|largura)x', r' \\1 ', text)\n    text = re.sub(r'(compra|embalagem)x', r' \\1 ', text) \n    text = re.sub(r'(ruim|regular|bom|[o\u00f3]timo|excelente)', r' \\1 ', text)\n\n    # Remove trailing spaces\n    text = remove_extra_space.sub(' ', text)\n\n    # Instatiate a TweetTokenizer object\n    tokenizer = TweetTokenizer(preserve_case=False, # lowercasing\n                               reduce_len=True, \n                               strip_handles=True)  # remove mentions\n                               \n    # Tokenize the text    \n    tokens = tokenizer.tokenize(text)\n\n    # Remove punctuation\n    tokens = [token for token in tokens if token not in punctuation]\n    \n    # Remove non-alphabetic and longer than twenty chars words\n    tokens = [token for token in tokens if token.isalpha() and len(token) <= 20]\n\n    return ' '.join(tokens)","3297fc01":"%%time\nraw_train_data['review_clean'] = raw_train_data['review'].apply(preprocess_text)\nraw_test_data['review_clean'] = raw_test_data['review'].apply(preprocess_text)","dc57926d":"corpus = ' '.join([*raw_train_data['review_clean']])\nvocab = nltk.FreqDist(TweetTokenizer().tokenize(corpus))","1d6abe83":"print(f'The training data has {len(vocab):,} unique words.', '---' , sep='\\n', end='\\n\\n')\n\n# Show the 100 most common words\npprint(vocab.most_common(100),  compact=True)","242cab57":"# Show the 100 least common words\npprint(vocab.most_common()[:-101:-1],  compact=True)","9e1425bc":"short_words = [word for word in vocab if len(word) <= 3]\n\n# Show some short words\npprint(short_words[:105], compact=True)","7cdbac0e":"%%time\n\nX = raw_train_data['review_clean']\ny = raw_train_data['rating']\n\nvectorizer = TfidfVectorizer(strip_accents='unicode', # remove accents                              \n                             token_pattern=r'\\w{2,}',\n                             ngram_range=(1, 3),\n                             min_df=3,\n                             use_idf=1, \n                             smooth_idf=1, \n                             sublinear_tf=1)\n\nmodels = [\n    ('Naive Bayes', MultinomialNB()),\n    ('Logistic Regression', LogisticRegression(class_weight='balanced', random_state=0, n_jobs=-1)),\n    ('SVM', LinearSVC(class_weight='balanced', random_state=0)),\n    ('KNN', KNeighborsClassifier(n_jobs=-1)),\n    ('Decision Tree', DecisionTreeClassifier(random_state=0, class_weight='balanced')),    \n    ('Random Forest', RandomForestClassifier(n_jobs=-1, random_state=0, class_weight='balanced')), \n    ('Extra Trees', ExtraTreesClassifier(n_jobs=-1, random_state=0, class_weight='balanced')), \n    ('XGBoost', XGBClassifier(random_state=0, n_jobs=-1)), \n    ('LightGBM', LGBMClassifier(objective='multiclass',class_weight='balanced', random_state=0, n_jobs=-1))\n]\n\n# Perform cross-validation\nfor name, model in tqdm(models, leave=None):\n\n    pipeline = Pipeline([('vectorizer', vectorizer),\n                         (name, model)])\n    \n    scores = cross_val_score(pipeline, X, y, scoring='accuracy')\n    \n    acc = np.mean(scores)\n    std = np.std(scores)\n    \n    print(f'Accuracy {acc:.2%} +\/- {std:.2%} = {(acc - std):.2%} -- {name}.')","8ab8ff7b":"## After data preprocessing","6c1439e3":"## Loading training and testing data","a0fd4c1f":"## Loading useful libraries and modules","5904c3ce":"## Preprocessing raw text data","4b1e8447":"## Exploratory data analysis","a8953ec0":"## Training and evaluating some models"}}