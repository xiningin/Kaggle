{"cell_type":{"e849ead2":"code","eb670128":"code","1b0f3703":"code","02507584":"code","2c11d56f":"code","200b895f":"code","a2c98b80":"code","e6b303de":"code","15481995":"code","fec7428e":"code","ccf2356c":"code","dfb21534":"code","44ba66ca":"code","25e65631":"code","79638b99":"code","9b2840e6":"code","a1ad1e43":"code","d6809de6":"code","a129cfb2":"code","ee4cac90":"code","f9fb9533":"code","134f0a62":"code","9be614e1":"code","9ba2f541":"code","c5388522":"code","bffe127b":"code","7e88e4b0":"code","0e1e270e":"code","2719488b":"code","43ebe05b":"code","c976df62":"code","980cff92":"code","65257898":"code","1a897c88":"markdown","e1f3804a":"markdown","e29d138b":"markdown","b38018df":"markdown","64f7e60f":"markdown","ba5204fe":"markdown","4de3cdd5":"markdown","a249e458":"markdown","fb24ffab":"markdown","91793ca6":"markdown","f28ee678":"markdown","36de992b":"markdown","1dc2a852":"markdown","f1f7c28c":"markdown","c21d6228":"markdown","241f425a":"markdown","74aabd85":"markdown","761a244a":"markdown","ab899d2c":"markdown","a7b46ca4":"markdown","f9c9e8b5":"markdown","11dccfc3":"markdown","35c67921":"markdown","237d78ed":"markdown","5e6ed77d":"markdown","07c61a49":"markdown","05f0adc4":"markdown","f3fd4530":"markdown"},"source":{"e849ead2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","eb670128":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns","1b0f3703":"import warnings\nwarnings.filterwarnings('ignore')","02507584":"data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv',usecols=['Age','Fare','Survived'])\ndata.head()","2c11d56f":"data.isna().sum()","200b895f":"data['Age'].fillna(data['Age'].median(),inplace=True)","a2c98b80":"import scipy.stats as stat\nimport pylab","e6b303de":"def plot_data(df,feature):\n    plt.figure(figsize=(10,6))\n    plt.subplot(1,2,1)\n    df[feature].hist()   #1st plot\n    plt.subplot(1,2,2)\n    stat.probplot(df[feature],dist='norm',plot=pylab)\n    plt.show()","15481995":"plot_data(data,'Age')","fec7428e":"data['Age_log'] = np.log(data['Age'])\nplot_data(data,'Age_log')","ccf2356c":"data['fare_log'] = np.log(data['Fare'])\nplot_data(data,'Fare')","dfb21534":"data['Age_sq'] = np.sqrt(data['Age'])\nplot_data(data,'Age_sq')","44ba66ca":"data['Age_reciprocal'] = 1\/ data['Age']\nplot_data(data,'Age_reciprocal')","25e65631":"data['Age_Sq'] = data['Age'] ** data['Age']\nplot_data(data,'Age_sq')","79638b99":"data['Age_boxcox'],parameters = stat.boxcox(data['Age']) ","9b2840e6":"print(parameters)","a1ad1e43":"plot_data(data,'Age_boxcox')","d6809de6":"data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv',usecols=['Age','Fare','Survived'])\ndata.head()","a129cfb2":"data['Age'].fillna(data.Age.median(),inplace=True)","ee4cac90":"mean_age = data.Age.mean()\nmax_min = data.Age.max() - data.Age.min()","f9fb9533":"data['age_scaled'] = (data['Age'] - mean_age) \/ max_min\ndata.head()","134f0a62":"data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv',usecols=['Age','Fare','Survived'])\n#data.head()","9be614e1":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()","9ba2f541":"df_scaled = scaler.fit_transform(data)\nss = pd.DataFrame(df_scaled,columns=['Survived','Age','Fare'])\nss.head()","c5388522":"data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv',usecols=['Age','Fare','Survived'])\n#data.head()","bffe127b":"from sklearn.preprocessing import MinMaxScaler\nmin_max = MinMaxScaler()","7e88e4b0":"minmax_scaled = min_max.fit_transform(data)\nminmax = pd.DataFrame(minmax_scaled)\nminmax.head()","0e1e270e":"data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv',usecols=['Age','Fare','Survived'])\n#data.head()","2719488b":"from sklearn.preprocessing import RobustScaler\nscaler = RobustScaler()","43ebe05b":"rob_scale = scaler.fit_transform(data)\nrs = pd.DataFrame(rob_scale)\nrs.head()","c976df62":"data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv',usecols=['Age','Fare','Survived'])\n#data.head()","980cff92":"from sklearn.preprocessing import MaxAbsScaler\nmaxabs = MaxAbsScaler()","65257898":"mx = maxabs.fit_transform(data)\nmxb = pd.DataFrame(mx)\nmxb.head()","1a897c88":"### 4) Robust Scaling\n**It is also known as scaling by median because it do not centers the mean, it centers the median towards 0.**\n\n**Its procedure is subtracting the median of observation and dividing with interquantile difference which is difference of 75th quantile and 25th quantile**\n\n`IQR = 75th quantile - 25th quantile`\n\n`x_scaled = x - median(x) \/ IQR`\n\n**Key-Points**\n1. centers the median at 0.\n2. maximum and minimum value may vary.\n3. variance vary across the variable\n4. it may not preserve the shape of distribution of variable\n5. it is robust to outliers.\n6. scale the features between the range of -1 to 1.","e1f3804a":"### 5) Box-Cox Transformation\n\n**Box-Cox transformation is the most sucessfull transformation and it's the evolution of exponential transformation. By applying this transformation we looks as various exponents instead of trying them manually.**\n\n**By doing so, we are searching and evaluating at all the other transformation and choose the best option**\n\n**The Box-Cox transformation is defined as:\n\n`T(Y)=(Y exp(\u03bb)\u22121)\/\u03bb`\n\n**where Y is the response variable and \u03bb is the transformation parameter.**\n\n**The exponent here is the term called lambda (\u03bb) which varies from -5 to 5. and in the process of searching it examines at each value of lambda and find the optimal value(which is best approximation for normal distribution) for our variable.**\n","e29d138b":"### 4) Exponential Transformation or Power Transformation\n\n**Exponential transformation has a reasonable effect on shape of distribution. generally it is used to reduce the shape of left-skewness**","b38018df":"**So, to overcome this conditions it's necessary to scale the features under some particular range**\n\n#### Scaling Methods\n1. Mean Normalization\n2. Standardization\n3. Scaling to Maximum and Minimum\n4. Robust Scaling(Scale to median and IQR)\n5. Scale to absolute Maximum.","64f7e60f":"**Here, I will like to end the discussion of techniques to transform and scale the fetaures. In the next kernel I will be continuing this feature engineering part with the Techniques to Handle the outliers. If you found something interesting and useful then please upvote this notebook, It is will be motivation for me to help anyone and to move ahead with my journey**\n\n**Thank You**","ba5204fe":"### 5) scaling To absolute Maximum\n\n**It is pretty simple technique which scale the variable between the range of -1 to 1.**\n\n**The procedure is simple as to only divide the observation by max value of observation**\n`x_bar = x \/ max(x)`\n\n**Key-Points**\n1. resulting mean is not centered\n2. sensitive to outliers.\n3. doesn't scale the variance.\n\nthe code with MaxAbsScaler is here.","4de3cdd5":"* We can see what the significant change it gives to distribution.\n* Moreover, in this case our Age distribution has been distrubed but then also on other variable it has much good effect.","a249e458":"### Why Feature Scaling Matters?\n\n1. The scale of the variable directly influence the regression coefficients.\n2. Gradient Descent converges faster if the variables are on similar scale.\n3. it is easy to reduce the time to find the support vectors for SVC\n4. the variables with significant magnitute dominant most compare to less significant magnitude.\n5. Euclidian Distance is more sentitive to feature magnitude.","fb24ffab":"**let's see the distribution of Age using normal distribution plot & QQ plot**","91793ca6":"### 2) Square-root Transformation\n\n**It has the average effect on the shape of distribution, It's weaker then Logarithmic transformation and also mostly used for reducing right-skewed distribution**\n\n**It has one advantage that we can apply it to zero-values But function is defined for only positive numbers**","f28ee678":"**Importing libraries**","36de992b":"### 3) Min-Max Scaler\n\n**It is scaling the values between maximum and minimum range and scaled down between the range of [0-1]**\n\n**The procedure is that subtracts the minimum value from observation and divide it by difference between max and min values**\n`x-bar = x - min(x) \/ max(x) - min(x)`\n\n**Key-points to remember**\n1. does not center the mean towards 0.\n2. variance vary across the variables.\n3. it may not preserve the shape of distribution.\n4. It is sensitive to outliers.\n5. scale the variable between the range of 0-1.","1dc2a852":"* we can see what an transformation it has done","f1f7c28c":"**for more:**\n\n**Missing Value Imputation & Categorical Encoding Techniques please visit:**\nhttps:\/\/www.kaggle.com\/rxsraghavagrawal\/all-technique-for-missing-imputation-cat-encoding\n","c21d6228":"### 1) Logarithmic Transformation\n**This is mone of the most commonly used transformation technique and has a significant effect on shape of distribution**\n\n**you can simply use the `natural log` or `log base 10` to make extremly skewed distribution less skewed especially it is used for right-skewed distributions**\n\n**NOTE: This transformation can only be applied on Strictly positive Numbers**","241f425a":"**Some of the Machine learning models like Linear and Logistic Regression assumes that the feature follow a Normal Distribution. More likely, while working with the real-time datasets the condition is not so easy beacuse the features do not follow so.**\n\n**So, by applying a number of transformation techniques and mappings to the skewed distribution we can transform the variables to a normal distribution which can increase the performance of ML model.**\n\n**First, we have to observe the distribution of the variable, whether it's skewed or normally distributed. and this can be done with help of histogram or QQ-plot.**","74aabd85":"## FEATURE SCALING","761a244a":"### 3) Reciprocal Transformation\n\n**the reciprocal transformation has a radical effect on shape of distribution. reciprocal simply means reversing the order amoung the values of same sign. the negative reciprocal preserves the order amoung same sign.**\n\n**NOTE: It cannot be applied on zero values**","ab899d2c":"* what a change it has given to shape. although its not to our need but can be used in many more cases","a7b46ca4":"**Here the age is approximate normally distributed on the line but OK we need to study all the transformation techniques here, and as well we will try the technique on Fare also** ","f9c9e8b5":"**Feature scaling refers to normalize the values in a particular range. In other words, the ways to set the feature value range within a similar scale.**","11dccfc3":"* now we can have a look how it is scaled between the -1 to 1.","35c67921":"* It does not disturb the scale of binary variable(target) or survived column as it is as 0 and 1 only.\n* and the age and Fair are scaled between 0-1","237d78ed":"### How to Transform the variables\n**Well, there are some of the pre-defined techniques to do so and I am going to talk about each technique with there advantages and limitations. so please follow this notebook and if you find it useful then plese don't forgot to upvote. it will be so motivating and appreciating for me to move ahead.**\n\n**Some of techniques we are going to take is:**\n1. Logarithmic Transformation\n2. Square-root Transformation\n3. Reciprocal Transformation\n4. Exponential Transformation\n5. Box-Cox Transformation","5e6ed77d":"* Nice, we can see upto the extent it is giving us again back the original distribution what we have there.","07c61a49":"* Nice, we can see that upto the extent it is converted the shape into normally distribution.","05f0adc4":"### 1) Mean Normalization\n\n**mean normalization suggest centring the variable towards the 0 and re-scaling the variable between the range of -1 to 1.**\n\n**mean normalization include the difference between observation and mean of all the observations and divide it by difference between maximum and minimum value of all the observations**\n`x-bar = x- mean(x) \/ max(x) - min(x)`\n\n**Points to remember for these**\n* It may change the shape of distribution.\n* it centers the mean at 0\n* the resulting variance will be different.\n* scale the variable between -1 to 1\n* preserve the outliers if exist.","f3fd4530":"### 2) Standardization\n\n**It is mostly used scaling technique which scale down the feature between the range of 0 to 1. it centers the variable at 0 and variance at 1.\n\n**The procedure of these is subracting the mean from each observation and dividing it by standard deviation**\n`x-bar = x - mean(x) \/ std(x)`\n\n**Key-Points of StandardScaler**\n1. cetres the mean at 0\n2. cetres the variance at 1.\n3. it preserves the shape of distribution\n4. maximum and minimum value vary\n5. it preserves the outliers if present."}}