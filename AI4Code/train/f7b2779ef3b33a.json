{"cell_type":{"904902b8":"code","dc8e22b1":"code","f3eb8cbe":"code","59584708":"code","e15c16e8":"code","14f5cffd":"code","b59e11d0":"code","34df8b81":"code","04175511":"code","6b7354fe":"code","e76b2e81":"code","cb155727":"code","82f4adb4":"code","7e238f12":"code","b4ea5262":"code","a9ead562":"code","1670cea8":"code","e057d329":"code","a9584c01":"code","6b329eb9":"code","0ebcb879":"code","2897742c":"code","2d492702":"code","07215b87":"code","79158986":"code","ca58aa9e":"code","b4c914b2":"code","f26ac0ec":"code","7f923950":"code","6a9ee949":"code","434c86bc":"code","12d649ea":"code","fbbd3f9a":"code","680ae592":"markdown","32f1a849":"markdown","3f40ec90":"markdown","53b9fb7e":"markdown"},"source":{"904902b8":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport random\nimport cv2\nimport tensorflow as tf\n\nfrom math import ceil, floor\nfrom copy import deepcopy\nfrom tqdm.notebook import tqdm\nfrom imgaug import augmenters as iaa\n\nimport tensorflow.keras as keras\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.callbacks import Callback, ModelCheckpoint\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout\nfrom tensorflow.keras.models import Model, load_model, Sequential\nfrom tensorflow.keras.utils import Sequence\nfrom tensorflow.keras.losses import binary_crossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import AUC, Recall, Precision, BinaryCrossentropy\nfrom tensorflow.keras.applications.densenet import DenseNet121\nfrom tensorflow.keras.layers import *\nfrom sklearn.utils.class_weight import compute_class_weight\n\ndef calculating_class_weights(y_true):\n    number_dim = np.shape(y_true)[1]\n    weights = np.empty([number_dim, 2])\n    for i in range(number_dim):\n        weights[i] = compute_class_weight('balanced', classes=np.unique(y_true[:, i]), y=y_true[:, i])\n    return weights\n\ndef _read(path, SHAPE):\n    img = cv2.imread('..\/input\/rsna-cq500-abnormal-data\/'+path)\n    img = cv2.resize(img, dsize=(256, 256))\n    return img\/255.0\n\n# Image Augmentation\nsometimes = lambda aug: iaa.Sometimes(0.25, aug)\naugmentation = iaa.Sequential([ iaa.Fliplr(0.25),\n                                iaa.Flipud(0.10),\n                                sometimes(iaa.Crop(px=(0, 25), keep_size = True, sample_independently = False))   \n                            ], random_order = True)       \n        \n# Generators\nclass TrainDataGenerator(keras.utils.Sequence):\n    def __init__(self, dataset, class_names, batch_size = 16, img_size = (256, 256, 3), \n                 augment = False, shuffle = True, *args, **kwargs):\n        self.dataset = dataset\n        self.ids = self.dataset['imgfile'].values\n        self.labels = self.dataset[class_names].values\n        self.batch_size = batch_size\n        self.img_size = img_size\n        self.augment = augment\n        self.shuffle = shuffle\n        self.on_epoch_end()\n\n    def __len__(self):\n        return int(ceil(len(self.ids) \/ self.batch_size))\n\n    def __getitem__(self, index):\n        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n        X, Y = self.__data_generation(indices)\n        return X, Y\n\n    def augmentor(self, image):\n        augment_img = augmentation        \n        image_aug = augment_img.augment_image(image)\n        return image_aug\n\n    def on_epoch_end(self):\n        self.indices = np.arange(len(self.ids))\n        if self.shuffle:\n            np.random.shuffle(self.indices)\n\n    def __data_generation(self, indices):\n        X = np.empty((self.batch_size, *self.img_size))\n        Y = np.empty((self.batch_size, len(class_names)), dtype=np.float32)\n        \n        for i, index in enumerate(indices):\n            ID = self.ids[index]\n            image = _read(ID, self.img_size)\n            if self.augment:\n                X[i,] = self.augmentor(image)\n            else:\n                X[i,] = image\n            Y[i,] = self.labels[index]        \n        return X, Y\n\ndef ModelCheckpointFull(model_name):\n    return ModelCheckpoint(model_name, \n                            monitor = 'val_loss', \n                            verbose = 1, \n                            save_best_only = True, \n                            save_weights_only = True, \n                            mode = 'min', \n                            period = 1)\n\n# Create Model\ndef create_model(num_classes):\n    K.clear_session()\n    \n    input_shape = (256, 256, 3)\n    img_input = Input(shape=input_shape)\n    base_model = DenseNet121(\n        include_top=False,\n        input_tensor=img_input,\n        input_shape=input_shape,\n        weights='imagenet'\n    )\n    x = base_model.output\n    headModel = Sequential()(x)\n    headModel = Convolution2D(128, (3, 3), name='CONV2D_C1', padding='same')(headModel)\n    headModel = Activation('relu', name='CONV2D_Relu_C1')(headModel)\n    headModel = Convolution2D(64, (3, 3), name='CONV2D_C2', padding='same')(headModel)\n    headModel = Activation('relu', name='CONV2D_Relu_C2')(headModel)\n    headModel = Convolution2D(32, (3, 3), name='CONV2D_C3', padding='same')(headModel)\n    headModel = Activation('relu', name='CONV2D_Relu_C3')(headModel)\n    headModel = Convolution2D(16, (3, 3), name='CONV2D_C4', padding='same')(headModel)\n    headModel = Activation('relu', name='CONV2D_Relu_C4')(headModel)\n    headModel = MaxPooling2D(pool_size = (2,2), name='MAXPOOL')(headModel)\n    headModel = BatchNormalization(name='BatchNorm')(headModel)\n    headModel = GlobalMaxPool2D()(headModel)\n    x = Dropout(0.15)(headModel)\n    predictions = Dense(num_classes, activation='sigmoid', name=\"new_predictions\")(x)\n    model = Model(inputs=img_input, outputs=predictions)\n\n    return model\n\ndef metrics_define(num_classes):\n    metrics_all = ['accuracy',\n    AUC(curve='PR',multi_label=True,name='auc_pr'),\n    AUC(multi_label=True, name='auc_roc'),\n    Recall(),\n    Precision(),\n    BinaryCrossentropy(name='bi_crent')\n    ]\n\n    return metrics_all\n\ndef get_weighted_loss(weights):\n    def weighted_loss(y_true, y_pred):\n        return K.mean((weights[:,0]**(1-y_true))*(weights[:,1]**(y_true))*K.binary_crossentropy(y_true, y_pred), axis=-1)\n    return weighted_loss\n\nfrom prettytable import PrettyTable\nfrom sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\nfrom sklearn.metrics import multilabel_confusion_matrix\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport seaborn as sns\n\n\ndef print_metrics_table(y_true, y_hat, y_pred, class_names):\n    myTable = PrettyTable([\"Class Name\", \"ROC_AUC\", \"Precsion\", \"Recall\", \"F1_Score\", \"Accuracy\"])\n\n    for i in range(len(class_names)) :\n        \n        myTable.add_row([class_names[i], \"%.4f\" % roc_auc_score(y_true[:, i], y_hat[:, i]),\n                        \"%.4f\" % precision_score(y_true[:, i], y_pred[:, i]), \"%.4f\" % recall_score(y_true[:, i], y_pred[:, i]),\n                        \"%.4f\" % f1_score(y_true[:, i], y_pred[:, i]), \"%.4f\" % accuracy_score(y_true[:, i], y_pred[:, i])\n                        ])\n\n    myTable.add_row(['Average', \"%.4f\" % roc_auc_score(y_true, y_hat),\n                    \"%.4f\" % precision_score(y_true, y_pred, average='macro'), \"%.4f\" % recall_score(y_true, y_pred, average='macro'),\n                    \"%.4f\" % f1_score(y_true, y_pred, average='macro'), \"%.4f\" % accuracy_score(y_true, y_pred)\n                    ])\n    print(myTable)\n\ndef print_precision_recall_curves(y_true, y_hat, y_pred, class_names):\n        # For each class\n    precision = dict()\n    recall = dict()\n    average_precision = dict()\n    for i in range(len(class_names)):\n        precision[i], recall[i], _ = precision_recall_curve(y_true[:, i],\n                                                            y_hat[:, i])\n        average_precision[i] = average_precision_score(y_true[:, i], y_hat[:, i])\n\n    # A \"micro-average\": quantifying score on all classes jointly\n    precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_true.ravel(),\n        y_hat.ravel())\n    average_precision[\"micro\"] = average_precision_score(y_true, y_hat,\n                                                        average=\"micro\")\n    print('Average precision score, micro-averaged over all classes: {0:0.2f}'\n        .format(average_precision[\"micro\"]))\n    plt.figure()\n    plt.step(recall['micro'], precision['micro'], where='post')\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.ylim([0.0, 1.05])\n    plt.xlim([0.0, 1.0])\n    plt.title(\n        'Average precision score, micro-averaged over all classes: AP={0:0.2f}'\n        .format(average_precision[\"micro\"]))\n    plt.show()\n    for i in range(len(class_names)):\n        plt.figure()\n        plt.plot(recall[i], precision[i], label='Precision-recall for class {0} (area = {1:0.2f})'.format(i, average_precision[i]))\n        plt.plot([0, 1], [0, 1], 'k--')\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('Recall')\n        plt.ylabel('Precision')\n        plt.title('Precision-Recall Curve for Class {}'.format(class_names[i]))\n        plt.legend(loc=\"lower right\")\n        plt.show()\n\ndef print_auc_curves(y_true, y_hat, y_pred, class_names):\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    roc_auc_sc = dict()\n    for i in range(len(class_names)):\n        fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_hat[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n        roc_auc_sc[i] = roc_auc_score(y_true[:, i], y_hat[:, i])\n\n    # Compute micro-average ROC curve and ROC area\n    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true.ravel(), y_hat.ravel())\n    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n    for i in range(len(class_names)):\n        plt.figure()\n        plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f)' % roc_auc[i])\n        plt.plot([0, 1], [0, 1], 'k--')\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('Receiver operating characteristic for class {}'.format(class_names[i]))\n        plt.legend(loc=\"lower right\")\n        plt.show()\n\ndef print_confusion_matrix(y_true, y_hat, y_pred, class_names):\n    print(multilabel_confusion_matrix(y_true, y_pred))\n\ndef print_on_vs_all_cmatrix(y_true, y_hat, y_pred, class_names):\n    confusion = multilabel_confusion_matrix(y_true, y_pred)\n\n    # Plot confusion matrix \n    fig = plt.figure(figsize = (14, 8))\n    for i, (label, matrix) in enumerate(zip(class_names[0:6], confusion[0:6])):\n        plt.subplot(f'23{i+1}')\n        labels = [f'not_{label}', label]\n        cm = matrix.astype('float') \/ matrix.sum(axis=1)[:, np.newaxis]\n        sns.heatmap(cm, annot = True, square = True, cbar = False, cmap = 'Blues', \n                    xticklabels = labels, yticklabels = labels, linecolor = 'black', linewidth = 1)\n        plt.title(labels[0])\n\n    plt.tight_layout()\n    plt.show()","dc8e22b1":"train_df = pd.read_csv('..\/input\/kfold-splits-rsna-cq500\/Train_f4.csv')\nval_df = pd.read_csv('..\/input\/kfold-splits-rsna-cq500\/Validation_f4.csv')\n\nHEIGHT = 256\nWIDTH = 256\nCHANNELS = 3\nTRAIN_BATCH_SIZE = 32\nVALID_BATCH_SIZE = 64\nSHAPE = (HEIGHT, WIDTH, CHANNELS)\n\nclass_names = ['epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']\n\nweights = calculating_class_weights((train_df[class_names].values).astype(np.float32))\nprint(weights)\n\ndata_generator_train = TrainDataGenerator(train_df,\n                                          class_names,\n                                          TRAIN_BATCH_SIZE,\n                                          SHAPE,\n                                          augment = True,\n                                          shuffle = True)\ndata_generator_val = TrainDataGenerator(val_df,\n                                        class_names, \n                                        VALID_BATCH_SIZE, \n                                        SHAPE,\n                                        augment = False,\n                                        shuffle = True\n                                        )\n\nTRAIN_STEPS = int(len(data_generator_train))\nprint(TRAIN_STEPS)\nVal_STEPS = int(len(data_generator_val))\nprint(Val_STEPS)\nLR = 5e-5","f3eb8cbe":"Metrics = metrics_define(len(class_names))\n\nmodel = create_model(len(class_names))\n# model.load_weights('..\/input\/rsna-cq500-abnormal-weight\/model.h5')\nmodel.compile(optimizer = Adam(learning_rate = LR),\n              loss = get_weighted_loss(weights),\n              metrics = Metrics)","59584708":"# history = model.fit(data_generator_train,\n#                     validation_data = data_generator_val,\n#                     validation_steps = Val_STEPS,\n#                     steps_per_epoch = TRAIN_STEPS,\n#                     epochs = 25,\n#                     callbacks = [ModelCheckpointFull('model_densenet_fold4_lastconv16.h5')],\n#                     verbose = 1, workers=4\n#                     )","e15c16e8":"# plt.plot(history.history['loss'])\n# plt.plot(history.history['val_loss'])\n# plt.title('model loss')\n# plt.ylabel('loss')\n# plt.xlabel('epoch')\n# plt.legend(['train', 'val'], loc='upper left')\n# plt.show()","14f5cffd":"# plt.plot(history.history['auc_pr'])\n# plt.plot(history.history['val_auc_pr'])\n# plt.title('model auc_precision')\n# plt.ylabel('auc_pr')\n# plt.xlabel('epoch')\n# plt.legend(['train', 'val'], loc='upper left')\n# plt.show()","b59e11d0":"model = create_model(5)\nmodel.load_weights('..\/input\/hemorrhage-abnormal-densenet121-fold4\/model_densenet_fold4_lastconv16.h5')","34df8b81":"val_df = pd.read_csv('..\/input\/kfold-splits-rsna-cq500\/Validation_f4.csv')\nprint(len(val_df))\n\nHEIGHT = 256\nWIDTH = 256\nCHANNELS = 3\nVALID_BATCH_SIZE = 64\nSHAPE = (HEIGHT, WIDTH, CHANNELS)\n\nclass_names = ['epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']\ndata_generator_test = TrainDataGenerator(val_df,\n                                        class_names, \n                                        VALID_BATCH_SIZE, \n                                        SHAPE,\n                                        augment = False,\n                                        shuffle = False\n                                        )\n\ny_true = val_df[class_names].values\ny_hat = model.predict(data_generator_test, verbose=1)\n\ny_hat = y_hat[0:len(y_true)]\ny_pred = np.round(y_hat)","04175511":"print_metrics_table(y_true, y_hat, y_pred, class_names)","6b7354fe":"print_precision_recall_curves(y_true, y_hat, y_pred, class_names)","e76b2e81":"print_auc_curves(y_true, y_hat, y_pred, class_names)","cb155727":"print_confusion_matrix(y_true, y_hat, y_pred, class_names)","82f4adb4":"print_on_vs_all_cmatrix(y_true, y_hat, y_pred, class_names)","7e238f12":"val_df = pd.read_csv('..\/input\/kfold-splits-rsna-cq500\/CQ500_Validation_f3.csv')\nprint(len(val_df))\n\nHEIGHT = 256\nWIDTH = 256\nCHANNELS = 3\nVALID_BATCH_SIZE = 64\nSHAPE = (HEIGHT, WIDTH, CHANNELS)\n\nclass_names = ['epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']\ndata_generator_test = TrainDataGenerator(val_df,\n                                        class_names, \n                                        VALID_BATCH_SIZE, \n                                        SHAPE,\n                                        augment = False,\n                                        shuffle = False\n                                        )\n\n\ny_true = val_df[class_names].values\ny_hat = model.predict(data_generator_test, verbose=1)\n\ny_hat = y_hat[0:len(y_true)]\ny_pred = np.round(y_hat)","b4ea5262":"print_metrics_table(y_true, y_hat, y_pred, class_names)","a9ead562":"print_precision_recall_curves(y_true, y_hat, y_pred, class_names)","1670cea8":"print_auc_curves(y_true, y_hat, y_pred, class_names)","e057d329":"print_confusion_matrix(y_true, y_hat, y_pred, class_names)","a9584c01":"print_on_vs_all_cmatrix(y_true, y_hat, y_pred, class_names)","6b329eb9":"val_df = pd.read_csv('..\/input\/kfold-splits-rsna-cq500\/RSNA_Validation_f3.csv')\nprint(len(val_df))\n\nHEIGHT = 256\nWIDTH = 256\nCHANNELS = 3\nVALID_BATCH_SIZE = 64\nSHAPE = (HEIGHT, WIDTH, CHANNELS)\n\nclass_names = ['epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']\ndata_generator_test = TrainDataGenerator(val_df,\n                                        class_names, \n                                        VALID_BATCH_SIZE, \n                                        SHAPE,\n                                        augment = False,\n                                        shuffle = False\n                                        )\n\n\ny_true = val_df[class_names].values\ny_hat = model.predict(data_generator_test, verbose=1)\n\ny_hat = y_hat[0:len(y_true)]\ny_pred = np.round(y_hat)","0ebcb879":"print_metrics_table(y_true, y_hat, y_pred, class_names)","2897742c":"print_precision_recall_curves(y_true, y_hat, y_pred, class_names)","2d492702":"print_auc_curves(y_true, y_hat, y_pred, class_names)","07215b87":"print_confusion_matrix(y_true, y_hat, y_pred, class_names)","79158986":"print_on_vs_all_cmatrix(y_true, y_hat, y_pred, class_names)","ca58aa9e":"val_df = pd.read_csv('..\/input\/kfold-splits-rsna-cq500\/Validation_f4.csv')\nprint(len(val_df))\n\nHEIGHT = 256\nWIDTH = 256\nCHANNELS = 3\nVALID_BATCH_SIZE = 64\nSHAPE = (HEIGHT, WIDTH, CHANNELS)\n\nclass_names = ['epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']\ndata_generator_test = TrainDataGenerator(val_df,\n                                        class_names, \n                                        VALID_BATCH_SIZE, \n                                        SHAPE,\n                                        augment = False,\n                                        shuffle = False\n                                        )\n\ny_true = val_df[class_names].values\ny_hat = model.predict(data_generator_test, verbose=1)\n\ny_hat = y_hat[0:len(y_true)]","b4c914b2":"class_cut_offs = []\nfor i in range(len(class_names)):\n    print('Class {}'.format(class_names[i]))\n    y_true_class = y_true[:,i]\n    y_hat_class = y_hat[:,i]\n    f1_scores_class = []\n    cutt_offs = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n    for j in cutt_offs:\n        y_hat_round = (y_hat_class >= j).astype(np.uint8)\n        f1_scores_class.append(f1_score(y_true_class, y_hat_round))\n    print('cutt_off = {}, f1_score = {}'.format(cutt_offs[np.argmax(f1_scores_class)], \"%.2f\" %f1_scores_class[np.argmax(f1_scores_class)]))\n    class_cut_offs.append(cutt_offs[np.argmax(f1_scores_class)])\n    print('------------------------------------------')","f26ac0ec":"y_pred = np.empty(y_hat.shape)\nfor i in range(len(class_names)):\n    y_pred[:,i] = (y_hat[:,i] >= class_cut_offs[i]).astype(np.uint8)","7f923950":"print_metrics_table(y_true, y_hat, y_pred, class_names)","6a9ee949":"print_precision_recall_curves(y_true, y_hat, y_pred, class_names)","434c86bc":"print_auc_curves(y_true, y_hat, y_pred, class_names)","12d649ea":"print_confusion_matrix(y_true, y_hat, y_pred, class_names)","fbbd3f9a":"print_on_vs_all_cmatrix(y_true, y_hat, y_pred, class_names)","680ae592":"# Cut-offs","32f1a849":"# CQ500 Results","3f40ec90":"# RSNA Results","53b9fb7e":"# Validation Results on all Data"}}