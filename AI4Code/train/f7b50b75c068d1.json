{"cell_type":{"a494a619":"code","999d7f0a":"code","b8e05bf3":"code","d4c8c08b":"code","fc31e0d4":"code","53148038":"code","8359292a":"code","4b03a508":"code","01286677":"code","fd80d9d2":"code","6d261dff":"code","03347b5f":"code","c3274bc6":"code","35530252":"code","d5dff42d":"code","af933f69":"code","be10c98b":"code","c10dd782":"code","63dbd045":"code","69e4d361":"code","0b86a60b":"code","4c0cd076":"code","612be89c":"code","c9d49646":"code","86920f34":"code","ad426dd1":"code","a14c7c75":"code","ec82a280":"code","ce7623d2":"code","1dc7a993":"code","2d1874d7":"code","ebfb7315":"code","bef1a1b3":"code","20b484ea":"code","ad46538f":"code","b95b5c4a":"code","fa7cf5a0":"code","c472a834":"code","07045e64":"code","ddc04ee7":"code","9a8518a6":"code","aa659a82":"code","b5c9349e":"code","80e63151":"code","48c876da":"code","bfb2b2d2":"code","0fa29997":"code","5b1eb454":"code","019c8049":"code","c67408f0":"code","3ad1803e":"code","43759d70":"code","967cb09e":"code","ec510594":"code","145a347c":"code","a6e13e10":"code","96606ede":"code","cc3d596c":"code","af82f685":"code","a28367eb":"code","78333345":"code","be24c7aa":"code","258888cd":"code","aa18072f":"code","92066485":"code","d4588e90":"code","c9757813":"code","d61cde76":"code","3ed050e0":"code","e58ee889":"code","921b8741":"code","a67a67ba":"code","b9c79cc2":"code","98ff4056":"code","039f79d3":"markdown","4ba092f4":"markdown","645e8baa":"markdown","734067ff":"markdown","4ae59c86":"markdown","9b2a42c4":"markdown","be145d94":"markdown","5e285288":"markdown","cd1bd1d8":"markdown","44d5e565":"markdown","8b299e08":"markdown","497c374a":"markdown","6cf34aa6":"markdown","09123269":"markdown","af91cc39":"markdown","f023abe2":"markdown","066c6bcc":"markdown","bb10b4f3":"markdown","7b03361e":"markdown","c3887fdd":"markdown","74327417":"markdown","f471b60b":"markdown","4f1508ee":"markdown","a2d2e3cb":"markdown","9994db4e":"markdown","8fb1b989":"markdown","474fd843":"markdown","763660ad":"markdown","b629f6d5":"markdown","d29a07b2":"markdown","69d731a8":"markdown"},"source":{"a494a619":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","999d7f0a":"import pandas as pd\nimport datetime\nimport pandas_profiling\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n%matplotlib inline\n\nfrom sklearn.feature_selection import f_classif, mutual_info_classif\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.metrics import auc, roc_auc_score, roc_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom scipy.stats import randint\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\n\nfrom mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\nfrom math import log as log\nimport os","b8e05bf3":"path = '\/kaggle\/input\/sf-dst-scoring\/'","d4c8c08b":"train = pd.read_csv(path +'\/train.csv')\ntest = pd.read_csv(path +'test.csv')\nsample = pd.read_csv(path +'\/sample_submission.csv')","fc31e0d4":"print(train.info())\nprint('Train size: ', train.shape)\ntrain.head()","53148038":"print(test.info())\nprint('Test size: ', train.shape)\ntest.head(5)","8359292a":"print(sample.info())\nprint(sample.shape)\nsample.head(5)","4b03a508":"# Let's look at default count\nplot = sns.countplot(train['default'])","01286677":"# Unite datasets to ease the preprocessing\ntrain['sample'] = 1   # train\ntest['sample'] = 0    # test\ntest['default'] = -1  # imaginary value for now\ndata = train.append(test, sort=False).reset_index(drop=True)","fd80d9d2":"print(data.info())\ndata.shape","6d261dff":"# full correlation table\ndata.corr().style.background_gradient(cmap='viridis')","03347b5f":"data.drop(columns='work_address', inplace=True) # removing multicolinearity","c3274bc6":"num_cols = ['age','decline_app_cnt','score_bki','income','bki_request_cnt','region_rating'] \ncat_cols = ['education','home_address','sna','first_time'] \nbin_cols = ['sex','car','car_type','good_work','foreign_passport'] ","35530252":"# Let's look at numerical data\nfig, axes = plt.subplots(2, 3, figsize=(25,15))\n\nfor i,col in enumerate(num_cols):\n    sns.distplot(data[col], kde=False, ax=axes.flat[i], color=\"r\")","d5dff42d":"fig, axes = plt.subplots(1, 4, figsize=(10,7))\nfor i,col in enumerate(['decline_app_cnt', 'bki_request_cnt', 'income','age']):\n    data[col] = np.log(data[col] + 1)\n    sns.distplot(data[col][data[col] > 0].dropna(), ax=axes.flat[i],kde = False, rug=False,color=\"r\")\n    \n    ","af933f69":"plt.figure(figsize=(15, 20))\n\nfor i, c in enumerate(data.drop(columns=['client_id','default'], axis=1)[num_cols].columns):\n    plt.subplot(10,2,i*2+1)\n    sns.boxplot(data[c], color='blue')\n    plt.title('Distribution plot for field:' + c)\n    plt.xlabel('')\n    plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n\n    \n    plt.subplot(10,2,i*2+2)\n    sns.boxplot(data[c].apply('log1p'), color='red')\n    plt.title('Log1p distribution plot for field:' + c)\n    plt.xlabel('')\n    plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)","be10c98b":"plt.figure(figsize=(15, 12))\n\nfor i, c in enumerate(data[num_cols].columns):\n    plt.subplot(5,2,i+1)\n    sns.distplot(data[c])\n    plt.title('Distribution plot for field:' + c)\n    plt.xlabel('')\n    plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)","c10dd782":"data.education.value_counts().plot(kind=\"bar\",figsize=(8,6),color=\"r\")\nprint(\"Missing education values:\\n\",data.education.isna().sum())","63dbd045":"data.education = data.education.fillna(\"SCH\") # replace with the most common","69e4d361":"plt.figure(figsize=(15, 8))\nsns.boxplot(x=\"education\", y=\"income\", data=data, showfliers=False)","0b86a60b":"plt.rcParams['figure.figsize'] = (15,10)\nsns.heatmap(data.corr(), vmin=0, vmax=1, annot = True)","4c0cd076":"data['app_date'] = pd.to_datetime(data.app_date)\ndata['app_date'] = data['app_date'].apply(lambda x: (x - data['app_date'].min()).days) # days from the first date","612be89c":"df = data.copy()","c9d49646":"# mean income by age\nmean_income = df.groupby('age')['income'].mean().to_dict()\ndf['mean_income_age'] = df['age'].map(mean_income)","86920f34":"# Max income by age\nmax_income = df.groupby('age')['income'].max().to_dict()\ndf['max_income_age'] = df['age'].map(max_income)","ad426dd1":"# and normalize it\ndf[\"normalized_income\"] = abs((df.income - df.mean_income_age)\/df.max_income_age)","a14c7c75":"# requests to BKI by age\nmean_bki = df.groupby('age')['bki_request_cnt'].mean().to_dict()\ndf['mean_requests_age'] = df['age'].map(mean_bki)","ec82a280":"# mean BKI requests by income\nmean_bki_inc = df.groupby('income')['bki_request_cnt'].mean().to_dict()\ndf['mean_requests_income'] = df['income'].map(mean_bki_inc)","ce7623d2":"# Mean income by region\nmean_income_rat = df.groupby('region_rating')['income'].mean().to_dict()\ndf['mean_income_region'] = df['region_rating'].map(mean_income_rat)","1dc7a993":"mapp = {}\nlabel_encoder = LabelEncoder()\nfor col in bin_cols:\n    df[col] = label_encoder.fit_transform(df[col])\n    mapp[col] = dict(enumerate(label_encoder.classes_))","2d1874d7":"mappc = {}\nenc = OneHotEncoder()\nfor col in cat_cols:\n    df[col] = label_encoder.fit_transform(df[col])\n    mappc[col] = dict(enumerate(label_encoder.classes_))\n","ebfb7315":"print(mapp)\nprint(mappc)","bef1a1b3":"df.columns","20b484ea":"# need to rename the columns\nnum_cols = ['age','decline_app_cnt','score_bki','income','bki_request_cnt','app_date', 'mean_income_age','region_rating','max_income_age', 'normalized_income',\n       'mean_requests_age', 'mean_requests_income', 'mean_income_region'] # numerical\ncat_cols = ['education','home_address','sna','first_time'] # categorical\nbin_cols = ['sex','car','car_type','good_work','foreign_passport'] # binary","ad46538f":"for col in num_cols:\n    median = df[col].median()\n    IQR = df[col].quantile(0.75) - df[col].quantile(0.25)\n    perc25 = df[col].quantile(0.25)\n    perc75 = df[col].quantile(0.75)\n    print(\"Columns: \", col)\n    print('25%: {},'.format(perc25), '75%: {},'.format(perc75), \n          \"IQR: {}, \".format(IQR),\"Borderline: [{f}, {l}].\".format(f=perc25 - 1.5*IQR, l=perc75 + 1.5*IQR))","b95b5c4a":"df1 = df.copy()\ndataset = df.copy()","fa7cf5a0":"dataset[num_cols] = pd.DataFrame(StandardScaler().fit_transform(df[num_cols]), columns = df[num_cols].columns)","c472a834":"data_temp = dataset.loc[data['sample'] == 1] ","07045e64":"imp_num = pd.Series(f_classif(data_temp[num_cols], data_temp['default'])[0], index = num_cols)\nimp_num.sort_values(inplace = True)\nimp_num.plot(kind = 'barh', color='pink')","ddc04ee7":"imp_cat = pd.Series(mutual_info_classif(data_temp[bin_cols + cat_cols], data_temp['default'],\n                                     discrete_features =True), index = bin_cols + cat_cols)\nimp_cat.sort_values(inplace = True)\nimp_cat.plot(kind = 'barh', color='pink')","9a8518a6":"df = pd.get_dummies(dataset, prefix=cat_cols, columns=cat_cols) # categorical dummies","aa659a82":"from sklearn.utils import resample\ndf1 = df.query('sample == 1')\ndf1.default.value_counts()","b5c9349e":"# Split data on majority and minority.. minority is dataset == 2\n\nminority = df1[df1.default==1]\nmajority = df1[df1.default==0]\nprint('Minority size:', minority.shape)\nprint('Majority size:', majority.shape)","80e63151":"# choosing upsample as even now we do not have too much data\nminority_upsample = resample(minority, replace=True, n_samples=majority.shape[0])\nprint('Minority upsampled size:', minority_upsample.shape)","48c876da":"# merge majority with upsampled minority\ndf1 = pd.concat([minority_upsample, majority], axis=0)","bfb2b2d2":"data = df1.copy()","0fa29997":"data['sample'].value_counts()","5b1eb454":"data.head(3)","019c8049":"train_df = data.drop(['sample','client_id'], axis=1)\ntest_df = df.query('sample == 0').drop(['sample','client_id'], axis=1)","c67408f0":"test_df","3ad1803e":"X = train_df.drop(['default'], axis=1).values\ny = train_df['default'].values # <--- target","43759d70":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20, random_state=69)","967cb09e":"lr = LogisticRegression(max_iter = 1000)","ec510594":"lr.fit(X_train, y_train)\ny_pred = lr.predict(X_valid)","145a347c":"probs = lr.predict_proba(X_valid)\nprobs = probs[:,1]\n\n\nfpr, tpr, threshold = roc_curve(y_valid, probs)\nroc_auc = roc_auc_score(y_valid, probs)\n\nplt.figure()\nplt.plot([0, 1], label='Baseline', linestyle='--')\nplt.plot(fpr, tpr, label = 'Regression')\nplt.title('Logistic Regression ROC AUC = %0.3f' % roc_auc)\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.legend(loc = 'lower right')\nplt.show()","a6e13e10":"print('accuracy_score:',accuracy_score(y_valid,y_pred))\nprint('precision_score:',precision_score(y_valid,y_pred))\nprint('recall_score:',recall_score(y_valid,y_pred))\nprint('f1_score:',f1_score(y_valid,y_pred))","96606ede":"import matplotlib as mpl\nmpl.rcParams.update(mpl.rcParamsDefault)\ncm = confusion_matrix(y_valid, y_pred)\ncmd = ConfusionMatrixDisplay(cm, display_labels=['non_default','default'])\ncmd.plot()\ncmd.ax_.set(xlabel='Predicted', ylabel='True')","cc3d596c":"from sklearn.model_selection import GridSearchCV, KFold","af82f685":"'''# Random forest\n# n_jobs=-1 to allow run it on all cores\nparams = {\n    'n_estimators': [100, 200, 500],\n    'criterion': ['gini', 'entropy'],\n    'min_samples_split': [1,2,4,5],\n    'min_samples_leaf': [1,2,4,5],\n    'max_leaf_nodes': [4,10,20,50,None]\n}\n\ngs1 = GridSearchCV(RandomForestClassifier(n_jobs=-1), params, n_jobs=-1, cv=KFold(n_splits=3), scoring='roc_auc')\ngs1.fit(X_train, y_train)\n\nprint('Best score:', gs1.best_score_)\nprint('Best score:', gs1.best_params_)'''","a28367eb":"'''# XGBoost\n# n_jobs=-1 to allow run it on all cores\nparams = {\n    'n_estimators': [100, 200, 500],\n    'learning_rate': [0.01,0.05,0.1],\n    'booster': ['gbtree'],\n    'gamma': [0, 0.5, 1],\n    'reg_alpha': [0, 0.5, 1],\n    'reg_lambda': [0.5, 1, 5],\n    'base_score': [0.2, 0.5, 1]\n}\n\ngs2 = GridSearchCV(XGBClassifier(n_jobs=-1), params, n_jobs=-1, cv=KFold(n_splits=3), scoring='roc_auc')\ngs2.fit(X_train, y_train)\n\nprint('Best score:', gs2.best_score_)\nprint('Best score:', gs2.best_params_)'''","78333345":"'''# Extra Tree\n# n_jobs=-1 to allow run it on all cores\nparams = {\n    'n_estimators': [100, 200, 500],\n    'criterion': ['gini', 'entropy'],\n    'min_samples_split': [1,2,4,5],\n    'min_samples_leaf': [1,2,4,5],\n    'max_leaf_nodes': [4,10,20,50,None]\n}\n\ngs3 = GridSearchCV(ExtraTreesClassifier(n_jobs=-1), params, n_jobs=-1, cv=KFold(n_splits=3), scoring='roc_auc')\ngs3.fit(X_train, y_train)\n\nprint('Best score:', gs3.best_score_)\nprint('Best score:', gs3.best_params_)\n'''","be24c7aa":"'''from sklearn.model_selection import cross_validate\nfrom sklearn.ensemble import VotingClassifier\n\n# votes = [\n#     ('rf', RandomForestClassifier(**gs1.best_params_)),\n#     ('xgb', XGBClassifier(**gs2.best_params_)),\n#     ('xt', ExtraTreesClassifier(**gs3.best_params_))\n# ]\n\nvotes = [\n    ('rf', gs1.best_estimator_),\n    ('xgb', gs2.best_estimator_),\n    ('xt', gs3.best_estimator_)\n]\n\n# soft voting based on weights\nvotesClass = VotingClassifier(estimators=votes, voting='soft', n_jobs=-1)\nvotesClass_cv = cross_validate(votesClass, X_train, y_train, cv=KFold(3, random_state=123))\nvotesClass.fit(X_train, y_train)\n\nvotesClass_cv'''","258888cd":"'''from sklearn.ensemble import ExtraTreesClassifier\n\nmodel = votesClass\n#model.fit(X_train, y_train)\ny_train_hat = model.predict(X_train)\ny_test_hat = model.predict(X_test)\n\nprint(model)\nprint('Train performance')\nprint('-------------------------------------------------------')\nprint(classification_report(y_train, y_train_hat))\n\nprint('Test performance')\nprint('-------------------------------------------------------')\nprint(classification_report(y_test, y_test_hat))\n\nprint('Roc_auc score')\nprint('-------------------------------------------------------')\nprint(roc_auc_score(y_test, y_test_hat))\nprint('')\n\nprint('Confusion matrix')\nprint('-------------------------------------------------------')\nprint(confusion_matrix(y_test, y_test_hat))'''","aa18072f":"params = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5]\n        }","92066485":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import StratifiedKFold","d4588e90":"train_data = data.drop(['sample','client_id'], axis=1)\ntest_data = df.query('sample == 0').drop(['sample','client_id'], axis=1)\n\nX_train = train_data.drop(['default'], axis=1)\ny_train = train_data.default.values\nX_test = test_data.drop(['default'], axis=1)","c9757813":"df1","d61cde76":"xgb = XGBClassifier(learning_rate=0.02, n_estimators=600, objective='binary:logistic',\n                    silent=True, nthread=1)","3ed050e0":"folds = 3\nparam_comb = 5\n\nskf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n\nrandom_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4, cv=skf.split(X_train,y_train), verbose=3, random_state=1001 )\n\nrandom_search.fit(X_train, y_train)\n","e58ee889":"print('\\n All results:')\nprint(random_search.cv_results_)\nprint('\\n Best estimator:')\nprint(random_search.best_estimator_)\nprint('\\n Best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))\nprint(random_search.best_score_ * 2 - 1)\nprint('\\n Best hyperparameters:')\nprint(random_search.best_params_)\nresults = pd.DataFrame(random_search.cv_results_)\n#results.to_csv('xgb-random-grid-search-results-01.csv', index=False)","921b8741":"X_test","a67a67ba":"test_data = X_test\ny_pred = random_search.predict_proba(test_data)\nresults_df = pd.DataFrame(data={'client_id':test['client_id'], 'default':y_pred[:,1]})\n","b9c79cc2":"results_df","98ff4056":"results_df.to_csv('submission3.csv', index=False)","039f79d3":"### ***Numerical***","4ba092f4":"# Tuning","645e8baa":"## Balancing Dataset","734067ff":"## Optimization","4ae59c86":"## Encoding","9b2a42c4":"### Confusion matrix","be145d94":"No outliers anymore","5e285288":"### ***Categorical and Binary***","cd1bd1d8":"## What can we see here?\n\n* people who do not default, generally, older.\n\n* people who default have higher scoring.\n\n* Region rating affects default.\n\n* BKI requests are higher in people who do not default.\n\n* On average, people who do not default have higher income.","44d5e565":"## We can notice some outliers, but the distribution is normal now","8b299e08":"## Soft Voting","497c374a":"SHeeeesh.. our model will give out credit to practically everyone, that is not good, let's make it better by tweeking hyperparameters","6cf34aa6":"# XGB with RandomSearchCV","09123269":"# Modeling","af91cc39":"Metrics show that in actuality, the model sucks","f023abe2":"## Scaling","066c6bcc":"### Hmm.. distribution is skewed to the left for some variables, lets apply log function to them","bb10b4f3":"# Information","7b03361e":"# *Feature engineering*","c3887fdd":"### Encoding","74327417":"## **ROC AUC**","f471b60b":"Let's see if we can make it better","4f1508ee":"### We do not have any extreme multicollinerality","a2d2e3cb":"### Now let's deal with the outliers","9994db4e":"Hmm.. there are way more clients who go into default. Will take care of the undersampling later","8fb1b989":"# *Categorical*","474fd843":"\u0421\u043c\u043e\u0442\u0440\u0438\u043c \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u044c \u0434\u043e\u0445\u043e\u0434\u0430 \u043e\u0442 \u0443\u0440\u043e\u0432\u043d\u044f \u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u044f","763660ad":"# *Numerical*","b629f6d5":"## *Data Description:*\nclient_id - identification\n\neducation - education level\n\nsex - sex\n\nage - age\n\ncar - binary\/ has a car or not\n\ncar_type - whether car is international\n\ndecline_app_cnt - declined application count in the past\n\ngood_work - binary\/ has 'good' work or not\n\nbki_request_cnt - requests to BKI\n\nhome_address - category of home address\n\nwork_address - category of work address\n\nincome - income\n\nforeign_passport - binary\/ has foreign passport\n\nsna - connection with bank employee\n\nfirst_time - age of information about the client\n\nscore_bki - BKI score\n\nregion_rating - region rating\n\napp_date - application date\n\ndefault - default flag","d29a07b2":"## Now let's see what features are useless","69d731a8":"# EDA"}}