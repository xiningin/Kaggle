{"cell_type":{"b536a58c":"code","a2a55f62":"code","0cdeee09":"code","5881a4d9":"code","2da656f5":"code","302a9098":"code","bb9a1a4c":"code","277f5587":"code","328a63b5":"code","5a662c71":"code","d67d9fe2":"code","67538eda":"code","7dd60e48":"code","9ef4cd1e":"code","247a1cc5":"code","92adbd37":"code","687a92b5":"code","567a9a1b":"code","69a66a4c":"code","31484840":"markdown","44029b4f":"markdown","6471c48c":"markdown","f179d84f":"markdown","29ea806c":"markdown","f689fc98":"markdown","6136e3ae":"markdown","eaafe6bd":"markdown","38299442":"markdown"},"source":{"b536a58c":"import os\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder, RobustScaler\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix","a2a55f62":"df = pd.read_csv('\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv', index_col='id')\ndf","0cdeee09":"df_to_enc_list = df.iloc[:,[0,4,5,6,9]].columns.values.tolist()","5881a4d9":"for i in range(len(df_to_enc_list)):\n    df[df_to_enc_list[i]] = LabelEncoder().fit_transform(df[df_to_enc_list[i]])","2da656f5":"df.info()","302a9098":"target = df.dropna().stroke\nfeat = df.dropna().drop(columns=['stroke'])\nscaler = RobustScaler()\nfeat = pd.DataFrame(scaler.fit_transform(feat),columns=feat.columns.values.tolist())","bb9a1a4c":"plt.figure()\nplt.hist(target)\nplt.title('Stroke distribution')\nplt.show()","277f5587":"clfDTC = DecisionTreeClassifier().fit(feat, target)\nclfRFC = RandomForestClassifier().fit(feat, target)\nclfGBC = GradientBoostingClassifier().fit(feat, target)\n\nim_DTC = clfDTC.feature_importances_\nim_RFC = clfRFC.feature_importances_\nim_GBC = clfGBC.feature_importances_\n\nfig, ax = plt.subplots(1,3,sharey=True,figsize=(16,6))\nfig.suptitle('Feature Importances')\n\nsns.heatmap(ax=ax[0],data=np.expand_dims(im_DTC,axis=1),\n            annot=True, robust=True, \n            xticklabels=[None],yticklabels=feat.columns.tolist())\n\nax[0].set_title('Decision Tree Classifier')\n\nsns.heatmap(ax=ax[1],data=np.expand_dims(im_RFC,axis=1),\n            annot=True, robust=True,\n            xticklabels=[None],yticklabels=feat.columns.tolist())\n\nax[1].set_title('Random Forest Classifier')\n\nsns.heatmap(ax=ax[2],data=np.expand_dims(im_GBC,axis=1),\n            annot=True, robust=True,\n            xticklabels=[None],yticklabels=feat.columns.tolist())\n\nax[2].set_title('Gradient Boosting Classifier')\n\nplt.show()","328a63b5":"var_exp=sorted(zip(feat.columns.values.tolist(),im_GBC),key=lambda x: x[1], reverse=True)\ncum_var_exp = []\ncum_var = 0\nfor i in range(len(var_exp)):\n    cum_var += var_exp[i][1]\n    cum_var_exp.append([var_exp[i][0],cum_var])\nprint('Cummulative variance explained:')    \ncum_var_exp","5a662c71":"predictor_feat = []\nfor i in range(len(cum_var_exp)):\n    if  cum_var_exp[i][1]<=0.98:\n        predictor_feat.append(cum_var_exp[i][0])\n\nprint('Predictor features')\npredictor_feat","d67d9fe2":"X_tr, X_ts, Y_tr, Y_ts = train_test_split(feat[predictor_feat], target, test_size=0.25,stratify=target,\n                                         shuffle=True, random_state=42)","67538eda":"clf = GradientBoostingClassifier().fit(X_tr,Y_tr)\n\nY_predGBC = clf.predict(X_ts)\n\nscore = accuracy_score(Y_ts,Y_predGBC)\nconf_mat = confusion_matrix(Y_ts, Y_predGBC)\n\nplt.figure()\n\nsns.heatmap(data = conf_mat, annot=True, robust=True, fmt='.4g',\n            xticklabels=['NO','YES'],yticklabels=['NO','YES'])\nplt.title('Accuracy Score: {:.3f}'.format(score))\n\nplt.show()","7dd60e48":"var_exp=sorted(zip(feat.columns.values.tolist(),im_RFC),key=lambda x: x[1], reverse=True)\ncum_var_exp = []\ncum_var = 0\nfor i in range(len(var_exp)):\n    cum_var += var_exp[i][1]\n    cum_var_exp.append([var_exp[i][0],cum_var])\nprint('Cummulative variance explained:')    \ncum_var_exp","9ef4cd1e":"predictor_feat = []\nfor i in range(len(cum_var_exp)):\n    if  cum_var_exp[i][1]<=0.98:\n        predictor_feat.append(cum_var_exp[i][0])\n\nprint('Predictor features')\npredictor_feat","247a1cc5":"X_tr, X_ts, Y_tr, Y_ts = train_test_split(feat[predictor_feat], target, test_size=0.25,stratify=target,\n                                         shuffle=True, random_state=42)\n\nclf = RandomForestClassifier().fit(X_tr,Y_tr)\n\nY_predRFC = clf.predict(X_ts)\n\nscore = accuracy_score(Y_ts,Y_predRFC)\nconf_mat = confusion_matrix(Y_ts, Y_predRFC)\n\nplt.figure()\n\nsns.heatmap(data = conf_mat, annot=True, robust=True, fmt='.4g',\n            xticklabels=['NO','YES'],yticklabels=['NO','YES'])\nplt.title('Accuracy Score: {:.3f}'.format(score))\n\nplt.show()","92adbd37":"var_exp=sorted(zip(feat.columns.values.tolist(),im_DTC),key=lambda x: x[1], reverse=True)\ncum_var_exp = []\ncum_var = 0\nfor i in range(len(var_exp)):\n    cum_var += var_exp[i][1]\n    cum_var_exp.append([var_exp[i][0],cum_var])\nprint('Cummulative variance explained:')    \ncum_var_exp","687a92b5":"predictor_feat = []\nfor i in range(len(cum_var_exp)):\n    if  cum_var_exp[i][1]<=0.98:\n        predictor_feat.append(cum_var_exp[i][0])\n\nprint('Predictor features')\npredictor_feat","567a9a1b":"X_tr, X_ts, Y_tr, Y_ts = train_test_split(feat[predictor_feat], target, test_size=0.25,stratify=target,\n                                         shuffle=True, random_state=42)\n\nclf = DecisionTreeClassifier().fit(X_tr,Y_tr)\n\nY_predDTC = clf.predict(X_ts)\n\nscore = accuracy_score(Y_ts,Y_predDTC)\nconf_mat = confusion_matrix(Y_ts, Y_predDTC)\n\nplt.figure()\n\nsns.heatmap(data = conf_mat, annot=True, robust=True, fmt='.4g',\n            xticklabels=['NO','YES'],yticklabels=['NO','YES'])\nplt.title('Accuracy Score: {:.3f}'.format(score))\n\nplt.show()","69a66a4c":"pd.DataFrame(data=np.array([Y_predGBC,Y_predRFC,Y_predDTC]).T,\n             columns=['Gradient Boosting','Random Forest','Decision Tree'],\n            index = X_ts.index)","31484840":"Around 200 missing values for bmi","44029b4f":"# Solution","6471c48c":"## Gradient Boosting Classifier","f179d84f":"# Data","29ea806c":"## Feature Importances","f689fc98":"* *Age*, *avg_glucose_level* and *bmi* are the most important features explaining 70-80% of the variance\nChoosing Gradient Boosting Classifier:\n* *Gender*, *Residence_type*, *Ever_married* are pure noise.","6136e3ae":"## Decission Tree Classifier","eaafe6bd":"# Model Prediction","38299442":"## Random Forest Classifier"}}