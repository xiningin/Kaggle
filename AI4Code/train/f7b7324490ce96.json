{"cell_type":{"0e1bb1d0":"code","3b3431bd":"code","49610246":"code","20c5514a":"code","7d503eb8":"code","44107bb5":"code","3d48f1ec":"code","25bd2786":"code","36569dc0":"code","d565e80f":"code","9e8c9ae5":"code","6c29f752":"code","fc3d932b":"code","f3a03221":"code","ba505a3a":"code","14705a06":"code","957b1f7f":"code","1697292e":"code","4f657a9b":"code","c6915e2e":"code","595aa266":"code","9538b7e7":"code","99b2e915":"code","1ccaecc9":"code","2b5bbbea":"code","459c3b74":"code","71d47c18":"code","fcd042bd":"code","cba4e6df":"code","aff37fd6":"code","9976e18a":"code","b24e4844":"code","594f0fba":"code","6a74ac79":"code","54a3a701":"code","b9afbed5":"code","a4409c18":"code","c251fa48":"code","f264ce0b":"code","f268b120":"code","03b9b3c9":"markdown","3506b0a9":"markdown","2e8ec6a2":"markdown","7132a203":"markdown","7ba67a0a":"markdown","3f550e0d":"markdown","de341801":"markdown","53fa3233":"markdown","82a2a553":"markdown","3c665723":"markdown","9d1339e3":"markdown","df1aa33e":"markdown","faceb399":"markdown","08a8d78d":"markdown","d0f79b8c":"markdown","4da733cd":"markdown","27c73a3d":"markdown","de9b08e4":"markdown","d250d034":"markdown","678e3855":"markdown","9f1387e3":"markdown","2d596866":"markdown","4bb071e6":"markdown","c9d31915":"markdown","9a8519f4":"markdown","e76c7bfa":"markdown","b44fb0ae":"markdown","c2368d57":"markdown","e969f3b3":"markdown","23fb3e50":"markdown","178e38ed":"markdown","3e730133":"markdown","30985d5e":"markdown","42ad12e1":"markdown","5b62e1fd":"markdown","643a1810":"markdown","55b9c7f7":"markdown","77cfc61b":"markdown","e96a1987":"markdown","a94a39ad":"markdown","7c236939":"markdown","706a85be":"markdown","6ee92711":"markdown","46eca1d9":"markdown","3249f41d":"markdown","cfe2223c":"markdown","98bddc9d":"markdown","c8ca3564":"markdown","0e4eb5ff":"markdown","c8e60eef":"markdown","7bbaf4b4":"markdown","fefddc73":"markdown"},"source":{"0e1bb1d0":"# importing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(action = 'ignore')\n\n#importing data\ndata = pd.read_csv('..\/input\/banking-churn-prediction\/Banking_churn_prediction.csv')\n\n# converting churn\/brnch_code\/customer_nw_category to category type\ndata['churn'] = data['churn'].astype('category')\ndata['branch_code'] = data['branch_code'].astype('category')\ndata['customer_nw_category'] = data['customer_nw_category'].astype('category')\ndata.dtypes[data.dtypes == 'int64']\n\n# converting \"dependents\" and \"city\" to their respective types\ndata['dependents'] = data['dependents'].astype('Int64')\ndata['city'] = data['city'].astype('category')\n\n# typecasting \"gender\" and \"occupation\" to category type\ndata['gender'] = data['gender'].astype('category')\ndata['occupation'] = data['occupation'].astype('category')\n\n# creating an instance(date) of DatetimeIndex class using \"last_transaction\"\ndate = pd.DatetimeIndex(data['last_transaction'])\n\n##### extracting new columns from \"last_transaction\"\n\n# last day of year when transaction was done\ndata['doy_ls_tran'] = date.dayofyear\n\n# week of year when last transaction was done\ndata['woy_ls_tran'] = date.weekofyear\n\n# month of year when last transaction was done\ndata['moy_ls_tran'] = date.month\n\n# day of week when last transaction was done\ndata['dow_ls_tran'] = date.dayofweek\n\n# Removing the original datetime column\ndata = data.drop(columns = ['last_transaction'])\n\n\n#dropping customer_id\ndata = data.drop(columns=['customer_id'])\n\n#checking\ndata.dtypes","3b3431bd":"# seggregating variables into groups\ncustomer_details = ['customer_id','age','vintage']\ncurrent_month = ['current_balance','current_month_credit','current_month_debit','current_month_balance']\nprevious_month = ['previous_month_end_balance','previous_month_credit','previous_month_debit','previous_month_balance']\nprevious_quarters = ['average_monthly_balance_prevQ','average_monthly_balance_prevQ2']\ntransaction_date = ['doy_ls_tran','woy_ls_tran','moy_ls_tran','dow_ls_tran']","49610246":"# isolating numerical datatypes\nnumerical = data.select_dtypes(include=['int64','float64','Int64'])[:]\nnumerical.dtypes","20c5514a":"# calculating correlation\ncorrelation = numerical.dropna().corr()\ncorrelation","7d503eb8":"# plotting heatmap usingl all methods for all numerical variables (peason, kendall, spearman)\nplt.figure(figsize=(36,6), dpi=140)\nfor j,i in enumerate(['pearson','kendall','spearman']):\n  plt.subplot(1,3,j+1)\n  correlation = numerical.dropna().corr(method=i)\n  sns.heatmap(correlation, linewidth = 2)\n  plt.title(i, fontsize=18)\n\n","44107bb5":"# extracting transaction information of current and previous months\nvar = []\nvar.extend(previous_month)\nvar.extend(current_month)\nvar.extend(previous_quarters)","3d48f1ec":"# plotting heatmap usill all methods for all transaction variables\nplt.figure(figsize=(36,6), dpi=140)\nfor j,i in enumerate(['pearson','kendall','spearman']):\n  plt.subplot(1,3,j+1)\n  correlation = numerical[var].dropna().corr(method=i)\n  sns.heatmap(correlation, linewidth = 2)\n  plt.title(i, fontsize=18)","25bd2786":"# Grouping variables\ntransactions = ['current_month_credit','current_month_debit','previous_month_credit','previous_month_debit']\nbalance = ['previous_month_end_balance','previous_month_balance','current_balance','current_month_balance']","36569dc0":"# scatter plot for transactional variables\nplt.figure(dpi=140)\nsns.pairplot(numerical[transactions])\nplt.show()","d565e80f":"#taking log of every value to negate outliers\nfor column in var:\n  mini=1\n  if numerical[column].min()<0:\n    mini =  abs(numerical[column].min()) + 1\n  \n  numerical[column] = [i+mini for i in numerical[column]]\n  numerical[column] = numerical[column].map(lambda x : np.log(x))","9e8c9ae5":"# scatter plot for transactional variables\nplt.figure(dpi=140)\nsns.pairplot(numerical[transactions])\nplt.show()","6c29f752":"# balance variables\nplt.figure(dpi=140, figsize = (20,20))\nsns.pairplot(numerical[balance])\nplt.show()","fc3d932b":"# previous quarters\nplt.figure(dpi=140)\nsns.scatterplot(numerical['average_monthly_balance_prevQ'], numerical['average_monthly_balance_prevQ2'])\nplt.show()","f3a03221":"def TwoSampZ(X1, X2, sigma1, sigma2, N1, N2):\n  '''\n  takes mean, standard deviation, and number of observations and returns p-value calculated for 2-sampled Z-Test\n  '''\n  from numpy import sqrt, abs, round\n  from scipy.stats import norm\n  ovr_sigma = sqrt(sigma1**2\/N1 + sigma2**2\/N2)\n  z = (X1 - X2)\/ovr_sigma\n  pval = 2*(1 - norm.cdf(abs(z)))\n  return pval","ba505a3a":"def TwoSampT(X1, X2, sd1, sd2, n1, n2):\n  '''\n  takes mean, standard deviation, and number of observations and returns p-value calculated for 2-sample T-Test\n  '''\n  from numpy import sqrt, abs, round\n  from scipy.stats import t as t_dist\n  ovr_sd = sqrt(sd1**2\/n1 + sd2**2\/n2)\n  t = (X1 - X2)\/ovr_sd\n  df = n1+n2-2\n  pval = 2*(1 - t_dist.cdf(abs(t),df))\n  return pval","14705a06":"def Bivariate_cont_cat(data, cont, cat, category):\n  #creating 2 samples\n  x1 = data[cont][data[cat]==category][:]\n  x2 = data[cont][~(data[cat]==category)][:]\n  \n  #calculating descriptives\n  n1, n2 = x1.shape[0], x2.shape[0]\n  m1, m2 = x1.mean(), x2.mean()\n  std1, std2 = x1.std(), x2.mean()\n  \n  #calculating p-values\n  t_p_val = TwoSampT(m1, m2, std1, std2, n1, n2)\n  z_p_val = TwoSampZ(m1, m2, std1, std2, n1, n2)\n\n  #table\n  table = pd.pivot_table(data=data, values=cont, columns=cat, aggfunc = np.mean)\n\n  #plotting\n  plt.figure(figsize = (20,4), dpi=140)\n  \n  #barplot\n  plt.subplot(1,3,1)\n  sns.barplot([str(category),'not {}'.format(category)], [m1, m2])\n  plt.ylabel('mean {}'.format(cont))\n  plt.xlabel(cat)\n  plt.title('t-test p-value = {} \\n z-test p-value = {}\\n {}'.format(t_p_val,\n                                                                z_p_val,\n                                                                table))\n\n  # category-wise distribution\n  plt.subplot(1,3,2)\n  sns.kdeplot(x1, shade= True, color='blue', label = 'churned')\n  sns.kdeplot(x2, shade= False, color='green', label = 'not churned', linewidth = 1)\n  plt.title('categorical distribution')\n    \n  # boxplot\n  plt.subplot(1,3,3)\n  sns.boxplot(x=cat, y=cont, data=data)\n  plt.title('categorical boxplot')","957b1f7f":"Bivariate_cont_cat(data, 'vintage', 'churn', 1)","1697292e":"Bivariate_cont_cat(data, 'average_monthly_balance_prevQ', 'churn', 1)","4f657a9b":"Bivariate_cont_cat(data, 'average_monthly_balance_prevQ2', 'churn', 1)","c6915e2e":"Bivariate_cont_cat(data, 'previous_month_balance', 'churn', 1)","595aa266":"Bivariate_cont_cat(data, 'current_month_balance', 'churn', 1)","9538b7e7":"# Extracting drop of balance in previous and current month\ndifference = data[['churn','previous_month_balance','current_month_balance']][:]\ndifference['bal_diff'] = difference['current_month_balance']-difference['previous_month_balance']","99b2e915":"Bivariate_cont_cat(difference, 'bal_diff', 'churn', 1)","1ccaecc9":"def BVA_categorical_plot(data, tar, cat):\n  '''\n  take data and two categorical variables,\n  calculates the chi2 significance between the two variables \n  and prints the result with countplot & CrossTab\n  '''\n  #isolating the variables\n  data = data[[cat,tar]][:]\n\n  #forming a crosstab\n  table = pd.crosstab(data[tar],data[cat],)\n  f_obs = np.array([table.iloc[0][:].values,\n                    table.iloc[1][:].values])\n\n  #performing chi2 test\n  from scipy.stats import chi2_contingency\n  chi, p, dof, expected = chi2_contingency(f_obs)\n  \n  #checking whether results are significant\n  if p<0.05:\n    sig = True\n  else:\n    sig = False\n\n  #plotting grouped plot\n  sns.countplot(x=cat, hue=tar, data=data)\n  plt.title(\"p-value = {}\\n difference significant? = {}\\n\".format(round(p,8),sig))\n\n  #plotting percent stacked bar plot\n  #sns.catplot(ax, kind='stacked')\n  ax1 = data.groupby(cat)[tar].value_counts(normalize=True).unstack()\n  ax1.plot(kind='bar', stacked='True',title=str(ax1))\n  int_level = data[cat].value_counts()","2b5bbbea":"BVA_categorical_plot(data, 'churn', 'gender')","459c3b74":"# segregating customers into segments\nchurn = data[['churn','age']][:]\nchurn['age_group'] = 'str'\nchurn['age_group'][churn['age']>=80] = 'very old'\nchurn['age_group'][(churn['age']<80) & (churn['age']>=60)] = 'senior citizen'\nchurn['age_group'][(churn['age']<60) & (churn['age']>=18)] = 'adult'\nchurn['age_group'][churn['age']<18] = 'young'","71d47c18":"BVA_categorical_plot(churn, 'churn', 'age_group')","fcd042bd":"BVA_categorical_plot(data, 'churn', 'customer_nw_category')","cba4e6df":"# segregating dependents into categories\ndependents = data[['churn','dependents']][:]\ndependents.dropna(inplace=True)\ndependents['dep_group'] = None\ndependents['dep_group'][dependents['dependents']==0] = 'single'\ndependents['dep_group'][(dependents['dependents']>=1) & (dependents['dependents']<=3)] = 'small family'\ndependents['dep_group'][(dependents['dependents']>=4) & (dependents['dependents']<=9)] = 'large family'\ndependents['dep_group'][(dependents['dependents']>=10)] = 'joint family'","aff37fd6":"BVA_categorical_plot(dependents, 'churn', 'dep_group')","9976e18a":"# getting city codes which have less than 280 (1%) of accounts\ntmp = data['city'].value_counts()[:]\ncities = tmp[tmp<280].index\n\nchurn_acc = data[['churn','city']][:]\nchurn_acc['city_cat'] = None\nchurn_acc['city_cat'][churn_acc['city'].isin(cities[:])] = 'low accounts'\nchurn_acc['city_cat'][~churn_acc['city'].isin(cities[:])] = 'high accounts'","b24e4844":"BVA_categorical_plot(churn_acc, 'churn', 'city_cat')","594f0fba":"# getting branch codes with more than 0.5% of total accounts\ntmp = data['branch_code'].value_counts()[:]\nbranch = tmp[tmp<140].index\n\n# making two segments\nchurn_acc = data[['churn','branch_code']][:]\nchurn_acc['branch_cat'] = None\nchurn_acc['branch_cat'][churn_acc['branch_code'].isin(branch[:])] = 'low accounts'\nchurn_acc['branch_cat'][~churn_acc['branch_code'].isin(branch[:])] = 'high accounts'","6a74ac79":"BVA_categorical_plot(churn_acc, 'churn', 'branch_cat')","54a3a701":"# isolating rows with missing gender\nmiss_gender = data[:]\nmiss_gender['missing_gender'] = 'not_missing'\nmiss_gender['missing_gender'][~miss_gender['gender'].isin(['Male','Female'])] = 'missing value'","b9afbed5":"BVA_categorical_plot(miss_gender, 'churn', 'missing_gender')","a4409c18":"# isolating rows with missing gender\nmiss_dependents = data[:]\nmiss_dependents['missing_dependents'] = 'not_missing'\nmiss_dependents['missing_dependents'][~miss_dependents['dependents'].isin([0, 2, 3, 1, 7, 4,\n                                                                           6, 5, 9, 52, 36, 50,\n                                                                           8, 25, 32])] = 'missing value'","c251fa48":"BVA_categorical_plot(miss_dependents, 'churn', 'missing_dependents')","f264ce0b":"# isolating rows with missing gender\nmiss_occupation = data[:]\nmiss_occupation['missing_occupation'] = 'not_missing'\nmiss_occupation['missing_occupation'][~miss_occupation['occupation'].isin(['self_employed',\n                                                                           'salaried',\n                                                                           'retired',\n                                                                           'student',\n                                                                           'company'])] = 'missing value'","f268b120":"BVA_categorical_plot(miss_occupation, 'churn', 'missing_occupation')","03b9b3c9":"**Result:**\n\nNumber of dependents also play significant role in churning.","3506b0a9":"**Missing values in occupation does not have any significantly different relation with churn rate.**","2e8ec6a2":"## 2. Recapping generated hypothesis:\nDuring the univariate anlysis we saw that hypothesis testing was not possible because hypothesis testing primarily deals with the combination of some independent variable with the target variable. As we are not diving into bivariate analysis, **we will be performing hypothesis testing extensively.**\n\n\nGiven below are the hypothesis we will be working with in this EDA\n\n**On basis of Demographics**\n1. Are females less likely to churn than males?\n2. Are young customers more likely to churn?\n3. Are customers in the lower income bracket more likely to churn?\n4. Are customers with dependent(s) less likely to churn?\n5. Customers with an average family size less than 4 are more likely to churn?\n\n**On the basis of customer behaviour**\n1. Are vintage customers less likely to churn?\n2. Are customers with higher average balance less likely to churn?\n3. Are customers dropping monthly balance highly likely to churn?\n4. Are customers with no transaction is the last 3 months more likely to churn?\n5. Are customers who have large withdrawal amounts in the last month more likely to churn?\n6. Are customers who have large withdrawal amounts in the last quarter more likely to churn?\n7. Customers who have not engaged with the bank in the last quarter are more likely to churn?","7132a203":"### 6.2 Are customers with higher average balance less likely to churn?","7ba67a0a":"#### previous month\/current month","3f550e0d":"**Inferences**\n1.    This validates the high correlation between the two previous quarters\n2.    This high correlation can be used for feature engineering during the later stages.\n\n\n**Key Insight**\n\nWe can generate dozens of new features from these highly correlated variables during the feature engineering phase, which should be able to explain the presence of outliers and may contribute to better model performance.","de341801":"**Result**\nthe missing values in gender have significantly higher churn rate.\n\nWe can make a new feature for this","53fa3233":"#### 3. Customers from low income bracket more likely to churn","82a2a553":"## 6. Bivariate Analysis: Continuous-Categorical u to plot the categorical mean and the categorical distribution.\nMoreover, in this section we will working with hypothesis testing. I you need a quick refresher on hypothesis testing and how p-value works, just follow along the this article.\n\nhttps:\/\/medium.com\/analytics-vihttps:\/\/medium.com\/analytics-vidhya\/everything-you-should-know-about-p-value-from-scratch-for-data-science-f3c0bfa3c4cc","3c665723":"**Inferences**\n1.    This validates the high correlation between the transaction variables.\n2.    This high correlation can be used for feature engineering during the later stages.","9d1339e3":"### Heatmap\nHeatmap will allow us to visually figure out the key correlation between variables and filter the down the essential variables so that we will have lesss to deal with during the scatter plots.\n\nIn order to have different perspectives on the correlation of the independent variables, we will be plotting the heatmaps using three methods of calculating the correlation.\n\n1. Pearson Correlation\n2. Kendal's Tau\n3. Spearman Correlation","df1aa33e":"**Inferences**\n1.    This validates the high correlation between the balance variables.\n2.    This high correlation can be used for feature engineering during the later stages.","faceb399":"**Inferences**\n\nWe can see that people who churned actually had significantly higher balance during their previous two quarters.**This validates the previous plot conveying the message that people whoc hurned actually had higher balance.\n**","08a8d78d":"#### Missing Values : Dependents\nWhether the missing values in dependents have some common behavior among themselves.","d0f79b8c":"**Inferences**\n\n> Customers who churned had significantly high balance throughout the previous two quarters and previous month. But their average balance reduced significantly in the current month. Moreover the customers who are maintaining higher balance are more prone to churning (so it seems)\n","4da733cd":"#### 2. Are young customers more likely to churn?\nFor this I will be making 4 segments:\n1. young\n2. adult\n3. senior citizen\n4. very old","27c73a3d":"**Result**    \n\np-value < 0.05, the the two samples are significantly different.\n\nCustomers who churned have significantly higher balance during immediate preceeding quarter, which is contrary to what we were were testing but the result is significant.","de9b08e4":"## Bivariate: Categorical Categorical\nIn this section we will be working with the categorical categorical combination of variables. **Grouped bar plot and stacked bar plots are the 2 key ways to visualise them.** Also we will be performing the the hypothesis testing using chi-square.","d250d034":"**Result**\n\ncities having less than 1 percent of the total have significantly lower churn rates as compared to the cities with more accounts. This is contrary to what we assumed.\n\nthis information can be used to generate new feature.","678e3855":"#### 7. Possibility that cities and branch code with very few accounts may lead to churning.","9f1387e3":"### Missing values : Occupation","2d596866":"#### 1. Are females less likely to churn than males?","4bb071e6":"## Overview:\n\n1. Recapping the problem statement\n2. Recapping generated hypothesis\n3. Recapping investigation leads from univariate analysis\n4. Importing dataset + variabe typecasting\n5. Bivariate Analysis : Numerical Numerical\n6. Bivariate Analysis : Numerical Categorical\n7. Bivariate Analysis : Categorical Categorical\n8. Summary of Bivariate analysis  ","c9d31915":"**the scatter plot is is not meaningful due to the presence of outliers**\nOne way to visualise them is to take logarithm transform of every variable is to nullify the effect of outliers.","9a8519f4":"**Result**:\nAge group has significant effect on the churning rate. Each group has a significantly different churning rate with respect to the expected churning rate (20\/80 approx)\n\n**This info can be used to generate new features**","e76c7bfa":"This is a a very absurd plot to gain insight from, so what actually happening here is that.... \n* the customers who churned have a negative average balance differenceand that too is a huge number.\n* Whereas the customers who did not churn slighly positive balance difference betwwen the previous month and the current month.","b44fb0ae":"* Kendall and Spearman correlation seem to have very similar pattern between them, except the slight variation in magnitude of correlation.\n*  Too many variables with insignificant correlation.\n*  Major correlation lies between the transaction variables and balance variables.\n\n**As the there are are many variables with insignificant correlation, let's filter down to the most important ones.**","c2368d57":"## 1. Recapping problem statement:\nA Bank wants to take care of customer retention for their product; savings accounts. The bank wants you to identify customers likely to churn balances below the minimum balance. You have the customers information such as age, gender, demographics along with their transactions with the bank. Your task as a data scientist would be to predict the propensity to churn for each customer.","e969f3b3":"**Result**\n\nHere we see that the branches with low number of accounts do not have any significant difference from the branches with higher number of accounts.","23fb3e50":"### 6.1 Are vintage customers less likely to churn?\n\n","178e38ed":"**Inferences**\n\n1.    Vintage customers churned more, but results are not significantly different\n2.    Boxplot shows very similar distribution with outliers on the lower end.\n\n**Result**\n\np-value is >0.05, which means that the two samples are more or less similar to each other.\n\nThefore, we can safely reject the hypothesis that vintage customers are more likely to churn.","3e730133":"## 4. Importing libraries + Datset + Variable identification and typecasting.","30985d5e":"**Inferences:**\n\n\n1.   Transaction variables like credit\/debit have a strong correlation among themselves.\n2.  Balance variables have strong correlation among themselves.\n3.   Transaction variables like credit\/debit have insignificant or no correlation with the Balance variables.\n\n","42ad12e1":"### Correlation Matrix\nA straight forward goto method is to print the correlation matrix.","5b62e1fd":"List of Hypothesis and investigation to perform under this combination.\n\n1.  Are vintage customers less likely to churn?for large number of observations.\n2.  Are customers with higher average balance less likely to churn?\n3.  Are customers dropping monthly balance highly likely to churn?\n\nWe will be performing the hypothesis testing as we go along plotting the graphs. This will save a lot of time in the long run. For this we will be making three functions.\n1. Function for 2sample Z-Test\n2. Function for 2 sample T-Test\n3. Function for plotting which uses the above mentioned two functions.\n\nNote that I am using both Z-test and T-test here to quantify that they perform similarly.","643a1810":"#### City : Isolating cities with less than 1% of total customers","55b9c7f7":"**There is no diffrent behaviour of the missing values in gender wrt churn variable. Or in other words, male and female customers are equally likely to churn.**\n","77cfc61b":"### 6.3 Are customers dropping monthly balance highly likely to churn?","e96a1987":"## 5. Bivariate Analysis : Numerical-Numerical\nIn this section we will be performing bivariate analysis for the Numerical Numerical combination of variables.\n\n**Although we do not have have any hypothesis which falls under this combination of variables, but we will still perform the numerical numerical bivariate analysis and relation between the independent variables can be used during the preprocessing and feature engineering.**","a94a39ad":"**As number of variables are too large, correlation matrix is not much help.**","7c236939":"**Result:**\n\nDifferent income brackets have significant effect on the churn rate.\n\n**This information can be used for feature engineering**","706a85be":"## Objective: To demonstrate structured format of Performing Exploratory data Analysis.\n\nNote that this notebook only contains Bivariate analysis. the preceeding Univaiate analysis can be found in a separate notebook, I highly recommend that you go through that notebook first.\n\n\nhttps:\/\/www.kaggle.com\/lonewolf95\/eda-101-structured-univariate-analysis\n","6ee92711":"## Summary of Bivariare Analysis:\n\n### numerical numerical\n1. Transactional variables, balance variables, previous quarter variables have strong correlations among themselves. This can be used to generate a bunch of meaningful new features.\n\n### Numerical Categorical\n1. Customers who are mantaining a higher balance in their accounts are actually more susceptible to churning.\n2. The customers who churned drastically dropped their balance in the most recent month. (which validates the definition of churn).\n\n### Categorical categorical\n1. Rate of churning among the different age groups vary significantly.\n2. Different age brackets also affect the rate of churning.\n3. Number of dependents also affect the churning rate significantly.\n\n### Things to investigate further in Multivariate Analysis:\n* Whether there are any independent (or combination of them) variables which can explain missing values or outliers. (this will govern the preprocessing step)","46eca1d9":"### Scatterplot\n**Now that we have a bird's eye view of the correlations, let's look over them closely with the help of scatter plots.**","3249f41d":"#### List of Hypothesis to check under this combination\n1.   Are females less likely to churn than males?\n2.   Are young customers more likely to churn?\n3.   Are customers in the lower income bracket more likely to churn?\n4.   Are customers with dependent(s) less likely to churn?\n5.   Customers with an average family size less than 4 are more likely to churn?\n6.   Customers whose last transaction was more than 6 months ago, do they have higher churn rate?\n7.   Possibility that cities and branch code with very few accounts may lead to churning.\n\n**Missing Values** - finding behaviour\n\n**Gender**: \n  *  Do missing values churn more?\n\n**Dependents**:\n  *  Do missing values have any relation with churn?\n\n**Occupation:**\n   * Do they have some relation with churn?","cfe2223c":"**Result:**\n\nthe difference between the males and females customer churning is significant. Males churn significantly more than females.\n\n**this info can be used to generate new feature.**\n","98bddc9d":"## 3. Investigation leads from Univariate analysis:\n\n1. Is there there any common trait\/relation between the customers who are performing high transaction credit\/debits?\n    * customer_nw_category might explain that.\n    * Occupation = Company might explain them\n    * popular cities might explain this\n2. Customers whose last transaction was 6 months ago, did all of them churn?\n3. Possibility that cities and branch code with very few accounts may lead to churning.\n\n","c8ca3564":"The succeeding notebook addressing the multivariate analysis and other miscellaneous analysis can be found at...\n\n<Link>","0e4eb5ff":"#### Missing Values : Gender\nTo check whether the missing values in gender have some common behaviour among themselves.","c8e60eef":"**Inference**\n\nCustomers who churned had a very high drop in their balance which is signified by the negative value in this bar plot.\n\n**This factor can be used generate a new feature.**","7bbaf4b4":"#### 4,5. Are customers with dependent(s) less likely to churn?\nFor this I am making 4 segments:\n1. single\n2. small family\n3. large family\n6. joint family","fefddc73":"#### Branch code: Isolating branches with less than 0.5%of accounts"}}