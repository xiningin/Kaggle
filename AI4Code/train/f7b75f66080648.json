{"cell_type":{"2e6ed736":"code","e80de9af":"code","62b08ed8":"code","8480b522":"code","19c69623":"code","3d3c03e0":"code","c1737ff3":"code","3bb17e0e":"code","737551e5":"code","f357d248":"code","f047f414":"code","6643c3e4":"code","1263ffb6":"code","7ddb0e4a":"code","d7ae5e47":"code","aa5915df":"code","ff43ebf7":"code","96419c3e":"code","819e3e2c":"code","100af0b0":"code","ab4b0304":"code","7303186c":"code","843172b1":"code","b59af22f":"code","e4ee57cf":"code","b2a166bb":"code","72bd64d4":"code","50b5db09":"code","9ecdbc0f":"code","ed8db49e":"code","d2b28125":"code","eb2fd4ac":"code","41886c8d":"code","732bfbe1":"code","3f53eb2e":"code","87a6b88f":"code","0ea31fd1":"code","38bb196f":"code","3c823cc3":"code","abe3c399":"code","5cb9a9cb":"code","22f09cbe":"code","b06e075d":"code","addfbf5e":"code","20de2c74":"code","6a2868ba":"code","55da3bcb":"code","8e5dae4f":"code","61bd2bac":"code","832a9bdd":"code","c580adca":"code","94a121a9":"code","b7835832":"code","bbb6f485":"code","c637d8ea":"code","79552e37":"code","04f5678c":"code","9202e798":"code","367fe3c0":"code","896a0eca":"code","ab8c9021":"code","b7ee818e":"code","ec695ca3":"code","bf66ce33":"code","e677b2ce":"code","d8f0c3a9":"code","2e42a0d8":"code","b951321a":"code","beac6d01":"code","733a2dd6":"code","ec7931f7":"code","b61460c6":"code","d25987da":"code","82d5bf71":"code","96ef9051":"code","f88a2fd1":"code","7a812784":"code","7714c089":"code","5a869bfc":"code","78fb2d59":"code","c0e89dea":"code","41c631da":"code","439cbdd7":"code","0f36af67":"code","c7855cc7":"code","6b1a9994":"code","c0840c2a":"code","39a1a53b":"code","da14376b":"code","60b417da":"code","bc402885":"code","75a86342":"code","f9ec7051":"code","c873d0c1":"code","7b40dde0":"code","0060ee0f":"code","9c22b555":"code","373a3b37":"code","47fc9488":"code","ff23cc9a":"markdown","f266abb5":"markdown","33b9fba3":"markdown","07527338":"markdown","2281207e":"markdown","033ecf6f":"markdown","e31a4322":"markdown","2a6797e3":"markdown","25540f4d":"markdown","6e4c643f":"markdown","a5736375":"markdown","54fc5e74":"markdown","7a9a2d61":"markdown","cbad2ce0":"markdown","df8c3ec5":"markdown"},"source":{"2e6ed736":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","e80de9af":"import nltk","62b08ed8":"import os\nimport re\nimport pickle\nfrom tqdm import tqdm\nfrom itertools import product","8480b522":"from sklearn.linear_model import(LinearRegression, SGDRegressor)","19c69623":"import xgboost as xgb","3d3c03e0":"#!python3.7 -m pip install --upgrade pip\n!pip install pymystem3","c1737ff3":"import nltk\nnltk.download('stopwords')\n\nfrom nltk.corpus import stopwords\nfrom pymystem3 import Mystem\nfrom string import punctuation","3bb17e0e":"from sklearn.preprocessing import LabelEncoder","737551e5":"from sklearn.feature_extraction.text import (\n    CountVectorizer, HashingVectorizer, TfidfVectorizer)","f357d248":"from scipy.stats.stats import pearsonr","f047f414":"os.listdir('..\/input\/competitive-data-science-predict-future-sales')","6643c3e4":"root = '..\/input\/competitive-data-science-predict-future-sales'","1263ffb6":"df_train = pd.read_csv(f'{root}\/sales_train.csv')\ndf_items = pd.read_csv(f'{root}\/items.csv')\ndf_cat = pd.read_csv(f'{root}\/item_categories.csv')\ndf_shops = pd.read_csv(f'{root}\/shops.csv')\ndf_test = pd.read_csv(f'{root}\/test.csv')\nsample_submit = pd.read_csv(f'{root}\/sample_submission.csv')","7ddb0e4a":"df_test.head()","d7ae5e47":"df_train.head()","aa5915df":"df_train.info()","ff43ebf7":"# checking and removing NaN values - **No Nan values found**\nsns.heatmap(df_train.isna())","96419c3e":"# item_price can't be negative or zer0\nprint(df_train['item_price'][df_train['item_price']<=0])\n# removing negative priced items\ndf_train = df_train[df_train['item_price']>0]","819e3e2c":"# item_cnt_day can't be negative or floating - assuming the items were refunded\n# we will set the negative values to zer0\nneg_items_sold = df_train['item_cnt_day'][df_train['item_cnt_day']<0].shape[0]\nprint('Total no. negative values:', neg_items_sold)\nprint('Percentage of negative values', neg_items_sold*100\/df_train.shape[0])\ndf_train['item_cnt_day'] = df_train['item_cnt_day'].apply(lambda x: 0 if x<0 else x)\ndf_train['item_cnt_day'] = df_train['item_cnt_day'].apply(lambda x: np.round(x))","100af0b0":"df_train['item_cnt_day'].nunique()","ab4b0304":"print('Min val:', df_train['item_price'].min())\nprint('Max val:', df_train['item_price'].max())","7303186c":"print('Min val:', df_train['item_cnt_day'].min())\nprint('Max val:', df_train['item_cnt_day'].max())","843172b1":"fig, axs = plt.subplots(ncols=3, figsize=(20, 6))\nsns.distplot(df_train['item_price'], hist=False, ax=axs[0])\nsns.boxplot(df_train['item_price'], ax=axs[1])\nsns.distplot(df_train['item_price'][df_train['item_price']<50000], ax=axs[2])","b59af22f":"# removing all values greater than 100,000 from `item_price` column\ndf_train = df_train[df_train['item_price']<100000]","e4ee57cf":"df_train_datewise = df_train.copy()\ndf_train_datewise['date'] =  pd.to_datetime(df_train_datewise['date'])\ndf_train_datewise.set_index('date', inplace=True)","b2a166bb":"df_train.drop(['date_block_num','shop_id'], axis=1).groupby(['item_id']).sum().sort_values(by='item_cnt_day', ascending=False)[:20].plot(kind='bar')\nplt.title('top 20 items sold (sum)')\nplt.ylabel('#no of items sold')","72bd64d4":"df_train.drop(['date_block_num','item_id'], axis=1).groupby(['shop_id']).mean().sort_values(by='item_cnt_day', ascending=False)[:20].plot(kind='bar')\nplt.title('top 20 shops with the highest sale (mean)')\nplt.ylabel('#no of items sold')","50b5db09":"monthly_sum = df_train_datewise.resample('M').sum()\nmonthly_sum['month'] = monthly_sum.index.month\n\nmonthly_mean = df_train_datewise.resample('M').mean()\nmonthly_mean['month'] = monthly_mean.index.month","9ecdbc0f":"f, axes = plt.subplots(2, 1, figsize=(22, 10), sharex=True)\nsns.lineplot(x=\"month\", y=\"item_cnt_day\", data=monthly_mean, ax=axes[0]).set_title(\"Monthly mean\")\nsns.lineplot(x=\"month\", y=\"item_cnt_day\", data=monthly_sum, ax=axes[1]).set_title(\"Monthly sum\")\nplt.show()","ed8db49e":"df_items.head()","d2b28125":"df_items['item_name']","eb2fd4ac":"# creating lemmatizer and stopwords list\nmystem = Mystem()\nrussian_sw = stopwords.words(\"russian\")","41886c8d":"# preprocessing russian\ndef pre_pro_text(text):\n    \n    # lemmatizing\n    tokens = mystem.lemmatize(text.lower())\n    \n    # removing sw and punctuations\n    tokens = [token for token in tokens if \\\n                token not in russian_sw \\\n                and token!=\" \" \\\n                and token.strip() not in punctuation]\n    text = \" \".join(tokens)\n    \n    # removing single letters\n    pattern = r\"(((?<=^)|(?<= )).((?=$)|(?= )))|[*&^%@#$\\(\\)+]\"\n    text = re.sub(\"\\s+\", \" \", re.sub(pattern, '', text).strip())\n    return text","732bfbe1":"df_items['item_name'] = df_items['item_name'].apply(pre_pro_text)","3f53eb2e":"df_items['item_name'].head(10)","87a6b88f":"def create_text_features(df, col='item_name'):\n    vectorizers = [ \n                ('cvec', CountVectorizer(analyzer='char_wb',\n                                         ngram_range=(1, 2))),\n                ('hvec', HashingVectorizer()),\n                ('tfidf',TfidfVectorizer(ngram_range=(1,2)))\n            ]\n    text_features = {}\n    for vec, vec_f in vectorizers:\n        text_features[vec] = vec_f.fit_transform(df[col])\n    \n    return text_features","0ea31fd1":"df_shops.head(10)","38bb196f":"df_shops['city'] = df_shops['shop_name'].apply(lambda x: x.split()[0]).apply(pre_pro_text)","3c823cc3":"def sentence_corr_heat_map(documents, threshold=0.7):\n    all_uni_words = list(set(' '.join(documents).split(' ')))\n    word_to_num = {w:i for i, w in enumerate(all_uni_words)}\n\n    sim_mat = np.zeros((shop_names_t.shape[0], len(all_uni_words)))\n\n    for i, sentence in enumerate(documents):\n        for word in sentence.split():\n            sim_mat[i, word_to_num[word]] = 1\n\n    corr_mat = np.zeros((sim_mat.shape[0], sim_mat.shape[0]))\n    for i in range(sim_mat.shape[0]):\n        for j in range(sim_mat.shape[0]):\n            corr_mat[i, j] = pearsonr(sim_mat[i], sim_mat[j])[0]\n    \n    plt.figure(figsize=(10, 8))\n    sns.heatmap((corr_mat>threshold)*corr_mat, linewidths=0.1)","abe3c399":"shop_names_t = df_shops['shop_name'].apply(pre_pro_text).values\nsentence_corr_heat_map(shop_names_t)","5cb9a9cb":"df_shops['shop_name'][10:12]","22f09cbe":"df_shops['shop_name'][23:25]","b06e075d":"df_shops['shop_name'].loc[[0,57]]","addfbf5e":"df_shops['shop_name'].loc[[1,58]]","20de2c74":"df_shops['shop_name'].loc[[39, 40]]","6a2868ba":"#10    \u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c?\n#11    \u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c\u00b2\ndf_train.loc[df_train.shop_id == 10, 'shop_id'] = 11\ndf_test.loc[df_test.shop_id == 10, 'shop_id'] = 11\n#23    \u041c\u043e\u0441\u043a\u0432\u0430 \u0422\u041a \"\u0411\u0443\u0434\u0435\u043d\u043e\u0432\u0441\u043a\u0438\u0439\" (\u043f\u0430\u0432.\u04102)\n#24    \u041c\u043e\u0441\u043a\u0432\u0430 \u0422\u041a \"\u0411\u0443\u0434\u0435\u043d\u043e\u0432\u0441\u043a\u0438\u0439\" (\u043f\u0430\u0432.\u041a7)\ndf_train.loc[df_train.shop_id == 23, 'shop_id'] = 25\ndf_test.loc[df_test.shop_id == 23, 'shop_id'] = 25\n#0     !\u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56 \u0444\u0440\u0430\u043d\n#57          \u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56\ndf_train.loc[df_train.shop_id == 0, 'shop_id'] = 57\ndf_test.loc[df_test.shop_id == 0, 'shop_id'] = 57\n#39              \u0420\u043e\u0441\u0442\u043e\u0432\u041d\u0430\u0414\u043e\u043d\u0443 \u0422\u0420\u041a \"\u041c\u0435\u0433\u0430\u0446\u0435\u043d\u0442\u0440 \u0413\u043e\u0440\u0438\u0437\u043e\u043d\u0442\"\n#40    \u0420\u043e\u0441\u0442\u043e\u0432\u041d\u0430\u0414\u043e\u043d\u0443 \u0422\u0420\u041a \"\u041c\u0435\u0433\u0430\u0446\u0435\u043d\u0442\u0440 \u0413\u043e\u0440\u0438\u0437\u043e\u043d\u0442\" \u041e\u0441\u0442\u0440\u043e\u0432\u043d\u043e\u0439\ndf_train.loc[df_train.shop_id == 39, 'shop_id'] = 40\ndf_test.loc[df_test.shop_id == 39, 'shop_id'] = 40","55da3bcb":"# fixing shop name\ndf_shops.loc[df_shops.shop_name == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"', 'shop_name'] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'","8e5dae4f":"df_shops['city'].value_counts().plot(kind='bar', figsize=(15, 7))\nplt.title('Shops per City')\nplt.xlabel('Cities')\nplt.ylabel('Number of Shops')","61bd2bac":"# encoding cities\nenc_city = LabelEncoder()\ndf_shops['city_encoded'] = enc_city.fit_transform(df_shops['city'])","832a9bdd":"df_shops['shop_name'] = df_shops['shop_name'].apply(pre_pro_text)","c580adca":"pre_pro_text(df_shops['shop_name'][1])","94a121a9":"df_shops = df_shops.drop('city', axis=1)","b7835832":"df_cat['item_cats'] = df_cat['item_category_name']\\\n                        .apply(lambda x: x.split('-'))\ndf_cat['main_cat'] = df_cat['item_cats']\\\n                        .apply(lambda x: x[0].strip())\ndf_cat['sub_cat'] =  df_cat['item_cats']\\\n                        .apply(lambda x: x[1].strip() if len(x)>1 else x[0])","bbb6f485":"df_cat.head()","c637d8ea":"# encoding main category\nenc_mn_cat = LabelEncoder()\ndf_cat['main_cat_enc'] = enc_mn_cat.fit_transform(df_cat['main_cat'])\n\n# encoding sub category\nenc_sb_cat = LabelEncoder()\ndf_cat['sub_cat_enc'] = enc_sb_cat.fit_transform(df_cat['sub_cat'])\n","79552e37":"df_cat = df_cat[['item_category_id', 'main_cat_enc',\n                 'sub_cat_enc']]","04f5678c":"df_test.info()","9202e798":"df_test.head()","367fe3c0":"df_test['date_block_num'] = 34\ndf_test['item_cnt_month'] = 0","896a0eca":"df_test = df_test[['date_block_num', 'shop_id', 'item_id', 'item_cnt_month']]","ab8c9021":"len(set(df_test['item_id']) - set(df_test['item_id']).intersection(df_train['item_id'])), len(set(df_test['item_id'])), len(df_test)","b7ee818e":"df_train.drop(['date', 'item_price'], axis=1, inplace=True)","ec695ca3":"%%time\n\n# grouping the data month-wise\nmatrix = []\ncols = ['date_block_num', 'shop_id', 'item_id']\n\nfor i in tqdm(range(max(df_train.date_block_num)+1)):\n    sales = df_train[df_train.date_block_num==i]\n    matrix.append(np.array(list(product([i],\n        df_train[df_train.date_block_num==i].shop_id.unique(),\n        df_train[df_train.date_block_num==i].item_id.unique())),\n                           dtype='int16'))\n    \nmatrix = pd.DataFrame(np.vstack(matrix), columns=cols)\nmatrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\nmatrix['shop_id'] = matrix['shop_id'].astype(np.int8)\nmatrix['item_id'] = matrix['item_id'].astype(np.int16)\n\nmatrix.sort_values(cols, inplace=True)\n","bf66ce33":"%%time\n\ngroup = df_train.groupby(['date_block_num', 'shop_id', 'item_id']).agg({'item_cnt_day':['sum']})\ngroup.columns = ['item_cnt_month']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=cols, how='left')\nmatrix['item_cnt_month'] = (matrix['item_cnt_month']\n                               .fillna(0)\n                               .clip(0, 20)\n                               .astype(np.float16))","e677b2ce":"matrix = matrix.append(df_test)","d8f0c3a9":"matrix.item_cnt_month.value_counts().plot(kind='bar')","2e42a0d8":"mat_val_cnt = matrix.item_cnt_month.value_counts()\nmat_val_cnt.plot.pie(figsize=(8,8),\n    labels=['' for _ in range(matrix.item_cnt_month.nunique())])\nplt.legend(mat_val_cnt.index.values, loc='center left',\n           bbox_to_anchor=(1.0, 0.5))","b951321a":"sns.boxplot(x=matrix.item_cnt_month)","beac6d01":"matrix = pd.merge(matrix, df_shops, on=['shop_id'], how='left')\nmatrix = pd.merge(matrix, df_items, on=['item_id'], how='left')\nmatrix = pd.merge(matrix, df_cat, on=['item_category_id'], how='left')\n\nmatrix['city_encoded'] = matrix['city_encoded'].astype(np.int8)\nmatrix['item_category_id'] = matrix['item_category_id'].astype(np.int8)\nmatrix['main_cat_enc'] = matrix['main_cat_enc'].astype(np.int8)\nmatrix['sub_cat_enc'] = matrix['sub_cat_enc'].astype(np.int8)","733a2dd6":"### helper functions\ndef get_month_year(num):\n\n    init_year = 2013\n    return int(num%12)+1, int(num\/12)+init_year\n\ndef downcast_dtypes(df):\n    \n    float_cols = [col for col in df.columns if df[col].dtype=='float64']\n    int_cols = [col for col in df.columns if df[col].dtype=='int64']\n    \n    df[float_cols] = df[float_cols].astype(np.float16)\n    df[int_cols] = df[int_cols].astype(np.int16)\n    \n    return df","ec7931f7":"matrix = matrix.drop(['shop_name','item_name'], axis=1)","b61460c6":"matrix = downcast_dtypes(matrix)","d25987da":"matrix.dtypes","82d5bf71":"def generate_lag(df, col_list, lags, col):\n    for l in tqdm(lags):\n        df_shift = df[col_list+[col]].copy()\n        df_shift.columns = col_list+[col+'_lag_'+str(l)]\n        df_shift['date_block_num'] +=l\n        df = pd.merge(df, df_shift,\n                      on=col_list,\n                     how='left')\n    return df","96ef9051":"matrix = generate_lag(matrix, ['date_block_num', 'shop_id', 'item_id'],\n                      [1,2,3,4,5,6,12], 'item_cnt_month')","f88a2fd1":"matrix.head()","7a812784":"%%time\nnew_col = 'item_month_mean'\ngroup = matrix.groupby(['date_block_num', 'item_id'])['item_cnt_month']\\\n            .mean().rename(new_col).reset_index()\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'item_id'],\n                 how='left')\nmatrix = generate_lag(matrix, ['date_block_num', 'shop_id', 'item_id'],\n                     [1,2,3,6,12], new_col)\nmatrix.drop([new_col],axis=1, inplace=True)","7714c089":"%%time\nnew_col = 'shop_month_mean'\ngroup = matrix.groupby(['date_block_num', 'shop_id'])['item_cnt_month']\\\n            .mean().rename(new_col).reset_index()\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id'],\n                 how='left')\nmatrix = generate_lag(matrix, ['date_block_num', 'shop_id', 'item_id'],\n                     [1,2,3,6,12], new_col)\nmatrix.drop([new_col],axis=1, inplace=True)","5a869bfc":"matrix.head()","78fb2d59":"%%time\nnew_col = 'shop_category_month_mean'\ngroup = matrix.groupby(['date_block_num', 'shop_id', 'item_category_id'])['item_cnt_month']\\\n            .mean().rename(new_col).reset_index()\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id','item_category_id'],\n                 how='left')\nmatrix = generate_lag(matrix, ['date_block_num', 'shop_id', 'item_id'],\n                     [1,2,12], new_col)\nmatrix.drop([new_col],axis=1, inplace=True)","c0e89dea":"%%time\nnew_col = 'main_category_month_mean'\ngroup = matrix.groupby(['date_block_num', 'main_cat_enc'])['item_cnt_month']\\\n            .mean().rename(new_col).reset_index()\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'main_cat_enc'],\n                 how='left')\nmatrix = generate_lag(matrix, ['date_block_num', 'shop_id', 'item_id'],\n                     [1], new_col)\nmatrix.drop([new_col],axis=1, inplace=True)","41c631da":"%%time\nnew_col = 'sub_category_month_mean'\ngroup = matrix.groupby(['date_block_num', 'sub_cat_enc'])['item_cnt_month']\\\n            .mean().rename(new_col).reset_index()\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'sub_cat_enc'],\n                 how='left')\nmatrix = generate_lag(matrix, ['date_block_num', 'shop_id', 'item_id'],\n                     [1], new_col)\nmatrix.drop([new_col],axis=1, inplace=True)","439cbdd7":"matrix.head()","0f36af67":"matrix['month'] = matrix['date_block_num'].apply(lambda x: get_month_year(x)[0])\nmatrix['year'] = matrix['date_block_num'].apply(lambda x: get_month_year(x)[1])","c7855cc7":"# number of public holidays in russia every month\nnum_holidays_dict = {\n    1: 6,\n    2: 3,\n    3: 2,\n    4: 8,\n    5: 3,\n    6: 3,\n    7: 2,\n    8: 8,\n    9: 4,\n    10: 8,\n    11: 5,\n    12: 4,\n}","6b1a9994":"matrix['public_holidays'] = matrix['month'].map(num_holidays_dict)","c0840c2a":"# stock exchange trading volume(in Trillions)\nmoex = {\n    12: 659, 13: 640, 14: 1231,\n    15: 881, 16: 764, 17: 663,\n    18: 743, 19: 627, 20: 692,\n    21: 736, 22: 680, 23: 1092,\n    24: 657, 25: 863, 26: 720,\n    27: 819, 28: 574, 29: 568,\n    30: 633, 31: 658, 32: 611,\n    33: 770, 34: 723,\n}","39a1a53b":"matrix['moex_val'] = matrix['date_block_num'].map(moex)","da14376b":"matrix = matrix[matrix.date_block_num > 11]","60b417da":"# filling nan values with 0\nfor c in matrix.columns:\n    if ('_lag_' in c) & (matrix[c].isnull().any()):\n        matrix[c].fillna(0, inplace=True)","bc402885":"matrix.isnull().any()","75a86342":"matrix.date_block_num","f9ec7051":"def xgtrain():\n    reg = xgb.XGBRegressor(n_estimators=5000,\n                           learning_rate=0.01,\n                           max_depth=10,\n                           subsample=0.5,\n                           colsample_bytree=0.5)\n    \n    reg_ = reg.fit(matrix[matrix.date_block_num<33]\\\n                   .drop(['item_cnt_month'], axis=1).values,\n                  matrix[matrix.date_block_num<33]['item_cnt_month'].values,\n                  eval_metric='rmse',\n                   eval_set=[(matrix[matrix.date_block_num<33]\\\n                             .drop(['item_cnt_month'], axis=1).values,\n                             matrix[matrix.date_block_num<33]['item_cnt_month'].values),\n                            (matrix[matrix.date_block_num==33]\\\n                             .drop(['item_cnt_month'], axis=1).values,\n                             matrix[matrix.date_block_num==33]['item_cnt_month'].values)],\n                  verbose=True,\n                  early_stopping_rounds=50)\n    return reg_","c873d0c1":"%%time\nreg_ = xgtrain()","7b40dde0":"file_name = \"xgb_reg.pkl\"\n\n# save\npickle.dump(reg_, open(file_name, \"wb\"))","0060ee0f":"# load\n#reg_ = pickle.load(open(file_name, \"rb\"))\npredictions = reg_.predict(matrix[matrix.date_block_num==34]\\\n                           .drop(['item_cnt_month'], axis=1).values)","9c22b555":"from matplotlib import rcParams\nrcParams['figure.figsize'] = 11.7,8.27\n\ncols = matrix.drop('item_cnt_month', axis = 1).columns\nfeat_importances = [(c,f_i) for ]\nplt.barh(cols, reg_.feature_importances_)\nplt.show()","373a3b37":"sample_submit['item_cnt_month'] = predictions","47fc9488":"sample_submit.to_csv('sales_first_base', index=False)","ff23cc9a":"### adding encoded features","f266abb5":"### generating lag feats","33b9fba3":"### Understanding `sales_train`","07527338":"### Undestanding `test`","2281207e":"#### cleaning impractical values","033ecf6f":"### Undestanding `shops`","e31a4322":"### Understanding `Categories`","2a6797e3":"### Training","25540f4d":"#### understanding date-based data","6e4c643f":"#### Generating text-based features\n- Count Vectorizer\n- Tf-Idf\n- Hash Vectorizer\n","a5736375":"#### Removing outliers","54fc5e74":"### Understanding `items`","7a9a2d61":"We will map the followins shop_names to a single shop_id as they are essentially the same shops","cbad2ce0":"## EDA","df8c3ec5":"We will try to find out if different `shops_names` are in-fact same the same shop with an extra word or two"}}