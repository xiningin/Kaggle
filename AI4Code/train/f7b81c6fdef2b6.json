{"cell_type":{"3e707aba":"code","05b7ec7b":"code","90275c08":"code","87f51ecd":"code","e0d67848":"code","8d1495e5":"code","0eb81a5d":"code","681570a9":"code","367ec929":"code","47e2db94":"code","281f8e3e":"code","abc720dc":"code","a664d6e7":"code","7ad55b35":"code","42cd069b":"code","5928ac29":"code","3e094337":"code","c85f9137":"code","40e0870e":"code","76d443f7":"code","852f6b15":"code","a048dd88":"code","7e26fe82":"code","23ead4d0":"code","8442ed33":"code","189141a1":"code","86e5befc":"code","c8ec49a4":"code","07bec128":"code","d22a54b5":"code","274f08d3":"code","911e400d":"code","f0abccb0":"code","89e24d2b":"code","3bc62d02":"code","8733ebdf":"markdown","e55ae223":"markdown","a410f31e":"markdown","18fb7975":"markdown","36058626":"markdown","c5ad3088":"markdown","67f4a3ee":"markdown","5a2201bb":"markdown","687cce4d":"markdown","b5acd499":"markdown","f4637f1f":"markdown","b5f5b088":"markdown","f41ba787":"markdown","5cb0a805":"markdown","76aa7cda":"markdown","fb37e7e8":"markdown","20fe3db5":"markdown","b2aa4ba6":"markdown","0259bd8e":"markdown","b08fd1d2":"markdown","d2b4cacb":"markdown","05754501":"markdown"},"source":{"3e707aba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","05b7ec7b":"#read malnutriton file\ndata = pd.read_csv(\"..\/input\/child-malnutrition\/mal_dataset.csv\")\nprint('File load successfuly!')","90275c08":"#convert data into dataframes\ndf = pd.DataFrame(data)    #if file is .csv, not need to convert it into dataframe","87f51ecd":"#have a look on data\ndf.head()","e0d67848":"#column SAMPLE, SAMPLESTR, COUNTRY and YEAR are useless for us. So, delete these columns\ndeleted_Columns = ['SAMPLE', 'SAMPLESTR', 'COUNTRY' , 'YEAR']\n\n#selected columns deleted\ncheck =df.drop(deleted_Columns, axis=1, inplace=True)\n\n#see the columns are deleted.\ndf.columns","8d1495e5":"#change the columns names\ndf.rename(columns={'HWHAZNCHS':'stunted',\n                   'HWWAZNCHS':'underweight',\n                   'HWWHZNCHS':'wasted',\n                   'URBAN':'residence',\n                   'WEALTHQUR':'wealthStatus',\n                   'EDUCLVL':'eduStatus',\n                   'WKCURRJOB':'workStatus',\n                   'DECBIGHH':'emp'}, inplace=True)\nprint('Names are succesfully changes\\n', df.columns)","0eb81a5d":"#check the unique values for independent variables\nprint(df.residence.unique(), '\\n')\nprint(df.workStatus.unique(), '\\n')\nprint(df.wealthStatus.unique(), '\\n')\nprint(df.eduStatus.unique(), '\\n')\nprint(df.emp.unique())","681570a9":"#change residence columns values ----- 2 into 0\nprint(\"Before Manipulation of values\")\nprint(df.residence.value_counts())\n\n#change values\nfor item in range(len(df.residence)):\n    if (df.residence[item] == 2):\n        df.residence[item] = 0\n\nprint(\"\\nAfter Manipulation of values\")\nprint(df.residence.value_counts())","367ec929":"#change workStatus columns values ----- 30 into 1 ..... 10,21,22 into 2 ....... 40,51,52,98 into 3\nprint(\"Before Manipulation of values\")\nprint(df.workStatus.value_counts())\n\n#change values\nfor item in range(len(df.workStatus)):\n    if (df.workStatus[item] == 30):\n        df.workStatus[item] = 1\n    elif ((df.workStatus[item] == 10) | (df.workStatus[item] == 21) | (df.workStatus[item] == 22)):\n        df.workStatus[item] = 2\n    elif ((df.workStatus[item] == 40) | (df.workStatus[item] == 51) | (df.workStatus[item] == 52) | (df.workStatus[item] == 98)):\n        df.workStatus[item] = 3\n\nprint(\"\\nAfter Manipulation of values\")\nprint(df.workStatus.value_counts())","47e2db94":"#change emp columns values ----- 10,20 into 1 ..... 40,50,60,99 into 0\nprint(\"Before Manipulation of values\")\nprint(df.emp.value_counts())\n\n#change values\nfor item in range(len(df.emp)):\n    if ( (df.emp[item] == 10) | (df.emp[item] == 20) ):\n        df.emp[item] = 1\n    elif ((df.emp[item] == 40) | (df.emp[item] == 50) | (df.emp[item] == 60) | (df.emp[item] == 99) | (df.emp[item] == 98)):\n        df.emp[item] = 0\n\nprint(\"\\nAfter Manipulation of values\")\nprint(df.emp.value_counts())","281f8e3e":"#change stunted columns values ----- stunted=1 if stunted < -200 ..... stunted=0 if stunted > -200 & stunted < 9999\nfor item in range(len(df.stunted)):\n    if ( (df.stunted[item] <= -200) ):\n        df.stunted[item] = 1\n    elif ( (df.stunted[item] > -200) | (df.stunted[item] <= 9999) ):\n        df.stunted[item] = 0\n\nprint(\"Stunted After Manipulation of values\")\nprint(df.stunted.value_counts())","abc720dc":"#change wasted columns values \n#    wasted=1 if wasted <= -200 \n#    wasted=0 if wasted > -200 & wasted <= 9999\nfor item in range(len(df.wasted)):\n    if ( (df.wasted[item] <= -200) ):\n        df.wasted[item] = 1\n    elif ( (df.wasted[item] > -200) | (df.wasted[item] <= 9999) ):\n        df.wasted[item] = 0\n\nprint(\"Wasted After Manipulation of values\")\nprint(df.wasted.value_counts())","a664d6e7":"#change underweight columns values\n#    underweight=1 if underweight <= -200 \n#    underweight=0 if underweight > -200 & underweight <= 9999\nfor item in range(len(df.underweight)):\n    if ( (df.underweight[item] <= -200) ):\n        df.underweight[item] = 1\n    elif ( (df.underweight[item] > -200) | (df.underweight[item] <= 9999) ):\n        df.underweight[item] = 0\n\nprint(\"underweight After Manipulation of values\")\nprint(df.underweight.value_counts())","7ad55b35":"#see the head of data\ndf.head()","42cd069b":"#it tell the\n# 1. Total number of obs.\n# 2. count of missing values\n# 3. Type of data\n\ndf.info()\n\n# See there is no missing value in our data and number of obs = 12708","5928ac29":"#import visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns","3e094337":"#compare all independent variables with the stunted(dependent) variables\n\n#set the size of the figure\nplt.figure(figsize=(14,6))\n#set the title of the figure\nplt.title(\"Shows relation b\/w all independent variables and stunted\")\n\n\nsns.lineplot(x=df.residence, y= df.stunted, label=\"residence\")\nsns.lineplot(x=df.emp, y= df.stunted, label=\"empowerment\")\nsns.lineplot(x=df.workStatus, y= df.stunted, label=\"workStatus\")\nsns.lineplot(x=df.wealthStatus, y= df.stunted, label=\"wealthStatus\")\nsns.lineplot(x=df.eduStatus, y= df.stunted, label=\"eduStatus\")","c85f9137":"#compare all independent variables with the wasted(dependent) variables\n\n#set the size of the figure\nplt.figure(figsize=(14,6))\n#set the title of the figure\nplt.title(\"Shows relation b\/w all independent variables and wasted\")\n\n\nsns.lineplot(x=df.residence, y= df.wasted, label=\"residence\")\nsns.lineplot(x=df.emp, y= df.wasted, label=\"empowerment\")\nsns.lineplot(x=df.workStatus, y= df.wasted, label=\"workStatus\")\nsns.lineplot(x=df.wealthStatus, y= df.wasted, label=\"wealthStatus\")\nsns.lineplot(x=df.eduStatus, y= df.wasted, label=\"eduStatus\")","40e0870e":"#compare all independent variables with the underweight(dependent) variables \n#set the size of the figure \nplt.figure(figsize=(14,6)) \n#set the title of the figure \nplt.title(\"Shows relation b\/w all independent variables and underweight\") \nsns.lineplot(x=df.residence, y= df.underweight, label=\"residence\") \nsns.lineplot(x=df.emp, y= df.underweight, label=\"empowerment\") \nsns.lineplot(x=df.workStatus, y= df.underweight, label=\"workStatus\") \nsns.lineplot(x=df.wealthStatus, y= df.underweight, label=\"wealthStatus\") \nsns.lineplot(x=df.eduStatus, y= df.underweight, label=\"eduStatus\")","76d443f7":"#select the target(dependent) variable\ny = df.stunted\n\n#select the featured(independent) variables\nmal_features = ['residence', 'workStatus', 'wealthStatus', 'eduStatus', 'emp']\nx = df[mal_features]","852f6b15":"#apply train test split\nfrom sklearn.model_selection import train_test_split\ntrain_X, test_X, train_Y, test_Y = train_test_split(x,y,train_size = 0.7, test_size = 0.3, random_state=0)","a048dd88":"#import the required model\nfrom sklearn.tree import DecisionTreeRegressor \n#Define a model\ndecision_model = DecisionTreeRegressor()\n#Fit the model\ndecision_model.fit(train_X, train_Y)\n#Predict the model\nmal_Prediction = decision_model.predict(test_X)","7e26fe82":"#metrics approach is used to check the accuracy of model \nfrom sklearn.metrics import mean_absolute_error\n#Evaluate the model\nprint(mean_absolute_error(test_Y,mal_Prediction))","23ead4d0":"#define a function to taking care of max_leaf_nodes parameter\ndef get_mae(max_leaf_nodes, train_X, test_X, train_y, test_y):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(test_X)\n    mae = mean_absolute_error(test_Y, preds_val)\n    return(mae)","8442ed33":"# compare MAE with differing values of max_leaf_nodes\nfor max_leaf_nodes in [5, 50, 500, 5000]:\n    my_mae = get_mae(max_leaf_nodes, train_X, test_X, train_Y, test_Y)\n    print(\"Max leaf nodes: {} \\t\\t Mean Absolute Error: {}\".format(max_leaf_nodes, my_mae))\n\n#but did'nt see any imporvement by using parameter tuning","189141a1":"#Import model\nfrom sklearn.ensemble import RandomForestRegressor\n#define model\nrandom_model = RandomForestRegressor(n_estimators=8, random_state=0)\n#fit model\nrandom_model.fit(train_X, train_Y)\n#predict model\npred = random_model.predict(test_X)\n#evaluate model\nprint(mean_absolute_error(test_Y, pred))     #do not improve the results","86e5befc":"#linear model\nfrom sklearn.linear_model import LinearRegression\nlinear_model = LinearRegression(n_jobs=10)\nlinear_model.fit(train_X, train_Y)\nlinear_Prediction = linear_model.predict(test_X)\n#a very little better than above models, but not good\nprint(mean_absolute_error(test_Y,linear_Prediction))","c8ec49a4":"#logistic model\nfrom sklearn.linear_model import LogisticRegression\nlogistic_model = LogisticRegression()\nlogistic_model.fit(train_X, train_Y)\nlogistic_Prediction = logistic_model.predict(test_X)\n#Here, we see the big improvement in the results\nprint(mean_absolute_error(test_Y,logistic_Prediction))","07bec128":"from xgboost import XGBRegressor\ngradient_model = XGBRegressor()\ngradient_model.fit(train_X, train_Y,\n                   early_stopping_rounds=5,\n                   eval_set=[(test_X, test_Y)],\n                   verbose=False)\ngradient_pred = gradient_model.predict(test_X)\n#shows same results of above models, except Logistic model. hence, it also, not improving the accuracy. \nprint(\"Mean absolute Error: \", mean_absolute_error(test_Y, gradient_pred))","d22a54b5":"print(\"\\tScores of all model\")\nprint(\"Decision Tree Score: \" , decision_model.score(test_X,test_Y))\nprint(\"Randon Forest Score: \" ,random_model.score(test_X,test_Y))\nprint(\"Linear Regression Score: \" ,linear_model.score(test_X,test_Y))\nprint(\"Gradiest Decent Model_1 Score: \" ,gradient_model.score(test_X,test_Y))\n\n#see Logistic Regression shows 88% accuracy of model, as our data problem is binary classification. \nprint(\"Logistic Regression Score: \" ,logistic_model.score(test_X,test_Y))","274f08d3":"#import required Library\nfrom sklearn.model_selection import cross_val_score\n\nscores= -1 * cross_val_score(decision_model, test_X, test_Y, cv = 5, scoring='neg_mean_absolute_error')\nprint(\"neg_mean_absolute_error of Decision Tree: \",np.mean(scores))\n\nscores= -1 * cross_val_score(random_model, test_X, test_Y, cv = 5, scoring='neg_mean_absolute_error')\nprint(\"neg_mean_absolute_error of Randon Forest: \",np.mean(scores))\n\nscores= -1 * cross_val_score(linear_model, test_X, test_Y, cv = 5, scoring='neg_mean_absolute_error')\nprint(\"neg_mean_absolute_error of Linear Regression: \",np.mean(scores))\n\nscores= -1 * cross_val_score(gradient_model, test_X, test_Y, cv = 5, scoring='neg_mean_absolute_error')\nprint(\"neg_mean_absolute_error of Gradiest Decent Model_1: \",np.mean(scores))\n\n#we double chehcked our result as mean_absolute_error of logistic regression is small, than other models\nscores= -1 * cross_val_score(logistic_model, test_X, test_Y, cv = 5, scoring='neg_mean_absolute_error')\nprint(\"neg_mean_absolute_error of Logistic Regression: \",np.mean(scores))","911e400d":"#triple check the accuracy of Logistic Model\nfrom sklearn.metrics import accuracy_score as acc\nprint(acc(test_Y, logistic_Prediction))\n\n#Note metric is only applicable for CLassification Model","f0abccb0":"#test_train_split\ntrain_X, test_X, train_Y, test_Y = train_test_split(x,y,train_size = 0.8, test_size = 0.2, random_state=0)\n\n\"\"\"By changing train_size = 0.7 to train_size = 0.8\n           and test_size = 0.3 to test_size = 0.2\n    We see 1% improvemnt in the model.\n    \n    I randomly used the  parameter tuning of Logistic Regression but, accuracy results as same as previous.\n    Moreover, Same result get from using ### LogisticRegressionCV and ###SGDClassifier model.\n    Try these model by yourself\"\"\"\n\n#defile model\nlogistic_model_2 = LogisticRegression(random_state=0)\n#Fit the model\nlogistic_model_2.fit(train_X, train_Y)\n#make predictions\nlogistic_Prediction_2 = logistic_model_2.predict(test_X)\n\n#Mean absolute Error\nprint(\"mean absolute Error of Stunted Varaible: \", mean_absolute_error(test_Y,logistic_Prediction_2))\n\n#Score of the model\nprint(\"Logistic Regression Score Stunted Varaible: \" ,logistic_model_2.score(test_X,test_Y))","89e24d2b":"#test_train_split\ntrain_X, test_X, train_Y, test_Y = train_test_split(x,y,train_size = 0.8, test_size = 0.2, random_state=0)\n\n\"\"\"Same accuracy achieved as stunted model\"\"\"\n\n#defile model\nlogistic_model_underweight = LogisticRegression()\n#Fit the model\nlogistic_model_underweight.fit(train_X, train_Y)\n#make predictions\nlogistic_Prediction_underweight = logistic_model_underweight.predict(test_X)\n\n#Mean absolute Error\nprint(\"mean absolute Error of Under_weight Varaible: \", mean_absolute_error(test_Y,logistic_Prediction_underweight))\n\n#Score of the model\nprint(\"Logistic Regression Score Under_weight Varaible: \" ,logistic_model_underweight.score(test_X,test_Y))","3bc62d02":"#test_train_split\ntrain_X, test_X, train_Y, test_Y = train_test_split(x,y,train_size = 0.8, test_size = 0.2, random_state=0)\n\n\"\"\"Same accuracy achieved as stunted and underweight models\"\"\"\n\n#defile model\nlogistic_model_weighted = LogisticRegression()\n#Fit the model\nlogistic_model_weighted.fit(train_X, train_Y)\n#make predictions\nlogistic_Prediction_weighted = logistic_model_weighted.predict(test_X)\n\n#Mean absolute Error\nprint(\"mean absolute Error of Weighted Varaible: \", mean_absolute_error(test_Y,logistic_Prediction_weighted))\n\n#Score of the model\nprint(\"Logistic Regression Score Weighted Varaible: \" ,logistic_model_weighted.score(test_X,test_Y))","8733ebdf":"### Modeling of data\nFirst use **Stunted variable** in th modeling","e55ae223":"Every model is divide into 4 parts:\n\n##### Define a model\n##### Fit the model\n##### Predict the model\n##### Evaluate the model\nI start by applying\n\n#### 1. Decison Tree model","a410f31e":"#### Manipulation of Independent Variables\nDetailed Info of varaibles\n\nresidence --> 0 = Rural; 1 = Urban\n\nworkStatus --> 0 = Not Currently Working; 1 = Agricultural; 2 = White Collar; 3 = Blue Collar\n\nwealthStatus --> 1 = Poorest; 2 = Poorer; 3 = Middle; 4 = Richer; 5 = Richest\n\neduStatus --> 0 = Not Education; 1 = Primary; 2 = Secondary; 3 = Higher\n\nemp --> 0 = Not Empowered; 1 = Empowered","18fb7975":"**Accuracy Scores of all applied models**","36058626":"I am trying to see the impact of \"Women Empowerment Variables\" on nutrition level of Children. Here, independent variables are URBAN, WKCURRJOB, WEALTH, WEALTHQUR and EDUCLVL. and the dependent variables are HWWHZNCHS, HWWAZNCHS and HWHAZNCHS.","c5ad3088":"### Data preprocessing","67f4a3ee":"#### 3. Linear Regression Model","5a2201bb":"### Modeling of 2nd dependent variable\nNow, use **underweight** variable in the modeling","687cce4d":"### Data Visualization","b5acd499":"#### 5. Extreme Gradient Boosting (XGBoost)\nis used for modeling, it dramatically improves the results","f4637f1f":"**Negative Mean Absolute Error Scores of all applied Models**","b5f5b088":"**apply different techniques to improve the model accuracy**","f41ba787":"##### Line plot\nis used to check the trends of the varaibles","5cb0a805":"### Modeling of 3rd dependent variable\nLastly, the modeling of **weighted** variable.","76aa7cda":"#### 4. Logistic Regression Model","fb37e7e8":"Now, use\n\n#### 2. Random Forest model","20fe3db5":"### End of Preprocessing","b2aa4ba6":"**Validation Check by Classification Metrics**","0259bd8e":"#### End of Modeling\n***Best of Luck! and Try your techniques to improve the results of modeling.***","b08fd1d2":"#### Manipulation of Dependent variables\nNow, convert the values of dependent variables.\n\nAccording to WHO Z-score is below than -2 standard deviation than reference population are considered stunted, wastied and underweight","d2b4cacb":"**Let's Try to Improve the accuracy of Logistic Regression Model**","05754501":"Change variable name:\n\n**Dependent variables**\n\nHWHAZNCHS -> stunted\n\nHWWAZNCHS -> underweight\n\nHWWHZNCHS -> wasted\n\n**Independent variables***\n\nURBAN -> residence\n\nWEALTHQUR -> wealthStatus\n\nEDUCLVL -> eduStatus\n\nWKCURRJOB -> workStatus\n\nDECBIGHH -> emp"}}