{"cell_type":{"4eca90f4":"code","bd10eb52":"code","c7f03af5":"code","42122986":"code","d754c775":"code","1f0d76b5":"code","97caf197":"code","43779320":"code","f91c10a1":"code","0498d9c5":"code","9a3bd71a":"code","ae13fd83":"code","bc3984f0":"code","d3bc742c":"code","b3e72829":"code","2dfafb78":"code","ba65a0c7":"code","45955fa4":"code","dab8cd5f":"code","a83303ec":"code","4674f2ca":"code","21b87042":"code","de31219d":"code","f2b5ec4d":"code","e068fcc1":"markdown","5cf2af7a":"markdown","c403dce9":"markdown","afc23391":"markdown","02fbdeb5":"markdown","9a103823":"markdown","31812e93":"markdown"},"source":{"4eca90f4":"import os\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\n\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler, LabelEncoder\n\nfrom tqdm import tqdm\n\nimport lightgbm as lgb\nimport xgboost as xgb\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.utils import class_weight\n\nfrom sklearn.metrics import accuracy_score, make_scorer\nfrom sklearn.metrics import roc_curve, auc, accuracy_score, cohen_kappa_score\nfrom sklearn.metrics import mean_squared_error, f1_score, confusion_matrix, mean_squared_log_error","bd10eb52":"## since so many data file you'll write a code to find what you need  :D \n\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        file_extension = os.path.splitext(filename)[1]\n#        if file_extension == '.csv':\n#            print(os.path.join(dirname, filename))","c7f03af5":"def display_missing(df, head=True):\n    total = df.isnull().sum().sort_values(ascending=False)\n    percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    return missing_data","42122986":"needed_files = ['submission.csv', 'test.csv', 'train.csv']\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        if filename in needed_files:\n            print(os.path.join(dirname, filename))","d754c775":"%%time\n\nPATH = '\/kaggle\/input\/covid19-global-forecasting-week-4\/'\n\ndef load_ds(file_name):\n    return pd.read_csv(PATH + file_name)\n\ntrain = load_ds('train.csv')\ntest = load_ds('test.csv')\nsubmission = load_ds('submission.csv')\n#print(train.shape, test.shape)\n\n## adding mortality and geo data to train and test\n\nusa_mortality_rates = pd.read_csv('\/kaggle\/input\/coronavirus-covid19-mortality-rate-by-country\/usa_covid19_mortality_rates.csv')\nworld_mortality_rates = pd.read_csv('\/kaggle\/input\/coronavirus-covid19-mortality-rate-by-country\/global_covid19_mortality_rates.csv')\n\n## it seems, there is a dataset from previous week2 competition with all information one may need for a competition like this one :)\n## not sure, who created this dataset, was too late, but thank you.\nenriched = pd.read_csv('\/kaggle\/input\/covid19-enriched-dataset-week-2\/enriched_covid_19_week_2.csv')\n\nworld_enriched_cols = ['Date', 'Country_Region', 'age_75-79', 'age_80-84', 'age_85-89', 'age_90-94',\n       'age_95-99', 'age_100+', 'total_pop', 'smokers_perc', 'density',\n       'urbanpop', 'hospibed', 'lung', 'femalelung', 'malelung', 'restrictions', 'quarantine', 'schools']\n\nus_enriched_cols = ['Date', 'Province_State', 'age_75-79', 'age_80-84', 'age_85-89', 'age_90-94',\n       'age_95-99', 'age_100+', 'total_pop', 'smokers_perc', 'density',\n       'urbanpop', 'hospibed', 'lung', 'femalelung', 'malelung', 'restrictions', 'quarantine', 'schools']\n\nus_train = train[train['Country_Region'] == 'US']\nus_test = test[test['Country_Region'] == 'US']\ntrain = train.drop(train[train['Country_Region'] == 'US'].index)\ntest = test.drop(test[test['Country_Region'] == 'US'].index)\n\nmort_df = world_mortality_rates[['Country', 'Mortality Rate', 'Latitude', 'Longitude']]\ntrain = train.merge(mort_df, how='left', left_on='Country_Region', right_on='Country').drop(['Country'], axis=1)\ntrain = train.merge(enriched[world_enriched_cols], how='left', left_on=['Date', 'Country_Region'], right_on=['Date', 'Country_Region'])\ntest = test.merge(mort_df, how='left', left_on='Country_Region', right_on='Country').drop(['Country'], axis=1)\ntest = test.merge(enriched[world_enriched_cols], how='left', left_on=['Date', 'Country_Region'], right_on=['Date', 'Country_Region'])\n\nus_mort_df = usa_mortality_rates[['State', 'Mortality Rate', 'Latitude', 'Longitude']]\nus_train = us_train.merge(us_mort_df, how='left', left_on='Province_State', right_on='State').drop(['State'], axis=1)\nus_train = us_train.merge(enriched[us_enriched_cols], how='left', left_on=['Date', 'Province_State'], right_on=['Date', 'Province_State'])\n\nus_test = us_test.merge(us_mort_df, how='left', left_on='Province_State', right_on='State').drop(['State'], axis=1)\nus_test = us_test.merge(enriched[us_enriched_cols], how='left', left_on=['Date', 'Province_State'], right_on=['Date', 'Province_State'])\n\n## even more useful features\n## need to find US data first\n\n#useful_cols = ['Country_Region', 'Tourism','Date_FirstFatality','Date_FirstConfirmedCase', 'Mean_Age', 'Date_FirstFatality', 'Date_FirstConfirmedCase']\n#cluster_data = pd.read_csv(\"\/kaggle\/input\/covid19-useful-features-by-country\/Countries_usefulFeatures.csv\")\n#train = train.merge(cluster_data[useful_cols], how='left', left_on='Country_Region', right_on='Country_Region')\n#test = test.merge(cluster_data[useful_cols], how='left', left_on='Country_Region', right_on='Country_Region')\n\ntrain['Province_State'] = train['Province_State'].fillna('Unknown')\ntest['Province_State'] = test['Province_State'].fillna('Unknown')\n\ntrain = pd.concat([train,us_train])\ntest = pd.concat([test,us_test])\n\nenriched_cols = ['age_75-79', 'age_80-84', 'age_85-89', 'age_90-94', 'age_95-99', 'age_100+', 'total_pop', 'smokers_perc', 'density',\n       'urbanpop', 'hospibed', 'lung', 'femalelung', 'malelung', 'restrictions', 'quarantine', 'schools', 'Mortality Rate']\n\ntrain['Country_Region'] = train['Country_Region'].fillna('Unknown')\ntrain['Province_State'] = train['Province_State'].fillna('Unknown')\n#train[enriched_cols] = train.groupby(['Country_Region', 'Province_State'])[enriched_cols].transform(lambda x: x.fillna(x.mean()))\n\ntest['Country_Region'] = test['Country_Region'].fillna('Unknown')\ntest['Province_State'] = test['Province_State'].fillna('Unknown')\n#test[enriched_cols] = test.groupby(['Country_Region', 'Province_State'])[enriched_cols].transform(lambda x: x.fillna(x.mean()))\n\ntest.reset_index(drop=True, inplace=True)\ntrain.reset_index(drop=True, inplace=True)\n\nprint('Shapes: ')\nprint(train.shape, test.shape)\n\nprint('Duplicates: ')\nprint(len(test.loc[test[enriched_cols].index.duplicated()]))\nprint(len(train.loc[train[enriched_cols].index.duplicated()]))","1f0d76b5":"#train.drop(train.loc[train[(train['Country_Region'] == 'Diamond Princess')].index].index, axis=0, inplace=True)\n#test.drop(test.loc[test[(test['Country_Region'] == 'Diamond Princess')].index].index, axis=0, inplace=True)\ntrain.loc[train.Country_Region == 'Diamond Princess', 'Mortality Rate'] = 0\ntrain.loc[train.Country_Region == 'Diamond Princess', 'Latitude'] = 0\ntrain.loc[train.Country_Region == 'Burma', 'Longitude'] = 0\ntest.loc[test.Country_Region == 'Diamond Princess', 'Mortality Rate'] = 0\ntest.loc[test.Country_Region == 'Diamond Princess', 'Latitude'] = 0\ntest.loc[test.Country_Region == 'Diamond Princess', 'Longitude'] = 0\n\n\ntrain.loc[train.Province_State == 'Guam', 'Mortality Rate'] = 0\ntrain.loc[train.Province_State == 'Guam', 'Latitude'] = 13.444304\ntrain.loc[train.Province_State == 'Guam', 'Longitude'] = 144.793732\ntest.loc[test.Province_State == 'Guam', 'Mortality Rate'] = 0\ntest.loc[test.Province_State == 'Guam', 'Latitude'] = 13.444304\ntest.loc[test.Province_State == 'Guam', 'Longitude'] = 144.793732\n\ntrain.loc[train.Province_State == 'Virgin Islands', 'Mortality Rate'] = 0\ntrain.loc[train.Province_State == 'Virgin Islands', 'Latitude'] = 0\ntrain.loc[train.Province_State == 'Virgin Islands', 'Longitude'] = 0\ntest.loc[test.Province_State == 'Virgin Islands', 'Mortality Rate'] = 0\ntest.loc[test.Province_State == 'Virgin Islands', 'Latitude'] = 0\ntest.loc[test.Province_State == 'Virgin Islands', 'Longitude'] = 0\n\ntrain.loc[train.Country_Region == 'Burma', 'Mortality Rate'] = 0\ntrain.loc[train.Country_Region == 'Burma', 'Latitude'] = 16.871311\ntrain.loc[train.Country_Region == 'Burma', 'Longitude'] = 96.199379\ntest.loc[test.Country_Region == 'Burma', 'Mortality Rate'] = 0\ntest.loc[test.Country_Region == 'Burma', 'Latitude'] = 16.871311\ntest.loc[test.Country_Region == 'Burma', 'Longitude'] = 96.199379\n\ntrain.loc[train.Country_Region == 'Cabo Verde', 'Mortality Rate'] = 0\ntrain.loc[train.Country_Region == 'Cabo Verde', 'Latitude'] = 0\ntrain.loc[train.Country_Region == 'Cabo Verde', 'Longitude'] = 0\ntest.loc[test.Country_Region == 'Cabo Verde', 'Mortality Rate'] = 0\ntest.loc[test.Country_Region == 'Cabo Verde', 'Latitude'] = 0\ntest.loc[test.Country_Region == 'Cabo Verde', 'Longitude'] = 0\n\ntrain.loc[train.Country_Region == 'West Bank and Gaza', 'Mortality Rate'] = 0\ntrain.loc[train.Country_Region == 'West Bank and Gaza', 'Latitude'] = 0\ntrain.loc[train.Country_Region == 'West Bank and Gaza', 'Longitude'] = 0\ntest.loc[test.Country_Region == 'West Bank and Gaza', 'Mortality Rate'] = 0\ntest.loc[test.Country_Region == 'West Bank and Gaza', 'Latitude'] = 0\ntest.loc[test.Country_Region == 'West Bank and Gaza', 'Longitude'] = 0\n\ntrain.loc[train.Country_Region == 'Congo (Brazzaville)', 'Mortality Rate'] = 0\ntrain.loc[train.Country_Region == 'Congo (Brazzaville)', 'Latitude'] = 0\ntrain.loc[train.Country_Region == 'Congo (Brazzaville)', 'Longitude'] = 0\ntest.loc[test.Country_Region == 'Congo (Brazzaville)', 'Mortality Rate'] = 0\ntest.loc[test.Country_Region == 'Congo (Brazzaville)', 'Latitude'] = 0\ntest.loc[test.Country_Region == 'Congo (Brazzaville)', 'Longitude'] = 0\n\ntrain.loc[train.Country_Region == 'Congo (Kinshasa)', 'Mortality Rate'] = 0\ntrain.loc[train.Country_Region == 'Congo (Kinshasa)', 'Latitude'] = 0\ntrain.loc[train.Country_Region == 'Congo (Kinshasa)', 'Longitude'] = 0\ntest.loc[test.Country_Region == 'Congo (Kinshasa)', 'Mortality Rate'] = 0\ntest.loc[test.Country_Region == 'Congo (Kinshasa)', 'Latitude'] = 0\ntest.loc[test.Country_Region == 'Congo (Kinshasa)', 'Longitude'] = 0\n\ntrain.loc[train.Country_Region == 'Cote d\\'Ivoire', 'Mortality Rate'] = 0\ntrain.loc[train.Country_Region == 'Cote d\\'Ivoire', 'Latitude'] = 0\ntrain.loc[train.Country_Region == 'Cote d\\'Ivoire', 'Longitude'] = 0\ntest.loc[test.Country_Region == 'Cote d\\'Ivoire', 'Mortality Rate'] = 0\ntest.loc[test.Country_Region == 'Cote d\\'Ivoire', 'Latitude'] = 0\ntest.loc[test.Country_Region == 'Cote d\\'Ivoire', 'Longitude'] = 0\n\ntrain.loc[train.Country_Region == 'Czechia', 'Mortality Rate'] = 0\ntrain.loc[train.Country_Region == 'Czechia', 'Latitude'] = 12.841150\ntrain.loc[train.Country_Region == 'Czechia', 'Longitude'] = 15.530190\ntest.loc[test.Country_Region == 'Czechia', 'Mortality Rate'] = 0\ntest.loc[test.Country_Region == 'Czechia', 'Latitude'] = 12.841150\ntest.loc[test.Country_Region == 'Czechia', 'Longitude'] = 15.530190\n\nprint(train.shape, test.shape)","97caf197":"display_missing(train)","43779320":"train[train.isna().any(axis=1)]","f91c10a1":"train.fillna(0, inplace=True)\ntest.fillna(0, inplace=True)\ndisplay_missing(train)","0498d9c5":"#train[\"fatality_rate\"] = train.Fatalities * 100 \/ train.ConfirmedCases\n#train.groupby(['Country_Region', 'Province_State'])[\"fatality_rate\"].bfill()\n#test['fatality_rate'] = 0.0;\n\n#def set_rate(grp, val):\n#    grp['fatality_rate'] = val\n#    return grp\n\n## for sure the is a better way in pandas, let me know\n#for key, group in train.groupby(['Country_Region', 'Province_State'])['fatality_rate']:\n#    val = train[((train.Country_Region == key[0]) & (train.Province_State == key[1]))]['fatality_rate'][:1].values[0]\n#    test.groupby(['Country_Region', 'Province_State']).apply(lambda x: set_rate(x,val))","9a3bd71a":"train","ae13fd83":"def ds_prep(df):\n    df['Days'] = (pd.to_datetime(df['Date']) - pd.to_datetime('2020-01-01')).dt.days\n    df['Day'] = pd.to_datetime(df['Date']).dt.day\n    df['Month'] = pd.to_datetime(df['Date']).dt.month\n    #df['Day'] = df['Day'].astype('category')\n    #df['Month'] = df['Month'].astype('category')\n\n    df['province_code'] = LabelEncoder().fit_transform(df['Province_State']).astype(int)\n    df['country_code'] = LabelEncoder().fit_transform(df['Country_Region']).astype(int)\n    #df['province_code'] = df['province_code'].astype('category')\n    #df['country_code'] = df['country_code'].astype('category')\n    return df\n\ntrain = ds_prep(train)\ntrain['ConfirmedCases'] = train['ConfirmedCases'].astype(int)\ntrain['Fatalities'] = train['Fatalities'].astype(int)\n\ntest = ds_prep(test)","bc3984f0":"FEATURES = ['Days', 'Day', 'Month', 'province_code', 'country_code', 'Mortality Rate', 'Latitude', 'Longitude', \n            'total_pop', 'density',  \n            #'age_75-79', 'urbanpop', \n            ##'Tourism', 'Mean_Age',\n            #'age_75-79', 'age_80-84', 'age_85-89', 'age_90-94',\n           #'age_95-99', 'age_100+', 'total_pop', 'smokers_perc', 'density', 'urbanpop', 'hospibed', 'lung', 'femalelung', 'malelung',\n           #'restrictions', 'quarantine', 'schools'\n           ]\n\nX_train = train[FEATURES]\ny_confirmed = train['ConfirmedCases']\ny_fatalities = train['Fatalities']\n\nX_test = test[FEATURES]\n\n#scaler = RobustScaler()\n#X_train = scaler.fit_transform(X_train)\n#X_test = scaler.transform(X_test)\n\nprint(X_train.shape, y_confirmed.shape, y_fatalities.shape)\nprint(X_test.shape)","d3bc742c":"#from sklearn.decomposition import PCA\n\n#NOTIN_FEATURES = [ 'age_75-79', 'age_80-84', 'age_85-89', 'age_90-94', 'age_95-99', 'age_100+']\n\n#pca = PCA(n_components=1)\n#pca_train = pca.fit_transform(train[NOTIN_FEATURES].fillna(0))\n#pca_test = pca.transform(test[NOTIN_FEATURES].fillna(0))\n\n#X_train['pca_1'] = pca_train[:,0]\n#X_test['pca_1'] = pca_test[:,0]\n#FEATURES.append('pca_1')\n\n#del pca_train, pca_test","b3e72829":"## this func throws an error on NaN or negative values still in works\ndef RMSLE(Y, pred):\n    y_data = np.nan_to_num(Y.data, nan=0, posinf=0, neginf=0)\n    y_data = np.clip(y_data, 0, 1000000) \n    pred_data = np.nan_to_num(pred.data, nan=0, posinf=0, neginf=0)\n    pred_data = np.clip(pred_data, 0, 1000000) \n    return np.sqrt(mean_squared_log_error(y_data.values, pred_data.values))\n\ndef rmsle(y, y_pred):\n        y = y.data\n        y_pred = y_pred.data\n        assert len(y) == len(y_pred)\n        terms_to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n        return (sum(terms_to_sum) * (1.0\/len(y))) ** 0.5\n\ndef train_model(X, y, params, split_size, random_seed):\n    X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=split_size, random_state=random_seed)\n    evals_result = {}\n    model = lgb.train(\n        params, \n        train_set=lgb.Dataset(X_tr, y_tr), \n        num_boost_round = NUM_BOOST_ROUND,\n        valid_sets = lgb.Dataset(X_val, y_val), \n        verbose_eval = VERBOSE_EVAL,\n        evals_result = evals_result,\n        early_stopping_rounds = EARLY_STOPPING_ROUNDS\n    )    \n    return model, evals_result","2dfafb78":"NUM_BOOST_ROUND = 2000\nEARLY_STOPPING_ROUNDS = 40\nVERBOSE_EVAL = 500\nRANDOM_SEED = 13\nLEARNING_RATE = 0.1\nMAX_DEPTH = -1\nNUM_LEAVES = 200\n\n\nlgb_params = {\n    'learning_rate': LEARNING_RATE, \n    'max_depth': MAX_DEPTH, \n    'num_leaves': NUM_LEAVES,\n    'random_state': RANDOM_SEED, \n    'n_jobs':-1, \n    'metric':'rmse'\n    }\n\n#confirmed_model, confirmed_evals_result = train_model(X_train, y_confirmed, lgb_params, 0.2, RANDOM_SEED)\n#fatalities_model, fatalities_evals_result = train_model(X_train, y_fatalities, lgb_params, 0.2, RANDOM_SEED)","ba65a0c7":"#f, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize=(15, 6)) \n#lgb.plot_metric(confirmed_evals_result, metric='rmse', ax=ax1) \n#lgb.plot_metric(fatalities_evals_result, metric='rmse', ax=ax2) \n#plt.show()","45955fa4":"def train_xgb_model(X, y, params, split_size, random_seed):\n    X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=split_size, random_state=random_seed)\n\n    dtrain = xgb.DMatrix(X_tr, label=y_tr)\n    dtest = xgb.DMatrix(X_val, label=y_val)\n    evals_result = {}\n    model = xgb.train( params, dtrain,\n        num_boost_round = NUM_BOOST_ROUND,\n        evals=[(dtest, 'eval')], \n        #eval_metric=['rmse'],\n        evals_result = evals_result,\n        verbose_eval = VERBOSE_EVAL,\n        early_stopping_rounds=EARLY_STOPPING_ROUNDS)    \n    return model, evals_result\n\nxgb_params = {\n    'objective':'reg:squarederror', \n    'colsample_bytree': 0.96, \n    #'gamma': 0.9, \n    'learning_rate': 0.05, \n    'max_depth': 10, \n    #'min_child_weight': 1, \n    'subsample': 0.95,\n    'eval_metric':'rmse'\n}\n\n\nconfirmed_model, confirmed_evals_result = train_xgb_model(X_train, y_confirmed, xgb_params, 0.2, RANDOM_SEED)\nfatalities_model, fatalities_evals_result = train_xgb_model(X_train, y_fatalities, xgb_params, 0.2, RANDOM_SEED)","dab8cd5f":"f, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize=(15, 6)) \nlgb.plot_metric(confirmed_evals_result, metric='rmse', ax=ax1) \nlgb.plot_metric(fatalities_evals_result, metric='rmse', ax=ax2) \nplt.show()","a83303ec":"f, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize=(15, 6)) \nxgb.plot_importance(confirmed_model, ax=ax1) \nxgb.plot_importance(fatalities_model, ax=ax2) \nplt.show()","4674f2ca":"yhat_confirmed = confirmed_model.predict(xgb.DMatrix(X_test))\nyhat_fatalities = fatalities_model.predict(xgb.DMatrix(X_test))\n\ntest['ConfirmedCases'] = yhat_confirmed\ntest['Fatalities'] = yhat_fatalities","21b87042":"def lineplot_for_case(df, column, title, ax):\n    unique_dates = np.unique(df['Date'].values)\n    date_ticks = range(0, len(unique_dates), 5)\n    ax.set_xticks(date_ticks);\n    ax.set_xticklabels([unique_dates[i] for i in date_ticks], rotation='vertical');\n    ax.set_xlabel('Date');\n    ax.set_ylabel(column);\n    ax.set_title(title)\n    sns.lineplot( x=df['Date'], y = df[column], ax=ax)\n    \n    \ndef plots_for_country(country):    \n    test_afg = test[test['Country_Region'] == country]\n    f, ax = plt.subplots(1, 2, figsize=(15, 5))\n    lineplot_for_case(test_afg, 'ConfirmedCases', f'Confirmed cases for {country}', ax[0])\n    lineplot_for_case(test_afg, 'Fatalities', f'Fatalities for {country}', ax[1])    ","de31219d":"plots_for_country('Italy')\nplots_for_country('US')\nplots_for_country('Russia')\nplots_for_country('France')\nplots_for_country('Australia')","f2b5ec4d":"submission['ConfirmedCases'] = np.round(yhat_confirmed).astype(int)\nsubmission['Fatalities'] = np.round(yhat_fatalities).astype(int)\n\nsubmission.to_csv('submission.csv', header=True, index=False)\nsubmission.head(15)","e068fcc1":"![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F518134%2F485aa04e87e4e45c91815101784c6d95%2Fcorona-4930541_1280.jpg?generation=1585438527494582&alt=media)\n\n\n# WEEK 4\n\n### The Challenge\n\nKaggle is launching a companion COVID-19 forecasting challenges to help answer a subset of the NASEM\/WHO questions. While the challenge involves forecasting confirmed cases and fatalities between April 15 and May 14 by region, the primary goal isn't only to produce accurate forecasts. It\u2019s also to identify factors that appear to impact the transmission rate of COVID-19.\n\nYou are encouraged to pull in, curate and share data sources that might be helpful. If you find variables that look like they impact the transmission rate, please share your finding in a notebook.\n\n### Data\n\nIn this challenge, you will be predicting the cumulative number of confirmed COVID19 cases in various locations across the world, as well as the number of resulting fatalities, for future dates.\n\n\n### Thanks to great notebook, used as start: \n\nhttps:\/\/www.kaggle.com\/corochann\/covid-19-current-situation-on-may-daily-update\n\n\nI did not participated in previous competitions, so if I miss anything, let me kindly know :)","5cf2af7a":"### Submission","c403dce9":"### Cleanup and feature engineering","afc23391":"### Let's plot the results for some countries","02fbdeb5":"### LGB Model","9a103823":"### XGB Model","31812e93":"### missing data"}}