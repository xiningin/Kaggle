{"cell_type":{"80b28da9":"code","d5f1785b":"code","c1ff6588":"code","3d28ef6b":"code","ad43b03a":"code","34274b6a":"code","9ecba3fc":"code","3aa7f1f1":"code","b71db52b":"code","2d1f861a":"code","6d8a0fe2":"code","2eaf0f52":"code","36316d0a":"code","16aba548":"code","57052fbb":"markdown","be63c979":"markdown","c283e05a":"markdown","1e84dd61":"markdown","6a924a31":"markdown","c6dce7a0":"markdown","7e91e0e2":"markdown","7b8fdd41":"markdown","c6e647ad":"markdown","bf9be18c":"markdown","ab482aac":"markdown","14078ea8":"markdown","ee761220":"markdown","654bd7cd":"markdown","02009f33":"markdown","cf9af8b6":"markdown","76ee7baa":"markdown"},"source":{"80b28da9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d5f1785b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom mlxtend.classifier import StackingCVClassifier\nfrom sklearn.model_selection import cross_validate","c1ff6588":"import pandas as pd\ntrain = pd.read_csv('..\/input\/learn-together\/train.csv', index_col = 'Id')\ntest = pd.read_csv('..\/input\/learn-together\/test.csv', index_col = 'Id')\ntrain.columns\ntrain.info()\ntrain.head()","3d28ef6b":"# Make a copy of train df for ML experiments\ntrain_2 = train.copy()\ntrain_2.columns","ad43b03a":"# Separate feature and target arrays as X and y\nX = train_2.drop('Cover_Type', axis = 1)\ny=train_2.Cover_Type\nprint(X.columns)\ny[:5]","34274b6a":"# Split X and y into Train and Validation sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X,y,test_size = 0.2,random_state = 99)","9ecba3fc":"# Plug in Random Forest Best Estimator Parameters\nrf = RandomForestClassifier(n_estimators = 1930, \n                            min_samples_split = 5, \n                            min_samples_leaf = 1, \n                            max_features = 0.3, \n                            max_depth = 46, \n                            bootstrap = False,\n                            random_state=42)\nrf.fit(X_train,y_train)\ny_pred_rf = rf.predict(X_val)\nprint('Random Forest Best Estimator validation set accuracy is: ', accuracy_score(y_val,y_pred_rf))","3aa7f1f1":"# Plug in Extra Trees Best Estimator Parameters\nextra_trees = ExtraTreesClassifier(n_estimators = 3162,\n                                   min_samples_split = 5, \n                                   min_samples_leaf = 1, \n                                   max_features = 0.5,\n                                   max_depth = 464, \n                                   bootstrap = False,\n                                   random_state=42)\nextra_trees.fit(X_train,y_train)\ny_pred_extra = extra_trees.predict(X_val)\nprint('Extra Trees Best Estimator validation set accuracy is: ', accuracy_score(y_val,y_pred_extra))","b71db52b":"# Plug in LightGBM Best Estimator Parameters\nlgbm = LGBMClassifier(random_state = 42,\n                      n_estimators = 268, \n                      min_samples_split = 5, \n                      min_data_in_leaf = 1, \n                      max_depth = 21, \n                      learning_rate = 0.05, \n                      feature_fraction = 0.5, \n                      bagging_fraction = 0.5 , \n                      is_training_metric = True)\nlgbm.fit(X_train,y_train)\ny_pred_lgbm = lgbm.predict(X_val)\nprint('LightGBM Best Estimator validation set accuracy is: ', accuracy_score(y_val,y_pred_lgbm))","2d1f861a":"# Plug in LightGBM 'Manual' Potential Best Estimator parameters\nlgbm = LGBMClassifier(random_state=42,\n                      n_estimators=3162, \n                      min_samples_split=5, \n                      min_data_in_leaf=1, \n                      max_depth=21, \n                      learning_rate=0.05,\n                      feature_fraction=0.9, \n                      bagging_fraction=0.6 , \n                      is_training_metric = True)\nlgbm.fit(X_train,y_train)\ny_pred_lgbm = lgbm.predict(X_val)\nprint('LightGBM Best Estimator validation set accuracy is: ', accuracy_score(y_val,y_pred_lgbm))","6d8a0fe2":"stack = StackingCVClassifier(classifiers=[rf,\n                                         extra_trees,\n                                         lgbm],\n                            use_probas=True,\n                            meta_classifier=extra_trees, random_state = 42)\n\nstack.fit(X_train,y_train)\nprint('Stacking Classifier Cross-Validation accuracy scores are: ',cross_val_score(stack,X_train,y_train, cv = 5))\ny_pred_stack = stack.predict(X_val)\nprint('Stacking Classifier validation set accuracy is: ', accuracy_score(y_val,y_pred_stack))\n","2eaf0f52":"# Feature Importances from Random Forest Best Estimator\nrf_importance = pd.DataFrame(list(zip(X_train.columns, list(rf.feature_importances_))), columns = ['Feature', 'Importance'])\nrf_importance.sort_values('Importance', ascending = False)","36316d0a":"# Feature Importances from Extra Trees Best Estimator\nextra_importance = pd.DataFrame(list(zip(X_train.columns, list(extra_trees.feature_importances_))), columns = ['Feature', 'Importance'])\nextra_importance.sort_values('Importance', ascending = False)","16aba548":"# Feature Importances from LGBM Best Estimator\nlgbm_importance = pd.DataFrame(list(zip(X_train.columns, list(lgbm.feature_importances_))), columns = ['Feature', 'Importance'])\nlgbm_importance.sort_values('Importance', ascending = False)","57052fbb":"# Extra Trees Best Estimator","be63c979":"## Feature Importances from Random Forest Best Estimator","c283e05a":"# Random Forest Best Estimator ","1e84dd61":"# Training Data Preparation","6a924a31":"Wow..The stacking classifier seems to have done a bit better than the individual classifiers. We could repeat this with different other classifiers to see how they 'stack' up against other stacks!! For now, we have a reasonable baseline which seems to be decent enough to give us a tough threshold for feature selection. ","c6dce7a0":"# Up Next:\n\nWe will experiment with different types of feature selection approaches and see how they provide us good ways to improve performance. For one of them based on feature importances, we can use the above feature importances to filter out some features and apply hyperparameter tuning for the remaining features. That will be for the next kernel.","7e91e0e2":"# Feature Importances\nWe will get the feature importances for the 3 individual Tree based best estimator classifiers","7b8fdd41":"# LightGBM Best Estimator","c6e647ad":"# Objective of this Kernel: \n\nWe earlier performed hyperparameter tuning for Random Forest, Extremely Randomized Trees and LightGBM classifiers in my previous kernel -  [ForestML: All features - Tree Algo HP Optimization](http:\/\/www.kaggle.com\/aravindankrishnan\/forestml-all-features-tree-algo-hp-optimization). We did this for the training set without any feature selection to generate a baseline accuracy view before applying any feature selection measures. We will now just plug in the best estimators and check out our validation set accuracy for these 3 classifiers.\n\nOnce we have done that, we will also fit a stacking classifier that is an ensemble of the best estimators of these 3 classifiers (*ensemble of ensembles!!*) and check out its performance with respect to the individual best estimators.\n","bf9be18c":"## Feature Importances from Extra Trees Best Estimator","ab482aac":"# Stacking Classifier","14078ea8":"Since this is a new kernel, we need to run some code for getting the training data in place for ML.","ee761220":"Wow! This manual pick seemed a decent idea. However, for future purposes, i will try to pick different grids with Random Search and with much less n_iter so that i can 'sample test' the param grids. once i feel i have the right grid, go in for our 300 model execution marathon run (overnight, if needed!!)","654bd7cd":"## Feature Importances from LightGBM Best Estimator","02009f33":"Plug in the best hyper parameters for Random Forest, Extra Trees and LightGBM found in my previous kernel - [ForestML: All features - Tree Algo HP Optimization](http:\/\/www.kaggle.com\/aravindankrishnan\/forestml-all-features-tree-algo-hp-optimization)","cf9af8b6":"## Stacking Classifier - Combine best estimators of Random Forest, Extra Trees and LightGBM\nWe will now build a Stacking classifier based on the above 3 classifiers and see if it outperforms all or some of the individual estimators. We will use StackingCVClassifier which also performs cross validation to ensure robustness of the model predictions. For a quick reading, You can go through [StackingCVClassifier ](http:\/\/rasbt.github.io\/mlxtend\/user_guide\/classifier\/StackingCVClassifier\/). We need to specify a meta classifier that will combine the individual classifiers. We can use any model here as the meta classifier, not just the classifiers we are using in the stack.\n\nI will plan to use extra trees for 2 reasons\n\n1. Faster execution as even split points are random, so much faster execution\n2. If we want to predict unseen sets, we should probably have more diverse trees which Extra trees can build on account of its randomized execution.\n\nI will perform 5 fold CV on the stack classifer to get a grasp on the training performance.","76ee7baa":"I got a number of warnings during LGBM execution that i left num_leaves parameter default value while my max_depth was way larger such that 2^maxdepth was way larger than num_leaves. So my accuracy was going to go for a toss. For LGBM Classifier, num_leaves is a more sensible parameter since trees are grown by leaves first than by level as is the case with other boosting models. The max_depth is only to avoid over fitting as the leaves can go very deep in a lightGBM model. One option is to choose a different max_depth grid and set num_leaves to a pretty large value so that num_leaves is able to optimize for the highest accuracy, which means rerunning the HP optimization for LGBM, which will be atleast 2 hours.\n\nFor now, i am going with a simple hack. From the fitted models, Choose the best combination of accuracy and NLL one that has much more number of estimators so that we can be a little more confident of accuracy. So i just manually analyze the 300 fitted models' estimator, accuracy and NLL and pick out a candidate 'best' estimator manually."}}