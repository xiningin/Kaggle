{"cell_type":{"ce4a274e":"code","886bf69c":"code","9ee1ac31":"code","f6d407d2":"code","a50e2190":"code","f9372faa":"code","84185a14":"code","1d6d4b49":"code","02724874":"code","1e83cd58":"code","9652b1c1":"code","23bbf957":"code","b9e87fe4":"code","06f06770":"code","61cfe5ba":"code","fcfe9e2f":"code","d7993125":"code","d998761a":"code","aa877a4e":"code","ecffb30e":"code","06973ea8":"code","e65160c7":"markdown","f0d87f86":"markdown","b46edbc0":"markdown"},"source":{"ce4a274e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nimport gc\n\nfrom sklearn.model_selection import train_test_split\nimport optuna\n\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nimport gc","886bf69c":"# rapids installation (make sure to turn on GPU)\nimport sys\n!cp ..\/input\/rapids\/rapids.0.16.0 \/opt\/conda\/envs\/rapids.tar.gz\n!cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz > \/dev\/null\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\/site-packages\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + sys.path\n\nimport cudf","9ee1ac31":"%%time\ntrain_df = cudf.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/train.csv\")\ntest_df = cudf.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/test.csv\")\nsample_df = cudf.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv\")","f6d407d2":"train_df = train_df.to_pandas()\ntest_df = test_df.to_pandas()\nsample_df = sample_df.to_pandas()","a50e2190":"print('Training Data Shape.....',train_df.shape)\nprint(\"Testing dat shape....\",test_df.shape)","f9372faa":"fig, ax = plt.subplots(figsize=(5, 5))\npie = ax.pie([len(train_df), len(test_df)],\n             labels=[\"Train dataset\", \"Test dataset\"],\n             colors=[\"salmon\", \"teal\"],\n             textprops={\"fontsize\": 15},\n             autopct='%1.1f%%')\nax.axis(\"equal\")\nax.set_title(\"Dataset length comparison\", fontsize=18)\nfig.set_facecolor('white')\nplt.show();","84185a14":"train_df.head()","1d6d4b49":"fig, ax = plt.subplots(figsize=(10, 7.5))\nsns.countplot(x='target',data=train_df)\nplt.title(\"Target values distribution\", fontsize=20, pad=15)\nplt.ylabel(\"Amount of values\", fontsize=14, labelpad=15)\nplt.xlabel(\"Target value\", fontsize=14, labelpad=10)\nplt.show()","02724874":"features = [x for x in train_df.columns if x[0]==\"f\"]\n\n# df = pd.concat([train_df[features], test_df[features]], axis=0)\n# df.reset_index(inplace=True, drop=True)\n\n# unique_values = df[features].nunique() < 10\n# cat_features = unique_values[unique_values==True].index\n# unique_values = df[features].nunique() > 10\n# num_features = unique_values[unique_values==True].index\n\n# print(f\"There are {len(cat_features)} categorical features: {cat_features}\")\n# print(f\"There are {len(num_features)} continuous features: {num_features}\")","1e83cd58":"train_df.head()","9652b1c1":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn import metrics\nimport gc\nimport xgboost as xgb\nimport lightgbm as lgb\n\npd.set_option('display.max_columns', 200)","23bbf957":"!nvidia-smi","b9e87fe4":"MAX_TREE_DEPTH = 8\nTREE_METHOD = 'gpu_hist'\nITERATIONS = 1000\nSUBSAMPLE = 0.6\nREGULARIZATION = 0.1\nGAMMA = 0.3\nPOS_WEIGHT = 1\nEARLY_STOP = 10\n\nparams = {'tree_method': TREE_METHOD, 'max_depth': MAX_TREE_DEPTH, 'alpha': REGULARIZATION,\n          'gamma': GAMMA, 'subsample': SUBSAMPLE, 'scale_pos_weight': POS_WEIGHT, 'learning_rate': 0.05, \n          'silent': 1, 'objective':'binary:logistic', 'eval_metric': 'auc',\n          'n_gpus': 1}","06f06770":"%%time\nnfold = 5\nskf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=2019)\n\noof = np.zeros(len(train_df))\npredictions = np.zeros(len(test_df))\n\ntarget = 'target'\npredictors = train_df.columns.values.tolist()[1:-1]\n\ni = 1\nfor train_index, valid_index in skf.split(train_df, train_df.target.values):\n    print(\"\\nFold {}\".format(i))\n    xg_train = xgb.DMatrix(train_df.iloc[train_index][predictors].values,\n                           train_df.iloc[train_index][target].values,                           \n                           )\n    xg_valid = xgb.DMatrix(train_df.iloc[valid_index][predictors].values,\n                           train_df.iloc[valid_index][target].values,                           \n                           )   \n\n    \n    clf = xgb.train(params, xg_train, ITERATIONS, evals=[(xg_train, \"train\"), (xg_valid, \"eval\")],\n                early_stopping_rounds=EARLY_STOP, verbose_eval=False)\n    oof[valid_index] = clf.predict(xgb.DMatrix(train_df.iloc[valid_index][predictors].values)) \n    \n    predictions += clf.predict(xgb.DMatrix(test_df[predictors].values)) \/ nfold\n    i = i + 1\n\nprint(\"\\n\\nCV AUC: {:<0.2f}\".format(metrics.roc_auc_score(train_df.target.values, oof)))\n","61cfe5ba":"sub_df = pd.DataFrame({\"id\": test_df.id.values})\nsub_df[\"target\"] = predictions\nsub_df[:10]","fcfe9e2f":"sub_df.to_csv(\"sub_xgb.csv\",index=False)","d7993125":"del sub_df","d998761a":"\nparam = {\n        'num_leaves': 10,\n        'max_bin': 127,\n        'min_data_in_leaf': 11,\n        'learning_rate': 0.02,\n        'min_sum_hessian_in_leaf': 0.00245,\n        'bagging_fraction': 1.0, \n        'bagging_freq': 5, \n        'feature_fraction': 0.05,\n        'lambda_l1': 4.972,\n        'lambda_l2': 2.276,\n        'min_gain_to_split': 0.65,\n        'max_depth': 14,\n        'save_binary': True,\n        'seed': 1337,\n        'feature_fraction_seed': 1337,\n        'bagging_seed': 1337,\n        'drop_seed': 1337,\n        'data_random_seed': 1337,\n        'objective': 'binary',\n        'boosting_type': 'gbdt',\n        'verbose': 1,\n        'metric': 'auc',\n        'is_unbalance': True,\n        'boost_from_average': False,\n        'device': 'gpu',\n        'gpu_platform_id': 0,\n        'gpu_device_id': 0\n    }","aa877a4e":"%%time\nnfold = 2\n\n\ntarget = 'target'\npredictors = train_df.columns.values.tolist()[1:-1]\n\nskf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=2019)\n\noof = np.zeros(len(train_df))\npredictions = np.zeros(len(test_df))\n\ni = 1\nfor train_index, valid_index in skf.split(train_df, train_df.target.values):\n    print(\"\\nfold {}\".format(i))\n    xg_train = lgb.Dataset(train_df.iloc[train_index][predictors].values,\n                           label=train_df.iloc[train_index][target].values,\n                           feature_name=predictors,\n                           free_raw_data = False\n                           )\n    xg_valid = lgb.Dataset(train_df.iloc[valid_index][predictors].values,\n                           label=train_df.iloc[valid_index][target].values,\n                           feature_name=predictors,\n                           free_raw_data = False\n                           )   \n\n    \n    clf = lgb.train(param, xg_train, 5000, valid_sets = [xg_valid], verbose_eval=50, early_stopping_rounds = 50)\n    oof[valid_index] = clf.predict(train_df.iloc[valid_index][predictors].values, num_iteration=clf.best_iteration) \n    \n    predictions += clf.predict(test_df[predictors], num_iteration=clf.best_iteration) \/ nfold\n    i = i + 1\n\nprint(\"\\n\\nCV AUC: {:<0.2f}\".format(metrics.roc_auc_score(train_df.target.values, oof)))\n","ecffb30e":"sub_df = pd.DataFrame({\"id\": test_df.id.values})\nsub_df[\"target\"] = predictions\nsub_df[:10]","06973ea8":"sub_df.to_csv(\"lgbm.csv\",index=False)","e65160c7":"# LightGBM GPU","f0d87f86":"# XGBoost GPU","b46edbc0":"![image.png](attachment:260636f6-13ce-4b3c-8f80-6d3efcc46496.png)\n\nRapids is a great option to scale data processing on GPUs. With a lot of machine learning modelling moving to GPUs, Rapids enables to build end-to-end data science solutions on one or more GPUs.\n\nDocumentation: https:\/\/docs.rapids.ai\/"}}