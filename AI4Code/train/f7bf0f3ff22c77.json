{"cell_type":{"81e766dc":"code","2585a48c":"code","dc201aed":"code","bfef0201":"code","a50d148b":"code","a4919c19":"code","26b9d33b":"code","accb3cd3":"code","a9687969":"code","0aabd55d":"code","b841f0fe":"code","8fc289e0":"code","40ca9d37":"code","5f1121f2":"code","40472167":"code","728b6e68":"code","52a6811b":"code","a33891a6":"code","8b93b73b":"code","f9bc6a93":"code","c98a0a7c":"code","efa567bd":"code","c6f1a70f":"code","b0d77488":"code","7a5d38ea":"code","c4cf3f9c":"code","0fb1388c":"code","97dd50d6":"code","d1f207ba":"code","df1994fe":"code","a2daf3a2":"code","8ed47de6":"code","de7e137e":"code","29fc4e9c":"code","e717417c":"code","1d949646":"markdown","e71e8fb3":"markdown","73145e14":"markdown","c7de72ad":"markdown","b0c0308f":"markdown","21dca60b":"markdown","8ae0d5d3":"markdown","4cdbd1c6":"markdown","fc1867c8":"markdown","75943827":"markdown","51230be6":"markdown","7bdcab39":"markdown","5bd5dc96":"markdown","5f53e103":"markdown","783dcda1":"markdown","003cb60f":"markdown","bcd0fe41":"markdown","c92162b7":"markdown","8bc1d50e":"markdown","bbbe425f":"markdown","e422ef78":"markdown","b22d1334":"markdown","3b8209b1":"markdown","45c3d0ed":"markdown","324f561c":"markdown","8fa874d6":"markdown","a30e73da":"markdown","ad6b3b51":"markdown","9926afa3":"markdown","6eb3dd80":"markdown","609def74":"markdown","8e4b37cd":"markdown","1501f433":"markdown","7f7efd2a":"markdown","e144b545":"markdown","4d402a2e":"markdown","5caa5f69":"markdown","9fa9762a":"markdown","3214e21f":"markdown","9aa44d88":"markdown","c0168270":"markdown","6104108d":"markdown","a4b7d858":"markdown","84c3649e":"markdown","a4e01add":"markdown","fe618525":"markdown","639b2da1":"markdown","c400c413":"markdown","c06c67b5":"markdown","a83f2517":"markdown","b4e051ad":"markdown","2cb82243":"markdown","04c6fecc":"markdown","55767559":"markdown","1d1c2fd3":"markdown","72e8289b":"markdown","ebee2c14":"markdown","db7cc7ad":"markdown","3cb353af":"markdown","25d4d605":"markdown"},"source":{"81e766dc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns # data visualization library\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory\n# e.g. running the below will list the files in the input directory\nimport time\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","2585a48c":"data = pd.read_csv('..\/input\/data.csv')","dc201aed":"data.head()  # head method show only first 5 rows","bfef0201":"# feature names as a list\ncol = data.columns\nprint(col)","a50d148b":"# y includes our labels and x includes our features\ny = data.diagnosis\nlist = ['id', 'diagnosis', 'Unnamed: 32']\nx = data.drop(list, axis=1)\nx.head()","a4919c19":"f, ax = plt.subplots(1, 2, figsize=(12, 6))\n\ndata.diagnosis.value_counts().plot.pie(explode=[0, 0.1], autopct='%1.1f%%', ax=ax[0])\nsns.countplot('diagnosis', data=data, order=data['diagnosis'].value_counts().index, ax=ax[1])\nax[0].legend()\nax[1].legend()\nB, M = y.value_counts()\nprint(\"Number of Benign: \", B)\nprint(\"Number of Malignant: \", M)","26b9d33b":"x.describe()","accb3cd3":"data_dia = y\ndata = x\n# don't nomalize features\n\ndata = pd.concat([data_dia, data.iloc[:, 0:10]], axis=1)\nprint(data.shape)\n\ndata = pd.melt(data, id_vars=\"diagnosis\",\n                      var_name=\"features\",\n                      value_name=\"value\")\nprint(data.head())","a9687969":"plt.figure(figsize=(10, 10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data, split=True, inner=\"quart\")\nplt.xticks(rotation=90)","0aabd55d":"# first ten features\ndata_dia = y\ndata = x\ndata_n_2 = (data - data.mean()) \/ data.std()  # pd.std(): return sample standardization over requested axis\n\ndata = pd.concat([data_dia, data_n_2.iloc[:, 0:10]], axis=1)\nprint(data.shape)\n# data.head()\ndata = pd.melt(data, id_vars=\"diagnosis\",\n                      var_name=\"features\",\n                      value_name=\"value\")\nprint(data.head())\nprint(data.shape)","b841f0fe":"plt.figure(figsize=(10, 10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data, split=True, inner=\"quart\")\nplt.xticks(rotation=90)","8fc289e0":"# second ten features\ndata = pd.concat([y, data_n_2.iloc[:, 10:20]], axis=1)\ndata = pd.melt(data, id_vars=\"diagnosis\",\n                      var_name=\"features\",\n                      value_name=\"value\")\nplt.figure(figsize=(10, 10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data, split=True, inner=\"quart\")\nplt.xticks(rotation=90)","40ca9d37":"# Third ten features\ndata = pd.concat([y,data_n_2.iloc[:,20:31]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data,split=True, inner=\"quart\")\nplt.xticks(rotation=90)","5f1121f2":"# As an alternative of violin plot, box plot can be used\n# box plots are also useful in terms of seeing outliers\n# I do not visualize all features with box plot\n# In order to show you lets have an example of box plot\n# If you want, you can visualize other features as well.\n\n\n# violin plot\uc758 \ub300\uccb4\uc7ac\ub85c box plotdl \uc0ac\uc6a9\ub420 \uc218 \uc788\ub2e4\nplt.figure(figsize=(10, 10))\nsns.boxplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\nplt.xticks(rotation=90)","40472167":"sns.jointplot(x.loc[:, 'concavity_worst'], x.loc[:, 'concave points_worst'], kind=\"regg\", color=\"#ce1414\")","728b6e68":"sns.set(style=\"white\")\ndf = x.loc[:, ['radius_worst', 'perimeter_worst', 'area_worst']]\ng = sns.PairGrid(df, diag_sharey=False)\ng.map_lower(sns.kdeplot, cmap=\"Blues_d\")  # \uc544\ub798\ucabd\uc5d0\ub294 KDE plot\uc744 \uadf8\ub824\uc900\ub2e4\ng.map_upper(plt.scatter)  # \uc704\ucabd\uc5d0\ub294 scatter plot\uc744 \uadf8\ub824\uc900\ub2e4\ng.map_diag(sns.kdeplot, lw=3)","52a6811b":"sns.set(style=\"whitegrid\", palette=\"muted\")\ndata_dia = y\ndata = x\ndata_n_2 = (data-data.mean()) \/ (data.std())  # standardization\n\ndata = pd.concat([y, data_n_2.iloc[:, 0:10]], axis=1)\ndata = pd.melt(data, id_vars='diagnosis',\n                      var_name=\"features\",\n                      value_name='value')\nplt.figure(figsize=(10, 10))\ntic = time.time()\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\n\nplt.xticks(rotation=90)","a33891a6":"data = pd.concat([y,data_n_2.iloc[:,10:20]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\nplt.xticks(rotation=90)","8b93b73b":"data = pd.concat([y,data_n_2.iloc[:,20:31]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\ntoc = time.time()\nplt.xticks(rotation=90)\nprint(\"swarm plot time: \", toc-tic ,\" s\")","f9bc6a93":"# correlation map\nf, ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(x.corr(), annot=True, linewidth=.5, fmt='.1f', ax=ax)","c98a0a7c":"drop_list1 = ['perimeter_mean','radius_mean','compactness_mean','concave points_mean','radius_se','perimeter_se','radius_worst','perimeter_worst','compactness_worst','concave points_worst','compactness_se','concave points_se','texture_worst','area_worst']\nx_1 = x.drop(drop_list1,axis = 1 )        # do not modify x, we will use it later \nx_1.head()\n","efa567bd":"#correlation map\nf,ax = plt.subplots(figsize=(14, 14))\nsns.heatmap(x_1.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","c6f1a70f":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score, confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\n# split data train 70% and test 30%\nx_train, x_test, y_train, y_test = train_test_split(x_1, y, test_size=0.3, random_state=42)\n\n# random forest classifier with n_esitimate=10(default)\nclf_rf = RandomForestClassifier(random_state=43)\nclf_rf = clf_rf.fit(x_train, y_train)\n\nac = accuracy_score(y_test, clf_rf.predict(x_test))\nprint(\"Accuracy is: \", ac)\ncm = confusion_matrix(y_test, clf_rf.predict(x_test))\nsns.heatmap(cm, annot=True, fmt=\"d\")","b0d77488":"from sklearn.feature_selection import SelectKBest, chi2\n\n# find best scored 5 features\nselect_feature = SelectKBest(chi2, k=5).fit(x_train, y_train)","7a5d38ea":"print(\"Score list: \", select_feature.scores_)  # scores of features\nprint(\"Feature list: \", x_train.columns)","c4cf3f9c":"x_train_2 = select_feature.transform(x_train)\nx_test_2 = select_feature.transform(x_test)\n\n# random forest classifier with n_estimators=10 (default)\nclf_rf_2 = RandomForestClassifier()\nclf_rf_2 = clf_rf_2.fit(x_train_2, y_train)\nac_2 = accuracy_score(y_test, clf_rf_2.predict(x_test_2))\nprint(\"Accuracy is: \", ac_2)\ncm_2 = confusion_matrix(y_test, clf_rf_2.predict(x_test_2))\nsns.heatmap(cm_2, annot=True, fmt=\"d\")","0fb1388c":"from sklearn.feature_selection import SelectKBest, chi2\n\n# find best scored 5 features\nselect_feature = SelectKBest(chi2, k=10).fit(x_train, y_train)","97dd50d6":"x_train_2 = select_feature.transform(x_train)\nx_test_2 = select_feature.transform(x_test)\n# random forest classifier with n_estimators=10 (default)\nclf_rf_2 = RandomForestClassifier()\nclf_rf_2 = clf_rf_2.fit(x_train_2, y_train)\nac_2 = accuracy_score(y_test, clf_rf_2.predict(x_test_2))\nprint(\"Accuracy is: \", ac_2)\n# also print confusion matrix\ncm_2 = confusion_matrix(y_test, clf_rf_2.predict(x_test_2))\nsns.heatmap(cm_2, annot=True, fmt=\"d\")","d1f207ba":"from sklearn.feature_selection import RFE\n# Create the RFE object and rank each pixel\nclf_rf_3 = RandomForestClassifier()\nrfe = RFE(estimator=clf_rf_3, n_features_to_select=5, step=1)\nrfe = rfe.fit(x_train, y_train)","df1994fe":"print(\"Chosen best 5 feature by rfe: \", x_train.columns[rfe.support_])","a2daf3a2":"x_train_3 = x_train.loc[:, x_train.columns[rfe.support_]]\nx_test_3 = x_test.loc[:, x_test.columns[rfe.support_]]\n\nclf_rf_3 = RandomForestClassifier()\nclf_rf_3.fit(x_train_3, y_train)\n\nac_3 = accuracy_score(y_test, clf_rf_3.predict(x_test_3))\nprint(\"Accuracy: \", ac_3)\n# confusion matrics\ncm_3 = confusion_matrix(y_test, clf_rf_3.predict(x_test_3))\nsns.heatmap(cm_3, annot=True, fmt=\"d\")","8ed47de6":"from sklearn.feature_selection import RFECV\n\n# The \"accuracy\" scoring is proportional to the number of correct classifications\nclf_rf_4 = RandomForestClassifier() \nrfecv = RFECV(estimator=clf_rf_4, step=1, cv=5,scoring='accuracy')   #5-fold cross-validation\nrfecv = rfecv.fit(x_train, y_train)\n\nprint('Optimal number of features :', rfecv.n_features_)\nprint('Best features :', x_train.columns[rfecv.support_])","de7e137e":"# Plot number of features vs cross-validation scores\nimport matplotlib.pyplot as plt\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score of number of selected feature\")\nplt.plot(range(1, len(rfecv.grid_scores_) +1), rfecv.grid_scores_)\nplt.show()    ","29fc4e9c":"clf_rf_5 = RandomForestClassifier()      \nclr_rf_5 = clf_rf_5.fit(x_train,y_train)\nimportances = clr_rf_5.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in clf_rf.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(x_train.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\n\nplt.figure(1, figsize=(14, 13))\nplt.title(\"Feature importances\")\nplt.bar(range(x_train.shape[1]), importances[indices],\n       color=\"g\", yerr=std[indices], align=\"center\")\nplt.xticks(range(x_train.shape[1]), x_train.columns[indices],rotation=90)\nplt.xlim([-1, x_train.shape[1]])\nplt.show()","e717417c":"# split data train 70 % and test 30 %\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n#normalization\nx_train_N = (x_train-x_train.mean())\/(x_train.max()-x_train.min())\nx_test_N = (x_test-x_test.mean())\/(x_test.max()-x_test.min())\n\nfrom sklearn.decomposition import PCA\npca = PCA()\npca.fit(x_train_N)\n\nplt.figure(1, figsize=(14, 13))\nplt.clf()\nplt.axes([.2, .2, .7, .7])\nplt.plot(pca.explained_variance_ratio_, linewidth=2)\nplt.axis('tight')\nplt.xlabel('n_components')\nplt.ylabel('explained_variance_ratio_')","1d949646":"# README\n## TLDR;\n* Dear, beginner\n* who want to explore EDA from basic\n* who want to do Data Analysis process from scratch\n* who are beginner like me :)","e71e8fb3":"## 5) Tree based feature selection and random forest classification\n> <http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html>\nIn random forest classification method there is a **feature_importances_** attributes that is the feature importances (the higher, the more important the feature). **!!! To use feature_importance method, in training data there should not be correlated features. Random forest choose randomly at each iteration, therefore sequence of feature importance list can change.**\n","73145e14":"In swarm plot, I will do three part like violin plot not to make plot very complex appearance\n\n### swarm plot\n* Draw a categorical scatterplot with non-overlapping points.\n* categorical scatter plot\uc744 \ud3ec\uc778\ud2b8\uac00 \uacb9\uce58\uc9c0 \uc54a\uac8c \uadf8\ub824\uc90d\ub2c8\ub2e4\n    * \uc5ec\uae30\uc11c \ud3ec\uc778\ud2b8\uac00 \uacb9\uce58\uc9c0 \uc54a\uac8c \uadf8\ub824\uc900\ub2e4 \ud568\uc744 \ub2e4\uc2dc \ud55c\ubc88 \uc0dd\uac01\ud574\ubcf4\uc2dc\uba74 \uc88b\uc744 \uac83 \uac19\uc2b5\ub2c8\ub2e4!","c7de72ad":"* variance\ub97c \ubcf4\ub2e4 \uc120\uba85\ud558\uac8c \uad00\uce21\ud560 \uc218 \uc788\uac8c \ub418\uc5c8\ub2e4\n* \ub9c8\uc9c0\ub9c9 plot\uc758 area_worst\ub294 malignant\uc640 benign\uc744 \ub300\ubd80\ubd84 \ubd84\ub958\ud574\uc8fc\ub294 \uac83\uc744 \ubcfc \uc218 \uc788\ub2e4\n* \ub450 \ubc88\uc9f8 plot\uc758 smoothness_se\uc758 \uacbd\uc6b0 malignant\uc640 benign\uc744 \uc81c\ub300\ub85c \ubd84\ub958\ud558\uc9c0 \ubabb\ud558\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4","b0c0308f":"# Feature Selection and Random Forest Classification\n\n>Today our purpuse is to try new cocktails. For example, we are finaly in the pub(\ud83c\udf7a) and we want to drink different tastes. Therefore, we need to compare ingredients of drinks. If one of them includes lemon, after drinking it we need to eliminate other drinks which includes lemon so as to experience very different tastes.","21dca60b":"> In this part we will select feature with different methods that are feature selection with correlation, univariate feature selection, recursive feature elimination (RFE), recursive feature elimination with cross validation (RFECV) and tree based feature selection. We will use random forest classification in order to train our model and predict. ","8ae0d5d3":"They looks cool right. And you can see variance more clear. Let me ask you a question, **in these three plots which feature looks like more clear in terms of classification.** In my opinion **area_worst** in last swarm plot looks like malignant and benign are seprated not totaly but mostly. Hovewer, **smoothness_se** in swarm plot 2 looks like malignant and benign are mixed so it is hard to classfy while using this feature.","4cdbd1c6":"Finally, we find best 11 features that are **texture_mean, area_mean, concavity_mean, texture_se, area_se, concavity_se, symmetry_se, smoothness_worst, concavity_worst, symmetry_worst and fractal_dimension_worst** for best classification. Lets look at best accuracy with plot.\n","fc1867c8":"## 2) Univariate feature selection and random forest classification\nIn univariate feature selection, we will use SelectKBest that removes all but the k highest scoring features.\n<http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest>","75943827":"* \uc6b0\ub9ac\ub294 \uc0c1\uad00\uad00\uacc4\uac00 \ub192\uc740 feature\ub4e4\uc744 \uc81c\uac70\ud574\uc8fc\ub294 \ubc29\uc2dd\uc73c\ub85c feature\ub97c \uc120\ud0dd\ud574\uc8fc\uc5c8\ub2e4 (feature selection)\n* \uc774\ub7ec\ud55c \ubc29\ubc95\uc774 \ud6a8\uacfc\uc801\uc778\uc9c0 random forest \ubd84\ub958\uae30\ub97c \ud1b5\ud574 \ud655\uc778\ud574\ubcf4\uc790","51230be6":"* normalization\uc744 \ud558\uae30 \uc804\/\ud6c4\ub85c \ube44\uad50\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4","7bdcab39":"* \ud574\uc11d\ud574\ubcf4\uc790\n    * testure_mean\uc5d0\uc11c Malignant(\uc545\uc131)\uacfc Benign\uc758 median(\uc911\uc704\uac12)\uc740 \ubd84\ub958\ub418\ub294 \uac83 \ucc98\ub7fc \ubcf4\uc778\ub2e4. \ud574\ub2f9 feature\ub97c \ubd84\ub958\uae30\ub97c \ub9cc\ub4e4\ub54c \uace0\ub824\ud558\uba74 \uc88b\uc744 \uac83 \uac19\ub2e4\n    * \ubc18\uba74 fractal_dimension_mean feature\uc5d0\uc11c Malignant\uc640 Benign\uc758 median(\uc911\uc704\uac12)\uc740 \uc545\uc131\uacfc \uc815\uc0c1\uc744 \uad6c\ubd84\ud558\uc9c0 \ubabb\ud558\ub294 \uac83\ucc98\ub7fc \ubcf4\uc778\ub2e4\n \n* \ub2e4\ub978 feature\ub4e4\ub3c4 \uc0b4\ud3b4\ubcf4\ub3c4\ub85d \ud55c\ub2e4","5bd5dc96":"## 4) Recursive feature elimination with cross validation and random forest classification\n> <http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.RFECV.html>\nNow we will not only **find best features** but we also find **how many features do we need** for best accuracy.","5f53e103":"![](https:\/\/preview.ibb.co\/bKsv9k\/k.jpg)\n# INTRODUCTION\n* \ubcf8 \ub178\ud2b8\ubd81\uc740 <a href=\"https:\/\/www.kaggle.com\/kanncaa1\/feature-selection-and-data-visualization\">Feature Selection and Data Visualization<\/a> \ub178\ud2b8\ubd81\uc744 \ubc88\uc5ed\ud55c \ub178\ud2b8\ubd81\uc785\ub2c8\ub2e4\n    * Kaggle Notebook Grandmaster\uc758 \ub178\ud2b8\ubd81\uc744 \ud544\uc0ac\ud558\uba70 \ub370\uc774\ud130 \ubd84\uc11d\uc758 \uae30\ubcf8\uc744 \ub2e4\uc9c8 \uc218 \uc788\uc2b5\ub2c8\ub2e4\n    * \ub370\uc774\ud130 \ubd84\uc11d\uc744 \uc6b0\ub9ac\uac00 \uc88b\uc544\ud558\ub294 pub(\ud83c\udf7a)\uc5d0 \ube44\uc720\ud558\ub294 \ub4f1 \ubd84\uc11d\uc758 \uc7ac\ubbf8\ub97c \ub290\ub084 \uc218 \uc788\uc2b5\ub2c8\ub2e4!\n    \n    \n* \ubcf8 \ub178\ud2b8\ubd81\uc5d0\uc11c\ub294 \ub2e4\ub978 \ub178\ud2b8\ubd81\uacfc \ub2ec\ub9ac feature visualization\uacfc selection\uc5d0 \ucd08\uc810\uc744 \ub9de\ucda5\ub2c8\ub2e4\n    * Visualization\n        * Violin plot, Joint plot, pair grid, swarm plot, heatmap \ub4f1\uc744 \ud1b5\ud574 \ub370\uc774\ud130\uc5d0\uc11c insight\ub97c \uc5bb\uc2b5\ub2c8\ub2e4\n    * feature selection\n        * correlation, univariate feature selection, recursive feature elimination, recursive feature elimination with cross validation, random forest\uc640 \ud568\uaed8 tree based feature selection\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4\n\n\n* \ud55c\ud3b8 \uc8fc\uc131\ubd84 \ubd84\uc11d\uc744 \uc704\ud574 PCA\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4 \n\n> **Enjoy your data analysis!!!**<br>\n> \ubcf8 \ub178\ud2b8\ubd81\uc744 \ud1b5\ud574 \ub370\uc774\ud130 \ubd84\uc11d\uc744 \uc990\uae30\uc2dc\uae38 \ubc14\ub78d\ub2c8\ub2e4!\n\n","783dcda1":"* \uba87 \uac1c\uc758 feature\ub97c \ub354 \uc0b4\ud3b4\ubcf4\uba74...\n    * Compactness_mean\uacfc concavity_mean, concave points_mean\uc5d0\uc11c concavity_mean\uc744 \uc120\ud0dd (swarm plot)\uc744 \ubcf4\uc790\n    * radius_se, perimeter_se, area_se\ub85c\ubd80\ud130 area_se \uc120\ud0dd (swarm plot\uc744 \ubcf4\uc790)\n    * radius_wrost, perimeter_worst, area_worst\uc5d0\uc11c area_worst \uc120\ud0dd\n    * concavity_worst, concavity_worst, concave point_worst\uc5d0\uc11c oncavity_worst \uc120\ud0dd\n    * ...\n","003cb60f":"* \uc624\ub298 \uc6b0\ub9ac\ub294 pub\uc5d0\uc11c \uc0c8\ub85c\uc6b4 cocktail\uc744 \uc2dc\ub3c4\ud574\ubcfc \uac83\uc774\ub2e4\n    * \ub2e4\ub978 \uc885\ub958\uc758 drink\ub97c \uc2dc\ub3c4\ud560 \uac83\uc774\ub2e4\n    * drink\uc758 \uc131\ubd84\uc744 \ube44\uad50\ud574\uc11c lemen\uc744 \uba39\uc5c8\uc73c\uba74 \ub2e4\uc74c \uc74c\ub8cc\ub85c\ub294 lemon\uc774 \uc81c\uc678\ub41c \uc74c\ub8cc\ub97c \uc2dc\ud0a4\ub294 \uc2dd\uc73c\ub85c \ub9d0\uc774\ub2e4","bcd0fe41":"* heatmap\uc744 \ubcf4\uba74 raidus_mean feature\uc640 perimeter_mean, area_mean\uc740 \ub192\uc740 \uc591\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \ubcf4\uc5ec\uc8fc\uace0 \uc788\ub2e4\n* \uc774\uc911\uc5d0\uc11c \ud544\uc790\ub294 radius_mean\uc744 \uc9c1\uad00\uc801\uc73c\ub85c \uc120\ud0dd\ud588\ub2e4\uace0 \ud55c\ub2e4\n    * Why?\n        * swarm plot\uc744 \uc0b4\ud3b4\ubcf4\uba74 radius_mean feature\uac00 clear\ud558\uac8c malignant\uc640 benign\uc744 \uad6c\ubd84\ud574\uc918\uc11c\ub77c\uace0 \ud55c\ub2e4\n        * (\ub0b4 \uc0dd\uac01\uc5d4) perimeter_mean\uacfc area_mean\ub3c4 \ub9c8\ucc2c\uac00\uc9c0\uc778 \uac83 \uac19\uc740\ub370...\u314e\u314e\n","c92162b7":"After drop correlated features, as it can be seen in below correlation matrix, there are no more correlated features. Actually, I know and you see there is correlation value 0.9 but lets see together what happen if we do not drop it.","8bc1d50e":"Well, we choose our features but **did we choose correctly ?** Lets use random forest and find accuracy according to chosen features.","bbbe425f":"* pair plot\uc744 \uc0b4\ud3b4\ubcf4\uba74 \uc138 \ubcc0\uc218\uac00 \ub192\uc740 \uc0c1\uad00\uad00\uacc4\uac00 \uc788\ub2e4\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4","e422ef78":"# Data Analysis","b22d1334":"**There are 4 things that take my attention**\n\n1) There is an **id** that cannot be used for classificaiton<br>\n2) **Diagnosis** is our class label<br>\n3) **Unnamed: 32** feature includes NaN so we do not need it.<br>\n4) I do not have any idea about other feature names actually I do not need because machine learning is awesome **:)<br>**\n\n\nTherefore, drop these unnecessary features. However do not forget this is not a feature selection. This is like a browse a pub, we do not choose our drink yet !!!\n\n\n1) \ubd84\ub958\uc5d0\uc11c\ub294 \uc0ac\uc6a9\ub418\uc9c0 \uc54a\ub294 id feature\uac00 \uc788\ub2e4<br>\n2) Diagnosis\ub294 \uc6b0\ub9ac\uc758 target label\uc774\ub2e4<br>\n3) Unnamed: 32 feature\ub294 NaN\uac12\ub9cc \ud3ec\ud568\ud558\uace0 \uc788\ub2e4. \ubd84\uc11d \ub2e8\uacc4\uc5d0\uc11c\ub294 \ud544\uc694\uc5c6\uc744 \uac83 \uac19\ub2e4<br>\n4) \ub2e4\ub978 feature\ub4e4\uc5d0 \ub300\ud574\uc11c\ub294 \ud558\ub098\uc529 \uc0b4\ud3b4\ubcfc \ud544\uc694\uac00 \uc788\uc744 \uac83 \uac19\ub2e4","3b8209b1":"* correlation heatmap\uc744 \ud1b5\ud574 \uc0c1\uad00\uad00\uacc4\uac00 \ub192\uc740 feature\ub4e4\uc744 \uc81c\uac70\ud574\uc8fc\uc5c8\ub2e4\n* \uc0c1\uad00\uad00\uacc4\uac00 \ub192\uc740 feature\ub4e4\uc744 \uc81c\uac70\ud574\uc900 \ub4a4 \ub2e4\uc2dc heatmap\uc744 \uadf8\ub824\ubcf4\uc790","45c3d0ed":"* \uc131\ub2a5\uc774 \uc88b\uc740 \ubcc0\uc218\ub97c 10\uac1c \ucd94\ucd9c\ud55c\ub2e4\uba74 \ubaa8\ub378\uc758 \uc131\ub2a5\uc774 \ub354 \uc88b\uc544\uc9c8\uae4c?","324f561c":"Lets look at what we did up to this point. Lets accept that guys this data is very easy to classification. However, our first purpose is actually not finding good accuracy. Our purpose is learning how to make **feature selection and understanding data.** Then last make our last feature selection method.","8fa874d6":"* Standardization vs Normalization\n    * > The terms normalization and standardization are sometimes used interchangeably, but they usually refer to different things. Normalization usually means to scale a variable to have a values between 0 and 1, while standardization transforms data to have a mean of zero and a standard deviation of 1.\n    * by Googling\n    * Standardization\uc740 \ub370\uc774\ud130\uac00 mean 0, standard deviation 1\uc744 \uac16\ub3c4\ub85d \ub370\uc774\ud130\ub97c \ubcc0\ud615\ud574\uc8fc\ub294 \uac83\uc785\ub2c8\ub2e4\n    * \ubc18\uba74 Normalization\uc740 \ubcc0\uc218 \uac12\uc758 \ubc94\uc704\ub97c 0\uc5d0\uc11c 1\uc0ac\uc774\ub85c \uc2a4\ucf00\uc77c \ud574\uc8fc\ub294 \uac83\uc785\ub2c8\ub2e4","a30e73da":"* feature extraction\uc744 \uc704\ud574 PCA\ub97c \uc0ac\uc6a9\ud560 \uac83\uc774\ub2e4\n* PCA\uc758 \uc131\ub2a5\uc744 \ub192\uc774\uae30 \uc704\ud574\uc11c\ub294 \ub370\uc774\ud130\ub97c normalize \ud574\uc918\uc57c \ud55c\ub2e4","ad6b3b51":"\ub370\uc774\ud130\ub97c \ud55c \ubc88 \uc0b4\ud3b4\ubcf4\uc790. \uc5b4\ub5a4 insight\ub97c \uc5bb\uc744 \uc218 \uc788\uc744\uae4c?","9926afa3":"As you can seen in plot above, after 5 best features importance of features decrease. Therefore we can focus these 5 features. As I sad before, I give importance to understand features and find best of them. ","6eb3dd80":"Best 5 feature to classify is that **area_mean, area_se, texture_mean, concavity_worst and concavity_mean**. So lets se what happens if we use only these best scored 5 feature.","609def74":"> Like previous method, we will use 5 features. However, which 5 features will we use ? We will choose them with RFE method.","8e4b37cd":"Well, finaly we are in the pub and lets choose our drinks at feature selection part while using heatmap(correlation matrix).","1501f433":"**What if we want to observe all correlation between features?** Yes, you are right. The answer is heatmap that is old but powerful plot method.","7f7efd2a":"# Conclusion\n* \ubcf8 \ub178\ud2b8\ubd81\uc5d0\uc11c\ub294 feature selection\uacfc data visualization\uc744 \uc0c1\uc138\ud788 \ub2e4\ub918\uc2b5\ub2c8\ub2e4\n* \ucc98\uc74c \uc6b0\ub9ac\uac00 \ub9de\uc774\ud55c \ub370\uc774\ud130\ub294 33\uac1c\uc758 feature\ub97c \uac00\uc9c0\uace0 \uc788\uc5c8\uc9c0\ub9cc feature selection\uc744 \ud1b5\ud574 feature\uc758 \uc218\ub97c 33\uac1c\uc5d0\uc11c 5\uac1c\uae4c\uc9c0 \uc904\uc77c \uc218 \uc788\uc5c8\uc2b5\ub2c8\ub2e4. \uc774 \ub54c\uc758 \uc815\ud655\ub3c4\ub294 95%\uc5d0 \ub2ec\ud588\uc2b5\ub2c8\ub2e4\n* \uc624\ub298 \ubc30\uc6b4 \ubc29\ubc95\uc73c\ub85c \ub370\uc774\ud130 \ubd84\uc11d\uc744 \uc2dc\uc791\ud558\ub294 \uacc4\uae30\uac00 \ub418\uc5c8\uc73c\uba74 \uc88b\uaca0\uc2b5\ub2c8\ub2e4. \uac10\uc0ac\ud569\ub2c8\ub2e4!","e144b545":"* joint plot\uc744 \ud1b5\ud574 \uc0b4\ud3b4\ubcf8 \uacb0\uacfc \ud574\ub2f9 feature\ub4e4\uc774 \uc2e4\uc81c\ub85c \uc0c1\uad00\uad00\uacc4\uac00 \uc788\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc5c8\ub2e4\n* pearson value\ub294 correlation \uac12\uc73c\ub85c 1\uc774\uba74 \ub192\uc740 \uc591\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \uc758\ubbf8\ud55c\ub2e4\n    * \ud574\ub2f9 \ub3c4\ud45c\uc5d0\uc11c \ubcfc \uc218 \uc788\ub4ef\uc774 pearson value\uac00 0.86\uc73c\ub85c \ub9e4\uc6b0 \ub192\uc740 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4","4d402a2e":"* \uc774\uc81c \uc6b0\ub9ac\ub294 feature\ub97c \uac16\uac8c \ub418\uc5c8\ub2e4. \uadf8\ub7f0\ub370 \uc800\ub7ec\ud55c feature\ub4e4\uc774 \ubb34\uc5c7\uc744 \uc758\ubbf8\ud558\ub294\uc9c0 \uc5b4\ub5bb\uac8c \uc54c \uc218 \uc788\uc744\uae4c? feature\ub4e4\uc5d0 \ub300\ud574 \uc5bc\ub9c8\ub098 \uc54c\uc544\uc57c \ud560\uae4c?\n    * \uc815\ub2f5\uc740 \ubaa8\ub4e0 feature\uc5d0 \ub300\ud574 \uc54c \ud544\uc694\ub294 \uc5c6\ub2e4\ub294 \uac83\uc774\ub2e4\n    * \ub300\uc2e0\uc5d0 feature\uc758 variance, stardard deviation, number of sample \ub610\ub294 min\/max value \ub4f1\uc5d0 \ub300\ud574\uc11c\ub294 \ubc18\ub4dc\uc2dc \uc54c\uc544\uc57c \ud55c\ub2e4\n* variance, stardard deviation, number of sample, min\/max \ub4f1\uc758 \uc815\ubcf4\ub294 \ub370\uc774\ud130\uc5d0 \ub300\ud574 \uc774\ud574\ud558\ub294 \uac83\uc744 \ub3c4\uc640\uc900\ub2e4\n    * Pandas Dataframe\uc5d0\uc11c\ub294 \uce5c\uc808\ud558\uac8c\ub3c4 .describe()\uba54\uc11c\ub4dc\ub97c \ud1b5\ud574 \ub370\uc774\ud130\uc5d0 \ub300\ud55c \ud1b5\uacc4\uce58\ub97c \uc81c\uacf5\ud574\uc900\ub2e4\n","5caa5f69":"> \uadf8\ub807\ub2e4\uba74 3\uac1c \uc774\uc0c1\uc758 \ubcc0\uc218\ub294 \uc5b4\ub5bb\uac8c \ube44\uad50\ud560 \uc218 \uc788\uc744\uae4c?","9fa9762a":"> In this method we need to choose how many features we will use. For example, will k (number of features) be 5 or 10 or 15? The answer is only trying or intuitively. I do not try all combinations but I only choose k = 5 and find best 5 features.","3214e21f":"* area_mean\uacfc smoothness_mean\uc744 \ubcf4\uba74 \ucd5c\ub300\uac12\uc774 \uac01\uac01 2501\uacfc 0.163400\uc73c\ub85c \ud06c\uac8c \ucc28\uc774\ub098\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4\n* \ub530\ub77c\uc11c \uc2dc\uac01\ud654\uc640 feature selection, feature extraction, classification\uc744 \uc704\ud574\uc11c\ub294 standardization\uacfc normalization\uc744 \ud574\uc918\uc57c \ud55c\ub2e4","9aa44d88":"* Modeling it!","c0168270":"> \ub4dc\ub514\uc5b4 pub\uc5d0 \ub3c4\ucc29\ud588\ub2e4\n\n* \ud83c\udf7a  \uc6b0\ub9ac\uac00 \uc6d0\ud558\ub294 \ub4dc\ub9c1\ud06c\ub97c \uace0\ub974\uc790!\n* 'feature selection'\uc744 \ud558\uc790\ub294 \ub9d0\uc774\ub2e4!","6104108d":"* Chosen 5 best features by rfe is **texture_mean, area_mean, concavity_mean, area_se, concavity_worst**. \n* They are exactly similar with previous (selectkBest) method. Therefore we do not need to calculate accuracy again. \n* Shortly, we can say that we make good feature selection with rfe and selectkBest methods. However as you can see there is a problem, okey I except we find best 5 feature with two different method and these features are same but why it is **5**. \n* Maybe if we use best 2 or best 15 feature we will have better accuracy. \n* Therefore lets see how many feature we need to use with rfecv method.","a4b7d858":"Up to this point, we make some comments and discoveries on data already. If you like what we did, I am sure swarm plot will open the pub's door :) ","84c3649e":"* \ub3c4\ud45c\ub97c \ud574\uc11d\ud574\ubcf4\uc790\n    * box plot\uc744 \ubcf4\uba74 concavity_worst\uc640 concave point_worst\uac00 \ube44\uc2b7\ud574\ubcf4\uc778\ub2e4\n    * \uadf8\ub807\ub2e4\uba74 \uc2e4\uc81c\ub85c \uc0c1\uad00\uad00\uacc4\uac00 \uc788\ub294\uc9c0 \uc5b4\ub5bb\uac8c \uc54c \uc218 \uc788\uc744\uae4c?\n    * joint plot\uc744 \ud1b5\ud574 feature\ub97c \uc870\uae08 \ub354 \uae4a\uac8c \uc0b4\ud3b4\ubcf4\uc790","a4e01add":"## 3) Recursive feature elimination (RFE) with random forest\n> <http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.RFE.html>\nBasically, it uses one of the classification methods (random forest in our example), assign weights to each of features. Whose absolute weights are the smallest are pruned from the current set features. That procedure is recursively repeated on the pruned set until the desired number of features","fe618525":"* \uc774\uc81c\ub294 \ucd5c\uc801\uc758 feature\ubfd0\ub9cc\uc544\ub2c8\ub77c \uba87 \uac1c\uc758 feature\uac00 \ud544\uc694\ud55c\uc9c0\ub3c4 \uc54c\uc544\ubcf4\ub3c4\ub85d \ud558\uc790\n* \ucd5c\uace0\uc758 \uc131\ub2a5\uc744 \uc704\ud55c feature\uc758 \uac1c\uc218\ub294 \uba87 \uac1c\uc778\uc9c0 \uc54c\uc544\ubcf4\uc790!","639b2da1":"* selectKBest mothod\uc640 RFE\ub97c \ud1b5\ud574 5\uac1c\uc758 feature\ub97c \ucd94\ucd9c\ud560 \uc218 \uc788\uc5c8\ub2e4\n* \ub450 \ubc29\ubc95 \ubaa8\ub450 \uac19\uc740 feature\ub97c \ucd94\ucd9c\ud574\uc92c\uc73c\uba70 RandomForest Classifier\ub97c \ud1b5\ud574 95%\uc758 \uc131\ub2a5\uc744 \ub2ec\uc131\ud560 \uc218 \uc788\uc5c8\ub2e4\n* \ud558\uc9c0\ub9cc \uc5ec\uae30\uc5d0\ub294 \ud55c \uac00\uc9c0 \ubb38\uc81c\uc810\uc774 \uc874\uc7ac\ud55c\ub2e4\n    * 5\uac1c\uc758 feature\uac00 \ucd5c\uc801\uc758 feature\ub77c\uace0 \ud560 \uc218 \uc788\uc744\uae4c?\n    * rfecv \ubc29\ubc95\uc744 \ud1b5\ud574 \ucd5c\uc801\uc758 feature\ub97c \ucd94\ucd9c\ud574\ubcf4\uc790!","c400c413":"According to variance ration, 3 component can be chosen.","c06c67b5":"* \uc131\ub2a5\uc774 \ub354 \uc88b\uc544\uc84c\ub2e4!\n* \ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130 \uc11c\uce58\ub97c \ud1b5\ud574 \uac00\uc7a5 \uc88b\uc740 \uc131\ub2a5\uc758 \ubaa8\ub378\uc744 \ub9cc\ub4e4\uc218\ub3c4 \uc788\uc744 \uac83 \uac19\ub2e4\n    * \ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130 \uc11c\uce58\ub3c4 \ud55c \ubc88 \uc5f0\uc2b5\ud574\ubcf4\uc790","a83f2517":"Before violin and swarm plot we need to normalization or standirdization. Because differences between values of features are very high to observe on plot. I plot features in 3 group and each group includes 10 features to observe better.\n\n\n* violin plot\uacfc swarm plot\uc744 \uadf8\ub9ac\uae30 \uc804\uc5d0 normalization\uacfc standardization\uc744 \ud574\uc918\uc57c \ud55c\ub2e4\n    * \uc65c \ud574\uc57c\ud560\uae4c?\n        * normalization\uc744 \ud558\uc9c0 \uc54a\uace0 plot\uc744 \uadf8\ub824\ubcf4\uba74 normalization\uacfc standardization\uc774 '\ud544\uc218'\ub77c\ub294 \uc0ac\uc2e4\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc744 \uac83\uc774\ub2e4","b4e051ad":"Accuracy is almost 95% and as it can be seen in confusion matrix, we make few wrong prediction. \nNow lets see other feature selection methods to find better results.","2cb82243":"# Feature Extraction\n> <http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.PCA.html>\nWe will use principle component analysis (PCA) for feature extraction. Before PCA, we need to normalize data for better performance of PCA.\n ","04c6fecc":"## 1) Feature selection with correlation and random forest classification","55767559":"* tree \uae30\ubc18\uc758 feature selection\uacfc randomforest classifier\n* randomforest classifier\uc5d0\uc11c \uc81c\uacf5\ud558\ub294 feature_importance \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud558\ub824\uba74 \ud559\uc2b5 \ub370\uc774\ud130\uc5d0 \uc0c1\uad00\uad00\uacc4\uac00 \uc788\ub294 feature\uac00 \uc788\uc5b4\uc11c\ub294 \uc548\ub41c\ub2e4\n","1d1c2fd3":"* correlation\uc774 \ub192\uc740 feature\ub97c \uc81c\uac70\ud558\ub294 feature selection\uc5d0 \uc774\uc5b4 \ub2e4\ub978 feature selection \ubc29\ubc95\uc5d0 \ub300\ud574 \uc54c\uc544\ubcf4\uc790\n* Univariate feature selection\n    * 'univariable analysis' \ub450 \ubcc0\uc218 \uc0ac\uc774\uc758 \uad00\uacc4\ub97c \ubc1d\ud788\ub294 \ubc29\ubc95\n    * SelctKBest\ub97c \uc0ac\uc6a9\ud55c\ub2e4\n    * \"Select features according to the k highest scores\"\n    * k\uac1c\uc758 \ub192\uc740 \uc2a4\ucf54\uc5b4\ub97c \uac16\ub294 feature\ub97c \uc120\ud0dd\ud55c\ub2e4\n    * \uc774\uacf3 <a href='https:\/\/datascienceschool.net\/03%20machine%20learning\/14.03%20%ED%8A%B9%EC%A7%95%20%EC%84%A0%ED%83%9D.html'>link<\/a>\uc5d0\uc11c \uc790\uc138\ud55c \ub0b4\uc6a9\uc744 \uc54c\uc544\ubcfc \uc218 \uc788\ub2e4","72e8289b":"# Visualization\n\n* seaborn plot\uc744 \uc0ac\uc6a9\ud574 \uc2dc\uac01\ud654\ub97c \ud560 \uac83\uc774\ub2e4\n* \uc2dc\uac01\ud654 \ub2e8\uacc4\uc5d0\uc11c \uc8fc\uc758\ud560 \uc810\uc740 '\uc2dc\uac01\ud654'\uc640 'feature selection'\uc774 \ub2e4\ub974\ub2e4\ub294 \uac83\uc774\ub2e4","ebee2c14":"\nAccuracy is almost 96% and as it can be seen in confusion matrix, we make few wrong prediction. What we did up to now is that we choose features according to correlation matrix and according to selectkBest method. Although we use 5 features in selectkBest method accuracies look similar.\nNow lets see other feature selection methods to find better results.","db7cc7ad":"* \uc774\ubc88 \ud30c\ud2b8\uc5d0\uc11c\ub294 \ub2e4\ub978 \ubc29\ubc95\ub4e4\ub85c\ubd80\ud130 feature\ub97c \uc120\ud0dd\ud560 \uac83\uc774\ub2e4\n  * correlation, univariate feature selection, recursive feature elimination(RFE), recursive feature elimination with cross validation(EFECV), tree based feature selection \ub4f1\uc744 \uc54c\uc544\ubcf4\uc790\n  * \ubaa8\ub378\uc758 \ud559\uc2b5\uc740 random forest classifier\ub97c \uc0ac\uc6a9\ud560 \uac83\uc774\ub2e4","3cb353af":"> Before making anything like feature selection,feature extraction and classification, firstly we start with basic data analysis. <br>\nLets look at features of data.<br>\n\n\ud83d\udd90\ufe0f  Notice! (\uc2dc\uc791 \uc804!)<br>\n* feature selection, feature extraction, classification\uc744 \ud558\uae30 \uc804\uc5d0 \ud574\uc57c \ud560 \uac83\uc740 \ubb34\uc5c7\uc77c\uae4c?\n* feature selection, feature extraction, classification\uc744 \ud558\uae30 \uc804\uc5d0 \uc6b0\uc120 \ub370\uc774\ud130\uc758 feature\ubd80\ud130 \uc0b4\ud3b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4!\n","25d4d605":"* \uc704\uc758 \ub3c4\ud45c\uc5d0\uc11c \ubcfc \uc218 \uc788\ub294 \uac83\ucc98\ub7fc normalization\uc744 \ud558\uc9c0 \uc54a\uc73c\uba74 \uc2dc\uac01\ud654\ub97c \ud1b5\ud574 \ub370\uc774\ud130\ub97c \ud30c\uc545\ud558\uae30\uac00 \ub9e4\uc6b0 \uc5b4\ub835\ub2e4\n* \uc774\uc81c noramization\uc744 \ud55c \ub4a4\uc5d0 graph\ub97c \ud574\uc11d\ud574\ubcf4\uc790"}}