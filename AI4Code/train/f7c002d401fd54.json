{"cell_type":{"e36b51ca":"code","0afaa528":"code","db868af1":"code","5d3c869c":"code","8b3a5452":"code","34395b1d":"code","4129f3bb":"code","f40f567b":"code","da5f7da2":"code","10c18066":"code","020a334f":"code","ed7bc182":"code","9511724f":"code","65aaa46a":"code","6187fd1d":"code","d4f9fb73":"code","362246d5":"code","daeb892f":"code","9dc457d8":"code","b673690f":"code","1dfdb5c8":"code","062130a1":"code","6bf42db9":"code","89c7a407":"code","d64f0421":"code","7e9f8377":"code","8fe69f1a":"code","9022caac":"code","563be665":"code","c004ae48":"code","22731db9":"code","e5a4054b":"code","3c1da303":"code","11d34d92":"code","2a6aa91c":"code","a52a3a61":"code","634f0495":"code","5fb21451":"code","c3d32a10":"code","b5236e62":"code","9fbf5d29":"code","a9754f7e":"code","6664aa80":"code","4bdc8c31":"code","c6105762":"code","50637480":"code","979db007":"code","2719c675":"code","d57dbc6b":"code","c00cb498":"code","1c21ed2b":"code","d435c15f":"code","8914d7db":"code","bf2de513":"code","f8b4627b":"code","5d5142e7":"code","70313d63":"code","68feb5aa":"code","c7e4cf11":"code","3e7a952d":"code","133c72f3":"code","d40202d5":"code","44ece79d":"code","a0b00b72":"code","1e461c03":"code","d6fa77c1":"code","5fefe9c1":"code","fe45cd8f":"code","02a5ecde":"code","b38d31c8":"code","ea461921":"code","77e41396":"code","e8d3d2ca":"code","e30c475d":"code","08c2d535":"code","ab2c64b3":"code","86248516":"code","6d36dcec":"code","7df13cf8":"code","f3d0bf3e":"code","9002548b":"code","422dd19a":"code","1f07a3ab":"code","0942b626":"code","20308128":"code","15afbe5f":"code","de4b6a4a":"code","bf748efc":"code","42f7ace8":"code","c894d2d1":"code","f096e60f":"code","1fe02ffb":"code","4a78c021":"code","a5138387":"code","9efec97a":"code","c2e35dc3":"code","d68e4edd":"code","671ef7c4":"code","76affa9a":"markdown","fa5fe8c1":"markdown","0edc72f5":"markdown","7261a70a":"markdown","7ab4ef69":"markdown","3231bac0":"markdown","a3bbeeea":"markdown","1e80b86e":"markdown","be3beb1c":"markdown","b68414f5":"markdown","0b75f69b":"markdown","d072a1f6":"markdown","34c25543":"markdown","dd1b73bf":"markdown","ddff8d68":"markdown","aa595c49":"markdown","b5a70325":"markdown","1cbeb285":"markdown","d7a8ea29":"markdown","fc684f10":"markdown","aba4b462":"markdown","e25a24c6":"markdown","a9a8b08c":"markdown","e79dc714":"markdown","bca49b8a":"markdown","5994d186":"markdown","4a7b81cb":"markdown","5b38e6ff":"markdown","a261e8aa":"markdown","45e4d8f4":"markdown","9291da5a":"markdown","509430af":"markdown","0c8e6173":"markdown","9e5f3404":"markdown","1dc2648c":"markdown","66bf3818":"markdown","8fb9d486":"markdown","7c41c51b":"markdown","c0f0f794":"markdown","ad9f9446":"markdown","f1ed82de":"markdown","741168c4":"markdown","293e735b":"markdown","1a9e7316":"markdown","76fc49c9":"markdown","603f57b1":"markdown","7c9facd5":"markdown","50089f72":"markdown","32a404ec":"markdown","e7547415":"markdown","6447b825":"markdown","cf7cc542":"markdown","cc02055b":"markdown","b033a022":"markdown","6156818e":"markdown","55d02f1a":"markdown","7bdbef62":"markdown","6d5044e6":"markdown","fb5e6fa4":"markdown","ee970fca":"markdown","335272de":"markdown","a7ddce9c":"markdown","e7c15265":"markdown","4c085a79":"markdown","cfdf103a":"markdown","08db3d59":"markdown","151f805a":"markdown","22e26851":"markdown","979449e7":"markdown","eeb1d3a4":"markdown","668cc295":"markdown","ad3a0edb":"markdown"},"source":{"e36b51ca":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns #visualization of the variables\n\nfrom scipy.stats import chi2_contingency, ttest_ind\n\nfrom xgboost import XGBClassifier\nfrom sklearn import preprocessing\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0afaa528":"#Import Dataset to Pandas Dataframe\ndata = pd.read_csv(os.path.join(dirname, filename))","db868af1":"data.head()","5d3c869c":"data.tail()","8b3a5452":"data.columns","34395b1d":"data.info()","4129f3bb":"#Categorical info\ncat_feat = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']","f40f567b":"#remove categorial data from our set to create the model. Can be added encoded later in the process.\nnum_feat = data.drop(cat_feat, axis = 1)","da5f7da2":"data.shape\n#12 variables and 5110 observations","10c18066":"#data = data.drop('Id', axis = 1).describe()\ndata.describe()","020a334f":"num_feat = num_feat.drop('id', axis = 1)","ed7bc182":"num_feat.describe()","9511724f":"# num_feat.groupby(num_feat['bmi'].isnull()).mean()\nnum_feat.isna().sum()","65aaa46a":"#97.6 BMI? That is odd. Let's find out how many \nnum_feat[num_feat['bmi']==97.6]","6187fd1d":"data[data['bmi']==97.6]","d4f9fb73":"num_feat = num_feat[num_feat['bmi']!=97.6]","362246d5":"#Checking again\nnum_feat[num_feat['bmi']>40] #sort_values('bmi')","daeb892f":"num_feat.describe()","9dc457d8":"num_feat[num_feat.bmi > 40].describe()","b673690f":"num_feat.groupby('stroke').mean()","1dfdb5c8":"num_feat.groupby(num_feat.bmi > 40)[['stroke', 'hypertension', 'heart_disease']].sum()","062130a1":"bmi_over_40 = num_feat[num_feat['bmi'] > 40 ]","6bf42db9":"bmi_over_40[num_feat['stroke'] == 1 ].sort_values(by='age')","89c7a407":"plt.figure(figsize = (9,7))\nsns.scatterplot(x = 'bmi', y = 'avg_glucose_level', hue = 'stroke', data =bmi_over_40)\nplt.show()","d64f0421":"#check for unique values\ndata.nunique()","7e9f8377":"data.gender.unique()","8fe69f1a":"data.gender.value_counts()","9022caac":"data = data[data['gender']!='Other']","563be665":"data.smoking_status.unique()","c004ae48":"data.smoking_status.value_counts()","22731db9":"data[data['smoking_status'] == 'Unknown']","e5a4054b":"#smokers and goverment jobs\nsmokers = data[data['smoking_status']=='smokes']\n\nsmokers.work_type.value_counts(normalize=True)","3c1da303":"smokers['age'].groupby(smokers['age']).count()","11d34d92":"age_smokers = smokers['age'].groupby(smokers['age']).count()\n\nage_smokers[35:65].sort_values(ascending=False, axis=0)","2a6aa91c":"#smokers and goverment jobs\nunkn_smokers = data[data['smoking_status']=='Unknown']\n\nunkn_smokers.work_type.value_counts(normalize=True)","a52a3a61":"data.work_type.unique()","634f0495":"data.isnull().sum() ","5fb21451":"data['bmi'].isnull().sum()\/len(data)*100 ","c3d32a10":"#handling missing values\ndata['bmi'] = data['bmi'].fillna(round (data['bmi'].median(), 2))\ndata.isnull().sum()","b5236e62":"data.columns","9fbf5d29":"corelation = data.drop('id', axis = 1).corr()","a9754f7e":"plt.figure(figsize=(7,7))\nsns.heatmap(corelation, xticklabels =corelation.columns, yticklabels = corelation.columns, annot=True)\nplt.show()","6664aa80":"plt.figure(figsize=(7,7))\nsns.heatmap(corelation, xticklabels =corelation.columns, yticklabels = corelation.columns,\n            vmin=-1, vmax=1, center=0,annot=True)\nplt.show()","4bdc8c31":"data.columns","c6105762":"sns.countplot(x = 'smoking_status', data = data)\nplt.title(\"Count Plot for smoking status\")\nplt.show()","50637480":"sns.countplot(x = 'work_type', data = data)\nplt.title('Count Plot for Work Type')\nplt.show()","979db007":"num_data = num_feat","2719c675":"#Ploting the distribution of Stroke\nsns.countplot(x='stroke', data=num_data)\nplt.show()","d57dbc6b":"x = pd.DataFrame(num_data.groupby(['stroke'])['stroke'].count())\n\n# plot\nfig, ax = plt.subplots(figsize = (6,6), dpi = 70)\nax.barh([1], x.stroke[1], height = 0.7, color = 'red')\nplt.text(-1150,-0.08, 'Healthy',{'font': 'Serif','weight':'bold','Size': '16','style':'normal', 'color':'green'})\n#plt.text(5000,-0.08, '95%',{'font':'Serif','weight':'bold' ,'size':'16','color':'green'})\nplt.text(5000,-0.08, f\"{(num_data.shape[0]\/num_data.shape[0]*100) - (x.shape[0]\/(num_data.shape[0])*100)*100:.0f}%\" ,{'font':'Serif','weight':'bold' ,'size':'16','color':'green'})\nax.barh([0], x.stroke[0], height = 0.7, color = 'green')\nplt.text(-1000,1, 'Stroke', {'font': 'Serif','weight':'bold','Size': '16','style':'normal', 'color':'red'})\nplt.text(300,1, f\"{((x.shape[0]\/data.shape[0])*100)*100:.0f}%\",{'font':'Serif', 'weight':'bold','size':'16','color':'red'})\n\nfig.patch.set_facecolor('#f6f5f5')\nax.set_facecolor('#f6f5f5')\n\nplt.text(-1150,1.77, 'Percentage of People Having Strokes' ,{'font': 'Serif', 'Size': '25','weight':'bold', 'color':'black'})\nplt.text(4650,1.65, 'Stroke ', {'font': 'Serif','weight':'bold','Size': '16','weight':'bold','style':'normal', 'color':'red'})\nplt.text(5650,1.65, '|', {'color':'black' , 'size':'16', 'weight': 'bold'})\nplt.text(5750,1.65, 'Healthy', {'font': 'Serif','weight':'bold', 'Size': '16','style':'normal', 'weight':'bold','color':'green'})\nplt.text(-1150,1.5, 'It is a highly unbalanced distribution,\\nand clearly seen that 4 in 100 people are susceptible \\nto strokes.', \n        {'font':'Serif', 'size':'12.5','color': 'black'})\n\nax.axes.get_xaxis().set_visible(False)\nax.axes.get_yaxis().set_visible(False)\nax.spines['bottom'].set_visible(False)\nax.spines['left'].set_visible(True)\nax.spines['right'].set_visible(False)\nax.spines['top'].set_visible(False)","c00cb498":"plt.figure(figsize = (16,11))\nplt.subplot(2,3,1)\nsns.countplot(x = 'gender', data = data)\nplt.title('Countplot of Gender distribution')\n\nplt.subplot(2,3,2)\nsns.countplot(x = 'ever_married', data = data)\nplt.title('Countplot of Married status distribution')\n\nplt.subplot(2,3,3)\nsns.countplot(x='work_type', data = data)\nplt.title('Countplot of Work Type distribution')\n\nplt.subplot(2,3,4)\nsns.countplot(x = 'Residence_type', data = data)\nplt.title('Countplot of Residence type distribution')\n\nplt.subplot(2,3,5)\nsns.countplot(x = 'smoking_status',data = data)\nplt.title('Countplot of Smoking status distribution')\n\nplt.subplot(2,3,6)\nsns.countplot(x = 'heart_disease',data = data)\nplt.title('Countplot of Heart Disease distribution')\nplt.show()","1c21ed2b":"num_data = num_feat\n#handling missing values\nnum_data['bmi'] = num_data['bmi'].fillna(round (num_data['bmi'].median(), 2))","d435c15f":"# Checking the distribution of the predictor variables. \n# Here, we will use both distplot and boxplot as shown below. \n# Let us plot each variable to show its distribution in the dataset.\n#fig, ax = plt.subplots(figsize = (6,6), dpi = 70)\nplt.figure(1)\nplt.title('BMI Distribution before droping the abnormal entry')\nplt.subplot(121), sns.distplot(num_data['bmi'])\nplt.subplot(122), num_data['bmi'].plot.box(figsize=(16,5))\nplt.show()","8914d7db":"plt.figure(1)\nplt.title('Stroke Distribution with BMI over 40')\nplt.subplot(121), sns.distplot(bmi_over_40['bmi'])\nplt.subplot(122), bmi_over_40['bmi'].plot.box(figsize=(16,5))\nplt.show()\n","bf2de513":"plt.figure(1)\nplt.subplot(121), sns.distplot(data['age'])\nplt.subplot(122), data['age'].plot.box(figsize=(16,5))\nplt.show()","f8b4627b":"plt.figure(1)\nplt.subplot(121), sns.countplot(data['heart_disease'])\nplt.subplot(122), data['heart_disease'].plot.box(figsize=(16,5))\nplt.show()","5d5142e7":"plt.figure(1)\nplt.subplot(121), sns.distplot(data['avg_glucose_level'])\nplt.subplot(122), data['avg_glucose_level'].plot.box(figsize=(16,5))\nplt.show()","70313d63":"#sns.pairplot(corelation)\n\nimport warnings\nwarnings.filterwarnings('ignore')\nsns.pairplot(data, hue= 'stroke')\nplt.show()","68feb5aa":"sns.relplot(x='stroke', y='age', hue='gender', data=data ) \nplt.show()","c7e4cf11":"# Scatter Plot\nplt.figure(figsize = (9,7))\nsns.scatterplot(x = 'bmi', y = 'avg_glucose_level', hue = 'stroke', data =bmi_over_40)\nplt.title('Stroke cases - For those with a BMI over 40 ',y=1.05)\n\nplt.xlabel('BMI Level')\nplt.ylabel('Avg Glucose Level')\nplt.show()","3e7a952d":"def chi2_dependency(data_df, x,y):\n    ctab = pd.crosstab(data_df[x], data_df[y])\n    stat, p, dof, expected = chi2_contingency(ctab)\n    alpha1 = 0.05\n    alpha2 = 0.01\n    print('--------------Chi Squared Hypothesis Test Results-------------------')\n    print('Variable X: ',x)\n    print('Variable Y: ',y)\n    if p<alpha1 and p > alpha2:\n        print('P-value: ',p)\n        print('We reject the NUll Hypothesis H0')\n        print('There is some evidence to suggest that {} and {} are dependent'.format(x,y))\n    if p < alpha1 and p < alpha2:\n        print('P-value: ',p)\n        print('We reject the NUll Hypothesis H0')\n        print('There is substantial evidence to suggest that {} and {} are dependent'.format(x,y))\n    else:\n        print('P-value: ',p)\n        print('We fail to reject the NUll Hypothesis H0')\n        print('There is no evidence to suggest that {} and {} are independent'.format(x,y))\n        \n    print()","133c72f3":"chi2_dependency(data,'gender','stroke')\nchi2_dependency(data,'ever_married','stroke')\nchi2_dependency(data,'hypertension','stroke')\nchi2_dependency(data,'heart_disease','stroke')\nchi2_dependency(data,'work_type','stroke')\nchi2_dependency(data,'Residence_type','stroke')\nchi2_dependency(data,'smoking_status','stroke')","d40202d5":"data.columns","44ece79d":"ctab = pd.crosstab(data['smoking_status'], data['stroke'])\n\n\nctab.plot.bar(stacked = True, figsize = (8,5))\nplt.xlabel('Smoking Status')\nplt.ylabel('Stroke')\nplt.title('Smoking Status and Stroke')\nplt.show()","a0b00b72":"ctab = pd.crosstab(data['ever_married'], data['stroke'])\n\nctab.plot.bar(stacked = True, figsize = (8,5))\nplt.xlabel('Ever Married')\nplt.ylabel('Stroke')\nplt.title('Married Status and Stroke')\nplt.show()\n\nprint('Ratio of stroke affected from ever_married class',\n      len(data[data['stroke']==1])\/len(data[data['ever_married']=='Yes']))\n      \nprint('Ratio of stroke affected from never married class',\n      len(data[data['stroke']==1])\/len(data[data['ever_married']=='No']))","1e461c03":"ctab = pd.crosstab(data['hypertension'], data['stroke'])\n\nctab.plot.bar(stacked = True, figsize = (8,5))\nplt.xlabel('hypertension')\nplt.ylabel('Stroke')\nplt.title('hypertension Status and Stroke')\nplt.show()\n\nprint('Ratio of stroke affected from hypertension=1 class',\n      len(data[data['stroke']==1])\/len(data[data['hypertension']==1]))\n      \nprint('Ratio of stroke affected from no hypertension class',\n      len(data[data['stroke']==1])\/len(data[data['hypertension']==0]))","d6fa77c1":"ctab = pd.crosstab(data['heart_disease'], data['stroke'])\n\nctab.plot.bar(stacked = True, figsize = (8,5))\nplt.xlabel('heart_disease')\nplt.ylabel('Stroke')\nplt.title('heart_disease and Stroke')\nplt.show()\n\nprint('Ratio of stroke affected from heart_disease = 1 class',\n      len(data[data['stroke']==1])\/len(data[data['heart_disease']==1]))\n      \nprint('Ratio of stroke affected from no heart_disease class',\n      len(data[data['stroke']==1])\/len(data[data['heart_disease']==0]))","5fefe9c1":"data[data['stroke']==0]['bmi'].var()\/data[data['stroke']==1]['bmi'].var()","fe45cd8f":"statistic, pval = ttest_ind(a=data[data['stroke']==0]['bmi']  , b = data[data['stroke']==1]['bmi'], equal_var=True)\npval","02a5ecde":"target_col = ['stroke']\nnum_cols = ['id', 'age', 'avg_glucose_level', 'bmi']\ncat_cols = [col for col in data.columns if col not in num_cols+target_col]","b38d31c8":"from sklearn import preprocessing\n\nlabel_encoder = preprocessing.LabelEncoder()\ndata['gender'] = label_encoder.fit_transform(data['gender'])\ndata['ever_married'] = label_encoder.fit_transform(data['ever_married'])\ndata['Residence_type'] = label_encoder.fit_transform(data['Residence_type'])","ea461921":"data = pd.get_dummies(data, prefix = ['work_type'], columns = ['work_type'])\ndata = pd.get_dummies(data, prefix = ['smoking_status'], columns = ['smoking_status'])","77e41396":"#Splitting the dataset\nx = num_data.drop('stroke', axis=1)\ny = num_data.stroke\n","e8d3d2ca":"from sklearn.model_selection import train_test_split\n\nxtrain, xtest, ytrain, ytest = train_test_split(x, y, train_size = 0.3, random_state=1)","e30c475d":"#include categorical values in the dataset\n#Since we are using a Tree based model, One-Hot encoding is not an absolute necessity\n#However, this dataset, train and test sets will be updated whenever one-hot encoding will be used\nfrom sklearn.model_selection import train_test_split\n\n#Splitting the dataset\nx = data.drop('stroke', axis=1)\ny = data.stroke\n\nxtrain, xtest, ytrain, ytest = train_test_split(x, y, train_size = 0.3, random_state=1)","08c2d535":"#Let explore with the Random Forests Algo","ab2c64b3":"#Before proceeding for tree based models, lets check rank of feature importance on a decision tree\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(xtrain,ytrain)","86248516":"print(len(xtrain.columns.tolist()))\nlen(dt.feature_importances_)","6d36dcec":"plt.figure(figsize = (8,8))\nsns.barplot(x = dt.feature_importances_, y = xtrain.columns.tolist())","7df13cf8":"#Building the model using RandomForest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier(n_estimators=500)\nrfc.fit(xtrain, ytrain)\npreds = rfc.predict(xtest)\n\nprint('Predictions',list(preds[0:500]))","f3d0bf3e":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(ytest, preds)","9002548b":"#To find the False Negatives and Predictions. \nxp = (ytest == 0 and preds == 1)","422dd19a":"from sklearn.metrics import accuracy_score\naccuracy_score(ytest,preds)","1f07a3ab":"from sklearn.metrics import f1_score\nf1_score(ytest, preds, average='micro')","0942b626":"#Splitting the data set\nx = data.drop('stroke', axis = 1)\ny = data.stroke\n\nxtrain, xtest, ytrain, ytest = train_test_split(x,y, train_size = 0.3, random_state =1)","20308128":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbc = GradientBoostingClassifier(random_state = 123, n_estimators = 500)\ngbc.fit(xtrain, ytrain)","15afbe5f":"preds = gbc.predict(xtest)\n\nprint(preds)\naccuracy_score(ytest,preds)","de4b6a4a":"print(confusion_matrix(ytest, preds))\nprint(classification_report(ytest, preds))","bf748efc":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbc = GradientBoostingClassifier(random_state = 123, n_estimators = 500)\ngbc.fit(xtrain,ytrain)","42f7ace8":"preds = gbc.predict(xtest)\nprint(confusion_matrix(ytest, preds))\nprint(classification_report(ytest, preds, output_dict = True))\nprint('Accuracy Score: ',accuracy_score(ytest, preds))\nprint('F1 Score: ',f1_score(ytest,preds))","c894d2d1":"from imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state = 2)\nxtrain_mod, ytrain_mod = sm.fit_resample(xtrain, ytrain)","f096e60f":"gbc2 = GradientBoostingClassifier(random_state = 123, n_estimators = 30, max_depth = 2)\ngbc2.fit(xtrain_mod,ytrain_mod)\npreds = gbc2.predict(xtest)\nprint(confusion_matrix(ytest, preds))\n#print(classification_report(ytest, preds, output_dict = True))\nprint('Accuracy Score: ',accuracy_score(ytest, preds))\nprint('F1 Score: ',f1_score(ytest,preds))","1fe02ffb":"#Fitting model on non-SMOTE dataset\nxgb1 = XGBClassifier(n_estimators = 250)\nxgb1.fit(xtrain, ytrain)\npreds = xgb1.predict(xtest)\nprint(confusion_matrix(ytest, preds))\n\ntrain_preds = xgb1.predict(xtrain)\nprint('Train Accuracy Score: ',accuracy_score(ytrain, train_preds))\nprint('Train F1 Score: ',f1_score(ytrain, train_preds))\n\nprint('Test Accuracy Score: ',accuracy_score(ytest, preds))\nprint('F1 Score: ',f1_score(ytest,preds))","4a78c021":"#Fitting model on SMOTE dataset\nxgb = XGBClassifier(n_estimators = 255, reg_alpha=0.5, reg_lambda = 0.4, max_depth= 1)\nxgb.fit(xtrain_mod, ytrain_mod)\npreds = xgb.predict(xtest)\nprint(confusion_matrix(ytest, preds))\n#print(classification_report(ytest, preds, output_dict = True))\n\ntrain_preds = xgb.predict(xtrain_mod)\nprint('Train Accuracy Score: ',accuracy_score(ytrain_mod, train_preds))\nprint('Train F1 Score: ',f1_score(ytrain_mod, train_preds))\n\nprint('Accuracy Score: ',accuracy_score(ytest, preds))\nprint('F1 Score: ',f1_score(ytest,preds))","a5138387":"#Applying on non-SMOTE dataset\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(xtrain, ytrain)\n\npreds = gnb.predict(xtest)\nprint(confusion_matrix(ytest, preds))\n\ntrain_preds = gnb.predict(xtrain)\nprint('Train Accuracy Score: ',accuracy_score(ytrain, train_preds))\nprint('Train F1 Score: ',f1_score(ytrain, train_preds))\n\nprint('Accuracy Score: ',accuracy_score(ytest, preds))\nprint('F1 Score: ',f1_score(ytest,preds))","9efec97a":"#Applying on SMOTE dataset\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(xtrain_mod, ytrain_mod)\n\npreds = gnb.predict(xtest)\nprint(confusion_matrix(ytest, preds))\n\ntrain_preds = gnb.predict(xtrain_mod)\nprint('Train Accuracy Score: ',accuracy_score(ytrain_mod, train_preds))\nprint('Train F1 Score: ',f1_score(ytrain_mod, train_preds))\n\nprint('Accuracy Score: ',accuracy_score(ytest, preds))\nprint('F1 Score: ',f1_score(ytest,preds))","c2e35dc3":"#Removing non-impactful features\nxtrain_mod_dropped = xtrain_mod.drop(['gender', 'Residence_type'], axis = 1)\nxtest_dropped = xtest.drop(['gender', 'Residence_type'], axis = 1)\n\ngnb = GaussianNB()\ngnb.fit(xtrain_mod_dropped, ytrain_mod)\n\npreds = gnb.predict(xtest_dropped)\nprint(confusion_matrix(ytest, preds))\n\ntrain_preds = gnb.predict(xtrain_mod_dropped)\nprint('Train Accuracy Score: ',accuracy_score(ytrain_mod, train_preds))\nprint('Train F1 Score: ',f1_score(ytrain_mod, train_preds))\n\nprint('Accuracy Score: ',accuracy_score(ytest, preds))\nprint('F1 Score: ',f1_score(ytest,preds))","d68e4edd":"#Applying on SMOTE dataset\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(xtrain, ytrain)\n\npreds = knn.predict(xtest)\nprint(confusion_matrix(ytest, preds))\n\ntrain_preds = knn.predict(xtrain)\nprint('Train Accuracy Score: ',accuracy_score(ytrain, train_preds))\nprint('Train F1 Score: ',f1_score(ytrain, train_preds))\n\nprint('Accuracy Score: ',accuracy_score(ytest, preds))\nprint('F1 Score: ',f1_score(ytest,preds))","671ef7c4":"#Applying on SMOTE dataset\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=2, p=1)\n\nknn.fit(xtrain_mod, ytrain_mod)\n\npreds = knn.predict(xtest)\nprint(confusion_matrix(ytest, preds))\n\ntrain_preds = knn.predict(xtrain_mod)\nprint('Train Accuracy Score: ',accuracy_score(ytrain_mod, train_preds))\nprint('Train F1 Score: ',f1_score(ytrain_mod, train_preds))\n\nprint('Accuracy Score: ',accuracy_score(ytest, preds))\nprint('F1 Score: ',f1_score(ytest,preds))","76affa9a":"### Gradient Boost Classifier","fa5fe8c1":"### Distribution of Heart Disease","0edc72f5":"Almost 90% of samples having heartdisease were found to have suffered stroke","7261a70a":"### Relationship Analysis","7ab4ef69":"This is excessively strange. What should do with this data row. Only one entry with a very high bmi. Has hypertension. It's a young age male, who work in the private sector and live in a rural area with a glucose level that seems correct and has not suffered a stroke.\n\nProably we will need to do an imputation to update his bmi base on median bmi for his age and other related features.\n\nBut for now we are going to remove it for the purpose of fixing the distribution.","3231bac0":"Checking for outliers:\nis a datapoint that differ from other observations","a3bbeeea":"Stroke Distrution of people with a BMI over 40","1e80b86e":"\n\nNow we can start creating our model and start our predictions. Also, we can include other features to see if there is any other related variable. ","be3beb1c":"With this it seems that a confusion matrix and a logistic regression may whow a better relationship because this is showing that there is not a linear relationship. ","b68414f5":"### XGBoost Classifier","0b75f69b":"True positive are on the upper left. Then the botton right is the true negative. Which means, that I was supposed a negative and the model got a negative.\n\nThe false positive is the number on the upper right. \nFalse negative are the numbers on the bottom left. ","d072a1f6":"* Gender and Residential Type do not seem to have an impact on stroke\n* Smoking Status, Work Type, Heart Disease, Hypertension and Married status have an impact on stroke","34c25543":"* Since pvalue < 0.01, we reject the Null Hypothesis H0\n* We can conclude that the population BMI mean of stroke vs non-stroke groups are different","dd1b73bf":"From this quick overview, it comes to mind that we can use different variables correlations. Like age against stroke. We see that there are many variables that can be related to having a stroke: age, being married, work, etc.","ddff8d68":"## Data Visualizations","aa595c49":"We observe that there is a significant increase in True negatives, but there are also a significant increase in false negatives","b5a70325":"### Distribution of Age","1cbeb285":"Another way to get the number of rows and columsn is using the data.shape panda feature. It returns a tuple. First one is for the rows and second one is for the number of columns.","d7a8ea29":"f1-score is very good for umbalanced data. ","fc684f10":"The average age of people that has suffered a strok are a 67 with a bmi of 30 or over.\n\nSo, it shows there is is more entries with abnormal Body Mass Index. Let's check for those with a BMI over 40 with obesity class 2","aba4b462":"Checking the distribution of the target variable(stroke)","e25a24c6":"Earlier, we had noticed that Gender and Residence_type do not have an impact on stroke. \n<br><\/br>\nTo strengthen our classifier, they can be dropped from train and test X dataset","a9a8b08c":"We will perform 2 sample t-test on 'BMI' column to check if the mean BMI of stroke group is different from the non stroke group.\n<br><\/br>\nBefore performing this test we will check ratio of variance of each group","e79dc714":"### Exploratory Data Analysis (EDA)\n\nIs nothing but data exploration technique to understand the various aspects of the data. The idea is to check for relationship between variables and to check their distributions.\n\n\n* It follow a systematic set of steps to explore the data in the most efficient way possible","bca49b8a":"### Features or data points. ","5994d186":"### Data Transformation","4a7b81cb":"Questions:\nNo missing values except for BMI. Should we need to fill those empty values","5b38e6ff":"Given the fact that of those who are smokers. Only a few smoke at young ages and at late ages as well. So, let's see if we can slice the data from 35 - 65 years of age.","a261e8aa":"### Distribution of gender\n","45e4d8f4":"The describe function allows us to have basic statistical information of the data. This is useful because it allows us to detect possible outliers or any strange data.","9291da5a":"We see that most people are on the 43 years of age. Then we can see that the mean of bmi of the population in study is at 28.\n\nA healty range of a person BMI is between 18-25. \n\nThe BMI depends on different factors, Like height, muscle and body type.\n\nNote on the avg of glucose level below 140 is normal. Between 140-199 is pre-diabetes. \n","509430af":"### K Nearest Neighbour Classifier","0c8e6173":"## Hypothesis Testing\n\n##### Chi Square testing","9e5f3404":"Almost 50% of samples having hypertension were found to have suffered stroke","1dc2648c":"## References","66bf3818":"Since the ratio of variance < 4, we will assume them to be having equal variance","8fb9d486":"According to this result we can see that the mayority of smokers account for more than 10 are effectively on the age range of 35 through 63. With the exception of of less smokers at the age of 41, 37, 62 and 64 of age, only 9 smokers. \n\nOn the visualiaztion section we can plot this one out to see the histogram distribution. ","7c41c51b":"### Plotting relationships in the dataset. \n\nThere are different ways to display relationships using a dataset. You can use pair plots, joint plots, correlations, etc. we will the use pairplot to find out relationships in the dataset.","c0f0f794":"### Checking Specific Unique values","ad9f9446":"One-hot encode the multi category columns","f1ed82de":"After successfully splitting the dataset, let us train it using train_test_split.","741168c4":"### Distribution of BMI","293e735b":"True positive are on the upper left. Then the botton right is the true negative. Which means, that I was supposed a negative and the model got a negative.\n\nThe false positive is the number on the upper right. \nFalse negative are the numbers on the bottom left. ","1a9e7316":"Here we have the True negatives or 0s because we don't have many cases of strokes. Meaning that 3,388 people did not have a stroke.\n\nOn the inverse we have the True positive or 1s for those who suffered a stroke.\n\n  is the False negative, those who were predicted as 1 but they were 0s. Number 13 ","76fc49c9":"As we can see it looks that the more related variable to stroke is the age feature. We may consider to use a model to only use the wanted variables to remove id for example.","603f57b1":"Shape and the spread with histograms and box plots","7c9facd5":"#### Using SMOTE\n<br>\nSMOTE is a technique to artificially oversample the minority class by creating synthetic samples. These synthetic samples are created by finding the intermediate values between neighbouring samples of minority class<br>\n<br>\nSMOTE is applied ONLY on the training set and not on the test set to avoid biased results","50089f72":"## Building the Models\n\nAs I stated earlier, we will use four models i.e. Random Forests, Decision Trees, Support Vector Machine and XGBoost to get the best accuracy score. \u2018Accuracy\u2019 metric is used to evaluate models. It is the ratio of the number of correctly predicted instances in a dataset divided by the total number of instances in the dataset. We will proceed further to explore more metrics to determine the best model.","32a404ec":"### Random Forests Classifier","e7547415":"### Check for unique values","6447b825":"### Adult Body Mass Index (BMI)\n\nBMI does not measure body fat directly, but research has shown that BMI is moderately correlated with more direct measures of body fat obtained from skinfold thickness measurements, bioelectrical impedance, underwater weighing, dual energy x-ray absorptiometry (DXA) and other methods 1,2,3. Furthermore, BMI appears to be strongly correlated with various adverse health outcomes consistent with these more direct measures of body fatness","cf7cc542":"Label encode the binary categorical columns containing strings","cc02055b":"### Describing the data statistically speaking function","b033a022":"From this bar chart we can clearly see that for people over 40 years old the majority suffered a stroke. We have an uptick at age 40 then it drops until about age 55 through 65 and drops again and goes all the way up at age 80.","6156818e":"### Distribution of AVG Glucose Level","55d02f1a":"### Gradient Boost Classifier","7bdbef62":"Given the fact the other gender is only 1 value. We can remove that data point from our study.","6d5044e6":"To find out how many columns, how many entries and if there are some missing values. We can use dataframe.info()","fb5e6fa4":"### Gaussian Naive Bayes","ee970fca":"## Step 2: Cleaning the data","335272de":"#### T-tests","a7ddce9c":"Include categorical values in the dataset.\nSince we are using a Tree based model, One-Hot encoding is not an absolute necessity\n\nHowever, this dataset, tran and test sets will be update whenever one-hot enconding will be use","e7c15265":"### Label encoding","4c085a79":"## Training the Data\n\nWe will now split our dataset before we train it. X will contain all the Independent variables while y will have the Dependent variable ('stroke')","cfdf103a":"### Age distribution of smokers\n","08db3d59":"## Stroke Prediction Research:\n\nThe main question is that we want to understand how the predictor variables can help estimate the probability of sufferign a stroke. \n\n* Is there other than age relationship?\n* Does having a heart disease or high BMI and glucose level related to have a higher change of suffering a stroke?\n\n### Plans:\nWe should visualize a distribution of the target variable, which is the stroke, then a distribution of variables in respect to the target variable.\n1. Split the model into categorial features and objects. - Done. Do Hot encoding?\n2. Call the distributions on an object based way e.g. fig, ax.\n3. Continue building on the models. Next XGBoost.\n4. Predict, predict, predict.\n5. Draw final conclusions.\n6. Add an index to notebook.\n7. Add more distribution visualizations.\n\n### Models: \nLogistic regression, random forest and xgboost.","151f805a":"We see that the corelation of suffering a stroke is not just age, but having a bmi over 40 and a higher sugar level. ","22e26851":"We have 4% of BMI missing data. ","979449e7":"Steps:\n1. Understand the Data\n\n2. Clean up the Data\n3. Analysis of Relationship between variables","eeb1d3a4":"## Accuracy Score\n\nThe accuracy score for this Random forest classifier","668cc295":"### 1. Understanding the Data","ad3a0edb":"It seems the majorities of values are binaries, which mean that they are categorical values e.g. \"yes\" or \"no\" except for gender which is says it has 3 types. We need to check if that is not because a typo or blank entries. \n\nThe categorial variables with more different values are the following in ascending order:\n1. smoking_status          4\n2. work_type               5\n"}}