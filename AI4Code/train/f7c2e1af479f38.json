{"cell_type":{"a8d5a8cb":"code","8080ceef":"code","1cd1add6":"code","44d3c25d":"code","8774a8ca":"code","e30e30b9":"code","89cb7b57":"code","454be998":"code","3927edb7":"code","90023117":"code","55dc1531":"code","93ebfc4c":"code","0d228059":"code","d0f4ea75":"code","8a7464a5":"markdown","bcfc25c8":"markdown","b4d0126d":"markdown","3404c71d":"markdown","aa21013b":"markdown"},"source":{"a8d5a8cb":"from bayes_opt import BayesianOptimization","8080ceef":"import numpy as np \nimport pandas as pd \n# Data processing, metrics and modeling\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, StratifiedKFold,KFold\nfrom bayes_opt import BayesianOptimization\nfrom datetime import datetime\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score, roc_auc_score, f1_score, roc_curve, auc,precision_recall_curve\nfrom sklearn import metrics\nfrom sklearn import preprocessing\n# Lgbm\nimport lightgbm as lgb\n# Suppr warning\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport itertools\nfrom scipy import interp\n\n# Plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom matplotlib import rcParams\nimport os\nprint(os.listdir('..\/input\/'))\nRandom_Seed = 4520","1cd1add6":"%%time\nDataFile = pd.read_csv('..\/input\/creditcard.csv')\ntrain_df = DataFile.drop(['Time'], axis=1)","44d3c25d":"features = list(train_df)\nfeatures.remove('Class')","8774a8ca":"#cut tr and val\nbayesian_tr_idx, bayesian_val_idx = train_test_split(train_df, test_size = 0.3, random_state = 42, stratify = train_df['Class'])\nbayesian_tr_idx = bayesian_tr_idx.index\nbayesian_val_idx = bayesian_val_idx.index","e30e30b9":"# Take the hyper parameters you want to consider\n\nparamsLGB = {\n    'learning_rate': (0.001,0.005),\n    'num_leaves': (50, 500), \n    'bagging_fraction' : (0.1, 0.9),\n    'feature_fraction' : (0.1, 0.9),\n    'min_child_weight': (0.00001, 0.01),   \n    'min_data_in_leaf': (20, 70),\n    'max_depth':(-1,50),\n    'reg_alpha': (1, 2), \n    'reg_lambda': (1, 2)\n    \n}","89cb7b57":"def LGB_bayesian(\n    learning_rate,\n    num_leaves, \n    bagging_fraction,\n    feature_fraction,\n    min_child_weight, \n    min_data_in_leaf,\n    max_depth,\n    reg_alpha,\n    reg_lambda\n     ):\n    \n    # LightGBM expects next three parameters need to be integer. \n    num_leaves = int(num_leaves)\n    min_data_in_leaf = int(min_data_in_leaf)\n    max_depth = int(max_depth)\n\n    assert type(num_leaves) == int\n    assert type(min_data_in_leaf) == int\n    assert type(max_depth) == int\n    \n\n    param = {\n              'num_leaves': num_leaves, \n              'min_data_in_leaf': min_data_in_leaf,\n              'min_child_weight': min_child_weight,\n              'bagging_fraction' : bagging_fraction,\n              'feature_fraction' : feature_fraction,\n              'learning_rate' : learning_rate,\n              'max_depth': max_depth,\n              'reg_alpha': reg_alpha,\n              'reg_lambda': reg_lambda,\n              'objective': 'binary',\n              'save_binary': True,\n              'seed': Random_Seed,\n              'feature_fraction_seed': Random_Seed,\n              'bagging_seed': Random_Seed,\n              'drop_seed': Random_Seed,\n              'data_random_seed': Random_Seed,\n              'boosting_type': 'gbdt',\n              'verbose': 1,\n              'is_unbalance': False,\n              'boost_from_average': True,\n              'metric':'auc'}    \n    \n    oof = np.zeros(len(train_df))\n    trn_data= lgb.Dataset(train_df.iloc[bayesian_tr_idx][features].values, label=train_df.iloc[bayesian_tr_idx]['Class'].values)\n    val_data= lgb.Dataset(train_df.iloc[bayesian_val_idx][features].values, label=train_df.iloc[bayesian_val_idx]['Class'].values)\n\n    clf = lgb.train(param, trn_data,  num_boost_round=50, valid_sets = [trn_data, val_data], verbose_eval=0, early_stopping_rounds = 50)\n    \n    oof[bayesian_val_idx]  = clf.predict(train_df.iloc[bayesian_val_idx][features].values, num_iteration=clf.best_iteration)  \n    \n    score = roc_auc_score(train_df.iloc[bayesian_val_idx]['Class'].values, oof[bayesian_val_idx])\n\n    return score","454be998":"LGB_BO = BayesianOptimization(LGB_bayesian, paramsLGB, random_state=42)","3927edb7":"print(LGB_BO.space.keys)","90023117":"init_points = 9\nn_iter = 15","55dc1531":"print('-' * 130)\nwith warnings.catch_warnings():\n    warnings.filterwarnings('ignore')\n    LGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)","93ebfc4c":"LGB_BO.max['target']","0d228059":"LGB_BO.max['params']","d0f4ea75":"param_lgb = {\n        'min_data_in_leaf': int(LGB_BO.max['params']['min_data_in_leaf']), \n        'num_leaves': int(LGB_BO.max['params']['num_leaves']), \n        'learning_rate': LGB_BO.max['params']['learning_rate'],\n        'min_child_weight': LGB_BO.max['params']['min_child_weight'],\n        'bagging_fraction': LGB_BO.max['params']['bagging_fraction'], \n        'feature_fraction': LGB_BO.max['params']['feature_fraction'],\n        'reg_lambda': LGB_BO.max['params']['reg_lambda'],\n        'reg_alpha': LGB_BO.max['params']['reg_alpha'],\n        'max_depth': int(LGB_BO.max['params']['max_depth']), \n        'objective': 'binary',\n        'save_binary': True,\n        'seed': Random_Seed,\n        'feature_fraction_seed': Random_Seed,\n        'bagging_seed': Random_Seed,\n        'drop_seed': Random_Seed,\n        'data_random_seed': Random_Seed,\n        'boosting_type': 'gbdt',\n        'verbose': 1,\n        'is_unbalance': False,\n        'boost_from_average': True,\n        'metric':'auc'\n    }","8a7464a5":"Following are four common methods of hyperparameter optimization for machine learning in order of increasing efficiency:\n\n* Manual\n* Grid search\n* Random search\n* Bayesian model-based optimization","bcfc25c8":"The aim of hyperparameter optimization in machine learning is to find the hyperparameters of a given machine learning algorithm that return the best performance as measured on a validation set. (Hyperparameters, in contrast to model parameters, are set by the machine learning engineer before training. The number of trees in a random forest is a hyperparameter while the weights in a neural network are model parameters learned during training. if you are looking for better scores in this competition, you may ignore the kernel.","b4d0126d":"Now use these parameters for your final model. You can use a similar technique for any other models.","3404c71d":"Some references for the text  : https:\/\/towardsdatascience.com\/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f\n\nSome references for the code : https:\/\/www.kaggle.com\/vincentlugat\/ieee-lgb-bayesian-opt (Please upvote his kernel)","aa21013b":"This kernel provides an example to tuning the hyper - parameters using Bayesian Optimization on Light GBM (one of the most popular algorithm in Kaggle)"}}