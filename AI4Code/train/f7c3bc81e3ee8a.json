{"cell_type":{"20760d8e":"code","b17e0cb9":"code","c4886939":"code","a880c7ba":"code","7a15de8b":"code","eb5e50a7":"code","45f20b27":"code","3656e1ef":"code","11fb01f3":"code","db501366":"code","6e396497":"code","ba9ef0a9":"code","376d5f9a":"code","0cd8f381":"code","55de6537":"code","f2ff8b59":"code","ed8968e0":"code","4bd057d4":"code","1d794366":"markdown","4bed14e5":"markdown","57a9b093":"markdown","63ae4433":"markdown","dda9fcdc":"markdown","9c56b52e":"markdown","079c1357":"markdown","5f20051c":"markdown","e5ee2b55":"markdown","9ade22de":"markdown"},"source":{"20760d8e":"import pandas as pd\nimport numpy as np\n\nfrom datetime import datetime\nfrom unidecode import unidecode\nfrom itertools import combinations\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\n\nimport category_encoders as ce\n\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b17e0cb9":"train = pd.read_csv('\/kaggle\/input\/klps-creditscring-challenge-for-students\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/klps-creditscring-challenge-for-students\/test.csv')","c4886939":"# Drop some columns are duplicated, with correlation = NaN\nignore_columns = ([\"gioiTinh\", \"info_social_sex\", \"ngaySinh\", \"namSinh\"] + \n        [f\"Field_{c}\" for c in [14, 16, 17, 24, 26, 30, 31, 37, 52, 57]] + \n        ['partner0_K', 'partner0_L', \n         'partner1_B', 'partner1_D', 'partner1_E', 'partner1_F', 'partner1_K', 'partner1_L',\n         'partner2_B', 'partner2_G', 'partner2_K', 'partner2_L',\n         'partner3_B', 'partner3_C', 'partner3_F', 'partner3_G', 'partner3_H', 'partner3_K', 'partner3_L',\n         *['partner4_' + i for i in 'ABCDEFGHK'],\n         'partner5_B', 'partner5_C', 'partner5_H', 'partner5_K', 'partner5_L'])\n\n# Some auto columns could make new better columns\nall_auto_columns = list(set([c for c in train.columns if train[c].dtype in [np.int64, np.float64]])\n                    .difference(ignore_columns + ['currentLocationLocationId', 'homeTownLocationId', 'label', 'id']))\n\nauto_columns_1 = [c for c in all_auto_columns if 'Field_' in c]\nauto_columns_2 = [c for c in all_auto_columns if 'partner' in c]\nauto_columns_3 = [c for c in all_auto_columns if 'num' in c]\nauto_columns_4 = [c for c in all_auto_columns if c not in auto_columns_1 + auto_columns_2 + auto_columns_3]\nprint(len(auto_columns_1), len(auto_columns_2), len(auto_columns_3), len(auto_columns_4), len(all_auto_columns))","a880c7ba":"date_cols = [\"Field_{}\".format(i) for i in [5, 6, 7, 8, 9, 11, 15, 25, 32, 33, 35, 40]]\ndatetime_cols = [\"Field_{}\".format(i) for i in [1, 2, 43, 44]]\ncorrect_dt_cols = ['Field_34', 'ngaySinh']\ncat_cols = date_cols + datetime_cols + correct_dt_cols\n\n# Normalize Field_34, ngaySinh\ndef ngaysinh_34_normalize(s):\n    if s != s: return np.nan\n    try: s = int(s)\n    except ValueError: s = s.split(\" \")[0]\n    return datetime.strptime(str(s)[:6], \"%Y%m\")\n\n# Normalize datetime data\ndef datetime_normalize(s):\n    if s != s: return np.nan\n    s = s.split(\".\")[0]\n    if s[-1] == \"Z\": s = s[:-1]\n    return datetime.strptime(s, \"%Y-%m-%dT%H:%M:%S\")\n\n# Normalize date data\ndef date_normalize(s):\n    if s != s: return np.nan\n    try: t = datetime.strptime(s, \"%m\/%d\/%Y\")\n    except: t = datetime.strptime(s, \"%Y-%m-%d\")\n    return t\n\ndef process_datetime_cols(df):\n    df[datetime_cols] = df[datetime_cols].applymap(datetime_normalize)  \n    df[date_cols] = df[date_cols].applymap(date_normalize)\n    df[correct_dt_cols] = df[correct_dt_cols].applymap(ngaysinh_34_normalize)\n\n    # Some delta columns\n    for i, j in zip('43 1 2'.split(), '1 2 44'.split()): df[f'DT_{j}_{i}'] = (df[f'Field_{j}'] - df[f'Field_{i}']).dt.seconds\n    for i, j in zip('5 6 7 33 8 11 9 15 25 6 7 8 9 15 25 2'.split(), '6 34 33 40 11 35 15 25 32 7 8 9 15 25 32 8'.split()): \n        df[f'DT_{j}_{i}'] = (df[f'Field_{j}'] - df[f'Field_{i}']).dt.days\n    \n    # Age, month\n    df['age'] = 2020 - pd.DatetimeIndex(df['ngaySinh']).year\n    df['birth_month'] = pd.DatetimeIndex(df['ngaySinh']).month\n    \n    # Days from current time & isWeekday\n    for col in cat_cols:\n        name = col.split('_')[-1]\n        df[f'is_WD_{name}'] = df[col].dt.dayofweek.isin(range(5))\n        df[f'days_from_now_{name}'] = (datetime.now() - pd.DatetimeIndex(df[col])).days\n        df[col] = df[col].dt.strftime('%m-%Y')\n    \n    # Delta for x_startDate and x_endDate\n    for cat in ['F', 'E', 'C', 'G', 'A']:\n        df[f'{cat}_startDate'] = pd.to_datetime(df[f\"{cat}_startDate\"], infer_datetime_format=True)\n        df[f'{cat}_endDate'] = pd.to_datetime(df[f\"{cat}_endDate\"], infer_datetime_format=True)\n        \n        df[f'{cat}_start_end'] = (df[f'{cat}_endDate'] - df[f'{cat}_startDate']).dt.days\n        \n    for i, j in zip('F E C G'.split(), 'E C G A'.split()):\n        df[f'{j}_{i}_startDate'] = (df[f'{j}_startDate'] - df[f'{i}_startDate']).dt.days\n        df[f'{j}_{i}_endDate'] = (df[f'{j}_endDate'] - df[f'{i}_endDate']).dt.days\n    \n    temp_date = [f'{i}_startDate' for i in 'ACEFG'] + [f'{i}_endDate' for i in 'ACEFG']\n    \n    for col in temp_date:\n        df[col] = df[col].dt.strftime('%m-%Y')\n        \n    for col in cat_cols + temp_date:\n        df[col] = df[col]\n        \n    return df","7a15de8b":"unicode_cols = ['Field_18', 'maCv', 'diaChi', 'Field_46', 'Field_48', 'Field_49', 'Field_56', 'Field_61', 'homeTownCity', \n                'homeTownName', 'currentLocationCity', 'currentLocationName', 'currentLocationState', 'homeTownState']\nobject_cols = (unicode_cols + \n               [f'Field_{str(i)}' for i in '4 12 36 38 47 62 45 54 55 65 66 68'.split()] +\n               ['data.basic_info.locale', 'currentLocationCountry', 'homeTownCountry', 'brief'])\n\ndef str_normalize(s):\n    s = str(s).strip().lower()\n    s = re.sub(' +', \" \", s)\n    return s\n\ndef combine_gender(s):\n    x, y = s \n    if x != x and y != y: return \"nan\"\n    if x != x: return y.lower()\n    return x.lower()\n\ndef process_categorical_cols(df):\n    df['diaChi'] = df['diaChi'].str.split(',').str[-1]\n    df[unicode_cols] = df[unicode_cols].applymap(str_normalize).applymap(lambda x: unidecode(x) if x==x else x)\n    \n    # Normalize some columns\n    df[\"Field_38\"] = df[\"Field_38\"].map({0: 0.0, 1: 1.0, \"DN\": np.nan, \"TN\": np.nan, \"GD\": np.nan})\n    df[\"Field_62\"] = df[\"Field_62\"].map({\"I\": 1, \"II\": 2, \"III\": 3, \"IV\": 4, \"V\": 5, \"Ngo\u00e0i qu\u1ed1c doanh Qu\u1eadn 7\": np.nan})\n    df[\"Field_47\"] = df[\"Field_47\"].map({\"Zezo\": 0, \"One\": 1, \"Two\": 2, \"Three\": 3, \"Four\": 4})\n    \n    # Make some new features\n    df['Field_45_Q'] = df['Field_45'].str[:-3].astype('category')\n    df['Field_45_TP_55'] = df['Field_45'].str[:2] == df['Field_55']\n    df['is_homeTown_diaChi'] = df['homeTownCity'] == df['diaChi']\n    df['is_homeTown_current_city'] = df['homeTownCity'] == df['currentLocationCity']\n    df['is_homeTown_current_state'] = df['homeTownState'] == df['currentLocationState']\n    df['F48_49'] = df['Field_48'] == df['Field_49']\n    \n    df[\"gender\"] = df[[\"gioiTinh\", \"info_social_sex\"]].apply(combine_gender, axis=1).astype(\"category\")\n    \n    df[[\"currentLocationLocationId\", \"homeTownLocationId\", \"currentLocationLatitude\", \"currentLocationLongitude\", \n        \"homeTownLatitude\", \"homeTownLongitude\"]].replace(0, np.nan, inplace=True) # value == 0: noisy\n\n    df[[\"currentLocationLocationId\", \"homeTownLocationId\"]] = (df[[\"currentLocationLocationId\", \"homeTownLocationId\"]]\n                                                             .applymap(str_normalize).astype(\"category\"))\n    df[object_cols] = df[object_cols].astype('category')\n    \n    return df","eb5e50a7":"# New feature from columns 63, 64\ndef process_63_64(z):\n    x, y = z\n    if x != x and y != y:\n        return np.nan\n    if (x, y) in [(1.0, 2.0), (2.0, 3.0), (3.0, 4.0), (4.0, 8.0), (7.0, 5.0), (5.0, 6.0), (9.0, 43.0), (8.0, 9.0)]: return True\n    else: return False\n    \ndef process_others(df):        \n    df[[\"Field_27\", \"Field_28\"]].replace(0.0, np.nan, inplace=True)\n    df['F18_isnumeric'] = df['Field_18'].str.isnumeric()\n    df['F18_isalpha'] = df['Field_18'].str.isalpha()\n    \n    # Delta from some pairs of columns\n    for i, j in [(20, 27), (28, 27), (39, 41), (41, 42), (50, 51), (51, 53)]:\n        df[f'F{str(i)}_{str(j)}_delta'] = df[f'Field_{str(j)}'] - df[f'Field_{str(i)}']\n    df['F_59_60'] = df['Field_59'] - df['Field_60'] - 2\n    df['F_63_64'] = df[['Field_63', 'Field_64']].apply(process_63_64, axis=1).astype('category')\n    \n    # Mean, std from partnerX columns\n    for i in '1 2 3 4 5'.split():\n        col = [c for c in df.columns if f'partner{i}' in c]\n        df[f'partner{i}_mean'] = df[col].mean(axis=1)\n        df[f'partner{i}_std'] = df[col].std(axis=1)\n\n    # Reference columns\n    columns = set(df.columns).difference(ignore_columns)\n    df['cnt_NaN'] = df[columns].isna().sum(axis=1)\n    df['cnt_True'] = df[columns].applymap(lambda x: isinstance(x, bool) and x).sum(axis=1)\n    df['cnt_False'] = df[columns].applymap(lambda x: isinstance(x, bool) and not x).sum(axis=1)\n\n    # Combinations of auto columns\n    lst_combination = (list(combinations(auto_columns_2, 2)) + list(combinations(auto_columns_3, 2)) + list(combinations(auto_columns_4, 2)))\n    for l, r in lst_combination:\n        for func in 'add subtract divide multiply'.split():\n            df[f'auto_{func}_{l}_{r}'] = getattr(np, func)(df[l], df[r])\n            \n    return df","45f20b27":"def transform(df):\n    df = process_datetime_cols(df)\n    df = process_categorical_cols(df)\n    df = process_others(df)\n    return df.drop(ignore_columns, axis=1)\n\ntrain = transform(train)\ntest = transform(test)","3656e1ef":"def split_dates(df):\n    dates = [f\"{c}_startDate\" for c in ['F','E','C','G','A']] + [f\"{c}_endDate\" for c in ['F','E','C','G','A']]\n    for date in dates:\n        df[date+'_day'] = df[date].dt.day\n        df[date+'_month'] = df[date].dt.month\n        df[date+'_year'] = df[date].dt.year\n        df[date+'_week'] = df[date].dt.week\n        df[date+'_dayofweek'] = df[date].dt.dayofweek\n    return df","11fb01f3":"def days_between_startEnd(df):\n    start_dates = [f\"{c}_startDate\" for c in ['F','E','C','G','A']]\n    end_dates = [f\"{c}_endDate\" for c in ['F','E','C','G','A']]\n    col = ['F','E','C','G','A']\n    for i in range(5):\n        df[col[i]+'_delta'] = (df[end_dates[i]]-df[start_dates[i]]).dt.total_seconds()\/(60*60*24)\n    return df","db501366":"def to_datetime(df):\n    dates = [f\"{c}_startDate\" for c in ['F','E','C','G','A']] + [f\"{c}_endDate\" for c in ['F','E','C','G','A']]\n    for col in dates:\n        df[col] = pd.to_datetime(df[col], errors='coerce')\n    return df","6e396497":"dates = [f\"{c}_startDate\" for c in ['F','E','C','G','A']] + [f\"{c}_endDate\" for c in ['F','E','C','G','A']]\ndates_columns = ['F_delta','E_delta','C_delta','G_delta','A_delta']\nfor d in dates:\n    dates_columns.append(d+'_day')\n    dates_columns.append(d+'_month')\n    dates_columns.append(d+'_year')\n    dates_columns.append(d+'_week')\n    dates_columns.append(d+'_dayofweek')\ndates_columns","ba9ef0a9":"def impute_df(df):    \n    for col in dates_columns:\n        df[col] = df[col].fillna(df[col].mean())\n    return df","376d5f9a":"train = to_datetime(train)\ntest = to_datetime(test)\ntrain = split_dates(train)\ntest = split_dates(test)\ntrain = days_between_startEnd(train)\ntest = days_between_startEnd(test)","0cd8f381":"train = impute_df(train)\ntest = impute_df(test)","55de6537":"#Support catboost modelling\ncat_features = [c for c in train.columns if (train[c].dtype not in [np.float64, np.int64])]\ntrain[cat_features] = train[cat_features].astype(str)\ntest[cat_features] = test[cat_features].astype(str)","f2ff8b59":"# Create the encoder\nt = pd.concat([train, test]).reset_index(drop=True)\ncount_enc = ce.CountEncoder().fit_transform(t[cat_features])\ntt = t.join(count_enc.add_suffix(\"_count\"))\n\nf2_train = tt.loc[tt.index < train.shape[0]]\nf2_test = tt.loc[tt.index >= train.shape[0]]\n\ncolumns = sorted(set(f2_train.columns).intersection(f2_test.columns))\nprint(len(columns))","ed8968e0":"TRAIN, TEST = f2_train[columns].drop(['id', 'label'], axis=1), f2_test[columns].drop(['id', 'label'], axis=1)\nLABEL = f2_train['label']\npreds, test_preds, gini = np.zeros(TRAIN.shape[0]), {}, {}\n\ncv = StratifiedKFold(n_splits=5, shuffle=True)\nfor i, (train_idx, val_idx) in enumerate(cv.split(TRAIN, LABEL)):\n    X_train, y_train = TRAIN.iloc[train_idx], LABEL.iloc[train_idx]\n    X_val, y_val = TRAIN.iloc[val_idx], LABEL.iloc[val_idx]\n\n    model = CatBoostClassifier(eval_metric='AUC', \n                             use_best_model=True,\n                             iterations=1000, \n                             learning_rate=0.1, \n                             random_seed=42).fit(X_train, y_train, \n                                                 cat_features=set(cat_features),\n                                                 eval_set=(X_val, y_val), verbose=500)\n\n    y_pred = model.predict(X_val)\n    y_pred_proba = model.predict_proba(X_val)[:, 1]\n        \n    preds[val_idx] = y_pred_proba\n    test_preds[f'F{i+1}'] = model.predict_proba(TEST)[:, 1]\n    \n    gini[f'F{i+1}'] = 2 * roc_auc_score(y_val, y_pred_proba) - 1\n    \n    \n# Resulting\nroc_auc = roc_auc_score(LABEL, preds)\nprint('Avg GINI score:', 2*roc_auc - 1)\n\nresult = np.array(list(gini.values()))\nprint('GINI: {:.5f} +- {:.5f}'.format(result.mean(), result.std()))","4bd057d4":"test['label'] = pd.DataFrame(test_preds).mean(axis=1).values\ntest[['id', 'label']].to_csv('submission.csv', index=False)","1d794366":"# 3. Modelling","4bed14e5":"# 2. Feature Engineering","57a9b093":"### 2.4. Combine all parts","63ae4433":"### 2.2. Categorical columns","dda9fcdc":"# 4. Submisison","9c56b52e":"### 2.1. Datetime columns","079c1357":"## Datetime preprocessing - Add some columns","5f20051c":"# 1. Library & Input data","e5ee2b55":"### 2.5. Try Count Encoding","9ade22de":"### 2.3. Others"}}