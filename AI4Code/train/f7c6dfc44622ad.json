{"cell_type":{"b521bf9e":"code","589864b4":"code","5262e56b":"code","423c1325":"code","85a585b0":"code","4086b82b":"code","1662dbc9":"code","b3919125":"code","3ff9531d":"code","54796db4":"code","659ff117":"code","b0aed24a":"code","2daaca4b":"code","79f34f2d":"code","7bc30717":"code","d1363cde":"code","239b8f84":"code","0df32cd5":"code","2143c275":"code","fae90621":"code","40d5590d":"code","2e783642":"code","08926a86":"code","8bc0cc10":"code","8b964926":"code","8188b4ed":"code","5d0605ad":"code","f7d546cf":"code","b5eaf0e9":"code","3792b859":"code","10758619":"code","d9ba04d1":"code","596b4472":"code","8330c24a":"code","74532b9e":"code","502369ff":"code","d699d17b":"code","5a70d811":"code","92648eea":"code","44061bfb":"code","55892f74":"code","362dfc57":"code","e4ec1821":"code","a266ed1b":"code","9f87833b":"code","c10c614e":"code","4b5bcf5b":"code","a67b2a3d":"code","861b5d15":"code","45adbc1a":"code","5501c2c2":"code","9248ec29":"code","1558570c":"code","3d9f0855":"code","1b2088ec":"code","5672f0bf":"code","3440179c":"code","fe7f7348":"code","f1a40718":"code","1464a41f":"code","a5c910ad":"code","28d313c1":"code","f41872d1":"code","2b3a3271":"code","57c9d6a2":"code","ae09eda2":"code","f3ca2d4e":"code","6c1e7201":"code","fc3438d9":"code","ae270bbc":"code","8c66f3e2":"code","b5e35217":"code","c9676117":"code","eda816e2":"code","07be2ad6":"code","e7776588":"code","0dfc7a22":"code","60876fae":"code","b1b39307":"code","6bddc179":"code","02880234":"code","0855b893":"code","6981a56e":"code","e44bfd53":"code","4571c256":"code","cd90c07c":"code","d56b551f":"code","5954bb87":"code","2b4077b1":"code","0dd3387c":"code","75884df7":"code","7ca3c68f":"code","b8b238c2":"code","98551401":"code","9530d2a7":"code","78908097":"code","789f9869":"code","3f17d151":"code","e6c6cf41":"code","82178039":"code","91b5c3e9":"code","d2640848":"code","60d959e5":"code","8beaece7":"code","e0d22299":"code","7b728703":"code","e5cb5a89":"code","a0424897":"code","dff5d0c1":"code","583efe6c":"code","06d9a045":"code","ca4792b4":"code","73d004f7":"code","e3c7e5ff":"code","f2b0c872":"code","15a1ae5f":"code","f65062a1":"code","aac62f66":"code","d3cd500d":"code","bcbcfcd8":"code","d6baa37f":"code","cc4c1b22":"code","1d77bfe2":"code","fd96c6e6":"code","d6f093fb":"code","676fe3f6":"code","bb54b569":"code","210878c4":"code","176fd9a5":"code","9082ee74":"code","68370e73":"code","fb54be98":"code","60476297":"code","0db37ad4":"code","a310f3d5":"code","f16e38ac":"code","1c944c71":"code","78d59c07":"code","db8a9569":"markdown","e15e90a5":"markdown","ebec8807":"markdown","2254663d":"markdown","69e27db0":"markdown","a2ad0446":"markdown","30f52c65":"markdown","427e6d8c":"markdown","b7de791c":"markdown","faff9135":"markdown","5c50f659":"markdown","1ffdcc82":"markdown","2681fbfa":"markdown","a897d608":"markdown","58f561f3":"markdown","e926330f":"markdown","86cbae28":"markdown","a1915cc3":"markdown","e74fa24f":"markdown","94a247ca":"markdown","79d1f47f":"markdown","be7e4475":"markdown","89ef8bfe":"markdown","64ba82fd":"markdown","7291cdbc":"markdown","1cbf4cce":"markdown","c2f2abd9":"markdown","fd686097":"markdown","8a0cae59":"markdown","71fd4cb9":"markdown","0a89a67a":"markdown","9b66c4a5":"markdown","139507da":"markdown","0b0814c2":"markdown","f22bb9dd":"markdown","063d9aa4":"markdown","b78e037b":"markdown","c831f961":"markdown","2bc68054":"markdown","dfdcccf0":"markdown","afa20e01":"markdown","15dae6c8":"markdown","d46b389d":"markdown","35f644ac":"markdown","a35de8d9":"markdown","9f1e0190":"markdown","d48cc92a":"markdown","4723569c":"markdown","2c3498ce":"markdown","b4d4ff7b":"markdown","2ac35102":"markdown"},"source":{"b521bf9e":"import pandas as pd","589864b4":"df_feats = pd.read_csv('..\/input\/ds1-kaggle-challenge\/train_features.csv')\ndf_label = pd.read_csv('..\/input\/ds1-kaggle-challenge\/train_labels.csv') ","5262e56b":"df_label.describe(include='object')","423c1325":"#let us see the whole column profile of the data frame \npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\ndf_feats.head()","85a585b0":"df_feats.shape, df_label.shape","4086b82b":"df_label.status_group.value_counts(normalize = True)","1662dbc9":"full = pd.DataFrame.merge(df_label,df_feats)","b3919125":"full.head()","3ff9531d":"full.isnull().sum()","54796db4":"clean = full.dropna(axis = 1)","659ff117":"clean.isna().sum()","b0aed24a":"from sklearn.model_selection import train_test_split\nX1 = clean.drop(columns = ['status_group',], axis = 1)\ny = clean['status_group']\nX_train, X_test, y_train, y_test = train_test_split(X1, y,test_size = .5, random_state=42)","2daaca4b":"X_train.head()","79f34f2d":"X_train.isna().sum().sum()","7bc30717":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nimport category_encoders as ce\nimport numpy as np \nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\ndef dummyEncode(df):\n        columnsToEncode = list(df.select_dtypes(include=['category','object']))\n        le = LabelEncoder()\n        for feature in columnsToEncode:\n            try:\n                df[feature] = le.fit_transform(df[feature])\n            except:\n                print('Error encoding '+feature)\n        return df\n      ","d1363cde":"X_train_DC = dummyEncode(X_train)\nX_train_DC.head()\nX_test_DC = dummyEncode(X_test)\nX_test_DC.head()\nX = dummyEncode(X1)","239b8f84":"X_train_DC.isna().sum().sum()\nX_train_DC.shape","0df32cd5":"model= LogisticRegression()\nmodel.fit(X_train_DC, y_train)\ny_pred = model.predict(X_test_DC)\naccuracy_score(y_test, y_pred)\n","2143c275":"pipeline = make_pipeline(ce.OneHotEncoder(use_cat_names=True),\n                         StandardScaler(), LogisticRegression(solver ='lbfgs',n_jobs=-1, multi_class = 'auto',C=2))\npipeline.fit(X_train_DC, y_train)","fae90621":"y_pred = pipeline.predict(X_train)\n","40d5590d":"pred = pd.DataFrame(y_pred, X_train_DC['id'])","2e783642":"pred.columns = ['status_group']","08926a86":"pred.head()\npred.shape\npred.head()","8bc0cc10":"newsub = pd.DataFrame(pred)\nnewsub.shape\nsub_2 = newsub.index\nsubm = pd.DataFrame( newsub['status_group'],sub_2)\nsubm.head()\nsubm.reset_index(inplace = True)","8b964926":"#subm.to_csv('C:\/Users\/dakot\/Documents\/GitHub\/sumbission1.csv',columns = ['id','status_group'], index = False )","8188b4ed":"subm.shape","5d0605ad":"df_test = pd.read_csv('..\/input\/ds1-kaggle-challenge\/test_features.csv') ","f7d546cf":"df_test.head()","b5eaf0e9":"df_test.isna().sum()\nnona = df_test.dropna(axis = 1)","3792b859":"nona.shape","10758619":"X = dummyEncode(nona)","d9ba04d1":"X.head()","596b4472":"pipeline.fit(X_test, y_test)\ny_preds = pipeline.predict(X)\n\n","8330c24a":"y_preds.shape","74532b9e":"preds = pd.DataFrame(y_preds, X['id'])\npreds.columns = ['status_group']\npreds.head()","502369ff":"newsubs = pd.DataFrame(preds)\nnewsubs.shape\nsub_2s = newsubs.index\nsubms = pd.DataFrame( newsubs['status_group'],sub_2s)\nsubms.head()\nsubms.reset_index(inplace = True)","d699d17b":"subms.head()","5a70d811":"from sklearn import tree\nfrom sklearn.metrics import classification_report\n \nclf = tree.DecisionTreeClassifier(random_state=42)\nclf = clf.fit(X_train, y_train)\n \ny_pred2 = clf.predict(X)\n#print(classification_report(y_test, y_pred2))\n#print('\\nAccuracy: {0:.4f}'.format(accuracy_score(y_test, y_pred2)))","92648eea":"y_pred2.shape","44061bfb":"def format(predictions):\n    pre = pd.DataFrame(predictions, X['id'])\n    pre.columns = ['status_group']\n    new = pd.DataFrame(pre)\n    sub_2s = new.index\n    subs = pd.DataFrame( new['status_group'],sub_2s)\n    subs.reset_index(inplace = True)\n    print(subs.head(),subs.shape)\n    subs.to_csv('C:\/Users\/dakot\/Documents\/GitHub\/sumbission1.csv',columns = ['id','status_group'], index = False )\n    return 'YAY!'\n","55892f74":"pipeline = make_pipeline(ce.OneHotEncoder(use_cat_names=True),\n                         StandardScaler(), LogisticRegression(solver ='lbfgs',n_jobs=-1, multi_class = 'auto',C=2))\npipeline.fit(X_train, y_train)","362dfc57":"pred3 = pipeline.predict(X)","e4ec1821":"pred3","a266ed1b":"treepipe = make_pipeline(ce.OneHotEncoder(use_cat_names=True),\n                         StandardScaler(),tree.DecisionTreeClassifier(random_state=42) )\ntreepipe.fit(X_train, y_train)","9f87833b":"tpred = treepipe.predict(X_test)\nprint(accuracy_score(y_test,tpred))\npred4 = treepipe.predict(X)","c10c614e":"from sklearn.preprocessing import RobustScaler\ntreepipe2 = make_pipeline(ce.OneHotEncoder(use_cat_names=True),\n                         RobustScaler(),tree.DecisionTreeClassifier(random_state=42) )\ntreepipe2.fit(X_train, y_train)\npred = treepipe.predict(X_test)","4b5bcf5b":"accuracy_score(y_test,pred)","a67b2a3d":"pred5 = treepipe2.predict(X)","861b5d15":"pred5\n","45adbc1a":"# the training data set \nfull.isna().sum()","5501c2c2":"full.funder.fillna(full.funder.describe().top,inplace = True)\nfull.installer.fillna(full.installer.describe().top,inplace = True)\nfull.subvillage.fillna(full.subvillage.describe().top, inplace = True)\nfull.public_meeting.fillna(full.public_meeting.describe().top,inplace = True)\nfull.scheme_management.fillna(full.scheme_management.describe().top, inplace = True)\nfull.scheme_name.fillna(full.scheme_name.describe().top, inplace = True)\nfull.permit.fillna(full.permit.describe().top,inplace = True)","9248ec29":"full.isna().sum().sum()","1558570c":"full.columns","3d9f0855":"Xi = full.drop(columns= ['status_group','date_recorded'], axis = 1)\nyi = full['status_group']","1b2088ec":"Xi.shape, yi.shape","5672f0bf":"# DJ split that S*&&%\nX_train, X_test, y_train, y_test = train_test_split(Xi, yi,test_size = .5, random_state=42)\n#now encode it\nX_trains = dummyEncode(X_train)\nX_tests = dummyEncode(X_test)","3440179c":"#how does it like the trees\ncl = tree.DecisionTreeClassifier(random_state=42)\ncl = clf.fit(X_trains, y_train)\n \ny_predictor = clf.predict(X_tests)\nprint(classification_report(y_test, y_predictor))\nprint('\\nAccuracy: {0:.4f}'.format(accuracy_score(y_test, y_predictor)))\n#accuracy of .699 for the train data when split how about the test data","fe7f7348":"test = df_test\nprint(test.shape)\ntest.funder.fillna(test.funder.describe().top,inplace = True)\ntest.installer.fillna(test.installer.describe().top,inplace = True)\ntest.subvillage.fillna(test.subvillage.describe().top, inplace = True)\ntest.public_meeting.fillna(test.public_meeting.describe().top,inplace = True)\ntest.scheme_management.fillna(test.scheme_management.describe().top, inplace = True)\ntest.scheme_name.fillna(test.scheme_name.describe().top, inplace = True)\ntest.permit.fillna(test.permit.describe().top,inplace = True)","f1a40718":"test.head()","1464a41f":"Xt = test.drop(columns = ['date_recorded'], axis = 1)\nXT = dummyEncode(Xt)\n#TREE ME!!!\ncl = tree.DecisionTreeClassifier(random_state=42)\ncl = clf.fit(X_trains, y_train)\n \ny_predictors = clf.predict(XT)\n# print(classification_report(y_test, y_predictor))\n# print('\\nAccuracy: {0:.4f}'.format(accuracy_score(y_test, y_predictor)))","a5c910ad":"#lets hit the pipe testing with standard scale then robust, log_reg and tree\nlogpipe = make_pipeline(RobustScaler(),\n                        tree.DecisionTreeClassifier(random_state=42) )\nlogpipe.fit(X_trains, y_train)\npredlog = logpipe.predict(X_test)\naccuracy_score(y_test,predlog)\n# yeah .64 is no bueno with standard scaler logistic regression\n#robust scale log_regression is.63 which doesnt tickle my fancy \n# Standard scale D tree gives .69 but im not impressed\n#robust scale Dtree gives a slightly higher .699","28d313c1":"# What about different encoding?\nimport category_encoders as ce\nencoder = ce.HashingEncoder()\nhashingpipe = make_pipeline(ce.HashingEncoder(),RobustScaler(),\n                        tree.DecisionTreeClassifier(random_state=42) )\nhashingpipe.fit(X_train, y_train)\npredlogs = hashingpipe.predict(X_test)\naccuracy_score(y_test,predlogs)\n","f41872d1":"df_test1 = pd.read_csv('..\/input\/ds1-kaggle-challenge\/test_features.csv') ","2b3a3271":"from sklearn.preprocessing import MinMaxScaler\ndf_test1.isna().sum()\ndf_test1['gps_height'].replace(0.0, np.nan, inplace=True)\ndf_test1['population'].replace(0.0, np.nan, inplace=True)\ndf_test1['amount_tsh'].replace(0.0, np.nan, inplace=True)\ndf_test1.isnull().sum()","57c9d6a2":"df_test1['gps_height'].fillna(df_test1.groupby(['region', 'district_code'])['gps_height'].transform('mean'), inplace=True)\ndf_test1['gps_height'].fillna(df_test1.groupby(['region'])['gps_height'].transform('mean'), inplace=True)\ndf_test1['gps_height'].fillna(df_test1['gps_height'].mean(), inplace=True)\ndf_test1['population'].fillna(df_test1.groupby(['region', 'district_code'])['population'].transform('median'), inplace=True)\ndf_test1['population'].fillna(df_test1.groupby(['region'])['population'].transform('median'), inplace=True)\ndf_test1['population'].fillna(df_test1['population'].median(), inplace=True)\ndf_test1['amount_tsh'].fillna(df_test1.groupby(['region', 'district_code'])['amount_tsh'].transform('median'), inplace=True)\ndf_test1['amount_tsh'].fillna(df_test1.groupby(['region'])['amount_tsh'].transform('median'), inplace=True)\ndf_test1['amount_tsh'].fillna(df_test1['amount_tsh'].median(), inplace=True)\ndf_test1.isnull().sum()\nfeatures=['amount_tsh', 'gps_height', 'population']\nscaler = MinMaxScaler(feature_range=(0,20))\ndf_test1[features] = scaler.fit_transform(df_test1[features])\ndf_test1[features].head(20)\ndf_test1.isna().sum()\ndf_test1['longitude'].replace(0.0, np.nan, inplace=True)\ndf_test1['latitude'].replace(0.0, np.nan, inplace=True)\ndf_test1['construction_year'].replace(0.0, np.nan, inplace=True)\ndf_test1['latitude'].fillna(df_test1.groupby(['region', 'district_code'])['latitude'].transform('mean'), inplace=True)\ndf_test1['longitude'].fillna(df_test1.groupby(['region', 'district_code'])['longitude'].transform('mean'), inplace=True)\ndf_test1['longitude'].fillna(df_test1.groupby(['region'])['longitude'].transform('mean'), inplace=True)\ndf_test1['construction_year'].fillna(df_test1.groupby(['region', 'district_code'])['construction_year'].transform('median'), inplace=True)\ndf_test1['construction_year'].fillna(df_test1.groupby(['region'])['construction_year'].transform('median'), inplace=True)\ndf_test1['construction_year'].fillna(df_test1.groupby(['district_code'])['construction_year'].transform('median'), inplace=True)\ndf_test1['construction_year'].fillna(df_test1['construction_year'].median(), inplace=True)\ndf_test1['date_recorded'] = pd.to_datetime(df_test1['date_recorded'])\ndf_test1['years_service'] = df_test1.date_recorded.dt.year - df_test1.construction_year\nprint(df_test1.isnull().sum())\n","ae09eda2":"garbage=['wpt_name','num_private','subvillage','region_code','recorded_by','management_group',\n         'extraction_type_group','extraction_type_class','scheme_name','payment',\n        'quality_group','quantity_group','source_type','source_class','waterpoint_type_group',\n        'ward','public_meeting','permit','date_recorded','construction_year']\ndf_test1.drop(garbage,axis=1, inplace=True)","f3ca2d4e":"#take out any random capital letters in the entries\ndf_test1.waterpoint_type = df_test1.waterpoint_type.str.lower()\ndf_test1.funder = df_test1.funder.str.lower()\ndf_test1.basin = df_test1.basin.str.lower()\ndf_test1.region = df_test1.region.str.lower()\ndf_test1.source = df_test1.source.str.lower()\ndf_test1.lga = df_test1.lga.str.lower()\ndf_test1.management = df_test1.management.str.lower()\ndf_test1.quantity = df_test1.quantity.str.lower()\ndf_test1.water_quality = df_test1.water_quality.str.lower()\ndf_test1.payment_type=df_test1.payment_type.str.lower()\ndf_test1.extraction_type=df_test1.extraction_type.str.lower()","6c1e7201":"df_test1.columns\n","fc3438d9":"df_test1[\"funder\"].fillna(\"other\", inplace=True)\ndf_test1[\"scheme_management\"].fillna(\"other\", inplace=True)\ndf_test1[\"installer\"].fillna(\"other\", inplace=True)\ndf_test1.isna().sum()","ae270bbc":"df_test1.head()\ndf_test1.shape","8c66f3e2":"#AUTOMATE ALL THE THINGS!!!\ndef MrClean(df):\n    df_t= df\n    df_t['gps_height'].replace(0.0, np.nan, inplace=True)\n    df_t['population'].replace(0.0, np.nan, inplace=True)\n    df_t['amount_tsh'].replace(0.0, np.nan, inplace=True)\n    df_t['gps_height'].fillna(df_t.groupby(['region', 'district_code'])['gps_height'].transform('mean'), inplace=True)\n    df_t['gps_height'].fillna(df_t.groupby(['region'])['gps_height'].transform('mean'), inplace=True)\n    df_t['gps_height'].fillna(df_t['gps_height'].mean(), inplace=True)\n    df_t['population'].fillna(df_t.groupby(['region', 'district_code'])['population'].transform('median'), inplace=True)\n    df_t['population'].fillna(df_t.groupby(['region'])['population'].transform('median'), inplace=True)\n    df_t['population'].fillna(df_t['population'].median(), inplace=True)\n    df_t['amount_tsh'].fillna(df_t.groupby(['region', 'district_code'])['amount_tsh'].transform('median'), inplace=True)\n    df_t['amount_tsh'].fillna(df_t.groupby(['region'])['amount_tsh'].transform('median'), inplace=True)\n    df_t['amount_tsh'].fillna(df_t['amount_tsh'].median(), inplace=True)\n    features=['amount_tsh', 'gps_height', 'population']\n    scaler = MinMaxScaler(feature_range=(0,20))\n    df_t[features] = scaler.fit_transform(df_t[features])\n    df_t['longitude'].replace(0.0, np.nan, inplace=True)\n    df_t['latitude'].replace(0.0, np.nan, inplace=True)\n    df_t['construction_year'].replace(0.0, np.nan, inplace=True)\n    df_t['latitude'].fillna(df_t.groupby(['region', 'district_code'])['latitude'].transform('mean'), inplace=True)\n    df_t['longitude'].fillna(df_t.groupby(['region', 'district_code'])['longitude'].transform('mean'), inplace=True)\n    df_t['longitude'].fillna(df_t.groupby(['region'])['longitude'].transform('mean'), inplace=True)\n    df_t['construction_year'].fillna(df_t.groupby(['region', 'district_code'])['construction_year'].transform('median'), inplace=True)\n    df_t['construction_year'].fillna(df_t.groupby(['region'])['construction_year'].transform('median'), inplace=True)\n    df_t['construction_year'].fillna(df_t.groupby(['district_code'])['construction_year'].transform('median'), inplace=True)\n    df_t['construction_year'].fillna(df_t['construction_year'].median(), inplace=True)\n    df_t['date_recorded'] = pd.to_datetime(df_t['date_recorded'])\n    df_t['years_service'] = df_t.date_recorded.dt.year - df_t.construction_year\n   \n    garbage=['wpt_name','num_private','subvillage','region_code','recorded_by','management_group',\n         'extraction_type_group','extraction_type_class','scheme_name','payment',\n        'quality_group','quantity_group','source_type','source_class','waterpoint_type_group',\n        'ward','public_meeting','permit','date_recorded','construction_year']\n    df_t.drop(garbage,axis=1, inplace=True)\n    df_t.waterpoint_type = df_t.waterpoint_type.str.lower()\n    df_t.funder = df_t.funder.str.lower()\n    df_t.basin = df_t.basin.str.lower()\n    df_t.region = df_t.region.str.lower()\n    df_t.source = df_t.source.str.lower()\n    df_t.lga = df_t.lga.str.lower()\n    df_t.management = df_t.management.str.lower()\n    df_t.quantity = df_t.quantity.str.lower()\n    df_t.water_quality = df_t.water_quality.str.lower()\n    df_t.payment_type=df_t.payment_type.str.lower()\n    df_t.extraction_type=df_t.extraction_type.str.lower()\n    df_t[\"funder\"].fillna(\"other\", inplace=True)\n    df_t[\"scheme_management\"].fillna(\"other\", inplace=True)\n    df_t[\"installer\"].fillna(\"other\", inplace=True)\n    return df_t","b5e35217":"#Full is the df of both the train_features csv and train_labels merged \nfull = pd.DataFrame.merge(df_label,df_feats)\nfull.shape\nprint(full.columns)","c9676117":"#Call out mrclean!\nsoclean =  MrClean(full)","eda816e2":"soclean.head()\nsoclean.isna().sum()","07be2ad6":"yc = soclean['status_group']\nXc = soclean","e7776588":"Xc.head()","0dfc7a22":"Xc.drop(columns = ['status_group'], axis = 1, inplace = True)","60876fae":"Xc.columns","b1b39307":"#split this ish \nX_train, X_test, y_train, y_test = train_test_split(Xc, yc,test_size = .2, random_state=42)","6bddc179":"X_train.head()","02880234":"# TREES!!!\ncleanpipe = make_pipeline(ce.OneHotEncoder(use_cat_names=True),StandardScaler(),\n                        tree.DecisionTreeClassifier(random_state=42) )\ncleanpipe.fit(X_train, y_train)\npreds = cleanpipe.predict(X_test)\naccuracy_score(y_test,preds)","0855b893":"preddi = cleanpipe.predict(df_test1)","6981a56e":"preddi.shape","e44bfd53":"full = pd.DataFrame.merge(df_label,df_feats)\nfull.shape\nprint(full.columns)\nsoclean =  MrClean(full)\ntrain = soclean\ntest = df_test1","4571c256":"train.shape,test.shape","cd90c07c":"target = train.pop('status_group')\ntrain['train']=1\ntest['train']=0","d56b551f":"combo = pd.concat([train, test])\ncombo.info()","5954bb87":"combo['funder'] = pd.factorize(combo['funder'])[0]\ncombo['installer'] = pd.factorize(combo['installer'])[0]\ncombo['scheme_management'] = pd.factorize(combo['scheme_management'])[0]\ncombo['extraction_type'] = pd.factorize(combo['extraction_type'])[0]\ncombo['management'] = pd.factorize(combo['management'])[0]\ncombo['payment_type'] = pd.factorize(combo['payment_type'])[0]\ncombo['water_quality'] = pd.factorize(combo['water_quality'])[0]\ncombo['quantity'] = pd.factorize(combo['quantity'])[0]\ncombo['source'] = pd.factorize(combo['source'])[0]\ncombo['waterpoint_type'] = pd.factorize(combo['waterpoint_type'])[0]\ncombo['basin'] = pd.factorize(combo['basin'])[0]\ncombo['region'] = pd.factorize(combo['region'])[0]\ncombo['lga'] = pd.factorize(combo['lga'])[0]\ncombo['district_code'] = pd.factorize(combo['district_code'])[0]\ncombo['years_service'] = pd.factorize(combo['years_service'])[0]\ncombo.head()","2b4077b1":"train_df = combo[combo[\"train\"] == 1]\ntest_df = combo[combo[\"train\"] == 0]\ntrain_df.drop([\"train\"], axis=1, inplace=True)\ntrain_df.drop(['id'],axis=1, inplace=True)\ntest_df.drop([\"train\"], axis=1, inplace=True)","0dd3387c":"X = train_df\ny = target","75884df7":"X.shape,y.shape","7ca3c68f":"y.head()","b8b238c2":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nmodel_rfc = RandomForestClassifier(n_estimators=1000, n_jobs = -1)","98551401":"score = cross_val_score(model_rfc, X, y, cv=3, n_jobs = -1)","9530d2a7":"score.mean()","78908097":"X_test=test_df\nX_test.shape","789f9869":"model_rfc.fit(X,y)","3f17d151":"X.info()\nimportances = model_rfc.feature_importances_\nimportances","e6c6cf41":"X_test.shape, X.shape","82178039":"a=X_test['id']\nX_test.drop(['id'],axis=1, inplace=True)\ny_pred = model_rfc.predict(X_test)","91b5c3e9":"y_pred","d2640848":"a.head()","60d959e5":"y_pred.shape,a.shape","8beaece7":"y_pred=pd.DataFrame(y_pred)\ny_pred['id']=a\ny_pred.columns=['status_group','id']\ny_pred=y_pred[['id','status_group']]","e0d22299":"y_pred.head()","7b728703":"from xgboost import XGBClassifier\nmodelxgb = XGBClassifier(objective = 'multi:softmax', booster = 'gbtree', nrounds = 'min.error.idx', \n                      num_class = 4, maximize = False, eval_metric = 'merror', eta = .2,\n                      max_depth = 14, colsample_bytree = .4)","e5cb5a89":"#print(cross_val_score(modelxgb, X, y, cv=3,n_jobs = -1))\nmodelxgb.fit(X,y)","a0424897":"y_preds = modelxgb.predict(X_test)","dff5d0c1":"y_preds=pd.DataFrame(y_preds)\ny_preds['id']=a\ny_preds.columns=['status_group','id']\ny_preds=y_preds[['id','status_group']]","583efe6c":"y_preds.shape","06d9a045":"y_preds.head()","ca4792b4":"from sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=1000)","73d004f7":"scores = (cross_val_score(knn, X, y, cv=3,n_jobs = -1))\nscores.mean()","e3c7e5ff":"cl = tree.DecisionTreeClassifier(random_state=42)\n","f2b0c872":"cl.fit(X,y)","15a1ae5f":"y_predcl = cl.predict(X_test)","f65062a1":"y_predcl=pd.DataFrame(y_predcl)\ny_predcl['id']=a\ny_predcl.columns=['status_group','id']\ny_predcl=y_predcl[['id','status_group']]","aac62f66":"y_predcl.head()","d3cd500d":"y_predcl.shape","bcbcfcd8":"log = LogisticRegression(solver ='saga',n_jobs=-1, multi_class = 'auto',C=1.0)","d6baa37f":"print(cross_val_score(log, X, y, cv=3,n_jobs = -1))\nlog.fit(X,y)","cc4c1b22":"y_predlog =log.predict(X_test)","1d77bfe2":"y_predlog=pd.DataFrame(y_predlog)\ny_predlog['id']=a\ny_predlog.columns=['status_group','id']\ny_predlog=y_predlog[['id','status_group']]","fd96c6e6":"y_predlog.head()","d6f093fb":"from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import cross_val_score","676fe3f6":"clf = ExtraTreesClassifier(n_estimators=500, max_depth=None,\n                           min_samples_split=10, random_state=0)\nscores = cross_val_score(clf, X, y, cv=5)","bb54b569":"scores.mean()","210878c4":"clf = RandomForestClassifier(n_estimators=1000, max_depth=None,\n                             min_samples_split=8, random_state=0 , \n                             n_jobs=-1)\nscores = cross_val_score(clf, X, y, cv=5)\nscores.mean()                               ","176fd9a5":"clf.fit(X,y)","9082ee74":"pred_rfc =clf.predict(X_test)","68370e73":"pred_rfc=pd.DataFrame(pred_rfc)\npred_rfc ['id']=a\npred_rfc .columns=['status_group','id']\npred_rfc = pred_rfc[['id','status_group']]","fb54be98":"pred_rfc.head()","60476297":"from xgboost import XGBClassifier\nmodelxgb = XGBClassifier(objective = 'multi:softmax', booster = 'gbtree', nrounds = 'min.error.idx', \n                      num_class = 3, maximize = False, eval_metric = 'merror', eta = .1,\n                      max_depth = 14, colsample_bytree = .4)","0db37ad4":"score = (cross_val_score(modelxgb, X, y, cv=5,n_jobs = -1))\nscore.mean()","a310f3d5":"modelxgb.fit(X,y)","f16e38ac":"predict = modelxgb.predict(X_test)","1c944c71":"predict=pd.DataFrame(predict)\npredict ['id']=a\npredict .columns=['status_group','id']\npredict = predict[['id','status_group']]\n","78d59c07":"predict.head()","db8a9569":"**The above got me a baseline of .53754 on kaggle. we can do better than that. **","e15e90a5":"## Ok for real this time decision tree in a pipeline \n","ebec8807":"clf = RandomForestClassifier(n_estimators=2000, max_depth=None,\n                             min_samples_split=10, random_state=0  \n                             )\nscores = cross_val_score(clf, X, y, cv=5)\nscores.mean()    \n0.8134174558068722\n\nclf = RandomForestClassifier(n_estimators=1000, max_depth=None,\n                             min_samples_split=10, random_state=0 , \n                             n_jobs=-1)\nscores = cross_val_score(clf, X, y, cv=5)\nscores.mean()                               \n0.8135689539533196\n\n","2254663d":"**It was at this point I realized I was working on my test data and not the train, so now in order to test the models I need to clean the training data. **","69e27db0":"![](https:\/\/media1.tenor.com\/images\/b9f65b516415511fcca2ffcca76d7c8e\/tenor.gif?itemid=12454910)![image.png](attachment:image.png)","a2ad0446":"For simplicity I will combine the two data frames to process before splitting. \n","30f52c65":"# XGboost falls short with 81292","427e6d8c":"![](https:\/\/media.giphy.com\/media\/X6w1HKaFXLAAw\/giphy.gif)","b7de791c":"# Random forest gets the lead. 0.81487","faff9135":"![image.png](attachment:image.png)","5c50f659":"**The goal is to identify with above 60% accuracy which water wells are faulty or non functional.** I will be using data from Taarifa and the Tanzanian Ministry of Water. The submission of my predictions will be in the format of .CSV with columns for 'id' as well as 'status_group'. Lets start by loading the data and getting a feel for it. ","1ffdcc82":"# A few more lose ends to tie up...","2681fbfa":"**Score = 0.71040**","a897d608":"![image.png](attachment:image.png)","58f561f3":"![](https:\/\/media1.tenor.com\/images\/e92324f31edc80fe7beb1c80194de76f\/tenor.gif?itemid=9643872)","e926330f":"#  Predict which water pumps are faulty","86cbae28":"# Lets find Null values and replace them or send them to the dumpster fire out back. ","a1915cc3":"** Better results were found when grouping subsets together then applying the mean or median of those groupings to the missing values. The code below contains many iterations worth of groupings and modifications, some are useful in the position they're in and others are not but as a whole this code block is operational and the outcome is the one expected. Also a column has been added ('years_service') to track the years since construction because the older the well the more likely it is fatigued and will fail. **","e74fa24f":"**Now that we have no NaN values lets make some test and tarin sets with our data. We want to predict status so we will call that the 'y' variable. All other features will be called our 'X' matrix of features.** ","94a247ca":"**After merging the data frames together I find there are some Null values that may skew our results or otherwise break our functions during the process.** For now we will drop all instances that are missing values, but later we may impute some values to help our models predict better if neeeded.  ","79d1f47f":"**The df_feats Data Frame contains all the features we will use to predict the status of any given well.** For the first iteration I will run a simple baseline. We see the shape of the features df is 59400 by 40 and the shape of the label df is 59400 by 2. Lets check the distribution of the status of the wells. ","be7e4475":"# With that low of cross vals I wont even submit the KNN ","89ef8bfe":"# Alright thats a lot of code for a little cleaning. Lets dump some trash ","64ba82fd":"** These categories called Garbage were the features I found to be redundant, colinear, or otherwise too far gone to be worth my limited time in this competition. At some point I will revisit these columns to see which can be spared when time is less restricted. **","7291cdbc":"**Some values could potentially be zero, but would be quite unlikely. For instance the hieght of the gps recorded in \"gps_hieght\" COULD be exactly 0.0 but is highly unlikely and is probably the effect of someone just not recording an actual hieght. So lets fix those instances!**","1cbf4cce":"**Monkey patch complete on training data. Lets see the effects**","c2f2abd9":"**0.71973 now for some other models**","fd686097":"**Your submission scored 0.68881**","8a0cae59":"# Decision Tree Classifier leads!\nkaggle score for the tree without a pipline  = 0.71054\nNow lets pipeline this baby!","71fd4cb9":"![](https:\/\/media1.giphy.com\/media\/ujvW8qiDfbCJRnIHPq\/giphy.gif?cid=3640f6095c5c88d06450675932107813)","0a89a67a":"![](https:\/\/media3.giphy.com\/media\/hLPNDUZ3ntKEM\/200w.webp?cid=3640f6095c5c99ae696f576c41729322)","9b66c4a5":"Now to make it work for the actual test set. \n","139507da":"# XGB Score = 0.81292","0b0814c2":"# Allright so far im not breaking through this barrier. So far the best score came from dropping all the nan values and a decision tree. Time for a change of thought..  \n","f22bb9dd":"When looking through columns individually I noticed some variation in placement of capital letters so lets fix that. ","063d9aa4":"**Score = 0.71040**","b78e037b":"# Monkey Patch All The DATAS!!!!!!","c831f961":"# Thanks for reading and be sure to vote!","2bc68054":"# The things you are about to see are the ramblings of a mad man. If any of this triggers a lightbulb at any point you may be mad as well. Enjoy \n*** For those with a short attention span scroll down till you see the flashing stop sign.***","dfdcccf0":"**The df_label data frame contains the id along with status of the well. The status will be our target.**","afa20e01":"# logistic regression = 0.61359","15dae6c8":"# MONKEY PATCHING TIME!!!!","d46b389d":"**Overall it appears the majority of wells are functional(54.3%), non functional is the second highest(38.42%). Functional needing repair rounds out the data set(7.26%).** If I were to make a blind prediction saying that all the wells were functional I would be correct around 54 percent of the time. Not bad but not nearly conclusive or useful for the real world. Lets dig deeper. ","35f644ac":"# Lets set up a different way, it may make it easier to work with. ","a35de8d9":"**Standard scaled one hot encoded log_reg =  0.63769** ","9f1e0190":"Lets try to automate the formatting for submission. ","d48cc92a":"# Mr. Clean did his job, no missing values! ","4723569c":"# TIME FOR TREES","2c3498ce":"# Ok this far I've dropped all rows with NAN's, lets fix some of the columns and see if that helps","b4d4ff7b":"# Decision tree classifier gets 0.70566","2ac35102":"# Now that process again on the training data....YAY!\nlets try to automate that mess "}}