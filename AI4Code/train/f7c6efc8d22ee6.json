{"cell_type":{"9d2f949f":"code","cce8820b":"code","f273ec14":"code","15d7f336":"code","c22f1a52":"code","ad75995e":"code","32d09311":"code","bcbf500c":"code","da8752ba":"code","6ba25462":"code","7cd2fcdd":"code","af2e599e":"code","9d829927":"code","af4dd6b7":"code","0cab398b":"code","93500061":"code","e4990288":"code","55480851":"code","711813bf":"code","af59ec74":"code","753716b6":"code","18a548bd":"code","8bce5c0d":"code","57054abf":"markdown","3d817abf":"markdown","c7159335":"markdown","735e7efb":"markdown","78b40cc2":"markdown","788ec508":"markdown","6693b7aa":"markdown","e71dff6b":"markdown"},"source":{"9d2f949f":"import numpy as np \nimport pandas as pd \nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Conv2D, Activation, MaxPool2D, BatchNormalization, Flatten, Dense, Dropout\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import SGD\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.applications.vgg16 import VGG16\nfrom glob import glob\nimport cv2\nimport os","cce8820b":"training_dir = '..\/input\/fruits\/fruits-360\/Training\/'\nvalidation_dir = '..\/input\/fruits\/fruits-360\/Test\/'\ntest_dir = '..\/input\/fruits\/fruits-360\/test-multiple_fruits\/'","f273ec14":"print(len(next(os.walk(training_dir))[1]))","15d7f336":"my_list = os.listdir(training_dir)\nprint(my_list)","c22f1a52":"fig = plt.figure(figsize=(8,8))\n\nax1 = fig.add_subplot(3,3,1)\nax1.imshow(np.array(Image.open(\"..\/input\/fruits\/fruits-360\/Training\/Apple Braeburn\/101_100.jpg\")))\n\nax2 = fig.add_subplot(3,3,2)\nax2.imshow(np.array(Image.open(\"..\/input\/fruits\/fruits-360\/Training\/Banana\/0_100.jpg\")))\n\nax3 = fig.add_subplot(3,3,3)\nax3.imshow(np.array(Image.open(\"..\/input\/fruits\/fruits-360\/Training\/Corn\/0_100.jpg\")))\n\nax4 = fig.add_subplot(3,3,4)\nax4.imshow(np.array(Image.open(\"..\/input\/fruits\/fruits-360\/Training\/Cherry 2\/108_100.jpg\")))\n\nax5 = fig.add_subplot(3,3,5)\nax5.imshow(np.array(Image.open(\"..\/input\/fruits\/fruits-360\/Training\/Lemon\/100_100.jpg\")))\n\nax6 = fig.add_subplot(3,3,6)\nax6.imshow(np.array(Image.open(\"..\/input\/fruits\/fruits-360\/Training\/Mango\/102_100.jpg\")))\n\nax7 = fig.add_subplot(3,3,7)\nax7.imshow(np.array(Image.open(\"..\/input\/fruits\/fruits-360\/Training\/Orange\/102_100.jpg\")))\n\nax8 = fig.add_subplot(3,3,8)\nax8.imshow(np.array(Image.open(\"..\/input\/fruits\/fruits-360\/Training\/Peach 2\/11_100.jpg\")))\n\nax9 = fig.add_subplot(3,3,9)\nax9.imshow(np.array(Image.open(\"..\/input\/fruits\/fruits-360\/Training\/Strawberry\/101_100.jpg\")))","ad75995e":"vgg = VGG16()","32d09311":"print(vgg.summary())","bcbf500c":"vgg_layer_list = vgg.layers\nprint(len(vgg_layer_list))","da8752ba":"model = Sequential()\nfor i in range(len(vgg_layer_list)-1):\n    model.add(vgg_layer_list[i])\n    \nprint(model.summary())","6ba25462":"numberOfClass = 131\n\nfor layers in model.layers:\n    layers.trainable = False\n    \nmodel.add(Dense(numberOfClass,activation=\"softmax\"))\nprint(model.summary())","7cd2fcdd":"model_layer_list = model.layers\nprint(len(model_layer_list)) #22'ye d\u00fc\u015ft\u00fc layer say\u0131s\u0131.","af2e599e":"model.compile(loss=\"categorical_crossentropy\",\n             optimizer=\"rmsprop\",\n             metrics=[\"accuracy\"])\n\ntrain_data = ImageDataGenerator().flow_from_directory(training_dir,target_size=(224,224))\n\ntest_data = ImageDataGenerator().flow_from_directory(validation_dir,target_size=(224,224))","9d829927":"batch_size=64\nhist = model.fit_generator(train_data,\n                           steps_per_epoch=1600\/\/batch_size,\n                           epochs=10,\n                           validation_data=test_data,\n                           validation_steps=800\/\/batch_size)","af4dd6b7":"model.save_weights(\"fruits_model.h5\") #save the model","0cab398b":"print(hist.history.keys())","93500061":"plt.figure(figsize=(18,6))\nplt.plot(hist.history[\"accuracy\"],label=\"training accuracy\")\nplt.plot(hist.history[\"val_accuracy\"],label=\"validation accuracy\")\nplt.legend()\nplt.show();","e4990288":"plt.figure(figsize=(15,8))\nplt.plot(hist.history[\"loss\"],label=\"training loss\")\nplt.plot(hist.history[\"val_loss\"],label=\"validation loss\")\nplt.legend()\nplt.show();","55480851":"print(\"Model accuracy is: \",hist.history[\"accuracy\"][-1])","711813bf":"idx_to_name = {x:i for (x,i) in enumerate(train_data.class_indices)}\n\ndef predict(img):\n    to_predict = np.zeros(shape=train_data[0][0].shape)\n    to_predict[0] = img\n    \n    return idx_to_name[np.argmax(model(to_predict)[0])]","af59ec74":"apple = cv2.imread(\"..\/input\/fruits\/fruits-360\/Training\/Apple Braeburn\/101_100.jpg\")\nbanana = cv2.imread(\"..\/input\/fruits\/fruits-360\/Training\/Banana\/0_100.jpg\")\nmango = cv2.imread(\"..\/input\/fruits\/fruits-360\/Training\/Mango\/102_100.jpg\")\ncorn = cv2.imread(\"..\/input\/fruits\/fruits-360\/Training\/Corn\/0_100.jpg\")\npeach = cv2.imread(\"..\/input\/fruits\/fruits-360\/Training\/Peach 2\/11_100.jpg\")\ncherry = cv2.imread(\"..\/input\/fruits\/fruits-360\/Training\/Cherry 2\/108_100.jpg\")\nlemon = cv2.imread(\"..\/input\/fruits\/fruits-360\/Training\/Lemon\/100_100.jpg\")\norange = cv2.imread(\"..\/input\/fruits\/fruits-360\/Training\/Orange\/102_100.jpg\")\nstrawberry = cv2.imread(\"..\/input\/fruits\/fruits-360\/Training\/Strawberry\/101_100.jpg\")","753716b6":"'''INTER_AREA \u2013 resampling using pixel area relation. It may be a preferred method for image decimation, \nas it gives moire\u2019-free results. But when the image is zoomed, it is similar to the INTER_NEAREST method'''\n\nresized = cv2.resize(corn, (224,224), interpolation = cv2.INTER_AREA) #Predict i\u00e7in burdaki meyve ismini de\u011fi\u015ftir.","18a548bd":"plt.imshow(resized)","8bce5c0d":"predict(resized)","57054abf":"## Architecture <br>\nVGG is an acronym for the **Visual Geometric Group** from Oxford University and VGG-16 is **a network with 16 layers** proposed by the Visual Geometric Group. These 16 layers contain the trainable parameters and there are other layers also like the Max pool layer but those do not contain any trainable parameters.\n![](https:\/\/miro.medium.com\/max\/1400\/1*_Lg1i7wv1pLpzp2F4MLrvw.png)","3d817abf":"### Import","c7159335":"### Features of VGG-16 network\n* **Input Layer**: It accepts color images as an input with the size 224 x 224 and 3 channels i.e. Red, Green, and Blue.\n* **Convolution Layer:** The images pass through a stack of convolution layers where every convolution filter has a very small receptive field of 3 x 3 and stride of 1. Every convolution kernel uses row and column padding so that the size of input as well as the output feature maps remains the same or in other words, the resolution after the convolution is performed remains the same.\n* **Max pooling:** It is performed over a max-pool window of size 2 x 2 with stride equals to 2, which means here max pool windows are non-overlapping windows. <br> \n\n***NOTES:*** <br>\n*Not every convolution layer is followed by a max pool layer as at some places a convolution layer is following another convolution layer without the max-pool layer in between.* <br>\n*The hidden layers have ReLU as their activation function.*","735e7efb":"### Implementation","78b40cc2":"The flattened architecture of VGG-16 is as shown below:\n![](https:\/\/miro.medium.com\/max\/1400\/1*ZqkQYVB3_Gw0hjrAMzi6_A.png)","788ec508":"### Data ","6693b7aa":"### Prediction","e71dff6b":"How many fruit classes do we have?"}}