{"cell_type":{"387f40f9":"code","a13f6243":"code","5cafa493":"code","1ea6cb4b":"code","6ff305bb":"code","4289895d":"code","5f4145d8":"code","30aa1cd9":"code","264a68de":"code","551d12d9":"code","7f97412a":"code","b91043da":"code","9e0fda4d":"code","5d65677d":"markdown"},"source":{"387f40f9":"# Loading libraries\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas_profiling as pp\nimport sklearn.preprocessing as preprocessing\n%matplotlib inline\nimport re","a13f6243":"# Loading in the data\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\nall_data = [train, test]\n\n# Df display settings\npd.pandas.set_option('display.max_rows', None)\npd.pandas.set_option('display.max_columns', None)\n\n# For referencing\nTest_PassengerId = test['PassengerId']\nTrain_PassengerId = train['PassengerId']","5cafa493":"train.head()","1ea6cb4b":"test.head()","6ff305bb":"# Creating pandas.df.profileReports\ntrain_profile = pp.ProfileReport(train, minimal=False)\n#train_profile","4289895d":"################################\n########## CLEANING ############\n################################\n\n#  Drop the missing lines (2) of Embarked\ntrain.dropna(subset=['Embarked'],inplace = True)\n\n# Cabin's data has HIGH CARDINALITY (Distinct=72.1%) and a lot of MISSING values.\n#  We can clean this up a little by making a new feature (Has_Cabin) as a binary summary\nfor dataset in all_data:\n    dataset['Cabin'] = dataset['Cabin'].fillna(0);\n    dataset['Has_Cabin'] = dataset[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n\n#  Parch's data has HIGH CORRELATION but a number of zeros... (76.1%), SibSp's data has HIGH CORRELATION but a number of zeros... (68.2%) \n#  We can clean this up a little by combining the data into a new feature (RelativesOnboard) - combination of SibSp and Parch \nfor dataset in all_data:\n    dataset['RelativesOnboard'] = dataset['SibSp'] + dataset['Parch']\n\n#  Ticket's data has HIGH CARDINALITY (Distinct=76.4%) \n#  We could categorise ticket codes but there is such a high cardinality that it's worth just dropping due to our encoding requirements.\n#dataset.drop('Ticket', axis='columns', inplace=True)\n\n#  Fare has 15 (1.7%) zeros then apply quantile-based discretization\nfor dataset in all_data:\n    dataset['Fare'] = dataset['Fare'].fillna(dataset['Fare'].median())\n\n#  Extract person titles into a new column, then group less cardinal title groups -> other\nfor dataset in all_data:\n    dataset['Title'] = dataset.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\n    dataset['Title'] = dataset['Title'].replace(['Dr', 'Rev', 'Major', 'Col', 'the Countess', 'Capt', 'Sir', 'Lady', 'Mme', 'Don', 'Dona','Jonkheer'], 'Other')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n\n\n#  Fill the missing age values (~280) with the median of that sex\/pclass title\nfor dataset in all_data:\n    dataset['Age'].fillna(dataset.groupby([\"Sex\",\"Pclass\",\"Title\"])['Age'].transform('median'),inplace=True)","5f4145d8":"################################\n########### MAPPING ############\n################################\n#  Titles...  \ntitles_map = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Other\": 5}\ntrain['Title'] = train['Title'].map(titles_map) \ntest['Title'] = test['Title'].map(titles_map).astype(int)\n\n#  Sex...\nage_map = {'female': 0, 'male': 1}\nfor dataset in all_data:\n    dataset['Sex'] = dataset['Sex'].map(age_map).astype(int)\n\n#  Embarked...\nembarked_map = {'S': 0, 'C': 1, 'Q': 2}\nfor dataset in all_data:\n    dataset['Embarked'] = dataset['Embarked'].map(embarked_map)\n\n#  Fare...\nfor dataset in all_data:\n    dataset['FareCat'] = pd.qcut(dataset['Fare'], 4, labels=False)\n\n#  Age...\nfor dataset in all_data:\n    dataset['AgeCat'] = pd.qcut(dataset['Age'], 6, labels=False)","30aa1cd9":"################################\n###### FEATURES SELECTION ######\n################################\n#  Create reference for elements_to_drop\nelements_to_drop = ['PassengerId', 'Name', 'Ticket', 'Age', 'Cabin', 'SibSp', 'Parch', 'Fare']\n\n#  Drop unneccessary columns\ntrain = train.drop(elements_to_drop, axis = 1)\ntest = test.drop(elements_to_drop, axis = 1)\ntrain.head(6)","264a68de":"test.head(6)","551d12d9":"#  Model libraries\nimport sklearn\nimport xgboost as xgb\nfrom sklearn import tree\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\n\n#  Train\/test split\ny_train = train['Survived'].ravel()\ntrain = train.drop(['Survived'], axis=1)\nx_train = train.values\nx_test = test.values ","7f97412a":"#  Baseline model builds\n#  GaussianNB\ngnb = GaussianNB()\ngnb_cv = cross_val_score(gnb,x_train,y_train,cv=5).mean()\ngnb_cv_percent = \"{:.2%}\".format(gnb_cv) \n\n#  Random Forest\nrf = RandomForestClassifier(random_state = 1).fit(x_train, y_train)\nrf_cv = cross_val_score(rf,x_train, y_train,cv=5).mean()\nrf_cv_percent = \"{:.2%}\".format(rf_cv)\n\n#  Logistic Regression\nlr = LogisticRegression(max_iter = 2000)\nlr_cv = cross_val_score(lr,x_train, y_train,cv=5).mean()\nlr_cv_percent = \"{:.2%}\".format(lr_cv)\n\n#  XGBoost \nxgboost = xgb.XGBClassifier(\n    random_state =1,\n    use_label_encoder=False,\n    eval_metric='mlogloss').fit(x_train, y_train)\nxgb_cv = cross_val_score(xgboost,x_train, y_train,cv=5).mean()\nxgb_cv_percent = \"{:.2%}\".format(xgb_cv)\n\n#  K Nearest Neighbors\nknn = KNeighborsClassifier()\nknn_cv = cross_val_score(knn,x_train, y_train,cv=5).mean()\nknn_cv_percent = \"{:.2%}\".format(knn_cv)\n\n#  Voting Classifier - makes prediction using the average of the estimator models\nvoter = VotingClassifier(estimators = [('lr',lr),('knn',knn),('rf',rf),('gnb',gnb),('xgb',xgboost)], voting = 'soft') \nvoter_cv = cross_val_score(voter,x_train,y_train,cv=5).mean()\nvoter_cv_percentage = \"{:.2%}\".format(voter_cv)\n\nprint('\\033[1m' + 'Voted Accuracy: '+ '\\033[0m', voter_cv_percentage)\nprint('--------------------')\nprint('\\033[1m' + 'GaussianNB Accuracy: '+ '\\033[0m', gnb_cv_percent)\nprint('\\033[1m' + 'Random Forestr Accuracy: '+ '\\033[0m', rf_cv_percent)\nprint('\\033[1m' + 'Logistic Regression Accuracy: '+ '\\033[0m', lr_cv_percent)\nprint('\\033[1m' + 'XGB Classifier Accuracy: '+ '\\033[0m', xgb_cv_percent)\nprint('\\033[1m' + 'KNN Accuracy: '+ '\\033[0m', knn_cv_percent)\n","b91043da":"# Initial submission \nvoter.fit(x_train,y_train)\npredictions = voter.predict(x_test).astype(int)\ninitial_submission = {'PassengerId': Test_PassengerId, 'Survived': predictions}\ninitial_submission = pd.DataFrame(data=initial_submission)\ninitial_submission.head(4)","9e0fda4d":"initial_submission.to_csv('submission.csv', index=False)","5d65677d":"## Model building\nStarting with 5 fold validation to establish baselines.\n\n- Naive Bayes (79.7%)\n- Random Forest (79.8%)\n- **Logistic Regression (81.2%)**\n- XGBoost (80.8%)\n- K Nearest Neighbour (80.2%)"}}