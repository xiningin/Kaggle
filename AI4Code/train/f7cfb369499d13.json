{"cell_type":{"3a99ff32":"code","759bede5":"code","b8f7d5c0":"code","b2b8ab7d":"code","addf26cf":"code","ef2fccf5":"code","ddcc3bb6":"code","986ce7e6":"code","ebc851cb":"code","0f2453e3":"code","0dec7657":"code","bce23ca8":"code","e8ef5122":"code","7b1f56e2":"code","a9184b29":"code","885c7240":"code","88605ad3":"code","d9d0835d":"code","277ac92a":"code","addb38fc":"code","4cd18747":"code","2af96789":"code","dfcb54c5":"code","25214c05":"code","06fcc0b0":"code","803c5ab7":"code","d077b0d9":"code","f463b5ff":"code","369b0ce4":"code","110861c0":"code","e272d02c":"code","dfe9c50d":"code","44c68706":"code","51e7aa65":"code","a7597b82":"code","aa873bed":"code","8a5ded1d":"markdown","d0680b44":"markdown","ac83c8f7":"markdown","9fe98db6":"markdown"},"source":{"3a99ff32":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/\"))\n\n# Any results you write to the current directory are saved as output.","759bede5":"from datetime import datetime\n\nfrom sklearn.metrics import accuracy_score,precision_score,roc_curve,roc_auc_score,classification_report,confusion_matrix\nfrom sklearn.model_selection import GridSearchCV,KFold,train_test_split,learning_curve,cross_val_score,RandomizedSearchCV\nfrom sklearn.preprocessing import LabelEncoder,StandardScaler,MinMaxScaler\nimport seaborn as se\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.figure_factory as ff\nimport seaborn as sns\nfrom collections import Counter\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.pipeline import Pipeline\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import ExtraTreeClassifier\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.linear_model import LogisticRegression","b8f7d5c0":"data = pd.read_csv(\"\/kaggle\/input\/voicegender\/voice.csv\") #","b2b8ab7d":"data.head()","addf26cf":"print(\"The shape of the DataFrame {}\".format(data.shape))","ef2fccf5":"data.info()","ddcc3bb6":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()\/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))","986ce7e6":"missing_data(data)","ebc851cb":"M = data[(data['label'] == 'male')]\nB = data[(data['label'] == 'female')]\ntrace = go.Bar(x = (len(M), len(B)), y = ['male','female'], orientation = 'h', opacity = 0.8, marker=dict(\n        color=['red','green'],\n        line=dict(color='#000000',width=1.5)))\n\nlayout = dict(title =  'Count of Target variable')\n                    \nfig = dict(data = [trace], layout=layout)\npy.iplot(fig)","0f2453e3":"sns.pairplot(data)","0dec7657":"g = sns.distplot(data['skew'])\ng.set_title(\"Skewness Distribuition\", fontsize=18)\ng.set_xlabel(\"\")\ng.set_ylabel(\"Probability\", fontsize=12)","bce23ca8":"sns.distplot(data['dfrange'],bins=30)","e8ef5122":"# Coorelation analysis\nplt.figure(figsize=(15,10))\nsns.heatmap(data.corr(), annot=True, cmap='coolwarm')","7b1f56e2":"data.head()","a9184b29":"plt.plot(data['meanfreq'])\nplt.xlabel('MeanFreqency')\nplt.ylabel('Range')\nplt.show()","885c7240":"trace = go.Scatter(x=data['meanfreq'], y=data['sd'],\n                    mode='markers',\n                    name='markers')\n\nlayout = dict(title =  'Plot b\/w meanfreq & sd')\n                    \nfig = dict(data = [trace], layout=layout)\npy.iplot(fig)","88605ad3":"trace = go.Scatter(x=data['minfun'], y=data['maxfun'],\n                    mode='markers',\n                    name='markers')\n\nlayout = dict(title =  'Plot b\/w Minfun & Maxfun')\n                    \nfig = dict(data = [trace], layout=layout)\npy.iplot(fig)","d9d0835d":"trace = go.Scatter(x=data['skew'], y=data['kurt'],\n                    mode='markers',\n                    name='markers')\n\nlayout = dict(title =  'Plot b\/w skewness & kurtosis')\n                    \nfig = dict(data = [trace], layout=layout)\npy.iplot(fig)","277ac92a":"trace = go.Scatter(x=data['mindom'], y=data['maxdom'],\n                    mode='markers',\n                    name='markers')\n\nlayout = dict(title =  'Plot b\/w mindom & maxdom')\n                    \nfig = dict(data = [trace], layout=layout)\npy.iplot(fig)","addb38fc":"data.label = data.label.apply(lambda x: 1 if x == 'male' else 0)\nX = data.drop('label',1)\ny = data.label\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)","4cd18747":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","2af96789":"clfs = []\nseed = 3\n\nclfs.append((\"LogReg\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"LogReg\", LogisticRegression())])))\n\nclfs.append((\"XGBClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"XGB\", XGBClassifier())]))) \nclfs.append((\"KNN\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"KNN\", KNeighborsClassifier())]))) \n\nclfs.append((\"DecisionTreeClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"DecisionTrees\", DecisionTreeClassifier())]))) \n\nclfs.append((\"RandomForestClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"RandomForest\", RandomForestClassifier())]))) \n\nclfs.append((\"GradientBoostingClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"GradientBoosting\", GradientBoostingClassifier(max_features=15, \n                                                                       n_estimators=600))]))) \n\nclfs.append((\"RidgeClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"RidgeClassifier\", RidgeClassifier())])))\n\n\nclfs.append((\"ExtraTreesClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"ExtraTrees\", ExtraTreeClassifier())])))\n\nscoring = 'accuracy'\nn_folds = 10\nmsgs = []\nresults, names  = [], [] \n\nfor name, model  in clfs:\n    kfold = KFold(n_splits=n_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, y_train, \n                                 cv=kfold, scoring=scoring, n_jobs=-1)    \n    names.append(name)\n    results.append(cv_results)    \n    msg = \"%s: %f (+\/- %f)\" % (name, cv_results.mean(),  \n                               cv_results.std())\n    msgs.append(msg)\n    print(msg)","dfcb54c5":"# creating a model\nmodel = RandomForestClassifier()\n\n# feeding the training set into the model\nmodel.fit(X_train, y_train)\n\n# predicting the test set results\ny_pred = model.predict(X_test)\n\n# Calculating the accuracies\nprint(\"Training accuracy :\", model.score(X_train, y_train))\nprint(\"Testing accuarcy :\", model.score(X_test, y_test))\n\n# classification report\ncr = classification_report(y_test, y_pred)\nprint(cr)\n\n# confusion matrix \ncm = confusion_matrix(y_test, y_pred)\nplt.rcParams['figure.figsize'] = (5, 5)\nse.heatmap(cm, annot = True, cmap = 'winter')\nplt.title('Confusion Matrix', fontsize = 20)\nplt.show()","25214c05":"def plot_confusion_matrix(cm, classes,\n                          normalize = False,\n                          title = 'Confusion matrix\"',\n                          cmap = plt.cm.Blues) :\n    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation = 0)\n    plt.yticks(tick_marks, classes)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])) :\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment = 'center',\n                 color = 'white' if cm[i, j] > thresh else 'black')\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    \n# Show metrics \ndef show_metrics():\n    tp = cm[1,1]\n    fn = cm[1,0]\n    fp = cm[0,1]\n    tn = cm[0,0]\n    print('Accuracy  =     {:.3f}'.format((tp+tn)\/(tp+tn+fp+fn)))\n    print('Precision =     {:.3f}'.format(tp\/(tp+fp)))\n    print('Recall    =     {:.3f}'.format(tp\/(tp+fn)))\n    print('F1_score  =     {:.3f}'.format(2*(((tp\/(tp+fp))*(tp\/(tp+fn)))\/\n                                                 ((tp\/(tp+fp))+(tp\/(tp+fn))))))","06fcc0b0":"def plot_precision_recall():\n    plt.step(recall, precision, color = 'b', alpha = 0.2,\n             where = 'post')\n    plt.fill_between(recall, precision, step ='post', alpha = 0.2,\n                 color = 'b')\n\n    plt.plot(recall, precision, linewidth=2)\n    plt.xlim([0.0,1])\n    plt.ylim([0.0,1.05])\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision Recall Curve')\n    plt.show();","803c5ab7":"def plot_roc():\n    plt.plot(fpr, tpr, label = 'ROC curve', linewidth = 2)\n    plt.plot([0,1],[0,1], 'k--', linewidth = 2)\n   # plt.xlim([0.0,0.001])\n   # plt.ylim([0.0,1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.show();","d077b0d9":"def cross_val_metrics(model) :\n    scores = ['accuracy', 'precision', 'recall']\n    for sc in scores:\n        scores = cross_val_score(model, X, y, cv = 5, scoring = sc)\n        print('[%s] : %0.5f (+\/- %0.5f)'%(sc, scores.mean(), scores.std()))","f463b5ff":"def timer(start_time=None):\n    if not start_time:\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n        \n        \nrf_cfl = RandomForestClassifier(n_jobs = -1)\n\n# Number of trees in random forest\nn_estimators = [100,200,300,400]\n# Number of features to consider at every split\nmax_features = ['auto', 'log2']\n# Maximum number of levels in tree\nmax_depth = [10,20,30,40,50]\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\nparams = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\nfolds = 5\nparam_comb = 10\n\nrandom_search = RandomizedSearchCV(rf_cfl,param_distributions=params, n_iter=param_comb, scoring='accuracy', n_jobs=-1, cv=5, verbose=3, random_state=42)\n\nstart_time = timer(None) \nrandom_search.fit(X, y)\nprint(random_search.best_params_)\ntimer(start_time)","369b0ce4":"random_clf = RandomForestClassifier(n_estimators = 100,min_samples_split=5,min_samples_leaf=1,max_features='auto',max_depth=50,bootstrap=True)\n\nrandom_clf.fit(X_train,y_train)\ny_pred = random_clf.predict(X_test)\ntrain_pred = random_clf.predict(X_train)\n\n# Confusion maxtrix & metrics\ncm = confusion_matrix(y_test, y_pred)\nclass_names = [0,1]","110861c0":"show_metrics()","e272d02c":"cross_val_metrics(random_clf)","dfe9c50d":"# ROC curve\nfpr, tpr, t = roc_curve(y_test, y_pred)\nplot_roc()","44c68706":"model = Sequential()\nmodel.add(Dense(64,input_dim = 20,activation='relu'))\nmodel.add(Dense(32,activation='relu',init = 'uniform'))\nmodel.add(Dense(16,activation='relu',init = 'uniform'))\nmodel.add(Dense(1,activation = 'sigmoid'))\nmodel.summary()","51e7aa65":"model.compile(loss = 'binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)\nhistory=model.fit(X_train,y_train ,epochs=100,batch_size=128, validation_data=(X_test,y_test))","a7597b82":"# evaluate the keras model\n_, accuracy_train = model.evaluate(X_train, y_train)\n_, accuracy = model.evaluate(X_test, y_test)\nprint('Accuracy: %.2f' % (accuracy*100))","aa873bed":"from matplotlib import pyplot\npyplot.plot(history.history['loss'], label='train')\npyplot.plot(history.history['val_loss'], label='test')\npyplot.legend()\npyplot.show()","8a5ded1d":"# Gender Recognition by Voice and Speech Analysis\n\n###### This database was created to identify a voice as male or female, based upon acoustic properties of the voice and speech. The dataset consists of 3,168 recorded voice samples, collected from male and female speakers. The voice samples are pre-processed by acoustic analysis in R using the seewave and tuneR packages, with an analyzed frequency range of 0hz-280hz (human vocal range).","d0680b44":"## The Dataset\n##### The following acoustic properties of each voice are measured and included within the CSV:\n\n - meanfreq: mean frequency (in kHz)\n - sd: standard deviation of frequency\n - median: median frequency (in kHz)\n - Q25: first quantile (in kHz)\n - Q75: third quantile (in kHz)\n - IQR: interquantile range (in kHz)\n - skew: skewness (see note in specprop description)\n - kurt: kurtosis (see note in specprop description)\n - sp.ent: spectral entropy\n - sfm: spectral flatness\n - mode: mode frequency\n - centroid: frequency centroid (see specprop)\n - peakf: peak frequency (frequency with highest energy)\n - meanfun: average of fundamental frequency measured across acoustic signal\n - minfun: minimum fundamental frequency measured across acoustic signal\n - maxfun: maximum fundamental frequency measured across acoustic signal\n - meandom: average of dominant frequency measured across acoustic signal\n - mindom: minimum of dominant frequency measured across acoustic signal\n - maxdom: maximum of dominant frequency measured across acoustic signal\n - dfrange: range of dominant frequency measured across acoustic signal\n - modindx: modulation index. Calculated as the accumulated absolute difference between adjacent measurements of fundamental frequencies divided by the frequency range\n - label: male or female","ac83c8f7":"##### To be Continued........\n##### If you liked the kernal Pls don't forget to upvote\n##### Cheers!!!","9fe98db6":"### Machine Learning Model Selection & Building"}}