{"cell_type":{"6d5cfb9d":"code","fde0b094":"code","2eca056a":"code","3cc1dfba":"code","961a047b":"code","a98514ba":"code","5b1e88cb":"code","0c306210":"code","19d8d50f":"code","65aeea72":"code","34c76c59":"code","47e334b8":"code","4e7631a8":"code","05177a51":"code","e9a2f240":"code","ccf07957":"code","acca65ef":"code","bf1745fa":"code","e443f943":"code","97443746":"code","ffae4298":"code","4900bb1a":"code","fe06ad56":"code","0cc5c0cc":"code","bde8ad9f":"code","376374a1":"code","032fa7c9":"code","482c2181":"code","1e66b576":"code","714949eb":"code","b2daa9b4":"code","5a62eda0":"code","020f9830":"markdown","8fdaad63":"markdown","772a49cf":"markdown","7bb19356":"markdown","7c2f7239":"markdown","eecc2cc3":"markdown","4ffb0ff3":"markdown","8eeb44c5":"markdown","48ed6b26":"markdown","f2a5fef5":"markdown","00a03342":"markdown","4b4253e1":"markdown","72c22503":"markdown","fbae6749":"markdown","da218bfe":"markdown","c27dbef5":"markdown","83e55889":"markdown"},"source":{"6d5cfb9d":"!pip uninstall -y lightgbm\n!apt-get install -y libboost-all-dev\n!git clone --recursive https:\/\/github.com\/Microsoft\/LightGBM","fde0b094":"%%bash\ncd LightGBM\nrm -r build\nmkdir build\ncd build\ncmake -DUSE_GPU=1 -DOpenCL_LIBRARY=\/usr\/local\/cuda\/lib64\/libOpenCL.so -DOpenCL_INCLUDE_DIR=\/usr\/local\/cuda\/include\/ ..\nmake -j$(nproc)","2eca056a":"!cd LightGBM\/python-package\/;python setup.py install --precompile","3cc1dfba":"!mkdir -p \/etc\/OpenCL\/vendors && echo \"libnvidia-opencl.so.1\" > \/etc\/OpenCL\/vendors\/nvidia.icd\n!rm -r LightGBM","961a047b":"# Asthetics\nimport warnings\nimport sklearn.exceptions\nfrom pandas.core.common import SettingWithCopyWarning\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=UserWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore', category=SettingWithCopyWarning)\nwarnings.filterwarnings('ignore', category=sklearn.exceptions.UndefinedMetricWarning)\n\n# General\nfrom tqdm.auto import tqdm\nimport pandas as pd\nimport numpy as np\nimport os\nimport random\nimport gc\nimport itertools\ngc.enable()\n\n# Visialisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set(style=\"whitegrid\")\n\n# Machine Learning\n## Utils\nfrom sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold, cross_validate\nfrom sklearn.model_selection import cross_val_score, train_test_split, KFold\nfrom sklearn import preprocessing\nimport category_encoders as ce\n## Feature Selection\nfrom sklearn.feature_selection import chi2, f_classif, f_regression\nfrom sklearn.feature_selection import mutual_info_classif, mutual_info_regression\nfrom sklearn.feature_selection import SelectKBest, SelectPercentile, VarianceThreshold\n## Classification Models\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression, LinearRegression, ElasticNet\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import StackingClassifier, VotingClassifier\nimport lightgbm as lgb\n\n# Metrics\nfrom sklearn.metrics import roc_auc_score\n\n# Deep Learning\nimport torch\n\n# Fixing Seed\nRANDOM_SEED = 42\n\ndef seed_everything(seed=RANDOM_SEED):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    \nseed_everything()\n\n# Device Optimization\nif torch.cuda.is_available():\n    GPU = True\nelse:\n    GPU = False\n    \nprint(f'GPU Available: {GPU}')","a98514ba":"data_dir = '..\/input\/tabular-playground-series-sep-2021'\n\ntrain_file_path = '..\/input\/tpssep2021folds\/train_10_fold_nulls.csv'\ntest_file_path = os.path.join(data_dir, 'test.csv')\nsample_sub_file_path = os.path.join(data_dir, 'sample_solution.csv')\n\nprint(f'Train file: {train_file_path}')\nprint(f'Test file: {test_file_path}')\nprint(f'Sample Sub file: {sample_sub_file_path}')","5b1e88cb":"train_df = pd.read_csv(train_file_path)\ntest_df = pd.read_csv(test_file_path)\nsub_df = pd.read_csv(sample_sub_file_path)","0c306210":"target = ['claim']\nnot_features = ['id', 'kfold', 'claim']\ncols = list(train_df.columns)\nfeatures = [feat for feat in cols if feat not in not_features]","19d8d50f":"# From https:\/\/www.kaggle.com\/hiro5299834\/tps-sep-2021-single-lgbm\ntrain_df['n_missing'] = train_df[features].isna().sum(axis=1)\ntest_df['n_missing'] = test_df[features].isna().sum(axis=1)\n\ntrain_df['std'] = train_df[features].std(axis=1)\ntest_df['std'] = test_df[features].std(axis=1)\n\nfeatures += ['n_missing', 'std']","65aeea72":"test_pred_all = None\nvalid_pred_all = {}\nall_roc = []\n\nfor i in tqdm(range(train_df['kfold'].nunique())):\n    train = train_df[train_df['kfold'] != i].copy()\n    valid = train_df[train_df['kfold'] == i].copy()\n    test = test_df.copy()\n    \n    valid_ids = valid.id.values.tolist()\n    \n    if GPU:\n        xgb_params = {\n            'n_estimators': 18382,\n            'learning_rate': 0.010019309106542829,\n            'reg_lambda': 0.06489247352086885,\n            'reg_alpha': 38.607681064365444,\n            'max_depth': 5,\n            'subsample': 0.7338887825604986,\n            'colsample_bytree': 0.28819599059198336,\n            'booster': 'gbtree',\n            'random_state': RANDOM_SEED,\n            'verbosity': 0,\n            'tree_method':'gpu_hist',\n            'gpu_id': 0,\n            'predictor': 'gpu_predictor'\n        }\n    else:\n        xgb_params = {\n            'n_estimators': 1000,\n            'learning_rate': 0.011159116340385172,\n            'reg_lambda': 0.07868674215849121,\n            'reg_alpha': 26.263704018098185,\n            'max_depth': 5,\n            'subsample': 0.7612480691247493,\n            'colsample_bytree': 0.2695349533886053,\n            'booster': 'gbtree',\n            'random_state': RANDOM_SEED,\n            'verbosity': 0,\n            'n_jobs': 4\n        }\n        \n    clf = XGBClassifier(**xgb_params)  \n    clf.fit(train[features].values, train[target].values,\n            eval_set = [(valid[features].values, valid[target].values)],\n            eval_metric = 'auc',\n            early_stopping_rounds = 300,\n            verbose=False)\n    \n    valid_pred = clf.predict_proba(valid[features].values)[:, 1]\n    valid_pred_all.update(dict(zip(valid_ids, valid_pred)))\n    test_pred = clf.predict_proba(test[features].values)[:, 1]\n    roc = roc_auc_score(valid[target].values, valid_pred)\n    all_roc.append(roc)\n\n    if test_pred_all is None:\n        test_pred_all = test_pred\n    else:\n        test_pred_all += test_pred\n    \n    print(f'Fold {i+1} ROC-AUC: {round(roc, 4)}')\n    \nprint('')\nprint(f'Average ROC-AUC: {round(np.mean(all_roc), 4)} Std: {round(np.std(all_roc), 4)}')\ntest_pred_all \/= train_df['kfold'].nunique()\n\nvalid_pred_all = pd.DataFrame.from_dict(valid_pred_all, orient='index').reset_index()\nvalid_pred_all.columns = ['id', 'pred_1']\nvalid_pred_all.to_csv('train_pred_1.csv', index=False)\n\nsub_2 = pd.DataFrame()\nsub_2['id'] = test_df['id']\nsub_2['pred_1'] = test_pred_all\nsub_2.to_csv('test_pred_1.csv', index=False)","34c76c59":"test_pred_all = None\nvalid_pred_all = {}\nall_roc = []\n\nfor i in tqdm(range(train_df['kfold'].nunique())):\n    train = train_df[train_df['kfold'] != i].copy()\n    valid = train_df[train_df['kfold'] == i].copy()\n    test = test_df.copy()\n    \n    valid_ids = valid.id.values.tolist()\n    \n    lgb_train = lgb.Dataset(train[features], train[target])\n    lgb_valid = lgb.Dataset(valid[features], valid[target], reference=lgb_train)\n    \n    if GPU:\n        lgbm_params = {\n            'n_estimators': 10000,\n            'learning_rate': 0.1,\n            'early_stopping_round': 300,\n            'boosting_type': 'gbdt',\n            'objective': 'binary',\n            'metric': 'auc',\n            'verbosity': -1,\n            'device': 'gpu',\n            'gpu_platform_id': 0,\n            'gpu_device_id': 0,\n            'random_state': RANDOM_SEED\n        }\n    else:\n        lgbm_params = {\n            'n_estimators': 1000,\n            'learning_rate': 0.1,\n            'early_stopping_round': 300,\n            'boosting_type': 'gbdt',\n            'objective': 'binary',\n            'metric': 'auc',\n            'verbosity': -1,\n            'n_jobs': 4,\n            'random_state': RANDOM_SEED\n        }\n    \n    clf = lgb.train(lgbm_params, lgb_train, valid_sets=[lgb_valid], verbose_eval=False)\n    \n    valid_pred = clf.predict(valid[features].values)\n    valid_pred_all.update(dict(zip(valid_ids, valid_pred)))\n    test_pred = clf.predict(test[features].values)\n    roc = roc_auc_score(valid[target].values, valid_pred)\n    all_roc.append(roc)\n\n    if test_pred_all is None:\n        test_pred_all = test_pred\n    else:\n        test_pred_all += test_pred\n    \n    print(f'Fold {i+1} ROC-AUC: {round(roc, 4)}')\n    \nprint('')\nprint(f'Average ROC-AUC: {round(np.mean(all_roc), 4)} Std: {round(np.std(all_roc), 4)}')\ntest_pred_all \/= train_df['kfold'].nunique()\n\nvalid_pred_all = pd.DataFrame.from_dict(valid_pred_all, orient='index').reset_index()\nvalid_pred_all.columns = ['id', 'pred_2']\nvalid_pred_all.to_csv('train_pred_2.csv', index=False)\n\nsub_2 = pd.DataFrame()\nsub_2['id'] = test_df['id']\nsub_2['pred_2'] = test_pred_all\nsub_2.to_csv('test_pred_2.csv', index=False)","47e334b8":"test_pred_all = None\nvalid_pred_all = {}\nall_roc = []\n\nfor i in tqdm(range(train_df['kfold'].nunique())):\n    train = train_df[train_df['kfold'] != i].copy()\n    valid = train_df[train_df['kfold'] == i].copy()\n    test = test_df.copy()\n    \n    valid_ids = valid.id.values.tolist()\n    \n    if GPU:\n        cat_params = {\n            'depth' : 5,\n            'grow_policy' : 'SymmetricTree',\n            'l2_leaf_reg' : 3.0,\n            'random_strength' : 1.0,\n            'learning_rate' : 0.1,\n            'iterations' : 10000,\n            'loss_function' : 'CrossEntropy',\n            'eval_metric' : 'AUC',\n            'use_best_model' : True,\n            'early_stopping_rounds' : 300,\n            'task_type' : 'GPU',\n            'verbose' : False\n        }\n    else:\n        cat_params = {\n            'depth' : 5,\n            'grow_policy' : 'SymmetricTree',\n            'l2_leaf_reg' : 3.0,\n            'random_strength' : 1.0,\n            'learning_rate' : 0.1,\n            'iterations' : 1000,\n            'loss_function' : 'CrossEntropy',\n            'eval_metric' : 'AUC',\n            'use_best_model' : True,\n            'early_stopping_rounds' : 100,\n            'task_type' : 'CPU',\n            'thread_count' : 4,\n            'verbose' : False\n        }\n        \n    clf = CatBoostClassifier(**cat_params)  \n    clf.fit(train[features].values, train[target].values,\n            eval_set = [(valid[features].values, valid[target].values)])\n    \n    valid_pred = clf.predict_proba(valid[features].values)[:, 1]\n    valid_pred_all.update(dict(zip(valid_ids, valid_pred)))\n    test_pred = clf.predict_proba(test[features].values)[:, 1]\n    roc = roc_auc_score(valid[target].values, valid_pred)\n    all_roc.append(roc)\n\n    if test_pred_all is None:\n        test_pred_all = test_pred\n    else:\n        test_pred_all += test_pred\n    \n    print(f'Fold {i+1} ROC-AUC: {round(roc, 4)}')\n    \nprint('')\nprint(f'Average ROC-AUC: {round(np.mean(all_roc), 4)} Std: {round(np.std(all_roc), 4)}')\ntest_pred_all \/= train_df['kfold'].nunique()\n\nvalid_pred_all = pd.DataFrame.from_dict(valid_pred_all, orient='index').reset_index()\nvalid_pred_all.columns = ['id', 'pred_3']\nvalid_pred_all.to_csv('train_pred_3.csv', index=False)\n\nsub_2 = pd.DataFrame()\nsub_2['id'] = test_df['id']\nsub_2['pred_3'] = test_pred_all\nsub_2.to_csv('test_pred_3.csv', index=False)","4e7631a8":"test_pred_all = None\nvalid_pred_all = {}\nall_roc = []\n\nfor i in tqdm(range(train_df['kfold'].nunique())):\n    train = train_df[train_df['kfold'] != i].copy()\n    valid = train_df[train_df['kfold'] == i].copy()\n    test = test_df.copy()\n    \n    train.fillna(train.mean(), inplace=True)\n    valid.fillna(valid.mean(), inplace=True)\n    test.fillna(test.mean(), inplace=True)\n    \n    scaler = preprocessing.StandardScaler()\n    train[features] = scaler.fit_transform(train[features])\n    valid[features] = scaler.transform(valid[features])\n    test[features] = scaler.transform(test[features])\n    \n    valid_ids = valid.id.values.tolist()\n    \n    lgb_train = lgb.Dataset(train[features], train[target])\n    lgb_valid = lgb.Dataset(valid[features], valid[target], reference=lgb_train)\n    \n    if GPU:\n        lgbm_params = {\n            'n_estimators': 20000,\n            'learning_rate': 5e-3,\n            'subsample': 0.6,\n            'subsample_freq': 1,\n            'colsample_bytree': 0.4,\n            'reg_alpha': 10.0,\n            'reg_lambda': 1e-1,\n            'min_child_weight': 256,\n            'min_child_samples': 20,\n            'early_stopping_round': 200,\n            'objective': 'binary',\n            'metric': 'auc',\n            'verbosity': -1,\n            'device': 'gpu',\n            'gpu_platform_id': 0,\n            'gpu_device_id': 0,\n            'random_state': 2021\n        }\n    else:\n        lgbm_params = {\n            'n_estimators': 20000,\n            'learning_rate': 5e-3,\n            'subsample': 0.6,\n            'subsample_freq': 1,\n            'colsample_bytree': 0.4,\n            'reg_alpha': 10.0,\n            'reg_lambda': 1e-1,\n            'min_child_weight': 256,\n            'min_child_samples': 20,\n            'early_stopping_round': 200,\n            'objective': 'binary',\n            'metric': 'auc',\n            'verbosity': -1,\n            'n_jobs': 4,\n            'random_state': 2021\n        }\n    \n    clf = lgb.train(lgbm_params, lgb_train, valid_sets=[lgb_valid], verbose_eval=False)\n    \n    valid_pred = clf.predict(valid[features].values)\n    valid_pred_all.update(dict(zip(valid_ids, valid_pred)))\n    test_pred = clf.predict(test[features].values)\n    roc = roc_auc_score(valid[target].values, valid_pred)\n    all_roc.append(roc)\n\n    if test_pred_all is None:\n        test_pred_all = test_pred\n    else:\n        test_pred_all += test_pred\n    \n    print(f'Fold {i+1} ROC-AUC: {round(roc, 4)}')\n    \nprint('')\nprint(f'Average ROC-AUC: {round(np.mean(all_roc), 4)} Std: {round(np.std(all_roc), 4)}')\ntest_pred_all \/= train_df['kfold'].nunique()\n\nvalid_pred_all = pd.DataFrame.from_dict(valid_pred_all, orient='index').reset_index()\nvalid_pred_all.columns = ['id', 'pred_4']\nvalid_pred_all.to_csv('train_pred_4.csv', index=False)\n\nsub_2 = pd.DataFrame()\nsub_2['id'] = test_df['id']\nsub_2['pred_4'] = test_pred_all\nsub_2.to_csv('test_pred_4.csv', index=False)","05177a51":"test_pred_all = None\nvalid_pred_all = {}\nall_roc = []\n\nfor i in tqdm(range(train_df['kfold'].nunique())):\n    train = train_df[train_df['kfold'] != i].copy()\n    valid = train_df[train_df['kfold'] == i].copy()\n    test = test_df.copy()\n    \n    valid_ids = valid.id.values.tolist()\n    \n    if GPU:\n        cat_params = {\n            'depth': 3,\n            'grow_policy': 'Lossguide',\n            'l2_leaf_reg': 5.0682077505381e-06,\n            'random_strength': 0.0001395042015032738,\n            'learning_rate': 0.022430394051758566,\n            'iterations': 14805,\n            'loss_function': 'Logloss',\n            'bagging_temperature': 0.6867277487647423,\n            'border_count': 210,\n            'eval_metric' : 'AUC',\n            'use_best_model' : True,\n            'early_stopping_rounds' : 300,\n            'task_type' : 'GPU',\n            'verbose' : False\n        }\n    else:\n        cat_params = {\n            'depth': 3,\n            'grow_policy': 'Lossguide',\n            'l2_leaf_reg': 5.0682077505381e-06,\n            'random_strength': 0.0001395042015032738,\n            'learning_rate': 0.022430394051758566,\n            'iterations': 14805,\n            'loss_function': 'Logloss',\n            'bagging_temperature': 0.6867277487647423,\n            'border_count': 210,\n            'eval_metric' : 'AUC',\n            'use_best_model' : True,\n            'early_stopping_rounds' : 300,\n            'task_type' : 'CPU',\n            'thread_count' : 4,\n            'verbose' : False\n        }\n        \n    clf = CatBoostClassifier(**cat_params)  \n    clf.fit(train[features].values, train[target].values,\n            eval_set = [(valid[features].values, valid[target].values)])\n    \n    valid_pred = clf.predict_proba(valid[features].values)[:, 1]\n    valid_pred_all.update(dict(zip(valid_ids, valid_pred)))\n    test_pred = clf.predict_proba(test[features].values)[:, 1]\n    roc = roc_auc_score(valid[target].values, valid_pred)\n    all_roc.append(roc)\n\n    if test_pred_all is None:\n        test_pred_all = test_pred\n    else:\n        test_pred_all += test_pred\n    \n    print(f'Fold {i+1} ROC-AUC: {round(roc, 4)}')\n    \nprint('')\nprint(f'Average ROC-AUC: {round(np.mean(all_roc), 4)} Std: {round(np.std(all_roc), 4)}')\ntest_pred_all \/= train_df['kfold'].nunique()\n\nvalid_pred_all = pd.DataFrame.from_dict(valid_pred_all, orient='index').reset_index()\nvalid_pred_all.columns = ['id', 'pred_5']\nvalid_pred_all.to_csv('train_pred_5.csv', index=False)\n\nsub_2 = pd.DataFrame()\nsub_2['id'] = test_df['id']\nsub_2['pred_5'] = test_pred_all\nsub_2.to_csv('test_pred_5.csv', index=False)","e9a2f240":"train_df = pd.read_csv(train_file_path)\ntest_df = pd.read_csv(test_file_path)\nsub_df = pd.read_csv(sample_sub_file_path)","ccf07957":"prev_features = list(train_df.columns)","acca65ef":"df1 = pd.read_csv('train_pred_1.csv')\ndf2 = pd.read_csv('train_pred_2.csv')\ndf3 = pd.read_csv('train_pred_3.csv')\ndf4 = pd.read_csv('train_pred_4.csv')\ndf5 = pd.read_csv('train_pred_5.csv')\n\ndf_test1 = pd.read_csv('test_pred_1.csv')\ndf_test2 = pd.read_csv('test_pred_2.csv')\ndf_test3 = pd.read_csv('test_pred_3.csv')\ndf_test4 = pd.read_csv('test_pred_4.csv')\ndf_test5 = pd.read_csv('test_pred_5.csv')","bf1745fa":"train_df = train_df.merge(df1, on='id', how='left')\ntrain_df = train_df.merge(df2, on='id', how='left')\ntrain_df = train_df.merge(df3, on='id', how='left')\ntrain_df = train_df.merge(df4, on='id', how='left')\ntrain_df = train_df.merge(df5, on='id', how='left')\n\ntest_df = test_df.merge(df_test1, on='id', how='left')\ntest_df = test_df.merge(df_test2, on='id', how='left')\ntest_df = test_df.merge(df_test3, on='id', how='left')\ntest_df = test_df.merge(df_test4, on='id', how='left')\ntest_df = test_df.merge(df_test5, on='id', how='left')","e443f943":"cols = list(train_df.columns)\nblend_features = [feat for feat in cols if str(feat).startswith('pred')]\nprint(blend_features)","97443746":"test_pred_all = None\nvalid_pred_all = {}\nall_roc = []\n\nfor i in tqdm(range(train_df['kfold'].nunique())):\n    train = train_df[train_df['kfold'] != i]\n    valid = train_df[train_df['kfold'] == i]\n    test = test_df.copy()\n\n    valid_ids = valid.id.values.tolist()\n\n    model = LogisticRegression(solver='liblinear')\n    model.fit(train[blend_features].values, train[target].values)\n\n    valid_pred = model.predict_proba(valid[blend_features].values)[:, 1]\n    valid_pred_all.update(dict(zip(valid_ids, valid_pred)))\n    test_pred = model.predict_proba(test[blend_features].values)[:, 1]\n    roc = roc_auc_score(valid[target].values, valid_pred)\n    all_roc.append(roc)\n\n    if test_pred_all is None:\n        test_pred_all = test_pred\n    else:\n        test_pred_all += test_pred\n    \n    print(f'Fold {i+1} ROC: {round(roc, 4)}')\n    \nprint('')\nprint(f'Average ROC: {round(np.mean(all_roc), 4)} Std: {round(np.std(all_roc), 4)}')\ntest_pred_all \/= train_df['kfold'].nunique()\n\nvalid_pred_all = pd.DataFrame.from_dict(valid_pred_all, orient='index').reset_index()\nvalid_pred_all.columns = ['id', 'pred_L1_1']\nvalid_pred_all.to_csv('L1_train_pred_1.csv', index=False)\n\nsub_2 = pd.DataFrame()\nsub_2['id'] = test_df['id']\nsub_2['pred_L1_1'] = test_pred_all\nsub_2.to_csv('L1_test_pred_1.csv', index=False)","ffae4298":"test_pred_all = None\nvalid_pred_all = {}\nall_roc = []\n\nfor i in tqdm(range(train_df['kfold'].nunique())):\n    train = train_df[train_df['kfold'] != i]\n    valid = train_df[train_df['kfold'] == i]\n    test = test_df.copy()\n\n    valid_ids = valid.id.values.tolist()\n\n    model = GaussianNB()\n    model.fit(train[blend_features].values, train[target].values)\n\n    valid_pred = model.predict_proba(valid[blend_features].values)[:, 1]\n    valid_pred_all.update(dict(zip(valid_ids, valid_pred)))\n    test_pred = model.predict_proba(test[blend_features].values)[:, 1]\n    roc = roc_auc_score(valid[target].values, valid_pred)\n    all_roc.append(roc)\n\n    if test_pred_all is None:\n        test_pred_all = test_pred\n    else:\n        test_pred_all += test_pred\n    \n    print(f'Fold {i+1} ROC: {round(roc, 4)}')\n    \nprint('')\nprint(f'Average ROC: {round(np.mean(all_roc), 4)} Std: {round(np.std(all_roc), 4)}')\ntest_pred_all \/= train_df['kfold'].nunique()\n\nvalid_pred_all = pd.DataFrame.from_dict(valid_pred_all, orient='index').reset_index()\nvalid_pred_all.columns = ['id', 'pred_L1_2']\nvalid_pred_all.to_csv('L1_train_pred_2.csv', index=False)\n\nsub_2 = pd.DataFrame()\nsub_2['id'] = test_df['id']\nsub_2['pred_L1_2'] = test_pred_all\nsub_2.to_csv('L1_test_pred_2.csv', index=False)","4900bb1a":"test_pred_all = None\nvalid_pred_all = {}\nall_roc = []\n\nfor i in tqdm(range(train_df['kfold'].nunique())):\n    train = train_df[train_df['kfold'] != i]\n    valid = train_df[train_df['kfold'] == i]\n    test = test_df.copy()\n\n    valid_ids = valid.id.values.tolist()\n\n    model = LinearRegression()\n    model.fit(train[blend_features].values, train[target].values)\n\n    valid_pred = model.predict(valid[blend_features].values)\n    valid_pred_all.update(dict(zip(valid_ids, valid_pred)))\n    test_pred = model.predict(test[blend_features].values)\n    roc = roc_auc_score(valid[target].values, valid_pred)\n    all_roc.append(roc)\n\n    if test_pred_all is None:\n        test_pred_all = test_pred\n    else:\n        test_pred_all += test_pred\n    \n    print(f'Fold {i+1} ROC: {round(roc, 4)}')\n    \nprint('')\nprint(f'Average ROC: {round(np.mean(all_roc), 4)} Std: {round(np.std(all_roc), 4)}')\ntest_pred_all \/= train_df['kfold'].nunique()\n\nvalid_pred_all = pd.DataFrame.from_dict(valid_pred_all, orient='index').reset_index()\nvalid_pred_all.columns = ['id', 'pred_L1_3']\nvalid_pred_all.to_csv('L1_train_pred_3.csv', index=False)\n\nsub_2 = pd.DataFrame()\nsub_2['id'] = test_df['id']\nsub_2['pred_L1_3'] = test_pred_all\nsub_2.to_csv('L1_test_pred_3.csv', index=False)","fe06ad56":"test_pred_all = None\nvalid_pred_all = {}\nall_roc = []\n\nfor i in tqdm(range(train_df['kfold'].nunique())):\n    train = train_df[train_df['kfold'] != i]\n    valid = train_df[train_df['kfold'] == i]\n    test = test_df.copy()\n\n    valid_ids = valid.id.values.tolist()\n\n    model = QuadraticDiscriminantAnalysis()\n    model.fit(train[blend_features].values, train[target].values)\n\n    valid_pred = model.predict_proba(valid[blend_features].values)[:, 1]\n    valid_pred_all.update(dict(zip(valid_ids, valid_pred)))\n    test_pred = model.predict_proba(test[blend_features].values)[:, 1]\n    roc = roc_auc_score(valid[target].values, valid_pred)\n    all_roc.append(roc)\n\n    if test_pred_all is None:\n        test_pred_all = test_pred\n    else:\n        test_pred_all += test_pred\n    \n    print(f'Fold {i+1} ROC: {round(roc, 4)}')\n    \nprint('')\nprint(f'Average ROC: {round(np.mean(all_roc), 4)} Std: {round(np.std(all_roc), 4)}')\ntest_pred_all \/= train_df['kfold'].nunique()\n\nvalid_pred_all = pd.DataFrame.from_dict(valid_pred_all, orient='index').reset_index()\nvalid_pred_all.columns = ['id', 'pred_L1_4']\nvalid_pred_all.to_csv('L1_train_pred_4.csv', index=False)\n\nsub_2 = pd.DataFrame()\nsub_2['id'] = test_df['id']\nsub_2['pred_L1_4'] = test_pred_all\nsub_2.to_csv('L1_test_pred_4.csv', index=False)","0cc5c0cc":"test_pred_all = None\nvalid_pred_all = {}\nall_roc = []\n\nfor i in tqdm(range(train_df['kfold'].nunique())):\n    train = train_df[train_df['kfold'] != i]\n    valid = train_df[train_df['kfold'] == i]\n    test = test_df.copy()\n\n    valid_ids = valid.id.values.tolist()\n    \n    if GPU:\n        xgb_params = {\n            'n_estimators': 10000,\n            'booster': 'gbtree',\n            'random_state': RANDOM_SEED,\n            'verbosity': 0,\n            'tree_method':'gpu_hist',\n            'gpu_id': 0,\n            'predictor': 'gpu_predictor'\n        }\n    else:\n        xgb_params = {\n            'n_estimators': 1000,\n            'booster': 'gbtree',\n            'random_state': RANDOM_SEED,\n            'verbosity': 0,\n            'n_jobs': 4\n        }\n        \n    model = XGBClassifier(**xgb_params)  \n    model.fit(train[blend_features].values, train[target].values,\n            eval_set = [(valid[blend_features].values, valid[target].values)],\n            eval_metric = 'auc',\n            early_stopping_rounds = 300,\n            verbose=False)\n\n    valid_pred = model.predict_proba(valid[blend_features].values)[:, 1]\n    valid_pred_all.update(dict(zip(valid_ids, valid_pred)))\n    test_pred = model.predict_proba(test[blend_features].values)[:, 1]\n    roc = roc_auc_score(valid[target].values, valid_pred)\n    all_roc.append(roc)\n\n    if test_pred_all is None:\n        test_pred_all = test_pred\n    else:\n        test_pred_all += test_pred\n    \n    print(f'Fold {i+1} ROC: {round(roc, 4)}')\n    \nprint('')\nprint(f'Average ROC: {round(np.mean(all_roc), 4)} Std: {round(np.std(all_roc), 4)}')\ntest_pred_all \/= train_df['kfold'].nunique()\n\nvalid_pred_all = pd.DataFrame.from_dict(valid_pred_all, orient='index').reset_index()\nvalid_pred_all.columns = ['id', 'pred_L1_5']\nvalid_pred_all.to_csv('L1_train_pred_5.csv', index=False)\n\nsub_2 = pd.DataFrame()\nsub_2['id'] = test_df['id']\nsub_2['pred_L1_5'] = test_pred_all\nsub_2.to_csv('L1_test_pred_5.csv', index=False)","bde8ad9f":"test_pred_all = None\nvalid_pred_all = {}\nall_roc = []\n\nfor i in tqdm(range(train_df['kfold'].nunique())):\n    train = train_df[train_df['kfold'] != i]\n    valid = train_df[train_df['kfold'] == i]\n    test = test_df.copy()\n\n    valid_ids = valid.id.values.tolist()\n        \n    lgbm_params = {\n        'learning_rate': 0.1,\n        'n_estimators': 10000,\n        'max_depth': 2,\n        'boosting_type': 'gbdt',\n        'objective': 'binary',\n        'metric': 'auc',\n        'verbose': -1,\n        'n_jobs': 4,\n        'random_state': RANDOM_SEED\n    }\n        \n    model = LGBMClassifier(**lgbm_params)  \n    model.fit(train[blend_features].values, train[target].values,\n            eval_set = [(valid[blend_features].values, valid[target].values)],\n            early_stopping_rounds = 300,\n            verbose=False)\n\n    valid_pred = model.predict_proba(valid[blend_features].values)[:, 1]\n    valid_pred_all.update(dict(zip(valid_ids, valid_pred)))\n    test_pred = model.predict_proba(test[blend_features].values)[:, 1]\n    roc = roc_auc_score(valid[target].values, valid_pred)\n    all_roc.append(roc)\n\n    if test_pred_all is None:\n        test_pred_all = test_pred\n    else:\n        test_pred_all += test_pred\n    \n    print(f'Fold {i+1} ROC: {round(roc, 4)}')\n    \nprint('')\nprint(f'Average ROC: {round(np.mean(all_roc), 4)} Std: {round(np.std(all_roc), 4)}')\ntest_pred_all \/= train_df['kfold'].nunique()\n\nvalid_pred_all = pd.DataFrame.from_dict(valid_pred_all, orient='index').reset_index()\nvalid_pred_all.columns = ['id', 'pred_L1_6']\nvalid_pred_all.to_csv('L1_train_pred_6.csv', index=False)\n\nsub_2 = pd.DataFrame()\nsub_2['id'] = test_df['id']\nsub_2['pred_L1_6'] = test_pred_all\nsub_2.to_csv('L1_test_pred_6.csv', index=False)","376374a1":"test_pred_all = None\nvalid_pred_all = {}\nall_roc = []\n\nfor i in tqdm(range(train_df['kfold'].nunique())):\n    train = train_df[train_df['kfold'] != i]\n    valid = train_df[train_df['kfold'] == i]\n    test = test_df.copy()\n\n    valid_ids = valid.id.values.tolist()\n        \n    if GPU:\n        cat_params = {\n            'depth' : 3,\n            'grow_policy' : 'SymmetricTree',\n            'learning_rate' : 0.1,\n            'iterations' : 10000,\n            'loss_function' : 'CrossEntropy',\n            'eval_metric' : 'AUC',\n            'use_best_model' : True,\n            'early_stopping_rounds' : 300,\n            'task_type' : 'GPU',\n            'verbose' : False\n        }\n    else:\n        cat_params = {\n            'depth' : 3,\n            'grow_policy' : 'SymmetricTree',\n            'learning_rate' : 0.1,\n            'iterations' : 1000,\n            'loss_function' : 'CrossEntropy',\n            'eval_metric' : 'AUC',\n            'use_best_model' : True,\n            'early_stopping_rounds' : 100,\n            'task_type' : 'CPU',\n            'thread_count' : 4,\n            'verbose' : False\n        }\n        \n    model = CatBoostClassifier(**cat_params)  \n    model.fit(train[blend_features].values, train[target].values,\n            eval_set = [(valid[blend_features].values, valid[target].values)])\n\n    valid_pred = model.predict_proba(valid[blend_features].values)[:, 1]\n    valid_pred_all.update(dict(zip(valid_ids, valid_pred)))\n    test_pred = model.predict_proba(test[blend_features].values)[:, 1]\n    roc = roc_auc_score(valid[target].values, valid_pred)\n    all_roc.append(roc)\n\n    if test_pred_all is None:\n        test_pred_all = test_pred\n    else:\n        test_pred_all += test_pred\n    \n    print(f'Fold {i+1} ROC: {round(roc, 4)}')\n    \nprint('')\nprint(f'Average ROC: {round(np.mean(all_roc), 4)} Std: {round(np.std(all_roc), 4)}')\ntest_pred_all \/= train_df['kfold'].nunique()\n\nvalid_pred_all = pd.DataFrame.from_dict(valid_pred_all, orient='index').reset_index()\nvalid_pred_all.columns = ['id', 'pred_L1_7']\nvalid_pred_all.to_csv('L1_train_pred_7.csv', index=False)\n\nsub_2 = pd.DataFrame()\nsub_2['id'] = test_df['id']\nsub_2['pred_L1_7'] = test_pred_all\nsub_2.to_csv('L1_test_pred_7.csv', index=False)","032fa7c9":"train_df = pd.read_csv(train_file_path)\ntest_df = pd.read_csv(test_file_path)\nsub_df = pd.read_csv(sample_sub_file_path)","482c2181":"prev_features = list(train_df.columns)","1e66b576":"df1 = pd.read_csv('L1_train_pred_1.csv')\ndf2 = pd.read_csv('L1_train_pred_2.csv')\ndf3 = pd.read_csv('L1_train_pred_3.csv')\ndf4 = pd.read_csv('L1_train_pred_4.csv')\ndf5 = pd.read_csv('L1_train_pred_5.csv')\ndf6 = pd.read_csv('L1_train_pred_6.csv')\ndf7 = pd.read_csv('L1_train_pred_7.csv')\n\ndf_test1 = pd.read_csv('L1_test_pred_1.csv')\ndf_test2 = pd.read_csv('L1_test_pred_2.csv')\ndf_test3 = pd.read_csv('L1_test_pred_3.csv')\ndf_test4 = pd.read_csv('L1_test_pred_4.csv')\ndf_test5 = pd.read_csv('L1_test_pred_5.csv')\ndf_test6 = pd.read_csv('L1_test_pred_6.csv')\ndf_test7 = pd.read_csv('L1_test_pred_7.csv')","714949eb":"train_df = train_df.merge(df1, on='id', how='left')\ntrain_df = train_df.merge(df2, on='id', how='left')\ntrain_df = train_df.merge(df3, on='id', how='left')\ntrain_df = train_df.merge(df4, on='id', how='left')\ntrain_df = train_df.merge(df5, on='id', how='left')\ntrain_df = train_df.merge(df6, on='id', how='left')\ntrain_df = train_df.merge(df7, on='id', how='left')\n\ntest_df = test_df.merge(df_test1, on='id', how='left')\ntest_df = test_df.merge(df_test2, on='id', how='left')\ntest_df = test_df.merge(df_test3, on='id', how='left')\ntest_df = test_df.merge(df_test4, on='id', how='left')\ntest_df = test_df.merge(df_test5, on='id', how='left')\ntest_df = test_df.merge(df_test6, on='id', how='left')\ntest_df = test_df.merge(df_test7, on='id', how='left')","b2daa9b4":"cols = list(train_df.columns)\nstack_features = [feat for feat in cols if str(feat).startswith('pred')]\nprint(stack_features)","5a62eda0":"test_pred_all = None\nall_roc = []\n\nfor i in tqdm(range(train_df['kfold'].nunique())):\n    train = train_df[train_df['kfold'] != i]\n    valid = train_df[train_df['kfold'] == i]\n    test = test_df.copy()\n\n    model = LogisticRegression(solver='liblinear')\n    model.fit(train[stack_features].values, train[target].values)\n\n    valid_pred = model.predict_proba(valid[stack_features].values)[:, 1]\n    test_pred = model.predict_proba(test[stack_features].values)[:, 1]\n    roc = roc_auc_score(valid[target].values, valid_pred)\n    all_roc.append(roc)\n\n    if test_pred_all is None:\n        test_pred_all = test_pred\n    else:\n        test_pred_all += test_pred\n    \n    print(f'Fold {i+1} ROC: {round(roc, 4)}')\n    \nprint('')\nprint(f'Average ROC: {round(np.mean(all_roc), 4)} Std: {round(np.std(all_roc), 4)}')\ntest_pred_all \/= train_df['kfold'].nunique()\n\nsub_2 = pd.DataFrame()\nsub_2['id'] = test_df['id']\nsub_2['claim'] = test_pred_all\nsub_2.to_csv('Stacked_Submission_1.csv', index=False)","020f9830":"# Stacking (L2 Model)","8fdaad63":"# L0 Models","772a49cf":"## 2. LGBM - 1","7bb19356":"# Blending (L1 Models)","7c2f7239":"## 2. Naive Bayes","eecc2cc3":"## 1. Logistic Regression","4ffb0ff3":"## 5. Catboost - 2","8eeb44c5":"# Imports","48ed6b26":"## 3. Catboost - 1","f2a5fef5":"## 4. LGBM - 2","00a03342":"## 5. XGBoost","4b4253e1":"## 1. XGBoost - 1","72c22503":"## 6. LGBM","fbae6749":"## 3. Linear Regression","da218bfe":"## 7. Cat Boost","c27dbef5":"## 4. QDA","83e55889":"# Read Data"}}