{"cell_type":{"3129a62a":"code","2408def5":"code","746b6058":"code","a3dadbfc":"code","a224c097":"code","5e9cdf6f":"code","80439b7b":"code","da415d4c":"code","1526f923":"code","01d00d18":"code","1e0e7cc4":"markdown"},"source":{"3129a62a":"#imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom keras.models import Model, Sequential\nfrom keras.layers import Dense, Embedding, LSTM, Dropout\nfrom keras.preprocessing.text import Tokenizer\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.preprocessing import sequence\nfrom sklearn.metrics import confusion_matrix\nimport itertools","2408def5":"#Reading data\ndf1 = pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/True.csv')\ndf1['label'] = 1\ndf2 = pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/Fake.csv')\ndf2['label'] = 0\ndf = pd.concat([df1,df2])","746b6058":"df.head()","a3dadbfc":"plt.hist(df['text'].apply(lambda x: len(x.split())))","a224c097":"#Splitting the dataset\nX = df.text\ny = df.label\n#converting label format so that model understand it\ny = to_categorical(y, num_classes = 2)\nX_train,X_test,y_train,y_test = train_test_split(X, y, test_size = 0.1)","5e9cdf6f":"\"\"\"\nThese steps are very much important since these steps make your language data into the type of data on which RNN works.\nfor eg. [\"I am a boy\",\"I like kaggle\",....] --(tokenization)-->[[1 2 3 4],[1 5 6],....]\n\n\"\"\"\nvocab = 10000  #maximum no. of words in our dictionary\nmax_len = 600  #maximum length of sentence (if more than that sentence will be trucated otherwise \"0\" will be added to complete max len)\ntokens = Tokenizer(num_words = vocab)\ntokens.fit_on_texts(X_train)\nsequences = tokens.texts_to_sequences(X_train)\nsequence_matrix = sequence.pad_sequences(sequences, maxlen = max_len)\n#for test data\ntest_sequences = tokens.texts_to_sequences(X_test)\ntest_sequence_matrix = sequence.pad_sequences(test_sequences, maxlen = max_len)","80439b7b":"\"\"\"\nThis step seems to be very easy but what is going behind this ie. how LSTM works, I have tried demonstrate an idea but you are\nfree to do comments if find any thing unrelatable:)\n \n \n                                      [prob. of real, prob. of fake]\n                                                   |\n                                          _____________________\n                                         |                     |\n                                         |_____________________|\n                                          |        |        |\n                                         ___      ___      ___\n                                        |   |    |   |    |   |\n                                    ----|   |----|   |----|   |--\n                                        |___|    |___|    |___|\n                                          |        |        |\n                                         ___      ___      ___\n                                        |   |    |   |    |   |\n                                    ----|   |----|   |----|   |--\n                                        |___|    |___|    |___|\n                                          |        |        |\n                                          I       like     Kaggle\n\n\"\"\"\nmodel = Sequential()\nmodel.add(Embedding(vocab,300,input_length = max_len))\nmodel.add(LSTM(64))\nmodel.add(Dense(1000, activation = 'relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(2,activation = 'sigmoid'))\nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","da415d4c":"#Fit the model along with a validation split\nmodel.fit(sequence_matrix,y_train,validation_split = 0.2,epochs= 5)","1526f923":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.1f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n    plt.show()","01d00d18":"%matplotlib inline\nlabels = [\"Real\", \"Fake\"]\n\n# plotting confusion matrix\nY_pred = model.predict(np.array(test_sequence_matrix))\nY_pred_classes = np.argmax(Y_pred,axis = 1) \nY_true = np.argmax(np.array(y_test),axis = 1) \n\n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = labels,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues)\n","1e0e7cc4":"# Computer Vision V\/S  NLP\n\n### Note:- You have to accept NLP first in your subconcious mind if you have some experience in computer vision.\n\n- I have written above sentence because it happened to me as I was having some experience in computer vision and I don't know how I was facing problems while studying NLP than one of my friend had given me this advice that -\" hey, yash you are not accepting nlp try to accept it\". after this sentence I don't know how I had worked on NLP for 2 months which was not possible for me before, I'm writing this so that it may help someone like me:)\n\n- Now I think many of the good kernels are already present which will teach you NLP much better than me, the only thing I want to convey through this notebook is how a beginner will approach a NLP problem.\n\n- there are mainly two steps in nlp - Tokenization(converting words to tokens)+Model(RNNs\/LSTMs?Transformers)\n\n- The code is self explanatory and only thing I want to convey just read any others well descripted kernel than have some initial approach in your mind and then see this kernel and repeat this definitely you will develop confidence in nlp since this is one of the most basic dataset in NLP.\n\n- After repeating it please upload your notebook as that will encourage to write a documented code and improve your confidence in sharing things in the community:)\n\n- to hindi speakers - \n  \"NLP karo aur apna knowledge uplift karo,\n  ab begginer mahashay grand master banne ki taiyaari karo\".\n  \n- Please if you find any thing useful in it Don't forgot to upvote:)\n   "}}