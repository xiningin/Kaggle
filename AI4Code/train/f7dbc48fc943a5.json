{"cell_type":{"c6f050b8":"code","f206dfee":"code","49d144e1":"code","39917b92":"code","62d0ff0f":"code","3fcf1158":"code","e98f72d1":"code","8276cf04":"code","c6368cf9":"code","1070fb82":"code","1cef3e46":"code","9e6acf14":"code","2a45dcd9":"code","7b4817b9":"code","f756233e":"code","eeac60d4":"code","194f3c06":"code","2d833373":"code","d4617d34":"code","d07c2b88":"code","af49040e":"code","a62c5cc2":"code","a9170feb":"code","12dee639":"code","ebbcd0d4":"code","4a6230c5":"code","83da3742":"code","061b25c3":"code","a4b8b652":"code","3b047fe5":"code","eeca7f5c":"code","8a9e2354":"code","bcd5b25c":"code","7e56bd39":"code","f04864dd":"code","6d0284aa":"code","5d742499":"code","f8e53e19":"code","624c94ed":"code","3042afe9":"code","0479c457":"code","c295c4c4":"code","0593f40d":"code","6e96c3f4":"code","d49925b1":"code","dd1e6d53":"code","1e52b80c":"code","f4dc1eee":"code","9dffdfbd":"code","76dfc9dc":"code","0a1b7b7f":"code","41b3acdc":"code","bd9b1880":"code","9a6ab565":"code","8042476b":"code","08331075":"code","a8210540":"code","29874a69":"code","8740149e":"code","0e54d6cd":"code","01452572":"code","e488aab9":"code","dc43f803":"code","e18e482e":"code","4101aa03":"code","b3a5f0a3":"code","1d268ac0":"code","4629724f":"code","92b164e7":"code","e48a3263":"code","e757aae5":"code","3e608d63":"code","8393cfe8":"code","f549cc91":"code","2eaaebea":"code","6226dd8e":"code","4819136a":"code","cc15d7e8":"code","e4bc643f":"code","d13041c3":"code","ba79ada6":"code","1659bd87":"code","179c25bf":"code","a9f3d65d":"code","f2413f44":"code","fea809bb":"code","ad7e05dd":"code","673391a7":"code","84e010a7":"code","a25d11e1":"code","d76509ec":"code","5a75a1c5":"code","33befcfd":"code","bc6442ae":"code","db6f94d6":"code","28768f4b":"code","ac4773c2":"markdown","afeb4460":"markdown","69193ec5":"markdown","f8b2595d":"markdown","53a8a031":"markdown","5ca7e35a":"markdown","eb840a7e":"markdown","d709cac8":"markdown","cc57e7c9":"markdown","90ace5db":"markdown","bdf371ed":"markdown","1c4af8b5":"markdown","8b91cf7f":"markdown","fe35bbfd":"markdown","f9427f65":"markdown","536e8a12":"markdown","0ed52f11":"markdown","cbf702e3":"markdown","1042f336":"markdown","6511a2bc":"markdown","e4f682d8":"markdown","261060b6":"markdown","d60978d5":"markdown","a4a0f526":"markdown","1eae7576":"markdown","d4fc710e":"markdown","055eabc7":"markdown","4633df50":"markdown","845e7061":"markdown","c2624ed1":"markdown","f6249a77":"markdown","0833fc9d":"markdown","98d85a06":"markdown","9d519c49":"markdown","448c3f02":"markdown","d6e6d063":"markdown","29920458":"markdown","9893dfea":"markdown","ecfc7d2f":"markdown","529bed23":"markdown","181e8e9b":"markdown","3b0a7b71":"markdown","e357237a":"markdown","e2ceb062":"markdown","4f401c7a":"markdown","b194c6bd":"markdown","4ef8e7da":"markdown","03177a78":"markdown","0fc28cc5":"markdown","707fe5ce":"markdown","ee41524f":"markdown","6a70b068":"markdown","3e0f34a7":"markdown","bc272d5e":"markdown","7a9a89c6":"markdown","8cc75484":"markdown","9e1a8490":"markdown","c203ac4d":"markdown","c10fab46":"markdown","0ea7b094":"markdown","684dd642":"markdown","57575a99":"markdown","790f7d6d":"markdown"},"source":{"c6f050b8":"# Importing modules.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime as dt\nimport os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfrom pandas import Series\nfrom sklearn import metrics \nfrom sklearn.feature_selection import f_classif, mutual_info_classif\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor \nfrom sklearn.metrics import confusion_matrix, auc, roc_auc_score, roc_curve, accuracy_score, precision_score, recall_score, f1_score \n\n%matplotlib inline","f206dfee":"#Setting the conditions for experiments.\nrandom_seed = 42\ncurrent_date = pd.to_datetime('21\/10\/2020')\npd.set_option('display.max_columns', None)\ndata_directory = '\/kaggle\/input\/sf-dst-scoring\/'\n!pip freeze > requirements.txt","49d144e1":"# Defining a function for detecting outliers.\ndef outlier_detect(data, column):\n    Q1 = np.percentile(column, 25)\n    Q3 = np.percentile(column, 75)\n    IQR = Q3 - Q1\n    lower_range = Q1 - (1.5 * IQR)\n    upper_range = Q3 + (1.5 * IQR)\n    lower_number = len(data[column<lower_range])\n    upper_number = len(data[column>upper_range])\n    print('Lower Range:', lower_range,\n          'Upper Range:', upper_range,\n          'Lower Outliers:', lower_number,\n          'Upper Outliers:', upper_number, \n          sep='\\n')","39917b92":"# Defining a function for visualization of confusion matrix.\ndef show_confusion_matrix(y_true, y_pred):\n    color_text = plt.get_cmap('PuBu')(0.95)\n    class_names = ['Default', 'Non-Default']\n    cm = confusion_matrix(y_true, y_pred)\n    cm[0,0], cm[1,1] = cm[1,1], cm[0,0]\n    df = pd.DataFrame(cm, index=class_names, columns=class_names)\n    \n    fig, ax = plt.subplots(figsize=(6, 6))\n    ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]), title=\"Confusion Matrix\")\n    ax.title.set_fontsize(15)\n    sns.heatmap(df, square=True, annot=True, fmt=\"d\", linewidths=1, cmap=\"PuBu\")\n    plt.setp(ax.get_yticklabels(), rotation=0, ha=\"right\", rotation_mode=\"anchor\", fontsize=12)\n    plt.setp(ax.get_xticklabels(), rotation=0, ha=\"center\", rotation_mode=\"anchor\", fontsize=12)\n    ax.set_ylabel('Predicted Values', fontsize=14, color = color_text)\n    ax.set_xlabel('Real Values', fontsize=14, color = color_text)\n    b, t = plt.ylim()\n    plt.ylim(b+0.5, t-0.5)\n    fig.tight_layout()\n    plt.show()","62d0ff0f":"# Defining a function for visualization of metrics for logistic regression.\ndef all_metrics(y_true, y_pred, y_pred_prob):\n    dict_metric = {}\n    P = np.sum(y_true==1)\n    N = np.sum(y_true==0)\n    TP = np.sum((y_true==1)&(y_pred==1))\n    TN = np.sum((y_true==0)&(y_pred==0))\n    FP = np.sum((y_true==0)&(y_pred==1))\n    FN = np.sum((y_true==1)&(y_pred==0))\n    \n    dict_metric['Positive, P'] = [P,'default']\n    dict_metric['Negative, N'] = [N,'non-default']\n    dict_metric['True Positive, TP'] = [TP,'correctly identified default']\n    dict_metric['True Negative, TN'] = [TN,'correctly identified non-default']\n    dict_metric['False Positive, FP'] = [FP,'incorrectly identified default']\n    dict_metric['False Negative, FN'] = [FN,'incorrectly identified non-default']\n    dict_metric['Accuracy'] = [accuracy_score(y_true, y_pred),'Accuracy=(TP+TN)\/(P+N)']\n    dict_metric['Precision'] = [precision_score(y_true, y_pred),'Precision = TP\/(TP+FP)'] \n    dict_metric['Recall'] = [recall_score(y_true, y_pred),'Recall = TP\/P']\n    dict_metric['F1-score'] = [f1_score(y_true, y_pred),'Harmonical mean of Precision \u0438 Recall']\n    dict_metric['ROC_AUC'] = [roc_auc_score(y_true, y_pred_prob),'ROC AUC Score']    \n\n    temp_df = pd.DataFrame.from_dict(dict_metric, orient='index', columns=['Value', 'Description'])\n    display(temp_df)   ","3fcf1158":"# Importing datasets.\ndata_train = pd.read_csv(data_directory+'train.csv')\ndata_test = pd.read_csv(data_directory+'test.csv')\nsample_submission = pd.read_csv(data_directory+'\/sample_submission.csv')","e98f72d1":"# Checking the data.\ndata_train.info()\ndata_train.head()","8276cf04":"# Checking the data.\ndata_test.info()\ndata_test.head()","c6368cf9":"# Merging the datasets.\ndata_train['sample'] = 1\ndata_test['sample'] = 0\ndata = data_train.append(data_test, sort=False).reset_index(drop=True)","1070fb82":"# Checking the data.\ndata.info()\ndata.head()","1cef3e46":"# Checking for missing values.\ndata.isna().sum()","9e6acf14":"# Checking the number of unique values.\ndata.nunique()","2a45dcd9":"# Grouping column names by data type.\ntime_cols = ['app_date']\ncat_cols = ['education', 'region_rating', 'home_address', 'work_address', 'sna', 'first_time']\nbin_cols = ['sex', 'car', 'car_type', 'good_work', 'foreign_passport']\nnum_cols = ['age','decline_app_cnt','score_bki','bki_request_cnt','income']","7b4817b9":"# Checking the missing values.\ndata['education'].value_counts(dropna = False)","f756233e":"# Creating a new feature.\ndata['education_nan'] = pd.isna(data['education']).astype('uint8')","eeac60d4":"# Filling the missing values with the most frequent value ('SCH').\ndata['education'] = data['education'].fillna('SCH')","194f3c06":"# Encoding binary variables.\nlabel_encoder = LabelEncoder()\nfor column in bin_cols:\n    data[column] = label_encoder.fit_transform(data[column])","2d833373":"# Checking the data.\ndata.head()","d4617d34":"# Converting the data to datetime.\ndata['app_date'] = pd.to_datetime(data['app_date'], format='%d%b%Y')\ndata.head()","d07c2b88":"# Finding the minimum.\ndata_min = min(data['app_date'])\ndata_min","af49040e":"# Creating a new feature.\ndata['app_date_timedelta'] = (data['app_date'] - data_min).dt.days.astype('int')\ndata.head()","a62c5cc2":"# Adding a feature to the list.\nnum_cols.append('app_date_timedelta')","a9170feb":"# Checking the frequency distribution.\ndata['app_date_timedelta'].hist(bins=50)","12dee639":"# Checking the frequency distribution.\ndata.boxplot(column=['app_date_timedelta'])","ebbcd0d4":"# Detection of outliers.\noutlier_detect(data,data['app_date_timedelta'])","4a6230c5":"# Checking the frequency distribution.\ndata['education'].hist()","83da3742":"# Encoding a categorical variable.\neducation_dict = {\n    'SCH': 1,\n    'GRD': 2,\n    'UGR': 3,\n    'PGR': 4,\n    'ACD': 5,\n}\n\ndata['education'] = data['education'].map(education_dict)","061b25c3":"# Checking the frequency distribution.\ndata['sex'].hist(bins=2)","a4b8b652":"# Checking the frequency distribution.\ndata['age'].hist()","3b047fe5":"# Checking the frequency distribution.\ndata.boxplot(column=['age'])","eeca7f5c":"# Detection of outliers.\noutlier_detect(data,data['age'])","8a9e2354":"# Taking the logarithm.\nnp.log(data['age'] + 1).hist()","bcd5b25c":"# Taking the logarithm.\ndata['age'] = np.log(data['age'] + 1)\ndata.head()","7e56bd39":"# Checking the frequency distribution.\ndata.boxplot(column=['age'])","f04864dd":"# Detection of outliers.\noutlier_detect(data,data['age'])","6d0284aa":"# Checking the frequency distribution.\ndata['car'].hist(bins=2)","5d742499":"# Checking the frequency distribution.\ndata['car_type'].hist(bins=2)","f8e53e19":"# Checking the frequency distribution.\ndata['decline_app_cnt'].hist()","624c94ed":"# Checking the frequency distribution.\ndata.boxplot(column=['decline_app_cnt'])","3042afe9":"# Detection of outliers.\noutlier_detect(data,data['decline_app_cnt'])","0479c457":"# Taking the logarithm.\nnp.log(data['decline_app_cnt'] + 1).hist()","c295c4c4":"# Taking the logarithm.\ndata['decline_app_cnt'] = np.log(data['decline_app_cnt'] + 1)","0593f40d":"# Checking the frequency distribution.\ndata.boxplot(column=['decline_app_cnt'])","6e96c3f4":"# Detection of outliers.\noutlier_detect(data,data['decline_app_cnt'])","d49925b1":"# Checking the frequency distribution.\ndata['good_work'].hist(bins=2)","dd1e6d53":"# Checking the frequency distribution.\ndata['score_bki'].hist()","1e52b80c":"# Checking the frequency distribution.\ndata.boxplot(column=['score_bki'])","f4dc1eee":"# Detection of outliers.\noutlier_detect(data,data['score_bki'])","9dffdfbd":"# Checking the frequency distribution.\ndata['bki_request_cnt'].hist()","76dfc9dc":"# Checking the frequency distribution.\ndata.boxplot(column=['bki_request_cnt'])","0a1b7b7f":"# Detection of outliers.\noutlier_detect(data,data['bki_request_cnt'])","41b3acdc":"# Taking the logarithm.\nnp.log(data['bki_request_cnt'] + 1).hist()","bd9b1880":"# Taking the logarithm.\ndata['bki_request_cnt'] = np.log(data['bki_request_cnt'] + 1)","9a6ab565":"# Checking the frequency distribution.\ndata.boxplot(column=['bki_request_cnt'])","8042476b":"# Detection of outliers.\noutlier_detect(data,data['bki_request_cnt'])","08331075":"# Checking the frequency distribution.\ndata['region_rating'].hist()","a8210540":"# Checking the frequency distribution.\ndata['home_address'].hist(bins=3)","29874a69":"# Checking the frequency distribution.\ndata['work_address'].hist(bins=3)","8740149e":"# Checking the frequency distribution.\ndata['income'].hist(bins=100)","0e54d6cd":"# Checking the frequency distribution.\ndata.boxplot(column=['income'])","01452572":"# Checking the frequency distribution.\noutlier_detect(data,data['income'])","e488aab9":"# Taking the logarithm.\nnp.log(data['income'] + 1).hist(bins=100)","dc43f803":"# Taking the logarithm.\ndata['income'] = np.log(data['income'] + 1)","e18e482e":"# Checking the frequency distribution.\ndata.boxplot(column=['income'])","4101aa03":"# Detection of outliers.\noutlier_detect(data,data['income'])","b3a5f0a3":"# Checking the frequency distribution.\ndata['sna'].hist()","1d268ac0":"# Checking the frequency distribution.\ndata['first_time'].hist()","4629724f":"# Checking the frequency distribution.\ndata['foreign_passport'].hist(bins=2)","92b164e7":"# Checking the frequency distribution.\ndata['default'].hist(bins=2)","e48a3263":"# Checking the correlation matrix\ndata_train_temp = data[data['sample']==1]\nsns.heatmap(data_train_temp[num_cols].corr().abs(), vmin=0, vmax=1)","e757aae5":"# Checking the correlation matrix\ndata_train_temp[num_cols].corr().abs().sort_values(by='decline_app_cnt', ascending=False)","3e608d63":"# Checking the frequency distribution.\nfig, axes = plt.subplots(2, 3, figsize=(15, 15))\naxes = axes.flatten()\nfor i in range(len(num_cols)):\n    sns.boxplot(x=\"default\", y=num_cols[i], data=data_train_temp, ax=axes[i])","8393cfe8":"# Checking the importance of features.\nimp_num = Series(f_classif(data_train_temp[num_cols], \n                           data_train_temp['default'])[0], index = num_cols)\nimp_num.sort_values(inplace = True)\nimp_num.plot(kind = 'barh')","f549cc91":"# Checking the importance of features.\nimp_cat = Series(mutual_info_classif(\n    data_train_temp[bin_cols + cat_cols], data_train_temp['default'], \n    discrete_features =True\n), index = bin_cols + cat_cols)\n\nimp_cat.sort_values(inplace = True)\nimp_cat.plot(kind = 'barh')","2eaaebea":"# Standardization of data.\nss = StandardScaler()\ndata[num_cols] = pd.DataFrame(ss.fit_transform(data[num_cols]),columns = data[num_cols].columns)","6226dd8e":"# Checking the data.\ndata.info()\ndata.head(5)","4819136a":"# Data processing and model training.\ndata_temp = data.drop(['sample', 'client_id', 'app_date', 'default'], axis=1)\ndata_education_nan = data_temp[data_temp['education_nan']==1]\ndata_no_nan = data_temp[data_temp['education_nan']==0]\ny = data_no_nan['education'].values\nX = data_no_nan.drop(['education'], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = random_seed)\nmodel = RandomForestRegressor(n_estimators=100, verbose=1, n_jobs=-1, random_state = random_seed)\nmodel.fit(X_train, y_train)\ny_pred = np.round(model.predict(X_test))","cc15d7e8":"# Predicting the values.\npredict = np.round(model.predict(data_education_nan.drop(['education'], axis=1)))","e4bc643f":"# Adding predicted values to the dataset.\nindex_education_nan = data[data['education_nan']==1].index\ndata.loc[index_education_nan,'education'] = predict","d13041c3":"# Encoding categorical variables.\ndata = pd.get_dummies(data, prefix=cat_cols, columns=cat_cols)\ndata.info()","ba79ada6":"# Checking the data.\ndata.head()","1659bd87":"# Splitting the dataset.\ndata_train = data.query('sample == 1').drop(['sample', 'client_id', 'app_date'], axis=1)\ndata_test = data.query('sample == 0').drop(['sample', 'client_id', 'app_date'], axis=1)","179c25bf":"# Training and predicting.\nX = data_train.drop(['default'], axis=1)\ny = data_train['default'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=random_seed)\n\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\ny_pred_prob = model.predict_proba(X_test)[:,1]\ny_pred = model.predict(X_test)","a9f3d65d":"# Plotting the ROC curve\nprobs = model.predict_proba(X_test)\nprobs = probs[:,1]\n\nfpr, tpr, threshold = roc_curve(y_test, probs)\nroc_auc = roc_auc_score(y_test, probs)\n\nplt.figure()\nplt.plot([0, 1], label='Baseline', linestyle='--')\nplt.plot(fpr, tpr, label = 'Regression')\nplt.title('Logistic Regression ROC AUC = %0.3f' % roc_auc)\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.legend(loc = 'lower right')\nplt.show()","f2413f44":"# Checking the confusion matrix.\nshow_confusion_matrix(y_test, y_pred)","fea809bb":"# Checking the metrics.\nall_metrics(y_test, y_pred, y_pred_prob)","ad7e05dd":"model = LogisticRegression(random_state=random_seed)\n\niter_ = 50\nepsilon_stop = 1e-3\n\nparam_grid = [\n    {'penalty': ['l1'], \n     'solver': ['liblinear', 'lbfgs'], \n     'class_weight':['none', 'balanced'], \n     'multi_class': ['auto','ovr'], \n     'max_iter':[iter_],\n     'tol':[epsilon_stop]},\n    {'penalty': ['l2'], \n     'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], \n     'class_weight':['none', 'balanced'], \n     'multi_class': ['auto','ovr'], \n     'max_iter':[iter_],\n     'tol':[epsilon_stop]},\n    {'penalty': ['none'], \n     'solver': ['newton-cg', 'lbfgs', 'sag', 'saga'], \n     'class_weight':['none', 'balanced'], \n     'multi_class': ['auto','ovr'], \n     'max_iter':[iter_],\n     'tol':[epsilon_stop]},\n]\n\ngridsearch = GridSearchCV(model, param_grid, scoring='f1', n_jobs=-1, cv=5)\ngridsearch.fit(X_train, y_train)\nmodel = gridsearch.best_estimator_\n\nbest_parameters = model.get_params()\nfor param_name in sorted(best_parameters.keys()):\n        print('\\t%s: %r' % (param_name, best_parameters[param_name]))\n\npreds = model.predict(X_test)\nprint('Accuracy: %.4f' % accuracy_score(y_test, preds))\nprint('Precision: %.4f' % precision_score(y_test, preds))\nprint('Recall: %.4f' % recall_score(y_test, preds))\nprint('F1: %.4f' % f1_score(y_test, preds))","673391a7":"# Training and predicting.\nmodel = LogisticRegression(random_state=random_seed, \n                           C=1, \n                           class_weight='balanced', \n                           dual=False, \n                           fit_intercept=True, \n                           intercept_scaling=1, \n                           l1_ratio=None, \n                           multi_class='auto', \n                           n_jobs=None, \n                           penalty='l1', \n                           solver='liblinear', \n                           verbose=0, \n                           warm_start=False)\n\nmodel.fit(X_train, y_train)\n\ny_pred_prob = model.predict_proba(X_test)[:,1]\ny_pred = model.predict(X_test)","84e010a7":"# Plotting the ROC curve\nprobs = model.predict_proba(X_test)\nprobs = probs[:,1]\n\nfpr, tpr, threshold = roc_curve(y_test, probs)\nroc_auc = roc_auc_score(y_test, probs)\n\nplt.figure()\nplt.plot([0, 1], label='Baseline', linestyle='--')\nplt.plot(fpr, tpr, label = 'Regression')\nplt.title('Logistic Regression ROC AUC = %0.3f' % roc_auc)\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.legend(loc = 'lower right')\nplt.show()","a25d11e1":"# Checking the confusion matrix.\nshow_confusion_matrix(y_test, y_pred)","d76509ec":"# Checking the metrics.\nall_metrics(y_test, y_pred, y_pred_prob)","5a75a1c5":"data_train = data.query('sample == 1').drop(['sample', 'client_id', 'app_date'], axis=1)\ndata_test = data.query('sample == 0').drop(['sample', 'client_id', 'app_date'], axis=1)","33befcfd":"X_train = data_train.drop(['default'], axis=1)\ny_train = data_train['default'].values\nX_test = data_test.drop(['default'], axis=1)","bc6442ae":"model = LogisticRegression(random_state=random_seed, \n                           C=1, \n                           class_weight='balanced', \n                           dual=False, \n                           fit_intercept=True, \n                           intercept_scaling=1, \n                           l1_ratio=None, \n                           multi_class='auto', \n                           n_jobs=None, \n                           penalty='l1', \n                           solver='liblinear', \n                           verbose=0, \n                           warm_start=False,\n                           max_iter=1000)\n\nmodel.fit(X_train, y_train)","db6f94d6":"y_pred_prob = model.predict_proba(X_test)[:,1]","28768f4b":"submit = pd.DataFrame(data.query('sample == 0')['client_id'])\nsubmit['default'] = y_pred_prob\nsubmit.to_csv('submission.csv', index=False)","ac4773c2":"The balance of classes in our dataset is strongly skewed towards customers who do not have a default.","afeb4460":"We have identified optimal hyperparameters. Now let's train the model using these hyperparameters.","69193ec5":"## 3.5 Car","f8b2595d":"## 3.13 Work address","53a8a031":"# 3. Analysis of variables","5ca7e35a":"## 3.11 Region rating","eb840a7e":"Let's see what is the earliest request date in the dataset.","d709cac8":"# 6. Model","cc57e7c9":"## 3.17 Foreign passport","90ace5db":"# 2. Preliminary data examination & engineering\n","bdf371ed":"The code for regularization was taken from [here](https:\/\/www.kaggle.com\/sokolovaleks\/sf-dst-10-creditscoring-golobokov-sokolov) and slightly modified. Kudos to Alexandr Sokolov and Andrey Golobokov.","1c4af8b5":"It seems that the bank has much more female clients.","8b91cf7f":"# 8. Regularization","fe35bbfd":"The ROC AUC score is good, but the confusion matrix shows us that our model predicts defaults very poorly. Of the 1827 defaults, only 40 were correctly predicted, or about 2 percent (very low Recall). Let's try to improve the situation by using custom hyperparameters.","f9427f65":"## 3.14 Income","536e8a12":"## 3.10 BKI requests","0ed52f11":"The vast majority of borrowers do not have rejected applications, so our function treats a significant part of the results as outliers. Let's try to correct the situation by taking the logarithm of the column.","cbf702e3":"Everything looks logical: the higher the level of education, the fewer clients apply for a loan. Now let's convert education feature to numeric format using 'map' function.","1042f336":"Let's add our new feature to the list of continuous variables.","6511a2bc":"## 3.15 SNA (level of connection with other clients)","e4f682d8":"Logarithmization improved the situation, the distribution changed from lognormal to normal, and the number of outliers decreased from 7000 to 2609.","261060b6":"![](https:\/\/miro.medium.com\/max\/884\/1*UDi7KpyFX8gwV1k7aeMS-g.jpeg)\n\nThis was my first experience using a logistic regression for classification purpose, which I have done in the Data Science course from SkillFactory. The legend was that we have a dataset for credit scoring. The task was to build a model for predicting the probability of default of secondary clients. The development time was limited to 48 hours.","d60978d5":"## 3.18 Default","a4a0f526":"Most of the clients in this dataset don't have a well-paid job.","1eae7576":"## 3.3 Gender","d4fc710e":"Let's convert binary variables to numeric format using LabelEncoder.","055eabc7":"There were no strong correlations between the features, except for the number of declined applications and the BKI score. Unfortunately, I didn't have enough time to test the model with a different set of features.","4633df50":"Most of the clients in this dataset don't have a foreign passport.","845e7061":"# 10. Recap & Conclusions","c2624ed1":"Of the continuous variables, the most important are the number of declined applications and the BKI rating. Of the categorical and binary non-variables, the most important are connections with other clients and the time spent in the database.","f6249a77":"## 5.2 Using RandomForestRegressor to fill missing values in \"education\" feature","0833fc9d":"People in their mid-30s are more likely to make big purchases, so it seems logical that we have a maximum there. Now let's log the data and look at the distribution.","98d85a06":"# 9. Submission","9d519c49":"## 3.2 Education","448c3f02":"## 3.7 Declined applications","d6e6d063":"## 3.4 Age","29920458":"# 5. Data preprocessing\u00b6\n","9893dfea":"Most car owners from this dataset have a car of domestic production.","ecfc7d2f":"## 3.8 Good work (well-paid job)","529bed23":"# General information","181e8e9b":"# 7. Model evaluation","3b0a7b71":"## 3.16 First time (how long the client has been in the database)","e357237a":"As a temporary measure, let's fill in the missing values with school education.","e2ceb062":"## 3.1 Application date (+new feature: timedelta)","4f401c7a":"Default clients are on average younger and earn less, have more applications and rejections from the bank, but simultaneously they have a higher BKI rating.","b194c6bd":"Now let's introduce a new variable - the difference between the request date and the minimum.","4ef8e7da":"* app_date - date of request, time variable, requires processing.\n* education - level of education, categorical variable, requires processing and missing values correction.\n* sex - binary variable, requires processing.\n* age - continuous variable, requires processing.\n* car - car availability, binary variable, requires processing.\n* car_type - foreign-made car availability, binary variable, requires processing.\n* decline_app_cnt - number of rejected requests, continuous variable.\n* good_work - flag of a well-paid job, binary variable.\n* score_bki - BKI (credit reporting agency) internal score, continuous variable.\n* bki_request_cnt - number of requests to the BKI (credit reporting agency), continuous variable.\n* region_rating - rating of the region, categorical variable.\n* home_address - home address categorizer, categorical variable.\n* work_address - work address categorizer, categorical variable.\n* income - client's income level, continuous variable. \n* sna - level of connection with another clients, categorical variable.\n* first_time - how long the client has been in the database, categorical variable.\n* foreign_passport - passport availability, binary variable, requires processing.\n* default - default in the past, binary target variable.  ","03177a78":"The distribution looks normal, so let's keep the logarithm.","0fc28cc5":"Let's follow the actions taken:\n\n* We initialized necessary libraries, set visualization conditions and loaded the dataset.\n* We analyzed the features, identified the target variable, looked at external sources, and suggested which features we can rely on for feature engineering.\n* We checked each variable, frequency distributions and created several new features.\n* We filled in the missing values by training the model on the available data.\n* We encoded categorical variables and standardized the data.\n* We trained logistic regression on the available data and evaluated the quality of its prediction using confusion matrix, ROC AUC, and other metrics.\n* We selected hyperparameters and trained the model on them, improving its consumer qualities.\n\nThe following conclusions can be drawn from the results:\n* We can fill in the missing values using a prediction model based on the available data.\n* Built-in sklearn features make pre-processing of data much easier.\n* We should not blindly focus on metrics, the consumer qualities of the model are also very important.","707fe5ce":"Most of the clients in this dataset don't have a car.","ee41524f":"# 1. Initial setup","6a70b068":"# 4. Feature importance","3e0f34a7":"## 3.12 Home address","bc272d5e":"## 5.1 Standardization","7a9a89c6":"The distribution looks normal, and there aren't many outliers.","8cc75484":"Let's create a new binary variable for missing values in the \"education\"column.","9e1a8490":"## 5.3 One-hot encoding","c203ac4d":"Logarithmization significantly improved the situation, the number of outliers was reduced from 2636 to 15.\n","c10fab46":"## 3.6 Foreign-made car","0ea7b094":"## 3.9 BKI score","684dd642":"Let's use the knowledge gained in [previous competition](https:\/\/www.kaggle.com\/ogurrw\/sf-tripadvisor-rating-akbar-murataliev) to predict the missing values in the \"education\"column.","57575a99":"Thanks to the use of hyperparameters, Recall increased from 2 to 68 percent. Unfortunately, the Precision has decreased, because now our algorithm is more likely to detect defaults even where there are none. Harmonic mean between Recall and Precision (F1 Score) also decreased. However, from the point of view of the consumer, i.e. the bank, we can say that the quality of the model has increased. However, there is still enough room for improvement.","790f7d6d":"Unfortunately, the situation has not changed for the better."}}