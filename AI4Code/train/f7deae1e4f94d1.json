{"cell_type":{"08c066de":"code","78a750a5":"code","cfbdec73":"code","74c49d4b":"code","38d1a932":"code","97dedb81":"code","d576a30d":"code","cdef752e":"code","7a2e4182":"code","4cccb839":"code","703ec57b":"code","dc5f5f79":"code","09963bf5":"code","1ae61943":"code","12b8279f":"code","8df5406b":"code","5c2cea8b":"code","a0042b49":"code","3870ef91":"code","eceb2a85":"code","2b444912":"code","fb510a7a":"code","0f997d5c":"markdown","e481af5f":"markdown","d03c4917":"markdown","bbb3c1f5":"markdown","64ed08cb":"markdown","77609735":"markdown"},"source":{"08c066de":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torchvision.utils import make_grid\nfrom torchvision import transforms\nfrom torch.utils.data import random_split, TensorDataset, DataLoader","78a750a5":"df = pd.read_csv('..\/input\/digit-recognizer\/train.csv')","cfbdec73":"df.head()","74c49d4b":"labels_array = np.array(df.label)\nfeatures_array = df.loc[:, df.columns != 'label'].values \/ 255\n\nX_train = torch.from_numpy(features_array).view(-1, 1, 28, 28).float()\ntargets = torch.from_numpy(labels_array).type(torch.LongTensor)\nprint(X_train.shape)\n\ndataset = TensorDataset(X_train, targets)\n\ntrain_size = 32000\nval_size = 10000\ntrain_data, val_data = random_split(dataset, [train_size, val_size])\n\ntrain_loader = DataLoader(train_data, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\nval_loader = DataLoader(val_data, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)","38d1a932":"for imgs, _ in train_loader:\n    print('Original image shape', imgs.shape)\n    plt.figure(figsize=(14, 8))\n    plt.axis('off')\n    plt.imshow(make_grid(imgs, nrow=16).permute(1, 2, 0))\n    break","97dedb81":"def get_device():\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n\ndef to_device(data, device):\n    if isinstance(data, (list, tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)","d576a30d":"class Device():\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n    \n    def __iter__(self):\n        for b in self.dl:\n            yield to_device(b, self.device)\n    \n    def __len__(self):\n        return len(self.dl)","cdef752e":"device = get_device()\n\ntrain_loader = Device(train_loader, device)\nval_loader = Device(val_loader, device)","7a2e4182":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))","4cccb839":"class DigitRecognizerModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.cnn = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            nn.Conv2d(32, 32, kernel_size=3),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            nn.Conv2d(32, 64, kernel_size=3),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            nn.Conv2d(64, 10, kernel_size=3, stride=1, padding=1),\n            \n            nn.Flatten(),\n            nn.Sigmoid()\n        )\n        \n    \n    def forward(self, xb):\n        out = self.cnn(xb)\n        xb = xb.view(xb.size(0), -1)\n        return out\n    \n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  \n        loss = F.cross_entropy(out, labels)\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    \n        loss = F.cross_entropy(out, labels)\n        acc = accuracy(out, labels)\n        return {'val_loss': loss, 'val_acc': acc}\n    \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n","703ec57b":"def evaluate(model, validation_loader):\n    outputs = [model.validation_step(batch) for batch in validation_loader]\n    return model.validation_epoch_end(outputs)\n\ndef fit(epochs, model, train_loader, val_loader, learning_rate, opt_func=torch.optim.SGD):\n        history = []\n        optimizer = opt_func(model.parameters(), learning_rate)\n        for epoch in range(epochs):\n            for batch in train_loader:\n                loss = model.training_step(batch)\n                loss.backward()\n                optimizer.step()\n                optimizer.zero_grad()\n            result = evaluate(model, val_loader)\n            model.epoch_end(epoch, result)\n            history.append(result)\n        return history","dc5f5f79":"model = DigitRecognizerModel()\nto_device(model, device)","09963bf5":"for images, labels in train_loader:\n    outputs = model(images)\n    loss = F.cross_entropy(outputs, labels)\n    print('Loss: ', loss.item())\n    break","1ae61943":"history = [evaluate(model, val_loader)]\nhistory","12b8279f":"fit(100, model, train_loader, val_loader, learning_rate=0.1)","8df5406b":"test_df = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\ns_subm = pd.read_csv('..\/input\/digit-recognizer\/sample_submission.csv')","5c2cea8b":"s_subm.head()","a0042b49":"test_features = test_df.loc[:, ].values \/ 255\n\nX_test = torch.from_numpy(test_features).view(-1, 1, 28, 28).float()\n\nX_loader = DataLoader(X_test, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n\nX_test = Device(X_loader, device)","3870ef91":"def predict(test_data):\n    model.eval()\n    \n    test_prediction = torch.LongTensor()\n    \n    for i, imgs in enumerate(test_data):\n        output = model(imgs)\n    \n        pred = output.cpu().data.max(1, keepdim=True)[1]\n        test_prediction = torch.cat((test_prediction, pred), dim=0)\n    \n    return test_prediction","eceb2a85":"preds = predict(X_test)","2b444912":"image_ids = np.arange(1, len(s_subm)+1)\nsubmission_df = pd.DataFrame({\n    'ImageId': image_ids,\n    'Label': preds.numpy().reshape(preds.numpy().shape[0])\n})","fb510a7a":"submission_df.to_csv('submission.csv', index=False)","0f997d5c":"Make predictions","e481af5f":"The best score on validation dataset - 98.05 % accuracy\n\nHow to improve this model to get better results?\n\n* Increase a numbers of hidden layers\n* Try to use LeakyReLU as activation function with different negative slope.\n* Choose another optimizer\n* Change learning rate\n* Give into training dataset more examples.","d03c4917":"Load the data and split it to train and validation","bbb3c1f5":"Load our data to GPU","64ed08cb":"Without training our model, we have 9% of accuracy on validation data","77609735":"We will use a CNN model"}}