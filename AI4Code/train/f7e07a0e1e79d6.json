{"cell_type":{"fdb1642b":"code","79ed90d2":"code","c312c6fd":"code","3b22be68":"code","08191c11":"code","aef41efb":"code","d8448915":"code","6fd7cb35":"code","1dc566ca":"code","025e216e":"code","792f233c":"code","d7c47586":"code","6ab24df4":"code","55471f80":"code","1c5dffb2":"code","d7354e15":"code","e8b254f8":"code","f982e68a":"code","7f2a5809":"code","e3537b0c":"markdown","7bd4ea2d":"markdown","63b678c7":"markdown","ca43f667":"markdown"},"source":{"fdb1642b":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport os # accessing directory structure\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.utils import shuffle\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer \nfrom nltk.tokenize import word_tokenize\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils.np_utils import to_categorical\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding, Dense, Input, Flatten, Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, concatenate\nfrom keras.models import Model, Sequential\nfrom tensorflow.keras import regularizers\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","79ed90d2":"! conda install -y gdown\nimport gdown\n!gdown https:\/\/drive.google.com\/file\/d\/1ozuMHtM1PXj-csgcr8dh6p_oiRhyLcjO\/view?usp=sharing\n\nurl = 'https:\/\/drive.google.com\/file\/d\/1ozuMHtM1PXj-csgcr8dh6p_oiRhyLcjO\/view?usp=sharing'\n\n# output = 'glove.6B.100d.txt'\n\ngdown.download(url)","c312c6fd":"df1 = pd.read_csv('\/kaggle\/input\/hate-speech-and-offensive-language-dataset\/labeled_data.csv')","3b22be68":"print(df1.columns)\ndf1.head(10)","08191c11":"del df1[df1.columns[0]]","aef41efb":"df1.isnull().sum()","d8448915":"# Converting all string to lower case\ndf1 = df1.apply(lambda x: x.astype(str).str.lower())\n\n# Removing Punctuations\ndf1.tweet = df1.tweet.str.replace('[^\\s\\w]','')\n\n# Removing HTML Tags\ndf1.tweet = df1.tweet.str.replace('[^\\s\\w]','')\n\n# Tokenizing\nnltk.download('punkt')\n\ndf1['tweet_token'] = df1['tweet'].apply(lambda x: word_tokenize(x))\n\n# Stemming\nps = PorterStemmer() \n\ndf1.tweet = df1.tweet_token.apply(lambda x: list(ps.stem(i) for i in x))\n\n# Removing the stop words and Rejoining \nnltk.download('stopwords')\nstops = set(stopwords.words(\"english\"))                  \n\ndf1.tweet = df1.tweet.apply(lambda x: ' '.join(list(i for i in x if i not in stops)))\n\n# Lammatizing\nnltk.download('wordnet')\nlamatizer = WordNetLemmatizer()\n\ndf1.tweet.apply(lambda x: lamatizer.lemmatize(x))","6fd7cb35":"df1.columns","1dc566ca":"tokenizer = Tokenizer(num_words = 4500, filters='!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n', lower = True, split = ' ')\ntokenizer.fit_on_texts(texts = df1.tweet)\nX1 = tokenizer.texts_to_sequences(texts = df1.hate_speech)\nX2 = tokenizer.texts_to_sequences(texts = df1.offensive_language)\nX3 = tokenizer.texts_to_sequences(texts = df1.neither)\nX4 = tokenizer.texts_to_sequences(texts = df1.tweet)\nword_index = tokenizer.word_index","025e216e":"data1 = pad_sequences(sequences= X1 , maxlen = 1000)\nclass1 = to_categorical(np.asarray(df1['class']), num_classes = 3)\n\ndata2 = pad_sequences(sequences= X2 , maxlen = 1000)\nclass2 = to_categorical(np.asarray(df1['class']), num_classes = 3)\n\ndata3 = pad_sequences(sequences= X3 , maxlen = 1000)\nclass3 = to_categorical(np.asarray(df1['class']), num_classes = 3)\n\ndata4 = pad_sequences(sequences= X4 , maxlen = 1000)\nclass4 = to_categorical(np.asarray(df1['class']), num_classes = 3)\n\nprint('Length of data1 tensor:', data1.shape)\nprint('Length of labels1 tensor:', class1.shape)\nprint('Length of data1 tensor:', data2.shape)\nprint('Length of labels1 tensor:', class2.shape)\nprint('Length of data1 tensor:', data3.shape)\nprint('Length of labels1 tensor:', class3.shape)\nprint('Length of data1 tensor:', data4.shape)\nprint('Length of labels1 tensor:', class4.shape)","792f233c":"indices1 = np.arange(df1.shape[0])\nnp.random.shuffle(indices1)\ndata1 = data1[indices1]\nclass1 = class1[indices1]\nx_train1, x_test1, y_train1, y_test1 = train_test_split(data1, class1, test_size=0.2, random_state=42)\nx_test1, x_val1, y_test1, y_val1 = train_test_split(data1, class1, test_size=0.4, random_state=42)\n\n# data2 = data2[indices1]\n# class2 = class2[indices1]\n# x_train2, x_test2, y_train2, y_test2 = train_test_split(data2, class2, test_size=0.2, random_state=42)\n# x_test2, x_val2, y_test2, y_val2 = train_test_split(data2, class2, test_size=0.4, random_state=42)\n\n# dat3 = data3[indices1]\n# class3 = class3[indices1]\n# x_train3, x_test3, y_train3, y_test3 = train_test_split(data3, class3, test_size=0.2, random_state=42)\n# x_test3, x_val3, y_test3, y_val3 = train_test_split(data3, class3, test_size=0.4, random_state=42)\n\n# data3 = data3[indices1]\n# class3 = class3[indices1]\n# x_train3, x_test3, y_train3, y_test3 = train_test_split(data3, class3, test_size=0.2, random_state=42)\n# x_test3, x_val1, y_test3, y_val3 = train_test_split(data3, class3, test_size=0.4, random_state=42)\n\nprint('1:')\nprint(x_train1.shape)\nprint(y_train1.shape)\nprint(x_test1.shape)\nprint(y_test1.shape)\nprint(x_val1.shape)\nprint(y_val1.shape)","d7c47586":"#Using Pre-trained word embeddings\nMAX_SEQUENCE_LENGTH = 1000\nGLOVE_DIR = \"..\/input\/glove-global-vectors-for-word-representation\/\" \nembeddings_index = {}\nf = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"utf8\")\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Total %s word vectors in Glove.' % len(embeddings_index))\n\nembedding_matrix = np.random.random((len(word_index) + 1, 100))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\n        \nembedding_layer = Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], input_length=1000)","6ab24df4":"### Model for 1st data set (test_data1)\ndef DCNN_model(n, x_train, y_train, x_val, y_val, x_test, y_test):\n    sequence_input = Input(shape=(1000,), dtype='int32')\n    embedded_sequences = embedding_layer(sequence_input)\n    l_cov1= Conv1D(8, 5, activation='relu')(embedded_sequences)\n    l_pool1 = MaxPooling1D(5)(l_cov1)\n    l_cov2 = Conv1D(8, 5, activation='relu')(l_pool1)\n    l_pool2 = MaxPooling1D(5)(l_cov2)\n    l_cov3 = Conv1D(8, 5, activation='relu')(l_pool2)\n    l_pool3 = MaxPooling1D(35)(l_cov3)\n    l_flat = Flatten()(l_pool3)\n    l_dense = Dense(8, activation='relu')(l_flat)\n    l_dense1 = Dense(8, activation='relu')(l_dense)\n    l_dense2 = Dense(8, activation='relu')(l_dense1)\n    preds = Dense(3, activation='softmax')(l_dense2)\n\n    dcnn_model = Model(sequence_input, preds)\n    dcnn_model.compile(loss='categorical_crossentropy',optimizer='adadelta',metrics=['acc'])\n    print(\"Fitting the simple convolutional neural network model\")\n    dcnn_model.summary()\n    history = dcnn_model.fit(x_train, y_train, validation_data=(x_val, y_val),epochs=10, batch_size=8)\n    print('\\nModel Training Completed !')\n\n    ### PREDICTING \n    y_preds = dcnn_model.predict(x_test)\n    y_pred = np.round(y_preds)\n    cpred = float(sum(y_pred == y_test)[0])\n    cm = confusion_matrix(y_test.argmax(1), y_pred.argmax(1))\n    print(\"\\n-> Correct predictions:\", cpred)\n    print(\"\\n-> Total number of test examples:\", len(y_test))\n    print(\"\\n-> Accuracy of model: \", cpred\/float(len(y_test)))\n    print(\"\\n-> Confusion matrix for Dataset\",n,\": \", cm)\n\n    plt.matshow(cm, cmap=plt.cm.binary, interpolation='nearest')\n    plt.title('Confusion matrix - CNN Model 1')\n    plt.colorbar()\n    plt.ylabel('Expected label')\n    plt.xlabel('Predicted label')\n    plt.show()\n    return history\n","55471f80":"history = DCNN_model(x_train1, y_train1, x_val1, y_val1, x_test1, y_test1,\n                     x_train2, y_train2, x_val2, y_val2, x_test2, y_test2,\n                     x_train3, y_train3, x_val3, y_val3, x_test3, y_test3,\n                     x_train4, y_train4, x_val4, y_val4, x_test4, y_test4)\nprint(' ')\n\n# list all data in history\nprint(history.history.keys())\n\n# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","1c5dffb2":"def define_model(x_train1, y_train1, x_val1, y_val1, x_test1, y_test1):\n    \n    sequence_input1 = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n    embedded_sequences1 = embedding_layer(sequence_input1)\n    cov1= Conv1D(32, 5, activation='relu')(embedded_sequences1)\n    pool1 = MaxPooling1D(5)(cov1)\n    flat1 = Flatten()(pool1)\n\n    sequence_input2 = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n    embedded_sequences2 = embedding_layer(sequence_input2)\n    cov2 = Conv1D(32, 5, activation='relu')(embedded_sequences2)\n    pool2 = MaxPooling1D(5)(cov2)\n    flat2 = Flatten()(pool2)\n\n    sequence_input3 = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n    embedded_sequences3 = embedding_layer(sequence_input3)\n    cov3 = Conv1D(32, 5, activation='relu')(embedded_sequences3)\n    pool3 = MaxPooling1D(35)(cov3)\n    flat3 = Flatten()(pool3)\n    \n#     sequence_input4 = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n#     embedded_sequences4 = embedding_layer(sequence_input4)\n#     cov4 = Conv1D(32, 5, activation='relu')(embedded_sequences4)\n#     pool4 = MaxPooling1D(35)(cov4)\n#     flat4 = Flatten()(pool4)\n\n    merge = concatenate([flat1, flat2, flat3])\n\n    # flat4 = Flatten()(merge)\n    dense = Dense(32, activation='relu')(merge)\n    preds = Dense(3, activation='softmax')(dense)\n\n    model = Model(inputs = [sequence_input1, sequence_input2, sequence_input3], outputs = preds)\n    model.compile(loss='categorical_crossentropy',optimizer='adadelta',metrics=['acc'])\n    print(\"Fitting the simple convolutional neural network model\")\n    model.summary()\n    # history = model.fit([x_train, x_train, x_train], y_train, epochs=2, batch_size=32)\n    history = model.fit([x_train1, x_train2, x_train3], y_train1,validation_data = ([x_val1, x_val1, x_val1], y_val1), epochs=10, batch_size=32)\n    print('\\nModel Training Completed !')\n\n    ### PREDICTION\n    y_pred = np.round(model.predict([x_test1, x_test1, x_test1]))\n    cpred = float(sum(y_pred == y_test1)[0])\n    cm = confusion_matrix(y_test1.argmax(1), y_pred.argmax(1))\n    print(\"\\n-> Correct predictions:\", cpred)\n    print(\"\\n-> Total number of test examples:\", len(y_test1))\n    print(\"\\n-> Accuracy of model: \", cpred\/float(len(y_test1)))\n    print(\"\\n-> Confusion Matrix: \", cm)\n\n    plt.matshow(cm, cmap=plt.cm.binary, interpolation='nearest')\n    plt.title('Confusion matrix - CNN Model 1')\n    plt.colorbar()\n    plt.ylabel('Expected label')\n    plt.xlabel('Predicted label')\n    plt.show()\n    \n    return history","d7354e15":"history = define_model(x_train1, y_train1, x_val1, y_val1, x_test1, y_test1)\n\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","e8b254f8":"### Model for 1st data set (test_data1)\ndef LSTM_model(n, x_train, y_train, x_val, y_val, x_test, y_test):\n    lstm_model = Sequential()\n    lstm_model.add(Embedding(len(word_index) + 1, 100, weights = [embedding_matrix], input_length = MAX_SEQUENCE_LENGTH, trainable = False))\n    lstm_model.add(LSTM(128))\n    lstm_model.add(Dense(128, activation = 'relu'))\n    lstm_model.add(Dense(64, activation = 'relu'))\n    lstm_model.add(Dense(32, activation = 'relu'))\n    lstm_model.add(Dense(3, activation = 'softmax'))\n    lstm_model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    lstm_model.summary()\n    history = lstm_model.fit(x_train, y_train, validation_data = (x_val, y_val), epochs = 15, batch_size = 128)\n    print('\\nModel Training Complete !')\n    \n    ### PREDICTION\n    y_preds = lstm_model.predict(x_test)\n    y_pred = np.round(y_preds)\n    cpred = float(sum(y_pred == y_test)[0])\n    cm = confusion_matrix(y_test.argmax(1), y_pred.argmax(1))\n    print(\"\\n-> Correct predictions:\", cpred)\n    print(\"\\n-> Total number of test examples:\", len(y_test))\n    print(\"\\n-> Accuracy of model: \", cpred\/float(len(y_test)))\n    print(\"\\n-> Confusion for Dataset\",n,\": \", cm)\n\n    plt.matshow(cm, cmap=plt.cm.binary, interpolation='nearest')\n    plt.title('Confusion matrix - CNN Model 1')\n    plt.colorbar()\n    plt.ylabel('Expected label')\n    plt.xlabel('Predicted label')\n    plt.show()\n    return history","f982e68a":"history = LSTM_model(1, x_train1, y_train1, x_val1, y_val1, x_test1, y_test1)","7f2a5809":"# list all data in history\nprint(history.history.keys())\n\n# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","e3537b0c":"# Multi Channel CNN","7bd4ea2d":"# Deep CNN","63b678c7":"# LSTM","ca43f667":"# Data Pre-Processing"}}