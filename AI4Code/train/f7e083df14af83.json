{"cell_type":{"d96243a6":"code","23f345d1":"code","8760bdda":"code","babce3d4":"code","35e0b144":"code","345a94d6":"code","1b0623d9":"code","30e11e55":"code","731581c1":"code","84be16d1":"code","0b50cf75":"code","43d8d1c4":"code","af990482":"code","8121f65a":"code","5c87f19f":"code","4df534d2":"code","6f9dc007":"code","ec31c640":"code","8cf17e21":"code","0640e4d9":"code","88ee1f9a":"code","3ca141bc":"code","c701d6c1":"code","8e6259c1":"code","669511f3":"code","673142c9":"code","c2f80f3a":"code","eb5c9027":"code","f1c82850":"code","3a71e14b":"code","ca514c02":"code","ccd55c97":"markdown","6f217d55":"markdown","92edef6a":"markdown","917c8003":"markdown","b79966b9":"markdown","5b16de89":"markdown","f9cca6c8":"markdown","37290f4b":"markdown","cbf9a43e":"markdown"},"source":{"d96243a6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","23f345d1":"df = pd.read_csv('..\/input\/heart-disease-prediction-using-logistic-regression\/framingham.csv')\ndf.head()","8760bdda":"print(\"The dataset is consisted of {} entries and {} features\".format(df.shape[0], df.shape[1]))","babce3d4":"df.info()","35e0b144":"df.isnull().sum()","345a94d6":"df = df.drop(['education'],axis=1)","1b0623d9":"df.cigsPerDay.describe()","30e11e55":"df['cigsPerDay'].fillna(df['cigsPerDay'].median(), inplace=True)","731581c1":"df.BPMeds.value_counts()","84be16d1":"df['BPMeds'].fillna(df['BPMeds'].value_counts().index[0], inplace=True)","0b50cf75":"df.totChol.describe()","43d8d1c4":"df['totChol'].fillna(df['totChol'].mean(), inplace=True)","af990482":"df.BMI.describe()","8121f65a":"df['BMI'].fillna(df['BMI'].mean(), inplace=True)","5c87f19f":"df.heartRate.describe()","4df534d2":"df['heartRate'].fillna(df['heartRate'].value_counts().index[0], inplace=True)","6f9dc007":"df.glucose.describe()","ec31c640":"df['glucose'].fillna(df['glucose'].mean(), inplace=True)","8cf17e21":"df.isnull().sum()","0640e4d9":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom statsmodels.tools import add_constant\nimport warnings\nwarnings.filterwarnings('ignore')\n\nX = df.drop(['TenYearCHD'], axis=1)\nX = add_constant(X)\ny = df['TenYearCHD']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123)\n\nlogReg = LogisticRegression().fit(X_train, y_train)\n\ntrain_pred = logReg.predict(X_train)\ntest_pred = logReg.predict(X_test)\n\nprint('Train set accuracy score:', accuracy_score(y_train, train_pred))\nprint('Test set accuracy score:', accuracy_score(y_test, test_pred))","88ee1f9a":"import statsmodels.api as sm\n\nresult = sm.Logit(y, X).fit()\nresult.summary()","3ca141bc":"X.drop(['currentSmoker'], axis=1, inplace=True)\n\nresult = sm.Logit(y, X).fit()\nresult.summary()","c701d6c1":"X.drop(['BMI'], axis=1, inplace=True)\n\nresult = sm.Logit(y, X).fit()\nresult.summary()","8e6259c1":"X.drop(['heartRate'], axis=1, inplace=True)\n\nresult = sm.Logit(y, X).fit()\nresult.summary()","669511f3":"X.drop(['diaBP'], axis=1, inplace=True)\n\nresult = sm.Logit(y, X).fit()\nresult.summary()","673142c9":"X.drop(['diabetes'], axis=1, inplace=True)\n\nresult = sm.Logit(y, X).fit()\nresult.summary()","c2f80f3a":"X.drop(['BPMeds'], axis=1, inplace=True)\n\nresult = sm.Logit(y, X).fit()\nresult.summary()","eb5c9027":"X.drop(['prevalentHyp'], axis=1, inplace=True)\n\nresult = sm.Logit(y, X).fit()\nresult.summary()","f1c82850":"X.drop(['totChol'], axis=1, inplace=True)\n\nresult = sm.Logit(y, X).fit()\nresult.summary()","3a71e14b":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123)\n\nlogReg = LogisticRegression().fit(X_train, y_train)\ntrain_pred = logReg.predict(X_train)\ntest_pred = logReg.predict(X_test)\n\nprint('New train set accuracy:', accuracy_score(y_train, train_pred))\nprint('New test set accuracy:', accuracy_score(y_test, test_pred))","ca514c02":"confusion_matrix(y_test, test_pred)","ccd55c97":"I'm using backward elimination to select a certain amount of features and drop the rest of them to see if the model can perform better with new dataset","6f217d55":"# First glimpse of the dataset","92edef6a":"# Filling missing data","917c8003":"We need to drop one feature with the highest p-value among those above 0.05 (5%), and continue this process until there is no feature which its p-value is higher than 0.05.","b79966b9":"After performing backward elimination, there is no significant improvement in prediction accuracy, either training and test set. The problem could be the way I performed backward elimination with the feature chosen. ","5b16de89":"# Backward Elimination","f9cca6c8":"I decided to drop education because it seems not related to the target we want to predict","37290f4b":"# Fitting Default Model","cbf9a43e":"I first fit the data into logistic regression model without modifying any of its features"}}