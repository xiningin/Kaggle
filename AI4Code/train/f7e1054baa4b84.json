{"cell_type":{"fde9f85d":"code","ede05ea9":"code","71c7da2c":"code","c7206995":"code","6f400570":"code","c9b319a6":"code","ed4d72a1":"code","294c1814":"code","41897459":"code","9696035c":"code","1cb2f8b9":"code","c8a80d06":"code","01f93ea9":"code","e393401e":"code","409306e3":"code","e2dcaf36":"code","bf413cca":"code","15971aca":"code","28992fbf":"code","0e7fc8f0":"code","e68dd4ea":"code","ed8779fd":"code","51cc489a":"code","6d0fe025":"code","700be7d8":"code","b2719f78":"code","0e357e09":"code","5702912c":"code","a7b2af5e":"markdown","abf4b659":"markdown","41f00bf5":"markdown","a467bc22":"markdown","ba62c67d":"markdown","852f269f":"markdown","9a6c7ef6":"markdown","60b1d384":"markdown","836d7615":"markdown","175a949c":"markdown","1a842bde":"markdown","41be3087":"markdown","51f9cebb":"markdown","f3e24d20":"markdown","e11dae89":"markdown","46e40e3a":"markdown","8a98c5c1":"markdown","40b16567":"markdown","3f91a37e":"markdown","6297c484":"markdown","30a32ae5":"markdown","033e9290":"markdown"},"source":{"fde9f85d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n#\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nprint(os.listdir(\"..\/input\"))\n\nsize_img = 28\nthreshold_color = 100 \/ 255\n\n# Any results you write to the current directory are saved as output.\nfile = open(\"..\/input\/digit-recognizer\/train.csv\")\ndata_train = pd.read_csv(file)\n\ny_train = np.array(data_train.iloc[:, 0])\nx_train = np.array(data_train.iloc[:, 1:])\n\nfile = open(\"..\/input\/digit-recognizer\/test.csv\")\ndata_test = pd.read_csv(file)\nx_test = np.array(data_test)\n\nn_features_train = x_train.shape[1]\nn_samples_train = x_train.shape[0]\nn_features_test = x_test.shape[1]\nn_samples_test = x_test.shape[0]\nprint(n_features_train, n_samples_train, n_features_test, n_samples_test)\nprint(x_train.shape, y_train.shape, x_test.shape)","ede05ea9":"def show_img(x):\n    plt.figure(figsize=(8,7))\n    if x.shape[0] > 100:\n        print(x.shape[0])\n        n_imgs = 16\n        n_samples = x.shape[0]\n        x = x.reshape(n_samples, size_img, size_img)\n        for i in range(16):\n            plt.subplot(4, 4, i+1) #devide figure into 4x4 and choose i+1 to draw\n            plt.imshow(x[i])\n        plt.show()\n    else:\n        plt.imshow(x)\n        plt.show()","71c7da2c":"def int2float_grey(x):\n    x = x \/ 255\n    return x","c7206995":"# x_train[x_train<100] = 0\n# x_train[x_train>=100] = 1\n# # print(x_train[0])\n# show_img(x_train)","6f400570":"# find the left egde\n# Note: the problem is that I don't do the parrallel part\ndef find_left_edge(x):\n    edge_left = []\n    n_samples = x.shape[0]\n    for k in range(n_samples):\n        for j in range(size_img):\n            for i in range(size_img):\n                if (x[k, size_img*i+j] >= threshold_color):\n                    edge_left.append(j)\n                    break\n            if (len(edge_left) > k):\n                break\n    return edge_left\n# find_left_edge(x_train)","c9b319a6":"# find the right egde\n# Note: the problem is that I don't do the parrallel part\ndef find_right_edge(x):\n    edge_right = []\n    n_samples = x.shape[0]\n    for k in range(n_samples):\n        for j in range(size_img):\n            for i in range(size_img):\n                if (x[k, size_img*i+(size_img-1-j)] >= threshold_color):\n                    edge_right.append(size_img-1-j)\n                    break\n            if (len(edge_right) > k):\n                break\n    return edge_right\n# find_right_edge(x_train)","ed4d72a1":"# find the top egde\n# Note: the problem is that I don't do the parrallel part\ndef find_top_edge(x):\n    edge_top = []\n    n_samples = x.shape[0]\n    for k in range(n_samples):\n        for i in range(size_img):\n            for j in range(size_img):\n                if (x[k, size_img*i+j] >= threshold_color):\n                    edge_top.append(i)\n                    break\n            if (len(edge_top) > k):\n                break\n    return edge_top\n# find_top_edge(x_train)","294c1814":"# find the bottom egde\n# Note: the problem is that I don't do the parrallel part\ndef find_bottom_edge(x):\n    edge_bottom = []\n    n_samples = x.shape[0]\n    for k in range(n_samples):\n        for i in range(size_img):\n            for j in range(size_img):\n                if (x[k, size_img*(size_img-1-i)+j] >= threshold_color):\n                    edge_bottom.append(size_img-1-i)\n                    break\n            if (len(edge_bottom) > k):\n                break\n    return edge_bottom\n# find_bottom_edge(x_train)","41897459":"#Note\uff1awhen we do the stretch part by ourselves,there may be some blank cell when the scale factor is more than 2\n\nfrom skimage import transform\ndef stretch_image(x):\n    #get edges\n    edge_left = find_left_edge(x)\n    edge_right = find_right_edge(x)\n    edge_top = find_top_edge(x)\n    edge_bottom = find_bottom_edge(x)\n    \n    #cropping and resize\n    n_samples = x.shape[0]\n    x = x.reshape(n_samples, size_img, size_img)\n    for i in range(n_samples):      \n        x[i] = transform.resize(x[i][edge_top[i]:edge_bottom[i]+1, edge_left[i]:edge_right[i]+1], (size_img, size_img))\n    x = x.reshape(n_samples, size_img ** 2)\n    show_img(x)\n# show_img(x_train)","9696035c":"from sklearn.feature_selection import VarianceThreshold\ndef get_threshold(x_train, x_test):\n    selector = VarianceThreshold(threshold=0).fit(x_train)\n    x_train = selector.transform(x_train)\n    x_test = selector.transform(x_test)\n    print(x_train.shape)\n    return x_train, x_test","1cb2f8b9":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\n#\u9009\u62e9K\u4e2a\u6700\u597d\u7684\u7279\u5f81\uff0c\u8fd4\u56de\u9009\u62e9\u7279\u5f81\u540e\u7684\u6570\u636e\n# selector = SelectKBest(chi2, k=500).fit(x_train, y_train)\n# x_train = selector.transform(x_train)\n# x_test = selector.transform(x_test)\n# print(x_train.shape)","c8a80d06":"from sklearn.decomposition import PCA\ndef get_pca(x_train, x_test):\n    pca = PCA(n_components=0.95)\n    pca.fit(x_train)\n    x_train = pca.transform(x_train)\n    x_test = pca.transform(x_test)\n    print(x_train.shape, x_test.shape)\n    return x_train, x_test","01f93ea9":"# do the pre-process part\n\nx_train = int2float_grey(x_train)\nx_test = int2float_grey(x_test)\n# stretch_image(x_train)\n# stretch_image(x_test)\n# x_train, x_test = get_threshold(x_train, x_test)\n# x_train, x_test = get_pca(x_train, x_test)","e393401e":"def general_function(mod_name, model_name):\n    y_pred = model_train_predict(mod_name, model_name)\n    output_prediction(y_pred, model_name)","409306e3":"from sklearn.model_selection import cross_val_score\ndef model_train_predict(mod_name, model_name):\n    import_mod = __import__(mod_name, fromlist = str(True))\n    if hasattr(import_mod, model_name):\n         f = getattr(import_mod, model_name)\n    else:\n        print(\"404\")\n        return []\n    clf = f()\n    clf.fit(x_train, y_train)\n    y_pred = clf.predict(x_train)\n    get_acc(y_pred, y_train)\n    scores = cross_val_score(clf, x_train, y_train, cv=5)\n    print(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n    y_pred = clf.predict(x_test)\n    return y_pred","e2dcaf36":"def get_acc(y_pred, y_train):\n    right_num = (y_train == y_pred).sum()\n    print(\"acc: \", right_num\/n_samples_train)","bf413cca":"def output_prediction(y_pred, model_name):\n    print(y_pred)\n    data_predict = {\"ImageId\":range(1, n_samples_test+1), \"Label\":y_pred}\n    data_predict = pd.DataFrame(data_predict)\n    data_predict.to_csv(\"dr output %s.csv\" %model_name, index = False)","15971aca":"# from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\nmod_name = \"sklearn.naive_bayes\"\nmodel_name = \"GaussianNB\"\n# model_name = \"MultinomialNB\"\n# general_function(mod_name, model_name)","28992fbf":"# from sklearn.neighbors import KNeighborsClassifier\nmod_name = \"sklearn.neighbors\"\nmodel_name = \"KNeighborsClassifier\"\n# general_function(mod_name, model_name)","0e7fc8f0":"# from sklearn.cluster import KMeans\nmod_name = \"sklearn.cluster\"\nmodel_name = \"KMeans\"\n# general_function(mod_name, model_name)","e68dd4ea":"# from sklearn.svm import SVC\nmod_name = \"sklearn.svm\"\nmodel_name = \"SVC\"\n# general_function(mod_name, model_name)","ed8779fd":"from sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nmod_name = \"sklearn.tree\"\nmodel_name = \"DecisionTreeClassifier\"\n# general_function(mod_name, model_name)","51cc489a":"# from sklearn.ensemble import RandomForestClassifier\nmod_name = \"sklearn.ensemble\"\nmodel_name = \"RandomForestClassifier\"\n# general_function(mod_name, model_name)","6d0fe025":"import tensorflow as tf\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Reshape, Conv2D, AveragePooling2D, Flatten\nfrom keras.layers import MaxPooling2D\nfrom keras.optimizers import adam\n\ny_train = keras.utils.to_categorical(y_train, num_classes=10)\ny_all_pred = np.zeros((3, n_samples_test)).astype(np.int64)\nprint(y_all_pred.dtype)","700be7d8":"model_name = \"LaNet5\"\nmodel = Sequential()\nmodel.add(Reshape(target_shape=(1, 28, 28), input_shape=(784,)))\nmodel.add(Conv2D(kernel_size=(3, 3), filters=6, padding=\"same\", data_format=\"channels_first\", kernel_initializer=\"uniform\", use_bias=False))\nmodel.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\"))\nmodel.add(Conv2D(kernel_size=(5, 5), filters=16, padding=\"same\", data_format=\"channels_first\", kernel_initializer=\"uniform\", use_bias=False))\nmodel.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\"))\nmodel.add(Conv2D(kernel_size=(5, 5), filters=120, padding=\"same\", data_format=\"channels_first\", kernel_initializer=\"uniform\", use_bias=False))\nmodel.add(Flatten())\nmodel.add(Dense(output_dim=120, activation='relu'))\nmodel.add(Dense(output_dim=120, activation='relu'))\nmodel.add(Dense(output_dim=10, activation='softmax'))\n\nadam = keras.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\nmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n\nmodel.fit(x_train, y_train, epochs=30, batch_size=64)\ny_pred = model.predict_classes(x_test)\noutput_prediction(y_pred, model_name)\ny_all_pred[0] = y_pred","b2719f78":"model_name = \"CNN\"\nmodel = Sequential()\n\nmodel.add(Reshape(target_shape=(1, 28, 28), input_shape=(784,)))\nmodel.add(Conv2D(kernel_size=(3, 3), filters=32, padding=\"same\", data_format=\"channels_first\", kernel_initializer=\"uniform\", use_bias=False))\nmodel.add(AveragePooling2D(pool_size=(2, 2), data_format=\"channels_first\"))\nmodel.add(Conv2D(kernel_size=(3, 3), filters=32, padding=\"same\", data_format=\"channels_first\", kernel_initializer=\"uniform\", use_bias=False))\nmodel.add(AveragePooling2D(pool_size=(2, 2), data_format=\"channels_first\"))\nmodel.add(Conv2D(kernel_size=(3, 3), filters=64, padding=\"same\", data_format=\"channels_first\", kernel_initializer=\"uniform\", use_bias=False))\nmodel.add(AveragePooling2D(pool_size=(2, 2), data_format=\"channels_first\"))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(output_dim=256, activation='relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(output_dim=256, activation='relu'))\nmodel.add(Dense(output_dim=10, activation='softmax'))\n\nadam = keras.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\nmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n\nmodel.fit(x_train, y_train, epochs=100, batch_size=128)\ny_pred = model.predict_classes(x_test)\noutput_prediction(y_pred, model_name)\n\ny_all_pred[1] = y_pred","0e357e09":"model_name = \"ComplexCNN\"\n\nmodel = Sequential()\nmodel.add(Reshape(target_shape=(1, 28, 28), input_shape=(784,)))\nmodel.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\nmodel.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\"))\nmodel.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\nmodel.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\"))\nmodel.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\nmodel.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\"))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(output_dim=256, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(output_dim=256, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(output_dim=10, activation='softmax'))\n\nadam = keras.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\nmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n\nmodel.fit(x_train, y_train, epochs=80, batch_size=128)\ny_pred = model.predict_classes(x_test)\noutput_prediction(y_pred, model_name)\n\ny_all_pred[2] = y_pred","5702912c":"model_name = \"Ensemble\"\nprint(y_pred.shape)\ny_ensem_pred = np.zeros((n_samples_test,))\nfor i,line in enumerate(y_all_pred.T):\n    y_ensem_pred[i] = np.argmax(np.bincount(line))\nprint(y_ensem_pred.shape, y_ensem_pred)\ny_ensem_pred = y_ensem_pred.astype(\"int64\")\noutput_prediction(y_ensem_pred, model_name)","a7b2af5e":"# use k-means","abf4b659":"# pre-process","41f00bf5":"### 0-1 format","a467bc22":"# General function","ba62c67d":"# use the decision tree","852f269f":"# use Naive Bayes","9a6c7ef6":"### chi2","60b1d384":"## Complex CNN","836d7615":"# show the image","175a949c":"# use CNN","1a842bde":"# use SVM","41be3087":"### Cropping and Resize","51f9cebb":"## Feature Selection\n### VarianceThreshold","f3e24d20":"### PCA","e11dae89":"## Simple CNN\nAlthough I use some technics that AlexNet uses like dropout, relu function,  the performance reaches a plateau because of the depth.","46e40e3a":"# use Random Forest","8a98c5c1":"## change grey value from int to float","40b16567":"# import data","3f91a37e":"# Model Ensemble","6297c484":"# use KNN","30a32ae5":"## LaNet5\n![LaNet5 architecture](https:\/\/endlesslethe.com\/wordpress\/wp-content\/uploads\/2018\/06\/LaNet5-architecture.jpg)\nLeNet-5 comprises 7 layers, not counting theinput, all of which contain trainable parameters (weights).  \n\nThe input is a 32x32 pixel image. This is significantly larger than the largest character in the database (at most 20x20 pixels centered in a 28x28 field).   \nThe reason is that it is desirable that potential distinctive features such as stroke end-points or corner can appear in the center of the receptive ield of the highest-level feature detectors.\n\ninput layer: 32*32  \ncov layer(3): C1 6x5x5 C3 16x5x5 C5 120x5x5  \npooling layer(2):S2 S4 size is 2x2  \nfull connection layer(2): F6'active function is \"tanh\" and output layer  uses RBF\noutput layer\uff1a10\uff08possibilities of digit 0-9\uff09  \n\nNote: \nHere I use 28x28 pixel image as input, and on the first conv layer I use 6*3*3 fliter.  \nOn the output layer I use softmax fuction to get the predicted labels rather than RBF function, and I use two full connection layer with \"relu\" function instead of one layer with \"tanh\" fumction.\n","033e9290":"# summary\n**Note: There're some posts saying that it's impossible to make acc to 1 beacuse some samples are so hard to recognize for both machines and human beings, and some of these whose model has acc \"1.0\" cheats. So I won't keep improving my model. **\n\nAnd interestingly, my last submission ranks 21th, but after one night it ranks now 20th XD. Happy.\n![dr](https:\/\/endlesslethe.com\/wordpress\/wp-content\/uploads\/2018\/05\/dr.png)\n\n## data processing\nThe cropping and resize technique improve models' performance slightly, and reduce the number of features significantly with pca. Before this, when we choose 90% principal component, there are 87 features. After this, there's only 37 features. So that we can increase the percentage of principal component to 95%, which stage has 58 features.\n\nPCA works well with GaussianNB. It reduce the number of features significantly. However, Its disadvantage is that we have to use GaussianNB because of the meaningless position in an new vector space.\n\nAnd chi2 doesn't improve this model because the label has the same importance. It's easily to find out that the more points we take into account, the more accurate our model will be.\n\n## model\nNaive Bayes is so simple, but it shows a decent result.\nDecision tree works so well on the train set, but on the k-cross-hold test set it works even worse than Naive Bayes.\nKNN, SVM and RF all give a good answer whose accuracy is more than 0.95.\nAnd RF performs so well based on decision tree with vote strategy.\n\nCNN is of course the best way to solve this problem.\n\n# result\n## Naive Bayes\nWhen I use **MultinomialNB only**, the acc on the train set is **0.8261190476190476**, and on the test set is **0.83114**, ranks 2180th.  \nWhen I use **BernoulliNB only**, the acc on the train set is **0.8347857142857142**.    \nWhen i use **GaussianNB only**, the acc on the train set is  **0.5571904761904762**.  \nWhen i use **GaussianNB with pca**, the acc on the train set is  **0.8739285714285714**, and on the test set is **0.87385**, ranks 2136th.\n\n## KNN\nWhen I use **KNN with PCA**, the acc on the train set is ** 0.982452380952381**, and on the test set is **0.97371**, ranks 1400th. \n\n**Note: On the dataset of the following models, we use pca and the cropping and resize technique.**\n\n## SVM\nBased on the basic dataset, the acc on the train set is ** 0.9966190476190476**, and on the test set is **0.98585**, ranks 1039th.  \nAnd based on the bigger dataset, the acc on the train set is ** 0.9966190476190476**, and on the test set is **0.99614**, ranks 214th.\n\n## the decision tree\nthe acc on the train set is **1.0**, but on the test set is only **0.83857**\n\n## RF\nBased the basic dataset, the acc on the train set is ** 0.998**, and on k-cross-hold test set, the acc is **0.98 +- 0.02**.  \nAnd based on the bigger dataset, the acc on the test set is **0.99914**, ranks 36th.\n\n## CNN\nthe acc on the train set is **1**, and on the test set is **0.99942**, ranks 21th.\n\n# screen shoots\n## knn\n![dr knn](https:\/\/endlesslethe.com\/wordpress\/wp-content\/uploads\/2018\/05\/dr-knn.png)\n\n## svm\nuse the basic dataset:\n![dr svm](https:\/\/endlesslethe.com\/wordpress\/wp-content\/uploads\/2018\/05\/dr-svm.png)\nuse the bigger dataset:\n![dr svm 2](https:\/\/endlesslethe.com\/wordpress\/wp-content\/uploads\/2018\/05\/dr-svm-2.png)\n\n## decision tree\n![dr dt](https:\/\/endlesslethe.com\/wordpress\/wp-content\/uploads\/2018\/05\/dr-dt.png)\n\n## rf\nuse the bigger dataset:\n![dr rf](https:\/\/endlesslethe.com\/wordpress\/wp-content\/uploads\/2018\/05\/dr-rf.png)\n\n## cnn\n![dr cnn](https:\/\/endlesslethe.com\/wordpress\/wp-content\/uploads\/2018\/05\/dr-cnn.png)\n"}}