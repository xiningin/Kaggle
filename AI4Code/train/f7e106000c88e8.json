{"cell_type":{"dbb7b96f":"code","d3d2039d":"code","364e0a82":"code","de5f1b7d":"code","1fe76710":"code","19e0b31d":"code","55be07fa":"code","3f01660e":"code","1a84ed7e":"code","8b279e66":"markdown","bd9d19bc":"markdown","1bd144b2":"markdown","1a1f5656":"markdown","2488582d":"markdown","0ad72aa2":"markdown","a94a0436":"markdown"},"source":{"dbb7b96f":"import pandas as pd # verinin organizasyonu i\u00e7in\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_validate\nfrom sklearn import svm\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.cluster import KMeans\n\n#Grafik \u00e7izdirme k\u00fct\u00fcphanesi\nimport matplotlib.pyplot as plt\n\nimport os #Sistem \nimport warnings #uyar\u0131lar\n#print(os.listdir(\"..\/input\/\"))\nwarnings.filterwarnings(\"ignore\")","d3d2039d":"from sklearn import datasets\niris =datasets.load_iris()","364e0a82":"X=iris.data\nY=iris.target","de5f1b7d":"plt.scatter(X[:,0], X[:,1]);\n","1fe76710":"plt.scatter(X[:,0], X[:,1], c=Y, cmap='gist_rainbow')\nplt.xlabel('Spea1 Length', fontsize=18)\nplt.ylabel('Sepal Width', fontsize=18)","19e0b31d":"x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.3,random_state=1)\n","55be07fa":"def computing(cm):\n    Eba = cm[1,0]\n    Eca = cm[2,0]\n    Eab = cm[0,1]\n    Ecb = cm[2,1]\n    Eac = cm[0,2]\n    Ebc = cm[1,2]\n    TPa = cm[0,0]\n    TPb = cm[1,1]\n    TPc = cm[2,2]\n    FNa = Eba+Eca\n    FNb = Eab+Ecb\n    FNc = Eac+Ebc\n    FPa = Eab+Eac\n    FPb = Eba+Ebc\n    FPc = Eca+Ecb\n    TNa = TPb+Ebc+Ecb+TPc\n    TNb = TPa+Eac+Eca+TPc\n    TNc = TPa+Eab+Eba+TPb\n    Total = TPa+Eab+Eac+Eba+TPb+Ebc+Eca+Ecb+TPc\n    accuracy = (TPa+TPb+TPc)\/Total\n    sensitivityA = (TPa)\/(TPa+FNa)\n    sensitivityB = (TPb)\/(TPb+FNb)\n    sensitivityC = (TPc)\/(TPc+FNc)\n    specificityA = (TNa)\/(TNa+FPa)\n    specificityB = (TNb)\/(TNb+FPb)\n    specificityC = (TNc)\/(TNc+FPc)\n    print(\"accuracy: \",accuracy)\n    print(\"sensitivityA: \",sensitivityA)\n    print(\"sensitivityB: \",sensitivityB)\n    print(\"sensitivityC: \",sensitivityC)\n    print(\"specificityA: \",specificityA)\n    print(\"specificityB: \",specificityB)\n    print(\"specificityC: \",specificityC)\n    \n    matrisim=[[\"accuracy: \",accuracy],[\"sensitivityA: \",sensitivityA],\n          [\"sensitivityB: \",sensitivityB],[\"sensitivityC: \",sensitivityC],\n          [\"specificityA: \",specificityA],[\"specificityB: \",specificityB],\n          [\"specificityC: \",specificityC]\n          ]\n    return matrisim\n\ndef classifier(model,name,n1,n2,n3):\n    print(\"-------------\")\n    print(\"model name: \",str(name))\n    print(\"-------------\")\n    fig=plt.gcf()\n    fig.set_size_inches(10,5)\n    plt.subplot(n1,n2,n3)\n    plt.title('train')\n    model.fit(x_train,y_train)\n    y_pred0=cross_val_predict(model,x_train,y_train,cv=10)\n    cm=confusion_matrix(y_train,y_pred0)\n    sns.heatmap(cm,annot=True,fmt=\"d\")\n    print(\"accuracy_score\")\n    print(metrics.accuracy_score(y_train, y_pred0))\n    print(\"sensitivity\")\n    print(metrics.recall_score(y_train, y_pred0, average='macro'))\n    print(\"precision\")\n    print(metrics.precision_score(y_train, y_pred0, average='macro'))\n\n    \n    plt.subplot(n1,n2,n3+1)\n    plt.title('test')\n    model.fit(x_test,y_test)\n    y_pred00=cross_val_predict(model,x_test,y_test,cv=10)\n    cm2=confusion_matrix(y_test,y_pred00)\n    sns.heatmap(cm2,annot=True,fmt=\"d\")\n\n    plt.subplot(n1,n2,n3+2)\n    plt.title('validation all')\n    model.fit(X,Y)\n    y_pred2=cross_val_predict(model,X,Y,cv=10)\n    conf_mat2=confusion_matrix(Y,y_pred2)\n    sns.heatmap(conf_mat2,annot=True,fmt=\"d\")\n    # plt.show()\n\n#    a='iris'+str(name)+'.png'\n#    fig.savefig(a,dpi=100)\n\n\n    cv1 = cross_validate(model, x_train, y_train, cv=10)\n    cv2 = cross_validate(model, x_test, y_test, cv=10)\n    cv3 = cross_validate(model, X, Y, cv=10)\n\n    print('train '+str(name)+'accuracy is: ',cv1['test_score'].mean())\n    print('test '+str(name)+' accuracy is: ',cv2['test_score'].mean())\n    print('validation all'+str(name)+'accuracy is: ',cv3['test_score'].mean())\n    print('')\n    \n    matris1 = computing(cm)\n    matris2 = computing(cm2)\n    matris3 = computing(conf_mat2)\n    \n    return matris1,matris2,matris3","3f01660e":"log_class=LogisticRegression(penalty = 'l2', C = 10,random_state = 0)\n","1a84ed7e":"matris111,matris222,matris333=classifier(log_class,\"logisticReg\",4,3,7)","8b279e66":"Model de\u011ferlendirme S\u0131n\u0131fland\u0131r\u0131c\u0131n\u0131n veya modelin \u00e7e\u015fitlerin t\u00fcr\u00fcn\u00fc ne kadar do\u011fru tahmin edebilece\u011fini tahmin edelim.\n\nDo\u011fruluk, ger\u00e7ek test seti de\u011ferleri ve \u00f6ng\u00f6r\u00fclen de\u011ferler kar\u015f\u0131la\u015ft\u0131r\u0131larak hesaplanabilir.","bd9d19bc":"Verileri y\u00fckleme\n","1bd144b2":"Kullanaca\u011f\u0131m\u0131z hesaplama ve training fonksiyonlar\u0131n\u0131 olu\u015ftural\u0131m\n\n","1a1f5656":"\u00d6znitelik Se\u00e7imi Burada, verilen s\u00fctunlar\u0131 ba\u011f\u0131ml\u0131 (veya hedef de\u011fi\u015fken) ve ba\u011f\u0131ms\u0131z de\u011fi\u015fken (veya \u00f6zellik de\u011fi\u015fkenleri) olmak \u00fczere iki t\u00fcr de\u011fi\u015fkene b\u00f6lmeniz gerekir.\n\n","2488582d":"Logistic Regression Modeli Olu\u015fturma Scikit-learn kullanarak h\u0131zl\u0131ca olu\u015ftural\u0131m.","0ad72aa2":"**Logistic Regression CLASSIFIER\n**\n\nGerekli K\u00fct\u00fcphaneleri \u0130\u00e7e Aktarma \u0130lk \u00f6nce gerekli k\u00fct\u00fcphaneleri y\u00fckleyelim.\n\n","a94a0436":"Veri B\u00f6lme Model performans\u0131n\u0131 anlamak i\u00e7in, veri setini bir e\u011fitim setine ve bir test setine b\u00f6lmek iyi bir stratejidir.\n\n"}}