{"cell_type":{"a060e428":"code","bf153c20":"code","3c6cc281":"code","ea3a5c4c":"code","3503161a":"code","482bbd6d":"code","214eeca3":"code","bd5dc962":"code","4438f7cc":"code","251f4459":"code","2106531f":"code","5eabffa6":"code","6e667274":"code","1b2ca52b":"code","ab7df470":"code","b079b419":"code","a58d3c86":"code","d58c8606":"code","4c31c5a5":"code","472b01fe":"code","d3902a0e":"code","1d088582":"code","064c77d4":"code","788fd533":"code","ce494683":"code","cd9ad93e":"code","2e6e054e":"code","d5d14015":"code","347a01f4":"code","600696e7":"code","fa82424b":"code","42834534":"code","2373e8e7":"code","0e51e07f":"code","fbdda9f8":"code","23812b56":"markdown","7afe91d9":"markdown","f01eca3e":"markdown","ae2c171c":"markdown","46cfb8d5":"markdown","c8c99690":"markdown","9eb00785":"markdown","5695918b":"markdown","5a099c6a":"markdown","deec216a":"markdown","86f42593":"markdown","25b08712":"markdown","210039d4":"markdown","5116c2e1":"markdown","106666ed":"markdown","ee4871e5":"markdown","1ec3631c":"markdown"},"source":{"a060e428":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","bf153c20":"import seaborn as sns\nimport matplotlib.pyplot as plt","3c6cc281":"# checking xgboost version available for the kernel\n\n# import xgboost\n# xgboost.__version__","ea3a5c4c":"from xgboost import XGBRegressor\nfrom xgboost import plot_importance","3503161a":"from sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold","482bbd6d":"train_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv', \n                         index_col=0)\ntrain_data","214eeca3":"train_cols = train_data.columns.to_list()\ntrain_cols","bd5dc962":"# setting prediction target\n\ntgt_col = train_cols[-1]\nprint('Prediction target name: {}'.format(tgt_col))","4438f7cc":"train_data.info()","251f4459":"train_data.describe()","2106531f":"# checking features with most missing values (\"train\" dataset)\n\ntrain_data.isna().sum().sort_values(ascending=False)","5eabffa6":"test_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv', \n                        index_col=0)\ntest_data","6e667274":"test_cols = test_data.columns.to_list()\ntest_cols","1b2ca52b":"test_data.info()","ab7df470":"test_data.describe()","b079b419":"# checking features with most missing values (\"test\" dataset)\n\ntest_data.isna().sum().sort_values(ascending=False)","a58d3c86":"def inspect_feature(feature):\n    \n    print(feature.describe())\n    \n    # categorical features - checking for unique values and their counts\n    if feature.dtype == object:\n        print('\\nCategorical feature - getting unique values:')\n        print(feature.unique())\n        \n        print('\\nUnique value counts:')\n        print(feature.value_counts())        \n        sns.countplot(feature, palette='Blues_d')\n        sns.utils.plt.show()\n    \n    # numerical features - checking distribution\n    else:\n        print('\\nNumerical feature - showing distplot:')\n        sns.distplot(feature.dropna())\n        sns.utils.plt.show()","d58c8606":"for col in train_cols:\n    feature = train_data[col]\n    print('\\nExploratory analysis for feature: {}'.format(col))\n    inspect_feature(feature)","4c31c5a5":"for col in train_cols[:-1]:\n    \n    print('\\nChecking correlation between {} and {}'.format(tgt_col, col))\n    \n    if train_data[col].dtype != object:\n        sns.pairplot(train_data[[col, tgt_col]])\n        sns.utils.plt.show()\n        \n    else:\n        print('Categorical feature - cannot generate pairplot')","472b01fe":"for col in test_cols:\n    feature = test_data[col]\n    print('\\nExploratory analysis for feature: {}'.format(col))\n    inspect_feature(feature)","d3902a0e":"sample_submission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv', index_col='Id')\nsample_submission","1d088582":"# setting the value to replace all missing feature values\n\nmissing = -999","064c77d4":"# preparing train data by dropping target variable and filling missing values\n\nX_train = train_data.drop(columns=[tgt_col])\nX_train = X_train.fillna(missing)\nX_train.shape","788fd533":"# preparing prediction target\ny_train = train_data[tgt_col]\n\n# checking if there are no missing target values\ny_train.isna().sum() == 0","ce494683":"# preparing test data by filling missing values\n\nX_test = test_data.copy()\nX_test = X_test.fillna(missing)\nX_test.shape","cd9ad93e":"# checking if number of columns match for train and test data\n\nprint(X_train.shape[1] == X_test.shape[1])","2e6e054e":"# deleting dataset no longer necessary\n\ndel train_data, test_data","d5d14015":"# Label Encoding for train&test explanatory variables\n\nfor col in test_cols:\n    if X_train[col].dtype=='object' or X_test[col].dtype=='object': \n        le = LabelEncoder()\n        le.fit(list(X_train[col].values) + list(X_test[col].values))\n        X_train[col] = le.transform(list(X_train[col].values))\n        X_test[col] = le.transform(list(X_test[col].values))","347a01f4":"\"\"\"helper function to show feature importances for a model provided\"\"\"\n\ndef show_feat_importances(model):\n    \n    # getting feature importances\n    imp_type = model.importance_type\n    feat_imp = pd.DataFrame(model.feature_importances_, \n                            columns = ['importance'], index=test_cols)\n    feat_imp.sort_values(by='importance', axis=0, ascending=False, inplace=True)\n    \n    print('\\nFeature importances (type: {}) for this model: '\n          .format(imp_type))    \n    print(feat_imp)\n    \n    # plotting feature importances    \n    plot = plot_importance(model, importance_type='gain', \n                           title='Feature importance by gain')\n    plot.figure.set_size_inches(12,16)\n    plt.show()\n\n    plot = plot_importance(model, importance_type='weight', \n                           title='Feature importance by weight')\n    plot.figure.set_size_inches(12,16)\n    plt.show()\n    \n    return feat_imp","600696e7":"\"\"\"helper function to perform cross-validation for model, features and target provided\n    using sklearn's cross_val_score function with Kfold cross-validator\"\"\"\n\n\ndef check_cv_scores(model, X, y):\n    \n    cv=KFold(n_splits=3, shuffle=False, random_state=42)\n    # scoring = 'neg_mean_squared_error'\n    scoring = 'neg_mean_squared_log_error'\n    \n    print('\\nPerforming cross-validation using the following cross-validator:')\n    print(cv)\n    \n    scores = cross_val_score(estimator=model, \n                             X=X, y=y, \n                             scoring=scoring, cv=cv, \n                             n_jobs=-1, verbose=True)\n    \n    print('{} scores for this model:'.format(scoring))\n    print(scores)\n    \n    return scores","fa82424b":"# maximum tree depth\nmax_depth = 9\n\n# number of trees to fit\nn_estimators = 1000\n\n# setting the learning task and the corresponding learning objective\n\"\"\"NOTE: accoring to evaluation metric: https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/overview\/evaluation,\nobjective = 'reg:squaredlogerror' should be used, but it is not supported in xgboost v. 0.90 - \nwe get: XGBoostError: Unknown objective function reg:squaredlogerror\nTherefore we use:\"\"\"\nobjective = 'reg:squarederror'\n\n# setting the feature importance type for the feature_importances_ property\nimportance_type='gain'","42834534":"model_XGBR = XGBRegressor(booster='gbtree', \n                          importance_type=importance_type,\n                          max_depth=max_depth, \n                          missing=missing, \n                          n_estimators=n_estimators, \n                          n_jobs=-1, \n                          objective=objective,\n                          random_state=42, \n                          verbosity=1\n                         )\nmodel_XGBR","2373e8e7":"check_cv_scores(model=model_XGBR, X=X_train, y=y_train)","0e51e07f":"# fitting model\nmodel_XGBR.fit(X_train, y_train)\n\n# making predictions for train dataset (i.e. in-sample check)\ny_pred_train = model_XGBR.predict(X_train)\n\n# calculating error metrics for in-sample check\nMSE = mean_squared_error(y_true=y_train, y_pred=y_pred_train)\nMSLE = mean_squared_log_error(y_true=y_train, y_pred=y_pred_train)\nprint('\\nError metrics on full train data:')\nprint('MSE: {}'.format(MSE))\nprint('MSLE: {}'.format(MSLE))\n\n# showing feature importances\nshow_feat_importances(model=model_XGBR)\n    \n# making predictions on test data\ny_pred_test = model_XGBR.predict(X_test)\nprint('\\nGenerated {} test predictions'.format(y_pred_test.shape[0]))    \n    \nsample_submission['SalePrice'] = y_pred_test","fbdda9f8":"sample_submission.to_csv('submission.csv')","23812b56":"##### Note: this notebook is still under development","7afe91d9":"### Data loading & inspection","f01eca3e":"#### TRAIN data: explanatory features distributions","ae2c171c":"#### Cross-validation","46cfb8d5":"### EDA","c8c99690":"##### helper functions","9eb00785":"Some ideas (especially data preparation and model hyperparameters) as well as substantial part of code in this part of the notebook borrowed from the following source: \n\nhttps:\/\/www.kaggle.com\/inversion\/ieee-simple-xgboost\n\nSincere thanks @inversion for sharing.\n","5695918b":"### Setup & Imports","5a099c6a":"#### TRAIN data: inspecting Features vs. Target variable correlations","deec216a":"##### Helper functions","86f42593":"#### Preparing data for XGBoost","25b08712":"#### \"train.csv\"","210039d4":"#### \"test.csv\"","5116c2e1":"### Modelling","106666ed":"#### Training model on full data","ee4871e5":"#### TEST data: explanatory features distributions","1ec3631c":"#### Setting model parameters"}}