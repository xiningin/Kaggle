{"cell_type":{"1c49bcf0":"code","a59659d0":"code","2d445535":"code","f74e9c4f":"code","4fc32ebe":"code","1f3b87c4":"code","c49adc5b":"code","11a1b17c":"code","0f41e504":"code","36fd05a5":"markdown","619233c7":"markdown","dcee9554":"markdown","ab5af85c":"markdown","8e2441be":"markdown","fbe4aa9a":"markdown","1210484f":"markdown","a190644e":"markdown","085a05a4":"markdown","d36e26d5":"markdown","b1183b2e":"markdown","26c930db":"markdown","d7dd607f":"markdown","43cd700d":"markdown","403b7796":"markdown","1e6f1397":"markdown"},"source":{"1c49bcf0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n# visualisation\nimport seaborn as sns\n\n\n#Transformer\nimport torch\nfrom transformers import LongformerModel, LongformerTokenizer\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a59659d0":"news=pd.read_csv('\/kaggle\/input\/news_articles.csv')# reading the file","2d445535":"print(news.columns)\nprint('\\n')\nprint(news.dtypes)\nnews.head()","f74e9c4f":"news['len_content']=news['Content'].apply(lambda x : len(x))","4fc32ebe":"sns.distplot(news['len_content'])","1f3b87c4":"model = LongformerModel.from_pretrained('allenai\/longformer-base-4096',output_hidden_states = True)\ntokenizer = LongformerTokenizer.from_pretrained('allenai\/longformer-base-4096')\n\n# Put the model in \"evaluation\" mode, meaning feed-forward operation.\nmodel.eval()\n","c49adc5b":"all_content=list(news['Content'][:5])\n#doing only for first 5 contents\n# because doing for complete takes 2 hours on CPU\n\ndef sentence_bert():\n    list_of_emb=[]\n    for i in range(len(all_content)):\n        SAMPLE_TEXT = all_content[i]  # long input document\n        input_ids = torch.tensor(tokenizer.encode(SAMPLE_TEXT)).unsqueeze(0)  # batch of size 1\n\n        # Attention mask values -- 0: no attention, 1: local attention, 2: global attention\n        attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=input_ids.device) # initialize to local attention\n        attention_mask[:, [0,-1]] = 2  # Set global attention based on the task. For example,\n                                            # classification: the <s> token\n                                            # QA: question tokens\n                                            # LM: potentially on the beginning of sentences and paragraphs\n\n        # we have set <s> and <\/s> token's attention mask =2\n        # because acc to Longformer documentation these tokens must be given \n        # global attention when we are doing sentence classification\n        # Run the text through BERT, and collect all of the hidden states produced\n\n\n        # from all 12 layers. \n        with torch.no_grad():\n\n            outputs = model(input_ids, attention_mask=attention_mask)\n\n            # Evaluating the model will return a different number of objects based on \n            # how it's  configured in the `from_pretrained` call earlier. In this case, \n            # becase we set `output_hidden_states = True`, the third item will be the \n            # hidden states from all layers. See the documentation for more details:\n            # https:\/\/huggingface.co\/transformers\/\n            hidden_states = outputs[2]\n            \n            #outputs[0] gives us sequence_output\n            #outputs[1] gives us pooled_output\n            #outputs[2] gives us Hidden_output\n            \n\n\n        # Concatenate the tensors for all layers. We use `stack` here to\n        # create a new dimension in the tensor.\n        token_embeddings = torch.stack(hidden_states, dim=0)\n\n        # Remove dimension 1, the \"batches\".\n        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n        # Swap dimensions 0 and 1.\n        token_embeddings = token_embeddings.permute(1,0,2)\n\n\n        token_vecs_sum = []\n\n\n\n        # For each token in the sentence...\n        for token in token_embeddings:\n\n            \n\n\n            #but preferrable is\n            sum_vec=torch.sum(token[-4:],dim=0)\n\n            # Use `sum_vec` to represent `token`.\n            token_vecs_sum.append(sum_vec)\n\n#         print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))\n\n        h=0\n        for i in  range(len(token_vecs_sum)):\n            h+=token_vecs_sum[i]\n            \n            \n        list_of_emb.append(h)\n    return list_of_emb\nf=sentence_bert()\nprint(len(f))\n    ","11a1b17c":"f[0].shape  #  embedding  for first sentence","0f41e504":"import pickle\nwith open(\"\/kaggle\/working\/emeddings_bert_mean.txt\", \"wb\") as fp:\n    pickle.dump(f, fp)","36fd05a5":"### Some Data Inspection :)","619233c7":"# <font color='purple'>Hugging Face<\/font> comes to Rescue\n![](https:\/\/telegra.ph\/file\/f06787c5d492d528d342c.jpg)","dcee9554":"## One Question you might face?\n\n### What are sequence and pooled outputs?\n\nAnswer is [here](https:\/\/www.kaggle.com\/questions-and-answers\/86510)","ab5af85c":"# Before Diving Into Code\n\n\n## Now what is  2?\nLong-former supports the concept of both local and Global attention\nAnd  to give any token Local attention assign it value **1**\nTo give any token Global  Attention assign it value **2**\nand \nEven if we do not want to give any attention assign it value **0**\n\n##  <font color='purple'>HUGGING FACE<\/font> SAYS\n\n> Longformer self attention employs self attention on both a \u201clocal\u201d context and a \u201cglobal\u201d context. Most tokens only attend \u201clocally\u201d to each other meaning that each token attends to its 12w previous tokens and 12w succeding tokens with w being the window length as defined in config.attention_window. Note that config.attention_window can be of type list to define a different w for each layer. A selecetd few tokens attend \u201cglobally\u201d to all other tokens, as it is conventionally done for all tokens in e.g. BertSelfAttention.\n> \n> Note that \u201clocally\u201d and \u201cglobally\u201d attending tokens are projected by different query, key and value matrices. Also note that every \u201clocally\u201d attending token not only attends to tokens within its window w, but also to all \u201cglobally\u201d attending tokens so that global attention is symmetric.\n> \n> The user can define which tokens attend \u201clocally\u201d and which tokens attend \u201cglobally\u201d by setting the tensor global_attention_mask at run-time appropriately. Longformer employs the following logic for global_attention_mask: 0 - the token attends \u201clocally\u201d, 1 - token attends \u201cglobally\u201d. For more information please also refer to forward() method.\n> \n> Using Longformer self attention, the memory and time complexity of the query-key matmul operation, which usually represents the memory and time bottleneck, can be reduced from O(ns\u00d7ns) to O(ns\u00d7w), with ns being the sequence length and w being the average window size. It is assumed that the number of \u201cglobally\u201d attending tokens is insignificant as compared to the number of \u201clocally\u201d attending tokens.","8e2441be":"## Necessary Imports\n","fbe4aa9a":"### Refrencing The Jay Alammar\nHe said to get Best Contextualised word  embeddings from these Attention Based Models \nWe Will Sum the Outputs of <font color='blue'>Last 4 Hidden layers<\/font> of BERT model.\n\n![](http:\/\/jalammar.github.io\/images\/bert-feature-extraction-contextualized-embeddings.png)\n\nThis point lead me to the conclusion if we can get Best Word Embeddings from last 4 layers\nSo Our Whole content is basically Summation of Words So we will get Best Content Embeddings\nIf we <font color='red'>Element Wise Sum Word Embeddings of Each word<\/font> in the Content.","1210484f":"### Saving the Embeddings","a190644e":"### Some Very Important Refrences\n1. [Hugging Face Implementation](https:\/\/huggingface.co\/transformers\/model_doc\/longformer.html)\n2. [Original Paper](https:\/\/arxiv.org\/pdf\/2004.05150.pdf?forcedefault=true)\n3. [Blog By Chris Mccormick](https:\/\/mccormickml.com\/2019\/05\/14\/BERT-word-embeddings-tutorial\/)\n4. [Jay Alammar](https:\/\/jalammar.github.io\/illustrated-bert\/)","085a05a4":"<font color='red'> If you liked my work and teams work pls  consider an  upvote<\/font>","d36e26d5":"# <font color='orange'>LONG-FORMER:The Long Document Transformer<\/font>","b1183b2e":"# <font color='orange'>OUR TASK<\/font>\nWe have used Long-Former for creating Content Embeddings which will be used In Clustering.\nAnd further those clusters are used to Handle\n1. <font color='red'>User-cold start<\/font> problem = This is the situation when any new user enters into our news recommendation platform\n2. <font color='red'>Item-cold start<\/font>  problem = This is the situation when any new item( in our case news articles) enters into our platform.\n\nWe have explained our approach clearly in our report","26c930db":"## Summing the results from last 4 layers of Model\n## to get word embedding","d7dd607f":"### SOME MORE THEORY  :-)\n![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcR3I2YkJ3IOBvwdZsPuBXUJQaajl2X9oGMaxO1SGh1V3DXBbdNP&usqp=CAU)","43cd700d":"## Very Important Point\n> Above Plot clearly shows if we had used BERT or some other models we would have lot of information loss because most of our content lies above length 500","403b7796":"## WHY NOT BERT or Other BERT BASED MODELS\n\n[HUGGING FACE SAYS](https:\/\/huggingface.co\/transformers\/model_doc\/longformer.html)\n> Transformer-based models are unable to process long sequences due to their <font color='red'>self-attention<\/font> operation, which scales quadratically with the sequence length. To address this limitation, we introduce the <font color='orange'>Longformer<\/font> with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer\u2019s attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention.\n> \nIn simple words there is <font color='red'>limitation of 512 tokens<\/font> input for BERT,Roberta,Distillbert or Albert.\nBUT \nThere is no such cap in LONG-FORMER","1e6f1397":"For quick execution I have created emeddings just for first 5 articles\n\nTo get embeddings replace first line of below cell\n\n> all_content=list(news['Content'][:])"}}