{"cell_type":{"1fb4a89a":"code","f0ac3627":"code","26adb583":"code","4c26a920":"code","0e14f913":"code","47e9728d":"code","6c6b36f6":"code","185793cc":"code","283eae16":"code","8756684d":"code","4837a6df":"code","0fb4e739":"code","f74a35c8":"code","1807038d":"code","bd614d4f":"code","bcb7bb59":"code","e443e30f":"code","6b196776":"code","f43b7541":"code","758cc521":"code","72c31f34":"code","4f655d51":"code","23df1c15":"code","e98ef860":"markdown","272c6693":"markdown","2ff42b11":"markdown","59810abc":"markdown","65a187a5":"markdown","5fd3d48a":"markdown","57335427":"markdown","7004e57e":"markdown","d8023a84":"markdown","1f88db23":"markdown","6b3449b0":"markdown","e08d5f35":"markdown","6d874941":"markdown","3de1238c":"markdown","3a0acb54":"markdown","106896cc":"markdown","9b36e936":"markdown","09bdd160":"markdown","34f81ad6":"markdown","e0c8ab7f":"markdown"},"source":{"1fb4a89a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\nfrom sklearn.model_selection import RandomizedSearchCV,train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.linear_model import LogisticRegression\n\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f0ac3627":"train_dataset = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_dataset =  pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n","26adb583":"train_dataset.head()","4c26a920":"train_dataset.shape","0e14f913":"train_dataset.info()","47e9728d":"train_dataset.describe()","6c6b36f6":"train_dataset.describe(include=['O'])\n","185793cc":"numerical_data=['Age','Fare']","283eae16":"train_dataset[numerical_data].hist(figsize=(10, 5));\n","8756684d":"def plot_hist_with_survived(dataset,quantitave_feature):\n    plot = sns.FacetGrid(dataset, col='Survived');\n    plot.map(plt.hist,quantitave_feature, bins=20)","4837a6df":"plot_hist_with_survived(train_dataset,'Age')","0fb4e739":"plot_hist_with_survived(train_dataset,'Fare')","f74a35c8":"plot_hist_with_survived(train_dataset,'SibSp')","1807038d":"plot_hist_with_survived(train_dataset,'Parch')","bd614d4f":"test_dataset.info()","bcb7bb59":"median_fare = train_dataset['Fare'].median()\ntest_dataset['Fare'].fillna(median_fare,inplace=True)\n\n\nmedian_age_train = train_dataset['Age'].median()\nmedian_age_test = train_dataset['Age'].median()\ntrain_dataset['Age'].fillna(median_age_train,inplace=True)\ntest_dataset['Age'].fillna(median_age_test,inplace=True)\ntrain_dataset.dropna(subset=['Embarked'],inplace=True)","e443e30f":"train_dataset.info()","6b196776":"plot=sns.relplot(x='Age',y='Fare',data=train_dataset,hue='Survived')\nplot.fig.suptitle('Relation between age and fare',y=1.05);","f43b7541":"plot=sns.catplot(x='Sex',data=train_dataset,kind='count',hue='Survived')\nplot.fig.suptitle('Relation between gender and survival chance',y=1.05);","758cc521":"plot=sns.catplot(x='Embarked',data=train_dataset,kind='count',hue='Survived')\nplot.fig.suptitle('Relation between embarked and survival chance',y=1.05);","72c31f34":"# X = pd.get_dummies(train_dataset[features])\n# X.describe()","4f655d51":"# def print_metrics(y_true, preds, model_name=None):\n#     '''\n#     INPUT:\n#     y_true - the y values that are actually true in the dataset (numpy array or pandas series)\n#     preds - the predictions for those values from some model (numpy array or pandas series)\n#     model_name - (str - optional) a name associated with the model if you would like to add it to the print statements \n    \n#     OUTPUT:\n#     None - prints the accuracy, precision, recall, and F1 score\n#     '''\n#     if model_name == None:\n#         print('Accuracy score: ', format(accuracy_score(y_true, preds)))\n#         print('\\n\\n')\n    \n#     else:\n#         print('Accuracy score for ' + model_name + ' :' , format(accuracy_score(y_true, preds)))\n#         print('\\n\\n')","23df1c15":"y = train_dataset['Survived']\n\nfeatures = ['Sex','SibSp','Age','Fare','Embarked','Pclass']\n\nX = pd.get_dummies(train_dataset[features])\n\nX_test = pd.get_dummies(test_dataset[features])\n\n# X_train, X_validate, y_train, y_validate = train_test_split(X, \n#                                                     y, \n#                                                     random_state=42)\n\n\nmodel = GradientBoostingClassifier(random_state=42)\nparam_dist = {\"max_depth\": [4,5,6,7,8,9],\n              \"n_estimators\": [100,200,300,400],\n              'learning_rate': [0.1, 0.05, 0.01,0.001],\n         \n              \"min_samples_split\": list(range(2, 11)),\n              \"min_samples_leaf\": list(range(1, 11)),\n#               \"bootstrap\": [True, False],\n#               \"criterion\": [\"gini\", \"entropy\"]\n             }\n\n\n# Run a randomized search over the hyperparameters\nrandom_search = RandomizedSearchCV(model, param_distributions=param_dist)\n\n# Fit the model on the training data\nrandom_search.fit(X, y)\n\n# Make predictions on the test data\npredictions = random_search.best_estimator_.predict(X_test)\n\n# accuracy_score(y_validate, predictions)\n\n\noutput = pd.DataFrame({'PassengerId': test_dataset.PassengerId, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n\n\n","e98ef860":"**Now we will explore the realtion between survived column and categories features**\n\nStarting with the gender column","272c6693":"we can see that people with s class are the least likely to survive","2ff42b11":"#### Loading the data:\n\nIn this section we will load 2 datasets,\n- Training data: Which is the data of passengers that we know survived or not, We will use it to train our model\n- Testing data : which is the data of passengers that we want to predict whether they survived or not","59810abc":"## Workflow\n\n- Define our problem\n- Loading the data\n- Analyzing the data through useful graphs\n- wrangling and cleaning the data\n- Feature selection\n- Choosing the best model to solve the problem\n- Visualize and dicuss the results\n","65a187a5":"Now we will explore the embarked column","5fd3d48a":"#### Problem definition:\n\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this dataset, we will build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).","57335427":"We can see from this graph that females are much more likely to survive than males","7004e57e":"From the previous histograms we can deduce the following\n\n- Age is slightly skewed with the majority being around 20-40\n- Fare is right skewed and we can preform log transformation to it","d8023a84":"#### Analyzing data:","1f88db23":"**Now we will look the numerical features distribution**","6b3449b0":"From the previous statistics we can deduce the following:\n\n- The survived feature is a categorical value with 0\/1 values\n- 38% of the passengers survived\n- Pclass is a categorical feature with values (1,2,3)\n- The average age of passengers is around 30\n","e08d5f35":"Our dataset has 891 records and 12 features","6d874941":"# Titanic Dataset Predictions\n##### In this notebook we will explore and titanic dataset, Look into the relations between the features associated with each record and finally predict whether a passenger survived the titanic disaster or not","3de1238c":"here we can deduce that the lower the fare the less likely the person will survive","3a0acb54":"Now we will fill the missing values of age and fare with the median of the values","106896cc":"**Now we will look the Numerical features with respect to the survived feature**","9b36e936":"Now we will explore the relation between age and fare with respect to the the survived column ","09bdd160":"**Now we are going to plot historgram of all numerical values in order to check the distribution  of numerical data**","34f81ad6":"First we will take a look at the training dataset to investigate the features","e0c8ab7f":"the following features have missing values\n\n- Age\n- Cabin\n- Embarked\n"}}