{"cell_type":{"5e46f2b5":"code","545ccb6b":"code","8e5766d8":"code","3622c6fd":"code","8c3554e3":"code","71df4b4a":"code","edebf143":"code","cd694eb0":"code","7ee5a27e":"code","dd8e7677":"code","971cf463":"code","25e9aecf":"code","657c2964":"code","b76f5a2e":"code","93be7d85":"code","83898ad3":"code","4be1a364":"code","ee111c36":"code","aa3e2db5":"code","960546a5":"code","a9276b60":"code","657e0c07":"code","f4cf9c36":"code","70d5050e":"code","7ae73822":"code","e64acb74":"code","fa6c7cbd":"code","ca6155ae":"code","ae9da776":"code","04d27280":"code","f531f499":"code","1c99b05b":"code","ff849929":"code","63521240":"code","81265537":"code","a1c26fa7":"code","1831f00b":"code","b6e5b08e":"code","64142225":"code","d7917655":"code","b5ba1886":"code","7d334134":"code","6ca613f8":"code","fc9bb3df":"code","dab43714":"code","4b78ec6e":"code","e1bbd3ea":"code","2043f341":"code","b19afae7":"code","7556117c":"code","eb70d70e":"code","02ea427a":"code","bb95de67":"code","63db81a6":"code","cf8d05d8":"code","633b9a3c":"code","afbe89a0":"code","c221ef5a":"code","dbed52c5":"code","4dc8e513":"code","55ba8d30":"code","25f4f6cc":"code","20b79d3b":"code","61944109":"code","2403d4db":"code","e2b3bd6b":"code","2f2d57f6":"code","29d4188f":"code","3b7dc4be":"code","255b7880":"markdown","969d5cd5":"markdown","b6971891":"markdown","a4a2a10c":"markdown","aadb0e04":"markdown","01afae58":"markdown","db9fba54":"markdown","b27d3b9c":"markdown","3993575c":"markdown","33f4e294":"markdown","07088002":"markdown","d1b8d268":"markdown","a5541145":"markdown","87db77b8":"markdown","f30c16b1":"markdown","1ec3d8d1":"markdown","04d6191b":"markdown","38531f91":"markdown","63c9645f":"markdown","dcc10a3c":"markdown","a4d68d84":"markdown","a1941a63":"markdown","33836cac":"markdown","ec3321c4":"markdown","d68e56e3":"markdown","5efe688d":"markdown","c7790873":"markdown"},"source":{"5e46f2b5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pandas.io.json import json_normalize\nimport json\nimport matplotlib.pyplot as plt\nimport lightgbm as lgb\nimport datetime\nimport seaborn as sns\nfrom bayes_opt import BayesianOptimization\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nimport gc\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold,train_test_split\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Any results you write to the current directory are saved as output.","545ccb6b":"def load_df(csv_path='..\/input\/train.csv', nrows=None):\n    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n    \n    df = pd.read_csv(csv_path, \n                     converters={column: json.loads for column in JSON_COLUMNS}, \n                     dtype={'fullVisitorId': 'str'}, # Important!!\n                     nrows=nrows)\n    \n    for column in JSON_COLUMNS:\n        column_as_df = json_normalize(df[column])\n        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n    print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n    return df","8e5766d8":"def add_time_features(df):\n    df['date'] = pd.to_datetime(df['date'], format='%Y%m%d', errors='ignore')\n    df['year'] = df['date'].apply(lambda x: x.year)\n    df['month'] = df['date'].apply(lambda x: x.month)\n    df['day'] = df['date'].apply(lambda x: x.day)\n    df['weekday'] = df['date'].apply(lambda x: x.weekday())\n    df['visitStartTime_'] = pd.to_datetime(df['visitStartTime'],unit=\"s\")\n    df['visitStartTime_year'] = df['visitStartTime_'].apply(lambda x: x.year)\n    df['visitStartTime_month'] = df['visitStartTime_'].apply(lambda x: x.month)\n    df['visitStartTime_day'] = df['visitStartTime_'].apply(lambda x: x.day)\n    df['visitStartTime_weekday'] = df['visitStartTime_'].apply(lambda x: x.weekday())\n    return df\ndate_features = [#\"year\",\"month\",\"day\",\"weekday\",'visitStartTime_year',\n    \"visitStartTime_month\",\"visitStartTime_day\",\"visitStartTime_weekday\"]","3622c6fd":"%%time\ntrain_df = load_df(\"..\/input\/train.csv\")\ntest_df = load_df(\"..\/input\/test.csv\")","8c3554e3":"train_df.head()","71df4b4a":"train_df.dtypes","edebf143":"test_df.info()","cd694eb0":"pd.value_counts(train_df.dtypes).plot(kind=\"bar\")\nplt.title(\"type of train data\")","7ee5a27e":"def bar_plot(column,**args):\n    pd.value_counts(train_df[column]).plot(kind=\"bar\",**args)\n    ","dd8e7677":"constant_column = [col for col in train_df.columns if len(train_df[col].unique()) == 1]\nprint(list(constant_column))\ntrain_df.drop(columns=constant_column,inplace=True)","971cf463":"num_col = [\"totals.hits\", \"totals.pageviews\", \"visitNumber\", \n           'totals.bounces',  'totals.newVisits']","25e9aecf":"for col in num_col:\n    train_df[col] = train_df[col].fillna(\"0\").astype(\"int32\")\n    test_df[col] = test_df[col].fillna(\"0\").astype(\"int32\")","657c2964":"train_df.dtypes","b76f5a2e":"train_df.head()","93be7d85":"bar_plot(\"channelGrouping\")","83898ad3":"bar_plot(\"device.browser\",figsize=(12,10))","4be1a364":"bar_plot(\"device.isMobile\")","ee111c36":"bar_plot(\"device.deviceCategory\")","aa3e2db5":"new_features = [\"hits_per_pageviews\"]\nnew_category_features = [\"is_high_hits\"]\ndef feature_engineering(df):\n    line = 4\n    df['hits_per_pageviews'] = (df[\"totals.hits\"]\/(df[\"totals.pageviews\"])).apply(lambda x: 0 if np.isinf(x) else x)\n    df['is_high_hits'] = np.logical_or(train_df[\"totals.hits\"]>line,train_df[\"totals.pageviews\"]>line).astype(np.int32)","960546a5":"feature_engineering(train_df)\nfeature_engineering(test_df)\nadd_time_features(train_df)\n_ = add_time_features(test_df)","a9276b60":"category_features = [\"channelGrouping\", \"device.browser\", \n            \"device.deviceCategory\", \"device.operatingSystem\", \n            \"geoNetwork.city\", \"geoNetwork.continent\", \n            \"geoNetwork.country\", \"geoNetwork.metro\",\n            \"geoNetwork.networkDomain\", \"geoNetwork.region\", \n            \"geoNetwork.subContinent\",\n            #\"trafficSource.adContent\", \n            #\"trafficSource.adwordsClickInfo.adNetworkType\", \n            #\"trafficSource.adwordsClickInfo.gclId\", \n            #\"trafficSource.adwordsClickInfo.page\", \n            #\"trafficSource.adwordsClickInfo.slot\",\n            #\"trafficSource.campaign\",\n            #\"trafficSource.keyword\", \n            \"trafficSource.medium\", \n            #\"trafficSource.referralPath\", \n            \"trafficSource.source\",\n\n            #'trafficSource.adwordsClickInfo.isVideoAd',\n            'trafficSource.isTrueDirect',\n            #\"filtered_keyword\"\n            ] + date_features\ntarget = 'totals.transactionRevenue'\nuseless_col = [\"trafficSource.adContent\", \n              \"trafficSource.adwordsClickInfo.adNetworkType\", \n              \"trafficSource.adwordsClickInfo.page\",\n              \"trafficSource.adwordsClickInfo.slot\",\n              \"trafficSource.campaign\",\n              \"trafficSource.referralPath\",\n              'trafficSource.adwordsClickInfo.isVideoAd',\n              \"trafficSource.adwordsClickInfo.gclId\",\n              \"trafficSource.keyword\"]","657e0c07":"train_df.head()","f4cf9c36":"useless_df = train_df[useless_col]\nuseless_df.info()","70d5050e":"for col in useless_col:\n    print(\"-\"*10,col,\"-\"*10)\n    print(\"unique value numbers:\",len(useless_df[col].unique()))\n    print(\"null rate:\",useless_df[col].isna().sum()\/len(useless_df[col]))","7ae73822":"for col in category_features:\n    print(\"-\"*10,col,\"-\"*10)\n    print(\"unique value numbers:\",len(train_df[col].unique()))\n    print(\"null rate:\",train_df[col].isna().sum()\/len(train_df[col]))","e64acb74":"train_df[target] = train_df[target].fillna(\"0\").astype(\"int32\")","fa6c7cbd":"all_features = category_features+num_col+new_features+new_category_features\nall_features\n# dev_df = train_df[train_df['date']<=pd.to_datetime('20170531', format='%Y%m%d')]\n# val_df = train_df[train_df['date']>pd.to_datetime('20170531', format='%Y%m%d')]\n\n# dev_x = dev_df[all_features]\n# dev_y = dev_df[target]\n# val_x = val_df[all_features]\n# val_y = val_df[target]\n# test_x = test_df[all_features]\n# for col in category_features:\n#     print(\"transform column {}\".format(col))\n#     lbe = LabelEncoder()\n#     lbe.fit(pd.concat([train_df[col],test_x[col]]).astype(\"str\"))\n#     dev_x[col] = lbe.transform(dev_x[col].astype(\"str\"))\n#     val_x[col] = lbe.transform(val_x[col].astype(\"str\"))\n#     test_x[col] = lbe.transform(test_x[col].astype(\"str\"))","ca6155ae":"train_df[\"totals.hits\"].describe()","ae9da776":"sns.distplot(train_df[\"totals.hits\"],kde=False)","04d27280":"train_df[\"totals.pageviews\"].describe()","f531f499":"sns.distplot(train_df[\"totals.pageviews\"],kde=False)","1c99b05b":"sns.jointplot(\"totals.pageviews\",\"totals.hits\",data=train_df)","ff849929":"sns.jointplot(\"totals.pageviews\",target,data=train_df)\nsns.jointplot(\"totals.hits\",target,data=train_df)","63521240":"line = 4\nhigh_hits_pageviews_df = train_df[np.logical_or(train_df[\"totals.hits\"]>line,train_df[\"totals.pageviews\"]>line)]\nlow_hits_pageviews_df = train_df[np.logical_and(train_df[\"totals.hits\"]<=line,train_df[\"totals.pageviews\"]<=line)]","81265537":"print(\"high rate :\",high_hits_pageviews_df.shape[0]\/train_df.shape[0])\nprint(\"low rate :\",low_hits_pageviews_df.shape[0]\/train_df.shape[0])","a1c26fa7":"high_hits_pageviews_df[target].describe()","1831f00b":"low_hits_pageviews_df[target].describe()","b6e5b08e":"fig,axes = plt.subplots(nrows=1,ncols=2,figsize=(16,8))\nsns.distplot(high_hits_pageviews_df[target],kde=False,ax=axes[0])\naxes[0].set_title(\"distribution of high hits transactionRevenue\")\n\nsns.distplot(low_hits_pageviews_df[target],kde=False,ax=axes[1])\naxes[1].set_title(\"distribution of low hits transactionRevenue\")","64142225":"print(\"zero rate of transactionRevenue:\",(train_df[target]==0).sum()\/train_df.shape[0])\nprint(\"zero rate of high hits transactionRevenue:\",(high_hits_pageviews_df[target]==0).sum()\/ high_hits_pageviews_df.shape[0])\nprint(\"zero rate of low hits transactionRevenue:\",(low_hits_pageviews_df[target]==0).sum()\/ low_hits_pageviews_df.shape[0])","d7917655":"train_df[\"hits_per_pageviews\"].describe()","b5ba1886":"sns.jointplot(\"hits_per_pageviews\",target,data=train_df)","7d334134":"visitStartTime_df = train_df[[\"visitStartTime\",'visitStartTime_year',\"visitStartTime_month\",\"visitStartTime_day\",\"visitStartTime_weekday\",target]]\nvisitStartTime_df[\"visitStartTime\"] = pd.to_datetime(visitStartTime_df[\"visitStartTime\"],unit=\"s\")\nvisitStartTime_df[\"visitStartDate\"] = visitStartTime_df[\"visitStartTime\"].apply(lambda x: x.date())","6ca613f8":"def plot_dist_date(col,kind=\"bar\"):\n    fig,axes = plt.subplots(nrows=1,ncols=2,figsize=(12,6))\n    visitStartTime_df.groupby(col)[target].agg([\"sum\"]).plot(kind=kind,title=\"sum of transactionRevenue:\"+col,ax=axes[0])\n    visitStartTime_df.groupby(col)[target].agg([\"count\"]).plot(kind=kind,title=\"count of transactionRevenue:\"+col,ax=axes[1])\n    plt.tight_layout()","fc9bb3df":"plot_dist_date(\"visitStartDate\",kind=\"line\")","dab43714":"test_visitStartTime_df = test_df[[\"visitStartTime\",'visitStartTime_year',\"visitStartTime_month\",\"visitStartTime_day\",\"visitStartTime_weekday\"]]\ntest_visitStartTime_df[\"visitStartTime\"] = pd.to_datetime(test_visitStartTime_df[\"visitStartTime\"],unit=\"s\")\ntest_visitStartTime_df[\"visitStartDate\"] = test_visitStartTime_df[\"visitStartTime\"].apply(lambda x: x.date())","4b78ec6e":"test_visitStartTime_df.groupby(\"visitStartDate\")[\"visitStartTime\"].agg(\"count\").plot(figsize=(8,6),title=\"count of test\")","e1bbd3ea":"plot_dist_date(\"visitStartTime_year\")","2043f341":"plot_dist_date(\"visitStartTime_month\")","b19afae7":"plot_dist_date(\"visitStartTime_day\")","7556117c":"plot_dist_date(\"visitStartTime_weekday\")","eb70d70e":"col = \"trafficSource.cleanedkeyword\"\ntrain_df[col] = train_df[\"trafficSource.keyword\"].apply(lambda x :x if isinstance(x,float) and np.isnan(x) else x.lower()).apply(lambda x :x if isinstance(x,float) and np.isnan(x) else x.replace(\"+\", \"\"))\ntest_df[col] = test_df[\"trafficSource.keyword\"].apply(lambda x :x if isinstance(x,float) and np.isnan(x) else x.lower())\nprint(\"-\"*10,\"train\",\"-\"*10)\nprint(\"unique value numbers:\",len(train_df[col].unique()))\nprint(\"null rate:\",train_df[col].isna().sum()\/len(train_df[col]))\nprint(\"-\"*10,\"test\",\"-\"*10)\nprint(\"unique value numbers:\",len(test_df[col].unique()))\nprint(\"null rate:\",test_df[col].isna().sum()\/len(test_df[col]))","02ea427a":"pd.value_counts(train_df[col]).sort_values(ascending=False)[0:20]","bb95de67":"pd.value_counts(test_df[col]).sort_values(ascending=False)[0:20]","63db81a6":"train_df.groupby(col)[target].agg(\"sum\").sort_values(ascending=False)[0:29].apply(lambda x: np.log1p(x)).plot(kind=\"bar\",figsize=(20,5),title=\"sum revenue of keyword\")","cf8d05d8":"none_zero_keywords= set(train_df.groupby(col)[target].agg(\"sum\").sort_values(ascending=False)[0:28].index)\ntest_keywords_set = set(test_df[col].unique())\nintersection_keyword = none_zero_keywords.intersection(test_keywords_set)\nprint(\"len:\",len(intersection_keyword))\nintersection_keyword","633b9a3c":"train_df.groupby(col)[target].agg(\"sum\").sort_values(ascending=False)[0:29].apply(lambda x: np.log1p(x)).plot(kind=\"bar\",figsize=(20,5),title=\"zero revenue of keyword\")","afbe89a0":"def add_keyword_feature(df):\n    col_name =\"filtered_keyword\"\n    sets = intersection_keyword.difference({'(automatic matching)',\n     '(not provided)',\n     '(remarketing\/content targeting)'})\n    df[col_name] = df[col].apply(lambda x: x if x in sets else \"other\")\nadd_keyword_feature(train_df)\nadd_keyword_feature(test_df)\n# no improvement ,something wrong","c221ef5a":"none_zero_keywords.difference(test_keywords_set)","dbed52c5":"train_df.groupby(col)[target].agg(\"count\").sort_values(ascending=False)[0:40].apply(lambda x: np.log1p(x)).plot(kind=\"bar\",figsize=(20,5),title=\"count revenue of keyword\")","4dc8e513":"train_x = train_df[all_features]\ntrain_y = train_df[target]\ntest_x = test_df[all_features]\nfor col in category_features:\n    print(\"transform column {}\".format(col))\n    lbe = LabelEncoder()\n    lbe.fit(pd.concat([train_df[col],test_x[col]]).astype(\"str\"))\n    train_x[col] = lbe.transform(train_x[col].astype(\"str\"))\n    test_x[col] = lbe.transform(test_x[col].astype(\"str\"))","55ba8d30":"def lgb_eval(num_leaves,max_depth,lambda_l2,lambda_l1,min_child_samples,bagging_fraction,feature_fraction):\n    params = {\n    \"objective\" : \"regression\",\n    \"metric\" : \"rmse\", \n    \"num_leaves\" : int(num_leaves),\n    \"max_depth\" : int(max_depth),\n    \"lambda_l2\" : lambda_l2,\n    \"lambda_l1\" : lambda_l1,\n    \"num_threads\" : 4,\n    \"min_child_samples\" : int(min_child_samples),\n    \"learning_rate\" : 0.03,\n    \"bagging_fraction\" : bagging_fraction,\n    \"feature_fraction\" : feature_fraction,\n    \"subsample_freq\" : 5,\n    \"bagging_seed\" : 42,\n    \"verbosity\" : -1\n    }\n    lgtrain = lgb.Dataset(train_x, label=np.log1p(train_y.apply(lambda x : 0 if x < 0 else x)),categorical_feature=category_features)\n    cv_result = lgb.cv(params,\n                       lgtrain,\n                       10000,\n                       categorical_feature=category_features,\n                       early_stopping_rounds=100,\n                       stratified=False,\n                       nfold=5)\n    return -cv_result['rmse-mean'][-1]\n\ndef lgb_train(num_leaves,max_depth,lambda_l2,lambda_l1,min_child_samples,bagging_fraction,feature_fraction):\n    params = {\n    \"objective\" : \"regression\",\n    \"metric\" : \"rmse\", \n    \"num_leaves\" : int(num_leaves),\n    \"max_depth\" : int(max_depth),\n    \"lambda_l2\" : lambda_l2,\n    \"lambda_l1\" : lambda_l1,\n    \"num_threads\" : 4,\n    \"min_child_samples\" : int(min_child_samples),\n    \"learning_rate\" : 0.01,\n    \"bagging_fraction\" : bagging_fraction,\n    \"feature_fraction\" : feature_fraction,\n    \"subsample_freq\" : 5,\n    \"bagging_seed\" : 42,\n    \"verbosity\" : -1\n    }\n    t_x,v_x,t_y,v_y = train_test_split(train_x,train_y,test_size=0.2)\n    lgtrain = lgb.Dataset(t_x, label=np.log1p(t_y.apply(lambda x : 0 if x < 0 else x)),categorical_feature=category_features)\n    lgvalid = lgb.Dataset(v_x, label=np.log1p(v_y.apply(lambda x : 0 if x < 0 else x)),categorical_feature=category_features)\n    model = lgb.train(params, lgtrain, 2000, valid_sets=[lgvalid], early_stopping_rounds=100, verbose_eval=100)\n    pred_test_y = model.predict(test_x, num_iteration=model.best_iteration)\n    return pred_test_y, model\n    \ndef param_tuning(init_points,num_iter,**args):\n    lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (25, 50),\n                                                'max_depth': (5, 15),\n                                                'lambda_l2': (0.0, 0.05),\n                                                'lambda_l1': (0.0, 0.05),\n                                                'bagging_fraction': (0.5, 0.8),\n                                                'feature_fraction': (0.5, 0.8),\n                                                'min_child_samples': (20, 50),\n                                                })\n\n    lgbBO.maximize(init_points=init_points, n_iter=num_iter,**args)\n    return lgbBO","25f4f6cc":"result = param_tuning(5,15)","20b79d3b":"result.res['max']['max_params']","61944109":"prediction1,model1 = lgb_train(**result.res['max']['max_params'])\nprediction2,model2 = lgb_train(**result.res['max']['max_params'])\nprediction3,model3 = lgb_train(**result.res['max']['max_params'])","2403d4db":"# param = {'num_leaves': 45.61216380347129,\n#  'max_depth': 11.578579827303919,\n#  'lambda_l2': 0.0107663924764632,\n#  'lambda_l1': 0.046541310399201855,\n#  'bagging_fraction': 0.7851516324443661,\n#  'feature_fraction': 0.7944881085591733,\n#  'min_child_samples': 28.5601473698899}\n# prediction,model = lgb_train(**param)","e2b3bd6b":"test_df['PredictedLogRevenue'] = (np.expm1(prediction1)+np.expm1(prediction2)+np.expm1(prediction3))\/3\nsubmit = test_df[['fullVisitorId', 'PredictedLogRevenue']].groupby('fullVisitorId').sum()['PredictedLogRevenue'].apply(np.log1p).fillna(0).reset_index()","2f2d57f6":"submit.to_csv('submission.csv', index=False)","29d4188f":"lgb.plot_importance(model, figsize=(15, 10),height=0.8)\nplt.show()","3b7dc4be":"lgb.plot_importance(model, figsize=(15, 10),height=0.8,importance_type=\"gain\")\nplt.show()","255b7880":"When I convert visitStartTime to numerical type, the feature has high importance. It can not make sence. I try to remove this feature. LB score decrease.\n\n**What happen?**","969d5cd5":"train_data: (2016-08,2017-08)\ntest_data: (2017-08,2018-05)\n\nThe count distribution of test data is different from train data\n* The year feature may be useless","b6971891":"## Data type","a4a2a10c":"Transaction count in day is uniform distribution, unless 31, because of many months no 31 days.\n\ntransaction revenue is not uniform distribution, **visitStartTime_day may be a useful feature .**","aadb0e04":"# trafficSource.keyword\nTransaction revenue must related to goods. The keyword may have relation to goods.\n\ncontinue to analyse","01afae58":"# Glimpse of data","db9fba54":"# Feature engineering","b27d3b9c":"# LGBM","3993575c":"## Samples of train data","33f4e294":"# Hits and Pageviews","07088002":"**High hits_per_pageviews has 0 revenue. Maybe I can use this feature.**","d1b8d268":"## device.isMobile","a5541145":"# Univariant viariable analysis","87db77b8":"# Remove constant column","f30c16b1":"## device.deviceCategory","1ec3d8d1":"Most of hits are less than 4.","04d6191b":"There are only 3 keywords has none zero revenues. \n\nDoes these keywords occurs in test data? \n\nHow do I encode this feature?","38531f91":"**Filter out high hits or pageviews record**","63c9645f":"## device.browser","dcc10a3c":"Pageviews has same distribution with hits.","a4d68d84":"# visitStartTime","a1941a63":"## Useless features","33836cac":"## If this kernel is useful to you, please give a vote. This is my first depth EDA kernel. Your vote is my inspiration.","ec3321c4":"## ChannelGrouping\n```\nThe channel via which the user came to the Store.\n```","d68e56e3":"## totals.hits","5efe688d":"high hits and pageviews has low zero rate, low hits and pageviews has high zero rate\n\n**how about hits per pageviews? **","c7790873":"## totals.pageviews"}}