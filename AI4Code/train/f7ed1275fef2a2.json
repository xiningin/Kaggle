{"cell_type":{"633e1b84":"code","63f1464c":"code","9e83062f":"code","f2452ba1":"code","a3f2b498":"code","e7d5df8c":"code","7d685239":"code","86efce14":"code","0f6caee4":"code","5b485407":"code","6c240a96":"code","ab5f85a6":"markdown","1a8d4afe":"markdown","8e250b00":"markdown","3edd5e1c":"markdown","39979568":"markdown","647a7939":"markdown","802c9f6e":"markdown","cdbd1730":"markdown","01c29dc2":"markdown"},"source":{"633e1b84":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\nfrom transformers import DistilBertTokenizer, DistilBertConfig, TFDistilBertModel\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score\n\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Embedding, LSTM, Conv2D, Conv1D, MaxPooling1D, Dense, Dropout, GlobalMaxPooling1D, Input, Bidirectional, concatenate, Flatten, GlobalAveragePooling1D\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.utils import plot_model\n\n# physical_devices = tf.config.list_physical_devices('GPU')\n# tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n\n# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","63f1464c":"TRAIN_FILE_PATH = '\/kaggle\/input\/ag-news-classification-dataset\/train.csv'\nTEST_FILE_PATH = '\/kaggle\/input\/ag-news-classification-dataset\/test.csv'\n\n\ndata = pd.read_csv(TRAIN_FILE_PATH)\ntestdata = pd.read_csv(TEST_FILE_PATH)\n\nX_train = data['Title'] + \" \" + data['Description']\ny_train = data['Class Index'].apply(lambda x: x-1).values # Classes need to begin from 0\n\nx_test = testdata['Title'] + \" \" + testdata['Description']\ny_test = testdata['Class Index'].apply(lambda x: x-1).values # Classes need to begin from 0\n\nmaxlen = X_train.map(lambda x: len(x.split())).max()\ndata.describe()","9e83062f":"vocab_size = 20000\nembed_size = 32\ndistil_bert = 'distilbert-base-uncased'\n\ntokenizer = DistilBertTokenizer.from_pretrained(distil_bert, do_lower_case=True, add_special_tokens=True,\n                                                max_length=maxlen, pad_to_max_length=True)\n\ndef tokenize(sentences, tokenizer):\n    input_ids, input_masks, input_segments = [],[],[]\n    for sentence in tqdm(sentences):\n        inputs = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=maxlen, pad_to_max_length=True, \n                                             return_attention_mask=True, return_token_type_ids=True)\n        input_ids.append(inputs['input_ids'])\n        input_masks.append(inputs['attention_mask'])\n        input_segments.append(inputs['token_type_ids'])        \n        \n    return np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32'), np.asarray(input_segments, dtype='int32')","f2452ba1":"# Tokenize desc and title train data\nX_train = tokenize(X_train, tokenizer)\nx_test = tokenize(x_test, tokenizer)","a3f2b498":"with tpu_strategy.scope():\n    config = DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n    config.output_hidden_states = False\n    transformer_model = TFDistilBertModel.from_pretrained(distil_bert, config=config)\n\n    input_ids_in = tf.keras.layers.Input(shape=(maxlen,), name='input_token', dtype='int32')\n    input_masks_in = tf.keras.layers.Input(shape=(maxlen,), name='masked_token', dtype='int32') \n\n    embedding_layer = transformer_model(input_ids_in, attention_mask=input_masks_in)[0]\n    X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(embedding_layer)\n    X = tf.keras.layers.GlobalMaxPool1D()(X)\n    X = tf.keras.layers.Dense(64, activation='relu')(X)\n    X = tf.keras.layers.Dropout(0.2)(X)\n    X = tf.keras.layers.Dense(4, activation='sigmoid')(X)\n    model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)\n\n    for layer in model.layers[:3]:\n        layer.trainable = False\n\n    model.summary()","e7d5df8c":"callbacks = [\n#     EarlyStopping(\n#         monitor='val_accuracy',\n#         min_delta=1e-4,\n#         patience=4,\n#         verbose=1\n#     ),\n    ModelCheckpoint(\n        filepath='weights.h5',\n        monitor='val_accuracy', \n        mode='max', \n        save_best_only=True,\n        save_weights_only=True,\n        verbose=1\n    )\n]","7d685239":"model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(X_train, y_train, batch_size=1024, validation_data=(x_test, y_test), epochs=20, callbacks=callbacks)","86efce14":"model.load_weights('weights.h5')","0f6caee4":"labels = ['World News', 'Sports News', 'Business News', 'Science-Technology News']\n\ntest = ['New evidence of virus risks from wildlife trade', 'Coronavirus: Bank pumps \u00a3100bn into UK economy to aid recovery', \n        'Trump\\'s bid to end Obama-era immigration policy ruled unlawful', 'David Luiz\u2019s future with Arsenal to be decided this week']\ntest_seq = tokenize(test, tokenizer)\ntest_preds = [labels[np.argmax(i)] for i in model.predict(test_seq)]\n\nfor news, label in zip(test, test_preds):\n    print('{} - {}'.format(news, label))","5b485407":"preds = [np.argmax(i) for i in model.predict(x_test)]\ncm  = confusion_matrix(y_test, preds)\nplt.figure()\nplot_confusion_matrix(cm, figsize=(16,12), hide_ticks=True, cmap=plt.cm.Blues)\nplt.xticks(range(4), labels, fontsize=12)\nplt.yticks(range(4), labels, fontsize=12)\nplt.show()","6c240a96":"print(\"Recall of the model is {:.2f}\".format(recall_score(y_test, preds, average='micro')))\nprint(\"Precision of the model is {:.2f}\".format(precision_score(y_test, preds, average='micro')))","ab5f85a6":"# Define model in TPU scope","1a8d4afe":"# Load data","8e250b00":"# Plot confusion matrix","3edd5e1c":"# Define tokenizer","39979568":"# Tokenize data using defined tokenizer","647a7939":"# Compile and fit model","802c9f6e":"# Get precision and recall scores","cdbd1730":"# Import libraries\n\nWe are going to use TPUs for this, since transformers including distilBERT are very heavy duty, and we'll need a lot of computational power for this one.","01c29dc2":"# Test model with some arbitrary data"}}