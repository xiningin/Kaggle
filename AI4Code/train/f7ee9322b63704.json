{"cell_type":{"294dab30":"code","226cf159":"code","285f26ee":"code","88efba12":"code","99bdb1c7":"code","3b017e51":"code","06876790":"code","c425f525":"code","a9150952":"code","a3c21308":"code","a3bf525f":"code","2e9d19f6":"code","a0e470c4":"code","75eddf26":"code","6e8c4b91":"code","44299777":"code","c5c0fa76":"code","211f957e":"code","e2655486":"code","225a74db":"code","25c3f622":"code","f3b789bf":"code","ab7ccaf6":"code","c7b91ca8":"code","c3b9ea49":"code","322df811":"code","faa9803e":"code","90ed5ed8":"code","0dd8cc29":"code","c511c74a":"code","81eb8450":"code","6ddd1f8d":"code","f28bf1c2":"code","4fcef3e9":"code","0158bc9a":"code","134592d4":"markdown","ad241aac":"markdown","c3e2f469":"markdown","55e719f4":"markdown","3692f1c0":"markdown"},"source":{"294dab30":"from tqdm import tqdm\nimport seaborn as sns\nimport networkx as nx\nimport pandas as pd\nimport numpy as np\nimport gensim\nimport spacy\nimport os\nimport gc\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom spacy.matcher import Matcher \nfrom collections import  Counter\nimport matplotlib.pyplot as plt\nimport tensorflow_hub as hub\nimport tensorflow as tf","226cf159":"path=\"..\/input\/CORD-19-research-challenge\/\"\ndata=pd.read_csv(path+\"metadata.csv\")","285f26ee":"data.isna().sum()","88efba12":"stop=set(stopwords.words('english'))\n\ndef build_corpus(df,col=\"title\"):\n    corpus=[]\n    lem=WordNetLemmatizer()\n    stop=set(stopwords.words('english'))\n    new= df[col].dropna().str.split()\n    new=new.values.tolist()\n    corpus=[lem.lemmatize(word.lower()) for i in new for word in i if(word) not in stop]\n    \n    return corpus","99bdb1c7":"# x1=[]\n# x2=[]\n# corpus= build_corpus(data, \"abstract\")\n# counter= Counter(corpus)\n# common= counter.most_common()\n# for word, count in common[:10]:\n#     if(word not in stop):\n#         x1.append(word)\n#         x2.append(count)\n\n# sns.barplot(x=x1, y=x2)","3b017e51":"# def prepare_similarity(vectors):\n#     similarity=cosine_similarity(vectors)\n#     return similarity\n\n# def get_top_similar(sentence, sentence_list, similarity_matrix, topN):\n#     # find the index of sentence in list\n#     index = sentence_list.index(sentence)\n#     # get the corresponding row in similarity matrix\n#     similarity_row = np.array(similarity_matrix[index, :])\n#     # get the indices of top similar\n#     indices = similarity_row.argsort()[-topN:][::-1]\n#     return [(i,sentence_list[i]) for i in indices]","06876790":"path=\"..\/input\/cord-19-eda-parse-json-and-generate-clean-csv\/\"","c425f525":"clean_comm=pd.read_csv(path+\"clean_comm_use.csv\",nrows=1000)\nclean_comm['source']='clean_comm'\nbiox = pd.read_csv(path+\"biorxiv_clean.csv\")\nbiox['source']='biorx'\n\narticles=pd.concat([biox,clean_comm])","a9150952":"del biox,clean_comm\ngc.collect()","a3c21308":"articles.shape","a3bf525f":"articles.head()","2e9d19f6":"articles.shape","a0e470c4":"articles.fillna(\"Unknown\",inplace=True)","75eddf26":"# titles=[]\ntexts=[]\nfor i in range(100):\n#     titles.append(articles.iloc[i]['title'])\n    texts.append(articles.iloc[i]['text'])","6e8c4b91":"import re\ndef clean(txt):\n    txt=re.sub(r'\\n',' ',txt)\n    txt=re.sub(r'\\([^()]*\\)',' ',txt)\n    txt=re.sub(r'https?:\\S+\\sdoi',' ',txt)\n    return txt","44299777":"texts=list(map(clean,texts))\ntext_list=' '.join(texts)","c5c0fa76":"print(text_list)","211f957e":"import spacy\nnlp=spacy.load('en_core_web_sm')","e2655486":"# def get_entities(sent):\n#     ## chunk 1\n#     ent1 = \"\"\n#     ent2 = \"\"\n\n#     prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n#     prv_tok_text = \"\"   # previous token in the sentence\n\n#     prefix = \"\"\n#     modifier = \"\"\n\n#   #############################################################\n  \n#     for tok in nlp(sent):\n#         ## chunk 2\n#         # if token is a punctuation mark then move on to the next token\n#         if tok.dep_ != \"punct\":\n#           # check: token is a compound word or not\n#           if tok.dep_ == \"compound\":\n#             prefix = tok.text\n#             # if the previous word was also a 'compound' then add the current word to it\n#             if prv_tok_dep == \"compound\":\n#                    prefix = prv_tok_text + \" \"+ tok.text\n      \n#       # check: token is a modifier or not\n#         if tok.dep_.endswith(\"mod\") == True:\n#             modifier = tok.text\n#             # if the previous word was also a 'compound' then add the current word to it\n#             if prv_tok_dep == \"compound\":\n#               modifier = prv_tok_text + \" \"+ tok.text\n\n#           ## chunk 3\n#         if tok.dep_.find(\"subj\") == True:\n#             ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n#             prefix = \"\"\n#             modifier = \"\"\n#             prv_tok_dep = \"\"\n#             prv_tok_text = \"\"      \n\n#           ## chunk 4\n#         if tok.dep_.find(\"obj\") == True:\n#             ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n\n#           ## chunk 5  \n#           # update variables\n#         prv_tok_dep = tok.dep_\n#         prv_tok_text = tok.text\n#   #############################################################\n\n#     return [ent1.strip(), ent2.strip()]","225a74db":"# def get_relation(sent):\n\n#   doc = nlp(sent)\n\n#   # Matcher class object \n#   matcher = Matcher(nlp.vocab)\n\n#   #define the pattern \n#   pattern = [{'DEP':'ROOT'}, \n#             {'DEP':'prep','OP':\"?\"},\n#             {'DEP':'agent','OP':\"?\"},  \n#             {'POS':'ADJ','OP':\"?\"}] \n\n#   matcher.add(\"matching_1\", None, pattern) \n\n#   matches = matcher(doc)\n#   k = len(matches) - 1\n\n#   span = doc[matches[k][1]:matches[k][2]] \n\n#   return(span.text)","25c3f622":"import pandas as pd\nimport re\n\nimport spacy\nnlp=spacy.load('en_core_web_sm')\n\ndef get_entity_pairs(text, coref=True):\n    # preprocess text\n    text = re.sub(r'\\n+', '.', text)  # replace multiple newlines with period\n    text = re.sub(r'\\[\\d+\\]', ' ', text)  # remove reference numbers\n    text = nlp(text)\n    \n    def refine_ent(ent, sent):\n        unwanted_tokens = (\n            'PRON',  # pronouns\n            'PART',  # particle\n            'DET',  # determiner\n            'SCONJ',  # subordinating conjunction\n            'PUNCT',  # punctuation\n            'SYM',  # symbol\n            'X',  # other\n        )\n        ent_type = ent.ent_type_  # get entity type\n        if ent_type == '':\n            ent_type = 'NOUN_CHUNK'\n            ent = ' '.join(str(t.text) for t in\n                           nlp(str(ent)) if t.pos_\n                           not in unwanted_tokens and t.is_stop == False)\n        elif ent_type in ('NOMINAL', 'CARDINAL', 'ORDINAL') and str(ent).find(' ') == -1:\n            refined = ''\n            for i in range(len(sent) - ent.i):\n                if ent.nbor(i).pos_ not in ('VERB', 'PUNCT'):\n                    refined += ' ' + str(ent.nbor(i))\n                else:\n                    ent = refined.strip()\n                    break\n\n        return ent, ent_type\n\n    sentences = [sent.string.strip() for sent in text.sents]  # split text into sentences\n    ent_pairs = []\n    for sent in sentences:\n        sent = nlp(sent)\n        spans = list(sent.ents) + list(sent.noun_chunks)  # collect nodes\n        spans = spacy.util.filter_spans(spans)\n        with sent.retokenize() as retokenizer:\n            [retokenizer.merge(span, attrs={'tag': span.root.tag,\n                                            'dep': span.root.dep}) for span in spans]\n        deps = [token.dep_ for token in sent]\n\n        # limit our example to simple sentences with one subject and object\n        if (deps.count('obj') + deps.count('dobj')) != 1\\\n                or (deps.count('subj') + deps.count('nsubj')) != 1:\n            continue\n\n        for token in sent:\n            if token.dep_ not in ('obj', 'dobj'):  # identify object nodes\n                continue\n            subject = [w for w in token.head.lefts if w.dep_\n                       in ('subj', 'nsubj')]  # identify subject nodes\n            if subject:\n                subject = subject[0]\n                # identify relationship by root dependency\n                relation = [w for w in token.ancestors if w.dep_ == 'ROOT']\n                if relation:\n                    relation = relation[0]\n                    # add adposition or particle to relationship\n#                     if relation.nbor(1).pos_ in ('ADP', 'PART'):\n#                         relation = ' '.join((str(relation), str(relation.nbor(1))))\n                else:\n                    relation = 'unknown'\n\n                subject, subject_type = refine_ent(subject, sent)\n                token, object_type = refine_ent(token, sent)\n\n                ent_pairs.append([str(subject), str(relation), str(token),\n                                  str(subject_type), str(object_type)])\n\n    ent_pairs = [sublist for sublist in ent_pairs\n                          if not any(str(ent) == '' for ent in sublist)]\n    pairs = pd.DataFrame(ent_pairs, columns=['subject', 'relation', 'object',\n                                             'subject_type', 'object_type'])\n    print('Entity pairs extracted:', str(len(ent_pairs)))\n\n    return pairs","f3b789bf":"nlp.max_length=1000000000","ab7ccaf6":"pairs=get_entity_pairs(text_list)","c7b91ca8":"pd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.max_colwidth', -1)","c3b9ea49":"print(pairs)","322df811":"pairs['relation'].value_counts()[:50]","faa9803e":"import networkx as nx\nimport matplotlib.pyplot as plt\n\n\ndef draw_kg(pairs):\n    k_graph = nx.from_pandas_edgelist(pairs, 'subject', 'object',\n            create_using=nx.MultiDiGraph())\n    node_deg = nx.degree(k_graph)\n    layout = nx.spring_layout(k_graph, k=0.15, iterations=20)\n    plt.figure(num=None, figsize=(120, 90), dpi=80)\n    nx.draw_networkx(\n        k_graph,\n        node_size=[int(deg[1]) * 500 for deg in node_deg],\n        arrowsize=20,\n        linewidths=1.5,\n        pos=layout,\n        edge_color='red',\n        edgecolors='black',\n        node_color='white',\n        )\n    labels = dict(zip(list(zip(pairs.subject, pairs.object)),\n                  pairs['relation'].tolist()))\n    nx.draw_networkx_edge_labels(k_graph, pos=layout, edge_labels=labels,\n                                 font_color='red')\n    plt.axis('off')\n    plt.show()","90ed5ed8":"\n# def draw_kg(pairs,c1='red',c2='blue',c3='yellow'):\n#     k_graph = nx.from_pandas_edgelist(pairs, 'subject', 'object',\n#             create_using=nx.MultiDiGraph())\n  \n#     node_deg = nx.degree(k_graph)\n#     degrees= list(k_graph.out_degree())\n#     print(degrees)\n#     layout = nx.spring_layout(k_graph, k=0.15, iterations=20)\n#     plt.figure(num=None, figsize=(50, 40), dpi=80)\n#     nx.draw_networkx(\n#         k_graph,\n#         node_size=[int(deg[1]) * 500 for deg in node_deg],\n#         arrowsize=20,\n#         linewidths=1.5,\n#         pos=layout,\n#         edge_color=c1,\n#         edgecolors=c2,\n#         node_color=c3,\n#         )\n#     labels = dict(zip(list(zip(pairs.subject, pairs.object)),\n#                   pairs['relation'].tolist()))\n#     nx.draw_networkx_edge_labels(k_graph, pos=layout, edge_labels=labels,\n#                                  font_color='red')\n#     plt.axis('off')\n#     plt.show()","0dd8cc29":"draw_kg(pairs)","c511c74a":"!pip install markov_clustering[drawing]","81eb8450":"import markov_clustering as mcl","6ddd1f8d":"k_graph = nx.from_pandas_edgelist(pairs, 'subject', 'object',\n            create_using=nx.MultiDiGraph())\nadj_matrix = nx.to_numpy_matrix(k_graph)\nres = mcl.run_mcl(adj_matrix)\nclusters = mcl.get_clusters(res)","f28bf1c2":"print(clusters)","4fcef3e9":"def MCL(pairs):\n    k_graph = nx.from_pandas_edgelist(pairs, 'subject', 'object',\n            create_using=nx.MultiDiGraph())\n    adj_matrix = nx.to_numpy_matrix(k_graph)\n    res = mcl.run_mcl(adj_matrix)\n    clusters = mcl.get_clusters(res)\n#     mcl.drawing.draw_graph(adj_matrix, clusters, edge_color=\"red\",node_size=40, with_labels=True)\n    mcl.draw_graph(adj_matrix, clusters, node_size=150, with_labels=True, edge_color=\"silver\")","0158bc9a":"MCL(pairs)","134592d4":"Checking for missing data","ad241aac":"**Constructing knowldege graphs**","c3e2f469":"Performing Markov Clustering on Knowledge Graph","55e719f4":"Most common words in the abstracts","3692f1c0":"**A directed multigraph network is created with nodes sized in proportion to degree centrality.**"}}