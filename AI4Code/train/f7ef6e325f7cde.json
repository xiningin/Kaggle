{"cell_type":{"000f7964":"code","ce68dfbb":"code","8d8adf34":"code","03f53926":"code","121fc314":"code","47a27fa8":"code","e279a425":"code","e13cddb7":"code","01635f92":"code","e615c7e6":"code","80d13966":"code","4301212a":"code","11b29c2a":"markdown","cdbe909f":"markdown","c6419cc5":"markdown","766c78ba":"markdown","1a277e3d":"markdown","29846ad3":"markdown","c850a6eb":"markdown"},"source":{"000f7964":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn\nimport seaborn as sns","ce68dfbb":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndf = pd.read_csv(\"\/kaggle\/input\/health-care-data-set-on-heart-attack-possibility\/heart.csv\")\ndf.head()","8d8adf34":"df[\"target\"].value_counts() #165 positive and 138 negative","03f53926":"sns.heatmap(df.corr(),cmap=\"YlGn\")\n# As we can see, the cp, thalach, slope are more correlated.","121fc314":"from sklearn.model_selection import train_test_split\ny,x = df[\"target\"], df.drop(\"target\", axis=1)\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)","47a27fa8":"from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve","e279a425":"from sklearn.linear_model import LogisticRegression\n# We have to determine several hypers first :\n# Since data is noisy, we should use regularization. C=0.1 can enlarge the regularization.\n# However, setting a grid would be nice too. \n# Panelty is default to l2, a strong regularization.\n# Solver should be \"liblinar\" since it is a small dataset and we only have two classes\nfor c in [0.8,0.5,0.2,0.1,0.05,0.02,0.01]:\n    logit_model = LogisticRegression(C=c, solver=\"liblinear\")\n    logit_model.fit(x_train, y_train)\n    y_pred = logit_model.predict(x_test)\n    print(\"Accuracy for c = %f is %f\" %(c, accuracy_score(y_test, y_pred)))\n#We get the best result for c=0.1\/0.2\/0.5 #We will choose 0.2 here","e13cddb7":"#For SVM, we should scale the data.\nfrom sklearn import preprocessing\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV\n#x_train_scaled = preprocessing.StandardScaler().fit(x_train)\n# Set the parameters by cross-validation\ntuned_parameters = [{'kernel': ['rbf','sigmoid'], 'gamma': [1e-3, 1e-4],\n                     'C': [1, 10, 100, 1000]},\n                    {'kernel': ['linear'], 'C': [0.1, 1, 10, 100, 1000]}]\nclf = GridSearchCV(SVC(), tuned_parameters, scoring=\"precision\")\nclf.fit(x_train, y_train)\nprint(\"Best Parameters Are\")\nprint(clf.best_params_)","01635f92":"#Based on the best parameters we can get the result\ny_pred = clf.predict(x_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Accuracy score for SVC is %f\" %(accuracy_score(y_test, y_pred)))\n#We can an accuracy of 86.88% which is slightly better than logistic regression.","e615c7e6":"#Agian we will use grid search here\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(random_state=42)\nn_estimators = [100, 300, 500, 700, 900]\nmax_depth = [5, 8, 10, 15, 25, 30]\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [1, 2, 5] \n\nhyperF = dict(n_estimators = n_estimators, max_depth = max_depth,  \n              min_samples_split = min_samples_split, \n             min_samples_leaf = min_samples_leaf)\n\ngridF = GridSearchCV(rfc, hyperF,  verbose = 1, n_jobs = -1)\ncv_rfc = gridF.fit(x_train, y_train)\ncv_rfc.best_params_","80d13966":"rfc = RandomForestClassifier(random_state=42,max_depth=5,min_samples_leaf=5,min_samples_split=2,n_estimators=100)\nrfc.fit(x_train,y_train)\ny_pred = rfc.predict(x_test)\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Accuracy score for RFC is %f\" %(accuracy_score(y_test, y_pred)))\n#Agian we get the same results as SVC\n#However, for medical purpose, this result is better since FN value is smaller.","4301212a":"#Since we dont have much data, the key point here is to avoid overfitting the training set. \n\nimport tensorflow.keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nmodel = Sequential()\nmodel.add(Dense(50, input_dim=x.shape[1], activation='relu',kernel_initializer='random_normal'))\nmodel.add(Dense(25,activation='relu',kernel_initializer='random_normal'))\nmodel.add(Dense(25,activation='relu',kernel_initializer='random_normal'))\nmodel.add(Dense(1,activation='sigmoid',kernel_initializer='random_normal'))\nmodel.compile(loss='binary_crossentropy', \n              optimizer=tensorflow.keras.optimizers.Adam(),\n              metrics =['accuracy'])\nmonitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, \n    patience=30, verbose=1, mode='auto', restore_best_weights=True)\nmodel.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=2,epochs=1000)","11b29c2a":"## 2. Support Vector Classifier with GridSearch","cdbe909f":"The NN models shows better results with an accuracy of 86.89%, highest result. \n\nI got the result of 88.52% when using the Jupyter notebook.","c6419cc5":"1) age\n\n2) sex\n\n3) chest pain type (4 values)\n\n4) resting blood pressure\n\n5) serum cholestoral in mg\/dl\n\n6) fasting blood sugar > 120 mg\/dl\n\n7) resting electrocardiographic results (values 0,1,2)\n\n8) maximum heart rate achieved\n\n9) exercise induced angina\n\n10) oldpeak = ST depression induced by exercise relative to rest\n\n11) the slope of the peak exercise ST segment\n\n12) number of major vessels (0-3) colored by flourosopy\n\n13) thal: 0 = normal; 1 = fixed defect; 2 = reversable defect\n\n14) target: 0= less chance of heart attack 1= more chance of heart attack","766c78ba":"## 1. Logistic Regression","1a277e3d":"Now we are going to use different models for classification\nBefore we really start, we can predict the performance by heuristic first : \n1. Logistic Regression can be a classic model for comparison.\n2. SVM (Or SVC) is another important choice since we have a lot of important features.\n3. Ensembled Trees is another choice: since we have a lot of noise in the data, random forest is a better choice compared to boosted trees. \n4. At last, we can try the neural network ","29846ad3":"## 4. Neural Network","c850a6eb":"## 3. Random Forest"}}