{"cell_type":{"52449ba3":"code","66d3b9e5":"code","92841fc4":"code","ced1658b":"code","46441758":"code","5b9096a0":"code","b2333bce":"code","25bcd6e2":"code","952045f3":"code","95ace53f":"code","e51e2667":"code","6d843ce1":"code","e3749d03":"code","77836cb5":"code","b4016bbd":"code","e81c0043":"code","d0e9a8c0":"code","24e8c65c":"code","ddb41f04":"code","80715fcd":"code","cee46a48":"code","4db06f49":"code","0708528b":"code","4c09a141":"markdown","6df156e7":"markdown","53b3e2dd":"markdown","aa614680":"markdown","e8de3de8":"markdown","98913782":"markdown","a9b4c0fd":"markdown","8d94a400":"markdown"},"source":{"52449ba3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","66d3b9e5":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nimport re\nfrom gensim.models import Word2Vec\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom wordcloud import WordCloud,STOPWORDS\n\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import hamming_loss\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.model_selection import train_test_split\nfrom skmultilearn.problem_transform import BinaryRelevance\nfrom sklearn.naive_bayes import MultinomialNB\nfrom skmultilearn.problem_transform import ClassifierChain\nfrom skmultilearn.problem_transform import LabelPowerset","92841fc4":"train_data = pd.read_csv('\/kaggle\/input\/topic-modeling-for-research-articles\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/topic-modeling-for-research-articles\/test.csv')\ntrain_data.head()\n","ced1658b":"test_data.head()","46441758":"print(train_data.shape)\nprint(test_data.shape)","5b9096a0":"x=train_data.iloc[:,3:].sum()\nrowsums=train_data.iloc[:,2:].sum(axis=1)\nno_label_count = 0\nfor sum in rowsums.items():\n    if sum==0:\n        no_label_count +=1\n\nprint(\"Total number of articles = \",len(train_data))\nprint(\"Total number of articles without label = \",no_label_count)\nprint(\"Total labels = \",x.sum())","b2333bce":"print(\"Check for missing values in Train dataset\")\nprint(train_data.isnull().sum().sum())\nprint(\"Check for missing values in Test dataset\")\nnull_check=test_data.isnull().sum()\nprint(null_check)","25bcd6e2":"x=train_data.iloc[:,3:].sum()\n#plot\nplt.figure(figsize=(12,12))\nax= sns.barplot(x.index, x.values, alpha=0.8)\nplt.title(\"Class counts\")\nplt.ylabel('# of Occurrences', fontsize=12)\nplt.xlabel('Label ', fontsize=12)\n\nrects = ax.patches\nlabels = x.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()\/2, height + 5, label, ha='center', va='bottom')\n\nplt.show()\n","952045f3":"x=rowsums.value_counts()\n\n#plot\nplt.figure(figsize=(12,12))\nax = sns.barplot(x.index, x.values, alpha=0.8)\nplt.title(\"Multiple tags per article\")\nplt.ylabel('# of Occurrences', fontsize=12)\nplt.xlabel('# of Labels ', fontsize=12)\n\n#adding the text labels\nrects = ax.patches\nlabels = x.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()\/2, height + 5, label, ha='center', va='bottom')\n\nplt.show()\n","95ace53f":"train_data['Text']=train_data['TITLE']+' '+train_data['ABSTRACT']","e51e2667":"train_data.drop(columns=['TITLE','ABSTRACT'], inplace=True)\ntrain_data.head(10)","6d843ce1":"plt.figure(figsize=(12,12))\n#text = description_category.description.values\ncloud = WordCloud(stopwords=STOPWORDS, background_color='black', collocations=False, width=2500, height=1800).generate(\" \".join(train_data['Text']))\nplt.axis('off')\nplt.imshow(cloud)","e3749d03":"train_data['Text'][5]","77836cb5":"#Remove Stopwords\nstop_words = set(stopwords.words('english'))\n\n# function to remove stopwords\ndef remove_stopwords(text):\n    no_stopword_text = [w for w in text.split() if not w in stop_words]\n    return ' '.join(no_stopword_text)\n\ntrain_data['Text'] = train_data['Text'].apply(lambda x: remove_stopwords(x))","b4016bbd":"\nstemmer = SnowballStemmer(\"english\")\ndef stemming(sentence):\n    stemSentence = \"\"\n    for word in sentence.split():\n        stem = stemmer.stem(word)\n        stemSentence += stem\n        stemSentence += \" \"\n    stemSentence = stemSentence.strip()\n    return stemSentence\n\ntrain_data['Text'] = train_data['Text'].apply(stemming)","e81c0043":"train_data['Text'][5]","d0e9a8c0":"categories=['Computer Science', 'Physics', 'Mathematics', 'Statistics', 'Quantitative Biology', 'Quantitative Finance'] \ntrain_data[categories].head()","24e8c65c":"#split the data\n\nx_train, x_test, y_train, y_test = train_test_split(train_data['Text'], train_data[categories], test_size=0.2, random_state=40, shuffle=True)\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\n\n","ddb41f04":"# using binary relevance\n\n\n\npipeline = Pipeline([\n                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n                ('clf', BinaryRelevance(MultinomialNB())),\n            ])\n\n# train\npipeline.fit(x_train, y_train)\n\n# predict\npredictions = pipeline.predict(x_test)\n\n\nfrom sklearn.metrics import accuracy_score\nprint('Accuracy = ', accuracy_score(y_test,predictions))\nprint('F1 score is ',f1_score(y_test, predictions, average=\"micro\"))\nprint('Hamming Loss is ', hamming_loss(y_test, predictions))","80715fcd":"# using binary relevance with Logistic Regression\n\npipeline = Pipeline([\n                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n                ('clf', BinaryRelevance(LogisticRegression(solver='sag'))),\n            ])\n\n# train\npipeline.fit(x_train, y_train)\n\n# predict\npredictions = pipeline.predict(x_test)\n\n\nfrom sklearn.metrics import accuracy_score\nprint('Accuracy = ', accuracy_score(y_test,predictions))\nprint('F1 score is ',f1_score(y_test, predictions, average=\"micro\"))\nprint('Hamming Loss is ', hamming_loss(y_test, predictions))","cee46a48":"# using classifier chains with MultinomialNB\n\n\n# initialize classifier chains multi-label classifier\n# with a gaussian naive bayes base classifier\npipeline = Pipeline([\n                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n                ('clf', ClassifierChain(MultinomialNB())),\n            ])\n# ClassifierChain(GaussianNB())\n\n# train\npipeline.fit(x_train, y_train)\n\n# predict\npredictions = pipeline.predict(x_test)\n\nprint('Accuracy = ', accuracy_score(y_test,predictions))\nprint('F1 score is ',f1_score(y_test, predictions, average=\"micro\"))\nprint('Hamming Loss is ', hamming_loss(y_test, predictions))","4db06f49":"pipeline = Pipeline([\n                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n                ('clf', ClassifierChain(LogisticRegression(solver='sag'))),\n            ])\n# ClassifierChain(GaussianNB())\n\n# train\npipeline.fit(x_train, y_train)\n\n# predict\npredictions = pipeline.predict(x_test)\n\nprint('Accuracy = ', accuracy_score(y_test,predictions))\nprint('F1 score is ',f1_score(y_test, predictions, average=\"micro\"))\nprint('Hamming Loss is ', hamming_loss(y_test, predictions))","0708528b":"# using Label Powerset\n\n# initialize label powerset multi-label classifier\npipeline = Pipeline([\n                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n                ('clf',  LabelPowerset(LogisticRegression())),\n            ])\n#classifier = LabelPowerset(LogisticRegression())\n# train\npipeline.fit(x_train, y_train)\n# predict\npredictions = pipeline.predict(x_test)\n# accuracy\nprint(\"Accuracy = \",accuracy_score(y_test,predictions))\nprint('F1 score is ',f1_score(y_test, predictions, average=\"micro\"))\nprint('Hamming Loss is ', hamming_loss(y_test, predictions))","4c09a141":"# Label Powerset","6df156e7":"# Classifier Chains","53b3e2dd":"# Binary Relevace","aa614680":"Please check this [article](https:\/\/medium.com\/analytics-vidhya\/an-introduction-to-multi-label-text-classification-b1bcb7c7364c?sk=8a30075009552cfd4a7534663edaed7e) for detailed explanation.","e8de3de8":"As the dataset is imbalanced, MLSMOTE technique can be used for sampling. As of now, I am not using any sampling techniques.","98913782":"We have to remove symbols and punctuations","a9b4c0fd":"# MultiLabel Classification","8d94a400":"# References\n\n* https:\/\/www.analyticsvidhya.com\/blog\/2017\/08\/introduction-to-multi-label-classification\/\n\n* https:\/\/towardsdatascience.com\/journey-to-the-center-of-multi-label-classification-384c40229bff"}}