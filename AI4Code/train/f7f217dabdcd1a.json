{"cell_type":{"8435001a":"code","4b1b433c":"code","57ae68b8":"code","cfccd86a":"code","aae90ae2":"code","302a891c":"code","2496bedf":"code","6c88913c":"code","84fdf023":"code","6ee11cd9":"code","8dcd7cfd":"code","9f4e7d96":"code","74761029":"code","9b448cf2":"code","ac2e1ab9":"code","3b0c6139":"code","637ab3e8":"code","02ccfecb":"code","6fe97089":"code","3730cdad":"code","0adf45c0":"code","bf2df9f0":"code","5df4778e":"code","553155b7":"code","6d1ab766":"code","77ba0534":"code","818e0d5f":"code","a06eea7b":"code","065314f3":"code","edad4b44":"code","419fe2c2":"code","fad27316":"code","0dada9e1":"code","95cbccb8":"code","63859aa2":"code","6ff19932":"code","e8e5a069":"code","0894fa31":"code","06b9c148":"code","4bf978dc":"code","bbeb4671":"code","bc23df2b":"code","e8d78f32":"code","ea573df5":"code","a90c5ff2":"code","a07223cd":"code","4e236631":"code","b9909aa5":"markdown","5a024bb3":"markdown","3cb8aeb1":"markdown","06f75064":"markdown","a95e207d":"markdown","cb6886f6":"markdown","d9cfc237":"markdown","9797088b":"markdown","e0152912":"markdown","e48e3b97":"markdown","a7664a6e":"markdown","c76fc3f1":"markdown","2656f07c":"markdown","366033f7":"markdown","178464ce":"markdown"},"source":{"8435001a":"# Data exploration\nimport numpy as np\nimport math\nimport pandas as pd\nimport datetime as dt\nimport scipy\n\n# Time Series Features\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.graphics.tsaplots import plot_acf\n\n# Time Series Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Additional informations\nimport time\nimport holidays\nimport dateutil.easter as easter\n\n# Patching sklearn\n!pip install scikit-learn-intelex\nfrom sklearnex import patch_sklearn\npatch_sklearn()\n\n# Model Building \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import make_scorer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\n\n# XGBoost\nfrom xgboost import XGBRegressor\n\n# Catboost\nimport catboost\nfrom catboost import CatBoostRegressor, Pool\nimport shap\n\n# Lgbm\nfrom lightgbm import LGBMRegressor\nimport lightgbm as lgb\n\n# Settings\nsns.set(rc={'figure.figsize':(26, 8)})\ncustom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\nsns.set_theme(style=\"ticks\", rc=custom_params)","4b1b433c":"train_data = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/train.csv', parse_dates=True)\ntest_data = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/test.csv\", index_col=0, parse_dates=True)\n\nprint(f'Train Data rows: {train_data.shape[0]} \\nTrain Data Columns: {train_data.shape[1]}\\n')\nprint(f'Test Data rows: {test_data.shape[0]} \\nTest Data Columns: {test_data.shape[1]}')\n\n# Date formatting\ntrain_data['date'], test_data['date'] = (pd.to_datetime(train_data['date']), \n                                         pd.to_datetime(test_data['date']))\n\nprint('\\n','-'*22, sep='')\nprint(f'\\nTrain Data start date:', train_data['date'].min().strftime('%B %d, %Y'), \n      '\\nTrain Data end date: ', train_data['date'].max().strftime('%B %d, %Y'), '\\n')\n\nprint(f'Test Data start date:', test_data['date'].min().strftime('%B %d, %Y'), \n      '\\nTest Data end date: ', test_data['date'].max().strftime('%B %d, %Y'))","57ae68b8":"# Creating support lists\n\ndef check_consistency():\n    train_country_list, test_country_list = train_data['country'].unique(), test_data['country'].unique()\n    train_store_list, test_store_list = train_data['store'].unique(), test_data['store'].unique() \n    train_product_list, test_product_list = train_data['product'].unique(), test_data['product'].unique()\n    \n    result = []\n    \n    if train_country_list.all() == test_country_list.all():\n        print(\"Consistency test 1, countries: passed\")\n        result.append(train_country_list)\n    else: \n        print(\"Consistency test 1, countries: failed\")\n        \n    if train_store_list.all() == test_store_list.all():\n        print(\"Consistency test 2, stores: passed\")\n        result.append(train_store_list)\n    else: \n        print(\"Consistency test 2, stores: failed\")\n        \n    if train_product_list.all() == test_product_list.all():\n        print(\"Consistency test 3, products: passed\")\n        result.append(train_product_list)\n    else: \n        print(\"Consistency test 3, products: failed\")\n    \n    return result\n\ntrain_country_list, train_store_list, train_product_list = check_consistency()\n\nprint('\\nCountries: ', train_country_list, '\\n',\n      'Stores: ', train_store_list, '\\n',\n      'Products: ', train_product_list, sep='')","cfccd86a":"# Checking NaNs\n\ndef check_na(train_data, test_data):\n    if train_data.isnull().values.any() == False:\n        print('No missing values detected in Train Data')\n    else: \n        print('Missing values detected in Train Data')\n    \n    if test_data.isnull().values.any() == False:\n        print('No missing values detected in Test Data')\n    else: \n        print('Missing values detected in Test Data')\n    \ncheck_na(train_data, test_data)","aae90ae2":"# Adding holidays info\ndef add_holidays(train_data):\n    finland_holidays = holidays.CountryHoliday('FI', years = list(range(2015, 2020)))\n    norway_holidays  = holidays.CountryHoliday('NO', years = list(range(2015, 2020)))\n    sweden_holidays  = holidays.CountryHoliday('SE', years = list(range(2015, 2020)))\n\n    holiday_d = finland_holidays.copy()\n    holiday_d.update(norway_holidays)\n    holiday_d.update(sweden_holidays)\n\n    train_data['name'] = train_data['date'].map(holiday_d)\n    train_data['holiday'] = np.where(train_data['name'].isnull(), 0, 1)\n    train_data.drop(['name', 'row_id'], axis = 1, inplace = True)\n    \n    test_data['name'] = test_data['date'].map(holiday_d)\n    test_data['holiday'] = np.where(test_data['name'].isnull(), 0, 1)\n    test_data.drop(['name'], axis = 1, inplace = True)\n    \n    print('Holidays added for years 2015-2018, Train Data \\nHolidays added for year 2019, Test Data')\n    \n    return train_data, test_data\n\ntrain_data, test_data = add_holidays(train_data)","302a891c":"# Thanks to @AmbrosM for the hints\n\ndef engineer_more(df):\n    \"\"\"Return a new dataframe with more engineered features\"\"\"\n    new_df = df\n    # End of year\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"dec{d}\":\n                                      (df.date.dt.month == 12) & (df.date.dt.day == d)\n                                      for d in range(24, 32)}),\n                        pd.DataFrame({f\"n-dec{d}\":\n                                      (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Norway')\n                                      for d in range(24, 32)}),\n                        pd.DataFrame({f\"f-jan{d}\":\n                                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Finland')\n                                      for d in range(1, 14)}),\n                        pd.DataFrame({f\"jan{d}\":\n                                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Norway')\n                                      for d in range(1, 10)}),\n                        pd.DataFrame({f\"s-jan{d}\":\n                                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Sweden')\n                                      for d in range(1, 15)})],\n                       axis=1)\n    \n    # May\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"may{d}\":\n                                      (df.date.dt.month == 5) & (df.date.dt.day == d) \n                                      for d in list(range(1, 10))}), #  + list(range(17, 25))\n                        pd.DataFrame({f\"may{d}\":\n                                      (df.date.dt.month == 5) & (df.date.dt.day == d) & (df.country == 'Norway')\n                                      for d in list(range(19, 26))})],\n                       axis=1)\n    \n    # June and July\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"june{d}\":\n                                      (df.date.dt.month == 6) & (df.date.dt.day == d) & (df.country == 'Sweden')\n                                      for d in list(range(8, 14))}),\n                        #pd.DataFrame({f\"june{d}\":\n                        #              (df.date.dt.month == 6) & (df.date.dt.day == d) & (df.country == 'Norway')\n                        #              for d in list(range(22, 31))}),\n                        #pd.DataFrame({f\"july{d}\":\n                        #              (df.date.dt.month == 7) & (df.date.dt.day == d) & (df.country == 'Norway')\n                        #              for d in list(range(1, 3))})],\n                       ],\n                       axis=1)\n    \n    # Last Wednesday of June\n    wed_june_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-24')),\n                                         2016: pd.Timestamp(('2016-06-29')),\n                                         2017: pd.Timestamp(('2017-06-28')),\n                                         2018: pd.Timestamp(('2018-06-27')),\n                                         2019: pd.Timestamp(('2019-06-26'))})\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"wed_june{d}\": \n                                      (df.date - wed_june_date == np.timedelta64(d, \"D\")) & (df.country != 'Norway')\n                                      for d in list(range(-4, 6))})],\n                       axis=1)\n    \n    # First Sunday of November\n    sun_nov_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-11-1')),\n                                         2016: pd.Timestamp(('2016-11-6')),\n                                         2017: pd.Timestamp(('2017-11-5')),\n                                         2018: pd.Timestamp(('2018-11-4')),\n                                         2019: pd.Timestamp(('2019-11-3'))})\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"sun_nov{d}\": \n                                      (df.date - sun_nov_date == np.timedelta64(d, \"D\")) & (df.country != 'Norway')\n                                      for d in list(range(0, 9))})],\n                       axis=1)\n    \n    # First half of December (Independence Day of Finland, 6th of December)\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"dec{d}\":\n                                      (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Finland')\n                                      for d in list(range(6, 14))})],\n                       axis=1)\n\n    # Easter\n    easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"easter{d}\": \n                                      (df.date - easter_date == np.timedelta64(d, \"D\"))\n                                      for d in list(range(-2, 11)) + list(range(40, 48)) + list(range(50, 59))})],\n                       axis=1)\n    \n    return new_df.astype(np.float32, errors='ignore')\n\n\ntrain_data, test_data = engineer_more(train_data), engineer_more(test_data)","2496bedf":"# adding GDP \ngdp_exponent = 1.2121103201489674 # see https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-03-linear-model for an explanation\ngdp_df = pd.read_csv('..\/input\/gdp-20152019-finland-norway-and-sweden\/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv')\ngdp_df.set_index('year', inplace=True)\n\ndef get_gdp(row):\n    country = 'GDP_' + row.country\n    return gdp_df.loc[row.date.year, country]\n\ntrain_data['gdp'] = np.log1p(train_data.apply(get_gdp, axis=1))\ntest_data['gdp'] = np.log1p(test_data.apply(get_gdp, axis=1))","6c88913c":"# From https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-03-linear-model#Simple-feature-engineering-(without-holidays)\ndef FourierFeatures(df):\n    # temporary one hot encoding\n    for product in ['Kaggle Mug', 'Kaggle Hat']:\n        df[product] = df['product'] == product\n    \n    # The three products have different seasonal patterns\n    dayofyear = df.date.dt.dayofyear\n    for k in range(1, 3):\n        df[f'sin{k}'] = np.sin(dayofyear \/ 365 * 2 * math.pi * k)\n        df[f'cos{k}'] = np.cos(dayofyear \/ 365 * 2 * math.pi * k)\n        df[f'mug_sin{k}'] = df[f'sin{k}'] * df['Kaggle Mug']\n        df[f'mug_cos{k}'] = df[f'cos{k}'] * df['Kaggle Mug']\n        df[f'hat_sin{k}'] = df[f'sin{k}'] * df['Kaggle Hat']\n        df[f'hat_cos{k}'] = df[f'cos{k}'] * df['Kaggle Hat']\n        df=df.drop([f'sin{k}', f'cos{k}'], axis=1)\n    \n    # drop temporary one hot encoding\n    df=df.drop(['Kaggle Mug','Kaggle Hat'], axis=1)\n    \n    return df\n\n# add fourier features\ntrain_data=FourierFeatures(train_data)\ntest_data=FourierFeatures(test_data)","84fdf023":"# Time series plotting\ndef plotting_ts(country = 'Finland', store = 'KaggleMart', product = 'Kaggle Mug'):\n\n    figure = sns.lineplot(data = train_data[(train_data['country'] == country) &\n                         (train_data['store'] == store) & \n                         (train_data['product'] == product)], \n                          y = 'num_sold', \n                          x = 'date').set_title(f'{product} in {store}, {country}')\n    plt.show()\n    \n\n# Time Series Decomposition\ndef decomposition_ts(country = 'Finland', store = 'KaggleMart', product = 'Kaggle Mug'):\n    \n    table = train_data[(train_data['country'] == country) &\n                         (train_data['store'] == store) & \n                         (train_data['product'] == product)][['num_sold', 'date']]\n    table = table.set_index('date')\n\n    plt.rc('font',size=15)\n    result = seasonal_decompose(table, model='multiplicative')\n    print('\\nTime Series Decomposition\\n')\n    result.plot()\n    \n    return result\n\n\n# Auto Correlation Function (ACF) plot\ndef acf_ts(country = 'Finland', store = 'KaggleMart', product = 'Kaggle Mug'):\n    \n    table = train_data[(train_data['country'] == country) &\n                         (train_data['store'] == store) & \n                         (train_data['product'] == product)][['num_sold', 'date']]\n    table = table.set_index('date')\n\n    plt.rc('font',size=15)\n    plot_acf(table)\n    print('\\nAuto Correlation Function plot\\n')\n    plt.show()   \n\n    \nplotting_ts()\nacf_ts()\nresult = decomposition_ts()","6ee11cd9":"# Country selection\ncountry = 'Finland'\n\n# Store selection\nstore = 'KaggleRama'\n\n# Product selection\nproduct = 'Kaggle Sticker'\n\nplotting_ts(country, store, product)\nacf_ts(country, store, product)\nresult = decomposition_ts(country, store, product)","8dcd7cfd":"x = result.resid.index\ny = result.resid.values\n\nsns.lineplot(y = y, x = x).set_title(f'Possible outliers in {product} - {store}, {country}')\nplt.text('2016-01', 1.34, \"Possible Outlier\", horizontalalignment='left', size='medium', color='Red', weight='semibold')\nplt.text('2016-01', 0.75, \"Possible Outlier\", horizontalalignment='left', size='medium', color='Red', weight='semibold')\nplt.text('2017-01', 1.25, \"Possible Outlier\", horizontalalignment='left', size='medium', color='Red', weight='semibold')\nplt.text('2017-01', 0.78, \"Possible Outlier\", horizontalalignment='left', size='medium', color='Red', weight='semibold')\nplt.text('2019-01', 0.77, \"Possible Outlier\", horizontalalignment='right',size='medium', color='Red', weight='semibold')\n\nplt.show()","9f4e7d96":"# Total Sales\ntot_comp_tab = train_data.groupby(['date', 'store']).sum()\nsns.lineplot(data = tot_comp_tab, \n             x = 'date', \n             y = 'num_sold', \n             hue = 'store').set_title('KaggleMart VS KaggleRama Total Sales')\nplt.show()","74761029":"fig, axes = plt.subplots(3, 1, figsize=(26,18))\nfig.suptitle('Country Comparison')\n\n# Finland\nfin_comp_tab = train_data.groupby(['date', 'country', 'store']).sum().reset_index()\nfin_comp_tab = fin_comp_tab[fin_comp_tab['country']=='Finland']\n\n# Norway\nnor_comp_tab = train_data.groupby(['date', 'country', 'store']).sum().reset_index()\nnor_comp_tab = nor_comp_tab[nor_comp_tab['country']=='Norway']\n\n# Sweden\nswe_comp_tab = train_data.groupby(['date', 'country', 'store']).sum().reset_index()\nswe_comp_tab = swe_comp_tab[swe_comp_tab['country']=='Sweden']\n\nsns.lineplot(data = fin_comp_tab, \n             ax=axes[0],\n             x = 'date', \n             y = 'num_sold', \n             hue = 'store')\naxes[0].set_title('KaggleMart VS KaggleRama Total Sales - Finland')\n\nsns.lineplot(data = nor_comp_tab, \n             ax=axes[1], \n             x = 'date', \n             y = 'num_sold', \n             hue = 'store')\naxes[1].set_title('KaggleMart VS KaggleRama Total Sales - Norway')\n\nsns.lineplot(data = fin_comp_tab, \n             ax=axes[2],  \n             x = 'date', \n             y = 'num_sold', \n             hue = 'store')\naxes[2].set_title('KaggleMart VS KaggleRama Total Sales - Sweden')\n\nplt.show()","9b448cf2":"fig, axes = plt.subplots(3, 1, figsize=(26,18))\nfig.suptitle('Product Comparison')\n    \n# Kaggle Mug\nmug_comp_tab = train_data.groupby(['date', 'product', 'store']).sum().reset_index()\nmug_comp_tab = mug_comp_tab[mug_comp_tab['product']=='Kaggle Mug']\n\n# Kaggle Hat\nhat_comp_tab = train_data.groupby(['date', 'product', 'store']).sum().reset_index()\nhat_comp_tab = hat_comp_tab[hat_comp_tab['product']=='Kaggle Hat']\n\n# Kaggle Sticker\nsticker_comp_tab = train_data.groupby(['date', 'product', 'store']).sum().reset_index()\nsticker_comp_tab = sticker_comp_tab[sticker_comp_tab['product']=='Kaggle Sticker']\n\nsns.lineplot(data = mug_comp_tab, \n             ax=axes[0],\n             x = 'date', \n             y = 'num_sold', \n             hue = 'store')\naxes[0].set_title('KaggleMart VS KaggleRama Total Sales - Kaggle Mug')\n\nsns.lineplot(data = hat_comp_tab, \n             ax=axes[1], \n             x = 'date', \n             y = 'num_sold', \n             hue = 'store')\naxes[1].set_title('KaggleMart VS KaggleRama Total Sales - Kaggle Hat')\n\nsns.lineplot(data = sticker_comp_tab, \n             ax=axes[2],  \n             x = 'date', \n             y = 'num_sold', \n             hue = 'store')\naxes[2].set_title('KaggleMart VS KaggleRama Total Sales - Kaggle Sticker')\n\nplt.show()","ac2e1ab9":"# Features Encoding\ndef var_encoding(train, test):\n    dummy = ['country', 'store', 'product']\n\n    train = pd.get_dummies(train, columns=dummy)\n    test = pd.get_dummies(test, columns=dummy)\n    \n    return train, test\n\ntrain_data, test_data = var_encoding(train_data, test_data)","3b0c6139":"def further_features(train, test):\n    train['year'], test['year'] = train['date'].dt.year, test['date'].dt.year\n    train['quarter'], test['quarter'] = train['date'].dt.quarter, test['date'].dt.quarter\n    train['month'], test['month'] = train['date'].dt.month, test['date'].dt.month \n    train['week'], test['week'] = train['date'].dt.isocalendar().week, test['date'].dt.isocalendar().week \n    train['day'], test['day'] = train['date'].dt.day, test['date'].dt.day  \n    train['weekday'], test['weekday'] = train['date'].dt.weekday, test['date'].dt.weekday\n    train['dayofyear'], test['dayofyear'] = train['date'].dt.dayofyear, test['date'].dt.dayofyear\n    train['is_weekend'] = np.where((train['weekday'] == 5) | (train['weekday'] == 6), 1, 0)\n    test['is_weekend']  = np.where((test['weekday'] == 5) | (test['weekday'] == 6), 1, 0)\n    \n    return train, test\n\ntrain_data, test_data = further_features(train_data, test_data)","637ab3e8":"# Train test spliting\ntrain_set = train_data[train_data['date'] <= '2017-12-31'].drop(['num_sold'], axis = 1)\ny_train = train_data[train_data['date'] <= '2017-12-31'].loc[:,'num_sold']\n\nvalidation_set = train_data[train_data['date'] > '2017-12-31'].drop(['num_sold'], axis = 1)\ny_val = train_data[train_data['date'] > '2017-12-31'].loc[:,'num_sold']\n\ntest_set = test_data\n\n# Date adjustment\ntrain_set.drop('date', axis = 1, inplace = True)\nvalidation_set.drop('date', axis = 1, inplace = True)\ntest_set.drop('date', axis = 1, inplace = True)","02ccfecb":"# Data Standardization\ndef scaling_feat(train_set, validation_set, test_set):\n    print(f'Dimensions before scaling: \\ntrain_set: {train_set.shape} \\nvalidation_set: {validation_set.shape} \\ntest_set: {test_set.shape}')\n    \n    scaler = StandardScaler()\n\n    train_set_scaled = scaler.fit_transform(train_set)\n    validation_set_scaled = scaler.transform(validation_set)\n    test_set_scaled = scaler.transform(test_set)\n\n    train_set = pd.DataFrame(train_set_scaled, index=train_set.index, columns=train_set.columns)\n    validation_set = pd.DataFrame(validation_set_scaled, index=validation_set.index, columns=validation_set.columns)\n    test_set = pd.DataFrame(test_set_scaled, index=test_set.index, columns=test_set.columns)\n    \n    print(f'\\nDimensions after scaling: \\ntrain_set: {train_set.shape} \\nvalidation_set: {validation_set.shape} \\ntest_set: {test_set.shape}')\n    \n    return train_set, validation_set, test_set\n\ntrain_set, validation_set, test_set = scaling_feat(train_set, validation_set, test_set)","6fe97089":"# Creating the Score Metric\ndef SMAPE(y_true, y_pred):\n    den = (y_true + np.abs(y_pred)) \/ 200.0\n    d = np.abs(y_true - y_pred) \/ den\n    d[den == 0] = 0.0\n    return np.mean(d)\n\nSMAPE_score = make_scorer(SMAPE, greater_is_better = False)","3730cdad":"# Defining the model\n#model = RandomForestRegressor()\n#param_search = { \n#    'n_estimators': [10, 20, 50, 100, 150],\n#    'max_features': ['auto', 'sqrt', 'log2'],\n#    'max_depth' : [i for i in range(5,10)]\n#}\n#\n#\n#tscv = TimeSeriesSplit(n_splits=12)\n#gsearch = GridSearchCV(estimator=model, cv=tscv, param_grid=param_search, scoring = SMAPE_score)\n#\n#gsearch.fit(train_set, y_train)\n#\n#best_score = gsearch.best_score_\n#best_model = gsearch.best_estimator_\n#\n#print('Lowest Training SMAPE: ', round(best_score,2))","0adf45c0":"#y_true = np.array(y_val.values)\n#y_pred = best_model.predict(validation_set)\n#\n#print('Validation SMAPE: ', round(SMAPE(y_true, y_pred), 2))","bf2df9f0":"#d = {'Actual' : pd.Series(y_true), 'Forecasted' : pd.Series(y_pred)}\n#comparison_df = pd.DataFrame(d)\n#\n#sns.scatterplot(data = comparison_df, \n#             x = comparison_df.index, \n#             y = 'Forecasted',\n#             color = 'darkred')\n#sns.lineplot(data = comparison_df, \n#             x = comparison_df.index, \n#             y = 'Actual',\n#             alpha = 0.8)\n#\n#plt.show()","5df4778e":"# Final prediction\n#y_pred = best_model.predict(test_set)\n#\n# CSV Submission\n#submission = pd.DataFrame(test_data.index)\n#submission['num_sold'] = y_pred\n#submission['num_sold'] = np.round(submission['num_sold']).astype(int) \n#submission.to_csv('submission.csv', index=False)","553155b7":"complete_series = train_set.append(validation_set)\ncomplete_y = y_train.append(y_val)\n\ncomplete_series = complete_series.astype('float64', errors = 'ignore')\ncomplete_y = complete_y.astype('float64', errors = 'ignore')","6d1ab766":"# SMAPE: Symmetric mean absolute percentage error (adjusted MAPE)\n#def smape(preds, target):\n#    n = len(preds)\n#    masked_arr = ~((preds == 0) & (target == 0))\n#    preds, target = preds[masked_arr], target[masked_arr]\n#    num = np.abs(preds-target)\n#    denom = np.abs(preds)+np.abs(target)\n#    smape_val = (200*np.sum(num\/denom))\/n\n#    return smape_val\n#\n#def lgbm_smape(y_true, y_pred):\n#    smape_val = smape(y_true, y_pred)\n#    return 'SMAPE', smape_val, False","77ba0534":"#lgbm_params = { \"num_leaves\":[20, 31, 40],\n#                \"max_depth\":[-1, 20, 30, 40],\n#                \"learning_rate\":[0.125, 0.1, 0.05], \n#                \"n_estimators\":[10000, 15000, 50000],\n#                \"min_split_gain\":[0.0, 2,5], \n#                \"min_child_samples\":[5, 10, 20, 30], \n#                \"colsample_bytree\":[0.5, 0.8, 1.0], \n#                \"reg_alpha\":[0.0, 0.5, 1, 1.5], \n#                \"reg_lambda\":[0.0, 0.5, 1]}\n\n#model = lgb.LGBMRegressor(random_state=1505)\n\n#tscv = TimeSeriesSplit(n_splits=3)\n\n#rsearch = RandomizedSearchCV(model, \n#                             lgbm_params, \n#                             random_state=1505, \n#                             cv=tscv, \n#                             scoring=make_scorer(smape), \n#                             verbose = True, \n#                             n_jobs = -1).fit(complete_series, complete_y)\n\n#print(rsearch.best_params_)","818e0d5f":"#model_tuned = lgb.LGBMRegressor(**rsearch.best_params_, random_state=1505).fit(train_set, y_train)\n#\n#print(\"TRAIN SMAPE:\", smape(y_train, model_tuned.predict(train_set)))\n#\n#print(\"VALID SMAPE:\", smape(y_val, model_tuned.predict(validation_set)))","a06eea7b":"#model_tuned = lgb.LGBMRegressor(**rsearch.best_params_, random_state=1505, metric = \"custom\")\n#              \n#model_tuned.fit(\n#    complete_series, complete_y,\n#    eval_metric= lambda y_true, y_pred: [lgbm_smape(y_true, y_pred)],\n#    eval_set = [(train_set, y_train), (validation_set, y_val)],\n#    eval_names = [\"Train\", \"Valid\"],\n#    callbacks = [lgb.log_evaluation(1000), \n#                 lgb.early_stopping(2500)])\n#\n#print(\"Best Iteration:\", model_tuned.booster_.best_iteration)","065314f3":"#def geometric_round(arr):\n#    result_array = arr\n#    result_array = np.where(result_array < np.sqrt(np.floor(arr)*np.ceil(arr)), np.floor(arr), result_array)\n#    result_array = np.where(result_array >= np.sqrt(np.floor(arr)*np.ceil(arr)), np.ceil(arr), result_array)\n#    return result_array\n#\n#y_true = np.array(complete_y.values)\n#y_pred = geometric_round(model_tuned.predict(complete_series))\n#\n#print('Validation SMAPE: ', round(SMAPE(y_true, y_pred), 2))","edad4b44":"#d = {'Actual' : pd.Series(y_true), 'Forecasted' : pd.Series(y_pred)}\n#comparison_df = pd.DataFrame(d)\n#\n#sns.scatterplot(data = comparison_df, \n#             x = comparison_df.index, \n#             y = 'Forecasted',\n#             color = 'darkred')\n#sns.lineplot(data = comparison_df, \n#             x = comparison_df.index, \n#             y = 'Actual',\n#             alpha = 0.8)\n#\n#plt.show()","419fe2c2":"# Make predictions\n#test_set = test_set.astype('float64', errors = 'ignore')\n#preds_test = geometric_round(model_tuned.predict(test_set))\n\n# Save predictions to file\n#output = pd.DataFrame({'row_id': test_set.index,\n#                       'num_sold': preds_test})\n\n# Check format\n#output.head()","fad27316":"#output.to_csv('submission.csv', index=False)","0dada9e1":"#from sklearn.model_selection import KFold\n#from sklearn.metrics import mean_squared_error\n#import optuna","95cbccb8":"#\n#N_split = 5\n#kf = KFold(n_splits=N_split)\n#\n#def objective(trial):\n#    params = {\n#                'device_config' : 'GPU',\n#                'eval_metric': 'SMAPE', \n#                'use_best_model': True,\n#                'random_seed' : 1,\n#                'learning_rate' :trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n#                \"depth\": trial.suggest_int(\"depth\", 1, 15),\n#                'l2_leaf_reg' :trial.suggest_loguniform('l2_leaf_reg', 1e-8, 20),\n#                'random_strength' : trial.suggest_loguniform('random_strength', 1, 50),\n#                'grow_policy':trial.suggest_categorical ('grow_policy', ['Lossguide','SymmetricTree']),\n#                'max_bin': trial.suggest_int(\"max_bin\", 20, 500),\n#                'min_data_in_leaf':trial.suggest_int('min_data_in_leaf', 1, 100),\n#                'bootstrap_type': trial.suggest_categorical(\"bootstrap_type\", [\"Bayesian\", \"Bernoulli\"]),\n#                'logging_level' : 'Silent'\n#            }\n#    \n#    if params['grow_policy'] == 'Lossguide':\n#        params['max_leaves']:trial.suggest_int('max_leaves', 1, 100)\n#    if params[\"bootstrap_type\"] == \"Bayesian\":\n#        params[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n#    elif params[\"bootstrap_type\"] == \"Bernoulli\":\n#        params[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n#        \n#    \n#    score_list = []\n#    \n#    for fold, (train_idx, val_idx) in enumerate(kf.split(train_set, y_train)):\n#        X_tr = train_set.loc[train_idx]\n#        X_va = train_set.iloc[val_idx]\n#        \n#        # Preprocess the data\n#        X_tr_f = X_tr \n#        y_tr = y_train.loc[train_idx].values\n#\n#        X_va_f = X_va\n#        y_va = y_train.loc[val_idx].values\n#\n#        # Train the model\n#        model = CatBoostRegressor(**params) \n#        model.fit(        \n#                X_tr_f,\n#                y_tr,\n#                eval_set =[( X_va_f,y_va)],\n#                verbose =0,\n#                early_stopping_rounds = 200)\n#\n#        # Predictions\n#        y_va_pred = model.predict(X_va_f)\n#        score = mean_squared_error(y_va, y_va_pred,squared = True)\n#        score_list.append(score)\n#        \n#    return sum(score_list) \/ len(score_list)\n#\n#study = optuna.create_study(direction='minimize')\n#study.optimize(objective, n_trials=20)","63859aa2":"#cat_params = {  \n#        'eval_metric': 'SMAPE',\n#        'logging_level' : 'Silent',\n#        'random_state' : 1518,\n#        'device_config' : 'GPU',\n#        'early_stopping_rounds' : 200}\n#\n#model = CatBoostRegressor(**cat_params) \n#grid = {'learning_rate': [0.1, 0.05, 0.025, 0.005],\n#        'depth': [4, 6, 8],\n#        'l2_leaf_reg': [0.5, 1, 3, 5]}\n#\n#gscv = GridSearchCV(estimator = model, param_grid = grid, scoring = SMAPE_score, cv = 5, n_jobs=-1, verbose = 1)\n#gscv.fit(train_set, y_train)\n#\n#best_score = gscv.best_score_\n#best_model = gscv.best_estimator_\n#\n#print('Lowest Training SMAPE: ', round(best_score,2))","6ff19932":"#best_model = CatBoostRegressor(eval_metric =  'SMAPE', \n#                               logging_level = 'Silent', \n#                               device_config = 'GPU', \n#                               random_seed = 1)\n#\n#best_model.set_params(**study.best_params)\n#best_model.fit(train_set, y_train)","e8e5a069":"#y_true = np.array(y_val.values)\n#y_pred = best_model.predict(validation_set)\n#\n#print('Validation SMAPE: ', round(SMAPE(y_true, y_pred), 2))","0894fa31":"#d = {'Actual' : pd.Series(y_true), 'Forecasted' : pd.Series(y_pred)}\n#comparison_df = pd.DataFrame(d)\n#\n#sns.scatterplot(data = comparison_df, \n#             x = comparison_df.index, \n#             y = 'Forecasted',\n#             color = 'darkred')\n#sns.lineplot(data = comparison_df, \n#             x = comparison_df.index, \n#             y = 'Actual',\n#             alpha = 0.8)\n#\n#plt.show()","06b9c148":"#shap_values = best_model.get_feature_importance(Pool(validation_set, \n#                                                label = y_val), \n#                                                type=\"ShapValues\")\n#expected_value = shap_values[0,-1]\n#shap_values = shap_values[:,:-1]\n#\n#shap.initjs()\n#shap.force_plot(expected_value, shap_values[3,:], test_set.iloc[3,:])\n","4bf978dc":"#shap.summary_plot(shap_values, test_set)","bbeb4671":"## Final prediction catboost\n#y_pred = best_model.predict(test_set)\n#\n## CSV Submission\n#submission = pd.DataFrame(test_data.index)\n#submission['num_sold'] = y_pred\n#submission['num_sold'] = np.round(submission['num_sold']).astype(int) \n#submission.to_csv('submission.csv', index=False)","bc23df2b":"class HybridModel:\n    def __init__(self, model_1, model_2, grid=None):\n        self.model_1 = model_1\n        self.model_2 = model_2\n        self.grid=grid\n        \n    def fit(self, X_train_1, X_train_2, y):\n        # Train model 1\n        self.model_1.fit(X_train_1, y)\n        \n        # Predictions from model 1 (trend)\n        y_trend = self.model_1.predict(X_train_1)\n\n        if self.grid:\n            # Grid search\n            tscv = TimeSeriesSplit(n_splits=3)\n            grid_model = GridSearchCV(estimator=self.model_2, cv=tscv, param_grid=self.grid)\n        \n            # Train model 2 on detrended series\n            grid_model.fit(X_train_2, y-y_trend)\n            \n            # Model 2 preditions (for residual analysis)\n            y_resid = grid_model.predict(X_train_2)\n            \n            # Save model\n            self.grid_model=grid_model\n        else:\n            # Train model 2 on residuals\n            self.model_2.fit(X_train_2, y-y_trend)\n            \n            # Model 2 preditions (for residual analysis)\n            y_resid = self.model_2.predict(X_train_2)\n        \n        # Save data\n        self.y_train_trend = y_trend\n        self.y_train_resid = y_resid\n        \n    def predict(self, X_test_1, X_test_2):\n        # Predict trend using model 1\n        y_trend = self.model_1.predict(X_test_1)\n        \n        if self.grid:\n            # Grid model predictions\n            y_resid = self.grid_model.predict(X_test_2)\n        else:\n            # Model 2 predictions\n            y_resid = self.model_2.predict(X_test_2)\n        \n        # Add predictions together\n        y_pred = y_trend + y_resid\n        \n        # Save data\n        self.y_test_trend = y_trend\n        self.y_test_resid = y_resid\n        \n        return y_pred","e8d78f32":"# Choose models\nmodel_1=LinearRegression()\nmodels_2=[LGBMRegressor(random_state=1505), CatBoostRegressor(random_state=0, verbose=False), XGBRegressor(random_state=1505)]\n\n# Parameter grid\nparam_grid = {'n_estimators': [100, 150, 200, 225, 250, 275, 300],\n        'max_depth': [4, 5, 6, 7],\n        'learning_rate': [0.1, 0.12, 0.13, 0.14, 0.15]}\n\n# Initialise output vectors\ny_pred=np.zeros(len(test_data))\ntrain_preds=np.zeros(len(complete_y))\n\n# Ensemble predictions\nfor model_2 in models_2:\n    # Start timer\n    start = time.time()\n    \n    # Construct hybrid model\n    model = HybridModel(model_1, model_2, grid=param_grid)\n\n    # Train model\n    model.fit(complete_series, complete_series, np.log(complete_y))\n\n    # Save predictions\n    y_pred += np.exp(model.predict(test_set,test_set))\n    \n    # Training set predictions (for residual analysis)\n    train_preds += np.exp(model.y_train_trend+model.y_train_resid)\n    \n    # Stop timer\n    stop = time.time()\n    \n    print(f'Model_2:{model_2} -- time:{round((stop-start)\/60,2)} mins')\n    \n    if model.grid:\n        print('Best parameters:',model.grid_model.best_params_,'\\n')\n    \n# Scale\ny_pred = y_pred\/len(models_2)\ntrain_preds = train_preds\/len(models_2)","ea573df5":"# From https:\/\/www.kaggle.com\/fergusfindley\/ensembling-and-rounding-techniques-comparison\ndef geometric_round(arr):\n    result_array = arr\n    result_array = np.where(result_array < np.sqrt(np.floor(arr)*np.ceil(arr)), np.floor(arr), result_array)\n    result_array = np.where(result_array >= np.sqrt(np.floor(arr)*np.ceil(arr)), np.ceil(arr), result_array)\n    return result_array\n\ny_pred=geometric_round(y_pred)\n\n# Save predictions to file\noutput = pd.DataFrame({'row_id': test_data.index, 'num_sold': y_pred})\n\n# Check format\noutput.head()","a90c5ff2":"# need to ensemble\ntrain_preds = np.exp(model.y_train_trend + model.y_train_resid)\n\n# Residuals on training set (SMAPE)\nresiduals = 200 * (train_preds - complete_y) \/ (train_preds + complete_y)\n\n# Plot residuals\nplt.figure(figsize = (12,4))\nplt.scatter(np.arange(len(residuals)),residuals, s = 1)\nplt.hlines([0], 0, residuals.index.max(), color='k')\nplt.title('Residuals on training set')\nplt.xlabel('Sample')\nplt.ylabel('SMAPE')","a07223cd":"mu, std = scipy.stats.norm.fit(residuals)\n\nplt.figure(figsize=(12,4))\nplt.hist(residuals, bins=100, density=True)\nx = np.linspace(plt.xlim()[0], plt.xlim()[1], 200)\nplt.plot(x, scipy.stats.norm.pdf(x, mu, std), 'r', linewidth=2)\nplt.title(f'Histogram of residuals; mean = {residuals.mean():.4f}, '\n          f'$\\sigma = {residuals.std():.1f}$, SMAPE = {residuals.abs().mean():.5f}')\nplt.xlabel('Residual (percent)')\nplt.ylabel('Density')\nplt.show()","4e236631":"output.to_csv('submission.csv', index=False)","b9909aa5":"### Visual Comparison","5a024bb3":"## Notes\n\nFrom this preliminar analysis we can already notice that KaggleRama overall performances are betther than KaggleMart performances, both for countries and products. These results suggests that also in 2019 KaggleRama should outperform KaggleMart. In the next section these results are formalized in a mathematical model.","3cb8aeb1":"# LGBM","06f75064":"# Random Forest Regressor","a95e207d":"## Notes\n\nIn this specific case, we are dealing with sales. It may be that due to high volumes of transactions in December several invoices are registered in January.\n\n## Time Series Comparison and Further Visualizations","cb6886f6":"# Hybrid Model","d9cfc237":"# Catboost","9797088b":"![](https:\/\/centralnicregistry.com\/assets\/img\/blog\/graph.png)\n\n# Tabular Playground Series - Jan 2022\n## A Statistical Analysis\n\n**Objective**: \n> Choose the best store chains selling Kaggle merchandise\n\n**Data**:  \n* train.csv - it includes the sales data for each date-country-store-item combination\n* test.csv - the task is to predict the corresponding item sales for each date-country-store-item combination.\n* sample_submission.csv - a sample submission file in the correct format\n\n**Evaluation**: \n> Submissions are evaluated on SMAPE between forecasts and actual values. \n> SMAPE belongs to the family of percentage errors, for this reason is unit-free an pretty useful when comparing forecast performances between data sets. Symmetric MAPE (sMAPE) was proposed by [Armstrong (1978, p. 348)](https:\/\/econpapers.repec.org\/article\/eeeintfor\/v_3a2_3ay_3a1986_3ai_3a3_3ap_3a387-390.htm) in order to overcome the disadvantage that they put a heavier penalty on negative errors than on positive errors. (See [Evaluating point forecast accuracy](https:\/\/otexts.com\/fpp3\/accuracy.html))\n\n## Importing libraries and data preparation\n","e0152912":"## Detecting Outliers: Point Outliers\n\n> A point outlier behaves unusually in a specific time instance when compared to other values in the time series (global outlier), or to its neighborhood (local outlier)","e48e3b97":"## Comparison by Product","a7664a6e":"# Time Series Visualization","c76fc3f1":"## Comparison by Country","2656f07c":"# Models fitting","366033f7":"## Notes\n\n1. Regarding the decomposition plot, this is a simple decomposition. You should also evaluate if an additive or a multiplicative decomposition fits your needs. \n\n> The additive decomposition is the most appropriate if the magnitude of the seasonal fluctuations, or the variation around the trend-cycle, does not vary with the level of the time series. When the variation in the seasonal pattern, or the variation around the trend-cycle, appears to be proportional to the level of the time series, then a multiplicative decomposition is more appropriate.   \n>  \n> Additive decomposition: \n> $$y_{t} = S_{t} + T_{t} + R_t$$\n> Multiplicative decomposition: \n> $$y_{t} = S_{t} \\times T_{t} \\times R_t$$\n>  \n> The results are obtained by first estimating the trend by applying a convolution filter to the data. The trend is then removed from the series and the average of this de-trended series for each period is the returned seasonal component.\n\n2. Regarding the ACF plot, it can be interpretated as follows:\n\n> * When data have a trend, the autocorrelations for small lags tend to be large and positive because observations nearby in time are also nearby in value. So the ACF of a trended time series tends to have positive values that slowly decrease as the lags increase.\n> * When data are seasonal, the autocorrelations will be larger for the seasonal lags (at multiples of the seasonal period) than for other lags.\n> * When data are both trended and seasonal, you see a combination of these effects. \n>\n> In this example you can notice a combination of trend and seasonality. There is a positive trend since we have values that slowly decrease and seasonality since  the autocorrelation tends to be larger for the seasonal lags.\n\n3. Residuals (random component) analysis should be performed in order to spot possible inconsistent behaviors.\n\n> In this example the total number of 'Kaggle Mug' sold spikes each January and suddenly decreases. This behavior should be investigated in order to fully understand the time series. See 'Detecting Outliers: Point outliers'.\n\n\n## Further Example","178464ce":"### Visual Comparison"}}