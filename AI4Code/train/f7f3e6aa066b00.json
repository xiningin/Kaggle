{"cell_type":{"3396a66e":"code","d0706287":"code","638b7ade":"code","bf0063ce":"code","7e5f32db":"code","77c4c1c8":"code","f3e09052":"code","7ee81a50":"code","45bf8854":"code","8d514968":"code","e4a4b8f9":"code","dcbd45ba":"code","e1ad346e":"code","15bb9402":"code","5d07ed12":"markdown","32a4cb56":"markdown"},"source":{"3396a66e":"# impor pustaka yang diperlukan\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.layers import AveragePooling2D\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications.mobilenet_v2 import preprocess_input\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.utils import np_utils\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import MultiLabelBinarizer\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nimport time","d0706287":"# download imutils\n!pip install imutils\nfrom imutils import paths\nfrom imutils.video import VideoStream","638b7ade":"# buat argparse class\n# karena pada kaggle tidak dapat menginstall argparse\nclass ap:\n    # untuk training model \n    dataset_1 = '..\/input\/pascal-voc-cropping'\n    dataset_2 = \"..\/input\/face-mask-detection-data\/without_mask\"\n    plot = 'plot.png'\n    model = 'hardhat_detector.model'\n    \n    # untuk implementasi model\n    image = []\n    face = 'face_detector'\n    confidence = 0.5","bf0063ce":"# inisiasi deep learning hyperparameter\nINIT_LR = 1e-4 # learning rate\nEPOCHS = 20 # epocsh\nBS = 32 # batch size","7e5f32db":"# mengambil gambar dari dataset\n# dan membaginya menjadi data images dan labels\nprint(\"[INFO] loading images...\")\nimagePaths = list(paths.list_images(ap.dataset_1))\ndata = []\nlabels = []\n\nlabels_helmet = []\nlabels_non_helmet = []\n\ni = 0\n\n# loop pada image paths\nfor imagePath in imagePaths:\n    # ambil label dari image paths\n    label = imagePath.split(os.path.sep)[-2]\n    \n    # ambil image dan preprocess dengan ukuran 224x224\n    image = load_img(imagePath, target_size=(224, 224)) # resize image\n    image = img_to_array(image)\n    image = preprocess_input(image) # scaling intensitas pixel [-1,1]\n    \n    # update data dan labels\n    data.append(image)\n    labels.append(label)\n    \n#     if labels == \"helmet\":\n#         labels_helmet.append(label)\n#     else:\n#         labels_non_helmet.append(label)\n    \n    i = i + 1\n    if i >= 3000:\n        print(\"[INFO] loading image {}\/{}\".format(i, len(imagePaths)))\n        break\n    \n# pastikan data dan labels dalam format numpy arrays\ndata = np.array(data, dtype='float32')\nlabels = np.array(labels)\nprint(\"[INFO] loading image done...\")\n# print(\"[INFO] helmet data :\", len(labels_helmet))\n# print(\"[INFO] non-helmet data :\", len(labels_non_helmet))","77c4c1c8":"# ubah label dari string ke integer\n# dengan metode one-hot encoding\nlb = LabelBinarizer()\nlabels = lb.fit_transform(labels)\nlabels = to_categorical(labels)\n\n# partisi data menjadi test 80% dan training 20%\n(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.20, random_state=42)\n\n# buat training image generator\n# untuk augmentasi data\naug = ImageDataGenerator(rotation_range=20,\n                        zoom_range=0.15,\n                        width_shift_range=0.2,\n                        height_shift_range=0.2,\n                        shear_range=0.15,\n                        horizontal_flip=True,\n                        fill_mode=\"nearest\")","f3e09052":"# proses fine-tuning\n# load MobileNetV2 network\n# pastikan head FC layers tidak terpasang\nbaseModel = MobileNetV2(weights='imagenet', \n                        include_top=False,\n                        input_tensor=Input(shape=(224,224,3)))\n\n# buat head model FC layers\nheadModel = baseModel.output\nheadModel = AveragePooling2D(pool_size=(7,7))(headModel)\nheadModel = Flatten(name=\"flatten\")(headModel)\nheadModel = Dense(128, activation=\"relu\")(headModel)\nheadModel = Dropout(0.5)(headModel)\nheadModel = Dense(2, activation=\"softmax\")(headModel)\n\n# pasang headModel diatas baseModel\n# hanya headModel yang akan di training\nmodel = Model(inputs=baseModel.input, outputs=headModel)\n\n# freeze baseModel sehingga tidak ter-training\n# untuk freeze, loop setiap layer pada base model\nfor layer in baseModel.layers:\n    layer.trainable = False","7ee81a50":"# compile model yang telah dibuat\n# untuk model 2 classes gunakan binary_crossentropy\n# untuk model >2 classes gunakan categorical_crossentropy\nprint(\"[INFO] compiling model...\")\nopt = Adam(lr=INIT_LR, decay=INIT_LR\/EPOCHS)\nmodel.compile(loss=\"binary_crossentropy\",\n             optimizer=opt,\n             metrics=[\"accuracy\"])\n\n# training headModel\nprint(\"[INFO] training head...\")\n\n# H = model.fit(trainX, trainY, validation_data=(testX, testY), epochs=20, batch_size=32)\n\nH = model.fit(aug.flow(trainX, trainY, batch_size=BS),\n             steps_per_epoch=len(trainX)\/\/BS,\n             validation_data=(testX, testY),\n             validation_steps=len(testX)\/\/BS,\n             epochs=EPOCHS)","45bf8854":"# buat prediksi dari data testing\nprint(\"[INFO] evaluating network...\")\npredIdxs = model.predict(testX, batch_size=BS)\n\n# cari indeks label dengan nilai\n# prediksi terbesar --> argmax\npredIdxs = np.argmax(predIdxs, axis=1)\n\n# tampilkan hasil klasifikasi\nprint(classification_report(testY.argmax(axis=1),\n                           predIdxs,\n                           target_names=lb.classes_))\n\n# serialize model to disk\nprint(\"[INFO] saving mask detector model...\")\nmodel.save(ap.model, save_format=\"h5\")","8d514968":"# plot training loss dan akurasi\n# dalam bentuk grafik matplotlib\nN = EPOCHS\nplt.style.use(\"ggplot\")\nplt.figure()\nplt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\nplt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\nplt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_accuracy\")\nplt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_accuracy\")\nplt.title(\"Training Loss and Accuracy\")\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss\/Accuracy\")\nplt.legend(loc=\"lower left\")\nplt.savefig(ap.plot)","e4a4b8f9":"# load model face detektor\nprint(\"[INFO] loading face detector model...\")\nprototxtPath = \"..\/input\/face-detection-ssd-model\/CAFFE_DNN\/deploy.prototxt.txt\"\nweightsPath = \"..\/input\/face-detection-ssd-model\/CAFFE_DNN\/res10_300x300_ssd_iter_140000.caffemodel\"\nfaceNet = cv2.dnn.readNet(prototxtPath, weightsPath)\n\n\n\n# load model face-mask detector\nprint(\"[INFO] loading face mask detector model...\")\nmaskNet = load_model(\".\/hardhat_detector.model\")","dcbd45ba":"# buat fungsi untuk mendeteksi mask\n# frame : video\n# faceNet : model pendeteksi face\n# maskNet : model pendeteksi mask\ndef detect_and_predict_mask(frame, faceNet, maskNet):\n    # buat blob\n    (h, w) = frame.shape[:2]\n    blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300),\n                                (104.0, 177.0, 123.0))\n    # lewatkan blob pada face detection\n    faceNet.setInput(blob)\n    detections = faceNet.forward()\n    \n    # initialize list dari face, lokasi, dan prediksi\n    faces = []\n    locs = []\n    preds = []\n    \n    # loop pada detections\n    for i in range(0, detections.shape[2]):\n        # extract confidence (probabilitas) pada detections\n        confidence = detections[0, 0, i, 2]\n        \n        # filter weak detections yang kurang dari\n        # confidence minimum\n        if confidence > ap.confidence:\n            # buat bounding box pada face yang terdeteksi\n            box = detections[0, 0, i, 3:7]*np.array([w, h, w, h])\n            (startX, startY, endX, endY) = box.astype(\"int\")\n            \n            # pastikan bounding box berada dalam frame\n            (startX, startY) = (max(0, startX), max(0, startY))\n            (endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n            \n            # extract face ROI (Region of Interest)\n            # convert dari BGR to RGB\n            # ordering, resize (224x224) dan preprocess\n            face = frame[startY:endY, startX:endX]\n            face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n            face = cv2.resize(face, (224, 224))\n            face = img_to_array(face)\n            face = preprocess_input(face)\n            \n            # append face dan bounding box pada list yg telah dibuat\n            faces.append(face)\n            locs.append((startX, startY, endX, endY))\n    \n    # run faces yang telah didapat ke face mask detector\n    # buat prediksi saat ada face terdeteksi\n    if len(faces)>0:\n        # buat batch predictions untuk mendeteksi semua face secara bersamaan\n        faces = np.array(faces, dtype=\"float32\")\n        preds = maskNet.predict(faces, batch_size=32)\n        \n    # return 2-tuple, face locations dan nilai prediksi\n    return (locs, preds)","e1ad346e":"# initialize file video\nprint(\"[INFO] loading video stream...\")\nvideo_path = \"..\/input\/hardhat-v2\/Hardhat Video.mp4\"\nvs = cv2.VideoCapture(video_path)\n\n# ambil variable height, width, dan fps dari video\nheight = int(vs.get(cv2.CAP_PROP_FRAME_HEIGHT))\nwidth = int(vs.get(cv2.CAP_PROP_FRAME_WIDTH))\nfps = int(vs.get(cv2.CAP_PROP_FPS))\n\n# simpan video output\nfourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\noutput = cv2.VideoWriter(\"face-mask-detections.avi\", \n                               fourcc, fps, (width, height), True)\n\n# loop pada frame video\nwhile True:\n    \n    (grabbed, frame) = vs.read()\n\n    if not grabbed:\n        print(\"[INFO] video process has been done...\")\n        break\n    \n    # deteksi face pada video\n    # dan tentukan apakah memakai masker atau tidak\n    (locs, preds) = detect_and_predict_mask(frame, faceNet, maskNet)\n    \n    # loop pada lokasi face yang terdeteksi\n    for (box, pred) in zip(locs, preds):\n        # unpack bounding box dan nilai prediksi\n        (startX, startY, endX, endY) = box\n        (mask, withoutMask) = pred\n        \n        # tentukan label yang digunakan -\n        # dan warna untuk label tersebut (hijau, dan merah)\n        label = \"Bermasker\" if mask > withoutMask else \"Tidak Bermasker\"\n        color = (0, 255, 0) if label == \"Bermasker\" else (0,0,255)\n\n        # sertakan nilai probabilitas pada label\n        label = \"{}: {:.2f}%\".format(label, max(mask, withoutMask)*100)\n\n        # tampilkan label dan bounding box pada frame\n        cv2.putText(frame, label, (startX, startY-10),\n                   cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n        cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n    \n    # write video yang telah di proses ke output\n    output.write(frame)","15bb9402":"# load image\nimg_path = \"..\/input\/face-mask-datasets\/with_mask\/with_mask\/123-with-mask.jpg\"\nimage = cv2.imread(img_path)\n\n# plt.figure()\n# plt.imshow(image)\n# plt.show()\n\norig = image.copy()\n(h, w) = image.shape[:2]\n\n# buat blob dari image\nblob = cv2.dnn.blobFromImage(image, 1.0, (300, 300), \n                            (104.0, 177.0, 123.0))\n\n# lewatkan blob pada face detection\nprint(\"[INFO] computing face detections...\")\nfaceNet.setInput(blob)\ndetections = faceNet.forward()\n\n# loop pada detections\nfor i in range(0, detections.shape[2]):\n    # extract confidence (probabilitas)\n    confidence = detections[0, 0, i, 2]\n    \n    # filter weak detections yang kurang dari\n    # confidence minimum\n    if confidence > ap.confidence:\n        # buat bounding box pada face yang terdeteksi\n        box = detections[0, 0, i, 3:7]*np.array([w, h, w, h])\n        (startX, startY, endX, endY) = box.astype(\"int\")\n            \n        # pastikan bounding box berada dalam frame\n        (startX, startY) = (max(0, startX), max(0, startY))\n        (endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n        \n        # extract face ROI (Region of Interest)\n        # convert dari BGR to RGB\n        # ordering, resize (224x224) dan preprocess\n        face = image[startY:endY, startX:endX]\n        face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n        face = cv2.resize(face, (224, 224))\n        face = img_to_array(face)\n        face = preprocess_input(face)\n        face1 = np.expand_dims(face, axis=0)\n        \n        (mask, withoutMask) = maskNet.predict(face1)[0]\n        \n        # tentukan label yang digunakan -\n        # dan warna untuk label tersebut (hijau, dan merah)\n        label = \"Bermasker\" if mask > withoutMask else \"Tidak Bermasker\"\n        color = (0, 255, 0) if label == \"Bermasker\" else (0,0,255)\n\n        # sertakan nilai probabilitas pada label\n        label = \"{}: {:.2f}%\".format(label, max(mask, withoutMask)*100)\n\n        # tampilkan label dan bounding box pada frame\n        cv2.putText(image, label, (startX, startY-10),\n                   cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n        cv2.rectangle(image, (startX, startY), (endX, endY), color, 2)\n\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)        \n\nplt.figure()\nplt.imshow(image)\nplt.show()","5d07ed12":"# Implementasi Detektor\nUntuk implementasi jangan menjalankan semua cell diatas kecuali cell no 1, 2 dan 3","32a4cb56":"# Image Mask Detector "}}