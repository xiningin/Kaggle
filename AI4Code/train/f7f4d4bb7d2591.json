{"cell_type":{"9685d9ec":"code","10657af6":"code","86473e10":"code","2bd79d0a":"code","9e5853b8":"code","a7e5b6d2":"code","700aecae":"code","b58b1a09":"code","b6d97bab":"code","88ed0a30":"code","153e833a":"code","24bdc348":"code","a31bda00":"code","d53a7d13":"markdown","0bb909d7":"markdown","3dc47204":"markdown","8e744007":"markdown","70af34ac":"markdown","b4f1d6ac":"markdown","6e12ba2e":"markdown","a4e29c06":"markdown","e3dc89a1":"markdown","124aea99":"markdown","b0446733":"markdown"},"source":{"9685d9ec":"import tensorflow as tf # Imports the tensor flow library\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport os\ntf.__version__\n","10657af6":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","86473e10":"message = tf.constant('Welcome to the exciting world of Deep Neural Networks!')","2bd79d0a":"with tf.Session() as sess:\n     print(sess.run(message).decode()) # with decode\n\nsession = tf.Session()\nprint(session.run(message))# with out decode","9e5853b8":"\nmnist = tf.keras.datasets.mnist # 28 x 28 images of handwritten digits 0-9\npath = '\/kaggle\/input\/mnist.npz'\n(X_train, y_train),(X_test,y_test) = mnist.load_data(path) # loading the mnist data\n","a7e5b6d2":"plt.imshow(X_train[10], cmap = plt.cm.binary)\nplt.show()\nprint(X_train[10])","700aecae":"X_train = tf.keras.utils.normalize(X_train, axis=1)\nX_test = tf.keras.utils.normalize(X_test, axis=1)\nplt.imshow(X_train[10], cmap = plt.cm.binary)\nplt.show()","b58b1a09":"model = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Flatten()) # Input layer\nmodel.add(tf.keras.layers.Dense(128, activation = tf.nn.relu)) # Hidden layer 1\nmodel.add(tf.keras.layers.Dense(128, activation = tf.nn.relu)) # Hidden layer 2\nmodel.add(tf.keras.layers.Dense(10, activation = tf.nn.softmax)) # Output layer","b6d97bab":"model.compile(optimizer = 'adam',\n             loss = 'sparse_categorical_crossentropy',\n             metrics = ['accuracy'])","88ed0a30":"model.fit(X_train,y_train,epochs = 50, batch_size = 10)","153e833a":"val_loss, val_acc = model.evaluate(X_test,y_test)\nprint(val_loss, val_acc)","24bdc348":"predictions = model.predict([X_test])   #predict always takes a list\nprint(predictions)","a31bda00":"print(np.argmax(predictions[0]))\nplt.imshow(X_test[0], cmap = plt.cm.binary)\nplt.show()","d53a7d13":"\nSince the message we want to print is a constant string, we use tf.constant:\n\nTo execute the graph element, we need to define the **Session** using **with** and **run** the session using <b><u>run<\/u><\/b>","0bb909d7":"## Model Prediction ##","3dc47204":"Model building using keras sequential type of model and there are three types of models in keras API\n\n<b>The Sequential Model<\/b>\n\n- Dead simple\n- Only for single-input, single-output, sequential layer stacks\n- Good for 70+% of use cases\n\n<b>The functional API<\/b>\n\n- Like playing with Lego bricks\n- Multi-input, multi-output, arbitrary static graph topologies\n- Good for 95% of use cases\n\n<b>Model subclassing<\/b>\n\n- Maximum flexibility\n- Larger potential error surface","8e744007":"Load the mnist dataset ","70af34ac":"## Model definition ##","b4f1d6ac":"## Model compile & run","6e12ba2e":"\nMNIST images are like 28 X 28 multi dimensional array. But, we want them to be a flatten (single column),which can be achieved either by using numpy,reshape or we can use one of the layers that's built into keras which is flattened","a4e29c06":"Pixel data is varying from 0 to 255 and it is better to normalize the pixel data using ``tf normalize`` option","e3dc89a1":"\n``model.add(tf.keras.layers.Dense(128, activation = tf.nn.relu))``\n\n- It defines a hidden layer with 128 neurons\/units, connected to the input layer or previous hidden layer that use relu activation function.\n\n``model.add(Dense(128, input_dim=8, init='uniform', activation='relu'))``\n\n- It defines the input layer as having 8 units\n- It defines a hidden layer with 128 neurons\/units, connected to the input layer that use relu activation function.\n- It initializes all weights using a sample of unfirom random numbers\n\nNow that the model is defined, we can compile it.\n\nWhen compiling, we must specify some additional properties required when training the network.\n\n- Must specify the loss function to evaluate the weights\n- Must specify the optimizer, which will do search through different weights for the network\n- Optional metrics, would like to collect & report during training\n\n<i>Remember training a network means finding the best set of weights to make predictions for this problem.<\/i>\n\nIn this case, we will use logarithmic loss, which for a Multi classification problem. For this example, we are considering \u201csparse_categorical_crossentropy\u201c as loss parameter. We will also use the efficient gradient descent algorithm \u201cadam\u201d for no other reason that it is an efficient default. Learn more about the Adam optimization algorithm in the paper \u201c **Adam: A Method for Stochastic Optimization** --> https:\/\/arxiv.org\/abs\/1412.6980 \u201c.","124aea99":"\nWe have defined our model and compiled it ready for computation. Now it is time to fit the model on mnist data\n\n- We can train or fit our model on our loaded data by calling the ``fit()`` function on the model.\n\nThe training process will run for a fixed number of iterations through the dataset called **epochs**, that we must specify using the ``nepochs`` argument. We can also set the number of instances that are evaluated before a weight update in the network is performed, called the **batch size** and set using the ``batch_size`` argument.\n\n- Epoch is just a \"full pass\" through your entire training dataset. 3 epochs means it passed over your data set 3 times","b0446733":"## Data Visualization ##\nLet's see an image from training set using matplotlib"}}