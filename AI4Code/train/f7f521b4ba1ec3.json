{"cell_type":{"843a2667":"code","14f72e4d":"code","930d1a9f":"code","47a16ac1":"code","88ff8918":"code","7a8dc316":"code","f9bd119d":"code","3b99816a":"code","58bb66a4":"code","9ca87772":"markdown","48a66e74":"markdown","9dba7862":"markdown","d30c203d":"markdown","3f8c83b4":"markdown","070cd33a":"markdown","757ea6a5":"markdown"},"source":{"843a2667":"import os\nimport sys\nsys.path.append('\/kaggle\/input\/raft-pytorch')\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport torch\n\nfrom glob import glob\nfrom PIL import Image\nfrom tqdm import tqdm","14f72e4d":"from raft.core.raft import RAFT\nfrom raft.core.utils import flow_viz\nfrom raft.core.utils.utils import InputPadder\nfrom raft.config import RAFTConfig","930d1a9f":"config = RAFTConfig(\n    dropout=0,\n    alternate_corr=False,\n    small=False,\n    mixed_precision=False\n)\n\nmodel = RAFT(config)\nmodel","47a16ac1":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'device: {device}')\n\nweights_path = '\/kaggle\/input\/raft-pytorch\/raft-sintel.pth'\n# weights_path = '\/kaggle\/input\/raft-pytorch\/raft-things.pth'\n\nckpt = torch.load(weights_path, map_location=device)\nmodel.to(device)\nmodel.load_state_dict(ckpt)","88ff8918":"image_files = glob('\/kaggle\/input\/raft-pytorch\/raft\/demo-frames\/*.png')\nimage_files = sorted(image_files)\n\nprint(f'Found {len(image_files)} images')\nprint(sorted(image_files))","7a8dc316":"def load_image(imfile, device):\n    img = np.array(Image.open(imfile)).astype(np.uint8)\n    img = torch.from_numpy(img).permute(2, 0, 1).float()\n    return img[None].to(device)\n\n\ndef viz(img1, img2, flo):\n    img1 = img1[0].permute(1,2,0).cpu().numpy()\n    img2 = img2[0].permute(1,2,0).cpu().numpy()\n    flo = flo[0].permute(1,2,0).cpu().numpy()\n    \n    # map flow to rgb image\n    flo = flow_viz.flow_to_image(flo)\n    \n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 4))\n    ax1.set_title('input image1')\n    ax1.imshow(img1.astype(int))\n    ax2.set_title('input image2')\n    ax2.imshow(img2.astype(int))\n    ax3.set_title('estimated optical flow')\n    ax3.imshow(flo)\n    plt.show()","f9bd119d":"model.eval()\nn_vis = 3\n\nfor file1, file2 in tqdm(zip(image_files[:n_vis], image_files[1:1+n_vis])):\n    image1 = load_image(file1, device)\n    image2 = load_image(file2, device)\n\n    padder = InputPadder(image1.shape)\n    image1, image2 = padder.pad(image1, image2)\n    \n    with torch.no_grad():\n        flow_low, flow_up = model(image1, image2, iters=20, test_mode=True)\n        \n    viz(image1, image2, flow_up)","3b99816a":"video_file = '\/kaggle\/input\/nfl-impact-detection\/train\/57583_000082_Endzone.mp4'\n\ncap = cv2.VideoCapture(video_file)\n\nframes = []\nwhile True:\n    has_frame, image = cap.read()\n    \n    if has_frame:\n        image = image[:, :, ::-1] # convert BGR -> RGB\n        frames.append(image)\n    else:\n        break\nframes = np.stack(frames, axis=0)\n\nprint(f'frame shape: {frames.shape}')    \nplt.imshow(frames[0])","58bb66a4":"n_vis = 3\n\nfor i in range(n_vis):\n    image1 = torch.from_numpy(frames[i]).permute(2, 0, 1).float().to(device)\n    image2 = torch.from_numpy(frames[i+1]).permute(2, 0, 1).float().to(device)\n    \n    image1 = image1[None].to(device)\n    image2 = image2[None].to(device)\n\n    padder = InputPadder(image1.shape)\n    image1, image2 = padder.pad(image1, image2)\n    \n    with torch.no_grad():\n        flow_low, flow_up = model(image1, image2, iters=20, test_mode=True)\n        \n    viz(image1, image2, flow_up)","9ca87772":"# RAFT introduction\n\nI introduce the model: **RAFT: Recurrent All-Pairs Field Transforms for Optical Flow** which is originally introduced in ECCV2020 by Teed et. al. in Princeton University and prized Best Paper Award!.\n* https:\/\/arxiv.org\/abs\/2003.12039\n* https:\/\/github.com\/princeton-vl\/RAFT (licensed under the BSD 3-Clause License)\n\nBriefly, RAFT has below features\n* Recurrent optical flow estimation\n* Compute pixel-wise correlation between pair-wise input images and reuse it in the following recurrent step\n* Lightweight, rapid inference, and high accuracy\n\n![RAFT architecture image from https:\/\/github.com\/princeton-vl\/RAFT](https:\/\/github.com\/princeton-vl\/RAFT\/raw\/master\/RAFT.png)\n\nThis is [my explanation slide](https:\/\/speakerdeck.com\/daigo0927\/raft-recurrent-all-pairs-field-transforms-for-optical-flow) in Japanese.","48a66e74":"# Run on NFL video","9dba7862":"# What's Optical flow?\n\n> Optical flow or optic flow is the pattern of apparent motion of objects, surfaces, and edges in a visual scene caused by the relative motion between an observer and a scene. - https:\/\/en.wikipedia.org\/wiki\/Optical_flow\n\n\n**In this competition, each player motion can be essential for detecting the helmet impact. I'm going to introduce a deep learning model for optical flow estimation.**","d30c203d":"# Run RAFT on sample images","3f8c83b4":"The first and second columns are input paired images and right column is the predicted optical flow.","070cd33a":"RAFT seems to capture the motion of each player.","757ea6a5":"# Have a nice football flow!"}}