{"cell_type":{"145ffc1d":"code","6dce1a74":"code","b22743da":"code","5449b6f1":"code","7b5b7672":"code","64197ba4":"code","3f611e68":"code","b580713a":"code","c23974f6":"code","6e9f6fd7":"code","d8f61d28":"code","a0c16e43":"code","4202147f":"code","b161abbb":"code","5ce6971c":"code","80aeba10":"code","70580cc8":"code","7f694140":"code","7dcd7a70":"code","4a6cc90c":"code","128d6d84":"code","4a190b64":"code","fd992d69":"code","444c958a":"code","fe7d976c":"code","7ecce110":"code","41e4c203":"code","63b3ce0d":"code","d3323197":"code","172feff4":"code","76ddb477":"code","16073c84":"code","38c46d78":"code","4c8e5d01":"markdown","57eaf3fb":"markdown","4ed7d02f":"markdown","5548c11b":"markdown","0ff81132":"markdown","8997e404":"markdown","b9f798ec":"markdown","2d69f470":"markdown","30761cdc":"markdown","fd7a55b8":"markdown","16e11429":"markdown","5c1544c0":"markdown","c4919098":"markdown","45d9040c":"markdown","4ffb20c3":"markdown","0cffdc10":"markdown","94fa40f1":"markdown","38ca0f41":"markdown","297cf095":"markdown","056d09dd":"markdown","a8526fc0":"markdown","18abe52e":"markdown","e94ea2ec":"markdown","3a5e76f4":"markdown","8cfb64a0":"markdown","010a6aad":"markdown","0310634f":"markdown","8179b063":"markdown","8eab3d49":"markdown","56093648":"markdown","9ce900e8":"markdown"},"source":{"145ffc1d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport statsmodels.api as sma\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nimport warnings\nwarnings.filterwarnings('ignore')","6dce1a74":"df = pd.read_csv('..\/input\/abalone-dataset\/abalone.csv')","b22743da":"df.head()","5449b6f1":"#rename output variable\ndf.rename(columns={\"Sex\":\"sex\", \"Length\":\"length\", \"Diameter\":\"diameter\",\n                   \"Height\":\"height\", \"Whole weight\":\"whole_weight\",\n                   \"Shucked weight\":\"shucked_weight\", \"Viscera weight\":\"viscera_weight\",\n                   \"Shell weight\":\"shell_weight\", \"Rings\":\"rings\"}, inplace = True)","7b5b7672":"df.shape","64197ba4":"df.describe()","3f611e68":"df[df['height'] == 0]  #need to drop these rows.","b580713a":"df.drop(index=[1257,3996], inplace = True)\ndf.shape","c23974f6":"df['age'] = df['rings']+1.5 #AS per the problem statement\ndf.drop('rings', axis = 1, inplace = True)\ndf.head()","6e9f6fd7":"#categorical features\ntemp = pd.concat([df['age'], df['sex']], axis=1)\n\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxenplot(x='sex', y=\"age\", data=df)\nfig.axis(ymin=0, ymax=30);","d8f61d28":"sns.countplot('sex', data=df)\nplt.title('Distributed Classes', fontsize=14)\nplt.show()","a0c16e43":"df.hist(figsize = (20,10), layout = (2,4))","4202147f":"df.skew().sort_values(ascending = False)","b161abbb":"corr = df.corr()\nplt.figure(figsize = (10,10))\nax = sns.heatmap(corr, vmin = -1, center = 0, annot = True, cmap = 'mako')","5ce6971c":"upper_tri = corr.where(np.triu(np.ones(corr.shape),k=1).astype(np.bool))\ncolumns_to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)] #highly correlated variables to be removed.\n\nprint(\"Columns to drop:\\n\", columns_to_drop)","80aeba10":"df.drop(columns_to_drop, axis=1, inplace = True)","70580cc8":"df.head()","7f694140":"#calculating VIF\ndef vif_arr(df1):\n    vif = pd.DataFrame()\n    df1 = sma.add_constant(df1)\n    vif[\"Features\"] = df1.columns\n    vif['VIF'] = [variance_inflation_factor(df1.values, i) for i in range(df1.shape[1])]\n    return(vif)","7dcd7a70":"temp = df.drop('sex', axis = 1) #i.e. categorical\nvif_arr(temp).sort_values(by = 'Features', ascending = False)","4a6cc90c":"f, axes = plt.subplots(ncols = 3, figsize = (20,4))\n\nsns.boxplot(x = 'sex', y = 'whole_weight', data = df, ax = axes[0])\naxes[0].set_title('whole_weight vs sex')\n\nsns.boxplot(x = 'sex', y = 'length', data = df, ax = axes[1])\naxes[1].set_title('length vs sex')\n\nsns.boxplot(x = 'sex', y = 'height', data = df, ax = axes[2])\naxes[2].set_title('height vs sex')\n","128d6d84":"df['height'] = np.sqrt(df['height'])","4a190b64":"#checking skewness again\ndf.skew().sort_values(ascending = False)","fd992d69":"df.hist(figsize = (20,10), layout = (2,4), bins = 30)","444c958a":"df.head()","fe7d976c":"Age = []\nfor i in df[\"whole_weight\"]:\n    if i < 0.6:\n        Age.append(1)\n    elif i > 0.6 and i < 1.0 :\n        Age.append(2)\n    else:\n        Age.append(3)\ndf[\"age\"] = Age\n#df_1.drop(\"age\" , axis =1,inplace=True)\ndf.head()","7ecce110":"sns.countplot('age', data=df)\nplt.title('Distributed Classes')\nplt.show()","41e4c203":"new_df = pd.get_dummies(df, columns = ['sex'], prefix_sep='_', drop_first = True)\nnew_df['age'].value_counts()","63b3ce0d":"#NEW VIF\nvif_arr(new_df).sort_values(by = 'Features', ascending = False)","d3323197":"new_df['length'] = np.sqrt(new_df['length'])\nvif_arr(new_df).sort_values(by = 'Features', ascending = False)","172feff4":"X = new_df.drop('age', axis = 1)\ny = new_df['age']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nsc = StandardScaler()\nsc.fit(X_train)\nX_train_std = sc.transform(X_train)\nX_test_std = sc.transform(X_test)\nX_train_std = pd.DataFrame(X_train_std, columns=X_train.columns)\nX_test_std = pd.DataFrame(X_test_std, columns=X_train.columns)\n\nX_train = X_train_std.values\nX_test = X_test_std.values\ny_train = y_train.values\ny_test = y_test.values\n\nclassifiers = {\"LogisiticRegression\": LogisticRegression(),\n               \"KNearest\": KNeighborsClassifier(),\n               \"Support Vector Classifier\": SVC(),\n               \"DecisionTreeClassifier\": DecisionTreeClassifier()}\n\nfor key, classifier in classifiers.items():\n    classifier.fit(X_train, y_train)\n    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")","76ddb477":"log_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n                  'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n\ngrid_log = GridSearchCV(LogisticRegression(), log_reg_params)\ngrid_log.fit(X_train, y_train)\nlog_reg = grid_log.best_estimator_\nprint('Best Estimators: ', log_reg)","16073c84":"log_reg_score = cross_val_score(log_reg, X_train, y_train, cv=5) #We will use best parameters for modeling\nprint('Logistic Regression Cross Validation Score: ', round(log_reg_score.mean() * 100, 2).astype(str) + '%')","38c46d78":"model = LogisticRegression(C=1000, solver='newton-cg')\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))","4c8e5d01":"**Skewness of the Variables**","57eaf3fb":"![](https:\/\/i.pinimg.com\/736x\/88\/fd\/0e\/88fd0ea8830ef739e830439f8eff13c6--abalone-shell-sea-shells.jpg)","4ed7d02f":"**VIF For All Independent Features CombinedVIF For All Independent Features Combined**","5548c11b":"# Model Creation","0ff81132":"**Checking skewness again**","8997e404":"**ANALYSIS**\n- As we can see, under height column, the minimum value is 0. (Not possible)\n- We need to remove that, may be that height is not calculated for that particular sample","b9f798ec":"**ANALYSIS**\n- All independent varibles values are close to zero. So, they are now close to gaussian distribution. :)","2d69f470":"# Import Libraries","30761cdc":"**ANALYSIS**\n- Taking Sqrt Transoformation for Length column.","fd7a55b8":"**ANALYSIS:**\n- Skewness is close to 0 for Normal distribution curve.\n- Height has the highest skewness of 3.17.\n   - May be there are outliers in height, we need to check that and remove them before modeling.\n   - Will check the coorelation with the dependent variable (Rings)\n   - Will use IQR algorithm to remove outliers.","16e11429":"**Boxen Plot: Age Vs Sex**","5c1544c0":"**Normalise Height**\n- By doing Square root transofrmation","c4919098":"# Variance Inflation Factor","45d9040c":"**ANALYSIS**\n- We will remove the above columns, before proceeding any further.","4ffb20c3":"**ANALYSIS**\n- There is no difference in age of rings for male and female (8-19). But in infants, it lies between (5-10)","0cffdc10":"**ANALYSIS**\n- Whole weight and length have slighter higher VIF. We will consider this as of now.","94fa40f1":"# Dependent Variable","38ca0f41":"# We will choose Logistic Regression for further analysis","297cf095":"# Explortory Data Analysis","056d09dd":"**ANALYSIS**\n- No Negative correlation found\n- High coorelation between **Length & Diameter**\n- High corelation between **shucked weight, viscera weight Vs Whole_weight** & **Shell weight vs Whole_weight**","a8526fc0":"# The End","18abe52e":"**Coorelation Plot**","e94ea2ec":"**DROP COLUMNS**","3a5e76f4":"**Count Plot**","8cfb64a0":"**We will be looking into abalone dataset. We will be following traditional approach by applying all the classification algorithms (which will lead to overfitting problem). Using Balanced Undersampling tecnique to get better model and accuracy with no overfitting. Please upvote if you like my dataset.**","010a6aad":"**Box Plots**","0310634f":"# Histograms: Distribution of the Numerical Features","8179b063":"**Independent & Dependent Features**","8eab3d49":"**ANALYSIS**\n- As we can see, previously Height was highly skewed. But now, its normally distributed\n- All skewed data values are close to 0.","56093648":"**Histograms: Understanding the Distribution of the Numerical Features**","9ce900e8":"**ANALYSIS**\n- Skewness of the height is too high. (need to normalise later...)\n- Need to check skewness for all varibles"}}