{"cell_type":{"7264ab1d":"code","ca73943b":"code","782bf275":"code","bf9e7427":"code","3adf1319":"code","76f8ace2":"code","252f1217":"code","393d4ab4":"code","9b2f7950":"code","069a806c":"code","8317c02f":"markdown","eca4da91":"markdown","b0cc7362":"markdown","ebcfef99":"markdown","63b781bf":"markdown","2fa52a8e":"markdown","77569566":"markdown","be716485":"markdown","cdd23003":"markdown","45850c0f":"markdown"},"source":{"7264ab1d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pandas import DataFrame, Series\nimport sklearn \nimport os\n'''for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))'''\nX=pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/train.csv')\nY=X['loss']\nX.drop(['id','loss'],axis=1,inplace=True)","ca73943b":"X.head()","782bf275":"import matplotlib.pyplot as plt\nimport numpy as np\nfig, axs = plt.subplots(10, 10,figsize=[100, 100])\nfor i in range(10):\n    for j in range(10):\n        axs[i,j].plot(X[X.columns[(10*i)+j]][0:1000],Y[0:1000])","bf9e7427":"fig, axs = plt.subplots(10, 10,figsize=[100, 100])\nfor i in range(10):\n    for j in range(10):\n        axs[i,j].plot(X[X.columns[(10*i)+j]][15000:16000],Y[15000:16000])","3adf1319":"from sklearn.metrics import mean_squared_error\nfrom sklearn.mixture import GaussianMixture\nmodel_list=[]\nfor i in range(10):\n    for j in range(10):\n        s=GaussianMixture(n_components=int(max(Y[0:1000])), random_state=10).fit(np.vstack(X[X.columns[(10*i)+j]][0:1000]),np.vstack(Y[0:1000]))\n        model_list.append(s)","76f8ace2":"s=np.zeros((1,1000))\nfor i in range(100):\n    s=s+model_list[i].predict(np.vstack(X[X.columns[i]][1000:2000]))\nmis=[]\nfor i in range(1,1000):\n    r=s\/i\n    rms=mean_squared_error(r[0],Y[1000:2000], squared=False)\n    mis.append(rms)\nprint(mis.index(min(mis)))\nprint(min(mis))","252f1217":"s=np.zeros((1,10000))\nfor i in range(100):\n    s=s+model_list[i].predict(np.vstack(X[X.columns[i]][3000:13000]))\ns=s\/279\nrms=mean_squared_error(s[0],Y[3000:13000], squared=False)\nprint(rms)","393d4ab4":"import numpy as np\nfrom sklearn.mixture import GaussianMixture\nmodel_list=[]\nfor i in range(10):\n    for j in range(10):\n        s=GaussianMixture(n_components=int(max(Y[0:250000])), random_state=5).fit(np.vstack(X[X.columns[(10*i)+j]][0:250000]),np.vstack(Y[0:250000]))\n        model_list.append(s)\n        print(str(10*i+j)) #This could take a while to run, so this print statement serves as a timer","9b2f7950":"s=np.zeros((1,150000))\nX_test=pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/test.csv')\nids=X_test['id'][0:150000]\nX_test.drop(['id'],axis=1,inplace=True)\nfor i in range(100):\n    s=s+model_list[i].predict(np.vstack(X_test[X_test.columns[i]][0:150000]))\n    print(i)\ns=s\/279\nap=pd.DataFrame(s[0])\nconcated=pd.concat([ids,ap],axis=1)\nconcated.rename(columns={0:'loss'},inplace=True)","069a806c":"concated.to_csv('gaussian_sub',index=False)","8317c02f":"Interesting! It would appear that there are definite trends and shapes. I was curious on whether a similar relationship is displayed across the entire dataset. So I selected another subset of a thousand training samples (from 15000-16000) and checked.","eca4da91":"It appears that dividing by 279 yields the lowest RMS value. Applying this information to a larger subset of the test data also yields similar RMS.","b0cc7362":"In this notebook, I tried to use the concept of Gaussian Mixture Models to get the relationship between each feature and the final loss. This was used to predict the loss for the test set.\n\nStarting with the typical data imports:","ebcfef99":"Now we can find how the final loss varies with each of the hundred features. I checked the relationship between each feature and the final loss for the first one thousand training samples.","63b781bf":"To find the best weightage to give to each of the one hundred predictions, I checked the best value to divide the sum of predictions such that mean squared error was minimum. The validation set was the dataset sampled from indices 1000-2000.","2fa52a8e":"Surprisingly (or not surprisingly), the features show a very similar trend and distribution across the dataset with respect to their relationship with the final loss value.\n\nBuilding on this, I tried to fit a Gaussian Mixture Model onto the relationship between each feature and the final loss. I found that setting the number of components to be the maximum loss value was helpful in modelling a suitable distribution.","77569566":"Okay, time to implement this idea for the entire training and test data. Let's train 100 GMMs on the entire training set.","be716485":"Now let's sum up every model's predictions of the test set and divide by 279. We can place the final result into a dataframe 'concated'.","cdd23003":"Here we see the training data set","45850c0f":"In the final step, we convert the dataframe to a csv."}}