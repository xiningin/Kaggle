{"cell_type":{"f6cb6955":"code","daa4727d":"code","cdb675e2":"code","53663492":"code","9e1dff27":"code","8046d4c3":"code","ea3f0b69":"code","13c9f24d":"code","c5e24dc6":"code","dc17318a":"code","10e4f482":"code","26c66a06":"code","aeae3208":"code","64138cac":"code","0b77019a":"code","d311e341":"code","808bec9d":"code","47850cbd":"code","a57c3f2a":"code","ccea9ba0":"code","01fbb4e5":"code","3b0bcf55":"code","998a9ea5":"code","dfd4e5cf":"code","29bad970":"code","b3b961ae":"code","5fc04daf":"code","6163fcfa":"code","b5bcaf22":"code","b6e073f1":"code","443735d1":"code","815bd76e":"code","f259a9c4":"code","7bb2441b":"code","f98c9649":"code","1277d0cf":"code","d3636a0b":"markdown","12746c84":"markdown","ee0494c6":"markdown","dd38410e":"markdown","da8cdb20":"markdown","13e85eb1":"markdown","699e6241":"markdown","755f5e2b":"markdown","68d6955c":"markdown","7360098a":"markdown","98bed5f7":"markdown","ad10773b":"markdown","7fbd78e7":"markdown","a3e6e03b":"markdown","8ce29cae":"markdown","826cb6e7":"markdown","818f94e1":"markdown","e357e850":"markdown","5a19ab87":"markdown","e599c0c5":"markdown","6c7119ce":"markdown","e2442a8a":"markdown","ad618672":"markdown","b940e417":"markdown","23ff7023":"markdown","7f915df6":"markdown","fef17bb4":"markdown","808eb568":"markdown","7cf42899":"markdown","c7eb733c":"markdown","cedddc63":"markdown","4c418c41":"markdown","b209cf14":"markdown","6442b57e":"markdown","86e16cbf":"markdown","3c5b9a5e":"markdown","4e7f0229":"markdown","b484453f":"markdown","5f5dc46c":"markdown","792488fd":"markdown","aac65f07":"markdown","26cd20b9":"markdown","e211e1c0":"markdown","6d089aa8":"markdown","e1e01e83":"markdown","306b2bc7":"markdown","ca3f28d8":"markdown","8ab15696":"markdown","c5f4a136":"markdown","5c80a8ce":"markdown","54c0b4b6":"markdown","6aad1bd8":"markdown","f09e34ab":"markdown","11a05ab5":"markdown","db834a50":"markdown","2d3c61ed":"markdown","9348bf8e":"markdown","692361f4":"markdown","6258d551":"markdown","9f7feaff":"markdown","390ce175":"markdown","88b42e35":"markdown","3584bfd5":"markdown","7395f559":"markdown","301b742c":"markdown"},"source":{"f6cb6955":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\nfrom sklearn.model_selection import KFold,cross_val_score, RepeatedStratifiedKFold,StratifiedKFold\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.preprocessing import OneHotEncoder,StandardScaler,PowerTransformer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.dummy import DummyClassifier\nfrom imblearn.over_sampling import SMOTE\n\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nimport optuna\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import make_column_transformer\n\nfrom sklearn.model_selection import KFold, cross_val_predict, train_test_split,GridSearchCV,cross_val_score\nfrom sklearn.metrics import accuracy_score,classification_report\n\n#importing plotly and cufflinks in offline mode\nimport cufflinks as cf\nimport plotly.offline\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\n\nimport plotly \nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.offline as py\nfrom plotly.offline import iplot\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\n\nimport missingno as msno\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","daa4727d":"pd.set_option('max_columns',100)\npd.set_option('max_rows',900)\n\npd.set_option('max_colwidth',200)\n\ndf = pd.read_csv('..\/input\/heart-failure-prediction\/heart.csv')\ndf.head()","cdb675e2":"df.info()","53663492":"df.duplicated().sum()","9e1dff27":"def missing (df):\n    missing_number = df.isnull().sum().sort_values(ascending=False)\n    missing_percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n    missing_values = pd.concat([missing_number, missing_percent], axis=1, keys=['Missing_Number', 'Missing_Percent'])\n    return missing_values\n\nmissing(df)","8046d4c3":"numerical= df.drop(['HeartDisease'], axis=1).select_dtypes('number').columns\n\ncategorical = df.select_dtypes('object').columns\n\nprint(f'Numerical Columns:  {df[numerical].columns}')\nprint('\\n')\nprint(f'Categorical Columns: {df[categorical].columns}')","ea3f0b69":"df[categorical].nunique()","13c9f24d":"y = df['HeartDisease']\nprint(f'Percentage of patient had a HeartDisease:  {round(y.value_counts(normalize=True)[1]*100,2)} %  --> ({y.value_counts()[1]} patient)\\nPercentage of patient did not have a HeartDisease: {round(y.value_counts(normalize=True)[0]*100,2)}  %  --> ({y.value_counts()[0]} patient)')","c5e24dc6":"df['HeartDisease'].iplot(kind='hist')","dc17318a":"df[numerical].describe()","10e4f482":"df[numerical].iplot(kind='hist');","26c66a06":"df[numerical].iplot(kind='histogram',subplots=True,bins=50)","aeae3208":"skew_limit = 0.75 # This is our threshold-limit to evaluate skewness. Overall below abs(1) seems acceptable for the linear models. \nskew_vals = df[numerical].drop('FastingBS', axis=1).skew()\nskew_cols= skew_vals[abs(skew_vals)> skew_limit].sort_values(ascending=False)\nskew_cols","64138cac":"numerical1= df.select_dtypes('number').columns\n\n\nmatrix = np.triu(df[numerical1].corr())\nfig, ax = plt.subplots(figsize=(14,10)) \nsns.heatmap (df[numerical1].corr(), annot=True, fmt= '.2f', vmin=-1, vmax=1, center=0, cmap='coolwarm',mask=matrix, ax=ax);","0b77019a":"df[categorical].head()","d311e341":"print (f'A female person has a probability of {round(df[df[\"Sex\"]==\"F\"][\"HeartDisease\"].mean()*100,2)} % have a HeartDisease')\n\nprint()\n\nprint (f'A male person has a probability of {round(df[df[\"Sex\"]==\"M\"][\"HeartDisease\"].mean()*100,2)} % have a HeartDisease')\n\nprint()\n","808bec9d":"fig = px.histogram(df, x=\"Sex\", color=\"HeartDisease\",width=400, height=400)\nfig.show()","47850cbd":"df.groupby('ChestPainType')['HeartDisease'].mean().sort_values(ascending=False)","a57c3f2a":"fig = px.histogram(df, x=\"ChestPainType\", color=\"HeartDisease\",width=400, height=400)\nfig.show()","ccea9ba0":"df.groupby('RestingECG')['HeartDisease'].mean().sort_values(ascending=False)","01fbb4e5":"fig = px.histogram(df, x=\"RestingECG\", color=\"HeartDisease\",width=400, height=400)\nfig.show()","3b0bcf55":"df.groupby('ExerciseAngina')['HeartDisease'].mean().sort_values(ascending=False)","998a9ea5":"fig = px.histogram(df, x=\"ExerciseAngina\", color=\"HeartDisease\",width=400, height=400)\nfig.show()","dfd4e5cf":"df.groupby('ST_Slope')['HeartDisease'].mean().sort_values(ascending=False)","29bad970":"fig = px.histogram(df, x=\"ST_Slope\", color=\"HeartDisease\",width=400, height=400)\nfig.show()","b3b961ae":"accuracy =[]\nmodel_names =[]\n\n\nX= df.drop('HeartDisease', axis=1)\ny= df['HeartDisease']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nohe= OneHotEncoder()\nct= make_column_transformer((ohe,categorical),remainder='passthrough')  \n\n\nmodel = DummyClassifier(strategy='constant', constant=1)\npipe = make_pipeline(ct, model)\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\naccuracy.append(round(accuracy_score(y_test, y_pred),4))\nprint (f'model : {model} and  accuracy score is : {round(accuracy_score(y_test, y_pred),4)}')\n\nmodel_names = ['DummyClassifier']\ndummy_result_df = pd.DataFrame({'Accuracy':accuracy}, index=model_names)\ndummy_result_df","5fc04daf":"accuracy =[]\nmodel_names =[]\n\n\nX= df.drop('HeartDisease', axis=1)\ny= df['HeartDisease']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nohe= OneHotEncoder()\nct= make_column_transformer((ohe,categorical),remainder='passthrough')  \n\n\nlr = LogisticRegression(solver='liblinear')\nlda= LinearDiscriminantAnalysis()\nsvm = SVC(gamma='scale')\nknn = KNeighborsClassifier()\n\nmodels = [lr,lda,svm,knn]\n\nfor model in models: \n    pipe = make_pipeline(ct, model)\n    pipe.fit(X_train, y_train)\n    y_pred = pipe.predict(X_test)\n    accuracy.append(round(accuracy_score(y_test, y_pred),4))\n    print (f'model : {model} and  accuracy score is : {round(accuracy_score(y_test, y_pred),4)}')\n\nmodel_names = ['Logistic','LinearDiscriminant','SVM','KNeighbors']\nresult_df1 = pd.DataFrame({'Accuracy':accuracy}, index=model_names)\nresult_df1","6163fcfa":"accuracy =[]\nmodel_names =[]\n\n\nX= df.drop('HeartDisease', axis=1)\ny= df['HeartDisease']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nohe= OneHotEncoder()\ns= StandardScaler()\nct1= make_column_transformer((ohe,categorical),(s,numerical))  \n\n\nlr = LogisticRegression(solver='liblinear')\nlda= LinearDiscriminantAnalysis()\nsvm = SVC(gamma='scale')\nknn = KNeighborsClassifier()\n\nmodels = [lr,lda,svm,knn]\n\nfor model in models: \n    pipe = make_pipeline(ct1, model)\n    pipe.fit(X_train, y_train)\n    y_pred = pipe.predict(X_test)\n    accuracy.append(round(accuracy_score(y_test, y_pred),4))\n    print (f'model : {model} and  accuracy score is : {round(accuracy_score(y_test, y_pred),4)}')\n\nmodel_names = ['Logistic_scl','LinearDiscriminant_scl','SVM_scl','KNeighbors_scl']\nresult_df2 = pd.DataFrame({'Accuracy':accuracy}, index=model_names)\nresult_df2","b5bcaf22":"accuracy =[]\nmodel_names =[]\n\n\nX= df.drop('HeartDisease', axis=1)\ny= df['HeartDisease']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nohe= OneHotEncoder()\nct= make_column_transformer((ohe,categorical),remainder='passthrough')  \n\nada = AdaBoostClassifier(random_state=0)\ngb = GradientBoostingClassifier(random_state=0)\nrf = RandomForestClassifier(random_state=0)\net=  ExtraTreesClassifier(random_state=0)\n\n\n\nmodels = [ada,gb,rf,et]\n\nfor model in models: \n    pipe = make_pipeline(ct, model)\n    pipe.fit(X_train, y_train)\n    y_pred = pipe.predict(X_test)\n    accuracy.append(round(accuracy_score(y_test, y_pred),4))\n    print (f'model : {model} and  accuracy score is : {round(accuracy_score(y_test, y_pred),4)}')\n\nmodel_names = ['Ada','Gradient','Random','ExtraTree']\nresult_df3 = pd.DataFrame({'Accuracy':accuracy}, index=model_names)\nresult_df3","b6e073f1":"accuracy =[]\nmodel_names =[]\n\n\nX= df.drop('HeartDisease', axis=1)\ny= df['HeartDisease']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nohe= OneHotEncoder()\nct= make_column_transformer((ohe,categorical),remainder='passthrough')  \n\nxgbc = XGBClassifier(random_state=0)\nlgbmc=LGBMClassifier(random_state=0)\n\n\nmodels = [xgbc,lgbmc]\n\nfor model in models: \n    pipe = make_pipeline(ct, model)\n    pipe.fit(X_train, y_train)\n    y_pred = pipe.predict(X_test)\n    accuracy.append(round(accuracy_score(y_test, y_pred),4))\n\nmodel_names = ['XGBoost','LightGBM']\nresult_df4 = pd.DataFrame({'Accuracy':accuracy}, index=model_names)\nresult_df4","443735d1":"accuracy =[]\nmodel_names =[]\n\n\nX= df.drop('HeartDisease', axis=1)\ny= df['HeartDisease']\ncategorical_features_indices = np.where(X.dtypes != np.float)[0]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nmodel = CatBoostClassifier(verbose=False,random_state=0)\n\nmodel.fit(X_train, y_train,cat_features=categorical_features_indices,eval_set=(X_test, y_test))\ny_pred = model.predict(X_test)\naccuracy.append(round(accuracy_score(y_test, y_pred),4))\n\nmodel_names = ['Catboost_default']\nresult_df5 = pd.DataFrame({'Accuracy':accuracy}, index=model_names)\nresult_df5\n\n","815bd76e":"def objective(trial):\n    X= df.drop('HeartDisease', axis=1)\n    y= df['HeartDisease']\n    categorical_features_indices = np.where(X.dtypes != np.float)[0]\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n    param = {\n        \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n        \"depth\": trial.suggest_int(\"depth\", 1, 12),\n        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n        \"bootstrap_type\": trial.suggest_categorical(\n            \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n        ),\n        \"used_ram_limit\": \"3gb\",\n    }\n\n    if param[\"bootstrap_type\"] == \"Bayesian\":\n        param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n    elif param[\"bootstrap_type\"] == \"Bernoulli\":\n        param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n\n    cat_cls = CatBoostClassifier(**param)\n\n    cat_cls.fit(X_train, y_train, eval_set=[(X_test, y_test)], cat_features=categorical_features_indices,verbose=0, early_stopping_rounds=100)\n\n    preds = cat_cls.predict(X_test)\n    pred_labels = np.rint(preds)\n    accuracy = accuracy_score(y_test, pred_labels)\n    return accuracy\n\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=50, timeout=600)\n\n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n\n    print(\"Best trial:\")\n    trial = study.best_trial\n\n    print(\"  Value: {}\".format(trial.value))\n\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))","f259a9c4":"accuracy =[]\nmodel_names =[]\n\n\nX= df.drop('HeartDisease', axis=1)\ny= df['HeartDisease']\ncategorical_features_indices = np.where(X.dtypes != np.float)[0]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nmodel = CatBoostClassifier(verbose=False,random_state=0,\n                          objective= 'CrossEntropy',\n    colsample_bylevel= 0.04292240490294766,\n    depth= 10,\n    boosting_type= 'Plain',\n    bootstrap_type= 'MVS')\n\nmodel.fit(X_train, y_train,cat_features=categorical_features_indices,eval_set=(X_test, y_test))\ny_pred = model.predict(X_test)\naccuracy.append(round(accuracy_score(y_test, y_pred),4))\nprint(classification_report(y_test, y_pred))\n\nmodel_names = ['Catboost_tuned']\nresult_df6 = pd.DataFrame({'Accuracy':accuracy}, index=model_names)\nresult_df6\n\n","7bb2441b":"feature_importance = np.array(model.get_feature_importance())\nfeatures = np.array(X_train.columns)\nfi={'features':features,'feature_importance':feature_importance}\ndf_fi = pd.DataFrame(fi)\ndf_fi.sort_values(by=['feature_importance'], ascending=True,inplace=True)\nfig = px.bar(df_fi, x='feature_importance', y='features',title=\"CatBoost Feature Importance\",height=500)\nfig.show()","f98c9649":"result_final = pd.concat([dummy_result_df,result_df1,result_df2,result_df3,result_df4,result_df5,result_df6],axis=0)","1277d0cf":"result_final.sort_values(by=['Accuracy'], ascending=True,inplace=True)\nfig = px.bar(result_final, x='Accuracy', y=result_final.index,title='Model Comparison',height=600,labels={'index':'MODELS'})\nfig.show()","d3636a0b":"<a id=\"5\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Categorical Features<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","12746c84":"- We'll use dummy classifier model as a base model\n-  And then we will use Logistic & Linear Discriminant & KNeighbors and Support Vector Machine models with and without scaler.\n- And then we will use ensemble models, Adaboost, Randomforest, Gradient Boosting and Extra Trees\n- We will see famous trio: XGBoost,LightGBM & Catboost\n- Finally we will look in detail to hyperparameter tuning for Catboost\n- Let's start.","ee0494c6":"### **RestingECG and Heart Disease**","dd38410e":"gif credit: https:\/\/giphy.com","da8cdb20":"### **ST_Slope and Heart Disease**","13e85eb1":"- As expected, with scaler, both KNN and SVM did a better job with the scaler than their previous performances.","699e6241":"- We have developed model to classifiy heart disease cases.\n\n- First, we  made the detailed exploratory analysis.\n- We have decided which metric to use.\n- We analyzed both target and features in detail.\n- We transform categorical variables into numeric so we can use them in the model.\n- We use pipeline to avoid data leakage.\n- We looked at the results of the each model and selected the best one for the problem on hand.\n- We looked in detail Catboost\n- We made hyperparameter tuning of the Catboost with Optuna to see the improvement\n- We looked at the feature importance.\n\n\n\n- After this point it is up to you to develop and improve the models.  **Enjoy** \ud83e\udd18","755f5e2b":"- Based on the  matrix, we can observe weak level correlation between the numerical features and the target variable\n- Oldpeak (depression related number) has a positive correlation with the heart disease.\n- Maximum heart rate has negative correlation with the heart disease.\n- interestingly cholesterol has negative correlation with the heart disease.\n","68d6955c":"<a id=\"15\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Model Comparison<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","7360098a":"<a id=\"16\"><\/a>\n<font color=\"darkblue\" size=+1.5><b>Conclusion<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>\n","98bed5f7":"- So far so good. No zero variance and no extremely high variance.","ad10773b":"<a id=\"17\"><\/a>\n<font color=\"darkblue\" size=+1.5><b>References & Further Reading<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>\n\n\n[Machine Learning - Beginner &Intermediate-Friendly BOOKS](https:\/\/www.kaggle.com\/general\/255972)","7fbd78e7":"image credit: https:\/\/avatars.mds.yandex.net","a3e6e03b":"- With their deafult values, Catboost did better job than the other two models.","8ce29cae":"<a id=\"13\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Catboost HyperParameter Tuning with Optuna<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","826cb6e7":"# Heart Failure Prediction Dataset","818f94e1":"- No missing values.","e357e850":"<a id=\"12\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b> CATBOOST<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","5a19ab87":"![](https:\/\/media.giphy.com\/media\/3o7TKUM3IgJBX2as9O\/giphy.gif)","e599c0c5":"DATA DICTONARY\t\t\t\t\t\t\n\t\t\t\t\t\t\n1\t**Age**: \t\t\tAge of the patient [years] \t\t\n2\t**Sex**:  \t\t\t Sex of the patient [M: Male, F: Female] \t\t\n3\t**ChestPainType**: \t\t\t[TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic] \t\t\n4\t**RestingBP**:\t\t\tResting blood pressure [mm Hg] \t\t\n5\t**Cholesterol**:\t\t\tSerum cholesterol [mm\/dl] \t\t\n6\t**FastingBS**:\t\t\t Fasting blood sugar [1: if FastingBS > 120 mg\/dl, 0: otherwise]\t\t\n7\t**RestingECG**:\t\t\t Resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria] \t\t\n8\t**MaxHR**:\t\t\tMaximum heart rate achieved [Numeric value between 60 and 202]\t\t\n9\t**ExerciseAngina**:\t\t\tExercise-induced angina [Y: Yes, N: No]\t\t\n10\t**Oldpeak**:\t\t\t ST [Numeric value measured in depression] (\t\t\n11\t**ST_Slope**:\t\t\t The slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping] \t\t\n12\t**HeartDisease**:\t\t\t Output class [1: heart disease, 0: Normal] \t\t\n\nReference: https:\/\/www.kaggle.com\/fedesoriano\/heart-failure-prediction","6c7119ce":"**Purpose**: \n   \n   Training and applying models for the classification problems. Provides compatibility with the scikit-learn tools.\n\n**The default optimized objective depends on various conditions**:\n\n**Logloss** \u2014 The target has only two different values or the target_border parameter is not None.\n\n**MultiClass** \u2014 The target has more than two different values and the border_count parameter is None.\n\nReference: https:\/\/catboost.ai\/en\/docs\/concepts\/python-reference_catboostclassifier\n","e2442a8a":"- No duplicates","ad618672":"- Let's see how ensemble models do with the problem at hand.","b940e417":"#### Hi all.  \ud83d\ude4b\n\n#### We continue our **Beginner-Intermediate Friendly Machine Learning series**, which would help anyone who wants to learn or refresh the basics of ML.\n\n#### What we have covered: \n\n#### [Beginner Friendly Detailed Explained EDAs \u2013 For anyone at the beginnings of DS\/ML journey](https:\/\/www.kaggle.com\/general\/253911#1393015) \u2714\ufe0f\n\n#### [BIAS & VARIANCE TRADEOFF](https:\/\/www.kaggle.com\/kaanboke\/ml-basics-bias-variance-tradeoff) \u2714\ufe0f\n\n#### [LINEAR ALGORITHMS](https:\/\/www.kaggle.com\/kaanboke\/ml-basics-linear-algorithms)  \u2714\ufe0f\n\n#### [NONLINEAR ALGORITHMS](https:\/\/www.kaggle.com\/kaanboke\/nonlinear-algorithms)  \u2714\ufe0f\n\n#### [The Most Used Methods to Deal with MISSING VALUES](https:\/\/www.kaggle.com\/kaanboke\/the-most-used-methods-to-deal-with-missing-values)  \u2714\ufe0f\n\n#### [Beginner Friendly End to End ML Project- Classification with Imbalanced Data](https:\/\/www.kaggle.com\/kaanboke\/beginner-friendly-end-to-end-ml-project-enjoy)  \u2714\ufe0f\n\n#### [How to Prevent the Data Leakage ?](https:\/\/www.kaggle.com\/kaanboke\/how-to-prevent-the-data-leakage) \u2714\ufe0f\n\n#### [The Most Common EVALUATION METRICS- A Gentle Intro](https:\/\/www.kaggle.com\/kaanboke\/the-most-common-evaluation-metrics-a-gentle-intro) \u2714\ufe0f\n\n#### [Feature Selection-The Most Common Methods to Know](https:\/\/www.kaggle.com\/kaanboke\/feature-selection-the-most-common-methods-to-know) \u2714\ufe0f\n\n#### [Beginner Friendly End to End ML Project- Car Price Prediction](https:\/\/www.kaggle.com\/kaanboke\/car-price-prediction-beginner-friendly-94-3)  \u2714\ufe0f\n\n#### In this notebook we will implement new beginner friendly end to end ML model by using **CATBOOST with Optuna HyperParameter Tuning**\n\n\n#### Enjoy \ud83e\udd18","23ff7023":"### **Gender and Heart Disease**","7f915df6":"### Overall Insights from the Exploratory Data Analysis","fef17bb4":"- ST_Slope: the slope of the peak exercise ST segment has differences.\n-  ST_Slope Up significantly less likely has heart disease than the other two segment.","808eb568":"- Almost 55% of the patients had a heart disease.\n-  508 patient had a heart disease.\n- Almost 45%  of patients didn't have a heart disease.\n- 410 patient didn't have a heart disease.\n","7cf42899":"<a id=\"3\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Target Variable<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","c7eb733c":"<a id=\"9\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b> Logistic & Linear Discriminant & SVC & KNN with Scaler<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","cedddc63":"- Nothing much for the skewness. Quite a normal like distribution for the numerical features.","4c418c41":"![](https:\/\/media.giphy.com\/media\/MuAz5ozYsaC7rkRUe2\/giphy-downsized-large.gif)","b209cf14":"gif credit: https:\/\/media.giphy.com\/","6442b57e":"<a id=\"6\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>MODEL SELECTION<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","86e16cbf":"<a id=\"8\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Logistic & Linear Discriminant & SVC & KNN<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","3c5b9a5e":"- We can observe clear differences among the chest pain type.\n- Person with ASY: Asymptomatic chest pain  has almost 6 times more likely have a heart disease than person with ATA Atypical Angina chest pain.\n","4e7f0229":"- Accuracy scores are very close to each other.\n- Both Random Forest and Extra tree got similar accuracy scores.\n- Both model can be improved by hyperparameter tuning.","b484453f":"- Let's import the libraries","5f5dc46c":"<a id=\"toc\"><\/a>\n\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Table of Contents<\/h3>\n    \n* [Data](#0)\n* [What Problem We Have and Which Metric to Use?](#1)\n\n* [Exploratory Data Analysis](#2)\n    * [Target Variable](#3)\n    * [Numerical Features](#4)\n    * [Categorical Features](#5)    \n    \n* [Model Selection](#6)    \n    * [Baseline Model](#7)\n    * [Logistic & Linear Discriminant & SVC & KNN](#8)\n    * [Logistic & Linear Discriminant & SVC & KNN with Scaler](#9)    \n    * [Ensemble Models (AdaBoost & Gradient Boosting & Random Forest & Extra Trees)](#10)\n    * [Famous Trio (XGBoost & LightGBM & Catboost)](#11)\n    * [CATBOOST](#12)\n    * [Catboost HyperParameter Tuning with OPTUNA](#13)\n    * [Feature Importance](#14)    \n    * [Model Comparison](#15)  \n    \n    \n\n\n* [Conclusion](#16)\n\n* [References & Further Reading](#17)","792488fd":"- Let's make some adjustment on the Catboost model to see its' peak performance on the problem.","aac65f07":"- OK. Let's see the very famous trio:\n  - XGBoost\n  - Light GBM\n  - Catboost","26cd20b9":"- I'll use Catboost alone by using its capability to handle categorical variables without doing any preprocessing.\n- Let's first look at the XGBoost and LightGBM","e211e1c0":"- Now let's see Catboost","6d089aa8":"- RestingECG: resting electrocardiogram results don't differ much.\n- Person with ST: having ST-T wave abnormality is more likely have a heart disease than the others.","e1e01e83":"<a id=\"4\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Numerical Features<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","306b2bc7":"- Target variable has close to balanced data.\n- Numerical features have weak correlation with the target variable.\n- Oldpeak (depression related number) has a positive correlation with the heart disease.\n- Maximum heart rate has negative correlation with the heart disease.\n- Interestingly cholesterol has negative correlation with the heart disease.\n- Based on the gender; Men are almost 2.44 times more likely have a heart disease than women.\n- We can observe clear differences among the chest pain type.\n- Person with ASY: Asymptomatic chest pain  has almost 6 times more likely have a heart disease than person with ATA Atypical Angina chest pain.\n- RestingECG: resting electrocardiogram results don't differ much.\n- Person with ST: having ST-T wave abnormality is more likely have a heart disease than the others.\n- ExerciseAngina: exercise-induced angina with 'Yes' almost 2.4 times more likley have a heart disaese than exercise-induced angina with 'No'\n- ST_Slope: the slope of the peak exercise ST segment has differences.\n- ST_Slope Up significantly less likely has heart disease than the other two segment.\n\n\n","ca3f28d8":"<a id=\"2\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>Exploratory Data Analysis<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","8ab15696":"- We have lift from 0.8804 to .9094","c5f4a136":"<a id=\"14\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Feature Importance<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","5c80a8ce":"- There is a little imblanace but nothing in the disturbing level.\n- We can use 'accuracy' metric as our evaluation metric.","54c0b4b6":"<a id=\"1\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>What Problem We Have and Which Metric to Use?<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","6aad1bd8":"<a id=\"7\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Baseline Model<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","f09e34ab":"![](https:\/\/avatars.mds.yandex.net\/get-bunker\/56833\/dba868860690e7fe8b68223bb3b749ed8a36fbce\/orig)","11a05ab5":"- Based on the data and data dictionary, We have a classification problem.\n- We wil make classification on the target variable **Heart Disease**\n- And we will build a model to get best calssification possible on the target variable.\n- For that we will look at the balance of the target variable.\n- As we will see later, our target variable has balanced or balanced like data\n- For that reason we will use **Accuracy score**.\n- [For the detailed info about the evaluation metrics](https:\/\/www.kaggle.com\/kaanboke\/the-most-common-evaluation-metrics-a-gentle-intro)","db834a50":"### **Chest Pain Type and Heart Disease**","2d3c61ed":"#### By the way, when you like the topic, you can show it by supporting \ud83d\udc4d\n\n####  **Feel free to leave a comment**. \n\n#### All the best \ud83e\udd18","9348bf8e":"- Overall data types seems ok.","692361f4":"**Parameters**:\n\n- **Objective**:  Supported metrics for overfitting detection and best model selection \n\n- **colsample_bylevel**: this parameter speeds up the training and usually does not affect the quality.\n\n- **depht** : Depth of the tree.\n\n\n- **boosting_type** : By default, the boosting type is set to for small datasets. This prevents overfitting but it is expensive in terms of computation. Try to set the value of this parameter to  to speed up the training.\n\n- **bootstrap_type** : By default, the method for sampling the weights of objects is set to . The training is performed faster if the method is set and the value for the sample rate for bagging is smaller than 1.\n\n\nReference: https:\/\/catboost.ai\/\n\n\n","6258d551":"<a id=\"11\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Famous Trio (XGBoost & LightGBM & Catboost)<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","9f7feaff":"<a id=\"0\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>Data<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","390ce175":"- ExerciseAngina: exercise-induced angina with 'Yes' almost 2.4 times more likley have a heart disaese than exercise-induced angina with 'No'","88b42e35":"### **ExerciseAngina and Heart Disease**","3584bfd5":"<a id=\"10\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Ensemble Models (AdaBoost & Gradient Boosting & Random Forest & Extra Trees)<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","7395f559":"- Ok let's use our best model with new parameters.","301b742c":"- Bad news guys....\n- Men are almost 2.44 times more likely have a heart disease than women."}}