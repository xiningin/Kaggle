{"cell_type":{"1dd90070":"code","218739a2":"code","ea20f8ca":"code","af614cc2":"code","b98e7b21":"code","88057558":"code","69e5cd87":"code","cfbbec9a":"code","932481cc":"code","89a50e6b":"code","9d5bbafa":"code","6ad0c936":"code","311bcfa2":"code","820bcbff":"code","b3c2e890":"code","319a9311":"code","638c1532":"code","92e472ff":"code","107503f3":"code","7bce4126":"code","ffc2e812":"code","8d99501e":"code","9fc410e2":"code","82067a38":"code","115e87d6":"code","a8a2618d":"code","ca54a4dc":"code","43cd4214":"code","f47062dd":"code","2f1693af":"code","6e30fbe7":"code","bcacd9b9":"code","c9f57c50":"markdown","ffa28cc6":"markdown","a471113e":"markdown","bdf4705f":"markdown","fc76b2c8":"markdown","c6cf4355":"markdown","e75eff09":"markdown","015c80a3":"markdown","bd3a3662":"markdown","7d01960d":"markdown","1adf8f8a":"markdown","66f35910":"markdown","f0effd11":"markdown","b2b52f2f":"markdown","779e5d46":"markdown"},"source":{"1dd90070":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","218739a2":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport re\n\nimport nltk \nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nfrom wordcloud import WordCloud\n\nfrom PIL import Image\n\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nimport gensim.models.keyedvectors as word2vec\n\nfrom tensorflow.keras.layers import Embedding,Dense,LSTM,Bidirectional\nfrom tensorflow.keras.layers import BatchNormalization,Dropout\n\nfrom tensorflow.keras import Sequential\n\nimport pickle","ea20f8ca":"cols = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\nencoding = 'latin'\ndf = pd.read_csv('\/kaggle\/input\/sentiment140\/training.1600000.processed.noemoticon.csv',encoding=encoding,names=cols)\ndf.head()","af614cc2":"print('Length of dataset is:',len(df))","b98e7b21":"df.info()","88057558":"#The polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n#Let's look at the count\nplt.figure(figsize=(16,8))\nsns.countplot(df['target'])\nplt.title('Distribution of tweets',fontsize = 15)\nplt.show()","69e5cd87":"#Here we can see that only 0 and 4 are present.Let's substitute 0 to 0 and 4 to 1 for convenience sake\ndf['target'].replace({0:0,4:1},inplace = True)","cfbbec9a":"#Removing links,special character,@usernames and stopwords.\n#Also applying lemmatization to the words\n\npattern = '@\\S+|https?:\\S+|http?:\\S|[^A-Za-z]+|com|net'\n\nstop_words = stopwords.words('english')\nlemma = WordNetLemmatizer()\n\ndef preprocess(text):\n    text = re.sub(pattern, ' ', str(text).lower()).strip()\n    tokens = []\n    for token in text.split():\n        if token not in stop_words:\n            tokens.append(lemma.lemmatize(token))\n    return ' '.join(tokens)","932481cc":"df.text = df.text.apply(lambda x: preprocess(x))\ndf.head()","89a50e6b":"#Positive words\npos = ' '.join(df[df['target'] == 1].text)\n\n\n\nimg_pos = np.array(Image.open('..\/input\/mask-img\/6ir5bE75T.jpg'))\nplt.figure(figsize = (15,9))\npw = WordCloud(mask = img_pos).generate(pos)\nplt.imshow(pw)","9d5bbafa":"#Negative words\nneg = ' '.join(df[df['target'] == 0].text)\n\nimg_neg = np.array(Image.open('..\/input\/mask-img\/angry-emoji_53876-25519.jpg'))\nplt.figure(figsize = (15,9))\nnw = WordCloud(mask = img_neg).generate(neg)\nplt.imshow(nw)","6ad0c936":"test_size = 0.2\n\ndf_train,df_test = train_test_split(df,test_size = test_size)","311bcfa2":"df_train.head()","820bcbff":"df_train_clean = df_train[['target','text']]\ndf_test_clean = df_test[['target','text']]","b3c2e890":"df_train_clean.head()","319a9311":"max_length = 20\ntrunc_type = 'post'\npadd_type = 'post'\n\n\ntokens = Tokenizer()\ntokens.fit_on_texts(df_train_clean['text'])\n\ntraining_seq = tokens.texts_to_sequences(df_train_clean['text'])\nX_train = pad_sequences(training_seq,maxlen = max_length,padding=padd_type,truncating=trunc_type)\n\ntesting_seq =  tokens.texts_to_sequences(df_test_clean['text'])\nX_test = pad_sequences(testing_seq,maxlen = max_length,padding=padd_type,truncating=trunc_type)\n","638c1532":"print('Shape of X_train is ',X_train.shape)\nprint('Shape of X_test is ',X_test.shape)","92e472ff":"#Declaring target labels \ny_train = df_train_clean['target']\ny_test = df_test_clean['target']","107503f3":"#Converting everything to numpy arrays\n\nX_train = np.array(X_train)\ny_train = np.array(y_train)\nX_test = np.array(X_test)\ny_test = np.array(y_test)","7bce4126":"word2vec_dict = word2vec.KeyedVectors.load_word2vec_format('..\/input\/googlenewsvectorsnegative300\/GoogleNews-vectors-negative300.bin',binary = True)\n\n\nembeddings_index = dict()\nfor word in word2vec_dict.vocab:\n    embeddings_index[word] = word2vec_dict.word_vec(word)\n    \nprint('Length of word2vec dictionary is :',len(embeddings_index))","ffc2e812":"vocab_size = len(tokens.word_index) + 1\n\nembed_size = 300\nembedding_matrix = np.zeros((vocab_size,embed_size))\n\nfor word,tok in tokens.word_index.items():\n    if word in embeddings_index.keys():\n        embedding_vector = embeddings_index[word]\n        embedding_matrix[tok] =  embedding_vector\n\nprint('Shape of Embedding Matrix is :',embedding_matrix.shape)","8d99501e":"#Initialising Embedding Layer\n\nembedding_layer = Embedding(vocab_size,embed_size,weights = [embedding_matrix],input_length = max_length,trainable = False)\n","9fc410e2":"model = Sequential()\nmodel.add(embedding_layer)\nmodel.add(Bidirectional(LSTM(64,return_sequences = True)))\nmodel.add(BatchNormalization())\nmodel.add(Bidirectional(LSTM(64)))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(64,activation = 'relu'))\nmodel.add(Dense(32,activation = 'relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(1,activation = 'sigmoid'))\n\nmodel.summary()\n\nmodel.compile(loss = 'binary_crossentropy',optimizer = 'adam',metrics = ['accuracy'])\n","82067a38":"#Training\nbatch_size = 16384\nnum_epochs = 30\n\nhist = model.fit(X_train,y_train,batch_size = batch_size,epochs = num_epochs,verbose = 1)","115e87d6":"score = model.evaluate(X_test, y_test, batch_size=batch_size)\nprint()\nprint(\"Accuracy of model is :\",round(score[1],2))\nprint(\"Loss of model is :\",round(score[0],2))","a8a2618d":"acc = hist.history['accuracy']\nloss = hist.history['loss']\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'b', label='Training acc')\nplt.title('Training accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()","ca54a4dc":"plt.plot(epochs, loss, 'r', label='Training loss')\nplt.title('Training loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","43cd4214":"def decode_sentiment(score):\n    label = None\n    if score <= 0.4:\n        label = 'Negative'\n    elif score >= 0.7:\n        label = 'Positive'\n    else:\n        label = 'Neutral'\n    \n    return label\n","f47062dd":"def predict(text):\n    x_test = pad_sequences(tokens.texts_to_sequences([text]), maxlen=max_length,padding=padd_type,truncating=trunc_type)\n    score = model.predict([x_test])[0]\n    \n    label = decode_sentiment(score)\n\n    return {\"label\": label, \"score\": float(score)}  ","2f1693af":"predict('I love fishing')","6e30fbe7":"predict('Economy is going down')","bcacd9b9":"model.save('model.h5')\npickle.dump(tokens,open('tokens.pkl','wb'),protocol = 0)","c9f57c50":"# Importing libraries","ffa28cc6":"We can train whole embedding in Keras but i am gonna use pretrained embedding using Transfer Learning.\n\nI am going to use Word2Vec here\n\nLink for the pretrained embedding [Word2Vec](https:\/\/www.kaggle.com\/sandreds\/googlenewsvectorsnegative300)","a471113e":"# Model Training","bdf4705f":"# Tokenisation","fc76b2c8":"Here we can see that words like *lol,love,quot* occur both in positive and negative sentiment.","c6cf4355":"# Evaluating model","e75eff09":"No null values then","015c80a3":"# Visualising the Positive and Negative words using Wordcloud","bd3a3662":"# Data Preprocessing","7d01960d":"# Saving model","1adf8f8a":"# Train Test Split","66f35910":"Regex Cheatsheet - [Link](https:\/\/cheatography.com\/mutanclan\/cheat-sheets\/python-regular-expression-regex\/)","f0effd11":"# Predicting","b2b52f2f":"# Word Embedding","779e5d46":"Now the columns except target and text are useless.Hence removing it from both train and test set"}}