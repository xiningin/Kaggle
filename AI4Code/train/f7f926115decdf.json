{"cell_type":{"b794b620":"code","e3eadf40":"code","06effcd6":"code","e6dbffb9":"code","9a6f40d5":"code","c7aa65c1":"code","6c6c5fc4":"code","c6230313":"code","5e41f214":"code","0d3104b8":"code","61e8b0c4":"code","9fa9363b":"code","2fe20d4e":"code","7cb0d036":"code","d3c51eb8":"code","cab8037a":"code","5e98aad5":"code","060f67ec":"code","12500270":"code","6ec726d5":"code","24ac8b17":"code","c8c4a585":"code","b77776e2":"code","cbf12fb3":"code","8501d962":"code","aef1432f":"code","e48df138":"code","93957a3e":"code","55521c2f":"code","7a3af746":"code","5e1a0a09":"code","f99f3d1e":"code","770c93b1":"code","c29085da":"code","6ab87487":"code","d03d7327":"code","09aef4e5":"code","665d6640":"code","5bba19cc":"code","59aec83a":"code","66f472ad":"code","4bcc528b":"code","bee693b4":"code","36e22ad2":"code","d252e501":"code","3cc8936d":"code","118a0064":"code","3961f06d":"code","4a999bdb":"code","ed31cf13":"code","07653534":"code","d04ef558":"code","2c0f06e6":"code","b00e0ce8":"code","66594655":"code","a915b089":"code","10a99a7f":"code","c3d77cce":"code","e94dfaae":"code","72760ef4":"code","e42d549c":"code","2fef43d3":"code","c12b320d":"code","19405faf":"code","01ed1c51":"code","fd4fe7f0":"code","3c6384d5":"code","d40ac2a3":"code","ee9b4ac3":"code","91b751e8":"code","664a02d8":"code","f6434ec0":"code","ba48f617":"code","0eded769":"code","657f31fc":"code","880cf43a":"code","a97c8271":"code","8b81a494":"code","853a117c":"code","6a8d0890":"markdown","50db3e48":"markdown","a888ea9b":"markdown","a5de4ca8":"markdown","e298991a":"markdown","c62e2015":"markdown","cd40ab36":"markdown","1947726d":"markdown","c884e817":"markdown","e60fb767":"markdown","f6c7d203":"markdown","7f5cc8f8":"markdown","803e3e03":"markdown"},"source":{"b794b620":"import os\nimport numpy as np\nimport pandas as pd\n# import dask.dataframe as dd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error as sk_mse\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.linear_model import LinearRegression\nfrom bayes_opt import BayesianOptimization\nimport matplotlib.pyplot as plt\n\nsns.set(color_codes=True)","e3eadf40":"FOLDER = \"..\/input\/\"\nPATH_WRITE = \"\/kaggle\/working\/\"\n\nfolder_nyse = \"nyse\"\nfilename_securities   = \"securities.csv\"\nfilename_fundamentals = \"fundamentals.csv\"\nfilename_prices       = \"prices-split-adjusted.csv\"\n\nmax_rows = None","06effcd6":"[elem for elem in os.listdir(\"\/kaggle\/working\")]","e6dbffb9":"df_sec = pd.read_csv(os.path.join(FOLDER, folder_nyse, filename_securities), nrows=max_rows)","9a6f40d5":"def parse_state(address):\n    return address.split(\",\")[-1].strip()\n\ndf_sec[\"state\"] = df_sec[\"Address of Headquarters\"].apply(parse_state)","c7aa65c1":"df_sec.head()","6c6c5fc4":"sns.catplot(\n    y=\"state\", kind=\"count\", data=df_sec,\n    aspect=2.0,\n    order = df_sec['state'].value_counts().index\n           )","c6230313":"sns.catplot(\n    y=\"GICS Sub Industry\", kind=\"count\", data=df_sec,\n    aspect=2.0,\n    order = df_sec['GICS Sub Industry'].value_counts().index[:30]\n           )","5e41f214":"df_fun = pd.read_csv(os.path.join(FOLDER, folder_nyse, filename_fundamentals), nrows=max_rows)","0d3104b8":"df_fun.drop(columns=\"Unnamed: 0\", inplace=True)\n\ndf_fun.head()","61e8b0c4":"df_fun.info()","9fa9363b":"df_fun['Ticker Symbol'].value_counts().max()","2fe20d4e":"folder_fundamentals10 = \"some-nice-data-indeed\"\nfilename_fundamentals10 = \"sim_fin_nyse_fundamentals_2007-2016.csv\"","7cb0d036":"df_fun10 = pd.read_csv(os.path.join(FOLDER, folder_fundamentals10, filename_fundamentals10),\n                       nrows=max_rows)\n\n# n_samples = len(df_fun10) if max_rows is None else max_rows\n# df_fun10.sample(n=n_samples)\n\ndf_fun10.shape","d3c51eb8":"df_fun10.head()","cab8037a":"df_fun10[\"datetime\"] = pd.to_datetime(df_fun10[\"date\"])\ndf_fun10[\"quarter\"] = df_fun10[\"datetime\"].dt.to_period(\"Q\")\n\ndf_fun10.head()","5e98aad5":"df_fun10[\"year\"] = df_fun10[\"datetime\"].dt.year.map(str)\ndf_fun10[\"year\"].value_counts().plot(kind=\"barh\")","060f67ec":"# df_fun10.set_index([\"Ticker\", \"year_quarter\"])\ndf_fun_pivot = df_fun10.pivot_table(\n    index=[\"Ticker\", \"quarter\"],\n    columns=\"Indicator Name\",\n    values=\"Indicator Value\",\n    aggfunc=np.mean\n)\ndf_fun_pivot.head()","12500270":"df_fun_pivot.shape","6ec726d5":"df_pri_daily = pd.read_csv(os.path.join(FOLDER, folder_nyse, filename_prices), nrows=max_rows)","24ac8b17":"df_pri_daily[\"datetime\"] = pd.to_datetime(df_pri_daily[\"date\"])\ndf_pri_daily[\"quarter\"] = df_pri_daily[\"datetime\"].dt.to_period(\"Q\")\n\ndf_pri_daily.head()","c8c4a585":"df_pri = (df_pri_daily\n          .sort_values(by=[\"symbol\", \"date\"], ascending=True)\n          .drop_duplicates(subset=[\"symbol\", \"quarter\"], keep=\"last\")\n         )","b77776e2":"sns.catplot(\n    y=\"quarter\", kind=\"count\", data=df_pri,\n    aspect=2.0,\n    order = df_pri['quarter'].value_counts().index[:30]\n           )","cbf12fb3":"target = \"close_rel_increase_prev_quarter\"\n\ndf_pri.sort_values([\"symbol\", \"quarter\"], inplace=True)\ndf_pri[target] = (df_pri\n                  .groupby([\"symbol\"])[\"close\"]\n                  .transform(lambda x: x.pct_change())\n                 )\ndf_pri.sort_index(inplace=True)","8501d962":"df_pri.head()","aef1432f":"df_pri[\"baseline_pred_same_prev\"]       = df_pri.groupby([\"symbol\"])[target].transform(lambda x: x.shift())\ndf_pri[\"baseline_pred_mean\"]            = df_pri.groupby([\"symbol\"])[target].transform(lambda x: x.expanding().mean().shift())\ndf_pri[\"baseline_pred_exp_weight_mean\"] = df_pri.groupby([\"symbol\"])[target].transform(lambda x: x.ewm(halflife=1).mean().shift())\ndf_pri.dropna(subset=[\"baseline_pred_same_prev\", \"baseline_pred_mean\", \"baseline_pred_exp_weight_mean\"])","e48df138":"df_pri.head()","93957a3e":"df_fun_pivot.reset_index(inplace=True)\ndf_fun_pivot.info()","55521c2f":"df_fun_pivot[\"next_quarter\"] = (df_fun_pivot[\"quarter\"].dt.end_time + pd.Timedelta(days=365\/4)\n                               ).dt.to_period(\"Q\")\n\ndf_fun_pivot.head()","7a3af746":"baseline_cols = [\"baseline_pred_same_prev\", \"baseline_pred_mean\", \"baseline_pred_exp_weight_mean\"]\n\nfor col in baseline_cols + [target]:\n    sns.distplot(df_pri[col].dropna(), norm_hist=True, hist=False, label=col)\nplt.legend()","5e1a0a09":"corr = df_pri[baseline_cols + [target]].corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    ax = sns.heatmap(corr, mask=mask, vmax=.3, square=True, cmap=sns.light_palette(\"green\"), linewidths=.5)","f99f3d1e":"symbols = set(df_pri[\"symbol\"].unique())\ntickers = set(df_fun_pivot[\"Ticker\"].unique())\n\nn_stocks_common = len(symbols.intersection(tickers))\nn_symbols_unmatched = len(symbols.difference(tickers))\nn_tickers_unmatched = len(tickers.difference(symbols))\n\nprint(n_stocks_common, n_symbols_unmatched, n_tickers_unmatched)","770c93b1":"df = df_pri.merge(df_fun_pivot, left_on=[\"symbol\", \"quarter\"], right_on=[\"Ticker\", \"next_quarter\"])","c29085da":"df[df[\"Ticker\"] == \"AMZN\"]","6ab87487":"df.head()","d03d7327":"df_fun_pivot.head(20)","09aef4e5":"df.shape","665d6640":"# df.to_csv(os.path.join(FOLDER, \"df.csv\"))","5bba19cc":"target = \"close_rel_increase_prev_quarter\"\nbaseline_cols = [\"baseline_pred_same_prev\", \"baseline_pred_mean\", \"baseline_pred_exp_weight_mean\"]\n\n# df[\"baseline_pred\"] = df.groupby([\"symbol\"])[target].transform(lambda x: x.shift())","59aec83a":"df = df.sort_values(\"quarter_x\")\n\nn_quarters = df[\"quarter_x\"].nunique()\n\ncols_numeric = df.select_dtypes(include=[np.number]).columns.tolist()\ncols_numeric.remove(target)\n\nfor col in cols_numeric:  # do not forget about expanding for avoiding data leakage!\n    df[col + \"_todate_mean\"] = df.groupby([\"symbol\"])[col].transform(lambda x: x.expanding().mean())\n    df[col + \"_todate_var\"]  = df.groupby([\"symbol\"])[col].transform(lambda x: x.expanding().var())\n                            \ndf.head(20)","66f472ad":"df = df.dropna(subset=[target] + baseline_cols, axis=0)","4bcc528b":"filnename_df = \"df.pickle\"","bee693b4":"df.to_pickle(os.path.join(PATH_WRITE, filnename_df))","36e22ad2":"df = pd.read_pickle(os.path.join(PATH_WRITE, filnename_df))","d252e501":"df.shape","3cc8936d":"last_quarter = df[\"quarter_x\"].max()\ntrain = df[df[\"quarter_x\"] < last_quarter]\ntest = df[df[\"quarter_x\"] == last_quarter]\n\nlen(train), len(test)","118a0064":"cat_cols = [\n    \"symbol\",\n    \"quarter_x\",\n    \"Ticker\",\n    \"quarter_y\"\n]\n\nrejected_cols = [\n    \"date\",\n    \"open\",\n    \"close\",\n    \"low\",\n    \"high\",\n    \"volume\",\n    \"datetime\",\n    \"next_quarter\",\n] + cat_cols\n\ntrain.drop(columns=rejected_cols, inplace=True)\ntest.drop(columns=rejected_cols, inplace=True)","3961f06d":"def plot_together(values, names):\n    for pred, name in zip(values, names):\n        sns.distplot(pred, norm_hist=True, hist=False, label=name)\n    plt.legend()","4a999bdb":"values = [train[target].dropna(), test[target].dropna()]\nnames = [\"train\", \"test\"]\n\nplot_together(values, names)","ed31cf13":"train_x = train.drop(columns=target)\ntrain_y = train[target]\ntest_x = test.drop(columns=target)\ntest_y = test[target]","07653534":"# train_x_safe = (train_x\n#                 .replace([np.inf, -np.inf], np.nan)\n#                 .fillna(train_x.mean())\n#                )\n\n# test_x_safe = (test_x\n#                 .replace([np.inf, -np.inf], np.nan)\n#                 .fillna(train_x.mean())\n#                )","d04ef558":"train_x_mean = train_x.mean()\ntrain_x_std = train_x.std()\n\ntrain_x_norm = ((train_x - train_x_mean) \/ train_x_std).fillna(0)\ntest_x_norm = ((test_x - train_x_mean) \/ train_x_std).fillna(0)","2c0f06e6":"lgb_model = lgb.LGBMRegressor()\nlgb_model.fit(train_x_norm, train_y)","b00e0ce8":"do_incremental_time_cross_validation = True\n# n_folds = n_quarters - 1\nn_folds = 5\n\ndo_incremental_time_cross_validation_full_quarters = \"not yet implemented\"\n\nif do_incremental_time_cross_validation:\n    folds = TimeSeriesSplit(n_splits=n_folds)\nelse:\n    folds = None\n\n    \ndef fit_type_params(params):\n    int_params = [\"num_trees\", \"max_depth\", \"num_leaves\", \"max_bin\"]\n    fitted_params = {}\n    for (k, v) in params.items():\n        if k in int_params:\n            v = int(round(v))\n        fitted_params[k] = v\n    return fitted_params\n\n\ndef bayes_parameter_opt_lgb(train_x, train_y, init_rounds=10, optimization_rounds=10):\n    \n    random_seed = 42\n    train_set = lgb.Dataset(data=train_x, label=train_y)\n    metric = \"mse\"\n    params_static = {\n        \"objective\": \"regression\",\n        \"metric\":     metric\n    }\n    \n    params_ranges = {\n        \"num_trees\":        (10, 100),\n        \"learning_rate\":    (0.001, 0.3),\n        \"num_leaves\":       (2, 50),\n        \"feature_fraction\": (0.5, 1.0),\n        \"max_bin\":          (2, 2048),\n        'bagging_fraction': (0.7, 1.0),\n        'lambda_l2' :       (0.0, 100.0),\n    }\n    \n    def eval_cv_mse_negative(\n        num_trees,\n        learning_rate,\n        num_leaves,\n        feature_fraction,\n        max_bin,\n        bagging_fraction,\n        lambda_l2\n    ):\n        \n        params = {}\n        params[\"num_trees\"]        = num_trees\n        params['learning_rate']    = learning_rate\n        params['num_leaves']       = num_leaves\n        params['feature_fraction'] = feature_fraction\n        params['max_bin']          = max_bin\n        params['bagging_fraction'] = bagging_fraction\n        params['lambda_l2']        = lambda_l2\n        \n        params = fit_type_params({**params_static, ** params})\n    \n        cv_result = lgb.cv(params,\n                           train_set=train_set,\n                           folds=folds,\n                           seed=random_seed,\n                           metrics=[metric]\n                          )\n        mse_cv_neg = -np.array(cv_result[\"l2-mean\"]).mean()\n        return mse_cv_neg\n\n    optimizer = BayesianOptimization(f=eval_cv_mse_negative, random_state=0, pbounds=params_ranges)\n    optimizer.maximize(init_points=init_rounds, n_iter=optimization_rounds)\n    \n    result = pd.DataFrame.from_records(optimizer.res)\n    print(result)\n    best_result_row = result.loc[result[\"target\"].idxmax()]\n    best_result = {\n        \"params\" :  best_result_row[\"params\"],\n        metric :   -best_result_row[\"target\"]\n    }\n    return best_result\n\nbest_result = bayes_parameter_opt_lgb(train_x_norm[:1000], train_y[:1000], init_rounds=2, optimization_rounds=5)\nbest_result","66594655":"lgb_model_cv = lgb.LGBMRegressor(\n    **fit_type_params(best_result[\"params\"])\n)\nlgb_model_cv.fit(train_x_norm, train_y)","a915b089":"lgb.plot_importance(lgb_model_cv, height=0.8, max_num_features=20)","10a99a7f":"lin_model = LinearRegression()\nlin_model.fit(train_x_norm, train_y)","c3d77cce":"df_lin_coefs = pd.DataFrame(\n    list(zip(train_x_norm.columns, lin_model.coef_)),\n    columns=[\"feature\", \"linear_coef\"]\n).sort_values(\"linear_coef\", ascending=False)\n\nmost_positive_feats = df_lin_coefs.head()\nmost_negative_feats = df_lin_coefs.tail()\n\npd.concat([most_positive_feats, most_negative_feats]).set_index(\"feature\").plot.barh()","e94dfaae":"train_pred_lgm = lgb_model.predict(train_x_norm)\ntest_pred_lgm  = lgb_model.predict(test_x_norm)\nmse_train_lgm  = sk_mse(train_y, train_pred_lgm)\nmse_test_lgm   = sk_mse(test_y, test_pred_lgm)\n\ntrain_pred_lgm_cv = lgb_model_cv.predict(train_x_norm)\ntest_pred_lgm_cv  = lgb_model_cv.predict(test_x_norm)\nmse_train_lgm_cv  = sk_mse(train_y, train_pred_lgm_cv)\nmse_test_lgm_cv   = sk_mse(test_y, test_pred_lgm_cv)\n\ntrain_x_safe = (train_x_norm\n                .replace([np.inf, -np.inf], np.nan)\n                .fillna(train_x.mean())\n               )\ntest_x_safe = (test_x_norm\n                .replace([np.inf, -np.inf], np.nan)\n                .fillna(train_x.mean())\n               )\ntrain_pred_linear = lin_model.predict(train_x_safe)\ntest_pred_linear = lin_model.predict(test_x_safe)\n\nmse_train_linear = sk_mse(train_y, train_pred_linear)\nmse_test_linear = sk_mse(test_y, test_pred_linear)\n\n\nmse_summary = [\n    (\"lgm\", mse_train_lgm, \"train\"),\n    (\"lgm\", mse_test_lgm, \"test\"),\n    (\"linear\", mse_train_linear, \"train\"),\n    (\"linear\", mse_test_linear, \"test\"),\n    (\"lgm_cv\", mse_train_lgm_cv, \"train\") ,\n    (\"lgm_cv\", mse_test_lgm_cv, \"test\") ,\n]\n\n\nfor col in baseline_cols:\n    mse_train_baseline = sk_mse(train_y, train[col])\n    mse_summary.append((col, mse_train_baseline, \"train\"))\nfor col in baseline_cols:\n    mse_test_baseline = sk_mse(test_y, test[col])\n    mse_summary.append((col, mse_test_baseline, \"test\"))\n    \nmse_summary_df = pd.DataFrame(mse_summary, columns=[\"model\", \"mean_squared_error\", \"dataset\"])\n\nmse_summary_df = mse_summary_df.sort_values(\"mean_squared_error\")\n# .set_index(\"model\")\n# mse_summary_df.plot.barh()\nsns.barplot(x=\"mean_squared_error\", y=\"model\", hue=\"dataset\",\n            data=mse_summary_df,\n            order = mse_summary_df[mse_summary_df[\"dataset\"] == \"test\"].sort_values(\"mean_squared_error\")[\"model\"]\n)","72760ef4":"sns.barplot?","e42d549c":"mse_summary_df","2fef43d3":"df = pd.read_pickle(os.path.join(PATH_WRITE, filnename_df))","c12b320d":"df.head().T","19405faf":"# from keras.models import Sequential\n# from keras.layers import Dense, Embedding, LSTM","01ed1c51":"# train_x.head()","fd4fe7f0":"# train_y.head()","3c6384d5":"# model = Sequential()\n# model.add(LSTM(4, input_shape=(1000, n_look_back)))\n# model.add(Dense(1))\n# model.compile(loss='mean_squared_error', optimizer='adam')","d40ac2a3":"# model.fit(train_x_norm[:1000], train_y[:1000], epochs=100, batch_size=1, verbose=2)","ee9b4ac3":"from fastai.tabular import *\ndf = add_datepart(df, \"date\", drop=False)","91b751e8":"last_quarter = df[\"quarter_x\"].max()\ntrain = df[df[\"quarter_x\"] < last_quarter]\ntest = df[df[\"quarter_x\"] == last_quarter]\n\n# valid_quarter = train[\"quarter_x\"].max()\n# valid = train[train[\"quarter_x\"] == valid_quarter]\n# train = train[train[\"quarter_x\"] < valid_quarter]\n\nlen(train), len(test)","664a02d8":"feats_cat  = [\n    \"symbol\",\n    \"quarter_x\",\n    \"Year\",\n    \"Month\"\n]\n\nrejected_cols = [\n    \"Ticker\",\n    \"date\",\n    \"open\",\n    \"close\",\n    \"low\",\n    \"high\",\n    \"volume\",\n    \"datetime\",\n    \"next_quarter\",\n    \"quarter_y\"\n]\n\ntrain.drop(columns=rejected_cols, inplace=True)\n# valid.drop(columns=rejected_cols, inplace=True)\ntest.drop(columns=rejected_cols, inplace=True)","f6434ec0":"feats_num = train.columns\nfeats_num = list(set(feats_num).difference(set(feats_cat + [target])))","ba48f617":"train = train.reset_index()\ntest = test.reset_index()\nvalid_quarter = train[\"quarter_x\"].max()\nvalid_idx = train[train[\"quarter_x\"] == valid_quarter].index","0eded769":"procs = [FillMissing, Categorify, Normalize]\n# procs = [Categorify]\n# procs=None\ndata = TabularDataBunch.from_df(\n    os.path.join(PATH_WRITE, \"databunch\"),\n    df=train,\n    test_df=test,\n    dep_var=target,\n    valid_idx=valid_idx,\n    procs=procs,\n    cat_names=feats_cat,\n    cont_names=feats_num\n)","657f31fc":"data","880cf43a":"layers = [2, 3, 2]\nlearner = tabular_learner(data, layers=layers, metrics=[RMSE()])\nlearning_rate = 0.01\nn_epochs = 3\nlearner.fit_one_cycle(n_epochs, learning_rate)","a97c8271":"test_predicts, _ = learner.get_preds(ds_type=DatasetType.Test)\ntest_probs = to_np(test_predicts)\npd.Series(list(test_probs)).describe()\n# test_predicts","8b81a494":"train_predicts, _ = learner.get_preds(ds_type=DatasetType.Train)\nvalid_predicts, _ = learner.get_preds(ds_type=DatasetType.Valid)\nbound = 0.9\ntrain_probs = to_np(train_predicts).clip(-bound, bound)\nvalid_probs = to_np(valid_predicts).clip(-bound, bound)\n\nvalues = [train_probs, valid_probs, test_probs]\nnames = [\"train\", \"valid\", \"test\"]\n\nplot_together(values, names)","853a117c":"\nvalues = [test_probs, test[target]]\nnames = [\"test_probs\", \"test_truth\"]\n\nplot_together(values, names)","6a8d0890":"Few things are more attractive than money. Predicting the future is a big candidate, indeed. Let's try this time to get both at once by predicting stock market returns, not in the short or long run, but somewhere in the middle. I have always wanted to invest not blindly, not checking online brokers five times in the morning. The return for the next quarter looks best. Here we use [New York Stock Exchange stock prices](https:\/\/www.kaggle.com\/dgawlik\/nyse#prices.csv) data combined with last 10 years fundamentals for most of those companies.\n\nHere is a list of motivations:\n1. Find the stocks with the highest expected value increase\n2. Understand the main factors driving price dynamics\n3. Reveal the performance of prediction models from naive to sophisticated\n\n\n","50db3e48":"The fundamentals history is 4 years long. Thus, it is going to be almost useless for predicting the impact on prices. Let's search somewhere else.","a888ea9b":"During the evolution of this work, a silent mistake came to the surface in the shape of baseline_pred_mean beating every other model.\nThe same way, models had baseline_pred_mean among the top features, sometimes accounting for almost all of the weight and importances among features.\n\nAfter becaming ovbious, the root cause was the inocent use of mean without expanding aggregation. Thus, each datapoint contained illicit info about future mean of the target, instead of averaging target exclusively up to the present.\n\nAlways keep yourself safe from data leakage!","a5de4ca8":"## Checkpoint: Persist dataframe","e298991a":"It can be seen how baseline_pred_mean is remarkably the most conservative model, with estimated targets very tight around the global mean. At the other side, the actual target, close_rel_increase_prev_quarter, shows a much higher volatility with scattered estimates. Somehow, more information is needed to be incorporated into the model to safely expand the baseline_pred_mean.","c62e2015":"Some observations:\n* Baseline models are not expressive enough even to explain known data.\n* LightGBM with default parameters gets the most accurate picture of known data, but suffers from greatest overfit.\n* However, defult parameters of linear regression beats previous due to better generalization, as expected for a simpler model.\n* Beating linear regression is possible for lightGBM only with incremental time cross validation and bayesian grid search.\n\nIt can be concluded too that clean data approach and cautions feature engineering (not allow future data to leak into the past!) but not affraid of beeing exhaustive (yes, let's make the cloud work it out with several agggregations for every feature) can pave a golden way for understimated models, just like the right weapon and a taste of boldness gave the edge to David over Goliath.","cd40ab36":"Model tunning:\n\nBayesian optimization is pretty nice for minimizing heavy functions, like model performance vs. hyperparameters. A reasonable analogy is stochastig gradien descent, but with bayesian inference instead of differentiation as a means to efficiently explore the domain.\nAs well, dealing with naturally order data, such as time series, cross-validating to random folds is less natural than validating against a fold with data more recent that any in the training folds.","1947726d":"The data is originaly formatted as sparse records of (stock, time,  metric, value). It is transfomed into a 2-d matrix of (stock, time) x metric using pandas pivot function.","c884e817":"Where are NSYE stocks from?","e60fb767":"#### If you like, you can skip EDA and data preprocessing to get the persisted dataframe ready to model, by going to the checkpoint.","f6c7d203":"The target is defined as:","7f5cc8f8":"Coming soon: DeeP Learning Enriched Time Series!","803e3e03":"Feature insights reveal that trading world is wininig on its self-fulfilling prophecy: \"search for momentum and will get returns\". Said otherwise, prev price dinamycs is better predictor than fundamentals. At the end, price is a very valuable info about actual value and gathers consensus about many well informed players.\nIt can get more audacious and try to state\n* high valuation (open and close mean) -> low target\n* high volatility (intraperiod max and min) -> high target\n\nHowever, there is valuable info too about driving factors within the fundamental indicators to dive into or to select when researching a company."}}