{"cell_type":{"9010a36b":"code","4e45d335":"code","df01795a":"code","9082751b":"code","e1e69e4b":"code","48191fbe":"code","b52f44df":"code","de3bd498":"code","8cda8b66":"code","8411e95c":"code","e615555e":"code","a4455e7f":"code","e1ba58f5":"code","4ef50374":"code","9ca709df":"code","d2bf5874":"code","90725f2b":"code","6859aa99":"code","d8a9d41e":"code","7083481e":"code","8dd51fe4":"code","111c5567":"code","ad33ab21":"code","61c48326":"code","073c365d":"code","2682bf1e":"markdown","60d0d178":"markdown","2d076aaf":"markdown","8b50444e":"markdown"},"source":{"9010a36b":"import pandas as pd, numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom scipy.sparse import hstack\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\nfrom nltk.corpus import stopwords\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\nimport warnings\nwarnings.filterwarnings('ignore')","4e45d335":"train2 = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-unintended-bias-train.csv\")\ntrain2.toxic = train2.toxic.round().astype(int)\nvalid1 = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/validation.csv')\ntest = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/test.csv')\nsubm = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/sample_submission.csv')","df01795a":"train2=train2[:500000] # as this a too large dataset","9082751b":"train = pd.concat([train2[['comment_text', 'toxic']].query('toxic==1'),\n    train2[['comment_text', 'toxic']].query('toxic==0'),valid1[['comment_text', 'toxic']]]).reset_index(drop=True)","e1e69e4b":"train.head(5)","48191fbe":"test.head(5)","b52f44df":"train['comment_text'][0]","de3bd498":"test['content'][0]","8cda8b66":"lens = train.comment_text.str.len()\nlens.mean(), lens.std(), lens.max()","8411e95c":"lens.hist();","e615555e":"lens = test.content.str.len()\nlens.mean(), lens.std(), lens.max()","a4455e7f":"lens.hist()","e1ba58f5":"len(train),len(test)","4ef50374":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,6))\ncomment_len=train[train['toxic']==1]['comment_text'].str.len()\nax1.hist(comment_len,color='cyan',linewidth=2,edgecolor='k')\nax1.set_title('toxic comments')\ncomment_len=train[train['toxic']==0]['comment_text'].str.len()\nax2.hist(comment_len,color='blue',linewidth=2,edgecolor='k')\nax2.set_title('Not toxic comments')\nfig.suptitle('Characters in Comments',fontsize=20)\n\nplt.show()","9ca709df":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,6))\ncomment_words=train[train['toxic']==1]['comment_text'].str.split().map(lambda x: len(x))\nax1.hist(comment_words,color='magenta',linewidth=2,edgecolor='k')\nax1.set_title('toxic comments')\ncomment_words=train[train['toxic']==0]['comment_text'].str.split().map(lambda x: len(x))\nax2.hist(comment_words,color='red',linewidth=2,edgecolor='k')\nax2.set_title('Non toxic comments')\nfig.suptitle('Words in comments',fontsize=20)\nplt.show()","d2bf5874":"label_cols = ['toxic']\n\ntrain.describe()","90725f2b":"train['comment_text'].fillna(\"unknown\", inplace=True)\ntest['content'].fillna(\"unknown\", inplace=True)","6859aa99":"def lower(words):\n    return words.lower()\ntrain['comment_text']=train['comment_text'].apply(lambda x:lower(x))\ndef remove_numbers(words):\n    return re.sub(r'\\d+','',words)\ntrain['comment_text']=train['comment_text'].apply(lambda x: remove_numbers(x))\ndef remove_punctuation(words):\n    table=str.maketrans('','',string.punctuation)\n    return words.translate(table)\ntrain['comment_text']=train['comment_text'].apply(lambda x: remove_punctuation(x))\ntrain['comment_text']=train['comment_text'].apply(lambda x:word_tokenize(x))\ndef remove_stopwords(words):\n    stop_words=set(stopwords.words('english'))\n    return [word for word in words if word not in stop_words]\ntrain['comment_text']=train['comment_text'].apply(lambda x: remove_stopwords(x))\ndef remove_links(words):\n    \n    return [re.sub(r'(https?:\/\/\\S+)','',word)for word in words]\ntrain['comment_text']=train['comment_text'].apply(lambda x:remove_links(x))\ndef lemmatizing(words):\n    lemmatizer =WordNetLemmatizer()\n    return [lemmatizer.lemmatize(word) for word in words]\ntrain['comment_text']=train['comment_text'].apply(lambda x: lemmatizing(x))\ndef final_text(words):\n     return ' '.join(words)\ntrain['comment_text']=train['comment_text'].apply(lambda x:final_text(x))\n   \n                 ","d8a9d41e":"n = train.shape[0]\nvec = TfidfVectorizer(ngram_range=(1,3),\n               strip_accents='unicode', use_idf=1,\n               smooth_idf=1, sublinear_tf=1 )\ntrn_term_doc = vec.fit_transform(train['comment_text'])\ntest_term_doc = vec.transform(test['content'])\n","7083481e":"def pr(y_i, y):\n    p = x[y==y_i].sum(0)\n    return (p+1) \/ ((y==y_i).sum()+1)","8dd51fe4":"x = trn_term_doc\ntest_x = test_term_doc","111c5567":"def get_model(y):\n    y = y.values\n    r = np.log(pr(1,y) \/ pr(0,y))\n    m = LogisticRegression(C=4, dual=False)\n    x_nb = x.multiply(r)\n    return m.fit(x_nb, y), r","ad33ab21":"preds = np.zeros((len(test), len(label_cols)))\n\nfor i, j in enumerate(label_cols):\n    print('fit', j)\n    m,r = get_model(train[j])\n    preds[:,i] = m.predict_proba(test_x.multiply(r))[:,1]","61c48326":"submid = pd.DataFrame({'id': subm[\"id\"]})\nsubmission = pd.concat([submid, pd.DataFrame(preds, columns = label_cols)], axis=1)\n\nsubmission.to_csv('submission.csv', index=False)","073c365d":"submission.head(n=20)","2682bf1e":"# Text Preprocessing","60d0d178":"# Training ML model","2d076aaf":"# Loading dataset","8b50444e":"# Importing Libraries"}}