{"cell_type":{"cb81b998":"code","b8c09750":"code","edc989a5":"code","a0c2e86c":"code","c3d97004":"code","2049240c":"code","4355723d":"code","ad2a6218":"code","4f4180f7":"code","5bed0c51":"code","d141fc9e":"code","ea3a5dc0":"code","67a7aa5d":"code","7ef0e9a9":"code","8e575fe5":"code","663290e9":"code","b9c04c2c":"code","479eca16":"code","9317ce7d":"code","db0bce72":"code","be2d541e":"code","714e6821":"code","d2296875":"code","0432fa5a":"code","8ff5f2cd":"code","6050980f":"code","2f04bfc8":"code","bfee0f3b":"code","943a16ba":"code","383f8d40":"code","32e8a620":"code","2ae88f5e":"code","259c42d2":"code","44c1ea08":"code","f332282d":"code","779b6bcc":"markdown","75e1dcc1":"markdown","32b19941":"markdown","70cda26f":"markdown","2f3f7a22":"markdown"},"source":{"cb81b998":"!pip install ..\/input\/pytorch-ignite-020\/pytorch_ignite-0.2.0-py2.py3-none-any.whl","b8c09750":"import pprint\nimport random\nimport os\nimport pathlib\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\nimport torchtext\nfrom torchtext.vocab import Vectors\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data.dataset import Subset\n\nfrom ignite.engine import Engine, Events\nfrom ignite.metrics import Accuracy, Loss, RunningAverage, Precision, Recall\nfrom ignite.handlers import ModelCheckpoint, EarlyStopping\nfrom ignite.contrib.handlers import ProgressBar","edc989a5":"torch.cuda.is_available()","a0c2e86c":"device = 'cuda'\nseed = 632\n\n# .pt is omitted to emulate cache pytorch files for torchtext Vectors\ntorch_glove_path = '..\/input\/glove-840b300d-for-torchtext\/glove.840B.300d.txt'\ntorch_fasttext_path = '..\/input\/fastextcrawl300d2m\/crawl-300d-2M.vec'\ninput_path = '..\/input\/jigsaw-unintended-bias-in-toxicity-classification'","c3d97004":"def seed_torch(seed=632):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True","2049240c":"def preprocess(data):\n    '''\n    Credit goes to https:\/\/www.kaggle.com\/gpreda\/jigsaw-fast-compact-solution\n    '''\n    punct = \"\/-'?!.,#$%()*+-\/:;<=>@[\\\\]^_`{|}~`\" + \\\n        '\"\"\u201c\u201d\u2019' + '\u221e\u03b8\u00f7\u03b1\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014\u2013&'\n\n    def clean_special_chars(text, punct):\n        for p in punct:\n            text = text.replace(p, ' ')\n        return text\n\n    return clean_special_chars(data, punct)\n\ndef tokenize(data):\n    data = preprocess(data)\n    return data.split()\n","4355723d":"TEXT = torchtext.data.Field(lower=True, include_lengths=False, \n#                             fix_length=MAX_LEN,\n                            batch_first=True, tokenize=tokenize)\nLABEL = torchtext.data.Field(use_vocab=False, sequential=False,\n                             batch_first=True, is_target=True, \n                             preprocessing=lambda x: int(float(x) >= 0.5))\nLABEL_IDENTITY = torchtext.data.Field(use_vocab=False, sequential=False,\n                             batch_first=True, \n                             preprocessing=lambda x: 0 if x == '' else int(float(x) >= 0.5))\n\ntrain_fields = {\n    'comment_text': ('text', TEXT),\n    'target': ('label', LABEL),\n    'male': ('male', LABEL_IDENTITY),\n    'female': ('female', LABEL_IDENTITY),\n    'homosexual_gay_or_lesbian': ('homosexual_gay_or_lesbian', LABEL_IDENTITY),\n    'christian': ('christian', LABEL_IDENTITY),\n    'jewish': ('jewish', LABEL_IDENTITY),\n    'muslim': ('muslim', LABEL_IDENTITY),\n    'black': ('black', LABEL_IDENTITY),\n    'white': ('white', LABEL_IDENTITY),\n    'psychiatric_or_mental_illness': ('psychiatric_or_mental_illness', LABEL_IDENTITY),\n}\n\ntrain_dataset = torchtext.data.TabularDataset(path=f'{input_path}\/train.csv',\n                                                   format='csv',\n                                                   fields=train_fields)\n  ","ad2a6218":"train_dataset[0].text[:15]","4f4180f7":"TEXT.build_vocab(train_dataset, min_freq=1)\nvocab = TEXT.vocab","5bed0c51":"len(TEXT.vocab)\n","d141fc9e":"vocab.load_vectors([\n    Vectors(torch_glove_path, cache='.'),\n    Vectors(torch_fasttext_path, cache='.')\n])","ea3a5dc0":"print('Attributes of TEXT : ', [attr for attr in dir(TEXT) if '_' not in attr])\nprint('Attributes of TEXT.vocab : ', [\n      attr for attr in dir(TEXT.vocab) if '_' not in attr])\nprint('First 5 values TEXT.vocab.itos : ', TEXT.vocab.itos[0:5])\nprint('First 5 key, value pairs of TEXT.vocab.stoi : ', {\n      key: value for key, value in list(TEXT.vocab.stoi.items())[0:5]})","67a7aa5d":"class SpatialDropout(nn.Dropout2d):\n    def forward(self, x):\n        x = x.unsqueeze(2)    # (N, T, 1, K)\n        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n        x = x.squeeze(2)  # (N, T, K)\n        return x\n\nclass TextModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, kernel_sizes, num_filters,\n                 num_classes, d_prob, mode, hidden_dim, lstm_units,\n                 emb_vectors=None, spatial_drop=0.1):\n        super(TextModel, self).__init__()\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.kernel_sizes = kernel_sizes\n        self.num_filters = num_filters\n        self.num_classes = num_classes\n        self.d_prob = d_prob\n        self.mode = mode\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=1)\n        self.embedding_dropout = SpatialDropout(spatial_drop)\n\n        if emb_vectors is not None:\n            self.load_embeddings(emb_vectors)\n\n        self.conv = nn.ModuleList([nn.Conv1d(in_channels=embedding_dim,\n                                             out_channels=num_filters,\n                                             kernel_size=k, stride=1) for k in kernel_sizes])\n        self.lstm1 = nn.LSTM(embedding_dim, lstm_units,\n                             bidirectional=True, batch_first=True)\n        self.lstm2 = nn.LSTM(lstm_units * 2, lstm_units,\n                             bidirectional=True, batch_first=True)\n        self.lstm_body = nn.LSTM(\n            embedding_dim, lstm_units, bidirectional=True, batch_first=True)\n        self.dropout = nn.Dropout(d_prob)\n        self.fc = nn.Linear(len(kernel_sizes) * num_filters, hidden_dim)\n        self.fc_total = nn.Linear(hidden_dim * 1 + lstm_units * 4, hidden_dim)\n        self.fc_final = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, x):\n        x_emb = self.embedding(x)\n        x_emb = self.embedding_dropout(x_emb)\n        \n        # pad for CNN kernel 5\n        if x_emb.shape[1] < 5:\n            x_emb = F.pad(x_emb, (0, 0, 0, 5 - x_emb.shape[1]), value=0)\n            \n        x = [F.relu(conv(x_emb.transpose(1, 2))) for conv in self.conv]\n        x = [F.max_pool1d(c, c.size(-1)).squeeze(dim=-1) for c in x]\n        x = torch.cat(x, dim=1)\n        x = self.fc(self.dropout(x))\n\n        h_lstm1, _ = self.lstm1(x_emb)\n        h_lstm2, _ = self.lstm2(h_lstm1)\n\n        # average pooling\n        avg_pool2 = torch.mean(h_lstm2, 1)\n        # global max pooling\n        max_pool2, _ = torch.max(h_lstm2, 1)\n\n\n        out = torch.cat([x, avg_pool2, max_pool2], dim=1)\n        out = F.relu(self.fc_total(self.dropout(out)))\n        out = self.fc_final(out)\n\n        return out\n\n    def load_embeddings(self, emb_vectors):\n        if 'static' in self.mode:\n            self.embedding.weight.data.copy_(emb_vectors)\n            if 'non' not in self.mode:\n                self.embedding.weight.data.requires_grad = False\n                print('Loaded pretrained embeddings, weights are not trainable.')\n            else:\n                self.embedding.weight.data.requires_grad = True\n                print('Loaded pretrained embeddings, weights are trainable.')\n        elif self.mode == 'rand':\n            print('Randomly initialized embeddings are used.')\n        else:\n            raise ValueError(\n                'Unexpected value of mode. Please choose from static, nonstatic, rand.')\n","7ef0e9a9":"class JigsawEvaluator:\n    \"\"\"Credits to https:\/\/www.kaggle.com\/c\/jigsaw-unintended-bias-in-toxicity-classification\/discussion\/90527\"\"\"\n    def __init__(self, y_true, y_identity, power=-5, overall_model_weight=0.25):\n        self.y = y_true\n        self.y_i = y_identity\n        self.n_subgroups = self.y_i.shape[1]\n        self.power = power\n        self.overall_model_weight = overall_model_weight\n\n    @staticmethod\n    def _compute_auc(y_true, y_pred):\n        try:\n            return roc_auc_score(y_true, y_pred)\n        except ValueError:\n            return np.nan\n\n    def _compute_subgroup_auc(self, i, y_pred):\n        mask = self.y_i[:, i] == 1\n        return self._compute_auc(self.y[mask], y_pred[mask])\n\n    def _compute_bpsn_auc(self, i, y_pred):\n        mask = self.y_i[:, i] + self.y == 1\n        return self._compute_auc(self.y[mask], y_pred[mask])\n\n    def _compute_bnsp_auc(self, i, y_pred):\n        mask = self.y_i[:, i] + self.y != 1\n        return self._compute_auc(self.y[mask], y_pred[mask])\n\n    def compute_bias_metrics_for_model(self, y_pred):\n        records = np.zeros((3, self.n_subgroups))\n        for i in range(self.n_subgroups):\n            records[0, i] = self._compute_subgroup_auc(i, y_pred)\n            records[1, i] = self._compute_bpsn_auc(i, y_pred)\n            records[2, i] = self._compute_bnsp_auc(i, y_pred)\n        return records\n\n    def _calculate_overall_auc(self, y_pred):\n        return roc_auc_score(self.y, y_pred)\n\n    def _power_mean(self, array):\n        total = sum(np.power(array, self.power))\n        return np.power(total \/ len(array), 1 \/ self.power)\n\n    def get_final_metric(self, y_pred):\n        bias_metrics = self.compute_bias_metrics_for_model(y_pred)\n        bias_score = np.average([\n            self._power_mean(bias_metrics[0]),\n            self._power_mean(bias_metrics[1]),\n            self._power_mean(bias_metrics[2])\n        ])\n        overall_score = self.overall_model_weight * self._calculate_overall_auc(y_pred)\n        bias_score = (1 - self.overall_model_weight) * bias_score\n        return overall_score + bias_score","8e575fe5":"identity_columns = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']","663290e9":"criterion = nn.CrossEntropyLoss()","b9c04c2c":"def process_function(engine, batch):\n    model.train()\n    optimizer.zero_grad()\n    x, y = batch.text, batch.label\n    y_pred = model(x)\n    loss = criterion(y_pred, y)\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n\n","479eca16":"def eval_function(engine, batch):\n    model.eval()\n    with torch.no_grad():\n        x, y = batch.text, batch.label\n        y_pred = model(x)\n        return y_pred, y\n\n","9317ce7d":"trainer = Engine(process_function)\ntrain_evaluator = Engine(eval_function)\nvalidation_evaluator = Engine(eval_function)","db0bce72":"RunningAverage(output_transform=lambda x: x).attach(trainer, 'loss')","be2d541e":"def thresholded_output_transform(output):\n    y_pred, y = output\n    y_pred = torch.round(y_pred)\n    return y_pred, y","714e6821":"Accuracy(output_transform=thresholded_output_transform).attach(\n    train_evaluator, 'accuracy')\nLoss(criterion).attach(train_evaluator, 'ce')\n\n\nAccuracy(output_transform=thresholded_output_transform).attach(\n    validation_evaluator, 'accuracy')\nLoss(criterion).attach(validation_evaluator, 'ce')","d2296875":"pbar = ProgressBar(persist=True, bar_format=\"\")\npbar.attach(trainer, ['loss'])","0432fa5a":"def log_training_results(engine):\n    train_evaluator.run(train_loader)\n    metrics = train_evaluator.state.metrics\n    pbar.log_message(\n        \"Training Results - Epoch: {} \\nMetrics\\n{}\"\n        .format(engine.state.epoch, pprint.pformat(metrics)))\n\ndef log_validation_results(engine):\n    validation_evaluator.run(val_loader)\n    metrics = validation_evaluator.state.metrics\n    pbar.log_message(\n        \"Validation Results - Epoch: {} \\nMetrics\\n{}\"\n        .format(engine.state.epoch, pprint.pformat(metrics)))\n    pbar.n = pbar.last_print_n = 0\n\n\n\n# trainer.add_event_handler(Events.EPOCH_COMPLETED, log_training_results)\n# if USE_VALIDATION:\ntrainer.add_event_handler(Events.EPOCH_COMPLETED, log_validation_results)","8ff5f2cd":"\n\ndef score_function(engine):\n    val_loss = engine.state.metrics['ce']\n    return -val_loss\n\nhandler = EarlyStopping(patience=1, score_function=score_function, trainer=trainer)\n# if USE_VALIDATION:\nvalidation_evaluator.add_event_handler(Events.COMPLETED, handler)\n","6050980f":"best_model_save = ModelCheckpoint(\n    'best_model', 'textcnn', n_saved=1,\n    create_dir=True, save_as_state_dict=True,\n    score_function=score_function)\n\n","2f04bfc8":"def get_predictions(model, loader):\n    model.eval()\n\n    with torch.no_grad():\n        predictions = []\n        for batch in loader:\n            x = batch.text\n            logits = model(x)\n            y_pred = F.softmax(logits, dim=1)[:, 1]\n            # move from GPU to CPU and convert to numpy array\n            y_pred_numpy = y_pred.cpu().numpy()\n\n            predictions.append(y_pred_numpy)\n        predictions = np.concatenate(predictions)\n    return predictions","bfee0f3b":"class TorchtextSubset(Subset):\n    def __init__(self, dataset, indices):\n        super(TorchtextSubset, self).__init__(dataset, indices)\n        self.fields = self.dataset.fields\n        self.sort_key = self.dataset.sort_key","943a16ba":"kernel_sizes = [3, 4, 5]\nnum_filters = 64\nnum_classes = 2\nd_prob = 0.5\nmode = 'nonstatic'\nhidden_dim = 256\nlstm_units = 128\nspatial_drop = 0.1\n\nbatch_size = 1024\nn_folds = 10\nnum_folds_use = 1\nnum_epoch = 10","383f8d40":"skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\nlabels = [example.label for example in train_dataset]","32e8a620":"batch_test_size = 1024\n\ntest_dataset = torchtext.data.TabularDataset(f'{input_path}\/test.csv',\n                                            format='csv',\n                                            fields={'comment_text': ('text', TEXT)})\n\ntest_loader = torchtext.data.Iterator(test_dataset, batch_size=batch_test_size,\n                                      device='cuda', shuffle=False, sort=False)\n\n# test_loader = torchtext.data.BucketIterator(\n#     test_dataset, batch_size=batch_test_size, device=device,\n#     sort_key=lambda x: len(x.text),\n#     sort_within_batch=True, repeat=False, shuffle=False)","2ae88f5e":"idx_splits = list(skf.split(range(len(train_dataset)), y=labels))[:num_folds_use]\n\nfor train_idx, val_idx in idx_splits:\n    seed_torch(seed)\n    train_ds = TorchtextSubset(train_dataset, train_idx)\n    val_ds = TorchtextSubset(train_dataset, val_idx)\n\n    train_loader, val_loader = torchtext.data.BucketIterator.splits(\n        [train_ds, val_ds], batch_sizes=[batch_size, batch_size], device=device,\n        sort_key=lambda x: len(x.text),\n        sort_within_batch=True, repeat=False)\n\n#     train_loader, val_loader = torchtext.data.Iterator.splits(\n#         [train_ds, val_ds], batch_sizes=[batch_size, batch_size],\n#         device=device, sort=False)\n    \n    vocab_size, embedding_dim = vocab.vectors.shape\n    model = TextModel(vocab_size=vocab_size,\n                      embedding_dim=embedding_dim,\n                      kernel_sizes=kernel_sizes,\n                      num_filters=num_filters,\n                      num_classes=num_classes,\n                      d_prob=d_prob,\n                      mode=mode,\n                      hidden_dim=hidden_dim,\n                      lstm_units=lstm_units,\n                      spatial_drop=spatial_drop,\n                      emb_vectors=vocab.vectors)\n    model.to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n    \n    validation_evaluator.add_event_handler(Events.EPOCH_COMPLETED, best_model_save, {'text_model': model})\n    trainer.run(train_loader, max_epochs=num_epoch)\n    \n    # load best model \n    model_path = next(pathlib.Path('best_model').rglob('*'))\n    model_state_dict = torch.load(model_path)\n    model.load_state_dict(model_state_dict)\n    \n    predictions = get_predictions(model, test_loader)","259c42d2":"df_sub = pd.read_csv(f'{input_path}\/sample_submission.csv')\ndf_sub['prediction'] = predictions","44c1ea08":"df_sub.head()","f332282d":"df_sub.to_csv('submission.csv', index=False)","779b6bcc":"## Ignite processing and metrics","75e1dcc1":"## Model","32b19941":"## Evaluator (not yet implemented)","70cda26f":"## Dataset and embeddings","2f3f7a22":"## Training with Stratified folds"}}