{"cell_type":{"876d8d4a":"code","427a978c":"code","18b79f70":"code","4b2a9f55":"code","58655b7e":"code","c826897d":"code","773c27b6":"code","858a465c":"code","d9c05888":"code","5336a420":"code","c4bf90eb":"code","2b22958b":"code","f95f8e1c":"code","fa311c3f":"code","2fc8f86a":"code","b94dac0f":"code","94b790d4":"code","3a29523b":"code","f22cd162":"code","a8681889":"code","73c20105":"code","12217bb5":"code","7cccde05":"code","a6b47b17":"code","7e92b59e":"markdown","29cfc5d9":"markdown","2ca0683d":"markdown","ff3a14e8":"markdown","43d19aed":"markdown","37aa143f":"markdown","c4216f34":"markdown","560b06a2":"markdown","c8e2be10":"markdown","aa255a2b":"markdown","465b7734":"markdown","6b30a08b":"markdown"},"source":{"876d8d4a":"\nimport pandas as pd\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sb\n\nimport tensorflow as tf\n\nimport sys\n\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n%matplotlib inline\n\nSEED = 1111\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\n","427a978c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","18b79f70":"TRAIN_FILE = '\/kaggle\/input\/jane-street-market-prediction\/train.csv'\nSAMPLE_TRAINNING_FILE = '.\/train-sample.csv'\n\n\n# Get all trainning data\ndef get_trainning_data():\n    print(\"Reading training data...\")\n    df = pd.read_csv(TRAIN_FILE)\n    return df\n\n# Create sample of trainning data\ndef create_trainning_sample(frac = 0.2):\n    df_train = get_trainning_data();\n    print(\"Creating sample file...\")\n    df_sample = df_train.sample(frac=frac)\n    df_sample.to_csv(SAMPLE_TRAINNING_FILE, index=False, header=True)\n    \ndef get_sample_trainning_data():\n    print(\"Reading sample training data...\")\n    # Read training data\n    return pd.read_csv(SAMPLE_TRAINNING_FILE)    ","4b2a9f55":"#create_trainning_sample(frac = 0.02);","58655b7e":"# Read only sample data created in previous step above\n#df_train = get_sample_trainning_data()\n\n# Read all the data\ndf_train = get_trainning_data()\n","c826897d":"print(\"Generate action column...\")\ndf_train['action'] = 0\ndf_train.loc[(df_train['resp'] > 0), 'action'] = 1\n","773c27b6":"df_train.info()","858a465c":"# Check for balance\nsb.distplot(df_train['action']);","d9c05888":"df_train.sort_values(by=['date', 'ts_id'], inplace=True)","5336a420":"df_train = df_train[df_train['weight'] != 0]","c4bf90eb":"columns_to_delete = ['date', 'weight', 'resp_1', 'resp_2', 'resp_3', 'resp_4', 'resp', 'ts_id']\nfor column in columns_to_delete:\n    df_train.drop(column, axis=1, inplace=True)\n    \nfeature_count = len(df_train.columns) - 1","2b22958b":"df_train.fillna(df_train.mean(),inplace=True)\n","f95f8e1c":"df_train = df_train.reset_index(drop=True)\ndf_train","fa311c3f":"NORMALIZE_NONE = 0\nNORMALIZE_MIN_MAX = 1\nNORMALIZE_MEAN = 2\n\nNORMALIZE_TYPE = NORMALIZE_NONE\n","2fc8f86a":"# 20 \/ 80 split\ndf_validation, df_train = np.split(df_train, [int(.2*len(df_train))])\n\ndf_train = df_train.reset_index(drop=True)\ndf_validation = df_validation.reset_index(drop=True)\n\ndf_train_labels = df_train['action'].copy()\ndf_train.drop('action', inplace=True, axis=1)\n\ndf_validation_labels = df_validation['action'].copy()\ndf_validation.drop('action', inplace=True, axis=1)\n\nprint(\"Shapes: (train={}, train_labels={}, validation={}, validation_labels={})\".format(df_train.shape, df_train_labels.shape, df_validation.shape, df_validation_labels.shape))\n","b94dac0f":"def normalize_data(df):\n    if NORMALIZE_TYPE == NORMALIZE_MIN_MAX:\n        return (df-df.min())\/(df.max()-df.min())\n    elif NORMALIZE_TYPE == NORMALIZE_MEAN:\n        return (df-df.mean())\/df.std()\n    else:\n        return df;\n\ndf_train = normalize_data(df_train)\ndf_validation = normalize_data(df_validation)","94b790d4":"\ninput_length = 7\nbatch_size = 70\n\ngenerator_train = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n    df_train, \n    df_train_labels,\n    length=input_length,\n    batch_size=batch_size)\n\ngenerator_validation = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n    df_validation, \n    df_validation_labels,\n    length=input_length,\n    batch_size=batch_size)","3a29523b":"# some values that may or may not get used as I experiment\nEPOCHS = 100\nSTEPS = len(df_train) \/ batch_size\nLEARNING_RATE = 0.0005\n","f22cd162":"\nmodel = tf.keras.models.Sequential()\n\nmodel.add(tf.keras.layers.LSTM(\n    feature_count, \n    activation='relu', \n    input_shape=(input_length, feature_count), \n    return_sequences=True))\n\nmodel.add(tf.keras.layers.Dropout(0.02))\n\nmodel.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(), \n                optimizer=tf.optimizers.Adam(learning_rate=LEARNING_RATE),\n                metrics=[\"accuracy\"])\nmodel.summary()","a8681889":"\nmodel.fit(generator_train,\n          validation_data = generator_validation,\n          epochs = EPOCHS,\n          steps_per_epoch = STEPS,\n          validation_steps=STEPS\/10,\n          verbose = 1,\n          callbacks = [\n                EarlyStopping(monitor='loss', verbose=1, patience=10),\n                EarlyStopping(monitor='val_loss', verbose=1, patience=10)\n          ]\n    )  \n","73c20105":"train_history = model.history.history\n\nplt.figure(1)\nplt.subplot(211)\nplt.ylim(top=5)\nplt.plot(train_history['loss'])\nplt.ylabel('Average Loss Per Epoch')\nplt.xlabel('Epoch')\nplt.title('Average Loss Per Epoch vs Epoch')\n\nplt.subplot(212)\nplt.ylim(top=15)\nplt.plot(train_history['val_loss'])\nplt.ylabel('Val Loss per Epoch')\nplt.xlabel('epoch')\nplt.title('Val Loss per Epoch vs Epoch')\nplt.show()\n","12217bb5":"# During prediction phase, what is the threshold to set action = 1\nPREDICTION_THRESHOLD = 0.5\nPREDICTION_IGNORE_WEIGHT = False","7cccde05":"import janestreet\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set","a6b47b17":"print(\"Starting predictions...\")\ndf_submission = pd.DataFrame(columns = ['ts_id', 'action'])\n\nzero_weight_count = 0\nprediction_count = 0\nprediction_within_threshold_count = 0\nprediction_value = 0;\n\n# TODO:\n#   1) Figure out how to add a rolling cache of previous test_df. per Gena (https:\/\/www.kaggle.com\/gdonchyts)\n#\n\nfor (test_df, sample_prediction_df) in iter_test:\n    \n    # drop columns\n    weight = test_df['weight'].item()\n    columns_to_delete = ['date', 'weight']\n    for column in columns_to_delete:\n        test_df.drop(column, axis=1, inplace=True)\n        \n    ts_id = sample_prediction_df.index.values[0]\n    action = 0\n\n    if weight != 0 and PREDICTION_IGNORE_WEIGHT == False:  \n        # handle missing values\n        test_df.fillna(test_df.mean(),inplace=True)\n        # normalize\n        test_df = normalize_data(test_df)\n        # predict\n        prediction = model.predict(np.array(test_df.values).reshape(1,1,130));\n        prediction_value = prediction[0][0][0]\n        if prediction_value > PREDICTION_THRESHOLD:\n            action = 1\n            prediction_within_threshold_count = prediction_within_threshold_count + 1\n        prediction_count = prediction_count + 1\n    else:\n        zero_weight_count = zero_weight_count + 1\n        \n    sample_prediction_df.action = action\n    print(\"\\rPrediciton: (index={}, action={}, prediction={}) Metrics: (submission_size={}, prediction_count={}, prediction_within_threshold_count={}, zero_weight_count={})\".format(\n        # Prediction\n        ts_id, \n        action, \n        prediction_value,\n        # Notes\n        len(df_submission),\n        prediction_count, \n        prediction_within_threshold_count,\n        zero_weight_count\n    ), end=\"\");\n    \n    df_submission = df_submission.append( {'ts_id': ts_id, 'action': action}, ignore_index=True )\n    env.predict(sample_prediction_df)   \n\nprint(\"\\nFinished predictions\")\ndf_submission.to_csv(\".\/submission.csv\", index=False, header=True)\nprint(\"Submission file created\")\nprint(\"Metrics: (submission_size={}, prediction_count={}, prediction_within_threshold_count={}, zero_weight_count={})\".format(\n    len(df_submission),\n    prediction_count, \n    prediction_within_threshold_count,\n    zero_weight_count\n))\n","7e92b59e":"# Start Exploration","29cfc5d9":"## Prediction","2ca0683d":"### Create Trainning Data Sample\n\nRun this block to create or recreate the sample trainning data file.","ff3a14e8":"# Reset Index","43d19aed":"## Handle Missing Data","37aa143f":"## Functions To Get Trainning Data\n\nDue to the size of trainning data, supporting functions are available to create and read a sample of trainning data.","c4216f34":"# Jane Street Market Prediction\n\nReference:  https:\/\/www.kaggle.com\/c\/jane-street-market-prediction","560b06a2":"## Drop unecessary columns","c8e2be10":"## Sort By Time","aa255a2b":"## Delete rows","465b7734":"## Normalize Data","6b30a08b":"# Train"}}