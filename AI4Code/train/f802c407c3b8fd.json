{"cell_type":{"bbc5c59d":"code","989f2cd8":"code","4ffb1f29":"code","9db6391a":"code","250385c2":"code","f50b79ce":"code","34d621b8":"code","64903201":"code","2edd9b5e":"markdown","f4d17a56":"markdown","c9590464":"markdown","535e9782":"markdown","f1ccc89f":"markdown","4e986da5":"markdown","e5c1d65d":"markdown","208c16c4":"markdown"},"source":{"bbc5c59d":"from scipy.stats import beta\nfrom scipy import integrate\n\ndef pdf(p, weights, rewards, normalization=1):\n    s = 1\n    for weight, reward in zip(weights, rewards):\n        if reward == 1:\n            s *= (weight * p)\n        else:\n            s *= (1 - weight * p)\n    return s \/ normalization\n\ndef get_expected_mean_std(weights, rewards):\n    normalization = integrate.quad(\n        lambda x: pdf(x, weights, rewards), 0, 1\n    )[0]\n    first_order = integrate.quad(\n        lambda x: x * pdf(x, weights, rewards, normalization), 0, 1\n    )[0]\n    second_order = integrate.quad(\n        lambda x: x * x * pdf(x, weights, rewards, normalization), 0, 1\n    )[0]\n    return first_order, (second_order - first_order ** 2) ** 0.5","989f2cd8":"import numpy as np\ntrials = 20\ndiscountings = [1 for _ in range(trials)]\nrewards = [int(np.random.choice([0, 1], p=(0.3, 0.7))) for _ in range(trials)]","4ffb1f29":"get_expected_mean_std(discountings, rewards)","9db6391a":"(beta.mean(1 + sum(rewards), 21 - sum(rewards)),\n beta.std(1 + sum(rewards), 21 - sum(rewards)))","250385c2":"# Thanks Daniel for your post. \n\nfor it in range(trials):\n    discountings[it] = 0.97**it\n    p_k = 0.7 * discountings[it]\n    rewards[it] = int(np.random.choice([0, 1], p=(1-p_k, p_k)))\n\n# Use the with-weightings formula\nprint(\"Weighted approach gives  mean, std:\")\nprint(get_expected_mean_std(discountings, rewards))\n\n# Do the equivalent calculations using beta function:\nprint(\"The beta function gives  mean, std:\")\nprint(\n    (\n        beta.mean(1 + sum(rewards), trials+1 - sum(rewards)),\n        beta.std(1 + sum(rewards), trials+1 - sum(rewards))\n    )\n)","f50b79ce":"%%time\nimport numpy as np\nfrom scipy.optimize import newton\nimport matplotlib.pyplot as plt\ndef sample_one(weights, rewards):\n    normalization = integrate.quad(\n        lambda x: pdf(x, weights, rewards), 0, 1\n    )[0]\n    cdf = lambda x: integrate.quad(\n        lambda p: pdf(p, weights, rewards, normalization), 0, x\n    )[0]\n    x = float(np.random.uniform())\n    return newton(lambda p: cdf(p) - x, x0=0.5)\n\n\nDF_t = 0.5\nsamples = [sample_one([1, 1, 1], [0, 1, 0]) * DF_t for _ in range(1000)]\nplt.hist(samples)","34d621b8":"import numpy as np\n\n\ndef sample_oversimplified_strategy(p):\n    sample = []\n    for i in range(5):\n        sample.append(int(np.random.uniform() < p))\n    while np.random.uniform() < p:\n        sample.append(1)\n    sample.append(0)\n    return sample\n        \n\ndef generate_oversimplified_strategy_estimation(random_arrival_time=True):\n\n    initial_probability = np.linspace(0, 0.95, 200)\n    if random_arrival_time:\n        priority = [\n            float(np.random.uniform(0.4, 0.5)) for _ in range(200)\n        ]\n    else:\n        priority = [\n            1 for _ in range(200)\n        ]\n\n    observations = [\n        sample_oversimplified_strategy(\n            priority[i]* p\n        ) for i, p in enumerate(initial_probability)\n    ]\n\n    discountings = [\n        [\n            priority[i] * 0.97 ** i for i in range(len(ob))\n        ] for i, ob in enumerate(observations)\n    ]\n\n\n    return [\n        get_expected_mean_std(df, ob)[0] for df, ob in zip(\n            discountings, observations\n        )\n    ]","64903201":"import matplotlib.pyplot as plt\nplt.plot(generate_oversimplified_strategy_estimation(random_arrival_time=True), label=\"random arrival time\")\nplt.plot(generate_oversimplified_strategy_estimation(random_arrival_time=False), label=\"no opponent\")\nplt.legend()","2edd9b5e":"# Matching $\\beta$ distribution when no discounting factor <a id='verification'><\/a>","f4d17a56":"# Code <a id='code'><\/a>","c9590464":"# Update log: <a id='update'><\/a>\n\n12\/14\/2020: Formula typo fix.\n\n12\/15\/2020: 1. Sampling from custom distribution provided. 2. Change example from 100 -> 20. 3.bug fix. (introduced when reducing the trial number) 4. add DF_t in sampling example.\n\n12\/16\/2020: Shows examples of information loss. Explain that it may come from sampling generated by strategy not random.","535e9782":"# Inverse Transform Sampling <a id='sampling'><\/a>\n\nPeople may want to sample from this distribution, for example see a nice post here https:\/\/www.kaggle.com\/ilialar\/simple-multi-armed-bandit. Here we have a function to sample from p_i|D. More infor about inverse transform sampling, https:\/\/en.wikipedia.org\/wiki\/Inverse_transform_sampling. (The function below has no performance optimization ... at all.. It taks ~500ms to sample 1000. -.-|||)","f1ccc89f":"# Purpose <a id='purpose'><\/a>\n\nThe purpose of this notebook is to provide sloopy baseline estimation of $\\mathbb{P}(p_i|D)$. Many ideas here were tested, __none of them__ are silver bullets to this competition. Let's collaborate and have some candies together during this holiday season. ","4e986da5":"\n\n\n\n\n# Formula <a id='formula'><\/a>\n\nPosterior calculation\n\nFor i-th slot machine, estimate $\\mathbb{P}(t=0, i)=p_{i}$\n\n\nSuppose you pull the machine several times what you observe are $(d_k, r_k)$, here $d_{k}$ is the discounting factor, $r_{k}$ is the reward.\n\n\nPrior is trivial, $\\mathbb{P}(D_{k}|D_{k-1}, p_i)=(d_k p_{i})^{r_{k}}(1 - d_k p_{i})^{1 - r_{k}}$, we have:\n\n$$\\mathbb{P}(p_i|D) \\sim \\prod_{k}\\left[(d_k p_{i})^{r_{k}}(1 - d_k p_{i})^{1 - r_{k}}\\right].$$\n\n\n\n**Step 1:** Normalization constant:\n$$N = \\int_{0}^1 \\prod_{k}\\left[(d_k p_{i})^{r_{k}}(1 - d_k p_{i})^{1 - r_{k}}\\right] dp_i.$$\n\n\n**Step 2:** Compute expected mean and std:\n$$\\mathbb{E}(p_i^k|D) = \\int_{0}^1 p_i^k\\mathbb{P}(p_i|D) dp_i=\\int_{0}^1 p_i^k \\frac{\\prod_{k}\\left[(d_k p_{i})^{r_{k}}(1 - d_k p_{i})^{1 - r_{k}}\\right]}{N} dp_i.$$","e5c1d65d":"# Table of contents\n1. [Purpose](#purpose)\n2. [Update log](#update)\n3. [Formula](#formula)\n4. [Code](#code)\n5. [Verifications](#verification)\n6. [Sampling](#sampling)\n7. [Information loss](#time_random)\n\n\n![](https:\/\/i.ibb.co\/VwzVbgh\/pexels-nadi-lindsay-3521963.jpg)\n\npic hyperlink (free license): https:\/\/www.pexels.com\/photo\/gingerbread-man-near-coffee-mug-3521963\/","208c16c4":"# Information loss <a id='time_random'><\/a>\n\nWhat people observe is not random sampling, but samples from their strategy & enemy moves.\n\nNow consider one over simplified case:\n* Your opponent pulled several times before you.\n* Start from 5 exploration steps.\n* Stop until getting 1 zeros at time $\\tau$."}}