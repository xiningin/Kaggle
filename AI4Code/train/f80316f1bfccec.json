{"cell_type":{"691de976":"code","14039b7f":"code","6455594a":"code","bb38e95c":"code","e87c6152":"code","599aea39":"code","4afcd4c1":"code","de9829ea":"code","afc13380":"code","a4fc5598":"code","6cfd583f":"code","219f2493":"code","f8f7a6a7":"code","ee52bff9":"code","e32ecd5b":"code","172bdcb2":"code","7ac17171":"code","d4e0a28c":"code","d94abbef":"code","9a1b39c8":"code","e757b0b6":"code","e2652c46":"code","34dc1750":"code","e3d77c0a":"code","1e522646":"code","a3bc8e71":"code","212d1e4b":"code","572da521":"code","0726fe0d":"code","5a47ebd0":"code","c1792527":"code","eb16adae":"code","43940032":"code","b528f3c2":"code","d00f01bf":"code","0b0b2e00":"markdown","30ea158a":"markdown","f6fadbbe":"markdown","910d3d11":"markdown","706ee430":"markdown","0bce101b":"markdown","ebcafea5":"markdown","c7c1689b":"markdown","6e056d63":"markdown","7f0d6d11":"markdown","c1b131c6":"markdown","31119aaa":"markdown","48aa3481":"markdown","9affd9be":"markdown","9950e9ae":"markdown","9c7b28b5":"markdown","083937a9":"markdown","86cf9f42":"markdown","3dd5f38e":"markdown"},"source":{"691de976":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport re\n%pylab inline\nimport matplotlib.image as mpimg\nfrom keras.models import  Sequential\nfrom keras.layers.core import  Lambda , Dense, Flatten, Dropout\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import BatchNormalization, Convolution2D , MaxPooling2D\n\nfrom keras.preprocessing import image\nimport tensorflow as tf\n","14039b7f":"(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\nprint('x_train shape:', x_train.shape)\nprint('y_train shape:', y_train.shape)\nprint(x_train.shape[0], 'taken for training the model')\nprint(x_test.shape[0], 'taken for testing')","6455594a":"print(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\nprint('classes ', y_train.ravel())","bb38e95c":"import seaborn as sns\nfig, ax = plt.subplots(1,2,figsize=(15,5)) \n\nsns.countplot(y_train.ravel(), ax=ax[0] )\nax[0].set_title(\"Visualization of training dataset\", y=1.01, fontsize=20)\nax[0].set_ylabel(\"Name of pictures\", labelpad=15)\nax[0].set_xlabel(\"classes of pictures\", labelpad=15)\n\nsns.countplot(y_test.ravel(), ax=ax[1] )\nplt.title(\"Visualization of the test dataset\", y=1.01, fontsize=20)\nplt.ylabel(\"Name of pictures\", labelpad=15)\nplt.xlabel(\"classes of pictures\", labelpad=15)\n","e87c6152":"\nfig=plt.figure(figsize=(10, 10))\ncolumns = 3\nrows = 2\nfor i in range(1, columns*rows +1):\n    fig.add_subplot(rows, columns, i)\n    img = x_train[i]\n    plt.imshow(img)\n    # if want to show gray image\n    # plt.imshow(X_train[i], cmap=plt.get_cmap('gray'))\n    plt.title(y_train[i])\nplt.show()\n","599aea39":"x_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\n\nx_test.shape","4afcd4c1":"mean_px = x_train.mean().astype(np.float32)\nstd_px = x_train.std().astype(np.float32)\n\ndef standardize(x): \n    return (x-mean_px)\/std_px","de9829ea":"#Normalize\n\nx_train, x_test = x_train \/ 255.0, x_test \/ 255.0","afc13380":"from sklearn.preprocessing import LabelEncoder\nfrom keras.utils import to_categorical\n\nprint(y_train[:3])\n\ny_train = to_categorical(y_train, 10)\ny_test = to_categorical(y_test, 10)\n\nprint(y_train[:3])","a4fc5598":"# fixing random seed for reproducibility\nseed = 43\nnp.random.seed(seed)","6cfd583f":"gen = image.ImageDataGenerator()\nfrom keras.preprocessing.image import ImageDataGenerator\n\ndatagen = ImageDataGenerator(featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, \n        width_shift_range=0.1,  \n        height_shift_range=0.1, \n        horizontal_flip=False,  \n        vertical_flip=False)\n\ndatagen.fit(x_train)\n","219f2493":"#define the convnet\nfrom keras.layers import Conv2D, MaxPool2D\nfrom keras.layers import Dense, Dropout, Activation, Flatten\n\nfrom keras.layers import Convolution2D, MaxPooling2D\n\nmodel6 = Sequential()\nmodel6.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\nmodel6.add(BatchNormalization())\nmodel6.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\nmodel6.add(BatchNormalization())\nmodel6.add(MaxPool2D((2, 2)))\nmodel6.add(Dropout(0.2))\nmodel6.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\nmodel6.add(BatchNormalization())\nmodel6.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\nmodel6.add(BatchNormalization())\nmodel6.add(MaxPool2D((2, 2)))\nmodel6.add(Dropout(0.3))\nmodel6.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\nmodel6.add(BatchNormalization())\nmodel6.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\nmodel6.add(BatchNormalization())\nmodel6.add(MaxPool2D((2, 2)))\nmodel6.add(Dropout(0.4))\nmodel6.add(Flatten())\nmodel6.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\nmodel6.add(BatchNormalization())\nmodel6.add(Dropout(0.5))\nmodel6.add(Dense(10, activation='softmax'))\n# compile model\n# opt = SGD(lr=0.001, momentum=0.9)\nmodel6.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nmodel = model6\nmodel.summary()","f8f7a6a7":"opt = tf.keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6)\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\nhistory = model.fit_generator(datagen.flow(x_train, y_train,\n                                batch_size=64),\n                                epochs=40,\n                              steps_per_epoch=int(x_train.shape[0] \/ 64),\n                                validation_data=(x_test, y_test),\n                                workers=4)","ee52bff9":"def plotmodelhistory(history): \n    fig, axs = plt.subplots(1,2,figsize=(15,5)) \n    # summarize history for accuracy\n    axs[0].plot(history.history['accuracy']) \n    axs[0].plot(history.history['val_accuracy']) \n    axs[0].set_title('Model Accuracy')\n    axs[0].set_ylabel('Accuracy') \n    axs[0].set_xlabel('Epoch')\n    axs[0].legend(['train', 'validate'], loc='upper left')\n    # summarize history for loss\n    axs[1].plot(history.history['loss']) \n    axs[1].plot(history.history['val_loss']) \n    axs[1].set_title('Model Loss')\n    axs[1].set_ylabel('Loss') \n    axs[1].set_xlabel('Epoch')\n    axs[1].legend(['train', 'validate'], loc='upper left')\n    plt.show()\n\n\nplotmodelhistory(history)","e32ecd5b":"# Predict the values from the validation dataset\nY_pred = model.predict(x_test)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1)  \nY_pred_classes[:5]\n\nY_true = np.argmax(y_test, axis=1)\n","172bdcb2":"print(Y_pred_classes[:4])\nprint(Y_true[:4])","7ac17171":"from sklearn.metrics import confusion_matrix\n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \nconfusion_mtx","d4e0a28c":"def heatmap(data, row_labels, col_labels, ax=None, cbar_kw={}, cbarlabel=\"\", **kwargs):\n    \"\"\"\n    Create a heatmap from a numpy array and two lists of labels.\n    \"\"\"\n    if not ax:\n        ax = plt.gca()\n\n    # Plot the heatmap\n    im = ax.imshow(data, **kwargs)\n\n    # Create colorbar\n    cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw)\n    cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=\"bottom\")\n\n    # Let the horizontal axes labeling appear on top.\n    ax.tick_params(top=True, bottom=False,\n                   labeltop=True, labelbottom=False)\n    # We want to show all ticks...\n    ax.set_xticks(np.arange(data.shape[1]))\n    ax.set_yticks(np.arange(data.shape[0]))\n    # ... and label them with the respective list entries.\n    ax.set_xticklabels(col_labels)\n    ax.set_yticklabels(row_labels)\n    \n    ax.set_xlabel('Predicted Label') \n    ax.set_ylabel('True Label')\n    \n    return im, cbar\n\ndef annotate_heatmap(im, data=None, fmt=\"d\", threshold=None):\n    \"\"\"\n    A function to annotate a heatmap.\n    \"\"\"\n    # Change the text's color depending on the data.\n    texts = []\n    for i in range(data.shape[0]):\n        for j in range(data.shape[1]):\n            text = im.axes.text(j, i, format(data[i, j], fmt), horizontalalignment=\"center\",\n                                 color=\"white\" if data[i, j] > thresh else \"black\")\n            texts.append(text)\n\n    return texts","d94abbef":"import seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport itertools\nimport matplotlib.pyplot as plt\n\nlabels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', \n          'ship', 'truck']\n# Errors are difference between predicted labels and true labels\nerrors = (Y_pred_classes - Y_true != 0)\n\nY_pred_classes_errors = Y_pred_classes[errors]\nY_pred_errors = Y_pred[errors]\nY_true_errors = Y_true[errors]\nX_test_errors = x_test[errors]\n\ncm = confusion_matrix(Y_true, Y_pred_classes) \nthresh = cm.max() \/ 2.\n\nfig, ax = plt.subplots(figsize=(12,12))\nim, cbar = heatmap(cm, labels, labels, ax=ax,\n                   cmap=plt.cm.Blues, cbarlabel=\"count of predictions\")\ntexts = annotate_heatmap(im, data=cm, threshold=thresh)\n\nfig.tight_layout()\nplt.show()","9a1b39c8":"print(classification_report(Y_true, Y_pred_classes))\n","e757b0b6":"R = 2\nC = 4\nfig, axes = plt.subplots(R, C, figsize=(12,8))\naxes = axes.ravel()\n\nmisclassified_idx = np.where(Y_pred_classes != Y_true)[0]\nfor i in np.arange(0, R*C):\n    axes[i].imshow(x_test[misclassified_idx[i]])\n    axes[i].set_title(\"True: %s \\nPredicted: %s\" % (labels[Y_true[misclassified_idx[i]]], \n                                                  labels[Y_pred_classes[misclassified_idx[i]]]))\n    axes[i].axis('off')\n    plt.subplots_adjust(wspace=1)","e2652c46":"fig = plt.figure(figsize = (3,3))\ntest_image = np.expand_dims(x_test[26], axis=0)\ntest_result = model.predict_classes(test_image)\nplt.imshow(x_test[26])\ndict_key = test_result[0]\nplt.title(\"Predicted: {} \\nTrue Label: {}\".format(labels[dict_key], labels[Y_true[26]]))","34dc1750":"import os\nsave_dir = os.path.join(os.getcwd(), 'saved_models')\nmodel_name = 'cifar10_trained_model.h5'\n\n# Save model and weights\nif not os.path.isdir(save_dir):\n    os.makedirs(save_dir)\nmodel_path = os.path.join(save_dir, model_name)\nmodel.save(model_path)\nprint('Saved trained model at %s ' % model_path)\n\n# checking the model\nscores = model.evaluate(x_test, y_test, verbose=1)\nprint('Test loss:', scores[0])\nprint('Test accuracy:', scores[1])","e3d77c0a":"Y_pred_classes[:2]","1e522646":"!pip install py7zr\nfrom py7zr import unpack_7zarchive\nimport shutil\nimport os\n\nshutil.register_unpack_format('7zip', ['.7z'], unpack_7zarchive)","a3bc8e71":"shutil.unpack_archive('\/kaggle\/input\/cifar-10\/test.7z', '\/kaggle\/working')","212d1e4b":"test_dir = os.listdir(\".\/test\")\ntest_dir_len = len(test_dir)\n\nprint(\".\\\\test:\\t\",test_dir_len)\nprint(\"files:\\t\\t\",test_dir[:3])\n\n","572da521":"test_data_generator = ImageDataGenerator(rescale=1.\/255.)\ntest_generator = test_data_generator.flow_from_directory(directory='\/kaggle\/working',\n            batch_size=64,\n            shuffle=False,color_mode='rgb',\n            target_size=(32,32),\n            class_mode=None)\n\n","0726fe0d":"test_prediction = model.predict_generator(test_generator)\n\npredicted_class = np.argmax(test_prediction, axis=1)","5a47ebd0":"print(predicted_class[:3])\npredicted_class.shape\n","c1792527":"submission1 = [labels[i] for i in predicted_class]\n\nsubmission = pd.DataFrame({\"id\": list(range(1, len(predicted_class)+1)),\n                          \"label\": submission1})\n\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission[:2]","eb16adae":"var = pd.read_csv(\".\/submission.csv\")\nvar.shape","43940032":"var.head(101)","b528f3c2":"import matplotlib.image as mpimg\nfig = plt.figure(figsize = (3,3))\nimg = mpimg.imread(\"\/kaggle\/working\/test\/\"+ str(test_dir[1]))\nplt.imshow(img)","d00f01bf":"index = 0    \nfig = plt.figure(figsize = (16,10))\nfor item in submission.values[50:70]:\n    index += 1\n    plt.subplot(5, 5, index)\n    test_path = '\/kaggle\/working\/test\/'+str(item[0])+'.png'\n    print(test_path)\n    test_image = image.load_img(test_path, target_size=(32,32))\n    plt.imshow(test_image)\n    plt.colorbar()\n    plt.grid(False)\n    plt.axis(\"off\")\n    \n    test_result = model.predict_classes(test_image)\n    dict_key = test_result[0]\n    plt.title(labels[dict_key])\nplt.show()\n\n\n# test_result = model.predict_classes(test_image)\n# plt.imshow(x_test[26])\n# dict_key = test_result[0]\n# plt.title(\"Predicted: {} \\nTrue Label: {}\".format(labels[dict_key], labels[Y_true[26]]))","0b0b2e00":"# Visualizing the dataset","30ea158a":"# Preprocessing images","f6fadbbe":"# confusion matrix\n**calculating confusion matrix to get the false negative, false positive, true negative and true positive values predicted by the classfier.**","910d3d11":"# ****importing all libraries****","706ee430":"**Classification report**","0bce101b":"X_train_dataset = X_train_dataset.reshape(\n*                         X_train_dataset.shape[0], 28, 28)\n* print(X_train_dataset.shape)\n* print(y_train_dataset.shape)\n\nx_test_dataset = x_test_dataset.reshape(x_test_dataset.shape[0], 28, 28)\nx_test_dataset.shape","ebcafea5":"# **Building CNN model**","c7c1689b":"# Transform categorical data\n**Transforming all the labels into dummy variables as it is a multiclass classification problem.**","6e056d63":"# Data type conversion\ni am converting data to float as it's efficient for computation","7f0d6d11":"# Evaluating the model","c1b131c6":"**incorrect prediction**","31119aaa":"# Convolutional Neural Network\n> making change in images so that the classifier can learn more uniquely","48aa3481":"**saving the prediction to a csv file**","9affd9be":"**Predicting new value using the trained model.**","9950e9ae":"*In keras, fit() is much similar to sklearn's fit method, where you pass array of features as x values and target as y values. You pass your whole dataset at once in fit method. Also, use it if you can load whole data into your memory (small dataset).*\n\nIn fit_generator(), you don't pass the x and y directly, instead they come from a generator. As it is written in keras documentation, generator is used when you want to avoid duplicate data when using multiprocessing. ","9c7b28b5":"# this is a multiclass classification problem\n*Here multiple classes are present in the y_train part of dataset*","083937a9":"# Convert train datset to (num_images, img_rows, img_cols) format","86cf9f42":"# Loading training and test data","3dd5f38e":"**compiling the model**"}}