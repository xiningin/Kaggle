{"cell_type":{"2e898741":"code","b5f9bc5a":"code","ada5e854":"code","15eb03f4":"code","d4313173":"code","ad16449d":"code","9fed84a1":"code","82ffd958":"code","96099610":"code","cd55111a":"code","1ffaded6":"code","e232752b":"code","6f1f22e6":"code","47447dac":"code","a47a4ffc":"code","d818f6db":"code","e1d5a092":"code","7d924f63":"code","d0f7c0a1":"code","1306ddbe":"code","400be6be":"code","d9ac3141":"code","654d57cb":"code","5d1350e1":"code","9d65868a":"code","92c9260c":"code","c42f00ab":"code","589e2726":"code","bd5c86f7":"code","8c8ba530":"code","1adca0da":"code","c078b2ba":"code","cd017fa5":"code","d5914626":"code","28c54a53":"code","26841e84":"code","8ea8b377":"code","805108c7":"code","dbe87be1":"code","c46229e2":"code","943eaf43":"code","74bc3f5e":"code","1cf6848c":"code","a4d43953":"code","f1ee3ee6":"code","b7364deb":"code","8b900e11":"code","3754f32c":"code","e180d701":"code","abb7538e":"code","a5b120b2":"code","50c15ee3":"code","edab9cb1":"code","d6f611df":"markdown","1c546fec":"markdown","be3afc99":"markdown","197a35a6":"markdown","dee71c88":"markdown","979184e0":"markdown","51f61e70":"markdown","0b2c6256":"markdown","ebffd5e2":"markdown","b0c5aef1":"markdown","465aaad6":"markdown","cc469cc3":"markdown","42b08d83":"markdown"},"source":{"2e898741":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b5f9bc5a":"import pandas as pd\nimport numpy as np\nimport html\nimport string\nimport re\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nfrom textblob import TextBlob\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport datetime as dt\nimport seaborn as sns\nfrom pandas.io.json import json_normalize\nfrom random import randint\nimport json\nimport time\nimport sys\nimport networkx as nx\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, iplot\nimport plotly\nfrom sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nimport igraph as ig\nfrom plotly.graph_objs import *\nfrom PIL import Image","ada5e854":"biden_file = '..\/input\/united-states-political-tweets\/biden_timeline.json'\nharris_file = '..\/input\/united-states-political-tweets\/harris_timeline.json'\npence_file = '..\/input\/united-states-political-tweets\/pence_timeline.json'\ntrump_file = '..\/input\/united-states-political-tweets\/trump_timeline.json'\n\nwith open(biden_file, 'r') as file:\n    biden_data = json.load(file)\n\nwith open(harris_file, 'r') as file:\n    harris_data = json.load(file)\n\nwith open(pence_file, 'r') as file:\n    pence_data = json.load(file)\n\nwith open(trump_file, 'r') as file:\n    trump_data = json.load(file)\n\nprint([len(biden_data), len(harris_data), len(pence_data), len(trump_data)])","15eb03f4":"biden_data[0]","d4313173":"df_biden = pd.DataFrame([t['created_at'] for t in biden_data], columns=['create_time'])\ndf_biden['text'] = [t['text'] for t in biden_data]\ndf_biden['retweets'] = [t['retweet_count'] for t in biden_data]\ndf_biden['favorites'] = [t['favorite_count'] for t in biden_data]\n\nbiden_tokens = [word_tokenize(m) for m in df_biden['text']]\nbiden_tokens_cleaned = [[q for q in p if q.isalpha()] for p in biden_tokens]\nbiden_token_list = []\nfor n in range(len(biden_tokens_cleaned)):\n     biden_token_list.append(len(biden_tokens_cleaned[n]))\ndf_biden['word_tokens'] = biden_token_list\n\ndf_biden","ad16449d":"avg_favorites_biden = round(df_biden.loc[df_biden['favorites']!=0, 'favorites'].mean(), 2)\ndf_biden=df_biden.replace({'favorites': {0: avg_favorites_biden}})\ndf_biden","9fed84a1":"df_harris = pd.DataFrame([t['created_at'] for t in harris_data], columns=['create_time'])\ndf_harris['text'] = [t['text'] for t in harris_data]\ndf_harris['retweets'] = [t['retweet_count'] for t in harris_data]\ndf_harris['favorites'] = [t['favorite_count'] for t in harris_data]\n\nharris_tokens = [word_tokenize(m) for m in df_harris['text']]\nharris_tokens_cleaned = [[q for q in p if q.isalpha()] for p in harris_tokens]\nharris_token_list = []\nfor n in range(len(harris_tokens_cleaned)):\n     harris_token_list.append(len(harris_tokens_cleaned[n]))\ndf_harris['word_tokens'] = harris_token_list\n\ndf_harris","82ffd958":"df_pence = pd.DataFrame([t['created_at'] for t in pence_data], columns=['create_time'])\ndf_pence['text'] = [t['text'] for t in pence_data]\ndf_pence['retweets'] = [t['retweet_count'] for t in pence_data]\ndf_pence['favorites'] = [t['favorite_count'] for t in pence_data]\n\npence_tokens = [word_tokenize(m) for m in df_pence['text']]\npence_tokens_cleaned = [[q for q in p if q.isalpha()] for p in pence_tokens]\npence_token_list = []\nfor n in range(len(pence_tokens_cleaned)):\n     pence_token_list.append(len(pence_tokens_cleaned[n]))\ndf_pence['word_tokens'] = pence_token_list\n\ndf_pence","96099610":"df_trump = pd.json_normalize(trump_data)\ndf_trump = df_trump[['created_at', 'text', 'retweet_count', 'favorite_count']]\ndf_trump = df_trump.rename(columns={\"created_at\": \"create_time\", 'retweet_count':'retweets', 'favorite_count':'favorites'})\n\ntrump_tokens = [word_tokenize(m) for m in df_trump['text']]\ntrump_tokens_cleaned = [[q for q in p if q.isalpha()] for p in trump_tokens]\ntrump_token_list = []\nfor n in range(len(trump_tokens_cleaned)):\n     trump_token_list.append(len(trump_tokens_cleaned[n]))\ndf_trump['word_tokens'] = trump_token_list\n\ndf_trump","cd55111a":"avg_favorites_trump = round(df_trump.loc[df_trump['favorites']!=0, 'favorites'].mean(), 2)\ndf_trump=df_trump.replace({'favorites': {0: avg_favorites_trump}})\ndf_trump","1ffaded6":"stop_words = set(stopwords.words('english'))\n\ndef text_cleanup(c):\n    clean_unesc = html.unescape(re.sub(r\"http\\S+\", \"\", re.sub('\\n+', ' ', c)))\n    clean_noemoji = clean_unesc.encode('ascii', 'ignore').decode('ascii')\n    wt = word_tokenize(clean_noemoji.lower())\n    wt_filt = [w for w in wt if (w not in stop_words) and (w not in string.punctuation) and (w.isalnum())]\n    return ' '.join(wt_filt)\n\ndef sentim_polarity(s):\n    return TextBlob(s).sentiment.polarity\n\ndef sentim_subject(s):\n    return TextBlob(s).sentiment.subjectivity","e232752b":"df_biden['text_clean'] = df_biden['text'].apply(text_cleanup)\ndf_biden","6f1f22e6":"df_biden['polarity'] = df_biden['text_clean'].apply(sentim_polarity)\ndf_biden['subjectivity'] = df_biden['text_clean'].apply(sentim_subject)\ndf_biden","47447dac":"df_harris['text_clean'] = df_harris['text'].apply(text_cleanup)\ndf_harris['polarity'] = df_harris['text_clean'].apply(sentim_polarity)\ndf_harris['subjectivity'] = df_harris['text_clean'].apply(sentim_subject)\ndf_harris","a47a4ffc":"df_pence['text_clean'] = df_pence['text'].apply(text_cleanup)\ndf_pence['polarity'] = df_pence['text_clean'].apply(sentim_polarity)\ndf_pence['subjectivity'] = df_pence['text_clean'].apply(sentim_subject)\ndf_pence","d818f6db":"df_trump['text_clean'] = df_trump['text'].apply(text_cleanup)\ndf_trump['polarity'] = df_trump['text_clean'].apply(sentim_polarity)\ndf_trump['subjectivity'] = df_trump['text_clean'].apply(sentim_subject)\ndf_trump","e1d5a092":"df_biden_post = df_biden[['create_time', 'retweets', 'favorites']]\ndf_biden_post","7d924f63":"biden_time_list = pd.to_datetime(df_biden_post['create_time'], format='%a %b %d %H:%M:%S +0000 %Y')\ndf_biden_post = df_biden_post.assign(create_time=biden_time_list)\nbiden_hour_list = df_biden_post['create_time'].dt.strftime('%H')\ndf_biden_post = df_biden_post.assign(create_hour=biden_hour_list)\nbiden_post_avg = df_biden_post.groupby('create_hour').mean().round(2)\nbiden_post_avg","d0f7c0a1":"biden_max_retweets = biden_post_avg.loc[:, 'retweets'].max()\nbiden_max_likes = biden_post_avg.loc[:, 'favorites'].max()\nbiden_post_avg['retweet_percent'] = (biden_post_avg['retweets'] \/ biden_max_retweets).round(2)\nbiden_post_avg['like_percent'] = (biden_post_avg['favorites'] \/ biden_max_likes).round(2)\nbiden_post_avg.head()","1306ddbe":"biden_post_heatmap = biden_post_avg[['retweet_percent', 'like_percent']]\nplt.subplots(figsize=(16, 8))\ng = sns.heatmap(biden_post_heatmap, cmap = 'YlGnBu')\ng.set_xticklabels(['Retweet','Favorite'])\nplt.xlabel('Post Engagement')\nplt.ylabel('Post Time')\nplt.show()","400be6be":"df_trump_post = df_trump[['create_time', 'retweets', 'favorites']]\n\ntrump_time_list = pd.to_datetime(df_trump_post['create_time'], format='%a %b %d %H:%M:%S +0000 %Y')\ndf_trump_post = df_trump_post.assign(create_time=trump_time_list)\ntrump_hour_list = df_trump_post['create_time'].dt.strftime('%H')\ndf_trump_post = df_trump_post.assign(create_hour=trump_hour_list)\ntrump_post_avg = df_trump_post.groupby('create_hour').mean().round(2)\n\ntrump_max_retweets = trump_post_avg.loc[:, 'retweets'].max()\ntrump_max_likes = trump_post_avg.loc[:, 'favorites'].max()\ntrump_post_avg['retweet_percent'] = (trump_post_avg['retweets'] \/ trump_max_retweets).round(2)\ntrump_post_avg['like_percent'] = (trump_post_avg['favorites'] \/ trump_max_likes).round(2)\ntrump_post_avg.head()","d9ac3141":"trump_post_heatmap = trump_post_avg[['retweet_percent', 'like_percent']]\nplt.subplots(figsize=(16, 8))\ng = sns.heatmap(trump_post_heatmap, cmap = 'YlGnBu')\ng.set_xticklabels(['Retweet','Favorite'])\nplt.xlabel('Post Engagement')\nplt.ylabel('Post Time')\nplt.show()","654d57cb":"def similar_color_blue(word=None, font_size=None, position=None, orientation=None,\nfont_path=None, random_state=None):\n    h = 220 # 0 - 360\n    s = 60 # 0 - 100\n    l = random_state.randint(30, 70) # 0 - 100\n    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n\ndef similar_color_black(word=None, font_size=None, position=None, orientation=None,\nfont_path=None, random_state=None):\n    h = 220 # 0 - 360\n    s = 0 # 0 - 100\n    l = random_state.randint(10, 70) # 0 - 100\n    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n\ndef similar_color_red(word=None, font_size=None, position=None, orientation=None,\nfont_path=None, random_state=None):\n    h = 0 # 0 - 360\n    s = 100 # 0 - 100\n    l = random_state.randint(10, 70) # 0 - 100\n    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n\ndef similar_color_green(word=None, font_size=None, position=None, orientation=None,\nfont_path=None, random_state=None):\n    h = 140 # 0 - 360\n    s = 100 # 0 - 100\n    l = random_state.randint(10, 70) # 0 - 100\n    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n\ndef similar_color_orange(word=None, font_size=None, position=None, orientation=None,\nfont_path=None, random_state=None):\n    h = 40 # 0 - 360\n    s = 100 # 0 - 100\n    l = random_state.randint(10, 70) # 0 - 100\n    return \"hsl({}, {}%, {}%)\".format(h, s, l)","5d1350e1":"biden_text_clean = ' '.join(df_biden['text_clean'])\nbiden_picture = '..\/input\/d\/foolwuilin\/picture\/sentiment_01.jpg'\n\nmask = np.array(Image.open(biden_picture))\nplt.subplots(figsize=(16, 8))\nwc = WordCloud(width=mask.shape[1], height=mask.shape[0], max_font_size=60, \n               collocations=False, mask=mask, background_color='white',\n              random_state=44, color_func=similar_color_blue).generate(biden_text_clean)\nplt.axis(\"off\")\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.show()","9d65868a":"trump_text_clean = ' '.join(df_trump['text_clean'])\ntrump_picture = '..\/input\/d\/foolwuilin\/picture\/sentiment_02.jpg'\n\nmask = np.array(Image.open(trump_picture))\nplt.subplots(figsize=(16, 8))\nwc = WordCloud(width=mask.shape[1], height=mask.shape[0], max_font_size=60, \n               collocations=False, mask=mask, background_color='white',\n              random_state=44, color_func=similar_color_red).generate(trump_text_clean)\nplt.axis(\"off\")\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.show()","92c9260c":"df_biden['create_time']= pd.to_datetime(df_biden['create_time'])\ndf_biden = df_biden.set_index('create_time')\ndf_biden","c42f00ab":"df_trump['create_time']= pd.to_datetime(df_trump['create_time'])\ndf_trump = df_trump.set_index('create_time')\ndf_trump","589e2726":"biden_trump = df_biden['text'].str.contains('trump', case=False)\nprint(np.sum(biden_trump)\/df_biden.shape[0])","bd5c86f7":"trump_biden = df_trump['text'].str.contains('biden', case=False)\nprint(np.sum(trump_biden)\/df_trump.shape[0])","8c8ba530":"biden_trump_day = biden_trump.resample('1440min').mean().round(2)\nbiden_trump_day","1adca0da":"trump_biden_day = trump_biden.resample('1440min').mean().round(2)\ntrump_biden_day","c078b2ba":"fig = go.Figure()\nfig.add_trace(go.Scatter(\n    x=biden_trump_day.index.day,\n    y=biden_trump_day,\n    line_color='rgb(0,100,80)',\n    showlegend=True,\n    name='Biden Mentioned Trump',\n))\nfig.add_trace(go.Scatter(\n    x=trump_biden_day.index.day, \n    y=trump_biden_day,\n    line_color='rgb(231,107,243)',\n    showlegend=True,\n    name='Trump Mentioned Biden',\n))\nfig.update_layout({  \n      'showlegend':True, 'legend':{'x':0.2, 'y':0.96, 'bgcolor':'rgb(246, 228, 129)'}\n      })\nfig.update_xaxes(\n        title_text = \"Oct 2020\",\n        title_font = {\"size\": 16},\n        title_standoff = 12)\nfig.update_yaxes(\n        title_text = \"Average Mention Times\",\n        title_font = {\"size\": 16},\n        title_standoff = 12)\nfig.update_traces(mode='lines')\nfig.show()","cd017fa5":"df_biden_reset = df_biden.reset_index()\ndf_biden_reset['Oct'] = df_biden_reset['create_time'].dt.strftime('%d')\ndf_biden_pol = df_biden_reset.groupby('Oct').agg({'polarity':'mean'})\ndf_biden_pol","d5914626":"df_trump_reset = df_trump.reset_index()\ndf_trump_reset['Oct'] = df_trump_reset['create_time'].dt.strftime('%d')\ndf_trump_pol = df_trump_reset.groupby('Oct').agg({'polarity':'mean'})\ndf_trump_pol","28c54a53":"fig = go.Figure()\nfig.add_trace(go.Scatter(\n    x=df_biden_pol.index,\n    y=df_biden_pol['polarity'],\n    line_color='rgb(0,100,80)',\n    showlegend=True,\n    name='Sentiment (Biden Mentioned Trump)',\n))\nfig.add_trace(go.Scatter(\n    x=df_trump_pol.index, \n    y=df_trump_pol['polarity'],\n    line_color='rgb(231,107,243)',\n    showlegend=True,\n    name='Sentiment (Trump Mentioned Biden)',\n))\nfig.update_layout({  \n      'showlegend':True, 'legend':{'x':0.02, 'y':0.96, 'bgcolor':'rgb(246, 228, 129)'}\n      })\nfig.update_xaxes(\n        title_text = \"Oct 2020\",\n        title_font = {\"size\": 16},\n        title_standoff = 12)\nfig.update_yaxes(\n        title_text = \"Average Mention Times\",\n        title_font = {\"size\": 16},\n        title_standoff = 12)\nfig.update_traces(mode='lines')\nfig.show()","26841e84":"harris_network = pd.json_normalize(harris_data)\nharris_network = harris_network[['user.screen_name', 'retweeted_status.user.screen_name']]\nharris_network","8ea8b377":"harris_network = harris_network.dropna(axis=0)\n\nharris_retweeted_G = nx.from_pandas_edgelist(\n    harris_network,\n    source = 'user.screen_name',\n    target = 'retweeted_status.user.screen_name',\n    create_using = nx.DiGraph())\n \nprint('Nodes in RT network:', len(harris_retweeted_G.nodes()))\nprint('Edges in RT network:', len(harris_retweeted_G.edges()))","805108c7":"harris_retweeted_G.nodes()","dbe87be1":"harris_retweeted_G.edges()","c46229e2":"retweet_bt = nx.in_degree_centrality(harris_retweeted_G)\nharris_reteet_bt = pd.DataFrame(list(retweet_bt.items()), columns = ['screen_name', 'centrality'])\nharris_reteet_bt = harris_reteet_bt.dropna(axis=0)\nharris_reteet_bt","943eaf43":"pos = nx.random_layout(harris_retweeted_G)\n\nsizes = [x[1]*100 for x in harris_retweeted_G.degree()]\n\nnx.draw_networkx(harris_retweeted_G, pos, \n    with_labels = True, \n    node_size = sizes,\n    width = 0.1, alpha = 0.7,\n    arrowsize = 2, linewidths = 0)\n\nplt.axis('off')\nplt.show()","74bc3f5e":"pence_network = pd.json_normalize(pence_data)\npence_network = pence_network[['user.screen_name', 'retweeted_status.user.screen_name']]\npence_network","1cf6848c":"trump_network = pd.json_normalize(trump_data)\ntrump_network = trump_network[['user.screen_name', 'retweeted_status.user.screen_name']]\ntrump_network","a4d43953":"network = pd.concat([pence_network, trump_network], axis=0)\nnetwork = network.dropna(axis=0)\nnetwork","f1ee3ee6":"network_A = list(network[\"user.screen_name\"].unique())\nnetwork_B = list(network[\"retweeted_status.user.screen_name\"].unique())\nnode_list = list(set(network_A+network_B))\nG = nx.Graph()\nfor i in node_list:\n    G.add_node(i)\nfor i,j in network.iterrows():\n    G.add_edges_from([(j[\"user.screen_name\"],j[\"retweeted_status.user.screen_name\"])])\npos = nx.spring_layout(G, k=0.5, dim=2, iterations=80)\nfor n, p in pos.items():\n    G.nodes[n]['pos'] = p\n\nedge_trace = go.Scatter(\n    x=[],\n    y=[],\n    line=dict(width=0.5,color='#888'),\n    hoverinfo='none',\n    mode='lines')\nfor edge in G.edges():\n    x0, y0 = G.nodes[edge[0]]['pos']\n    x1, y1 = G.nodes[edge[1]]['pos']\n    edge_trace['x'] += tuple([x0, x1, None])\n    edge_trace['y'] += tuple([y0, y1, None])\n\nnode_trace = go.Scatter(\n    x=[],\n    y=[],\n    text=[],\n    mode='markers',\n    hoverinfo='text',\n    marker=dict(\n        showscale=True,\n        colorscale='YlGnBu',\n        reversescale=True,\n        color=[],\n        size=15,\n        colorbar=dict(\n            thickness=10,\n            title='Node Connections',\n            xanchor='left',\n            titleside='right'\n        ),\n        line=dict(width=0)))\nfor node in G.nodes():\n    x, y = G.nodes[node]['pos']\n    node_trace['x'] += tuple([x])\n    node_trace['y'] += tuple([y])\n\nfor node, adjacencies in enumerate(G.adjacency()):\n    node_trace['marker']['color']+=tuple([len(adjacencies[1])])\n    node_info = str(adjacencies[0]) +' has {} connections'.format(str(len(adjacencies[1])))\n    node_trace['text']+=tuple([node_info])\n\nfig = go.Figure(data=[edge_trace, node_trace],\n             layout=go.Layout(\n                title='Retweet Network',\n                titlefont=dict(size=16),\n                showlegend=False,\n                hovermode='closest',\n                margin=dict(b=20,l=5,r=5,t=40),\n                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))\niplot(fig)","b7364deb":"network","8b900e11":"#Another network plot by igraph\nnetwork_A = list(network[\"user.screen_name\"].unique())\nnetwork_B = list(network[\"retweeted_status.user.screen_name\"].unique())\nG = ig.Graph.DataFrame(network, directed=False)\nlabels = list(set(network_A + network_B))\nN=len(labels)\nE=[e.tuple for e in G.es]\nlayt=G.layout('kk')\n\nXn=[layt[k][0] for k in range(N)]\nYn=[layt[k][1] for k in range(N)]\nXe=[]\nYe=[]\nfor e in E:\n    Xe+=[layt[e[0]][0],layt[e[1]][0], None]\n    Ye+=[layt[e[0]][1],layt[e[1]][1], None]\n\ntrace1=Scatter(x=Xe,\n               y=Ye,\n               mode='lines',\n               line= dict(color='rgb(210,210,210)', width=1),\n               hoverinfo='none'\n               )\n\ntrace2=Scatter(x=Xn,\n               y=Yn,\n               mode='markers',\n               name='ntw',\n               marker=dict(symbol='circle-dot',\n                           size=10,\n                           color='#6959CD',\n                           line=dict(color='rgb(50,50,50)', width=0.5)\n                          ),\n               text=labels,\n               hoverinfo='text'\n               )\n\naxis=dict(showline=False,\n          zeroline=False,\n          showgrid=False,\n          showticklabels=False,\n          title=''\n          )\n\nwidth=800\nheight=800\nlayout=Layout(title= \"test\",\n    font= dict(size=12),\n    showlegend=False,\n    autosize=False,\n    width=width,\n    height=height,\n    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)\n    )\n\ndata=[trace1, trace2]\nfig=Figure(data=data, layout=layout)\niplot(fig)","3754f32c":"df_biden['like_core']=pd.qcut(df_biden['favorites'].rank(method='first'), q=3, labels=range(1,4))\ndf_biden.head()","e180d701":"biden_tv=TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS, ngram_range=(1, 2), max_features=1000,token_pattern=r'\\b[^\\d\\W][^\\d\\W]+\\b').fit(df_biden['text_clean'])\nbiden_tranformed = biden_tv.transform(df_biden['text_clean'])\nbiden_matrix = pd.DataFrame(biden_tranformed.toarray(), columns=biden_tv.get_feature_names())\nbiden_matrix","abb7538e":"X_b = biden_matrix\ny_b = df_biden['like_core']\n\nc_space = np.arange(1, 50)\nparam_grid = {'C': c_space, 'penalty': ['none', 'l2']}\n\nX_b_train, X_b_test, y_b_train, y_b_test = train_test_split(X_b, y_b, test_size=0.2, random_state=44)\nlogreg = LogisticRegression(max_iter=1000, random_state=44)\nlogreg_cv = GridSearchCV(logreg, param_grid=param_grid,\n                 refit=True, cv=3, n_jobs=24)\nlogreg_cv.fit(X_b_train, y_b_train)\nlogreg_best = logreg_cv.best_estimator_\nlogreg_best.fit(X_b_train, y_b_train)\ny_b_predicted = logreg_best.predict(X_b_test)\n\nprint(\"Tuned Logistic Regression Parameter: {}\".format(logreg_cv.best_params_))\nprint(\"Tuned Logistic Regression Accuracy: {}\".format(logreg_cv.best_score_))\nprint('Test Accuracy (Biden) -', accuracy_score(y_b_test, y_b_predicted))\nprint(confusion_matrix(y_b_test, y_b_predicted))\nprint(\"Test Classification Report (Biden)\\n\", classification_report(y_b_test, y_b_predicted))","a5b120b2":"df_trump['like_core']=pd.qcut(df_trump['favorites'].rank(method='first'), q=3, labels=range(1,4))\ndf_trump.head()","50c15ee3":"trump_tv=TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS, ngram_range=(1, 2), max_features=1000,token_pattern=r'\\b[^\\d\\W][^\\d\\W]+\\b').fit(df_trump['text_clean'])\ntrump_tranformed = biden_tv.transform(df_trump['text_clean'])\ntrump_matrix = pd.DataFrame(trump_tranformed.toarray(), columns=trump_tv.get_feature_names())\ntrump_matrix","edab9cb1":"X_t = trump_matrix\ny_t = df_trump['like_core']\n\nc_space = np.logspace(-5, 8, 15)\nparam_grid = {'C': c_space, 'penalty': ['none', 'l2']}\n\nX_t_train, X_t_test, y_t_train, y_t_test = train_test_split(X_t, y_t, test_size=0.2, random_state=44)\nlogreg = LogisticRegression(max_iter=1000, random_state=44)\nlogreg_cv = GridSearchCV(logreg, param_grid=param_grid,\n                 refit=True, cv=3, n_jobs=24)\nlogreg_cv.fit(X_t_train, y_t_train)\nlogreg_best = logreg_cv.best_estimator_\nlogreg_best.fit(X_t_train, y_t_train)\ny_t_predicted = logreg_best.predict(X_t_test)\n\nprint(\"Tuned Logistic Regression Parameter: {}\".format(logreg_cv.best_params_))\nprint(\"Tuned Logistic Regression Accuracy: {}\".format(logreg_cv.best_score_))\nprint('Test Accuracy (Trump) -', accuracy_score(y_t_test, y_t_predicted))\nprint(confusion_matrix(y_t_test, y_t_predicted))\nprint(\"Test Classification Report (Trump)\\n\", classification_report(y_t_test, y_t_predicted))","d6f611df":"\nTime to understand how vice-president retweeted the message. Where are these message from? It is ideal to know the situation by plotting the networks.","1c546fec":"\nSearch the messages that Biden and Trump mentioned each other. Then, plot a line chart to understand the frequency that the name is mentioned.","be3afc99":"\nStart cleaning the text for the further WordCloud graphics. In addition, add the polarity and subjectivity columns for sentiment scores.","197a35a6":"Prepare dataframes containing the propotion of the retweets and favorites in different hours. Then, plot a heatmap to understand when to post to get the highest retweets and favorites. ","dee71c88":"# 1. Introduction\n## 1.1 Summary\nThis analysis is a review of the 2020 United States presidential election. This election was not only an election in the real world but also a digital competition. President candidates were sending out messages online. Thus, people could react to the messages and also interact with other fans through the digital world.\n\n## 1.2 Analytics Tool and Dataset\nThe given dataset is the Twitter timeline of Joe Biden, Kamala Harris, Mike Pence, and Donald Trump. There are 4 .json files. The files of Joe Biden, Kamala Harris, and Donald Trump have 200 pieces of data per each. Mike Pence has a missing one, so it has 199 pieces of data.","979184e0":"\nLoad 4 .json files and review how many pieces of data inside.","51f61e70":"## 4. Analyze\nGive a level by scoring the likes. This is the criterion for machine learning to train the model and then predict the like level is going to be when posting. ","0b2c6256":"# 5. Conclusion\nAfter building the models, the accuracy of the prediction for Biden is 70%. Although this number is not ideal, the prediction seems not so bad if looking into the confusion matrix. When predicting the like level is 1, 8 posts are correctly predicted. 3 posts are predicted as 2. It is good that no post is actually 3, which should be a post with many likes.\n\nWhen predicting posts as 2, 7 posts are correctly predicted. 5 posts are predicted as level 2, but actually, they are level 3. The good thing is that only 1 post is predicted as level 2 but actually gets a lower number of likes, which means only 1 post will come below the expectation.\n\nAs for 15 posts predicted as level 3, only 3 posts are below the expectation as level 2. No post is actually level 1. These are a good sign for accuracy, although it is only 70%.\n\nIn contrast, The prediction for Trump's posts is not so good. The accuracy is only 57.5%. Also, in regards to the predicted posts as level 3, 4 posts are actually in level 1. 5 posts are predicted as level 2, but they are the posts getting lower likes. This situation is way below the expectation, which is not acceptable. This situation tells there is more noise in Trump's posts. It is more difficult to predict the taste of the audience. Plus, it generates more uncertainty when the election.","ebffd5e2":"# 3. Process\n## 3.1 Exploratory Data Analysis\nThis section involves data cleansing with an exploratory data analysis. It is a preparation of the dataset for further analysis. First of all, load the essential pakages.","b0c5aef1":"## 3.2 EDA Summary\nIn this section, there are several findings as below.\n1. When Biden posted at 12 pm, the posts got the most retweets and likes. In contrast, Trump got more retweets and likes when posting from 1 pm to 3 pm.\n2. The WordCloud graphics demonstrate Biden and Trump usually mentioned to each other.\n3. Because of a lack of data, it is hard to tell the difference of how they mentioned to each other.\n4. Pence and Trump frequently retweet each other's posts.","465aaad6":"\nDefine color settings for WordCloud. Plot the graphics for Biden and Trump's text (cleaned text) to understand the most common keywords of the messages.","cc469cc3":"# 2. Prepare\n## 2.1 Analysis Plan\nThe analysis plan is to answer the questions.\n1. What are the most common words used in Twitter messages?\n2. Which messages are the most sentimental?\n3. When to post would help the messages get more retweets and likes?\n4. Are threse posts retweets and from whom?\n5. How accurate of a model to predict the likes that the messages are going to get?\n6. How many times Biden and Trump mention each other?\n\n## Method\nThis analysis will load the 4 datasets for 4 candidates, Joe Biden, Kamala Harris, Mike Pence, and Donald Trump. The analysis steps will be as follows.\n1. Load the .json files and transform them into dataframes with needed columns\n2. Clean the text messages\n3. Plot a heatmap to easily understand when to post.\n4. Use WordCloud to plot the common keywords in the messages\n5. Plot a line chart to understand the frequency that Biden and Trump mention each other.\n6. Plot a network to know where the retweets are from\n7. Build a model to predict the level of likes and understant how accurate the model is","42b08d83":"\nTurn the raw data into dataframes. There are some zero values in the favorites column. Thus, replace these 0 values with the average number of the favorites. This approach is only applied to Biden and Trump's data because only these two dataframe will be used for generating the predicted model by the favorite count. Additionally, add a column with the word count of the text."}}