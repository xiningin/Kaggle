{"cell_type":{"af81253d":"code","26abe358":"code","749c5abf":"code","5a91ba7a":"code","f4e86a82":"code","46a7e1fa":"code","bbf37cab":"code","3ccf23b1":"code","6be768d8":"code","72d7f4a0":"code","113a70eb":"code","b49124b3":"code","1ba485ef":"code","553c469f":"code","ead1d00e":"code","5254a55c":"code","3c19c1d3":"code","b9c00218":"code","02e02f6d":"code","9ab2dcbf":"code","6d21374e":"markdown","d5b20f07":"markdown","1eb1e11b":"markdown","9ca316f9":"markdown","bb1e7609":"markdown","5dc1ea91":"markdown","06467208":"markdown","c075d350":"markdown","e823b17a":"markdown","d0fe75a3":"markdown"},"source":{"af81253d":"import numpy as np\nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport re\nimport string\n\nimport spacy\n\nimport gensim\nfrom gensim import corpora\n\n# libraries for visualization\nimport pyLDAvis\nimport pyLDAvis.gensim_models as gensimvis\npyLDAvis.enable_notebook()\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","26abe358":"review_data= pd.read_csv(\"..\/input\/amazon-fine-food-reviews\/Reviews.csv\")\nprint(review_data.head(2))\nprint(len(review_data))\nprint('Unique Products')\nprint(len(review_data.groupby('ProductId')))\nprint('Unique Users')\nprint(len(review_data.groupby('UserId')))","749c5abf":"def clean_text(text ): \n    delete_dict = {sp_character: '' for sp_character in string.punctuation} \n    delete_dict[' '] = ' ' \n    table = str.maketrans(delete_dict)\n    text1 = text.translate(table)\n    #print('cleaned:'+text1)\n    textArr= text1.split()\n    text2 = ' '.join([w for w in textArr if ( not w.isdigit() and  ( not w.isdigit() and len(w)>3))]) \n    \n    return text2.lower()","5a91ba7a":"import nltk\nnltk.download('stopwords')","f4e86a82":"review_data.dropna(axis = 0, how ='any',inplace=True) \n\nreview_data['Text'] = review_data['Text'].apply(clean_text)\nreview_data['Num_words_text'] = review_data['Text'].apply(lambda x:len(str(x).split())) \n\nprint('-------Dataset --------')\nprint(review_data['Score'].value_counts())\nprint(len(review_data))\nprint('-------------------------')\nmax_review_data_sentence_length  = review_data['Num_words_text'].max()\n\nmask = (review_data['Num_words_text'] < 100) & (review_data['Num_words_text'] >=20)\ndf_short_reviews = review_data[mask]\ndf_sampled = df_short_reviews.groupby('Score').apply(lambda x: x.sample(n=20000)).reset_index(drop = True)\n\nprint('No of Short reviews')\nprint(len(df_short_reviews))","46a7e1fa":"from nltk.corpus import stopwords\nstop_words = stopwords.words('english')\n# function to remove stopwords\ndef remove_stopwords(text):\n    textArr = text.split(' ')\n    rem_text = \" \".join([i for i in textArr if i not in stop_words])\n    return rem_text\n\n# remove stopwords from the text\ndf_sampled['Text']=df_sampled['Text'].apply(remove_stopwords)","bbf37cab":"nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n\ndef lemmatization(texts,allowed_postags=['NOUN', 'ADJ']): \n       output = []\n       for sent in texts:\n                doc = nlp(sent) \n                output.append([token.lemma_ for token in doc if token.pos_ in allowed_postags ])\n       return output","3ccf23b1":"text_list=df_sampled['Text'].tolist()\nprint(text_list[1])\ntokenized_reviews = lemmatization(text_list)\nprint(tokenized_reviews[1])","6be768d8":"dictionary = corpora.Dictionary(tokenized_reviews)\ndoc_term_matrix = [dictionary.doc2bow(rev) for rev in tokenized_reviews]","72d7f4a0":"# Creating the object for LDA model using gensim library\nLDA = gensim.models.ldamodel.LdaModel\n\n# Build LDA model\nlda_model = LDA(corpus=doc_term_matrix,\n                id2word=dictionary,\n                num_topics=10,\n                random_state=100,\n                chunksize=1000,\n                passes=50,\n                iterations=100)","113a70eb":"lda_model.print_topics()","b49124b3":"pyLDAvis.enable_notebook()\nvis = gensimvis.prepare(lda_model, doc_term_matrix, dictionary)\nvis","1ba485ef":"print('\\nPerplexity: ', lda_model.log_perplexity(doc_term_matrix,total_docs=10000))","553c469f":"from gensim.models.coherencemodel import CoherenceModel\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=tokenized_reviews, dictionary=dictionary , coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","ead1d00e":"\ndef compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n    \n    coherence_values = []\n    model_list = []\n    for num_topics in range(start, limit, step):\n        model = gensim.models.ldamodel.LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary)\n        model_list.append(model)\n        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n        coherence_values.append(coherencemodel.get_coherence())\n\n    return model_list, coherence_values","5254a55c":"model_list, coherence_values = compute_coherence_values(dictionary=dictionary,\n                                                       corpus=doc_term_matrix,\n                                                       texts=tokenized_reviews,\n                                                       start=2,\n                                                       limit=50,\n                                                       step=1)","3c19c1d3":"limit=50; start=2; step=1;\nx = range(start, limit, step)\nplt.plot(x, coherence_values)\nplt.xlabel(\"Num Topics\")\nplt.ylabel(\"Coherence score\")\nplt.legend((\"coherence_values\"), loc='best')\nplt.show() # Print the coherence scores","b9c00218":"for m, cv in zip(x, coherence_values):\n    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))","02e02f6d":"optimal_model = model_list[7]\nmodel_topics = optimal_model.show_topics(formatted=False)\noptimal_model.print_topics(num_words=10)","9ab2dcbf":"pyLDAvis.enable_notebook()\nvis = gensimvis.prepare(optimal_model, doc_term_matrix, dictionary)\nvis","6d21374e":"Filtering out reviews of length less than 100 and greater than 20. ","d5b20f07":"# **Upvote if you liked the analysis!**","1eb1e11b":"**Checking which topic is giving us the highest coherence score.**","9ca316f9":"**Topic 9 gives us the best coherence value.**","bb1e7609":"**Preprocessing**","5dc1ea91":"**Computing Coherence Score**","06467208":"**Visualizing the topics**","c075d350":"**Context**\n\nThis dataset consists of reviews of fine foods from amazon. The data span a period of more than 10 years, including all ~500,000 reviews up to October 2012. Reviews include product and user information, ratings, and a plain text review. It also includes reviews from all other Amazon categories.\n\n**Contents**\n\nReviews.csv: Pulled from the corresponding SQLite table named Reviews in database.sqlite\ndatabase.sqlite: Contains the table 'Reviews'\n\n**Data includes:**\n\nReviews from Oct 1999 - Oct 2012\n\n568,454 reviews\n\n256,059 users\n\n74,258 products\n260 users with > 50 reviews","e823b17a":"**Creating vocabulary dictionary and document term matrix**","d0fe75a3":"**LDA Topic Modelling**\n\nIn this notebook i will be demonstarting Latent Dirchlet Allocation(LDA) for topic modelling. I will be using the Amazon fine food reviews dataset from Kaggle(https:\/\/www.kaggle.com\/snap\/amazon-fine-food-reviews) for performing LDA based topic modelling I will be using the gensim package for LDA topic modelling and pyLDAvis for visualization of LDA topic model"}}