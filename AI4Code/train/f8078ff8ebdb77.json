{"cell_type":{"44025778":"code","be48d007":"code","e2c111d9":"code","ca3e76b2":"code","7620d890":"code","a23edfe4":"code","68a92bf1":"code","ad377062":"code","432e02d4":"code","cb794996":"code","526c666d":"code","13678efc":"code","182adbf9":"code","46c197fb":"code","7fb003c2":"code","e0d4de11":"code","e1293cf8":"code","d351a503":"code","1a5e718f":"code","a12d3d12":"code","f50d6f72":"code","b72a5d40":"code","2b1c98e6":"code","3fe395ab":"code","cf92f16b":"markdown","1dbbda5e":"markdown","7ce42344":"markdown","c5646489":"markdown","bc0b7530":"markdown","91cc382e":"markdown","b1d131b6":"markdown","a59eec3b":"markdown"},"source":{"44025778":"import pandas as pd\n\ndf = pd.read_csv(\"\/kaggle\/input\/pubmed-abstracts\/pubmed_abstracts.csv\", index_col=0)\ndf.head()","be48d007":"import re\n\ndef preprocess_tex(text):\n    text = text.str.lower() # lowercase\n    text = text.str.replace(r\"[^a-zA-Z\u0430-\u044f\u0410-\u042f1-9]+\", ' ') # remove () & []\n    text = text.str.replace(r\"\\#\",\"\") # replaces hashtags\n    text = text.str.replace(r\"http\\S+\",\"URL\")  # remove URL addresses\n    text = text.str.replace(r\"@\",\"\")\n    text = text.str.replace(\"\\s{2,}\", \" \")\n    return text\n\n\ncols = ['deep_learning', 'covid_19', \n        'human_connectome','virtual_reality', \n        'brain_machine_interfaces', 'electroactive_polymers', \n        'pedot_electrodes', 'neuroprosthetics']\n\ndf = df[cols].apply(lambda x: preprocess_tex(x))\ndf.head()","e2c111d9":"# Data\nlabel_names = []\n\ndl = df.deep_learning.dropna()\nlabel_names += len(dl) * [\"deep_learning\"]\ncovid_19 = df.covid_19.dropna()\nlabel_names += len(covid_19) * [\"covid_19\"]\nhuman_connectome = df.human_connectome.dropna()\nlabel_names += len(human_connectome) * [\"human_connectome\"]\nvirtual_reality = df.virtual_reality.dropna()\nlabel_names += len(virtual_reality) * [\"virtual_reality\"]\nbrain_machine_interfaces = df.brain_machine_interfaces.dropna()\nlabel_names += len(brain_machine_interfaces) * [\"brain_machine_interfaces\"]\nelectroactive_polymers = df.electroactive_polymers.dropna()\nlabel_names += len(electroactive_polymers) * [\"electroactive_polymers\"]\npedot_electrodes = df.pedot_electrodes.dropna()\nlabel_names += len(pedot_electrodes) * [\"pedot_electrodes\"]\nneuroprosthetics = df.neuroprosthetics.dropna()\nlabel_names += len(neuroprosthetics) * [\"neuroprosthetics\"]\n\n\ntext = list(pd.concat([dl, covid_19, \n                        human_connectome, virtual_reality, \n                        brain_machine_interfaces, electroactive_polymers, \n                        pedot_electrodes, neuroprosthetics]))#.unique())\n\ndf = pd.DataFrame({\"text\" : text,\n                  \"label_name\" : label_names})\n\n\n# create a new digital column matching labels\nlabels = {\"deep_learning\": 0 , \"covid_19\": 1, \n          \"human_connectome\": 2, \"virtual_reality\": 3, \n          \"brain_machine_interfaces\": 4, \"electroactive_polymers\": 5,\n          \"pedot_electrodes\": 6, \"neuroprosthetics\": 7}\n\ndf['label'] = df['label_name'].apply(lambda x: labels[x])\n\n# removing duplicates\ndf = df.drop_duplicates('text').reset_index(drop=True)\ndf","ca3e76b2":"# Plot distribution\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")\n\ndf['label_name'].value_counts().plot(kind='bar',\n                                     title=\"Distribution of data grouped by classes\", \n                                     figsize=(10, 6));\n","7620d890":"import torch\nimport torch.nn as nn\n\nfrom torchtext import data","a23edfe4":"TEXT = data.Field(tokenize = 'spacy', include_lengths = True)\nLABEL = data.LabelField(dtype = torch.float)","68a92bf1":"from sklearn.model_selection import train_test_split\n# split data into train and validation \ntrain_df, valid_df = train_test_split(df[['text', 'label']])\ntrain_df, valid_df = train_df.reset_index(drop=True), valid_df.reset_index(drop=True)","ad377062":"class DataFrameDataset(data.Dataset):\n\n    def __init__(self, df, fields, is_test=False, **kwargs):\n        examples = []\n        for i, row in df.iterrows():\n            label = row.label if not is_test else None\n            text = row.text\n            examples.append(data.Example.fromlist([text, label], fields))\n\n        super().__init__(examples, fields, **kwargs)\n\n    @staticmethod\n    def sort_key(ex):\n        return len(ex.text)\n\n    @classmethod\n    def splits(cls, fields, train_df, val_df=None, test_df=None, **kwargs):\n        train_data, val_data, test_data = (None, None, None)\n        data_field = fields\n\n        if train_df is not None:\n            train_data = cls(train_df.copy(), data_field, **kwargs)\n        if val_df is not None:\n            val_data = cls(val_df.copy(), data_field, **kwargs)\n        if test_df is not None:\n            test_data = cls(test_df.copy(), data_field, True, **kwargs)\n\n        return tuple(d for d in (train_data, val_data, test_data) if d is not None)","432e02d4":"fields = [('text',TEXT), ('label',LABEL)]\n\ntrain_ds, val_ds = DataFrameDataset.splits(fields, train_df=train_df, val_df=valid_df)","cb794996":"# Lets look at a random example\nprint(vars(train_ds[15]))\n\n# Check the type \nprint(type(train_ds[15]))","526c666d":"MAX_VOCAB_SIZE = 25000\n\nTEXT.build_vocab(train_ds, \n                 max_size = MAX_VOCAB_SIZE, \n                 vectors = 'glove.6B.200d',\n                 unk_init = torch.Tensor.zero_)","13678efc":"LABEL.build_vocab(train_ds)","182adbf9":"BATCH_SIZE = 128\n\ndevice = 'cuda:0'#torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ntrain_iterator, valid_iterator = data.BucketIterator.splits(\n    (train_ds, val_ds), \n    batch_size = BATCH_SIZE,\n    sort_within_batch = True,\n    device = device)","46c197fb":"# Hyperparameters\nnum_epochs = 20\nlearning_rate = 0.001\n\nINPUT_DIM = len(TEXT.vocab)\nEMBEDDING_DIM = 200\nHIDDEN_DIM = 256\nOUTPUT_DIM = 8\nN_LAYERS = 2\nBIDIRECTIONAL = True\nDROPOUT = 0.2\nPAD_IDX = TEXT.vocab.stoi[TEXT.pad_token] # padding","7fb003c2":"class LSTM_net(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n                 bidirectional, dropout, pad_idx):\n        super().__init__()\n        \n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n        \n        self.rnn = nn.LSTM(embedding_dim, \n                           hidden_dim, \n                           num_layers=n_layers, \n                           bidirectional=bidirectional, \n                           dropout=dropout)\n        \n        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n        \n        self.fc2 = nn.Linear(hidden_dim, 8)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, text, text_lengths):\n        \n        embedded = self.embedding(text)\n        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n        output = self.fc1(hidden)\n        output = self.dropout(self.fc2(output))\n            \n        return output","e0d4de11":"#creating instance of our LSTM_net class\nmodel = LSTM_net(INPUT_DIM, \n            EMBEDDING_DIM, \n            HIDDEN_DIM, \n            OUTPUT_DIM, \n            N_LAYERS, \n            BIDIRECTIONAL, \n            DROPOUT, \n            PAD_IDX)","e1293cf8":"pretrained_embeddings = TEXT.vocab.vectors\n\nprint(pretrained_embeddings.shape)\nmodel.embedding.weight.data.copy_(pretrained_embeddings)","d351a503":"#  to initiaise padded to zeros\nmodel.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n\nprint(model.embedding.weight.data)","1a5e718f":"model = model.cuda()\n\n\n# Loss and optimizer\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","a12d3d12":"import numpy as np\n\ndef accuracy(preds, y):\n    \"\"\"\n    Returns accuracy per batch, i.e. if you get 8\/10 right, this returns 0.8, NOT 8\n    \"\"\"\n\n    indicies = torch.argmax(preds, 1)\n\n    correct = (indicies == y).float()\n    acc = correct.sum() \/ len(correct)\n    return acc","f50d6f72":"# training function \ndef train(model, iterator):\n    \n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.train()\n    \n    for batch in iterator:\n        text, text_lengths = batch.text\n        \n        optimizer.zero_grad()\n        predictions = model(text.cuda(), text_lengths).squeeze(1)\n        loss = criterion(predictions, batch.label.long().cuda())\n        acc = accuracy(predictions, batch.label)\n\n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        \n\n    return epoch_loss \/ len(iterator), epoch_acc \/ len(iterator)","b72a5d40":"def evaluate(model, iterator):\n    \n    epoch_acc = 0\n    model.eval()\n    \n    with torch.no_grad():\n        for batch in iterator:\n            text, text_lengths = batch.text\n            predictions = model(text.cuda(), text_lengths).squeeze(1)\n            acc = accuracy(predictions.detach().cpu(), batch.label.cpu())\n            \n            epoch_acc += acc.item()\n        \n    return epoch_acc \/ len(iterator)","2b1c98e6":"import time\n\nt = time.time()\nloss=[]\nacc=[]\nval_acc=[]\n\nfor epoch in range(num_epochs):\n    \n    train_loss, train_acc = train(model, train_iterator)\n    valid_acc = evaluate(model, valid_iterator)\n    \n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\t Val. Acc: {valid_acc*100:.2f}%')\n    \n    loss.append(train_loss)\n    acc.append(train_acc)\n    val_acc.append(valid_acc)\n    \nprint(f'time:{time.time()-t:.3f}')","3fe395ab":"plt.xlabel(\"runs\")\nplt.ylabel(\"normalised measure of loss\/accuracy\")\nx_len=list(range(len(acc)))\nplt.axis([0, max(x_len), 0, 1])\nplt.title('result of LSTM')\nloss=np.asarray(loss)\/max(loss)\nplt.plot(x_len, loss, 'r.',label=\"loss\")\nplt.plot(x_len, acc, 'b.', label=\"accuracy\")\nplt.plot(x_len, val_acc, 'g.', label=\"val_accuracy\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.2)\nplt.show()","cf92f16b":"Let's tokenize all words in into indices.","1dbbda5e":"# Data Pre-Processing","7ce42344":"+ https:\/\/towardsdatascience.com\/multiclass-text-classification-using-lstm-in-pytorch-eac56baed8df\n![](https:\/\/miro.medium.com\/max\/1400\/0*pq30znWeMlVXUxxX.png)","c5646489":"# Simple neural network classifier with LSTM cells.\nThis kernel is fork of this [kernel](https:\/\/www.kaggle.com\/swarnabha\/pytorch-text-classification-torchtext-lstm).\n### Steps:\n+ Data Pre-processing\n+ RNN with LSTM","bc0b7530":"# RNN with LSTM","91cc382e":"### train model","b1d131b6":"**Rows - data**\n\n**Columns - labels**","a59eec3b":"**Field**: Field object from data module is used to specify preprocessing steps for each column in the dataset.\n\n**LabelField**: LabelField object is a special case of Field object which is used only for the classification tasks."}}