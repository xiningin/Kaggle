{"cell_type":{"15e89255":"code","a6105b5d":"code","66f780ce":"code","80d2cb27":"code","f07c2a16":"code","adce5e47":"code","8f4ce857":"code","4a4fb69c":"code","192afdd7":"code","6f016a91":"code","26cd3650":"code","3b24a010":"code","8694a3e0":"code","931b4d9b":"code","d68b14bc":"code","4f9d1c0c":"code","c7515fcc":"code","8ad87a9c":"code","ff46d6c4":"code","feb27eb7":"code","497ee27e":"code","af36c378":"code","4d0f07bd":"code","991ff4aa":"code","1e7d5fb8":"code","8157938a":"code","07b353ad":"code","10c55494":"code","53bbbcc6":"code","d2cbf667":"code","a9dad1ca":"code","aac192f6":"code","1043908e":"code","736cdec1":"code","ed242cad":"code","34aaf023":"code","61955586":"markdown","42f97184":"markdown","0ef72cd3":"markdown","a8b55762":"markdown","8bfebee7":"markdown","66dc4411":"markdown","e5fd716e":"markdown","0ed84131":"markdown","7ad2dfe4":"markdown","9c5db1a9":"markdown","602cf9f5":"markdown","60719b1d":"markdown","4f228231":"markdown","f4662547":"markdown","1b08a5fc":"markdown","55904e09":"markdown","65ef52a4":"markdown","372185b8":"markdown","3d7792f4":"markdown","951f0ee6":"markdown","b74127f8":"markdown","3adf17f5":"markdown","0512eac1":"markdown","eb77aeba":"markdown","11f653f3":"markdown","0cb527f1":"markdown","fec1c771":"markdown"},"source":{"15e89255":"import numpy as np, pandas as pd\nimport os\nimport pandas_profiling as ppf\nimport matplotlib.pyplot as plt\nimport gc","a6105b5d":"appl = pd.read_csv(\"\/kaggle\/input\/credit-card-approval-prediction\/application_record.csv\")\ncred = pd.read_csv(\"\/kaggle\/input\/credit-card-approval-prediction\/credit_record.csv\")","66f780ce":"print(appl.isna().sum().loc[appl.isna().sum() > 0])\nprint(cred.isna().sum().loc[cred.isna().sum() > 0])","80d2cb27":"appl = appl.fillna('NA')\nprint(appl.OCCUPATION_TYPE.unique())","f07c2a16":"# loan paid \/ not existing is set as -1\ncred.STATUS = cred.STATUS.replace(['C','X'], -1).astype('int8')","adce5e47":"# pivot by status\ndf = (cred.pivot_table(values='MONTHS_BALANCE', index='ID', columns='STATUS', aggfunc='count')\n          .fillna(0)\n          .assign(TOTAL_MONTHS=lambda x: x.sum(axis=1)).reset_index())\ndf.columns.name = ''\ndf.columns = ['STATUS_'+str(i) if i not in ['ID', 'TOTAL_MONTHS'] else i for i in df.columns]\n\n# join back to see if the loan already terminated\n# if latest (max) MONTHS_BALANCE != 0, then loan must have ended\ncred = (cred.groupby(['ID'], as_index=False)['MONTHS_BALANCE'].max()\n            .assign(LOAN_TERMINATED = lambda x: x.MONTHS_BALANCE<0)[['ID', 'LOAN_TERMINATED']].astype('int')\n            .merge(df, how='inner', on='ID'))\ndel df\ngc.collect()\n\ncred.sample(2)","8f4ce857":"%%capture\n\ncred_profile = ppf.ProfileReport(cred)","4a4fb69c":"cred_norm = cred.copy()\nfor i in cred_norm.columns:\n    if 'STATUS' in i:\n        cred_norm[i] = cred_norm[i] \/ cred_norm['TOTAL_MONTHS']\n        \ncred_norm.describe()","192afdd7":"%%capture\n\ncred_norm_profile = ppf.ProfileReport(cred_norm)","6f016a91":"cred_norm_profile","26cd3650":"# STATUS_2 and upwards are highly correlated - and also, signifies overdue - so, should be of concern\n\ncols_2to5    = [i for i in cred.columns if ('STATUS' in i) and (int(i.split('_')[1]) >= 2)]\nconcern_cols = cols_2to5\n\ndef check_risky_population(concern_cols, min_cutoff=0, max_cutoff=50, step=5):\n    df = pd.DataFrame(['False', 'True'], columns=['IS_RISKY'])\n\n    for i in range(min_cutoff, max_cutoff, step):\n        _ = (cred_norm.assign(IS_RISKY = lambda x: x[concern_cols].apply(sum, axis=1) > (i\/100))\n                      .groupby(['IS_RISKY'], as_index=False)['ID'].count()\n            )\n        df['at '+str(i)+'% cutoff'] = _['ID'] * 100 \/ cred_norm['ID'].count()\n\n    return df\n\ncheck_risky_population(concern_cols)","3b24a010":"concern_cols = [i for i in cred.columns if ('STATUS' in i) and (int(i.split('_')[1]) == 1)]\n\ncheck_risky_population(concern_cols)","8694a3e0":"cols = ['STATUS_1_decile', 'STATUS_-1', 'STATUS_0', 'STATUS_1', 'STATUS_2to5','ID']\n\n(cred_norm.assign(STATUS_1_decile = lambda x: x.STATUS_1.round(1))\n          .assign(STATUS_2to5 = lambda x: x[cols_2to5].apply(sum, axis=1))\n          .groupby(['STATUS_1_decile'], as_index=False).agg({i:('mean' if i!='ID' else 'count') for i in cols})\n)","931b4d9b":"concern_cols = [i for i in cred.columns if ('STATUS' in i) and (int(i.split('_')[1]) == 0)]\n\ncheck_risky_population(concern_cols, min_cutoff=50, max_cutoff=100, step=5)","d68b14bc":"STATUS_1_limit = 0.3","4f9d1c0c":"cols_4to5 = [i for i in cred.columns if ('STATUS' in i) and (int(i.split('_')[1]) >= 4)]\ncols_2to5 = cols_4to5\n\nSTATUS_1_limit = 1","c7515fcc":"cred_norm = cred_norm.assign(IS_RISKY = lambda x: (x.STATUS_1.apply(lambda y: y > STATUS_1_limit) \n                                                   + x[cols_2to5].apply(sum, axis=1) > 0\n                                                  ).astype('int8')\n                            )\n\n(cred_norm.groupby(\"IS_RISKY\", as_index=False)['ID'].count()\n          .assign(PERCENT = lambda x: x['ID'] * 100 \/ cred_norm.ID.count())\n)","8ad87a9c":"cols = ['STATUS_-1', 'STATUS_0', 'STATUS_1', 'STATUS_2to5', 'LOAN_TERMINATED', 'IS_RISKY', 'ID']\n\n(cred_norm.assign(loan_duration = pd.cut(cred_norm['TOTAL_MONTHS'],12))\n          .assign(STATUS_2to5 = lambda x: x[cols_2to5].apply(sum, axis=1))\n          .groupby(['loan_duration'], as_index=False)\n          .agg({i:(('mean' if i not in ['IS_RISKY'] else ['mean', 'sum']) \n                   if i!= 'ID' else 'count'\n                  ) for i in cols\n               })\n)","ff46d6c4":"cols = ['STATUS_-1', 'STATUS_0', 'STATUS_1', 'STATUS_2to5', 'IS_RISKY', 'TOTAL_MONTHS', 'ID']\n\n(cred_norm.assign(STATUS_2to5 = lambda x: x[cols_2to5].apply(sum, axis=1))\n          .groupby(['LOAN_TERMINATED'], as_index=False)\n          .agg({i:('mean' if i!= 'ID' else 'count') for i in cols\n               })\n)","feb27eb7":"print(len(appl), len(appl.ID.unique()))\nprint(len(cred_norm), len(cred_norm.ID.unique()))\nprint(f\"# of IDs present in both: {len(set(appl.ID).intersection(set(cred_norm.ID)))}\")","497ee27e":"df = appl.merge(cred_norm[['ID', 'IS_RISKY']], how='inner', on='ID')\n\nassert len(df) == len(df.ID.unique())\ndf.sample(2)","af36c378":"%%capture\ndf_profile = ppf.ProfileReport(df)","4d0f07bd":"df_profile","991ff4aa":"df['Age'] = (df['DAYS_BIRTH'] * -1 \/ 365).round(1)\ndf['CNT_FAM_MEMBERS_MINUS_CHILDREN'] = df['CNT_FAM_MEMBERS'] - df['CNT_CHILDREN']\ndf['Income_Age_ratio'] = df['AMT_INCOME_TOTAL'] \/ df['Age']\ndf['Is_Unemployed'] = (df['DAYS_EMPLOYED'] > 0).astype('int8')\ndf.loc[df.DAYS_EMPLOYED > 0, 'DAYS_EMPLOYED'] = 0\ndf['Years_Employed'] = (df['DAYS_EMPLOYED'] * -1 \/ 365).round(1)\ndf['Employed_Age_ratio'] = df['Years_Employed'] \/ df['Age']\ndf['Employed_Income_ratio'] = df['Years_Employed'] \/ df['AMT_INCOME_TOTAL']\n\n\ndf =  df.drop(columns=['FLAG_MOBIL', 'DAYS_BIRTH', 'CNT_FAM_MEMBERS', 'DAYS_EMPLOYED'])","1e7d5fb8":"(df.groupby(['Is_Unemployed','OCCUPATION_TYPE'], as_index=False)\n   .agg({'ID':'count', 'IS_RISKY':'mean'})\n   .sort_values(by=['IS_RISKY'], ascending=False)\n)","8157938a":"df.loc[df.Is_Unemployed == 1, 'OCCUPATION_TYPE'] = 'Unemployed'\ndf = df.drop(columns=['Is_Unemployed'])","07b353ad":"(df.groupby(['OCCUPATION_TYPE','FLAG_WORK_PHONE'], as_index=False)\n   .agg({'ID':'count', 'IS_RISKY':'mean'})\n   .sort_values(by=['IS_RISKY'], ascending=False)\n)","10c55494":"def sort_by_risk(x): \n    return (df.groupby(x, as_index=False)\n              .agg({'ID':'count', 'IS_RISKY':'mean'})\n              .sort_values(by=['IS_RISKY'], ascending=False)\n           )\n\nsort_by_risk(['NAME_INCOME_TYPE'])","53bbbcc6":"print(df.shape, df.loc[df.NAME_INCOME_TYPE != 'Student', :].shape)\ndf = df.loc[df.NAME_INCOME_TYPE != 'Student', :]","d2cbf667":"sort_by_risk(['NAME_EDUCATION_TYPE'])","a9dad1ca":"print(df.shape, df.loc[df.NAME_EDUCATION_TYPE != 'Academic degree', :].shape)\ndf = df.loc[df.NAME_EDUCATION_TYPE != 'Academic degree', :]","aac192f6":"sort_by_risk(['NAME_FAMILY_STATUS'])","1043908e":"sort_by_risk(['NAME_HOUSING_TYPE'])","736cdec1":"%%capture\ndf_profile = ppf.ProfileReport(df)","ed242cad":"df_profile","34aaf023":"df.to_csv(\"fc_cc_data_v1.csv\", index=False)","61955586":"The same argument can be followed here as well","42f97184":"The profiling report below captures the behaviour described above with even more detail\n\nLooking at correlation, we see that (as expected) statuses `2`, `3`, `4`, `5` are very highly correlated as well\nAlthough `STATUS_2` shares a medium correlation with `STATUS_1` as well\n\nThis gives credence to the idea that we should dock points for having too many `STATUS_1`\n\n**In a proper business context, this needs to be decided after looking into the economic impact and\/or consulting with the business knowldege of the client**\n\nHere, we will go ahead with a bit of analysis paired with our own rationale","0ef72cd3":"Also, we see that at the `0.3` decile of `STATUS_1` the average % for being in statuses `>=2` shoots up to `2.4%` - while `STATUS_-1` (i.e. no debt) is down to `17%` \n\n> Now, based on this, if we want to more **conservative**, we could choose `0.5` as the cut-off as well - because there `STATUS_-1` reaches just `10%`\n\nBut, with increasing decile, the corresponding count also decreases quite rapidly","a8b55762":"Let's just look into the data profile now","8bfebee7":"**iteration 2 note:**\n\nSadly, when we model the data - the performance becomes incredibly flaky - the good performance in CV and validation doesn't carry forward to test\n\nOne reason might be that we have been incredibly loose with the definition of bad customer - so, a lot of the customers might have been termed as bad although they share a very similar profile to good ones\n\nTo test for that quickly, we will just restrict our criterion right now - only to statuses `>=4` - and see how that affects!","66dc4411":"We should probably drop the `Student` here - since it's so few - and all of them are `0`s\n\nAlthough this needs a discussion with client to see if they are aligned to this (for any number of business reasons)","e5fd716e":"Now, let's actually look at the insection of people who are unemployed, and people for whom we had `NA` as `OCCUPATION_TYPE`\n\nWell, all the people termed as **Unemployed** actually have `NA` as their occupation\n\nSo, we can get rid of the `Is_Unemployed` feature and replace the `NA` for them with `Unemployed`\n\nAlso, ordering by `IS_RISKY` % gives us an indicaton about encoding these categorical values\n> Although interestingly `IT staff` seems to be most risky - but, that might be the effect of the small sample size","0ed84131":"Now, We calculate the count of each `STATUS` for each of the customers\n> this information will be used to see how many times a customer fall into a `STATUS` that's not worthwhile","7ad2dfe4":"### Finalizing the response variable\n\nhere, the label is not given, we are going to analyze the customer's behaviour post loan sanction to derive the label of whether the loan should have been given or not\n\n> here, we replace the values `C` and `X` that signify either a paid loan or no loan for the month - with value `-1` - so that everything can be turned `int`","9c5db1a9":"### Joining the label to the application data\n\nOnce we use the credit information to decide whether a customer is risky or not, we need to actually join it to the application data to prepare the dataset for using in our ML model","602cf9f5":"So, finally we are gonna go ahead with \n> customers who have either `>=1` instances with statuses `>=2`\n\n> or, customers who have `>=30%` of their total months as `STATUS_1`\n\nAnd in total we get `2.03%` as risky!\n\n> btw this again needs to be validated with business \/ domain knowledge","60719b1d":"#### Issues to be careful about\n\nThe data has `GENDER` as a variable - and this raises a very important issue : [NY Times reporting on DHH's tweet on Apple Card's gender bias](https:\/\/www.nytimes.com\/2019\/11\/10\/business\/Apple-credit-card-investigation.html)\n\nAnd this may warrant a discussion with the stakeholders before deploying a model in the real world.\nBut, for now, we will just **note** this as something to look more into later on!","4f228231":"**Finally, a further data profile with all the new features**\n\nWe see that the newly created features (from income) are highly correlated with income - for the obvious reason\nBut, they capture different facets of income - which is why they are created in the first place!\n\nThe same goes for features created from years employed","f4662547":"<s>**iteration 3 note:**\n\niteration 2 shows that status 4 and 5 are indeed better as labels - because we see a well enough performance transfer for valid to test\n\nLet's try and include `status 3` as well - and see how that carries over <\/s>","1b08a5fc":"Now, let's quickly check what impact `TOTAL_MONTHS` i.e the duration of the loan has\n\nInterestingly, with increasing loan duration, the proportion of `STATUS_-1` increases - while that for `STATUS_0` decreases\n> although their `sum` holds to be pretty stable\n\nAnd as expected, the proportion of `STATUS_2to5` increases - leading to increase in `IS_RISKY` as well!\n> this can be used to set separate criterion (by duration) for `IS_RISKY` as well\n\n*sidenote:* `LOAN_TERMINATED` as expected is inversely related to the loan duration","55904e09":"So, after we normalize the counts in each status by the TOTAL_MONTHS, we see that 52% of all statuses are in `-1` i.e. either **paid off or no loan** - this is obviously the customers we want\n\nAnd, 46% are in `0` - which is **1-29 days past due** - so, this is very common - but, we may still want to limit something like this\n\n1.2% are in `1` - which is **30-59 days past due** - this seems to be the anomaly territory we want to avoid\n\nThe rest are are overdues for longer duration or outright writeoff - given their relative low frequency and propensity for loss we may want to outright tag the customers with these behaviour as anomalies","65ef52a4":"Let's see how many customers are having statuses `>1` - overdure more than 60 days\n\n> we calculate the %age of statues `>1` normalized by total duration \n\nCounting every customer with atleast 1 status of `2 or higher` gives `1.45%` risky population","372185b8":"Actually, we can just replace the `nan` values with `'NA'` here - and use it as a new category","3d7792f4":"Let's check the same for other categorical features as well","951f0ee6":"Let's just check first for the overlap in these 2 datasets\n\nSo, we have duplicate `ID`s in the application data - this might be because the same customer has submitted multiple applications\n> or it might be genuine duplicate data that needs to be removed\n\nBut, given the overlap between the application data and credit record data is only `36K` - let's just **merge** them first before looking into the duplicates","b74127f8":"Ok, so, `FLAG_MOBIL` has only a single *unique* value - so, it can be dropped easily\n\nLet's also convert `DAYS_BIRTH` to age (in years) - and round it up to one decimal place\n\nWe also see that `CNT_CHILDREN` and `CNT_FAMILY_MEMBERS` are highly correlated - instead of dropping one, one easy way to decorrelate them would be to create a new feature that tracks the number of family members excluding children\n> although there still might be correlation, if it's likely that people with larger families also have more children\n\nLooking at the ratio of income and age also should be a good indicator of financial wellness\n\n`DAYS_EMPLOYED > 0` means currently unemployed - let's create a feature for this\nAnd then let's also convert it to years\n\nPlus, the ratio of income and working years, and ratio of working years and age can be interesting to look at too","3adf17f5":"Let's do the same for `STATUS_0` - 1-29 days past due\n\nAnd as we had seen before (`STATUS_0` had 46% of all months) - this is very **common** scenario indeed","0512eac1":"And once we merge the datasets (inner join), we see that the duplicate issue is gone","eb77aeba":"Let's quickly check if we have any missing data\n\nAnd we have missing values only in the `OCCUPATION_TYPE` in application data","11f653f3":"Next, we look at the interaction between `OCCUPATION_TYPE` and `FLAG_WORK_PHONE`\n> the idea being - a job without a work phone might be an indication of job quality\n\nAnd indeed we see that `IT staff` without work phone is one of the **most risky** one here!","0cb527f1":"Finally, let's have a quick look at `LOAN_TERMINATED` (i.e. loans that aren't updated in the recent months)\n\nInterestingly, we see that loans that are still continuing generally have a higher `STATUS_2to5` - consequently higher `IS_RISKY`\n> this information can be harnessed while deciding the rules for a loan being risky or not, as well","fec1c771":"So, let's look a bit more into `STATUS_1` - 30-59 days past due (on average shows up 1.2% of all the months)\n> it also shows medium correlation with `STATUS_2`\n\nWe look at what happens if we classify everyone with `> 0.x` percentage of `STATUS_1`s\n\nDeeming people with more than `30%` of months in `STATUS_1` gives further `0.68%` (including overlaps)"}}