{"cell_type":{"66e41de5":"code","4521e801":"code","5f4f44fa":"code","fcced5aa":"code","e444acbe":"code","f35f676b":"code","d14ace36":"code","371437c5":"code","88772274":"code","cee18f1d":"code","8d2c76d4":"code","f65f23b4":"code","e313d26a":"code","1f47947f":"code","d7677ae7":"code","1de1aaed":"code","2571d21a":"code","a8e0717b":"code","7a17b960":"code","2597b64c":"code","81905912":"code","2e2935e8":"code","471760b8":"code","1917c411":"code","9ccad189":"code","2a9e497f":"code","8dae13e0":"code","97d62752":"code","ad06ba08":"code","64753e8f":"code","fdd970db":"markdown","9af70569":"markdown","05a8a7ee":"markdown","264a15b2":"markdown","645ccc89":"markdown","81cfb06b":"markdown","6d1d6418":"markdown","05ae3039":"markdown","1e20330c":"markdown","04e1a364":"markdown","031bc475":"markdown","b9d4d48c":"markdown","4b0c1015":"markdown","dd3e2456":"markdown","1cca764a":"markdown","068e73da":"markdown","d958085e":"markdown","ed9b0963":"markdown"},"source":{"66e41de5":"! pip install pytorch-tabnet # if not installed","4521e801":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\n\nimport pytorch_tabnet\nfrom pytorch_tabnet.tab_model import TabNetClassifier\nimport torch\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import roc_auc_score, accuracy_score\n\nrandom = 200\ntorch.manual_seed(random)\n\ntrain = '..\/input\/tabular-playground-series-sep-2021\/train.csv'\ntest = '..\/input\/tabular-playground-series-sep-2021\/test.csv'","5f4f44fa":"train_df = pd.read_csv(train)\ntest_df = pd.read_csv(test)","fcced5aa":"display(train_df.head())\n\n# Lets see the null values\nsum_na = train_df.isna().sum().sum()\nlength = len(train_df)\nprint(f'Total Nan values {sum_na} out of {length} : {sum_na\/length:.2}%')","e444acbe":"train_df.pop('id')\ntest_id = test_df.pop('id')","f35f676b":"train_df.hist(figsize=(30,30), bins=40, xlabelsize=0, ylabelsize=0, color='#cf1f1f')\nplt.show()","d14ace36":"import seaborn as sns\n\ntrain_corr = train_df.corr()\ntrain_mask = np.triu(np.ones_like(train_corr, dtype=bool))\n\nfig = plt.figure(figsize=(16, 16))\n\ntrain_corr1 = train_corr[train_corr > 0.003]\nsns.heatmap(train_corr, \n            square=True, \n            mask=train_mask,\n            annot=False,\n            cmap=plt.cm.Reds\n           )","371437c5":"cor_target = abs(train_corr[\"claim\"])\n\nrelevant_features = cor_target[cor_target>0.003]\nrelevant_features","88772274":"relevent_train = train_df.loc[:, relevant_features.index]\nrelevent_train.head()","cee18f1d":"y = relevent_train['claim']\nX = relevent_train.drop('claim', axis=1)\n\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X, y, \n    test_size=0.3,\n    train_size=0.7, \n    shuffle=True,\n    random_state=random)","8d2c76d4":"pd.options.mode.chained_assignment = None\n\nf = [x for x in X_train.columns.values if x[0]==\"f\"]\n\nX_train['missing'] = X_train.loc[:,f].isna().sum(axis=1)\nX_train['abs_sum'] = X_train.loc[:,f].abs().sum(axis=1)\nX_train['median'] = X_train.loc[:,f].median(axis=1)\nX_train['var'] = X_train.loc[:,f].var(axis=1)\nX_train['std'] = X_train.loc[:,f].std(axis=1)\nX_train['mean'] = X_train.loc[:,f].mean(axis=1)\nX_train['max'] = X_train.loc[:,f].max(axis=1)\nX_train['min'] = X_train.loc[:,f].min(axis=1)\n\nX_train.head()","f65f23b4":"X_valid['missing'] = X_valid.loc[:,f].isna().sum(axis=1)\nX_valid['abs_sum'] = X_valid.loc[:,f].abs().sum(axis=1)\nX_valid['median'] = X_valid.loc[:,f].median(axis=1)\nX_valid['var'] = X_valid.loc[:,f].var(axis=1)\nX_valid['std'] = X_valid.loc[:,f].std(axis=1)\nX_valid['mean'] = X_valid.loc[:,f].mean(axis=1)\nX_valid['max'] = X_valid.loc[:,f].max(axis=1)\nX_valid['min'] = X_valid.loc[:,f].min(axis=1)\n\npd.options.mode.chained_assignment = 'warn'\nX_valid.head()","e313d26a":"my_imputer = SimpleImputer(strategy=\"median\")\ndef impute(X_t, X_v):\n    return pd.DataFrame(my_imputer.fit_transform(X_t)), pd.DataFrame(my_imputer.transform(X_v))\n\nX_train_imp, X_val_imp = impute(X_train, X_valid)","1f47947f":"from sklearn.preprocessing import RobustScaler\n\ndef robust_scale(X_t, X_v):\n    scaler = RobustScaler()\n    \n    return pd.DataFrame(scaler.fit_transform(X_t)), pd.DataFrame(scaler.transform(X_v))\n\nX_train_imp_st, X_val_imp_st = robust_scale(X_train_imp, X_val_imp)","d7677ae7":"display(X_train_imp_st.head())\n\n# Lets see the null values\nna_sum = X_train_imp_st.isna().sum().sum()\nlength = len(X_train_imp_st)\nprint(f'Nan: {na_sum} out of {length} : {na_sum\/length:.2}%')","1de1aaed":"from sklearn.preprocessing import QuantileTransformer\n\ntrans = QuantileTransformer(n_quantiles=64, output_distribution='normal')","2571d21a":"from sklearn.preprocessing import KBinsDiscretizer\n\nkbin = KBinsDiscretizer(n_bins=64, encode='ordinal',strategy='uniform')","a8e0717b":"from sklearn.pipeline import Pipeline\n\npipe = Pipeline([\n    (\"scaler\", trans),\n    (\"binning\", kbin)\n])\n\ntrain_final = pd.DataFrame(pipe.fit_transform(X_train_imp_st))\nval_final = pd.DataFrame(pipe.transform(X_val_imp_st))","7a17b960":"train_final.hist(figsize=(15,10), bins=64, color='#cf1f1f')\nplt.show()","2597b64c":"val_final.hist(figsize=(15,10), bins=64, color='#cf1f1f')\nplt.show()","81905912":"Xtrain = train_final.to_numpy()\nXvalid = val_final.to_numpy()","2e2935e8":"model1 = TabNetClassifier(\n    optimizer_fn=torch.optim.Adam,\n    optimizer_params=dict(lr=2e-2),\n    scheduler_params={\"step_size\":5,\"gamma\":0.9},\n    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n    mask_type='entmax'\n)          ","471760b8":"model1.fit(\n    Xtrain, y_train,\n    eval_set=[(Xtrain, y_train), (Xvalid, y_valid)],\n    eval_name=['train', 'valid'],\n    eval_metric=['auc'],\n    max_epochs=15, patience=10,\n    batch_size=256, virtual_batch_size=128,\n    num_workers=0,\n    weights=1,\n    drop_last=False\n)","1917c411":"feature = [f for f in relevant_features.index if 'f' in f]\n\nrelevent_test = test_df.loc[:, feature]\nrelevent_test.head()","9ccad189":"relevent_test['missing'] = relevent_test.loc[:,f].isna().sum(axis=1)\nrelevent_test['abs_sum'] = relevent_test.loc[:,f].abs().sum(axis=1)\nrelevent_test['median'] = relevent_test.loc[:,f].median(axis=1)\nrelevent_test['var'] = relevent_test.loc[:,f].var(axis=1)\nrelevent_test['std'] = relevent_test.loc[:,f].std(axis=1)\nrelevent_test['mean'] = relevent_test.loc[:,f].mean(axis=1)\nrelevent_test['max'] = relevent_test.loc[:,f].max(axis=1)\nrelevent_test['min'] = relevent_test.loc[:,f].min(axis=1)","2a9e497f":"my_imputer = SimpleImputer(strategy=\"median\")\ntest_imp = pd.DataFrame(my_imputer.fit_transform(relevent_test))","8dae13e0":"scaler = RobustScaler()\nX_test = pd.DataFrame(scaler.fit_transform(test_imp))","97d62752":"trans = QuantileTransformer(n_quantiles=64, output_distribution='normal')\nkbin = KBinsDiscretizer(n_bins=64, encode='ordinal',strategy='uniform')\n\npipe = Pipeline([\n    (\"scaler\", trans),\n    (\"binning\", kbin)\n])\n\ntest_final = pd.DataFrame(pipe.fit_transform(X_test))","ad06ba08":"preds = model1.predict(test_final.to_numpy())","64753e8f":"df = pd.DataFrame({\n    'id': test_id,\n    'claim': preds\n})\n\ndf = df.set_index('id')\ndf.to_csv('final.csv')","fdd970db":"### Impute \nI'll use **median** because of the reasons stated previously.","9af70569":"### Data Smoothing \nThere are 3 techniques to smooth data: \n* Binning\n* Regression \n* Outlier analysis   \n\n[This](https:\/\/machinelearningmastery.com\/discretization-transforms-for-machine-learning\/) was helpful. I'll be using Binning technique. (o\u2032\u250f\u25bd\u2513\uff40o) ","05a8a7ee":"# Feature Selection\nThere are multiple ways to select features:  \n* Filter Method\n* Wrapper Method\n* Embedded Method \n\nIn this notebook we will use the filter method","264a15b2":"# Model","645ccc89":"## Supervised","81cfb06b":"It looks like we have Nan values **(\u00b4\u3002\uff3f\u3002\uff40)**. There are multiple approaches to deal with missing values \n* Drop Columns with Missing Values\n* Imputation \n* Interpolate\n\nWe will try both Imputation and Interpolation.  \nAnother thing to note here, lets get rid of the id because it creates [**data leakage.**](https:\/\/www.kaggle.com\/alexisbcook\/data-leakage) \n\n> *\u201c if any other feature whose value would not actually be available in practice at the time you\u2019d want to use the model to make a prediction, is a feature that can introduce leakage to your model \u201d* ~ **Data Skeptic**","6d1d6418":"When viewing different feature distribution, you could see that they are mostly **skewed**. We cannot use the **mean** when imputing, because our features are not symmetric. So we either replace with **median** or **mode**. [This article maybe useful to you](https:\/\/vitalflux.com\/pandas-impute-missing-values-mean-median-mode\/), it was useful to me.","05ae3039":"### Add Features","1e20330c":"# Imports \n(\uff89\u25d5\u30ee\u25d5)\uff89*:\uff65\uff9f\u2727 Lets do dis ..  \n\n## Content\n1. [Exploring data](#Exploring-data)\n2. [Feature Selection](#Feature-Selection)\n3. [Preprocessing](#Preprocessing)\n    1. [Train valid split](#Train-valid-split)\n    2. [Add Features](#Add-Features)\n    3. [Missing values](#Impute)\n    4. [Scaler](#Scaler)\n    5. [Data Smoothing](#Data-Smoothing)\n4. [Define Model](#Define-Model)\n    1. [TabNet Supervised](#Supervised)\n5. [Submit](#Submit)","04e1a364":"### Train valid split","031bc475":"# Preprocessing \n\nOne thing to note here, if we are planing to impute or add features to our data, we need to first split it into train and val, to avoid [**data leakage**](https:\/\/www.kaggle.com\/alexisbcook\/data-leakage). Then deal with them separately.  ","b9d4d48c":"### Fix Skewness\nWe will use quantile to automatically transfer our numeric inputs to have a standard probability distribution, [this](https:\/\/machinelearningmastery.com\/quantile-transforms-for-machine-learning\/) post was really helpful at understanding the use of Quantile Transforms and why it's useful.","4b0c1015":"# Exploring data ","dd3e2456":"### Scaler\n[This](https:\/\/stackoverflow.com\/questions\/51841506\/data-standardization-vs-normalization-vs-robust-scaler) helped me decide on which preprocessing approach is better.","1cca764a":"# Submit","068e73da":"## Define Model","d958085e":"We will pick the features that have a correlation higher than 0.003 with our target feature **o(\\*\uffe3\u25bd\uffe3\\*)\u30d6**","ed9b0963":"# Imports"}}