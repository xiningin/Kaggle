{"cell_type":{"ac4856c0":"code","5731638b":"code","70079c62":"code","83161182":"code","9e6d01d5":"code","a6fa9f34":"code","f25eb931":"code","e31d6f14":"code","622e0f1c":"code","91c37cdb":"code","cc8fe856":"code","2c81b5b7":"code","feca3975":"code","f6b73cbd":"code","64bab026":"code","6467ac90":"code","469ef53b":"code","dc178fb9":"code","d19cf2a5":"code","761b3750":"code","e94ec3f6":"code","eff495bc":"code","515f46f3":"code","5e29419d":"code","961e8540":"code","c570ef82":"code","1feb18a4":"code","e15566eb":"code","97095dae":"code","f6f2ee8e":"code","6f0b2df3":"code","4f560d32":"code","7dec921d":"code","7a2dbffb":"code","9d46cd66":"code","745b2c32":"code","7d3cb6c5":"code","071106c7":"code","02f9af54":"code","a0e88455":"markdown","8d02ca8d":"markdown","444beb33":"markdown","e48a3313":"markdown","598a6646":"markdown","b3616d49":"markdown","59719671":"markdown","1dd9b3c4":"markdown","4ea807f8":"markdown","b8ba7a4b":"markdown","9109df7c":"markdown","d4172add":"markdown","9de4f069":"markdown","636b9d42":"markdown"},"source":{"ac4856c0":"import pandas as pd","5731638b":"\nfilepath_dict = {'yelp':   '..\/input\/sentiment-labelled-sentences-data-set\/sentiment labelled sentences\/yelp_labelled.txt',\n                 'amazon': '..\/input\/sentiment-labelled-sentences-data-set\/sentiment labelled sentences\/amazon_cells_labelled.txt',\n                 'imdb':   '..\/input\/sentiment-labelled-sentences-data-set\/sentiment labelled sentences\/imdb_labelled.txt'}\n\ndf_list = []\nfor source, filepath in filepath_dict.items():\n    df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n    df['source'] = source  # Add another column filled with the source name\n    df_list.append(df)\n\ndf = pd.concat(df_list)\ndf.head()","70079c62":"df.shape","83161182":"df.info()","9e6d01d5":"df.describe()","a6fa9f34":"print(df.iloc[0])\n","f25eb931":"sentences = ['John likes ice cream', 'John hates chocolate.']","e31d6f14":"from sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer(min_df=0, lowercase=False)\nvectorizer.fit(sentences)\nvectorizer.vocabulary_\n","622e0f1c":"vectorizer.transform(sentences).toarray()\n","91c37cdb":"from sklearn.model_selection import train_test_split\n\ndf_yelp = df[df['source'] == 'yelp']\n\nsentences = df_yelp['sentence'].values\ny = df_yelp['label'].values\n\nsentences_train, sentences_test, y_train, y_test = train_test_split(\n   sentences, y, test_size=0.25, random_state=1000)","cc8fe856":"from sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer()\nvectorizer.fit(sentences_train)\n\nX_train = vectorizer.transform(sentences_train)\nX_test  = vectorizer.transform(sentences_test)\n","2c81b5b7":"X_train\n","feca3975":"from sklearn.linear_model import LogisticRegression\n\nclassifier = LogisticRegression()\nclassifier.fit(X_train, y_train)\nscore = classifier.score(X_test, y_test)\n\nprint(\"Accuracy:\", score)\n","f6b73cbd":"for source in df['source'].unique():\n    df_source = df[df['source'] == source]\n    sentences = df_source['sentence'].values\n    y = df_source['label'].values\n\n    sentences_train, sentences_test, y_train, y_test = train_test_split(\n        sentences, y, test_size=0.25, random_state=1000)\n\n    vectorizer = CountVectorizer()\n    vectorizer.fit(sentences_train)\n    X_train = vectorizer.transform(sentences_train)\n    X_test  = vectorizer.transform(sentences_test)\n\n    classifier = LogisticRegression()\n    classifier.fit(X_train, y_train)\n    score = classifier.score(X_test, y_test)\n    print('Accuracy for {} data: {:.4f}'.format(source, score))","64bab026":"from keras.models import Sequential\nfrom keras import layers\n\ninput_dim = X_train.shape[1]  # Number of features\n\nmodel = Sequential()\nmodel.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))","6467ac90":"model.compile(loss='binary_crossentropy', \n              optimizer='adam', \n              metrics=['accuracy'])\nmodel.summary()","469ef53b":"history = model.fit(X_train, y_train,\n                    epochs=100,\n                    verbose=False,\n                    validation_data=(X_test, y_test),\n                    batch_size=10)","dc178fb9":"loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test, y_test, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))","d19cf2a5":"import matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\ndef plot_history(history):\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(acc) + 1)\n\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, acc, 'b', label='Training acc')\n    plt.plot(x, val_acc, 'r', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()\n    ","761b3750":"plot_history(history)","e94ec3f6":"cities = ['London', 'Berlin', 'Berlin', 'New York', 'London']\ncities\n","eff495bc":"from sklearn.preprocessing import LabelEncoder\n\nencoder = LabelEncoder()\ncity_labels = encoder.fit_transform(cities)\ncity_labels","515f46f3":"from sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder(sparse=False)\ncity_labels = city_labels.reshape((5, 1))\nencoder.fit_transform(city_labels)","5e29419d":"from keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(sentences_train)\n\nX_train = tokenizer.texts_to_sequences(sentences_train)\nX_test = tokenizer.texts_to_sequences(sentences_test)\n\nvocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n\nprint(sentences_train[2])\nprint(X_train[2])","961e8540":"# print(tokenizer.word_index)","c570ef82":"for word in ['the', 'all', 'love', 'wonderful']:\n    print('{}: {}'.format(word, tokenizer.word_index[word]))","1feb18a4":"from keras.preprocessing.sequence import pad_sequences\n\nmaxlen = 100\n\nX_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\nX_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n\nprint(X_train[0, :])","e15566eb":"from keras.models import Sequential\nfrom keras import layers\n\nembedding_dim = 50\n\nmodel = Sequential()\nmodel.add(layers.Embedding(input_dim=vocab_size, \n                           output_dim=embedding_dim, \n                           input_length=maxlen))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(10, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()","97095dae":"history = model.fit(X_train, y_train,\n                    epochs=20,\n                    verbose=False,\n                    validation_data=(X_test, y_test),\n                    batch_size=10)\nloss, accuracy = model.evaluate(X_train, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test, y_test, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))\nplot_history(history)","f6f2ee8e":"from keras.models import Sequential\nfrom keras import layers\n\nembedding_dim = 50\n\nmodel = Sequential()\nmodel.add(layers.Embedding(input_dim=vocab_size, \n                           output_dim=embedding_dim, \n                           input_length=maxlen))\nmodel.add(layers.GlobalMaxPool1D())\nmodel.add(layers.Dense(10, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()","6f0b2df3":"history = model.fit(X_train, y_train,\n                    epochs=50,\n                    verbose=False,\n                    validation_data=(X_test, y_test),\n                    batch_size=10)\nloss, accuracy = model.evaluate(X_train, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test, y_test, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))\nplot_history(history)","4f560d32":"\nimport numpy as np\n\ndef create_embedding_matrix(filepath, word_index, embedding_dim):\n    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n\n    with open(filepath) as f:\n        for line in f:\n            word, *vector = line.split()\n            if word in word_index:\n                idx = word_index[word] \n                embedding_matrix[idx] = np.array(\n                    vector, dtype=np.float32)[:embedding_dim]\n\n    return embedding_matrix","7dec921d":"embedding_dim = 50\nembedding_matrix = create_embedding_matrix(\n    '..\/input\/glove6b50dtxt\/glove.6B.50d.txt',\n    tokenizer.word_index, embedding_dim)","7a2dbffb":"embedding_matrix","9d46cd66":"nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\nnonzero_elements \/ vocab_size","745b2c32":"model = Sequential()\nmodel.add(layers.Embedding(vocab_size, embedding_dim, \n                           weights=[embedding_matrix], \n                           input_length=maxlen, \n                           trainable=False))\nmodel.add(layers.GlobalMaxPool1D())\nmodel.add(layers.Dense(10, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()","7d3cb6c5":"history = model.fit(X_train, y_train,\n                    epochs=50,\n                    verbose=False,\n                    validation_data=(X_test, y_test),\n                    batch_size=10)\nloss, accuracy = model.evaluate(X_train, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test, y_test, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))\nplot_history(history)","071106c7":"model = Sequential()\nmodel.add(layers.Embedding(vocab_size, embedding_dim, \n                           weights=[embedding_matrix], \n                           input_length=maxlen, \n                           trainable=True))\nmodel.add(layers.GlobalMaxPool1D())\nmodel.add(layers.Dense(10, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()","02f9af54":"history = model.fit(X_train, y_train,\n                    epochs=50,\n                    verbose=False,\n                    validation_data=(X_test, y_test),\n                    batch_size=10)\nloss, accuracy = model.evaluate(X_train, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test, y_test, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))\nplot_history(history)","a0e88455":"![](https:\/\/databricks.com\/wp-content\/uploads\/2019\/01\/deep-learning.jpg)","8d02ca8d":"#### Accuracy","444beb33":"## Read Datasets and Check out the Data","e48a3313":"This repository is an excellent source of machine learning data that can be used to test algorithms. The dataset includes reviews tagged from the IMDB, Amazon and Yelp websites. Each review has a score from 0 (for negative emotions) to 1 (for positive emotions). Now you need to extract the existing folder in the data folder and then load the data using the Pandas library.","598a6646":"\n## TensorFlow & Keras\nTensorFlow is an end-to-end open-source platform for machine learning. It\u2019s a comprehensive and flexible ecosystem of tools, libraries and other resources that provide workflows with high-level APIs. The framework offers various levels of concepts for you to choose the one you need to build and deploy machine learning models.\n\n**For instance**, if you need to do some large machine learning tasks, you can use the Distribution Strategy API in order to perform distributed hardware configurations and if you need a full production machine learning pipeline, you can simply use TensorFlow Extended (TFX). Some of the salient features are described below:\n\n**Easy Model Building:** TensorFlow offers multiple levels of abstraction to build and train models. \n**Robust ML Production Anywhere:** TensorFlow lets you train and deploy your model easily, no matter what language or platform you use. \n**Powerful Experimentation For Research:** TensorFlow gives you flexibility and control with features like the Keras Functional API and Model Subclassing API for the creation of complex topologies.\nKeras, on the other hand, is a high-level neural networks library that is running on the top of TensorFlow, CNTK, and Theano. Using Keras in deep learning allows for easy and fast prototyping as well as running seamlessly on CPU and GPU. This framework is written in Python code which is easy to debug and allows ease for extensibility. The main advantages of Keras are described below:\n\n\n**User-Friendly:** Keras has a simple, consistent interface optimized for common use cases which provides clear and actionable feedback for user errors.\n**Modular and Composable:** Keras models are made by connecting configurable building blocks together, with few restrictions.\n\n**Easy To Extend:** With the help of Keras, you can easily write custom building blocks for new ideas and researches.\n\n**Easy To Use:** Keras offers consistent & simple APIs which helps in minimizing the number of user actions required for common use cases, also it provides clear and actionable feedback upon user error.","b3616d49":"#### Accuracy","59719671":"### Making a model with keras","1dd9b3c4":"## Import Libraries","4ea807f8":" It is used to transform a given text into a vector on the basis of the frequency (count) of each word that occurs in the entire text.","b8ba7a4b":"### Introduction","9109df7c":"### set optimizer and loss function ","d4172add":"### use CountVectorizer  from scikit-learn ","9de4f069":"### LogisticRegression Model","636b9d42":"## Deep Learning \nis a subset of machine learning concerned with large amounts of data with algorithms that have been inspired by the structure and function of the human brain, which is why deep learning models are often referred to as deep neural networks.\n\n#### How Does Deep Learning Work?\nIn deep learning, a computer model learns to perform classification tasks directly from images, text, or sound. It performs a task repeatedly, making a little tweak to improve the outcome.\n\nThe most important part of a Deep Learning neural network is a layer of computational nodes called \u201cneurons\u201d. Every neuron connects to all of the neurons in the underlying layer. Due to \u201cdeep learning\u201d the neural network leverages at least two hidden layers. The addition of the hidden layers enables researchers to make more in-depth calculations. How does the algorithm work then? The thing is, each connection has its weight or importance.but, with the help of the deep neural networks we can automatically find out the most important features for classification. This is performed with the help of the Activation Function that evaluates the way the signal should take for every neuron, just like in the case of a human brain\n\n#### Types of Deep Learning Layers:\nThe **input layer** of nodes receives the information and transfers it to the underlying nodes;here the network fixates on patterns of local contrast as important.\n\n**Hidden node** layers are the ones where the computations appear, this is the layer that uses those patterns of local contrast to fixate on things that resemble\nIn the **output node layer**, the results of the computations will show up. In this layer, the features will be applied to templates."}}