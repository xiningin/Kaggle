{"cell_type":{"9d2e822e":"code","96e6bb63":"code","f404023f":"code","427b1854":"code","6021575d":"code","b0c19187":"code","30347e19":"code","0638b422":"code","6779fdf2":"code","91b9dd69":"code","d6bfd96c":"code","d53f473f":"code","8e704eaf":"code","b6050030":"code","9748fe10":"code","6e0ad905":"code","db782983":"code","9bce2ff5":"code","092f7f4c":"code","f8f15ec4":"code","36fcb63f":"code","0ddb3693":"code","1d69d113":"code","aba0efe7":"code","977162f8":"code","d94ff4af":"code","539f6b84":"code","cb7aa52a":"code","08805e18":"code","16230e56":"code","87e9f6d3":"code","84752463":"code","2a72d02b":"code","208a9f56":"code","0becb675":"code","31ec7626":"code","56441fb8":"code","4782d0bd":"code","b4e0bedc":"code","77efe1e1":"code","e671d4c3":"markdown","e78153f7":"markdown","7c2ed406":"markdown","9cf2f806":"markdown","a9eca73b":"markdown","d2a83c1f":"markdown","15af1bd5":"markdown","40f761d9":"markdown","bae40179":"markdown"},"source":{"9d2e822e":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","96e6bb63":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","f404023f":"df = pd.read_csv('\/kaggle\/input\/car-price-prediction\/CarPrice_Assignment.csv')\ndf.head()","427b1854":"df.info()","6021575d":"df.isna().sum()","b0c19187":"df.describe()","30347e19":"numeric_features = df.describe().columns","0638b422":"# plot a bar plot for each categorical feature count (except car_ID)\n\nfor col in numeric_features[1:]:\n    fig = plt.figure(figsize=(9, 6))\n    ax = fig.gca()\n    feature = df[col]\n    feature.hist(bins=50, ax = ax)\n    ax.axvline(feature.mean(), color='magenta', linestyle='dashed', linewidth=2)\n    ax.axvline(feature.median(), color='cyan', linestyle='dashed', linewidth=2)\n    ax.set_title(col)\nplt.show()","6779fdf2":"# plot scatter plots that show the intersection of feature and label values. (except car_ID,symboling)\n# calculate the correlation statistic to quantify the apparent relationship.\n\nfor col in numeric_features[2:-2]:\n    fig = plt.figure(figsize=(9, 6))\n    ax = fig.gca()\n    feature = df[col]\n    label = df['price']\n    correlation = feature.corr(label)\n    plt.scatter(x=feature, y=label)\n    plt.xlabel(col)\n    plt.ylabel('Price')\n    ax.set_title('price vs ' + col + '- correlation: ' + str(correlation))\nplt.show()","91b9dd69":"# change symbloling type to categorical\n\ndf['symboling'] = df['symboling'].astype('category')","d6bfd96c":"df.describe(include=['object','category'])","d53f473f":"# drop car name column due to it has 147 unique name compare to only 205 cars that we have in the dataset\n\ndf = df.drop('CarName',axis=1)","8e704eaf":"categorical_features = df.describe(include=['object','category']).columns","b6050030":"# plot a bar plot for each categorical feature count  \n\nfor col in categorical_features:\n    counts = df[col].value_counts().sort_index()\n    fig = plt.figure(figsize=(9, 6))\n    ax = fig.gca()\n    counts.plot.bar(ax = ax, color='steelblue')\n    ax.set_title(col + ' counts')\n    ax.set_xlabel(col) \n    ax.set_ylabel(\"Frequency\")\nplt.show()","9748fe10":"# plot a boxplot for the label by each categorical feature  \n\nfor col in categorical_features:\n    fig = plt.figure(figsize=(9, 6))\n    ax = fig.gca()\n    df.boxplot(column = 'price', by = col, ax = ax)\n    ax.set_title('Label by ' + col)\n    ax.set_ylabel(\"Price\")\nplt.show()","6e0ad905":"X = df.drop(['car_ID','price'],axis=1)\ny = df.price","db782983":"categorical_features","9bce2ff5":"# get dummmmies for all categorical features\nX = pd.get_dummies(X,columns=categorical_features)","092f7f4c":"print('Features:',X[:3], '\\nLabels:', y[:3], sep='\\n')","f8f15ec4":"from sklearn.model_selection import train_test_split\n\n# Split data 90%-10% into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=38)","36fcb63f":"numeric_features","0ddb3693":"numeric_features = numeric_features[2:-1]\nnumeric_features","1d69d113":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import LinearRegression\n\nnumeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n\n#categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, X.columns)])\n        #('cat', categorical_transformer, categorical_features)])","aba0efe7":"model = Pipeline(steps=[('preprocessor', preprocessor),\n                  ('classifier', LinearRegression())]) ","977162f8":"model.fit(X_train,y_train)","d94ff4af":"predictions = model.predict(X_test)","539f6b84":"from sklearn.metrics import mean_squared_error, r2_score\n\nmse = mean_squared_error(y_test, predictions)\nprint(\"MSE:\", mse)\n\nrmse = np.sqrt(mse)\nprint(\"RMSE:\", rmse)\n\nr2 = r2_score(y_test, predictions)\nprint(\"R2:\", r2)","cb7aa52a":"plt.scatter(y_test, predictions)\nplt.xlabel('Actual Labels')\nplt.ylabel('Predicted Labels')\n# overlay the regression line\nz = np.polyfit(y_test, predictions, 1)\np = np.poly1d(z)\nplt.plot(y_test,p(y_test), color='red')\nplt.show()","08805e18":"coef_ = model.named_steps.classifier.coef_\ncoef_","16230e56":"intercept_ = model.named_steps.classifier.intercept_\nintercept_","87e9f6d3":"# match column names to coefficients\n\nfor coef, col in enumerate(X_train.columns):\n    print(f'{col}:  {coef_[coef]}')","84752463":"# create data frame for coef and variables\n\noriginal_variable = list(X_train.columns)\n\nzipped = list(zip(original_variable,coef_))\n\ncoefs = [list(z) for z in zipped]\n\ncoefs = pd.DataFrame(coefs,columns=['variable','coefficients'])\n\ncoefs.head()","2a72d02b":"# top 5 coefficients\n\ncoefs.sort_values(by=['coefficients'],axis=0,ascending=False,inplace=True)\ncoefs.head()","208a9f56":"# bottom 5 coefficients\ncoefs.tail()","0becb675":"# plot show features importance \nplt.subplots(figsize=(15,15))\nplt.barh(X.columns,coef_)\nplt.ylabel('Coefficients')\nplt.xticks(rotation=90) \nplt.show()","31ec7626":"from sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nalg = [LGBMRegressor(), XGBRegressor(),GradientBoostingRegressor(),RandomForestRegressor()]\n\nfor regressor in alg:\n\n    model = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier', regressor)])  \n\n    model.fit(X_train,y_train)\n\n    predictions = model.predict(X_test)\n    \n    print(regressor)\n    print()\n    mse = mean_squared_error(y_test, predictions)\n    print(\"MSE:\", mse)\n\n    rmse = np.sqrt(mse)\n    print(\"RMSE:\", rmse)\n\n    r2 = r2_score(y_test, predictions)\n    print(\"R2:\", r2)\n\n    plt.scatter(y_test, predictions)\n    plt.xlabel('Actual Labels')\n    plt.ylabel('Predicted Labels')\n    \n    # overlay the regression line\n    z = np.polyfit(y_test, predictions, 1)\n    p = np.poly1d(z)\n    plt.plot(y_test,p(y_test), color='red')\n    plt.show()","56441fb8":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\n\n# Try these hyperparameter values with RandomForestRegressor()\n\nparams = {\n 'max_depth': range(4,8),\n 'n_estimators' : range(100,1000,100)\n }\n\nscore = make_scorer(r2_score)\ngridsearch = GridSearchCV(RandomForestRegressor(), params, scoring=score, cv=3, return_train_score=True)\n\n# Use a Random Forest Regressor algorithm\nmodel = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('gridsearch',gridsearch)])  \n\n# Find the best hyperparameter combination to optimize the R2 metric\nmodel.fit(X_train, y_train)\n\nprint(\"Best parameter combination:\", gridsearch.best_params_, \"\\n\")\n\n# Evaluate the model using the test data\npredictions = model.predict(X_test)\nmse = mean_squared_error(y_test, predictions)\nprint(\"MSE:\", mse)\nrmse = np.sqrt(mse)\nprint(\"RMSE:\", rmse)\nr2 = r2_score(y_test, predictions)\nprint(\"R2:\", r2)\n\n# Plot predicted vs actual\nplt.scatter(y_test, predictions)\nplt.xlabel('Actual Labels')\nplt.ylabel('Predicted Labels')\n# overlay the regression line\nz = np.polyfit(y_test, predictions, 1)\np = np.poly1d(z)\nplt.plot(y_test,p(y_test), color='magenta')\nplt.show()","4782d0bd":"from sklearn.model_selection import RandomizedSearchCV\n \n    \n# Try these hyperparameter values \n\nparams = {\n 'max_depth': range(2,10),\n 'n_estimators' : range(100,1000,100),\n 'max_features' : ['auto', 'sqrt'],\n 'min_samples_split' : [2, 5, 10, 15],\n 'min_samples_leaf' : [1, 2, 5, 10]\n }    \n    \nscore = make_scorer(r2_score)\nrandomsearch = RandomizedSearchCV(RandomForestRegressor(), params, scoring=score, cv=3, return_train_score=True)\n\n# Use a Random Forest Regressor algorithm\nmodel = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('randomsearch',randomsearch)])  \n\n# Find the best hyperparameter combination to optimize the R2 metric\nmodel.fit(X_train, y_train)\n\nprint(\"Best parameter combination:\", randomsearch.best_params_, \"\\n\")\n\n# Evaluate the model using the test data\npredictions = model.predict(X_test)\nmse = mean_squared_error(y_test, predictions)\nprint(\"MSE:\", mse)\nrmse = np.sqrt(mse)\nprint(\"RMSE:\", rmse)\nr2 = r2_score(y_test, predictions)\nprint(\"R2:\", r2)\n\n# Plot predicted vs actual\nplt.scatter(y_test, predictions)\nplt.xlabel('Actual Labels')\nplt.ylabel('Predicted Labels')\n# overlay the regression line\nz = np.polyfit(y_test, predictions, 1)\np = np.poly1d(z)\nplt.plot(y_test,p(y_test), color='magenta')\nplt.show()","b4e0bedc":"# Try these hyperparameter values with GradientBoostingRegressor()\n\nparams = {\n 'max_depth': range(4,8),\n 'n_estimators' : range(100,1000,100),\n 'learning_rate' : [0.1,0.01,0.001]\n }\n\nscore = make_scorer(r2_score)\ngridsearch = GridSearchCV(GradientBoostingRegressor(), params, scoring=score, cv=3, return_train_score=True)\n\n# Use a Random Forest Regressor algorithm\nmodel = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('gridsearch',gridsearch)])  \n\n# Find the best hyperparameter combination to optimize the R2 metric\nmodel.fit(X_train, y_train)\n\nprint(\"Best parameter combination:\", gridsearch.best_params_, \"\\n\")\n\n# Evaluate the model using the test data\npredictions = model.predict(X_test)\nmse = mean_squared_error(y_test, predictions)\nprint(\"MSE:\", mse)\nrmse = np.sqrt(mse)\nprint(\"RMSE:\", rmse)\nr2 = r2_score(y_test, predictions)\nprint(\"R2:\", r2)\n\n# Plot predicted vs actual\nplt.scatter(y_test, predictions)\nplt.xlabel('Actual Labels')\nplt.ylabel('Predicted Labels')\n# overlay the regression line\nz = np.polyfit(y_test, predictions, 1)\np = np.poly1d(z)\nplt.plot(y_test,p(y_test), color='magenta')\nplt.show()","77efe1e1":"# Try these hyperparameter values with XGBRegressor()\n\nparams = {\n 'max_depth': range(4,8),\n 'n_estimators' : range(100,1000,100),\n 'learning_rate' : [0.1,0.01,0.001]\n }\n\nscore = make_scorer(r2_score)\ngridsearch = GridSearchCV( XGBRegressor(), params, scoring=score, cv=3, return_train_score=True)\n\n# Use a Random Forest Regressor algorithm\nmodel = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('gridsearch',gridsearch)])  \n\n# Find the best hyperparameter combination to optimize the R2 metric\nmodel.fit(X_train, y_train)\n\nprint(\"Best parameter combination:\", gridsearch.best_params_, \"\\n\")\n\n# Evaluate the model using the test data\npredictions = model.predict(X_test)\nmse = mean_squared_error(y_test, predictions)\nprint(\"MSE:\", mse)\nrmse = np.sqrt(mse)\nprint(\"RMSE:\", rmse)\nr2 = r2_score(y_test, predictions)\nprint(\"R2:\", r2)\n\n# Plot predicted vs actual\nplt.scatter(y_test, predictions)\nplt.xlabel('Actual Labels')\nplt.ylabel('Predicted Labels')\n# overlay the regression line\nz = np.polyfit(y_test, predictions, 1)\np = np.poly1d(z)\nplt.plot(y_test,p(y_test), color='magenta')\nplt.show()   \n    ","e671d4c3":"## Train a Regression Model","e78153f7":"### Comment:\n\n* With linear regression, we get the results as follow:\n\n    - RMSE: 2420.56 \n    - R2: 0.93\n\n* engine size is the most important feature with positive coefficient of 4253.9 whereas engine type (ohcv)has the most negative effect on the price with negative coefficient of -1566.47.","7c2ed406":"## Randomized Search ","9cf2f806":"## Other regressor algorithms \n* LGBMRegressor, XGBRegressor,Gradient Boosting Regressor, Random Forest Regressor","a9eca73b":"### Problem Statement\n\nA Chinese automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts.\n\nThey have contracted an automobile consulting company to understand the factors on which the pricing of cars depends. Specifically, they want to understand the factors affecting the pricing of cars in the American market, since those may be very different from the Chinese market. The company wants to know:\n\nWhich variables are significant in predicting the price of a car\nHow well those variables describe the price of a car\nBased on various market surveys, the consulting firm has gathered a large data set of different types of cars across the America market.\n\n### Business Goal\n\nWe are required to model the price of cars with the available independent variables. It will be used by the management to understand how exactly the prices vary with the independent variables. They can accordingly manipulate the design of the cars, the business strategy etc. to meet certain price levels. Further, the model will be a good way for management to understand the pricing dynamics of a new market.\n\nDataSet Information:\n\n* Car_ID: Unique id of each observation (Interger)\n* Symboling: Its assigned insurance risk rating, A value of +3 indicates that the auto is risky, -3 that it is probably pretty safe.(Categorical)\n* CarName: Name of car company (Categorical)\n* fueltype: Car fuel type i.e gas or diesel (Categorical)\n* aspiration: Aspiration used in a car (Categorical)\n* doornumber: Number of doors in a car (Categorical)\n* carbody: body of car (Categorical)\n* drivewheel: type of drive wheel (Categorical)\n* enginelocation: Location of car engine (Categorical)\n* wheelbase: Weelbase of car (Numeric)\n* carlength: Length of car (Numeric)\n* carwidth: Width of car (Numeric)\n* carheight: height of car (Numeric)\n* curbweight: The weight of a car without occupants or baggage. (Numeric)\n* enginetype: Type of engine. (Categorical)\n* cylindernumber: cylinder placed in the car (Categorical)\n* enginesize: Size of car (Numeric)\n* fuelsystem: Fuel system of car (Categorical)\n* boreratio: Boreratio of car (Numeric)\n* stroke: Stroke or volume inside the engine (Numeric)\n* compressionratio: compression ratio of car (Numeric)\n* horsepower: Horsepower (Numeric)\n* peakrpm: car peak rpm (Numeric)\n* citympg: Mileage in city (Numeric)\n* highwaympg: Mileage on highway (Numeric)\n* price(Dependent variable): Price of car (Numeric)\n\nFeatures:\n* 13 numerical ;\n* 11 categorical;\n* 1 Car_ID: Unique id of each observation (Interger)\n\nTarget:\n* 1 numerical - price","d2a83c1f":"## Grid Search","15af1bd5":"## Conclusion:\n* XGBRegressor regression provides the best result - RMSE:1615 , R2:0.97\n* Best parameter combination for XGBRegressor regression: learning_rate: 0.1, max_depth: 4, n_estimators: 300","40f761d9":"Comment:\n* symboling has 6 distinct values (-2 to 3), consider to change type to categorical","bae40179":"## Optimize Hyperparameters - Grid Search"}}