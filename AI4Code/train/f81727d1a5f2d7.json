{"cell_type":{"145e525a":"code","fd63e48e":"code","7acef595":"code","fae8aff4":"code","72fe5108":"code","db58b698":"code","21c1219d":"code","bb0d410e":"code","b105a9ee":"code","fc6d380e":"code","50c811d5":"code","da5fdfff":"code","0940d9e1":"code","10fbb0a4":"code","ab5de727":"code","e5a1c09d":"code","430662a5":"code","ad489c64":"code","c3f54149":"markdown","8f550ab2":"markdown","0178725a":"markdown","b1ce5c8d":"markdown","d8e8bd7c":"markdown","283a3628":"markdown","185dc157":"markdown","e89b81fd":"markdown","ef0478dd":"markdown","24b6beb7":"markdown","f22e1fad":"markdown","630f524e":"markdown","ae98d558":"markdown","e5abb03c":"markdown","94b4da37":"markdown","84f3b45d":"markdown"},"source":{"145e525a":"!free -h","fd63e48e":"!cat \/proc\/cpuinfo","7acef595":"import pandas as pd\nimport pyarrow\nimport pyarrow.parquet as pq\nfrom pyarrow.parquet import ParquetFile\n\ntry:   import parquet\nexcept Exception as exception: print(exception)\n    \ntry:   import fastparquet\nexcept Exception as exception: print(exception)    ","fae8aff4":"import pandas as pd\nimport numpy as np\nimport pyarrow\nimport glob2\nimport gc\nimport time\nimport sys\nimport humanize\nimport math\nimport time\nimport psutil\nimport gc\nimport simplejson\nimport skimage\nimport skimage.measure\nfrom timeit import timeit\nfrom time import sleep\nfrom pyarrow.parquet import ParquetFile\nimport pyarrow\nimport pyarrow.parquet as pq\nimport signal\nfrom contextlib import contextmanager\n\npd.set_option('display.max_columns',   500)\npd.set_option('display.max_colwidth',  None)","72fe5108":"@contextmanager\ndef timeout(time):\n    # Register a function to raise a TimeoutError on the signal.\n    signal.signal(signal.SIGALRM, raise_timeout)\n    # Schedule the signal to be sent after ``time``.\n    signal.alarm(time)\n\n    try:\n        yield\n    except TimeoutError:\n        pass\n    finally:\n        # Unregister the signal so it won't be triggered\n        # if the timeout is not reached.\n        signal.signal(signal.SIGALRM, signal.SIG_IGN)\n\ndef raise_timeout(signum, frame):\n    raise TimeoutError","db58b698":"from memory_profiler import profile","21c1219d":"!python --version  # Python 3.6.6 :: Anaconda, Inc == original + latest docker (2020-03-14)","bb0d410e":"pd.__version__  # 0.25.3 == original + latest docker (2020-03-14)","b105a9ee":"filenames = sorted(glob2.glob('..\/input\/bengaliai-cv19\/train_image_data_*.parquet')); filenames","fc6d380e":"def read_parquet_via_pandas(files=4, cast='uint8', resize=1):\n    gc.collect(); sleep(5);  # wait for gc to complete\n    memory_before = psutil.virtual_memory()[3]\n    # NOTE: loading all the files into a list variable, then applying pd.concat() into a second variable, uses double the memory\n    df = pd.concat([ \n        pd.read_parquet(filename).set_index('image_id', drop=True).astype('uint8')\n        for filename in filenames[:files] \n    ])\n    memory_end= psutil.virtual_memory()[3]        \n\n    print( \"  sys.getsizeof():\", humanize.naturalsize(sys.getsizeof(df)) )\n    print( \"  memory total:   \", humanize.naturalsize(memory_end - memory_before), '+system', humanize.naturalsize(memory_before) )        \n    return df\n\n\ngc.collect(); sleep(2);  # wait for gc to complete\nprint('single file:')\ntime_start = time.time()\nread_parquet_via_pandas(files=1); gc.collect()\nprint(f\"  time:            {time.time() - time_start:.1f}s\" )\nprint('------------------------------')\nprint('pd.concat() all files:')\ntime_start = time.time()\nread_parquet_via_pandas(files=4); gc.collect()\nprint(f\"  time:            {time.time() - time_start:.1f}s\" )\npass","50c811d5":"import pyarrow\nimport pyarrow.parquet as pq\nfrom pyarrow.parquet import ParquetFile\n\npyarrow.__version__  # 0.16.0 == original + latest docker (2020-03-14)","da5fdfff":"# DOCS: https:\/\/arrow.apache.org\/docs\/python\/generated\/pyarrow.parquet.ParquetFile.html\ndef read_parquet_via_pyarrow_file():\n    pqfiles = [ ParquetFile(filename) for filename in filenames ]\n    print( \"sys.getsizeof\", humanize.naturalsize(sys.getsizeof(pqfiles)) )\n    for pqfile in pqfiles[0:1]: print(pqfile.metadata)\n    return pqfiles\n\ngc.collect(); sleep(2);  # wait for gc to complete\ntime_start = time.time()\nread_parquet_via_pyarrow_file(); gc.collect()\nprint( \"time: {time.time() - time_start:.1f}s\" )\npass","0940d9e1":"# DOCS: https:\/\/arrow.apache.org\/docs\/python\/parquet.html\n# DOCS: https:\/\/arrow.apache.org\/docs\/python\/generated\/pyarrow.Table.html\n# NOTE: Attempting to read all tables into memory, causes an out of memory exception\ndef read_parquet_via_pyarrow_table():\n    shapes  = []\n    classes = []\n    sizes   = 0\n    for filename in filenames:\n        table = pq.read_table(filename) \n        shapes.append( table.shape )\n        classes.append( table.__class__ )\n        size = sys.getsizeof(table); sizes += size\n        print(\"sys.getsizeof(): \",   humanize.naturalsize(sys.getsizeof(table))  )        \n    print(\"sys.getsizeof() total:\", humanize.naturalsize(sizes) )\n    print(\"classes:\", classes)\n    print(\"shapes: \",  shapes)    \n\n\ngc.collect(); sleep(2);  # wait for gc to complete\ntime_start = time.time()\nread_parquet_via_pyarrow_table(); gc.collect()\nprint( f\"time:   {time.time() - time_start:.1f}s\" )\npass","10fbb0a4":"import time, psutil, gc\n\ngc.collect(); sleep(2)  # wait for gc to complete\nmem_before   = psutil.virtual_memory()[3]\nmemory_usage = []\n\ndef read_parquet_via_pyarrow_table_generator(batch_size=128):\n    for filename in filenames[0:1]:  # only loop over one file for demonstration purposes\n        gc.collect(); sleep(1)\n        for batch in pq.read_table(filename).to_batches(batch_size):\n            mem_current = psutil.virtual_memory()[3]\n            memory_usage.append( mem_current - mem_before )\n            yield batch.to_pandas()\n\n\ntime_start = time.time()\ncount = 0\nfor batch in read_parquet_via_pyarrow_table_generator():\n    count += 1\n\nprint( \"time:             \", time.time() - time_start )\nprint( \"count:            \", count )\nprint( \"min memory_usage: \", humanize.naturalsize(min(memory_usage))  )\nprint( \"max memory_usage: \", humanize.naturalsize(max(memory_usage))  )\nprint( \"avg memory_usage: \", humanize.naturalsize(np.mean(memory_usage)) )\npass    ","ab5de727":"memory_before = psutil.virtual_memory()[3]\nmemory_usage  = []\n\ndef read_parquet_via_pandas_generator(batch_size=128, reads_per_file=5):\n    for filename in filenames:\n        num_rows    = ParquetFile(filename).metadata.num_rows\n        cache_size  = math.ceil( num_rows \/ batch_size \/ reads_per_file ) * batch_size\n        batch_count = math.ceil( cache_size \/ batch_size )\n        for n_read in range(reads_per_file):\n            cache = pd.read_parquet(filename).iloc[ cache_size * n_read : cache_size * (n_read+1) ].copy()\n            gc.collect(); sleep(1);  # sleep(1) is required to allow measurement of the garbage collector\n            for n_batch in range(batch_count):            \n                memory_current = psutil.virtual_memory()[3]\n                memory_usage.append( memory_current - memory_before )                \n                yield cache[ batch_size * n_batch : batch_size * (n_batch+1) ].copy()\n\n                \nfor reads_per_file in [1,2,3,5]: \n    gc.collect(); sleep(5);  # wait for gc to complete\n    memory_before = psutil.virtual_memory()[3]\n    memory_usage  = []\n    \n    time_start = time.time()\n    count = 0\n    for batch in read_parquet_via_pandas_generator(batch_size=128, reads_per_file=reads_per_file):\n        count += 1\n        \n    print( \"reads_per_file\", reads_per_file, '|', \n           'time', int(time.time() - time_start),'s', '|', \n           'count', count,  '|',\n           'memory', {\n                \"min\": humanize.naturalsize(min(memory_usage)),\n                \"max\": humanize.naturalsize(max(memory_usage)),\n                \"avg\": humanize.naturalsize(np.mean(memory_usage)),\n                \"+system\": humanize.naturalsize(memory_before),               \n            }\n    )\npass    ","e5a1c09d":"def read_single_parquet_via_pandas_with_cast(dtype='uint8', normalize=False, denoise=False, invert=True, resize=1, resize_fn=None):\n    gc.collect(); sleep(2);\n    \n    memory_before = psutil.virtual_memory()[3]\n    time_start = time.time()        \n    \n    train = (pd.read_parquet(filenames[0])\n               .set_index('image_id', drop=True)\n               .values.astype(dtype)\n               .reshape(-1, 137, 236, 1))\n    \n    if invert:                                         # Colors | 0 = black      | 255 = white\n        train = (255-train)                            # invert | 0 = background | 255 = line\n   \n    if denoise:                                        # Set small pixel values to background 0\n        if invert: train *= (train >= 25)              #   0 = background | 255 = line  | np.mean() == 12\n        else:      train += (255-train)*(train >= 230) # 255 = background |   0 = line  | np.mean() == 244     \n        \n    if isinstance(resize, bool) and resize == True:\n        resize = 2    # Reduce image size by 2x\n    if resize and resize != 1:                  \n        # NOTEBOOK: https:\/\/www.kaggle.com\/jamesmcguigan\/bengali-ai-image-processing\/\n        # Out of the different resize functions:\n        # - np.mean(dtype=uint8) produces produces fragmented images (needs float16 to work properly - but RAM intensive)\n        # - np.median() produces the most accurate downsampling\n        # - np.max() produces an enhanced image with thicker lines (maybe slightly easier to read)\n        # - np.min() produces a  dehanced image with thiner lines (harder to read)\n        resize_fn = resize_fn or (np.max if invert else np.min)\n        cval      = 0 if invert else 255\n        train = skimage.measure.block_reduce(train, (1, resize,resize, 1), cval=cval, func=resize_fn)  # train.shape = (50210, 137, 236, 1)\n        \n    if normalize:\n        train = train \/ 255.0          # division casts: int -> float64 \n\n\n    time_end     = time.time()\n    memory_after = psutil.virtual_memory()[3] \n    return ( \n        str(round(time_end - time_start,2)).rjust(5),\n        # str(sys.getsizeof(train)),\n        str(memory_after - memory_before).rjust(5), \n        str(train.shape).ljust(20),\n        str(train.dtype).ljust(7),\n    )\n\n\nfor dtype in ['uint8', 'uint16', 'uint32', 'float16', 'float32']:  # 'float64' caused OOM error\n    seconds, memory, shape, dtype = read_single_parquet_via_pandas_with_cast(dtype=dtype)\n    print(f'dtype {dtype}'.ljust(18) + f'| {dtype} | {shape} | {seconds}s | {humanize.naturalsize(memory).rjust(8)}')\n\nfor denoise in [False, True]:\n    seconds, memory, shape, dtype = read_single_parquet_via_pandas_with_cast(denoise=denoise)\n    print(f'denoise {denoise}'.ljust(18) + f'| {dtype} | {shape} | {seconds}s | {humanize.naturalsize(memory).rjust(8)}')\n\nfor normalize in [False, True]:\n    seconds, memory, shape, dtype = read_single_parquet_via_pandas_with_cast(normalize=normalize)\n    print(f'normalize {normalize}'.ljust(18) + f'| {dtype} | {shape} | {seconds}s | {humanize.naturalsize(memory).rjust(8)}')    ","430662a5":"# division casts: int -> float64 \nfor dtype in ['float16', 'float32']:\n    seconds, memory, shape, dtype = read_single_parquet_via_pandas_with_cast(dtype=dtype, normalize=True)\n    print(f'normalize {dtype}'.ljust(18) + f'| {dtype} | {shape} | {seconds}s | {humanize.naturalsize(memory).rjust(8)}')    ","ad489c64":"# skimage.measure.block_reduce() casts: unit8 -> float64    \nfor resize in [2, 3, 4]:\n    for dtype in ['float16', 'float32', 'uint8']:  # 'float32' almosts causes OOM error \n        gc.collect()\n        with timeout(10*60):\n            seconds, memory, shape, dtype = read_single_parquet_via_pandas_with_cast(dtype=dtype, resize=resize)\n            print(f'resize {resize} {dtype}'.ljust(18) + f'| {dtype} | {shape} | {seconds:6.2f}s | {humanize.naturalsize(memory).rjust(8)}')","c3f54149":"Both `pandas` and `pyarrow` are the two possible libaries to use\n\nNOTE: `parquet` and `fastparquet` are not in the Kaggle pip repo, even with the latest available docket images. Whilst these can be obtained via `!pip install parquet fastparquet`, this requires an internet connection which is not allowed for Kernel only competitions.","8f550ab2":"Other imports","0178725a":"# Pandas Batch Generator Function","b1ce5c8d":"Creating a `ParquetFile` is very quick, and memory efficent. It only creates a pointer to the file, but allows us to read the metadata.\n\nHowever there the dataset only contains a single `row_group`, meaning the file can only be read out as a single chunk (no easy row-by-row streaming)","d8e8bd7c":"Memory useage can vary by an order of magnitude based on the implcit cast dtype. \n\n- Raw pixel values are read from the parquet file as `uint8`\n- `\/ 255.0` or `skimage.measure.block_reduce()` will do an implict cast of `int` -> `float64`\n- `float64` results in a datastructure 8x as large as `uint8` (`13.0 GB` vs `1.8 GB`)\n  - This can be avoided by doing an explict cast to `float16` (`3.3 GB`)\n- `skimage.measure.block_reduce(df, (1,n,n,1), func=np.mean, cval=0)` == `AveragePooling2D(n)` \n  - reduces data structure memory by `n^2` \n\nCPU time: \n- `float32` (+0.5s) is the fastest cast; `float16` (+8s) is 2x slower than cast `float64` (+4s).\n- `skimage.measure.block_reduce()` is an expensive operation (3-5x IO read time)","283a3628":"# Dtypes and Memory Usage","185dc157":"# Read Parquet via Pandas\n\nPandas is the simplest and recommended option\n- it takes 40s seconds to physically read all the data\n- pandas dataset is 6.5GB in RAM. \n","e89b81fd":"A generator can be written around pyarrow, but this still reads the contents of an entire file into memory and this function is really slow","ef0478dd":"# RAM\/CPU Available In Kaggle Kernel\n\nIn theory there is 18GB of Kaggle RAM, but loading the entire dataset at once often causes out of memory errors, and doesn't leave anything for the tensorflow model. In practice, datasets need to be loaded one file at a time (or even 75% of a file) to permit a successful compile and submission run.","24b6beb7":"Using a pyarrow.Table is faster than pandas (`28s` vs `45s`), but uses more memory (`7.6GB` vs `6.5GB`) and causes an Out-Of-Memory exception if everything is read at once","f22e1fad":"# Read Parquet via PyArrow","630f524e":"# Reading Parquet Files - RAM\/CPU Optimization \n\nThe Bengali AI dataset is used to explore the different methods available for reading Parquet files (pandas + pyarrow).\n\nA common source of trouble for Kernel Only Compeitions, is Out-Of-Memory errors, and the 120 minute submission time limit.\n\nThis notebook contains:\n- Syntax and performance for reading Parquet via both Pandas and Pyarrow\n- Kaggle Kernel RAM\/CPU allocation\n  - 18G RAM\n  - 2x Intel(R) Xeon(R) CPU @ 2.00GHz CPU\n- RAM optimized generator function around `pandas.read_parquet()`\n  - trade 50% RAM (1700MB -> 780MB) for 2x disk IO time (5.8min -> 10.2min runtime) \n- RAM\/CPU profiling of implict dataframe dtype casting \n  - beware of implicit cast between `unit8` -> `float64` = 8x memory usage\n  - `skimage.measure.block_reduce(train, (1,2,2,1), func=np.mean, cval=0)` can downsample images\n","ae98d558":"# Available Libaries","e5abb03c":"2x Intel(R) Xeon(R) CPU @ 2.00GHz CPU\n\nIn theory this might allow for optimizations using `pathos.multiprocessing`","94b4da37":"# Memory Profiler Decorator\nIts also worth mentioning the memory_profiler `@profile` decorator for interactive debugging. \n- NOTE: @profile \/ %mprun can only be used on functions defined in physical files, and not in the IPython environment.\n- https:\/\/pypi.org\/project\/memory-profiler\/","84f3b45d":"It is possible to write a batch generator using pandas. In theory this should save memory, at the expense of disk IO. \n\n- Timer show that disk IO increase linarly with the number of filesystem reads. \n- Memory measurements require `gc.collect(); sleep(1)`, but show that average\/min memory reduces linearly with filesystem reads\n\nThere are 8 files to read (including test files in the submission), so the tradeoffs are as follows:\n- reads_per_file 1 |  44s * 8 =  5.8min + 1700MB RAM (potentually crashing the kernel)\n- reads_per_file 2 |  77s * 8 = 10.2min +  781MB RAM (minimum required to solve the memory bottleneck)\n- reads_per_file 3 | 112s * 8 = 14.9min +  508MB RAM (1\/8th of total 120min runtime)\n- reads_per_file 5 | 183s * 8 = 24.4min +  314MB RAM (1\/5th of total 120min runtime)\n\nThis is a memory\/time tradeoff, but demonstrates a practical solution to out-of-memory errors"}}