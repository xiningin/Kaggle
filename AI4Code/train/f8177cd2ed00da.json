{"cell_type":{"4469ee6a":"code","f2b271d3":"code","53c7afd2":"code","2247631e":"code","5388979b":"code","2a638c52":"code","f65f83a1":"code","06cb2c70":"code","7f408cac":"code","a9a2b46f":"code","2a5e8a9b":"code","d6a9c0f9":"code","419f72d4":"code","3e17733a":"code","f0bdb226":"code","dbee790d":"code","9febb68b":"code","8a2c2124":"code","21556b10":"code","253965b6":"markdown","315f1be9":"markdown","e9c3a003":"markdown","398984ca":"markdown","05b772be":"markdown","9acfb8aa":"markdown","0d13be9f":"markdown","b8d1fde4":"markdown","b4629c8d":"markdown","dbf1e197":"markdown","e85c86c3":"markdown","51e91415":"markdown"},"source":{"4469ee6a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f2b271d3":"import pandas as pd\nimport numpy as np\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom gensim.models import KeyedVectors\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Embedding\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","53c7afd2":"data_spam=pd.read_csv(\"..\/input\/spam-text-message-classification\/SPAM text message 20170820 - Data.csv\")\ndata_spam.head(10)","2247631e":"# Replace Spam with 1 and Ham with 0\ndata_spam['Category']=data_spam['Category'].replace({'ham':0,'spam':1})\ndata_spam.head()","5388979b":"data_spam.shape    # 5572 rows and 2 columns available","2a638c52":"# Original sentence before text pre-processing\ndata_spam['Message'][2]","f65f83a1":"stop_words=stopwords.words('english')\nlemma=WordNetLemmatizer()\n\nsentence=data_spam['Message']\n\nnew_sentence=[]\nfor i in range(len(sentence)):\n    \n    # excluding the irrelavent characters\n    word=re.sub(\"[^a-zA-Z\\.\\!]\",\" \",sentence[i])\n    \n    #Lower casing all words\n    word=word.lower()\n    \n    #Splitting each sentence into words\n    word=word.split()\n    \n    #Performing lemmatization to retain the root word out of all the inflected words. Also eliminating stopwords\n    word=[lemma.lemmatize(w) for w in word if w not in stop_words]\n    word=\" \".join(word)\n    new_sentence.append(word)\n\n#Sentence after pre-processing\nnew_sentence[2]","06cb2c70":"data_spam['New_Message']=new_sentence\ndata_spam.head()","7f408cac":"# Loading a pre-trained word2vec model \nembeddings=KeyedVectors.load_word2vec_format('..\/input\/word2vec-google\/GoogleNews-vectors-negative300.bin',binary=True)\n","a9a2b46f":"# Splitting data into train and test\ntrain_X,test_X,train_y,test_y=train_test_split(data_spam['New_Message'],data_spam['Category'],test_size=0.3,random_state=10)","2a5e8a9b":"# Initialising the Tokenizer\ntokenizer=Tokenizer()\n\n# Fitting the tokenizer on train_x\ntokenizer.fit_on_texts(train_X)\n\n# Number of words \nvocab_size=len(tokenizer.word_index)+1\n\n#Converting the words to sequence of their corresponding index\ntrain_X_seq=tokenizer.texts_to_sequences(train_X)\ntest_X_seq=tokenizer.texts_to_sequences(test_X)","d6a9c0f9":"\n#Obtaining the length of each sequence list inoder to find the maximum length which is to be used for padding the train and test\nlen_doc=[]\nfor doc in train_X_seq:\n    doc_size=len(doc)\n    len_doc.append(doc_size)\n\nprint(\"Length of First 10 sequence:\",len_doc[:10])\nprint(\"Maximum length of sequence:\",max(len_doc))\n\nsns.boxplot(len_doc,color='green')\nplt.title(\"Distribution of length of sequence list\")\nplt.show()","419f72d4":"#Padding each list of sequence to ensure each of them have same dimension.Considering max length as 97\n\ntrain_X_pad=pad_sequences(train_X_seq,maxlen=97,padding='post')\ntest_X_pad=pad_sequences(test_X_seq,maxlen=97,padding='post')","3e17733a":"# Building the weight matrix \nembeddings_mat=np.zeros((vocab_size,300))\nwords_available=[]\nwords_not_available=[]\n\nfor w,wid in tokenizer.word_index.items():\n    if w in embeddings:\n        embeddings_mat[wid]=embeddings[w]\n        words_available.append(w)\n        \n    else:\n        words_not_available.append(w)","f0bdb226":"# Amount of words not available in the pre-trained embeddings model\nlen(words_not_available)\/vocab_size *100","dbee790d":"# Initialising the model\nmodel=Sequential()\n\n# Since we already have the weights matrix, there is not need to further train the weights and hence setting trainable parameter as False\nmodel.add(Embedding(vocab_size,300,weights=[embeddings_mat],input_length=97,trainable=False))\n\n# Flattening the layers\nmodel.add(Dense(units=16,activation='relu'))\n\n#Output Layer\nmodel.add(Dense(units=1,activation='sigmoid'))","9febb68b":"model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])","8a2c2124":"my_model=model.fit(x=train_X_pad,y=train_y,batch_size=32,epochs=50,validation_data=(test_X_pad,test_y))","21556b10":"plt.figure(figsize=(15,5))\nplt.subplot(121)\nplt.plot(my_model.history['loss'],label='train_loss')\nplt.plot(my_model.history['val_loss'],label='test_loss')\nplt.title(\"Loss\")\nplt.legend()\nplt.suptitle(\"Performance evaluation\")\n\nplt.subplot(122)\nplt.plot(my_model.history['accuracy'],label='train_accuracy')\nplt.plot(my_model.history['val_accuracy'],label='test_accuracy')\nplt.title(\"Accuracy\")\nplt.legend()\nplt.show()","253965b6":"## Loading the data:","315f1be9":"## Text Pre-processing:","e9c3a003":"## Comparing Performance:","398984ca":"## Fitting the model on Train data:","05b772be":"- A spam classifier was built using  a pre-trained word embedding model\n\n- The model gave an accuracy of 87.4% on Train data and 87.8% on Test data.\n\n- The model gave a binaray cross-entropy loss of 0.37 on Train data and 0.36 on test data.\n\n- As per the scores, the model built exhibited exceptionally low over-fitting.","9acfb8aa":"To build a classification model using word embeddings to classify email messages as Spam or Ham.","0d13be9f":"## Building a Classification model:","b8d1fde4":"## Import Required Libraries:","b4629c8d":"## Probelm Statement:","dbf1e197":"## Building a Word Embedding Matrix:","e85c86c3":"## Compiling the model:","51e91415":"## Conclusion:"}}