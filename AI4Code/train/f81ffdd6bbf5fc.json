{"cell_type":{"e8e7843e":"code","7ef47aee":"code","d96b3ef7":"code","dd084777":"code","25c8ad9c":"code","207d7774":"code","dc1606cf":"code","f3977ee3":"code","8772603d":"code","9f82f6c2":"code","13b0d382":"code","47284d61":"code","db6404e5":"code","0a7f8dad":"code","53793ce5":"code","f99b7553":"code","3ce07509":"code","b6718b86":"code","8f246078":"code","b956272d":"code","d855400f":"code","280cafcf":"code","c74d014d":"code","24f1285e":"code","92a412f7":"code","3956521f":"code","63a821db":"code","3f0c5c42":"code","8feef7cf":"code","9bce6897":"code","e7a98150":"code","86fd598f":"markdown","6754a957":"markdown","c2ad2d30":"markdown","1fe88e15":"markdown","2b85c692":"markdown","2fc99d2d":"markdown","70283b7f":"markdown","434fe31f":"markdown","58a01dd2":"markdown","0c141367":"markdown","16c1227d":"markdown","a6a513e3":"markdown","4a6a5551":"markdown","3656cdae":"markdown","ffadb15c":"markdown","e6ba7487":"markdown","47fd4339":"markdown","d28be178":"markdown","56bd76df":"markdown","1ae6ed78":"markdown","33529a60":"markdown","a0daac72":"markdown","2dc4b830":"markdown","88eab67c":"markdown","45b60881":"markdown","80b6d195":"markdown","b62c1e16":"markdown","71e616dc":"markdown","24d1d1b1":"markdown","54ef6f18":"markdown","bff05c82":"markdown"},"source":{"e8e7843e":"#import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.gridspec as gridspec\nimport matplotlib.pylab as pl\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score \nimport warnings\nwarnings.filterwarnings(\"ignore\") ","7ef47aee":"#dataset\ndata= pd.read_csv(\"..\/input\/insurance-dataset\/insurance_data.csv\")\ndata.head()","d96b3ef7":"# plot\nplt.figure(figsize= (9,4))\nplt.scatter(data[\"age\"], data['bought_insurance'])\nplt.xlabel(\"age\", fontsize=14)\nplt.ylabel(\"insurance ?: 0=No and 1=Yes\", fontsize=14)","dd084777":"# separating independent and dependent variable\nx= data[[\"age\"]]\ny= data[[\"bought_insurance\"]]\n\n# defining Linear Regression model\nfrom sklearn.linear_model import LinearRegression\nreg= LinearRegression()\n\n# fitting the model\nreg.fit(x,y)","25c8ad9c":"# plotting best fit line\nplt.figure(figsize= (9,4))\nplt.scatter(data[\"age\"], data['bought_insurance'])\nplt.plot(data[\"age\"], reg.predict(x), color=\"orange\")\nplt.xlabel(\"age\", fontsize=14)\nplt.ylabel(\"insurance: 0=No and 1=Yes\", fontsize=14)\nplt.legend([ \"best fit regression line\", \"actual\"])","207d7774":"print(\"intercept :\", reg.intercept_)\nprint(\"coefficient :\", reg.coef_)","dc1606cf":"# probability for age 15 and 75\nprint(\"probability(yes=15) is\" , reg.predict([[15]]))\nprint(\"probability(yes=75) is\" , reg.predict([[75]]))","f3977ee3":"# creating same dataset with outlier\ndata.to_csv(\"insurance_data(with_outlier).csv\", index=False)","8772603d":"# to create an outlier, I have added a data point for age=120 and 1 as the corresponding output\nimport csv\n\nwith open(\"insurance_data(with_outlier).csv\", \"a\") as csvfile:\n    writer= csv.writer(csvfile)\n    writer.writerow([120,1])  ","9f82f6c2":"# same dataset with outliers\ndata2= pd.read_csv(\"insurance_data(with_outlier).csv\")\ndata2.tail()","13b0d382":"# fitting linear regression model on the new dataset with outlier\nreg2= LinearRegression()\nreg2.fit(data2[[\"age\"]], data2[[\"bought_insurance\"]])","47284d61":"plt.figure(figsize= (9,4))\nplt.scatter(data2[[\"age\"]], data2[[\"bought_insurance\"]])\nplt.plot(data2[[\"age\"]], reg2.predict(data2[[\"age\"]]), color=\"orange\")\nplt.xlabel(\"age\", fontsize=14)\nplt.ylabel(\"insurance: 0=No and 1=Yes\", fontsize=14)\nplt.legend([ \"best fit regression line\", \"actual\"])\n\n# intercept and coefficient of regression line\nprint(\"intercept :\", reg2.intercept_)\nprint(\"coefficient :\", reg2.coef_)","db6404e5":"print(\"probability(yes=40) without outlier was {}. This person was buying the insurance.\".format(reg.predict([[40]])))\nprint(\"probability(yes=40) with outlier is {}. Now this person will not buy the insurance.\".format(reg2.predict([[40]])))","0a7f8dad":"# upload dataset\ndf= pd.read_csv(\"..\/input\/titanicdataset-traincsv\/train.csv\")\n\n# shape of dataset\nprint(\"dataset shape:\", df.shape)\ndf.head()","53793ce5":"gs= gridspec.GridSpec(2,2)\nplt.figure(figsize=(15,12))\n\nplt.style.use('fivethirtyeight')\nax=pl.subplot(gs[0,0])\nsns.countplot(x= \"Survived\", data=df, palette=\"husl\")\n\n\nax=pl.subplot(gs[0,1])\nsns.countplot(x=\"Survived\", hue=\"Sex\", data=df)\n\nax=pl.subplot(gs[1,0])\nsns.countplot(x=\"Survived\", hue=\"Pclass\", data=df)\n\nax= pl.subplot(gs[1,1])\nsns.countplot(x= \"Survived\", hue=\"Embarked\", data=df, palette=\"husl\")","f99b7553":"# percentage survived\ndf[\"Survived\"].value_counts()\/ len(df)*100","3ce07509":"df.isnull().sum()","b6718b86":"sns.boxplot(df[\"Age\"])","8f246078":"# function to impute median\ndef impute_age(dataframe, feature, median):\n    dataframe[feature]= dataframe[feature].fillna(median)","b956272d":"impute_age(df, \"Age\", df[\"Age\"].median())","d855400f":"print(\"percentage of missing values in Cabin :\",df[\"Cabin\"].isnull().mean() )\n\ndf.drop(columns= [\"Cabin\"], axis=1, inplace= True)","280cafcf":"# function to impute mode\ndef impute_mode(dataframe, feature):\n    dataframe[feature]= dataframe[feature].fillna(dataframe[feature].mode()[0])","c74d014d":"impute_mode(df, \"Embarked\")","24f1285e":"sns.countplot(df[\"Survived\"])\nplt.title(\"target variable\")","92a412f7":"print(\"percentage of class in target:\\n\", df[\"Survived\"].value_counts()\/ len(df)*100)","3956521f":"df.drop(columns= [\"PassengerId\", \"Name\", \"Ticket\"], axis=1, inplace=True)","63a821db":"# encoding categorical fetaures\ndf= pd.get_dummies(df, drop_first=True)\n\n# dummy encoded dataset\ndf.head()","3f0c5c42":"# features\nx= df.iloc[:, 1:]\n\n# target\ny= df.iloc[:, 0]","8feef7cf":"x_train, x_test, y_train, y_test= train_test_split(x,y, test_size=0.2, random_state=10)","9bce6897":"# selecting the classifier\nreg= LogisticRegression()\n\n# fitting model on train data\nreg.fit(x_train, y_train)","e7a98150":"# checking model performance\ny_predicted= reg.predict(x_test)\n\ncm= confusion_matrix(y_test, y_predicted)\nprint(cm)\nsns.heatmap(cm, annot=True)\nprint(accuracy_score(y_test, y_predicted))\nprint(classification_report(y_test, y_predicted))","86fd598f":"# Logistic Regression in python\nStarting with Titanic dataset. In this dataset we have to predict whether a passenger will survive or not.","6754a957":"###### In short:\nprobability (p)= f(x), a linear function where x is an independent variables.<br> But we need to constrain probability (p) such that 0<= p <=1.\n","c2ad2d30":"## Problem in using Linear Regression\n### Problem 1.\nOn the best fit line if we try to find whether the person with age=15 and age=75 will buy the insurance or not, we are getting the probability as negative and greater than 1 respectively.<br> And since the probability always lie between 0 and 1, it can never be negative or >1, Linear Regression cannot be used here.","1fe88e15":"## Data Preprocessing","2b85c692":"## Solving the above problems of using Linear Regression\nLet\u2019s consider y as linear function in the above univariate regression model, where x = independent variable and y = dependent variable.\n$${y} = {\\beta}_{0} + {\\beta}_{1}x $$ \n\n$${probability (p)} = f(x) = {\\beta}_{0} + {\\beta}_{1}x $$ \n###### To make probability(p) >= 0\nThe exponential of any number always gives the positive value. Now the probability (p) becomes\n\n$${p} = e^{f(x)} = e^{ {\\beta}_{0} + {\\beta}_{1}x } $$ \n###### To make probability(p) < 1\nDividing any number by a number which is slightly greater than it gives a value less than 1. Now the probability (p) becomes\n\n$${p} = \\frac{ e^{f(x)}}  {1 + e^{f(x)} }   = \\frac{ e^{ {\\beta}_{0} + {\\beta}_{1}x } }  {1 + e^{ {\\beta}_{0} + {\\beta}_{1}x }  }$$ \n\n$$ or $$\n\n$${p} = \\frac{1}  {1 + e^{- ({\\beta}_{0} + {\\beta}_{1}x) }  }$$\n\n$$ Sigmoid Function $$","2fc99d2d":"***This Sigmoid Function will also take care of Outliers***\n![1_EVzm1iP8tx0Opvw-ph-XrA.png](attachment:1_EVzm1iP8tx0Opvw-ph-XrA.png)","70283b7f":"### Missing values in `Cabin ` column","434fe31f":"### Observations\n- Only 38% of passengers survived\n- Females survived in higher proportion than males\n- Passengers who survived mostly belonged to First Class and that who did not survive mostly belonged to Third class","58a01dd2":"### intercept and coefficient of regression line","0c141367":"### Splitting train and test datset","16c1227d":"Since there are only two missing values, we will impute them with the mode of the Embarked column.","a6a513e3":"### Missing values in `Embarked  ` column","4a6a5551":"### Missing values in `Age` column","3656cdae":"Since `Cabin` has over 77% of missing values, we can drop it because it will not going to add any predictive power to our model.","ffadb15c":"From above equatioin,\n### The *logit* of *probability* gives the linear regression and hence the name LOGISTIC REGRESSION","e6ba7487":"The class distribution in the target variable is 61:38 indicating a *balanced dataset*","47fd4339":"Whether to fill missing values of `Age` with *mean* or *median*, first find out if `Age` has outliers.","d28be178":"## Check for class imbalance","56bd76df":"### Problem 2.\nIn case the dataset contains outliers.","1ae6ed78":"## Encoding categorical features","33529a60":"On solving the Sigmoid Function we get\n$$\\frac{p}{1-p} = e^{ {\\beta}_{0} + {\\beta}_{1}x }$$\n\n$$ log(\\frac{p}{1-p}) = {\\beta}_{0} + {\\beta}_{1}x $$\n\n$$ log(odds) = {\\beta}_{0} + {\\beta}_{1}x $$, here p \/ (1-p) = probability of odds\n\n$$logit (p) = {\\beta}_{0} + {\\beta}_{1}x$$\n And, log(probability of odd)= logit transformation of probability","a0daac72":"## Exploratory data analysis","2dc4b830":"## Handling missing values","88eab67c":"### Separating features and target ","45b60881":"If we plot this dataset, we can see that all the points lie on 0 and 1","80b6d195":"## Mathematics behind logistic regression\nSuppose we have a dataset of whether a person will buy insurance or not. In the dependent variable(bought_insurance), **1** means *he will buy insurance* i.e a *positive class* and **0** means *he will not buy insurance* i.e a *negative class*. \n","b62c1e16":"## Droping unnecessary columns","71e616dc":"Logistic Regression is an *extension* of *Linear regression*, except that, here, the dependent variable is categorical and not continuous. It predicts the **probability** of the outcome variable. <br> <br>\nSo, Logistic Regression in one of the machine learning algorithm to solve **binary classification** problem. <br> \nSome real life classification examples would be\n* to classify the mail as spam or not spam\n* to classify the transaction as fraudulent or genuine\n* whether a person will buy insurance or not <br>\nIn the above examples Logistic Regression estimates the **probability** of an event occurring i.e either mail is spam **(1)** or not spam **(0)**.\n\n\n\n","24d1d1b1":"Seeing the plot we find that there is nothing special about the dependent variable aside from being binary. So we try using Linear Regression by assuming the threshold value as 0.5. Which means that if the **probability(yes) >= 0.5**, the person will buy the insurance and if  **probability(yes) < 0.5**, the person will not buy the insurance.","54ef6f18":"From the plot we can see that because of the outlier, both the ***intercept*** and the ***coefficient*** is changed resulting in the ***shift of regression line***.<br> Now some datapoints which were in *positive class* (probability(yes)>= 0.5) comes in *negative class* (probability(yes)< 0.5), giving the incorrect prediction.","bff05c82":"Since `Age` has outliers, we will fill the missing values with the *median* because outliers can have significant change in the *mean*"}}