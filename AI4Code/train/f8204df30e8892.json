{"cell_type":{"183aee31":"code","6b91bbcd":"code","30b2102c":"code","70bd5d3c":"code","b6d46efa":"code","1fdf5500":"code","f6d0de73":"code","bf4d6ae6":"code","1b8f5eb6":"code","c17e5d34":"code","a96c9e89":"code","e3d978bd":"code","239c0921":"code","57f8e6e7":"code","7786d93b":"code","90c54ef3":"code","8d74a66e":"code","9c78f542":"code","f341aec2":"code","cb554544":"code","89f6ebfb":"code","ed9236b6":"code","73caa2f3":"code","7a1a51e9":"code","f7f00cf3":"code","abcba910":"code","9274af38":"code","448173c0":"code","087d8bea":"code","cbc0e828":"code","1d5c02ba":"code","9c4c78a2":"code","9188b929":"code","d1291820":"code","75cdf37a":"code","2d0eb5f2":"code","6a065af3":"code","4f2a49a9":"code","0d3b04b0":"code","919520da":"code","8b92f4e3":"code","0540fa2a":"code","cba24ef6":"code","01e4b70c":"code","89ddf34a":"code","15a817a9":"code","d8a4bb33":"code","5fd25f97":"code","d8c7ebbe":"code","9b7c3b7a":"code","0147baa6":"code","d46da448":"code","eed14023":"code","8b9e3246":"code","b116e2e1":"code","765ab99b":"code","f831ed60":"code","0b62ac34":"code","cd168658":"code","23265e91":"code","d6b959f3":"code","12114b32":"code","c8ab832a":"code","7e34c3cf":"code","2cb268c9":"code","efc36d87":"code","fe9728e2":"code","bd61949b":"code","984ae5da":"code","d1c93828":"code","825e112f":"code","81eb0265":"code","903ab01a":"code","0d4dcbec":"code","cc037ae4":"code","c7442b50":"code","91a0946a":"code","d5730c9c":"code","0fc20dc2":"code","a7095236":"code","7dd172e3":"code","cf87a975":"code","66f1d9bc":"code","ebe1cfb4":"code","85f78dea":"code","96d9e789":"code","37a5e307":"code","6723cf46":"code","9d869c49":"code","ba8078b5":"code","5c301e87":"code","395baeed":"code","5cbbe663":"code","acbdd3c2":"code","8c43e56c":"code","8295c10a":"code","1670f76c":"code","8a399969":"code","ad2c6340":"code","f283c2cd":"code","152ac97b":"code","8c6d01a7":"code","6476cfed":"code","a04c6b22":"code","43aeb4c6":"code","c7351e7e":"code","7b2b2e24":"code","671d50ca":"code","0f6aa479":"code","1057f520":"code","96084c45":"code","76a1aeda":"code","cafc1d06":"code","f7ddd232":"code","c8d6c40f":"code","3d2656ab":"code","615a5345":"code","7e85752b":"code","37c6e8d8":"code","1ccb2f93":"code","69e14200":"code","8eddce02":"code","25878381":"code","f097d028":"code","898d4359":"code","ef202a3a":"code","6a174f65":"code","241fc428":"code","d8b0ee55":"code","665ad287":"code","a02893fe":"code","2c4e2127":"code","0cb6515e":"code","2587245f":"code","ad24c97e":"code","37438b4f":"code","6d2e6bd5":"code","ab8905ec":"code","d0a91a9e":"code","b328ed75":"code","e37b6c9f":"code","b7066a12":"code","f84fd78d":"code","5136e7df":"code","c24bef89":"code","e29c810c":"code","b75df719":"code","e2ae14ce":"code","2ed204ab":"code","35be14b1":"code","116d4088":"code","0b507559":"code","f787b0bd":"code","d5c84c3b":"code","9abe2d3b":"code","5bb93ad2":"code","407ab660":"code","a4c2428d":"code","a409b30c":"code","b69cd473":"code","858bd08e":"code","6726ddcf":"code","d9277836":"code","2f0ec89a":"code","cd560da8":"code","692ec5da":"code","78c4e1de":"code","a142417e":"code","516b8147":"code","67c32b1e":"code","1ac6b0b6":"code","9f47db02":"code","c2319bd8":"code","4a90e845":"code","dc21b963":"code","24bc4299":"code","7c88dd04":"code","8d4b18c7":"code","15e5834b":"code","c809a9fc":"code","8dafae7e":"code","d9e8ec74":"code","4393f483":"code","ed82efa7":"code","3c5f0819":"code","a3fb6249":"code","d6ca1b46":"code","9a89b98d":"code","7ec39243":"code","2e4d26ee":"code","9172fee0":"code","fc541689":"code","dd554060":"code","4f4ebe52":"code","d59d3ea7":"code","59add045":"code","f13cc5df":"code","2ed23904":"code","ba2f9d79":"code","506748b0":"code","a2e74a73":"code","dcc75be5":"code","a776abbf":"code","8e07bbc0":"code","1539dc38":"code","3fb7c3b4":"code","a5669898":"code","e1287d01":"code","d1b5eb4f":"code","9847ea24":"code","3e0f89fd":"code","b60088af":"code","87e05506":"code","906e38e5":"code","070c71ba":"code","cb8da95a":"code","9fb67b77":"code","a0707698":"code","d5347587":"code","ecdc0222":"code","41f40b68":"code","2a1f6cc0":"code","70f6d815":"code","f6a7b5ec":"code","63d7521c":"code","fffa86b1":"code","1b320cec":"code","ec21a67a":"code","30324d04":"code","2a6ec084":"code","23885c22":"code","b2d42123":"code","0fe8c216":"code","683b6239":"code","05647a94":"code","87b555b7":"code","c6571a50":"code","7ddba65c":"code","79d52c1d":"code","da3a921f":"code","9acdcec1":"code","349dea3c":"code","b9df46a6":"code","63312acc":"code","18e63a40":"code","21e004cd":"code","c72c53c1":"code","14f058b2":"code","b402a174":"code","333d61de":"code","cc68f62b":"code","19c1a7e2":"code","8029f341":"code","d50a6927":"code","d0593fea":"code","4554380d":"code","ec88c35a":"code","19c359f4":"code","5e5e4c2c":"code","a4254a34":"code","b22759b4":"code","5e98f0e1":"code","b0bdd186":"code","50b2bcf7":"code","8feff29b":"code","d76ef75a":"code","e27f5276":"code","78df15e7":"code","058e7e71":"code","6056b916":"code","5e62f796":"code","4b6292bb":"code","37e2f738":"code","c8b9b577":"code","65d80ce2":"code","6b9ef3ac":"code","c8aad006":"code","64a4b47a":"markdown","ec858093":"markdown","da3ae4da":"markdown","4333460d":"markdown","99f48511":"markdown","d937d0c1":"markdown","d9c010c3":"markdown","ab890e67":"markdown","c0843f2b":"markdown","6b4e6459":"markdown","8a21b30e":"markdown","560767b5":"markdown","b60675e8":"markdown","276f86f3":"markdown","a80507e4":"markdown","c69644bb":"markdown","6f0889b5":"markdown","fdd3d313":"markdown","40ad2b3e":"markdown","7c962ec2":"markdown","b37b2c50":"markdown","83124cd7":"markdown","002ba79f":"markdown","1b78c0f3":"markdown","4efbf3fb":"markdown","c251a064":"markdown","b1efea89":"markdown","f077e696":"markdown","5e17c0b6":"markdown","c009802c":"markdown","6a013757":"markdown","c601ebc1":"markdown","46b4767b":"markdown","e81a201d":"markdown","6cb54fe0":"markdown","b3ee624f":"markdown","13a3fc94":"markdown","adf57997":"markdown","f1d1f658":"markdown","b67d478f":"markdown","27bd429f":"markdown","faea1d8c":"markdown","57cb9802":"markdown","f40517cb":"markdown","5adbbdc3":"markdown","4309d2c9":"markdown","2d084763":"markdown","a5336c16":"markdown","7ae836a1":"markdown","7a854785":"markdown","f3129a6c":"markdown","0324c9a4":"markdown","d43d60e8":"markdown","b528db55":"markdown","38b30f03":"markdown","429c6ef8":"markdown","037ae50a":"markdown","623720cf":"markdown","17929076":"markdown","36bc6a25":"markdown","76282880":"markdown","fa0e84b4":"markdown","c91f14b7":"markdown","97bcc8cd":"markdown","4362122c":"markdown","cb7a2af7":"markdown","bfcaf164":"markdown","cbaed93a":"markdown","88313c38":"markdown","d0c9b4dc":"markdown","3e7ae51f":"markdown","2a699275":"markdown","497fd674":"markdown","f3dcb1e4":"markdown","35d6f26d":"markdown","234dfa3e":"markdown","c6f27344":"markdown","95369bf4":"markdown","ac38008b":"markdown","4b3ba047":"markdown","c64111aa":"markdown","9147d307":"markdown","da4c5758":"markdown","e0a2037f":"markdown","fd1d9ced":"markdown","ef3a612a":"markdown","fec5f39a":"markdown","737cd6f6":"markdown","68a09658":"markdown","55b4caf7":"markdown","324d0d09":"markdown","6d969363":"markdown","9484d049":"markdown","56e1a858":"markdown","1ab0a46b":"markdown","c5c400f8":"markdown","7bbadb30":"markdown","448c313e":"markdown","144cee8f":"markdown","2ab966de":"markdown","75e3c0a6":"markdown","349fbeff":"markdown","f2b62c5a":"markdown","86a2f93b":"markdown","5207506b":"markdown","43da4aa9":"markdown","0ceb2147":"markdown","6740e384":"markdown","84bea86d":"markdown","e78426d0":"markdown","c6043ea4":"markdown","136bbcab":"markdown","cb3002f6":"markdown","f24f9a88":"markdown","f616ea1b":"markdown","4f331607":"markdown","8c70742a":"markdown","781838fc":"markdown","dc5bccc6":"markdown","97727d88":"markdown","73f46f09":"markdown","5df9c4f2":"markdown","d282ceba":"markdown","3c643128":"markdown"},"source":{"183aee31":"import pandas as pd","6b91bbcd":"import numpy as np","30b2102c":"from matplotlib import pyplot as plt","70bd5d3c":"import seaborn as sns","b6d46efa":"import statsmodels.api as sm","1fdf5500":"import statsmodels.formula.api as smf","f6d0de73":"from sklearn.linear_model import LinearRegression","bf4d6ae6":"from sklearn.metrics import mean_squared_error,r2_score","1b8f5eb6":"from sklearn.model_selection import train_test_split,cross_val_score,cross_val_predict","c17e5d34":"from sklearn.decomposition import PCA","a96c9e89":"from sklearn.cross_decomposition import PLSRegression,PLSSVD","e3d978bd":"from sklearn.preprocessing import scale","239c0921":"from sklearn import model_selection","57f8e6e7":"from sklearn.linear_model import Ridge,Lasso,ElasticNet","7786d93b":"from sklearn.linear_model import RidgeCV,LassoCV,ElasticNetCV","90c54ef3":"ads = pd.read_csv(\"..\/input\/advertising-dataset\/advertising.csv\")","8d74a66e":"ads.head()","9c78f542":"ads = ads[['TV', 'Radio', 'Newspaper', 'Sales']]","f341aec2":"ads.rename(columns={\"Radio\": \"radio\", \"Newspaper\": \"newspaper\",\"Sales\": \"sales\"},inplace=True)","cb554544":"ads.head()","89f6ebfb":"ads.describe().T","ed9236b6":"ads.dtypes","73caa2f3":"ads.shape","7a1a51e9":"ads.isna().sum()","f7f00cf3":"ads.corr()","abcba910":"g= sns.pairplot(ads,kind=\"reg\",diag_kws= {'color': 'red'})\n\ng.fig.suptitle(\"Correlation of Advertising Dataset\", y=1.08)\n\nplt.show()","9274af38":"sns.jointplot(x=\"TV\", y=\"sales\",data=ads,kind=\"reg\",color=\"green\")\n\nplt.show()","448173c0":"X = ads.TV\nX = sm.add_constant(X) # It will add a constant to X.\nX.head()","087d8bea":"y = ads.sales # Dependent Variable -Target","cbc0e828":"slr = sm.OLS(y,X) ","1d5c02ba":"model = slr.fit()","9c4c78a2":"model.summary()","9188b929":"model.params # Main parameters","d1291820":"model.summary().tables[1]","75cdf37a":"model.conf_int()","2d0eb5f2":"#Signifigant Level - P value\n\nmodel.f_pvalue","6a065af3":"print(\"f_pvalue: \", \"%.4f\" % model.f_pvalue)","4f2a49a9":"model.fvalue","0d3b04b0":"model.tvalues","919520da":"#Sum of squares error of model\n\nmodel.mse_model #This is very bad","8b92f4e3":"model.rsquared","0540fa2a":"model.rsquared_adj","cba24ef6":"#Predicted Values\nmodel.fittedvalues[:5]","01e4b70c":"#real values\ny[:5]","89ddf34a":"#Manual formula of our model\nprint(\"Sales: \" , model.params[0] , \"+ (TV*\",model.params[1],\")\")","15a817a9":"ax = sns.regplot(ads[\"TV\"],ads[\"sales\"],ci=None,scatter_kws={\"color\":\"purple\"},marker=\"x\")\nax.set_title(\"Sales:  7.03259 + TV*0.04753\")\nax.set_ylabel(\"# of Sales\")\nax.set_xlabel(\"Expenses for TV\")\n\nplt.show()","d8a4bb33":"X = ads[[\"TV\"]]\nsm.add_constant(X)\ny = ads[\"sales\"]","5fd25f97":"lr = LinearRegression()\nmodel = lr.fit(X,y)","d8c7ebbe":"model.coef_.item()","9b7c3b7a":"model.intercept_","0147baa6":"model.score(X,y) #R Squared","d46da448":"model.predict(X)[:10]","eed14023":"model.predict([[20]])","8b9e3246":"model.predict([[43],[20],[32]])","b116e2e1":"slr = sm.OLS(y,X) \nmodel = slr.fit()\nmodel.summary()","765ab99b":"mean_square = mean_squared_error(y,model.fittedvalues)\nmean_square","f831ed60":"rmse = np.sqrt(mean_square)\nrmse","0b62ac34":"results = pd.DataFrame({\"Real\": y, \"Prediction\": model.predict(X),\n                        \"Residuals\": y -(model.predict(X)),\n                        \"Square of Residuals\": (y -(model.predict(X)))**2})","cd168658":"results.head(10)","23265e91":"np.sum(results[\"Square of Residuals\"]) # root mean_square error","d6b959f3":"np.mean(results[\"Square of Residuals\"]) # mean_square error","12114b32":"model.resid[:10]","c8ab832a":"plt.plot(model.resid,c=\"r\")\nplt.title(\"Plot of Residuals\")\nplt.show()","7e34c3cf":"ads = pd.read_csv(\"..\/input\/advertising-dataset\/advertising.csv\")\nads.rename(columns={\"Radio\": \"radio\", \"Newspaper\": \"newspaper\",\"Sales\": \"sales\"},inplace=True)","2cb268c9":"ads = ads[['TV', 'radio', 'newspaper', 'sales']]\nads.head()","efc36d87":"ads.shape","fe9728e2":"X = ads.drop(\"sales\",axis=1)\nX.head()","bd61949b":"y = ads[\"sales\"]\ny[:5]","984ae5da":"ads.shape","d1c93828":"X_train = X.iloc[:160]\nX_test = X.iloc[160:]\ny_train = y[:160]\ny_test = y[160:]","825e112f":"print(\"X_train Shape: \",X_train.shape)\nprint(\"X_test Shape: \",X_test.shape)\nprint(\"y_train Shape: \",y_train.shape)\nprint(\"y_test Shape: \",y_test.shape)","81eb0265":"mlr = sm.OLS(y_train,X_train)\nmodel = mlr.fit()\nmodel.summary()","903ab01a":"mlr = LinearRegression()\nmodel = mlr.fit(X_train,y_train)","0d4dcbec":"print(\"Intercept of Model-Bias: \",model.intercept_)\nprint(\"Coefficients of Model: \",model.coef_)","cc037ae4":"print(\"Sales:\", model.intercept_ ,\" + \",\n      model.coef_[0],\"* TV +\",\n      model.coef_[1],\"* Radio +\",\n      model.coef_[2],\"* Newspaper\")","c7442b50":"model.predict(pd.DataFrame([[35],[20],[45]]).T).item()","91a0946a":"train_root_mean_square = np.sqrt(mean_squared_error(y_train,model.predict(X_train)))","d5730c9c":"print(\"root_mean_square_error of Training Set: \",train_root_mean_square)","0fc20dc2":"test_root_mean_square = np.sqrt(mean_squared_error(y_test,model.predict(X_test)))","a7095236":"print(\"root_mean_square_error of Test Set: \",test_root_mean_square)","7dd172e3":"X = ads.drop(\"sales\",axis=1)\ny = ads[\"sales\"]\n\nX_train = X.iloc[:160]\nX_test = X.iloc[160:]\ny_train = y[:160]\ny_test = y[160:]","cf87a975":"mlr = LinearRegression()\nmodel = mlr.fit(X_train,y_train)","66f1d9bc":"np.sqrt(mean_squared_error(y_train,model.predict(X_train)))","ebe1cfb4":"model.score(X_train,y_train)","85f78dea":"cross_val_score(model,X,y,cv=10,scoring=\"r2\").mean()","96d9e789":"#Train Rsquare\ncross_val_score(model,X_train,y_train,cv=20,scoring=\"r2\").mean()","37a5e307":"np.sqrt(-cross_val_score(model,X_train,y_train,cv=20,scoring=\"neg_mean_squared_error\"))","6723cf46":"np.sqrt(-cross_val_score(model,X_train,y_train,cv=20,scoring=\"neg_mean_squared_error\")).mean()","9d869c49":"#Test Rsquare\ncross_val_score(model,X_test,y_test,cv=20,scoring=\"r2\").mean()","ba8078b5":"np.sqrt(-cross_val_score(model,X_test,y_test,cv=20,scoring=\"neg_mean_squared_error\"))","5c301e87":"np.sqrt(-cross_val_score(model,X_test,y_test,cv=20,scoring=\"neg_mean_squared_error\")).mean()","395baeed":"hts = pd.read_csv(\"..\/input\/hitters\/Hitters.csv\")\nhts.head()","5cbbe663":"hts.dropna(inplace=True)","acbdd3c2":"hts.describe().T","8c43e56c":"one_hot_encoded = pd.get_dummies(hts[[\"League\",\"Division\",\"NewLeague\"]])\none_hot_encoded.head()","8295c10a":"new_hts = hts.drop([\"League\",\"Division\",\"NewLeague\",\"Salary\"],axis=1).astype(\"float64\")","1670f76c":"new_hts.head()","8a399969":"X = pd.concat([new_hts,one_hot_encoded[[\"League_N\",\"Division_W\",\"NewLeague_N\"]]],axis=1)\nX.head()","ad2c6340":"y = hts.Salary # Target-dependent variable","f283c2cd":"X.shape","152ac97b":"y.shape","8c6d01a7":"X_train = X.iloc[:200]\nX_test = X.iloc[200:]\ny_train = y[:200]\ny_test = y[200:]\n\nprint(\"X_train Shape: \",X_train.shape)\nprint(\"X_test Shape: \",X_test.shape)\nprint(\"y_train Shape: \",y_train.shape)\nprint(\"y_test Shape: \",y_test.shape)","6476cfed":"pca = PCA()","a04c6b22":"X_reduced_train = pca.fit_transform(scale(X_train))\nX_reduced_test = pca.fit_transform(scale(X_test))","43aeb4c6":"X_reduced_train[:5]","c7351e7e":"np.cumsum(np.round(pca.explained_variance_ratio_,decimals=3)*100)[:6]","7b2b2e24":"hts = pd.read_csv(\"..\/input\/hitters\/Hitters.csv\")\nhts.head()","671d50ca":"hts.dropna(inplace=True)","0f6aa479":"hts.describe().T","1057f520":"one_hot_encoded = pd.get_dummies(hts[[\"League\",\"Division\",\"NewLeague\"]])\none_hot_encoded.head()","96084c45":"new_hts = hts.drop([\"League\",\"Division\",\"NewLeague\",\"Salary\"],axis=1).astype(\"float64\")","76a1aeda":"X = pd.concat([new_hts,one_hot_encoded[[\"League_N\",\"Division_W\",\"NewLeague_N\"]]],axis=1)\nX.head()","cafc1d06":"y = hts.Salary # Target-dependent variable","f7ddd232":"hts.shape","c8d6c40f":"#Independent Variables\nX.shape","3d2656ab":"#Dependent Variables\ny.shape","615a5345":"X_train = X.iloc[:210]\nX_test = X.iloc[210:]\ny_train = y[:210]\ny_test = y[210:]\n\nprint(\"X_train Shape: \",X_train.shape)\nprint(\"X_test Shape: \",X_test.shape)\nprint(\"y_train Shape: \",y_train.shape)\nprint(\"y_test Shape: \",y_test.shape)","7e85752b":"pca = PCA()","37c6e8d8":"X_reduced_train = pca.fit_transform(scale(X_train))\nX_reduced_test = pca.fit_transform(scale(X_test))","1ccb2f93":"np.cumsum(np.round(pca.explained_variance_ratio_,decimals=3)*100)[:6]","69e14200":"pcr = LinearRegression()\npcr_model = pcr.fit(X_reduced_train,y_train)","8eddce02":"print(\"Intercept: \",pcr_model.intercept_)\nprint(\"Coefficients: \",pcr_model.coef_)","25878381":"y_pred = pcr_model.predict(X_reduced_train)\ny_pred[:10]","f097d028":"#root mean square error for Train Set\nnp.sqrt(mean_squared_error(y_train,y_pred))","898d4359":"#r2 score for Train Set\nr2_score(y_train,y_pred)","ef202a3a":"y_pred = pcr_model.predict(X_reduced_test)\ny_pred[:10]","6a174f65":"#root mean square error for Test Set\nnp.sqrt(mean_squared_error(y_test,y_pred))","241fc428":"#r2 score for Test Set\nr2_score(y_test,y_pred)","d8b0ee55":"pcr = LinearRegression()\npcr_model = pcr.fit(X_reduced_train,y_train)\ny_pred = pcr_model.predict(X_reduced_test)\nprint(\"Root mean square error: \",np.sqrt(mean_squared_error(y_test,y_pred)))","665ad287":"pcr_model = pcr.fit(X_reduced_train[:,:3],y_train)\ny_pred = pcr_model.predict(X_reduced_test[:,:3])\nprint(\"Root mean square error: \",np.sqrt(mean_squared_error(y_test,y_pred)))","a02893fe":"cross_val = model_selection.KFold(n_splits=7,\n                                  shuffle=True,\n                                  random_state=45)\npcr = LinearRegression()\nRoot_mean_sqaure_error = []","2c4e2127":"X_reduced_train.shape","0cb6515e":"for num in np.arange(X_reduced_train.shape[1]+1):\n    score = np.sqrt(-1*model_selection.cross_val_score(pcr,X_reduced_train[:,:num],y_train.ravel(),\n                                                       cv=cross_val,scoring=\"neg_mean_squared_error\")).mean()\n    \n    Root_mean_sqaure_error.append(score)","2587245f":"plt.plot(Root_mean_sqaure_error,\"-v\",c=\"r\")\nplt.xlabel(\"Component Numbers\")\nplt.ylabel(\"Root_mean_sqaure_error\")\nplt.title(\"PCR Model Tuning\")\n\nplt.show()","ad24c97e":"pcr = LinearRegression()\npcr_model = pcr.fit(X_reduced_train[:,:6],y_train)","37438b4f":"y_pred = pcr_model.predict(X_reduced_train[:,:6])\nprint(\"Root mean square error for Train set: \",np.sqrt(mean_squared_error(y_train,y_pred)))","6d2e6bd5":"y_pred = pcr_model.predict(X_reduced_test[:,:6])\nprint(\"Root mean square error for Text set: \",np.sqrt(mean_squared_error(y_test,y_pred)))","ab8905ec":"hts = pd.read_csv(\"..\/input\/hitters\/Hitters.csv\")\nhts.head()","d0a91a9e":"hts.dropna(inplace=True)","b328ed75":"hts.describe().T","e37b6c9f":"one_hot_encoded = pd.get_dummies(hts[[\"League\",\"Division\",\"NewLeague\"]])\none_hot_encoded.head()","b7066a12":"new_hts = hts.drop([\"League\",\"Division\",\"NewLeague\",\"Salary\"],axis=1).astype(\"float64\")","f84fd78d":"X = pd.concat([new_hts,one_hot_encoded[[\"League_N\",\"Division_W\",\"NewLeague_N\"]]],axis=1)\nX.head()","5136e7df":"y = hts.Salary # Target-dependent variable","c24bef89":"hts.shape","e29c810c":"#Independent Variables\nX.shape","b75df719":"#Dependent Variables\ny.shape","e2ae14ce":"X_train = X.iloc[:210]\nX_test = X.iloc[210:]\ny_train = y[:210]\ny_test = y[210:]\n\nprint(\"X_train Shape: \",X_train.shape)\nprint(\"X_test Shape: \",X_test.shape)\nprint(\"y_train Shape: \",y_train.shape)\nprint(\"y_test Shape: \",y_test.shape)","2ed204ab":"pls_model = PLSRegression(n_components=7).fit(X_train,y_train)","35be14b1":"pls_model.coef_","116d4088":"pls_model","0b507559":"X_train.head()","f787b0bd":"pls_model.predict(X_train)[:10]","d5c84c3b":"y_pred=pls_model.predict(X_train)","9abe2d3b":"#Train Error\nnp.sqrt(mean_squared_error(y_train,y_pred))","5bb93ad2":"r2_score(y_train,y_pred)","407ab660":"y_pred=pls_model.predict(X_test)","a4c2428d":"#Test Error\nnp.sqrt(mean_squared_error(y_test,y_pred))","a409b30c":"r2_score(y_test,y_pred)","b69cd473":"pls_model","858bd08e":"cross_val = model_selection.KFold(n_splits=15,\n                                  shuffle=True,\n                                  random_state=45)\nRoot_mean_sqaure_error = []","6726ddcf":"for num in np.arange(1,X_train.shape[1]+1):\n    pls= PLSRegression(n_components=num)\n    score = np.sqrt(-1*model_selection.cross_val_score(pls,X_train,y_train,\n                                                       cv=cross_val,scoring=\"neg_mean_squared_error\")).mean()\n    \n    Root_mean_sqaure_error.append(score)","d9277836":"len(Root_mean_sqaure_error)","2f0ec89a":"X_train.shape[1]","cd560da8":"plt.plot(np.arange(1,X_train.shape[1]+1),np.array(Root_mean_sqaure_error),\"-v\",c=\"g\")\nplt.xlabel(\"Component Numbers\")\nplt.ylabel(\"Root_mean_sqaure_error\")\nplt.title(\"PLS Model Tuning\")\n\nplt.show()","692ec5da":"pls_model = PLSRegression(n_components=8).fit(X_train,y_train)","78c4e1de":"y_pred=pls_model.predict(X_train)","a142417e":"#Train Error\nnp.sqrt(mean_squared_error(y_train,y_pred))","516b8147":"r2_score(y_train,y_pred)","67c32b1e":"y_pred=pls_model.predict(X_test)","1ac6b0b6":"#Test Error\nnp.sqrt(mean_squared_error(y_test,y_pred))","9f47db02":"r2_score(y_test,y_pred)","c2319bd8":"hts = pd.read_csv(\"..\/input\/hitters\/Hitters.csv\")\nhts.head()","4a90e845":"hts.dropna(inplace=True)","dc21b963":"hts.describe().T","24bc4299":"one_hot_encoded = pd.get_dummies(hts[[\"League\",\"Division\",\"NewLeague\"]])\none_hot_encoded.head()","7c88dd04":"new_hts = hts.drop([\"League\",\"Division\",\"NewLeague\",\"Salary\"],axis=1).astype(\"float64\")","8d4b18c7":"X = pd.concat([new_hts,one_hot_encoded[[\"League_N\",\"Division_W\",\"NewLeague_N\"]]],axis=1)\nX.head()","15e5834b":"y = hts.Salary # Target-dependent variable","c809a9fc":"hts.shape","8dafae7e":"#Independent Variables\nX.shape","d9e8ec74":"#Dependent Variables\ny.shape","4393f483":"X_train = X.iloc[:210]\nX_test = X.iloc[210:]\ny_train = y[:210]\ny_test = y[210:]\n\nprint(\"X_train Shape: \",X_train.shape)\nprint(\"X_test Shape: \",X_test.shape)\nprint(\"y_train Shape: \",y_train.shape)\nprint(\"y_test Shape: \",y_test.shape)","ed82efa7":"ridge_model = Ridge(alpha=0.2).fit(X_train,y_train)","3c5f0819":"ridge_model","a3fb6249":"ridge_model.coef_","d6ca1b46":"lambda_values= 10**np.linspace(5,-2,150)*0.5\nridge_model = Ridge()\ncoefficients = []\n\nfor lam in lambda_values:\n    ridge_model.set_params(alpha=lam)\n    ridge_model.fit(X_train,y_train)\n    coefficients.append(ridge_model.coef_)","9a89b98d":"lambda_values[:10]","7ec39243":"coefficients[:3]","2e4d26ee":"ax = plt.gca()\nax.plot(lambda_values,coefficients)\nax.set_xscale(\"log\")\n\nplt.xlabel(\"Lambda Values\")\nplt.ylabel(\"Coefficients\")\nplt.title(\"Ridge Coefficients\")\nplt.show()","9172fee0":"ridge_model","fc541689":"ridge_model.predict(X_train)[:10]","dd554060":"y_pred=ridge_model.predict(X_train)","4f4ebe52":"#Train Error\nnp.sqrt(mean_squared_error(y_train,y_pred))","d59d3ea7":"r2_score(y_train,y_pred)","59add045":"y_pred=ridge_model.predict(X_test)","f13cc5df":"#Test Error\nnp.sqrt(mean_squared_error(y_test,y_pred))","2ed23904":"r2_score(y_test,y_pred)","ba2f9d79":"lambda_values= 10**np.linspace(5,-2,150)*0.5","506748b0":"lambda_values[:10]","a2e74a73":"Ridge_cv = RidgeCV(alphas=lambda_values,\n                   scoring=\"neg_mean_squared_error\",\n                   normalize=True)","dcc75be5":"Ridge_cv.fit(X_train,y_train)","a776abbf":"Ridge_cv.alpha_","8e07bbc0":"ridge_tuned = Ridge(alpha=Ridge_cv.alpha_,normalize=True).fit(X_train,y_train)","1539dc38":"y_pred=ridge_tuned.predict(X_train)","3fb7c3b4":"#Train Error\nnp.sqrt(mean_squared_error(y_train,y_pred))","a5669898":"r2_score(y_train,y_pred)","e1287d01":"y_pred=ridge_tuned.predict(X_test)","d1b5eb4f":"#Test Error\nnp.sqrt(mean_squared_error(y_test,y_pred))","9847ea24":"r2_score(y_test,y_pred)","3e0f89fd":"hts = pd.read_csv(\"..\/input\/hitters\/Hitters.csv\")\nhts.head()","b60088af":"hts.dropna(inplace=True)","87e05506":"hts.describe().T","906e38e5":"one_hot_encoded = pd.get_dummies(hts[[\"League\",\"Division\",\"NewLeague\"]])\none_hot_encoded.head()","070c71ba":"new_hts = hts.drop([\"League\",\"Division\",\"NewLeague\",\"Salary\"],axis=1).astype(\"float64\")","cb8da95a":"X = pd.concat([new_hts,one_hot_encoded[[\"League_N\",\"Division_W\",\"NewLeague_N\"]]],axis=1)\nX.head()","9fb67b77":"y = hts.Salary # Target-dependent variable","a0707698":"hts.shape","d5347587":"#Independent Variables\nX.shape","ecdc0222":"#Dependent Variables\ny.shape","41f40b68":"X_train = X.iloc[:210]\nX_test = X.iloc[210:]\ny_train = y[:210]\ny_test = y[210:]\n\nprint(\"X_train Shape: \",X_train.shape)\nprint(\"X_test Shape: \",X_test.shape)\nprint(\"y_train Shape: \",y_train.shape)\nprint(\"y_test Shape: \",y_test.shape)","2a1f6cc0":"lasso_model = Lasso(alpha=0.1).fit(X_train,y_train)","70f6d815":"lasso_model","f6a7b5ec":"lasso_model.coef_","63d7521c":"lambda_values= 10**np.linspace(5,-2,150)*0.5\nlasso_model = Lasso()\ncoefficients = []\n\nfor lam in lambda_values:\n    lasso_model.set_params(alpha=lam)\n    lasso_model.fit(X_train,y_train)\n    coefficients.append(lasso_model.coef_)","fffa86b1":"ax = plt.gca()\nax.plot(lambda_values*2,coefficients)\nax.set_xscale(\"log\")\n\nplt.axis(\"tight\")\nplt.xlabel(\"Lambda Values - Alpha\")\nplt.ylabel(\"Coefficients - Weights\")\nplt.title(\"Lasso Coefficients\")\nplt.show()","1b320cec":"lasso_model","ec21a67a":"lasso_model.predict(X_train)[:10]","30324d04":"y_pred=lasso_model.predict(X_train)","2a6ec084":"#Train Error\nnp.sqrt(mean_squared_error(y_train,y_pred))","23885c22":"r2_score(y_train,y_pred)","b2d42123":"y_pred=lasso_model.predict(X_test)","0fe8c216":"#Test Error\nnp.sqrt(mean_squared_error(y_test,y_pred))","683b6239":"r2_score(y_test,y_pred)","05647a94":"Lasso_cv = LassoCV(alphas=None,\n                   cv=15,\n                   max_iter=15000,\n                   normalize=True)","87b555b7":"Lasso_cv.fit(X_train,y_train)","c6571a50":"Lasso_cv.alpha_","7ddba65c":"lasso_tuned = Lasso(alpha=Lasso_cv.alpha_).fit(X_train,y_train)","79d52c1d":"y_pred=lasso_tuned.predict(X_train)","da3a921f":"#Train Error\nnp.sqrt(mean_squared_error(y_train,y_pred))","9acdcec1":"r2_score(y_train,y_pred)","349dea3c":"y_pred=lasso_tuned.predict(X_test)","b9df46a6":"#Test Error\nnp.sqrt(mean_squared_error(y_test,y_pred))","63312acc":"r2_score(y_test,y_pred)","18e63a40":"hts = pd.read_csv(\"..\/input\/hitters\/Hitters.csv\")\nhts.head()","21e004cd":"hts.dropna(inplace=True)","c72c53c1":"hts.describe().T","14f058b2":"one_hot_encoded = pd.get_dummies(hts[[\"League\",\"Division\",\"NewLeague\"]])\none_hot_encoded.head()","b402a174":"new_hts = hts.drop([\"League\",\"Division\",\"NewLeague\",\"Salary\"],axis=1).astype(\"float64\")","333d61de":"X = pd.concat([new_hts,one_hot_encoded[[\"League_N\",\"Division_W\",\"NewLeague_N\"]]],axis=1)\nX.head()","cc68f62b":"y = hts.Salary # Target-dependent variable","19c1a7e2":"hts.shape","8029f341":"#Independent Variables\nX.shape","d50a6927":"#Dependent Variables\ny.shape","d0593fea":"X_train = X.iloc[:210]\nX_test = X.iloc[210:]\ny_train = y[:210]\ny_test = y[210:]\n\nprint(\"X_train Shape: \",X_train.shape)\nprint(\"X_test Shape: \",X_test.shape)\nprint(\"y_train Shape: \",y_train.shape)\nprint(\"y_test Shape: \",y_test.shape)","4554380d":"elastic_net_model = ElasticNet().fit(X_train,y_train)","ec88c35a":"elastic_net_model","19c359f4":"elastic_net_model.coef_","5e5e4c2c":"elastic_net_model.intercept_","a4254a34":"elastic_net_model","b22759b4":"elastic_net_model.predict(X_train)[:10]","5e98f0e1":"y_pred=elastic_net_model.predict(X_train)","b0bdd186":"#Train Error\nnp.sqrt(mean_squared_error(y_train,y_pred))","50b2bcf7":"r2_score(y_train,y_pred)","8feff29b":"y_pred=elastic_net_model.predict(X_test)","d76ef75a":"#Test Error\nnp.sqrt(mean_squared_error(y_test,y_pred))","e27f5276":"r2_score(y_test,y_pred)","78df15e7":"elastic_net_cv = ElasticNetCV(cv=15,random_state=42)","058e7e71":"elastic_net_cv.fit(X_train,y_train)","6056b916":"elastic_net_cv.alpha_","5e62f796":"elastic_net_tuned = ElasticNet(alpha=elastic_net_cv.alpha_).fit(X_train,y_train)","4b6292bb":"y_pred=elastic_net_tuned.predict(X_train)","37e2f738":"#Train Error\nnp.sqrt(mean_squared_error(y_train,y_pred))","c8b9b577":"r2_score(y_train,y_pred)","65d80ce2":"y_pred=elastic_net_tuned.predict(X_test)","6b9ef3ac":"#Test Error\nnp.sqrt(mean_squared_error(y_test,y_pred))","c8aad006":"r2_score(y_test,y_pred)","64a4b47a":"Now we will remove NA values.","ec858093":"Rsquare is explanation rate. Results means we explain 0.98 of sales variable.","da3ae4da":"Principal Component Analysis, or PCA for short, is a method for reducing the dimensionality of data.\n\nIt can be thought of as a projection method where data with m-columns (features) is projected into a subspace with m or fewer columns, whilst retaining the essence of the original data.\n\nThe PCA method can be described and implemented using the tools of linear algebra.","4333460d":"Let's see correlations between variables.","99f48511":"##### Model","d937d0c1":"Now we will create our last model with optimal alpha value that seems 0.009568617603791272.","d9c010c3":"#### Model","ab890e67":"#### Model","c0843f2b":"Least Absolute Shrinkage and Selection Operator Regression (usually simply called Lasso Regression) is another regularized version of Linear Regression: just like Ridge Regression, it adds a regularization term to the cost function, but it uses the \u21131 norm of the weight vector instead of half the square of the \u21132 norm. This penalty allows some coefficient values to go to the value of zero, allowing input variables to be effectively removed from the model, providing a type of automatic feature selection.","6b4e6459":"Now we will remove NA values.","8a21b30e":"Now we will create our last model with optimal value that seems 8.","560767b5":"#### Model Tuning","b60675e8":"## Content\n\n- **Linear Methods for Regression**\n    - What is Linear Regression?\n    - Simple Linear Regression (Theory - Model- Tuning)\n    - Multiple Linear Regression (Theory - Model- Tuning)\n    - Least-Squares Regression(Ordinary Least Squares) (Theory - Model- Tuning)\n    - Principal Component Analysis (PCA) \n    - Principal component regression(PCR) (Theory - Model- Tuning)\n    - Shrinkage(Regularization) Methods\n        - Partial Least Squares (Theory - Model- Tuning)\n        - Ridge Regression(L2 Regularization) (Theory - Model- Tuning)\n        - Lasso Regression(L1 Regularization) (Theory - Model- Tuning)\n        - Elastic Net Regression (Theory - Model- Tuning)\n","276f86f3":"Let's select all independent variables.","a80507e4":"Let's do dimensionality reduction.","c69644bb":"#### Prediction","6f0889b5":"### Principal Component Analysis (PCA) ","fdd3d313":"Optimum value seems 6. Now we will create a model with 6 components.","40ad2b3e":"![image.png](attachment:image.png)","7c962ec2":"![image.png](attachment:image.png)","b37b2c50":"For a real world example, we will work with **Hitters** dataset.\n\nIt can be downloaded here: https:\/\/www.kaggle.com\/floser\/hitters","83124cd7":"This photo is taken by: https:\/\/miro.medium.com\/max\/2366\/1*tQkyTR9yxDcS1GKVFhdQQA.jpeg","002ba79f":"##### Prediction","1b78c0f3":"### Multiple Linear Regression","4efbf3fb":"Now we will remove NA values.","c251a064":"Now we will split our dataset as train and test set.","b1efea89":"##### Model","f077e696":"Let's see cumulative explanatioon rates.","5e17c0b6":"#### Lasso Regression( L1 Regularization)","c009802c":"We will do **One Hot Encoding** to categorical columns.","6a013757":"Now we will create our last model with optimal alpha value that seems 0.07340278835886885.","c601ebc1":"Let's create a model.","46b4767b":"We will do **One Hot Encoding** to categorical columns.","e81a201d":"#### Ridge Regression ( L2 Regularization)","6cb54fe0":"We will do **One Hot Encoding** to categorical columns.","b3ee624f":"## Linear Methods for Regression","13a3fc94":"A good way to reduce overfitting is to regularize the model (i.e., to constrain it): the fewer degrees of freedom it has, the harder it will be for it to overfit the data. A simple way to regularize a polynomial model is to reduce the number of polynomial degrees.\n\nFor a linear model, regularization is typically achieved by constraining the weights of the model. We will now look at Partial Least Squares, Ridge Regression, Lasso Regression, and Elastic Net, which implement four different ways to constrain the weights.","adf57997":"For a real world example, we will work with **Hitters** dataset.\n\nIt can be downloaded here: https:\/\/www.kaggle.com\/floser\/hitters","f1d1f658":"We will do **One Hot Encoding** to categorical columns.","b67d478f":"**Created by Berkay Alan**\n\n**Linear Regression**\n\n**19 July 2021**\n\n**For more Tutorial:** https:\/\/github.com\/berkayalan","27bd429f":"#### Partial Least Squares (PLS)","faea1d8c":"### What is Linear Regression?","57cb9802":"Ridge Regression is a regularized version of Linear Regression. Ridge Regression, like Linear Regression, aims to minimize the Residual Sum of Squares(RSS) but with a slight change. While Linear Regression estimates the coefficients using the values that minimize the following equation:","f40517cb":"Simple linear regression is used to estimate the relationship between two quantitative variables. We can use simple linear regression when WE want to know:\n\n1. How strong the relationship is between two variables (e.g. the relationship between rainfall and soil erosion).\n\n2. The value of the dependent variable at a certain value of the independent variable (e.g. the amount of soil erosion at a certain level of rainfall).\n\n**Example** \n\nI am a social researcher interested in the relationship between income and happiness. I survey 500 people whose incomes range from *$15k* to *$125k* and ask them to rank their happiness on a scale from 1 to 10.\n\nMy independent variable (income) and dependent variable (happiness) are both quantitative, so I can do a regression analysis to see if there is a linear relationship between them.","5adbbdc3":"For a real world example, we will work with **Hitters** dataset.\n\nIt can be downloaded here: https:\/\/www.kaggle.com\/floser\/hitters","4309d2c9":"##### Model Tuning","2d084763":"For a real world example, we will use *advertising* dataset.\n\nIt can be downloaded from here: https:\/\/www.kaggle.com\/ashydv\/advertising-dataset","a5336c16":"For a real world example, we will work with **Hitters** dataset.\n\nIt can be downloaded here: https:\/\/www.kaggle.com\/floser\/hitters","7ae836a1":"##### Theory","7a854785":"Now we will split our dataset as train and test set.","f3129a6c":"For a real world example, we will work with **Hitters** dataset.\n\nIt can be downloaded here: https:\/\/www.kaggle.com\/floser\/hitters","0324c9a4":"Now we will create a model with **sklearn** library.","d43d60e8":"![Screen%20Shot%202021-07-16%20at%2008.39.49.png](attachment:Screen%20Shot%202021-07-16%20at%2008.39.49.png)\n\nPhoto is cited by here: http:\/\/busigence.com\/blog\/shrinkage-methods-in-linear-regression\/","b528db55":"#### Model","38b30f03":"##### Model","429c6ef8":"##### Model Tuning","037ae50a":"#### Residuals","623720cf":"#### Elastic Net Regression","17929076":"For a real world example, we will use *advertising* dataset.\n\nIt can be downloaded from here: https:\/\/www.kaggle.com\/ashydv\/advertising-dataset","36bc6a25":"This image is cited by: https:\/\/www.researchgate.net\/profile\/Hieu-Tran-17\/publication\/340271573\/figure\/fig3\/AS:874657431437319@1585545990533\/Linear-Regression-model-sample-illustration.ppm","76282880":"Now we will split our dataset as train and test set.","fa0e84b4":"#### Theory","c91f14b7":"Now we will remove NA values.","97bcc8cd":"##### Theory","4362122c":"![image.png](attachment:image.png)\n\nPhoto is cited by: https:\/\/www.datacamp.com\/community\/tutorials\/tutorial-ridge-lasso-elastic-net","cb7a2af7":"### Summary","bfcaf164":"Partial Least Squares regression (PLS) is a quick, efficient and optimal regression method based on covariance. It is recommended in cases of regression where the number of explanatory variables is high, and where it is likely that the explanatory variables are correlated.\n\nPLS helps to handle the case of a large number of correlated independent variables, which is common in chemometrics. Itis an alternative to ordinary least squares (OLS) regression. Firstly it extracts a set of latent factors that explain as much of the covariance as possible between the independent and dependent variables. Then a regression step predicts values of the dependent variables using the decomposition of the independent variables.\n\nPLS is a supevised dimensiion reduction method.","cbaed93a":"Now we will split our dataset as train and test set.","88313c38":"\n\n| Model | Train Error | Test Error |\n| --- | --- | --- |\n| Partial Least Squares(PLS) | 309.05 | 330.64 |\n| Ridge Regression(L2 Regularization) | 306.60 | 327.87 |\n| Lasso Regression(L1 Regularization) | 303.25 | 336.56 |\n| Elastic Net Regression | 326.91 | 328.92 |\n","d0c9b4dc":"##### Prediction","3e7ae51f":"Now we will create our last model with optimal alpha value that seems 1116.4729085556469.","2a699275":"#### Theory","497fd674":"Let's do dimensionality reduction.","f3dcb1e4":"First we will create our model with **statsmodel**.","35d6f26d":"Let's try different dimensions.","234dfa3e":" Elastic Net aims at minimizing the following loss function:","c6f27344":"## Resources","95369bf4":"Now we will remove NA values.","ac38008b":"Let's see correlations in pairplot.","4b3ba047":"![Screen%20Shot%202021-07-16%20at%2008.41.36.png](attachment:Screen%20Shot%202021-07-16%20at%2008.41.36.png)\n\nPhoto is cited by here: http:\/\/busigence.com\/blog\/shrinkage-methods-in-linear-regression\/","c64111aa":"Now we will create a simple linear regression model by using **statsmodel** library.","9147d307":"##### Model Tuning","da4c5758":"Let's calculate validated r2 score.","e0a2037f":"##### Theory","fd1d9ced":"#### Model","ef3a612a":"##### Model","fec5f39a":"The hyperparameter *lambda(\ud835\udf06)* controls how much we want to regularize the model. If  \ud835\udf06 = 0, then Ridge Regression is just Linear Regression. If \u03b1 is very large, then all weights end up very close to zero and the result is a flat line going through the data\u2019s mean.","737cd6f6":"Multiple linear regression is used to estimate the relationship between two or more independent variables and one dependent variable. You can use multiple linear regression when you want to know:\n\n1. How strong the relationship is between two or more independent variables and one dependent variable (e.g. how rainfall, temperature, and amount of fertilizer added affect crop growth).\n2. The value of the dependent variable at a certain value of the independent variables (e.g. the expected yield of a crop at certain levels of rainfall, temperature, and fertilizer addition).\n\n**Note:** In multiple linear regression, it is possible that some of the independent variables are actually correlated with one another, so it is important to check these before developing the regression model. If two independent variables are too highly correlated (r2 > ~0.6), then only one of them should be used in the regression model.\n\n**Example**\n\nYou are a public health researcher interested in social factors that influence heart disease. You survey 500 towns and gather data on the percentage of people in each town who smoke, the percentage of people in each town who bike to work, and the percentage of people in each town who have heart disease.\n\nBecause you have two independent variables and one dependent variable, and all your variables are quantitative, you can use multiple linear regression to analyze the relationship between them.","68a09658":"In statistics, principal component regression (PCR) is a regression analysis technique that is based on principal component analysis (PCA). More specifically, PCR is used for estimating the unknown regression coefficients in a standard linear regression model.\n\nWe first perform principal components analysis (PCA) on the original data, then perform dimension reduction by selecting the number of principal components (m) using cross-validation or test set error, and finally conduct regression using the first m dimension reduced principal components.\n\n","55b4caf7":"We will understand the dataset first.","324d0d09":"Linear regression attempts to model the relationship between two variables by fitting a linear equation to observed data. One variable is considered to be an explanatory variable, and the other is considered to be a dependent variable. For example, a modeler might want to relate the weights of individuals to their heights using a linear regression model.","6d969363":"Let's see cumulative explanatioon rates.","9484d049":"Now we will split our dataset as train and test set.","56e1a858":"#### Theory","1ab0a46b":"The most common method for fitting a regression line is the method of least-squares. This method calculates the best-fitting line for the observed data by minimizing the sum of the squares of the vertical deviations from each data point to the line (if a point lies on the fitted line exactly, then its vertical deviation is 0). Because the deviations are first squared, then summed, there are no cancellations between positive and negative values.\n\nThe Ordinary Least Squares procedure seeks to minimize the sum of the squared residuals. This means that given a regression line through the data we calculate the distance from each data point to the regression line, square it, and sum all of the squared errors together. This is the quantity that ordinary least squares seeks to minimize.","c5c400f8":"Now we will split our dataset as train and test set.","7bbadb30":"We will understand the dataset first.","448c313e":"#### Prediction","144cee8f":"Now we will split our dataset as train and test set.","2ab966de":"We will do **One Hot Encoding** to categorical columns.","75e3c0a6":"##### Prediction","349fbeff":"- **The Elements of  Statistical Learning** - Trevor Hastie,  Robert Tibshirani, Jerome Friedman -  Data Mining, Inference, and Prediction (Springer Series in Statistics) \n\n- [**Linear Regression by Yale University**](http:\/\/www.stat.yale.edu\/Courses\/1997-98\/101\/linreg.htm)\n\n- [**An introduction to simple linear regression**](https:\/\/www.scribbr.com\/statistics\/simple-linear-regression\/)\n\n- [**An introduction to Multiple linear regression**](https:\/\/www.scribbr.com\/statistics\/multiple-linear-regression\/)\n\n- [**How to Develop Elastic Net Regression Models in Python**](https:\/\/machinelearningmastery.com\/elastic-net-regression-in-python\/)\n\n- [**Linear Regression for Machine Learning**](https:\/\/machinelearningmastery.com\/linear-regression-for-machine-learning\/)\n\n- [**Applied Data Mining and Statistical Learning by the PennState University**](https:\/\/online.stat.psu.edu\/stat508\/lesson\/7\/7.1)\n\n- [**How to Calculate Principal Component Analysis (PCA) from Scratch in Python**](https:\/\/machinelearningmastery.com\/calculate-principal-component-analysis-scratch-python\/)\n\n- [**Validating your Machine Learning Model**](https:\/\/towardsdatascience.com\/validating-your-machine-learning-model-25b4c8643fb7)\n\n- [**Understanding Confusion Matrix**](https:\/\/towardsdatascience.com\/understanding-confusion-matrix-a9ad42dcfd62)\n\n- [**Shrinkage methods by Stanford**](https:\/\/web.stanford.edu\/class\/stats202\/notes\/Model-selection\/Shrinkage.html)\n\n- [**Partial Least Squares regression by Xlstat**](https:\/\/www.xlstat.com\/en\/solutions\/features\/partial-least-squares-regression)\n\n- [**Ridge Regression by Statquest**](https:\/\/www.youtube.com\/watch?v=Q81RR3yKn30&ab_channel=StatQuestwithJoshStarmer)\n\n- [**Lasso Regression by Statquest**](https:\/\/www.youtube.com\/watch?v=NGf0voTMlcs&ab_channel=StatQuestwithJoshStarmer)\n\n- [**Elastic Net Regression by Statquest**](https:\/\/www.youtube.com\/watch?v=1dKRdX9bfIo&ab_channel=StatQuestwithJoshStarmer)\n\n- [**Regularization: Ridge, Lasso and Elastic Net by Datacamp**](https:\/\/www.datacamp.com\/community\/tutorials\/tutorial-ridge-lasso-elastic-net)","f2b62c5a":"### Shrinkage(Regularization) Methods","86a2f93b":"## Importing Libraries","5207506b":"Now we will select dependent variable.","43da4aa9":"##### Prediction","0ceb2147":"#### Theory","6740e384":"A linear regression model assumes that the regression function E(Y |X) is linear in the inputs X1,...,Xp. Linear models were largely developed in the precomputer age of statistics, but even in today\u2019s computer era there are still good reasons to study and use them. They are simple and often provide an adequate and interpretable description of how the inputs affect the output. ","84bea86d":"Let's predict a spesific value.","e78426d0":"We will do **One Hot Encoding** to categorical columns.","c6043ea4":"![Screen%20Shot%202021-07-18%20at%2020.39.59.png](attachment:Screen%20Shot%202021-07-18%20at%2020.39.59.png)\n\nPhoto is cited by: https:\/\/www.google.com\/url?sa=i&url=https%3A%2F%2Fmedium.com%2Fgeekculture%2Fridge-and-lasso-regression-51705b608fb9&psig=AOvVaw2KdB1GWJI9AWSkW5SyaXKy&ust=1626716220613000&source=images&cd=vfe&ved=0CAsQjRxqFwoTCJCY8YOV7fECFQAAAAAdAAAAABAD","136bbcab":"### Principal Component Regression(PCR)","cb3002f6":"### Least-Squares Regression(Ordinary Least Squares)","f24f9a88":"Let's predict a spesific value.","f616ea1b":"##### Model Tuning","4f331607":"Elastic-Net Regression is combines Lasso Regression with Ridge Regression to give you the best of both worlds. It works well when there are lots of useless variables that need to be removed from the equation and it works well when there are lots of useful variables that need to be retained. And it does better than either one when it comes to handling correlated variables. ","8c70742a":"##### Theory","781838fc":"For a real world example, we will work with **Hitters** dataset.\n\nIt can be downloaded here: https:\/\/www.kaggle.com\/floser\/hitters","dc5bccc6":"Now we will create our model with **sklearn**.","97727d88":"### Simple Linear Regression","73f46f09":"Now we will remove NA values.","5df9c4f2":"#### Model Tuning","d282ceba":"Ridge Regression adds a **penalty term(lambda \ud835\udf06)** to this to shrink the coefficients to 0 :","3c643128":"This is the cost function of Lasso regression:"}}