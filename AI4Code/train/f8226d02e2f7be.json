{"cell_type":{"709fe267":"code","e651a399":"code","7798122e":"code","08dff4a7":"code","ea86ed59":"code","6a3ce8c6":"code","95f293a4":"code","831f4568":"code","78771044":"code","07d5c7ea":"code","6510f5e9":"code","a068fa03":"code","9df09eef":"code","483e13a8":"code","93b619d4":"code","90238c92":"code","5751a4d1":"code","84ecb8e0":"code","05188a66":"code","f60a178e":"code","5a94e4b9":"code","3b0f6731":"code","350ad230":"code","745d9b4d":"code","68ea064f":"code","86070a63":"code","a0976bb9":"code","b8f7cc9d":"code","f7b195bf":"code","af01f09d":"code","ca9f3441":"code","be2732ae":"code","ae872b1a":"code","cf2644eb":"code","3019165a":"code","e651784b":"markdown","9c35f759":"markdown","c77e79fc":"markdown","920423de":"markdown"},"source":{"709fe267":"%config Completer.use_jedi = False","e651a399":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport random\nfrom scipy import stats\nfrom statsmodels import robust\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow, imread\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold\nfrom sklearn.metrics import mean_squared_error, accuracy_score, roc_auc_score\nfrom sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.impute import SimpleImputer\nimport scikitplot as skplt\n\nimport scipy.stats as stats\n\nimport lightgbm as lgb\nimport warnings\n\nimport optuna\n\nimport gc","7798122e":"R_SEED = 37\nN_FOLDS = 5","08dff4a7":"submission_ex = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')\ntrain_data_a = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/train.csv')\ntest_data_a  = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/test.csv')","ea86ed59":"target_data = train_data_a[['claim']].copy()\nfor_me_data_train = train_data_a[['id']].copy()\nfor_me_data_test = test_data_a[['id']].copy()\ntrain_data_a.drop(['id', 'claim'], axis=1, inplace=True)\nsubmit_data = test_data_a[['id']].copy()\ntest_data_a.drop(['id'], axis=1, inplace=True)\nn_missing_train = train_data_a.isna().sum(axis=1)","6a3ce8c6":"all_data = pd.concat([train_data_a, test_data_a])\nall_data.reset_index(drop=True, inplace=True)","95f293a4":"all_data_normalized = StandardScaler().fit_transform(all_data)\nall_data = pd.DataFrame(all_data_normalized.copy(), columns=all_data.columns)\ngc.collect()","831f4568":"new_features_dict = {}\n\nnew_features_dict['n_of_miss'] = all_data.isna().astype(int).sum(axis=1)\nfor i in range(10):\n    new_features_dict['miss_bt_' + str(i)] = (new_features_dict['n_of_miss'] > i).astype(int)\n    \nnew_features_dict['r_min'] = all_data.min(axis=1)\nnew_features_dict['r_std'] = all_data.std(axis=1)\nnew_features_dict['r_max'] = all_data.max(axis=1)\nnew_features_dict['r_median'] = all_data.median(axis=1)\nnew_features_dict['r_mean'] = all_data.mean(axis=1)\nnew_features_dict['r_var'] = all_data.var(axis=1)\nnew_features_dict['r_sum'] = all_data.sum(axis=1)\nnew_features_dict['r_sem'] = all_data.sem(axis=1)\nnew_features_dict['r_skew'] = all_data.skew(axis=1)","78771044":"for feature_name, feature_value in new_features_dict.items():\n    all_data[feature_name] = feature_value\ndel new_features_dict","07d5c7ea":"gc.collect()","6510f5e9":"code_data = train_data_a.isna().astype(int)\ncode_data = pd.concat([code_data, target_data], axis=1, join='inner')\ncode_data.head()","a068fa03":"features = [f for f in code_data.columns if f.startswith('f')]\nf_t1 = np.array([])\nfor f in features:\n    f_t1 = np.append(f_t1, code_data[code_data[f] == 1]['claim'].sum())","9df09eef":"code_data['n_of_miss'] = code_data.iloc[:,:-1].sum(axis=1)\ncode_data.head()","483e13a8":"n_of_miss_unique = np.unique(code_data['n_of_miss'])\nf_n_of_miss_to_prob = []\nfor n in n_of_miss_unique:\n    filtered = code_data[code_data['n_of_miss'] == n]\n    f_n_of_miss_to_prob.append((n, filtered['claim'].sum() \/ filtered.shape[0]))","93b619d4":"all_data['prob_1_for_miss'] = 0","90238c92":"warnings.filterwarnings(\"ignore\")\n\nfor (m, p) in f_n_of_miss_to_prob:\n    all_data['prob_1_for_miss'][all_data['n_of_miss'] == m] = p","5751a4d1":"del code_data\ngc.collect()","84ecb8e0":"tmp_a  = pd.read_csv('\/kaggle\/input\/0049-imputerdata-from-ae\/imputer_data_all.csv')\nfor c in tmp_a.columns:\n    all_data[c] = np.nan_to_num(all_data[c].values) + tmp_a[c].values\n    gc.collect()\n\ndel tmp_a\ngc.collect()","05188a66":"gc.collect()","f60a178e":"new_features_dict = {}\n\nnew_features_dict['r_zscore'] = (np.abs(stats.zscore(all_data))).sum(axis=1)\nfor i in range(3, 9):\n    new_features_dict['r_zscore_' + str(i)] = (np.abs(stats.zscore(all_data)) < i).all(axis=1).astype(int)\n\ngc.collect()\n\nnew_features_dict['median_abs_deviation'] = stats.median_abs_deviation(all_data, axis=1)\n\ngc.collect()\n\n# new_features_dict['dist_from_center_cos'] = cosine_similarity(all_data, [all_data.mean(axis=0).values])\n\n# gc.collect()\n\n# new_features_dict['dist_from_center_euc'] = euclidean_distances(all_data, [all_data.mean(axis=0).values])\n\n# gc.collect()\n\n# new_features_dict['mean_abs_deviation'] = all_data.mad(axis=1)\n\n# gc.collect()","5a94e4b9":"for feature_name, feature_value in new_features_dict.items():\n    all_data[feature_name] = feature_value\ndel new_features_dict","3b0f6731":"all_data['median_abs_deviation']","350ad230":"gc.collect()","745d9b4d":"train_data_a, test_data_a = all_data.iloc[:train_data_a.shape[0],:].copy(), all_data.iloc[train_data_a.shape[0]:,:].copy()","68ea064f":"train_data = train_data_a\ntest_data = test_data_a","86070a63":"del train_data_a, test_data_a, all_data","a0976bb9":"gc.collect()","b8f7cc9d":"params_1 = {\n    'objective': 'binary',\n    'metric': 'auc',\n    'boosting_type': 'gbdt',\n    'n_estimators': 35000, \n    'learning_rate': 0.005, \n    'num_leaves': 29,\n    'min_child_samples': 236, \n    'lambda_l1': 7.702002052840491, \n    'lambda_l2': 9.738840335016775, \n    'feature_fraction': 0.07, #0.3811137625854881, \n    'bagging_fraction': 0.7345219542805319, \n    'bagging_freq': 3, \n    'min_child_weight': 280.0714278010327}\n\n\nmodel_1 = lgb.LGBMRegressor(**params_1,\n                          n_jobs=-1,\n                          random_state = R_SEED)","f7b195bf":"gc.collect()","af01f09d":"def plot_fea_imp(model, model_name):\n    print('Plotting feature importances...')\n    fea_imp = pd.DataFrame({'imp': model.feature_importances_, 'col': train_data.columns})\n    fea_imp = fea_imp.sort_values(['imp', 'col'], ascending=[True, False])#.iloc[-10:]\n    fea_imp.plot(kind='barh', x='col', y='imp', figsize=(20, 70), legend=None)\n    plt.title('%s - Feature Importance' % (model_name))\n    plt.ylabel('Features')\n    plt.xlabel('Importance')","ca9f3441":"kfolds = StratifiedKFold(n_splits = N_FOLDS, shuffle = True, random_state = R_SEED)\npred = []\nlgb_oof_1 = np.zeros(train_data.shape[0])\n\nfm_i = 0\nfor train_index, test_index in kfolds.split(X=train_data, y=n_missing_train):\n\n    X_train, X_val = train_data.iloc[train_index], train_data.iloc[test_index]\n    y_train, y_val = target_data.iloc[train_index], target_data.iloc[test_index]\n    \n    gc.collect()\n    \n    print(y_train.shape[0], y_train['claim'].sum())\n    \n    model_1.fit(\n        X_train, \n        np.ravel(y_train), \n        eval_metric = \"auc\", \n        eval_set = [(X_val, y_val)],\n        verbose = 100,\n        early_stopping_rounds = 3000)\n    \n    plot_fea_imp(model_1, 'lightGBM_' + str(fm_i))\n    \n    oof_pred_1 = model_1.predict(X_val)\n    lgb_oof_1[test_index] = oof_pred_1\n    \n    _p = model_1.predict(test_data)\n    pred.append(_p)\n    \n    for_me_data_test['hm_' + str(fm_i)] = _p\n    fm_i += 1\n\nfor_me_data_train['hm_1'] = lgb_oof_1\n        \nfinal_p = np.sum(pred, axis = 0) \/ len(pred)\n\nsubmit_data['claim'] = final_p\nsubmit_data.to_csv('submission.csv', index=False)\nfor_me_data_train.to_csv('for_me_data_train.csv', index=False)\nfor_me_data_test.to_csv('for_me_data_test.csv', index=False)","be2732ae":"p_1 = [(1-e, e) for e in lgb_oof_1]\n\nfig = plt.figure(figsize = (10, 10))\nax = fig.gca()\n\nskplt.metrics.plot_roc(target_data.claim.values, p_1, plot_micro=False, plot_macro=False, classes_to_plot=[1], ax=ax, cmap='Reds')\n# skplt.metrics.plot_roc(target_data.claim.values, p_1, plot_micro=False, plot_macro=False, classes_to_plot=[0], ax=ax, cmap='ocean')\nplt.show()","ae872b1a":"fig = plt.figure(figsize = (30, 15))\nax = fig.gca()\nax.set_facecolor('cadetblue')\n\nsns.kdeplot(for_me_data_train['hm_1'], color = \"aliceblue\", ax = ax, linewidth=3, label='test')\n\nplt.legend()\nplt.show()","cf2644eb":"fig = plt.figure(figsize = (30, 15))\nax = fig.gca()\nax.set_facecolor('cadetblue')\n\nsns.kdeplot(submit_data['claim'], color = \"aliceblue\", ax = ax, linewidth=3, label='test')\n\nplt.legend()\nplt.show()","3019165a":"fig = plt.figure(figsize = (30, 15))\nax = fig.gca()\nax.set_facecolor('cadetblue')\nfor f in for_me_data_test.columns:\n    if f.startswith('hm'):\n        sns.kdeplot(for_me_data_test[f], ax = ax, linewidth=3, label='test')\n\nplt.legend()\nplt.show()","e651784b":"#### code [start]","9c35f759":"#### code [end]","c77e79fc":"Curve is drawn with oof data","920423de":"#### hm"}}