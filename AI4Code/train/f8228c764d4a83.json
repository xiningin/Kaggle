{"cell_type":{"e93df3f7":"code","bddaba82":"code","916ca83d":"code","7298da10":"code","b8ba8f62":"code","c683620d":"code","296042c3":"code","9e952f8f":"code","a28d02b5":"code","1ac88f5b":"code","fbf0ffee":"code","390e7019":"code","20b80962":"code","0a2a57de":"code","e2557bea":"code","e0386b4a":"code","2550e808":"code","83bd51ac":"code","3c25638d":"code","d101c5b4":"code","4a91b4c9":"code","6f8c8b54":"code","36be356c":"code","9f2de46d":"code","003f0589":"code","5cb4dfa8":"code","0f2a889d":"code","2819a832":"code","e767364a":"code","89015b6b":"code","b0d04fd6":"code","f8a3e88d":"code","8f1c57d9":"code","8cdcbb56":"code","ea5269fa":"code","11707c97":"code","f396f185":"code","c676f28c":"code","c59773cf":"code","1f240f42":"code","12937290":"code","2acdd67c":"code","7857a59d":"code","793f6382":"code","820e1d03":"code","9796f55a":"code","6adb3519":"code","5fbd319b":"code","611c36df":"code","5f415d14":"code","2780053b":"code","045398b1":"code","33b11c56":"code","d3125ca2":"code","c51bcaa1":"code","5ded0764":"code","0ddb7d21":"code","bfcbb00d":"code","0ec8921d":"code","ffb30193":"code","27a7b462":"code","9cd81f6f":"code","a88bfd5f":"code","4f0ee41f":"code","2d7773d3":"code","e51f0ae4":"code","c085a686":"code","58b314e4":"code","ffd55ed9":"code","052e5fb4":"code","6163dfdf":"code","193b54bd":"markdown","d1cdbac2":"markdown","4b9a0ebc":"markdown","5fd4d256":"markdown","c787f208":"markdown","55ea24d5":"markdown","fa6449d2":"markdown","b6601544":"markdown","5430c021":"markdown","23af9cc6":"markdown","d7b526d3":"markdown","d18939a1":"markdown","9fe05930":"markdown","538cf74e":"markdown","a58fb098":"markdown","fbd28141":"markdown","7f2c0063":"markdown","b2f19f22":"markdown","4635f805":"markdown","aadb7704":"markdown","4fa901e0":"markdown","13a05a88":"markdown","2287ccf5":"markdown","866c748f":"markdown","351cf96f":"markdown","cac90c20":"markdown","7336fc78":"markdown","3cf960c6":"markdown","e459e409":"markdown","ee6ab6db":"markdown","0b33fbc6":"markdown","9f92ea47":"markdown","3b916381":"markdown","2d4b1e40":"markdown","10a8d7b3":"markdown","e7c01ced":"markdown","85ed971d":"markdown"},"source":{"e93df3f7":"#import kaggle \nimport warnings\nwarnings.filterwarnings('ignore')","bddaba82":"#!kaggle datasets list -s rossman-store-sales","916ca83d":"#!kaggle datasets download realvinay\/rossmann-store-sales","7298da10":"#!unzip rossmann-store-sales.zip","b8ba8f62":"import pandas as pd\nimport numpy as np\n\npd.set_option('display.max_columns', 200)\npd.set_option('display.max_rows', 200)","c683620d":"store_df =  pd.read_csv('..\/input\/rossmann-store-sales\/store.csv');\nross_df = pd.read_csv('..\/input\/rossmann-store-sales\/train.csv', low_memory = False);\ntest_df = pd.read_csv('..\/input\/rossmann-store-sales\/test.csv');\nsubmission_df = pd.read_csv('..\/input\/rossmann-store-sales\/sample_submission.csv');","296042c3":"store_df","9e952f8f":"ross_df","a28d02b5":"submission_df","1ac88f5b":"merged_df = ross_df.merge(store_df, how = 'left', on = 'Store')\nmerged_test_df = test_df.merge(store_df, how = 'left', on = 'Store')\n\n# Checking the merged dataset :\nmerged_df.head()","fbf0ffee":"import matplotlib.pyplot as plt \nimport seaborn as sns\n%matplotlib inline \nplt.style.use('seaborn-whitegrid')","390e7019":"plt.figure(figsize=(12,6), dpi = 80)\nplt.title('Sales Distribution', fontsize=15)\nsns.distplot(merged_df['Sales'].sample(17000), hist = False, color = 'seagreen')\nplt.legend(['sales']);","20b80962":"print(merged_df['StoreType'].unique())","0a2a57de":"import plotly.express as px\nimport plotly.graph_objects as go\n\nfig = px.histogram(merged_df, x ='StoreType', y = 'Sales',\n                   color = 'StoreType', height = 580, width = 900)\n\nfig.update_layout(title = 'Sales per store',\n                 xaxis_title = 'Store type',\n                 yaxis_title = 'Sales',\n                 font = dict(family = 'Droid Serif', size=14))\nfig.show()","e2557bea":"lables, values = merged_df['StoreType'], merged_df['Sales']\n\nfig = go.Figure(data=[go.Pie(labels = lables, values = values, hole=.3)])\nfig.update_layout(title = 'Total Sales per store',\n                 font = dict(family = 'Droid Serif', size=12))\nfig.show()","e0386b4a":"merged_df.info()","2550e808":"def split_date(df):\n    df['Date'] = pd.to_datetime(df['Date'])\n    df['Year'] = df.Date.dt.year\n    df['Month'] =df.Date.dt.month\n    df['Day'] = df.Date.dt.day\n    df['WeekOfYear'] = df.Date.dt.isocalendar().week","83bd51ac":"#coverted datda :\nsplit_date(merged_df)\nsplit_date(merged_test_df)\n\nmerged_df.head()","3c25638d":"merged_df[merged_df.Open == 0].Sales.value_counts()","d101c5b4":"# You can verify that :\nmerged_df[merged_df.Open == 0].Sales","4a91b4c9":"merged_df = merged_df[merged_df.Open == 1].copy()","6f8c8b54":"def comp_months(df):\n    df['CompetitionOpen'] = 12 * (df.Year - df.CompetitionOpenSinceYear) + (df.Month - df.CompetitionOpenSinceMonth)\n    df['CompetitionOpen'] = df['CompetitionOpen'].map(lambda x: 0 if x < 0 else x).fillna(0)","36be356c":"comp_months(merged_df)\ncomp_months(merged_test_df)","9f2de46d":"merged_df","003f0589":"merged_df[['Date','CompetitionDistance','CompetitionOpenSinceYear','CompetitionOpenSinceMonth',\n           'CompetitionOpen']].sample(20).sort_values('Date')","5cb4dfa8":"def check_promo_month(row):\n    month2str = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun',              \n                 7:'Jul', 8:'Aug', 9:'Sept', 10:'Oct', 11:'Nov', 12:'Dec'}\n    try:\n        months = (row['PromoInterval'] or '').split(',')\n        if row['Promo2Open'] and month2str[row['Month']] in months:\n            return 1\n        else:\n            return 0\n    except Exception:\n        return 0\n\n    \ndef promo_cols(df):\n    # Months since Promo2 was open\n    df['Promo2Open'] = 12 * (df.Year - df.Promo2SinceYear) +  (df.WeekOfYear - df.Promo2SinceWeek)*7\/30.5\n    df['Promo2Open'] = df['Promo2Open'].map(lambda x: 0 if x < 0 else x).fillna(0) * df['Promo2']\n    \n    # Whether a new round of promotions was started in the current month\n    df['IsPromo2Month'] = df.apply(check_promo_month, axis=1) * df['Promo2']","0f2a889d":"promo_cols(merged_df)\npromo_cols(merged_test_df)","2819a832":"merged_df[['Date','Promo2','Promo2SinceYear','Promo2SinceWeek','PromoInterval','Promo2Open',\n           'IsPromo2Month']].sample(20).sort_values('Date')","e767364a":"print(list(merged_df.columns))","89015b6b":"input_cols = ['Store','DayOfWeek','Promo','StateHoliday','SchoolHoliday', \n              'StoreType', 'Assortment', 'CompetitionDistance','CompetitionOpen', \n              'Day','Month','Year','WeekOfYear','Promo2', \n              'Promo2Open','IsPromo2Month']\ntarget_col = 'Sales'","b0d04fd6":"# inputs & target :\ninputs = merged_df[input_cols].copy()\ntargets = merged_df[target_col].copy()\n\n# test inputs : \ntest_inputs = merged_test_df[input_cols].copy()","f8a3e88d":"numeric_cols = ['Store','Promo','SchoolHoliday', \n              'CompetitionDistance','CompetitionOpen','Promo2','Promo2Open','IsPromo2Month',\n              'Day','Month','Year','WeekOfYear',  ]\ncategorical_cols = ['DayOfWeek','StateHoliday','StoreType','Assortment']","8f1c57d9":"inputs[numeric_cols].isnull().sum().sort_values(ascending=False)","8cdcbb56":"max_distance = inputs.CompetitionDistance.max()","ea5269fa":"inputs['CompetitionDistance'].fillna(max_distance, inplace = True)\ntest_inputs['CompetitionDistance'].fillna(max_distance, inplace = True)\n\n# last Check\ninputs.isnull().sum()","11707c97":"from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n\n# Scalling numeric values to the 0 to 1 range :\nscaler = MinMaxScaler().fit(inputs[numeric_cols])\n\ninputs[numeric_cols] = scaler.transform(inputs[numeric_cols])\ntest_inputs[numeric_cols] = scaler.transform(test_inputs[numeric_cols])\n\n# Encoding categorical columns :\nencoder = OneHotEncoder(sparse=False, handle_unknown='ignore').fit(inputs[categorical_cols])\nencoded_cols = list(encoder.get_feature_names(categorical_cols))\n\ninputs[encoded_cols] = encoder.transform(inputs[categorical_cols])\ntest_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])","f396f185":"X = inputs[numeric_cols + encoded_cols]\nX_test = test_inputs[numeric_cols + encoded_cols]","c676f28c":"from xgboost import XGBRegressor\nmodel = XGBRegressor(n_jobs = -1, n_estimators = 20, random_state = 42, max_depth = 4)","c59773cf":"#?model","1f240f42":"%%time\nmodel.fit(X, targets)","12937290":"%%time\npreds = model.predict(X)\nprint(preds)","2acdd67c":"from sklearn.metrics import mean_squared_error\n\ndef rmse(a, b):\n    return mean_squared_error(a, b, squared = False)","7857a59d":"print('RMSE:', rmse(targets, preds))","793f6382":"from matplotlib.pyplot import rcParams\nimport matplotlib.pyplot as plt\nfrom xgboost import plot_tree\n%matplotlib inline\n\nrcParams['figure.figsize'] = 25,30","820e1d03":"#Uncomment if you have graphviz installed and located in your environment path :\nplot_tree(model, rankdir = 'LR');","9796f55a":"plot_tree(model, rankdir='LR', num_trees=1);","6adb3519":"plot_tree(model, rankdir='LR', num_trees=19);","5fbd319b":"trees = model.get_booster().get_dump()\nlen(trees)","611c36df":"print(trees[0]);","5f415d14":"importance_df=pd.DataFrame({'features':X.columns,\n                            'importance':model.feature_importances_}).sort_values('importance',ascending = False)","2780053b":"importance_df.head(10).style.highlight_max(axis=0)","045398b1":"fig = px.histogram(importance_df.head(10), \n                   x = 'importance', y = 'features', \n                   color = 'features', width = 900, height = 570)\nfig.update_layout(title = 'Important features',\n                 xaxis_title = 'importance',\n                 yaxis_title = 'features',\n                 font = dict(family = 'Droid Serif', size = 15))\nfig.show()","33b11c56":"from sklearn.model_selection import KFold\n\n\ndef train_and_eval(X_train, train_targets, X_val, val_targets, **params):\n    model = XGBRegressor(n_jobs = -1, random_state = 42, **params)\n    model.fit(X_train, train_targets)\n    train_rmse = rmse(model.predict(X_train), train_targets)\n    val_rmse = rmse(model.predict(X_val), val_targets)\n    return model, train_rmse, val_rmse","d3125ca2":"kfold = KFold(n_splits = 5, shuffle = True)","c51bcaa1":"models = []\n\nfor train_idxs, val_idxs in kfold.split(X) :\n    X_train, train_targets = X.iloc[train_idxs], targets.iloc[train_idxs]\n    X_val, val_targets = X.iloc[val_idxs], targets.iloc[val_idxs]\n    model, train_rmse, val_rmse = train_and_eval(X_train, train_targets,\n                                                 X_val, val_targets,\n                                                 max_depth = 4, n_estimators = 20)\n    models.append(model)\n    print('Train RMSE: {}, Validation RMSE: {}'.format(train_rmse, val_rmse))","5ded0764":"def pred_avg(models, inputs) :\n    return np.mean([model.predict(inputs) for model in models], axis = 0)\n\npreds = pred_avg(models, X)\nprint(preds)","0ddb7d21":"#def test_params_kfold(n_splits, **params):\n#    train_rmses, val_rmses, models = [], [], []\n#    kfold = KFold(n_splits)\n#    for train_idxs, val_idxs in kfold.split(X):\n#        X_train, train_targets = X.iloc[train_idxs], targets.iloc[train_idxs]\n#        X_val, val_targets = X.iloc[val_idxs], targets.iloc[val_idxs]\n#        model, train_rmse, val_rmse = train_and_evaluate(X_train, train_targets, X_val, val_targets, **params)\n#        models.append(model)\n#        train_rmses.append(train_rmse)\n#        val_rmses.append(val_rmse)\n#    print('Train RMSE: {}, Validation RMSE: {}'.format(np.mean(train_rmses), np.mean(val_rmses)))\n#    return models","bfcbb00d":"from sklearn.model_selection import train_test_split as tts\nX_train, X_val, train_targets, val_targets = tts(X, targets, test_size=0.1)","0ec8921d":"def test_params(**params):\n    model = XGBRegressor(n_jobs=-1, random_state=42, **params)\n    model.fit(X_train, train_targets)\n    train_rmse = rmse(model.predict(X_train), train_targets)\n    val_rmse = rmse(model.predict(X_val), val_targets)\n    print('Train RMSE: {}, Validation RMSE: {}'.format(train_rmse, val_rmse))","ffb30193":"test_params(n_estimators=10)\nprint('-'*70)\ntest_params(n_estimators=30)\nprint('-'*70)\ntest_params(n_estimators=100)\nprint('-'*70)\ntest_params(n_estimators=240)","27a7b462":"test_params(max_depth=2)\nprint('-'*70)\ntest_params(max_depth=5)\nprint('-'*70)\ntest_params(max_depth=10)","9cd81f6f":"test_params(n_estimators=50, learning_rate=0.01)\nprint('-'*70)\ntest_params(n_estimators=50, learning_rate=0.1)\nprint('-'*70)\ntest_params(n_estimators=50, learning_rate=0.3)\nprint('-'*70)\ntest_params(n_estimators=50, learning_rate=0.9)\nprint('-'*70)\ntest_params(n_estimators=50, learning_rate=0.99)","a88bfd5f":"test_params(booster='gblinear')","4f0ee41f":"model = XGBRegressor(n_jobs=-1, random_state=42, n_estimators=1000, \n                     learning_rate=0.2, max_depth=10, subsample=0.9, \n                     colsample_bytree=0.7)","2d7773d3":"%%time\nmodel.fit(X, targets)","e51f0ae4":"test_preds = model.predict(X_test)","c085a686":"submission_df.head()","58b314e4":"submission_df['Sales'] = test_preds","ffd55ed9":"test_df.Open.isnull().sum()","052e5fb4":"submission_df['Sales'] = submission_df['Sales']*test_df.Open.fillna(1.)\n\nsubmission_df.head(20)","6163dfdf":"#submission_df.to_csv('submission.csv', index = None)","193b54bd":"## Store sales :","d1cdbac2":"## K Fold Cross Validation\n\nNotice that we didn't create a validation set before training our XGBoost model. We'll use a different validation strategy this time, called K-fold cross validation ([source](https:\/\/vitalflux.com\/k-fold-cross-validation-python-example\/)):\n\n![](https:\/\/vitalflux.com\/wp-content\/uploads\/2020\/08\/Screenshot-2020-08-15-at-11.13.53-AM.png)","4b9a0ebc":"__Important :__\n- Instead of trying to model this relationship, it would be better to hard-code it in our predictions, and remove the rows where the store is closed. \n- We won't remove any rows from the test set, since we need to make predictions for every row.","5fd4d256":"## Feature Engineering :","c787f208":"# Hyperparameter tuning :","55ea24d5":"- Seems like competition distance is the only missing value, and we can simply fill it with the highest value (to indicate that competition is very far away).","fa6449d2":"### Evaluation\n\nLet's evaluate the predictions using __RMSE__ error.","b6601544":"### Store Open\/Closed\n\nNext, notice that the sales are zero whenever the store is closed.","5430c021":"## Sales :","23af9cc6":"- Since it may take a long time to perform 5-fold cross validation for each set of parameters we wish to try, we'll just pick a random 10% sample of the dataset as the validation set.","d7b526d3":"#### `n_estimators`\n\nThe number of trees to be created. More trees = greater capacity of the model.","d18939a1":"- We are off by 2377 which is not really good also not that bad.","9fe05930":"#### `booster`\n\nInstead of using Decision Trees, XGBoost can also train a linear model for each iteration. This can be configured using `booster`.","538cf74e":"## Putting it Together and Making Predictions\n\nLet's train a final model on the entire training set with custom hyperparameters. ","a58fb098":"### Insights :\n- Store __'a'__ earns 3.16 Billions of profit. \n- Store __'b'__ : 1.7 Billions.\n- Store __'c'__ : 783.224 Millions.\n- Store __'d'__ : 159.2341 Millions.","fbd28141":"Let's merge the information from `store_df` into `train_df` and `test_df`.","7f2c0063":"- The features related to competition and promotion are now much more useful.\n\n\n### Input and Target Columns\n\nLet's select the columns that we'll use for training.","b2f19f22":"- Finally, let's extract out all the numeric data for training.","4635f805":"### Prediction\n\nWe can now make predictions and evaluate the model using `model.predict`.","aadb7704":"#### `max_depth`\n\nAs you increase the max depth of each tree, the capacity of the tree increases and it can capture more information about the training set.","4fa901e0":"### Notes :\n- Huge outliers\n- Data is skewed. No normal distribution of data.","13a05a88":"Adding the predictions into `submission_df`.","2287ccf5":"#### `learning_rate`\n\nThe scaling factor to be applied to the prediction of each tree. A very high learning rate (close to 1) will lead to overfitting, and a low learning rate (close to 0) will lead to underfitting.","866c748f":"### Additional Promotion\n\nWe can also add some additional columns to indicate how long a store has been running `Promo2` and whether a new round of `Promo2` starts in the current month.","351cf96f":"### Visualization\n\nWe can visualize individual trees using `plot_tree` (note: this requires the `graphviz` library to be installed).","cac90c20":"### Important :\nRecall, however, if if the store is not open, then the sales must be 0. Thus, wherever the value of `Open` in the test set is 0, we can set the sales to 0. Also, there some missing values for `Open` in the test set. We'll replace them with 1 (open).","7336fc78":"- Let's also identify numeric and categorical columns. Note that we can treat binary categorical columns (0\/1) as numeric columns.","3cf960c6":"### Feature importance\n\nJust like decision trees and random forests, XGBoost also provides a feature importance score for each column in the input.","e459e409":"### Competition\n\nNext, we can use the columns `CompetitionOpenSince[Month\/Year]` columns from `store_df` to compute the number of months for which a competitor has been open near the store.","ee6ab6db":"- Let's also define a function to average predictions from the 5 different models.","0b33fbc6":"- Let's view the results of the new columns we've created.","9f92ea47":"### The following topics are covered in this Notebook:\n\n- Downloading a real-world dataset from a Kaggle competition\n- Performing feature engineering and prepare the dataset for training\n- Training and interpreting a gradient boosting model using XGBoost\n- Training with KFold cross validation and ensembling results\n- Configuring the gradient boosting model and tuning hyperparamters\n\n## Problem Statement\n\nThis notebook takes a practical and coding-focused approach. We'll learn gradient boosting by applying it to a real-world dataset from the [Rossmann Store Sales](https:\/\/www.kaggle.com\/c\/rossmann-store-sales) competition on Kaggle:\n\n> Rossmann operates over 3,000 drug stores in 7 European countries. Currently, Rossmann store managers are tasked with predicting their daily sales for up to six weeks in advance. Store sales are influenced by many factors, including promotions, competition, school and state holidays, seasonality, and locality. \n>\n>\n> With thousands of individual managers predicting sales based on their unique circumstances, the accuracy of results can be quite varied. You are provided with historical sales data for 1,115 Rossmann stores. The task is to forecast the \"Sales\" column for the test set. Note that some stores in the dataset were temporarily closed for refurbishment.\n>\n> View and download the data here: https:\/\/www.kaggle.com\/c\/rossmann-store-sales\/data","3b916381":"# Data Preprocessing :\n\n\n### Scale Numeric Values & Encode Categorical columns\n\nLet's scale numeric values to the 0 to 1 range.<br>\n\nThen encode the categorical columns.","2d4b1e40":"- Clearly, a linear model is not well suited for this dataset.","10a8d7b3":"# Visual Analysis","e7c01ced":"# Gradient Boosting : Xgboost","85ed971d":"__Note :__\n- From column zero Store to Assortment & Promo 2 we have non null values.\n- Rest of the columns carries great number of missing values.\n- We will deal with them further.\n\n### Date\n\nFirst, let's convert `Date` to a `datecolumn` and extract different parts of the date."}}