{"cell_type":{"9833cf07":"code","1d65f2a4":"code","52824cde":"code","084639aa":"code","eb894f36":"code","e8e39216":"code","ec365290":"code","42d18b8f":"code","a404b6d0":"code","a6c16732":"code","7ff63212":"code","873809f6":"code","e9a662ef":"code","d0c09ec5":"code","878e8f5e":"code","5360f7e7":"code","871dd6b4":"code","2fd604b5":"code","566235e8":"code","bf6db010":"code","e57844a2":"code","2c5bb336":"code","2d9e30a2":"code","a132586c":"code","4848be88":"code","9886de9a":"code","f9bf92e6":"code","d37be4e6":"code","188034d8":"code","bf1a3903":"code","db7ec06f":"code","87252874":"code","69bdb452":"code","0565fc4c":"code","90cb37d0":"code","227971e8":"code","c28929f5":"code","b1c76fb6":"code","e779bb5e":"code","c96a400a":"code","ec0b84e9":"code","2d949de7":"code","7d0de25a":"code","86c5909c":"code","d9464bb9":"code","9d86a690":"code","2f1f5936":"markdown","457337d2":"markdown","1220a472":"markdown","05659bbb":"markdown","da15d02f":"markdown","8e54f8ef":"markdown","f83b1301":"markdown","bfe78bc5":"markdown","401443dc":"markdown","91b6cce0":"markdown","26ec9383":"markdown","df47a5d7":"markdown","f12306aa":"markdown","ac383e0a":"markdown","93daf24e":"markdown","309f9f42":"markdown","3c21433e":"markdown","38b75440":"markdown","37b8abe7":"markdown","b40d7f07":"markdown","9b325936":"markdown","d27b7acb":"markdown","a7ae4649":"markdown","297ffa38":"markdown","be1a498e":"markdown","9e9f225a":"markdown"},"source":{"9833cf07":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","1d65f2a4":"import torch","52824cde":"def grad_example(x, i):\n    x.requires_grad_(True)\n    ex = torch.exp(x)\n    sm = ex[i] \/ ex.sum()\n    res = sm.log()\n    res.backward()\n    print(x.grad)\n    ","084639aa":"grad_example(torch.tensor([1.,2.,3.]), 1)","eb894f36":"def foo():\n    x = torch.tensor([4., 17.], requires_grad=True) # \u041d\u0430\u0447\u0430\u043b\u044c\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f x\n    lr = 0.05 # \u0441\u043a\u043e\u0440\u043e\u0441\u0442\u044c \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438\n    for iteration in range(100):\n        with torch.no_grad(): #pytorch \u0437\u0430\u043f\u0440\u0435\u0449\u0430\u0435\u0442 \u0438\u0437\u043c\u0435\u043d\u044f\u0442\u044c \u0442\u0435\u043d\u0437\u043e\u0440\u044b, \u0434\u043b\u044f \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043f\u043e\u0434\u0441\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u0442\u0441\u044f \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442, \n            # torch.no_grad() \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u0438\u0441\u043f\u043e\u043b\u043d\u0438\u0442\u044c \u043a\u043e\u0434, \u043d\u0435 \u043e\u0442\u0441\u043b\u0435\u0436\u0438\u0432\u0430\u044f \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442, \u0432 \u0442.\u0447. \u0438\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0432 \u0442\u0435\u043d\u0437\u043e\u0440\u0435\n            if x.grad is not None: # \u041d\u0430 \u043f\u0435\u0440\u0432\u043e\u0439 \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0438 x.grad == None\n                x.grad.zero_() # \u0443\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u043c \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442 \u0432 \u043d\u0443\u043b\u044c (\u0432 \u043f\u0440\u043e\u0442\u0438\u0432\u043d\u043e\u043c \u0441\u043b\u0443\u0447\u0430\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u0433\u043e \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u044f \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u0430 \u043f\u0440\u0438\u0431\u0430\u0432\u0438\u0442\u0441\u044f \u043a \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0435\u043c\u0443)\n                \n        f = torch.sum(x ** 2) # \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u043c \u043c\u0438\u043d\u0438\u043c\u0438\u0437\u0438\u0440\u0443\u0435\u043c\u0443\u044e \u0444\u0443\u043d\u043a\u0446\u0438\u044e\n        # \u0412\u044b\u0432\u043e\u0434\u0438\u043c (x.data - \u0442\u0435\u043d\u0437\u043e\u0440, \u0440\u0430\u0437\u0434\u0435\u043b\u044f\u044e\u0449\u0438\u0439 \u043f\u0430\u043c\u044f\u0442\u044c \u0441 x, \u043d\u043e \u0431\u0435\u0437 \u043e\u0442\u0441\u043b\u0435\u0436\u0438\u0432\u0430\u043d\u0438\u044f \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u0430, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0435\u0433\u043e, \u0447\u0442\u043e\u0431\u044b \u0432\u044b\u0432\u043e\u0434\u0438\u043b\u0438\u0441\u044c \u0442\u043e\u043b\u044c\u043a\u043e \u0434\u0430\u043d\u043d\u044b\u0435_\n        # \u0415\u0441\u043b\u0438 \u0432 \u0442\u0435\u043d\u0437\u043e\u0440\u0435 \u0442\u043e\u043b\u044c\u043a\u043e \u043e\u0434\u0438\u043d \u044d\u043b\u0435\u043c\u0435\u043d\u0442, item \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u0435\u0433\u043e \u043a\u0430\u043a \u0447\u0438\u0441\u043b\u043e\n        print(x.data, f.item()) \n        # \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u043c \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\n        f.backward()\n        with torch.no_grad():\n            # \u0414\u0435\u043b\u0430\u0435\u043c \u0448\u0430\u0433 (\u0441\u0434\u0432\u0438\u0433\u0430\u0435\u043c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0432 \u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0438, \u043f\u0440\u043e\u0442\u0438\u0432\u043e\u043f\u043e\u043b\u043e\u0436\u043d\u043e\u043c \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u0443, \u043d\u0430 \u0432\u0435\u043b\u0438\u0447\u0438\u043d\u0443, \u043f\u0440\u043e\u043f\u043e\u0440\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u0443\u044e \u0447\u0430\u0441\u0442\u043d\u044b\u043c \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u043d\u044b\u043c \u0438 \u0441\u043a\u043e\u0440\u043e\u0441\u0442\u0438 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f)\n            x -= lr * x.grad\n    print(x.data, f.item())","e8e39216":"foo()","ec365290":"def task_2():\n    X = torch.tensor([[-1 ,-1],[3, 5.1], [0, 4], [-1, 5], [3, -2], [4, 5]])\n    w = torch.tensor([0.1, -0.1])\n    b = torch.tensor(0.)\n    y = torch.tensor([0.436, 14.0182, 7.278, 6.003, 7.478, 15.833])\n    lr = 0.02\n    w.requires_grad_(True)\n    b.requires_grad_(True)\n    for iterations in range(100):\n        with torch.no_grad():\n            if w.grad is not None:\n                w.grad.zero_()\n                b.grad.zero_()\n        f = ((X.mv(w) + b - y)**2).sum()\n        f.backward()\n        with torch.no_grad():\n            w -= lr*w.grad\n            b -= lr*b.grad\n        print(w, b)\n    print(((X.mv(w) + b - y)**2).sum())\n","42d18b8f":"task_2()","a404b6d0":"from sklearn.datasets import fetch_openml","a6c16732":"covertype = fetch_openml(data_id=180)","7ff63212":"print(type(covertype), covertype)","873809f6":"cover_df = pd.DataFrame(data=covertype.data, columns=covertype.feature_names)","e9a662ef":"print(covertype.target)","d0c09ec5":"from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder().fit(covertype.target)","878e8f5e":"print(label_encoder.classes_) ","5360f7e7":"cover_target = label_encoder.transform(covertype.target)","871dd6b4":"print(cover_target)","2fd604b5":"print(cover_df.shape)","566235e8":"from sklearn.model_selection import train_test_split","bf6db010":"df_train, df_test, y_train, y_test = train_test_split(cover_df, cover_target, test_size=0.15, stratify=cover_target)","e57844a2":"to_normalize = [(i, col) for i, col in enumerate(cover_df.columns)\n                        if not col.startswith('wilderness_area') and not col.startswith('soil_type')]\nidx_to_normalize = [i for i,col in to_normalize] #\u043d\u043e\u043c\u0435\u0440\u0430 \u0441\u0442\u043e\u043b\u0431\u0446\u043e\u0432\ncolumns_to_normalize = [col for i, col in to_normalize] #\u043d\u0430\u0437\u0432\u0430\u043d\u0438\u044f\n\nprint(columns_to_normalize)\nprint(idx_to_normalize)","2c5bb336":"cover_df[columns_to_normalize].sample(4)","2d9e30a2":"from torch.utils.data import TensorDataset,DataLoader","a132586c":"tensor_train = torch.from_numpy(df_train.values).type(torch.FloatTensor)","4848be88":"print(tensor_train[:3]) #\u041f\u0435\u0440\u0432\u044b\u0435 \u0442\u0440\u0438 \u044d\u043a\u0437\u0435\u043c\u043f\u043b\u044f\u0440\u0430 \u0434\u0430\u043d\u043d\u044b\u0445","9886de9a":"tensor_test = torch.from_numpy(df_test.values).type(torch.FloatTensor)","f9bf92e6":"train_mean = torch.mean(tensor_train[:,idx_to_normalize], dim=0)\ntrain_std = torch.std(tensor_train[:,idx_to_normalize], dim=0)","d37be4e6":"print(train_mean, train_std)","188034d8":"tensor_train[:,idx_to_normalize] -= train_mean\ntensor_train[:,idx_to_normalize] \/= train_std\ntensor_test[:,idx_to_normalize] -= train_mean\ntensor_test[:,idx_to_normalize] \/= train_std","bf1a3903":"print(tensor_train[:3])","db7ec06f":"y_train_tensor = torch.from_numpy(y_train).type(torch.LongTensor)\ny_test_tensor = torch.from_numpy(y_test).type(torch.LongTensor)","87252874":"train_ds = TensorDataset(tensor_train, y_train_tensor)\ntest_ds = TensorDataset(tensor_test, y_test_tensor)","69bdb452":"print(train_ds[400])","0565fc4c":"train_loader = DataLoader(train_ds, batch_size=256, shuffle=True)\ntest_loader = DataLoader(test_ds, batch_size=256)","90cb37d0":"for xx, yy in train_loader:\n    print(xx[0])\n    print(yy[0])\n    break","227971e8":"from sklearn.metrics import classification_report\n\nclass Net():\n    def __init__(self, t1, t2, t3):\n        self.w = [torch.randn((t1, t2), device='cuda'), torch.randn((t2,t3), device='cuda')]\n        self.b = [torch.randn(t2, device='cuda'), torch.randn(t3, device='cuda')]\n        #self.w = [torch.randn(t1, t2), torch.randn(t2,t3)]\n        #self.b = [torch.randn(t2), torch.randn(t3)]\n        self.best_w = self.w.copy()\n        self.best_b = self.b.copy()\n        self.best_loss = None\n        self.result = None\n        \n        \n    def fit(self, train_loader, epoches, lr):\n        for i in range(len(self.w)):\n            self.w[i].requires_grad_(True)\n            self.b[i].requires_grad_(True)        \n\n        for epoche in range(epoches):\n            batch_loss = 0\n            for xx, yy in train_loader:\n                xx.to(torch.device('cuda'))\n                if self.w[0].grad is not None:\n                    for i in range(len(self.w)):\n                        self.w[i].grad.zero_()\n                        self.b[i].grad.zero_()\n                outh = (xx.cuda().mm(self.w[0]) + self.b[0]).relu()\n                outy = (outh.cuda().mm(self.w[1]) + self.b[1]).log_softmax(1)\n                loss = -outy[torch.arange(len(xx)),yy].sum()\/len(xx)\n                \n                loss.backward()\n                    \n                with torch.no_grad():\n                    batch_loss += loss.item()\n                    for i in range(len(self.w)):\n                        self.w[i] -= lr * self.w[i].grad\n                        self.b[i] -= lr * self.b[i].grad\n            with torch.no_grad():\n                batch_loss \/= len(xx)\n                if self.best_loss is None or batch_loss < self.best_loss:\n                    self.best_w = self.w.copy()\n                    self.best_b = self.b.copy()\n                    self.best_loss = batch_loss\n            print(\"Batch loss at {} = {}\".format(epoche+1, batch_loss))\n        print(self.best_loss)\n\n        \n    def predict(self, test_loader):\n        res = None\n        with torch.no_grad():\n            for xx, yy in test_loader:\n                outh = (xx.cuda().mm(self.best_w[0]) + self.best_b[0]).relu()\n                outy = (outh.cuda().mm(self.best_w[1]) + self.best_b[1]).log_softmax(1)\n                if res is None:\n                    res = outy.argmax(1)\n                else:\n                    res = torch.cat((res, outy.argmax(1)))\n        print(classification_report(res.tolist(), y_test_tensor.tolist()))\n        return res","c28929f5":"n = Net(54, 72, 7)\n","b1c76fb6":"lr = 0.02\nepoches = 20\nn.fit(train_loader, epoches, lr)","e779bb5e":"n.predict(test_loader)","c96a400a":"wh.requires_grad_(True)\nwy.requires_grad_(True)\nbh.requires_grad_(True)\nby.requires_grad_(True)\nif best_loss is not None:\n    wh = best_wh\n    wy = best_wy\n    bh = best_bh\n    by = best_by\nfor i in range(20):    \n    count = 1\n    for xx, yy in train_loader:\n        if wh.grad is not None:\n            wh.grad.zero_()\n            wy.grad.zero_()\n            bh.grad.zero_()\n            by.grad.zero_()\n        \n        outh = (xx.mm(wh) + bh).relu()\n        outy = (outh.mm(wy) + by).log_softmax(1)\n        j = -outy[torch.arange(len(xx)),yy].sum()\/len(xx)\n        \n        j.backward()\n        with torch.no_grad():\n            if best_loss is None or j.item() < best_loss:\n                best_wh = wh\n                best_wy = wy\n                best_bh = bh\n                best_by = by\n                best_loss = j\n            #lr \/= 10\n            wh -= lr*wh.grad\n            wy -= lr*wy.grad\n            bh -= lr*bh.grad\n            by -= lr*by.grad\n        if(count%300 == 0):\n            s = \"Loss({}, {}): {}\".format(i+1, count, j.item())\n            print(s)\n        count += 1\nprint(best_loss)","ec0b84e9":"from sklearn.metrics import classification_report\nres = None\nwith torch.no_grad():\n    for xx, yy in test_loader:\n        outh = (xx.mm(best_wh) + best_bh).relu()\n        outy = (outh.mm(best_wy) + best_by).log_softmax(1)\n        if res is None:\n            res = outy.argmax(1)\n        else:\n            res = torch.cat((res, outy.argmax(1)))\n    print(classification_report(res.tolist(), y_test_tensor.tolist()))","2d949de7":"import matplotlib.pyplot as plt\nimport matplotlib.colors","7d0de25a":"phi = np.linspace(0, 20, 300)\nr = 1 + phi\n\nx = r * np.cos(phi)\ny = r * np.sin(phi)\nclasses = np.zeros(300)\nclasses[:100] = 0\nclasses[100:200] = 1\nclasses[200:] = 2\npoints = np.column_stack((x + np.random.normal(scale=0.3,size = x.size), y + np.random.normal(scale=0.3,size=y.size)))\n","86c5909c":"plt.plot(x,y)\nplt.scatter(points[:,0], points[:,1], c=classes, cmap=matplotlib.colors.ListedColormap(['r','g','b']))","d9464bb9":"points_train, points_test, classes_train, classes_test = train_test_split(points,classes, test_size=0.2)","9d86a690":"points_train, points_test = [torch.from_numpy(points_train).type(torch.FloatTensor), torch.from_numpy(points_test).type(torch.FloatTensor)]","2f1f5936":"## \u0417\u0430\u0434\u0430\u043d\u0438\u0435 5 \n\n\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u0443\u044e \u0441\u0435\u0442\u044c \u0438\u0437 \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0435\u0433\u043e \u0448\u0430\u0433\u0430 \u0434\u043b\u044f \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0441\u0438\u043d\u0442\u0435\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u043d\u0430\u0431\u043e\u0440\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 (\u043a\u043b\u0430\u0441\u0441 \u0437\u0430\u0432\u0438\u0441\u0438\u0442 \u043e\u0442 \u043f\u043e\u043b\u043e\u0436\u0435\u043d\u0438\u044f \u043d\u0430 \u0441\u043f\u0438\u0440\u0430\u043b\u0438). \u041f\u043e\u0441\u0442\u0440\u043e\u0438\u0442\u044c \u0433\u0440\u0430\u043d\u0438\u0446\u0443 \u043f\u0440\u0438\u043d\u044f\u0442\u0438\u044f \u0440\u0435\u0448\u0435\u043d\u0438\u0439 (\u0447\u0435\u0440\u0435\u0437 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0435 \u043d\u0430 \u0441\u0435\u0442\u043a\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439).","457337d2":"\u0412\u044b\u0431\u0435\u0440\u0435\u043c \u0441\u0442\u043e\u043b\u0431\u0446\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u0440\u0435\u0434\u043f\u043e\u0447\u0442\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c (\u0446\u0435\u043d\u0442\u0440\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0432\u043e\u043a\u0440\u0443\u0433 \u043d\u0443\u043b\u044f \u0438 \u043f\u0440\u0438\u0432\u0435\u0441\u0442\u0438 \u0432 \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u044b\u0439 \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d) \u043f\u0435\u0440\u0435\u0434 \u043e\u0442\u043f\u0440\u0430\u0432\u043a\u043e\u0439 \u0432 \u0441\u0435\u0442\u044c. \u0412\u044b\u0431\u0435\u0440\u0435\u043c \u0432\u0441\u0435 \u043d\u0435\u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 (\u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0435 \u0443\u0436\u0435 \u0437\u0430\u043a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u044b)","1220a472":"\u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0443\u0435\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f.\n\u041f\u043e\u0434\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0443\u0435\u043c\u044b\u0445 \u0441\u0442\u043e\u043b\u0431\u0446\u043e\u0432 \u0438 \u0438\u0445 \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u044b\u0435 \u043e\u0442\u043a\u043b\u043e\u043d\u0435\u043d\u0438\u044f","05659bbb":"[](http:\/\/)****\u041c\u0435\u0442\u043a\u0438 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u044b \u0441\u0442\u0440\u043e\u043a\u0430\u043c\u0438, \u043d\u0430\u0437\u043d\u0430\u0447\u0438\u043c \u043a\u0430\u0436\u0434\u043e\u0439 \u0438\u0437 \u043d\u0438\u0445 \u043d\u043e\u043c\u0435\u0440 (\u043d\u043e\u043c\u0435\u0440 \u043a\u043b\u0430\u0441\u0441\u0430)","da15d02f":"****\u0420\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 \u043d\u0430 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0435 \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435","8e54f8ef":"Torch \u043f\u0440\u0435\u0434\u043b\u0430\u0433\u0430\u0435\u0442 \u0434\u043b\u044f \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0438 \u0434\u0430\u043d\u043d\u044b\u0445 \u043f\u0440\u0438 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0434\u0432\u0430 \u0438\u043d\u0442\u0435\u0440\u0444\u0435\u0439\u0441\u0430 - Dataset \u0438 DataLoader.\nDataset - \u0430\u0431\u0441\u0442\u0440\u0430\u043a\u0442\u043d\u0430\u044f \u043e\u0431\u0435\u0440\u0442\u043a\u0430 \u043d\u0430\u0434 \u0434\u0430\u043d\u043d\u044b\u043c\u0438, \u0441 \u0435\u0434\u0438\u043d\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u043c\u0438 \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u044f\u043c\u0438 - \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u044d\u043a\u0437\u0435\u043c\u043f\u043b\u044f\u0440\u0430 \u043f\u043e \u043d\u043e\u043c\u0435\u0440\u0443 \u0438 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u0434\u043b\u0438\u043d\u044b \u0432\u0441\u0435\u0433\u043e \u043d\u0430\u0431\u043e\u0440\u0430 \u0434\u0430\u043d\u043d\u044b\u0445.\nTensorDataset - \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f Dataset \u0434\u043b\u044f \u0433\u043e\u0442\u043e\u0432\u044b\u0445 \u0442\u0435\u043d\u0437\u043e\u0440\u043e\u0432. \u041a\u043e\u043d\u0441\u0442\u0440\u0443\u043a\u0442\u043e\u0440 \u043f\u0440\u0438\u043d\u0438\u043c\u0430\u0435\u0442 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0442\u0435\u043d\u0437\u043e\u0440\u043e\u0432 \u0438 \u043e\u0431\u0440\u0430\u0449\u0435\u043d\u0438\u0435 \u043f\u043e \u043d\u043e\u043c\u0435\u0440\u0443 \u0432\u0435\u0440\u043d\u0435\u0442 \u043a\u043e\u0440\u0442\u0435\u0436 \u0438\u0437 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0438\u0437 \u044d\u0442\u0438\u0445 \u0442\u0435\u043d\u0437\u043e\u0440\u043e\u0432","f83b1301":"\u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0443\u0435\u043c \u043a\u0430\u0436\u0434\u044b\u0439 \u043e\u0442\u043e\u0431\u0440\u0430\u043d\u043d\u044b\u0439 \u0441\u0442\u043e\u043b\u0431\u0435\u0446:\n$$X^{(i)}_j = \\frac{X^{(i)}_j - \\overline{X_j}}{\\sqrt{\\sigma_j^2}}$$\n$$\\overline{X_j} = \\frac{1}{N}\\sum_{i=1}^N X^{(i)}_j$$\n$${\\sigma_j^2} = \\frac{1}{N}\\sum_{i=1}^N  (X^{(i)}_j - \\overline{X_j})^2$$","bfe78bc5":"## \u0417\u0430\u0434\u0430\u043d\u0438\u0435 4\n\u041f\u043e\u0441\u0442\u0440\u043e\u0438\u0442\u044c \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u0443\u044e \u0441\u0435\u0442\u044c \u0434\u043b\u044f \u043c\u043d\u043e\u0433\u043e\u043a\u043b\u0430\u0441\u0441\u043e\u0432\u043e\u0439 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438, \u043d\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u043c\u043e\u0434\u0443\u043b\u044c torch.nn.\n\u041e\u0431\u0443\u0447\u0438\u0442\u044c \u0435\u0451 \u0434\u043b\u044f \u043d\u0430\u0431\u043e\u0440\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 https:\/\/www.openml.org\/d\/180 (\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0442\u0438\u043f\u0430 \u043b\u0435\u0441\u0430 \u043f\u043e \u043a\u0430\u0440\u0442\u043e\u0433\u0440\u0430\u0444\u0438\u0447\u0435\u0441\u043a\u0438\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u043c), \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u0441\u0442\u043e\u0445\u0430\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0441\u043f\u0443\u0441\u043a (\u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442 \u0432\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442\u0441\u044f \u043e\u0442 \u043f\u0430\u0447\u043a\u0438 \u044d\u043a\u0437\u0435\u043c\u043f\u043b\u044f\u0440\u043e\u0432).\n\u0421\u0435\u0442\u044c \u0434\u043e\u043b\u0436\u043d\u0430 \u0438\u043c\u0435\u0442\u044c \u043c\u0438\u043d\u0438\u043c\u0443\u043c 1 \u0441\u043a\u0440\u044b\u0442\u044b\u0439 \u0441\u043b\u043e\u0439.\n\n\u041f\u0440\u0435\u0434\u043f\u043e\u043b\u0430\u0433\u0430\u0435\u043c\u0430\u044f \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0430\n$$ \\vec{h} = \\text{relu}(W^{(h)}\\vec{x} + \\vec{b^{(h)}})$$\n$$ \\vec{p} = \\text{softmax}(W^{(p)}\\vec{h} + \\vec{b^{(p)}})$$\n\n$$ \\vec{x} \\in \\mathbb{R}^{D_x}$$\n$$ \\vec{h}, \\vec{b_h}  \\in \\mathbb{R}^{D_h},~W^{(h)} \\in \\mathbb{R}^{D_h \\times D_x}$$\n$$ \\vec{p}, \\vec{b_p}  \\in \\mathbb{R}^{N_c} ,~W^{(p)} \\in \\mathbb{R}^{N_p \\times D_h}$$\n\u0413\u0434\u0435 $D_x$, $D_h$ - \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438 \u0432\u0445\u043e\u0434\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u0441\u043a\u0440\u044b\u0442\u043e\u0433\u043e \u0441\u043b\u043e\u044f, $N_c$ - \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u043b\u0430\u0441\u0441\u043e\u0432.\n\n$$ \\text{relu}(x) = \\text{max}(0,x) $$\n$$ \\text{softmax}(\\vec{x})_i = \\frac{e^{x_i}}{\\sum_j e^{x_j}} $$\n\n\u042d\u043b\u0435\u043c\u0435\u043d\u0442 $p_i$ \u0432\u0435\u043a\u0442\u043e\u0440\u0430 $\\vec{p}$ \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c\u044e \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c \u043a\u043b\u0430\u0441\u0441\u0430 $i$ \u0434\u043b\u044f \u0434\u0430\u043d\u043d\u043e\u0433\u043e \u044d\u043a\u0437\u0435\u043c\u043f\u043b\u044f\u0440\u0430. \u0417\u0430\u0434\u0430\u0447\u0435\u0439 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0438 \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043c\u0430\u043a\u0441\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u044f \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 \u043a\u043e\u0440\u0440\u0435\u043a\u0442\u043d\u044b\u0445 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u044d\u043a\u0437\u0435\u043c\u043f\u043b\u044f\u0440\u0430:\n$$ P_{\\text{model}}(C^{(1)} = c^{(1)} \\land C^{(2)} = c^{(2)} \\land~... ~\\land C^{(N)} = c^{(n)}) = \\prod_{i=1}^N p_{c^{(i)}}$$\n\u0433\u0434\u0435 $C^{(i)}$ - \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u0430\u044f \u0432\u0435\u043b\u0438\u0447\u0438\u043d\u0430, \u043e\u0431\u043e\u0437\u043d\u0430\u0447\u0430\u044e\u0449\u0430\u044f \u043a\u043b\u0430\u0441\u0441 $i$-\u0433\u043e \u044d\u043a\u0437\u0435\u043c\u043f\u043b\u044f\u0440\u0430, \u0430 $c^{(i)}$ - \u043d\u043e\u043c\u0435\u0440 \u043a\u043e\u0440\u0440\u0435\u043a\u0442\u043d\u043e\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u0430 \u0434\u043b\u044f $i$-\u0433\u043e \u044d\u043a\u0437\u0435\u043c\u043f\u043b\u044f\u0440\u0430.\n\u041a\u0440\u0438\u0442\u0435\u0440\u0438\u0439 \u043c\u043e\u0436\u043d\u043e \u043f\u0435\u0440\u0435\u043f\u0438\u0441\u0430\u0442\u044c \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u043b\u043e\u0433\u0430\u0440\u0438\u0444\u043c\u0430 \u0438 \u0443\u0441\u0440\u0435\u0434\u043d\u0435\u043d\u0438\u044f (\u043e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u043e\u0441\u0442\u0430\u043d\u0443\u0442\u0441\u044f \u0442\u0435\u043c\u0438 \u0436\u0435), \u0430 \u0442\u0430\u043a \u0436\u0435 \u0438\u0437\u043c\u0435\u043d\u0438\u0432 \u0437\u043d\u0430\u043a \u0434\u043b\u044f \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u0432 \u0437\u0430\u0434\u0430\u0447\u0443 \u043c\u0438\u043d\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438, \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u043d\u044b\u0439 \u043a\u0440\u0438\u0442\u0435\u0440\u0438\u0439 \u0431\u043e\u043b\u0435\u0435 \u0441\u0442\u0430\u0431\u0438\u043b\u0435\u043d \u0434\u043b\u044f \u0447\u0438\u0441\u043b\u0435\u043d\u043d\u043e\u0439 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0438 \u0437\u0430 \u0441\u0447\u0435\u0442 \u0443\u0441\u0440\u0435\u0434\u043d\u0435\u043d\u0438\u044f \u0431\u043e\u043b\u0435\u0435 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0442\u0438\u0432\u0435\u043d (\u043d\u0435 \u0437\u0430\u0432\u0438\u0441\u0438\u0442 \u043e\u0442 \u0440\u0430\u0437\u043c\u0435\u0440\u0430 \u043d\u0430\u0431\u043e\u0440\u0430 \u0434\u0430\u043d\u043d\u044b\u0445):\n$$\\underset{\\theta}{\\operatorname{argmin}}  -\\frac{1}{N}\\sum_{i=1}^N \\log p_{c^{(i)}}$$\n\u0433\u0434\u0435 $\\theta$ - \u043c\u043d\u043e\u0436\u0435\u0441\u0442\u0432\u043e \u0432\u0441\u0435\u0445 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u043c\u043e\u0434\u0435\u043b\u0438 ($W$, $b$)","401443dc":"\u041c\u0430\u0442\u0440\u0438\u0446\u044b $W$ \u0432 \u0444\u043e\u0440\u043c\u0443\u043b\u0430\u0445 \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u044b \u0438\u0437 \u0443\u0441\u043b\u043e\u0432\u0438\u044f, \u0447\u0442\u043e \u043a\u0430\u0436\u0434\u0430\u044f \u0441\u0442\u0440\u043e\u043a\u0430 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u0435\u0442 \u043e\u0434\u043d\u043e\u043c\u0443 \u043d\u0435\u0439\u0440\u043e\u043d\u0443 \u0438 \u0443\u043c\u043d\u043e\u0436\u0430\u0435\u043c\u044b\u0439 \u043d\u0430 \u043d\u0435\u0451 \u0432\u0435\u043a\u0442\u043e\u0440 \u0441\u0447\u0438\u0442\u0430\u0435\u0442\u0441\u044f \u0441\u0442\u043e\u043b\u0431\u0446\u043e\u043c. \u0422\u0430\u043a\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c $i$-\u044b\u0439 \u044d\u043b\u0435\u043c\u0435\u043d\u0442 \u043f\u0440\u043e\u0438\u0437\u0432\u0435\u0434\u0435\u043d\u0438\u044f $W\\vec{x}$ \u0440\u0430\u0432\u0435\u043d $W^{(i)} \\cdot \\vec{x}$, \u0442.\u0435. \u0432\u0437\u0432\u0435\u0448\u0435\u043d\u043d\u043e\u0439 \u0441\u0443\u043c\u043c\u0435 \u0432\u0445\u043e\u0434\u043e\u0432 i-\u0433\u043e \u043d\u0435\u0439\u0440\u043e\u043d\u0430. \u041e\u0434\u043d\u0430\u043a\u043e \u0443\u0447\u0438\u0442\u044b\u0432\u0430\u044f, \u0447\u0442\u043e \u043d\u0443\u0436\u043d\u043e \u043e\u0431\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u0442\u044c \u043f\u0430\u0447\u043a\u0438 \u044d\u043a\u0437\u0435\u043c\u043f\u043b\u044f\u0440\u043e\u0432 $X$ \u0438 \u044d\u043a\u0437\u0435\u043c\u043f\u043b\u044f\u0440\u044b \u043f\u0440\u0438\u043d\u044f\u0442\u043e \u0445\u0440\u0430\u043d\u0438\u0442\u044c \u0432 \u0441\u0442\u0440\u043e\u043a\u0430\u0445 \u043c\u0430\u0442\u0440\u0438\u0446\u044b (\u0442.\u0435. $X \\in \\mathbb{R}^{B \\times D_x}$, \u0433\u0434\u0435 $B$ - \u0440\u0430\u0437\u043c\u0435\u0440 \u043f\u0430\u0447\u043a\u0438), \u0443\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u0435 \u0441\u043b\u043e\u044f (\u043f\u0435\u0440\u0435\u0434 \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u0435\u043c \u0430\u043a\u0442\u0438\u0432\u0430\u0446\u0438\u0438) \u043b\u0443\u0447\u0448\u0435 \u043f\u0435\u0440\u0435\u043f\u0438\u0441\u0430\u0442\u044c:\n$$ H = XW^{T} + \\vec{b} $$, \u0433\u0434\u0435 $\\vec{b}$ \u043f\u0440\u0438\u0431\u0430\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043a \u043a\u0430\u0436\u0434\u043e\u0439 \u0441\u0442\u0440\u043e\u043a\u0435(__!__) \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0438\u0440\u0443\u044e\u0449\u0435\u0439 \u043c\u0430\u0442\u0440\u0438\u0446\u044b. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u043c \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u0430 $H \\in \\mathbb{R}^{B \\times D_h}$, \u0442.\u0435. \u043a\u0430\u0436\u0434\u0430\u044f \u0435\u0451 \u0441\u0442\u0440\u043e\u043a\u0430 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u0435\u0442 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u043d\u043e\u0439 \u0441\u0442\u0440\u043e\u043a\u0435 \u0438\u0437 $X$. \u0412\u043c\u0435\u0441\u0442\u043e \u0442\u0440\u0430\u043d\u0441\u043f\u043e\u043d\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f $W$ \u0438\u0445 \u0441\u0440\u0430\u0437\u0443 \u043c\u043e\u0436\u043d\u043e \u0441\u043e\u0437\u0434\u0430\u0442\u044c \u0438\u0437 \u0443\u0441\u043b\u043e\u0432\u0438\u044f, \u0447\u0442\u043e \u043d\u0435\u0439\u0440\u043e\u043d\u0430\u043c \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0442 \u0441\u0442\u043e\u043b\u0431\u0446\u044b, \u0442.\u0435. $W^{(h)} \\in \\mathbb{R}^{D_x \\times D_h}$.","91b6cce0":"## \u0417\u0430\u0434\u0430\u043d\u0438\u0435 6\n\u041f\u043e\u0441\u0442\u0440\u043e\u0438\u0442\u044c \u0438 \u043e\u0431\u0443\u0447\u0438\u0442\u044c \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u0443\u044e \u0441\u0435\u0442\u044c \u0438\u0437 \u0437\u0430\u0434\u0430\u043d\u0438\u044f 4 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u043c\u043e\u0434\u0443\u043b\u0438 torch.nn \u0438 torch.optim","26ec9383":"\u041f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u043c \u0441\u0442\u0440\u043e\u043a\u0438 \u0432 \u043d\u043e\u043c\u0435\u0440\u0430","df47a5d7":"\u0417\u0430\u0433\u043e\u0442\u043e\u0432\u043a\u0430","f12306aa":"1. \u0414\u043b\u044f \u043d\u0430\u0433\u043b\u044f\u0434\u043d\u043e\u0441\u0442\u0438 \u043f\u043e\u0441\u0442\u0440\u043e\u0438\u043c \u0442\u0430\u0431\u043b\u0438\u0446\u0443 pandas","ac383e0a":"## \u0417\u0430\u0434\u0430\u043d\u0438\u0435 2\n\n\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0441\u043f\u0443\u0441\u043a, \u043d\u0430\u0439\u0442\u0438 \u043c\u0438\u043d\u0438\u043c\u0443\u043c \u0444\u0443\u043d\u043a\u0446\u0438\u0438:\n$$ f(\\vec{w}, b;X,\\vec{y}) = \\sum_{i=1}^N [\\vec{w} \\cdot X^{(i)} + b - y_i]^2 = \\sum_{i=1}^N(\\hat{y}_i - y_i)^2;~~ \\hat{y} = X\\vec{w} + b$$\n\u0413\u0434\u0435 $X$:\n\\begin{bmatrix}\n-1 & -1 \\\\\n3 & 5.1 \\\\\n0 & 4 \\\\\n-1 & 5 \\\\\n3 & -2 \\\\\n4 & 5\n\\end{bmatrix}\n\u0438 $y$ =:\n\\begin{bmatrix}\n0.436 \\\\\n14.0182 \\\\\n7.278\\\\\n6.003 \\\\\n7.478 \\\\\n15.833 \n\\end{bmatrix}","93daf24e":"covertype \u0438\u043c\u0435\u0435\u0442 \u043a\u043b\u0430\u0441\u0441 Bunch - \u0441\u043b\u043e\u0432\u0430\u0440\u044c, \u043a \u043a\u043b\u044e\u0447\u0430\u043c \u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e \u043c\u043e\u0436\u043d\u043e \u043e\u0431\u0440\u0430\u0449\u0430\u0442\u044c\u0441\u044f \u043a\u0430\u043a \u043a \u043f\u043e\u043b\u044f\u043c (b['key'] -> b.key)","309f9f42":"\u041f\u0440\u0438\u043c\u0435\u0440:\n\u041d\u0430\u0439\u0442\u0438 \u043c\u0438\u043d\u0438\u043c\u0443\u043c \u0444\u0443\u043d\u043a\u0446\u0438\u0438: $$f(\\vec{x}) = \\sum_i x_i^2$$\n\u041e\u0447\u0435\u0432\u0438\u0434\u043d\u043e, \u0447\u0442\u043e \u0440\u0435\u0448\u0435\u043d\u0438\u0435 \u043d\u0443\u043b\u0435\u0432\u043e\u0435, \u043e\u0434\u043d\u0430\u043a\u043e \u043f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u043f\u0440\u0438\u0439\u0442\u0438 \u043a \u043d\u0435\u043c\u0443 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u043c \u0441\u043f\u0443\u0441\u043a\u043e\u043c","3c21433e":"\u041f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u043c \u0432 \u0444\u043e\u0440\u043c\u0430\u0442 pytorch","38b75440":"## \u0417\u0430\u0434\u0430\u043d\u0438\u0435 1\n\u0412\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u044c \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 (\u0434\u043b\u044f \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432, \u043e\u0442\u0434\u0435\u043b\u0435\u043d\u043d\u044b\u0445 \u0447\u0435\u0440\u0435\u0437 ; \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442 \u043d\u0435 \u043d\u0443\u0436\u0435\u043d):\n0. $$f(\\vec{x}; i) = \\log \\frac{e^{x_i}}{\\sum_{j=1}^n{e^{x_j}}}$$\n1. $$f(x,\\vec{y},\\vec{z}) = 2x + (3 - sin(\\vec{y} \\cdot \\vec{z}))^x$$\n2. $$f(\\vec{w}; \\vec{x}) = \\frac{1}{1 + e^{- \\vec{w} \\cdot \\vec{x}}}$$\n3. $$f(\\vec{a}; \\alpha, \\vec{b}, \\vec{c}) = \\text{max}(0, \\alpha + ||\\vec{a} - \\vec{b}||_2 - ||\\vec{a} - \\vec{c}||_2); ~~||\\vec{a} - \\vec{b}||_2 = \\sum_{i}(a_i - b_i)^2$$\n4. $$f(\\vec{x}) = \\max_i(|x_i|)$$\n5. $$f(\\vec{u}; \\vec{x}) = \\sum_i \\alpha_i x_i; ~\\alpha_i = \\frac{e^{u_i}} {\\sum_{j=1}^n e^{u_j}}$$\n6. $$f(\\vec{x}, \\vec{y}) = \\cos(\\vec{x}, \\vec{y})$$\n7. $$f(\\vec{x}, \\vec{y}) = \\cos(\\vec{x} - \\overline{x}, \\vec{y} - \\overline{y});~ \\overline{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$$\n8. $$f(\\vec{x}, \\vec{y}) = \\frac{  \\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y}) }{\\sqrt{ \\sum_{i=1}^n (x_i - \\overline{x})^2} \\sqrt{ \\sum_{i=1}^n (y_i - \\overline{y})^2} } $$\n9. $$f(\\vec{u}) = -\\sum_{i=1}^n p_i \\log p_i ; ~ p_i = \\frac{e^{u_i}} {\\sum_{j=1}^n e^{u_j}}$$\n10. $$f(\\vec{w};  \\vec{x}) = \\begin{cases} \n\\vec{w} \\cdot \\vec{x} & \\mbox{if } \\vec{w} \\cdot \\vec{x} > 0 \\\\ \na(e^{\\vec{w} \\cdot \\vec{x}}-1) & \\mbox{otherwise}\n\\end{cases}$$\n11. $$f(x, \\vec{a},\\vec{b}) = \\max_{i}[(a_i - x)^2 + b_i^2]$$","37b8abe7":"from_numpy \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u0442\u0435\u043d\u0437\u043e\u0440, \u0440\u0430\u0437\u0434\u0435\u043b\u044f\u044e\u0449\u0438\u0439 \u043f\u0430\u043c\u044f\u0442\u044c \u0441 numpy \u043c\u0430\u0441\u0441\u0438\u0432\u043e\u043c, .type \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u0442\u0435\u043d\u0437\u043e\u0440 \u043d\u043e\u0432\u043e\u0433\u043e \u0442\u0438\u043f\u0430 (\u0432 \u0434\u0430\u043d\u043d\u043e\u043c \u0441\u043b\u0443\u0447\u0430\u0435, \u043c\u044b \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u043c \u0438\u0437 float64 \u0432 float32)","b40d7f07":"\u0432\u0441\u0435\u0433\u043e 7 \u043a\u043b\u0430\u0441\u0441\u043e\u0432","9b325936":"\u0422\u0435\u043f\u0435\u0440\u044c \u043a\u0430\u0436\u0434\u044b\u0439 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u044b\u0439 \u0441\u0442\u043e\u043b\u0431\u0435\u0446 \u0438\u043c\u0435\u0435\u0442 \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 0 \u0438 \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u043e\u0435 \u043e\u0442\u043a\u043b\u043e\u043d\u0435\u043d\u0438\u0435 1","d27b7acb":"## \u0417\u0430\u0434\u0430\u043d\u0438\u0435 3 \n1. \u0414\u0430\u043d \u0442\u0435\u043d\u0437\u043e\u0440 \u0440\u0430\u0437\u043c\u0435\u0440\u043e\u043c $N \\times D$. \u0421 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c\u044e p \u043e\u0431\u043d\u0443\u043b\u0438\u0442\u044c \u043a\u0430\u0436\u0434\u044b\u0439 \u044d\u043b\u0435\u043c\u0435\u043d\u0442.\n2. \u0414\u0430\u043d \u0442\u0435\u043d\u0437\u043e\u0440 \u0440\u0430\u0437\u043c\u0435\u0440\u043e\u043c $N \\times D$. \u0421 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c\u044e p \u043e\u0431\u043d\u0443\u043b\u0438\u0442\u044c \u043a\u0430\u0436\u0434\u044b\u0439 \u0441\u0442\u043e\u043b\u0431\u0435\u0446.\n3. \u0414\u0430\u043d \u0442\u0435\u043d\u0437\u043e\u0440 $K$ \u0440\u0430\u0437\u043c\u0435\u0440\u043e\u043c $N \\times D$ \u0438 \u0432\u0435\u043a\u0442\u043e\u0440 $\\vec{q}$ \u0440\u0430\u0437\u043c\u0435\u0440\u043e\u043c $D$. \u041f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u0432\u0435\u043a\u0442\u043e\u0440 $\\vec{a}$ \u043f\u043e \u0444\u043e\u0440\u043c\u0443\u043b\u0435:\n$$\\vec{a} = \\sum_{i=1}^{N} \\alpha_i \\vec{K}^{(i)}$$\n\u0433\u0434\u0435\n$$\\alpha_i = \\frac{e^{z_i}}{\\sum_{j=1}^{N}{e^{z_j}}}$$\n$$z_i = \\vec{q} \\cdot \\vec{K}^{(i)}$$\n","a7ae4649":"DataLoader \u043f\u0440\u0435\u0434\u043d\u0430\u0437\u043d\u0430\u0447\u0435\u043d \u0434\u043b\u044f \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0438 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438\u0437 Dataset \u043f\u0430\u0447\u043a\u0430\u043c\u0438 \u044d\u043a\u0437\u0435\u043c\u043f\u043b\u044f\u0440\u043e\u0432. Dataloader \u0438\u043c\u0435\u0435\u0442 \u043c\u043d\u043e\u0433\u043e \u043d\u0430\u0441\u0442\u0440\u043e\u0435\u043a, \u043d\u043e \u0432 \u0434\u0430\u043d\u043d\u043e\u043c \u0441\u043b\u0443\u0447\u0430\u0435, \u043c\u044b \u043f\u0440\u043e\u0441\u0442\u043e \u0431\u0443\u0434\u0435\u043c \u043f\u043e\u043b\u0443\u0447\u0430\u0442\u044c \u0432\u044b\u0440\u0435\u0437\u043a\u0438 \u0442\u0435\u043d\u0437\u043e\u0440\u043e\u0432 (256 \u044d\u043a\u0437\u0435\u043c\u043f\u043b\u044f\u0440\u043e\u0432 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 256 \u043c\u0435\u0442\u043e\u043a)","297ffa38":"\u0412\u044b\u0432\u0435\u0434\u0435\u043c \u043f\u0435\u0440\u0432\u044b\u0439 \u044d\u043b\u0435\u043c\u0435\u043d\u0442 train_loader","be1a498e":"### \u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445\n\u0421\u043a\u0430\u0447\u0430\u0435\u043c \u043d\u0430\u0431\u043e\u0440 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f sklearn (\u0432 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430\u0445 kernel \u043d\u0443\u0436\u043d\u043e \u0432\u043a\u043b\u044e\u0447\u0438\u0442\u044c \u0434\u043e\u0441\u0442\u0443\u043f \u043a \u0418\u043d\u0442\u0435\u0440\u043d\u0435\u0442\u0443)","9e9f225a":"\u041f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u043c \u043c\u0435\u0442\u043a\u0438 \u0432 \u0442\u0435\u043d\u0437\u043e\u0440\u044b"}}