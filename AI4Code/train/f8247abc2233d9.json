{"cell_type":{"b446657b":"code","71efa398":"code","02174f15":"code","ea2364dd":"code","1419b198":"code","ffbaedf9":"code","e4977cad":"code","14479387":"code","9f2e8c0c":"code","54130e0f":"code","ce878cb8":"code","a32774f0":"code","b396bc3f":"code","31ffedc3":"code","ddef5249":"code","e79099c8":"code","1fccc57f":"code","82133c1e":"code","af64bf21":"code","0950f8e1":"code","db70e8be":"code","0791685e":"code","172330f2":"code","5bb69864":"code","47ff8c78":"code","21f90c85":"code","1cfe97e8":"code","e15d6dea":"code","ea52aa96":"code","0dbcbb27":"code","8abf826c":"code","cf775b5d":"code","47456669":"code","7e084e9d":"code","9b94e828":"code","f034e809":"code","d21106c6":"code","54ff240b":"code","96046def":"code","27542837":"code","332bc909":"code","546ea2a2":"code","75c135f1":"code","be6e1b1b":"code","02e3afb2":"code","a84c4827":"code","9f2e08da":"code","8d071ffb":"code","5df3e1a2":"code","cf558209":"code","dfe1c924":"code","a8f5eb71":"code","f6045d53":"code","a99c0db2":"code","0947686c":"code","da8deaaf":"code","0a41df32":"code","99673bee":"code","52c48310":"code","ea5a0d3d":"code","3c0d146b":"code","2a318c04":"code","59351374":"code","4ec7caba":"code","03a3b447":"code","0ae3c09f":"code","00022185":"code","97aafbdd":"code","a8b3d9c6":"code","64faca22":"code","c5ba697c":"code","3acffe69":"code","f683f2ff":"code","9f42f2f7":"code","74012ee8":"code","4150d74a":"code","7f304b1f":"code","126a0016":"code","395853ce":"code","43f4ebd5":"code","d6c04004":"code","8d6505e5":"code","3d0effe4":"markdown","8ffecd49":"markdown","571bba88":"markdown","50b03134":"markdown","00752d12":"markdown","aa4a800c":"markdown","8fd0e698":"markdown","6fa6ca04":"markdown","3c565600":"markdown","3c9790fe":"markdown","7384da9b":"markdown","971dc40b":"markdown","a92a5c14":"markdown","0b0ec349":"markdown","97fb9977":"markdown","d34633c4":"markdown","b9879434":"markdown","ab6d317b":"markdown","e97e3f55":"markdown","051d4339":"markdown","99431f66":"markdown","2c3e0f6c":"markdown","aa95ba14":"markdown","edb89ee2":"markdown","265728f4":"markdown","857f4aee":"markdown","30398543":"markdown","a913331c":"markdown","1b60e06f":"markdown","64c4110f":"markdown","bb68ee4f":"markdown","be1db380":"markdown","2b90c188":"markdown","66f0ea4d":"markdown","41d67f78":"markdown","68861c09":"markdown","397cc0f0":"markdown","bba6aa71":"markdown","601cfb61":"markdown","703e989d":"markdown","a6577c8e":"markdown","8f85605a":"markdown","ac1b4cd9":"markdown","c7e99bdc":"markdown","bdab6c04":"markdown","c2a92f18":"markdown","0272f1cd":"markdown","d30b3fc4":"markdown","98171538":"markdown","33220ebd":"markdown","ddc0086d":"markdown","7b768d3c":"markdown","781da347":"markdown","50f72763":"markdown","a4b51a11":"markdown","b3637a0f":"markdown","7ae5a1a5":"markdown","3ed39b12":"markdown","9e68ec71":"markdown","4360b35a":"markdown","6dd00cfb":"markdown","59aeba49":"markdown","b2e80aec":"markdown","983f998a":"markdown","2c5f7c4a":"markdown","198228d0":"markdown","43ab2b2a":"markdown","7f536d1b":"markdown","f5c97006":"markdown","5ff06954":"markdown","8e9b3340":"markdown","73524590":"markdown","5cacf53b":"markdown","3cb5e2a9":"markdown","a35125b5":"markdown","6802c88c":"markdown","6d191554":"markdown","7015f3c9":"markdown","b3b4b818":"markdown","819fbdce":"markdown","1e668450":"markdown","37449760":"markdown","7b0f825a":"markdown","ff19c8a7":"markdown","e7b7bc12":"markdown","d291be95":"markdown","93e16358":"markdown","16c3cef2":"markdown","21320ee2":"markdown","ecd8449b":"markdown","64fb413e":"markdown","5a923197":"markdown","69f13167":"markdown","6e321f43":"markdown","411a420d":"markdown","b9ba43f0":"markdown","3829bd03":"markdown","89e8e798":"markdown","8d3d04d3":"markdown","77396899":"markdown","854001e1":"markdown","53553b1e":"markdown","9a0fc5b4":"markdown","20ae33a1":"markdown","b40c0859":"markdown","9f4f7f85":"markdown","36de2457":"markdown","987d8bfb":"markdown","6bc15dea":"markdown","16ca2b4f":"markdown","1bcceff6":"markdown","11031a1a":"markdown","b195e1ec":"markdown","7498288c":"markdown","0253d3d5":"markdown","65c238ac":"markdown","3bae9200":"markdown","bb470fc0":"markdown","bc24433a":"markdown","6997e636":"markdown","a98c20cb":"markdown","6676730a":"markdown","37b4bffe":"markdown","36233be1":"markdown","db1d8c18":"markdown"},"source":{"b446657b":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nfrom statsmodels.stats.weightstats import ztest as ztest\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score, confusion_matrix, f1_score, r2_score\nfrom collections import Counter\nfrom sklearn.datasets import make_classification\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom scipy.stats.mstats import winsorize\n\n# pd.set_option('display.max_rows', 1500)\npd.set_option('display.max_columns', 100)\n# pd.set_option('display.width', 1000)\n\nraw_data = pd.read_csv('..\/input\/ibm-hr-analytics-attrition-dataset\/WA_Fn-UseC_-HR-Employee-Attrition.csv')\nengineered_data = pd.DataFrame([raw_data.Attrition]).T\nprint(\"Set up libraries and loaded raw_data\")","71efa398":"raw_data.info()","02174f15":"data_description = raw_data.describe(include=\"all\")\ndisplay(data_description[[\"EmployeeCount\", \"EmployeeNumber\", \"Over18\", \"StandardHours\"]])","ea2364dd":"print(f\"There is only {round(raw_data['Attrition'].replace({'Yes': 1, 'No': 0}).mean(), 3) * 100} % records with Attritioned employees\")","1419b198":"raw_data.head()","ffbaedf9":"sns.set(rc={'figure.figsize':(30,30)})\nsns.heatmap(raw_data.corr(method=\"pearson\"), annot=True)\nsns.set(rc={'figure.figsize':(10,10)})","e4977cad":"print(f\"Employee Number is unique for every record: {raw_data.EmployeeNumber.nunique() == raw_data.shape[0]}\")\nprint(f\"Employee Count is same for every record: {raw_data.EmployeeCount.nunique() == 1}\")\nprint(f\"Over 18 is same for every record: {raw_data.Over18.nunique() == 1}\")\nprint(f\"Standard Hours is same for every record: {raw_data.StandardHours.nunique() == 1}\")\n","14479387":"raw_data.drop([\"EmployeeNumber\", \"EmployeeCount\", \"Over18\", \"StandardHours\"], axis=1, inplace=True)\nprint(\"Dropped EmloyeeNumber, EmployeeCount, Over18 and StandardHours features.\")","9f2e8c0c":"raw_data.head()","54130e0f":"cont_income_columns = [\"MonthlyRate\", \"MonthlyIncome\", \"Daily Rate * 20\", \"DailyRate\", \"Hourly Rate * 80\", \"HourlyRate\"]\ncont_income_data = pd.concat([raw_data.MonthlyRate, raw_data.MonthlyIncome, raw_data.DailyRate*20, raw_data.DailyRate, raw_data.HourlyRate * 80, raw_data.HourlyRate], axis=1, keys=cont_income_columns)\ncont_income_data.head()","ce878cb8":"sns.heatmap(cont_income_data.corr(method=\"pearson\"), annot=True)","a32774f0":"f, axes = plt.subplots(1, 3, figsize=(30, 10))\n\nx = 0\nfor col in cont_income_columns:\n    if col not in raw_data.columns or col == \"MonthlyIncome\":\n        continue\n    g = sns.boxplot(data=raw_data, x=\"JobLevel\", y=col, ax=axes[x], hue=\"Attrition\") \n    axes[x].set( title = f'{col} distribution among Job Level and Attrition')\n    sns.move_legend(g, \"upper right\")\n    x += 1","b396bc3f":"sns.set(rc={'figure.figsize':(16,12)})\ng = sns.boxplot(data=raw_data, x=\"JobLevel\", y=\"MonthlyIncome\", hue=\"Attrition\")","31ffedc3":"sns.set(rc={'figure.figsize':(16,12)})\ng = sns.boxplot(data=raw_data, y=\"MonthlyIncome\", x=\"Attrition\")","ddef5249":"raw_data.loc[:, \"MonthlyIncome\"] = winsorize(raw_data[\"MonthlyIncome\"], limits=[0., 0.08])\nsns.boxplot(data=raw_data, y=\"MonthlyIncome\", x=\"Attrition\")","e79099c8":"engineered_data = engineered_data.assign(MonthlyIncome=raw_data[\"MonthlyIncome\"])\nengineered_data.head()","1fccc57f":"cat_income_columns = [\"StockOptionLevel\", \"JobRole\", \"JobLevel\", \"Department\"]\ncat_income_data = raw_data[cat_income_columns + [\"Attrition\"]]\ncat_income_data.head()","82133c1e":"cat_income_desc = cat_income_data.groupby(\"Attrition\").describe()\ncat_income_desc.iloc[:, cat_income_desc.columns.get_level_values(1).isin([\"mean\", \"std\", \"min\", \"max\"])]","af64bf21":"attritioned = cat_income_data[\"Attrition\"] == \"Yes\"\nattritioned_workers = cat_income_data[attritioned]\nnon_attritioned_workers = cat_income_data[~attritioned]\n\nsimilar_means = []\ndifferent_means = []\n\nfor col in [\"StockOptionLevel\", \"JobLevel\"]:\n    if col == \"Attrition\":\n        continue\n        \n    test_statistic, p_value = ztest(attritioned_workers[col], non_attritioned_workers[col], value=0) \n    print(f\"Feature: {col}\")\n    \n    attritioned_statistics = attritioned_workers[col].describe()\n    non_attritioned_statistics = non_attritioned_workers[col].describe()\n    print(f\"Attritioned: mean: {attritioned_statistics['mean']}, std: {attritioned_statistics['std']}\")\n    print(f\"Non-Attritioned: mean: {non_attritioned_statistics['mean']}, std: {non_attritioned_statistics['std']}\")\n    print(f\"The test statistic for the two sample z-test of {col} is {test_statistic} and the corresponding p-value is {p_value}.\")\n    if p_value < 0.05:\n        print(f\"Mean of {col} is different\")\n        different_means.append(col)\n    else:\n        print(\"Mean of {col} is similar\")\n        similar_means.append(col)\n    print()\n    \nprint(f\"Similar means: {similar_means}\")\nprint(f\"Different means: {different_means}\")","0950f8e1":"f, axes = plt.subplots(2, 4, figsize=(40, 20))\ngrouped_cat_income_data = cat_income_data.groupby(\"Attrition\")\n\nx = 0\nfor col in cat_income_columns:\n    if col == \"Attrition\":\n        continue\n    try:\n        g = sns.boxenplot(data=cat_income_data, x=\"Attrition\", y=col, ax=axes[0, x]) \n        axes[0, x].set( title = f'{col} dependency on Attrition')\n    except:\n        pass\n    \n    feature_val_counts = grouped_cat_income_data[col].value_counts(normalize=True)\n    feature_val_counts = feature_val_counts.mul(100).rename('Percent').reset_index()\n    grouped_feature = feature_val_counts.groupby(col)\n    for group_name, group_data in grouped_feature:\n        feature_val_counts.loc[feature_val_counts[col] == group_name, \"Percent\"] *= 100\/group_data.Percent.sum()\n    feature_val_counts = feature_val_counts.round(1)\n    \n    g = sns.barplot(data=feature_val_counts, x=col, y=\"Percent\", hue=\"Attrition\", ax=axes[1, x], alpha=1., zorder=10) \n    for container in g.containers:\n        g.bar_label(container)\n    axes[1, x].set( title = f'{col} dependency on Attrition, distribution inside each value')\n    if col == \"JobRole\":\n        axes[1, x].tick_params(labelrotation=45)\n        \n    x += 1","db70e8be":"engineered_data = engineered_data.assign(StockOptionLevel=raw_data[\"StockOptionLevel\"], JobRole=raw_data[\"JobRole\"], JobLevel=raw_data[\"JobLevel\"], Department=raw_data[\"Department\"])\nengineered_data.head()","0791685e":"job_satisfaction_columns = [\"JobInvolvement\", \"EnvironmentSatisfaction\", \"JobSatisfaction\", \"WorkLifeBalance\", \"RelationshipSatisfaction\", \"PerformanceRating\", \"DistanceFromHome\", \"PercentSalaryHike\"]\njob_satisfaction_data = raw_data[job_satisfaction_columns + [\"Attrition\"]]\njob_satisfaction_data.head()","172330f2":"sns.heatmap(job_satisfaction_data.corr(method=\"pearson\"), annot=True)","5bb69864":"job_satisfaction_desc = job_satisfaction_data.groupby(\"Attrition\").describe()\njob_satisfaction_desc.iloc[:, job_satisfaction_desc.columns.get_level_values(1).isin([\"mean\", \"std\", \"min\", \"max\"])]","47ff8c78":"f, axes = plt.subplots(2, 8, figsize=(80, 20))\ngrouped_demographics_data = job_satisfaction_data.groupby(\"Attrition\")\n\nx = 0\nfor col in job_satisfaction_columns: # + [\"TotalSatisfaction\"]:\n    if col == \"Attrition\":\n        continue\n    g = sns.boxplot(data=job_satisfaction_data, x=\"Attrition\", y=col, ax=axes[0, x]) \n    axes[0, x].set( title = f'{col} dependency on Attrition')\n    if job_satisfaction_data[col].max() <= 4.:\n        axes[0,x].set(ylim=(1,4))\n    \n    feature_val_counts = grouped_demographics_data[col].value_counts(normalize=True)\n    feature_val_counts = feature_val_counts.mul(100).rename('Percent').reset_index()\n    grouped_feature = feature_val_counts.groupby(col)\n    for group_name, group_data in grouped_feature:\n        feature_val_counts.loc[feature_val_counts[col] == group_name, \"Percent\"] *= 100\/group_data.Percent.sum()\n    feature_val_counts = feature_val_counts.round(1)\n    \n    if feature_val_counts[col].nunique() <= 5:\n        g = sns.barplot(data=feature_val_counts, x=col, y=\"Percent\", hue=\"Attrition\", ax=axes[1, x], alpha=1., zorder=10) \n        for container in g.containers:\n            g.bar_label(container)\n    else:\n        g = sns.lineplot(data=feature_val_counts, x=col, y=\"Percent\", hue=\"Attrition\", ax=axes[1, x])\n    axes[1, x].set( title = f'{col} dependency on Attrition, distribution inside each value')\n    x += 1","21f90c85":"attritioned = job_satisfaction_data[\"Attrition\"] == \"Yes\"\nattritioned_workers = job_satisfaction_data[attritioned]\nnon_attritioned_workers = job_satisfaction_data[~attritioned]\n\nsimilar_means = []\ndifferent_means = []\n\nfor col in job_satisfaction_columns:\n    if col == \"Attrition\":\n        continue\n        \n    test_statistic, p_value = ztest(attritioned_workers[col], non_attritioned_workers[col], value=0) \n    print(f\"Feature: {col}\")\n    \n    attritioned_statistics = attritioned_workers[col].describe()\n    non_attritioned_statistics = non_attritioned_workers[col].describe()\n    print(f\"Attritioned: mean: {attritioned_statistics['mean']}, std: {attritioned_statistics['std']}\")\n    print(f\"Non-Attritioned: mean: {non_attritioned_statistics['mean']}, std: {non_attritioned_statistics['std']}\")\n    print(f\"The test statistic for the two sample z-test of {col} is {test_statistic} and the corresponding p-value is {p_value}.\")\n    if p_value < 0.05:\n        print(f\"Mean of {col} is different\")\n        different_means.append(col)\n    else:\n        print(\"Mean of {col} is similar\")\n        similar_means.append(col)\n    print()\n    \nprint(f\"Similar means: {similar_means}\")\nprint(f\"Different means: {different_means}\")","1cfe97e8":"sns.boxplot(data=raw_data, x=\"Attrition\", y=\"DistanceFromHome\")","e15d6dea":"distance_from_home = pd.Series((1 - (raw_data.DistanceFromHome - raw_data.DistanceFromHome.min()) \/ (raw_data.DistanceFromHome.max() - raw_data.DistanceFromHome.min())) * 4 , name=\"UpdatedDistanceFromHome\")\ndisplay(pd.concat([distance_from_home, raw_data.DistanceFromHome], axis=1).head())","ea52aa96":"total_satisfaction_add = pd.Series(raw_data.JobInvolvement + raw_data.EnvironmentSatisfaction + raw_data.JobSatisfaction + raw_data.WorkLifeBalance + distance_from_home, name=\"TotalSatisfaction\")\ntotal_satisfaction_mul = pd.Series(raw_data.JobInvolvement * raw_data.EnvironmentSatisfaction * raw_data.JobSatisfaction * raw_data.WorkLifeBalance * distance_from_home, name=\"TotalSatisfaction\")","0dbcbb27":"job_satisfaction_data = job_satisfaction_data.assign(TotalSatisfactionAdd=total_satisfaction_add, TotalSatisfactionMul=total_satisfaction_mul)\njob_satisfaction_data.head()","8abf826c":"f, axes = plt.subplots(1, 2, figsize=(20, 10))\n\ng = sns.boxplot(data=job_satisfaction_data, y=\"TotalSatisfactionAdd\", x=\"Attrition\", ax=axes[0])\ng = sns.boxplot(data=job_satisfaction_data, y=\"TotalSatisfactionMul\", x=\"Attrition\", ax=axes[1])","cf775b5d":"job_satisfaction_data.loc[:, \"TotalSatisfactionMul\"] = winsorize(job_satisfaction_data[\"TotalSatisfactionMul\"], limits=[0., 0.03])\nsns.boxplot(data=job_satisfaction_data, y=\"TotalSatisfactionMul\", x=\"Attrition\")","47456669":"col = \"TotalSatisfaction\"\nattritioned_workers_total_satisfaction = job_satisfaction_data[attritioned]\nnon_attritioned_workers_total_satisfaction = job_satisfaction_data[~attritioned]\n\nsimilar_means = []\ndifferent_means = []\n\nfor col in [\"TotalSatisfactionAdd\", \"TotalSatisfactionMul\"]:\n    test_statistic, p_value = ztest(attritioned_workers_total_satisfaction[col], non_attritioned_workers_total_satisfaction[col], value=0) \n    print(f\"Col: {col}\")\n\n    attritioned_statistics = attritioned_workers_total_satisfaction[col].describe()\n    non_attritioned_statistics = non_attritioned_workers_total_satisfaction[col].describe()\n    print(f\"Attritioned: mean: {attritioned_statistics['mean']}, std: {attritioned_statistics['std']}\")\n    print(f\"Non-Attritioned: mean: {non_attritioned_statistics['mean']}, std: {non_attritioned_statistics['std']}\")\n    print(f\"The test statistic for the two sample z-test of {col} is {test_statistic} and the corresponding p-value is {p_value}.\")\n    if p_value < 0.05:\n        print(f\"Mean of {col} is different\")\n        different_means.append(col)\n    else:\n        print(\"Mean of {col} is similar\")\n        similar_means.append(col)\n        \nprint(f\"Similar means: {similar_means}\")\nprint(f\"Different means: {different_means}\")","7e084e9d":"engineered_data = engineered_data.assign(TotalSatisfactionAdd=job_satisfaction_data[\"TotalSatisfactionAdd\"], TotalSatisfactionMul=job_satisfaction_data[\"TotalSatisfactionAdd\"])\nengineered_data.head()","9b94e828":"demographics_columns = [\"Age\", \"Education\", \"EducationField\", \"Gender\", \"MaritalStatus\", \"Attrition\"]\ndemographics_data = raw_data[demographics_columns].copy(deep=True)\ndemographics_data.head()","f034e809":"grouped_demographics_data = demographics_data.groupby(\"Attrition\")","d21106c6":"sns.boxplot(data=raw_data, y=\"Age\", x=\"Attrition\")","54ff240b":"col = \"Age\"\nfeature_val_counts = grouped_demographics_data[col].value_counts(normalize=True)\nfeature_val_counts = feature_val_counts.mul(100).rename('Percent').reset_index()\ngrouped_feature = feature_val_counts.groupby(col)\nfor group_name, group_data in grouped_feature:\n    feature_val_counts.loc[feature_val_counts[col] == group_name, \"Percent\"] *= 100\/group_data.Percent.sum()\n\ng = sns.lineplot(data=feature_val_counts, x=col, y=\"Percent\", hue=\"Attrition\") \ng.set( title = f'{col} dependency on Attrition')","96046def":"demographics_data.loc[:, \"Education\"] = demographics_data[\"Education\"].replace({1: 10, 2: 12, 3: 16, 4: 18, 5: 22})\ngrouped_demographics_data = demographics_data.groupby(\"Attrition\")","27542837":"f, axes = plt.subplots(1, 4, figsize=(50, 10))\n\nx = 0\nfor col in demographics_columns:\n    if col in [\"Attrition\", \"Age\"]:\n        continue\n    feature_val_counts = grouped_demographics_data[col].value_counts(normalize=True)\n    feature_val_counts = feature_val_counts.mul(100).rename('Percent').reset_index()\n    grouped_feature = feature_val_counts.groupby(col)\n    for group_name, group_data in grouped_feature:\n        feature_val_counts.loc[feature_val_counts[col] == group_name, \"Percent\"] *= 100\/group_data.Percent.sum()\n    feature_val_counts = feature_val_counts.round(1)\n    \n    g = sns.barplot(data=feature_val_counts, x=col, y=\"Percent\", hue=\"Attrition\", ax=axes[x]) \n    axes[x].set( title = f'{col} dependency on Attrition')\n    x += 1\n    for container in g.containers:\n        g.bar_label(container)","332bc909":"engineered_data = engineered_data.assign(Age=demographics_data[\"Age\"], EducationField=demographics_data[\"EducationField\"], MaritalStatus=demographics_data[\"MaritalStatus\"], Education=demographics_data[\"Education\"])\nengineered_data.head()","546ea2a2":"cat_work_place_columns = [\"OverTime\", \"BusinessTravel\", \"TrainingTimesLastYear\", \"NumCompaniesWorked\"]\ncat_work_place_data = raw_data[cat_work_place_columns + [\"Attrition\"]]\ncat_work_place_data.head()","75c135f1":"grouped_demographics_data = cat_work_place_data.groupby(\"Attrition\")","be6e1b1b":"work_place_desc = grouped_demographics_data.describe()\nwork_place_desc.iloc[:, work_place_desc.columns.get_level_values(1).isin([\"mean\", \"std\", \"min\", \"max\"])]","02e3afb2":"attritioned = cat_work_place_data[\"Attrition\"] == \"Yes\"\nattritioned_workers = cat_work_place_data[attritioned]\nnon_attritioned_workers = cat_work_place_data[~attritioned]\n\nsimilar_means = []\ndifferent_means = []\n\nfor col in [\"TrainingTimesLastYear\", \"NumCompaniesWorked\"]:\n    if col == \"Attrition\":\n        continue\n        \n    test_statistic, p_value = ztest(attritioned_workers[col], non_attritioned_workers[col], value=0) \n    print(f\"Feature: {col}\")\n    \n    attritioned_statistics = attritioned_workers[col].describe()\n    non_attritioned_statistics = non_attritioned_workers[col].describe()\n    print(f\"Attritioned: mean: {attritioned_statistics['mean']}, std: {attritioned_statistics['std']}\")\n    print(f\"Non-Attritioned: mean: {non_attritioned_statistics['mean']}, std: {non_attritioned_statistics['std']}\")\n    print(f\"The test statistic for the two sample z-test of {col} is {test_statistic} and the corresponding p-value is {p_value}.\")\n    if p_value < 0.05:\n        print(f\"Mean of {col} is different\")\n        different_means.append(col)\n    else:\n        print(\"Mean of {col} is similar\")\n        similar_means.append(col)\n    print()\n    \nprint(f\"Similar means: {similar_means}\")\nprint(f\"Different means: {different_means}\")","a84c4827":"f, axes = plt.subplots(1, 4, figsize=(40, 10))\ndistribution_display = False\n\nx = 0\nfor col in cat_work_place_columns:\n    if col in [\"Attrition\", \"Age\"]:\n        continue\n    feature_val_counts = grouped_demographics_data[col].value_counts(normalize=True)\n    feature_val_counts = feature_val_counts.mul(100).rename('Percent').reset_index()\n    grouped_feature = feature_val_counts.groupby(col)\n    for group_name, group_data in grouped_feature:\n        feature_val_counts.loc[feature_val_counts[col] == group_name, \"Percent\"] *= 100\/group_data.Percent.sum()\n    feature_val_counts = feature_val_counts.round(1)\n    \n    if distribution_display:\n        if col != \"OverTime\":\n            ax2 = axes[x].twinx()\n            for i, j in grouped_demographics_data:\n                sns.kdeplot(data=j, x=col, ax=ax2, fill=True, alpha=0.2, linewidth=2, label=i) \n                plt.legend(loc=\"upper left\")\n            ax22 = axes[x+1].twinx()\n            for i, j in grouped_demographics_data:\n                sns.kdeplot(data=j, x=col, ax=ax22, fill=True, alpha=0.2, linewidth=2, label=i) \n                plt.legend(loc=\"upper left\")\n\n            sns.move_legend(ax2, \"upper left\")\n            sns.countplot(data=work_place_data, x=col, hue=\"Attrition\", ax=axes[x+1], alpha=0.8,) \n    g = sns.barplot(data=feature_val_counts, x=col, y=\"Percent\", hue=\"Attrition\", ax=axes[x], alpha=1., zorder=10) \n    axes[x].set( title = f'{col} dependency on Attrition')\n    x += 1\n    for container in g.containers:\n        g.bar_label(container)","9f2e08da":"engineered_data = engineered_data.assign(OverTime=raw_data[\"OverTime\"], BusinessTravel=raw_data[\"BusinessTravel\"], TrainingTimesLastYear=raw_data[\"TrainingTimesLastYear\"], NumCompaniesWorked=raw_data[\"NumCompaniesWorked\"])\nengineered_data.head()","8d071ffb":"cont_work_place_columns = [\"YearsAtCompany\", \"YearsInCurrentRole\", \"YearsSinceLastPromotion\", \"YearsWithCurrManager\", \"TotalWorkingYears\"]\ncont_work_place_data = raw_data[cont_work_place_columns + [\"JobLevel\", \"Age\", \"Attrition\"]].copy(deep=True)\ncont_work_place_data.head()","5df3e1a2":"sns.heatmap(cont_work_place_data.corr(method=\"pearson\"), annot=True)","cf558209":"f, axes = plt.subplots(1, 5, figsize=(50, 10))\nf.suptitle(\"Correlated Work-place related features with Age and JobLevel encoded\")\n\nx = 0\nfor feature in cont_work_place_columns:\n    s = np.linspace(0, 3, 10)\n    cmap = sns.cubehelix_palette(start=0.0, light=1, as_cmap=True)\n\n    # Generate and plot\n    a = winsorize(cont_work_place_data[feature], limits=[0., 0.08])\n    cont_work_place_data.loc[:, feature] = a\n    sns.boxplot(data=cont_work_place_data, y=feature, x=\"Attrition\", ax=axes[x]) # cut=5, \n    axes[x].set( title=f'Total working years against {feature}')\n    x += 1       ","dfe1c924":"f, axes = plt.subplots(1, 4, figsize=(40, 10))\nf.suptitle(\"Correlated Work-place related features with Age and JobLevel encoded\")\n\nx = 0\nfor feature in cont_work_place_columns:\n    if feature == \"TotalWorkingYears\":\n        continue\n    s = np.linspace(0, 3, 10)\n    cmap = sns.cubehelix_palette(start=0.0, light=1, as_cmap=True)\n\n    # Generate and plot\n    sns.scatterplot(data=cont_work_place_data, x=\"TotalWorkingYears\", y=feature, size=\"Age\", hue=\"JobLevel\", cmap=cmap, ax=axes[x]) # cut=5, \n    axes[x].set( title=f'Total working years against {feature}')\n    x += 1       ","a8f5eb71":"# Separating out the features\nstandardized_features = raw_data.loc[:, cont_work_place_columns]\n# Standardizing the features\nstandardized_features = pd.DataFrame(StandardScaler().fit_transform(standardized_features), columns=standardized_features.columns)\nstandardized_features.head()","f6045d53":"pca = PCA(n_components=4)\nprincipalComponents = pd.DataFrame(pca.fit_transform(standardized_features), columns=[\"PCA 1\", \"PCA 2\", \"PCA 3\", \"PCA 4\"])","a99c0db2":"display(principalComponents.head())","0947686c":"display(pca.components_)","da8deaaf":"sum(pca.explained_variance_ratio_)","0a41df32":"sns.heatmap(principalComponents.corr(method=\"pearson\"), annot=True)","99673bee":"engineered_data_pca = engineered_data.copy(deep=True)\n\nengineered_data_pca = engineered_data_pca.assign(PCA1=principalComponents[\"PCA 1\"], PCA2=principalComponents[\"PCA 2\"], PCA3=principalComponents[\"PCA 3\"], PCA4=principalComponents[\"PCA 4\"])\nengineered_data_pca.head()","52c48310":"from sklearn.cluster import KMeans\nscores = []\ncont_work_place_data = raw_data.loc[:, cont_work_place_columns]\n    \nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters=i, random_state=0).fit(StandardScaler().fit_transform(raw_data.loc[:, cont_work_place_columns]))    \n    scores.append(kmeans.inertia_)\n    \nH_rule = []\nfor i in range(1, 10):\n    H_rule.append((scores[i-1] - scores[i]) \/ (scores[i] * (len(cont_work_place_data) - i - 1)))\n    \nscores = pd.DataFrame([pd.Series(range(1, 11), name=\"No Clusters\"), pd.Series(scores, name=\"Inertia\"), pd.Series(H_rule, name=\"Hartigan\")]).T\ndisplay(scores)\n","ea5a0d3d":"f, axes = plt.subplots(1, 2, figsize=(20, 10))\nsns.lineplot(data=scores, x=\"No Clusters\", y=\"Inertia\", markers=True, ax=axes[0])\nsns.lineplot(data=scores, x=\"No Clusters\", y=\"Hartigan\", markers=True, ax=axes[1])","3c0d146b":"scaler = StandardScaler()\nscaled_data = scaler.fit_transform(raw_data.loc[:, cont_work_place_columns])\n\nkmeans = KMeans(n_clusters=4, random_state=0).fit(scaled_data)\n\ncluster_centers = pd.DataFrame(scaler.inverse_transform(kmeans.cluster_centers_), columns=cont_work_place_columns)\ncluster_centers","2a318c04":"raw_data = raw_data.assign(JobSeniority=kmeans.labels_)\nraw_data.JobSeniority = raw_data.JobSeniority.replace({0: \"Junior\", 1: \"Devoted Senior\", 2: \"Senior\", 3: \"Medior\"})\ndisplay(pd.DataFrame([raw_data.groupby(\"JobSeniority\").JobLevel.mean(), raw_data.groupby(\"JobSeniority\").MonthlyIncome.mean(), raw_data.groupby(\"JobSeniority\").Education.mean()]).T.sort_values(by=['JobLevel'], ascending=False))","59351374":"engineered_data_kmeans = engineered_data.copy(deep=True)\nengineered_data_kmeans = engineered_data_kmeans.assign(JobSeniority=kmeans.labels_)\nengineered_data_kmeans.JobSeniority = engineered_data_kmeans.JobSeniority.replace({0: \"Senior\", 1: \"Medior\", 2: \"Devoted Senior\", 3: \"Junior\"})\nengineered_data_kmeans.head()","4ec7caba":"display(engineered_data_kmeans)","03a3b447":"display(engineered_data_pca)","0ae3c09f":"X_pca = engineered_data_pca.loc[:, engineered_data_pca.columns != 'Attrition'].copy(deep=True)\ny_pca = engineered_data_pca.loc[:, 'Attrition'].copy(deep=True)\nX_pca.head()","00022185":"X_kmeans = engineered_data_kmeans.loc[:, engineered_data_kmeans.columns != 'Attrition'].copy(deep=True)\ny_kmeans = engineered_data_kmeans.loc[:, 'Attrition'].copy(deep=True)\nX_kmeans.head()","97aafbdd":"enc = OneHotEncoder(handle_unknown='ignore')\nlabel_columns = [\"JobSeniority\", \"JobRole\", \"Department\", \"EducationField\", \"MaritalStatus\", \"OverTime\", \"BusinessTravel\"]","a8b3d9c6":"for feature_name in label_columns:\n    if feature_name in X_pca.columns:\n        e = pd.DataFrame(enc.fit_transform(X_pca[[feature_name]]).toarray(), columns=enc.categories_)\n        X_pca[enc.categories_[0]] = pd.DataFrame(enc.fit_transform(X_pca[[feature_name]]).toarray(), columns=enc.categories_)\nX_pca = X_pca.drop(label_columns, axis=1, errors='ignore')\nX_pca = X_pca.rename(columns={'Yes': 'OverTime: Yes', 'No': 'OverTime: No'})\ndisplay(X_pca.head())\n\ny_pca = y_pca.replace({\"No\": 0, \"Yes\": 1})    ","64faca22":"for feature_name in label_columns:\n    if feature_name in X_kmeans.columns:\n        e = pd.DataFrame(enc.fit_transform(X_kmeans[[feature_name]]).toarray(), columns=enc.categories_)\n        X_kmeans[enc.categories_[0]] = pd.DataFrame(enc.fit_transform(X_kmeans[[feature_name]]).toarray(), columns=enc.categories_)\nX_kmeans = X_kmeans.drop(label_columns, axis=1, errors='ignore')\nX_kmeans = X_kmeans.rename(columns={'Yes': 'OverTime: Yes', 'No': 'OverTime: No'})\ndisplay(X_kmeans.head())\n\ny_kmeans = y_kmeans.replace({\"No\": 0, \"Yes\": 1})    ","c5ba697c":"X_pca_train, X_pca_test, y_pca_train, y_pca_test = train_test_split(X_pca, y_pca, test_size=0.2, random_state=0, stratify=y_pca)\nX_pca_train, X_pca_val, y_pca_train, y_pca_val = train_test_split(X_pca_train, y_pca_train, test_size=0.25, random_state=0, stratify=y_pca_train)\nX_kmeans_train, X_kmeans_test, y_kmeans_train, y_kmeans_test = train_test_split(X_kmeans, y_kmeans, test_size=0.2, random_state=0, stratify=y_kmeans)\nX_kmeans_train, X_kmeans_val, y_kmeans_train, y_kmeans_val = train_test_split(X_kmeans_train, y_kmeans_train, test_size=0.25, random_state=0, stratify=y_kmeans_train)","3acffe69":"model = RandomForestClassifier(random_state=0)\nparams = [{\"max_depth\":[3, 4, 5, 6, 7, 8, 9, 10], }]\ngs_knn = GridSearchCV(model,\n                      param_grid=params,\n                      scoring='f1',\n                      cv=10)\n\nfig, ax = plt.subplots(1, 2, figsize=(20, 10))\n\nx=0\nfor X_train, y_train, X_val, y_val, dataset in [[X_kmeans_train, y_kmeans_train, X_kmeans_val, y_kmeans_val,\"kmeans\"], [X_pca_train, y_pca_train, X_pca_val, y_pca_val, \"pca\"]]:\n    print(f\"Training Random Forest Classifier on imbalanced '{dataset}' dataset\")\n    gs_knn.fit(X_train, y_train)\n    print(f\"Best parameters: {gs_knn.best_params_}\")\n    print(f\"Corresponding f1 score: {gs_knn.score(X_val, y_val)}\")\n    y_pred = gs_knn.best_estimator_.predict(X_val)\n    print(f\"With confusion matrix:\")\n    print(confusion_matrix(y_val, y_pred))    \n    \n    importances = gs_knn.best_estimator_.feature_importances_\n    std = np.std([tree.feature_importances_ for tree in gs_knn.best_estimator_.estimators_], axis=0)\n\n    forest_importances = pd.Series(importances, index=X_train.columns).sort_values().T\n\n    forest_importances.plot.barh(yerr=std, ax=ax[x])\n    ax[x].set_title(f\"{dataset} feature importances using MDI\")\n    ax[x].set_ylabel(\"Mean decrease in impurity\")\n    fig.tight_layout()\n    ax[x].set(xlim=(0,0.15))\n    x += 1","f683f2ff":"def SMOTE_transform(X, y, dataset):\n    # summarize class distribution\n    counter = Counter(y)\n    print(f\"Old target '{dataset}' dataset data count: {counter}\")\n    # define pipeline\n    over = SMOTE(sampling_strategy=0.5)\n    under = RandomUnderSampler(sampling_strategy=1.)\n    steps = [('o', over), ('u', under)]\n    pipeline = Pipeline(steps=steps)\n    # transform the dataset\n    X, y = pipeline.fit_resample(X, y)\n    # summarize the new class distribution\n    counter = Counter(y)\n    print(f\"Old target '{dataset}' dataset data count: {counter}\")\n    return X, y","9f42f2f7":"# Oversample with SMOTE and random undersample for imbalanced dataset\n\nX_pca, y_pca = SMOTE_transform(X_pca, y_pca, \"pca\")\nX_kmeans, y_kmeans = SMOTE_transform(X_kmeans, y_kmeans, \"kmeans\")","74012ee8":"X_pca_train, X_pca_test, y_pca_train, y_pca_test = train_test_split(X_pca, y_pca, test_size=0.2, random_state=0, stratify=y_pca)\nX_pca_train, X_pca_val, y_pca_train, y_pca_val = train_test_split(X_pca_train, y_pca_train, test_size=0.25, random_state=0, stratify=y_pca_train)\nX_kmeans_train, X_kmeans_test, y_kmeans_train, y_kmeans_test = train_test_split(X_kmeans, y_kmeans, test_size=0.2, random_state=0, stratify=y_kmeans)\nX_kmeans_train, X_kmeans_val, y_kmeans_train, y_kmeans_val = train_test_split(X_kmeans_train, y_kmeans_train, test_size=0.25, random_state=0, stratify=y_kmeans_train)","4150d74a":"model = RandomForestClassifier(random_state=0)\nparams = [{\"max_depth\":[5, 6, 7, 8, 9, 10, 11, 12], }]\nRFC_CV = GridSearchCV(model,\n                      param_grid=params,\n                      scoring='f1',\n                      cv=10)\n\n\nfig, ax = plt.subplots(1, 2, figsize=(20, 10))\n\nx=0\nfor X_train, y_train, X_val, y_val, dataset in [[X_kmeans_train, y_kmeans_train, X_kmeans_val, y_kmeans_val,\"kmeans\"], [X_pca_train, y_pca_train, X_pca_val, y_pca_val, \"pca\"]]:\n    print(f\"Training Random Forest Classifier on balanced '{dataset}' dataset\")\n    RFC_CV.fit(X_train, y_train)\n    print(f\"Best parameters: {RFC_CV.best_params_}\")\n    print(f\"Corresponding f1 score: {RFC_CV.score(X_val, y_val)}\")\n    y_pred = RFC_CV.best_estimator_.predict(X_val)\n    print(f\"With confusion matrix:\")\n    print(confusion_matrix(y_val, y_pred))    \n    print(F\"R2: score: {r2_score(y_val, y_pred)}\")\n    \n    importances = RFC_CV.best_estimator_.feature_importances_\n    std = np.std([tree.feature_importances_ for tree in RFC_CV.best_estimator_.estimators_], axis=0)\n\n    forest_importances = pd.Series(importances, index=X_train.columns).sort_values().T\n\n    forest_importances.plot.barh(yerr=std, ax=ax[x])\n    ax[x].set_title(f\"{dataset} feature importances using MDI\")\n    ax[x].set_ylabel(\"Mean decrease in impurity\")\n    fig.tight_layout()\n    ax[x].set(xlim=(0,0.15))\n    x += 1","7f304b1f":"from sklearn.ensemble import AdaBoostClassifier\n\n\nclf = AdaBoostClassifier(random_state=0)\nparams = [{\"n_estimators\": [10, 20, 50, 80, 100, 150, 200, 250, 300]}]\nABC_CV = GridSearchCV(clf,\n                      param_grid=params,\n                      scoring='f1',\n                      cv=10)\n\nx=0\nfor X_train, y_train, X_val, y_val, dataset in [[X_kmeans_train, y_kmeans_train, X_kmeans_val, y_kmeans_val,\"kmeans\"], [X_pca_train, y_pca_train, X_pca_val, y_pca_val, \"pca\"]]:\n    print(f\"Training Adaboost classifier on balanced '{dataset}' dataset\")\n    ABC_CV.fit(X_train, y_train)\n    print(f\"Best parameters: {ABC_CV.best_params_}\")\n    print(f\"Corresponding f1 score: {ABC_CV.score(X_val, y_val)}\")\n    y_pred = ABC_CV.best_estimator_.predict(X_val)\n    print(f\"With confusion matrix:\")\n    print(confusion_matrix(y_val, y_pred))    \n    print(F\"R2: score: {r2_score(y_val, y_pred)}\")\n    \n    importances = ABC_CV.best_estimator_.feature_importances_\n    std = np.std([tree.feature_importances_ for tree in ABC_CV.best_estimator_.estimators_], axis=0)\n\n    forest_importances = pd.Series(importances, index=X_train.columns).sort_values().T\n\n    forest_importances.plot.barh(yerr=std, ax=ax[x])\n    ax[x].set_title(f\"{dataset} feature importances using MDI\")\n    ax[x].set_ylabel(\"Mean decrease in impurity\")\n    fig.tight_layout()\n    ax[x].set(xlim=(0,0.15))\n    x += 1","126a0016":"clf = make_pipeline(StandardScaler(), SVC(random_state=0))\nparams = [{\"svc__kernel\": ['linear', 'poly', 'rbf', 'sigmoid'], \"svc__C\": [0.5, 1., 2, 5., 10.]}]\nSVM_CV = GridSearchCV(clf,\n                      param_grid=params,\n                      scoring='f1',\n                      cv=10)\n\nfor X_train, y_train, X_val, y_val, dataset in [[X_kmeans_train, y_kmeans_train, X_kmeans_val, y_kmeans_val,\"kmeans\"], [X_pca_train, y_pca_train, X_pca_val, y_pca_val, \"pca\"]]:\n    print(f\"Training SVM classifier on balanced '{dataset}' dataset\")\n    SVM_CV.fit(X_train, y_train)\n    print(f\"Best parameters: {SVM_CV.best_params_}\")\n    print(f\"Corresponding f1 score: {SVM_CV.score(X_val, y_val)}\")\n    y_pred = SVM_CV.best_estimator_.predict(X_val)\n    print(f\"With confusion matrix:\")\n    print(confusion_matrix(y_val, y_pred))    \n    print(F\"R2: score: {r2_score(y_val, y_pred)}\")","395853ce":"clf = LogisticRegression(solver='liblinear', random_state=0)\nparams = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], \"penalty\": ['l2', 'l1'] }\nLR_CV = GridSearchCV(clf,\n                      param_grid=params,\n                      scoring='f1',\n                      cv=10)\n\nfor X_train, y_train, X_val, y_val, dataset in [[X_kmeans_train, y_kmeans_train, X_kmeans_val, y_kmeans_val,\"kmeans\"], [X_pca_train, y_pca_train, X_pca_val, y_pca_val, \"pca\"]]:\n    print(f\"Training Logistic Regression classifier on balanced '{dataset}' dataset\")\n    LR_CV.fit(X_train, y_train)\n    print(f\"Best parameters: {LR_CV.best_params_}\")\n    print(f\"Corresponding f1 score: {LR_CV.score(X_val, y_val)}\")\n    y_pred = LR_CV.best_estimator_.predict(X_val)\n    print(f\"With confusion matrix:\")\n    print(confusion_matrix(y_val, y_pred))    \n    print(F\"R2: score: {r2_score(y_val, y_pred)}\")","43f4ebd5":"from sklearn.naive_bayes import GaussianNB\nGaus = GaussianNB()\nfor X_train, y_train, X_val, y_val, dataset in [[X_kmeans_train, y_kmeans_train, X_kmeans_val, y_kmeans_val,\"kmeans\"], [X_pca_train, y_pca_train, X_pca_val, y_pca_val, \"pca\"]]:\n    print(f\"Training Naive Bayes classifier on balanced '{dataset}' dataset\")\n    Gaus.fit(X_train, y_train)\n    print(f\"Corresponding f1 score: {Gaus.score(X_val, y_val)}\")\n    y_pred = Gaus.predict(X_val)\n    print(f\"With confusion matrix:\")\n    print(confusion_matrix(y_val, y_pred))    \n    print(F\"R2: score: {r2_score(y_val, y_pred)}\")","d6c04004":"from sklearn.neighbors import KNeighborsClassifier\n\nclf = make_pipeline(StandardScaler(), KNeighborsClassifier())\nparams = [{\"kneighborsclassifier__n_neighbors\": [2, 3, 4, 5 ,6], \"kneighborsclassifier__weights\": [\"uniform\", \"distance\"], \"kneighborsclassifier__leaf_size\": [10, 20, 30, 40, 50]}]\nKNN_CV = GridSearchCV(clf,\n                      param_grid=params,\n                      scoring='f1',\n                      cv=10)\n\nfor X_train, y_train, X_val, y_val, dataset in [[X_kmeans_train, y_kmeans_train, X_kmeans_val, y_kmeans_val,\"kmeans\"], [X_pca_train, y_pca_train, X_pca_val, y_pca_val, \"pca\"]]:\n    print(f\"Training KNN classifier on balanced '{dataset}' dataset\")\n    KNN_CV.fit(X_train, y_train)\n    print(f\"Best parameters: {KNN_CV.best_params_}\")\n    print(f\"Corresponding f1 score: {KNN_CV.score(X_val, y_val)}\")\n    y_pred = KNN_CV.best_estimator_.predict(X_val)\n    print(f\"With confusion matrix:\")\n    print(confusion_matrix(y_val, y_pred))    \n    print(F\"R2: score: {r2_score(y_val, y_pred)}\")","8d6505e5":"print(f\"Corresponding f1 score: {RFC_CV.score(X_pca_test, y_pca_test)}\")\ny_pred = RFC_CV.best_estimator_.predict(X_pca_test)\nprint(f\"With confusion matrix:\")\nprint(confusion_matrix(y_pca_test, y_pred))    \nprint(F\"R2: score: {r2_score(y_pca_test, y_pred)}\")","3d0effe4":"# Job satisfaction ","8ffecd49":"Transformed data example:","571bba88":"Additionally I winsorize the *Total Satisfaction Multiplication* feature with 3% upper limit","50b03134":"It is amazing to see, that amongst top features, there are both Total Satisfaction augmented features!","00752d12":"Looking at the linear correlation, it is clear that, at least for some *Work-Place* features, the assignment is correct, as some of the features are linearly correlated.  \nThese features are also correlated with *Age* and *Monthly Income* and *Job Level*. Another pair to keep an eye on is *Percent Salary Hike* and *Performance Rating* .  \nAt the end of next part I am gong to check, whether the correlation is still present in the dataset.","aa4a800c":"Continuous features are: *Years At Company*, *Years In Current Role*, *Years Since Last Promotion*, *Years With Current Manager*, *Total Working Years*.   \n\nData look as follows:","8fd0e698":"And logically so. Only with increasing age the employee is able to work for a long time, work for his current company for a long time, work with the same manager, or wait for a promotion a couple of years.","6fa6ca04":"From *Demographics* group i choose *Age*, *Education Field* and *Marital Status* features, as they affect the attrition the biggest.\nFuture Note: Both *Education Field* and *Marital Status* can be factorized both naively and cleverly with sorted ratios.","3c565600":"Other than that, the dataset is skewed, as the target variable is not uniformly distributed.","3c9790fe":"# Exploratory Data Analysis and Feature Engineering","7384da9b":"## Ada Boost Classifier","971dc40b":"## Income","a92a5c14":"Now I find the ideal number of clusters using the elbow method and Hartigan criterion for extra robustness.  \nIt is not clear as to which no of cluster is ideal, either 4 or 6. I choose 4.","0b0ec349":"And if, in addition to work-place related features I add *Age* and *Job Level*, it can be seen, how much all of these features are correlated.","97fb9977":"Let's take a look how different features affect employees attrition rates.","d34633c4":"Once again I have to split datasets into train and test splits.","b9879434":"Alright, in the first part of this notebook, I have done some quick analysis of dataset as a whole and found out that the dataset has 1470 records with 35 features.  \nHowever, as the target variable is not uniformly distributed I will have to do some countermeasures as reweighting or sampling later on.   \nThe dataset also contains 4 features that are of no use for us.  \nFurthermore I divided features into groups of somewhat connected meaning. These groups will be analysed in detail. I am going to find out whether they affect each other, how they affect *Attrition* a look for other interesting details.  \nLast but not least I will have to deal with correlated features.","ab6d317b":"As all of significant features are connected, and similar in meaning, I am going to create a new artificial feature *Total Satisfaction* combining them together.  \nI hope that it will create more significant feature, and also reduce dimensionality of dataset.  ","e97e3f55":"### PCA Dimensionality Reduction","051d4339":"And visualize their distribution in order to display whether features' values affect how employees are prone to attrition ","99431f66":"Moreover, The *Monthly Rate*, *Daily Rate* and *Hourly Rate* features (their mins and maxes) do not significantly differ even when divided between different job levels.","2c3e0f6c":"Some k-means assumptions:  \n#1 Spherical data  \n#2 Evenly sized clusters  \n#3 Same variance for axes (and ideally also features)","aa95ba14":"In this notebook I analysed HR Attrition dataset.  \nIn first part I examined dataset in general and found out that it is imbalanced in terms of target variable, and divided features into 5 related groups.  \nAfter that I did extensive exploratory data analysis and chose features that I used as input for models based on visualizations and z-tests. I also created some artificially generated features, did some dimensionality reduction with PCA and clustering with K-Means.  \nAt last I showed, that imbalanced dataset seriously damages models' performance. Then, I resampled dataset with SMOTE algorithm and executed multiple algorithms with various performance reaching up to 0.9 f1-score.  \n\nIn the future, I would like to improve the dataset even more, for example by further reducing dimensionality.  \n\nI will be glad for any comments and tips,  \nThank you for your attention!  ","edb89ee2":"First I standardize correlated features","265728f4":"Lets execute Random Forest Classifier with new balanced Datasets.  \nThe performance measured by f1 score increased around 0.4 and 0.4 to almost 0.9.  \nImpresive result.    \nIt is also amazing that we over doubled the performance only by changing the ratio of target class samples.  \n\nR2 score is only about 0.6.\nHowever, each 1% of f1 score increases R2 score by approx 5%. So it is increasingly hard to improve.\n\nBelow are also added feature importance plots based on impurity.  \nIt is interesting to see, that even that K means is not supported by assuptions, it is also similar in performance as PCA.","857f4aee":"Using PCA i got rid of correlations, and kept 96.4 % of former variance.","30398543":"The description of Numerical Categorical data shows that they could have a real impact on Attrition as they are not simmilar for different *Attrition* values.","a913331c":"In order to boost performance, I am going to introduce new artificially generated samples for minority class using SMOTE algorithm.  \nMore on SMOTE on [Jason Brownlee's blog post](https:\/\/machinelearningmastery.com\/smote-oversampling-for-imbalanced-classification\/).  ","1b60e06f":"Resulting dataset looks as follows:","64c4110f":"As previously, I again use z-test to determine, whether the mean for both attritioned and non attritioned employees is similar for new features.\nAs expected the difference is statistically significant.","bb68ee4f":"## Support Vector Machine","be1db380":"There are also 8 features connected with income or rate.  \n4 of them are continous and 4 remaining are categorical.  ","2b90c188":"Even when Adaboost performs only 0.03 worse than Random Forest Classifier, it describes only one half of variance that is described by Random Forest Classifier!","66f0ea4d":"This way i got rid of feature colinearities.  \nHowever, along the way I lost the interpretability of the features. (New base of axes is known, however not easy to interpret)  \nI create new engineered data set and assign new features to it.","41d67f78":"# Conclusion","68861c09":"Over Time is a massive deal for attritioned employees!  \n\nEven that the mean of *Number of companies worked For* is similar for both *Attrition* values, it also shows that it is also good for dividing between attrition-like workers as the older ones prefer to stay at their current job.  \n\nBusiness Travel clearly shows that employees that need to travel in business matters are twice as likely to attrition.","397cc0f0":"The winning Random Forest Classifier resulted as well almost 0.9 f1 score on test dataset.","bba6aa71":"# Introduction To Data Set","601cfb61":"In this section I discovered that Monthly, Daily and Hourly rates are not easily interpretable and seems to not affect the *Attrition*.  \nAs the *Monthly Income* feature is the only one that did I added it to engineered dataset.  ","703e989d":"### Continuous Income Features","a6577c8e":"And apply PCA so that creates 4 new features, it lowers the dimensionality and should keelp over 90 % of information.","8f85605a":"I execute the K-Means algorithm with n_clusters = 4 configuration and display all cluster centers.  \nAccording to cluster center, the representative Job Seniorities can be described probably as:  \n1) *Junior* - the youngest employees  \n2) *Devoted Senior* - Highest total working years, 90% of their work experience worked with current company, long time since last promotion  \n3) *Senior* - High total working years, but mean time in current company is only 4 years, promoted couple years ago  \n4) *Medior* - Medium time in current company, loyal  ","ac1b4cd9":"4 continuous features are called *Monthly Rate*, *Daily Rate*, *Hourly Rate* and *Monthly Income*.  \n\nThese features do not have any easily recognizable dependencies.","c7e99bdc":"This is the end of the explanatory data analysis.   \n\nIn this part I removed features from the *Others* group that simply did not created any value.  \n\nThen I kept *Monthly Income* only as it was the only feature that increased with increasing Job level.\n  \nAlso the categorical features from *Income* category proved significant.  \n\nFrom group called *Job Satisfaction* I created new artificial feature *Total Satisfaction* that is combined from other 5 features and I believe it to be more significant and also to reduce dimensionality.  \n\nFrom *Demographics* group i choose *Age*, *Education Field* and *Marital Status* features, as they affect the attrition the biggest.\n\nAll categorical Work-Place related features affect the attrition rates. \n\nContinoues Work-Place related features were strongly correlated.  \nI employed two solutions:  \n1) dim. reduction and orthogonalization using PCA  \n2) segmentation using K-Means  \n\nFrom which I created two data-sets.\n\nThese data sets are yet to be encoded into numerical values.\n\n**K Means Data Set**","bdab6c04":"According to z-test from above, the partition of *Stock Option Level* and *Job Level* features does have an impact on *Attrition*.\nMoreover, with clever factoring, even *Job Role* and *Department* will mater.","c2a92f18":"Next, for each feature, I use z-test to determine whether the mean of attritioned and non attritioned employees' {job satisfaction feature} is similar meaning that the partition according to *Attrition* is not significant or it is simmilar.  \nFormally:  \nHnull: There is no significant difference between {job satisfaction feature} of dissatisfied and contented employees  \nHa: The {job satisfaction feature] do in fact differ.  ","0272f1cd":"In general, there are markdown cells commenting code \/ visualizations \/ hypotheses \/ etc. that are executed in cells after these markdown cells. I believe it improves the readability of the notebook in such a way that it is more clear what my intentions are.\n\nIn the notebook I am also carying out both EDA and feature engineering in one place. As a result, my notebook does not seem scattered, and flows naturally as I discover new features.\n\nIn some continuous features I use winsorizing in to reduce the effect of outliers.","d30b3fc4":"Lets execute SVM classifier and couple of other hyperplane classifiers","98171538":"Description of numerical features *Training Times Last Year* and *Num Companies Worked For* looks like following:","33220ebd":"All **significant** features have value ranges from 1 to 4 including both (higher the value, better the result).  \n*Distance from Home* does not. Moreover, higher the value, the more distant are journeys of that employee, this feature is reversed.  \nLet's change its values into range from 1 to 4, such that 4 means that it is as near as possible.","ddc0086d":"**PCA Data Set**","7b768d3c":"Let's group the data according to *Attrition*","781da347":"### Engineering out colinearities","50f72763":"# Work-Place ","a4b51a11":"As former features were correlated with *Job Level* etc. it only makes sense that even other features as *Monthly Income* increases with increasing *Job Seniority*","b3637a0f":"Only *Monthly Income* increases with increasing Job level.","7ae5a1a5":"However, it contains some features, that are the same for all records (*Employee Count* and *Over18* and *Standard Hours*) or features that are unique for each record (*Employee Number*).","3ed39b12":"Now I encode both pca and kmeans dataset","9e68ec71":"And assign them as a new columns in a *job_satisfaction_data* ","4360b35a":"As there are still categorical non-numeric values in datasets I am going to one hot encode them.\n\nFuture Note: Instead of one hot encoding, some variables should\/could be encoded as ordered numerical categorical feature","6dd00cfb":"Furthermore, matrix of pearson correlation shows, that continuous work-place related features are heavily corelated.  \nThese features are correlated with *Age* and *Job Level* features as well.","59aeba49":"I am going to save pca and kmeans dataset into X_pca, y_pca and X_kmeans and y_kmeans respectively","b2e80aec":"*Other* feature group seems to be useless for our cause as the *Emloyee Number* is unique for every record and *Employee Count*, *Over 18*, *Standard Hours* are the same for all records.","983f998a":"Visualizing the ratio of attritioned and non attritioned between single values of *Age* feature shows that younger employees tend to leave their workplace more often as they for example seek for better benefits. More older the employee is, the less prone he is to leave his job, as he is more near to his \/ her retirement.","2c5f7c4a":"# Demographics","198228d0":"From group called *Job Satisfaction* I chose two new artificially generated features *Total Satisfaction Addition* and *Total Satisfaction Multiplication*. They are combined from other 5 features and I believe it to be more significant and also to reduce dimensionality.  \n*Relationship Satisfaction*, *Performance Rating* and *Percent Salary Hike* features were similar for both attritioned and non-attritioned employees and I won't use them in final model.","43ab2b2a":"And finally, include all significant features in artificialy engineered feature *Total Satisfaction*.  \nOne feature, that sums app, other one that multiplies values of all significant features.  ","7f536d1b":"Categorical features are: *Business Travel*, *Over Time*, *Training Times Last Year*, *Number of Companies Worked For*.\n\nData looks as follows:","f5c97006":"Features related to demographics are: *Age*, *Education*, *Ed. Field*, *Gender*, *Marital Status*.  \n\nThis is how they look like:","5ff06954":"As these features are useless I am going to remove them.","8e9b3340":"First I define one hot encoder and columns that are to be encoded","73524590":"Other than *Performance Rating* and *Percent Salary Hike*, the features are not correlated.  \nWe have to watch out for those two, otherwise we create colinearity in our model.","5cacf53b":"### K-Means Factorization","3cb5e2a9":"At first, I set up libraries, load data, and create *engineered_data* dataset containing only target value.","a35125b5":"To balance dataset I first over sample attritioned employees to half of non-attritioned employees.  \nAfter that I under sample non-attritioned employees so that both groups have equal amount of records","6802c88c":"Principal axes in feature space, representing the directions of maximum variance in the data, are following.","6d191554":"## Logistic Regression classifier","7015f3c9":"4 categorical features are called *Stock Option Level*, *Job Role*, *Job Level*, *Department*.  \n\nData look like this:","b3b4b818":"The dataset contains 1470 records with 35 features each. Features contain no null or otherwise \"dirty\" data.","819fbdce":"Nice, it is time for the first hypotheses!  \nFor hypotheses testing I use z-test in majority of cases.  \nPaired z-test assumes that we know the std of both populations, and the std's are also different.  \n\nLet's test it out!  \nFormally:  \nHnull: There is no significant difference between *Stock Option Level* or *JobLevel* (test each separatelly) of dissatisfied and contented employees  \nHa: The *Stock Option Level* or *JobLevel*  do in fact differ.  ","1e668450":"### Naive imbalanced dataset Random Forest Classifier model","37449760":"I am going to winsorize the feature.  \nIt can be seen that majority of outliers have been capped.\nImplementatio note: Throughout the notebook, I have decided that 8% upper limit is fine for this dataset.","7b0f825a":"All categorical Work-Place related features affect the attrition rates. I am going to use them in my model.","ff19c8a7":"And by modeling these data with Random Forrest Classifier we only get 0.4 f1-score.  \nBad performance is a result of imbalanced dataset. Classifier prioritizes non-attritioned employees as can be seen in confusion matrix:","e7b7bc12":"At the first glance, the features (without target variable \"Attrition\") can be divided into few categories:  \n**1) Income related**  \n      continuous features - Daily Rate, Hourly Rate, Monthly Income, Monthly Rate  \n      categorical features - Stock Option Level, Job Level, Job Role, Department  \n **2) Job Satisfaction** - Job Involvement, Environment Satisfaction, Job Satisfaction, Work Life Balance, RelationShip Satisfaction, Performance Rating, Distance From Home, Percent Salary Hike    \n **3) Demographics** - Age, Education, Ed. Field, Gender, Marital Status  \n **4) Work-Place** - Years At Company, Years In Current Role, Years Since Last Promotion, Years With Current Manager, Business Travel, Num Companies Worked, Over Time, Total Working Years, Training Times Last Year  \n and specific category with already redundant variables  \n **5) Others** - Emloyee Number, Employee Count, Over 18, Standard Hours  ","d291be95":"I assign new column to engineered data and replace with new factored values","93e16358":"In this part I showed that balanced dataset is alpha and omega of well delivered data science task.  \nI compared imbalanced dataset with balanced ones on both datasets containing pca reduced data in former case and k-means clustering in the latter one.  \nI executed Random Forest, AdaBoost, SVM, Logistic Regression and Naive Bayes classifiers of which the Random Forrest Classifier performed the best on validation dataset with almost 0.9 f1 score.  ","16c3cef2":"Lets try out another ensemble interpretable classifier - AdaBoost.","21320ee2":"## K Neighbors Classifier","ecd8449b":"Now I am going to split both datasets into train, validation and test datasets with ratio 3:1:1.  \nAlso I used stratification to keep same target ratio in both train and test datasets.","64fb413e":"Once again I winsorize top values","5a923197":"Monthly Income has lots of outliers, majority of them in non Attritioned group.","69f13167":"Using multiple execution of K-Means Algorithm I compute the error and Hartigan criterion for 1 to 10 clusters.  \nImplementation note: I work with transformed to mean = 0, std=1","6e321f43":"## Random Forest Classifier","411a420d":"Possible improvements:  \n    - Detect dataset-wise outliers using K-Means \/ DBSCAN \n    - Manifold learning (? Is there any way to say whether manifold is linear for example?)  \n    - Does 0\/1, -1\/1 classificator encoding make any difference?  \n    - I may use Maximal information Criterion with \/ instead of pearson's correlation.  \n    \nImplemented Improvements\n    - Check how many samples are there in K-Means encoding classes - There are 745, 406, 171 and 148 labels assigned to each cluter. These assignment, even that skewed, clearly show the distribution of job seniorities in company.  \n    - Use Hartigan Criterion as well for K-Means - Hartigan criterion confirms elbow' method result  \n    - Check K-means and PCA assumptions  - PCA is OK, K-Means is probably not\n    - Try out using Multiplicative combination of satisfaction instead of addition - Works best when using both\n    - Detect feature-wise outliers - Done\n    - Create Bayes (Does not mind high dimensionality) and kNN Model (I am interested in its performance), LDA  \n    - Try out t-SME dim. reduction for classification. - Does not work\n    - Compare models with (adjusted) R^2 as well.  \n    \nUpdates:  \n    v3 - Fixed *Distance From Home* transformation formula, more explanatory comments\n       - Added Hartigan Criterion for K-Means\n       - Fixed Additive Total Satisfaction and added Multiplicative one\n       - Added other models as SVM or KNN\n    \nWrong assumptions:  \n    - Use t-test instead of z-test - I correctly used z-test as it works with known stds. T-test assumes similar std of populations, which is not correct in this use-case.  ","b9ba43f0":"Comparison of new *Total Satisfaction* column displayed using boxplot looks as follows:","3829bd03":"## Naive Bayes","89e8e798":"Amongst job satisfaction features I use *Job Involvement*, *Environment Satisfaction*, *Job Satisfaction*, *Work Life Balance*, *RelationShip Satisfaction*, *Performance Rating*, *Distance From Home*, *Percent Salary Hike*    ","8d3d04d3":"The fact, that there is not uniformly distributed target variable means that without proper care, the model will prioritize non-attritioned records and won't perform well.  \nWhen constructing models, I am going to create a new dataset with uniformly represented *Attrition* and compare its results with non-uniform one.","77396899":"Lets execute Logistic Regression classifier.\nIt reaches 0.75 for both datasets which is significantly lower than previous classifiers.","854001e1":"### Categorical Income Features","53553b1e":"Welcome to the notebook dealing with the IBM HR analytics attrition & performance dataset.\n\nIBM HR analytics attrition & performance is a fictional data set created by IBM data scientists. It deals with factors that may or may not lead to an employee leaving his\/her job.\n\nIn this notebook I am going to explore this dataset, clean and improve features, pronounce and test some hypothesis, create some regression or\/and classification models and at the end I am going to discuss the results.\n\nIn the end I would like to compare models for raw data with minimal changes, engineered data and engineered data with reweighting and\/or other techniques.","9a0fc5b4":"As continoues features related to Work-Place are linearly correlated I am going to:  \na) transform features to not be linearly correlated with PCA - this way, however, I lost interpretability  \nb) factorize records with k-means clustering into *Job Seniority* feature  \nIn the end I am going to compare both solutions.  ","20ae33a1":"Statistics for attritioned and non attritioned workers differ only in following features: *JobInvolvement*, *EnvironmentSatisfaction*, *JobSatisfaction*, *WorkLifeBalance* and *DistanceFromHome*. As they do not include *Performance Rating* and *Percentage Salary Hike*, we no longer need to care for colinearity problem\n\nThese features are significant for attrition.","b40c0859":"Other than the third assumption, I can not say clearly whether I they are met or not (probably they are not).","9f4f7f85":"Before visualizing other demographic features, I am going to encode Education with years that took studying each education level as range 1 to 5 is strongly simplified.  \nAs we see here, there are five Education level.  \nFrom high School to PhD (HighSchool=10 years, College=2 years, Bachelor=4 years,Master=2 years,PhD= four years).  \nAddition of those will be our new values.  ","36de2457":"Visualizing other features: *Education*, *Ed. Field*, *Gender*, *Marital Status*, we can see that the *Education* and *Gender* features affect the attrition in only a small percentage of records. On the other hand, *Education Field* - employees from HR, Marketing or Technical Fields are more prone to attrition, as well as *Marital Status* Singles.\n\n(Following visualizations do not take into account that there is approx. 80 % of non attritioned records.)","987d8bfb":"With general analysis on dataset as a whole completed, lets take a look at the data itself.","6bc15dea":"Correlation in new space are zero.","16ca2b4f":"In this part I am going to come up with some models and compare them.  \nFirst, I am going to show, how unbalanced dataset skews the model decision making. Then I am going to balance dataset using SMOTE algorithm and present result on a few ML algorithms.","1bcceff6":"In this part I will go through feature groups, analyze them and change them accordingly, if needed.  \nIn some cases I will use hypothesis testing to find out whether there is significant difference between attritioned workers and non-attritioned ones.  \nI expect to use other methods as reweighting, sampling, dim. reduction or clustering where needed.","11031a1a":"# Setting up models","b195e1ec":"### Balancing datasets using SMOTE algorithm","7498288c":"PCA assumptions:\n\n#1: You have multiple variables that should be measured at the continuous level.  \n#2: There needs to be a linear relationship between all variables.  \n#3: You should have sampling adequacy, which simply means that for PCA to produce a reliable result, large enough sample sizes are required (150+ samples).  \n#4: Your data should be suitable for data reduction. Effectively, you need to have adequate correlations between the variables.  \n#5: There should be no significant outliers.  ","0253d3d5":"According to z-test the *Training Times Last Year* features does affect *Attrition* and *Num Companies Worker For* does not.","65c238ac":"## Categorical Work-Place Features","3bae9200":"## Continuous Work-Place Features","bb470fc0":"I have met all assumptions.","bc24433a":"The fact that they are not linearly correlated is confirmed by pearson's correlation matrix as well","6997e636":"## Others","a98c20cb":"In Work-Place Related features I consider 5 continuous features and 4 categorical features.  \n","6676730a":"Let's add *Monthly Income* into a engineered dataset.","37b4bffe":"Ass all categorical income features may have impact on *Attrition* I am adding them to engineered dataset.","36233be1":"In Income group, I have identified that only one continuous feature *Monthly Income* out of four features differs with *Job Level* feature and affects *Attrition* .   \nAmongst categorical variables, I have tested that both *Stock Option Level*, *Job Level* have statistically significant difference in their means, and as such have their partition has impact on Attrition.   \nThe visualizations of *Job Role* and *Department* show that their partition makes sense as well.  \nFuture Notes: Both Job Role and Department features can be factorized \"randomly\" or in such a way, that the (non-)attritioned ratio is sorted - I believe such encoding can further improve the models' performance.  ","db1d8c18":"In my opinion, happy employees are less prone to attrition.  \nFirst, I examine the description of job satisfaction features."}}