{"cell_type":{"e6a32f4a":"code","3bf4d9b9":"code","e86c91e1":"code","bace43a4":"code","53b4141d":"code","44693c89":"code","734e66ad":"code","7f9630bb":"code","8d5e66a9":"code","9faf7088":"code","45a6be75":"code","f11a92b2":"code","f174b5a5":"code","87660229":"markdown","8b61a4f1":"markdown","45f000f7":"markdown","ab126f01":"markdown","2bf02713":"markdown","e1a4613b":"markdown","c000ece0":"markdown","84f1bf0f":"markdown","cae814c4":"markdown","4935b1ec":"markdown","114de875":"markdown","7c7b9a66":"markdown","529effb8":"markdown","64bee2a3":"markdown"},"source":{"e6a32f4a":"import numpy as np \nimport pandas as pd \nimport json\nimport re","3bf4d9b9":"pd_train = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/train.csv')\npd_test = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/test.csv')","e86c91e1":"def find_all(input_str, search_str):\n    l1 = []   \n    length = len(input_str)\n    index = 0\n    while index < length:\n        i = input_str.find(search_str, index)\n        if i == -1:\n            return l1\n        l1.append(i)\n        index = i + 1\n    return l1","bace43a4":"def remove_html(text):\n    text = re.sub(\"&quot;\", '\"', text)\n    text = re.sub(\"&gt;\", \">\", text)\n    text = re.sub(\"&lt;\", \"<\", text)\n    text = re.sub(\"&le;\", \"\u2264\", text)\n    text = re.sub(\"&ge;\", \"\u2265\", text)\n    text = re.sub(\"&amp;\", \"&\", text)\n    return text\n\ndef add_html(text):\n    text = re.sub(\"&\", \"&amp;\", text)   \n    text = re.sub('\"', \"&quot;\",  text)\n    text = re.sub(\">\", \"&gt;\", text)\n    text = re.sub(\"<\", \"&lt;\", text)\n    text = re.sub(\"\u2264\", \"&le;\", text)\n    text = re.sub(\"\u2265\", \"&ge;\", text)\n    return text","53b4141d":"def f(selected):\n     return \" \".join(set(selected.lower().split()))","44693c89":"!mkdir data\n!mkdir results_roberta_large","734e66ad":"output = {}\noutput['version'] = 'v1.0'\noutput['data'] = []\ntrain = np.array(pd_train)\n\nconverted = 0\nfor line in train:\n    paragraphs = []\n    context = line[1]\n    qas = []\n    qid = line[0]\n    answers = []\n    orig_answer = line[2]\n    question = line[3]\n    if type(orig_answer) != str or type(context) != str or type(question) != str:\n        print(context, type(context))\n        print(orig_answer, type(orig_answer))\n        print(question, type(question))\n        continue\n    \n    # get start index and then covert html-tag\n    answer_starts = find_all(context, orig_answer)\n    context = remove_html(context)\n\n    for answer_start in answer_starts:\n        # get new answer, if there are no html-tags answer will be the same as given in train.csv\n        answer = context[answer_start:answer_start+len(orig_answer)]\n        answers.append({'answer_start': answer_start, 'text': answer})\n        if orig_answer != answer:\n#             print(\"original:\", orig_answer)\n#             print(\"new:\", answer)\n            converted += 1\n    qas.append({'question': question, 'id': qid, 'is_impossible': False, 'answers': answers})\n    \n    paragraphs.append({'context': context, 'qas': qas})\n    output['data'].append({'title': 'None', 'paragraphs': paragraphs})\n\nwith open('data\/train.json', 'w') as outfile:\n    json.dump(output, outfile)\n    \nprint(\"converted:\" , converted)","7f9630bb":"# Convert pd_test data\n\noutput = {}\noutput['version'] = 'v1.0'\noutput['data'] = []\n\n# covert html-tags\npd_test.text = pd_test.text.map(remove_html)\ntest_array = np.array(pd_test)\n\nfor line in test_array:\n    paragraphs = []\n    context = line[1]\n    qas = []\n    question = line[-1]\n    qid = line[0]\n \n    if type(context) != str or type(question) != str:\n        print(context, type(context))\n        print(answer, type(answer))\n        print(question, type(question))\n        continue\n\n    answers = []\n    answers.append({'answer_start': 1000000, 'text': '__None__'})\n    qas.append({'question': question, 'id': qid, 'is_impossible': False, 'answers': answers})\n    \n    paragraphs.append({'context': context, 'qas': qas})\n    output['data'].append({'title': 'None', 'paragraphs': paragraphs})\n\nwith open('data\/test.json', 'w') as outfile:\n    json.dump(output, outfile)","8d5e66a9":"!cd \/kaggle\/input\/pytorchtransformers\/transformers-2.5.1; pip install .","9faf7088":"!python \/kaggle\/input\/pytorchtransformers\/transformers-2.5.1\/examples\/run_squad.py \\\n--model_type roberta \\\n--model_name_or_path \/kaggle\/input\/roberta-large-model\/results_roberta_large\/ \\\n--do_lower_case \\\n--do_eval \\\n--data_dir .\/data \\\n--cache_dir \/kaggle\/input\/cached-roberta-large-pretrained\/cache \\\n--train_file train.json \\\n--predict_file test.json \\\n--learning_rate 2.5e-5 \\\n--num_train_epochs 3 \\\n--max_seq_length 192 \\\n--doc_stride 64 \\\n--output_dir results_roberta_large \\\n--per_gpu_eval_batch_size=16 \\\n--per_gpu_train_batch_size=16 \\\n--save_steps=100000","45a6be75":"predictions = json.load(open('results_roberta_large\/predictions_.json', 'r'))\n\nfor i in range(len(pd_test)):\n    id_ = pd_test['textID'][i]\n    selected_text = predictions[id_]\n    text = pd_test['text'][i]\n    text = \" \".join(text.split())\n    starts = find_all(text, selected_text)\n    \n    # if none of (&><\u2264\u2265\") exist before the end of the answer nothing will change by adding html-tags \n    # if there is more than one answer in the context we cannot know which to modify\n    if len(starts) == 1 and any(c in text[:starts[0]+len(selected_text)] for c in list('&><\u2264\u2265\"')):\n        text = add_html(text)\n        start = starts[0]\n#         print(\"original:\", selected_text)\n        selected_text = text[start:start+len(selected_text)]\n#         print(\"new:\", selected_text)\n    \n    if pd_test['sentiment'][i] == 'neutral': # neutral postprocessing\n        pd_test.loc[i, 'selected_text'] = pd_test['text'][i]\n    else:\n        pd_test.loc[i, 'selected_text'] = selected_text\n\npd_test.selected_text = pd_test.selected_text.map(f)","f11a92b2":"pd_test.head()","f174b5a5":"# Save the submission file.\npd_test[[\"textID\", \"selected_text\"]].to_csv(\"submission.csv\", index=False)","87660229":"## Data Preprocessing","8b61a4f1":"## Modify answers with html-tags and submit","45f000f7":"## Import Packages","ab126f01":"#### Make output dirs","2bf02713":"### Convert training data to json, remove html-tags and fix answers","e1a4613b":"### @aerdem4 trick for boosting jaccard","c000ece0":"If you find this kernel helpful, Please check and upvote the original notebook by @cheongwoongkang and the fork by @raghaw. \nhttps:\/\/www.kaggle.com\/cheongwoongkang\/roberta-baseline-starter-simple-postprocessing  \nhttps:\/\/www.kaggle.com\/raghaw\/roberta-baseline-starter-test?scriptVersionId=31358810","84f1bf0f":"### Preprocessing\nI formulate this task as an extractive question answering problem, such as SQuAD.  \nGiven a question and context, the model is trained to find the answer spans in the context.\n\nTherefore, I use sentiment as question, text as context, selected_text as answer.\n- Question: sentiment\n- Context: text\n- Answer: selected_text\n","cae814c4":"### train and\/or evaluate ","4935b1ec":"This script is meant to display the \"tricks\" you can use in this competition, these are due to the labeling errors in the data as well as the evaluation implementation of the metric.\n\nNotebook explaining the labeling errors:\nhttps:\/\/www.kaggle.com\/dhananjay3\/investigating-html\n\nPost Processing to increase jaccard, credit to @aerdem4 for discovering it\nhttps:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction\/discussion\/140942\n","114de875":"## Finetuning RoBERTa","7c7b9a66":"### Load Data","529effb8":"### Removing and adding html tags","64bee2a3":"Install the pytorch-transformers package (v2.5.1) of [huggingface](https:\/\/github.com\/huggingface\/transformers\/tree\/v2.5.1)."}}