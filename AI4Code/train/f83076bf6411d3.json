{"cell_type":{"2eefa3f8":"code","278da906":"code","90a89c5a":"code","aeb7afdb":"code","f8c56a10":"code","4c99c6de":"code","277fcc40":"code","057bdcf8":"code","691e1f27":"code","5ee5c5a1":"code","9c78c61a":"code","84a68fa3":"code","2b151ab0":"code","a8160444":"code","d7560384":"code","1c086493":"code","c4199a36":"code","54f08206":"code","d16c7502":"code","6acfac61":"code","aae02ed5":"code","66b125c0":"code","ea5214be":"code","9652054c":"code","8bef6325":"code","80c073c6":"code","a867622c":"code","bfa40395":"code","6335cf32":"code","3f320bb5":"code","ba91b5fe":"code","cb557602":"code","9b499fc2":"code","1f45a580":"code","ba699ee5":"code","a9953d5d":"code","9e90bc3a":"code","e4e471a6":"code","98db1d85":"code","9ffccbc1":"code","bdc7acc7":"code","3e5c0bdc":"code","989ccd25":"code","28980b03":"code","85c25493":"code","90aeb28a":"code","34753b36":"code","06e08498":"code","354bf8f5":"code","ecb26af1":"code","5ad84693":"code","f7222966":"code","2c971160":"code","b0ecd75b":"code","57ebc49f":"code","9d7094c9":"code","aa11884d":"code","ecf28ad6":"code","6e28e71c":"code","e40e614b":"code","2de3bc0e":"code","50d1514f":"code","122a4f15":"code","ae6ddf82":"code","118837b5":"code","968be93b":"code","c362cbdc":"code","559538be":"code","45937c32":"code","c91d7332":"code","6adfa81c":"code","f9ad9f14":"code","a624b3dd":"code","99057854":"code","12d062a6":"code","740b3c8d":"code","a7645eea":"code","9dc9d310":"code","36ccca24":"code","e1cf6cb2":"code","1cb7f9dc":"code","d778811d":"code","3163a150":"code","3d14af32":"code","ed04f321":"code","560caae8":"code","9051c8eb":"code","2bb671f9":"code","81562fb2":"code","91c0675f":"code","bd5fd442":"code","15729433":"code","a1e95b86":"code","7dbe4710":"code","1524d630":"code","682afa64":"code","c9d42e5d":"code","3edb8c4f":"code","6587eada":"code","16f6de1e":"code","7fa540ad":"code","04bf7e98":"code","0ce0c15b":"code","63849cee":"code","fc2670d3":"code","7e7c396a":"code","6796f428":"code","251cb588":"code","bc953530":"code","2c5775ce":"code","2d1625df":"code","bf5f3582":"code","b34f5a2b":"code","fdf437f3":"code","1121e2e4":"code","11732aac":"code","261a2d96":"code","ff01ff47":"code","62f2386c":"code","1a2b0089":"code","09f37b6a":"code","7fdefadd":"code","74d6481d":"code","5d5e5f83":"code","d49da185":"code","8fe72e99":"code","59287124":"code","3119baaf":"code","bea8cf5d":"code","617af939":"code","852fb964":"code","aa71dc91":"code","60f1951a":"code","639a36eb":"code","77951527":"code","4b174213":"code","7efb32b1":"code","e633a932":"code","0f69a52c":"code","4c597f59":"code","63dbd46e":"code","d9be2e7c":"code","118c2a06":"code","8c9a3794":"code","38b8b416":"code","c1b1ae71":"code","a51aaa16":"code","1bf5f8d8":"code","2d0bdfac":"code","8e31a339":"code","85594682":"code","9fa14b74":"code","36a28835":"code","c3ab4dfe":"code","e71a6b85":"code","f5a24755":"code","45aeedc2":"code","9ccd7172":"code","ac050b1b":"code","81480ae6":"code","538c8772":"code","2e2610a9":"code","a96ed15b":"code","273f5f9a":"code","2c908f5d":"code","3665b00d":"markdown","aa1c681e":"markdown","fe1ec3d8":"markdown","97e7fa4a":"markdown","21a85471":"markdown","aa1163ea":"markdown","2b39a7d9":"markdown","7cfcb30d":"markdown","8514a1ce":"markdown","9f3c3c01":"markdown","f71874eb":"markdown","87fcb15b":"markdown","56b1ea71":"markdown","e9aeff29":"markdown","60175f58":"markdown","e9dc1586":"markdown","b54f4917":"markdown","6baaac9d":"markdown","cca44d0b":"markdown","711782b9":"markdown","adb58f16":"markdown","bc8ea199":"markdown","a21946f3":"markdown","aa99155d":"markdown","248c2ba5":"markdown","5812fe8f":"markdown","b374ffd2":"markdown","c6f5aa5b":"markdown","39eadbd9":"markdown","d0e157f6":"markdown","f8f48102":"markdown","601008f2":"markdown","fe487b95":"markdown","1b9999c0":"markdown","26a04b55":"markdown","d7de2dd8":"markdown","da6d9e7e":"markdown","7f0a4704":"markdown","6b17143d":"markdown","e93b90bb":"markdown","e3c7d740":"markdown","2b379521":"markdown","88518f4d":"markdown","03a751b0":"markdown","b1d243bf":"markdown","2530e786":"markdown","aaba9657":"markdown","985cdea9":"markdown","40b3344f":"markdown","7d0699f7":"markdown","fb5eea28":"markdown","54312128":"markdown","ab4ab32a":"markdown","25cf3984":"markdown","fa03e3d9":"markdown","2b962572":"markdown","eeee0960":"markdown","913f0faa":"markdown","0110aacc":"markdown","69647a49":"markdown","5004cda0":"markdown","4338439c":"markdown","aa9cf3b1":"markdown","cdcf09f3":"markdown","cfbc5a3f":"markdown","a2587ccb":"markdown","23d8d1d5":"markdown","138fcce8":"markdown","0a17c24f":"markdown","d865ed51":"markdown","a58db125":"markdown","50e29af5":"markdown","97e385a0":"markdown","1a2a0640":"markdown","01cabb84":"markdown","c68a0e50":"markdown","a443730c":"markdown","c4343c4e":"markdown","331c90a2":"markdown","839145cd":"markdown","a1b61cf3":"markdown","9b26de60":"markdown","724c6c0d":"markdown","24f57e25":"markdown","e99b01e1":"markdown","ef8ea2ef":"markdown","b153a3e1":"markdown","230074c5":"markdown","19e9f79d":"markdown","88a616e6":"markdown","51be3169":"markdown","28c17071":"markdown","9d77b3a8":"markdown","d3ff02f0":"markdown","897cec5f":"markdown","632d0c8a":"markdown","c3708714":"markdown","52ada293":"markdown","21ef6a59":"markdown","66b90b6a":"markdown","fc6e2030":"markdown","2dba8661":"markdown","3d6199df":"markdown","d5834f4f":"markdown","1f68c96f":"markdown","94ce18ae":"markdown","60ea2f94":"markdown","210e1aec":"markdown","3ca760f1":"markdown","78f4f8f4":"markdown","a4b09f6b":"markdown","517e1fdf":"markdown","a8efa042":"markdown","427d5f75":"markdown","9437cc21":"markdown","fca516d8":"markdown","35ce67bb":"markdown","254fb066":"markdown","1571ad6d":"markdown","89f9fc4d":"markdown","81692d89":"markdown","a567acbe":"markdown","f8b1fce2":"markdown","676730a2":"markdown","f13a9eea":"markdown","b4a86e82":"markdown","2896dd34":"markdown","12f917ea":"markdown","34e4c144":"markdown","4bf040a2":"markdown","c43b3027":"markdown","bafdbba1":"markdown","89fc2a07":"markdown","8274d651":"markdown","23f6d0c3":"markdown","db0acb76":"markdown","6d5a7c5c":"markdown","65c101b4":"markdown","53533b50":"markdown","a4586453":"markdown","4fa52446":"markdown","b11b0194":"markdown","beb0e70e":"markdown","b7dbd7e7":"markdown","555bccd0":"markdown","7454ae78":"markdown","839a1d26":"markdown","ca3f7679":"markdown","5cbf9de4":"markdown","3afa306c":"markdown","5f258918":"markdown","61e43e51":"markdown","da8acc87":"markdown","e6fa8459":"markdown","740a342a":"markdown","7a5adc7d":"markdown","f79808a5":"markdown","72125f2c":"markdown","1d5b06df":"markdown","d0ab9d39":"markdown","c7ecfbcb":"markdown","c1956411":"markdown","e3301cae":"markdown","b828772f":"markdown","b12bb963":"markdown","00e9bc90":"markdown","996762f7":"markdown","ad532028":"markdown","0804645f":"markdown","f70d1a46":"markdown","d9d20056":"markdown","4ddb17fc":"markdown","539a7257":"markdown","69beeb9b":"markdown","c09c681a":"markdown","8de02a77":"markdown","6560cdd0":"markdown","96c0f147":"markdown","08147954":"markdown","f9862926":"markdown","b81142d7":"markdown","f450eb79":"markdown","f2349d1d":"markdown","8e3c17ba":"markdown","46723ab8":"markdown","73ff1659":"markdown","d9c432fa":"markdown","a6f8c20a":"markdown","4bb26ce4":"markdown","bba69ab5":"markdown","a5c1e63a":"markdown","22609961":"markdown","abd9dff9":"markdown","4b71e029":"markdown","42464318":"markdown","4cbb49b2":"markdown","55b17ddd":"markdown","3c2e549f":"markdown","47eed479":"markdown","cf94053b":"markdown","43075cba":"markdown","dbb52732":"markdown","387b36e7":"markdown","6eeb8c52":"markdown","2487504d":"markdown","b460b3a0":"markdown","64c990b5":"markdown","cdb31c77":"markdown","df46d25d":"markdown","aba24cba":"markdown","77a08851":"markdown","e0dfbd55":"markdown","97b29f4c":"markdown","255439bc":"markdown","914b2a89":"markdown","4eae5c3d":"markdown","631c1084":"markdown","8aa8fa51":"markdown","0347ca34":"markdown","6a8444f4":"markdown","fed35c6f":"markdown","e9734b42":"markdown","b7e79926":"markdown","8206f26a":"markdown"},"source":{"2eefa3f8":"!pip install uszipcode","278da906":"#loading dataset\nimport pandas as pd\nimport numpy as np\n\n#visualisation\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport plotly\nimport plotly.graph_objects as go\nimport plotly.offline as pyo\nfrom plotly.offline import iplot\n\n# data preprocessing\nfrom sklearn.preprocessing import StandardScaler\n\n# data splitting\nfrom sklearn.model_selection import train_test_split\n\n# data modeling\nfrom sklearn.metrics import confusion_matrix,accuracy_score,roc_curve,classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nfrom sklearn.model_selection import GridSearchCV\n\n# Mondel performance\nfrom sklearn import metrics\nfrom sklearn.metrics import f1_score,accuracy_score, recall_score, precision_score, roc_auc_score, roc_curve, confusion_matrix, precision_recall_curve\nimport scipy.stats as stats\n\n#warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#SFS\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n\n# Zip Code\nfrom uszipcode import SearchEngine\nsearch = SearchEngine(simple_zipcode=True) \n\n# Printing style\n!pip install tabulate\nfrom tabulate import tabulate","90a89c5a":"# Dataframe styles\n\ndef style_negative(v, props=''):\n    return props if v < 0 else None\n\ndef highlight_max(s, props=''):\n    return np.where(s == np.nanmax(s.values), props, '')\n\npd.set_option('display.float_format', lambda x: '%.3f' % x)\npd.set_option('display.max_rows', 300)\npd.set_option('display.max_colwidth', 400)\npd.set_option('display.float_format', lambda x: '%.5f' % x) \n\n# Plot styles\nplt.style.use('ggplot')","aeb7afdb":"# Read the CSV file using pandas read_csv method, and creating a copy of the original dataset\n\ndf = pd.read_csv('..\/input\/personal-loan-modeling\/Bank_Personal_Loan_Modelling.csv')\ndata = df.copy()","f8c56a10":"data.shape","4c99c6de":"data.dtypes","277fcc40":"data.isnull().sum()","057bdcf8":"data.duplicated().sum()","691e1f27":"data.describe().T","5ee5c5a1":"category_columns = ['Personal Loan', 'Securities Account', 'CD Account', 'Online', 'CreditCard', 'Education']\ndata[category_columns] = data[category_columns].astype('category')","9c78c61a":"data.drop(columns='ID', axis='columns', inplace=True)","84a68fa3":"data[data['Experience'] < 0 ].style.applymap(style_negative, props='color:red;')","2b151ab0":"data[data['Experience'] < 0].groupby(by='Age')['Experience'].agg(['count','min','max']).T","a8160444":"data[(data['Age'] >= 23) & (data['Age'] <= 29)].groupby(['Age','Education'])['Experience'].describe().T","d7560384":"data.loc[data['Experience']<0,'Experience']=np.abs(data['Experience'])","1c086493":"data.columns = [i.replace(\" \", \"_\").lower() for i in data.columns]","c4199a36":"data.info()","54f08206":"data[['age', 'income', 'ccavg']].describe().T","d16c7502":"# Creating categories from Age, CC Avg, and Income to analyze the trend of borrowing Personal Loan\n\ndata[\"income_bin\"] = pd.cut(\n    x=data[\"income\"],\n    bins=[0, 39, 98, 224],\n    labels=[\"Low\", \"Mid\", \"High\"],\n)\n\ndata[\"cc_spending_bin\"] = pd.cut(\n    x=data[\"ccavg\"],\n    bins=[-0.0001, 0.7, 2.5, 10.0],\n    labels=[\"Low\", \"Mid\", \"High\"],\n)\n\ndata[\"age_bin\"] = pd.cut(\n    x=data[\"age\"],\n    bins=[0, 35, 55, 67],\n    labels=[\"Young Adults\", \"Midle Aged\", \"Senior\"],\n)","6acfac61":"len(data.zip_code.unique())","aae02ed5":"data['county'] = data['zip_code'].apply(lambda x: search.by_zipcode(x).county)","66b125c0":"data['county'].unique()","ea5214be":"data[data['county'].isnull()]['zip_code'].unique()","9652054c":"data.loc[data.isin({'zipcode' : [92717, 92634]})['zip_code'], 'county'] = 'Orange County'\ndata.loc[data.isin({'zipcode' : [93077]})['zip_code'], 'county'] = '93077'\ndata.loc[data.isin({'zipcode' : [96651]})['zip_code'], 'county'] = '96651'","8bef6325":"data.groupby(['county'])['zip_code'].count().sort_values(ascending=False)","80c073c6":"county_region = {\n'Los Angeles County':'Los Angeles Region',\n'San Diego County':'San Diego - Imperial',\n'Santa Clara County':'San Francisco Bay Area',\n'Alameda County':'San Francisco Bay Area',\n'Orange County':'Orange County Area',\n'San Francisco County':'San Francisco Bay Area',\n'San Mateo County':'San Francisco Bay Area',\n'Sacramento County':'Superior California',\n'Santa Barbara County':'Central Coast',\n'Yolo County':'Superior California',\n'Monterey County':'Central Coast',            \n'Ventura County':'Central Coast',             \n'San Bernardino County':'Inland Empire',       \n'Contra Costa County':'San Francisco Bay Area',        \n'Santa Cruz County':'Central Coast',           \n'Riverside County':'Inland Empire',            \n'Kern County':'Southern San Joaquin Valley',                 \n'Marin County':'San Francisco Bay Area',                \n'San Luis Obispo County':'Central Coast',     \n'Solano County':'San Francisco Bay Area',              \n'Humboldt County':'North Coast',            \n'Sonoma County':'North Coast',                \n'Fresno County':'Southern San Joaquin Valley',               \n'Placer County':'Superior California',                \n'Butte County':'Superior California',               \n'Shasta County':'Superior California',                \n'El Dorado County':'Superior California',             \n'Stanislaus County':'Northern San Joaquin Valley',            \n'San Benito County':'Central Coast',          \n'San Joaquin County':'Northern San Joaquin Valley',           \n'Mendocino County':'North Coast',             \n'Tuolumne County':'Northern San Joaquin Valley',                \n'Siskiyou County':'Superior California',              \n'Trinity County':'North Coast',                \n'Merced County':'Northern San Joaquin Valley',                  \n'Lake County':'North Coast',                 \n'Napa County':'North Coast',                   \n'Imperial County':'San Diego - Imperial',\n'93077':'Central Coast', # Since 93065 and 93101 zip codes are in Central Coast\n'96651':'Superior California' # Since 96145 and 96150 zip codes are in Superior California area\n}\n\ndata['region'] = data['county'].map(county_region).astype('category')\ndata['county'] = data['county'].astype('category')","a867622c":"data.dtypes","bfa40395":"data.isnull().sum()","6335cf32":"data.sample(5)","3f320bb5":"data.describe().T","ba91b5fe":"for cat_cols in data.select_dtypes(exclude=[np.int64, np.float64]).columns.unique().to_list():\n    print('Unique values and corresponding data counts for feature: '+cat_cols)\n    print('-'*90)\n    print(data[cat_cols].value_counts())\n    print('-'*90)","cb557602":"for cat_cols in data.select_dtypes(exclude=[np.int64, np.float64]).columns.unique().to_list():\n    print('Unique values and corresponding data count percentages for feature: '+cat_cols)\n    print('-'*90)\n    print(data[cat_cols].value_counts(normalize='index') * 100)\n    print('-'*90)","9b499fc2":"def summary(x):\n    '''\n    The function prints the 5 point summary and histogram, box plot, \n    violin plot, and cumulative density distribution plots for each \n    feature name passed as the argument.\n    \n    Parameters:\n    ----------\n    \n    x: str, feature name\n    \n    Usage:\n    ------------\n    \n    summary('age')\n    '''\n    x_min = data[x].min()\n    x_max = data[x].max()\n    Q1 = data[x].quantile(0.25)\n    Q2 = data[x].quantile(0.50)\n    Q3 = data[x].quantile(0.75)\n    \n    dict={'Min': x_min, 'Q1': Q1, 'Q2': Q2, 'Q3': Q3, 'Max': x_max}\n    df = pd.DataFrame(data=dict, index=['Value'])\n    print(f'5 Point Summary of {x.capitalize()} Attribute:\\n')\n    print(tabulate(df, headers = 'keys', tablefmt = 'psql'))\n\n    fig = plt.figure(figsize=(16, 8))\n    plt.subplots_adjust(hspace = 0.6)\n    sns.set_palette('Pastel1')\n    \n    plt.subplot(221, frameon=True)\n    ax1 = sns.distplot(data[x], color = 'purple')\n    ax1.axvline(\n        np.mean(data[x]), color=\"purple\", linestyle=\"--\"\n    )  # Add mean to the histogram\n    ax1.axvline(\n        np.median(data[x]), color=\"black\", linestyle=\"-\"\n    )  # Add median to the histogram\n    plt.title(f'{x.capitalize()} Density Distribution')\n    \n    plt.subplot(222, frameon=True)\n    ax2 = sns.violinplot(x = data[x], palette = 'Accent', split = True)\n    plt.title(f'{x.capitalize()} Violinplot')\n    \n    plt.subplot(223, frameon=True, sharex=ax1)\n    ax3 = sns.boxplot(x=data[x], palette = 'cool', width=0.7, linewidth=0.6, showmeans=True)\n    plt.title(f'{x.capitalize()} Boxplot')\n    \n    plt.subplot(224, frameon=True, sharex=ax2)\n    ax4 = sns.kdeplot(data[x], cumulative=True)\n    plt.title(f'{x.capitalize()} Cumulative Density Distribution')\n    \n    plt.show()","1f45a580":"summary('age')","ba699ee5":"summary('experience')","a9953d5d":"summary('income')","9e90bc3a":"summary('ccavg')","e4e471a6":"summary('mortgage')","98db1d85":"plt.figure(figsize=(10,4));\norder = data['personal_loan'].value_counts(ascending=False).index    \nax=sns.countplot(x='personal_loan', data=data , order=order, palette='nipy_spectral');\nfor p in ax.patches:\n    percentage = '{:.1f}% ({})'.format(100 * p.get_height()\/len(data['personal_loan']), p.get_height())\n    x = p.get_x() + p.get_width() \/ 2 - 0.05\n    y = p.get_y() + p.get_height() + 75\n    plt.annotate(percentage, (x, y), ha='center', color='black', fontsize='medium');\n    plt.xticks(color='black', fontsize='medium', ticks=[0,1], labels=['No', 'Yes']);\nplt.tight_layout()\nplt.title('Personal Loan Distribution by Percentage', color='black');","9ffccbc1":"# Below code plots grouped bar for each categorical feature\n\ndef perc_on_bar(cat_columns):\n    '''\n    The function takes a category column as input and plots bar chart with percentages on top of each bar\n    \n    Usage:\n    ------\n    \n    perc_on_bar('county')\n    '''\n    plt.figure(figsize=(16,14))\n    for i, col in enumerate(cat_columns):\n        plt.subplot(3,2,i+1)\n        order = data[col].value_counts(ascending=False).index  # Data order  \n        ax=sns.countplot(data=data, x=col, palette = 'crest', order=order);\n        for p in ax.patches:\n            percentage = '{:.1f}%\\n({})'.format(100 * p.get_height()\/len(data['personal_loan']), p.get_height())\n            # Added percentage and actual value\n            x = p.get_x() + p.get_width() \/ 2\n            y = p.get_y() + p.get_height() + 40\n            plt.annotate(percentage, (x, y), ha='center', color='black', fontsize='medium'); # Annotation on top of bars\n            plt.xticks(color='black', fontsize='medium', rotation= (-90 if col=='region' else 0));\n            plt.tight_layout()\n            plt.title(col.capitalize() + ' Percentage Bar Charts\\n\\n')\n","bdc7acc7":"cat_columns = data.select_dtypes(exclude=[np.int64, np.float64]).columns.unique().tolist() # Only numerical columns\ncat_columns.remove('county')\ncat_columns.remove('personal_loan')\ncat_columns.remove('age_bin')\ncat_columns.remove('income_bin')\ncat_columns.remove('cc_spending_bin')\n\nperc_on_bar(cat_columns)","3e5c0bdc":"cat_columns = ['income_bin', 'cc_spending_bin', 'age_bin']\n\nperc_on_bar(cat_columns)","989ccd25":"# Below code plots box charts for each numerical feature by each type of Personal Loan (0: Not Borrowed, 1: Borroed)\n\nplt.style.use('ggplot') # Setting plot style\nnumeric_columns = data.select_dtypes(include=np.number).columns.unique().tolist() # Only numerical columns\nnumeric_columns.remove('zip_code') # Excluding zip code, as there are too many, and it won't make sense\nplt.figure(figsize=(20,30))\n\nfor i, col in enumerate(numeric_columns):\n    plt.subplot(8,2,i+1)\n    sns.boxplot(data=data, x='personal_loan', y=col, orient='vertical', palette=\"Blues\")\n    plt.xticks(ticks=[0,1], labels=['No (0)', 'Yes (1)'])\n    plt.tight_layout()\n    plt.title(str(i+1)+ ': Personal Loan vs. ' + col, color='black')","28980b03":"# Create a function that returns a Pie chart and a Bar Graph for the categorical variables:\ndef cat_view(x):\n    \"\"\"\n    Function to create a Bar chart and a Pie chart for categorical variables.\n    \"\"\"\n    from matplotlib import cm\n    color1 = cm.inferno(np.linspace(.4, .8, 30))\n    color2 = cm.viridis(np.linspace(.4, .8, 30))\n    sns.set_palette('cubehelix')\n    fig, ax = plt.subplots(1, 2, figsize=(16, 4))\n    \n     \n    \"\"\"\n    Draw a Pie Chart on first subplot.\n    \"\"\"    \n    s = data.groupby(x).size()\n\n    mydata_values = s.values.tolist()\n    mydata_index = s.index.tolist()\n\n    def func(pct, allvals):\n        absolute = int(pct\/100.*np.sum(allvals))\n        return \"{:.1f}%\\n({:d})\".format(pct, absolute)\n\n\n    wedges, texts, autotexts = ax[0].pie(mydata_values, autopct=lambda pct: func(pct, mydata_values),\n                                      textprops=dict(color=\"w\"))\n\n    ax[0].legend(wedges, mydata_index,\n              title=x.capitalize(),\n              loc=\"center left\",\n              bbox_to_anchor=(1, 0, 0.5, 1))\n\n    plt.setp(autotexts, size=12)\n\n    ax[0].set_title(f'{x.capitalize()} Pie Chart')\n    \n    \"\"\"\n    Draw a Bar Graph on second subplot.\n    \"\"\"\n    \n    df = pd.pivot_table(data, index = [x], columns = ['personal_loan'], values = ['income'], aggfunc = len)\n\n    labels = df.index.tolist()\n    loan_no = df.values[:, 0].tolist()\n    loan_yes = df.values[:, 1].tolist()\n    \n    l = np.arange(len(labels))  # the label locations\n    width = 0.35  # the width of the bars\n\n    rects1 = ax[1].bar(l - width\/2, loan_no, width, label='No Loan', color = color1)\n    rects2 = ax[1].bar(l + width\/2, loan_yes, width, label='Loan', color = color2)\n\n    # Add some text for labels, title and custom x-axis tick labels, etc.\n    ax[1].set_ylabel('Scores')\n    ax[1].set_title(f'{x.capitalize()} Bar Graph')\n    ax[1].set_xticks(l)\n    ax[1].set_xticklabels(labels)\n    ax[1].legend()\n    \n    def autolabel(rects):\n        \n        \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n        \n        for rect in rects:\n            height = rect.get_height()\n            ax[1].annotate('{}'.format(height),\n                        xy=(rect.get_x() + rect.get_width() \/ 2, height),\n                        xytext=(0, 3),  # 3 points vertical offset\n                        textcoords=\"offset points\",\n                        fontsize = 'medium',   \n                        ha='center', va='bottom')\n\n\n    autolabel(rects1)\n    autolabel(rects2)\n\n    fig.tight_layout()\n    plt.show()\n    \n    \"\"\"\n    Draw a Stacked Bar Graph on bottom.\n    \"\"\"\n    \n    sns.set(palette=\"tab10\")\n    tab = pd.crosstab(data[x], data['personal_loan'].map({0:'No Loan', 1:'Loan'}), normalize=\"index\")\n    \n    tab.plot.bar(stacked=True, figsize=(16, 3))\n    plt.title(x.capitalize() + ' Stacked Bar Plot')\n    plt.legend(loc=\"upper right\", bbox_to_anchor=(0,1))\n    plt.show()","85c25493":"cat_view('education')","90aeb28a":"cat_view('family')","34753b36":"cat_view('securities_account')","06e08498":"cat_view('online')","354bf8f5":"cat_view('cd_account')","ecb26af1":"cat_view('creditcard')","5ad84693":"cat_view('age_bin')","f7222966":"cat_view('income_bin')","2c971160":"cat_view('cc_spending_bin')","b0ecd75b":"# Below code plots grouped bar chart for each region by each type of personal loan (borrowed: Yes or No)\nplt.figure(figsize=(16,5));\norder = data['region'].value_counts(ascending=False).index  # Data order  \nax=sns.countplot(data=data, x='region', hue='personal_loan', palette = 'crest', order=order);\n\nfor p in ax.patches:\n    percentage = '{:.1f}%\\n({})'.format(100 * p.get_height()\/len(data['personal_loan']), p.get_height())\n    # Added percentage and actual value\n    x = p.get_x() + p.get_width() \/ 2\n    y = p.get_y() + p.get_height() + 40\n    plt.annotate(percentage, (x, y), ha='center', color='black', fontsize='medium'); # Annotation on top of bars\n    plt.xticks(color='black', fontsize='medium', rotation= -90);\n\nplt.title('Personal Loan Distribution by Region\\n0: No Loan, 1: Loan', color='black');","57ebc49f":"# Below code shows swarm plot for customers by Income and Education level, seggregated by Personal Loan opted or not\n\nsns.set(palette='icefire')\nplt.figure(figsize=(15,5))\nsns.swarmplot(data=data, x='education', y='income', hue='personal_loan').set(title='Swarmplot: Education vs Income by Personal Loan\\n0: Not Borrowed, 1: Borrowed');\nplt.legend(loc=\"upper left\" ,title=\"Opted Personal Loan\", bbox_to_anchor=(1,1));","9d7094c9":"# Below code shows swarm plot for customers by Mortgage and Age Categories level, seggregated by Personal Loan opted or not\n\nplt.figure(figsize=(15,5))\nsns.swarmplot(data=data, x='age_bin', y='mortgage', hue='personal_loan').set(title='Swarmplot: Age Bin vs Mortgage by Personal Loan\\n0: Not Borrowed, 1: Borrowed');\nplt.legend(loc=\"upper left\" ,title=\"Opted Personal Loan\", bbox_to_anchor=(1,1));","aa11884d":"# Below code shows swarm plot for customers by Income and Spending Categories, seggregated by Personal Loan opted or not\n\nplt.figure(figsize=(15,5))\nsns.swarmplot(data=data, x='cc_spending_bin', y='income', hue='personal_loan').set(title='Swarmplot: CC Spending Group vs Income by Personal Loan\\n0: Not Borrowed, 1: Borrowed');\nplt.legend(loc=\"upper left\" ,title=\"Opted Personal Loan\", bbox_to_anchor=(1,1));","ecf28ad6":"sns.set_palette('tab10')\n\nsns.jointplot(data=data, x='income', y='mortgage', \\\n              hue='personal_loan');","6e28e71c":"sns.set_palette('tab10')\n\nsns.jointplot(data=data, x='income', y='ccavg', \\\n              hue='personal_loan');","e40e614b":"# Below plot shows correlations between the numerical features in the dataset\n\nplt.figure(figsize=(20,20));\nsns.set(palette=\"nipy_spectral\");\nsns.pairplot(data=data, hue='personal_loan', corner=True);","2de3bc0e":"# Plotting correlation heatmap of the features\n\ncategory_columns = ['personal_loan', 'securities_account', 'cd_account', 'online', 'creditcard', 'education']\ndata[category_columns] = data[category_columns].astype('int')\n\nsns.set(rc={\"figure.figsize\": (15, 15)})\nsns.heatmap(\n    data.corr(),\n    annot=True,\n    linewidths=0.5,\n    center=0,\n    cbar=False,\n    cmap=\"YlGnBu\",\n    fmt=\"0.2f\",\n)\nplt.show()\n\ndata[category_columns] = data[category_columns].astype('category')","50d1514f":"sns.set(palette='Accent')\n#Income Vs Education Vs Personal_Loan\nplt.figure(figsize=(15,5))\nsns.boxplot(data=data,y='income',x='education',hue='personal_loan')\nplt.show()","122a4f15":"#Income Vs Education Vs Personal_Loan\nplt.figure(figsize=(15,5))\nsns.boxplot(data=data,y='income',x='family',hue='personal_loan')\nplt.show()","ae6ddf82":"#Income Vs Education Vs Personal_Loan\nplt.figure(figsize=(15,5))\nsns.boxplot(data=data,y='mortgage',x='family',hue='personal_loan')\nplt.show()","118837b5":"#Income Vs Education Vs Personal_Loan\nplt.figure(figsize=(15,5))\nsns.boxplot(data=data,y='ccavg',x='creditcard',hue='personal_loan')\nplt.show()","968be93b":"data_loan = data.copy()","c362cbdc":"data_loan.describe(include='all').T","559538be":"data_loan.drop(columns=['experience', 'zip_code', 'county', 'income_bin', 'cc_spending_bin', 'age_bin'], inplace=True)","45937c32":"data_loan.describe(include='all').T","c91d7332":"numerical_col = data_loan.select_dtypes(include=np.number).columns.tolist()\nplt.figure(figsize=(20,5))\n\nfor i, variable in enumerate(numerical_col):\n    plt.subplot(1,5,i+1)\n    plt.boxplot(data_loan[variable],whis=1.5)\n    plt.tight_layout()\n    plt.title(variable)\n\nplt.show()","6adfa81c":"def treat_outliers(data,col):\n    '''\n    treats outliers in a varaible\n    col: str, name of the numerical varaible\n    data: data frame\n    col: name of the column\n    '''\n    Q1=data[col].quantile(0.25) # 25th quantile\n    Q3=data[col].quantile(0.75)  # 75th quantile\n    IQR=Q3-Q1\n    Lower_Whisker = Q1 - 1.5*IQR \n    Upper_Whisker = Q3 + 1.5*IQR\n    data[col] = np.clip(data[col], Lower_Whisker, Upper_Whisker) \n    # all the values smaller than Lower_Whisker will be assigned value of Lower_whisker                                                         \n    # and all the values above upper_whisker will be assigned value of upper_Whisker \n    return data\n\ndef treat_outliers_all(data, col_list):\n    '''\n    treat outlier in all numerical varaibles\n    col_list: list of numerical varaibles\n    data: data frame\n    '''\n    for c in col_list:\n        data = treat_outliers(data,c)\n        \n    return data","f9ad9f14":"numerical_col = data_loan.select_dtypes(include=np.number).columns.tolist()\n# getting list of numerical columns\n\nnumerical_col.remove('age')\nnumerical_col.remove('family')\n\n# treating outliers\ndata_loan = treat_outliers_all(data_loan,numerical_col)","a624b3dd":"numerical_col = data_loan.select_dtypes(include=np.number).columns.tolist()\nplt.figure(figsize=(20,5))\n\nfor i, variable in enumerate(numerical_col):\n    plt.subplot(1,5,i+1)\n    plt.boxplot(data_loan[variable],whis=1.5)\n    plt.tight_layout()\n    plt.title(variable)\n\nplt.show()","99057854":"X = data_loan.drop(['personal_loan'], axis=1)\ny = data_loan['personal_loan']\n\nX = pd.get_dummies(data=X, columns=['education', 'region'], drop_first=True)\n\n#Splitting data in train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state = 6)","12d062a6":"print('Shape of Training set : ',X_train.shape )\nprint('Shape of test set : ',X_test.shape )\nprint('Percentage of classes in training set:\\n',y_train.value_counts(normalize=True)*100)\nprint('Percentage of classes in test set:\\n',y_test.value_counts(normalize=True)*100)","740b3c8d":"def get_metrics_score(model,train,test,train_y,test_y,threshold=0.5,flag=True,roc=False):\n    '''\n    Function to calculate different metric scores of the model - Accuracy, Recall, Precision, and F1 score\n    model: classifier to predict values of X\n    train, test: Independent features\n    train_y,test_y: Dependent variable\n    threshold: thresold for classifiying the observation as 1\n    flag: If the flag is set to True then only the print statements showing different will be displayed. The default value is set to True.\n    roc: If the roc is set to True then only roc score will be displayed. The default value is set to False.\n    '''\n    # defining an empty list to store train and test results\n    \n    score_list=[] \n    \n    pred_train = (model.predict_proba(train)[:,1]>threshold)\n    pred_test = (model.predict_proba(test)[:,1]>threshold)\n\n    pred_train = np.round(pred_train)\n    pred_test = np.round(pred_test)\n    \n    train_acc = accuracy_score(pred_train,train_y)\n    test_acc = accuracy_score(pred_test,test_y)\n    \n    train_recall = recall_score(train_y,pred_train)\n    test_recall = recall_score(test_y,pred_test)\n    \n    train_precision = precision_score(train_y,pred_train)\n    test_precision = precision_score(test_y,pred_test)\n    \n    train_f1 = f1_score(train_y,pred_train)\n    test_f1 = f1_score(test_y,pred_test)\n    \n    \n    score_list.extend((train_acc,test_acc,train_recall,test_recall,train_precision,test_precision,train_f1,test_f1,pred_train,pred_test))\n        \n    \n    if flag == True: \n        print(\"Accuracy on training set : \",accuracy_score(pred_train,train_y))\n        print(\"Accuracy on test set : \",accuracy_score(pred_test,test_y))\n        print(\"Recall on training set : \",recall_score(train_y,pred_train))\n        print(\"Recall on test set : \",recall_score(test_y,pred_test))\n        print(\"Precision on training set : \",precision_score(train_y,pred_train))\n        print(\"Precision on test set : \",precision_score(test_y,pred_test))\n        print(\"F1 on training set : \",f1_score(train_y,pred_train))\n        print(\"F1 on test set : \",f1_score(test_y,pred_test))\n   \n    if roc == True:\n        pred_train_prob = model.predict_proba(train)[:,1]\n        pred_test_prob = model.predict_proba(test)[:,1]\n        print(\"ROC-AUC Score on training set : \",roc_auc_score(train_y,pred_train))\n        print(\"ROC-AUC Score on test set : \",roc_auc_score(test_y,pred_test))\n    \n    return score_list # returning the list with train and test scores","a7645eea":"def make_confusion_matrix(model,test_X,y_actual,i,seg,labels=[1, 0]):\n    '''\n    model : classifier to predict values of X\n    test_X: test set\n    y_actual : ground truth  \n    \n    '''\n    y_predict = model.predict(test_X)\n    cm=metrics.confusion_matrix( y_actual, y_predict, labels=[1,0])\n    df_cm = pd.DataFrame(cm, index = [i for i in ['Actual - Borrowed', 'Actual - Not Borrowed']],\n                  columns = [i for i in ['Predicted - Borrowed','Predicted - Not Borrowed']])\n    group_counts = [\"{0:0.0f}\".format(value) for value in\n                cm.flatten()]\n    group_percentages = [\"{0:.2%}\".format(value) for value in\n                         cm.flatten()\/np.sum(cm)]\n    labels = [f\"{v1}\\n{v2}\" for v1, v2 in\n              zip(group_counts,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n    plt.figure(figsize = (10,7))\n    sns.heatmap(df_cm, annot=labels,fmt='', ax=axes[i], cmap='Blues').set(title='Confusion Matrix of {} Set'.format(seg))","9dc9d310":"# # defining empty lists to add train and test results\nacc_train = []\nacc_test = []\nrecall_train = []\nrecall_test = []\nprecision_train = []\nprecision_test = []\nf1_train = []\nf1_test = []\n\ndef add_score_model(score):\n    '''Add scores to list so that we can compare all models score together'''   \n    acc_train.append(score[0])\n    acc_test.append(score[1])\n    recall_train.append(score[2])\n    recall_test.append(score[3])\n    precision_train.append(score[4])\n    precision_test.append(score[5])\n    f1_train.append(score[6])\n    f1_test.append(score[7])","36ccca24":"# There are different solvers available in Sklearn logistic regression\n# The newton-cg solver is faster for high-dimensional data\n# Adding class weight since the data was not balanced\n\nlg1 = LogisticRegression(solver='newton-cg',random_state=1)\nmodel1  = lg1.fit(X_train, y_train)","e1cf6cb2":"# checking model performances for this model\nscores_LR = get_metrics_score(model1, X_train, X_test, y_train, y_test)\nadd_score_model(scores_LR)\n\nfig, axes = plt.subplots(1,2,figsize=(16,5));\n# creating confusion matrix on train\nmake_confusion_matrix(lg1, X_train, y_train, i=0, seg='Training')\n\n# creating confusion matrix on test\nmake_confusion_matrix(lg1, X_test, y_test, i=1, seg='Testing')","1cb7f9dc":"fig, axes = plt.subplots(1,2, figsize=(20, 7))\n\nlogit_roc_auc_train = roc_auc_score(y_train, lg1.predict_proba(X_train)[:,1])\nfpr, tpr, thresholds = roc_curve(y_train, lg1.predict_proba(X_train)[:,1])\nsns.lineplot(fpr, tpr, ax=axes[0]).set(title = 'Receiver operating characteristic on Train\\nLogistic Regression (area = %0.2f)' % logit_roc_auc_train)\naxes[0].plot([0, 1], [0, 1],'r--')\naxes[0].set_xlim([0.0, 1.0])\naxes[0].set_ylim([0.0, 1.05])\naxes[0].set_xlabel('False Positive Rate')\naxes[0].set_ylabel('True Positive Rate')\n\nlogit_roc_auc_test = roc_auc_score(y_test, lg1.predict_proba(X_test)[:,1])\nfpr, tpr, thresholds = roc_curve(y_test, lg1.predict_proba(X_test)[:,1])\nsns.lineplot(fpr, tpr, ax=axes[1]).set(title = 'Receiver operating characteristic on Test\\nLogistic Regression (area = %0.2f)' % logit_roc_auc_test)\naxes[1].plot([0, 1], [0, 1],'r--')\naxes[1].set_xlim([0.0, 1.0])\naxes[1].set_ylim([0.0, 1.05])\naxes[1].set_xlabel('False Positive Rate')\naxes[1].set_ylabel('True Positive Rate')\nplt.show()","d778811d":"# There are different solvers available in Sklearn logistic regression\n# The newton-cg solver is faster for high-dimensional data\n# Adding class weight since the data was not balanced\n\nlg2 = LogisticRegression(solver='newton-cg',random_state=1, class_weight={0:0.09,1:0.91})\nmodel2  = lg2.fit(X_train, y_train)","3163a150":"# checking model performances for this model\nscores_LR = get_metrics_score(model2, X_train, X_test, y_train, y_test)\nadd_score_model(scores_LR)\n\nfig, axes = plt.subplots(1,2,figsize=(16,5));\n# creating confusion matrix on train\nmake_confusion_matrix(lg2, X_train, y_train, i=0, seg='Training')\n\n# creating confusion matrix on test\nmake_confusion_matrix(lg2, X_test, y_test, i=1, seg='Testing')","3d14af32":"fig, axes = plt.subplots(1,2, figsize=(20, 7))\n\nlogit_roc_auc_train = roc_auc_score(y_train, lg2.predict_proba(X_train)[:,1])\nfpr, tpr, thresholds = roc_curve(y_train, lg2.predict_proba(X_train)[:,1])\nsns.lineplot(fpr, tpr, ax=axes[0]).set(title = 'Receiver operating characteristic on Train\\nLogistic Regression (area = %0.2f)' % logit_roc_auc_train)\naxes[0].plot([0, 1], [0, 1],'r--')\naxes[0].set_xlim([0.0, 1.0])\naxes[0].set_ylim([0.0, 1.05])\naxes[0].set_xlabel('False Positive Rate')\naxes[0].set_ylabel('True Positive Rate')\n\nlogit_roc_auc_test = roc_auc_score(y_test, lg2.predict_proba(X_test)[:,1])\nfpr, tpr, thresholds = roc_curve(y_test, lg2.predict_proba(X_test)[:,1])\nsns.lineplot(fpr, tpr, ax=axes[1]).set(title = 'Receiver operating characteristic on Test\\nLogistic Regression (area = %0.2f)' % logit_roc_auc_test)\naxes[1].plot([0, 1], [0, 1],'r--')\naxes[1].set_xlim([0.0, 1.0])\naxes[1].set_ylim([0.0, 1.05])\naxes[1].set_xlabel('False Positive Rate')\naxes[1].set_ylabel('True Positive Rate')\nplt.show()","ed04f321":"#Calculate Odds Ratio, probability\n##create a data frame to collate Odds ratio, probability and p-value of the coef\nlg2_coef = pd.DataFrame(lg2.coef_[0], X_train.columns, columns=['coef'])\nlg2_coef.loc[:, \"Odds Ratio\"] = np.exp(lg2_coef.coef)\nlg2_coef['Probability'] = lg2_coef['Odds Ratio']\/(1+lg2_coef['Odds Ratio'])\nlg2_coef['Percentage Change of Odds']=(np.exp(lg2_coef.coef)-1)*100\npd.options.display.float_format = '{:.2f}'.format\nlg2_coef = lg2_coef.sort_values(by=\"Odds Ratio\", ascending=False)\nlg2_coef","560caae8":"# Optimal threshold as per AUC-ROC curve\n# The optimal cut off would be where tpr is high and fpr is low\nfpr, tpr, thresholds = metrics.roc_curve(y_test, lg2.predict_proba(X_test)[:,1])\n\noptimal_idx = np.argmax(tpr - fpr)\noptimal_threshold_auc_roc = thresholds[optimal_idx]\nprint(optimal_threshold_auc_roc)","9051c8eb":"scores_LR = get_metrics_score(lg2, X_train, X_test, y_train, y_test, threshold=optimal_threshold_auc_roc, roc=True)\nadd_score_model(scores_LR)","2bb671f9":"y_scores=lg2.predict_proba(X_train)[:,1]\nprec, rec, tre = precision_recall_curve(y_train, y_scores,)\n\ndef plot_prec_recall_vs_tresh(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], 'b--', label='precision')\n    plt.plot(thresholds, recalls[:-1], 'g--', label = 'recall')\n    plt.xlabel('Threshold')\n    plt.legend(loc='upper left')\n    plt.ylim([0,1])\nplt.figure(figsize=(10,7))\nplot_prec_recall_vs_tresh(prec, rec, tre)\nplt.show()","81562fb2":"optimal_threshold_curve = 0.82\n\nscores_LR = get_metrics_score(lg2,X_train,X_test,y_train,y_test,threshold=optimal_threshold_curve,roc=True)\nadd_score_model(scores_LR)","91c0675f":"model = LogisticRegression(solver='newton-cg', n_jobs=-1,random_state=1,max_iter=100)","bd5fd442":"X_train.shape","15729433":"# we will first build model with all varaible\nsfs = SFS(model, k_features=20, forward=True, floating=False, scoring='recall', verbose=2, cv=5, n_jobs=-1)\n\nsfs = sfs.fit(X_train, y_train)","a1e95b86":"fig1 = plot_sfs(sfs.get_metric_dict(),kind='std_dev',figsize=(12,5))\nplt.ylim([0.2, 0.7])\nplt.title('Sequential Forward Selection (w. StdDev)')\nplt.xticks(rotation=90)\nplt.show()","7dbe4710":"sfs1 = SFS(model, k_features=9, forward=True, floating=False, scoring='recall', verbose=2, cv=5, n_jobs=-1)\n\nsfs1 = sfs1.fit(X_train, y_train)\n\nfig1 = plot_sfs(sfs1.get_metric_dict(),kind='std_dev',figsize=(10,5))\n\nplt.ylim([0.2, 0.7])\nplt.title('Sequential Forward Selection (w. StdDev)')\nplt.grid()\nplt.show()","1524d630":"feat_cols = list(sfs1.k_feature_idx_)\nprint(feat_cols)","682afa64":"X_train.columns[feat_cols]","c9d42e5d":"X_train_final = X_train[X_train.columns[feat_cols]]\n\n#Creating new x_test with the same variables that we selected for x_train\nX_test_final = X_test[X_train_final.columns]","3edb8c4f":"#Fitting logistic regession model\n\nlogreg = LogisticRegression(solver='newton-cg',max_iter=1000,penalty='none',verbose=True,n_jobs=-1,random_state=0)\n\n# There are several optimizer, we are using optimizer called as 'newton-cg' with max_iter equal to 10000 \n# max_iter indicates number of iteration needed to converge\n\nlogreg.fit(X_train_final, y_train)","6587eada":"scores_LR = get_metrics_score(logreg,X_train_final,X_test_final,y_train,y_test,flag=True)\nadd_score_model(scores_LR)\n\n# creating confusion matrix\nfig, axes = plt.subplots(1,2,figsize=(16,5));\nmake_confusion_matrix(logreg,X_train_final,y_train, i=0, seg='Training')\nmake_confusion_matrix(logreg,X_test_final,y_test, i=1, seg='Testing')","16f6de1e":"#Fitting logistic regession model\n\nlogreg2 = LogisticRegression(solver='newton-cg',class_weight={0:0.09,1:0.91},\\\n                            max_iter=1000,penalty='none',verbose=True,n_jobs=-1,random_state=0)\n\n# There are several optimizer, we are using optimizer called as 'newton-cg' with max_iter equal to 10000 \n# max_iter indicates number of iteration needed to converge\n\nlogreg2.fit(X_train_final, y_train)","7fa540ad":"scores_LR = get_metrics_score(logreg2,X_train_final,X_test_final,y_train,y_test,flag=True)\nadd_score_model(scores_LR)\n\n# creating confusion matrix\nfig, axes = plt.subplots(1,2,figsize=(16,5));\nmake_confusion_matrix(logreg2,X_train_final,y_train, i=0, seg='Training')\nmake_confusion_matrix(logreg2,X_test_final,y_test, i=1, seg='Testing')","04bf7e98":"comparison_frame = pd.DataFrame({'Model':['LR Model',\n                                          'LR Model - w\/ Class Weight',\n                                          'LR Model - w\/ Class Weight, Optimal threshold = 0.51',\n                                          'LR Model - w\/ Class Weight, Optimal threshold = 0.82',\n                                          'LR Model - SFS (9 variable)',\n                                          'LR Model - w\/ Class Weight, SFS (9 variable)'\n                                          ],\n                                          'Train_Accuracy':acc_train, \n                                          'Test_Accuracy':acc_test,\n                                          'Train Recall':recall_train,\n                                          'Test Recall':recall_test, \n                                          'Train Precision':precision_train,\n                                          'Test Precision':precision_test,\n                                          'Train F1':f1_train,\n                                          'Test F1':f1_test\n                                })\n\nprint('Class Weight used: {0: 0.91, 1: 0.09}')\ncomparison_frame.style.highlight_max(color = 'lightgreen', axis = 0).highlight_min(color = 'pink',axis = 0)                                    ","0ce0c15b":"dTree = DecisionTreeClassifier(criterion = 'gini', random_state=1)\ndTree.fit(X_train, y_train)","63849cee":"print(\"Accuracy on training set : \",dTree.score(X_train, y_train))\nprint(\"Accuracy on test set : \",dTree.score(X_test, y_test))","fc2670d3":"##  Function to calculate recall score\ndef get_recall_score(model):\n    '''\n    model : classifier to predict values of X\n\n    '''\n    pred_train = model.predict(X_train)\n    pred_test = model.predict(X_test)\n    print(\"Recall on training set : \",metrics.recall_score(y_train,pred_train))\n    print(\"Recall on test set : \",metrics.recall_score(y_test,pred_test))","7e7c396a":"# Recall on train and test\nget_recall_score(dTree)","6796f428":"fig, axes = plt.subplots(1,2,figsize=(16,5));\n\nmake_confusion_matrix(dTree, X_train, y_train, i=0, seg='Training')\nmake_confusion_matrix(dTree, X_test, y_test, i=1, seg='Testing')","251cb588":"feature_names = list(X.columns)\nprint(feature_names)","bc953530":"plt.figure(figsize=(20,30))\nout = tree.plot_tree(dTree,feature_names=feature_names,filled=True,fontsize=9,node_ids=True,class_names=True)\nfor o in out:\n     arrow = o.arrow_patch\n     if arrow is not None:\n        arrow.set_edgecolor('black')\n        arrow.set_linewidth(1)\nplt.show()\nplt.show()","2c5775ce":"# Text report showing the rules of a decision tree -\n\nprint(tree.export_text(dTree,feature_names=feature_names,show_weights=True))","2d1625df":"# importance of features in the tree building ( The importance of a feature is computed as the \n#(normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )\n\nprint (pd.DataFrame(dTree.feature_importances_, columns = [\"Imp\"], \\\n                    index = X_train.columns).sort_values(by = 'Imp', ascending = False))","bf5f3582":"importances = dTree.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(12,12))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='green', align='center')\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","b34f5a2b":"dTree1 = DecisionTreeClassifier(criterion = 'gini',max_depth=7,random_state=1, class_weight={0:0.91, 1:0.09})\ndTree1.fit(X_train, y_train)","fdf437f3":"# Accuracy on train and test\nprint(\"Accuracy on training set : \",dTree1.score(X_train, y_train))\nprint(\"Accuracy on test set : \",dTree1.score(X_test, y_test))\n# Recall on train and test\nget_recall_score(dTree1)","1121e2e4":"dTree1 = DecisionTreeClassifier(criterion = 'gini',max_depth=7,random_state=1)\ndTree1.fit(X_train, y_train)","11732aac":"# Accuracy on train and test\nprint(\"Accuracy on training set : \",dTree1.score(X_train, y_train))\nprint(\"Accuracy on test set : \",dTree1.score(X_test, y_test))\n# Recall on train and test\nget_recall_score(dTree1)","261a2d96":"fig, axes = plt.subplots(1,2,figsize=(16,5));\n\nmake_confusion_matrix(dTree1, X_train, y_train, i=0, seg='Training')\nmake_confusion_matrix(dTree1, X_test, y_test, i=1, seg='Testing')","ff01ff47":"plt.figure(figsize=(15,10))\n\nout = tree.plot_tree(dTree1,feature_names=feature_names,filled=True,fontsize=9,node_ids=True,class_names=True)\nfor o in out:\n     arrow = o.arrow_patch\n     if arrow is not None:\n        arrow.set_edgecolor('black')\n        arrow.set_linewidth(1)\nplt.show()\nplt.show()","62f2386c":"# importance of features in the tree building ( The importance of a feature is computed as the \n#(normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )\n\nprint (pd.DataFrame(dTree1.feature_importances_, columns = [\"Imp\"], \\\n                    index = X_train.columns).sort_values(by = 'Imp', ascending = False))","1a2b0089":"importances = dTree1.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(12,12))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='green', align='center')\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","09f37b6a":"# Choose the type of classifier. \nestimator = DecisionTreeClassifier(random_state=1)\n\n# Grid of parameters to choose from\n## add from article\nparameters = {'max_depth': np.arange(1,13), \n              'min_samples_leaf': [1, 2, 5, 7, 10, 15, 20],\n              'max_leaf_nodes' : [5, 10, 15, 20, 25, 30],\n              'min_impurity_decrease': [0.001,0.01,0.1]\n             }\n\n# Type of scoring used to compare parameter combinations\nacc_scorer = metrics.make_scorer(metrics.recall_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer,cv=5)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\nestimator = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data. \nestimator.fit(X_train, y_train)","7fdefadd":"# Accuracy on train and test\nprint(\"Accuracy on training set : \",estimator.score(X_train, y_train))\nprint(\"Accuracy on test set : \",estimator.score(X_test, y_test))\n# Recall on train and test\nget_recall_score(estimator)","74d6481d":"fig, axes = plt.subplots(1,2,figsize=(16,5));\n\nmake_confusion_matrix(estimator, X_train, y_train, i=0, seg='Training')\nmake_confusion_matrix(estimator, X_test, y_test, i=1, seg='Testing')","5d5e5f83":"plt.figure(figsize=(15,10))\n\nout = tree.plot_tree(estimator,feature_names=feature_names,filled=True,fontsize=9,node_ids=True,class_names=True)\nfor o in out:\n     arrow = o.arrow_patch\n     if arrow is not None:\n        arrow.set_edgecolor('black')\n        arrow.set_linewidth(1)\nplt.show()\nplt.show()","d49da185":"# importance of features in the tree building ( The importance of a feature is computed as the \n#(normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )\n\nprint (pd.DataFrame(estimator.feature_importances_, columns = [\"Imp\"], \\\n                    index = X_train.columns).sort_values(by = 'Imp', ascending = False))","8fe72e99":"importances = estimator.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(12,12))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='green', align='center')\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","59287124":"clf = DecisionTreeClassifier(random_state=1)\npath = clf.cost_complexity_pruning_path(X_train, y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities","3119baaf":"pd.DataFrame(path).T","bea8cf5d":"fig, ax = plt.subplots(figsize=(10,5))\nax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle=\"steps-post\")\nax.set_xlabel(\"effective alpha\")\nax.set_ylabel(\"total impurity of leaves\")\nax.set_title(\"Total Impurity vs effective alpha for training set\")\nplt.show()","617af939":"clfs = []\nfor ccp_alpha in ccp_alphas:\n    clf = DecisionTreeClassifier(random_state=1, ccp_alpha=ccp_alpha)\n    clf.fit(X_train, y_train)\n    clfs.append(clf)\nprint(\"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n      clfs[-1].tree_.node_count, ccp_alphas[-1]))","852fb964":"clfs = clfs[:-1]\nccp_alphas = ccp_alphas[:-1]\n\nnode_counts = [clf.tree_.node_count for clf in clfs]\ndepth = [clf.tree_.max_depth for clf in clfs]\nfig, ax = plt.subplots(2, 1,figsize=(16,12))\nax[0].plot(ccp_alphas, node_counts, marker='o', drawstyle=\"steps-post\")\nax[0].set_xlabel(\"alpha\")\nax[0].set_ylabel(\"number of nodes\")\nax[0].set_title(\"Number of nodes vs alpha\")\nax[1].plot(ccp_alphas, depth, marker='o', drawstyle=\"steps-post\")\nax[1].set_xlabel(\"alpha\")\nax[1].set_ylabel(\"depth of tree\")\nax[1].set_title(\"Depth vs alpha\")\nfig.tight_layout()","aa71dc91":"train_scores = [clf.score(X_train, y_train) for clf in clfs]\ntest_scores = [clf.score(X_test, y_test) for clf in clfs]","60f1951a":"fig, ax = plt.subplots(figsize=(10,5))\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker='o', label=\"train\",\n        drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker='o', label=\"test\",\n        drawstyle=\"steps-post\")\nax.legend()\nplt.show()","639a36eb":"recall_train=[]\nfor clf in clfs:\n    pred_train3=clf.predict(X_train)\n    values_train=metrics.recall_score(y_train,pred_train3)\n    recall_train.append(values_train)\n\nrecall_test=[]\nfor clf in clfs:\n    pred_test3=clf.predict(X_test)\n    values_test=metrics.recall_score(y_test,pred_test3)\n    recall_test.append(values_test)","77951527":"fig, ax = plt.subplots(figsize=(15,5))\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"Recall\")\nax.set_title(\"Recall vs alpha for training and testing sets\")\nax.plot(ccp_alphas, recall_train, marker='o', label=\"train\",\n        drawstyle=\"steps-post\")\nax.plot(ccp_alphas, recall_test, marker='o', label=\"test\",\n        drawstyle=\"steps-post\")\nax.legend()\nplt.show()","4b174213":"# creating the model where we get highest train and test recall\nindex_best_model = np.argmax(recall_test)\nbest_model = clfs[index_best_model]\nprint(best_model)","7efb32b1":"# Accuracy on train and test\nprint(\"Accuracy on training set : \",best_model.score(X_train, y_train))\nprint(\"Accuracy on test set : \",best_model.score(X_test, y_test))\n# Recall on train and test\nget_recall_score(best_model)","e633a932":"fig, axes = plt.subplots(1,2,figsize=(16,5));\n\nmake_confusion_matrix(best_model, X_train, y_train, i=0, seg='Training')\nmake_confusion_matrix(best_model, X_test, y_test, i=1, seg='Testing')","0f69a52c":"plt.figure(figsize=(15,10))\n\nout = tree.plot_tree(best_model,feature_names=feature_names,filled=True,fontsize=9,node_ids=True,class_names=True)\nfor o in out:\n     arrow = o.arrow_patch\n     if arrow is not None:\n        arrow.set_edgecolor('black')\n        arrow.set_linewidth(1)\nplt.show()\nplt.show()","4c597f59":"# Text report showing the rules of a decision tree -\n\nprint(tree.export_text(best_model,feature_names=feature_names,show_weights=True))","63dbd46e":"# importance of features in the tree building ( The importance of a feature is computed as the \n#(normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )\n\nprint (pd.DataFrame(best_model.feature_importances_, columns = [\"Imp\"], \\\n                    index = X_train.columns).sort_values(by = 'Imp', ascending = False))","d9be2e7c":"importances = best_model.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(12,12))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='green', align='center')\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","118c2a06":"clf = DecisionTreeClassifier(random_state=1, class_weight={0: 0.91, 1:0.09})\npath = clf.cost_complexity_pruning_path(X_train, y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities","8c9a3794":"pd.DataFrame(path).T","38b8b416":"fig, ax = plt.subplots(figsize=(10,5))\nax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle=\"steps-post\")\nax.set_xlabel(\"effective alpha\")\nax.set_ylabel(\"total impurity of leaves\")\nax.set_title(\"Total Impurity vs effective alpha for training set\")\nplt.show()","c1b1ae71":"clfs = []\nfor ccp_alpha in ccp_alphas:\n    clf = DecisionTreeClassifier(random_state=1, ccp_alpha=ccp_alpha, class_weight={0:0.91, 1:0.09})\n    clf.fit(X_train, y_train)\n    clfs.append(clf)\nprint(\"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n      clfs[-1].tree_.node_count, ccp_alphas[-1]))","a51aaa16":"clfs = clfs[:-1]\nccp_alphas = ccp_alphas[:-1]\n\nnode_counts = [clf.tree_.node_count for clf in clfs]\ndepth = [clf.tree_.max_depth for clf in clfs]\nfig, ax = plt.subplots(2, 1,figsize=(16,12))\nax[0].plot(ccp_alphas, node_counts, marker='o', drawstyle=\"steps-post\")\nax[0].set_xlabel(\"alpha\")\nax[0].set_ylabel(\"number of nodes\")\nax[0].set_title(\"Number of nodes vs alpha\")\nax[1].plot(ccp_alphas, depth, marker='o', drawstyle=\"steps-post\")\nax[1].set_xlabel(\"alpha\")\nax[1].set_ylabel(\"depth of tree\")\nax[1].set_title(\"Depth vs alpha\")\nfig.tight_layout()","1bf5f8d8":"train_scores = [clf.score(X_train, y_train) for clf in clfs]\ntest_scores = [clf.score(X_test, y_test) for clf in clfs]","2d0bdfac":"fig, ax = plt.subplots(figsize=(10,5))\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker='o', label=\"train\",\n        drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker='o', label=\"test\",\n        drawstyle=\"steps-post\")\nax.legend()\nplt.show()","8e31a339":"recall_train=[]\nfor clf in clfs:\n    pred_train3=clf.predict(X_train)\n    values_train=metrics.recall_score(y_train,pred_train3)\n    recall_train.append(values_train)\n\nrecall_test=[]\nfor clf in clfs:\n    pred_test3=clf.predict(X_test)\n    values_test=metrics.recall_score(y_test,pred_test3)\n    recall_test.append(values_test)","85594682":"fig, ax = plt.subplots(figsize=(15,5))\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"Recall\")\nax.set_title(\"Recall vs alpha for training and testing sets\")\nax.plot(ccp_alphas, recall_train, marker='o', label=\"train\",\n        drawstyle=\"steps-post\")\nax.plot(ccp_alphas, recall_test, marker='o', label=\"test\",\n        drawstyle=\"steps-post\")\nax.legend()\nplt.show()","9fa14b74":"# creating the model where we get highest train and test recall\nindex_best_model = np.argmax(recall_test)\nbest_model_cw = clfs[index_best_model]\nprint(best_model_cw)","36a28835":"# Accuracy on train and test\nprint(\"Accuracy on training set : \",best_model_cw.score(X_train, y_train))\nprint(\"Accuracy on test set : \",best_model_cw.score(X_test, y_test))\n# Recall on train and test\nget_recall_score(best_model_cw)","c3ab4dfe":"fig, axes = plt.subplots(1,2,figsize=(16,5));\n\nmake_confusion_matrix(best_model_cw, X_train, y_train, i=0, seg='Training')\nmake_confusion_matrix(best_model_cw, X_test, y_test, i=1, seg='Testing')","e71a6b85":"plt.figure(figsize=(15,10))\n\nout = tree.plot_tree(best_model_cw,feature_names=feature_names,filled=True,fontsize=9,node_ids=True,class_names=True)\nfor o in out:\n     arrow = o.arrow_patch\n     if arrow is not None:\n        arrow.set_edgecolor('black')\n        arrow.set_linewidth(1)\nplt.show()\nplt.show()","f5a24755":"##  Function to calculate recall score\ndef model_scores(model, X_train, X_test, y_train, y_test):\n    score_list = []\n    pred_train = model.predict(X_train)\n    pred_test = model.predict(X_test)\n    recall_train = metrics.recall_score(y_train,pred_train)\n    recall_test = metrics.recall_score(y_test,pred_test)\n    accuracy_train = model.score(X_train, y_train)\n    accuracy_test = model.score(X_test, y_test)\n    \n    score_list.extend((accuracy_train, accuracy_test, recall_train, recall_test))\n    \n    return score_list","45aeedc2":"score_dtree = model_scores(dTree, X_train, X_test, y_train, y_test)\nscore_dtree1 = model_scores(dTree1, X_train, X_test, y_train, y_test)\nscore_estimator = model_scores(estimator, X_train, X_test, y_train, y_test)\nscore_ccp_pruned = model_scores(best_model, X_train, X_test, y_train, y_test)\nscore_ccp_pruned_with_cw = model_scores(best_model_cw, X_train, X_test, y_train, y_test)","9ccd7172":"scores_arr = [score_dtree , score_dtree1, score_estimator, score_ccp_pruned, score_ccp_pruned_with_cw]\n\ndf_comp = pd.DataFrame(scores_arr, index=['Decision Tree', 'Decision Tree - Max Depth 7', \n                                         'Decision Tree - Hyperparameter Tuned', 'Decision Tree - Post-pruning', 'Decision Tree - Post-pruning + class weight'],\n                      columns=['Train Accuracy', 'Test Accuracy', 'Train Recall', 'Test Recall'])\n\nfor col in df_comp.columns.to_list():\n    df_comp[col] = (round(df_comp[col] * 100, 0)).astype(int)\n\ndf_comp.style.highlight_max(color = 'lightgreen')","ac050b1b":"fig, axes = plt.subplots(1,2,figsize=(16,5));\n\nmake_confusion_matrix(best_model, X_train, y_train, i=0, seg='Training')\nmake_confusion_matrix(best_model, X_test, y_test, i=1, seg='Testing')","81480ae6":"test_dataset = X_test.copy()\ntest_dataset['personal_loan_actual'] = y_test\ntest_dataset['personal_loan_predicted'] = best_model.predict(X_test)","538c8772":"test_dataset[(test_dataset['personal_loan_actual'] == 0) & (test_dataset['personal_loan_predicted'] == 1)].T","2e2610a9":"test_dataset[(test_dataset['personal_loan_actual'] == 1) & (test_dataset['personal_loan_predicted'] == 0)].T","a96ed15b":"plt.figure(figsize=(15,7))\nplt.subplot(121)\nplt.title('Actual % Distribution of Loan Borrowed or Not')\nplt.pie(data=test_dataset,x=test_dataset[\"personal_loan_actual\"].value_counts(),autopct='%1.1f%%', \n        colors=['lightgreen', 'cyan'], labels=['Not Borrowed', 'Borrowed']);\nplt.subplot(122)\nplt.title('Predicted % Distribution of Loan Borrowed or Not')\nplt.pie(data=test_dataset,x=test_dataset[\"personal_loan_predicted\"].value_counts(),autopct='%1.1f%%', \n        colors=['lightgreen', 'cyan'], labels=['Not Borrowed', 'Borrowed']);","273f5f9a":"pd.crosstab(test_dataset['personal_loan_actual'],test_dataset['personal_loan_predicted'],\n            normalize='index').plot.bar(stacked=True, figsize=(16, 3), color=['lightgreen', 'cyan']);","2c908f5d":"pd.crosstab(test_dataset['personal_loan_predicted'],test_dataset['personal_loan_actual'],\n            normalize='index').plot.bar(stacked=True, figsize=(16, 3), color=['lightgreen', 'cyan']);","3665b00d":">`13 columns` are `integers`, only `CCAvg` column is `float`","aa1c681e":"#### Feature Importance","fe1ec3d8":"#### CC Spending buckets and Income-wise customer distribution, by Personal Loan, opted or not","97e7fa4a":"For the remainder, we remove the last element in `clfs` and `ccp_alphas`, because it is the trivial tree with only one node. Here we show that the number of nodes and tree depth decreases as alpha increases.","21a85471":">- As Education level increases, Mean Income also increases.\n>- Customers with Education Graduate and Advanced level who have personal loans have a much higher mean income than Education level Undergraduate customers","aa1163ea":"Let's check the categorical feature-wise value distributions by percentage of count","2b39a7d9":"##### ROC - AUC on LR Model 2","7cfcb30d":"#### Region wise customer distribution by Personal Loan (Borrowed or not)","8514a1ce":"#### Check Model Score after removing class weight","9f3c3c01":"#### Personal Loan categorical percentage distribution","f71874eb":"### Categorical column statistics","87fcb15b":">- Customers age is in range of `23 - 67`, with mean and median of `~45`.\n>- Maximum experience is `43` years. where as mean and median are `~20`.\n>- Income are in range `8K - 224k` USD. Mean is `73k` USD and median is `64k` USD. \n>- Max salary `224K` USD, which is quite higher than 3rd quartile. Need to be treated.\n>- Maximum mortgage taken is `635K` USD. Need to be treated, since it is quite higher than 3rd quartile.\n>- Average spending on credit card per month ranges from `1K - 10K` USD with mean of `1.9K` USD and median of `1.5K` USD","56b1ea71":">*Most of these customers have mid level income, family size is 1\/2, and did not have advanced education, thus they had been misclassified as customers who would not borrow a personal loan. These are some exceptions who actually did borrow a `Personal Loan`.*","e9aeff29":"> Both the accuracy and Recall of the training and Testing dataset are really impressive","60175f58":"> Best `ccp_alpha` appears to be 0.0004761904761904762","e9dc1586":"#### Confusion Matrix","b54f4917":"#### CC Spending Bucket by Personal Loan (borrowed or not)","6baaac9d":">***Observation***:\n>- The model shows `Income` is the most important feature, followed by `Family Size` and `Education`\n>- It is a very complex tree, and appears to be over-fitting, accuracy on train and test `100%`, recall on test is `94%`","cca44d0b":"### Dropping columns","711782b9":"#### Decision Tree model comparison","adb58f16":">The negative experience values are distributed among 23-29 Age group\n\nChecking `Experience` by `Age` and `Education` for only the 23-29 Age group","bc8ea199":"#### Build the model","a21946f3":"### Create bins for Age, Income and Credit Card Expenditure Average","aa99155d":"### Decision Tree: CCP with Class Weight","248c2ba5":"#### Using Sequential Feature Selection","5812fe8f":"##### Trying to find Optimal AUR ROC threshold to check model performance improvement","b374ffd2":"The statistical summary","c6f5aa5b":">*Most of these customers have high income, live in San Francisco Bay Area, quite high expense as well as higher level of education. These are some exceptions who actually did not borrow a `Personal Loan`*","39eadbd9":"#### Visualize the Decision Tree","d0e157f6":"### Multi-variate Plots","f8f48102":"## Initial set-up","601008f2":"#### Check Model Score","fe487b95":">***Observation***:\n>- The model shows `Income` is the most important feature, followed by `Family Size` and `Education`\n>- It also is a simpler `Decision Tree` with 93% test recall, and no over-fitting","1b9999c0":"### Univariate Analysis","26a04b55":"##### Model Score","d7de2dd8":"Statistical summary, one last time","da6d9e7e":">`Experience` has negative values, which is not possible. Need to explore and decide best imputation strategy","7f0a4704":">- `1095` customers are from `Los Angeles County`. (`~22%` of all customers)\n>- `1696` customers are from `San Francisco Bay Area` (`~33%` of all customers)\n>- `480` customers had `borrowed loan` before (only `9%` of all the customers)\n>- `89%` customers do not have Security Account\n>- `93%` customers do not have CD Account\n>- `70%` customers do not use a credit card issued by any other banks","6b17143d":"#### Check Model Score","e93b90bb":"> Our pre-pruned Decision Tree at Depth = 7 appears to be slightly better than this model since the test accuracy (98.8%) and test recall (94.3%) were better than this one","e3c7d740":"#### Mortgage by Family size boxplot for each Personal Loan category","2b379521":"### Revisiting the Confusion Matrix using Decision Tree with Post-Pruning","88518f4d":"#### Securities Account (Yes\/No) by Personal Loan (borrowed or not)","03a751b0":"##### ROC - AUC","b1d243bf":">- Most important features appear to be `CD Account`, `Education`, `Family Size`, `Spending\/CC Avg`, `Income`\n>\n>***Interpretation of coeffecients, probability and odds***:\n>\n>- Coefficient of `Income`, `Graduate and Advanced Education`, `Family`, `CCavg`, `CD account` are positive , hence, one unit increase in these feature values would increase the probability of a customer borrowing personal loan.\n>- Coefficient of `Securities account`, `online`, `credit card` are negative. Hence, increase in these will lead to decrease in chances of a person borrowing a personal loan.\n>- Having a `CD Account` increases the odds of borrowing `Personal Loan` to `12` times, or `92%` chance\n>- Having advanced `Education` increases odds of borrowing `Personal Loan` to `9` times, or `90%` chance\n>- Unit change in `Family Size` increases odds of borrowing `Personal Loan` to `1.7` times, or `63%` chance","2530e786":"#### Income Bucket by Personal Loan (borrowed or not)","aaba9657":"#### Credit Card (With\/Not with AllLife Bank) by Personal Loan (borrowed or not)","985cdea9":"### Numerical column statistics","40b3344f":"##### Model Scores of Logistic Regression Model 2","7d0699f7":"### Decision Tree : Reducing the over-fitting with Pre-pruning","fb5eea28":"#### Build the model","54312128":">Since our goal is to increase the recall score, we'll use the above observation to determine optimal `ccp_alpha`","ab4ab32a":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Background-and-Context\" data-toc-modified-id=\"Background-and-Context-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Background and Context<\/a><\/span><\/li><li><span><a href=\"#Objective\" data-toc-modified-id=\"Objective-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Objective<\/a><\/span><\/li><li><span><a href=\"#Data-Dictionary\" data-toc-modified-id=\"Data-Dictionary-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Data Dictionary<\/a><\/span><\/li><li><span><a href=\"#Initial-set-up\" data-toc-modified-id=\"Initial-set-up-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Initial set-up<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Installing-US-Zip-Codes-library\" data-toc-modified-id=\"Installing-US-Zip-Codes-library-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;<\/span>Installing US Zip Codes library<\/a><\/span><\/li><li><span><a href=\"#Import-required-libraries\" data-toc-modified-id=\"Import-required-libraries-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;<\/span>Import required libraries<\/a><\/span><\/li><li><span><a href=\"#Read-the-data\" data-toc-modified-id=\"Read-the-data-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;<\/span>Read the data<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Understanding-the-data\" data-toc-modified-id=\"Understanding-the-data-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Understanding the data<\/a><\/span><\/li><li><span><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>Data Preprocessing<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Data-Type-Conversions\" data-toc-modified-id=\"Data-Type-Conversions-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;<\/span>Data Type Conversions<\/a><\/span><\/li><li><span><a href=\"#Dropping-ID-column\" data-toc-modified-id=\"Dropping-ID-column-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;<\/span>Dropping ID column<\/a><\/span><\/li><li><span><a href=\"#Handling-negative-values-in-Experience\" data-toc-modified-id=\"Handling-negative-values-in-Experience-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;<\/span>Handling negative values in Experience<\/a><\/span><\/li><li><span><a href=\"#Standardizing-column-names\" data-toc-modified-id=\"Standardizing-column-names-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;<\/span>Standardizing column names<\/a><\/span><\/li><li><span><a href=\"#Create-bins-for-Age,-Income-and-Credit-Card-Expenditure-Average\" data-toc-modified-id=\"Create-bins-for-Age,-Income-and-Credit-Card-Expenditure-Average-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;<\/span>Create bins for Age, Income and Credit Card Expenditure Average<\/a><\/span><\/li><li><span><a href=\"#Deriving-Regions-from-Zip-Codes\" data-toc-modified-id=\"Deriving-Regions-from-Zip-Codes-6.6\"><span class=\"toc-item-num\">6.6&nbsp;&nbsp;<\/span>Deriving Regions from Zip Codes<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Exploratory-Data-Analysis\" data-toc-modified-id=\"Exploratory-Data-Analysis-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;<\/span>Exploratory Data Analysis<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Numerical-column-statistics\" data-toc-modified-id=\"Numerical-column-statistics-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;<\/span>Numerical column statistics<\/a><\/span><\/li><li><span><a href=\"#Categorical-column-statistics\" data-toc-modified-id=\"Categorical-column-statistics-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;<\/span>Categorical column statistics<\/a><\/span><\/li><li><span><a href=\"#Univariate-Analysis\" data-toc-modified-id=\"Univariate-Analysis-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;<\/span>Univariate Analysis<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Age:\" data-toc-modified-id=\"Age:-7.3.1\"><span class=\"toc-item-num\">7.3.1&nbsp;&nbsp;<\/span>Age:<\/a><\/span><\/li><li><span><a href=\"#Experience\" data-toc-modified-id=\"Experience-7.3.2\"><span class=\"toc-item-num\">7.3.2&nbsp;&nbsp;<\/span>Experience<\/a><\/span><\/li><li><span><a href=\"#Income\" data-toc-modified-id=\"Income-7.3.3\"><span class=\"toc-item-num\">7.3.3&nbsp;&nbsp;<\/span>Income<\/a><\/span><\/li><li><span><a href=\"#CC-Average\" data-toc-modified-id=\"CC-Average-7.3.4\"><span class=\"toc-item-num\">7.3.4&nbsp;&nbsp;<\/span>CC Average<\/a><\/span><\/li><li><span><a href=\"#Mortgage\" data-toc-modified-id=\"Mortgage-7.3.5\"><span class=\"toc-item-num\">7.3.5&nbsp;&nbsp;<\/span>Mortgage<\/a><\/span><\/li><li><span><a href=\"#Personal-Loan-categorical-percentage-distribution\" data-toc-modified-id=\"Personal-Loan-categorical-percentage-distribution-7.3.6\"><span class=\"toc-item-num\">7.3.6&nbsp;&nbsp;<\/span>Personal Loan categorical percentage distribution<\/a><\/span><\/li><li><span><a href=\"#Percentage-on-bar-chart-for-Categorical-Features\" data-toc-modified-id=\"Percentage-on-bar-chart-for-Categorical-Features-7.3.7\"><span class=\"toc-item-num\">7.3.7&nbsp;&nbsp;<\/span>Percentage on bar chart for Categorical Features<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Bi-variate-Analysis\" data-toc-modified-id=\"Bi-variate-Analysis-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;<\/span>Bi-variate Analysis<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Personal-Loan-vs.-All-numerical-columns\" data-toc-modified-id=\"Personal-Loan-vs.-All-numerical-columns-7.4.1\"><span class=\"toc-item-num\">7.4.1&nbsp;&nbsp;<\/span>Personal Loan vs. All numerical columns<\/a><\/span><\/li><li><span><a href=\"#Education-by-Personal-Loan-(borrowed-or-not)\" data-toc-modified-id=\"Education-by-Personal-Loan-(borrowed-or-not)-7.4.2\"><span class=\"toc-item-num\">7.4.2&nbsp;&nbsp;<\/span>Education by Personal Loan (borrowed or not)<\/a><\/span><\/li><li><span><a href=\"#Family-by-Personal-Loan-(borrowed-or-not)\" data-toc-modified-id=\"Family-by-Personal-Loan-(borrowed-or-not)-7.4.3\"><span class=\"toc-item-num\">7.4.3&nbsp;&nbsp;<\/span>Family by Personal Loan (borrowed or not)<\/a><\/span><\/li><li><span><a href=\"#Securities-Account-(Yes\/No)-by-Personal-Loan-(borrowed-or-not)\" data-toc-modified-id=\"Securities-Account-(Yes\/No)-by-Personal-Loan-(borrowed-or-not)-7.4.4\"><span class=\"toc-item-num\">7.4.4&nbsp;&nbsp;<\/span>Securities Account (Yes\/No) by Personal Loan (borrowed or not)<\/a><\/span><\/li><li><span><a href=\"#Online-Banking-(Yes\/No)-by-Personal-Loan-(borrowed-or-not)\" data-toc-modified-id=\"Online-Banking-(Yes\/No)-by-Personal-Loan-(borrowed-or-not)-7.4.5\"><span class=\"toc-item-num\">7.4.5&nbsp;&nbsp;<\/span>Online Banking (Yes\/No) by Personal Loan (borrowed or not)<\/a><\/span><\/li><li><span><a href=\"#CD-Account-(Yes\/No)-by-Personal-Loan-(borrowed-or-not)\" data-toc-modified-id=\"CD-Account-(Yes\/No)-by-Personal-Loan-(borrowed-or-not)-7.4.6\"><span class=\"toc-item-num\">7.4.6&nbsp;&nbsp;<\/span>CD Account (Yes\/No) by Personal Loan (borrowed or not)<\/a><\/span><\/li><li><span><a href=\"#Credit-Card-(With\/Not-with-AllLife-Bank)-by-Personal-Loan-(borrowed-or-not)\" data-toc-modified-id=\"Credit-Card-(With\/Not-with-AllLife-Bank)-by-Personal-Loan-(borrowed-or-not)-7.4.7\"><span class=\"toc-item-num\">7.4.7&nbsp;&nbsp;<\/span>Credit Card (With\/Not with AllLife Bank) by Personal Loan (borrowed or not)<\/a><\/span><\/li><li><span><a href=\"#Age-Bucket-by-Personal-Loan-(borrowed-or-not)\" data-toc-modified-id=\"Age-Bucket-by-Personal-Loan-(borrowed-or-not)-7.4.8\"><span class=\"toc-item-num\">7.4.8&nbsp;&nbsp;<\/span>Age Bucket by Personal Loan (borrowed or not)<\/a><\/span><\/li><li><span><a href=\"#Income-Bucket-by-Personal-Loan-(borrowed-or-not)\" data-toc-modified-id=\"Income-Bucket-by-Personal-Loan-(borrowed-or-not)-7.4.9\"><span class=\"toc-item-num\">7.4.9&nbsp;&nbsp;<\/span>Income Bucket by Personal Loan (borrowed or not)<\/a><\/span><\/li><li><span><a href=\"#CC-Spending-Bucket-by-Personal-Loan-(borrowed-or-not)\" data-toc-modified-id=\"CC-Spending-Bucket-by-Personal-Loan-(borrowed-or-not)-7.4.10\"><span class=\"toc-item-num\">7.4.10&nbsp;&nbsp;<\/span>CC Spending Bucket by Personal Loan (borrowed or not)<\/a><\/span><\/li><li><span><a href=\"#Region-wise-customer-distribution-by-Personal-Loan-(Borrowed-or-not)\" data-toc-modified-id=\"Region-wise-customer-distribution-by-Personal-Loan-(Borrowed-or-not)-7.4.11\"><span class=\"toc-item-num\">7.4.11&nbsp;&nbsp;<\/span>Region wise customer distribution by Personal Loan (Borrowed or not)<\/a><\/span><\/li><li><span><a href=\"#Education-Level-and-Income-wise-customer-distribution,-by-Personal-Loan,-opted-or-not\" data-toc-modified-id=\"Education-Level-and-Income-wise-customer-distribution,-by-Personal-Loan,-opted-or-not-7.4.12\"><span class=\"toc-item-num\">7.4.12&nbsp;&nbsp;<\/span>Education Level and Income-wise customer distribution, by Personal Loan, opted or not<\/a><\/span><\/li><li><span><a href=\"#Age-bucket-and-Mortgage-wise-customer-distribution,-by-Personal-Loan,-opted-or-not\" data-toc-modified-id=\"Age-bucket-and-Mortgage-wise-customer-distribution,-by-Personal-Loan,-opted-or-not-7.4.13\"><span class=\"toc-item-num\">7.4.13&nbsp;&nbsp;<\/span>Age bucket and Mortgage-wise customer distribution, by Personal Loan, opted or not<\/a><\/span><\/li><li><span><a href=\"#CC-Spending-buckets-and-Income-wise-customer-distribution,-by-Personal-Loan,-opted-or-not\" data-toc-modified-id=\"CC-Spending-buckets-and-Income-wise-customer-distribution,-by-Personal-Loan,-opted-or-not-7.4.14\"><span class=\"toc-item-num\">7.4.14&nbsp;&nbsp;<\/span>CC Spending buckets and Income-wise customer distribution, by Personal Loan, opted or not<\/a><\/span><\/li><li><span><a href=\"#Jointplot-between-Income-and-Mortgage\/CC-Avg,-by-Personal-Loan,-opted-or-not\" data-toc-modified-id=\"Jointplot-between-Income-and-Mortgage\/CC-Avg,-by-Personal-Loan,-opted-or-not-7.4.15\"><span class=\"toc-item-num\">7.4.15&nbsp;&nbsp;<\/span>Jointplot between Income and Mortgage\/CC Avg, by Personal Loan, opted or not<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Multi-variate-Plots\" data-toc-modified-id=\"Multi-variate-Plots-7.5\"><span class=\"toc-item-num\">7.5&nbsp;&nbsp;<\/span>Multi-variate Plots<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Pairplot-of-all-available-numeric-columns,-hued-by-Personal-Loan\" data-toc-modified-id=\"Pairplot-of-all-available-numeric-columns,-hued-by-Personal-Loan-7.5.1\"><span class=\"toc-item-num\">7.5.1&nbsp;&nbsp;<\/span>Pairplot of all available numeric columns, hued by Personal Loan<\/a><\/span><\/li><li><span><a href=\"#Heatmap-to-understand-correlations-between-independent-and-dependent-variables\" data-toc-modified-id=\"Heatmap-to-understand-correlations-between-independent-and-dependent-variables-7.5.2\"><span class=\"toc-item-num\">7.5.2&nbsp;&nbsp;<\/span>Heatmap to understand correlations between independent and dependent variables<\/a><\/span><\/li><li><span><a href=\"#Income-by-Education-boxplot-for-each-Personal-Loan-category\" data-toc-modified-id=\"Income-by-Education-boxplot-for-each-Personal-Loan-category-7.5.3\"><span class=\"toc-item-num\">7.5.3&nbsp;&nbsp;<\/span>Income by Education boxplot for each Personal Loan category<\/a><\/span><\/li><li><span><a href=\"#Income-by-Family-size-boxplot-for-each-Personal-Loan-category\" data-toc-modified-id=\"Income-by-Family-size-boxplot-for-each-Personal-Loan-category-7.5.4\"><span class=\"toc-item-num\">7.5.4&nbsp;&nbsp;<\/span>Income by Family size boxplot for each Personal Loan category<\/a><\/span><\/li><li><span><a href=\"#Mortgage-by-Family-size-boxplot-for-each-Personal-Loan-category\" data-toc-modified-id=\"Mortgage-by-Family-size-boxplot-for-each-Personal-Loan-category-7.5.5\"><span class=\"toc-item-num\">7.5.5&nbsp;&nbsp;<\/span>Mortgage by Family size boxplot for each Personal Loan category<\/a><\/span><\/li><li><span><a href=\"#CC-Avg-by-Credit-Card-boxplot-for-each-Personal-Loan-category\" data-toc-modified-id=\"CC-Avg-by-Credit-Card-boxplot-for-each-Personal-Loan-category-7.5.6\"><span class=\"toc-item-num\">7.5.6&nbsp;&nbsp;<\/span>CC Avg by Credit Card boxplot for each Personal Loan category<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#insights-based-on-EDA\" data-toc-modified-id=\"insights-based-on-EDA-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;<\/span>insights based on EDA<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Customer-Profile\" data-toc-modified-id=\"Customer-Profile-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;<\/span><strong>Customer Profile<\/strong><\/a><\/span><\/li><li><span><a href=\"#Observations-on-Patterns\" data-toc-modified-id=\"Observations-on-Patterns-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;<\/span><strong>Observations on Patterns<\/strong><\/a><\/span><\/li><li><span><a href=\"#General-Observations-on-Dataset\" data-toc-modified-id=\"General-Observations-on-Dataset-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;<\/span><strong>General Observations on Dataset<\/strong><\/a><\/span><\/li><li><span><a href=\"#Outliers\" data-toc-modified-id=\"Outliers-8.4\"><span class=\"toc-item-num\">8.4&nbsp;&nbsp;<\/span><strong>Outliers<\/strong><\/a><\/span><\/li><li><span><a href=\"#Columns-to-be-dropped\" data-toc-modified-id=\"Columns-to-be-dropped-8.5\"><span class=\"toc-item-num\">8.5&nbsp;&nbsp;<\/span><strong>Columns to be dropped<\/strong><\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Data-Preparation\" data-toc-modified-id=\"Data-Preparation-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;<\/span>Data Preparation<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Dropping-columns\" data-toc-modified-id=\"Dropping-columns-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;<\/span>Dropping columns<\/a><\/span><\/li><li><span><a href=\"#Outliers-detection-using-Box-plot\" data-toc-modified-id=\"Outliers-detection-using-Box-plot-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;<\/span>Outliers detection using Box plot<\/a><\/span><\/li><li><span><a href=\"#Treating-Outliers\" data-toc-modified-id=\"Treating-Outliers-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;<\/span>Treating Outliers<\/a><\/span><\/li><li><span><a href=\"#Creating-training-and-testing-datasets\" data-toc-modified-id=\"Creating-training-and-testing-datasets-9.4\"><span class=\"toc-item-num\">9.4&nbsp;&nbsp;<\/span>Creating training and testing datasets<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Model-Building\" data-toc-modified-id=\"Model-Building-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;<\/span>Model Building<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Model-Evaluation-Criteria\" data-toc-modified-id=\"Model-Evaluation-Criteria-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;<\/span>Model Evaluation Criteria<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Model-can-make-wrong-predictions-as:\" data-toc-modified-id=\"Model-can-make-wrong-predictions-as:-10.1.1\"><span class=\"toc-item-num\">10.1.1&nbsp;&nbsp;<\/span>Model can make wrong predictions as:<\/a><\/span><\/li><li><span><a href=\"#Which-case-is-more-important?\" data-toc-modified-id=\"Which-case-is-more-important?-10.1.2\"><span class=\"toc-item-num\">10.1.2&nbsp;&nbsp;<\/span>Which case is more important?<\/a><\/span><\/li><li><span><a href=\"#How-to-reduce-this-loss-i.e-need-to-reduce-False-Negatives?\" data-toc-modified-id=\"How-to-reduce-this-loss-i.e-need-to-reduce-False-Negatives?-10.1.3\"><span class=\"toc-item-num\">10.1.3&nbsp;&nbsp;<\/span>How to reduce this loss i.e need to reduce False Negatives?<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Model-Evaluation-Functions---Scoring-&amp;-Confusion-Matrix\" data-toc-modified-id=\"Model-Evaluation-Functions---Scoring-&amp;-Confusion-Matrix-10.2\"><span class=\"toc-item-num\">10.2&nbsp;&nbsp;<\/span>Model Evaluation Functions - Scoring &amp; Confusion Matrix<\/a><\/span><\/li><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-10.3\"><span class=\"toc-item-num\">10.3&nbsp;&nbsp;<\/span>Logistic Regression<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Build-Model\" data-toc-modified-id=\"Build-Model-10.3.1\"><span class=\"toc-item-num\">10.3.1&nbsp;&nbsp;<\/span>Build Model<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Model-Score\" data-toc-modified-id=\"Model-Score-10.3.1.1\"><span class=\"toc-item-num\">10.3.1.1&nbsp;&nbsp;<\/span>Model Score<\/a><\/span><\/li><li><span><a href=\"#ROC---AUC\" data-toc-modified-id=\"ROC---AUC-10.3.1.2\"><span class=\"toc-item-num\">10.3.1.2&nbsp;&nbsp;<\/span>ROC - AUC<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Build-Model-with-Class-Weight\" data-toc-modified-id=\"Build-Model-with-Class-Weight-10.3.2\"><span class=\"toc-item-num\">10.3.2&nbsp;&nbsp;<\/span>Build Model with Class Weight<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Model-Scores-of-Logistic-Regression-Model-2\" data-toc-modified-id=\"Model-Scores-of-Logistic-Regression-Model-2-10.3.2.1\"><span class=\"toc-item-num\">10.3.2.1&nbsp;&nbsp;<\/span>Model Scores of Logistic Regression Model 2<\/a><\/span><\/li><li><span><a href=\"#ROC---AUC-on-LR-Model-2\" data-toc-modified-id=\"ROC---AUC-on-LR-Model-2-10.3.2.2\"><span class=\"toc-item-num\">10.3.2.2&nbsp;&nbsp;<\/span>ROC - AUC on LR Model 2<\/a><\/span><\/li><li><span><a href=\"#Coefficients-and-Odds\" data-toc-modified-id=\"Coefficients-and-Odds-10.3.2.3\"><span class=\"toc-item-num\">10.3.2.3&nbsp;&nbsp;<\/span>Coefficients and Odds<\/a><\/span><\/li><li><span><a href=\"#Trying-to-find-Optimal-AUR-ROC-threshold-to-check-model-performance-improvement\" data-toc-modified-id=\"Trying-to-find-Optimal-AUR-ROC-threshold-to-check-model-performance-improvement-10.3.2.4\"><span class=\"toc-item-num\">10.3.2.4&nbsp;&nbsp;<\/span>Trying to find Optimal AUR ROC threshold to check model performance improvement<\/a><\/span><\/li><li><span><a href=\"#Trying-to-find-intersect-point-of-Precision-and-Recall-curve-to-find-threshold-point-for-model-improvement\" data-toc-modified-id=\"Trying-to-find-intersect-point-of-Precision-and-Recall-curve-to-find-threshold-point-for-model-improvement-10.3.2.5\"><span class=\"toc-item-num\">10.3.2.5&nbsp;&nbsp;<\/span>Trying to find intersect point of Precision and Recall curve to find threshold point for model improvement<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Using-Sequential-Feature-Selection\" data-toc-modified-id=\"Using-Sequential-Feature-Selection-10.3.3\"><span class=\"toc-item-num\">10.3.3&nbsp;&nbsp;<\/span>Using Sequential Feature Selection<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Model-Score\" data-toc-modified-id=\"Model-Score-10.3.3.1\"><span class=\"toc-item-num\">10.3.3.1&nbsp;&nbsp;<\/span>Model Score<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Model-with-SFS-and-Class-Weight\" data-toc-modified-id=\"Model-with-SFS-and-Class-Weight-10.3.4\"><span class=\"toc-item-num\">10.3.4&nbsp;&nbsp;<\/span>Model with SFS and Class Weight<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Model-Score\" data-toc-modified-id=\"Model-Score-10.3.4.1\"><span class=\"toc-item-num\">10.3.4.1&nbsp;&nbsp;<\/span>Model Score<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Logistic-Regression-Model-Score-Comparison\" data-toc-modified-id=\"Logistic-Regression-Model-Score-Comparison-10.3.5\"><span class=\"toc-item-num\">10.3.5&nbsp;&nbsp;<\/span>Logistic Regression Model Score Comparison<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Decision-Tree\" data-toc-modified-id=\"Decision-Tree-10.4\"><span class=\"toc-item-num\">10.4&nbsp;&nbsp;<\/span>Decision Tree<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Build-Tree\" data-toc-modified-id=\"Build-Tree-10.4.1\"><span class=\"toc-item-num\">10.4.1&nbsp;&nbsp;<\/span>Build Tree<\/a><\/span><\/li><li><span><a href=\"#Score-the-Tree\" data-toc-modified-id=\"Score-the-Tree-10.4.2\"><span class=\"toc-item-num\">10.4.2&nbsp;&nbsp;<\/span>Score the Tree<\/a><\/span><\/li><li><span><a href=\"#Create-function-to-get-Recall-Score-only\" data-toc-modified-id=\"Create-function-to-get-Recall-Score-only-10.4.3\"><span class=\"toc-item-num\">10.4.3&nbsp;&nbsp;<\/span>Create function to get Recall Score only<\/a><\/span><\/li><li><span><a href=\"#Check-Recall-Score\" data-toc-modified-id=\"Check-Recall-Score-10.4.4\"><span class=\"toc-item-num\">10.4.4&nbsp;&nbsp;<\/span>Check Recall Score<\/a><\/span><\/li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-10.4.5\"><span class=\"toc-item-num\">10.4.5&nbsp;&nbsp;<\/span>Confusion Matrix<\/a><\/span><\/li><li><span><a href=\"#Visualize-the-Decision-Tree\" data-toc-modified-id=\"Visualize-the-Decision-Tree-10.4.6\"><span class=\"toc-item-num\">10.4.6&nbsp;&nbsp;<\/span>Visualize the Decision Tree<\/a><\/span><\/li><li><span><a href=\"#Feature-Importance\" data-toc-modified-id=\"Feature-Importance-10.4.7\"><span class=\"toc-item-num\">10.4.7&nbsp;&nbsp;<\/span>Feature Importance<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Decision-Tree-:-Reducing-the-over-fitting-with-Pre-pruning\" data-toc-modified-id=\"Decision-Tree-:-Reducing-the-over-fitting-with-Pre-pruning-10.5\"><span class=\"toc-item-num\">10.5&nbsp;&nbsp;<\/span>Decision Tree : Reducing the over-fitting with Pre-pruning<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Limiting-the-maximum-depth-to-7\" data-toc-modified-id=\"Limiting-the-maximum-depth-to-7-10.5.1\"><span class=\"toc-item-num\">10.5.1&nbsp;&nbsp;<\/span>Limiting the maximum depth to 7<\/a><\/span><\/li><li><span><a href=\"#Check-Model-Score\" data-toc-modified-id=\"Check-Model-Score-10.5.2\"><span class=\"toc-item-num\">10.5.2&nbsp;&nbsp;<\/span>Check Model Score<\/a><\/span><\/li><li><span><a href=\"#Removing-class-weight\" data-toc-modified-id=\"Removing-class-weight-10.5.3\"><span class=\"toc-item-num\">10.5.3&nbsp;&nbsp;<\/span>Removing class weight<\/a><\/span><\/li><li><span><a href=\"#Check-Model-Score-after-removing-class-weight\" data-toc-modified-id=\"Check-Model-Score-after-removing-class-weight-10.5.4\"><span class=\"toc-item-num\">10.5.4&nbsp;&nbsp;<\/span>Check Model Score after removing class weight<\/a><\/span><\/li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-10.5.5\"><span class=\"toc-item-num\">10.5.5&nbsp;&nbsp;<\/span>Confusion Matrix<\/a><\/span><\/li><li><span><a href=\"#Visualizing-the-Decision-Tree\" data-toc-modified-id=\"Visualizing-the-Decision-Tree-10.5.6\"><span class=\"toc-item-num\">10.5.6&nbsp;&nbsp;<\/span>Visualizing the Decision Tree<\/a><\/span><\/li><li><span><a href=\"#Feature-Importance\" data-toc-modified-id=\"Feature-Importance-10.5.7\"><span class=\"toc-item-num\">10.5.7&nbsp;&nbsp;<\/span>Feature Importance<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#GridSearch-for-hyperparameter-tuning\" data-toc-modified-id=\"GridSearch-for-hyperparameter-tuning-10.6\"><span class=\"toc-item-num\">10.6&nbsp;&nbsp;<\/span>GridSearch for hyperparameter tuning<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Check-Model-Score\" data-toc-modified-id=\"Check-Model-Score-10.6.1\"><span class=\"toc-item-num\">10.6.1&nbsp;&nbsp;<\/span>Check Model Score<\/a><\/span><\/li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-10.6.2\"><span class=\"toc-item-num\">10.6.2&nbsp;&nbsp;<\/span>Confusion Matrix<\/a><\/span><\/li><li><span><a href=\"#Visualizing-the-Decision-Tree\" data-toc-modified-id=\"Visualizing-the-Decision-Tree-10.6.3\"><span class=\"toc-item-num\">10.6.3&nbsp;&nbsp;<\/span>Visualizing the Decision Tree<\/a><\/span><\/li><li><span><a href=\"#Feature-Importance\" data-toc-modified-id=\"Feature-Importance-10.6.4\"><span class=\"toc-item-num\">10.6.4&nbsp;&nbsp;<\/span>Feature Importance<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Decision-Tree-:-Cost-Complexity-Pruning\" data-toc-modified-id=\"Decision-Tree-:-Cost-Complexity-Pruning-10.7\"><span class=\"toc-item-num\">10.7&nbsp;&nbsp;<\/span>Decision Tree : Cost Complexity Pruning<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Build-the-model\" data-toc-modified-id=\"Build-the-model-10.7.1\"><span class=\"toc-item-num\">10.7.1&nbsp;&nbsp;<\/span>Build the model<\/a><\/span><\/li><li><span><a href=\"#Check-Model-Score\" data-toc-modified-id=\"Check-Model-Score-10.7.2\"><span class=\"toc-item-num\">10.7.2&nbsp;&nbsp;<\/span>Check Model Score<\/a><\/span><\/li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-10.7.3\"><span class=\"toc-item-num\">10.7.3&nbsp;&nbsp;<\/span>Confusion Matrix<\/a><\/span><\/li><li><span><a href=\"#Visualizing-the-Decision-Tree\" data-toc-modified-id=\"Visualizing-the-Decision-Tree-10.7.4\"><span class=\"toc-item-num\">10.7.4&nbsp;&nbsp;<\/span>Visualizing the Decision Tree<\/a><\/span><\/li><li><span><a href=\"#Feature-Importance\" data-toc-modified-id=\"Feature-Importance-10.7.5\"><span class=\"toc-item-num\">10.7.5&nbsp;&nbsp;<\/span>Feature Importance<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Decision-Tree:-CCP-with-Class-Weight\" data-toc-modified-id=\"Decision-Tree:-CCP-with-Class-Weight-10.8\"><span class=\"toc-item-num\">10.8&nbsp;&nbsp;<\/span>Decision Tree: CCP with Class Weight<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Build-the-model\" data-toc-modified-id=\"Build-the-model-10.8.1\"><span class=\"toc-item-num\">10.8.1&nbsp;&nbsp;<\/span>Build the model<\/a><\/span><\/li><li><span><a href=\"#Check-Model-Score\" data-toc-modified-id=\"Check-Model-Score-10.8.2\"><span class=\"toc-item-num\">10.8.2&nbsp;&nbsp;<\/span>Check Model Score<\/a><\/span><\/li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-10.8.3\"><span class=\"toc-item-num\">10.8.3&nbsp;&nbsp;<\/span>Confusion Matrix<\/a><\/span><\/li><li><span><a href=\"#Visualizing-the-Decision-Tree\" data-toc-modified-id=\"Visualizing-the-Decision-Tree-10.8.4\"><span class=\"toc-item-num\">10.8.4&nbsp;&nbsp;<\/span>Visualizing the Decision Tree<\/a><\/span><\/li><li><span><a href=\"#Decision-Tree-model-comparison\" data-toc-modified-id=\"Decision-Tree-model-comparison-10.8.5\"><span class=\"toc-item-num\">10.8.5&nbsp;&nbsp;<\/span>Decision Tree model comparison<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;<\/span>Conclusion<\/a><\/span><\/li><li><span><a href=\"#False-Positive-and-False-Negative-Analysis\" data-toc-modified-id=\"False-Positive-and-False-Negative-Analysis-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;<\/span>False Positive and False Negative Analysis<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Revisiting-the-Confusion-Matrix-using-Decision-Tree-with-Post-Pruning\" data-toc-modified-id=\"Revisiting-the-Confusion-Matrix-using-Decision-Tree-with-Post-Pruning-12.1\"><span class=\"toc-item-num\">12.1&nbsp;&nbsp;<\/span>Revisiting the Confusion Matrix using Decision Tree with Post-Pruning<\/a><\/span><\/li><li><span><a href=\"#Check-the-data-where-classification-is-incorrect\" data-toc-modified-id=\"Check-the-data-where-classification-is-incorrect-12.2\"><span class=\"toc-item-num\">12.2&nbsp;&nbsp;<\/span>Check the data where classification is incorrect<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#False-Positives\" data-toc-modified-id=\"False-Positives-12.2.1\"><span class=\"toc-item-num\">12.2.1&nbsp;&nbsp;<\/span>False Positives<\/a><\/span><\/li><li><span><a href=\"#False-Negatives\" data-toc-modified-id=\"False-Negatives-12.2.2\"><span class=\"toc-item-num\">12.2.2&nbsp;&nbsp;<\/span>False Negatives<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#Exploratory-Data-Analysis-on-the-Incorrectly-Predicted-Data\" data-toc-modified-id=\"Exploratory-Data-Analysis-on-the-Incorrectly-Predicted-Data-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;<\/span>Exploratory Data Analysis on the Incorrectly Predicted Data<\/a><\/span><\/li><li><span><a href=\"#Actionable-Insights-&amp;-Recommendations\" data-toc-modified-id=\"Actionable-Insights-&amp;-Recommendations-14\"><span class=\"toc-item-num\">14&nbsp;&nbsp;<\/span>Actionable Insights &amp; Recommendations<\/a><\/span><\/li><\/ul><\/div>","25cf3984":"\n\n## Background and Context\n\nAllLife Bank is a US bank that has a growing customer base. The majority of these customers are liability customers (depositors) with varying sizes of deposits. The number of customers who are also borrowers (asset customers) is quite small, and the bank is interested in expanding this base rapidly to bring in more loan business and in the process, earn more through the interest on loans. In particular, the management wants to explore ways of converting its liability customers to personal loan customers (while retaining them as depositors).\n\nA campaign that the bank ran last year for liability customers showed a healthy conversion rate of over 9% success. This has encouraged the retail marketing department to devise campaigns with better target marketing to increase the success ratio.\n\nWe have to build a model that will help the marketing department to identify the potential customers who have a higher probability of purchasing the loan.\n\n## Objective\n\n- To predict whether a liability customer will buy a personal loan or not.\n- Which variables are most significant.\n- Which segment of customers should be targeted more.\n\n## Data Dictionary\n\n* `ID`: Customer ID\n* `Age`: Customer\u2019s age in completed years\n* `Experience`: #years of professional experience\n* `Income`: Annual income of the customer (in thousand dollars)\n* `ZIP Code`: Home Address ZIP code. \n* `Family`: the Family size of the customer\n* `CCAvg`: Average spending on credit cards per month (in thousand dollars)\n* `Education`: Education Level. 1: Undergrad; 2: Graduate;3: Advanced\/Professional\n* `Mortgage`: Value of house mortgage if any. (in thousand dollars)\n* `Personal_Loan`: Did this customer accept the personal loan offered in the last campaign?\n* `Securities_Account`: Does the customer have securities account with the bank?\n* `CD_Account`: Does the customer have a certificate of deposit (CD) account with the bank?\n* `Online`: Do customers use internet banking facilities?\n* `CreditCard`: Does the customer use a credit card issued by any other Bank (excluding All life Bank)?\n\n**Important note**: We need **uszipcode** package to get higher granularity from US zip codes.","fa03e3d9":"`ID` column is not required. Hence, dropping it from the dataset.","2b962572":">**Observation**:\n>- Customers with `higher Income`, `higher Mortgage`, and `higher Credit Card expenditure` tend to borrow `Personal Loan`\n>- Customers with `low Income`, `low Mortgage`, and `low Credit Card expenditure` tend to not borrow `Personal Loan` at all\n>- Customers with `medium Income`, `medium Mortgage`, and `medium Credit Card expenditure` have a mixed tendency of borrowing (or not borrowing) `Personal Loan`\n>- These three features are helpful to categorize customers in `High, Mid and Low profile`","eeee0960":"## Data Preprocessing","913f0faa":">**Observation**:\n>- Customers with `higher income and higher expenditure` tend to borrow `personal loan`","0110aacc":"#### Confusion Matrix","69647a49":">`There are no NULL values`","5004cda0":"### **Observations on Patterns**\n\n- People having `higher income` have taken `personal loan`\n- People with `2 - 4 family members` are likelier to take `personal loan`\n- People with high `mortgages` opted for `loan`.\n- People with `higher credit card average` opted for `personal loan`\n- People with `higher mortgage` opted for `personal loan`\n- Number of Customers with `Advanced\/Professional` education level has borrowed personal loan more than `Graduated and Under-grads`\n- Number of Customers with `Family Size 3 or More` has borrowed personal loan more than other people\n- `60` of those who had `Personal loan` with the bank also had `Securities Account`.\n- Almost 50% of customers having `Certified Deposit`, had borrowed `Personal Loan`.\n- However, `4358` customers out of `5000`, do not have `Certified Deposit Account` and did not borrow `Personal Loan`, which means if a customer does not have a `CD Account`, is likely not to take `Personal Loan`\n- Majority customers who did have `Personal Loan` with the bank did not use `Credit Card` from other banks.\n- Customers from `San Francisco Bay Area` and `Los Angeles Region` mostly took `Personal Loan`.\n- `Age` and `Experience` are highly correlated\n- Majority customers who did have `Personal Loan` with the bank, are `Middle Aged (35 - 55 Years)`\n- Majority customers who did have `Personal Loan` with the bank, are having `High Income (98K - 224k USD)`\n- <span style='background:lightgreen'\/>Important features for determining if a customer would borrow Personal Loan or not, are \n    - <span style='background:lightgreen'\/>Income\n    - <span style='background:lightgreen'\/>Family Size\n    - <span style='background:lightgreen'\/>Education\n    - <span style='background:lightgreen'\/>CD Account\n    - <span style='background:lightgreen'\/>Region","4338439c":">Internet search shows `92717` and `92634` zip codes are in`Orange County`. The remaining two appears to be in `Germany`. However, we'll fill in the regions with respect to the nearby zip codes.","aa9cf3b1":"Let's now check how the accuracy of the model changes with increasing `ccp_alpha` values for train and test datasets","cdcf09f3":"#### Feature Importance","cfbc5a3f":"#### Personal Loan vs. All numerical columns","a2587ccb":"#### Online Banking (Yes\/No) by Personal Loan (borrowed or not)","23d8d1d5":"- The goal of the marketing team or sales team should be not to miss an opportunity, meaning, there should be minimal chance of predicting a customer would not purchase a personal loan, actually would have borrowed one. This is achieved by minimizing false negatives of our prediction.\n\n\n- First we built model using Logistic Regression. The most important features according to Logistic Regression classification were `Income`, `Education`, `CD account`, `Family` and `CCAvg`\/`Spending`.\n\n\n- We measured the performance, and tried to improve it using `class weight`, `forward feature selection`, and `ROC-AUC threshold values`. However, the model with `class weight` of `0: 0.91, 1:0.09` provided best performance with respect to `recall` and `accuracy`.\n\n\n- We used decision trees with pre-pruning and post-pruning. The `Post Pruning Decision Tree` model gave `96% recall` with `99% accuracy`. Decision Trees are easy to train and understand. Hence, it appears, this is the model we are going to use.\n\n\n- According to decision tree, `Income`, customers with `Graduate or Advanced degree`, customers having higher `Family Size` are some of the most important points for predicting the probability of a customer borrowing `Personal Loan`.\n\n\n- Customer Profile:\n\n\n    - High Profile Customers --> Higher income (98K USD +), Advanced\/Graduate level education, 3\/4 family members, high expenditure\n    \n    \n    - Average Profile Customers --> Medium income (39K - 98K USD), Graduate level education.2\/3 family members, medium expenditure\n    \n    \n    - Low Profile Customers --> Lower income (< 39K USD), Undergraduates ,1\/2 family Member, low expenditure\n    \n    \n\n- According to the profiles, AllLife bank should apply various strategies to sell more Personal Loan packages\n\n\n- Dedicated relationship managers for high profile customers would be a good idea\n\n\n- Monthly\/Quarterly follow-up with average(mid) profile customers should be helpful to attract more customers to take personal loan\n\n\n- As it usually goes, high and mid profile customers should be provided and informed about pre-approved loans","138fcce8":"#### Visualizing the Decision Tree","0a17c24f":"#### Visualizing the Decision Tree","d865ed51":"### Read the data","a58db125":"#### Check Recall Score","50e29af5":">***Logistic Regression model is giving a good performance on training and test set but the recall is bad.***","97e385a0":"> The model performance was better without the `class_weight`. Also, it is overfitting since accuracy and recall on training set are 100%, which is quite higher that what we get for testing dataset!","1a2a0640":"##### Model Score","01cabb84":">**Observation**:\n>- `Age` and `Experience` are highly correlated. We'll drop `Experience` since we had to impute some data there.","c68a0e50":"Removing the spaces from column names, and standardizing the column names to lower case","a443730c":">There appears to be some unmapped `zipcodes`","c4343c4e":"### **Outliers**\n\n- `Income`, `CC Avg`, `Mortgage` columns have right skewed data, with higher outliers, which must be treated","331c90a2":"> There are outliers on higher side for Income, CC Avg and Mortgage features","839145cd":">***Percentage of value predicted by our model are very close to the actual values.***","a1b61cf3":"#### Income by Family size boxplot for each Personal Loan category","9b26de60":">**Observation**\n>- Number of Customers with `Advanced\/Professional` education level has borrowed personal loan more than `Graduated and Under-grads`","724c6c0d":">**Observation**\n>- `CC Avg` is right skewed\n>- `Range of CC Avg` varies from `$0k - $10k`\n>- `Max CC Avg` is quite higher than `Q3`. We'll validate the record.\n>- There are lots of outliers in `CC Avg` on the higher side, which requires to be validated and treated if required.","24f57e25":"### Dropping ID column","e99b01e1":"At the threshold of 0.82, we get balanced recall and precision","ef8ea2ef":"##### Coefficients and Odds","b153a3e1":"Next, we train a decision tree using the effective alphas. The last value in `ccp_alphas` is the alpha value that prunes the whole tree, leaving the tree, clfs[-1], with one node.","230074c5":"Goal of `Bi-variate` analysis is to find inter-dependencies between features.","19e9f79d":"#### Confusion Matrix","88a616e6":"##### Model Score","51be3169":">**Observation**:\n>- Majority customers who did have `Personal Loan` with the bank, are having `High Income (98K - 224k USD)`","28c17071":">**Observation**:\n>- Customers with `higher education and higher income` tend to borrow `personal loan`","9d77b3a8":"We are first creating a copy of the dataset before proceeding with data preparation for modelling","d3ff02f0":"> We are adding a class weight in the Decision Tree model - {0: 0.91, 1:0.09}","897cec5f":"### Treating Outliers","632d0c8a":"## Model Building","c3708714":"- The coefficients of the logistic regression model are in terms of log(odd), to find the odds we have to take the exponential of the coefficients. \n- Therefore, **odds =  exp(b)**\n- The percentage change in odds is given as **odds = (exp(b) - 1) * 100**","52ada293":"#### Check Model Score","21ef6a59":"> The model is better performing without the `class_weight` parameter","66b90b6a":"For the remainder, we remove the last element in `clfs` and `ccp_alphas`, because it is the trivial tree with only one node. Here we show that the number of nodes and tree depth decreases as alpha increases.","fc6e2030":"#### Feature Importance","2dba8661":">**Observation**\n>- Customers from `San Francisco Bay Area` and `Los Angeles Region` mostly took `Personal Loan`.\n>- However, total number of customers is also higher in those regions","3d6199df":"#### Age Bucket by Personal Loan (borrowed or not)","d5834f4f":"#### CD Account (Yes\/No) by Personal Loan (borrowed or not)","1f68c96f":"#### Feature Importance","94ce18ae":"> The tree has become very simple now","60ea2f94":">**Observation**\n>- Customers using `Online` banking or not, is not very impactful to determine if the customer had taken `Personal Loan`","210e1aec":"We will build our model using the `DecisionTreeClassifier` function. Using default `gini` criteria to split. Other option include `entropy`.","3ca760f1":"## False Positive and False Negative Analysis","78f4f8f4":"#### Percentage on bar chart for Categorical Features","a4b09f6b":">**Observation**\n>- `60` of those who had `Personal loan` with the bank also had `Securities Account`.\n>- However, `420` customers having `Security Accounts`, did not borrow a `Personal Loan`.","517e1fdf":"### **General Observations on Dataset**\n\n- Dependent variable is the `Personal_loan` which is of categorical data type\n- `Age`, `Experience`, `Income`, `Mortage`, `CCavg` are of integer\/float type while other variables are of categorical type\n- There were no missing values in the dataset\n- There is no data duplication\n- The dataset contained `467` unique `Zip Codes`, which we mapped to `40 counties` and then to `10 regions`","a8efa042":"#### Family by Personal Loan (borrowed or not)","427d5f75":">**Observation**\n>- Only `9.6%` of all customers, have borrowed `Personal Loan` from the bank.","9437cc21":">**Observations**:\n>- `50.2%` customers are `Middle Aged (35-55)`\n>- `48.8%` customers are `Mid Level Earners (39K - 98K)`\n>- `47.4%` customers spends `0.7K - 2.5K` on `Credit Card`","fca516d8":"### Decision Tree","35ce67bb":">***Recall is almost same as with threshold = 0.5, which is expected given the scores we got at 0.5 threshold***","254fb066":"#### Experience","1571ad6d":">There are `467` unique `Zip Codes`","89f9fc4d":"#### Income by Education boxplot for each Personal Loan category","81692d89":"## Conclusion","a567acbe":"### Import required libraries","f8b1fce2":"## insights based on EDA","676730a2":">***Observation***:\n>\n>- The `Logistic Regression Model with Class Weight 0 : 0.91, 1 : 0.09` appears to be providing best `Test Recall` of `94%` with `Test Accuracy` of `90%`. \n>- The `Logistic Regression Model` with `Class Weight 0 : 0.91, 1 : 0.09` and `9 features` (using `Sequential Feature Selection`) also provides similar accuracy with `92%` `Test Recall`.","f13a9eea":"### Logistic Regression","b4a86e82":">Since our goal is to increase the recall score, we'll use the above observation to determine optimal `ccp_alpha`","2896dd34":"### Creating training and testing datasets","12f917ea":"### Handling negative values in Experience","34e4c144":"### Decision Tree : Cost Complexity Pruning","4bf040a2":"#### Pairplot of all available numeric columns, hued by Personal Loan","c43b3027":"### Outliers detection using Box plot","bafdbba1":"## Data Preparation","89fc2a07":">**Observation**\n>- `Age` is well distributed, but has 5 spikes in the dataset.\n>- `Minimum Age` is 23 years\n>- `Maximum Age` is 67 years\n>- `Mean and Median Age` are around 45 years","8274d651":"## Actionable Insights & Recommendations","23f6d0c3":">- There are several outliers in Family size 1 and 2 for customers who don't have a Personal loan compared to the rest.\n>- We also see that as Family size increases, the Mortgage value also rises and the customers have Personal Loans","db0acb76":"#### Income","6d5a7c5c":"### GridSearch for hyperparameter tuning","65c101b4":">There are `5000 rows` and `14 columns`. That makes `13 features` for `1 target`","53533b50":"#### Model with SFS and Class Weight","a4586453":"### Check the data where classification is incorrect","4fa52446":"#### Create function to get Recall Score only","b11b0194":">**Observation**\n>- Majority customers who did have `Personal Loan` with the bank, are `Middle Aged (35 - 55 Years)`","beb0e70e":">**Observation**\n>- Not noticing any clustering pattern for `Personal Loan Opted or Not` vs. `Age` and `Experience`\n>- People having `higher income` have taken `personal loan`\n>- People with `2 - 4 family members` are likelier to take `personal loan`\n>- People with high `mortgages` opted for `loan`.\n>- People with `higher credit card average` opted for `personal loan`\n>- People with `higher mortgage` opted for `personal loan`","b7dbd7e7":"Treating the outliers","555bccd0":"#### Education by Personal Loan (borrowed or not)","7454ae78":">When ``ccp_alpha`` is set to zero and keeping the other default parameters\n>of `DecisionTreeClassifier`, the tree overfits.\n>As alpha increases, more\n>of the tree is pruned, thus creating a decision tree that generalizes better.","839a1d26":"Imputing `Orange County` for the two zipcodes we could find on internet. County for the remaining two are being imputed as the zipcode values for now.","ca3f7679":"> There are no more outliers in our dataset","5cbf9de4":"Next, we train a decision tree using the effective alphas. The last value in `ccp_alphas` is the alpha value that prunes the whole tree, leaving the tree, clfs[-1], with one node.","3afa306c":"#### Build Model with Class Weight","5f258918":"#### Confusion Matrix","61e43e51":"### Model Evaluation Criteria","da8acc87":"`zipcode` is a key information that can be used to categorize the location of the customer. However, there are too many `zipcodes` if we consider it as the category. Thus, we'll use the us zip code library to get corresponding counties, and later, will map the counties to regions.","e6fa8459":"Let's now check how the recall of the model changes with increasing `ccp_alpha` values for train and test datasets","740a342a":"We'll split the dataset first into dependent and independent variable sets. Then we'll One-Hot encode the categorical columns `Education` and `Region` only. The rest of the categorical columns are not being encoded as they hold binary values, 1 or 0. After that we split the datasets into training and testing dataset (30% for testing).","7a5adc7d":"- Dropping `Experience`, `Age Bin`, `CC Avg Bin`, `Income Bin`, `Zip Code`, `County`\n- There are many outliers in the data which we will treat (perform capping of outliers).\n- All the values smaller than the lower whisker will be assigned the value of the lower whisker, and all the values above the upper whisker will be assigned the value of the upper whisker.","f79808a5":">***Observation***:\n>- The model shows `Income` is the most important feature, followed by `Family Size` and `Education`\n>- It is a much simpler `Decision Tree` with better recall (`94%`), and no over-fitting","72125f2c":"#### Age:","1d5b06df":">**Observation**\n>- `Experience` is well distributed, but has 4 spikes in the dataset.\n>- `Minimum Experience` is 0 years\n>- `Maximum Experience` is 43 years\n>- `Mean and Median Experience` are around 20 years\n>- The distribution of `Experience` looks suspiciously similar to `Age`. They might be correlated. We'll check this using `pairplot` and `heatmap`","d0ab9d39":"> We have split the dataset into Training and Testing dataset. In both datasets, the target variable has around `91:9` distribution for the values 0 and 1 respectively. We'll use this information later for the `class_weight` parameter.","c7ecfbcb":"#### Check Model Score","c1956411":"#### CC Average","e3301cae":"There is huge imbalance in the labels of `Personal Loan` in the dataset, almost 91:9. Hence, trying to adjust the imbalance using the `class_weight` parameter.","b828772f":"#### Logistic Regression Model Score Comparison","b12bb963":"### **Customer Profile**\n\n- Customers with `higher Income`, `higher Mortgage`, and `higher Credit Card expenditure` tend to borrow `Personal Loan`\n- Customers with `low Income`, `low Mortgage`, and `low Credit Card expenditure` tend to not borrow `Personal Loan` at all\n- Customers with `medium Income`, `medium Mortgage`, and `medium Credit Card expenditure` have a mixed tendency of borrowing (or not borrowing) `Personal Loan`\n- These three features are helpful to categorize customers in `High, Mid and Low profile`","00e9bc90":"<span style='background:lightyellow'\/>Mapping the counties to regions now. Source: https:\/\/census.ca.gov\/regions\/","996762f7":">**Observation**:\n>- Majority customers who did have `Personal Loan` with the bank, are having `High Expenditure (2.5K - 10k USD)`","ad532028":">***Accuracy is super high, and recall improved with respect to the first model. Thus, trying class_weight parameter settings to further check the scores***","0804645f":"#### Jointplot between Income and Mortgage\/CC Avg, by Personal Loan, opted or not","f70d1a46":"#### False Positives","d9d20056":"### Model Evaluation Functions - Scoring & Confusion Matrix","4ddb17fc":"Dropping columns which are not required for the model","539a7257":">**Observation**\n>- `Income` is right skewed\n>- `Range of Income` varies from `$8k - $224k`\n>- `Max Income` is quite higher than `Q3`. We'll validate the record.\n>- There are outliers in `Income` on the higher side, which requires to be validated and treated if required.","69beeb9b":"#### Build Model","c09c681a":">**Observation**:\n>- People with higher `Mortgage` in general opted for `Personal Loan`","8de02a77":">- Income level among all Family groups is significantly higher for customers who have a Personal Loan.","6560cdd0":"### Standardizing column names","96c0f147":"The first step of univariate analysis is to check the distribution\/spread of the data. This is done using primarily `histograms` and `box plots`. Additionally we'll plot each numerical feature on `violin plot` and ` cumulative density distribution plot`. For these 4 kind of plots, we are building below `summary()` function to plot each of the numerical attributes. Also, we'll display feature-wise `5 point summary`.","08147954":">**Observation**\n>- Majority customers who did have `Personal Loan` with the bank did not use `Credit Card` from other banks.","f9862926":"Below function will take a category column as input and plot `percentage distribution on pie chart`, and `bar chart` & `stacked bar chart` for the input category by the target variable `personal loan`","b81142d7":"- We analyzed the Personal Loan campaign data using EDA and by using different models like Logistic Regression and Decision Tree Classifier to build a likelihood of Customer borrowing `Personal Loan`.\n\n\n- `Performance Metric` used is `Recall` because we want to `minimize false negatives`\n\n\n- First we built model using Logistic Regression. The most important features for classification were `Income`, `Education`, `CD account`, `Family` and `CCAvg`\/`Spending`.\n\n\n- We measured the performance, and tried to improve it using `class weight`, `forward feature selection`, and `ROC-AUC threshold values`. However, the model with `class weight` of `0: 0.91, 1:0.09` provided best performance with respect to `recall` and `accuracy`.\n\n\n- Coefficient of `Income`, `Graduate and Advanced Education`, `Family`, `CCavg`, `CD account` are positive , hence, one unit increase in these feature values would increase the probability of a customer borrowing personal loan.\n\n\n- Coefficient of `Securities account`, `online`, `credit card` are negative. Hence, increase in these will lead to decrease in chances of a person borrowing a personal loan.\n\n\n- Decision tree can easily over-fit. They require less data preprocessing compared to Logistic Regression and are easy to understand.\n\n\n- We used decision trees with pre-pruning and post-pruning. The `Post Pruning Decision Tree` model gave `96% recall` with `99% accuracy`.\n\n\n- `Income`, customers with `Graduate or Advanced degree`, customers having higher `Family Size` are some of the most important variables for predicting the probability of a customer borrowing `Personal Loan`.","f450eb79":"## Understanding the data","f2349d1d":"#### Education Level and Income-wise customer distribution, by Personal Loan, opted or not","8e3c17ba":"#### Visualizing the Decision Tree","46723ab8":">**Observation**\n>- Number of Customers with `Family Size 3 or More` has borrowed personal loan more than other people","73ff1659":"### **Columns to be dropped**\n- `Experience`, `Age Bin`, `CC Avg Bin`, `Income Bin`, `Zip Code`, `County` columns need to be dropped before starting with model building","d9c432fa":"#### Build Tree","a6f8c20a":"Let's check the categorical feature-wise value distributions by count","4bb26ce4":"`Personal_Loan`, `Securities_Account`, `CD_Account`, `Online`, `CreditCard`, `Education` are of int\/object type in the original dataset, although they actually are categories. Hence, converting the data types as required.","bba69ab5":">**Observation**\n>- `Mortgage` is highly right skewed\n>- `Range of Mortgage` varies from `$0k - $635k`\n>- `Max Mortgage` is very much higher than `Q3`. We'll treat the outlier in the record.\n>- There are lots of outliers in `Mortgage` on the higher side, which requires to be validated and treated if required.","a5c1e63a":"We'll build below 2 functions to treat outliers by below rule\n\n- all the values smaller than Lower Whisker will be assigned value of Lower_whisker                                  \n- all the values above Upper Whisker will be assigned value of upper_Whisker ","22609961":">The `Experience` values are all 0 or negative for people of age 23 and 24. Rest of the people have positive and negative values as well. However, judging by the education and age, most probably the experience was entered as negative by mistake. Thus taking the absolute values for the experience.","abd9dff9":">***Observation***\n>- `Decision Tree` model `with post pruning` has given us best `Recall Score` of `96%` on `test data` with `99% Accuracy`.\n>- It suggests that the most important features are `Income`, `Family`, and `Education`. \n>- `Decision Tree with post-pruning` appears to be best suited for our prediction.","4b71e029":"#### Heatmap to understand correlations between independent and dependent variables","42464318":"#### Mortgage","4cbb49b2":"## Exploratory Data Analysis on the Incorrectly Predicted Data","55b17ddd":"## Exploratory Data Analysis","3c2e549f":">**Observation**\n>- `42%` customers are `Undergraduates`\n>- `89.6%` customers do not have `Security Account`\n>- `94%` customers do not have `Certified Deposits`\n>- `59.7%` customers have `Online` banking enabled\n>- `70.6%` customers do not `Credit Card` from other banks","47eed479":"### Deriving Regions from Zip Codes","cf94053b":"### Data Type Conversions","43075cba":">`No duplicate values as well`","dbb52732":"### Bi-variate Analysis","387b36e7":">***Observation***:\n>- The model shows `Income` is the most important feature, followed by `Family Size` and `Education`\n>- It also is a simpler `Decision Tree` with below stats:\n>\n>    - Accuracy on training set :  0.9945714285714286\n>    - Accuracy on test set :  0.986\n>    - Recall on training set :  0.9704142011834319\n>    - Recall on test set :  0.9647887323943662","6eeb8c52":"#### Visualizing the Decision Tree","2487504d":"##### Trying to find intersect point of Precision and Recall curve to find threshold point for model improvement","b460b3a0":"For the categorical variables, it is best to analyze them at percentage of total on bar charts\nBelow function takes a category column as input and plots bar chart with percentages on top of each bar","64c990b5":"#### Removing class weight","cdb31c77":"Let's check it grouped by `Age`","df46d25d":"We are creating a few functions to score the models, show the confusion matrix","aba24cba":"#### False Negatives","77a08851":"`Experience` column contains negative values. Exploring the dataset to check on it.\nHighlighting negatives.","e0dfbd55":"The `DecisionTreeClassifier` provides parameters such as `min_samples_leaf` and `max_depth` to prevent a tree from overfiting. `Cost complexity pruning` provides another option to control the size of a tree. In `DecisionTreeClassifier`, this pruning technique is parameterized by the `cost complexity parameter, ccp_alpha`. Greater values of `ccp_alpha` increase the number of nodes pruned.","97b29f4c":"#### Age bucket and Mortgage-wise customer distribution, by Personal Loan, opted or not","255439bc":">***The accuracy and F1 scores improved a lot. However, Recall is bad***","914b2a89":"#### Score the Tree","4eae5c3d":"#### Confusion Matrix","631c1084":"### Installing US Zip Codes library","8aa8fa51":">**Observation**\n>- Almost 50% of customers having `Certified Deposit`, had borrowed `Personal Loan`\n>- However, `4358` customers out of `5000`, do not have `Certified Deposit Account` and did not borrow `Personal Loan`","0347ca34":"#### Limiting the maximum depth to 7","6a8444f4":">***Logistic Regression model with `class_weight` defined to tackle the imbalance in the data, is giving a good performance on training and test set and the recall is high as well.***","fed35c6f":"#### Model can make wrong predictions as:\n\n1. Predicting a person will buy a loan but he actually doesn't - **Loss of Resource**\n2. Predicting a person will not buy a loan but he actually does - **Loss of Opportunity**\n\n#### Which case is more important? \n\n<span style='background:yellow'\/>The whole purpose of the campaign is to bring in more customers. 2nd case is more important to us. If a potential customer is missed by the sales\/marketing team, it will be loss of opportunity. The evaluation criteria should be minimal `Loss of Opportunity`.\n\n#### How to reduce this loss i.e need to reduce False Negatives?\n\n`Recall` should be maximized, the greater the `Recall`, higher the chances of identifying both the classes correctly.","e9734b42":"Verifying outlier treatment status","b7e79926":">- Customers who have Personal loans have a higher credit card Average.\n>- There are several outliers in customers who don't have personal loans.","8206f26a":"#### CC Avg by Credit Card boxplot for each Personal Loan category"}}