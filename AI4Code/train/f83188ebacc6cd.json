{"cell_type":{"50e4a5ae":"code","54ef8b37":"code","6e87b67f":"code","b00dc654":"code","35fe5a1a":"code","395df766":"code","4758bfd0":"code","9a998b5b":"code","4755ed4f":"code","225cd567":"code","c7850920":"code","b150100a":"code","5822236d":"code","ef2c5bdf":"code","a4257473":"markdown","72ca97e9":"markdown","a644651b":"markdown","259dc92d":"markdown"},"source":{"50e4a5ae":"import numpy as np # linear algebra\nimport csv\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current sessio","54ef8b37":"X_test = np.loadtxt('\/kaggle\/input\/ei-71058-mlo-wine-quality\/test_data.csv', delimiter=',', skiprows=1)\nX_train = np.loadtxt('\/kaggle\/input\/ei-71058-mlo-wine-quality\/train_data.csv', delimiter=',', skiprows=1)\ny_train = np.loadtxt('\/kaggle\/input\/ei-71058-mlo-wine-quality\/train_target.csv', delimiter=',', skiprows=1)","6e87b67f":"X_test_id = X_test[:,0:1]\nX_test = X_test[:,1:]\nX_train = X_train[:,1:]\nprint(y_train.shape)\ny_train = y_train[:,1:]\nprint(y_train.shape)\n\n","b00dc654":"# random_labels = np.random.randint(9, size=(X_test.shape[0],1))","35fe5a1a":"# **Ridge regression**\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\n\nridge = Ridge()\nparameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3, 1e-2, 1, 5, 10, 20, 30, 40, 50]}\nridge_regressor = GridSearchCV(ridge, parameters, scoring='neg_mean_squared_error', cv=5)\nridge_regressor.fit(X_train, y_train)\n\nprint(ridge_regressor.best_params_)\nprint(ridge_regressor.best_score_)\n\n# **Random Forest**\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor()\nparameters = {'n_estimators': [5, 10, 15, 20, 25, 30, 35, 40, 45, 50], 'max_depth': [2, 4, 6, 8, 10, 12, 14, 16]}\nrf_regressor = GridSearchCV(rf, parameters, scoring='neg_mean_squared_error', cv=5)\nrf_regressor.fit(X_train, y_train.ravel())\n\nprint(rf_regressor.best_params_)\nprint(rf_regressor.best_score_)\n\n# **Gradient Boosting**\nfrom sklearn.ensemble import GradientBoostingRegressor\n\ngb = GradientBoostingRegressor()\nparameters = {'n_estimators': [5, 10, 15, 20, 25, 30, 35, 40, 45, 50], 'max_depth': [2, 4, 6, 8, 10, 12, 14, 16]}\ngb_regressor = GridSearchCV(gb, parameters, scoring='neg_mean_squared_error', cv=5)\ngb_regressor.fit(X_train, y_train.ravel())\n\nprint(gb_regressor.best_params_)\nprint(gb_regressor.best_score_)\n\n# **Linear Regression**\nfrom sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\nlr.fit(X_train, y_train.ravel())\n\n# **Extreme Gradient Boosting**\nimport xgboost as xgb\n\nxgb_model = xgb.XGBRegressor()\nparameters = {'n_estimators': [5, 10, 15, 20, 25, 30], 'max_depth': [2, 4, 6, 8, 10]}\nxgb_regressor = GridSearchCV(xgb_model, parameters, scoring='neg_mean_squared_error', cv=5)\nxgb_regressor.fit(X_train, y_train.ravel())\n\nprint(xgb_regressor.best_params_)\nprint(xgb_regressor.best_score_)\n","395df766":"\nimport xgboost as xgb\n\nxgb_model = xgb.XGBRegressor()\nparameters = {'n_estimators': [5, 10, 15, 20, 25, 30], 'max_depth': [2, 4, 6, 8, 10]}\nxgb_regressor = GridSearchCV(xgb_model, parameters, scoring='neg_mean_squared_error', cv=5)\nxgb_regressor.fit(X_train, y_train.ravel())\n\nprint(xgb_regressor.best_params_)\nprint(xgb_regressor.best_score_)","4758bfd0":"#now evaluate the results of the regression models using MSE\n\ny_test = np.loadtxt('\/kaggle\/input\/ei-71058-mlo-wine-quality\/train_target.csv', delimiter=',', skiprows=1)\ny_test = y_test[:1300,:1]\n\nfrom sklearn.metrics import mean_squared_error\n\ny_pred_ridge = ridge_regressor.predict(X_test)\ny_pred_rf = rf_regressor.predict(X_test)\ny_pred_gb = gb_regressor.predict(X_test)\ny_pred_lr = lr.predict(X_test)\ny_pred_xgb = xgb_regressor.predict(X_test)\n\nprint(\"MSE Ridge: \", mean_squared_error(y_test, y_pred_ridge))\nprint(\"MSE RF: \", mean_squared_error(y_test, y_pred_rf))\nprint(\"MSE GB: \", mean_squared_error(y_test, y_pred_gb))\nprint(\"MSE LR: \", mean_squared_error(y_test, y_pred_lr))\nprint(\"MSE XGB: \", mean_squared_error(y_test, y_pred_xgb))\n","9a998b5b":"random_labels = np.column_stack((X_test_id, y_pred_xgb))\nnp.savetxt('random_labels_xgb.csv', random_labels, delimiter=',', header='Id,Quality',fmt='%i', comments='')","4755ed4f":"#approach1 Supervised Learning\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\ndef TensorflowApproach():\n  model = Sequential()\n  model.add(Dense(64, input_dim=11, activation='relu'))\n  model.add(Dense(64, activation='relu'))\n  model.add(Dense(1))\n  model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n  return model\n\nestimator = KerasRegressor(build_fn=TensorflowApproach, epochs=100, batch_size=5, verbose=0)\nkfold = KFold(n_splits=10)\nresults = cross_val_score(estimator, X_train, y_train, cv=kfold)\nprint(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))","225cd567":"#TensorflowApproach3 with different hyperparameters\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import RootMeanSquaredError\nfrom tensorflow.keras.losses import MeanSquaredError\n\nmodel = Sequential()\nmodel.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1, activation='linear'))\n\nmodel.compile(loss=MeanSquaredError(), optimizer=Adam(), metrics=[RootMeanSquaredError()])\n\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\nmodel.fit(X_train, y_train, epochs=100, validation_split=0.2, batch_size=32, callbacks=[early_stop], verbose=1)\ny_pred_tf1 = model.predict(X_test)\nrandom_labels = np.column_stack((X_test_id, y_pred_tf1))\nnp.savetxt('random_labelsTFA3.csv', random_labels, delimiter=',', header='Id,Quality',fmt='%i', comments='')","c7850920":"# **AutoML**\nfrom tpot import TPOTRegressor\n\ntpot = TPOTRegressor(generations=5, population_size=50, verbosity=2)\ntpot.fit(X_train, y_train)\ny_pred_tpot = tpot.predict(X_test)\nrandom_labels = np.column_stack((X_test_id, y_pred_tpot))\nnp.savetxt('random_labels.csv', random_labels, delimiter=',', header='Id,Quality',fmt='%i', comments='')\n\n# Now evaluate the models using MSE.\n\n# **Evaluation**\nfrom sklearn.metrics import mean_squared_error\n\ny_pred_tpot = tpot.predict(X_test)\nprint(\"MSE TPOT: \", mean_squared_error(y_test, y_pred_tpot))","b150100a":"# **Bootstrapping**\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import resample\n\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.3, random_state=0)\n\nX_train_bootstrap, y_train_bootstrap = resample(X_train, y_train, n_samples=X_train.shape[0], random_state=0)\n\n# **Cross Validation**\nfrom sklearn.model_selection import KFold\n\nkf = KFold(n_splits=5)\nkf.get_n_splits(X_train_bootstrap)\n\nprint(kf)\n\nfor train_index, test_index in kf.split(X_train_bootstrap):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train_bootstrap_train, X_train_bootstrap_test = X_train_bootstrap[train_index], X_train_bootstrap[test_index]\n    y_train_bootstrap_train, y_train_bootstrap_test = y_train_bootstrap[train_index], y_train_bootstrap[test_index]\n\n# **Linear Regression**\nfrom sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\nlr.fit(X_train_bootstrap_train, y_train_bootstrap_train)\n\ny_pred_lr_bootstrap = lr.predict(X_train_bootstrap_test)\nprint(\"MSE LR Bootstrap: \", mean_squared_error(y_train_bootstrap_test, y_pred_lr_bootstrap))\n\n# **Ridge Regression**\nfrom sklearn.linear_model import Ridge\n\n\nridge = Ridge()\nparameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3, 1e-2, 1, 5, 10, 20, 30, 40, 50]}\nridge_regressor = GridSearchCV(ridge, parameters, scoring='neg_mean_squared_error', cv=5)\nridge_regressor.fit(X_train_bootstrap_train, y_train_bootstrap_train)\n\nprint(ridge_regressor.best_params_)\nprint(ridge_regressor.best_score_)\n\ny_pred_ridge_bootstrap = ridge_regressor.predict(X_train_bootstrap_test)\nprint(\"MSE Ridge Bootstrap: \", mean_squared_error(y_train_bootstrap_test, y_pred_ridge_bootstrap.ravel()))\n\n# **Random Forest**\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor()\nparameters = {'n_estimators': [5, 10, 15, 20, 25, 30, 35, 40, 45, 50], 'max_depth': [2, 4, 6, 8, 10, 12, 14, 16]}\nrf_regressor = GridSearchCV(rf, parameters, scoring='neg_mean_squared_error', cv=5)\nrf_regressor.fit(X_train_bootstrap_train, y_train_bootstrap_train.ravel())\n\nprint(rf_regressor.best_params_)\nprint(rf_regressor.best_score_)\n\ny_pred_rf_bootstrap = rf_regressor.predict(X_train_bootstrap_test)\nprint(\"MSE RF Bootstrap: \", mean_squared_error(y_train_bootstrap_test, y_pred_rf_bootstrap.ravel()))\n\n# **Gradient Boosting**\nfrom sklearn.ensemble import GradientBoostingRegressor\n\ngb = GradientBoostingRegressor()\nparameters = {'n_estimators': [5, 10, 15, 20, 25, 30, 35, 40, 45, 50], 'max_depth': [2, 4, 6, 8, 10, 12, 14, 16]}\ngb_regressor = GridSearchCV(gb, parameters, scoring='neg_mean_squared_error', cv=5)\ngb_regressor.fit(X_train_bootstrap_train, y_train_bootstrap_train.ravel())\n\nprint(gb_regressor.best_params_)\nprint(gb_regressor.best_score_)\n\ny_pred_gb_bootstrap = gb_regressor.predict(X_train_bootstrap_test)\nprint(\"MSE GB Bootstrap: \", mean_squared_error(y_train_bootstrap_test, y_pred_gb_bootstrap.ravel()))\n\n# **Extreme Gradient Boosting**\nimport xgboost as xgb\n\nxgb_model = xgb.XGBRegressor()\nparameters = {'n_estimators': [5, 10, 15, 20, 25, 30], 'max_depth': [2, 4, 6, 8, 10]}\nxgb_regressor = GridSearchCV(xgb_model, parameters, scoring='neg_mean_squared_error', cv=5)\nxgb_regressor.fit(X_train_bootstrap_train, y_train_bootstrap_train.ravel())\n\nprint(xgb_regressor.best_params_)\nprint(xgb_regressor.best_score_)\n\ny_pred_xgb_bootstrap = xgb_regressor.predict(X_train_bootstrap_test)\nprint(\"MSE XGB Bootstrap: \", mean_squared_error(y_train_bootstrap_test, y_pred_xgb_bootstrap.ravel()))","5822236d":"X_test = np.loadtxt('\/kaggle\/input\/ei-71058-mlo-wine-quality\/test_data.csv', delimiter=',', skiprows=1)\nX_test_id = X_test[:,0:1]\nX_test = X_test[:,1:]","ef2c5bdf":"from sklearn.metrics import mean_squared_error\n\n#ridge\ny_pred_ridge_bootstrap = ridge_regressor.predict(X_test)\nrandom_labels = np.column_stack((X_test_id, y_pred_ridge_bootstrap))\nnp.savetxt('\/random_labelsCrossValidatedBootstrappedridge.csv', random_labels, delimiter=',', header='Id,Quality',fmt='%i', comments='')\n\n#random forest\ny_pred_rf_bootstrap = rf_regressor.predict(X_test)\nrandom_labels = np.column_stack((X_test_id, y_pred_rf_bootstrap))\nnp.savetxt('\/random_labelsCrossValidatedBootstrappedrf.csv', random_labels, delimiter=',', header='Id,Quality',fmt='%i', comments='')\n\n#gradient boost regression\ny_pred_gb_bootstrap = gb_regressor.predict(X_test)\nrandom_labels = np.column_stack((X_test_id, y_pred_gb_bootstrap))\nnp.savetxt('\/random_labelsCrossValidatedBootstrappedGB.csv', random_labels, delimiter=',', header='Id,Quality',fmt='%i', comments='')\n\n#linear regression\ny_pred_lr_bootstrap = lr.predict(X_test)\nrandom_labels = np.column_stack((X_test_id, y_pred_lr_bootstrap))\nnp.savetxt('\/random_labelsCrossValidatedBootstrappedLR.csv', random_labels, delimiter=',', header='Id,Quality',fmt='%i', comments='')\n\n#extreme gradient boost\ny_pred_xgb_bootstrap = xgb_regressor.predict(X_test)\nrandom_labels = np.column_stack((X_test_id, y_pred_xgb_bootstrap))\nnp.savetxt('\/random_labelsCrossValidatedBootstrappedXGB.csv', random_labels, delimiter=',', header='Id,Quality',fmt='%i', comments='')","a4257473":"### Get the x_test data and save the models as csv","72ca97e9":"This doesn't work too well. Let's use cross-validation and bootstrapping and redo all the modeling","a644651b":"Choose model with lowest MSE value and save the results","259dc92d":"## I added an AutoML library to select the best model"}}