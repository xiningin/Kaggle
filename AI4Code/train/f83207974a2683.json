{"cell_type":{"2a8104a8":"code","279a0da4":"code","59a4c970":"code","5f72044d":"code","5955f65b":"code","3c78ed57":"code","c52a624d":"code","48f3bd4b":"code","67f7a88d":"code","4218080f":"code","76613af3":"code","339ca660":"code","4256c2a5":"code","114672ef":"code","509d55a2":"code","9d277f4d":"code","dd767705":"code","70b32a46":"code","895b0d43":"code","28372680":"code","3693aeaa":"code","84d4d2ae":"code","cd7dae8f":"code","c6420dce":"code","5624d83f":"code","58006c29":"code","f971467a":"code","f11584e0":"code","22fe5e4f":"code","8f5b097f":"code","3674c0c7":"code","c5233d51":"code","6087e9d9":"code","02f33ace":"code","a85f6fca":"code","b33a3f2c":"code","635e1381":"code","36b5474e":"code","340cd878":"code","ed6ab0be":"code","bd5a95c1":"code","6599a54e":"code","157734dd":"code","9c9f3550":"code","427c7a24":"code","ed7a0834":"code","ec73fbfa":"markdown","1d1d62fc":"markdown","fe19187a":"markdown","9cc0dcc7":"markdown","25fb5718":"markdown"},"source":{"2a8104a8":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","279a0da4":"df=pd.read_csv('\/kaggle\/input\/autompg-dataset\/auto-mpg.csv', na_values=['?'])","59a4c970":"df.info()","5f72044d":"df.columns","5955f65b":"df.shape","3c78ed57":"df.describe().T","c52a624d":"df.head()","48f3bd4b":"df['model year']=df['model year']+1900","67f7a88d":"plt.figure(figsize=(8,5))\nsns.countplot(x='model year',data=df)\nplt.title('Number of cars by years')\nplt.show()","4218080f":"df['origin'] = df['origin'].replace([1, 2, 3], ['USA', 'Europe', 'Japan'])","76613af3":"df['origin'].value_counts().plot.pie(autopct='%1.1f%%')","339ca660":"df.nunique()","4256c2a5":"df.isnull().sum()","114672ef":"df.horsepower.nunique()","509d55a2":"df.horsepower.isnull().sum()\/df.shape[0]","9d277f4d":"df.dropna(axis=0,inplace=True)","dd767705":"df.isnull().sum().sum()","70b32a46":"def correlation(df, size=[10, 7]):\n    f, ax = plt.subplots(figsize= size)\n    sns.heatmap(df.corr(), annot=True, fmt=\".2f\", ax=ax, cmap = \"magma\" )\n    ax.set_title(\"Correlation Matrix\", fontsize=20)\n    plt.show()","895b0d43":"correlation(df)","28372680":"num_cols = [col for col in df.columns if df[col].dtypes != \"O\"]","3693aeaa":"for i in num_cols:\n    sns.boxplot(df[i])\n    plt.show()","84d4d2ae":"def outliers(df, col_name, q1=0.05, q3=0.95):\n    quartile1 = df[col_name].quantile(q1)\n    quartile3 = df[col_name].quantile(q3)\n    IQR = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * IQR\n    low_limit = quartile1 - 1.5 * IQR\n    return df[(df[col_name]>=low_limit) & (df[col_name]<=up_limit)]","cd7dae8f":"df=outliers(df, 'acceleration')","c6420dce":"df=outliers(df, 'horsepower')","5624d83f":"df=pd.get_dummies(df, drop_first=True)","58006c29":"df.head()","f971467a":"from sklearn.preprocessing import MinMaxScaler","f11584e0":"def minmax_scaler(df, col_names, feature_range=(0,1)):\n    minmax_scaler = MinMaxScaler(feature_range=feature_range)\n    col_names=[col for col in col_names]\n    df[col_names] = minmax_scaler.fit_transform(df[col_names])\n    return df","22fe5e4f":"num_cols=[col for col in df.columns if df[col].dtypes != \"O\"]","8f5b097f":"df=minmax_scaler(df, num_cols)","3674c0c7":"df.head()","c5233d51":"X=df.drop(['mpg'], axis=1)\nY=df['mpg']","6087e9d9":"from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, RidgeCV, LassoCV, ElasticNetCV, LogisticRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor","02f33ace":"def randomstate(x,y):\n    min_error=100\n    model=Lasso()\n    for i in range(1,201):\n        xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.25,random_state=i)\n        model.fit(xtrain,ytrain)\n        p=model.predict(xtest)\n        mse=mean_squared_error(ytest, p)\n        if mse<min_error:\n            min_error=mse\n            j=i\n    return j","a85f6fca":"rs=randomstate(X,Y)\nrs","b33a3f2c":"def reg_models(x, y, algo):\n    X_train,X_test,Y_train,Y_test=train_test_split(x,y,test_size=0.25,random_state=rs)\n    algo.fit(X_train,Y_train)\n    Y_train_pred=algo.predict(X_train)\n    Y_test_pred=algo.predict(X_test)\n    score=cross_val_score(algo,x,y,cv=10)\n    train_error, train_score = mean_squared_error(Y_train, Y_train_pred), algo.score(X_train, Y_train)\n    test_error,  test_score = mean_squared_error(Y_test, Y_test_pred), algo.score(X_test, Y_test)\n    cross_score = np.round(score.mean(),4)\n    print(type(algo).__name__)\n    print('Train mse: {}'.format(train_error))\n    print('Train Score: {}'.format(train_score))\n    print('Test mse: {}'.format(test_error))\n    print('Test Score: {}'.format(test_score))\n    print('Mean of Cross Validation Score: {}'.format(cross_score))\n    print('------------------------------------------------------')\n    return (train_error, train_score, test_error,  test_score, cross_score)","635e1381":"models=[LinearRegression(), Ridge(), Lasso(), ElasticNet(), \n            KNeighborsRegressor(n_neighbors=3), DecisionTreeRegressor(max_depth=2)]","36b5474e":"for model in models:\n    reg_models(X, Y, model)","340cd878":"def light_gbm(df, Y):\n    lgbm=LGBMRegressor()\n    X=df.drop(Y, axis=1)\n    Y=df[[Y]]\n    X_train, X_test, Y_train, Y_test=train_test_split(X, Y, random_state=rs, test_size=0.20)\n    lgbm.fit(X_train, Y_train)\n\n    Y_pred=lgbm.predict(X_test,num_iteration=lgbm.best_iteration_)\n\n    print((mean_squared_error(Y_test, Y_pred)))","ed6ab0be":"light_gbm(df,'mpg')","bd5a95c1":"params = {'iterations': 6000,\n          'learning_rate': 0.005,\n          'depth': 4,\n          'l2_leaf_reg': 1,\n          'eval_metric':'RMSE',\n          'early_stopping_rounds': 200,\n          'verbose': 200,\n          'random_seed': 42}\n\n\nX_train, X_test, Y_train, Y_test=train_test_split(X, Y, random_state=rs, test_size=0.20)\n\n         \ncat_f = CatBoostRegressor(**params)\ncat_model_f = cat_f.fit(X_train,Y_train,\n                     eval_set = (X_test,Y_test),\n                     plot=True,\n                     verbose = False)\n\n\ncatf_pred = cat_model_f.predict(X_test)\ntest_rmse=np.sqrt(mean_squared_error(Y_test, catf_pred))\nprint('Test RMSE: ', test_rmse)","6599a54e":"def model_tuning(x, y, algo_cv, algo, alphas, test_size=0.20, cv=10):\n    X_train, X_test, Y_train, Y_test=train_test_split(X, Y, random_state=rs, test_size=test_size)\n    model_cv=algo_cv(alphas=alphas, cv=cv)\n    model_cv.fit(X_train, Y_train)\n    model_tuned=algo(alpha=model_cv.alpha_)\n    model_tuned.fit(X_train, Y_train)\n    print(type(model_tuned).__name__)\n    Y_train_pred=model_tuned.predict(X_train)\n    train_rmse=mean_squared_error(Y_train, Y_train_pred)\n    print(\"Train mse: {}\".format(train_rmse))\n    Y_test_pred=model_tuned.predict(X_test)\n    test_rmse=mean_squared_error(Y_test, Y_test_pred)\n    print(\"Test mse: {}\".format(test_rmse))\n    print('----------------------------------------')\n    return (type(model_tuned).__name__, train_rmse, test_rmse)","157734dd":"cv_models={Ridge: RidgeCV, Lasso:LassoCV, ElasticNet:ElasticNetCV}\nresults_tuned={'model':[], 'train_rmse':[], 'test_rmse':[]}\nalphas = [0.1,0.01, 0.005, 0.05, 0.001,0.2,0.3,0.5,0.8,0.9,1]","9c9f3550":"for model in cv_models:\n    res=model_tuning(X, Y, cv_models[model], model, alphas)\n    results_tuned['model'].append(res[0])\n    results_tuned['train_rmse'].append(res[1])\n    results_tuned['test_rmse'].append(res[2])","427c7a24":"def light_gbm_tuning(df, Y):\n    X=df.drop(Y, axis=1)\n    Y=df[[Y]]\n    X_train, X_test, Y_train, Y_test=train_test_split(X, Y, random_state=42, test_size=0.20)\n    lgbm_grid={\n    'colsample_bytree':[0.4, 0.5, 0.6, 0.9, 1],\n    'learning_rate':[0.01, 0.1, 0.5, 1],\n           'n_estimators':[20, 40, 100, 200, 500, 1000],\n           'max_depth':[1, 2, 3, 4, 5, 6, 7, 8]}\n\n    lgbm=LGBMRegressor()\n\n    lgbm_cv_model=GridSearchCV(lgbm, lgbm_grid, cv=10,\n                           n_jobs=-1, verbose=2)\n\n    lgbm_cv_model.fit(X_train, Y_train)\n\n    #lgbm_cv_model.best_params_\n    lgbm_tuned=LGBMRegressor(learning_rate=0.1,\n                         max_depth=2,\n                         n_estimators=100,\n                         colsample_bytree=0.9)\n\n    lgbm_tuned.fit(X_train, Y_train)\n\n    Y_pred=lgbm_tuned.predict(X_test)\n\n    print(mean_squared_error(Y_test, Y_pred))","ed7a0834":"light_gbm_tuning(df, 'mpg')","ec73fbfa":"# Model","1d1d62fc":"# Encoding ","fe19187a":"# Libraries","9cc0dcc7":"# Final Model","25fb5718":"# Model Tuning"}}