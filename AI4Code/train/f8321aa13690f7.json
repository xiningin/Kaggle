{"cell_type":{"d2f88474":"code","e0c43524":"code","b5d44919":"code","6845d2a5":"code","76b1bb58":"code","14e303af":"code","cf85edfc":"code","d3849012":"code","69c01947":"code","2ac1bae5":"code","e3456a14":"code","a01a12f9":"code","ade649d1":"code","2b926d7b":"code","88262f9e":"code","fa09ae7a":"code","c7626f28":"code","1b64d322":"code","2f955f76":"code","4710ef50":"code","dcc460de":"code","edae5f97":"code","ee8f49a4":"code","ad3d0a3f":"code","348fce37":"code","4a02198e":"code","cc82f608":"code","5b9756b4":"code","4fdb73cd":"code","b3b62ca3":"code","f846144b":"code","4321d5b6":"code","533cb732":"code","e4ed0887":"code","bea31422":"code","23b188bf":"code","b65c20fd":"code","cf49b7c7":"code","b145e96f":"code","08248d41":"code","69e55531":"code","53ed2363":"code","90bec47a":"code","92663f4a":"code","b0f5403c":"code","ca9ce563":"code","ed696752":"code","bec87dd5":"code","8c938b85":"code","d75b8a95":"code","ef7155dd":"code","5067b640":"code","417f1297":"code","56b4eab3":"code","7dac3c62":"code","d269fcf7":"code","85c11061":"markdown","2a967d21":"markdown","9221c1bd":"markdown","b8de616f":"markdown","7d87a19d":"markdown","0e86e52e":"markdown","e82e9bf0":"markdown","87d9e84e":"markdown","9808826e":"markdown","7951d6ff":"markdown","1ff91619":"markdown","1cc5dc0a":"markdown","1123e591":"markdown","7a989bc2":"markdown"},"source":{"d2f88474":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e0c43524":"# To prevent from warnings\n\nimport warnings\nwarnings.filterwarnings('ignore')","b5d44919":"# Importing Libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport plotly.express as px\nfrom statsmodels.graphics.gofplots import qqplot\nimport seaborn as sns\n%matplotlib inline","6845d2a5":"# Reading the dataset\ndf = pd.read_csv('\/kaggle\/input\/the-boston-houseprice-data\/boston.csv')","76b1bb58":"# Display top 5 rows of the datasets\ndf.head()","14e303af":"# Getting the shape of dataset ( means number of rows and column)\ndf.shape","cf85edfc":"# To view some basic statistical details \ndf.describe()","d3849012":"# getting the information about dataframe\ndf.info()","69c01947":"#  check for null value \ndf.isnull().sum()","2ac1bae5":"# checking number of unique values in each column\ndf.nunique()","e3456a14":"# Visualizing the presence of null value using heatmap\nsns.heatmap(df.isnull())","a01a12f9":"# correlation using heatmap\nplt.figure(figsize=(10,8))\nsns.heatmap(df.corr(), annot = True, cmap='coolwarm')","ade649d1":"# Unstacking the correlation values to check the correlation between the feature columns\n\ncorr = df.corr()\nc1 = corr.abs().unstack()\nc1.sort_values(ascending = False)[14:28:2]","2b926d7b":"fig, (ax1,ax2) = plt.subplots(ncols=2, figsize=(15,4))\nsns.distplot(df['LSTAT'], ax=ax1 , color ='red')\nax1.set(title='LSTAT distribution')\nqqplot(df['LSTAT'], ax=ax2, line='s')\nax2.set(title='LSTAT quantile plot')","88262f9e":"skew_val = df.skew().sort_values(ascending=False)\nskew_val","fa09ae7a":"# Checking outliers using box plot\n\nfig, ax = plt.subplots(ncols = 7, nrows = 2, figsize = (20, 10))\nindex = 0\nax = ax.flatten()\n\nfor col, value in df.items():\n    sns.boxplot(y=col, data=df, ax=ax[index])\n    index += 1\nplt.tight_layout(pad = 0.5, w_pad=0.7, h_pad=5.0)","c7626f28":"# Individual box plot for each feature\ndef Box(df):\n    plt.title(\"Box Plot\")\n    sns.boxplot(df)\n    plt.show()\nBox(df['CRIM'])","1b64d322":"# Individual histogram for each feature\ndef hist_plots(df):\n    plt.title(\"Histogram\")\n    plt.hist(df)\n    plt.show()\nhist_plots(df['CRIM'])","2f955f76":"# Individual Distribution plot for each feature\ndef dist_plots(df):\n    plt.title(\"Distribution Plot\")\n    sns.distplot(df)\n    plt.show()\ndist_plots(df['CRIM'])","4710ef50":"df.nunique()","dcc460de":"fig = px.box(df, x=\"CHAS\", y=\"MEDV\", color=\"CHAS\", width=800, height=400)\nfig.show()","edae5f97":"fig = px.box(df, x=\"RAD\", y=\"MEDV\", color=\"RAD\")\nfig.show()","ee8f49a4":"fig =  px.pie (df, names = \"CHAS\", hole = 0.4, template = \"plotly_dark\")\nfig.show ()","ad3d0a3f":"fig = px.scatter (df, x = \"MEDV\", y = \"RM\", color = \"CHAS\", template = \"plotly_dark\",  trendline=\"ols\")\nfig.show ()","348fce37":"fig = px.scatter (df, x = \"MEDV\", y = \"DIS\", color = \"ZN\", template = \"plotly_dark\",  trendline=\"ols\")\nfig.show ()","4a02198e":"fig = px.scatter (df, x = \"MEDV\", y = \"AGE\", color = \"ZN\", template = \"plotly_dark\",  trendline=\"lowess\")\nfig.show ()","cc82f608":"fig = px.scatter (df, x = \"MEDV\", y = \"DIS\", color = \"RAD\", template = \"plotly_dark\",  trendline=\"lowess\")\nfig.show ()","5b9756b4":"# Min-Max normalization is used to  bring the values in a particular arrange ( i.e. here 0 to 1)\n\n# Here we are taking only 4 column for normalization because in this the value are too high as compare to others\n\ncols = ['CRIM', 'ZN', 'TAX', 'B']\nfor col in cols:\n    minimum = min(df[col])\n    maximum = max(df[col])\n    df[col] = (df[col] - minimum)\/ (maximum - minimum)\n    ","4fdb73cd":"# Here we can see that values are now between 0 and 1\n\nfig, ax = plt.subplots(ncols = 7, nrows = 2, figsize = (20, 10))\nindex = 0\nax = ax.flatten()\n\nfor col, value in df.items():\n    sns.boxplot(y=col, data=df, ax=ax[index])\n    index += 1\nplt.tight_layout(pad = 0.5, w_pad=0.7, h_pad=5.0)","b3b62ca3":"# Now values are between 0 and 1 \ndef hist_plots(df):\n    plt.title(\"Histogram\")\n    plt.hist(df)\n    plt.show()\nhist_plots(df['CRIM'])","f846144b":"# Here we can see that after min-max normalization values now ranges from 0 to 1\ndf.head()","4321d5b6":"# dropping 'MEDV' from dataframe and saving dataframe in X which is now acting as input column\nX = df.drop(columns=['MEDV'], axis=1)\nX.head()","533cb732":"X.shape","e4ed0887":"# y have only 'MEDV' column which is the output column\ny = df['MEDV']\ny.shape","bea31422":"from sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error","23b188bf":"# Performing train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,train_size=0.8,random_state=42)","b65c20fd":"# creating object of Linear Regression\nmodel_lrg = LinearRegression()","cf49b7c7":"# Training model\nmodel_lrg.fit(X_train, y_train)","b145e96f":"# Predicting values\npred_tst = model_lrg.predict(X_test)","08248d41":"# Evaluation metrics \"Mean Squared Error\"\nmae_lrg = np.sqrt(mean_squared_error(y_test, pred_tst))\nprint(mae_lrg)","69e55531":"from sklearn.ensemble import RandomForestRegressor","53ed2363":"# creating object of Random Forest Regressor\nmodel_rf = RandomForestRegressor()","90bec47a":"# Training model\nmodel_rf.fit(X_train, y_train)","92663f4a":"# Predicting values\npred_rf = model_rf.predict(X_test)","b0f5403c":"# Evaluation metrics \"Mean Squared Error\"\nmae_rf = np.sqrt(mean_squared_error(y_test, pred_rf))\nprint(mae_rf)","ca9ce563":"from sklearn.tree import DecisionTreeRegressor","ed696752":"# creating object of Decision Tree Regressor\nmodel_dt = DecisionTreeRegressor()","bec87dd5":"# Training model \nmodel_dt.fit(X_train, y_train)","8c938b85":"# Predicting values\npred_dt = model_dt.predict(X_test)","d75b8a95":"# Evaluation metrics \"Mean Squared Error\"\nmae_dt = np.sqrt(mean_squared_error(y_test, pred_dt))\nprint(mae_dt)","ef7155dd":"from xgboost import XGBRFRegressor","5067b640":"# creating object of XGBoost\nmodel_xgb = XGBRFRegressor(max_depth=8, n_estimators = 10)","417f1297":"# Training model\nmodel_xgb.fit(X_train, y_train)","56b4eab3":"# Predicting values\npred_xgb = model_xgb.predict(X_test)","7dac3c62":"# Evaluation metrics \"Mean Squared Error\"\nmae_xgb = np.sqrt(mean_squared_error(y_test, pred_xgb))\nprint(mae_xgb)","d269fcf7":"models = pd.DataFrame({\n    'Model':['Linear Regression', 'Decision Tree', 'Random Forest', 'XGBoost'],\n    'MAE' :[mae_lrg, mae_dt, mae_rf, mae_xgb]\n})\nmodels","85c11061":"* Above data is clearly right skewed as outliers are present in right side.","2a967d21":"### Random Forest ","9221c1bd":"##### Conclusion :- Here after changing few hyperparameter XGBoost is working best here","b8de616f":"### Decision Tree","7d87a19d":"#### Following are the list of algorithms that are used in this notebook \n\n|     Algorithms     |\n| ------------------ | \n| Linear Regression  |\n| Decision Tree      | \n| Random Forest      |\n| XGBoost            | ","0e86e52e":"<!-- Attribute Information\n\nInput features in order:\n1) CRIM: per capita crime rate by town\n2) ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n3) INDUS: proportion of non-retail business acres per town\n4) CHAS: Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n5) NOX: nitric oxides concentration (parts per 10 million) [parts\/10M]\n6) RM: average number of rooms per dwelling\n7) AGE: proportion of owner-occupied units built prior to 1940\n8) DIS: weighted distances to five Boston employment centres\n9) RAD: index of accessibility to radial highways\n10) TAX: full-value property-tax rate per $10,000 [$\/10k]\n11) PTRATIO: pupil-teacher ratio by town\n12) B: The result of the equation B=1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n13) LSTAT: % lower status of the population\n\nOutput variable:\n1) MEDV: Median value of owner-occupied homes in $1000's [k$] -->","e82e9bf0":"* Here we can see that few variables are categorical but they are present in integer format.","87d9e84e":"### XGBoost","9808826e":"* In similar manner we can check the normality and skewness of each variables.","7951d6ff":"## Linear Regression","1ff91619":"<!-- As we can see that is above box plot visualization that \"CRIM\", \"ZN\", \"B\", has so many outliers -->","1cc5dc0a":"## <center> Boston House Price Predction <\/center>","1123e591":"* Shortcut for checking Normality and Skewness using pandas skew function.\n* If the skewness value is between 0.5 and -0.5 then it is normal distribution else will be right of left depending upon data.","7a989bc2":"<center> <img src=\"https:\/\/media.thestar.com.my\/Prod\/D4A838DE-5A4E-4A3B-B970-C7E9E6A8EB7B\" > <\/center>"}}