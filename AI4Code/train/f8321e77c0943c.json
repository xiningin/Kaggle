{"cell_type":{"1db23d64":"code","f1c85d99":"code","dd4a1147":"code","6674749f":"code","fa9787fe":"code","fb99bb89":"code","ee467f78":"code","1b6c05a9":"code","a2ac039c":"code","7150de6d":"code","bb7dc41f":"code","4fdaf229":"code","b70158f7":"code","0a095f68":"code","29dd627b":"code","834f473a":"code","a948bc3b":"code","a0bea7e7":"code","ba2e008e":"code","9eed9eb5":"code","30e36b23":"code","6ee6f802":"code","67cd36ae":"code","7c92cf43":"code","b040a8ee":"code","50430f01":"code","7a3cd817":"code","bd43c426":"code","bec378b1":"code","b627bcdf":"code","b495fb66":"code","c98f2786":"code","b896d979":"code","4a976d3c":"code","f4f0068a":"code","943d5c4e":"code","e3994fb7":"code","8335a286":"code","52b5a34a":"code","2c134fca":"code","600e101a":"code","7a2871a4":"markdown","6dc5de89":"markdown","7f056804":"markdown","cf97cb55":"markdown","89d32cc7":"markdown","f1331f13":"markdown","ee03b335":"markdown","9556bee6":"markdown","9d5b91c2":"markdown","a096896f":"markdown","f8cde52f":"markdown","6f8c465f":"markdown","a8df37f7":"markdown","3aea6326":"markdown","cd50ea1e":"markdown","7c3a174a":"markdown","62be5427":"markdown","ef56acdd":"markdown","380fe485":"markdown","f8f68a3c":"markdown","c2e6a47f":"markdown","2906b3f1":"markdown","e91e1615":"markdown","e8a10c2e":"markdown","40f679ae":"markdown","db28b935":"markdown","47b42f0e":"markdown","996aaa01":"markdown","7c36a481":"markdown","76c7782f":"markdown","ea77318a":"markdown","2aadb511":"markdown","d2a964c0":"markdown","60ca9309":"markdown","143389e3":"markdown","0708381c":"markdown","be12edc4":"markdown","292efe2c":"markdown","8a59a96a":"markdown","2942ec70":"markdown","20aba316":"markdown","34bcfd95":"markdown","0890ccdf":"markdown","546de4f9":"markdown","82299061":"markdown","5d19083a":"markdown","65586af6":"markdown"},"source":{"1db23d64":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f1c85d99":"#Importing required libraries\n#Importing the required libraries and data set \nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\nprint(\"Important libraries loaded successfully\")","dd4a1147":"ds_train=pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\nds_test=pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nds_result=pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\nprint(\"Train and Test data sets are imported successfully\")","6674749f":"print(\"Test and Training data details are as follows: \")\nprint('Number of Training Examples = {}'.format(ds_train.shape[0]))\nprint('Number of Test Examples = {}\\n'.format(ds_test.shape[0]))","fa9787fe":"ds_train.head()","fb99bb89":"#Drop columns from training data set\nds_train=ds_train.drop(['Ticket','Cabin'],axis=1)\nprint(\"Columns Dropped Successfully\")\nds_train.head()","ee467f78":"#Converting Age into series and visualizing the age distribution\nage_series=pd.Series(ds_train['Age'].value_counts())\nfig=px.scatter(age_series,y=age_series.values,x=age_series.index)\nfig.update_layout(\n    title=\"Age Distribution\",\n    xaxis_title=\"Age in Years\",\n    yaxis_title=\"Count of People\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=18,\n    )\n)\nfig.show()","1b6c05a9":"print(\"Number of teenagers and child passengers in ship are {}\".format(len(ds_train[ds_train['Age'] < 20 ])))","a2ac039c":"print(\"Number of Passengers Gender Wise \\n{}\".format(ds_train['Sex'].value_counts()))\n#Gender wise distribution\nfig = go.Figure(data=[go.Pie(labels=ds_train['Sex'],hole=.4)])\nfig.update_layout(\n    title=\"Sex Distribution\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=18\n    ))\nfig.show()","7150de6d":"#Create categorical variable graph for Age,Sex and Survived variables\nsns.catplot(x=\"Survived\", y=\"Age\", hue=\"Sex\", kind=\"swarm\", data=ds_train,height=10,aspect=1.5)\nplt.title('Passengers Survival Distribution: Age and Sex',size=25)\nplt.show()\n","bb7dc41f":"#Visualize relation between Pclass and Survival\nfig = go.Figure(data=[go.Pie(labels=ds_train['Pclass'],hole=.4)])\nfig.update_layout(\n    title=\"PClass Distribution\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=18\n    ))\nfig.show()","4fdaf229":"#Visualize PClass and Survival\n#Create categorical variable graph for Age,Pclass and Survived variables\nsns.catplot(x=\"Survived\", y=\"Age\", hue=\"Pclass\", kind=\"swarm\", data=ds_train,height=10,aspect=1.5)\nplt.title('Passengers Survival Distribution: Age and Pclass',size=25)\nplt.show()","b70158f7":"#Visualize Fare and Survival\n#Create categorical variable graph for Sex,Fare and Survived variables\nsns.catplot(x=\"Survived\", y=\"Fare\", hue=\"Sex\", kind=\"swarm\", data=ds_train,height=8,aspect=1.5)\nplt.title('Passengers Survival Distribution: Fare and Sex',size=20)\nplt.show()","0a095f68":"#Visualize relation between Embarked and Survival\nfig = go.Figure(data=[go.Pie(labels=ds_train['Embarked'],hole=.4)])\nfig.update_layout(\n    title=\"Embarked Distribution\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=18\n    ))\nfig.show()","29dd627b":"#Visualize Embarked and Survival\n#Create categorical variable graph for Embarked,Age and Survived variables\nsns.catplot(x=\"Survived\", y=\"Age\", hue=\"Embarked\", kind=\"swarm\", data=ds_train,height=8,aspect=1.5)\nplt.title('Passengers Survival Distribution: Embarked and Age',size=20)\nplt.show()","834f473a":"#Drop columns from training data set\nds_train=ds_train.drop(['Embarked','Name'],axis=1)\nprint(\"Columns Dropped Successfully\")\nds_train.head()","a948bc3b":"# Training set high correlations\nds_train.corr()","a0bea7e7":"#Add new column 'Family Size' in training model set\nds_train['Family_Size'] = ds_train['SibSp'] + ds_train['Parch'] + 1\nprint(\"Family Size column created sucessfully\")\nds_train.head()","ba2e008e":"#Visualize Family size and Survival\nsns.barplot(x=\"Family_Size\", y=\"Age\", hue=\"Survived\", data=ds_train,palette = 'rainbow')\nplt.title('Family Size - Age Survival Distribution',size=20)\nplt.show()\n","9eed9eb5":"sns.catplot(y=\"Family_Size\", x=\"Survived\", hue='Sex',kind=\"swarm\", data=ds_train,height=8,aspect=1.5)\nplt.title('Family Size - Gender Survival Distribution',size=20)\nplt.show()","30e36b23":"print(\"Information on Train Data Set :\")\nds_train.info()","6ee6f802":"age_by_pclass_sex = ds_train.groupby(['Sex', 'Pclass']).median()['Age']\n\nfor pclass in range(1, 4):\n    for sex in ['female', 'male']:\n        print('Median age of Pclass {} {}s: {}'.format(pclass, sex, age_by_pclass_sex[sex][pclass]))\nprint('Median age of all passengers: {}'.format(ds_train['Age'].median()))\n\n# Filling the missing values in Age with the medians of Sex and Pclass groups\nds_train['Age'] = ds_train.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))","67cd36ae":"print(\"Information on Train Data Set :\")\nds_train.info()","7c92cf43":"#Replacing 'Male' and 'Female' with '0' and '1' respectively\nds_train=ds_train.replace(to_replace='male',value=0)\nds_train=ds_train.replace(to_replace='female',value=1)\nds_train.head()","b040a8ee":"X_train=ds_train.drop(['Survived'],axis=1)\ny_train=ds_train['Survived'].values\nprint('X_train shape: {}'.format(X_train.shape))\nprint('y_train shape: {}'.format(y_train.shape))","50430f01":"classifier_rf=RandomForestClassifier(criterion='gini', \n                                           n_estimators=1100,\n                                           max_depth=5,\n                                           min_samples_split=4,\n                                           min_samples_leaf=5,\n                                           max_features='auto',\n                                           oob_score=True,\n                                           random_state=42,\n                                           n_jobs=-1,\n                                           verbose=1)\nclassifier_rf.fit(X_train,y_train)","7a3cd817":"classifier_xgb=XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05)\nclassifier_xgb.fit(X_train,y_train)","bd43c426":"ds_test.info()","bec378b1":"age_by_pclass_sex = ds_test.groupby(['Sex', 'Pclass']).median()['Age']\n\nfor pclass in range(1, 4):\n    for sex in ['female', 'male']:\n        print('Median age of Pclass {} {}s: {}'.format(pclass, sex, age_by_pclass_sex[sex][pclass]))\nprint('Median age of all passengers: {}'.format(ds_test['Age'].median()))\n\n# Filling the missing values in Age with the medians of Sex and Pclass groups\nds_test['Age'] = ds_test.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))","b627bcdf":"ds_test.info()","b495fb66":"#Filling missing fare with median fare\nnull_index=ds_test['Fare'].isnull().index\nmedianFare=ds_test['Fare'].median()\nds_test.at[null_index,'Fare'] = medianFare\nprint(\"Missing Fare updated as Median Fare :{}\".format(medianFare))","c98f2786":"ds_test.info()","b896d979":"#Drop columns from test data set\nds_test=ds_test.drop(['Ticket','Cabin','Embarked','Name'],axis=1)\nprint(\"Columns Dropped Successfully\")\n\n#Creating Family Size columns from test data set\nds_test['Family_Size'] = ds_test['SibSp'] + ds_test['Parch'] + 1\nprint(\"Family Size column created sucessfully\")\n\n#Encoding Gender column from test data set\nds_test=ds_test.replace(to_replace='male',value=0)\nds_test=ds_test.replace(to_replace='female',value=1)\nX_test=ds_test\n\nX_test.head()","4a976d3c":"#Prediction test results\ny_pred_rf=classifier_rf.predict(X_test)\ny_pred_xgb=classifier_xgb.predict(X_test)\n\n#Converting 2 dimensional  y_pred array into single dimension \ny_pred_rf=y_pred_rf.ravel()\ny_pred_xgb=y_pred_xgb.ravel()\n\n#Creating submission data frame and subsequent csv file for submission\nsubmission_df_rf = pd.DataFrame(columns=['PassengerId', 'Survived'])\nsubmission_df_rf['PassengerId'] = X_test['PassengerId'].astype(int)\nsubmission_df_rf['Survived'] = y_pred_rf\nsubmission_df_rf.to_csv('submissions_rf.csv', header=True, index=False)\n\nsubmission_df_xgb = pd.DataFrame(columns=['PassengerId', 'Survived'])\nsubmission_df_xgb['PassengerId'] = X_test['PassengerId'].astype(int)\nsubmission_df_xgb['Survived'] = y_pred_xgb\nsubmission_df_xgb.to_csv('submissions_xgb.csv', header=True, index=False)","f4f0068a":"#Apply K-fold in current model to check model accuracy\nfrom sklearn.model_selection import cross_val_score\naccuracies_rf = cross_val_score(estimator = classifier_rf, X = X_train, y = y_train, cv = 10)\naccuracies_xgb = cross_val_score(estimator = classifier_xgb, X = X_train, y = y_train, cv = 10)","943d5c4e":"#Checking accuracies for 10 fold in Random Forest and XG Boost Models\nprint(\"Accuracies for 10 Fold in Random Forest Model is {}\".format(accuracies_rf))\nprint(\"Accuracies for 10 Fold in XG Boost Model is {}\".format(accuracies_xgb))","e3994fb7":"#Checking Mean and Standard Deviation between Accuracies\nprint(\"Mean Accuracy for Random Forest Model is {}\".format(accuracies_rf.mean()))\nprint(\"Mean Accuracy for XG Boost Model is {}\".format(accuracies_xgb.mean()))\nprint(\"Standard Deviation for Random Forest Model is {}\".format(accuracies_rf.std()))\nprint(\"Standard Deviation for XG Boost Model is {}\".format(accuracies_xgb.std()))","8335a286":"#Importing required library for Grid Search\nfrom sklearn.model_selection import GridSearchCV\n#Create the parameter grid based on the results of random search\nparam_grid = { 'bootstrap': [True],\n              'max_depth': [80, 90, 100, 110],\n              'max_features': [2, 3], \n              'min_samples_leaf': [3, 4, 5],\n              'min_samples_split': [8, 10, 12],\n              'n_estimators': [100, 300, 500, 1000] }\ngrid_search = GridSearchCV(estimator = classifier_rf, param_grid = param_grid,cv = 3, n_jobs = -1) \ngrid_search = grid_search.fit(X_train, y_train)","52b5a34a":"#Getting the best params\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(\"Best Accuracy for Random Forest Classifier is {}\".format(best_accuracy))\nprint(\"Best Parameters for Random Forest Classifier is {}\".format(best_parameters))","2c134fca":"#Creating new classifier and fitting Training set\nclassifier_rf_new = RandomForestClassifier(n_estimators = 719,\n                                           bootstrap=False,\n                                           max_depth=464,\n                                           max_features=0.3,\n                                           min_samples_leaf=1,\n                                           min_samples_split=2,\n                                           random_state=42)\nclassifier_rf_new.fit(X_train, y_train)","600e101a":"print(\"Predicting Results from new Classifier and Converting into Submission file\")\n# Predicting the Train set results\ny_pred_rf_new=classifier_rf_new.predict(X_test)\n#Converting 2 dimensional  y_pred array into single dimension \ny_pred_rf_new=y_pred_rf_new.ravel()\n#Creating submission data frame and subsequent csv file for submission\nsubmission_df_rf_new = pd.DataFrame(columns=['PassengerId', 'Survived'])\nsubmission_df_rf_new['PassengerId'] = X_test['PassengerId'].astype(int)\nsubmission_df_rf_new['Survived'] = y_pred_rf_new\nsubmission_df_rf_new.to_csv('submissions_rf_new.csv', header=True, index=False)\nprint(\"Created Submission file from new classifier successfully\")","7a2871a4":"## Missing Values","6dc5de89":"We can see correlation between 'Survived' and 'Fare' but other variables are not directly related with Survived but related to other variable\n\n\ud83d\udccc **Take Away Points**\n\n* Age is correlated to Fare and Fare is correlated to Survived and our previous analysis also show how Age played role in survival\n* SibSp and Parch are realted to each other and also both related to Fare which make sense becuase more number of people means more fare, by virtue of this both can be related to Survived, we can further analysis on this in next section of Feature Engineering","7f056804":"Before start with modeling let's check with missing values in training data set columns.\n\n","cf97cb55":"After learning from some exceptional work from incredible Kaggler's I have decided to come up with this Kernel for Exploratory Data Analysis and Feature Engineering along with basic data modeling and model evaluation. \n\nThis is primarily for newbies in Machine Learning to introduce them with these terms and ways to use them. I have kept the language, code, and explanation as simple as possible for ease of understanding.\n\nI have used [dataset](https:\/\/www.kaggle.com\/c\/titanic\/data) which is provided by <a>Kaggle<\/a> for [Titanic: Machine Learning from Disaster Competition](https:\/\/www.kaggle.com\/c\/titanic\/overview)\n\nIf you like the work please **upvote** and do leave a comment for any feedback.","89d32cc7":"Let's try XGBoost classifier model also","f1331f13":"Now , only one value is missing from 'Fare' column which we can fill by median fare","ee03b335":"\ud83d\udccc **Take Away Points**\n* Majority of passengers aged more than **20** years and less than **50** years\n* Maximum number of passengers (30 in numbers) are of **24** years old\n* There are **164** passengers who are less than 20 years old  \n\nLet's break this further and add **Gender** with Age. First let's see how diversified among passenger","9556bee6":"\ud83d\udccc **Take Away Points**\n\n* Chances of survival are less for large Family (>5 memebers) \n* If family size is small then main passenger gender decides on survival , this prove previous deduction that gender played major role in survival\n* Survival data is marked for main passenger and not for whole family, whereas family members name must be there in the list and they may or may not survived . In other words on just looking at survival column we can not dedeuce that fate of all family member were same","9d5b91c2":"# Exploratory Data Analysis","a096896f":"Let's start Feature Engineering with creating new variable **Family Size** by adding **SibSp** , **Parch** and **One**(Current Passenger)","f8cde52f":"# Feature Engineering","6f8c465f":"It's pretty evident from above graph that majority of female passengers are survived\n\n\ud83d\udccc **Take Away Points**\n\n * Majority of Male passengers aged between 20 to 50 years had not survived . It means **most of the young men had not survived this disaster**\n * Oldest male passenger aged 80 years ,had survived\n * Age and Sex were major factors in deciding passenger's fate\n \n Now , let's see **Pclass** variable relation with survival","a8df37f7":"# Model Improvement ","3aea6326":"Now we will see how Family size will is realted with Survived variable","cd50ea1e":"## Features Analysis and Explanation","7c3a174a":"We can see that there are some number of passengers who less than 20 years , let's calculate the count","62be5427":"For this data set I will be using Random Forest Classifier , we can use other classifier models but for the sake of simplicity I will use only one model here","ef56acdd":"Let's check the data set information one more time to verify missing values","380fe485":"Let's check one more time for missing values","f8f68a3c":"Let's start with moving target and feature variables","c2e6a47f":"# Modeling and Prediction","2906b3f1":"Let's check one more time for missing values","e91e1615":"Well , there is no missing values in our train data set but before we start our modeling let's encode 'Sex' column as this is the only column left as categorical variable.\n\nAs this column consist of only two values let's encode this with **1** for feamle and **0** for male , we can use hot encoder also but for starters let's avoid that as we have very simple column to encode","e8a10c2e":"Before predicting from test set we need to clean test data set to make it equivalent with training data set i.e. need to drop unnecessary columns and encoded Sex column and missing values\n\nLet's start with missing values","40f679ae":"With above data it's evident that Random Forest Accuracy and Standard Deviation is better than XG boost for training data.\n\nTo get best parameters for Random Forest let's do **Grid Search** for Random Forest model","db28b935":"It's quiet evident that number of male passengers are almost double of female passengers.\n\nLet's see how many female and male survived.","47b42f0e":"It's clear that female passengers with lowest fare also survived the disaster and passenger with highest fare also survived , irrespective of the gender and this proves our theory that **Socio Economic Status played an improtant role in survival**\n\nAt last we will see **Embarked** variable's impact on survival","996aaa01":"Now , we will try to see some relation between these features.\n\nFirst start with passenger's **Age**","7c36a481":"> **If you like the work please upvote and do leave a comment for any feedback**","76c7782f":"After importing the library let's check how many rows are present in Train and Test set.","ea77318a":"Let's try to understand each features in training data set","2aadb511":"Submission with this is not increasing our score from previous Random Classifier Model , there can be several reason behind this but I found [this](https:\/\/towardsdatascience.com\/optimizing-hyperparameters-in-random-forest-classification-ec7741f9d3f6) explanation very useful","d2a964c0":"Now, every feature is in same scale let's start with Data Modeling and Prediction","60ca9309":"Let's start with importing libraries and data set","143389e3":"Only 'Age' is having missing values and we can replace missing values with median age , but putting median age for whole data set is not a good idea becuase passenger belongs to different Age group.\n\nTo overcome this we can calculate Median age based on 'Pclass' and 'Sex'\n\n> **Note: I took some help from this [Kernel](https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial#1.-Exploratory-Data-Analysis) for this median value of Age**","0708381c":"* **Survived** is a target variable where survival is predicted in binanry format i.e. **0** for Not Survived and **1** for Survived\n* **PassengerId** and **Ticket** variables can be assumed as Random unique Identifiers of Passengers and they don't have any impact on outcome ,hence we can ignore them\n* **Pclass** is an ordinal datatype for the ticket class,it's a passenger's socio-economic status which played an important role in survival , it may impact target variable so we will keep it in our train data set. It's unique values are **1 = Upper Class** , **2 = Middle Class** and **3 = Lower Class**\n* **Name** It could be used to derive socio-economic status from title (like Doctor or Master)\n* **Sex** Gender played an important role in survival , so we will keep this in our feature list \n* **SibSp and Parch** These two variables represent total number of the passenger's siblings\/spouse and parents\/children on board , it could be used to create a new variable 'Family Size'. This is an example of Feature Engineering\n* **Age** , Like Gender Age could also played a role in survival , will keep this is our feature list\n* **Fare** , price of ticket also represnt socio-economic status , let's keep this also \n* **Cabin** this is Cabin number of the passenger and it can be used in feature engineering to get an approximate position of passenger when accident happened,also from deck level we can deduce socio-economic status. However, on closer look at data it looks like that there are many null values so we can drop this column from our feature list\n* **Embarked** is port of embarkation and it is a categorical feature which has following **3** unique values **C = Cherbourg**,**Q = Queenstown** and **S = Southampton** , this may have impact on target variable we will keep this variable for now.\n\nLet's drop **Ticket** and **Cabin** columns from training data set","be12edc4":"More than half of the passengers were travelling in **Lower Class**. Let's see how survival is linked with Pclass","292efe2c":"Above Models are giving good score(**0.779**) but this can be improved , let's try **K-Fold techniques** to check model accuracy","8a59a96a":"Let's check correlation cofficent between our features\n","2942ec70":"Well,it looks like that majority of young passengers who are travelling in lower class had not survived\n\n\ud83d\udccc **Take Away Points**\n\n * Majority of young male passengers aged between 20 to 50 years and travelling in lower class had not survived \n * Oldest male passenger who survived the disaster was travelling in upper class\n * Young men who survived the disaster were travelling in upper class\n * Passengers Socio Economic Status palyed a vital role in survival\n \n > **We can deduce one thing clearly ,if passenger was man aged between 20-50 and not so rich at the time of travel then their chances of survival were very less**\n\nTo support our Socio Economic Status theory let's focus on one more variable **Fare**\n\n\n","20aba316":"With this we are ready to get predicted values and submission file","34bcfd95":"## Data Overview","0890ccdf":"Majority of passengers embarked from **Southampton** , it may be the journey start point","546de4f9":"Here also Age is missing , let's fill in the similar way how we did it for training data","82299061":"As there is no direct releation between Embarked and Survived variables we can drop this from our feature list.\n\nAlso, we can drop 'Name' column from our feature list as we have other features\/columns for Socio Economic Status relation with survival\n\n","5d19083a":"# Objective","65586af6":"With above best parameters let's create one more classifier and predict from test data"}}