{"cell_type":{"6cd9f736":"code","fbf567a4":"code","8dfc1dbe":"code","b77def94":"code","6c7ff53a":"code","6e02a8d6":"code","6bdf7b5d":"code","531562df":"code","26e4d5f6":"code","55876ac8":"code","246a89db":"code","6839b1c7":"code","bf6c086e":"code","43c6b38e":"code","700720d3":"code","3c4f46f5":"code","174cbafd":"code","9f36d150":"code","fbc70735":"markdown","6a0e5668":"markdown","f099774e":"markdown","2f79f3d9":"markdown","1250f232":"markdown","0dbe722f":"markdown","64b416e4":"markdown","f86eae4c":"markdown","6a811850":"markdown","eb16d507":"markdown","6582a934":"markdown","6fb2005f":"markdown","4a911e22":"markdown","8178802c":"markdown","f886f49f":"markdown","0b208e30":"markdown","50f04aef":"markdown","21648e5e":"markdown","959e86cc":"markdown","d5678461":"markdown","2f5244f3":"markdown","a93e1058":"markdown","b5b7fd82":"markdown","04a8cb60":"markdown","4a67e844":"markdown","b258d30b":"markdown","4faaef38":"markdown","f60755cb":"markdown","d830e48f":"markdown","8232943d":"markdown","ec562dad":"markdown","450c8343":"markdown","3da888f0":"markdown","6af28753":"markdown"},"source":{"6cd9f736":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data visualization\nimport seaborn as sns # statistical data visualization\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n","fbf567a4":"# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","8dfc1dbe":"# Load and preview data\ndf = pd.read_csv('\/kaggle\/input\/california-housing-prices\/housing.csv')\ndf.head()","b77def94":"# View summary of data\ndf.info()","6c7ff53a":"# Plot the distribution of total bedrooms\ndf['total_bedrooms'].value_counts().plot.bar()","6e02a8d6":"# Imputing missing values in total_bedrooms by median\ndf['total_bedrooms'].fillna(df['total_bedrooms'].median(), inplace=True)","6bdf7b5d":"# now check for missing values in total bedrooms\ndf.isnull().sum()","531562df":"# Declare feature vector and target variable\nX = df[['longitude','latitude','housing_median_age','total_rooms',\n        'total_bedrooms','population','households','median_income']]\ny = df['median_house_value']","26e4d5f6":"# Split the data into train and test data:\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)","55876ac8":"# Build the model with Random Forest Classifier :\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(max_depth=6, random_state=0, n_estimators=10)\nmodel.fit(X_train, y_train)","246a89db":"y_pred = model.predict(X_test)","6839b1c7":"from sklearn.metrics import mean_squared_error\nmse = mean_squared_error(y_test, y_pred)**(0.5)\nmse","bf6c086e":"# import shap library\nimport shap\n\n# explain the model's predictions using SHAP\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_train)\n\n# visualize the first prediction's explanation \nshap.initjs()\nshap.force_plot(explainer.expected_value, shap_values[0,:], X_train.iloc[0,:])","43c6b38e":"# visualize the training set predictions\nshap.force_plot(explainer.expected_value, shap_values, X_train)","700720d3":"shap_values = shap.TreeExplainer(model).shap_values(X_train)\nshap.summary_plot(shap_values, X_train, plot_type=\"bar\")","3c4f46f5":"shap.summary_plot(shap_values, X_train)","174cbafd":"shap.dependence_plot('median_income', shap_values, X_train)","9f36d150":"shap.dependence_plot('longitude', shap_values, X_train)","fbc70735":"## **3.8 Generate Predictions** <a class=\"anchor\" id=\"3.8\"><\/a>\n\n[Table of Contents](#0.1)","6a0e5668":"# **8. References** <a class=\"anchor\" id=\"8\"><\/a>\n\n[Table of Contents](#0.1)\n\nThe ideas and concepts are taken from following books and websites:-\n\n- 1 https:\/\/github.com\/slundberg\/shap\n- 2 https:\/\/www.kaggle.com\/dansbecker\/shap-values\n- 3 https:\/\/www.kaggle.com\/dansbecker\/advanced-uses-of-shap-values\n- 4 https:\/\/christophm.github.io\/interpretable-ml-book\/\n- 5 https:\/\/christophm.github.io\/interpretable-ml-book\/shapley.html\n- 6 https:\/\/christophm.github.io\/interpretable-ml-book\/shap.html\n- 7 https:\/\/towardsdatascience.com\/explain-your-model-with-the-shap-values-bc36aac4de3d\n\n","f099774e":"<a class=\"anchor\" id=\"0.1\"><\/a>\n# **Table of Contents**\n\n1. [Interpretable Machine Learning](#1)\n2. [Introduction to SHAP library and Shapely Values](#2)\n  - 2.1 [Shapely Values](#2.1)\n  - 2.2 [SHAP Library](#2.2)\n3. [Python implementation of model development](#3)\n4. [SHAP Explanation Force Plots](#4)\n5. [SHAP Feature Importance](#5)\n6. [SHAP Summary Plot](#6)\n7. [SHAP Dependence Plot](#7)\n8. [References](#8) \n  \n","2f79f3d9":"- The target variable is `median_house_value`.","1250f232":"# **2. Introduction to SHAP library and Shapely values** <a class=\"anchor\" id=\"2\"><\/a>\n\n[Table of Contents](#0.1)\n\n\n- Python provides a library called [SHAP (SHapley Additive exPlanations)](https:\/\/christophm.github.io\/interpretable-ml-book\/shap.html) by Lundberg and Lee, It is used to explain individual model predictions. SHAP is based on the game theoretically optimal [Shapley Values](https:\/\/christophm.github.io\/interpretable-ml-book\/shapley.html#shapley).\n\n- Let's first talk about Shapley Values.","0dbe722f":"- The above plot shows the SHAP summary plot. The summary plot combines feature importance with feature effects. \n\n- Each point on the summary plot is a Shapley value for a feature and an instance. The position on the y-axis is determined by the feature and on the x-axis by the Shapley value. The color represents the value of the feature from low to high. Overlapping points are jittered in y-axis direction, so we get a sense of the distribution of the Shapley values per feature. The features are ordered according to their importance.\n\n- This plot is made of all the dots in the train data. It demonstrates the following information:\n\n  - *Feature importance*: Variables are ranked in descending order.\n  - *Impact*: The horizontal location shows whether the effect of that value is associated with a higher or lower prediction.\n  - *Original value*: Color shows whether that variable is high (in red) or low (in blue) for that observation.\n  - *Correlation*: A high level of the `median_income` has a high and positive impact on the `median_house_value`. The \u201chigh\u201d comes from the red color, and the \u201cpositive\u201d impact is shown on the X-axis. \n  \n- Similarly, `housing_median_age` is positively correlated with the target variable `median_house_value`.","64b416e4":"- The `total_bedrooms` distribution have skewed distribution. So, I will use median to fill the missing values.","f86eae4c":"## **3.5 Feature Vector and Target Variable** <a class=\"anchor\" id=\"3.5\"><\/a>\n\n[Table of Contents](#0.1)","6a811850":"## **3.9 Evaluating Performance** <a class=\"anchor\" id=\"3.9\"><\/a>\n\n[Table of Contents](#0.1)","eb16d507":"# **3. Python Implementation of Model Development** <a class=\"anchor\" id=\"3\"><\/a>\n\n[Table of Contents](#0.1) ","6582a934":"### **Interpretation**\n\n- The above plot shows features each contributing to push the model output from the base value (the average model output over the training dataset we passed) to the model output. Features pushing the prediction higher are shown in red and those pushing the prediction lower are in blue.\n\n- So, `housing_median_age` pushes the prediction higher and `median_income`,`latitude` and `longitude` pushes the prediction lower.\n\n- The base value of the `median_house_value` is 2.063e+5 = 206300.\n\n- The output value is 70189.83 with `housing_median_age=52`, `median_income=1.975`, `latitude=36.73` and  `longitude=-119.8`.","6fb2005f":" # **4. SHAP Explanation Force Plots** <a class=\"anchor\" id=\"4\"><\/a>\n\n[Table of Contents](#0.1)\n\n\n- We will use SHAP to explain individual predictions. We can use the fast TreeSHAP estimation method instead of the slower KernelSHAP method, since a random forest is an ensemble of trees.\n\n- Since SHAP computes Shapley values, the interpretation is the same as in the [Shapley value chapter](https:\/\/christophm.github.io\/interpretable-ml-book\/shapley.html#shapley. But with the Python shap package comes a different visualization: You can visualize feature attributions such as Shapley values as \u201cforces\u201d. Each feature value is a force that either increases or decreases the prediction. The prediction starts from the baseline. The baseline for Shapley values is the average of all predictions. In the plot, each Shapley value is an arrow that pushes to increase (positive value) or decrease (negative value) the prediction. These forces balance each other out at the actual prediction of the data instance.\n\n- The following figure shows SHAP explanation force plots for the California Housing Prices dataset.","4a911e22":"## **3.1 Initial Set-Up** <a class=\"anchor\" id=\"3.1\"><\/a>\n\n[Table of Contents](#0.1) ","8178802c":"- Now, suppose we want to know `longitude` and the variable that it interacts the most.\n\n- We can do `shap.dependence_plot(\u201clongitude\u201d, shap_values, X_train). \n\n- The plot below shows there exists an approximately linear but negative relationship between `longitude` and the target variable. This negative relationship is already demonstrated in the variable importance plot. It interacts with `median_income` variable frequently.","f886f49f":"- If we take many explanations such as the one shown above, rotate them 90 degrees, and then stack them horizontally, we can see explanations for an entire dataset as shown below.\n\n- The following plot is interactive. Just scroll the mouse and see the different values.","0b208e30":"# **6. SHAP Summary Plot** <a class=\"anchor\" id=\"6\"><\/a>\n\n[Table of Contents](#0.1)\n\n\n- The summary plot combines feature importance with feature effects. Each point on the summary plot is a Shapley value for a feature and an instance. The position on the y-axis is determined by the feature and on the x-axis by the Shapley value. \n\n- The color represents the value of the feature from low to high. Overlapping points are jittered in y-axis direction, so we get a sense of the distribution of the Shapley values per feature. The features are ordered according to their importance.\n","50f04aef":"## **3.4 Missing Value Treatment** <a class=\"anchor\" id=\"3.4\"><\/a>\n\n[Table of Contents](#0.1)","21648e5e":" # **5. SHAP Feature Importance** <a class=\"anchor\" id=\"5\"><\/a>\n\n[Table of Contents](#0.1)\n\n\n- The idea behind SHAP feature importance is simple. Features with large absolute Shapley values are important. Since we want the global importance, we average the absolute Shapley values per feature across the data.\n\n- Next, we sort the features by decreasing importance and plot them. The following figure shows the SHAP feature importance for the trained random forest model.","959e86cc":"[Go to Top](#0)","d5678461":"## **3.7 Build the model** <a class=\"anchor\" id=\"3.7\"><\/a>\n\n[Table of Contents](#0.1)","2f5244f3":"## **2.1 Shapley Values** <a class=\"anchor\" id=\"2.1\"><\/a>\n\n\n[Table of Contents](#0.1)\n\n\n- In terms of [Interpretable Machine Learning - Shapley Values](https:\/\/christophm.github.io\/interpretable-ml-book\/shapley.html#shapley), **Shapley Values** can be defined as-\n\n     **A prediction can be explained by assuming that each feature value of the instance is a \u201cplayer\u201d in a game    where the prediction is the payout. Shapley values \u2013 a method from coalitional game theory \u2013 tells us how to fairly distribute the \u201cpayout\u201d among the features.**\n\n\n- For an in-depth discussion of Shapley Values, please read the chapter - [Shapley Values](https:\/\/christophm.github.io\/interpretable-ml-book\/shapley.html#shapley).\n","a93e1058":"- The above plot shows the SHAP feature importance measured as the mean absolute Shapley values. \n\n- The variable `median_income` was the most important feature, changing the predicted `median_house_value` on average by 56000 on x-axis.\n\n- SHAP is based on magnitude of feature attributions. The feature importance plot is useful, but contains no information beyond the importances. For a more informative plot, we will next look at the summary plot.","b5b7fd82":"- We can see that `total_bedrooms` have missing values.","04a8cb60":"# **Most commonly used methods for explainability**\n\n- These methods do not rely on any particularity of the model. The advantage of these methods lies in their flexibility. Machine learning developers are free to use any machine learning model they like. The interpretation methods can be applied to any model. These methods are given below:-\n\n - 1 Shapley values (explained in this kernel)\n - 2 LIME \n - 3 Feature importance\n - 4 Feature interaction \n - 5 Surrogate Models ","4a67e844":"# **1. Interpretable Machine Learning** <a class=\"anchor\" id=\"1\"><\/a>\n\n[Table of Contents](#0.1)\n\n- Machine learning has great potential for improving products, processes and services. A dataset is supplied as input and algorithms produce the desired output. But, algorithms do not explain their predictions. It acts as a barrier to the adoption of machine learning. In this case, interpretable machine learning models come to the rescue.\n\n- Tim Miller - \u201cExplanation in Artificial Intelligence: Insights from the Social Sciences.\u201d defines interpretability as -\n\n    **\u201cthe degree to which a human can understand the cause of a decision in a model\u201d.** So it means it\u2019s something        that you achieve in some sort of \u201cdegree\u201d.       \n       \n- In the context of machine learning, interpretability helps us to understand how a model has made a particular decision. \n\n- Our model should be interpretable and they should also display the following traits:-\n\n  - 1 **Fairness**  -  Ensuring that predictions are unbiased and do not implicitly or explicitly discriminate against protected groups. An interpretable model can tell you why it has decided that a certain person should not get a loan, and it becomes easier for a human to judge whether the decision is based on a learned demographic (e.g. racial) bias.\n  - 2 **Privacy**  -  Ensuring that sensitive information in the data is protected.\n  - 3 **Reliability or Robustness**  -  Ensuring that small changes in the input do not lead to large changes in the prediction.\n  - 4 **Causality**  -  Check that only causal relationships are picked up.\n  - 5 **Trust**  -  It is easier for humans to trust a system that explains its decisions compared to a black box.   \n","b258d30b":"# **7. SHAP Dependence Plot** <a class=\"anchor\" id=\"7\"><\/a>\n\n[Table of Contents](#0.1)\n\n\n- The SHAP Dependence plot shows the marginal effect one or two features have on the predicted outcome of a machine learning model  It tells whether the relationship between the target and a feature is linear, monotonic or more complex. \n\n- We can create a dependence plot as follows:-","4faaef38":"## **3.6 Train-Test Split** <a class=\"anchor\" id=\"3.6\"><\/a>\n\n[Table of Contents](#0.1)","f60755cb":"**As always, I hope you find this kernel useful and your <font color=\"red\"><b>UPVOTES<\/b><\/font> would be highly appreciated**.","d830e48f":"<a class=\"anchor\" id=\"0\"><\/a>\n# **Explain your model predictions with Shapley Values**\n\n\nHello friends,\n\n\nIn this kernel, I will introduce you to **SHAP library** and **Shapley Values** in Python. These are used to explain your model predictions and get insights into the model development process.\n\nSo, let's get started.","8232943d":"## **3.2 Reading Data** <a class=\"anchor\" id=\"3.2\"><\/a>\n\n[Table of Contents](#0.1) ","ec562dad":"## **2.2 SHAP Library** <a class=\"anchor\" id=\"2.2\"><\/a>\n\n\n[Table of Contents](#0.1)\n\n\n- The SHAP library in Python has inbuilt functions to use Shapley values for interpreting machine learning models. It has optimized functions for interpreting tree-based models and a model agnostic explainer function for interpreting any black-box model for which the predictions are known.\n\n- Lundberg and Lee implemented SHAP in the [SHAP](https:\/\/github.com\/slundberg\/shap) Python package. This implementation works for tree-based models in the scikit-learn machine learning library for Python.\n\n- The SHAP authors proposed **KernelSHAP**, an alternative, kernel-based estimation approach for Shapley values inspired by [local surrogate models](https:\/\/christophm.github.io\/interpretable-ml-book\/lime.html#lime). \n\n- Also they proposed **TreeSHAP**, an efficient estimation approach for tree-based models. \n\n- [SHAP](https:\/\/github.com\/slundberg\/shap) comes with many global interpretation methods based on aggregations of Shapley values. We will demonstrate them in this kernel.\n\n- For an in-depth discussion of [SHAP](https:\/\/github.com\/slundberg\/shap) , please read the chapter - [SHAP](https:\/\/christophm.github.io\/interpretable-ml-book\/shap.html).\n\n- Now, let's get to the implementation.","450c8343":"## **3.3 View Summary of data** <a class=\"anchor\" id=\"3.3\"><\/a>\n\n[Table of Contents](#0.1)","3da888f0":"- The function automatically includes another variable that the chosen variable interacts most with. The above plot shows there is an approximately linear and positive trend between `median_income` and the target variable, and `median_income` interacts with `housing_median_age` frequently.","6af28753":"There are no missing values in the dataset."}}