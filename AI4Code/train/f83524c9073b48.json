{"cell_type":{"6c941381":"code","2782e025":"code","f8c8d90f":"code","52813152":"code","4664ead4":"code","4ddb5541":"code","6f14a5f8":"code","55f9fb63":"code","f74c8777":"code","5b7ce0af":"code","837a127d":"code","e072d0f4":"code","38834b28":"code","9bc868d6":"code","199e2e2e":"code","1721c213":"code","1204fb7c":"markdown","96140964":"markdown","f64c2700":"markdown","5cf1767b":"markdown","8b63754f":"markdown","91bd6afa":"markdown","ca4ecfd3":"markdown","f2b6e16c":"markdown","b87546bb":"markdown","6fd0ac8c":"markdown","bb05a916":"markdown","a3e9529a":"markdown","077c7664":"markdown","6cd9dd36":"markdown","cd9320c9":"markdown","eaf875b4":"markdown","305d48b2":"markdown","1399c73e":"markdown"},"source":{"6c941381":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2782e025":"# load csv files from titanic\n\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntrain.head()","f8c8d90f":"# view types of the data\ntrain.info()","52813152":"# view statistics of the data\ntrain.describe()","4664ead4":"# visualise the data\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.pairplot(train)\n\nplt.show()","4ddb5541":"# drop columns for training set\nuseful_train = train.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin'])\nuseful_train.head()","6f14a5f8":"# label encoding\nlabel_train = useful_train.copy()\nlabel_train[['Sex', 'Embarked']] = label_train[['Sex', 'Embarked']].astype('category')\ncat_columns = label_train.select_dtypes(['category']).columns\nlabel_train[cat_columns] = label_train[cat_columns].apply(lambda x: x.cat.codes)\n\nlabel_train.head()","55f9fb63":"# one hot encoding\nhot_train = pd.get_dummies(useful_train)\nhot_train.head()","f74c8777":"useful_train.isna().sum()","5b7ce0af":"# missing value handling function\ndef missing_handling(data):  \n    # deletion\n    del_data = data.dropna()\n\n    # mean\n    mean_data = data.fillna(data.mean())\n\n    # median\n    median_data = data.fillna(data.median())\n\n    # mode, not working :(\n    # mode_data = data.fillna(data.mode())\n    \n    # KNN\n    if data is not useful_train:\n        from sklearn.impute import KNNImputer\n\n        dummy_data = data.copy()\n        imputer = KNNImputer(n_neighbors=5)\n        knn_data = pd.DataFrame(imputer.fit_transform(dummy_data),columns = dummy_data.columns)\n        \n        return del_data, mean_data, median_data, knn_data #, mode_data, knn_data\n    return del_data, mean_data, median_data #, mode_data\n\n# test\nfor each in missing_handling(label_train):\n    print(each.isna().sum())","837a127d":"# normalisation function\nfrom sklearn import preprocessing\n\ndef normalise(data):\n    std_scale = preprocessing.StandardScaler().fit(data[['Age', 'Fare']])\n    std_norm = data.copy()\n    std_norm[['Age', 'Fare']] = std_scale.transform(data[['Age', 'Fare']])\n\n    minmax_scale = preprocessing.MinMaxScaler().fit(data[['Age', 'Fare']])\n    minmax_norm = data.copy()\n    minmax_norm[['Age', 'Fare']] = minmax_scale.transform(data[['Age', 'Fare']])\n    \n    return std_norm, minmax_norm\n\n# test\nstd_norm, minmax_norm = normalise(useful_train)\nstd_norm","e072d0f4":"# prediction models wrapped by models function\n\ndef train_models(X, y):\n    # decision tree\n    from sklearn.tree import DecisionTreeClassifier\n    decisiont_clf = DecisionTreeClassifier(random_state = 0)\n    decisiont = decisiont_clf.fit(X, y)\n    \n    # linear regression\n    from sklearn.linear_model import LinearRegression\n    reg = LinearRegression().fit(X, y)\n    \n    # random forest\n    from sklearn.ensemble import RandomForestClassifier\n    randomf_clf = RandomForestClassifier(random_state = 0)\n    randomf = randomf_clf.fit(X, y)\n    \n    # neural network\n    from sklearn.neural_network import MLPClassifier\n    nn_clf = MLPClassifier(random_state = 0)\n    nn_clf.learning_rate_init = 0.05\n    nn = nn_clf.fit(X, y)\n    \n    # gradient boosting\n    from sklearn.ensemble import GradientBoostingClassifier\n    gradboost_clf = GradientBoostingClassifier(random_state=0)\n    gradboost = gradboost_clf.fit(X, y)\n    \n    return decisiont, reg, randomf, nn, gradboost","38834b28":"# data splitting function for evaluation\ndef split(data):\n    from sklearn.model_selection import train_test_split\n    target = data['Survived']\n    data = data.drop(columns=['Survived'])\n    X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.30, random_state=0)\n    \n    return X_train, X_test, y_train, y_test\n\n# missing value output options:\n# default: deletion, mean, median, \/mode\n# other: deletion, mean, median, \/mode, knn\n\ndef simple_score(encoding_method):\n    arr1 = []\n    for methods in missing_handling(encoding_method):\n        arr2 = []\n        X_train, X_test, y_train, y_test = split(methods)\n        # simple scorer\n        for each in train_models(X_train, y_train):\n            arr2.append(each.score(X_test, y_test))\n        arr1.append(arr2)\n    return arr1\n\ndef index_handler(encoder):\n    index = ['Deletion', 'Mean', 'Median', 'KNN']\n    holder = []\n    if 'Default' in encoder:\n        index = ['Deletion', 'Mean', 'Median']\n    for each in index:\n        holder.append(encoder + each)\n    return holder\n\nprint(index_handler('Label '))\n\n# compare performance of each method, likely through a visual representation\nheadings = ['Decision Tree', 'Linear Regression', 'Random Forest', 'Neural Network', 'Gradient Boosting']\n\n#default = pd.DataFrame(data=simple_score(useful_train), index=index_handler('Default '))\ndf = pd.DataFrame(data=simple_score(label_train), index=index_handler('Label '))\nhot = pd.DataFrame(data=simple_score(hot_train), index=index_handler('Hot '))\n\ndf = df.append(hot)\ndf.columns = headings\nplt.figure(figsize=(10,4))\nsns.lineplot(data=df).set_title(\"Performance of Different Classifiers\")","9bc868d6":"# normalised dataset test\n\nstd_label, minmax_label = normalise(label_train)\nstd_hot, minmax_hot = normalise(hot_train)\n\n# compare performance of each method, likely through a visual representation\nheadings = ['Decision Tree', 'Linear Regression', 'Random Forest', 'Neural Network', 'Gradient Boosting']\n\n#default = pd.DataFrame(data=simple_score(useful_train), index=index_handler('Default '))\ndf = pd.DataFrame(data=simple_score(std_label), index=index_handler('Std Label '))\ndf1 = pd.DataFrame(data=simple_score(minmax_label), index=index_handler('MinMax Label '))\nhot1 = pd.DataFrame(data=simple_score(std_hot), index=index_handler('Std Hot '))\nhot2 = pd.DataFrame(data=simple_score(minmax_hot), index=index_handler('MinMax Hot '))\n\ndf = df.append(df1)\ndf = df.append(hot1)\ndf = df.append(hot2)\n\ndf.columns = headings\nplt.figure(figsize=(15,6))\nplt.xticks(rotation=90)\nsns.lineplot(data=df).set_title(\"Performance of Different Classifiers\")","199e2e2e":"# test set pre-processing\n# drop columns\nuseful_test = test.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin'])\n\n# label encoding\nlabel_test = useful_test.copy()\nlabel_test[['Sex', 'Embarked']] = label_test[['Sex', 'Embarked']].astype('category')\ncat_columns = label_test.select_dtypes(['category']).columns\nlabel_test[cat_columns] = label_test[cat_columns].apply(lambda x: x.cat.codes)\n\nlabel_test.head()\n\n# one hot encoding\nhot_test = pd.get_dummies(useful_test)\n\n# missing value handling\ndelete, mean, median, knn = missing_handling(hot_test)\nstd, minmax = normalise(median)","1721c213":"# use best classifier - could be automated\nfrom sklearn.ensemble import GradientBoostingClassifier\nbest_clf = GradientBoostingClassifier(random_state=0)\ndelete, mean, median, knn = missing_handling(hot_train)\nstd1, minmax1 = normalise(median)\nX_train, X_test, y_train, y_test = split(minmax1)\nbest_clf.fit(X_train, y_train)\n\n# output prediction\nprediction = best_clf.predict(minmax)\n\n# transform to dataframe\npred = pd.DataFrame(data = prediction)\nid = pd.DataFrame(data = test['PassengerId'])\n\n# generate csv file for submission\noutput = id\noutput.set_index('PassengerId')\noutput.insert(loc=1, column='Survived', value=pred)\noutput.set_index('PassengerId')\noutput.to_csv('submission.csv', index=False)","1204fb7c":"**Possible Missing Values**\n* Age\n* Cabin\n* Embarked","96140964":"Start by removing columns that do not appear useful for modelling\n* PassengerId: just a form of identification\n* Name: another form of identification, though their titles could be of use\n* Ticket: doesn't seem to be of use, with random numbers\n* Cabin: too many missing values","f64c2700":"# General data preparation","5cf1767b":"So we found 177 missing values in Age, and 2 in Embarked\n\nMultiple methods exist to deal with missing values. As a learning experience many of them will be used and results will be compared.\n\nMethods:\n* Deletion\n* Mean\n* Median\n* Mode\n* KNN","8b63754f":"# **Titanic Dataset - Learning and Exploration**","91bd6afa":"# Normalisation (Outlier Treatment)","ca4ecfd3":"Achieved score of: 0.78468\n\nCould be improved upon by:\n- parameter optimization\n- better outlier and anomaly treatment\n- inclusion of titles from name as data\n- trying more predictive models","f2b6e16c":"# Feature Engineering\n\nMethod #1 Convert categorical data to numerical (label encoding)\nCons: introduces ranking\/order\n\nMethod #2 Binarise columns (one hot encoding)\nCons: dummy variable trap, multicollinearity","b87546bb":"# Sections:\n1. Data Understanding\n    - Data exploration\n2. Data Preparation\n    - General data preparation\n    - Feature engineering\n    - Missing value handling\n    - Outlier detection and treatment\n3. Modelling\n4. Validation and Evaluation\n","6fd0ac8c":"# Modelling\n\nMany different models will be used. The models are:\n    * speedy options -> decision tree, linear regression\n    * accurate options -> random forest, neural network, gradient boosting tree","bb05a916":"The mode imputation for Age does not seem to work, possibly due to it being a numerical value\n\n*Mode imputation for Age is removed*","a3e9529a":"# Data Exploration","077c7664":"# Output Prediction","6cd9dd36":"**Data Types**\n\nNumerical: PassengerId, Survived, Pclass, Age, SibSp, Parch, Fare\n\nCategorical: Name, Sex, Ticket, Cabin, Embarked","cd9320c9":"Missing values appear to be in:\n* Age\n* Cabin\n* Embarked\n\nCabin was removed, so no further analysis required","eaf875b4":"# Implementation\n\nGeneration of the Normalisation, Missing Value, Encoding Combinations","305d48b2":"Outliers can be spotted on the data:\n* passengers with a fare price of 500\n* passenger with an age of 80\n\n*Treatment of outliers will be dealt with through normalisation*","1399c73e":"# Missing value handling"}}