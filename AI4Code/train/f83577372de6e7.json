{"cell_type":{"48efc7e5":"code","2baa4454":"code","a927e675":"code","462ccfb5":"code","aaef2a6f":"code","b03053fb":"code","caefd302":"code","01b4d107":"code","26c35282":"code","e162ae90":"code","d6dec3af":"code","f9a241c3":"code","f37c1b8d":"code","2e1fc647":"code","e223b349":"code","6ff0ff18":"code","cc5cc56d":"code","77028bbf":"code","3a97f1f0":"code","006b3c5c":"code","6af4cc03":"code","608324b4":"code","32ea02d0":"code","69323ddb":"code","95047603":"code","3aae1203":"markdown","a1789dfd":"markdown","4f73f120":"markdown","0ec78057":"markdown","f08f16ee":"markdown","2a20e800":"markdown","46c56c05":"markdown","3d7aabed":"markdown","a8d80b04":"markdown","0fbf775d":"markdown","0910ea55":"markdown","0427f74f":"markdown","0285a3b6":"markdown"},"source":{"48efc7e5":"import numpy as np\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom scipy.sparse import hstack\n\ntrainDataSet = pd.read_csv('..\/input\/train.csv')\ntestDataSet = pd.read_csv('..\/input\/test.csv')\ntrainDataSet.head\nclass_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\nprint (trainDataSet.shape)\nprint (testDataSet.shape)\nprint (trainDataSet.columns)","2baa4454":"def cleanText(text):\n    text = text.lower()\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"can not \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n    text = re.sub('\\W', ' ', text)\n    text = re.sub('\\s+', ' ', text)\n    text = re.sub('\\d+', ' ', text)\n    text = text.strip(' ')\n    return text","a927e675":"for className in class_names:\n    print (trainDataSet[className].value_counts())","462ccfb5":"print('Percentage of comments that are not labelled:')\nprint(len(trainDataSet[(trainDataSet['toxic']==0) & \n             (trainDataSet['severe_toxic']==0) & \n             (trainDataSet['obscene']==0) & \n             (trainDataSet['threat']== 0) & \n             (trainDataSet['insult']==0) &\n             (trainDataSet['identity_hate']==0)]) \/ len(trainDataSet))","aaef2a6f":"tempToxicDataSet = trainDataSet[trainDataSet['toxic'] == 0][0:1]\ntempInsultDataSet = trainDataSet[trainDataSet['toxic'] == 1][0:1]\nframes = [tempToxicDataSet, tempInsultDataSet]\ntempTrainDataSet = pd.concat(frames)\nprint (tempTrainDataSet.shape)\n\ntempTestDataSet = testDataSet[0:1]\nprint (tempTestDataSet.shape)","b03053fb":"train_text = tempTrainDataSet['comment_text']\ntrain_target = tempTrainDataSet.loc[:, class_names]\ntest_text = tempTestDataSet['comment_text']\nall_text = pd.concat([train_text, test_text])\nStringData = \"\"\nfor i in all_text:\n    StringData += i\nStringData","caefd302":"print (\"Original Text - \",StringData,\"\\n\")\nsentences = nltk.sent_tokenize(StringData)\nprint (len(sentences))\nsentences","01b4d107":"words = nltk.word_tokenize(StringData)\nprint (len(words))\nwords","26c35282":"from nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\n\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\nfor i in range(len(sentences)):\n    print (\"Actual Sentence - \",sentences[i],\"\\n\")\n    stemmingWords = nltk.word_tokenize(sentences[i])\n    lemmatizingWords = nltk.word_tokenize(sentences[i])\n    stemmedWords = [stemmer.stem(word) for word in stemmingWords]\n    lemmatizedWords = [lemmatizer.lemmatize(word) for word in lemmatizingWords]\n    print (\"Stemmed Words - \\n\")\n    print (stemmedWords)\n    print (\"Lemmatized Words - \\n\")\n    print (lemmatizedWords)\n    print (\"____________________________________________\")","e162ae90":"from nltk.corpus import stopwords\n\nfor i in range(len(sentences)):\n    print (\"Actual Sentence - \",sentences[i],\"\\n\")\n    words = nltk.word_tokenize(sentences[i])\n    newwords = [word for word in words if word not in stopwords.words('english')]\n    print (newwords)\n    print (\"____________________________________________\")","d6dec3af":"words = nltk.word_tokenize(StringData)\ntagged_words = nltk.pos_tag(words)\nwords_tags = []\nfor tw in tagged_words:\n    words_tags.append(tw[0]+\"_\"+tw[1])\nprint (words_tags)","f9a241c3":"import matplotlib\n\nwords = nltk.word_tokenize(StringData)\ntagged_words = nltk.pos_tag(words)\nnamedEntity = nltk.ne_chunk(tagged_words)\nprint (namedEntity)","f37c1b8d":"import re\n\nsentences = nltk.sent_tokenize(StringData)\nfor i in range(len(sentences)):\n    sentences[i] = sentences[i].lower()\n    sentences[i] = re.sub(r'\\W',' ',sentences[i])\n    sentences[i] = re.sub(r'\\s+',' ',sentences[i])\nsentences","2e1fc647":"word2count = {}\nfor data in sentences:\n    words = nltk.word_tokenize(data)\n    for word in words:\n        if word not in word2count.keys():\n            word2count[word] = 1\n        else:\n            word2count[word] += 1\nimport heapq\nfreq_words = heapq.nlargest(30, word2count, key=word2count.get)\nfreq_words","e223b349":"bagOfWords = []\nfor data in sentences:\n    vector = []\n    for word in freq_words:\n        if word in nltk.word_tokenize(data):\n            vector.append(1)\n        else:\n            vector.append(0)\n    bagOfWords.append(vector)\nbagOfWords = np.asarray(bagOfWords)\nbagOfWords","6ff0ff18":"# IDF Matrix\nword_idfs = {}\nfor word in freq_words:\n    document_count = 0\n    for data in sentences:\n        if word in nltk.word_tokenize(data):\n            document_count += 1\n    word_idfs[word] = np.log((len(sentences)\/document_count)+1) # +1 is the bias and standard way of calulating TF-IDF\nword_idfs","cc5cc56d":"# TF Matrix\ntf_matrix = {}\nfor word in freq_words:\n    doc_tf = []\n    for data in sentences:\n        frequency = 0\n        for w in nltk.word_tokenize(data):\n            if w == word:\n                frequency += 1\n        tf_word = frequency\/len(nltk.word_tokenize(data))\n        doc_tf.append(tf_word)\n    tf_matrix[word] = doc_tf\n# tf_matrix","77028bbf":"# TF_IDF MATRIX\ntfidf_matrix = []\nfor word in tf_matrix.keys():\n    tfidf = []\n    for value in tf_matrix[word]:\n        score = value * word_idfs[word]\n        tfidf.append(score)\n    tfidf_matrix.append(tfidf)\ntfidf_matrix","3a97f1f0":"X = np.asarray(tfidf_matrix)\nX = np.transpose(X)\nX.shape","006b3c5c":"import random\n\ntext = train_text[0]\nwords = nltk.word_tokenize(text)\nprint (text)\nprint (\"\\n\")\nngrams = {}\nn = 3\nfor i in range(len(words) - n):\n    gram = ' '.join(words[i:i+n])\n    if gram not in ngrams.keys():\n        ngrams[gram] = []\n    ngrams[gram].append(words[i+n])\n\ncurrentgram = ' '.join(words[0:n])\nresult = currentgram\nfor i in range(10):\n    if currentgram not in ngrams.keys():\n        break\n    possibilities = ngrams[currentgram]\n    nextItem = possibilities[random.randrange(len(possibilities))]\n    result  += ' '+nextItem\n    rwords = nltk.word_tokenize(result)\n    currentGram = ' '.join(rwords[len(rwords)-n:len(rwords)])\n    \nprint (ngrams)","6af4cc03":"from sklearn.decomposition import TruncatedSVD\n\ndataset = [\"The amount of population is increasing day by day\",\n           \"The concert was great\",\n           \"I love to see Gordan Ramsay cook\",\n           \"Google is introducing a new technology\",\n           \"AI Robots are the example of great technology present today\",\n           \"All of us were singing in the concert\",\n           \"We have launch campaigns to stop pollution and global warming\"\n          ]\ndataset = [line.lower() for line in dataset]\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(dataset)\n#print (X[0]) # (documentNumber , position) tfidf value\n\nlsa = TruncatedSVD(n_components = 4, n_iter = 100) # n_components are the number of concepts that you want to find from the data\nlsa.fit(X)\n#print (lsa.components_[3])\nterms = vectorizer.get_feature_names()\nconcept_words = {}\nfor i,comp in enumerate(lsa.components_):\n    componentTerms = zip(terms,comp)\n    sortedTerms = sorted(componentTerms, key=lambda x:x[1], reverse=True)\n    sortedTerms =  sortedTerms[:10]\n    concept_words[\"Concept \"+str(i)] = sortedTerms\n\nfor key in concept_words.keys():\n    sentence_scores = []\n    for sentence in dataset:\n        words = nltk.word_tokenize(sentence)\n        score = 0\n        for word in words:\n            for word_with_score in concept_words[key]:\n                if word == word_with_score[0]:\n                    score += word_with_score[1]\n        sentence_scores.append(score)\n    print(\"\\n\"+key+\"\\n\")\n    for sent_score in sentence_scores:\n        print (sent_score)","608324b4":"from nltk.corpus import wordnet\n\nsynonyms = []\nantonyms = []\n\nfor syn in wordnet.synsets(\"good\"):\n    for s in syn.lemmas():\n        synonyms.append(s.name())\n        for a in s.antonyms():\n            antonyms.append(a.name())\nprint (set(antonyms))\nprint (set(synonyms))","32ea02d0":"import matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n\nwc = WordCloud().generate(StringData)\nplt.figure(figsize=(15,15))\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()\n","69323ddb":"from nltk.corpus import stopwords\nimport string\n\noneSetOfStopWords = set(stopwords.words('english')+['``',\"''\"])\ntotalWords =[]\nSentences = trainDataSet['comment_text'].values\ncleanedSentences = \"\"\nfor i in range(0,5000):\n    cleanedText = cleanText(Sentences[i])\n    cleanedSentences += cleanedText\n    requiredWords = nltk.word_tokenize(cleanedText)\n    for word in requiredWords:\n        if word not in oneSetOfStopWords and word not in string.punctuation:\n            totalWords.append(word)\n    \nwordfreqdist = nltk.FreqDist(totalWords)\nmostcommon = wordfreqdist.most_common(50)\nprint(mostcommon)\n\nwc = WordCloud().generate(cleanedSentences)\nplt.figure(figsize=(15,15))\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","95047603":"import string\nprint (string.punctuation)","3aae1203":"|### SENT TOKENIZE - Convert the paragraph into sentences\n### WORD TOKENIZE - Convert a sentence or paragraph into words","a1789dfd":"## LATENT SYMANTIC ANALYSIS (LSA) - Latent Semantic Analysis is a technique for creating a vector representation of a document. Having a vector representation of a document gives you a way to compare documents for their similarity by calculating the distance between the vectors. This in turn means you can do handy things like classifying documents to determine which of a set of known topics they most likely belong to.\n#### We need to build SVD model . SVD - Singular Value Decomposition. Refer to the link for more information - https:\/\/machinelearningmastery.com\/singular-value-decomposition-for-machine-learning\/\n#### LSA  is used for following applications:\n#### 1.  Article Bucketing in websites\n#### 2. Finding relationships between articles\/words\n#### 3. Page indexing in search engines","4f73f120":"### Create the bag of words model","0ec78057":"####  N-GRAM MODEL - Need to know the Markov chains.  An n-gram is a contiguous sequence of n items from a given sample of text or speech. N-grams of texts are extensively used in text mining and natural language processing tasks. They are basically a set of co-occuring words within a given window and when computing the n-grams you typically move one word forward (although you can move X words forward in more advanced scenarios). For example, for the sentence \"The cow jumps over the moon\". If N=2 (known as bigrams), then the ngrams would be:\n* the cow\n* cow jumps\n* jumps over\n* over the\n* the moon\n\n#### Generating a word n-gram in the given code","f08f16ee":"### PARTS OF SPEECH TAGGING","2a20e800":"1. Convert all letters to lower case\n1. Remove all special characters\n1. Remove all extra spaces","46c56c05":"### REMOVAL OF STOP WORDS\n### Example - (to, be etc....)\n### Stopwords are not required while doing sentiment analysis and we need to carry out this step for better performance","3d7aabed":"#### STEMMING - Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma. Stemming is important in natural language understanding (NLU) and natural language processing (NLP).\n#### Example - (Intelligence or Intelligently => Intelligen)\n#### Problem - Produced intermediate representation of the word may not have any meaning \n#### LEMMATIZATION - Same as stemming but the intermediate representation has a meaning\n#### Example - (Intelligence or Intelligently => Intelligent)","a8d80b04":"\n### BAG OF WORDS MODEL\n### The bag-of-words model is a way of representing text data when modeling text with machine learning algorithms.\n### Study Link  - https:\/\/machinelearningmastery.com\/gentle-introduction-bag-words-model\/\n","0fbf775d":"#### Find the synonyms and antonyms for words - Refer to link - https:\/\/wordnet.princeton.edu\/","0910ea55":"### NAMED ENTITY RECOGNITION","0427f74f":"### Drawbacks of Bag Of Words model -\n### 1.  All words have same importance. \n### 2. No semantic information is preserved\n### Solution to the above problems is - TF-IDF model (Term Frequency - Inverse Document Frequency )\n### TF-IDF = (Number of occurences of word in the document \/ Total Number of words in a document)\n","0285a3b6":"### 1. Create the word count dictionary\n### 2. Get the first 5 frequently occuring words."}}