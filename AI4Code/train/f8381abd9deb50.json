{"cell_type":{"930ec9f5":"code","be257f43":"code","c1c9510a":"code","735e6dc1":"code","f7133115":"code","7511f800":"code","04afcb73":"code","b3e1efc2":"code","8554f109":"code","3ebea232":"code","4aa2c81e":"code","0f2c28f7":"code","b2a06848":"code","4c60ca7f":"code","5506fe11":"code","ca7c4eff":"code","c0a78f04":"markdown","90adf3fe":"markdown","fd57d664":"markdown","288e8ac9":"markdown","fe3b80e1":"markdown","1f0c38db":"markdown","a3253149":"markdown"},"source":{"930ec9f5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","be257f43":"import numpy as ny\nimport pandas as ps\nimport seaborn as sn\nimport plotly.express as plex\n\n# Modelling\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.metrics import classification_report\n\ninsure_set = ps.read_csv('\/kaggle\/input\/travel-insurance-prediction-data\/TravelInsurancePrediction.csv')\ninsure_set.head(10)","c1c9510a":"insure_set.describe() #info()","735e6dc1":"insure_set.isnull().sum() # Nulls are NULL. Cleanup may not be needed. Great !","f7133115":"#Dropping Unnamed: 0 column\ninsure_set = insure_set.drop(columns = ['Unnamed: 0'])\ninsure_set.head(10)","7511f800":"# Encoding a few labels before hand.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\ninsure_set['Employment Type'] = le.fit_transform(insure_set['Employment Type'])\ninsure_set['FrequentFlyer'] = le.fit_transform(insure_set['FrequentFlyer'])\ninsure_set['EverTravelledAbroad'] = le.fit_transform(insure_set['EverTravelledAbroad'])\ninsure_set['GraduateOrNot'] = le.fit_transform(insure_set['GraduateOrNot'])\n\ninsure_set.head(10)\n\n# Now we have all numeric dataset !","04afcb73":"# kaggle's notebook kept crashing at this point. Skipping the viz part. but hey, Heatmap has great observations.\n\nheatmap = plex.imshow(insure_set.corr(),title=\"Correaltion Heatmap\")\nheatmap.show()\n\n# TravelledAbroad, FreqFlyer,AnnualIncome seem to hold higher corr values !\n# Trying something new: Let's drop ChornicDiseases, Graduate and Age for Dim reduction.","b3e1efc2":"mini_insure_set = insure_set.drop(columns = ['Age','GraduateOrNot','ChronicDiseases'])  # Just some alteration.\nmini_insure_set.head(10)","8554f109":"y = mini_insure_set[['TravelInsurance']]\nX = mini_insure_set.drop(columns=['TravelInsurance']) \n\n# T-T-S\nX_train,X_test,y_train,y_test = train_test_split(X, y, train_size=0.8,random_state=42)\ntree_model = tree.DecisionTreeClassifier()\ntree_model.fit(X_train,y_train)\n#predict and score\ntree_model.predict(X_test)\ntree_model.score(X_test,y_test)\n","3ebea232":"from sklearn import metrics\ny_predicted = tree_model.predict(X_test)\n\nprint(\"\\n Tree Model R-Squared : \", r2_score(y_test, y_predicted) * 100)\nprint(\"\\n Tree Model Accuracy : \",metrics.accuracy_score(y_predicted,y_test))","4aa2c81e":"svm_model = SVC(kernel = 'poly',degree=5)\nsvm_model.fit(X_train,ny.ravel(y_train))\n#predict and score\nsvm_model.predict(X_test)\nsvm_model.score(X_test,y_test)\n\n","0f2c28f7":"y_predicted = svm_model.predict(X_test)\n\nprint(\"\\n SVM Model R-Squared : \", r2_score(y_test, y_predicted) * 100)\nprint(\"\\n SVM Model Accuracy : \",metrics.accuracy_score(y_predicted,y_test))","b2a06848":"forest_model = RandomForestClassifier(n_estimators=5)\nforest_model.fit(X_train,y_train.values.ravel())\n#predict and score\nforest_model.predict(X_test)\nforest_model.score(X_test,y_test)","4c60ca7f":"y_predicted = forest_model.predict(X_test)\n\nprint(\"\\n Forest Model R-Squared : \", r2_score(y_test, y_predicted) * 100)\nprint(\"\\n Forest Model Accuracy : \",metrics.accuracy_score(y_predicted,y_test))","5506fe11":"print(classification_report(y_predicted,y_test))","ca7c4eff":"# Kept it short and simple. Let me know your thoughts\/comments.","c0a78f04":"**3.1 Decision tree**","90adf3fe":"# Step 3: Modelling","fd57d664":"# Step 1: Primary checks and data cleanup.","288e8ac9":"**3.3 Random Forest Classifier**","fe3b80e1":"**Initial thoughts:**\n\nPurpose: To get a sensible classification result for given set of inputs.\n\n1. All labels do not hold numeric data, may be encoding\/transformation.\n2. Need to check nulls if valid\n3. Need to check for outliers\n4. Let's see if we can drop a few columns for closer swing at scores\/accuracy.","1f0c38db":"**3.2 Kernel SVM**","a3253149":"# Step 2: Visualizing the numeric dataframe."}}