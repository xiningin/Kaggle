{"cell_type":{"1abe51a9":"code","0b2248ab":"code","ba5c010e":"code","fc2e1357":"code","b2c2da91":"code","e53b823a":"code","cd75c673":"code","2cb661a9":"code","b48fd552":"code","e1d9f6a7":"code","8f170cee":"code","5bc1c09c":"code","fec4c574":"code","a0d9d4db":"code","bdc0e920":"code","c985e71c":"code","06536195":"code","c9a06e40":"code","ce8117c7":"code","405f8111":"code","3cb53581":"code","1cac5dbf":"code","c008d38b":"code","7747da42":"code","01ffb172":"code","b8fad2a9":"code","575bb221":"code","1284285e":"code","deeaac09":"code","c47314bd":"code","aeaacaaf":"code","2c9961a3":"code","034ee4ba":"code","c55237d5":"code","68d9b9fa":"code","3260db4c":"code","0fecc7d8":"code","d1f1721d":"code","408df206":"code","00a1c48d":"code","165571f3":"code","07162cbc":"code","1f61b8ce":"code","fd0c1f18":"code","3b86eeaf":"code","79c7a9f7":"markdown","85127d60":"markdown","46243efd":"markdown","590c83a2":"markdown","5f659907":"markdown","d17aba13":"markdown","ebb791f7":"markdown","56638fff":"markdown","0dbb9382":"markdown","d00d6305":"markdown","26f37c4f":"markdown","ef5ed6df":"markdown","a9b6243b":"markdown","1d099abd":"markdown","9df6fe6c":"markdown","6803cf17":"markdown","2b287614":"markdown","695cb941":"markdown","78da89f6":"markdown","3eca001d":"markdown","37f80dc3":"markdown","ae432611":"markdown"},"source":{"1abe51a9":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport time\nimport sys\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nseed = 782\nnp.random.seed(seed)","0b2248ab":"df = pd.read_csv(\"..\/input\/train.csv\")\ntrain = df.as_matrix()\n\ntrain_y = train[:,0].astype('int8')\ntrain_x = train[:,1:].astype('float64')\n\ntrain = None\n\nprint(\"Shape Train Images: (%d,%d)\" % train_x.shape)\nprint(\"Shape Labels: (%d)\" % train_y.shape)","ba5c010e":"df = pd.read_csv(\"..\/input\/test.csv\")\ntest = df.as_matrix().astype('float64')\nprint(\"Shape Test Images: (%d,%d)\" % test.shape)","fc2e1357":"def show_image(image, shape, label=\"\", cmp=None):\n    img = np.reshape(image,shape)\n    plt.imshow(img,cmap=cmp, interpolation='none')\n    plt.title(label)","b2c2da91":"%matplotlib inline\nplt.figure(figsize=(12,10))\n\ny, x = 5,10\nfor i in range(0,(y*x)):\n    plt.subplot(y, x, i+1)\n    ni = np.random.randint(0,train_x.shape[0],1)[0]\n    show_image(train_x[ni],(28,28), train_y[ni], cmp=\"gray\")\nplt.show()","e53b823a":"def count_exemple_per_digit(exemples):\n    hist = np.ones(10)\n\n    for y in exemples:\n        hist[y] += 1\n\n    colors = []\n    for i in range(10):\n        colors.append(plt.get_cmap('viridis')(np.random.uniform(0.0,1.0,1)[0]))\n\n    bar = plt.bar(np.arange(10), hist, 0.8, color=colors)\n\n    plt.grid()\n    plt.show()\n\ncount_exemple_per_digit(train_y)","cd75c673":"num = 1e9\n\nfor i in range(1000000):\n    num += 1e-6\n\nnum -= 1e9\n\nprint(num)\n    ","2cb661a9":"def normalization(x, mu, sigma):\n    \n    x_norm = np.zeros_like(x)\n\n    for n in range(len(x)):\n        for j in range(len(x[n])):\n            if(sigma[j]!=0):\n                x_norm[n,j] = (x[n,j] - mu[j]) \/ sigma[j]\n            else:\n                x_norm[n,j] = 0\n                    \n    return x_norm","b48fd552":"mu = np.mean(train_x, axis=0)\nsigma = np.max(train_x, axis=0)-np.min(train_x, axis=0)","e1d9f6a7":"test = normalization(test, mu, sigma)","8f170cee":"train_x = normalization(train_x,mu, sigma)","5bc1c09c":"print(\"Test Min: %.2f\" % np.min(test))\nprint(\"Test Max: %.2f\" % np.max(test))\nprint(\"Train Min: %.2f\" % np.min(train_x))\nprint(\"Train Max: %.2f\" % np.max(train_x))","fec4c574":"%matplotlib inline\nplt.figure(figsize=(12,10))\n\ny, x = 5,10\nfor i in range(0,(y*x)):\n    plt.subplot(y, x, i+1)\n    ni = np.random.randint(0,train_x.shape[0],1)[0]\n    show_image(train_x[ni],(28,28), train_y[ni], cmp=\"gray\")\nplt.show()","a0d9d4db":"%matplotlib inline\nplt.figure(figsize=(12,10))\n\ny, x = 5,10\nfor i in range(0,(y*x)):\n    plt.subplot(y, x, i+1)\n    ni = np.random.randint(0,test.shape[0],1)[0]\n    show_image(test[ni],(28,28),  cmp=\"gray\")\nplt.show()","bdc0e920":"train_y = pd.get_dummies(train_y).as_matrix()\nprint(train_y[0])","c985e71c":"def ReLu(x, derivative=False):\n    if(derivative==False):\n        return x*(x > 0)\n    else:\n        return 1*(x > 0)\n\nx = np.arange(20)-10\nrelu = ReLu(x)\n\nplt.plot(x, relu)\nplt.show()","06536195":"def Softmax(x):\n    x -= np.max(x)\n    sm = (np.exp(x).T \/ np.sum(np.exp(x),axis=1)).T\n    return sm\n\nx = np.arange(20)-10\nsoftmax = Softmax([x])\n\nplt.plot(x, softmax[0])\nplt.show()","c9a06e40":"def CreateWeights():\n    ##Initialization of the Weights and the Biases with the random gaussian function with mean zeron, and variance between 1\/sqtr(num_inputs_layer)\n    \n    ninputs = 784\n    wl1 = 500 ##Number of neurons in the first layer\n    wl2 = 300 ##Number of neurons in the second layer\n    nclass = 10 ##Numer of the class, in this case it is the number of the digits.\n    \n    #layer1\n    w1 = np.random.normal(0, ninputs**-0.5, [ninputs,wl1])\n    b1 = np.random.normal(0, ninputs**-0.5, [1,wl1])\n    \n    #Layer2\n    w2 = np.random.normal(0, wl1**-0.5, [wl1,wl2])\n    b2 = np.random.normal(0, wl1**-0.5, [1,wl2])\n\n    #Layer3\n    w3 = np.random.normal(0, wl2**-0.5, [wl2,nclass])\n    b3 = np.random.normal(0, wl2**-0.5, [1,nclass])\n    \n    return [w1,w2,w3,b1,b2,b3]","ce8117c7":"def Dropout(x, dropout_percent):\n    mask = np.random.binomial([np.ones_like(x)],(1-dropout_percent))[0]  \/ (1-dropout_percent)\n    return x*mask","405f8111":"def predict(weights, x, dropout_percent=0):\n    \n    w1,w2,w3,b1,b2,b3  = weights \n    \n    #1-Hidden Layer\n    first = ReLu(x@w1+b1)\n    first = Dropout(first, dropout_percent)\n\n    #2-Hidden Layer\n    second = ReLu(first@w2+b2)\n    second = Dropout(second, dropout_percent)\n    \n    #Output Layer\n    return [first, second, Softmax(second@w3+b3)]","3cb53581":"def accuracy(output, y):\n    hit = 0\n    output = np.argmax(output, axis=1)\n    y = np.argmax(y, axis=1)\n    for y in zip(output, y):\n        if(y[0]==y[1]):\n            hit += 1\n\n    p = (hit*100)\/output.shape[0]\n    return p","1cac5dbf":"fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(14, 4))\n\ny = np.arange(100)\/100\n\ne = -np.log(y)\ne2 = -np.log(1-y)\n\nax0.plot(y, e)\nax0.set_title(\"For t (right label) = 1\")\nax0.set_xlabel(\"Predict\")\nax0.set_ylabel(\"Error\")\nax1.plot(y, e2)\nax1.set_title(\"For t (right label) = 0\")\nax1.set_xlabel(\"Predict\")\nax1.set_ylabel(\"Error\")\nplt.show()","c008d38b":"def log2(x):\n    if(x!=0):\n        return np.log(x)\n    else:\n        return -np.inf\n    \ndef log(y):\n    return [[log2(nx) for nx in x]for x in y]\n\ndef cost(Y_predict, Y_right, weights, nabla):\n    w1,w2,w3,b1,b2,b3  = weights\n    weights_sum_square = np.mean(w1**2) + np.mean(w2**2) + np.mean(w3**2)\n    Loss = -np.mean(Y_right*log(Y_predict) + (1-Y_right)*log(1-Y_predict)) + nabla\/2 *  weights_sum_square\n    return Loss","7747da42":"porcent_valid = 0.1\nVALID_SIZE = round(train_x.shape[0]*porcent_valid)\n\nindex_data = np.arange(train_x.shape[0])\nnp.random.shuffle(index_data)\n\nx_train = train_x[index_data[VALID_SIZE:]]\nx_valid = train_x[index_data[:VALID_SIZE]]\n\n\nd_train = train_y[index_data[VALID_SIZE:]]\nd_valid = train_y[index_data[:VALID_SIZE]]\n\ntrain_x = None\ntrain_y = None\n\nx_train.shape","01ffb172":"def SGD(weights, x, t, outputs, eta, gamma, nabla, cache=None):\n    \n    w1,w2,w3,b1,b2,b3  = weights\n    \n    \n    if(cache==None):\n            vw1 = np.zeros_like(w1)\n            vw2 = np.zeros_like(w2)\n            vw3 = np.zeros_like(w3)\n            vb1 = np.zeros_like(b1)\n            vb2 = np.zeros_like(b2)\n            vb3 = np.zeros_like(b3)\n    else:\n        vw1,vw2,vw3,vb1,vb2,vb3 = cache\n    \n    first, second, y = outputs\n   \n    w3_delta = (t-y)\n   \n    w2_error = w3_delta@w3.T\n    \n    w2_delta = w2_error * ReLu(second,derivative=True)\n\n    w1_error = w2_delta@w2.T\n    w1_delta = w1_error * ReLu(first,derivative=True)\n    \n    eta = -eta\/x.shape[0]\n \n    vw3 = gamma*vw3 + eta * (second.T@w3_delta + nabla*w3)\n    vb3 = gamma*vb3 + eta * w3_delta.sum(axis=0)\n\n    vw2 = gamma*vw2 + eta * (first.T@w2_delta + nabla*w2)\n    vb2 = gamma*vb2 + eta * w2_delta.sum(axis=0)\n\n    vw1 = gamma*vw1 + eta * (x.T@w1_delta + nabla*w1)\n    vb1 = gamma*vb1 + eta * w1_delta.sum(axis=0)\n    \n    \n    w3 -= vw3\n    b3 -= vb3\n\n    w2 -= vw2\n    b2 -= vb2\n\n    w1 -= vw1\n    b1 -= vb1\n    \n    weights = [w1,w2,w3,b1,b2,b3]\n    cache = [vw1,vw2,vw3,vb1,vb2,vb3]\n    \n    return weights, cache","b8fad2a9":"from scipy.ndimage.interpolation import map_coordinates\nfrom scipy.ndimage.filters import gaussian_filter\n\ndef elastic_transform(image, alpha, sigma, random_state=None):\n    \"\"\"Elastic deformation of images as described in [Simard2003]_.\n    .. [Simard2003] Simard, Steinkraus and Platt, \"Best Practices for\n       Convolutional Neural Networks applied to Visual Document Analysis\", in\n       Proc. of the International Conference on Document Analysis and\n       Recognition, 2003.\n    \"\"\"\n    if random_state is None:\n        random_state = np.random.RandomState(None)\n\n    shape = image.shape\n    dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n    dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n\n    x, y = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]))\n    indices = np.reshape(y+dy, (-1, 1)), np.reshape(x+dx, (-1, 1))\n\n    return map_coordinates(image, indices, order=1).reshape(shape)","575bb221":"x_t = np.array([elastic_transform(xx.reshape(28,28),15,3).reshape(784) for xx in x_train[0:10]])\nplt.subplot(1,2,1)\nplt.imshow(x_t[0].reshape(28,28))\nplt.subplot(1,2,2)\nplt.imshow(x_train[0].reshape(28,28))","1284285e":"\ndef run(weights, x_train, y_train, x_valid, y_valid, epochs = 10, nbatchs=25, alpha = 1e-3, decay = 0, momentum = 0, l2 = 0.001, dropout_percent = 0):\n    \n    pross = x_train.shape[0]*0.05\n    \n    history = [[],[]]\n    \n    index = np.arange(x_train.shape[0])\n    cache = None\n    print(\"Train data: %d\" % (x_train.shape[0]))\n    print(\"Validation data: %d \\n\" % (x_valid.shape[0]))\n    mtime = 0\n    \n    r_weights = []\n    max_accuracy_valid = 0\n    \n    for j in range(epochs):\n        np.random.shuffle(index)\n        t = 0\n        iterations = round(x_train.shape[0]\/nbatchs)\n        prog = \"\"\n        sacurr = 0\n        sloss = 0\n        sys.stdout.write(\"\\nEpochs: %2d \\ %2d \\n\"% (j+1,epochs))\n        stime = 0\n        timeIT = time.time()\n        for i in range(iterations):\n            timeI = time.time()\n            f = i*nbatchs\n            l = f+nbatchs\n            \n            if(l>(x_train.shape[0]-1)):\n                l = x_train.shape[0]\n                \n            x = np.array([elastic_transform(xx.reshape(28,28),15,3).reshape(784) for xx in x_train[index[f:l]]])\n            y = y_train[index[f:l]]\n\n            outputs = predict(weights, x, dropout_percent)\n            \n            loss = cost(outputs[-1], y, weights, l2)\n            \n            \n            accuracy_t = accuracy(outputs[-1], y)\n            \n            sacurr += accuracy_t\n            sloss += loss\n            \n            accuracy_train = sacurr\/(i+1)\n            loss_train = sloss\/(i+1)\n            \n            weights, cache = SGD(weights, x, y, outputs, alpha, momentum, l2, cache)\n            \n            t+= x.shape[0]\n            \n            qtd = round(t\/pross)\n            prog = \"[\"\n            for p in range(20):\n                if(p<qtd-1):\n                    prog += \"=\"\n                elif(p==qtd-1):\n                    prog += \">\"\n                else:\n                    prog += \".\"\n            prog += \"]\"\n\n            \n            stime += time.time()-timeI\n            mtime = stime\/(i+1)\n            mTimeT = mtime * (iterations-i-1)\n            \n            sys.stdout.write(\"\\r%5d\/%5d %s ETA: %3d s - loss: %.4f  acc: %.4f\" % (t, x_train.shape[0], prog, mTimeT, loss_train, accuracy_train))\n            \n            history[0].append([loss_train, accuracy_train])\n        mtime = time.time()-timeIT\n        alpha = alpha - (alpha*decay)\n        \n        outputs = predict(weights, x_valid)\n        \n        loss_valid = cost(outputs[-1], y_valid, weights, l2)\n        accuracy_valid = accuracy(outputs[-1], y_valid)\n        \n        sys.stdout.write(\"\\r%5d\/%5d %s ETA: %3d s loss: %.4f  acc: %.4f - lossValid: %.4f  accValid: %.4f \" % ( t, x_train.shape[0], prog, mtime, loss_train, accuracy_train, loss_valid, accuracy_valid))\n        history[1].append([loss_valid, accuracy_valid])\n            \n        if(accuracy_valid>=max_accuracy_valid):\n            w1,w2,w3,b1,b2,b3  = weights\n            r_weights = [w1.copy(),w2.copy(),w3.copy(),b1.copy(),b2.copy(),b3.copy()]\n            max_accuracy_valid = accuracy_valid\n        \n    return r_weights, history","deeaac09":"weights = CreateWeights()\n\nalpha = 5e-2\nepochs = 40\nnbatchs = 100\nweights, history = run(weights, \n              x_train, d_train, \n              x_valid, d_valid, \n              epochs = epochs,\n              nbatchs=nbatchs, \n              alpha = alpha, \n              decay = 0.05, \n              momentum = 0.9, \n              l2 = 1e-3, \n              dropout_percent = 0.20)","c47314bd":"train_history = np.array(history[0])\nt_loss = train_history[:,:1]\nt_acc = train_history[:,1:2]","aeaacaaf":"valid_history = np.array(history[1])\ntrain_history.shape\nv_loss = valid_history[:,:1]\nv_acc = valid_history[:,1:2]","2c9961a3":"plt.figure(figsize=(12,10))\n\nplt.subplot(2, 1, 1)\nx = np.arange(epochs)*int(x_train.shape[0]\/nbatchs)\nplt.plot(x,v_acc)\nplt.plot(t_acc)\n\nplt.subplot(2, 1, 2)\nplt.plot(x,v_loss)\nplt.plot(t_loss)\n\nplt.show()","034ee4ba":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            plt.text(j, i, cm[i, j],\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","c55237d5":"outputs = predict(weights, x_valid)","68d9b9fa":"p = np.argmax(outputs[-1],axis=1)\npredict_dummies = pd.get_dummies(p).as_matrix().astype('int8')\n\ncm = np.zeros((10,10)).astype(np.int64)\n\nd_valid_int = np.argmax(d_valid, axis=1)\n\nfor i in range(predict_dummies.shape[0]):\n    cm[d_valid_int[i]] += predict_dummies[i]\n\nprint(cm)","3260db4c":"plot_confusion_matrix(cm, range(10))","0fecc7d8":"s_cm = np.sum(cm,axis=0)\nfor i in range(10):\n    p = cm[i][i]\/s_cm[i]\n    print(\"%d - %.3f %%\" % (i,p))\n\nprint(\" \");\nprint(\"%f %%\" % accuracy(outputs[-1], d_valid))","d1f1721d":"w1,w2,w3,b1,b2,b3  = weights","408df206":"%matplotlib inline\nplt.figure(figsize=(12,10))\n\ny, x = 4,10\nfor i in range(0,(y*x)):\n    ni = np.random.randint(0,w1.shape[1],1)[0]\n    plt.subplot(y, x, i+1)\n    plt.imshow(w1[:,ni].reshape((28,28)))\n\nplt.show()","00a1c48d":"%matplotlib inline\nplt.figure(figsize=(12,10))\n\ny, x = 4,10\nfor i in range(0,(y*x)):\n    ni = np.random.randint(0,w2.shape[1],1)[0]\n    plt.subplot(y, x, i+1)\n    plt.imshow(w2[:,ni].reshape((20,25)))\n\nplt.show()","165571f3":"%matplotlib inline\nplt.figure(figsize=(12,10))\n\ny, x = 1,10\nfor i in range(0,(y*x)):\n    plt.subplot(y, x, i+1)\n    plt.imshow(w3[:,i].reshape((20,15)))\n\nplt.show()","07162cbc":"outputs = predict(weights, test)\n","1f61b8ce":"d = np.argmax(outputs[-1],axis=1)","fd0c1f18":"%matplotlib inline\nplt.figure(figsize=(20,12))\n\ny, x = 5,11\nfor i in range(0,(y*x)):\n    plt.subplot(y, x, i+1)\n    ni = np.random.randint(0,test.shape[0],1)[0]\n    v = str(d[ni])\n    show_image(test[ni],(28,28), v, cmp=\"gray\")\nplt.show()","3b86eeaf":"#ImageId,Label\n\npd.DataFrame({\"ImageId\": list(range(1,len(d)+1)), \"Label\": d}).to_csv('output.csv', index=False, header=True)","79c7a9f7":"#### PREDICT\n<br\/>\nThe model will be a Neural Network with 2 layers Fully Connected with  respectively, 500 and 300 neurons, the boths layers with Activation Function  ReLu. The last layer contain 10 neurons and used the Activation Function Softmax. Between the layer we will use the Dropout for prevent overfitting.\n\n> 500 NN - DROPOUT- 300 NN - DROPOUT - 10 NN\n\nHow we are using the matrix representation, we have to use the matrix multiplication. The matrix ","85127d60":"## REFERENCES\n\nhttp:\/\/users.isr.ist.utl.pt\/~wurmd\/Livros\/school\/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf","46243efd":"This Loss function works well, but we need to be careful with the **log()**, because the log of the number zeros isn't exist so we consider **log(0)** as the lowest value -inf after use the np.nan_to_num().\n\nThe derivative of the L(t|y) will be important for train the model.\n\n$$ \\dfrac{dL(t|y)}{dy}  = \\dfrac{1}{N}. \\dfrac{t}{y}-\\dfrac{(1-t)}{(1-y)} $$\n$$ = \\dfrac{1}{N}. \\dfrac{t.(1-y) - (1-t).y}{y.(1-y)} $$\n$$ = \\dfrac{1}{N}. \\dfrac{t-ty - y+ty}{y.(1-y)} $$\n$$ = \\dfrac{1}{N}. \\dfrac{t - y}{y.(1-y)} $$\n\nhttp:\/\/users.isr.ist.utl.pt\/~wurmd\/Livros\/school\/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf pg. 206","590c83a2":"### ONE HOT ENCODING\n\nOne Hot Encodding or Dummies is a group of the digits when each digit corresponds the propabilite of the class is True. It's very used for classifying. To do it, we'll use the pandas.","5f659907":"### DESCRIBE THE MODEL\n\nIn this link <a style='color:#003d99'>http:\/\/yann.lecun.com\/exdb\/mnist\/<\/a> we can see some examples of models and yours respectives error in porcent. In this website the bests models are using the Convolution Neural Network, but it isn't the goal this Notebook. \n\nIn this notebook we will use a Neural Network with 3 layer, 800-300 Hidden Units with Softmax Classification, Cross-Entropy as Loss Function and weight decay, as describe by Hinton, unpublished, 2005. Where he got 1,53% porcent error in this Neural Network.","d17aba13":"#### METRICS\n<br\/>\nAfter to run the model we need to evaluate the model for measure how well the it is going, we can use the accuracy metric and \/ or loss metric.\nThe accuracy is measure by the percent of the hit of the model did, and we can measure with the function bellow.","ebb791f7":"### ACTIVATION FUNCTIONS\n\nExist a lot of types of activations functions, but in this model we'll use the ReLu  (Rectifier Linear Function), this function is similar to the functioning of the biological neuron, and allow us to run the model more fast. ","56638fff":"The Softmax function allow us predict the model, because it normalize the data in one hot encoding.\n\n$$ y = \\sigma(x) = \\dfrac{e^x}{\\sum_{i=1}^{N} e^x}  $$\n\nThe derivative of the \u03c3(x) will be important for train the model.\n\n$$ \\dfrac{dy}{dZ} = \\dfrac{d\\sigma(x)}{dZ} = \\dfrac{e^x.\\sum_{i=1}^{N} e^x - e^x. e^x}{[\\sum_{i=1}^{N} e^x]^2}$$\n$$ \\dfrac{d\\sigma(x)}{dZ} = \\dfrac{e^x.(\\sum_{i=1}^{N}  e^x. e^x)}{\\sum_{i=1}^{N} e^x.\\sum_{i=1}^{N} e^x} $$\n$$ \\dfrac{d\\sigma(x)}{dZ} = \\dfrac{e^x}{\\sum_{i=1}^{N} e^x }. \\dfrac{(\\sum_{i=1}^{N}  e^x - e^x)}{\\sum_{i=1}^{N} e^x} $$\n$$ \\dfrac{d\\sigma(x)}{dZ} = \\dfrac{e^x}{\\sum_{i=1}^{N} e^x }. (\\dfrac{(\\sum_{i=1}^{N}  e^x)}{\\sum_{i=1}^{N} e^x}-\\dfrac{e^x}{\\sum_{i=1}^{N} e^x}) $$\n$$ \\dfrac{d\\sigma(x)}{dZ} = y. (1-y) $$\n","0dbb9382":"Exist a lot of loss functions, but as the variance of the numbers of inputs are small the changes will be small also, so we chose the cross-entropy as loss function, because it have high sensibility by small changes. The cross-entropy is used with probability, and compare the outputs of the model with the true results and return the distorcing of the diference.\n\n$$ L(t|y) = - \\dfrac{1}{N}. \\sum_{i=1}^{N} [t \\cdot log(y) + (1-t) \\cdot log(1-y)] $$\n\nWhere, **N** is the number of the exemples, **t** is the right labels and **y** is the predict labels.","d00d6305":"It can be a problem, because Neural Network uses a lot of sum. So, we have used normalization techniques for solving it. In general is a good practice normalize of the data with mean zero and a small variance.\n\nFor image exists a very easy normalization, how the values of an image is between 0 - 255, we just divide by 255. With this normalization the values of the images are between 0 - 1.","26f37c4f":"### VISUALIZATION OF THE DATA","ef5ed6df":"## IMPORT LIBRARIES\n\nIt was used the Numpy, Pandas, Sys, Matplotlib and Time.","a9b6243b":"### COUNT OF EXAMPLES PER DIGIT\n\nCount of number the examples per digit is important, because if the data isn't uniform we can have a bad accuracy. In this dataset the number of examples per digit is uniform, so we won't worry.","1d099abd":"### Training process\n\nIn the training process is divided all the data in batchs, it's a process part because it's principle fundament of the stochastic process. These batchs need to be divide with randomlly for improved the accuracy of the model.\n\n### Image Transformation\n\nSimard, Steinkraus and Platt in yours paper \"Best Practices for Convolutional Neural Networks applied to Visual Document Analysis\"(http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.10.5032&rep=rep1&type=pdf) show that the transformation images can improved the Neural Network. In yours tests the Elastic Transformation showed the best results.\n\nI found out in the github a method that implement the elastic transformation. (https:\/\/gist.github.com\/fmder\/e28813c1e8721830ff9c)","9df6fe6c":"#### DROPOUT\n<br\/>\nDropout is a simple technique for prevent overfitting, it was approached in the paper \"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\"<a style='color:#003d99'>(https:\/\/www.cs.toronto.edu\/~hinton\/absps\/JMLRdropout.pdf)<\/a>.\nThis technique consist in randomly dropping neurons, for that layer not to depend of these neurons, to predict right.\n![image.png](attachment:image.png)","6803cf17":"### NORMALIZATION\n\nThe precision of the computers is increasing, but isn't perfect. If we set 1e9 (1 billion), add 1e-6 to it million (1e6) times, and after subtract 1e9 (billion) from it, mathematically you should get 1 but we get 0.95... It happens because we add a lot of times a small number in a big number, breaking the precision of the computer.","2b287614":"# **NEURAL NETWORK USING STOCHASTIC GRADIENT DESCENT**\nby **Jean Carlo Codogno** - 17\/03\/2018\n\n<br\/>\n\n+ **INTRODUCTION**\n    + **DATA EXPLORATION**\n        + IMPORT THE DATASET\n        + VISUALIZATION OF THE DATA\n        + COUNT OF EXAMPLES PER DIGIT\n        + NORMALIZATION\n        + ONE HOT ENCODING\n    + **DESCRIBE THE MODEL**\n        + ACTIVATION FUNCTIONS\n        + DESCRIBE THE MODEL\n            + CREATING WEIGHTS\n            + DROPOUT\n            + PREDICT\n            +  METRICS\n            + CROSS - VALIDATION\n\n    + **TRAINING**\n        + SGD (Stochastic Gradient Descent)\n        + MOMENTUM\n        + L2 REGULIZER\n\n\n## INTRODUCTION\n\nThis is a simple Fully Connected  Neural Network using Stochastic Gradient Descent to train it. We use 2 hidden layer with activation function relu and 1 output layer with activation function softmax for classification. ","695cb941":"#### CROSS - VALIDATION\n<br\/>\nThe cross-validation is a techinique used for measure the accuracy and visualization overfitting. This technique consist in split the data in train_data anda test_data, where the test_data consist of 10% - 30% of the all data.","78da89f6":"####   CREATING WEIGHTS \n<br\/>\nTo simplify and helpin the calculus, we are use the matriz representation for representate the weights, inputs and the outputs.\nIn the method below will create the weights and the biases with mean zero and the variance proportional to root of the numbers of inputs. This method use the Numpy to create the ramdom number with normal function. This method return a list with the weights and biases.","3eca001d":"## DATA EXPLORATION\n\nThe dataset of Digit Recognizer Competition is the famous **MNIST** (The dataset of handwritten digits), it can be found out at <a style='color:#003d99'>http:\/\/yann.lecun.com\/exdb\/mnist\/<\/a>. This dataset consist of gray-scale images of hand-drawn digits, from zero through nine with shape 28x28 (or 784 pixels) and position in the center. \n\nIn the competition is divided of the dataset in the two parts, train-set (train.csv), test-set (test.csv). In the  train-set contains 42000 examples and in the test-set contain 28000 images to classify.\n\n### IMPORT THE DATASET\n\nIt was used the Pandas to import the data set and transform in numpy array.","37f80dc3":"## DEFINING THE MODEL\n\nArtificial Neural Network or Neural Network(ANN or NN) is a branch of Machine Learning, that it is inspired by the biological neural networks. The NN consist of the conection of artifical neurons divide in layers. The most famous neuron is the perceptron. \n\nThe artificial neuron consist in simulate the function of the biological neuron. The biological neural learn with its connections, if the connections is enough strong the signal is activate. In artificial neurons this feature is representate with variable Weght (wi), that they are changed in the training. These Weights represente the knowlegde of the Neural Networks.\n\nMathematically a artificial neuron can be represented by the sum of product between Input values and its respective weight, as equation bellow.\n\n$$ Z = \\sum_{i=1}^{n} X_i \\cdot W_i + b_i$$ \n<br\/>\nIn the artificial neuron contain the activation function, when the Z is apply. Exist a lot of activation functions, most famous are Sigmoid, Tahn, ReLu, LReLu and PReLu.\n\nThe derivative of the Z will be important for train the model.\n\n$$ \\dfrac{dZ}{dW} = X_i$$ \n\n![image.png](attachment:image.png)\n\nFonte: <a style='color:#003d99'>http:\/\/www.theprojectspot.com\/tutorial-post\/introduction-to-artificial-neural-networks-part-1\/7<\/a>","ae432611":"## TRAINING\n### SGD (Stochastic Gradient Descent)\n\nSGD is a optimizer used for fit the neural network, this technique is based by Gradient Descent.\nIn SGD is used the matriz representation, the equation for represent the update the weights is bellow.\n\n$$ V_{k+1} = V_k - \\eta . \\nabla L(W_{ij}) $$\n<br\/>\n$$  W = V_{k+1}  $$\n\nWhere the **\u03b7** is the step size, and the **\u2207L(W)** is the gradient of the Loss.\n\nThe gradient can be solved using the chain rule of the derivate the Loss funtion by Weights. The equation for output layer is bellow.\n\n$$ \\nabla L(W_{ij}) = \\dfrac{dL(W_{ij})}{dW_{ij}} $$\n<br\/>\n$$ =\\dfrac{dL(t|y)}{dy}.\\dfrac{dy}{dZ}.\\dfrac{dZ}{dW_{ij}} $$\n<br\/>\n$$ =\\dfrac{1}{N} . \\dfrac{(t-y)}{y.(1-y)}.y.(1-y). X_{ij} $$\n<br\/>\n$$ = \\dfrac{1}{N} . (t-y)X_{ij} $$\n<br\/>\n<center>For the hidden layer:<\/center>\n<br\/>\n$$ \\nabla L(W_{ij}) = \\dfrac{dL(W_{ij})}{dW_{ij}} $$\n<br\/>\n$$ =\\dfrac{dL(t|y_{ij})}{dy_{ij}}.\\dfrac{dy_{ij}}{dZ_{ij}}.\\dfrac{dZ_{ij}}{dy_{ij}}.\\dfrac{dy_{ij}}{dZ_{ij}}.\\dfrac{dZ_{ij}}{dW_{ij}} $$\n<br\/>\n$$ = \\dfrac{1}{N} . \\dfrac{(t-y)}{y.(1-y)}.y.(1-y). W_{ij}.\\sigma(x)'.X_{ij} $$\n<br\/>\n$$ = \\dfrac{1}{N} . (t-y).W_{ij}.\\sigma(x)'.X_{ij} $$\n<br\/>\n### MOMENTUM\n\nThe SGD in training have a lot of oscillations, so the momentum term was invented. The momentum term is used for soft the oscillations and accelerates of the convergence.\n\n$$  V_{i+1} = \\gamma V_i + \\eta . \\nabla L(W) $$ \n<br\/>\n$$  W = W - V_{i+1} $$\n<br\/>\n\n### L2 REGULIZER \n\nThe L2 regulizer is the technique which is used for penalize the higher weights, \n\n$$ L(t|y|w) = -\\dfrac{1}{N} . \\sum_{i=1}^{N} [(t \\cdot log(y) + (1-t) \\cdot log(1-y)) +\\dfrac{\\lambda}{2}. \\sum_{j=1}^{nj}\\sum_{i=1}^{ni} W_{ij}^2] $$\n\nWith this modification in the Loss function the equation of the update the weights is changed. \nFor the output layer.\n \n$$ \\nabla L(W_{ij}) = \\dfrac{1}{N} . [(t-y).X_{ij} + \\lambda W_{ij}] $$\n\nFor the hidden layer.\n\n$$ \\nabla L(W_{ij}) = \\dfrac{1}{N} . [(t-y).W_{ij}.\\sigma(x)'.X_{ij} +  \\lambda W_{ij} ] $$"}}