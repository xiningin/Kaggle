{"cell_type":{"2510be16":"code","c3870d2a":"code","be9976fc":"code","a6751edd":"code","38977f34":"code","8da72f36":"code","51f5ea3e":"code","ac8956f6":"code","96178400":"code","5c4a4fac":"code","e57269d5":"code","809718ef":"code","2aba864e":"code","78ca2b35":"code","56c3917e":"code","ff55fb9a":"code","5a467110":"code","48c62c53":"code","d5196726":"code","8b90e8eb":"code","4b8d5b05":"code","746d5977":"code","80428608":"code","58597b03":"code","0356651f":"code","2ea3553a":"code","a1f45778":"code","675f15b1":"code","7e91de2d":"code","dea10c72":"code","70d65a69":"code","2fa97199":"code","0b41e409":"markdown","cbd029c9":"markdown","75324644":"markdown","11aed89d":"markdown","2c405514":"markdown","8189b8fd":"markdown","3c1a0be9":"markdown","9d6263ed":"markdown","9838282e":"markdown","c0b0e0ad":"markdown","0a6be6a6":"markdown","fc348213":"markdown","eb3295fd":"markdown","82c8b894":"markdown","4a632f24":"markdown","3b8ba536":"markdown","063e3f39":"markdown","0e360eb4":"markdown","a864b00b":"markdown","c19d2a06":"markdown","d63623bd":"markdown","6c1fa6a0":"markdown","b9ceb093":"markdown","a3e7c350":"markdown","de81a8e7":"markdown"},"source":{"2510be16":"import warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_selection import chi2, VarianceThreshold\nfrom sklearn.model_selection import train_test_split\nimport sklearn.linear_model\n\n\n#supressing warnings for readability\nwarnings.filterwarnings(\"ignore\")\n\n# To plot pretty figures directly within Jupyter\n%matplotlib inline\n\n# choose your own style: https:\/\/matplotlib.org\/3.1.0\/gallery\/style_sheets\/style_sheets_reference.html\nplt.style.use('seaborn-whitegrid')\n\n# Go to town with https:\/\/matplotlib.org\/tutorials\/introductory\/customizing.html\n# plt.rcParams.keys()\nmpl.rc('axes', labelsize=14, titlesize=14)\nmpl.rc('figure', titlesize=20)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n\n# contants for figsize\nS = (8,8)\nM = (12,12)\nL = (14,14)\n\n# pandas options\npd.set_option(\"display.max.columns\", None)\npd.set_option(\"display.max.rows\", None)\npd.set_option(\"display.precision\", 2)\n\n# import data and do simple train-test-split\ndf_raw = pd.read_parquet('..\/input\/nhs-proms-case-study\/data\/interim\/knee-ccg.parquet')\ndf, df_test = train_test_split(df_raw, test_size=0.3, random_state=42)","c3870d2a":"from itertools import product\n\n# a product generates a carterian product\n['_'.join(col) for col in product(['t0', 't1'],  ['assisted', 'previous_surgery', 'disability'])]","be9976fc":"# helper function to count top 10 unique values for dataframe\n# pd.concat takes as list of Series or DataFrames to concatenate\n# make this list using a list comprehension\ndef count_values(df):\n  _value_counts = [df[col].value_counts().sort_values(ascending=False).head(10) for col in df.columns]\n  return pd.concat(_value_counts, axis=1).transpose()","a6751edd":"def oks_questions(t='t0'):\n  return [f'oks_{t}_pain', f'oks_{t}_night_pain', f'oks_{t}_washing',\n          f'oks_{t}_transport', f'oks_{t}_walking', f'oks_{t}_standing',\n          f'oks_{t}_limping', f'oks_{t}_kneeling', f'oks_{t}_work',\n          f'oks_{t}_confidence', f'oks_{t}_shopping', f'oks_{t}_stairs', ]\n\ndef eq5d_questions(t='t0'):\n  return [f'{t}_mobility', f'{t}_self_care',\n          f'{t}_activity', f'{t}_discomfort', f'{t}_anxiety']\n\noks_questions('t1')","38977f34":"comorb = ['heart_disease', 'high_bp', 'stroke', 'circulation', 'lung_disease', 'diabetes',\n           'kidney_disease', 'nervous_system', 'liver_disease', 'cancer', 'depression', 'arthritis']\nboolean = ['t0_assisted', 't0_previous_surgery', 't0_disability']\ncount_values(df[['gender'] + comorb + boolean])","8da72f36":"eq5d = ['t0_mobility', 't0_self_care', 't0_activity', 't0_discomfort', 't0_anxiety']\ncategorical = ['t0_symptom_period', 't0_previous_surgery', 't0_living_arrangements']\n\n# useless variables\nuseless = ['t0_assisted_by', 't0_eq5d_index', 't0_eq5d_index_profile']\n\ncount_values(df[eq5d + categorical])","51f5ea3e":"# list of columns which contain 9s\ncols_with_9 = [col for col in df.columns if (df[col]==9).any()]\n\n# columns where 9 is *not* a sentinel value\ncols_keep_9 = comorb + [ 't0_eq_vas', 't1_eq_vas','oks_t0_score', 'oks_t1_score']\n\n# columns with 9 as sentinel value\ncols_sentinel_9 = list(set(cols_with_9) ^ set(cols_keep_9))\ncount_values(df[cols_sentinel_9]).sort_index()","ac8956f6":"from sklearn.impute import SimpleImputer\n\nimpute_median = SimpleImputer(strategy='median')\nimpute_most_frequent = SimpleImputer(strategy='most_frequent')\n\n# use copy of original data\ndfc = df.copy()\n\n# replace 9 with np.nan and impute most frequent\ndfc.loc[:,cols_sentinel_9] = df.loc[:,cols_sentinel_9].replace(9, np.nan)\nimpute_most_frequent.fit(dfc[cols_sentinel_9])\npd.DataFrame({'columns': cols_sentinel_9, 'most_frequent': impute_most_frequent.statistics_}).head(10)","96178400":"# assign imputed data\ndfc.loc[:, cols_sentinel_9] = impute_most_frequent.transform(dfc[cols_sentinel_9])","5c4a4fac":"# same procedure for 999 sentinel values, using impute median\neq_vas = ['t0_eq_vas', 't1_eq_vas']\ndfc.loc[:, eq_vas] = dfc.loc[:,eq_vas].replace(999, np.nan)\nimpute_median.fit(dfc.loc[:,eq_vas])\ndfc.loc[:,eq_vas] = impute_median.transform(dfc.loc[:,eq_vas])\nimpute_median.statistics_","e57269d5":"# transform into boolean\ndfc['female'] = dfc.loc[:,'gender'].replace({1: False, 2: True})\ndfc.loc[:, comorb] = dfc.loc[:, comorb].replace({9: False, 1: True})\ndfc.loc[:, boolean] = dfc.loc[:, boolean].replace({1: True, 2: False})","809718ef":"# helper function for counting boolean attribues\ndef count_boolean(series):\n    '''\n    Returns absolute and normalized value counts of pd.series as a dataframe with \n    index = series.name\n    columns with absolute and normalized counts of each value\n    '''\n    try:\n        count = series.value_counts().to_frame().transpose()\n        norm = series.value_counts(normalize=True).to_frame().transpose()\n        return count.join(norm, lsuffix='_count', rsuffix='_normalized') \n    except:\n        print('Error: expecting a pandas.Series object as input. \\n' + count_boolean.__doc__)\n        return None\n\npd.concat([count_boolean(dfc[col]) for col in ['female'] + comorb + boolean]).round(2)","2aba864e":"# convert to categories\ndfc.loc[:, ['provider_code', 'age_band']] = dfc.loc[:, ['provider_code', 'age_band']].astype('category')","78ca2b35":"# input features\n# TO DO: decide what to do with years?!\n\n# feature engineering\nfor t in (\"t0\", \"t1\"):\n    dfc[f\"oks_{t}_pain_total\"] = dfc[f\"oks_{t}_pain\"] + dfc[f\"oks_{t}_night_pain\"]\n    dfc[f\"oks_{t}_functioning_total\"] = (\n        dfc.loc[:, [col for col in oks_questions(t) if \"pain\" not in col]]\n        .sum(axis=1)\n    )\ndfc[\"n_comorb\"] = dfc.loc[:, comorb].sum()\n\nX_features = (\n    [\"provider_code\", \"female\", \"age_band\"]\n    + comorb\n    + boolean\n    + eq5d_questions(\"t0\")\n    + oks_questions(\"t0\")\n    + [\"t0_eq_vas\", \"oks_t0_pain_total\", \"n_comorb\"]\n)\nX_features","56c3917e":"dfc[['oks_t1_pain', 'oks_t1_night_pain', 'oks_t1_pain_total']].hist(bins=range(0,9), figsize=S);","ff55fb9a":"dfc.loc[:, [col for col in oks_questions(\"t1\") if \"pain\" not in col],].hist(figsize=M);","5a467110":"dfc['oks_t1_functioning_total'].hist(figsize=S);","48c62c53":"dfc[['oks_t0_pain_total', 'oks_t1_pain_total']].plot.hist(bins=range(0,9), alpha=0.5, figsize=S);","d5196726":"dfc[['oks_t0_functioning_total', 'oks_t1_functioning_total']].plot.hist(bins=range(0,40), alpha=0.5, figsize=S);","8b90e8eb":"BINS = 8\nSAMPLE = 1000\n\n# https:\/\/stackoverflow.com\/questions\/45540886\/reduce-line-width-of-seaborn-timeseries-plot\nsns.set(rc={\"lines.linewidth\": 0.7})\nfig1_layout = {\n    'kind': \"hex\",\n    'height': 8,\n    'ratio': 3,\n    'stat_func': None, \n    'marginal_kws': {'bins': BINS, 'rug': False},\n}\nj1 = (sns.jointplot(\n            'oks_t0_pain_total', 'oks_t0_functioning_total',\n            data=dfc[['oks_t0_pain_total', 'oks_t0_functioning_total']].head(SAMPLE),\n            **fig1_layout)\n         .plot_joint(sns.kdeplot, color='lawngreen'))\nj2 = (sns.jointplot(\n            'oks_t1_pain_total', 'oks_t1_functioning_total',\n            data=dfc[['oks_t1_pain_total', 'oks_t1_functioning_total']].head(SAMPLE), \n            **fig1_layout)\n         .plot_joint(sns.kdeplot, color='lawngreen' ));","4b8d5b05":"# add boolean columns\nfor t in ('t0', 't1'):\n    dfc[f'y_{t}_pain_good'] = dfc[f'oks_{t}_pain_total'].apply(lambda s: True if s >= 4 else False)\n    dfc[f'y_{t}_functioning_good'] = dfc[f'oks_{t}_functioning_total'].apply(lambda s: True if s >= 26 else False)","746d5977":"# inspect whether new features are correct\ndfc[[col for col in dfc.columns if '_good' in col]].head(10)","80428608":"pd.crosstab(dfc.y_t0_pain_good, dfc.y_t0_functioning_good, normalize=True)","58597b03":"pd.crosstab(dfc.y_t1_pain_good, dfc.y_t1_functioning_good, normalize=True)","0356651f":"# add delta_oks_score and Y\ndef good_outcome(oks_t1, delta_oks, abs_threshold=43, mcid=13):\n  if oks_t1 > abs_threshold or delta_oks > mcid:\n    return True\n  else:\n    return False\n\ndfc['delta_oks_score'] = dfc.oks_t1_score - dfc.oks_t0_score\ndfc['Y'] = dfc.apply(lambda row: good_outcome(row['oks_t1_score'], row['delta_oks_score']), axis=1)\ndfc.Y.value_counts(normalize=True)","2ea3553a":"dfc.plot(kind='scatter', x='t0_eq_vas', y='t1_eq_vas', figsize=M, alpha=0.2);","a1f45778":"from sklearn.linear_model import LinearRegression\n\n# x needs to be a column vector or an matrix\nx = dfc.t0_eq_vas.values.reshape(-1, 1)\n\n# y is a row vector\ny = dfc.t1_eq_vas.values\nprint(f'x: {x[:5]}\\n y: {y[:5]}')","675f15b1":"# linear regression with t0_eq_vas\nlr = LinearRegression().fit(x, y)\nr2 = lr.score(x, y)\nprint(f'r2: {r2:.2f}')","7e91de2d":"from sklearn.tree import DecisionTreeRegressor\n\n# regression can only take numeric input features\nX = dfc.loc[:, X_features].select_dtypes(include='float64').drop(columns=['n_comorb'])\ny = dfc.t1_eq_vas.values\ndtr = DecisionTreeRegressor().fit(X,y)\ndt_r2 = dtr.score(X, y)\nprint(f'DecisionTreeRegressor r2: {dt_r2:.2f}')","dea10c72":"# this doesn't make sense, probably overfitting\ndtr = DecisionTreeRegressor(max_depth=5).fit(X,y)\ndt_r2 = dtr.score(X, y)\nprint(f'DecisionTreeRegressor r2: {dt_r2:.2f}')","70d65a69":"from sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.metrics import confusion_matrix, roc_auc_score\n\ny_binary = dfc.Y\nlogit = LogisticRegressionCV(cv=5, scoring='roc_auc', random_state=42).fit(X, y_binary)\ny_predict = logit.predict(X)\nconfusion_matrix(y_binary, y_predict, normalize='all')","2fa97199":"roc_auc_score(y_binary, y_predict)","0b41e409":"### Inspect boolean features (T0)","cbd029c9":"# Data Preparation\n\nThis is day 2 from the [5-day JADS NHS PROMs data science case study](https:\/\/github.com\/jads-nl\/execute-nhs-proms\/blob\/master\/README.md).\n","75324644":"### Explore binary Y good outcome with logistic regression","11aed89d":"## Selecting outcome Y\n\nFigure 4 from the [narrative on outcomes of osteoarthritis](https:\/\/github.com\/dkapitan\/jads-nhs-proms\/blob\/master\/references\/hunter2019osteaoarthritis.pdf) states that the [European League Against Rheumatism (EULAR)](https:\/\/eular.org) standard considers three primary symptoms, namely:\n- Hip\/knee pain\n- Early morning stiffness (EMS)\n- Functional limitations\n\nComparing these with the [ICHOM standard set for hip & knee osteoarthritis](https:\/\/www.ichom.org\/portfolio\/hip-knee-osteoarthritis\/) we take pain and functional limitations as the primairy outcome parameters as ICHOM does not include EMS as a relevant outcome parameter. We now turn to the NHS dataset to choose which (engineered) features are available as a measure for pain and physical functioning at `t1`.\n\n![](https:\/\/ichom.org\/wp-content\/uploads\/2018\/06\/ichom_outcome_oa_3.jpg)","2c405514":"\n## Learning objectives\n\n### Data preparation: select, clean and construct data\n\n- Impute missing values\n- Select main input variables X (feature engineering)\n- Define target Y (clustered classes, categories)\n- Decide how to handle correlated input features\n\n\n### Learning objectives: Python\n\n- [Pythonic data cleaning](https:\/\/realpython.com\/python-data-cleaning-numpy-pandas\/)\n- [When to use list comprehensions](https:\/\/realpython.com\/list-comprehension-python\/)\n- [Python f-strings](https:\/\/realpython.com\/python-f-strings\/)\n- [Using sets](https:\/\/realpython.com\/python-sets\/)\n- [pandas GroupBy](https:\/\/realpython.com\/pandas-groupby\/)\n- [Correlations in with numpy, scipy and pandas](https:\/\/realpython.com\/numpy-scipy-pandas-correlation-python\/)\n- [Linear Regression in Python](https:\/\/realpython.com\/linear-regression-in-python\/)\n- Extra: [Using itertools](https:\/\/realpython.com\/python-itertools\/#what-is-itertools-and-why-should-you-use-it)","8189b8fd":"One way to combine pain and functioning into one outcome parameter is to define the cut-off points:\n- `oks_pain_total >= 4`: good outcome with less pain\n- `oks_functioning_total >= 26`: good outcome with improved functioning\n\nVisualize this using jointplots, and one can image that at `t0` the majority of patients are in the bottom-left quadrant (much pain, poor functioning). Subsequently, at `t1` most patient are in the top-right quadrant with less pain and improved functioning.","3c1a0be9":"### Pain","9d6263ed":"### Intermezzo: Python skills\n\n","9838282e":"### Inspect categorical features (T0)\n","c0b0e0ad":"### Alternative outcome parameter: combine absolute value with MCID","0a6be6a6":"### Discussion of results\n\n* What outcome parameter would you use?\n* What features would you include in your first iteration","fc348213":"### Checklist for results from data preparation process\n* Input regarding the moment of prediction\n* Input for data cleaning (handling missing data; removing variables not known at time of prediction, near-zero variance variables, etc)\n* Input for feature engineering (adjusting variables based on tree-analyses, based on correlations, based on domain-analysis)\n* Input for defining the outcome variable Y\n* Input for defining the business objective in terms of generalizability (in case of missing Y values)\n* Input for choosing the business objective in case there are still multiple options at the table\n* Input for defining the scope of the business objective (e.g. limiting to a subgroup to get a better balanced outcome variable)\n* A potential revision of the goal of your business objective\n* Input for which variables and combination of variables seem particularly relevant within the to-be-developed algorithms ","eb3295fd":"#### Using list comprehensions\n\nThere are three ways to work with lists in Python:\n1. with `for` loops\n2. using `map` (map-reduce pattern)\n3. using list comprehensions\n\nThe third option finds its roots in functional programming and is considered the most Pythonic. It is particularly useful for data science.\n\nIn this case, we want to make group of columns by name, without having to continously re-type the names. We will use some functions from the [itertools](https:\/\/docs.python.org\/3.7\/library\/itertools.html#module-itertools) standard library.\n\n**Excercise:** find all columns that contain the value 9.\nUsing [`pd.Series.any()`]\\(https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.Series.any.html), create a list of all columns that contain the value 9. (Answer is given later in this notebook).","82c8b894":"#### Using f-strings","4a632f24":"### Combined outcome Y\n\nOne of the challenges in quantifying outcomes is to find a relevant measure to detect meaningful clinical change. This is well-known subject in e.g.\n[development of psychometic tests](https:\/\/github.com\/dkapitan\/jads-nhs-proms\/blob\/master\/references\/wise2004methods.pdf). More heuristically, one can think of [analyzing the intersection between the histograms](http:\/\/blog.datadive.net\/histogram-intersection-for-change-detection\/), in this case measured at `t0` and `t1`.","3b8ba536":"### Physical functioning","063e3f39":"## Cleaning data\n","0e360eb4":"# Background to osteoarthritis case study\n\n_taken from [narrative seminar Osteoarthritis by Hunter & Bierma-Zeinstra (2019) in the Lancet](https:\/\/github.com\/dkapitan\/jads-nhs-proms\/blob\/master\/references\/hunter2019osteaoarthritis.pdf)._\n\nOutcomes from total joint replacement can be optimised if patient selection identifies marked joint space narrowing. Most improvement will be made in patients with complete joint space loss and evident bone attrition. Up to 25% of patients presenting for total joint replacement continue to complain of pain and disability 1 year after well performed surgery. Careful preoperative patient selection (including consideration of the poor outcomes that are more common in people who are depressed, have minimal radiographic disease, have minimal pain, and who are morbidly obese), shared decision making about surgery, and informing patients about realistic outcomes of surgery are needed to minimise the likelihood of dissatisfaction.\n\n__Recap from previous lecture__\n- Good outcome for knee replacement Y is measured using difference in Oxford Knee Score (OKS)\n- Research has shown that an improvement in OKS score of approx. 30% is relevant ([van der Wees 2017](https:\/\/github.com\/dkapitan\/jads-nhs-proms\/blob\/master\/references\/vanderwees2017patient-reported.pdf)). Hence an increase of +14 points is considered a 'good' outcome.\n- to account for the ceiling effect, a high final `t1_oks_score` is also considered as a good outcome (even if `delta_oks_score` is smaller than 14).","a864b00b":"### Format data","c19d2a06":"#### Explore Y = t1_eq_vas with decision tree regression\nNext, let's try more parameters to perform regression on `t1_eq_vas`.","d63623bd":"### Explore Y with simple baseline model\nAt the end of the data preparation phase, you usually want to perform a first quick test on modeling the target Y. A simple linear regression often helps to get a better you understanding. We illustrate this with two simple models:\n- In case of a regression task, do a linear regression and\/or a decision tree regression\n- In case of a classification task, do a logistic regression and\/or a decision tree regression\n\n#### Explore Y = t1_eq_vas with linear regression\nTo illustrate linear regression, we use `t1_eq_vas` as numeric outcome. First we will assess whether there is a correlation between `t0` and `t1` values of `eq_vas`","6c1fa6a0":"So let's add boolean columns indicating whether pain is 'good' (i.e. no pain) and functioning is good, both at `t0` and `t1`.","b9ceb093":"### Selecting input features X","a3e7c350":"## Conclusion and reflection","de81a8e7":"### Impute missing values\n\n* No missing values: gender, age_band and all comorbidity variables\n* Booleans with missing values: most frequent\n* Categorical:\n  * eq5d: use most frequent\n  * symptom_period, living arrangements: use most frequent\n* Numerical: impute EQ-VAS with median\n\nSince we are going to replace values, we choose to convert the data to the most optimal type.\n\nWrite code in such a way that you can easily adopt this strategy."}}