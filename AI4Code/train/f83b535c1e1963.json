{"cell_type":{"1f192661":"code","5214a00f":"code","3007e12f":"code","597f54f6":"code","09ae387e":"code","ed11d858":"code","d5c84619":"code","ca2fd3b4":"code","c6996bb6":"code","5e2a3a9c":"code","216b3987":"code","39eaa429":"code","f04de311":"code","97e68e53":"code","ef58ea43":"code","5ba76f36":"code","0e55584b":"code","04659e19":"code","a1fbdd39":"code","6592b27d":"code","768efbf0":"code","b144c72e":"code","b516e0a2":"code","7d64dd32":"code","cf27618b":"code","699f72c4":"code","30df9a5f":"code","92ec2858":"code","6ac73750":"markdown","96978db3":"markdown","127f240c":"markdown","723e5132":"markdown","d2c4fcc2":"markdown","1d707352":"markdown","bbce39b6":"markdown","71fce81a":"markdown","81bb9b11":"markdown","8287b9bd":"markdown","d35aade6":"markdown","db4a88c9":"markdown","43686b88":"markdown","9e29f21f":"markdown","174f7169":"markdown","01b7a537":"markdown","7c04be06":"markdown","50355ba8":"markdown","90563ba1":"markdown","768ab6cf":"markdown","bf7e8de2":"markdown","104ff176":"markdown","cb7a5c28":"markdown"},"source":{"1f192661":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import f1_score\nfrom nltk.corpus import stopwords\nimport string\nimport re\n%matplotlib inline","5214a00f":"df = pd.read_csv('..\/input\/articles.csv')\ndf.dtypes","3007e12f":"df.head(5)","597f54f6":"df['claps'] = df['claps'].apply(lambda x: int(float(x[:-1]) * 1000) if x[-1] == 'K' else int(x))\ndf.dtypes","09ae387e":"df.isnull().any()","ed11d858":"df['title_len'] = df['title'].str.len()\ndf['text_len'] = df['text'].str.len()\n\ndf['title'] = df['title'].apply(lambda x: x.lower())\ndf['text'] = df['text'].apply(lambda x: x.lower())\ndf['author'] = df['author'].apply(lambda x: x.lower())\n\ndf['title_clean'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords.words('english')]))\ndf['text_clean'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords.words('english')]))\n\ndf['title_clean'] = df['title_clean'].apply(lambda x: re.sub('[' + string.punctuation + '\u2014]', '', x))\ndf['text_clean'] = df['text_clean'].apply(lambda x: re.sub('[' + string.punctuation + '\u2014]', '', x))\n\ndf['title_clean'] = df['title_clean'].apply(lambda x: x.translate(str.maketrans('', '', string.digits)))\ndf['text_clean'] = df['text_clean'].apply(lambda x: x.translate(str.maketrans('', '', string.digits)))\n\ndf['title_clean'] = df['title_clean'].apply(lambda x: re.sub(' +', ' ', x))\ndf['text_clean'] = df['text_clean'].apply(lambda x: re.sub(' +', ' ', x))\n\ndf['title_clean_len'] = df['title_clean'].str.len()\ndf['text_clean_len'] = df['text_clean'].str.len()\n\ndf['full_text'] = df['author'] + ' ' + df['title_clean'] + ' ' + df['text_clean']\n\ndf.head(10)","d5c84619":"df = df.drop('link', axis=1)\ndf = df.drop('text', axis=1)\ndf = df.drop('title', axis=1)\ndf = df.drop('title_clean', axis=1)\ndf = df.drop('text_clean', axis=1)\ndf = df.drop('author', axis=1)\n\ndf = df.drop_duplicates()\n\ndf.describe(include='all')","ca2fd3b4":"df.boxplot(column=['claps', \n                   'text_len', \n                   'text_clean_len'])\nplt.show()","c6996bb6":"df.boxplot(column=['reading_time', \n                   'title_len', \n                   'title_clean_len'])\nplt.show()","5e2a3a9c":"sns.pairplot(df[['claps', \n                 'reading_time',\n                 'title_len', \n                 'title_clean_len',\n                 'text_len', \n                 'text_clean_len']], kind='reg')\nplt.show()","216b3987":"df[df['claps'] > 18000]","39eaa429":"df[df['text_len'] > 28000]","f04de311":"df[df['reading_time'] > 22]","97e68e53":"vectorizer = TfidfVectorizer(max_features=None)\nfull_text_features = vectorizer.fit_transform(df['full_text'])\nfull_text_features.shape","ef58ea43":"scaler = StandardScaler()\nnum_features = scaler.fit_transform(df[['reading_time', \n                                        'title_len',\n                                        'text_len',\n                                        'title_clean_len',\n                                        'text_clean_len']])\nnum_features.shape","5ba76f36":"full_text_features = np.concatenate([full_text_features.toarray(), num_features], axis=1)\nfull_text_features.shape","0e55584b":"X_train, X_test, y_train, y_test = train_test_split(full_text_features, df[['claps']].values, test_size=0.3)\nX_train.shape","04659e19":"y_test.shape","a1fbdd39":"reg = LinearRegression().fit(X_train, y_train)","6592b27d":"y_pred = reg.predict(X_test)\ny_pred.shape","768efbf0":"r2_score(y_test, y_pred)","b144c72e":"df[['claps']].hist()\nplt.show()","b516e0a2":"df['claps_categorical'] = df['claps'].apply(lambda x: 'rising star' if x >= 0 and x <= 10000 else 'star' if x >= 10001 and x <= 20000 else 'super star')\ndf[['claps', 'claps_categorical']].head(15)","7d64dd32":"X_train, X_test, y_train, y_test = train_test_split(full_text_features, df[['claps_categorical']].values, test_size=0.3)\nX_train.shape","cf27618b":"y_test.shape","699f72c4":"clf = RandomForestClassifier(n_estimators=1000, max_depth=2, random_state=0)\nclf.fit(X_train, y_train)","30df9a5f":"y_pred = clf.predict(X_test)\naccuracy_score(y_test, y_pred)","92ec2858":"confusion_matrix(y_test, y_pred, labels=['rising star', 'star', 'super star'])","6ac73750":"All variables has different scale, so I am using standard scaler to make scale equal.","96978db3":"Using Random Forest classifier for classification task.","127f240c":"In order to increase recognition accuracy need to play more with feature engineering and classifier hyperparameters tunning. What is more this dataset is to small to get good and confident results. ","723e5132":"By looking at accuracy score I can tell that model is performing well, 85 % accuracy.","d2c4fcc2":"Lets analyze some outliers and check if there are any reasonable grounds to exclude that data from data set.","1d707352":"In this notebook I will try to perform data analysis and evaluate most popular ML algorithms for clap prediction task.","bbce39b6":"Now I am going to remove unnecessary data columns and data will be ready for analysis. Lets do that.","71fce81a":"Testing linear regression model.","81bb9b11":"Performing train\/test split.","8287b9bd":"Performing new train.test split.","d35aade6":"After analysing the data found no reason to remove outliers (if I can call them so). It is normal data points and because of small data set they look like outliers.","db4a88c9":"Claps are devided into categories:\n0 - 10000: rising start\n10001 - 20000: star\n20001 - all other: super star","43686b88":"R2 metric shows that it is hard task for linear regression model to learn having so much features (R-squared=1\u20131=0). So I am changing number of claps to categorical values.","9e29f21f":"By looking at the data it seems we have some outliers, lets plot blox plot and check them out.","174f7169":"There are no NaN values in this data set so I am moving to the next step: feature engineering. I will add few more fields to my pandas data frame: len_title, len_text, title_clean, text_clean, len_title_clean, len_text_clean. I think those fields are self explainable. What is more before cleaning my text data I will change all text to lower case. After all I will combine author, title and text fields to single column. ","01b7a537":"As text features I desided to use TF-IDF. ","7c04be06":"It seems that claps data are in wrong format. Need to fix that.","50355ba8":"Distributions of claps, reading_time, text_len and text_clean_len are positive skewed. It shows that highest frequencies of particular entities are distributed near small values.\n\nWhat is more claps (dependant variable) has weak positive linear relationship with every independant variable. \n\nOf course we see strong relationship between reading_time and text_len and text_clean_len. Variables text_len, title_len and text_clean_len and title_clean_len are correlated.\n\nBlox plots shows that outliers are detected when claps > 18000, text_len > 28000, text_clean_len > 18500, reading_time > 22, title_len > 95 and title_clean_len > 81. Lets look at those data points.","90563ba1":"Predicting claps category.","768ab6cf":"Well confusion matrix shows that only rising star category was predicted correct. All other classes were predicted incorrectly.","bf7e8de2":"First of all let's read data and analyze data types.","104ff176":"After all text features are concatenated with left text length features.","cb7a5c28":"Lets look if there are any NaN values."}}