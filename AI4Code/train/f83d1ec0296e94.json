{"cell_type":{"9ed32595":"code","5de1ec2f":"code","04daf185":"code","cf7fbadc":"code","0a2d370e":"code","b3650a16":"code","ad6ff50b":"code","0bfff74b":"code","6ec72291":"code","d8bf937a":"code","10b6df5b":"code","03baf7cb":"code","3e89f160":"code","298d9351":"code","c6c0bce3":"code","305e7b2c":"code","7de0fad2":"code","1c33e0d0":"code","eaf31922":"code","87f2e9ad":"code","82a64cf6":"code","0cb952fa":"code","d80fe41a":"code","e15b1bfd":"code","079feb5a":"code","287a7d68":"code","155944de":"code","79c94f9a":"code","64217a99":"code","514abc2b":"code","7ecdd76f":"code","7e36e522":"code","48c2bdae":"code","7dc1087c":"code","67ae0aca":"code","857148c0":"code","62a11064":"code","7a3bf6fe":"code","9849b3c4":"code","8307ec6b":"code","a3728ca9":"code","57f69b94":"code","ef3332e9":"code","e2b6719f":"code","93441dfc":"code","d09127f2":"code","34fd5af1":"code","cc37ee84":"code","0a5f3e99":"code","abb138bc":"markdown","df5ef455":"markdown","92a11947":"markdown","153f5d52":"markdown","d665362e":"markdown","65fecd87":"markdown","d18c79b4":"markdown","9e68856e":"markdown","5bbb0566":"markdown","a51264f7":"markdown","146bcd38":"markdown","a9e60f79":"markdown","ea710c58":"markdown","5c788d39":"markdown","7333be3d":"markdown","c39048d6":"markdown","5d89372d":"markdown","e7884a1a":"markdown","55117888":"markdown","e240a087":"markdown","9b2c01c1":"markdown"},"source":{"9ed32595":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# Any results you write to the current directory are saved as output.","5de1ec2f":"# load data\nplayer_stats = pd.read_csv('..\/input\/game_skater_stats.csv')\nteam_stats = pd.read_csv('..\/input\/game_teams_stats.csv')\ngame = pd.read_csv('..\/input\/game.csv')\nplayer_info = pd.read_csv('..\/input\/player_info.csv')","04daf185":"# look at first few rows\nplayer_stats.head()","cf7fbadc":"# info on player_stats\nplayer_stats.info()","0a2d370e":"# summary statistics\nplayer_stats.describe().T","b3650a16":"def draw_4_distplots(var1, var2, var3, var4, data):\n    \"\"\"\n    Displays 4 distplots on a 2x2 grid\n    \"\"\"\n    fig, axes = plt.subplots(2,2,figsize=(20,12))\n\n    sns.distplot(data[var1], ax=axes[0,0])\n    axes[0,0].set_title(var1 + ' distribution')\n    sns.distplot(data[var2], ax=axes[0,1])\n    axes[0,1].set_title(var2 + ' distribution')\n    sns.distplot(data[var3], ax=axes[1,0])\n    axes[1,0].set_title(var3 + ' distribution')\n    sns.distplot(data[var4], ax=axes[1,1])\n    axes[1,1].set_title(var4 + ' distribution')\n\n    plt.tight_layout()\n    plt.show()","ad6ff50b":"# look at distribution of few of the variables\ndraw_4_distplots('shots', 'timeOnIce', 'goals', 'assists', player_stats)","0bfff74b":"# heatmap of correlations between variables in the data\nfig, ax = plt.subplots(figsize=(24,16))\nsns.heatmap(player_stats.corr(),\n            xticklabels=player_stats.corr().columns,\n            yticklabels=player_stats.corr().columns,\n            annot=True, fmt='.2f', ax=ax, cmap='bone')\nplt.show()","6ec72291":"# drop irrelevant columns\nplayer_stats.drop(['hits', 'penaltyMinutes', 'faceOffWins',\n                   'faceoffTaken', 'giveaways', 'shortHandedGoals',\n                   'shortHandedAssists', 'blocked', 'evenTimeOnIce',\n                   'shortHandedTimeOnIce', 'powerPlayTimeOnIce'],\n                    axis=1, inplace=True)","d8bf937a":"def draw_regplot(x_data,y_data,x_label,y_label,title):\n    \"\"\"\n    Display scatterplot with regression line between x_data and y_data\n    \"\"\" \n    fig, ax = plt.subplots(figsize=(18,12))\n    sns.regplot(x=x_data, y=y_data, color='b', ax=ax)\n    plt.xlabel(x_label)\n    plt.ylabel(y_label)\n    plt.title(title)\n    plt.show()","10b6df5b":"# take averages of the variables in the data and display relationship between shots and goals\navgs = player_stats.groupby('player_id').mean()\ndraw_regplot(avgs.goals, avgs.shots,\n             'Average goals', 'Average shots',\n             'Average goals by average shots')","03baf7cb":"# relationship between avg ice time and avg shots\ndraw_regplot(avgs.timeOnIce, avgs.shots,\n             'Average ice time', 'Average shots',\n             'Average ice time by average shots')","3e89f160":"# get primary position column from player_info table\nplayer_stats = pd.merge(player_stats, player_info[['player_id','primaryPosition']],\n                        on='player_id', how='left')","298d9351":"def draw_barplot(x_data,y_data,x_label,y_label,title,c):\n    \"\"\"\n    Display barplot of x_dta and y_data\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(16,10))                                        \n    sns.barplot(x=x_data, y=y_data, palette=c, ax=ax) \n    plt.xlabel(x_label)\n    plt.ylabel(y_label)\n    plt.title(title)\n    plt.show()","c6c0bce3":"# plot average shots\/game by different player positions\ndraw_barplot(player_stats.primaryPosition, player_stats.shots,\n             'Position', 'Mean shots', 'Average shots taken by position', 'Blues')","305e7b2c":"# look at head of team_stats data\nteam_stats.head()","7de0fad2":"# the columns I'm interested in are home\/away and team shots\n# there are 2 entries for each game, 1 for each team, so we have to merge on both game_id and team_id\nplayer_stats = pd.merge(player_stats, team_stats[['game_id', 'team_id', 'HoA', 'shots']],\n                        on=['game_id', 'team_id'], how='left', suffixes=('','_team'))","1c33e0d0":"# plot average shots taken in home and away games\ndraw_barplot(player_stats.HoA, player_stats.shots,\n             'Home\/Away', 'Mean shots', 'Average shots taken home\/away', 'Blues')","eaf31922":"# look at first rows of game table \ngame.head()","87f2e9ad":"# merge player table with relevant columns in game table\nplayer_stats = pd.merge(player_stats,\n                        game[['game_id','season', 'date_time', 'away_team_id', 'home_team_id']],\n                        on='game_id', how='left')","82a64cf6":"player_stats.date_time = pd.to_datetime(player_stats.date_time)\nplayer_stats.date_time = player_stats.date_time.dt.normalize()\nplayer_stats.set_index('date_time', inplace=True)\nplayer_stats.sort_index(inplace=True)","0cb952fa":"# create a new column in the dataframe with opponent_id for each players entry\ndef get_opponent(df):\n    if df['HoA'] == 'away':\n        return df['home_team_id']\n    else:\n        return df['away_team_id']\n    \nplayer_stats['opponent_id'] = player_stats.apply(get_opponent, axis=1)\nplayer_stats.drop(['away_team_id','home_team_id'], axis=1, inplace=True)","d80fe41a":"# calculate how many shots each opponent allows on average\nshots_allowed = pd.DataFrame(player_stats.groupby('opponent_id', as_index=False)['shots_team'].mean())","e15b1bfd":"draw_barplot(shots_allowed.opponent_id, shots_allowed.shots_team,\n             'Team', 'Average shots allowed', 'Average shots allowed\/game', 'husl')","079feb5a":"# select 4 different players and display how they shoot against different teams on average\nplayer1 = player_stats.loc[(player_stats.player_id == 8471724) & (player_stats.season == 20172018)]\nplayer2 = player_stats.loc[(player_stats.player_id == 8474190) & (player_stats.season == 20172018)]\nplayer3 = player_stats.loc[(player_stats.player_id == 8473512) & (player_stats.season == 20172018)]\nplayer4 = player_stats.loc[(player_stats.player_id == 8475167) & (player_stats.season == 20172018)]\n\nfig, axes = plt.subplots(2,2,figsize=(26,18))\n\nsns.barplot(player1.opponent_id, player1.shots, ax=axes[0,0])\naxes[0,0].set_title('Player 1')\nsns.barplot(player2.opponent_id, player2.shots, ax=axes[0,1])\naxes[0,1].set_title('Player 2')\nsns.barplot(player3.opponent_id, player3.shots, ax=axes[1,0])\naxes[1,0].set_title('Player 3')\nsns.barplot(player4.opponent_id, player4.shots, ax=axes[1,1])\naxes[1,1].set_title('Player 4')\n\nplt.tight_layout()\nplt.show()","287a7d68":"draw_barplot(player_stats.shots.value_counts().sort_index().index,\n             player_stats.shots.value_counts().sort_index().values,\n             'Number of shots', 'Count', 'Count of shots taken\/game', 'husl')","155944de":"# filter out player who did not play at least 10 games\nrowsBefore = player_stats.shape[0]\nplayer_stats = player_stats.groupby('player_id').filter(lambda x: len(x) >= 10)\nrowsAfter = player_stats.shape[0]\nprint('{} rows filtered out'.format(rowsBefore - rowsAfter))","79c94f9a":"# drop rows where a player has more than one entry on the same day, keep the first one\ndef drop_dup(df):\n    grp = df.groupby(['player_id', 'date_time'])\n    dropped = grp.first()\n    dropped.reset_index(level=0, inplace=True)\n    dropped.sort_index(inplace=True)\n    return dropped\nrowsBefore = player_stats.shape[0]\nplayer_stats = drop_dup(player_stats)\nrowsAfter = player_stats.shape[0]\nprint('{} rows filtered out'.format(rowsBefore - rowsAfter))","64217a99":"# take the average number of shots for each player each season\nmean_season = pd.DataFrame(player_stats.groupby(['player_id', 'season'])['shots'].mean())\n\nplayer_stats_1 = pd.merge(player_stats, mean_season, on=['player_id', 'season'], how='left', suffixes=('', '_mean_season'))\nplayer_stats_1.index = player_stats.index","514abc2b":"# take the standard deviation of shots for each player each season\nstd_season = pd.DataFrame(player_stats.groupby(['player_id', 'season'])['shots'].std())\n\nplayer_stats_1 = pd.merge(player_stats_1, std_season, on=['player_id', 'season'], how='left', suffixes=('', '_std_season'))\nplayer_stats_1.index = player_stats.index","7ecdd76f":"# get the average number of shots against each opponent for each player\nmean_v_opponent = pd.DataFrame(player_stats_1.groupby(['player_id', 'opponent_id'])['shots'].mean())\n\nplayer_stats_1 = pd.merge(player_stats_1, mean_v_opponent, on=['player_id', 'opponent_id'], how='left', suffixes=('', '_mean_v_opp'))\nplayer_stats_1.index = player_stats.index","7e36e522":"# calculate the ratio of games a player has taken more than 2 shots\ntemp = pd.DataFrame()\nfor plyr in player_stats_1['player_id'].unique():\n    for szn in player_stats_1['season'].unique():\n        l = player_stats_1[['player_id', 'shots', 'season']].loc[(player_stats_1['player_id'] == plyr) & (player_stats_1['season'] == szn)]\n        if (len(l)>0):\n            games_o_2s = (len(l.loc[l['shots'] > 2]))\/(len(l))\n        else:\n            games_o_2s = 0\n        temp = temp.append([[plyr, games_o_2s, szn]])\n\ntemp.rename(columns={0:'player_id', 1:'shots_over2', 2:'season'}, inplace=True)\n\nplayer_stats_1 = pd.merge(player_stats_1, temp, on=['player_id', 'season'], how='left')\nplayer_stats_1.index = player_stats.index","48c2bdae":"# calculate sum of shots over the previous 5 games\ndef shots_last5(df):\n    df['shots_shifted'] = df.groupby('player_id')['shots'].shift() # need to shift the data 1 step to exclude the 'current' game\n    grouped = pd.DataFrame(df.groupby('player_id')['shots_shifted'].rolling(5).sum())\n    grouped.reset_index(level=0, inplace=True)\n    grouped['shots_shifted'] = grouped.groupby('player_id')['shots_shifted'].transform(lambda x: x.fillna(x.mean()))\n    return grouped\n\ns_last5 = shots_last5(player_stats_1)\nplayer_stats_1 = pd.merge(player_stats_1, s_last5, on=['date_time', 'player_id'], suffixes=('', '_last5'))\nplayer_stats_1.drop('shots_shifted', axis=1, inplace=True)\nplayer_stats_1.rename(columns={'shots_shifted_last5':'shots_last5'}, inplace=True)","7dc1087c":"# calculate the sum of shots in the last 5 games against each opponent\ndef shots_last5_v_opp(df):\n    df['shots_shifted'] = df.groupby(['player_id','opponent_id'])['shots'].shift() \n    grouped = pd.DataFrame(df.groupby(['player_id','opponent_id'])['shots_shifted'].rolling(5).sum())\n    grouped.reset_index(level=[0,1], inplace=True)\n    grouped['shots_shifted'] = grouped.groupby(['player_id','opponent_id'])['shots_shifted'].transform(lambda x: x.fillna(x.mean()))\n    return grouped\n\ns_last5opp = shots_last5_v_opp(player_stats_1)\nplayer_stats_1 = pd.merge(player_stats_1, s_last5opp, on=['date_time', 'player_id', 'opponent_id'], suffixes=('', '_last5_vs_opp'))\nplayer_stats_1.drop('shots_shifted', axis=1, inplace=True)\nplayer_stats_1.rename(columns={'shots_shifted_last5_vs_opp':'shots_last5_vs_opp'}, inplace=True)\nplayer_stats_1.fillna(player_stats_1['shots_last5_vs_opp'].mean(), inplace=True)","67ae0aca":"# take the average ice time over the last 5 games\ndef ice_mean_last5(df):\n    df['icetime_shifted'] = df.groupby('player_id')['timeOnIce'].shift()\n    grouped = pd.DataFrame(df.groupby('player_id')['icetime_shifted'].rolling(5).mean())\n    grouped.reset_index(level=0, inplace=True)\n    grouped['icetime_shifted'] = grouped.groupby('player_id')['icetime_shifted'].transform(lambda x: x.fillna(x.mean()))\n    return grouped\n\nice_last5 = ice_mean_last5(player_stats_1)\nplayer_stats_1 = pd.merge(player_stats_1, ice_last5, on=['date_time', 'player_id'], suffixes=('', '_last5'))\nplayer_stats_1.drop('icetime_shifted', axis=1, inplace=True)\nplayer_stats_1.rename(columns={'icetime_shifted_last5':'mean_ice_last5'}, inplace=True)","857148c0":"draw_4_distplots('shots_mean_season',\n                'shots_std_season',\n                'shots_over2',\n                'mean_ice_last5',\n                player_stats_1)","62a11064":"# missing data check\nplayer_stats_1.isnull().any()","7a3bf6fe":"# downsample the dataset to get a balance between classes\nfrom sklearn.utils import resample\n\nmajority = player_stats_1[player_stats_1.shots<=2]\nminority = player_stats_1[player_stats_1.shots>2]\n\nmajority_downsampled = resample(majority, replace=False,\n                                n_samples=len(minority), random_state=1)\nplayer_stats_downsampled = pd.concat([majority_downsampled, minority])\nplayer_stats_downsampled.sort_index(inplace=True)\nplayer_stats_downsampled = player_stats_downsampled.groupby('player_id').filter(lambda x: len(x) >= 10)","9849b3c4":"before = player_stats_1.shots.map(lambda x: 1 if x>2 else 0).value_counts().sort_index()\nafter = player_stats_downsampled.shots.map(lambda x: 1 if x>2 else 0).value_counts().sort_index()\nbefore.rename(index={0: 'Under', 1:'Over'}, inplace=True)\nafter.rename(index={0: 'Under', 1:'Over'}, inplace=True)\n\nfig, (ax1,ax2) = plt.subplots(1,2, figsize=(16,8))\n\nax1.set(title='Before downsample',\n        xlabel='Shots under and over 2',\n        ylabel='No of samples')\nsns.barplot(before.index, before.values, palette='Blues', ax=ax1)\nax2.set(title='After downsample',\n        xlabel='Shots under and over 2',\n        ylabel='No of samples')\nsns.barplot(after.index, after.values, palette='Blues', ax=ax2)\n\nplt.tight_layout()\nplt.show()","8307ec6b":"# look at how the first rows of the current dataset\nplayer_stats_downsampled.head()","a3728ca9":"# grab the target column\nY = player_stats_downsampled['shots'].map(lambda x: 1 if x>2 else 0).values","57f69b94":"# convert the categorical columns to numerical\nplayer_stats_downsampled.primaryPosition = player_stats_downsampled.primaryPosition.map(lambda x: 0 if x == 'D' else 1) # 1 if forward 0 if defense\nplayer_stats_downsampled.HoA = player_stats_downsampled.HoA.map(lambda x: 1 if x == 'home' else 0) # 1 if home 0 if away","ef3332e9":"# grab the features to use for training the model\nfeatures = player_stats_downsampled[['primaryPosition', 'HoA', 'shots_mean_season',\n                                     'shots_std_season', 'shots_mean_v_opp', 'shots_over2',\n                                     'shots_last5', 'shots_last5_vs_opp', 'mean_ice_last5']]","e2b6719f":"# first few rows of the features\nfeatures.head()","93441dfc":"# standardize the data\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nfeatures_scaled = scaler.fit_transform(features)","d09127f2":"# split up the data with the last 20% (time wise) as testing portion\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(features_scaled, Y,\n                                                    test_size=.2, shuffle=False)","34fd5af1":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n\ntscv = TimeSeriesSplit(n_splits=5)\nlogit = LogisticRegression(random_state=1, class_weight='balanced')\n\nparams = {'C': np.logspace(-2,2,10),\n          'solver': ['liblinear', 'lbfgs', 'sag']}\n\nlogit_grid_searcher = GridSearchCV(estimator=logit,\n                                   param_grid=params,\n                                   cv=tscv,\n                                   scoring='roc_auc',\n                                   verbose=1)\nlogit_grid_searcher.fit(X_train, y_train)\n\nprint('Best params: {}'.format(logit_grid_searcher.best_params_))","cc37ee84":"from sklearn.metrics import accuracy_score, confusion_matrix\n\npreds = logit_grid_searcher.predict(X_test)\nprint('Prediction accuracy on test data: {}%'.format(np.round(accuracy_score(y_test,preds)*100, 2)))\n\ncf = confusion_matrix(y_test, preds)\nfig,ax=plt.subplots(figsize=(10,8))\nsns.heatmap(cf, annot=True, cmap='Blues', ax=ax, fmt='d')\nax.set_title('Confusion Matrix')\nax.set_ylabel('True Label')\nax.set_xlabel('Predicted Label')\nplt.show()","0a5f3e99":"np.round(pd.DataFrame(logit_grid_searcher.predict_proba(X_test),\n                      columns=['UNDER 2.5', 'OVER 2.5']).apply(lambda x: 1\/x).head(10), 2)","abb138bc":"The percentage of correctly classified samples on the test data is just under 73%, with a prettry similar precision for both classes as seen above(slilghtly better precision for class OVER 2 shots). My sense is that the model captures the probabilites for each outcome well, but obviosly missclassifies outlies\/upsets, or \"puck luck\", which there is a lot of in hockey\n\nIn order to convert the predictions to a desired odds format(decimal odds in my case) to compare to the odds offered from a bookmaker, simply use the predict_proba method to recieve probabilities for each class and then convert the values.","df5ef455":"Now we just have to grab the target column, select the features and do some final preparations before training the model.","92a11947":"Look at distribution of some of the new features","153f5d52":"The goal is to predict future games based on historical games, so when training the model later the data needs to be sorted by date, so that we train on older data and test on newer data.","d665362e":"Finally, the game dataframe contains an overview of each game. There are some information useful for this project, such as home\/away id, date and season.","65fecd87":"For the features I will create next, I will use the rolling method which includes the 'current' game(we only want to calculate on previous games excluding the current one). To handle this I use the shift method, to move the data 1 step in the right direction. These operations produces NaNs in the first few rows which I will just fill with the mean for each player.","d18c79b4":"The majority of shots taken is 2 or less, so will have to balance it out later. Now we filter out players who did not play at least 10 games, and filter out players who have 2 entries on the  same day.","9e68856e":"# NHL Player Shots","5bbb0566":"To try and classify wheter a player will take over or under 2 shots, I will use a logistic regression estimator, together with a time series split as cross validation in a grid search to find the optimal hyperparameters for the model.","a51264f7":"Now we have done some exploration, visualization and manipulation of the data.\nBut currently, most of the features related to shots taken is statistics that is only available after a match has concluded(except for home\/away etc). Since we can't use this data when predicting, we should not train the model on it either. Therefore, in the following section I will create some new features to use as training data for the model.","146bcd38":"We can see that different team allow different amount of shots against their own goal. Lets pick out a few players and compare how they shoot against different opponents.","a9e60f79":"In this kernel I will explore the NHL player stats data, specifically shots taken and variables related to it, with the end goal of building a model that can predict whether a player will take either 2 shots or less, or more than 2 shots in a given game.","ea710c58":"Earlier we saw that the majority of entries in the dataset is of players taking less than 2 shots. Since we have so much data, I'm choosing to downsample the majority class to balance out the two classes.","5c788d39":"My intuition tells me that players behave differently against different opponents, so lets pair together an opponent id to each entry in the dataframe.","7333be3d":"The majority of data I'm interested in for this project is from the player_stats dataframe, so lets start by exploring the structure and contents of it.","c39048d6":"The team_stats dataframe has team information on each game. The only data I'm interested in here are if they played home or away and total team shots.","5d89372d":"Remember that the goal is to predict over\/under 2 shots in the end to lets look at the distribution of shots taken again.","e7884a1a":"Most players does not score a goal or assist in a game and most players take between 0 and 3 shots which all seems reasonable. Lets take a look at correlations in between columns.","55117888":"The player_info table contains information on each player, which are not really relevant here, except for their position. ","e240a087":"The column of interest is shots, and here we can see that the most related variables to shots are ice time and goals.","9b2c01c1":"No missing data and the min value for timeOnIce is 1, which means players who did not play in a game is not included."}}