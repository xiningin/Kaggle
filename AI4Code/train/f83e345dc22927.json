{"cell_type":{"18cf3a87":"code","bdff5b0e":"code","4aced6cd":"code","1eabd87b":"code","ee397d54":"code","ced8485c":"code","bd0a0e0e":"code","22fadd84":"code","887d43e2":"code","717aeba4":"code","811f057d":"code","11f6da19":"code","86fc76b1":"code","95cc84bb":"code","df06d641":"code","41335247":"code","34c199cd":"code","285f8a8e":"code","5871b5a4":"code","5bb29faf":"code","fe07e1ca":"code","3debbc32":"code","1e8360b7":"code","ed86b35b":"markdown","0516c960":"markdown","385e9aa3":"markdown","d52691e0":"markdown","c01b7aef":"markdown","807b13fd":"markdown","b6b17fd8":"markdown","fa52b2ae":"markdown","b6200b6d":"markdown","6e3709fa":"markdown","f105153e":"markdown","593e6907":"markdown"},"source":{"18cf3a87":"!pip install kaggle-environments -U\n!cp -r ..\/input\/lux-ai-2021\/* .","bdff5b0e":"!git clone https:\/\/github.com\/aaiit\/lux-AI.git","4aced6cd":"!cp lux-AI\/Game\/*.py . \n!cp lux-AI\/Game\/lux\/* lux\/ \n!cp lux-AI\/*.png .","1eabd87b":"!mkdir snapshots\nfrom kaggle_environments import make","ee397d54":"from  tqdm import tqdm\n\"\"\"\nThe agent which was uploaded from Github follow a good strategy and save automatically the history in snapshots folder.\n\"\"\"\nenv = make(\"lux_ai_2021\", debug=True, configuration={\"annotations\": True, \"width\":12, \"height\":12})\nsteps = env.run([\"agent.py\", \"simple_agent\"])\n\n# we have already run the previous lines 900 episodes, for a size of the map 12x12 https:\/\/www.kaggle.com\/aithammadiabdellatif\/lux-ai-900-episodes-12x12","ced8485c":"env.render(mode=\"ipython\", width=400, height=400)","bd0a0e0e":"import os\nsn = os.listdir('..\/input\/lux-ai-900-episodes-12x12\/snapshots12\/snapshots')\n\n# missions_ = [i for i in sn if \"missions\" in i]\n# observation_ = [i for i in sn if \"observation\" in i]\ngame_state_ = [i for i in sn if \"game_state\" in i]\nactions_ = [i for i in sn if \"actions\" in i]\n\nlen(actions_)","22fadd84":"from sklearn.model_selection import train_test_split\ngame_state_train, game_state_test, actions_train, actions_test = train_test_split( game_state_, actions_, test_size=0.2, random_state=42)","887d43e2":"import numpy as np\nimport pickle\nfrom agent import game_logic\n\nstr_step = game_state_[0].split('-')[1].split(\".\")[0]\nwith open('..\/input\/lux-ai-900-episodes-12x12\/snapshots12\/snapshots\/game_state-{}.pkl'.format(str_step), 'rb') as handle:\n    game_state = pickle.load(handle)\n# with open('snapshots\/missions-{}.pkl'.format(str_step), 'rb') as handle:\n#     missions = pickle.load(handle)\n\nwith open('..\/input\/lux-ai-900-episodes-12x12\/snapshots12\/snapshots\/actions_as_Matrix-{}.pkl'.format(str_step), 'rb') as handle:\n    Y = pickle.load(handle)\n    \n# game_logic(game_state, missions, DEBUG=False)\n# print(np.array(game_state.convolved_rate_matrix))","717aeba4":"def get_inputs(game_state):\n    # Teh shape of the map\n    w,h = game_state.map.width, game_state.map.height\n    # The map of ressources\n    M = [ [0  if game_state.map.map[j][i].resource==None else game_state.map.map[j][i].resource.amount for i in range(w)]  for j in range(h)]\n    \n    M = np.array(M).reshape((h,w,1))\n    \n    # The map of units features\n    U_player = [ [[0,0,0,0,0] for i in range(w)]  for j in range(h)]    \n    units = game_state.player.units\n    for i in units:\n        U_player[i.pos.y][i.pos.x] = [i.type,i.cooldown,i.cargo.wood,i.cargo.coal,i.cargo.uranium]\n    U_player = np.array(U_player)\n    \n    U_opponent = [ [[0,0,0,0,0] for i in range(w)]  for j in range(h)]\n    units = game_state.opponent.units\n    for i in units:\n        U_opponent[i.pos.y][i.pos.x] = [i.type,i.cooldown,i.cargo.wood,i.cargo.coal,i.cargo.uranium]\n\n    U_opponent = np.array(U_opponent)\n    \n    # The map of cities featrues\n    e = game_state.player.cities\n    C_player = [ [[0,0,0] for i in range(w)]  for j in range(h)]\n    for k in e:\n        citytiles = e[k].citytiles\n        for i in citytiles:\n            C_player[i.pos.y][i.pos.x] = [i.cooldown,e[k].fuel,e[k].light_upkeep]\n    C_player = np.array(C_player)\n\n    e = game_state.opponent.cities\n    C_opponent = [ [[0,0,0] for i in range(w)]  for j in range(h)]\n    for k in e:\n        citytiles = e[k].citytiles\n        for i in citytiles:\n            C_opponent[i.pos.y][i.pos.x] = [i.cooldown,e[k].fuel,e[k].light_upkeep]\n    C_opponent = np.array(C_opponent)\n    \n    # stacking all in one array\n    E = np.dstack([M,U_opponent,U_player,C_opponent,C_player])\n    return E\nx = get_inputs(game_state)\n\n# Input\nx.shape","811f057d":"from tensorflow.keras.utils import  plot_model ,Sequence\nfrom random import random\n\nclass DataGenerator(Sequence):\n    'Generates data for Keras'\n    \n    def __init__(self, game_state_paths, actions_paths, dir_path ,batch_size=300 ,shuffle=True):\n        'Initialization'\n        self.game_state_paths = game_state_paths\n        self.actions_paths = actions_paths\n        self.dir_path = dir_path\n        \n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.game_state_paths) \/ self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Find list of IDs\n        list_IDs_temp = [k for k in indexes]\n\n        # Generate data\n        X, y = self.__data_generation(list_IDs_temp)\n\n        return X, y\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.actions_paths))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, list_IDs_temp):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        batch_game_state = list()\n        batch_actions = list()\n\n        # Generate data\n        for i in list_IDs_temp:\n            with open(self.dir_path+self.game_state_paths[i], 'rb') as handle:\n                game_state = pickle.load(handle)\n            with open(self.dir_path+self.actions_paths[i], 'rb') as handle:\n                y = pickle.load(handle)\n            \n            x = get_inputs(game_state)\n            batch_game_state.append(x)\n            batch_actions.append(y)\n        batch_game_state = np.array(batch_game_state,dtype = np.float32 )\n\n        return  batch_game_state,np.array(batch_actions , dtype = np.float32 )\n#         return  batch_game_state,batch_game_state\n    \n    \ntrain_generator = DataGenerator(game_state_train,actions_train,'..\/input\/lux-ai-900-episodes-12x12\/snapshots12\/snapshots\/',batch_size= 500 ,shuffle=True)\ntrain_steps = train_generator.__len__()\ntrain_steps","11f6da19":"val_generator = DataGenerator(game_state_test,actions_test,'..\/input\/lux-ai-900-episodes-12x12\/snapshots12\/snapshots\/',batch_size= 100 ,shuffle=True)\nval_steps = train_generator.__len__()\nval_steps","86fc76b1":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import layers\nimport tensorflow_hub as hub\nfrom collections import deque\nimport random\nimport math\nfrom tensorflow.keras import backend as K\n\ndef custom_mean_squared_error(y_true, y_pred):\n    y_units_true = y_true[:,:,:,:6]\n    y_cities_true = y_true[:,:,:,6:]\n\n    y_units_pred = y_pred[:,:,:,:6]\n    y_cities_pred = y_pred[:,:,:,6:]\n    \n    \n    is_unit = tf.keras.backend.max(y_units_true,axis = -1)\n    is_city = tf.keras.backend.max(y_cities_true,axis = -1)\n    \n    y_units_pred*= K.stack([is_unit]*6, axis=-1)\n    y_cities_pred*= K.stack([is_city]*2, axis=-1)\n    \n    loss1 = K.square(y_units_pred - y_units_true)\/K.maximum(K.sum(is_unit),1)\n    loss2 = K.square(y_cities_pred - y_cities_true)\/K.maximum(K.sum(is_city),1)\n    return K.concatenate([loss1,loss2])\n\ndef units_accuracy(y_true, y_pred):\n    y_units_true = y_true[:,:,:,:6]\n    y_cities_true = y_true[:,:,:,6:]\n\n    y_units_pred = y_pred[:,:,:,:6]\n    y_cities_pred = y_pred[:,:,:,6:]\n    \n    is_unit = tf.keras.backend.max(y_units_true,axis = -1)\n    y_units_pred*= K.stack([is_unit]*6, axis=-1)\n    return K.cast(K.equal(y_units_true, K.round(y_units_pred)), \"float32\")\/K.maximum(K.sum(is_unit),1)\n\ndef cities_accuracy(y_true, y_pred):\n    y_units_true = y_true[:,:,:,:6]\n    y_cities_true = y_true[:,:,:,6:]\n\n    y_units_pred = y_pred[:,:,:,:6]\n    y_cities_pred = y_pred[:,:,:,6:]\n    \n    is_city = tf.keras.backend.max(y_cities_true,axis = -1)\n    y_cities_pred*= K.stack([is_city]*2, axis=-1)\n    \n    return K.cast(K.equal(y_cities_true, K.round(y_cities_pred)), \"float32\")\/K.maximum(K.sum(is_city),1)","95cc84bb":"# from keras import  models\n# base_model = VGG16(weights=None,input_shape = (120,120,17))\n# model_VGG16 = models.Model(inputs=base_model.input, outputs=base_model.get_layer('flatten').output)","df06d641":"# import numpy as np\n# import pandas as pd\n\n# from keras.models import Model\n\n# # this is our input placeholder\n# input_ = layers.Input(shape=(12,12,17))\n# # \"encoded\" is the encoded representation of the input\n\n\n\n# encoded = layers.Dense(17, activation='sigmoid')(input_)\n# # \"decoded\" is the lossy reconstruction of the input\n\n\n# decoded = layers.Dense(17, activation='relu')(encoded)\n\n# # this model maps an input to its reconstruction\n# autoencoder = Model(input_, decoded)\n# autoencoder.summary()\n\n\n\n# autoencoder.compile(optimizer='adam', loss=\"mse\")\n","41335247":"# autoencoder.fit_generator(train_generator, steps_per_epoch=5,epochs= 10,validation_data=val_generator,validation_steps=5)","34c199cd":"from tensorflow.keras import activations\n\ndef get_model(game_state):\n    inputs = keras.Input(shape=get_inputs(game_state).shape,name = 'The game map')\n    \n    f = layers.Flatten()(inputs)\n    \n    f = layers.Dense(12*12*1,activation = \"sigmoid\")(f)\n    f = layers.Dense(12*12*8,activation = \"sigmoid\")(f)\n    f = layers.Reshape((12,12,8))(f)\n    \n    units = layers.Lambda(lambda x : x[:,:,:,:6])(f)\n    cities = layers.Lambda(lambda x : x[:,:,:,6:])(f)\n\n    units = layers.Activation(activations.softmax)(units)\n    cities = layers.Activation(activations.softmax)(cities)\n    \n    \n    output = layers.Concatenate()([units,cities])\n    model = keras.Model(inputs = inputs, outputs = output)\n    opt = keras.optimizers.Adam(learning_rate=0.000001)\n\n    model.compile(optimizer= \"adam\", loss= custom_mean_squared_error ,metrics = [\"accuracy\"])\n    \n    return model\n\n\nmodel =get_model(game_state)\nmodel.summary()","285f8a8e":"tf.keras.utils.plot_model(\n    model,\n    to_file=\"model.png\",\n    show_shapes=1,\n    show_dtype=1,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)","5871b5a4":"model.compile(optimizer= \"adam\", loss= custom_mean_squared_error ,metrics = [])\nresults = model.fit_generator(train_generator, steps_per_epoch=5,epochs= 50,validation_data=val_generator,validation_steps=5)","5bb29faf":"import matplotlib.pyplot as plt\n\nloss = results.history[\"loss\"]\nval_loss = results.history[\"val_loss\"]\nplt.plot(loss,label = \"loss\")\nplt.xlabel(\"iterations\")\nplt.legend()\nplt.savefig(\"loss.png\")","fe07e1ca":"model.save_weights('model.h5')","3debbc32":"def get_prediction_actions(y,player):\n    option = np.argmax(y,axis = 2) \n    \"\"\"\n    1  : c\n    2  : s\n    3  : n\n    4  : w\n    5  : e\n    6  : build_city\n    7  : research\n    8  : buid_worker  \n    \"\"\"\n    actions = []\n    for i in player.units:\n        d = \"csnwe#############\"[option[i.pos.y,i.pos.x]]\n        if option[i.pos.y,i.pos.x]<5:actions.append(i.move(d))\n        elif option[i.pos.y,i.pos.x]==5 and i.can_build(game_state.map):actions.append(i.build_city())\n    \n    city_tiles: List[CityTile] = []\n    for city in player.cities.values():\n        for city_tile in city.citytiles:\n            if option[city_tile.pos.y,city_tile.pos.x]==6:\n                action = city_tile.research()\n                actions.append(action)\n            if option[city_tile.pos.y,city_tile.pos.x]==7:\n                action = city_tile.build_worker()\n                actions.append(action)\n    return actions,option\n\n\n\nx = get_inputs(game_state)\nprint(x.shape)\n\ny = model.predict(np.asarray([x]))[0]\nprint(\"y shape\",y.shape)\nactions = get_prediction_actions(y,game_state.players[0])[0]\n\nprint(\"predicted actions : \",actions)","1e8360b7":"!rm *.py && rm -rf __pycache__\/ && rm -rf lux\/ && rm snapshots -rf && rm  .\/lux-AI -rf","ed86b35b":"# Splitting to train an test","0516c960":"![image.png](attachment:9c143a43-e633-46b4-a856-7e2b940b96a2.png)","385e9aa3":"# Data generator","d52691e0":"# Load deterministic agent ","c01b7aef":"# Model","807b13fd":"# Test","b6b17fd8":"# Read saved episodes","fa52b2ae":"# Game_state to inputs matrix","b6200b6d":"The inputs of the model will be the map of the features, The output also will be a map of actions as matrix.","6e3709fa":"Last Game Scenario Rendering\n\n","f105153e":"# Create episodes\nRun several episodes, automatically the game_state and actions made by the agent will be save to be used further.","593e6907":"<!-- - [ ] Run a deterministic agent play many scenarios, and save game_states and actions.\n- [ ] building the model\n- [ ] custom loss and metrics\n- [ ] Train the model\n- [ ] Prepare inferance -->"}}