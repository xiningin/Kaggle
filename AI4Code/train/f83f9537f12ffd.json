{"cell_type":{"c40ce3bf":"code","151c390f":"code","d4b08fdb":"code","def3fe62":"code","2d5577d2":"code","55f03604":"code","7b61b928":"code","b2f44351":"code","d7c4bda0":"code","f2f1a62b":"code","fb614374":"code","18a3c98a":"code","0393bc6d":"code","54d68be7":"code","c90f4048":"code","56866b30":"code","569b10b8":"code","d8a8e65a":"code","afad8a79":"code","fe5845ee":"code","436cdef9":"markdown","907068e4":"markdown","5933ce1f":"markdown","f8b9557a":"markdown","0711a064":"markdown","eebfb86a":"markdown","09659f13":"markdown","029062b4":"markdown","a8b7fd6d":"markdown","2a07db56":"markdown","24d06621":"markdown","c16963a6":"markdown","dd2a245c":"markdown","e7fa6dc1":"markdown","941ba92f":"markdown","b3b5d8ed":"markdown"},"source":{"c40ce3bf":"import pandas as pd\n\nd_fake = pd.read_csv('..\/input\/fake-news-data\/fnn_politics_fake.csv')\nheadlines_fake = d_fake.drop(['id', 'news_url', 'tweet_ids'], axis=1).rename(columns={'title': 'headline'})\nheadlines_fake['fake'] = 1\n\nd_real = pd.read_csv('..\/input\/fake-news-data\/fnn_politics_real.csv')\nheadlines_real = d_real.drop(['id', 'news_url', 'tweet_ids'], axis=1).rename(columns={'title': 'headline'})\nheadlines_real['fake'] = 0\n\neval_data = pd.concat([headlines_fake, headlines_real])","151c390f":"import os\n\ndef read_data(d):\n    files = os.listdir(d)\n    headlines, contents = [], []\n    for fname in files:\n        if fname[:5] != 'polit':\n            continue\n        \n        f = open(d + '\/' + fname)\n        text = f.readlines()\n        f.close()\n\n        if len(text) == 2:\n            # One of the lines is missing\n            if len(text[1]) <= 1:\n                # There is no article content or headline\n                continue\n        elif len(text) >= 3:\n            # More than one empty line encountered\n            text[1] = text[-1]\n        else:\n            # Only one or zero lines is file\n            continue\n        \n        headline, content = text[0][:-1].strip().rstrip(), text[1][:-1]\n        headlines.append(headline)\n        contents.append(content)\n    \n    return headlines, contents\n\n\nfake_dir = '..\/input\/fake-news-data\/fnd_news_fake'\nfake_headlines, fake_content = read_data(fake_dir)\nfake_headlines = pd.DataFrame(fake_headlines, columns=['headline'])\nfake_headlines['fake'] = 1\n\nreal_dir = '..\/input\/fake-news-data\/fnd_news_real'\nreal_headlines, real_content = read_data(real_dir)\nreal_headlines = pd.DataFrame(real_headlines, columns=['headline'])\nreal_headlines['fake'] = 0","d4b08fdb":"eval_data = pd.concat([eval_data, fake_headlines, real_headlines])\neval_data['fake'].value_counts()\neval_data.head()","def3fe62":"all_news = pd.read_csv('..\/input\/all-the-news\/articles3.csv', nrows=300000)\nall_news = all_news.rename(columns={'title': 'headline'})\nall_news['fake'] = 0\ndata = all_news[['headline', 'fake']]\n\n# data = pd.concat([data, all_news])\ndata.head()","2d5577d2":"import warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nimport numpy as np\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\ndef format_data(data, max_features, maxlen, tokenizer=None, shuffle=False):\n    if shuffle:\n        data = data.sample(frac=1).reset_index(drop=True)\n    \n    data['headline'] = data['headline'].apply(lambda x: str(x).lower())\n\n    X = data['headline']\n    Y = data['fake'].values # 0: Real; 1: Fake\n\n    if not tokenizer:\n        filters = \"\\\"#$%&()*+.\/<=>@[\\\\]^_`{|}~\\t\\n\"\n        tokenizer = Tokenizer(num_words=max_features, filters=filters)\n        tokenizer.fit_on_texts(list(X))\n\n    X = tokenizer.texts_to_sequences(X)\n    X = pad_sequences(X, maxlen=maxlen)\n\n    return X, Y, tokenizer","55f03604":"max_features, max_len = 5000, 25\nX, Y, tokenizer = format_data(data, max_features, max_len, shuffle=True)\nX_eval, Y_eval, tokenizer = format_data(eval_data, max_features, max_len, tokenizer=tokenizer)","7b61b928":"import pickle\npickle.dump(tokenizer, open('tokenizer.pkl', 'wb'))","b2f44351":"from keras.layers import Input, Dense, Bidirectional, GRU, Embedding, Dropout, LSTM\nfrom keras.layers import concatenate, SpatialDropout1D, GlobalAveragePooling1D, GlobalMaxPooling1D\nfrom keras.models import Model\nfrom keras import regularizers\n\nepochs=20\n\n# Input shape\ninp = Input(shape=(max_len,))\n\nencoder = Embedding(max_features, 50)(inp)\nencoder = Bidirectional(LSTM(75, return_sequences=True))(encoder)\nencoder = Bidirectional(LSTM(25, return_sequences=True,\n                        activity_regularizer=regularizers.l1(10e-5)))(encoder)\n\ndecoder = Bidirectional(LSTM(75, return_sequences=True))(encoder)\ndecoder = GlobalMaxPooling1D()(decoder)\ndecoder = Dense(50, activation='relu')(decoder)\ndecoder = Dense(max_len)(decoder)\n\nmodel = Model(inputs=inp, outputs=decoder)\nmodel.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\nmodel.fit(X, X, epochs=epochs, batch_size=64, verbose=1)\n\nmodel.save_weights('model{}.h5'.format(epochs))","d7c4bda0":"model.evaluate(X, X)","f2f1a62b":"results = model.predict(X_eval, batch_size=1, verbose=1)","fb614374":"mse = np.mean(np.power(X_eval - results, 2), axis=1)\nerror_df = pd.DataFrame({'reconstruction_error': mse,\n                         'true_class': Y_eval})\nerror_df.describe()","18a3c98a":"from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,\n                             roc_curve, recall_score, classification_report, f1_score,\n                             precision_recall_fscore_support)","0393bc6d":"LABELS = ['REAL', 'FAKE']\nbest, threshold = -1, -1\n\n# General Search\nfor t in range(0, 3500000, 10000):\n    y_pred = [1 if e > t else 0 for e in error_df.reconstruction_error.values]\n    score = f1_score(y_pred, error_df.true_class, average='micro', labels=[0, 1])\n    if score > best:\n        best, threshold = score, t\n\n# Specialized Search around general best\nfor t in range(threshold-10000, threshold+10000):\n    y_pred = [1 if e > t else 0 for e in error_df.reconstruction_error.values]\n    score = f1_score(y_pred, error_df.true_class, average='micro', labels=[0, 1])\n    if score > best:\n        best, threshold = score, t\n\nprint(threshold, best)","54d68be7":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ngroups = error_df.groupby('true_class')\nfig, ax = plt.subplots()\n\nfor name, group in groups:\n    ax.plot(group.index, group.reconstruction_error, marker='o', ms=3.5, linestyle='',\n            label=\"Fake\" if name == 1 else \"Real\")\n\nax.hlines(threshold, ax.get_xlim()[0], ax.get_xlim()[1], colors=\"r\", zorder=100, label='Threshold')\nax.legend()\nplt.title(\"Reconstruction error for different classes\")\nplt.ylabel(\"Reconstruction error\")\nplt.xlabel(\"Data point index\")\nplt.show();","c90f4048":"LABELS = ['FAKE', 'REAL']\nerrors = error_df.reconstruction_error.values\ny_pred = [1 if e > threshold else 0 for e in errors] # final predictions\nconf_matrix = confusion_matrix(error_df.true_class, y_pred)\nplt.figure(figsize=(12, 12))\nsns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\nplt.title(\"Confusion matrix\")\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.show()","56866b30":"from sklearn.metrics import f1_score\n\ndef accuracy_f1(preds, correct):\n    return f1_score(preds, correct, average='micro', labels=[0, 1])\n\naccuracy_f1(y_pred, error_df.true_class)","569b10b8":"from sklearn.preprocessing import MinMaxScaler\nminmax_0_05 = MinMaxScaler(feature_range=(0, 0.5))\nminmax_05_1 = MinMaxScaler(feature_range=(0.5, 1))","d8a8e65a":"errors_below = np.array([i for i, e in enumerate(errors) if e <= threshold])\nerrors_above = np.array([i for i, e in enumerate(errors) if e > threshold])\n\nminmax_0_05.fit(errors[errors_below].reshape(-1, 1))\nminmax_05_1.fit(errors[errors_above].reshape(-1, 1))","afad8a79":"errors_mm = np.array([minmax_0_05.transform(e.reshape(1, -1)) if i in errors_below\n                      else minmax_05_1.transform(e.reshape(1, -1))\n                      for i, e in enumerate(errors)]).flatten()\n\ny_pred2 = [1 if e > 0.5 else 0 for e in errors_mm]","fe5845ee":"def accuracy_percentile(preds, Y_validate):\n    real_correct, fake_correct, total_correct = 0, 0, 0\n    _, (fake_count, real_count) = np.unique(Y_validate, return_counts=True)\n\n    for i, r in enumerate(preds):\n        if r == Y_validate[i]:\n            total_correct += 1\n            if r == 0:\n                fake_correct += 1\n            else:\n                real_correct += 1\n\n    print('Real Accuracy:', real_correct\/real_count * 100, '%')\n    print('Fake Accuracy:', fake_correct\/fake_count * 100, '%')\n    print('Total Accuracy:', total_correct\/(real_count + fake_count) * 100, '%')\n\n\naccuracy_percentile(y_pred2, error_df.true_class)","436cdef9":"## Classification\nNow we need to calculate the reconstruction error of the test set.","907068e4":"We are now going to calculate the percentile accuracy of the scaled predictions, alongside the F1-score (this score may differ slightly for values right around the threshold, but the difference is negligible).","5933ce1f":"Time to compute our results!","f8b9557a":"Finally, we are going to convert our errors array to the scaled outputs.","0711a064":"The `max_feature`s and `max_len` variables denote the length of each vector and the vocabulary length.","eebfb86a":"We now need to compute the optimal threshold to make our predictions. We will split the process into two:\n1. Find the general range where the threshold lies.\n2. In that range, find a more specific threshold value.","09659f13":"We will now concatenate these two new datasets into an evaluation dataset.","029062b4":"Next, we are going to compute the F1 score as well.","a8b7fd6d":"Next we will read fake and real headlines from the Fake News Dataset. Each file has a headline as the first line, followed by some white space and then the article content. We need to exract the headline and the content of each file and store them in lists.","2a07db56":"With the scalers initialized, we need to fit them. We are going to fit `minmax_0_05` to items below the threshold, and `minmax_05_1` to items above the threshold.","24d06621":"Scaling Error\nRight now the errors lie in  `[0,\u221e)` . It is useful in some cases (for example, using these predictions in ensembling) to scale the error in  `[0,1]` . We cannot though simply min-max all of the values together, since then the final output wouldn't be a representative probability. Instead, we are going to do the following: Values below the threshold will be scaled to  `[0,0.5]`  and values above the threshold will be scaled to  `[0.5,1]` . The threshold, after scaling, is set to 0.5.","c16963a6":"## Reading Data\nFirst we are going to read the evaluation data for real and fake headlines, and then we are going to concatenate the two dataframes.","dd2a245c":"Now let's read our training data:","e7fa6dc1":"## Model\nThe model we will use is based around a bi-directional RNN (either GRU or LSTM), with max pooling. The encoder is comprised of two RNN layers, while the decoder uses an RNN with a dense layer on top of it. The reconstruction of the original input occurs on a final Dense layer.","941ba92f":"## Data Processing\nWe also need to format the data. We will split the dataset into features `X` and target `Y`. For `Y`, we simply store the label at the target column. For `X`, we are first going to tokenise and pad our text input before storing it","b3b5d8ed":"We are going to visualize the data points against the threshold line."}}