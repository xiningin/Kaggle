{"cell_type":{"4b665072":"code","476f1f29":"code","f4c6c095":"code","a3276730":"code","64f2a3a7":"code","1600b37c":"code","583295c1":"code","c0b7535a":"code","ad516e6b":"code","ad87c2a6":"code","4a27e34d":"code","31c2fbc0":"code","66b4f9da":"code","2a91dede":"code","d7a92c48":"code","a67058e9":"code","5910576d":"code","647de501":"code","f02c4d33":"code","705f68e2":"code","2d11cdc4":"code","b909b2c3":"code","578bfc1e":"code","dde51df3":"code","d8b062d3":"code","62a6e0c4":"code","af8c8c57":"code","6b0f083a":"code","ea4cab73":"code","52fd0e93":"code","7b33db51":"code","8c91e22b":"code","d74af8bd":"code","d1e309e6":"code","146abcd8":"code","83139255":"code","9498060c":"code","cd1dae1a":"markdown","d17eb72b":"markdown","69b85ffd":"markdown","d59982fc":"markdown","95ad7e6c":"markdown","25ff2d80":"markdown","89eeda63":"markdown","16eb94bd":"markdown","4bebdabf":"markdown","2c05bbf8":"markdown","da1c742e":"markdown","aef53bc0":"markdown","4e0c7a26":"markdown","db944cc1":"markdown","8e85d364":"markdown","159ef16c":"markdown","4046daf9":"markdown"},"source":{"4b665072":"pip install -U lightautoml","476f1f29":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport pandas_profiling \nimport datetime as dt\n\nimport os\nimport time\nimport re\n\n# Installed libraries\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.model_selection import train_test_split\nimport torch\n\n# Imports from our package\nfrom lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\nfrom lightautoml.dataset.roles import DatetimeRole\nfrom lightautoml.tasks import Task\nfrom lightautoml.utils.profiler import Profiler","f4c6c095":"N_THREADS = 4 # threads cnt for lgbm and linear models\nN_FOLDS = 5 # folds cnt for AutoML\nRANDOM_STATE = 42 # fixed random state for various reasons\nTEST_SIZE = 0.2 # Test size for metric check\nTIMEOUT = 1800 # Time in seconds for automl run","a3276730":"train_df=pd.read_csv(\"..\/input\/train.csv\")\ntest_df=pd.read_csv(\"..\/input\/test.csv\")","64f2a3a7":"train_df.head()","1600b37c":"test_df.head()","583295c1":"def missingdata(data):\n    total = data.isnull().sum().sort_values(ascending = False)\n    percent = (data.isnull().sum()\/data.isnull().count()*100).sort_values(ascending = False)\n    ms=pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    ms= ms[ms[\"Percent\"] > 0]\n    f,ax =plt.subplots(figsize=(8,6))\n    plt.xticks(rotation='90')\n    fig=sns.barplot(ms.index, ms[\"Percent\"],color=\"green\",alpha=0.8)\n    plt.xlabel('Features', fontsize=15)\n    plt.ylabel('Percent of missing values', fontsize=15)\n    plt.title('Percent missing data by feature', fontsize=15)\n    return ms","c0b7535a":"missingdata(train_df)","ad516e6b":"missingdata(test_df)","ad87c2a6":"test_df['Age'].mean()","4a27e34d":"train_df['Embarked'].fillna(train_df['Embarked'].mode()[0], inplace = True)","31c2fbc0":"test_df['Fare'].fillna(test_df['Fare'].median(), inplace = True)","66b4f9da":"drop_column = ['Cabin']\ntrain_df.drop(drop_column, axis=1, inplace = True)\ntest_df.drop(drop_column,axis=1,inplace=True)","2a91dede":"test_df['Age'].fillna(test_df['Age'].median(), inplace = True)\ntrain_df['Age'].fillna(train_df['Age'].median(), inplace = True)","d7a92c48":"print('Checking the nan value in train data')\nprint(train_df.isnull().sum())\nprint('___'*20)\nprint('Checking the nan value in test data')\nprint(test_df.isnull().sum())","a67058e9":"## combine test and train as single to apply some function\nall_data=[train_df,test_df]","5910576d":"# Create new feature FamilySize as a combination of SibSp and Parch\nfor dataset in all_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1","647de501":"import re\n# Define function to extract titles from passenger names\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n# Create a new feature Title, containing the titles of passenger names\nfor dataset in all_data:\n    dataset['Title'] = dataset['Name'].apply(get_title)\n# Group all non-common titles into one single grouping \"Rare\"\nfor dataset in all_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', \n                                                 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n","f02c4d33":"## create bin for age features\nfor dataset in all_data:\n    dataset['Age_bin'] = pd.cut(dataset['Age'], bins=[0,12,20,40,120], labels=['Children','Teenage','Adult','Elder'])\n","705f68e2":"## create bin for fare features\nfor dataset in all_data:\n    dataset['Fare_bin'] = pd.cut(dataset['Fare'], bins=[0,7.91,14.45,31,120], labels=['Low_fare','median_fare',\n                                                                                      'Average_fare','high_fare'])","2d11cdc4":"### for our reference making a copy of both DataSet start working for copy of dataset\ntraindf=train_df\ntestdf=test_df","b909b2c3":"all_dat=[traindf,testdf]","578bfc1e":"for dataset in all_dat:\n    drop_column = ['Age','Fare','Name','Ticket']\n    dataset.drop(drop_column, axis=1, inplace = True)","dde51df3":"drop_column = ['PassengerId']\ntraindf.drop(drop_column, axis=1, inplace = True)","d8b062d3":"testdf.head(2)","62a6e0c4":"traindf = pd.get_dummies(traindf, columns = [\"Sex\",\"Title\",\"Age_bin\",\"Embarked\",\"Fare_bin\"],\n                             prefix=[\"Sex\",\"Title\",\"Age_type\",\"Em_type\",\"Fare_type\"])","af8c8c57":"testdf = pd.get_dummies(testdf, columns = [\"Sex\",\"Title\",\"Age_bin\",\"Embarked\",\"Fare_bin\"],\n                             prefix=[\"Sex\",\"Title\",\"Age_type\",\"Em_type\",\"Fare_type\"])","6b0f083a":"sns.heatmap(traindf.corr(),annot=True,cmap='RdYlGn',linewidths=0.2) #data.corr()-->correlation matrix\nfig=plt.gcf()\nfig.set_size_inches(20,12)\nplt.show()","ea4cab73":"g = sns.pairplot(data=train_df, hue='Survived', palette = 'seismic',\n                 size=2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\ng.set(xticklabels=[])","52fd0e93":"# Start of Pandas Profiling process\nstart_time = dt.datetime.now()\nprint(\"Started at \", start_time)\nreport = pandas_profiling.ProfileReport(traindf)\nreport","7b33db51":"print('Pandas Profling finished!!')\nfinish_time = dt.datetime.now()\nprint(\"Finished at \", finish_time)\nelapsed = finish_time - start_time\nprint(\"Elapsed time: \", elapsed)","8c91e22b":"def acc_score(y_true, y_pred, **kwargs):\n    return accuracy_score(y_true, (y_pred > 0.5).astype(int), **kwargs)\n\ndef f1_metric(y_true, y_pred, **kwargs):\n    return f1_score(y_true, (y_pred > 0.5).astype(int), **kwargs)\n\ntask = Task('binary', metric = f1_metric)","d74af8bd":"roles = {\n    'target': 'Survived',\n    'drop': ['PassengerId', 'Name','Ticket'],\n}","d1e309e6":"%%time \n\nautoml = TabularUtilizedAutoML(task = task, \n                       timeout = TIMEOUT,\n                       cpu_limit = N_THREADS,\n                       general_params = {'use_algos': [['linear_l2', 'lgb', 'lgb_tuned',]]},\n                       reader_params = {'n_jobs': N_THREADS})\noof_pred = automl.fit_predict(traindf, roles = roles)\nprint('oof_pred:\\n{}\\nShape = {}'.format(oof_pred[:10], oof_pred.shape))","146abcd8":"%%time\n\ntest_pred = automl.predict(testdf)\nprint('Prediction for test data:\\n{}\\nShape = {}'.format(test_pred[:10], test_pred.shape))\n\nprint('Check scores...')\nprint('OOF score: {}'.format(acc_score(traindf['Survived'].values, oof_pred.data[:, 0])))","83139255":"submission = pd.DataFrame()\nsubmission[\"PassengerId\"] = test_df[\"PassengerId\"]\nsubmission['Survived'] = (test_pred.data[:, 0] > 0.5).astype(int)\nsubmission.to_csv('automl_utilized_3600_f1_metric.csv', index = False)","9498060c":"submission","cd1dae1a":"# Model Training \nHere we are using lightAutoML (LAMA) library. Learn and explore more about it : [here](https:\/\/lightautoml.readthedocs.io\/en\/latest\/automl.html)","d17eb72b":"### Pairplots\nFinally let us generate some pairplots to observe the distribution of data from one feature to the other. Once again we use Seaborn to help us.","69b85ffd":"Creating dummies : )","d59982fc":"## Identifying & Filling Missing Value ","95ad7e6c":"### Filling missing Values","25ff2d80":"### Cabin Featueres has more than 75% of missing data in both Test and train data so we are remove the Cabin ","89eeda63":"### Correlation Between The Features","16eb94bd":"Interpreting The Heatmap : \nThe first thing to note is that only the numeric features are compared as it is obvious that we cannot correlate between alphabets or strings. Before understanding the plot, let us see what exactly correlation is.\n\nPOSITIVE CORRELATION: If an increase in feature A leads to increase in feature B, then they are positively correlated. A value 1 means perfect positive correlation.\n\nNEGATIVE CORRELATION: If an increase in feature A leads to decrease in feature B, then they are negatively correlated. A value -1 means perfect negative correlation.\n\nNow lets say that two features are highly or perfectly correlated, so the increase in one leads to increase in the other. This means that both the features are containing highly similar information and there is very little or no variance in information. This is known as MultiColinearity as both of them contains almost the same information.\n\nSo do you think we should use both of them as one of them is redundant. While making or training models, we should try to eliminate redundant features as it reduces training time and many such advantages.","4bebdabf":"#### Now every thing almost ready only one step we converted the catergical features in numerical by using dummy variable","2c05bbf8":"## Feature engineering\n\nFeature engineering is the art of converting raw data into useful features. ","da1c742e":"# \ud83d\udcca Automatic EDA using Pandas Profiling \ud83d\udcda\nPandas profiling is an open source Python module with which we can quickly do an exploratory data analysis with just a few lines of code. In short, what pandas profiling does is save us all the work of visualizing and understanding the distribution of each variable.","aef53bc0":"Hope you liked this kernel, if you, then please **UPVOTE** and **COMMENT** below your remarks, I would be glad to listen to you all ; )\nEvery suggestion and correction is accepted happily \ud83d\ude01\n# Thank You \ud83d\ude0a","4e0c7a26":"### Both the test and train Age features contains more the 15% of missing Data so we are fill with the median","db944cc1":"## Load the DataSet","8e85d364":"## Introduction\nIn this kernel I will be creating machine learning model usnig LightAutoML (LAMA) library and will be doing Auotmatic EDA using Pandas Profilling on the vary famous **Titanic Dataset**.\nHope you all enjoy it and please let me know about the mistakes I made in this kernel. I would appreciate your remarks and glad to improve. ; )\n\nIf you like the notebook and find it helpful then **PLEASE UPVOTE**. It motivates me a lot. ","159ef16c":"# Submission","4046daf9":"# Prediction"}}