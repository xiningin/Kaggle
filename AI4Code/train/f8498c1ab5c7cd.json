{"cell_type":{"b16dcee0":"code","618b31ea":"code","11853a9e":"code","ef1a4dcc":"code","bbe1c9ae":"code","e46629e2":"code","884680ec":"code","4aac2082":"code","50d25028":"code","5ff82f33":"code","be0005a6":"code","29663ec7":"code","95bdf206":"code","640e43fe":"code","35b254f5":"code","ad907d18":"code","741d22d7":"code","86cab46b":"code","54004d5d":"code","12bd1dbb":"code","d62b557e":"code","6a5d59ff":"code","1ff72fa4":"code","2c784805":"code","f3267d99":"code","15822e47":"code","dca72edf":"code","036b207e":"code","c33e9e2f":"code","68654e03":"code","58d85bb7":"code","fd4934c9":"code","5a1f7cc0":"code","230875db":"code","1f25b4df":"code","5275f683":"code","f1815bf4":"code","e00583fe":"code","da06870e":"code","ca007cdd":"code","afb8cc96":"code","89a58123":"code","b9c6a7f3":"code","6c2b0913":"code","5408e2e1":"code","c353860f":"code","578deb97":"code","394a5929":"code","67b70789":"code","dbe369a6":"code","e5a441c9":"code","3abd888c":"code","668322d4":"code","5c0776ad":"code","802ee5a6":"code","fa2296a9":"code","2175099e":"code","e848b18b":"code","603e9e3d":"code","7ee0e9c2":"code","10b5bebf":"code","c3dac39b":"code","a3ef627a":"markdown","a352168e":"markdown","9d5d5f51":"markdown","9340af33":"markdown","57b3e5a1":"markdown","0cd41253":"markdown","ed8cc8d6":"markdown","127ae95b":"markdown","28127225":"markdown","7d36a9a0":"markdown","ff83fc84":"markdown","347df769":"markdown","9024ea79":"markdown","e2b70255":"markdown","fbda421d":"markdown","25fdfa22":"markdown","5e925299":"markdown","fe66dce7":"markdown","c4d8adbb":"markdown","b8cc5fcf":"markdown","af02cade":"markdown","6b521709":"markdown","cccb3724":"markdown","eaa5eb96":"markdown","4b21450d":"markdown","6b1f6f65":"markdown","df72772c":"markdown","249f5ef4":"markdown","1115ee92":"markdown","b27e3bd7":"markdown","048b53ae":"markdown","2fd608bf":"markdown","cb8d22ec":"markdown","e6ca6e00":"markdown","9099bfdd":"markdown","d8405cdf":"markdown","6cfd2a90":"markdown","f5318ac9":"markdown","54a7663f":"markdown","2d437249":"markdown","f977a26c":"markdown","8ce1ea50":"markdown","2d833a4a":"markdown","c0a05ceb":"markdown","77970766":"markdown","ae20d06b":"markdown","9cfd4114":"markdown","7591e825":"markdown","ee8bac3b":"markdown","e4a6afa2":"markdown","81e63fda":"markdown","fb60b003":"markdown"},"source":{"b16dcee0":"# Import required packages\nimport pandas as pd\npd.set_option('display.max_columns', 100)\nimport numpy as np\nnp.random.seed(27)\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\ncolours = plt.rcParams['axes.prop_cycle'].by_key()['color']   \n%matplotlib inline\nimport seaborn as sns\nsns.color_palette('muted')\ncurrent_palette = sns.color_palette()\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom scipy.stats import mode, kstest, levene, boxcox, f_oneway\n\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom mlxtend.feature_selection import SequentialFeatureSelector\nfrom mlxtend.regressor import LinearRegression as ml_lr\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\nfrom sklearn.feature_selection import SelectKBest, VarianceThreshold, f_regression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression as sk_lr\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor, DMatrix","618b31ea":"#Import data and take a copy for experimenting during exploration\n\n# Import data when working on local machine\n# test = pd.read_csv('test.csv')\n# train = pd.read_csv('train.csv')\n\n# Import data for Kaggle notebook\ntrain = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\ntest_id = test['Id'] # save id column for indexing final submission\nhouse_test = test.copy()\n# house_test.drop(['Id'],inplace=True,axis=1)\n\ntrain_id = train['Id']\nhouse_train = train.copy()\nhouse_train.drop(['Id'],inplace=True,axis=1)","11853a9e":"# Look at the head of the training data frame to see what an observation looks like in this data set\nhouse_train.head()","ef1a4dcc":"# Inspect the numerical columns of the training and test sets to get a feel for the shape of the data\nprint('Descriptive Statistics for the training data set')\ndisplay(house_train.describe().T)\n\nprint('\\n Descriptive Statistics for the test data set')\ndisplay(house_test.describe().T)","bbe1c9ae":"print('Features containing NaNs in test set = {}'.format(house_test.isna().any().sum()))\nprint('Features containing NaNs in training set = {}'.format(house_train.isna().any().sum()))\n\nna_col_train = house_train.isna().any()\ntrain_na = house_train.loc[:,na_col_train].isna().sum()\n\nna_col_test = house_test.isna().any()\ntest_na = house_test.loc[:,na_col_test].isna().sum()\n\ndisplay(pd.DataFrame([train_na,test_na],index=['train','test']).T)","e46629e2":"# Inspect Mason Veneer Area and Type NaNs\ndisplay((house_train.loc[(house_train['MasVnrArea'].notna()) & (house_train['MasVnrType'].isna())\\\n                         ,['MasVnrArea','MasVnrType']]).isna().any().sum())\ndisplay((house_test.loc[(house_test['MasVnrArea'].notna()) & (house_test['MasVnrType'].isna())\\\n                         ,['MasVnrArea','MasVnrType']]).isna().any().sum())\n\ndisplay(house_test.loc[(house_test['MasVnrArea'].notna()) & (house_test['MasVnrType'].isna())])","884680ec":"# Inspect single observation with NaN for total basement area\ndisplay(house_test.loc[house_test['TotalBsmtSF'].isna()])\n\n#Investigate missing BsmtQual values\ndisplay(house_test[(house_test['BsmtQual'].isna()) & (house_test['TotalBsmtSF']>0)])\ndisplay(house_train.loc[house_train['Neighborhood']=='IDOTRR'].groupby(['MSZoning','BsmtQual'])\\\n.agg({'BsmtQual':'count','MSZoning':'count'}))\n\n#Inspecting training set observation with Bsmt2 area but not finish type\ndisplay(house_train.loc[house_train['BsmtFinType1']=='GLQ']['BsmtFinType2'].value_counts())","4aac2082":"# Inspect missing value from training set Electrical feature\ndisplay(house_train.loc[house_train['Electrical'].isna()])","50d25028":"# Investigate missing values in Garage features\n# Garage Cars and Garage Area both have a single missing value in the test set only\ndisplay(house_test.loc[(house_test['GarageCars'].isna()) | (house_test['GarageArea'].isna())])\n\n# Single test record where year built is 2207\ndisplay(house_test.loc[house_test['GarageYrBlt']==2207])\n\n# Check other missing garage values correspond to not having a garage\ngarage_cols = [col for col in test.columns if 'Garage' in col]\ngarage_cols.remove('GarageArea')\nmissing_train_garage = house_train[garage_cols].loc[(house_train['GarageArea']>0)].isna().sum()\nmissing_test_garage = house_test[garage_cols].loc[(house_test['GarageArea']>0)].isna().sum()\ndisplay('Training',missing_train_garage)\ndisplay('Test',missing_test_garage)","5ff82f33":"ind = house_test[garage_cols].index[(house_test['GarageArea']>0) & (house_test['GarageYrBlt'].isna())]\ndisplay(house_test.iloc[ind])","be0005a6":"# Missing pool values\ndisplay(house_test.loc[(house_test['PoolArea']>0) & (house_test['PoolQC'].isna())])\ndisplay(house_train['PoolQC'].value_counts())","29663ec7":"#Process NaNs in training and test data frames\n\n#Deal with Garage NaNs in test record 1116 of the test set\ngarage_cols = [col for col in test.columns if 'Garage' in col]\ngarage_cols.remove('GarageType')\ngarage_groups = house_train.groupby(['MSZoning','GarageType'])[garage_cols].agg(lambda x: mode(x)[0])\n\ndef fill_row_na(input_df,row,fill_group):\n    '''function to fill in missing values for a particular dataframe row using a groupby object created outside the function'''\n    df = input_df.copy() # take copy of data frame so as not to double modify\n    zone = df.iloc[row,df.columns.get_loc(\"MSZoning\")]\n    gtype = df.iloc[row,df.columns.get_loc('GarageType')]\n    fill_group = fill_group.loc[zone,gtype]\n    for ind, item in fill_group.iteritems():\n        if np.isnan(df.loc[row,ind]):\n            df.loc[row,ind] = item\n    return df\n\nhouse_test = fill_row_na(house_test,1116,garage_groups)\nhouse_test = fill_row_na(house_test,666,garage_groups)\n\n#Correct GarageYrBlt = 2207 in test set\nhouse_test.loc[1132,'GarageYrBlt'] = 2007\n\n#Test set record 660 creates a specific problem as it records a NaN for TotalBsmtSF. Setting this to 0 will allow\n#the na_processing function below to handle the other NaNs\nhouse_test.loc[house_test['TotalBsmtSF'].isna(),'TotalBsmtSF'] = 0\n\n#One test observation has a veneer area but no type, set this to BrkFace as it best fits the other\nhouse_test.loc[(house_test['Neighborhood']=='Mitchel') & (house_test['MasVnrArea']>0),'MasVnrType'] = 'BrkFace'\n\ndef na_processing(input_df,training):\n    '''Function for processing remaining NaNs in training and test data sets. Values are either imputed, or set to 0 or None'''\n    \n    df = input_df.copy() # take copy of dataframe to avoid modifying the original other than with function call\n    \n    #Lot Frontage\n    lot_frontage_fill = training.groupby('Neighborhood').agg({'LotFrontage':'mean'})\n    df = df.set_index('Neighborhood')\n    df['LotFrontage'].fillna(lot_frontage_fill['LotFrontage'],inplace=True)\n    df = df.reset_index()\n    \n    #Alley\n    df['Alley'].fillna('None',inplace=True)\n    \n    #Masonary Veneer Area and Typr\n    df['MasVnrArea'].fillna(0,inplace=True)\n    df['MasVnrType'].fillna('None',inplace=True)\n    \n    #Basement Variables\n    df.loc[(df['TotalBsmtSF']>0) & (df['BsmtQual'].isna()) ,'BsmtQual'] = 'TA'\n    \n    df.loc[(df['TotalBsmtSF']>0) & (df['BsmtCond'].isna()),'BsmtCond'] = 'TA'\n    \n    df.loc[df['TotalBsmtSF']==0,['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2']] = \\\n    df.loc[df['TotalBsmtSF']==0,['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2']].fillna('None')\n    \n    df.loc[(df['TotalBsmtSF']>0) & (df['BsmtExposure'].isna()),'BsmtExposure'] = 'No'\n    \n    df.loc[(df['BsmtFinType1'].notna()) & (df['BsmtFinType2'].isna()),'BsmtFinType2'] = 'Unf'\n    \n    df.loc[(df['TotalBsmtSF']==0) & (df['BsmtFinSF1'].isna()),['BsmtFinSF1','BsmtUnfSF']] = 0\n    \n    df['BsmtFinSF2'].fillna(0,inplace=True)\n    \n    df.loc[:,['BsmtFullBath','BsmtHalfBath']] = df.loc[:,['BsmtFullBath','BsmtHalfBath']].fillna(0)\n\n    #Electrical\n    df['Electrical'].fillna(training['Electrical'].mode()[0],inplace=True)\n    \n    #Fireplace Quality\n    df['FireplaceQu'].fillna('None',inplace=True)\n    \n    #Garage Variables\n    garage_cols = [col for col in df.columns if 'Garage' in col]\n    garage_cols.remove('GarageCars')\n    garage_cols.remove('GarageArea')\n    df.loc[:,garage_cols] = df.loc[:,garage_cols].fillna('None')\n    \n    #Pool Quality\n    df.loc[(df['PoolArea']>0) & (df['PoolQC'].isna()),'PoolQC'] = 'Gd'\n    df['PoolQC'].fillna('None',inplace=True)\n    \n    #Fence\n    df['Fence'].fillna('None',inplace=True)\n    \n    #Misc Feature\n    df['MiscFeature'].fillna('None',inplace=True)\n    \n    #MS Zoning\n    df['MSZoning'].fillna(training['MSZoning'].mode()[0],inplace=True)\n\n    #Utilities\n    df['Utilities'].fillna(training['Utilities'].mode()[0],inplace=True)\n    \n    #Exterior Covering\n    exterior_fill = training.groupby('Neighborhood').agg({'Exterior1st': lambda x: mode(x)[0],\\\n                                                        'Exterior2nd': lambda x: mode(x)[0]})\n    \n    df = df.set_index('Neighborhood')\n    df['Exterior1st'].fillna(exterior_fill['Exterior1st'],inplace=True)\n    df['Exterior2nd'].fillna(exterior_fill['Exterior2nd'],inplace=True)\n    df = df.reset_index()\n    \n    #Kitchen Quality\n    df.loc[(df['KitchenAbvGr']>0) & (df['KitchenQual'].isna()),'KitchenQual'] = 'TA'\n    \n    #Functionality\n    df['Functional'].fillna('Typ',inplace=True)\n    \n    #Sale Type\n    df['SaleType'].fillna('WD',inplace=True)\n       \n    return df\n\ntest_processed = na_processing(house_test,house_train)\ntrain_processed = na_processing(house_train,house_train)\n\n\nprint('Features containing NaNs in test set = {}'.format(test_processed.isna().any().sum()))\nprint('Features containing NaNs in training set = {}'.format(train_processed.isna().any().sum()))","95bdf206":"# Save data frames with NaNs removed\ntest_clean = pd.concat([test_id,test_processed],axis=1)\ntrain_clean = pd.concat([train_id,train_processed],axis=1)\n\ntest_clean.to_csv('test_clean.csv')\ntrain_clean.to_csv('train_clean.csv')","640e43fe":"# Clean up data artefacts created during NaN cleaning\ndel [na_col_train, na_col_test, train_na, test_na, missing_train_garage, missing_test_garage, garage_groups\\\n     , house_train, house_test]","35b254f5":"# Create training and validation sets.\nhouse_train = train_clean\nhouse_test = test_clean\n\nX_train, X_valid, y_train, y_valid = train_test_split(house_train.drop('SalePrice',axis=1),\\\n                                            house_train['SalePrice'], test_size=0.2, random_state=27)\n\nX_train.drop('Id',inplace=True,axis=1) #drop Id columns as they are not needed\nX_valid.drop('Id',inplace=True,axis=1)\n\ndisplay(X_train.shape,X_valid.shape) # Check shape of training and validation data sets\n\n# Use rough eyeball test to check that the validation target set is representative of the training set  \nplt.hist(y_train, bins=50, alpha=0.5, label='training')\nplt.hist(y_valid, bins=50, alpha=0.5, label='validation')\nplt.legend(loc='upper right')\nplt.xlabel('Sale Price')\nplt.title('Sale Price Distribution in Training and Validation Sets')\nplt.show()\n\n# Recombine features and target for EDA and model tuning purposes\ndf_train = pd.concat([X_train,y_train],axis=1)\ndf_valid = pd.concat([X_valid,y_valid],axis=1)","ad907d18":"# Look at whether the Sale Price should be log transformed\nfig, ax = plt.subplots(1,3,figsize=(20,8))\ndf_train.hist(column='SalePrice',bins=20,ax=ax[0],color=colours[0])\n\nprint('Sale Price Skew = {:.2f}'.format(df_train['SalePrice'].skew()))\nprint('Sale Price Kurtosis = {:.2f}'.format(df_train['SalePrice'].kurtosis()))\n\nsale_price_log = np.log(df_train['SalePrice'])\nax[1].hist(sale_price_log,bins=20,color=colours[1])\nax[1].set_title('Log Sale Price')\nprint('Log Skew = {:.2f}'.format(sale_price_log.skew()))\nprint('Log Kurtosis = {:.2f}'.format(sale_price_log.kurtosis()))\n\n_ = sns.boxplot(y='SalePrice',data=df_train,ax=ax[2],color=colours[2]).set_title('Sale Price')\n","741d22d7":"# Look at how taking the log of the sale price affects feature correlation. Also examine skewness and kurtosis of the features.\ndf_train['Log_SalePrice'] = np.log(df_train['SalePrice'])\nlog_correlation = df_train.corr()['Log_SalePrice']\ncorrelation = df_train.corr()['SalePrice'].sort_values(ascending=False)\nkurt = df_train.kurtosis()\nskew = df_train.skew()\n\nlog_train_correlation = pd.concat([correlation,log_correlation,kurt,skew],axis=1)\nlog_train_correlation.rename(columns={'Log_SalePrice':'Log_Price_Correlation','SalePrice':'Price_Correlation',\\\n                                     0:'Kurtosis',1:'Skewness'},inplace=True)\nlog_train_correlation.drop('Log_SalePrice',inplace=True)\n\ndisplay(log_train_correlation.head(10))","86cab46b":"# Create dictionary of data types by column in training set\ntype_dict = {str(k) : list(v) for k,v in X_train.groupby(df_train.dtypes,axis=1)}\nfor k,v, in type_dict.items():\n    print(k, len(v))\ndisplay(df_train.loc[:,type_dict['float64']].head(10))\n\n# The features classed as float will not lose an important level of detail by being recast as int64\ndf_train = df_train.astype({\"LotFrontage\": np.int64, \"MasVnrArea\": np.int64})\ndf_valid = df_valid.astype({\"LotFrontage\": np.int64, \"MasVnrArea\": np.int64})\nhouse_test = house_test.astype({\"LotFrontage\": np.int64, \"MasVnrArea\": np.int64})","54004d5d":"# Examine distribution of all categorical features\ncat_features = pd.melt(df_train.loc[:,type_dict['object']], value_vars=type_dict['object'])\nfig = sns.FacetGrid(cat_features, col='variable', col_wrap=4, sharex=False, sharey=False)\nplt.xticks(rotation='vertical')\nfig = fig.map(sns.countplot, 'value')\n[plt.setp(ax.get_xticklabels(), rotation=60) for ax in fig.axes.flat]\nfig.fig.tight_layout()\nplt.show()","12bd1dbb":"# Look at feature category correlations with Log_SalePrice\ndf_features = df_train.loc[:,type_dict['object']]\nmax_correlation = []\n\nfor col in df_features.columns:\n    # set up dummy variables for each categorical feature column in sequence\n    dummies = pd.get_dummies(df_train[col])\n    dummies = pd.concat([df_train['Log_SalePrice'],dummies],axis=1)\n    # find the maximum correlation of any categoriy in the feature\n    max_corr = dummies.corr()['Log_SalePrice'][1:].abs().max()\n    max_correlation.append(max_corr)\n    \n\nfeature_correlations = pd.DataFrame({'Features':df_features.columns,'Correlation':max_correlation})\nfeature_correlations = feature_correlations.loc[feature_correlations['Correlation']>0.3]\\\n        .sort_values(['Correlation'],ascending=False)\n\nfeature_correlations","d62b557e":"anova = {'feature':[], 'f':[], 'p':[]}\nfor feature in df_train.select_dtypes(include=['object']).columns: #better way of selecting features by dtype\n    category_prices = []\n    for category in df_train[feature].unique():\n        category_prices.append(df_train[df_train[feature] == category]['SalePrice'].values)\n    f, p = f_oneway(*category_prices)\n    anova['feature'].append(feature)\n    anova['f'].append(f)\n    anova['p'].append(p)\nanova = pd.DataFrame(anova)\nanova.sort_values('p', inplace=True)\n\n# Plot\nplt.figure(figsize=(14,6))\nsns.barplot(anova['feature'], np.log(1\/anova['p']))\nplt.xticks(rotation=90)\nplt.show()\n\n_ = sns.barplot(feature_correlations['Correlation'],feature_correlations['Features'], orient='h')","6a5d59ff":"# Examine correlations of each observation type by dummy enociding\next_dummies = pd.get_dummies(df_train['ExterQual'])\next_dummies = pd.concat([df_train['Log_SalePrice'],ext_dummies],axis=1)\next_corr = ext_dummies.corr()['Log_SalePrice'][1:]\n\ndisplay(ext_corr, df_train['ExterQual'].value_counts())","1ff72fa4":"# Group ExterQuals into only two categories and recheck correlation.\ndf_train['ExterQualRed'] = df_train['ExterQual'].replace({'Ex':1,'Gd':1,'Fa':0,'TA':0})\n\next_qual = df_train[['Log_SalePrice','ExterQualRed']]\next_corr = ext_qual.corr()['Log_SalePrice'][1:]\n\ndisplay(ext_corr)\n","2c784805":"# Repeat analysis above for KitchenQual\next_dummies = pd.get_dummies(df_train['KitchenQual'])\next_dummies = pd.concat([df_train['Log_SalePrice'],ext_dummies],axis=1)\next_corr = ext_dummies.corr()['Log_SalePrice'][1:]\n\ndisplay(ext_corr, df_train['KitchenQual'].value_counts())\n\n# Group ExterQuals into only two categories and recheck correlation.\ndf_train['KitchenQualRed'] = df_train['KitchenQual'].replace({'Ex':1,'Gd':1,'Fa':0,'TA':0})\n\next_qual = df_train[['Log_SalePrice','ExterQualRed','KitchenQualRed']]\next_corr = ext_qual.corr()\n\ndisplay(ext_corr)","f3267d99":"# Examine correlations of each observation type by dummy enociding\next_dummies = pd.get_dummies(df_train['Foundation'])\next_dummies = pd.concat([df_train['Log_SalePrice'],ext_dummies],axis=1)\next_corr = ext_dummies.corr()['Log_SalePrice'][1:]\n\ndisplay(ext_corr, df_train['Foundation'].value_counts())","15822e47":"plt.figure(figsize=(20,12))\n\nax1 = plt.subplot2grid((2, 2), (0, 0), colspan=1)\nax2 = plt.subplot2grid((2, 2), (0, 1), colspan=1)\nax3 = plt.subplot2grid((2, 2), (1, 0), colspan=2)\n\n_ = sns.swarmplot(x='FireplaceQu',y='SalePrice',data=df_train,ax=ax1)\n_ = sns.swarmplot(x='MasVnrType',y='SalePrice',data=df_train,ax=ax2)\n_ = sns.swarmplot(x='Neighborhood',y='SalePrice',data=df_train,ax=ax3)","dca72edf":"# Use dummy encoding to look at the correlation of each category in garage type with the log of sale price\ngartype_dummies = pd.get_dummies(df_train['GarageType'])\ngartype_dummies = pd.concat([df_train['Log_SalePrice'],gartype_dummies],axis=1)\ngartype_corr = gartype_dummies.corr()['Log_SalePrice'][1:]\n\n# Display correlations sorted from most negative to most positive\ndisplay(gartype_corr.sort_values(),gartype_corr.sort_values().index)\n\n# Create an ordinal encoding of GarageType to see how well the correlates with sale price\nGarTypes = gartype_corr.sort_values().index\nGarTypeOrd = [num for num in range(len(GarTypes))]\nGarType_dict = dict(zip(GarTypes,GarTypeOrd))\ndf_train['GarageType_Ordinal'] = df_train['GarageType'].map(GarType_dict)","036b207e":"garfin_dummies = pd.get_dummies(df_train['GarageFinish'])\ngarfin_dummies = pd.concat([df_train['Log_SalePrice'],garfin_dummies],axis=1)\ngarfin_corr = garfin_dummies.corr()['Log_SalePrice'][1:]\n\n# Display correlations sorted from most negative to most positive\ndisplay(garfin_corr.sort_values(),garfin_corr.sort_values().index)\n\n# Create an ordinal encoding of GarageType to see how well the correlates with sale price\nGarFins = garfin_corr.sort_values().index\nGarFinOrd = [num for num in range(len(GarFins))]\nGarFin_dict = dict(zip(GarFins,GarFinOrd))\ndf_train['GarageFinish_Ordinal'] = df_train['GarageFinish'].map(GarFin_dict)\n\nGarageOrdinal_corr = df_train[['Log_SalePrice','GarageFinish_Ordinal','GarageType_Ordinal']].corr()\n\ndisplay(GarageOrdinal_corr)","c33e9e2f":"#Check new binary features for multicolinearity\ndf_train['PConc'] = np.where(df_train['Foundation']=='PConc',1,0)\ndf_train['HasFireplace'] = np.where(df_train['FireplaceQu']=='None',0,1)\ndf_train['MasVnrRed'] = np.where(df_train['MasVnrType']=='None',0,1)\n\next_qual = df_train[['Log_SalePrice','ExterQualRed','KitchenQualRed','PConc','HasFireplace','MasVnrRed']]\next_corr = ext_qual.corr()\ndisplay(ext_corr[1:])","68654e03":"#Clean up added feature. Final processing will be done on main feature column feature\ndf_train.drop(['ExterQualRed','KitchenQualRed','PConc','HasFireplace','MasVnrRed'],axis=1,inplace=True)","58d85bb7":"def create_cat_features(df):\n    df['ExterQual'] = df['ExterQual'].replace({'Ex':1,'Gd':1,'Fa':0,'TA':0}).astype(np.int64)\n    df['HasFireplace'] = np.where(df['FireplaceQu']=='None',0,1).astype(np.int64)\n    df['MasVnr'] = np.where(df['MasVnrType']=='None',0,1).astype(np.int64)\n    \n    df_dummies = pd.get_dummies(df['Neighborhood'],prefix='Neighbourhood')\n    df = pd.concat([df,df_dummies],axis=1)\n    \n    garfin_dummies = pd.get_dummies(df_train['GarageFinish'])\n    garfin_dummies = pd.concat([df_train['Log_SalePrice'],garfin_dummies],axis=1)\n    garfin_corr = garfin_dummies.corr()['Log_SalePrice'][1:]\n\n    # Create an ordinal encoding of GarageType to see how well the correlates with sale price\n    GarFins = garfin_corr.sort_values().index\n    GarFinOrd = [num for num in range(len(GarFins))]\n    GarFin_dict = dict(zip(GarFins,GarFinOrd))\n    df['GarageFinish_Ordinal'] = df['GarageFinish'].map(GarFin_dict)\n    \n    return df\n\ndf_train = create_cat_features(df_train)\ndf_valid = create_cat_features(df_valid)\nhouse_test = create_cat_features(house_test)","fd4934c9":"# Examine distribution of all numerical features\nnum_features = pd.melt(df_train.select_dtypes(include='int64'), value_vars=type_dict['int64'])\nfig = sns.FacetGrid(num_features, col='variable', col_wrap=4, sharex=False, sharey=False)\nfig = fig.map(plt.hist,'value',bins=30)","5a1f7cc0":"# Inspect numerical columns for correlation to sale price\n# Remove neighborhood columns as they will make the plot harder to parse\ncorr_cols = [col for col in df_train.columns if 'Neighbourhood' not in col]\n\ncorrelation = df_train[corr_cols].corr()['SalePrice'].sort_values(ascending=False)\n\nplt.figure(figsize=(8,10))\nsns.barplot(correlation[2:], correlation.index[2:], orient='h')\nplt.show()","230875db":"# Examine Overall Quality and Condition\nfig, ax = plt.subplots(1,3,figsize=(20,8))\nsns.boxplot(x='OverallQual',y='SalePrice',data=df_train,ax=ax[0])\nsns.boxplot(x='OverallCond',y='SalePrice',data=df_train,ax=ax[1])\nsns.scatterplot(x='OverallQual',y='OverallCond',data=df_train,ax=ax[2])\n\nQualCon = log_train_correlation.loc[['OverallQual','OverallCond']]\ndisplay(QualCon)\n\nQual_var = df_train.groupby('OverallQual').agg({'SalePrice':'var'}).rename(columns={'SalePrice':'Sale Price Variance'})\ndisplay(Qual_var.T)\n\nprint('Levene test of OverallQual and Sale Price = {}'.format(levene(df_train['OverallQual'],df_train['SalePrice'])))\nprint('Levene test of OverallQual and Log Sale Price = {}'.format(levene(df_train['OverallQual'],df_train['Log_SalePrice'])))","1f25b4df":"# Remove outlier sale prices\ndf_train.drop(df_train.loc[df_train['SalePrice']>700000].index,inplace=True)\n_ = df_train.hist(column='SalePrice',bins=20,color=colours[2],alpha=0.7)","5275f683":"# Inspect Living Area related features\ndisplay(log_train_correlation.loc[['GrLivArea','TotalBsmtSF','LotArea','TotRmsAbvGrd','FullBath','HalfBath']].\\\n       sort_values('Log_Price_Correlation',ascending=False))\n\n# Box Plots of continuous variables\nfig, ax = plt.subplots(3,1,figsize=(10,5))\nsns.boxplot(y='GrLivArea',data=df_train,ax=ax[0],orient='h',color=current_palette[0]).set_ylabel('GrLivArea')\nsns.boxplot(y='TotalBsmtSF',data=df_train,ax=ax[1],orient='h',color=current_palette[1]).set_ylabel('TotalBsmtSF')\nsns.boxplot(y='LotArea',data=df_train,ax=ax[2],orient='h',color=current_palette[2]).set_ylabel('LotArea')","f1815bf4":"# Drop outliers from GrLivArea - this also accounts for the single large basement\ndf_train.drop(df_train.loc[df_train['GrLivArea']>4000].index,inplace=True)\n# Drop large lot area outlier\ndf_train.drop(df_train.loc[df_train['LotArea']>100000].index,inplace=True)\n\n# Box Plots of continuous variables following outlier removal\nfig, ax = plt.subplots(3,1,figsize=(10,5))\n_ = sns.boxplot(y='GrLivArea',data=df_train,ax=ax[0],orient='h',color=current_palette[0]).set_ylabel('GrLivArea')\n_ = sns.boxplot(y='TotalBsmtSF',data=df_train,ax=ax[1],orient='h',color=current_palette[1]).set_ylabel('TotalBsmtSF')\n_ = sns.boxplot(y='LotArea',data=df_train,ax=ax[2],orient='h',color=current_palette[2]).set_ylabel('LotArea')","e00583fe":"# See how correlation with Sale Price has changed by removing outliers\nold_correlation = log_train_correlation.loc[['GrLivArea','TotalBsmtSF','LotArea','TotRmsAbvGrd','FullBath','HalfBath']\\\n                                            ,'Log_Price_Correlation']\nnew_correlation = df_train[['GrLivArea','TotalBsmtSF','LotArea','TotRmsAbvGrd','FullBath','HalfBath','Log_SalePrice']].\\\n                                        corr()['Log_SalePrice']\nnew_correlation.drop('Log_SalePrice',inplace=True)\n\nliving_area_correlation = pd.concat([new_correlation,old_correlation],axis=1)\nliving_area_correlation.columns = ['Outliers Removed','Outliers Present']\ndisplay(living_area_correlation)","da06870e":"# Engineer new features from area and room count features\nArea_df = df_train[['Log_SalePrice','GrLivArea','TotalBsmtSF','TotRmsAbvGrd','FullBath','HalfBath']]\n\nArea_df['TotalArea'] = Area_df['GrLivArea'] + Area_df['TotalBsmtSF']\nArea_df['TotalRooms'] = Area_df['TotRmsAbvGrd'] + Area_df['FullBath'] + Area_df['HalfBath']\nArea_df['AvgRmSize'] = Area_df['GrLivArea']\/Area_df['TotRmsAbvGrd']\nArea_df[['Log_SalePrice','GrLivArea','TotalBsmtSF','TotalArea','TotalRooms','AvgRmSize']].corr().iloc[1:]\n","ca007cdd":"garage_df = df_train[['Log_SalePrice','GarageCars','GarageArea']]\ngarage_df['AreaPerCar'] = garage_df['GarageArea']\/garage_df['GarageCars']\ngarage_corr = garage_df.corr()\ngarage_corr.iloc[1:]","afb8cc96":"work_df = df_train[['Log_SalePrice','YearBuilt','YearRemodAdd']]\nwork_df['LastMod'] = work_df[['YearBuilt','YearRemodAdd']].max(axis=1)\nwork_df.corr().iloc[1:]","89a58123":"lot_df = df_train[['Log_SalePrice','LotArea']]\nlot_df = pd.concat([lot_df,Area_df['TotalArea']],axis=1)\nlot_df['Lot_Proportion'] = lot_df['LotArea']\/lot_df['TotalArea']\nlot_df.corr().iloc[1:]","b9c6a7f3":"plt.figure(figsize = (8,6))\nax1 = sns.boxplot(x = 'KitchenAbvGr', y = 'SalePrice', data = df_train)\ndisplay(df_train['KitchenAbvGr'].value_counts())\n\ntwo_kit = df_train.loc[df_train['KitchenAbvGr']==2,['LotArea','YearBuilt','TotRmsAbvGrd','GarageCars']]\none_kit = df_train.loc[df_train['KitchenAbvGr']==1,['LotArea','YearBuilt','TotRmsAbvGrd','GarageCars']]\ndisplay(one_kit.describe(),two_kit.describe())","6c2b0913":"porch_cols = [col for col in df_train.columns if 'Porch' in col]\nporch_df = df_train.loc[:,porch_cols]\nporch_df = pd.concat([df_train['Log_SalePrice'],porch_df],axis=1)\n\n# Create feature for whether or not a house has a porch\nporch_df['HasPorch'] = np.where(df_train[porch_cols].sum(axis=1)>0,1,0)\n\n# Create feature which is an ordinal encoding of porch type based on univariate correlation\ndef porch_func(row):\n    if row['EnclosedPorch']>0:\n        return 0\n    elif row['3SsnPorch']>0:\n        return 1\n    elif row['ScreenPorch']>0:\n        return 2\n    else:\n        return 3\n\nporch_df['PorchQual'] = porch_df.apply(porch_func,axis=1)\n\nporch_corr = porch_df.corr()\nporch_corr.iloc[1:]","5408e2e1":"def create_num_features(df):\n    df['TotalArea'] = df['GrLivArea'] + df['TotalBsmtSF']\n    df['TwoKitchens'] = np.where(df['KitchenAbvGr']==2,0,1).astype(np.int64)\n    porch_cols = [col for col in df.columns if 'Porch' in col]\n    df['HasPorch'] = np.where(df[porch_cols].sum(axis=1)>0,1,0)\n    \n    return df\n\ndf_train = create_num_features(df_train)\ndf_train['Log_SalePrice'] = np.log(df_train['SalePrice'])\n\ndf_valid = create_num_features(df_valid)\ndf_valid['Log_SalePrice'] = np.log(df_valid['SalePrice'])\n\nhouse_test = create_num_features(house_test)","c353860f":"# Look at heatmap or the correlation between non-neighbourhood columns\n\ncorr_cols = [col for col in df_train.columns if 'Neighbourhood' not in col]\ncorr = df_train[corr_cols].corr()\ncorr = corr.reindex(index =(['SalePrice','Log_SalePrice'] + list([col for col in corr.columns if 'SalePrice' not in col]))\\\n                ,columns=(['SalePrice','Log_SalePrice'] + list([col for col in corr.columns if 'SalePrice' not in col])))\n\nfig = plt.figure(figsize=(16,15))\nax = fig.add_subplot(111)\ncmap = sns.diverging_palette(220, 20, as_cmap=True)\nsns.heatmap(corr, \n           xticklabels=corr.columns.values,\n           yticklabels=corr.index.values,\n           cmap = cmap)\nax.xaxis.tick_top()\nplt.setp(ax.get_xticklabels(), rotation=90)\nplt.show()","578deb97":"predictors = ['ExterQual','HasFireplace','MasVnr','OverallQual','TotalArea','GarageCars','YearBuilt','TwoKitchens'\\\n                ,'HasPorch','GarageFinish_Ordinal'] + [col for col in df_train.columns if 'Neighbourhood' in col]\n\ntarget_col = ['Log_SalePrice']","394a5929":"df_train[['ExterQual','HasFireplace','MasVnr','OverallQual','TotalArea','GarageCars','YearBuilt','TwoKitchens'\\\n                ,'HasPorch','GarageFinish_Ordinal']].describe()","67b70789":"scaler = StandardScaler()\nscaler.fit(df_train[['TotalArea','YearBuilt']])\n\ndef scale_data(df):\n    df['OverallQual'] = df['OverallQual']\/10\n    df['GarageCars'] = df['GarageCars']\/4\n    df['GarageFinish_Ordinal'] = df['GarageFinish_Ordinal']\/3\n    df[['TotalArea','YearBuilt']] = scaler.transform(df[['TotalArea','YearBuilt']])\n    \n    return df\n    ","dbe369a6":"df_train = scale_data(df_train)\ndf_valid = scale_data(df_valid)\nhouse_test = scale_data(house_test)","e5a441c9":"df_train[['ExterQual','HasFireplace','MasVnr','OverallQual','TotalArea','GarageCars','YearBuilt','TwoKitchens'\\\n                ,'HasPorch','GarageFinish_Ordinal']].describe()","3abd888c":"# SelectKBest\nselector = SelectKBest(f_regression, k=10)\nselector.fit(df_train[predictors], df_train[target_col])\n\ndf_new = selector.transform(df_train[predictors])\nprint(df_new.shape)\n\n# Look at selected columns\ndf_train[predictors].columns[selector.get_support(indices=True)].tolist()","668322d4":"models = [('sk_LinearRegression',sk_lr()),('Ridge',Ridge()),('Lasso',Lasso()),\\\n          ('DecisionTree',DecisionTreeRegressor()),('RandomForest',RandomForestRegressor())\\\n          ,('GradientBoost',GradientBoostingRegressor())]\n\nmodel_comp = pd.DataFrame(columns = ['Model','CV_mean','CV_std'])\n\nfor ind, (name, model) in enumerate(models):\n    cv_results = cross_val_score(model, df_train[predictors],df_train[target_col],cv=5,scoring='neg_root_mean_squared_error',\\\n                                verbose = 1)\n    model_comp.loc[ind,'Model'] = name\n    model_comp.loc[ind,'CV_mean'] = cv_results.mean()\n    model_comp.loc[ind,'CV_std'] = cv_results.std()\n\nmodel_comp.sort_values(['CV_mean'],ascending=False)","5c0776ad":"# # Ridge Regression\n# ridge = Ridge()\n# params = {'alpha':[0.05,0.1,0.2,0.4,0.7,1]}\n# gs_ridge = GridSearchCV(ridge,param_grid=params,cv=5,scoring='neg_mean_squared_error',verbose=1)\n# gs_ridge.fit(df_valid[predictors],df_valid[target_col])\n# print(gs_ridge.score(df_valid[predictors],df_valid[target_col]))\n# print(gs_ridge.best_params_)","802ee5a6":"# # GradientBoosting\n# gbr = GradientBoostingRegressor()\n# params = {'learning_rate':[0.01,0.05,0.1,0.2],'n_estimators':[50,75,100,125,150],'subsample':[0.5,0.8,1],\\\n#          'max_depth':[1,3,5]}\n# gs_gbr = GridSearchCV(gbr,param_grid = params,cv=5,scoring='neg_mean_squared_error',verbose=1)\n# gs_gbr.fit(df_valid[predictors],df_valid[target_col])\n# print(gs_gbr.score(df_valid[predictors],df_valid[target_col]))\n# print(gs_gbr.best_params_)","fa2296a9":"# Join training and validation data sets together for maximum training data\ntraining = pd.concat([df_train[predictors],df_valid[predictors]])\ntarget = pd.concat([df_train[target_col],df_valid[target_col]])","2175099e":"# # Ridge Regression\n# tuned_ridge = Ridge(**gs_ridge.best_params_)\n# tuned_ridge.fit(training, target)\n# ridge_output = tuned_ridge.predict(house_test[predictors])\n\n# # Convert back to real $\n# dollar_ridge_output = np.exp(ridge_output)\n\n# # Flatten array to become df\n# dollar_ridge_output = np.ndarray.flatten(dollar_ridge_output)\n\n# ridge_submission = pd.DataFrame({'Id':test_id,'SalePrice':dollar_ridge_output})","e848b18b":"# # GradientBoost Regression\n# tuned_gbr = GradientBoostingRegressor(**gs_gbr.best_params_)\n# tuned_gbr.fit(training, target)\n# gbr_output = tuned_gbr.predict(house_test[predictors])\n\n# # Convert back to real $\n# dollar_gbr_output = np.exp(gbr_output)\n\n# # Flatten array to become df\n# dollar_gbr_output = np.ndarray.flatten(dollar_gbr_output)\n\n# gbr_submission = pd.DataFrame({'Id':test_id,'SalePrice':dollar_gbr_output})","603e9e3d":"# # Produce submission files\n# ridge_submission.to_csv('ridge_submission.csv',index=False)\n# gbr_submission.to_csv('gbr_submission.csv',index=False)","7ee0e9c2":"# XGBoost\nxgbr = XGBRegressor()\nxgbr_param_grid = {'objective':['reg:squarederror'],\n                   'learning_rate': [0.1],\n                    'n_estimators': [200],\n                    'subsample': [ 0.7, 0.75],\n                   'max_depth': [3,4],\n                  'gamma': [0, 0.01],\n                  'lambda':[0.15, 0.2, 0.25]}\n# xgbr_param_grid = {'objective':['reg:squarederror'],\n#                     'learning_rate': [0.1,0.5],\n#                     'n_estimators': [100],}\n\ntuned_xgbr = GridSearchCV(estimator = xgbr, param_grid = xgbr_param_grid, cv=3, verbose=1, scoring='neg_mean_squared_error')\ntuned_xgbr.fit(training,target)\nprint('Best Score = {:.3f}'.format(tuned_xgbr.best_score_))\nprint('Best Parameters: ',tuned_xgbr.best_params_)","10b5bebf":"xgbr_final = XGBRegressor(**tuned_xgbr.best_params_)\nxgbr_final.fit(training,target)\nxgbr_output = xgbr_final.predict(house_test[predictors])\n\n# Convert back to real $\ndollar_xgbr_output = np.exp(xgbr_output)\n\n# Flatten array to become df\ndollar_xgbr_output = np.ndarray.flatten(dollar_xgbr_output)\n\nxgbr_submission = pd.DataFrame({'Id':test_id,'SalePrice':dollar_xgbr_output})","c3dac39b":"xgbr_submission.to_csv('xgbr_submission.csv',index=False)","a3ef627a":"The simplification to whether or not a house has a porch has a better correlation than any individual porch feature or an ordinal feature.","a352168e":"#### Look At Best Features\nI have already selected a subset of features based on my EDA but I will now use more analytical methods to determine the most useful features in case I later want to restrict the model further.","9d5d5f51":"##### Pool Quality\nThere are some Pools which have an area but no quality value. These have been set to Gd as this is the most common quality in the training set (albeit with a small sample size). All other NaNs are 'None'.\n\n##### Fence\nNaNs are assumed as not having a fence so set to 'None'.\n\n##### Misc Feature\nNaNs set to 'None'.\n\n##### MSZoning\nMSZoning - all hosues must be zoned, impute as modal zone type.\n\n##### Utilities\nAll houses should have some utility access so this has been imputed as the modal value for the feature.\n\n##### Exterior Covering\nI have assumed that houses in the same neighbourhood have similar styles and that all houses have some exterior covering. NaNs have been imputed as the modal type for their neighbourhood.\n\n##### Kitchen Quality\nWhere a kitchen is present but has a NaN for quality these has been set to typical 'TA'.\n\n##### Functionality\nNaNs for this feature have been set to 'Typ' as this fits with the most common entry and likelihood based on feature description.\n\n##### Sale Type\nNaNs set to 'WD' as the most likely value.","9340af33":"##### Electrical\nThe house missing this info (training id 1379) has air con which implies it must have electrical. I will impute this as the modal electrical type.\n\n##### FireplaceQu\nAll NaNs for this feature correspond to the 'Fireplace' feature being 0 so are set to 'None'.","57b3e5a1":"Now I will identify which features in the training and test sets have NaNs present. The documentation provided describing what each feature in the data set means will be useful in understanding which columns might be expected to contain NaNs.\n* [Dataset documentation](https:\/\/ww2.amstat.org\/publications\/jse\/v19n3\/decock\/DataDocumentation.txt)","0cd41253":"As expected only one of these features should be included in a regression model so I will select Garage Cars.","ed8cc8d6":"The year built looks to have the most impact so this will be preferred for modelling.","127ae95b":"### Categorical Feature Processing\nHaving identified the most promising categorical features to use as predictors these now need to be processed into a useable ML format in the trainig, test and validation dataframes.","28127225":"#### Porch\nThere are a number of porch related columns with a degree of positive correlation. It may be possible to engineer a single feature to replace multiple.","7d36a9a0":"Taking the log of the Sale Price improves the correlation factor of most nuerical variables, including nine of the top 10, without changing their order. This implies that using the log of the Sale Price may improve model accuracy, particularly in simpler models. I will continue to do base EDA using the Sale Price as this is the real-world value but may use its log in model buidling.","ff83fc84":"#### ANOVA\nAs well as how well correlated an individual categorical feature is we can also test to see how much of the variance in Sale Price each categorical feature explains. Combining these two insights will point to which features are most likely to be useful and how to process them.","347df769":"### EDA\nHaving cleaned the data of NaNs I will now dig deeper to understand which features are likley to be of use in machine learning models (feature selection) and whether there are combinations of, or derivations from, the features which will yield more predictive information, or more efficient predictive information, on the target feature (feature engineering).\n\nFrom now on I will observe the discipline of only looking at the training set. I will further split the cleaned training data into a true training set and a validation set to be used for hyperparamter turning.","9024ea79":"#### Standardise Features\nThe selected features have differing scales and given that I will be trialing linear regression algorithms I will need to transform the features with larger scales.\n\nFirst inspect the data again to identify the features which need scaling.","e2b70255":"##### Garage Variables\nTest observation 1116 appears to have a garage but is missing almost all information about it other than it is detached. I have dealt with this as a special case rather than in the main function. To impute the missing values I have grouped houses by their zone and garage type and worked out the modal value for other garage features and input these into test observation 1116.\n\nTest record 666 is responsible for the other anomalous data where there is a non-zero garage area but other garage features record NaNs. I will use the function described above to process record 666 also.\n\nTest observation 1132 has a 'GarageYrBlt' value of 2207. Looking at this record it seems that this should have been 2007 so this has been explicitely corrected.\n\nOther qualitative garage feature NaNs correspond to houses without garages so have been set to 'None'","fbda421d":"#### Exterior Quality and Kitchen Quality\nThe Exterior Quality feature has four possible categories and the highest single correlated observation type of any categorical feature.","25fdfa22":"The Sale Price target is normally distributed though in its base form is right-tail skewed. Taking the log of Sale Price corrects this so it may help the model to predict log Sale Price and then take the exponential to create the final predictions.\n\nThere are two clear outliers which should probably be removed from the training set before modelling.","5e925299":"All three new features have a useful correlation with the sale price but they are also highly correlated with each other. I will therefore only keep the TotalArea feature as this has the highest correlation factor.","fe66dce7":"#### Garage Cars & Garage Area\nThese two features seem likely to be highly correlated but it may be possible to combine them in such a way as to yield a more useful single feature.","c4d8adbb":"The ordinal encodings of both the garage finish and type have useful predictive power but are also correlated. Therefore I will keep the garage finish feature as it has a slightly higher correlation with the log of the sale price.","b8cc5fcf":"## Kaggle House Price Prediction Competition\n\nIn this notebook we tackle the Ames, Iowa housing data set. We first clean the data to remove any NaNs that are present before moving on to exploratory data analysis. The final section selects and tunes machine learning models to be deployed on the test set.\n\nI have uploaded a stripped down version of this notebook cotaning only the NaN removal component for others to use if they wish to save themselves time cleaning the data.\n* **[Ames, Iowa NaN removal](https:\/\/www.kaggle.com\/ricksoc\/nan-processing-for-ames-iowa-dataset)**","af02cade":"The categories in ExterQual are not evenly soaced in terms or correlation so shouldn't just be interval encoded. TA and Gd dominate and interestingly Ex has a lower positive correlation than Gd. I will try the approach of grouping the positively and negatively correlation types together.","6b521709":"#### Lot Frontage\nThe Lot Frontage will be a function of the overall lot area but there might be some value attached to the layout of the house with respect to what percentage the Frontage makes up.","cccb3724":"### Categorical Feature\n\nWith two distinct groups of features I will first look at the object\/ categorical set. While some models, e.g. decision trees, would be able to use these fields in their basic form in order to open up a wider set of models these features will need some processing. Dummy encoding is the most likely option, though it may be possible to map some ordinal fields to numeric values if there is confidence that the resulting intervals would be valid.","eaa5eb96":"ExterQual and KitchenQual can both be usefully processed into a binary feature with good correlation, though with some risk of adversely influencing some low frequency cases. \n\nHowever, they are highly correlated with each other so only one of them should be included. I will choose ExterQual as it has the higher correlation and lower p-value.","4b21450d":"The distribution of the training and validation sets looks similar and looks to have a right skew which should be investigated further. First I will look at how well correlated the numerical features of the data set are with the sale price. The categorical features will require some further processing before they can be checked in the same way.","6b1f6f65":"#### FireplaceQu, MasVrnType, Neighborhood","df72772c":"There does seem to be a genuine correlation with having two kitchens and a lower sale price. Whether this is causal or not it may help the model so I will code a feature which is 1 is two kitchens and 0 otherwise.","249f5ef4":"The top two performing models in base configuration are GradientBoosting and Ridge Regression. As these take two fundementally different approaches I will optimise both on the validation set and choose the best performing as my final model.\n\n#### Model Tuning","1115ee92":"#### GrLivArea\nThe feature with the highest correlation to sale price after overall quality is 'GrLivArea'. Other features which may work well with this are 'LotArea' and 'TotalBsmtSF' to give an overall size feature and 'TotRmsAbvGrd', 'FullBath' and 'HalfBath' which can be combined to give the total number of (non) basement rooms in the house.","b27e3bd7":"#### Garage Type and Garage Finish","048b53ae":"The primary difference in FireplaceQu and MasVnrType seems to be around whether or not the value is recorded as 'None' so I will create a new feature to capture this.\n\nFor neighbourhood there is too much going on and given it's low p-value I will just create a full dummy encoding.\n\nI will now look at these features for multicolineraity.","2fd608bf":"### Numerical Feature Processing\nNow I need to create my engineered numerical features in all three dataframes","cb8d22ec":"#### Kitchens Above Grade\nThis shows up has having one of the stronger negative correlations so I want to investigate it further","e6ca6e00":"#### Overall Quality\nThe feature most highly correlated with sale price is the Overall Quality. Analysing this in conjunction with the Overall Condition might be instructive.","9099bfdd":"On initial inspection Lot Area has a much lower correlation with the Sale Price than the other area features so I will not proceed with mixing it in for a new feature. There also look to be clear outliers on Living Area above 4000 sqft and a single large basement above 600 sqft which I will remove from the training set.","d8405cdf":"##### Basement Variables\nTest observation 660 has a NaN for TotalBsmtSF. Examining this record it does not appear that there is a basement. I will therefore set TotalBsmtSF to 0 which will allow the other basement feature processing to occur without error.\n\nThere are two values in the test set where there is a non-zero TotalBsmtSF area recorded but NA BsmtQual. Both houses\ncome from the same Neighbourhood and Zoning. Looking at the training set houses with these characteristics have TA BsmtQual\nso I will impute as this.\n\nWhere there is a total basement area greater than 0 but missing basement quality and\/ or condition entries these will be assumed as 'TA', which is the coding for typical quality.\n\nWhere the total basement area is zero and qualitative qualitative basement feature values are missing these wil be set to 'None', except for 'BasementExposure' which will be set to 'No' to fit with the convention for this feature.\n\nIf a basement has a valid FinType1 and a NaN for FinType to this will be set to 'Unf'.\n\nIf total basement SF = 0 and BsmtFinSF1 is NaN then BsmtFinSF1 and BsmtUnfSF will be set to 0.\n\nMissing BsmtFinSF2 will be set to 0.\n","6cfd2a90":"Looking at the data OverallQual, GarageCars and GarageFinish_Ordinal can all be sensible scaled to be between 0 and 1 while TotalArea and YearBuilt are probably better standardized.","f5318ac9":"Dummy encode each categorical feature in turn then measure the correlation with the sale price. This will tell us which features under a univariate analysis are useful predictors.","54a7663f":"#### Test Initial Models\nA number of regression model are available. I will test the base version of these on the training data set to see which performs the best","2d437249":"### Numerical Features\n","f977a26c":"### NaN removal explained\nI have not set out to create an exhaustive process for removing all NaNs from any data set using these features. Rather I have looked to tackle the NaNs missing from these specific training and test sets but generalised where possible.\n\n#### Starting Point\n18 features in the training set contain NaNs, 33 features in the test set contain NaNs.\n\n##### Lot Frontage\n\nAll dwellings are houses so should have some frontage, i.e. there are no flats which would not have any street connected to the property. I will impute this with the mean frontage for the Neighbourhood of the house.\n\n##### Alley \n\nNaN for this feature is defined in data description as meaning there is no alley access -> change to \"None\".","8ce1ea50":"##### Mason Venner Type and Area\n\nMasVrnType and MasVrnArea have multiple instances where the Area is 0 in which case the Type NaN should be set to 'None'. There is one instance in the test set where an Area is given but no type. This is for record 1150. Inspecting this record the house is made of plywood and the most common Veneer type for Plywood houses when they have one is \"BrkFace\".","2d833a4a":"GradientBoosting is marginally better but I will produce predictions for the test set using the best parameters found for both models and the combined training and validation sets for training. The final predictions will need converting back into actual dollars instead of log(dollars)","c0a05ceb":"Having loaded the data I will run some basic examinations and descriptive statistics to get a feel for the data before examining what NaNs are present and fixing them.","77970766":"There is clearly heteroscedasticity present is the house price as the price increases. Further transformation of the feature might reduce this but at the cost of model interpretability and ease of converting the final model output back into a $ figure. I will therefore proceed with using the log of the Sale Price and accept the heteroscedasticity.\n\nOverall Condition does not have a high correlation with Sale Price and what correlation there is in the inverse of Overall Quality so combining these will not produce a useful feature.\n\nI will remove the outliers with a Sale Price in excess of $700,000 from the training set. In a real life situation it would be important to understand how important to the client it was to factor in very high sale prices.","ae20d06b":"#### Foundation\nFoundation has six categories though the value counts are heavily dominated by two of them. It should hopefully be able to again reduce which of the categories need to be considered.","9cfd4114":"ExterQual, KitchenQual and having a PConc foundation are more highly correlated with each other than with the Sale Price. I will therefore only include one of these in the final model. I will choose ExterQual as it has the lowest p-value and highest correlation.","7591e825":"#### Year Built and Year Remodelled\nBoth of these features have high correlations to Sale Price. I will investigate whether the last year which work was carried out is more predictive than wither individually.","ee8bac3b":"As expecting from the aboce analysis TotalArea is highly correlated with a number of other features in the original data set as well as those which I engineered. Based on univariate correlation and colinearity I will use an initial subset of features:\n\nOverallQual, HasFireplace, MasVrn, OverallQual, TotalArea, GarageCars, YearBuilt, TwoKitchens, HasPorch and the binary encoded Neighbourhood columns\n\n#### Create selected feature subset","e4a6afa2":"I will now engineer the features above to create new features which will hopefully have more predictive power.","81e63fda":"Neighborhood, ExterQual, KitchenQual, Foundation, HeatingQC and MasVnrType all have a significant p-value and a single categoriy with a relatively high correlation to sale price.","fb60b003":"The CBlock foundation type has a stronger correlation than any other category so I will binary encode this feature with 1 for CBlock else 0."}}