{"cell_type":{"5dada7ea":"code","74f0864f":"code","91a7416c":"code","907baa76":"code","25a46e60":"code","3240f2b2":"code","1dc3c927":"code","0b0273f8":"code","5f44224e":"code","c97c760b":"code","816d25da":"code","e709c434":"code","f7ad5e5a":"code","16845df2":"code","e7398676":"code","de31fdaf":"code","3c0d8c84":"code","f7202825":"code","3eb446f7":"code","67088f4f":"code","d92a9a5a":"code","6c78a376":"code","935932d1":"code","a6c62232":"code","d33b3f56":"code","3f737112":"code","0d82a183":"code","3d8291b4":"code","cb783f42":"code","b32aa122":"code","78839543":"code","04620838":"code","8049d86d":"code","6f6c1973":"code","19bb739d":"code","5a8910fe":"code","6e2ba886":"code","47388d54":"code","d4e40424":"code","256985f5":"code","cc479473":"code","26e629ac":"code","09e78fc0":"code","126aa278":"code","ab6cc948":"code","a602e92c":"code","feea5fbf":"code","8de66dbd":"code","bebcf5ff":"code","9edad566":"code","b0e72832":"code","14c60547":"code","330ecc14":"code","edbca8ac":"code","d611e1c2":"code","c6a67902":"markdown","7fcbdd92":"markdown","82e3fd1a":"markdown","cd9dc896":"markdown","cfbad2e4":"markdown","d495fa77":"markdown","96f179d7":"markdown","4a209747":"markdown","4f00b76c":"markdown","26b1b92a":"markdown","4ba4d358":"markdown","26ff5c76":"markdown","ddefdc02":"markdown","56592a45":"markdown","52fe23a4":"markdown","f222e4b8":"markdown","8e74cfa6":"markdown","eaca31db":"markdown","b75d211d":"markdown","fd332cc6":"markdown","a2e4e90e":"markdown","67b77cfd":"markdown","4f416d9a":"markdown","0839ab76":"markdown","20943cac":"markdown","dee83ee2":"markdown","a5d6fa93":"markdown","9b423c5d":"markdown"},"source":{"5dada7ea":"# import statements\nimport requests","74f0864f":"# fetch web page\nr = requests.get('https:\/\/www.udacity.com\/courses\/all')","91a7416c":"# Installing bs4\n!pip install bs4","907baa76":"# import statements\nfrom bs4 import BeautifulSoup","25a46e60":"soup = BeautifulSoup(r.text,'lxml')","3240f2b2":"print(soup.get_text())","1dc3c927":"# Find all course summaries\nsummaries = soup.find_all(\"li\",{\"class\":\"catalog-card-list_catalogCardListItem__AZBy6\"})\nprint('Number of Courses:', len(summaries))\n","0b0273f8":"print(summaries[0].prettify())","5f44224e":"# Extract course title\nsummaries[0].select_one(\"h2\").get_text().strip()","c97c760b":"# Extract school\nsummaries[0].select_one(\"h3\").get_text().strip()","816d25da":"courses = []\nfor summary in summaries:\n    # append name and school of each summary to courses list\n    title = summary.select_one(\"h2\").get_text().strip()\n    school = summary.select_one(\"h3\").get_text().strip()\n    courses.append((title, school))","e709c434":"# display results\nprint(len(courses), \"course summaries found. Sample:\")\ncourses[:5]","f7ad5e5a":"text = \"The first time you see The Second Renaissance it may look boring. Look at it at least twice and definitely watch part 2. It will change your view of the matrix. Are the human people the ones who started the war ? Is AI a bad thing ?\"\nprint(text)","16845df2":"# Convert to lowercase\ntext = text.lower()\nprint(text)","e7398676":"# Remove punctuation characters\nimport re\ntext = re.sub(r\"[^a-zA-Z0-9]\",\" \",text)\nprint(text)","de31fdaf":"import nltk\nnltk.download('punkt')","3c0d8c84":"# import statements\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import sent_tokenize","f7202825":"text = \"Dr. Smith graduated from the University of Washington. He later started an analytics firm called Lux, which catered to enterprise customers.\"\nprint(text)","3eb446f7":"# Split text into words using NLTK\nwords = word_tokenize(text)\nprint(words)","67088f4f":"# Split text into sentences using NLTK\nwords = sent_tokenize(text)\nprint(words)","d92a9a5a":"nltk.download('stopwords')","6c78a376":"# import statements\nfrom nltk.corpus import stopwords\nimport re\nfrom nltk.tokenize import word_tokenize\n","935932d1":"text = \"The first time you see The Second Renaissance it may look boring. Look at it at least twice and definitely watch part 2. It will change your view of the matrix. Are the human people the ones who started the war ? Is AI a bad thing ?\"\nprint(text)","a6c62232":"# Normalize text\ntext = text.lower()\ntext = re.sub(r\"[^a-zA-Z0-9]\",\" \",text)\n","d33b3f56":"# Tokenize text\nwords = word_tokenize(text)\nprint(words)","3f737112":"# remove stop wors\nwords = [w for w in words if w not in stopwords.words(\"english\")]\nprint(words)","0d82a183":"print(stopwords.words(\"english\"))","3d8291b4":"nltk.download('words')\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('maxent_ne_chunker')","cb783f42":"#  import statements\nfrom nltk import pos_tag\nfrom nltk import ne_chunk","b32aa122":"text = \"I always lie down to tell a lie.\"","78839543":"# tokenize text\nsentence = word_tokenize(text)\n\n# tag each word with part of speech\npos_tag(sentence)","04620838":"text = \"Antonio joined Udacity Inc. in California.\"","8049d86d":"# tokenize, pos tag, then recognize named entities in text\ntree = ne_chunk(pos_tag(word_tokenize(text)))\nprint(tree)","6f6c1973":"# Define a custom grammar\nmy_grammar = nltk.CFG.fromstring(\"\"\"\nS -> NP VP\nPP -> P NP\nNP -> Det N | Det N PP | 'I'\nVP -> V NP | VP PP\nDet -> 'an' | 'my'\nN -> 'elephant' | 'pajamas'\nV -> 'shot'\nP -> 'in'\n\"\"\")\nparser = nltk.ChartParser(my_grammar)","19bb739d":"# Parse a sentence\nsentence = word_tokenize(\"I shot an elephant in my pajamas\")\nfor tree in parser.parse(sentence):\n    print(tree)","5a8910fe":"nltk.download('wordnet') # download for lemmatization","6e2ba886":"text = \"The first time you see The Second Renaissance it may look boring. Look at it at least twice and definitely watch part 2. It will change your view of the matrix. Are the human people the ones who started the war ? Is AI a bad thing ?\"\n\n# Normalize text\ntext = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n\n# Tokenize text\nwords = text.split()\nprint(words)","47388d54":"# Remove stop words\nwords = [w for w in words if w not in stopwords.words(\"english\")]\nprint(words)","d4e40424":"from nltk.stem.porter import PorterStemmer\n\n# Reduce words to their stems\nstemmed = [PorterStemmer().stem(w) for w in words]\nprint(stemmed)","256985f5":"from nltk.stem.wordnet import WordNetLemmatizer\n\n# Reduce words to their root form\nlemmed = [WordNetLemmatizer().lemmatize(w) for w in words]\nprint(lemmed)","cc479473":"# Lemmatize verbs by specifying pos\nlemmed = [WordNetLemmatizer().lemmatize(w, pos='v') for w in lemmed]\nprint(lemmed)","26e629ac":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')","09e78fc0":"corpus = [\"The first time you see The Second Renaissance it may look boring.\",\n        \"Look at it at least twice and definitely watch part 2.\",\n        \"It will change your view of the matrix.\",\n        \"Are the human people the ones who started the war?\",\n        \"Is AI a bad thing ?\"]","126aa278":"stop_words = stopwords.words(\"english\")\nlemmatizer = WordNetLemmatizer()","ab6cc948":"def tokenize(text):\n    # normalize case and remove punctuation\n    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n    \n    # tokenize text\n    tokens = word_tokenize(text)\n    \n    # lemmatize andremove stop words\n    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n\n    return tokens","a602e92c":"from sklearn.feature_extraction.text import CountVectorizer\n\n# initialize count vectorizer object\nvect = CountVectorizer(tokenizer=tokenize)","feea5fbf":"# get counts of each token (word) in text data\nX = vect.fit_transform(corpus)","8de66dbd":"# convert sparse matrix to numpy array to view\nX.toarray()","bebcf5ff":"# view token vocabulary and counts\nvect.vocabulary_","9edad566":"from sklearn.feature_extraction.text import TfidfTransformer\n\n# initialize tf-idf transformer object\ntransformer = TfidfTransformer(smooth_idf=False)","b0e72832":"# use counts from count vectorizer results to compute tf-idf values\ntfidf = transformer.fit_transform(X)","14c60547":"# convert sparse matrix to numpy array to view\ntfidf.toarray()","330ecc14":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# initialize tf-idf vectorizer object\nvectorizer = TfidfVectorizer()","edbca8ac":"# compute bag of word counts and tf-idf values\nX = vectorizer.fit_transform(corpus)","d611e1c2":"# convert sparse matrix to numpy array to view\nX.toarray()","c6a67902":"# Text Processing\nThe first part of this notebook will explore the steps involved in **text processing**, the first stage of the NLP pipeline.","7fcbdd92":"## `TfidfVectorizer`\n`TfidfVectorizer` = `CountVectorizer` + `TfidfTransformer`","82e3fd1a":"### Lemmatization","cd9dc896":"## Stemming and Lemmatization\n","cfbad2e4":"### Step 1: Get text from Udacity's course catalog web page\nYou can use the `requests` library to do this.\n\nOutputting all the javascript, CSS, and text may overload the space available to load this notebook, so we omit a print statement here.","d495fa77":"## `CountVectorizer` (Bag of Words)","96f179d7":"# Modeling\nThe final stage of the NLP pipeline is modeling, which includes designing a statistical or machine learning model, fitting its parameters to training data, using an optimization procedure, and then using it to make predictions about unseen data.\n\nThe nice thing about working with numerical features is that it allows you to choose from all machine learning models or even a combination of them.\n\nOnce you have a working model, you can deploy it as a web app, mobile app, or integrate it with other products and services. The possibilities are endless!","4a209747":"![text%20processing.jpg](attachment:text%20processing.jpg)","4f00b76c":"# Stop Words","26b1b92a":"Look for selectors contain the the courses title and school name text you want to extract. Then, use the select_one method on the summary object to pull out the html with those selectors. Afterwards, don't forget to do some extra cleaning to isolate the names (get rid of unnecessary html).","4ba4d358":"# Normalization","26ff5c76":"# Natural Language Processing Pipelines\n\nThis notebook is part of my learning journey of Udacity's Natural Language processing program, which I really helped me learn and excel basics and advanced NLP topics \n\n![nlp%20pipeline.jpg](attachment:nlp%20pipeline.jpg)\n\nThe 3 stages of an NLP pipeline are: Text Processing > Feature Extraction > Modeling.\n\n- **Text Processing:** Take raw input text, clean it, normalize it, and convert it into a form that is suitable for feature extraction.\n- **Feature Extraction:** Extract and produce feature representations that are appropriate for the type of NLP task you are trying to accomplish and the type of model you are planning to use.\n- **Modeling:** Design a statistical or machine learning model, fit its parameters to training data, use an optimization procedure, and then use it to make predictions about unseen data.\n\nThis process isn't always linear and may require additional steps.","ddefdc02":"### Step 3: Find all course summaries\nUse the BeautifulSoup's `find_all` method to select based on tag type and class name. Just like in the video, you can right click on the item, and click \"Inspect\" to view its html on a web page.","56592a45":"### Step 2: Use BeautifulSoup to remove HTML tags","52fe23a4":"## Named Entity Recognition (NER)","f222e4b8":"## Sentence Parsing","8e74cfa6":"## Bag of Words and TF-IDF\nBelow, we'll look at three useful methods of vectorizing text.\n- `CountVectorizer` - Bag of Words\n- `TfidfTransformer` - TF-IDF values\n- `TfidfVectorizer` - Bag of Words AND TF-IDF values\n ","eaca31db":"## Parts of Speech (POS) Tagging\n","b75d211d":"### Case Normalization","fd332cc6":"## `TfidfTransformer`","a2e4e90e":"> ## Cleaning: Udacity's Course Catalog\n\nUdacity's [course catalog page](https:\/\/www.udacity.com\/courses\/all)\n\nIn this activity, you're going to extract the following information from each course listing on the page:\n1. The course name - e.g. \"Data Analyst\"\n2. The school the course belongs to - e.g. \"School of Data Science\"","67b77cfd":"# Tokenization","4f416d9a":"### Step 5: Collect names and schools of ALL course listings\nReuse your code from the previous step, but now in a loop to extract the name and school from every course summary in `summaries`!","0839ab76":"# Feature Extraction","20943cac":"**Why Do We Need to Process Text?**\n\n- **Extracting plain text:** Textual data can come from a wide variety of sources: the web, PDFs, word documents, speech recognition systems, book scans, etc. Your goal is to extract plain text that is free of any source specific markup or constructs that are not relevant to your task.\n- **Reducing complexity:** Some features of our language like capitalization, punctuation, and common words such as a, of, and the, often help provide structure, but don't add much meaning. Sometimes it's best to remove them if that helps reduce the complexity of the procedures you want to apply later.\n\nYou'll prepare text data from different sources with the following text processing steps:\n\n1. Cleaning to remove irrelevant items, such as HTML tags\n2. Normalizing by converting to all lowercase and removing punctuation\n3. Splitting text into words or tokens\n4. Removing words that are too common, also known as stop words\n5. Identifying different parts of speech and named entities\n6. Converting words into their dictionary forms, using stemming and lemmatization\n\nAfter performing these steps, your text will capture the essence of what was being conveyed in a form that is easier to work with.","dee83ee2":"### Step 4: Inspect the first summary to find selectors for the course name and school","a5d6fa93":"### Punctuation Removal\nUse the `re` library to remove punctuation with a regular expression (regex). Feel free to referGoogle to get your regular expression. You can learn more about regex [here](https:\/\/docs.python.org\/3\/howto\/regex.html).","9b423c5d":"### Stemming"}}