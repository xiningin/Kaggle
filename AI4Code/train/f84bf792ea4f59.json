{"cell_type":{"fed3b2b4":"code","c67475dd":"code","e5b9ec32":"code","57d699b3":"code","cb11db34":"code","29e96ce7":"code","aa9c8c4a":"code","7f3642e0":"code","9ee58e02":"code","2934151d":"code","dcb1ea20":"code","e119fbe0":"code","25bbdd2c":"markdown","1c949792":"markdown","929d4c59":"markdown","839c336b":"markdown","9dd64fba":"markdown"},"source":{"fed3b2b4":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers as L\nfrom tensorflow_addons.activations import sparsemax\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import StratifiedKFold\nimport joblib\n","c67475dd":"data = pd.read_csv('..\/input\/song-popularity-prediction\/train.csv')\nprint(data.shape)\ndata.head()","e5b9ec32":"test = pd.read_csv('..\/input\/song-popularity-prediction\/test.csv')\nX_test = test.drop(['id'], axis=1)","57d699b3":"X = data.drop(['id', 'song_popularity'], axis=1)\ny = data[['song_popularity']]","cb11db34":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)","29e96ce7":"def register_keras_custom_object(cls):\n    tf.keras.utils.get_custom_objects()[cls.__name__] = cls\n    return cls\n\n\ndef glu(x, n_units=None):\n    if n_units is None:\n        n_units = tf.shape(x)[-1] \/\/ 2\n\n    return x[..., :n_units] * tf.nn.sigmoid(x[..., n_units:])\n\n\n@register_keras_custom_object\n@tf.function\ndef sparsemax(logits, axis):\n    logits = tf.convert_to_tensor(logits, name=\"logits\")\n\n    shape = logits.get_shape()\n    rank = shape.rank\n    is_last_axis = (axis == -1) or (axis == rank - 1)\n\n    if is_last_axis:\n        output = _compute_2d_sparsemax(logits)\n        output.set_shape(shape)\n        return output\n\n    rank_op = tf.rank(logits)\n    axis_norm = axis % rank\n    logits = _swap_axis(logits, axis_norm, tf.math.subtract(rank_op, 1))\n\n    output = _compute_2d_sparsemax(logits)\n    output = _swap_axis(output, axis_norm, tf.math.subtract(rank_op, 1))\n\n    output.set_shape(shape)\n    return output\n\n\ndef _swap_axis(logits, dim_index, last_index, **kwargs):\n    return tf.transpose(\n        logits,\n        tf.concat(\n            [\n                tf.range(dim_index),\n                [last_index],\n                tf.range(dim_index + 1, last_index),\n                [dim_index],\n            ],\n            0,\n        ),\n        **kwargs,\n    )\n\n\ndef _compute_2d_sparsemax(logits):\n    shape_op = tf.shape(logits)\n    obs = tf.math.reduce_prod(shape_op[:-1])\n    dims = shape_op[-1]\n    z = tf.reshape(logits, [obs, dims])\n    z_sorted, _ = tf.nn.top_k(z, k=dims)\n    z_cumsum = tf.math.cumsum(z_sorted, axis=-1)\n    k = tf.range(1, tf.cast(dims, logits.dtype) + 1, dtype=logits.dtype)\n    z_check = 1 + k * z_sorted > z_cumsum\n    k_z = tf.math.reduce_sum(tf.cast(z_check, tf.int32), axis=-1)\n    k_z_safe = tf.math.maximum(k_z, 1)\n    indices = tf.stack([tf.range(0, obs), tf.reshape(k_z_safe, [-1]) - 1], axis=1)\n    tau_sum = tf.gather_nd(z_cumsum, indices)\n    tau_z = (tau_sum - 1) \/ tf.cast(k_z, logits.dtype)\n    p = tf.math.maximum(tf.cast(0, logits.dtype), z - tf.expand_dims(tau_z, -1))\n    p_safe = tf.where(\n        tf.expand_dims(\n            tf.math.logical_or(tf.math.equal(k_z, 0), tf.math.is_nan(z_cumsum[:, -1])),\n            axis=-1,\n        ),\n        tf.fill([obs, dims], tf.cast(float(\"nan\"), logits.dtype)),\n        p,\n    )\n    p_safe = tf.reshape(p_safe, shape_op)\n    return p_safe","aa9c8c4a":"class TransformBlock(tf.keras.Model):\n\n    def __init__(\n        self, features, momentum=0.9, virtual_batch_size=None, block_name='', **kwargs):\n        super().__init__(**kwargs)\n        self.features = features\n        self.momentum = momentum\n        self.virtual_batch_size = virtual_batch_size\n\n        self.transform = tf.keras.layers.Dense(self.features, use_bias=False, name=f'transformblock_dense_{block_name}')\n        self.bn = tf.keras.layers.BatchNormalization(\n            axis=-1, momentum=momentum, virtual_batch_size=virtual_batch_size, name=f'transformblock_bn_{block_name}'\n        )\n\n    def call(self, inputs, training=None):\n        x = self.transform(inputs)\n        x = self.bn(x, training=training)\n        return x\n","7f3642e0":"class TabNet(tf.keras.Model):\n    def __init__(\n        self, \n        feature_columns, \n        feature_dim=64,\n        output_dim=64,\n        num_features=None,\n        num_decision_steps=5,\n        relaxation_factor=1.5,\n        sparsity_coefficient=1e-5,\n        batch_momentum=0.98,\n        virtual_batch_size=None,\n        epsilon=1e-5,\n        **kwargs\n    ):\n        super(TabNet, self).__init__(**kwargs)\n\n        if feature_columns is not None:\n            if type(feature_columns) not in (list, tuple):\n                raise ValueError(\"`feature_columns` must be a list or a tuple.\")\n\n            if len(feature_columns) == 0:\n                raise ValueError(\"`feature_columns` must be contain at least 1 tf.feature_column !\")\n\n            if num_features is None:\n                num_features = len(feature_columns)\n            else:\n                num_features = int(num_features)\n\n        else:\n            if num_features is None:\n                raise ValueError(\"If `feature_columns` is None, then `num_features` cannot be None.\")\n\n        if num_decision_steps < 1:\n            raise ValueError(\"Num decision steps must be greater than 0.\")\n\n        if feature_dim <= output_dim:\n            raise ValueError(\"To compute `features_for_coef`, feature_dim must be larger than output dim\")\n\n        feature_dim = int(feature_dim)\n        output_dim = int(output_dim)\n        num_decision_steps = int(num_decision_steps)\n        relaxation_factor = float(relaxation_factor)\n        sparsity_coefficient = float(sparsity_coefficient)\n        batch_momentum = float(batch_momentum)\n        epsilon = float(epsilon)\n\n        if relaxation_factor < 0.:\n            raise ValueError(\"`relaxation_factor` cannot be negative !\")\n\n        if sparsity_coefficient < 0.:\n            raise ValueError(\"`sparsity_coefficient` cannot be negative !\")\n\n        if virtual_batch_size is not None:\n            virtual_batch_size = int(virtual_batch_size)\n\n        self.feature_columns = feature_columns\n        self.num_features = num_features\n        self.feature_dim = feature_dim\n        self.output_dim = output_dim\n\n        self.num_decision_steps = num_decision_steps\n        self.relaxation_factor = relaxation_factor\n        self.sparsity_coefficient = sparsity_coefficient\n        self.batch_momentum = batch_momentum\n        self.virtual_batch_size = virtual_batch_size\n        self.epsilon = epsilon\n\n        if num_decision_steps > 1:\n            features_for_coeff = feature_dim - output_dim\n            print(f\"[TabNet]: {features_for_coeff} features will be used for decision steps.\")\n\n        if self.feature_columns is not None:\n            self.input_features = tf.keras.layers.DenseFeatures(feature_columns, trainable=True)\n\n            self.input_bn = tf.keras.layers.BatchNormalization(axis=-1, momentum=batch_momentum, name='input_bn')\n            \n        else:\n            self.input_features = None\n            self.input_bn = None\n\n        self.transform_f1 = TransformBlock(\n            2 * self.feature_dim,self.batch_momentum, self.virtual_batch_size, block_name='f1'\n        )\n\n        self.transform_f2 = TransformBlock(\n            2 * self.feature_dim, self.batch_momentum, self.virtual_batch_size, block_name='f2'\n        )\n\n        self.transform_f3_list = [\n            TransformBlock(2 * self.feature_dim, self.batch_momentum, self.virtual_batch_size, block_name=f'f3_{i}')\n            for i in range(self.num_decision_steps)\n        ]\n\n        self.transform_f4_list = [\n            TransformBlock(2 * self.feature_dim, self.batch_momentum, self.virtual_batch_size, block_name=f'f4_{i}')\n            for i in range(self.num_decision_steps)\n        ]\n\n        self.transform_coef_list = [\n            TransformBlock(self.num_features, self.batch_momentum, self.virtual_batch_size, block_name=f'coef_{i}')\n            for i in range(self.num_decision_steps - 1)\n        ]\n\n        self._step_feature_selection_masks = None\n        self._step_aggregate_feature_selection_mask = None\n\n    def call(self, inputs, training=None):\n        if self.input_features is not None:\n            features = self.input_features(inputs)\n            features = self.input_bn(features, training=training)\n\n        else:\n            features = inputs\n\n        batch_size = tf.shape(features)[0]\n        self._step_feature_selection_masks = []\n        self._step_aggregate_feature_selection_mask = None\n\n        output_aggregated = tf.zeros([batch_size, self.output_dim])\n        masked_features = features\n        mask_values = tf.zeros([batch_size, self.num_features])\n        aggregated_mask_values = tf.zeros([batch_size, self.num_features])\n        complementary_aggregated_mask_values = tf.ones(\n            [batch_size, self.num_features])\n\n        total_entropy = 0.0\n        entropy_loss = 0.\n\n        for ni in range(self.num_decision_steps):\n            transform_f1 = self.transform_f1(masked_features, training=training)\n            transform_f1 = glu(transform_f1, self.feature_dim)\n\n            transform_f2 = self.transform_f2(transform_f1, training=training)\n            transform_f2 = (glu(transform_f2, self.feature_dim) +\n                            transform_f1) * tf.math.sqrt(0.5)\n\n            transform_f3 = self.transform_f3_list[ni](transform_f2, training=training)\n            transform_f3 = (glu(transform_f3, self.feature_dim) +\n                            transform_f2) * tf.math.sqrt(0.5)\n\n            transform_f4 = self.transform_f4_list[ni](transform_f3, training=training)\n            transform_f4 = (glu(transform_f4, self.feature_dim) +\n                            transform_f3) * tf.math.sqrt(0.5)\n\n            if (ni > 0 or self.num_decision_steps == 1):\n                decision_out = tf.nn.relu(transform_f4[:, :self.output_dim])\n                output_aggregated += decision_out\n                scale_agg = tf.reduce_sum(decision_out, axis=1, keepdims=True)\n\n                if self.num_decision_steps > 1:\n                    scale_agg = scale_agg \/ tf.cast(self.num_decision_steps - 1, tf.float32)\n\n                aggregated_mask_values += mask_values * scale_agg\n\n            features_for_coef = transform_f4[:, self.output_dim:]\n\n            if ni < (self.num_decision_steps - 1):\n                mask_values = self.transform_coef_list[ni](features_for_coef, training=training)\n                mask_values *= complementary_aggregated_mask_values\n                mask_values = sparsemax(mask_values, axis=-1)\n\n                complementary_aggregated_mask_values *= (\n                        self.relaxation_factor - mask_values)\n                total_entropy += tf.reduce_mean(\n                    tf.reduce_sum(\n                        -mask_values * tf.math.log(mask_values + self.epsilon), axis=1)) \/ (\n                                     tf.cast(self.num_decision_steps - 1, tf.float32))\n\n                entropy_loss = total_entropy\n\n                masked_features = tf.multiply(mask_values, features)\n\n                mask_at_step_i = tf.expand_dims(tf.expand_dims(mask_values, 0), 3)\n                self._step_feature_selection_masks.append(mask_at_step_i)\n\n            else:\n                entropy_loss = 0.\n\n        self.add_loss(self.sparsity_coefficient * entropy_loss)\n\n        agg_mask = tf.expand_dims(tf.expand_dims(aggregated_mask_values, 0), 3)\n        self._step_aggregate_feature_selection_mask = agg_mask\n\n        return output_aggregated\n\n    @property\n    def feature_selection_masks(self):\n        return self._step_feature_selection_masks\n\n    @property\n    def aggregate_feature_selection_mask(self):\n        return self._step_aggregate_feature_selection_mask\n\n\nclass TabNetClassifier(tf.keras.Model):\n\n    def __init__(\n        self, \n        feature_columns,\n        num_classes,\n        num_features=None,\n        feature_dim=64,\n        output_dim=64,\n        num_decision_steps=5,\n        relaxation_factor=1.5,\n        sparsity_coefficient=1e-5,\n        batch_momentum=0.98,\n        virtual_batch_size=None,\n        epsilon=1e-5,\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n\n        self.num_classes = num_classes\n\n        self.tabnet = TabNet(\n            feature_columns=feature_columns,\n            num_features=num_features,\n            feature_dim=feature_dim,\n            output_dim=output_dim,\n            num_decision_steps=num_decision_steps,\n            relaxation_factor=relaxation_factor,\n            sparsity_coefficient=sparsity_coefficient,\n            batch_momentum=batch_momentum,\n            virtual_batch_size=virtual_batch_size,\n            epsilon=epsilon,\n            **kwargs\n        )\n\n        self.clf = tf.keras.layers.Dense(num_classes, activation='softmax', use_bias=False, name='classifier')\n\n    def call(self, inputs, training=None):\n        self.activations = self.tabnet(inputs, training=training)\n        out = self.clf(self.activations)\n        return out\n\n    def summary(self, *super_args, **super_kwargs):\n        super().summary(*super_args, **super_kwargs)\n        self.tabnet.summary(*super_args, **super_kwargs)\n\n\nclass TabNetRegressor(tf.keras.Model):\n\n    def __init__(\n        self, \n        feature_columns,\n        num_regressors,\n        num_features=None,\n        feature_dim=64,\n        output_dim=64,\n        num_decision_steps=5,\n        relaxation_factor=1.5,\n        sparsity_coefficient=1e-5,\n        batch_momentum=0.98,\n        virtual_batch_size=None,\n        epsilon=1e-5,\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.num_regressors = num_regressors\n        self.tabnet = TabNet(\n            feature_columns=feature_columns,\n            num_features=num_features,\n            feature_dim=feature_dim,\n            output_dim=output_dim,\n            num_decision_steps=num_decision_steps,\n            relaxation_factor=relaxation_factor,\n            sparsity_coefficient=sparsity_coefficient,\n            batch_momentum=batch_momentum,\n            virtual_batch_size=virtual_batch_size,\n            epsilon=epsilon,\n            **kwargs\n        )\n        self.regressor = tf.keras.layers.Dense(num_regressors, use_bias=False, name='regressor')\n\n    def call(self, inputs, training=None):\n        self.activations = self.tabnet(inputs, training=training)\n        out = self.regressor(self.activations)\n        return out\n\n    def summary(self, *super_args, **super_kwargs):\n        super().summary(*super_args, **super_kwargs)\n        self.tabnet.summary(*super_args, **super_kwargs)","9ee58e02":"get_cat_pipeline = lambda: Pipeline([\n    ('imputer', SimpleImputer(strategy='most_frequent')), \n    ('encoder', OneHotEncoder(sparse=False))\n])\n\nget_num_pipeline = lambda: Pipeline([\n    ('imputer', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])","2934151d":"class model_config:\n    NUMERIC_FEATURE_NAMES=[\n        'song_duration_ms', 'acousticness', 'danceability', 'energy', 'instrumentalness', 'liveness', 'loudness',\n        'speechiness', 'tempo', 'audio_valence'\n    ]\n    CATEGORICAL_FEATURE_NAMES=[\n        'key','audio_mode','time_signature'   \n    ]\n\nMAX_EPOCHS  = 250\n\nget_callbacks = lambda : [\n    keras.callbacks.EarlyStopping(min_delta=1e-4, patience=10, verbose=1, restore_best_weights=True),\n    keras.callbacks.ReduceLROnPlateau(patience=3, verbose=1)\n]","dcb1ea20":"preds = []\n\nfor fold, (train_index, valid_index) in enumerate(skf.split(X, y)):\n    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n\n    num_pipeline = get_num_pipeline().fit(X_train[model_config.NUMERIC_FEATURE_NAMES])\n    cat_pipeline = get_cat_pipeline().fit(X_train[model_config.CATEGORICAL_FEATURE_NAMES])\n    \n    X_train = np.hstack((\n        num_pipeline.transform(X_train[model_config.NUMERIC_FEATURE_NAMES]),\n        cat_pipeline.transform(X_train[model_config.CATEGORICAL_FEATURE_NAMES])\n    ))\n    X_valid = np.hstack((\n        num_pipeline.transform(X_valid[model_config.NUMERIC_FEATURE_NAMES]),\n        cat_pipeline.transform(X_valid[model_config.CATEGORICAL_FEATURE_NAMES])\n    ))\n    X_test_ = np.hstack((\n        num_pipeline.transform(X_test[model_config.NUMERIC_FEATURE_NAMES]),\n        cat_pipeline.transform(X_test[model_config.CATEGORICAL_FEATURE_NAMES])\n    ))\n    \n    model = TabNetClassifier(\n        feature_columns=None, num_classes=2, num_features=X_train.shape[1], feature_dim=16, output_dim=12)\n    model.compile(\n        loss='sparse_categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'], run_eagerly=True\n    )\n    model.fit(\n        X_train, y_train, validation_data=(X_valid, y_valid), callbacks=get_callbacks(), \n        epochs=MAX_EPOCHS\n    )  \n    preds.append(model.predict(X_test_))","e119fbe0":"submissions = pd.read_csv('..\/input\/song-popularity-prediction\/sample_submission.csv')\nsubmissions['song_popularity'] = np.array([arr[:, 1] for arr in preds]).mean(axis=0)\nsubmissions.to_csv('preds.csv', index=False)","25bbdd2c":"# Submissions","1c949792":"# TabNet\n\nTabNet (introduced [TabNet: Attentive Interpretable Tabular Learning](https:\/\/arxiv.org\/pdf\/1908.07442.pdf)) is a novel high-performance and interpretable canonical deep tabular data learning architecture. It uses sequential attention to choose which features to reason from at each decision step, enabling interpretability and more efficient learning as the learning capacity is used for the most salient features.","929d4c59":"# Data","839c336b":"# Training","9dd64fba":"# Model"}}