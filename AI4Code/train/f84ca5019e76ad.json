{"cell_type":{"0d238223":"code","17929b4d":"code","560b84e9":"code","5440cac8":"code","bc52552c":"code","c7147226":"code","c6f711f2":"code","8837bd99":"code","121123a1":"code","a5cdfde1":"code","e2d5ec03":"code","d7e45185":"code","b39b30d7":"code","472e3e19":"code","80260823":"code","3be776fc":"code","ff5fcaa8":"code","8a7fac99":"code","3dd62a32":"code","1a9b830c":"code","8a277b66":"code","b2154dbb":"code","4ee937f6":"code","19acb39d":"code","2b2d00dc":"code","c6e5cd24":"code","5af5c9c3":"code","07250493":"code","a7343c8b":"code","006f57dc":"code","73cfb9d3":"code","3dd64373":"code","0a728569":"code","095a505f":"code","40dc2230":"code","de7a7ea0":"code","8bef15f2":"code","ba9e3fe7":"code","b3248322":"code","40433a42":"code","885fbf8a":"code","d05931cd":"code","13d40a94":"code","0aa494f5":"code","c2355fec":"code","45387f6e":"code","00f73c56":"code","e5f0877d":"code","5ef20c9a":"code","ccdb53d5":"code","2521c03c":"code","47a3362a":"code","87e5600e":"code","40f5ca9a":"code","033e638a":"code","a34fbe18":"code","d9c956c2":"code","4edebe2c":"code","0dfdacc9":"code","0edf4444":"code","7005ea04":"code","902bbebc":"code","3b3efc43":"code","ec033a91":"code","dd122fda":"code","897952f6":"code","77d00358":"code","3eeeb139":"code","ab9389fe":"code","e5edbac7":"code","aafb90b4":"code","28e80443":"code","f06beb2a":"code","8ac183bd":"code","1b69213b":"code","b1c0dda8":"code","8a92b726":"code","b173203f":"code","6542bb54":"code","465ea297":"code","ae23da57":"code","48e01f02":"code","8e0c1efa":"code","38ea21a4":"code","5ff0ee12":"code","9646070d":"markdown","e35a781b":"markdown","9233f1f0":"markdown","d6252163":"markdown","c19df879":"markdown","a093fa52":"markdown","0381cddb":"markdown","368d133a":"markdown","f8751dd8":"markdown","06c156bb":"markdown","9c24d0f1":"markdown","036413fa":"markdown","e696aee3":"markdown","139b8f66":"markdown","75c5c540":"markdown","6c09bbc0":"markdown","74d1072c":"markdown","480721bd":"markdown","6dbeccf2":"markdown","2dedf70d":"markdown","885d97f2":"markdown","a4c9bbd9":"markdown","3c5b3e26":"markdown","ae84cf9d":"markdown","970aa97e":"markdown","6483f78b":"markdown","23c6c8fd":"markdown","df3dcc93":"markdown","7e728930":"markdown","94915e00":"markdown","920e39e6":"markdown","37fd37bb":"markdown","2ddb842e":"markdown","61a4097e":"markdown","a70c950c":"markdown","f5a288ad":"markdown","4ffeec32":"markdown","5d25a192":"markdown","c1efa176":"markdown","cef2b353":"markdown","071accc4":"markdown","684ae4e1":"markdown"},"source":{"0d238223":"import pandas as pd\ntrain_data = pd.read_csv(\"..\/input\/allstate-claims-severity\/train.csv\") \ntest_data = pd.read_csv(\"..\/input\/allstate-claims-severity\/test.csv\")","17929b4d":"train_data.head()","560b84e9":"test_data.head()","5440cac8":"# Check for data types of the columns\ntrain_data.info()","bc52552c":"test_data.info()","c7147226":"# Check for null values\npd.isnull(train_data).values.any()","c6f711f2":"pd.isnull(test_data).values.any()","8837bd99":"!pip install dabl","121123a1":"%%javascript\nIPython.OutputArea.prototype._should_scroll = function() {\n    return False;\n}","a5cdfde1":"import dabl\nimport warnings\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nwarnings.filterwarnings('ignore')\ndabl.plot(train_data, target_col = 'loss')","e2d5ec03":"# View outliers for loss amount\nplt.figure(figsize=(10,5))\nplt.xlabel('Id')\nplt.ylabel('Loss Value')\nplt.title('Loss Value per Claim Id and Visualization of Outliers')\nplt.xlim([-1000, 195000])\nplt.ylim([-1000, 140000])\nplt.plot(train_data.index, train_data[\"loss\"], marker='o', markeredgecolor='k')\nplt.show()","d7e45185":"from copy import deepcopy\n\ntrain_d = train_data.drop(['id','loss'], axis=1)\ntest_d = test_data.drop(['id'], axis=1)\ntrain_d['Target'] = 1\ntest_d['Target'] = 0\n# Merge\nprep_data = pd.concat((train_d, test_d))\n\n# Label encoding for categorical features\ndata_le = deepcopy(prep_data)\n\nlist_of_cat_cols = list(train_data.select_dtypes(include=['object']).columns)\nfor c in range(len(list_of_cat_cols)):\n    data_le[list_of_cat_cols[c]] = data_le[list_of_cat_cols[c]].astype('category').cat.codes\n\n# One-hot encoding for categorical features\nprep_data = pd.get_dummies(data=prep_data, columns=list_of_cat_cols)","b39b30d7":"import numpy as np\ndata = prep_data.iloc[np.random.permutation(len(prep_data))] # Shuffle data\ndata.reset_index(drop = True, inplace = True)\n\nx = data.drop(['Target'], axis = 1)\ny = data.Target\n\nfew_examples = 50000 # Took only some data\n\nx_train = x[:few_examples]\nx_test = x[few_examples:]\ny_train = y[:few_examples]\ny_test = y[few_examples:]","472e3e19":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score as AUC\nfrom sklearn.model_selection import cross_val_score\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\nclf_lr = LogisticRegression()\nclf_lr.fit(x_train, y_train)\npred = clf_lr.predict_proba(x_test)[:,1]\nauc_lr = AUC(y_test, pred)\nprint(\"Logistic Regression ROC_AUC: {:.2%}\".format(auc_lr))\n\nclf_rf = RandomForestClassifier(n_estimators=100, n_jobs=-1)\nclf_rf.fit(x_train, y_train)\npred = clf_rf.predict_proba(x_test)[:,1]\nauc_rf = AUC(y_test, pred)\nprint(\"Random Forest ROC_AUC: {:.2%}\".format(auc_rf))\n\nscores_lr = cross_val_score(LogisticRegression(), x, y, scoring='roc_auc', cv=2)\nprint(\"Mean ROC_AUC for Logistic Regression : {:.2%}, std: {:.2%}\".format( scores_lr.mean(), scores_lr.std()))\n\nscores_rf = cross_val_score(RandomForestClassifier(n_estimators=100, n_jobs=-1), x, y, scoring='roc_auc', cv=2)\nprint(\"Mean ROC_AUC for Random Forest : {:.2%}, std: {:.2%}\".format( scores_rf.mean(), scores_rf.std()))","80260823":"from sklearn.decomposition import PCA\n\n# Shuffle\ndata_le = data_le.iloc[np.random.permutation(len(data_le))]\nX = data_le.iloc[:, :130]\ny = data_le.iloc[:, 130:]\npca = PCA(n_components=2)\nX_reduced = pca.fit_transform(X)\n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\n# Train = 1, Test = 0\nplt.figure(figsize=(16,12))\nplt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=np.array(y), \n            edgecolor='white', s=75,\n            cmap=plt.cm.get_cmap('Accent',2))\nplt.title('PCA transformed train and test sets')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.colorbar()\nplt.show()","3be776fc":"import plotly.express as px\nimport plotly.graph_objects as go\npca_3 = PCA(n_components=3)\nX_reduced_3 = pca_3.fit_transform(X)\nfig = px.scatter_3d(X_reduced_3, x=X_reduced_3[:,0], y=X_reduced_3[:, 1], z=X_reduced_3[:, 2], color=np.array(y), width=750, height=450, opacity=0.7)\nfig.update_layout(scene = dict(\n                    xaxis_title='PC1',\n                    yaxis_title='PC2',\n                    zaxis_title='PC3'))\nfig.show()","ff5fcaa8":"# Correlation for continuous variables\ncorr_mat = train_data.iloc[:,117:132].corr()","8a7fac99":"# Spot the continuous feature pairs with high correlation\nthreshold = 0.8\nhigh_corrs = (corr_mat[abs(corr_mat) > threshold][corr_mat != 1.0]) .unstack().dropna().to_dict()\nunique_high_corrs = pd.DataFrame(list(set([(tuple(sorted(key)), high_corrs[key]) for key in high_corrs])), columns=['cont_feature_pair', 'correlation_coefficient'])\nunique_high_corrs = unique_high_corrs.loc[abs(unique_high_corrs['correlation_coefficient']).argsort()[::-1]]\nunique_high_corrs","3dd62a32":"# Clustermap of correlations of continuous variables\nimport seaborn as sns\ncont_data = train_data.iloc[:,117:132]\ncont_data = cont_data.corr().abs()\nmap = sns.clustermap(cont_data, annot = True, annot_kws = {'size': 11})\nplt.setp(map.ax_heatmap.yaxis.get_majorticklabels(),rotation = 0)\nplt.show()","1a9b830c":"# Pair plot for judging the inter-relations among the continuous variables in 1st cluster\nsns.pairplot(train_data, vars=[\"cont7\", \"cont11\", \"cont12\", \"cont13\", \"cont1\", \"cont9\", \"cont6\", \"cont10\", \"loss\"])\nplt.show()","8a277b66":"# Pair plot for judging the inter-relations among the continuous variables in 2nd cluster\nsns.pairplot(train_data, vars=[\"cont4\", \"cont8\", \"cont2\", \"cont3\", \"cont5\", \"cont14\", \"loss\"])\nplt.show()","b2154dbb":"# Convert categorical features to continuous features with Label Encoding in train data\nfrom sklearn.preprocessing import LabelEncoder\nlencoders = {}\nfor col in train_data.select_dtypes(include=['object']).columns:\n    lencoders[col] = LabelEncoder()\n    train_data[col] = lencoders[col].fit_transform(train_data[col])","4ee937f6":"# Convert categorical features to continuous features with Label Encoding in test data\nfrom sklearn.preprocessing import LabelEncoder\nlencoders_2 = {}\nfor col in test_data.select_dtypes(include=['object']).columns:\n    lencoders_2[col] = LabelEncoder()\n    test_data[col] = lencoders_2[col].fit_transform(test_data[col])","19acb39d":"# Correlation for categorical variables\ncorr_mat_2 = train_data.iloc[:,1:116].corr()","2b2d00dc":"# Spot the categorical feature pairs with high correlation\nthreshold = 0.8\nhigh_corrs_2 = (corr_mat_2[abs(corr_mat_2) > threshold][corr_mat_2 != 1.0]) .unstack().dropna().to_dict()\nunique_high_corrs_2 = pd.DataFrame(list(set([(tuple(sorted(key)), high_corrs_2[key]) for key in high_corrs_2])), columns=['cat_feature_pair', 'correlation_coefficient'])\nunique_high_corrs_2 = unique_high_corrs_2.loc[abs(unique_high_corrs_2['correlation_coefficient']).argsort()[::-1]]\nunique_high_corrs_2","c6e5cd24":"# Correlation Heatmap\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"whitegrid\", {'axes.grid' : False})\nmask = np.triu(np.ones_like(corr_mat_2, dtype=np.bool))\nf, ax = plt.subplots(figsize=(20, 20))\ncmap = sns.diverging_palette(250, 15, as_cmap=True)\nsns.heatmap(corr_mat_2, mask=mask, cmap=cmap, vmax=None, center=0,square=True, annot=False, linewidths=.5, cbar_kws={\"shrink\": 0.9})","5af5c9c3":"def get_feature_importance_df(feature_importances,\n                              column_names, \n                              top_n=30):\n     \n    imp_dict = dict(zip(column_names, feature_importances))\n    \n    # get name features sorted\n    top_features = sorted(imp_dict, key=imp_dict.get, reverse=True)[0:top_n]\n    \n    # get values\n    top_importances = [imp_dict[feature] for feature in top_features]\n    \n    # create dataframe with feature_importance\n    df = pd.DataFrame(data={'feature': top_features, 'importance': top_importances})\n    return df","07250493":"import numpy as np\n\ndef get_col(df: 'dataframe', type_descr: 'numpy') -> list:\n  \n    try:\n        col = (df.describe(include=type_descr).columns)  # pandas.core.indexes.base.Index  \n    except ValueError:\n        print(f'Dataframe not contains {type_descr} columns !', end='\\n')    \n    else:\n        return col.tolist()\n    \nlist_columns = get_col(df=train_data, type_descr=[np.object, np.number])","a7343c8b":"list_columns.remove('loss')","006f57dc":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(n_estimators = train_data.shape[1], # number of component trees\n                            max_depth = 8,\n                            min_samples_leaf = train_data.shape[1],\n                            max_features = 0.2, # each tree's 20% utility in the features\n                            n_jobs = -1)","73cfb9d3":"rf.fit(train_data[list_columns], train_data['loss'])\nfeatures = train_data[list_columns].columns.values","3dd64373":"feature_importance = get_feature_importance_df(rf.feature_importances_, features)\ndisplay(feature_importance)","0a728569":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig,ax = plt.subplots()\nplt.xticks(rotation=65)\n\nfig.set_size_inches(18,10)\n#sns.set_color_codes('pastel')\nsns.barplot(data=feature_importance[:30], \n            x=\"feature\", \n            y=\"importance\", \n            edgecolor='k',\n            ax=ax)\nax.set(xlabel=\"Variable Names\",\n       ylabel='Importance',\n       title=\"Feature Importances\")","095a505f":"train_data['loss'].skew()","40dc2230":"# Target Feature: Loss\n\nsns.set_style(\"darkgrid\", {'axes.grid' : False})\nplt.figure(figsize = (8, 4))\nplt.title('Loss Severity Distribution')\nplt.xlabel('Loss Severity')\nplt.ylabel('Frequency')\ntrain_data['loss'].hist(bins=50)\nplt.tight_layout()\nxt = plt.xticks(rotation=45)\nplt.xlim([-10000,140000])\nplt.ylim([-10000,120000])\nplt.annotate('Outliers\\n present\\n till\\n this point', xy=(120000, 100), xytext=(120000, 35000), arrowprops=dict(facecolor='black'), color='black')\nplt.show()","de7a7ea0":"# Target Feature: Loss (Focus on Main Region)\n\nplt.figure(figsize = (7, 4))\nplt.title('Distribution of Loss Severity (Focus Mode: from 0 - 20,000 USD)')\nplt.xlabel('Loss Severity')\nplt.ylabel('Frequency')\ntrain_data['loss'].hist(bins=500)\nplt.tight_layout()\nxt = plt.xticks(rotation=45)\nplt.xlim([-1000,20000])\nplt.ylim([-1500,20500])\nplt.show()","8bef15f2":"# Feature Transformation Trial 1 : Apply Log on Loss\n\ntrain_data['log_loss'] = np.log(train_data['loss'])\n\nplt.figure(figsize = (7, 4))\nplt.title('Loss Severity Distribution (Log Transformation)')\nplt.xlabel('Log Loss Severity')\nplt.ylabel('Frequency')\nsns.distplot(train_data['log_loss'], kde = True, hist_kws={'alpha': 0.60})\nplt.tight_layout()\nxt = plt.xticks(rotation=0)\nplt.xlim([-1,13])\nplt.ylim([-0.01,0.5])\nplt.show()","ba9e3fe7":"# Feature Transformation Trial 2 : Apply Log on (loss+100)\n\ntrain_data['log_loss_+_100'] = np.log(100 + train_data['loss'])\n\nplt.figure(figsize = (7, 4))\nplt.title('Loss Severity Distribution (Log Transformation on Loss + 100)')\nplt.xlabel('Complex Log Loss Severity')\nplt.ylabel('Frequency')\nsns.distplot(train_data['log_loss_+_100'], kde = True, hist_kws={'alpha': 0.60}) \nplt.tight_layout()\nxt = plt.xticks(rotation=0)\nplt.ylim([-0.01,0.55])\nplt.show()","b3248322":"train_data['log_loss'].skew()","40433a42":"train_data['log_loss_+_100'].skew()","885fbf8a":"import statsmodels.api as sm\nsample = np.random.normal(0,1, 1000)\nsm.qqplot(sample, line='45')\nplt.title('Ideal QQ-Plot for Normal Distribution')\nplt.show()","d05931cd":"# Normality Test for the Target Feature\nsm.qqplot(train_data['loss'], line='45')\nplt.title('QQ-Plot for Loss Severity Distribution (Original)')\nplt.show()","13d40a94":"sm.qqplot(train_data['log_loss'], line='45')\nplt.title('QQ-Plot for Loss Severity Distribution (Log Transformation)')\nplt.show()","0aa494f5":"sm.qqplot(train_data['log_loss_+_100'], line='45')\nplt.title('QQ-Plot for Loss Severity Distribution (Log of Loss + 100)')\nplt.show()","c2355fec":"from sklearn.model_selection import train_test_split\nseed = 12345\n\n# considering only top 30 imp features\ntrainx = ['cat80', 'cat79', 'cat87', 'cat57', 'cat101', 'cat12', 'cont2', 'cat81', 'cat89', 'cont7', 'cat7', 'cat10', 'cont12', 'cont11', 'cat1', 'cat72', 'cat103', 'cat94', \n                    'cat2', 'cont3', 'cat11', 'cat106', 'cat111', 'cat114', 'cat53', 'cat13', 'cat9', 'cont6', 'cat100', 'cat44']\ntrainy = train_data.columns[-2] #considering log_loss\nX = train_data[trainx]\nY = train_data[trainy]\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=seed)","45387f6e":"import statsmodels.api as sm\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_absolute_error, make_scorer, SCORERS\n\nmodel1 = LinearRegression(n_jobs=-1)\nmae_val= make_scorer(mean_absolute_error, greater_is_better=False)\nresults1 = cross_val_score(model1, X_train, y_train, cv=5, scoring=mae_val, n_jobs=-1)\nprint(\"Linear Regression (Manual Tuning): ({0:.3f}) +\/- ({1:.3f})\".format(-1*results1.mean(), results1.std()))","00f73c56":"X2 = sm.add_constant(X)\nmodel = sm.OLS(Y, X2)\nmodel_ = model.fit()\nprint(model_.summary())","e5f0877d":"from sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_absolute_error, make_scorer, SCORERS\n\nmodel2 = Ridge(alpha=1,random_state=seed)\nmae_val= make_scorer(mean_absolute_error, greater_is_better=False)\nresults2 = cross_val_score(model2, X_train, y_train, cv=5, scoring= mae_val, n_jobs=1)\nprint(\"Linear Regression Ridge (Manual Tuning): ({0:.3f}) +\/- ({1:.3f})\".format(results2.mean(), results2.std()))","5ef20c9a":"import time\n\nstart_time = time.perf_counter()\nclf = Ridge()\ncoefs = []\n# this alpha parameter of scikit-learn's Ridge actually corresponds to our lambda\nalphas = np.logspace(-6, 9, 200)\n\nfor a in alphas:\n    clf.set_params(alpha=a)\n    clf.fit(X2, Y)\n    coefs.append(clf.coef_)\n\nplt.figure(figsize=(18, 6))\nplt.subplot(121)\nax = plt.gca()\nax.plot(alphas, coefs)\nax.set_xscale('log')\nplt.xlabel('alpha')\nplt.ylabel('beta coefficients')\nplt.title('Ridge coefficients (\u03b2) as a function of regularization parameter (\u03b1)')\nplt.axis('tight')\nplt.annotate('All coefficients are shrinked to zero at this point \\nfor Lasso (see proof later)', \nxy=(1, -0.5), xytext=(1, -0.3), arrowprops=dict(facecolor='black'), color='black')\nplt.grid(color='black', linestyle='dotted')\nplt.show()\n\nend_time = time.perf_counter()\nprint(\"Total Run Time:\")\nprint(end_time - start_time, \"Seconds\")","ccdb53d5":"start_time = time.perf_counter()\n\nclf = Ridge()\nerror = []\nalphas = np.logspace(-6, 9, 200)\n\nfor a in alphas:\n    clf.set_params(alpha=a)\n    mae_val = make_scorer(mean_absolute_error, greater_is_better=False)\n    error.append(cross_val_score(clf, X2, Y, cv=5, scoring=mae_val, n_jobs=1).mean())\n\nplt.figure(figsize=(18, 6))\n\nplt.subplot(121)\nax = plt.gca()\nax.plot(alphas, error)\nax.set_xscale('log')\nplt.xlabel('alpha')\nplt.ylabel('mean absolute error')\nplt.title('Mean absolute error as a function of regularization parameter (\u03b1)')\nplt.axis('tight')\n\nplt.annotate('Lasso is done at this point \\nand all coefficients would be \\nshrinked to zero (see proof later)', \nxy=(1, -0.65), xytext=(1, -0.62), arrowprops=dict(facecolor='black'), color='black')\nplt.annotate('', \nxy=(1000, -0.48), xytext=(1000, -0.51), arrowprops=dict(facecolor='black'), color='black')\nplt.grid(color='black', linestyle='dotted')\nplt.ylim([-0.68,-0.45])\nplt.show()\n\nend_time = time.perf_counter()\nprint(\"Total Run Time:\")\nprint(end_time - start_time, \"Seconds\")","2521c03c":"from sklearn.linear_model import Lasso\nmodel3 = Lasso(alpha=0.0001,random_state=seed)\nmae_val = make_scorer(mean_absolute_error, greater_is_better=False)\nresults3 = cross_val_score(model3, X_train, y_train, cv=5, scoring=mae_val, n_jobs=1)\nprint(\"Linear Regression Lasso (Manual Tuning): ({0:.3f}) +\/- ({1:.3f})\".format(results3.mean(), results3.std()))","47a3362a":"start_time = time.perf_counter()\n\n#clf = Lasso(tol=0.00001, max_iter=10000)\nclf = Lasso()\ncoefs = []\nalphas = np.logspace(-6, 2, 200)\n\nfor a in alphas:\n    clf.set_params(alpha=a)\n    clf.fit(X2, Y)\n    coefs.append(clf.coef_)\n\nplt.figure(figsize=(18, 6))\nplt.subplot(121)\nax = plt.gca()\nax.plot(alphas, coefs)\nax.set_xscale('log')\nplt.xlabel('alpha')\nplt.ylabel('coefficients')\nplt.title('Lasso coefficients (\u03b2) as a function of regularization parameter (\u03b1)')\nplt.axis('tight')\n\nplt.annotate('see proof now', \nxy=(1, 0.1), xytext=(1, 0.3), arrowprops=dict(facecolor='black'), color='black')\nplt.grid(color='black', linestyle='dotted')\nplt.show()\n\nend_time = time.perf_counter()\nprint(\"Total Run Time:\")\nprint(end_time - start_time, \"Seconds\")","87e5600e":"start_time = time.perf_counter()\n\nclf = Lasso()\nerror = []\nalphas = np.logspace(-6, 9, 200)\n\nfor a in alphas:\n    clf.set_params(alpha=a)\n    mae_val = make_scorer(mean_absolute_error, greater_is_better=False)\n    error.append(cross_val_score(clf, X2, Y, cv=5, scoring=mae_val, n_jobs=1).mean())\n\nplt.figure(figsize=(18, 6))\n\nplt.subplot(121)\nax = plt.gca()\nax.plot(alphas, error)\nax.set_xscale('log')\nplt.xlabel('alpha')\nplt.ylabel('mean absolute error')\nplt.title('Mean absolute error as a function of the regularization parameter (\u03b1)')\nplt.axis('tight')\n\nplt.annotate('Lasso is done at this point \\nand all weights are \\nshrinked to zero', \nxy=(1, -0.60), xytext=(1, -0.58), arrowprops=dict(facecolor='black'), color='black')\nplt.annotate('Ridge still perform good at this point', \nxy=(0.001, -0.49), xytext=(0.001, -0.51), arrowprops=dict(facecolor='black'), color='black')\nplt.grid(color='black', linestyle='dotted')\nplt.ylim([-0.68,-0.44])\nplt.show()\n\nend_time = time.perf_counter()\nprint(\"Total Run Time:\")\nprint(end_time - start_time, \"Seconds\")","40f5ca9a":"from sklearn.linear_model import ElasticNet\nmodel4 = ElasticNet(alpha=0.0001,l1_ratio=0.5,random_state=seed)\nmae_val = make_scorer(mean_absolute_error, greater_is_better=False)\nresults4 = cross_val_score(model4, X_train, y_train, cv=5, scoring=mae_val, n_jobs=1)\nprint(\"Linear Regression Elastic Net (Manual Tuning): ({0:.3f}) +\/- ({1:.3f})\".format(results4.mean(), results4.std()))","033e638a":"lasso = Lasso(alpha=0.0001)\nlasso.fit(X_train, y_train)\nridge = Ridge(alpha=1,random_state=seed)\nridge.fit(X_train, y_train)\nlinear = LinearRegression()\nlinear.fit(X_train, y_train)\nenet = ElasticNet(alpha=0.0001, l1_ratio=0.5)\nenet.fit(X_train, y_train)\n\nplt.figure(figsize = (16, 6))\nplt.plot(enet.coef_, color='red', linewidth=0.5, marker='^', label='Elastic net coefficients with \u03b1 = 0.0001 & L1 Ratio = 0.5')\nplt.plot(lasso.coef_, color='black', linewidth=0.5, marker='s', label='Lasso coefficients with \u03b1 = 0.0001')\nplt.plot(ridge.coef_, color='green', linewidth=0.5, marker='x', label='Ridge coefficients with \u03b1 = 1')\nplt.plot(linear.coef_, color='blue', linewidth=0.5, marker='o', label='Linear coefficients without regularization')\nplt.grid(color='black', linestyle='dotted')\nplt.ylim([-0.6,0.7])\nplt.xlim([-1,30])\nplt.legend(loc='best')\nplt.title('Coefficient Distribution According to the Linear Regression type')\nplt.xlabel('Variables in Ranked Order')\nplt.ylabel('Estimated Coefficient Value')\nplt.show()","a34fbe18":"from yellowbrick.regressor import ResidualsPlot\nviz_r = ResidualsPlot(ridge)\nviz_r.fit(X_train, y_train) \nviz_r.score(X_test, y_test)  \nviz_r.show()  ","d9c956c2":"viz_l = ResidualsPlot(lasso)\nviz_l.fit(X_train, y_train)  \nviz_l.score(X_test, y_test)  \nviz_l.show()                 ","4edebe2c":"viz_l = ResidualsPlot(Lasso(alpha=0.1))\nviz_l.fit(X_train, y_train)  \nviz_l.score(X_test, y_test)  \nviz_l.show() ","0dfdacc9":"viz_e = ResidualsPlot(enet)\nviz_e.fit(X_train, y_train)  \nviz_e.score(X_test, y_test)  \nviz_e.show()","0edf4444":"from yellowbrick.regressor import PredictionError\nviz_pe = PredictionError(enet)\nviz_pe.fit(X_train, y_train)  \nviz_pe.score(X_test, y_test)  \nviz_pe.show() ","7005ea04":"from sklearn.ensemble import RandomForestRegressor\nstart_time = time.perf_counter()\n\nmodel5 = RandomForestRegressor(n_jobs=-1,n_estimators=300, max_features=12, random_state=seed)\nmae_val = make_scorer(mean_absolute_error, greater_is_better=False)\nresults5 = cross_val_score(model5, X_train, y_train, cv=5, scoring=mae_val, n_jobs=1)\nprint(\"Random Forest Regressor (Manual Tuning): ({0:.10f}) +\/- ({1:.3f})\".format(-1*results5.mean(), results5.std()))\n\nend_time = time.perf_counter()\nprint(\"Total Run Time:\")\nprint(end_time - start_time, \"Seconds\")","902bbebc":"start_time = time.perf_counter()\n\n# let's check forest performance with different no. of trees (n_estimators)\nh = [1, 2, 5, 10, 100, 500, 1000]\nscores = []\n\nfor val in h:\n    model = RandomForestRegressor(n_jobs=-1,n_estimators=val, max_features=12, random_state=seed)\n    mae_val = make_scorer(mean_absolute_error, greater_is_better=False)\n    results = cross_val_score(model, X_train, y_train, cv=2, scoring=mae_val, n_jobs=1)\n    scores.append(-1*results.mean())\n        \nend_time = time.perf_counter()\nprint(\"Total Run Time:\")\nprint(end_time - start_time, \"Seconds\")","3b3efc43":"scores","ec033a91":"import pandas as pd\ndf = pd.DataFrame(list(zip(h, scores)), columns=['n_estimators','cross_val_scores'])\nax = sns.lineplot(x=\"n_estimators\", y=\"cross_val_scores\", data=df)","dd122fda":"from sklearn.ensemble import RandomForestRegressor\nmodel_1n = RandomForestRegressor(n_jobs=-1,n_estimators=1, max_features=12, random_state=seed)\nmodel_1n.fit(X_train, y_train)","897952f6":"model_10n = RandomForestRegressor(n_jobs=-1,n_estimators=10, max_features=12, random_state=seed)\nmodel_10n.fit(X_train, y_train)","77d00358":"from sklearn.tree import _tree\ndef leaf__depths(estimator, nodeid = 0):\n     left__child = estimator.children_left[nodeid]\n     right__child = estimator.children_right[nodeid]\n     \n     if left__child == _tree.TREE_LEAF:\n         depths = np.array([0])\n     else:\n         left__depths = leaf__depths(estimator, left__child) + 1\n         right__depths = leaf__depths(estimator, right__child) + 1\n         depths = np.append(left__depths, right__depths)\n \n     return depths","3eeeb139":"def leaf__samples(estimator, nodeid = 0):  \n     left__child = estimator.children_left[nodeid]\n     right__child = estimator.children_right[nodeid]\n\n     if left__child == _tree.TREE_LEAF: \n         samples = np.array([estimator.n_node_samples[nodeid]])\n     else:\n         left__samples = leaf__samples(estimator, left__child)\n         right__samples = leaf__samples(estimator, right__child)\n         samples = np.append(left__samples, right__samples)\n\n     return samples","ab9389fe":"\ndef visualization__estimator(ensemble, tree_id=0):\n\n     plt.figure(figsize=(8,8))\n     plt.subplot(211)\n\n     estimator = ensemble.estimators_[tree_id].tree_\n     depths = leaf__depths(estimator)\n     \n     plt.hist(depths, histtype='step', bins=range(min(depths), max(depths)+1))\n     plt.grid(color='black', linestyle='dotted')\n     plt.xlabel(\"Depth of leaf nodes (tree %s)\" % tree_id)\n     plt.show()","e5edbac7":"def visualization__forest(ensemble):\n\n     plt.figure(figsize=(8,8))\n     plt.subplot(211)\n\n     depths__all = np.array([], dtype=int)\n\n     for x in ensemble.estimators_:\n         estimator = x.tree_\n         depths = leaf__depths(estimator)\n         depths__all = np.append(depths__all, depths)\n         plt.hist(depths, histtype='step', bins=range(min(depths), max(depths)+1))\n\n     plt.hist(depths__all, histtype='step',\n              bins=range(min(depths__all), max(depths__all)+1), \n              weights=np.ones(len(depths__all))\/len(ensemble.estimators_), \n              linewidth=2)\n     plt.grid(color='black', linestyle='dotted')\n     plt.xlabel(\"Depth of leaf nodes\")\n    \n     plt.show()","aafb90b4":"start_time = time.perf_counter()\n\nvisualization__estimator(model_1n)\n\nend_time = time.perf_counter()\nprint(\"Total Run Time:\")\nprint(end_time - start_time, \"Seconds\")","28e80443":"start_time = time.perf_counter()\n\nvisualization__estimator(model_10n)\n\nend_time = time.perf_counter()\nprint(\"Total Run Time:\")\nprint(end_time - start_time, \"Seconds\")","f06beb2a":"start_time = time.perf_counter()\n\nvisualization__forest(model_1n)\n\nend_time = time.perf_counter()\nprint(\"Total Run Time:\")\nprint(end_time - start_time, \"Seconds\")","8ac183bd":"start_time = time.perf_counter()\n\nvisualization__forest(model_10n)\n\nend_time = time.perf_counter()\nprint(\"Total Run Time:\")\nprint(end_time - start_time, \"Seconds\")","1b69213b":"model_1n_md = RandomForestRegressor(n_jobs=-1,n_estimators=1, max_features=12, max_depth=20, random_state=seed)\nmodel_1n_md.fit(X_train, y_train)","b1c0dda8":"model_10n_md = RandomForestRegressor(n_jobs=-1,n_estimators=10, max_features=12, max_depth=20, random_state=seed)\nmodel_10n_md.fit(X_train, y_train)","8a92b726":"start_time = time.perf_counter()\n\nvisualization__forest(model_1n_md)\n\nend_time = time.perf_counter()\nprint(\"Total Run Time:\")\nprint(end_time - start_time, \"Seconds\")","b173203f":"start_time = time.perf_counter()\n\nvisualization__forest(model_10n_md)\n\nend_time = time.perf_counter()\nprint(\"Total Run Time:\")\nprint(end_time - start_time, \"Seconds\")","6542bb54":"model_1n_msl = RandomForestRegressor(n_jobs=-1,n_estimators=1, max_features=12, min_samples_leaf=6, max_depth=20, random_state=seed)\nmodel_1n_msl.fit(X_train, y_train)","465ea297":"model_10n_msl = RandomForestRegressor(n_jobs=-1,n_estimators=10, max_features=12, min_samples_leaf=6, max_depth=20, random_state=seed)\nmodel_10n_msl.fit(X_train, y_train)","ae23da57":"start_time = time.perf_counter()\n\nvisualization__forest(model_1n_msl)\n\nend_time = time.perf_counter()\nprint(\"Total Run Time:\")\nprint(end_time - start_time, \"Seconds\")","48e01f02":"start_time = time.perf_counter()\n\nvisualization__forest(model_10n_msl)\n\nend_time = time.perf_counter()\nprint(\"Total Run Time:\")\nprint(end_time - start_time, \"Seconds\")","8e0c1efa":"pred=test_data.drop('id',axis=1)\ntest_data_ = test_data[['cat80', 'cat79', 'cat87', 'cat57', 'cat101', 'cat12', 'cont2', 'cat81', 'cat89', 'cont7', 'cat7', 'cat10', 'cont12', 'cont11', 'cat1', 'cat72', 'cat103', 'cat94', \n                       'cat2', 'cont3', 'cat11', 'cat106', 'cat111', 'cat114', 'cat53', 'cat13', 'cat9', 'cont6', 'cat100', 'cat44']]\nyp=model_1n_md.predict(test_data_)","38ea21a4":"submission = pd.read_csv('..\/input\/allstate-claims-severity\/sample_submission.csv')\nsubmission['loss'] = yp\nsubmission.head()","5ff0ee12":"submission.to_csv('AllstateClaimsPred.csv', index=False)\nprint(\"Submission successful\")","9646070d":"We are using two classifiers: Logistic Regression and Random Forest to check if they are able enough to separate train and test data points. ","e35a781b":"So, log(loss) has given lower skewness as compared to skewness given by log(loss+100). Let's draw **QQ-plots** now to check which one is complying with normality better. A **Q-Q plot** is a scatterplot created by plotting two sets of quantiles against one another. If both sets of quantiles came from the same distribution, we should see the points forming a line that\u2019s roughly straight like below. (Ref: [University of Virginia Library](https:\/\/data.library.virginia.edu\/understanding-q-q-plots\/#:~:text=A%20Q%2DQ%20plot%20is%20a,truly%20come%20from%20Normal%20distributions.))","9233f1f0":"So, the highest correlation is among cont11 and cont12. There is no perfect correlation of 1 between any pair. We will check correlations plotting a cluster heatmap as well. The motto is to identify the cluster of continuous features having similar trend of correlation among each other.","d6252163":"* No significant trend is observed in above pair plot.\n\nNext, we will convert categorical variables to continuous ones.","c19df879":"Well, both of these feature transformation trials have proved to take symmetrical bell-shaped curve now. Let's check the skewness of transformed target variables.","a093fa52":"# Feature Importance :\n\nAmong 130 features, we will check which ones are having dominant contribution towards \"loss\". We have chosen top 30 features here. You can try with a bigger number.","0381cddb":"In previous figures, we mentioned about visualizing proof later. Now, we can see that at alpha = 1, the coefficients are skrinking down to exact zero for Lasso. ","368d133a":"By observation, it is evident that the cross val score is increasing as we are decreasing the no. of trees (n_estimators). There is almost no improvement when no. of estimators are crossing 100. So, ideally we should use n_estimators = 1 to get the best result. Now, there comes a quick question. **Is random forest with one tree same as a decision tree model?** \n\nThe answer is **\"No\"**. I would request the curious kagglers to go through [desertnaut's answer on this topic in Stack Overflow](https:\/\/stackoverflow.com\/questions\/48239242\/why-is-random-forest-with-a-single-tree-much-better-than-a-decision-tree-classif) for finding out the reasons behind the answer being a \"no\".","f8751dd8":"We can spot the furthest outlier with loss value around 1,20,000 USD.","06c156bb":"**Observation :** We see a stepwise increase (evenly incremented) in depth of leaf nodes. However, the increase is exceptionally steep while moving from depth of leaf nodes '19' to '20'.\n\nNext, we will introduce min_samples_leaf parameter and observe the plots further.","9c24d0f1":"# Modelling with Linear Regression :","036413fa":"**Acknowledgement :** For plotting Ridge and Lasso coefficient as a function of alpha, I consulted [Jerome Blanchet's kernel](https:\/\/www.kaggle.com\/jeromeblanchet\/ridge-lasso-coefficients-as-a-function-of-alpha). All thanks to him for sharing his work.\n\n# Elastic Net Regression:\nElastic Net is a mix of Ridge and Lasso. If we put l1_ratio = 0,  the penalty is 100% L2 penalty (Ridge Regression). Else if we take l1_ratio = 1, it is 100% L1 penalty (Lasso Regression). For any value l1_ratio = x where 0 < x < 1, the penalty is a combination of L1 (x%) and L2 (100-x)%.","e696aee3":"# Ridge Regression (L2 Penalty) :\n\nRidge is a way to regularize regression to curb overfitting. To decrease model complexity, number of features get reduced by penalizing some of the redundant features' sum of squared coefficients (\u03b2-coefficients) almost nearing to zero (never exactly zero). Ideally, when we use \u03bb (regularization penalty) tends to infinity, \u03b2 tends to \"0\". Here, we are using scikit-learn library's Ridge regression with regularization parameter (\u03b1) = 1. ","139b8f66":"The prediction error plot shows the actual targets from the dataset against the predicted values generated by the model (Elastic Net). This allows us to see how much variance is present in the model. The variance can be diagnosed by comparing the best-fit line against the 45 degree diagonal. The 45 degree diagonal line denotes the case when actual targets exactly match the predicted values generated by model. (Ref: [Scikit-YB site](https:\/\/www.scikit-yb.org\/en\/latest\/api\/regressor\/peplot.html)). In this case, there is substantial amount of variance is present in the model.\n\n\nLet us try a non-linear tree-based algorithm next. We will try only Random Forest here.","75c5c540":"Yeah, it is very skewed indeed! ('3' being very high skewness as compared to desired skewness '0')","6c09bbc0":"# Random Forest\n","74d1072c":"Visibly, two major clusters are there : \n1. One with very high correlations among other {cont7, cont11, cont12, cont13, cont1, cont9, cont6, cont10}. \n2. The other is with weak correlations among each other {cont4, cont8, cont2, cont3, cont5, cont14}. The target variable \"loss\" is not very highly correlated with any continuous variable.","480721bd":"So we see that mainly categorical variables have high feature importance. Only a few continous variables like cont2, cont7, cont12, cont11 have high feature importance.  \n\n# Feature Transformation \n\nLet's check skewness of the target variable. Highly skewed variable is not desired in the training model since it violates normality. For this purpose, we will experiment with simple application of log transformation on target variable \"loss\" and application of log on (loss+100). Later, we will check normality compliance with the QQ-plot. ","6dbeccf2":"See, the trend of pattern followed by train and test data points are similar.\n\n* **Projection into 3-Dimensional PCA :**\n\nWe can also try decomposing the features to 3 principal components and view the 3D projection plot.","2dedf70d":"* Steep linear relationship is observed between: (cont11, cont 12)\n* Linear relationship is observed between: (cont1, cont9)","885d97f2":"The results above shows ROC-AUC score is just a marginal 50%. So the classifiers are not able to clearly separate training and testing data points. So we can conclude that the training and testing data points are homogenous and there is low possibility of model-overfitting.\n\n* **Projection into 2-Dimensional PCA :**\n\nNext, we will also visualize both data points using PCA. First, we are decomposing the features to 2 principal components and view the 2D projection plot.","a4c9bbd9":"Well, here also the highest correlation we observe is around 0.9557. No pair is having a value of 1. Let's visualize it.","3c5b3e26":"Increasing '\u03b1' worsens the performance of Lasso. Hence, we are sticking to lower \u03b1-value which we used earlier. Let's check the plot for Elastic Net with \u03b1 = 0.0001 and L1 ratio = 0.5","ae84cf9d":"We can see variation in ranking of variables (for estimated coefficient values) in range 10-15 only. For other range, the graphs are pretty similar coinciding the points. The fluctuation in coefficient value is the highest for blue curve i.e. for linear coefficients without regularization which is obvious. \n\nLet's try plotting residuals for Ridge, Lasso and Elastic Net models we used here.","970aa97e":"The yellow points (train) and blue points (test) are so closely mashed together. Evidently, it is difficult to separate them. Hence, we can conclude both train and test data points are coming from the same sample, not from different samples, and there is a minimal possibility of overfitting.\n\n# Correlation Analysis :\n\nWe will now proceed to perform correlation analysis among the continuous variables to see if there is multicollinearity i.e. perfect correlation of 1 between any feature pair.","6483f78b":"For the Ridge (\u03b1=1) and Lasso (\u03b1=0.0001) models we used, the residual plots show similar strength for both of them. Both the models have similar \\\\( R^2 \\\\) value for train and test data sets i.e. around 45%. This is to recall that we also got a similar \\\\( R^2 \\\\)  value using OLS regression. We can experiment with trying a higher \u03b1-value for Lasso.","23c6c8fd":"We observed similar performance here as well. \n\n**Common Observation from all Residual Plots :** The main purpose of viewing residuals plot is to analyze the variance of the error of the regressor. Here, the points are not randomly dispersed around the horizontal axis. This gives an indication that a linear regression model is not appropriate for the data. A non-linear model (perhaps tree-based model) is more appropriate. \n\nBefore trying a tree-based model, we will check prediction error plot for regression.","df3dcc93":"# Initial Data Exploration : \n\nWe will do initial data exploration using Data Analysis Baseline Library.","7e728930":"# Adversarial Validation :\n\nIt is important to compare overall distribution of train and test set. For improving the quality of prediction on test set, it is required to ensure that both data sets follow almost similar distribution. To check this, we will remove target labels from train set and merge it with test set to make a big data set. Then we will label the train data points with \"1\" and test data points with \"0\". Afterwards, we will apply ML algorithms to check ROC_AUC metric. If ROC_AUC metric is much higher than 50% (preferably above 60%) then we can conclude that there is a significant different between train and test data distribution. This procedure is known as ***Adversarial Validation***.\n\n**Acknowledgement : [YouTube video by Zak Jost](https:\/\/www.youtube.com\/watch?v=7cUCDRaIZ7I)**","94915e00":"In this notebook, I tried to go in-depth with understanding ML models (one linear model **Regression** and one tree based model **Random Forest** to be specific). I have kept the motto to understand different parameters\/ hyperparameters used by the models, so the focus is not on earning a high score here. Keeping this objective in mind, I avoided using XGBoost or LightGBM (which are otherwise popular models for getting high score).\n\nKey takeaways:\n* **Adversarial Validation** - A diagnostic way to check the similarity between Training and Test data sets\n* **Feature Transformation** - Log transformation to reduce variable skewness and test normality with QQ-plot\n* **Regularizations (L1 and L2 Penalties)** - Effect of regularization parameter on Lasso and Ridge coefficients\n* **Residuals Plot and Prediction Error Plot** - Regression model's performance analysis visually\n* **Visualize Random Forest** - Visual change in model plot w.r.t. change in different hyperparameters viz. n_estimators, max_depth, min_samples_leaf.","920e39e6":"So, Ordinary Least Square Regression is giving \\\\( R^2 \\\\) value around 45% which is actually a weak value for the model. Also, high AIC and BIC values do not indicate a good fit.","37fd37bb":"So, we have got the list of top 30 important features. Let's visualize it.","2ddb842e":"Well, here also we observe, QQ-plot for **simple log transformation of loss** is parallel to the diagonal red line. So we will work with 'log_loss' as transformed target variable for training purpose. Next, we proceed for splitting data set into training and testing.\n\n# Train-Test Split :","61a4097e":"Let's take a look at the results!","a70c950c":"**Observation :** While visualizing forest, we can see 10 curves are overlapping for the model with n_estimators = 10. There is variation of depth of leaf nodes for each estimator in the middle-placed bins. \n\nNow, we will introduce max_depth parameter in the model and check the plots.","f5a288ad":"Now we will be noticing the level of randomness in forests made of 1 tree and made of 10 trees respectively.\n\n**Acknowledgement :** I have taken the functions of Random Forest visualization from [Aysen Tatarinov's work](https:\/\/github.com\/aysent\/random-forest-leaf-visualization). He has written codes in detail to visualize forests made of many trees on a simple graph.","4ffeec32":"**Description of Data :**\n* \"id\": Unique identifier of an insurance claim lodged \n* \"cat1\" to \"cat116\": Categorical variables (masked)\n* \"cont1\" to \"cont14\": Continuous variables (masked)\n* \"loss\": The amount of claim the company has to pay out. This is the target variable present in training data.\n\nNow, we will check datatypes of columns and if there is any missing value (null) present.","5d25a192":"# Lasso Regression (L1 Penalty) :\nLasso is another way to curb overfitting. It also adds a penalty for non-zero \u03b2-coefficients. Unlike Ridge regression which penalizes sum of squared coefficients, lasso penalizes the sum of the absolute values of \u03b2-coefficients. Hence, for very high values of \u03bb, many \u03b2-coefficients are not only shrinked but exactly zeroed under Lasso. Here, we are trying Lasso Regression with regularization parameter (\u03b1) = 0.0001","c1efa176":"**Synopsis of DABL Plots :**\n\n1. Claim amounts mostly hover below 20,000 USD\n2. Among continuous features: cont2, cont7, cont3, cont11, cont12, cont6, cont4, cont8, cont10, cont14, cont9, cont5, cont1, cont13 are contributing dominantly towards the loss amount. Eg: when cont2 is having a value between 0.6 and 0.8, the loss amount is high; when cont14 is having a value between 0.2 and 0.4 or 0.6 and 0.8, the loss amount is high.\n3. Among categorical features: cat1, cat2, cat3, cat4, cat5, cat6, cat8, cat9, cat10, cat11 are mainly contributing towards the loss amount. Eg: when cat1 is having value 'A', the loss amount is high. Also, when cat3 is having value 'A', the loss amount is high.\n\nNext, we will visualize loss amount per claim id. This plot will also highlight the outlier claim amounts.","cef2b353":"**Observation :** We see almost no difference in both the plots. This is perhaps the difference in no. of estimators is not huge. The gap between 1 and 10 is quite small. The times taken for execution are also very close. ","071accc4":"**Observation :** We can see the increase in depth has become non-evenly incremented on introducing min_samples_leaf. Sometimes the curve is decreasing as well (e.g.: for the forest with one tree, there is a decrement observed while moving from depth '18' to '19'). This is absolutely logical, because min_samples_leaf denote the no. of leaves to be there in each leaf node. Keeping it as high a number as '6' will put a constraint on fixing the depth of tree, and the curve will fluctuate. A low no. of min_samples_leaf is desired. We will keep it default. \n\n**Final Model :** We will choose \"model_1n_md\" since it has produced the best plot.","684ae4e1":"We started checking with a forest model made of 300 trees. It performed worse than linear regression. The cross val score is 0.45 which is lesser than 0.47 i.e. the cross val score we observed with different regression models. Hence, we should proceed with trying different no. of trees (optimal no. of trees) to check when the forest performs better."}}