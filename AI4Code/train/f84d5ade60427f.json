{"cell_type":{"27d045dc":"code","c10ac5ef":"code","4db1007c":"code","68a6d9ec":"code","a3757159":"code","150293d2":"code","5e7f9dde":"code","4dd430ed":"code","ed144e83":"code","3b9c343b":"code","bad6015a":"code","ebaf36bc":"code","8ef37841":"code","4282ea98":"code","fef64a1f":"code","fd7d9ab2":"code","f69341dc":"code","235571e7":"code","777e398b":"code","0cdc9c32":"code","19853dd7":"code","367e6a1e":"code","eaa31859":"code","a113ffee":"code","919a8e82":"code","f9f5e9d8":"code","919f5b9e":"code","bc7b863a":"code","38161db0":"code","a970ad03":"code","406655da":"code","af1c505c":"code","66f7bd9d":"code","21c0f379":"code","387a56ed":"code","b0705860":"code","7e2a8ee7":"code","9bb46c60":"code","4c0a48a5":"code","78caef69":"code","d622a481":"code","f6f8b0bb":"markdown","5f7cfd14":"markdown","ab9204eb":"markdown","d67040d8":"markdown","f36d75f2":"markdown","18896e04":"markdown","7bd4dfbb":"markdown","0fadc3b1":"markdown","0c89e5d3":"markdown","fdccf3a1":"markdown","e7f36dd9":"markdown","0321f453":"markdown","43cd374d":"markdown","e8a645a6":"markdown","3ad3b0c0":"markdown","beff30dc":"markdown","06a2c89b":"markdown","bc9f2749":"markdown","98e849c5":"markdown","0aa890f8":"markdown","eea16768":"markdown","b2aff7ac":"markdown","c28b38a9":"markdown","a86cfeff":"markdown","d1828d91":"markdown","407aaca4":"markdown","948692f7":"markdown","8425e532":"markdown","ae3cbfae":"markdown","1bd2c664":"markdown","f8a70e21":"markdown","a75f6595":"markdown","949789ef":"markdown","bfbcb3b7":"markdown","ca4010e2":"markdown","ab24d10d":"markdown","314ca042":"markdown"},"source":{"27d045dc":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n#read csv file        \ndf = pd.read_csv(\"..\/input\/retail-store-sales-transactions\/scanner_data.csv\")        ","c10ac5ef":"df.head()","4db1007c":"df.dtypes","68a6d9ec":"#checking for missing data\ndf.isnull().sum()","a3757159":"#Turn date column into datetime and create new features\ndf[\"Date\"] = pd.to_datetime(df[\"Date\"])\ndf[\"Day of Week\"] = df[\"Date\"].dt.dayofweek\ndf[\"Month\"] = df[\"Date\"].dt.month\n\ndf[\"Month Truncated\"] = df[\"Date\"] + pd.offsets.MonthBegin(-1)\ndf[\"Month Truncated\"] = df[\"Month Truncated\"].dt.date\n\n#Create a price parameter\ndf[\"Price\"] = df[\"Sales_Amount\"]\/df[\"Quantity\"]","150293d2":"df.head()","5e7f9dde":"fig, ax = plt.subplots(1, 1, figsize = (16,8))\n\ndf.groupby(\"Date\")[\"Sales_Amount\"].sum().plot(ax = ax)\nax.set_title(\"Sales Across Time\", fontsize = 14);","4dd430ed":"#add in monthly sales data as well\nsales_month = df.groupby([\"Month Truncated\"])[\"Sales_Amount\"].sum().reset_index()\nsales_date = df.groupby([\"Date\", \"Day of Week\"])[\"Sales_Amount\"].sum().reset_index()\n\ndow_labels = [\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"]\nmonth = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n\nfig, (ax1, ax2) = plt.subplots(ncols = 2, figsize = (10, 5))\n\nsales_month.plot(ax = ax1, rot = 90) #sum has been used to check for the sales since the transactions only showed for year 2017\nsales_date.groupby([\"Day of Week\"])[\"Sales_Amount\"].mean().plot(ax = ax2, rot = 90) \n\nax1.set_title(\"Month\", fontsize = 12)\nax2.set_title(\"Day of Week\", fontsize = 12)\n\nax1.xaxis.set_major_locator(mdates.MonthLocator(interval=1))\nax1.set_xticklabels(month)\nax1.set_xlabel(\"\")\n\nax2.set_xticks(range(0,7))\nax2.set_xticklabels(dow_labels)\nax2.set_xlabel(\"\")\n\nplt.suptitle(\"Average Sales\", fontsize = 15);","ed144e83":"total_prod = df[\"SKU\"].nunique()\ntotal_prod_cat = df[\"SKU_Category\"].nunique()\n\nprint(\"There are a total of {} products and {} product categories\".format(total_prod, total_prod_cat))","3b9c343b":"products = df[\"SKU\"].value_counts().reset_index()\nproducts.columns = [\"SKU\", \"Items Sold\"]\n\nproduct_cat = df[\"SKU_Category\"].value_counts().reset_index()\nproduct_cat.columns = [\"SKU_Category\", \"Items Sold\"]","bad6015a":"fig, (ax1, ax2) = plt.subplots(ncols = 2, figsize = (10, 5))\n\nproducts[\"Items Sold\"].plot(kind = \"kde\", rot = 90, ax = ax1)\nax1.set_xlim([0,200])\nax1.locator_params(axis=\"x\", nbins= 20)\nax1.set_title(\"Products\")\n\nproduct_cat[\"Items Sold\"].plot(kind = \"kde\", rot = 90, ax = ax2)\nax2.set_xlim([0,8000])\nax2.locator_params(axis=\"x\", nbins= 20)\nax2.set_title(\"Product Category\")\n\nfig.suptitle(\"Distribution of Items Sold\", fontsize = 15)\nplt.tight_layout()","ebaf36bc":"fig, (ax1, ax2) = plt.subplots(ncols =2, nrows = 1, figsize = (10, 5))\n\ndf[\"SKU_Category\"].value_counts().head(10).plot(kind = \"bar\", ax = ax1)\ndf[\"SKU\"].value_counts().head(10).plot(kind = \"bar\", ax = ax2)\n\nax1.set_title(\"By SKU Category\", fontsize = 12)\nax2.set_title(\"By SKU\", fontsize = 12)\n\nax1.set_xlabel(\"\")\nax2.set_xlabel(\"\")\n\nfig.suptitle(\"Top 10 Best Selling Items\", fontsize =  18, y = 1.03);","8ef37841":"median_prod = products[\"Items Sold\"].median()\nmedian_prod_cat = product_cat[\"Items Sold\"].median()\n\nprint(\"There are {} items, about {:.2f}% ,that have been sold less than {} times. For product categories there are over {} groups, {:.2f}% of total categories,that have only sold less than {} times.\"\n      .format(products[products[\"Items Sold\"] <= median_prod].shape[0], \n              products[products[\"Items Sold\"] <= median_prod].shape[0]\/total_prod*100,\n              median_prod,\n              product_cat[product_cat[\"Items Sold\"] <= median_prod_cat].shape[0],\n              product_cat[product_cat[\"Items Sold\"] <= median_prod_cat].shape[0]\/total_prod_cat*100,\n              median_prod_cat))","4282ea98":"low_selling_items = df[df[\"SKU\"].isin(products[products[\"Items Sold\"] <= median_prod][\"SKU\"])][\"Sales_Amount\"].sum()\/df[\"Sales_Amount\"].sum()*100\nlow_selling_product_cat = df[df[\"SKU_Category\"].isin(product_cat[product_cat[\"Items Sold\"] <= median_prod_cat][\"SKU_Category\"])][\"Sales_Amount\"].sum()\/df[\"Sales_Amount\"].sum()*100\n\nprint(\"SKUs that have sold less than 7 times have contributed to about {:.2f}% of the total sales.\".format(low_selling_items))\nprint(\"Breaking down the lowest selling SKU categories that have sold less than 213 items, it only consisted about {:.2f}% of the revenue\".format(low_selling_product_cat))","fef64a1f":"customer_trans = df.groupby(\"Customer_ID\").agg({\"Transaction_ID\":\"nunique\"}).rename(columns = {\"Transaction_ID\":\"Transactions\"}).reset_index()\ncustomer_trans[\"Buyer Status\"] = np.where(customer_trans[\"Transactions\"] > 1, \"Repeat Purchaser\", \"First Time\")\n\n#create a new table into the main dataframe\ndf = df.merge(customer_trans, on = \"Customer_ID\")","fd7d9ab2":"customer_trans[\"Buyer Status\"].value_counts().plot(kind = \"pie\", autopct = \"%.2f\", labels = None, figsize = (6,6))\nplt.title(\"Share of Repeat and First Time Purchasers\", fontsize = 18)\nplt.legend(loc = 4, labels = [\"Repeat Purchaser\", \"First Time\"])\nplt.ylabel(\"\");","f69341dc":"time_series = df.groupby([\"Month Truncated\", \"Buyer Status\"]).agg({\"Customer_ID\":\"nunique\"}).reset_index()\ntime_series.columns = [\"Month Truncated\", \"Buyer Status\", \"Customre_Count\"]\n\ndf.groupby([\"Month Truncated\", \"Buyer Status\"]).agg({\"Customer_ID\":\"nunique\"}).unstack().plot(kind = \"bar\", figsize = (8,5))\n\nplt.legend(loc = 2, labels = [\"First Time\", \"Repeat Purchaser\"])\nplt.title(\"Customers Segemented by Frequency\", fontsize = 15)\nplt.xlabel(\"\");","235571e7":"#import libraries for MBA\nfrom mlxtend.frequent_patterns import apriori, association_rules\nimport mlxtend as ml","777e398b":"#data pre-processing \ndf1 = df.set_index(\"Transaction_ID\")[\"SKU_Category\"].str.get_dummies().reset_index() #one hot encode SKU category which is grouped by Transaction ID\n\nlist_name = df1.columns.tolist()\nlist_name.remove(\"Transaction_ID\")\n\nmba = df1.groupby(\"Transaction_ID\")[list_name].apply(lambda x:x.sum()) #sums up all the items bought per transactions\n\ndef encode_units(x): #data should only be 0 or 1 \n    if x <= 0:\n        return 0 #item does not exist in this transaction\n    if x >= 1:\n        return 1 # item exists for this transaction\n    \nmba = mba.applymap(encode_units)\n\nfrequent_itemsets = apriori(mba, min_support = 0.01, use_colnames = True) #only shows the itemsets where the support >= 1%\nrules = association_rules(frequent_itemsets, metric = \"lift\")\n\n#showed percentage to easily interpret the data\nrules[\"support\"] = rules[\"support\"]*100\nrules[\"confidence\"] = rules[\"confidence\"]*100","0cdc9c32":"print(\"Maximum support for the retail store is {:.2f}%, and minimum is at {:.2f}%\".format(rules[\"support\"].max(), rules[\"support\"].min()))","19853dd7":"mba_table = rules[(rules[\"confidence\"] >= 30) & (rules[\"lift\"] >= 1)]\nmba_table[[\"antecedents\", \"consequents\",\"support\", \"confidence\", \"lift\"]].sort_values(\"lift\", ascending = False).head(10) ","367e6a1e":"!pip install Lifetimes\n\nfrom lifetimes import BetaGeoFitter #used to compute for the number of transactions at period T, and proabability that the customer is alive\nfrom lifetimes import GammaGammaFitter #used to compute for the average sales per transaction\nfrom lifetimes.plotting import plot_frequency_recency_matrix, plot_probability_alive_matrix\nfrom lifetimes.utils import summary_data_from_transaction_data, calculate_alive_path","eaa31859":"from lifetimes.plotting import plot_frequency_recency_matrix, plot_period_transactions, plot_calibration_purchases_vs_holdout_purchases, plot_probability_alive_matrix\nfrom lifetimes.utils import calibration_and_holdout_data","a113ffee":"df.columns","919a8e82":"#prepare and transform transactional data into RFM\nsummary_trans = df.groupby([\"Customer_ID\", \"Date\", \"Transaction_ID\"])[\"Sales_Amount\"].sum().reset_index()\ndata = summary_data_from_transaction_data(summary_trans, customer_id_col = \"Customer_ID\", datetime_col = \"Date\", monetary_value_col = \"Sales_Amount\")\n\n#modelling using the lifetimes package is a lot similar to using \nbgf = BetaGeoFitter(penalizer_coef=0.001)\nbgf.fit(data['frequency'], data['recency'], data['T'])","f9f5e9d8":"plot_period_transactions(bgf)","919f5b9e":"summary_cal_holdout = calibration_and_holdout_data(transactions = df, \n                                                   customer_id_col = 'Customer_ID', \n                                                   datetime_col = 'Date',\n                                                   monetary_value_col = \"Sales_Amount\",\n                                        calibration_period_end='2017-11-01',\n                                        observation_period_end='2017-12-31' )\n\nbgf = BetaGeoFitter(penalizer_coef=0.001)\nbgf.fit(summary_cal_holdout['frequency_cal'], summary_cal_holdout['recency_cal'], summary_cal_holdout['T_cal'])\nplot_calibration_purchases_vs_holdout_purchases(bgf, summary_cal_holdout)","bc7b863a":"#refit the bgf model with the entire dataset since it last fitted the data using the holdout and calibration\nbgf.fit(data['frequency'], data['recency'], data['T'])","38161db0":"plot_frequency_recency_matrix(bgf, T = 30);","a970ad03":"plot_probability_alive_matrix(bgf)","406655da":"returning_customers = data[data[\"frequency\"] > 0]","af1c505c":"returning_customers[['monetary_value', 'frequency']].corr()","66f7bd9d":"from sklearn.metrics import mean_absolute_error\ncal_sample = summary_cal_holdout[summary_cal_holdout[\"frequency_cal\"] > 0]\nggf = GammaGammaFitter()\n\nggf.fit(cal_sample[\"frequency_cal\"], cal_sample[\"monetary_value_cal\"])\n\nx = ggf.conditional_expected_average_profit(summary_cal_holdout[\"frequency_holdout\"], summary_cal_holdout[\"monetary_value_holdout\"]).reset_index()\nx.columns = [\"Customer_ID\", \"Predicted_AOV\"]\n\n# (summary_trans[\"Date\"] > '2017-11-01') &  (summary_trans[\"Date\"] < '2017-07-01')\ny = summary_trans[(summary_trans[\"Date\"] > '2017-11-01') &  (summary_trans[\"Date\"] < '2018-01-01')].groupby(\"Customer_ID\").agg({\"Sales_Amount\":\"sum\", \"Transaction_ID\":\"nunique\"}).reset_index()\ny[\"True AOV\"] = y[\"Sales_Amount\"]\/y[\"Transaction_ID\"]\n\nz = x.merge(y[[\"Customer_ID\", \"True AOV\"]], on = \"Customer_ID\")\nz[\"Error\"] = z[\"True AOV\"] - z[\"Predicted_AOV\"]","21c0f379":"mae = mean_absolute_error(z[\"True AOV\"], z[\"Predicted_AOV\"])\nprint(\"MAE using the Gamma-Gamma model is: {:.2f}\".format(mae))","387a56ed":"z[\"Error\"].plot(kind = \"kde\")\nplt.xlim([-100,100])\nplt.title(\"Error Distribution\", fontsize = 15);","b0705860":"print(\"Median error is {:.2f}\".format(z[\"Error\"].median()))\nprint(\"Max error is at {:.2f} with over {} customers that have an error of greater than 13\".format(z[\"Error\"].max(), z[z[\"Error\"] >= 13].shape[0]))\nprint(\"Min error is {:.2f} with over {} customers that have an error of less than -10\".format(z[\"Error\"].min(), z[z[\"Error\"] < -10].shape[0]))","7e2a8ee7":"#refit ggf using the entire dataset\nggf = GammaGammaFitter()\nggf.fit(returning_customers['frequency'], returning_customers['monetary_value'])","9bb46c60":"period = 30 #30 days\n\ndata['predicted_purchases'] = bgf.conditional_expected_number_of_purchases_up_to_time(period, data['frequency'], data['recency'], data['T'])\ndata[\"p_alive\"] = bgf.conditional_probability_alive(data[\"frequency\"], data[\"recency\"], data[\"T\"])\n\ndata[\"average_sales\"] = ggf.conditional_expected_average_profit(data['frequency'],data['monetary_value'])\ndata[\"LTV\"] = ggf.customer_lifetime_value(bgf, frequency = data[\"frequency\"], recency = data[\"recency\"], T = data[\"T\"], monetary_value = data[\"monetary_value\"], \n                            time = period)","4c0a48a5":"data[\"average_sales\"].plot(kind = \"kde\")\nplt.xlim([-20,50])\nplt.title(\"Distribution of AOV for Each Customer\", fontsize = 15);","78caef69":"ref_table = df[[\"Customer_ID\", \"Buyer Status\"]].drop_duplicates()\ndata = data.merge(ref_table, on = \"Customer_ID\")\n\n\nfirst_time_aov = data[data[\"Buyer Status\"] == \"First Time\"][\"average_sales\"].median()\nrepeat_aov = data[data[\"Buyer Status\"] == \"Repeat Purchaser\"][\"average_sales\"].median()\n\nprint(\"Median AOV for first time purchasers is {:.2f}USD, while repeat customers are at {:.2f}USD\".format(first_time_aov, repeat_aov))","d622a481":"\ndata[\"p_alive category\"] = np.where(data[\"p_alive\"] > 0.8, \"Returning\", \"High Risk\")\n\ndata[\"p_alive category\"].value_counts().plot(kind = \"pie\", autopct = \"%.2f\", figsize = (6,6))\nplt.ylabel(\"\")\nplt.title(\"Expected % of Customers for the next 30 days\", fontsize = 15);","f6f8b0bb":"## Sales by time","5f7cfd14":"Historically, only when a customer has an age of about 50 days would show high probability of doing another purchase.One way to improve of this would probably use a retargeting campaign to these users to better improve the chances of a repeat purchase. In a marketing standpoint, enticing repeat customers are much cheaper than finding new purchasers.","ab9204eb":"## BG\/NBD - Model Evaluation\n\nBefore predicting the number of transactions of each customer, it is important to understand first if the model describes the data well. This will be done by various tests, which is also under the lifetimes package.","d67040d8":"There are 2 peaks found in the distribution plot, which is at around 11 USD and 25 USD. The AOV at 25 USD are the first time purchasers, while the repeat customers are expected to have less AOV.","f36d75f2":"Purchasers who have had been a customer for over 150 days, is where they usually do a single purchase. It is only when the customer's age in the business is over 300 days would you expect to have high number of transactions.","18896e04":"One assumption when doing the Gamma-Gamma model is that there should be ideally 0 or very little correlation between frequency and the monetary value. Using Pearson correlation test, it is shown that there is hardly any correlation between the two parameters thus assumption has been satisfied and we can proceed with the modelling.","7bd4dfbb":"Focusing on itemsets that have high lift, I did this because they have high potential for cross-selling, and the top 3 items have low support.\n\nUnderstanding the data:\n\n1. Since these itemsets have high lift, then these can be used to do cross-selling and let customers buy more items in the store. \n2. Although these itemsets have the highest lift, the support (number of times these items are being bought in a transactions), are on the lower end of the spectrum. Since it is a retail store, one way to improve this would be to put these two categories near each other or probably place it near the entrace point. It is also possible to do digitally target the customers and entice them to buy both items together.\n\nDo note that I filtered out the data with a lift greater than 1, and confidence to 30 to get the best itemsets.","0fadc3b1":"Part of data processing using the Gamma-Gamma model is to take out the customers who did not do any repeat purchases.","0c89e5d3":"## Predicting Customer Purchases and Status","fdccf3a1":"# How is the business doing?\n\nThis section will be broken by their sales across the entire dataset, their top and worst selling products, and knowing who their customers are.","e7f36dd9":"A right skewed distribution plot, which means that a bulk majority of both the products and the product categories have sold poorly. Identifying these items would be very important to help minimize the production cost to create these items.","0321f453":"# How Is The Business Doing?\n\nWhat this notebook aims to do is to understand the nuances of the business, and try to find ways to elevate their performance. It will be broken down into the following parts:\n\n1. How sales have performed throughout entire time period.\n2. What are the best and worst selling products.\n3. Understanding Customer Behavior\n    1. Repeat Purchasers vs First Time Purchasers\n    2. Predicted probability of customers who'll still perform a repeat purchase\n    3. Estimating Customer Lifetime Value for the next 30 days for each customer.","43cd374d":"## Understanding Customer Behavior\n\nIn the previous section, we know that the sales of the business have been relatively stagnant all throughout, and there a several products that have sold very poorly which have added a lot of costs to the business. Next let's try to understand purchaser behavior.\n\nFor this section, it will be broken down by the number of repeat and first time purchasers and also look at the sales broken down into these segments.","e8a645a6":"## Market Basket Analysis\n\nMarket basket analsysis is a technique that tries to find the relationship between two items, which can be described in three metrics (support, confidence, lift). It is important to understand how the correlation between items to help improve sales by cross-selling. What cross selling does is let the crusomer buy more by getting more items in the cart. It is also possible to target customers online and make them entice to buy more of the associated product.\n\n\nUnderstanding the basic concepts of MBA:\n\n1. Support - Answers on how frequent when both items have been bought with respect to all transactions.\n2. Confidence - How likely both products are being bought together IF your antecedent item has been added to the cart.\n3. Lift - How strongly correlated are the items being bought.\n   1. Lift > 1 - Products are more likely to be bought together\n   2. Lift = 1 - Both items don't have any relation\n   3. Lift < 1 - Products are less likely to be bought together\n    \nLibrary being used to do market basket analysis is [mlxtend](https:\/\/pypi.org\/project\/Lifetimes\/).","3ad3b0c0":"Most of the error plays around the value of 3. However, the model tends to be more conservative and underestimates the actual average order value, since there are over 1036 customers who have an error of larger than 13. One way to resolve this would be segment the customers and create a different Gamma-Gamma model for each segment to have a better model performance.","beff30dc":"When using the lifetimes package, it is important to transform your transactional data into RFM (Recency, Frequency, Monetary Value). As these \nare the inputs being placed into the BG\/NBD model and the Gamma-Gamma Model.\n\nIf you do your own RFM data, there are slight differences with the classical definition of it:\n1. Recency - The difference between your first purchase transaction and the last purchase. \n2. Frequency - The number of times a customer has done a repeat purchase. However, in case a customer has done multiple transactions in a single day it will still be considered as 1. Computation for frequency: Number of total transactions that have unique days - 1\n3. Monetary Value - This is the average sales per transaction for each customer.\n4. T - How long has the customer existed. This equates to the end of the period minus the date of first purchase.","06a2c89b":"# What to do next?\n\n1. As the business has already stagnated, the next phase to elevate would be through optimization.\n    1. We have identified several SKUs and SKU categories that have underperformed and possibly have an impact with the operational cost\n        1. Do a further analysis on the cost made for that product and decide whether to take it out from production or minimize it.\n    2. Several of the item categories that have strong correlation with each other, don't show up on many transactions\n        1. Since they have a high chance of cross-selling, placing those two categories beside each other could work well.\n        2. Another would be to email the customers and do cross-selling from there.\n    3. First time purchasers show a decreasing trend\n        1. Doing marketing campaigns to help grow brand awareness would really help to entice potentially new customers to go the store.\n    4. About 35% of the existing customer base have a chance of not returning in the next 30 days.\n        1. Retarget the existing customers to entice them to get back to the store. ","bc9f2749":"## Expected Future Customer Behavior","98e849c5":"Insights:\n\n1. Difficult to find any real trends on the monthly data since the dataset being given is only for a single year. But judging from the graph, the dips are coming from winter and summer months, but sales rises rapidly and peaks during December where people usually buy a lot of stuff.\n2. Sunday and Monday have the lowest average daily sales.","0aa890f8":"Insights:\n\n1. On the early months of the year, about 1\/3 of the purchasers were first timers. The succeesing months, showed that there is a slight decreasing trend of first time customers. \n    1. Months June to August had the lowest number of first time purchasers, it could be because the business is highly affected duing the summer months.\n2. There's a massive jump between the first two months and the succeeding from 2.25K repeat purchasers and jumps to near 3K a month. Probable cause for this would be that marketing campaigns from March onwards brought in more quality users, which made the do a repeat purchase.","eea16768":"# What are the best and worst selling products?","b2aff7ac":"## Customer Analysis\n\nNow the model is working well, next is to proceed with the analysis of the future behavior of the customers.","c28b38a9":"## Estimating Average Order Value of Customer\n\nThe Gamma-Gamma model will be used to  estimate the future AOV of the customer.","a86cfeff":"Under the calibration_and_holdout_data, the calibration_period_end is used as a training set, while the observation period is used as the test set. As shown, the model describes the data well as there is very little error.\n\nNote: Originally, when initally setting the penalizer at 0, the model predictions against the holdout data, were way off. Setting it to 0.001 made the model perform a lot more closely to the data.","d1828d91":"Unfortunately, we don't really have much information about the SKU's so it would be difficult to find any reasons for why these are the top selling items and categories.","407aaca4":"## Estimating customer LTV\n\nIn a retail business where they constantly do marketing campaigns to acquire new and old customers to their store, it is important to know the customer lifetime value or the total sales they've brought to the business since they first purchased. It is important to know the LTV, as this can be used to understand how much value is the customer giving to the store. \n\n\nThe traditional approach to estimating LTV is through this equation:\n\nCustomer Lifetime Value = Average Order Value * Estimated Number of Purchases at some fixed time period * the average length of the customer relationship\n\n    Where: Average Order Value = Sum of Sales for the customer\/Total transactions\n    \n\nOne problem when using this equation is that a single churn rate is used all throughout the entire customer base. However, that is impossible since every customer has their own level of engagement with the store. Another is that the average order value, which is unique for each customer, assumes that it is consistent all throughout time. To overcome this, the Beta Geometric Negative Binomial Distribution model willl be used, to address the issues of the traditional formula and answer the number of purchases as well as the churn rate. This will then be tied into the Gamma-Gamma model which predicts the average order value of the customer at that instant of time.\n\nThe Beta Geomteric Negative Binomial Distribution model, creates an output that would estimate the number of purchases within some time period, as well as the probability of the customer going back to the store. This has the following assumptions:\n\n1. That the number of transactions within some period can be modelled using a poisson distribution. The poisson distribution is being used here since this type of process models how many events would occur within some fixed period with a rate of transaction, lambda.\n2. The transaction rate of each customer can be modelled using a Gamma distributed plot. The Gamma distribution answers on how long of a time should someone wait till the nth event will occur.\n3. Each customer becomes inactive after each transaction with probability, p, and is estimated using a beta distribution. This kind of distribution answers the probabilities. In this case, since we're trying to answer the probability of the customer going back to the store, then this would fit well to the problem.\n\n\nLastly, the Gamma-Gamma model predicts the AOV of the customer in the future. \n\n1. Similar to the BG NBD model, it assumes that the transaction is Gamma distributed, with shape p and scale v.\n2. While scale v is being also assumed having a Gamma distribution.\n\nNow that we know the idea behind gettting the LTV of the customer, we're using the [lifetimes library](https:\/\/pypi.org\/project\/Lifetimes\/) to model and predict the future purchases in the next 30 days.","948692f7":"# Gamma-Gamma - Model Evaluation","8425e532":"Insights:\n\n1. It seems that the time series data is showing a weekly seasonality.\n2. Business sales shows no trend which means that it possibly have operated for sometime now and has started to stagnate. The next step to help grow the business is to optimize their operations. This can be done by checking the performance of their products, and their customer behavior.\n    1. No context has been given in this dataset except for the transactional data, so 1 thing that can be done would be to check their online stores vs. the brick and mortar sales. \n    2. Understand why customers would come back to the store, since repeating purchasers are far easier to acquire than find new customers.\n    3. Check all the operational costs of the business and try to minimize it.\n    4. Do cross-selling, which is to try make the customers buy more items per transaction.\n    \n    \nNote: I'll be assuming that the retail store has both a physical and e-commerce shop.","ae3cbfae":"Now the worst selling products have been identified, the next best thing would be to check how much supply is their around, how much did it cost to create these items and try to weigh the options. If they are detrimental to the business they can stop the production of these items and figure a way out to dispose the remaining.","1bd2c664":"Over 35% of the customers are at high risk of not going back to the store.","f8a70e21":"## Worst Selling Products\n\nI didn't add in the list of worst selling products because there were way too many to mention.","a75f6595":"# Data Cleaning\n\nChecking the data for any nuances as well as change parameters into the appropriate datatypes for analysis.","949789ef":"There's a column known as \"Unnamed: 0\" which will not be used for the analysis of the business. This is probably the index of the csv file being exported.","bfbcb3b7":"The \"Date\" columns has an object data type. Let's turn it into a datetime format for analysis.","ca4010e2":"Note:\n\nI originally used the SKU column but there were over 5200 unique categories and the Kaggle notebook couldn't handle it. So instead I used SKU Category as it has lower number of groups of < 200. I also tried taking the top 200 items but the support (how many items the combination of items have shown together, was extremely low (< 0.1%).","ab24d10d":"Using the mean absolute error to better understand how large is the error between the estimated AOV and the True AOV.","314ca042":"I've defined repeat purchasers as people who have bought more than once, not the customers who are on their 2nd transaction. As shown, share of customers are about 50\/50. This means that 50% of the people only bought once but never returned for a repeat purchase, high churn rate."}}