{"cell_type":{"644a6291":"code","a480407e":"code","cdbbca47":"code","b1a2f980":"code","2d965612":"code","9e942a22":"code","ddd30231":"code","d51eba7c":"code","5003d6b5":"code","d3d6be8f":"code","b7792f53":"code","9756a0e6":"code","b6c1a39f":"code","01efdf07":"code","b2ca4caa":"code","5cb79de8":"code","53b2c3c0":"code","83fcbcf9":"code","3a5d2a59":"code","e19619e7":"code","de6de1b6":"code","cb54b912":"code","65335fa6":"code","d37c242b":"code","00e04ef0":"code","0c957957":"markdown","285de8d2":"markdown","f3678601":"markdown","f09920df":"markdown","a245ac43":"markdown","b01034e8":"markdown","06ab760b":"markdown","f2270e5e":"markdown","750b3eb2":"markdown"},"source":{"644a6291":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a480407e":"import matplotlib.pyplot as plt\nimport seaborn as sns","cdbbca47":"path = '..\/input\/environmental-sensor-data-132k\/iot_telemetry_data.csv'\ndataIot = pd.read_csv(path, index_col='ts')\ndataIot.head()","b1a2f980":"dataIot.shape","2d965612":"dataIot.describe()","9e942a22":"dataIot.info()","ddd30231":"dataIot.isnull().sum()","d51eba7c":"data = dataIot.copy()\ndata = data.drop(['motion'], axis = 1)\ndata.head()","5003d6b5":"from sklearn.preprocessing import LabelEncoder\n\nencLab = LabelEncoder()\ndeviceEnc = pd.DataFrame(encLab.fit_transform(data['device']), columns=['condition'])\ndeviceEnc.index = data.index\n\ndataEnc = pd.concat([data, deviceEnc], axis=1)\ndataEnc.head()","d3d6be8f":"encLab2 = LabelEncoder()\nlightEnc = pd.DataFrame(encLab2.fit_transform(dataEnc['light']), columns=['lights'])\nlightEnc.index = dataEnc.index\n\ndataEnc2 = pd.concat([dataEnc, lightEnc], axis=1)\ndataEnc2.head()","b7792f53":"dataProcessed = dataEnc2.drop(['device', 'light'], axis=1)\ndataProcessed.head()","9756a0e6":"for i in dataProcessed.columns:\n    if dataProcessed[i].nunique() > 5:\n        plt.figure(figsize=(8, 6))\n        sns.kdeplot(x=i, data=dataProcessed)\n        plt.show()","b6c1a39f":"def barplot(columnname):\n    val = dataProcessed[columnname]\n    valCount = val.value_counts()\n    \n    plt.figure(figsize=(10, 6))\n    sns.barplot(valCount.index, valCount)\n    plt.title(columnname)\n    plt.ylabel('Frequency')\n    plt.show()","01efdf07":"for i in dataProcessed.columns:\n    if dataProcessed[i].nunique() <= 5:\n        barplot(i)","b2ca4caa":"for k in dataProcessed.columns:\n    if dataProcessed[k].nunique() > 5:\n        plt.figure(figsize=(8, 6))\n        sns.boxplot(x=dataProcessed['condition'], y=dataProcessed[k])\n        plt.show()\n        ","5cb79de8":"dataCorr = dataProcessed.drop(['lights', 'condition'], axis=1)\ncorr = dataCorr.corr()\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr, annot=True, linewidths=.5)","53b2c3c0":"dataCleanInput = dataProcessed.copy()\nremovedCol = ['lpg', 'smoke', 'co']\n\ndataCleaned = dataCleanInput.drop(removedCol, axis=1)\ndataCleaned.head()","83fcbcf9":"Q1 = dataCleaned.quantile(.25)\nQ3 = dataCleaned.quantile(.75)\nIQR = Q3-Q1\ndataClean = dataCleaned[~((dataCleaned<(Q1-1.5*IQR))|(dataCleaned>(Q3+1.5*IQR))).any(axis=1)]\ndataClean.head()","3a5d2a59":"from sklearn.preprocessing import StandardScaler\n\nscaler1 = StandardScaler()\nhumidScaled =pd.DataFrame(scaler1.fit_transform(dataClean[['humidity']]), columns=['humid'])\nhumidScaled.index = dataClean.index\ndataClean1 = pd.concat([dataClean, humidScaled], axis=1)\ndataClean1.head()","e19619e7":"scaler2 = StandardScaler()\ntempScaled =pd.DataFrame(scaler1.fit_transform(dataClean[['temp']]), columns=['temperature'])\ntempScaled.index = dataClean1.index\ndataClean2 = pd.concat([dataClean1, tempScaled], axis=1)\ndataClean2.head()","de6de1b6":"y = dataClean2['condition']\nX = dataClean2[['temp', 'humid', 'lights']]\nX","cb54b912":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=0)\n","65335fa6":"xTrain, xVal, yTrain, yVal = train_test_split(x_train, y_train,test_size=0.2, random_state=0 )","d37c242b":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\nmodel = RandomForestClassifier(random_state=0, max_depth=1)\nmodel.fit(xTrain, yTrain)\npredResult = model.predict(xVal)\nreport = classification_report(yVal, predResult)\nconfMat = confusion_matrix(yVal, predResult)\nprint(report)\nprint(confMat)","00e04ef0":"model = RandomForestClassifier(random_state=0, max_depth=1)\nmodel.fit(x_train, y_train)\nprediction = model.predict(x_train)\nreports = classification_report(y_train, prediction)\nconf = confusion_matrix(y_train, prediction)\nprint(reports)\nprint(conf)","0c957957":"## Multivariate Analysis","285de8d2":"## Data Cleaning\n\n I am removed some columns that high correlation coefficient. Also, I delete outliers in numerical data in dataset. Then, I standardize the value of cleaned using standard scaler form scikit learn. ","f3678601":"## Univariate Analysis","f09920df":"# Loading Data Set and Basic Data Exploration\n\nIn this part I load dataset with pandas library and do some basic exploration to the dataset. In There we have found that dataset has 405184 rows and 8 columns. There are following columns in the dataset:\n          \n          1. ts (timestamp) ==> epoch\n          2. device id ==> object\n          3. CO (Carbon Monoxide) in ppm ==> float64\n          4. humidity in percent ==> float64\n          5. light ==> bool\n          6. LPG (liquified Petroleum Gas) in ppm ==> float64\n          7. motion ==> bool\n          8. smoke in ppm ==> float64\n          9. Temperature in Fahrenheit ==> float64\n          \nThere are no missing value in this datasset. ","a245ac43":"# Data Preprocessing\n\nBefore I am analyzing data in to more detail, I do some preprocessing to make data ready for more detail analysis. I remove **motion** column because its only contains False value and I assume it can't help to predict result we want. Then I encode a label for some columns. There are **device** and **light** columns. It's because the data in device column represent environment condtion that we want to predict. There are the following data on device and the label to represent it.\n\n        1. 00:0f:00:70:91:0a (stable conditions, cooler and more humid) ==> 0\n        2. 1c:bf:ce:15:ec:4d (highly variable temperature and humidity) ==> 1\n        3. b8:27:eb:bf:9d:51 (stable conditions, warmer and dryer) ==> 2\n\nAnd for the light column I represent the absence of light by label 0 (False) and 1 (True). It is because data on the light column has boolean data type.","b01034e8":"## Machine Learning Modelling\n\nIn this section I make baseline model to know what optimal what appropriate the model to classify environment condition and optimum hyperparameter. I try to solve the problem with Random Forest Classifier. Baseline model itself trained with training dataset and verify with validation dataset. Then, I evaluate the baseline model and model with confusion matrix, precision and recall.","06ab760b":"## Discussion \n\nFrom the analysis above we know some features can be removed to gain better performance classifier. These removed features are co, lpg and smoke. It is because these features have high correlation coefficient among them. Hence, we can build a model only with humidity, temperature and lights features. ","f2270e5e":"# Exploratory Data Analysis","750b3eb2":"## Data Splitting\n\nFirst, I split whole dataset into two parts there are training dataset and test dataset with the proportion training (80%) and test (20%). After that, I do split the training dataset into two parts : training data (80%) and validation data (20%). "}}