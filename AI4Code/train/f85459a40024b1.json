{"cell_type":{"8132dee2":"code","536df44b":"code","db72a162":"code","fa69a2a0":"code","51b549aa":"code","b389c0a3":"code","46309996":"code","fb6ae492":"code","ede11b01":"code","21daa76c":"code","9feea04b":"code","4d69f2d5":"code","39821859":"code","c13053fb":"code","af86592b":"code","b3d789f3":"code","41658db6":"code","d88c19b7":"markdown"},"source":{"8132dee2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","536df44b":"x_l= np.load('\/kaggle\/input\/sign-language-digits-dataset\/X.npy')\ny_l= np.load('\/kaggle\/input\/sign-language-digits-dataset\/Y.npy')\nimg_size = 64\nplt.subplot(1,2,1)\nplt.imshow(x_l[260].reshape(img_size,img_size))\nplt.axis('off')\nplt.subplot(1,2,2)\nplt.imshow(x_l[900].reshape(img_size,img_size))\nplt.axis('off')","db72a162":"x_l","fa69a2a0":"# Join a sequence of arrays along an row axis.\nX = np.concatenate((x_l[204:409], x_l[822:1027] ), axis=0) # from 0 to 204 is zero sign and from 205 to 410 is one sign \nz = np.zeros(205)\no = np.ones(205)\nY = np.concatenate((z, o), axis=0).reshape(X.shape[0],1)\nprint(\"X shape: \" , X.shape)\nprint(\"Y shape: \" , Y.shape)","51b549aa":"from sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.15, random_state=42)\nnumber_of_train = X_train.shape[0]\nnumber_of_test = X_test.shape[0]\n","b389c0a3":"Y_train.shape","46309996":"X_train.shape","fb6ae492":"X_test.shape","ede11b01":"X_train_flatten = X_train.reshape(number_of_train,X_train.shape[1]*X_train.shape[2])\nX_test_flatten = X_test .reshape(number_of_test,X_test.shape[1]*X_test.shape[2])\nprint(\"X train flatten\",X_train_flatten.shape)\nprint(\"X test flatten\",X_test_flatten.shape)","21daa76c":"x_train = X_train_flatten.T\nx_test = X_test_flatten.T\ny_train = Y_train.T\ny_test = Y_test.T\nprint(\"x train: \",x_train.shape)\nprint(\"x test: \",x_test.shape)\nprint(\"y train: \",y_train.shape)\nprint(\"y test: \",y_test.shape)","9feea04b":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(random_state = 42 , max_iter = 150)\nlr.fit(x_train.T,y_train.T)\nprint(\"Test Accuracy {}\".format(lr.score(x_test.T,y_test.T)))","4d69f2d5":"from sklearn.linear_model import  LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor , RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier","39821859":"random_state = 42\nclassifier = [LogisticRegression(random_state = random_state),\n                  RandomForestClassifier(random_state = random_state),\n                  DecisionTreeClassifier(),\n                  KNeighborsClassifier()]\n\nlogreg_param_grid = {'C': np.logspace(-3,3,7),\n                    'penalty': ['l1','l2']}\n\n\nrf_param_grid = rf_param_grid = {'max_features':[1,5],\n                'min_samples_split': [2,3],\n                'min_samples_leaf':[1,3],\n                'bootstrap':[False],\n                'n_estimators':[100],\n                'criterion': ['gini']}\n\ndt_param_grid = {'min_samples_split': range(10,50,2),\n                'max_depth': range(1,10,2)}\n\nknn_param_grid = {'n_neighbors': np.linspace(1,6,2, dtype = int).tolist()}\n\nclassifier_param = [logreg_param_grid ,rf_param_grid,dt_param_grid, knn_param_grid]","c13053fb":"from sklearn.model_selection import StratifiedKFold,GridSearchCV\nfrom sklearn.metrics import accuracy_score \ncv_results = []\nbest_estimators = []\nfor i in range(len(classifier)):\n    clf = GridSearchCV(classifier[i],param_grid= classifier_param[i],cv = StratifiedKFold(n_splits = 10),scoring = 'accuracy',n_jobs = -1,verbose = 1)\n    clf.fit(x_train.T,y_train.T)\n    cv_results.append(clf.best_score_)\n    best_estimators.append(clf.best_estimator_)\n    print(cv_results[i])","af86592b":"import seaborn as sns\n\ncv_result = pd.DataFrame({'Cross Validation Means': cv_results, 'ML Models': ['LogisticRegression',\n              'RandomForestClassifier',\n              'DecisionTreeClassifier',\n              'KNeighborsClassifier'\n                ] })\n\ng = sns.barplot('Cross Validation Means','ML Models',data = cv_result)\ng.set_xlabel('Mean Accuracy')\ng.set_title('Cross Validation Scores')\nplt.show()","b3d789f3":"cv_result","41658db6":"best_estimators\n","d88c19b7":"**CROSS VALIDATION**"}}