{"cell_type":{"e6874063":"code","4125d2c3":"code","9b2e0114":"code","9694ff0e":"code","6d28c5ce":"code","3b728f49":"code","95112462":"code","32632dc9":"code","6e4a6967":"code","e9ec6141":"code","64a6eb28":"code","282fe6ff":"code","1cb37d41":"code","c515ef92":"code","334e6b0e":"code","3d1e7804":"code","089c8191":"code","9383dc05":"code","9561c191":"code","d934bc99":"code","84c56dba":"code","4932cb77":"code","98b86af3":"code","9f96b2fd":"code","17f3aabb":"code","fb5cae51":"code","617dbf77":"code","ea9aff16":"code","7fc868c1":"code","d8b08dbd":"code","07c65518":"code","4b1c9f00":"code","b824d11f":"code","282abfa8":"code","45186db5":"code","996e7916":"code","4f32d054":"code","20c69df9":"code","289767c4":"code","16fb1ca5":"code","faa49a8d":"code","702af2bc":"code","bc36db51":"code","d8309873":"code","6debbdb0":"code","612d4995":"code","49324440":"code","eb32386f":"code","eda15a08":"code","10be36fe":"code","edf71840":"code","2beb3f41":"code","8092b916":"code","28cf801a":"code","d22a6b54":"code","60f3005e":"code","d0b69f05":"code","7248c48a":"code","53939fdc":"code","107c65b7":"code","fe1d2460":"code","9b1545e5":"code","62b664e9":"markdown","b018429c":"markdown","a30e3be4":"markdown","506f940e":"markdown","2b90b8f8":"markdown","eb204713":"markdown","4a54aa4c":"markdown","82171355":"markdown","8c55bcbd":"markdown","88399d43":"markdown","326d13d5":"markdown","cdafa0cf":"markdown","34ccc6e1":"markdown","a2283894":"markdown","35827993":"markdown","97e119ee":"markdown","eb9ecc7e":"markdown","83999d58":"markdown","deceda3b":"markdown","c60b8ef2":"markdown","d9a28844":"markdown","e14e2c4f":"markdown","fa35619c":"markdown","a35ee562":"markdown","3f874bc8":"markdown","62e448ff":"markdown","d5cadc03":"markdown","dd773ad0":"markdown","a6cf2eb7":"markdown","0007055c":"markdown","630dc75a":"markdown","be3c1bc5":"markdown","8ab2ca7e":"markdown","43211420":"markdown","e4713b0e":"markdown","f3ce3feb":"markdown","11f9115f":"markdown","5498bfe6":"markdown","cdf2c939":"markdown","9fe41545":"markdown","08968180":"markdown","09d7d2c9":"markdown","7c36b22d":"markdown","db823fd0":"markdown","c6c4c006":"markdown","ef6b1415":"markdown","f68d6139":"markdown","9885f387":"markdown","232bcbbf":"markdown","275b4f7a":"markdown","77357392":"markdown","27265525":"markdown","2f5c41f9":"markdown","d93414cf":"markdown","8621f6ac":"markdown","a8a12d40":"markdown","4395da89":"markdown","39d05f69":"markdown","ac08532d":"markdown","e0446848":"markdown","41456903":"markdown","281022c9":"markdown","0d10efbe":"markdown","b3d97661":"markdown","aa1418e4":"markdown","8aca1ef1":"markdown","3c681cfc":"markdown","3d90a8f6":"markdown","8c65db0b":"markdown"},"source":{"e6874063":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4125d2c3":"df = pd.read_csv('\/kaggle\/input\/amazon-reviews-unlocked-mobile-phones\/Amazon_Unlocked_Mobile.csv')","9b2e0114":"df.head()","9694ff0e":"df.info()","6d28c5ce":"print(\"Shape of the dataset: \", df.shape)\nprint(df.isnull().sum())\n","3b728f49":"df.dropna(subset=['Reviews'], inplace = True)","95112462":"df[\"Brand Name\"].fillna(value = \"Missing\", inplace = True)\ndf[\"Price\"].fillna(value = 0, inplace = True)\ndf[\"Review Votes\"].fillna(value = 0, inplace = True)","32632dc9":"df.isnull().any()","6e4a6967":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nplt.figure(figsize=(10,5))\nsns.countplot(df['Rating'], palette=sns.color_palette(\"RdYlGn\", 5))\nplt.title(\"Distribution of Ratings across the entire dataset\", fontweight='bold', fontsize=15)\nplt.xlabel(\"Ratings of Reviews\")\nplt.ylabel(\"Number of reviews corresponding to each of 5 ratings\")\nplt.show();","e9ec6141":"# Pie chart, where the slices will be ordered and plotted counter-clockwise:\nlabels = [f'{k} ({df[\"Rating\"].value_counts()[k]} samples)' for k in df['Rating'].value_counts().keys()]\nsizes = dict(df['Rating'].value_counts()).values()\n\nfig1, ax1 = plt.subplots(figsize=(8,8))\nax1.pie(sizes, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\nax1.set_title(\"Distribution of ratings in reviews\",pad=40, fontweight='bold', fontsize=15)\nplt.show();","64a6eb28":"df.drop(['Product Name', 'Brand Name', 'Price', 'Review Votes'], axis = 1, inplace = True)\ndf.head(5)","282fe6ff":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df['Reviews'], df['Rating'], test_size=0.30, random_state=1, stratify=df['Rating'])\nplt.figure(figsize=(10,5))\nsns.countplot(y_train, palette=sns.color_palette(\"RdYlGn\", 5), alpha = 1)\nsns.countplot(y_test,facecolor=(0, 0, 0, 0), linewidth=2, edgecolor=sns.color_palette(\"dark\", 3))","1cb37d41":"#Concatenating the training instances (reviews and target Variable)\nX_train_df = pd.concat([X_train, y_train], axis = 1)\nX_train_df = X_train_df.reset_index(drop=True)","c515ef92":"X_train_df.head()","334e6b0e":"# a temporary dataframe to show how the data is getting processed after every step\npost_this_step = pd.DataFrame(columns=['Reviews'])\npost_this_step['Reviews'] = X_train_df[\"Reviews\"][5000:5010]","3d1e7804":"import nltk\nimport re","089c8191":"#installing contractions library\n!pip install contractions","9383dc05":"import contractions","9561c191":"def expandContractions(text):\n    expandedText = contractions.fix(text)\n    return expandedText\n\nX_train_df['Reviews'] = X_train_df['Reviews'].apply(expandContractions)","d934bc99":"# after applying expansion\npost_this_step['Expanded Reviews'] = X_train_df[\"Reviews\"][5000:5010]\npost_this_step","84c56dba":"from nltk.tokenize import RegexpTokenizer\n\n# based on the study of this dataset, we can ignore :), ;( \ndef tokenization(text):\n        tokenizer = RegexpTokenizer(\"[a-zA-Z]+\", discard_empty=True)\n        text = tokenizer.tokenize(text)\n        text = \" \".join(text)\n        return text.lower()\n    \n# applying tokenization along with punctuation removal and lowercasing\nX_train_df['Reviews'] = X_train_df['Reviews'].apply(lambda doc: tokenization(doc))","4932cb77":"#after applying tokenization \npost_this_step['Tokenized Reviews'] = X_train_df[\"Reviews\"][5000:5010]\npost_this_step","98b86af3":"stop_words= ['yourself','yourselves','herself', 'itself', 'themselves', 'himself', 'ourselves',\n             'myself', 'between', 'whom', 'is', \"she\", 'up', 'here', 'your', 'each', 'we', 'he',\n             'my', 'you', 'having', 'in', 'both', 'for', 'are', 'them', 'other','and', 'an', \n             'during', 'their', 'can', 'she', 'until', 'so', 'these', 'ours', 'above', 'what', \n             'while', 'have', 'more', 'only', 'when', 'just', 'that', 'were',\n             'should', 'any', 'who',  'a', 'they', 'to', 'has', 'before',\n             'into', 'yours', \"it\", 'do', 'on',  'now', 'her', 'by', 'an', 'from', \n             'about', 'further', \"would\", 'as', 'how', 'been', 'the', 'or', \n             'doing', 'such','his',  'was', 'through', 'out', 'below', 'own', 'theirs', \n             'me', 'why', 'once',  'him', 'than', 'be', 'same', 'some', 'with',\n             'it','at', 'after', 'its', 'which', 'there','our', 'this', 'hers', 'being', \n             'did', 'of', 'had', 'under', 'over','again', 'where', 'those', 'then', 'i',\n             'because', 'does', 'all', 'will', 'shall', 's','t','n','d', 'e','u', 'x','am','get','phone']","9f96b2fd":"stop_words = sorted(set(stop_words))\nprint(stop_words)","17f3aabb":"# function to filter out stopwords\ndef removeStopwords(text):\n    updatedtext = []\n    for word in text.split():\n        if(word not in stop_words):\n            updatedtext.append(word)\n            \n    return \" \".join(updatedtext)\n\n# removing stopwords from the Reviews attribute\nX_train_df['Reviews'] = X_train_df['Reviews'].apply(lambda doc: removeStopwords(doc))","fb5cae51":"post_this_step.drop(['Expanded Reviews'], axis = 1, inplace = True)\npost_this_step['Stopwords Removed Reviews'] = X_train_df[\"Reviews\"][5000:5010]\npost_this_step","617dbf77":"from nltk.stem.wordnet import WordNetLemmatizer\n\ndef lemmatizeText(text):\n    lemmatizedList = []\n    lemmatizer = WordNetLemmatizer()\n    for word in text.split():\n        lem_word = lemmatizer.lemmatize(word)\n        lemmatizedList.append(lem_word)\n    return \" \".join(lemmatizedList)\n\nX_train_df['Reviews'] = X_train_df['Reviews'].apply(lemmatizeText)","ea9aff16":"post_this_step.drop(['Tokenized Reviews'], axis = 1, inplace = True)\npost_this_step['Lemmatized Reviews'] = X_train_df[\"Reviews\"][5000:5010]\npost_this_step","7fc868c1":"# encapsulate all the pre-processing functions together\ndef preprocess_review(review):\n    expanded_review = expandContractions(review)\n    tokenized_review = tokenization(expanded_review)\n    removed_stopwords_review = removeStopwords(tokenized_review)\n    lemmatized_review = lemmatizeText(removed_stopwords_review)\n    return lemmatized_review","d8b08dbd":"# function to count characters in a list of strings\ndef getWordCount(text):\n    word_count = 0\n    for word in text.split():\n        word_count += 1\n    return word_count\n\ndef getReviewLength(text):\n    review_length = 0\n    for word in text.split():\n        review_length += len(word)\n    return review_length","07c65518":"X_train_df['review_length'] = np.array(X_train_df['Reviews'].apply(getReviewLength))\nX_train_df['word_count'] = np.array(X_train_df['Reviews'].apply(getWordCount))","4b1c9f00":"X_train_df.head(5)","b824d11f":"plt.rcParams['figure.figsize'] = (12.0, 6.0)\nbins = 1000\nplt.hist(X_train_df['review_length'], facecolor='goldenrod', alpha=1, bins=bins)\nplt.title('Review Length Distribution')\nplt.xlabel('Review Length')\nplt.ylabel('Count')\nplt.legend(loc='upper right')\nplt.xlim(0,700)\nplt.grid(True)\nplt.show()","282abfa8":"print(len(X_train_df[X_train_df['review_length'] > 200])*100\/len(X_train_df))","45186db5":"# Updating dataframe\n\nindex_names = X_train_df[X_train_df['review_length'] > 200].index","996e7916":"X_train_df.drop(index_names, inplace = True)","4f32d054":"plt.rcParams['figure.figsize'] = (12.0, 6.0)\nplt.title('Word Count Distribution')\nplt.hist(X_train_df['word_count'], bins=50,facecolor='goldenrod', label=['review.length < 200'])\nplt.xlim(0,60)\nplt.xlabel('Word Count')\nplt.ylabel('Reviews Count')\nplt.legend(loc='upper right')\nplt.grid(True)\nplt.show()","20c69df9":"print(X_train_df[X_train_df['word_count'] == 0].count())\nX_train_df.drop(X_train_df[X_train_df['word_count'] == 0].index , inplace = True)","289767c4":"dfNegReviews = X_train_df[X_train_df[\"Rating\"] < 3]\ndfPosReviews = X_train_df[X_train_df[\"Rating\"] > 3]","16fb1ca5":"# importing the library\nfrom wordcloud import WordCloud","faa49a8d":"text = \"\"\nfor review in dfNegReviews['Reviews']:\n    text += review\n        \nwordcloud = WordCloud(background_color=\"white\",width=1400, height=600).generate(text)\nplt.figure( figsize=(20,10))\nplt.axis('off')\nplt.imshow(wordcloud)","702af2bc":"text = \"\"\nfor review in dfPosReviews['Reviews']:\n        text += review\n        \nwordcloud = WordCloud(background_color=\"white\",width=1400, height=600).generate(text)\nplt.figure( figsize=(20,10))\nplt.axis('off')\nplt.imshow(wordcloud)","bc36db51":"# importing the libaries \nfrom nltk.util import ngrams\nfrom collections import Counter","d8309873":"# function to generate N-grams based on the size(= 1(unigram), 2(bigram), 3(trigram))\n\ndef generateNgrams(documents, size):\n    '''\n    Input: \n    documents: dataframe having only reviews stacked\n    size: size of ngram(as 1,2,3..)\n    \n    Output:\n    dataframe:{columns = {'words','count'}}\n    '''\n    \n    \n    ngrams_all = []\n    \n    for document in documents:  # the document is not splitted already\n         \n        document = document.split()\n        \n        if len(document) < size:\n            continue\n        else:\n            output = list(ngrams(document, size)) # using NLTK's ngrams function\n            \n        for ngram in output:\n            ngrams_all.append(\" \".join(ngram))\n            \n    count_ngram = Counter()\n    \n    for word in ngrams_all:  # making word frequency dictionary\n        count_ngram[word] += 1\n        \n    \n    # converting from dictionary to dataframe data structure\n    \n    df = pd.DataFrame.from_dict(count_ngram, orient='index').reset_index()\n    \n    df = df.rename(columns={'index':'words', 0:'count'})\n    \n    df = df.sort_values(by='count', ascending=False) # highest freq words on the top\n    \n    df = df.head(25) # taking 20 most frequent n-grams\n    \n    df = df.sort_values(by='count')\n    \n    \n    return df","6debbdb0":"# function to plot the word frequency distribution\n\ndef plotNgrams(documents):\n    \n    '''\n    documents : dataframe attribute having reviews text or lists of reviews\n    '''\n    \n    unigrams = generateNgrams(documents, 1)\n    bigrams = generateNgrams(documents, 2)\n    trigrams = generateNgrams(documents, 3)\n    \n    # Set plot figure size\n    fig = plt.figure(figsize = (20, 7))\n    plt.subplots_adjust(wspace=.5)\n    \n    # plot 1\n    ax = fig.add_subplot(131)\n    ax.barh(np.arange(len(unigrams['words'])), unigrams['count'], align='center', alpha=1)\n    ax.set_title('Unigrams')\n    plt.yticks(np.arange(len(unigrams['words'])), unigrams['words'])\n    plt.xlabel('Count')\n    \n    # plot 2\n    ax2 = fig.add_subplot(132)\n    ax2.barh(np.arange(len(bigrams['words'])), bigrams['count'], align='center', alpha=1, color = 'red')\n    ax2.set_title('Bigrams')\n    plt.yticks(np.arange(len(bigrams['words'])), bigrams['words'])\n    plt.xlabel('Count')\n    \n    # plot 3\n    ax3 = fig.add_subplot(133)\n    ax3.barh(np.arange(len(trigrams['words'])), trigrams['count'], align='center', alpha=1, color = 'green')\n    ax3.set_title('Trigrams')\n    plt.yticks(np.arange(len(trigrams['words'])), trigrams['words'])\n    plt.xlabel('Count')\n\n    plt.show()","612d4995":"plotNgrams(dfPosReviews['Reviews'])","49324440":"plotNgrams(dfNegReviews['Reviews'])","eb32386f":"# importing library\nfrom sklearn.feature_extraction.text import TfidfVectorizer","eda15a08":"documents = list(X_train_df['Reviews'])","10be36fe":"vectorizer_uni = TfidfVectorizer(ngram_range=(1,1))\nvectorized_uni = vectorizer_uni.fit_transform(documents)\nX_unigram = vectorized_uni.toarray()","edf71840":"X_unigram","2beb3f41":"X_unigram.sum(axis=0)","8092b916":"print(\"The length of the feature vector is :\",X_unigram.shape[1])","28cf801a":"vectorizer_uni.get_feature_names()[20050:20075]","d22a6b54":"traindf = pd.DataFrame(X_train_df, columns =['Reviews', 'Rating'])\ntraindf.to_csv('SRPtrain.csv', index=False,header=True)","60f3005e":"testdf = pd.concat([X_test, y_test], axis=1)\ntestdf.to_csv('SRPtest.csv')","d0b69f05":"from sklearn.naive_bayes import MultinomialNB","7248c48a":"y_train = X_train_df['Rating']","53939fdc":"# Train the model\nmodel = MultinomialNB()\nmodel.fit(X_unigram,y_train)","107c65b7":"#make predictions using the trained model\ny_pred = model.predict(X_unigram) ","fe1d2460":"from sklearn.metrics import accuracy_score\nprint(\"Accuracy:\", accuracy_score(y_train, y_pred))","9b1545e5":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\ncm = confusion_matrix(y_train, y_pred, labels=model.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=model.classes_)\ndisp.plot()\nplt.show()","62b664e9":"# **2. Data Exploration**","b018429c":"> **5.4.1 Let's look at positive n-grams**","a30e3be4":"# Feature Vector Generation for Bi-grams, Tri-grams will be done in another notebook :)","506f940e":"# **3. Splitting into training and testing data.**\n\n> We could have done this much earlier but it's OK to do it here. We can apply preprocessing on the unseen review text separately. We have already done with that here.\n> **(Train ,Test) ratio is (70%,30%)**","2b90b8f8":"**Brand Name**, **Review Votes** and **Price** doesn't add any value to our objective of the project. We are thinking to impute them.\n\nSo, let's focus on **Review** text. I don't think dropping would be a problem as there are only **62 null** values.","eb204713":"> We will be using the **\"Natural Language Toolkit (NLTK) package\"**, an open-source Python library for natural language processing and **\"re\"** library to perform regular expression operations","4a54aa4c":"**5.1 Review Text Length Distribution**\n\n> Let's check out the length of review text","82171355":"> **6.2.1 Importing required libararies**","8c55bcbd":"> **Most of the reviews have 10-20 words per review :)**","88399d43":"# All the major preprocessing steps are done. Let's do some more visualization adding more features !","326d13d5":"**5.4 Let's do bi-gram and tri-gram analysis as well**","cdafa0cf":"\n**Observation:**\n\n1. ~**68 percent** of the reviews in the dataset are **positive reviews** having ratings >3 (4 and 5)\n2. ~**23 percent** of the reviews in the dataset are **negative reviews** having ratings <3 (1 and 2)\n3. Remaining ~**8 percent** reviews have a rating of 3.\n\nSince a major portion of the reviews are positive, we can say that most of the users have a good experience with their purchases.\n","34ccc6e1":"> **After removing stopwords,**","a2283894":"> **Now let\u2019s see which are the words used mostly in positive reviews and the most used words in negative reviews.**","35827993":"**6.2.2.3 Looking at some of the features**","97e119ee":"> **Most of the review lengths falls in between 0-200. It would be better to consider reviews having reviews.length <= 200 as it will help us to get less sparse feature vectors and save us a lot of memory space.**","eb9ecc7e":"#  **4. Preprocessing text data**","83999d58":"**5.2 Word Count Distribution**\n\n> Let's check out the word count distribution","deceda3b":"**We can conclude that there are a lot misspelled words that are counted differently and contributing to the features and two words are combined together at some places.**\n\n**Done with feature vector generation!!!**","c60b8ef2":"![image.png](attachment:32df3d83-2842-4fc8-9f9a-b41b000a14df.png)","d9a28844":"> Dropping the columns which are irrelevant for now","e14e2c4f":"**6.2.3 Calculating tf-idf values and corresponding matrix of feature vectors for Bi-grams**\n(Due to memory limit exceeded, I'll do Bi-gram, Tri-gram analysis in the another Notebook)","fa35619c":"**4.4 Lemmatization**\n\n*  Lemmatization is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item. \n* Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meanings to one word. \n* Text preprocessing includes both Stemming as well as Lemmatization. Many times people find these two terms confusing. Some treat these two as the same. Actually, lemmatization is preferred over Stemming because lemmatization does morphological analysis of the words.\n\n<pre>\nExamples of lemmatization:\n-> corpora : corpus\n-> better : good\n<\/pre>","a35ee562":" Description of attributes: \n\n  1.  **Product Name** - name of the product, e.g. *Apple iPhone 5 16GB Factory Unlocked GSM Cell Phone - White'*\n  2.  **Brand Name** - name of the brand, e.g. *Apple*\n  3.  **Price** - Price of the product, e.g.  *224.7k*         \n  4.  **Rating** - Rating given by the user, e.g. *5*          \n  5.  **Reviews** - Review of the product given by the user, e.g. *amazing display and long lasting battery*       \n  6.  **Review Votes** - no. of likes on the review, e.g. *3*  ","3f874bc8":"* **We are done with handling all the NULL values.**\n* **Let's move ahead for more exploration.**","62e448ff":"**4.1 Expanding contractions**\n\n> Contractions are words or combinations of words that are shortened by dropping letters and replacing them by an apostrophe. \n\ne.g. as can't -> can not\n \n**Text**: *I'm pleased.I'd suggest you to buy this smartphone.* <br>\n**ExpandedText**: *I am pleased.I would suggest you to buy this smartphone*","d5cadc03":"> Importing the Multinomial Naive Bayes Classifier","dd773ad0":"# 1. Handling NaN\/NA values","a6cf2eb7":"> **Adding review length i.e no. of characters in a review and no.of words in a review attribute to explore more**","0007055c":"**Now, we have two dataframes one with reviews having review.length < 200 and another is the original one**","630dc75a":"A Python library for expanding and creating common English contractions in text. This is very useful for dimensionality reduction by normalizing the text before generating word or character vectors. It performs contraction by simple replacement rules of the commonly used English contractions.","be3c1bc5":"> **Checking for Null values if any**","8ab2ca7e":"**Saving the data to avoid all the computations done to get this.**","43211420":"\n> **5.3.1 WordCloud of negative reviews**","e4713b0e":"> **5.4.2 Let's look at negative n-grams**","f3ce3feb":"> There are numerous uses of doing this. We can use this tokenized form to:\n\n* Count the number of words in the text, \n* Count the frequency of the word, that is, the number of times a particular word is present\n","11f9115f":"# Let's focus on Training Accuracy\n\n**Let's test the training accuracy using the Multinomial Naive Bayes's Algorithm. We are still using imbalanced dataset.**","5498bfe6":"> **It is clearly visible how high the class imbalance is ! We will look at it later**","cdf2c939":"* We have only **62** NULL values in **Reviews** attribute.\n* **0** NULL values in **Rating** attribute.\n* **Brand Name(~65k)**, **Review Votes(~12k)** and **Price(~6k)** attribute have considerable number of NULL values. \n\n","9fe41545":"> **6.2.2.1 Calculating tf-idf values and corresponding matrix of feature vectors for Unigrams**\n\n**fit_transform() method will generate the vocabulary using that the term frequency and inverse document frequency and return the document term matrix i.e stacked feature vectors corresponding to each review**","08968180":"Review length :Training Accuracy(with imbalanced dataset)\n* 200: 72%\n* 100: 75%\n* 30: 80%\n* 20: 82%\n* 15: 82.2%","09d7d2c9":"> **Apparently it is looking all zero but it is actually a sparse representation having more no. of zeros**","7c36b22d":"**References to Kaggle Notebooks**\n\n* [Data Exploration](https:\/\/www.kaggle.com\/benroshan\/sentiment-analysis-amazon-reviews)\n\n* [Regular Expression](https:\/\/www.analyticsvidhya.com\/blog\/2021\/03\/beginners-guide-to-regular-expressions-in-natural-language-processing\/)\n\n* [Text Preprocessing](https:\/\/www.kaggle.com\/sudalairajkumar\/getting-started-with-text-preprocessing)\n\n* [Feature Vector Generation](https:\/\/www.kaggle.com\/paulrohan2020\/tf-idf-tutorial\/notebook)","db823fd0":"**4.2 Tokenization**\n\n> Tokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. Each of these smaller units are called tokens.","c6c4c006":"> NLTK's stopwords list consists of some useful words as well. So we have prepared a list manually adding alphabets(like S,G,T etc because they denote the Product Model as in Samsung Galaxy S, i phone X12, Moto E2 etc).","ef6b1415":"> **6.2 TF-IDF Model**\n\nThere are some potential problems that might arise with the Bag of Words model when it is used on large corpora. Since the feature vectors are based on absolute term frequencies, there might be some terms that occur frequently across all documents and these may tend to overshadow other terms in the feature set. Especially words that don\u2019t occur as frequently, but might be more interesting and effective as features to identify specific categories. This is where TF-IDF comes into the picture. TF-IDF stands for **term frequency-inverse document frequency**. It\u2019s a combination of two metrics, term frequency (tf ) and inverse document frequency (idf ). This technique was originally developed as a metric for ranking search engine results based on user queries and has come to be a part of information retrieval and text feature extraction.\n\n\n* The **term frequency** of a word in a document. There are several ways of calculating this frequency, with the simplest being a raw count of instances a word appears in a document. Then, there are ways to adjust the frequency, by length of a document(or review), or by the raw frequency of the most frequent word in a document.\n\n* The **inverse document frequency** of the word across a set of documents. This means, how common or rare a word is in the entire document set. The closer it is to 0, the more common a word is. This metric can be calculated by taking the total number of documents, dividing it by the number of documents that contain a word, and calculating the logarithm.\n\n","f68d6139":"**After dropping NULL values Reviews attr.:**\n* **Now, imputing NULL Brand Name values with \"Missing\"**\n* **NULL Price and Review Votes values with 0**","9885f387":"**Let's remove the rows where word count is 0 since stop words removal can lead us to that as well**","232bcbbf":"> 2.2 We can also use pie chart to see percentage of the categories.","275b4f7a":"**Implementation with sklearn**","77357392":"**We are done with text-preprocessing and visualization. Let's move towards feature vector generation :)**","27265525":"> **Lets do a final check for NULL values**","2f5c41f9":"> 2.1 Plotting countplot to see the categories count visually.","d93414cf":">**~13 percent** of the reviews have review.length > 200. It's a reasonable count though. But for now let's move ahead and drop them","8621f6ac":"In any machine learning task, cleaning or preprocessing the data is as important as model building if not more. And when it comes to unstructured data like text, this process is even more important.\n\n","a8a12d40":"> **6.2.2.2 Document term matrix**","4395da89":"> **Look at few instances**","39d05f69":"![image.png](attachment:53e41184-4c36-4cd1-a123-10536e517699.png)\n[Image Source](https:\/\/towardsdatascience.com\/tf-term-frequency-idf-inverse-document-frequency-from-scratch-in-python-6c2b61b78558)","ac08532d":"**4.3 Stopwords Removal**\n\n* Stopwords are commonly occuring words in a language like 'the', 'a' and so on. They can be removed from the text most of the times, as they don't provide valuable information for downstream analysis. In cases like Part of Speech tagging, we should not remove them as provide very valuable information about the POS.\nThese stopword lists are already compiled for different languages and we can safely use them. For example, the stopword list for english language from the nltk package can be seen below.","e0446848":"> **5.3.2 WordCloud of positive reviews**","41456903":"**5.3 Segregating positive and negative reviews**","281022c9":"# **Loading the dataset**","0d10efbe":"# **6. Feature vector generation**","b3d97661":"> **Plotting the confusion matrix where review.length <= 200 and Unigrams are used**","aa1418e4":"> Training the model","8aca1ef1":"**Dataset Details**\n\n  This dataset has **Product Name ,Brand Name ,Price ,Rating, Reviews ,Review Votes**   as attributes","3c681cfc":"> **Information about the attributes**","3d90a8f6":"**With tokenization we have removed below mentioned punctuation symbols**\n\n> We contains the following punctuation symbols\n\n> !\"#$%&\\'()*+,-.\/:;<=>?@[\\\\]^_{|}~`\n\n> We need to retain emotion conveying punctuation marks such as :), :)), :(\n\n> We can add or remove more punctuations as per our need.","8c65db0b":"> **6.1 Bag of Words Model**\n\nThis is perhaps the most simple vector space representational model for unstructured text. A vector space model is simply a mathematical model to represent unstructured text (or any other data) as numeric vectors, such that each dimension of the vector is a specific feature\/attribute. The Bag of Words model represents each text document as a numeric vector where each dimension is a specific word from the corpus and the value could be its frequency in the document, occurrence (denoted by 1 or 0), or even weighted values. **The model\u2019s name is such because each document is represented literally as a bag of its own words, disregarding word order, sequences, and grammar.**\n\ne.g,\n> **Review1: It is really of no value for money.(Negative sentiment)**\n\n> **Review2: No! it is really of value for money.(Positive sentiment)**\n\n**BoW model will treat them negative as it does not take an account of the order of words.**"}}