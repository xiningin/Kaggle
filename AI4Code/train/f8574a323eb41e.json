{"cell_type":{"ee82ce2d":"code","6c992a01":"code","3ed5540b":"code","8e0c708a":"code","3fbc6269":"code","3a64ea09":"code","5b805e5e":"code","7dce924e":"code","8b632f2c":"code","985284c4":"code","390a7da4":"code","1aa4b026":"code","be84d4b3":"code","d1026d94":"code","77ad11fd":"code","3062e6da":"code","fabe94af":"code","47c10649":"code","9a7067d0":"code","b27822e4":"code","5177c003":"code","8d90ba07":"code","3c7dd151":"code","1659b610":"code","01726df5":"code","14a6f8e2":"code","d64d02aa":"code","725ca8bc":"code","ae686ff0":"code","7cd1fd1d":"code","29935e0a":"code","bf11b4a4":"code","65f2ec4f":"code","37bbde5a":"code","7d65d1d5":"code","27f38849":"code","ae9d2ddf":"code","6868dfa1":"code","f4b8d495":"code","08711e9c":"code","41aafb77":"code","b735ac49":"markdown","e068aaf5":"markdown","2ceefaac":"markdown","ef3295ff":"markdown","0f15621a":"markdown","2f72d433":"markdown","5d34e52f":"markdown","ed77e9a0":"markdown","2d109ec2":"markdown","08527997":"markdown","53be4faa":"markdown","481b566a":"markdown","d3f47990":"markdown","66313c3b":"markdown","9ae364fc":"markdown","aa60a2b4":"markdown","66ed1e5a":"markdown","7152af8b":"markdown","00c40235":"markdown","b0c7a17a":"markdown","da5b924a":"markdown","0fda8156":"markdown","4daa9202":"markdown","67ee0d72":"markdown","4e826506":"markdown","f7d2e7ab":"markdown","b6d6bb53":"markdown","684e7c1a":"markdown","6846344e":"markdown","cab6124e":"markdown","a97395cb":"markdown","78ab5d2a":"markdown","723be3b0":"markdown","2883e52e":"markdown","89491c16":"markdown","b011d761":"markdown","811e77cc":"markdown","bed4806a":"markdown","d625d4a2":"markdown","859df943":"markdown"},"source":{"ee82ce2d":"# Tratamento de dados\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\n\n# An\u00e1lise de dados\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\n# Bibliotecas auxiliares\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_val_predict\n\n# Estimadores\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import  RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier","6c992a01":"trainAdult = pd.read_csv(\"..\/input\/adult-pmr3508\/train_data.csv\",\n                   na_values=\"?\")\n\ntrainAdult = trainAdult.drop(columns = [\"Id\", \"fnlwgt\"])\n\ntestAdult = pd.read_csv(\"..\/input\/adult-pmr3508\/test_data.csv\",\n                        na_values=\"?\")\n\ntestAdult = testAdult.drop(columns = [\"Id\", \"fnlwgt\"])","3ed5540b":"for col in testAdult.columns:\n    trainAdult[col].fillna(trainAdult[col].mode()[0], inplace=True)\n    testAdult[col].fillna(testAdult[col].mode()[0], inplace=True)","8e0c708a":"analise = trainAdult.copy()\n\nanalise[\"capital\"] = analise[\"capital.gain\"] - analise[\"capital.loss\"]\nanalise.drop(columns=[\"capital.gain\", \"capital.loss\"])\nanalise.head(1)","3fbc6269":"num_cols = [\"age\", \"education.num\", \"hours.per.week\"]\n\nfor n in num_cols:\n    plt.figure(figsize=(6,3))\n    analise[analise['income']=='<=50K'][n].hist()\n    analise[analise['income']=='>50K'][n].hist(color='red')\n    plt.title(n)\n    plt.show()","3a64ea09":"plt.figure(figsize=(6,3))\nanalise[analise['income']=='<=50K'][\"capital\"].hist()\nanalise[analise['income']=='>50K'][\"capital\"].hist(color='red')\nplt.title(\"Capital\")\nplt.show()","5b805e5e":"analise[\"income\"] = LabelEncoder().fit_transform(analise['income'])","7dce924e":"sns.catplot(y=\"workclass\", x=\"income\", kind=\"bar\", data=analise)","8b632f2c":"trainAdult[\"workclass\"].value_counts(normalize=True)","985284c4":"sns.catplot(y=\"education\", x=\"income\", kind=\"bar\", data=analise)","390a7da4":"sns.catplot(y=\"marital.status\", x=\"income\", kind=\"bar\", data=analise)","1aa4b026":"sns.catplot(y=\"relationship\", x=\"income\", kind=\"bar\", data=analise)","be84d4b3":"sns.catplot(y=\"occupation\", x=\"income\", kind=\"bar\", data=analise)","d1026d94":"sns.catplot(y=\"race\", x=\"income\", kind=\"bar\", data=analise)\ntrainAdult[\"race\"].value_counts(normalize=True)","77ad11fd":"sns.catplot(y=\"sex\", x=\"income\", kind=\"bar\", data=analise)","3062e6da":"sns.catplot(y=\"native.country\", x=\"income\", kind=\"bar\", data=analise)","fabe94af":"analise[\"native.country\"].value_counts()","47c10649":"trainAdult[\"income\"] = LabelEncoder().fit_transform(trainAdult['income'])","9a7067d0":"xTrain = trainAdult.drop(columns=[\"income\"])\nyTrain = trainAdult[\"income\"]","b27822e4":"xTrain = xTrain.drop(columns=[\"education\", \"native.country\"])","5177c003":"naoPrivado = [\"Self-emp-not-inc\", \"Local-gov\", \"State-gov\", \"Self-emp-inc\", \"Federal-gov\", \"Without-pay\", \"Never-worked\"]\n\nxTrain.replace(naoPrivado,'N\u00e3o-privado',inplace=True)","8d90ba07":"naoBranco = [\"Black\", \"Asian-Pac-Islander\", \"Amer-Indian-Eskimo\", \"Other\"]\n\nxTrain.replace(naoBranco,'N\u00e3o branco',inplace=True)","3c7dd151":"casado = [\"Married-civ-spouse\", \"Married-AF-spouse\"]\nsolteiro = [\"Divorced\", \"Separated\", \"Widowed\", \"Married-spouse-absent\", \"Never-married\"]\nxTrain.replace(casado,'Casado',inplace=True)\nxTrain.replace(solteiro,'Solteiro',inplace=True)\nxTrain[\"marital.status\"] = LabelEncoder().fit_transform(xTrain['marital.status'])","1659b610":"relacionamento = [\"Husband\", \"Wife\"]\nforaRelacionamento = [\"Own-child\", \"Not-in-family\", \"Unmarried\", \"Other-relative\"]\n\nxTrain.replace(relacionamento,'Em-relacionamento',inplace=True)\nxTrain.replace(foraRelacionamento,'Foral-relacionamento',inplace=True)","01726df5":"xTrain[\"workclass\"] = LabelEncoder().fit_transform(xTrain['workclass'])\nxTrain[\"race\"] = LabelEncoder().fit_transform(xTrain['race'])\nxTrain[\"occupation\"] = LabelEncoder().fit_transform(xTrain['occupation'])\nxTrain[\"relationship\"] = LabelEncoder().fit_transform(xTrain['relationship'])\nxTrain[\"sex\"] = LabelEncoder().fit_transform(xTrain['sex'])","14a6f8e2":"sc = StandardScaler()\n\nxTrain[[\"age\", \"education.num\", \"hours.per.week\"]] = sc.fit_transform(xTrain[[\"age\", \"education.num\", \"hours.per.week\"]])\nxTrain[['capital.gain', 'capital.loss']] = RobustScaler().fit_transform(xTrain[['capital.gain', 'capital.loss']])","d64d02aa":"xTrain.head(2)","725ca8bc":"xTest = testAdult\n\nxTest = xTest.drop(columns=[\"native.country\", \"education\"])\n\nnaoPrivado = [\"Self-emp-not-inc\", \"Local-gov\", \"State-gov\", \"Self-emp-inc\", \"Federal-gov\", \"Without-pay\", \"Never-worked\"]\n\nxTest.replace(naoPrivado,'N\u00e3o-privado',inplace=True)\nxTest[\"workclass\"] = LabelEncoder().fit_transform(xTest['workclass'])\n\nnaoBranco = [\"Black\", \"Asian-Pac-Islander\", \"Amer-Indian-Eskimo\", \"Other\"]\n\nxTest.replace(naoBranco,'N\u00e3o branco',inplace=True)\nxTest[\"race\"] = LabelEncoder().fit_transform(xTest['race'])\n\ncasado = [\"Married-civ-spouse\", \"Married-AF-spouse\"]\nsolteiro = [\"Divorced\", \"Separated\", \"Widowed\", \"Married-spouse-absent\", \"Never-married\"]\nxTest.replace(casado,'Casado',inplace=True)\nxTest.replace(solteiro,'Solteiro',inplace=True)\nxTest[\"marital.status\"] = LabelEncoder().fit_transform(xTest['marital.status'])\n\nrelacionamento = [\"Husband\", \"Wife\"]\nforaRelacionamento = [\"Own-child\", \"Not-in-family\", \"Unmarried\", \"Other-relative\"]\n\nxTest.replace(relacionamento,'Em-relacionamento',inplace=True)\nxTest.replace(foraRelacionamento,'Foral-relacionamento',inplace=True)\nxTest[\"relationship\"] = LabelEncoder().fit_transform(xTest['relationship'])\n\nxTest[\"occupation\"] = LabelEncoder().fit_transform(xTest['occupation'])\n#xTest[\"relationship\"] = LabelEncoder().fit_transform(xTest['relationship'])\nxTest[\"sex\"] = LabelEncoder().fit_transform(xTest['sex'])\n\nsc = StandardScaler()\n\nxTest[[\"age\", \"education.num\", \"hours.per.week\"]] = sc.fit_transform(xTest[[\"age\", \"education.num\", \"hours.per.week\"]])\nxTest[['capital.gain', 'capital.loss']] = RobustScaler().fit_transform(xTest[['capital.gain', 'capital.loss']])","ae686ff0":"score = {}","7cd1fd1d":"# Cria\u00e7\u00e3o do modelo\ngnb = GaussianNB()\n\n# Score\nscore['Naive Bayes'] = cross_val_score(gnb, xTrain, yTrain).mean()\n\nprint(score['Naive Bayes'])","29935e0a":"# Cria\u00e7\u00e3o do modelo\nlr = LogisticRegression(solver='liblinear')\n\n# Cria\u00e7\u00e3o dos par\u00e2metros\nparameters = {'penalty':['l1', 'l2'], 'C':[*range(1, 11)]}\n\n# GridSearch\ngs = GridSearchCV(lr, parameters)\ngs.fit(xTrain, yTrain)\nscore['Logistic Regression'] = gs.best_score_\n\nprint(score['Logistic Regression'])","bf11b4a4":"# Cria\u00e7\u00e3o do modelo\nsvc = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n\nscore['Support Vector Classfier'] = cross_val_score(svc, xTrain, yTrain).mean()\n\nprint(score['Support Vector Classfier'])","65f2ec4f":"# Cria\u00e7\u00e3o do modelo\ndtc = DecisionTreeClassifier()\n\n# Score\nscore['Decision Tree'] = cross_val_score(dtc, xTrain, yTrain).mean()\n\nprint(score['Decision Tree'])","37bbde5a":"# Cria\u00e7\u00e3o do modelo\nrfc = RandomForestClassifier()\n\n# Score\nscore['Random Forest'] = cross_val_score(rfc, xTrain, yTrain).mean()\n\nprint(score['Random Forest'])","7d65d1d5":"# Cria\u00e7\u00e3o do modelo\ngbc = GradientBoostingClassifier(n_estimators=200, learning_rate=0.05, min_samples_split=625, min_samples_leaf=35, \n                                            max_depth=10, max_features='sqrt', subsample=0.8, random_state=42)\n\n# Score\nscore['Gradient Boosting'] = cross_val_score(gbc, xTrain, yTrain).mean()\n\nprint(score['Gradient Boosting'])","27f38849":"# Cria\u00e7\u00e3o do modelo\nnn = MLPClassifier()\n\n# Score\nscore['Neural Network'] = cross_val_score(nn, xTrain, yTrain).mean()\n\nprint(score['Neural Network'])","ae9d2ddf":"# Cria\u00e7\u00e3o do modelo\nknn = KNeighborsClassifier()\n\n#Grid Search\ngs = GridSearchCV(knn, {'n_neighbors':[*range(25, 36)]})\n\n# Melhor score\ngs.fit(xTrain, yTrain)\nscore['KNN'] = gs.best_score_\n\nprint(score['KNN'])","6868dfa1":"score = dict(reversed(sorted(score.items(), key=lambda item: item[1])))\n\nfor key, value in score.items():\n    print(key, '-', value)","f4b8d495":"sns.heatmap(confusion_matrix(yTrain, cross_val_predict(gbc, xTrain, yTrain, cv=10)), annot=True)","08711e9c":"sns.heatmap(confusion_matrix(yTrain, cross_val_predict(gnb, xTrain, yTrain, cv=10)), annot=True)","41aafb77":"# Treinar\ngbc.fit(xTrain, yTrain)\n# Estimar\nY = gbc.predict(xTest)\n#Salvar\nresultados = []\n\nfor resultado in Y:\n    if resultado == 0:\n        resultados.append('<=50K')\n    else:\n        resultados.append('>50K')\n\narquivo = pd.DataFrame()\narquivo[0] = testAdult.index\narquivo[1] = resultados\narquivo.columns = ['Id', 'income']\narquivo.head()\narquivo.to_csv('submission.csv', index = False)","b735ac49":"## Atributos quantitativos\n\nOs tr\u00eas primeiros atributos que iremos analisar ser\u00e3o a idade, anos de estudo e quantidade de horas trabalhadas por semana:","e068aaf5":"Nesses tr\u00eas gr\u00e1ficos acima, podemos ver claramente que nas idades mais altas e entre as pessoas com mais anos de educa\u00e7\u00e3o a densidade de indiv\u00edduos com renda acima de 50 mil d\u00f3lares \u00e9 maior. A quantidade de horas trabalhadas tamb\u00e9m influencia positivamente na renda.","2ceefaac":"Em seguida, dividir o dataframe em duas partes. A primeira dever\u00e1 conter as features e a segunda a vari\u00e1vel que desejamos estimar.","ef3295ff":"## Neural Network","0f15621a":"Por fim, podemos verificar que o pa\u00eds de origem da pessoa possui impacto em sua renda. Pessoas de pa\u00edses desenvolvidos como Inglaterra, Fran\u00e7a e Taiwan possuem uma densidade maior de indiv\u00edduos com renda acima de 50 mil d\u00f3lares do que imigrantes do M\u00e9xico e Guatemala, pa\u00edses mais pobres. Entretanto, isso n\u00e3o \u00e9 uma regra universal.\n\nPara a maior parte dos casos, o n\u00famero de amostras para cada pa\u00eds \u00e9 menor que 100, enquanto existem quase 30 mil americanos na nossa base de dados. Por esse motivo, iremos retirar essa categoria do nosso modelo.","2f72d433":"# An\u00e1lise de dados\n\nNessa terceira etapa, iremos utilizar as bibliotecas MatPlotLib e Seaborn para realizar uma an\u00e1lise dos dados dispon\u00edveis. Assim, poderemos saber como cada uma das vari\u00e1veis pode impactar na renda do indiv\u00edduo. Essa an\u00e1lise tamb\u00e9m \u00e9 importante para identificar quais vari\u00e1veis utilizaremos e como devemos trat\u00e1-las.\n\nTamb\u00e9m, iremos unir as colunas \"capital.gain\" e \"capital.loss\" em uma que represente o saldo. Dessa forma, a an\u00e1lise num\u00e9rica se torna mais f\u00e1cil.","5d34e52f":"## Naive Bayes","ed77e9a0":"Escolaridade:","2d109ec2":"Por meio das matrizes acima, podemos verificar que nos dois casos a quantidade de *True negatives* foi similar. Entretanto, a quantidade de *True positives* foi quase o dobro utilizando-se o Gradient Boosting.\n\nTamb\u00e9m, podemos calcular a precis\u00e3o e a revoca\u00e7\u00e3o para ambos.\n\nNaive Bayes:\n\nPrecis\u00e3o | Revoca\u00e7\u00e3o\n:---------| ----------:\n35,9% | 68,3%\n\nGradient Boosting:\n\nPrecis\u00e3o | Revoca\u00e7\u00e3o\n:---------| ----------:\n65,4% | 77,3%\n\nDiante desses dados, fica muito claro que devemos utilizar o Gradient Boosting com os hiperpar\u00e2metros encontrados. Por isso iremos utiliz\u00e1-lo para gerar o arquivo de submiss\u00e3o.\n\n# Submiss\u00e3o","08527997":"Iremos fazer o mesmo com a feature \"race\", da maneira que foi explicada na an\u00e1lise explorat\u00f3ria.","53be4faa":"Workclass:","481b566a":"## Gradient Boosting","d3f47990":"Agora, iremos retirar as features \"Education\", \"Native country\" pelos motivos expostos na se\u00e7\u00e3o anterior. Tamb\u00e9m retiraremos \"income\" j\u00e1 que agora faz parte de outro dataframe.","66313c3b":"## Regress\u00e3o Log\u00edstica\n\nIremos testar o comportamento da regress\u00e3o log\u00edstica. Para isso, utilizaremos o GridSearch para testar v\u00e1rios par\u00e2metros e descobrir os que apresentam melhor score. Faremos um processo parecido com os demais estimadores.","9ae364fc":"Resultado final:","aa60a2b4":"Podemos ver claramente que o Gradient Boosting possui a maior acur\u00e1cia para a base de dados selecionada. Ele conseguiu superar o KNN criado no projeto anterior. \n\nPara entender melhor o desempenho desse modelo, podemos criar uma matriz de confus\u00e3o dele e compar\u00e1-la com a do Naive Bayes, que atingiu o menor desempenho.","66ed1e5a":"# Conclus\u00f5es\n\nA acur\u00e1cia encontrada para cada classificador foi:","7152af8b":"As \u00faltimas duas tarefas consistem em normalizar a escala das vari\u00e1veis num\u00e9ricas e retirar os outliers das categorias \"capital.gain\" e \"capital.loss\".","00c40235":"# Estimadores\n\nAgora, iremos realizar o teste com v\u00e1rios estimadores diferentes para descobrir o mais adequado para nossa base de dados. Os algoritmos escolhidos forma *Naive Bayes, Logistic Regression, Support Vector Classifier, Decision Tree, Random Forest, Gradient Boosting e Neural Network*. Tamb\u00e9m, iremos refazer o modelo KNN do projeto anterior a t\u00edtulo de compara\u00e7\u00e3o. O estimador Linear Discriminant Analysis n\u00e3o foi utilizado porque seu comportamento se assemelha a uma regress\u00e3o log\u00edstica para labels bin\u00e1rios.\n\nTamb\u00e9m, faremos uso da classe GridSearch para que possamos estimar os melhores hiperpar\u00e2metros em cada modelo. \n\nPor fim, iremos criar um dicion\u00e1rio para armazenar o valor dos scores para posterior compara\u00e7\u00e3o.","b0c7a17a":"## Random Forest","da5b924a":"Al\u00e9m disso, no gr\u00e1fico acima tamb\u00e9m fica evidente a desigualdade entre homens e mulheres na sociedade.","0fda8156":"# Leitura de dados\n\nPrecisamos realizar a leitura dos dados de treino e teste do dataset. Faremos isso utilizando a biblioteca Pandas. No processo, iremos retirar as colunas ID e FnlWgt por n\u00e3o serem pertinentes ao nosso prop\u00f3sito.","4daa9202":"# PMR3508 - Aprendizado de M\u00e1quina e Reconhecimento de Padr\u00f5es\n\nEsse Jupyter tem como objetivo avaliar o desempenho de alguns estimadores e seus respectivos hiperparametros no dataset Adult.\n\n# Bibliotecas\n\nO primeiro passo \u00e9 importar algumas bibliotecas que iremos utilizar ao longo deste Notebook. S\u00e3o elas Pandas, Numpy, Seaborn, MatPlotLib e Scikit-Learn.","67ee0d72":"Valores positivos de saldo de capital contribuem para renda da pessoa analisada. Entretanto, podemos verificar que existem alguns valores muito distantes da m\u00e9dia, por isso, precisaremos retirar esses outliers.\n\n## Atributos qualitativos\n\nPara realizar a an\u00e1lise das categorias qualitativas, iremos utilizar um label na coluna \"income\". Caso a amostra possua renda inferior ou igual a cinquenta mil d\u00f3lares receber\u00e1 o label 0, caso contr\u00e1rio 1. Isso facilitar\u00e1 nossa an\u00e1lise gr\u00e1fica dos valores n\u00e3o num\u00e9ricos.","4e826506":"## Support Vector Classifier","f7d2e7ab":"A an\u00e1lise dos relacionamentos corrobora as afirma\u00e7\u00f5es feitas acima. Pessoas casadas tendem a ganhar mais. Assim como na situa\u00e7\u00e3o anterior, iremos dividir em dois grupos: \"em um relacionamento\" e \"fora de um relacionamento\".\n\nOcupa\u00e7\u00e3o:","b6d6bb53":"Podemos ver que o tipo de trabalho do indiv\u00edduo altera a probabilidade dele receber mais ou menos que o valor limite. Dois casos interessantes s\u00e3o os dos trabalhadores sem rendimento e as pessoas que nunca trabalharam. Evidentemente, a probabilidade dessas duas categorias possuir renda superior a cinquenta mil \u00e9 zero.\n\nAlgo que devemos nos atentar \u00e9 que a maioria absoluta das pessoas trabalham no setor privador. Por isso, precisaremos fornecer um tratamento especial para essa feature.","684e7c1a":"Agora, devemos realizar o mesmo processo para base de teste.","6846344e":"## Decision Tree","cab6124e":"As pessoas ser\u00e3o divididas entre \"em relacionamento\" e \"fora de relacionamento\".","a97395cb":"\u00c9 evidente que, devido a quest\u00f5es hist\u00f3ricas e sociais, existe uma desigualdade entre os brancos e outros. Tamb\u00e9m, em alguns casos, o n\u00famero de indiv\u00edduos amostrados \u00e9 baixo. Por essas duas quest\u00f5es, iremos dividir essa categoria em brancos e n\u00e3o brancos como forma de quantificar essa desigualdade.","78ab5d2a":"Tamb\u00e9m, \u00e9 poss\u00edvel visualizar que o n\u00edvel de escolaridade do indiv\u00edduo possui enorme impacto na sua renda. Quase 80% das pessoas com doutorado est\u00e3o na faixa de renda mais elevada. Como essa \u00e9 uma vari\u00e1vel n\u00e3o num\u00e9rica e n\u00f3s possu\u00edmos outra que representa a mesma coisa (\"education.num\"), iremos descart\u00e1-la.\n\nEstado civil:","723be3b0":"O gr\u00e1fico acima nos mostra que a propor\u00e7\u00e3o de pessoas com renda acima de 50 mil d\u00f3lares \u00e9 maior entre as pessoas que mant\u00eam uma uni\u00e3o. Posteriormente, iremos dividir esse grupo em duas categorias: casados e solteiros. Como, na divis\u00e3o atual, mais de uma categoria pode representar uma situa\u00e7\u00e3o parecida, a nova divis\u00e3o favorece a nossa an\u00e1lise.","2883e52e":"Em seguida, devemos transformar as vari\u00e1veis categ\u00f3ricas em labels.","89491c16":"Podemos dividir o estado civil em \"casado\" e \"solteiro\"","b011d761":"A ocupa\u00e7\u00e3o do indiv\u00edduo tamb\u00e9m parece impactar de maneira significativa na renda do indiv\u00edduo. Enquanto metade das amostras com cargos executivos recebem mais que o valor limite estabelecido, praticamente ningu\u00e9m que trabalha com servi\u00e7os dom\u00e9sticos est\u00e1 nessa categoria.","811e77cc":"## Dados faltantes\n\nCom o prop\u00f3sito de melhorar a acur\u00e1cia dos nossos modelos, iremos incluir os dados faltantes. Para isso iremos substituir os campos vazios, representados por \"?\" pela moda da sua respectiva categoria.","bed4806a":"Como a maioria da popula\u00e7\u00e3o trabalha no setor privado, podemos dividir as pessoas entre setor privado e n\u00e3o privado.","d625d4a2":"## KNN\n\nPor fim, iremos comparar com o classificador KNN que fizemos no \u00faltimo projeto.","859df943":"# Tratamento de dados\n\nAntes de utilizar os dados no modelo, \u00e9 necess\u00e1rio realizar um tratamento de dados com o que descobrimos na an\u00e1lise explorat\u00f3ria.\n\nPrimeiramente, iremos transformar a vari\u00e1vel income em um label, assim como fizemos na an\u00e1lise de dados."}}