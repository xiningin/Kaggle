{"cell_type":{"5963d34b":"code","54b56a49":"code","17807632":"code","70bf1117":"code","89ed1673":"code","f3d065a9":"code","398a9421":"code","3e9907af":"code","afba1a2b":"code","96286111":"code","70a00e7e":"code","cf7b71f9":"code","a7daa35b":"code","6f30e049":"code","f7bdcf5d":"code","c84d4416":"code","975b8f0a":"code","975cf9d9":"code","b95809e5":"code","4f4c4a48":"code","e456b3d6":"code","0b0a07a4":"code","d3805dbe":"code","49d01fc9":"code","56482659":"code","5c736245":"code","2f1f3eb8":"code","217a4a2a":"code","23d55ff0":"code","6e2901e2":"code","1643c03f":"code","247409b4":"code","bdf3d9ed":"code","e0197638":"code","3efec85b":"code","17231471":"code","3453aabd":"code","c47293bb":"code","dd53dfb3":"code","b2ca15ce":"code","5dda968e":"code","d44cf030":"markdown","fb526ae7":"markdown","a91d4549":"markdown","9fc80c1b":"markdown","8fdc2cb5":"markdown","0ced2b9c":"markdown","a59e7a49":"markdown","a76e59af":"markdown","c7942713":"markdown","2974a68a":"markdown","bf119f32":"markdown","2684db9e":"markdown","773a8331":"markdown","c299a8c5":"markdown"},"source":{"5963d34b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","54b56a49":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\nimport random","17807632":"snap=pd.read_csv('\/kaggle\/input\/10k-snapchat-reviews\/Snapchat_app_store_reviews.csv')","70bf1117":"snap.head(10)","89ed1673":"snap.info()","f3d065a9":"pd.isnull(snap).sum()","398a9421":"sns.countplot(x='rating',data=snap)","3e9907af":"words=snap['review'].tolist()\nwords=''.join(words)\nplt.figure(figsize=(12,12))\nplt.imshow(WordCloud().generate(words))\n","afba1a2b":"snap['sentiment'] =''","96286111":"snap['label'] = np.nan\nsnap['label']= snap['rating'].apply(lambda x: 1 if (x > 3) else 0) \nsnap","70a00e7e":"snap['sentiment']= snap['rating'].apply(lambda x: 'good' if (x > 3) else 'bad') ","cf7b71f9":"snap","a7daa35b":"positive = snap.loc[(snap['sentiment'] == 'good')]\npositive","6f30e049":"negative = snap.loc[(snap['sentiment'] == 'bad')]\nnegative","f7bdcf5d":"words=positive['review'].tolist()\nwords=''.join(words)\nplt.figure(figsize=(12,12))\nplt.imshow(WordCloud().generate(words))","c84d4416":"words=negative['review'].tolist()\nwords=''.join(words)\nplt.figure(figsize=(12,12))\nplt.imshow(WordCloud().generate(words))","975b8f0a":"words=snap['review'].tolist()\nwords=''.join(words)\nplt.figure(figsize=(12,12))\nplt.imshow(WordCloud().generate(words))","975cf9d9":"snap[\"data\"] = snap.apply(lambda x: str(x[\"title\"]) + \" \" + str(x[\"review\"]), axis=1)\nsnap[\"split\"] = snap.apply(lambda x: \"train\" if random.randrange(0,100) > 10 else \"valid\", axis=1)","b95809e5":"labels = pd.DataFrame(snap['label']).to_numpy()","4f4c4a48":"labels","e456b3d6":"labels.shape","0b0a07a4":"snap['data'][0]","d3805dbe":"import re\nsnap1 = []\nfor i in range(len(snap['data'])):\n    new=re.sub('[^a-zA-Z]', ' ', snap['data'][i])\n    snap1.append(new)","49d01fc9":"snap1[0]","56482659":"snap2 = []\nfor i in range(len(snap1)):\n    new1=snap1[i].strip()\n    snap2.append(new1)","5c736245":"snap2[0]","2f1f3eb8":"snap3 = []\nfor i in range(len(snap2)):\n    new2=snap2[i].lower()\n    snap3.append(new2)","217a4a2a":"snap3[0]","23d55ff0":"from nltk.corpus import stopwords\nimport string\nstop_words = set(stopwords.words('english'))\npunctuation = string.punctuation\nname = ['snapchat', 'app', 'account', 'now', 'phone', 'snap', 'people']\n\nsnap4 = []\n\nfor i in snap3:\n    sentence = i.split()\n    s_p=[]\n    for cor in sentence:\n        if cor not in stop_words and cor not in punctuation and cor not in name:\n            s_p.append(cor)\n    s_p=\" \".join(s_p)    \n    snap4.append(s_p)\n","6e2901e2":"snap4[0]","1643c03f":"texts=[]\nfor x in range(0,9559):\n    texts.append(snap4[x])","247409b4":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nmaxlen = 200\ntraining_samples = 500\nvalidation_samples = 1000\nmax_words = 1000\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(texts)\nsequences = tokenizer.texts_to_sequences(texts)\nword_index = tokenizer.word_index\nprint('Znaleziono %s unikatowych token\u00f3w'%len(word_index))\ndata = pad_sequences(sequences,maxlen=maxlen)\nlabels = np.asarray(labels)\nprint(data.shape)\nprint(labels.shape)\nindicies=np.arange(data.shape[0])\ndata = data[indicies]\nlabels=labels[indicies]\nx_train = data[:training_samples]\ny_train = labels[:training_samples]\nx_val = data[training_samples:training_samples + validation_samples]\ny_val = labels[training_samples:training_samples + validation_samples]","bdf3d9ed":"glove_dir=\"\/kaggle\/input\/glovedataset\/\"\n\nembedding_index = {}\nf = open(os.path.join(glove_dir,'glove.6B.100d.txt'),encoding='utf8')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:],dtype='float32')\n    embedding_index[word] = coefs\nf.close()\nprint('Found %s word vectors ' % len(embedding_index))","e0197638":"embedding_dim = 100\nembedding_matrix = np.zeros((max_words,embedding_dim))\n\nfor word, i in word_index.items():\n    embedding_vector = embedding_index.get(word)\n    if i < max_words:\n        if embedding_vector is not None:\n            embedding_matrix[i]=embedding_vector\n    ","3efec85b":"from keras.models import Sequential\nfrom keras.layers import Embedding,Flatten,Dense,Dropout\n\nmodel = Sequential()\nmodel.add(Embedding(max_words,embedding_dim,input_length=maxlen))\nmodel.add(Flatten())\nmodel.add(Dense(32,activation='relu'))\nmodel.add(Flatten())\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.summary()","17231471":"from keras.models import Sequential\nfrom keras.layers import Embedding,Flatten,Dense,Dropout\n\nmodel = Sequential()\nmodel.add(Embedding(max_words,embedding_dim,input_length=maxlen))\nmodel.add(Flatten())\nmodel.add(Dense(32,activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\nhistory = model.fit(x_train,y_train,epochs=8,batch_size=32,validation_data=(x_val,y_val))","3453aabd":"history_dict = history.history\nhistory_dict.keys()","c47293bb":"accuracy = history.history['acc']\nval_accuracy = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1,len(accuracy)+1)\nplt.plot(epochs,loss,'g',label='Strata treningowa')\nplt.plot(epochs,val_loss,'b',label='Strata walidacji')\nplt.title('Strata treningowa i walidacji')\nplt.xlabel('Epoki')\nplt.ylabel('Strata')\nplt.legend()\nplt.show()","dd53dfb3":"plt.clf()\nplt.plot(epochs,accuracy,'g',label = 'Dok\u0142adno\u015b\u0107 trenowania')\nplt.plot(epochs,val_accuracy,'b',label = 'Dok\u0142adno\u015b\u0107 walidacji')\nplt.title('Dok\u0142adno\u015b\u0107 trenowania i walidacji')\nplt.xlabel('Epoki')\nplt.ylabel('Strata')\nplt.legend()\nplt.show()","b2ca15ce":"model.layers[0].set_weights([embedding_matrix])\nmodel.layers[0].trainable = False","5dda968e":"model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\nhistory = model.fit(x_train,y_train,epochs=10,batch_size=32,validation_data=(x_val,y_val))","d44cf030":"**Importuemy potrzebne biblioteki**","fb526ae7":"Sprawdzenie nazw kluczy i utworzenie wykres\u00f3w pokazuj\u0105cych zale\u017cno\u015bci pomi\u0119dzy strat\u0105 treningow\u0105 i walidacji oraz dok\u0142adno\u015bci\u0105 trenowania i walidacji ","a91d4549":"Utworzenie kolumn sentiment i label oraz wype\u0142nienie ich warto\u015bciami zale\u017cnie od warto\u015bci kolumny rating\nZa\u0142o\u017cy\u0142em \u017ce ocena poni\u017cej 4 jest ocen\u0105 z\u0142\u0105, jako \u017ce przegl\u0105daj\u0105c kilka rekord\u00f3w z ocena 3 s\u0105 to raczej\nrecenzje zawieraj\u0105ce jakies uwagi, problemy.","9fc80c1b":"Przyjrzyjmy sie naszemu zbiorowi danych ","8fdc2cb5":"Przyjrzenie si\u0119 danym ","0ced2b9c":"Wyci\u0105gni\u0119cie uprzednio utworzonej kolumny label i przekszta\u0142cenie jej w tablice numpy","a59e7a49":"Sprawdzenie wymiar\u00f3w label","a76e59af":"Jak wida\u0107 model zaczyna ulega\u0107 nadmiernemu dopasowaniu. Aktualnie pracuje nad regularyzacj\u0105 tego modelu","c7942713":"Wykres pokazuj\u0105cy ilo\u015b\u0107 poszczeg\u00f3lnych ocen ","2974a68a":"Utworzenie modelu. Jako \u017ce nasz model ma do czynienia tylko z dwoma klasami u\u017cy\u0142em w ostatniej warstwie metody aktywacyjnej **sigmoid** oraz jako funkcje straty u\u017cy\u0142em **binary crossentropy**","bf119f32":"Przedstawione poni\u017cej p\u0119tle odpowiadaj\u0105 kolejno za:\n-usuwanie znak\u00f3w innych ni\u017c litery\n-usuwanie spacji\n-zamiana na ma\u0142e litery\n-usuni\u0119cie stopwords czyli s\u0142\u00f3w, kt\u00f3re nie maj\u0105 wp\u0142ywu na znaczenie recenzji a tylko niepotrzebnie zanieczyszczaj\u0105 dane \n**Wzorowa\u0142em si\u0119 tutaj na kodzie innego u\u017cytkownika kaggle**","2684db9e":"Sprawdzamy czy nie ma \u017cadnych pustych rekord\u00f3w","773a8331":"Utworzenie dw\u00f3ch osobnych zbior\u00f3w positive\/negative i utworzenie mapy s\u0142\u00f3w dla poszczeg\u00f3lnych zbior\u00f3w","c299a8c5":"Tutaj ustawiam d\u0142ugo\u015b\u0107 recenzji, rozmiar pr\u00f3bek, korzystam z narz\u0119dzia tokenizer\nTworze zbiory treningowe i walidacyjne.\n**Od tego momentu posi\u0142kuje sie ksi\u0105\u017ck\u0105 \"Deep Learning. Praca z j\u0119zykiem Python i bibliotek\u0105 Keras\"**"}}