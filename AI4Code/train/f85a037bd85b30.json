{"cell_type":{"ebe66344":"code","757e9a94":"code","7dc11628":"code","868e893a":"code","bfe079a4":"code","e3ce35fa":"code","f02d7a27":"code","42c0af29":"code","d1ec5daa":"code","c4cf55ef":"code","c757a684":"code","59ffdf92":"code","b82f7dd9":"code","27b0f485":"code","8c98da1f":"code","10718522":"code","19408a32":"code","a800907e":"code","e14853f2":"code","108d5900":"code","4a372c78":"code","95635e45":"code","ea60d112":"code","7d0cea97":"code","4b2f067f":"code","90fcbec2":"code","6019df92":"code","83cdac35":"code","9dccfdfd":"code","aeb7f311":"code","9b8fdcdb":"code","f49fc33b":"markdown","6ebab451":"markdown","7de483b3":"markdown","757f740e":"markdown","08040358":"markdown","520ad73c":"markdown","3d44ff29":"markdown","49bdee63":"markdown","e4df83fa":"markdown","41595624":"markdown"},"source":{"ebe66344":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport itertools\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import make_circles\nfrom mlxtend.plotting import plot_decision_regions\nimport seaborn.apionly as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n%matplotlib notebook\nsns.set()","757e9a94":"rng = np.random.RandomState(0)\niris = pd.read_csv(\"..\/input\/iris\/Iris.csv\", index_col=[\"Id\"])\nX_circle, y_cicle = make_circles(n_samples=1000, random_state=123, noise=0.1, factor=0.2)\nX_regres = 100 * rng.rand(100, 1) + 10\nX_xor = rng.randn(300, 2)","7dc11628":"iris","868e893a":"sns.pairplot(iris, hue=\"Species\")","bfe079a4":"data = {\n    \"regresi\u00f3n\": {\n        \"X\": X_regres,\n        \"y\": 200 + 1500 * X_regres[:, 0] + rng.rand(X_regres.shape[0]) * 50000\n    },\n    \"regresi\u00f3n2\": {\n        \"X\": X_regres,\n        \"y\": 200 + X_regres[:, 0] ** 4 + rng.rand(X_regres.shape[0]) * 50000000\n    },\n    \"iris\": {\n        \"X\": iris.drop(\"Species\", axis=1).values[:, [1, 2]],\n        \"y\": iris.Species.astype(\"category\").cat.codes.values\n    },\n    \"xor\": {\n        \"X\": X_xor,\n        \"y\": np.array(np.logical_xor(X_xor[:, 0] > 0, X_xor[:, 1] > 0), dtype=int)\n    },\n    \"circulo\": {\n        \"X\": X_circle,\n        \"y\": y_cicle\n    }   \n\n}","e3ce35fa":"from sklearn.linear_model import LinearRegression\nplt.rcParams['figure.figsize'] = (15, 12)\n\ntipo = \"regresi\u00f3n\"\nmodel = LinearRegression()\nmodel.fit(data[tipo][\"X\"], data[tipo][\"y\"])\nplt.scatter(data[tipo][\"X\"], data[tipo][\"y\"])\nplt.plot(np.linspace(0, 120), model.predict(np.linspace(0, 120)[:, None]))\nplt.show()","f02d7a27":"tipo = \"regresi\u00f3n2\"\nmodel = LinearRegression()\nmodel.fit(data[tipo][\"X\"], data[tipo][\"y\"])\nplt.scatter(data[tipo][\"X\"], data[tipo][\"y\"])\nplt.plot(np.linspace(0, 120), model.predict(np.linspace(0, 120)[:, None]))\nplt.show()","42c0af29":"def getOLSCoef(X, y):\n    X = np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)\n    alpha = 0\n    beta = 0\n    return alpha, beta\n\ngetOLSCoef(data[\"regresi\u00f3n\"][\"X\"], data[\"regresi\u00f3n\"][\"y\"])","d1ec5daa":"from sklearn.linear_model import LogisticRegression\n\ngs = gridspec.GridSpec(2, 2)\n\nfor tipo, grd  in zip([\"iris\", \"xor\", \"circulo\"], itertools.product([0, 1], repeat=2)):\n    clf = LogisticRegression()\n    clf.fit(data[tipo][\"X\"], data[tipo][\"y\"])\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=data[tipo][\"X\"], y=data[tipo][\"y\"], clf=clf, legend=2)\n    plt.title(tipo)","c4cf55ef":"from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n\ngs = gridspec.GridSpec(2, 3)\n\nfor (tipo, clfClass), grd  in zip([(\"regresi\u00f3n\", KNeighborsRegressor),\n                                   (\"regresi\u00f3n2\", KNeighborsRegressor),\n                                   (\"iris\", KNeighborsClassifier),\n                                   (\"xor\", KNeighborsClassifier),\n                                   (\"circulo\", KNeighborsClassifier)], itertools.product([0, 1, 2], repeat=2)):\n    clf = clfClass()\n    clf.fit(data[tipo][\"X\"], data[tipo][\"y\"])\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    try:\n        fig = plot_decision_regions(X=data[tipo][\"X\"], y=data[tipo][\"y\"], clf=clf, legend=2)\n    except:\n        plt.scatter(data[tipo][\"X\"], data[tipo][\"y\"])\n        plt.plot(np.linspace(0, 120), clf.predict(np.linspace(0, 120)[:, None]))\n    plt.title(tipo)","c757a684":"from sklearn.naive_bayes import GaussianNB\n\ngs = gridspec.GridSpec(2, 2)\n\nfor tipo, grd  in zip([\"iris\", \"xor\", \"circulo\"], itertools.product([0, 1], repeat=2)):\n    clf = GaussianNB()\n    clf.fit(data[tipo][\"X\"], data[tipo][\"y\"])\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=data[tipo][\"X\"], y=data[tipo][\"y\"], clf=clf, legend=2)\n    plt.title(tipo)","59ffdf92":"from sklearn.svm import LinearSVC, LinearSVR\n\ngs = gridspec.GridSpec(2, 3)\n\nfor (tipo, clfClass), grd  in zip([(\"regresi\u00f3n\", LinearSVR),\n                                   (\"regresi\u00f3n2\", LinearSVR),\n                                   (\"iris\", LinearSVC),\n                                   (\"xor\", LinearSVC),\n                                   (\"circulo\", LinearSVC)], itertools.product([0, 1, 2], repeat=2)):\n    clf = clfClass()\n    clf.fit(data[tipo][\"X\"], data[tipo][\"y\"])\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    try:\n        fig = plot_decision_regions(X=data[tipo][\"X\"], y=data[tipo][\"y\"], clf=clf, legend=2)\n    except:\n        plt.scatter(data[tipo][\"X\"], data[tipo][\"y\"])\n        plt.plot(np.linspace(0, 120), clf.predict(np.linspace(0, 120)[:, None]))\n    plt.title(tipo)","b82f7dd9":"from sklearn.svm import SVC, SVR\n\ngs = gridspec.GridSpec(2, 3)\n\nfor (tipo, clfClass, params), grd  in zip([(\"regresi\u00f3n\", SVR, {\"kernel\": \"linear\"}),\n                                   (\"regresi\u00f3n2\", SVR, {\"kernel\": \"poly\"}),\n                                   (\"iris\", SVC, {}),\n                                   (\"xor\", SVC, {}),\n                                   (\"circulo\", SVC, {})], itertools.product([0, 1, 2], repeat=2)):\n    clf = clfClass(**params)\n    clf.fit(data[tipo][\"X\"], data[tipo][\"y\"])\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    try:\n        fig = plot_decision_regions(X=data[tipo][\"X\"], y=data[tipo][\"y\"], clf=clf, legend=2)\n    except:\n        plt.scatter(data[tipo][\"X\"], data[tipo][\"y\"])\n        plt.plot(np.linspace(0, 120), clf.predict(np.linspace(0, 120)[:, None]))\n    plt.title(tipo)","27b0f485":"from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n\ngs = gridspec.GridSpec(2, 3)\n\nfor (tipo, clfClass), grd  in zip([(\"regresi\u00f3n\", DecisionTreeRegressor),\n                                   (\"regresi\u00f3n2\", DecisionTreeRegressor),\n                                   (\"iris\", DecisionTreeClassifier),\n                                   (\"xor\", DecisionTreeClassifier),\n                                   (\"circulo\", DecisionTreeClassifier)], itertools.product([0, 1, 2], repeat=2)):\n    clf = clfClass()\n    clf.fit(data[tipo][\"X\"], data[tipo][\"y\"])\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    try:\n        fig = plot_decision_regions(X=data[tipo][\"X\"], y=data[tipo][\"y\"], clf=clf, legend=2)\n    except:\n        plt.scatter(data[tipo][\"X\"], data[tipo][\"y\"])\n        plt.plot(np.linspace(0, 120), clf.predict(np.linspace(0, 120)[:, None]))\n    plt.title(tipo)","8c98da1f":"from sklearn.ensemble  import RandomForestClassifier, RandomForestRegressor\n\ngs = gridspec.GridSpec(2, 3)\n\nfor (tipo, clfClass), grd  in zip([(\"regresi\u00f3n\", RandomForestRegressor),\n                                   (\"regresi\u00f3n2\", RandomForestRegressor),\n                                   (\"iris\", RandomForestClassifier),\n                                   (\"xor\", RandomForestClassifier),\n                                   (\"circulo\", RandomForestClassifier)], itertools.product([0, 1, 2], repeat=2)):\n    clf = clfClass()\n    clf.fit(data[tipo][\"X\"], data[tipo][\"y\"])\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    try:\n        fig = plot_decision_regions(X=data[tipo][\"X\"], y=data[tipo][\"y\"], clf=clf, legend=2)\n    except:\n        plt.scatter(data[tipo][\"X\"], data[tipo][\"y\"])\n        plt.plot(np.linspace(0, 120), clf.predict(np.linspace(0, 120)[:, None]))\n    plt.title(tipo)","10718522":"from lightgbm import LGBMClassifier, LGBMRegressor\n\ngs = gridspec.GridSpec(2, 3)\n\nfor (tipo, clfClass), grd  in zip([(\"regresi\u00f3n\", LGBMRegressor),\n                                   (\"regresi\u00f3n2\", LGBMRegressor),\n                                   (\"iris\", LGBMClassifier),\n                                   (\"xor\", LGBMClassifier),\n                                   (\"circulo\", LGBMClassifier)], itertools.product([0, 1, 2], repeat=2)):\n    clf = clfClass()\n    clf.fit(data[tipo][\"X\"], data[tipo][\"y\"])\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    try:\n        fig = plot_decision_regions(X=data[tipo][\"X\"], y=data[tipo][\"y\"], clf=clf, legend=2)\n    except:\n        plt.scatter(data[tipo][\"X\"], data[tipo][\"y\"])\n        plt.plot(np.linspace(0, 120), clf.predict(np.linspace(0, 120)[:, None]))\n    plt.title(tipo)","19408a32":"data = pd.read_csv(\"..\/input\/titanic\/train.csv\", index_col=[\"PassengerId\"])\ndata","a800907e":"data.dtypes","e14853f2":"data.isnull().sum()","108d5900":"for c in data.select_dtypes(\"O\"):\n    data[c] = data[c].astype(\"category\")","4a372c78":"data[\"NumFam\"] = data.SibSp + data.Parch","95635e45":"data","ea60d112":"pd.crosstab(data.NumFam, data.Survived)","7d0cea97":"pd.crosstab(data.NumFam, data.Survived).apply(lambda x: x \/ x.sum(), axis=1)","4b2f067f":"pd.crosstab(data.Pclass, data.Survived).apply(lambda x: x \/ x.sum(), axis=1)","90fcbec2":"pd.crosstab(data.NumFam, data.Pclass, values=data.Survived, aggfunc=len)","6019df92":"pd.crosstab(data.NumFam, data.Pclass, values=data.Survived, aggfunc=np.mean)","83cdac35":"from sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import roc_auc_score\n\nX_train, X_test, y_train, y_test = train_test_split(data.drop(\"Survived\", axis=1), \n                                                      data.Survived, test_size=0.1, random_state=2)\nkf = KFold(n_splits=5)\nfolds = [(X_train.iloc[train_idx].index, X_train.iloc[valid_idx].index)\n         for train_idx, valid_idx in kf.split(X_train)]\n\nnum_leaves = list(range(10, 39, 3))\n\nres = pd.DataFrame([], index=[str(d) for d in num_leaves],\n                   columns=[\"fold_\" + str(i) for i in range(len(folds))] + [\"ensamble\"])\n\nfor nl in num_leaves:\n    test_probs = []\n    for i, (train_idx, valid_idx) in enumerate(folds):\n        print(\"doing fold {0} of depth {1}\".format(i + 1, str(nl)))\n        Xt = X_train.loc[train_idx]\n        yt = y_train.loc[train_idx]\n\n        Xv = X_train.loc[valid_idx]\n        yv = y_train.loc[valid_idx]\n\n        learner = LGBMClassifier(n_estimators=10000, num_leaves=nl)\n        learner.fit(Xt, yt, early_stopping_rounds=10, eval_metric=\"auc\",\n                    eval_set=[(Xt, yt), (Xv, yv)], verbose=3)\n        probs = pd.Series(learner.predict_proba(X_test)[:, -1],\n                          index=X_test.index, name=\"fold_\" + str(i))\n        test_probs.append(probs)\n        res.loc[str(nl), \"fold_\" + str(i)] = roc_auc_score(y_test, probs)\n        \n    test_probs = pd.concat(test_probs, axis=1).mean(axis=1)\n    res.loc[str(nl), \"ensamble\"] = roc_auc_score(y_test, test_probs)","9dccfdfd":"res","aeb7f311":"res.var().sort_values()","9b8fdcdb":"pd.Series(learner.feature_importances_, index=X_train.columns).sort_values(ascending=False)","f49fc33b":"## Ejercicio: implementar una regresi\u00f3n lineal con m\u00ednimos cuadrados ordinarios\n\n$$ \\mathbf{\\theta} = \\left( \\mathbf{X}^{'}\\mathbf{X} \\right)^{-1} \\mathbf{X}^{'}\\mathbf{y} $$\n\ninversa: np.linalg.inv\n\ntraspuesta: .T\n\nproducto interno: .dot","6ebab451":"# Regresi\u00f3n Logistica\n\n### Tarea: s\u00f3lo clasificaci\u00f3n\n\n### Modelo: $$\\hat{y} = \\frac{1}{1 + e^{-\\sum_{i=0}^p{\\theta_i . x_i}}}$$\n\n### Costo: $$\\sum_{i=0}^n{y_i . \\log{(\\hat{y_i} + \\epsilon)} + (1 - y_i) . \\log{(\\hat{1 - y_i + \\epsilon) }}} $$","7de483b3":"## Support Vector Machines\n\n### Tareas: regresi\u00f3n y clasificaci\u00f3n\n\n### Busca maximizar los margenes entre clases, cuando no es posible una separaci\u00f3n lineal, se puede usar el \"kernel trick\"","757f740e":"# Regresi\u00f3n Lineal\n\n### Tarea: s\u00f3lo regresi\u00f3n\n\n### Modelo: $$\\hat{y} = \\sum_{i=0}^p{\\theta_i . x_i}$$\n\n### Costo: $$\\sum_{i=0}^n{(\\hat{y_i} - y_i)^2} $$","08040358":"# Naive Bayes\n\n### Tarea: s\u00f3lo clasficaci\u00f3n\n\n### Modelo: Busca encontrar la probabilidad de la variable explicada condicionada a las variables explicativas, para ello parte del teorema de bayes\n\n$$P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) P(x_1, \\dots x_n \\mid y)}\n                                 {P(x_1, \\dots, x_n)}$$\n                                 \n### La parte \"Naive\" viene de asumir muy \"inocentemente\" que todas las variables explicativas son independientes entre si, ergo:\n\n$$P(x_i | y, x_1, \\dots, x_{i-1}, x_{i+1}, \\dots, x_n) = P(x_i | y)$$\n\n$$P(x_1, \\dots x_n \\mid y) = \\prod_{i=1}^{n} P(x_i \\mid y)$$\n\n$$P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) \\prod_{i=1}^{n} P(x_i \\mid y)}\n                                 {P(x_1, \\dots, x_n)}$$","520ad73c":"# Ensamble 1: Boosting (Adaboosting & GraientBoosting)\n\n### Tareas: regresi\u00f3n y clasificaci\u00f3n\n\n### Son metodos que tambi\u00e9n agregan clasificadores base, pero en lugar de promediarlos, van ajustando los pesos de los posteriores, basados en los erroes de los anteriores","3d44ff29":"# Ensamble 1: Bagging\n\n### Tareas: regresi\u00f3n y clasificaci\u00f3n\n\n### Consiste en promediar una serie de algortimos base, entreganos en un sub-set de casos y variables. Busca disminuir la varianza de la estimaci\u00f3n\n","49bdee63":"### Bonus: auto-tuning y bagging\n\nUniendo las idea de cv, boosting con early stopping y bagging, se puede componer un clasificador de simple ejecuci\u00f3n y muy gran poder de predicci\u00f3n, ","e4df83fa":"# \u00c1rboles de decision\n\n### Tareas: regresi\u00f3n y clasificaci\u00f3n\n\n### Son metodos de inducci\u00f3n no param\u00e9tricos (no hay modelo tipo ecuaci\u00f3n). Se basan en la creaci\u00f3n \"greedy\" de simples reglas de decisi\u00f3n que permitan modelar el problema.","41595624":"# KNN\n\n### Tareas: regresi\u00f3n y clasificaci\u00f3n\n\n### Modelo: no hay, basado en memoria\n\n### Par\u00e1metros: cantidad de vecinos \/ radio del vecindario\n"}}