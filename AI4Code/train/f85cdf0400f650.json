{"cell_type":{"0f656413":"code","69e8ee29":"code","f5932f0d":"code","0eeb726a":"code","a8bca10d":"code","8c67acf9":"code","eddda82a":"code","04f6caa3":"code","d94741de":"code","f6544621":"code","4a9c3336":"markdown","279f45bf":"markdown","82b869e3":"markdown","8d894aee":"markdown"},"source":{"0f656413":"imagenet_1024 = False \nimagenet_16384 = True\ncoco = False\nfaceshq = False\nwikiart_1024 = False \nwikiart_16384 = False \nsflckr = False\nopenimages_8192 = False","69e8ee29":"!git clone https:\/\/github.com\/openai\/CLIP\n!pip install taming-transformers\n!git clone https:\/\/github.com\/CompVis\/taming-transformers.git\n#!pip install -e .\/taming-transformers\n!pip install ftfy regex tqdm omegaconf pytorch-lightning\n!pip install kornia\n!pip install imageio-ffmpeg   \n!pip install einops          \n!mkdir steps\n#@title Selection of models to download\n#@markdown By default, the notebook downloads the 1024 and 16384 models from ImageNet. There are others like COCO-Stuff, WikiArt 1024, WikiArt 16384, FacesHQ or S-FLCKR, which are heavy, and if you are not going to use them it would be pointless to download them, so if you want to use them, simply select the models to download.\n\n\nif imagenet_1024:\n  !curl -L -o vqgan_imagenet_f16_1024.yaml -C - 'http:\/\/mirror.io.community\/blob\/vqgan\/vqgan_imagenet_f16_1024.yaml' #ImageNet 1024\n  !curl -L -o vqgan_imagenet_f16_1024.ckpt -C - 'http:\/\/mirror.io.community\/blob\/vqgan\/vqgan_imagenet_f16_1024.ckpt'  #ImageNet 1024\nif imagenet_16384:\n  !curl -L -o vqgan_imagenet_f16_16384.yaml -C - 'https:\/\/heibox.uni-heidelberg.de\/d\/a7530b09fed84f80a887\/files\/?p=%2Fconfigs%2Fmodel.yaml&dl=1' #ImageNet 16384\n  !curl -L -o vqgan_imagenet_f16_16384.ckpt -C - 'https:\/\/heibox.uni-heidelberg.de\/d\/a7530b09fed84f80a887\/files\/?p=%2Fckpts%2Flast.ckpt&dl=1' #ImageNet 16384\nif openimages_8192:\n  !curl -L -o vqgan_openimages_f16_8192.yaml -C - 'https:\/\/heibox.uni-heidelberg.de\/d\/2e5662443a6b4307b470\/files\/?p=%2Fconfigs%2Fmodel.yaml&dl=1' #ImageNet 16384\n  !curl -L -o vqgan_openimages_f16_8192.ckpt -C - 'https:\/\/heibox.uni-heidelberg.de\/d\/2e5662443a6b4307b470\/files\/?p=%2Fckpts%2Flast.ckpt&dl=1' #ImageNet 16384\n\nif coco:\n  !curl -L -o coco.yaml -C - 'https:\/\/dl.nmkd.de\/ai\/clip\/coco\/coco.yaml' #COCO\n  !curl -L -o coco.ckpt -C - 'https:\/\/dl.nmkd.de\/ai\/clip\/coco\/coco.ckpt' #COCO\nif faceshq:\n  !curl -L -o faceshq.yaml -C - 'https:\/\/drive.google.com\/uc?export=download&id=1fHwGx_hnBtC8nsq7hesJvs-Klv-P0gzT' #FacesHQ\n  !curl -L -o faceshq.ckpt -C - 'https:\/\/app.koofr.net\/content\/links\/a04deec9-0c59-4673-8b37-3d696fe63a5d\/files\/get\/last.ckpt?path=%2F2020-11-13T21-41-45_faceshq_transformer%2Fcheckpoints%2Flast.ckpt' #FacesHQ\nif wikiart_1024: \n  !curl -L -o wikiart_1024.yaml -C - 'http:\/\/mirror.io.community\/blob\/vqgan\/wikiart.yaml' #WikiArt 1024\n  !curl -L -o wikiart_1024.ckpt -C - 'http:\/\/mirror.io.community\/blob\/vqgan\/wikiart.ckpt' #WikiArt 1024\nif wikiart_16384: \n  !curl -L -o wikiart_16384.yaml -C - 'http:\/\/mirror.io.community\/blob\/vqgan\/wikiart_16384.yaml' #WikiArt 16384\n  !curl -L -o wikiart_16384.ckpt -C - 'http:\/\/mirror.io.community\/blob\/vqgan\/wikiart_16384.ckpt' #WikiArt 16384\nif sflckr:\n  !curl -L -o sflckr.yaml -C - 'https:\/\/heibox.uni-heidelberg.de\/d\/73487ab6e5314cb5adba\/files\/?p=%2Fconfigs%2F2020-11-09T13-31-51-project.yaml&dl=1' #S-FLCKR\n  !curl -L -o sflckr.ckpt -C - 'https:\/\/heibox.uni-heidelberg.de\/d\/73487ab6e5314cb5adba\/files\/?p=%2Fcheckpoints%2Flast.ckpt&dl=1' #S-FLCKR\n    \n    \n# @title Load libraries and variables\n\nimport argparse\nimport math\nfrom pathlib import Path\nimport sys\n\nsys.path.insert(1, '.\/taming-transformers')\nfrom IPython import display\nfrom base64 import b64encode\nfrom omegaconf import OmegaConf\nfrom PIL import Image\nfrom taming.models import cond_transformer, vqgan\nimport taming.modules \nimport torch\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nfrom torchvision import transforms\nfrom torchvision.transforms import functional as TF\nfrom tqdm.notebook import tqdm\n\nfrom CLIP import clip\nimport kornia.augmentation as K\nimport numpy as np\nimport imageio\nfrom PIL import ImageFile, Image\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\ndef sinc(x):\n    return torch.where(x != 0, torch.sin(math.pi * x) \/ (math.pi * x), x.new_ones([]))\n\n\ndef lanczos(x, a):\n    cond = torch.logical_and(-a < x, x < a)\n    out = torch.where(cond, sinc(x) * sinc(x\/a), x.new_zeros([]))\n    return out \/ out.sum()\n\n\ndef ramp(ratio, width):\n    n = math.ceil(width \/ ratio + 1)\n    out = torch.empty([n])\n    cur = 0\n    for i in range(out.shape[0]):\n        out[i] = cur\n        cur += ratio\n    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n\n\ndef resample(input, size, align_corners=True):\n    n, c, h, w = input.shape\n    dh, dw = size\n\n    input = input.view([n * c, 1, h, w])\n\n    if dh < h:\n        kernel_h = lanczos(ramp(dh \/ h, 2), 2).to(input.device, input.dtype)\n        pad_h = (kernel_h.shape[0] - 1) \/\/ 2\n        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n        input = F.conv2d(input, kernel_h[None, None, :, None])\n\n    if dw < w:\n        kernel_w = lanczos(ramp(dw \/ w, 2), 2).to(input.device, input.dtype)\n        pad_w = (kernel_w.shape[0] - 1) \/\/ 2\n        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n        input = F.conv2d(input, kernel_w[None, None, None, :])\n\n    input = input.view([n, c, h, w])\n    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n\n\nclass ReplaceGrad(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x_forward, x_backward):\n        ctx.shape = x_backward.shape\n        return x_forward\n\n    @staticmethod\n    def backward(ctx, grad_in):\n        return None, grad_in.sum_to_size(ctx.shape)\n\n\nreplace_grad = ReplaceGrad.apply\n\n\nclass ClampWithGrad(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, input, min, max):\n        ctx.min = min\n        ctx.max = max\n        ctx.save_for_backward(input)\n        return input.clamp(min, max)\n\n    @staticmethod\n    def backward(ctx, grad_in):\n        input, = ctx.saved_tensors\n        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n\n\nclamp_with_grad = ClampWithGrad.apply\n\n\ndef vector_quantize(x, codebook):\n    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n    indices = d.argmin(-1)\n    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n    return replace_grad(x_q, x)\n\n\nclass Prompt(nn.Module):\n    def __init__(self, embed, weight=1., stop=float('-inf')):\n        super().__init__()\n        self.register_buffer('embed', embed)\n        self.register_buffer('weight', torch.as_tensor(weight))\n        self.register_buffer('stop', torch.as_tensor(stop))\n\n    def forward(self, input):\n        input_normed = F.normalize(input.unsqueeze(1), dim=2)\n        embed_normed = F.normalize(self.embed.unsqueeze(0), dim=2)\n        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n        dists = dists * self.weight.sign()\n        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n\n\ndef parse_prompt(prompt):\n    vals = prompt.rsplit(':', 2)\n    vals = vals + ['', '1', '-inf'][len(vals):]\n    return vals[0], float(vals[1]), float(vals[2])\n\n\nclass MakeCutouts(nn.Module):\n    def __init__(self, cut_size, cutn, cut_pow=1.):\n        super().__init__()\n        self.cut_size = cut_size\n        self.cutn = cutn\n        self.cut_pow = cut_pow\n\n        self.augs = nn.Sequential(\n            # K.RandomHorizontalFlip(p=0.5),\n            # K.RandomVerticalFlip(p=0.5),\n            # K.RandomSolarize(0.01, 0.01, p=0.7),\n            # K.RandomSharpness(0.3,p=0.4),\n            # K.RandomResizedCrop(size=(self.cut_size,self.cut_size), scale=(0.1,1),  ratio=(0.75,1.333), cropping_mode='resample', p=0.5),\n            # K.RandomCrop(size=(self.cut_size,self.cut_size), p=0.5),\n            K.RandomAffine(degrees=15, translate=0.1, p=0.7, padding_mode='border'),\n            K.RandomPerspective(0.7,p=0.7),\n            K.ColorJitter(hue=0.1, saturation=0.1, p=0.7),\n            K.RandomErasing((.1, .4), (.3, 1\/.3), same_on_batch=True, p=0.7),\n            \n)\n        self.noise_fac = 0.1\n        self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n        self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n\n    def forward(self, input):\n        sideY, sideX = input.shape[2:4]\n        max_size = min(sideX, sideY)\n        min_size = min(sideX, sideY, self.cut_size)\n        cutouts = []\n        \n        for _ in range(self.cutn):\n\n            # size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n            # offsetx = torch.randint(0, sideX - size + 1, ())\n            # offsety = torch.randint(0, sideY - size + 1, ())\n            # cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n            # cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n\n            # cutout = transforms.Resize(size=(self.cut_size, self.cut_size))(input)\n            \n            cutout = (self.av_pool(input) + self.max_pool(input))\/2\n            cutouts.append(cutout)\n        batch = self.augs(torch.cat(cutouts, dim=0))\n        if self.noise_fac:\n            facs = batch.new_empty([self.cutn, 1, 1, 1]).uniform_(0, self.noise_fac)\n            batch = batch + facs * torch.randn_like(batch)\n        return batch\n\n\ndef load_vqgan_model(config_path, checkpoint_path):\n    config = OmegaConf.load(config_path)\n    if config.model.target == 'taming.models.vqgan.VQModel':\n        model = vqgan.VQModel(**config.model.params)\n        model.eval().requires_grad_(False)\n        model.init_from_ckpt(checkpoint_path)\n    elif config.model.target == 'taming.models.vqgan.GumbelVQ':\n        model = vqgan.GumbelVQ(**config.model.params)\n        model.eval().requires_grad_(False)\n        model.init_from_ckpt(checkpoint_path)\n    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n        parent_model.eval().requires_grad_(False)\n        parent_model.init_from_ckpt(checkpoint_path)\n        model = parent_model.first_stage_model\n    else:\n        raise ValueError(f'unknown model type: {config.model.target}')\n    del model.loss\n    return model\n\n\ndef resize_image(image, out_size):\n    ratio = image.size[0] \/ image.size[1]\n    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n    size = round((area * ratio)**0.5), round((area \/ ratio)**0.5)\n    return image.resize(size, Image.LANCZOS)\n\n!pip install madgrad\n\n\nimport math\nfrom typing import TYPE_CHECKING, Any, Callable, Optional\n\nimport torch\nimport torch.optim\n\n\nif TYPE_CHECKING:\n    from torch.optim.optimizer import _params_t\nelse:\n    _params_t = Any\n\n\nfrom madgrad import MADGRAD\n\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau","f5932f0d":"#download image from internet\n#also you can add image by dataset, write in settings something like init_image = '\/input\/NAME_OF_YOUR_DATASET\/IMAGE_NAME.png'\n!wget https:\/\/faktoved.ru\/wp-content\/uploads\/fakti-o-gogole.jpg -O goo.jpg","0eeb726a":"#15.4GB vram and 1.5it\/sec on this settings\ntexts = \"turtle ray tracing\"# \"; trending on artstation; hyper realistic, ray tracing, fine detail, ultra settings\" at end can improve quality\nwidth =  512\nheight = 512\nimages_interval =  35\ninit_image = \"\" #file name like \"fakti-o-gogole.jpg\" or nothing\ntarget_images = \"\" #same as init_image\nseed = -1 # -1 or any int number\nmax_iterations = 2100\n\nsave_every = 35 \noptimizer = 'MADGRAD' # 'Adam' (actually any another str will make that Adam but..) or 'MADGRAD' (its seems what MADGRAD nedds such more step size, like 50x Adam  step size or more to equal)\n#also i dont know why, but Adam needs much more vram when image size more than 400x400. with this settings, Adam will give OOM error and MADGRAD dont, but \nstep_size = 6 #set 0.1 for Adam\nschedulche = True #if True, lr will be reduced when losses reduces. Mast have for MADGRAD but for Adam maybe useless\nmodel = \"vqgan_imagenet_f16_16384\" #\"vqgan_imagenet_f16_16384\", \"vqgan_imagenet_f16_1024\", \"vqgan_openimages_f16_8192\", \"wikiart_1024\", \"wikiart_16384\", \"coco\", \"faceshq\", \"sflckr\"\n\n\n\n\nmodel_names={\"vqgan_imagenet_f16_16384\": 'ImageNet 16384',\"vqgan_imagenet_f16_1024\":\"ImageNet 1024\", 'vqgan_openimages_f16_8192':'OpenImages 8912',\n                 \"wikiart_1024\":\"WikiArt 1024\", \"wikiart_16384\":\"WikiArt 16384\", \"coco\":\"COCO-Stuff\", \"faceshq\":\"FacesHQ\", \"sflckr\":\"S-FLCKR\"}\nname_model = model_names[model]     \n\n\n\nif seed == -1:\n    seed = None\nif init_image == \"None\":\n    init_image = None\nif target_images == \"None\" or not target_images:\n    target_images = []\nelse:\n    target_images = target_images.split(\"|\")\n    target_images = [image.strip() for image in target_images]\n\ntexts = [phrase.strip() for phrase in texts.split(\"|\")]\nif texts == ['']:\n    texts = []\n\n\nargs = argparse.Namespace(\n    prompts=texts,\n    image_prompts=target_images,\n    noise_prompt_seeds=[],\n    noise_prompt_weights=[],\n    size=[width, height],\n    init_image=init_image,\n    init_weight=0.,\n    clip_model='ViT-B\/32',\n    vqgan_config=f'{model}.yaml',\n    vqgan_checkpoint=f'{model}.ckpt',\n    step_size=step_size,\n    cutn=32,\n    cut_pow=1.,\n    display_freq=images_interval,\n    seed=seed,\n)","a8bca10d":"!rm -r .\/steps\/*png\n\n\nimport gc\ngc.collect()\ntry:\n    del model\nexcept:\n    pass\ntry:\n    del perceptor\nexcept:\n    pass\ntry:\n    del one_hot\nexcept:\n    pass\n\ngc.collect()\ntorch.cuda.empty_cache()\n\n\ndef get_lr(opt):\n  for param_groups in opt.param_groups:\n      return param_groups['lr']\n\n#@title Actually do the run...\nfrom urllib.request import urlopen\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\nif texts:\n    print('Using texts:', texts)\nif target_images:\n    print('Using image prompts:', target_images)\nif args.seed is None:\n    seed = torch.seed()\nelse:\n    seed = args.seed\ntorch.manual_seed(seed)\nprint('Using seed:', seed)\n\nmodel = load_vqgan_model(args.vqgan_config, args.vqgan_checkpoint).to(device)\nperceptor = clip.load(args.clip_model, jit=False)[0].eval().requires_grad_(False).to(device)\n# clock=deepcopy(perceptor.visual.positional_embedding.data)\n# perceptor.visual.positional_embedding.data = clock\/clock.max()\n# perceptor.visual.positional_embedding.data=clamp_with_grad(clock,0,1)\n\ncut_size = perceptor.visual.input_resolution\n\nf = 2**(model.decoder.num_resolutions - 1)\nmake_cutouts = MakeCutouts(cut_size, args.cutn, cut_pow=args.cut_pow)\n\ntoksX, toksY = args.size[0] \/\/ f, args.size[1] \/\/ f\nsideX, sideY = toksX * f, toksY * f\n\nif args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n    e_dim = 256\n    n_toks = model.quantize.n_embed\n    z_min = model.quantize.embed.weight.min(dim=0).values[None, :, None, None]\n    z_max = model.quantize.embed.weight.max(dim=0).values[None, :, None, None]\nelse:\n    e_dim = model.quantize.e_dim\n    n_toks = model.quantize.n_e\n    z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n    z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n# z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n# z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n\n# normalize_imagenet = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n#                                            std=[0.229, 0.224, 0.225])\n\nif args.init_image:\n    if 'http' in args.init_image:\n      img = Image.open(urlopen(args.init_image))\n    else:\n      img = Image.open(args.init_image)\n    pil_image = img.convert('RGB')\n    pil_image = pil_image.resize((sideX, sideY), Image.LANCZOS)\n    pil_tensor = TF.to_tensor(pil_image)\n    z, *_ = model.encode(pil_tensor.to(device).unsqueeze(0) * 2 - 1)\nelse:\n    one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\n    # z = one_hot @ model.quantize.embedding.weight\n    if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n        z = one_hot @ model.quantize.embed.weight\n    else:\n        z = one_hot @ model.quantize.embedding.weight\n    z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2) \n    z = torch.rand_like(z)*2\nz_orig = z.clone()\nz.requires_grad_(True)\nif optimizer == 'MADGRAD':\n    opt = MADGRAD([z], lr=args.step_size)\nelse:\n    opt = optim.Adam([z], lr=args.step_size)\nprint(opt)\nif schedulche:\n    scheduler = ReduceLROnPlateau(opt, 'min', factor=0.999, patience=0)\n    \nnormalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n                                  std=[0.26862954, 0.26130258, 0.27577711])\n\n\n\npMs = []\n\nfor prompt in args.prompts:\n    txt, weight, stop = parse_prompt(prompt)\n    embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n    pMs.append(Prompt(embed, weight, stop).to(device))\n\nfor prompt in args.image_prompts:\n    path, weight, stop = parse_prompt(prompt)\n    img = Image.open(path)\n    pil_image = img.convert('RGB')\n    img = resize_image(pil_image, (sideX, sideY))\n    batch = make_cutouts(TF.to_tensor(img).unsqueeze(0).to(device))\n    embed = perceptor.encode_image(normalize(batch)).float()\n    pMs.append(Prompt(embed, weight, stop).to(device))\n\nfor seed, weight in zip(args.noise_prompt_seeds, args.noise_prompt_weights):\n    gen = torch.Generator().manual_seed(seed)\n    embed = torch.empty([1, perceptor.visual.output_dim]).normal_(generator=gen)\n    pMs.append(Prompt(embed, weight).to(device))\n\ndef synth(z):\n    if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embed.weight).movedim(3, 1)\n    else:\n        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n    return clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n\n@torch.no_grad()\ndef checkin(i, losses):\n    global loss_idx\n    losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n    tqdm.write(f'last image i: {i}, loss: {round(sum(losses).item(), 5):g}, losses: {losses_str}')\n    display.display(display.Image('.\/steps\/'+ str(i) + '.png'))\n    min_loss = loss_idx[0]\n    min_loss_i = i\n    for e in range(i):\n        if loss_idx[e] < min_loss and e % save_every == 0:\n            min_loss = loss_idx[e]\n            min_loss_i = e \n    if i !=0:\n        tqdm.write(f'last best image i: {min_loss_i}, loss: {round(min_loss, 5)}')\n        display.display(display.Image('.\/steps\/'+ str(min_loss_i) + '.png'))\n\n\ndef ascend_txt():\n    global i\n    out = synth(z)\n    iii = perceptor.encode_image(normalize(make_cutouts(out))).float()\n    \n    result = []\n\n    if args.init_weight:\n        # result.append(F.mse_loss(z, z_orig) * args.init_weight \/ 2)\n        result.append(F.mse_loss(z, torch.zeros_like(z_orig)) * ((1\/torch.tensor(i*2 + 1))*args.init_weight) \/ 2)\n    for prompt in pMs:\n        result.append(prompt(iii))\n    if save_every != None:\n        if i % save_every == 0:\n            out = synth(z)\n            TF.to_pil_image(out[0].cpu()).save('.\/steps\/'+ str(i) + '.png')\n\n    return result\n\n\n\ndef train(i):\n    global loss_idx\n    opt.zero_grad()\n    lossAll = ascend_txt()\n       \n    loss = sum(lossAll)\n    \n    loss_idx.append(loss.item())\n    if i > 100: #use only 100 last looses to avg\n        avg_loss = sum(loss_idx[i-100:])\/len(loss_idx[i-100:]) \n    else:\n        avg_loss = sum(loss_idx)\/len(loss_idx)\n        \n        \n    if i % args.display_freq == 0:\n        checkin(i, lossAll)\n        \n        \n    scheduler.step(avg_loss)\n    \n    loss.backward()\n    opt.step()\n    with torch.no_grad():\n        z.copy_(z.maximum(z_min).minimum(z_max))\n    return loss, avg_loss, lossAll\nloss_idx = []\navg = []\ni = 0\ntry:\n    with tqdm(total=max_iterations) as pbar:\n        while True:\n            loss, avg_loss, lossAll = train(i)\n            avg.append(avg_loss)\n            pbar.set_description('Loss: ' + str(round((loss).item(), 3)) + '    AVG: ' + str(round(avg_loss, 3)) + '     LR: ' + str(round(get_lr(opt), 10)))\n            if i == max_iterations:\n                checkin(i, lossAll)\n                break\n            i += 1\n            pbar.update()\nexcept KeyboardInterrupt:\n    pass\nexcept RuntimeError:\n    print('RuntimeError: CUDA out of memory')\n    print('Restart that cell, or low settings if this error shows again')","8c67acf9":"import matplotlib.pyplot as plt\nplt.figure(dpi=100)\nplt.plot(loss_idx)\nplt.show()","eddda82a":"import matplotlib.pyplot as plt\nplt.figure(dpi=100)\nplt.plot(avg)\nplt.show()\n","04f6caa3":"#Clear all vram by deleting clip and vqgan model if you get OOM error\n\nimport gc\n\ntry:\n    del model\nexcept:\n    print('skip')\n    pass\ntry:\n    del perceptor\nexcept:\n    print('skip')\n    pass\ntry:\n    del one_hot #idk what is that but ok\nexcept:\n    print('skip')\n    pass\ngc.collect()\n\ntorch.cuda.empty_cache()\n!nvidia-smi","d94741de":"\n# #much faster \n\n\n# import base64\n# import io\n# import re\n# import imageio\n# from IPython.display import HTML\n# import glob\n# import numpy as np\n# fppps =  10\n\n# frames = []\n# filenames = glob.glob(\".\/steps\/*.png\")\n# img = sorted(filenames, key=lambda x:float(re.findall(\"(\\d+)\",x)[0]))\n\n# print(len(img))\n# for b in img:\n#   frames.append(imageio.imread(b))\n# frames = np.array(frames)\n# imageio.mimsave(\"video.mp4\", frames, fps=fppps)\n\n# print('Done')\n\n\n# video = io.open('video.mp4', 'r+b').read()\n# encoded = base64.b64encode(video)\n# play_html = ''\n# play_html = play_html + ('<video alt=\"test\" controls><source src=\"data:video\/mp4;base64,{0}\" type=\"video\/mp4\" \/> <\/video>'.format(encoded.decode('ascii')))\n\n# HTML(data=play_html)","f6544621":"#very long, \ninit_frame = 1 #This is the frame where the video will start\nlast_frame = i #You can change i to the number of the last frame you want to generate. It will raise an error if that number of frames does not exist.\n\nmin_fps = 10\nmax_fps = 60\n\ntotal_frames = last_frame - init_frame\n\nlength = 60 #Desired time of the video in seconds\n\nframes = []\ntqdm.write('Generating video...')\nfor i in range(init_frame,last_frame): #\n    frames.append(Image.open(\".\/steps\/\"+ str(i) +'.png'))\nprint(len(frames))\n#fps = last_frame\/10\nfps = np.clip(total_frames\/length,min_fps,max_fps)\n\nfrom subprocess import Popen, PIPE\np = Popen(['ffmpeg', '-y', '-f', 'image2pipe', '-vcodec', 'png', '-r', str(fps), '-i', '-', '-vcodec', 'libx264', '-r', str(fps), '-pix_fmt', 'yuv420p', '-crf', '16.5', '-preset', 'medium', 'video.mp4'], stdin=PIPE)\nfor im in tqdm(frames):\n    im.save(p.stdin, 'PNG')\np.stdin.close()\np.wait()\nmp4 = open('video.mp4','rb').read()\ndata_url = \"data:video\/mp4;base64,\" + b64encode(mp4).decode()\ndisplay.HTML(\"\"\"\n<video width=400 controls>\n      <source src=\"%s\" type=\"video\/mp4\">\n<\/video>\n\"\"\" % data_url)\n\n!rm -r .\/steps\/*png","4a9c3336":"## Install all (change first cell to download models to use)","279f45bf":"## Based on https:\/\/colab.research.google.com\/drive\/1ZAus_gn2RhTZWzOWUpPERNC0Q8OhZRTZ?usp=sharing and https:\/\/colab.research.google.com\/drive\/1L8oL-vLJXVcRzCFbPwOoMkPKJ8-aYdPN","82b869e3":"## Set settings","8d894aee":"## And run"}}