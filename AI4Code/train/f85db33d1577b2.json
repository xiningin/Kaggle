{"cell_type":{"2a7fedab":"code","e4d79970":"code","e9fc6c8e":"code","1cc69070":"code","e5cb83ac":"code","e71ed1ec":"code","43f83a2c":"code","566f905d":"code","8f7aad99":"code","9a29859e":"code","45dea818":"code","adbcfcd6":"code","4b0a81de":"code","fe8ad3e6":"markdown","78810964":"markdown","c6aa6221":"markdown","95998624":"markdown","ce106104":"markdown","148d5775":"markdown","1afc8611":"markdown","16c71f3d":"markdown","b3081652":"markdown","cce68a8b":"markdown","dcf1cffc":"markdown"},"source":{"2a7fedab":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport dask.dataframe as dd\nimport dask\nimport time\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e4d79970":"!ls -lsh ..\/input\/PLAsTiCC-2018\/test_set.csv","e9fc6c8e":"# monitor cpu memory usage\n!free -g","1cc69070":"%%time\n# kernel died running the following pandas command due to OOM\n#df = pd.read_csv('..\/input\/PLAsTiCC-2018\/test_set.csv') \n\ndf = dd.read_csv('..\/input\/PLAsTiCC-2018\/test_set.csv') ","e5cb83ac":"df","e71ed1ec":"df.head() # dask just reads the head","43f83a2c":"%%time\ndf.shape # dask is not getting the actual shape since it is lazy","566f905d":"%%time\ndask.compute(df.shape)","8f7aad99":"%%time\n# simple column-wise reduction operations\ndf['flux'].mean().compute() # returns a scalar","9a29859e":"!free -g","45dea818":"%%time\ndf_sample = df.loc[df.object_id==13].compute()","adbcfcd6":"%%time\n\n# it runs for more than 9 hours and is killed by kaggle.\n#flux_stats_of_each_mjd = df.groupby('mjd').agg({'flux':['std']}).compute()\n# This will return a pandas dataframe\n\n#flux_stats_of_each_mjd.head()","4b0a81de":"#print(type(flux_stats_of_each_mjd),flux_stats_of_each_mjd.shape)","fe8ad3e6":"This is a CPU only kernel simply to test dask dataframe's capability to handle dataframes larger than cpu memory.\n\nTakeaways:\n* dask can handle dataframes larger than memory by breaking it down into chunks.\n* dask arrary operations are utilizing multi-threads out of the box.\n* element wise operation such as masking, reduction can be done in reasonable time.\n* groupby-aggregation might be doable but it is too slow to be useful.","78810964":"Next let's do a grouby aggregation","c6aa6221":"You might think that we need to the whole dataframe into memory to do the groupby aggregation. However, dask adopts an [apply-concat-apply](https:\/\/blog.dask.org\/2019\/10\/08\/df-groupby) paradigm, where aggregation is done first for each chunk, and then the aggregated intermediate results are concatenated to form a new dataframe, and aggregated again. ","95998624":"By using the `compute` method, dask reads the big csv file chunk by chunk and calculate the mean value. ","ce106104":"`flux_stats_of_each_mjd` is a pandas dataframe. Unfortunately, it is a slow operation that runs for more than 9 hours and killed by kaggle but in theory eventually it should get things done.","148d5775":"As shown above, dask breaks down the big dataframe into 310 chunks.","1afc8611":"It should be noted that the wall time is roughly 25% of the total CPU time, indicating that `dask` is using 4 threads to do things in parallel.","16c71f3d":"Notice that the CPU memory usage is not increased since dask has already released memory of all the intermediate variables in the process of calculating `mean`","b3081652":"dask is lazy so the dataframe is NOT read yet but we can still access the header instantly","cce68a8b":"The number of rows is a `delayed` object and the number of columns is trivial to get, which is `6`","dcf1cffc":"Let's caculate the mean value of a column."}}