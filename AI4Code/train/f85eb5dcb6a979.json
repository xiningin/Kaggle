{"cell_type":{"e526bdeb":"code","9fa44830":"code","51801d3a":"code","c212bede":"code","84bd4aea":"code","caa0bd4b":"code","169a99f9":"code","c9d88355":"code","35eb3446":"code","2fc04138":"code","09fb82da":"code","6be76472":"code","a4efa06e":"code","bc2699a9":"code","ae6d3375":"code","84ffdc25":"code","27ee1cb4":"code","bba4ccdc":"code","826a1d3e":"code","155782cf":"code","f37895cd":"code","200a30b1":"code","c1e64687":"code","b7a44581":"code","493c7291":"code","813fd0fa":"code","25d8f20a":"code","b06fa322":"code","c02c0c30":"code","dfadef7b":"code","ecc67fbb":"code","e79945dc":"code","f50afffd":"code","1350902a":"code","d0cdc81e":"code","fa162fbb":"code","3780fa71":"code","941d6da8":"code","6e6bff32":"code","722b1605":"code","f84750d0":"code","751fc48c":"code","43996a8c":"code","fbbde186":"code","59ff4ce5":"code","5d86e448":"code","23c67db2":"markdown","c6c0bfe9":"markdown"},"source":{"e526bdeb":"!pip install sentence-transformers","9fa44830":"!nvidia-smi ","51801d3a":"import pandas as pd\nimport time\nfrom tqdm import tqdm\nimport seaborn as sns\nimport numpy as np\nfrom textblob import TextBlob\nimport matplotlib.pyplot as plt\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('msmarco-distilbert-base-dot-prod-v3')","c212bede":"def fetch_movie_info(dataframe_idx):\n    info = df.iloc[dataframe_idx]\n    meta_dict = {}\n    meta_dict['Title'] = info['Title']\n    meta_dict['Plot'] = info['Plot'][:500]\n    return meta_dict\n    \ndef search(query, top_k, index, model):\n    t=time.time()\n    query_vector = model.encode([query])\n    top_k = index.search(query_vector, top_k)\n    print('>>>> Results in Total Time: {}'.format(time.time()-t))\n    top_k_ids = top_k[1].tolist()[0]\n    top_k_ids = list(np.unique(top_k_ids))\n    results =  [fetch_movie_info(idx) for idx in top_k_ids]\n    return results","84bd4aea":"data = pd.read_csv('..\/input\/wikipedia-movie-plots\/wiki_movie_plots_deduped.csv',memory_map=True)\ndata.info()","caa0bd4b":"data.head()","169a99f9":"import gc\ndf = data[['Title','Plot']]\n","c9d88355":"df.dropna(inplace=True)\ndf.drop_duplicates(subset=['Plot'],inplace=True)","35eb3446":"!pip install faiss-gpu","2fc04138":"import faiss\nencoded_data = model.encode(df.Plot.tolist())\nencoded_data = np.asarray(encoded_data.astype('float32'))\nindex = faiss.IndexIDMap(faiss.IndexFlatIP(768))\nindex.add_with_ids(encoded_data, np.array(range(0, len(df))))\nfaiss.write_index(index, 'movie_plot.index')","09fb82da":"from pprint import pprint\n\nquery=\"Artificial Intelligence based action movie\"\nresults=search(query, top_k=5, index=index, model=model)\n\nprint(\"\\n\")\nfor result in results:\n    print('\\t',pprint(result))","6be76472":"## Load our cross-encoder. Use fast tokenizer to speed up the tokenization\nfrom sentence_transformers import CrossEncoder\ncross_model = CrossEncoder('cross-encoder\/ms-marco-TinyBERT-L-6', max_length=512)","a4efa06e":"\ndef cross_score(model_inputs):\n    scores = cross_model.predict(model_inputs)\n    return scores\n\nmodel_inputs = [[query,item['Plot']] for item in results]\nscores = cross_score(model_inputs)\n#Sort the scores in decreasing order\nranked_results = [{'Title': inp['Title'], 'Score': score} for inp, score in zip(results, scores)]\nranked_results = sorted(ranked_results, key=lambda x: x['Score'], reverse=True)\n","bc2699a9":"print(\"\\n\")\nfor result in ranked_results:\n    print('\\t',pprint(result))","ae6d3375":"!pip install bert-score","84ffdc25":"\n# check your installation\nimport bert_score\nbert_score.__version__","27ee1cb4":"from bert_score import score","bba4ccdc":"ref=[\"Artificial Intelligence based action movie\"]","826a1d3e":"ranked_results_bert = []\n\nfor cand in results:\n    P, R, F1 = score([cand['Plot']], ref, lang='en')\n    ranked_results_bert.append({'Title': cand['Title'], 'Score': F1.numpy()[0]})\n    ","155782cf":"#Sort the scores in decreasing order\nranked_results_bert = sorted(ranked_results_bert, key=lambda x: x['Score'], reverse=True)\nprint(\"\\n\")\nfor result in ranked_results_bert:\n    print('\\t',pprint(result))","f37895cd":"final_results = pd.DataFrame()\nfinal_results['faiss_ranking'] = [item['Title'] for item in results]\nfinal_results['cross_encoder'] = [item['Title'] for item in ranked_results]\nfinal_results['bert_score'] = [item['Title'] for item in ranked_results_bert]\n","200a30b1":"final_results.head()","c1e64687":"\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport seaborn as sns","b7a44581":"item_i = model.encode(['action'])\nitem_i = np.asarray(item_i.astype('float32'))\n\nitem_j = model.encode(['drama'])\nitem_j = np.asarray(item_j.astype('float32'))\n\nitem_k = model.encode(['romance'])\nitem_k = np.asarray(item_k.astype('float32'))","493c7291":"data = np.asarray([item_i, item_j, item_k])","813fd0fa":"data = data.reshape((3,768))","25d8f20a":"tsne = TSNE(n_components=2)\ntsne_results = tsne.fit_transform(data)","b06fa322":"tsne_results.shape","c02c0c30":"pca_df = pd.DataFrame()\npca_df['x'] = tsne_results[:,0]\npca_df['y'] = tsne_results[:,1]","dfadef7b":"plt.figure(figsize=(8,8))\nsns.scatterplot(\n    x=\"x\", y=\"y\",\n    hue=['action','drama','romance'],\n    palette=sns.color_palette(\"hls\", 3),\n    data=pca_df,\n    legend=\"full\",\n    alpha=0.8\n)","ecc67fbb":"# generate random integer values\nfrom random import seed\nfrom random import randint\n# seed random number generator\nseed(1)\n\naction_movie_scores=[]\ndrama_movie_scores=[]\nromance_movie_scores=[]\n# generate some integers\nfor _ in range(10):\n    action_movie_score = randint(0, 10)\n    drama_movie_score = randint(0, 10)\n    romance_movie_score = randint(0, 10)\n    action_movie_scores.append(action_movie_score)\n    drama_movie_scores.append(drama_movie_score)\n    romance_movie_scores.append(romance_movie_score)","e79945dc":"user_watch_hist = pd.DataFrame()\nuser_watch_hist['action_movie'] = action_movie_scores\nuser_watch_hist['drama_movie'] = drama_movie_scores\nuser_watch_hist['romance_movie'] = romance_movie_scores","f50afffd":"\nuser_watch_hist.plot(kind='bar',figsize=(16,10))","1350902a":"user_watch_hist","d0cdc81e":"import scipy.stats as stats","fa162fbb":"action_movie_zscore =   stats.zscore(user_watch_hist['action_movie'])[-1]\ndrama_movie_zscore = stats.zscore(user_watch_hist['drama_movie'])[-1]\nromance_movie_zscore = stats.zscore(user_watch_hist['romance_movie'])[-1]","3780fa71":"def weight(i):\n    alpha = 0.5\n    return alpha*pow((1-alpha),10-i)","941d6da8":"def softmax(x):\n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n    e_x = np.exp(x - np.max(x))\n    return e_x \/ e_x.sum()","6e6bff32":"x = np.asarray([action_movie_zscore, drama_movie_zscore, romance_movie_zscore])\ngenre_weights = softmax(x)","722b1605":"user_encoded_vector = np.asarray([item_i*genre_weights[0]+ item_j*genre_weights[1] +item_k*genre_weights[2]])\nuser_encoded_vector = user_encoded_vector.reshape((1,768))","f84750d0":"from pprint import pprint\n\nquery=\"Artificial Intelligence based action movie\"\nresults=search(query, top_k=5, index=index, model=model)\n\n\nprint(\"\\n\")\nfor result in results:\n    print('\\t',pprint(result))","751fc48c":"candidate_plots = [x['Plot'] for x in results]","43996a8c":"from sentence_transformers import SentenceTransformer, util\n\n#Compute embeddings\nembeddings = model.encode(candidate_plots)\n\n#Compute cosine-similarities for each sentence with each other sentence\ncosine_scores = util.pytorch_cos_sim(user_encoded_vector, embeddings)\n\n#Find the pairs with the highest cosine similarity scores\ntitles = [x['Title'] for x in results]\n\nranked_user_behaviour = [{'Title':x ,'Score': y} for x,y in zip(titles,cosine_scores.numpy()[0])]\nranked_user_behaviour = sorted(ranked_user_behaviour, key=lambda x: x['Score'], reverse=True)","fbbde186":"final_results = pd.DataFrame()\nfinal_results['faiss_ranking'] = [item['Title'] for item in results]\nfinal_results['cross_encoder'] = [item['Title'] for item in ranked_results]\nfinal_results['bert_score'] = [item['Title'] for item in ranked_results_bert]\nfinal_results['user_interaction_ranking'] = [item['Title'] for item in ranked_user_behaviour]","59ff4ce5":"final_results","5d86e448":"def fetch_movie_info(dataframe_idx):\n    info = df.iloc[dataframe_idx]\n    meta_dict = {}\n    meta_dict['Title'] = info['Title']\n    return meta_dict\n    \n\nt=time.time()\nquery_vector = user_encoded_vector\ntop_k = index.search(query_vector, 20)\nprint('>>>> Recommendation Results in Total Time: {}'.format(time.time()-t))\ntop_k_ids = top_k[1].tolist()[0]\ntop_k_ids = list(np.unique(top_k_ids))\n[fetch_movie_info(idx) for idx in top_k_ids]\n","23c67db2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c6c0bfe9":"# Re-Ranker: Cross-Encoder\n\nThe retriever has to be efficient for large document collections with millions of entries. However, it might return irrelevant candidates.\n\nA re-ranker based on a Cross-Encoder can substantially improve the final results for the user. The query and a possible document is passed simultaneously to transformer network, which then outputs a single score between 0 and 1 indicating how relevant the document is for the given query.\n\nThe advantage of Cross-Encoders is the higher performance, as they perform attention across the query and the document.\n\nScoring thousands or millions of (query, document)-pairs would be rather slow. Hence, we use the retriever to create a set of e.g. 100 possible candidates which are then re-ranked by the Cross-Encoder.\n"}}