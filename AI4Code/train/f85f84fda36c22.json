{"cell_type":{"ab6dba2c":"code","afd2760c":"code","3ce0e0b8":"code","f66ccdeb":"code","cdd198f7":"code","2d9e719c":"code","0ecf68c3":"code","b6e4bd67":"code","dab4a9f3":"code","e2563620":"code","059eb713":"markdown","830e09c0":"markdown","1fe8703d":"markdown","00b771b2":"markdown","b615a4a6":"markdown"},"source":{"ab6dba2c":"import pandas as pd, numpy as np\nfrom matplotlib import pyplot as plt\n\nimport scipy.stats  as stats","afd2760c":"pd.options.display.max_columns = 50","3ce0e0b8":"best = pd.read_csv(\"..\/input\/accuracy-best-public-lbs\/kkiller_first_public_notebook_under050_v5.csv\")\nbest.head()","f66ccdeb":"sales = pd.read_csv(\"..\/input\/m5-forecasting-uncertainty\/sales_train_validation.csv\")\nsales.head()","cdd198f7":"sub = best.merge(sales[[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]], on = \"id\")\nsub[\"_all_\"] = \"Total\"\nsub.shape","2d9e719c":"sub.head()","0ecf68c3":"qs = np.array([0.005,0.025,0.165,0.25, 0.5, 0.75, 0.835, 0.975, 0.995])\nqs.shape\n\n\ndef get_ratios(coef=0.15):\n    qs2 = np.log(qs\/(1-qs))*coef\n    ratios = stats.norm.cdf(qs2)\n    ratios \/= ratios[4]\n    ratios = pd.Series(ratios, index=qs)\n    return ratios.round(3)\n\n#coef between 0.05 and 0.24 is used, probably suboptimal values for now\n\nlevel_coef_dict = {\"id\": get_ratios(coef=0.3), \"item_id\": get_ratios(coef=0.15),\n                   \"dept_id\": get_ratios(coef=0.08), \"cat_id\": get_ratios(coef=0.07),\n                   \"store_id\": get_ratios(coef=0.08), \"state_id\": get_ratios(coef=0.07), \"_all_\": get_ratios(coef=0.05),\n                   (\"state_id\", \"item_id\"): get_ratios(coef=0.19),  (\"state_id\", \"dept_id\"): get_ratios(coef=0.1),\n                    (\"store_id\",\"dept_id\") : get_ratios(coef=0.11), (\"state_id\", \"cat_id\"): get_ratios(coef=0.08),\n                    (\"store_id\",\"cat_id\"): get_ratios(coef=0.1)\n                  }\n\n","b6e4bd67":"level_coef_dict[\"id\"]","dab4a9f3":"level_coef_dict[\"cat_id\"]","e2563620":"\n\ndef quantile_coefs(q, level):\n    ratios = level_coef_dict[level]\n               \n    return ratios.loc[q].values\n\ndef get_group_preds(pred, level):\n    df = pred.groupby(level)[cols].sum()\n    q = np.repeat(qs, len(df))\n    df = pd.concat([df]*9, axis=0, sort=False)\n    df.reset_index(inplace = True)\n    df[cols] *= quantile_coefs(q, level)[:, None]\n    if level != \"id\":\n        df[\"id\"] = [f\"{lev}_X_{q:.3f}_validation\" for lev, q in zip(df[level].values, q)]\n    else:\n        df[\"id\"] = [f\"{lev.replace('_validation', '')}_{q:.3f}_validation\" for lev, q in zip(df[level].values, q)]\n    df = df[[\"id\"]+list(cols)]\n    return df\n\ndef get_couple_group_preds(pred, level1, level2):\n    df = pred.groupby([level1, level2])[cols].sum()\n    q = np.repeat(qs, len(df))\n    df = pd.concat([df]*9, axis=0, sort=False)\n    df.reset_index(inplace = True)\n    df[cols] *= quantile_coefs(q, (level1, level2))[:, None]\n    df[\"id\"] = [f\"{lev1}_{lev2}_{q:.3f}_validation\" for lev1,lev2, q in \n                zip(df[level1].values,df[level2].values, q)]\n    df = df[[\"id\"]+list(cols)]\n    return df\n\nlevels = [\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\", \"_all_\"]\ncouples = [(\"state_id\", \"item_id\"),  (\"state_id\", \"dept_id\"),(\"store_id\",\"dept_id\"),\n                            (\"state_id\", \"cat_id\"),(\"store_id\",\"cat_id\")]\ncols = [f\"F{i}\" for i in range(1, 29)]\n\ndf = []\nfor level in levels :\n    df.append(get_group_preds(sub, level))\nfor level1,level2 in couples:\n    df.append(get_couple_group_preds(sub, level1, level2))\ndf = pd.concat(df, axis=0, sort=False)\ndf.reset_index(drop=True, inplace=True)\ndf = pd.concat([df,df] , axis=0, sort=False)\ndf.reset_index(drop=True, inplace=True)\ndf.loc[df.index >= len(df.index)\/\/2, \"id\"] = df.loc[df.index >= len(df.index)\/\/2, \"id\"].str.replace(\n                                    \"_validation$\", \"_evaluation\")\n\ndf.shape\n\ndf.head()\n\ndf.to_csv(\"submission01.csv\", index = False)","059eb713":"My aim is to show that uncertainty, in some sense, decreases on higher levels of aggregation.\n\nThe bulk of the work in this kernel is done by [@kneroma](https:\/\/www.kaggle.com\/kneroma) in [this kernel](https:\/\/www.kaggle.com\/kneroma\/from-point-to-uncertainty-prediction), so make sure to check it out.\n\n","830e09c0":"## Different ratios for different aggregation levels\n\nThe higher the aggregation level, the more confident we are in the point prediction --> lower coef, relatively smaller range of quantiles","1fe8703d":"For the the lowest level (30490 series), the smallest and biggest quantiles are 20% and 180% of the point prediction. For categories (3 series), the model will be way more confident: the smallest quantile will be 71%, the biggest will be 129% of the point prediction.","00b771b2":"If you like it, please upvote:)","b615a4a6":"Let's see how ranges differ!"}}