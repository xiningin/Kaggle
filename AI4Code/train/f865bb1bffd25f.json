{"cell_type":{"b6d34299":"code","9d9f8729":"code","fb61264b":"code","1a645bb5":"code","46b92311":"code","32afdf3b":"code","59ba4e10":"code","5947c382":"code","09494421":"code","53128086":"code","03c12455":"code","0dad341c":"code","0a27672d":"code","b2e20e42":"code","eaf1c1ae":"code","8a369bbe":"code","3f0407ac":"code","a47da508":"code","ae7f4e8a":"code","2599dfba":"code","cd9bb6a6":"code","a9754ca0":"code","b7804d72":"code","2085c34b":"code","872a5412":"code","62c0a53f":"code","f73cf6a6":"code","0730b67e":"code","8d14d5bd":"code","3eda167d":"code","8f0fc0ad":"code","9148901a":"code","c7a596ec":"code","6ff259ba":"code","b19c76e7":"code","c39bf4c0":"code","c3b6bf0e":"code","eb725c1c":"code","40b49e4f":"code","ae1ef2b9":"code","1f99e0d3":"code","e314916a":"code","e3c8011f":"code","8c8e3b0a":"code","002e6302":"code","29d74114":"code","49caa7f9":"code","e9b4959a":"code","dd12a05f":"code","46c6d609":"code","cbc094ca":"code","fafa4a8a":"code","2b8a2f1f":"code","22e8341b":"code","cebdb05e":"code","d7d1c028":"code","840eaefa":"code","ffb66716":"code","58c1a6df":"code","57f95922":"code","4fa324b3":"code","a135df80":"code","74f26f6d":"code","7b170b75":"code","61b8d624":"code","921d88c1":"code","82336850":"code","7f812732":"code","2c6d1aca":"code","f78eb94e":"code","1a098e5c":"code","5588a648":"code","7abb973c":"markdown","255fcd1a":"markdown","e85d1fff":"markdown","295785c7":"markdown","26e307d1":"markdown","fc1de563":"markdown","564d9b02":"markdown","5ab9ccf4":"markdown","19e8ec32":"markdown","f765cb02":"markdown","f68103e6":"markdown","3a926be7":"markdown","eb0be883":"markdown","5f2a8b6c":"markdown","1f3d030d":"markdown","0800ebda":"markdown","0994655b":"markdown","de5cc4bd":"markdown","71c826b7":"markdown","c135de02":"markdown","842c4593":"markdown","844498ec":"markdown","6f01a0d1":"markdown","cc306957":"markdown","9d6326dd":"markdown","a4f3601c":"markdown","8fd5af0e":"markdown","30b61c29":"markdown","6b816cd5":"markdown","90eb9868":"markdown","5ef38770":"markdown","c7bcbe70":"markdown","fb064c69":"markdown","812b4ef5":"markdown","6079a7e7":"markdown","8ca9a10a":"markdown","e1f0bcc6":"markdown","541277d8":"markdown","dcda14a1":"markdown"},"source":{"b6d34299":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt","9d9f8729":"from keras.preprocessing.text import text_to_word_sequence\nfrom keras.preprocessing.text import Tokenizer  \nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import models\nfrom keras import layers\nfrom keras import losses\nfrom keras import metrics\nfrom keras import optimizers","fb61264b":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom keras.initializers import Constant","1a645bb5":"from tqdm import tqdm\nimport re\nimport string\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('wordnet')\nfrom nltk.corpus import wordnet\nimport unicodedata\nimport html\nstop_words = stopwords.words('english')","46b92311":"train= pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","32afdf3b":"train.head(5)","59ba4e10":"train.info()","5947c382":"test.info()","09494421":"print('Train shape: ', train.shape)\nprint('Test shape: ', test.shape)","53128086":"def remove_special_chars(text):\n    re1 = re.compile(r'  +')\n    x1 = text.lower().replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n        '<br \/>', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n        ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n    return re1.sub(' ', html.unescape(x1))\n\n\ndef remove_non_ascii(text):\n    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n\n\ndef to_lowercase(text):\n    return text.lower()\n\n\n\ndef remove_punctuation(text):\n    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n    translator = str.maketrans('', '', string.punctuation)\n    return text.translate(translator)\n\n\ndef replace_numbers(text):\n    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n    return re.sub(r'\\d+', '', text)\n\n\ndef remove_whitespaces(text):\n    return text.strip()\n\n\ndef remove_stopwords(words, stop_words):\n    \"\"\"\n    :param words:\n    :type words:\n    :param stop_words: from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n    or\n    from spacy.lang.en.stop_words import STOP_WORDS\n    :type stop_words:\n    :return:\n    :rtype:\n    \"\"\"\n    return [word for word in words if word not in stop_words]\n\n\ndef stem_words(words):\n    \"\"\"Stem words in text\"\"\"\n    stemmer = PorterStemmer()\n    return [stemmer.stem(word) for word in words]\n\ndef lemmatize_words(words):\n    \"\"\"Lemmatize words in text\"\"\"\n\n    lemmatizer = WordNetLemmatizer()\n    return [lemmatizer.lemmatize(word) for word in words]\n\ndef lemmatize_verbs(words):\n    \"\"\"Lemmatize verbs in text\"\"\"\n\n    lemmatizer = WordNetLemmatizer()\n    return ' '.join([lemmatizer.lemmatize(word, pos='v') for word in words])\n\ndef text2words(text):\n    return word_tokenize(text)\n\ndef normalize_text( text):\n    text = remove_special_chars(text)\n    text = remove_non_ascii(text)\n    text = remove_punctuation(text)\n    text = to_lowercase(text)\n    text = replace_numbers(text)\n    words = text2words(text)\n    words = remove_stopwords(words, stop_words)\n    #words = stem_words(words)# Either stem ovocar lemmatize\n    words = lemmatize_words(words)\n    words = lemmatize_verbs(words)\n\n    return ''.join(words)","03c12455":"def normalize_corpus(corpus):\n    return [normalize_text(t) for t in corpus]","0dad341c":"train_labels = train[\"target\"]\ntrain = train.drop([\"target\"], axis = 1)","0a27672d":"# work only on tweets text.\ntrain_text = train[\"text\"]\ntest_text = test[\"text\"]","b2e20e42":"# normalize tweets\ntrain_text = normalize_corpus(train_text)\ntest_text = normalize_corpus(test_text)","eaf1c1ae":"# Fit the tokenizer\n\nvocab_sz = 10000\ntok = Tokenizer(num_words=vocab_sz, oov_token='UNK')\ntok.fit_on_texts(train_text + test_text)","8a369bbe":"# Extract binary BoW features\nx_train = tok.texts_to_matrix(train_text, mode='binary')\nx_test = tok.texts_to_matrix(test_text, mode='binary')\ny_train = np.asarray(train_labels).astype('float32')","3f0407ac":"print(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)","a47da508":"model = models.Sequential()\nmodel.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.summary()","ae7f4e8a":"model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n              loss=losses.binary_crossentropy,\n              metrics=[metrics.binary_accuracy])","2599dfba":"x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=42)","cd9bb6a6":"history = model.fit(x_train,\n                    y_train,\n                    epochs=20,\n                    batch_size=512,\n                    validation_data=(x_val, y_val))","a9754ca0":"history_dict = history.history\nhistory_dict.keys()\n\n\nacc = history.history['binary_accuracy']\nval_acc = history.history['val_binary_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss, 'bo', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","b7804d72":"plt.clf()   # clear figure\nacc_values = history_dict['binary_accuracy']\nval_acc_values = history_dict['val_binary_accuracy']\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","2085c34b":"# Extract binary BoW features\nx_train = tok.texts_to_matrix(train_text, mode='count')\nx_test = tok.texts_to_matrix(test_text, mode='count')\ny_train = np.asarray(train_labels).astype('float32')","872a5412":"# Split the train and test data\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=42)","62c0a53f":"model = models.Sequential()\nmodel.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))","f73cf6a6":"model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n              loss=losses.binary_crossentropy,\n              metrics=[metrics.binary_accuracy])","0730b67e":"history = model.fit(x_train,\n                    y_train,\n                    epochs=20,\n                    batch_size=512,\n                    validation_data=(x_val, y_val))\nhistory_dict = history.history\nhistory_dict.keys()","8d14d5bd":"acc = history.history['binary_accuracy']\nval_acc = history.history['val_binary_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss, 'bo', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","3eda167d":"plt.clf()   # clear figure\nacc_values = history_dict['binary_accuracy']\nval_acc_values = history_dict['val_binary_accuracy']\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","8f0fc0ad":"# Extract binary BoW features\nx_train = tok.texts_to_matrix(train_text, mode='freq')\nx_test = tok.texts_to_matrix(test_text, mode='freq')\ny_train = np.asarray(train_labels).astype('float32')","9148901a":"x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=42)","c7a596ec":"model = models.Sequential()\nmodel.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))","6ff259ba":"model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n              loss=losses.binary_crossentropy,\n              metrics=[metrics.binary_accuracy])","b19c76e7":"history = model.fit(x_train,\n                    y_train,\n                    epochs=20,\n                    batch_size=512,\n                    validation_data=(x_val, y_val))\nhistory_dict = history.history\nhistory_dict.keys()","c39bf4c0":"\nacc = history.history['binary_accuracy']\nval_acc = history.history['val_binary_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss, 'bo', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","c3b6bf0e":"\nplt.clf()   # clear figure\nacc_values = history_dict['binary_accuracy']\nval_acc_values = history_dict['val_binary_accuracy']\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","eb725c1c":"# Extract binary BoW features\nvocab_sz = 10000\nvectorizer = TfidfVectorizer(max_features=vocab_sz) # limit the vocab\n# tokenize and build vocab\nvectorizer.fit(train_text + test_text)\n\n\nx_train = vectorizer.transform(train_text)\nx_test =vectorizer.transform(test_text)\ny_train = np.asarray(train_labels).astype('float32')","40b49e4f":"x_train = x_train.toarray()\nx_test = x_test.toarray()","ae1ef2b9":"x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=42)","1f99e0d3":"model = models.Sequential()\nmodel.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))","e314916a":"model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n              loss=losses.binary_crossentropy,\n              metrics=[metrics.binary_accuracy])","e3c8011f":"history = model.fit(x_train,\n                    y_train,\n                    epochs=20,\n                    batch_size=4,\n                    validation_data=(x_val, y_val))\nhistory_dict = history.history\nhistory_dict.keys()","8c8e3b0a":"\nacc = history.history['binary_accuracy']\nval_acc = history.history['val_binary_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss, 'bo', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","002e6302":"plt.clf()   # clear figure\nacc_values = history_dict['binary_accuracy']\nval_acc_values = history_dict['val_binary_accuracy']\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","29d74114":"# read the data again\ntrain= pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","49caa7f9":"# Tokenizing the text\nvocab_sz = 10000\ntok = Tokenizer(num_words=vocab_sz, oov_token='UNK')\ntok.fit_on_texts(train_text + test_text)","e9b4959a":"# Extract binary BoW features\nx_train = tok.texts_to_matrix(train_text, mode='freq')\nx_test = tok.texts_to_matrix(test_text, mode='freq')\ny_train = np.asarray(train_labels).astype('float32')","dd12a05f":"# split the data\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=42)","46c6d609":"model = models.Sequential()\nmodel.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))","cbc094ca":"model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n              loss=losses.binary_crossentropy,\n              metrics=[metrics.binary_accuracy])","fafa4a8a":"history = model.fit(x_train,\n                    y_train,\n                    epochs=20,\n                    batch_size=512,\n                    validation_data=(x_val, y_val))\nhistory_dict = history.history\nhistory_dict.keys()","2b8a2f1f":"acc = history.history['binary_accuracy']\nval_acc = history.history['val_binary_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss, 'bo', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","22e8341b":"plt.clf()   # clear figure\nacc_values = history_dict['binary_accuracy']\nval_acc_values = history_dict['val_binary_accuracy']\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","cebdb05e":"from numpy import asarray\nfrom numpy import zeros\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import Embedding","d7d1c028":"!wget http:\/\/nlp.stanford.edu\/data\/glove.6B.zip\n!unzip glove.6B.zip","840eaefa":"def create_corpus_new(df):\n    corpus=[]\n    for tweet in tqdm(df['text']):\n        words=[word.lower() for word in word_tokenize(tweet)]\n        corpus.append(words)\n    return corpus   ","ffb66716":"# read the data again\ntrain = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\n\ntrain_labels = train[\"target\"]\ntrain = train.drop([\"target\"], axis = 1)\n\ntrain_labels = np.asarray(train_labels).astype('float32')\n\ntrain_text = train[\"text\"]\ntest_text = test[\"text\"]\n\ntrain_text = normalize_corpus(train_text)\ntest_text = normalize_corpus(test_text)\n\n\n# tok = Tokenizer()\n# # Fit the tokenizer\n# tok.fit_on_texts(train_text + test_text)\n# # Tokenizing the text\n# vocab_size = len(tok.word_index) + 1\n# tok = Tokenizer(num_words=vocab_sz, oov_token='UNK')","58c1a6df":"df=pd.concat([train,test])","57f95922":"corpus=create_corpus_new(df)","4fa324b3":"# load the whole embedding into memory\nembedding_dict={}\nwith open('glove.6B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word = values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","a135df80":"MAX_LEN=50\ntok =Tokenizer()\n# Fit the tokenizer\ntok.fit_on_texts(corpus)\nsequences = tok.texts_to_sequences(corpus)\n# # Tokenizing the text\n# vocab_size = len(tok.word_index) + 1\n# tok = Tokenizer(num_words=vocab_sz, oov_token='UNK')\n\ntweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","74f26f6d":"word_index = tok.word_index\nprint('Number of unique words:',len(word_index))","7b170b75":"num_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in tqdm(word_index.items()):\n    if i < num_words:\n        emb_vec=embedding_dict.get(word)\n        if emb_vec is not None:\n            embedding_matrix[i]=emb_vec    ","61b8d624":"tweet_pad[0][0:]","921d88c1":"n_latent_factors = 100\nmodel = models.Sequential()\nmodel.add(layers.Embedding(num_words, n_latent_factors, weights=[embedding_matrix], \n                           input_length=MAX_LEN, trainable=True))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.summary()","82336850":"train=tweet_pad[:train.shape[0]]\ntest=tweet_pad[train.shape[0]:]","7f812732":"X_train,x_val,y_train,y_val = train_test_split(train,train_labels,test_size=0.25)\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",x_val.shape)","2c6d1aca":"model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n              loss=losses.binary_crossentropy,\n              metrics=[metrics.binary_accuracy])\n\nhistory = model.fit(X_train,\n                    y_train,\n                    epochs=20,\n                    batch_size=512,\n                    validation_data=(x_val, y_val))\nhistory_dict = history.history\nhistory_dict.keys()","f78eb94e":"\nacc = history.history['binary_accuracy']\nval_acc = history.history['val_binary_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss, 'bo', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","1a098e5c":"\nplt.clf()   # clear figure\nacc_values = history_dict['binary_accuracy']\nval_acc_values = history_dict['val_binary_accuracy']\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","5588a648":"submission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\ntest_pred = model.predict(test)\ntest_pred_int = test_pred.round().astype('int')\nsubmission['target'] = test_pred_int\nsubmission.to_csv('submission.csv', index=False)","7abb973c":"##### Train and Validation Accuracy","255fcd1a":"# Frequency Model - No Pre-Processing","e85d1fff":"Almost all the models achieved the same accuracy, so we will choose the frequency model and try it on the data without preprocessing.","295785c7":"Use Keras Tokenizer","26e307d1":"# Bag of Words","fc1de563":"## Model","564d9b02":"##### Training and validation accuracy","5ab9ccf4":"##### Train and Validation Accuracy","19e8ec32":"##### Train and Validation Loss","f765cb02":"Now after preparing the whole text, lets try different models and see how it differs from one to another.","f68103e6":"To solve this, we will use an Embedding layer, that will encode latent factors of the word features. ","3a926be7":"As we limitied our vocab to 10,000, this minimized the effect of bagging.\nTh above model suffers two issues:\n\nThose models suffered from many issues: \n* Sequence info is lost.\n\n* Sparsity: the input scalars represent words indices. Those are sparse, and do not include any info about the word. this input is very sparse, where in a sentence, we only have few words of the vocab. ","eb0be883":"##### Training and validation accuracy","5f2a8b6c":"##### \u0644\u0625raining and validation loss","1f3d030d":"# TF-IDF","0800ebda":"# Count Features","0994655b":"# Data loading","de5cc4bd":"# Frequency Model","71c826b7":"# Text Pre-processing","c135de02":"### Tokenization ","842c4593":"### Model","844498ec":"##### Train and Validation Loss","6f01a0d1":"##### Train and Validation Loss","cc306957":"Apply some operations on the text so it can be helpful and useful to the model.","9d6326dd":"### Model","a4f3601c":"### Model","8fd5af0e":"# Submission","30b61c29":"Validation accuracy gets lower every epoch, this is because counts are not normalized values, so we will try the frequency based model which will normalize the values.","6b816cd5":"##### Training and validation loss","90eb9868":"### Model","5ef38770":"After normalizing the values, it didn't improve the model but it limited the underfitting effect on the last model.","c7bcbe70":"# Embedding","fb064c69":"Now lets split the data to train and validation","812b4ef5":"##### Train and Validation Accuracy","6079a7e7":"From the very first look to the data, we have 7600 tweet, 2k of them without location and around 80 without keywords.","8ca9a10a":"##### Train and Validation Accuracy","e1f0bcc6":"# Binary Features","541277d8":"### Model","dcda14a1":"##### Train and Validation Loss"}}