{"cell_type":{"03ed78ac":"code","41f2a637":"code","6ae7b309":"code","158c3cc4":"code","ff69d8aa":"code","dce07e92":"code","7bd59bc7":"code","ad23ce17":"code","b5851a0a":"code","b5bdb36b":"code","1132ed52":"markdown","018fb33c":"markdown","4016d176":"markdown","fb2df15f":"markdown","182f67f5":"markdown","4263493e":"markdown","04950083":"markdown","b92fbf7d":"markdown","878f5849":"markdown","c7110d6b":"markdown","e0fedab9":"markdown","323f8873":"markdown","1b6d6347":"markdown"},"source":{"03ed78ac":"from IPython.display import Image\nImage(\"..\/input\/jobbulletindata\/results.png\")","41f2a637":"from IPython.display import Image\nImage(\"..\/input\/jobbulletindata\/nonnative-2.png\")","6ae7b309":"from IPython.display import Image\nImage(\"..\/input\/jobbulletindata\/nonnative.png\")","158c3cc4":"from IPython.display import Image\nImage(\"..\/input\/jobbulletindata\/male.png\")\n","ff69d8aa":"from IPython.display import Image\nImage(\"..\/input\/jobbulletindata\/male2.png\")","dce07e92":"from IPython.display import Image\nImage(\"..\/input\/jobbulletindata\/female.png\")","7bd59bc7":"from IPython.display import Image\nImage(\"..\/input\/jobbulletindata\/female2.png\")","ad23ce17":"'''\nScript:     biasFinder.py\nProject:    City of Los Angeles Job Bulletin analysis\nPurpose:    Extract data from bias terms analysis into a csv file:\n\nCSV database row types:\n\nJobClass   TermLocation   Term            Bias\n1117       None           None            gender_terms       #example when no biased terms are found by the detector\n1117       [1554-1565]    examination     native_speakers    #when possibly biased terms are found, record term location offsets, term, and detector\n'''\n\nimport os\nimport pandas as pd\nimport re\nimport string\n\ndef process_file(jobClass, file, b, skip_language_recommendations):\n    '''\n    Parse bias detector files generated by gender-bias toolkit\n\n    jobClass - 4 digit number\n    file - file generated by gender-bias toolkit\n    b - list for storing results\n    skip_language_recommendations - gender-bias toolkit stores recommendations for synonyms in these files but this information is not used here so skip it\n    \n    '''\n    r = []  # list for each row of terms\n    bias = ''\n    strippables = \"!#$%&'()*+,-.\/;<=>?@\\^_`{|}~\" + string.whitespace\n\n    f = open(file)\n    for line in f:\n        line = line.strip(strippables)\n        if skip_language_recommendations:\n            if line.startswith('Consider replacing'):\n                continue\n        if line.startswith('Unnecessary use of gender terms'):\n                bias = 'gender_terms'\n        elif line.startswith('Terms biased towards native speakers'):\n                bias = 'native_speakers'\n        elif line.startswith('Terms focusing on effort vs accomplishment'):\n                bias = 'effort_accomplishment'            \n        elif line.startswith('Terms about personal life'):\n                bias = 'personal_life'\n        elif line.startswith('Terms biased towards men'):\n                bias = 'male_terms'\n        elif line.startswith('Terms biased towards women'):\n                bias = 'female_terms'\n\n        elif 'SUMMARY: Too few words about concrete accomplishments' in line:\n            r.append(jobClass)\n            r.append('0-0')\n            r.append('none')\n            r.append(bias)\n            b.append(r)\n            r = []\n\n        elif 'SUMMARY: None' in line:\n            r.append(jobClass)\n            r.append('0-0')\n            r.append('none')\n            r.append(bias)\n            b.append(r)\n            r = []\n\n        elif line.startswith('['):\n            terms = line.split(':')\n            termLocation = terms[0].strip('[]')\n            term = terms[1].strip(' ')\n            if \"=\" in term:\n                continue\n            elif \"\/\" in term:\n                continue\n            else:\n                r.append(jobClass)\n                r.append(termLocation)\n                r.append(term)\n                r.append(bias)\n                b.append(r)\n            r = []\n    return b\n\ndef main():\n    LAJobFiles = [os.path.join(root, file) for root, folder, LAJobFile in os.walk('..\/input\/jobbulletindata\/JBR_BiasText\/') for file in LAJobFile]\n    print(\"\\n\",len(LAJobFiles), \"LA Job Files\\n\")\n\n    b = []    \n    biasAnalysis = [process_file(LAJobFile[:4], LAJobFile, b, skip_language_recommendations=True) for LAJobFile in LAJobFiles]\n    ba = pd.DataFrame(biasAnalysis[0], columns=('JobClass','TermLocation','Term','Bias'))\n\n    #print(\"\\nSample rows\")\n    #print(ba.head(5))\n    # TODO - consider replacing job classes with ###. with 0###\n    ba.to_csv('biasTerms.csv', index=False)\n\n    baTerms = ba['Term'].unique()\n    print(\"\\n\")\n    print((len(baTerms)-1), \" possible biased words\\n\")\n    #print(baTerms[1:])\n\nif __name__ == '__main__':\n    main()\n","b5851a0a":"'''\nScript:     biasTermsAnalyzer.py\nPurpose:    Evaluated possible bias terms found in City of Los Angeles Job Bulletins\n'''\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\npd.options.display.max_columns=5\n\ndef main():\n    '''\n    input files:\n\n        ba             bias analysis DataFrame input from: \/JBR_Output\/biasTerms.csv\n        simpleWords    list of English synonyms for complex terms input from \/genderbias\/nativespeakers\/Englishwords.wordlist\n        nonBiasTerms   terms specific to City of Los Angeles input from \/JBR_Resources\/nonBiasTerms.csv\n\n    output files:\n\n        fo             output file with recommendations        JBR_Output\/BiasStudyRecommendations.txt       \n    '''\n    \n    fo = open(\"BiasStudyRecommendations.txt\", \"w+\")\n    ba = pd.read_csv('..\/input\/jobbulletindata\/JBR_Output\/biasTerms.csv', dtype='str')\n\n    print(\"STATISTICS:\\n\", file=fo)\n    print(\"STATISTICS:\\n\")\n\n    # Number of job classes processed\n    jobClasses = ba['JobClass'].unique()\n    possiblyUsed = len(ba)\n    a = len(jobClasses)\n    print(a, \" job classes evaluated\\n\", file=fo)\n    print(\"Biased terms may have been used \", possiblyUsed,\" times\\n\", file=fo)\n    print(a, \" job classes evaluated\\n\")\n    print(\"Biased terms may have been used \",possiblyUsed,\" times\\n\")\n\n    # Bias categories (also called bias detectors)\n    biasCats = ba['Bias'].unique().tolist()\n    print(len(biasCats), ' bias detectors used:', file=fo)\n    print(len(biasCats), ' bias detectors used:')\n    print(biasCats, \"\\n\", file=fo)\n    print(biasCats, \"\\n\")\n \n    # Possible biased words found\n    biasTerms = ba['Term'].unique().tolist()\n    biasTerms.sort()\n    possiblyBiased = len(biasTerms) - 1\n    print(possiblyBiased, \" unique words were found :\\n \", biasTerms[1:], file=fo)\n    print(possiblyBiased, \" unique words were found:\\n \", biasTerms[1:])\n\n\n    # NATIVE LANGUAGE BIASES\n    complexTermsCnt = pd.value_counts(ba['Term'].loc[ba['Bias']=='native_speakers'], ascending=True)\n\n    # WordCloud\n    wordcloud = WordCloud(max_font_size=30, max_words=20,background_color=\"white\").generate(complexTermsCnt.sort_values(ascending=False).head(20).to_string())\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.title('Words that can be simplifed for non-native speakers')\n    plt.show()\n\n    # Horizontal bar graph\n    complexTermsCnt = complexTermsCnt.tail(20)\n    complexTermsCnt.plot(figsize=(10,5), x='Count', y='Complex Term', kind='barh', color='black', title='Words that can be simplified for non-native speakers')\n    plt.show()\n\n    print(\"\\nNATIVE LANGUAGE BIASES\", file=fo)\n\n    # Words that can be simplified for non-native speakers\n    print(\"\\nWords that can be simplifed for non-native speakers:\", file=fo)\n    print(\"\\nWord       Number of times used\", file=fo)\n    print(\"\\nWords that can be simplifed for non-native speakers:\")\n    print(\"\\nWord       Number of times used\")\n    print(\"\\n\", complexTermsCnt.sort_values(ascending=False), file=fo)\n    print(\"\\n\", complexTermsCnt.sort_values(ascending=False))\n\n    # Recommend shorter synonyms\n    print(\"\\nRECOMMENDATIONS: use simple language:\\n\", file=fo)\n    print(\"\\nRECOMMENDATIONS: use simple language:\\n\")\n    # lookup synonyms\n    simpleWords = pd.read_csv('..\/input\/jobbulletindata\/Englishwords.wordlist')\n    c = ba['Term'].loc[ba['Bias']=='native_speakers'].unique()\n    complexWords = pd.DataFrame(data=c.flatten(), columns = ['Used'])\n    # find the shortest two synonyms for complex words\n    synonyms = pd.merge(complexWords, simpleWords, how='left')\n    synonyms.dropna(subset=['Recommend1'], inplace=True)\n    print(synonyms, file=fo)\n    print(synonyms)\n    print(\"\\nBiasStudyResults.txt contains instructions for finding sentences where biased words are used.\")\n\n    # GENDER-BIASED TERMS\n    maleTerms = pd.value_counts(ba['Term'].loc[ba['Bias']=='male_terms'], ascending=True)\n    maleTerms = maleTerms.tail(20)\n\n    # WordCloud\n    wordcloud = WordCloud(max_font_size=50, max_words=100,background_color=\"white\", colormap=\"Blues\").generate(maleTerms.to_string())\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.title(\"Top 20 Possible Male-biased Terms\")\n    plt.show()\n\n    # Horizontal bar graph\n    maleTerms.plot(figsize=(10,6), x='Count', y='Male Biased', kind='barh', title='Top 20 possible male biased terms')\n    plt.show()\n    print(\"\\nGENDER BIASES\", file=fo)\n    print('\\nPossible male-biased terms:', file=fo)\n    print(\"\\nWord       Number of times used\", file=fo)\n\n    print(\"\\nGENDER BIASES\")\n    print('\\nPossible male-biased terms:')\n    print(\"\\nWord       Number of times used\")\n    print(maleTerms.sort_values(ascending=False), file=fo)\n    print(maleTerms.sort_values(ascending=False))\n        \n    femaleTerms = pd.value_counts(ba['Term'].loc[ba['Bias'].isin(['female_terms','personal_life','gender_terms','effort_accommplishment'])], ascending=True)\n    femaleTerms = femaleTerms.tail(20)\n\n    # WordCloud\n    wordcloud = WordCloud(max_font_size=50, max_words=100,background_color=\"white\", colormap=\"PuRd\").generate(femaleTerms.to_string())\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.title(\"Top 20 Possible Female-biased Terms\")\n    plt.axis('off')\n    plt.show()\n\n    # Horizontal bar graph\n    femaleTerms[:-1].plot(figsize=(10,6), x='Count', y='Female Biased', kind='barh', color='red', title='Top 20 possible female biased terms')\n    plt.show()\n    print('\\nPossible female-biased terms:', file=fo)\n    print(\"\\nWord       Number of times used\", file=fo)\n    print('\\nPossible female-biased terms:')\n    print(\"\\nWord       Number of times used\")\n    print(femaleTerms[:-1].sort_values(ascending=False), file=fo)\n    print(femaleTerms[:-1].sort_values(ascending=False))\n\n    fo.close()\n    \nif __name__ == '__main__':\n    main()\n","b5bdb36b":"'''\nScript:  biasVerifier\nPurpose: create log and csv file with biased words\n\nFiles:\n\nBiasStudyResults.txt\nbiasTerms.csv\n'''\nimport os\nimport pandas as pd\nimport re\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n\ndef getJobClass(JobBulletins):\n    '''\n        get the Job Class number from the filename where filename is JobBulletin and Job Class is always the first 4 digits after the name and before the version and date\n    '''\n    jbDict = {}\n    for JobBulletin in JobBulletins:\n        #JobBulletin = re.sub(\"[\/]\",\"\/'\",JobBulletin)   # original folder structure.  use JobClass[1:5]\n        JobBulletin = re.sub(\"..\/input\/jobbulletindata\/JBR_Output\/\", \"\", JobBulletin)  # kaggle folder structure us JobClass[2:6]\n        JobBulletin = re.sub(\"txt\", \"txt'\",JobBulletin)\n        JobClass = re.sub('[a-zA-Z \/]','',JobBulletin)\n        jbDict[JobClass[2:6]] = JobBulletin\n    return jbDict\n\ndef getWordLocation(loc):\n        ''' get the beginning and ending offsets'''\n        return loc.split('-')\n\ndef getNLPstats():\n    # get the stats from JBR_NLP.py\n    with open (\"..\/input\/jobbulletindata\/JBR_Output\/totWords.txt\") as fs:\n        return [line.strip(\"\\n\") for line in fs]\n\ndef printLog(fo,msg, fileOnly=False):\n    if fileOnly:\n        print(msg,file=fo)\n    else:\n        print(msg, file=fo)\n        print(msg)\n\ndef main():\n\n    fo = open(\"BiasStudyResults.txt\", \"w+\")\n    ba = pd.read_csv('..\/input\/jobbulletindata\/JBR_Output\/biasTerms.csv', dtype='str')  \n\n    # find the number of job classes processed\n    jobClasses = ba['JobClass'].unique()\n    possiblyUsed = len(ba)\n\n    # get the Job Bulletins from the JobBulletin folder\n    JobBulletins = [os.path.join(root, file) for root, folder, JobBulletin in os.walk('..\/input\/jobbulletindata\/JobBulletins') for file in JobBulletin]\n\n    # get jobclass from filename\n    jbDict = getJobClass(JobBulletins)\n    #jb = pd.DataFrame.from_dict(jbDict.items())   # code works in Ubuntu\n    #jb.columns=('JobClass', 'jbFilename')         # code works in Ubuntu\n    jb = pd.DataFrame(list(jbDict.items()),columns=['JobClass','jbFilename'])    # code works on Kaggle\n\n    printLog(fo,\"VERIFYING WHETHER WORDS ARE BIASED: \")\n    printLog(fo,\"\\nFirst, I read \" + str(len(JobBulletins)) + \" Job Bulletins and found words that may be biased\") \n\n    # get a list of words that we know are not biased\n    notBias = pd.read_csv('..\/input\/jobbulletindata\/JBR_Resources\/nonBiasTerms.csv')\n    printLog(fo,\"Then, I checked these words against the not-biased list which contains \" + str(len(notBias))+ \" words\")\n    \n    # get the list of possibly biased terms from gender-bias output\n    b = ba['Term'].loc[ba['Bias'].isin(['female_terms','personal_life','gender_terms','effort_accommplishment', 'male_terms'])].unique()\n    b = b[1:]     # remove 'none'\n    possibleBias = pd.DataFrame(data=b.flatten(), columns=['Term'])\n\n    # \"subtract\" the not-biased words so we have a list of words to be verified\n    bias = pd.merge(possibleBias,notBias,how='left')\n    printLog(fo,\"After checking the not-biased list, there are \" + str(len(bias)) + \" words that need to be verified.  \")\n\n    # find the location of each biased word in each Job Bulletin\n    verifyWords = pd.merge(bias,ba,how='inner')\n    printLog(fo,\"Biased words may have been used \" + str(len(verifyWords)) + \" times, but more analysis is needed.\")\n\n    # find the Job Bulletins where biased words are located\n    vWordsFilename = pd.merge(verifyWords, jb, how='inner')\n    printLog(fo,\"\\nHere's a partial list:\")\n    pd.options.display.max_columns=5\n    printLog(fo,vWordsFilename.head(10))\n    \n    biasedTerms = []                # for reporting a summary of biased terms\n    biasedRow = []                  # for recording each row of biased terms\n    kF = 0                          # count the number of files opened\n    kFP = 0                         # count the number of false positives\n\n    # look up each biased word and print out the phrase where it is used\n    # open each Job Bulletin file only once and process all the biased words in that file then close it\n    priorFilename = ''\n    for i, row in vWordsFilename.iterrows():\n        filename = re.sub('[\\']','',row[\"jbFilename\"])  # strip the single quote so the file will open\n        biasedRow.append(filename)  # store the filename WITHOUT quote so it matches the filename from JBR_NLP.py.  Thus the final statistics will be computed properly\n        \n        # open the Job Bulletin once but verify many words\n        if priorFilename != filename:\n            try:\n                f.close()\n                priorFilename = filename\n            except:\n                if len(priorFilename) > 0:\n                    print(priorFilename, \" STILL OPEN\")\n            try:\n                f = open(filename)\n                kF +=1\n            except:\n                print(\"DOUBLE CHECK THIS FILE: \", filename)\n                continue\n\n        # get the phrase where the biased word is located\n        loc = getWordLocation(row['TermLocation'])\n        msg = \"VERIFIED that '\" + row['Term'] + \"' can be found at offset: \" + str(int(loc[0])) + \" for Job Class: \" + row['JobClass'] + \" in Job Bulletin: \" + row['jbFilename']\n\n        z = 100             # read z characters after the biased word\n        sentence = ''       # capture the phrase with biased word\n        offset = 0          # beginning offset\n\n        _text = f.read()\n        offset = _text.find(row['Term'], offset)\n        f.seek(int(loc[0]))\n        for k in range(offset, offset+z):\n            ch = f.read(1)\n            if k == offset:\n                sentence += \" >>> \"\n            if k == offset + len(row['Term']):\n                sentence += \" <<< \"\n            sentence += ch\n        row[\"SENTENCE\"] = sentence\n\n        # store the results\n        biasedRow.append(row[\"JobClass\"])\n        biasedRow.append(row[\"TermLocation\"])\n        biasedRow.append(row[\"Term\"])\n        biasedRow.append(row[\"Bias\"])\n        biasedRow.append(row[\"SENTENCE\"])\n        biasedTerms.append(biasedRow)\n        biasedRow = []\n        \n        printLog(fo,\"\\n\\nPHRASE FROM JOB BULLETIN MAY HAVE MULTIPLE LINES:\\n\" + sentence + \"...\", fileOnly=True)\n        printLog(fo,msg, fileOnly=True)\n\n        # report false positives\n        searchTerm = row['Term'].lower()\n        falsePositive = re.search(searchTerm, sentence.lower())\n        if not falsePositive:\n            printLog(fo,\"FALSE REPORT WARNING:  the term found doesn't match the search term\", fileOnly=True)\n            kFP += 1\n\n    # REPORT TERMS LIKELY TO BE BIASED\n    likelyBiased = len(biasedTerms)\n    biasedDF = pd.DataFrame(biasedTerms, columns=('FILE_NAME', 'JOB_CLASS', 'WORD_LOCATION','WORD','BIAS','SENTENCE'))\n    biasedDF.index.name = 'IDX'\n    results = biasedDF['WORD'].unique()\n    \n    score = ((likelyBiased - kFP) \/ possiblyUsed * 100)\n    totals = getNLPstats()\n    percentBiased = (len(results) \/ int(totals[1])) * 100\n\n    # print results\n    printLog(fo,\"\\nWhen I began, I thought that biased words were used  \" + str(possiblyUsed) + \"  times.  \")\n    printLog(fo,\"After verifying, I believe that biased words are used  \" + str(likelyBiased - kFP) + \"  times\")\n    printLog(fo,\"I verified each word by looking up the phrase where the word was used.  If the word was not found, then I reported a false positive\")\n    printLog(fo, str(kFP) + \"  false positives were found\")\n    printLog(fo,\"\\n\" + str(np.round(score,0)) + \"% percent of the time biased words are used in Job Bulletins (A lower score is better.  0% is a perfect score)\\n\")\n    printLog(fo, str(np.round(percentBiased,0)) + \"% of the words are biased\")\n    printLog(fo,\"\\n\" + str(len(results)) + \" words that may be biased:\\n\" + str(results))\n\n    # WordCloud\n    wordcloud = WordCloud(max_font_size=50, max_words=100,background_color=\"white\").generate(biasedDF['WORD'].to_string())\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.title('Words used in Job Bulletins that discourage applicants')\n    plt.show()\n\n    # save results in a csv file\n    biasedDF.to_csv(\"BiasedWords.csv\")\n\nif __name__ == '__main__':\n    main()","1132ed52":"```\nThese words tend to be biased towards men:\n\nWord       Number of times used\ncompetitive        1298\ndetermined          822\ncompetencies        479\nanalysis            256\nanalyze             180\nanalyst             178\nanalytical          100\nleadership           88\nanalyzing            55\nanalyses             49\nanalyzes             35\nindependently        35\ndetermining          32\nlogical              23\nconfidentiality      22\nlogically            19\ncompetency           13\ndetermines           12\ncompetent            10\nfighting              8\n```","018fb33c":"Here's a list of the 20 most used complex words:\n\n```\nWord       Number of times used\n\nexamination       5682\nqualifications    3804\napplication       3387\napplications      3025\npromotional       2765\npositions         2284\nappointment       2237\ninformation       2043\naccommodation     2030\nqualifying        1947\npersonnel         1725\nopportunity       1472\nrequirements      1368\ncompetitive       1298\naccordance        1282\navailable         1243\nprinciples        1229\nfollowing         1213\nactivities        1168\ndisqualified      1166\n```","4016d176":"### Use shorter words when possible to make it easier to read Job Bulletins.","fb2df15f":"### Finding biased words and getting synonyms from BigHugeThesaurus\n\nFirst, ```biasFinder.py``` creates a Pandas DataFrame containing data from the gender-bias raw text files.  Next, the ```biasAnalyzer.py``` evaluates the data and reports statistics.  It uses the [BigHugeThesaurus](https:\/\/www.kaggle.com\/nelsondata\/bighugethesaurusapi-jobads) plug-in to find synonyms for complex words.  It recommends the shortest two synonyms.\n","182f67f5":"### Good results \n\nThe Job Bulletins contain 1,060,919 words. However, only 8,200 different words are used. This indicates that these Job Bulletins consistently use a select group of words. The most used words are grammar words, such as \"the\" (used 52,261 times). Grammar words are also called \"stopwords\".  Looking past stopwords, the Job Bulletins focus on candidates because \"candidates\" was used 6,647  times and \"applicants\" was used 5,150 times. Many jobs require an \"examination\" (used 5,691 times) and experience (used 4,115 times) plus qualifications (used 3,810  times).  \n\nOf the original 186 possible biased words, 108 words were verified as likely to be biased.  Biased words were verified by looking up the phrase where the word was used.  If the word was not found, then a false positive was reported.  Initially it appeared that biased words were used 65,134 times.  After verifying, it is more likely that biased words are used 8,001 times.  Zero false positives were found.\n\nOnly 2% of the words appear biased meaning 98% are bias free.  12% percent of the time it appears that biased words are used.\n\nThe most common biased word was \"competitive\" which occurs 1293 times.  Words such as \"competitive\" or \"compete\" attract male applicants more frequently. \n\n1434 complex words were found meaning these words had more than 10 characters.  Using shorter words can make Job Bulletins easier to read.  For example, \"examination\" was used 5,691 times.  Consider using \"exam\" or \"test\".\n\n### Pointing out where Job Bulletins are missing data\n\nOne Job Bulletin has embedded characters that prevent it from opening properly:  POLICE COMMANDER 2251 092917.txt\n\nThese Job Bulletins do not have job class numbers in the file name:\n```\nELECTRIC SERVICE REPRESENTATIVE\nREFUSE COLLECTION TRUCK OPERATOR\nELEVATOR REPAIR SUPERVISOR\nREHABILITATION CONSTRUCTION SPECIALIST\nELECTRICAL SERVICES MANAGER\n```\nThis Job Bulletin was removed because it has no Job Class in the Job Bulletin or the file name:\n```Vocational Worker  DEPARTMENT OF PUBLIC WORKS.txt```\n\n### Recommending improvements for Human Resources\n\nConsider replacing the word 'competitive'.  For example, \"open competitive basis\", might be called \"new-hire basis\" or \"open hiring basis\".  Or the City of Los Angeles can make a decision to add this word to the non-bias list.","4263493e":"Here are specific examples from app:  \n\n```\nConsider replacing 'abilities' with 'skills' or 'talents'\nConsider replacing 'accommodation' with 'aid' or 'help'\nConsider replacing 'accordance' with 'following' or 'like'\nConsider replacing 'activities' with 'tasks' or 'work'\nConsider replacing 'additional' with 'also' or 'plus'\nConsider replacing 'anticipated' with 'expected' or 'future'\nConsider replacing 'application' with 'use' or 'cure'\nConsider replacing 'appointment' with 'job' or 'date'\nConsider replacing 'available' with 'ready' or 'here'\nConsider replacing 'compensation' with 'pay' or 'salary'\nConsider replacing 'competencies' with 'skills' or 'expertise'\nConsider replacing 'competitive' with 'fair' or 'matched'\nConsider replacing 'determine' with 'choose' or 'select'\nConsider replacing 'disqualified' with 'not qualified'\nConsider replacing 'evaluation' with 'review' or 'exam'\nConsider replacing 'examination' with 'exam' or 'test'\nConsider replacing 'following' with 'after' or 'next'\nConsider replacing 'information' with 'data' or 'info'\nConsider replacing 'necessary' with 'needed' \nConsider replacing 'opportunity' with 'chance \nConsider replacing 'personnel' with 'staff' or 'team'\nConsider replacing 'positions' with 'jobs'\nConsider replacing 'principles' with 'rules' or 'guidelines'\nConsider replacing 'professional' with 'expert \nConsider replacing 'promotional' with 'advanced \nConsider replacing 'qualification' with 'making' or 'fitness'\nConsider replacing 'qualifying' with 'matching' or 'meeting'\nConsider replacing 'questionnaire' with 'form \nConsider replacing 'requirement' with 'duty' or 'thing'\nConsider replacing 'submitted' with 'turned in' or 'given'\nConsider replacing 'sufficient' with 'enough' \n```","04950083":"### Use gender-neutral terms","b92fbf7d":"## Technical details\n\nGender-bias is an app that uses the Natural Language Toolkit (NLTK) to find gender-biased words.  I customized it to read and evaluate the City of Los Angeles Job Bulletins.  Words were checked against a not-biased list.  This list was based on my estimates.  It can and should be customized by Human Resources.  Six hiring bias detectors were used but more can be added:\n\n- native_speakers - detects words that could be simpler, for example, \"exam\" rather than \"examination\"\n\n- gender_terms - detects unnecessary use of gendered pronouns such as \"she\" or \"he\"\n\t\n- effort_accomplishment - detects subtle focus towards men or women.  Studies show that women are more associated with \"effort\" and men with \"accomplishment\" \n\t\n- personal_life - detects subtle bias towards women because women are more likely to speak about personal life\n\t\n- male_terms - detects words that attract men, such as, \"competitive\"\n\t\n- female_terms - detects words that attract women, such as, \"interpersonal\"\n\n### gender-bias app finds the location of biased words\n\nGender-bias outputs text files.  Take a look at a snippet from ```9206.txt```, based on the review of ```311 DIRECTOR  9206 041814.txt```.  The numbers in brackets show the location in the file where the word was found.\n\n```\nConsider replacing ' information ' with ' data '  or 'info'\nConsider replacing ' professional ' with ' expert ' \nConsider replacing ' principles ' with ' rules '  or 'guidelines'\nConsider replacing ' opportunity ' with ' chance ' \nConsider replacing ' requirement ' with ' duty '  or 'thing'\n\nUnnecessary use of gender terms\n SUMMARY: None\n\nTerms biased towards native speakers:\n [216-227]: information\n [497-509]: professional\n [662-672]: principles\n [757-768]: opportunity\n [813-825]: requirements\n SUMMARY: To encourage non-native speakers, use short words and simple sentences\n\nTerms focusing on effort vs accomplishment\n [367-376]: efficient\n SUMMARY: None\n\nTerms about personal life\n SUMMARY: None\n\nTerms biased towards men:\n [893-900]: analyst\n [2876-2887]: competitive\n SUMMARY: Depending on context, these words may be biased towards recruiting men\n\nTerms biased towards women\n [131-142]: responsible\n SUMMARY: Depending on context, these words may be biased towards recruiting women\n\n```","878f5849":"```\nThese words tend to be biased towards women:\n\nWord       Number of times used\n\nresponsibilities     641\nresponses            178\nunderstanding        173\ncommittee            146\nresponsible           97\nconnection            85\nresponse              80\nresponsibility        47\nconnections           23\nagreements            23\nresponsiveness        16\ncommittees            15\nconscientiousness     14\nfamily                13\nshares                12\nsupporting            10\nresponding             8\nchildren               8\nagreement              7\n```\n","c7110d6b":"# Finding Bias in Job Bulletins\n\n\n<b>98% of the words used in the City of Los Angeles Job Bulletins are FREE from gender-bias and non-native speakers bias.<\/b>  \n\nJob Bulletins are consistent and easy to read.  Only 12% percent of the time it appears that biased words are used.  The most common biased word was \"competitive\" which occurs 1293 times.  \n\n### Gender bias research\n\nGender-bias means using words to target male or female job applicants.  Gaucher, Friesen, and Kay researched gender bias in job advertisements and made a list of gender-coded words.  They published this list in \"Evidence That Gendered Wording in Job Advertisements Exists and Sustains Gender Inequality\" in the Journal of Personality and Social Psychology, July 2011, Vol 101(1), p109-28.  The gender-bias app finds these gender-coded words.  I customized this app to read the Los Angeles Job Bulletins and check for gender-bias.  gender-bias has three bias detectors.  I added two more.  \n\n### Easier to read \n\n\"Non-native speakers\" bias detector check for complex words that might discourage people who speak English as a second language.  I based this bias detector on my experience staffing project teams in 27 different countries where I was not a native speaker.  To find complex words, I modified a text analyzer from Allen Downey's book, Think Python, 2nd Edition. It read the Job Bulletins and reported a list of the most commonly used words.  It also evaluated the 50 most complex words over 10 characters long.  This script is explained in the [WordCounts](https:\/\/www.kaggle.com\/nelsondata\/wordcounts\/edit\/run\/15428692) kernel.  WordCounts is a plug-in modules.  \n\n### Plugging-in other tools and techniques\n\nKnowing the Kaggle community, I realize the City of Los Angeles will receive many valuable submissions.  My prototype is modular so you can plug-in these tools or mix and match to meet your needs.\n\n### Technology\n\nNOTICE:  Please do not confuse any files in this kernel with the Job Bulletin CSV file requested for this competition.  The Job Bulletin CSV file is in the [Text2CSV](https:\/\/www.kaggle.com\/nelsondata\/text2csv) kernel.\n\n```\nUbuntu 18.04 LTS\nPython\nPandas\nMatplotlib\nGraphViz\nBigHugeThesaurus API\nWordCloud\nregex\nNLTK\nGender-bias app\n```","e0fedab9":"### Verifying biased words\n\n```biasVerifier.py``` verifies each word.  It reports the sentence where each word is found in a log and a CSV file.\n\nBiasWords.csv has these fields:\n```\nFILE_NAME\nJOB_CLASS\nWORD_LOCATION\nWORD\nBIAS\nSENTENCE\n```\n\nBiasStudyResults.txt uses this format:\n```\nPHRASE FROM JOB BULLETIN MAY HAVE MULTIPLE LINES:\n  >>> competitive <<<  list.  However, if open competitive candidates receive a higher score, without military  ...\nVERIFIED that 'competitive' can be found at offset: 9254 for Job Class: 3734 in Job Bulletin: JobBulletins\/'EQUIPMENT SPECIALIST 3734 111717 revised 11.21.txt'\n```\n\n#### REMEMBER:  JobBulletin.csv required for the competition is explained in [Text2CSV](https:\/\/www.kaggle.com\/nelsondata\/text2csv)\n\n","323f8873":"### 108 Words that may be biased\n\n ['determined' 'responsibilities' 'understanding' 'determines' 'analyst'\n 'analysis' 'competitive' 'competencies' 'analytical' 'connection'\n 'shares' 'leading' 'responsible' 'responses' 'analyzing' 'guy'\n 'understandably' 'leadership' 'responsiveness' 'connecting' 'supporting'\n 'cooperation' 'independently' 'analyze' 'understandable'\n 'interdependence' 'competency' 'response' 'determinations' 'connections'\n 'her' 'logical' 'decisiveness' 'thoroughness' 'analytic'\n 'conscientiousness' 'confidentiality' 'confidential' 'analyses'\n 'analyzes' 'supportive' 'responsibility' 'connected' 'determining'\n 'sharpening' 'competent' 'committee' 'women' 'family' 'carefully'\n 'connectors' 'determination' 'shared' 'cooperative' 'fighting'\n 'responding' 'logically' 'committees' 'responsibilites' 'assertive'\n 'assertiveness' 'Family' 'decision-making' 'athletic' 'children'\n 'agreements' 'sensitivity' 'competing' 'analytics' 'decisively'\n 'sharepoint' 'leader' 'analyzed' 'sharing' 'child' 'analyzers'\n 'agreement' 'competitve' 'cooperatively' 'respondents' 'commitment'\n 'responds' 'analyzer' 'challenging' 'collaborate' 'depending' 'warming'\n 'individualized' 'leaders' 'analysts' 'sensitively' 'determinate'\n 'childcare' 'childhood' 'challenges' 'yielding' 'thoroughly' 'forced-air'\n 'competitors' 'hierarchies' 'Child' 'diligently' 'hierarchical'\n 'competence' 'cooperates' 'competition' 'independence' 'commitments']","1b6d6347":"### Pre-processed files\n\nTo save time, I pre-processed one gender-bias text file for each Job Bulletin.  I stored them in the ```\/JBR_BiasWords``` folder.  If you want to pre-process files, follow this manual installation process.  First, install gender-bias by following these instructions: \n\n```\ngit clone https:\/\/github.com\/gender-bias\/gender-bias\ncd gender-bias\npip3 install -e .\n```\n\nThen, go to https:\/\/github.com\/NelsonPython\/TextAnalyzer and copy these folders into the gender-bias folder.  You will replace the original bias detectors because I fixed a bug.  And you will add the new bias detectors.\n\n```\n\/effort  (replace)\n\/genderedwords (replace)\n\/personal_life (replace)\n\/malewords    (add)\n\/femalewords    (add)\n\/nonnativewords    (add)\n```\n\nCreate a folder called JBR_BiasText in your home folder.  Then, run gender-bais using this command.\n\n```\ncat \/home\/YOUR_HOME_FOLDER\/gender-bias\/YOUR_JOB_BULLETINS_FOLDER\/'311 DIRECTOR  9206 041814.txt' | genderbias > JBR_BiasText\/3119.txt\n```"}}