{"cell_type":{"9a89da8a":"code","46ec95f0":"code","3b698f3f":"code","6b7d0824":"code","ee431100":"code","ad44033c":"code","1856ba1c":"code","1656caee":"code","9556a0b2":"code","a71ae19b":"code","78cd3083":"code","6eae970f":"code","7fbf2d09":"code","db2004a7":"code","ed346ac8":"code","9cc2ef24":"code","8ead19ae":"code","9e3a10f8":"code","54d4d08d":"code","ac85e558":"code","5bf33cd6":"code","da80c973":"code","945ce0eb":"code","28104a7a":"code","14691dc8":"code","72d2c008":"code","e682a031":"code","210f592a":"code","b13434f0":"code","4c801cb0":"code","9815e5d3":"code","79009082":"code","8a91804b":"markdown","7b7d428b":"markdown","4a5b9b96":"markdown","540d60b6":"markdown","2d997bd3":"markdown","81399be4":"markdown","4674cbc3":"markdown","067a3c84":"markdown","c4048fab":"markdown","fb8cfdb2":"markdown","94472f46":"markdown","1fdc9b6b":"markdown","86600062":"markdown","37b920ec":"markdown","c74f3642":"markdown","2119fe2f":"markdown","1c41f67d":"markdown"},"source":{"9a89da8a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\ndf = pd.read_csv('\/kaggle\/input\/unsw-nb15\/UNSW_NB15_training-set.csv')","46ec95f0":"print(f'Column number: { df.columns.size }')","3b698f3f":"print('Column names')\nprint(df.columns)","6b7d0824":"print('Column types')\nprint(df.dtypes)","ee431100":"print(f'Size of the dataset : { len(df) }')","ad44033c":"df.head()","1856ba1c":"print(df.dtypes[df.dtypes == 'object'].index)\n","1656caee":"for category in df.dtypes[ df.dtypes == 'object' ].index:\n    print(category)\n    print(list(set(df[category])))","9556a0b2":"print('Numeric columns :')\nnewdf = df._get_numeric_data()\nprint(newdf.columns)","a71ae19b":"print(\"Extract min, max, mean, median and standard deviation values for 'rate'\")\nfor f in [ 'min', 'max', 'mean', 'median', 'std']:\n    print(f'{ f } = { getattr(df.rate, f)() }')","78cd3083":"print('Extract min, max, mean, median and standard deviation values for all numeric colums')\nfunction_list = [ 'min', 'max', 'mean', 'median', 'std']\nstats = pd.DataFrame(columns=[ 'name' ] + function_list)\nfor c in newdf:\n    line = { 'name': c }\n    for f in function_list:\n        line[f] = newdf[c].aggregate(f)\n    stats = stats.append(line, ignore_index = True)\nstats","6eae970f":"label_normal = df.loc[df.attack_cat == 'Normal'].label.unique()\nprint(f'There is { len(label_normal) } label where attack_cat == Normal, label = { label_normal }')\n\nlabel_attack = df.loc[df.attack_cat != 'Normal'].label.unique()\nprint(f'There is { len(label_attack) } label where attack_cat != Normal, label = { label_attack }')\n","7fbf2d09":"print('Number of occurrences for each attack category :')\ndf.groupby(\"attack_cat\").count()[\"id\"]","db2004a7":"print('protocols appearing in negatively labelled entries :')\nprint(df.loc[df.label == 0].groupby('proto').count()['id'].sort_values(ascending=False).index.tolist())","ed346ac8":"print('protocols appearing in positively labelled entries:')\nprint(df.loc[df.label == 1].groupby('proto').count()['id'].sort_values(ascending=False).index.tolist())","9cc2ef24":"print('services appearing in negatively labelled entries :')\nprint(df.loc[df.label == 0].groupby('service').count()['id'].sort_values(ascending=False).index.tolist())","8ead19ae":"print('services appearing in positively labelled entries:')\nprint(df.loc[df.label == 1].groupby('service').count()['id'].sort_values(ascending=False).index.tolist())","9e3a10f8":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig = plt.gcf()\nfig.set_size_inches(8, 5)\ncplot = sns.countplot(y='service', data=df)\ncplot.set_title('Repartition of services')\nplt.show()","54d4d08d":"plt.figure(figsize=(30, 15))\nbarplot = sns.countplot(y='proto', data=df)\nbarplot.set_title('Repartition of protocols')\nplt.show()","ac85e558":"table = df[['proto', 'id']].pivot_table(index=['proto'], aggfunc='count').sort_values(['id'],ascending=False,inplace=False).head(10)\ntable.plot(kind='bar', title='Repartition of top 10 protocols', legend=False)","5bf33cd6":"df.groupby('proto').count().describe()","da80c973":"barplot = sns.countplot(y='attack_cat', data=df)\nbarplot.set_title('Repartition of attack types')","945ce0eb":"plt.figure(figsize=(30,15))\nheatmap = sns.heatmap(df.corr(), vmin=-1, vmax=1, annot=True, cmap='BrBG')\nheatmap.set_title('Correlation heatmap', fontdict={'fontsize': 12}, pad=10)\nplt.show()","28104a7a":"print('Correlation matrix for labelled entries')\nplt.figure(figsize=(30,15))\nheatmap = sns.heatmap(df[df.label == 1].corr(), vmin=-1, vmax=1, annot=True, cmap='BrBG')\nheatmap.set_title('Correlation heatmap', fontdict={'fontsize': 12}, pad=10)\nplt.show()","14691dc8":"print('Correlation matrix for unlabelled entries')\nplt.figure(figsize=(30,15))\nheatmap = sns.heatmap(df[df.label == 0].corr(), vmin=-1, vmax=1, annot=True, cmap='BrBG')\nheatmap.set_title('Correlation heatmap', fontdict={'fontsize': 12}, pad=10)\nplt.show()","72d2c008":"print('Correlation matrix for labelled - unlabelled entries')\nplt.figure(figsize=(30,15))\nheatmap = sns.heatmap(df[df.label == 1].corr() - df[df.label == 0].corr(), vmin=-1, vmax=1, annot=True, cmap='BrBG')\nheatmap.set_title('Correlation heatmap', fontdict={'fontsize': 12}, pad=10)\nplt.show()","e682a031":"print('Top 20 differences between matrix of labelled correlation - matric of unlabelled correlation')\nx = (df[df.label == 1].corr() - df[df.label == 0].corr()).stack().sort_values()\nt = pd.DataFrame(columns=['a', 'b', 'diff', 'abs'])\nfor a, b in x.index:\n    if (a != b):\n        t = t.append({ 'a': a, 'b': b, 'diff': x[(a, b)], 'abs': abs(x[(a,b)]) }, ignore_index=True)\nprint(t.sort_values('abs', ascending=False)[['a', 'b', 'diff']].head(20))\n","210f592a":"print('correlation of sttl and dttl, for labelled entries')\nprint(df.loc[df.label == 1][['sttl', 'dttl']].corr())\nprint('correlation of sttl and dttl, for unlabelled entries')\nprint(df.loc[df.label == 0][['sttl', 'dttl']].corr())","b13434f0":"print('min, max, mean, median and standard deviation values for rate, sttl and dttl for unlabelled entries')\nfunction_list = [ 'min', 'max', 'mean', 'median', 'std']\nstats = pd.DataFrame(columns=[ 'name' ] + function_list)\nfor c in 'rate', 'sttl','dttl':\n    line = { 'name': c }\n    for f in function_list:\n        line[f] = df.loc[df.label == 0][c].aggregate(f)\n    stats = stats.append(line, ignore_index = True)\nstats","4c801cb0":"print('min, max, mean, median and standard deviation values for sttl and dttl for labelled entries')\nstats = pd.DataFrame(columns=[ 'name' ] + function_list)\nfor c in 'rate', 'sttl','dttl':\n    line = { 'name': c }\n    for f in function_list:\n        line[f] = df.loc[df.label == 1][c].aggregate(f)\n    stats = stats.append(line, ignore_index = True)\nstats\n","9815e5d3":"df.loc[df.label == 0][['sttl','dttl']].plot.hist(bins=256, alpha=0.5, title='unlabelled entries')\n","79009082":"df.loc[df.label == 1][['sttl','dttl']].plot.hist(bins=256, alpha=0.5, title='labelled entries')","8a91804b":"### 8.2. Identify the target properties you will want to analyse\nI will want to analyse the 'label' and 'attack_cat' properties versus the others properties.","7b7d428b":"### 4.2. Column names","4a5b9b96":"### 4.3. Column types","540d60b6":"Auditeur : Didier GORGES\n\nkaggle notebook : https:\/\/www.kaggle.com\/dgcnam\/sec201-lab-session-predicting-attacks\/edit\/run\/61813075\n\n# Loading data\n\n## 1. Open a new Jupyter notebook and name it \u2018SEC201 - Lab session - predicting attacks\u2019\n\nDone\n\n## 2. In File > Add or Include Data, search for \u201cUNSW_NB15\u201d dataset and include it\n\nDone\n\n## 3. In \u2018Data > input > unsw-nb15\u2018, get the exact path of CSV file 'UNSW_NB15_training-set.csv' and load it as training_set using Pandas.","2d997bd3":"## 10. Which is the number of occurrences for each attack category?","81399be4":"### 4.4. Size of the data set","4674cbc3":"## 12. What do you conclude about the traffic being analysed?\n\nIn this data set, the 'label' and 'attack_cat' properties are coherents. If 'label' is 1, the 'attack_cat' is not 'Normal'. If 'label' is 0, the attack_cat is 'Normal'.\n\nThere are big differences between mean and median on some properties ('rate', 'sload', 'dload', 'sjiy', ...) which means we expect to see some outliers due to attacks.\n\nAttackers' traffic uses more various protocols and services than the legitimate one. Protocols and services that do not appear in legitimate traffic are suspicious.","067a3c84":"## 4. List following information for the training set\n\n\n### 4.1. Column number","c4048fab":"## 5. Look at the file head using: df.head()","fb8cfdb2":"## 7. Which columns are numeric? List them; extract min, max, mean, median and standard deviation values for \u2018rate\u2019.","94472f46":"## 9. Check whether the positive label (1) match attack categories and whether attack categories match labelled data.\n\nThe following code shows that the positive label matchs the Normal attack category, and that the negative label matchs all the other attack categories.","1fdc9b6b":"## 11. Which protocols and services appear in the positively labelled entries? In the negatively labelled ones?","86600062":"## 15. Based on the Exploratory Data Analysis\n### 15.1. Describe what you learnt from the dataset\n\nI learnt that the label field is a flag that indicates if the entry is considered as an attack.\n\nThe kind of attack is written in the attack_cat field. The label values of the entries are coherents with their attack_cat values.\n\nThe protocols and services use in an attack are more various than for a legitimate traffic.\n\nCorrelation matrix show that, for labelled entries, sttl and dttl fields does not have the same distributions as for unlabelled entries. There is correlation differences for some other fields too.\n\nrate median is 118 for unlabelled entries and 100000 for labelled entries.\n\ndttl median is 29 for unlabelled entries and 0 for labelled entries.\n\nsttl median is 62 for unlabelled entries en 254 for labelled entries.\n\n### 15.2. Draw the first conclusions\n\nThe entries contain normal and attack traffic, and seem correctly classified without identified bias. When aggregated according the label field, data have a different metrics profile.\n\n### 15.3. Emit recommendations for enforcing the cybersecurity of the target system\n\nWe can use this data set to train machine learning classifiers, like XGBoost, in order to estimate a probability of attack on new entries.","37b920ec":"## 14. Build the correlation matrix between parameters for labelled and unlabelled entries.","c74f3642":"## 6. Which columns are categories? List them; extract existing values.","2119fe2f":"# Data visualisation\n\n## 13. Visualise the repartition of services, protocols, attack types, as histograms Use pyplot and seaborn libraries.","1c41f67d":"## 8. Based on this information\n### 8.1. Define the goal of the analysis.\n\nThe goal of the analysis is to check if the data are correctly labelled to reveal an attack.\n"}}