{"cell_type":{"fcbe18d1":"code","b778769f":"code","26b435e9":"code","9f8b1e16":"code","2fc6b63b":"code","7a339804":"code","0240db7b":"code","9ee097b3":"code","f6a8af2b":"code","4bda44b2":"code","6173d8b0":"code","8d041f88":"code","7f56df09":"code","572c12ae":"code","778c8733":"code","17ce305d":"code","9ea62804":"code","3139323d":"code","539d7fbd":"code","8c946a74":"code","d38af42e":"code","83b50ec0":"code","8a335355":"code","1211ff61":"code","9cac7331":"code","cada4432":"code","26c02e7a":"code","9e13a094":"code","54732b6e":"code","367f6dc5":"code","efa77fea":"code","211d16fe":"code","ca473818":"code","c2bdc7b6":"code","ca1034ea":"code","0b80474d":"code","66db74a6":"code","60f84af6":"code","c47d6b73":"code","39ece675":"markdown","d1b275b2":"markdown","21334517":"markdown","bb95094e":"markdown"},"source":{"fcbe18d1":"import transformers\nfrom transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\n\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom collections import defaultdict\nfrom textwrap import wrap\n\nfrom tqdm import tqdm\n\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n\nsns.set(style='whitegrid', palette='muted', font_scale=1.2)\nHAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n\nsns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\nrcParams['figure.figsize'] = 12, 8\n\nRANDOM_SEED = 23\nnp.random.seed(RANDOM_SEED)\n\ntorch.manual_seed(RANDOM_SEED)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","b778769f":"!pip freeze > requirements.txt","26b435e9":"data = pd.read_json('..\/input\/news-category-dataset\/News_Category_Dataset_v2.json', lines=True)\ndata.head()\n\ndata = data.sample(n=1000)","9f8b1e16":"data.category.unique()","2fc6b63b":"text = pd.DataFrame({\n    'text':data.headline + data.short_description,\n    'label':data.category\n})","7a339804":"text.head()","0240db7b":"encoder = LabelEncoder()\ntext.label = encoder.fit_transform(text.label)\n\ntext.head()","9ee097b3":"train, test = train_test_split(text, test_size=0.1, random_state=23)\ntrain, val = train_test_split(train, test_size=0.3,random_state=23)","f6a8af2b":"# train.reset_index(drop=True, inplace=True)\n# val.reset_index(drop=True, inplace=True)\n# test.reset_index(drop=True, inplace=True)","4bda44b2":"# test.drop('label', 1, inplace=True)","6173d8b0":"PRE_TRAINED_MODEL_NAME = 'bert-base-cased'","8d041f88":"tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)","7f56df09":"sample_txt = 'So give me reason to prove me wrong to wash this memory clean. Let the flood the cross the distance in your eyes.'\n\ntokens = tokenizer.tokenize(sample_txt)\n\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\n\nprint(f' Sentence: {sample_txt}')\nprint(f'   Tokens: {tokens}')\nprint(f'Token IDs: {token_ids}')","572c12ae":"tokenizer.special_tokens_map","778c8733":"encoding = tokenizer.encode_plus(\n  sample_txt,\n  max_length=32,\n  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n  return_token_type_ids=False,\n  pad_to_max_length=True,\n  return_attention_mask=True,\n  return_tensors='pt',  # Return PyTorch tensors\n)\n\n\nencoding.keys()\n\n","17ce305d":"tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])","9ea62804":"class Tokenisation(Dataset):\n    \n    \n    def __init__(self, data, targets, tokenizer, max_len):\n        self.data = data\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.pad_vid = self.tokenizer.vocab[\"[PAD]\"]\n        \n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, item):\n        data = str(self.data[item])\n        data = \" \".join(data.split())\n        target = self.targets\n        \n        encoding = self.tokenizer.encode_plus(\n          data,\n          add_special_tokens=True,\n          max_length=self.max_len,\n          return_token_type_ids=False,\n          pad_to_max_length=True,\n          return_attention_mask=True,\n          return_tensors='pt',\n        )\n        \n        ids = encoding['input_ids']\n        masks = encoding['attention_mask']\n        token_type_ids = encoding['input_ids']\n        \n        true_seq_length = len(encoding['input_ids'][0])\n        pad_size = self.max_len - true_seq_length\n        pad_ids = torch.Tensor([self.pad_vid] * pad_size).long()\n        ids = torch.cat((encoding['input_ids'][0], pad_ids))\n        \n        \n#         padding_len = self.max_len - len(ids)\n#         ids = ids + ([0] * padding_len)\n#         masks = ids + ([0] * padding_len)\n#         token_type_ids = token_type_ids + ([0] * padding_len)\n        \n        return {\n          'text': data,\n          'input_ids': ids.flatten(),\n          'attention_mask': encoding['attention_mask'].flatten(),\n          'targets': torch.tensor(target[item], dtype=torch.long)\n        }","3139323d":"def dataLoader(df, tokenizer, max_len, batch_size):\n    ds = Tokenisation(\n    data=df['text'].to_numpy(),\n    targets=df['label'].to_numpy(),\n    tokenizer=tokenizer,\n    max_len=max_len\n    )\n    return DataLoader(ds,batch_size=batch_size, num_workers=4)","539d7fbd":"BATCH_SIZE = 16\nMAX_LEN = 128\n\ntrain_data_loader = dataLoader(train, tokenizer, MAX_LEN, BATCH_SIZE)\nval_data_loader = dataLoader(val, tokenizer, MAX_LEN, BATCH_SIZE)\ntest_data_loader = dataLoader(test, tokenizer, MAX_LEN, BATCH_SIZE)","8c946a74":"data = next(iter(train_data_loader))\ndata.keys()","d38af42e":"print(data['input_ids'].shape)\nprint(data['attention_mask'].shape)\nprint(data['targets'].shape)","83b50ec0":"class TextClassifier(nn.Module):\n    \n    def __init__(self, n_classes):\n        super(TextClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n        self.drop = nn.Dropout(p=0.3)\n        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n            \n    \n    def forward(self, input_ids, attention_mask):\n        _, pooled_output = self.bert(\n          input_ids=input_ids,\n          attention_mask=attention_mask\n        )\n        output = self.drop(pooled_output)\n        return self.out(output)\n    \n    def unfreeze(self,start_layer,end_layer):\n        def children(m):\n            return m if isinstance(m, (list, tuple)) else list(m.children())\n        def set_trainable_attr(m, b):\n            m.trainable = b\n            for p in m.parameters():\n                p.requires_grad = b\n        def apply_leaf(m, f):\n            c = children(m)\n            if isinstance(m, nn.Module):\n                f(m)\n            if len(c) > 0:\n                for l in c:\n                    apply_leaf(l, f)\n        def set_trainable(l, b):\n            apply_leaf(l, lambda m: set_trainable_attr(m, b))\n\n        # You can unfreeze the last layer of bert by calling set_trainable(model.bert.encoder.layer[23], True)\n        set_trainable(self.bert, False)\n        for i in range(start_layer, end_layer+1):\n            set_trainable(self.bert.encoder.layer[i], True)","8a335355":"device","1211ff61":"len(text.label.unique())","9cac7331":"model = TextClassifier(len(text.label.unique()))\nmodel = model.to(device)","cada4432":"input_ids = data['input_ids'].to(device)\nattention_mask = data['attention_mask'].to(device)\n\nprint(input_ids.shape) # batch size x seq length\nprint(attention_mask.shape) # batch size x seq length","26c02e7a":"F.softmax(model(input_ids, attention_mask), dim=1)","9e13a094":"EPOCHS = 3\nMAX_LENGTH = 128\nTRAIN_BATCH_SIZE = 8\nVALID_BATCH_SIZE = 4\n\nLearningRate = 5e-5\n\nBETAS = (0.9, 0.999)\nBERT_WEIGHT_DECAY = 0.01\nEPS = 1e-8\n\n# Define identifiers & group model parameters accordingly \nbert_identifiers = ['embedding', 'encoder', 'pooler']\nno_weight_decay_identifiers = ['bias', 'LayerNorm.weight']\ngrouped_model_parameters = [\n        {'params': [param for name, param in model.named_parameters()\n                    if any(identifier in name for identifier in bert_identifiers) and\n                    not any(identifier_ in name for identifier_ in no_weight_decay_identifiers)],\n        'lr': LearningRate,\n        'betas': BETAS,\n        'weight_decay': BERT_WEIGHT_DECAY,\n        'eps': EPS},\n        {'params': [param for name, param in model.named_parameters()\n                    if any(identifier in name for identifier in bert_identifiers) and\n                    any(identifier_ in name for identifier_ in no_weight_decay_identifiers)],\n        'lr': LearningRate,\n        'betas': BETAS,\n        'weight_decay': 0.0,\n        'eps': EPS},\n        {'params': [param for name, param in model.named_parameters()\n                    if not any(identifier in name for identifier in bert_identifiers)],\n        'lr': LearningRate,\n        'betas': BETAS,\n        'weight_decay': 0.0,\n        'eps': EPS}\n]\n\n# Define optimizer\noptimizers = AdamW(grouped_model_parameters)\n\noptimizer = AdamW(model.parameters(), lr=5e-5, correct_bias=False)\n\ntotal_steps = len(train_data_loader) * EPOCHS\n\nscheduler = get_linear_schedule_with_warmup(\n  optimizer,\n  num_warmup_steps=0,\n  num_training_steps=total_steps\n)\n\nloss_fn = nn.CrossEntropyLoss().to(device)","54732b6e":"def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n    \n    \n    model = model.train()\n    losses = []\n    correct_predictions = 0\n    for d in tqdm(data_loader):\n        input_ids = d[\"input_ids\"].to(device)\n        attention_mask = d[\"attention_mask\"].to(device)\n        targets = d[\"targets\"].to(device)\n        outputs = model(\n          input_ids=input_ids,\n          attention_mask=attention_mask\n        )\n        _, preds = torch.max(outputs, dim=1)\n        loss = loss_fn(outputs, targets)\n        correct_predictions += torch.sum(preds == targets)\n        losses.append(loss.item())\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n        \n    return correct_predictions.double() \/ n_examples, np.mean(losses)","367f6dc5":"def eval_model(model, data_loader, loss_fn, device, n_examples):\n    \n    model = model.eval()\n    \n    losses = []\n    \n    correct_predictions = 0\n    \n    with torch.no_grad():\n        for d in tqdm(data_loader):\n            input_ids = d[\"input_ids\"].to(device)\n            attention_mask = d[\"attention_mask\"].to(device)\n            targets = d[\"targets\"].to(device)\n            \n            outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n            )\n            \n            _, preds = torch.max(outputs, dim=1)\n            loss = loss_fn(outputs, targets)\n            correct_predictions += torch.sum(preds == targets)\n            losses.append(loss.item())\n            \n    return correct_predictions.double() \/ n_examples, np.mean(losses)","efa77fea":"%%time\n\nhistory = defaultdict(list)\nbest_accuracy = 0\n\nfor epoch in range(EPOCHS):\n    print(f'Epoch {epoch + 1}\/{EPOCHS}')\n    print('-' * 10)\n    train_acc, train_loss = train_epoch(\n                                        model,\n                                        train_data_loader,\n                                        loss_fn,\n                                        optimizer,\n                                        device,\n                                        scheduler,\n                                        len(train)\n    )\n    print(f'Train loss {train_loss} accuracy {train_acc}')\n    val_acc, val_loss = eval_model(\n                                    model,\n                                    val_data_loader,\n                                    loss_fn,\n                                    device,\n                                    len(val)\n  )\n    print(f'Val   loss {val_loss} accuracy {val_acc}')\n    print()\n    history['train_acc'].append(train_acc)\n    history['train_loss'].append(train_loss)\n    history['val_acc'].append(val_acc)\n    history['val_loss'].append(val_loss)\n    \n    if val_acc > best_accuracy:\n        torch.save(model.state_dict(), 'best_model_state.bin')\n        best_accuracy = val_acc","211d16fe":"plt.plot(history['train_acc'], label='train accuracy')\nplt.plot(history['val_acc'], label='validation accuracy')\nplt.title('Training history')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()","ca473818":"test","c2bdc7b6":"test_acc, _ = eval_model(\n  model,\n  test_data_loader,\n  loss_fn,\n  device,\n  len(df_test)\n)\n\n\ntest_acc.item()","ca1034ea":"def get_predictions(model, data_loader):\n    model = model.eval()\n    review_texts = []\n    predictions = []\n    prediction_probs = []\n    real_values = []\n    with torch.no_grad():\n    for d in data_loader:\n        texts = d[\"review_text\"]\n        input_ids = d[\"input_ids\"].to(device)\n        attention_mask = d[\"attention_mask\"].to(device)\n        targets = d[\"targets\"].to(device)\n        outputs = model(\n        input_ids=input_ids,\n        attention_mask=attention_mask\n        )\n        _, preds = torch.max(outputs, dim=1)\n        review_texts.extend(texts)\n        predictions.extend(preds)\n        prediction_probs.extend(outputs)\n        real_values.extend(targets)\n    predictions = torch.stack(predictions).cpu()\n    prediction_probs = torch.stack(prediction_probs).cpu()\n    real_values = torch.stack(real_values).cpu()\n    return review_texts, predictions, prediction_probs, real_values","0b80474d":"y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n  model,\n  test_data_loader\n)","66db74a6":"print(classification_report(y_test, y_pred, target_names=class_names))","60f84af6":"def show_confusion_matrix(confusion_matrix):\n    hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n    hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n    hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n    plt.ylabel('True sentiment')\n    plt.xlabel('Predicted sentiment');","c47d6b73":"cm = confusion_matrix(y_test, y_pred)\ndf_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\nshow_confusion_matrix(df_cm)","39ece675":"Bert uses special tokens to navigate through the training process. The tokens include:\n1. Unknown\n2. Seperate\n3. Padding\n4. Class\n5. Mask","d1b275b2":"### Sample Test\n\nThe idea is to preserve the purity of the sentence and not to convert it into all lower cases and vise-versa.","21334517":"## Tokenisation or data-preprocessing","bb95094e":"## Building the model"}}