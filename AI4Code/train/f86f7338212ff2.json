{"cell_type":{"bed29d8d":"code","34212833":"code","50332a3e":"code","9f88ed0a":"code","32e3c8bf":"code","7745b317":"code","33d59d4f":"code","e8c9dffb":"code","9fddec7d":"code","1f15e1da":"code","1c675157":"code","0c8c21e8":"code","bae435e5":"code","f74f531d":"code","1343c170":"code","0ce67d93":"code","fc1a8b0a":"code","c68f9e59":"code","33c7e316":"code","45c0e0ab":"code","1f23e809":"code","c18223ff":"code","b8a8c683":"code","dc2ed72b":"code","38b2c202":"code","ada0fe3a":"code","91203304":"code","369f1d90":"code","972d85e5":"code","d6b7d1b3":"code","166a9dcb":"code","dc4a5960":"code","ee320ecb":"code","5d307f50":"code","54126bc0":"code","a1948523":"code","12f9a36a":"code","88902aaf":"code","fa6923dd":"code","3ef13866":"code","61febde4":"code","38b6a15c":"markdown","87c415b6":"markdown","63aa8fe5":"markdown","d69a72a3":"markdown","70b7c451":"markdown","693146a8":"markdown","d8ac92e5":"markdown","71853039":"markdown","ece91252":"markdown","c6f84be2":"markdown","641d295a":"markdown","384baddd":"markdown","aed1debf":"markdown"},"source":{"bed29d8d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport networkx as nx\nfrom wordcloud import WordCloud","34212833":"df_2021=pd.read_csv(r'..\/input\/kaggle-survey-2021\/kaggle_survey_2021_responses.csv',encoding='ISO-8859-1')","50332a3e":"df_2021.head(5)","9f88ed0a":"df_lang = df_2021.iloc[:,7:18]\ndf_vis = df_2021.iloc[:,59:69]\ndf_algo = df_2021.iloc[:,90:100]\ndf_frame = df_2021.iloc[:,72:88]","32e3c8bf":"df_lang.head(5)","7745b317":"df_lang.columns=['Python','R','SQL','C','C++','Java','Javascript','Julia','Swift','Bash','MATLAB']","33d59d4f":"col1 = df_lang.columns\ncol2 = [df_lang['Python'].value_counts()[0],\ndf_lang['R'].value_counts()[0],\ndf_lang['SQL'].value_counts()[0],\ndf_lang['C'].value_counts()[0],\ndf_lang['C++'].value_counts()[0],\ndf_lang['Java'].value_counts()[0],\ndf_lang['Javascript'].value_counts()[0],\ndf_lang['Julia'].value_counts()[0],\ndf_lang['Swift'].value_counts()[0],\ndf_lang['Bash'].value_counts()[0],\ndf_lang['MATLAB'].value_counts()[0]]\nlanguages=pd.DataFrame({'Lang':col1,'Count':col2})\n","e8c9dffb":"c = df_lang.stack().groupby(level=0).apply(tuple).value_counts()\nout = [i + (j,) for i, j in c.items()]\nout=[word for word in out if len(word)==3]\nlang_net=pd.DataFrame(out)\nlang_net.columns=['Lang 1','Lang 2','Count']\ng = nx.from_pandas_edgelist(lang_net,source='Lang 1',target='Lang 2')\ncmap = plt.cm.RdYlGn\ncolors = [n for n in range(len(g.nodes()))]\nk = 0.35\npos=nx.spring_layout(g, k=k)\nnx.draw_networkx(g,node_size=languages['Count'].values, cmap = cmap, node_color=colors, edge_color='grey', font_size=22, width=lang_net['Count'].values*0.01)\nnx.write_gexf(g,'language.gexf')\nplt.title('Languages Network',size=50)\nplt.gcf().set_size_inches(25,20)","9fddec7d":"df_algo.head(5)","1f15e1da":"df_algo.columns=['Linear or Logistic Regression','Decision Trees or Random Forests','Gradient Boosting Machines (xgboost, lightgbm, etc)','Bayesian Approaches','Evolutionary Approaches'\n,'Dense Neural Networks (MLPs, etc)','Convolutional Neural Networks','Generative Adversarial Networks','Recurrent Neural Networks','Transformer Networks (BERT, gpt-3, etc)']","1c675157":"col3 = df_algo.columns\ncol4 = [df_algo['Linear or Logistic Regression'].value_counts()[0],\ndf_algo['Decision Trees or Random Forests'].value_counts()[0],\ndf_algo['Gradient Boosting Machines (xgboost, lightgbm, etc)'].value_counts()[0],\ndf_algo['Bayesian Approaches'].value_counts()[0],\ndf_algo['Evolutionary Approaches'].value_counts()[0],\ndf_algo['Dense Neural Networks (MLPs, etc)'].value_counts()[0],\ndf_algo['Convolutional Neural Networks'].value_counts()[0],\ndf_algo['Generative Adversarial Networks'].value_counts()[0],\ndf_algo['Recurrent Neural Networks'].value_counts()[0],\ndf_algo['Transformer Networks (BERT, gpt-3, etc)'].value_counts()[0]]\nalgorithms = pd.DataFrame({'Algo':col3,'Count':col4})","0c8c21e8":"c = df_algo.stack().groupby(level=0).apply(tuple).value_counts()\nout = [i + (j,) for i, j in c.items()]\nout=[word for word in out if len(word)==3]\nalgo_net=pd.DataFrame(out)\nalgo_net.columns=['Algo 1','Algo 2','Count']\ng = nx.from_pandas_edgelist(algo_net,source='Algo 1',target='Algo 2')\ncmap = plt.cm.RdYlGn\ncolors = [n for n in range(len(g.nodes()))]\nk = 0.35\npos=nx.spring_layout(g, k=k)\nnx.draw_networkx(g,node_size=algorithms['Count'].values, cmap = cmap, node_color=colors, edge_color='grey', font_size=22, width=algo_net['Count'].values*0.01)\nnx.write_gexf(g,'algorithm.gexf')\nplt.title('Algorithm Network',size=50)\nplt.gcf().set_size_inches(25,20)","bae435e5":"df_frame.head(5)","f74f531d":"df_frame.columns=['Scikit-learn','TensorFlow','Keras','PyTorch','Fast.ai','MXNet','Xgboost','LightGBM','CatBoost','Prophet','H2O 3','Caret','Tidymodels','JAX','PyTorch Lightning','Huggingface']","1343c170":"col5 = df_frame.columns\ncol6 = [df_frame['Scikit-learn'].value_counts()[0],\ndf_frame['TensorFlow'].value_counts()[0],\ndf_frame['Keras'].value_counts()[0],\ndf_frame['PyTorch'].value_counts()[0],\ndf_frame['Fast.ai'].value_counts()[0],\ndf_frame['MXNet'].value_counts()[0],\ndf_frame['Xgboost'].value_counts()[0],\ndf_frame['LightGBM'].value_counts()[0],\ndf_frame['CatBoost'].value_counts()[0],\ndf_frame['Prophet'].value_counts()[0],\ndf_frame['H2O 3'].value_counts()[0],\ndf_frame['Caret'].value_counts()[0],\ndf_frame['Tidymodels'].value_counts()[0],\ndf_frame['JAX'].value_counts()[0],\ndf_frame['PyTorch Lightning'].value_counts()[0],\ndf_frame['Huggingface'].value_counts()[0]]\nframework = pd.DataFrame({'Frame':col5,'Count':col6})","0ce67d93":"c = df_frame.stack().groupby(level=0).apply(tuple).value_counts()\nout = [i + (j,) for i, j in c.items()]\nout=[word for word in out if len(word)==3]\nframe_net=pd.DataFrame(out)\nframe_net.columns=['Frame 1','Frame 2','Count']\ng = nx.from_pandas_edgelist(frame_net,source='Frame 1',target='Frame 2')\ncmap = plt.cm.RdYlGn\ncolors = [n for n in range(len(g.nodes()))]\nk = 0.35\npos=nx.spring_layout(g, k=k)\nnx.draw_networkx(g,node_size=framework['Count'].values, cmap = cmap, node_color=colors, edge_color='grey', font_size=22, width=frame_net['Count'].values*0.01)\nnx.write_gexf(g,'framework.gexf')\nplt.title('Framework Network',size=50)\nplt.gcf().set_size_inches(25,20)","fc1a8b0a":"df_vis.head(5)","c68f9e59":"df_vis.columns = ['Matplotlib','Seaborn','Plotly \/ Plotly Express','Ggplot \/ ggplot2','Shiny','D3 js','Altair','Bokeh','Geoplotlib','Leaflet \/ Folium']","33c7e316":"col7 = df_vis.columns\ncol8 = [df_vis['Matplotlib'].value_counts()[0],\ndf_vis['Seaborn'].value_counts()[0],\ndf_vis['Plotly \/ Plotly Express'].value_counts()[0],\ndf_vis['Ggplot \/ ggplot2'].value_counts()[0],\ndf_vis['Shiny'].value_counts()[0],\ndf_vis['D3 js'].value_counts()[0],\ndf_vis['Altair'].value_counts()[0],\ndf_vis['Bokeh'].value_counts()[0],\ndf_vis['Geoplotlib'].value_counts()[0],\ndf_vis['Leaflet \/ Folium'].value_counts()[0]]\nvisualization = pd.DataFrame({'Frame':col7,'Count':col8})","45c0e0ab":"c = df_vis.stack().groupby(level=0).apply(tuple).value_counts()\nout = [i + (j,) for i, j in c.items()]\nout=[word for word in out if len(word)==3]\nvis_net=pd.DataFrame(out)\nvis_net.columns=['Vis 1','Vis 2','Count']\ng = nx.from_pandas_edgelist(vis_net,source='Vis 1',target='Vis 2')\ncmap = plt.cm.RdYlGn\ncolors = [n for n in range(len(g.nodes()))]\nk = 0.35\npos=nx.spring_layout(g, k=k)\nnx.draw_networkx(g,node_size=visualization['Count'].values, cmap = cmap, node_color=colors, edge_color='grey', font_size=22, width=vis_net['Count'].values*0.01)\nnx.write_gexf(g,'visualization.gexf')\nplt.title('Visualization Tools Network',size=50)\nplt.gcf().set_size_inches(25,20)","1f23e809":"df_env = df_2021.iloc[:,21:32]\ndf_share = df_2021.iloc[:,233:241]\ndf_start = df_2021.iloc[:,243:253]","c18223ff":"df_env.columns = ['Jupyter (JupyterLab, Jupyter Notebooks, etc)','RStudio','Visual Studio','Visual Studio Code (VSCode)','PyCharm','Spyder','Notepad++','Sublime Text','Vim \/ Emacs','MATLAB','Jupyter Notebook']","b8a8c683":"col9 = df_env.columns\ncol10 = [df_env['Jupyter (JupyterLab, Jupyter Notebooks, etc)'].value_counts()[0],\ndf_env['RStudio'].value_counts()[0],\ndf_env['Visual Studio'].value_counts()[0],\ndf_env['Visual Studio Code (VSCode)'].value_counts()[0],\ndf_env['PyCharm'].value_counts()[0],\ndf_env['Spyder'].value_counts()[0],\ndf_env['Notepad++'].value_counts()[0],\ndf_env['Sublime Text'].value_counts()[0],\ndf_env['Vim \/ Emacs'].value_counts()[0],\ndf_env['MATLAB'].value_counts()[0],\ndf_env['Jupyter Notebook'].value_counts()[0]]\n\nenvironment = pd.DataFrame({'Env':col9,'Count':col10})","dc2ed72b":"c = df_env.stack().groupby(level=0).apply(tuple).value_counts()\nout = [i + (j,) for i, j in c.items()]\nout=[word for word in out if len(word)==3]\nenv_net=pd.DataFrame(out)\nenv_net.columns=['Env 1','Env 2','Count']\ng_env = nx.from_pandas_edgelist(env_net,source='Env 1',target='Env 2')\ncmap = plt.cm.RdYlGn\ncolors = [n for n in range(len(g_env.nodes()))]\nk = 0.35\npos=nx.spring_layout(g_env, k=k)\nnx.draw_networkx(g_env,node_size=environment['Count'].values, cmap = cmap, node_color=colors, edge_color='grey', font_size=22, width=env_net['Count'].values*0.01)\nnx.write_gexf(g_env,'visualization.gexf')\nplt.title('Develop Environment Network',size=50)\nplt.gcf().set_size_inches(25,20)","38b2c202":"df_share.columns = ['Plotly Dash','Streamlit','NBViewer','GitHub','Personal blog','Kaggle','Colab','Shiny']","ada0fe3a":"col11 = df_share.columns\ncol12 = [df_share['Plotly Dash'].value_counts()[0],\ndf_share['Streamlit'].value_counts()[0],\ndf_share['NBViewer'].value_counts()[0],\ndf_share['GitHub'].value_counts()[0],\ndf_share['Personal blog'].value_counts()[0],\ndf_share['Kaggle'].value_counts()[0],\ndf_share['Colab'].value_counts()[0],\ndf_share['Shiny'].value_counts()[0]]\n\nshare = pd.DataFrame({'Share':col11,'Count':col12})","91203304":"c = df_share.stack().groupby(level=0).apply(tuple).value_counts()\nout = [i + (j,) for i, j in c.items()]\nout=[word for word in out if len(word)==3]\nshare_net=pd.DataFrame(out)\nshare_net.columns=['Share 1','Share 2','Count']\ng_share = nx.from_pandas_edgelist(share_net,source='Share 1',target='Share 2')\ncmap = plt.cm.RdYlGn\ncolors = [n for n in range(len(g_share.nodes()))]\nk = 0.35\npos=nx.spring_layout(g_share, k=k)\nnx.draw_networkx(g_share,node_size=[ 4586, 3065,  1848, 705,  387, 305, 293,  136], cmap = cmap, node_color=colors, edge_color='grey', font_size=22, width=share_net['Count'].values*0.01)\nnx.write_gexf(g_share,'share.gexf')\nplt.title('Sharing Platform Network',size=30)\nplt.gcf().set_size_inches(15,10)","369f1d90":"df_start.columns = ['Coursera','edX','Kaggle Learn Courses','DataCamp','Fast.ai','Udacity','Udemy','LinkedIn Learning','Cloud-certification programs','University Courses']","972d85e5":"col13 = df_start.columns\ncol14 = [df_start['Coursera'].value_counts()[0],\ndf_start['edX'].value_counts()[0],\ndf_start['Kaggle Learn Courses'].value_counts()[0],\ndf_start['DataCamp'].value_counts()[0],\ndf_start['Fast.ai'].value_counts()[0],\ndf_start['Udacity'].value_counts()[0],\ndf_start['Udemy'].value_counts()[0],\ndf_start['LinkedIn Learning'].value_counts()[0],\ndf_start['Cloud-certification programs'].value_counts()[0],\ndf_start['University Courses'].value_counts()[0]]\n\nstart = pd.DataFrame({'Share':col13,'Count':col14})","d6b7d1b3":"c = df_start.stack().groupby(level=0).apply(tuple).value_counts()\nout = [i + (j,) for i, j in c.items()]\nout=[word for word in out if len(word)==3]\nstart_net=pd.DataFrame(out)\nstart_net.columns=['Start 1','Start 2','Count']\ng_start = nx.from_pandas_edgelist(start_net,source='Start 1',target='Start 2')\ncmap = plt.cm.RdYlGn\ncolors = [n for n in range(len(g_start.nodes()))]\nk = 0.35\npos=nx.spring_layout(g_start, k=k)\nnx.draw_networkx(g_start,node_size=start['Count'].values, cmap = cmap, node_color=colors, edge_color='grey', font_size=22, width=start_net['Count'].values*0.01)\nnx.write_gexf(g_start,'start.gexf')\nplt.title('Starting Platform Network',size=50)\nplt.gcf().set_size_inches(25,20)","166a9dcb":"languages.columns = ['Text','Count']\nalgorithms.columns = ['Text','Count']\nframework.columns = ['Text','Count']\nvisualization.columns = ['Text','Count']\nenvironment.columns = ['Text','Count']\nshare.columns = ['Text','Count']\nstart.columns = ['Text','Count']","dc4a5960":"text_dic = pd.concat([languages,algorithms,framework,visualization,environment,share,start])","ee320ecb":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\nwc = WordCloud(background_color='white')\nfp = text_dic\nname = list(fp.Text)  # Word\nvalue = fp.Count  # Frequency\nfor i in range(len(name)):\n    name[i] = str(name[i])\ndic = dict(zip(name, value)) \nwc.generate_from_frequencies(dic)  # generate wordcloud\n\nplt.imshow(wc)\nplt.axis(\"off\")  \nplt.show()\nwc.to_file('wordcloud.png') \n","5d307f50":"df_2021_raw = pd.read_csv(r'..\/input\/kaggle-survey-2021\/kaggle_survey_2021_responses.csv',encoding='ISO-8859-1')\ndf_2021 = df_2021_raw.iloc[1:,:]\ndf_2020_raw = pd.read_csv(r'..\/input\/kaggle-survey-2020\/kaggle_survey_2020_responses.csv',encoding='ISO-8859-1')\ndf_2020 = df_2020_raw.iloc[1:,:]\ndf_2019_raw = pd.read_csv(r'..\/input\/kaggle-survey-2019\/multiple_choice_responses.csv',encoding='ISO-8859-1')\ndf_2019 = df_2019_raw.iloc[1:,:]\ndf_2018_raw = pd.read_csv(r'..\/input\/kaggle-survey-2018\/multipleChoiceResponses.csv',encoding='ISO-8859-1')\ndf_2018 = df_2018_raw.iloc[1:,:]\ndf_2017 = pd.read_csv(r'..\/input\/kaggle-survey-2017\/multipleChoiceResponses.csv',encoding='ISO-8859-1')\n","54126bc0":"df_2017.head(5)","a1948523":"import plotly.offline as py\npy.offline.init_notebook_mode()\n\ndef plot_in_map(locations,counts,title):\n    data = [ dict(\n            type = 'choropleth',\n            locations = locations,\n            z = counts,\n            zmax = 8000,\n            zmin = 0,\n            locationmode = 'country names',\n            autocolorscale = True,\n            marker = dict(\n                line = dict(color = 'rgb(58,100,69)', width = 0.6)),\n                colorbar = dict(autotick = True, tickprefix = '', title = 'Number of participants')\n                )\n           ]\n    layout = dict(\n        title = title,\n        geo = dict(\n            showframe = True,\n            showcoastlines = True,\n            projection = dict(\n            type = 'equirectangular'\n            ),\n        margin = dict(b = 0, t = 0, l = 0, r = 0)\n                ),\n        )\n\n    fig = dict(data=data, layout=layout)\n    \n    py.iplot(fig, validate=False, filename='world-map')","12f9a36a":"z_2021 = df_2021['Q3'].value_counts()\nplot_in_map(locations=z_2021.index,\n            counts=z_2021.values,\n            title='Participants by Country 2021')","88902aaf":"z_2020 = df_2020['Q3'].value_counts()\nplot_in_map(locations=z_2020.index,\n            counts=z_2020.values,\n            title='Participants by Country 2020')","fa6923dd":"z_2019 = df_2019['Q3'].value_counts()\nplot_in_map(locations=z_2019.index,\n            counts=z_2019.values,\n            title='Participants by Country 2019')","3ef13866":"z_2018 = df_2018['Q3'].value_counts()\nplot_in_map(locations=z_2018.index,\n            counts=z_2018.values,\n            title='Participants by Country 2018')","61febde4":"z_2017 = df_2017['Country'].value_counts()\nplot_in_map(locations=z_2017.index,\n            counts=z_2017.values,\n            title='Participants by Country 2017')","38b6a15c":"## There are several bit nodes sharing the popularity, which are Matplotlib, Seaborn, Plotly \/ Plotly Express and Ggplot \/ ggplot2. The first three of them comes from Python, and Ggplot comes from R, both Python and R are widely used programming languages and they are often used together as discussed before, so the result here seems fairly reasonable. One thing easy to notice is the big chunk between Matplotlib and Seaborn, which are the two most popular visualization packages in Python, and they are very often used together according to the result. This really makes sense not only from research, but also applies well to ourselves, considering their complements to each other.","87c415b6":"# Import packages","63aa8fe5":"# Take a brief look at the data set.","d69a72a3":"# Generate some networks","70b7c451":"# **Some analysis using kaggle 2021 survey responses**\n## Tool network analysis and spatio-temporal visualization for global participants with the inspiration from course projects and reference from some other kagglers.","693146a8":"# Load the data set","d8ac92e5":"# Visualize the global data scientists and the change in year 2017 - 2021","71853039":"## The wordcloud is a vivid presentation of all topics above about what appears most frequently in this research, which is pretty consistent with the visualizations we have before, and shows us a world of people in the field of data science.","ece91252":"## We can clearly see that Python is pretty much used by most data scientists and used with other languages. This shows the popularity and versatility of Python. Besides, from the size of the edges, we see the major links of Python is with R and SQL, it makes sense because of the obvious fact that these data munging and analysis tools are the most frequently used tool-set, and associating these tools often makes efficient and complement of each other.","c6f84be2":"## We constructed some global heat maps intending to explore how the number of data scientists changed during several years. Spatio-temporal maps for global data scientists from 2017 to 2021 based on the number of participants from different countries are shown as follows. There is no obvious increase for most countries except India, which is counterintuitive, and it may be related to the variation of participants of each year\u2019s survey.","641d295a":"## As for some other networks, ways of understanding them are quite similar, we can learn what are the popular development environments, machine learning algorithms, data science sharing platforms and learning platforms, from which you can know where to find others\u2019 data science projects and learning materials. At the same time, they also illustrate the preference of using the combination of tools for each topic.","384baddd":"## In fact, each participant can use more than one language and a variety of libraries; each job needs to be used on different platforms. As a result, these languages and algorithms are related to each other.\n## These connections may result from multiple skills required for the same job, or from similarities in algorithms or the language-self. However, after all, they are related, and this relation may lead us to learn them with a better plan.\n## Thus, here we construct the network on different topics for every data record to find the relation. They are not big-scaled complex networks, so even we skip the topological structure characteristics, there are quite some intuitive insights that we can have from these visualizations, with nodes\u2019 size showing the number of users and edges representing a combined use of some tools for one participant.","aed1debf":"# Generate a wordcloud"}}