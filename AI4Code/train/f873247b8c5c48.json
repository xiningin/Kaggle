{"cell_type":{"731c1b5f":"code","3b3ff667":"code","ee0562db":"code","575bbf40":"code","8ea7b51e":"code","92f8ede8":"code","5a17a33a":"code","c7638c5f":"code","444bea3d":"code","119369a2":"code","7f03ec45":"code","fa39f6dd":"code","edb7ece2":"code","6b4e53f3":"code","28b958fe":"code","749459a1":"code","104b1d34":"code","dd84c506":"code","d4d26bf0":"code","b3d46de7":"code","efe1557f":"code","56cd7906":"code","edcf20b9":"code","b5d2ce29":"code","f994d105":"code","cf32693c":"code","da769ea4":"code","ca799f59":"code","37c8c2e0":"code","e9b05180":"code","10ace194":"code","7a98e5ba":"code","2cbcab08":"code","130ef2f3":"code","59a62599":"code","48782625":"code","b89f6579":"code","7c20e954":"code","244843df":"code","5107db6c":"code","04720d58":"code","f276af35":"code","1047b581":"code","bfc56202":"code","c60aed42":"code","ea997e5e":"code","0ef9cbd2":"code","404022a9":"code","251c241d":"code","af2beda9":"code","837a94c9":"code","3dfd7c40":"code","6c318c31":"code","c120d84c":"code","9d1745b1":"code","ef4291c1":"code","db12d7ef":"code","cdc0c5d5":"code","2a4262e1":"code","37952316":"code","ffa4f82f":"code","0c8bcb71":"code","ee7b290d":"code","cfe33989":"code","3bb23dc4":"code","8dcae74c":"code","a062cb6b":"code","8aab505a":"code","20159017":"code","c5024f0c":"code","8cff6b3c":"code","47e93ffc":"code","4b6a5a30":"markdown","6cb4a5a5":"markdown","3933bdf2":"markdown","e32f78c8":"markdown","370c698c":"markdown","da0f8cc2":"markdown","68589be2":"markdown","cae41470":"markdown","87b6f320":"markdown","1aeed72f":"markdown","1b42197e":"markdown","cb50b309":"markdown","27898019":"markdown","05c999ad":"markdown","9b556669":"markdown","cae16ac3":"markdown","e9719391":"markdown","4d34d8f0":"markdown","bf89da90":"markdown","614d167f":"markdown","0a7b80fd":"markdown","a56766e8":"markdown","998961ee":"markdown","371beacf":"markdown","421d14ea":"markdown","6c813657":"markdown","b53bf755":"markdown","cda957ea":"markdown","332709cf":"markdown","d190e088":"markdown","13973546":"markdown","84d26221":"markdown","2be21657":"markdown","e34d513a":"markdown","b6175825":"markdown","6c3fc7a3":"markdown","ec8130c6":"markdown","66b72bbf":"markdown","d7dcc2db":"markdown","7c0b4cc9":"markdown","2318dcd4":"markdown","6acbce71":"markdown","8bd0168f":"markdown","0f8498df":"markdown","4ed09b58":"markdown","388fac15":"markdown","eabae46a":"markdown","fcff4197":"markdown","8fc3e222":"markdown","0c428c87":"markdown","8dde0c9b":"markdown"},"source":{"731c1b5f":"import numpy as np\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom matplotlib import pyplot as plt\nimport seaborn as sb\n\nFACEBOOK_DATA_PATH = \"\/kaggle\/input\/sandp500\/individual_stocks_5yr\/individual_stocks_5yr\/FB_data.csv\"\nAMAZON_DATA_PATH = \"\/kaggle\/input\/sandp500\/individual_stocks_5yr\/individual_stocks_5yr\/AMZN_data.csv\"\nAPPLE_DATA_PATH = \"\/kaggle\/input\/sandp500\/individual_stocks_5yr\/individual_stocks_5yr\/AAPL_data.csv\"\nNETFLIX_DATA_PATH = \"\/kaggle\/input\/sandp500\/individual_stocks_5yr\/individual_stocks_5yr\/NFLX_data.csv\"\nGOOGLE_DATA_PATH = \"\/kaggle\/input\/sandp500\/individual_stocks_5yr\/individual_stocks_5yr\/GOOGL_data.csv\"\n\nraw_fb_data = pd.read_csv(FACEBOOK_DATA_PATH)\nraw_amzn_data = pd.read_csv(AMAZON_DATA_PATH)\nraw_aapl_data = pd.read_csv(APPLE_DATA_PATH)\nraw_nflx_data = pd.read_csv(NETFLIX_DATA_PATH)\nraw_googl_data = pd.read_csv(GOOGLE_DATA_PATH)","3b3ff667":"print(f\"Facebook Data Set Shape: {raw_fb_data.shape}, Missing Data: {raw_fb_data.isnull().values.any()}\")\nprint(f\"Amazon Data Set Shape: {raw_amzn_data.shape}, Missing Data: {raw_amzn_data.isnull().values.any()}\")\nprint(f\"Apple Data Set Shape: {raw_aapl_data.shape}, Missing Data: {raw_aapl_data.isnull().values.any()}\")\nprint(f\"Netflix Data Set Shape: {raw_nflx_data.shape}, Missing Data: {raw_nflx_data.isnull().values.any()}\")\nprint(f\"Google Data Set Shape: {raw_googl_data.shape}, Missing Data: {raw_googl_data.isnull().values.any()}\")","ee0562db":"# show only the Facebook data set\nraw_fb_data.head()","575bbf40":"# the following function takes a list of stock data sets and generates candlestick plots for each\n# the function uses plotly candlestick\ndef candlestick_plot(stock_data: list):\n\n  for data_set in stock_data:\n    fig = go.Figure(data=[go.Candlestick(x=data_set[\"date\"],\n                          open=data_set[\"open\"],\n                          high=data_set[\"high\"],\n                          low=data_set[\"low\"],\n                          close=data_set[\"close\"])]\n                   )\n\n    fig.update_layout(\n        title=f\"{data_set['Name'][0]} Stock Prices Time-Series\",\n        xaxis_title=\"Date\",\n        yaxis_title=\"Price ($USD)\",\n        xaxis_rangeslider_visible=False)\n\n    fig.show()\n\nfaang_stocks = [raw_fb_data, raw_amzn_data, raw_aapl_data, raw_nflx_data, raw_googl_data]\n\ncandlestick_plot(faang_stocks)","8ea7b51e":"# take a look at the GOOGL data\nraw_googl_data.head()","92f8ede8":"univariate_googl_data = raw_googl_data[\"open\"].values\n# verify the ensuing data set is as expected\nprint(univariate_googl_data)","5a17a33a":"train_array = univariate_googl_data[:759].reshape(-1,1)\nval_array = univariate_googl_data[759:1000].reshape(-1,1)\ntest_array = univariate_googl_data[1000:1259].reshape(-1,1)","c7638c5f":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\ntrain_array = scaler.fit_transform(train_array)\nval_array = scaler.transform(val_array)\ntest_array = scaler.transform(test_array)","444bea3d":"def series_generator(data: np.array, time_interval: int, total_time_steps: int) -> np.array:\n\n  X_time_series, y_time_series = [], []\n\n  for time_step in range(time_interval, total_time_steps):\n    X_time_series.append(data[time_step - time_interval:time_step, 0])\n    y_time_series.append(data[time_step, 0])\n\n  X_time_series, y_time_series = np.array(X_time_series), np.array(y_time_series)\n  return X_time_series, y_time_series","119369a2":"X_train, y_train = series_generator(train_array, 60, len(train_array))\nX_val, y_val = series_generator(val_array, 60, len(val_array))\nX_test, y_test = series_generator(test_array, 60, len(test_array))","7f03ec45":"X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\nX_val = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)\nX_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n\n# verify the array dimensions are as expected\nprint(f\"X_train shape: {X_train.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")","fa39f6dd":"# define a function to plot time-series data and predicted values\ndef plot_time_series(time_series_data: np.array, time_steps: int, y_true: float, y_pred: float, time_series_idx: str):\n\n  sb.set_theme()\n  plt.rcParams['figure.figsize'] = [20, 8]\n\n  plt.plot(time_series_data, \".-\")\n  plt.plot(time_steps, y_true, \"go\", label=\"True Price\")\n  plt.plot(time_steps, y_pred, \"rx\", label=\"Model Predicted Price\")\n  \n  plt.xlabel(\"Time-Step\")\n  plt.ylabel(\"Transformed Open Price\")\n  plt.title(f\"{time_series_idx} Google Stock Time-Series\")\n  plt.grid(True)\n  plt.legend(fontsize=16)\n\n# define a function to plot time-series neural network mean squared error (MSE) history\ndef training_plots(training_progress: dict, model_name: str):\n    \n    sb.set_theme()\n    plt.rcParams['figure.figsize'] = [20, 8]\n    \n    plt.plot(training_progress[\"loss\"], \"g\", label=\"Train Loss\")\n    plt.plot(training_progress[\"val_loss\"], \"b\", label=\"Validation Loss\")\n    plt.title(f\"{model_name} Mean Squared Error (MSE) Plot\", fontsize=16)\n    plt.xlabel(\"Epoch\", fontsize=16)\n    plt.ylabel(\"Loss\", fontsize=16)\n    plt.legend(fontsize=16)","edb7ece2":"import tensorflow as tf\n\n# the naive approach simply copies the last value in each time series\nnaive_y_pred = X_val[:, -1]\nnaive_mse = np.mean(tf.keras.losses.mean_squared_error(y_val, naive_y_pred))\nprint(naive_mse)\n\n# store the naive mse in a dictionary for comparison later\narchitectures_mse = {\"Naive\": naive_mse}","6b4e53f3":"plot_time_series(X_val[0, :], 60, y_val[0], naive_y_pred[0], \"Naive Prediction\")","28b958fe":"# set seeds to 42 for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# define an early stopping callback, monitoring the mean squared error (MSE) validation set loss\nval_stop = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n\n# define a performance scheduling callback, monitoring the mean squared error (MSE) validation set loss\nlr_monitor = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5)\n\n# simply fully connected neural network\nFCNN_model = tf.keras.models.Sequential([\n                tf.keras.layers.Flatten(input_shape=[60, 1]),\n                tf.keras.layers.Dense(1)\n             ])\n\n# compile the model using standard settings\nFCNN_model.compile(loss=\"mse\",\n                   optimizer=\"adam\")\n\n# display a of the model\nFCNN_model.summary()","749459a1":"# train the model\n# epochs is set to 1000 as callbacks are used to stop training\nFCNN_history = FCNN_model.fit(X_train, y_train, epochs=1000, verbose=0, batch_size=64,\n                              validation_data=(X_val, y_val),\n                              callbacks=[val_stop, lr_monitor])","104b1d34":"training_plots(FCNN_history.history, \"Fully Connected Neural Network\")","dd84c506":"FCNN_mse = FCNN_model.evaluate(X_val, y_val, verbose=0)\nprint(f\"Model Accuracy: {FCNN_mse}\")\n\n# store the FCNN mse in a dictionary for comparison later\narchitectures_mse[\"FCNN\"] = FCNN_mse\n\nFCNN_y_pred = FCNN_model.predict(X_val)","d4d26bf0":"plt.subplot(1, 2, 1)\nplot_time_series(X_val[0, :], 60, y_val[0], FCNN_y_pred[0], \"1st\")\nplt.subplot(1, 2, 2)\nplot_time_series(X_val[25, :], 60, y_val[25], FCNN_y_pred[25], \"25th\")\n\nplt.figure()\n\nplt.subplot(1, 2, 1)\nplot_time_series(X_val[50, :], 60, y_val[50], FCNN_y_pred[50], \"50th\")\nplt.subplot(1, 2, 2)\nplot_time_series(X_val[75, :], 60, y_val[75], FCNN_y_pred[75], \"75th\")\n\nplt.figure()","b3d46de7":"print(architectures_mse)","efe1557f":"# set seeds to 42 for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# single neuron RNN\nsingle_neuron_RNN_model = tf.keras.models.Sequential([\n                             tf.keras.layers.SimpleRNN(1, input_shape=[None, 1])\n                          ])\n\n# compile the model using standard settings\nsingle_neuron_RNN_model.compile(loss=\"mse\",\n                                optimizer=\"adam\")\n\n# display a of the model\nsingle_neuron_RNN_model.summary()","56cd7906":"# train the model\n# epochs is set to 1000 as callbacks are used to stop training\nsingle_neuron_RNN_history = single_neuron_RNN_model.fit(X_train, y_train, epochs=1000, verbose=0, batch_size=64,\n                                                        validation_data=(X_val, y_val),\n                                                        callbacks=[val_stop, lr_monitor])","edcf20b9":"training_plots(single_neuron_RNN_history.history, \"Single Neuron Recurrent Neural Network (RNN)\")","b5d2ce29":"single_neuron_RNN_mse = single_neuron_RNN_model.evaluate(X_val, y_val, verbose=0)\nprint(f\"Model Accuracy: {single_neuron_RNN_mse}\")\n\narchitectures_mse[\"Single Neuron RNN\"] = single_neuron_RNN_mse\n\nsingle_neuron_RNN_y_pred = single_neuron_RNN_model.predict(X_val)","f994d105":"plt.subplot(1, 2, 1)\nplot_time_series(X_val[0, :], 60, y_val[0], single_neuron_RNN_y_pred[0], \"1st\")\nplt.subplot(1, 2, 2)\nplot_time_series(X_val[25, :], 60, y_val[25], single_neuron_RNN_y_pred[25], \"25th\")\n\nplt.figure()\n\nplt.subplot(1, 2, 1)\nplot_time_series(X_val[50, :], 60, y_val[50], single_neuron_RNN_y_pred[50], \"50th\")\nplt.subplot(1, 2, 2)\nplot_time_series(X_val[75, :], 60, y_val[75], single_neuron_RNN_y_pred[75], \"75th\")\n\nplt.figure()","cf32693c":"print(architectures_mse)","da769ea4":"# set seeds to 42 for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# deep RNN\nRNN_model = tf.keras.models.Sequential([\n               tf.keras.layers.SimpleRNN(50, return_sequences=True, input_shape=[None, 1]),\n               tf.keras.layers.Dropout(0.2),\n               tf.keras.layers.SimpleRNN(50, return_sequences=True),\n               tf.keras.layers.Dropout(0.2),\n               tf.keras.layers.SimpleRNN(50),\n               tf.keras.layers.Dropout(0.2),\n               tf.keras.layers.Dense(1)\n             ])\n\n# compile the model using standard settings\nRNN_model.compile(loss=\"mse\",\n                   optimizer=\"adam\")\n\n# display a of the model\nRNN_model.summary()","ca799f59":"# train the model\n# epochs is set to 1000 as callbacks are used to stop training\nRNN_history = RNN_model.fit(X_train, y_train, epochs=1000, verbose=0, batch_size=64,\n                            validation_data=(X_val, y_val),\n                            callbacks=[val_stop, lr_monitor])","37c8c2e0":"training_plots(RNN_history.history, \"Deep Recurrent Neural Network (RNN)\")","e9b05180":"RNN_mse = RNN_model.evaluate(X_val, y_val, verbose=0)\nprint(f\"Model Accuracy: {RNN_mse}\")\n\narchitectures_mse[\"Deep RNN\"] = RNN_mse\n\nRNN_y_pred = RNN_model.predict(X_val)","10ace194":"plt.subplot(1, 2, 1)\nplot_time_series(X_val[0, :], 60, y_val[0], RNN_y_pred[0], \"1st\")\nplt.subplot(1, 2, 2)\nplot_time_series(X_val[25, :], 60, y_val[25], RNN_y_pred[25], \"25th\")\n\nplt.figure()\n\nplt.subplot(1, 2, 1)\nplot_time_series(X_val[50, :], 60, y_val[50], RNN_y_pred[50], \"50th\")\nplt.subplot(1, 2, 2)\nplot_time_series(X_val[75, :], 60, y_val[75], RNN_y_pred[75], \"75th\")\n\nplt.figure()","7a98e5ba":"print(architectures_mse)","2cbcab08":"# set seeds to 42 for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# deep RNN\nRNN_LSTM_model = tf.keras.models.Sequential([\n                    tf.keras.layers.LSTM(50, return_sequences=True, input_shape=[None, 1]),\n                    tf.keras.layers.Dropout(0.2),\n                    tf.keras.layers.LSTM(50, return_sequences=True),\n                    tf.keras.layers.Dropout(0.2),\n                    tf.keras.layers.LSTM(50),\n                    tf.keras.layers.Dropout(0.2),\n                    tf.keras.layers.Dense(1)\n                 ])\n\n# compile the model using standard settings\nRNN_LSTM_model.compile(loss=\"mse\",\n                       optimizer=\"adam\")\n\n# display a of the model\nRNN_LSTM_model.summary()","130ef2f3":"# train the model\n# epochs is set to 1000 as callbacks are used to stop training\nRNN_LSTM_history = RNN_LSTM_model.fit(X_train, y_train, epochs=1000, verbose=0, batch_size=64,\n                                      validation_data=(X_val, y_val),\n                                      callbacks=[val_stop, lr_monitor])","59a62599":"training_plots(RNN_LSTM_history.history, \"Deep Recurrent Neural Network (RNN) With Long Short-Term Memory Cells (LSTM)\")","48782625":"RNN_LSTM_mse = RNN_LSTM_model.evaluate(X_val, y_val, verbose=0)\nprint(f\"Model Accuracy: {RNN_LSTM_mse}\")\n\narchitectures_mse[\"Deep LSTM RNN\"] = RNN_LSTM_mse\n\nRNN_LSTM_y_pred = RNN_LSTM_model.predict(X_val)","b89f6579":"plt.subplot(1, 2, 1)\nplot_time_series(X_val[0, :], 60, y_val[0], RNN_LSTM_y_pred[0], \"1st\")\nplt.subplot(1, 2, 2)\nplot_time_series(X_val[25, :], 60, y_val[25], RNN_LSTM_y_pred[25], \"25th\")\n\nplt.figure()\n\nplt.subplot(1, 2, 1)\nplot_time_series(X_val[50, :], 60, y_val[50], RNN_LSTM_y_pred[50], \"50th\")\nplt.subplot(1, 2, 2)\nplot_time_series(X_val[75, :], 60, y_val[75], RNN_LSTM_y_pred[75], \"75th\")\n\nplt.figure()","7c20e954":"print(architectures_mse)","244843df":"RNN_test_mse = RNN_model.evaluate(X_test, y_test, verbose=0)\nprint(f\"Deep RNN Validation Set MSE: {architectures_mse['Deep RNN']}\")\nprint(f\"Deep RNN Test Set MSE: {RNN_test_mse}\")\n\nprint(RNN_LSTM_model.evaluate(X_train, y_train, verbose=0))","5107db6c":"# take a look at the GOOGL data\nraw_googl_data.head()","04720d58":"multivariate_googl_data = raw_googl_data.drop([\"date\", \"Name\"], axis=1).values\nprint(multivariate_googl_data)","f276af35":"train_array = multivariate_googl_data[:759]\nval_array = multivariate_googl_data[759:1000]\ntest_array = multivariate_googl_data[1000:1259]\nprint(train_array.shape)","1047b581":"multivariate_scaler = MinMaxScaler()\ntrain_array = multivariate_scaler.fit_transform(train_array)\nval_array = multivariate_scaler.transform(val_array)\ntest_array = multivariate_scaler.transform(test_array)","bfc56202":"def multivariate_series_generator(data: np.array, time_interval: int, total_time_steps: int) -> np.array:\n\n  X_time_series, y_time_series = [], []\n\n  for time_step in range(time_interval, total_time_steps):\n    X_time_series.append(data[time_step - time_interval:time_step, 0:data.shape[1]])\n    y_time_series.append(data[time_step, 0:1])\n\n  X_time_series, y_time_series = np.array(X_time_series), np.array(y_time_series)\n  return X_time_series, y_time_series","c60aed42":"# \"mv\" to denote multivariate\nmv_X_train, mv_y_train = multivariate_series_generator(train_array, 60, len(train_array))\nmv_X_val, mv_y_val = multivariate_series_generator(val_array, 60, len(val_array))\nmv_X_test, mv_y_test = multivariate_series_generator(test_array, 60, len(test_array))","ea997e5e":"print(mv_X_train.shape)","0ef9cbd2":"# set seeds to 42 for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# simply fully connected neural network\nmultivariate_FCNN_model = tf.keras.models.Sequential([\n                             tf.keras.layers.Flatten(input_shape=[60, 5]),\n                             tf.keras.layers.Dense(1)\n                          ])\n\n# compile the model using standard settings\nmultivariate_FCNN_model.compile(loss=\"mse\",\n                   optimizer=\"adam\")\n\n# display a of the model\nmultivariate_FCNN_model.summary()","404022a9":"# train the model\n# epochs is set to 1000 as callbacks are used to stop training\nmultivariate_FCNN_history = multivariate_FCNN_model.fit(mv_X_train, mv_y_train, epochs=1000, verbose=0, batch_size=64,\n                                                        validation_data=(mv_X_val, mv_y_val),\n                                                        callbacks=[val_stop, lr_monitor])","251c241d":"training_plots(multivariate_FCNN_history.history, \"Multivariate Fully Connected Neural Network\")","af2beda9":"multivariate_FCNN_mse = multivariate_FCNN_model.evaluate(mv_X_val, mv_y_val, verbose=0)\nprint(f\"Model Accuracy: {multivariate_FCNN_mse}\")\n\n# store the FCNN mse in a dictionary for comparison later\narchitectures_mse[\"MV FCNN\"] = multivariate_FCNN_mse\n\nmultivariate_FCNN_y_pred = multivariate_FCNN_model.predict(mv_X_val)","837a94c9":"plt.subplot(1, 2, 1)\nplot_time_series(mv_X_val[0][:, 0], 60, mv_y_val[0], multivariate_FCNN_y_pred[0], \"1st\")\nplt.subplot(1, 2, 2)\nplot_time_series(mv_X_val[25][:, 0], 60, mv_y_val[25], multivariate_FCNN_y_pred[25], \"25th\")\n\nplt.figure()\n\nplt.subplot(1, 2, 1)\nplot_time_series(mv_X_val[50][:, 0], 60, mv_y_val[50], multivariate_FCNN_y_pred[50], \"50th\")\nplt.subplot(1, 2, 2)\nplot_time_series(mv_X_val[75][:, 0], 60, mv_y_val[75], multivariate_FCNN_y_pred[75], \"75th\")","3dfd7c40":"print(architectures_mse)","6c318c31":"# set seeds to 42 for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# single neuron RNN\nmultivariate_single_neuron_RNN_model = tf.keras.models.Sequential([\n                                          tf.keras.layers.SimpleRNN(1, input_shape=[None, 5])\n                                       ])\n\n# compile the model using standard settings\nmultivariate_single_neuron_RNN_model.compile(loss=\"mse\",\n                                             optimizer=\"adam\")\n\n# display a of the model\nmultivariate_single_neuron_RNN_model.summary()","c120d84c":"# train the model\n# epochs is set to 1000 as callbacks are used to stop training\nmultivariate_single_neuron_RNN_history = multivariate_single_neuron_RNN_model.fit(mv_X_train, mv_y_train, epochs=1000, verbose=0, batch_size=64,\n                                                                                  validation_data=(mv_X_val, mv_y_val),\n                                                                                  callbacks=[val_stop, lr_monitor])","9d1745b1":"training_plots(multivariate_single_neuron_RNN_history.history, \"Multivariate Single Neuron Recurrent Neural Network (RNN)\")","ef4291c1":"multivariate_single_neuron_RNN_mse = multivariate_single_neuron_RNN_model.evaluate(mv_X_val, mv_y_val, verbose=0)\nprint(f\"Model Accuracy: {multivariate_single_neuron_RNN_mse}\")\n\narchitectures_mse[\"MV Single Neuron RNN\"] = multivariate_single_neuron_RNN_mse\n\nmultivariate_single_neuron_RNN_y_pred = multivariate_single_neuron_RNN_model.predict(mv_X_val)","db12d7ef":"plt.subplot(1, 2, 1)\nplot_time_series(mv_X_val[0][:, 0], 60, mv_y_val[0], multivariate_single_neuron_RNN_y_pred[0][0], \"1st\")\nplt.subplot(1, 2, 2)\nplot_time_series(mv_X_val[25][:, 0], 60, mv_y_val[25], multivariate_single_neuron_RNN_y_pred[25][0], \"25th\")\n\nplt.figure()\n\nplt.subplot(1, 2, 1)\nplot_time_series(mv_X_val[50][:, 0], 60, mv_y_val[50], multivariate_single_neuron_RNN_y_pred[50][0], \"50th\")\nplt.subplot(1, 2, 2)\nplot_time_series(mv_X_val[75][:, 0], 60, mv_y_val[75], multivariate_single_neuron_RNN_y_pred[75][0], \"75th\")","cdc0c5d5":"print(architectures_mse)","2a4262e1":"# set seeds to 42 for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# deep RNN\nmultivariate_RNN_model = tf.keras.models.Sequential([\n                            tf.keras.layers.SimpleRNN(50, return_sequences=True, input_shape=[None, 5]),\n                            tf.keras.layers.Dropout(0.2),\n                            tf.keras.layers.SimpleRNN(50, return_sequences=True),\n                            tf.keras.layers.Dropout(0.2),\n                            tf.keras.layers.SimpleRNN(50),\n                            tf.keras.layers.Dropout(0.2),\n                            tf.keras.layers.Dense(1)\n                         ])\n\n# compile the model using standard settings\nmultivariate_RNN_model.compile(loss=\"mse\",\n                               optimizer=\"adam\")\n\n# display a of the model\nmultivariate_RNN_model.summary()","37952316":"# train the model\n# epochs is set to 1000 as callbacks are used to stop training\nmultivariate_RNN_history = multivariate_RNN_model.fit(mv_X_train, mv_y_train, epochs=1000, verbose=0, batch_size=64,\n                                                      validation_data=(mv_X_val, mv_y_val),\n                                                      callbacks=[val_stop, lr_monitor])","ffa4f82f":"training_plots(multivariate_RNN_history.history, \"Multivariate Deep Recurrent Neural Network (RNN)\")","0c8bcb71":"multivariate_RNN_mse = multivariate_RNN_model.evaluate(mv_X_val, mv_y_val, verbose=0)\nprint(f\"Model Accuracy: {multivariate_RNN_mse}\")\n\narchitectures_mse[\"MV Deep RNN\"] = multivariate_RNN_mse\n\nmultivariate_RNN_y_pred = multivariate_RNN_model.predict(mv_X_val)","ee7b290d":"plt.subplot(1, 2, 1)\nplot_time_series(mv_X_val[0][:, 0], 60, mv_y_val[0], multivariate_RNN_y_pred[0][0], \"1st\")\nplt.subplot(1, 2, 2)\nplot_time_series(mv_X_val[25][:, 0], 60, mv_y_val[25], multivariate_RNN_y_pred[25][0], \"25th\")\n\nplt.figure()\n\nplt.subplot(1, 2, 1)\nplot_time_series(mv_X_val[50][:, 0], 60, mv_y_val[50], multivariate_RNN_y_pred[50][0], \"50th\")\nplt.subplot(1, 2, 2)\nplot_time_series(mv_X_val[75][:, 0], 60, mv_y_val[75], multivariate_RNN_y_pred[75][0], \"75th\")","cfe33989":"print(architectures_mse)","3bb23dc4":"# set seeds to 42 for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# deep RNN\nmultivariate_RNN_LSTM_model = tf.keras.models.Sequential([\n                              tf.keras.layers.LSTM(50, return_sequences=True, input_shape=[None, 5]),\n                              tf.keras.layers.Dropout(0.2),\n                              tf.keras.layers.LSTM(50, return_sequences=True),\n                              tf.keras.layers.Dropout(0.2),\n                              tf.keras.layers.LSTM(50),\n                              tf.keras.layers.Dropout(0.2),\n                              tf.keras.layers.Dense(1)\n                              ])\n\n# compile the model using standard settings\nmultivariate_RNN_LSTM_model.compile(loss=\"mse\",\n                                    optimizer=\"adam\")\n\n# display a of the model\nmultivariate_RNN_LSTM_model.summary()","8dcae74c":"# train the model\n# epochs is set to 1000 as callbacks are used to stop training\nmultivariate_RNN_LSTM_history = multivariate_RNN_LSTM_model.fit(mv_X_train, mv_y_train, epochs=1000, verbose=0, batch_size=64,\n                                                                validation_data=(mv_X_val, mv_y_val),\n                                                                callbacks=[val_stop, lr_monitor])","a062cb6b":"training_plots(multivariate_RNN_LSTM_history.history, \"Multivariate Deep Recurrent Neural Network (RNN) With Long Short-Term Memory Cells (LSTM)\")","8aab505a":"multivariate_RNN_LSTM_mse = multivariate_RNN_LSTM_model.evaluate(mv_X_val, mv_y_val, verbose=0)\nprint(f\"Model Accuracy: {multivariate_RNN_LSTM_mse}\")\n\narchitectures_mse[\"MV Deep LSTM RNN\"] = multivariate_RNN_LSTM_mse\n\nmultivariate_RNN_LSTM_y_pred = multivariate_RNN_LSTM_model.predict(mv_X_val)","20159017":"plt.subplot(1, 2, 1)\nplot_time_series(mv_X_val[0][:, 0], 60, mv_y_val[0], multivariate_RNN_LSTM_y_pred[0][0], \"1st\")\nplt.subplot(1, 2, 2)\nplot_time_series(mv_X_val[25][:, 0], 60, mv_y_val[25], multivariate_RNN_LSTM_y_pred[25][0], \"25th\")\n\nplt.figure()\n\nplt.subplot(1, 2, 1)\nplot_time_series(mv_X_val[50][:, 0], 60, mv_y_val[50], multivariate_RNN_LSTM_y_pred[50][0], \"50th\")\nplt.subplot(1, 2, 2)\nplot_time_series(mv_X_val[75][:, 0], 60, mv_y_val[75], multivariate_RNN_LSTM_y_pred[75][0], \"75th\")","c5024f0c":"print(architectures_mse)","8cff6b3c":"pd.options.display.max_columns = 10\n\nfinal_model_mse = pd.DataFrame([architectures_mse])\nprint(final_model_mse)","47e93ffc":"deep_RNN_test_mse = RNN_model.evaluate(X_test, y_test, verbose=0)\nprint(deep_RNN_test_mse)","4b6a5a30":"Manually split the raw `GOOGL` dataset into train\/val\/test splits following ~ 80\/20\/20 ","6cb4a5a5":"Show 4 predictions for time-series separated by 25 time-steps:","3933bdf2":"With a baseline metric computed, let's now build the first fully connected network architecture to perform Linear Regression.","e32f78c8":"#### Model 2: Simple Recurrent Neural Network (RNN)\n\nTry a single neuron RNN.","370c698c":"## Data Visualization\n\nImport standard data handling and visualization libraries.","da0f8cc2":"All the data sets should look like the code block output above. A standard visualization of stock prices time-series is through a candlestick representation with displays the open, high, and low prices as a function of time progression. Let's write a function to generate these plots.","68589be2":"The deep RNN performs much better than the single neuron RNN for multivariate time-series analysis but worse than the fully connected neural network. Let's \ninvestigate the deep RNN using LSTM cells.","cae41470":"# Time-Series Forecasting Neural Network Architectures Exploration: S&P 500 Stock Prices \n\n## Time-Series Forecasting\n\nThis Jupyter notebook uses Keras to build different neural network architectures including Recurrent Neureal Networks (RNNs) to explore different time-series forecasting for stock prices.\n\n## Dataset\n\nhttps:\/\/www.kaggle.com\/camnugent\/sandp500\n\n## Objective\n\nExplore the performances of different architectures applied to stock prices forecasting.","87b6f320":"#### Univariate Time-Series: Define Utility Functions\n\nWe will define 2 plotting functions to help analyze time-series predictions.","1aeed72f":"#### Model 1: Simple Fully Connected Neural Network","1b42197e":"Define a modified function to generate the multivariate time-series data.\n","cb50b309":"#### Model 3: Deep Recurrent Neural Network (RNN)\n\nDropout will be incorporated into the following RNN for regularization.","27898019":"Let's remove the metrics that are not relevant to this section (keep only \"date\" and \"open\").","05c999ad":"Ensure the data sets do not have missing values.","9b556669":"Show 4 predictions for time-series separated by 25 time-steps:","cae16ac3":"We observe that the single neuron RNN performs much worse than the fully connected neural network but better than the naive baseline approach. Let's next try a RNN with more hidden layers.","e9719391":"The Multiple Regression neural network performs very well! Just behind the deep RNN. This is most likely due to feeding in more data. Let's now investigate a single neuron RNN.","4d34d8f0":"Based on the performance on the validations sets, the univariate Deep RNN performs the best. Let's evaluate the model's performance on the test set.","bf89da90":"#### Model 3: Deep Recurrent Neural Network (RNN)\n\nDropout will be incorporated into the following RNN for regularization.","614d167f":"Show 4 predictions for time-series separated by 25 time-steps:","0a7b80fd":"Show 4 predictions for time-series separated by 25 time-steps:","a56766e8":"We are now ready to investigate the different model architectures. The naive baseline metric is the same as the previous case (where we simply copy the last value in a given time-series and take that as the predicted value for 1 day ahead). The naive baseline metric is the same in the current multivariate time-series analysis because we are still only prediction \"open\" price. The difference compared to the previous univariate time-series analysis is that we are now using all the other features (which we assume has an effect on \"open\" price) such as \"volume\" (volume traded).","998961ee":"## Univariate Time-Series\n\nIn this section, we will use different neural network architectures to forecast \"open\" stock prices. As there is only 1 variable (\"open\" price) associated with each time step, the following models perform univariate time-series forecasting. To narrow the scope of the analysis and reduce training time, only `GOOGL` stock will be considered.","371beacf":"This time, all the other variables will be used to train our models. Drop the \"date\" and \"Name\" columns for simplicity.","421d14ea":"Next, define a function that generates time series data as follows:\n\n1) Receives as input a pandas DataFrame containing time-series data\n\n2) Receives as input a time inteval integer which denotes the length of time-series sequences to generate\n\n3) Receives as input the total time steps in the dataset. In the case of `GOOGL` stock data, this denotes the number of data entries\n\n4) The function returns a list of arrays. Each array represents a time series sequence with length denoted by time interval. For example, if time interval were chosen as 60 the function appends to a list arrays containing entries `[0-59], [1-60], [2,61]`, etc. In essence, the stock dataset will be partitioned into the maximum number of sequential arrays possible given the provided time interval.","6c813657":"Each time-step in a given series is represented by a single neuron and the overall architecture performins Linear Regression. Let's now train the model.","b53bf755":"#### Model 1: Simple Fully Connected Neural Network\n\nPreviously, univariate time-series analysis essentially used Linear Regression. Here, we are using Multiple Regression (we are using more than 1 variable (\"close\" price, \"volume\", etc.) to predict the output dependent variable (\"open\" price).","cda957ea":"## Univariate Time-Series: Data Preparation\n\nBefore the data is ready to be used, it needs to be properly scaled and transformed into arrays of appropriate diemensions. Let's first take a look at the raw unmodified data.","332709cf":"Let's take a look at the data which consists of 7 columns. ","d190e088":"We observe that the fully connected neural network performs much better than the naive baseline approach. This is quantitatively supported by the lower MSE. Let's next try a simple RNN architecture.","13973546":"The data is now ready to be fed into a neural network.","84d26221":"## Multivariate Time-Series\n\nIn this section, we will use only the RNN architectures as above to explore multivariate time-series analysis. Previously, only the \"open\" stock price was used to forecast the next \"open\" price. Other variables must also affect the \"open\" price such as \"volume\" traded. This section will use all the variables in the raw `GOOGL` stock data to predict \"open\" price.","2be21657":"Show 4 predictions for time-series separated by 25 time-steps:","e34d513a":"#### Model 2: Simple Recurrent Neural Network (RNN)","b6175825":"Next, reshape X_train, X_val, and X_test to the form (to feed into an RNN later):\n\n`[batch size, time steps, dimension]`","6c3fc7a3":"#### Model 4: Deep Recurrent Neural Network (RNN) with Long Short-Term Memory Cells (LSTM)\n\nDropout will again be incorporated for regularization.","ec8130c6":"The single neuron RNN perform by far the worst! Even worse than the naive approach. This is most likely due to an insufficient architecture to learn from the data. We are employing a single recurrent neuron to learn 5 different features. Let's now investigate a deep RNN.","66b72bbf":"Show 4 predictions for time-series separated by 25 time-steps:","d7dcc2db":"Next, we need to scale the data for training. The minimum and maximum values used in the scaler should only consider the train data to avoid data leakage. We can ensure this by using fit_transform on the train array and using only transform on the val and test arrays.","7c0b4cc9":"## Multivariate Time-Series: Data Preparation\n\nBefore the data is ready to be used, it needs to be properly scaled and transformed into arrays of appropriate diemensions again. Let's take a look at the raw unmodified data again.","2318dcd4":"Verify the shapes of the arrays are correct. Recall the RNN array shape:\n\n`[batch size, time steps, dimension]`\n\nDimension should be 5 now as there are 5 features.","6acbce71":"Generate the train, val, and test sets using the function defined above. We will use a time interval of 60 time-steps.","8bd0168f":"The test set mean squared error (MSE) is much worse than on the validation set. This may suggest more regularization was needed during model training. In addition, it may be more optimal to use longer time-series sequences during training to potentially help the model learn better. \n\nFinally, neural networks are very data demanding and the relatively small dataset used in this notebook is not ideal in training a good neural network. The code in this notebook was for personal practise in creating time-series data and applying different models for forecasting.","0f8498df":"Show 4 predictions for time-series separated by 25 time-steps:","4ed09b58":"#### Model 4: Deep Recurrent Neural Network (RNN) with Long Short-Term Memory Cells (LSTM)\n\nDropout will again be incorporated for regularization.","388fac15":"The deep RNN with LSTM cells performs better than the simple RNN cells. This may be explained by their increased ability to retain memory. Previously, univariate analysis only contained a single feature. With 5 features in the multivariate time-series analysis, retaining memory is even more important as presumably, all features affect the \"open\" price.","eabae46a":"We observe that the deep RNN is now the best performing architecture. Finally, let's now try a RNN architecture which incorporates Long Short-Term Memory (LSTM) Cells.","fcff4197":"## Univariate Time-Series: Neural Network Architectures Exploration\n\nThe models evaluated in this section will include:\n\n1) Fully connected network \n\n2) Simple RNN\n\n3) Deep RNN\n\n4) Deep RNN with LSTM Cells\n\nBefore training any model, let's first obtain a baseline metric: simply copy the last value of each time-series as the predicted price. Let's see the mean squared error (MSE) associated with this naive approach.","8fc3e222":"Show 4 predictions for time-series separated by 25 time-steps:","0c428c87":"For the univariate time-series analysis, let's deploy the best model (Deep RNN) and measure it's performance on the test set.","8dde0c9b":"## Conclusion\n\nLet's first display the performances of all models."}}