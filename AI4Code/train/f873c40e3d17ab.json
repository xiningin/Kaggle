{"cell_type":{"356c0ed6":"code","2dcdba06":"code","6ec5c5aa":"code","4fe21e18":"code","7e17307a":"code","9392383d":"code","173955ec":"code","b78cf28d":"code","348c8ad3":"code","df4cf609":"code","c84a8e56":"code","d291aef6":"code","da2c90e6":"code","d130f24f":"code","7cee42d7":"code","2c085da5":"code","35448049":"code","8aa9c538":"code","8851f535":"code","37b4c150":"code","d22f3398":"code","ee9fbb33":"code","c878788d":"code","d351587b":"code","7367995f":"code","cb435f7b":"code","255accd9":"code","3a6aa7df":"code","7e90b383":"code","2d3c95f4":"code","55beb2f1":"code","df8cb641":"code","07d41f31":"code","0f9c3af6":"code","b3fe2b26":"code","ece8a4eb":"code","250a23b5":"code","7b5037a2":"code","8f4cccf1":"code","3bea1d5a":"code","c119176d":"code","39d56901":"code","2ddc5936":"code","0fd4cca2":"code","c9c71d56":"code","6669c861":"code","ebcab815":"code","535b256d":"code","f98aa12a":"code","06e69058":"code","5049177e":"code","f1f8d8d2":"code","410d0b6f":"code","c167ebf2":"code","160f3068":"code","cc1567b6":"code","d945bb77":"code","5adb9a28":"code","26911388":"code","b2577fb2":"code","73dc1276":"code","131c1cde":"code","e0dba71c":"code","7ed1a5a3":"code","d5a0c18e":"code","95028213":"code","88e2d19b":"code","0af50038":"code","946c4e00":"code","b00c727d":"code","35f5856f":"code","cb14733f":"code","248e3c1a":"code","6d90b446":"code","2a14b9bc":"code","f5d86033":"code","5c08d8b9":"code","f13f4f3b":"code","3c5a7e7a":"code","a3c73729":"code","69559d6e":"code","09698d72":"code","2ebba716":"code","0b879d80":"code","83c57bc1":"markdown","fe5292b4":"markdown","adeb07a8":"markdown","b1ccb106":"markdown","e818a79e":"markdown","927ffd61":"markdown","b14a094f":"markdown","2a9a8f27":"markdown","9c53df5b":"markdown","20ff05af":"markdown","82df22b9":"markdown","6d2cbd51":"markdown","bc87a7cf":"markdown","e54cee34":"markdown","8d4af548":"markdown","956d1c05":"markdown","7c8ba514":"markdown","4147e99d":"markdown","be05c163":"markdown","e2045018":"markdown","c1e6aba1":"markdown","5ee497df":"markdown","423ec150":"markdown","05d76fbf":"markdown","a43c936e":"markdown","d0da7f6b":"markdown","627bc73b":"markdown","12f268f6":"markdown","1286a333":"markdown","f8c3ab2e":"markdown","3e25ca18":"markdown","eca1a07a":"markdown","1a727f9d":"markdown","3f01203f":"markdown","378c175e":"markdown","4ec3c967":"markdown","e1f3fed8":"markdown","56dec9e6":"markdown","58fbe663":"markdown","fd46db68":"markdown","bd22813a":"markdown","6c9cc22c":"markdown","04d0b822":"markdown","270fff52":"markdown","3a8f95b9":"markdown","a7861707":"markdown","01bfaa71":"markdown","094059d6":"markdown","e10ce0a2":"markdown","56c28973":"markdown","b7603d93":"markdown","5009a8db":"markdown","8fdb84c8":"markdown","3e2b2826":"markdown"},"source":{"356c0ed6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2dcdba06":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_validate\nfrom collections import Counter\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve","6ec5c5aa":"#Reading data in the .csv format (no special encoding schemes required)\n\ndf_train = pd.read_csv('..\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('..\/input\/titanic\/test.csv')\nid_t = df_test['PassengerId']\nprint(df_train.shape)\nprint(df_test.shape)","4fe21e18":"#Datatypes of the training set - Categorical and numerical variables\n\ndf_train.dtypes","7e17307a":"#Outlier detection using the Tukey method\n\ndef detect(dataframe,n,features):\n    ind = []\n    for column in features:\n        q1 = np.percentile(dataframe[column],25)\n        q3 = np.percentile(dataframe[column],75)\n        res = q3 - q1\n        new = 1.5 * res\n        out = dataframe[(dataframe[column]<q1 - new) | (dataframe[column] >q3 + new)].index\n        ind.extend(out)\n    ind = Counter(ind)\n    val = list(s for s,i in ind.items() if i>n)\n    return val\n\n#Detecting outliers for the numerical features\nott = detect(df_train,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])","9392383d":"#Our final outliers - can have a harmful effect on regression tasks\ndf_train.loc[ott]","173955ec":"df_train.drop(ott,axis=0,inplace=True) #dropping on the row axis","b78cf28d":"#Dataset combining both the training and testing datasets\nt_len = len(df_train)\nfinal = pd.concat([df_train,df_test],axis=0).reset_index(drop=True)\nfinal.shape","348c8ad3":"df_train.head()","df4cf609":"df_train.columns","c84a8e56":"df_test.head()","d291aef6":"df_train.info()\ndf_train.isnull().sum()","da2c90e6":"df_test.info()\ndf_test.isnull().sum()","d130f24f":"#Brief statistics\ndf_train.describe()","7cee42d7":"df_test.describe()","2c085da5":"#Correlation between numerical values\n\nhm = sns.heatmap(df_train[['Survived','SibSp','Parch','Age','Fare']].corr(),annot=True,cmap='Blues')","35448049":"df_train[['Sex','Survived']].groupby(['Sex'],as_index=False).mean().sort_values(by='Survived',ascending=False)","8aa9c538":"#Plot for gender distribition of survival rate\nsns.barplot(x='Sex',y='Survived',data=df_train)\nplt.title('Survival %: Gender distribution')\nplt.ylabel('% of passengers survived')\nplt.xlabel('Sex')","8851f535":"#Survivor count for each gender\nm = sns.countplot(x='Sex',hue='Survived',data=df_train)\nplt.title('Survival count: Gender distribution')\nplt.xlabel('Sex')\nplt.ylabel('Number of passengers survived')\na = m.get_legend()\nb = a.texts\nb[0].set_text('No')\nb[1].set_text('Yes')","37b4c150":"#Age survival ratio\ns = sns.FacetGrid(df_train,col='Survived')\ns.map(plt.hist,'Age',bins=20)","d22f3398":"#Survival probability vs Class\nb = sns.barplot(x='Pclass',y='Survived',data=df_train)\nb.set_ylabel('Survival probability')\nb.set_xlabel('Passenger class')","ee9fbb33":"c = sns.barplot(x='Pclass',y='Fare',data=df_train)\nc.set_ylabel('Fare')\nc.set_xlabel('Pclass')","c878788d":"#Survival Probability vs Class and Sex\nb = sns.barplot(x='Pclass',y='Survived',hue='Sex',data=df_train)\nb.set_ylabel('Survival Probability')\nb.set_xlabel('Passenger class')","d351587b":"#Exploring Embarked vs Survival probability\nax = sns.barplot(x='Embarked',y='Survived',data=df_train)\nax.set_ylabel('Survival probability')\nax.set_xlabel('Embarked')","7367995f":"g1 = sns.FacetGrid(df_train,col='Survived',row='Pclass')\ng1.map(plt.hist,'Age',bins=20)","cb435f7b":"#Fare distribution\na1 = sns.kdeplot(df_train.loc[(df_train['Survived']==0),'Fare'],shade=True,label='Not Survived')\na1 = sns.kdeplot(df_train.loc[(df_train['Survived']==1),'Fare'],shade=True,label='Survived')\nplt.ylabel('Survival frequency')\nplt.xlabel('Fare distribution')","255accd9":"#Survival by Fare,Age and Sex\ns = sns.FacetGrid(df_train,size=5,hue='Survived',col='Sex')\ns.map(plt.scatter,'Fare','Age',edgecolor='w').add_legend()","3a6aa7df":"#Plotting fare in the combined dataset\ns = sns.distplot(final['Fare'])","7e90b383":"final['log_Fare'] = final['Fare'].map(lambda n: np.log(n) if n>0 else 0)","2d3c95f4":"#Log mapped fare values\nsns.distplot(final['log_Fare'])","55beb2f1":"final.isnull().sum()","df8cb641":"#As we have only 1 null value, we can just fill it up with the median value taking into account the rest of the features\nfare=final.loc[(final['Embarked'] == \"S\") & (final['Pclass'] == 3), 'Fare'].median()\nfinal['Fare']=final['Fare'].fillna(fare)","07d41f31":"final[final['Embarked'].isnull()]","0f9c3af6":"sns.barplot(x='Embarked',y='Fare',data=final)","b3fe2b26":"#Replacing the NULL values with C\nfinal['Embarked']=final['Embarked'].fillna('C')\nfinal[final['Embarked'].isnull()]","ece8a4eb":"final['Cabin'].isnull().sum()","250a23b5":"final['Cabin'].describe()","7b5037a2":"final['Cabin'].unique()","8f4cccf1":"final['Cabin'] = final['Cabin'].fillna('O')","3bea1d5a":"final['Cabin'] = pd.Series(a[0] for a in final['Cabin'])","c119176d":"sns.countplot(final['Cabin'],order=['A','B','C','D','E','F','G','T','O'])","39d56901":"final = pd.get_dummies(final,columns=['Cabin'],prefix='Cabin')","2ddc5936":"#Percentage of values missing in the whole dataset\nprint('% of missing age data',(final.Age.isnull().sum()\/len(final.Age)))","0fd4cca2":"from sklearn.ensemble import RandomForestRegressor\n\nmini = final[['Age','Sex','Pclass','Parch','SibSp']]\nmini = pd.get_dummies(mini)\ntrain = mini[mini.Age.notnull()].to_numpy()\ntest = mini[mini.Age.isnull()].to_numpy()\ny = train[:,0]\nx = train[:,1:]\nclf = RandomForestRegressor(n_estimators=150,n_jobs=-1,random_state=0)\nclf.fit(x,y)\npredicts = clf.predict(test[:,1:])\nfinal.loc[(final.Age.isnull()),'Age'] = predicts","c9c71d56":"final.Age.isnull().sum()","6669c861":"#Each name has a title associated with it - Could give important information\nfinal['Name']","ebcab815":"final_title = [a for i in final['Name'] for a in i.split(' ') if a.endswith('.')]\nfinal['prefix'] = pd.Series(final_title)\nfinal['prefix'].head()","535b256d":"g = sns.countplot(x='prefix',data=final)\ng = plt.setp(g.get_xticklabels(),rotation=45)","f98aa12a":"final['prefix'].unique()","06e69058":"name_mapping = {\n    \"Capt.\": \"Officer\",\n    \"Col.\": \"Officer\",\n    \"Major.\": \"Officer\",\n    \"Jonkheer.\": \"Master\",\n    \"Don.\": \"Royalty\",\n    \"Sir.\" : \"Royalty\",\n    \"Dr.\": \"Officer\",\n    \"Rev.\": \"Officer\",\n    \"the Countess.\":\"Royalty\",\n    \"Mme.\": \"Mrs\",\n    \"Mlle.\": \"Miss\",\n    \"Ms.\": \"Mrs\",\n    \"Mr.\" : \"Mr\",\n    \"Mrs.\" : \"Mrs\",\n    \"Miss.\" : \"Miss\",\n    \"Master.\" : \"Master\",\n    \"Lady.\" : \"Royalty\",\n    \"Dona.\" : \"Royalty\" \n}\nfinal['prefix']=final['prefix'].map(name_mapping)\nfinal['prefix']","5049177e":"g = sns.countplot(final['prefix'])","f1f8d8d2":"final.drop(['Name'],axis=1,inplace=True)","410d0b6f":"final['family'] = final['SibSp'] + final['Parch'] + 1\ng = sns.lineplot(x='family',y='Survived',data=final)","c167ebf2":"final['Single'] = final['family'].map(lambda s: 1 if s==1 else 0)\nfinal['small'] = final['family'].map(lambda s:1 if s==2 else 0)\nfinal['medium'] = final['family'].map(lambda s:1 if 3<=s<=4 else 0)\nfinal['large'] = final['family'].map(lambda s:1 if s>=5 else 0)","160f3068":"fig, (ax1, ax2, ax3, ax4) = plt.subplots(4,1,figsize=(8,16))\n\np = sns.barplot(x='Single',y='Survived',data=final,ax=ax1)\nq = sns.barplot(x='small',y='Survived',data=final,ax=ax2)\nr = sns.barplot(x='medium',y='Survived',data=final,ax=ax3)\ns = sns.barplot(x='large',y='Survived',data=final,ax=ax4)","cc1567b6":"final['Ticket'].head()","d945bb77":"#Extracting the prefix of the tickets\nTicket = []\nfor i in list(final.Ticket):\n    if not i.isdigit() :\n        Ticket.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(' ')[0]) #Take prefix\n    else:\n        Ticket.append(\"X\")\n\nfinal['Ticket'] = Ticket\nfinal['Ticket'].head()","5adb9a28":"final.drop(['Fare'],axis=1,inplace=True)","26911388":"final = pd.get_dummies(final,columns=['prefix'])\nfinal = pd.get_dummies(final,columns=['Embarked'],prefix='Em')","b2577fb2":"final = pd.get_dummies(final,columns=['Ticket'],prefix='T')","73dc1276":"final['Pclass'] = final['Pclass'].astype('category')\nfinal = pd.get_dummies(final,columns=['Pclass'],prefix='Pc')","131c1cde":"#Map sex to numerical variables\nfinal[\"Sex\"] = final[\"Sex\"].map({\"male\": 0, \"female\":1})","e0dba71c":"modelling = final.drop(['PassengerId'],axis=1)","7ed1a5a3":"training = modelling[:t_len]\ntest = modelling[t_len:]\ntest.drop(['Survived'],axis=1,inplace=True)","d5a0c18e":"training['Survived'] = training['Survived'].astype(int)\ny_train = training['Survived']\nx_train = training.drop(['Survived'],axis=1)","95028213":"x_train.head()","88e2d19b":"y_train.shape","0af50038":"kfold = StratifiedKFold(n_splits=10)","946c4e00":"random_state = 0\nmodels = []\nmodels.append(SVC(random_state=random_state))\nmodels.append(DecisionTreeClassifier(random_state=random_state))\nmodels.append(AdaBoostClassifier(random_state=random_state))\nmodels.append(RandomForestClassifier(random_state=random_state))\nmodels.append(ExtraTreesClassifier(random_state=random_state))\nmodels.append(GradientBoostingClassifier(random_state=random_state))\nmodels.append(MLPClassifier(random_state=random_state))\nmodels.append(KNeighborsClassifier())\nmodels.append(LogisticRegression(random_state = random_state))\nmodels.append(XGBClassifier(random_state = random_state))","b00c727d":"crossval = []\nfor classifier in models:\n    crossval.append(cross_val_score(classifier,x_train,y=y_train,scoring='accuracy',cv=kfold))\ncv_mean = []\ncv_std = []\nfor res in crossval:\n    cv_mean.append(res.mean())\n    cv_std.append(res.std())\n\ncross_val_df = pd.DataFrame({'CVMean':cv_mean,'CVErrors':cv_std,'Algorithm':[\"SVC\",\"DecisionTree\",\"AdaBoost\",\"RandomForest\",\"ExtraTrees\",\"GradientBoosting\",\"MultipleLayerPerceptron\",\"KNeighbors\",\"LogisticRegression\",\"XGBClassifier\"]})\nax = sns.barplot('CVMean','Algorithm',data=cross_val_df,**{'xerr':cv_std})\nax.set_xlabel('Average Accuracy')\nax = ax.set_title(\"CV Scores\")","35f5856f":"gbc = GradientBoostingClassifier()\nparam_grid = {'loss':['exponential'],\n             'n_estimators':[150,200,250],\n             'learning_rate':[0.1,0.05,0.01],\n             'max_depth':[4,8,10],\n             'min_samples_leaf':[50,100],\n             'max_features':[0.5,0.4,0.3]\n             }\n\nGBC_cv = GridSearchCV(gbc,param_grid=param_grid,cv=kfold,scoring='accuracy',n_jobs=4,verbose=1)\nGBC_cv.fit(x_train,y_train)\nprint(GBC_cv.best_params_)\nGBC_b = GBC_cv.best_estimator_\nprint(GBC_cv.best_score_)","cb14733f":"gbc = GradientBoostingClassifier()\nparam_grid = {'loss':['exponential'],\n             'n_estimators':[150,200,250],\n             'min_samples_leaf':[50,100],\n             'max_features':[0.5,0.3]\n             }\n\nGBC_cv = GridSearchCV(gbc,param_grid=param_grid,cv=4,scoring='accuracy',n_jobs=4,verbose=1)\nGBC_cv.fit(x_train,y_train)\nprint(GBC_cv.best_params_)\nGBC_b = GBC_cv.best_estimator_\nprint(GBC_cv.best_score_)","248e3c1a":"pd.DataFrame(GBC_cv.cv_results_)","6d90b446":"xgb = XGBClassifier()\nparam_grid = {\n    'min_child_weight': [1, 5, 10],\n    'gamma': [0.5, 1, 1.5, 2, 5],\n    'subsample': [0.6, 0.8, 1.0],\n    'colsample_bytree': [0.6, 0.8, 1.0],\n    'max_depth': [3, 4, 5]\n}\nxgb_cv = GridSearchCV(xgb,param_grid=param_grid,cv=kfold,scoring='accuracy',n_jobs=4,verbose=1)\nxgb_cv.fit(x_train,y_train)\nprint(xgb_cv.best_params_)\nxgb_b = xgb_cv.best_estimator_","2a14b9bc":"lr = LogisticRegression()\nparam_grid = {\n    'penalty' : ['l1', 'l2'],\n    'C' : np.logspace(-4, 4, 30)\n}\nlr_cv = GridSearchCV(lr,param_grid=param_grid,cv=kfold,scoring='accuracy',n_jobs=4,verbose=1)\nlr_cv.fit(x_train,y_train)\nprint(lr_cv.best_params_)\nlr_b = lr_cv.best_estimator_","f5d86033":"# rf = RandomForestClassifier()\n\n# param_grid = {\n#     \"max_depth\": [None],\n#     \"max_features\": [1,3,5,710],\n#     \"min_samples_split\": [2,3,5,7,10],\n#     \"min_samples_leaf\": [1,2,3,5,10],\n#     \"bootstrap\": [False],\n#     \"n_estimators\" :[100,150,200,300],\n#     \"criterion\": [\"gini\"]\n# }\n# rf_cv = GridSearchCV(rf,param_grid=param_grid,cv=kfold,scoring='accuracy',n_jobs=4,verbose=1)\n# rf_cv.fit(x_train,y_train)\n# print(rf_cv.best_params_)\n# rf_b = rf_cv.best_estimator_\n# print(rf_cv.best_score_)\n\nmp = MLPClassifier()\nparam_grid = {'solver': ['lbfgs'], 'max_iter': [500,1000,1500], 'alpha': 10.0 ** -np.arange(1, 7), 'hidden_layer_sizes':np.arange(5, 12)}\nmp_cv = GridSearchCV(mp,param_grid=param_grid,cv=kfold,scoring='accuracy',n_jobs=4,verbose=1)\nmp_cv.fit(x_train,y_train)\nprint(mp_cv.best_params_)\nmp_b = mp_cv.best_estimator_\nprint(mp_cv.best_score_)","5c08d8b9":"ada = AdaBoostClassifier()\nada.fit(x_train,y_train)\nl1 = np.argsort(ada.feature_importances_)[::-1][:40]\nplt.figure(figsize=(15,10))\ng1 = sns.barplot(y=x_train.columns[l1][:40],x=ada.feature_importances_[l1][:40],orient='h')\ng1.set_xlabel('Relative importance')\ng1.set_ylabel('Features')\ng1.set_title('Adaboost Feature importance')","f13f4f3b":"etc = ExtraTreesClassifier()\netc.fit(x_train,y_train)\ne1 = np.argsort(etc.feature_importances_)[::-1][:40]\nplt.figure(figsize=(15,10))\ng2 = sns.barplot(y=x_train.columns[e1][:40],x=etc.feature_importances_[e1][:40],orient='h')\ng2.set_xlabel('Relative importance')\ng2.set_ylabel('Features')\ng2.set_title('ExtraTreesClassifier Feature importance')","3c5a7e7a":"rf_b = RandomForestClassifier()\nrf_b.fit(x_train,y_train)\nf1 = np.argsort(rf_b.feature_importances_)[::-1][:40]\nplt.figure(figsize=(15,10))\ng3 = sns.barplot(y=x_train.columns[f1][:40],x=rf_b.feature_importances_[f1][:40],orient='h')\ng3.set_xlabel('Relative importance')\ng3.set_ylabel('Features')\ng3.set_title('Random forest Feature importance')","a3c73729":"z1 = np.argsort(GBC_b.feature_importances_)[::-1][:40]\nplt.figure(figsize=(15,10))\ng4 = sns.barplot(y=x_train.columns[z1][:40],x=GBC_b.feature_importances_[z1][:40],orient='h')\ng4.set_xlabel('Relative importance')\ng4.set_ylabel('Features')\ng4.set_title('Random forest Feature importance')","69559d6e":"ts_gbc = pd.Series(GBC_b.predict(test),name='GB')\nts_xgb = pd.Series(xgb_b.predict(test),name='XGB')\nts_lr = pd.Series(lr_b.predict(test),name='LR')\nts_mp = pd.Series(mp_b.predict(test),name='mp_b')\nensembled = pd.concat([ts_gbc,ts_xgb,ts_lr,ts_mp],axis=1)\nhm = sns.heatmap(ensembled.corr(),annot=True)","09698d72":"vc_soft = VotingClassifier(estimators=[('XBG',xgb_b),('GB',GBC_b),('LR',lr_b),('MP',mp_b)],voting='hard',n_jobs=4)\nvc_soft = vc_soft.fit(x_train,y_train)","2ebba716":"result = pd.Series(vc_soft.predict(test),name='Survived')\nresults = pd.concat([id_t,result],axis=1)\nresults.to_csv('mp_hard_voting.csv',index=False)","0b879d80":"results.shape","83c57bc1":"Many of the titles are quite similar in nature and we can group them up to prevent giving rise to further features while one-hot encoding the whole dataset.","fe5292b4":"We observe a similar trend here where the survival probability is in direct correlation with the gender as well as the Passenger class. Higher the passenger class equalled low fare which results in low survival chance.","adeb07a8":"<h3>Family size<\/h3>","b1ccb106":"# Data cleaning","e818a79e":"Analyzing the ticket column","927ffd61":"List of columns present in the dataset","b14a094f":"<h3>Age<\/h3>","2a9a8f27":"We see that the fare values of C are nearest to 80, therefore we fill the given columns with 'C' embarked","9c53df5b":"The describe method provides us a brief summary of basic statistical measures such as quartiles, mean, count etc.","20ff05af":"Again, we see that there is a large difference between the number of survivors between both of the genders.","82df22b9":"<h3>Ticket Prefix<\/h3>","6d2cbd51":"It seems that the rich people present in Passenger Class 1 had a much higher probability of survival in comparison to the other passenger classes. The fare is proportional to the survivability result.","bc87a7cf":"<h3>Embarked<\/h3>\n\n**We will first deal with the Embarked column with 2 NULL values**","e54cee34":"These rows were identified as outliers according to our given function so we will proceed to drop them from our dataframe","8d4af548":"<h3>Cabin<\/h3>","956d1c05":"We'll first read the csv(comma separated values) files into a training and testing dataframes which we can manipulate for analysis and predictions","7c8ba514":"There is a much higher probability of females surviving on the Titanic than men, Let us now proceed to visualize it.","4147e99d":"Visualizing the different titles present on the ship","be05c163":"Checking the datatype of the respective columns ","e2045018":"# EDA (exploratory data analysis)","c1e6aba1":"Here, we achieve high performance with logistic regression, XGBClassifier, Gradient Boosting and Random forests.\nWe will choose these models for our voting classifier","5ee497df":"# Feature modelling","423ec150":"<h3>Prefix\/Title<\/h3>","05d76fbf":"Going over the cabin column for NULL values","a43c936e":"All the missing values have Sex: Female, Pclass: 1 and fare: 80, let us look at the relation between Fare and embarked","d0da7f6b":"Age is one of the key factors while dealing with survival rates. We cannot simply impute it with other values. We can make a regressor that predicts the age values through the other numerical values","627bc73b":"# ML model","12f268f6":"We see that the fare distribution is very skewed: It can result in some weight differences in the model. ","1286a333":"# Introduction\n\nThis jupyter notebook will go over all the basic steps in-depth while given a certain machine learning problem.\n\n* Aquiring Data and analyzing the dataset\n* EDA - Exploratory data analysis\n* Data cleaning \n* Feature tuning\n* Basic Modeling and hyperparameter optimization\n* Voting classifiers and optimized models","f8c3ab2e":"**Building a random forest regressor to predict age through the other features**","3e25ca18":"<h3>One hot encoding remaining columns<\/h3>","eca1a07a":"To fill the NULL values, we observe that not everyone as a passenger is given a cabin while on a ship.The titanic had a very huge passenger count and it probably wasn't possible to assign each and every one a separate cabin, therefore we can just give it a value 'O' to define no cabin associated.","1a727f9d":"It seems that the middle-aged and old people had a very low survival probability as compared to infants, childrens and young adults. Age plays an important role in saving yourself during a time of crisis.","3f01203f":"Converting family to categorical variables:","378c175e":"**Family size**: New feature containing the siblings and parents","4ec3c967":"1. **Dealing with NULL values in the respective columns**","e1f3fed8":"We proceed to check the unique values for the Cabin column","56dec9e6":"We see that there's high correlation only between fare and survival probabilities, this feature may turn out to be our main focus","58fbe663":"We can clearly see that gender plays a huge role in calculating the survival probability for a passenger. This is a crucial feature that would help to improve upon our predictions.\n\nWe will now visualize the total passenger count according to gender and survivability.","fd46db68":"We see that Pclass 1 consists of more middle-aged and old men while P-class 2 and 3 consist of infants, children and the like. It is a similar occurence in real-life where usually old and rich people travel in First-Class with the presenece of children being very rare.\n\nWe also see that the survived histogram dwindles down in height as we move lower down the graph. The histogram still rises up pretty well for the lower age group which is what we saw above as well. Children, teenagers and young adults have a higher survival rate than the middle-aged and old people.","bd22813a":"# Fine tuning individual models","6c9cc22c":"**Outlier detection**","04d0b822":"# Exploring Feature importance of the classifiers","270fff52":"**Checking for NULL values**\n\nMultiple NULL values present in Age and Cabin while only 2 in the Embarked column.","3a8f95b9":"We will proceed to use one-hot encoding to further encode our other categorical variables. We use different prefix to separate out any similarities after encoding.","a7861707":"Checking the correlation among the numerical variables present in the dataset","01bfaa71":"Create a final dataset (used while feature tuning and cleaning) by combining the training and testing dataframe","094059d6":"# Dataset\n\nWe will be going over the kaggle titanic survival probability dataset \n\n**TASK** - Calculate the survival probability of a passenger list (given in test.csv)\n\n**Scoring** - Accuracy will be the scoring criteria","e10ce0a2":"# Analyzing the dataset","56c28973":"The title of a person may hold great value while calculating survival predictions. People of higher status may be rescued first and others with titles corresponding to armies or the crew end up in the other side of the spectrum.","b7603d93":"# Aquiring Data","5009a8db":"<h3>Fare<\/h3>","8fdb84c8":"Viewing the first 5 rows of the dataset","3e2b2826":"Outliers can have dramatic effects on our predictions and can also result in harming our final result. We find outliers for the numerical features in the given dataset and we outlined rows with atleast two outliers"}}