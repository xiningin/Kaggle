{"cell_type":{"30149efd":"code","5dd29490":"code","05c3402b":"code","27fd38c0":"code","15ffe034":"code","cd0d5878":"code","ffa4d650":"code","265dfff2":"code","e9cb286d":"code","c72af203":"code","1cda7c1d":"code","c5563aae":"code","87e5ae98":"code","009e3975":"code","4db816bc":"code","01095337":"code","eb743f46":"code","6e766d9c":"code","6d24484b":"code","84271624":"code","627c918c":"code","9fdca944":"code","ffcbabc9":"code","3d01018e":"code","ce16c702":"code","6b22b5d1":"code","5d1a265c":"code","5495a9c1":"code","a7fbaf7e":"code","6fd4e811":"code","607e006d":"code","489c8df7":"code","9033dc77":"code","e4aecefe":"code","7979ffbd":"code","b0c75ab6":"code","4b294ae8":"code","80905ea1":"code","cbce0098":"code","5f7f172f":"code","62d91962":"code","b398e6cf":"code","b31480e2":"code","0e8b8cb0":"code","28300430":"code","3d6114ee":"code","6cf73a05":"code","f1c6c92d":"code","0482554c":"code","82247bf4":"code","3148bc58":"code","402706e2":"code","0e414a11":"code","bf74b1a0":"code","e5270d4c":"code","92ed7321":"code","08cf9c9e":"code","c0305f7b":"code","c036c86b":"code","c6a913a1":"code","43e5c685":"code","39d03b4d":"code","4bb8bc93":"code","8582a14f":"code","71d42ca6":"code","b0d1728d":"code","ef7611c6":"code","b1590c58":"code","de01925c":"code","517c675c":"code","c8aec65e":"code","2d64be51":"code","c1c55570":"code","cb770461":"code","bc5268de":"code","793d795d":"code","bd9fbe0c":"code","4967035f":"code","85d6d99c":"code","051e3908":"code","25a0c131":"code","5b748935":"code","26d4a0ef":"code","4d3b01a6":"code","7c1f004c":"code","1ebebfbf":"code","f5c3cd50":"code","48fdf4f3":"code","53a82e54":"code","76a737c4":"code","92154702":"code","4030c53a":"code","b60a629c":"code","65b3b84e":"code","a2ed301b":"code","18bad785":"code","da0c63f8":"code","e65d2e29":"code","49ef5625":"code","00aeb392":"code","b20a557e":"code","cc125dcd":"code","8fae0e94":"code","59e184c5":"code","cb83b1fa":"code","b4cbcbac":"code","6932be20":"code","f967de54":"code","938a0888":"code","0dd25802":"code","6b5e7ae0":"code","f260a421":"code","e11ec301":"code","56d88286":"code","2d7a017a":"code","9702c68c":"code","5414b09f":"code","7d94fbc5":"code","9caac03b":"code","2401720e":"code","fb127220":"code","4d6f5124":"code","49b8438f":"code","17126e7d":"code","db001750":"code","9adc9c9f":"code","e7c61788":"code","26408719":"code","102f8e13":"code","9ad27f96":"code","20e1da88":"code","ff6c2f72":"markdown","1dc5d702":"markdown","566a6b7e":"markdown","795c8ec4":"markdown","8c05554e":"markdown","a358c0d2":"markdown","087e3dad":"markdown","8f55d3be":"markdown","757053a7":"markdown","0968f5cf":"markdown","096c6b6d":"markdown","dc4b72e6":"markdown","4c8dee94":"markdown","18384e4a":"markdown","13ef6043":"markdown","4878aee8":"markdown","4456aeb0":"markdown","fb0f0e4b":"markdown","87567bc1":"markdown","426047da":"markdown","2b700814":"markdown","6564fdff":"markdown","5aa44e14":"markdown","d3699d35":"markdown","b346c206":"markdown","cba444d0":"markdown","1f93f373":"markdown","fcf1308b":"markdown","ef8e5de8":"markdown","b8982b27":"markdown","600caf89":"markdown","66cea3bd":"markdown","4759de5f":"markdown","f42cedad":"markdown","44e0e8b6":"markdown","1da054c4":"markdown","7f169e2c":"markdown"},"source":{"30149efd":"from __future__ import division\nimport numpy as np\nimport pandas as pd \nimport os\nimport timeit\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom tqdm import tqdm","5dd29490":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nfrom plotly import tools\n# link pandas and plotly\n# import cufflinks as cf\n\ninit_notebook_mode(connected=True)\n# cf.set_config_file(offline=True, world_readable=True, theme='ggplot')\n%matplotlib inline","05c3402b":"from matplotlib import rcParams\nrcParams['figure.figsize'] = 22, 14\nrcParams['axes.titlesize'] = 24\nrcParams['axes.labelsize'] = 20\nrcParams['xtick.labelsize'] = 16\nrcParams['ytick.labelsize'] = 16\nrcParams['legend.fontsize'] = 14","27fd38c0":"def sub_boxenplots(x, data, y=\"totals.transactionRevenue_ln\", rot=15):\n    order = data[x].unique()\n    fig, axes = plt.subplots(ncols=2, nrows=1, squeeze=False, figsize=(22, 8))\n    sns.boxenplot(x=x, y=y, data=data, ax=axes[0, 0], order=order)\n    axes[0, 0].set_title(\"All Instances\")\n    axes[0, 0].set_xticklabels(axes[0, 0].get_xticklabels(), rotation=rot, ha='right')\n    sns.boxenplot(x=x, y=y, data=data.loc[data[y] > 0, :], ax=axes[0, 1], order=order)\n    axes[0, 1].set_title(\"Instances With Non-zero Revenue\")\n    axes[0, 1].set_xticklabels(axes[0, 1].get_xticklabels(), rotation=rot, ha='right')\n    fig.tight_layout()","15ffe034":"prefix = \"..\/input\/ga-customer-revenue-prediction\/\"","cd0d5878":"import json\nfrom pandas.io.json import json_normalize\ndef load_df(csv_path='..\/input\/train.csv', nrows=None):\n    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n    \n    read_start = timeit.default_timer()\n    df = pd.read_csv(csv_path, \n                     converters={column: json.loads for column in JSON_COLUMNS}, \n                     dtype={'fullVisitorId': 'str'}, # Important!!\n                     nrows=nrows,\n                     parse_dates=['date']\n                    )\n    read_end = timeit.default_timer()\n    print(\"Finish reading {0}, time usage: {1}\".format(csv_path, read_end - read_start))\n    \n    process_start = timeit.default_timer()\n    for column in JSON_COLUMNS:\n        column_as_df = json_normalize(df[column])\n        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n    process_end = timeit.default_timer()\n    print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}, time usage: \", process_end - process_start)\n    return df","ffa4d650":"# Just for convinence, uncomment before submite the codes\nX_train_df = load_df(prefix + \"train.csv\")\nX_test_df = load_df(prefix + \"test.csv\")\n\nprint(\"Done!\")","265dfff2":"X_train_df.head(1)","e9cb286d":"submit_id = X_test_df[\"fullVisitorId\"].unique()","c72af203":"X_train_df.loc[:, \"totals.transactionRevenue_ln\"] = np.log1p(X_train_df[\"totals.transactionRevenue\"].fillna(0).astype(\"float\"))","1cda7c1d":"labels = ['Zero revenue instance', 'Non-zero revenue instance']\nvalues = [X_train_df.loc[X_train_df[\"totals.transactionRevenue_ln\"] == 0, \"totals.transactionRevenue_ln\"].count(),\n         X_train_df.loc[X_train_df[\"totals.transactionRevenue_ln\"] != 0, \"totals.transactionRevenue_ln\"].count()]\ntrace = go.Pie(labels=labels, values=values)\nlayout = dict(\n    title = 'Instance Revenue'\n)\n\nfig = dict(data=[trace], layout=layout)\niplot(fig, filename='basic_pie_chart')","c5563aae":"sum_target_series = X_train_df.groupby(\"fullVisitorId\")[\"totals.transactionRevenue_ln\"].sum()\n\nlabels = ['Zero revenue customer', 'Non-zero customer']\nvalues = [sum_target_series[sum_target_series == 0].count(),\n         sum_target_series[sum_target_series != 0].count()]\ntrace = go.Pie(labels=labels, values=values)\nlayout = dict(\n    title = 'Unique Customer Revenue'\n)\n\nfig = dict(data=[trace], layout=layout)\niplot(fig, filename='basic_pie_chart')","87e5ae98":"trace = go.Box(y=X_train_df.loc[X_train_df[\"totals.transactionRevenue_ln\"] > 0, \"totals.transactionRevenue_ln\"],\n              name='Instance')\nlayout = dict(\n    title = 'Instance With Revenue Greater Than Zero'\n)\n\nfig = dict(data=[trace], layout=layout)\niplot(fig, filename='basic_pie_chart')","009e3975":"trace = go.Box(y=sum_target_series[sum_target_series > 0], name=\"Unique User\")\niplot([trace])","4db816bc":"to_drop_cols = [c for c in X_train_df.columns if X_train_df[c].nunique(dropna=False)==1 ]\n\nX_train_df.drop(to_drop_cols, axis=1, inplace=True)\nX_test_df.drop(to_drop_cols, axis=1, inplace=True)","01095337":"print(\"Training set shape: \", X_train_df.shape, \" Testing set shape: \", X_test_df.shape)\nprint(\"Difference features between two sets:\")\nfor col in X_train_df.columns:\n    if col not in X_test_df:\n        print(col)","eb743f46":"X_train_df[\"trafficSource.campaignCode\"].unique()","6e766d9c":"X_train_df.drop(\"trafficSource.campaignCode\", axis=1, inplace=True)\n\nX_test_df = pd.concat([X_test_df, \n                       pd.Series(np.nan, name='totals.transactionRevenue'), \n                       pd.Series(np.nan, name='totals.transactionRevenue_ln')], \n                      axis=1)\n\nX_all_df = pd.concat([X_train_df, X_test_df], ignore_index=True, sort=False)","6d24484b":"X_all_df.drop(['sessionId', 'visitId'], axis=1, inplace=True)","84271624":"obj_cols = [column for column in X_all_df.columns if X_all_df[column].dtype == object]","627c918c":"for col in obj_cols:\n    try:\n        X_all_df.loc[:, col] = pd.to_numeric(X_all_df.loc[:, col], errors='raise')\n#         print(\"Successfully parse column: \", col)\n    except ValueError as e:\n        # mute the string column that cannot be converted to numeric values.\n        pass\n#         print(\"Unable to parse column: \", col)","9fdca944":"for col in X_all_df.select_dtypes(include='object'):\n    if col == \"fullVisitorId\":\n        continue\n    try:\n        X_all_df.loc[:, col] = pd.Categorical(X_all_df.loc[:, col])\n#         print(\"Successfully parse column: \", col)\n    except ValueError as e:\n        print(\"Unable to parse column: \", col)","ffcbabc9":"bool_cols = []\nfor col in X_all_df:\n    if X_all_df[col].nunique() == 2:\n        bool_cols.append(col)\nX_all_df.loc[:, bool_cols] = X_all_df.loc[:, bool_cols].astype('bool')","3d01018e":"def investigate(col_name, data):\n    print(\"The number of unique category\")\n    print(col_name, \": \", data[col_name].nunique())\n    print(\"*\" * 20)\n    print(\"Value count\")\n    print(data[col_name].value_counts())","ce16c702":"def ratio_nan(data):\n    num_data = X_all_df.shape[0]\n    null_sum = data.isnull().sum()\n    null_val_features = null_sum[null_sum > 0]\n    if \"totals.transactionRevenue_ln\" in null_val_features:\n        null_val_features.drop([\"totals.transactionRevenue\", \"totals.transactionRevenue_ln\"], inplace=True)\n    print(null_val_features\/ num_data)","6b22b5d1":"print(\"The features with NaN value percentage: \")\nratio_nan(X_all_df)","5d1a265c":"X_all_df.loc[:, \"totals.bounces\"].fillna(0, inplace=True)\nX_all_df.loc[:, \"totals.bounces\"] = X_all_df.loc[:, \"totals.bounces\"].astype('bool')","5495a9c1":"sub_boxenplots(data=X_all_df.loc[X_all_df[\"totals.transactionRevenue_ln\"].notnull(), :], x=\"totals.bounces\")","a7fbaf7e":"X_all_df.drop(X_all_df.loc[X_all_df[\"totals.bounces\"], \"fullVisitorId\"].index, axis=0, inplace=True)\n\n# We don't need this feature anymore\nX_all_df.drop(\"totals.bounces\", axis=1, inplace=True)\n\nX_all_df.reset_index(drop=True, inplace=True)","6fd4e811":"X_all_df.loc[:, \"totals.newVisits\"].fillna(0, inplace=True)\nX_all_df.loc[:, \"totals.newVisits\"] = X_all_df.loc[:, \"totals.newVisits\"].astype('bool')","607e006d":"X_all_df.loc[:, \"totals.pageviews\"].fillna(0, inplace=True)","489c8df7":"def plot_cmp_stack(data, col_name):\n    \n    total_null_num = data[col_name].isnull().sum()\n    total_not_null_num = data[col_name].notnull().sum()\n    \n    null_zero_num =  (data.loc[data[col_name].isnull(), \"totals.transactionRevenue_ln\"] == 0).sum()\n    null_non_zero_num = (data.loc[data[col_name].isnull(), \"totals.transactionRevenue_ln\"] != 0).sum()\n    not_null_zero_num = (data.loc[data[col_name].notnull(), \"totals.transactionRevenue_ln\"] == 0).sum()\n    not_null_non_zero_num = (data.loc[data[col_name].notnull(), \"totals.transactionRevenue_ln\"] != 0).sum()\n    \n    trace1 = go.Bar(\n        x=['NULL', 'Not NULL'],\n        y=[null_zero_num\/ total_null_num, not_null_zero_num\/ total_not_null_num],\n        name='Instances With Zero Revenue',\n        marker=dict(\n            color='rgb(158,202,225)',\n            line=dict(\n                color='rgb(8,48,107)',\n                width=1.5),\n        ),\n        opacity=0.6\n    )\n    \n    trace2 = go.Bar(\n        x=['NULL', 'Not NULL'],\n        y=[null_non_zero_num\/ total_null_num, not_null_non_zero_num\/ total_not_null_num],\n        name='Instances With Non-Zero Revenue',\n        marker=dict(\n            color='rgb(58,200,225)',\n            line=dict(\n                color='rgb(8,48,107)',\n                width=1.5),\n            ),\n        opacity=0.6        \n    )\n    \n    trace3 = go.Pie(\n        labels = ['Ratio Of Non-zero Revenue With This Feature Is Null', 'Ratio Of Non-zero Revenue With This Feature Is Not NULL'],\n        values = [null_non_zero_num, not_null_non_zero_num],\n        domain = {\"x\": [0.5, 1]},\n        hole = .4,\n        name = 'Instances With Non-zero Revenue',\n        text = ['Non-zero Ratio']\n    )\n    \n    data=[trace1, trace2, trace3]\n    \n    layout=go.Layout(\n        barmode='stack',\n        title = col_name + \": Revenue Comparison Between NULL and Not NULL Value\",\n        yaxis = {'title': 'Percentage'},\n        xaxis = {\n            'domain': [0, 0.5]\n        },\n        showlegend=False\n    )\n    \n    fig = go.Figure(data=data, layout=layout)\n    iplot(fig, filename=col_name + '_stack_bar')","9033dc77":"plot_cmp_stack(data=X_train_df, col_name=\"trafficSource.adwordsClickInfo.gclId\")","e4aecefe":"plot_cmp_stack(data=X_train_df, col_name=\"trafficSource.isTrueDirect\")","7979ffbd":"plot_cmp_stack(data=X_train_df, col_name=\"trafficSource.keyword\")","b0c75ab6":"plot_cmp_stack(data=X_train_df, col_name=\"trafficSource.referralPath\")","4b294ae8":"to_drop_cols = [\"trafficSource.adContent\", \"trafficSource.adwordsClickInfo.adNetworkType\", \n                \"trafficSource.adwordsClickInfo.gclId\", \"trafficSource.adwordsClickInfo.isVideoAd\",\n                \"trafficSource.adwordsClickInfo.page\", \"trafficSource.adwordsClickInfo.slot\"]\n\nX_all_df.drop(to_drop_cols, axis=1, inplace=True)\n\nX_all_df.loc[:, \"trafficSource.isTrueDirect\"] = X_all_df.loc[:, \"trafficSource.isTrueDirect\"].fillna(0).astype('bool')","80905ea1":"ratio_nan(X_all_df)","cbce0098":"X_all_df.loc[X_all_df[\"fullVisitorId\"] == \"0824839726118485274\", \"visitNumber\"].sort_values().head(3)","5f7f172f":"X_all_df.groupby(\"fullVisitorId\")[\"visitNumber\"].min().unique()","62d91962":"X_all_df.loc[X_all_df[\"channelGrouping\"] == \"Direct\", \"trafficSource.isTrueDirect\"].value_counts()","b398e6cf":"X_all_df.loc[X_all_df[\"trafficSource.isTrueDirect\"] == 1, \"channelGrouping\"].value_counts()","b31480e2":"X_all_df.loc[X_all_df[\"trafficSource.isTrueDirect\"] == 0, \"channelGrouping\"].value_counts()","0e8b8cb0":"X_all_df.loc[X_all_df[\"trafficSource.isTrueDirect\"] == 1, \"trafficSource.medium\"].value_counts()","28300430":"X_all_df.loc[X_all_df[\"trafficSource.isTrueDirect\"] == 0, \"trafficSource.medium\"].value_counts()","3d6114ee":"temp_df = X_all_df.loc[X_all_df[\"trafficSource.isTrueDirect\"] == 1, :]","6cf73a05":"temp_df.loc[ temp_df[\"trafficSource.medium\"] == 'referral', 'channelGrouping'].value_counts()","f1c6c92d":"X_all_df.loc[X_all_df[\"trafficSource.keyword\"].notnull(), \"channelGrouping\"].value_counts()","0482554c":"X_all_df.loc[X_all_df[\"trafficSource.keyword\"].isnull(), \"channelGrouping\"].value_counts()","82247bf4":"temp_df = X_all_df.loc[X_all_df[\"trafficSource.keyword\"].isnull(), :]\ntemp_df.loc[(temp_df[\"channelGrouping\"] == \"Organic Search\") | (temp_df[\"channelGrouping\"] == \"Paid Search\"), \"trafficSource.medium\"].value_counts()","3148bc58":"X_all_df.loc[temp_df.index, \"trafficSource.keyword\"] = \"(not provided)\"","402706e2":"X_all_df.loc[X_all_df[\"trafficSource.referralPath\"].isnull(), \"channelGrouping\"].value_counts()","0e414a11":"temp_df = X_all_df.loc[X_all_df[\"trafficSource.referralPath\"].isnull(), :]","bf74b1a0":"X_all_df.loc[temp_df.loc[(temp_df[\"channelGrouping\"] == \"Referral\") | (temp_df[\"channelGrouping\"] == \"Social\")].index, \"trafficSource.referralPath\"] = X_all_df[\"trafficSource.referralPath\"].value_counts().index[0]","e5270d4c":"X_all_df.loc[X_all_df[\"trafficSource.source\"] == \"(direct)\", \"channelGrouping\"].value_counts()","92ed7321":"X_all_df.loc[X_all_df[\"trafficSource.source\"] == \"(direct)\", \"channelGrouping\"] = \"Direct\"","08cf9c9e":"X_all_df[\"trafficSource.referralPath\"] = X_all_df[\"trafficSource.referralPath\"].cat.add_categories(['Not Referral'])\n\nX_all_df.loc[:, \"trafficSource.referralPath\"] = X_all_df.loc[:, \"trafficSource.referralPath\"].fillna('Not Referral')","c0305f7b":"def percentage_counts(data, col_name):\n    total = data[col_name].count()\n    return data[col_name].value_counts()\/ total * 100","c036c86b":"def keep_greater_than_percentage_entries(data, col_name, percentage=0.1, new_entry=\"Others\"):\n    percentage_counts_series = percentage_counts(temp_df, col_name)\n    entries_to_keep = percentage_counts_series[percentage_counts_series >= percentage].index\n    data[col_name] = data[col_name].cat.add_categories([new_entry])\n    data.loc[~data[col_name].isin(entries_to_keep), col_name] = new_entry\n    data[col_name] = data[col_name].cat.remove_unused_categories()","c6a913a1":"features_to_merge = [\n    'device.browser', 'geoNetwork.city', 'geoNetwork.country',\n    'geoNetwork.metro', 'geoNetwork.networkDomain', 'geoNetwork.region',\n    'geoNetwork.subContinent', 'trafficSource.campaign', 'trafficSource.keyword',\n    'trafficSource.referralPath', 'trafficSource.source', 'device.operatingSystem'\n                    ]","43e5c685":"for feature in features_to_merge:\n    keep_greater_than_percentage_entries(data=X_all_df, col_name=feature)","39d03b4d":"X_train_df = X_all_df.loc[X_all_df[\"totals.transactionRevenue_ln\"].notnull(), :].reset_index(drop=True)\nX_test_df = X_all_df.loc[X_all_df[\"totals.transactionRevenue_ln\"].isnull(), :].reset_index(drop=True)","4bb8bc93":"X_train_df.groupby(\"date\")[\"totals.transactionRevenue_ln\"].agg(['sum', 'count', 'mean']).plot(subplots=True, sharex=True, title=\"Revenue Base On Date\", linewidth=2)","8582a14f":"sub_boxenplots(x='channelGrouping', data=X_train_df)","71d42ca6":"sub_boxenplots(x='device.deviceCategory', data=X_train_df)","b0d1728d":"sub_boxenplots(x='device.isMobile', data=X_train_df)","ef7611c6":"sub_boxenplots(x='geoNetwork.continent', data=X_train_df)","b1590c58":"sub_boxenplots(x='totals.newVisits', data=X_train_df)","de01925c":"sub_boxenplots(x='trafficSource.isTrueDirect', data=X_train_df)","517c675c":"sub_boxenplots(x='trafficSource.campaign', data=X_train_df, rot=20)","c8aec65e":"sub_boxenplots(x='trafficSource.medium', data=X_train_df)","2d64be51":"sub_boxenplots(x='device.operatingSystem', data=X_train_df, rot=20)","c1c55570":"g = sns.jointplot('totals.hits', 'totals.transactionRevenue_ln', data=X_train_df[X_train_df['totals.transactionRevenue_ln'] > 0],\n                 kind='reg', height=10)","cb770461":"g = sns.jointplot('visitStartTime', 'totals.transactionRevenue_ln', data=X_train_df[X_train_df['totals.transactionRevenue_ln'] > 0],\n                 kind='reg', height=10)","bc5268de":"g = sns.jointplot('totals.pageviews', 'totals.transactionRevenue_ln', data=X_train_df[X_train_df['totals.transactionRevenue_ln'] > 0],\n                 kind='reg', height=10)","793d795d":"cat_feature_list = ['channelGrouping', 'device.browser', 'device.deviceCategory', 'device.operatingSystem', \n                    'geoNetwork.city', 'geoNetwork.continent', 'geoNetwork.country', 'geoNetwork.metro', 'geoNetwork.networkDomain', 'geoNetwork.region', 'geoNetwork.subContinent',\n                    'trafficSource.campaign', 'trafficSource.isTrueDirect', 'trafficSource.keyword', 'trafficSource.medium', 'trafficSource.referralPath', 'trafficSource.source'\n                   ]\nbool_feature_list = ['device.isMobile', 'totals.newVisits']\nid_feature_list = ['fullVisitorId']\nnum_feature_list = ['visitNumber', 'totals.hits', 'totals.pageviews', 'visitStartTime']\ntime_feature_list = ['date']\nlabel_list = ['totals.transactionRevenue', 'totals.transactionRevenue_ln']","bd9fbe0c":"def check_features():\n    found = True\n    total_feature_list = id_feature_list + label_list + cat_feature_list + num_feature_list + bool_feature_list + time_feature_list\n    for feature in X_train_df.columns:\n        if feature not in total_feature_list:\n            found = False\n        assert found, \"You forgot \" + feature\n        total_feature_list.remove(feature)\n    if found:\n        if total_feature_list == []:\n            print(\"All the features are found!\")\n        else:\n            print(\"There are features left: \", total_feature_list)\ntry:\n    check_features()\nexcept AssertionError as e:\n    print(e)","4967035f":"X_all_df = X_all_df[id_feature_list + label_list + cat_feature_list + num_feature_list + bool_feature_list + time_feature_list]","85d6d99c":"from sklearn.preprocessing import LabelEncoder","051e3908":"def cat_feature_encoding(data, cat_feature_list):\n    feature_encoder_dict = {}\n    with tqdm(cat_feature_list, desc=cat_feature_list[0]) as t:\n        for cat_feature in t:\n            if cat_feature == \"fullVisitorId\": # leave the ID as it is \n                continue\n            t.set_description_str(cat_feature)\n            my_label_encoder = LabelEncoder()\n            encoded_col = my_label_encoder.fit_transform(data[cat_feature])\n            data.loc[:, cat_feature] = encoded_col\n            feature_encoder_dict[cat_feature] = my_label_encoder\n    return feature_encoder_dict","25a0c131":"X_train_df = X_all_df.loc[X_all_df[\"totals.transactionRevenue_ln\"].notnull(), :].reset_index(drop=True)\nX_test_df = X_all_df.loc[X_all_df[\"totals.transactionRevenue_ln\"].isnull(), :].reset_index(drop=True)","5b748935":"train_df = X_train_df\ntest_df = X_test_df","26d4a0ef":"def printShapes(name1, df1, name2, df2):\n    print(name1, df1.shape)\n    print(name2, df2.shape)\n    \ndef ratio_nan(data):\n    num_data = data.shape[0]\n    null_sum = data.isnull().sum()\n    null_val_features = null_sum[null_sum > 0]\n    print(null_val_features\/num_data)\n    \ndef compare_nan(name1, df1, name2, df2):\n    print(name1)\n    print(ratio_nan(df1))\n    print(\"\\n\" + name2)\n    print(ratio_nan(df2))\n    \nimport datetime as dt\ndef toPandasTimestamp(val):\n    lDate = dt.datetime.fromtimestamp(val)\n    lTimestamp = pd.Timestamp(lDate)\n    return lTimestamp\n    \ndef printTTShapes():\n    printShapes(\"Train Shape:\", train_df, \"Test Shape:\", test_df)","4d3b01a6":"train_df[\"visitStartTimestamp\"] = train_df[\"visitStartTime\"].apply(toPandasTimestamp)\ntest_df[\"visitStartTimestamp\"] = test_df[\"visitStartTime\"].apply(toPandasTimestamp)","7c1f004c":"printTTShapes()","1ebebfbf":"compare_nan(\"Train\", train_df, \"Test\", test_df)","f5c3cd50":"train_df[\"totals.transactionRevenue\"] = train_df[\"totals.transactionRevenue\"].apply(lambda x: 0.0 if np.isnan(x) else x) \ntest_df[\"totals.transactionRevenue\"] = test_df[\"totals.transactionRevenue\"].apply(lambda x: 0.0 if np.isnan(x) else x)","48fdf4f3":"columnsOfInterest = [\"fullVisitorId\", \"visitStartTime\", \"totals.hits\", \"totals.pageviews\", \"visitNumber\"]\ntrain_grouped = train_df[columnsOfInterest].groupby(\"fullVisitorId\")\ntest_grouped = test_df[columnsOfInterest].groupby(\"fullVisitorId\")","53a82e54":"train_ds = train_grouped[\"visitStartTime\"].max() - train_grouped[\"visitStartTime\"].min() # ds difference in seconds\ntest_ds = test_grouped[\"visitStartTime\"].max() - test_grouped[\"visitStartTime\"].min()\n\ntrain_sh = train_grouped[\"totals.hits\"].sum() # sh - summed hits\ntrain_spv = train_grouped[\"totals.pageviews\"].sum() # spv - summed page views\ntrain_lv = train_grouped[\"visitNumber\"].max() #lv - largestVisit number\ntest_sh = test_grouped[\"totals.hits\"].sum() # sh - summed hits\ntest_spv = test_grouped[\"totals.pageviews\"].sum() # spv - summed page views\ntest_lv = test_grouped[\"visitNumber\"].max() #lv - largestVisit number\n\ntrain_seconds_per_hit = train_ds \/ train_sh\ntrain_seconds_per_pageview = train_ds \/ train_spv \ntrain_seconds_per_visit = train_ds \/ train_lv\ntest_seconds_per_hit = test_ds \/ test_sh\ntest_seconds_per_pageview = test_ds \/ test_spv \ntest_seconds_per_visit = test_ds \/ test_lv\n\ntrain_nf = pd.concat([train_ds, train_seconds_per_hit, train_seconds_per_pageview, train_seconds_per_visit], \n                    axis = 1, join = \"outer\", \n                    join_axes = [train_ds.reset_index()[\"fullVisitorId\"]])\ntest_nf= pd.concat([test_ds, test_seconds_per_hit, test_seconds_per_pageview, test_seconds_per_visit], \n                    axis = 1, join = \"outer\", \n                    join_axes = [test_ds.reset_index()[\"fullVisitorId\"]])\n\ntrain_nf.reset_index(inplace = True)\ntrain_nf.columns = [\"fullVisitorId\", \"visitTimeRange\", \"secondsPerHit\", \"secondsPerPageview\", \"secondsPerVisit\"]\ntest_nf.reset_index(inplace = True)\ntest_nf.columns = [\"fullVisitorId\", \"visitTimeRange\", \"secondsPerHit\", \"secondsPerPageview\", \"secondsPerVisit\"]","76a737c4":"printShapes(\"Train New Features:\", train_nf, \"Test New Features:\", test_nf)","92154702":"compare_nan(\"Train New Features\", train_nf, \"Test New Features\", test_nf)","4030c53a":"train_nf = train_nf.apply(lambda x: x.fillna(0) if x.dtype.kind in 'biufc' else x.fillna('.'))\ntest_nf = test_nf.apply(lambda x: x.fillna(0) if x.dtype.kind in 'biufc' else x.fillna('.'))","b60a629c":"compare_nan(\"Train New Features\", train_nf, \"Test New Features\", test_nf)","65b3b84e":"train_nf.head()","a2ed301b":"test_nf.head()","18bad785":"fm_columns = [\"fullVisitorId\", \"totals.pageviews\", \"totals.hits\", \"visitStartTimestamp\", \"geoNetwork.city\"]","da0c63f8":"ftrain_df = train_df[fm_columns].reset_index()\nftest_df = test_df[fm_columns].reset_index()","e65d2e29":"ftrain_df.head()","49ef5625":"ftest_df.head()","00aeb392":"import featuretools as ft\nfrom featuretools import variable_types as vtype\n\nmy_variable_types = {\n    \"fullVisitorId\" : vtype.Id,\n    \"geoNetwork.city\" : vtype.Categorical\n}","b20a557e":"train_es = ft.EntitySet(id = \"train_data\")\ntest_es = ft.EntitySet(id = \"test_data\")","cc125dcd":"train_es.entity_from_dataframe(entity_id = \"train_log\",\n                               dataframe = ftrain_df,\n                               index = \"index\",\n                               time_index = \"visitStartTimestamp\",\n                               variable_types = my_variable_types)\n\ntest_es.entity_from_dataframe(entity_id = \"test_log\",\n                              dataframe = ftest_df,\n                              index = \"index\",\n                              time_index = \"visitStartTimestamp\",\n                              variable_types = my_variable_types)\n\nprint(train_es, test_es)","8fae0e94":"train_es.normalize_entity(base_entity_id = \"train_log\",\n                          new_entity_id = \"visitors\",\n                          index = \"fullVisitorId\")\n\ntest_es.normalize_entity(base_entity_id = \"test_log\",\n                         new_entity_id = \"visitors\",\n                         index = \"fullVisitorId\")\n\nprint(train_es, test_es)","59e184c5":"my_aggs = [\"sum\", \"max\", \"min\", \"mean\", \"std\", \"mode\"]\nmy_trans = [\"month\", \"day\"]\n\ntrain_features = ft.dfs(entityset = train_es,\n                        target_entity = \"visitors\",\n                        max_depth = 2,\n                        agg_primitives = my_aggs,\n                        trans_primitives = my_trans,\n                        verbose = 1,\n                        max_features = 100,\n                        n_jobs = 4,\n                        features_only = True)\n\ntest_features = ft.dfs(entityset = test_es,\n                        target_entity = \"visitors\",\n                        max_depth = 2,\n                        agg_primitives = my_aggs,\n                        trans_primitives = my_trans,\n                        verbose = 1,\n                        max_features = 100,\n                        n_jobs = 4,\n                        features_only = True)\n\nprint(\"We're generating %d features for train and %d for test\" % (len(train_features), len(test_features)))\nfor feat in train_features:\n    print(feat)","cb83b1fa":"train_feature_matrix, train_features = ft.dfs(entityset = train_es,\n                                              target_entity = \"visitors\",\n                                              max_depth = 2,\n                                              agg_primitives = my_aggs,\n                                              trans_primitives = my_trans,\n                                              verbose = 1,\n                                              max_features = 100,\n                                              n_jobs = 4)\n\ntest_feature_matrix, test_features = ft.dfs(entityset = test_es,\n                                            target_entity = \"visitors\",\n                                            max_depth = 2,\n                                            agg_primitives = my_aggs,\n                                            trans_primitives = my_trans,\n                                            verbose = 1,\n                                            max_features = 100,\n                                            n_jobs = 4)","b4cbcbac":"train_feature_matrix.head()","6932be20":"test_feature_matrix.head()","f967de54":"printShapes(\"Train Feature Matrix:\", train_feature_matrix,\n            \"Test Feature Matrix\", test_feature_matrix)","938a0888":"compare_nan(\"Train FM\", train_feature_matrix,\n            \"Test FM\", test_feature_matrix)","0dd25802":"train_feature_matrix = train_feature_matrix.apply(lambda x: x.fillna(0) if x.dtype.kind in 'biufc' else x.fillna('.'))\ntest_feature_matrix = test_feature_matrix.apply(lambda x: x.fillna(0) if x.dtype.kind in 'biufc' else x.fillna('.'))","6b5e7ae0":"compare_nan(\"Train FM\", train_feature_matrix,\n            \"Test FM\", test_feature_matrix)","f260a421":"train_feature_matrix.reset_index(inplace = True)\ntest_feature_matrix.reset_index(inplace = True)","e11ec301":"combined_train = train_nf.merge(train_feature_matrix, how = \"outer\", on = \"fullVisitorId\")\ncombined_test = test_nf.merge(test_feature_matrix, how = \"outer\", on = \"fullVisitorId\")","56d88286":"combined_train.head()","2d7a017a":"combined_test.head()","9702c68c":"compare_nan(\"CTrain\", combined_train, \"CTest\", combined_test)","5414b09f":"train_rev = train_df[[\"fullVisitorId\", \"totals.transactionRevenue\"]].groupby(\"fullVisitorId\").sum().reset_index()\ntest_rev = test_df[[\"fullVisitorId\", \"totals.transactionRevenue\"]].groupby(\"fullVisitorId\").sum().reset_index()","7d94fbc5":"combined_train_wrev = combined_train.merge(train_rev, how = \"outer\", on = \"fullVisitorId\")\ncombined_test_wrev = combined_test.merge(test_rev, how = \"outer\", on = \"fullVisitorId\")","9caac03b":"combined_train_wrev = combined_train_wrev.rename(columns = {\"totals.transactionRevenue\" : \"SUM(transactionRevenue)\"})\ncombined_test_wrev = combined_test_wrev.rename(columns = {\"totals.transactionRevenue\" : \"SUM(transactionRevenue)\"})","2401720e":"import lightgbm as lgb\nfrom sklearn.model_selection import KFold","fb127220":"new_train_features_encoding_dict = cat_feature_encoding(data=combined_train_wrev, cat_feature_list=combined_train_wrev.select_dtypes(include='object').columns)","4d6f5124":"new_test_features_encoding_dict = cat_feature_encoding(data=combined_test_wrev, cat_feature_list=combined_test_wrev.select_dtypes(include='object').columns)","49b8438f":"y = np.log1p(combined_train_wrev.iloc[:, -1])\n\nX = combined_train_wrev.iloc[:, 1:-1]\n\nX_pred = combined_test_wrev.iloc[:, 1:-1]","17126e7d":"lgb_clf = lgb.LGBMRegressor(learning_rate=0.05, n_estimators=1000, min_child_weight=np.power(10.0, 2), metric='rmse', \n                             num_leaves=128, reg_alpha=np.power(10.0, -3.2454), reg_lambda = np.power(10.0, -4.8571), silent=True, n_jobs=-1,\n                             colsample_bytree =  0.6810, min_child_samples = 95,  subsample = 0.2217, min_split_gain=np.power(10.0, -4.9380))","db001750":"from sklearn.metrics import mean_squared_error\nkfold = 5\nkf = KFold(n_splits=kfold, shuffle=True)\n\npredicts_result = []\ntest_result = []\nfor train_index, test_index in kf.split(X, y):\n    print(\"#\"*10)\n    X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n    lgb_clf.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=400, verbose=200, eval_metric='rmse') #eval_metric=f1_macro_evaluation)\n#     test_result.append(f1_score(y_pred=lgb_clf.predict(X_val), y_true=y_val, average=\"macro\"))\n    y_val_pred = lgb_clf.predict(X_val)\n    # Modify the value that is below the zero\n    y_val_pred[y_val_pred<0] = 0\n    test_result.append(mean_squared_error(y_true=y_val, y_pred=y_val_pred))\n    y_pred = lgb_clf.predict(X_pred, lgb_clf.best_iteration_)\n    y_pred[y_pred<0] = 0\n    predicts_result.append(y_pred)","9adc9c9f":"print(\"The average test RMSE is: \", np.mean(np.sqrt(test_result)))","e7c61788":"def plot_features(col_list, feature_importances, index, most_important=True):\n    indices = np.argsort(feature_importances)[::-1]\n    indices = indices[:index]\n\n    # Visualise these with a barplot\n    plt.subplots(figsize=(20, 15))\n    g = sns.barplot(y=col_list[indices], x = lgb_clf.feature_importances_[indices], orient='h')\n    g.set_xlabel(\"Relative importance\",fontsize=20)\n    g.set_ylabel(\"Features\",fontsize=20)\n    g.tick_params(labelsize=15)\n    g.set_title(\"LightGBM feature importance\", fontsize=20);","26408719":"plot_features(col_list=X.columns, feature_importances=lgb_clf.feature_importances_, index=-1)","102f8e13":"no_bounce_pred_df = pd.DataFrame({\"fullVisitorId\": combined_test_wrev.fullVisitorId, \"PredictedLogRevenue\": np.mean(predicts_result, axis=0)})\n\nsubmit_df = pd.DataFrame({\"fullVisitorId\": submit_id})\n\nsubmit_df = submit_df.merge(no_bounce_pred_df, on=\"fullVisitorId\", how='left').fillna(0)\n\nsubmit_df.to_csv(\"submission.csv\", index=False)","9ad27f96":"submit_df[submit_df.PredictedLogRevenue != 0].hist()","20e1da88":"labels = ['Zero revenue customer', 'Non-zero customer']\nvalues = [submit_df[submit_df.PredictedLogRevenue == 0].PredictedLogRevenue.count(),\n         submit_df[submit_df.PredictedLogRevenue != 0].PredictedLogRevenue.count()]\ntrace = go.Pie(labels=labels, values=values)\nlayout = dict(\n    title = 'Unique Customer Revenue Prediction'\n)\n\nfig = dict(data=[trace], layout=layout)\niplot(fig, filename='basic_pie_chart')","ff6c2f72":"Here is where the actual functions are used and applied. This part takes around 5 minutes.","1dc5d702":"This is where we pull an entity from the original (train\/test_log) and make a new entity. Note: Relationships is no longer empty in the output. ","566a6b7e":"Here we clean up the data we have generated.","795c8ec4":"# LightGBM\n---","8c05554e":"Ensure that we get rid of all NaNs","a358c0d2":"In an attempt to make things quicker we have also narrowed down the aggregation and transformation functions to be applied. In the output, you can see the features that will be generated.","087e3dad":"Rearrange the features by column categories.","8f55d3be":"From the data revenue plots we can tell that, from the middle of November till near the Christmas, there is a surge both in visit counts and sum up revenue. The visit counts drop significantly after the Christmas.","757053a7":"Take a look what our prediction looks like.","0968f5cf":"## Feature Tools\n\nHere we utilize the feature tools library to generate some features we know are good. Previously, we have trained on all columns with all entries. However, that takes over two hours to run. So, once we did that and found the best features, we have tried to narrow it down to reduce run time and make our lives easier. ","096c6b6d":"**Note that the visit number of an  instance is not neccessarily start from 1**","dc4b72e6":"It is useful to create a category that is a pandas timestamp for feature tools library to use. Here we do that.","4c8dee94":"This parameters comes from this [kernel](http:\/\/https:\/\/www.kaggle.com\/ogrellier\/user-level-lightgbm-lb-1-4480\/code). As we don't want to tune the parameters to control the experiment variables.","18384e4a":"## Traffic Source Analysis","13ef6043":"Merge the prediction with the bounce instance we filter.","4878aee8":"Here is the description from **Google**\n<table>\n  <tbody>\n    <tr>\n      <td><code>trafficSource.isTrueDirect<\/code><\/td>\n      <td>BOOLEAN<\/td>\n      <td>True if the source of the session was Direct (meaning the user typed the name of your website URL into the browser or came to your site via a bookmark), This field will also be true if 2 successive but distinct sessions have exactly the same campaign details. Otherwise NULL.<\/td>\n    <\/tr>\n  <\/tbody>\n<\/table>","4456aeb0":"Thanks Stan Wang, Dale Euinton, and Hang Zhao ","fb0f0e4b":"## Feature Explore On Import Features\nThe features we explore below will be the features that _LightGBM_ thinks it is important. Let's find out are they make sense to us.","87567bc1":"## Merging Data","426047da":"# Feature Engineering \n**What is going to be achieved in this section:**\n\n* Create custom features\n* Utilize the  [featuretools library](https:\/\/www.featuretools.com\/) to create useful features\n* Merge them together ","2b700814":"## Analysis Base On Date","6564fdff":"Now split the X_all_df to training and testing dataset again.","5aa44e14":"We can tell that the people who choose `Chrome OS` will contribute slightly more than other operating system. ","d3699d35":"### EntitySets\nFeaturetools uses EntitySets to represent dataframes and the relationships between them. Here we create an entity set from our whole dataframe and then pull another dataframe out from that (using fullVisitorId). This is very powerful and later allows us to automatically use grouped information. ","b346c206":"1. # Conclusion\nIn this kernel we have tried three different iterations. \n\n1. Naive - Score : 1.7340\n2. Feature Engineering - Score : 1.5665\n3. Reduced Feature Engineering (for runtime) with custom features added - Score :  1.5372\n\nClearly, 1.5665 was the best iteration. \n\nThis kernel has highlighted the usefulness of Plotly, Featuretools, lightGBM. We would use these all again. ","cba444d0":"## Custom Feature Engineering\n\nIn trying to engineer some new features, I came up with these ideas:\n\n1. Difference in time between min and max visitStartTimes\n2. Number of seconds between hits (a lot will just be zero)\n3. Number of seconds between visits\n4. Number of seconds between pageviews\n\nThe following section will focus on creating these features.","1f93f373":"Conclusion: `trafficSource.isTrueDirect` have a lot wrong values and `trafficSource.medium` is highly redundant to `channelGrouping`.","fcf1308b":"People who use the desktop are still the main income source of GA store.","ef8e5de8":"The average test RMSE will much higher than our actual prediction is because we filter up all the bouce instance to reduce the work during the feature engineering.","b8982b27":"## Features Analysis\n---\nAfter all efforts in preprocessing, we can now look into the features we will deal with. As this is a iterative developing workflow, we will go through some features at first time and focus on the top three most important features.","600caf89":"## Feature Encoding\n---\nString feature is not usable for machine learning algorithm. We need to encode them to either numeric featuers or one-hot encoding features. This deponds on the machine learning algorithm you choose. Before we start, we need to find out the category of the features. Unfortinately, it has to be done by hand pick.","66cea3bd":"Above two binary features are quite important and intuitive as well. An old user is more likely to make a purchase than a new user. Those who visit website directly are more objective than the other means.","4759de5f":"Americas are still the main market for the GA store but among the group who spends the money that `Africa` has the most dense non-zero nature log revenue within the range of 18-22.","f42cedad":"Check if any features are missing.","44e0e8b6":"This information might be redundant to the previous one `device.deviceCategory`. Drop this feature if the feature importance is  not high.","1da054c4":"From the plot we can infer that the instances who comes from `referral` website has the highest ritio of non-zero revenue, while the instances come from `Display` channel have the highest median non-zero revenue.","7f169e2c":"**Observation:** Above are the most three important features that _LightGBM_ thinks that are important. `pageviews` and `hits` make sense while `visitStartTime` seems a big mistake. Like Teo mentioned during our presentation that, a single algorithm will have its bias."}}