{"cell_type":{"36cfad37":"code","0f638e94":"code","e6d75a42":"code","03841a72":"code","2e1d7e1c":"code","7a09b70c":"code","44f738c4":"code","1619ebf6":"code","4a968559":"code","010cc4a7":"code","38b92d86":"code","edd770ff":"markdown"},"source":{"36cfad37":"import pandas as pd\nimport os\nimport numpy as np\nimport pandas as pd\nimport zipfile\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport sys\nimport datetime","0f638e94":"#downloading weights and cofiguration file for the model\n!wget https:\/\/storage.googleapis.com\/bert_models\/2018_10_18\/uncased_L-12_H-768_A-12.zip","e6d75a42":"repo = 'model_repo'\nwith zipfile.ZipFile(\"uncased_L-12_H-768_A-12.zip\",\"r\") as zip_ref:\n    zip_ref.extractall(repo)","03841a72":"!ls 'model_repo\/uncased_L-12_H-768_A-12'","2e1d7e1c":"!wget https:\/\/raw.githubusercontent.com\/google-research\/bert\/master\/modeling.py \n!wget https:\/\/raw.githubusercontent.com\/google-research\/bert\/master\/optimization.py \n!wget https:\/\/raw.githubusercontent.com\/google-research\/bert\/master\/run_classifier.py \n!wget https:\/\/raw.githubusercontent.com\/google-research\/bert\/master\/tokenization.py ","7a09b70c":"# Available pretrained model checkpoints:\n#   uncased_L-12_H-768_A-12: uncased BERT base model\n#   uncased_L-24_H-1024_A-16: uncased BERT large model\n#   cased_L-12_H-768_A-12: cased BERT large model\n#We will use the most basic of all of them\nBERT_MODEL = 'uncased_L-12_H-768_A-12'\nBERT_PRETRAINED_DIR = f'{repo}\/uncased_L-12_H-768_A-12'\nOUTPUT_DIR = f'{repo}\/outputs'\nprint(f'***** Model output directory: {OUTPUT_DIR} *****')\nprint(f'***** BERT pretrained directory: {BERT_PRETRAINED_DIR} *****')\n","44f738c4":"!wget https:\/\/raw.githubusercontent.com\/google-research-datasets\/gap-coreference\/master\/gap-development.tsv\n!wget https:\/\/raw.githubusercontent.com\/google-research-datasets\/gap-coreference\/master\/gap-validation.tsv\n!wget https:\/\/raw.githubusercontent.com\/google-research-datasets\/gap-coreference\/master\/gap-test.tsv\n!ls","1619ebf6":"def _row_to_y(row):\n    if row.loc['A-coref']:\n        return '0'\n    if row.loc['B-coref']:\n        return '1'\n    return '2'","4a968559":"from sklearn.model_selection import train_test_split\n\ntrain_df =  pd.read_csv('gap-test.tsv', sep='\\t')\ndev_df = pd.read_csv('gap-development.tsv', sep='\\t')\ntest_df = pd.read_csv('gap-validation.tsv', sep='\\t')\n\ntrain_df = pd.concat([train_df, test_df]) \ntest_df = dev_df\n\ntrain_lines, train_labels = train_df.Text.values, train_df.apply(_row_to_y, axis=1)\ntest_lines, test_labels = test_df.Text.values, test_df.apply(_row_to_y, axis=1)","010cc4a7":"import modeling\nimport optimization\nimport run_classifier\nimport tokenization\nimport tensorflow as tf\n\n\ndef create_examples(lines, set_type, labels=None):\n#Generate data for the BERT model\n    guid = f'{set_type}'\n    examples = []\n    if guid == 'train':\n        for line, label in zip(lines, labels):\n            text_a = line\n            label = str(label)\n            examples.append(\n              run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n    else:\n        for line in lines:\n            text_a = line\n            label = '0'\n            examples.append(\n              run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n    return examples\n\n# Model Hyper Parameters\nTRAIN_BATCH_SIZE = 32\nEVAL_BATCH_SIZE = 8\nLEARNING_RATE = 2e-5\nNUM_TRAIN_EPOCHS = 4.0\nWARMUP_PROPORTION = 0.1\nMAX_SEQ_LENGTH = 128\n# Model configs\nSAVE_CHECKPOINTS_STEPS = 1000 #if you wish to finetune a model on a larger dataset, use larger interval\n# each checpoint weights about 1,5gb\nITERATIONS_PER_LOOP = 1000\nNUM_TPU_CORES = 8\nVOCAB_FILE = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\nCONFIG_FILE = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\nINIT_CHECKPOINT = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\nDO_LOWER_CASE = BERT_MODEL.startswith('uncased')\n\nlabel_list = ['0', '1', '2']\ntokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=DO_LOWER_CASE)\ntrain_examples = create_examples(train_lines, 'train', labels=train_labels)\n\ntpu_cluster_resolver = None #Since training will happen on GPU, we won't need a cluster resolver\n#TPUEstimator also supports training on CPU and GPU. You don't need to define a separate tf.estimator.Estimator.\nrun_config = tf.contrib.tpu.RunConfig(\n    cluster=tpu_cluster_resolver,\n    model_dir=OUTPUT_DIR,\n    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n    tpu_config=tf.contrib.tpu.TPUConfig(\n        iterations_per_loop=ITERATIONS_PER_LOOP,\n        num_shards=NUM_TPU_CORES,\n        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n\nnum_train_steps = int(\n    len(train_examples) \/ TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\nnum_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n\nmodel_fn = run_classifier.model_fn_builder(\n    bert_config=modeling.BertConfig.from_json_file(CONFIG_FILE),\n    num_labels=len(label_list),\n    init_checkpoint=INIT_CHECKPOINT,\n    learning_rate=LEARNING_RATE,\n    num_train_steps=num_train_steps,\n    num_warmup_steps=num_warmup_steps,\n    use_tpu=False, #If False training will fall on CPU or GPU, depending on what is available  \n    use_one_hot_embeddings=True)\n\nestimator = tf.contrib.tpu.TPUEstimator(\n    use_tpu=False, #If False training will fall on CPU or GPU, depending on what is available \n    model_fn=model_fn,\n    config=run_config,\n    train_batch_size=TRAIN_BATCH_SIZE,\n    eval_batch_size=EVAL_BATCH_SIZE)","38b92d86":"\"\"\"\nNote: You might see a message 'Running train on CPU'. \nThis really just means that it's running on something other than a Cloud TPU, which includes a GPU.\n\"\"\"\n\n# Train the model.\nprint('Please wait...')\ntrain_features = run_classifier.convert_examples_to_features(\n    train_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\nprint('***** Started training at {} *****'.format(datetime.datetime.now()))\nprint('  Num examples = {}'.format(len(train_examples)))\nprint('  Batch size = {}'.format(TRAIN_BATCH_SIZE))\ntf.logging.info(\"  Num steps = %d\", num_train_steps)\ntrain_input_fn = run_classifier.input_fn_builder(\n    features=train_features,\n    seq_length=MAX_SEQ_LENGTH,\n    is_training=True,\n    drop_remainder=True)\nestimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\nprint('***** Finished training at {} *****'.format(datetime.datetime.now()))","edd770ff":"I use the code from [Introducing BERT with Tensorflow](http:\/\/www.kaggle.com\/sergeykalutsky\/introducing-bert-with-tensorflow)"}}