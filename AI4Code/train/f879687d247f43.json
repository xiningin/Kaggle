{"cell_type":{"69d291ac":"code","375d0e66":"code","64b0d201":"code","9ae0862f":"code","18ad037b":"code","432b0a37":"code","e525cab7":"code","2619650f":"code","87e94dfd":"code","5def2298":"code","bf9832b2":"code","7d5605d9":"code","bc63249a":"code","8416f084":"code","d446b6f8":"code","c1681b37":"code","759dd1d9":"code","eb310768":"code","4925c801":"code","53a5cf6d":"code","4bb274db":"code","57d82fdd":"code","e1e6563e":"code","6e63bf1a":"code","1cf401b0":"code","29637476":"code","3b03da51":"code","c36a417f":"code","018d0b76":"code","0481c77b":"code","5a0cd6c5":"code","f20fd0ca":"code","97818e47":"code","3be73b90":"code","5b16cb63":"code","f03520d4":"code","14dab185":"code","027a9b96":"code","af8dd76d":"code","990a6c63":"code","aba2d85c":"code","4896be2b":"code","692ac46e":"code","c9bf1652":"code","f49228f8":"code","2c07b501":"code","01b09789":"code","cf7b5e9c":"code","6223c8c3":"code","157365c3":"code","c76189f0":"code","981dd870":"code","69e8e693":"code","e69dc7f8":"code","fe394624":"code","5d51002f":"code","7b7d4064":"code","bc8c442b":"code","687212f7":"code","797e660a":"code","ccbca2c6":"code","8ceaa3a2":"code","42f19981":"code","3ded877d":"code","1828a18e":"code","65bca529":"code","108f6d7a":"code","8ee912c5":"code","4aa6d521":"code","31af9195":"code","7e716f9c":"code","e195bdbb":"code","a33a6c79":"code","eb3670ac":"code","d4c49b67":"code","29765c3a":"code","55883806":"code","5ee61908":"code","1ea1ddb2":"code","7a0baf2e":"code","190b91ba":"code","6ceacdd4":"markdown","51fc65f5":"markdown","6fb7cfa0":"markdown","d171ef04":"markdown","ba525abc":"markdown","c41dcc9b":"markdown","1ac81d7e":"markdown","abd6510a":"markdown","793e11ce":"markdown"},"source":{"69d291ac":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nplt.style.use(\"seaborn-whitegrid\")\n\nimport seaborn as sns\n\nfrom collections import Counter\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","375d0e66":"train_df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n\ntest_PassengerId = test_df[\"PassengerId\"]","64b0d201":"train_df.columns","9ae0862f":"train_df.head()","18ad037b":"train_df.describe()","432b0a37":"train_df.info()","e525cab7":"# Categorical Variable\n\ndef bar_plot(variable):\n    \n    \"\"\"\n        input: variable ex:\"Sex\"\n        output: bar plot & value count\n    \"\"\"\n    \n    # get feature\n    var = train_df[variable]\n    # count number of categorical variable(value\/sample)\n    varValue = var.value_counts()\n    \n    # visualize\n    plt.figure(figsize = (9, 3))\n    plt.bar(varValue.index, varValue)\n    plt.xticks(varValue.index, varValue.index.values)\n    plt.ylabel(\"Frequency\")\n    plt.title(variable)\n    plt.show()\n    print(\"{}:\\n{}\".format(variable, varValue))\n    ","2619650f":"category1 = [\"Survived\", \"Sex\", \"Pclass\", \"Embarked\", \"SibSp\", \"Parch\"]\n\nfor c in category1:\n    bar_plot(c)","87e94dfd":"category2 = [\"Cabin\", \"Name\", \"Ticket\"]\n\nfor c in category2:\n    print(\"{}\\n\".format(train_df[c].value_counts()))","5def2298":"def plot_hist(variable):\n    plt.figure(figsize = (9, 3))\n    plt.hist(train_df[variable], bins = 50)\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} distribution with hist\".format(variable))\n    plt.show()","bf9832b2":"numericVar = [\"Fare\", \"Age\", \"PassengerId\"]\n\nfor n in numericVar:\n    plot_hist(n)","7d5605d9":"# Pclass vs Survived\n\ntrain_df[[\"Pclass\", \"Survived\"]].groupby([\"Pclass\"], as_index = False).mean().sort_values(by = \"Survived\", ascending = False)","bc63249a":"# Sex vs Survived\n\ntrain_df[[\"Sex\", \"Survived\"]].groupby([\"Sex\"], as_index = False).mean().sort_values(by = \"Survived\", ascending = False)","8416f084":"# SibSp vs Survived\n\ntrain_df[[\"SibSp\", \"Survived\"]].groupby([\"SibSp\"], as_index = False).mean().sort_values(by = \"Survived\", ascending = False)","d446b6f8":"# Parch vs Survived\n\ntrain_df[[\"Parch\", \"Survived\"]].groupby([\"Parch\"], as_index = False).mean().sort_values(by = \"Survived\", ascending = False)","c1681b37":"def detect_outliers(df, features):\n    outlier_indices = []\n    \n    for c in features:\n        # 1st quantile\n        Q1 = np.percentile(df[c], 25)\n        # 3rd quantile\n        Q3 = np.percentile(df[c], 75)\n        # IQR\n        IQR = Q3 - Q1\n        # Outlier step\n        outlier_step = IQR * 1.5\n        # Detect outlier and their indices\n        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index\n        # Store indeces\n        outlier_indices.extend(outlier_list_col)\n        \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)\n    \n    return multiple_outliers\n        ","759dd1d9":"train_df.loc[detect_outliers(train_df, [\"Age\", \"SibSp\", \"Parch\", \"Fare\"])]","eb310768":"# drop outliers\n\ntrain_df = train_df.drop(detect_outliers(train_df, [\"Age\", \"SibSp\", \"Parch\", \"Fare\"]), axis = 0).reset_index(drop = True)","4925c801":"# Missing Values\n\ntrain_df_len = len(train_df)\ntrain_df = pd.concat([train_df, test_df], axis = 0).reset_index(drop = True)\n\n","53a5cf6d":"train_df_len","4bb274db":"# Find Missing Values\n\ntrain_df.columns[train_df.isnull().any()]","57d82fdd":"train_df.isnull().sum()","e1e6563e":"# Fill Missing Values\n\n# Embarked has 2 missing values and Fare has only 1\n\ntrain_df[train_df[\"Embarked\"].isnull()]","6e63bf1a":"train_df.boxplot(column = \"Fare\", by = \"Embarked\")\nplt.show()","1cf401b0":"train_df[\"Embarked\"] = train_df[\"Embarked\"].fillna(\"C\")\ntrain_df[train_df[\"Embarked\"].isnull()]","29637476":"train_df[train_df[\"Fare\"].isnull()]","3b03da51":"train_df[\"Fare\"] = train_df[\"Fare\"].fillna(np.mean(train_df[train_df[\"Pclass\"] == 3][\"Fare\"]))\ntrain_df[train_df[\"Fare\"].isnull()]","c36a417f":"# Visualization\n\n# Correlation Matrix\n\nlist1 = [\"SibSp\", \"Parch\", \"Age\", \"Fare\", \"Survived\"]\n\nsns.heatmap(train_df[list1].corr(), annot = True, fmt = \".2f\")\nplt.show()\n\n# Fare feature seems to have correlation with survived feature (0.26).","018d0b76":"# SibSp - Survived\n\ng = sns.factorplot(x = \"SibSp\", y = \"Survived\", data = train_df, kind = \"bar\", size = 6)\ng.set_ylabels(\"Survived Probability\")\nplt.show()","0481c77b":"# Parch - Survived\n\ng = sns.factorplot(x = \"Parch\", y = \"Survived\", data = train_df, kind = \"bar\", size = 6)\ng.set_ylabels(\"Survived Probability\")\nplt.show()","5a0cd6c5":"# Pclass - Survived\n\ng = sns.factorplot(x = \"Pclass\", y = \"Survived\", data = train_df, kind = \"bar\", size = 6)\ng.set_ylabels(\"Survived Probability\")\nplt.show()","f20fd0ca":"# Age - Survived\n\ng = sns.FacetGrid(train_df, col = \"Survived\")\ng.map(sns.distplot, \"Age\", bins = 25)\nplt.show()","97818e47":"# Pclass - Survived - Age\n\ng = sns.FacetGrid(train_df, col = \"Survived\", row = \"Pclass\", size = 2)\ng.map(plt.hist, \"Age\", bins = 25)\ng.add_legend()\nplt.show()","3be73b90":"# Embarked - Sex - Pclass - Survived\n\ng = sns.FacetGrid(train_df, row = \"Embarked\", size = 2.5)\ng.map(sns.pointplot, \"Pclass\", \"Survived\", \"Sex\")\ng.add_legend()\nplt.show()","5b16cb63":"# Embarked - Sex - Fare - Survived\n\ng = sns.FacetGrid(train_df, row = \"Embarked\", col = \"Survived\", size = 2.5)\ng.map(sns.barplot, \"Sex\", \"Fare\")\ng.add_legend()\nplt.show()","f03520d4":"# Fill Missing Age Feature\n\ntrain_df[train_df[\"Age\"].isnull()]","14dab185":"sns.factorplot(x = \"Sex\", y = \"Age\", data = train_df, kind = \"box\")\nplt.show()","027a9b96":"sns.factorplot(x = \"Sex\", y = \"Age\", hue = \"Pclass\", data = train_df, kind = \"box\")\nplt.show()","af8dd76d":"sns.factorplot(x = \"Parch\", y = \"Age\", data = train_df, kind = \"box\")\nsns.factorplot(x = \"SibSp\", y = \"Age\", data = train_df, kind = \"box\")\nplt.show()","990a6c63":"train_df[\"Sex\"] = [1 if i == \"male\" else 0 for i in train_df[\"Sex\"]]","aba2d85c":"sns.heatmap(train_df[[\"Age\", \"Sex\", \"SibSp\", \"Parch\", \"Pclass\"]].corr(), annot = True)\nplt.show()","4896be2b":"index_nan_age = list(train_df[\"Age\"][train_df[\"Age\"].isnull()].index)\n\nfor i in index_nan_age:\n    age_pred = train_df[\"Age\"][(train_df[\"SibSp\"] == train_df.iloc[i][\"SibSp\"]) & \n                               (train_df[\"Parch\"] == train_df.iloc[i][\"Parch\"]) &\n                               (train_df[\"Pclass\"] == train_df.iloc[i][\"Pclass\"])].median()\n    \n    age_med = train_df[\"Age\"].median()\n    \n    if not np.isnan(age_pred):\n        train_df[\"Age\"].iloc[i] = age_pred\n    else:\n        train_df[\"Age\"].iloc[i] = age_med","692ac46e":"train_df[train_df[\"Age\"].isnull()]","c9bf1652":"# Feature Engineering\n\n# Name - Title\n\ntrain_df[\"Name\"].head(10)","f49228f8":"name = train_df[\"Name\"]\ntrain_df[\"Title\"] = [i.split(\".\")[0].split(\",\")[-1].strip() for i in name]\n\ntrain_df[\"Title\"].head(10)","2c07b501":"sns.countplot(x = \"Title\", data = train_df)\nplt.xticks(rotation = 60)\nplt.show()","01b09789":"# Convert to categorical\n\ntrain_df[\"Title\"] = train_df[\"Title\"].replace([\"Lady\", \"the Countess\", \"Capt\", \"Col\", \"Don\", \"Dr\", \"Dona\", \"Major\", \"Rev\", \"Sir\", \"Jonkheer\"], \"Other\")\ntrain_df[\"Title\"] = [0 if i == \"Master\" else 1 if i == \"Miss\" or i == \"Ms\" or i == \"Mlle\" or i == \"Mrs\" else 2 if i == \"Mr\" else 3 for i in train_df[\"Title\"]]\ntrain_df[\"Title\"].head(20)","cf7b5e9c":"sns.countplot(x = \"Title\", data = train_df)\nplt.xticks(rotation = 60)\nplt.show()","6223c8c3":"g = sns.factorplot(x = \"Title\", y = \"Survived\", data = train_df, kind = \"bar\")\ng.set_xticklabels([\"Master\", \"Mrs\", \"Mr\", \"Other\"])\ng.set_ylabels(\"Survival Probability\")\nplt.show()","157365c3":"train_df.drop(labels = [\"Name\"], axis = 1, inplace = True)","c76189f0":"train_df.head()","981dd870":"train_df = pd.get_dummies(train_df, columns = [\"Title\"])\ntrain_df.head()","69e8e693":"# Family Size\n\ntrain_df[\"Fsize\"] = train_df[\"SibSp\"] + train_df[\"Parch\"] + 1\n\n# SipSb ve Parch 0 olsa dahi bireyin kendisinden dolay\u0131 +1 ekliyoruz.","e69dc7f8":"g = sns.factorplot(x = \"Fsize\", y = \"Survived\", data = train_df, kind = \"bar\")\ng.set_ylabels(\"Survival\")\nplt.show()","fe394624":"train_df[\"family_size\"] = [1 if i < 5 else 0 for i in train_df[\"Fsize\"]]","5d51002f":"train_df.head(10)","7b7d4064":"sns.countplot(x = \"family_size\", data = train_df)\nplt.show()","bc8c442b":"g = sns.factorplot(x = \"family_size\", y = \"Survived\", data = train_df, kind = \"bar\")\ng.set_ylabels(\"Survival\")\nplt.show()","687212f7":"train_df = pd.get_dummies(train_df, columns = [\"family_size\"])\ntrain_df.head()","797e660a":"# Embarked\n\ntrain_df[\"Embarked\"].head()","ccbca2c6":"sns.countplot(x = \"Embarked\", data = train_df)\nplt.show()","8ceaa3a2":"train_df = pd.get_dummies(train_df, columns = [\"Embarked\"])\ntrain_df.head()","42f19981":"# Ticket\n\ntrain_df[\"Ticket\"].head(10)","3ded877d":"tickets = []\n\nfor i in list(train_df.Ticket):\n    if not i.isdigit():\n        tickets.append(i.replace(\".\", \"\").replace(\"\/\", \"\").strip().split(\" \")[0])\n    else:\n        tickets.append(\"X\")\ntrain_df[\"Ticket\"] = tickets","1828a18e":"train_df[\"Ticket\"].head(10)","65bca529":"train_df = pd.get_dummies(train_df, columns = [\"Ticket\"], prefix = \"T\")\ntrain_df.head(10)","108f6d7a":"# Pclass\n\nsns.countplot(x = \"Pclass\", data = train_df)\nplt.show()","8ee912c5":"train_df[\"Pclass\"] = train_df[\"Pclass\"].astype(\"category\")\ntrain_df = pd.get_dummies(train_df, columns = [\"Pclass\"])\ntrain_df.head(10)","4aa6d521":"# Sex\n\ntrain_df[\"Sex\"] = train_df[\"Sex\"].astype(\"category\")\ntrain_df = pd.get_dummies(train_df, columns = [\"Sex\"])\ntrain_df.head(10)","31af9195":"# Drop PassengerId and Cabin\n\ntrain_df.drop(labels = [\"PassengerId\", \"Cabin\"], axis = 1, inplace = True)","7e716f9c":"# Modeling\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score","e195bdbb":"# Train - Test Split\n\ntrain_df_len","a33a6c79":"test = train_df[train_df_len:]\ntest.drop(labels = [\"Survived\"], axis = 1, inplace = True)","eb3670ac":"train = train_df[:train_df_len]\nX_train = train.drop(labels = [\"Survived\"], axis = 1)\ny_train = train[\"Survived\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.33, random_state = 42)\n\nprint(\"X_train\", len(X_train))\nprint(\"X_test\", len(X_test))\nprint(\"y_train\", len(y_train))\nprint(\"y_test\", len(y_test))\nprint(\"test\", len(test))","d4c49b67":"# Simple Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nacc_log_train = round(logreg.score(X_train, y_train) * 100, 2)\nacc_log_test = round(logreg.score(X_test, y_test) *100, 2)\nprint(\"Training Accuracy: % {}\".format(acc_log_train))\nprint(\"Test Accuracy: % {}\".format(acc_log_test))","29765c3a":"# Hyperparameter Tuning - Grid Search - Cross Validation\n\n# We'll compare 5 ML classifier and evaluate mean accuracy of each of them by stratified cross validation.\n\n# Decision Tree\n# SVM\n# Random Forest\n# KNN\n# Logistic Regression","55883806":"random_state = 42\nclassifier = [DecisionTreeClassifier(random_state = random_state),\n             SVC(random_state = random_state),\n             RandomForestClassifier(random_state = random_state),\n             LogisticRegression(random_state = random_state),\n             KNeighborsClassifier()]\n\ndt_param_grid = {\"min_samples_split\" : range(10, 500, 20),\n                \"max_depth\" : range(1, 20, 2)}\n\nsvc_param_grid = {\"kernel\" : [\"rbf\"],\n                 \"gamma\" : [0.001, 0.01, 0.1, 1],\n                 \"C\" : [1, 50, 100, 200, 300, 1000]}\n\nrf_param_grid = {\"max_features\" : [1, 3, 10],\n                \"min_samples_split\" : [2, 3, 10],\n                \"min_samples_leaf\" : [1, 3, 10],\n                \"bootstrap\" : [False],\n                \"n_estimators\" : [100, 300],\n                \"criterion\" : [\"gini\"]}\n\nlogreg_param_grid = {\"C\" : np.logspace(-3, 3, 7),\n                    \"penalty\" : [\"l1\", \"l2\"]}\n\nknn_param_grid = {\"n_neighbors\" : np.linspace(1, 19 ,10, dtype = int).tolist(),\n                 \"weights\" : [\"uniform\", \"distance\"],\n                 \"metric\" : [\"euclidean\", \"manhattan\"]}\n\nclassifier_param = [dt_param_grid,\n                   svc_param_grid,\n                   rf_param_grid,\n                   logreg_param_grid,\n                   knn_param_grid]","5ee61908":"cv_result = []\nbest_estimators = []\n\nfor i in range(len(classifier)):\n    clf = GridSearchCV(classifier[i], param_grid = classifier_param[i], cv = StratifiedKFold(n_splits = 10), scoring = \"accuracy\", n_jobs = -1, verbose = 1)\n    clf.fit(X_train, y_train)\n    cv_result.append(clf.best_score_)\n    best_estimators.append(clf.best_estimator_)\n    print(cv_result[i])\n","1ea1ddb2":"cv_results = pd.DataFrame({\"Cross Validation Means\" : cv_result,\n                         \"ML Models\" : [\"DecisionTreeClassifier\", \"SVM\", \"RandomForestClassifier\",\n                                       \"LogisticRegression\", \"KNeighborsClassifier\"]})\n\ng = sns.barplot(\"Cross Validation Means\", \"ML Models\", data = cv_results)\ng.set_xlabel(\"Mean Accuracy\")\ng.set_title(\"Cross Validation Scores\")","7a0baf2e":"# Ensemble Modeling\n\n# Ba\u015far\u0131 oran\u0131 %80'nin \u00fczerinde olan modelleri alal\u0131m.\n\nvotingC = VotingClassifier(estimators = [(\"dt\", best_estimators[0]),\n                                        (\"rfc\", best_estimators[2]),\n                                        (\"lr\", best_estimators[3])],\n                                        voting = \"soft\", n_jobs = -1)\n\nvotingC = votingC.fit(X_train, y_train)\nprint(accuracy_score(votingC.predict(X_test), y_test))","190b91ba":"# Prediction and Submission\n\ntest_survived = pd.Series(votingC.predict(test), name = \"Survived\").astype(int)\nresults = pd.concat([test_PassengerId, test_survived], axis = 1)\nresults.to_csv(\"titanic.csv\", index = False)","6ceacdd4":"* Sex is not informative for age prediction, age distribution seems to be same.","51fc65f5":"* Female passengers have much better survival rate than males\n* Males have better survival rate in Pclass 3 in C\n* Embarked and Sex will be used training","6fb7cfa0":"* Pclass is important feature for model training.","d171ef04":"* Having a lot of SibSp have less chance to survive.\n* If SibSp == 0 or 1 or 2, passenger has more chance to survive.\n* We can consider a new feature describing these categories.","ba525abc":"* SibSp and Parch can be used for new feature extraction with th = 3\n* Small families have more chance to survive\n* There is a std in survival of passenger with Parch = 3","c41dcc9b":"* Age <= 10 has a high survival rate\n* Oldest passengers (~80) survived\n* Large number of 20 years old did not survive\n* Most passengers are in 15-35 age range\n* Use age feature in training\n* Use age distribution for missing value of age","1ac81d7e":"## Introduction\n\nThe sinking of Titanic is one of the most notorious shipwrecks in the history. In 1912, during her voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew.\n\nContent:\n\n1. Load and Check Data\n\n2. Variable Description\n\n    *    PassengerId: unique id number to each passenger\n    *    Survived: passenger died (0) or survived (1)\n    *    Pclass: passenger class\n    *    Name: name\n    *    Sex: gender of passenger\n    *    Age: age of passenger\n    *    SibSp: number of siblings\/spouses\n    *    Parch: number of parents\/children\n    *    Ticket: ticket number\n    *    Fare: amount of money spent on ticket\n    *    Cabin: cabin category\n    *    Embarked: port where passenger embarked (C = Cherbourg, Q = Queenstown, S = Southampton)\n    \n    \n3. Basic Data Analysis\n\n4. Outlier Detection\n\n5. Missing Values\n\n6. Visualization\n\n7. Feature Engineering\n\n8. Modeling","abd6510a":"* 1st class passengers are older than 2nd class, and 2nd class is older than 3rd class.","793e11ce":"* Passengers who pay higher fare have better survival. Fare can bue used as categorical for training."}}