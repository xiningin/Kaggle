{"cell_type":{"65c655d4":"code","9334d3e7":"code","d566bd11":"code","aacba84f":"code","22c16521":"code","707d26cf":"code","7c288c9e":"code","ebc55365":"code","f7f77e45":"code","d378fb4d":"code","9562365f":"code","27e560c9":"code","660b4793":"code","1dce8ec8":"code","c778d0d7":"code","992af86e":"code","a88e62bb":"code","ae041981":"code","7dbaf45d":"code","01d682bc":"code","0a3f79fe":"code","85839147":"code","08834047":"markdown","b9fd96f5":"markdown"},"source":{"65c655d4":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom pandas import read_csv","9334d3e7":"filename = (\"..\/input\/housing.csv\")\nnames = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\ndata = read_csv(filename, delim_whitespace=True, names=names)","d566bd11":"print(data.shape)","aacba84f":"data.head()","22c16521":"plt.figure(figsize=(12,10))\nsns.heatmap(data.corr().round(2),cmap='coolwarm',annot=True)","707d26cf":"boston = pd.DataFrame(np.c_[data['LSTAT'], data['RM'], data['MEDV']], columns = ['LSTAT','RM','MEDV'])","7c288c9e":"boston.head()","ebc55365":"def featureNormalization(X):\n\n    mean=np.mean(X,axis=0) \n    std=np.std(X,axis=0)\n    \n    X_norm = (X - mean)\/std\n    \n    return X_norm , mean , std\n\nboston_n=boston.values\nm=len(boston_n[:,-1])\nX=boston_n[:,0:2].reshape(m,2)\nX, mean_X, std_X = featureNormalization(X)\nX = np.append(np.ones((m,1)),X,axis=1)\ny=boston_n[:,-1].reshape(m,1)\ntheta=np.zeros((3,1))","f7f77e45":"def computeCost(X,y,theta):\n    \n    m=len(y)\n    predictions=X.dot(theta)\n    square_err=(predictions - y)**2\n    \n    return 1\/(2*m) * np.sum(square_err)","d378fb4d":"computeCost(X,y,theta)","9562365f":"def gradientDescent(X,y,theta,alpha,num_iters):\n     \n    m=len(y)\n    J_history=[]\n    \n    for i in range(num_iters):\n        predictions = X.dot(theta)\n        error = np.dot(X.transpose(),(predictions -y))\n        descent=alpha * 1\/m * error\n        theta-=descent\n        J_history.append(computeCost(X,y,theta))\n    \n    return theta, J_history","27e560c9":"theta,J_history = gradientDescent(X,y,theta,0.01,300)\nprint(\"h(x) =\"+str(round(theta[0,0],2))+\" + \"+str(round(theta[1,0],2))+\"x1 + \"+str(round(theta[2,0],2))+\"x2\");","660b4793":"theta","1dce8ec8":"plt.plot(J_history)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"$J(\\Theta)$\")\nplt.title(\"Cost function using Gradient Descent\")","c778d0d7":"computeCost(X,y,theta)","992af86e":"predictions = X.dot(theta)","a88e62bb":"# sum of square of residuals\nssr = np.sum((predictions - y)**2)\n\n#  total sum of squares\nsst = np.sum((y - np.mean(y))**2)\n\n# R2 score\nr2_score = 1 - (ssr\/sst)","ae041981":"print(' R2 score =' +str(round(r2_score,3)))","7dbaf45d":"# mean squared error\nmse = np.sum((predictions - y)**2)\n\n# root mean squared error\n# m is the number of training examples\nrmse = np.sqrt(mse\/m)","01d682bc":"print('MSE = ', mse, 'RMSE = ', rmse)","0a3f79fe":"def predict(x,theta):\n    \n    predictions= np.dot(theta.transpose(),x)\n    \n    return predictions[0]","85839147":"x_new1=np.array([8.25,5.50])\nx_new1=np.append(np.ones(1),x_new1)\npredict_new1=predict(x_new1,theta)\n\nprint(predict_new1)","08834047":"Some prediction:","b9fd96f5":"For LSTAT = 8.25 and RM = 5.50, we predict MEDV of 5.54***"}}