{"cell_type":{"2d673231":"code","90db8d64":"code","19d6a09b":"code","2943d138":"code","8c99e690":"code","61fe3d42":"code","4d567d20":"code","6a562dd6":"code","f5686c62":"code","9fd5bae0":"code","65965a8e":"code","69d7e4a0":"code","48c84181":"code","78dc7ed5":"code","55912c1f":"code","7b3856e1":"code","d47a0860":"code","2c6e7b86":"code","d7375031":"code","ab453a36":"code","52e592af":"code","9e3449bf":"code","8f0c47ef":"code","2d8867be":"code","ea315d42":"code","40b64de8":"code","49dd8574":"code","ac22d9fe":"code","4407f48c":"code","766ffe48":"code","b2678dbb":"code","f5b4ebf5":"code","6cc65432":"code","cab8cdfd":"code","313d5649":"code","29e526e1":"markdown","914067b1":"markdown","ce0391fe":"markdown","c9d6d1a1":"markdown","f8abbf69":"markdown","9cb5c935":"markdown","a4bd93a4":"markdown","76b156c6":"markdown","dcf8254f":"markdown","e0d063e7":"markdown","505b822a":"markdown","62640bad":"markdown","9ca23411":"markdown","a1122db9":"markdown","e8c770fa":"markdown","dcc0575e":"markdown","be4d52d9":"markdown","86a90073":"markdown","28cf48aa":"markdown","6315ace3":"markdown","ff165877":"markdown","0695c339":"markdown","3b67099e":"markdown","93c79fc1":"markdown","05940a67":"markdown","583fa755":"markdown","c5d094b2":"markdown","63d019db":"markdown","5cd0001d":"markdown"},"source":{"2d673231":"c","90db8d64":"!ls \/kaggle\/input\/praktikumword2vecnlp\/idwiki.txt","19d6a09b":"#Preparation\n\nfrom sklearn.manifold import TSNE\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport warnings\nwarnings.filterwarnings('ignore')","2943d138":"!cp -r \/kaggle\/input\/praktikumword2vecnlp\/158 \/kaggle\/working\/158\n!git clone https:\/\/github.com\/HIT-SCIR\/ELMoForManyLangs\n!pip install -e ELMoForManyLangs\/\n!cp ELMoForManyLangs\/configs\/cnn_50_100_512_4096_sample.json \/kaggle\/working\/158\/config.json","8c99e690":"file = open(\"\/kaggle\/working\/158\/config.json\", \"w\")\n\nteks = '{\"seed\": 1, \"gpu\": 3, \"train_path\": \"\/users4\/conll18st\/raw_text\/Indonesian\/id-20m.raw\", \"valid_path\": null, \"test_path\": null, \"config_path\": \"\/kaggle\/working\/ELMoForManyLangs\/configs\/cnn_50_100_512_4096_sample.json\", \"word_embedding\": null, \"optimizer\": \"adam\", \"lr\": 0.001, \"lr_decay\": 0.8, \"model\": \"\/users4\/conll18st\/elmo\/src\/final_models\/id.model\", \"batch_size\": 32, \"max_epoch\": 10, \"clip_grad\": 5, \"max_sent_len\": 20, \"min_count\": 3, \"max_vocab_size\": 150000, \"save_classify_layer\": false, \"valid_size\": 0, \"eval_steps\": 10000}'\n\nfile.write(teks)\nfile.close()","61fe3d42":"import ELMoForManyLangs.elmoformanylangs as elmo","4d567d20":"from gensim.models import Word2Vec\n\nmodelword2vec = Word2Vec.load(\"\/kaggle\/input\/praktikumword2vecnlp\/idwiki_word2vec_300.model\")\nimport os\nfrom IPython.display import FileLink\nFileLink(r'\/kaggle\/input\/praktikumword2vecnlp\/idwiki.txt')","6a562dd6":"def plot(model, words):\n    \n    arr = np.empty((0,100), dtype='f')\n    word_labels = []\n    \n    for word in words:\n        wrd_vector = model[word]\n        word_labels.append(word)\n        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n        \n        \n    # find tsne coords for 2 dimensions\n    pca = PCA(n_components=2, copy=False, whiten=True)\n    Y = pca.fit_transform(arr)\n\n    x_coords = Y[:, 0]\n    y_coords = Y[:, 1]\n    # display scatter plot\n    plt.scatter(x_coords, y_coords)\n\n    for label, x, y in zip(word_labels, x_coords, y_coords):\n        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n    plt.show()","f5686c62":"print(modelword2vec[\"ui\"])","9fd5bae0":"# Jawaban kode Soal 1\nprint(\"dimensi dari vektor word2vec yang kita gunakan sekarang adalah:\", modelword2vec[\"ui\"].shape[0])","65965a8e":"plot(modelword2vec, [\"jakarta\", \"bandung\" , \"bekasi\" , \"serpong\" ,# Barat Jawa\n             \"surabaya\" , \"malang\" , \"yogyakarta\"  , # Timur Jawa\n             \"banjarmasin\" , \"balikpapan\" , \"samarinda\" , # Kalimantan\n             \"medan\" , \"palembang\" , \"jambi\" , # Sumatera\n             \"manado\" , \"gorontalo\" , \"palu\"  , # Sulawesi\n            \"ambon\" , \"sofifi\" , \"tual\" , # Maluku\n             \"fakfak\" , \"jayapura\" , \"mamuju\" ]) # Papua","69d7e4a0":"# Jawaban kode nomor 2\nplot(modelword2vec, [\"sungai\", \"danau\", \"laut\", \"rawa\", # tempat alami di perairan\n                     \"sabana\", \"stepa\", \"hutan\", \"gurun\", \"pegunungan\", # tempat alami di daratan\n                     \"waduk\", \"kolam\", # tempat buatan manusia di perairan\n                     \"kebun\", \"sawah\", \"pabrik\", \"rumah\", # tempat buatan manusia di daratan\n             ])","48c84181":"modelword2vec.most_similar(positive = [\"presiden\"], topn=5)","78dc7ed5":"modelword2vec.most_similar(positive = [\"makan\"], topn=5)","55912c1f":"modelword2vec.most_similar(positive = [\"inggris\", \"jakarta\"], negative = [\"indonesia\"], topn=5)","7b3856e1":"plot(modelword2vec, [\"inggris\" , \"london\",\n                     \"filipina\" , \"manila\" ,\n                    \"rusia\" , \"moscow\" , \n                    \"jepang\" , \"tokyo\",\n                    \"taiwan\" , \"taipei\",\n                    \"kanada\" , \"ottawa\"])","d47a0860":"modelword2vec.similarity('zebra' , 'refrigerator')\nmodelword2vec.similarity('zebra' , 'house')","2c6e7b86":"modelword2vec.doesnt_match(['jokowi' , 'sby' , 'suharto' , 'sule'])","d7375031":"# Jawaban kode nomor 3\nplot(modelword2vec, [\n    \"semut\", \"gula\",\n    \"harimau\", \"hutan\",\n    \"tentara\", \"perang\",\n    \"kerbau\", \"kandang\",\n    \"burung\", \"langit\",\n    \"penjara\", \"narapidana\"\n])","ab453a36":"print('setelah diplot yang paling mirip arah dan jaraknya adalah kerbau:kandang')","52e592af":"# Jawaban kode nomor 4\ndef cosine_similarity(word1, word2):\n    vec1 = modelword2vec[word1]\n    vec2 = modelword2vec[word2]\n    return (vec1.dot(vec2)) \/ (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n\ncosine_similarity('raja', 'presiden')","9e3449bf":"# Ambil word2vec setiap kata\nw2v = dict(zip(modelword2vec.wv.index2word, modelword2vec.wv.syn0))\n\nclass MeanEmbeddingVectorizer(object):\n    def __init__(self, word2vec):\n        self.word2vec = word2vec\n        self.dim = len(word2vec['dan'])\n        \n    def tokenize(self, sentences):\n        return [sentence.lower().split(\" \") for sentence in sentences]\n\n    \n    def transform(self, X):\n        # Ambil kata-katanya lalu rata-rata\n        return np.array([\n            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n                    or [np.zeros(self.dim)], axis=0)\n            for words in X\n        ])\n    \nvectorizer = MeanEmbeddingVectorizer(w2v)","8f0c47ef":"# Task : Sentimen Analisis\n# 0 negatif , 1 positif\n\ntrain_teks = [\"Saya sedih karena warung pasta ditutup\" ,\n              \"Sekarang adalah waktunya untuk berbahagia dan bersyukur\" , \n              \"Bangun segan , mati tidak mau ketika menghadapi sprint\" ,\n              \"OH MY GOD AKU DAPAT TANDA TANGAN LISA DARI BLACKPINK\" ,\n              \"NLP itu seru !\" ,\n              \"Gue bahagia karena keterima magang\" ,\n              \"' Mampus aku bisnis aku bakal bangkrut ' , pikir CEO Traveloka\" , \n              \"Cacing di perut mencuri semua nutrisi penting\"\n             ]\n\ntrain_y = [0 , 1, 0 ,1 , 1,  1, 0 , 0]\n\ntrain_X = vectorizer.transform(vectorizer.tokenize(train_teks))\n\ntest_teks = [\"Memang tidak salah untuk berharap , tapi aku harus tahu kapan berhenti\" ,\n              \"Mengapur berdebu , kita semua gagal , ambil s'dikit tisu , bersedihlah secukupnya\" , \n              \"Ini adalah waktunya kamu untuk bersinar\" ,\n             \"Kita akan berhasil menghadapi cobaan \"\n             ]\n\ntest_y = [0 , 0 , 1 , 1]\n\ntest_X = vectorizer.transform(vectorizer.tokenize(test_teks))\n","2d8867be":"# Jawaban kode nomor 5\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\n\nSVCpipe = Pipeline([('scale', StandardScaler()),\n                   ('SVC',LinearSVC())])\n\n# Gridsearch to determine the value of C\nparam_grid = {'SVC__C':[10**(i\/8) for i in range(-24, 25)]}\nclf = GridSearchCV(SVCpipe,param_grid,cv=4,return_train_score=True)\nclf.fit(train_X,train_y)\nprint(clf.best_params_)\n#linearSVC.coef_\n#linearSVC.intercept_\n\nbestlinearSVC = clf.best_estimator_\nbestlinearSVC.fit(train_X,train_y)\nbestlinearSVC.coef_ = bestlinearSVC.named_steps['SVC'].coef_\nbestlinearSVC.score(test_X,test_y)","ea315d42":"from gensim.test.utils import common_texts\nfrom gensim.models import Word2Vec","40b64de8":"common_texts","49dd8574":"!ls \/kaggle\/input\/praktikumword2vecnlp\/idwiki.txt","ac22d9fe":"!wc -l \/kaggle\/input\/praktikumword2vecnlp\/idwiki.txt\nimport nltk\ncommon_texts = []\nf = open('\/kaggle\/input\/praktikumword2vecnlp\/idwiki.txt')\ncounter = 0\nfor line in f:\n    counter += 1\n    if counter % 1000 == 0:\n        print(counter * 100 \/ 392172, '%')\n    common_texts.append(nltk.word_tokenize(line))\nprint(len(common_texts))","4407f48c":"model = Word2Vec(common_texts, size=300, window=3, min_count=1, workers=4)\n!ls\nmodel.save(\"word2vec.300.model\")\n!ls","766ffe48":"!cp word2vec.300.model* \/kaggle\/input\/","b2678dbb":"!ls \/kaggle\/","f5b4ebf5":"e = elmo.Embedder('\/kaggle\/working\/158')","6cc65432":"def encode_elmo(elmo,  kalimat):\n    vektor = elmo.sents2elmo([kalimat.split(\" \")])\n    return vektor[0]","cab8cdfd":"matematika = encode_elmo(e, \"Tujuh kali dua sama dengan empat belas\")[1]\nsungai1 = encode_elmo(e, \"Saya tinggal di samping kali Ciliwung\")[4]\nsungai2 = encode_elmo(e, \"Indonesia Lawyers Club mempertanyakan kualitas kali yang menjadi water way\")[5]\nsekarang = encode_elmo(e, \"Untuk kali ini dia yang kena batunya\")[1]\nperbandingan = encode_elmo(e, \"Sejak korona , harga telur menjadi dua kali lipat\")[7]\nfrekuensi = encode_elmo(e, \"Saya sudah tidur lima kali\")[4]","313d5649":"arr = np.empty((0,1024), dtype='f')\narr = np.append(arr, np.array([matematika]), axis=0)\narr = np.append(arr, np.array([sungai1]), axis=0)\narr = np.append(arr, np.array([sungai2]), axis=0)\narr = np.append(arr, np.array([sekarang]), axis=0)\narr = np.append(arr, np.array([perbandingan]), axis=0)\narr = np.append(arr, np.array([frekuensi]), axis=0)\n\npca = PCA(n_components=2, copy=False, whiten=True)\nY = pca.fit_transform(arr)\n\nx_coords = Y[:, 0]\ny_coords = Y[:, 1]\n\nplt.scatter(x_coords, y_coords)\n\nnama_label = ['matematika' , 'sungai1' , 'sungai2' , \n              'sekarang' , 'perbandingan' , 'frekuensi']\nfor label, x, y in zip(nama_label, x_coords, y_coords):\n    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\nplt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\nplt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\nplt.show()","29e526e1":"2. Lalu kita train. Akan tetapi, ada beberapa parameter yang perlu kita perhatikan:\n\n\n* size => ukuran vektor\n\n* window => ukuran window\n\n* min_count => berapa banyak kali suatu kata harus muncul sebelum disimpan\n\n* epoch => berapa kali data ditrain ulang\n\n* workers => berapa banyak thread yang akan digunakan\n\nTidak ada *heuristic* spesifik untuk mencari parameter yang tepat. Akan tetapi,\n\n* Mayoritas paper yang menggunakan embedding umumnya ukurannya >= 100\n* Window umumnya memiliki ukuran 2 dan 3 , tetapi jika kita merasa datanya kurang banyak , kita bisa meningkatkan window ke 4 atau 5.","914067b1":"~ Jawaban Teks Soal 1","ce0391fe":"~Jawaban Teks Nomor 2","c9d6d1a1":"Jawaban teks nomor 5","f8abbf69":"Bagaimana hal itu bisa terjadi? Entah mengapa, Word2Vec bisa menyimpan hubungan tersebut dalam bentuk arah dan jarak antara dua vektor tersebut. Jika kita lihat di grafik di bawah, jika kamu tarik garis dari ibu kota ke negaranya masing-masing, garisnya hampir konsisten.","9cb5c935":"Selain itu, kita bisa menggunakan model tersebut untuk mencari kata yang paling mirip berdasarkan seberapa mirip vektornya","a4bd93a4":"Pertama-tama, mari kita bermain dengan vektor-vektor tersebut. Kita bisa mengakses dengan cara sebagai berikut","76b156c6":"### Training Word2Vec dari awal","dcf8254f":"Persiapan sudah selesai. Untuk kepentingan latihan kali ini, kalian akan menggunakan Word2Vec yang sudah dilatih dengan bahasa Indonesia.","e0d063e7":"Soal :\n    \n[1] Dimensi vektor Word2Vec dapat ditentukan saat proses training awal. Berapakah dimensi vektor word2vec yang kita gunakan sekarang?","505b822a":"Soal:\n\n[3] \n\nSoal 3 ada 2 versi. Cek nomor bundel Tugas Akhir yang kalian gunakan. Jika nomor bundel Tugas Akhir kalian adalah ganjil, gunakan versi 1. Jika nomor bundel Tugas Akhir kalian adalah genap, gunakan versi 2.\n\nVersi 1 :\nGunakan Word2Vec untuk menjawab soal TPA SBMPTN tersebut.\n\n![](https:\/\/i.imgur.com\/xXBPdvD.png)\n\nVersi 2 :\nGunakan Word2Vec untuk menjawab soal TPA SBMPTN tersebut.\n\n![](https:\/\/www.zenius.net\/blog\/wp-content\/uploads\/2015\/02\/dompet.png)\n\n\n[4]  Fungsi model.similiarity menggunakan rumus cosine distance. Implementasikan sebuah fungsi yang menerima dua buah kata dan meng-return cosine distance antara vektor-vektor tersebut. Jika implementasinya benar, saat fungsi tersebut digunakan untuk menghitung raja dan presiden, jawabannya mendekati 0.3518753.","62640bad":"# Elmo\n\nWord2Vec memiliki kelemahan. Sebuah kata mungkin memiliki definisi yang berbeda, tetapi word2vec untuk definisi yang berbeda tetaplah yang sama karena kata yang sama. Sebagai contoh, kata \"kali\" bisa berarti perkalian matematika atau sungai. Meskipun kali mungkin memiliki definisi yang berbeda, vector word2vec yang merepresentasikan tetap sama. Elmo menghasilkan embedding ala Word2Vec, tetapi embedding kata tersebut juga berdasarkan kata-kata di sebelahnya. Mari kita coba.","9ca23411":"![](https:\/\/media.giphy.com\/media\/1iqPlDCyyXeiQRvqWI\/giphy.gif)","a1122db9":"Library preparation. Jangan sentuh.","e8c770fa":"## Code Exercise","dcc0575e":"**Materi tambahan!**\nKalian hanya perlu mengerjakan lima soal tersebut. Materi di bawah adalah bonus pengetahuan =)\n","be4d52d9":"Dari contoh di atas, embedding \"kali\" untuk definisi yang berbeda terpisah, tetapi sungai1 dan sungai2, kalimat yang definisi \"kali\" sebagai \"sungai\", vektornya berdekatan.","86a90073":"Model Word2Vec yang kalian gunakan ditrain menggunakan data dari seluruh artikel di Wikipedia Indonesia. Kalian boleh menggunakan model ini untuk tugas kelompok NLP, tapi bagaimana jika ingin membuat model baru dari awal?","28cf48aa":"Word2Vec dapat digunakan untuk mencari kalimat yang \"beda\" sendiri dengan mencari vektor mana yang paling jauh dari lainnya","6315ace3":"1. Teks harus sudah ditokenisasi pada tingkat dokumen (sesuai kebijaksanaan sendiri apakah pada tingkat dokumen, paragraf, atau kalimat). Lalu teks tersebut ditokenisasi pada tingkat kata. (Lihat contoh di bawah)","ff165877":"Salah satu hal yang membuat Word2Vec spesial adalah vektor-vektor tersebut sebenarnya secara implisit menyimpan konteks dari kata tersebut. Cara termudah adalah dengan melakukan plotting terhadap vektor tersebut.","0695c339":"Dari plot diatas, kita bisa lihat bahwa kota-kota tercluster secara geografi. meskipun tidak sempurna. Kota dari Barat Jawa di kanan atas. Timur Jawa di kanan bawah. Indonesia timur ada di sebelah kiri.  Sumatra & Kalimantan tercampur di tengah, tapi kemungkinan besar karena masalah visualisasi.\n\nUniknya, word2vec seperti sadar kalau Serpong itu bukan kota, tapi sebenarnya kecamatan, makanya dibuang ke atas.\n\nSalah satu penjelasan mengapa ini terjadi karena kota yang terletak berdekatan lebih sering muncul bersama-sama. Sebagai contoh, wajar untuk membaca artikel yang membahas Jakarta dan Depok, tetapi langka untuk membaca artikel yang membahas Jakarta dan Fakfak. Jika dua buah kota sering muncul di kalimat yang sama, berati training dataset CBOW \/ Skipgramnya banyak yang sama. Ini mengakibatkan kota-kota tersebut mirip vektornya.\n\nSoal :\n\n[2] Dari visual di atas, saya bisa membuat hipotesis bahwa Word2Vec dapat digunakan untuk task \"prediksi lokasi sebuah kota hanya dari namanya\". Kita sudah belajar Pos-Tagging dan Syntatic Parsing. Buat sebuah visual yang membuktikan Word2Vec bisa digunakan untuk task lain. Untuk nomor 2, anda tidak harus meng-cover task Pos-Tagging dan Syntatic Parsing. Anda boleh pilih task NLP lain atau buat task lain dari kreativitas. ","3b67099e":"# <center>Demo Word Embedding<\/center>\n<center> Deadline : Kamis , 14 Mei 2020 , 16:00 WIB <\/center>\n<center>CSC4602354 - Natural Language Processing<\/center>\n<center>Gasal 2019 \/ 2020<\/center>","93c79fc1":"Tentu saja kita dapat mencari seberapa mirip dua buah kata dari seberapa mirip vektornya.","05940a67":"Soal:\n\n[5] Soal ini ada 2 versi. Cek NPM kalian. Jika NPM kalian ganjil, kerjakan versi pertama. Jika NPM kalian genap, kerjakan versi kedua.\n\nVersi 1 :\n\nTrain model SVM Linear menggunakan data training dan testing di atas. Laporkan akurasi model. Anda boleh menggunakan model milik https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.LinearSVC.html\n\nVersi 2 :\n\nTrain model Logistic Regression menggunakan data training dan testing di atas. Laporkan akurasi model. Anda boleh menggunakan model milik https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html\n\n\n\nHint : Karena datanya sedikit, kita sebaiknya menggunakan model klasik. Tugas Akhir akan memiliki dataset yang jauh lebih besar, kita mungkin bisa menggunakan deep learning.","583fa755":"> terlihat bahwa komponen di atas terkluster, \ntempat alami di perairan cenderung berada di bawah kiri\ntempat buatan di perairan cenderung berada di bawah kanan\ntempat alami di daratan cenderung berada di atas kanan\ntempat buatan di perairan cenderung berada di atas kanan\n","c5d094b2":"~ Jawaban teks nomor 3","63d019db":"Kita bisa menggunakan Word2Vec sebagai fitur untuk kebutuhan klasifikasi. Sebagai contoh, kita bisa mengambil rata-rata Word2Vec setiap kata sebuah kalimat untuk membentuk \"vektor kalimat\" tersebut","5cd0001d":"Selain itu, kita bisa melakukan \"vector composition\" di mana kita melakukan penjumlahan dan pengurangan vektor kata untuk mencapai kata lain yang relasinya masih sama seperti Raja - Pria + Wanita = Ratu atau Jakarta - Indonesia + Inggris = London\n![](https:\/\/i.imgur.com\/l4Uawww.png)"}}