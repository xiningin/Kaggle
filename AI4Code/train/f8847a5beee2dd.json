{"cell_type":{"9cdb58d5":"code","de2d7b4d":"code","c5495ceb":"code","53484eec":"code","49140139":"code","aad37969":"code","a1aeced2":"code","5b5257e0":"code","995b511e":"code","e6574a05":"code","4fa0ca12":"code","249ec4ea":"code","38b04416":"code","1efd3d1c":"markdown","555d05f6":"markdown","b218f165":"markdown"},"source":{"9cdb58d5":"from __future__ import print_function, division\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport numpy as np\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport matplotlib.pyplot as plt\nimport time\nimport os\nimport copy\n\nplt.ion()   # interactive mode","de2d7b4d":"data_transforms = {\n    'train': transforms.Compose([\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'test': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}","c5495ceb":"data_dir = '..\/input\/rock-paper-scissors-dataset\/Rock-Paper-Scissors'\nimage_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'test']}\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n                                             shuffle=True, num_workers=4)\n              for x in ['train', 'test']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\nclass_names = image_datasets['train'].classes\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","53484eec":"def train_model(model, criterion, optimizer, scheduler, device = 'cpu', num_epochs=25):\n    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    losses = []\n    acces = []\n\n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'test']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n            if phase == 'train':\n                scheduler.step()\n\n            epoch_loss = running_loss \/ dataset_sizes[phase]\n            epoch_acc = running_corrects.double() \/ dataset_sizes[phase]\n            \n            losses.append(epoch_loss)\n            acces.append(epoch_acc)\n            \n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'test' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model, losses, acces","49140139":"model_plain = models.resnet18(pretrained=False)\nnum_ftrs = model_plain.fc.in_features\nmodel_plain.fc = nn.Linear(num_ftrs, 3)\n\nmodel_plain = model_plain.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n\n# Observe that all parameters are being optimized\noptimizer_plain = optim.SGD(model_plain.parameters(), lr=0.001, momentum=0.9)\n\n# Decay LR by a factor of 0.1 every 7 epochs\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_plain, step_size=7, gamma=0.1)","aad37969":"model_plain, losses_plain, acces_plain = train_model(model_plain, criterion, optimizer_plain, exp_lr_scheduler,device=device,\n                       num_epochs=25)","a1aeced2":"model_tl = models.resnet18(pretrained=True)\nnum_ftrs = model_tl.fc.in_features\nfor param in model_tl.parameters():\n    param.requires_grad = False\n\nmodel_tl.fc = nn.Linear(num_ftrs, 3)\n\nmodel_tl = model_tl.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n\n# Observe that all parameters are being optimized\noptimizer_tl = optim.SGD(filter(lambda p: p.requires_grad, model_tl.parameters()), lr=0.001, momentum=0.9)\n\n# Decay LR by a factor of 0.1 every 7 epochs\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_tl, step_size=7, gamma=0.1)","5b5257e0":"model_tl, losses_tl, acces_tl = train_model(model_tl, criterion, optimizer_tl, exp_lr_scheduler,device=device,\n                       num_epochs=25)","995b511e":"model_ft = models.resnet18(pretrained=True)\nnum_ftrs = model_ft.fc.in_features\n\nmodel_ft.fc = nn.Linear(num_ftrs, 3)\n\nmodel_ft = model_ft.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n\n# Observe that all parameters are being optimized\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n\n# Decay LR by a factor of 0.1 every 7 epochs\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)","e6574a05":"model_ft, losses_ft, acces_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,device=device,\n                       num_epochs=25)","4fa0ca12":"plt.figure(figsize=(12,8))\nplt.plot(acces_ft, label='Fine tuning')\nplt.plot(acces_plain, label='Plain')\nplt.plot(acces_tl, label='Transfer learning (re-trained only the last layer)')\nplt.legend(); plt.grid(True);\nplt.show()","249ec4ea":"list(model_ft.named_parameters())[-3]","38b04416":"for name, param in model_ft.named_parameters():\n    print(name)","1efd3d1c":"## Plain model","555d05f6":"## Fine tunning","b218f165":"## Transfer learning (without retraining a whole model)"}}