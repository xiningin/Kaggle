{"cell_type":{"a1a2eab3":"code","d6db7ee9":"code","977846cc":"code","7aa4b5ab":"code","b3666ac2":"code","146017ce":"code","8bbc4f11":"code","076f0ac7":"code","39ade139":"code","065fa4aa":"code","b1ed606d":"code","a2505a26":"code","e1a5e374":"code","e5b13583":"code","66416250":"code","fd3454ac":"code","0366ea57":"code","066a663f":"code","8b03cd03":"markdown","b94d4039":"markdown","76de1681":"markdown","0943e75a":"markdown","d00107aa":"markdown","0cf22f0e":"markdown","4b616d4d":"markdown","d719be01":"markdown","1ce30853":"markdown","a4901291":"markdown","925bcdcc":"markdown","dd339a54":"markdown","e327ac32":"markdown","b4efe347":"markdown","0e9773a2":"markdown","f1eaf711":"markdown"},"source":{"a1a2eab3":"import pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.metrics import mean_squared_error,make_scorer\nfrom sklearn.preprocessing import LabelEncoder\npd.set_option('max_columns',100)","d6db7ee9":"train = pd.read_csv('\/kaggle\/input\/sec3-eda-fe-categorical\/eng_filt_train.csv')\ntest = pd.read_csv('\/kaggle\/input\/sec3-eda-fe-categorical\/eng_filt_test.csv')\ntrain.shape,test.shape","977846cc":"train.columns","7aa4b5ab":"train['MSSubClass'] = train['MSSubClass'].astype('category')\ntest['MSSubClass'] = test['MSSubClass'].astype('category')\ntraintest = pd.concat([train,test],axis=0,ignore_index=True)\ncats = traintest.select_dtypes(include = ['object','category']).columns\n\nle =LabelEncoder()\ntraintest[cats] = traintest[cats].apply(le.fit_transform)\ntraintest.drop(['LogPrice','SalePrice','Id'],axis=1,inplace=True)\ntrs = traintest[:1425]\ntes = traintest[1425:]\n\ntraintest.head()","b3666ac2":"def rmse(y,y_):\n    mse = mean_squared_error(y,y_)\n    return np.sqrt(mse)\n\nscorer = make_scorer(rmse,greater_is_better=False)\ndef default_eval(model):\n    scores = cross_val_score(model,trs,train['SalePrice'],scoring=scorer,cv=5)\n    mean = np.mean(-scores)\n    std = np.std(-scores)\n    print(\"Mean score:{:.2f}    Std. Dev.:{:.2f}\".format(mean,std))","146017ce":"dt = DecisionTreeRegressor(random_state=1)\ndefault_eval(dt)","8bbc4f11":"rf = RandomForestRegressor(random_state=1)\ndefault_eval(rf)","076f0ac7":"gb = GradientBoostingRegressor(random_state=1)\ndefault_eval(gb)\n","39ade139":"xgb = XGBRegressor(random_state=1)\ndefault_eval(dt)","065fa4aa":"def grid_analysis(hyperparameters):\n    gb = GradientBoostingRegressor()\n    grid = GridSearchCV(gb,param_grid=hyperparameters,scoring = scorer,cv=5,n_jobs = -1)\n    grid.fit(trs,train['SalePrice'])\n    print(\"Best Parameters:\\n\",grid.best_params_)\n    print(\"\\nBest score \",-grid.best_score_)","b1ed606d":"hyperparameters = {'learning_rate':[0.02,0.05,0.08,0.09],\n                   'n_estimators':[500,700,800,900,1000],\n                  'random_state':[1],\n                   'min_samples_split':[10],\n                   'min_samples_leaf' : [5],\n                   'max_depth': [8],\n                   'max_features' : ['sqrt'] ,\n                   'subsample' : [0.8],\n                  }\ngrid_analysis(hyperparameters)\n","a2505a26":"hyperparameters = {'learning_rate':[0.02],\n                   'n_estimators':[500],\n                  'random_state':[1],\n                   'min_samples_split':[3,4,5,10,15,20],\n                   'max_depth': range(3,16),\n                   'max_features' : ['sqrt'] ,\n                   'subsample' : [0.8],\n                  }\ngrid_analysis(hyperparameters)","e1a5e374":"hyperparameters = {'learning_rate':[0.02],\n                   'n_estimators':[500],\n                  'random_state':[1],\n                   'min_samples_split':[5],\n                   'max_depth':[4] ,\n                   'min_samples_leaf': [1,2,3,4],\n                   'max_features' : ['sqrt'] ,\n                   'subsample' : [0.8]\n                  }\ngrid_analysis(hyperparameters)","e5b13583":"hyperparameters = {'learning_rate':[0.02],\n                   'n_estimators':[500],\n                  'random_state':[1],\n                   'min_samples_split':[5],\n                   'min_samples_leaf': [3],\n                   'max_depth': [4],\n                   'max_features' : range(5,27,2) ,\n                   'subsample' : [0.8]\n                  }\ngrid_analysis(hyperparameters)","66416250":"hyperparameters = {'learning_rate':[0.02],\n                   'n_estimators':[500],\n                  'random_state':[1],\n                   'min_samples_split':[5],\n                   'min_samples_leaf': [3],\n                   'max_depth': [4],\n                   'max_features' : [19] ,\n                   'subsample' : [0.6,0.7,0.75,0.8,0.85,0.9,0.95]\n                  }\ngrid_analysis(hyperparameters)","fd3454ac":"\nbest_params = {'learning_rate': 0.02 ,     \n                   'n_estimators':500,   \n                  'random_state':1,\n                   'min_samples_split':5,\n                   'min_samples_leaf': 3,\n                   'max_depth': 4,\n                   'max_features' : 19 ,\n                   'subsample' : 0.8\n                  }\ngb = GradientBoostingRegressor(**best_params)\n\nscores = cross_val_score(gb,trs,train['SalePrice'],scoring=scorer,cv=5)\nnp.mean(-scores),np.std(-scores)\n","0366ea57":"from sklearn.model_selection import learning_curve\n\n\nestimator = GradientBoostingRegressor(**best_params)\n\ntrain_sizes, train_scores, test_scores = learning_curve(estimator,trs,train['SalePrice'],cv=5,\n                                                        train_sizes = np.linspace(0.1, 1.0, 10), n_jobs=-1,scoring=scorer)\ntrain_scores = -train_scores\ntest_scores = -test_scores\n\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\ntest_scores_std = np.std(test_scores, axis=1)\n\n# Plot learning curve\n\nplt.figure(figsize=(10,7))\nplt.grid()\nplt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\nplt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1,\n                     color=\"g\")\nplt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training error (RMSE)\")\nplt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation error (RMSE)\")\nplt.axhline(y=10000, color='b', linestyle='--',label='Desired RMSE Threshold')\nplt.title('Learning Curve for Best Model')\nplt.xlabel('Training Sizes')\nplt.ylabel('RMSE score')\nplt.legend(loc=\"best\")\n\n","066a663f":"model = GradientBoostingRegressor(**best_params).fit(trs,train['SalePrice'])\n\n\npred = model.predict(tes)\n\nsubmission = pd.DataFrame({'Id':test['Id'].astype(int),'SalePrice':pred})\nsubmission.to_csv('submission.csv',index=False)","8b03cd03":"# Load Data:","b94d4039":"### Fix max_samples_leaf:","76de1681":"## Learning Curve\nIn machine learning, a learning curve shows the validation and training score (RMSE in this context) of an estimator for varying numbers of training samples. It essentially tells about the model if it is undefit \/overfit\/ best fit.","0943e75a":"No change in subsample setting that means previously selected value was appropriate.","d00107aa":"## Final set of parameter","0cf22f0e":"# Introduction:\nIn this last section, various tree based models are evaluated and best model will be selected based on lowest RMSE. Decision Tree, Random Forest, Gradient boosting , XGboost regressor are used for study. \n\nFiltered and engineered train and test data from third section(Ref:https:\/\/www.kaggle.com\/lajari\/sec3-eda-fe-categorical) has been used. In order to use categorical data in model, it need to be converted to numeric i.e. encode. Simple label encoding is sufficient for tree based modelling as well as we do not require feature scaling as they don't have much impact on performance of tree based model. As well as in this section we are mapping features directly to SalePrice unlike Log version as in previous section.","4b616d4d":"### Fixing max_features:","d719be01":"### Fixing sub_samples:","1ce30853":"# Prepare Data:\n\nSimple label encoding has been applied to categorical features and other numeric features kept as it is.","a4901291":"With this final parameter setiing we have signifcantly reduced mean RMSE(by 1500 compared to default model) as well as Std. Deviation of RMSE (by 300 compared to default model) is reduced little bit. This result highlights how hyperparameter tuning phase is important for getting best out of best model.","925bcdcc":"# Model Evaluation:\nThis step involve following substeps:\n1. Default evaluation made for well known tree based models\n2. Select best model based on lowest mean_cross_val_score in first step\n3. Hyperparameter tuning for best model.\n4. Analysis for learning curve\n\n### Default Evaluation:\nIn this step we are evaluating model with their default parameters.","dd339a54":"For given problem we must define desired RMSE and it should be not more than 10000. For. e.g. if we predict the given propery sale price say 200K then it would not surprise buyer if he hears actual sale price within range 190K to 210K. \n\nFrom above graph, it is observed that our training error lies within desired performance range, while cross validation RMSE is far away and above the desired performance. Also distance between training curve and cross validation curve is large. Overall it indicates that current model has suffered with high variance issue. But cross validation score seems slowly decreasing with training size because more samples cause reduction in variance. \n\nFor final test data we would be using whole training set. Therefore, we may obtain more improved performance on test set than expected.","e327ac32":"RMSE further reduced by 100","b4efe347":"We have obtained Test score(RMSE):14060 which seems better than cross validation score. It means adding more data improved result. As well as predictability of model will improve more and more with the data. \n\nSecond thing to note that this score is less better compared to best linear model in previous section. It indicates that linear model are capable to give satisfactory and competitive performance.","0e9773a2":"### Fixing max_depth and min_samples_split:\nTuning hyperparameter should be in order and hyperparamters with larger impact must be tuned first. max_depth and min_samples_split have significant impact. Let's tune those first.\n\nBest values max_depth:4 and min_samples_split:5. RMSE on cross validation is reduced further by 350 as compared previous.","f1eaf711":"## Hyperparameter tuning for best model:\nGradient boosting regressor has lowet RMSE score. It has large number of parameter to tune. n_estimator, learning_rate, min_samples split, min_samples_leaf and many more. Refer this nice article for more details(https:\/\/www.analyticsvidhya.com\/blog\/2016\/02\/complete-guide-parameter-tuning-gradient-boosting-gbm-python\/)\n\n### Fix n_estimator and learning rate:\nThere is a trade-off between learning_rate and n_estimators.Lower the learning rate, estimators need to be increase proportionally to get more robust models. But, before trying out these combinations other parameters are fixed to some appropriate value in following way\n\n* 'min_samples_split':10 (This should be ~0.5-1% of total observation)\n* 'min_samples_leaf' : 5,(Intuition)\n* 'max_depth': 8,(Should be chosen (5-8) based on the number of observations and predictors)\n* 'max_features' : 'sqrt' (general thumb-rule to start with square root.)\n* 'subsample' : 0.8 (commonly used start value)\n\nNote: These are not final estimation. They will be tuned further based on their impact on result.\n\nn_estimators from 10 to 400 has been tried but it haven't shown improved performance. n_estimators more than 500 have been tried over the low learning rate range 0.01 to 0.1. Thus, best parameter values for n_estimator=500, learning_rate=0.02. RMSE Score decreased by almost 1000 as compared to our base model (Default model in step 1)"}}