{"cell_type":{"122af3f7":"code","5358ca1f":"code","368518d7":"code","4d20c1bc":"code","8508f66d":"code","2cc3b27f":"code","5e56ed0a":"code","181b38a7":"code","c7607419":"code","42785dd8":"code","702bdcd1":"code","353c23b3":"code","c39dde92":"code","cb0e24e9":"code","24589315":"code","3e553de8":"code","95f17d11":"code","ff514c45":"code","7729af7b":"code","7d266f2b":"code","b1cf773c":"code","bcf7375c":"code","44c40195":"code","cef7b53d":"code","f2d2343e":"code","d46ccb3a":"code","ea8717c6":"code","236621a3":"code","bbdddf4a":"code","5e34709b":"code","ad43d8f8":"code","f7cdb2ab":"code","830ed7a3":"code","87f0be62":"code","7aa9062c":"code","32f95c65":"code","2c324bb7":"markdown","f309243e":"markdown","fad9e4c2":"markdown","0da8c4b7":"markdown","f02f8c33":"markdown","c7393b0b":"markdown","e871b732":"markdown","95d50be3":"markdown","b575c06b":"markdown","7620936d":"markdown","9bed97c3":"markdown","fbc4c7ee":"markdown","d4351b38":"markdown","5bf7d7df":"markdown","a61bbf80":"markdown","72d219b3":"markdown","d014197c":"markdown","8d3c5dc3":"markdown","116a1d4a":"markdown","bfe6665e":"markdown","fc35534b":"markdown","99342329":"markdown","f384f84c":"markdown","44c76f25":"markdown","e30b1b70":"markdown","89fb6d16":"markdown","e3d9b83f":"markdown","06de1ed8":"markdown","ce53466b":"markdown","04bc6fb5":"markdown","c95ad6a6":"markdown"},"source":{"122af3f7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5358ca1f":"%pwd","368518d7":"import pandas as pd \nimport numpy as np \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.svm import SVC\nfrom sklearn import tree\nfrom sklearn.metrics import accuracy_score\nimport seaborn as sns\nimport matplotlib \nimport matplotlib.pyplot as plt\nmatplotlib.style.use('ggplot')\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier","4d20c1bc":"train=pd.read_csv('\/kaggle\/input\/data-science-london-scikit-learn\/train.csv', sep=',', header=None)\ntest=pd.read_csv('\/kaggle\/input\/data-science-london-scikit-learn\/test.csv', sep=',', header=None)\ntrain_label=pd.read_csv('\/kaggle\/input\/data-science-london-scikit-learn\/trainLabels.csv', header=None)","8508f66d":"train.info()","2cc3b27f":"test.info()","5e56ed0a":"test.describe().transpose()","181b38a7":"train.describe().transpose()","c7607419":"print('Train: Does NaN values exist',train.isna().values.any())\nprint('Train: Does Null values exist',train.isnull().values.any())\nprint('Test: Does NaN values exist',test.isna().values.any())\nprint('Test: Does NaN exists',test.isnull().values.any())","42785dd8":"class_0=np.where(train_label==0)\nlen(class_0[0])\nclass_1=np.where(train_label==1)\nlen(class_1[0])\n\nprint('Number of samples for class 0:',len(class_0[0]) )\nprint('Number of samples for class 1:',len(class_1[0]))","702bdcd1":"X_train, X_test, y_train, y_test = train_test_split(train, train_label, test_size=0.2, random_state=30)","353c23b3":"print('X train dimension:',X_train.shape)\nprint('X test dimension :', X_test.shape)","c39dde92":"normalizer = MinMaxScaler()\nnormalized_train_X = normalizer.fit_transform(X_train)### we fit the normalizer on training data \n### Testing \nnormalized_test_X = normalizer.transform(X_test)\n","cb0e24e9":"scaler = StandardScaler()\nscaled_train_X = scaler.fit_transform(X_train)### we fit the normalizer on training data \n### Testing \nscaled_test_X = scaler.transform(X_test)\n","24589315":"clf=tree.DecisionTreeClassifier(criterion=\"gini\",random_state = 42) # implementation of the hierarchical\/decision trees\nclf.fit(X_train, y_train)\ny_predDT=clf.predict(X_test)\n# The score method returns the accuracy of the model\naccuracy_score(y_test,y_predDT)","3e553de8":"print('confusion matrix:')\n# confusion matrix \nprint(confusion_matrix(y_test,y_predDT))","95f17d11":"print('Classification report:')\nprint(classification_report(y_test, y_predDT))","ff514c45":"clf=tree.DecisionTreeClassifier(criterion=\"gini\",random_state = 42) # implementation of the hierarchical\/decision trees\nclf.fit(normalized_train_X, y_train)\ny_predDT1=clf.predict(normalized_test_X)\n# The score method returns the accuracy of the model\naccuracy_score(y_test,y_predDT1)","7729af7b":"clf=tree.DecisionTreeClassifier(criterion=\"gini\",random_state = 42) # implementation of the hierarchical\/decision trees\nclf.fit(scaled_train_X, y_train)\ny_predDT2=clf.predict(scaled_test_X)\n# The score method returns the accuracy of the model\naccuracy_score(y_test,y_predDT2)","7d266f2b":"svclassifier=SVC(kernel='rbf',random_state=30, gamma='auto')\n# train the classifier\nsvclassifier.fit(X_train,y_train)\ny_predSVM=svclassifier.predict(X_test)\nsvclassifier.score(X_test, y_test)\naccuracy_score(y_test, y_predSVM)","b1cf773c":"print('confusion matrix:')\n# confusion matrix \nprint(confusion_matrix(y_test,y_predSVM))","bcf7375c":"print('Classification report:')\nprint(classification_report(y_test, y_predSVM))","44c40195":"svclassifier=SVC(kernel='rbf',random_state=30, gamma='auto').fit(normalized_train_X,y_train)\ny_predSVM1=svclassifier.predict(normalized_test_X)\naccuracy_score(y_test, y_predSVM1)","cef7b53d":"svclassifier=SVC(kernel='rbf',random_state=30, gamma='auto').fit(scaled_train_X,y_train)\ny_predSVM2=svclassifier.predict(scaled_test_X)\naccuracy_score(y_test, y_predSVM2)","f2d2343e":"LR = LogisticRegression(solver='sag').fit(X_train, y_train)\ny_predLR=LR.predict(X_test)\naccuracy_score(y_test, y_predLR)","d46ccb3a":"print('confusion matrix:')\n# confusion matrix \nprint(confusion_matrix(y_test,y_predLR))\n\nprint('Classification report:')\nprint(classification_report(y_test, y_predLR))","ea8717c6":"LR1 = LogisticRegression(solver='sag').fit(normalized_train_X, y_train)\ny_predLR1=LR1.predict(normalized_test_X)\naccuracy_score(y_test, y_predLR1)","236621a3":"LR2 = LogisticRegression(solver='sag').fit(scaled_train_X, y_train)\ny_predLR2=LR2.predict(scaled_test_X)\naccuracy_score(y_test, y_predLR2)","bbdddf4a":"knn = KNeighborsClassifier(n_neighbors=7).fit(X_train, y_train.values.ravel())\npred_KNN = knn.predict(X_test)\nprint('KNN',accuracy_score(y_test, pred_KNN))","5e34709b":"print('confusion matrix:')\n# confusion matrix \nprint(confusion_matrix(y_test,pred_KNN))\n\nprint('Classification report:')\nprint(classification_report(y_test, pred_KNN))","ad43d8f8":"knn1 = KNeighborsClassifier(n_neighbors=7).fit(normalized_train_X, y_train.values.ravel())\npred_KNN1 = knn.predict(normalized_test_X)\nprint('KNN',accuracy_score(y_test, pred_KNN1))","f7cdb2ab":"knn2 = KNeighborsClassifier(n_neighbors=7).fit(scaled_train_X, y_train.values.ravel())\npred_KNN2 = knn.predict(scaled_test_X)\nprint('KNN',accuracy_score(y_test, pred_KNN2))","830ed7a3":"RF = RandomForestClassifier(criterion='gini',n_estimators=75,random_state =42)\nRF.fit(X_train, y_train.values.ravel())\npred_RF = RF.predict(X_test)\nprint('RF',accuracy_score(pred_RF,y_test))","87f0be62":"RF1 = RandomForestClassifier(criterion='gini',n_estimators=75,random_state =42).fit(normalized_train_X, y_train.values.ravel())\npred_RF1 = RF1.predict(normalized_test_X)\nprint('RF',accuracy_score(pred_RF1,y_test))","7aa9062c":"RF2 = RandomForestClassifier(criterion='gini',n_estimators=75,random_state =42).fit(scaled_train_X, y_train.values.ravel())\npred_RF2 = RF2.predict(scaled_test_X)\nprint('RF',accuracy_score(pred_RF2,y_test))","32f95c65":"prediction=knn.predict(test)\nmysubmission = pd.DataFrame(prediction)\n\nmysubmission.index += 1\n\nmysubmission.columns = ['Solution']\nmysubmission['Id'] = np.arange(1,mysubmission.shape[0]+1)\nmysubmission = mysubmission[['Id', 'Solution']]\n\nmysubmission.to_csv('Submission.csv',index=False)\n","2c324bb7":"### b. After data Normalization","f309243e":"Random Forest is a supervised learning algorithm. It is one of the most adopted algorithms that could be used to solve regression or classification tasks. It builds a 'forest' which is an ensemble of decision trees usually trained with the 'bagging' method. \n\nRandom Forest builds multiple decision trees and merges them together to get a more accurate and stable prediction. In addition, instead of searching for the most important feature while splitting a node, it searches for the best feature among a random subset of features. ","fad9e4c2":"# 1- Reading the trainin and testing datasets","0da8c4b7":"# 2- Splitting the total dataset into training and testing ","f02f8c33":"### b. After data Normalization","c7393b0b":"Decision tree is the most powerful and popular algorithm for classification and prediction. It is named decision tree because it builds a structure like the tree. It starts by splitting the total dataset into two subsets based on the most important feature. The most important feature is selected after considering a classification criterion.  \n\nThis process is repeated on each derived subset in a recursive manner called recursive partitioning until all the last nodes are pure means all of its data belongs to a single class. The construction of decision tree classifier does not require any domain knowledge or parameter setting, and therefore is appropriate for exploratory knowledge discovery.  In general decision tree classifier has good accuracy.","e871b732":"The Normalization and the Standardization are called feature scaling techniques. Normalization is also known under the name Min-Max scaling. This technique consists of shifting and rescaling the values, using the minimum and the maximum values of the training dataset, so that they end up ranging between 0 and 1. \n\nStandardization is another scaling technique that consists of scaling the values using the mean and standard deviation of the training dataset. All the values must be centered around the mean value which is null and the standard deviation is 1. ","95d50be3":"### c. After data Standardisation","b575c06b":"### b. After data Normalization","7620936d":"It is a supervised machine learning algorithm that can be used for both classification or regression tasks but it is most adopted in classification problems. \n\nThe linear SVM classifier works by drawing a straight line between two classes. It mainly consists of finding the hyperplane equation that distinctly classifies data points or observations.\n\nOur goal is to find a plane that has the maximum margin or maximum distance between the data points of both classes.","9bed97c3":"# 4- Application of our algorithm \n## 4.1-Decision tree \n### a. Before data Normalization and Standardisation ","fbc4c7ee":"It is a supervised machine learning algorithm that can used to solve only classification tasks. This algorithm stores all available cases and classifies new cases based on a similarity measure (e.g., distance functions). The distance could be Euclidean, Manhattan or MinKowski. It is \u201cnon-parametric\u201d (only k needs to be set) and is based only on training data.\n\n","d4351b38":"### c. After data Standardisation","5bf7d7df":"## 4.4- K Nearest Neighbors\n### a. Before data Normalization and Standardisation","a61bbf80":"It is clear that the best applied algorithm were KNN and SVM with accuracy equal to 91% and 90.5%, respectively. ","72d219b3":"### c. After data Standardisation","d014197c":"# 3- Data Normalisation and Standardization\n## 3.1- Data Normalisation ","8d3c5dc3":"##### Counting the number of classes ==> To check if classes are well balanced","116a1d4a":"## 4.2-SVM: Support Vector Machine \n### a. Before data Normalization and Standardisation","bfe6665e":"## 4.5- Random Forest \n### a. Before data Normalization and Standardisation","fc35534b":"##### Printing the dimensions of the datasets ","99342329":"### Importing the required libraries ","f384f84c":"### b. After data Normalization","44c76f25":"## 4.3- Logistic regression\n### a. Before data Normalization and Standardisation","e30b1b70":"We had noticed that data normalization or standardization does not improve the result so we can avoid it. ","89fb6d16":"## 3.2- Data Standardization ","e3d9b83f":"Logistic regression is a widely used method in machine learning to solve binary classification problems where two classes are predicted. It is called Logistic Regression because it is based on the logistic function or sigmoid function. The only difference from linear regression is that here our output is 0 or 1 while in linear regression it is a continuous value","06de1ed8":"#### Checking if there is any null or NaN value in both training and testing sets","ce53466b":"### c. After data Standardisation ","04bc6fb5":"### b. After data Normalization","c95ad6a6":"### c. After data Standardisation"}}