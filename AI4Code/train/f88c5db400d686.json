{"cell_type":{"758a6e7e":"code","6572fec2":"code","cfc8d380":"code","cd7dc7e7":"code","4f0f53e6":"code","d24e632b":"code","b52e7955":"code","8d87eca7":"code","a69b5687":"code","45333d3b":"code","07f294cb":"code","670441d6":"code","b9f8ec7e":"code","84449021":"markdown"},"source":{"758a6e7e":"import time\nnotebookstart= time.time()\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.options.display.max_colwidth = 500\nimport os\nimport gc\nimport re\nprint(\"Data:\\n\",os.listdir(\"..\/input\"))\n\n# Models Packages\nfrom sklearn import metrics\nfrom sklearn import feature_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\n\n# Gradient Boosting\nimport lightgbm as lgb\n\n# Tf-Idf\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.pipeline import FeatureUnion\nfrom scipy.sparse import hstack, csr_matrix\nfrom nltk.corpus import stopwords \n\n# Viz\nimport seaborn as sns\nimport matplotlib.pyplot as plt","6572fec2":"N_ROWS = None\nTEXT_COLUMN = 'text'\nTARGET_COLUMN = 'target'","cfc8d380":"print(\"Read Data\")\ntrain_df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv', nrows = N_ROWS, index_col = 'id')\ntest_df = pd.read_csv('..\/input\/nlp-getting-started\/test.csv', nrows = N_ROWS, index_col = 'id')\n\ny = train_df[TARGET_COLUMN].values\ntraindex = train_df.index.copy()\ntestdex = test_df.index.copy()\n\nprint(\"Train Shape: {} Rows, {} Columns\".format(*train_df.shape))\ndisplay(train_df.sample(5))\nprint(\"Test Shape: {} Rows, {} Columns\".format(*test_df.shape))\ndisplay(test_df.sample(5))\nprint('Dependent Variable Factor Ratio: ',train_df[TARGET_COLUMN].value_counts(normalize=True).to_dict())","cd7dc7e7":"# Join Train \/ Test\ndf = pd.concat([train_df,test_df],axis=0,sort=False)\ndisplay(df.sample(5))","4f0f53e6":"print(\"Text Features\")\ndf['keyword'] = df['keyword'].str.replace(\"%20\", \" \")\ndf['hashtags'] = df['text'].apply(lambda x: \" \".join(re.findall(r\"#(\\w+)\", x)))\ndf['hash_loc_key'] = df[['hashtags', 'location','keyword']].astype(str).apply(lambda x: \" \".join(x), axis=1)\n\ndf['hash_loc_key'] = df[\"hash_loc_key\"].astype(str).str.lower().fillna('missing')\ndf['hash_loc_key'] = df[\"hash_loc_key\"].astype(str).str.lower().fillna('missing')\n\ntextfeats = ['hash_loc_key', 'text']\nfor cols in textfeats:\n    df[cols + '_num_words'] = df[cols].apply(lambda comment: len(comment.split())) # Count number of Words\n    df[cols + '_num_unique_words'] = df[cols].apply(lambda comment: len(set(w for w in comment.split())))\n    df[cols + '_words_vs_unique'] = df[cols+'_num_unique_words'] \/ df[cols+'_num_words'] * 100 # Count Unique Words\n\nprint(\"\\n[TF-IDF] Term Frequency Inverse Document Frequency Stage\")\nstring_stopwords = set(stopwords.words('english'))\n\ntfidf_para = {\n#     \"stop_words\": string_stopwords,\n#     \"analyzer\": 'word',\n#     \"token_pattern\": r'\\w{1,}',\n#     \"sublinear_tf\": True,\n#     \"dtype\": np.float32,\n#     \"norm\": 'l2',\n#     \"min_df\":5,\n#     \"max_df\":.9,\n#     \"smooth_idf\":False\n}\n\n\ndef get_col(col_name): return lambda x: x[col_name]\nvectorizer = FeatureUnion([\n        ('text',TfidfVectorizer(\n            ngram_range=(1, 1),\n            max_features=5000,\n            **tfidf_para,\n            preprocessor=get_col('text'))),\n        ('hash_loc_key',TfidfVectorizer(\n            ngram_range=(1, 1),\n            max_features=5000,\n            **tfidf_para,\n            preprocessor=get_col('hash_loc_key')))\n    ])\n    \nstart_vect=time.time()\nvectorizer.fit(df.to_dict('records'))\ntfidf_sparse = vectorizer.transform(df.to_dict('records'))\ntfvocab = vectorizer.get_feature_names()\nprint(\"Vectorization Runtime: %0.2f Minutes\"%((time.time() - start_vect)\/60))","d24e632b":"dense_vars =  [\n    'text_num_words',\n    'text_num_unique_words',\n    'text_words_vs_unique',\n    'hash_loc_key_num_words',\n    'hash_loc_key_num_unique_words',\n    'hash_loc_key_words_vs_unique'\n]\n\n# Dense Features Correlation Matrix\nf, ax = plt.subplots(figsize=[5,5])\nsns.heatmap(df[dense_vars].corr(),\n            annot=False, fmt=\".2f\",\n            cbar_kws={'label': 'Correlation Coefficient'},\n            cmap=\"plasma\",ax=ax, linewidths=.5)\n\nax.set_title(\"Dense Features Correlation Matrix\")\nplt.show()","b52e7955":"print(\"Modeling Stage\")\nX = hstack([csr_matrix(df.loc[traindex,dense_vars].values),tfidf_sparse[0:traindex.shape[0]]]) # Sparse Matrix\ntesting = hstack([csr_matrix(df.loc[testdex,dense_vars].values),tfidf_sparse[traindex.shape[0]:]])\ntfvocab = df[dense_vars].columns.tolist() + tfvocab\nfor shape in [X,testing]:\n    print(\"{} Rows and {} Cols\".format(*shape.shape))\nprint(\"Feature Names Length: \",len(tfvocab))","8d87eca7":"print(\"\\nModeling Stage\")\nprint(\"Light Gradient Boosting Regressor\")\n\ndef lgb_f1_score(y_hat, data):\n    y_true = data.get_label()\n    y_hat = np.where(y_hat < 0.5, 0, 1)  \n    return 'f1', metrics.f1_score(y_true, y_hat, average='macro'), True\n\nlgbm_params = {\n        \"objective\": \"binary\",\n        \"boosting_type\": \"gbdt\",\n        \"num_threads\": -1,\n#         \"bagging_fraction\": 0.8,\n#         \"feature_fraction\": 0.8,\n#         \"learning_rate\": 0.05,\n#         \"max_depth\": 9,\n#         \"num_leaves\": 150,\n#         \"min_split_gain\": .1,\n#         \"reg_alpha\": .1\n    }\n\n# Training and Validation Set\nmodelstart = time.time()\nn_folds = 4\nfolds = KFold(n_splits=n_folds, shuffle=True, random_state=1)\noof_preds = np.zeros(traindex.shape[0])\nfold_preds = np.zeros(testdex.shape[0])\n\n# Fit 5 Folds\nmodelstart = time.time()\nfor trn_idx, val_idx in folds.split(X):\n    \n    lgtrain = lgb.Dataset(X.tocsr()[trn_idx], y[trn_idx])\n    lgvalid = lgb.Dataset(X.tocsr()[val_idx], y[val_idx])\n    \n    clf = lgb.train(\n        params=lgbm_params,\n        train_set=lgtrain,\n        valid_sets=lgvalid,\n        num_boost_round=3500, \n        early_stopping_rounds=200,\n        feval=lgb_f1_score,\n        verbose_eval=150\n    )\n    oof_preds[val_idx] = clf.predict(X.tocsr()[val_idx])\n    fold_preds += clf.predict(testing) \n    print('Metric:', metrics.f1_score(\n        y[val_idx], np.where(oof_preds[val_idx] < 0.5, 0, 1), average='macro'))\nprint(\"Model Runtime: %0.2f Minutes\"%((time.time() - modelstart)\/60))","a69b5687":"# OOF F1 Cutoff\nsave_f1_opt = []\ncutoff = .5\nfor cutoff in np.arange(.38,.62, .01):\n    save_f1_opt.append([cutoff, f1_score(y, (oof_preds > cutoff).astype(int), average='macro')])\nf1_pd = pd.DataFrame(save_f1_opt, columns = ['cutoff', 'f1_score'])\n\nbest_cutoff = f1_pd.loc[f1_pd['f1_score'].idxmax(),'cutoff']\nprint(\"F1 Score: {:.4f}, Optimised Cufoff: {:.2f}\".format(f1_pd.loc[f1_pd['f1_score'].idxmax(),'f1_score'], best_cutoff))\n\nf,ax = plt.subplots(1,2,figsize = [10,4])\n\nax[0].plot(f1_pd['cutoff'], f1_pd['f1_score'], c = 'red')\nax[0].set_ylabel(\"F1 Score\")\nax[0].set_xlabel(\"Cutoff\")\nax[0].axvline(x=best_cutoff, c='black')\nax[0].set_title(\"Macro Avg F1 Score and Cutoff on OOF\")\n\ntrain_df['oof_preds'] = oof_preds\ntrain_df['error'] = train_df['target'] - train_df['oof_preds']\n\nsns.distplot(train_df['error'], ax = ax[1])\nax[1].set_title(\"Classification Errors: Target - Pred Probability\")\nax[1].axvline(x=.5, c='black')\nax[1].axvline(x=-.5, c='black')\nplt.tight_layout(pad=1)\nplt.show()","45333d3b":"print(\"OOF Classification Report for Optimised Threshold: {:.3f}\".format(best_cutoff))\nprint(classification_report(y, (oof_preds > best_cutoff).astype(int), digits = 4))\nprint(\"\\nOOF Non-Optimised Cutoff (.5)\")\nprint(classification_report(y, (oof_preds > .5).astype(int), digits = 4))\n\ncnf_matrix = confusion_matrix(y, (oof_preds > .5).astype(int))\nprint(\"OOF Confusion Matrix\")\nprint(cnf_matrix)\nprint(\"OOF Normalised Confusion Matrix\")\nprint((cnf_matrix.astype('float') \/ cnf_matrix.sum(axis=1)[:, np.newaxis]).round(3))","07f294cb":"print(\"Look at False Negative\")\ndisplay(train_df.sort_values(by = 'error', ascending=False).iloc[:20])\n\nprint(\"Look at False Positives\")\ndisplay(train_df.sort_values(by = 'error', ascending=True).iloc[:20])","670441d6":"submission = pd.DataFrame.from_dict({\n    'id': test_df.index,\n    TARGET_COLUMN:  np.where((fold_preds\/n_folds) < 0.5, 0, 1)\n})\nsubmission.to_csv('lgbm_submission.csv', index=False)\nprint(submission[TARGET_COLUMN].value_counts(normalize = True).to_dict())\nsubmission.head()","b9f8ec7e":"print(\"Notebook Runtime: %0.2f Minutes\"%((time.time() - notebookstart)\/60))","84449021":"## LGBM Baseline - Disaster Text\n_by Nick Brooks, Janurary 2020_"}}