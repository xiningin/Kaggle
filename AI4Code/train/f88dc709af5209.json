{"cell_type":{"330f0f20":"code","2b3df2f6":"code","f5276079":"code","067911e2":"code","6d347cd5":"code","ed41cb14":"code","5e90a300":"code","73a33edf":"code","b38931ba":"code","b08f2c44":"code","0dbef66d":"code","4ddfadb8":"code","05c0cfd8":"code","0b24dfda":"code","b05affa0":"code","652ef8df":"code","69510299":"code","4e5b3b2a":"code","79950ed1":"code","063fecf2":"code","24daaf0e":"code","42d095d8":"code","695d9b2d":"code","1603af8c":"code","1b1456fb":"code","6933e749":"code","8be9cc7e":"code","a2850941":"code","5791e8f8":"code","3f07d1f2":"code","2342778e":"code","3db3d427":"code","dde5395e":"code","f4eb1c94":"code","455fe380":"markdown"},"source":{"330f0f20":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn import preprocessing, linear_model,model_selection,impute, metrics\nfrom sklearn import ensemble\nfrom xgboost import XGBRegressor\nfrom scipy.stats import skew\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","2b3df2f6":"train_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")","f5276079":"train_df.head()","067911e2":"test_df.head()","6d347cd5":"X = train_df.drop(['SalePrice','Id'],axis=1)\ny = train_df['SalePrice']\n\nX_validate = test_df.drop('Id',axis=1)\n\nprint(\"X shape: {} X_validate shape: {}\".format(X.shape, X_validate.shape))","ed41cb14":"combine_df = X.append(X_validate, ignore_index = True)","5e90a300":"missing_data = pd.DataFrame()\nmissing_data['count'] = combine_df.isnull().sum()\nmissing_data['% count'] = (missing_data['count']\/ len(combine_df)) * 100\n    \nmask1 = missing_data['% count']>40\nmask2 = missing_data['% count']>0\n    \nprint(\"missing_data > 40%: \\n\",missing_data[mask1])\ncolumns_to_drop = missing_data[mask1].index.tolist()\nprint(\"columns_to_drop: \\n\",columns_to_drop)\ncombine_df.drop(columns_to_drop, axis=1, inplace=True)\n\nprint(\"*\"*30)   \nprint(\"\\n 0% < missing_data < 40%: \\n\",missing_data[~mask1 & mask2])\ncolumns_with_NaN =missing_data[~mask1 & mask2].index.tolist()\nprint(\"columns_with_NaN: \\n\",columns_with_NaN)","73a33edf":"num_columns_with_NaN = combine_df[columns_with_NaN]._get_numeric_data().columns.tolist()\nprint(\"\\n Numerical columns: \\n\", num_columns_with_NaN)\ncat_columns_with_NaN = list(set(columns_with_NaN) - set(num_columns_with_NaN))\nprint(\"\\n Categorical columns: \\n\", cat_columns_with_NaN)","b38931ba":"combine_df['MSZoning'].unique()","b08f2c44":"cols_to_move = ['BsmtFullBath','BsmtHalfBath','GarageYrBlt','GarageCars']\n\nfor col in cols_to_move:\n    num_columns_with_NaN.remove(col)\n    cat_columns_with_NaN.append(col)","0dbef66d":"from sklearn.impute import SimpleImputer\n\n# Fill missing values for 'num_columns_with_NaN'\ncombine_df['LotFrontage'] = combine_df.groupby('Neighborhood')['LotFrontage'].transform(lambda x:x.fillna(x.median()))\n\nimp_zero = SimpleImputer(missing_values=np.nan, strategy='constant')\ncombine_df[['MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','GarageArea']] = \\\n                    imp_zero.fit_transform(combine_df[['MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','GarageArea']])\n\ncombine_df['TotalBsmtSF']  = combine_df['TotalBsmtSF'].fillna(combine_df['BsmtFinSF1'] \\\n                            + combine_df['BsmtFinSF2']+ combine_df['BsmtUnfSF'])","4ddfadb8":"# Fill missing values for 'cat_columns_with_NaN'\nimp_zero = SimpleImputer(missing_values=np.nan, strategy='constant')\ncombine_df[['BsmtFullBath','BsmtHalfBath','GarageYrBlt','GarageCars']] = \\\n                    imp_zero.fit_transform(combine_df[['BsmtFullBath','BsmtHalfBath','GarageYrBlt','GarageCars']])\n\nimp_none = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='None')\ncombine_df[['KitchenQual','MasVnrType','Functional','Electrical','Utilities']] = \\\n                    imp_none.fit_transform(combine_df[['KitchenQual','MasVnrType','Functional','Electrical','Utilities']])\n\nimp_other = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='Other')\ncombine_df[['Exterior1st','Exterior2nd','MSZoning','SaleType']] = \\\n                    imp_other.fit_transform(combine_df[['Exterior1st','Exterior2nd','MSZoning','SaleType']])\ncombine_df['SaleType'] = combine_df['SaleType'].fillna('Oth')\n\nimp_bsmt = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='No_Basement')\ncombine_df[['BsmtQual','BsmtCond','BsmtFinType1','BsmtFinType2','BsmtExposure']] = \\\n                    imp_bsmt.fit_transform(combine_df[['BsmtQual','BsmtCond','BsmtFinType1','BsmtFinType2','BsmtExposure']])\n\nimp_garage = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='No_Garage')\ncombine_df[['GarageType','GarageCond','GarageFinish','GarageQual']] = \\\n                    imp_other.fit_transform(combine_df[['GarageType','GarageCond','GarageFinish','GarageQual']])","05c0cfd8":"combine_df.isnull().sum().sum()","0b24dfda":"num_cols = combine_df._get_numeric_data().columns.tolist()\ncat_cols = list(set(combine_df.columns) - set(num_cols))\nprint(\"num_cols: {} \\n cat_cols: {}\".format(num_cols, cat_cols))","b05affa0":"cols_to_move = ['OverallQual','OverallCond','YearBuilt', 'YearRemodAdd','BsmtFullBath', 'BsmtHalfBath',\\\n                'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces',  \\\n                'GarageYrBlt', 'GarageCars','MoSold', 'YrSold']\nfor col in cols_to_move:\n    num_cols.remove(col)\n    cat_cols.append(col)","652ef8df":"combine_df[num_cols].describe()","69510299":"corr = combine_df[num_cols].corr()\ncorr","4e5b3b2a":"corr = combine_df[num_cols].corr()\nf, ax = plt.subplots(figsize = (15,12))\nsns.heatmap(abs(corr), annot=True, cmap=\"YlGnBu\")","79950ed1":"threshold = 0.5\ncorr_var_list = []\n\nfor i in range(1, len(num_cols)):\n    for j in range(i):\n        if((abs(corr.iloc[i,j]) > threshold) & (abs(corr.iloc[i,j]) < 1)):\n            corr_var_list.append([corr.iloc[i,j], i, j])\n\n# Sort the list showing higher ones first \ns_corr_list = sorted(corr_var_list ,key = lambda x:abs(x[0]))\n\n#Print correlations and column names\nfor corr_value, i, j in s_corr_list:\n    print (\"%s , %s = %.2f\" % (num_cols[i],num_cols[j],corr_value))","063fecf2":"combine_df.drop(['1stFlrSF','2ndFlrSF','BsmtFinSF1'], axis=1, inplace=True)\n\nfor col in ['1stFlrSF','2ndFlrSF','BsmtFinSF1']:\n    num_cols.remove(col)","24daaf0e":"combine_df = pd.get_dummies(combine_df, drop_first=True)\ncombine_df.shape","42d095d8":"sns.distplot(y)","695d9b2d":"skewness = pd.DataFrame(data={'name':num_cols,'skewness':skew(combine_df[num_cols])})\n\nhighly_skewed_cols = skewness[abs(skewness['skewness'])>=1]['name']\n\nfor col in highly_skewed_cols:\n    combine_df[col] = np.log1p(combine_df[col])\n    \ny = np.log1p(y)","1603af8c":"robust_scaler = preprocessing.RobustScaler()\nfor col in num_cols:\n    combine_df[col] = robust_scaler.fit_transform(combine_df[[col]])","1b1456fb":"sns.distplot(y)","6933e749":"X = combine_df.iloc[:1460,:]\nX_validate = combine_df.iloc[1460:,:]\nprint(\"X shape: {}, X_validate shape: {}\".format(X.shape,X_validate.shape))","8be9cc7e":"X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.33, random_state=0)","a2850941":"lr_model = linear_model.LinearRegression()\nlr_model.fit(X_train, y_train)\ny_pred_lr = np.expm1(lr_model.predict(X_test))\nrmse = metrics.mean_squared_error(y_pred_lr,np.expm1(y_test))\nr2 = metrics.r2_score(y_pred_lr,np.expm1(y_test))\nprint(\"rmse: {}, r2: {} \".format(rmse, r2))","5791e8f8":"lasso_lambdas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1]\n\nlasso_model = linear_model.Lasso()\nlasso_grid = model_selection.GridSearchCV(estimator=lasso_model, param_grid=dict(alpha=lasso_lambdas))\nlasso_grid.fit(X_train,y_train).fit(X_train, y_train)\nprint(lasso_grid.best_estimator_)\ny_pred_lasso = np.expm1(lasso_grid.predict(X_test))\nrmse = np.sqrt(metrics.mean_squared_error(y_pred_lasso,np.expm1(y_test)))\nr2 = metrics.r2_score(y_pred_lasso,np.expm1(y_test))\nprint(\"rmse: {}, r2: {} \".format(rmse, r2))","3f07d1f2":"ridge_lambdas = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60]\n\nridge_model = linear_model.Ridge()\nridge_grid = model_selection.GridSearchCV(estimator=ridge_model, param_grid=dict(alpha=ridge_lambdas))\nridge_grid.fit(X_train,y_train)\nprint(ridge_grid.best_estimator_)\ny_pred_ridge = np.expm1(ridge_grid.predict(X_test))\nrmse = np.sqrt(metrics.mean_squared_error(y_pred_ridge,np.expm1(y_test)))\nr2 = metrics.r2_score(y_pred_ridge,np.expm1(y_test))\nprint(\"rmse: {}, r2: {} \".format(rmse, r2))","2342778e":"from sklearn.ensemble import BaggingRegressor\n\nbagging_model = BaggingRegressor(base_estimator = ridge_grid.best_estimator_, n_estimators = 400,\\\n                                max_samples = 200, random_state = 0)\n\nbagging_model.fit(X_train, y_train)\ny_pred_bagging = np.expm1(bagging_model.predict(X_test))\nrmse = np.sqrt(metrics.mean_squared_error(y_pred_bagging,np.expm1(y_test)))\nr2 = metrics.r2_score(y_pred_bagging,np.expm1(y_test))\nprint(\"rmse: {}, r2: {} \".format(rmse, r2))","3db3d427":"gBoost_model = ensemble.GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\ngBoost_model.fit(X_train,y_train)\ny_pred_gBoost = np.expm1(gBoost_model.predict(X_test))\nrmse = np.sqrt(metrics.mean_squared_error(y_pred_gBoost,np.expm1(y_test)))\nr2 = metrics.r2_score(y_pred_gBoost,np.expm1(y_test))\nprint(\"rmse: {}, r2: {} \".format(rmse, r2))","dde5395e":"\nparams = {'min_child_weight':[4,5], 'gamma':[i\/10.0 for i in range(3,6)],  'subsample':[i\/10.0 for i in range(6,11)],\n'colsample_bytree':[i\/10.0 for i in range(6,11)], 'max_depth': [2,3,4]}\n\n# xgb_model = XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n#                              learning_rate=0.05, max_depth=3, \n#                              min_child_weight=1.7817, n_estimators=2200,\n#                              reg_alpha=0.4640, reg_lambda=0.8571,\n#                              subsample=0.5213, silent=1,\n#                              random_state =7, nthread = -1)\n\nxgb_model = XGBRegressor(nthread=-1)           \nxgb_grid_model = model_selection.GridSearchCV(estimator=xgb_model, param_grid=params)\nxgb_grid_model.fit(X_train,y_train)\nprint(xgb_grid_model.best_estimator_)\ny_pred_xgb = np.expm1(xgb_grid_model.predict(X_test))\nrmse = np.sqrt(metrics.mean_squared_error(y_pred_xgb,np.expm1(y_test)))\nr2 = metrics.r2_score(y_pred_xgb_regressor,np.expm1(y_test))\nprint(\"rmse: {}, r2: {} \".format(rmse, r2))","f4eb1c94":"params = dict(learning_rate = [0.1, 0.05, 0.001, 0.0001], n_estimators = [50,70, 100, 200, 300],\n                          loss = ['linear','exponential','square'])\n\nadab_model = ensemble.AdaBoostRegressor(base_estimator= ridge_grid.best_estimator_, random_state=0)\nadab_grid_model = model_selection.GridSearchCV(estimator=adab_model, param_grid=params)\nadab_grid_model.fit(X_train,y_train)\nprint(adab_grid_model.best_estimator_)\ny_pred_ada_grid = np.expm1(adab_grid_model.predict(X_test))\nrmse = np.sqrt(metrics.mean_squared_error(y_pred_ada_grid,np.expm1(y_test)))\nr2 = metrics.r2_score(y_pred_ada_grid,np.expm1(y_test))\nprint(\"rmse: {}, r2: {} \".format(rmse, r2))","455fe380":"**Missing Values!**"}}