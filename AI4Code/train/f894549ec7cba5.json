{"cell_type":{"3ea9303a":"code","700d7958":"code","348b2f55":"code","bedbe0bc":"code","9869aad2":"code","5dc807e4":"code","43f0284b":"code","44d16b2c":"code","7222e1ba":"code","fdeb98ab":"code","b229b8fe":"code","5dba9430":"code","b1e12f54":"code","e1c1b9e0":"code","920a236c":"code","eb48f8da":"code","313474e6":"code","c78b26c8":"code","c2e8cf97":"code","f0e1d091":"code","a78a4b4f":"code","b4868ad5":"code","f3b38a72":"code","5b1b94ae":"code","2b7a9347":"code","17a70a90":"code","16eacfe9":"code","df64e743":"code","96bcfa16":"code","65f0f5d2":"code","ae8dc118":"code","b8bd212f":"code","9940da96":"code","3b28ebf5":"code","2fa241b7":"code","4e55d6ba":"code","953deed4":"code","14a3c4e2":"markdown","c961cf2b":"markdown","b7e11961":"markdown","8b8bb366":"markdown","c92f13c5":"markdown","eed2d6b8":"markdown","1413571f":"markdown","3f025591":"markdown","b851aeaf":"markdown","54cc12a3":"markdown","2def0a6d":"markdown","fab2af91":"markdown","b1f59556":"markdown","f550d31e":"markdown"},"source":{"3ea9303a":"# always start with checking out the files!\n!ls ..\/input\/*","700d7958":"# Basic packages\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport random as rd # generating random numbers\nimport datetime # manipulating date formats\n# Viz\nimport matplotlib.pyplot as plt # basic plotting\nimport seaborn as sns # for prettier plots\n\n\n# TIME SERIES\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom pandas.plotting import autocorrelation_plot\nfrom statsmodels.tsa.stattools import adfuller, acf, pacf,arma_order_select_ic\nimport statsmodels.formula.api as smf\nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm\nimport scipy.stats as scs\n\n\n# settings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n","348b2f55":"# Import all of them \nsales=pd.read_csv(\"..\/input\/sales_train.csv\")\n\n# settings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nitem_cat=pd.read_csv(\"..\/input\/item_categories.csv\")\nitem=pd.read_csv(\"..\/input\/items.csv\")\nsub=pd.read_csv(\"..\/input\/sample_submission.csv\")\nshops=pd.read_csv(\"..\/input\/shops.csv\")\ntest=pd.read_csv(\"..\/input\/test.csv\")","bedbe0bc":"#formatting the date column correctly\nsales.date=sales.date.apply(lambda x:datetime.datetime.strptime(x, '%d.%m.%Y'))\n# check\nprint(sales.info())","9869aad2":"# Aggregate to monthly level the required metrics\n\nmonthly_sales=sales.groupby([\"date_block_num\",\"shop_id\",\"item_id\"])[\n    \"date\",\"item_price\",\"item_cnt_day\"].agg({\"date\":[\"min\",'max'],\"item_price\":\"mean\",\"item_cnt_day\":\"sum\"})\n\n## Lets break down the line of code here:\n# aggregate by date-block(month),shop_id and item_id\n# select the columns date,item_price and item_cnt(sales)\n# Provide a dictionary which says what aggregation to perform on which column\n# min and max on the date\n# average of the item_price\n# sum of the sales","5dc807e4":"# take a peak\nmonthly_sales.head(20)","43f0284b":"# number of items per cat \nx=item.groupby(['item_category_id']).count()\nx=x.sort_values(by='item_id',ascending=False)\nx=x.iloc[0:10].reset_index()\nx\n# #plot\nplt.figure(figsize=(8,4))\nax= sns.barplot(x.item_category_id, x.item_id, alpha=0.8)\nplt.title(\"Items per Category\")\nplt.ylabel('# of items', fontsize=12)\nplt.xlabel('Category', fontsize=12)\nplt.show()","44d16b2c":"ts=sales.groupby([\"date_block_num\"])[\"item_cnt_day\"].sum()\nts.astype('float')\nplt.figure(figsize=(16,8))\nplt.title('Total Sales of the company')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nplt.plot(ts);","7222e1ba":"plt.figure(figsize=(16,6))\nplt.plot(ts.rolling(window=12,center=False).mean(),label='Rolling Mean');\nplt.plot(ts.rolling(window=12,center=False).std(),label='Rolling sd');\nplt.legend();","fdeb98ab":"import statsmodels.api as sm\n# multiplicative\nres = sm.tsa.seasonal_decompose(ts.values,freq=12,model=\"multiplicative\")\n#plt.figure(figsize=(16,12))\nfig = res.plot()\n#fig.show()","b229b8fe":"# Additive model\nres = sm.tsa.seasonal_decompose(ts.values,freq=12,model=\"additive\")\n#plt.figure(figsize=(16,12))\nfig = res.plot()\n#fig.show()","5dba9430":"# R version ported into python  \n\n# alas ! rpy2 does not exist in Kaggle kernals :( \n# from rpy2.robjects import r\n# def decompose(series, frequency, s_window, **kwargs):\n#     df = pd.DataFrame()\n#     df['date'] = series.index\n#     s = [x for x in series.values]\n#     length = len(series)\n#     s = r.ts(s, frequency=frequency)\n#     decomposed = [x for x in r.stl(s, s_window, **kwargs).rx2('time.series')]\n#     df['observed'] = series.values\n#     df['trend'] = decomposed[length:2*length]\n#     df['seasonal'] = decomposed[0:length]\n#     df['residual'] = decomposed[2*length:3*length]\n#     return df","b1e12f54":"# Stationarity tests\ndef test_stationarity(timeseries):\n    \n    #Perform Dickey-Fuller test:\n    print('Results of Dickey-Fuller Test:')\n    dftest = adfuller(timeseries, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput)\n\ntest_stationarity(ts)\n","e1c1b9e0":"# to remove trend\nfrom pandas import Series as Series\n# create a differenced series\ndef difference(dataset, interval=1):\n    diff = list()\n    for i in range(interval, len(dataset)):\n        value = dataset[i] - dataset[i - interval]\n        diff.append(value)\n    return Series(diff)\n\n# invert differenced forecast\ndef inverse_difference(last_ob, value):\n    return value + last_ob\n\n","920a236c":"ts=sales.groupby([\"date_block_num\"])[\"item_cnt_day\"].sum()\nts.astype('float')\nplt.figure(figsize=(16,16))\nplt.subplot(311)\nplt.title('Original')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nplt.plot(ts)\nplt.subplot(312)\nplt.title('After De-trend')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nnew_ts=difference(ts)\nplt.plot(new_ts)\nplt.plot()\n\nplt.subplot(313)\nplt.title('After De-seasonalization')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nnew_ts=difference(ts,12)       # assuming the seasonality is 12 months long\nplt.plot(new_ts)\nplt.plot()","eb48f8da":"# now testing the stationarity again after de-seasonality\ntest_stationarity(new_ts)","313474e6":"def tsplot(y, lags=None, figsize=(10, 8), style='bmh',title=''):\n    if not isinstance(y, pd.Series):\n        y = pd.Series(y)\n    with plt.style.context(style):    \n        fig = plt.figure(figsize=figsize)\n        #mpl.rcParams['font.family'] = 'Ubuntu Mono'\n        layout = (3, 2)\n        ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n        acf_ax = plt.subplot2grid(layout, (1, 0))\n        pacf_ax = plt.subplot2grid(layout, (1, 1))\n        qq_ax = plt.subplot2grid(layout, (2, 0))\n        pp_ax = plt.subplot2grid(layout, (2, 1))\n        \n        y.plot(ax=ts_ax)\n        ts_ax.set_title(title)\n        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax, alpha=0.5)\n        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax, alpha=0.5)\n        sm.qqplot(y, line='s', ax=qq_ax)\n        qq_ax.set_title('QQ Plot')        \n        scs.probplot(y, sparams=(y.mean(), y.std()), plot=pp_ax)\n\n        plt.tight_layout()\n    return ","c78b26c8":"# Simulate an AR(1) process with alpha = 0.6\nnp.random.seed(1)\nn_samples = int(1000)\na = 0.6\nx = w = np.random.normal(size=n_samples)\n\nfor t in range(n_samples):\n    x[t] = a*x[t-1] + w[t]\nlimit=12    \n_ = tsplot(x, lags=limit,title=\"AR(1)process\")","c2e8cf97":"# Simulate an AR(2) process\n\nn = int(1000)\nalphas = np.array([.444, .333])\nbetas = np.array([0.])\n\n# Python requires us to specify the zero-lag value which is 1\n# Also note that the alphas for the AR model must be negated\n# We also set the betas for the MA equal to 0 for an AR(p) model\n# For more information see the examples at statsmodels.org\nar = np.r_[1, -alphas]\nma = np.r_[1, betas]\n\nar2 = smt.arma_generate_sample(ar=ar, ma=ma, nsample=n) \n_ = tsplot(ar2, lags=12,title=\"AR(2) process\")","f0e1d091":"# Simulate an MA(1) process\nn = int(1000)\n# set the AR(p) alphas equal to 0\nalphas = np.array([0.])\nbetas = np.array([0.8])\n# add zero-lag and negate alphas\nar = np.r_[1, -alphas]\nma = np.r_[1, betas]\nma1 = smt.arma_generate_sample(ar=ar, ma=ma, nsample=n) \nlimit=12\n_ = tsplot(ma1, lags=limit,title=\"MA(1) process\")","a78a4b4f":"# Simulate MA(2) process with betas 0.6, 0.4\nn = int(1000)\nalphas = np.array([0.])\nbetas = np.array([0.6, 0.4])\nar = np.r_[1, -alphas]\nma = np.r_[1, betas]\n\nma3 = smt.arma_generate_sample(ar=ar, ma=ma, nsample=n)\n_ = tsplot(ma3, lags=12,title=\"MA(2) process\")","b4868ad5":"# Simulate an ARMA(2, 2) model with alphas=[0.5,-0.25] and betas=[0.5,-0.3]\nmax_lag = 12\n\nn = int(5000) # lots of samples to help estimates\nburn = int(n\/10) # number of samples to discard before fit\n\nalphas = np.array([0.8, -0.65])\nbetas = np.array([0.5, -0.7])\nar = np.r_[1, -alphas]\nma = np.r_[1, betas]\n\narma22 = smt.arma_generate_sample(ar=ar, ma=ma, nsample=n, burnin=burn)\n_ = tsplot(arma22, lags=max_lag,title=\"ARMA(2,2) process\")","f3b38a72":"# pick best order by aic \n# smallest aic value wins\nbest_aic = np.inf \nbest_order = None\nbest_mdl = None\n\nrng = range(5)\nfor i in rng:\n    for j in rng:\n        try:\n            tmp_mdl = smt.ARMA(arma22, order=(i, j)).fit(method='mle', trend='nc')\n            tmp_aic = tmp_mdl.aic\n            if tmp_aic < best_aic:\n                best_aic = tmp_aic\n                best_order = (i, j)\n                best_mdl = tmp_mdl\n        except: continue\n\n\nprint('aic: {:6.5f} | order: {}'.format(best_aic, best_order))\n","5b1b94ae":"#\n# pick best order by aic \n# smallest aic value wins\nbest_aic = np.inf \nbest_order = None\nbest_mdl = None\n\nrng = range(5)\nfor i in rng:\n    for j in rng:\n        try:\n            tmp_mdl = smt.ARMA(new_ts.values, order=(i, j)).fit(method='mle', trend='nc')\n            tmp_aic = tmp_mdl.aic\n            if tmp_aic < best_aic:\n                best_aic = tmp_aic\n                best_order = (i, j)\n                best_mdl = tmp_mdl\n        except: continue\n\n\nprint('aic: {:6.5f} | order: {}'.format(best_aic, best_order))\n","2b7a9347":"# Simply use best_mdl.predict() to predict the next values","17a70a90":"# adding the dates to the Time-series as index\nts=sales.groupby([\"date_block_num\"])[\"item_cnt_day\"].sum()\nts.index=pd.date_range(start = '2013-01-01',end='2015-10-01', freq = 'MS')\nts=ts.reset_index()\nts.head()","16eacfe9":"from fbprophet import Prophet\n#prophet reqiures a pandas df at the below config \n# ( date column named as DS and the value column as Y)\nts.columns=['ds','y']\nmodel = Prophet( yearly_seasonality=True) #instantiate Prophet with only yearly seasonality as our data is monthly \nmodel.fit(ts) #fit the model with your dataframe","df64e743":"# predict for five months in the furure and MS - month start is the frequency\nfuture = model.make_future_dataframe(periods = 5, freq = 'MS')  \n# now lets make the forecasts\nforecast = model.predict(future)\nforecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()","96bcfa16":"model.plot(forecast)","65f0f5d2":"model.plot_components(forecast)","ae8dc118":"total_sales=sales.groupby(['date_block_num'])[\"item_cnt_day\"].sum()\ndates=pd.date_range(start = '2013-01-01',end='2015-10-01', freq = 'MS')\n\ntotal_sales.index=dates\ntotal_sales.head()","b8bd212f":"# get the unique combinations of item-store from the sales data at monthly level\nmonthly_sales=sales.groupby([\"shop_id\",\"item_id\",\"date_block_num\"])[\"item_cnt_day\"].sum()\n# arrange it conviniently to perform the hts \nmonthly_sales=monthly_sales.unstack(level=-1).fillna(0)\nmonthly_sales=monthly_sales.T\ndates=pd.date_range(start = '2013-01-01',end='2015-10-01', freq = 'MS')\nmonthly_sales.index=dates\nmonthly_sales=monthly_sales.reset_index()\nmonthly_sales.head()","9940da96":"nodeToForecast.head()","3b28ebf5":"monthly_shop_sales=sales.groupby([\"date_block_num\",\"shop_id\"])[\"item_cnt_day\"].sum()\n# get the shops to the columns\nmonthly_shop_sales=monthly_shop_sales.unstack(level=1)\nmonthly_shop_sales=monthly_shop_sales.fillna(0)\nmonthly_shop_sales.index=dates\nmonthly_shop_sales=monthly_shop_sales.reset_index()\nmonthly_shop_sales.head()","2fa241b7":"start_time=time.time()\n\n# Calculating the base forecasts using prophet\n# From HTSprophet pachage -- https:\/\/github.com\/CollinRooney12\/htsprophet\/blob\/master\/htsprophet\/hts.py\nforecastsDict = {}\nfor node in range(len(monthly_shop_sales)):\n    # take the date-column and the col to be forecasted\n    nodeToForecast = pd.concat([monthly_shop_sales.iloc[:,0], monthly_shop_sales.iloc[:, node+1]], axis = 1)\n#     print(nodeToForecast.head())  # just to check\n# rename for prophet compatability\n    nodeToForecast = nodeToForecast.rename(columns = {nodeToForecast.columns[0] : 'ds'})\n    nodeToForecast = nodeToForecast.rename(columns = {nodeToForecast.columns[1] : 'y'})\n    growth = 'linear'\n    m = Prophet(growth, yearly_seasonality=True)\n    m.fit(nodeToForecast)\n    future = m.make_future_dataframe(periods = 1, freq = 'MS')\n    forecastsDict[node] = m.predict(future)\n    ","4e55d6ba":"#predictions = np.zeros([len(forecastsDict[0].yhat),1]) \nnCols = len(list(forecastsDict.keys()))+1\nfor key in range(0, nCols-1):\n    f1 = np.array(forecastsDict[key].yhat)\n    f2 = f1[:, np.newaxis]\n    if key==0:\n        predictions=f2.copy()\n       # print(predictions.shape)\n    else:\n       predictions = np.concatenate((predictions, f2), axis = 1)","953deed4":"predictions_unknown=predictions[-1]\npredictions_unknown","14a3c4e2":"Of course, there is a lot more that we can explore in this dataset, but let's dive into the time-series part.\n\n# Single series:\n\nThe objective requires us to predict sales for the next month at a store-item combination.\n\nSales over time of each store-item is a time-series in itself. Before we dive into all the combinations, first let's understand how to forecast for a single series.\n\nI've chosen to predict for the total sales per month for the entire company.\n\nFirst let's compute the total sales per month and plot that data.\n","c961cf2b":"## Basics of TS:\n\nCollation of different basic concepts of the different traditional time-series models and some basic intuition behind them\n\n## Objective:\nThis kernel was made to serve as repository of various time-series concepts for beginners and I hope it would be useful as a refresher to some of the experts too :)\n\n## Table of contents:\n* Competition and data overview\n* Imports ( data and packages )\n* Basic exploration\/EDA\n* Single time-series \n    * Stationarity\n    * Seasonality , Trend and Remainder\n    * AR , MA , ARMA , ARIMA\n    * Selecting P and Q using AIC\n    * ETS\n    * Prophet \n    * UCM\n* Hierarchical time-series\n    * Bottom's up\n    * AHP\n    * PHA \n    * FP \n    \n    \n## Competition and data overview:\n\nIn this playground competition, we are provided with the challenge of predicting total sales for every product and store in the next month for Russian Software company-[1c company](http:\/\/1c.ru\/eng\/title.htm). \n\n**What does the IC company do?:**\n\n1C: Enterprise 8 system of programs is intended for automation of everyday enterprise activities: various business tasks of economic and management activity, such as management accounting, business accounting, HR management, CRM, SRM, MRP, MRP, etc.\n\n**Data**:\nWe are provided with daily sales data for each store-item combination, but our task is to predict sales at a monthly level.\n\n## Imports:\n","b7e11961":"## AR(2) process -- has ACF tailing out and PACF cutting off at lag=2","8b8bb366":"## We've correctly identified the order of the simulated process as ARMA(2,2). \n\n### Lets use it for the sales time-series.\n","c92f13c5":"## Foot-notes:\n\nI'm not a stats major, so please do let me know in the comments if you feel that I've left out any important technique or if there was any mistake in the content.\n\nI plan to add another kernel about Time-series here which would be about adapting the open-source solutions from the recent time-series competitions ( Favorita, Recruit,etc. ) to this playground dataset.\n\nDo leave a comment\/upvote :) ","eed2d6b8":"## MA(1) process -- has ACF cut off at lag=1","1413571f":"## MA(2) process -- has ACF cut off at lag=2","3f025591":"**Quick observations:**\nThere is an obvious \"seasonality\" (Eg: peak sales around a time of year) and a decreasing \"Trend\".\n\nLet's check that with a quick decomposition into Trend, seasonality and residuals.\n","b851aeaf":"## AR(1) process -- has ACF tailing out and PACF cutting off at lag=1","54cc12a3":"Awesome. The trend and seasonality from Prophet look similar to the ones that we had earlier using the traditional methods.\n\n## UCM:\n\nUnobserved Components Model. The intuition here is similar to that of the prophet. The model breaks down the time-series into its components, trend, seasonal, cycle and regresses them and then predicts the next point for the components and then combines them.\n\nUnfortunately, I could not find a good package\/code that can perform this model in Python :( \n\nR version of UCM: https:\/\/bicorner.com\/2015\/12\/28\/unobserved-component-models-in-r\/\n\n# Hierarchical time series:\n\nThe [Forecasting: principles and practice](https:\/\/www.otexts.org\/fpp\/9\/4) , is the ultimate reference book for forecasting by Rob J Hyndman.\n\nHe lays out the fundamentals of dealing with grouped or Hierarchical forecasts. Consider the following simple scenario.\n\n![](https:\/\/www.otexts.org\/sites\/default\/files\/resize\/fpp\/images\/hts1-550x274.png)\n\nHyndman proposes the following methods to estimate the points in this hierarchy. I've tried to simplify the language to make it more intuitve.\n\n### Bottom up approach:\n* Predict all the base level series using any method, and then just aggregate it to the top.\n* Advantages: Simple , No information is lost due to aggregation.\n* Dis-advantages: Lower levels can be noisy\n\n### Top down approach:\n* Predict the top level first. (Eg: predict total sales first)\n* Then calculate **weights** that denote the proportion of the total sales that needs to be given to the base level forecast(Eg:) the contribution of the item's sales to the total sales \n* There are different ways of arriving at the \"weights\". \n    * **Average Historical Proportions** - Simple average of the item's contribution to sales in the past months\n    * **Proportion of historical averages** - Weight is the ratio of average value of bottom series by the average value of total series (Eg: Weight(item1)= mean(item1)\/mean(total_sales))\n    * **Forecasted Proportions** - Predict the proportion in the future using changes in the past proportions\n* Use these weights to calcuate the base -forecasts and other levels\n\n### Middle out:\n* Use both bottom up and top down together.\n* Eg: Consider our problem of predicting store-item level forecasts.\n    * Take the middle level(Stores) and find forecasts for the stores\n    * Use bottoms up approach to find overall sales\n    * Dis-integrate store sales using proportions to find the item-level sales using a top-down approach\n    \n### Optimal combination approach:\n* Predict for all the layers independently\n* Since, all the layers are independent, they might not be consistent with hierarchy\n    * Eg: Since the items are forecasted independently, the sum of the items sold in the store might not be equal to the forecasted sale of store  or as Hyndman puts it \u201caggregate consistent\u201d\n* Then some matrix calculations and adjustments happen to provide ad-hoc adjustments to the forecast to make them consistent with the hierarchy\n\n\n### Enough with the theory. Lets start making forecasts! :P\nThe problem at hand here, has 22170 items and 60 stores . This indicates that there can be around a **million** individual time-series(item-store combinations) that we need to predict!\n\nConfiguring each of them would be nearly impossible. Let's use Prophet which does it for us.\n\nStarting off with the bottoms up approach.\n\nThere are some other points to consider here: \n* Not all stores sell all items\n* What happens when a new product is introduced? \n* What if a product is removed off the shelves?","2def0a6d":"we assume an additive model, then we can write\n\n> yt=St+Tt+Et \n\nwhere yt is the data at period t, St is the seasonal component at period t, Tt is the trend-cycle component at period tt and Et is the remainder (or irregular or error) component at period t\nSimilarly for Multiplicative model,\n\n> yt=St  x Tt x Et \n\n## Stationarity:\n\n![q](https:\/\/static1.squarespace.com\/static\/53ac905ee4b003339a856a1d\/t\/5818f84aebbd1ac01c275bac\/1478031479192\/?format=750w)\n\nStationarity refers to time-invariance of a series. (ie) Two points in a time series are related to each other by only how far apart they are, and not by the direction(forward\/backward)\n\nWhen a time series is stationary, it can be easier to model. Statistical modeling methods assume or require the time series to be stationary.\n\n\nThere are multiple tests that can be used to check stationarity.\n* ADF( Augmented Dicky Fuller Test) \n* KPSS \n* PP (Phillips-Perron test)\n\nLet's just perform the ADF which is the most commonly used one.\n\nNote: [Step by step guide to perform dicky fuller test in Excel](http:\/\/www.real-statistics.com\/time-series-analysis\/stochastic-processes\/dickey-fuller-test\/)\n\n[Another Useful guide](http:\/\/www.blackarbs.com\/blog\/time-series-analysis-in-python-linear-models-to-garch\/11\/1\/2016#AR) \n\n[good reference](https:\/\/github.com\/ultimatist\/ODSC17\/blob\/master\/Time%20Series%20with%20Python%20(ODSC)%20STA.ipynb)\n","fab2af91":"### Now after the transformations, our p-value for the DF test is well within 5 %. Hence we can assume Stationarity of the series\n\nWe can easily get back the original series using the inverse transform function that we have defined above.\n\nNow let's dive into making the forecasts!\n\n# AR, MA and ARMA models:\nTL: DR version of the models:\n\nMA - Next value in the series is a function of the average of the previous n number of values\nAR - The errors(difference in mean) of the next value is a function of the errors in the previous n number of values\nARMA - a mixture of both.\n\nNow, How do we find out, if our time-series in AR process or MA process?\n\nLet's find out!","b1f59556":"## Now things get a little hazy. Its not very clear\/straight-forward.\n\nA nifty summary of the above plots:\n\nACF Shape\t| Indicated Model |\n-- | -- |\nExponential, decaying to zero |\tAutoregressive model. Use the partial autocorrelation plot to identify the order of the autoregressive model |\nAlternating positive and negative, decaying to zero\tAutoregressive model. |  Use the partial autocorrelation plot to help identify the order. |\nOne or more spikes, rest are essentially zero | Moving average model, order identified by where plot becomes zero. |\nDecay, starting after a few lags |\tMixed autoregressive and moving average (ARMA) model. | \nAll zero or close to zero | Data are essentially random. |\nHigh values at fixed intervals | Include seasonal autoregressive term. |\nNo decay to zero |\tSeries is not stationary |\n\n\n## Let's use a systematic approach to finding the order of AR and MA processes.","f550d31e":"# Prophet: \n\nRecently open-sourced by Facebook research. It's a very promising tool, that is often a very handy and quick solution to the frustrating **flatline** :P\n\n![FLATLINE](https:\/\/i.stack.imgur.com\/fWzyX.jpg)\n\nSure, one could argue that with proper pre-processing and carefully tuning the parameters the above graph would not happen. \n\nBut the truth is that most of us don't either have the patience or the expertise to make it happen.\n\nAlso, there is the fact that in most practical scenarios- there is often a lot of time-series that needs to be predicted.\nEg: This competition. It requires us to predict the next month sales for the **Store - item level combinations** which could be in the thousands.(ie) predict 1000s of parameters!\n\nAnother neat functionality is that it follows the typical **sklearn** syntax.\n\nAt its core, the Prophet procedure is an additive regression model with four main components:\n* A piecewise linear or logistic growth curve trend. Prophet automatically detects changes in trends by selecting changepoints from the data.\n* A yearly seasonal component modeled using Fourier series.\n* A weekly seasonal component using dummy variables.\n* A user-provided list of important holidays.\n\n**Resources for learning more about prophet:**\n* https:\/\/www.youtube.com\/watch?v=95-HMzxsghY\n* https:\/\/facebook.github.io\/prophet\/docs\/quick_start.html#python-api\n* https:\/\/research.fb.com\/prophet-forecasting-at-scale\/\n* https:\/\/blog.exploratory.io\/is-prophet-better-than-arima-for-forecasting-time-series-fa9ae08a5851"}}