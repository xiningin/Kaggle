{"cell_type":{"2d095550":"code","d4d1c207":"code","b3e508d9":"code","bead23fb":"code","feb7e7ea":"code","e0bd78be":"code","81e5c854":"code","b9d2170f":"code","d08dede7":"code","e2e9b563":"code","360f5044":"code","9b016f16":"code","ec89451b":"code","57fd51ba":"code","9e6e1510":"code","8e54a165":"code","0b6d387e":"code","055816e0":"markdown","2b5d52ac":"markdown","734d5305":"markdown","7a9dc3c0":"markdown","bd52eee5":"markdown","f9461abb":"markdown"},"source":{"2d095550":"# import libraries\nimport numpy as np\nimport torch\nimport torchvision \nimport torchvision.transforms as transforms\nimport torch.utils.data as data\nfrom torch.utils.data.sampler import SubsetRandomSampler as SRS\nimport torch.nn as nn\nimport torch.nn.functional as F","d4d1c207":"#default parameters\ndata_dir= '\/kaggle\/working\/MNIST\/'\nval_per=0.2\nout_dir='\/kaggle\/working\/output\/'\nlearning_rate = 0.0001\noptimizer= 'SGD'\nbatch_size = 128\nnum_hidn_layers = 2\nneurons = [256,256]\nnon_linear_function = 'relu'\nloss='categorical_cross_entropy'\nnum_workers = 0","b3e508d9":"train_data = torchvision.datasets.MNIST(root='\/kaggle\/working', train=True,\n                                       download=True, transform=transforms.ToTensor())\ntest_data = torchvision.datasets.MNIST(root='\/kaggle\/working', train=False,\n                                       download=True, transform=transforms.ToTensor()\n)\n","bead23fb":"# Using Sampler from Pytorch utils to extract the validation set. If not using\n# pytorch sklearn is also a better option for this.\n\n#. Step1: get the indices\ntotal_train = len(train_data)\nindexs = list(range(total_train))\nnp.random.shuffle(indexs)\n\n# #Step 2: Create the indices based on the valid percentage\n# split_indices = int(np.floor(val_per*total_train))\n# train_split, val_split = indexs[:-split_indices], indexs[-split_indices: ]\n\n# #step3: Get batches from the sampler\n# train_sampler = SRS(train_split)\n# val_sampler = SRS(val_split)","feb7e7ea":"train_sampler[0]","e0bd78be":"# Prepare data loaders\n# train_loader =  data.DataLoader(train_data, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers )\n# # val_loader = data.DataLoader(train_data, batch_size=batch_size, sampler=val_sampler, num_workers=num_workers )\n# test_loader = data.DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n\ntrain_loader =  data.DataLoader(train_data, batch_size=batch_size,shuffle=True, num_workers=num_workers )\n# val_loader = data.DataLoader(train_data, batch_size=batch_size, sampler=val_sampler, num_workers=num_workers )\ntest_loader = data.DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)","81e5c854":"type(val_loader)","b9d2170f":"import matplotlib.pyplot as plt\n%matplotlib inline\n\n# train_data_itr = iter(train_loader)\nfig = plt.figure(figsize=(16,16))\nfor idx, (imgs, labels) in enumerate(train_loader):\n    if idx < 8:\n        im = imgs[0,0,...].numpy()\n        sp = plt.subplot(4,4, idx+1)\n        sp.axis('Off')\n        plt.imshow(im, cmap='gray')\nplt.show()","d08dede7":"class MLP(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(28*28, neurons[0]) \n#         self.fc2 = nn.Linear(neurons[0], neurons[1]) \n#         self.dropout = nn.Dropout(0.2)\n        self.out = nn.Linear(neurons[1], 10)\n    \n    def forward(self, x):\n        # flatten image input\n        x = x.view(-1,28*28)\n        # add hidden layer, with relu activation function\n        x = F.relu(self.fc1(x))\n        # add dropout layer\n#         x = self.dropout(x)\n         # add hidden layer, with relu activation function\n#         x = F.relu(self.fc2(x))\n#         # add dropout layer\n#         x = self.droput(x)\n        # add output layer\n        x = self.out(x)\n        # there is no need for a activation function at the last layer. SInce Pytorch Loss functions internally takes care of it.\n        # https:\/\/discuss.pytorch.org\/t\/activation-function-for-last-layer\/41151\n        return x\n\nmodel = MLP()\nprint(model)\n    ","e2e9b563":"if torch.cuda.is_available():\n    print(\"Found GPU\")\n    use_device = torch.device(\"cuda:0\")\n    print(f\"Number of GPU's: {torch.cuda.device_count()}\")\nelse:\n    print(\"Found CPU\")\n    use_device = torch.device(\"cpu\")","360f5044":"# PAss the model to the GPU\nmodel.to(use_device)","9b016f16":"def num_parameters_to_train():\n    from prettytable import PrettyTable as PT\n    tp, tl = 0, 1\n    for i, v in enumerate(model.named_parameters()):\n        parameter, l_name = v[1], v[0]\n        if not v[1].requires_grad: continue\n        param = parameter.numel()\n        tl += 1\n#         print(l_name)\n#         table.add_row([l_name, param])\n        tp += param\n    print(f\"Layers: {int(tl\/2)} \\nTotal Param: {tp}\")\n    \n# 2nd method\n#     print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n#     pri\nnum_parameters_to_train()\n","ec89451b":"entropy_loss = nn.CrossEntropyLoss() # Multi-class classification 0-9\n\n#optimizer for back propagation\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","57fd51ba":"# Train the network\n\nepochs = 200\nvalid_loss_min = np.Inf # intial loss\n\nfor epoch in range(epochs):\n    train_loss = valid_loss = 0\n    \n    # Ready for training\n    model.train()\n    running_loss = 0.0\n    for i, batch in enumerate(train_loader):\n        # intialize all gradients to zero\n        optimizer.zero_grad()\n        img, label = batch\n        img = img.to(use_device)\n        label = label.to(use_device)\n        # Forward Propogation: Pss input to the network\n        prediction = model(img)\n        \n        # compute loss\n        loss = entropy_loss(prediction,label)\n        \n        # back propagate\n        loss.backward()\n        \n        # Update the gradients with new calculated values using SGD\n        optimizer.step()\n        \n        # Update training loss\n        running_loss += loss.item()* img.size(0)\n#         running_loss += loss.item()\n#         print(i)\n#         if i % 300 == 0:    # print every 2000 mini-batches\n#             print('[%d, %5d] loss: %.3f' %\n#                   (epoch + 1, i + 1, running_loss \/ 300))\n    print('[%d] loss: %.3f' % (epoch + 1, running_loss))\n#     print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1,train_loss))\n\n#         Validation\n        # Read model for evaluation\n#         model.eval()\n#         print(type(val_loader))\n#         for img, label in val_loader:\n#             val_pred = model(img)\n#             loss = entropy_loss(val_pred, label)\n            \n#             # Update loss for validation\n#             valid_loss = loss.item()*img.size(0)\n            \n#             # Output the validation and training stats\n#             train_loss = train_loss \/ len(train_loader.sampler)\n#             valid_loss = valid_loss \/ len(val_loader.sampler)\n            \n#             print('Epoch')\n#             print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n#             epoch+1, \n#             train_loss,\n#             valid_loss\n#             ))\n    \n#             # save model if validation loss has decreased\n#             if valid_loss <= valid_loss_min:\n#                 print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n#                 valid_loss_min,\n#                 valid_loss))\n#                 torch.save(model.state_dict(), 'model.pt')\n#                 valid_loss_min = valid_loss","9e6e1510":"model.load_state_dict(torch.load('model.pt'))","8e54a165":"# initialize lists to monitor test loss and accuracy\ntest_loss = 0.0\nclass_correct = list(0. for i in range(10))\nclass_total = list(0. for i in range(10))\nmodel.eval() # prep model for evaluation\nfor data, tst_labels in test_loader:\n    # forward pass: compute predicted outputs by passing inputs to the model\n    predictions = model(data)\n    # calculate the loss\n    loss = entropy_loss(predictions, tst_labels)\n    # update test loss \n    test_loss += loss.item()*data.size(0)\n    # convert output probabilities to predicted class\n    _, pred = torch.max(predictions, 1)\n    # compare predictions to true label\n    correct = np.squeeze(pred.eq(tst_labels.data.view_as(pred)))\n    # calculate test accuracy for each object class\n    for i in range(len(tst_labels)):\n        label = tst_labels.data[i]\n        class_correct[label] += correct[i].item()\n        class_total[label] += 1\n# calculate and print avg test loss\ntest_loss = test_loss\/len(test_loader.sampler)\nprint('Test Loss: {:.6f}\\n'.format(test_loss))\nfor i in range(10):\n    if class_total[i] > 0:\n        print('Test Accuracy of %5s: %2d%% (%2d\/%2d)' % (\n            str(i), 100 * class_correct[i] \/ class_total[i],\n            np.sum(class_correct[i]), np.sum(class_total[i])))\n    else:\n        print('Test Accuracy of %5s: N\/A (no training examples)' % (classes[i]))\nprint('\\nTest Accuracy (Overall): %2d%% (%2d\/%2d)' % (\n    100. * np.sum(class_correct) \/ np.sum(class_total),\n    np.sum(class_correct), np.sum(class_total)))","0b6d387e":"# obtain one batch of test images\ndataiter = iter(test_loader)\nimages, labels = dataiter.next()\n# get sample outputs\noutput = model(images)\n# convert output probabilities to predicted class\n_, preds = torch.max(output, 1)\n# prep images for display\nimages = images.numpy()\n# plot the images in the batch, along with predicted and true labels\nfig = plt.figure(figsize=(25, 4))\nfor idx in np.arange(20):\n    ax = fig.add_subplot(2, 20\/2, idx+1, xticks=[], yticks=[])\n    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n    ax.set_title(\"{} ({})\".format(str(preds[idx].item()), str(labels[idx].item())),\n                 color=(\"green\" if preds[idx]==labels[idx] else \"red\"))","055816e0":"**Define MLP Network**","2b5d52ac":"**Create Dataset: MNIST**","734d5305":"**Specifying loss function and optimizer**","7a9dc3c0":"# DATA VISUALIZATION","bd52eee5":"**Get total numbers of Parameters to train**","f9461abb":"**Get validation from the training dataset**\nValidation set is extracted from the training dataset and it is \"x\"% of the training dataset."}}