{"cell_type":{"67b55063":"code","71c780c9":"code","44254208":"code","22a734e2":"code","0d7647a7":"code","3d729938":"code","cf37f5b5":"code","c4bdcce0":"code","a41ea3b1":"code","e8940826":"code","af75cc00":"code","7dbf4357":"code","23b7ccb6":"code","e12ceaaf":"code","3edad4d9":"code","7391c4ac":"code","8689af07":"code","75a35f79":"code","a1d5368a":"code","d8c690ea":"code","0a9b9687":"code","c58bdd4f":"code","57414e48":"code","eb6287c5":"code","a2b0bdf4":"code","9e4d7a65":"code","a0d21a1d":"code","35c9441f":"code","8df64f68":"code","26d0f453":"code","2eadcbef":"markdown","f38508ce":"markdown","a9c5a58d":"markdown","f63663bd":"markdown","80564c54":"markdown","06ab09da":"markdown","1f9cc62d":"markdown","76d6b84e":"markdown","28a26bcf":"markdown","a554b23f":"markdown","b2cf6900":"markdown","4b5757b8":"markdown","95c325b3":"markdown","39b2c3f4":"markdown","c2b06425":"markdown"},"source":{"67b55063":"! pip install -U tensorboard_plugin_profile","71c780c9":"from datetime import datetime\nfrom packaging import version\n\nimport os","44254208":"import tensorflow as tf","22a734e2":"import tensorflow_datasets as tfds","0d7647a7":"(train, test), dataset_info = tfds.load(\n    'mnist',\n    split =['train', 'test'],\n    shuffle_files=True,\n    as_supervised=True,\n    with_info=True )","3d729938":"def rescale(image, label):\n    return tf.cast(image, tf.float32) \/ 255., label\n# rescaling the image\ntrain  = train.map(rescale)\ntest = test.map(rescale)\n# Batching the datasets\ntrain = train.batch(128)\ntest = test.batch(128)","cf37f5b5":"# Creating not so cool model :)\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n  tf.keras.layers.Dense(256,activation='relu'),\n  tf.keras.layers.Dense(10, activation='softmax')\n])\nmodel.compile(\n    loss='sparse_categorical_crossentropy',\n    optimizer=tf.keras.optimizers.Adam(0.001),\n    metrics=['accuracy']\n)","c4bdcce0":"logs = \"logs\/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\ntboard_callback = tf.keras.callbacks.TensorBoard(log_dir = logs,\n                                                 histogram_freq = 1,\n                                                 profile_batch = '500,520')\n\nmodel.fit(train,\n          epochs=2,\n          validation_data=test,\n          callbacks = [tboard_callback])","a41ea3b1":"#Load the TensorBoard notebook extension.\n%load_ext tensorboard","e8940826":"# Launch TensorBoard and navigate to the Profile tab to view performance profile\n%tensorboard --logdir=logs","af75cc00":"#again loading the datasets.\n(train, test), dataset_info = tfds.load(\n    'mnist',\n    split =['train', 'test'],\n    shuffle_files=True,\n    as_supervised=True,\n    with_info=True )\n\n# Creating the optimized input pipeline.\ndef rescale(image, label):\n    return tf.cast(image, tf.float32) \/ 255., label\n# rescaling the image\ntrain  = train.map(rescale)\ntest = test.map(rescale)\n\n\ntrain = train.batch(128)\n# applying cache in training set\ntrain = train.cache()\n# applying prefetching in training sets\ntrain = train.prefetch(tf.data.experimental.AUTOTUNE)\n\n\ntest = test.batch(128)\n# applying cache in test set\ntest = test.cache()\n# applying prefetching in test set\ntest = test.prefetch(tf.data.experimental.AUTOTUNE)","7dbf4357":"# again making and training the not so cool model but this time we'are using the optimised training\n\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n  tf.keras.layers.Dense(256,activation='relu'),\n  tf.keras.layers.Dense(10, activation='softmax')\n])\nmodel.compile(\n    loss='sparse_categorical_crossentropy',\n    optimizer=tf.keras.optimizers.Adam(0.001),\n    metrics=['accuracy']\n)\n\nmodel.fit(train,\n          epochs=2,\n          validation_data=test,\n          callbacks = [tboard_callback])","23b7ccb6":"# Launching the tensorboard.\n%tensorboard --logdir=logs","e12ceaaf":"# downloading tf_flowers it contains 5 classes of 5 differebt flowers species.\n\ntrain, train_info = tfds.load('tf_flowers', split='train[:80%]', \n                              as_supervised=True, \n                              with_info=True)\nval, val_info = tfds.load('tf_flowers', \n                          split='train[80%:]', \n                          as_supervised=True, \n                          with_info=True)","3edad4d9":"# preprocessing and making input pipeline\ndef resize(img, lbl):\n  img_size = 224\n  return (tf.image.resize(img, [img_size, img_size])\/255.) , lbl\n\ntrain = train.map(resize)\nval = val.map(resize)\n\ntrain = train.batch(32, drop_remainder=True)\nval = val.batch(32, drop_remainder=True)","7391c4ac":"# function to create resnet model with random weights and imagenet weights\ndef resnet(imagenet_weights=False):\n    num_classes = 5\n    if imagenet_weights is False:\n        return tf.keras.applications.ResNet50(include_top=True, \n                                        input_shape=(224, 224, 3), \n                                        weights=None, \n                                        classes=num_classes)\n    if imagenet_weights is True:\n        resnet = tf.keras.applications.ResNet50(include_top=False, \n                                        input_shape=(224, 224, 3), \n                                        weights='imagenet', \n                                        )\n        resnet.trainable = True\n        return  tf.keras.Sequential([resnet, \n                                     tf.keras.layers.GlobalAvgPool2D(), \n                                     tf.keras.layers.Dense(5, activation='softmax')])","8689af07":"# training the model in gpu if availiable\ndef try_gpu(i=0): \n    if len(tf.config.experimental.list_physical_devices('GPU')) >= i + 1:\n        return tf.device(f'\/GPU:{i}')\n    return tf.device('\/CPU:0')\ndevice_name = try_gpu()._device_name\nstrategy = tf.distribute.OneDeviceStrategy(device_name)","75a35f79":"with strategy.scope():\n  model = resnet(imagenet_weights=False)\n\nmodel.compile(optimizer='adam', \n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\ncallback = callback = tf.keras.callbacks.EarlyStopping(patience=8)\nhistory = model.fit(train, \n                    epochs=100, \n                    validation_data=val,\n                    callbacks = [callback])","a1d5368a":"# Saving validation accuracy ib the variable val_acc_1\nval_acc_1 = history.history['val_accuracy']\n","d8c690ea":"with strategy.scope():\n  model = resnet(imagenet_weights=True)\n\n# compiling and training.\nmodel.compile(optimizer='adam', \n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\ncallback = callback = tf.keras.callbacks.EarlyStopping(patience=8)\nhistory = model.fit(train, \n                    epochs=100, \n                    validation_data=val,\n                    callbacks = [callback])","0a9b9687":"# Saving validation accuracy of this model in the variable val_acc_2\nval_acc_2 = history.history['val_accuracy']","c58bdd4f":"print(\"Validation accuracy of first approach is {} VS Validation accuracy of secon approach is {}\".format(max(val_acc_1), max(val_acc_2)))","57414e48":"# loading the tf_flowers dataset\ntrain, train_info = tfds.load('tf_flowers', split='train[:80%]', \n                              as_supervised=True, \n                              with_info=True)\nval, val_info = tfds.load('tf_flowers', \n                          split='train[80%:]', \n                          as_supervised=True, \n                          with_info=True)","eb6287c5":"#function to augment the images\ndef augment(image,label):\n  image = tf.image.resize_with_crop_or_pad(image, 34, 34) # Add 6 pixels of padding\n  image = tf.image.random_brightness(image, max_delta=0.5) # Random brightness\n  image = tf.image.random_flip_up_down(image)  # Randomily flips the image up and down \n\n\n  return image,label","a2b0bdf4":"train = train.map(augment).map(resize).batch(32, drop_remainder=True).cache().prefetch(tf.data.experimental.AUTOTUNE)\nval = val.map(resize).batch(32, drop_remainder=True).cache().prefetch(tf.data.experimental.AUTOTUNE)","9e4d7a65":"for x,y in train.take(1):\n    print(x.shape)","a0d21a1d":"def resnet_transfer_learning():\n    # including_top=False means we're the last layer of resnet will not include fully connected dense layer\n    resnet = tf.keras.applications.ResNet50(include_top=False, \n                                        input_shape=(224, 224, 3), \n                                        weights='imagenet', \n                                        )\n    # freezing the layers of resnet.\n    resnet.trainable = True\n    # adding the dense layer to the resnet model\n    return  tf.keras.Sequential([resnet, \n                                     tf.keras.layers.GlobalAvgPool2D(), \n                                     tf.keras.layers.Dense(5, activation='softmax')])","35c9441f":"with strategy.scope():\n  model = resnet_transfer_learning()\n\nmodel.compile(optimizer='adam', \n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\ncallback = callback = tf.keras.callbacks.EarlyStopping(patience=8)\nhistory = model.fit(train, \n                    epochs=100, \n                    validation_data=val,\n                    callbacks = [callback])","8df64f68":"# Saving validation accuracy of this model in the variable val_acc_2\nval_acc_3 = history.history['val_accuracy']","26d0f453":"print(\"Validation accuracy of first approach is {} VS Validation accuracy of second approach is {} VS Validation accuracy of second approach is {}\".format(max(val_acc_1), max(val_acc_2), max(val_acc_3)))","2eadcbef":"Tensorflow Profiler is introduced in this year's TF Dev Summit. This is the same tool that Google's use for profiling their models\nInternally. And they made this tool Public in thhis year's Dev Summmit. \nSo, the TensorFlow Profiler provides a set of tools that you can use to measure the training performance and resource consumption of your TensorFlow models. And now it has been integerated in TensorBoard.","f38508ce":"We're doing below optimisation techiniques:\n\n1. Caching: The tf.data.Dataset.cache transformation can cache a dataset, either in memory or on local storage. This will save some operations (like file opening and data reading) from being executed during each epoch\n2. Prefetching: it overlaps the preprocessing and model execution of a training step. While the model is executing training step s, the input pipeline is reading the data for step s+1. Doing so reduces the step time to the maximum (as opposed to the sum) of the training and the time it takes to extract the data.\n\nNow, let's us put this methods into actions","a9c5a58d":"Now we'll create a TensorBoard callback for computing the performance of the model and then we'll call it while training","f63663bd":"### Now let's move to next section of the notebook, here we'll be seeing what is the practise to initialise the weights of the models.\n\nFirst we're going to train a resnet model from sractch by initialiing the weights ","80564c54":"In this section we're applying data some augmentation techniques and use transfer learning to see the impact of this technique in our validation accuracy.","06ab09da":"Now let's compare the results of all three aproaches.","1f9cc62d":"After launching the tensorboard, switch over the profile tab.\nThere you can see It's recommending some steps for optimisation.\nIt's saying \"Your program is HIGHLY input-bound because 77.4% of the total step time sampled is waiting for input. Therefore, you should first focus on reducing the input time. \"\n\nSo, we've to optimize our input pipeline. Let's appply some optimising techniques of td.data.Datasets.\n\n![Screenshot%20%28137%29.png](attachment:Screenshot%20%28137%29.png)","76d6b84e":"        **In this kernel I'm showing you some best practices in Deep Learning using TF 2x.\n        The things I'm going to show you in this kernel are:**\n        1. Introducing Tensorflow Profiler( Basics(How to analyze input pipelines)).\n        2. Then we're going to see what is the best practise to initialize weights of a model.\n        3. Transfer learning and effects of data Augmentation in accuracy.\n        ","28a26bcf":"In this kernel I'm going to introduce the TF profiler. We're going to see how to use TF profiler to Debug the Input Pipeline  and then we'll optimize it. We're going to use simplest dataset(Offcourse mnist) and a dummy model. \n\nFirst we'll train the dummy model and then we'll inspect it's training input pipeline using TF Profiler & finally we'll optimize it.\n\nLet's load the mnist dataset using Tensorflow Datasets.","a554b23f":"Now let's train the model again, but this time we're going to initialize the \nweights of our resnet model to the imagenet weights(trained on the imagenet datasets).","b2cf6900":"But this time you can see, it's saying our program is not input bound. Thanks to TF Profiler :) .\n![Screenshot%20%28139%29.png](attachment:Screenshot%20%28139%29.png)","4b5757b8":"Comparing the results of the validation accuracy of the first model whose weights are initialized randomly and the model which weights are not initialized randomly but with the weights of some model which is trained\non similiar datasets(here imagenet).","95c325b3":"Refrences: Tensorflow official documentations.","39b2c3f4":"You can clearly see the accuracy of second approach is much more higher than the first one ","c2b06425":"In this kernel we've first see the introduction of TF profiler and then we trained\nthe resnet50 model with scratch by initializing the model weights randomaly vs initializing the model weights with the wieghts of some\nmodel which is trained on similiar tasks. And then we compare the results.\nAtlast we used image augmentation technique and used transfered learning and we comapared the results of these 3 approaches.\n\nAtlast try fine tuning your model, use hyperparameter tuning and schedule the learning rates."}}