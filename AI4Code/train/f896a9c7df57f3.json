{"cell_type":{"dd8c4e5e":"code","939cb8a4":"code","39c7a898":"code","ae9a94f5":"code","591e0b8e":"code","8aaf8ce1":"code","95cce1b9":"code","9c5d2087":"code","60b83205":"code","110d9fa4":"code","20ba0b6f":"code","9e47ae5d":"code","2f6434b2":"code","9567eb1e":"code","478a1663":"code","8dc05620":"code","6f4f1fd7":"code","5c19f245":"code","c9c3c815":"code","2f7a46e6":"code","3eed5bc1":"code","eabdeccd":"code","7ea88550":"code","a32ac004":"code","9280ac3e":"code","fb4e8d88":"code","22aabd16":"code","11b0236b":"code","3fb0f7ab":"code","93ac48bb":"code","fbc7b0f4":"code","3ee4a4bf":"code","84f5cc80":"code","da327c4e":"code","dfc4fde6":"code","db4f1083":"code","dbac6af3":"code","f5f00c26":"code","74cb9bd4":"code","d0b301bf":"code","48341471":"code","ecebbe7b":"code","ccad2d0c":"markdown","5a8834c6":"markdown","a17aa87c":"markdown","bc52b1ff":"markdown","3ff07719":"markdown","7bb82fc2":"markdown"},"source":{"dd8c4e5e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","939cb8a4":"###Importing all the libraries needed \nimport numpy as np \nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n###improting visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pandas import read_csv\n\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.impute import  KNNImputer\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import f_classif,SelectKBest\nfrom sklearn.utils import resample\nimport category_encoders as ce\n###Import Keras to build the network\nfrom keras.callbacks import EarlyStopping\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Activation\nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import to_categorical\nfrom keras.optimizers import Adam","39c7a898":"##Loading the train and testdata\ntrain = pd.read_csv(\"\/kaggle\/input\/widsdatathon2021\/TrainingWiDS2021.csv\")\nprint(train.shape)\ntest = pd.read_csv(\"\/kaggle\/input\/widsdatathon2021\/UnlabeledWiDS2021.csv\")\nprint(test.shape)","ae9a94f5":"##Exploring the data. Column descriptions are provided DataDictionaryWiDS2021 file\n\ntrain.head()\n\n","591e0b8e":"##Exploring the test data\ntest.head()","8aaf8ce1":"##Check the positive and negative diabetic cases on the training set\nsns.countplot(train['diabetes_mellitus'])\nplt.show()\n","95cce1b9":"\n###distribution of dataset by race\ndata = train.groupby(['ethnicity']).count()['encounter_id']\nlabels = data.keys()\n#Using matplotlib\npie, ax = plt.subplots(figsize=[10,6])\n\nplt.pie(x=data, autopct=\"%.1f%%\",  labels=labels, pctdistance=0.5)\nplt.title(\"Ethnicity\", fontsize=14);\n\n\n","9c5d2087":"##Adjusting chart size\n%matplotlib inline\nplt.rcParams['figure.figsize'] = [10, 5]","60b83205":"sns.countplot(x=\"ethnicity\", data=train,hue = \"diabetes_mellitus\")","110d9fa4":"sns.countplot(x=\"gender\", data=train,hue = \"diabetes_mellitus\")","20ba0b6f":"##Dropping unwanted columns from the train and test set\n\nX=train.drop(['diabetes_mellitus','Unnamed: 0','encounter_id','hospital_id','icu_id'], axis=1)\ntest_model=test.drop(['Unnamed: 0','encounter_id','hospital_id','icu_id'], axis=1)\n\n## label column \ny = train['diabetes_mellitus']","9e47ae5d":"X = X.drop(['weight','height'],axis =1)\ntest_model= test_model.drop(['weight','height'],axis = 1 )","2f6434b2":"\n##Thank you #WIDS DATATHON 2020 for the code. We can convert BMI to category with this code\n\ndef bmiCategory(bmi):\n    if bmi < 18.5:\n        return \"underweight\"\n    elif bmi < 24.9:\n        return \"normal\"\n    elif bmi < 29.9:\n        return \"overweight\"\n    else:\n        return \"obese\"\n\nX[\"bmi_category\"] = X[\"bmi\"].apply(bmiCategory)\ntest_model[\"bmi_category\"] = test_model[\"bmi\"].apply(bmiCategory)\nX[\"bmi_category\"].value_counts(normalize=True).plot(kind='bar')\n","9567eb1e":"\nX = X.drop(['bmi'],axis =1)\ntest_model= test_model.drop(['bmi'],axis = 1 )","478a1663":"train_model = X.copy()\nlabel_model = y.copy()","8dc05620":"##List the categorical features\ncategorical_features=[]\nfor c in train_model.columns:\n    col_type = train_model[c].dtype\n    if col_type == 'object' or col_type.name == 'category':\n        train_model[c] = train_model[c].astype('category')\n        categorical_features.append(c)\nprint (categorical_features)","6f4f1fd7":"###Correlated features are just redundant information so removing them from the dataset.\n\ncorr_features = set()  # Set of all the names of correlated columns\ncorr_matrix = train_model.corr()\nfor i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > 0.99: # we are interested in absolute coeff value\n                colname = corr_matrix.columns[i]  # getting the name of column\n                corr_features.add(colname)\n\ncorr_features","5c19f245":"train_model_corr=train_model.drop(corr_features,axis=1)\n\ntest_model_corr=test_model.drop(corr_features,axis=1)\n","c9c3c815":"train_model_corr.shape","2f7a46e6":"##If a feature has about 80% of  missing information, then that column can be excluded .\nall_data_na = train_model_corr.isnull().sum() \/train_model_corr.shape[0] * 100.00\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\n\nColumns_with_Missing_Values = all_data_na[all_data_na >= 80].index\n","3eed5bc1":"Columns_with_Missing_Values","eabdeccd":"###Validate a column\ntrain_model_corr['h1_bilirubin_max'].value_counts(dropna=False).head()","7ea88550":"train_model_corr=  train_model_corr.drop(['h1_bilirubin_max', 'h1_albumin_max', 'h1_lactate_max',\n       'h1_lactate_min', 'h1_pao2fio2ratio_min', 'h1_pao2fio2ratio_max',\n       'h1_arterial_ph_min', 'h1_arterial_ph_max', 'h1_arterial_pco2_min',\n       'h1_arterial_pco2_max', 'h1_arterial_po2_min', 'h1_arterial_po2_max',\n       'h1_hco3_max', 'h1_wbc_max', 'h1_calcium_max', 'h1_calcium_min',\n       'h1_platelets_max', 'h1_bun_max', 'h1_diasbp_invasive_min',\n       'h1_diasbp_invasive_max', 'h1_sysbp_invasive_min',\n       'h1_sysbp_invasive_max', 'h1_creatinine_max', 'h1_mbp_invasive_min',\n       'h1_mbp_invasive_max'],axis = 1)\n\ntest_model_corr=  test_model_corr.drop(['h1_bilirubin_max', 'h1_albumin_max', 'h1_lactate_max',\n       'h1_lactate_min', 'h1_pao2fio2ratio_min', 'h1_pao2fio2ratio_max',\n       'h1_arterial_ph_min', 'h1_arterial_ph_max', 'h1_arterial_pco2_min',\n       'h1_arterial_pco2_max', 'h1_arterial_po2_min', 'h1_arterial_po2_max',\n       'h1_hco3_max', 'h1_wbc_max', 'h1_calcium_max', 'h1_calcium_min',\n       'h1_platelets_max', 'h1_bun_max', 'h1_diasbp_invasive_min',\n       'h1_diasbp_invasive_max', 'h1_sysbp_invasive_min',\n       'h1_sysbp_invasive_max', 'h1_creatinine_max', 'h1_mbp_invasive_min',\n       'h1_mbp_invasive_max'],axis = 1)","a32ac004":"##group the columns by data type and impute missing values accordingly\nfeatu_int=[]\nfeatu_float=[]\nfeatu_obj=[]\nfor col in train_model_corr.columns:\n    x=train_model_corr[col].dtype\n    if x=='int64' or x == 'int32':\n        train_model[col] = train_model_corr[col].astype('int32')\n        featu_int.append(col)\n    elif x=='float64' or x== 'float32':\n        train_model[col] = train_model_corr[col].astype('float32')\n        featu_float.append(col)\n    else:\n        featu_obj.append(col)","9280ac3e":"###Fill in the missing values\nimputer_cat = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\nimputer_int = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer_float = SimpleImputer(missing_values=np.nan, strategy='mean')\n# Train\ntrain_model_corr[featu_float] = imputer_float.fit_transform(train_model_corr[featu_float]) \ntrain_model_corr[featu_int] = imputer_int.fit_transform(train_model_corr[featu_int]).astype(int) \ntrain_model_corr[featu_obj] = imputer_cat.fit_transform(train_model_corr[featu_obj])\n# Test\ntest_model_corr[featu_float] = imputer_float.transform(test_model_corr[featu_float]) \ntest_model_corr[featu_int] = imputer_int.transform(test_model_corr[featu_int]).astype(int) \ntest_model_corr[featu_obj] = imputer_cat.transform(test_model_corr[featu_obj])   ","fb4e8d88":"train_model_corr.head()","22aabd16":"# creating instance of labelencoder\n#labelencoder = LabelEncoder()\n#train_model_corr = pd.get_dummies(train_model_corr, columns = featu_obj)\n","11b0236b":"#test_model_corr = pd.get_dummies(test_model_corr, columns = featu_obj)","3fb0f7ab":"##I have used Catbooster to encode categorical values .\n\ntarget_enc = ce.CatBoostEncoder(cols=featu_obj)\ntarget_enc.fit(train_model_corr[featu_obj], label_model)\n\n# Transform the features, rename columns with _cb suffix, and join to dataframe\ntrain_model_corr = train_model_corr.join(target_enc.transform(train_model_corr[featu_obj]).add_suffix('_cb'))\ntest_model_corr  = test_model_corr.join(target_enc.transform(train_model_corr[featu_obj]).add_suffix('_cb'))\n\n","93ac48bb":"train_model_corr.shape","fbc7b0f4":"#['ethnicity', 'gender', 'hospital_admit_source', 'icu_admit_source', 'icu_stay_type', 'icu_type']\ntrain_model_corr = train_model_corr.drop(['ethnicity','gender','bmi_category','hospital_admit_source','icu_admit_source','icu_stay_type','icu_type'], axis= 1)\ntest_model_corr = test_model_corr.drop(['ethnicity','gender','bmi_category','hospital_admit_source','icu_admit_source','icu_stay_type','icu_type'], axis= 1)","3ee4a4bf":"train_model_corr.shape\n","84f5cc80":"###We dont want to scale binary\/categorical values, so set a threshold of 2 unique values\n##############################################################################\nfeatures_info = pd.DataFrame()\nfeatures_info['unique values'] = train_model_corr.nunique()\n\n# Select the columns to scale (we don't want to scale binary and dummy features)\ncolumns_to_scale = features_info[features_info['unique values'] > 2].index.values\ncolumns_to_scale","da327c4e":"#\nscaler = preprocessing.MinMaxScaler()\nscaled_columns = scaler.fit_transform(train_model_corr[columns_to_scale])\nscaled_features_df = pd.DataFrame(scaled_columns, index=train_model_corr.index, columns=columns_to_scale)\ntrain_model_corr = train_model_corr.drop(columns_to_scale, axis=1)\ntrain_model_corr = train_model_corr.join(scaled_features_df)\n\nscaled_columns = scaler.fit_transform(test_model_corr[columns_to_scale])\ntest_scaled_features_df = pd.DataFrame(scaled_columns, index=test_model_corr.index, columns=columns_to_scale)\ntest_model_corr = test_model_corr.drop(columns_to_scale, axis=1)\ntest_model_corr = test_model_corr.join(test_scaled_features_df)\n\n\n","dfc4fde6":"\n#scaler = preprocessing.StandardScaler()\n#scaled_columns = scaler.fit_transform(train_model_corr[columns_to_scale])\n#scaled_features_df = pd.DataFrame(scaled_columns, index=train_model_corr.index, columns=columns_to_scale)\n#train_model_corr = train_model_corr.drop(columns_to_scale, axis=1)\n#train_model_corr = train_model_corr.join(scaled_features_df)\n\n\n#scaled_columns = scaler.fit_transform(test_model_corr[columns_to_scale])\n#test_scaled_features_df = pd.DataFrame(scaled_columns, index=test_model_corr.index, columns=columns_to_scale)\n#test_model_corr = test_model_corr.drop(columns_to_scale, axis=1)\n#test_model_corr = test_model_corr.join(test_scaled_features_df)\n","db4f1083":"train_model_enc = train_model_corr.copy()\ntest_model_enc = test_model_corr.copy()","dbac6af3":"## I have used feature selector to pick only the top 100 features \n\nk = 100\nfeature_selector = SelectKBest(f_classif, k=k)  \nfeature_selector.fit_transform(train_model_enc, y)\nbest_features_indexes = feature_selector.get_support(indices=True)\nX_top_features = train_model_enc.iloc[:, best_features_indexes]\nX_top_features.columns\ntest_top_features = test_model_enc.iloc[:, best_features_indexes]\n","f5f00c26":"#X_train, X_test, y_train, y_test = train_test_split(X_top_features, label_model, test_size=0.3, random_state=1)","74cb9bd4":"def create_model():\n    # define the keras model\n    model = Sequential()\n    model.add(Dense(50,activation = 'relu', input_shape=(100,)))\n    model.add(Dense(25, activation='relu'))\n    model.add(Dropout(0.1))\n    model.add(Dense(10, activation='relu'))\n    model.add(Dropout(0.1))\n    model.add(Dense(1,activation='sigmoid'))\n        # compile the keras model\n    adam = Adam()\n    model.compile(loss='binary_crossentropy', optimizer =  adam, metrics=['accuracy'] )\n    return model\n\n    \n","d0b301bf":"\n# Instantiate the cross validator\nskf = StratifiedKFold(n_splits=10, shuffle=True)\n# Loop through the indices the split() method returns\nprint(skf.get_n_splits(train_model_enc, label_model.values))\nyfull_train = dict()\nnum_fold = 0\nsum_score = 0\nmodels = []\nfor train_index, test_index in skf.split(X_top_features,label_model.values): \n   # print(\"Training on fold \" + str(index+1) + \"\/10...\")\n    print(train_index, test_index)\n    # Generate batches from indices\n    #upsampled_data, upsampled_label_model,\n    xtrain, xval = X_top_features.iloc[train_index], X_top_features.iloc[test_index]\n    ytrain, yval = label_model.iloc[train_index], label_model.iloc[test_index]\n    num_fold += 1\n    print('Start KFold number {} from {}'.format(num_fold, 10))\n    print('Split train: ', len(xtrain), len(ytrain))\n    print('Split valid: ', len(xval), len(yval))\n    # Clear model, and create it\n    history = None\n    model = create_model()\n    es = EarlyStopping(patience=5)\n    # Debug message I guess\n    history = model.fit( xtrain, ytrain,batch_size=32, epochs=20, verbose=2,callbacks=[es], validation_data=(xval, yval))\n    \n    predictions_valid = model.predict(xval, batch_size=32, verbose=2)\n    score = log_loss(yval, predictions_valid)\n    print('Score log_loss: ', score)\n    sum_score += score*len(test_index)\n\n        # Store valid predictions\n    for i in range(len(test_index)):\n        yfull_train[test_index[i]] = predictions_valid[i]\n\n    models.append(model)\n","48341471":"score = sum_score\/len(train_model_enc)\nprint(\"Log_loss train independent avg: \", score)\n","ecebbe7b":"num_fold = 0\nyfull_test = []\ntest_id = []\nnfolds = len(models)\n\nfor i in range(nfolds):\n    model = models[i]\n    num_fold += 1\n    print('Start KFold number {} from {}'.format(num_fold, nfolds))\n    #test_data, test_id = read_and_normalize_test_data()\n    test_prediction = model.predict(test_top_features, batch_size=32, verbose=2)\n    yfull_test.append(test_prediction)\n\n    #test_res = merge_several_folds_mean(yfull_test, nfolds)\n    #info_string = 'loss_' + info_string \\ + '_folds_' + str(nfolds)\na = np.array(yfull_test[0])\nfor i in range(1, nfolds):\n    a += np.array(yfull_test[i])\n    a \/= nfolds\ntest_res = a.tolist()\n\n#create_submission\n#predictions_new = np.where(predictions_t > 0.3 , 1, 0)\n\ntest[\"diabetes_mellitus\"]=pd.DataFrame(test_res) \nfinal_df=test[[\"encounter_id\",\"diabetes_mellitus\"]]\nfinal_df.to_csv(\"results_f_2.csv\",index=False)","ccad2d0c":"Looks the classes are highly imbalanced.","5a8834c6":"Since we have the BMI column. Height and Weight are redundant information. So we can drop them.","a17aa87c":"# **Goal**:\nThe goal is to determine whether a patient admitted to an ICU has been diagnosed with a particular type of diabetes, Diabetes Mellitus using data from the first 24 hours of intensive care\n\n# **Data Provided**\n5 files have been provided\n\nTrainingWiDS2021.csv - the training data. \nUnlabeledWiDS2021.csv - the unlabeled data (data without diabetes_mellitus provided). \nSampleSubmissionWiDS2021.csv - a sample submission file\nSolutionTemplateWiDS2021.csv - a list of all the rows (and encounters) that should be in your submissions.\nDataDictionaryWiDS2021.csv - supplemental information about the data.\n\n\nApproach:\n\nThis is a binary classification problem and I have tried to solve using deep learning technique.\n","bc52b1ff":"# **# Feature Selection and Extraction**","3ff07719":"# Features Scaling","7bb82fc2":"# Initial Data Exploration"}}