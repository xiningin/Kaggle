{"cell_type":{"a4fb48fb":"code","3596b238":"code","6bc41c50":"code","afd816e9":"code","7923820b":"code","67c66d01":"code","436aa8b1":"code","b3476573":"code","c83cbfd0":"code","322c3b6d":"code","703533c9":"code","fd9ae277":"code","95f84d47":"code","44c6e7da":"code","cd7dfdeb":"code","1695fb42":"code","5bbed3f1":"code","cba03912":"code","d108cced":"code","4b9395eb":"code","cfe2e3c7":"code","2282e869":"code","82c0481e":"code","64c4d9a0":"code","c10ecf29":"code","417f287d":"code","939b32f7":"code","b0bc10cb":"code","1e9a964c":"code","1b9d364d":"code","bdf9f9bb":"code","69aeed85":"code","dd25b2d5":"code","16dd0877":"code","9ccdff68":"code","8c6ae1f9":"code","f1ceebd3":"code","eb77c4b0":"code","f637caf0":"markdown","ef565625":"markdown","971c666b":"markdown","328931db":"markdown"},"source":{"a4fb48fb":"import numpy\nimport pandas as pd\nimport torch\nimport torch.nn as nn","3596b238":"train_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","6bc41c50":"train_data.head()","afd816e9":"train_data.shape,test_data.shape","7923820b":"print(train_data.iloc[0:4, [0,1,2,3,4,-2,-1]])","67c66d01":"all_features = pd.concat((train_data.iloc[:,1:-1], test_data.iloc[:, 1:]))","436aa8b1":"all_features.head()","b3476573":"all_features.dtypes","c83cbfd0":"all_features.dtypes[all_features.dtypes=='int64']","322c3b6d":"numeric_features = all_features.dtypes[all_features.dtypes!='object'].index","703533c9":"numeric_features","fd9ae277":"all_features[numeric_features] = all_features[numeric_features].apply(lambda x : (x-x.mean())\/x.std())","95f84d47":"all_features[numeric_features][:5]","44c6e7da":"all_features[numeric_features] = all_features[numeric_features].fillna(0)","cd7dfdeb":"all_features = pd.get_dummies(all_features, dummy_na=True)\nall_features[:5]","1695fb42":"train_data.iloc[:,-1], train_data.SalePrice","5bbed3f1":"n_train = len(train_data)\n\ntrain_features = torch.tensor(all_features[:n_train].values, dtype=torch.float32)\ntest_features = torch.tensor(all_features[n_train:].values, dtype=torch.float32)\n\ntrain_labels = torch.tensor(train_data.iloc[:,-1].values, dtype=torch.float32)","cba03912":"loss = nn.MSELoss()\n\nin_features = train_features.shape[1]\nout_features = 1\n\ndef get_net():\n    net = nn.Sequential(nn.Linear(in_features,out_features))\n    return net","d108cced":"def log_rmse(net, features, labels):\n    clipped_preds = torch.clamp(net(features), 1, float('inf'))\n#     print(clipped_preds, labels)\n    rmse = torch.sqrt(loss(torch.log(clipped_preds), torch.log(labels)))\n    return rmse.item()\n    ","4b9395eb":"def load_array(data_array, batch_size):\n    train_dataset = torch.utils.data.TensorDataset(*(data_array))\n    train_dataloader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n    return train_dataloader\n    ","cfe2e3c7":"def train(net, train_features, train_labels, test_features, test_labels, num_epochs, learning_rate, weight_decay, batch_size):\n    train_ls , test_ls = [],[]\n    train_iter = load_array((train_features, train_labels), batch_size)\n    optimizer = torch.optim.Adam(net.parameters(), lr = learning_rate, weight_decay=weight_decay)\n    \n    for epoch in range(num_epochs):\n        for X, y in train_iter:\n            y_hat = net(X)\n#             print(y_hat.shape, y.shape)\n            l = loss(y_hat,y.unsqueeze(1))\n            optimizer.zero_grad()\n            l.backward()\n            optimizer.step()\n        train_ls.append(log_rmse(net, train_features, train_labels))\n        if (test_labels is not None):\n            test_ls.append(log_rmse(net, test_features, test_labels))\n    return train_ls, test_ls\n            ","2282e869":"net = get_net()\nnet","82c0481e":"net(train_features).detach()","64c4d9a0":"test_labels = None\nnum_epochs = 100\nlearning_rate = 0.03\nweight_decay=0\nbatch_size = 64\n# train(net, train_features, train_labels, test_features, test_labels, num_epochs, learning_rate, weight_decay, batch_size)","c10ecf29":"# K fold validation\n\n# getting the ith slice\ndef get_k_fold_data(K, i, X, y):\n    X_train, X_valid, y_train, y_valid = None, None, None, None\n    len_of_slice = len(X)\/\/K\n    \n    for j in range(K):\n        index = slice(j * len_of_slice, (j+1) * len_of_slice)\n        if j==i:\n            X_valid, y_valid =  X[index,:], y[index]\n        elif X_train is None:\n            X_train, y_train = X[index, :], y[index]\n        else:\n            X_train = torch.cat([X_train, X[index,:]], 0)\n            y_train = torch.cat([y_train, y[index]], 0)\n    \n    return X_train, y_train, X_valid, y_valid","417f287d":"for X, y in load_array((train_features, train_labels), batch_size):\n    print(get_k_fold_data(5, 2, X, y))\n    break","939b32f7":"import matplotlib.pyplot as plt\n\ndef k_fold(k, X_train, y_train, num_epochs, learning_rate, weight_decay, batch_size):\n    train_l_sum, valid_l_sum = 0, 0\n    for i in range(k):\n        data = get_k_fold_data(k, i, X_train, y_train )\n        \n        net = get_net()\n        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate, weight_decay, batch_size)\n        \n        train_l_sum += train_ls[-1]\n        valid_l_sum += valid_ls[-1]\n        \n        if i == 0:\n            plt.plot(list(range(1, num_epochs + 1)), train_ls)\n            plt.plot(list(range(1, num_epochs + 1)), valid_ls)\n            plt.show()\n            \n        print(f'fold {i + 1}, train log rmse {float(train_ls[-1]):f}, '\n              f'valid log rmse {float(valid_ls[-1]):f}')\n    return train_l_sum \/ k, valid_l_sum \/ k\n        \n    ","b0bc10cb":"k, num_epochs, lr, weight_decay, batch_size = 5, 100, 5, 0, 64\n\ntrain_l, valid_l = k_fold(k, train_features, train_labels, num_epochs, learning_rate, weight_decay, batch_size)\nprint(f'{k}-fold validation: avg train log rmse: {float(train_l):f}, '\n      f'avg valid log rmse: {float(valid_l):f}')","1e9a964c":"def train_and_pred(train_features, test_feature, train_labels, test_data,\n                   num_epochs, lr, weight_decay, batch_size):\n    net = get_net()\n    train_ls, _ = train(net, train_features, train_labels, None, None, num_epochs, lr, weight_decay, batch_size)\n\n    plt.plot(range(1, num_epochs +1 ),train_ls)\n    plt.show()\n    print(f\"train log rmse : {float(train_ls[-1])}\")\n    \n    preds = net(test_features).detach().numpy()\n    print(preds, preds.reshape(1,-1))\n    test_data['SalePrice'] = pd.Series(preds.reshape(1,-1)[0])\n    submission = pd.concat([test_data['Id'], test_data['SalePrice']], axis=1)\n    submission.to_csv('submission.csv', index=False)\n    ","1b9d364d":"import numpy as np\ntrain_and_pred(train_features, test_features, train_labels, test_data,\n               num_epochs, lr, weight_decay, batch_size)","bdf9f9bb":"# 2\n#lets try to improve the model minimizing th elograithm of prices directly\n\ndef train(net, train_features, train_labels, test_features, test_labels, num_epochs, learning_rate, weight_decay, batch_size):\n    train_ls , test_ls = [],[]\n    train_iter = load_array((train_features, train_labels), batch_size)\n    optimizer = torch.optim.Adam(net.parameters(), lr = learning_rate, weight_decay=weight_decay)\n    \n    for epoch in range(num_epochs):\n        for X, y in train_iter:\n            y_hat = net(X)\n#             print(y_hat.shape, y.shape)\n\n#applying loghere\n            l = loss(torch.log(y_hat),torch.log(y.unsqueeze(1)))\n            optimizer.zero_grad()\n            l.backward()\n            optimizer.step()\n#         train_ls.append(log_rmse(net, train_features, train_labels))\n        train_ls.append(l)\n        if (test_labels is not None):\n            test_ls.append(log_rmse(net, test_features, test_labels))\n    return train_ls, test_ls\n","69aeed85":"train_l, valid_l = k_fold(k, train_features, train_labels, num_epochs, learning_rate, weight_decay, batch_size)\nprint(f'{k}-fold validation: avg train log rmse: {float(train_l):f}, '\n      f'avg valid log rmse: {float(valid_l):f}')","dd25b2d5":"train_and_pred(train_features, test_features, train_labels, test_data,\n               num_epochs, lr, weight_decay, batch_size)","16dd0877":"#4 #5\n\ndef train(net, train_features, train_labels, test_features, test_labels, num_epochs, learning_rate, weight_decay, batch_size):\n    train_ls , test_ls = [],[]\n    train_iter = load_array((train_features, train_labels), batch_size)\n    optimizer = torch.optim.Adam(net.parameters(), lr = learning_rate, weight_decay=weight_decay)\n    \n    for epoch in range(num_epochs):\n        for X, y in train_iter:\n            y_hat = net(X)\n#             print(y_hat.shape, y.shape)\n            l = loss(y_hat,y.unsqueeze(1))\n            optimizer.zero_grad()\n            l.backward()\n            optimizer.step()\n        train_ls.append(log_rmse(net, train_features, train_labels))\n        if (test_labels is not None):\n            test_ls.append(log_rmse(net, test_features, test_labels))\n    return train_ls, test_ls","9ccdff68":"train_and_pred(train_features, test_features, train_labels, test_data,\n               num_epochs, lr, weight_decay, batch_size)","8c6ae1f9":"loss = nn.MSELoss()\n\nin_features = train_features.shape[1]\nout_features = 1\n\ndef get_net():\n    net = nn.Sequential(nn.Linear(in_features,256), nn.Linear(256,64), nn.Linear(64, out_features))\n    return net","f1ceebd3":"k, num_epochs, lr, weight_decay, batch_size = 6, 100, 5, 0.1, 64\n\ntrain_l, valid_l = k_fold(k, train_features, train_labels, num_epochs, learning_rate, weight_decay, batch_size)\nprint(f'{k}-fold validation: avg train log rmse: {float(train_l):f}, '\n      f'avg valid log rmse: {float(valid_l):f}')","eb77c4b0":"train_and_pred(train_features, test_features, train_labels, test_data,\n               num_epochs, lr, weight_decay, batch_size)","f637caf0":"Dealing with numeric features, by noramlisation and filling it with na","ef565625":"### Data preprocessing","971c666b":"### Exercises\n1. Submit your predictions for this section to Kaggle. How good are your predictions?\n\n* I got a rmse value of 0.16703\n\n2. Can you improve your model by minimizing the logarithm of prices directly? What happens\nif you try to predict the logarithm of the price rather than the price?\n\n* The log rmse values are coming as Nan\n\n3. Is it always a good idea to replace missing values by their mean? Hint: can you construct a\nsituation where the values are not missing at random?\n\n* It might be the case where the date data is given and dates in between are missing in that case we can put in the date\n\n4. Improve the score on Kaggle by tuning the hyperparameters through K-fold cross\u0002validation.\n\n* I have improved it once.\n\n5. Improve the score by improving the model (e.g., layers, weight decay, and dropout).\n\n* will try\n\n6. What happens if we do not standardize the continuous numerical features like what we have\ndone in this section?\n\n* improper features size that are different for different features.","328931db":"dealing with discrete features, we do one hot encoding"}}