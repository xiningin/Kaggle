{"cell_type":{"0ca24a4c":"code","6f5086f2":"code","0fe18525":"code","618f3fe2":"code","a511641b":"code","0ac9405f":"code","03199366":"code","670059de":"code","7f58a124":"code","d78171b8":"code","f59725ac":"code","982ceccc":"code","b0886cb1":"code","0c2159dc":"code","7b65662a":"code","f6ca85a6":"code","a9fb9116":"code","11425633":"code","ace51bac":"code","f13e0e6d":"code","3f2b2538":"code","4c6637c6":"code","f83e65cc":"code","ad6a1cd2":"code","09292a6e":"code","c02d63b4":"code","b3b61b4b":"code","ba677a8e":"code","5eb2bc39":"code","b2f78443":"code","e41f7f25":"code","a548eceb":"code","9eeac518":"code","ff0de8e8":"code","acb42671":"code","ac70cec9":"code","2602dc47":"code","d84263ad":"code","6e3f4f3e":"code","49d2f731":"code","3529fb57":"code","36394df4":"code","c3ad4b8d":"code","470f367b":"code","873e0a5c":"code","e8e7528c":"code","36b81112":"code","18dd03ef":"code","bd0e64aa":"code","4539667b":"code","7c314184":"code","f6d70b69":"code","9e160d7f":"code","7d0d676b":"code","4d09d417":"code","47c74987":"code","81b8f68f":"markdown","eb197d48":"markdown","a0b8ed8b":"markdown","6bfdee01":"markdown","a045513f":"markdown","5ff37e90":"markdown","a3327f7a":"markdown","811942c6":"markdown","a581e667":"markdown","d0917853":"markdown","2483943f":"markdown","2891f613":"markdown","c057e7b3":"markdown","61246e8d":"markdown","af0d406d":"markdown","0c34d429":"markdown","661606e3":"markdown","35f59727":"markdown","a2d157d9":"markdown","517f8b20":"markdown","54dc2c1d":"markdown","be63db25":"markdown","0a8da9b5":"markdown","51b2f532":"markdown","69d74c2c":"markdown","7187d8b3":"markdown","7fe97736":"markdown","a4a82e0b":"markdown","1d8ad78c":"markdown","ef3f90c9":"markdown","35753402":"markdown","d0f0bea0":"markdown","313c5259":"markdown","a2e68310":"markdown","d51f8cf7":"markdown","615e1713":"markdown","90e31228":"markdown","ec35efe9":"markdown","2af3c520":"markdown","92406748":"markdown","7d9b00ee":"markdown","80b8181e":"markdown"},"source":{"0ca24a4c":"!pip install catboost\n!pip install ipywidgets\n!jupyter nbextension enable --py widgetsnbextension\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom catboost import CatBoostClassifier, Pool, cv\nfrom xgboost.sklearn import XGBClassifier\nimport xgboost as xgb\nfrom tqdm import tqdm\n\nfrom sklearn.metrics import f1_score, log_loss, confusion_matrix,classification_report\nimport scikitplot as skplt\nfrom sklearn.metrics import accuracy_score, roc_curve, auc\nfrom sklearn.preprocessing import StandardScaler","6f5086f2":"data_train = pd.read_csv('..\/input\/aps_failure_training_set.csv')\ndata_test = pd.read_csv('..\/input\/aps_failure_test_set.csv')","0fe18525":"data_train.head()","618f3fe2":"data_train.isnull().sum()","a511641b":"data_test.isnull().sum()","0ac9405f":"# NA replacemenet\ndata_train.replace('na','-1', inplace=True)\ndata_test.replace('na','-1', inplace=True)","03199366":"#categorical encoding\ndata_train['class'] = pd.Categorical(data_train['class']).codes\ndata_test['class'] = pd.Categorical(data_test['class']).codes\n\nprint(['neg', 'pos'])\nprint(np.bincount(data_train['class'].values))\nprint(np.bincount(data_test['class'].values))","670059de":"import seaborn as sns\nsns.set(style=\"white\")\nsns.set(style=\"whitegrid\", color_codes=True)\nsns.countplot(x='class', data=data_train, palette='hls')\nplt.show()","7f58a124":"# split train and test data into X_train,X_test and y_train,y_test\ny_train = data_train['class'].copy(deep=True)\nX_train = data_train.copy(deep=True)\nX_train.drop(['class'], inplace=True, axis=1)\n\ny_test = data_test['class'].copy(deep=True)\nX_test = data_test.copy(deep=True)\nX_test.drop(['class'], inplace=True, axis=1)\n\n# strings to float\nX_train = X_train.astype('float64')\nX_test = X_test.astype('float64')","d78171b8":"cat_features = list(range(0, X_train.shape[1]))\nprint(cat_features)","f59725ac":"print(X_train.dtypes)\ncategorical_features_indices = np.where(X_train.dtypes != np.float)[0]","982ceccc":"def evaluate(y_test,y_pred,y_pred_proba):\n    if len(y_pred)>0:\n        f1 = f1_score(y_test,y_pred,average=\"weighted\")\n        print(\"F1 score: \",f1)\n    if len(y_pred_proba)>0:\n        logloss = log_loss(y_test,y_pred_proba, eps=1e-15, normalize=True, sample_weight=None, labels=None)\n        print(\"Log loss for predicted probabilities:\",logloss)","b0886cb1":"forest_clf = RandomForestClassifier(n_estimators=250,n_jobs=-1)\nforest_clf.fit(X_train,y_train)\ny_pred_rf = forest_clf.predict(X_test)\ny_pred_proba_rf = forest_clf.predict_proba(X_test)\nevaluate(y_test,y_pred_rf,y_pred_proba_rf)","0c2159dc":"tn, fp, fn, tp = confusion_matrix(y_test, y_pred_rf ).ravel()\nskplt.metrics.plot_confusion_matrix(y_test, y_pred_rf, normalize=False)\nplt.show()\nprint(classification_report(y_test,y_pred_rf))","7b65662a":"#display ROC curve\nfrom sklearn.metrics import auc\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_rf)\nroc_auc = auc(fpr, tpr)\n\n\nplt.figure()\nplt.plot(fpr, tpr, color='red', label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","f6ca85a6":"y_test_predictions_rec = y_pred_proba_rf[:,1] > 0.1\ny_test_predictions_prec = y_pred_proba_rf[:,1] > 0.85","a9fb9116":"skplt.metrics.plot_confusion_matrix(y_test, y_test_predictions_prec, normalize=False)\nplt.show()\nprint(classification_report(y_test, y_test_predictions_prec))","11425633":"roc_auc = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='red',label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","ace51bac":"scores = forest_clf.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, scores)","f13e0e6d":"min_cost = np.inf\nbest_threshold = 0.5\ncosts = []\nfor threshold in tqdm(thresholds):\n    y_pred_threshold = scores > threshold\n    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_threshold).ravel()\n    cost = 10*fp + 500*fn\n    costs.append(cost)\n    if cost < min_cost:\n        min_cost = cost\n        best_threshold = threshold\nprint(\"Best threshold: {:.4f}\".format(best_threshold))\nprint(\"Min cost: {:.2f}\".format(min_cost))","3f2b2538":"y_pred_test_rf = forest_clf.predict_proba(X_test)[:,1] > best_threshold\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred_test_rf).ravel()\nskplt.metrics.plot_confusion_matrix(y_test,y_pred_test_rf, normalize=False)\n10*fp + 500*fn","4c6637c6":"bayes_clf = GaussianNB()\nbayes_clf.fit(X_train,y_train)","f83e65cc":"y_pred__bayes = bayes_clf.predict(X_test)\ny_pred_proba_bayes = bayes_clf.predict_proba(X_test)","ad6a1cd2":"evaluate(y_test,y_pred__bayes,y_pred_proba_bayes)","09292a6e":"tn, fp, fn, tp = confusion_matrix(y_test,y_pred__bayes).ravel()\nskplt.metrics.plot_confusion_matrix(y_test,y_pred__bayes, normalize=False)\nplt.show()\nprint(classification_report(y_test,y_pred__bayes))","c02d63b4":"from sklearn.metrics import roc_curve, auc\nfpr, tpr, thresholds = roc_curve(y_test,y_pred__bayes)\nroc_auc = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='red',label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=3, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","b3b61b4b":"xgb_clf = XGBClassifier(max_depth=5)\nxgb_clf.fit(X_train,y_train)","ba677a8e":"y_pred_xgb = xgb_clf.predict(X_test)\ny_pred_proba_xgb = xgb_clf.predict_proba(X_test)","5eb2bc39":"evaluate(y_test,y_pred_xgb,y_pred_proba_xgb)","b2f78443":"tn, fp, fn, tp = confusion_matrix(y_test,y_pred_xgb).ravel()\nskplt.metrics.plot_confusion_matrix(y_test,y_pred_xgb, normalize=False)\nplt.show()\nprint(classification_report(y_test,y_pred_xgb))","e41f7f25":"from sklearn.metrics import roc_curve, auc\nfpr, tpr, thresholds = roc_curve(y_test,y_pred_xgb)\nroc_auc = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='red',label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","a548eceb":"y_test_predictions_rec = y_pred_proba_xgb[:,1] > 0.1\ny_test_predictions_prec = y_pred_proba_xgb[:,1] > 0.85","9eeac518":"skplt.metrics.plot_confusion_matrix(y_test, y_test_predictions_prec, normalize=False)\nplt.show()\nprint(classification_report(y_test, y_test_predictions_prec))","ff0de8e8":"scores = xgb_clf.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, scores)","acb42671":"roc_auc = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='red',label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","ac70cec9":"min_cost = np.inf\nbest_threshold = 0.5\ncosts = []\nfor threshold in tqdm(thresholds):\n    y_pred_threshold = scores > threshold\n    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_threshold).ravel()\n    cost = 10*fp + 500*fn\n    costs.append(cost)\n    if cost < min_cost:\n        min_cost = cost\n        best_threshold = threshold\nprint(\"Best threshold: {:.4f}\".format(best_threshold))\nprint(\"Min cost: {:.2f}\".format(min_cost))","2602dc47":"y_pred_test_xgb = xgb_clf.predict_proba(X_test)[:,1] > best_threshold\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred_test_xgb).ravel()\nskplt.metrics.plot_confusion_matrix(y_test,y_pred_test_xgb, normalize=False)\n10*fp + 500*fn","d84263ad":"y_test.shape","6e3f4f3e":"X_test.shape","49d2f731":"X_train.fillna(-999, inplace=True)\nX_test.fillna(-999, inplace=True)","3529fb57":"model = CatBoostClassifier(\n    custom_loss=['Accuracy'],\n    random_seed=42,\n    logging_level='Silent'\n)\nmodel.fit(\n    X_train, y_train,\n    cat_features=categorical_features_indices,\n    eval_set=(X_test, y_test),\n    logging_level='Verbose',  # you can uncomment this for text output\n    plot=True\n);","36394df4":"model = CatBoostClassifier(\n    iterations=450,\n    random_seed=38,\n    learning_rate=0.2,\n    eval_metric=\"Accuracy\",\n    use_best_model=False\n)\n\nmodel.fit(\n    X_train, y_train,\n    cat_features=categorical_features_indices,\n    eval_set=(X_test, y_test),\n    verbose=False,\n    plot=True\n)","c3ad4b8d":"params = {}\nparams['loss_function'] = 'Logloss'\nparams['iterations'] = 450\nparams['custom_loss'] = 'AUC'\nparams['random_seed'] = 60\nparams['learning_rate'] = 0.2\n\ncv_data = cv(\n    params = params,\n    pool = Pool(X_train, label=y_train, cat_features=categorical_features_indices),\n    fold_count=5,\n    inverted=False,\n    shuffle=True,\n    partition_random_seed=0,\n    plot=True,\n    stratified=True,\n    verbose=False\n)","470f367b":"print(model.predict_proba(data=X_test))","873e0a5c":"import scikitplot as skplt","e8e7528c":"y_pred = model.predict(data=X_test)","36b81112":"tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()","18dd03ef":"skplt.metrics.plot_confusion_matrix(y_test, y_pred, normalize=False)\ny_pred_proba = model.predict_proba(X_test)\nplt.show()\nprint(classification_report(y_test, y_pred))","bd0e64aa":"fpr, tpr, thresholds = roc_curve(y_test, y_pred)\nroc_auc = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='red',label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","4539667b":"y_test_predictions_high_precision = y_pred_proba[:,1] > 0.8\ny_test_predictions_high_recall = y_pred_proba[:,1] > 0.1","7c314184":"skplt.metrics.plot_confusion_matrix(y_test, y_test_predictions_high_precision, normalize=False)\nplt.show()\nprint(classification_report(y_test, y_test_predictions_high_precision))","f6d70b69":"10*120+ 500*9","9e160d7f":"scores = model.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, scores)","7d0d676b":"roc_auc = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='red',label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","4d09d417":"min_cost = np.inf\nbest_threshold = 0.5\ncosts = []\nfor threshold in tqdm(thresholds):\n    y_pred_threshold = scores > threshold\n    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_threshold).ravel()\n    cost = 10*fp + 500*fn\n    costs.append(cost)\n    if cost < min_cost:\n        min_cost = cost\n        best_threshold = threshold\nprint(\"Best threshold: {:.4f}\".format(best_threshold))\nprint(\"Min cost: {:.2f}\".format(min_cost))","47c74987":"y_pred_test_final = model.predict_proba(X_test)[:,1] > best_threshold\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred_test_final).ravel()\nskplt.metrics.plot_confusion_matrix(y_test,y_pred_test_final, normalize=False)\n10*fp + 500*fn","81b8f68f":"More interesting than objective functions for classification tasks, it is important to deep dive on accuracy, precision, recall or f measure of the model","eb197d48":"### [Back to Top](#Top)","a0b8ed8b":"As it is too risky to maniupulate propability thresholds, a function is defined to process a propability threshold in order to finally trigger a robust failure cost model which is based on an improved confusion matrix.","6bfdee01":"### Conclusion\n* Random Forest failre cost is 8390\n* CatBoost failure cost is 10620\n* XGBoost failure cost is 9520\n","a045513f":"### Reading Dataset in dataframe","5ff37e90":"### Eval metric, custom metrics and best trees count","a3327f7a":"#### Total failure cost prediction with improved threshold (0.0440) is 8390 using Random Forest.","811942c6":"Recall and precision are highly valuable evaulation metrics for our imbalanced data, it should be considered to manipualte propability thresholds to evaulate impacts regarding the confusion matrix and finally to decide on further steps to search for threshold optimisation.","a581e667":"### Model prediction","d0917853":"### [Back to Top](#Top)","2483943f":"# Air Pressure system (APS) failure cost prediction model\n\n\n\n#### Acknowledgements\nThis file is part of APS Failure and Operational Data for Scania Trucks. It was imported from the <a href=\"https:\/\/archive.ics.uci.edu\/ml\/datasets\/APS+Failure+at+Scania+Trucks\">UCI ML Repository<\/a>.\n\n#### Inspiration\nThe total cost of a prediction model the sum of Cost_1 multiplied by the number of Instances with type 1 failure and Cost_2 with the number of instances with type 2 failure, resulting in a Total_cost. In this case Cost_1 refers to the cost that an unnecessary check needs to be done by an mechanic at an workshop, while Cost_2 refer to the cost of missing a faulty truck, which may cause a breakdown. Cost_1 = 10 and Cost_2 = 500, and Total_cost = Cost_1*No_Instances + Cost_2*No_Instances.\n\n<a id='Top'><\/a>\n_________________________________________________________________________________________\n\n### Notes\nThe dataset is highly imblanaced with a high amount of negative class and low amount of positive class, which needs to be considered for classifier model selection and evaulation. ML-Classifier algorithms like KNN and SVC has beend excluded due to the high imbalanaced dataset. In order to build a failure cost prediction model, it would make more sense to focus on Randomforest or Boosting alghorithms.\n\n\n### Table of Contents\n\n#### [Reading and preparing Dataset](#Data)\n#### [1.Random Forest](#Random)\n#### [2.Naive Bayes](#Naive)\n#### [3.XGBOOST](#XG)\n#### [4.CatBoost](#Cat)","2891f613":"##### first column is propability of class 0 and second column is propability of class 1","c057e7b3":"### [Back to Top](#Top)","61246e8d":"### [Back to Top](#Top)","af0d406d":"### [Back to Top](#Top)","0c34d429":"#### Total failure cost prediction with improved threshold (0.0049) is 10620 using CatBoost","661606e3":"Another Boosting API is CatBoost and a candidate for \"everybodys classifier\" due to these unique advantages:\n* 1.Indicies for categorical columns (CatBoost preprocessing step)\n* 2.Embedded vizualisation and comparsion for LogLoss and Accurancy,AUC etc. curves considering different parameter settings\n* 3.High flexibility for parameter settings e.g. number of trees, learning rate, iterations etc.","35f59727":"### Import necessary libraries","a2d157d9":"Recall and precision are highly valuable evaulation metrics for our imbalanced data, it should be considered to manipualte propability thresholds to evaulate impacts regarding the confusion matrix and finally to decide on further steps to search for threshold optimisation.","517f8b20":"### [Back to Top](#Top)","54dc2c1d":"<a id='XG'><\/a>\n## 3.XGBOOST","be63db25":"Recall and precision are highly valuable evaulation metrics for our imbalanced data, it should be considered to manipualte propability thresholds to evaulate impacts regarding the confusion matrix and finally to decide on further steps to search for threshold optimisation.","0a8da9b5":"#### Total failure cost prediction with improved threshold (0.0069) is 9520 using XGBOOST.","51b2f532":"### Standard output of the training","69d74c2c":"The CatboostClassifier library provides the opportunity to visualize the important objective functions Logloss and CrossEntropy.\nRelevant training parameters parameters like learning rate and iterations for optimising the objective functions can be set within CatboostClassifier.","7187d8b3":"<a id='Naive'><\/a>\n## 2.Naive Bayes","7fe97736":"As it is too risky to maniupulate propability thresholds, a function is defined to process a propability threshold in order to finally trigger a robust failure cost model which is based on an improved confusion matrix.","a4a82e0b":"### Crossvalidation","1d8ad78c":"Among training parameters, in CatBoost it is possible to define a logging_level for the standard output to shed light on parameters like  \n* error on learning & test set (value of objective function)\n* optimized metric\n* elapsed training time\n* remaining training time\n* best iteration\n* no. of trees\n\na file will be created on the local system for for defined training rates ","ef3f90c9":"<a id='Cat'><\/a>\n## 4.CatBoost","35753402":"<a id='Random'><\/a>\n## 1.Random Forest","d0f0bea0":"#### Definition of generic function for model evaulation scores","313c5259":"### [Back to Top](#Top)","a2e68310":"Catboost provides a MetricVisualizer in order to plott the error on learning and test rates for each tuned learning rate mentioned earlier.\nThe best iteration is plotted as a dot in the learning curve and also outlined in the metrics.","d51f8cf7":"As it is too risky to maniupulate propability thresholds, a function is defined to process a propability threshold in order to finally trigger a robust failure cost model which is based on an improved confusion matrix.","615e1713":"<a id='Data'><\/a>\n### Preparing Data","90e31228":"### [Back to Top](#Top)","ec35efe9":"The labelled data y need to be classified by the independed values x, in our case we are focusing as mentioned above on a classification problem in order to build a predictive failure cost model for the APS system. \nThe objective functions for optimising classification algorithms are\n1. Logloss function (for binary labeled data[0,1]) \n2. CrossEntropy (prediction of propabilities to estimate class of labeled data)","2af3c520":"### [Back to Top](#Top)","92406748":"### [Back to Top](#Top)","7d9b00ee":"### [Back to Top](#Top)","80b8181e":"### [Back to Top](#Top)"}}