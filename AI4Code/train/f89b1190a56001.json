{"cell_type":{"43a8b121":"code","efe15645":"code","f107eb84":"code","b4425c74":"code","7eb74389":"code","d8ffae59":"code","9ccf61a7":"code","6d87adb4":"code","8bfe0f4b":"code","1b7a3d4d":"code","600fe333":"code","56ec73e7":"code","70787477":"code","11b0ecbb":"code","6eecdbb2":"code","c6819ce4":"code","1d008fd9":"code","6d678ad5":"code","22d0eeab":"code","3401f9d4":"code","bcdc3afd":"code","68916957":"markdown","9e04367a":"markdown","38e8d105":"markdown","850809a2":"markdown","9bfdb194":"markdown","1c81efef":"markdown","a8c1c421":"markdown","0c30b143":"markdown","f86387f2":"markdown","1cbf887d":"markdown","8dfb53da":"markdown","c04bc175":"markdown","a5b759a7":"markdown"},"source":{"43a8b121":"from IPython.display import YouTubeVideo\nYouTubeVideo('0xC5YBOSOio', width=800, height=450)","efe15645":"import pandas as pd\nimport numpy as np\nfrom time import time\nimport json\nimport re\nfrom string import punctuation\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom tqdm import tqdm\nfrom gensim.models import Word2Vec\nfrom gensim.models.callbacks import CallbackAny2Vec\nimport torch\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\n!pip install sentence-transformers\nfrom sentence_transformers import SentenceTransformer, models\nimport scipy\nfrom collections import defaultdict\n\nstop_words = set(stopwords.words(\"english\"))","f107eb84":"directory = \"\/kaggle\/input\/CORD-19-research-challenge\/\"\n\nmetadata = pd.read_csv(directory + \"metadata.csv\")\nprint(metadata.shape)","b4425c74":"pdf_missing, pmc_missing = metadata.pdf_json_files.isnull(), metadata.pmc_json_files.isnull()\nfulltext = metadata[~pdf_missing | ~pmc_missing]\nprint(fulltext.shape)","7eb74389":"stop_words = stopwords.words(\"english\")\n\ndef tokenize(text):\n    \"\"\"\n        Cleans and tokenizes text\n        \n        Input:\n        text: str to be tokenized\n        \n        Output:\n        list of tokens\n    \"\"\"\n    regex = re.compile(\"\\[.*?\\]\")\n    text = regex.sub(\"\", text)\n    text = text.replace('\/', ' \/ ')\n    text = text.replace('.-', ' .- ')\n    text = text.replace('.', ' . ')\n    text = text.replace('\\'', ' \\' ')\n    text = text.lower()\n    return [token for token in text.split() if token not in stop_words and token not in punctuation] ","d8ffae59":"class W2VLogger(CallbackAny2Vec):\n    \n    def __init__(self):\n        self.epoch = 1\n        self.loss = 0\n\n    def on_epoch_end(self, model):\n        print(\"Epoch #{} end\".format(self.epoch))\n        print(\"Time since start: {}\".format((time() - t) \/ 3560))\n        curr_loss = model.get_latest_training_loss() - self.loss\n        print('Loss: {}'.format(curr_loss))\n        self.loss = curr_loss\n        self.epoch += 1\n\nembeddings = Word2Vec.load(directory + '..\/60k-abstract-w2v\/60k_abstracts_w2v.model')\n\n\ndef get_phrase_vector(phrase):\n    \"\"\"\n        Concatenates word vectors in phrase\n        \n        Input:\n        phrase: list of str tokens\n        \n        Output:\n        300-dimensional vector representation of phrase\n    \"\"\"\n    vector = [0] * 300\n    for word in phrase:\n        try:\n            vector += embeddings.wv[word]\n        except:\n            pass\n        \n    return vector\n\n\ndef phrase_cos_similarity(phrase1, phrase2):\n    \"\"\"\n        Computes cosine similarity score between phrase1 and phrase2\n        \n        Input:\n        phrase1: list of str tokens\n        phrase2: list of str tokens\n        \n        Output:\n        cosine similarity between phrase1 and phrase2\n    \"\"\"\n    vec1 = get_phrase_vector(phrase1)\n    vec2 = get_phrase_vector(phrase2)\n    cos_sim = np.dot(vec1,vec2) \/ (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n    return cos_sim\n\n\ndef get_article_similarity(doc_index, query):\n    \"\"\"\n        Computes cosine similarity between document title and query\n        \n        Input:\n        doc_index: index in metadata of the document title to be retrieved\n        query: list of str tokens\n        \n        Output:\n        cosine similarity between document title and query\n    \"\"\"\n    article_name = fulltext['title'][doc_index]\n    article_name = tokenize(str(article_name))\n    return phrase_cos_similarity(query,article_name)\n\n\ndef rank_similar_articles(corpus, query):\n    \"\"\"\n        Ranks the relevance of all documents in the corpus to the query\n        \n        Input:\n        corpus: pandas dataframe with all documents to be ranked\n        query: list of str tokens\n        \n        Output:\n        pandas dataframe with all documents in descending order based on cosine similarity to query\n    \"\"\"\n    indices = corpus.index.values.tolist()\n    similarities = [None] * len(indices)\n    for it in tqdm(range(len(indices))):\n        index = indices[it]\n        similarities[it] = get_article_similarity(index, query)\n    corpus['similarity'] = similarities\n    return corpus.sort_values('similarity',ascending=False)\n\n\ndef search_corpus(corpus, query, selection_ratio = 0.33):\n    \"\"\"\n        Ranks the relevance of all documents in the corpus to the query\n        \n        Input:\n        corpus: pandas dataframe with all documents to be ranked\n        query: str to be searched\n        selection_ratio: fraction of ranked documents to keep\n        \n        Output:\n        pandas dataframe with selection_ratio of documents in descending order based on cosine similarity to query\n    \"\"\"\n    return rank_similar_articles(corpus, query.split()).head(int(corpus.shape[0] * selection_ratio))","9ccf61a7":"# Filter full text documents for ones only about COVID-19\ncovidtext = search_corpus(fulltext, \"covid-19\")\ncovidtext.shape\n# Filter covid-related full text documents for ones only about diagnostic studies\ndiag_texts = search_corpus(covidtext, \"diagnostic\")\ndiag_texts.shape","6d87adb4":"relevanttext = search_corpus(diag_texts, \"antibody igg imm tests\")\nprint(relevanttext.shape)\nrelevanttext.head(50)","8bfe0f4b":"# Use BERT for mapping tokens to embeddings\ncovidbert_nli = models.Transformer(\"gsarti\/covidbert-nli\")\n\n# Apply mean pooling to get one fixed sized sentence vector\npooling_model = models.Pooling(covidbert_nli.get_word_embedding_dimension(),\n                               pooling_mode_mean_tokens=True,\n                               pooling_mode_cls_token=False,\n                               pooling_mode_max_tokens=False)\n\nmodel = SentenceTransformer(modules=[covidbert_nli, pooling_model])","1b7a3d4d":"def get_text(paper):\n    \"\"\"\n        Gets title, abstract, body paragraphs, and figure captions from paper\n        \n        Input:\n        paper: pandas dataframe row of document to be retrieved\n        \n        Output:\n        str representation of title, abstract, body paragrahs, and figure captions\n    \"\"\"\n    try:\n        data = open(directory + paper.pdf_json_files.split(\";\")[0])\n    except:\n        data = open(directory + paper.pmc_json_files.split(\";\")[0])\n    \n    doc = json.load(data)\n    data.close()\n    \n    title, abstract, body, captions = \"\", \"\", \"\", \"\"\n    \n    title = doc[\"metadata\"][\"title\"]\n    try:\n        for p in doc[\"abstract\"]:\n            abstract += p[\"text\"] + \" \"\n    except:\n        pass\n    \n    try:\n        for p in doc[\"body_text\"]:\n            body += p[\"text\"] + \" \"\n    except:\n        pass\n    \n    try:\n        for p in doc[\"ref_entries\"]:\n            captions += doc[\"ref_entries\"][p][\"text\"]\n    except:\n        pass\n    \n    return title, abstract, body, captions\n\n\ndef filter_sentences(text, question, topn = 15):\n    \"\"\"\n        Returns topn sentences from text that have the lowest word mover's distance from the question\n        \n        Input:\n        text: str representation of full text\n        question: str to be compared to sentences of text\n        topn: number of relevant sentences to return\n        \n        Output:\n        topn sentences in text that are relevant to question\n    \"\"\"\n    result = []\n    sents = sent_tokenize(text)\n    keywords = tokenize(question)\n    \n    for sent in sents:\n        if (len(sent) < 10) or (len(sent) > 400):\n            sents.remove(sent)\n\n    scores = [embeddings.wv.wmdistance(sent.lower(), question.lower()) for sent in sents]\n    result = sorted(zip(scores, sents), key = lambda x: x[0], reverse = False)[:topn]\n\n    return result","600fe333":"def create_summary_table(model, corpus, contexts):\n    \"\"\"\n        Creates summary table on \n        \n        Input:\n        model: BERT NLI model\n        corpus: pandas dataframe of documents to be included in summary table\n        contexts: list of exemplar sentences that model the question's answer\n        \n        Output:\n        topn sentences in text that are relevant to question\n    \"\"\"\n    table = defaultdict(list)\n    for i, doc in corpus.iterrows():\n        title, abstract, body, captions = get_text(doc)\n        \n        table[\"Publication Date\"].append(doc.publish_time) \n        table['Study'].append(doc.title)\n        table[\"Study Link\"].append(doc.url)\n        table[\"Journal\"].append(doc.journal)\n  \n        query_embeddings = model.encode(contexts)\n        for context, query_embedding in zip(contexts, query_embeddings):\n            try:\n                sentences = filter_sentences(str(abstract + \" \" + body), context)\n                sentences = list(zip(*sentences))[1] #only if you pre rank sentences\n                sentence_embeddings = model.encode(sentences)\n            \n                distances = scipy.spatial.distance.cdist([query_embedding], sentence_embeddings, \"cosine\")[0]\n                results = zip(range(len(distances)), distances)\n                results = sorted(results, key=lambda x: x[1], reverse = False) # should not be reverse = true, as scipy implementation is 1 - cos score\n        \n                for idx, distance in results[:1]:\n                    table[context].append(sentences[idx])\n            except:\n                table[context].append(\"-\")\n        \n        table[\"DOI\"].append(doc.doi)\n        table[\"CORD_UID\"].append(doc.cord_uid)\n        \n        result = pd.DataFrame(table)\n        result.columns = ['Publication Date','Study','Study Link','Journal',\n                          'Study Type', 'Detection Method', \"Sample Size\", 'Specimen Obtained', 'Speed of Assay',\n                          \"Available for Point-of-Care Use\", 'Sample Measure of Evidence', 'FDA Approval', 'DOI', 'CORD_UID']\n                \n    return result","56ec73e7":"contexts = [\n    \"This is a retrospective and observational study\",\n    \"The antibodies against SARS-CoV-2 were detected by an enzyme-linked immunosorbent assay (ELISA) based on the recombinant nucleocapsid protein of SARS-CoV-2 in patients with confirmed or suspected COVID-19\",\n    \"A total of 323 samples from 76 COVID-19 confirmed patients were analyzed\",\n    \"nasopharyngeal swabs, sputum, blood, feces and anal swabs of COVID-19 cases were collected\",\n    \"run-time of 45-50 minutes with hands on time limited to 2-3 minutes\",\n    \"We believe a point-of-care (PoC) device for the rapid detection of the 2019 novel Coronavirus (SARS-CoV-2) is crucial and urgently needed.\",\n    \"The sensitivity for both IgM and IgG tests ranges between 72.7% and 100%, while specificity ranges between 98.7% to 100% and the positive and negative predictive values were 96.21% and 98.68% respectively.\",\n    \"most molecular tests have been approved by the United States Food and Drug Administration (FDA) under emergency use authorization (EUA) \"\n]","70787477":"pd.set_option('display.max_colwidth', 0)","11b0ecbb":"corpus = search_corpus(diag_texts, \"nucleic acid tests\").iloc[:50]\nresult_na = create_summary_table(model, corpus, contexts)","6eecdbb2":"# Display the top 10 results of the summary table\nresult_na.to_csv(\"\/kaggle\/working\/diagnosing SARS-COV-2 with nucleic-acid based tech.csv\")\nresult_na.head(10)","c6819ce4":"corpus = search_corpus(diag_texts, \"antibody tests\").iloc[:50]\nresult_ab = create_summary_table(model, corpus, contexts)","1d008fd9":"result_ab.to_csv(\"\/kaggle\/working\/diagnosing SARS-COV-2 with antibodies\")\nresult_ab.head(10)","6d678ad5":"corpus = search_corpus(diag_texts, \"point-of-care and rapid bedside tests\").iloc[:50]\nresult_poc = create_summary_table(model, corpus, contexts)","22d0eeab":"result_poc.to_csv(\"\/kaggle\/working\/development of a point-of-care test and rapid bed-side tests.csv\")\nresult_poc.head(10)","3401f9d4":"corpus = search_corpus(diag_texts, \"viral load\").iloc[:50]\nresult_vl = create_summary_table(model, corpus, contexts)","bcdc3afd":"result_vl.to_csv(\"\/kaggle\/working\/how does viral load relate to disease presentations and likelihood of a positive diagnostic test.csv\")\nresult_vl.head(10)","68916957":"Highlights:\n* Home-based and point-of-care tests could address a need for accurate diagnostics in developing countries that do not have as much access to laboratory equipment like thermocyclers.\n* Point-of-care tests can detect COVID-19 in the oral cavity or nasal cavitiy, which is more comfortable for the patient being tested.","9e04367a":"## Point-of-Care and Rapid Bedside Diagnostics:","38e8d105":"# Summary Tables:\nThe summary tables exported as CSVs contain 50 entries, but only the top 10 entries of each summary table were displayed below.","850809a2":"# Pros:\n\n* Word and sentence embeddings do query expansion automatically so we can search similar words\n* The models do not take up too much memory\n* There is a good amount of context to see where the answers came from\n* The tiered w2v model retrieves relevant articles\n* The table indicates whether the test is suitable for point-of-care\n* The system can be applied to other questions and queries without having to redesign its architecture\n* W2v embeddings and pretrained COVID-BERT model can be easily imported for rapid generation\/expansion of tables in a matter of minutes\n* Semantic search with tiered w2v and bert sentence transformers\n\n# Cons:\n\n* Returned answers could be more concise\n* The semantic search results had to be found from online sources rather than using normal questions\n* The table has lower recall because the answers are not validated with BERT question answering\n* Only compatible with CORD-19 dataset\n* The accuracy of certain queries can be improved\n\n# Future Directions:\n* Design a UI to complement the search\n* Experiment with NER like sciSpacy to compare performance on some questions it could excel bert\n* Better data preprocessing step to remove citations and numbers that appear randomly in the text\n* Develop a table for studies that compare one diagnostic approach to another\n\n# Acknowledgements:\n\nWe would like to thank the following Notre Dame alumni for their time and expert opinions in assessing the functionality of our tool.\n\nTom Isenbarger \u201893 Ph.D. J.D.\n\nRich Shea \u201885 MBA\n\nGeorge Christopher \u201874 M.D.\n\nNathan Cuka \u201892 M.D.\n\nWe also would like to thank Gabriele Sarti for making available [his BERT model](https:\/\/huggingface.co\/gsarti\/covidbert-nli) which we tailored towards our tool. \n\n# Additional Citations:\n\n\nChen, Qingyu, et al. \u201cBioSentVec: Creating Sentence Embeddings for Biomedical Texts.\u201d Arxiv.org, 24 Jan. 2020, [arxiv.org\/abs\/1810.09302](arxiv.org\/abs\/1810.09302).\n\nMikolov, Tomas, et al. \u201cDistributed Representations of Words and Phrases and Their Compositionality.\u201d Papers.nips.cc, 2013, [papers.nips.cc\/paper\/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf](papers.nips.cc\/paper\/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf).\n\nReimers, Nils, and Iryna Gurevych. \u201cSentence-BERT: Sentence Embeddings Using Siamese BERT-Networks.\u201d Arxiv.org, 27 Aug. 2019, [arxiv.org\/abs\/1908.10084](arxiv.org\/abs\/1908.10084).\n\nWang, Lucy Lu, et al. \u201cCORD-19: The Covid-19 Open Research Dataset.\u201d Arxiv.org, 22 Apr. 2020, [arxiv.org\/abs\/2004.10706](arxiv.org\/abs\/2004.10706).\n","9bfdb194":"# **CORD2vec - a Coronavirus research tool**\n\nGeorge Yacu, Ethan Sunshine, Sean Seidl - University of Notre Dame - Siegfried Hall","1c81efef":"Highlights:\n* While earlier studies used small sample sizes leading to sometimes inaccurate measures of performance, more recent studies employed larger sample sizes.\n* RT-PCR tests are the golden standard for nucleic acid-based diagnostics due to their high sensitivity and specificity; however, these tests take roughly 3-4 hours to perform in addition to skilled technicians and expensive equipment.\n* LAMP is a possible nucleic acid test that could be used for point-of-care testing.","a8c1c421":"## Antibody Diagnostics:","0c30b143":"# Problem:\n\nAmidst the rise of the COVID-19 pandemic, the medical community has seen rapid advances in viral research. New coronavirus literature is published daily at overwhelming amounts, making it challenging for scientists to keep up to date on the most relevant information. To better ensure the successful overcoming of this pandemic, scientists must be able to efficiently sift through the literature to gather necessary insights. An inability to do so would hinder the advancement of combating COVID-19.\n\n# Resources:\n\n[CORD-19](https:\/\/www.semanticscholar.org\/cord19) is an open research dataset of over 130,000 scholarly articles (69,000 with full text) about the novel coronavirus. The Allen Institute, alongside leading research groups, curates the dataset with articles compatible with machine learning and natural language processing.\n\n# Solution:\n\nProvide an automated tool to read, organize, and filter the most relevant COVID-19 articles from the CORD-19 dataset. The information from these articles will be consolidated into summary tables that concisely and precisely answer targeted questions. With this information, scientists will be able to access key findings without having to read the published articles comprehensively.\n\n# Approach:\n\n![cord2vec%20flow%20chart.png](attachment:cord2vec%20flow%20chart.png)\n\n# Models:\n\n**[Word2vec](#Word2vec:)**\n\n**[BERT](#BERT:)**","f86387f2":"# Word2vec:\n\nOur word2vec model is trained exclusively on CORD-19 abstracts and improves the relevance of the search engine results compared to baseline information retrieval (IR) approaches. The model automatically performs query expansion using the 300-dimensional word embeddings. The model can generalize well to abstracts that are only recently added to the dataset without having to be retrained. Furthermore, the model serves as an effective filter to find articles in the dataset that are only relevant to COVID-19 and various diagnostic studies.\n\nNote: The model was trained for this project in a seperate notebook and then loaded into our final notebook. The model along with the hyper parameters have been made publicly available [here](https:\/\/www.kaggle.com\/georgeyacu\/60k-abstract-w2v).\n","1cbf887d":"## Viral Load Studies:","8dfb53da":"# BERT:\n\nOriginally, we intended to use a BERT model fine tuned for question answering that would provide concise answers. After interviewing several COVID researchers and clinicians in the Notre Dame alumni network, we received feedback to provide a sentence rather than a word to answer the question because it provided valuable context for the answer. For this reason, we used a BERT model based on Deepset\u2019s COVID-BERT and fine-tuned for natural language inference (NLI) tasks. For each document that we wished to summarize, we compared the sentence embeddings from the document with exemplar sentences and computed the cosine similarity between each sentence and exemplar to find the best sentence to answer the question.\n\nNote: The [covidbert-nli](https:\/\/huggingface.co\/gsarti\/covidbert-nli), made publicly available by Gabriele Sarti, was used  in this project.","c04bc175":"## Nucleic Acid-Based Diagnostics:","a5b759a7":"Highlights:\n* Antibody tests are useful for rapid, large-scale testing\n* Antibody tests might not be suitable for rapid diagnosis of acute COVID-19 infection due to low sensitivity"}}