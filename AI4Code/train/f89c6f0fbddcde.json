{"cell_type":{"933576b0":"code","16fb0501":"code","1cac6255":"code","96de07d9":"code","fe363f7e":"code","b2db341e":"code","d6340034":"code","420e435a":"code","2ad690cd":"code","b02fc746":"code","e20e0757":"code","3ec5fe6d":"code","0c1e18af":"code","72c15d63":"code","2b3a655f":"code","badcdb2f":"code","5330dc4c":"code","af867f97":"code","acffbd6d":"code","1d65aef9":"code","e5036d4d":"code","a6ecc721":"code","db0bf25d":"code","4bec59d2":"code","f0ad9de4":"code","59af3a37":"code","bd725e68":"code","3bd5faf3":"code","7d3e96af":"code","effeb450":"code","795e9acf":"code","ccd0fbaa":"code","adfa340f":"code","bc7b8e73":"code","834d08ac":"code","0d731cfd":"code","c1140ffe":"code","2cf54a9d":"code","a97b3c16":"code","ed12aa4e":"code","00ae5b0a":"code","5afb3c10":"code","e7021734":"code","3a9ce152":"markdown","c5bdbf05":"markdown","40eecadd":"markdown","3c21f34c":"markdown","9578f25f":"markdown","7d1dfd4f":"markdown","ba5537b5":"markdown","3aadc5a1":"markdown","fe26cc31":"markdown","cc93a8cd":"markdown","745878a7":"markdown","7bbdca50":"markdown","f3ad76ea":"markdown","a4184296":"markdown"},"source":{"933576b0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nfrom nltk.corpus import stopwords\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM,Dense, SpatialDropout1D, Dropout","16fb0501":"train_set = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest_set = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","1cac6255":"train_set.head()","96de07d9":"print('keyword null : ', train_set.keyword.isna().sum()\/len(train_set))\nprint('location null : ', train_set.location.isna().sum()\/len(train_set))","fe363f7e":"train_set.keyword.fillna('', inplace = True)\ntest_set.keyword.fillna('', inplace = True)","b2db341e":"def clean_url(x):\n    cleaned_x = re.sub(r'http\\S{0,}', r'', x)\n    \n    return cleaned_x","d6340034":"def lower_case(x):\n    return x.lower()","420e435a":"def clean_acronim(x):\n    x = re.sub('im', 'i am', x)\n    x = re.sub( \"i'm\", 'i am', x)\n    x = re.sub('ive', 'i have', x)\n    x = re.sub( \"i've\", 'i have', x)\n    x = re.sub(\"you're\", 'you are', x)\n    x = re.sub(\"they're\", 'they are', x)\n    x = re.sub(\"they've\", 'they have', x)\n    x = re.sub(\"we're\", 'we are', x)\n    x = re.sub(\"we've\", 'we have', x)\n    x = re.sub('cant', 'cannot', x)\n    \n    return x","2ad690cd":"train_set.text = train_set.text.apply(clean_url)\ntrain_set.text = train_set.text.apply(lower_case)","b02fc746":"test_set.text = test_set.text.apply(clean_url)\ntest_set.text = test_set.text.apply(lower_case)","e20e0757":"stopWords = set(stopwords.words('english'))","3ec5fe6d":"def remove_stopwords(x):\n    stripped = x.split()\n    stripped_clean = [x if x not in stopWords else '' for x in stripped]\n    return ' '.join(word for word in stripped_clean)","0c1e18af":"def clean_punc(x):\n    text = re.sub(r\"[^a-zA-Z\\s]\", \" \", x)\n    \n    return text","72c15d63":"def clean_new_line(x):\n    return re.sub(r\"\\n\", \" \", x)","2b3a655f":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","badcdb2f":"train_set.text = train_set.text.apply(clean_punc)\ntrain_set.text = train_set.text.apply(clean_new_line)\ntrain_set.text = train_set.text.apply(remove_stopwords)\ntrain_set.text = train_set.text.apply(remove_emoji)","5330dc4c":"test_set.text = test_set.text.apply(clean_punc)\ntest_set.text = test_set.text.apply(clean_new_line)\ntest_set.text = test_set.text.apply(remove_stopwords)\ntest_set.text = test_set.text.apply(remove_emoji)","af867f97":"train_set.text.head()","acffbd6d":"train_set.text += ' ' + train_set.keyword\ntest_set.text +=  ' ' + test_set.keyword","1d65aef9":"max_words = 1000 #max words that will be convert into seq\nmax_len = 120 #len of sequence","e5036d4d":"tokenizer = Tokenizer(num_words = max_words, oov_token = 'oov') #initialize tokenizer","a6ecc721":"tokenizer.fit_on_texts(train_set.text)","db0bf25d":"word_dict = tokenizer.word_index","4bec59d2":"train_seq = tokenizer.texts_to_sequences(train_set.text)","f0ad9de4":"test_seq = tokenizer.texts_to_sequences(test_set.text)","59af3a37":"padded_train_sequences = pad_sequences(train_seq, max_len)","bd725e68":"padded_test_sequences = pad_sequences(test_seq, max_len)","3bd5faf3":"glove_path = '..\/input\/glove-global-vectors-for-word-representation\/glove.6B.200d.txt'","7d3e96af":"embedding_index = {}","effeb450":"f = open(os.path.join(glove_path), encoding='utf8')\n\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embedding_index[word] = coefs\nf.close()","795e9acf":"embedding_dim = 200\nembedding_matrix = np.zeros((max_words, embedding_dim))","ccd0fbaa":"for i, word in enumerate(word_dict):\n    if i < max_words:\n        embedding_vector = embedding_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector","adfa340f":"model = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=max_len))\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()","bc7b8e73":"model.layers[0].set_weights([embedding_matrix])\nmodel.layers[0].trainable = False","834d08ac":"optimzer=Adam(learning_rate=3e-4)","0d731cfd":"n_fold = 5\nskf = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=562)","c1140ffe":"model_coll = []","2cf54a9d":"for tn_idx, val_idx in skf.split(padded_train_sequences, train_set.target):\n    X_train, y_train = padded_train_sequences[tn_idx], train_set.target[tn_idx]\n    X_val, y_val = padded_train_sequences[val_idx], train_set.target[val_idx]\n    \n    model.compile(optimizer=optimzer, loss='binary_crossentropy', metrics=['accuracy'])\n\n    callback_list = [\n        EarlyStopping(monitor='accuracy',patience=3),\n        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2)\n    ]\n    \n    history = model.fit(X_train, y_train, epochs=10, batch_size=5,\n                    validation_data=(X_val, y_val),)\n    \n    model_coll.append(model)","a97b3c16":"y_predict = pd.DataFrame()","ed12aa4e":"for i in range(n_fold):\n    y_predict[f'pred_{i}'] = pd.DataFrame(model_coll[i].predict(padded_test_sequences))[0]","00ae5b0a":"y_predict = y_predict.mean(axis=1)\ny_predict = pd.DataFrame([1 if predict >= 0.5 else 0 for predict in y_predict], columns=['target'])","5afb3c10":"sample_subm = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","e7021734":"subm = pd.concat([sample_subm.id, y_predict['target']], axis=1)\nsubm.to_csv('subm_file.csv', header=True, index=False)","3a9ce152":"pad each sequence to have uniform length of *max_len*","c5bdbf05":"add *keyword* feature into *text* feature","40eecadd":"In the next step we clean the text by deleting all the urls, remove english stopwords which is available by nltk corpus, clear all punctuations and emojis, replace new line with new space.","3c21f34c":"tokenize each word by the tokenizer. least frequent words will be replaced by 'oov' (which has value 1 in *word_dict*)","9578f25f":"It seems that we can drop *location* feature, because it contains 33% null values. But for *keyword* feature, we will use it and concate it with *text* feature. We will replace empty str for the null value of *keyword* feature","7d1dfd4f":"# Preprocess Dataset","ba5537b5":"Initialize and fit tokenizer into train text","3aadc5a1":"Use Stratified KFold to fold train set","fe26cc31":"Train the LSTM Model","cc93a8cd":"# Build LSTM Model for classification","745878a7":"# Tokenization and Pad Sequence","7bbdca50":"Embedding layer in first layer will have weight of *embedding_matrix* and will be untrainable","f3ad76ea":"# Create Embedding weights from Glove (Global Vector)","a4184296":"# Predict Test Set "}}