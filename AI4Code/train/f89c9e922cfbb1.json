{"cell_type":{"8920fbee":"code","03fb271d":"code","487452ee":"code","f2cdaab1":"code","0bed0780":"code","64b7bf04":"code","2b2e5578":"code","ab6620d1":"markdown","d477f047":"markdown","c85785cd":"markdown","2338b13a":"markdown","3c62e325":"markdown"},"source":{"8920fbee":"import pandas as pd\nfrom pathlib import Path\nfrom sklearn.model_selection import KFold, train_test_split\nfrom lightgbm import LGBMClassifier\nimport gc","03fb271d":"#LightGBM GPU installation didn't work due to \"OpenCL device not found :(\". Anyone successful with the current setup in Kaggle?\n#%%bash\n#apt install libboost-all-dev -y\n#pip uninstall lightgbm -y\n#pip install lightgbm --install-option=\"--gpu\" --install-option=\"--opencl-include-dir=\/usr\/local\/cuda\/include\/\" --install-option=\"--opencl-library=\/usr\/local\/cuda\/lib64\/libOpenCL.so\"","487452ee":"data_dir=Path(\"\/kaggle\/input\/ieee-fraud-detection\")\n\ndd=pd.read_csv(data_dir \/ \"train_transaction.csv\")\nddid=pd.read_csv(data_dir \/ \"train_identity.csv\")\ndd=dd.merge(ddid, on=\"TransactionID\", how=\"left\")\n\nddtest=pd.read_csv(data_dir \/ \"test_transaction.csv\")\nddtestid=pd.read_csv(data_dir \/ \"train_identity.csv\")\nddtest=ddtest.merge(ddtestid, on=\"TransactionID\", how=\"left\")\n\ndel ddid\ndel ddtestid\ngc.collect()\n\ndd.head()","f2cdaab1":"model_feats = [\n    'card1', 'card2', 'card5', 'addr1', 'id_19', 'id_20',\n    'dist1', 'P_emaildomain',\n    'C1', 'C2', 'C5', 'C6', 'C9', 'C11', 'C13', 'C14',\n    'D1', 'D2', 'D3', 'D4', 'D5', 'D8', 'D10', 'D11', 'D15',\n    'id_01', 'id_02', 'id_05', 'id_30',\n    'V45', 'V54', 'V62', 'V67', 'V83', 'V87', 'V100', 'V129', 'V135', 'V169', 'V313'\n    ]\n      \nfor col in [\"card1\", \"card2\", \"card3\", \"card5\", \"card6\", \"addr1\", \"P_emaildomain\", \"R_emaildomain\", \"TransactionAmt\", \"id_19\", \"id_20\", \"id_31\", \"id_33\", \"M5\", \"M6\", \"DeviceInfo\"]:\n    mapping = pd.concat([dd[col], ddtest[col]]).value_counts()\n    col_name = f\"cnts_{col}\"\n    \n    dd[col_name] = dd[col].map(mapping)\n    ddtest[col_name] = ddtest[col].map(mapping)\n    \n    model_feats.append(col_name)\n    \nfor col in [\"ProductCD\", \"card1\", \"card2\", \"card3\", \"card4\", \"card5\", \"addr2\", \"DeviceType\"]:\n    gr = dd.groupby(col)[\"TransactionAmt\"]\n    mean = gr.transform(\"mean\")\n    std = gr.transform(\"std\")\n    \n    col_name = f\"amount_score_{col}\"\n    dd[col_name] = (dd[\"TransactionAmt\"] - mean) \/ std\n    ddtest[col_name] = (ddtest[\"TransactionAmt\"] - mean) \/ std\n    \n    model_feats.append(col_name)\n    \nfor col in [\"P_emaildomain\", \"id_30\"]:\n    dd[col] = dd[col].astype(\"category\")\n    ddtest[col] = ddtest[col].astype(\"category\")\n \nlen(model_feats)","0bed0780":"from math import sqrt\nfrom sklearn.base import clone\nfrom sklearn.model_selection import cross_validate\nimport numpy as np\nfrom operator import itemgetter\nfrom functools import partial\nimport time\n\n\nclass Dummy:\n    def __getattr__(self, name):\n        return lambda x:x\n    \ncolorful=Dummy()     # should be library from `pip install colorful` but didnt work\n\n\ndef format_if_number(x, color=None, format=\"{:g}\"):\n    if isinstance(x, (int, float)):\n        text = format.format(x)\n    else:\n        text = x\n\n    if color is not None:\n        text = color(text)\n\n    return text\n\ncolor_score = partial(format_if_number, color=colorful.violet)\ncolor_number = partial(format_if_number, color=colorful.cornflowerBlue, format=\"{}\")\ncolor_param_val = partial(format_if_number, color=colorful.deepSkyBlue)\ncolor_param_name = partial(format_if_number, color=colorful.limeGreen, format=\"{}\")\n\n\nclass SearchStop(Exception):\n    pass\n\n\nclass GoldenSearch:\n    \"\"\"\n    def func(x):\n        return x**2\n\n    g=GoldenSearch(-10, 10)\n    gen=g.val_gen()\n    y=None\n    try:\n        while g.c-g.a>0.1:\n            x = gen.send(y)\n            y = func(x)\n            print(g)\n    except SearchStop as exc:\n        pass\n    \"\"\"\n\n    pos = 2 - (1 + sqrt(5)) \/ 2  # ~0.382\n\n    def __init__(\n        self,\n        x0,\n        x1,\n        y0=np.nan,\n        y1=np.nan,\n        *,\n        xm=np.nan,\n        ym=np.nan,\n        min_bound=True,\n        max_bound=True,\n        noise=0,\n        map_value=None,\n    ):\n        if map_value is None:\n            map_value = lambda x: x\n        self.map_value = map_value\n\n        self.a = map_value(x0)\n        self.c = map_value(x1)\n\n        if np.isnan(xm):\n            xm = map_value(self.a + self.pos * (self.c - self.a))\n        self.b = xm\n\n        if min_bound is True:\n            self.min_bound = self.a\n        else:\n            self.min_bound = min_bound\n\n        if max_bound is True:\n            self.max_bound = self.c\n        else:\n            self.max_bound = max_bound\n\n        self.noise = noise\n\n        self.ya = y0\n        self.yb = ym\n        self.yc = y1\n\n        self.new_x = np.nan\n        self.new_y = np.nan\n\n    def _map_value(self, value):\n        value = self.map_value(value)\n        if value == self.a or value == self.b or value == self.c:\n            raise SearchStop(f\"Repeated value {value}\")\n\n        return value\n\n    def val_gen(self):\n        if np.isnan(self.ya):\n            self.ya = yield self.a\n\n        if np.isnan(self.yc):\n            self.yc = yield self.c\n\n        if np.isnan(self.yb):\n            self.yb = yield self.b\n\n        while 1:\n            d1 = self.b - self.a\n            d2 = self.c - self.b\n\n            if self.ya < self.yb <= self.yc + self.noise:  # extend region left\n                if self.min_bound < self.a:\n                    self.new_x = self._map_value(self.b - d1 \/ self.pos)\n\n                    if self.new_x < self.min_bound:\n                        self.new_x = self.min_bound\n\n                    self.new_y = yield self.new_x\n                    self.a, self.b, self.c = self.new_x, self.a, self.b\n                    self.ya, self.yb, self.yc = self.new_y, self.ya, self.yb\n                else:\n                    self.new_x = self._map_value(self.a + self.pos * (self.b - self.a))\n\n                    self.new_y = yield self.new_x\n                    self.b, self.c = self.new_x, self.b\n                    self.yb, self.yc = self.new_y, self.yb\n\n            elif self.ya + self.noise >= self.yb > self.yc:  # extend region right\n                if self.c < self.max_bound:\n                    self.new_x = self._map_value(self.b + d2 \/ self.pos)\n\n                    if self.new_x > self.max_bound:\n                        self.new_x = self.max_bound\n\n                    self.new_y = yield self.new_x\n                    self.a, self.b, self.c = self.b, self.c, self.new_x\n                    self.ya, self.yb, self.yc = self.yb, self.yc, self.new_y\n                else:\n                    self.new_x = self._map_value(self.c - self.pos * (self.c - self.b))\n\n                    self.new_y = yield self.new_x\n                    self.a, self.b = self.b, self.new_x\n                    self.ya, self.yb = self.yb, self.new_y\n\n            elif self.ya >= self.yb - self.noise and self.yb - self.noise <= self.yc:\n                if d1 < d2:\n                    self.new_x = self._map_value(\n                        self.c - (1 - self.pos) * (self.c - self.b)\n                    )\n\n                    self.new_y = yield self.new_x\n\n                    if self.new_y > self.yc + self.noise:\n                        raise SearchStop(\"Inconsistent y > c\")\n\n                    if self.new_y < self.yb:\n                        self.a, self.b = self.b, self.new_x\n                        self.ya, self.yb = self.yb, self.new_y\n                    elif self.new_y > self.yb:\n                        self.c = self.new_x\n                        self.yc = self.new_y\n                    else:\n                        raise SearchStop(\"Inconsistent y = c\")\n                else:\n                    self.new_x = self._map_value(\n                        self.a + (1 - self.pos) * (self.b - self.a)\n                    )\n\n                    self.new_y = yield self.new_x\n\n                    if self.new_y > self.ya + self.noise:\n                        raise SearchStop(\"Inconsistent y > a\")\n\n                    if self.new_y < self.yb:\n                        self.b, self.c = self.new_x, self.b\n                        self.yb, self.yc = self.new_y, self.yb\n                    elif self.new_y > self.yb:\n                        self.a = self.new_x\n                        self.ya = self.new_y\n                    else:\n                        raise SearchStop(\"Inconsistent y = b\")\n            else:\n                raise SearchStop(\"Inconsistent a < b > c\")\n\n    def __repr__(self):\n        vals = [\n            (self.a, self.ya),\n            (self.b, self.yb),\n            (self.c, self.yc),\n            (self.new_x, np.nan),\n        ]\n        vals.sort(key=itemgetter(0))\n\n        format_if_not_nan = lambda x: f\"{x:g}\" if not np.isnan(x) else \"_\"\n\n        return (\n            f\"Golden( \"\n            + \" | \".join(\n                f\"{format_if_not_nan(x)}:{format_if_not_nan(y)}\" for x, y in vals\n            )\n            + f\" -> {min(self.ya, self.yb, self.yc):g} )\"\n        )\n\n\nclass GoldenSearcher:\n    def __init__(\n        self, param_name, target_precision, x0, x1, *golden_args, **golden_kwargs\n    ):\n        self.param_name = param_name\n        self.target_precision = target_precision\n\n        if (\n            \"map_value\" not in golden_kwargs\n            and isinstance(x0, int)\n            and isinstance(x1, int)\n        ):\n            golden_kwargs[\"map_value\"] = int\n\n        self.searcher = GoldenSearch(x0, x1, *golden_args, **golden_kwargs)\n        self.val_gen = self.searcher.val_gen()\n\n    def next_search_params(self, params, last_score):\n        val = self.val_gen.send(last_score)\n\n        if self.searcher.c - self.searcher.a < self.target_precision:\n            raise SearchStop(f\"Target precision {self.target_precision} reached\")\n\n        new_params = params.copy()\n        new_params[self.param_name] = val\n        return new_params\n\n    def state_info(self):\n        return str(self.searcher)\n\n    def __repr__(self):\n        return f\"GoldenSearcher({self.param_name})\"\n\n\nclass ListSearcher:\n    def __init__(self, param_name, val_list):\n        self.param_name = param_name\n        self.val_list = val_list\n        self.idx = -1\n\n    def next_search_params(self, params, last_score):\n        self.idx += 1\n\n        if self.idx == len(self.val_list):\n            raise SearchStop(f\"Last of {len(self.val_list)} list values reached\")\n\n        new_params = params.copy()\n        new_val = self.val_list[self.idx]\n        new_params[self.param_name] = new_val\n\n        return new_params\n\n    def state_info(self):\n        return f\"ListSearcher(val {self.idx+1}\/{len(self.val_list)})\"\n\n    def __repr__(self):\n        return f\"ListSearcher({self.param_name}, {len(self.val_list)} vals)\"\n\n\nclass SearcherCV:\n    def __init__(self, estimator, searchers, *, scoring, cv, num_feat_imps=5):\n        self.estimator = estimator\n\n        self.searchers = searchers\n\n        self.scoring = scoring\n        self.cv = cv\n        self.best_params_ = None\n        self.best_score_ = None\n\n        self.num_feat_imps = num_feat_imps\n\n    def fit(self, X, y, verbose_search=True, **fit_params):\n        if verbose_search:\n            print(\n                f\"Starting fit on {len(X.columns)} features and {len(X)} instances with folds {self.cv} and scoring {self.scoring}\"\n            )\n            print()\n\n        self.best_params_ = {}\n\n        for searcher in self.searchers:\n            cur_params = self.best_params_\n\n            if verbose_search:\n                print(f\">> Starting searcher {searcher}\")\n\n            try:\n                score = None\n                while 1:  # SearchStop expected\n                    cur_params = searcher.next_search_params(cur_params, score)\n\n                    mark = (\n                        lambda param_name: \"*\"\n                        if hasattr(searcher, \"param_name\")\n                        and searcher.param_name == param_name\n                        else \"\"\n                    )\n\n                    new_params_str = \", \".join(\n                        f\"{mark(param_name)}{param_name}{mark(param_name)} = {format_if_number(param_val)}\"\n                        for param_name, param_val in sorted(cur_params.items())\n                    )\n\n                    if verbose_search:\n                        print()\n                        print(\n                            f\"Current best score:\",\n                            color_score(self.best_score_)\n                            if self.best_score_ is not None\n                            else \"-\",\n                        )\n                        print(\n                            f\"Current best params:\",\n                            \", \".join(\n                                f\"{param}={format_if_number(val)}\"\n                                for param, val in sorted(self.best_params_.items())\n                            )\n                            if self.best_params_ is not None\n                            else \"-\",\n                        )\n                        print(f\"Searcher state: {searcher.state_info()}\")\n                        print(f\"-> Eval: {new_params_str} .......\")\n\n                    start_time = time.time()\n                    score = self._score(X, y, cur_params, fit_params)\n                    end_time = time.time()\n                    run_time_min = (end_time - start_time) \/ 60\n\n                    new_params_color_str = \", \".join(\n                        f\"{color_param_name(param_name)} = {color_param_val(param_val)}\"\n                        if hasattr(searcher, \"param_name\")\n                        and searcher.param_name == param_name\n                        else f\"{param_name} = {format_if_number(param_val)}\"\n                        for param_name, param_val in sorted(cur_params.items())\n                    )\n\n                    print(\n                        f\"....... ({run_time_min:.2g}min) {new_params_color_str} >>> {color_score(score)}\"\n                    )\n\n                    if self.best_score_ is None or score < self.best_score_:  #!!!\n                        self.best_score_ = score\n                        self.best_params_ = cur_params.copy()\n\n            except SearchStop as exc:\n                print()\n                print(f\"Searcher {searcher} stopped with: {exc}\")\n                print()\n\n        if verbose_search:\n            print(f\"Final best score: {color_score(self.best_score_)}\")\n            print(f\"Final best params:\")\n            for param, val in sorted(self.best_params_.items()):\n                print(f\"    {color_param_name(param)} = {color_param_val(val)},\")\n\n    def _score(self, X, y, params, fit_params):\n        estimator = clone(self.estimator)\n        estimator.set_params(**params)\n\n        cross_val_info = cross_validate(\n            estimator,\n            X,\n            y,\n            scoring=self.scoring,\n            cv=self.cv,\n            fit_params=fit_params,\n            return_train_score=True,\n            return_estimator=True,\n        )\n\n        for fold_idx, (clf, train_score, test_score) in enumerate(\n            zip(\n                cross_val_info[\"estimator\"],\n                cross_val_info[\"train_score\"],\n                cross_val_info[\"test_score\"],\n            ),\n            1,\n        ):\n            infos = [f\"{test_score:g} (train {train_score:g})\"]\n            if hasattr(clf, \"best_iteration_\") and clf.best_iteration_ is not None:\n                infos.append(f\"best iter {clf.best_iteration_}\")\n\n            if hasattr(clf, \"best_score_\") and clf.best_score_:\n                if isinstance(clf.best_score_, dict):\n                    best_score_str = \", \".join(\n                        (f\"{set_name}(\" if len(clf.best_score_) > 1 else \"\")\n                        + \", \".join(\n                            f\"{score_name}={score:g}\"\n                            for score_name, score in scores.items()\n                        )\n                        + (\")\" if len(clf.best_score_) > 1 else \"\")\n                        for set_name, scores in clf.best_score_.items()\n                    )\n                else:\n                    best_score_str = format_if_number(clf.best_score_)\n                    \n                infos.append(f\"stop score {best_score_str}\")\n\n            if hasattr(clf, \"feature_importances_\"):\n                feat_imps = sorted(\n                    zip(clf.feature_importances_, X.columns), reverse=True\n                )\n                infos.append(\n                    \"Top feat: \"\n                    + \" \u00b7 \".join(\n                        str(feat) for _score, feat in feat_imps[: self.num_feat_imps]\n                    )\n                )\n\n            print(f\"Fold {fold_idx}:\", \"; \".join(infos))\n\n        score = cross_val_info[\"test_score\"].mean()\n\n        return -score\n\n    \ndef earlystop(\n    clf,\n    X,\n    y,\n    *,\n    eval_metric=None,\n    early_stopping_rounds=100,\n    test_size=0.1,\n    verbose=False,\n    **fit_params,\n):\n    X_train, X_stop, y_train, y_stop = train_test_split(X, y, test_size=test_size)\n\n    clf.fit(\n        X_train,\n        y_train,\n        early_stopping_rounds=early_stopping_rounds,\n        eval_set=[(X_stop, y_stop)],\n        eval_metric=eval_metric,\n        verbose=verbose,\n        **fit_params,\n    )\n\n    infos = []\n    if hasattr(clf, \"best_iteration_\") and clf.best_iteration_ is not None:\n        infos.append(f\"Best iter {clf.best_iteration_}\")\n\n        if hasattr(clf, \"best_score_\") and clf.best_score_:\n            if isinstance(clf.best_score_, dict):\n                best_score_str = \", \".join(\n                    (f\"{set_name}(\" if len(clf.best_score_) > 1 else \"\")\n                    + \", \".join(\n                        f\"{score_name}={score:g}\" for score_name, score in scores.items()\n                    )\n                    + (\")\" if len(clf.best_score_) > 1 else \"\")\n                    for set_name, scores in clf.best_score_.items()\n                )\n            else:\n                best_score_str = format_if_number(clf.best_score_)\n\n            infos.append(f\"Stop scores {best_score_str}\")\n\n    if hasattr(clf, \"feature_importances_\"):\n        feat_imps = sorted(zip(clf.feature_importances_, X.columns), reverse=True)\n        infos.append(\n            \"Top feat: \"\n            + \" \u00b7 \".join(feat for _score, feat in feat_imps[: self.num_feat_imps])\n        )\n    print(\"\\n\".join(infos))","64b7bf04":"cv = KFold(3)\n\nbase_params=dict(\n    learning_rate=0.3,  # high learning rate because didnt manage to install GPU\n    #device_type=\"gpu\",\n    #max_bin=63,\n)\n\nclf = LGBMClassifier(\n    n_estimators=5000,\n    colsample_bytree=0.9,  # let's start with 0.9, but will be optimized later\n    **base_params,\n)\n\nX = dd[model_feats]   # it's only a few features for demo, so it won't perform well\ny = dd[\"isFraud\"]\n\nsearch = SearcherCV(\n    clf,\n    [\n        GoldenSearcher(\"num_leaves\", 30, 200, 800, noise=0.003),   # target_precision 30, try values between 200 and 800, noise makes the searcher not stop when it the results do not look unimodal\n        ListSearcher(\"colsample_bytree\", [0.3, 0.5, 0.7, 0.9]),\n    ],\n    scoring=\"roc_auc\",\n    cv=cv,\n)\n\nearlystop(search, X, y, eval_metric=\"auc\")","2b2e5578":"clf = LGBMClassifier(\n    **{**base_params,\n    **search.best_params_}\n  )\n\nclf.fit(X, y)\n\npreds = clf.predict_proba(ddtest[model_feats])[:, 1]\n\nsub = pd.DataFrame({\"TransactionID\": ddtest[\"TransactionID\"], \"isFraud\": preds})\nsub.to_csv(\"submission.csv.gz\", index=False)","ab6620d1":"Model tuning is an important part. The common options to do that are\n\n* GridSearch\n* RandomSearch\n* Bayesian Search\n\nAll of the these options seems rather wasteful, as they do check unnecessarily many parameter combinations. If you have your own steps for tuning, this still often means you sit and wait a lot.\n\nSo, I implemented something that can better automate your personal training schedule, by using a coordinate-wise [Golden section search](https:\/\/en.wikipedia.org\/wiki\/Golden-section_search) which assumes a single optimum.\n\nBelow you can configure your schedule and the searcher will successively shrink the range within which the optimum lies.\n\nHave fun!\n\n** Currently, I do not know how to make the GPU installation work, so this optimization still takes a long time. But you could copy the code **","d477f047":"# Basic feature engineering","c85785cd":"# Tuning","2338b13a":"# Make a prediction with the optimal parameters","3c62e325":"# Load data"}}