{"cell_type":{"a325807a":"code","a189c285":"code","b46cde34":"code","9b0bca6e":"code","e3758f25":"code","34028d9d":"code","057e623c":"code","8d5fc447":"code","bcd24540":"code","69785c6d":"code","2ce335de":"code","6608f005":"code","ca1e891a":"code","a3663d81":"code","e3f02711":"code","bdc77388":"code","64b3f76c":"code","b97e85a1":"code","232ce3ee":"code","17eea613":"code","0c8ea290":"code","b480fa43":"code","762ae024":"code","2977c557":"code","586407c3":"code","8264e014":"code","b258a1e0":"code","2735b209":"code","1c19ae95":"code","9b196bf6":"code","ae332088":"code","d44a7e36":"code","7c889122":"code","d820ff08":"code","6bae2360":"code","efedd389":"code","08eb326a":"code","360e523e":"code","d4737d21":"code","15a9e4e1":"code","2965db56":"code","e294011d":"code","ecafde67":"markdown","f31fe56b":"markdown","bafe3889":"markdown","c53cb70f":"markdown","e0bfa482":"markdown","d506e64b":"markdown","4484de8d":"markdown","136a0d89":"markdown","6bb20e33":"markdown","3d1cbb33":"markdown","c8c7b3ee":"markdown","fdc7c404":"markdown","539d319e":"markdown","be5d1d5a":"markdown","59baf545":"markdown","8706f028":"markdown","f3d8efe4":"markdown","4631bee2":"markdown","7e8e82e1":"markdown"},"source":{"a325807a":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn as sk\nimport numpy as np\nimport scipy.stats as spy","a189c285":"\ntrain=pd.read_csv('..\/input\/house-prices-data\/train.csv')","b46cde34":"train.head()","9b0bca6e":"train.describe()","e3758f25":"train.dtypes","34028d9d":"train.isnull().sum()","057e623c":"print(train.shape)","8d5fc447":"train.info","bcd24540":"#distribution of the target variable \"sale price\"\nf,ax=plt.subplots(figsize=(18,8))\nsns.distplot(train['SalePrice'],color='r')\n\nplt.figure(figsize=(16,5))\ntrain.SalePrice.plot(kind=\"box\",vert=False,color='g')\nplt.title(\"Sales Price value distribution\")\nplt.xlabel(\"Sales Price\")\n\nplt.show()","69785c6d":"#lets write the function to identify the outliers\n\ndef outlier(datacolumn):\n    sorted(datacolumn)\n    Q1,Q3=np.percentile(datacolumn,[25,75])\n    IQR=Q3-Q1\n    lower_range=Q1-(3*IQR)\n    upper_range=Q3+(3*IQR)\n    return lower_range,upper_range, Q1,Q3,IQR","2ce335de":"lowerbound,upperbound, q1,q3,iqr=outlier(train.SalePrice)\nprint(lowerbound,upperbound,q1,q3,iqr)","6608f005":"train[(train.SalePrice < lowerbound) | (train.SalePrice > upperbound)]","ca1e891a":"#train.drop(train[(train.SalePrice > upperbound) |(train.SalePrice < lowerbound) ].index, inplace=True)","a3663d81":"#before we drop the outliers, lets identify skewness and kurtosis of the distribution\n\nprint(f'Skewness of sale price is {round(train.SalePrice.skew(),2)}')\nprint(f'Kurtosis of sale price is {round(train.SalePrice.kurt(),2)}')","e3f02711":"logsaleprice=np.log(train.SalePrice.values)\n\nplt.figure(figsize=(20,10))\nsns.distplot(logsaleprice,color='r')\nplt.title(\"Log Sales Price value distribution\")\nplt.xlabel(\"Sales Price\")\n\nplt.show()\n\n","bdc77388":"logsalepricedf=pd.DataFrame(logsaleprice)\n\nplt.figure(figsize=(20,10))\nlogsalepricedf.plot(kind=\"box\",vert=False,color='g')\nplt.title(\"Log Sales Price value distribution\")\nplt.xlabel(\"Sales Price\")\n\nplt.show()","64b3f76c":"#lets find the skewness of the log transformed salesprice\nprint(f'Skewness of log transformed sale price is {round(logsalepricedf.skew(),2)}')\nprint(f'Kurtosis of log transformed sale price is {round(logsalepricedf.kurt(),2)}')","b97e85a1":"#lets look at the Sale price data\ntrain['SalePrice'].describe()","232ce3ee":"# Now we will see how the other variables are related to Price.\n# Lets delete the ID column, since it is of no use\n\ntrain.drop('Id', axis=1)","17eea613":"trcorr=train.drop('Id', axis=1).select_dtypes(include='number').corr()\n\nplt.figure(figsize=(18,8))\ntrcorr[\"SalePrice\"].sort_values(ascending=True)[:-1].plot(kind=\"barh\")\nplt.title(\"Correlation of Numerical variables to SalePrice\")\nplt.xlabel(\"Correlation to SalePrice\")\nplt.tight_layout()\nplt.show()","0c8ea290":"plt.figure(figsize=(18,10))\nsns.heatmap(trcorr,annot=False, cmap=\"Blues\")\nplt.title(\"Correlation of Numerical Variables with Price\")","b480fa43":"#lets analyse each variable with Salesprice\n#Start with most correlated variable \"OverallQual\"\n\nplt.figure(figsize=(16,6))\ntrain.groupby(\"OverallQual\")['SalePrice'].count().plot(kind='bar')\nplt.title('Distribution sales price on over all quality')\nplt.show()\n\nplt.figure(figsize=(16,6))\nsns.boxplot(x='OverallQual',y='SalePrice', data=train)\nplt.title('OverallQual vs SalePrice')\nplt.show()","762ae024":"#GrLivArea\n\nplt.figure(figsize=(16,6))\nsns.scatterplot(x='GrLivArea',y='SalePrice', hue='OverallQual', data=train, legend='full')\nplt.title('GrLivArea vs SalePrice')\nplt.show()","2977c557":"train_bin=pd.DataFrame(pd.cut(train.GrLivArea, bins=10, labels=np.arange(0,10)))\ntrain_concat=pd.concat([train_bin,train.SalePrice], axis=1)\n\nplt.figure(figsize=(16,6))\ntrain_concat.groupby('GrLivArea').SalePrice.count().plot(kind='bar')\nplt.title(\"Count of observations in living area (binned values)\")\nplt.ylabel(\"Count\")\nplt.tight_layout()\nplt.show()\n\n","586407c3":"train_bin=pd.DataFrame(pd.cut(np.log(train.GrLivArea), bins=10, labels=np.arange(0,10)))\ntrain_concat=pd.concat([train_bin,train.SalePrice], axis=1)\n\nplt.figure(figsize=(16,6))\ntrain_concat.groupby('GrLivArea').SalePrice.count().plot(kind='bar')\nplt.title(\"Count of observations in living area (binned values)\")\nplt.ylabel(\"Count\")\nplt.tight_layout()\nplt.show()\n\nplt.figure(figsize=(16,6))\nsns.boxplot(x='GrLivArea',y='SalePrice', data=train_concat)\nplt.title('GrLivArea vs SalePrice')\nplt.show()\n","8264e014":"#plotting the other features using for loop\nlist(train.select_dtypes(include='number'))","b258a1e0":"features=['TotalBsmtSF','LotArea','GarageArea','1stFlrSF']\nfor i in features:\n    plt.figure(figsize=(16,6))\n    sns.scatterplot(x=i,y='SalePrice', hue='OverallQual', data=train, legend='full')\n    plt.title(f'{i} vs SalePrice')\n    plt.show()","2735b209":"#now lets look at time related features\n\nlist(train.columns)","1c19ae95":"# 'YrSold','MoSold','YearBuilt' are time related features, lets try to get some insights\n\nplt.figure(figsize=(16,6))\ntrain.groupby(\"YrSold\")['SalePrice'].count().plot(kind='bar')\nplt.title('Sales over the years')\nplt.show()\n\nplt.figure(figsize=(16,6))\nsns.boxplot(x='YrSold', y='SalePrice', data=train)\nplt.title(\"Distribution of Sales over years\")\nplt.show()\n\nimport calendar\nmonth_names=calendar.month_name[1:13]\n\nplt.figure(figsize=(16,6))\ntrain.groupby(\"MoSold\")['SalePrice'].count().plot(kind='bar')\nplt.title('Sales in different Months')\nplt.xticks(ticks=np.arange(0, 12), labels=month_names, rotation=45)\nplt.ylabel('SalePrice')\nplt.show()\n\nplt.figure(figsize=(16,6))\nsns.boxplot(x='MoSold', y='SalePrice', data=train)\nplt.title(\"Sales in different Months\")\nplt.xticks(ticks=np.arange(0, 12), labels=month_names, rotation=45)\nplt.ylabel('SalePrice')\nplt.show()","9b196bf6":"#plot the year built\n\nplt.figure(figsize=(16,6))\ntrain.groupby(\"YearBuilt\")['SalePrice'].count().plot(kind='bar')\nplt.title('Observation counts of built year of the houses')\nplt.show()\n\nplt.figure(figsize=(16,6))\nsns.boxplot(x='YearBuilt', y='SalePrice', data=train)\nplt.title(\"Built Year vs SalePrice\")\nplt.show()","ae332088":"#above visuals have too much clutter, its difficult to analyse. Lets bin the observations to simplify\n\ndecades=np.arange(1870, 2015,10)\nyrbuilt_bin = pd.cut(train.YearBuilt, bins=decades, labels=decades[:-1])\nyrb_comb = pd.concat([yrbuilt_bin,train.SalePrice],axis=1)\n                         \n#df_cut = pd.cut(df.YearBuilt, bins=decades, labels=decades[:-1])\n#df_comb = pd.concat([df_cut, df.SalePrice], axis=1)\n                         \nplt.figure(figsize=(16,6))\nyrb_comb.groupby(\"YearBuilt\").SalePrice.count().plot(kind='bar')\nplt.title('Observation counts of built year of the houses')\nplt.xlabel('Built Year')\nplt.ylabel('Count')\nplt.show()\n\nplt.figure(figsize=(16,6))\nsns.boxplot(x='YearBuilt', y='SalePrice', data=yrb_comb)\nplt.title(\"Built Year vs SalePrice\")\nplt.show()\n","d44a7e36":"#lets calculate the age of the property & analyse the sales\ntrain['age']=train['YrSold']- train['YearBuilt']\nprint(f\"Oldest property sold: {train['age'].max()} Yrs\")\nprint(f\"Most new property sold: {train['age'].min()} Yrs\")","7c889122":"#train_age=pd.concat([train['Age'],train['SalePrice']], axis=1)\ndecades=np.arange(0,136,10)\nage_bin = pd.cut(train.age, bins=decades, labels=decades[:-1])\nage_comb = pd.concat([age_bin,train.SalePrice],axis=1)\n\nplt.figure(figsize=(16,6))\nage_comb.groupby(\"age\").SalePrice.count().plot(kind='bar')\nplt.title('Observation counts of property age')\nplt.xlabel('Age')\nplt.ylabel('Count')\nplt.show()\n\nplt.figure(figsize=(16,6))\nsns.boxplot(x='age', y='SalePrice', data=age_comb)\nplt.title(\"Age vs SalePrice\")\nplt.show()","d820ff08":"#we will visualise some of the categorical variables \n#BldgType: type of the building\n#Neighborhood\n#Utilities\n#SaleType\n#MSSubClass: the building class\n#SaleCondition: Condition of the sale","6bae2360":"feat=['BldgType','Neighborhood','Utilities','SaleType','MSSubClass','SaleCondition']\n\nfor i in feat:\n    plt.figure(figsize=(16,6))\n    train.groupby(i).SalePrice.count().plot(kind='bar')\n    plt.title(f'Observation counts of the {i}')\n    plt.xlabel(f'{i}')\n    plt.ylabel('Count')\n    plt.show()   \n    \n    plt.figure(figsize=(16,6))\n    sns.boxplot(x=i, y='SalePrice', data=train)\n    plt.title(f\"{i} vs SalePrice\")\n    plt.show()\n\n\n","efedd389":"#freshly load the train & test data\ndf_train=pd.read_csv('..\/input\/house-prices-data\/train.csv')\ndf_test=pd.read_csv('..\/input\/house-prices-data\/test.csv')\n\n\n#we will concatenate the features of both data sets to find the missing values\n\ndf_comb = pd.concat([df_train, df_test]).reset_index(drop=True).copy()\ndf_comb.isnull().any()","08eb326a":"#fig, axes = plt.subplots(nrows=18, ncols=2, figsize=(16,36))\nnum=df_comb.drop(['Id','SalePrice'],axis=1).select_dtypes('number')\nfor idx, column in enumerate(num.columns[1:]):\n    num[column].plot(kind=\"hist\", bins=100, rwidth=.9, title=column)\n    #ax=axes[idx\/\/2, idx%2])\n    #ax=axes[idx\/\/2, idx%2].yaxis.label.set_visible(False)\n\n    plt.tight_layout()\n    plt.show()","360e523e":"df_comb.isna().sum()","d4737d21":"missing = df_comb.columns[df_comb.isna().any()]\nprint(missing)","15a9e4e1":"#Fix Missing Values in features\n\n#Alley: NA means, there is no Alley. Filling missing values with None\ndf_comb['Alley']=df_comb['Alley'].fillna(\"None\")\n  \n    \n#LotFrontage: filling missing values with Zero and converting to int\ndf_comb['LotFrontage']=df_comb['LotFrontage'].fillna(0).astype(int)\n\n#Masonry veneer type: None means there is no Masonry, we will fill with None\ndf_comb['MasVnrType']=df_comb['MasVnrType'].fillna('None')\ndf_comb['MasVnrArea']=df_comb['MasVnrArea'].fillna(0).astype(int)\n\n\n#all Basement features: NA means, no basement. Filling the string variables as 'None'\nbsmt=['BsmtQual','BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']\nfor i in bsmt:\n    df_comb[i]=df_comb[i].fillna(\"None\")\n    \nbsmtA=['BsmtFinSF1','BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF','BsmtFullBath','BsmtHalfBath']\nfor i in bsmtA:\n    df_comb[i]=df_comb[i].fillna(0).astype(int)\n\n#Electrical: NA means electrical system is unknown, filling with most occuring value \"SBrkr\"\ndf_comb['Electrical']=df_comb['Electrical'].fillna(\"SBrkr\")\n\n#FireplaceQu: NA means No fire place, filling with \"None\"\ndf_comb['FireplaceQu']=df_comb['FireplaceQu'].fillna('None')\n\n#Garage features: NA means, there is no Garage, filling with 'None'\ngarage=['GarageType','GarageFinish', 'GarageQual', 'GarageCond']\nfor i in garage:\n    df_comb[i]=df_comb[i].fillna(\"None\")\n\n#Garage Sizes: Filling with 0 and converting to in type\ngrg=['GarageYrBlt','GarageArea','GarageCars']\nfor i in grg:\n    df_comb[i]=df_comb[i].fillna(0).astype(int)\n\n#'MSZoning': The general zoning classification: filling it with most frequent values \"RL\"\n\ndf_comb['MSZoning']=df_comb['MSZoning'].fillna(\"RL\")\n    \n#'Exterior1st': Filling with most frequent value 'VinylSd'\ndf_comb['Exterior1st']=df_comb['Exterior1st'].fillna(\"VinylSd\")\n\n#Exterior2nd:NA means there is no second exterior, filling it with None\ndf_comb['Exterior2nd']=df_comb['Exterior2nd'].fillna(\"None\")\n\n# 'KitchenQual':NA means unknown, filling with most frequest value \"TA\"\ndf_comb['KitchenQual']=df_comb['KitchenQual'].fillna('TA')\n\n#Functional: filling with 'Typ'\ndf_comb['Functional']=df_comb['Functional'].fillna('Typ')\n\n#'PoolQC': AN means likely no pool\ndf_comb['PoolQC']=df_comb['PoolQC'].fillna('None')\n\n#'Fence':NA means, likely no fence\ndf_comb['Fence']=df_comb['Fence'].fillna('None')\n\n#'MiscFeature': NA means likely no other features\ndf_comb['MiscFeature']=df_comb['MiscFeature'].fillna('None')\n\n#'SaleType': Filling with most frequent values 'WD'\ndf_comb['SaleType']=df_comb['SaleType'].fillna('WD')\n\n# 'Utilities':filing with frequent value'AllPub'\ndf_comb['Utilities']=df_comb['Utilities'].fillna('AllPub')","2965db56":"object_var=df_comb.select_dtypes(include=['object'])\nobject_var.columns\n","e294011d":"#Encoding thses columns, we will use label encoder\nfrom sklearn import preprocessing\nle=preprocessing.LabelEncoder\nencode=le.fit_transform(le, df_comb.Alley)\npd.DataFrame(df_comb.Alley.value_counts().index, pd.Series(encode).value_counts().index)","ecafde67":"The distribution looks skewed and we can see lot of outliers in the distribution. These outliers may decrease the accuracy of the model. Hence we need to remove them, to remove the outliers using IQR method.\n\nTo do that, we need find Q1,Q3 and IQR. ","f31fe56b":"As you can see the, new skewness & Kurtosis is almost near to zero, which means log transformed saleprice follow normal distribution","bafe3889":"### Observations:\n1. There are lot of categorical values which are defines as numerical\n2. There are many missing values\n\nLets see the total missing values","c53cb70f":"In this Step, we will\n1. Fix the missing values\n2. Set correct data types\n3. Remove the outliers\n\n##### We will start by plotting the numerical data according to current pandas data types","e0bfa482":"We cannot use Label encoder, since there is no order for data, we will encode it manually.\n\n","d506e64b":"### Observations:\n\n* The distribution is positively skewed\n* Kurtosis is more than 3, hence it a Platykurtic, this implies the curve is flat. Kurtosis value should be near to zero then we can say the data are normally distributed.\n\nIn order to get the normal distribution, we will use Log Transformation method to achieve the normal curve.","4484de8d":"### Now we will encode the categorical values","136a0d89":"Observations:\n1. Fewer sales in 2010, might due to economic solwdown in 2009\n2. 2006-2008 have steady sales not much variance.\n3. Most sales in the month of may, june & july and maky be due to summer hoidays in US.","6bb20e33":"### Observations:\n\n1. There are total 1460 sales\n2. Average value is 180921\n3. Cheapest house sold: $34900\n\n4. The most expensive house sold: $755000","3d1cbb33":"## Lets Start with EDA\n\n    *Total there are 81 columns & 1460 rows\n    *Sale Price is the Y variable\n    *ID is the randomly generated number, we can ignore the column.\n    *We have to work with remaining 79 variables","c8c7b3ee":"Now you see, the values are normally distributed and we can easily identify the outliers.\n","fdc7c404":"Lets plot for other coorelated variables","539d319e":"### Observations:\n1. More newly built properties were sold compared to old ones.\n2. the mean sale price of the properties upto 10yrs old doesnt show much variance.\n3. Properties more than 100yrs old show huge variance, it means these can be outliers, which we will remove in later stages.","be5d1d5a":"# 2. Data Cleaning and formatting","59baf545":"### Observations:\n1. Price increases with increase in overall quality index.\n2. You can see there are more sales with ratings 5 to 7 in overall quality index and sales decreases with increase in price. Less number of expensive houses are sold.\n3. There very few sales on 1-3 & 8-10, it means we can some outliers here.","8706f028":"If we do not take the log of the GrLivArea then distribution is skewed, hence to achieve the normal distribution we will log transform it","f3d8efe4":"Observations:\nAs you see salesprice increases with living area, you can also see some outliers in the higher price and more living area. we will try to remove them.\n\nSince GrLivArea is continous variable, lets convert it to categorical variable by applying binning concept.\n use pd.cut() function and then combine saleprice & GrLivarea\n","4631bee2":"### Observations:\n1. Most of the houses were built in 1950 or later and in year 2000 highest number of houses were built.\n2. Houses built in 1990 or later yield higher mean of the sale price\n3. Houses built in 1890 or earlier have unusual variance and is likely to be outliers\n","7e8e82e1":"# 1. Exploratory Data Analysis"}}