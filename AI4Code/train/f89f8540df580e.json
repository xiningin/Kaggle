{"cell_type":{"2c51bc17":"code","4616a36b":"code","e2bd9b80":"code","c4bc36d2":"code","4ad28ceb":"code","fcefcffc":"code","beccbdf9":"code","1c29cc96":"code","de385b27":"code","06cdf1e8":"code","5427ef08":"markdown","e55cb712":"markdown","2ffccb0f":"markdown","a00d3b1b":"markdown","f14dd6ca":"markdown"},"source":{"2c51bc17":"import warnings\nwarnings.filterwarnings('ignore')","4616a36b":"from transformers import AutoModelForCausalLM, AutoTokenizer\n\nprint(\"Downloading models...\")\n#Model: DialoGPT - small\ntokenizer_dialoGPT_small = AutoTokenizer.from_pretrained(\"microsoft\/DialoGPT-small\")\nmodel_dialoGPT_small = AutoModelForCausalLM.from_pretrained(\"microsoft\/DialoGPT-small\")\nprint(\"Done!\")","e2bd9b80":"def generator_dialo_gpt_small(user_input, number_of_candidates):\n    # get input\n    user_input = user_input\n    number_of_candidates = number_of_candidates\n    input = tokenizer_dialoGPT_small.encode(user_input, tokenizer_dialoGPT_small.eos_token, return_tensors='pt')\n    \n    number_of_candidates = int(number_of_candidates)\n\n    beam_outputs = model_dialoGPT_small.generate(\n        input,\n        max_length=60,\n        num_beams=number_of_candidates,\n        no_repeat_ngram_size=2,\n        num_return_sequences=number_of_candidates,\n        early_stopping=True\n    )\n    #print(beam_outputs)\n\n    beam_output = []\n    for step in range(number_of_candidates):\n        m = format(tokenizer_dialoGPT_small.decode(beam_outputs[:, input.shape[-1]:][step], skip_special_tokens=True))\n        beam_output.append(m)\n\n    return beam_output\n\n","c4bc36d2":"candidate = generator_dialo_gpt_small(\"Where did you go?\", 3)","4ad28ceb":"candidate","fcefcffc":"from transformers import pipeline\nfill_mask = pipeline(\"fill-mask\")","beccbdf9":"fill_mask(\"I'm going to the <mask> tonight!\")","1c29cc96":"fill_mask(\"<mask> went to school with him.\")","de385b27":"mask = fill_mask(\"I went to school with him. He's a <mask> guy.\")","06cdf1e8":"for tokens in mask:\n    print(tokens['token_str'])","5427ef08":"# Imports & Model downloading","e55cb712":"# Viewing the Tokens only for the **< mask >**","2ffccb0f":"# Generating Response for a given Dialogue","a00d3b1b":"# Predicting Mask\n#### The one with the highest score is considered the best response to the **< mask >**","f14dd6ca":"# Pipeline for prediction"}}