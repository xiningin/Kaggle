{"cell_type":{"4b7f446c":"code","eb2c43c1":"code","0cad7931":"code","a23f0afe":"code","63494e9e":"code","053a2e0e":"code","88d8186e":"code","e6525393":"code","b5930e42":"code","07260a76":"code","09ee892d":"code","9ffd0741":"code","72b2fe93":"code","cbbfcf25":"code","67db1e23":"code","bbaf42bb":"code","6b2d3201":"code","279b7427":"code","7de12791":"code","26c0eb86":"code","85cba405":"code","a30df785":"code","51647818":"code","2e76d571":"code","35fa3977":"code","1e0c01e5":"code","e1c3cc76":"code","1cb063c9":"code","e9115917":"code","d709c21d":"code","48ecf284":"code","b7b6553d":"code","aeaff9c7":"code","168fc7f8":"code","c886e045":"code","2c45dd4b":"markdown","51a76d7f":"markdown","9890c9d1":"markdown","69d5dcda":"markdown","1dab1a9c":"markdown","777442ea":"markdown","799499fe":"markdown","baee296d":"markdown","f663ede4":"markdown","4383c8c7":"markdown","1d122b4f":"markdown","83cffc60":"markdown","477dd8f4":"markdown","e735a039":"markdown","ad2d4ee4":"markdown","e6259531":"markdown","ef007a37":"markdown","ae89903d":"markdown","9b4074ab":"markdown","e46527f5":"markdown","42701082":"markdown","ea16e012":"markdown","33c2cb5a":"markdown","34769919":"markdown","ea87e55a":"markdown","04be7c5a":"markdown","b9704016":"markdown","d24222d6":"markdown","39219898":"markdown","085a78c1":"markdown","e85a73da":"markdown","ae2bad05":"markdown","c1fa55eb":"markdown","db6e1bff":"markdown","0c1841da":"markdown","c24b0812":"markdown","073d66dc":"markdown","d128d8a3":"markdown"},"source":{"4b7f446c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport datatable as dt\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.ticker import MaxNLocator\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.cluster import KMeans\n\nimport plotly.express as px\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew, kurtosis\n\nimport gc\nplt.style.use('ggplot')\n\ncust_color = [\n    '#EDC7B7',\n    '#EEE2DC',\n    '#BAB2B5',\n    '#123C69',\n    '#AC3B61'\n]\n\nplt.rcParams['figure.figsize'] = (18,14)\nplt.rcParams['figure.dpi'] = 300\nplt.rcParams[\"axes.grid\"] = True\nplt.rcParams[\"grid.color\"] = cust_color[0]\nplt.rcParams[\"grid.alpha\"] = 0.5\nplt.rcParams[\"grid.linestyle\"] = '--'\nplt.rcParams[\"font.family\"] = \"monospace\"\n\nplt.rcParams['axes.edgecolor'] = 'black'\nplt.rcParams['figure.frameon'] = False\nplt.rcParams['axes.spines.left'] = True\nplt.rcParams['axes.spines.bottom'] = True\nplt.rcParams['axes.spines.top'] = False\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.linewidth'] = 1.0\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n","eb2c43c1":"df = dt.fread('..\/input\/ubiquant-market-prediction\/train.csv').to_pandas()","0cad7931":"df.head()","a23f0afe":"print(f'Train df number of instance: {df.shape[0]}')","63494e9e":"print(f'Train df missing value count: {df.isna().sum().sum()}')","053a2e0e":"print(f'Train df number of unique investments: {df.investment_id.nunique()}')","88d8186e":"print(f'Train df number of unique time_id\\'s: {df.time_id.nunique()}')","e6525393":"time_count=df.groupby(\"investment_id\")['time_id'].count()\nfig, ax = plt.subplots(figsize=(12,9))\nsns.histplot(time_count, color=cust_color[-1], kde=True)\nplt.title('Number of time_id\\'s per Investment Distribution')\nplt.show()","b5930e42":"sampled_df = df.sample(frac=0.05, random_state=42)","07260a76":"# from statsmodels.stats.weightstats import ztest\n# diff = np.mean(df.target) - np.mean(sampled_df.target)\n# t, p = ztest(df.target, x2=sampled_df.target, value=diff)\n# (np.nanmean(sampled_df.target) - np.nanmean(df.target)) \/ df.target.std()","09ee892d":"del df\ngc.collect()","9ffd0741":"features = [f'f_{i}' for i in range(300)]\n\nfor f in features:\n    sampled_df[f] = sampled_df[f].astype('float16')","72b2fe93":"def plot_dist3(df, feature, title):\n    \n    # Creating a customized chart. and giving in figsize and everything.\n    \n    fig = plt.figure(constrained_layout=True)\n    \n    # creating a grid of 3 cols and 3 rows.\n    \n    grid = gridspec.GridSpec(ncols=3, nrows=2, figure=fig)\n\n    # Customizing the histogram grid.\n    \n    ax1 = fig.add_subplot(grid[0, :2])\n    \n    # Set the title.\n    \n    ax1.set_title('Histogram')\n    \n    # plot the histogram.\n    \n    sns.distplot(df.loc[:, feature],\n                 hist=True,\n                 kde=True,\n                 fit=norm,\n                  hist_kws={\n                 'rwidth': 0.85,\n                 'edgecolor': 'black',\n                 'linewidth':.5,\n                 'alpha': 0.8},\n                 ax=ax1,\n                 color=cust_color[-1])\n    \n    ax1.axvline(df.loc[:, feature].mean(), color='Green', linestyle='dashed', linewidth=3)\n\n    min_ylim, max_ylim = plt.ylim()\n    ax1.text(df.loc[:, feature].mean()*2, max_ylim*0.95, 'Mean: {:.2f}'.format(df.loc[:, feature].mean()), color='Green', fontsize='12',\n             bbox=dict(boxstyle='round',facecolor='red', alpha=0.5))\n    ax1.legend(labels=['Actual','Normal'])\n    ax1.xaxis.set_major_locator(MaxNLocator(nbins=12))\n    \n    ax2 = fig.add_subplot(grid[1, :2])\n    \n    # Set the title.\n    \n    ax2.set_title('Probability Plot')\n    \n    # Plotting the QQ_Plot.\n    stats.probplot(df.loc[:, feature],\n                   plot=ax2)\n    ax2.get_lines()[0].set_markerfacecolor('#e74c3c')\n    ax2.get_lines()[0].set_markersize(12.0)\n    ax2.xaxis.set_major_locator(MaxNLocator(nbins=16))\n\n    # Customizing the Box Plot:\n    \n    ax3 = fig.add_subplot(grid[:, 2])\n    # Set title.\n    \n    ax3.set_title('Box Plot')\n    \n    # Plotting the box plot.\n    \n    sns.boxplot(y=feature, data=df, ax=ax3, color=cust_color[-1])\n    ax3.yaxis.set_major_locator(MaxNLocator(nbins=24))\n    #ax3.set_ylim(0,clip_value)\n\n    plt.suptitle(f'{title}', fontsize=24, fontname = 'monospace', weight='bold')","cbbfcf25":"plot_dist3(sampled_df, 'target', 'Target Distribution')","67db1e23":"features_std = sampled_df.iloc[:,4:].apply(lambda x: np.std(x)).sort_values(\n    ascending=False)\nf_std = sampled_df[features_std.iloc[:20].index.tolist()]\n\nwith pd.option_context('mode.use_inf_as_na', True):\n    features_skew = np.abs(sampled_df.iloc[:,4:].apply(lambda x: np.abs(skew(x))).sort_values(\n        ascending=False)).dropna()\nskewed = sampled_df[features_skew.iloc[:20].index.tolist()]\n\nwith pd.option_context('mode.use_inf_as_na', True):\n    features_kurt = np.abs(sampled_df.iloc[:,4:].apply(lambda x: np.abs(kurtosis(x))).sort_values(\n        ascending=False)).dropna()\nkurt_f = sampled_df[features_kurt.iloc[:20].index.tolist()]","bbaf42bb":"def feat_dist(df, cols, rows=3, columns=3, title=None, figsize=(30, 25)):\n    \n    '''A function for displaying skew feat distribution'''\n    \n    fig, axes = plt.subplots(rows, columns, figsize=figsize, constrained_layout=True)\n    axes = axes.flatten()\n\n    for i, j in zip(cols, axes):\n        sns.distplot(\n                    df[i],\n                    ax=j,\n                    fit=norm,\n                    hist=False,\n                    color=cust_color[-1],\n                    kde_kws={'linewidth':3}\n        )   \n        \n        (mu, sigma) = norm.fit(df[i])\n        j.set_title('Dist of {0} Norm Fit: $\\mu=${1:.2g}, $\\sigma=${2:.2f}'.format(i, mu, sigma), weight='bold')\n        j.legend(labels=[f'{i}', 'Normal Dist'])\n        fig.suptitle(f'{title}', fontsize=24, weight='bold')","6b2d3201":"feat_dist(sampled_df, f_std.columns.tolist(), rows=2, columns=4, title='Distribution of High Std Features', figsize=(30, 8))","279b7427":"# Creating distplot of features which has high skewness\n\nfeat_dist(sampled_df, skewed.columns.tolist(), rows=5, columns=4, title='Distribution of Skewed Features')","7de12791":"# Creating distplot of features which has high Kurtosis\n\nfeat_dist(sampled_df, kurt_f.columns.tolist(), rows=5, columns=4, title='Distribution of High Kurtosis Features')","26c0eb86":"correlations = sampled_df.corrwith(sampled_df['target']).iloc[:-1].to_frame()\ncorrelations['Abs Corr'] = correlations[0].abs()\nsorted_correlations = correlations.sort_values('Abs Corr', ascending=False)['Abs Corr']\nfig, ax = plt.subplots(figsize=(6,8))\nsns.heatmap(sorted_correlations.iloc[1:].to_frame()[sorted_correlations>=.04], cmap='inferno', annot=True, vmin=-1, vmax=1, ax=ax)\nplt.title('Feature Correlations With Target')\nplt.show()","85cba405":"corr = sampled_df.iloc[:, 4:].corr()\nsns.clustermap(corr, metric=\"correlation\", cmap=\"inferno\", figsize=(20, 20))\nplt.suptitle('Correlations Between Features', fontsize=24, weight='bold')\nplt.show()\n","a30df785":"corr = corr.abs()\n\ncorrs = corr.unstack()\npair = corrs.sort_values(ascending=False)\npair = pair.reset_index(name='correlation').rename(columns={'level_0': 'feature_a', 'level_1': 'feature_b', 0: 'correlation'})\npair = pair[pair['feature_a'] != pair['feature_b']].iloc[::2,:]\npair = pair[:10]\npair","51647818":"sns.jointplot(sampled_df[pair['feature_a'].iloc[0]], sampled_df[pair['feature_b'].iloc[0]], kind=\"reg\", color=cust_color[0], height=8,\n              joint_kws={'scatter_kws':dict(alpha=0.5, edgecolor=\"r\", linewidth=0.5)})\nplt.show()","2e76d571":"def hex_plot(df, rows=3, columns=3, title=None):\n    \n    '''A function for displaying skew feat distribution'''\n    \n    fig, axes = plt.subplots(rows, columns, figsize=(30, 25), constrained_layout=True)\n    axes = axes.flatten()\n\n    for i,j in enumerate(axes):\n        j.hexbin(sampled_df[pair['feature_a'].iloc[i]], sampled_df[pair['feature_b'].iloc[i]],  gridsize=100, cmap='inferno', bins='log')\n        j.set_xlabel(pair['feature_a'].iloc[i])\n        j.set_ylabel(pair['feature_b'].iloc[i])\n\n        fig.suptitle(f'{title}', fontsize=24, weight='bold')","35fa3977":"hex_plot(sampled_df, rows=5, columns=2, title='Highly Correlated Features')","1e0c01e5":"features = sampled_df.iloc[:, 4:].columns.tolist()\n\n\npipe = Pipeline([('scaler', StandardScaler()),('pca', PCA())])\npipe.fit(sampled_df[features])\npca_samples = pipe.transform(sampled_df[features])\n\n# explaining variance ratio:\n\nfig, ax = plt.subplots(figsize=(14, 5))\nplt.plot(range(sampled_df[features].shape[1]), pipe.named_steps['pca'].explained_variance_ratio_.cumsum(), linestyle='--', drawstyle='steps-mid', color=cust_color[-1],\n         label='Cumulative Explained Variance', linewidth = 1.5)\nsns.barplot(np.arange(1,sampled_df[features].shape[1]+1), pipe.named_steps['pca'].explained_variance_ratio_, alpha=0.85, color=cust_color[0],\n            label='Individual Explained Variance', edgecolor='black', saturation = 2, linewidth = 0.5)\n\nplt.ylabel('Explained Variance Ratio', fontsize = 14, fontname = 'monospace', weight='semibold')\nplt.xlabel('Number of Principal Components', fontsize = 14, fontname = 'monospace', weight='semibold')\nax.set_title('Explained Variance', fontsize = 20, fontname = 'monospace', weight='bold')\nplt.xticks(fontsize=8, rotation=90)\nplt.legend(fontsize = 13)\nplt.axis([0,99,0,1])\nplt.show()","e1c3cc76":"loadings = pd.DataFrame(pipe.named_steps['pca'].components_[0:3, :], columns=features)\nmaxPC = 1.01 * np.max(np.max(np.abs(loadings.loc[0:5, :])))\n\nfig, axes = plt.subplots(3, 1, figsize=(12, 9))\nfor i, ax in enumerate(axes):\n    pc_loadings = loadings.loc[i, :]    \n    colors = [cust_color[0] if l > 0 else cust_color[-1] for l in pc_loadings]\n    sns.barplot(x=pc_loadings.index, y=pc_loadings, ax=ax, palette=colors)\n    ax.axhline(color='#888888')\n    ax.set_ylabel(f'PC{i+1}')\n    ax.set_ylim(-maxPC, maxPC)    \n    ax.xaxis.set_tick_params(labelsize=3, rotation=90)\n    \nplt.suptitle(\"Component Loadings\")\nplt.tight_layout()","1cb063c9":"kmeans_per_k = [Pipeline([('scaler', StandardScaler()),('km', KMeans(n_clusters=k, random_state=42, max_iter=100, n_init=5, tol=1e-4))]).fit(sampled_df[features])\n                for k in range(1, 8)]\ninertias = [model.named_steps['km'].inertia_ for model in kmeans_per_k]\n\nplt.figure(figsize=(6, 3))\nsns.lineplot(range(1, 8), inertias, color=cust_color[-2], linewidth = 1.5)\nplt.xlabel(\"k\", fontsize=15)\nplt.ylabel(\"Inertia\", fontsize=15)\n\nplt.title('Inertias and n_clusters', fontname = 'monospace', weight='bold')\nplt.show()","e9115917":"kmeans = Pipeline([('scaler', StandardScaler()),('km', KMeans(n_clusters=4, random_state=42, max_iter=100, tol=1e-4))]).fit(sampled_df[features])\nclusters = kmeans.fit_predict(sampled_df[features])\nclusters = [str(number) for number in clusters]","d709c21d":"pipe = Pipeline([('scaler', StandardScaler()),('pca', PCA(n_components=2))])\npipe.fit(sampled_df[features])\npca_samples = pipe.transform(sampled_df[features])\nsns.scatterplot(pca_samples[:,0], pca_samples[:,1], hue=clusters, palette=cust_color[1:5])\nplt.title(\"Clusters on Reduced Dimension\")\nplt.show()","48ecf284":"centers = pd.DataFrame(kmeans.named_steps['km'].cluster_centers_, columns=features)\nfig, axes = plt.subplots(4, 1, figsize=(12, 12))\nfor i, ax in enumerate(axes):\n    center = centers.loc[i, :]\n    maxPC = 1.01 * np.max(np.max(np.abs(center)))\n    colors = [cust_color[0] if l > 0 else cust_color[-1] for l in center]\n    ax.axhline(color='#888888')\n    sns.barplot(x=center.index, y=center, ax=ax, palette=colors)\n    ax.set_ylabel(f'Cluster {i}')\n    ax.set_ylim(-maxPC, maxPC)\n    ax.xaxis.set_tick_params(labelsize=3, rotation=90)\n    \nplt.suptitle(\"Centroid Coordinates\")\nplt.tight_layout()","b7b6553d":"pipe = Pipeline([('scaler', StandardScaler()),('pca', PCA(n_components=4))])\npipe.fit(sampled_df[features])\npca_samples = pipe.transform(sampled_df[features])\n\ntotal_var = pipe.named_steps['pca'].explained_variance_ratio_.sum() * 100\n\nlabels = {\n    str(i): f\"PC {i+1} ({var:.1f}%)\"\n    for i, var in enumerate(pipe.named_steps['pca'].explained_variance_ratio_ * 100)\n}\nlabels['color'] = 'Cluster'\n\nfig = px.scatter_matrix(\n    pca_samples,\n    color=clusters,\n    #symbol=clusters,\n    dimensions=range(4),\n    labels=labels,\n    title=f'Total Explained Variance: {total_var:.2f}% by Clusters',\n    opacity=0.5,\n    color_discrete_sequence=cust_color[1:5]\n)\nfig.update_traces(diagonal_visible=False)\nfig.show()","aeaff9c7":"sampled_df.sort_values(by='time_id', inplace=True)\nsampled_df['target_cumsum']=sampled_df.groupby(['investment_id'])['target'].transform('cumsum')","168fc7f8":"fig, ax = plt.subplots(3,1, figsize=(12,12))\n\nsns.lineplot(sampled_df.groupby('time_id')['investment_id'].nunique().index, sampled_df.groupby('time_id')['investment_id'].nunique(), color=cust_color[-1], ax=ax[0])\nax[0].set_ylabel('Observation Count')\nax[0].set_title('Number of Observations by Time')\n\nsns.regplot(sampled_df.groupby('time_id')['target'].mean().index, sampled_df.groupby('time_id')['target'].mean(), color=cust_color[0],\n           scatter_kws=dict(alpha=0.5, edgecolor=\"r\", linewidth=0.5), line_kws=dict(color=cust_color[-1]), ax=ax[1], order=2, ci=None)\nax[1].set_ylabel('Mean Target')\nax[1].set_title('Target Values by Time')\n\nsns.regplot(sampled_df.groupby('time_id')['target_cumsum'].mean().index, sampled_df.groupby('time_id')['target_cumsum'].mean(), color=cust_color[0],\n           scatter_kws=dict(alpha=0.5, edgecolor=\"r\", linewidth=0.5), line_kws=dict(color=cust_color[-1]), ax=ax[2], order=2, ci=None)\nax[2].set_ylabel('Mean Cumulative Target')\nax[2].set_title('Cumulative Target Values by Time')\nplt.tight_layout()","c886e045":"sampled_df['time_target_mean']=sampled_df.groupby(['time_id'])['target'].transform('mean')\nplot_dist3(sampled_df, 'time_target_mean', 'Mean Target by Time')","2c45dd4b":"Let's try our luck with clustering, maybe we can fit some instances into specific clusters so it can allow us to breakdown the problem and inspect different groups individualy. Let's see how many clusters we would need...","51a76d7f":"We can see a pattern when we inspect the table above. You can see most of the features diverges from others (bars going down) in cluster 0, it's little bit hard to detect differences in the next clusters. Especially when the feature names are not given. We could get some more insights if we had feature names and knowledge in the field. But anyways it reveals the nature of some clusters to us. Let's look one more thing;","9890c9d1":"# Target Distribution","69d5dcda":"Investments:","1dab1a9c":"# Correlation Between Features","777442ea":"It seems most of the investments ID's having 800+ timestamp records, while there are quite a number of them having much less with left skewed distribution...","799499fe":"So the first component loadings usually have same signs, typically it's expected when most of the variables share a common factor. Since it's investment data it could indicate something like bull run or booming economy, but without knowing actual variable names we can't be sure. We also don't observe a \"Dominant\" loading in first three principal components (these three only explain around 30% of the variance anyways) so we can't make precise interpretations from these tables above. But it's good to see them for getting more insights about features and their behaviours together.","baee296d":"This just confirms our suspicion there are some strongly correlated features, and we can see which features are mostly correlated. Let's take a closer look at the most correlated one: f_262 and f_228:","f663ede4":"We can see many features with asymmetric tails above.","4383c8c7":"Yeah we can see strong positive correlation between these two, let's take a look at the general picture with hexbins since there are many points to scatter this method can give us better picture:","1d122b4f":"When we include time aspect in the target distribution we can see the distribution gets some fairly long tails indicating some outliers, usually on the positive side; while big chunk of the data in on negative side. We should inspect this further to see which investments are affecting this statistic a lot. (Since we calculated them just by mean which isn't robust to outliers.) ","83cffc60":"Above we can see some features with sharp and tall central peaks with long tails","477dd8f4":"We do have many features but it seems we cannot reduce dimensions without losing some signals. Even for explaining the 80% variance we might have to use 100 principal components. Next let's take a look at component loadings for first three principal components:","e735a039":"Just one more thing since we working with full data...","ad2d4ee4":"# Correlations Between Features","e6259531":"Time ID's","ef007a37":"Looks like our features standardized too as our targets...","ae89903d":"We can see some strong correlations between features, and we can clearly see they get into some clusters by looking at the dendograms...","9b4074ab":"# Dimension Reduction and Clusters\n\nSince the data is anonymized and lacking categorical variables we might want to look at some reduced dimension plots and use some unsupervised techniques to see if we can find some patterns.","e46527f5":"Missing Values:","42701082":"# Time\n\nSo we looked at many of the variables independent from time, let's take a look how're things looking if we include the time.\n\nSince we know number of observations per time id is not uniformly distributed, we can check how it affects our target.","ea16e012":"# Exploratory Data Analysis","33c2cb5a":"# Some 'Odd' Feature Distributions\n\nThese are the top features (top 20 to be exact) where their distribution doesn't perfectly fit \"normal\" standards. We might something useful four our models by looking at them.","34769919":"# Feature Target Correlation","ea87e55a":"## Work in Progress...\nBest of luck in the competition :)","04be7c5a":"Target has decent distribution centered around 0 with a peak in the middle. We can notice long tails indicating some outliers and it can be confirmed by the othet plots. The mean is almost 0 so it's a strong clue for standardization.","b9704016":"Almost no linear correlation between features and target... Of course that doesn't mean they're useless, we didn't include lot's of aspects of the data into these.","d24222d6":"Number of instances:","39219898":"# Overview\n- The goal of the competition is to predict the value (obfuscated metric about investment's return rate) which is related to:\n    - Dozens of features anonymized by hosts and probably not only the names but values themselves are processed too.\n    - Different investments, again we only know their id's.\n    - Observations about these investments are taken in different times;\n        - ID's are in order but the time between them can vary.\n        - Each time ID doesn't neccesarily to include all investments.\n    - From competition page we have this:\n> In this competition, you\u2019ll build a model that forecasts an investment's return rate. Train and test your algorithm on historical prices.\n        - Not sure if this is just a general use of the term \"historical prices\" or giving us a hint about the features...\n- In test set we expect to see roughly one million instances. (Info shared by hosts)\n    - We also know test observations will be taken after the training observation period.\n    - This is code competition so our submissions will be made from inside of a Kaggle notebook.\n        - These submissions will be evaluated using time-series API, hosts kindly reminded us about the memory constraints of this approach.\n        - Evaluation metric is mean of the [Pearson correlation coefficient](https:\/\/en.wikipedia.org\/wiki\/Pearson_correlation_coefficient) for each time ID.\n        \n## Motivation\nThe main motivation behind this notebook is to gain insights about the data, finding underlying patterns from the anonymized variables that could help with creating predictive models. Since we have pretty big dataset I believe jumping right into modelling part by creating complex models would be waste of resources and your time. By doing EDA we could gain an advantage in the modelling part.","085a78c1":"Well we have clusters but they don't mean much yet... Clusters are looking pretty close to each other. Let's look at them in another way including cluster centers in 300d space...","e85a73da":"Hmm, Doesn't look good... Anyways we have the sharpest elbow at k=2 but let's try k=4 it has also somewhat decent curve.","ae2bad05":"We can see there is only one or two high std feature (f_170 and f_124) rest is just about same which indicates the preprocessing of the data by hosts...","c1fa55eb":"We can clearly see there are strong linear correlations between some features either negative or positive. We should take a closer look to these variables to prevent multicollinearity while modelling...","db6e1bff":"## First Look\n\nWe have train, test and submission .csv's, let's take a look at first instances of our train data first.","0c1841da":"The picture doesn't change much when we plot first four components against each other too. Oh well...\n\nNext we should decide what method we can use to get some more insights using classical EDA techniques.","c24b0812":"It seems number of observations taken into training data has increasing trend by time with some weird movements around 300-500th time id range (sharp drops). When we check mean target values by time_id we can see it's causing some outliers...\n\nWhen we check the cumulative gains by the target we can observe a negative trend though.","073d66dc":"Converting features \"float16\" to save some memory.","d128d8a3":"# Random Sampling\n\nSince we have high number of instances (3141410) let's take some samples representing the actual population. This will let us to do faster interpretations...\n"}}