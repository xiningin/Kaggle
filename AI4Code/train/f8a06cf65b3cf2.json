{"cell_type":{"76bb324e":"code","6d323f1f":"code","257e4125":"code","08633017":"code","585f7797":"code","86ee021c":"code","c0972f1a":"code","b7294917":"code","e770f1e0":"code","1861f370":"code","485df8d8":"code","6643e1ba":"code","b4ef4057":"code","a1780136":"code","fdbbbabe":"code","4793718d":"code","f46f98b1":"code","53ef38c4":"code","6da9e5a2":"code","41eb8949":"code","615e8870":"code","6f07391b":"code","49d116a2":"code","8440f12a":"code","565d9e6b":"code","8ba5951b":"markdown","326c03f9":"markdown","ef33514e":"markdown","cc60704c":"markdown","7012de2e":"markdown","6e955d2b":"markdown","a0e63c23":"markdown","11ef3c14":"markdown","2bcc7a77":"markdown","9b372696":"markdown","b44b31c0":"markdown","4420a201":"markdown","1473ef8f":"markdown","49837eb6":"markdown","17a36aeb":"markdown"},"source":{"76bb324e":"!mkdir ~\/.keras\n!mkdir ~\/.keras\/models\n!cp ..\/input\/keras-pretrained-models\/*notop* ~\/.keras\/models\/\n!cp ..\/input\/keras-pretrained-models\/imagenet_class_index.json ~\/.keras\/models\/\n!cp ..\/input\/keras-pretrained-models\/resnet50* ~\/.keras\/models\/","6d323f1f":"%matplotlib inline\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom glob import glob \nimport matplotlib.pyplot as plt\nimport os\nimport pandas as pd\nimport seaborn as sns\nfrom skimage.util.montage import montage2d\nfrom skimage.io import imread\nbase_dir = os.path.join('..', 'input', 'plant-seedlings-classification')\n","257e4125":"from sklearn.preprocessing import LabelEncoder\nfrom keras.utils.np_utils import to_categorical\ncat_enc = LabelEncoder()\nimg_paths = glob(os.path.join(base_dir, 'train', '*', '*.*'))\nprint('Image Files', len(img_paths))\nall_paths_df = pd.DataFrame(dict(path = img_paths))\nall_paths_df['category'] = all_paths_df['path'].map(lambda x: x.split('\/')[-2])\ncat_enc.fit(all_paths_df['category'])\nall_paths_df['cat_id'] = all_paths_df['category'].map(lambda x: \n                                                      cat_enc.transform([x])[0])\nall_paths_df['cat_vec'] = all_paths_df['cat_id'].map(lambda x: to_categorical(x, len(cat_enc.classes_)))\nall_paths_df['file_id'] = all_paths_df['path'].map(lambda x: os.path.splitext(os.path.basename(x))[0])\nall_paths_df.sample(5)","08633017":"all_paths_df[['cat_id']].hist(figsize = (10, 5))","585f7797":"from sklearn.model_selection import train_test_split\nraw_train_df, valid_df = train_test_split(all_paths_df, \n                                   test_size = 0.02, \n                                   random_state = 2018,\n                                   stratify = all_paths_df[['cat_id']])\nprint('train', raw_train_df.shape[0], 'validation', valid_df.shape[0])\nraw_train_df.sample(1)","86ee021c":"train_df = raw_train_df.groupby(['cat_id']).apply(lambda x: x.sample(500, replace = True)\n                                                      ).reset_index(drop = True)\nprint('New Data Size:', train_df.shape[0], 'Old Size:', raw_train_df.shape[0])\ntrain_df[['cat_id']].hist(bins = len(cat_enc.classes_), figsize = (10, 5))","c0972f1a":"from keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.imagenet_utils import preprocess_input\nfrom PIL import Image\nppi = lambda x: Image.fromarray(preprocess_input(np.array(x).astype(np.float32)))\nIMG_SIZE = (224, 224) # slightly smaller than vgg16 normally expects\ncore_idg = ImageDataGenerator(samplewise_center=False, \n                              samplewise_std_normalization=False, \n                              horizontal_flip = True, \n                              vertical_flip = False, \n                              height_shift_range = 0.15, \n                              width_shift_range = 0.15, \n                              rotation_range = 5, \n                              shear_range = 0.01,\n                              fill_mode = 'nearest',\n                              zoom_range=0.2)","b7294917":"def flow_from_dataframe(img_data_gen, in_df, path_col, y_col, **dflow_args):\n    base_dir = os.path.dirname(in_df[path_col].values[0])\n    print('## Ignore next message from keras, values are replaced anyways')\n    df_gen = img_data_gen.flow_from_directory(base_dir, \n                                     class_mode = 'sparse',\n                                    **dflow_args)\n    df_gen.filenames = in_df[path_col].values\n    df_gen.classes = np.stack(in_df[y_col].values)\n    df_gen.samples = in_df.shape[0]\n    df_gen.n = in_df.shape[0]\n    df_gen._set_index_array()\n    df_gen.directory = '' # since we have the full path\n    print('Reinserting dataframe: {} images'.format(in_df.shape[0]))\n    return df_gen","e770f1e0":"train_gen = flow_from_dataframe(core_idg, train_df, \n                             path_col = 'path',\n                            y_col = 'cat_vec', \n                            target_size = IMG_SIZE,\n                             color_mode = 'rgb',\n                            batch_size = 32)\nvalid_gen = flow_from_dataframe(core_idg, valid_df, \n                             path_col = 'path',\n                            y_col = 'cat_vec', \n                            target_size = IMG_SIZE,\n                             color_mode = 'rgb',\n                            batch_size = 256) # we can use much larger batches for evaluation\n# used a fixed dataset for evaluating the algorithm\ntest_X, test_Y = next(flow_from_dataframe(core_idg, \n                               valid_df, \n                             path_col = 'path',\n                            y_col = 'cat_vec', \n                            target_size = IMG_SIZE,\n                             color_mode = 'rgb',\n                            batch_size = 1024)) # one big batch","1861f370":"t_x, t_y = next(train_gen)\nfig, m_axs = plt.subplots(2, 4, figsize = (16, 8))\nfor (c_x, c_y, c_ax) in zip(t_x, t_y, m_axs.flatten()):\n    c_ax.imshow(c_x[:,:].clip(0, 255).astype(np.uint8))\n    c_ax.set_title('%s' % cat_enc.classes_[np.argmax(c_y)])\n    c_ax.axis('off')","485df8d8":"from keras.applications.vgg16 import VGG16\nfrom keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten, Input, Conv2D, multiply, LocallyConnected2D, Lambda\nfrom keras.models import Model\nin_lay = Input(t_x.shape[1:])\nbase_pretrained_model = VGG16(input_shape =  t_x.shape[1:], \n                              include_top = False, weights = 'imagenet')\nbase_pretrained_model.trainable = False\npt_depth = base_pretrained_model.get_output_shape_at(0)[-1]\npt_features = base_pretrained_model(in_lay)\nfrom keras.layers import BatchNormalization\nbn_features = BatchNormalization()(pt_features)\n\n# here we do an attention mechanism to turn pixels in the GAP on an off\n\nattn_layer = Conv2D(64, kernel_size = (1,1), padding = 'same', activation = 'relu')(bn_features)\nattn_layer = Conv2D(16, kernel_size = (1,1), padding = 'same', activation = 'relu')(attn_layer)\nattn_layer = Conv2D(1, \n                    kernel_size = (1,1), \n                    padding = 'valid', \n                    activation = 'sigmoid')(attn_layer)\n# fan it out to all of the channels\nup_c2_w = np.ones((1, 1, 1, pt_depth))\nup_c2 = Conv2D(pt_depth, kernel_size = (1,1), padding = 'same', \n               activation = 'linear', use_bias = False, weights = [up_c2_w])\nup_c2.trainable = False\nattn_layer = up_c2(attn_layer)\n\nmask_features = multiply([attn_layer, bn_features])\ngap_features = GlobalAveragePooling2D()(mask_features)\ngap_mask = GlobalAveragePooling2D()(attn_layer)\n# to account for missing values from the attention model\ngap = Lambda(lambda x: x[0]\/x[1], name = 'RescaleGAP')([gap_features, gap_mask])\ngap_dr = Dropout(0.5)(gap)\ndr_steps = Dropout(0.5)(Dense(128, activation = 'elu')(gap_dr))\nout_layer = Dense(len(cat_enc.classes_), activation = 'softmax')(dr_steps)\ntb_model = Model(inputs = [in_lay], outputs = [out_layer])\n\ntb_model.compile(optimizer = 'adam', loss = 'categorical_crossentropy',\n                           metrics = ['categorical_accuracy'])\n\ntb_model.summary()","6643e1ba":"!rm -rf ~\/.keras # clean up the model \/ make space for other things","b4ef4057":"from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nweight_path=\"{}_weights.best.hdf5\".format('seedlings')\n\ncheckpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n                             save_best_only=True, mode='min', save_weights_only = True)\n\n\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=10, verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=0.0001)\nearly = EarlyStopping(monitor=\"val_loss\", \n                      mode=\"min\", \n                      patience=5) # probably needs to be more patient, but kaggle time is limited\ncallbacks_list = [checkpoint, early, reduceLROnPlat]","a1780136":"tb_model.fit_generator(train_gen, \n                      steps_per_epoch = 40,\n                      validation_data = (test_X, test_Y), \n                      epochs = 8, \n                      callbacks = callbacks_list)","fdbbbabe":"# load the best version of the model\ntb_model.load_weights(weight_path)","4793718d":"# get the attention layer since it is the only one with a single output dim\nfor attn_layer in tb_model.layers:\n    c_shape = attn_layer.get_output_shape_at(0)\n    if len(c_shape)==4:\n        if c_shape[-1]==1:\n            print(attn_layer)\n            break","f46f98b1":"import keras.backend as K\nrand_idx = np.random.choice(range(len(test_X)), size = 6)\nattn_func = K.function(inputs = [tb_model.get_input_at(0), K.learning_phase()],\n           outputs = [attn_layer.get_output_at(0)]\n          )\nfig, m_axs = plt.subplots(len(rand_idx), 2, figsize = (8, 4*len(rand_idx)))\n[c_ax.axis('off') for c_ax in m_axs.flatten()]\nfor c_idx, (img_ax, attn_ax) in zip(rand_idx, m_axs):\n    cur_img = test_X[c_idx:(c_idx+1)]\n    attn_img = attn_func([cur_img, 0])[0]\n    img_ax.imshow(cur_img[0,:,:].clip(0, 255).astype(np.uint8))\n    attn_ax.imshow(attn_img[0, :, :, 0], cmap = 'viridis', \n                   vmin = 0, vmax = 1, \n                   interpolation = 'lanczos')\n    real_label = test_Y[c_idx]\n    img_ax.set_title('TB\\nClass:%s' % (cat_enc.classes_[np.argmax(real_label)]))\n    pred_confidence = tb_model.predict(cur_img)[0]\n    attn_ax.set_title('Attention Map\\nPred:%2.1f%%' % (100*pred_confidence[np.argmax(real_label)]))\nfig.savefig('attention_map.png', dpi = 300)","53ef38c4":"pred_Y = tb_model.predict(test_X, batch_size = 32, verbose = True)\npred_Y_cat = np.argmax(pred_Y, -1)","6da9e5a2":"from sklearn.metrics import classification_report, confusion_matrix\nplt.matshow(confusion_matrix(np.argmax(test_Y,-1), pred_Y_cat))\nprint(classification_report(np.argmax(test_Y,-1), pred_Y_cat.astype(int), \n                            target_names = cat_enc.classes_))","41eb8949":"dense_layers = [x for x in tb_model.layers if isinstance(x, Dense)]\nlast_layer = bn_features\nfor i, c_d in enumerate(dense_layers):\n    W, b = c_d.get_weights()\n    in_dim, out_dim = W.shape\n    new_W = np.expand_dims(np.expand_dims(W,0),0)\n    new_b = b\n    last_layer = Conv2D(out_dim, \n                        kernel_size = (1,1), \n                        weights = (new_W, new_b),\n                        activation = c_d.activation, \n                        name = 'd2cv_{}'.format(i))(last_layer)\nviz_model = Model(inputs = [in_lay], \n                  outputs = [last_layer],\n                  name = 'viz_model')\nviz_model.summary()","615e8870":"rand_idx = np.random.choice(range(len(test_X)), size = 4)\nch_count = viz_model.get_output_shape_at(0)[-1]\nfig, m_axs = plt.subplots(len(rand_idx), \n                          1+ch_count, \n                          figsize = (4*(1+ch_count), \n                                     4*len(rand_idx)))\n[c_ax.axis('off') for c_ax in m_axs.flatten()]\nfor c_idx, n_axs in zip(rand_idx, m_axs):\n    cur_img = test_X[c_idx:(c_idx+1)]\n    attn_img = viz_model.predict([cur_img])\n    pred_confidence = tb_model.predict(cur_img)[0]\n    n_axs[0].imshow(cur_img[0,:,:].clip(0, 255).astype(np.uint8))\n    real_label = np.argmax(test_Y[c_idx])\n    n_axs[0].set_title('Class:%s' % (cat_enc.classes_[real_label]))\n    for i, c_ax in enumerate(n_axs[1:]):\n        c_ax.imshow(attn_img[0, :, :, i], cmap = 'viridis', \n                       vmin = 0, vmax = 1, \n                       interpolation = 'lanczos')\n        c_ax.set_title('%s Map\\nPred:%2.1f%%' % (cat_enc.classes_[i], \n                                                 100*pred_confidence[i]))\n    \nfig.savefig('positive_map.png', dpi = 300)","6f07391b":"test_paths = glob(os.path.join(base_dir, 'test', '*.*'))\nprint('Image Files', len(test_paths))\ntest_paths_df = pd.DataFrame(dict(path = test_paths))\ntest_paths_df['file'] = test_paths_df['path'].map(os.path.basename)\ntest_paths_df.sample(3)","49d116a2":"# used a fixed dataset for evaluating the algorithm\ntest_X, _ = next(flow_from_dataframe(core_idg, \n                               test_paths_df, \n                             path_col = 'path',\n                            y_col = 'path', \n                            target_size = IMG_SIZE,\n                             color_mode = 'rgb',\n                             shuffle = False,\n                            batch_size = len(test_paths))) # one big batch","8440f12a":"pred_Y = tb_model.predict(test_X, batch_size = 8, verbose = True)\npred_Y_cat = np.argmax(pred_Y, -1)","565d9e6b":"test_paths_df['species'] = [cat_enc.classes_[x] for x in pred_Y_cat]\ntest_paths_df[['file', 'species']].to_csv('submission.csv', index=False)\ntest_paths_df[['file', 'species']].head(5)","8ba5951b":"# Split Data into Training and Validation","326c03f9":"dosyay\u0131 okundu i\u00e7inde g\u00f6r\u00fcnt\u00fclerin konum g\u00fczergah\u0131,kategorisi ,kategori id gibi bilgiler bar\u0131nd\u0131r\u0131yor\n","ef33514e":"## Preprocessing\nTurn the HDF5 into a data-frame and a folder full of TIFF files","cc60704c":"grafikte hangi kategoriye ka\u00e7 adet veriye sahip g\u00f6r\u00fclebilir","7012de2e":"# Attention Model\nThe basic idea is that a Global Average Pooling is too simplistic since some of the regions are more relevant than others. So we build an attention mechanism to turn pixels in the GAP on an off before the pooling and then rescale (Lambda layer) the results based on the number of pixels. The model could be seen as a sort of 'global weighted average' pooling. There is probably something published about it and it is very similar to the kind of attention models used in NLP.\nIt is largely based on the insight that the winning solution annotated and trained a UNET model to segmenting the hand and transforming it. This seems very tedious if we could just learn attention.","6e955d2b":"* burada data setini train ve test olark ikiye b\u00f6ld\u00fck\n","a0e63c23":"Temel fikir, K\u00fcresel Ortalama Havuzlaman\u0131n \u00e7ok basit olmas\u0131d\u0131r, \u00e7\u00fcnk\u00fc baz\u0131 b\u00f6lgeler di\u011ferlerinden daha alakal\u0131. Bu nedenle, GAP'deki pikselleri havuzlamadan \u00f6nce kapatmak ve ard\u0131ndan piksel say\u0131s\u0131na g\u00f6re sonu\u00e7lar\u0131 yeniden \u00f6l\u00e7eklendirmek (Lambda katman\u0131) i\u00e7in bir dikkat mekanizmas\u0131 olu\u015fturuyoruz. Model, bir t\u00fcr 'k\u00fcresel a\u011f\u0131rl\u0131kl\u0131 ortalama' havuzlamas\u0131 olarak g\u00f6r\u00fclebilir. Muhtemelen bunun hakk\u0131nda yay\u0131nlanan bir \u015fey vard\u0131r ve NLP'de kullan\u0131lan dikkat modellerine \u00e7ok benzer. B\u00fcy\u00fck \u00f6l\u00e7\u00fcde, kazanan \u00e7\u00f6z\u00fcm\u00fcn eli b\u00f6l\u00fcmlere ay\u0131rmak ve d\u00f6n\u00fc\u015ft\u00fcrmek i\u00e7in bir UNET modeline a\u00e7\u0131klama getirdi\u011fi ve onu e\u011fitti\u011fi anlay\u0131\u015f\u0131na dayanmaktad\u0131r. Dikkatini \u00f6\u011frenebilirsek, bu \u00e7ok s\u0131k\u0131c\u0131 g\u00f6r\u00fcn\u00fcyor.","11ef3c14":"# Examine the distributions\nShow how the data is distributed and why we need to balance it","2bcc7a77":"# Show Attention\nDid our attention model learn anything useful?","9b372696":"# Visualize Positive Regions","b44b31c0":"# Balance the distribution in the training set (E\u011fitim setindeki da\u011f\u0131l\u0131m\u0131 dengeleyin)","4420a201":"# Overview\nThis is just a simple first attempt at a model using VGG16 as a basis and attempting to do classification directly on the seedling images\n\nThis can be massively improved with \n* high-resolution images\n* better data sampling\n* ensuring there is no leaking between training and validation sets, ```sample(replace = True)``` is real dangerous\n* pretrained models\n* attention\/related techniques to focus on areas","1473ef8f":"# Predict on Test Data","49837eb6":"# Evaluate the results\nHere we evaluate the results by loading the best version of the model and seeing how the predictions look on the results. We then visualize spec","17a36aeb":"### Copy\ncopy the weights and configurations for the pre-trained models"}}