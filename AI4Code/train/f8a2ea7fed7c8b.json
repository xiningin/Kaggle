{"cell_type":{"30cdd027":"code","9bdb7cd9":"code","f8cf46f4":"code","3bcabed4":"code","9991fc5e":"code","12e8615c":"code","6fb0e601":"code","c5449fe1":"code","bb669824":"markdown","3b9c3e52":"markdown"},"source":{"30cdd027":"import datetime\nimport gc\nimport numpy as np\nimport os\nimport pandas as pd\nimport random\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.stats import skew, kurtosis\nimport lightgbm as lgb\n\nimport Levenshtein\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import KFold\n\nfrom tqdm import tqdm","9bdb7cd9":"id_col = 'Id'\ntarget_col = 'target'\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n","f8cf46f4":"def extract_features(df):\n    df['nunique'] = df['ciphertext'].apply(lambda x: len(np.unique(x)))\n    df['len'] = df['ciphertext'].apply(lambda x: len(x))\n\n    def count_chars(x):\n        n_l = 0 # count letters\n        n_n = 0 # count numbers\n        n_s = 0 # count symbols\n        n_ul = 0 # count upper letters\n        n_ll = 0 # count lower letters\n        for i in range(0, len(x)):\n            if x[i].isalpha():\n                n_l += 1\n                if x[i].isupper():\n                    n_ul += 1\n                elif x[i].islower():\n                    n_ll += 1\n            elif x[i].isdigit():\n                n_n += 1\n            else:\n                n_s += 1\n\n        return pd.Series([n_l, n_n, n_s, n_ul, n_ll])\n\n    cols = ['n_l', 'n_n', 'n_s', 'n_ul', 'n_ll']\n    for c in cols:\n        df[c] = 0\n    tqdm.pandas(desc='count_chars')\n    df[cols] = df['ciphertext'].progress_apply(lambda x: count_chars(x))\n    for c in cols:\n        df[c] \/= df['len']\n\n    tqdm.pandas(desc='distances')\n    df['Levenshtein_distance'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.distance(x, x[::-1]))\n    df['Levenshtein_ratio'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.ratio(x, x[::-1]))\n    df['Levenshtein_jaro'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.jaro(x, x[::-1]))\n    df['Levenshtein_hamming'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.hamming(x, x[::-1]))\n\n    for m in range(1, 5):\n        df['Levenshtein_distance_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.distance(x[:-m], x[m:]))\n        df['Levenshtein_ratio_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.ratio(x[:-m], x[m:]))\n        df['Levenshtein_jaro_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.jaro(x[:-m], x[m:]))\n        df['Levenshtein_hamming_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.hamming(x[:-m], x[m:]))\n    \n    df['Levenshtein_distance_h'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.distance(x[:len(x)\/\/2], x[len(x)\/\/2:]))\n    df['Levenshtein_ratio_h'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.ratio(x[:len(x)\/\/2], x[len(x)\/\/2:]))\n    df['Levenshtein_jaro_h'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.jaro(x[:len(x)\/\/2], x[len(x)\/\/2:]))\n    \n    # All symbols stats\n    def strstat(x):\n        r = np.array([ord(c) for c in x])\n        return pd.Series([\n            np.sum(r), \n            np.mean(r), \n            np.std(r), \n            np.min(r), \n            np.max(r),\n            skew(r), \n            kurtosis(r),\n            ])\n    cols = ['str_sum', 'str_mean', 'str_std', 'str_min', 'str_max', 'str_skew', 'str_kurtosis']\n    for c in cols:\n        df[c] = 0\n    tqdm.pandas(desc='strstat')\n    df[cols] = df['ciphertext'].progress_apply(lambda x: strstat(x))\n    \n    # Digit stats\n    def str_digit_stat(x):\n        r = np.array([ord(c) for c in x if c.isdigit()])\n        if len(r) == 0:\n            r = np.array([0])\n        return pd.Series([\n            np.sum(r), \n            np.mean(r), \n            np.std(r), \n            np.min(r), \n            np.max(r),\n            skew(r), \n            kurtosis(r),\n            ])\n    cols = ['str_digit_sum', 'str_digit_mean', 'str_digit_std', 'str_digit_min', \n        'str_digit_max', 'str_digit_skew', 'str_digit_kurtosis']\n    for c in cols:\n        df[c] = 0\n    tqdm.pandas(desc='str_digit_stat')\n    df[cols] = df['ciphertext'].progress_apply(lambda x: str_digit_stat(x))","3bcabed4":"print('Extracting features for train:')\nextract_features(train)\nprint('Extracting features for test:')\nextract_features(test)","9991fc5e":"train.head()","12e8615c":"N_DIFF = 4\nsubm = None","6fb0e601":"for difficulty in range(1, N_DIFF+1):\n    cur_train = train[train['difficulty'] == difficulty]\n    cur_test = test[test['difficulty'] == difficulty]\n    # TFIDF\n    for k in range(0, 3):\n        tfidf = TfidfVectorizer(\n            max_features=1000,\n            lowercase=False,\n            token_pattern='\\\\S+',\n        )\n\n        def char_pairs(x, k=1):\n            buf = []\n            for i in range(k, len(x)):\n                buf.append(x[i-k:i+1])\n            return ' '.join(buf)\n\n        cur_train['text_temp'] = cur_train.ciphertext.apply(lambda x: char_pairs(x, k))\n        cur_test['text_temp'] = cur_test.ciphertext.apply(lambda x: char_pairs(x, k))\n        train_tfids = tfidf.fit_transform(cur_train['text_temp'].values).todense()\n        test_tfids = tfidf.transform(cur_test['text_temp'].values).todense()\n\n        print('k = {}: train_tfids.shape = {}'.format(k, train_tfids.shape))\n\n        for i in range(train_tfids.shape[1]):\n            cur_train['text_{}_tfidf{}'.format(k, i)] = train_tfids[:, i]\n            cur_test['text_{}_tfidf{}'.format(k, i)] = test_tfids[:, i]\n\n        del train_tfids, test_tfids, tfidf\n        gc.collect()\n    \n    print(\"Training on difficulty = {}\".format(difficulty))\n    # Build the model\n    cnt = 0\n    p_buf = []\n    p_valid_buf = []\n    n_splits = 5\n    kf = KFold(\n        n_splits=n_splits, \n        random_state=0)\n    err_buf = []   \n    undersampling = 0\n\n    lgb_params = {\n        'boosting_type': 'gbdt',\n        'objective': 'multiclass',\n        'metric': 'multi_logloss',\n        'max_depth': 5,\n        'num_leaves': 31,\n        'learning_rate': 0.05,\n        'feature_fraction': 0.85,\n        'bagging_fraction': 0.85,\n        'bagging_freq': 5,\n        'verbose': -1,\n        'num_threads': -1,\n        'lambda_l1': 1.0,\n        'lambda_l2': 1.0,\n        'min_gain_to_split': 0,\n        'num_class': train[target_col].nunique(),\n    }\n\n    cols_to_drop = [\n        'difficulty',\n        id_col, \n        'ciphertext',\n        target_col,\n        'text_temp',\n    ]\n\n    X = cur_train.drop(cols_to_drop, axis=1, errors='ignore')\n    feature_names = list(X.columns)\n\n    X = X.values\n    y = cur_train[target_col].values\n\n    X_test = cur_test.drop(cols_to_drop, axis=1, errors='ignore')\n    id_test = cur_test[id_col].values\n\n    print(X.shape, y.shape)\n    print(X_test.shape)\n\n    n_features = X.shape[1]\n\n    for train_index, valid_index in kf.split(X, y):\n        print('Fold {}\/{}'.format(cnt + 1, n_splits))\n        params = lgb_params.copy() \n\n        lgb_train = lgb.Dataset(\n            X[train_index], \n            y[train_index], \n            feature_name=feature_names,\n            )\n        lgb_train.raw_data = None\n\n        lgb_valid = lgb.Dataset(\n            X[valid_index], \n            y[valid_index],\n            )\n        lgb_valid.raw_data = None\n\n        model = lgb.train(\n            params,\n            lgb_train,\n            num_boost_round=10000,\n            valid_sets=[lgb_train, lgb_valid],\n            early_stopping_rounds=100,\n            verbose_eval=100,\n        )\n\n        if cnt == 0:\n            importance = model.feature_importance()\n            model_fnames = model.feature_name()\n            tuples = sorted(zip(model_fnames, importance), key=lambda x: x[1])[::-1]\n            tuples = [x for x in tuples if x[1] > 0]\n            print('Important features:')\n            for i in range(20):\n                if i < len(tuples):\n                    print(tuples[i])\n                else:\n                    break\n\n            del importance, model_fnames, tuples\n\n        p = model.predict(X[valid_index], num_iteration=model.best_iteration)\n        err = f1_score(y[valid_index], np.argmax(p, axis=1), average='macro')\n\n        print('{} F1: {}'.format(cnt + 1, err))\n\n        p = model.predict(X_test, num_iteration=model.best_iteration)\n        if len(p_buf) == 0:\n            p_buf = np.array(p, dtype=np.float16)\n        else:\n            p_buf += np.array(p, dtype=np.float16)\n        err_buf.append(err)\n\n        cnt += 1\n\n        del model, lgb_train, lgb_valid, p\n        gc.collect\n\n        # Train on one fold\n        if cnt > 0:\n            break\n\n\n    err_mean = np.mean(err_buf)\n    err_std = np.std(err_buf)\n    print('F1 = {:.6f} +\/- {:.6f}'.format(err_mean, err_std))\n\n    preds = p_buf\/cnt\n    \n    cur_subm = pd.DataFrame()\n    cur_subm[id_col] = id_test\n    cur_subm['Predicted'] = np.argmax(preds, axis=1)\n    if difficulty == 1:\n        subm = cur_subm\n    else:\n        subm = pd.concat([subm,cur_subm])","c5449fe1":"subm = subm.set_index(\"Id\")\nsample = pd.read_csv(\"..\/input\/sample_submission.csv\").set_index(\"Id\")\nfor idx, row in sample.iterrows():\n    row[\"Predicted\"] = subm.loc[idx][\"Predicted\"]\n\nsample.to_csv(\"submission.csv\")","bb669824":"Orders fix","3b9c3e52":"This a fork of @opanichev 's great kernel: https:\/\/www.kaggle.com\/opanichev\/lightgbm-and-simple-features.\n\nInstead of using difficulty as a feature, we'll train a model for each of them."}}