{"cell_type":{"dc8ae4af":"code","e762aaf6":"code","716f0207":"code","31cefd44":"code","6febad79":"code","47733a00":"code","9dbc146e":"code","1fe0468b":"code","7cab2257":"code","7d1020ba":"code","a1eba5ba":"code","8ba64e26":"code","5a713fba":"code","117f87a0":"code","aa771c24":"code","054f726b":"code","32ee8707":"code","67994b2a":"code","224c9048":"code","b125c023":"markdown","28f94d2d":"markdown","7c0b95d4":"markdown","f23b16b9":"markdown","55191173":"markdown","8d93309f":"markdown","7be91486":"markdown","04065555":"markdown","dd35d1b8":"markdown","3cc47ad3":"markdown","a2e53023":"markdown","3dcb9141":"markdown","cd7e4dae":"markdown"},"source":{"dc8ae4af":"!pip install ..\/input\/python-datatable\/datatable-0.11.0-cp37-cp37m-manylinux2010_x86_64.whl > \/dev\/null 2>&1","e762aaf6":"import numpy as np\nimport pandas as pd\nfrom collections import defaultdict\nimport datatable as dt\nimport lightgbm as lgb\nfrom matplotlib import pyplot as plt\nimport riiideducation\nimport torch\n\n# Error handling, ignore all\nnp.seterr(divide = 'ignore', invalid = 'ignore')","716f0207":"data_types_dict = {\n    'user_id': 'int32', \n    'content_id': 'int16', \n    'answered_correctly': 'int8', \n    'prior_question_elapsed_time': 'float32', \n    'prior_question_had_explanation': 'bool'\n}\n\ntarget = 'answered_correctly'","31cefd44":"train_df = dt.fread('..\/input\/riiid-test-answer-prediction\/train.csv', columns = set(data_types_dict.keys())).to_pandas()","6febad79":"# Exclude lectures\ntrain_df = train_df[train_df[target] != -1].reset_index(drop = True, inplace = False)\n# Fill NaN values in the 'prior_question_had_explanation' columns\ntrain_df['prior_question_had_explanation'].fillna(False, inplace = True)\n# Set type\ntrain_df = train_df.astype(data_types_dict)","47733a00":"# Answer for the previous questions of users\ntrain_df['lag'] = train_df.groupby('user_id')[target].shift()\n# For each user (groupby('user_id')), compute the cummulative number of correct answers and number answers in general\ncum = train_df.groupby('user_id')['lag'].agg(['cumsum', 'cumcount'])\n# User correctness (measure the users' learning progress)\ntrain_df['user_correctness'] = cum['cumsum'] \/ cum['cumcount']\n# Drop the 'lag' feature\ntrain_df.drop(columns = ['lag'], inplace = True)","9dbc146e":"# Overall correctness of users\nuser_agg = train_df.groupby('user_id')[target].agg(['sum', 'count'])\n                                                                               \n# Overall difficulty of questions\ncontent_agg = train_df.groupby('content_id')[target].agg(['sum', 'count'])","1fe0468b":"# Take only 24 last observations of each user\ntrain_df = train_df.groupby('user_id').tail(24).reset_index(drop = True)","7cab2257":"train_df","7d1020ba":"questions_df = pd.read_csv(\n    '..\/input\/riiid-test-answer-prediction\/questions.csv', \n    usecols = [0, 3],\n    dtype = {'question_id': 'int16', 'part': 'int8'}\n)\ntrain_df = pd.merge(train_df, questions_df, left_on = 'content_id', right_on = 'question_id', how = 'left')\ntrain_df.drop(columns = ['question_id'], inplace = True)","a1eba5ba":"# How many questions have been answered in each content ID?\ntrain_df['content_count'] = train_df['content_id'].map(content_agg['count']).astype('int32')\n# How hard are questions in each content ID?\ntrain_df['content_id'] = train_df['content_id'].map(content_agg['sum'] \/ content_agg['count'])","8ba64e26":"# Ratio is 6\/24 = 25%\nvalid_df = train_df.groupby('user_id').tail(6)\ntrain_df.drop(valid_df.index, inplace = True)","5a713fba":"features = ['content_id', 'prior_question_elapsed_time', \n            'prior_question_had_explanation', 'user_correctness', \n            'part', 'content_count']\n\nparams = {\n    'loss_function': 'Logloss',\n    'eval_metric': 'AUC',\n    'task_type': 'GPU' if torch.cuda.is_available() else 'CPU',\n    'grow_policy': 'Lossguide',\n    'iterations': 1000,#2500\n    'learning_rate': 4e-3,\n    'random_seed': 0,\n    'l2_leaf_reg': 1e-1,\n    'depth': 15,\n    'max_leaves': 13,\n    'border_count': 128,\n    'verbose': 50,\n}\n\nparams1 = {\n    'objective': 'binary',\n    'seed': 42,\n    'metric': 'auc',\n    'learning_rate': 0.05,\n    'max_bin': 800,\n    'num_leaves': 85\n}","117f87a0":"tr_data = lgb.Dataset(train_df[features], label=train_df[target])\nva_data = lgb.Dataset(valid_df[features], label=valid_df[target])\n\nmodel1 = lgb.train(\n    params1, \n    tr_data, \n    num_boost_round=10000,\n    valid_sets=[tr_data, va_data], \n    early_stopping_rounds=50,\n    verbose_eval=50\n)\n\n# model.save_model(f'model.txt')\nlgb.plot_importance(model1, importance_type='gain')\nplt.show()","aa771c24":"from catboost import CatBoostClassifier, Pool\n\n# Training and validating data\ntrain_set = Pool(train_df[features], label = train_df[target])\nval_set = Pool(valid_df[features], label = valid_df[target])","054f726b":"# Model definition\nmodel = CatBoostClassifier(**params)\n\n# Fitting\nmodel.fit(train_set, eval_set = val_set, use_best_model = True)","32ee8707":"user_sum_dict = user_agg['sum'].astype('int16').to_dict(defaultdict(int))\nuser_count_dict = user_agg['count'].astype('int16').to_dict(defaultdict(int))\ncontent_sum_dict = content_agg['sum'].astype('int32').to_dict(defaultdict(int))\ncontent_count_dict = content_agg['count'].astype('int32').to_dict(defaultdict(int))","67994b2a":"try:\n    env = riiideducation.make_env()\nexcept:\n    pass\niter_test = env.iter_test()\nprior_test_df = None","224c9048":"%%time\n\nfor (test_df, sample_prediction_df) in iter_test:\n    if prior_test_df is not None:\n        prior_test_df[target] = eval(test_df['prior_group_answers_correct'].iloc[0])\n        prior_test_df = prior_test_df[prior_test_df[target] != -1].reset_index(drop = True)\n        \n        user_ids = prior_test_df['user_id'].values\n        content_ids = prior_test_df['content_id'].values\n        targets = prior_test_df[target].values\n        \n        for user_id, content_id, answered_correctly in zip(user_ids, content_ids, targets):\n            user_sum_dict[user_id] += answered_correctly\n            user_count_dict[user_id] += 1\n            content_sum_dict[content_id] += answered_correctly\n            content_count_dict[content_id] += 1\n\n    prior_test_df = test_df.copy()\n    \n    test_df = test_df[test_df['content_type_id'] == 0].reset_index(drop = True)\n    test_df = pd.merge(test_df, questions_df, left_on = 'content_id', right_on = 'question_id', how = 'left')\n    test_df['prior_question_had_explanation'] = test_df['prior_question_had_explanation'].fillna(False).astype('bool')    \n    \n    user_sum = np.zeros(len(test_df), dtype = np.int16)\n    user_count = np.zeros(len(test_df), dtype = np.int16)\n    content_sum = np.zeros(len(test_df), dtype = np.int32)\n    content_count = np.zeros(len(test_df), dtype = np.int32)\n    \n    for i, (user_id, content_id) in enumerate(zip(test_df['user_id'].values, test_df['content_id'].values)):\n        user_sum[i] = user_sum_dict[user_id]\n        user_count[i] = user_count_dict[user_id]\n        content_sum[i] = content_sum_dict[content_id]\n        content_count[i] = content_count_dict[content_id]\n\n    test_df['user_correctness'] = user_sum \/ user_count\n    test_df['content_count'] = content_count\n    test_df['content_id'] = content_sum \/ content_count\n       \n    test_df[target] = np.average([\n        model.predict_proba(test_df[features].values)[:,1],\n        model1.predict(test_df[features])\n    ], weights=[0.49934,0.50066], axis=0)\n    env.predict(test_df[['row_id', target]])","b125c023":"# Use the package 'datatable' for fast handling","28f94d2d":"# Extract the validation set","7c0b95d4":"The following notebooks help in creating ensemble of LightGBM and Catboost however I didn't seem to find massive improvement , PUBLIC LB:0.758\nReferences \nhttps:\/\/www.kaggle.com\/shinomoriaoshi\/riiid-catboost-baseline\nhttps:\/\/www.kaggle.com\/shoheiazuma\/riiid-lgbm-starter","f23b16b9":"# Inference","55191173":"* Question dataset comes into play","8d93309f":"* Construct data","7be91486":"# Necessary packages","04065555":"# Preprocessing","dd35d1b8":"* Data config","3cc47ad3":"* Information of the training dataset","a2e53023":"# Training","3dcb9141":"* Construct new features","cd7e4dae":"* Import data"}}