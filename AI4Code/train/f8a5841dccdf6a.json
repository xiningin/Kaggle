{"cell_type":{"1f5d6db3":"code","a91fff74":"code","cfd04843":"code","a2a59b8f":"code","ac3566ce":"code","078cc496":"code","b63d4cdf":"code","e124c1fe":"code","040b0828":"code","91408b0f":"code","28edad6a":"code","ef52a7f7":"code","8632f268":"code","a19782fb":"code","5ef5e438":"code","fdbe0c52":"code","ae7bb309":"code","34a8003d":"code","b4547c7d":"code","815b3c0a":"code","f374b6a7":"code","8ca19fed":"code","01c60d34":"code","e85e4cf1":"code","b823b883":"code","42d69b46":"markdown","2d9a4707":"markdown","f5c9d728":"markdown","bf41d303":"markdown","5fccce06":"markdown","fbe048f4":"markdown","011080b1":"markdown","f34a8fa9":"markdown","655f2006":"markdown","d0cdad83":"markdown"},"source":{"1f5d6db3":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nfrom bq_helper import BigQueryHelper # Safely navigate the giant dataset\nimport matplotlib.pyplot as plt # plotting library\nimport seaborn as sns # plotting parameters\n\nimport warnings\nwarnings.filterwarnings('ignore') # I was having some annoying scipy deprecation warnings so I'm going to ignore them\n\nsns.set_style(\"whitegrid\")\nsns.set_context(\"talk\")\nsns.set(rc={'figure.figsize':(11.7,8.27)})\n\nbq_assistant = BigQueryHelper(\"bigquery-public-data\", \"noaa_icoads\") \n# Get the data and loaf into BigQuery Helper object","a91fff74":"%%time\nbq_assistant.list_tables()","cfd04843":"%%time\nbq_assistant.table_schema(\"icoads_core_2017\")","a2a59b8f":"%%time\nbq_assistant.head(\"icoads_core_2017\", num_rows=10)","ac3566ce":"QUERY = \"\"\"\n        SELECT latitude, longitude, wind_direction_true, amt_pressure_tend,  air_temperature, sea_level_pressure, wave_height, timestamp\n        FROM `bigquery-public-data.noaa_icoads.icoads_core_2017`\n        WHERE longitude > -130 AND longitude <= -110 AND latitude > 45 AND latitude <= 60 AND wind_direction_true <= 360\n        \"\"\"","078cc496":"%%time\nbq_assistant.estimate_query_size(QUERY)","b63d4cdf":"df_bq = bq_assistant.query_to_pandas(QUERY)\nprint(df_bq.shape)\ndf = df_bq.dropna() # remove NaN values\nprint(df.shape)","e124c1fe":"def monte_carlo(df):\n    \"\"\"\n    Takes a pandas dataframe and runs a monte-carlo simulation on the target \n    labels.\n    ====================================================================\n    Inputs: Pandas df\n    \n    Outputs: Monte-carlo'd pandas df\n    \"\"\"\n    \n    nb_increase=10 # Number of montecarlo sample \n    size = df.shape[0]\n    df_old = df.copy()\n    for feature_name in df.columns:\n        for nb in range(1,nb_increase):\n            if feature_name!='timestamp':\n                df_n = df_old.copy()\n                df_n[feature_name] = df_old[feature_name].values + np.random.normal(0.0, 1.0, size)*df_old[feature_name].std()\n                df = pd.concat([df, df_n], ignore_index=True)\n    return df.dropna()\n#df=monte_carlo(df).copy()","040b0828":"print(df.shape)","91408b0f":"# Compute the correlation matrix\ncorr = df.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots()\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","28edad6a":"sns.scatterplot(df['air_temperature'].values, df['sea_level_pressure'].values)","ef52a7f7":"sns.scatterplot(df['longitude'].values, df['latitude'].values)","8632f268":"sns.distplot(df['amt_pressure_tend'])","a19782fb":"sns.distplot(df['sea_level_pressure'])","5ef5e438":"sns.distplot(df['wave_height'])","fdbe0c52":"sns.distplot(df.latitude)","ae7bb309":"sns.distplot(df.longitude)","34a8003d":"def normalize(df):\n    result = df.copy()\n    for feature_name in df.columns:\n        max_value = df[feature_name].max()\n        min_value = df[feature_name].min()\n        result[feature_name] = (df[feature_name] - min_value) \/ (max_value - min_value)\n    return result\n\nnormalized_df=normalize(df)","b4547c7d":"X = normalized_df[['wind_direction_true', 'amt_pressure_tend', 'air_temperature', 'sea_level_pressure', 'timestamp']].copy()\ny = normalized_df[['wave_height']].copy()","815b3c0a":"from keras.models import Sequential\nfrom keras.layers import Dense, BatchNormalization, Dropout\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom keras.optimizers import SGD","f374b6a7":"# define base model\ndef baseline_model():\n    # create model\n    droprate = 0.1\n    model = Sequential()\n    \n    model.add(Dense(32, input_dim=5, kernel_initializer='normal', activation='relu')) #input_dim=13,\n    model.add(BatchNormalization())\n    model.add(Dropout(droprate))#3    #model.add(Dense(5096, kernel_initializer='normal', activation='relu'))\n\n    model.add(Dense(256, kernel_initializer='normal', activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(droprate))#3\n    \n    model.add(Dense(64, kernel_initializer='normal', activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(droprate))#3\n    \n    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n    # Compile model\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model","8ca19fed":"# fix random seed for reproducibility\nseed = 7\nnp.random.seed(seed)\n# evaluate model with standardized dataset\nestimator = KerasRegressor(build_fn=baseline_model, epochs=30, batch_size=5000, verbose=0)","01c60d34":"kfold = KFold(n_splits=3, random_state=seed)\nresults = cross_val_score(estimator, X, y, cv=kfold)\n\nprint(\"Results: %.5f (%.5f) MSE\" % (results.mean(), results.std()))","e85e4cf1":"test_loss, test_acc, train_loss, train_acc = [], [], [], []\n\nfor train, test in kfold.split(X.values, y.values):\n    history = estimator.fit(X.values[train], y.values[train], validation_data=(X.values[test], y.values[test]), epochs=30, batch_size=5000, verbose=1)\n\n    test_loss.append(history.history['val_loss'])    \n    train_loss.append(history.history['loss'])\n\ntest_loss = np.asarray(test_loss)\ntrain_loss = np.asarray(train_loss)\n\ntest_loss_max = np.amax(test_loss, axis=0)\ntest_loss_min = np.amin(test_loss, axis=0)\ntrain_loss_max = np.amax(train_loss, axis=0)\ntrain_loss_min = np.amin(train_loss, axis=0)","b823b883":"fig, ax = plt.subplots(figsize=(8, 6))\n\nax.fill_between(range(0,len(test_loss_max)), test_loss_min, test_loss_max, alpha=0.5, color='red')\nax.fill_between(range(0,len(train_loss_max)), train_loss_min, train_loss_max, alpha=0.5)\nax.set_yscale('log')\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nax.legend(['Train', 'Test'], loc='upper left')\nplt.show()","42d69b46":"Display first 10 rows for all columns in table","2d9a4707":"Now that the data is loaded, we can start to visualize what we're up against.","f5c9d728":"# **K-folk Cross Validation Logistic Regression on NOAA ICOADS**\n\nUsing NOAA ICOADS global marine meteorlogical dataset, I would like to develop a Neural Network to predict oceanic waveheights using correlated features.\n\nOne major weakness in this dataset is quantifying the uncertainties and error. Many features are given an indicator code which is assigned to each measurement indicating the precision - however this is done coarsely. \n\nThis initial run only utilizes the 2017 dataset due to size and training time constraints. \n\nPlease comment if you have suggestions for improvements!","bf41d303":"Augment data by applying a 10-realization monte-carlo simulation of the data","5fccce06":"Here I select the data based on the descriptions of the NOAA ICOADS table [here](https:\/\/www.kaggle.com\/noaa\/noaa-icoads:\/\/)\n\nQUERY is an SQL query. \n\nI am mainly interested in the Pacific Northwest off of British Columbia, Canada hence the lattitude and longitude selectors.**","fbe048f4":"Time the loading of 2017, view available columns","011080b1":"Check all tables available","f34a8fa9":"Start visualizing the parameter space","655f2006":"Load the query results into pandas as a dataframe and display its shape.","d0cdad83":"Time the estimate of the query. "}}