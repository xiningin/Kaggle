{"cell_type":{"cc92f043":"code","4f9476ae":"code","35191546":"code","145cd454":"code","2d18aaad":"code","df918f92":"code","49d67d0e":"code","e0b56803":"code","27c78965":"code","ce649124":"code","9aa19634":"code","dd29dfa5":"code","fd7419ef":"code","870d1fb5":"code","8f3292eb":"code","6507efaa":"code","814c1e1e":"code","9926724f":"code","4e2fd1e6":"code","61f5950b":"code","3cce73c5":"code","e9da20ab":"code","1af00ed5":"code","13b36b6a":"code","b8f645fc":"code","8a6c997e":"code","5381f444":"markdown","fa162caa":"markdown","e7599408":"markdown","9fda9d03":"markdown","02f2671a":"markdown","325e7369":"markdown","8f6f8e66":"markdown"},"source":{"cc92f043":"import numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\nfrom tqdm import tqdm\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import accuracy_score, log_loss\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader","4f9476ae":"INPUT_DIR = \"..\/input\/uec2021-exercise-2\/\"\nOUTPUT_DIR = \".\/\"","35191546":"df = pd.read_csv(INPUT_DIR + \"train_student_info.csv\")\ndf_test = pd.read_csv(INPUT_DIR + \"test_student_info.csv\")\ndf_all = pd.concat([df, df_test], sort=False, ignore_index=True)\n\ndf_registration = pd.read_csv(INPUT_DIR + \"student_registration.csv\")\ndf_all = pd.merge(df_all, df_registration, how=\"left\")\ndf_all","145cd454":"df_assessment = pd.read_csv(INPUT_DIR + \"student_assessment.csv\")\ndf_assessment_meta = pd.read_csv(INPUT_DIR + \"assessments.csv\")\ndf_assessment = pd.merge(df_assessment, df_assessment_meta, how=\"left\")\ndf_assessment","2d18aaad":"# is_banked (sum, mean) group by (module, presentation, student)\nkeys = [\"code_module\", \"code_presentation\", \"id_student\"]\ndf_agg = df_assessment.groupby(keys)[\"is_banked\"].agg([\"sum\", \"mean\"]).reset_index()\ndf_agg.columns = keys + [\"is_banked_sum\", \"is_banked_mean\"]\ndf_all = pd.merge(df_all, df_agg, how=\"left\")\n\n# is_banked (sum, mean) group by (student)\nkeys = [\"id_student\"]\ndf_agg = df_assessment.groupby(keys)[\"is_banked\"].agg([\"sum\", \"mean\"]).reset_index()\ndf_agg.columns = keys + [\"is_banked_sum_student\", \"is_banked_mean_student\"]\ndf_all = pd.merge(df_all, df_agg, how=\"left\")","df918f92":"# \u7de0\u5207\u4f55\u65e5\u524d\u306b\u63d0\u51fa\u3057\u305f\u304b\ndf_assessment[\"date_submitted_before\"] = df_assessment[\"date\"] - df_assessment[\"date_submitted\"]\n\n# \u5e73\u5747\u63d0\u51fa\u65e5\u304b\u3089\u306e\u5dee\ndf_assessment[\"date_submitted_mean\"] = df_assessment.groupby(\"id_assessment\")[\"date_submitted\"].transform(\"mean\")\ndf_assessment[\"date_submitted_diff\"] = df_assessment[\"date_submitted\"] - df_assessment[\"date_submitted_mean\"]","49d67d0e":"# group by (module, presentation, student)\nkeys = [\"code_module\", \"code_presentation\", \"id_student\"]\ndf_agg = df_assessment.groupby(keys, as_index=False)[[\"date_submitted_before\", \"date_submitted_diff\"]].mean()\ndf_agg.columns = keys + [\"date_submitted_before_mean\", \"date_submitted_diff_mean\"]\ndf = pd.merge(df, df_agg, how=\"left\")\n\n# by student (student)\nkeys = [\"id_student\"]\ndf_agg = df_assessment.groupby(keys, as_index=False)[[\"date_submitted_before\", \"date_submitted_diff\"]].mean()\ndf_agg.columns = keys + [\"date_submitted_before_mean_student\", \"date_submitted_diff_mean_student\"]\ndf = pd.merge(df, df_agg, how=\"left\")","e0b56803":"# num of assessment groupby (module, presentation, student)\nkeys = [\"code_module\", \"code_presentation\", \"id_student\"]\ndf_agg = df_assessment.groupby(keys, as_index=False).size()\ndf_agg.columns = keys + [\"n_assessment\"]\n\n# \u5e73\u5747assessment\u6570\u304b\u3089\u306e\u5dee\ndf_agg[\"n_assessment_diff\"] = df_agg[\"n_assessment\"] - df_agg.groupby([\"code_module\", \"code_presentation\"])[\"n_assessment\"].transform(\"mean\")\n\ndf_all = pd.merge(df_all, df_agg, how=\"left\")","27c78965":"df_vle = pd.read_csv(INPUT_DIR + \"student_vle.csv\")\ndf_vle_meta = pd.read_csv(INPUT_DIR + \"vle.csv\")\ndf_vle = pd.merge(df_vle, df_vle_meta, how=\"left\")\ndf_vle","ce649124":"keys = [\"code_module\", \"code_presentation\", \"id_student\"]\n\n# num of unique site, num of day, sum of click group by (module, presentation, student)\ndf_agg = df_vle.groupby(keys)[[\"id_site\", \"sum_click\"]].agg({\"id_site\": \"nunique\", \"sum_click\": [\"count\", \"sum\"]}).reset_index()\ndf_agg.columns = keys + [\"n_site\", \"n_day\", \"sum_click\"]\n\ndf_agg[\"click_per_day\"] = df_agg[\"sum_click\"] \/ df_agg[\"n_day\"]\ndf_agg[\"click_per_site\"] = df_agg[\"sum_click\"] \/ df_agg[\"n_site\"]\ndf_agg[\"day_per_site\"] = df_agg[\"n_day\"] \/ df_agg[\"n_site\"]\n\n# log transformation for NN\nfor col in [\"n_site\", \"n_day\", \"sum_click\", \"click_per_day\", \"click_per_site\", \"day_per_site\"]:\n    df_agg[col] = np.log1p(df_agg[col])\ndf_all = pd.merge(df_all, df_agg, how=\"left\")","9aa19634":"for t in tqdm(df_vle[\"activity_type\"].unique()):\n    keys = [\"code_module\", \"code_presentation\", \"id_student\"]\n    \n    # num of unique site, num of day, sum of click group by (module, presentation, student)\n    df_agg = df_vle.query(\"activity_type == @t\").groupby(keys)[[\"id_site\", \"sum_click\"]].agg({\"id_site\": \"nunique\", \"sum_click\": [\"count\", \"sum\"]}).reset_index()\n    df_agg.columns = keys + [\"n_site\", \"n_day\", \"sum_click\"]\n    \n    df_agg[\"click_per_day\"] = df_agg[\"sum_click\"] \/ df_agg[\"n_day\"]\n    df_agg[\"click_per_site\"] = df_agg[\"sum_click\"] \/ df_agg[\"n_site\"]\n    df_agg[\"day_per_site\"] = df_agg[\"n_day\"] \/ df_agg[\"n_site\"]\n    \n    # log transformation for NN\n    for col in [\"n_site\", \"n_day\", \"sum_click\", \"click_per_day\", \"click_per_site\", \"day_per_site\"]:\n        df_agg[col] = np.log1p(df_agg[col])\n    df_all = pd.merge(df_all, df_agg, how=\"left\")\n    df_all = pd.merge(df_all, df_agg, on=keys, how=\"left\", suffixes=[\"\", f\"_{t}\"])","dd29dfa5":"df_all[\"gender\"] = (df_all[\"gender\"] == \"M\").astype(int)\ndf_all[\"disability\"] = (df_all[\"disability\"] == \"Y\").astype(int)\n\ndf_all[\"imd_band\"] = df_all[\"imd_band\"].str[0].astype(float)\ndf_all[\"age_band\"] = df_all[\"age_band\"].str[0].astype(float)","fd7419ef":"cat_vars = [\"code_module\", \"code_presentation\", \"region\", \"highest_education\"]\n\nenc = OrdinalEncoder(dtype=int)\ndf_all[cat_vars] = enc.fit_transform(df_all[cat_vars].fillna(\"\"))","870d1fb5":"fig, axes = plt.subplots(figsize=(15, 80), nrows=35, ncols=4)\n\nfor i, col in enumerate(df_all.columns.drop([\"id\", \"id_student\", \"final_result\"] + cat_vars)):\n    ax = axes[i \/\/ 4, i % 4]\n    ax.hist(df_all[col], bins=100)\n    ax.set_title(col)\n\nfig.tight_layout()\nplt.show()","8f3292eb":"df = df_all.iloc[:len(df), :].reset_index(drop=True)\ndf_test = df_all.iloc[len(df):, :].reset_index(drop=True)","6507efaa":"result_map = {\"Withdrawn\": 0, \"Fail\": 1, \"Pass\": 2, \"Distinction\": 3}\nresult_inv_map = {0: \"Withdrawn\", 1: \"Fail\", 2: \"Pass\", 3: \"Distinction\"}","814c1e1e":"%%time\n\nX = df.drop([\"id\", \"id_student\", \"final_result\"], axis=1)\ny = df[\"final_result\"].map(result_map)\ngroups = df[\"id_student\"].values\n\nn_splits = 5\nnum_boost_round = 5000\nearly_stopping_rounds = 100\nparams = {\n    \"learning_rate\": 0.05,\n    \"num_leaves\": 16,\n    \"objective\": \"multiclass\",\n    \"feature_fraction_bynode\": 0.3,\n    \"verbosity\": -1,\n    \"num_classes\": 4\n}\n\npreds_valid = np.zeros((len(df), 4))\npreds_test = np.zeros((len(df_test), 4))\ngkf = GroupKFold(n_splits=n_splits)\nfor i, (idx_train, idx_valid) in enumerate(gkf.split(X, groups=groups)):\n    print(f\"fold: {i}\")\n    X_train = X.iloc[idx_train, :]\n    X_valid = X.iloc[idx_valid, :]\n    y_train = y.iloc[idx_train]\n    y_valid = y.iloc[idx_valid]\n    X_test = df_test[X.columns]\n    \n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_valid = lgb.Dataset(X_valid, y_valid)\n    \n    model = lgb.train(\n        params,\n        lgb_train,\n        valid_sets=[lgb_train, lgb_valid],\n        valid_names=[\"train\", \"valid\"],\n        num_boost_round=num_boost_round,\n        categorical_feature=cat_vars,\n        early_stopping_rounds=early_stopping_rounds,\n        verbose_eval=0\n    )\n    \n    preds_valid[idx_valid] = model.predict(X_valid)\n    preds_test += model.predict(X_test) \/ n_splits\n\nlogloss = log_loss(y, preds_valid)\nacc = accuracy_score(y, np.argmax(preds_valid, axis=1))\nprint(f\"logloss: {logloss:.5f}\\tacc: {acc:.5f}\")","9926724f":"submission = pd.read_csv(INPUT_DIR + \"sample_submission.csv\")\nsubmission[\"pred\"] = pd.Series(np.argmax(preds_test, axis=1)).map(result_inv_map)\nsubmission.to_csv(OUTPUT_DIR + \"lgb.csv\", index=False)","4e2fd1e6":"submission[\"pred\"].value_counts()","61f5950b":"class MyDataset(Dataset):\n    def __init__(self, X_num, X_cat, y=None):\n        self.X_num = torch.FloatTensor(X_num)\n        self.X_cat = torch.LongTensor(X_cat)\n        if y is not None:\n            self.y = torch.LongTensor(y)\n    \n    def __len__(self):\n        return self.X_num.shape[0]\n    \n    def __getitem__(self, idx):\n        if \"y\" in dir(self):\n            return (self.X_num[idx, :], self.X_cat[idx, :], self.y[idx])\n        else:\n            return (self.X_num[idx, :], self.X_cat[idx, :])","3cce73c5":"class NNModel(nn.Module):\n    def __init__(self, input_size_num, output_size, n_categories, emb_size, hidden_sizes, dropout):\n        super().__init__()\n        \n        self.embs = nn.ModuleList()\n        for i in range(len(n_categories)):\n            self.embs.append(nn.Embedding(n_categories[i], emb_size))\n        \n        input_size = input_size_num + sum(emb.embedding_dim for emb in self.embs)\n        self.mlp = nn.Sequential(\n            nn.Linear(input_size, hidden_sizes[0]),\n            nn.BatchNorm1d(hidden_sizes[0]),\n            nn.Dropout(dropout),\n            nn.ReLU(),\n            nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n            nn.BatchNorm1d(hidden_sizes[1]),\n            nn.Dropout(dropout),\n            nn.ReLU(),\n            nn.Linear(hidden_sizes[1], output_size),\n        )\n    \n    def forward(self, x_num, x_cat):\n        x_cat = [emb(x_cat[:, i]) for i, emb in enumerate(self.embs)]\n        x = torch.cat([x_num] + x_cat, axis=1)\n        x = self.mlp(x)\n        return x","e9da20ab":"def train(model, data_loader, optimizer, criterion, device):\n    model.train()\n    \n    for batch in data_loader:\n        X_num = batch[0].to(device)\n        X_cat = batch[1].to(device)\n        y = batch[2].to(device)\n        \n        preds = model(X_num, X_cat)\n        loss = criterion(preds, y)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n\ndef evaluate(model, data_loader, criterion, device):\n    model.eval()\n    \n    n = 0\n    total_loss = 0.0\n    for batch in data_loader:\n        X_num = batch[0].to(device)\n        X_cat = batch[1].to(device)\n        y = batch[2].to(device)\n        \n        with torch.no_grad():\n            preds = model(X_num, X_cat)\n        \n        loss = criterion(preds, y)\n        total_loss += loss.item()\n        n += X_num.shape[0]\n    \n    avg_loss = total_loss \/ n\n    \n    return avg_loss\n\n\ndef predict(model, data_loader, device):\n    model.eval()\n    \n    preds_all = []\n    for batch in data_loader:\n        X_num = batch[0].to(device)\n        X_cat = batch[1].to(device)\n        \n        with torch.no_grad():\n            preds = model(X_num, X_cat)\n        preds = torch.softmax(preds, axis=1)    \n        preds = preds.cpu().numpy()\n        preds_all.append(preds)\n    \n    preds_all = np.concatenate(preds_all)\n        \n    return preds_all","1af00ed5":"n_categories = (df_all[cat_vars].max(axis=0)+1).tolist()","13b36b6a":"%%time\n\nX = df.drop([\"id\", \"id_student\", \"final_result\"], axis=1)\ny = df[\"final_result\"].map(result_map)\ngroups = df[\"id_student\"].values\n\nn_splits = 5\nbatch_size = 256\n\npreds_valid = np.zeros((len(df), 4))\npreds_test = np.zeros((len(df_test), 4))\n\ngkf = GroupKFold(n_splits=n_splits)\nfor i, (idx_train, idx_valid) in enumerate(gkf.split(X, y, groups=groups)):   \n    print(f\"fold: {i}\")\n    X_train_num = X.iloc[idx_train, :].drop(cat_vars, axis=1).fillna(0)\n    X_train_cat = X.iloc[idx_train, :][cat_vars]\n    y_train = y.iloc[idx_train]\n    \n    X_valid_num = X.iloc[idx_valid, :].drop(cat_vars, axis=1).fillna(0)\n    X_valid_cat = X.iloc[idx_valid, :][cat_vars]\n    y_valid = y.iloc[idx_valid]\n    \n    X_test_num = df_test[X.columns].drop(cat_vars, axis=1).fillna(0)\n    X_test_cat = df_test[cat_vars]\n    \n    # standardization\n    scaler = StandardScaler()\n    scaler.fit(X_train_num)\n    X_train_num = scaler.transform(X_train_num)\n    X_valid_num = scaler.transform(X_valid_num)\n    X_test_num = scaler.transform(X_test_num)\n    \n    # dataset\n    ds_train = MyDataset(X_train_num, X_train_cat.values, y_train.values)\n    ds_valid = MyDataset(X_valid_num, X_valid_cat.values, y_valid.values)\n    ds_test = MyDataset(X_test_num, X_test_cat.values)\n\n    # dataloader\n    dl_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True, \n                          num_workers=0, pin_memory=True, drop_last=True)\n    dl_valid = DataLoader(ds_valid, batch_size=batch_size, shuffle=False, \n                          num_workers=0, pin_memory=True, drop_last=False)\n    dl_test = DataLoader(ds_test, batch_size=batch_size, shuffle=False, \n                          num_workers=0, pin_memory=True, drop_last=False)\n    \n    # build model\n    torch.manual_seed(0)\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    model = NNModel(input_size_num=X_train_num.shape[1],\n                    output_size=4,\n                    n_categories=n_categories,\n                    emb_size=10,\n                    hidden_sizes=(128, 64),\n                    dropout=0.5)\n    model.to(device)\n    optimizer = optim.Adam(model.parameters(), lr=5e-4)\n    criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n    \n    best_loss = np.inf\n    for epoch in range(50):\n        train(model, dl_train, optimizer, criterion, device)\n        loss = evaluate(model, dl_valid, criterion, device)\n        if loss < best_loss:\n            best_loss = loss\n            torch.save(model.state_dict(), f\"model.pth\")\n            print(f\"epoch: {epoch}\\tvalid-loss: {loss}\\tbest!\")\n        else:\n            print(f\"epoch: {epoch}\\tvalid-loss: {loss}\")\n    \n    with torch.no_grad():\n        model.load_state_dict(torch.load(f\"model.pth\"))\n        preds_valid[idx_valid] = predict(model, dl_valid, device)    \n        preds_test += predict(model, dl_test, device) \/ n_splits\n    print()\n\nlogloss = log_loss(y, preds_valid)\nacc = accuracy_score(y, np.argmax(preds_valid, axis=1))\nprint(f\"logloss: {logloss:.5f}\\tacc: {acc:.5f}\")","b8f645fc":"submission = pd.read_csv(INPUT_DIR + \"sample_submission.csv\")\nsubmission[\"pred\"] = pd.Series(np.argmax(preds_test, axis=1)).map(result_inv_map)\nsubmission.to_csv(OUTPUT_DIR + \"nn.csv\", index=False)","8a6c997e":"submission[\"pred\"].value_counts()","5381f444":"## NN (Pytorch)","fa162caa":"## LightGBM","e7599408":"## Preprocess","9fda9d03":"## Feature Engineering","02f2671a":"### student_info + registration","325e7369":"### assessment","8f6f8e66":"### vle"}}