{"cell_type":{"3c427ea1":"code","91ae78b6":"code","27b347eb":"code","7b668bdb":"code","ddda4a38":"code","66ed49cb":"code","e321de14":"code","09abed59":"code","c1401045":"code","434a626e":"code","0e68031c":"code","2db998a5":"code","3105c9eb":"code","1575890a":"code","e4728fa4":"code","5b8efaee":"code","3f988ccd":"code","bc6ae356":"code","1326dc8c":"code","16ca9f53":"markdown"},"source":{"3c427ea1":"import sys\nsys.path.append(\"..\/usr\/lib\/\")","91ae78b6":"'''\nINPUT_ROOT = '\/home\/cszsolnai\/Projects\/kaggle_covid19_nlp\/input'\nOUTPUT_ROOT = '\/home\/cszsolnai\/Projects\/kaggle_covid19_nlp\/output'\nBIOBERT_ROOT = '\/home\/cszsolnai\/Projects\/kaggle_covid19_nlp\/input\/biobert_v1.1_pubmed'\n'''\n\nINPUT_ROOT = '\/kaggle\/input\/CORD-19-research-challenge'\nOUTPUT_ROOT = '.'\nBIOBERT_ROOT = '\/kaggle\/input\/biobert-pretrained\/biobert_v1.1_pubmed'\n\n\nMETADATA = INPUT_ROOT + '\/metadata.csv'\nMODEL_PATH = BIOBERT_ROOT + '\/model.ckpt-1000000'\nVOCAB_FILE = BIOBERT_ROOT + '\/vocab.txt'\nBERT_CONFIG = BIOBERT_ROOT + '\/bert_config.json'","27b347eb":"!pip install tensorflow-gpu==1.14.0\n!pip install bert-tensorflow","7b668bdb":"import pickle\nimport argparse\nimport re\nimport os\nimport glob\nimport json\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport nltk\nimport argparse\nimport re\nimport numpy as np\nimport pickle\nfrom tqdm import tqdm\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords","ddda4a38":"from bert_embeddings import bert_embedding_generator","66ed49cb":"nltk.download('stopwords')","e321de14":"def fold(e):\n    if isinstance(e, list):\n        return ' '.join(e)\n    else:\n        return e\n\ndef get_content(file_path):\n    with open(file_path) as file:\n        content = json.load(file)\n\n        if 'abstract' in content:\n            abstract = []\n            for entry in content['abstract']:\n                abstract.append(entry['text'])\n\n            full_abstract = '\\n'.join(abstract)\n        else:\n            full_abstract = ''\n\n        # Body text\n        articles_text = []\n        if 'body_text' in content:\n            for entry in content['body_text']:\n                articles_text.append(entry['text'])\n            full_text = '\\n'.join(articles_text)\n            concat_text = '\\n'.join([full_abstract, full_text]).strip()\n        else:\n            full_text = ''\n            concat_text = ''\n            \n\n\n        # Authors\n        authors = []\n        if 'authors' in content['metadata']:\n            for author in content['metadata']['authors']:\n                authors.append(' '.join([fold(author['first']), fold(author['middle']), fold(author['last'])]))\n\n        return content['paper_id'], concat_text, content['metadata'][\n            'title'], full_abstract, '\\n'.join(authors), full_text\n\n\ndef lower_case(input_str):\n    input_str = input_str.lower()\n    return input_str","09abed59":"all_json = glob.glob(INPUT_ROOT + '\/**\/*.json', recursive=True)","c1401045":"# https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/tasks?taskId=887\nquery_text = \"What is the best method to combat the hypercoagulable state seen in COVID-19 ?\"","434a626e":"converted_query = next(bert_embedding_generator([query_text], [0], BERT_CONFIG, VOCAB_FILE, MODEL_PATH))\nquery_embeddings = np.stack([e[0] for e in converted_query.values()])","0e68031c":"# Get the article with the closest embedding tot the query string embedding\n\n\nclass EmptyContent(Exception):\n    pass\n\n\ndef flatten_list(values):\n    l = []\n    for e in values:\n        if isinstance(e, list):\n               for embedding in e:\n                    l.append(embedding)\n        else:\n            l.append(e)\n    return l\n\n\ndef average_query_distance(article_embeddings):\n    avg_best_distance_from_query = 0\n    for i in range(query_embeddings.shape[0]):\n        avg_best_distance_from_query += np.min(np.linalg.norm(query_embeddings[i] - article_embeddings, axis=1))\n    return avg_best_distance_from_query \/ query_embeddings.shape[0]\n\n\ndef article_content_generator(all_json):\n    for idx, entry in tqdm(enumerate(all_json)):\n        paper_id, concat_text, title, abstract, authors, _ = get_content(entry)\n        if concat_text == '':\n            yield EmptyContent()\n        else:\n            yield concat_text\n\n\nclosest_ids = []\nclosest_num = 10\nclosest_distances = []\n\nupdate_frequency = 1000\n\nfor idx, converted_article in enumerate(bert_embedding_generator(article_content_generator(all_json), list(range(len(all_json))), BERT_CONFIG, VOCAB_FILE, MODEL_PATH)):\n    if isinstance(converted_article, EmptyContent):\n        continue\n    \n    article_embeddings = np.stack(flatten_list(converted_article.values()))\n            \n    closest_ids.append(idx)\n    closest_distances.append(average_query_distance(article_embeddings))\n    \n    # Remove article id with the highest distance\n    if len(closest_ids) > closest_num:\n        max_id = np.argmax(closest_distances)\n        del closest_distances[max_id]\n        del closest_ids[max_id]\n\n    if (idx+1) % update_frequency == 0:\n        with open('checkpoint.pkl', 'wb') as f:\n            data = {\n                'idx': idx,\n                'closest_ids': closest_ids,\n                'closest_distances': closest_distances\n            }\n            pickle.dump(data, f)\n\n\n","2db998a5":"with open('checkpoint.pkl', 'rb') as f:\n    data = pickle.load(f)\n    idx = data['idx']\n    closest_ids = data['closest_ids']\n    closest_distances = data['closest_distances']","3105c9eb":"idxs = np.argsort(closest_distances)\nclosest_ids = np.array(closest_ids)[idxs]\nclosest_distances = np.array(closest_distances)[idxs]\n\nselected_jsons = list(np.array(all_json)[idxs])\n\n#selected_jsons = [all_json[0]]","1575890a":"import torch\nfrom torch.utils.data import DataLoader, SequentialSampler\nfrom collections import namedtuple\nfrom transformers import BertTokenizer\nfrom modeling_bertabs import BertAbs, build_predictor\n\nfrom utils_bertabs import (\n    CovidDataset,\n    build_mask,\n    compute_token_type_ids,\n    encode_for_summarization,\n    truncate_or_pad,\n)","e4728fa4":"def build_data_iterator(all_jsons, args, tokenizer):\n    dataset = CovidDataset(all_jsons)\n    sampler = SequentialSampler(dataset)\n\n    def collate_fn(data):\n        return collate(data, tokenizer, block_size=512, device=args['device'])\n\n    iterator = DataLoader(dataset, sampler=sampler, batch_size=args['batch_size'], collate_fn=collate_fn, )\n\n    return iterator, dataset\n\n\ndef collate(data, tokenizer, block_size, device):\n    \"\"\" Collate formats the data passed to the data loader.\n\n    In particular we tokenize the data batch after batch to avoid keeping them\n    all in memory. We output the data as a namedtuple to fit the original BertAbs's\n    API.\n    \"\"\"\n    data = [x for x in data if not len(x[1]) == 0]  # remove empty_files\n    names = [name for name, _, _ in data]\n    summaries = [\" \".join(summary_list) for _, _, summary_list in data]\n\n    encoded_text = [encode_for_summarization(story, summary, tokenizer) for _, story, summary in data]\n    encoded_stories = torch.tensor(\n        [truncate_or_pad(story, block_size, tokenizer.pad_token_id) for story, _ in encoded_text]\n    )\n    encoder_token_type_ids = compute_token_type_ids(encoded_stories, tokenizer.cls_token_id)\n    encoder_mask = build_mask(encoded_stories, tokenizer.pad_token_id)\n\n    batch = Batch(\n        document_names=names,\n        batch_size=len(encoded_stories),\n        src=encoded_stories.to(device),\n        segs=encoder_token_type_ids.to(device),\n        mask_src=encoder_mask.to(device),\n        tgt_str=summaries,\n    )\n\n    return batch\n","5b8efaee":"Batch = namedtuple(\"Batch\", [\"document_names\", \"batch_size\", \"src\", \"segs\", \"mask_src\", \"tgt_str\"])\n\nBATCH_SIZE = 1\nMIN_LENGTH = 50\nMAX_LENGTH = 200\nBEAM_SIZE = 5\nDEVICE = torch.device(\"cuda\")\n\nargs = {\n    'documents_dir': INPUT_ROOT,\n    'summaries_output_dir': 'output_dir',\n    'compute_rouge': False,\n    'no_cuda': False,\n    'batch_size': BATCH_SIZE,\n    'min_length': MIN_LENGTH,\n    'max_length': MAX_LENGTH,\n    'beam_size': BEAM_SIZE,\n    'alpha': 0.95,\n    'block_trigram': True,\n    'device': DEVICE\n}","3f988ccd":"df = pd.read_csv(\n    METADATA,\n    usecols=[\"title\", \"abstract\", \"authors\", \"doi\", \"publish_time\"],\n)\n\n# drop duplicates\n# df=df.drop_duplicates()\ndf = df.drop_duplicates(subset=\"abstract\", keep=\"first\")\n# drop NANs\ndf = df.dropna()\n\ndf.head()","bc6ae356":"tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n\nmodel = BertAbs.from_pretrained(\"bertabs-finetuned-cnndm\")\nmodel.to(args['device'])\nmodel.eval()\n\nsymbols = {\n    \"BOS\": tokenizer.vocab[\"[unused0]\"],\n    \"EOS\": tokenizer.vocab[\"[unused1]\"],\n    \"PAD\": tokenizer.vocab[\"[PAD]\"],\n}\n\nargs['result_path'] = \"\"\nargs['temp_dir'] = \"\"\n\ndata_iterator, dataset = build_data_iterator(selected_jsons, args, tokenizer)\n\npredictor = build_predictor(args, tokenizer, symbols, model)","1326dc8c":"all_summaries =  []\n\ndata = {'Paper id': [], 'Title': [], 'Summary': []}\n\nfor i, batch in enumerate(tqdm(data_iterator)):\n    with open(selected_jsons[i]) as json_file:\n        article = json.load(json_file)\n    \n    batch_data = predictor.translate_batch(batch)\n    translations = predictor.from_batch(batch_data)\n    data['Paper id'].append(article['paper_id'])\n    data['Title'].append(article['metadata']['title'])\n    data['Summary'].append(translations[0][2])\ndf = pd.DataFrame.from_dict(data)\ndf.to_csv(os.path.join(OUTPUT_ROOT, 'summaries.csv'))","16ca9f53":"Save all articles as dataframe"}}