{"cell_type":{"08d0e08b":"code","2fed2e0e":"code","bb9fd176":"code","19ba2c99":"code","c799c8ae":"code","030cde28":"code","9fc98178":"code","0be419dc":"code","123068fd":"code","2d032eeb":"code","c61a30d3":"code","ef02901f":"code","883b9a69":"code","aeef22ee":"code","5809a9a8":"code","c913eb8b":"code","7da009d7":"code","6f88b9ce":"code","a319af1f":"code","8c462d2d":"code","f41817a8":"code","a272d514":"code","3835df3e":"code","c9e80075":"code","08689db3":"code","bee4c421":"code","9d8b30c6":"code","a83d75a8":"code","7f3361c9":"code","99692599":"code","068e6a5c":"code","095ed13c":"code","51babbea":"code","1eeb584e":"code","01f59169":"code","487c5c7e":"code","8b6a6c67":"code","648429b4":"code","8aa9054a":"code","fc4acef3":"code","7f1fdb2e":"code","59f792c7":"code","eda89780":"code","2886465e":"code","a4ad5f04":"code","336faa2c":"code","ad88fa70":"code","41a7b9c8":"code","e2d5e5c5":"code","0ba61f17":"code","61e3e3a8":"code","576478a0":"code","bd4a194c":"code","56f01de1":"code","5dfcbf2e":"code","11431379":"code","1e4d263b":"code","12dd0b23":"code","75c8ea21":"code","2448533b":"code","3f23613b":"code","fca810f0":"code","d0fe2c7c":"code","3e8667e0":"code","00376af8":"code","4268bbf2":"code","29a00ebc":"code","da00fb5d":"code","8096600f":"code","708f9996":"code","9d99763a":"code","714c49f2":"code","a6e92a9c":"code","1554b0a4":"code","94dd6397":"code","f6859d3a":"code","3d1e593e":"code","f1e2e719":"code","fff33e9a":"code","59b3650d":"code","cdb00e8c":"code","e44455cf":"code","072551c1":"code","6246277f":"code","a47957e8":"code","2993eade":"code","bc008d23":"code","bb7fe895":"code","2018358e":"code","facb7fb7":"code","ae54c572":"code","f5d50d4d":"code","875129df":"code","5c1af27f":"code","8a6b83fb":"code","0a8e5736":"code","ff679687":"code","f5e1db79":"code","26d32390":"code","2c7c59a9":"code","5497ebfe":"code","4d63043b":"code","1f73737b":"code","fbddb019":"markdown","df0631e5":"markdown","df1ae312":"markdown","a404242b":"markdown","c4e2db1c":"markdown","9f92c570":"markdown","584e3cf3":"markdown","bdc8f200":"markdown","da5d233c":"markdown","cd6d96f6":"markdown","8921a4d3":"markdown","9bbf8286":"markdown","943565a4":"markdown","7a400150":"markdown","b5838103":"markdown","5892a889":"markdown","7dc840f8":"markdown","71d47e81":"markdown","e2ea01c4":"markdown","70c8f205":"markdown","61b5adda":"markdown","e5a9cf4e":"markdown","db8dfb30":"markdown","01f95c71":"markdown","90797c9e":"markdown","c5459139":"markdown","c512dae1":"markdown","94275af6":"markdown","828005d8":"markdown"},"source":{"08d0e08b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import StratifiedKFold\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\nfrom hyperopt import tpe,hp,Trials\nfrom hyperopt.fmin import fmin\nimport gc\nimport os\nprint(os.listdir(\"..\/input\"))\nseed=5\n# Any results you write to the current directory are saved as output.","2fed2e0e":"train_identity=pd.read_csv('..\/input\/train_identity.csv')\ntest_identity=pd.read_csv('..\/input\/test_identity.csv')\ntrain_transaction=pd.read_csv('..\/input\/train_transaction.csv')\ntest_transaction=pd.read_csv('..\/input\/test_transaction.csv')","bb9fd176":"train=pd.merge(train_transaction,train_identity,how='left',on='TransactionID')\ntest=pd.merge(test_transaction,test_identity,how='left',on='TransactionID')","19ba2c99":"del train_identity,test_identity,train_transaction,test_transaction","c799c8ae":"train.info()","030cde28":"# most_null_values=[col for col in train.columns if (train[col].isna().sum()\/train.shape[0])>0.9]\n# len(most_null_values)","9fc98178":"# dominant_unique_values=[col for col in train.columns if (train[col].value_counts().values[0]\/train.shape[0])>0.9]\n# len(dominant_unique_values)","0be419dc":"# dominant_unique_values.remove('isFraud')","123068fd":"# cols_to_drop=list(set(most_null_values+dominant_unique_values+['TransactionID','TransactionDT']))\n# train=train.drop(cols_to_drop,axis=1)\n# test=test.drop(cols_to_drop,axis=1)","2d032eeb":"train.columns[:50]","c61a30d3":"sns.countplot(train['isFraud'])","ef02901f":"fig,ax=plt.subplots(1,2,figsize=(15,5))\nsns.distplot(train[train['isFraud']==0]['TransactionAmt'],ax=ax[0],hist=False,label='NonFraud')\nsns.distplot(train[train['isFraud']==1]['TransactionAmt'],ax=ax[0],hist=False,label='Fraud')\nax[0].set_title('Fraud and NonFraud TransactionAmt Distribution')\n\nsns.distplot(np.log(test['TransactionAmt']),ax=ax[1],hist=False,label='Test')\nsns.distplot(np.log(train['TransactionAmt']),ax=ax[1],hist=False,label='Train')\nax[1].set_title('Test and Train TransationAmt Distribution')","883b9a69":"fig,ax=plt.subplots(1,2,figsize=(10,5))\nsns.distplot(test['TransactionAmt'],hist=False,label='Test',ax=ax[0],color='orange')\nax[0].set_title('Distribution of TransactionAmt in Test')\nsns.distplot(train['TransactionAmt'],hist=False,label='Train',ax=ax[1])\nax[1].set_title('Distribution of TransactionAmt in Train')\nplt.tight_layout()","aeef22ee":"fig,ax=plt.subplots(1,2,figsize=(10,5))\nsns.countplot(train['ProductCD'],ax=ax[0])\nax[0].set_title('Train ProductCD Distribution')\n\nsns.countplot(test['ProductCD'],ax=ax[1],label='Test')\nax[1].set_title('Test ProductCD Distribution')\n\nplt.tight_layout()\n","5809a9a8":"card_cols=['card1','card2','card3','card4','card5','card6']\nfor c in card_cols:\n    print(f'Number of unique variables in {c}: ',train[c].nunique())","c913eb8b":"fig,ax=plt.subplots(1,2,figsize=(15,5))\nsns.distplot(train[train['isFraud']==0]['card1'],ax=ax[0],hist=False,label='NonFraud')\nsns.distplot(train[train['isFraud']==1]['card1'],ax=ax[0],hist=False,label='Fraud')\nax[0].set_title('Fraud and NonFraud Card1 Distribution')\n\nsns.distplot(test['card1'],ax=ax[1],hist=False,label='Test')\nsns.distplot(train['card1'],ax=ax[1],hist=False,label='Train')\nax[1].set_title('Test and Train Card1 Distribution')","7da009d7":"fig,ax=plt.subplots(1,2,figsize=(15,5))\nsns.distplot(train[train['isFraud']==0]['card2'],ax=ax[0],hist=False,label='NonFraud')\nsns.distplot(train[train['isFraud']==1]['card2'],ax=ax[0],hist=False,label='Fraud')\nax[0].set_title('Fraud and NonFraud Card1 Distribution')\n\nsns.distplot(test['card2'],ax=ax[1],hist=False,label='Test')\nsns.distplot(train['card2'],ax=ax[1],hist=False,label='Train')\nax[1].set_title('Test and Train Card1 Distribution')","6f88b9ce":"fig,ax=plt.subplots(2,2,figsize=(15,8))\nsns.distplot(train[train['isFraud']==0]['card3'],ax=ax[0,0],hist=False,label='NonFraud')\nsns.distplot(train[train['isFraud']==1]['card3'],ax=ax[0,0],hist=False,label='Fraud')\nax[0,0].set_title('Fraud and NonFraud Card3 Distribution')\n\nsns.distplot(test['card3'],ax=ax[0,1],hist=False,label='Test')\nsns.distplot(train['card3'],ax=ax[0,1],hist=False,label='Train')\nax[0,1].set_title('Test and Train Card3 Distribution')\n\nsns.distplot(train[train['isFraud']==0]['card5'],ax=ax[1,0],hist=False,label='NonFraud')\nsns.distplot(train[train['isFraud']==1]['card5'],ax=ax[1,0],hist=False,label='Fraud')\nax[1,0].set_title('Fraud and NonFraud Card5 Distribution')\n\nsns.distplot(test['card5'],ax=ax[1,1],hist=False,label='Test')\nsns.distplot(train['card5'],ax=ax[1,1],hist=False,label='Train')\nax[1,1].set_title('Test and Train Card5 Distribution')\n\nplt.tight_layout()","a319af1f":"fig,ax=plt.subplots(2,2,figsize=(10,8))\nsns.countplot(train['card4'],ax=ax[0,0])\nax[0,0].set_title('Train Card4 Distribution')\n\nsns.countplot(test['card4'],ax=ax[0,1])\nax[0,1].set_title('Test Card4 Distribution')\n\nsns.countplot(train['card6'],ax=ax[1,0])\nax[1,0].set_title('Train Card6 Distribution')\n\nsns.countplot(test['card6'],ax=ax[1,1])\nax[1,1].set_title('Test Card6 Distribution')\n\nplt.tight_layout()","8c462d2d":"train.columns[:50]","f41817a8":"addr=['addr1','addr2','dist1','dist2','P_emaildomain','R_emaildomain']\nfor c in addr:\n    print(f'Number of unique variables in {c}: ',train[c].nunique())","a272d514":"fig,ax=plt.subplots(2,2,figsize=(15,8))\nsns.distplot(train[train['isFraud']==0]['addr1'],ax=ax[0,0],hist=False,label='NonFraud')\nsns.distplot(train[train['isFraud']==1]['addr1'],ax=ax[0,0],hist=False,label='Fraud')\nax[0,0].set_title('Fraud and NonFraud addr1 Distribution')\n\nsns.distplot(test['addr1'],ax=ax[0,1],hist=False,label='Test')\nsns.distplot(train['addr1'],ax=ax[0,1],hist=False,label='Train')\nax[0,1].set_title('Test and Train addr1 Distribution')\n\nsns.distplot(train[train['isFraud']==0]['addr2'],ax=ax[1,0],hist=False,label='NonFraud')\nsns.distplot(train[train['isFraud']==1]['addr2'],ax=ax[1,0],hist=False,label='Fraud')\nax[1,0].set_title('Fraud and NonFraud addr2 Distribution')\n\nsns.distplot(test['addr2'],ax=ax[1,1],hist=False,label='Test')\nsns.distplot(train['addr2'],ax=ax[1,1],hist=False,label='Train')\nax[1,1].set_title('Test and Train addr2 Distribution')\n\nplt.tight_layout()","3835df3e":"fig,ax=plt.subplots(2,2,figsize=(15,8))\nsns.distplot(train[train['isFraud']==0]['dist1'],ax=ax[0,0],hist=False,label='NonFraud')\nsns.distplot(train[train['isFraud']==1]['dist1'],ax=ax[0,0],hist=False,label='Fraud')\nax[0,0].set_title('Fraud and NonFraud dist1 Distribution')\n\nsns.distplot(test['dist1'],ax=ax[0,1],hist=False,label='Test')\nsns.distplot(train['dist1'],ax=ax[0,1],hist=False,label='Train')\nax[0,1].set_title('Test and Train dist1 Distribution')\n\nsns.distplot(train[train['isFraud']==0]['dist2'],ax=ax[1,0],hist=False,label='NonFraud')\nsns.distplot(train[train['isFraud']==1]['dist2'],ax=ax[1,0],hist=False,label='Fraud')\nax[1,0].set_title('Fraud and NonFraud dist2 Distribution')\n\nsns.distplot(test['dist2'],ax=ax[1,1],hist=False,label='Test')\nsns.distplot(train['dist2'],ax=ax[1,1],hist=False,label='Train')\nax[1,1].set_title('Test and Train dist2 Distribution')\n\nplt.tight_layout()","c9e80075":"fig,ax=plt.subplots(1,2,figsize=(15,5))\nsns.distplot(test['dist2'],hist=False,label='Test',ax=ax[0],color='orange')\nax[0].set_title('Distribution of dist2 in Test')\nsns.distplot(train['dist2'],hist=False,label='Train',ax=ax[1])\nax[1].set_title('Distribution of dist2 in Train')\nplt.tight_layout()","08689db3":"fig,ax=plt.subplots(2,2,figsize=(15,8))\ntrain['P_emaildomain'].value_counts()[:10].plot.bar(ax=ax[0,0])\nax[0,0].set_title('Train P_emaildomain Distribution')\n\ntest['P_emaildomain'].value_counts()[:10].plot.bar(ax=ax[0,1])\nax[0,1].set_title('Test P_emaildomain Distribution')\n\ntrain['R_emaildomain'].value_counts()[:10].plot.bar(ax=ax[1,0])\nax[1,0].set_title('Train R_emaildomain Distribution')\n\ntest['R_emaildomain'].value_counts()[:10].plot.bar(ax=ax[1,1])\nax[1,1].set_title('Test R_emaildomain Distribution')\n\nplt.tight_layout()","bee4c421":"C_columns=[col for col in train.columns if 'C'==col[0]]\nfor c in C_columns:\n    print(f'Number of unique entries in {c}:',train[c].nunique())","9d8b30c6":"def get_subplots(feature1,feature2):\n    fig,ax=plt.subplots(2,2,figsize=(15,8))\n    sns.distplot(train[train['isFraud']==0][feature1],ax=ax[0,0],hist=False,label='NonFraud')\n    sns.distplot(train[train['isFraud']==1][feature1],ax=ax[0,0],hist=False,label='Fraud')\n    ax[0,0].set_title(f'Fraud and NonFraud {feature1} Distribution')\n\n    sns.distplot(test[feature1],ax=ax[0,1],hist=False,label='Test')\n    sns.distplot(train[feature1],ax=ax[0,1],hist=False,label='Train')\n    ax[0,1].set_title(f'Test and Train {feature1} Distribution')\n\n    sns.distplot(train[train['isFraud']==0][feature2],ax=ax[1,0],hist=False,label='NonFraud')\n    sns.distplot(train[train['isFraud']==1][feature2],ax=ax[1,0],hist=False,label='Fraud')\n    ax[1,0].set_title(f'Fraud and NonFraud {feature2} Distribution')\n\n    sns.distplot(test[feature2],ax=ax[1,1],hist=False,label='Test')\n    sns.distplot(train[feature2],ax=ax[1,1],hist=False,label='Train')\n    ax[1,1].set_title(f'Test and Train {feature2} Distribution')\n\n    plt.tight_layout()\n","a83d75a8":"get_subplots('C1','C2')","7f3361c9":"get_subplots('C3','C4')","99692599":"get_subplots('C5','C6')","068e6a5c":"get_subplots('C7','C8')","095ed13c":"get_subplots('C9','C10')","51babbea":"get_subplots('C11','C12')","1eeb584e":"get_subplots('C13','C14')","01f59169":"train.columns[:50]","487c5c7e":"D_cols=[col for col in train.columns if col[0]=='D']\nfor d in D_cols:\n    print(f'Number of unique entries in {d}:',train[d].nunique())","8b6a6c67":"get_subplots('D1','D2')","648429b4":"get_subplots('D3','D4')","8aa9054a":"get_subplots('D5','D6')","fc4acef3":"def detailed_subplot(feature):\n    fig,ax=plt.subplots(1,2,figsize=(15,5))\n    sns.distplot(test[feature],hist=False,label='Test',ax=ax[0],color='orange')\n    ax[0].set_title(f'Distribution of {feature} in Test')\n    sns.distplot(train[feature],hist=False,label='Train',ax=ax[1])\n    ax[1].set_title(f'Distribution of {feature} in Train')\n    plt.tight_layout()","7f1fdb2e":"detailed_subplot('D6')","59f792c7":"get_subplots('D7','D8')","eda89780":"print(train['D9'].value_counts())\nprint('*'*100)\nprint(test['D9'].value_counts())","2886465e":"get_subplots('D10','D11')","a4ad5f04":"get_subplots('D12','D13')","336faa2c":"detailed_subplot('D12')","ad88fa70":"get_subplots('D14','D15')","41a7b9c8":"detailed_subplot('D14')","e2d5e5c5":"M_cols=[col for col in train.columns if col[0]=='M']\nfor m in M_cols:\n    print(f'Number of unique entries in {m}:',train[m].nunique())","0ba61f17":"def get_bar_subplots(feature1,feature2,top=10,incre=0):\n    fig,ax=plt.subplots(2,2,figsize=(15,8))\n    train[feature1].value_counts()[:top].plot.bar(ax=ax[0,0])\n    ax[0,0].set_title(f'Train {feature1} Distribution')\n    \n    test[feature1].value_counts()[:top].plot.bar(ax=ax[0,1])\n    ax[0,1].set_title(f'Test {feature1} Distribution')\n\n    train[feature2].value_counts()[:top+incre].plot.bar(ax=ax[1,0])\n    ax[1,0].set_title(f'Train {feature2} Distribution')\n\n    test[feature2].value_counts()[:top+incre].plot.bar(ax=ax[1,1])\n    ax[1,1].set_title(f'Test {feature2} Distribution')\n\n    plt.tight_layout()\n","61e3e3a8":"get_bar_subplots('M1','M2',2)","576478a0":"get_bar_subplots('M3','M4',2,1)","bd4a194c":"get_bar_subplots('M5','M6',2)","56f01de1":"get_bar_subplots('M7','M8',2)","5dfcbf2e":"fig,ax=plt.subplots(1,2,figsize=(15,8))\ntrain['M9'].value_counts().plot.bar(ax=ax[0])\nax[0].set_title('Train M9 Distribution')\n    \ntest['M9'].value_counts().plot.bar(ax=ax[1])\nax[1].set_title('Test M9 Distribution')","11431379":"train.columns[50:100]","1e4d263b":"V_cols=[col for col in train.columns if col[0]=='V']\nV_type=[train[col].dtype for col in V_cols]\nprint(set(V_type))\nprint(len(V_cols))","12dd0b23":"unique=[(col,train[col].nunique()) for col in V_cols]\nsorted_unique=sorted(unique,key=lambda x: x[1])\nsorted_unique[:50]","75c8ea21":"def get_bar_subplots(feature1,feature2):\n    fig,ax=plt.subplots(2,2,figsize=(15,8))\n    train[feature1].value_counts().plot.bar(ax=ax[0,0])\n    ax[0,0].set_title(f'Train {feature1} Distribution')\n    \n    test[feature1].value_counts().plot.bar(ax=ax[0,1])\n    ax[0,1].set_title(f'Test {feature1} Distribution')\n\n    train[feature2].value_counts().plot.bar(ax=ax[1,0])\n    ax[1,0].set_title(f'Train {feature2} Distribution')\n\n    test[feature2].value_counts().plot.bar(ax=ax[1,1])\n    ax[1,1].set_title(f'Test {feature2} Distribution')\n\n    plt.tight_layout()","2448533b":"get_subplots('V339','V338')","3f23613b":"detailed_subplot('V328')","fca810f0":"train.columns[350:]","d0fe2c7c":"i_cols=[col for col in train.columns if col[0]=='i']\nunique_val=[(col,train[col].nunique()) for col in i_cols]\nunique_val","3e8667e0":"get_subplots('id_01','id_02')","00376af8":"get_subplots('id_03','id_04')","4268bbf2":"get_subplots('id_05','id_06')","29a00ebc":"get_subplots('id_07','id_08')","da00fb5d":"get_subplots('id_09','id_10')","8096600f":"get_subplots('id_11','id_13')","708f9996":"get_subplots('id_14','id_17')","9d99763a":"get_subplots('id_18','id_19')","714c49f2":"get_subplots('id_20','id_21')","a6e92a9c":"get_subplots('id_22','id_24')","1554b0a4":"get_subplots('id_25','id_26')","94dd6397":"get_bar_subplots('id_38','id_37')","f6859d3a":"get_bar_subplots('id_35','id_36')","3d1e593e":"get_bar_subplots('id_32','id_34')","f1e2e719":"get_bar_subplots('id_28','id_29')","fff33e9a":"get_bar_subplots('id_23','id_27')","59b3650d":"get_bar_subplots('id_15','id_16')","cdb00e8c":"get_bar_subplots('id_12','id_15')","e44455cf":"drop_cols=['dist2','C8','C9','C10','D6','D12','V27','V28','V45','V70','V71','V77','V78','V86','V87','V89','V91','V92','V95','V96','V97',\n          'V101','V102','V103','V107','V126','V127','V128','V130','V131','V132','V133','V134','V135','V136','V137','V143','V145','V150',\n          'V159','V160','V164','V165','V166','V167','V168','V171','V176','V177','V178','V179','V181','V182','V183','V190','V199','V202',\n          'V203','V204','V207','V211','V212','V213','V214','V215','V216','V221','V226','V227','V228','V230','V234','V240','V241','V245',\n          'V246','V255','V256','V257','V258','V259','V261','V263','V264','V265','V270','V271','V272','V273','V274','V275','V276','V277',\n          'V278','V279','V280','V291','V292','V293','V294','V295','V297','V306','V307','V308','V310','V312','V316','V317','V318','V319',\n          'V320','V321','V322','V323','V324','V331','V332','V333','V338','V339','TransactionDT','TransactionID']\n\ntrain=train.drop(drop_cols,1)\ntest=test.drop(drop_cols,1)\ntrain['TransactionAmt']=np.log(train['TransactionAmt'])\ntest['TransactionAmt']=np.log(test['TransactionAmt'])","072551c1":"\n# drop_col = ['TransactionDT','TransactionID','V300', 'V309', 'V111', 'C3', 'V124', 'V106', 'V125', 'V315', 'V134', 'V102', 'V123', \n#             'V316', 'V113', 'V136', 'V305', 'V110', 'V299', 'V289', 'V286', 'V318', 'V103', 'V304', 'V116', 'V298', \n#             'V284', 'V293', 'V137', 'V295', 'V301', 'V104', 'V311', 'V115', 'V109', 'V119', 'V321', 'V114', 'V133', \n#             'V122', 'V319', 'V105', 'V112', 'V118', 'V117', 'V121', 'V108', 'V135', 'V320', 'V303', 'V297', 'V120']\n\n# drop_col=['TransactionDT','TransactionID']\n# train=train.drop(drop_col,1)\n# test=test.drop(drop_col,1)","6246277f":"%%time\n# From kernel https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\n# WARNING! THIS CAN DAMAGE THE DATA \ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","a47957e8":"train=reduce_mem_usage(train)\ntest=reduce_mem_usage(test)","2993eade":"# obj_cols=[col for col in train.columns if train[col].dtype=='object']\n# if 'isFraud' in obj_cols:\n#     obj_cols.remove('isFraud')\n    \n# unique_values=sorted([(col,train[col].nunique()+test[col].nunique()) for col in obj_cols],key=lambda x: x[1],reverse=False)","bc008d23":"# dummy_cols=[col[0] for col in unique_values[:18]]\n# target=train['isFraud']\n# train=train.drop('isFraud',1)\n# ntrain=train.shape[0]\n# print(train.shape)\n# merged_data=pd.concat([train,test],axis=0,ignore_index=True)\n# X=pd.get_dummies(merged_data,columns=dummy_cols)\n# merged_data.drop(dummy_cols,axis=1,inplace=True)\n# merged_data=pd.concat([merged_data,X],axis=1)\n# del X\n# train=merged_data[:ntrain]\n# test=merged_data[ntrain:]\n# print(train.shape)\n# del merged_data\n# gc.collect()","bb7fe895":"num_cols=[col for col in train.columns if (('float' in str(train[col].dtype)) or ('int' in str(train[col].dtype)))]\nnum_cols.remove('isFraud')\ntrain['mean']=train[num_cols].mean(axis=1)\ntest['mean']=test[num_cols].mean(axis=1)\ntrain['std']=train[num_cols].std(axis=1)\ntest['std']=test[num_cols].std(axis=1)\ntrain['max']=train[num_cols].max(axis=1)\ntest['max']=test[num_cols].max(axis=1)\ntrain['min']=train[num_cols].min(axis=1)\ntest['min']=test[num_cols].min(axis=1)\ntrain['median']=train[num_cols].median(axis=1)\ntest['median']=test[num_cols].median(axis=1)\ntrain['skew']=train[num_cols].skew(axis=1)\ntest['skew']=test[num_cols].skew(axis=1)\ntrain['kurt']=train[num_cols].kurt(axis=1)\ntest['kurt']=test[num_cols].kurt(axis=1)","2018358e":"# columns=['TransactionAmt','card1','card2','addr2','dist1','C1','C2','D1','D2','V1','V2',\n#         'id_01','id_02']\n# obj_cols=['DeviceInfo','card4','card6','ProductCD','DeviceType']\n\n# for col in columns:\n#     for feat in obj_cols:\n#         train[f'{col}_mean_group_{feat}']=train[col]\/train.groupby(feat)[col].transform('mean')\n#         test[f'{col}_mean_group_{feat}']=test[col]\/test.groupby(feat)[col].transform('mean')\n#         train[f'{col}_max_group_{feat}']=train[col]\/train.groupby(feat)[col].transform('max')\n#         test[f'{col}_max_group_{feat}']=test[col]\/test.groupby(feat)[col].transform('max')\n#         train[f'{col}_min_group_{feat}']=train[col]\/train.groupby(feat)[col].transform('min')\n#         test[f'{col}_min_group_{feat}']=test[col]\/test.groupby(feat)[col].transform('min')\n#         train[f'{col}_skew_group_{feat}']=train[col]\/train.groupby(feat)[col].transform('skew')\n#         test[f'{col}_skew_group_{feat}']=test[col]\/test.groupby(feat)[col].transform('skew')\n#         train[f'{col}_skew_group_{feat}']=train[col]\/train.groupby(feat)[col].transform('count')\n#         test[f'{col}_skew_group_{feat}']=test[col]\/test.groupby(feat)[col].transform('count')\n   ","facb7fb7":"# def fill_missing(df):\n#     num_cols=[col for col in df.columns if df[col].dtype=='float64' or df[col].dtype=='int64']\n#     for col in num_cols:\n#         df[col]=df[col].fillna(df[col].mean())\n#     obj_cols=[col for col in df.columns if df[col].dtype=='object']\n#     for col in obj_cols:\n#         df[col]=df[col].fillna(df[col].mode()[0])\n        \n#     return df","ae54c572":"# train_df=fill_missing(train_df)\n# test_df=fill_missing(test_df)","f5d50d4d":"from sklearn.preprocessing import LabelEncoder\n\nobject_cols=[col for col in train.columns if (('category' in str(train[col].dtype)) or ('object' in str(train[col].dtype)))]\nle=LabelEncoder()\nfor col in object_cols:\n    le.fit(list(train[col].values)+list(test[col].values))\n    train[col]=le.transform(list(train[col].values))\n    test[col]=le.transform(list(test[col].values))","875129df":"train.shape , test.shape","5c1af27f":"target=train['isFraud']\ntrain=train.drop('isFraud',1)","8a6b83fb":"# from sklearn.model_selection import train_test_split\n# train_X,val_X,train_y,val_y=train_test_split(train,target,test_size=0.2,random_state=seed,stratify=target)","0a8e5736":"# from xgboost import XGBClassifier\n# from sklearn.metrics import roc_auc_score\n\n\n# def objective(params):\n#     params=dict(max_depth=int(params['max_depth']),\n#                subsample=np.round(params['subsample'],3),\n#                colsample_bytree=np.round(params['colsample_bytree'],3),\n#                learning_rate=np.round(params['learning_rate'],3),\n#                verbosity=0)\n    \n#     clf=XGBClassifier(n_estimators=1000,random_state=seed,**params,tree_method='gpu_hist')\n#     clf.fit(train_X,train_y,eval_set=[(val_X,val_y)],eval_metric='auc',early_stopping_rounds=10)\n#     val_pred=clf.predict(val_X)\n#     score=roc_auc_score(val_y,val_pred)\n#     return score\n\n# space={'max_depth':hp.quniform('max_depth',2,10,2),\n#       'subsample':hp.uniform('subsample',0.1,1),\n#       'colsample_bytree':hp.uniform('colsample_bytree',0.1,1),\n#       'learning_rate':hp.uniform('learning_rate',0.01,0.1)}\n\n# trial=Trials()\n# best=fmin(fn=objective,algo=tpe.suggest,space=space,max_evals=100,trials=trial,rstate=np.random.RandomState(seed))\n\n    ","ff679687":"# best['max_depth']=int(best['max_depth'])\n# print('Best parameters:',best)","f5e1db79":"# del train_X,val_X,train_y,val_y\n# gc.collect()","26d32390":"# TID=[t['tid'] for t in trial.trials]\n# Loss=[t['result']['loss'] for t in trial.trials]\n# maxd=[t['misc']['vals']['max_depth'][0] for t in trial.trials]\n# lr=[t['misc']['vals']['learning_rate'][0] for t in trial.trials]\n# sub=[t['misc']['vals']['subsample'][0] for t in trial.trials]\n# col_samp=[t['misc']['vals']['colsample_bytree'][0] for t in trial.trials]\n\n\n# hyperopt_xgb=pd.DataFrame({'tid':TID,'loss':Loss,\n#                           'max_depth':maxd,'learning_rate':lr,\n#                           'subsample':sub, 'colsample_bytree':col_samp})","2c7c59a9":"# plt.subplots(3,2,figsize=(10,10))\n# plt.subplot(3,2,1)\n# sns.scatterplot(x='tid',y='max_depth',data=hyperopt_xgb)\n# plt.subplot(3,2,2)\n# sns.scatterplot(x='tid',y='loss',data=hyperopt_xgb)\n# plt.subplot(3,2,3)\n# sns.scatterplot(x='tid',y='learning_rate',data=hyperopt_xgb)\n# plt.subplot(3,2,4)\n# sns.scatterplot(x='tid',y='subsample',data=hyperopt_xgb)\n# plt.subplot(3,2,5)\n# sns.scatterplot(x='tid',y='colsample_bytree',data=hyperopt_xgb)\n# plt.subplot(3,2,6)\n# sns.scatterplot(x='tid',y='loss',data=hyperopt_xgb)\n\n# plt.tight_layout()","5497ebfe":"from sklearn.model_selection import StratifiedKFold\nimport gc\n\nnfolds=10\n\n\nxgb_params=dict(n_estimators=1000,\n                verbosity=0,\n                tree_method='gpu_hist',\n                random_state=seed,\n               colsample_bytree=0.6,\n               subsample=0.6,\n               learning_rate=0.05,\n               max_depth=9)\n\nlgb_params=dict(objective='binary',\n               num_leaves=62,\n               seed=seed,\n               max_depth=9,\n               pos_bagging_fraction=0.5,\n               neg_bagging_fraction=1.0,\n               bagging_freq=5,\n               feature_fraction=0.9,\n                metric='auc',\n               learning_rate=0.05,\n               verbosity=-1,\n               device='gpu')\n\n\nskfold=StratifiedKFold(nfolds,random_state=seed)\n\n\n\ndef build_model(params,model='xgb',plot_feature_importance=True):\n    oof=np.zeros(train.shape[0])\n    pred=np.zeros(test.shape[0])\n    scores=[]\n    feature_importance=pd.DataFrame()\n    for i,(train_index,val_index) in enumerate(skfold.split(train,target)):\n        print('Fold :',i+1)\n\n        \n        if model=='xgb':\n            train_X,val_X=train.iloc[train_index,:],train.iloc[val_index,:]\n            train_y,val_y=target[train_index],target[val_index]\n            clf=XGBClassifier(**params)\n            clf.fit(train_X,train_y,eval_metric='auc',eval_set=[(val_X,val_y)],early_stopping_rounds=10,verbose=20)\n            val_pred=clf.predict_proba(val_X)[:,1]\n        \n        \n        if model=='lgb':\n        \n            train_d=lgb.Dataset(train.iloc[train_index,:].values,label=target[train_index].values)\n            val_d=lgb.Dataset(train.iloc[val_index,:].values,label=target[val_index].values)\n            clf=lgb.train(params,train_d,num_boost_round=1000,valid_sets=[val_d],verbose_eval=20,early_stopping_rounds=10)\n            val_pred=clf.predict(train.iloc[val_index,:].values)\n        \n    \n        oof[val_index]=val_pred\n        val_score=roc_auc_score(target[val_index],val_pred)\n        scores.append(val_score)\n        print(f'Validation score using {model} for fold {i} :'+ str(val_score))\n        print('-'*100)\n        \n        if model=='xgb':\n            pred+=clf.predict_proba(test)[:,1]\/nfolds\n        if model=='lgb':\n            pred+=clf.predict(test.values)\/nfolds\n            \n        if model=='xgb':\n            del train_X,val_X,train_y,val_y\n        if model=='lgb':\n            del train_d,val_d\n       \n        gc.collect()\n        \n        \n        fold_importance=pd.DataFrame()\n        fold_importance['feature']=train.columns\n        if model=='xgb':\n            fold_importance['importance']=clf.feature_importances_\n        if model=='lgb':\n            fold_importance['importance']=clf.feature_importance()\n        fold_importance['fold']=i+1\n        feature_importance=pd.concat([feature_importance,fold_importance],axis=0)\n            \n            \n    print('Mean validation score :',np.mean(scores)) \n    \n    if plot_feature_importance:\n        df=feature_importance[['feature','importance']].groupby('feature').mean().sort_values(by='importance',ascending=False).reset_index()\n        plt.figure(figsize=(10,10))\n        sns.barplot(x='importance',y='feature',data=df.iloc[:25,:])\n        plt.title('Feature Importances')\n        \n    return pred","4d63043b":"pred=build_model(xgb_params,model='xgb')","1f73737b":"sub=pd.read_csv('..\/input\/sample_submission.csv')\nsub['isFraud']=pred\nsub.to_csv('submission.csv',index=False)","fbddb019":"# <a id='2'>Distribution of features across Train and Test<\/a> ","df0631e5":"C4 seems very much useful for nonfraud and fraud distinction because its distribution is very different for those two categories.","df1ae312":"# <a id='1'>Importing Libraries and Modules<\/a> ","a404242b":"Remember to take log of Transaction Amount. ","c4e2db1c":"# <a id='3'>Dropping Columns<\/a> ","9f92c570":"# <a id='4'>Reducing memory size<\/a> ","584e3cf3":" Drop some columns : From https:\/\/www.kaggle.com\/jazivxt\/safe-box\/notebook","bdc8f200":"Remove the Transaction amout outliers or take log.","da5d233c":"We can try dropping this column.","cd6d96f6":"## <a id='22'>Distribution of Addr,Dist and Emaildomain across Train and Test<\/a> ","8921a4d3":"I think we can safely drop this column","9bbf8286":"I skimmed through the distribution of every V_column and removed the columns which satisfied following criteria:\n1. **extreme outliers in case of test or train** or \n2. **the Density of the major peak of test and train had very significant mismatch** or \n3. **test or train had lot of extra unique values compared to each other. **","943565a4":"We can try dropping this.","7a400150":"addr2 of test perfectly overlaps the addr2 of train and so it is not seen.","b5838103":"# <a id='6'>Model Development and Feature Importance<\/a> ","5892a889":"C8 and C7 have very different distribution in case of Fraud and NonFraud categories. So they might be useful. But again, C8 distribution in test and train is also very different. We will try dropping and not dropping it. ","7dc840f8":"<h1><center><font size=\"6\">IEEE FRAUD DETECTION<\/font><\/center><\/h1>\n\n\n\n\n<img src=\"https:\/\/gdprinformer.com\/wp-content\/uploads\/2017\/09\/fraud-laptop.jpg\" width=\"800\"><\/img>\n\n\n\n<br>","71d47e81":"## <a id='21'>Distribution of  Card features across Train and Test<\/a> ","e2ea01c4":"Clearly the distribution is a lot different. I shall eliminate dist2.","70c8f205":"# <a id='5'>Feature Engineering<\/a> ","61b5adda":"Same as done with other features. All the distributions are similar in test and train. So no ID column is dropped.","e5a9cf4e":"## <a id='23'>Distribution of C features across Train and Test<\/a> ","db8dfb30":"C10 can be useful for distinction. But its distribution in test and train is very different as well. Same in case of C9. But I think it can be definitely dropped because its distribution in case of fraud and nonfraud is almost same.","01f95c71":"# <a id='0'>Content<\/a>\n\n- <a href='#1'>Importing Libraries and Modules<\/a>  \n- <a href='#2'>Distribution of features across Train and Test<\/a>\n - <a href='#21'>Card Features<\/a>   \n  - <a href='#22'>Addr,Dist and EmailDomain<\/a>  \n  - <a href='#23'>C Features<\/a> \n  - <a href='#24'>D Features<\/a> \n  - <a href='#25'>M features<\/a> \n  - <a href='#26'>V Features<\/a>   \n  - <a href='#27'>ID features<\/a> \n- <a href='#3'>Dropping Columns<\/a>   \n- <a href='#4'>Reducing Memory Size<\/a>    \n- <a href='#5'>Feature Engineering<\/a>     \n- <a href='#6'>Model Development<\/a>\n- <a href='#7'>Feature Importance<\/a>\n\n\n**Please upvote if you like the kernel . Happy Kaggling and all the best for the competition**","90797c9e":"## <a id='24'>Distribution of D features across Train and Test<\/a> ","c5459139":"## <a id='26'>Distribution of V features across Train and Test<\/a> ","c512dae1":"## <a id='27'>Distribution of ID features across Train and Test<\/a> ","94275af6":"Randomly generating some features. Very illogical I know :P","828005d8":"## <a id='25'>Distribution of M features across Train and Test<\/a> "}}