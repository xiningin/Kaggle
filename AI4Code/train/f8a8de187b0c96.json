{"cell_type":{"16a96d0e":"code","32effbe5":"code","fb791b2e":"code","a6996b5e":"code","98b80766":"code","450345b1":"code","c3bfaa33":"code","b5f16e77":"code","0cd1fad4":"code","e2427ba5":"code","ed1dbb89":"code","e1b61bd3":"code","191d2de0":"code","3641c6de":"code","46b38d88":"code","8eef4811":"code","46f54645":"code","6a81bc6c":"code","9e1d1452":"code","2c79966d":"code","374c5ea8":"code","5bd23a5d":"code","ccceb14b":"code","d8dc63a1":"code","edc9041f":"code","b94ae333":"code","18ba11ea":"code","d646b7f4":"code","1a1c66f4":"code","b7b24af4":"code","583564dc":"code","19313141":"code","4610c0de":"code","36ffe6da":"markdown","6c317601":"markdown","e7dbf393":"markdown","4c3f0409":"markdown","4b061bb7":"markdown","b11adcc3":"markdown","ac57732a":"markdown"},"source":{"16a96d0e":"!pip install -qqq lyft-dataset-sdk","32effbe5":"import os\nimport math\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport matplotlib as mpl\n\nfrom lyft_dataset_sdk.lyftdataset import LyftDataset\nfrom lyft_dataset_sdk.utils.data_classes import LidarPointCloud","fb791b2e":"INP_DIR = '\/kaggle\/input\/3d-object-detection-for-autonomous-vehicles\/'","a6996b5e":"# Load the dataset\n# Adjust the dataroot parameter below to point to your local dataset path.\n# The correct dataset path contains at least the following four folders (or similar): images, lidar, maps, v1.0.1-train\n!ln -s {INP_DIR}\/train_images images\n!ln -s {INP_DIR}\/train_maps maps\n!ln -s {INP_DIR}\/train_lidar lidar","98b80766":"level5data = LyftDataset(\n    data_path='.',\n    json_path=os.path.join(INP_DIR + 'train_data'),\n    verbose=False\n)","450345b1":"my_scene = level5data.scene[0]","c3bfaa33":"my_sample_token = my_scene[\"first_sample_token\"]","b5f16e77":"my_sample = level5data.get('sample', my_sample_token)","0cd1fad4":"lidar_top_data_token = my_sample['data']['LIDAR_TOP']","e2427ba5":"lidar_top_data = level5data.get('sample_data', lidar_top_data_token)","ed1dbb89":"# lidar_top_ego_pose_token = lidar_top_data['ego_pose_token']","e1b61bd3":"# # car \/ lidar top coords\n# lidar_top_ego_pose_data = level5data.get('ego_pose', lidar_top_ego_pose_token)\n# lidar_top_coords = np.array(lidar_top_ego_pose_data['translation'])","191d2de0":"def get_coords_from_ann_idx(ann_idx):\n    return np.array(level5data.get('sample_annotation', my_sample['anns'][ann_idx])['translation'])","3641c6de":"anns_inds_to_show = [13, 27, 41, 42, 56] # i select several cars near lyft's pod\nann_tokens = []\nfor ind in anns_inds_to_show:\n    my_annotation_token = my_sample['anns'][ind]\n    print(f'{ind}: {my_annotation_token}')\n    ann_tokens.append(my_annotation_token)\n    level5data.render_annotation(my_annotation_token)\n    plt.show()","46b38d88":"my_sample['data'].keys() # available sensors for sampling","8eef4811":"# here we get all objects (bboxes) for selected sensor and filter by selected cars (above)\nret_sampled = level5data.get_sample_data(lidar_top_data_token, selected_anntokens=ann_tokens)[1]\n# we can not pass selected_anntokens and get full info of the bboxes around \n# car - literally a complete set of data for training a neural network","46f54645":"def get_data_from_sample(chanel_to_get):\n    return level5data.get('sample_data', my_sample['data'][chanel_to_get])","6a81bc6c":"def show_img_from_data(data):\n    plt.imshow(\n        cv2.cvtColor(\n            cv2.imread(data['filename']),\n            cv2.COLOR_BGR2RGB\n        )\n    );","9e1d1452":"lidar_channel = 'LIDAR_TOP'\ncamera1_chanel = 'CAM_BACK'\ncamera2_chanel = 'CAM_BACK_LEFT'\ncamera3_chanel = 'CAM_FRONT'\nlidar_data = get_data_from_sample(lidar_channel)\ncamera1_data = get_data_from_sample(camera1_chanel)\ncamera2_data = get_data_from_sample(camera2_chanel)\ncamera3_data = get_data_from_sample(camera3_chanel)","2c79966d":"# car 1 on back side\nshow_img_from_data(camera1_data);","374c5ea8":"# car 2 on back side\nshow_img_from_data(camera2_data);","5bd23a5d":"# car 3 on front side\nshow_img_from_data(camera3_data);","ccceb14b":"pc = LidarPointCloud.from_file(Path(lidar_data['filename']))","d8dc63a1":"plot_offset = 15\nplt.xlim(-plot_offset, plot_offset)\nplt.ylim(-plot_offset, plot_offset)\n\n# plot PointCloud\nplt.scatter(pc.points[0, :], pc.points[1, :], c=pc.points[2, :], s=0.1);\n# plot center of cars \/ bboxes\nfor cur_point_idx in range(len(ret_sampled)):\n    crds = ret_sampled[cur_point_idx].center\n    plt.scatter(crds[0], crds[1], c='red');","edc9041f":"pc.points[:, 0]","b94ae333":"(pc.points[3, :]==100).all()","18ba11ea":"sample_to_explore = ret_sampled[4]","d646b7f4":"sample_to_explore","1a1c66f4":"# rotation in radians of bbox: x, y and z. For degrees see example above (next MPL Figure)\nsample_to_explore.orientation.yaw_pitch_roll","b7b24af4":"# Width, leght and height of bbox: x, y and z\nsample_to_explore.wlh","583564dc":"# and of course coords of bbox \nsample_to_explore.center","19313141":"# class of object\nsample_to_explore.name","4610c0de":"fig = plt.figure()\nax = fig.add_subplot(111)\n\nplot_offset = 15\nplt.xlim(-plot_offset, plot_offset)\nplt.ylim(-plot_offset, plot_offset)\n\n# plot PointCloud\nax.scatter(pc.points[0, :], pc.points[1, :], c=pc.points[2, :], s=0.1);\n\n# plot center of cars \/ bboxes\ncrds = sample_to_explore.center\nax.scatter(crds[0], crds[1], c='red');\n\nw, l, h  = sample_to_explore.wlh\nangles_to_rotate = sample_to_explore.orientation.yaw_pitch_roll\n\nrect_angle = math.degrees(angles_to_rotate[0]) # radians to degrees\nmpl_rotate = (\n    mpl\n    .transforms\n    .Affine2D()\n    .rotate_deg_around(crds[0], crds[1], rect_angle)\n    + ax.transData\n)\n\nrect = patches.Rectangle(\n    (crds[0] - l \/ 2, crds[1] - w \/ 2), l, w, fill=False,\n)\n\nrect.set_transform(mpl_rotate)\n\nax.add_patch(rect);","36ffe6da":"This code shows how to get points from .bin files in Dataset to train your own neural network.","6c317601":"Here we get ego centered coordinates of bbox, rotation and etc:","e7dbf393":"Here we see car silhouette.","4c3f0409":"Dive into data.\nPoints have x, y and z coords and intensity - always 100.","4b061bb7":"Later we create pytorch dataloader, which provide all data for NN!","b11adcc3":"Now we have all ingridients!","ac57732a":"# Let's take a look at the cars around us"}}