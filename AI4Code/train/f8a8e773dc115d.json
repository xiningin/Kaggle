{"cell_type":{"84b1c788":"code","40bdf8d2":"code","b8a50c5d":"code","557ec81e":"code","b0b5664b":"code","c0905a40":"code","c13ab035":"code","fba1b117":"code","a3e334eb":"code","abc50abc":"code","fe4f2801":"code","1030b770":"code","4b75e19a":"code","27b9fece":"code","64f95886":"code","e6a9d533":"code","ebda664a":"code","dfe65d20":"code","a51f0ec0":"code","d66c6ac4":"code","04e3d31f":"code","b338fd08":"code","8ae890ab":"code","f37e16a0":"code","5e083b16":"code","05fa5bd9":"code","879b9bbd":"code","22252aa8":"code","eef59da2":"code","b8f8cc2b":"code","599b2380":"code","adf68348":"code","af397de8":"code","bd6e3a29":"code","f50d8c99":"code","b4fa26ad":"code","d747dbd2":"code","41257a80":"code","5fafa69c":"code","969d7880":"code","0a4811e1":"code","e58ebcd4":"code","4f427dd2":"code","a411f8e7":"code","edf5a5c3":"code","1b34dd1a":"code","827fed48":"code","706edd24":"code","7387cf57":"code","8a4a2014":"code","f986bb0a":"code","1b2c9e8c":"code","2dff3a65":"code","c5250123":"code","45331e2a":"code","544b71f9":"code","8726c870":"code","27cb379e":"code","f63272b4":"code","c86a7c00":"code","8d260f90":"code","cba14909":"code","00c5f579":"code","e9f66149":"code","9a274684":"code","bfe17f8c":"code","e75923fc":"code","485caf3c":"code","7ffc6e4e":"code","29321522":"code","c3dcc5dd":"markdown","8f2eaf26":"markdown","8d59d33f":"markdown","01711006":"markdown","f32d748f":"markdown","026041d5":"markdown","3f1b8d6b":"markdown","f21953b8":"markdown","60cf9a73":"markdown","8749c60e":"markdown","7139544a":"markdown","d3d1e514":"markdown","b8a1ccb7":"markdown","44cc0294":"markdown","cf5b1f26":"markdown","4143a5ec":"markdown","a070b077":"markdown","0fa6f46c":"markdown","44d18b14":"markdown","9ba59181":"markdown","276c8e06":"markdown","df7f7620":"markdown","9bbd4d13":"markdown","3caaac9c":"markdown","bb93e79d":"markdown","53c3c857":"markdown","d5890def":"markdown","1e54549a":"markdown","8dd273dd":"markdown","d9fbe013":"markdown","7facb1eb":"markdown","9323a7cf":"markdown","a5e97a49":"markdown","5d246144":"markdown","e8d74813":"markdown","7643de0e":"markdown","d896b07a":"markdown","30675198":"markdown","9b9e2726":"markdown","0882d021":"markdown","9e858c16":"markdown","43d9da06":"markdown","8deec26e":"markdown","ae7d9de3":"markdown","bfcc9155":"markdown","20fd24a0":"markdown","a1740abe":"markdown"},"source":{"84b1c788":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.image as mpimg\n\n# Data Preprocessing:\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import plot_tree\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report,confusion_matrix,roc_curve, roc_auc_score, accuracy_score\n\n# Models:\n\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import StackingClassifier\nfrom xgboost import XGBClassifier\n\n# A model that I learned by myself: CatBoost + Plotly\n\nfrom catboost import CatBoostClassifier\nimport plotly.graph_objects as go\nimport plotly.io as pio\nimport plotly.express as px\n\n# offline (for plotly)\n\nimport plotly.offline as pyo\n\n# Clustering:\n\nfrom sklearn.cluster import KMeans\n\n# PCA:\n\nfrom sklearn.decomposition import PCA\n\n# ICA:\n\nfrom sklearn.decomposition import FastICA\n\n# Scaling:\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n\n# Cross Validation:\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV","40bdf8d2":"# training df:\n\ndf_train = pd.read_csv(\"..\/input\/fashionmnist\/fashion-mnist_train.csv\") # csv pandas df\n\ndf_train.sample(n = 4, random_state = 8).sort_values(by = 'label')","b8a50c5d":"# testing df: we will not train our model on this df, we will test out model on it at the end of the notebook\n\ndf_test = pd.read_csv(\"..\/input\/fashionmnist\/fashion-mnist_test.csv\") # csv pandas df\n\ndf_test.sample(n = 4, random_state = 8).sort_values(by = 'label')","557ec81e":"df_train.shape # 60000 rows and 785 columns (784 pixels + label column)","b0b5664b":"df_test.shape # 10000 rows and 785 columns (784 pixels + label column) - test","c0905a40":"df_train.describe()","c13ab035":"df_train[df_train['label']==3].describe() # 3 = Dress","fba1b117":"df_train[df_train['label']==9].describe() # 9 = Ankle boot ","a3e334eb":"df_train.isnull().sum().sum() # the dataset has no NaN values","abc50abc":"df_train.label.unique() # we have 0-9 labels: 10 labels","fe4f2801":"cor = df_train.corr() # it calculates the correation between each two features","1030b770":"cor_target = abs(cor[\"label\"])\n\n# selecting highly correlated features\n\nrelevant_features = cor_target[cor_target > 0.5] # shows only corr with 0.5 or higher value\nrelevant_features","4b75e19a":"# 105 features is nice(106 - label), but I would like to know about higher corr than 0.5\n\nfor i in range(51):\n        print(\"Higher than\",round(0.5 + i*0.01, 2), \"correlation: # of Pixels:\",len(cor_target[cor_target > (0.5 + i*0.01)])-1)\n        if len(cor_target[cor_target > (0.5 + i*0.01)]) == 1:\n            break","27b9fece":"target = df_train['label'] # the feature we would like to predict, the label of picture\ndata = df_train.drop(['label'], axis = 1) # we will drop y from x, because we want to predict it","64f95886":"# we will split our testing dataset into data & target. We can't train this dataset. we will use it as a X_test & y_test\n\ntest_labels = df_test['label'] # the feature we would like to predict, the label of picture\ntest = df_test.drop(['label'], axis = 1) # we will drop y from x, because we want to predict it","e6a9d533":"# A function to show the labels\ndef num_to_name(label):\n    labeled = label.copy()\n    mapping = {0 :'T-Shirt\/Top',\n    1 :'Trouser',\n    2 :'Pullover',\n    3 :'Dress',\n    4 :'Coat',\n    5 :'Sandal',\n    6 :'Shirt',\n    7 :'Sneaker',\n    8 :'Bag',\n    9 :'Ankle Boot'}\n    labeled = label.map(mapping)\n    return labeled","ebda664a":"# exaple of pictures with their correct label\n\nfig, axes = plt.subplots(4, 4, figsize = (12,12))\naxes = axes.ravel()\n\nfor i in range(16):\n    axes[i].imshow(data.values.reshape((data.shape[0], 28, 28))[i], cmap=plt.get_cmap('binary'))\n    axes[i].set_title(\"Outfit \" + str(target[i]) + \": \"+ num_to_name(target)[i])\n    axes[i].axis('off')\nplt.show()","dfe65d20":"target.value_counts() # how much exapmles we have from each label","a51f0ec0":"# a simple counter graph for it\n\nplt.subplots(figsize = (15,5))\nplt.title(\"Outfits Counter\", size=20)\nfig = sns.countplot(num_to_name(target))","d66c6ac4":"X_train, X_val, y_train, y_val = train_test_split(data, target, test_size=0.2, random_state=18)\nX_test = test.copy() # since we already split test_df into data and labels, we cant do more actions on it","04e3d31f":"# important for infinty values cases\n\nX_train = X_train.astype(np.float32)\nX_val = X_val.astype(np.float32)\nX_test = X_test.astype(np.float32)","b338fd08":"X_train = X_train \/ 255\nX_val = X_val \/ 255\nX_test = X_test \/ 255","8ae890ab":"pca = PCA() # all 784 features\npca.fit(X_train)","f37e16a0":"# A graph to present the conection between the num of features & the explained variance: \nexp_var_cumul = np.cumsum(pca.explained_variance_ratio_)\n\nfig = px.area(\n    title = \"Explained variance as a function of the number of dimensions:\",\n    x=range(1, exp_var_cumul.shape[0] + 1),\n    y=exp_var_cumul * 100,\n    labels={\"x\": \"# of Pixels\", \"y\": \"Explained Variance\"},\n    width = 1000 ,\n    height = 500\n)\n\nfig.show()","5e083b16":"pca = PCA(n_components=0.80) # we can try using svd_solver=\"randomized\"\nX_train_reduced = pca.fit_transform(X_train)\nX_val_reduced = pca.transform(X_val)\npca.n_components_","05fa5bd9":"# A three-dimensional graph depicting the way our data is interpreted, plotly does it easily for us \ntotal_var = pca.explained_variance_ratio_.sum() * 100\nfig = px.scatter_3d(\n    X_train_reduced, x=0, y=1, z=2, color = num_to_name(y_train),\n    title=f'Total Explained Variance: {total_var:.2f}%',\n    labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'}\n)\nfig.show()","879b9bbd":"# 2D version: with x and y\ntotal_var = pca.explained_variance_ratio_.sum() * 100\nfig = px.scatter(\n    X_train_reduced, x=0, y=1, color = num_to_name(y_train),\n    title=f'Total Explained Variance: {total_var:.2f}%',\n    labels={'0': 'PC 1', '1': 'PC 2'}\n)\nfig.show()","22252aa8":"# 2D version: with y and z\ntotal_var = pca.explained_variance_ratio_.sum() * 100\nfig = px.scatter(\n    X_train_reduced, x=1, y=2, color = num_to_name(y_train),\n    title=f'Total Explained Variance: {total_var:.2f}%',\n    labels={'0': 'PC 1', '1': 'PC 2'}\n)\nfig.show()","eef59da2":"# 2D version: with z and y\ntotal_var = pca.explained_variance_ratio_.sum() * 100\nfig = px.scatter(\n    X_train_reduced, x=2, y=1, color = num_to_name(y_train),\n    title=f'Total Explained Variance: {total_var:.2f}%',\n    labels={'0': 'PC 1', '1': 'PC 2'}\n)\nfig.show()","b8f8cc2b":"plt.figure(figsize=(30,20))\n\nfor i in range(3):\n    plt.subplot(4,5,i+1)\n    plt.title(\"Outfit \" + str(y_train[i]) + \": \"+ num_to_name(y_train)[i], size = 24)\n    plt.imshow(X_train_reduced[i].reshape(6,4))\n\nplt.show;","599b2380":"ica = FastICA(n_components=24, random_state=18) # I took the results from PCA and applied them on ICA (24 pixels)\nX_train_reduced = ica.fit_transform(X_train) # fit ica on train\nX_val_reduced = ica.transform(X_val) # aplly ica on validation\nX_train_reduced.shape","adf68348":"# we can see that ica uses a completely different way to change the pictures\n\nplt.figure(figsize=(30,20))\n\nfor i in range(3):\n    plt.subplot(4,5,i+1)\n    plt.title(\"Outfit \" + str(y_train[i]) + \": \"+ num_to_name(y_train)[i], size = 24)\n    plt.imshow(X_train_reduced[i].reshape(6,4))\n\nplt.show;","af397de8":"X_train = pd.DataFrame(X_train_reduced)\nX_val = pd.DataFrame(X_val_reduced)","bd6e3a29":"bayes = GaussianNB()\nbayes.fit(X_train, y_train)\nbayes","f50d8c99":"y_pred = bayes.predict(X_val)\nbayes_acc = accuracy_score(y_val, y_pred)\nbayes_acc","b4fa26ad":"print (classification_report(y_val, y_pred))","d747dbd2":"knn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\nknn","41257a80":"y_pred = knn.predict(X_val)\nknn_acc = accuracy_score(y_val, y_pred)\nknn_acc","5fafa69c":"print (classification_report(y_val, y_pred))","969d7880":"lr = LogisticRegression(solver = 'lbfgs')\nlr.fit(X_train, y_train)\nlr","0a4811e1":"y_pred = lr.predict(X_val)\nlr_acc = accuracy_score(y_val, y_pred)\nlr_acc","e58ebcd4":"print (classification_report(y_val, y_pred))","4f427dd2":"cat = CatBoostClassifier(logging_level='Silent')\ncat.fit(X_train, y_train)\ncat","a411f8e7":"y_pred = cat.predict(X_val)\ny_pred_cat = y_pred\ncat_acc = accuracy_score(y_val, y_pred)\ncat_acc","edf5a5c3":"print (classification_report(y_val, y_pred))","1b34dd1a":"rfc = RandomForestClassifier(n_estimators=10)\nada = AdaBoostClassifier(n_estimators=100,learning_rate= 0.1, base_estimator=rfc)\nada.fit(X_train, y_train)\nada","827fed48":"y_pred = ada.predict(X_val)\nada_acc = accuracy_score(y_val, y_pred)\nada_acc","706edd24":"print (classification_report(y_val, y_pred))","7387cf57":"xgb = XGBClassifier(use_label_encoder =False)\nxgb.fit(X_train, y_train)\nxgb","8a4a2014":"y_pred = xgb.predict(X_val)\nxgb_acc = accuracy_score(y_val, y_pred)\nxgb_acc","f986bb0a":"print (classification_report(y_val, y_pred))","1b2c9e8c":"rf = RandomForestClassifier()\nrf.fit(X_train, y_train)\nrf","2dff3a65":"y_pred = rf.predict(X_val)\nrf_acc = accuracy_score(y_val, y_pred)\nrf_acc","c5250123":"print (classification_report(y_val, y_pred))","45331e2a":"clf1 = xgb\nclf2 = knn\nclf3 = rf\nclf4 = cat","544b71f9":"hv = VotingClassifier(estimators=[\n        ('xgb', clf1), ('knn', clf2), ('rf', clf3)], voting='hard')\nhv.fit(X_train, y_train)\nhv","8726c870":"y_pred = hv.predict(X_val)\nhv_acc = accuracy_score(y_val, y_pred)\nhv_acc","27cb379e":"print (classification_report(y_val, y_pred))","f63272b4":"sv = VotingClassifier(estimators=[\n        ('xgb', clf1), ('cat', clf4), ('rf', clf3)], voting='soft', weights=[1,3,1])\nsv.fit(X_train, y_train)\nsv","c86a7c00":"y_pred = sv.predict(X_val)\ny_pred_sv = y_pred.copy()\nsv_acc = accuracy_score(y_val, y_pred)\nsv_acc","8d260f90":"print (classification_report(y_val, y_pred))","cba14909":"acc_list = {'Model':  ['Naive Bayes', 'KNN','Logistic Regression','CatBoost', 'AdaBoost', 'XGBoost','Random Forest','Hard Voting', 'Soft Voting'],\n        'Accuracy': [bayes_acc,knn_acc,lr_acc,cat_acc,ada_acc,xgb_acc,rf_acc,hv_acc,sv_acc],\n        }","00c5f579":"fig = go.Figure(data=[\n    go.Bar(name='train set', x=acc_list['Model'], y=acc_list['Accuracy'],text=np.round(acc_list['Accuracy'],2),textposition='outside'),\n])\nfig.update_layout(barmode='group',title_text='Accuracy Comparison On Different Models',yaxis=dict(\n        title='Accuracy'))\nfig.show()","e9f66149":"cm = confusion_matrix(y_val, y_pred_cat)\nplt.figure(figsize=(8,8))\nplt.imshow(cm, interpolation='nearest', cmap = plt.cm.coolwarm)\nplt.title('Confusion matrix for CatBoost', size = 15)\nplt.colorbar()\ntick_marks = np.arange(10)\nplt.xticks(tick_marks, ['T-shirt\/top', 'Trouser', 'Pullover', \n                        'Dress', 'Coat', 'Sandal', 'Shirt', \n                        'Sneaker', 'Bag', 'Ankle boot'], rotation=45, size = 10)\nplt.yticks(tick_marks, ['T-shirt\/top', 'Trouser', 'Pullover', \n                        'Dress', 'Coat', 'Sandal', 'Shirt', \n                        'Sneaker', 'Bag', 'Ankle boot'], size = 10)\nplt.tight_layout()\nplt.ylabel('Actual label', size = 15)\nplt.xlabel('Predicted label', size = 15)\nwidth, height = cm.shape\nfor x in range(width):\n    for y in range(height):\n        plt.annotate(str(cm[x][y]), xy=(y, x), \n        horizontalalignment='center',\n        verticalalignment='center')","9a274684":"# we will concat our X,y train for cross validation for our best model - catboost\n\nX = pd.concat([X_train, X_val], axis = 0)\ny = pd.concat([y_train, y_val], axis = 0)\n\n# now the dataset is united again, we can split it 5 time for cv score\n\npipe = Pipeline(steps=[('CatBoost', cat)])\nsearch = GridSearchCV(pipe, param_grid={'CatBoost__iterations':[500, 750, 1000]}, cv=5) # 5 cv on iterations\nsearch.fit(X, y) # we will apply cv on our whole data because we would like our model to see more examples\nprint(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\nprint(search.best_params_)","bfe17f8c":"X_test = ica.transform(X_test) # aplly ica on X_test\ny_test = test_labels.copy()","e75923fc":"y_pred = search.predict(X_test) # search = catboost after cv","485caf3c":"print (classification_report(y_test, y_pred))","7ffc6e4e":"accuracy_score(y_test, y_pred)","29321522":"cm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(8,8))\nplt.imshow(cm, interpolation='nearest', cmap = plt.cm.coolwarm)\nplt.title('Confusion matrix', size = 15)\nplt.colorbar()\ntick_marks = np.arange(10)\nplt.xticks(tick_marks, ['T-shirt\/top', 'Trouser', 'Pullover', \n                        'Dress', 'Coat', 'Sandal', 'Shirt', \n                        'Sneaker', 'Bag', 'Ankle boot'], rotation=45, size = 10)\nplt.yticks(tick_marks, ['T-shirt\/top', 'Trouser', 'Pullover', \n                        'Dress', 'Coat', 'Sandal', 'Shirt', \n                        'Sneaker', 'Bag', 'Ankle boot'], size = 10)\nplt.tight_layout()\nplt.ylabel('Actual label', size = 15)\nplt.xlabel('Predicted label', size = 15)\nwidth, height = cm.shape\nfor x in range(width):\n    for y in range(height):\n        plt.annotate(str(cm[x][y]), xy=(y, x), \n        horizontalalignment='center',\n        verticalalignment='center')","c3dcc5dd":"##### We can present some of our new pics after the transfromation to 24 dimentionds:","8f2eaf26":"##### Since the feature that interests us is the label, we would like to see only the correlation between each feature and the label.","8d59d33f":"# Cross Validation by Pipeline & Grid Search","01711006":"The things that are important to consider before start to work with the data:\n- As we can see, when a pixel value is 0 (black pixel), it means that its an empty pixel. We would like to drop pixels which are empty in most of the pictures.\n- We will consider choosing to use one of the following methods in order to make our model more compact: PCA and K-means. Note that in the PCA method we would like to use as few pixels as possible in order to label the item of clothing. When using the K-Means method, we want to use as few colors as possible in the classification process.\n- We would like to understand which labels are more difficult to classify, and which are easier. We may consider using a more complex model in cases where the differences between two labels are the most minor and requires a maximum of data. On the other hand, when an item is easy to identify, less data is required to classify it.","f32d748f":"## Logistic Regression","026041d5":"# Fashion Mnist\n#####  @ Haim Goldfisher","3f1b8d6b":"##### After reducing the num of the features, we will convert X_train & X_validation to pandas data frames:","f21953b8":"### Conclusion:","60cf9a73":"# Dimensionality Reduction","8749c60e":"# ICA (Independent Component Analysis)","7139544a":"##### Due to Guessian Naive Bayes results, our model must be better than 74.5% of success on our validation test.  ","d3d1e514":"Soft Voting\/Majority Rule classifier for unfitted estimators.","b8a1ccb7":"## KNN","44cc0294":"### Scaling: ","cf5b1f26":"Independent Component Analysis (ICA) is a technique of array processing and data analysis, aiming at recovering unobserved signals or \u2018sources\u2019 from observed mixtures, exploiting only the assumption of mutual independence between the signals. The separation of the sources by ICA has great potential in applications such as the separation of sound signals (like voices mixed in simultaneous multiple records, for example), in telecommunication or in the treatment of medical signals. However, ICA is not yet often used by statisticians. While the goal in PCA is to find an orthogonal linear transformation that maximizes the variance of the variables, the goal of ICA is to find the linear transformation, which the basis vectors are statistically independent and non-Gaussian\n\nFor more info about how ICA is different from PCA: http:\/\/www2.hawaii.edu\/~kyungim\/papers\/baek_cvprip02.pdf","4143a5ec":"## Random Forest","a070b077":"## Data Processing","0fa6f46c":"##### As you can see, each label can be taken on its own. It is also possible to notice that in general, there are some differences between the labels.","44d18b14":"## AdaBoost","9ba59181":"##### In summary, it can be seen that there are around 36 pixels that have a significant effect on the label of each image (more than 0.6). We will consider using this value for the amount of pixels we will take from each image. On the other hand, it will be interesting to see if dimensionality reduction algorithms (PCA or ICA) will give us the same amount of features.","276c8e06":"Principal Component Analysis (PCA) is a classical technique in statistical data analysis, feature extraction and data reduction, aiming at explaining observed signals as a linear combination of orthogonal principal components.","df7f7620":"### Soft Voting:","9bbd4d13":"## XGBoost","3caaac9c":"# Testing our best model on test df:","bb93e79d":"## To sum up, our CatBoost Model did 87.6% of success on the testing dataset. By using ICA, we used 24 out of 784 pixels to achieve this result.","53c3c857":"# Models","d5890def":"##### We would like to find our optimal n_components value ","1e54549a":"##### An open-source software library developed by Yandex. It provides a Gradient Boosting framework which attempts to solve for Categorical features using a permutation driven alternative compared to the classical algorithm. For more information: https:\/\/catboost.ai\/","8dd273dd":"##### The data is very balanced in terms of the amount of samples we have from each label. We will conclude unequivocally that a dummy classifier model will give us a 10% success rate for ten labels. This is a very low percentage that will also be very easy to pass.","d9fbe013":"##### We can see that by taking only 24 features, we stay with 80% of explained variance. ","7facb1eb":"# PCA (Principal Component Analysis)","9323a7cf":"### Hard Voting:","a5e97a49":"### Training \/ Testing Split:","5d246144":"##### We will use some of the models we have learned trought this year to predict the type of the outfit. We would like to use the three best model to create a voting model.","e8d74813":"![image.png](attachment:image.png)","7643de0e":"After making our model from the train dataset, we would like to figure out if our model will succeed with new data that it never seen before. ","d896b07a":"##### I chose to use ICA instead of PCA because the pictures in this dataset are very neat (as opposed to a dataset of dogs vs cats). ICA algorithm is faster and I think that in this specific case, using a simpler algorithm will actually give better results, and more importantly, in less time. Also, we have already received from PCA's analysis the amount of pixels needed, so I will just put 24 pixels into the ICA algorithm.","30675198":"##### This model is extremly simple. We will use it as our dummy model. Dummy Classifier should do 10% of success in a multi class classification of 10 equal classes.","9b9e2726":"# The Best Model is CatBoost, with  87.5% of success, by using only 24 pixels.","0882d021":"## CatBoost","9e858c16":"##### Now we would like to take our top 3 models and mix  them into a voting model:","43d9da06":"As we can see, we have the label - the type of cloth, and 0-255 value for each pixel in the frame.","8deec26e":"## Voting","ae7d9de3":"##### There are 10 labels, it means that there are 10 different types of clothing to be classified:\n\n* 0 - T-Shirt\/Top\n* 1 - Trouser\n* 2 - Pullover\n* 3 - Dress\n* 4 - Coat\n* 5 - Sandal\n* 6 - Shirt\n* 7 - Sneaker\n* 8 - Bag\n* 9 - Ankle Boot","bfcc9155":"##### It is clear that our attempt to unify the models was unsuccessful. We will continue with CatBoost","20fd24a0":"## Naive Bayes","a1740abe":"##### Since we have 256 pixels (0-255), we can divide them into 255 in order to achieve the desired range values (0-1)."}}