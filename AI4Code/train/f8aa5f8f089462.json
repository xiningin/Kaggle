{"cell_type":{"a6f04a58":"code","4cdc9407":"code","95003eb4":"code","ac814847":"code","20d5089a":"code","d6515ff6":"code","cfdb502a":"code","d3f87834":"code","31399f05":"code","8f15836d":"code","ad261838":"code","a6bc3b62":"code","b7520b3e":"code","71b202d4":"code","d8b666ef":"code","c54838b7":"markdown","2ec671b9":"markdown","65bb189f":"markdown","a1663857":"markdown","7ce5fffb":"markdown","3107c3c7":"markdown","2d7ce50e":"markdown","11d1d1b2":"markdown","77d1e52b":"markdown","75d967cf":"markdown","5079c6e3":"markdown","b1ed193f":"markdown"},"source":{"a6f04a58":"import os\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nimport math\nimport sys\nfrom scipy import optimize\n%matplotlib inline","4cdc9407":"csv = '..\/input\/digit-recognizer\/train.csv'\ndata = pd.read_csv(csv)","95003eb4":"data","ac814847":"display = np.matrix(data)\noutput = display[:, 0]\ndisplay = np.delete(display, 0, 1)\nm = output.size\nrand_indices = np.random.choice(m, 5, replace=False)\nfor i in rand_indices:\n    img = display[i].reshape(28,28)\n    plt.figure()\n    plt.imshow(img, cmap=\"gray\")","20d5089a":"display.shape","d6515ff6":"input_layer_size  = 784  \nhidden_layer_size = 50   \nnum_labels = 10 ","cfdb502a":"def sigmoid(z):\n    return 1.0 \/ (1.0 + np.exp(-z))\n\ndef sigmoidGradient(z):\n    g = np.zeros(z.shape)\n    s = sigmoid(z)\n    g = np.multiply(s,(1-s))\n    \n    return g","d3f87834":"def randInitializeWeights(L_in, L_out):\n    \n    epsilon_init = 0.12\n    \n    #adding a bias layer\n    W = np.zeros((L_out, 1 + L_in))\n    \n    #initializing weights\n    W = np.random.rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init\n    print(W.shape)\n    return W","31399f05":"print('Initializing Neural Network Parameters ...')\n\ninitial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size)\ninitial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels)\n\ninitial_nn_params = np.concatenate([initial_Theta1.ravel(), initial_Theta2.ravel()], axis=0)","8f15836d":"def nnCostFunction(nn_params, input_layer_size, hidden_layer_size, X, y, num_labels, lamb =0.0):\n    \"\"\"\n    Implements the neural network regularized cost function and gradient for a two layer neural \n    network which performs classification. \n    Feedforward the neural network and return the cost in the variable J. Implement the backpropagation algorithm to compute the gradients\n    Theta1_grad and Theta2_grad. Implement regularization with the cost function and gradients.\n    \n    \"\"\"\n    \n    #number of examples in set\n    m = y.size\n    \n    Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n                        (hidden_layer_size, (input_layer_size + 1)))\n\n    Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n                        (num_labels, (hidden_layer_size + 1)))\n         \n    J = 0\n    Theta1_grad = np.zeros(Theta1.shape)\n    Theta2_grad = np.zeros(Theta2.shape)\n\n    X = np.concatenate([np.ones((m, 1)), X], axis=1)\n    \n    #creating necessary yk array\n    #if in 4th example, y = 9 then set 9th row, 4th column of yk as 1\n    #similarly do for all examples\n    yk = np.zeros((num_labels, m))\n    for i in range(0,m):\n        yk[y[i],i] = 1\n    \n    \n    '''\n    Forward Propagation\n    '''\n    \n    #first layer\n    z2 = Theta1@X.T\n    a2 = sigmoid(z2)\n    a2 = np.concatenate([np.ones((1,m)), a2], axis=0)\n    \n    \n    #second layer\n    z3 = Theta2@a2\n    a3 = sigmoid(z3)\n    \n    #to not account for bias in regularization\n    t1 = Theta1[:,1:]\n    t2 = Theta2[:,1:]   \n    \n    #regularized cost\n    J = 1\/m * np.sum(np.sum(-np.multiply(yk,np.log(a3)) - np.multiply((1-yk),np.log(1-a3)))) + lamb\/(2*m) * (np.sum(np.sum(t1**2))+np.sum(np.sum(t2**2)))\n\n    \n    '''\n    Backward Propapgation\n    '''\n    \n    d3 = a3 - yk\n    \n    z2 = np.concatenate([np.ones((1,m)), z2], axis=0)\n    d2 = np.multiply(Theta2.T@d3, sigmoidGradient(z2))\n    \n    Theta2_grad = Theta2_grad + d3@a2.T\n    Theta1_grad = Theta1_grad + d2[1:]@X\n    \n    \n    Theta1_grad[:,0] = np.divide(Theta1_grad[:,0],m)\n    Theta1_grad[:,1:] = np.divide(Theta1_grad[:,1:],m) + (lamb\/m)*Theta1[:,1:]\n    \n    Theta2_grad[:,0] = np.divide(Theta2_grad[:,0],m)\n    Theta2_grad[:,1:] = np.divide(Theta2_grad[:,1:],m)+ (lamb\/m)*Theta2[:,1:]\n    \n    r1 = Theta1_grad.ravel()\n    r2 = Theta2_grad.ravel()\n    \n    grad = np.concatenate([r1.T,r2.T])\n    \n    return J, grad","ad261838":"options= {'maxiter': 400}\n\nlambda_ = 9\n\ncostFunction = lambda p: nnCostFunction(p, \n                                        input_layer_size, hidden_layer_size, display, output, num_labels, lambda_)\n\nres = optimize.minimize(costFunction,\n                        initial_nn_params,\n                        jac=True,\n                        method='TNC',\n                        options=options)\n\nnn_params = res.x\n        \nTheta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n                    (hidden_layer_size, (input_layer_size + 1)))\n\nTheta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n                    (num_labels, (hidden_layer_size + 1)))","a6bc3b62":"def predict(Theta1, Theta2, X):\n    \n    m = X.shape[0]\n    num_labels = Theta2.shape[0]\n\n    p = np.zeros(m)\n    h1 = sigmoid(np.dot(np.concatenate([np.ones((m, 1)), X], axis=1), Theta1.T))\n    h2 = sigmoid(np.dot(np.concatenate([np.ones((m, 1)), h1], axis=1), Theta2.T))\n    p = np.argmax(h2, axis=1)\n    \n    return p\n\n\npred = predict(Theta1, Theta2, display)\nprint('Training Set Accuracy: %f' % (np.mean(pred == output) * 100))","b7520b3e":"csv = '..\/input\/digit-recognizer\/test.csv'\ntest = pd.read_csv(csv)","71b202d4":"X = np.matrix(test)\nm = X.shape[0]\n# X.shape\npred = predict(Theta1, Theta2, X)","d8b666ef":"ans = [[0,0] for i in range(m)]\nfor i in range(m):\n    ans[i][0] = i+1\n    ans[i][1] = int(pred[i])\n\ndf = pd.DataFrame(ans, columns = ['ImageId', 'Label'])\ndf.head(20)","c54838b7":"## 2. Model representation\n\nOur neural network is shown in the following figure.\n\n![image.png](attachment:image.png)\n\nIt has 3 layers - an input layer, a hidden layer and an output layer. Recall that our inputs are pixel values\nof digit images. Since the images are of size $28 \\times 28$, this gives us 784 input layer units (not counting the extra bias unit which always outputs +1).","2ec671b9":"## 3. Implementing the Neural Network\n\n### 3.1 Sigmoid Gradient\n\nWe will first implement the sigmoid gradient function. The gradient for the sigmoid function can be\ncomputed as\n\n$$ g'(z) = \\frac{d}{dz} g(z) = g(z)\\left(1-g(z)\\right) $$\n\nwhere\n\n$$ \\text{sigmoid}(z) = g(z) = \\frac{1}{1 + e^{-z}} $$\n","65bb189f":"## 4. Testing","a1663857":"### 3.2 Random Initialization\n\nWhen training neural networks, it is important to randomly initialize the parameters for symmetry breaking. One effective strategy for random initialization is to randomly select values for $\\Theta^{(l)}$ uniformly in the range $[-\\epsilon_{init}, \\epsilon_{init}]$.\n\nOne effective strategy for choosing epsilon is to base it on the number of units in the network.","7ce5fffb":"### 3.4 Learning parameters using `scipy.optimize.minimize`\n\nAfter successfully implementing the neural network cost function and gradient computation, the next step we will use `scipy`'s minimization to learn a good set parameters.","3107c3c7":"### 3.5 Prediction on Training Set","2d7ce50e":"In my submission, I have implemented a neural network with 3 layers (input, hidden, output), from scratch. I tried to implement the concepts I learnt in Andrew NG's Stanford course on Machine Learning. The Jupyter notebook contains all the necessary details for understanding my solution. My submission got a score of 90% on Kaggle.","11d1d1b2":"This submission gave me a score of 0.90689\/1 on Kaggle.","77d1e52b":"### 3.3 Cost function\n\nThe cost function for neural networks with regularization is given by:\n\n\n$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}\\sum_{k=1}^{K} \\left[ - y_k^{(i)} \\log \\left( \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) - \\left( 1 - y_k^{(i)} \\right) \\log \\left( 1 - \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) \\right] + \\frac{\\lambda}{2 m} \\left[ \\sum_{j=1}^{50} \\sum_{k=1}^{784} \\left( \\Theta_{j,k}^{(1)} \\right)^2 + \\sum_{j=1}^{10} \\sum_{k=1}^{50} \\left( \\Theta_{j,k}^{(2)} \\right)^2 \\right] $$\n\n\nwhere $h_\\theta \\left( x^{(i)} \\right)$ is computed as shown in the neural network figure above, and K = 10 is the total number of possible labels. Note that $h_\\theta(x^{(i)})_k = a_k^{(3)}$ is the activation (output\nvalue) of the $k^{th}$ output unit. Also, recall that whereas the original labels (in the variable y) were 0, 1, ..., 9, for the purpose of training a neural network, we need to encode the labels as vectors containing only values 0 or 1, so that\n\n$$ y = \n\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\\\vdots \\\\ 0 \\end{bmatrix}, \\quad\n\\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}, \\quad \\cdots  \\quad \\text{or} \\qquad\n\\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 1 \\end{bmatrix}.\n$$\n\nFor example, if $x^{(i)}$ is an image of the digit 5, then the corresponding $y^{(i)}$ should be a 10-dimensional vector with $y_5 = 1$, and the other elements equal to 0.\n\nWe will use the feedforward and back propagation algorithm to train the parameters and minimize the cost function. \n\n![image.png](attachment:image.png)\n","75d967cf":"# Digit Recognizer\n\nMNIST (\"Modified National Institute of Standards and Technology\") is the de facto \u201chello world\u201d dataset of computer vision. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike. The goal is to correctly identify digits from a dataset of tens of thousands of handwritten images.","5079c6e3":"## 1. Data\n\nThe data files train.csv and test.csv contain gray-scale images of hand-drawn digits, from zero through nine.\n\nEach image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive.\n\nThe training data set, (train.csv), has 785 columns. The first column, called \"label\", is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image.\n\nThe test data set, (test.csv), is the same as the training set, except that it does not contain the \"label\" column.","b1ed193f":"### 1.1 Visualizing the data\n\nBelow I'm displaying 5 random images from the given dataset to understand what the images look like."}}