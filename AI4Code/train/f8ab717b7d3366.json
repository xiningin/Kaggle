{"cell_type":{"3c412e94":"code","2dad40c7":"code","991e1a09":"code","8fbb173e":"code","c8366d71":"code","acb9cdb6":"code","5be117e9":"code","6f154644":"code","bc52db70":"code","7b456783":"code","30171c40":"code","cff35551":"code","0fa7fc1d":"code","88321f28":"code","180023b9":"code","20c857a2":"code","b23519cc":"code","724bba6a":"code","2ca295cb":"code","a0b03dfd":"code","379408d1":"code","cc5c796f":"code","bc6cf04e":"code","40883dc9":"code","ce2310ab":"code","90e4aa6d":"code","5134dcec":"code","6cc639eb":"code","f5980d3e":"code","f945bbdc":"code","1b1ca469":"code","ab7e660e":"code","8dd1c0d4":"code","23b2888a":"code","258d29dc":"markdown","32b0f7bd":"markdown","e1c0efc5":"markdown","010b6e82":"markdown","707a72c1":"markdown","13b65c79":"markdown","f2eb2942":"markdown","5c33a93f":"markdown","c393c9ae":"markdown"},"source":{"3c412e94":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tensorflow import set_random_seed\n\nnp.random.seed(1)\nset_random_seed(1)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","2dad40c7":"import nltk, re, string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer","991e1a09":"def clean_text(text):\n    print(text)\n    \n    ## Remove puncuation\n    text = text.translate(string.punctuation)\n    \n    ## Convert words to lower case and split them\n    text = text.lower()\n    \n    ## Remove stop words\n    #text = text.split()\n    #stops = set(stopwords.words(\"english\"))\n    #text = [w for w in text if not w in stops and len(w) >= 3]\n    \n    #text = \" \".join(text)\n\n    # Clean the text\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\\/'+-=]\", \" \", text)\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" \", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\\/\", \" \", text)\n    text = re.sub(r\"\\^\", \" ^ \", text)\n    text = re.sub(r\"\\+\", \" + \", text)\n    text = re.sub(r\"\\-\", \" - \", text)\n    text = re.sub(r\"\\=\", \" = \", text)\n    text = re.sub(r\"'\", \" \", text)\n    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n    text = re.sub(r\":\", \" : \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n    text = re.sub(r\" u s \", \" american \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e - mail\", \"email\", text)\n    text = re.sub(r\"j k\", \"jk\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    \n    #text = text.split()\n    #stemmer = SnowballStemmer('english')\n    #stemmed_words = [stemmer.stem(word) for word in text]\n    #text = \" \".join(stemmed_words)\n\n    print(text)\n    print(\"\")\n    return text","8fbb173e":"train_df = pd.read_csv('..\/input\/train.csv')\n#train_df[\"question_text\"] = train_df[\"question_text\"].map(lambda x: clean_text(x))\n\ntest_df = pd.read_csv('..\/input\/test.csv')\n#test_df[\"question_text\"] = test_df[\"question_text\"].map(lambda x: clean_text(x))","c8366d71":"X_train = train_df[\"question_text\"].fillna(\"na\").values\nX_test = test_df[\"question_text\"].fillna(\"na\").values\ny = train_df[\"target\"]","acb9cdb6":"from keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, concatenate\nfrom keras.layers import CuDNNGRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D\nfrom keras.layers import Add, BatchNormalization, Activation, CuDNNLSTM, Dropout\nfrom keras.layers import *\nfrom keras.models import *\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nimport gc\nfrom sklearn import metrics\n","5be117e9":"maxlen = 70\nmax_features = 50000\nembed_size = 300\n\ntokenizer = text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(X_train) + list(X_test))\n\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)\n\nx_train = sequence.pad_sequences(X_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(X_test, maxlen=maxlen)","6f154644":"def attention_3d_block(inputs):\n    # inputs.shape = (batch_size, time_steps, input_dim)\n    TIME_STEPS = inputs.shape[1].value\n    SINGLE_ATTENTION_VECTOR = False\n    \n    input_dim = int(inputs.shape[2])\n    a = Permute((2, 1))(inputs)\n    a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n    a = Dense(TIME_STEPS, activation='softmax')(a)\n    if SINGLE_ATTENTION_VECTOR:\n        a = Lambda(lambda x: K.mean(x, axis=1))(a)\n        a = RepeatVector(input_dim)(a)\n    a_probs = Permute((2, 1))(a)\n    output_attention_mul = Multiply()([inputs, a_probs])\n    return output_attention_mul","bc52db70":"from keras import backend as K\nfrom keras.engine.topology import Layer, InputSpec\nfrom keras import initializers\n\nclass AttLayer(Layer):\n    def __init__(self, attention_dim):\n        self.init = initializers.get('normal')\n        self.supports_masking = True\n        self.attention_dim = attention_dim\n        super(AttLayer, self).__init__()\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n        self.b = K.variable(self.init((self.attention_dim, )))\n        self.u = K.variable(self.init((self.attention_dim, 1)))\n        self.trainable_weights = [self.W, self.b, self.u]\n        super(AttLayer, self).build(input_shape)\n\n    def compute_mask(self, inputs, mask=None):\n        return mask\n\n    def call(self, x, mask=None):\n        # size of x :[batch_size, sel_len, attention_dim]\n        # size of u :[batch_size, attention_dim]\n        # uit = tanh(xW+b)\n        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n        ait = K.dot(uit, self.u)\n        ait = K.squeeze(ait, -1)\n\n        ait = K.exp(ait)\n\n        if mask is not None:\n            # Cast the mask to floatX to avoid float64 upcasting in theano\n            ait *= K.cast(mask, K.floatx())\n        ait \/= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n        ait = K.expand_dims(ait)\n        weighted_input = x * ait\n        output = K.sum(weighted_input, axis=1)\n\n        return output\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], input_shape[-1])","7b456783":"EMBEDDING_FILE = '..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix_1 = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix_1[i] = embedding_vector\n\ndel embeddings_index; gc.collect() ","30171c40":"EMBEDDING_FILE = '..\/input\/embeddings\/wiki-news-300d-1M\/wiki-news-300d-1M.vec'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix_2 = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix_2[i] = embedding_vector\ndel embeddings_index; gc.collect()","cff35551":"EMBEDDING_FILE = '..\/input\/embeddings\/paragram_300_sl999\/paragram_300_sl999.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix_3 = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix_3[i] = embedding_vector\n        \ndel embeddings_index; gc.collect()   ","0fa7fc1d":"# # https:\/\/www.kaggle.com\/strideradu\/word2vec-and-gensim-go-go-go\n# from gensim.models import KeyedVectors\n\n# EMBEDDING_FILE = '..\/input\/embeddings\/GoogleNews-vectors-negative300\/GoogleNews-vectors-negative300.bin'\n# embeddings_index = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n\n# word_index = tokenizer.word_index\n# nb_words = min(max_features, len(word_index))\n# embedding_matrix_4 = (np.random.rand(nb_words, embed_size) - 0.5) \/ 5.0\n# for word, i in word_index.items():\n#     if i >= max_features: continue\n#     if word in embeddings_index:\n#         embedding_vector = embeddings_index.get_vector(word)\n#         embedding_matrix_4[i] = embedding_vector\n        \n# del embeddings_index; gc.collect()        ","88321f28":"embedding_matrix = np.concatenate((embedding_matrix_1, embedding_matrix_2, embedding_matrix_3), axis=1)  \ndel embedding_matrix_1, embedding_matrix_2, embedding_matrix_3\ngc.collect()\nnp.shape(embedding_matrix)","180023b9":"from sklearn.model_selection import train_test_split\nX_tra, X_val, y_tra, y_val = train_test_split(x_train, y, test_size = 0.1, random_state=42)","20c857a2":"# def squash(x, axis=-1):\n#     # s_squared_norm is really small\n#     # s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n#     # scale = K.sqrt(s_squared_norm)\/ (0.5 + s_squared_norm)\n#     # return scale * x\n#     s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n#     scale = K.sqrt(s_squared_norm + K.epsilon())\n#     return x \/ scale\n\n\n# # A Capsule Implement with Pure Keras\n# class Capsule(Layer):\n#     def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n#                  activation='default', **kwargs):\n#         super(Capsule, self).__init__(**kwargs)\n#         self.num_capsule = num_capsule\n#         self.dim_capsule = dim_capsule\n#         self.routings = routings\n#         self.kernel_size = kernel_size\n#         self.share_weights = share_weights\n#         if activation == 'default':\n#             self.activation = squash\n#         else:\n#             self.activation = Activation(activation)\n\n#     def build(self, input_shape):\n#         super(Capsule, self).build(input_shape)\n#         input_dim_capsule = input_shape[-1]\n#         if self.share_weights:\n#             self.W = self.add_weight(name='capsule_kernel',\n#                                      shape=(1, input_dim_capsule,\n#                                             self.num_capsule * self.dim_capsule),\n#                                      # shape=self.kernel_size,\n#                                      initializer='glorot_uniform',\n#                                      trainable=True)\n#         else:\n#             input_num_capsule = input_shape[-2]\n#             self.W = self.add_weight(name='capsule_kernel',\n#                                      shape=(input_num_capsule,\n#                                             input_dim_capsule,\n#                                             self.num_capsule * self.dim_capsule),\n#                                      initializer='glorot_uniform',\n#                                      trainable=True)\n\n#     def call(self, u_vecs):\n#         if self.share_weights:\n#             u_hat_vecs = K.conv1d(u_vecs, self.W)\n#         else:\n#             u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n\n#         batch_size = K.shape(u_vecs)[0]\n#         input_num_capsule = K.shape(u_vecs)[1]\n#         u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n#                                             self.num_capsule, self.dim_capsule))\n#         u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n#         # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n\n#         b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n#         for i in range(self.routings):\n#             b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n#             c = K.softmax(b)\n#             c = K.permute_dimensions(c, (0, 2, 1))\n#             b = K.permute_dimensions(b, (0, 2, 1))\n#             outputs = self.activation(K.batch_dot(c, u_hat_vecs, [2, 2]))\n#             if i < self.routings - 1:\n#                 b = K.batch_dot(outputs, u_hat_vecs, [2, 3])\n\n#         return outputs\n\n#     def compute_output_shape(self, input_shape):\n#         return (None, self.num_capsule, self.dim_capsule)","b23519cc":"# def model1():\n#     inp = Input(shape=(maxlen, ))\n#     embed = Embedding(max_features, embed_size * 3, weights=[embedding_matrix], trainable=False)(inp)\n    \n#     #x = Reshape((maxlen, embed_size * 4, 1))(embed)\n    \n#     filter_sizes = [1,2,3,5]\n#     num_filters = 64\n    \n#     conv_0 = Conv1D(num_filters, filter_sizes[0], padding='valid', kernel_initializer='normal', activation='relu')(embed)\n#     conv_1 = Conv1D(num_filters, filter_sizes[1], padding='valid', kernel_initializer='normal', activation='relu')(embed)\n#     conv_2 = Conv1D(num_filters, filter_sizes[2], padding='valid', kernel_initializer='normal', activation='relu')(embed)\n#     conv_3 = Conv1D(num_filters, filter_sizes[3], padding='valid', kernel_initializer='normal', activation='relu')(embed)\n\n#     maxpool_0 = MaxPool1D(pool_size=(maxlen - filter_sizes[0] + 1), strides=(1), padding='valid')(conv_0)\n#     maxpool_1 = MaxPool1D(pool_size=(maxlen - filter_sizes[1] + 1), strides=(1), padding='valid')(conv_1)\n#     maxpool_2 = MaxPool1D(pool_size=(maxlen - filter_sizes[2] + 1), strides=(1), padding='valid')(conv_2)\n#     maxpool_3 = MaxPool1D(pool_size=(maxlen - filter_sizes[3] + 1), strides=(1), padding='valid')(conv_3)\n\n#     concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2, maxpool_3])\n    \n#     #gmp = GlobalMaxPooling1D()(concatenated_tensor)\n#     #gap = GlobalAveragePooling1D()(concatenated_tensor)\n    \n#     #conc = Concatenate(axis=1)([gmp, gap])\n    \n#     flatten = Flatten()(concatenated_tensor)\n    \n#     x = flatten\n#     x = Dropout(0.3)(x)\n#     x = Dense(128, activation='relu')(x)\n#     outp = Dense(1, activation=\"sigmoid\")(x)\n#     model = Model(inputs=inp, outputs=outp)\n#     model.compile(loss='binary_crossentropy',\n#                   optimizer='adam',\n#                   metrics=['accuracy'])    \n\n#     return model","724bba6a":"# MODEL1 = model1()\n# MODEL1.summary()\n\n# batch_size = 1536\n# epochs = 3\n\n# early_stopping = EarlyStopping(patience=3, verbose=1, monitor='val_loss', mode='min')\n# model_checkpoint = ModelCheckpoint('.\/model1.model', save_best_only=True, verbose=1, monitor='val_loss', mode='min')\n# reduce_lr = ReduceLROnPlateau(factor=0.5, patience=3, min_lr=0.0001, verbose=1)\n\n# hist = MODEL1.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val), verbose=True)\n# MODEL1.save('.\/model1.h5')","2ca295cb":"# pred_val_y_1 = MODEL1.predict([X_val], batch_size=1024, verbose=1)\n# thresholds = []\n# for thresh in np.arange(0.1, 0.501, 0.01):\n#     thresh = np.round(thresh, 2)\n#     res = metrics.f1_score(y_val, (pred_val_y_1 > thresh).astype(int))\n#     thresholds.append([thresh, res])\n#     print(\"F1 score at threshold {0} is {1}\".format(thresh, res))\n    \n# thresholds.sort(key=lambda x: x[1], reverse=True)\n# best_thresh_1 = thresholds[0][0]\n# print(\"Best threshold: \", best_thresh_1)\n\n# y_pred_1 = MODEL1.predict(x_test, batch_size=1024, verbose=True)","a0b03dfd":"def model2():\n    inp = Input(shape=(maxlen, ))\n    embed = Embedding(max_features, embed_size * 3, weights=[embedding_matrix], trainable=False)(inp)\n    x = embed\n    \n    x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\n    x = attention_3d_block(x)\n    x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\n    x = AttLayer(64)(x)\n    x = Dropout(0.3)(x)\n    x = Dense(128, activation='relu')(x)\n    outp = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])    \n\n    return model","379408d1":"MODEL2 = model2()\nMODEL2.summary()\n\nbatch_size = 2048\nepochs = 3\n\nearly_stopping = EarlyStopping(patience=3, verbose=1, monitor='val_loss', mode='min')\nmodel_checkpoint = ModelCheckpoint('.\/model2.model', save_best_only=True, verbose=1, monitor='val_loss', mode='min')\nreduce_lr = ReduceLROnPlateau(factor=0.5, patience=3, min_lr=0.0001, verbose=1)\n\nhist = MODEL2.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val), verbose=True)\nMODEL2.save('.\/model2.h5')","cc5c796f":"pred_val_y_2 = MODEL2.predict([X_val], batch_size=1024, verbose=1)\nthresholds = []\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    res = metrics.f1_score(y_val, (pred_val_y_2 > thresh).astype(int))\n    thresholds.append([thresh, res])\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, res))\n    \nthresholds.sort(key=lambda x: x[1], reverse=True)\nbest_thresh_2 = thresholds[0][0]\nprint(\"Best threshold: \", best_thresh_2)\n\ny_pred_2 = MODEL2.predict(x_test, batch_size=1024, verbose=True)","bc6cf04e":"# def model3():\n#     filters = 128\n    \n#     inp = Input(shape=(maxlen, ))\n#     embed = Embedding(max_features, embed_size * 3, weights=[embedding_matrix], trainable=False)(inp)\n#     x = embed\n    \n#     x = Conv1D(filters, 1, activation='relu')(x)\n#     x = Dropout(0.1)(x)\n    \n#     x = Conv1D(filters, 2, activation='relu')(x)\n#     x = Dropout(0.1)(x)\n    \n#     x = Conv1D(filters, 3, activation='relu')(x)\n#     x = Dropout(0.1)(x)\n    \n#     x = Conv1D(filters, 5, activation='relu')(x)\n#     x = Dropout(0.1)(x)\n    \n#     #x = Flatten()(x)\n#     x = GlobalAveragePooling1D()(x)\n    \n#     x = Dropout(0.3)(x)\n#     x = Dense(128, activation='relu')(x)\n#     outp = Dense(1, activation=\"sigmoid\")(x)\n#     model = Model(inputs=inp, outputs=outp)\n#     model.compile(loss='binary_crossentropy',\n#                   optimizer='adam',\n#                   metrics=['accuracy'])    \n#     return model","40883dc9":"# MODEL3 = model3()\n# MODEL3.summary()\n\n# batch_size = 2048\n# epochs = 1\n\n# early_stopping = EarlyStopping(patience=3, verbose=1, monitor='val_loss', mode='min')\n# model_checkpoint = ModelCheckpoint('.\/model3.model', save_best_only=True, verbose=1, monitor='val_loss', mode='min')\n# reduce_lr = ReduceLROnPlateau(factor=0.5, patience=3, min_lr=0.0001, verbose=1)\n\n# hist = MODEL3.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val), verbose=True)\n# MODEL3.save('.\/model3.h5')","ce2310ab":"# pred_val_y_3 = MODEL3.predict([X_val], batch_size=1024, verbose=1)\n# thresholds = []\n# for thresh in np.arange(0.1, 0.501, 0.01):\n#     thresh = np.round(thresh, 2)\n#     res = metrics.f1_score(y_val, (pred_val_y_3 > thresh).astype(int))\n#     thresholds.append([thresh, res])\n#     print(\"F1 score at threshold {0} is {1}\".format(thresh, res))\n    \n# thresholds.sort(key=lambda x: x[1], reverse=True)\n# best_thresh_3 = thresholds[0][0]\n# print(\"Best threshold: \", best_thresh_3)\n\n# y_pred_3 = MODEL3.predict(x_test, batch_size=1024, verbose=True)","90e4aa6d":"def model4():\n    inp = Input(shape=(maxlen, ))\n    embed = Embedding(max_features, embed_size * 3, weights=[embedding_matrix], trainable=False)(inp)\n    x = embed\n    \n    x = Bidirectional(CuDNNGRU(128, return_sequences=True))(x)\n    x = attention_3d_block(x)\n    x = Bidirectional(CuDNNGRU(128, return_sequences=True))(x)\n    x = AttLayer(64)(x)\n    \n    x = Dropout(0.3)(x)\n    x = Dense(128, activation='relu')(x)\n    outp = Dense(1, activation=\"sigmoid\")(x)\n    outp = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])    \n\n    return model","5134dcec":"MODEL4 = model4()\nMODEL4.summary()\n\nbatch_size = 1536\nepochs = 3\n\nearly_stopping = EarlyStopping(patience=3, verbose=1, monitor='val_loss', mode='min')\nmodel_checkpoint = ModelCheckpoint('.\/model4.model', save_best_only=True, verbose=1, monitor='val_loss', mode='min')\nreduce_lr = ReduceLROnPlateau(factor=0.5, patience=3, min_lr=0.0001, verbose=1)\n\n\nhist = MODEL4.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val), verbose=True)\nMODEL4.save('.\/model4.h5')","6cc639eb":"pred_val_y_4 = MODEL4.predict([X_val], batch_size=1024, verbose=1)\nthresholds = []\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    res = metrics.f1_score(y_val, (pred_val_y_4 > thresh).astype(int))\n    thresholds.append([thresh, res])\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, res))\n    \nthresholds.sort(key=lambda x: x[1], reverse=True)\nbest_thresh_4 = thresholds[0][0]\nprint(\"Best threshold: \", best_thresh_4)\n\ny_pred_4 = MODEL4.predict(x_test, batch_size=1024, verbose=True)","f5980d3e":"def model5():\n    inp = Input(shape=(maxlen, ))\n    embed = Embedding(max_features, embed_size * 3, weights=[embedding_matrix], trainable=False)(inp)\n    x = embed\n    \n    x0 = Bidirectional(CuDNNGRU(128, return_sequences=True))(x)\n    x1 = attention_3d_block(x0)\n    x2 = Bidirectional(CuDNNGRU(128, return_sequences=True))(x1)\n    x3 = Add()([x0, x2])\n    x4 = Bidirectional(CuDNNGRU(64, return_sequences=True))(x3)\n    x5 = AttLayer(64)(x4)\n    #x5 = Capsule(num_capsule=5, dim_capsule=32, routings=5, share_weights=True)(x4)\n    \n    x = Dropout(0.3)(x5)\n    x = Dense(128, activation='relu')(x)\n    outp = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])    \n\n    return model","f945bbdc":"MODEL5 = model5()\nMODEL5.summary()\n\nbatch_size = 1536\nepochs = 3\n\nearly_stopping = EarlyStopping(patience=3, verbose=1, monitor='val_loss', mode='min')\nmodel_checkpoint = ModelCheckpoint('.\/model5.model', save_best_only=True, verbose=1, monitor='val_loss', mode='min')\nreduce_lr = ReduceLROnPlateau(factor=0.5, patience=3, min_lr=0.0001, verbose=1)\n\nhist = MODEL5.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val), verbose=True)\nMODEL5.save('.\/model5.h5')","1b1ca469":"pred_val_y_5 = MODEL5.predict([X_val], batch_size=1024, verbose=1)\nthresholds = []\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    res = metrics.f1_score(y_val, (pred_val_y_5 > thresh).astype(int))\n    thresholds.append([thresh, res])\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, res))\n    \nthresholds.sort(key=lambda x: x[1], reverse=True)\nbest_thresh_5 = thresholds[0][0]\nprint(\"Best threshold: \", best_thresh_5)\n\ny_pred_5 = MODEL5.predict(x_test, batch_size=1024, verbose=True)","ab7e660e":"pred_val_y = (3*pred_val_y_2 + 4*pred_val_y_4 + 3*pred_val_y_5)\/10\n\nthresholds = []\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    res = metrics.f1_score(y_val, (pred_val_y > thresh).astype(int))\n    thresholds.append([thresh, res])\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, res))\n    \nthresholds.sort(key=lambda x: x[1], reverse=True)\nbest_thresh = thresholds[0][0]\nprint(\"Best threshold: \", best_thresh)","8dd1c0d4":"y_pred = (3*y_pred_2 + 4*y_pred_4 + 3*y_pred_5)\/10\ny_te = (y_pred[:,0] > best_thresh).astype(np.int)\n\nsubmit_df = pd.DataFrame({\"qid\": test_df[\"qid\"], \"prediction\": y_te})\nsubmit_df.to_csv(\"submission.csv\", index=False)","23b2888a":"from IPython.display import HTML\nimport base64  \nimport pandas as pd  \n\ndef create_download_link( df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv(index =False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\ncreate_download_link(submit_df)","258d29dc":"# Model 5: GRU Add","32b0f7bd":"# Concat Result & Best Threshold","e1c0efc5":"# Capsule Net Block","010b6e82":"# Submission File","707a72c1":"# MODEL 1: Conv2D","13b65c79":"# MODEL 2: LSTM","f2eb2942":"# Concatenating the embeddings","5c33a93f":"# MODEL 3: Conv1D","c393c9ae":"# MODEL 4: GRU"}}