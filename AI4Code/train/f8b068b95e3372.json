{"cell_type":{"203b08b8":"code","c11f5cff":"code","06852130":"code","eb4c8121":"code","987a9063":"code","55859369":"code","448e303d":"code","d366b419":"code","1e0e8eca":"code","b09a7f47":"code","1c15a902":"code","995f4a1f":"code","b139aed1":"code","e8c10371":"code","9cc6a29f":"code","28b09c14":"code","eb3a135c":"code","9d352bbf":"code","bf6f02a3":"code","8c4db31b":"code","e8327219":"code","f0c61412":"code","ba635001":"code","50d25464":"code","c07dbde1":"code","138c1ad9":"code","125adc36":"code","ff7e98f2":"code","c0d28276":"code","6266cbe0":"code","3e563ab4":"code","af02b571":"code","632750a9":"code","313d2d0e":"code","eceb893f":"code","d76bdb0b":"code","c871908a":"code","811afeec":"code","4ecbfeef":"code","a7e43fd4":"code","ce964dd9":"code","14e63ae5":"code","8c866d89":"code","1b128635":"code","31700e16":"code","f52fa550":"code","17385b4f":"code","9eee4617":"code","d4315d48":"code","4c834b8e":"code","3f631a03":"code","c0153899":"code","fc99a232":"code","c2b338f0":"code","4800904e":"code","26b36d82":"code","f9bcc0cd":"code","cd6c25ba":"code","eef3182a":"code","6ac4043a":"code","9194703d":"code","d814ad87":"code","9a5b6ca2":"code","0599ef19":"code","da221429":"code","7dcda1e9":"code","0be78d69":"code","51e977c4":"code","35d0a649":"code","ad6c1fce":"code","903243db":"code","78c83282":"code","d2cb32ac":"code","ef457608":"code","b8ff2adc":"code","967b886d":"code","f5db3f54":"code","89e48f91":"code","7eb22634":"code","c6f6eb7e":"code","b3a83158":"code","48800c09":"code","a9236c39":"code","afcd3460":"code","0505228e":"code","b3fc6b8c":"code","a10203f4":"code","a8f1f317":"code","fc4ae324":"code","aa9d442b":"code","940de9a9":"code","22056067":"code","cc46f397":"code","286c6d75":"code","23a8e039":"code","2f95cd57":"code","c95335ba":"code","1271445c":"code","d1758d74":"code","717e4560":"code","163afd6b":"code","ab7e97ce":"code","54223129":"code","6b56b53e":"code","71922937":"code","6228481c":"code","21d7d37f":"code","f99dd837":"code","20652efd":"code","5c00b04c":"code","502c25dc":"code","540543a4":"code","305b0674":"code","84c88b8d":"code","393c605e":"code","d9e3a284":"markdown","1842e3fb":"markdown","c3c12180":"markdown","a33d9507":"markdown","5e8640e8":"markdown","cf627d51":"markdown","2c877ae5":"markdown","0cbcad63":"markdown","2417be04":"markdown","3568447d":"markdown","dc7bea8d":"markdown","62b5dfcd":"markdown","211f892a":"markdown","8fcd75be":"markdown","49994e71":"markdown","814085c5":"markdown","af6ca343":"markdown","ebd5e824":"markdown","6a6f7527":"markdown","c48985a2":"markdown","46bd6de7":"markdown","fab4b1e7":"markdown","a3d72252":"markdown","e8baeba6":"markdown","b4285ddd":"markdown","070fae23":"markdown","1b3eb58d":"markdown","b600c202":"markdown","6d28586c":"markdown","2500ae6b":"markdown","cf4f49b6":"markdown","c455b40d":"markdown","44c2960d":"markdown","6fd644d9":"markdown","11d636d3":"markdown","dbbe2387":"markdown","e1951760":"markdown","daa724a2":"markdown","7c845114":"markdown","4d7074c6":"markdown","fc0861e4":"markdown","8b5eeae6":"markdown","bbab0eb2":"markdown","e988cd5e":"markdown","b710ccb2":"markdown","a889b1e1":"markdown","da3bf4ae":"markdown","2eb25da3":"markdown","d7f249fa":"markdown","28a905de":"markdown","510c94fc":"markdown","a324ac65":"markdown","14cc027a":"markdown","97adfce8":"markdown","bb0ffa99":"markdown","9400348c":"markdown","5bb1c579":"markdown","405a1ec3":"markdown","e4966f2f":"markdown","c8c2738f":"markdown","57b80f62":"markdown","929051c9":"markdown","aac44c40":"markdown","91f63966":"markdown","355d98bd":"markdown"},"source":{"203b08b8":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport random","c11f5cff":"print(f'Numpy: {np.__version__}')\nprint(f'Pandas: {pd.__version__}')\nprint(f'Matplotlib: {matplotlib.__version__}')\nprint(f'Seaborn: {sns.__version__}')","06852130":"train = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\nprint('Train Sales Data')\ntrain.head(2)","eb4c8121":"train.dtypes","987a9063":"train.isna().sum()","55859369":"train.describe()","448e303d":"items = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/items.csv')\nprint('Items Data')\nitems.head(2)","d366b419":"items.dtypes","1e0e8eca":"items.isna().sum()","b09a7f47":"items.nunique()","1c15a902":"items.shape[0]","995f4a1f":"it_cat = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\nprint('Item Categories Data')\nit_cat.head(2)","b139aed1":"it_cat.isna().sum()","e8c10371":"it_cat.nunique()","9cc6a29f":"it_cat.shape[0]","28b09c14":"shop = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/shops.csv')\nprint(\"Shops Data\")\nshop.head(2)","eb3a135c":"shop.isna().sum()","9d352bbf":"shop.nunique()","bf6f02a3":"shop.shape[0]","8c4db31b":"test = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv')\nprint('Test Data')\ntest.head(2)","e8327219":"test.isna().sum()","f0c61412":"test.nunique()","ba635001":"train.nunique()","50d25464":"sub = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv')\nsub.head(2)","c07dbde1":"train.head(2)","138c1ad9":"train.item_cnt_day.min(), train.item_cnt_day.max()","125adc36":"train[train.item_cnt_day<0]","ff7e98f2":"return_per_shop = train[train.item_cnt_day<0].groupby('shop_id')['item_cnt_day'].sum()\nreturn_per_shop","c0d28276":"return_per_shop.sort_values(ascending=True).iloc[:10]","6266cbe0":"return_per_shop.sort_values(ascending=False).iloc[:10]","3e563ab4":"return_per_item = train[train.item_cnt_day<0].groupby('item_id')['item_cnt_day'].sum()\nreturn_per_item","af02b571":"return_per_item.sort_values(ascending=True).iloc[:10]","632750a9":"return_per_item.sort_values(ascending=False).iloc[:10]","313d2d0e":"returns_monthly = train[train.item_cnt_day<0].groupby('date_block_num')['item_cnt_day'].sum()\nreturns_monthly","eceb893f":"returns_monthly.plot()\nplt.xlabel('date_block_num')\nplt.ylabel('Number of returned Items')\nplt.show()","d76bdb0b":"no_sold = train[train.item_cnt_day==0.0]\nno_sold","c871908a":"sold_per_month = train.groupby('date_block_num')['item_cnt_day'].sum()","811afeec":"sold_per_month.plot()\nplt.xlabel('date_block_num')\nplt.ylabel('Number of sold Items')\nplt.show()","4ecbfeef":"plt.figure()\ntmp_data = train.groupby(['date_block_num', 'shop_id'], as_index=False)['item_cnt_day'].sum()\nsns.lineplot(x='date_block_num', y='item_cnt_day', hue='shop_id', data = tmp_data)\nplt.xlabel('Date Month Block numbre')\nplt.ylabel('Number of Sold Items')\nplt.show()","a7e43fd4":"tmp_data = train.groupby('shop_id')['date_block_num'].min()\ntmp_data[tmp_data>0]","ce964dd9":"test[test['shop_id'].isin(tmp_data[tmp_data>0].index)]['shop_id'].unique()","14e63ae5":"test_items = test['item_id'].unique()\ntest_items","8c866d89":"tmp_data = train.groupby('item_id', as_index=False)['date_block_num'].min()\ntmp_data","1b128635":"tmp_data[tmp_data.item_id.isin(test_items)].sort_values(by='date_block_num', ascending=False)","31700e16":"test_items.shape[0]","f52fa550":"sns.boxplot(x='item_cnt_day', data=train)","17385b4f":"train[train['item_cnt_day']>=1000]","9eee4617":"items[items['item_id'].isin([20949, 11373])]['item_name'].values","d4315d48":"sns.boxplot(x='item_price', data=train)","4c834b8e":"train[train['item_price']>=100000]","3f631a03":"items[items['item_id']==6066]","c0153899":"train[train['item_id']==6066]","fc99a232":"train = train[train['item_cnt_day']<1000]\ntrain = train[train['item_price']<100000]\ntrain.reset_index(drop=True, inplace=True)","c2b338f0":"import holidays","4800904e":"ru_holidays = holidays.Russia()","26b36d82":"train['new_date'] = train['date'].apply(lambda x: x.split('.')[1]+'\/'+x.split('.')[0]+'\/'+x.split('.')[2])","f9bcc0cd":"train","cd6c25ba":"train['is_holiday'] = train['new_date'].apply(lambda x: x in ru_holidays)","eef3182a":"train","6ac4043a":"train[train['date_block_num']==0]['new_date'].nunique()","9194703d":"tmp_data = train.groupby(['date_block_num', 'new_date'], as_index=False)['is_holiday'].sum()\ntmp_data['is_holiday'] = tmp_data['is_holiday']>0\nn_holidays = tmp_data.groupby(['date_block_num'])['is_holiday'].sum()\nn_holidays","d814ad87":"train['new_date'] = pd.to_datetime(train['new_date'])","9a5b6ca2":"train['weekend'] = train['new_date'].apply(lambda x : x.weekday() in [5, 6])","0599ef19":"tmp_data = train.groupby(['date_block_num', 'date'], as_index=False)['weekend'].sum()\ntmp_data['weekend'] = tmp_data['weekend']>0\nn_weekends = tmp_data.groupby(['date_block_num'])['weekend'].sum()\nn_weekends","da221429":"train['money'] = train['item_price'] * train['item_cnt_day']","7dcda1e9":"train.groupby(['date_block_num', 'shop_id'], as_index=False)['money'].sum()","0be78d69":"train['month'] = train['date'].apply(lambda x: x.split('.')[1])\ntrain","51e977c4":"train.loc[train.shop_id == 0, \"shop_id\"] = 57\ntest.loc[test.shop_id == 0 , \"shop_id\"] = 57\ntrain.loc[train.shop_id == 1, \"shop_id\"] = 58\ntest.loc[test.shop_id == 1 , \"shop_id\"] = 58\ntrain.loc[train.shop_id == 11, \"shop_id\"] = 10\ntest.loc[test.shop_id == 11, \"shop_id\"] = 10\ntrain.loc[train.shop_id == 40, \"shop_id\"] = 39\ntest.loc[test.shop_id == 40, \"shop_id\"] = 39","35d0a649":"X_train = train.groupby(['date_block_num', 'shop_id', 'item_id'], as_index=False)['item_cnt_day'].sum()\nX_train","ad6c1fce":"X_train['Weekends'] = X_train['date_block_num'].map(n_weekends)\nX_train['Holidays'] = X_train['date_block_num'].map(n_holidays)\nX_train","903243db":"X_train['month'] = X_train['date_block_num'].map(train.groupby(['date_block_num'])['month'].unique())\nX_train['month'] = X_train['month'].apply(lambda x : int(x[0]))","78c83282":"X_train","d2cb32ac":"shop","ef457608":"shop.loc[ shop.shop_name == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"',\"shop_name\" ] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'\nshop[\"city\"] = shop.shop_name.str.split(\" \").map( lambda x: x[0] )\nshop[\"type\"] = shop.shop_name.str.split(\" \").map( lambda x: x[1] )\nshop.loc[shop.city == \"!\u042f\u043a\u0443\u0442\u0441\u043a\", \"city\"] = \"\u042f\u043a\u0443\u0442\u0441\u043a\"","b8ff2adc":"shop","967b886d":"shop.loc[~shop.type.isin(['\u0422\u0426', '\u0422\u0420\u0426', '\u0422\u041a', '\u0422\u0420\u041a', 'MTP\u0426']),\"type\"] = 'other'\nshop","f5db3f54":"shops = shop[['shop_id', 'city', 'type']]","89e48f91":"from sklearn.preprocessing import LabelEncoder","7eb22634":"shops[\"city\"] = LabelEncoder().fit_transform( shops.city )\nshops[\"type\"] = LabelEncoder().fit_transform( shops.type )","c6f6eb7e":"shops","b3a83158":"import re\ndef clean_name(x):\n    x = x.lower()\n    x = x.partition('[')[0]\n    x = x.partition('(')[0]\n    x = re.sub('[^A-Za-z0-9\u0410-\u042f\u0430-\u044f]+', ' ', x)\n    x = x.replace('  ', ' ')\n    x = x.strip()\n    return x","48800c09":"items[\"name1\"], items[\"name2\"] = items.item_name.str.split(\"[\", 1).str\nitems[\"name1\"], items[\"name3\"] = items.item_name.str.split(\"(\", 1).str\n\nitems[\"name2\"] = items.name2.str.replace('[^A-Za-z0-9\u0410-\u042f\u0430-\u044f]+', \" \").str.lower()\nitems[\"name3\"] = items.name3.str.replace('[^A-Za-z0-9\u0410-\u042f\u0430-\u044f]+', \" \").str.lower()\nitems = items.fillna('0')\n\nitems[\"item_name\"] = items[\"item_name\"].apply(lambda x: clean_name(x))\nitems.name2 = items.name2.apply( lambda x: x[:-1] if x !=\"0\" else \"0\")","a9236c39":"items[\"type\"] = items.name2.apply(lambda x: x[0:8] if x.split(\" \")[0] == \"xbox\" else x.split(\" \")[0] )\nitems.loc[(items.type == \"x360\") | (items.type == \"xbox360\") | (items.type == \"xbox 360\") ,\"type\"] = \"xbox 360\"\nitems.loc[ items.type == \"\", \"type\"] = \"mac\"\nitems.type = items.type.apply( lambda x: x.replace(\" \", \"\") )\nitems.loc[ (items.type == 'pc' )| (items.type == 'p\u0441') | (items.type == \"pc\"), \"type\" ] = \"pc\"\nitems.loc[ items.type == '\u0440s3' , \"type\"] = \"ps3\"","afcd3460":"group_sum = items.groupby([\"type\"], as_index=False).agg({\"item_id\": \"count\"})\ngroup_sum","0505228e":"drop_cols = group_sum.loc[group_sum['item_id']<40,'type'].values.tolist()","b3fc6b8c":"items.name2 = items.name2.apply( lambda x: \"etc\" if (x in drop_cols) else x )\nitems = items.drop([\"type\"], axis = 1)","a10203f4":"items.name2 = LabelEncoder().fit_transform(items.name2)\nitems.name3 = LabelEncoder().fit_transform(items.name3)\n\nitems.drop([\"item_name\", \"name1\"],axis = 1, inplace= True)\nitems.head()","a8f1f317":"from itertools import product\nimport time\nts = time.time()\nmatrix = []\ncols  = [\"date_block_num\", \"shop_id\", \"item_id\"]\nfor i in range(34):\n    sales = train[train.date_block_num == i]\n    matrix.append( np.array(list( product( [i], sales.shop_id.unique(), sales.item_id.unique() ) ), dtype = np.int16) )\n\nmatrix = pd.DataFrame( np.vstack(matrix), columns = cols )\nmatrix[\"date_block_num\"] = matrix[\"date_block_num\"].astype(np.int8)\nmatrix[\"shop_id\"] = matrix[\"shop_id\"].astype(np.int8)\nmatrix[\"item_id\"] = matrix[\"item_id\"].astype(np.int16)\nmatrix.sort_values( cols, inplace = True )\ntime.time()- ts","fc4ae324":"train[\"revenue\"] = train[\"item_cnt_day\"] * train[\"item_price\"]","aa9d442b":"ts = time.time()\ngroup = train.groupby( [\"date_block_num\", \"shop_id\", \"item_id\"] ).agg( {\"item_cnt_day\": [\"sum\"]} )\ngroup.columns = [\"item_cnt_month\"]\ngroup.reset_index( inplace = True)\nmatrix = pd.merge( matrix, group, on = cols, how = \"left\" )\nmatrix[\"item_cnt_month\"] = matrix[\"item_cnt_month\"].fillna(0).clip(0,20).astype(np.float16)\ntime.time() - ts","940de9a9":"test[\"date_block_num\"] = 34\ntest[\"date_block_num\"] = test[\"date_block_num\"].astype(np.int8)\ntest[\"shop_id\"] = test.shop_id.astype(np.int8)\ntest[\"item_id\"] = test.item_id.astype(np.int16)","22056067":"ts = time.time()\n\nmatrix = pd.concat([matrix, test.drop([\"ID\"],axis = 1)], ignore_index=True, sort=False, keys=cols)\nmatrix.fillna( 0, inplace = True )\ntime.time() - ts","cc46f397":"matrix","286c6d75":"ts = time.time()\nmatrix = pd.merge( matrix, shops, on = [\"shop_id\"], how = \"left\" )\nmatrix = pd.merge(matrix, items, on = [\"item_id\"], how = \"left\")\nmatrix[\"city\"] = matrix[\"city\"].astype(np.int8)\nmatrix[\"type\"] = matrix[\"type\"].astype(np.int8)\nmatrix[\"item_category_id\"] = matrix[\"item_category_id\"].astype(np.int8)\nmatrix[\"name2\"] = matrix[\"name2\"].astype(np.int8)\nmatrix[\"name3\"] = matrix[\"name3\"].astype(np.int16)\ntime.time() - ts","23a8e039":"def lag_feature( df,lags, cols ):\n    for col in cols:\n        print(col)\n        tmp = df[[\"date_block_num\", \"shop_id\",\"item_id\",col ]]\n        for i in lags:\n            shifted = tmp.copy()\n            shifted.columns = [\"date_block_num\", \"shop_id\", \"item_id\", col + \"_lag_\"+str(i)]\n            shifted.date_block_num = shifted.date_block_num + i\n            df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","2f95cd57":"ts = time.time()\n\nmatrix = lag_feature( matrix, [1,2,3], [\"item_cnt_month\"] )\ntime.time() - ts","c95335ba":"ts = time.time()\ngroup = matrix.groupby( [\"date_block_num\"] ).agg({\"item_cnt_month\" : [\"mean\"]})\ngroup.columns = [\"date_avg_item_cnt\"]\ngroup.reset_index(inplace = True)\n\nmatrix = pd.merge(matrix, group, on = [\"date_block_num\"], how = \"left\")\nmatrix.date_avg_item_cnt = matrix[\"date_avg_item_cnt\"].astype(np.float16)\nmatrix = lag_feature( matrix, [1], [\"date_avg_item_cnt\"] )\nmatrix.drop( [\"date_avg_item_cnt\"], axis = 1, inplace = True )\ntime.time() - ts","1271445c":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix.date_item_avg_item_cnt = matrix['date_item_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1,2,3], ['date_item_avg_item_cnt'])\nmatrix.drop(['date_item_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","d1758d74":"ts = time.time()\ngroup = matrix.groupby( [\"date_block_num\",\"shop_id\"] ).agg({\"item_cnt_month\" : [\"mean\"]})\ngroup.columns = [\"date_shop_avg_item_cnt\"]\ngroup.reset_index(inplace = True)\n\nmatrix = pd.merge(matrix, group, on = [\"date_block_num\",\"shop_id\"], how = \"left\")\nmatrix.date_avg_item_cnt = matrix[\"date_shop_avg_item_cnt\"].astype(np.float16)\nmatrix = lag_feature( matrix, [1,2,3], [\"date_shop_avg_item_cnt\"] )\nmatrix.drop( [\"date_shop_avg_item_cnt\"], axis = 1, inplace = True )\ntime.time() - ts","717e4560":"ts = time.time()\ngroup = matrix.groupby( [\"date_block_num\",\"shop_id\",\"item_id\"] ).agg({\"item_cnt_month\" : [\"mean\"]})\ngroup.columns = [\"date_shop_item_avg_item_cnt\"]\ngroup.reset_index(inplace = True)\n\nmatrix = pd.merge(matrix, group, on = [\"date_block_num\",\"shop_id\",\"item_id\"], how = \"left\")\nmatrix.date_avg_item_cnt = matrix[\"date_shop_item_avg_item_cnt\"].astype(np.float16)\nmatrix = lag_feature( matrix, [1,2,3], [\"date_shop_item_avg_item_cnt\"] )\nmatrix.drop( [\"date_shop_item_avg_item_cnt\"], axis = 1, inplace = True )\ntime.time() - ts","163afd6b":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'shop_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_subtype_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id'], how='left')\nmatrix.date_shop_subtype_avg_item_cnt = matrix['date_shop_subtype_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], ['date_shop_subtype_avg_item_cnt'])\nmatrix.drop(['date_shop_subtype_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","ab7e97ce":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'city']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_city_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', \"city\"], how='left')\nmatrix.date_city_avg_item_cnt = matrix['date_city_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], ['date_city_avg_item_cnt'])\nmatrix.drop(['date_city_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts\n","54223129":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'item_id', 'city']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_city_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'item_id', 'city'], how='left')\nmatrix.date_item_city_avg_item_cnt = matrix['date_item_city_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], ['date_item_city_avg_item_cnt'])\nmatrix.drop(['date_item_city_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","6b56b53e":"ts = time.time()\ngroup = train.groupby( [\"item_id\"] ).agg({\"item_price\": [\"mean\"]})\ngroup.columns = [\"item_avg_item_price\"]\ngroup.reset_index(inplace = True)\n\nmatrix = matrix.merge( group, on = [\"item_id\"], how = \"left\" )\nmatrix[\"item_avg_item_price\"] = matrix.item_avg_item_price.astype(np.float16)\n\n\ngroup = train.groupby( [\"date_block_num\",\"item_id\"] ).agg( {\"item_price\": [\"mean\"]} )\ngroup.columns = [\"date_item_avg_item_price\"]\ngroup.reset_index(inplace = True)\n\nmatrix = matrix.merge(group, on = [\"date_block_num\",\"item_id\"], how = \"left\")\nmatrix[\"date_item_avg_item_price\"] = matrix.date_item_avg_item_price.astype(np.float16)\nlags = [1, 2, 3]\nmatrix = lag_feature( matrix, lags, [\"date_item_avg_item_price\"] )\nfor i in lags:\n    matrix[\"delta_price_lag_\" + str(i) ] = (matrix[\"date_item_avg_item_price_lag_\" + str(i)]- matrix[\"item_avg_item_price\"] )\/ matrix[\"item_avg_item_price\"]\n\ndef select_trends(row) :\n    for i in lags:\n        if row[\"delta_price_lag_\" + str(i)]:\n            return row[\"delta_price_lag_\" + str(i)]\n    return 0\n\nmatrix[\"delta_price_lag\"] = matrix.apply(select_trends, axis = 1)\nmatrix[\"delta_price_lag\"] = matrix.delta_price_lag.astype( np.float16 )\nmatrix[\"delta_price_lag\"].fillna( 0 ,inplace = True)\n\nfeatures_to_drop = [\"item_avg_item_price\", \"date_item_avg_item_price\"]\nfor i in lags:\n    features_to_drop.append(\"date_item_avg_item_price_lag_\" + str(i) )\n    features_to_drop.append(\"delta_price_lag_\" + str(i) )\nmatrix.drop(features_to_drop, axis = 1, inplace = True)\ntime.time() - ts","71922937":"ts = time.time()\ngroup = train.groupby( [\"date_block_num\",\"shop_id\"] ).agg({\"revenue\": [\"sum\"] })\ngroup.columns = [\"date_shop_revenue\"]\ngroup.reset_index(inplace = True)\n\nmatrix = matrix.merge( group , on = [\"date_block_num\", \"shop_id\"], how = \"left\" )\nmatrix['date_shop_revenue'] = matrix['date_shop_revenue'].astype(np.float32)\n\ngroup = group.groupby([\"shop_id\"]).agg({ \"date_block_num\":[\"mean\"] })\ngroup.columns = [\"shop_avg_revenue\"]\ngroup.reset_index(inplace = True )\n\nmatrix = matrix.merge( group, on = [\"shop_id\"], how = \"left\" )\nmatrix[\"shop_avg_revenue\"] = matrix.shop_avg_revenue.astype(np.float32)\nmatrix[\"delta_revenue\"] = (matrix['date_shop_revenue'] - matrix['shop_avg_revenue']) \/ matrix['shop_avg_revenue']\nmatrix[\"delta_revenue\"] = matrix[\"delta_revenue\"]. astype(np.float32)\n\nmatrix = lag_feature(matrix, [1], [\"delta_revenue\"])\nmatrix[\"delta_revenue_lag_1\"] = matrix[\"delta_revenue_lag_1\"].astype(np.float32)\nmatrix.drop( [\"date_shop_revenue\", \"shop_avg_revenue\", \"delta_revenue\"] ,axis = 1, inplace = True)\ntime.time() - ts","6228481c":"matrix[\"month\"] = matrix[\"date_block_num\"] % 12\ndays = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\nmatrix[\"days\"] = matrix[\"month\"].map(days).astype(np.int8)","21d7d37f":"ts = time.time()\nmatrix[\"item_shop_first_sale\"] = matrix[\"date_block_num\"] - matrix.groupby([\"item_id\",\"shop_id\"])[\"date_block_num\"].transform('min')\nmatrix[\"item_first_sale\"] = matrix[\"date_block_num\"] - matrix.groupby([\"item_id\"])[\"date_block_num\"].transform('min')\ntime.time() - ts","f99dd837":"matrix['Weekends'] = matrix['date_block_num'].map(n_weekends).fillna(10)\nmatrix['Holidays'] = matrix['date_block_num'].map(n_holidays).fillna(1)","20652efd":"matrix","5c00b04c":"ts = time.time()\ndata = matrix[matrix[\"date_block_num\"] > 3]\ntime.time() - ts","502c25dc":"from xgboost import XGBRegressor\nimport pickle","540543a4":"X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = data[data.date_block_num < 33]['item_cnt_month']\nX_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = data[data.date_block_num == 33]['item_cnt_month']\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)","305b0674":"model = XGBRegressor(\n    max_depth=9,\n    n_estimators=1000,\n    min_child_weight=1.5, \n    colsample_bytree=0.6, \n    subsample=0.7, \n    eta=0.01,\n#     tree_method='gpu_hist',\n    seed=42)\n\nmodel.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=True, \n    early_stopping_rounds = 20)","84c88b8d":"Y_pred = model.predict(X_valid).clip(0, 20)\nY_test = model.predict(X_test).clip(0, 20)\n\nsubmission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": Y_test\n})\nsubmission.to_csv('xgb_submission.csv', index=False)\n\n# save predictions for an ensemble\npickle.dump(Y_pred, open('xgb_train.pickle', 'wb'))\npickle.dump(Y_test, open('xgb_test.pickle', 'wb'))","393c605e":"from xgboost import plot_importance\nfig, ax = plt.subplots(1,1,figsize=(10,15))\nplot_importance(booster=model, ax=ax)","d9e3a284":"Does this also apply for items? do we have items that are not present in the training data? or that we do not have a lot of data ? ","1842e3fb":"Well no, a lot of shops are involved in the return transactions. **Which ones have the highest number of returns?** ","c3c12180":"So we have a 7356 back to shop transaction. ","a33d9507":"One of these shops actually only started in the last period of time! This would effect our model. ","5e8640e8":"No and this is quite logical. \n","cf627d51":"Here are some observations we can derive from this graph: \n* First, it seems like the number of sold items is decreasing in time\n* also we have two pulses of increasing number of transactions around month number 11\/12 and 23\/24. \n* it seems like we have a seasonality in the data maybe related to a yearly evolution. ","2c877ae5":"**Let's check how the total monthly sold items evolves in time!**","0cbcad63":"this number of categories is equal to the one of the item's data. ","2417be04":"So for the test we are only going to predict the sales for a 42 shop and a 5100 item. Even though we have 60 shops and 22170 item in total. Let's check how many items and shops we have in the training set","3568447d":"* Data Types","dc7bea8d":"### i. Train Data","62b5dfcd":"* Missing Values","211f892a":"We have an item priced over 300000 let's check this one. ","8fcd75be":"## b. Basic Data Investigation","49994e71":"**Do we have outliers?**","814085c5":"So here the list of the 10 shop that have the highest number of returns. **What about those with low returns?**","af6ca343":"* Checking for missing data","ebd5e824":"actually it is a single record so let's drop it. ","6a6f7527":"Apperently, we have shops in the training data that we won't predict for the test data. **Maybe we should get rid of these shops.**","c48985a2":"In the submitted version of my notebook I used the following versions of the libraries:\n* Numpy 1.18.5\n* Pandas 1.1.5\n* Matplotlib 3.2.1\n* Seaborn 0.10.0","46bd6de7":"### ii. Items Extra Data","fab4b1e7":"So our objective is to predict the count of items that would be sold per(shop and item).","a3d72252":"I googled some of the shops names and took a glance at some of the EDA notebooks. It seems like the shop name contains more information than just the name. The first word represents the city and the second represents the type of the shop. So let's get these features! ","e8baeba6":"* Statistic Summary","b4285ddd":"We also have items that do not exist in the training set. ","070fae23":"# 1. Exploratory Data Analysis (EDA)","1b3eb58d":"In this notebook, I present my approach to solve the challenge of predicting future sales described in the Coursera course on How to win data science competitions. \n\nThis notebook is oganized as follows: \n1. Exploratory Data Analysis\n2. Data Preprocessing including feature engineering\n3. Modeling including feature selection and model hyperparameters optimization\n4. Ensembling\n5. Submitting the predictions","b600c202":"This is also an outlier and should either be fixed or droped **Do we have other sales record of this item?**","6d28586c":"Yes and this is the list of the shops that are in the test set.","2500ae6b":"* Checking for data types","cf4f49b6":"Since data is categorical I will just present the number of unique values ","c455b40d":"Let's investigate the range of number of items sold per day (i.e., the target value)","44c2960d":"* Statistical Summary ","6fd644d9":"### iii. Item Categories","11d636d3":"**Q: Do we have entries with no sold items?**","dbbe2387":"### iv. Shops Data","e1951760":"Let's see how this return is doing in relation with time (monthly returns). ","daa724a2":"* Let's see how many entries represents the back to shop transactions","7c845114":"For this challenge, we have 6 provided csv-data files. These files are: \n* sales_train.csv contains the daily historical data from january 2013 to october 2015\n* test.csv contains the entries we are supposed to predict for november 2015\n* sample_submission.csv is an example of submission file.\n* items.csv presents supplemental information about the items\/products.\n* item_categories.csv  presents the supplemental information about the items categories.\n* shops.csv contains the supplemental information about the shops.\n\nLet's load the data and investigate it. ","4d7074c6":"Before getting if the date is a holiday we should redefine the date formula. ","fc0861e4":"It seems that the number of returns is quite seasonly and it is evolving in time (not stationary). ","8b5eeae6":"At first glance, it seems like the shop with id 25 is the responsible for all these items returns. **Let's check that out and compute the total number of returned items per shop id.** ","bbab0eb2":"**Do these shops exist in the test set?**","e988cd5e":"## b. Feature engineering","b710ccb2":"## a. Loading Data and Overall View","a889b1e1":"Although the figure is not very clear we can have a sens that most shops have the same shape in the sales as the total number of sales. We can also notice that some shops do not have data for all dates this means some shops have opened after a certain date. Let's check the first date of sales for each shop! ","da3bf4ae":"# 2. Data Preprocessing","2eb25da3":"* Statistical Summary","d7f249fa":"## a. Drop Outliers","28a905de":"I think these are outliers and should be dropped. ","510c94fc":"Here are my main ideas of feature engineering:\n* From the date find out if that day is holiday or weekend\n* Compute the number of holidays per month \n* get the month id\n* convert item category name \n* convert item name\n* get sales of previous month ","a324ac65":"### vi. Submission Sample","14cc027a":"Lets check which items are these that are sold above 1000 piece a day.","97adfce8":"* Missing Values?","bb0ffa99":"Yes we also have new items. \n\nHow would we use this in our model. ","9400348c":"And those are the shops with the lowest returns. ","5bb1c579":"### v. Test Data","405a1ec3":"It seems to me that we have X types of shops the \u0422\u0426, \u0422\u0420\u0426, \u0422\u041a, \u0422\u0420\u041a, MTP\u0426, other categories I'm not sure they are really categories so I will just join them in a single category I call other. ","e4966f2f":"**Let's check the evolution in time of the total count of sold items per month and per shop**","c8c2738f":"**Does the number of returns depends on the item id?** Let's get the list of the 10 most returned items and the 10 less returned ones. ","57b80f62":"### i. Train Data","929051c9":"So apparently we have some days when items are given back to the shop. While some items are sold by the thousands in a single day. ","aac44c40":"No need for the data types investigation let's go directly into the missing data.\n* Missing Data","91f63966":"* Statistical Summary","355d98bd":"**Let's get features from the shops dataframe**"}}