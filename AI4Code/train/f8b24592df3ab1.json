{"cell_type":{"2ba6de66":"code","ba4bf1b3":"code","b472e681":"code","98e7df40":"code","f4cca1dd":"code","85ff9293":"code","69fa792a":"code","e8bebe7b":"code","315c18a2":"code","974374b8":"code","6a171905":"code","0104fa72":"code","d7a86b72":"code","a069500c":"code","65a622f3":"code","36cb8ae8":"code","2662bd9d":"code","d3d29a94":"code","98f5d8c3":"code","34f04509":"code","ce31bce6":"code","445e9d6f":"code","ca40e496":"code","55294bb9":"code","877f18c6":"code","01d3d8a5":"code","6e73d8a4":"code","07c276c7":"code","43f2c734":"code","db90de1a":"code","a95672d2":"code","2179fb7f":"code","a7a049a8":"code","93c2b0c6":"code","7cae1ec2":"code","26d1efad":"code","ddea6fa6":"code","e4d28aab":"code","63bd90bf":"code","2524f17a":"code","fd09a452":"code","3ab38bba":"code","07e64fd2":"code","b82227dc":"code","19f94e5f":"code","d9b701a5":"code","4905593b":"code","2ac5a9af":"code","37a4764b":"code","2cf4d327":"code","7eaf4bc2":"code","dbe556de":"code","c77a348f":"code","08aacf73":"code","38c6f189":"code","c7a98314":"code","e1d460fd":"code","ac38cea6":"code","6bc68205":"code","e321bce1":"code","ae472e57":"code","f8e8e945":"code","b7032059":"code","41db6836":"code","64c5c0d6":"code","5ef1aae4":"code","4a6c6501":"code","232b667c":"code","e5d18dfd":"code","95b1f4fc":"code","ef968b54":"code","9d1a0ef7":"code","545a54be":"code","1341d2de":"code","031d9bd6":"code","1371cd9c":"code","0bda8199":"code","0553c059":"code","7870f9a9":"code","e8c575f8":"code","a3f475bf":"code","09e80c1d":"code","db2d26be":"code","184307db":"code","37d1988c":"code","d8eebfc0":"code","5e278385":"code","651dc8ed":"code","da6c946c":"code","b88390a0":"code","9db3a6b8":"code","5eafe0da":"code","4f7dc825":"code","c123fa42":"code","0396f533":"code","b62bed1a":"code","85a1a084":"code","4cd39891":"code","806b473f":"code","e08e24c5":"code","c491d598":"code","2f9bcb92":"markdown","a535e80e":"markdown","fdd0685f":"markdown","edc88dbb":"markdown","f7d3f265":"markdown","e9bbbb70":"markdown","fa586078":"markdown","e7e34833":"markdown","9974a1f6":"markdown","e92d07f4":"markdown","8b930140":"markdown","5a9336a6":"markdown","7ea75bbc":"markdown","c52b053f":"markdown","a79fd2dc":"markdown","aac34d7b":"markdown","dfb34854":"markdown","a8b57570":"markdown","81afef4e":"markdown","2347fe83":"markdown","52d19f8f":"markdown","540fe3ca":"markdown","54b6bc16":"markdown","9013dd09":"markdown","5f6680bb":"markdown","deb0dc40":"markdown","082f94f7":"markdown","a967eba6":"markdown","372dd81a":"markdown","04d4249a":"markdown","850ce1c9":"markdown","cf6d694e":"markdown","cd75435e":"markdown","0ca02236":"markdown","45d24256":"markdown","155a1eb4":"markdown","923d0c2b":"markdown","1db90a6c":"markdown","45752bcc":"markdown","b3beb651":"markdown","3ac6659d":"markdown","41097bc5":"markdown","2358136c":"markdown","22995c26":"markdown","c4faaad7":"markdown","a688680e":"markdown","1c926a5e":"markdown","9acc73ac":"markdown","2da43521":"markdown","1223dbd4":"markdown","99057788":"markdown","86345c28":"markdown","8c0f1956":"markdown","53553f0b":"markdown","8ea3b706":"markdown","8b2d5735":"markdown","468e4b05":"markdown","3f03ce02":"markdown","82caeaba":"markdown"},"source":{"2ba6de66":"import pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set(font_scale=2.2)\nplt.style.use('seaborn')\n\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, ShuffleSplit\nfrom sklearn.metrics import f1_score\nimport itertools\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nimport shap\nfrom tqdm import tqdm\nimport featuretools as ft\nimport warnings \nwarnings.filterwarnings('ignore')\nimport time\n","ba4bf1b3":"df_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')","b472e681":"print('df_train shape:', df_train.shape, '  ', 'df_test shape: ', df_test.shape)","98e7df40":"df_train.head()","f4cca1dd":"df_test.head()","85ff9293":"description = [\n(\"v2a1\",\" Monthly rent payment\"),\n(\"hacdor\",\" =1 Overcrowding by bedrooms\"),\n(\"rooms\",\"  number of all rooms in the house\"),\n(\"hacapo\",\" =1 Overcrowding by rooms\"),\n(\"v14a\",\" =1 has toilet in the household\"),\n(\"refrig\",\" =1 if the household has refrigerator\"),\n(\"v18q\",\" owns a tablet\"),\n(\"v18q1\",\" number of tablets household owns\"),\n(\"r4h1\",\" Males younger than 12 years of age\"),\n(\"r4h2\",\" Males 12 years of age and older\"),\n(\"r4h3\",\" Total males in the household\"),\n(\"r4m1\",\" Females younger than 12 years of age\"),\n(\"r4m2\",\" Females 12 years of age and older\"),\n(\"r4m3\",\" Total females in the household\"),\n(\"r4t1\",\" persons younger than 12 years of age\"),\n(\"r4t2\",\" persons 12 years of age and older\"),\n(\"r4t3\",\" Total persons in the household\"),\n(\"tamhog\",\" size of the household\"),\n(\"tamviv\",\" number of persons living in the household\"),\n(\"escolari\",\" years of schooling\"),\n(\"rez_esc\",\" Years behind in school\"),\n(\"hhsize\",\" household size\"),\n(\"paredblolad\",\" =1 if predominant material on the outside wall is block or brick\"),\n(\"paredzocalo\",\" =1 if predominant material on the outside wall is socket (wood, zinc or absbesto\"),\n(\"paredpreb\",\" =1 if predominant material on the outside wall is prefabricated or cement\"),\n(\"pareddes\",\" =1 if predominant material on the outside wall is waste material\"),\n(\"paredmad\",\" =1 if predominant material on the outside wall is wood\"),\n(\"paredzinc\",\" =1 if predominant material on the outside wall is zink\"),\n(\"paredfibras\",\" =1 if predominant material on the outside wall is natural fibers\"),\n(\"paredother\",\" =1 if predominant material on the outside wall is other\"),\n(\"pisomoscer\",\" =1 if predominant material on the floor is mosaic ceramic   terrazo\"),\n(\"pisocemento\",\" =1 if predominant material on the floor is cement\"),\n(\"pisoother\",\" =1 if predominant material on the floor is other\"),\n(\"pisonatur\",\" =1 if predominant material on the floor is  natural material\"),\n(\"pisonotiene\",\" =1 if no floor at the household\"),\n(\"pisomadera\",\" =1 if predominant material on the floor is wood\"),\n(\"techozinc\",\" =1 if predominant material on the roof is metal foil or zink\"),\n(\"techoentrepiso\",\" =1 if predominant material on the roof is fiber cement,   mezzanine \"),\n(\"techocane\",\" =1 if predominant material on the roof is natural fibers\"),\n(\"techootro\",\" =1 if predominant material on the roof is other\"),\n(\"cielorazo\",\" =1 if the house has ceiling\"),\n(\"abastaguadentro\",\" =1 if water provision inside the dwelling\"),\n(\"abastaguafuera\",\" =1 if water provision outside the dwelling\"),\n(\"abastaguano\",\" =1 if no water provision\"),\n(\"public\",\" =1 electricity from CNFL,  ICE, ESPH\/JASEC\"),\n(\"planpri\",\" =1 electricity from private plant\"),\n(\"noelec\",\" =1 no electricity in the dwelling\"),\n(\"coopele\",\" =1 electricity from cooperative\"),\n(\"sanitario1\",\" =1 no toilet in the dwelling\"),\n(\"sanitario2\",\" =1 toilet connected to sewer or cesspool\"),\n(\"sanitario3\",\" =1 toilet connected to  septic tank\"),\n(\"sanitario5\",\" =1 toilet connected to black hole or letrine\"),\n(\"sanitario6\",\" =1 toilet connected to other system\"),\n(\"energcocinar1\",\" =1 no main source of energy used for cooking (no kitchen)\"),\n(\"energcocinar2\",\" =1 main source of energy used for cooking electricity\"),\n(\"energcocinar3\",\" =1 main source of energy used for cooking gas\"),\n(\"energcocinar4\",\" =1 main source of energy used for cooking wood charcoal\"),\n(\"elimbasu1\",\" =1 if rubbish disposal mainly by tanker truck\"),\n(\"elimbasu2\",\" =1 if rubbish disposal mainly by botan hollow or buried\"),\n(\"elimbasu3\",\" =1 if rubbish disposal mainly by burning\"),\n(\"elimbasu4\",\" =1 if rubbish disposal mainly by throwing in an unoccupied space\"),\n(\"elimbasu5\",\" =1 if rubbish disposal mainly by throwing in river,   creek or sea\"),\n(\"elimbasu6\",\" =1 if rubbish disposal mainly other\"),\n(\"epared1\",\" =1 if walls are bad\"),\n(\"epared2\",\" =1 if walls are regular\"),\n(\"epared3\",\" =1 if walls are good\"),\n(\"etecho1\",\" =1 if roof are bad\"),\n(\"etecho2\",\" =1 if roof are regular\"),\n(\"etecho3\",\" =1 if roof are good\"),\n(\"eviv1\",\" =1 if floor are bad\"),\n(\"eviv2\",\" =1 if floor are regular\"),\n(\"eviv3\",\" =1 if floor are good\"),\n(\"dis\",\" =1 if disable person\"),\n(\"male\",\" =1 if male\"),\n(\"female\",\" =1 if female\"),\n(\"estadocivil1\",\" =1 if less than 10 years old\"),\n(\"estadocivil2\",\" =1 if free or coupled uunion\"),\n(\"estadocivil3\",\" =1 if married\"),\n(\"estadocivil4\",\" =1 if divorced\"),\n(\"estadocivil5\",\" =1 if separated\"),\n(\"estadocivil6\",\" =1 if widow\/er\"),\n(\"estadocivil7\",\" =1 if single\"),\n(\"parentesco1\",\" =1 if household head\"),\n(\"parentesco2\",\" =1 if spouse\/partner\"),\n(\"parentesco3\",\" =1 if son\/doughter\"),\n(\"parentesco4\",\" =1 if stepson\/doughter\"),\n(\"parentesco5\",\" =1 if son\/doughter in law\"),\n(\"parentesco6\",\" =1 if grandson\/doughter\"),\n(\"parentesco7\",\" =1 if mother\/father\"),\n(\"parentesco8\",\" =1 if father\/mother in law\"),\n(\"parentesco9\",\" =1 if brother\/sister\"),\n(\"parentesco10\",\" =1 if brother\/sister in law\"),\n(\"parentesco11\",\" =1 if other family member\"),\n(\"parentesco12\",\" =1 if other non family member\"),\n(\"idhogar\",\" Household level identifier\"),\n(\"hogar_nin\",\" Number of children 0 to 19 in household\"),\n(\"hogar_adul\",\" Number of adults in household\"),\n(\"hogar_mayor\",\" # of individuals 65+ in the household\"),\n(\"hogar_total\",\" # of total individuals in the household\"),\n(\"dependency\",\" Dependency rate\"),\n(\"edjefe\",\" years of education of male head of household\"),\n(\"edjefa\",\" years of education of female head of household\"),\n(\"meaneduc\",\"average years of education for adults (18+)\"),\n(\"instlevel1\",\" =1 no level of education\"),\n(\"instlevel2\",\" =1 incomplete primary\"),\n(\"instlevel3\",\" =1 complete primary\"),\n(\"instlevel4\",\" =1 incomplete academic secondary level\"),\n(\"instlevel5\",\" =1 complete academic secondary level\"),\n(\"instlevel6\",\" =1 incomplete technical secondary level\"),\n(\"instlevel7\",\" =1 complete technical secondary level\"),\n(\"instlevel8\",\" =1 undergraduate and higher education\"),\n(\"instlevel9\",\" =1 postgraduate higher education\"),\n(\"bedrooms\",\" number of bedrooms\"),\n(\"overcrowding\",\" # persons per room\"),\n(\"tipovivi1\",\" =1 own and fully paid house\"),\n(\"tipovivi2\",\" =1 own,   paying in installments\"),\n(\"tipovivi3\",\" =1 rented\"),\n(\"tipovivi4\",\" =1 precarious\"),\n(\"tipovivi5\",\" =1 other(assigned\"),\n(\"computer\",\" =1 if the household has notebook or desktop computer,   borrowed)\"),\n(\"television\",\" =1 if the household has TV\"),\n(\"mobilephone\",\" =1 if mobile phone\"),\n(\"qmobilephone\",\" # of mobile phones\"),\n(\"lugar1\",\" =1 region Central\"),\n(\"lugar2\",\" =1 region Chorotega\"),\n(\"lugar3\",\" =1 region Pac\u00c3\u0192\u00c2\u00adfico central\"),\n(\"lugar4\",\" =1 region Brunca\"),\n(\"lugar5\",\" =1 region Huetar Atl\u00c3\u0192\u00c2\u00a1ntica\"),\n(\"lugar6\",\" =1 region Huetar Norte\"),\n(\"area1\",\" =1 zona urbana\"),\n(\"area2\",\" =2 zona rural\"),\n(\"age\",\" Age in years\"),\n(\"SQBescolari\",\" escolari squared\"),\n(\"SQBage\",\" age squared\"),\n(\"SQBhogar_total\",\" hogar_total squared\"),\n(\"SQBedjefe\",\" edjefe squared\"),\n(\"SQBhogar_nin\",\" hogar_nin squared\"),\n(\"SQBovercrowding\",\" overcrowding squared\"),\n(\"SQBdependency\",\" dependency squared\"),\n(\"SQBmeaned\",\" meaned squared\"),\n(\"agesq\",\" Age squared\"),]\n\ndescription = pd.DataFrame(description, columns=['varname', 'description'])","69fa792a":"description","e8bebe7b":"total = df_train.isnull().sum().sort_values(ascending=False)\npercent = 100 * (df_train.isnull().sum() \/ df_train.isnull().count()).sort_values(ascending=False)\nmissing_df = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n\nmissing_df.head(20)","315c18a2":"# if education is \"yes\" and person is head of household, fill with escolari\ndf_train.loc[(df_train['edjefa'] == \"yes\") & (df_train['parentesco1'] == 1), \"edjefa\"] = df_train.loc[(df_train['edjefa'] == \"yes\") & (df_train['parentesco1'] == 1), \"escolari\"]\ndf_train.loc[(df_train['edjefe'] == \"yes\") & (df_train['parentesco1'] == 1), \"edjefe\"] = df_train.loc[(df_train['edjefe'] == \"yes\") & (df_train['parentesco1'] == 1), \"escolari\"]\n\ndf_test.loc[(df_test['edjefa'] == \"yes\") & (df_test['parentesco1'] == 1), \"edjefa\"] = df_test.loc[(df_test['edjefa'] == \"yes\") & (df_test['parentesco1'] == 1), \"escolari\"]\ndf_test.loc[(df_test['edjefe'] == \"yes\") & (df_test['parentesco1'] == 1), \"edjefe\"] = df_test.loc[(df_test['edjefe'] == \"yes\") & (df_test['parentesco1'] == 1), \"escolari\"]\n\n# this field is supposed to be interaction between gender and escolari, but it isn't clear what \"yes\" means, let's fill it with 4\ndf_train.loc[df_train['edjefa'] == \"yes\", \"edjefa\"] = 4\ndf_train.loc[df_train['edjefe'] == \"yes\", \"edjefe\"] = 4\n\ndf_test.loc[df_test['edjefa'] == \"yes\", \"edjefa\"] = 4\ndf_test.loc[df_test['edjefe'] == \"yes\", \"edjefe\"] = 4\n\n# create feature with max education of either head of household\ndf_train['edjef'] = np.max(df_train[['edjefa','edjefe']], axis=1)\ndf_test['edjef'] = np.max(df_test[['edjefa','edjefe']], axis=1)\n\n# fix some inconsistencies in the data - some rows indicate both that the household does and does not have a toilet, \n# if there is no water we'll assume they do not\ndf_train.loc[(df_train.v14a ==  1) & (df_train.sanitario1 ==  1) & (df_train.abastaguano == 0), \"v14a\"] = 0\ndf_train.loc[(df_train.v14a ==  1) & (df_train.sanitario1 ==  1) & (df_train.abastaguano == 0), \"sanitario1\"] = 0\n\ndf_test.loc[(df_test.v14a ==  1) & (df_test.sanitario1 ==  1) & (df_test.abastaguano == 0), \"v14a\"] = 0\ndf_test.loc[(df_test.v14a ==  1) & (df_test.sanitario1 ==  1) & (df_test.abastaguano == 0), \"sanitario1\"] = 0","974374b8":"df_train['rez_esc'].fillna(0, inplace=True)\ndf_test['rez_esc'].fillna(0, inplace=True)","6a171905":"df_train['SQBmeaned'].fillna(0, inplace=True)\ndf_test['SQBmeaned'].fillna(0, inplace=True)","0104fa72":"df_train['meaneduc'].fillna(0, inplace=True)\ndf_test['meaneduc'].fillna(0, inplace=True)","d7a86b72":"df_train['v18q'].value_counts()","a069500c":"df_train.loc[df_train['v18q'] == 1, 'v18q1'].value_counts()","65a622f3":"df_train.loc[df_train['v18q'] == 0, 'v18q1'].value_counts()","36cb8ae8":"df_train['v18q1'].fillna(0, inplace=True)\ndf_test['v18q1'].fillna(0, inplace=True)","2662bd9d":"df_train['tipovivi3'].value_counts()","d3d29a94":"sns.kdeplot(df_train.loc[df_train['tipovivi3'] == 1, 'v2a1'], label='Monthly rent payment of household(rented=1)')\nsns.kdeplot(df_train.loc[df_train['tipovivi3'] == 0, 'v2a1'], label='Monthly rent payment of household(rented=0)')\nplt.xscale('log')\nplt.show()","98f5d8c3":"df_train['v2a1'].fillna(0, inplace=True)\ndf_test['v2a1'].fillna(0, inplace=True)","34f04509":"total = df_train.isnull().sum().sort_values(ascending=False)\npercent = 100 * (df_train.isnull().sum() \/ df_train.isnull().count()).sort_values(ascending=False)\nmissing_df = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n\nmissing_df.head(20)","ce31bce6":"total = df_test.isnull().sum().sort_values(ascending=False)\npercent = 100 * (df_test.isnull().sum() \/ df_test.isnull().count()).sort_values(ascending=False)\nmissing_df = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n\nmissing_df.head(20)","445e9d6f":"features_object = [col for col in df_train.columns if df_train[col].dtype == 'object']","ca40e496":"features_object","55294bb9":"# some dependencies are Na, fill those with the square root of the square\ndf_train['dependency'] = np.sqrt(df_train['SQBdependency'])\ndf_test['dependency'] = np.sqrt(df_test['SQBdependency'])","877f18c6":"# df_train['dependency'] = df_train['dependency'].replace({np.inf: 0})\n# df_test['dependency'] = df_test['dependency'].replace({np.inf: 0})\n\n# def replace_dependency(x):\n#     if x == 'yes':\n#         return 10\n#     elif x == 'no':\n#         return 0\n#     else:\n#         return x\n\n# df_train['dependency'] = df_train['dependency'].apply(replace_dependency).astype(float)\n# df_test['dependency'] = df_test['dependency'].apply(replace_dependency).astype(float)\n\n# - As you can see, setting yes -> 10 and no -> 0 is good choice.\n# - At first, fill inf value with 0.","01d3d8a5":"def replace_edjefe(x):\n    if x == 'yes':\n        return 1\n    elif x == 'no':\n        return 0\n    else:\n        return x\n\ndf_train['edjefe'] = df_train['edjefe'].apply(replace_edjefe).astype(float)\ndf_test['edjefe'] = df_test['edjefe'].apply(replace_edjefe).astype(float)","6e73d8a4":"def replace_edjefa(x):\n    if x == 'yes':\n        return 1\n    elif x == 'no':\n        return 0\n    else:\n        return x\n\ndf_train['edjefa'] = df_train['edjefa'].apply(replace_edjefa).astype(float)\ndf_test['edjefa'] = df_test['edjefa'].apply(replace_edjefa).astype(float)","07c276c7":"# create feature with max education of either head of household\ndf_train['edjef'] = np.max(df_train[['edjefa','edjefe']], axis=1)\ndf_test['edjef'] = np.max(df_test[['edjefa','edjefe']], axis=1)","43f2c734":"df_train['roof_waste_material'] = np.nan\ndf_test['roof_waste_material'] = np.nan\ndf_train['electricity_other'] = np.nan\ndf_test['electricity_other'] = np.nan\n\ndef fill_roof_exception(x):\n    if (x['techozinc'] == 0) and (x['techoentrepiso'] == 0) and (x['techocane'] == 0) and (x['techootro'] == 0):\n        return 1\n    else:\n        return 0\n    \ndef fill_no_electricity(x):\n    if (x['public'] == 0) and (x['planpri'] == 0) and (x['noelec'] == 0) and (x['coopele'] == 0):\n        return 1\n    else:\n        return 0\n\ndf_train['roof_waste_material'] = df_train.apply(lambda x : fill_roof_exception(x),axis=1)\ndf_test['roof_waste_material'] = df_test.apply(lambda x : fill_roof_exception(x),axis=1)\ndf_train['electricity_other'] = df_train.apply(lambda x : fill_no_electricity(x),axis=1)\ndf_test['electricity_other'] = df_test.apply(lambda x : fill_no_electricity(x),axis=1)","db90de1a":"binary_cat_features = [col for col in df_train.columns if df_train[col].value_counts().shape[0] == 2]","a95672d2":"continuous_features = [col for col in df_train.columns if col not in binary_cat_features]\ncontinuous_features = [col for col in continuous_features if col not in features_object]\ncontinuous_features = [col for col in continuous_features if col not in ['Id', 'Target', 'idhogar']]","2179fb7f":"print('There are {} continuous features'.format(len(continuous_features)))\nfor col in continuous_features:\n    print('{}: {}'.format(col, description.loc[description['varname'] == col, 'description'].values))","a7a049a8":"df_train['edjef'].value_counts()","93c2b0c6":"df_train.drop('tamhog', axis=1, inplace=True)\ndf_test.drop('tamhog', axis=1, inplace=True)","7cae1ec2":"df_train['adult'] = df_train['hogar_adul'] - df_train['hogar_mayor']\ndf_train['dependency_count'] = df_train['hogar_nin'] + df_train['hogar_mayor']\ndf_train['dependency'] = df_train['dependency_count'] \/ df_train['adult']\ndf_train['child_percent'] = df_train['hogar_nin'] \/ df_train['hogar_total']\ndf_train['elder_percent'] = df_train['hogar_mayor'] \/ df_train['hogar_total']\ndf_train['adult_percent'] = df_train['hogar_adul'] \/ df_train['hogar_total']\ndf_train['males_younger_12_years_percent'] = df_train['r4h1'] \/ df_train['hogar_total']\ndf_train['males_older_12_years_percent'] = df_train['r4h2'] \/ df_train['hogar_total']\ndf_train['males_percent'] = df_train['r4h3'] \/ df_train['hogar_total']\ndf_train['females_younger_12_years_percent'] = df_train['r4m1'] \/ df_train['hogar_total']\ndf_train['females_older_12_years_percent'] = df_train['r4m2'] \/ df_train['hogar_total']\ndf_train['females_percent'] = df_train['r4m3'] \/ df_train['hogar_total']\ndf_train['persons_younger_12_years_percent'] = df_train['r4t1'] \/ df_train['hogar_total']\ndf_train['persons_older_12_years_percent'] = df_train['r4t2'] \/ df_train['hogar_total']\ndf_train['persons_percent'] = df_train['r4t3'] \/ df_train['hogar_total']","26d1efad":"df_test['adult'] = df_test['hogar_adul'] - df_test['hogar_mayor']\ndf_test['dependency_count'] = df_test['hogar_nin'] + df_test['hogar_mayor']\ndf_test['dependency'] = df_test['dependency_count'] \/ df_test['adult']\ndf_test['child_percent'] = df_test['hogar_nin'] \/ df_test['hogar_total']\ndf_test['elder_percent'] = df_test['hogar_mayor'] \/ df_test['hogar_total']\ndf_test['adult_percent'] = df_test['hogar_adul'] \/ df_test['hogar_total']\ndf_test['males_younger_12_years_percent'] = df_test['r4h1'] \/ df_test['hogar_total']\ndf_test['males_older_12_years_percent'] = df_test['r4h2'] \/ df_test['hogar_total']\ndf_test['males_percent'] = df_test['r4h3'] \/ df_test['hogar_total']\ndf_test['females_younger_12_years_percent'] = df_test['r4m1'] \/ df_test['hogar_total']\ndf_test['females_older_12_years_percent'] = df_test['r4m2'] \/ df_test['hogar_total']\ndf_test['females_percent'] = df_test['r4m3'] \/ df_test['hogar_total']\ndf_test['persons_younger_12_years_percent'] = df_test['r4t1'] \/ df_test['hogar_total']\ndf_test['persons_older_12_years_percent'] = df_test['r4t2'] \/ df_test['hogar_total']\ndf_test['persons_percent'] = df_test['r4t3'] \/ df_test['hogar_total']","ddea6fa6":"df_train['males_younger_12_years_in_household_size'] = df_train['r4h1'] \/ df_train['hhsize']\ndf_train['males_older_12_years_in_household_size'] = df_train['r4h2'] \/ df_train['hhsize']\ndf_train['males_in_household_size'] = df_train['r4h3'] \/ df_train['hhsize']\ndf_train['females_younger_12_years_in_household_size'] = df_train['r4m1'] \/ df_train['hhsize']\ndf_train['females_older_12_years_in_household_size'] = df_train['r4m2'] \/ df_train['hhsize']\ndf_train['females_in_household_size'] = df_train['r4m3'] \/ df_train['hogar_total']\ndf_train['persons_younger_12_years_in_household_size'] = df_train['r4t1'] \/ df_train['hhsize']\ndf_train['persons_older_12_years_in_household_size'] = df_train['r4t2'] \/ df_train['hhsize']\ndf_train['persons_in_household_size'] = df_train['r4t3'] \/ df_train['hhsize']","e4d28aab":"df_test['males_younger_12_years_in_household_size'] = df_test['r4h1'] \/ df_test['hhsize']\ndf_test['males_older_12_years_in_household_size'] = df_test['r4h2'] \/ df_test['hhsize']\ndf_test['males_in_household_size'] = df_test['r4h3'] \/ df_test['hhsize']\ndf_test['females_younger_12_years_in_household_size'] = df_test['r4m1'] \/ df_test['hhsize']\ndf_test['females_older_12_years_in_household_size'] = df_test['r4m2'] \/ df_test['hhsize']\ndf_test['females_in_household_size'] = df_test['r4m3'] \/ df_test['hogar_total']\ndf_test['persons_younger_12_years_in_household_size'] = df_test['r4t1'] \/ df_test['hhsize']\ndf_test['persons_older_12_years_in_household_size'] = df_test['r4t2'] \/ df_test['hhsize']\ndf_test['persons_in_household_size'] = df_test['r4t3'] \/ df_test['hhsize']","63bd90bf":"df_train['overcrowding_room_and_bedroom'] = (df_train['hacdor'] + df_train['hacapo'])\/2\ndf_test['overcrowding_room_and_bedroom'] = (df_test['hacdor'] + df_test['hacapo'])\/2","2524f17a":"df_train['escolari_age'] = df_train['escolari']\/df_train['age']\ndf_test['escolari_age'] = df_test['escolari']\/df_test['age']\n\ndf_train['age_12_19'] = df_train['hogar_nin'] - df_train['r4t1']\ndf_test['age_12_19'] = df_test['hogar_nin'] - df_test['r4t1']  ","fd09a452":"df_train['phones-per-capita'] = df_train['qmobilephone'] \/ df_train['tamviv']\ndf_train['tablets-per-capita'] = df_train['v18q1'] \/ df_train['tamviv']\ndf_train['rooms-per-capita'] = df_train['rooms'] \/ df_train['tamviv']\ndf_train['rent-per-capita'] = df_train['v2a1'] \/ df_train['tamviv']","3ab38bba":"df_test['phones-per-capita'] = df_test['qmobilephone'] \/ df_test['tamviv']\ndf_test['tablets-per-capita'] = df_test['v18q1'] \/ df_test['tamviv']\ndf_test['rooms-per-capita'] = df_test['rooms'] \/ df_test['tamviv']\ndf_test['rent-per-capita'] = df_test['v2a1'] \/ df_test['tamviv']","07e64fd2":"(df_train['hogar_total'] == df_train['r4t3']).sum()","b82227dc":"family_size_features = ['adult', 'hogar_adul', 'hogar_mayor', 'hogar_nin', 'hogar_total', 'r4h1', \n                        'r4h2', 'r4h3', 'r4m1', 'r4m2', 'r4m3', 'r4t1', 'r4t2', 'r4t3', 'hhsize']\nnew_feats = []\nfor col in family_size_features:\n    new_col_name = 'new_{}_per_{}'.format('v2a1', col)\n    new_feats.append(new_col_name)\n    df_train[new_col_name] = df_train['v2a1'] \/ df_train[col]\n    df_test[new_col_name] = df_test['v2a1'] \/ df_test[col]","19f94e5f":"for col in new_feats:\n    df_train[col].replace([np.inf], np.nan, inplace=True)\n    df_train[col].fillna(0, inplace=True)\n    \n    df_test[col].replace([np.inf], np.nan, inplace=True)\n    df_test[col].fillna(0, inplace=True)","d9b701a5":"new_feats = []\nfor col in family_size_features:\n    new_col_name = 'new_{}_per_{}'.format('rooms', col)\n    new_feats.append(new_col_name)\n    df_train[new_col_name] = df_train['rooms'] \/ df_train[col]\n    df_test[new_col_name] = df_test['rooms'] \/ df_test[col]\n\nfor col in new_feats:\n    df_train[col].replace([np.inf], np.nan, inplace=True)\n    df_train[col].fillna(0, inplace=True)\n    \n    df_test[col].replace([np.inf], np.nan, inplace=True)\n    df_test[col].fillna(0, inplace=True)","4905593b":"new_feats = []\nfor col in family_size_features:\n    new_col_name = 'new_{}_per_{}'.format('bedrooms', col)\n    new_feats.append(new_col_name)\n    df_train[new_col_name] = df_train['bedrooms'] \/ df_train[col]\n    df_test[new_col_name] = df_test['bedrooms'] \/ df_test[col]\n\nfor col in new_feats:\n    df_train[col].replace([np.inf], np.nan, inplace=True)\n    df_train[col].fillna(0, inplace=True)\n    \n    df_test[col].replace([np.inf], np.nan, inplace=True)\n    df_test[col].fillna(0, inplace=True)","2ac5a9af":"print(df_train.shape, df_test.shape) # To check the same number of features between train and test (target is there in train)","37a4764b":"new_feats = []\nfor col in family_size_features:\n    new_col_name = 'new_{}_per_{}'.format('v18q1', col)\n    new_feats.append(new_col_name)\n    df_train[new_col_name] = df_train['v18q1'] \/ df_train[col]\n    df_test[new_col_name] = df_test['v18q1'] \/ df_test[col]\n\nfor col in new_feats:\n    df_train[col].replace([np.inf], np.nan, inplace=True)\n    df_train[col].fillna(0, inplace=True)\n    \n    df_test[col].replace([np.inf], np.nan, inplace=True)\n    df_test[col].fillna(0, inplace=True)","2cf4d327":"new_feats = []\nfor col in family_size_features:\n    new_col_name = 'new_{}_per_{}'.format('qmobilephone', col)\n    new_feats.append(new_col_name)\n    df_train[new_col_name] = df_train['qmobilephone'] \/ df_train[col]\n    df_test[new_col_name] = df_test['qmobilephone'] \/ df_test[col]\n\nfor col in new_feats:\n    df_train[col].replace([np.inf], np.nan, inplace=True)\n    df_train[col].fillna(0, inplace=True)\n    \n    df_test[col].replace([np.inf], np.nan, inplace=True)\n    df_test[col].fillna(0, inplace=True)","7eaf4bc2":"new_feats = []\nfor col in family_size_features:\n    new_col_name = 'new_{}_per_{}'.format('rez_esc', col)\n    new_feats.append(new_col_name)\n    df_train[new_col_name] = df_train['rez_esc'] \/ df_train[col]\n    df_test[new_col_name] = df_test['rez_esc'] \/ df_test[col]\n\nfor col in new_feats:\n    df_train[col].replace([np.inf], np.nan, inplace=True)\n    df_train[col].fillna(0, inplace=True)\n    \n    df_test[col].replace([np.inf], np.nan, inplace=True)\n    df_test[col].fillna(0, inplace=True)","dbe556de":"df_train['rez_esc_age'] = df_train['rez_esc'] \/ df_train['age']\ndf_train['rez_esc_escolari'] = df_train['rez_esc'] \/ df_train['escolari']\n\ndf_test['rez_esc_age'] = df_test['rez_esc'] \/ df_test['age']\ndf_test['rez_esc_escolari'] = df_test['rez_esc'] \/ df_test['escolari']","c77a348f":"df_train['tabulet_x_qmobilephone'] = df_train['v18q1'] * df_train['qmobilephone']\ndf_test['tabulet_x_qmobilephone'] = df_test['v18q1'] * df_test['qmobilephone']","08aacf73":"# wall and roof\nfor col1 in ['epared1', 'epared2', 'epared3']:\n    for col2 in ['etecho1', 'etecho2', 'etecho3']:\n        new_col_name = 'new_{}_x_{}'.format(col1, col2)\n        df_train[new_col_name] = df_train[col1] * df_train[col2]\n        df_test[new_col_name] = df_test[col1] * df_test[col2]\n        \n# wall and floor\nfor col1 in ['epared1', 'epared2', 'epared3']:\n    for col2 in ['eviv1', 'eviv2', 'eviv3']:\n        new_col_name = 'new_{}_x_{}'.format(col1, col2)\n        df_train[new_col_name] = df_train[col1] * df_train[col2]\n        df_test[new_col_name] = df_test[col1] * df_test[col2]\n\n# roof and floor\nfor col1 in ['etecho1', 'etecho2', 'etecho3']:\n    for col2 in ['eviv1', 'eviv2', 'eviv3']:\n        new_col_name = 'new_{}_x_{}'.format(col1, col2)\n        df_train[new_col_name] = df_train[col1] * df_train[col2]\n        df_test[new_col_name] = df_test[col1] * df_test[col2]","38c6f189":"for col1 in ['epared1', 'epared2', 'epared3']:\n    for col2 in ['etecho1', 'etecho2', 'etecho3']:\n        for col3 in ['eviv1', 'eviv2', 'eviv3']:\n            new_col_name = 'new_{}_x_{}_x_{}'.format(col1, col2, col3)\n            df_train[new_col_name] = df_train[col1] * df_train[col2] * df_train[col3]\n            df_test[new_col_name] = df_test[col1] * df_test[col2] * df_train[col3]","c7a98314":"print(df_train.shape, df_test.shape)","e1d460fd":"for col1 in ['public', 'planpri', 'noelec', 'coopele']:\n    for col2 in ['energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4']:\n        new_col_name = 'new_{}_x_{}'.format(col1, col2)\n        df_train[new_col_name] = df_train[col1] * df_train[col2]\n        df_test[new_col_name] = df_test[col1] * df_test[col2]","ac38cea6":"for col1 in ['sanitario1', 'sanitario2', 'sanitario3', 'sanitario5', 'sanitario6']:\n    for col2 in ['elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', 'elimbasu5', 'elimbasu6']:\n        new_col_name = 'new_{}_x_{}'.format(col1, col2)\n        df_train[new_col_name] = df_train[col1] * df_train[col2]\n        df_test[new_col_name] = df_test[col1] * df_test[col2]","6bc68205":"for col1 in ['abastaguadentro', 'abastaguafuera', 'abastaguano']:\n    for col2 in ['sanitario1', 'sanitario2', 'sanitario3', 'sanitario5', 'sanitario6']:\n        new_col_name = 'new_{}_x_{}'.format(col1, col2)\n        df_train[new_col_name] = df_train[col1] * df_train[col2]\n        df_test[new_col_name] = df_test[col1] * df_test[col2]","e321bce1":"print(df_train.shape, df_test.shape)","ae472e57":"for col1 in ['area1', 'area2']:\n    for col2 in ['instlevel1', 'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', 'instlevel9']:\n        new_col_name = 'new_{}_x_{}'.format(col1, col2)\n        df_train[new_col_name] = df_train[col1] * df_train[col2]\n        df_test[new_col_name] = df_test[col1] * df_test[col2]","f8e8e945":"for col1 in ['lugar1', 'lugar2', 'lugar3', 'lugar4', 'lugar5', 'lugar6']:\n    for col2 in ['instlevel1', 'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', 'instlevel9']:\n        new_col_name = 'new_{}_x_{}'.format(col1, col2)\n        df_train[new_col_name] = df_train[col1] * df_train[col2]\n        df_test[new_col_name] = df_test[col1] * df_test[col2]","b7032059":"print(df_train.shape, df_test.shape)","41db6836":"df_train['electronics'] = df_train['computer'] * df_train['mobilephone'] * df_train['television'] * df_train['v18q'] * df_train['refrig']\ndf_test['electronics'] = df_test['computer'] * df_test['mobilephone'] * df_test['television'] * df_test['v18q'] * df_test['refrig']\n\ndf_train['no_appliances'] = df_train['refrig'] + df_train['computer'] + df_train['television'] + df_train['mobilephone']\ndf_test['no_appliances'] = df_test['refrig'] + df_test['computer'] + df_test['television'] + df_test['mobilephone']","64c5c0d6":"for col1 in ['paredblolad', 'paredzocalo', 'paredpreb', 'pareddes', 'paredmad', 'paredzinc', 'paredfibras', 'paredother']:\n    for col2 in ['pisomoscer', 'pisocemento', 'pisoother', 'pisonatur', 'pisonotiene', 'pisomadera']:\n        new_col_name = 'new_{}_x_{}'.format(col1, col2)\n        df_train[new_col_name] = df_train[col1] * df_train[col2]\n        df_test[new_col_name] = df_test[col1] * df_test[col2]\n\nfor col1 in ['pisomoscer', 'pisocemento', 'pisoother', 'pisonatur', 'pisonotiene', 'pisomadera']:\n    for col1 in ['techozinc', 'techoentrepiso', 'techocane', 'techootro']:\n        new_col_name = 'new_{}_x_{}'.format(col1, col2)\n        df_train[new_col_name] = df_train[col1] * df_train[col2]\n        df_test[new_col_name] = df_test[col1] * df_test[col2]\n        \nfor col1 in ['paredblolad', 'paredzocalo', 'paredpreb', 'pareddes', 'paredmad', 'paredzinc', 'paredfibras', 'paredother']:\n    for col2 in ['techozinc', 'techoentrepiso', 'techocane', 'techootro']:\n        new_col_name = 'new_{}_x_{}'.format(col1, col2)\n        df_train[new_col_name] = df_train[col1] * df_train[col2]\n        df_test[new_col_name] = df_test[col1] * df_test[col2]        \n        \nfor col1 in ['paredblolad', 'paredzocalo', 'paredpreb', 'pareddes', 'paredmad', 'paredzinc', 'paredfibras', 'paredother']:\n    for col2 in ['pisomoscer', 'pisocemento', 'pisoother', 'pisonatur', 'pisonotiene', 'pisomadera']:\n        for col3 in ['techozinc', 'techoentrepiso', 'techocane', 'techootro']:\n            new_col_name = 'new_{}_x_{}_x_{}'.format(col1, col2, col3)\n            df_train[new_col_name] = df_train[col1] * df_train[col2] * df_train[col3]\n            df_test[new_col_name] = df_test[col1] * df_test[col2] * df_train[col3]","5ef1aae4":"print(df_train.shape, df_test.shape)","4a6c6501":"cols_with_only_one_value = []\nfor col in df_train.columns:\n    if col == 'Target':\n        continue\n    if df_train[col].value_counts().shape[0] == 1 or df_test[col].value_counts().shape[0] == 1:\n        print(col)\n        cols_with_only_one_value.append(col)","232b667c":"df_train.drop(cols_with_only_one_value, axis=1, inplace=True)\ndf_test.drop(cols_with_only_one_value, axis=1, inplace=True)","e5d18dfd":"cols_train = np.array(sorted([col for col in df_train.columns if col != 'Target']))\ncols_test = np.array(sorted(df_test.columns))","95b1f4fc":"(cols_train == cols_test).sum() == len(cols_train)","ef968b54":"def max_min(x):\n    return x.max() - x.min()","9d1a0ef7":"agg_train = pd.DataFrame()\nagg_test = pd.DataFrame()\n\nfor item in tqdm(family_size_features):\n    for i, function in enumerate(['mean','std','min','max','sum', 'count', max_min]):\n        group_train = df_train[item].groupby(df_train['idhogar']).agg(function)\n        group_test = df_test[item].groupby(df_test['idhogar']).agg(function)\n        if i == 6:\n            new_col = item + '_new_' + 'max_min'\n        else:\n            new_col = item + '_new_' + function\n        agg_train[new_col] = group_train\n        agg_test[new_col] = group_test\n\nprint('new aggregate train set has {} rows, and {} features'.format(agg_train.shape[0], agg_train.shape[1]))\nprint('new aggregate test set has {} rows, and {} features'.format(agg_test.shape[0], agg_test.shape[1]))","545a54be":"aggr_list = ['rez_esc', 'dis', 'male', 'female', \n                  'estadocivil1', 'estadocivil2', 'estadocivil3', 'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', \n                  'parentesco2', 'parentesco3', 'parentesco4', 'parentesco5', 'parentesco6', 'parentesco7', 'parentesco8', 'parentesco9', 'parentesco10', \n                  'parentesco11', 'parentesco12',\n                  'instlevel1', 'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', 'instlevel9',\n                 'epared1', 'epared2', 'epared3', 'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', 'refrig', 'television', 'mobilephone',\n            'area1', 'area2', 'v18q', 'edjef']\n\n\n\nfor item in tqdm(aggr_list):\n    for function in ['count', 'sum']:\n        group_train = df_train[item].groupby(df_train['idhogar']).agg(function)\n        group_test = df_test[item].groupby(df_test['idhogar']).agg(function)\n        new_col = item + '_new1_' + function\n        agg_train[new_col] = group_train\n        agg_test[new_col] = group_test\nprint('new aggregate train set has {} rows, and {} features'.format(agg_train.shape[0], agg_train.shape[1]))\nprint('new aggregate test set has {} rows, and {} features'.format(agg_test.shape[0], agg_test.shape[1]))","1341d2de":"aggr_list = ['escolari', 'age', 'escolari_age', 'dependency', 'bedrooms', 'overcrowding', 'rooms', 'qmobilephone', 'v18q1']\n\nfor item in tqdm(aggr_list):\n    for function in ['mean','std','min','max','sum', 'count', max_min]:\n        group_train = df_train[item].groupby(df_train['idhogar']).agg(function)\n        group_test = df_test[item].groupby(df_test['idhogar']).agg(function)\n        if i == 6:\n            new_col = item + '_new2_' + 'max_min'\n        else:\n            new_col = item + '_new2_' + function\n        agg_train[new_col] = group_train\n        agg_test[new_col] = group_test\n\nprint('new aggregate train set has {} rows, and {} features'.format(agg_train.shape[0], agg_train.shape[1]))\nprint('new aggregate test set has {} rows, and {} features'.format(agg_test.shape[0], agg_test.shape[1]))","031d9bd6":"agg_test = agg_test.reset_index()\nagg_train = agg_train.reset_index()\n\ntrain_agg = pd.merge(df_train, agg_train, on='idhogar')\ntest = pd.merge(df_test, agg_test, on='idhogar')\n\n#fill all na as 0\ntrain_agg.fillna(value=0, inplace=True)\ntest.fillna(value=0, inplace=True)\n\nprint('train shape:', train_agg.shape, 'test shape:', test.shape)","1371cd9c":"aggr_list = ['rez_esc', 'dis', 'male', 'female', \n                  'estadocivil1', 'estadocivil2', 'estadocivil3', 'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', \n                  'parentesco2', 'parentesco3', 'parentesco4', 'parentesco5', 'parentesco6', 'parentesco7', 'parentesco8', 'parentesco9', 'parentesco10', \n                  'parentesco11', 'parentesco12',\n                  'instlevel1', 'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', 'instlevel9',\n                 'epared1', 'epared2', 'epared3', 'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', 'refrig', 'television', 'mobilephone',\n            'area1', 'area2', 'v18q', 'edjef']\n    \nfor lugar in ['lugar1', 'lugar2', 'lugar3', 'lugar4', 'lugar5', 'lugar6']:\n    group_train = df_train[[lugar, 'idhogar'] + aggr_list].groupby([lugar, 'idhogar']).sum().reset_index()\n    group_train.columns = [lugar, 'idhogar'] + ['new3_{}_idhogar_{}'.format(lugar, col) for col in group_train][2:]\n\n    group_test = df_test[[lugar, 'idhogar'] + aggr_list].groupby([lugar, 'idhogar']).sum().reset_index()\n    group_test.columns = [lugar, 'idhogar'] + ['new3_{}_idhogar_{}'.format(lugar, col) for col in group_test][2:]\n\n    train_agg = pd.merge(train_agg, group_train, on=[lugar, 'idhogar'])\n    test = pd.merge(test, group_test, on=[lugar, 'idhogar'])\n    \nprint('train shape:', train_agg.shape, 'test shape:', test.shape)","0bda8199":"aggr_list = ['rez_esc', 'dis', 'male', 'female', \n                  'estadocivil1', 'estadocivil2', 'estadocivil3', 'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', \n                  'parentesco2', 'parentesco3', 'parentesco4', 'parentesco5', 'parentesco6', 'parentesco7', 'parentesco8', 'parentesco9', 'parentesco10', \n                  'parentesco11', 'parentesco12',\n                  'instlevel1', 'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', 'instlevel9',\n                 'epared1', 'epared2', 'epared3', 'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', 'refrig', 'television', 'mobilephone',\n            'area1', 'area2', 'v18q', 'edjef']\n    \nfor lugar in ['lugar1', 'lugar2', 'lugar3', 'lugar4', 'lugar5', 'lugar6']:\n    group_train = df_train[[lugar, 'idhogar'] + aggr_list].groupby([lugar, 'idhogar']).sum().reset_index()\n    group_train.columns = [lugar, 'idhogar'] + ['new4_{}_idhogar_{}'.format(lugar, col) for col in group_train][2:]\n\n    group_test = df_test[[lugar, 'idhogar'] + aggr_list].groupby([lugar, 'idhogar']).sum().reset_index()\n    group_test.columns = [lugar, 'idhogar'] + ['new4_{}_idhogar_{}'.format(lugar, col) for col in group_test][2:]\n\n    train_agg = pd.merge(train_agg, group_train, on=[lugar, 'idhogar'])\n    test = pd.merge(test, group_test, on=[lugar, 'idhogar'])\n    \nprint('train shape:', train_agg.shape, 'test shape:', test.shape)","0553c059":"cols_nums = ['age', 'meaneduc', 'dependency', \n             'hogar_nin', 'hogar_adul', 'hogar_mayor', 'hogar_total',\n             'bedrooms', 'overcrowding']\n\nfor function in tqdm(['mean','std','min','max','sum', 'count', max_min]):\n    for lugar in ['lugar1', 'lugar2', 'lugar3', 'lugar4', 'lugar5', 'lugar6']:\n        group_train = df_train[[lugar, 'idhogar'] + aggr_list].groupby([lugar, 'idhogar']).agg(function).reset_index()\n        group_train.columns = [lugar, 'idhogar'] + ['new5_{}_idhogar_{}_{}'.format(lugar, col, function) for col in group_train][2:]\n\n        group_test = df_test[[lugar, 'idhogar'] + aggr_list].groupby([lugar, 'idhogar']).agg(function).reset_index()\n        group_test.columns = [lugar, 'idhogar'] + ['new5_{}_idhogar_{}_{}'.format(lugar, col, function) for col in group_test][2:]\n\n        train_agg = pd.merge(train_agg, group_train, on=[lugar, 'idhogar'])\n        test = pd.merge(test, group_test, on=[lugar, 'idhogar'])\n        \nprint('train shape:', train_agg.shape, 'test shape:', test.shape)","7870f9a9":"train = train_agg.query('parentesco1==1')","e8c575f8":"train['dependency'].replace(np.inf, 0, inplace=True)\ntest['dependency'].replace(np.inf, 0, inplace=True)","a3f475bf":"submission = test[['Id']]\n\n#Remove useless feature to reduce dimension\ntrain.drop(columns=['idhogar','Id', 'agesq', 'hogar_adul', 'SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', 'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned'], inplace=True)\ntest.drop(columns=['idhogar','Id',  'agesq', 'hogar_adul', 'SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', 'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned'], inplace=True)\n\ncorrelation = train.corr()\ncorrelation = correlation['Target'].sort_values(ascending=False)","09e80c1d":"print('final_data size', train.shape, test.shape)","db2d26be":"print(f'The most 20 positive feature: \\n{correlation.head(40)}')","184307db":"print(f'The most 20 negative feature: \\n{correlation.tail(20)}')","37d1988c":"binary_cat_features = [col for col in train.columns if train[col].value_counts().shape[0] == 2]\nobject_features = ['edjefe', 'edjefa']\n\ncategorical_feats = binary_cat_features + object_features","d8eebfc0":"def evaluate_macroF1_lgb(truth, predictions):  \n    # this follows the discussion in https:\/\/github.com\/Microsoft\/LightGBM\/issues\/1483\n    pred_labels = predictions.reshape(len(np.unique(truth)),-1).argmax(axis=0)\n    f1 = f1_score(truth, pred_labels, average='macro')\n    return ('macroF1', f1, True) ","5e278385":"y = train['Target']\ntrain.drop(columns=['Target'], inplace=True)","651dc8ed":"def print_execution_time(start):\n    end = time.time()\n    hours, rem = divmod(end-start, 3600)\n    minutes, seconds = divmod(rem, 60)\n    print('*'*20, \"Execution ended in {:0>2}h {:0>2}m {:05.2f}s\".format(int(hours),int(minutes),seconds), '*'*20)","da6c946c":"def extract_good_features_using_shap_LGB(params, SEED):\n    clf = lgb.LGBMClassifier(objective='multiclass',\n                             random_state=1989,\n                             max_depth=params['max_depth'], \n                             learning_rate=params['learning_rate'],  \n                             silent=True, \n                             metric='multi_logloss',\n                             n_jobs=-1, n_estimators=10000, \n                             class_weight='balanced',\n                             colsample_bytree = params['colsample_bytree'], \n                             min_split_gain= params['min_split_gain'], \n                             bagging_freq = params['bagging_freq'],\n                             min_child_weight=params['min_child_weight'],\n                             num_leaves = params['num_leaves'], \n                             subsample = params['subsample'],\n                             reg_alpha= params['reg_alpha'],\n                             reg_lambda= params['reg_lambda'],\n                             num_class=len(np.unique(y)),\n                             bagging_seed=SEED,\n                             seed=SEED,\n                            )\n\n    kfold = 5\n    kf = StratifiedKFold(n_splits=kfold, shuffle=True)\n    feat_importance_df  = pd.DataFrame()\n\n    for i, (train_index, test_index) in enumerate(kf.split(train, y)):\n        print('='*30, '{} of {} folds'.format(i+1, kfold), '='*30)\n        start = time.time()\n        X_train, X_val = train.iloc[train_index], train.iloc[test_index]\n        y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n        clf.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], eval_metric=evaluate_macroF1_lgb, categorical_feature=categorical_feats,\n                early_stopping_rounds=500, verbose=500)\n        shap_values = shap.TreeExplainer(clf.booster_).shap_values(X_train)\n        fold_importance_df  = pd.DataFrame()\n        fold_importance_df['feature'] = X_train.columns\n        fold_importance_df['shap_values'] = abs(np.array(shap_values)[:, :].mean(1).mean(0))\n        fold_importance_df['feat_imp'] = clf.feature_importances_\n        feat_importance_df = pd.concat([feat_importance_df, fold_importance_df])\n        print_execution_time(start)\n\n    feat_importance_df_shap = feat_importance_df.groupby('feature').mean().sort_values('shap_values', ascending=False).reset_index()\n#     feat_importance_df_shap['shap_cumsum'] = feat_importance_df_shap['shap_values'].cumsum() \/ feat_importance_df_shap['shap_values'].sum()\n#     good_features = feat_importance_df_shap.loc[feat_importance_df_shap['shap_cumsum'] < 0.999].feature\n    return feat_importance_df_shap","b88390a0":"total_shap_df  = pd.DataFrame()\nNUM_ITERATIONS = 50\nfor SEED in range(NUM_ITERATIONS):\n    print('#'*40, '{} of {} iterations'.format(SEED+1, NUM_ITERATIONS), '#' * 40)\n    params = {'max_depth': np.random.choice([5, 6, 7, 8, 10, 12, -1]),\n             'learning_rate': np.random.rand() * 0.02,\n              'colsample_bytree': np.random.rand() * (1 - 0.5) + 0.5,\n              'subsample': np.random.rand() * (1 - 0.5) + 0.5,\n              'min_split_gain': np.random.rand() * 0.2,\n              'num_leaves': np.random.choice([32, 48, 64]),\n              'reg_alpha': np.random.rand() * 2,\n              'reg_lambda': np.random.rand() *2,\n              'bagging_freq': np.random.randint(4) +1,\n              'min_child_weight': np.random.randint(100) + 20\n             }\n    temp_shap_df = extract_good_features_using_shap_LGB(params, SEED)\n    total_shap_df = pd.concat([total_shap_df, temp_shap_df])","9db3a6b8":"shap_sorted_df = total_shap_df.groupby('feature').mean().sort_values('shap_values', ascending=False).reset_index()\nfeat_imp_sorted_df = total_shap_df.groupby('feature').mean().sort_values('feat_imp', ascending=False).reset_index()\nfeatures_top_shap = shap_sorted_df['feature'][:500]\nfeatures_top_feat_imp = feat_imp_sorted_df['feature'][:500]\ntop_features = pd.Series(features_top_shap.tolist() + features_top_feat_imp.tolist())\ntop_features = top_features.unique()","5eafe0da":"new_train = train[top_features].copy()\nnew_test = test[top_features].copy()","4f7dc825":"print('new_train shape:', new_train.shape, 'new_test shape:', new_test.shape)","c123fa42":"new_categorical_feats = [col for col in top_features if col in categorical_feats]","0396f533":"def LGB_OOF(params, categorical_feats, N_FOLDs, SEED=1989):\n    clf = lgb.LGBMClassifier(objective='multiclass',\n                             random_state=1989,\n                             max_depth=params['max_depth'], \n                             learning_rate=params['learning_rate'],  \n                             silent=True, \n                             metric='multi_logloss',\n                             n_jobs=-1, n_estimators=10000, \n                             class_weight='balanced',\n                             colsample_bytree = params['colsample_bytree'], \n                             min_split_gain= params['min_split_gain'], \n                             bagging_freq = params['bagging_freq'],\n                             min_child_weight=params['min_child_weight'],\n                             num_leaves = params['num_leaves'], \n                             subsample = params['subsample'],\n                             reg_alpha= params['reg_alpha'],\n                             reg_lambda= params['reg_lambda'],\n                             num_class=len(np.unique(y)),\n                             bagging_seed=SEED,\n                             seed=SEED,\n                            )\n\n    kfold = 10\n    kf = StratifiedKFold(n_splits=kfold, shuffle=True)\n    feat_importance_df  = pd.DataFrame()\n    predicts_result = []\n\n    for i, (train_index, test_index) in enumerate(kf.split(new_train, y)):\n        print('='*30, '{} of {} folds'.format(i+1, kfold), '='*30)\n        start = time.time()\n        X_train, X_val = new_train.iloc[train_index], new_train.iloc[test_index]\n        y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n        clf.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], eval_metric=evaluate_macroF1_lgb,categorical_feature=new_categorical_feats,\n                early_stopping_rounds=500, verbose=500)\n        shap_values = shap.TreeExplainer(clf.booster_).shap_values(X_train)\n        fold_importance_df  = pd.DataFrame()\n        fold_importance_df['feature'] = X_train.columns\n        fold_importance_df['shap_values'] = abs(np.array(shap_values)[:, :].mean(1).mean(0))\n        fold_importance_df['feat_imp'] = clf.feature_importances_\n        feat_importance_df = pd.concat([feat_importance_df, fold_importance_df])\n        predicts_result.append(clf.predict(new_test))\n        print_execution_time(start)\n    return predicts_result, feat_importance_df","b62bed1a":"params = {'max_depth': 6,\n         'learning_rate': 0.002,\n          'colsample_bytree': 0.8,\n          'subsample': 0.8,\n          'min_split_gain': 0.02,\n          'num_leaves': 48,\n          'reg_alpha': 0.04,\n          'reg_lambda': 0.073,\n          'bagging_freq': 2,\n          'min_child_weight': 40\n         }\n\nN_Folds = 20\nSEED = 1989\npredicts_result, feat_importance_df = LGB_OOF(params, new_categorical_feats, N_Folds, SEED=1989)","85a1a084":"fig, ax = plt.subplots(1, 2, figsize=(20, 20))\nfeat_importance_df_shap = feat_importance_df.groupby('feature').mean().sort_values('shap_values', ascending=False).reset_index()\n\nnum_features = 50\nsns.barplot(x=feat_importance_df_shap.shap_values[:num_features], y=feat_importance_df_shap.feature[:num_features], ax=ax[0])\nax[0].set_title('Feature importance based on shap values')\n\nfeat_importance_df = feat_importance_df.groupby('feature').mean().sort_values('feat_imp', ascending=False).reset_index()\n\nnum_features = 50\nsns.barplot(x=feat_importance_df.shap_values[:num_features], y=feat_importance_df.feature[:num_features], ax=ax[1])\nax[1].set_title('Feaure importance based on feature importance from lgbm')\nplt.show()","4cd39891":"submission['Target'] = np.array(predicts_result).mean(axis=0).round().astype(int)\nsubmission.to_csv('submission_with_new_feature_set.csv', index = False)","806b473f":"optimized_param = None\nlowest_cv = 1000\ntotal_iteration = 100\nfor i in range(total_iteration):\n    print('-'*20, 'For {} of {} iterations'.format(i+1, total_iteration), '-'*20)\n    learning_rate = np.random.rand() * 0.02\n    n_folds = 3\n\n    num_class = len(np.unique(y))\n\n    params = {}\n    params['application'] = 'multiclass'\n    params['metric'] = 'multi_logloss'\n    params['num_class'] = num_class\n    params['class_weight'] = 'balanced'\n    params['num_leaves'] = np.random.randint(24, 48)\n    params['max_depth'] = np.random.randint(5, 8)\n    params['min_child_weight'] = np.random.randint(5, 50)\n    params['min_split_gain'] = np.random.rand() * 0.09\n    params['colsample_bytree'] = np.random.rand() * (0.9 - 0.1) + 0.1\n    params['subsample'] = np.random.rand() * (1 - 0.8) + 0.8\n    params['bagging_freq'] = np.random.randint(1, 5)\n    params['bagging_seed'] = np.random.randint(1, 5)\n    params['reg_alpha'] = np.random.rand() * 2\n    params['reg_lambda'] = np.random.rand() * 2\n    params['learning_rate'] = np.random.rand() * 0.02\n    params['seed']  =1989\n\n    d_train = lgb.Dataset(data=new_train, label=y.values-1, categorical_feature=new_categorical_feats, free_raw_data=False)\n    cv_results = lgb.cv(params=params, train_set=d_train, num_boost_round=10000, categorical_feature=new_categorical_feats,\n                        nfold=n_folds, stratified=True, shuffle=True, early_stopping_rounds=1, verbose_eval=1000)\n\n    min_cv_results = min(cv_results['multi_logloss-mean'])\n\n    if min_cv_results < lowest_cv:\n        lowest_cv = min_cv_results\n        optimized_param = params","e08e24c5":"N_Folds = 20\nSEED = 1989\npredicts_result, feat_importance_df = LGB_OOF(optimized_param, new_categorical_feats, N_Folds, SEED=1989)","c491d598":"submission['Target'] = np.array(predicts_result).mean(axis=0).round().astype(int)\nsubmission.to_csv('submission_shap_randomized_search.csv', index = False)","2f9bcb92":"# 2. Feature engineering","a535e80e":"### Aggregation for family features","fdd0685f":"- wall, roof, floor may be key factor.\n- Let's multiply each of them. Becuase they are binary cat features, so mulitification of each features generates new categorical features","edc88dbb":"### phone per family features","f7d3f265":"- In this competition, each samples are member of spectific household(idhogar). So let's aggregate based on 'idhogar' values.","e9bbbb70":"- Below cell is from this kernel. https:\/\/www.kaggle.com\/skooch\/lgbm-w-random-split-2","fa586078":"### Squared features","e7e34833":"### rez_esz, SQBmeaned\n- rez_esc : Years behind in school -> filled with 0\n- SQBmeaned : square of the mean years of education of adults (>=18) in the household\nagesq, Age squared -> same with rez_esc -> filled with 0","9974a1f6":"### Remove feature with only one value","e92d07f4":"### Tabulet per family features","8b930140":"### meaneduc\n- meaneduc: average years of education for adults (18+) -> filled with 0","5a9336a6":"### BedRoom per family features","7ea75bbc":"## roof and electricity\n- I refered to https:\/\/www.kaggle.com\/mineshjethva\/exploratory-data-analysis-lightgbm. Thanks!","c52b053f":"### v18q1","a79fd2dc":"### edjefe\n- edjefe, years of education of male head of household, based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0\n- replace yes -> 1 and no -> 0","aac34d7b":"### Family features\n- hogar_nin, hogar_adul, hogar_mayor, hogar_total, r4h1, r4h2, r4h3, r4m1, r4m2, r4m3, r4t1, r4t2, r4t3, tmbhog, tamvid, rez_esc, escolari","dfb34854":"### Rent per  family features","a8b57570":"### Check whether both train and test have same features","81afef4e":"- v18q1: number of tablets household owns -> if v18q(Do you own a tablet?)  == 1, there are some values. If not, only NaN values in v18q1. See below 3 cells.","2347fe83":"- v2a1: number of tablets household owns -> if tipovivi3(rented?)  == 1, there are some values. If not, there are also some values.\n- NaN value could be replaced by 0.","52d19f8f":"- v18q1: number of tablets household owns -> if v18q  == 1, there are some values. If not, only NaN values there. See below two cells.","540fe3ca":"- Ratio feature can have infinite values. So Let them be filled with 0","54b6bc16":"# 4. Model development","9013dd09":"### Room per family features","5f6680bb":"## 1.1 Read dataset","deb0dc40":"## 2.1 Object features","082f94f7":"- Family size features (substract, ratio)","a967eba6":"## 1.2 Make description df","372dd81a":"- You can see that \"Total persons in the household\" !=  \"# of total individuals in the household\". \n- Somewhat weired. But for now I will keep it. ","04d4249a":"### Rich features\n- I think the more richer, the larger number of phones and tabulet","850ce1c9":"## 2.2 Extract cat features\n- According to data scription, there are many binary category features.","cf6d694e":"### dependecy","cd75435e":"- According to data descriptions,ONLY the heads of household are used in scoring. \/\n- All household members are included in test + the sample submission, but only heads of households are scored.","0ca02236":"## Randomized serach","45d24256":"# 4. Feature selection using shap","155a1eb4":"## 1.4 Filll missing values","923d0c2b":"### rez_esc(Years behind in school)  per family features ","1db90a6c":"## 2.4 aggregation features","45752bcc":"- combination using three features","b3beb651":"- I want to mix toilet and rubbish disposal features -> other_infra features","3ac6659d":"### For now, there are no NaN values.","41097bc5":"- As you can see, the meaning of two features are same but the exact number are different. Are they different?\n- I don't know.  For now, I decided to drop one feature 'tamhog'.","2358136c":"## 1.3 Check null data","22995c26":"### edjefa","c4faaad7":"- I want to mix electricity and energy features -> energy features","a688680e":"- Let's remove them!","1c926a5e":"- Mix region and education","9acc73ac":"## 2.3 Make new features using continuous feature","2da43521":"- Mix wall material of roof, floor, wall","1223dbd4":"-  There are many squared features. Actually, tree models like lightgbm don't need them. But at this kernel, I want to use lightgbm as feature filter model and set entity- embedding as classfier. So Let's keep them.","99057788":"- Good, Let's move!","86345c28":"- I will reduce the number of features using shap, so let's generate many features!! Hope catch some fortune features :)","8c0f1956":"- Multiply television \/ mobilephone \/ computer \/ tabulet \/ refrigerator -> electornics features","53553f0b":"- hhsize : household size\n- tamhog : size of the household\n\nWhat is different?","8ea3b706":"- I want to mix toilet and water provision features -> water features","8b2d5735":"- edjefa, years of education of female head of household, based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0\n- replace yes -> 1 and no -> 0","468e4b05":"- Wow! without any aggreation features, we hvae 446 features for now. Actually mixing the materials of walls make thousands of features. I don't want to do that because of computational costs!","3f03ce02":"# 1. Check datasets","82caeaba":"- I want mix education and area features -> education_zone_features"}}