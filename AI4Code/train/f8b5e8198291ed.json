{"cell_type":{"58d1c8b3":"code","0017c4a2":"code","d843945c":"code","e1f0aa9f":"code","eafd2045":"code","359fe172":"code","3408c45d":"code","0f795414":"code","e9412462":"code","7581d158":"code","2c0165d2":"code","315ec1b6":"code","58201150":"code","24f3bcdb":"code","82b806a7":"code","78d14070":"code","1c07c28f":"code","b5be5099":"code","092cf0b0":"code","708fa919":"code","f5d7fb89":"code","71cb9422":"code","2d87cf91":"code","98086447":"code","d1b8a186":"code","5da3d13a":"code","59351c99":"code","3e565f34":"code","78588b0f":"code","609b8e6c":"code","4c87375f":"code","bead58d1":"code","6c9caff3":"code","1d95c2f5":"code","8f55ec5e":"code","5cc51338":"code","1cd6ca79":"code","3f745cf8":"code","375fd550":"code","b9be86dc":"code","d3566908":"code","46fd295c":"code","f80ccd2e":"code","a6a926ef":"code","50632f89":"code","8f41fb92":"code","d4cfc6bd":"code","225c0c4e":"code","50137de7":"code","1cace49e":"code","da9c49bc":"code","7e63edef":"code","59453c0f":"code","cca30484":"code","4b4680fc":"markdown","93beffa7":"markdown","8e646a51":"markdown","62d91ca4":"markdown","10df896c":"markdown","2e7160f1":"markdown","7a23fe98":"markdown","4785ab14":"markdown","53a46e88":"markdown","8199b4f2":"markdown","fa556409":"markdown","c24943f5":"markdown","7445a9d2":"markdown","2b6ff6b4":"markdown","5ce8f7da":"markdown","cb4abda4":"markdown"},"source":{"58d1c8b3":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn import svm\nimport warnings","0017c4a2":"warnings.filterwarnings('ignore')","d843945c":"df_train = pd.read_csv(\"..\/input\/titanic\/train.csv\",index_col=\"PassengerId\")\ndf_train.head()","e1f0aa9f":"df_test = pd.read_csv(\"..\/input\/titanic\/test.csv\", index_col=\"PassengerId\")\ndf_test.head()","eafd2045":"df_train.Parch.unique()","359fe172":"df_test.info()","3408c45d":"Y_test = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\", index_col=\"PassengerId\")\nY_test.head()","0f795414":"df_train.info()","e9412462":"df_train.isnull().sum()","7581d158":"msno.matrix(df_train)\nplt.show()","2c0165d2":"df_train[[\"Embarked\",\"Name\"]].groupby(by=[\"Embarked\"],\n                    as_index=True).count().sort_values(\"Name\",ascending=False)","315ec1b6":"plt.style.use(\"seaborn\")\nplt.bar(df_train.Embarked.dropna().unique(), [*df_train.Embarked.value_counts()],\n        color=\"#0b91a3\",width=0.4, label=\"Embarked bar plot\")\nplt.show()","58201150":"# so we notes that Most repeated Embarked is S, so that we can replace null value in Embarked column with it.\nmost_repeated = \"S\"\ndf_train.Embarked.replace(np.nan, most_repeated, inplace=True)\ndf_test.Embarked.replace(np.nan, most_repeated, inplace=True)\nprint(\"the number of null value in Embarked Column =\",df_train.Embarked.isnull().sum())","24f3bcdb":"# transform Embarked Column to numeric.\nEmbarked_transform_dict = {\"S\":1, \"C\":2, \"Q\":3}\nfor value in Embarked_transform_dict:\n    df_train.Embarked.replace(value, Embarked_transform_dict.get(value), inplace=True)\n    df_test.Embarked.replace(value, Embarked_transform_dict.get(value), inplace=True)\ndf_train.head(5)\n# we now finish cleaning and transform column Emvarked to numeric.","82b806a7":"print(\"the number of null value in Cabin Column =\", df_train.Cabin.isnull().sum())\n# notes that null values is 687 from 891 (77%) of Cabin column is null, so i will droped it from data.","78d14070":"df_train.drop(\"Cabin\", axis=1, inplace=True)\ndf_test.drop(\"Cabin\", axis=1, inplace=True)\n\ndf_train","1c07c28f":"# Enter to Fare column\n# calculate the range of value in Fare column.\nprint(\"Range of Fare column values = \", df_train.Fare.max() - df_train.Fare.min())\n# min value is 0.0 and max value = 512.3292\n# i will divied this range to 10 sections.\ndf_test.Fare.replace(np.nan, df_test.Fare.mean(), inplace=True)\nprint(\"Range of Fare column values = \", df_test.Fare.max() - df_test.Fare.min())","b5be5099":"df_train.Fare = df_train.Fare.astype(\"int64\")\ndf_test.Fare = df_test.Fare.astype(\"int64\")\n\n# df_train.info()\ndf_test","092cf0b0":"bins_i = [-1, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550]\nlabels_i = [1,2,3,4,5,6,7,8,9,10,11]\n\ndf_train['stage'] = 0\ndf_train['stage'] = pd.cut(df_train.Fare, bins=bins_i, labels=labels_i)\n\ndf_test['stage'] = 0\ndf_test['stage'] = pd.cut(df_test.Fare, bins=bins_i, labels=labels_i)\n\ndf_train.stage.unique()","708fa919":"df_train.Fare = df_train.stage.astype(\"int64\")\ndf_test.Fare = df_test.stage.astype(\"int64\")\ndf_train.drop(\"stage\", axis=1, inplace=True)\ndf_test.drop(\"stage\", axis=1, inplace=True)","f5d7fb89":"df_train.head()","71cb9422":"df_test.Fare.unique()","2d87cf91":"len(df_train.Ticket.unique())","98086447":"# i drop this column, because this is column is outlayer of data not need.\ndf_train.drop(\"Ticket\", axis=1, inplace=True)\ndf_test.drop(\"Ticket\", axis=1, inplace=True)","d1b8a186":"df_train.head()","5da3d13a":"plt.style.use(\"seaborn\")\nplt.figure(figsize=(4,4))\nplt.bar(df_train.Sex.dropna().unique(), [*df_train.Sex.value_counts()],\n        color=\"#0b91a3\",width=0.3, label=\"Embarked bar plot\")\nplt.show()","59351c99":"# Sex column.\nSex_dict = {\"male\":1, \"female\":2}\nfor key, value in Sex_dict.items():\n    df_train.Sex.replace(key, value, inplace=True)\n    df_test.Sex.replace(key, value, inplace=True)\ndf_train.Sex = df_train.Sex.astype(\"int64\")\ndf_test.Sex = df_test.Sex.astype(\"int64\")\ndf_train.head()","3e565f34":"df_train[\"Title\"] = 0\n\ntitles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\ndata = [df_train, df_test]\nfor dataset in data:\n    # extract titles\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    # replace titles with a more common title or as Rare\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\\n                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace(['Mlle','Ms'], 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    # convert titles into numbers\n    dataset['Title'] = dataset['Title'].map(titles)\n    # filling NaN with 0, to get safe\n    dataset['Title'] = dataset['Title'].fillna(0)\ndf_train = df_train.drop(['Name'], axis=1)\ndf_test = df_test.drop(['Name'], axis=1)\ndf_train","78588b0f":"df_train.isnull().sum()","609b8e6c":"df_Age_train = df_train.loc[pd.notna(df_train.Age)]\ndf_Age_train.Age = df_Age_train.Age.astype(\"float64\")\n\nplt.hist(df_Age_train.Age,bins=20,color=\"#2d6bad\")\nplt.show()\n","4c87375f":"df_Age_train.Age = (df_Age_train.Age - df_Age_train.Age.mean()) \/ df_Age_train.Age.std()\ndf_Age_train","bead58d1":"df_Age_train.Survived.corr(df_Age_train.Age)\n# so i will drop Age column, because it is correlation between Age and Survived is very small.","6c9caff3":"df_train.drop(\"Age\", axis=1, inplace=True)\ndf_test.drop(\"Age\", axis=1, inplace=True)\ndf_train","1d95c2f5":"df_test","8f55ec5e":"data = [df_train, df_test]\nfor dataset in data:\n    dataset['FamilySize'] = dataset['SibSp'] +  dataset['Parch'] + 1","5cc51338":"\nfor dataset in data:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\nprint (df_train[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean())","1cd6ca79":"colormap=plt.cm.RdBu\nfigure = plt.figure(figsize=(12,12))\nsns.heatmap(df_train.corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)\nplt.title(\"Correlations\",size=15)\nplt.xlabel(\"Features\")\nplt.ylabel(\"Features\")\nplt.show()","3f745cf8":"columns = [\"Pclass\",\"Sex\", \"Fare\", \"Embarked\",\"Title\",\"IsAlone\"]\nX_train = df_train[columns]\nY_train = df_train[\"Survived\"]\nlen(Y_train)","375fd550":"X_test = df_test[columns]\nlen(X_test)","b9be86dc":"len(Y_test)","d3566908":"sgd_clf = SGDClassifier(random_state=42, max_iter=1000, tol=1e-3)\nsgd_clf.fit(X_train, Y_train)\nY_pred_SGD = sgd_clf.predict(X_test)\nprint(\"the train score of SGD = \",round(sgd_clf.score(X_train, Y_train) *100, 2),\"%\")","46fd295c":"random_forest = RandomForestClassifier(n_estimators=40, min_samples_leaf=2, max_features=0.1, n_jobs=-1)\nrandom_forest.fit(X_train, Y_train)\nY_pred_Random = random_forest.predict(X_test)\nprint(\"the train score of random_forest = \",round(random_forest.score(X_train, Y_train) *100, 2),\"%\")","f80ccd2e":"logistic_regression = LogisticRegression(solver='liblinear',max_iter=1000)\nlogistic_regression.fit(X_train, Y_train)\nY_pred_Logistic = logistic_regression.predict(X_test)\nprint(\"the train score of logistic_regression = \",round(logistic_regression.score(X_train, Y_train) *100, 2),\"%\")","a6a926ef":"tree = DecisionTreeClassifier(random_state=25)\ntree.fit(X_train, Y_train)\nY_pred_Tree= tree.predict(X_test)\nprint(\"the score of prediction = \",round(tree.score(X_train, Y_train) * 100,2), \"%\")","50632f89":"scores= cross_val_score(tree, X_train, Y_train, scoring=\"accuracy\", cv=100)\nscores.mean()","8f41fb92":"\nclf = svm.SVC(kernel = 'linear')\nclf.fit(X_train, Y_train)\nY_predict_svm = clf.predict(X_test)\nprint(\"the score of prediction = \",round(clf.score(X_train, Y_train) * 100,2), \"%\")","d4cfc6bd":"knn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, Y_train)\nY_pred_KNN= knn.predict(X_test)\nprint(\"the score of prediction = \",round(knn.score(X_train, Y_train) * 100,2), \"%\")","225c0c4e":"output_csv = {\"PassengerId\":[*range(892,892+len(Y_pred_KNN))], \"Survived\":Y_pred_KNN}\nY_pre = pd.DataFrame(output_csv)\nY_pre.set_index(\"PassengerId\", drop=True, append=False, inplace=True)\nY_pre.to_csv(\"\/kaggle\/working\/submission.csv\")","50137de7":"clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(3, 2), random_state=1)\nclf.fit(X_train, Y_train)\nY_pred_clf= clf.predict(X_test)\nprint(\"the score of prediction = \",round(clf.score(X_train, Y_train) * 100,2), \"%\")","1cace49e":"gaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred_gaussian = gaussian.predict(X_test)\nprint(\"the train score for Gaussian = \", round(gaussian.score(X_train, Y_train) * 100, 2), \"%\")","da9c49bc":"perceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred_perceptron = perceptron.predict(X_test)\nprint(\"the train score for Perceptron = \",round(perceptron.score(X_train, Y_train) * 100, 2), \"%\")","7e63edef":"import xgboost as xgb\nxgb_classifer = xgb.XGBClassifier(objective='binary:logistic',\n                                  eval_metric= 'logloss',\n                                  max_depth=3)\n\nxgb_classifer.fit(X_train, Y_train)\nY_pred_xgb = xgb_classifer.predict(X_test)\nprint(\"the train score for Perceptron = \",round(xgb_classifer.score(X_train, Y_train) * 100, 2), \"%\")","59453c0f":"output_csv = {\"PassengerId\":[*range(892,892+len(Y_pred_xgb))], \"Survived\":Y_pred_xgb}\nY_pre = pd.DataFrame(output_csv)\nY_pre.set_index(\"PassengerId\", drop=True, append=False, inplace=True)\nY_pre.to_csv(\"\/kaggle\/working\/xgb_submission.csv\")","cca30484":"model = [\"SGDClassifier\", \"Random Forest\", \"Logistic Regression\", \"Decision Tree\", \"SVM\",\n\"KNeighbors\", \"MlPClassifier\", \"GaussianNB\", \"Perceptron\"]\nscore = [66, 78, 76.6, 78, 76.55, 79.18, 77.5, 74.6, 78.2]\ndata_dict = {\"models\": model, \"test_score\": score}\ndata_score = pd.DataFrame(data_dict)\ndata_score.index = data_score.index + 1\ndata_score.sort_values(\"test_score\",ascending=False)","4b4680fc":"## ML Clasification Prediction","93beffa7":"### 4- Decision Tree (78%)","8e646a51":"### Thanks for read my Notebook :) ","62d91ca4":"### 8- GaussianNB (74.6%)","10df896c":"## EDA","2e7160f1":"### 7- MlPClassifier (77.5%)","7a23fe98":"### 1- SGDClassifier (66%)","4785ab14":"## Data Cleaning & Encoding","53a46e88":"### 2- Random Forest (78%)","8199b4f2":"### 9- Perceptron (78.2%)","fa556409":"### 6- KNeighbors (79.18%)","c24943f5":"### This notebook divided into 3 parts:\n> EDA\n\n> Data Cleaning & Encoding\n\n> ML Clasification Prediction","7445a9d2":"### 5- SVM (76.55%)","2b6ff6b4":"### Titanic | ML Classification Prediction Algorithms with accuracy (79.2%)","5ce8f7da":"### 3- Logistic Regression (76.6%)","cb4abda4":"## 10- XGBoost"}}