{"cell_type":{"91c7918c":"code","a7a4b833":"code","f6e7d0a6":"code","27201fe6":"code","12f95126":"code","a922aab0":"code","aaca8b34":"code","8b166cd8":"code","56bedbec":"code","17b3f701":"code","911c0962":"code","43888e7c":"code","a4a2675b":"code","60d1bce5":"code","0511162d":"code","6bc5b13e":"code","ce32eb54":"code","1a349578":"code","86602efc":"code","d558253e":"code","21886c96":"code","5804c261":"code","b16d9bd8":"code","33864c1d":"markdown","eafe670e":"markdown","880b0514":"markdown","141775fa":"markdown","611a2603":"markdown","e6b3f3bf":"markdown","d388f8ea":"markdown","690260be":"markdown","85f9e380":"markdown","b1caec59":"markdown","b84a3f83":"markdown","b80b97d3":"markdown","afa361f1":"markdown","fc178964":"markdown","99fe5f7b":"markdown","ce9d1ec6":"markdown","f3f8d377":"markdown","e38aab01":"markdown","876cc9c1":"markdown","8cc71c37":"markdown"},"source":{"91c7918c":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","a7a4b833":"df = pd.read_csv(\"\/kaggle\/input\/german-credit-data-with-risk\/german_credit_data.csv\", index_col=0)\ndf.head()","f6e7d0a6":"df.isna().sum()","27201fe6":"for col in ['Saving accounts', 'Checking account']:\n    df[col].fillna('none', inplace=True)\n","12f95126":"j = {0: 'unskilled and non-res', 1: 'unskilled and res', 2: 'skilled', 3: 'highly skilled'}\ndf['Job'] = df['Job'].map(j)\n\ndf.head()","a922aab0":"g = sns.FacetGrid(df, col=\"Sex\",  row=\"Job\")\ng = g.map(sns.countplot, \"Risk\")","aaca8b34":"sns.countplot(x=\"Housing\", hue=\"Risk\", data=df)\nplt.show()\n","8b166cd8":"fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,8))\nsns.countplot(x=\"Saving accounts\", hue=\"Risk\", data=df, ax=ax1)\nsns.countplot(x=\"Checking account\", hue=\"Risk\", data=df, ax=ax2)\nax1.set_xticklabels(ax1.get_xticklabels(), rotation=45)\nax2.set_xticklabels(ax2.get_xticklabels(), rotation=45)\n\n\nfig.show()","56bedbec":"fig, (ax1, ax2, ax3, ax4) = plt.subplots(4,1, figsize=(15,25))\n\nsns.boxplot(x=\"Purpose\", y=\"Credit amount\", data=df, hue=\"Risk\", palette=\"coolwarm\", ax=ax1)\nsns.boxplot(x=\"Saving accounts\", y=\"Credit amount\", data=df, hue=\"Risk\", palette=\"coolwarm\", ax=ax2)\nsns.boxplot(x=\"Checking account\", y=\"Credit amount\", data=df, hue=\"Risk\", palette=\"coolwarm\", ax=ax3)\nsns.boxplot(x=\"Job\", y=\"Credit amount\", data=df, hue=\"Risk\", palette=\"coolwarm\", ax=ax4)\n\nax1.set_title(\"Credit Amount Distribution by Purpose\", fontsize=16)\nax2.set_title(\"Credit Amount Distribution by Saving Account Status\", fontsize=16)\nax3.set_title(\"Credit Amount Distribution by Checking Account Status\", fontsize=16)\nax4.set_title(\"Credit Amount Distribution by Job Type\", fontsize=16)\n\nplt.subplots_adjust(hspace = 0.6, top = 0.6)\nplt.show()\n","17b3f701":"fig, (ax1, ax2) = plt.subplots(2,1, figsize=(12,14))\n\nsns.pointplot(x=\"Age\", y=\"Credit amount\", data=df, hue=\"Risk\", palette=\"coolwarm\", ax=ax1)\nsns.pointplot(x=\"Duration\", y=\"Credit amount\", data=df, hue=\"Risk\", palette=\"coolwarm\", ax=ax2)\n\nax1.set_title(\"Credit Amount Distribution by Age\", fontsize=16)\nax2.set_title(\"Credit Amount Distribution by Loan Duration\", fontsize=16)\n\nplt.subplots_adjust(wspace = 0.4, hspace = 0.4,top = 0.9)\nplt.show()","911c0962":"# getting dummies for all the categorical variables\ndummies_columns = ['Job', 'Purpose', 'Sex', 'Housing', 'Saving accounts', 'Checking account']\nfor col in dummies_columns:\n    df = df.merge(pd.get_dummies(df[col], drop_first=True, prefix=str(col)), left_index=True, right_index=True) \n\n# encoding risk as binary\nr = {\"good\":0, \"bad\": 1}\ndf['Risk'] = df['Risk'].map(r)\n\n# drop redundant variables\ncolumns_to_drop = ['Job', 'Purpose','Sex','Housing','Saving accounts','Checking account']\ndf.drop(columns_to_drop, axis=1, inplace=True)\n\ndf.head()\n\n    \n","43888e7c":"df['Log_CA'] = np.log(df['Credit amount'])","a4a2675b":"plt.figure(figsize=(14,12))\nsns.heatmap(df.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True,  linecolor='white', cmap='coolwarm')\nplt.show()","60d1bce5":"from sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, fbeta_score\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n","0511162d":"X = df.drop(['Risk', 'Credit amount'], axis=1).values\ny = df['Risk'].values\n\n# train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=42)\n\n","6bc5b13e":"log = LogisticRegression()\nlog.fit(X_train, y_train)\ny_pred_log = log.predict(X_test)\nprint(accuracy_score(y_pred_log, y_test))\nprint(confusion_matrix(y_test, y_pred_log))\nprint(classification_report(y_test, y_pred_log))\n\n","ce32eb54":"\"\"\"knn = KNeighborsClassifier()\ngrid = GridSearchCV(knn, param_grid={'n_neighbors':range(1,20)}, scoring='recall')\ngrid.fit(X_train, y_train)\n\ngrid.best_params_\n\nfor i in range(0, len(grid.cv_results_['mean_test_score'])):\n    print('N_Neighbors {}: {} '.format(i+1, grid.cv_results_['mean_test_score'][i]*100))\"\"\"\n    \n# recall peaks at k = 1\n\nknn = KNeighborsClassifier(n_neighbors = 1)\nknn.fit(X_train, y_train)\ny_pred_knn = knn.predict(X_test)\nprint(accuracy_score(y_pred_knn, y_test))\nprint(confusion_matrix(y_test, y_pred_knn))\nprint(classification_report(y_test, y_pred_knn))\n\n    ","1a349578":"\"\"\"param_grid_svc = {\"gamma\": [0.1,0.5,1,5,10,50,100],\n                  \"C\": [0.1,0.5,1,5,10,50,100]}\n\nsvc = SVC(kernel='linear')\n\ngs_svc = GridSearchCV(svc, param_grid = param_grid_svc, cv=5, scoring='recall', verbose=4)\ngs_svc.fit(X_train, y_train)\n\ngs_svc.best_params_ # gamma = , C = \"\"\"\n\nsvc = SVC(kernel='linear', gamma=10, C=0.8)\nsvc.fit(X_train, y_train)\ny_pred_svc = svc.predict(X_test)\nprint(accuracy_score(y_pred_svc, y_test))\nprint(confusion_matrix(y_test, y_pred_svc))\nprint(classification_report(y_test, y_pred_svc))\n    \n","86602efc":"\"\"\"param_grid_rf = {\"max_depth\": range(3,10),\n                  \"max_features\": [3,5,7,9,11,13,15,17,20],\n                  \"min_samples_leaf\": [5,10,15,20,25,30],\n                  \"n_estimators\": [3,5,10,25,50,150]}\n\nrf = RandomForestClassifier()\ngs_rf = GridSearchCV(rf, param_grid=param_grid_rf, cv=5, scoring=\"recall\", verbose=4)\ngs_rf.fit(X_train, y_train)\n\ngs_rf.best_params_\n\"\"\"\n# {'max_depth': 9, 'max_features': 15, 'min_samples_leaf': 5, 'n_estimators': 25}\n\n\nrf = RandomForestClassifier(max_depth=9, max_features=15, min_samples_leaf=5, n_estimators=25)\nrf.fit(X_train, y_train)\ny_pred_rf = rf.predict(X_test)\nprint(accuracy_score(y_pred_rf, y_test))\nprint(confusion_matrix(y_test, y_pred_rf))\nprint(classification_report(y_test, y_pred_rf))\n","d558253e":"nb = GaussianNB()\nnb.fit(X_train, y_train)\ny_pred_nb = nb.predict(X_test)\nprint(accuracy_score(y_pred_nb, y_test))\nprint(confusion_matrix(y_test, y_pred_nb))\nprint(classification_report(y_test, y_pred_nb))\n    \n","21886c96":"\"\"\"param_grid_xgb = {\"max_depth\": range(3,10),\n                  \"subsample\": [0.5,0.6,0.7,0.8,0.9,1],\n                  \"eta\": [0.01,0.03,0.05,0.07,0.09,0.14,0.19],\n                  \"colsample_bytree\": [0.5,0.6,0.7,0.8,0.9,1],\n                  \"n_estimators\": [3,5,10,25,50,150]}\n\nxgb = XGBClassifier()\ngs_xgb = GridSearchCV(xgb, param_grid=param_grid_xgb, cv=5, scoring=\"recall\", verbose=4)\ngs_xgb.fit(X_train, y_train)\n\ngs_xgb.best_params_ \"\"\"\n\n\"\"\"{'colsample_bytree': 1,\n 'eta': 0.19,\n 'max_depth': 8,\n 'n_estimators': 150,\n 'subsample': 0.8}\"\"\"\n\nxgb = XGBClassifier(eta=0.19, max_depth=8, n_estimators=150, subsample=0.8, colsample_bytree=1)\nxgb.fit(X_train, y_train)\ny_pred_xgb = xgb.predict(X_test)\nprint(accuracy_score(y_pred_xgb, y_test))\nprint(confusion_matrix(y_test, y_pred_xgb))\nprint(classification_report(y_test, y_pred_xgb))\n    \n","5804c261":"x_training, x_valid, y_training, y_valid = train_test_split(X_train, y_train,\n                                                            test_size=0.5,\n                                                            random_state=42)\n#specify models\nmodel1 = LogisticRegression()\nmodel2 = SVC(kernel='linear', gamma=10, C=0.8)\nmodel3 = GaussianNB()\nmodel4 = XGBClassifier(eta=0.19, max_depth=8, n_estimators=150, subsample=0.8, colsample_bytree=1)\n#fit models\nmodel1.fit(x_training, y_training)\nmodel2.fit(x_training, y_training)\nmodel3.fit(x_training, y_training)\nmodel4.fit(x_training, y_training)\n#make pred on validation\npreds1 = model1.predict(x_valid)\npreds2 = model2.predict(x_valid)\npreds3 = model3.predict(x_valid)\npreds4 = model4.predict(x_valid)\n#make pred on test\ntestpreds1 = model1.predict(X_test)\ntestpreds2 = model2.predict(X_test)\ntestpreds3 = model3.predict(X_test)\ntestpreds4 = model4.predict(X_test)\n#form new dataset from valid and test\nstackedpredictions = np.column_stack((preds1, preds2, preds3, preds4))\nstackedtestpredictions = np.column_stack((testpreds1, testpreds2,\n                                              testpreds3, testpreds4))\n#make meta model\nmetamodel = LogisticRegression()\nmetamodel.fit(stackedpredictions, y_valid)\nfinal_predictions = metamodel.predict(stackedtestpredictions)\n    \nprint(accuracy_score(final_predictions, y_test))\nprint(confusion_matrix(y_test, final_predictions))\nprint(classification_report(y_test, final_predictions))\n    \n","b16d9bd8":"from sklearn.metrics import roc_curve, roc_auc_score\n\nresults_table = pd.DataFrame(columns = ['models', 'fpr','tpr','auc'])\n\npredictions = {'LR': y_pred_log, 'SVC': y_pred_svc, 'NB': y_pred_nb, 'XGB': y_pred_xgb, 'Stacked': final_predictions}\n\nfor key in predictions:\n    fpr, tpr, _ = roc_curve(y_test, predictions[key])\n    auc = roc_auc_score(y_test, predictions[key])\n    \n    results_table = results_table.append({'models': key,\n                                         'fpr' : fpr,\n                                         'tpr' : tpr,\n                                         'auc' : auc}, ignore_index=True)\n    \nresults_table.set_index('models', inplace=True)\n\nprint(results_table)\n\nfig = plt.figure(figsize = (8,6))\n\nfor i in results_table.index:\n    plt.plot(results_table.loc[i]['fpr'], \n             results_table.loc[i]['tpr'], \n             label = \"{}, AUC={:.3f}\".format(i, results_table.loc[i]['auc']))\n    \nplt.plot([0,1], [0,1], color = 'black', linestyle = '--')\n\nplt.xticks(np.arange(0.0, 1.1, step=0.1))\nplt.xlabel(\"False Positive Rate\", fontsize=15)\n\nplt.yticks(np.arange(0.0, 1.1, step=0.1))\nplt.ylabel(\"True Positive Rate\", fontsize=15)\n\nplt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\nplt.legend(prop = {'size':13}, loc = 'lower right')\n\nplt.show()","33864c1d":"## 2. EDA & Data Visualization\n\n\n### 2.1. Distribution of Risk across Job Type and Gender\n\nObservations:\n1. Skilled workers are more likely to get 'good' risk rating. This makes sense, as skilled workers tend to earn more and have more job security.\n2. Men, throughout all job types, tend to get a 'good' rating much more often than women. Skilled female workers are more commonly classified as 'bad' than 'good'. This hints towards institutional sexism.","eafe670e":"We can see that we only have missing values in two columns (Saving accounts & Checking account). \n\nI think we can assume that NaNs in both of these columns imply that these people don't have bank accounts. Because if they did, I think that would've been logged into the data. Hence, we can fill in this missing values with 'none'. ","880b0514":"### 3.2. Getting Log Amounts\n\nInstead of using the raw credit amount as our dependent variable, we need to get the log of these amounts. This will make our credit amount distribution more normal. ","141775fa":"## 5. Evaluation\n\nAccording to our evaluation metric, we can see that the Naive-Bayes classifier works the best as it gives us the highest recall. However, when we plot the ROC curves and calculate the AUC for all our models, we can see that SVC and XGB give us the highest AUCs. \n\nSurprisingly, our stacked model does not perform very well. In fact, it has the lowest recall compared to all the base models. \n\nTo be short, as our primary aim is to identify bad loans to the best of our abilities, we should stick with the Naive-Bayes Classifier.","611a2603":"### 4.7. Stacked Model","e6b3f3bf":"## 1. Importing Data & Missing Values","d388f8ea":"### 4.5. Naive-Bayes","690260be":"### 4.1. Logistic Regression","85f9e380":"### 2.3. Distribution of Risk across Bank Account Status\n\nObservations:\n1. When it comes to checking accounts, the richer you are, the more likely you are to be classified as 'good'. However, the disparity between 'good' and 'bad' risk ratings when it comes to 'none' is quite astounding. Remember, these are the people who had 'NaN' in the Checking Account column in the original dataset.\n2. When it comes to saving accounts, the trend holds as well. The richer you are, the more likley you are to be classified as good. However, unlike checking accounts, where those with 'little' money have equal distribution of good and bad ratings, in savings accounts, there are visibly more good ratings than bad ones. This makes sense since the very existence of a savings account implies some degree of financial security. ","b1caec59":"Let's encode the actual values for jobs as described in the data description. Later on, this will make one hot encoding much easier. ","b84a3f83":"## 4. Base Models\n\nI test a number of models. For models that have a range of hyperparameters, I run a grid-search first to identify the ideal hyperparameters. \n\n\n**Evaluation Metric:**\n\nI will be using 'Recall' as my evaluation metric. To revise, Recall is = True positives \/ (True positives + False negatives). In our case, the numerator will represent the correct number of bad loans that our algorithm correctly classified as bad while the denomintor will represent the numerator PLUS the bad loans that our algorithm incorrectly classified as 'good'. In other words, the closer the recall value is to 1, the more compeltely were we able to identify all the bad loans. ","b80b97d3":"### 4.4. Random Forests","afa361f1":"### 4.6. XGBoost","fc178964":"### 4.2. K-Nearest Neighbors","99fe5f7b":"### 2.5. Credit Amount Distribution by Age and Loan Duration\n\nObservations:\n1. We can see that generally, throughout all age gaps, bad loans tend to be of a higher amount but when it comes to people over 55, this gap widens. \n2. We can see that when it comes to loan duration, bad and good loans amounts tend to move in tandem. Moreover, larger loans over a longer duration are more likely to be classified as 'good'. ","ce9d1ec6":"### 4.3. Support Vector Classification","f3f8d377":"### 2.4. Boxplots of Distribution of Credit Amounts Across Several Variables\n\nObservations:\n1. We notice quite a number of interesting things when we look at the 'Credit Amount Distribution by Purpose'. Firstly, we can see that certain 'purposes' are more likely to have 'bad'ratings, specially if the loan amount is high. For example, if we look at vacation\/others, we can see that 'bad' rated loans usually consist of a higher amount. Same can be observed for business loans. Generally, if we exclude furniture\/equipment and repairs categories, almost all other categories have a larger interquartile range when it comes to the credit amounts of 'bad' loans. \n2. The radio\/TV category has a lot of outliers, specially for loans classified as 'good'. This is quite interesting. It is hard to imagine why a person would borrow 15000 DMs from the bank just to buy a radio or a TV, let alone get his loan approved. \n3. When we look at the amount distributions according to bank account statuses, we observe that rich people tend to borrow less compared to poorer people. On one hand, this makes sense as poor people need more money to take care of their needs. On the other hand, it doesn't make sense for a relatively rich person to borrow a very small amount from the bank. \n4. When it comes to job types, skilled workers tend to have a bigger and higher IQR for bad loans than for good loans. This makes sense since larger amounts are more likely to be classified as 'bad'. However, if we look at unskilled borrowers, this disparity doesn't hold true.","e38aab01":"In this notebook, we will use the German credit risk data from the UCI Machine Learning repository and use it to build a classifier that can efficiently identify bad loans.\n\nBefore we start, there are couple of things we should be mindful of:\n\n1. This data is quite old (from 1994), hence the amounts represented are in Deutsch Marks (not Euros).\n2. Given the fact that this data was collected a couple of years after German reunification, there is a possibility that the banks would've been operating under different conditions than they are right now. For example, in the early 1990s, Germany had a much higher [interest rate](https:\/\/www.ecb.europa.eu\/pub\/pdf\/other\/mb200309_focus03.en.pdf) compared to right now. One of the reasons for this was to keep inflation under control. On the other hand, post-unification, a lot of people in the former East Germany realized the marked difference in living standards across the two halves. Hence, we can say that given the access to easy credit in a liberal democracy, German banks were receiving a lot of loan applications in the 1990s, specially from East Germans who wanted to catch up in terms of lifestyle. We can hypothesize that this would've made banks more cautionary when giving out loans. \n3. This data is quite limited. With only 1000 observations and very limited set of variables, predicting is hard. In today's day and age, credit scores (SCHUFA) in Germany are a good way od juding an applicant's credit-worthiness. However, in this dataset, we don't have access to this.","876cc9c1":"### 2.2. Distribution of Risk across Housing Status\n\nObservations:\n1. People who own their own home are more likely to get a 'good' risk rating on their creit compared to people who rent. This is obvious as credit worthiness goes hand in hand with asset ownership.","8cc71c37":"## 3. Feature Engineering\n\n### 3.1. Getting Dummies\n\nWe need to generate dummies for our categorical variables and then drop the redundant variables. Moreover, as Risk is encoded as 'good' or 'bad'. We need to reencode this as binary. I am encoding 'bad' loans as 1 as from the bank's perspective, we are more interested in identifying bad loans. "}}