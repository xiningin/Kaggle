{"cell_type":{"d6295f2a":"code","01d094e4":"code","bbec393e":"code","cbc0758f":"code","6324e46d":"code","abf79b33":"code","a9dc4ba2":"code","04115670":"code","315e013e":"code","b6721f20":"code","c5bcb935":"code","db3f4d70":"code","1f6c925b":"code","960dc442":"code","033ab46e":"code","f05f9645":"code","d30303d5":"code","4693eb5e":"code","88b3b2e0":"code","3bf6c642":"code","7cce75ca":"code","82667ae5":"code","febdfda0":"code","2600477c":"code","24765f49":"code","fd5fd60f":"code","94f36412":"code","f47e8d5b":"code","a0835fbc":"code","34cd2e85":"code","ef72774f":"code","62808afa":"code","26bcfd3f":"code","f35f29ea":"code","40cbb827":"code","df17f414":"code","b03866bf":"code","da9f9530":"code","d26d5815":"code","d7ce43f9":"code","cfb5498c":"code","4168adb0":"code","86cb2de0":"code","445c8c87":"code","95fd365b":"code","415ff18e":"code","4ba7d96b":"code","b8c8a074":"code","c0ea5246":"code","cc507227":"code","4bcf6860":"code","617f4125":"code","261d7cc2":"code","b102be21":"code","d669b793":"code","458479c7":"code","6791ee49":"code","2e39180f":"code","5b55b810":"code","11095d34":"code","e44f7b63":"code","518afcdc":"code","f9a90273":"code","bb61fa36":"code","ebef2dd8":"code","7eb1b83e":"code","1a05d8bc":"code","e72acae2":"code","80b88f03":"code","861c1b61":"code","30dc8615":"code","bf5c16df":"code","1b6c215c":"code","db0d01a0":"code","4e4b4e1b":"code","679d0ba1":"code","ac3313dd":"code","6257d067":"code","ae8f44ae":"code","6648ec6d":"code","fc503693":"code","fc163d02":"code","fcbba84c":"code","169d62c6":"code","fa695a0e":"code","9327f418":"code","e76d806d":"code","bd2e169d":"code","7a73e256":"code","79cb6f9e":"code","6f5a503a":"code","a49fc459":"code","c0153698":"code","b9903831":"code","a87d6ef6":"code","aa997e53":"code","4c38b312":"code","6cce99d8":"code","604694a4":"code","593d785f":"code","4e3b96c7":"code","cae2e41e":"code","910fcdb1":"code","e907d5ad":"code","c0180a2e":"code","fd70a276":"code","e4152a84":"code","52ea903b":"code","10ae6595":"code","c1df20b7":"code","2183a11c":"code","40d551db":"code","79542c55":"code","df3eff72":"code","dd4d4c66":"code","8ab3af6e":"code","c33c7239":"code","82df147d":"code","a113d0b5":"code","120120ad":"code","d351db36":"code","67b14b4b":"code","ccac3a81":"code","b1162ee4":"code","62bcb6dc":"code","be3a7918":"code","428d90f6":"code","ae58ae91":"code","d1b9d653":"code","08ab4efb":"code","5186a17d":"code","a26bd375":"code","d8826408":"code","1ec507fd":"code","54dac204":"code","26c9dca7":"code","0d2321b6":"code","d497b677":"code","c5b1c698":"code","48f6b500":"code","b447222e":"code","8f7f5728":"code","c1809e1d":"code","3aed11aa":"code","056d2512":"code","267b529d":"code","d08aa453":"code","5fd57efa":"code","3bfed493":"code","541dea79":"code","43d6a8e9":"code","864a1b4a":"code","2f6cde55":"code","319599a7":"code","6e4bee5d":"code","ac95f358":"code","d5ddb5cd":"code","243e122f":"code","03e76d04":"code","7b403cee":"code","78f6aa51":"code","098aa1ff":"code","8ab8d09d":"code","e2cd1c27":"code","94de4d0c":"code","b34d3653":"code","a9e11b35":"code","c4686a73":"code","f139ebf6":"code","01cdbe9b":"code","79db3f12":"code","8250e736":"code","5b5aa375":"code","615a9673":"code","b4fca9ec":"code","410d32d7":"code","b9b98626":"code","2dee5a1a":"code","5aa4d16c":"code","9408cdc1":"code","8a231271":"code","05672a5b":"code","93582ad0":"code","2023f609":"code","4f8b7261":"code","2fc33be5":"code","4f058637":"code","299c5ab4":"code","531a8c2d":"code","d765eef4":"code","187184c2":"code","283e49b8":"code","8491f981":"code","64d20680":"code","e60cb5a5":"code","efbe28bd":"code","d2a78929":"code","f754e624":"code","7c15be6a":"code","53659068":"code","50af33ea":"code","dd426d0e":"code","c5f862f7":"code","903265bc":"code","700d48cd":"code","d6b222c3":"code","f345d18c":"code","e14277ca":"code","c93b9fd6":"code","72fa350b":"code","0c205bb1":"code","0519cbfc":"code","89db1a55":"code","234134bd":"code","62905211":"code","98cb5834":"code","0f1148a6":"code","a7782af4":"code","6acb6274":"code","14bc3b38":"code","6487bd1a":"code","445ba819":"code","a27e7f61":"code","e7193453":"code","58dd2afc":"code","0dbbd82c":"code","25ca9461":"code","bbfec565":"code","d254ba24":"code","9b15a9d4":"code","49f7698f":"code","427d49dc":"code","606015b7":"code","14dc36b5":"code","55f6eff3":"code","37e542c4":"code","e0196aac":"code","3bdb862f":"code","12c3d88f":"code","257a3ba6":"code","6a80098a":"code","0a1743dc":"code","dc793e5e":"code","ed40b2a9":"code","e0e1aaef":"code","53f9fe75":"code","95228a42":"code","49601e6a":"code","96042742":"code","3d113d81":"code","572a95cb":"code","f1a5f761":"code","9ef727b9":"code","bffac7c4":"code","1044cc1c":"code","eb5165ae":"code","ac7fd8a3":"code","a734ac5f":"code","2c35d8af":"code","c5213e11":"code","9c01df43":"code","94a535ae":"code","daba7923":"code","9ed1aa30":"code","991fd724":"code","b3e7a468":"code","cdc765af":"code","3f31c194":"code","0d83fa7b":"code","c1fa1d05":"code","bdafd059":"code","b5c3cd81":"code","48260d8b":"code","7d4968cc":"code","db90fa6f":"code","140931cc":"code","b0930c5d":"code","04044143":"code","64b0dccd":"code","3e1dec95":"code","682bc513":"code","2d2c49be":"code","a7de7b3b":"code","e73a58c4":"code","a18fc4c9":"code","70c69d51":"code","8f096c79":"code","c8887bb8":"code","715fc4a7":"code","44e354ac":"code","d9673b63":"code","b0090795":"code","1018eff6":"code","7b7fdbc0":"code","8663dc52":"code","929a8fca":"code","598e2d53":"markdown","b6014fb2":"markdown","108c04bb":"markdown","b0456af2":"markdown","f01daad9":"markdown","492ba3ff":"markdown","bc566c9d":"markdown","837d53e0":"markdown","47ce5aea":"markdown","674849f9":"markdown","8036c949":"markdown","fe46083c":"markdown","34e20122":"markdown","6ab9bbfe":"markdown","df928df8":"markdown","a82d48fb":"markdown","9789f9e8":"markdown","db3acdcf":"markdown","1ef698ba":"markdown","26489a21":"markdown","dfe9408b":"markdown","66a50983":"markdown","200d5394":"markdown","dc05db3a":"markdown","cce1e214":"markdown","98fe5a2d":"markdown","899b174a":"markdown","7d682166":"markdown","a7e64573":"markdown","4b7604b3":"markdown","711aaeb9":"markdown","fc44faf6":"markdown","ff993950":"markdown","fc629b36":"markdown","f688bd67":"markdown","1db2c39a":"markdown","043b0351":"markdown","75b07071":"markdown","972ce885":"markdown","b74d9cea":"markdown","84096190":"markdown","b9f81294":"markdown","4abe8773":"markdown","bb15249d":"markdown","5c161dad":"markdown","1eb2324e":"markdown","40adf064":"markdown","0073e7aa":"markdown","721ce6e9":"markdown","c1b23259":"markdown","b0225ad7":"markdown","497ec1a8":"markdown","76396f13":"markdown","4e7cc4f9":"markdown","d6e04fe0":"markdown","96edf3d5":"markdown","f017ccbe":"markdown","45f38e3f":"markdown","8c022925":"markdown","e9082ad0":"markdown","38f9912f":"markdown","55501669":"markdown","58c3c1e2":"markdown","daba9432":"markdown","b16b3252":"markdown","0b3133f9":"markdown","097d6c60":"markdown","e4c05c2d":"markdown","0da3018e":"markdown","cbfd9bca":"markdown","ae39c8f8":"markdown","1750983a":"markdown","a2bd6894":"markdown","5ca18d2a":"markdown","e8ae4287":"markdown","6aeb485e":"markdown","6f014b7c":"markdown","f8ec6dd4":"markdown","a5787ec2":"markdown","86a8d00f":"markdown","8ef3a317":"markdown","5edda2a4":"markdown","cb8856ad":"markdown","f38afea8":"markdown","356e6a33":"markdown","8bea1111":"markdown","78f6030d":"markdown","1b4b39e9":"markdown","81157109":"markdown","fe3e41fc":"markdown","dedeab10":"markdown","8b7114af":"markdown","0996fa90":"markdown","9572857a":"markdown","8ce02f6d":"markdown","db34891d":"markdown","875cc969":"markdown","54cd8618":"markdown","5576a0e5":"markdown","1b6a2609":"markdown","b476f23e":"markdown","18eb177f":"markdown","7c8c2477":"markdown","8f7bd322":"markdown","b7304b37":"markdown","7bb6d1f5":"markdown","c3c4e196":"markdown","1336a374":"markdown","1c9d801a":"markdown","0033dd3c":"markdown","01c99428":"markdown","d2a5c47b":"markdown","0fe89119":"markdown","8ae392da":"markdown","69c5f88c":"markdown","abac3015":"markdown","e7cd8743":"markdown","ec490b49":"markdown","9d5a8e2c":"markdown","a380a730":"markdown","dfb0a7b3":"markdown","bf0db4d4":"markdown","789d7c7e":"markdown","29074e11":"markdown","b40e0950":"markdown","6337c720":"markdown","9ff4ff2b":"markdown","638f3c6d":"markdown","749e76a0":"markdown","8423c25c":"markdown","46d17140":"markdown","daf61870":"markdown","438bcda3":"markdown","0f9fdc66":"markdown","3390ea43":"markdown","1d781324":"markdown","8a9e35d1":"markdown"},"source":{"d6295f2a":"%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline","01d094e4":"# export\n# from exp.nb_02 import *\nimport torch.nn.functional as F","bbec393e":"#export\n# from exp.nb_01 import *\n\ndef get_data():\n    path = datasets.download_data(MNIST_URL, ext='.gz')\n    with gzip.open(path, 'rb') as f:\n        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n    return map(tensor, (x_train,y_train,x_valid,y_valid))\n\ndef normalize(x, m, s): return (x-m)\/s  #since we know about boardcasting we make a funktion that our tensor(x) subrat with the mean(m) and divided with standard diviation(s)\ndef test_eq(a,b): test(a,b,operator.eq,'==')","cbc0758f":"#export test if the floats are neer since we cant just compare floats with eachother \ndef near(a,b): return torch.allclose(a, b, rtol=1e-3, atol=1e-5)\ndef test_near(a,b): test(a,b,near)","6324e46d":"#export\nfrom pathlib import Path\nfrom IPython.core.debugger import set_trace\nfrom fastai import datasets\nimport pickle, gzip, math, torch, matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom torch import tensor\n\nMNIST_URL='http:\/\/deeplearning.net\/data\/mnist\/mnist.pkl'","abf79b33":"#export # to test if the mean is near 0 and std is near 0 \ndef test_near_zero(a,tol=1e-3): assert a.abs()<tol, f\"Near zero: {a}\"","a9dc4ba2":"path = datasets.download_data(MNIST_URL, ext='.gz'); path","04115670":"with gzip.open(path, 'rb') as f:\n    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')","315e013e":"x_train,y_train,x_valid,y_valid = map(tensor, (x_train,y_train,x_valid,y_valid))\nn,c = x_train.shape\nx_train, x_train.shape, y_train, y_train.shape, y_train.min(), y_train.max()","b6721f20":"mpl.rcParams['image.cmap'] = 'gray'","c5bcb935":"n,m = x_train.shape\nc = y_train.max()+1\nnh = 50","db3f4d70":"\n#export\nfrom torch import nn","1f6c925b":"class Model(nn.Module): #we made i in last lesson \n    def __init__(self, n_in, nh, n_out):\n        super().__init__()\n        self.layers = [nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)]\n        \n    def __call__(self, x):\n        for l in self.layers: x = l(x)\n        return x","960dc442":"model = Model(m, nh, 10) #make the model","033ab46e":"pred = model(x_train) #predictions ","f05f9645":"def log_softmax(x): return (x.exp()\/(x.exp().sum(-1,keepdim=True))).log() #this is the [None] = (-1,keepdim=True) it is to make sure the result will broadcast correctly against x.exp.\n# other then that we use .log() because in Pytorch the negative log likelyhood expect a log softmax not just a softmax","d30303d5":"sm_pred = log_softmax(pred)","4693eb5e":"y_train[:3] #that start looking at our depended veriable and lets just look at the first 3 values \n#so now we want find the proberbillety accosiated with 5, the proberbillety accosiated with 0 and the proberbillety accosiated with 4","88b3b2e0":"sm_pred[0][5] #this is how we index into one prediction ","3bf6c642":"sm_pred[[0,1,2], [5,0,4]] #this is how we index into the 3 numbers a list of dimensions [[0,1,2], [5,0,4]] and we have 2 dimensions\n# next is all he row indexses we want [0,1,2] and the [5,0,4] is a list of all he columns indexses we want. So this will\n# end up returning [0,5],[1,0] and [2,4] ","7cce75ca":"y_train.shape[0] #this is how we call on all the rows in our target","82667ae5":"#negative log likelyhood\ndef nll(input, target): return -input[range(target.shape[0]), target].mean() #input = lets look into our predictions\n#range(target.shape[0]) and for our row indexses, it is every singel row we takes so from 0-50000 and target is each column \n#we then take the mean() to it all, the reason we use -input is because it is the negative log likelyhood","febdfda0":"loss = nll(sm_pred, y_train) #now we can find the loss by taking the negative log likelyhood with the accatual prediction on the target","2600477c":"loss","24765f49":"def log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log() #note we can rewrite the softmax function, with the knowledge\n# of reforming the log(a\/b) to log(a)-log(b) and  ","fd5fd60f":"(nll(log_softmax(pred), y_train), loss) #we can see they are the same so the to formulars does the same ","94f36412":"#we can see from above that the code (x.exp().sum(-1,keepdim=True).log()) just takes the log then sum then the exponeltial, \n#so in Pytorch we have LogSumExp which we implement as so: and the reason we need this trick is because when we take somthe\n#to the power of x.exp() we can get exstreme big numbers, and every big number we have as a float will be inacurrat but the compter\n#think it is the same even though they can be 1000 or more apart\ndef logsumexp(x):\n    m = x.max(-1)[0] #lets find the maxisimum\n    return m + (x-m[:,None]).exp().sum(-1).log() #lets subrat it from all of w\u00f3ur x\u00b4ses and then lets do the\n#LogSumExp and in the end we add the maxsimum number to the \"ligning\" so it gives the same number ","f47e8d5b":"(logsumexp(pred), pred.logsumexp(-1)) #logsumexp is already in pytorch ","a0835fbc":"def log_softmax(x): return x - x.logsumexp(-1,keepdim=True) #pytorchs LogSumExp","34cd2e85":"(nll(log_softmax(pred), y_train), loss) #compare the selfwritten log_softmax with pytorch","ef72774f":"(F.nll_loss(F.log_softmax(pred, -1), y_train), loss) #same","62808afa":"(F.cross_entropy(pred, y_train), loss)  #this is called F.cross_entrop and is = (F.nll_loss(F.log_softmax\n#and we can see it is the same as our loss","26bcfd3f":"loss_func = F.cross_entropy","f35f29ea":"#export\ndef accuracy(out, yb): return (torch.argmax(out, dim=1)==yb).float().mean() #so we make a function that can find our accuracy\n# we did in part 1 by hand, but we use argmax here, that means that we take the highest number in out softmax\n#and the index of that is out prediction and we check if that is = to the accuracal (==yb). \n#we want to take the mean of that, but in order to do that in pytorch we have to convert it to a float. since we cant take the mean to an  int in pytorch ","40cbb827":"#lets check the accuracy function \n#so we grap our first batch (so minibatch)\nbs=64                  # batch size\n\nxb = x_train[0:bs]     # a mini-batch from x\npreds = model(xb)      # predictions\npreds[0], preds.shape","df17f414":"yb = y_train[0:bs] #now we can take the same size of the dependents veriables \nloss_func(preds, yb) #and calulate our loss ","b03866bf":"accuracy(preds, yb) #now we can calulate our accuracy\n#and it is so low because we havent trained our model yet so this is just a random answer","da9f9530":"#so now we will train it \n\nlr = 0.5   # learning rate\nepochs = 1 # how many epochs to train for\nprint(n)","d26d5815":"#training loop\n\nfor epoch in range(epochs):\n    for i in range((n-1)\/\/bs + 1): #lets go through i up to n-1 where n is 50000(number of rows in training data) \n        #we divide it with bs because we wanna do a batch at the time \n#         set_trace()\n        start_i = i*bs #then we grap everything i times the batch (the start of the minibatcj)\n        end_i = start_i+bs #and ending at the start_i + the bacth (the end of the minibatcj)\n        #so the to lines above is our first i\u00b4th minibatch \n        \n        xb = x_train[start_i:end_i] #then lets grap one x minibatch\n        yb = y_train[start_i:end_i] #one y minibatch\n        loss = loss_func(model(xb), yb) #and let pass that through out loss fucntion\n\n        loss.backward() #then lets do backward \n        with torch.no_grad(): #then we do our update and we use no_grad since this is not a part of gradient calulation but just the result from it \n            #now we have to update the model, so we use only layers that have paramenters(weight) in them. Here Relu is an activation layer and have no parameters\n            #so we just have to linear layers in our difined model above. This means also we have four tensors to deal with\n            for l in model.layers: #we are gonna go through all of our layers \n                if hasattr(l, 'weight'): #we do a check to make sure the layer have weight in it \n                    l.weight -= l.weight.grad * lr # update by taking the -= the gradient to the weight times the learning rate\n                    l.bias   -= l.bias.grad   * lr #update by taking the -= the gradient to the bias times the learning rate\n                    l.weight.grad.zero_() # now we zero those gradients so they are ready to next run \n                    l.bias  .grad.zero_()\n\n#the next thing we will do is to make the updata part better and use less code ","d7ce43f9":"loss_func(model(xb), yb), accuracy(model(xb), yb) #and now we can see the accuracy improved from 0.15 to 1 which is much better\n#though do note that the minibach is from the training set, that we also do the prediction on, so it dosent mean to much\n#but we can see that our model is learning something so this is good. ","cfb5498c":"class Model(nn.Module): #Pytorch also have a class that does the same as DummyModule() (defined below) \n    def __init__(self, n_in, nh, n_out):\n        super().__init__() #super() setup the _modules in the DummyModule() class. so this is why we need it \n        self.l1 = nn.Linear(n_in,nh)\n        self.l2 = nn.Linear(nh,n_out)\n        \n    def __call__(self, x): return self.l2(F.relu(self.l1(x)))","4168adb0":"model = Model(m, nh, 10)","86cb2de0":"for name,l in model.named_children(): print(f\"{name}: {l}\")","445c8c87":"model","95fd365b":"model.l1","415ff18e":"def fit():\n    for epoch in range(epochs):\n        for i in range((n-1)\/\/bs + 1):\n            start_i = i*bs\n            end_i = start_i+bs\n            xb = x_train[start_i:end_i]\n            yb = y_train[start_i:end_i]\n            loss = loss_func(model(xb), yb)\n\n            \n            #this does the updates and the parameters, use use the function (.parameters()) from the DummyModule() below \n            loss.backward()\n            with torch.no_grad():\n                for p in model.parameters(): p -= p.grad * lr\n                model.zero_grad()","4ba7d96b":"fit()\nloss_func(model(xb), yb), accuracy(model(xb), yb)","b8c8a074":"class DummyModule():\n    def __init__(self, n_in, nh, n_out):\n        self._modules = {}\n        self.l1 = nn.Linear(n_in,nh) #so we start by defineing, tha every time we define a veriable like l1 or l2 (in this chase to a linear)\n        #so we now want to update the self._modules = {} with a list of all the modules (l1 and l2 in this chase)\n        self.l2 = nn.Linear(nh,n_out)\n        \n    def __setattr__(self,k,v): # __setattr__ goes into self. \n        if not k.startswith(\"_\"): self._modules[k] = v #check if the key (like self.l1) dosent start with '_' because if it does\n            #it maight be _modules or something in python.  self._modules[k] = v we put this in our _modules dirsonary and call it k\n        super().__setattr__(k,v) #do whatever the superclass does and here the superclass is an object (defalt mode, when it isnt defined)\n        \n    def __repr__(self): return f'{self._modules}' #printing the list if called apon the class\n    \n    def parameters(self): #now we can define a method called parameters \n        for l in self._modules.values(): #go through everything in _modules (in tthis chase is it l1 and l2) \n            for p in l.parameters(): yield p #and then go through all of their parameters ","c0ea5246":"mdl = DummyModule(m,nh,10)\nmdl","cc507227":"[o.shape for o in mdl.parameters()]","4bcf6860":"layers = [nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10)] #making it a list of layers ","617f4125":"class Model(nn.Module):\n    def __init__(self, layers):\n        super().__init__()\n        self.layers = layers #pytorch are only going to accept something it accept as proper nn.modules\n        for i,l in enumerate(self.layers): self.add_module(f'layer_{i}', l) #so we just go through all of the layers \n            #self.add_module is the same as  self._modules[k] = v from he code above \n        \n    def __call__(self, x):\n        for l in self.layers: x = l(x)\n        return x","261d7cc2":"model = Model(layers)","b102be21":"model","d669b793":"class SequentialModel(nn.Module):\n    def __init__(self, layers):\n        super().__init__()\n        self.layers = nn.ModuleList(layers) #it would be esayer of pytorch did it for us, so the nn.ModuleList goes through all\n        #the layers\n        \n    def __call__(self, x):\n        for l in self.layers: x = l(x)\n        return x\n\n    #and even this can be written esayer ","458479c7":"model = SequentialModel(layers)","6791ee49":"model","2e39180f":"fit()\nloss_func(model(xb), yb), accuracy(model(xb), yb)","5b55b810":"#so this does the same as SequentialModel class we wrote above \nmodel = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))","11095d34":"fit()\nloss_func(model(xb), yb), accuracy(model(xb), yb)","e44f7b63":"nn.Sequential??","518afcdc":"model","f9a90273":"class Optimizer():\n    def __init__(self, params, lr=0.5): #pass in some parameter (params) and a learning rate (lr) and store those away\n        self.params,self.lr=list(params),lr \n        \n    def step(self): #making step()\n        with torch.no_grad(): #we use no_grad since this is not a part of gradient calulation but just the result from it\n            for p in self.params: p -= p.grad * lr #go through each parameter and update it with the gradient time the learning rate for all of the parameters\n\n    def zero_grad(self): #making zero_grad\n        for p in self.params: p.grad.data.zero_()  #going through the list of parameters you asked to update and zero those gradients ","bb61fa36":"model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10)) #creating model ","ebef2dd8":"opt = Optimizer(model.parameters()) #call optimizer class and pass the models paramters. Here we store it in opt","7eb1b83e":"for epoch in range(epochs):\n    for i in range((n-1)\/\/bs + 1):\n        start_i = i*bs\n        end_i = start_i+bs\n        xb = x_train[start_i:end_i]\n        yb = y_train[start_i:end_i]\n        pred = model(xb)\n        loss = loss_func(pred, yb)\n\n        loss.backward()\n        opt.step() #and now we can use it \n        opt.zero_grad()","1a05d8bc":"loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb) #and here we can see that we get a acc at 0.9375 which is okay\nloss,acc","e72acae2":"#export\nfrom torch import optim #and now since have impimentet the optimazer class we can just use pytorchs ","80b88f03":"# optim.SGD.step?? #here we can see how they implemented it thoug note that they alson have momentum and weight decay and more \n#of which we will implement later","861c1b61":"def get_model(): #greate a get model, where we create the model with optim.SGD, which does the same as our Optimazer in this chase\n    model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\n    return model, optim.SGD(model.parameters(), lr=lr) #lr=0.5 its defined above","30dc8615":"model,opt = get_model()\nloss_func(model(xb), yb) #check the loss function","bf5c16df":"#training loop \nfor epoch in range(epochs):\n    for i in range((n-1)\/\/bs + 1):\n        start_i = i*bs\n        end_i = start_i+bs\n        xb = x_train[start_i:end_i]\n        yb = y_train[start_i:end_i]\n        pred = model(xb)\n        loss = loss_func(pred, yb)\n\n        loss.backward()\n        opt.step()\n        opt.zero_grad()","1b6c215c":"loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\nloss,acc","db0d01a0":"assert acc>0.7 #we just test if the acc is better then 70%","4e4b4e1b":"#export #now we wnat the create our dataseet class \nclass Dataset():\n    def __init__(self, x, y): self.x,self.y = x,y #we create places for x and y \n    def __len__(self): return len(self.x) #take the lenth of x \n    def __getitem__(self, i): return self.x[i],self.y[i] #donda getitem means that when we index into len it returns x[i] and y[i]","679d0ba1":"train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid) #creating a validation and training dataset \nassert len(train_ds)==len(x_train) #check that the lenth is right \nassert len(valid_ds)==len(x_valid) #check that the lenth is right ","ac3313dd":"xb,yb = train_ds[0:5] #check the first 5 values \nassert xb.shape==(5,28*28) #make sure the shape is right \nassert yb.shape==(5,) #make sure the shape is right \nxb,yb #printing values","6257d067":"model,opt = get_model() #creating model and we can set them in model and opt, since those are the things we return in get_model class","ae8f44ae":"#creating trainig loop \nfor epoch in range(epochs): \n    for i in range((n-1)\/\/bs + 1):\n        xb,yb = train_ds[i*bs : i*bs+bs] #and now we have replaced the to lines of code with one \n        pred = model(xb)\n        loss = loss_func(pred, yb)\n\n        loss.backward()\n        opt.step()\n        opt.zero_grad()","6648ec6d":"loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb) #checking the acc score\nassert acc>0.7\nloss,acc","fc503693":"class DataLoader(): \n    def __init__(self, ds, bs): self.ds,self.bs = ds,bs #takes a dataset (ds) and a batch size (bs) and stores them away\n    def __iter__(self): #iterate through \n        for i in range(0, len(self.ds), self.bs): yield self.ds[i:i+self.bs] #for i in range(0, len(self.ds) so loop through from\n            #0 to the size of the dataset so from 0 til 50000. (, self.bs)) - taking one batch size (bs) at the time \n            # and each time we go through we will yield our dataset at a index starting at i til i+bacth size (bs)\n            #yield means it like a function that dosent return one thing ones. But it can return a lot of things and you\n            #can ask for it a lots of times. see it like a subroutine ","fc163d02":"train_dl = DataLoader(train_ds, bs) #now we can create aa training dataloader and\nvalid_dl = DataLoader(valid_ds, bs) #a validation one ","fcbba84c":"xb,yb = next(iter(valid_dl)) #iter create the co-routine and next grap the next thing yielded out of that co-routine \nassert xb.shape==(bs,28*28) #check shape\nassert yb.shape==(bs,) #check shape","169d62c6":"plt.imshow(xb[0].view(28,28))\nyb[0]","fa695a0e":"model,opt = get_model()","9327f418":"#now we cant fit the model or in other words its a traiing loop \ndef fit():\n    for epoch in range(epochs): #go through each epoch\n        for xb,yb in train_dl: #go through each batch in the independed and depended veriables in train data\n            pred = model(xb) #calulate the preditiction\n            loss = loss_func(pred, yb) ##calulate the losses\n            loss.backward() #calulate the gradients \n            opt.step() #update with a learning rate \n            opt.zero_grad() #reset the gradients ","e76d806d":"fit()","bd2e169d":"loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\nassert acc>0.7\nloss,acc","7a73e256":"class Sampler(): #samler class\n    def __init__(self, ds, bs, shuffle=False): #so we take in the dataset (ds) and the batch size (bs) and set the shuffle to False\n        self.n,self.bs,self.shuffle = len(ds),bs,shuffle #and then we store them away but here note we dont store the dataset away just the lenth of the dataset\n        #so we know how many items to sample \n    def __iter__(self): #so remember this is the one we can call the 'next' on a lot of times  \n        self.idxs = torch.randperm(self.n) if self.shuffle else torch.arange(self.n) #so iif we are shuffling (if self.shuffle) then\n        #let us take a random permutation from n til minus 1 and if we are ot shuffling then lets grap the number in order (torch.arange(self.n))\n        for i in range(0, self.n, self.bs): yield self.idxs[i:i+self.bs] #and now we dot hee same as above ","79cb6f9e":"small_ds = Dataset(*train_ds[:10]) #lets take 10 values from training set","6f5a503a":"s = Sampler(small_ds,3,False) #here we use shuffling where we set it to False (so we dont shuffle )\n[o for o in s] #and we see that it is ordered","a49fc459":"s = Sampler(small_ds,3,True) #here we use shuffling where we set it to True (so we do shuffle )\n[o for o in s] #and we see that it is not ordered","c0153698":"def collate(b):\n    #create tensorser \n    xs,ys = zip(*b) #grap the x and y\u00b4s \n    return torch.stack(xs),torch.stack(ys) #and stack them up. So  torch.stack just take a bunch of tensor and grue them together \n\nclass DataLoader():\n    def __init__(self, ds, sampler, collate_fn=collate): #we created samplers before so now are gonna use them. \n        self.ds,self.sampler,self.collate_fn = ds,sampler,collate_fn\n        \n    def __iter__(self):\n        for s in self.sampler: yield self.collate_fn([self.ds[i] for i in s])\n            #for s in sampler means that we are gonna loop through the samples in above like [6,1,7], [8,2,9] and so on. \n            #and because we used yield these (sampler) are only gonna be calulate when we call on them \n            #so we can use these on big dataset\n            # then by (for i in s]) we are gonna grap all of the indexses in a given sample and we gonna grap the dataset at that index (self.ds[i])\n            #so now we got a list of tensors and we need a way to collate them together into a singel per of tensors and the collate() function above does this \n            ","b9903831":" #now we can create our to samplers \ntrain_samp = Sampler(train_ds, bs, shuffle=True)\nvalid_samp = Sampler(valid_ds, bs, shuffle=False)","a87d6ef6":" #now we can create our to dataloaders. note here the training set is shuffled and the validation is not \ntrain_dl = DataLoader(train_ds, sampler=train_samp, collate_fn=collate)\nvalid_dl = DataLoader(valid_ds, sampler=valid_samp, collate_fn=collate)","aa997e53":"xb,yb = next(iter(valid_dl))\nplt.imshow(xb[0].view(28,28))\nyb[0]","4c38b312":"xb,yb = next(iter(train_dl))\nplt.imshow(xb[0].view(28,28))\nyb[0]","6cce99d8":"xb,yb = next(iter(train_dl))\nplt.imshow(xb[0].view(28,28))\nyb[0]","604694a4":"model,opt = get_model()\nfit()\n\nloss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\nassert acc>0.7\nloss,acc","593d785f":"#export\nfrom torch.utils.data import DataLoader, SequentialSampler, RandomSampler","4e3b96c7":"train_dl = DataLoader(train_ds, bs, sampler=RandomSampler(train_ds), collate_fn=collate) #RandomSampler shuffle them as above\nvalid_dl = DataLoader(valid_ds, bs, sampler=SequentialSampler(valid_ds), collate_fn=collate) #SequentialSampler order it as above","cae2e41e":"model,opt = get_model()\nfit()\nloss_func(model(xb), yb), accuracy(model(xb), yb)","910fcdb1":"train_dl = DataLoader(train_ds, bs, shuffle=True, drop_last=True) #another way of doing the same as RandomSampler\nvalid_dl = DataLoader(valid_ds, bs, shuffle=False) #another way of doing the same as SequentialSampler","e907d5ad":"model,opt = get_model()\nfit()\n\nloss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\nassert acc>0.7\nloss,acc","c0180a2e":"def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n    #same training loop as before \n    for epoch in range(epochs):\n        # Handle batchnorm \/ dropout\n        model.train()\n#         print(model.training)\n        for xb,yb in train_dl:\n            loss = loss_func(model(xb), yb)\n            loss.backward()\n            opt.step()\n            opt.zero_grad()\n\n        model.eval()\n#         print(model.training)\n        \n        #same as before but where torch.no_grad goes through the validation set \n        #note also that we dont use backward(since we isnt training and the validation set) insted we just keep\n        #track on the losses and track on the accuracy \n        with torch.no_grad():\n            tot_loss,tot_acc = 0.,0.\n            for xb,yb in valid_dl:\n                pred = model(xb)\n                tot_loss += loss_func(pred, yb)\n                tot_acc  += accuracy (pred,yb)\n        nv = len(valid_dl) # how big is our dataloader an dhow many batches are there \n        print(epoch, tot_loss\/nv, tot_acc\/nv) #and we devide the nv with the total loss \n    return tot_loss\/nv, tot_acc\/nv","fd70a276":"#export\ndef get_dls(train_ds, valid_ds, bs, **kwargs):\n    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n            DataLoader(valid_ds, batch_size=bs*2, **kwargs)) #since we dont have to do the backward pass, so we have twice as much space\n            # therefor we can multipy the batch size with 2.","e4152a84":"#now we can go and fit the model, and we do 5 epochs \ntrain_dl,valid_dl = get_dls(train_ds, valid_ds, bs)\nmodel,opt = get_model()\nloss,acc = fit(5, model, loss_func, opt, train_dl, valid_dl)","52ea903b":"assert acc>0.9 #and we just chech that the acc. is above 90 % \n#and the above is actuel acc. since we created a validations set that we used in the fit and ","10ae6595":"train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)\nnh,bs = 50,64 #so we got 50 hidden layer and batch size on 64 \nc = y_train.max().item()+1 #we define c as the last layer of activattions we need. which is our maxisinum y-value \nloss_func = F.cross_entropy","c1df20b7":"#lets create a class that put together training set and validation set. And therefor have a combined class for both\nclass DataBunch():\n    def __init__(self, train_dl, valid_dl, c=None): # let define training and validation sets and store them away and thats it\n        #we make it optinal to pass in 'c' and we pass None right now because we will use it later \n        self.train_dl,self.valid_dl,self.c = train_dl,valid_dl,c\n        \n    @property\n    def train_ds(self): return self.train_dl.dataset #but for conviense let make it eseay to grap them from the class\n        \n    @property\n    def valid_ds(self): return self.valid_dl.dataset","2183a11c":"data = DataBunch(*get_dls(train_ds, valid_ds, bs), c) #now lets use DataBunch and put all our data in a veriable called data \n#and we pass in 'c' which will be used later","40d551db":"#export\ndef get_model(data, lr=0.5, nh=50):\n    m = data.train_ds.x.shape[1] #the input number is size of the training data \n    model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,data.c)) #data.c is predefined since we from the beginning defined how many last activation layer need. Because the data know how many activations it needs  \n    return model, optim.SGD(model.parameters(), lr=lr)\n\nclass Learner():\n    def __init__(self, model, opt, loss_func, data): #now lets store away the rest from the into above \n        self.model,self.opt,self.loss_func,self.data = model,opt,loss_func,data #just storing them. And note that the learner has no logi itself ","79542c55":"learn = Learner(*get_model(data), loss_func, data) #now lets create the learn veriable where we pass in the functions from above ","df3eff72":"#training loop\ndef fit(epochs, learn):\n    for epoch in range(epochs):\n        learn.model.train() #it is the same as before thought we replace model.train() with learn.model.train()\n        for xb,yb in learn.data.train_dl: #every time we have data.train we replace it with learn.data.train\n            loss = learn.loss_func(learn.model(xb), yb)#the rest is the same as before \n            loss.backward()\n            learn.opt.step()\n            learn.opt.zero_grad()\n\n        learn.model.eval()\n        with torch.no_grad():\n            tot_loss,tot_acc = 0.,0.\n            for xb,yb in learn.data.valid_dl:\n                pred = learn.model(xb)\n                tot_loss += learn.loss_func(pred, yb)\n                tot_acc  += accuracy (pred,yb)\n        nv = len(learn.data.valid_dl)\n        print(epoch, tot_loss\/nv, tot_acc\/nv)\n    return tot_loss\/nv, tot_acc\/nv","dd4d4c66":"loss,acc = fit(1, learn)","8ab3af6e":"def one_batch(xb,yb):\n    pred = model(xb)\n    loss = loss_func(pred, yb)\n    loss.backward()\n    opt.step()\n    opt.zero_grad()\n\ndef fit():\n    for epoch in range(epochs):\n        for b in train_dl: one_batch(*b)","c33c7239":"\ndef one_batch(xb, yb, cb):\n    if not cb.begin_batch(xb,yb): return # here means cb=CallBack \n    loss = cb.learn.loss_func(cb.learn.model(xb), yb)\n    if not cb.after_loss(loss): return\n    loss.backward()\n    if cb.after_backward(): cb.learn.opt.step()\n    if cb.after_step(): cb.learn.opt.zero_grad()\n\ndef all_batches(dl, cb): #so all_batches go through every batch \n    for xb,yb in dl:\n        one_batch(xb, yb, cb)\n        if cb.do_stop(): return\n\ndef fit(epochs, learn, cb):\n    if not cb.begin_fit(learn): return\n    for epoch in range(epochs):\n        if not cb.begin_epoch(epoch): continue\n        all_batches(learn.data.train_dl, cb) #the trainig loopp has to loop through every batch \n        \n        if cb.begin_validate():\n            with torch.no_grad(): all_batches(learn.data.valid_dl, cb) #and the validation loop has to go throguh every batch \n        if cb.do_stop() or not cb.after_epoch(): break \n    cb.after_fit()","82df147d":"class Callback(): #these are all the CallBack\n    def begin_fit(self, learn):\n        self.learn = learn\n        return True\n    def after_fit(self): return True\n    def begin_epoch(self, epoch):\n        self.epoch=epoch\n        return True\n    def begin_validate(self): return True\n    def after_epoch(self): return True\n    def begin_batch(self, xb, yb):\n        self.xb,self.yb = xb,yb\n        return True\n    def after_loss(self, loss):\n        self.loss = loss\n        return True\n    def after_backward(self): return True\n    def after_step(self): return True","a113d0b5":"class CallbackHandler(): #and for the callback calls we need a callback handler \n    def __init__(self,cbs=None):\n        self.cbs = cbs if cbs else []\n\n    def begin_fit(self, learn):\n        self.learn,self.in_train = learn,True\n        learn.stop = False\n        res = True\n        for cb in self.cbs: res = res and cb.begin_fit(learn) #go through the call back and se if we resive a False, it means dont keep going any more\n        return res #and then return it \n    #we do this for all the callbacks \n\n    def after_fit(self):\n        res = not self.in_train\n        for cb in self.cbs: res = res and cb.after_fit()\n        return res\n    \n    def begin_epoch(self, epoch):\n        self.learn.model.train()\n        self.in_train=True\n        res = True\n        for cb in self.cbs: res = res and cb.begin_epoch(epoch)\n        return res\n\n    def begin_validate(self):\n        self.learn.model.eval()\n        self.in_train=False\n        res = True\n        for cb in self.cbs: res = res and cb.begin_validate()\n        return res\n    def after_epoch(self):\n        res = True\n        for cb in self.cbs: res = res and cb.after_epoch()\n        return res\n    \n    def begin_batch(self, xb, yb):\n        res = True\n        for cb in self.cbs: res = res and cb.begin_batch(xb, yb)\n        return res\n\n    def after_loss(self, loss):\n        res = self.in_train\n        for cb in self.cbs: res = res and cb.after_loss(loss)\n        return res\n\n    def after_backward(self):\n        res = True\n        for cb in self.cbs: res = res and cb.after_backward()\n        return res\n\n    def after_step(self):\n        res = True\n        for cb in self.cbs: res = res and cb.after_step()\n        return res\n    \n    def do_stop(self):\n        try:     return self.learn.stop\n        finally: self.learn.stop = False","120120ad":"#this is just an exsemple of a callback \nclass TestCallback(Callback):\n    def begin_fit(self,learn): #it will use the fit callback\n        super().begin_fit(learn)\n        self.n_iters = 0 #set the iterations to 0 \n        return True\n        \n    def after_step(self): #and then after every step \n        self.n_iters += 1 #it will set number of iterations to +=1 \n        print(self.n_iters) #and print it out \n        if self.n_iters>=10: self.learn.stop = True #and if we get pass 10 iterations we tell the learner to stop\n            #here we use the callbacj learn.stop defined in the CallBackHandler \n        return True","d351db36":"#so lets test it \nfit(1, learn, cb=CallbackHandler([TestCallback()]))","67b14b4b":"#export\nimport re\n\n_camel_re1 = re.compile('(.)([A-Z][a-z]+)')\n_camel_re2 = re.compile('([a-z0-9])([A-Z])')\ndef camel2snake(name):\n    s1 = re.sub(_camel_re1, r'\\1_\\2', name)\n    return re.sub(_camel_re2, r'\\1_\\2', s1).lower()\n\nclass Callback():\n    _order=0 #this dictat what order your callbacks run in, like \n    def set_runner(self, run): self.run=run\n    def __getattr__(self, k): return getattr(self.run, k) #this does that you can use self. in the callbacks defined in AvgStatsCallback. So this is done \n    #because we a object that contains another object so in the runner object is there a object for callback \n    @property\n    def name(self): #name function takes the name on fx a class and make it lowerchase and put a '_' as a seperation on the ords\n        #see eksampel below with function name(camel2snake) and then the Callbakc are removed \n        name = re.sub(r'Callback$', '', self.__class__.__name__)\n        return camel2snake(name or 'callback')","ccac3a81":"#export\nclass TrainEvalCallback(Callback):\n    def begin_fit(self):\n        self.run.n_epochs=0.\n        self.run.n_iter=0\n    \n    def after_batch(self):\n        if not self.in_train: return\n        self.run.n_epochs += 1.\/self.iters #keep track of how many epochs it has done \n        self.run.n_iter   += 1 #and how many iterations it has done \n        \n    def begin_epoch(self):\n        self.run.n_epochs=self.epoch\n        self.model.train() #so we call model.train\n        self.run.in_train=True\n\n    def begin_validate(self):\n        self.model.eval() #so we call model.eval \n        self.run.in_train=False","b1162ee4":"class TestCallback(Callback):\n    #_order=1 #so this will first run after _order=0\n    def after_step(self):\n        if self.n_iters>=10: return True # so the 'n_iters' which will stop it when we get to 10 iterations wil have to stop. Here\n        #the class above (TrainEvalCallback) use the 'n_iters' in the 'after_batch' function   #note True is returned so it stops","62bcb6dc":"cbname = 'TrainEvalCallback'\ncamel2snake(cbname)","be3a7918":"TrainEvalCallback().name","428d90f6":"\n#export\nfrom typing import *\n\ndef listify(o):\n    if o is None: return []\n    if isinstance(o, list): return o\n    if isinstance(o, str): return [o]\n    if isinstance(o, Iterable): return list(o)\n    return [o]","ae58ae91":"#export\nclass Runner():\n    def __init__(self, cbs=None, cb_funcs=None): #cb_funcs=None any callback functions you pass in, that is not being defined here (new callbacks)\n        cbs = listify(cbs)\n        for cbf in listify(cb_funcs):\n            cb = cbf()\n            setattr(self, cb.name, cb) #assign the new callbacks to a attribute with that name. So it will use the Callback() class above and then use the \n            #function called 'name' and use this. \n            cbs.append(cb)\n        self.stop,self.cbs = False,[TrainEvalCallback()]+cbs\n\n    @property\n    def opt(self):       return self.learn.opt\n    @property\n    def model(self):     return self.learn.model\n    @property\n    def loss_func(self): return self.learn.loss_func\n    @property\n    def data(self):      return self.learn.data\n\n    def one_batch(self, xb, yb):\n        self.xb,self.yb = xb,yb\n        if self('begin_batch'): return\n        self.pred = self.model(self.xb)\n        if self('after_pred'): return\n        self.loss = self.loss_func(self.pred, self.yb)\n        if self('after_loss') or not self.in_train: return\n        self.loss.backward()\n        if self('after_backward'): return\n        self.opt.step()\n        if self('after_step'): return\n        self.opt.zero_grad()\n\n    def all_batches(self, dl):\n        self.iters = len(dl)\n        for xb,yb in dl:\n            if self.stop: break\n            self.one_batch(xb, yb)\n            self('after_batch')\n        self.stop=False\n\n\n#note we do not use model.train or model.eval, so the code rough dont do anything so it just says, 'thise are the steps i need to run'\n#and the callbacks do the running. So this is being done in TrainEvalCallback class above \n\n    def fit(self, epochs, learn):\n        self.epochs,self.learn = epochs,learn #keep track of the numbers of epochs we are doing and kepping track of the learner \n        #and note that the learner has not logic in it \n\n        try:\n            for cb in self.cbs: cb.set_runner(self) #now we tell each callback what runner they are working with\n            if self('begin_fit'): return #call begin fit \n            for epoch in range(epochs): #go through each epoch \n                self.epoch = epoch #set the epoch \n                if not self('begin_epoch'): self.all_batches(self.data.train_dl) #and call all batches #if not means here to keep going because it return False\n\n                with torch.no_grad():  #and for no_grad we use begin_validate\n                    if not self('begin_validate'): self.all_batches(self.data.valid_dl) #call all batches  #if not means here to keep going because it return False\n                        #it will first stop when and only when True is returned \n                if self('after_epoch'): break #last thing we call after_epoch\n            \n        finally:\n            self('after_fit')# and after fit \n            self.learn = None\n\n    def __call__(self, cb_name): #to get rid of repetations from CallBackHandler we use __Call__ which note let us tried a object as if it was a function\n        for cb in sorted(self.cbs, key=lambda x: x._order): #after (self('after_epoch')) we are going through all out callbacks\n        #(self.cbs)\n            f = getattr(cb, cb_name, None) #getattr (get attribute) means look inside this object (cb)\n            #and try to find something of the this name (cb_name) and if you cant find it defound it to None \n            if f and f(): return True #and if it can find cb_name like 'begin_validate' then you can call it  #note True is returned so it stops\n        return False","d1b9d653":"#export\nclass AvgStats():\n    def __init__(self, metrics, in_train): self.metrics,self.in_train = listify(metrics),in_train\n    \n    def reset(self):\n        self.tot_loss,self.count = 0.,0\n        self.tot_mets = [0.] * len(self.metrics)\n        \n    @property\n    def all_stats(self): return [self.tot_loss.item()] + self.tot_mets #and los and metrixces to give all statistic\n    @property\n    def avg_stats(self): return [o\/self.count for o in self.all_stats] #making a property that goes through losses and matrixces and give us the avgerge (\"gennemsnitlige\")--'o\/self.count'\n    \n    def __repr__(self): #dona reprer so it prints it out \n        if not self.count: return \"\"\n        return f\"{'train' if self.in_train else 'valid'}: {self.avg_stats}\"\n\n    def accumulate(self, run):\n        bn = run.xb.shape[0]\n        self.tot_loss += run.loss * bn #gonna add up the total loss times the batch size \n        self.count += bn #count += the size of the batch \n        for i,m in enumerate(self.metrics): # and for each matrix...\n            self.tot_mets[i] += m(run.pred, run.yb) * bn #... its gonna add up the total matrixces times the batch size \n\nclass AvgStatsCallback(Callback):\n    def __init__(self, metrics):\n        self.train_stats,self.valid_stats = AvgStats(metrics,True),AvgStats(metrics,False) #we aare gonna use some object to keep track of matricses ans losses\n        #one for training and one for valid. \n        \n    def begin_epoch(self): #at the start of an epoch we will reset the statistics \n        self.train_stats.reset()\n        self.valid_stats.reset()\n        \n    def after_loss(self): #and after we got the loss calulated ...\n        stats = self.train_stats if self.in_train else self.valid_stats\n        with torch.no_grad(): stats.accumulate(self.run) #... we will accumulate the statisstics so we need a class that does the accumulate which is defined above\n    \n    def after_epoch(self): #at the end of an epoch we will print out the statistics\n        print(self.train_stats)\n        print(self.valid_stats)","08ab4efb":"learn = Learner(*get_model(data), loss_func, data) #create our learner ","5186a17d":"stats = AvgStatsCallback([accuracy]) #add our argstatscallback \nrun = Runner(cbs=stats)","a26bd375":"run.fit(2, learn)","d8826408":"loss,acc = stats.valid_stats.avg_stats\nassert acc>0.9\nloss,acc","1ec507fd":"#export\nfrom functools import partial","54dac204":"acc_cbf = partial(AvgStatsCallback,accuracy) #partial is a function that returns a function so we go in the AvgStatsCallback class and get the functions and pass it to the veriable","26c9dca7":"run = Runner(cb_funcs=acc_cbf) #now we can set it in our runner in the cb_funcs (which are for new callbacks)","0d2321b6":"run.fit(1, learn)","d497b677":"run.avg_stats.valid_stats.avg_stats","c5b1c698":"x_train,y_train,x_valid,y_valid = get_data()\ntrain_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)\nnh,bs = 50,512\nc = y_train.max().item()+1\nloss_func = F.cross_entropy","48f6b500":"data = DataBunch(*get_dls(train_ds, valid_ds, bs), c)","b447222e":"def create_learner(model_func, loss_func, data):\n    return Learner(*model_func(data), loss_func, data)","8f7f5728":"learn = create_learner(get_model, loss_func, data) #defoult a learing rate at 0.5 see code above in _04\nrun = Runner([AvgStatsCallback([accuracy])])\n\nrun.fit(3, learn)","c1809e1d":"learn = create_learner(partial(get_model, lr=0.3), loss_func, data) #try a learning rate at 0.3 insted \nrun = Runner([AvgStatsCallback([accuracy])])\n\nrun.fit(3, learn)","3aed11aa":"#export\ndef get_model_func(lr=0.5): return partial(get_model, lr=lr) #create a function so it is easayer to change the learning rate ","056d2512":"#export\nclass Recorder(Callback): #Recorder records the learning rate given in the hyper parameters given in the below code \n    def begin_fit(self): self.lrs,self.losses = [],[] #set the learning rates and losses to emty at the beginning \n\n    def after_batch(self): #so after each batch\n        if not self.in_train: return #as long as we are training (will return False if training and True if note training there we use 'if not self.in_train:'')\n        self.lrs.append(self.opt.param_groups[-1]['lr']) #it will append the current learning rate #since there are many learning rates we just take the last one \n        #therefor we use '-1' in param_groups and ['lr'] to get the learning rate \n        self.losses.append(self.loss.detach().cpu())#and append the current loss \n\n    def plot_lr  (self): plt.plot(self.lrs) #then we tell it to plot the learning rates \n    def plot_loss(self): plt.plot(self.losses) # and plot the losses \n\n\n#it is a good idea to schadule all the things we want to apply on the model, like dropouts, momentun and learning ratess, since we want to change them as the model improves\nclass ParamScheduler(Callback):\n    _order=1\n    def __init__(self, pname, sched_func): self.pname,self.sched_func = pname,sched_func #pass in a sched_func and a parameter(pname) to shedule\n\n    def set_param(self):\n        for pg in self.opt.param_groups: #param_groups same as layer groups in fastai, so it group different layers together so they can get a different learning rate \n            pg[self.pname] = self.sched_func(self.n_epochs\/self.epochs) #the sched_func takes a singel argument which number of epochs divided by total epochs#note this will be a float\n            #and the result of this will set the hyper parameter --> pg[self.pname] in this chase learing rate \n            \n    def begin_batch(self): \n        if self.in_train: self.set_param() #so if we are trianing (in_train) we will run our scheduler and set parameters (set_param) for learning rate in differetn layer groups ","267b529d":"#for linear schedule\ndef sched_lin(start, end): #starting learning rate to end learning like from 1e-01 to 1e-10\n    def _inner(start, end, pos): return start + pos*(end-start) #now return the learning rate so the start + position time the different in the end and start\n    return partial(_inner, start, end) #so now to only get posistion(pos) we return partial and pass the function _inner, start and end. since we dont pass 'pos'\n#it just takes position","d08aa453":"#a decorator is a function that returns a function \ndef annealer(f):\n    def _inner(start, end): return partial(f, start, end)\n    return _inner\n\n@annealer #the annealer decorator does the same as the partial \ndef sched_lin(start, end, pos): return start + pos*(end-start)","5fd57efa":"# shift-tab works too, in Jupyter!\n# sched_lin()","3bfed493":"\nf = sched_lin(1,2) #linear schedular and pass the learining as 1 to 2\nf(0.3) #and ask what should the learning rate be 30 % through training  #note pos=0.3","541dea79":"\n#export\n@annealer\ndef sched_cos(start, end, pos): return start + (1 + math.cos(math.pi*(1-pos))) * (end-start) \/ 2\n@annealer\ndef sched_no(start, end, pos):  return start #no schedular return always start\n@annealer\ndef sched_exp(start, end, pos): return start * (end\/start) ** pos\n\ndef cos_1cycle_anneal(start, high, end):\n    return [sched_cos(start, high), sched_cos(high, end)]\n\n#This monkey-patch is there to be able to plot tensors\ntorch.Tensor.ndim = property(lambda x: len(x.shape))","43d6a8e9":"\nannealings = \"NO LINEAR COS EXP\".split()\n\na = torch.arange(0, 100)\np = torch.linspace(0.01,1,100)\n\nfns = [sched_no, sched_lin, sched_cos, sched_exp]\nfor fn, t in zip(fns, annealings):\n    f = fn(2, 1e-2)\n    plt.plot(a, [f(o) for o in p], label=t)\nplt.legend();","864a1b4a":"#dont think to much about the below code \ndef combine_scheds(pcts, scheds):\n    assert sum(pcts) == 1.\n    pcts = tensor([0] + listify(pcts))\n    assert torch.all(pcts >= 0)\n    pcts = torch.cumsum(pcts, 0)\n    def _inner(pos):\n        idx = (pos >= pcts).nonzero().max()\n        actual_pos = (pos-pcts[idx]) \/ (pcts[idx+1]-pcts[idx])\n        return scheds[idx](actual_pos)\n    return _inner","2f6cde55":"sched = combine_scheds([0.3, 0.7], [sched_cos(0.3, 0.6), sched_cos(0.6, 0.2)]) #for the combined schedular we are gonna pass in the fases we want \n#so fase one wil be a learning rate from 0.3 to 0.6 and take up 30 procent of our bacthes and fase to will take 70 of our bathes and be a learning from 0.6 to 0.2","319599a7":"plt.plot(a, [sched(o) for o in p])","6e4bee5d":"cbfs = [Recorder,\n        partial(AvgStatsCallback,accuracy),\n        partial(ParamScheduler, 'lr', sched)] #cbfs= callbacks functions and sched from above","ac95f358":"learn = create_learner(get_model_func(0.3), loss_func, data)\nrun = Runner(cb_funcs=cbfs)","d5ddb5cd":"run.fit(3, learn)","243e122f":"run.recorder.plot_lr()","03e76d04":"run.recorder.plot_loss()","7b403cee":"\n%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline","78f6aa51":"x_train,y_train,x_valid,y_valid = get_data()\ntrain_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)\nnh,bs = 50,512\nc = y_train.max().item()+1\nloss_func = F.cross_entropy","098aa1ff":"data = DataBunch(*get_dls(train_ds, valid_ds, bs), c)","8ab8d09d":"class Callback():\n    _order=0\n    def set_runner(self, run): self.run=run\n    def __getattr__(self, k): return getattr(self.run, k)\n    \n    @property\n    def name(self):\n        name = re.sub(r'Callback$', '', self.__class__.__name__)\n        return camel2snake(name or 'callback')\n    \n    def __call__(self, cb_name):\n        f = getattr(self, cb_name, None)\n        if f and f(): return True\n        return False\n\nclass TrainEvalCallback(Callback):\n    def begin_fit(self):\n        self.run.n_epochs=0.\n        self.run.n_iter=0\n    \n    def after_batch(self):\n        if not self.in_train: return\n        self.run.n_epochs += 1.\/self.iters\n        self.run.n_iter   += 1\n        \n    def begin_epoch(self):\n        self.run.n_epochs=self.epoch\n        self.model.train()\n        self.run.in_train=True\n\n    def begin_validate(self):\n        self.model.eval()\n        self.run.in_train=False\n\nclass CancelTrainException(Exception): pass #just a class (CancelTrainException) that inhaied from Exception class \n#pass means that it will not get more \"opf\u00f8relser\" then the parant class\nclass CancelEpochException(Exception): pass\nclass CancelBatchException(Exception): pass","e2cd1c27":"#export\nclass Runner():\n    def __init__(self, cbs=None, cb_funcs=None):\n        cbs = listify(cbs)\n        for cbf in listify(cb_funcs):\n            cb = cbf()\n            setattr(self, cb.name, cb)\n            cbs.append(cb)\n        self.stop,self.cbs = False,[TrainEvalCallback()]+cbs\n\n    @property\n    def opt(self):       return self.learn.opt\n    @property\n    def model(self):     return self.learn.model\n    @property\n    def loss_func(self): return self.learn.loss_func\n    @property\n    def data(self):      return self.learn.data\n\n    def one_batch(self, xb, yb):\n        try:\n            self.xb,self.yb = xb,yb\n            self('begin_batch')\n            self.pred = self.model(self.xb)\n            self('after_pred')\n            self.loss = self.loss_func(self.pred, self.yb)\n            self('after_loss')\n            if not self.in_train: return\n            self.loss.backward()\n            self('after_backward')\n            self.opt.step()\n            self('after_step')\n            self.opt.zero_grad()\n        except CancelBatchException: self('after_cancel_batch') #except -- so if we call on CancelBatchException this will run. \n            #the name in self doesnt matter\n        finally: self('after_batch')\n\n    def all_batches(self, dl):\n        self.iters = len(dl)\n        try:\n            for xb,yb in dl: self.one_batch(xb, yb)\n        except CancelEpochException: self('after_cancel_epoch') #except -- so if we call on CancelBatchException this will run. the name\n            #in self doesnt matter\n\n    def fit(self, epochs, learn):\n        self.epochs,self.learn,self.loss = epochs,learn,tensor(0.)\n\n        try:\n            for cb in self.cbs: cb.set_runner(self)\n            self('begin_fit')\n            for epoch in range(epochs):\n                self.epoch = epoch\n                if not self('begin_epoch'): self.all_batches(self.data.train_dl)\n\n                with torch.no_grad(): \n                    if not self('begin_validate'): self.all_batches(self.data.valid_dl)\n                self('after_epoch')\n            \n        except CancelTrainException: self('after_cancel_train') #except -- so if we call on CancelBatchException this will run. the name\n            #in self doesnt matter\n        finally:\n            self('after_fit')\n            self.learn = None\n\n    def __call__(self, cb_name):\n        res = False\n        for cb in sorted(self.cbs, key=lambda x: x._order): res = cb(cb_name) or res\n        return res","94de4d0c":"learn = create_learner(get_model, loss_func, data)","b34d3653":"class TestCallback(Callback):\n    _order=1\n    def after_step(self):\n        print(self.n_iter)\n        if self.n_iter>=10: raise CancelTrainException() # raise CancelTrainException() so we go back and activiate it in 'Runner' class","a9e11b35":"run = Runner(cb_funcs=TestCallback)","c4686a73":"run.fit(3, learn)","f139ebf6":"#export\nclass AvgStatsCallback(Callback):\n    def __init__(self, metrics):\n        self.train_stats,self.valid_stats = AvgStats(metrics,True),AvgStats(metrics,False)\n        \n    def begin_epoch(self):\n        self.train_stats.reset()\n        self.valid_stats.reset()\n        \n    def after_loss(self):\n        stats = self.train_stats if self.in_train else self.valid_stats\n        with torch.no_grad(): stats.accumulate(self.run)\n    \n    def after_epoch(self):\n        print(self.train_stats)\n        print(self.valid_stats)\n        \nclass Recorder(Callback):\n    def begin_fit(self):\n        self.lrs = [[] for _ in self.opt.param_groups]\n        self.losses = []\n\n    def after_batch(self):\n        if not self.in_train: return\n        for pg,lr in zip(self.opt.param_groups,self.lrs): lr.append(pg['lr'])\n        self.losses.append(self.loss.detach().cpu())        \n\n    def plot_lr  (self, pgid=-1): plt.plot(self.lrs[pgid])\n    def plot_loss(self, skip_last=0): plt.plot(self.losses[:len(self.losses)-skip_last])\n        \n    def plot(self, skip_last=0, pgid=-1):\n        losses = [o.item() for o in self.losses]\n        lrs    = self.lrs[pgid]\n        n = len(losses)-skip_last\n        plt.xscale('log')\n        plt.plot(lrs[:n], losses[:n])\n\nclass ParamScheduler(Callback):\n    _order=1\n    def __init__(self, pname, sched_funcs): self.pname,self.sched_funcs = pname,sched_funcs\n        \n    def begin_fit(self):\n        if not isinstance(self.sched_funcs, (list,tuple)):\n            self.sched_funcs = [self.sched_funcs] * len(self.opt.param_groups)\n\n    def set_param(self):\n        assert len(self.opt.param_groups)==len(self.sched_funcs)\n        for pg,f in zip(self.opt.param_groups,self.sched_funcs):\n            pg[self.pname] = f(self.n_epochs\/self.epochs)\n            \n    def begin_batch(self): \n        if self.in_train: self.set_param()","01cdbe9b":"class LR_Find(Callback):\n    _order=1\n    def __init__(self, max_iter=100, min_lr=1e-6, max_lr=10):\n        self.max_iter,self.min_lr,self.max_lr = max_iter,min_lr,max_lr\n        self.best_loss = 1e9\n        \n    def begin_batch(self): \n        if not self.in_train: return\n        #using an exponential curve to set the learning rate \n        pos = self.n_iter\/self.max_iter # current iteration divided by the maxisimum iteration\n        lr = self.min_lr * (self.max_lr\/self.min_lr) ** pos #find learning rate\n        for pg in self.opt.param_groups: pg['lr'] = lr #set the leaerning rate in parameters same as we did in run scedular above \n            \n    def after_step(self): #after each step\n        if self.n_iter>=self.max_iter or self.loss>self.best_loss*10: #check if we have done more then maximum iterations(100) and\n            #if the loss is much worse then the best we have had this far \n            raise CancelTrainException() #if either of those happens raise CancelTrainException and stop training \n        if self.loss < self.best_loss: self.best_loss = self.loss #check if the current loss is better then the best loss and\n            #if it is set the current loss to the best loss","79db3f12":"learn = create_learner(get_model, loss_func, data)","8250e736":"run = Runner(cb_funcs=[LR_Find, Recorder])","5b5aa375":"\nrun.fit(2, learn)","615a9673":"run.recorder.plot(skip_last=5)","b4fca9ec":"run.recorder.plot_lr()","410d32d7":"%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline","b9b98626":"torch.set_num_threads(2)","2dee5a1a":"\nx_train,y_train,x_valid,y_valid = get_data()","5aa4d16c":"def normalize_to(train, valid):\n    m,s = train.mean(),train.std()\n    return normalize(train, m, s), normalize(valid, m, s)","9408cdc1":"\nx_train,x_valid = normalize_to(x_train,x_valid)\ntrain_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)","8a231271":"x_train.mean(),x_train.std()","05672a5b":"\nnh,bs = 50,512\nc = y_train.max().item()+1\nloss_func = F.cross_entropy\n\ndata = DataBunch(*get_dls(train_ds, valid_ds, bs), c)","93582ad0":"#export\nclass Lambda(nn.Module):\n    def __init__(self, func):\n        super().__init__()\n        self.func = func\n\n    def forward(self, x): return self.func(x) #the only thing it does is\n\ndef flatten(x):      return x.view(x.shape[0], -1)","2023f609":"\ndef mnist_resize(x): return x.view(-1, 1, 28, 28)","4f8b7261":"def get_cnn_model(data):\n    return nn.Sequential(\n        Lambda(mnist_resize),\n        nn.Conv2d( 1, 8, 5, padding=2,stride=2), nn.ReLU(), #14\n        nn.Conv2d( 8,16, 3, padding=1,stride=2), nn.ReLU(), # 7\n        nn.Conv2d(16,32, 3, padding=1,stride=2), nn.ReLU(), # 4\n        nn.Conv2d(32,32, 3, padding=1,stride=2), nn.ReLU(), # 2\n        nn.AdaptiveAvgPool2d(1), #averge pooling\n        Lambda(flatten), #flatting \n        nn.Linear(32,data.c)\n    )","2fc33be5":"model = get_cnn_model(data)","4f058637":"cbfs = [Recorder, partial(AvgStatsCallback,accuracy)]","299c5ab4":"\nopt = optim.SGD(model.parameters(), lr=0.4)\nlearn = Learner(model, opt, loss_func, data)\nrun = Runner(cb_funcs=cbfs)","531a8c2d":"%time run.fit(1, learn)","d765eef4":"# Somewhat more flexible way\ndevice = torch.device('cuda',0) #we create a device be calling torch.device '0' is the GPU number u wonna use if you only have one GPU it is device 0","187184c2":"\nclass CudaCallback(Callback):\n    def __init__(self,device): self.device=device #initiate a model\n    def begin_fit(self): self.model.to(self.device) #move the model to the device '.to' is from pytorch and it means that it moves some parameters to a device \n    def begin_batch(self): self.run.xb,self.run.yb = self.xb.to(self.device),self.yb.to(self.device) #remember that 'run' is a class longer up in the code and xb and yb are defined also but ere we just move them to the device","283e49b8":"#another way of doing this is use pytorchs version but here we use only one device \n# Somewhat less flexible, but quite convenient\ntorch.cuda.set_device(device)","8491f981":"#export\nclass CudaCallback(Callback):\n    def begin_fit(self): self.model.cuda() #note we dont have to move it to a device we just use .cuda\n    def begin_batch(self): self.run.xb,self.run.yb = self.xb.cuda(),self.yb.cuda()","64d20680":"#add it to our callback functions \ncbfs.append(CudaCallback)","e60cb5a5":"model = get_cnn_model(data) #grap our model","efbe28bd":"\nopt = optim.SGD(model.parameters(), lr=0.4)\nlearn = Learner(model, opt, loss_func, data)\nrun = Runner(cb_funcs=cbfs)","d2a78929":"\n%time run.fit(3, learn)","f754e624":"#since we use relu and conv2 alot we just make a function for it \ndef conv2d(ni, nf, ks=3, stride=2): #kernel size 3(ks) note this ans stride on 2 are defoults \n    return nn.Sequential(\n        nn.Conv2d(ni, nf, ks, padding=ks\/\/2, stride=stride), nn.ReLU())","7c15be6a":"#previus when we wrote the transforming of mnist it will only work with it but here we will make a more generel transformation of the data \nclass BatchTransformXCallback(Callback): #transform the indepened verible (X-training data) for a batch \n    _order=2\n    def __init__(self, tfm): self.tfm = tfm #you pass it som etransformation function (tfm) which its stores away \n    def begin_batch(self): self.run.xb = self.tfm(self.xb) #begin batch just replaces the current batch (xb) with the result of the transformation (tfm)\n\ndef view_tfm(*size): #view_tfm takes and transform the size\n    def _inner(x): return x.view(*((-1,)+size))\n    return _inner","53659068":"\nmnist_view = view_tfm(1,28,28) #just to view something at 1 by 28,28\ncbfs.append(partial(BatchTransformXCallback, mnist_view)) #now we can simply append anotheer callback  ","50af33ea":"\nnfs = [8,16,32,32] #hte numbers of filters per layer ","dd426d0e":"#now we make a model again \ndef get_cnn_layers(data, nfs):\n    nfs = [1] + nfs\n    return [\n        conv2d(nfs[i], nfs[i+1], 5 if i==0 else 3) #the first few layers are for everyone of the filters. its a Conv2d from the filter nfs[i] (current filter) to the next filter nfs[i+1]\n        #and then the kernel size depends it 5 for the fist layer otherwise it is 3 (this is explaned in the picture below)\n        for i in range(len(nfs)-1)\n    ] + [nn.AdaptiveAvgPool2d(1), Lambda(flatten), nn.Linear(nfs[-1], data.c)] #last few layers are averge pulling(AdaptiveAvgPool2d), flattening and a linear layer \n\ndef get_cnn_model(data, nfs): return nn.Sequential(*get_cnn_layers(data, nfs)) #we can now remove the mnist_view from the previus model since it is a callback.\n#this is now a generaric 'get cnn model' function that return s sequential model with some arbitory set of layers and some arbitory layers (nfs)","c5f862f7":"#lets put the model and the layers intot he function below \ndef get_runner(model, data, lr=0.6, cbs=None, opt_func=None, loss_func = F.cross_entropy): \n    if opt_func is None: opt_func = optim.SGD #grap optmisation function\n    opt = opt_func(model.parameters(), lr=lr) #grap the optimazer\n    learn = Learner(model, opt, loss_func, data) #grap our learner \n    return learn, Runner(cb_funcs=listify(cbs)) ","903265bc":"model = get_cnn_model(data, nfs)\nlearn,run = get_runner(model, data, lr=0.4, cbs=cbfs) #note we see both our filters -- 8,16,32,32 and out kernel size where the first sis 5 and the for the rest it is 3","700d48cd":"model #veiw model","d6b222c3":"run.fit(3, learn) #train model ","f345d18c":"#new model class\nclass SequentialModel(nn.Module):\n    def __init__(self, *layers):\n        super().__init__()\n        self.layers = nn.ModuleList(layers)\n        self.act_means = [[] for _ in layers] #make a list to use for act_means ([]for _ in layers) - means a list for every layer\n        self.act_stds  = [[] for _ in layers] #make a list to use for act_stds ([]for _ in layers) - means a list for every layer\n        \n    def __call__(self, x): #we have built a SequentialModel before so we will reuse the first to lines \n        for i,l in enumerate(self.layers): # go through each layer \n            x = l(x) #set x to the current layer of x \n            self.act_means[i].append(x.data.mean()) #graps the mean of the output and put it in act_means\n            self.act_stds [i].append(x.data.std ()) #grap the std of the output and put it in act_stds\n        return x\n    \n    def __iter__(self): return iter(self.layers)","e14277ca":"model =  SequentialModel(*get_cnn_layers(data, nfs)) #makae model\nlearn,run = get_runner(model, data, lr=0.9, cbs=cbfs) #learning model","c93b9fd6":"run.fit(2, learn) #fit model","72fa350b":"for l in model.act_means: plt.plot(l)\nplt.legend(range(6)); #so the means for every layers hit a pich in the beginning. so the mean get eksponential bigger to they suddenly colaps \n# other that it get slidtly better but not much. sothe consurn is that there are a lot of parameters and we dont know if all of the layers \n#get better or just some of them. since most of them culd have a zero gradient that this point. but it is clear that the first pich leaves\n#out model in a very sad place and we clealy see the result of it ","0c205bb1":"for l in model.act_stds: plt.plot(l)\nplt.legend(range(6)); #just look realy bad","0519cbfc":"for l in model.act_means: plt.plot(l[:10]) #see the first 10 means \nplt.legend(range(6)); #here we see the means getting close or are realy close the 0 which is what we want  ","89db1a55":"for l in model.act_stds: plt.plot(l[:10]) #see the std for the fiirst 10 batches \nplt.legend(range(6)); # we see a problem. the fist layer have a std not to far away from 1 but as we whould espect it gets eksponential worse\n#and the last layer in this group is almost 0 which we dont want. so this means that our final layer gets alomost no activations\n#and basic no gradiants. this happens since the gradient was so fast that it just falling of a clift'klippe' and having to star agian \n#this we will ty to fix ","234134bd":"#you can see hooks as the same as hooks \nmodel = get_cnn_model(data, nfs)\nlearn,run = get_runner(model, data, lr=0.5, cbs=cbfs)","62905211":"\nact_means = [[] for _ in model] #create global verible for list for every layer\nact_stds  = [[] for _ in model] #create global verible for list for every layer","98cb5834":"def append_stats(i, mod, inp, outp): #create a function to use for register_forward_hook and if we look at the docs for it\n    #it takes 3 things: the modeule that do the callback (mod), the input to the module (inp) and the output to the module (outp)\n    #note 'i' is the layer number we are using \n    act_means[i].append(outp.data.mean()) #to caluelate the mean of the last layer. note we use the 'outp' since it is the last layer we wnat for each layer \n    \n    act_stds [i].append(outp.data.std()) #to calulate the std for the last layer ","0f1148a6":"\nfor i,m in enumerate(model): m.register_forward_hook(partial(append_stats, i)) #or any module (m) register forward hook (register_forward_hook)\n    # and pass in a function (partial(append_stats, i) which is a call back which will be called when the m.regist.... has been calulated \n    #we can also use register_backward_hook which can be used after the backward are calulated \n    #note we use 'i' to nkow which layer we are at.","a7782af4":"run.fit(1, learn) #now we can call fit","6acb6274":"for o in act_means: plt.plot(o)\nplt.legend(range(5)); #and we will see the same gra as before where the layers means build up and then collapses","14bc3b38":"#export\ndef children(m): return list(m.children())\n\nclass Hook():\n    def __init__(self, m, f): self.hook = m.register_forward_hook(partial(f, self)) #when we initize it register a forward hook and some \n        #function called 'f' we also use self. which means we can acces it in append_stats -- note self.hook\n    def remove(self): self.hook.remove() #when we are done using a hook module we should call hook.remove since otherwise if we keep registering more hooks on the same module \n        #they are all gonna get called and eventually you are gonna run out of memory \n    def __del__(self): self.remove() #note we do this pythosch will outomatically call __del__ for delate when it is done with a module which in return will remove the hook \n\ndef append_stats(hook, mod, inp, outp): #--note hook \n    if not hasattr(hook,'stats'): hook.stats = ([],[]) #so in our hook we can pop in our to list to store away our mean and std \n    means,stds = hook.stats #name the to list one for means and one for stds\n    means.append(outp.data.mean()) #and now we can just append the means for each layer\n    stds .append(outp.data.std()) #and now we can just append the stds for each layer","6487bd1a":"model = get_cnn_model(data, nfs)\nlearn,run = get_runner(model, data, lr=0.5, cbs=cbfs)","445ba819":"hooks = [Hook(l, append_stats) for l in children(model[:4])] #so now we can just go for hook layer ind children of our model and we are just gonna grap the first 4 layers since it its the conv2d layers that are the most interresting and not hte linear layers ","a27e7f61":"\nrun.fit(1, learn)","e7193453":"#and we see it does the same thing as all the other grafs \nfor h in hooks:\n    plt.plot(h.stats[0])\n    h.remove()\nplt.legend(range(4));","58dd2afc":"#export just a list that contains a lot of usefull features not inparticular needed\nclass ListContainer():\n    def __init__(self, items): self.items = listify(items)\n    def __getitem__(self, idx): #being clled when we are useing 'firkant parantesterne' for correct operation usage \n        if isinstance(idx, (int,slice)): return self.items[idx] #bla bla bla ...\n        if isinstance(idx[0],bool): #bla bla bla...\n            assert len(idx)==len(self) # bool mask\n            return [o for m,o in zip(idx,self.items) if m]\n        return [self.items[i] for i in idx]\n    def __len__(self): return len(self.items) #return lenght \n    def __iter__(self): return iter(self.items) #iteration\n    def __setitem__(self, i, o): self.items[i] = o \n    def __delitem__(self, i): del(self.items[i])\n    def __repr__(self): #printing\n        res = f'{self.__class__.__name__} ({len(self)} items)\\n{self.items[:10]}'\n        if len(self)>10: res = res[:-1]+ '...]'\n        return res","0dbbd82c":"\nListContainer(range(10))","25ca9461":"\nListContainer(range(100)) #note ater a 100 it is no longer printing them all  just 9...","bbfec565":"t = ListContainer(range(10))\nt[[1,2]], t[[False]*8 + [True,False]]","d254ba24":"#now we can just use the ListContainer here to get teh features we want and this means also we now have hooks in it \n#export\nfrom torch.nn import init\n\nclass Hooks(ListContainer):\n    def __init__(self, ms, f): super().__init__([Hook(m, f) for m in ms])  #so now we can just go for hooks in each layer(m) in children of our model(ms).\n    def __enter__(self, *args): return self #used in below code but its what happens when you starts the 'with block' (defined longer done)\n    def __exit__ (self, *args): self.remove() #used in below code when you finish the 'with block' but when called on it will use .remove function defined here.\n    def __del__(self): self.remove() #so after each module it detaltes and call def remove\n\n    def __delitem__(self, i):\n        self[i].remove()\n        super().__delitem__(i)\n        \n    def remove(self):\n        for h in self: h.remove()  #goes through each layer and removes each registed hook. \n            #note we can say for h in self even thogh h is not in a iteration list in this cell block. but note we also have  ListContainer of which we use\n            #and it does it.","9b15a9d4":"model = get_cnn_model(data, nfs).cuda()\nlearn,run = get_runner(model, data, lr=0.9, cbs=cbfs)","49f7698f":"\nhooks = Hooks(model, append_stats) #parameters for hooks soeverything in out model with the append_stats\nhooks #print it out to see all the hooks ","427d49dc":"\nhooks.remove()","606015b7":"x,y = next(iter(data.train_dl)) #grap a batch of data \nx = mnist_resize(x).cuda()","14dc36b5":"\nx.mean(),x.std() #chech the one batchs of datas mean and std and it is about mean=0 and std about 1 whcich is what we want.","55f6eff3":"p = model[0](x) #pass it through our first layer f our model . note model[0] mean the first means the first layer \np.mean(),p.std() #note our mean is 0 and the std isnt 1 ","37e542c4":"#so now we just go ahead and inisiate it with keiming \nfor l in model:\n    if isinstance(l, nn.Sequential):\n        init.kaiming_normal_(l[0].weight)\n        l[0].bias.data.zero_()","e0196aac":"\np = model[0](x)\np.mean(),p.std() #and other that our mean is very close to 0.5 (becouse of the Relu) and std is very close to 1 ","3bdb862f":"with Hooks(model, append_stats) as hooks: #do our hooks #note that we use a with block which means make this object (Hooks(model, append_stats)\n    #and give it this name (hooks) and when it is finishs it will call something which in our case is __exit__ from Hooks class above \n    run.fit(2, learn) #do a fit with 2 epochs \n    fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4))\n    for h in hooks:\n        ms,ss = h.stats\n        ax0.plot(ms[:10]) #do our first 10 mean ...\n        ax1.plot(ss[:10])#... and std (layers)\n    plt.legend(range(6));\n    \n    fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4))\n    for h in hooks:\n        ms,ss = h.stats\n        ax0.plot(ms) #plot means \n        ax1.plot(ss) #plot stds\n    plt.legend(range(6));","12c3d88f":"#so what we can do is to modufy our append_stats so it also got a histogram (histc) of our activations \ndef append_stats(hook, mod, inp, outp):\n    if not hasattr(hook,'stats'): hook.stats = ([],[],[])\n    means,stds,hists = hook.stats\n    means.append(outp.data.mean().cpu())\n    stds .append(outp.data.std().cpu())\n    hists.append(outp.data.cpu().histc(40,0,10)) #histc isn't implemented on the GPU # pop them into 40 bins between 0 and 10 note there are no negatives becouse of Relu","257a3ba6":"model = get_cnn_model(data, nfs).cuda()\nlearn,run = get_runner(model, data, lr=0.9, cbs=cbfs)","6a80098a":"#same as before look up for detais \nfor l in model:\n    if isinstance(l, nn.Sequential):\n        init.kaiming_normal_(l[0].weight)\n        l[0].bias.data.zero_()","0a1743dc":"#same same \nwith Hooks(model, append_stats) as hooks: run.fit(1, learn)","dc793e5e":"\n# Thanks to @ste for initial version of histgram plotting code\ndef get_hist(h): return torch.stack(h.stats[2]).t().float().log1p()","ed40b2a9":"\nfig,axes = plt.subplots(2,2, figsize=(15,6))\nfor ax,h in zip(axes.flatten(), hooks[:4]):\n    ax.imshow(get_hist(h), origin='lower')\n    ax.axis('off')\nplt.tight_layout() #note that most of the histogram is where the yellow is and that is what we realy care about ","e0e1aaef":"def get_min(h):\n    h1 = torch.stack(h.stats[2]).t().float()\n    return h1[:2].sum(0)\/h1.sum(0) #to see how much ylleo there are lets take the first 2 histragrams bins are near 0 so lets get the sum of those two\n#divided byt he sum of all if the bins this gonna tell us what procent of the activations are near or close to 0. ","53f9fe75":"\nfig,axes = plt.subplots(2,2, figsize=(15,6))\nfor ax,h in zip(axes.flatten(), hooks[:4]):\n    ax.plot(get_min(h))\n    ax.set_ylim(0,1)\nplt.tight_layout()","95228a42":"#we are gonna try a few things to fix the above problem \ndef get_cnn_layers(data, nfs, layer, **kwargs): #note we use **kwargs since we can get extra arguments from GeneralRelu class\n    nfs = [1] + nfs\n    return [layer(nfs[i], nfs[i+1], 5 if i==0 else 3, **kwargs) #note we use **kwargs since we can get extra arguments from GeneralRelu class\n            for i in range(len(nfs)-1)] + [\n        nn.AdaptiveAvgPool2d(1), Lambda(flatten), nn.Linear(nfs[-1], data.c)]\n\ndef conv_layer(ni, nf, ks=3, stride=2, **kwargs): #note we use **kwargs since we can get extra arguments from GeneralRelu class\n    return nn.Sequential(\n        nn.Conv2d(ni, nf, ks, padding=ks\/\/2, stride=stride), GeneralRelu(**kwargs)) #note we use **kwargs since we can get extra arguments from GeneralRelu class\n\n#better Relu \nclass GeneralRelu(nn.Module):\n    def __init__(self, leak=None, sub=None, maxv=None): #so now we can use subret from the relu(which we found was good about 0.5)\n        #and we can handle leaking (leak). And maybe we want a limit so we can use maximum value (maxv)\n        super().__init__()\n        self.leak,self.sub,self.maxv = leak,sub,maxv\n\n    def forward(self, x): \n        x = F.leaky_relu(x,self.leak) if self.leak is not None else F.relu(x) #so if you pass leak(from GeneralRelu) it will\n        #use leaky_relu otherwise we will use normal relu (F.relu(x))\n        if self.sub is not None: x.sub_(self.sub) #if you want to subrat something: go do that\n        if self.maxv is not None: x.clamp_max_(self.maxv) #if you want to use maksimum value: go do that\n        return x\n\ndef init_cnn(m, uniform=False): #uniform boolien \n    f = init.kaiming_uniform_ if uniform else init.kaiming_normal_ #so some people say uniform is better then normal, so it is an optinal see buttom of this part when used\n    for l in m:\n        if isinstance(l, nn.Sequential):\n            f(l[0].weight, a=0.1)\n            l[0].bias.data.zero_()\n\ndef get_cnn_model(data, nfs, layer, **kwargs):\n    return nn.Sequential(*get_cnn_layers(data, nfs, layer, **kwargs))","49601e6a":"def append_stats(hook, mod, inp, outp):\n    if not hasattr(hook,'stats'): hook.stats = ([],[],[])\n    means,stds,hists = hook.stats\n    means.append(outp.data.mean().cpu())\n    stds .append(outp.data.std().cpu())\n    hists.append(outp.data.cpu().histc(40,-7,7)) #since our relu now can be negative since we can subrat and use leak argurments in GeneralRelu\n    #we set in -7","96042742":"model =  get_cnn_model(data, nfs, conv_layer, leak=0.1, sub=0.4, maxv=6.)\ninit_cnn(model)\nlearn,run = get_runner(model, data, lr=0.9, cbs=cbfs)","3d113d81":"\nwith Hooks(model, append_stats) as hooks:\n    run.fit(1, learn)\n    fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4))\n    for h in hooks:\n        ms,ss,hi = h.stats\n        ax0.plot(ms[:10])\n        ax1.plot(ss[:10])\n        h.remove()\n    plt.legend(range(5));\n    \n    fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4))\n    for h in hooks:\n        ms,ss,hi = h.stats\n        ax0.plot(ms)\n        ax1.plot(ss)\n    plt.legend(range(5));","572a95cb":"fig,axes = plt.subplots(2,2, figsize=(15,6))\nfor ax,h in zip(axes.flatten(), hooks[:4]):\n    ax.imshow(get_hist(h), origin='lower')\n    ax.axis('off')\nplt.tight_layout()","f1a5f761":"def get_min(h):\n    h1 = torch.stack(h.stats[2]).t().float()\n    return h1[19:22].sum(0)\/h1.sum(0) #note we now use the middel two histrogram bins instead of the first to since we can take negative numbers ","9ef727b9":"fig,axes = plt.subplots(2,2, figsize=(15,6))\nfor ax,h in zip(axes.flatten(), hooks[:4]):\n    ax.plot(get_min(h))\n    ax.set_ylim(0,1)\nplt.tight_layout()","bffac7c4":"#export\ndef get_learn_run(nfs, data, lr, layer, cbs=None, opt_func=None, uniform=False, **kwargs):\n    model = get_cnn_model(data, nfs, layer, **kwargs)\n    init_cnn(model, uniform=uniform)\n    return get_runner(model, data, lr=lr, cbs=cbs, opt_func=opt_func)","1044cc1c":"sched = combine_scheds([0.5, 0.5], [sched_cos(0.2, 1.), sched_cos(1., 0.1)]) #so now lets use shedular look longer up in this notebook if you dont rember what it is ","eb5165ae":"learn,run = get_learn_run(nfs, data, 1., conv_layer, cbs=cbfs+[partial(ParamScheduler,'lr', sched)])","ac7fd8a3":"\nrun.fit(8, learn)","a734ac5f":"learn,run = get_learn_run(nfs, data, 1., conv_layer, uniform=True,\n                          cbs=cbfs+[partial(ParamScheduler,'lr', sched)])","2c35d8af":"\nrun.fit(8, learn)","c5213e11":"%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline","9c01df43":"#grap our data as before \nx_train,y_train,x_valid,y_valid = get_data()\n\nx_train,x_valid = normalize_to(x_train,x_valid)\ntrain_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)\n\nnh,bs = 50,512\nc = y_train.max().item()+1\nloss_func = F.cross_entropy\n\ndata = DataBunch(*get_dls(train_ds, valid_ds, bs), c)","94a535ae":"#same as before\nmnist_view = view_tfm(1,28,28)\ncbfs = [Recorder,\n        partial(AvgStatsCallback,accuracy),\n        CudaCallback,\n        partial(BatchTransformXCallback, mnist_view)]","daba7923":"nfs = [8,16,32,64,64]","9ed1aa30":"learn,run = get_learn_run(nfs, data, 0.4, conv_layer, cbs=cbfs) #learning rate=0.4","991fd724":"\n%time run.fit(2, learn)","b3e7a468":"class BatchNorm(nn.Module):\n    def __init__(self, nf, mom=0.1, eps=1e-5): #mom=momentum\n        super().__init__()\n        # NB: pytorch bn mom is opposite of what you'd expect\n        self.mom,self.eps = mom,eps\n        self.mults = nn.Parameter(torch.ones (nf,1,1)) #note mults are a parameter #note we are multiply by tensor of 1 in the last line in forward function which does nothing \n        self.adds  = nn.Parameter(torch.zeros(nf,1,1)) #note adds are a parameter #note we are add by a tensor of 0 in the last line in the forward function which does nothing\n        #but the are parameters so they can learn (they are being updated) and note that 'adds ' is the same as 'bias' in a linear model. \n        self.register_buffer('vars',  torch.ones(1,nf,1,1)) # we use self.register_buffer('vars') since when train on the GPU everything called buffer will be aoutomatic places there aswell \n        self.register_buffer('means', torch.zeros(1,nf,1,1)) #start the means on zero tensor and variance on ones tensor \n\n    def update_stats(self, x):\n        m = x.mean((0,2,3), keepdim=True) #caluelate teh mean #note (0,2,3) means we averge our these axsis 0,2 and 3. in other words\n        #we averge over all the batches(0), and we averge over all x(2) and y(3) koordinates. So all we are left with is a mean for each chanel(filter)\n        #keepdim=True means its gonna leave an empty unit axisis in position 0,2,3 so it will still boardcast nicely \n        v = x.var ((0,2,3), keepdim=True) #calulate the veriance(var)\n        self.means.lerp_(m, self.mom) #save the mean of the last few running batches #deeper explornation for 'lerp_' see pictures below #mom=momentum\n        self.vars.lerp_ (v, self.mom) #save the veriance an of the last few running batches \n        #we save those since we doing training chould lose some importen information so we just save the lave few runnings the be save \n        return m,v\n        \n    def forward(self, x):\n        if self.training:\n            with torch.no_grad(): m,v = self.update_stats(x)\n        else: m,v = self.means,self.vars #we get the means and veriance from update_stats (function above) \n        x = (x-m) \/ (v+self.eps).sqrt() #subrat by the mean and divid by the \"kvadratrod\" of the veriance \n        return x*self.mults + self.adds #and now we multiply(mults) by the x and add it with adds  #note we are usin the bottum formular in the picture above","cdc765af":"def conv_layer(ni, nf, ks=3, stride=2, bn=True, **kwargs): #create new conv layer #note bn= BatchNorm layer\n    # No bias needed if using bn\n    layers = [nn.Conv2d(ni, nf, ks, padding=ks\/\/2, stride=stride, bias=not bn), #if we do use batchnorm we dont need bias since 'adds' is a bias\n              GeneralRelu(**kwargs)]\n    if bn: layers.append(BatchNorm(nf)) #uses the class batchnorm above\n    return nn.Sequential(*layers)","3f31c194":"\n#same as before\ndef init_cnn_(m, f):\n    if isinstance(m, nn.Conv2d):\n        f(m.weight, a=0.1)\n        if getattr(m, 'bias', None) is not None: m.bias.data.zero_()\n    for l in m.children(): init_cnn_(l, f)\n\ndef init_cnn(m, uniform=False):\n    f = init.kaiming_uniform_ if uniform else init.kaiming_normal_\n    init_cnn_(m, f)\n\ndef get_learn_run(nfs, data, lr, layer, cbs=None, opt_func=None, uniform=False, **kwargs):\n    model = get_cnn_model(data, nfs, layer, **kwargs)\n    init_cnn(model, uniform=uniform)\n    return get_runner(model, data, lr=lr, cbs=cbs, opt_func=opt_func)","0d83fa7b":"learn,run = get_learn_run(nfs, data, 0.9, conv_layer, cbs=cbfs)","c1fa1d05":"with Hooks(learn.model, append_stats) as hooks:\n    run.fit(1, learn)\n    fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4))\n    for h in hooks[:-1]:\n        ms,ss = h.stats\n        ax0.plot(ms[:10])\n        ax1.plot(ss[:10])\n        h.remove()\n    plt.legend(range(6));\n    \n    fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4))\n    for h in hooks[:-1]:\n        ms,ss = h.stats\n        ax0.plot(ms)\n        ax1.plot(ss)","bdafd059":"learn,run = get_learn_run(nfs, data, 1.0, conv_layer, cbs=cbfs)","b5c3cd81":"\n%time run.fit(3, learn)","48260d8b":"#export\ndef conv_layer(ni, nf, ks=3, stride=2, bn=True, **kwargs):\n    layers = [nn.Conv2d(ni, nf, ks, padding=ks\/\/2, stride=stride, bias=not bn),\n              GeneralRelu(**kwargs)]\n    if bn: layers.append(nn.BatchNorm2d(nf, eps=1e-5, momentum=0.1))\n    return nn.Sequential(*layers)","7d4968cc":"%time run.fit(3, learn)","db90fa6f":"sched = combine_scheds([0.3, 0.7], [sched_lin(0.6, 2.), sched_lin(2., 0.1)])","140931cc":"learn,run = get_learn_run(nfs, data, 0.9, conv_layer, cbs=cbfs\n                          +[partial(ParamScheduler,'lr', sched)])","b0930c5d":"run.fit(8, learn)","04044143":"#so layernorm is almost the same as batchnorm just without the running averge so instead we make an averge on one image and not call of them\n#though it doesnt realy work this can be seem by the acc. on 0.09 when we try to use in a model but when your doing RNN u have to use something like this\nclass LayerNorm(nn.Module):\n    __constants__ = ['eps']\n    def __init__(self, eps=1e-5):\n        super().__init__()\n        self.eps = eps\n        self.mult = nn.Parameter(tensor(1.))\n        self.add  = nn.Parameter(tensor(0.))\n\n    def forward(self, x):\n        m = x.mean((1,2,3), keepdim=True)\n        v = x.var ((1,2,3), keepdim=True)\n        x = (x-m) \/ ((v+self.eps).sqrt())\n        return x*self.mult + self.add","64b0dccd":"\ndef conv_ln(ni, nf, ks=3, stride=2, bn=True, **kwargs):\n    layers = [nn.Conv2d(ni, nf, ks, padding=ks\/\/2, stride=stride, bias=True),\n              GeneralRelu(**kwargs)]\n    if bn: layers.append(LayerNorm())\n    return nn.Sequential(*layers)","3e1dec95":"learn,run = get_learn_run(nfs, data, 0.8, conv_ln, cbs=cbfs)","682bc513":"\n%time run.fit(3, learn)","2d2c49be":"#same as layernorm but only using 2,3 and not 1,2,3. doesnt realy work for classification \nclass InstanceNorm(nn.Module):\n    __constants__ = ['eps']\n    def __init__(self, nf, eps=1e-0):\n        super().__init__()\n        self.eps = eps\n        self.mults = nn.Parameter(torch.ones (nf,1,1))\n        self.adds  = nn.Parameter(torch.zeros(nf,1,1))\n\n    def forward(self, x):\n        m = x.mean((2,3), keepdim=True)\n        v = x.var ((2,3), keepdim=True)\n        res = (x-m) \/ ((v+self.eps).sqrt())\n        return res*self.mults + self.adds","a7de7b3b":"def conv_in(ni, nf, ks=3, stride=2, bn=True, **kwargs):\n    layers = [nn.Conv2d(ni, nf, ks, padding=ks\/\/2, stride=stride, bias=True),\n              GeneralRelu(**kwargs)]\n    if bn: layers.append(InstanceNorm(nf))\n    return nn.Sequential(*layers)","e73a58c4":"learn,run = get_learn_run(nfs, data, 0.1, conv_in, cbs=cbfs)","a18fc4c9":"%time run.fit(3, learn)","70c69d51":"data = DataBunch(*get_dls(train_ds, valid_ds, 2), c) #batch size 2","8f096c79":"def conv_layer(ni, nf, ks=3, stride=2, bn=True, **kwargs):\n    layers = [nn.Conv2d(ni, nf, ks, padding=ks\/\/2, stride=stride, bias=not bn),\n              GeneralRelu(**kwargs)]\n    if bn: layers.append(nn.BatchNorm2d(nf, eps=1e-5, momentum=0.1))\n    return nn.Sequential(*layers)","c8887bb8":"learn,run = get_learn_run(nfs, data, 0.4, conv_layer, cbs=cbfs)","715fc4a7":"%time run.fit(1, learn)#note we get an acc. to about 20% because if the small batch size ","44e354ac":"#SO THERE ARE TREE CONCEPT TO THIS IDEA:\n# 1. we use the formular E[X^2]-E[X^2] to get the varaince \n# 2. the batch size can be different from minibatch to minibatch \n# 3. dbiasing - we want to make sure at each point that no observation is rated to highly and the normal problem\n#with moving averges is that the first point gets far to much weight because it appers in the firsst moving averge and secound and the third and the \n#fourth so like before when we toke the curretn point multiplied with the momentum added with the next point minus the momentum. but insted of that\n#we if the averge moving is 1 we divide it by the -momentum and the secound time we take two points divided it by -momentum^2 and so on.\n#this will be explained in more detail in a later. \nclass RunningBatchNorm(nn.Module):\n    def __init__(self, nf, mom=0.1, eps=1e-5):\n        super().__init__()\n        self.mom,self.eps = mom,eps\n        self.mults = nn.Parameter(torch.ones (nf,1,1))\n        self.adds = nn.Parameter(torch.zeros(nf,1,1))\n        self.register_buffer('sums', torch.zeros(1,nf,1,1)) #register a buffer for sum\n        self.register_buffer('sqrs', torch.zeros(1,nf,1,1)) #reister a buffer for 'kvadratrod'\n        self.register_buffer('batch', tensor(0.))\n        self.register_buffer('count', tensor(0.)) #so check for minibatches we make a register for counts \n        self.register_buffer('step', tensor(0.))\n        self.register_buffer('dbias', tensor(0.))\n\n    def update_stats(self, x):\n        bs,nc,*_ = x.shape\n        self.sums.detach_()\n        self.sqrs.detach_()\n        dims = (0,2,3)\n        s = x.sum(dims, keepdim=True)#we do the sum over 0,2,3 dimensions (dims) \n        ss = (x*x).sum(dims, keepdim=True)#and we do (x*x).sum so ^2--note formular is used \n        c = self.count.new_tensor(x.numel()\/nc) #so we have to devide the total number in the minibatch(numel) byt the number of chanels(nc)\n        mom1 = 1 - (1-self.mom)\/math.sqrt(bs-1)\n        self.mom1 = self.dbias.new_tensor(mom1)\n        self.sums.lerp_(s, self.mom1) #and we take the lerp_ --- the eksponential moving averga of the sums \n        self.sqrs.lerp_(ss, self.mom1)#and we take the lerp_ --- the eksponential moving averga of the sqrs\n        self.count.lerp_(c, self.mom1) #and take a eksponential moving averge of the counts of the batch sizes \n        self.dbias = self.dbias*(1-self.mom1) + self.mom1\n        self.batch += bs\n        self.step += 1\n\n    def forward(self, x): #in the forward function we dont divide by the batchs std and we dont subrat the batch mean but use the moving averge statistic at training time aswell \n        if self.training: self.update_stats(x)\n        sums = self.sums\n        sqrs = self.sqrs\n        c = self.count\n        if self.step<100:\n            sums = sums \/ self.dbias #note we divide by the dbias amount \n            sqrs = sqrs \/ self.dbias\n            c    = c    \/ self.dbias\n        means = sums\/c\n        vars = (sqrs\/c).sub_(means*means) #and for the variance sqrs divided by counts minus (.sub_) means^2 -- note fomular is used\n        #in chase we get the first minibatch that is close to 0 and we dont want that to destoy everything \n        if bool(self.batch < 20): vars.clamp_min_(0.01) #so if you having seem more them 20 items yet just clamp the variance to be no smaller then 0.01\n        x = (x-means).div_((vars.add_(self.eps)).sqrt())\n        return x.mul_(self.mults).add_(self.adds)","d9673b63":"\ndef conv_rbn(ni, nf, ks=3, stride=2, bn=True, **kwargs):\n    layers = [nn.Conv2d(ni, nf, ks, padding=ks\/\/2, stride=stride, bias=not bn),\n              GeneralRelu(**kwargs)]\n    if bn: layers.append(RunningBatchNorm(nf))\n    return nn.Sequential(*layers)","b0090795":"\nlearn,run = get_learn_run(nfs, data, 0.4, conv_rbn, cbs=cbfs)","1018eff6":"%time run.fit(1, learn)","7b7fdbc0":"\ndata = DataBunch(*get_dls(train_ds, valid_ds, 32), c)","8663dc52":"learn,run = get_learn_run(nfs, data, 0.9, conv_rbn, cbs=cbfs\n                          +[partial(ParamScheduler,'lr', sched_lin(1., 0.2))])","929a8fca":"\n%time run.fit(1, learn)","598e2d53":"# optim\nLet's replace our previous manually coded optimization step:\n\n    with torch.no_grad():\n        for p in model.parameters(): p -= p.grad * lr\n        model.zero_grad()\nand instead use just:\n\n    opt.step()\n    opt.zero_grad()","b6014fb2":"\nHaving given an __enter__ and __exit__ method to our Hooks class, we can use it as a context manager. This makes sure that onces we are out of the with block, all the hooks have been removed and aren't there to pollute our memory.","108c04bb":"PyTorch already provides this exact functionality in optim.SGD (it also handles stuff like momentum, which we'll look at later - except we'll be doing it in a more flexible way!)","b0456af2":"-------------------------------------------------------------------------------------------------------------------------","f01daad9":"\nNow, that's definitely faster!","492ba3ff":"note we did it after the keiming inisilatation of the first layer, so now we dont see the first pich where everything went to shit before. Bot we see a lot better learning of the fist 10 layers. ","bc566c9d":"So we will start merging some of the parameters, the things we want to keep is epochs the rests we want to put in one. ","837d53e0":"So now we need a loss function ","47ce5aea":"Then, there is a way to compute the log of the sum of exponentials in a more stable way, called the LogSumExp trick. The idea is to use the following formula:\n\n$$\\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )$$\nwhere a is the maximum of the $x_{j}$.","674849f9":"\nWe can use it to write a Hooks class that contains several hooks. We will also use it in the next notebook as a container for our objects in the data block API.","8036c949":"this above graf shows the four different schedulars but this enought since we want to get first a higher leairning rate and then a lower oone(se grafs a the bottum). so we combine the different schedulars","fe46083c":"\nAdd callbacks so we can remove complexity from loop, and make it flexible:","34e20122":"\nThis first callback is reponsible to switch the model back and forth in training or validation mode, as well as maintaining a count of the iterations, or the percentage of iterations ellapsed in the epoch.","6ab9bbfe":"and look at that we now see that in the last layer is lost less then 20 procent of the activations. so we are now using nearly all of our activations by being cearful about out initialisation and our relu ","df928df8":"# Random sampling\nWe want our training set to be in a random order, and that order should differ each iteration. But the validation set shouldn't be randomized.","a82d48fb":"\nNote that the formula\n\n$$\\log \\left ( \\frac{a}{b} \\right ) = \\log(a) - \\log(b)$$\ngives a simplification when we compute the log softmax, which was previously defined as (x.exp()\/(x.exp().sum(-1,keepdim=True))).log()","9789f9e8":"we have though a problem with batchnorm since it cannot take batches of one or very small ones like 2-4 since at a point it will divide by itself, and we will go to infinity. when usin RNN batch can also be a problem. so now we will try to fix it ","db3acdcf":"\nFrom the PyTorch docs:\n\nGroupNorm(num_groups, num_channels, eps=1e-5, affine=True)\n\nThe input channels are separated into num_groups groups, each containing num_channels \/ num_groups channels. The mean and standard-deviation are calculated separately over the each group. $\\gamma$ and $\\beta$ are learnable per-channel affine transform parameter vectorss of size num_channels if affine is True.\n\nThis layer uses statistics computed from input data in both training and evaluation modes.\n\nArgs:\n\nnum_groups (int): number of groups to separate the channels into\nnum_channels (int): number of channels expected in input\neps: a value added to the denominator for numerical stability. Default: 1e-5\naffine: a boolean value that when set to True, this module has learnable per-channel affine parameters initialized to ones (for weights) and zeros (for biases). Default: True.\nShape:\n\nInput: (N, num_channels, *)\nOutput: (N, num_channels, *) (same shape as input)\nExamples::\n\n    >>> input = torch.randn(20, 6, 10, 10)\n    >>> # Separate 6 channels into 3 groups\n    >>> m = nn.GroupNorm(3, 6)\n    >>> # Separate 6 channels into 6 groups (equivalent with InstanceNorm)\n    >>> m = nn.GroupNorm(6, 6)\n    >>> # Put all 6 channels into a single group (equivalent with LayerNorm)\n    >>> m = nn.GroupNorm(1, 6)\n    >>> # Activating the module\n    >>> output = m(input)","1ef698ba":"# Part 6 07_batchnorm","26489a21":"\nSo we can use it for our log_softmax function.","dfe9408b":"and we see in the last layer that over 90 procent of the activation actual zero. so if you are training you model like this, it might look like it is training nicely without you knowing that 90 procent o you activations are totaly 'tabt'. so you are never gonna get great result by wheasting 90 procent of your actiovations. so lets try to fix it ","66a50983":"## CUDA","200d5394":"----------------------------------------------------------------------------------------------------------------------------------------------","dc05db3a":"![image.png](attachment:image.png)","cce1e214":"\nThis way, we will avoid an overflow when taking the exponential of a big activation. In PyTorch, this is already implemented for us.","98fe5a2d":"\n## Hook class\nWe can refactor this in a Hook class. It's very important to remove the hooks when they are deleted, otherwise there will be references kept and the memory won't be properly released when your model is deleted.","899b174a":"\nWe can then use it in training and see how it helps keep the activations means to 0 and the std to 1.","7d682166":"# Part 5 \/06_cuda_cnn_hooks_init","a7e64573":"\n# Cross entropy loss\nFirst, we will need to compute the softmax of our activations. This is defined by:\n\n$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{e^{x_{0}} + e^{x_{1}} + \\cdots + e^{x_{n-1}}}$$\nor more concisely:\n\n$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum_{0 \\leq j \\leq n-1} e^{x_{j}}}$$\nIn practice, we will need the log of the softmax when we calculate the loss.","4b7604b3":"note that the mean starts on 0 and the std starts on 1 \nand note also that the training has gotten rid of the pich and collapses from before","711aaeb9":"no memory for more pictures so that... just happened... \nbut what we do instead is to take each current point multiply it with a momentum(mom) and add it to the secound point multiplied the 1-mom (1-momentum) and therefor we just have one point all the time and we can forget about the rest. Note also here that we only add a little bit of the secound point each time. this gives us a eksponential aftagende function and all of this is called 'linear interpolation' and in code is 'lerp_'","fc44faf6":"so it stops since our loss got much worse then the best worse i the graf above\nso we can se on the graf below that we dont hit the 100 iterations ","ff993950":"![image.png](attachment:image.png)","fc629b36":"note this is our pre-batchnorm we can now see the acc. on about 94 % and this what we will try to improve this by batchnorm ","f688bd67":"E[X^2]-E[X^2]","1db2c39a":"NB: In fastai we use a bool param to choose whether to make it a forward or backward hook. In the above version we're only supporting forward hooks.","043b0351":"\n## Generalized ReLU\nNow let's use our model with a generalized ReLU that can be shifted and with maximum value.","75b07071":"### Other callbacks","972ce885":"the first 4 layers and it is bautiful where we see that it doesnt collapse so it should use the fullnes of the activations. to realy be sure we see the next graf","b74d9cea":"# PyTorch DataLoader","84096190":"## Refactor model from above\nFirst we can regroup all the conv\/relu in a single function:","b9f81294":"\n# Basic training loop\nBasically the training loop repeats over the following steps:\n\n* get the output of the model on a batch of inputs\n* compare the output to the labels we have and compute a loss\n* calculate the gradients of the loss with respect to every parameter of the model\n* update said parameters with those gradients to make them a little bit better","4abe8773":"Question: Are these validation results correct if batch size varies?\n\nget_dls returns dataloaders for the training and validation sets:","bb15249d":"To refactor layers, it's useful to have a Lambda layer that can take a basic function and convert it to a layer you can put in nn.Sequential.\n\nNB: if you use a Lambda layer with a lambda function, your model won't pickle so you won't be able to save it with PyTorch. So it's best to give a name to the function you're using inside your Lambda (like flatten below).","5c161dad":"From the histograms, we can easily get more informations like the min or max of the activations","1eb2324e":"# Part 4 05b_early_stopping","40adf064":"The cross entropy loss for some target $x$ and some prediction $p(x)$ is given by:\n\n$$ -\\sum x\\, \\log p(x) $$\nBut since our $x$s are 1-hot encoded, this can be rewritten as $-\\log(p_{i})$ where i is the index of the desired target.\n\nThis can be done using numpy-style integer array indexing. Note that PyTorch supports all the tricks in the advanced indexing methods discussed in that link.","0073e7aa":"## Fix small batch sizes\nWhat's the problem?\nWhen we compute the statistics (mean and std) for a BatchNorm Layer on a small batch, it is possible that we get a standard deviation very close to 0. because there aren't many samples (the variance of one thing is 0. since it's equal to its mean).","721ce6e9":"So how does that look like?","c1b23259":"## This is last part of fastai part 1 lesson 9, though it is mixed with my notes, eksperiments and what I found usefull, if you want the pure version, check fastai github or the following link: https:\/\/github.com\/fastai\/course-v3\/tree\/master\/nbs\/dl2","b0225ad7":"for the model there are only to things the parameters (the things we are updating) and the activations(the things we calculate)  and the thing we want on the GPU is the parameters and inputs ","497ec1a8":"# Builtin batchnorm from pythorch","76396f13":"# Initial setup\n## data","4e7cc4f9":"## Early stopping","d6e04fe0":"# Part 3 05_anneal.ipynb","96edf3d5":"the the picture above is the first layer. (note this is an explanation to the code above)\nthe fist filter is 8 and thereby we make a 3 by 3 matrix to slide over the full matrix. tHis givea an out of 9. so there a 9 input activations to do out dot product with. this gives us a new matrix that is 8 by 9. and which we can transform to the 8 1.d tensor we want. so\nall we are doing is almost nothing since we are going from 9 to 8. so we use 5 or 7 to slide over the full matrix for the fist layer instead. ","f017ccbe":"\n### What can we do in a single epoch?\nNow let's see with a decent batch size what result we can get.","45f38e3f":"Helper function to quickly normalize with the mean and standard deviation from our training set:","8c022925":"and now since we have created it we can use pytorchs dataloader ","e9082ad0":"## Running Batch Norm\nTo solve this problem we introduce a Running BatchNorm that uses smoother running mean and variance for the mean and std.","38f9912f":"# CallbackHandler\nThis was our training loop (without validation) from the previous notebook, with the inner loop contents factored out:","55501669":"\n# ConvNet\nLet's get the data and training interface from where we left in the last notebook.","58c3c1e2":"A Hooks class\nLet's design our own class that can contain a list of objects. It will behave a bit like a numpy array in the sense that we can index into it via:\n\n* a single index\n* a slice (like 1:5)\n* a list of indices\n* a mask of indices ([True,False,False,True,...])\n\n        The __iter__ method is there to be able to do things like for x in ....","daba9432":"------------------------------------------------------------------------------------------------------------------------------------------------------------------","b16b3252":"In practice, we'll often want to combine different schedulers, the following function does that: it uses scheds[i] for pcts[i] of the training.","0b3133f9":"### LR Finder\nNB: You may want to also add something that saves the model before running this, and loads it back after running - otherwise you'll lose your weights!","097d6c60":"Now, our whole process of obtaining the data loaders and fitting the model can be run in 3 lines of code:","e4c05c2d":"\nAnd this helper function will quickly give us everything needed to run the training.","0da3018e":"\n## Other statistics\nLet's store more than the means and stds and plot histograms of our activations now.","cbfd9bca":"\nAnother thing is that we can do the mnist resize in a batch transform, that we can do with a Callback.","ae39c8f8":"picture from article about batchnorm. ","1750983a":"\nA hook is attached to a layer, and needs to have a function that takes three arguments: module, input, output. Here we store the mean and std of the output in the correct position of our list.","a2bd6894":"Note that PyTorch's DataLoader, if you pass num_workers, will use multiple threads to call your Dataset.\nso it will fire of more processes and each one wil seperated grap stuff from the dataset and then it will collate them afterwood\nit usefull if you are opening big jpeg files and all kind of images transformations ","5ca18d2a":"# Using parameters and optim\nParameters\nUse nn.Module.__setattr__ and move relu to functional:","e8ae4287":"here we will se that the first batches is every. in other words if we get the first batches right the model will get a much better scc score.\nfor this will be practising godd anniling ","6aeb485e":"----------------------------------------------------------------------------------------------------------------------------","6f014b7c":"## ConvNet","f8ec6dd4":"We'll also re-create our TestCallback","a5787ec2":"Thought experiment: can this distinguish foggy days from sunny days (assuming you're using it before the first conv)?\n\n## Instance norm\nFrom the paper:\n\nThe key difference between contrast and batch normalization is that the latter applies the normalization to a whole batch of images instead for single ones:\n\n$$\\label{eq:bnorm}\n    y_{tijk} =  \\frac{x_{tijk} - \\mu_{i}}{\\sqrt{\\sigma_i^2 + \\epsilon}},\n    \\quad\n    \\mu_i = \\frac{1}{HWT}\\sum_{t=1}^T\\sum_{l=1}^W \\sum_{m=1}^H x_{tilm},\n    \\quad\n    \\sigma_i^2 = \\frac{1}{HWT}\\sum_{t=1}^T\\sum_{l=1}^W \\sum_{m=1}^H (x_{tilm} - mu_i)^2.\n$$\nIn order to combine the effects of instance-specific normalization and batch normalization, we propose to replace the latter by the instance normalization (also known as contrast normalization) layer:\n\n$$\\label{eq:inorm}\n    y_{tijk} =  \\frac{x_{tijk} - \\mu_{ti}}{\\sqrt{\\sigma_{ti}^2 + \\epsilon}},\n    \\quad\n    \\mu_{ti} = \\frac{1}{HW}\\sum_{l=1}^W \\sum_{m=1}^H x_{tilm},\n    \\quad\n    \\sigma_{ti}^2 = \\frac{1}{HW}\\sum_{l=1}^W \\sum_{m=1}^H (x_{tilm} - mu_{ti})^2.\n$$","86a8d00f":"so now lets add callbacks ","8ef3a317":"This is roughly how fastai does it now (except that the handler can also change and return xb, yb, and loss). But let's see if we can make things simpler and more flexible, so that a single class has access to everything and can change anything at any time. The fact that we're passing cb to so many functions is a strong hint they should all be in the same class!","5edda2a4":"\nAnd here are other scheduler functions:","cb8856ad":"# Validation\nYou always should also have a validation set, in order to identify if you are overfitting.\n\nWe will calculate and print the validation loss at the end of each epoch.\n\n(Note that we always call model.train() before training, and model.eval() before inference, because these are used by layers such as nn.BatchNorm2d and nn.Dropout to ensure appropriate behaviour for these different phases.)","f38afea8":"This one takes the flat vector of size bs x 784 and puts it back as a batch of images of 28 by 28 pixels:","356e6a33":"lets look inside the model and try to get a higher acc.","8bea1111":"----------------------------------------------------------------------------------------------------------------------------------------------","78f6030d":"our problem is though that our training loop always loop through our training set in order. and so we loss the randomness of sjoveling it each time. so to fix this we do random sampling","1b4b39e9":"\nNB: In fastai we also use exponential smoothing on the loss. For that reason we check for best_loss*3 instead of best_loss*10.","81157109":"# Dataset and DataLoader\n## Dataset\nIt's clunky to iterate through minibatches of x and y values separately:\n\n    xb = x_train[start_i:end_i]\n    yb = y_train[start_i:end_i]\nInstead, let's do these two steps together, by introducing a Dataset class:\n\n    xb,yb = train_ds[i*bs : i*bs+bs]","fe3e41fc":"A hidden layer in an artificial neural network is a layer in between input layers and output layers, where artificial neurons take in a set of weighted inputs and produce an output through an activation function","dedeab10":"# Annealing\n\nWe define two new callbacks: the Recorder to save track of the loss and our scheduled learning rate, and a ParamScheduler that can schedule any hyperparameter as long as it's registered in the state_dict of the optimizer.","8b7114af":"\nWe can refactor this with a decorator.","0996fa90":"\nUniform init may provide more useful initial weights (normal distribution puts a lot of them at 0).","9572857a":"------------------------------------------------------------------------------------------------------------------------------------------------------------------","8ce02f6d":"remember that there are to types of numbers in a neural network paramteres(thing we learn) and activations(tings we calulate)","db34891d":"\nThird callback: how to compute metrics.","875cc969":"----------------------------------------------------------------------------------------------------------------------------------------------","54cd8618":"\n## Custom\nLet's start by building our own BatchNorm layer from scratch.","5576a0e5":"\nFactor out the connected pieces of info out of the fit() argument list\n\n    fit(epochs, model, loss_func, opt, train_dl, valid_dl)\n\nLet's replace it with something that looks like this:\n\n    fit(1, learn)\n\nThis will allow us to tweak what's happening inside the training loop in other places of the code because the Learner object will be mutable, so changing any of its attribute elsewhere will be seen in our training loop.","1b6a2609":"In PyTorch, F.log_softmax and F.nll_loss are combined in one optimized function, F.cross_entropy.","b476f23e":"Then use PyTorch's implementation.","18eb177f":"lets try to use batchnorm on a small batch size","7c8c2477":"so for 'self.means.lerp_(m, self.mom)' in the updaa_stats function above. we usally calulate the averge by taken the first 5 point and then the next 5 points and so on, like the picture above. but since every singel activation has one, there can be hundred of millions of activations. and we dont want to save a history of all of them just to get the averge. luckily there are another way","8f7bd322":"\n## Pytorch hooks\nHooks are PyTorch object you can add to any nn.Module. A hook will be called when a layer, it is registered to, is executed during the forward pass (forward hook) or the backward pass (backward hook).\n\nHooks don't require us to rewrite the model.","b7304b37":"Let's check it behaved properly.","7bb6d1f5":"# Runner","c3c4e196":"# nn.ModuleList\nnn.ModuleList does this for us.","1336a374":"----------------------------------------------------------------------------------------------------------------------------------------------","1c9d801a":"# Registering modules\nWe can use the original layers approach, but we have to register the modules.","0033dd3c":"\n# More norms\n## Layer norm\nFrom the paper: \"batch normalization cannot be applied to online learning tasks or to extremely large distributed models where the minibatches have to be small\".\n\nGeneral equation for a norm layer with learnable affine:\n\n$$y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta$$\nThe difference with BatchNorm is\n\n1. we don't keep a moving average\n2. we don't average over the batches dimension but over the hidden dimension, so it's independent of the batch size","01c99428":"----------------------------------------------------------------------------------------------------------------------------------------------","d2a5c47b":"# With scheduler\nNow let's add the usual warm-up\/annealing.","0fe89119":"new better version of the above code ","8ae392da":"# nn.Sequential\nnn.Sequential is a convenient class which does the same as the above:","69c5f88c":"Randomized tests can be very useful.","abac3015":"# Part 2 04_callbacks","e7cd8743":"We can now define a simple CNN.","ec490b49":"# Hooks\n## Manual insertion\nLet's say we want to do some telemetry, and want the mean and standard deviation of each activations in the model. First we can do it manually like this:","9d5a8e2c":"so the code is almost the same as before but with a few different things (se the # comment)","a380a730":"Answer: it is an incorrect way it  is done since, if we try to devide with a batch size of first a 10000  and then 1, we get differeent outcomes. We will implement an other way later. But most people do it this way","dfb0a7b3":"![image.png](attachment:image.png)","bf0db4d4":"Basic callbacks from the previous notebook:","789d7c7e":"\nNow we can have a look at the means and stds of the activations at the beginning of training.","29074e11":"With the AdaptiveAvgPool, this model can now work on any size input:","b40e0950":"\nLet's start with a simple linear schedule going from start to end. It returns a function that takes a pos argument (going from 0 to 1) such that this function goes from start (at pos=0) to end (at pos=1) in a linear fashion.","6337c720":"but to realy know whats going on we have to see in the activations. so to see if something is bad we will see in the activaitions and see how many of them are realy realy small and how well is it realy getting everything activated ","9ff4ff2b":"# Initial setup","638f3c6d":"# DataLoader\n\nPreviously, our loop iterated over batches (xb, yb) like this:\n\n    for i in range((n-1)\/\/bs + 1):\n        xb,yb = train_ds[i*bs : i*bs+bs]\n        ...\nLet's make our loop much cleaner, using a data loader:\n\n    for xb,yb in train_dl:\n        ...","749e76a0":"### Better callback cancellation","8423c25c":"so to fix this is to use 'epsilon' (eps) which just is a very small number (u define) its normally used to avoid float roundings and we see it typically in the bottum at a division. But here we just see it as a very good hyperparameter that we should use.\nso optinen one is to use a higher epsilon value in BatchNorm because he formular is '(x-m) \/ (v+self.eps).sqrt()' so if we just set eps to 0.1 \nwe can never divide by zero even though v(variance) is 0. so by makking eps bigger we make sure we get further and further away for our fomular to crachs. but still there are a better solution and its shown in the code below --- which is running batch norm ","46d17140":"----------------------------------------------------------------------------------------------------------------------------------------------","daf61870":"Question: why can't this classify anything?\n\nLost in all those norms? The authors from the group norm paper have you covered:\n![image.png](attachment:image.png)\n## Group norm","438bcda3":"\n# Batchnorm","0f9fdc66":"This solves the small batch size issue!","3390ea43":"This took a long time to run, so it's time to use a GPU. A simple Callback can make sure the model, inputs and targets are all on the same device.","1d781324":"Here is an example: use 30% of the budget to go from 0.3 to 0.6 following a cosine, then the last 70% of the budget to go from 0.6 to 0.2, still following a cosine.","8a9e35d1":"![image.png](attachment:image.png)"}}