{"cell_type":{"0708898a":"code","baca4807":"code","d8d61828":"code","cbac9725":"code","55345c5d":"code","980ae2d3":"code","b5506d7a":"code","ee738091":"code","235c760b":"code","10c7f3ac":"code","207ee384":"code","b9675194":"code","de05c91b":"code","01b46994":"code","3722f2f1":"code","8eaa32e4":"code","5e59127d":"code","ce60d0e3":"code","a3a4160e":"code","6f195a0f":"code","6574c825":"code","17d98ffd":"code","c8ac6746":"code","51fbdad4":"code","8c024ddc":"code","533e3765":"code","b4058b08":"code","052e682e":"code","bd98e825":"code","bb8310f8":"code","50f44c0d":"code","a6931e62":"code","afee2545":"code","36eb0c2f":"code","1b419707":"code","40d69410":"markdown","ba5879d7":"markdown","f0f0ff19":"markdown","68d817aa":"markdown","b6166a5f":"markdown","fcdda58a":"markdown","d252ba64":"markdown","71bb9b2d":"markdown","3361f7b9":"markdown","eaf33f15":"markdown","4fae8f04":"markdown","33d0705b":"markdown","0da100f0":"markdown","bab4dc3c":"markdown","a6cd27e9":"markdown","1efe7833":"markdown"},"source":{"0708898a":"%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\n\nimport pandas as pd\npd.options.display.max_columns = 100\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","baca4807":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ndata = pd.concat([train.drop('Survived',axis=1),test])","d8d61828":"train.head()","cbac9725":"plt.figure(figsize=(10,10))\nsns.heatmap(data.isnull(),cmap=\"viridis\",yticklabels=False,cbar=False)","55345c5d":"train.info()","980ae2d3":"test.info()","b5506d7a":"train['PassengerId'].value_counts()","ee738091":"train['Name'].value_counts()","235c760b":"train['Cabin'].value_counts()","10c7f3ac":"train['Ticket'].value_counts()","207ee384":"train = train.drop(['Cabin','Ticket','Name'],axis=1)\ntest = test.drop(['Cabin','Ticket','Name'],axis=1)","b9675194":"# rename columns to be more descriptive\ntrain.rename(columns={\"Pclass\": \"PClass\", \"Parch\": \"ParCh\"},inplace=True)\ntest.rename(columns={\"Pclass\": \"PClass\", \"Parch\": \"ParCh\"},inplace=True)","de05c91b":"# define default figsize helper function\n\ndef set_figsize():\n    '''\n    Sets default figsize to 12x8\n    '''\n    plt.figure(figsize=[12,8])","01b46994":"# define default legend helper function\n\ndef legend_survived():\n    '''\n    Plots legend with Not survived & Survived\n    '''\n    plt.legend(['Did not survive','Survived'],loc='best')","3722f2f1":"# create subsets of survived vs not_survived for hue in plots\n\nsurvived = train[train['Survived'] == 1]\nnot_survived = train[train['Survived'] == 0]","8eaa32e4":"set_figsize()\nplt.title('Survival count by Sex')\nsns.countplot('Sex',data=train,hue='Survived')\nlegend_survived()","5e59127d":"set_figsize()\nplt.title('Survival count by Passenger class')\nsns.countplot('PClass',data=train,hue='Survived')\nlegend_survived()","ce60d0e3":"set_figsize()\nplt.title('Survival count by Port of Embarkation')\nsns.countplot('Embarked',data=train,hue='Survived')\nlegend_survived()","a3a4160e":"set_figsize()\nplt.title('Survival count by Number of Siblings\/Spouse')\nsns.countplot('SibSp',data=train,hue='Survived')\nplt.xlabel('Number of Siblings\/Spouse')\nlegend_survived()","6f195a0f":"set_figsize()\nplt.title('Survival count by Number of Parents\/Children')\nsns.countplot('ParCh',data=train,hue='Survived')\nplt.xlabel('Number of Parents\/Children')\nplt.legend(['Did not survive','Survived'],loc='upper right')","6574c825":"plt.figure(figsize=(20,8))\n\nax1 = sns.kdeplot(not_survived['Fare'],shade=True)\nax1.set_xlim((0,150))\n\nax2 = sns.kdeplot(survived['Fare'],shade=True)\nax2.set_xlim((0,150))\n\nlegend_survived()\nplt.title('Survival density by Fare')\nplt.xlabel('Fare')\nplt.ylabel('Density')","17d98ffd":"plt.figure(figsize=(20,8))\n\nax1 = sns.kdeplot(not_survived['Age'],shade=True)\n\nax2 = sns.kdeplot(survived['Age'],shade=True)\n\nlegend_survived()\nplt.title('Survival density by Age')\nplt.xlabel('Age')\nplt.ylabel('Density')","c8ac6746":"# Imputing Missing Age Values: choose median due to outliers, which affect mean\ntrain['Age'].fillna(train['Age'].median(),inplace=True)\n\n# Imputing Missing Embarked Values\ntrain['Embarked'].fillna(train['Embarked'].value_counts().index[0], inplace=True)\n\n#Creating a dictionary to convert Passenger Class from 1,2,3 to 1st,2nd,3rd.\nd = {1:'1st',2:'2nd',3:'3rd'}\n\n#Mapping the column based on the dictionary\ntrain['PClass'] = train['PClass'].map(d)\n\n# Getting Dummies of Categorical Variables\ncat_vars = train[['PClass','Sex','Embarked']]\ndummies = pd.get_dummies(cat_vars,drop_first=True)\n\n# Drop original cat_vars\ntrain = train.drop(['PClass','Sex','Embarked'],axis=1)\n# Concatenate dummies and train\ntrain = pd.concat([train,dummies],axis=1)\n\n# Check the clean version of the train data.\ntrain.head()","51fbdad4":"# split features and label\nX = train.drop(['Survived'],1)\ny = train['Survived']\n\n# Use train test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","8c024ddc":"# choose GBC\n# make predictions\nfrom sklearn.ensemble import GradientBoostingClassifier\nGBC = GradientBoostingClassifier(learning_rate=0.1,max_depth=3)\nGBC.fit(X_train, y_train)\npred_GBC = GBC.predict(X_test)","533e3765":"# evaluate performance\nfrom sklearn.metrics import confusion_matrix, classification_report\nprint(\"GBC results:\\n\")\nprint(confusion_matrix(y_test, pred_GBC))\nprint(classification_report(y_test,pred_GBC))","b4058b08":"# Try RFC\nfrom sklearn.ensemble import RandomForestClassifier\nRFC = RandomForestClassifier()\nRFC.fit(X_train, y_train)\npred_RFC = RFC.predict(X_test)","052e682e":"# evaluate performance\nprint(\"RFC results:\\n\")\nprint(confusion_matrix(y_test, pred_RFC))\nprint(classification_report(y_test,pred_RFC))","bd98e825":"# Try logistic regression\nfrom sklearn.linear_model import LogisticRegression\nlogit = LogisticRegression()\nlogit.fit(X_train, y_train)\npred_logit = logit.predict(X_test)","bb8310f8":"# evaluate performance\nprint(\"Logistic Regression results:\\n\")\nprint(confusion_matrix(y_test, pred_logit))\nprint(classification_report(y_test,pred_logit))","50f44c0d":"# SVC\nfrom sklearn.svm import SVC\nsvc = SVC()\nsvc.fit(X_train, y_train)\npred_svc = svc.predict(X_test)","a6931e62":"# evaluate performance\nprint(\"SVC results:\\n\")\nprint(confusion_matrix(y_test, pred_svc))\nprint(classification_report(y_test,pred_svc))","afee2545":"# Imputing Missing Age Values: choose median due to outliers, which affect mean\ntest['Age'].fillna(test['Age'].median(),inplace=True)\n\n# Imputing Missing Embarked Values\ntest['Fare'].fillna(test['Fare'].median(), inplace=True)\n\n# Impute Embarked\ntest['Embarked'].fillna(test['Embarked'].value_counts().index[0], inplace=True)\n","36eb0c2f":"#Creating a dictionary to convert Passenger Class from 1,2,3 to 1st,2nd,3rd.\nd = {1:'1st',2:'2nd',3:'3rd'}\n\n#Mapping the column based on the dictionary\ntest['PClass'] = test['PClass'].map(d)\n\n# Getting Dummies of Categorical Variables\ncat_vars = test[['PClass','Sex','Embarked']]\ndummies = pd.get_dummies(cat_vars,drop_first=True)\n\n# Drop original cat_vars\ntest = test.drop(cat_vars,axis=1)\n# Concatenate dummies and train\ntest = pd.concat([test,dummies],axis=1)\n\nidx = test[['PassengerId']]\n","1b419707":"preds = model.predict(test)\nresults = idx.assign(Survived=preds)\nresults.to_csv('GBC_submission.csv',index=False)","40d69410":"* This gives us an accuracy of 80%","ba5879d7":"## Discrete Variables\nLet's start with some plots for discrete variables (categorical variables)","f0f0ff19":"# Step 2: EDA\n* Before I clean the dataset and prepare it for modelling, I want to do some initial Exploratory Data Analysis to get an idea of which factors are most indicative of survival\/fatality.","68d817aa":"* This gives a lower accuracy of 79%","b6166a5f":"## Logistic Regression\nNow let's try logistic regression.","fcdda58a":"# Predicting Titanic Survivors + EDA\nAt 2:20am on 15th April 1912, the British ocean liner Titanic sank into the North Atlantic Ocean about 400 miles south of Newfoundland, Canada. The massive ship, which carried 2,200 passengers and crew, had struck an iceberg two and half hours before.\n\nEverybody said it was 'unsinkable'; no-one predicted its tragic demise. For a moving depiction of the Titanic's sinking I recommend the 1959 movie [\"A Night to Remember\"](https:\/\/www.imdb.com\/title\/tt0051994\/).\n\nIn this notebook I explore the famous Titanic dataset, accessed through the [Kaggle page](https:\/\/www.kaggle.com\/c\/titanic).\nI use a range of visualisations and test several Machine Learning models to predict survivors. \n\n![alt text](https:\/\/www.gjenvick.com\/Images600\/Titanic\/Photos\/RMS-Titanic-1911-500.jpg)\n\nThe RMS Titanic\n\nSource: gjenvick.com","d252ba64":"# Step 1: Data Cleaning\n* Before I explore the data with some visualisations, I'll clean up the dataset, dealing with typos and dropping redundant variables.","71bb9b2d":"## Continuous Variables\nNow I will plot the remaining continuous variables ('Age' is not technically continuous, but can be treated as such for plots)","3361f7b9":"On the whole, we see that passengers who paid a higher fare are more likely to have survived.","eaf33f15":"The above 4 categories are redundant features for my purposes:\n* 'PassengerId', 'Cabin' and 'Ticket' are all arbitrary IDs that add no present insight;\n* However I will keep 'PassengerId' so that I can submit results of my model at the end of notebook\n* 'Name' does not tell me much that I can't derive from 'Sex' or 'Age'\n\nI will therefore drop these 4 columns.","4fae8f04":"It is evident that male passengers are far less likely to survive than females.","33d0705b":"## GradientBoostingClassifier\nFirst I'll try with a GBC","0da100f0":"## Random Forest\nNow I'll train a Random Forest model","bab4dc3c":"### Insights on Age\nSurprisingly, Age does not seem to contribute very much to survival.\n\nThe main conclusion is that passengers in their early 20s are least likely to survive.\n\n**The plot for 'Survived' is a bimodal distribution. This tells me that extra effort was made on board the Titanic to save the youngest passengers (between 0-5 years)**","a6cd27e9":"As we can see, passengers in 1st class are most likely to survive. Passengers in 3rd class are by far least likely to survive.","1efe7833":"# Step 3: Preprocessing\nNow that I've cleaned the data and performed EDA, I'll prepare the data for modelling by creating dummy variables and ensuring all data is numerical."}}