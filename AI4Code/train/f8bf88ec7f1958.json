{"cell_type":{"ed7a419d":"code","e59c373a":"code","d4ab462f":"code","5181bcf3":"code","1d9e9b7b":"code","c29ea133":"code","c564827b":"code","7d0932d1":"code","6c888c97":"code","0f4b9542":"code","6eef1d09":"code","c3aa98cf":"code","e86d2afc":"code","4ec80759":"code","3a20d246":"code","d8256a23":"code","5c99dff6":"code","aebb6638":"code","a4545416":"code","310d9a52":"code","f4fbb7a2":"code","9df00853":"code","32bb431b":"code","f6fea0fe":"code","b2de9aed":"code","c3adf8ae":"code","a1eafd80":"code","f454263f":"code","77fbfd04":"code","f1962188":"code","7d544e51":"code","8ea51b6d":"code","80edb69d":"code","1beaa7be":"code","75cccf13":"code","31ea01d9":"code","e3286c5c":"markdown","59d007c5":"markdown","17fac343":"markdown","06275016":"markdown","3b1562c7":"markdown","685fc78e":"markdown","633317e8":"markdown","754b1043":"markdown","7b6b8fe6":"markdown","9d7e8ae5":"markdown","50058e42":"markdown","9df2216f":"markdown","df1f932d":"markdown","eaf459c6":"markdown","52b98a56":"markdown","e3dacd9b":"markdown","292dabdc":"markdown","cb28f76d":"markdown","d97ceefb":"markdown","1914b20c":"markdown","982f69a2":"markdown","de17b272":"markdown"},"source":{"ed7a419d":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndiamonds = sns.load_dataset('diamonds')\n\nfrom sklearn.model_selection import train_test_split\nX = diamonds.drop(columns=['price'])\ny = diamonds['price']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.8, random_state=8)\nX_train.head()","e59c373a":"plt.subplots(figsize=(12, 8))\nsns.scatterplot(x=X_train['carat'], y=y_train)\nplt.show()","d4ab462f":"def my_dummy_model(carat): \n    return carat * 4000\n\ndummy_model_res = (\nX_train\n    .assign(\n        preds = lambda x: my_dummy_model(x['carat']), \n        price = y_train\n        )\n    .assign(\n        error = lambda x:  x['preds'] - x['price']\n    )\n)\ndummy_model_res","5181bcf3":"def plot_predictions(y_true, y_pred): \n    max_preds = min([max(y_pred.tolist()), max(y_true.tolist())])\n    min_preds = min([min(y_pred.tolist()), min(y_true.tolist())])\n    # plot\n    plt.figure(figsize=(8,8))\n    sns.scatterplot(x=y_pred, y=y_true)\n    sns.lineplot(x=[min_preds,max_preds], y=[min_preds, max_preds], color='red')\n    plt.ylabel('Reference')\n    plt.xlabel('Predictions')\n    plt.show()\nplot_predictions(dummy_model_res['price'], dummy_model_res['preds'])","1d9e9b7b":"np.array(X_train['carat']).reshape(-1, 1)","c29ea133":"from sklearn.linear_model import LinearRegression\n\nlm = LinearRegression()\nlm.fit(np.array(X_train['carat']).reshape(-1, 1), y_train)\nplot_predictions(y_train, lm.predict(np.array(X_train['carat']).reshape(-1, 1)))","c564827b":"print(\n    lm.coef_, \n    lm.intercept_\n    )","7d0932d1":"results = pd.DataFrame({\n    'predictions':lm.predict(np.array(X_test['carat']).reshape(-1, 1)), \n    'true_values':y_test\n})\nresults.head() ","6c888c97":"plot_predictions(results.true_values, results.predictions)","0f4b9542":"mse_pre = (results\n               .assign(diff = lambda x: (x.true_values - x.predictions)**2)\n               .agg({'diff':{'sum','count'}}))\nmse = mse_pre.loc['sum'] \/ mse_pre.loc['count']\nmse","6eef1d09":"# check our results with sklearn metrics\nmod_res = {}\nmod_res['1st_iter'] = {}\nfrom sklearn.metrics import mean_squared_error\nmod_res['1st_iter']['mse'] = mean_squared_error(results.true_values, results.predictions)\nmod_res['1st_iter']['mse']","c3aa98cf":"rmse = (mse_pre.loc['sum'] \/ mse_pre.loc['count'])**0.5\nrmse","e86d2afc":"# check our results with sklearn metrics\nmod_res['1st_iter']['rmse'] = mean_squared_error(results.true_values, results.predictions)**0.5\nmod_res['1st_iter']['rmse']","4ec80759":"mae_pre = (results\n               .assign(diff = lambda x: abs(x.true_values - x.predictions))\n               .agg({'diff':{'sum','count'}}))\nmod_res['1st_iter']['mae'] = mae_pre.loc['sum'] \/ mae_pre.loc['count']\nmod_res['1st_iter']['mae']","3a20d246":"# check our results with sklearn metrics\nfrom sklearn.metrics import mean_absolute_error\nmod_res['1st_iter']['mae'] = mean_absolute_error(results.true_values, results.predictions)\nmod_res['1st_iter']['mae']","d8256a23":"# Given that MAPE (mean absolute percentage error) doesn't exist in this version of sklearn, let's write a function:\ndef mean_abs_perc_error(y_true, y_pred):\n    results = pd.DataFrame({\n        'predictions':y_pred,\n        'true_values':y_true\n    })\n    mape_pre = (results\n               .assign(diff = lambda x: abs(x.true_values - x.predictions)\/x.true_values)\n               .agg({'diff':{'sum','count'}}))\n    mape = mape_pre.loc['sum'] \/ mape_pre.loc['count']\n    return mape.values[0]\n\n# from sklearn.metrics import mean_absolute_percentage_error\nmod_res['1st_iter']['mape'] = mean_abs_perc_error(results['true_values'], results['predictions'])\nmod_res['1st_iter']['mape']\nmod_res","5c99dff6":"def plot_predictions(y_true, y_pred): \n    print(\n        f\"\"\"\n        MSE: {mean_squared_error(y_true, y_pred)}\n        RMSE: {mean_squared_error(y_true, y_pred)**0.5}\n        MAE: {mean_absolute_error(y_true, y_pred)}\n        MAPE: {mean_abs_perc_error(y_true, y_pred)}\n        \"\"\"\n    )\n    max_preds = min([max(y_pred.tolist()), max(y_true.tolist())])\n    min_preds = max([min(y_pred.tolist()), min(y_true.tolist())])\n    print(max_preds, min_preds)\n    # plot\n    plt.figure(figsize=(8,8))\n    sns.scatterplot(x=y_pred, y=y_true)\n    sns.lineplot(x=[min_preds,max_preds], y=[min_preds, max_preds], color='red')\n    plt.ylabel('Reference')\n    plt.xlabel('Predictions')\n    plt.show()\n\n    return {\n        'mse': mean_squared_error(y_true, y_pred), \n        'rmse': mean_squared_error(y_true, y_pred)**0.5,\n        'mae': mean_absolute_error(y_true, y_pred),\n        'mape': mean_abs_perc_error(y_true, y_pred)\n    }\n\nmod_res['1st_iter'] = plot_predictions(y_test, results['predictions'])","aebb6638":"p_df = results.assign(error = lambda x: x['predictions'] - x['true_values'])\n\nplt.subplots(figsize=(12, 8))\nsns.histplot(p_df['error'])\nplt.vlines(x=0, ymin=0, ymax=600, color='red')\n# big problems with outliers\nplt.xlim(-5000, 5000)\nplt.show()\n# skewness to the right, my model tend to predict a higer price then it should be","a4545416":"# another way to visualise our errors\np_df = p_df.sort_values(by='true_values')\n\nplt.subplots(figsize=(12, 8))\nsns.scatterplot(data=p_df, x='true_values', y='error')\nplt.hlines(y=0, xmin=0, xmax=max(p_df['true_values']), color='red')\nplt.show()","310d9a52":"X_train.head()","f4fbb7a2":"from sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n\ncat_pipeline = Pipeline(steps=[\n    ('one_hot_enc', OneHotEncoder(drop='first'))\n])\nnum_pipeline = Pipeline(steps=[\n    ('scale', MinMaxScaler())\n])\n\ncat_cols = X_train.select_dtypes('category').columns.tolist()\nnum_cols = X_train.select_dtypes('float').columns.tolist()\nfull_processor = ColumnTransformer(transformers=[\n    ('number', num_pipeline, num_cols), \n    ('category', cat_pipeline, cat_cols)\n])\n\nlm_pipeline = Pipeline(steps=[\n    ('processor', full_processor), \n    ('model', LinearRegression())\n])\n\n_ = lm_pipeline.fit(X_train, y_train)\nlm_pipeline","9df00853":"def plot_predictions(y_true, y_pred): \n    print(\n        f\"\"\"\n        MSE: {mean_squared_error(y_true, y_pred)}\n        RMSE: {mean_squared_error(y_true, y_pred)**0.5}\n        MAE: {mean_absolute_error(y_true, y_pred)}\n        MAPE: {mean_abs_perc_error(y_true, y_pred)}\n        \"\"\"\n    )\n    max_preds = min([max(y_pred.tolist()), max(y_true.tolist())])\n    min_preds = max([min(y_pred.tolist()), min(y_true.tolist())])\n    print(max_preds, min_preds)\n    # plot\n    plt.figure(figsize=(8,8))\n    sns.scatterplot(x=y_pred, y=y_true)\n    sns.lineplot(x=[min_preds,max_preds], y=[min_preds, max_preds], color='red')\n    plt.ylabel('Reference')\n    plt.xlabel('Predictions')\n    plt.ticklabel_format(style='plain', axis='x')\n    plt.ticklabel_format(style='plain', axis='y')\n    plt.show()\n\n    errors = y_pred - y_true\n    plt.subplots(figsize=(12, 8))\n    sns.histplot(errors)\n    plt.vlines(x=0, ymin=0, ymax=600, color='red')\n    plt.ticklabel_format(style='plain', axis='x')\n    plt.ticklabel_format(style='plain', axis='y')\n    plt.show()\n\n    p_df = (\n        pd.DataFrame({'y_true':y_true, 'y_pred':y_pred})\n        .assign(error = lambda x: x['y_pred'] - x['y_true'])\n        .sort_values(by='y_true')\n        )\n\n    plt.subplots(figsize=(12, 8))\n    sns.scatterplot(data=p_df, x='y_true', y='error')\n    plt.hlines(y=0, xmin=0, xmax=max(p_df['y_true']), color='red')\n    plt.ticklabel_format(style='plain', axis='x')\n    plt.ticklabel_format(style='plain', axis='y')\n    plt.show()\n\n    return {\n        'mse': mean_squared_error(y_true, y_pred), \n        'rmse': mean_squared_error(y_true, y_pred)**0.5,\n        'mae': mean_absolute_error(y_true, y_pred),\n        'mape': mean_abs_perc_error(y_true, y_pred)\n\n    }\n\n    \n\n# plot_predictions(y_test, results['predictions'])\n\nmod_res['2nd_iter'] = plot_predictions(y_train, lm_pipeline.predict(X_train))","32bb431b":"sns.histplot(y_train)\nplt.title('Price distribution')\nplt.show()","f6fea0fe":"sns.histplot(np.log(y_train))\nplt.title('Price distribution')\nplt.show()","b2de9aed":"lm_pipeline.fit(X_train, np.log(y_train))\nmod_res['3rd_iter'] = plot_predictions(y_train, np.exp(lm_pipeline.predict(X_train)))","c3adf8ae":"sns.scatterplot(data=X_train, x=\"carat\", y=\"x\")\nplt.show()","a1eafd80":"import numpy as np\n\nX_train_copy = X_train.copy()\n\nX_train_copy['corrn'] = (\n    (X_train_copy['carat'] - np.mean(X_train_copy['carat'])) * \n    (X_train_copy['x'] - np.mean(X_train_copy['x']))\n)\nX_train_copy.head()\n\nX_train_copy['corr1'] = (X_train_copy['carat'] - np.mean(X_train_copy['carat']))**2\nX_train_copy.head()\n\nX_train_copy['corr2'] = (X_train_copy['x'] - np.mean(X_train_copy['x']))**2\nX_train_copy.head()\n\ncorrn = sum(X_train_copy['corrn'])\/np.sqrt(sum(X_train_copy['corr1']) * sum(X_train_copy['corr2']))\ncorrn","f454263f":"# 1. function that given two columns, it returns the corr. coeficient\ndef corr_coeff(df, var1, var2): \n    temp = df.copy()\n    temp['corrn'] = (\n        (temp[var1] - np.mean(temp[var1])) * \n        (temp[var2] - np.mean(temp[var2]))\n    )\n    temp['corr1'] = (temp[var1] - np.mean(temp[var1]))**2\n    temp['corr2'] = (temp[var2] - np.mean(temp[var2]))**2\n    corrn = sum(temp['corrn'])\/np.sqrt(sum(temp['corr1']) * sum(temp['corr2']))\n    return corrn\n\n\n# 2. Calculate the corr_coeficients for all the columns\ntemp = pd.concat([X_train, y_train], axis=1)\n\nprint(corr_coeff(temp, 'carat', 'z'))\nprint(corr_coeff(temp, 'price', 'x'))\nprint(corr_coeff(temp, 'price', 'carat'))\n\n","77fbfd04":"num_cols = X_train.select_dtypes('float').columns\nresults = []\nfor col in num_cols:\n    res2 = {}\n    for col2 in num_cols:\n        r = corr_coeff(X_train, col, col2)\n        print(f'r between {col} and {col2} is {r}')\n        res2[col2] = r\n    results.append(res2)\ndfr = pd.DataFrame(pd.json_normalize(results))\ndfr.index = num_cols\n\ndfr.head()","f1962188":"train = pd.concat([X_train, y_train], axis=1)\nplt.subplots(figsize=(12, 9))\nsns.heatmap(train.corr(), vmin=-1, vmax=1, cmap='RdGy')\nplt.show()","7d544e51":"g = sns.PairGrid(train)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\ng.add_legend()","8ea51b6d":"X = diamonds.drop(columns=['price'])\ny = diamonds['price']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.8, random_state=8)\n\nX_train.head()\n\nfrom sklearn.preprocessing import OrdinalEncoder\ncut = ['Fair', 'Good', 'Very Good', 'Premium', 'Ideal']\ncolor = ['D','E','F','G','H','I','J']\nclarity = ['I1', 'SI2', 'SI1', 'VS2', 'VS1', 'VVS2', 'VVS1', 'IF']\n\ncat_pipeline = Pipeline(steps=[\n    ('ord_enc', OrdinalEncoder(categories=[cut, color, clarity]))\n])\nnum_pipeline = Pipeline(steps=[\n    ('scale', MinMaxScaler())\n])\n\ncat_cols = X_train.select_dtypes('category').columns.tolist()\nnum_cols = X_train.select_dtypes('float').columns.tolist()\nencoding = ColumnTransformer(transformers=[\n    ('category', cat_pipeline, cat_cols),\n    ('number', \"passthrough\", num_cols) \n])\n\nlm_pipeline = Pipeline(steps=[\n    ('encoding', encoding), \n    ('scale', MinMaxScaler()),\n    ('model', LinearRegression())\n])\n\n_ = lm_pipeline.fit(X_train, np.log(y_train))","80edb69d":"y_preds = np.exp(lm_pipeline.predict(X_test))\nmod_res['4th_iter'] = plot_predictions(y_test, y_preds)","1beaa7be":"plt.subplots(figsize=(12, 8))\nsns.scatterplot(x=X_train['carat'], y=y_train)\nplt.axvline(x = 2.5, ymin=0, ymax=18000, c='red')\nplt.show()","75cccf13":"X_filt = (\npd.concat([X_train, y_train], axis=1)\n    .query(\"carat < 2\")\n)\nX_train_filt = X_filt.drop(columns='price')\ny_train_filt = X_filt['price']\n_ = lm_pipeline.fit(X_train_filt, np.log(y_train_filt))\ny_pred = np.exp(lm_pipeline.predict(X_test))\n\nmod_res['5th_iter'] = plot_predictions(y_test, y_pred)","31ea01d9":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nimport time\n\nfor model in [LinearRegression(), RandomForestRegressor(), KNeighborsRegressor(), SVR()]: \n    print('-'*20)\n    print(model)\n    start_time = time.time()\n    lm_pipeline = Pipeline(steps=[\n        ('encoding', encoding), \n        ('scale', MinMaxScaler()),\n        ('model', model)\n    ])\n    _ = lm_pipeline.fit(X_train, np.log(y_train))\n    y_pred = np.exp(lm_pipeline.predict(X_test))\n    _ = plot_predictions(y_test, y_pred)\n    print(\"--- %s seconds ---\" % (time.time() - start_time), sep='\/n')\n\n    ","e3286c5c":"* Root Mean Squared Error (RMSE): \n$$RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - y_{e})^{2}}$$","59d007c5":"* Mean Absolute Error (MAE)\n$$MAE = \\frac{1}{n}\\sum_{i=1}^{n} |y_{i} - y_{e}|$$","17fac343":"### Creating our first Linear Model","06275016":"## 3rd iteration: Logirithm tranformation","3b1562c7":"### Dummy model","685fc78e":"Have this errors a normal distribution?","633317e8":"## 7th iteration: model selection","754b1043":"* Mean Absolute Percentage Error (MAPE)\n\n$$MAPE = \\frac{1}{n}\\sum_{i=1}^{n} |\\frac{y_{i} - y_{e}}{y_{i}}|$$","7b6b8fe6":"## 6th iteration: outliers","9d7e8ae5":"Based on the graph below I am going to create a real simple model:\n\n$$\nprice = carat*4000\n$$","50058e42":"## 5th iteration: ordinal encoder","9df2216f":"## 4th iteration: exploring correlation","df1f932d":"Linear model: \n\n$$\nprice = -2254.5185037319566 + carat * 7752.35768657\n$$","eaf459c6":"We can also detect that our model tend to predict worse as the diamonds get more expensive. Maybe we are missing some kind of qualities there. ","52b98a56":"## 2nd iteration: Sklearn pipeline + all features","e3dacd9b":"## 1st iteration: dummy model and linear regression + regression errors","292dabdc":"# Supervised Machine Learning: Regression","cb28f76d":"\n### Correlation Coeficient (Pearson)\n![](..\/docs\/pearson-coeficient.png)\n\n### Examples\n\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/8\/83\/Pearson_Correlation_Coefficient_and_associated_scatterplots.png)\n\nMore information on wikipedia [link](https:\/\/en.wikipedia.org\/wiki\/Correlation)","d97ceefb":"### Results on test set","1914b20c":"### Problems \n\n1. [Correlation do not mean causality!](https:\/\/www.tylervigen.com\/spurious-correlations)\n2. Correlation only shows as a linear relation!\n\n![](https:\/\/www.researchgate.net\/profile\/Graeme-Hickey\/publication\/276360680\/figure\/fig1\/AS:324886896627716@1454470480110\/Scatterplots-of-four-different-datasets-known-as-Anscombes-quartet-99-Each-dataset.png)","982f69a2":"* Mean Squared Error (MSE): is calculated as the mean or average of the squared differences between predicted and expected target values.\n$$MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - y_{e})^{2}$$","de17b272":"* price: price in US dollars (\\$326\u2013\\$18,823)\n* carat: weight of the diamond (0.2\u20135.01)\n* cut: quality of the cut (Fair, Good, Very Good, Premium, Ideal)\n* color: diamond colour, from D (best) to J (worst)\n* clarity: a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))\n* x: length in mm (0\u201310.74)\n* y: width in mm (0\u201358.9)\n* z: depth in mm (0\u201331.8)\n* depth: total depth percentage = z \/ mean(x, y) = 2 * z \/ (x + y) (43\u201379)\n* table: width of top of diamond relative to widest point (43\u201395)"}}