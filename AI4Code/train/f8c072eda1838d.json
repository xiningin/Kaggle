{"cell_type":{"b12c40b3":"code","0a047f3c":"code","3713bfba":"code","54a9b0e2":"code","557a20bf":"code","7bd4c2bf":"code","e3e52989":"code","74b87acf":"code","2d390c0e":"code","7ad752a8":"code","83b5f21f":"code","b2a5ac6d":"code","9a614a4a":"code","b76b0511":"code","daae6632":"code","20315465":"code","84740bf7":"code","528f4981":"code","d50a905b":"code","b586e784":"code","c4febbfd":"code","31f8cba2":"code","f2be24b3":"code","4b01ec7f":"code","97cfab5f":"code","335ab77a":"code","2cc572d0":"code","60d914bb":"markdown","b44cbc3f":"markdown","2e32b9bf":"markdown","8ffe2546":"markdown","6123c88b":"markdown","36c96a25":"markdown","4fa08038":"markdown","0786edc6":"markdown","82e08588":"markdown","c7f19ab1":"markdown","b1e2ff3b":"markdown","134f113b":"markdown","7c40a6cc":"markdown","15015ab9":"markdown","6cd2ddda":"markdown","949f00b0":"markdown","f4e4dbdc":"markdown","f6c81c84":"markdown","9cfdf423":"markdown","710d54de":"markdown","41660af5":"markdown","864bca36":"markdown","70acd8f9":"markdown","c6703302":"markdown"},"source":{"b12c40b3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt;\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport warnings;\nwarnings.simplefilter(\"ignore\");\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0a047f3c":"data = pd.read_csv(\"..\/input\/eksik-veriler\/eksikveriler.csv\");\nnumFeatures = data.iloc[:, 1:4];\ncountries = data.iloc[:, 0];\nsex = data.iloc[:, 4];\ndata.head()","3713bfba":"from sklearn.impute import SimpleImputer;\n\nimputer = SimpleImputer(missing_values = np.nan, strategy = \"mean\"); \n# SimpleImputer fills the NaN values according to a stretegy. \n# Strategy may be mean, median, most_frequent or constant\n\nfor colCounter in range(0,3):\n    currentCol = numFeatures.iloc[:, colCounter].values.reshape(-1, 1);\n    imputer = imputer.fit(currentCol);\n    currentCol = imputer.transform(currentCol);\n    numFeatures.iloc[:, colCounter] = currentCol;\n    \nnumFeatures.head()","54a9b0e2":"from sklearn import preprocessing;\n\ncountries = countries.values.reshape(-1, 1);\n\nencoderType = \"oneHot\";\n\nif (encoderType == \"label\"):\n    encoder = preprocessing.LabelEncoder();\nelif (encoderType == \"oneHot\"):\n    encoder = preprocessing.OneHotEncoder();\nelse:\n    print(\"Undefined Encoder Type!\");\n\ncountries = encoder.fit_transform(countries);\n\nif (encoderType == \"oneHot\"):\n    countries = countries.toarray();\n\nprint(countries)","557a20bf":"country = pd.DataFrame(data = countries, index = range(22), columns = [\"fr\", \"tr\", \"us\"]);\nnumericFeatures = pd.DataFrame(data = numFeatures.values, index = range(22), columns = [\"Length\", \"Weight\", \"Age\"]);\nsex = pd.DataFrame(data = sex.values, index = range(22), columns = ['sex']);\n\ndata = pd.concat([country, numericFeatures, sex], axis = 1);\nprint(data)","7bd4c2bf":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(data.iloc[:, 0:-1], data.iloc[:, -1], test_size = 0.33, random_state = 42);\n","e3e52989":"from sklearn.preprocessing import StandardScaler\n\nstandardScaler = StandardScaler();\n\nxTrain = standardScaler.fit_transform(x_train);\nxTest = standardScaler.fit_transform(x_test);","74b87acf":"data = pd.read_csv(\"..\/input\/linear-regresyon\/satislar.csv\");\nmonths = data[\"Aylar\"];\nsales = data[\"Satislar\"];\n\nfrom sklearn.model_selection import train_test_split;\nxTrain, xTest, yTrain, yTest = train_test_split(months, sales, test_size = 0.33, random_state = 42);\n\n\nfrom sklearn.preprocessing import StandardScaler;\nsc = StandardScaler();\nxTrain = sc.fit_transform(xTrain.values.reshape(-1, 1));\nyTrain = sc.fit_transform(yTrain.values.reshape(-1, 1));\nxTest = sc.fit_transform(xTest.values.reshape(-1, 1));\nyTest = sc.fit_transform(yTest.values.reshape(-1, 1));\n\n\"\"\"\nxTrain = xTrain.values.reshape(-1, 1);\nyTrain = yTrain.values.reshape(-1, 1);\nxTest = xTest.values.reshape(-1, 1);\nyTest = yTest.values.reshape(-1, 1);\n\"\"\"\n\n# Lineer Regression\n\nfrom sklearn.linear_model import LinearRegression;\nlr = LinearRegression();\nlr.fit(xTrain, yTrain);\n\nyHead = lr.predict(xTest);\nplt.plot(yHead, color = \"red\", label = \"Prediction Result\");\nplt.plot(yTest, color = \"blue\", label = \"True Data\");\nplt.xlabel(\"Index\");\nplt.ylabel(\"Standardized yHead\");\nplt.legend();","2d390c0e":"data = pd.read_csv(\"..\/input\/veriler\/veriler.csv\");\n\n# Encoding\nfrom sklearn import preprocessing;\n\nsex = data.iloc[:, -1].values.reshape(-1, 1);\ncountries = data.iloc[:, 0].values.reshape(-1, 1);\n\nencoderType = \"oneHot\";\n\nif (encoderType == \"label\"):\n    encoder = preprocessing.LabelEncoder();\nelif (encoderType == \"oneHot\"):\n    encoder = preprocessing.OneHotEncoder();\nelse:\n    print(\"Undefined Encoder Type!\");\n\nsex = encoder.fit_transform(sex);\ncountries = encoder.fit_transform(countries);\n\nif (encoderType == \"oneHot\"):\n    sex = sex.toarray();\n    countries = countries.toarray();\n\ncolSet1 = pd.DataFrame(data = countries, index = range(22), columns = [\"fr\", \"tr\", \"us\"]);\ncolSet2 = pd.DataFrame(data = data.iloc[:, 1:4], index = range(22), columns = [\"boy\", \"kilo\", \"yas\"]);\ncolSet3 = pd.DataFrame(data = sex[:, 0], index = range(22), columns = [\"sex\"]);\n\ndata = pd.concat([colSet1, colSet2, colSet3], axis = 1);\n\n# Split the data\n\nfrom sklearn.model_selection import train_test_split\n\nxDataSag = data.iloc[:, 0:3];\nxDataSol = data.iloc[:, 4:];\nxData = pd.concat([xDataSag, xDataSol], axis = 1);\nyData = data.iloc[:, 3];\n\nxTrain, xTest, yTrain, yTest = train_test_split(xData, yData, test_size = 0.33, random_state = 42);\n\n# Scale the data\nfrom sklearn.preprocessing import StandardScaler;\nsc = StandardScaler();\n\nxTrain = sc.fit_transform(xTrain);\nyTrain = sc.fit_transform(yTrain.values.reshape(-1, 1));\nxTest = sc.fit_transform(xTest);\nyTest = sc.fit_transform(yTest.values.reshape(-1, 1));\n\n# Generate Multi-Variable Model\nfrom sklearn.linear_model import LinearRegression;\nregressor = LinearRegression();\n\nregressor.fit(xTrain, yTrain);\nyHead = regressor.predict(xTest);\n\nplt.plot(range(len(yTest)), yTest, color = \"red\");\nplt.plot(range(len(yTest)), yHead, color = \"blue\");\n\nimport statsmodels.api as sm;\nxData = np.append(arr = np.ones((22, 1)).astype(int), values = xData, axis = 1);\n\nxData_1 = xData[:, [0, 1, 2, 3, 4, 5]];\nxData_1 = np.array(xData_1, dtype = float);\nmodel = sm.OLS(yData, xData_1).fit();\nprint(model.summary());\n\nxData_1 = xData[:, [0, 1, 2, 3, 4]];\nxData_1 = np.array(xData_1, dtype = float);\nmodel = sm.OLS(yData, xData_1).fit();\nprint(model.summary());","7ad752a8":"data = pd.read_csv(\"..\/input\/tenisdata\/odev_tenis.csv\");\n\n# Encoding\nfrom sklearn import preprocessing;\n\noutlook = data.iloc[:, 0].values.reshape(-1, 1);\nwindy = data.iloc[:, 3].values.reshape(-1, 1);\nplay = data.iloc[:, 4].values.reshape(-1, 1);\n\nplay = preprocessing.LabelEncoder().fit_transform(play);\nwindy = preprocessing.LabelEncoder().fit_transform(windy);\noutlook = preprocessing.OneHotEncoder().fit_transform(outlook).toarray();\n\ncolSet1 = pd.DataFrame(data = outlook, index = range(len(windy)), columns = [\"overcast\", \"rainy\", \"sunny\"]);\ncolSet2 = pd.DataFrame(data = data.iloc[:, 1:3], index = range(len(windy)), columns = [\"temperature\", \"humidity\"]);\ncolSet3 = pd.DataFrame(data = windy, index = range(len(windy)), columns = [\"windy\"]);\ncolSet4 = pd.DataFrame(data = play, index = range(len(windy)), columns = [\"play\"]);\n\ndata = pd.concat([colSet3, colSet4, colSet1, colSet2], axis = 1);\n\n# Split the data\n\nfrom sklearn.model_selection import train_test_split\n\nxData = data.iloc[:, :6];\nyData = data.iloc[:, 6:];\n\nxTrain, xTest, yTrain, yTest = train_test_split(xData, yData, test_size = 0.33, random_state = 0);\n\nprint(xTrain)\n\n# Scale the data\n\"\"\"\nfrom sklearn.preprocessing import StandardScaler;\nsc = StandardScaler();\n\nxTrain = sc.fit_transform(xTrain);\nyTrain = sc.fit_transform(yTrain.values.reshape(-1, 1));\nxTest = sc.fit_transform(xTest);\nyTest = sc.fit_transform(yTest.values.reshape(-1, 1));\n\"\"\"\n\n# Generate Multi-Variable Model\nfrom sklearn.linear_model import LinearRegression;\nregressor = LinearRegression();\n\nregressor.fit(xTrain, yTrain);\nyHead = regressor.predict(xTest);\n\nplt.plot(range(len(yTest)), yTest, color = \"red\");\nplt.plot(range(len(yTest)), yHead, color = \"blue\");\nplt.show();\n\nimport statsmodels.api as sm;\nxData = np.append(arr = np.ones((len(windy), 1)).astype(int), values = xData, axis = 1);\n\n# Evaluate for backward elimination\nxData_1 = xData[:, [0, 1, 2, 3, 4, 5, 6]];\nxData_1 = np.array(xData_1, dtype = float);\nmodel = sm.OLS(yData, xData_1).fit();\nprint(model.summary());\n\n#Retrain\nxTrain.drop([\"windy\"], axis = 1, inplace = True);\nxTest.drop([\"windy\"], axis = 1, inplace = True);\nregressor.fit(xTrain, yTrain);\nyHead = regressor.predict(xTest);\n\nplt.plot(range(len(yTest)), yTest, color = \"red\");\nplt.plot(range(len(yTest)), yHead, color = \"blue\");\nplt.show();\n\n# Evaluate for backward elimination\nxData_1 = xData[:, [0, 2, 3, 4, 5, 6]];\nxData_1 = np.array(xData_1, dtype = float);\nmodel = sm.OLS(yData, xData_1).fit();\nprint(model.summary());\n\n#Retrain\nxTrain.drop([\"sunny\"], axis = 1, inplace = True);\nxTest.drop([\"sunny\"], axis = 1, inplace = True);\nregressor.fit(xTrain, yTrain);\nyHead = regressor.predict(xTest);\n\nplt.plot(range(len(yTest)), yTest, color = \"red\");\nplt.plot(range(len(yTest)), yHead, color = \"blue\");\n\n# Evaluate for backward elimination\nxData_1 = xData[:, [0, 2, 3, 4, 6]];\nxData_1 = np.array(xData_1, dtype = float);\nmodel = sm.OLS(yData, xData_1).fit();\nprint(model.summary());\n","83b5f21f":"data = pd.read_csv(\"..\/input\/salaries\/maaslar.csv\");\n\nxData = data.iloc[:, 1:2];\nyData = data.iloc[:, 2:];\n\n# Linear Regreesion Result\nfrom sklearn.linear_model import LinearRegression;\nlr = LinearRegression();\nlr.fit(xData, yData);\n\n# R2 Score\nfrom sklearn.metrics import r2_score;\nprint(\"Linear R2: \", r2_score(yData, lr.predict(xData)));\n\nplt.scatter(xData, yData, label = \"Original Data\");\nplt.plot(xData, lr.predict(xData), color = \"green\", label = \"Linear Regression Result\");\n\n# Polynomial Regression\nfrom sklearn.preprocessing import PolynomialFeatures;\ndegreeVal = 2;\npr = PolynomialFeatures(degree = degreeVal);\nxPoly = pr.fit_transform(xData);\nlr2 = LinearRegression();\nlr2.fit(xPoly, yData);\n\nplt.plot(xData, lr2.predict(xPoly), color = \"red\", label = \"2. Degree Polynomial Regression Result\");\n\n# R2 Score\nprint(\"Degree = 4 R2: \", r2_score(yData, lr2.predict(xPoly)));\n\ndegreeVal = 4;\npr = PolynomialFeatures(degree = degreeVal);\nxPoly = pr.fit_transform(xData);\nlr3 = LinearRegression();\nlr3.fit(xPoly, yData);\n\n# R2 Score\nprint(\"Degree = 4 R2: \", r2_score(yData, lr3.predict(xPoly)));\n\nplt.plot(xData, lr3.predict(xPoly), color = \"black\", label = \"4. Degree Polynomial Regression Result\");\nplt.title(\"Linear vs Polynomial Regression\");\nplt.legend();\nplt.show();","b2a5ac6d":"data = pd.read_csv(\"..\/input\/salaries\/maaslar.csv\");\nxData = data.iloc[:, 1:2];\nyData = data.iloc[:, 2:];\n\n# SVR is sensitive to outliers, therefore we must use scaling\nfrom sklearn.preprocessing import StandardScaler;\nsc1 = StandardScaler();\nxData = sc1.fit_transform(xData);\nsc2 = StandardScaler();\nyData = sc2.fit_transform(yData);\n\nfrom sklearn.svm import SVR;\nsvr = SVR(kernel = \"rbf\");\nsvr.fit(xData, yData);\n\norgX = sc1.inverse_transform(xData);\norgY = sc2.inverse_transform(yData);\n\nplt.scatter(orgX, orgY, color = \"red\");\nplt.plot(orgX, sc2.inverse_transform(svr.predict(xData)));\nplt.xlabel(\"Level\");\nplt.ylabel(\"Salary\");\nplt.show();\n\n# R2 Score\nfrom sklearn.metrics import r2_score;\nprint(\"SVR R2: \", r2_score(orgY, sc2.inverse_transform(svr.predict(xData))));\n","9a614a4a":"data = pd.read_csv(\"..\/input\/salaries\/maaslar.csv\");\nxData = data.iloc[:, 1:2];\nyData = data.iloc[:, 2:];\n\nfrom sklearn.tree import DecisionTreeRegressor;\ndtr = DecisionTreeRegressor(random_state = 0);\ndtr.fit(xData, yData);\n\nplt.scatter(xData, yData, color = \"red\");\nplt.plot(xData, dtr.predict(xData));\nplt.xlabel(\"Level\");\nplt.ylabel(\"Salary\");\nplt.show();\n\n# R2 Score\nfrom sklearn.metrics import r2_score;\nprint(\"Decision Tree R2: \", r2_score(yData, dtr.predict(xData)));","b76b0511":"data = pd.read_csv(\"..\/input\/salaries\/maaslar.csv\");\nxData = data.iloc[:, 1:2];\nyData = data.iloc[:, 2:];\n\nfrom sklearn.ensemble import RandomForestRegressor;\nrfr = RandomForestRegressor(n_estimators = 10, random_state = 0);\n\nrfr.fit(xData, yData);\n\nplt.scatter(xData, yData, color = \"red\");\nplt.plot(xData, rfr.predict(xData));\nplt.xlabel(\"Level\");\nplt.ylabel(\"Salary\");\nplt.show();\n\n# R2 Score\nfrom sklearn.metrics import r2_score;\nprint(\"Decision Tree R2: \", r2_score(yData, rfr.predict(xData)));","daae6632":"# Load the libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt;\n\n# Load the data\ndata = pd.read_csv(\"..\/input\/newsalaries\/maaslar_yeni.csv\")\ndata.head()","20315465":"# Control the missing data\nprint(data.isnull().mean())","84740bf7":"data.info()","528f4981":"# Let's convert the categorical features into numeric features by using one-hot labeling\n\ntitle = data.loc[:, \"unvan\"];\nfrom sklearn import preprocessing;\n\ntitle = title.values.reshape(-1, 1);\nencoder = preprocessing.OneHotEncoder();\ntitle = encoder.fit_transform(title);\ntitle = title.toarray();\ntitle = pd.DataFrame(data = title, index = range(len(data)), columns = ['Cayci', 'Sekreter', 'Uzman Yardimcisi', 'Uzman', 'Proje Yoneticisi', 'Sef', 'Mudur', 'Direktor', 'C-level', 'CEO']);","d50a905b":"# Concatenate the data\ndata = pd.concat([data.iloc[:, 2:]], axis = 1);","b586e784":"data.head()","c4febbfd":"# Split the data\nfrom sklearn.model_selection import train_test_split\n\nprint(data.corr());\n\nxData = data.iloc[:, :1];\nyData = data.iloc[:, 3:];\n\nxTrain, xTest, yTrain, yTest = train_test_split(xData, yData, test_size = 0.33, random_state = 32);","31f8cba2":"# Scale the data\nfrom sklearn.preprocessing import StandardScaler;\nsc1 = StandardScaler();\nsc2 = StandardScaler();\n\nxTrain = pd.DataFrame(data = sc1.fit_transform(xTrain), index = range(len(yTrain)), columns = ['UnvanSeviyesi']);\nyTrain = pd.DataFrame(data = sc2.fit_transform(yTrain.values.reshape(-1, 1)), index = range(len(yTrain)), columns = ['maas']);\nxTest = pd.DataFrame(data = sc1.fit_transform(xTest), index = range(len(yTest)), columns = ['UnvanSeviyesi']);\nyTest = pd.DataFrame(data = sc2.fit_transform(yTest.values.reshape(-1, 1)), index = range(len(yTest)), columns = ['maas']);","f2be24b3":"from sklearn.linear_model import LinearRegression;\nlr = LinearRegression();\nlr.fit(xTrain, yTrain);\n\nyHead = lr.predict(xTest);\nplt.plot(range(len(yHead)), yHead, color = \"red\", label = \"Prediction Result\");\nplt.plot(range(len(yTest)), yTest, color = \"blue\", label = \"True Data\");\nplt.xlabel(\"Index\");\nplt.ylabel(\"yHead\");\nplt.legend();\nplt.show();\n\n# Apply backward elimination\n\nimport statsmodels.api as sm;\n# Evaluate for backward elimination\nmodel = sm.OLS(lr.predict(xTrain), xTrain).fit();\nprint(model.summary());\n\n# R2 Score\nfrom sklearn.metrics import r2_score;\nprint(\"Linear R2: \", r2_score(yTest, lr.predict(xTest)));","4b01ec7f":"from sklearn.preprocessing import PolynomialFeatures;\ndegreeVal = 5;\npr = PolynomialFeatures(degree = degreeVal);\nxPoly = pr.fit_transform(xTrain);\nlr2 = LinearRegression();\nlr2.fit(xPoly, yTrain);\n\nxPoly = pr.fit_transform(xTest);\nyHead = lr2.predict(xPoly);\nplt.plot(range(len(yHead)), yHead, color = \"red\", label = \"Prediction Result\");\nplt.plot(range(len(yTest)), yTest, color = \"blue\", label = \"True Data\");\nplt.xlabel(\"Index\");\nplt.ylabel(\"yHead\");\nplt.legend();\nplt.show();\n\n# Apply backward elimination\n\nimport statsmodels.api as sm;\n# Evaluate for backward elimination\nmodel = sm.OLS(lr2.predict(xPoly), xPoly).fit();\nprint(model.summary());\n\n# R2 Score\nfrom sklearn.metrics import r2_score;\nprint(\"Polynomial R2: \", r2_score(yTest, lr2.predict(xPoly)));","97cfab5f":"# SVR is sensitive to outliers, therefore we must use scaling\nfrom sklearn.svm import SVR;\nsvr = SVR(kernel = \"rbf\");\nsvr.fit(xTrain, yTrain);\n\nyHead = svr.predict(xTest);\nplt.plot(range(len(yHead)), sc2.inverse_transform(yHead), color = \"red\", label = \"Prediction Result\");\nplt.plot(range(len(yTest)), sc2.inverse_transform(yTest), color = \"blue\", label = \"True Data\");\nplt.xlabel(\"Index\");\nplt.ylabel(\"yHead\");\nplt.legend();\nplt.show();\n\n# Apply backward elimination\n\nimport statsmodels.api as sm;\n# Evaluate for backward elimination\nmodel = sm.OLS(svr.predict(xTest), xTest).fit();\nprint(model.summary());\n\n# R2 Score\nfrom sklearn.metrics import r2_score;\nprint(\"SVR R2: \", r2_score(yTest, svr.predict(xTest)));","335ab77a":"from sklearn.tree import DecisionTreeRegressor;\ndtr = DecisionTreeRegressor(random_state = 0);\ndtr.fit(xTrain, yTrain);\n\nyHead = dtr.predict(xTest);\nplt.plot(range(len(yHead)), yHead, color = \"red\", label = \"Prediction Result\");\nplt.plot(range(len(yTest)), yTest, color = \"blue\", label = \"True Data\");\nplt.xlabel(\"Index\");\nplt.ylabel(\"yHead\");\nplt.legend();\nplt.show();\n\n# Apply backward elimination\n\nimport statsmodels.api as sm;\n# Evaluate for backward elimination\nmodel = sm.OLS(dtr.predict(xTrain), xTrain).fit();\nprint(model.summary());\n\n# R2 Score\nfrom sklearn.metrics import r2_score;\nprint(\"DTR R2: \", r2_score(yTest, dtr.predict(xTest)));","2cc572d0":"from sklearn.ensemble import RandomForestRegressor;\nrfr = RandomForestRegressor(n_estimators = 10, random_state = 0);\n\nrfr.fit(xTrain, yTrain);\n\nyHead = rfr.predict(xTest);\nplt.plot(range(len(yHead)), yHead, color = \"red\", label = \"Prediction Result\");\nplt.plot(range(len(yTest)), yTest, color = \"blue\", label = \"True Data\");\nplt.xlabel(\"Index\");\nplt.ylabel(\"yHead\");\nplt.legend();\nplt.show();\n\n# Apply backward elimination\n\nimport statsmodels.api as sm;\n# Evaluate for backward elimination\nmodel = sm.OLS(rfr.predict(xTrain), xTrain).fit();\nprint(model.summary());\n\n# R2 Score\nfrom sklearn.metrics import r2_score;\nprint(\"DTR R2: \", r2_score(yTest, rfr.predict(xTest)));","60d914bb":"# Random Forest Regression","b44cbc3f":"# Polynomial Regression","2e32b9bf":"## Data 1","8ffe2546":"## Data 2","6123c88b":"# Liner Regression","36c96a25":"# Decision Tree Regression","4fa08038":"## Split the Data into Train and Test Sets","0786edc6":"**Simple Linear Regression**","82e08588":"**Random Forest Regression**","c7f19ab1":"All of the features are int except unvan, it's a categorical parameter.","b1e2ff3b":"# Loading of Data","134f113b":"We generated \"title\" columns but won't use because it's a dummy variable. Additionally we won't take \"Calisan ID\"column, because it's useless.","7c40a6cc":"# Multi-Variable Linear Regression","15015ab9":"**Support Vector Regression**","6cd2ddda":"# Support Vector Regression","949f00b0":"# Preprocessing of Data","f4e4dbdc":"**Decision Tree Regression**","f6c81c84":"## Convert Categoric Features to Numeric","9cfdf423":"## Concatenate the Data","710d54de":"## Scale the Train and Test Data","41660af5":"## Missing Values","864bca36":"**Polynomial Regression**","70acd8f9":"There is no missing value in data.","c6703302":"# Homework #2"}}