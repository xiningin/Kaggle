{"cell_type":{"23d221ea":"code","63cf14ff":"code","13aa32f8":"code","dad2c747":"code","c358a5c4":"code","de28286e":"code","ab9f4014":"code","f3b48d92":"code","0910bc35":"code","8f44b015":"code","1c7b5b5e":"code","c9646675":"code","36d8a5ff":"code","c8ac1dd0":"code","0ca06437":"code","341d8685":"code","60482d7f":"code","c2e5c1f7":"code","0bbb1dba":"code","2da37afa":"code","f62e4e9c":"code","291aa776":"code","d994016c":"code","5b4ef06a":"code","a9a92fd7":"code","ca89a8f8":"code","0de0ae0c":"code","5502363f":"code","2ec227d6":"code","2cccc429":"code","6ed524c2":"code","394fa95d":"code","0ad5ed4a":"code","3456afa3":"code","54fa3020":"code","a6a2f757":"code","c58a4b26":"code","a2db5fdf":"code","2e883fca":"code","ac24818d":"markdown","ff88ab4f":"markdown","3058ca8b":"markdown","0a8f9c86":"markdown","14ff96d3":"markdown","298a883b":"markdown","ea1066d4":"markdown","2da49802":"markdown","0c397288":"markdown","e80c912a":"markdown","739b0d95":"markdown","52ff8275":"markdown","5108633e":"markdown","eb0612cd":"markdown"},"source":{"23d221ea":"import tensorflow\nfrom PIL import Image\nimport glob\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.preprocessing.image import load_img, save_img, img_to_array\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.layers import AveragePooling2D\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam,SGD,RMSprop,Adamax\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam,Adadelta,RMSprop,Adagrad\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau,EarlyStopping\nfrom tensorflow.keras.layers import Conv2D, SeparableConv2D, MaxPool2D, LeakyReLU, Activation\nfrom tensorflow.keras.applications.imagenet_utils import preprocess_input\nimport numpy as np\nimport argparse\nimport cv2\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom tensorflow.keras.models import Model, Sequential\nfrom sklearn.model_selection import StratifiedKFold\nfrom tensorflow.keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\nimport scipy\nimport skimage\nfrom skimage.transform import resize\nimport random\nfrom PIL import Image\nimport glob\nfrom tensorflow.keras.preprocessing.image import load_img, save_img, img_to_array\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.layers import AveragePooling2D\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam,SGD,RMSprop,Adamax\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam,Adadelta,RMSprop,Adagrad\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.layers import Conv2D, SeparableConv2D, MaxPool2D, LeakyReLU, Activation\nfrom tensorflow.keras.applications.imagenet_utils import preprocess_input\nimport numpy as np\nimport argparse\nimport cv2\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom tensorflow.keras.models import Model, Sequential\nfrom sklearn.model_selection import StratifiedKFold\nfrom tensorflow.keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\nimport scipy\nimport skimage\nfrom skimage.transform import resize\nimport random\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","63cf14ff":"def Images_details_Print_data(data, path):\n    print(\" ====== Images in: \", path)    \n    for k, v in data.items():\n        print(\"%s:\\t%s\" % (k, v))\n\ndef Images_details(path):\n    files = [f for f in glob.glob(path + \"**\/*.*\", recursive=True)]\n    data = {}\n    data['images_count'] = len(files)\n    data['min_width'] = 10**100  # No image will be bigger than that\n    data['max_width'] = 0\n    data['min_height'] = 10**100  # No image will be bigger than that\n    data['max_height'] = 0\n\n\n    for f in files:\n        im = Image.open(f)\n        width, height = im.size\n        data['min_width'] = min(width, data['min_width'])\n        data['max_width'] = max(width, data['max_height'])\n        data['min_height'] = min(height, data['min_height'])\n        data['max_height'] = max(height, data['max_height'])\n\n    Images_details_Print_data(data, path)","13aa32f8":"COV_DIR = \"\/kaggle\/input\/covid-cxr-image-dataset-research\/COVID_IEEE\/covid\/\"\nNORM_DIR = \"\/kaggle\/input\/covid-cxr-image-dataset-research\/COVID_IEEE\/normal\/\"\nVIR_DIR = \"\/kaggle\/input\/covid-cxr-image-dataset-research\/COVID_IEEE\/virus\/\"","dad2c747":"Images_details(NORM_DIR)","c358a5c4":"Images_details(COV_DIR)","de28286e":"Images_details(VIR_DIR)","ab9f4014":"Cimages = os.listdir(COV_DIR)\nNimages = os.listdir(NORM_DIR)\nVimages = os.listdir(VIR_DIR)","f3b48d92":"sample_images = random.sample(Cimages,6)\nf,ax = plt.subplots(2,3,figsize=(15,9))\n\nfor i in range(0,6):\n    im = cv2.imread('\/kaggle\/input\/covid-cxr-image-dataset-research\/COVID_IEEE\/covid\/'+sample_images[i])\n    ax[i\/\/3,i%3].imshow(im)\n    ax[i\/\/3,i%3].axis('off')\nf.suptitle('COVID-19 affected Chest X-Ray',fontsize=20)\nplt.show()","0910bc35":"sample_images = random.sample(Vimages,6)\nf,ax = plt.subplots(2,3,figsize=(15,9))\n\nfor i in range(0,6):\n    im = cv2.imread('\/kaggle\/input\/covid-cxr-image-dataset-research\/COVID_IEEE\/virus\/'+sample_images[i])\n    ax[i\/\/3,i%3].imshow(im)\n    ax[i\/\/3,i%3].axis('off')\nf.suptitle('Viral Infection affected Chest X-Ray',fontsize=20)\nplt.show()","8f44b015":"sample_images = random.sample(Nimages,6)\nf,ax = plt.subplots(2,3,figsize=(15,9))\n\nfor i in range(0,6):\n    im = cv2.imread('\/kaggle\/input\/covid-cxr-image-dataset-research\/COVID_IEEE\/normal\/'+sample_images[i])\n    ax[i\/\/3,i%3].imshow(im)\n    ax[i\/\/3,i%3].axis('off')\nf.suptitle('Normal Chest X-Ray',fontsize=20)\nplt.show()","1c7b5b5e":"## Enhancing the CXR Images using White balance and CLAHE\n\ndef wb(channel, perc = 0.05):\n    mi, ma = (np.percentile(channel, perc), np.percentile(channel,100.0-perc))\n    channel = np.uint8(np.clip((channel-mi)*255.0\/(ma-mi), 0, 255))\n    return channel","c9646675":"data=[]\nlabels=[]\nUninfected=os.listdir(\"\/kaggle\/input\/covid-cxr-image-dataset-research\/COVID_IEEE\/normal\/\")\nfor a in Uninfected:\n\t# extract the class label from the filename\n\t\n\n\t# load the image, swap color channels, and resize it to be a fixed\n\t# 224x224 pixels while ignoring aspect ratio\n\timage = cv2.imread(\"\/kaggle\/input\/covid-cxr-image-dataset-research\/COVID_IEEE\/normal\/\"+a)\n\timWB  = np.dstack([wb(channel, 0.05) for channel in cv2.split(image)] )\n\tgray_image = cv2.cvtColor(imWB, cv2.COLOR_BGR2GRAY)\n\tclahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))\n\timg_clahe1 = clahe.apply(gray_image)\n\timg = cv2.cvtColor(img_clahe1, cv2.COLOR_GRAY2RGB)\n\timage = cv2.resize(img, (224, 224))\n\n\t# update the data and labels lists, respectively\n\tdata.append(image)\n\tlabels.append(0)\n\nCovid=os.listdir(\"\/kaggle\/input\/covid-cxr-image-dataset-research\/COVID_IEEE\/covid\/\")\nfor b in Covid:\n\t# extract the class label from the filename\n\t\n\n\t# load the image, swap color channels, and resize it to be a fixed\n\t# 224x224 pixels while ignoring aspect ratio\n\timage = cv2.imread(\"\/kaggle\/input\/covid-cxr-image-dataset-research\/COVID_IEEE\/covid\/\"+b)\n\timWB  = np.dstack([wb(channel, 0.05) for channel in cv2.split(image)] )\n\tgray_image = cv2.cvtColor(imWB, cv2.COLOR_BGR2GRAY)\n\tclahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))\n\timg_clahe1 = clahe.apply(gray_image)\n\timg = cv2.cvtColor(img_clahe1, cv2.COLOR_GRAY2RGB)\n\timage = cv2.resize(img, (224, 224))\n\t# update the data and labels lists, respectively\n\tdata.append(image)\n\tlabels.append(1)\n \nVirus=os.listdir(\"\/kaggle\/input\/covid-cxr-image-dataset-research\/COVID_IEEE\/virus\/\")\nfor c in Virus:\n\t# extract the class label from the filename\n\t\n\n\t# load the image, swap color channels, and resize it to be a fixed\n\t# 224x224 pixels while ignoring aspect ratio\n\timage = cv2.imread(\"\/kaggle\/input\/covid-cxr-image-dataset-research\/COVID_IEEE\/virus\/\"+c)\n\timWB  = np.dstack([wb(channel, 0.05) for channel in cv2.split(image)] )\n\tgray_image = cv2.cvtColor(imWB, cv2.COLOR_BGR2GRAY)\n\tclahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))\n\timg_clahe1 = clahe.apply(gray_image)\n\timg = cv2.cvtColor(img_clahe1, cv2.COLOR_GRAY2RGB)\n\timage = cv2.resize(img, (224, 224))\n\n\t# update the data and labels lists, respectively\n\tdata.append(image)\n\tlabels.append(2)","36d8a5ff":"# data normalization by dividing image pixels by 255\ndata = np.array(data) \/ 255.0\nlabels = np.array(labels)","c8ac1dd0":"from sklearn.model_selection import train_test_split\n(trainX, testX, trainY, testY) = train_test_split(data, labels,stratify=labels,\n\ttest_size=0.20,  random_state=42)","0ca06437":"trainAug  = ImageDataGenerator(\n\trotation_range=15,\n\tfill_mode=\"nearest\")","341d8685":"inputs = Input(shape=(224, 224,3))\n# First conv block\nx = Conv2D(filters=4, kernel_size=(3, 3), activation='relu', padding='same')(inputs)\nx = Conv2D(filters=4, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = MaxPool2D(pool_size=(2, 2))(x)\n\n# second conv block\nx = Conv2D(filters=8, kernel_size=(3, 3), activation='relu', padding='same')(inputs)\nx = Conv2D(filters=8, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = MaxPool2D(pool_size=(2, 2))(x)\n\n# third conv block\nx = Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same')(inputs)\nx = Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = MaxPool2D(pool_size=(2, 2))(x)\n\n\n# first separable conv block\nx = SeparableConv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = SeparableConv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = BatchNormalization()(x)\nx = MaxPool2D(pool_size=(2, 2))(x)\n\n\n# second separable conv block\nx = SeparableConv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = SeparableConv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = BatchNormalization()(x)\nx = MaxPool2D(pool_size=(2, 2))(x)\n\n\n# third separable conv block\nx = SeparableConv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = SeparableConv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = BatchNormalization()(x)\nx = MaxPool2D(pool_size=(2, 2))(x)\nx = Dropout(rate=0.2)(x)\n\n# fourth separable conv block\nx = SeparableConv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = SeparableConv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = BatchNormalization()(x)\nx = MaxPool2D(pool_size=(2, 2))(x)\nx = Dropout(rate=0.2)(x)\n\n\n# fifth separable conv block\nx = SeparableConv2D(filters=512, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = SeparableConv2D(filters=512, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = BatchNormalization()(x)\nx = MaxPool2D(pool_size=(2, 2))(x)\nx = Dropout(rate=0.2)(x)\n\n# FC layer\nx = Flatten()(x)\nx = Dense(units=512, activation='relu')(x)\nx = Dropout(rate=0.7)(x)\nx = Dense(units=128, activation='relu')(x)\nx = Dropout(rate=0.5)(x)\nx = Dense(units=64, activation='relu')(x)\nx = Dropout(rate=0.3)(x)\nx = Dense(units=32, activation='relu')(x)\nx = Dropout(rate=0.3)(x)\n\n# Output layer\noutput = Dense(units=3, activation='softmax')(x)\n\n# Creating model and compiling\nmodel = Model(inputs=inputs, outputs=output)\n\nEPOCHS = 50\nBS = 16\n\n\n# compile our model\nprint(\"[INFO] compiling model...\")\nopt = \"adam\"\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])\ncallbacks = [ModelCheckpoint('.mdl_wts.hdf5', monitor='val_loss', save_best_only=True),\n             EarlyStopping(monitor='val_loss', patience=5)]\nmodel.summary()","60482d7f":"BS = 16\nprint(\"[INFO] training head...\")\nH = model.fit_generator(\n\ttrainAug.flow(trainX, trainY, batch_size=BS),\n\tsteps_per_epoch=len(trainX) \/\/ BS,\n\tvalidation_data=(testX, testY),\n\tvalidation_steps=len(testX) \/\/ BS,\n\tepochs=EPOCHS,callbacks=callbacks)","c2e5c1f7":"## Loading best model based on its weights\n\nfrom numpy import loadtxt\nfrom tensorflow.keras.models import load_model\nmodel = load_model('.mdl_wts.hdf5')\nscore = model.evaluate(testX, testY,verbose=1)\n\nprint(score)","0bbb1dba":"import itertools\npred_Y = model.predict(testX, batch_size = 8, verbose = True)\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    target_names =['Normal','Covid','Virus']\n\n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=45)\n        plt.yticks(tick_marks, target_names)\n    \n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Predict the values from the validation dataset\nY_pred = model.predict(testX, batch_size=BS)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(pred_Y,axis = 1) \n# Convert validation observations to one hot vectors\n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(testY, Y_pred_classes)\n\n \n\n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = range(3)) ","2da37afa":"predIdxs = model.predict(testX, batch_size=BS)\n\n# for each image in the testing set we need to find the index of the\n# label with corresponding largest predicted probability\npredIdxs = np.argmax(predIdxs, axis=1)\n\n\n# show a nicely formatted classification report\nprint(classification_report(testY, predIdxs,target_names=['normal','covid','virus']))","f62e4e9c":"import seaborn as sns\nimport pandas as pd\nfrom sklearn.datasets import make_classification\nfrom sklearn.preprocessing import label_binarize\nfrom scipy import interp\nfrom itertools import cycle\nimport pandas as pd\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc\n\ny_test = pd.get_dummies(testY)\ny_test = np.array(y_test)\n\nn_classes = 3\n\n\n# Plot linewidth.\nlw = 2\n\n# Compute ROC curve and ROC area for each class\n\n\n# Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], pred_Y[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n    # Compute micro-average ROC curve and ROC area\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), pred_Y.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n# Plot of a ROC curve for a specific class\nfor i in range(n_classes):\n    plt.figure()\n    plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f)' % roc_auc[i])\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n# First aggregate all false positive rates\nall_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n\n# Then interpolate all ROC curves at this points\nmean_tpr = np.zeros_like(all_fpr)\nfor i in range(n_classes):\n    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n\n# Finally average it and compute AUC\nmean_tpr \/= n_classes\n\nfpr[\"macro\"] = all_fpr\ntpr[\"macro\"] = mean_tpr\nroc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n# Plot all ROC curves\nfig = plt.figure(figsize=(12, 8))\nplt.plot(fpr[\"micro\"], tpr[\"micro\"],\n         label='micro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"micro\"]),\n         color='deeppink', linestyle=':', linewidth=4)\n\nplt.plot(fpr[\"macro\"], tpr[\"macro\"],\n         label='macro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"macro\"]),\n         color='navy', linestyle=':', linewidth=4)\n\ncolors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\nfor i, color in zip(range(n_classes), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n             label='ROC curve of class {0} (area = {1:0.2f})'\n             ''.format(i, roc_auc[i]))\n\n\nplt.plot([0, 1], [0, 1], 'k--', lw=lw)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Some extension of Receiver operating characteristic to multi-class')\nplt.legend(loc=\"lower right\")\nsns.despine()\nplt.show()","291aa776":"def specificity(y_true, y_pred):\n    \"\"\"\n    param:\n    y_pred - Predicted labels\n    y_true - True labels \n    Returns:\n    Specificity score\n    \"\"\"\n    neg_y_true = 1 - y_true\n    neg_y_pred = 1 - y_pred\n    fp = K.sum(neg_y_true * y_pred)\n    tn = K.sum(neg_y_true * neg_y_pred)\n    specificity = tn \/ (tn + fp + K.epsilon())\n    return specificity","d994016c":"print(specificity(y_test, Y_pred))","5b4ef06a":"# getting predictions on val set.\npred=model.predict(testX, batch_size=BS)\nY_pred_classes=np.argmax(pred,axis=1)\n\nY_true = np.argmax(testY,axis = 0) \n\n# Mapping Classes\nclasses = {0 : 'Normal',\n            1 : 'COVID',\n            2 : 'Viral'}","a9a92fd7":"incorrect = []\nfor i in range(len(testY)):\n    if(not Y_pred_classes[i] == testY[i]):\n        incorrect.append(i)\n    if(len(incorrect) == 22):\n        break\n\ncorrect = []\nfor i in range(len(testY)):\n    if(Y_pred_classes[i] == testY[i]):\n        correct.append(i)\n    if(len(correct) == 100):\n        break","ca89a8f8":"fig, ax = plt.subplots(4,2, figsize=(12,10))\nfig.set_size_inches(10,10)\nax[0,0].imshow(testX[correct[0]], cmap='gray')\nax[0,0].set_title(\"Predicted Label : \" + str(classes[Y_pred_classes[correct[0]]]) + \"\\n\"+\"Actual Label : \" + \n                 str(classes[testY[correct[0]]]))\n\nax[0,1].imshow(testX[correct[1]], cmap='gray')\nax[0,1].set_title(\"Predicted Label : \" + str(classes[Y_pred_classes[correct[1]]]) + \"\\n\"+\"Actual Label : \" + \n                 str(classes[testY[correct[1]]]))\nax[1,0].imshow(testX[correct[2]], cmap='gray')\nax[1,0].set_title(\"Predicted Label : \" + str(classes[Y_pred_classes[correct[2]]]) + \"\\n\"+\"Actual Label : \" + \n                 str(classes[testY[correct[2]]]))\nax[1,1].imshow(testX[correct[3]], cmap='gray')\nax[1,1].set_title(\"Predicted Label : \" + str(classes[Y_pred_classes[correct[3]]]) + \"\\n\"+\"Actual Label : \" + \n                 str(classes[testY[correct[3]]]))\nax[2,0].imshow(testX[correct[4]], cmap='gray')\nax[2,0].set_title(\"Predicted Label : \" + str(classes[Y_pred_classes[correct[4]]]) + \"\\n\"+\"Actual Label : \" + \n                 str(classes[testY[correct[4]]]))\nax[2,1].imshow(testX[correct[5]], cmap='gray')\nax[2,1].set_title(\"Predicted Label : \" + str(classes[Y_pred_classes[correct[5]]]) + \"\\n\"+\"Actual Label : \" + \n                 str(classes[testY[correct[5]]]))\nax[3,0].imshow(testX[correct[6]], cmap='gray')\nax[3,0].set_title(\"Predicted Label : \" + str(classes[Y_pred_classes[correct[6]]]) + \"\\n\"+\"Actual Label : \" + \n                 str(classes[testY[correct[6]]]))\nax[3,1].imshow(testX[correct[7]], cmap='gray')\nax[3,1].set_title(\"Predicted Label : \" + str(classes[Y_pred_classes[correct[7]]]) + \"\\n\"+\"Actual Label : \" + \n                 str(classes[testY[correct[7]]]))\nfig.tight_layout(pad=1.0)\nplt.show()","0de0ae0c":"import lime\nfrom lime import lime_image\n\nexplainer = lime_image.LimeImageExplainer()\n\nexplanation = explainer.explain_instance(testX[correct[5]], model.predict, top_labels=5, hide_color=0, num_samples=100)","5502363f":"from skimage.segmentation import mark_boundaries\ntemp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=True)\nplt.imshow(mark_boundaries(temp \/ 2 + 0.5, mask))","2ec227d6":"temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=False)\nplt.imshow(mark_boundaries(temp \/ 2 + 0.5, mask))","2cccc429":"temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=False, num_features=10, hide_rest=False)\nplt.imshow(mark_boundaries(temp \/ 2 + 0.5, mask))","6ed524c2":"!pip install tf-keras-vis","394fa95d":"import tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tf_keras_vis.activation_maximization import ActivationMaximization\nfrom tf_keras_vis.utils.callbacks import Print\n\n# Define modifier to replace a softmax function of the last layer to a linear function.\ndef model_modifier(m):\n    m.layers[-1].activation = tf.keras.activations.linear\n\n# Create Activation Maximization object\nactivation_maximization = ActivationMaximization(model, model_modifier)\n\n# Define loss function. 20 is the imagenet index corresponding to ouzel.\nloss = lambda x: K.mean(x[:, 1])\n\n# Generate max activation with debug printing\nactivation = activation_maximization(loss, callbacks=[Print(interval=100)])\nimage = activation[0].astype(np.uint8)\n\nf, ax = plt.subplots(figsize=(10, 5), subplot_kw={'xticks': [], 'yticks': []})\nax.imshow(image)\nplt.show()\n","0ad5ed4a":"import numpy as np\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nfrom tensorflow.keras.preprocessing.image import load_img\n\n# Load images\nimg1 = load_img('\/kaggle\/input\/covid-cxr-image-dataset-research\/COVID_IEEE\/normal\/NORMAL2-IM-0388-0001.jpeg', target_size=(224, 224))\nimg2 = load_img('\/kaggle\/input\/covid-cxr-image-dataset-research\/COVID_IEEE\/virus\/person704_virus_1301.jpeg', target_size=(224, 224))\nimg3 = load_img('\/kaggle\/input\/covid-cxr-image-dataset-research\/COVID_IEEE\/covid\/covid1900167.png', target_size=(224, 224))\n\nimages = np.asarray([np.array(img1), np.array(img2),np.array(img3)])\n\n# Prepare datasets\nX = preprocess_input(images)\n\n# Render\nsubprot_args = {\n    'nrows': 1,\n    'ncols': 3,\n    'figsize': (6, 3),\n    'subplot_kw': {'xticks': [], 'yticks': []}\n}\nf, ax = plt.subplots(**subprot_args)\nfor i in range(len(images)):\n    ax[i].imshow(images[i])\nplt.tight_layout()\nplt.show()","3456afa3":"# Define loss function. 1 is the imagenet index corresponding to Goldfish,\n# And 294 is one corresponding to Bear.\ndef loss(output):\n    return (output[1][1], output[0][1])\n\n# Define modifier to replace a softmax function of the last layer to a linear function.\ndef model_modifier(m):\n    m.layers[-1].activation = tf.keras.activations.linear\n    return m","54fa3020":"from matplotlib import cm\nfrom tf_keras_vis.gradcam import Gradcam\nfrom tf_keras_vis.utils import normalize\n\n# Create Gradcam object\ngradcam = Gradcam(model, model_modifier, clone=False)\n\n# Generate heatmap with GradCAM\ncam = gradcam(loss, X)\ncam = normalize(cam)\n\nf, ax = plt.subplots(**subprot_args)\nfor i in range(len(cam)):\n    heatmap = np.uint8(cm.jet(cam[i])[..., :3] * 255)\n    ax[i].imshow(images[i])\n    ax[i].imshow(heatmap, cmap='jet', alpha=0.5)\nplt.tight_layout()\nplt.show()","a6a2f757":"ins = np.asarray([np.array(testX[correct[6]]), np.array(testX[correct[7]])])\nS = preprocess_input(images)\n\n# Render\nsubprot_args = {\n    'nrows': 1,\n    'ncols': 2,\n    'figsize': (8, 4),\n    'subplot_kw': {'xticks': [], 'yticks': []}\n}\nf, ax = plt.subplots(**subprot_args)\nfor i in range(len(ins)):\n    ax[i].imshow(ins[i])\nplt.tight_layout()\nplt.show()","c58a4b26":"from matplotlib import cm\nfrom tf_keras_vis.gradcam import Gradcam\nfrom tf_keras_vis.utils import normalize\n\n# Create Gradcam object\ngradcam = Gradcam(model, model_modifier, clone=False)\n\n# Generate heatmap with GradCAM\ncam = gradcam(loss, S)\ncam = normalize(cam)\n\nf, ax = plt.subplots(**subprot_args)\nfor i in range(len(cam)):\n    heatmap = np.uint8(cm.jet(cam[i])[..., :3] * 255)\n    ax[i].imshow(ins[i])\n    ax[i].imshow(heatmap, cmap='jet', alpha=0.5)\nplt.tight_layout()\nplt.show()","a2db5fdf":"from tf_keras_vis.saliency import Saliency\nfrom tf_keras_vis.utils import normalize\n\n# Create Saliency object\nsaliency = Saliency(model, model_modifier, clone=False)\n\n# Generate saliency map\nsaliency_map = saliency(loss, S)\nsaliency_map = normalize(saliency_map)\n\nf, ax = plt.subplots(**subprot_args)\nfor i in range(len(saliency_map)):\n    ax[i].imshow(saliency_map[i], cmap='jet')\nplt.tight_layout()\nplt.show()\n","2e883fca":"# Generate saliency map with smoothing that reduce noise by adding noise\nsaliency_map = saliency(loss, S, smooth_samples=100)\nsaliency_map = normalize(saliency_map)\n\nf, ax = plt.subplots(**subprot_args)\nfor i in range(len(saliency_map)):\n    ax[i].imshow(saliency_map[i], cmap='jet')\nplt.tight_layout()\nplt.show()\n","ac24818d":"## Model Explanation","ff88ab4f":"## GradCAM Heatmap","3058ca8b":"## Train Test Split","0a8f9c86":"## Image Enhancement (White Balance and CLAHE)\n\nThis Kernel is inspired from  [COVIDLite](https:\/\/arxiv.org\/pdf\/2006.13873.pdf) Paper published in arXiv. In this paper authors have used White Balance and CLAHE as a image preprocessing step for detection of COVID-19 cases.\n\n### 1. White Balance\n\nWhite Balance is the image processing operation applied to adjust proper color fidelity in a digital image. Due to low lighting conditions in medical images, some of the parts of the image appeared dark and the image capturing equipment does not detect light precisely as the human eye does.\n\nDue to this, image processing or correction help to ensure that the final image represents the colors of the natural image. The objective of this operation is to enhance the visibility of the image so that Deep CNNs could extract useful features from the image. The white balance algorithm adjusts the colors of the active layers of the image by stretching red, green, and blue channels independently.\n\nFor doing this, pixel colors discarded, which are at the end of the three channels and are used by only 0.05% of the pixels in the image, while stretching is performed for the remaining color range.\n\n After this operation, pixel colors infrequently present at the end of the channel could not negatively influence the upper and lower bound values while stretching. In this solution, we have implemented a white balance algorithm in python language using NumPy and OpenCV library.\n\nThe steps of the White balance algorithm can be summarized as:\n\n![](https:\/\/i.ibb.co\/1rR3h7v\/white-balance.png)\n\nwhere Pi(C) represents the taking the ith percentile of channel C, and Clip(., min, max) operation depicts performing saturation operation within min and max values. C, Cupd denotes the input and updated channels pixel values after the operation respectively.\n\n### 2. CLAHE (Contrast Limited Adaptive Histogram Equalization)\n\nIt is an effective contrast enhancement method that effectively increases the contrast of the image. CLAHE is an improved version of the adaptivehistogram equation (AHE). \n\nHistogram equalization is the simple method for enhancing the contrast of the image by spreading out the intensity range of the image or stretching out the most frequent intensity value of the image. Stretching the intensity values changes the natural brightness of the input image and introduces some undesirable noise in the image. \n\nIn AHE, the input image split into several small images, also known as tiles. In this method, the histogram of each tile computed, which corresponds to different sections of the image and uses them to derive intensity remapping function for each tile. \nThis method introduces noise in the image due to over amplification. CLAHE works precisely the same as AHE, but it clips the histogram at specific values for limiting the amplification before computing the cumulative distributive function.\n\nThe overamplified part of the histogram is further redistributed over the histogram. In one of the previous studies, CLAHE showed exceptional results in enhancing chest CT images and considered useful in examining a wide variety of medical images. The computation of CLAHE is performed as:\n\n![](https:\/\/i.ibb.co\/MSM3MrN\/CL1.png)\n\nwhere, p represents pixel value after applying CLAHE, pmax , pmin represents maximum and minimum pixel value of an image respectively and P(f) represents cumulative probability distribution function.\n\n![](https:\/\/i.ibb.co\/DMkncVP\/CLAHE.png)\n","14ff96d3":"## Image normalization","298a883b":"## Saliency Maps","ea1066d4":"## Data preparation","2da49802":"## Classification Report","0c397288":"## Plotting Model Prediction","e80c912a":"## Model Evaluation","739b0d95":"### Observation:\n\nFrom the above sample images we can easily observed that:\n- Normal CXR Images have clear lungs.\n- Viral Infection CXR Images have slight congestion in lungs\n- COVID-19 images have serious congestion in lungs","52ff8275":"## Model Building\n\nIn this kernel I will be using Depthwise separable CNNs","5108633e":"## Plotting ROC-AUC Curve","eb0612cd":"Analyzing Correct predictions by Model Using LIME"}}