{"cell_type":{"006f7a09":"code","265c403d":"code","66d64650":"code","a116e130":"code","32c38e1b":"code","92578e6d":"code","7b659df8":"code","e1fc797f":"code","42076445":"code","1345b3da":"code","2ff8b9a8":"code","8be021b5":"code","540c5bc3":"code","7e884912":"code","630946f4":"code","bf51e932":"code","9eea9272":"code","7e4fbad0":"code","747873c0":"code","198df210":"code","588d8139":"code","cc3bf470":"code","ecc936fd":"markdown","3d2a3247":"markdown","d17358bc":"markdown","9e1c8249":"markdown","696ed2b9":"markdown","cab1d3f2":"markdown","6df05417":"markdown","6aa8945b":"markdown","9f8c8a9d":"markdown","336d818b":"markdown","27af1152":"markdown","7a7258b1":"markdown","713fbf48":"markdown","c91461cd":"markdown","5064bd0a":"markdown"},"source":{"006f7a09":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","265c403d":"import matplotlib.pyplot as plt","66d64650":"dataset=pd.read_csv('..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')","a116e130":"#!pip install pandas_profiling","32c38e1b":"import pandas_profiling","92578e6d":"pandas_profiling.ProfileReport(dataset)","7b659df8":"import seaborn as sns\nplt.figure(figsize = (10,8))\nsns.heatmap(dataset.corr(), annot=True, linewidth = 0.8)","e1fc797f":"corr_coef = dataset.corr()\nlist_corr = []\nfor i in range(0, len(dataset.dtypes)):\n    for j in range(0, len(dataset.dtypes)):\n        value = corr_coef.iloc[i:i+1, j:j+1].values\n        \n        if value > 0.8 and value != 1:\n            list_corr.append(corr_coef.columns[i])\nlist_corr\n","42076445":"dataset['quality'] = np.where(dataset['quality'] < 6, 0, (np.where(dataset['quality'] == 6, 1, 2)))\ndataset['quality'].value_counts()","1345b3da":"X = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values","2ff8b9a8":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","8be021b5":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","540c5bc3":"accuracy_scores = {}\ndef predictor(predictor, params):\n    global accuracy_scores\n    if predictor == 'lr':\n        print('Training Logistic Regression on Training Set')\n        from sklearn.linear_model import LogisticRegression\n        classifier = LogisticRegression(**params)\n\n    elif predictor == 'svm':\n        print('Training Support Vector Machine on Training Set')\n        from sklearn.svm import SVC\n        classifier = SVC(**params)\n\n    elif predictor == 'knn':\n        print('Training K-Nearest Neighbours on Training Set')\n        from sklearn.neighbors import KNeighborsClassifier\n        classifier = KNeighborsClassifier(**params)\n\n    elif predictor == 'dt':\n        print('Training Decision Tree Classifier on Training Set')\n        from sklearn.tree import DecisionTreeClassifier\n        classifier = DecisionTreeClassifier(**params)\n\n    elif predictor == 'nb':\n        print('Training Naive Bayes Classifier on Training Set')\n        from sklearn.naive_bayes import GaussianNB\n        classifier = GaussianNB(**params)\n\n    elif predictor == 'rfc':\n        print('Training Random Forest Classifier on Training Set')\n        from sklearn.ensemble import RandomForestClassifier\n        classifier = RandomForestClassifier(**params)\n\n    classifier.fit(X_train, y_train)\n    \n    print('''Prediciting Test Set Result''')\n    y_pred = classifier.predict(X_test)\n    \n    result = np.concatenate((y_pred.reshape(len(y_pred), 1),\n                             y_test.reshape(len(y_test), 1)),1)\n    print(result, '\\n')\n    \n    print('''Making Confusion Matrix''')\n    from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n    y_pred = classifier.predict(X_test)\n    cm = confusion_matrix(y_test, y_pred)\n    print(cm, '\\n')\n\n    print('''Classification Report''')\n    print(classification_report(y_test, y_pred,\n          target_names=['0', '1', '2'], zero_division=1))\n\n    print('''Evaluating Model Performance''')\n    accuracy = accuracy_score(y_test, y_pred)\n    print(accuracy, '\\n')\n\n    print('''Applying K-Fold Cross validation''')\n    from sklearn.model_selection import cross_val_score\n\n    accuracies = cross_val_score(\n        estimator=classifier, X=X_train, y=y_train, cv=10)\n    \n    print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n    accuracy_scores[classifier] = accuracies.mean()*100\n    print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100), '\\n')","7e884912":"predictor('lr', {'penalty': 'l1', 'solver': 'liblinear'})","630946f4":"predictor('svm', {'C': .5, 'gamma': 0.8,\n          'kernel': 'linear', 'random_state': 0})","bf51e932":"predictor('svm', {'C': .25, 'gamma': 0.1, 'kernel': 'rbf', 'random_state': 0})","9eea9272":"predictor('knn', {'algorithm': 'auto', 'n_jobs': 1,\n          'n_neighbors': 8, 'weights': 'distance'})","7e4fbad0":"predictor('dt', {'criterion': 'entropy', 'max_features': 'auto',\n          'splitter': 'best', 'random_state': 0})","747873c0":"predictor('nb', {})","198df210":"predictor('rfc', {'criterion': 'gini', 'max_features': 'log2', 'n_estimators': 100,'random_state':0})","588d8139":"maxKey = max(accuracy_scores, key=lambda x: accuracy_scores[x])\nprint('The model with highest K-Fold Validation Accuracy score is  {0} with an accuracy of  {1:.2f}'.format(\n    maxKey, accuracy_scores[maxKey]))","cc3bf470":"plt.figure(figsize=(12, 6))\nmodel_accuracies = list(accuracy_scores.values())\nmodel_names = ['LogisticRegression', 'SVC',\n               'K-SVC', 'KNN', 'Decisiontree', 'GaussianNB', 'RandomForest']\nsns.barplot(x=model_accuracies, y=model_names, palette='mako');","ecc936fd":"**Training Decision tree**","3d2a3247":"**Defining three wine categories 0= poor, 1=normal, 2= high**","d17358bc":"**separate independent variables and dependent variable**","9e1c8249":"EDA using pandas_profiling","696ed2b9":"**we can also loop find the co related features as below**","cab1d3f2":"**splitting the dataset in train_test**","6df05417":"**Training Kernel SVM**","6aa8945b":"**Training Logistic Regression**","9f8c8a9d":"**Training SVM**","336d818b":"**Training K-NN**","27af1152":"loading dataset","7a7258b1":"**pfoilereport doesn't give clarity on correlation between features**\n\n**so we check manually and try to find if there is any correlation > 0.8**","713fbf48":"**Training Navie Bayes**","c91461cd":"**applying feature scaling after split to avoid data leakage**","5064bd0a":"**Training Random Forest Classifier**"}}