{"cell_type":{"c24c007e":"code","4505c29f":"code","b70776f2":"code","20a11f43":"code","f4a1e0a2":"code","bcdf4305":"code","a00095e5":"code","4e9a0c3c":"code","91158eda":"code","198f9a41":"code","0b05e57a":"code","081e7b29":"code","32379e80":"code","c89d8b62":"code","1089011c":"code","d1e611e5":"code","424fdb05":"code","48c4da7d":"code","9a50750c":"code","4ea0ef11":"code","439fa4c2":"code","ee267977":"code","c12bc243":"code","0a11d1c2":"code","0175ceb9":"code","1ceaf07c":"code","8efb5331":"code","e99e32f5":"code","df60604d":"code","102c5180":"code","61f02be6":"code","538f660f":"code","8ecbef23":"code","2ddd99cc":"code","dec2cfaa":"code","f332c1e9":"code","3c57d39f":"code","2d80bc3c":"code","cbcada0f":"code","72e85cd1":"code","c9c66bad":"code","1fbf8976":"code","16d23e49":"code","68a06d32":"code","1d66e452":"code","bcfbff89":"code","111c0b82":"code","80f1fd93":"code","1f2022c0":"code","3d62fb58":"code","0959fbbe":"code","4bb36745":"code","ad16869f":"code","56f0d5a7":"code","68f63d50":"code","07a88240":"code","6ecbb000":"code","be131866":"code","fcfce340":"code","3b9e93cc":"code","caf8f0f5":"code","8fbf0e33":"code","603eb045":"code","44506900":"code","d7d53504":"code","0d5e8b52":"code","73c07b94":"code","ff8b4a35":"code","418c80fd":"code","14d69cfd":"code","0209b0a5":"markdown","c4b8f3bf":"markdown","ceb95880":"markdown","0c228ce5":"markdown","d3553bcf":"markdown","f1f87b3c":"markdown","90e4d054":"markdown","6a329f69":"markdown","b4d56b02":"markdown","68d5729e":"markdown","b924c851":"markdown","1ed58b1a":"markdown","3b6e4b3b":"markdown","fcccd8ba":"markdown","7d8f64b2":"markdown"},"source":{"c24c007e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4505c29f":"# Import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport folium\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import recall_score, confusion_matrix, roc_auc_score\nimport pickle\n\n\nimport warnings\nwarnings.filterwarnings('ignore')","b70776f2":"# load the data\nweather_data = pd.read_csv(\"..\/input\/weather-api-data\/data\/forecast_data.csv\")\nlocation_data = pd.read_csv(\"..\/input\/weather-api-data\/data\/location_data.csv\")","20a11f43":"location_data.head()","f4a1e0a2":"# Plotting the cities for which we have the data for\nINDIA_COORDINATES = [20.5937, 78.9629]\n\nmy_map = folium.Map(INDIA_COORDINATES) # Creates a map\ndef plot_map(df):\n  # The function will add markers at the coordinates present in our dataframe\n  folium.Marker(location=[df.lat, df.lon]).add_to(my_map)\n\n# Apply the function to the dataframe\nlocation_data.apply(plot_map, axis=1)\nmy_map.fit_bounds([[30.3753, 69.3451], [7.8731, 80.7718]])\nmy_map","bcdf4305":"weather_data.head()","a00095e5":"# Extract the condition text from the json string\nweather_data['condition'] = weather_data['condition'].apply(lambda x : eval(x)['text'])","4e9a0c3c":"weather_data.head()","91158eda":"# Many columns are same just with differnt units, for ex wind_mph and wind_kph. So, it's better to remove the redundancy\nredundant_columns = ['temp_f', 'wind_kph', 'pressure_in', 'precip_in', 'feelslike_f', 'windchill_f', 'heatindex_f', 'dewpoint_f', 'vis_miles', 'chance_of_rain', 'chance_of_snow','gust_kph']\n\n# Drop the redundant information\nweather_data.drop(redundant_columns, axis=1, inplace=True)","198f9a41":"weather_data.head()","0b05e57a":"# Visualise various continuous distributions\ncontinuous_distributions = ['temp_c', 'wind_mph', \n                            'wind_degree', 'pressure_mb', \n                            'precip_mm', 'humidity', \n                            'feelslike_c', 'windchill_c', \n                            'heatindex_c', 'dewpoint_c', \n                            'vis_km', 'gust_mph']\n\nplt.figure(figsize=(18, 16))\nfor i, dist_col in enumerate(continuous_distributions):\n\n  # Create subplots\n  plt.subplot(3, 4, i+1)\n  sns.histplot(weather_data[dist_col])\n  plt.title(dist_col + \" Distribution\")","081e7b29":"plt.style.use('seaborn')\nweather_data['precip_mm'].hist(bins=25)\nplt.xlabel(\"Precipitaion (in mm)\")\nplt.xticks(np.arange(0,21,1))\nplt.show()","32379e80":"# Plotting categorical discrete variables\n\n# 1. Plotting days and nights count\nplt.figure(figsize=(8,6))\nsns.countplot(data=weather_data, x='is_day')\nplt.show()\n","c89d8b62":"weather_data['condition'].value_counts().sort_values().plot(kind='barh', figsize=(10,8))\nplt.xlabel(\"Weather Conditions\")\nplt.ylabel(\"Frequency\")\nplt.grid()\nplt.show()","1089011c":"weather_data['wind_dir'].value_counts().sort_values().plot(kind='bar', figsize=(10,8))\nplt.xlabel(\"Wind Direction\")\nplt.ylabel(\"Frequency\")\nplt.grid()\nplt.show()","d1e611e5":"# Plot state distribution\nweather_data['state'].value_counts().sort_values().plot(kind='barh', figsize=(10,8))\nplt.xlabel(\"Frequency\")\nplt.ylabel(\"State\")\nplt.show()","424fdb05":"# Plot the count of rainy forecasts\nsns.countplot(data=weather_data, x='will_it_rain')\nplt.show()","48c4da7d":"# Plot the count of snow forecasts\nsns.countplot(data=weather_data, x='will_it_snow')\nplt.show()","9a50750c":"# Drop the snow column as it has constant variance\nweather_data.drop(['will_it_snow'], axis=1, inplace=True)","4ea0ef11":"# Group the data by states\ngrouped_state_data = weather_data.groupby('state')","439fa4c2":"# State wise analysis\nstate_data = weather_data.groupby('state').mean().reset_index()\n\n# Plotting each states mean temperature in degree celsius\nplt.figure(figsize=(12,8))\nplt.barh(y = state_data.sort_values(by='temp_c')['state'], width = state_data.sort_values(by='temp_c')['temp_c'])\nplt.xlabel(\"State\")\nplt.ylabel(\"Temperature in degree Celsius\")\nplt.show()","ee267977":"# store the name of the states in a list and sort them alphabetically\nstates = [state for state in weather_data['state'].value_counts().index]\nstates.sort()","c12bc243":"continuous_distributions.append('will_it_rain')\ncontinuous_distributions.append('cloud')","0a11d1c2":"columns_to_analyze_1 = continuous_distributions\ncolumns_to_analyze_2 = ['condition', 'wind_dir']","0175ceb9":"# Time Series Analysis\ndef statewise_data_analysis(columns_to_analyze_1, columns_to_analyze_2):\n\n  label_map = ['Day {}'.format(i) for i in range(1, 15)]\n  ticks = [i for i in range(1,15)]\n\n  # Plotting average daily distributions from 15\/10\/2021-22\/10\/2021\n\n  for i in range(len(states)):\n\n    # Perform analysis for each state\n    state = states[i]\n    state_i_data = grouped_state_data.get_group(state)\n    state_i_data['time'] = pd.to_datetime(state_i_data['time'])\n\n    print(\"=========================================================================={}'s ANALYSIS=================================================================================\".format(state.upper()))\n    print()\n\n    # Group the data by time\n    time_data = state_i_data.groupby('time').mean()\n\n    # Plot the daily average of all the continuous distributions and numerical variable\n    columns_to_analyze_1 = continuous_distributions\n    plt.figure(figsize=(25, 25))\n    for i, col in enumerate(columns_to_analyze_1):\n      plt.subplot(7, 2, i + 1)\n      plt.plot(time_data[col])\n      #plt.xticks(rotation=-45)\n      plt.xlabel(\"Date\")\n      plt.ylabel(col)\n    plt.show()\n\n    # Plotting categorical variable across states\n    plt.figure(figsize=(25, 12))\n    for j in range(len(columns_to_analyze_2)):\n      plt.subplot(1, 2, j+1)\n      curr_column = columns_to_analyze_2[j]\n      data_to_plot = state_i_data[curr_column].value_counts().sort_values()\n      plt.barh(y=data_to_plot.index, width=data_to_plot.values)\n      plt.xlabel(\"Frequency\")\n      plt.ylabel(curr_column)\n    plt.show()\n    \n    print(\"====================================================================================================================================================================================\".format(state.upper()))\n    print()\n\n# call the function\nstatewise_data_analysis(columns_to_analyze_1, columns_to_analyze_2)","1ceaf07c":"# Analysizng features and their dependence on predicting whether it will rain or not\nweather_data.groupby('will_it_rain').mean()","8efb5331":"columns_to_analyze_3 = list(weather_data.groupby('will_it_rain').mean().columns)\ncolumns_to_analyze_3.remove('time_epoch')","e99e32f5":"# Analysizng features and their dependence on predicting whether it will rain or not for each state\ndef statewise_rain_factor_analysis(columns_to_analyze_3):\n\n  # Plotting boxplots for the feature 'will_it_rain' against the average of all other continuous features for each state\n\n  for i in range(len(states)):\n\n    # Perform analysis for each state\n    state = states[i]\n    state_i_data = grouped_state_data.get_group(state)\n\n    print(\"=========================================================================={}'s ANALYSIS=================================================================================\".format(state.upper()))\n    print()\n\n    plt.figure(figsize=(25, 25))\n    for i, col in enumerate(columns_to_analyze_3):\n      plt.subplot(7, 2, i + 1)\n      sns.boxplot(data=state_i_data, x='will_it_rain', y=col)\n      #plt.xticks(rotation=-45)\n      plt.xlabel(\"Will it Rain 0 : No ; 1 : Yes\")\n      plt.ylabel(col)\n    plt.show()\n    \n    print(\"====================================================================================================================================================================================\".format(state.upper()))\n    print()\n\n# Call the above function\nstatewise_rain_factor_analysis(columns_to_analyze_3)","df60604d":"continuous_distributions","102c5180":"continuous_distributions.remove('will_it_rain')","61f02be6":"continuous_distributions.remove('cloud')","538f660f":"# Plotting scatter plots\n\n'''Plotting each continuous distribution against every other continuous distribution'''\nweather_data[continuous_distributions].head()\nfor i in range(len(continuous_distributions)):\n\n  if i != len(continuous_distributions)-1:\n    curr_column = continuous_distributions[i]\n    print(\"================================================= {} SCATTER PLOT ===================================================\".format(curr_column))\n    print()\n    num_rows = len(continuous_distributions) - i\n    for j in range(i+1, len(continuous_distributions)):\n      next_column = continuous_distributions[j]\n      plt.figure(figsize=(8,6))\n      sns.scatterplot(data=weather_data, x=curr_column, y=next_column)\n      plt.show()\n    print(\"=======================================================================================================================\")\n    print()","8ecbef23":"# PLot the correlation heatmap\ncorr_matrix = weather_data.corr()\nplt.figure(figsize=(14, 14))\nsns.heatmap(corr_matrix, annot=True, cbar=False)\nplt.show()","2ddd99cc":"weather_data.head()","dec2cfaa":"# Encode the categorical text values\ncols_to_encode = ['condition', 'wind_dir', 'state', 'city']\nfor col in cols_to_encode:\n  le = LabelEncoder()\n  weather_data[col] = le.fit_transform(weather_data[col])","f332c1e9":"weather_data.head()","3c57d39f":"# drop the time column\nweather_data.drop('time', axis=1, inplace=True)","2d80bc3c":"# Scale the data\nX = weather_data.drop('will_it_rain', axis=1)\ny = weather_data['will_it_rain']\n\nX_columns = X.columns\nss = StandardScaler()\nX_scaled_arr = ss.fit_transform(X)\n\n# Create a new dataframe called X_scaled\nX_scaled = pd.DataFrame(X_scaled_arr, columns=X_columns)","cbcada0f":"X_scaled.head()","72e85cd1":"# Using recursive feature elimination to select optimal number of features\n\n# Using an ensemble model like Random Forest to use as the model.\n\n'''\nQ. How to select a model for using RFE?\nA. One approach, use a model that offers scoring of features, like feature_importances_ in Random Forest.\n'''\n\nrfe_model = RandomForestClassifier()\nrfe = RFE(rfe_model, verbose=3)\nX_rfe = rfe.fit_transform(X_scaled, y)","c9c66bad":"# Extract the important features\nfeature_series = pd.Series(rfe.support_, index=X_columns)\nimportant_features = list(feature_series[feature_series == True].index)","1fbf8976":"print(important_features)","16d23e49":"# Prepare the final dataset\nX_final = X_scaled[important_features]","68a06d32":"# Split the data into training and testing set\nX_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.2, random_state=42)","1d66e452":"'''\nChosing Metric:\nThe problem is on a given day, predict whether it will rain or not. \nHence, in this case the priority should be reduce the number of False Negatives or reduce the type II error.\n\nSo, in this case Recall should be the ideal metric that should be optimised.\n\nRecall = TP\/(TP + FN) where, TP: True Positives ; FN: False Negatives\n'''\n\n# We are going to chose a model which gives maximum recall, in case of tie we are going to see which one gives maximum TPs.\n\n# 1. Compute Recall Score\ndef compute_recall_score(model_dict, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test):\n\n  model_name = list(model_dict.keys())[0]\n  model_obj = list(model_dict.values())[0]\n\n  # Make predictions\n\n  # 1. Training predictions\n  train_preds = model_obj.predict(X_train)\n\n  # 2. Testing predictions\n  test_preds = model_obj.predict(X_test)\n\n  # Compute Recall Score\n\n  # 1. Training Score\n  train_recall = recall_score(y_train, train_preds)\n\n  # 2. Testing score\n  test_recall = recall_score(y_test, test_preds)\n\n  # Display the result\n  result_arr = np.array([train_recall, test_recall])\n  result_df = pd.DataFrame(data = result_arr.reshape(1,2), columns = ['Train_Recall', 'Test_Recall'], index=[model_name])\n\n  return result_df\n\n# Plot the Confusion Matrix\ndef plot_confusion_matrix(model_dict, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test):\n\n  model_name = list(model_dict.keys())[0]\n  model_obj = list(model_dict.values())[0]\n\n  # Make predictions\n\n  # 1. Training predictions\n  train_preds = model_obj.predict(X_train)\n\n  # 2. Testing predictions\n  test_preds = model_obj.predict(X_test)\n\n  # Compute Recall Score\n\n  # 1. Training Score\n  train_recall = confusion_matrix(y_train, train_preds)\n\n  # 2. Testing score\n  test_recall = confusion_matrix(y_test, test_preds)\n\n  # Plot the heatmap\n  fig, ax = plt.subplots(1, 2, figsize=(15,8))\n\n  # PLot the trainig matrix\n  sns.heatmap(train_recall, annot=True, cbar=False, ax=ax[0], fmt='g')\n  ax[0].set_xlabel(\"Predicted Values\")\n  ax[0].set_ylabel(\"Actual Values\")\n  ax[0].set_title(\"Training Set Results\")\n\n  # Plot the testing matrix\n  sns.heatmap(test_recall, annot=True, cbar=False, ax=ax[1], fmt='g')\n  ax[1].set_xlabel(\"Predicted Values\")\n  ax[1].set_ylabel(\"Actual Values\")\n  ax[1].set_title(\"Testing Set Results\")\n\n  fig.show()","bcfbff89":"# 1. Baseline Model -> KNN\nknn_clf = KNeighborsClassifier()\nknn_clf.fit(X_train, y_train)\n\n# Compute Scores and plot confusion matrix\nmodel_dict={'KNN' : knn_clf}\nknn_results = compute_recall_score(model_dict=model_dict)\nplot_confusion_matrix(model_dict=model_dict)","111c0b82":"knn_results","80f1fd93":"# 2. Logistic Regression\nlr_clf = LogisticRegressionCV()\nlr_clf.fit(X_train, y_train)\n\n# Compute Scores and plot confusion matrix\nmodel_dict={'LogisticRegression' : lr_clf}\nlr_results = compute_recall_score(model_dict=model_dict)\nplot_confusion_matrix(model_dict=model_dict)","1f2022c0":"lr_results","3d62fb58":"# 3. Decision Tree\ndt_clf = DecisionTreeClassifier()\ndt_clf.fit(X_train, y_train)\n\n# Compute Scores and plot confusion matrix\nmodel_dict={'DecisionTree' : dt_clf}\ndt_results = compute_recall_score(model_dict=model_dict)\nplot_confusion_matrix(model_dict=model_dict)","0959fbbe":"dt_results","4bb36745":"# 4. SVM\nsvm_clf = SVC()\nsvm_clf.fit(X_train, y_train)\n\n# Compute Scores and plot confusion matrix\nmodel_dict={'SVM' : svm_clf}\nsvm_results = compute_recall_score(model_dict=model_dict)\nplot_confusion_matrix(model_dict=model_dict)","ad16869f":"svm_results","56f0d5a7":"# 5. Random Forest\nrf_clf = RandomForestClassifier()\nrf_clf.fit(X_train, y_train)\n\n# Compute Scores and plot confusion matrix\nmodel_dict={'RandomForest' : rf_clf}\nrf_results = compute_recall_score(model_dict=model_dict)\nplot_confusion_matrix(model_dict=model_dict)","68f63d50":"rf_results","07a88240":"# 6. Extra Trees\next_clf = ExtraTreesClassifier()\next_clf.fit(X_train, y_train)\n\n# Compute Scores and plot confusion matrix\nmodel_dict={'ExtraTrees' : ext_clf}\next_results = compute_recall_score(model_dict=model_dict)\nplot_confusion_matrix(model_dict=model_dict)","6ecbb000":"ext_results","be131866":"# 7. XGBoost\nxgb_clf = XGBClassifier()\nxgb_clf.fit(X_train, y_train)\n\n# Compute Scores and plot confusion matrix\nmodel_dict={'XGBoost' : xgb_clf}\nxgb_results = compute_recall_score(model_dict=model_dict)\nplot_confusion_matrix(model_dict=model_dict)","fcfce340":"xgb_results","3b9e93cc":"# 8. LightGBM\nlgbm_clf = LGBMClassifier()\nlgbm_clf.fit(X_train, y_train)\n\n# Compute Scores and plot confusion matrix\nmodel_dict={'LightGBM' : lgbm_clf}\nlgbm_results = compute_recall_score(model_dict=model_dict)\nplot_confusion_matrix(model_dict=model_dict)","caf8f0f5":"lgbm_results","8fbf0e33":"# Concatenate the results\nfinal_results = pd.concat((knn_results, lr_results, \n                           svm_results, dt_results, \n                           rf_results, ext_results, \n                           xgb_results, lgbm_results), axis=0).sort_values(by='Test_Recall', ascending=False)\nfinal_results","603eb045":"# 1. Light GBM\nlgbm_params = {\"num_leaves\" : [31, 50, 70, 90, 110],\n               \"max_depth\" : [10, 20, 30, 40, 50, 60],\n               \"learning_rate\" : [0.1, 0.5, 1, 1.5, 2.0],\n               \"n_estimators\" : [100, 150, 200, 250, 300, 350],\n               \"reg_alpha\" : [0.0, 0.25, 0.50, 0.75, 1.0, 2.0],\n               \"reg_lambda\" : [0.0, 0.25, 0.50, 0.75, 1.0, 2.0],\n               \"colsample_bytree\" : [0.0, 0.25, 0.50, 0.75, 1.0]\n               }\n\nlgbm_clf_2 = LGBMClassifier()\n\n# using randomised search cv\nrf_lgbm_clf = RandomizedSearchCV(lgbm_clf_2, lgbm_params, n_iter=20, scoring='recall', n_jobs=-1, cv=3, verbose=3, random_state=0)\nrf_lgbm_clf.fit(X_train, y_train)","44506900":"# Store the Best estimator\nlgbm_best = rf_lgbm_clf.best_estimator_\n\n# Generate Results\nmodel_dict = {'LGBM_Tuned' : lgbm_best}\nlgbm_best_results = compute_recall_score(model_dict)\nplot_confusion_matrix(model_dict)","d7d53504":"lgbm_best_results","0d5e8b52":"# 2. Extra Trees\next_params = { \"max_depth\" : [10, 20, 30, 40, 50, 60],\n               \"criterion\" : ['gini', 'entropy'],\n               \"n_estimators\" : [100, 150, 200, 250, 300, 350],\n               \"max_features\" : [\"auto\", \"sqrt\", \"log2\"],\n               \"min_samples_split\" : [2, 4, 6, 8, 10],\n               \"min_samples_leaf\" : [1, 2, 3, 4, 5, 6, 7],\n               \"bootstrap\" : [True, False]\n              }\n\next_clf_2 = ExtraTreesClassifier()\n\n# using randomised search cv\nrf_ext_clf = RandomizedSearchCV(ext_clf_2, ext_params, n_iter=20, scoring='recall', n_jobs=-1, cv=3, verbose=3, random_state=0)\nrf_ext_clf.fit(X_train, y_train)","73c07b94":"# Store the Best estimator\next_best = rf_ext_clf.best_estimator_\n\n# Generate Results\nmodel_dict = {'ExtraTrees_Tuned' : ext_best}\next_best_results = compute_recall_score(model_dict)\nplot_confusion_matrix(model_dict)","ff8b4a35":"ext_best_results","418c80fd":"# Train the best light GBM model on the entire dataset\nlgbm_best.fit(X_scaled, y)","14d69cfd":"# Dump the Hyperparameter tunes LGBM model\nmodel_file = '.\/LightGBM.pkl'\npickle.dump(lgbm_best, open(model_file, 'wb'))","0209b0a5":"**Logistic Regression seem to perform very poor**","c4b8f3bf":"**No forecasts of snow in the past 7 days in the country**","ceb95880":"**Statewise analysis of factors shows a more granular approach on finding relation between features and the chances of raining.**","0c228ce5":"**Overfitting can be observed in case of Decision Trees**","d3553bcf":"# EDA","f1f87b3c":"# Data Preprocessing","90e4d054":"# Feature Selection","6a329f69":"\n\n1.   State of Rajasthan has the maximum average temperature from 15\/10\/21 - 22\/10\/21.\n2.   State of Sikkim has the minimum average temperature from 15\/10\/21 - 22\/10\/21.\n\n","b4d56b02":"## Classification using ML techniques","68d5729e":"**From the above table, 'cloud' seems to be the feature which is makes a significant impact on predicting weather it will rain or not. (Which is quite logical).**","b924c851":"\n1.   XGBoost and SVM are giving generalised results.\n2.   Extra Trees, Decision Tree and Random Forest seem to overfit a little.\n","1ed58b1a":"# Classification","3b6e4b3b":"**precip_mm columns graph looks empty, lets plot again after altering xticks**","fcccd8ba":"## Hyperparameter Tuning\n\n**Tune the top 2 models i.e LightGBM and ExtraTrees**","7d8f64b2":"**Multicollinearity can be observed**"}}