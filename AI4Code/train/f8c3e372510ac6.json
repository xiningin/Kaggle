{"cell_type":{"f6b9cd10":"code","0b5a8e97":"code","dcf5a301":"code","1b7ba614":"code","08e3aef4":"code","28304e38":"code","13a123fa":"code","3609e433":"code","079b42db":"code","019c6bf0":"code","a2225d16":"code","4056607d":"code","80762199":"code","7174e3db":"code","91151970":"code","70544f49":"code","bf1c9920":"code","c72b43f8":"code","14321386":"code","1006dcd7":"code","8d15c2ff":"code","b792397c":"code","e87a208d":"code","cc591d71":"markdown","b627130b":"markdown","c408fa97":"markdown","d86124a8":"markdown","0c3698b1":"markdown","5bf02a1f":"markdown","e36ac6ef":"markdown","935dac55":"markdown","87c2bedf":"markdown","9d468d99":"markdown","412bc15b":"markdown","96e4a8e6":"markdown","f16668dd":"markdown","e5025722":"markdown","fa5c498b":"markdown","225e5d94":"markdown","8662537e":"markdown","72073bb9":"markdown","08668a87":"markdown","200ba785":"markdown","e1f6cfcb":"markdown"},"source":{"f6b9cd10":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import SGDClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.impute import SimpleImputer\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","0b5a8e97":"# load the data\ntrain_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-apr-2021\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-apr-2021\/test.csv')\n\ntrain_df.head()","dcf5a301":"feature_cols = train_df.drop(['Survived', 'PassengerId', 'Name'], axis=1).columns\n# split features with numerical and text values\nnum_cols = train_df[feature_cols].select_dtypes(include=['int64','float64']).columns\ncat_cols = train_df[feature_cols].select_dtypes(exclude=['int64','float64']).columns\n\nprint(num_cols)","1b7ba614":"print(train_df.info())\nprint('The share of survived in the train data: ' + str(np.mean(train_df[target_col])))","08e3aef4":"for i, col in enumerate(num_cols):\n    plt.figure(i)\n    sns.histplot(data=train_df[col])","28304e38":"#  make a dummy variable Male instead of Sex\ntrain_df['Male'] = 0\ntrain_df.loc[train_df['Sex'] == 'male', 'Male'] = 1","13a123fa":"#  later I will try to use 3 dummies for Pclass instead of Pclass itself\nPclass_1hot = pd.get_dummies(train_df.Pclass, prefix='Pclass')\n\n#. create dummies for 'Embarked' as well\nprint(train_df['Embarked'].unique())\nEmbarked_1hot = pd.get_dummies(train_df.Embarked, prefix='Embarked')\n\ntrain_df = pd.concat([train_df, Pclass_1hot, Embarked_1hot], axis=1)","3609e433":"# drop variable 'Sex', it is not needed anymore\ntrain_df = train_df.drop(['Sex', 'Embarked'], axis=1)","079b42db":"train_df.head(10)","019c6bf0":"# separate features from a target variable\nfeature_cols = train_df.drop(['Survived', 'PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1).columns\ntarget_col = 'Survived'","a2225d16":"# drop all rows with nan values for now\ntrain_df_wtNaN = train_df.dropna(axis=0)\n\n# split the sample into the train and validation subsamples\ny = train_df_wtNaN[target_col]\nX = train_df_wtNaN[feature_cols]\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)","4056607d":"model_Logistic = LogisticRegression(penalty='none', max_iter=1000, random_state=0).fit(X_train, y_train)\nmodel_Logistic.fit(X_train, y_train)\ny_pred = model_Logistic.predict(X_valid)\nscore_model_Logistic = accuracy_score(y_valid, y_pred)\nprint(score_model_Logistic)","80762199":"model_gnb = GaussianNB()\nmodel_gnb.fit(X_train, y_train)\ny_pred = model_gnb.predict(X_valid)\nscore_gnb = accuracy_score(y_valid, y_pred)\nprint(score_gnb)","7174e3db":"model_tree = tree.DecisionTreeClassifier(max_depth=10, min_samples_leaf=5, random_state=0)\nmodel_tree.fit(X_train, y_train)\ny_pred = model_tree.predict(X_valid)\nscore_tree = accuracy_score(y_valid, y_pred)\nprint(score_tree)","91151970":"model_RF = RandomForestClassifier(max_depth=10, min_samples_leaf=5, random_state=0)\nmodel_RF.fit(X_train, y_train)\ny_pred = model_RF.predict(X_valid)\nscore_RF = accuracy_score(y_valid, y_pred)\nprint(score_RF)","70544f49":"model_kNN = KNeighborsClassifier(n_neighbors=12)\nmodel_kNN.fit(X_train, y_train)\ny_pred = model_kNN.predict(X_valid)\nscore_kNN = accuracy_score(y_valid, y_pred)\nprint(score_kNN)","bf1c9920":"model_SVM = make_pipeline(StandardScaler(), SVC(gamma='auto'))\nmodel_SVM.fit(X_train, y_train)\ny_pred = model_SVM.predict(X_valid)\nscore_SVM = accuracy_score(y_valid, y_pred)\nprint(score_SVM)","c72b43f8":"mode_SVM_SGD = make_pipeline(StandardScaler(), SGDClassifier(max_iter=1000, tol=1e-3))\nmode_SVM_SGD.fit(X_train, y_train)\ny_pred = mode_SVM_SGD.predict(X_valid)\nscore_SVM_SGD = accuracy_score(y_valid, y_pred)\nprint(score_SVM_SGD)","14321386":"model_XGB = XGBClassifier()\nmodel_XGB.fit(X_train, y_train)\ny_pred = model_XGB.predict(X_valid)\nscore_XGB = accuracy_score(y_valid, y_pred)\nprint(score_XGB)","1006dcd7":"final_model = model_Logistic\nfinal_model.fit(X, y)","8d15c2ff":"test_df['Male'] = 0\ntest_df.loc[test_df['Sex'] == 'male', 'Male'] = 1\n\nPclass_1hot_test = pd.get_dummies(test_df.Pclass, prefix='Pclass')\n\n#. create dummies for 'Embarked' as well\nprint(test_df['Embarked'].unique())\nEmbarked_1hot_test = pd.get_dummies(test_df.Embarked, prefix='Embarked')\n\ntest_df = pd.concat([test_df, Pclass_1hot_test, Embarked_1hot_test], axis=1)\ntest_df = test_df.drop(['Sex', 'Embarked'], axis=1)\n\n","b792397c":"feature_cols_test = test_df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1).columns\nX_test = test_df[feature_cols_test]\n\nmy_imputer = SimpleImputer()\nimp_X_test = pd.DataFrame(my_imputer.fit_transform(X_test))\nimp_X_test.columns = X_test.columns\n\npreds_test = final_model.predict(imp_X_test)","e87a208d":"submission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-apr-2021\/sample_submission.csv')\nsubmission['Survived'] = preds_test\nsubmission.to_csv('submission.csv', index = False)","cc591d71":"## Other models\n### Gaussian Naive Bayes","b627130b":"Ok, now we have 75%, small improvement again, but the baseline is still the best.","c408fa97":"### KNeighborsClassifier","d86124a8":"The achieved accuracy is 74.4%.\n\nNothing outperforms the simplest baseline model so far, but I have to work on hyperparameters for all models that I tried.\nTo be continued.","0c3698b1":"The achieved score is 75%.","5bf02a1f":"## Encoding of Categorical Variables","e36ac6ef":"Results in accuracy 74.8%.\n\nAnother implementation of SVM: faster, but less accurate (accuracy is only 71.6%):","935dac55":"### Support Vector Classification","87c2bedf":"The resulting score is 72.4%","9d468d99":"### XGBClassifier","412bc15b":"The achieved score is 73.5% (small improvement!). Let't try with more trees then.","96e4a8e6":"There are missing values for Age, Ticket, Fare, Cabin(a lot of missing, but this variable doesn't seem to be useful), Embarked.\nThe share of the survived in the train data is not so different from 50%, the sample is about balanced.","f16668dd":"## Basic Data Visualisation","e5025722":"### DecisionTreeClassifier","fa5c498b":"The accuracy is only 68%.","225e5d94":"### RandomForestClassifier","8662537e":"## Data Preparation","72073bb9":"## Final model and submission","08668a87":"### Logistic Regression (without penalty)","200ba785":"## Baseline Model","e1f6cfcb":"To be continued"}}