{"cell_type":{"eb8670b5":"code","5d6310cf":"code","ae53cffb":"code","a9cd30a8":"code","3b72984c":"code","d46b138d":"code","6a8cd9ae":"code","b03663be":"code","27492ab4":"code","bd6b88bf":"code","cc6e371d":"code","756d9ff6":"code","966383db":"code","8803bd62":"code","f2dcbe4d":"code","65bc1c87":"code","6819b9b8":"code","b9160758":"code","13e14b7b":"code","551340ba":"code","4fa57089":"code","6bd076da":"code","c1328f5a":"code","60b3ebc3":"code","1b40a22e":"code","e2b9ceb3":"code","0c4d14b7":"code","d2f74c1a":"code","7c5264df":"code","ce7df984":"code","d80a6087":"code","bebd41ac":"markdown","2f602dc1":"markdown","a40d2a06":"markdown","a96632b3":"markdown","866b86b7":"markdown","93309541":"markdown","dd813310":"markdown","ef7500f6":"markdown","0e44c20e":"markdown","25f6110c":"markdown","120cba57":"markdown","67b839c4":"markdown"},"source":{"eb8670b5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport gc\n\nimport os\ntmp_dir='..\/input'\nprint(os.listdir(tmp_dir))\n\n\n# Any results you write to the current directory are saved as output.","5d6310cf":"train=pd.read_csv(tmp_dir+'\/train.csv')\ntest=pd.read_csv(tmp_dir+'\/test.csv')\ncol_describe=pd.read_excel(tmp_dir+'\/Data_Dictionary.xlsx')\nprint(train.shape)\nprint(test.shape)\ntrain.head()","ae53cffb":"train.isnull().sum()","a9cd30a8":"test.isnull().sum()","3b72984c":"for i in [ 'feature_1', 'feature_2', 'feature_3']:\n    train[i]=train[i].astype('category')\n    test[i]=test[i].astype('category')","d46b138d":"train.describe(include='all')","6a8cd9ae":"test.describe()","b03663be":"sns.distplot(train.target)","27492ab4":"train.target.value_counts().sort_index()","bd6b88bf":"train['target2']=train.target.map(lambda x:round(x,3))\ntrain=train[train.target2!=-33.219]\ndel train['target2']","cc6e371d":"### fill null \na=test.first_active_month.value_counts()\ntest.first_active_month.fillna(a[a.values==a.max()].index[0],inplace=True)","756d9ff6":"print(train.first_active_month.min())\nprint(train.first_active_month.max())\nprint(test.first_active_month.min())\nprint(test.first_active_month.max())\n\ntrain.first_active_month.describe()\n","966383db":"## count by first_active_month\ntest_month_count=test.first_active_month.value_counts().sort_index()\ntrain_month_count=train.first_active_month.value_counts().sort_index()\n\nplt.plot(test_month_count.index,test_month_count.values)\nplt.plot(train_month_count.index,train_month_count.values)","8803bd62":"## mean of target by first_active_month\n\nmean_target=train.groupby('first_active_month')['target'].mean().sort_index()\nplt.plot(mean_target.index,mean_target.values)","f2dcbe4d":"y_true=train.target\ndel train['target']\ntrain_count=train.shape[0]   \ndf=pd.concat([train,test],axis=0) ##train=df.iloc[:train_count]\ndel train\ndel test","65bc1c87":"df.first_active_month=df.first_active_month.astype('category')\ndf_dummy=pd.get_dummies(df[['feature_1','feature_2','feature_3','first_active_month']])\ndf=pd.concat([df,df_dummy],axis=1)\ndel df_dummy\n\ndf['first_active_month']=pd.to_datetime(df['first_active_month'])\ndf['first_month']=df.first_active_month.dt.month\nmax_date=df.first_active_month.dt.date.max()\ndf['lantency']=max_date - df.first_active_month.dt.date\ndf['lantency']=df.lantency.dt.days\n\ndf=df.drop(['feature_1', 'feature_2','feature_3','first_active_month'], axis=1)\ndf.first_month=df.first_month.astype('int8')\ndf.lantency=df.lantency.astype('int8')\ngc.collect()\ndf.head()\n","6819b9b8":"### merchants.csv - additional information about all merchants \/ merchant_ids in the dataset.\n# merchants=pd.read_csv(tmp_dir+'\/merchants.csv')\n# print(merchants.shape)\n# merchants.head()","b9160758":"# a=merchants.merchant_id.value_counts()\n# merchants2=merchants.groupby('merchant_id').head(1)\n# merchants2.head()","13e14b7b":"##new_merchant_transactions.csv - two months' worth of data for each card_id \n###containing ALL purchases that card_id made at merchant_ids that were not visited in the historical data.\n\nnew_merchant_transactions=pd.read_csv(tmp_dir+'\/new_merchant_transactions.csv')\nprint(new_merchant_transactions.shape)\nfor i in [ 'city_id','category_1','merchant_category_id','category_2','state_id',\n          'subsector_id','authorized_flag','category_3','merchant_id']:\n    new_merchant_transactions[i]=new_merchant_transactions[i].astype('category')\nnew_merchant_transactions.month_lag=new_merchant_transactions.month_lag.astype('int8')\nnew_merchant_transactions.installments=new_merchant_transactions.installments.astype('int8')\nnew_merchant_transactions.purchase_amount=new_merchant_transactions.purchase_amount.astype('float32')\n\nprint(new_merchant_transactions.dtypes)\nnew_merchant_transactions.head()","551340ba":"# new_merchant_transactions.head()\n# new_merchant_transactions.describe(include='category')\n# new_merchant_transactions.describe()\n# new_merchant_transactions.isnull().sum()","4fa57089":"from sklearn import preprocessing\nle=preprocessing.LabelEncoder()\nle.fit(new_merchant_transactions['category_1'])\nnew_merchant_transactions['category_1']=le.transform(new_merchant_transactions['category_1'])\nnew_merchant_transactions['category_1']=new_merchant_transactions['category_1'].astype('int8')\nnew_merchant_transactions['purchase_amount']=new_merchant_transactions['purchase_amount'].astype('float32')\n\n \ntmp=pd.get_dummies(new_merchant_transactions[['category_2','category_3']])\nnew_merchant_transactions=pd.concat([new_merchant_transactions,tmp],axis=1)\n\nnew_merchant_transactions.purchase_date=pd.to_datetime(new_merchant_transactions.purchase_date)\nfor i in ['hour','day','dayofweek','month']:\n    new_merchant_transactions['purchase_'+i]=getattr(new_merchant_transactions.purchase_date.dt,i)\n    new_merchant_transactions['purchase_'+i]=new_merchant_transactions['purchase_'+i].astype('int8')\nnew_merchant_transactions.head()","6bd076da":"%%time\n    \nagg_fun={'category_1': ['sum', 'mean'],\n'category_2_1.0': ['sum', 'mean'],\n'category_2_2.0': ['sum', 'mean'],\n'category_2_3.0': ['sum', 'mean'],\n'category_2_4.0': ['sum', 'mean'],\n'category_2_5.0': ['sum', 'mean'],\n'category_3_A': ['sum', 'mean'],\n'category_3_B': ['sum', 'mean'],\n'category_3_C': ['sum', 'mean'],\n'month_lag': ['sum', 'mean'],\n'installments': ['sum', 'mean','min','max','std'],\n'purchase_amount': ['sum', 'mean','min','max','std'],\n        \n'state_id':['nunique'], \n'city_id': ['nunique'],\n'merchant_category_id': ['nunique'],\n'subsector_id': ['nunique'],\n'merchant_id': ['nunique'],\n         \n'purchase_hour': ['min', 'max'],\n'purchase_day': ['min', 'max'],\n'purchase_dayofweek': ['min', 'max'],\n'purchase_month': ['min', 'max'],\n\n'card_id':'count',\n}\n\nnew_agg=new_merchant_transactions.groupby('card_id').agg(agg_fun)\n\nnew_agg.columns=['new_' + '_'.join(col).strip() for col in new_agg.columns.values]\ndel new_merchant_transactions\ngc.collect()\n# new_agg.to_csv('new_agg.csv')","c1328f5a":"%%time\n#### historical_transactions.csv - up to 3 months' worth of historical transactions for each card_id\n\nhistorical_transactions=pd.read_csv(tmp_dir+'\/historical_transactions.csv')\nprint(historical_transactions.shape)\nfor i in [ 'city_id','category_1','merchant_category_id','category_2','state_id',\n          'subsector_id','authorized_flag','category_3','merchant_id']:\n    historical_transactions[i]=historical_transactions[i].astype('category')\nhistorical_transactions['month_lag']=historical_transactions['month_lag'].astype('int8')\nhistorical_transactions.installments=historical_transactions.installments.astype('int8')\nhistorical_transactions.purchase_amount=historical_transactions.purchase_amount.astype('float32')\nprint(historical_transactions.dtypes)\nhistorical_transactions.head()","60b3ebc3":"le=preprocessing.LabelEncoder()\nle.fit(historical_transactions['category_1'])\nhistorical_transactions['category_1']=le.transform(historical_transactions['category_1'])\n\ntmp=pd.get_dummies(historical_transactions[['category_2','category_3']])\nhistorical_transactions=pd.concat([historical_transactions,tmp],axis=1)\n\nhistorical_transactions.purchase_date=pd.to_datetime(historical_transactions.purchase_date)\nfor i in ['hour','day','dayofweek','month']:\n    historical_transactions['purchase_'+i]=getattr(historical_transactions.purchase_date.dt,i)\n    historical_transactions['purchase_'+i]=historical_transactions['purchase_'+i].astype('int8')\nhistorical_transactions.head()","1b40a22e":"%%time\n    \nagg_fun={'category_1': ['sum', 'mean'],\n'category_2_1.0': ['sum', 'mean'],\n'category_2_2.0': ['sum', 'mean'],\n'category_2_3.0': ['sum', 'mean'],\n'category_2_4.0': ['sum', 'mean'],\n'category_2_5.0': ['sum', 'mean'],\n'category_3_A': ['sum', 'mean'],\n'category_3_B': ['sum', 'mean'],\n'category_3_C': ['sum', 'mean'],\n'month_lag': ['sum', 'mean'],\n'installments': ['sum', 'mean','min','max','std'],\n'purchase_amount': ['sum', 'mean','min','max','std'],\n        \n'state_id':['nunique'], \n'city_id': ['nunique'],\n'merchant_category_id': ['nunique'],\n'subsector_id': ['nunique'],\n'merchant_id': ['nunique'],\n         \n'purchase_hour': ['min', 'max'],\n'purchase_day': ['min', 'max'],\n'purchase_dayofweek': ['min', 'max'],\n'purchase_month': ['min', 'max'],\n\n'card_id':'count',\n}\n\nhis_agg=historical_transactions.groupby('card_id').agg(agg_fun)\nhis_agg.columns=['his_' + '_'.join(col).strip() for col in his_agg.columns.values]\ndel historical_transactions\ngc.collect()\n# his_agg.to_csv('new_agg.csv')","e2b9ceb3":"import lightgbm as lgb\nfrom sklearn.model_selection import GridSearchCV","0c4d14b7":"df=pd.merge(df,new_agg,left_on='card_id',right_index=True,how='left')\nprint(len(df.columns))\ndf=pd.merge(df,his_agg,left_on='card_id',right_index=True,how='left')\nprint(len(df.columns))\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\n\ndf=reduce_mem_usage(df)\ndf=df.set_index('card_id')\n\ndel new_agg\ndel his_agg\ndf_train=df.iloc[:train_count]\ndf_test=df.iloc[train_count:]\nprint('df_train.shape',df_train.shape,'df_test.shape',df_test.shape)\ndel df\ngc.collect()","d2f74c1a":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test =train_test_split(df_train, y_true, test_size=0.2, random_state=8)\n# del df_train","7c5264df":"%%time\n## trian by raw lightgbm\nfrom sklearn.metrics import mean_squared_error\ntrain_columns=X_train.columns.tolist()\ntrain_data = lgb.Dataset(X_train, label=y_train,feature_name=train_columns\n                        ,free_raw_data=False)\n\nparam = {'boosting_type':['gbdt'],'max_leaves':[33], 'min_data_in_leaf':[30],'max_depth':[5],\n         'objective':['regression'],'random_state':[8],'metric':['l2']}\nnum_round=100\nbst=lgb.train(param,train_data,num_round)\nmean_squared_error(y_test,bst.predict(X_test))\n","ce7df984":"## train by sklearn GridSearchCV\n\n# from sklearn.metrics import mean_squared_error\n\n\n# param = {'boosting_type':['gbdt'],'max_leaves':[33], 'min_data_in_leaf':[30,40,50],'max_depth':[4,5,6],\n#          'objective':['regression'],'random_state':[8],'metric':['l2']}\n\n# clf=GridSearchCV(lgb.LGBMRegressor(),param_grid=param,cv=5,n_jobs=4,verbose=2)\n# clf.fit(X_train, y_train,feature_name=['first_active_month', 'feature_1', 'feature_2', 'feature_3'],\n#         categorical_feature=['first_active_month', 'feature_1', 'feature_2', 'feature_3'])\n\n# mean_squared_error(y_test,clf.best_estimator_.predict(X_test))","d80a6087":"df_test.head()\n## predict by raw lightgbm\npredict_target=bst.predict(df_test)\n\n## predict  sklearn GridSearchCV\n# predict_target=clf.best_estimator_.predict(X_predict)\n\ndf_test['target']=predict_target\ndf_test=df_test.reset_index()\ndf_test[['card_id','target']].to_csv('submission.csv',index=False)\nprint('success asv to csv submission.csv')\n","bebd41ac":"## predict","2f602dc1":"## target","a40d2a06":"# history","a96632b3":"##  new_merchant_transactions","866b86b7":"## describe","93309541":"### abnormal\n\nit seems that -33.219281 is abnormal,I remove these data","dd813310":"## first_active_month","ef7500f6":"# train and test","0e44c20e":"\n## FeatureEngin ","25f6110c":"## merchants","120cba57":"#  new_merchant_transactions","67b839c4":"# baseline"}}