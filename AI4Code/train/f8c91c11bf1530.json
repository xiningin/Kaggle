{"cell_type":{"bfcc5797":"code","a336b287":"code","6a0160f8":"code","8985d9bc":"code","66f62616":"code","25a74c9a":"code","58086a64":"code","c8cf00b4":"code","a5def2c0":"code","770d0f1f":"code","a5842e28":"code","129f7ac6":"code","2a236ef1":"code","9e99d4be":"code","d836c1ec":"code","9ff22183":"code","d0dcc9b9":"code","a01dd22f":"code","1e5a886f":"code","d6a864c9":"code","203fb58f":"code","68a10301":"code","e1ec353c":"code","8984aed1":"code","8e0dfb0f":"code","2ea8e26b":"code","63ca704e":"code","12a92239":"code","551366cd":"code","e806f3dd":"code","a3ef5b10":"code","ac4d8fa5":"code","5e829e62":"code","e9c9b658":"code","173e3aa4":"code","d404589c":"code","0ae52643":"code","36d76bd2":"code","86ffedd9":"markdown","df6326c4":"markdown","6142cdb1":"markdown","d2f5c8bb":"markdown","7783ef07":"markdown","1c2fd4d4":"markdown","959c9c0b":"markdown","49fc672d":"markdown","9911dc3b":"markdown"},"source":{"bfcc5797":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntitanic = pd.read_csv('..\/input\/train.csv')\ntitanic.head()","a336b287":"titanic.isnull().sum()","6a0160f8":"titanic.describe()","8985d9bc":"titanic.groupby(titanic['Age'].isnull())['Survived'].mean()","66f62616":"titanic[\"Age\"].fillna(titanic['Age'].mean(), inplace=True)\ntitanic.head(10)","25a74c9a":"for i, col in enumerate(['SibSp', 'Parch']):\n    plt.figure(i)\n    sns.catplot(y='Survived', x=col, data=titanic, kind='point', aspect=2)","58086a64":"titanic['Famity_cnt'] = titanic['SibSp'] + titanic['Parch']","c8cf00b4":"titanic.drop(['SibSp', 'Parch', 'PassengerId'], axis=1, inplace=True)","a5def2c0":"titanic.head(10)","770d0f1f":"titanic.isnull().sum()","a5842e28":"titanic.groupby(titanic['Cabin'].isnull())['Survived'].mean()","129f7ac6":"titanic['Cabin_ind'] = np.where(titanic['Cabin'].isnull(), 0, 1)\ntitanic.head(10)","2a236ef1":"gender = {'male':0, 'female':1}\ntitanic['Sex'] = titanic['Sex'].map(gender)","9e99d4be":"titanic.head()","d836c1ec":"titanic.drop(['Name', 'Ticket', 'Cabin', 'Embarked'], axis=1, inplace=True)\ntitanic.head()","9ff22183":"titanic.to_csv('titanic_cleaned.csv', index=False)","d0dcc9b9":"from sklearn.model_selection import train_test_split","a01dd22f":"features = titanic.drop('Survived', axis=1)\nlabels = titanic['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.4, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)","1e5a886f":"for dataset in [y_train, y_val, y_test]:\n    print(round(len(dataset)\/len(labels), 2))","d6a864c9":"X_train.to_csv('train_features.csv', index=False)\nX_val.to_csv('val_features.csv', index=False)\nX_test.to_csv('test_features.csv', index=False)\n\ny_train.to_csv('train_labels', index=False)\ny_val.to_csv('val_labels', index=False)\ny_test.to_csv('test_labels', index=False)","203fb58f":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\nimport warnings\n\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore', category=DeprecationWarning)","68a10301":"def print_result(cv):\n    results = cv.cv_results_\n    print(f\"Best parameterers {cv.best_params_}\")\n    for mean_train, mean_test, params in zip(results['mean_train_score'], results['mean_test_score'], results['params']):\n        print(f\"mean train score: {round(mean_train, 2)}  mean test score: {round(mean_test, 2)} for {params}\")","e1ec353c":"lr = LogisticRegression()\nparameters = {\n    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n}\ncv = GridSearchCV(lr, parameters, cv=5)\ncv.fit(X_train, y_train.values.ravel())","8984aed1":"print_result(cv)","8e0dfb0f":"cv.best_estimator_","2ea8e26b":"import joblib\njoblib.dump(cv.best_estimator_, 'LR_model.pkl')","63ca704e":"from sklearn.svm import SVC\n\nsvc = SVC()\nparameters = {\n    'C': [0.1, 1, 10],\n    'kernel': ['linear', 'rbf']\n}\n\ncv = GridSearchCV(svc, parameters, cv=5)\ncv.fit(X_train, y_train.values.ravel())","12a92239":"print_result(cv)","551366cd":"joblib.dump(cv.best_estimator_, 'SVM_model.pkl')","e806f3dd":"from sklearn.neural_network import MLPClassifier\n\nmlp = MLPClassifier()\n\nparameters ={\n    'hidden_layer_sizes' : [(10,), (50,), (10,)],\n    'activation': ['relu', 'tanh', 'logistic'],\n    'learning_rate': ['constant', 'invscaling', 'adaptive']\n}\n\ncv = GridSearchCV(mlp, parameters, cv=5)\n\ncv.fit(X_train, y_train.values.ravel())\n\nprint_result(cv)\n\njoblib.dump(cv.best_estimator_, 'MLP_model.pkl')","a3ef5b10":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier()\nparameters = {\n    'n_estimators':[5, 50, 250],\n    'max_depth': [2, 4, 8, 16, 32, None]\n}\ncv = GridSearchCV(rf, parameters, cv=5)\ncv.fit(X_train, y_train.values.ravel())\nprint_result(cv)","ac4d8fa5":"joblib.dump(cv.best_estimator_, 'RF_model.pkl')","5e829e62":"from sklearn.ensemble import GradientBoostingClassifier\n\ngb = GradientBoostingClassifier()\nparameters ={\n    'n_estimators': [5, 50, 250, 500],\n    'max_depth': [1, 3, 5, 7, 9],\n    'learning_rate':[0.1, 1, 10, 100]\n}\n\ncv = GridSearchCV(gb, parameters, cv=5)\ncv.fit(X_train, y_train.values.ravel())\nprint_result(cv)","e9c9b658":"joblib.dump(cv.best_estimator_, 'GB_model.pkl')","173e3aa4":"from sklearn.metrics import accuracy_score, precision_score, recall_score\nimport time\ndef evaluate_model(name, model, features, labels):\n    start = time.time()\n    predictions = model.predict(features)\n    end = time.time()\n    accuracy = accuracy_score(labels, predictions)\n    precision = precision_score(labels, predictions)\n    recall = recall_score(labels, predictions)\n    print(f\"{name} accuracy: {round(accuracy, 3)} precision: {round(precision, 3)} recall: {round(recall, 3)}\")","d404589c":"models = {}\nfor name in ['LR','SVM', 'MLP', 'RF', 'GB']:\n    models[name] = joblib.load(f\"{name}_model.pkl\")","0ae52643":"for model_name, model in models.items():\n    evaluate_model(model_name, model, X_val, y_val.values.ravel())","36d76bd2":"evaluate_model('Gradient Boosted Trees', models['GB'], X_test, y_test)","86ffedd9":"**Random Forest**","df6326c4":"The **C** hyperparameter is a penalty that determines how closely the model fits to the training set.\n\n![](https:\/\/i.stack.imgur.com\/07jiy.png)\n\nLow value of C will apply a low penalty for misclassifying an example in the training set. Hence model will be more wiling to missiclassify examples in training set if it thinks it is doing a better job of capturing the overall pattern in the data.\nHigh value of C will apply a high penalty for missclassifying an example in the training set. Hence, model will try to fit to each example in the training set resulting in overfitting.","6142cdb1":"****SVM****","d2f5c8bb":"**Clean continuos varibales**","7783ef07":"**Split data into train, validatio and test set**","1c2fd4d4":"Support vector machine is a supervised machine learning algorith which classifies data by findig an optimal hyperplane that maximises margin between two classes.\n\n![](https:\/\/www.aitrends.com\/wp-content\/uploads\/2018\/01\/1-19SVM-2.jpg)\n\nIt maximizes the margin between the decision boundry and the closest points.\n\n**Kernel**   \nKernel method transforms the data which is not linearly separable in n-dimensional space to a higher dimension where it is linearly separable.\n\n![](https:\/\/blog-c7ff.kxcdn.com\/blog\/wp-content\/uploads\/2017\/02\/kernel.png)","959c9c0b":"**Clean categorical variables**","49fc672d":"**Drop unnecessary variables**","9911dc3b":"**Gradient Boosted trees**"}}