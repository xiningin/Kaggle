{"cell_type":{"6b6a2c6f":"code","dca3554b":"code","797087e6":"code","55b83050":"code","d5fd8018":"code","9869a951":"code","3648d38e":"code","50a01e91":"code","6cc2e98d":"code","4078981b":"code","924df970":"code","d26b9dca":"code","76a31ab8":"code","dd6a3c63":"code","31adfdc0":"code","4453cb30":"code","43de0233":"code","81d73272":"code","502a0f6a":"code","19369aeb":"code","237bcb75":"code","557e3ffd":"code","b49b7ed7":"code","cd40f293":"code","a96d8603":"code","d841a3eb":"code","158cab31":"code","0ec0cb0a":"code","7c827a58":"code","37a249cd":"code","2e1cf357":"code","3dd1f6e5":"code","4e40e291":"code","c7d15998":"code","d141df96":"code","0ab1e5c9":"code","70caf6a1":"code","7dcf2528":"code","c736c5b5":"code","94a77d6b":"code","bbff8d7b":"code","22052697":"code","8d34600f":"code","0fa7d8a6":"code","bd78b6f1":"code","20fd075f":"code","42aa370c":"code","138c1956":"code","5e2ab90c":"code","d1c972ce":"code","ae7c1a5d":"code","c43d3862":"code","933cc781":"code","e6bb947b":"code","e26d7afa":"code","96a77c6f":"code","7d721527":"markdown","9cd93eaa":"markdown","81a2be4f":"markdown","aa2bca78":"markdown","e22c5347":"markdown","5bfb15fa":"markdown","739b6b82":"markdown","2170b045":"markdown","7b21a2e0":"markdown","f67cad33":"markdown","57af569a":"markdown","0b9a917e":"markdown","50ecbc90":"markdown","d27636e0":"markdown","f4ce2e99":"markdown","562fb5f4":"markdown","a95be13e":"markdown","3c21018d":"markdown","dc666e0c":"markdown","fe044b5c":"markdown","47417298":"markdown","11c51e63":"markdown","e3a5794b":"markdown","0a5c361e":"markdown","1c6800d3":"markdown","5be7b687":"markdown","d3f3dd3e":"markdown","080f50ef":"markdown","47e9d21d":"markdown","75c9f85e":"markdown","8e14b194":"markdown","6defd3b7":"markdown","22265d54":"markdown","c5ef65c8":"markdown","a1198998":"markdown","11c67cb1":"markdown","78f55212":"markdown","4a5d56e8":"markdown","9c359fe2":"markdown","d7425b83":"markdown","8e5981d5":"markdown","d018dfa0":"markdown","b8b79d60":"markdown","4eda6e71":"markdown","69ffeee5":"markdown","f3a4466e":"markdown","cb278229":"markdown","299b4cfd":"markdown","20a667a0":"markdown","e748a683":"markdown","1d7ce882":"markdown","15f7e72a":"markdown","ac3f233d":"markdown","f9fe3d3b":"markdown","c0adebf5":"markdown","7c39f7dd":"markdown","b86c43f8":"markdown","c542fb6d":"markdown","580396d5":"markdown","3b6ce8f3":"markdown","4a2ed6f6":"markdown","8cf7e197":"markdown","d7d4dd72":"markdown","4d393063":"markdown","ef679078":"markdown","937fd0b8":"markdown","14b366e3":"markdown","f2cba17e":"markdown","71a3813f":"markdown","d95c8134":"markdown","610dc6e6":"markdown","32cf09fd":"markdown","2129264d":"markdown","6d81355c":"markdown","47a352e2":"markdown","91a108a7":"markdown","51bbed81":"markdown","575aa9c4":"markdown","c115ec09":"markdown","8060d3b4":"markdown","f2d907f3":"markdown"},"source":{"6b6a2c6f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.utils import shuffle\nfrom sklearn.ensemble import RandomForestClassifier\n\npd.set_option('display.max_columns', 100)","dca3554b":"train = pd.read_csv('..\/input\/porto-seguro-safe-driver-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/porto-seguro-safe-driver-prediction\/test.csv')","797087e6":"train.head()","55b83050":"train.shape","d5fd8018":"train.info()","9869a951":"train.shape","3648d38e":"test.shape","50a01e91":"train.drop_duplicates()   \ntrain.shape","6cc2e98d":"train.info()","4078981b":"#Create a frame that stores multiple data \ndata = []\nfor f in train.columns:\n    #Define the role\n    if f == 'target':\n        role = 'target'\n    elif f =='id':\n        role = 'id'\n    else:\n        role = 'input'\n    \n    #Define the level \n    if 'bin' in f or f =='target':\n        level = 'binary'\n    elif 'cat' in f or f == 'id':\n        level = 'nominal'\n    elif train[f].dtype == 'float64':\n        level = 'interval'\n    elif train[f].dtype == 'int64':\n        level = 'ordinal'\n    \n    #Define the keep\n    keep = True\n    if f =='id':\n        keep = False\n    \n    #Defining the dtype\n    dtype = train[f].dtype\n    \n    #Creating a Dict that contains all the metadata for the variable \n    f_dict = {\n        'varname': f,\n        'role': role,\n        'level': level,\n        'keep': keep,\n        'dtype': dtype\n    }\n    data.append(f_dict)\n    \nmeta = pd.DataFrame(data, columns = ['varname', 'role', 'level', 'keep', 'dtype'])\nmeta.set_index('varname', inplace=True)","924df970":"meta","d26b9dca":"meta.loc[(meta.level == 'nominal') & (meta.keep)].index","76a31ab8":"pd.DataFrame({'count' : meta.groupby(['role', 'level'])['role'].size()}).reset_index()","dd6a3c63":"v = meta[(meta.level == 'interval') & (meta.keep)].index\ntrain[v].describe()","31adfdc0":"v = meta[(meta.level == 'ordinal') & (meta.keep)].index\ntrain[v].describe()","4453cb30":"v = meta[(meta.level == 'binary') & (meta.keep)].index\ntrain[v].describe()","43de0233":"features = train.drop(['id','target'], axis=1).values\ntargets = train.target.values\n\nax = sns.countplot(x = targets ,palette=\"rocket_r\")\nsns.set(font_scale=1.5)\nax.set_xlabel(' ')\nax.set_ylabel(' ')\nfig = plt.gcf()\nfig.set_size_inches(10,5)\nax.set_ylim(top=700000)\nfor p in ax.patches:\n    ax.annotate('{:.2f}%'.format(100*p.get_height()\/len(targets)), (p.get_x()+ 0.3, p.get_height()+10000))\n\nplt.title('Distribution of 595212 Targets')\nplt.xlabel('Initiation of Auto Insurance Claim Next Year')\nplt.ylabel('Frequency [%]')\nplt.show()","81d73272":"desired_apriori=0.10\n\n# Get the indices per target value\nidx_0 = train[train.target == 0].index\nidx_1 = train[train.target == 1].index\n\n# Get original number of records per target value\nnb_0 = len(train.loc[idx_0])\nnb_1 = len(train.loc[idx_1])\n\n# Calculate the undersampling rate and resulting number of records with target=0\nundersampling_rate = ((1-desired_apriori)*nb_1)\/(nb_0*desired_apriori)\nundersampled_nb_0 = int(undersampling_rate*nb_0)\nprint('Rate to undersample records with target=0: {}'.format(undersampling_rate))\nprint('Number of records with target=0 after undersampling: {}'.format(undersampled_nb_0))\n\n# Randomly select records with target=0 to get at the desired a priori\nundersampled_idx = shuffle(idx_0, random_state=37, n_samples=undersampled_nb_0)\n\n# Construct list with remaining indices\nidx_list = list(undersampled_idx) + list(idx_1)\n\n# Return undersample data frame\ntrain = train.loc[idx_list].reset_index(drop=True)","502a0f6a":"train['target'].value_counts","19369aeb":"train_target = train[train['target'] ==0]\ntrain_target.value_counts","237bcb75":"train_target2 = train[train['target'] ==1]\ntrain_target2.value_counts","557e3ffd":"vars_with_missing = []\n\nfor f in train.columns:\n    missings = train[train[f] == -1][f].count()\n    if missings > 0:\n        vars_with_missing.append(f)\n        missings_perc = missings \/ train.shape[0]\n        \n        print('Variable {} has {} records ({:.2%}) with missing values'.format(f, missings, missings_perc))\nprint('In total, there are {} variables with missing values'.format(len(vars_with_missing)))        \n        ","b49b7ed7":"#you can see that it doesn't mean much.\ntrain[['ps_car_03_cat', 'target']].groupby('ps_car_03_cat').mean()","cd40f293":"#Dropping the variables with too many missing values \nvars_to_drop = ['ps_car_03_cat', 'ps_car_05_cat']\ntrain.drop(vars_to_drop, inplace = True, axis= 1)\nmeta.loc[vars_to_drop, 'keep'] = False #Updating the meta ","a96d8603":"#Imputing with the mean or mode \n#IMP = missing values(=-1)\nmean_imp = SimpleImputer(missing_values = -1, strategy = 'mean')\nmode_imp = SimpleImputer(missing_values = -1, strategy = 'most_frequent')\n\ntrain['ps_reg_03'] = mean_imp.fit_transform(train[['ps_reg_03']]).ravel()\ntrain['ps_car_14'] = mean_imp.fit_transform(train[['ps_car_14']]).ravel()\ntrain['ps_car_11'] = mode_imp.fit_transform(train[['ps_car_11']]).ravel()","d841a3eb":"v = meta[(meta.level == 'nominal') & (meta.keep)].index \nsum = 0\n\nfor f in v: \n    dist_values = train[f].value_counts().shape[0]\n    sum += dist_values\n    print('Variable {} has {} distinct values'.format(f, dist_values))","158cab31":"# Script by https:\/\/www.kaggle.com\/ogrellier\n# Code: https:\/\/www.kaggle.com\/ogrellier\/python-target-encoding-for-categorical-features\ndef add_noise(series, noise_level):\n    return series * (1 + noise_level * np.random.randn(len(series)))\n\ndef target_encode(trn_series=None, \n                  tst_series=None, \n                  target=None, \n                  min_samples_leaf=1, \n                  smoothing=1,\n                  noise_level=0):\n\n    assert len(trn_series) == len(target)\n    assert trn_series.name == tst_series.name\n    temp = pd.concat([trn_series, target], axis=1)\n    # Compute target mean \n    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n    # Compute smoothing\n    smoothing = 1 \/ (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) \/ smoothing))\n    # Apply average function to all target data\n    prior = target.mean()\n    # The bigger the count the less full_avg is taken into account\n    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n    # Apply averages to trn and tst series\n    ft_trn_series = pd.merge(\n        trn_series.to_frame(trn_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=trn_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_trn_series.index = trn_series.index \n    ft_tst_series = pd.merge(\n        tst_series.to_frame(tst_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=tst_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_tst_series.index = tst_series.index\n    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)","0ec0cb0a":"def add_noise(series, noise_level):\n    return series * (1+ noise_level * np.random.randn(len(series)))","7c827a58":"assert len(train[\"ps_car_11_cat\"]) == len(train['target'])\nassert train[\"ps_car_11_cat\"].name == test[\"ps_car_11_cat\"].name","37a249cd":"temp = pd.concat([train['ps_car_11_cat'], train['target']], axis=1)\nprint(temp)","2e1cf357":"averages = temp.groupby(train['ps_car_11_cat'].name)[train['target'].name].agg(['mean', 'count'])\nprint(averages)","3dd1f6e5":"smoothing = 1 \/ (1 + np.exp(-(averages[\"count\"] - 100) \/ 10))\nprint(smoothing)","4e40e291":"prior = train['target'].mean()\n\naverages[train['target'].name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\nprint(averages)","c7d15998":"averages.drop(['mean', 'count'], axis=1, inplace=True)","d141df96":"ft_trn_series = pd.merge(\n        train['ps_car_11_cat'].to_frame(train['ps_car_11_cat'].name),\n        averages.reset_index().rename(columns={'index': train['target'].name, train['target'].name: 'average'}),\n        on=train['ps_car_11_cat'].name,\n        how='left')['average'].rename(train['ps_car_11_cat'].name + '_mean').fillna(prior)\n\nprint(ft_trn_series)","0ab1e5c9":"ft_trn_series.index = train[\"ps_car_11_cat\"].index ","70caf6a1":"ft_tst_series = pd.merge(test['ps_car_11_cat'].to_frame(test['ps_car_11_cat'].name),\n                              averages.reset_index().rename(columns={'index' : 'target', 'target' : 'averages'}),\n                              on=test['ps_car_11_cat'].name,\n                              how='left')['averages'].rename(train['ps_car_11_cat'].name + '_mean').fillna(prior)\n\nft_tst_series.index = test[\"ps_car_11_cat\"].index","7dcf2528":"add_noise(ft_trn_series, 0.01), add_noise(ft_tst_series, 0.01)","c736c5b5":"train_encoded, test_encoded = target_encode(train[\"ps_car_11_cat\"], \n                             test[\"ps_car_11_cat\"], \n                             target=train.target, \n                             min_samples_leaf=100,\n                             smoothing=10,\n                             noise_level=0.01)\n    \ntrain['ps_car_11_cat_te'] = train_encoded\ntrain.drop('ps_car_11_cat', axis=1, inplace=True)\nmeta.loc['ps_car_11_cat','keep'] = False  # Updating the meta\ntest['ps_car_11_cat_te'] = test_encoded\ntest.drop('ps_car_11_cat', axis=1, inplace=True)","94a77d6b":"v = meta[(meta.level == 'nominal') & (meta.keep)].index\n\nfor f in v:\n    #plt.figure()\n    fig, ax = plt.subplots(figsize=(20,10))\n    #Calculate the percentage of target=1 per category value\n    cat_perc = train[[f,'target']].groupby([f],as_index=False).mean()\n    cat_perc.sort_values(by='target', ascending=False, inplace=True)\n   \n    # Bar plot\n    # Order the bars descending on target mean\n    sns.barplot(ax=ax, x=f, y='target', data=cat_perc, order=cat_perc[f])\n    plt.xlabel('%target')\n    plt.xlabel(f, fontsize=18)\n    plt.tick_params(axis='both', which='major')\n    plt.show();   ","bbff8d7b":"def corr_heatmap(v):\n    correlations = train[v].corr()\n\n    # Create color map ranging between two colors\n    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n    fig, ax = plt.subplots(figsize=(13,13))\n    sns.heatmap(correlations, cmap=cmap, vmax=1.0, center=0, fmt='.2f',\n                square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .75})\n    \n    plt.show();\n    \nv = meta[(meta.level == 'interval') & (meta.keep)].index\ncorr_heatmap(v)","22052697":"#Sample 10% of data from train data\ns = train.sample(frac = 0.1)","8d34600f":"sns.lmplot( 'ps_reg_02', 'ps_reg_03', data=s, hue='target', palette='rocket_r', height=8, scatter_kws = {'alpha': 0.3}) #\uc5ec\uae30\uc11c alpha\ub294 \ud22c\uba85\ub3c4\ub97c \ub9d0\ud574\uc8fc\ub294 \uac83\uc774\ub2e4. \nplt.show()","0fa7d8a6":"sns.lmplot('ps_car_12', 'ps_car_13', data=s, hue='target', palette='rocket_r', height=8, scatter_kws = {'alpha': 0.3}) #\uc5ec\uae30\uc11c alpha\ub294 \ud22c\uba85\ub3c4\ub97c \ub9d0\ud574\uc8fc\ub294 \uac83\uc774\ub2e4. \nplt.show()","bd78b6f1":"sns.lmplot('ps_car_12', 'ps_car_14', data=s, hue='target', palette='rocket_r', height=8, scatter_kws = {'alpha': 0.3}) #\uc5ec\uae30\uc11c alpha\ub294 \ud22c\uba85\ub3c4\ub97c \ub9d0\ud574\uc8fc\ub294 \uac83\uc774\ub2e4. \nplt.show()","20fd075f":"sns.lmplot('ps_car_15', 'ps_car_13', data=s, hue='target', palette='rocket_r', height=8, scatter_kws = {'alpha': 0.3}) #\uc5ec\uae30\uc11c alpha\ub294 \ud22c\uba85\ub3c4\ub97c \ub9d0\ud574\uc8fc\ub294 \uac83\uc774\ub2e4. \nplt.show()","42aa370c":"def corr_heatmap(v):\n    correlations = train[v].corr()\n\n    # Create color map ranging between two colors\n    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n    fig, ax = plt.subplots(figsize=(16,16))\n    sns.heatmap(correlations, cmap=cmap, vmax=1.0, center=0, fmt='.2f',\n                square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .75})\n    \n    plt.show();\n    \n\nv = meta[(meta.level == 'ordinal') & (meta.keep)].index\ncorr_heatmap(v)","138c1956":"v = meta[(meta.level == 'nominal') & (meta.keep)].index\nprint('Before dummification we have {} variables in train'.format(train.shape[1]))\n\ntrain = pd.get_dummies(train, columns=v, drop_first=True)\nprint('After dummification we have {} variables in train'.format(train.shape[1]))","5e2ab90c":"v = meta[(meta.level == 'interval') & (meta.keep)].index\n\npoly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\ninteractions = pd.DataFrame(data=poly.fit_transform(train[v]), columns=poly.get_feature_names(v))\ninteractions.drop(v, axis=1, inplace=True)  # Remove the original columns, \uae30\uc874\uc5d0 \uc788\ub358\uac83\ub4e4\uc740 \ube7c\ubc84\ub9b0\ub2e4\ub294 \uc758\ubbf8 \n# Concat the interaction variables to the train data\nprint('Before creating interactions we have {} variables in train'.format(train.shape[1])) #\uc774\uac74 \ubd99\uc774\uae30 \uc804\ntrain = pd.concat([train, interactions], axis=1)\nprint('After creating interactions we have {} variables in train'.format(train.shape[1])) #\uc774\uac74 \ubd99\uc774\uae30 \ud6c4 , \uadf8\ub9ac\uace0  axis \ub97c \uc606\uc73c\ub85c \ubd80\uccd0\uc57c\uc9c0 \uc880 \ubcf4\uae30 \uc88b\ub2e4. ","d1c972ce":"selector = VarianceThreshold(threshold=.01)\nselector.fit(train.drop(['id', 'target'], axis=1)) # Fit to train without id and target variables\n\nf = np.vectorize(lambda x : not x) # Function to toggle boolean array elements, \uc774\uac83\uc740 \ud55c\ubc88 \u3137\ub530\ub85c \ube7c\uc11c \uc368\ubcf4\uba74 \uc5b4\ub5a4 \uc758\ubbf8\uc778\uc9c0 \uc54c\uc218 \uc788\ub2e4. (\uc77c\ub2e8 \ubc11\uc5d0 \ub72f\uc5b4\ubcf4\uba74\uc11c \ubcf4\uae30)\n\n\nv = train.drop(['id', 'target'], axis=1).columns[f(selector.get_support())] #\uc77c\ub2e8 selector.get_support\uc774\uac70\ub294 true\uc778\uc9c0 false\uc778\uc9c0 \ub098\ud0c0\ub0b4\uc8fc\ub294 \uac83\uc774\ub2e4. \uc5ec\uae30\uc11c \ud569\uc744 \ud55c\ubc88\uc5d0 \ub300\uc785\ud574\ubc84\ub9ac\uba74 \nprint('{} variables have too low variance.'.format(len(v)))\nprint('These variables are {}'.format(list(v)))","ae7c1a5d":"#Training start \nX_train = train.drop(['id', 'target'], axis= 1)\ny_train = train['target']","c43d3862":"feat_labels = X_train.columns\n\n#just use 10 n_estimators since it takes too mich time \nrf = RandomForestClassifier(n_estimators=10, random_state=0, n_jobs=-1)\n \nrf.fit(X_train[:1000], y_train[:1000])\nimportances = rf.feature_importances_\n#sorting\nindices = np.argsort(rf.feature_importances_)[::-1]\n\nfor f in range(X_train.shape[1]):\n    print(\"%2d) %-*s %f\" % (f + 1, 30,feat_labels[indices[f]], importances[indices[f]]))\n","933cc781":"sfm = SelectFromModel(rf, threshold='median', prefit=True)  #prefit\uc774\uac83\uc740 rf\uc774\ub807\uac8c \uc774\ubbf8 \ud559\uc2b5\ud55c \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uaca0\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud574\uc8fc\ub294 \ucf54\ub4dc\uc774\ub2e4. \nprint('Number of features before selection: {}'.format(X_train.shape[1]))\nn_features = sfm.transform(X_train).shape[1]\nprint('Number of features after selection: {}'.format(n_features))\nselected_vars = list(feat_labels[sfm.get_support()])\n","e6bb947b":"train = train[selected_vars + ['target']]","e26d7afa":"train.shape ","96a77c6f":"scaler = StandardScaler()\nscaler.fit_transform(train.drop(['target'], axis=1))","7d721527":"* A priori(**'target'**) in the train data is 3.645%, which is strongly **imbalanced**.\n* From the means we can conclude that for most variables the value is zero in most cases.","9cd93eaa":"**ps_car_12 and ps_car_13**","81a2be4f":"We filled the continuous values with the average values and the ordinal values with the most common values. The missing value of the data should also not be filled carelessly. There are ways to fill miss values by grouping them in more detail, or to learn from features that do not have miss values at all and fill them with them. \n\nDepending on how you fill this missing value, you should be careful because it may be a factor in creating a bias.","aa2bca78":"### 2_Ordinal variables","e22c5347":"**.drop_duplicates()** = This means that if there is a duplicate column, it will be deleted.\n\nWe can check there is No duplicate rows. ","5bfb15fa":"# Reference","739b6b82":"Again, with the info() method we see that the data type is integer or float. No null values are present in the data set. That's normal because missing values are replaced by -1. We'll look into that later.. ","2170b045":"Aligns indexes of newly created series with existing training data series indexes\n","7b21a2e0":"# 5. Data quality checks\n\n### Checking missing values\nMissings are represented as -1 ","f67cad33":"# 2. Defining the metadata","57af569a":"### 3_Binary variables","0b9a917e":"Now, in fact, data are said to contain most of the strange data such as noise, and this process is now a partial noise-giving method to create a model that does not shake even with such noise. That is, it is the same vaccination that produces antibodies by exposing the virus to the body as part of it.\nUsing this method of putting noise into data as part of it is now said to be more powerful than reducing parameter values as we proceed with the learning. \n\nFor example, if you proceed with classification, if the value is now 10,0, then it will be 0.8, 0.1, 0.1, 0.1, more specific and smoother labeling ???It has the effect of doing it (although I don't really understand how to soften the labeling here) but it still works!)\nIn conclusion, one of the ways to improve the efficiency of the model is just used by sensing or bagging.","50ecbc90":"# 4. Handling imbalanced classes","d27636e0":"The test is 58 because of course there is no target value.","f4ce2e99":"This adds extra interaction variables to the train data. Thanks to the get_feature_names method we can assign column names to thes new variable. ","562fb5f4":"### Interval Variables\n**Checking the correlations between interval variables.**\n\nA heatmap is a good way to visualize the correlation between variables. ","a95be13e":"The bar graphs show the variables with missing values. We replaced the missing values earlier, but we didn't replace the categories separately. It may be a better way to see missing values as separate category values than replacing them with the least frequently values.\n\nThis is because customers with missing values have a much higher target average than other values!","3c21018d":"The plot shows that:\n* the target is imbalanced\n* high bias is expected to 0\n* class weight has to be balanced on training","dc666e0c":"# Loading Data ","fe044b5c":"We would lose rather many variables if we would select based on variance. But because we do not have so many variables, we'll let the classifier chose. For data sets with many more variables this could reduce the processing time.\n\nSklearn also comes with other feature selection methods. One of these methods is SelectFromModel in which you let another classifier select the best features and continue with these. Below I'll show you how to do that with a Random Forest. ","47417298":"# 1. Visual inspection of your data ","11c51e63":"The results will come in important order. However, we have set the n_estimator value here at 10, so the results are not produced. ","e3a5794b":"## Creating Interaction Variables","0a5c361e":"**reg variables**\n* only ps_reg_03 has missing values\n* the range (min to max) differs between the variables. We could apply scaling (e.g. StandardScaler), but it depends on the classifier we will want to use.\n\n**car variables**\n* ps_car_12 and ps_car_15 have missing values\n* again, the range differs and we could apply scaling.\n\n**calc variables**\n* no missing values, this seems to be some kind of ratio as the maximum is 0.9\n* all three _calc variables have very similar distributions\n\nOverall, we can see that the range of the interval variables is rather small. Perhaps, some transformation is already applied in order to anonymize the data.","1c6800d3":"The prior value is the average value of the train ['target']. Previously, we set the ratio of 0 to 1 to 9:1 through undersampling, so the priority value is 0.1.\n\nThen add a column named target to the targets. Values are converted using Smoothing.","5be7b687":"It seems that only the required value is Smoothed. Use drop to subtract mean and count.","d3f3dd3e":"As we can see from the variables with missing values, it is a good idea to keep the missing values as a separate category values, instead of replacing them by them by the mode for instance. The customers with a missing value appear to have a much higher probability to ask for an insurance claim. ","080f50ef":"# 9. Feature scaling\n\nAs mentioned before, we can apply standard scaling to the training data. Some classifiers perform better when this is done.","47e9d21d":"There are a strong correlation between the variable. \n\n* ps_reg_02 and ps_reg_03 (0.7)\n* ps_car_12 and ps_car13 (0.67)\n* ps_car_12 and ps_car14 (0.58)\n* ps_car_13 and ps_car15 (0.67)\n\nSeaborn has some handy plots to visualize the (linear) relationship between variables. We could use a pairplot to visualize the relationship between the variables. But because the heatmap already showed the limited number of correlated variables, we'll look at each of the highly correlated variables separately.\n**NOTE**: I take a sample of the train data to speed up the process. ","75c9f85e":"But you can't erase it like this. We need to check it before we fill out the missing values. Missing values may be meaningful, so you should check them.\n\nThere are some important meaning in missing value values in this way, so you should check them out.","8e14b194":"Allright, so now what? How can we decide which of the correlated variables to keep? We could perform **Principal Component Analysis (PCA)** on the variables to reduce the dimensions. In the AllState Claims Severity Competition I made this kernel to do that. But as the number of correlated variables is rather low, we will let the model do the heavy-lifting. ","6defd3b7":"Let's concat the two series based on the column and save them as a variable named temp.","22265d54":"Smoothing is usually done to help us better see patterns, trends for example, in time series. Generally smooth out the irregular roughness to see a clearer signal. ","c5ef65c8":"The trn_series we're going to use is train ['ps_car_11_cat'], and target is train['target'].\nVerify that the two series are the same length, and that train ['ps_car_11_cat'] and test ['ps_car_11_cat'] have the same name.","a1198998":"Below the number of variable per role and level are displayed ","11c67cb1":"* Only one missing variable: ps_car_11\n* We could apply scaling to deal with the different ranges","78f55212":"\uc5ec\uae30 \ubc11\uc5d0 \ucf54\ub4dc\ub294 \ud558\ub098\uc529 \ub72f\uc5b4\ubcf4\uae30  **\ub108\ubb34 Value\uac00 \ub9ce\uc740 Categorical Variables\ub97c \uac00\uacf5\ud558\ub294 \uacfc\uc815**","4a5d56e8":"* target ==0 : 195246\n* target ==1 : 21694","9c359fe2":"* **SimpleImputer**: Imputation transformer for completing missing values.\n* **PolynomialFeatures**: ![image.png](attachment:image.png)\n \nThe above data cannot be solved by general linear regression because the distribution of the data is curved. (Non-linear)\nTherefore, we use polynomial regression. Polynomial regression is a method of training nonlinear data with added characteristics as a linear regression model by adding a square of each characteristic to the data when the shape between the data is nonlinear as shown above.\n* **SelectFromModel**: Select attributes based on user-specified thresholds after model training ends\n* **RandomForestClassifier**: A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.","d7425b83":"Group the values in the target column by train ['ps_car_11_cat'] and store the values that the mean and count functions applied to the variables.\n\nThe averages function is,\nContains target average and number of times information for each Value","8e5981d5":"### Selecting features with a Random Forest and SelectFromModel\nHere we'll base feature selection on the feature importance of a random forest. With Sklearn's SelectFromModel you can then specify how many variables you want to keep. You can set a threshold on the level of feature importance manually. But we'll simply select the top 50% best variables.\n","d018dfa0":"Through this combination, you can learn how to sample **un-balance data** and how to handle data that have many categories. There are a variety of ways. Here. There are various method we can check that are not used in Titanic.","b8b79d60":"Import the ps_car_11_cat series into the data frame as to_frame.\nInitializes the index of the aggregates, and renames the index values to target, column names to target.\nSpecify the on and how values to merge.\nRename the series to ps_car_11_cat_mean and replace the missing values with the prior values.","4eda6e71":"**ps_car_13 and ps_car_15**","69ffeee5":"### Checking the cardinality of the categorical variables \n* Cardinality is an indicator of the duplicate numbers in a particular column for the entire row.\n* High redundancy(\uc911\ubcf5\ub3c4) means low cardinality, and low redundancy means high cardinality.\n* Cardinality should be understood as a relative concept.\n\nThus, cardinality refers to the number of different values within the variable. We are going to dummy up the categories later, and we need to check how many other values there are in them.Because if there are many values, a lot of dummy variables can be made.","f3a4466e":"So creating dummy variables adds 52 variables to the training set. ","cb278229":"### Checking the correlations between ordinal variables","299b4cfd":"**ps_car_12 and ps_car_14**","20a667a0":"* When working with high cardinality categorical features it is hard for a model to put every small category into a separate bucket, but if a lot of small categories can be put in one bin together based on their mean target value, then trees can learn way faster. \n* Less bias, since now lavels have more meaning: closer labels mean closer categories in terms of the mean target variable. ","e748a683":"## Data at first sight \n\nHere is an exception of the data description for the competition: \n\n* Features that belong to similar groupings tagged as such in the feature names(e.g. ind, reg, car, calc)\n* Feature names inculde the postfix bin to indicate binary features and cat to indicate categorical features.\n* Features without these designations are either continuous or ordinal. \n* Values of -1 indicate that the feature was missing from the observation \n* The target columns signifies whether or not a claim was filed for that policy holder \n\n\"Ind\" is related to individual or driver, \"reg\" is related to region, \"car\" is related to car itself and \"calc\" is an calculated feature.\n\nThis is the important information to get us started. let's have a quick look at the first and last rows to confirm all of this. \n","1d7ce882":"Define a function that causes Noise.\n\nThe series values and noise levels are taken as variables.\nReturns the series (1 + random number sampled from the noise level standard normal distribution).\n\nLet's start with the target encode function asset.","15f7e72a":"In conclusion, the essence of PCA is dimensionality reduction.\n\nIt's not the original data, it's the dimensions are reduced.\n\nConverted data == using main components\n\nIt means that it will proceed with analysis or modeling.","ac3f233d":"Use a sample of train data to speed up the process.","f9fe3d3b":"Two possible strategies to deal with this probelm are:\n\n* 1) **Oversampling records with target** = 1\n* 2) **Undersampling records with targe**t = 0\n\n**Oversampling** is the process of learning by adding more samples to existing samples. For example, there are currently only four target values(=1), which makes to raised to 50. It is to proceed with learning on sound.\n\nThe undersampling is rather diminishing. Undersampling has a target value of 96, which is reduced to 50. \n\nWe are going to use **undersampling**.  ","c0adebf5":"**ps_reg_02 and ps_reg_03**\n\nAs the regression line shows, there is a linear relationship between these variables. Thanks to the hue parameter we can see that the regression lines for target=0 and target=1 are the same.","7c39f7dd":"I made more explanation based on \n\nhttps:\/\/www.kaggle.com\/bertcarremans\/data-preparation-exploration.\n\nhttps:\/\/www.youtube.com\/watch?v=r26Ar4AiwVI&t=1736s","b86c43f8":"During the Descriptive Stats verification process, too many cases of Target == 0 were found for the Label Column Target column compared to Target == 1, so under sampling was performed to match the same ratio.\n\nThe appropriate number of records is calculated by applying the Under-sampling Rate to the number of class records each biased. We then randomly screen the index by that number and then finally adjust the Train Dataset so that the data remains 1:1 per each class.","c542fb6d":"To facilitate the data management, we'll store meta-information about the variables in a DataFrame. \n\nThis will be helpful when we want to select specific variables for analysis, visualization, modeling.\n\n* **role**: input, ID, target\n* **level**: nominal, interval, ordinal, binary\n* **keep**: True or False\n* **dtype**: int, float, str","580396d5":"#### We indeed see the following \n \n* Binary variables \n* Categorical variables of which the category values are integers\n* Other variable with integer or float values \n* Variables with -1 representing missing values \n* The target variable and an ID variable ","3b6ce8f3":"If you look at the code below, we are grouping by column. For example, the Label Column, Target, is defined as Target and input for the column to be used for analysis, and a column to record the usage of each variable is created through the \"Keep\" column to organize which column to write about in the train.","4a2ed6f6":"Code to find the undersampling rate \n*undersampled_nb_0 = int(undersampling_rate*nb_0)","8cf7e197":"# 7. Feature engineering\n\n### Creating dummy variables\nThe values of the categorical variable do not represent any order or magnitude. \nFor example, category 2 is not twice the value of category 1. Therfore we can create dummy variables to deal with that.\nWe drop the first dummy variables as this information can be derived from the other dummy variable generated for the categories of the orginal varible. ","d7d4dd72":"# 6. Exploratory data visualization\n\n### Categorical Variables\nCategorical variables and the proportion of customers with target = t","4d393063":"Ordinal variables do not appear to have much correlation. On the other hand, you can see what the distribution will look like when grouping into target values.","ef679078":"* **ps_car_03_cat and ps_car_05_cat** have a large proportion of records with missing values. Remove these variables.\n* For the other categorical variables with missing values, we can leave the missing value -1 as such.\n* **ps_car_11** (ordinal) has only 1 records with misisng values. Replace by the mode.\n* **ps_reg_03** (continuous) has missing values for 18% of all records. Replace by the mean.\n* **ps_car_14** (continuous) has missing values for 7% of all records. Replace by the mean.","937fd0b8":"For example, extract the index of data with a level of nominal.","14b366e3":"## Descriptive Statistics \n: is a summary statistic that quantitatively describes or summarizes features from a collection of information\n \nWe can also apply the describe method on the dataframe. However, it doesn't make much sense to calculate the mean, std,on categorical variables and the id variable. We'll explore the categorical variables visually later.\n\nThanks to our meta file we can easily select the variables on which we want to compute the descriptive statistics. To keep things clear, we'll do this per data type.\n\n### 1_Interval variables ","f2cba17e":"As we mentioned above the proportion of records with target=1 is far less than target=0. This can lead to a model that has great accuracy but does have any added value in practice. Two possible strategies to deal with this problem are:\n\nFor example, 96 out of 100 values are zero and only 4 have value one. Then, all of them might be called zero without making models, and the accuracy is 95 percent. In other words, it is important to get a good one, which makes the balance strange. Its accuracy is practically zero percent. Because all 1 is said to be zero, the accuracy is zero percent. Therefore, acuracy should not be used, so 'gini' and 'rucac' should be used.","71a3813f":"So later on we can create dummy variables for the 14 categorical variables(=nunique data). The bin variables are already binary and do not need dummification.\n![image.png](attachment:image.png)\n\n**'Dummy variable'** allows the computer to recognize the values of categorical materials when performing statistics or mechanical learning by entering values as '1' if they fall into the category and '0' if not.","d95c8134":"# The End","610dc6e6":"By using the info() method, we once again confirmed that the type of data is integer or floatd. The info() method determines that there are no missing values. As I mentioned earlier, the missing values are replaced by -1. We will deal with it later.","32cf09fd":"# 8. Feature selection\n\n## Removing Features with low or zero variance \n\n* Personally, I prefer to let the classifier algorithm chose which features to keep. But there is one thing that we can do ourselves. \n* That is removing features with no or a very low variance. Sklearn has a handy method to do that: VarianceThreshold. By default it removes features with zero variance. \n* This will not be applicable for this competition as we saw there are no zero-variance variables in the previous steps. \n* But if we would remove features with less than 1% variance, we would remove 31 variables.\n\nSo what they're saying now is that if the variation is small, there's less information you can get from the feature. We'll give you this information, but you and I choose here. These calculations are all possible at sklearn, but they are meaningless because there is no zero value. That's why we're setting the threshold. One percent means they will be erased.","2129264d":"# Contents of notebook\n","6d81355c":"\ubc11\uc5d0 \ub178\uc774\uc988\ub97c \ub123\uc5b4\uc8fc\uba74 \ub108\ubb34 \ub514\uc2a4\ud06c\ub9ac\ud2b8 \ud558\ub2c8\uae50?? \ub514\uc2a4\ud06c\ub9ac\ud2b8\uac00 \uadfc\ub370 \ubaac\uc9c0 \uc798\ubaa8\ub974\uaca0\uc74c , \ub178\uc774\uc988 \uac12\uc73c\ub85c \ub9e5\uc2dc\uba48 \uac12\uc744 1\uc744 \ub118\uc9c0 \uc54a\uac8c\ud574\uc900\ub2e5 \ud558\ub294\ub370 \uc774\uac83\ub3c4 \ubb34\uc2a8 \uc18c\ub9ac\uc778\uc9c0\ub294 \uc798 \ubaa8\ub974\uaca0\ub2e4. \ub178\uc774\uc988\ub97c \ub123\uc5b4\uc918\uc11c \ub514\uc2a4\ud06c\ub9ac\ud2b8 \ud558\uc9c0\uc54a\uace0 \ucee8\ud2f0\ub274\uc5b4\uc2a4 \ud55c \uac12\uc744 \ub9cc\ub4e4\uc5b4\uc8fc\ub294 \uac83\uc774\ub2e4. \uc77c\ub2e8 encoding\uc774 \ubb34\uc5c7\uc778\uc9c0 \uad6c\uae00\uc5d0\uc11c \ud55c\ubc88 \ucc3e\uc544\ubcf4\ub3c4\ub85d \ud558\uae30. ","47a352e2":"You must check the data column before you start the compilation.","91a108a7":"The previous work in the train data is also carried out in the test data.","51bbed81":"With SelectFromModel we can specify which prefit classifier to use and what the threshold is for the feature importances. With the get_support method we can then limit the number of variables in the train data.\n\n\nIt means that we will be able to decide on a model and how many to leave.","575aa9c4":"check out the target values ","c115ec09":" **'Metadata'** is defined as the data providing information about one or more aspects of the data; it is used to summarize basic information about data which can make tracking and working with specific data easier.","8060d3b4":"**1. Visual inspection of your data**\n\n**2. Defining the metadata** \n\n**3. Descriptive statistics** \n\n**4. Handling imbalanced classes** \n\n**5. Data quality checks** \n\n**6. Exploratory data visualization** \n\n**7. Feature engineering**\n\n**8. Feature selection** \n\n**9. Feature scaling** ","f2d907f3":"Takes advantage of the add_noise function that you initially defined to return values that caused noise.\nHere's a summary of the processes that we've done in the function.\n\n1) Define add_noise function to create noise\n\n2) Verify that the len of the train data and target data are the same, and that the test data and train data have the same name.\n\n3) train series and target series concat\n\n4) Calculate mean and count by value and save as Average\n \n5) Calculate Smoothing\n\n6) Define the prior as the average value of the target data.\n\n7) Smoothing the average of each value that was previously proceeded and remove unnecessary mean and count\n \n8) Define a new series (trn\/test_cat_mean) with values from Averages Returns a series with the initial definition of add_noise"}}