{"cell_type":{"f81ff755":"code","87637c4b":"code","d37a6d9d":"code","b78ea5b1":"code","c40686a9":"code","3f813dc0":"code","d52b6b03":"code","c00406ae":"code","4f8bc8f8":"code","799bfb6d":"code","03e3c014":"code","1d04b4f1":"code","50e5c11f":"code","ce2c1bd3":"code","70e4eb61":"code","2600dd49":"code","3ee2f113":"code","f6a1ca10":"code","7f56dd7c":"code","cc83c27c":"code","e63a8da8":"code","276964cc":"code","0d853ec6":"code","a550f236":"code","c1df6edb":"code","f1a11628":"code","289e98f6":"code","8e0b4f44":"markdown"},"source":{"f81ff755":"import os\nimport warnings\nwarnings.filterwarnings(\"ignore\")                     #Ignoring unnecessory warnings\n\nimport numpy as np                                  #for large and multi-dimensional arrays\nimport pandas as pd                                 #for data manipulation and analysis\nimport nltk                                         #Natural language processing tool-kit\n\nfrom nltk.corpus import stopwords                   #Stopwords corpus\nfrom nltk.stem import PorterStemmer                 # Stemmer\n\nfrom sklearn.feature_extraction.text import CountVectorizer          #For Bag of words\nfrom sklearn.feature_extraction.text import TfidfVectorizer          #For TF-IDF\nfrom gensim.models import Word2Vec                                   #For Word2Vec\n\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Dense\n","87637c4b":"train_file = \"..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\"\ntest_file = \"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\"\ncleaned_text = \"..\/input\/cleaned-toxic-comments\/train_preprocessed.csv\"\nsubmission = \"..\/input\/jigsaw-toxic-severity-rating\/sample_submission.csv\"\n# train_df = pd.read_csv(train_file)\n# train_df.head()","d37a6d9d":"cleaned_text_df = pd.read_csv(cleaned_text)\ncleaned_text_df","b78ea5b1":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LinearRegression\n\ncombined_comments =  pd.read_csv(cleaned_text).comment_text.tolist()","c40686a9":"for i in range(500,510):\n    print(combined_comments[i])\n    print('--------------------------------------------------------------------------------')","3f813dc0":"len(cleaned_text_df.toxicity)","d52b6b03":"# df_x = combined_comments\nvoc_size=5000\nonehot_repr=[one_hot(words,voc_size)for words in combined_comments] \ntype(onehot_repr)","c00406ae":"sent_length=400\nembedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\nprint(embedded_docs)","4f8bc8f8":"## Creating model\nembedding_vector_features=40\nmodel=Sequential()\nmodel.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\nmodel.add(Dropout(0.3))\nmodel.add(LSTM(100))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nprint(model.summary())","799bfb6d":"# df_y = cleaned_text_df['toxicity']","03e3c014":"from sklearn.preprocessing import LabelEncoder\nencode = LabelEncoder()\ndf_y2 = encode.fit_transform(pd.read_csv(cleaned_text)['toxicity'])\ntype(df_y2)","1d04b4f1":"df_y2.shape","50e5c11f":"embedded_docs.shape","ce2c1bd3":"import numpy as np\nX_final=np.array(embedded_docs)\ny_final=np.array(df_y2)","70e4eb61":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X_final, y_final, test_size=0.2, random_state=42)","2600dd49":"#we are feeding the \nmodel.fit(X_train,Y_train,validation_data=(X_test,Y_test),epochs=5,batch_size=64)","3ee2f113":"# score = model.evaluate(X_test,Y_test)","f6a1ca10":"# y_pred_model = model.predict(X_test)","7f56dd7c":"# print(\"Accuracy: %.2f%%\" % (score[1]*100))\n# diff = Y_test - y_pred_model\nmae = np.mean(abs(Y_test - model.predict(X_test)))\nmse = np.mean((Y_test - model.predict(X_test))**2)\nrmse = np.sqrt(mse)\nprint(mae)\nprint(mse)\nprint(rmse)","cc83c27c":"from keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\n\nlstm_cnn=Sequential()\nlstm_cnn.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\nlstm_cnn.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\nlstm_cnn.add(MaxPooling1D(pool_size=2))\nlstm_cnn.add(LSTM(100))\nlstm_cnn.add(Dense(1))\nlstm_cnn.compile(loss='mean_squared_error', optimizer='adam')\nprint(lstm_cnn.summary())","e63a8da8":"lstm_cnn.fit(X_train,Y_train,validation_data=(X_test,Y_test),epochs=5,batch_size=64)","276964cc":"# score = model.evaluate(X_test,Y_test)","0d853ec6":"# print(\"Accuracy: %.2f%%\" % (score[1]*100))","a550f236":"# print(\"Accuracy: %.2f%%\" % (score[1]*100))\n# diff = Y_test - y_pred_model\nmae = np.mean(abs(Y_test - lstm_cnn.predict(X_test)))\nmse = np.mean((Y_test - lstm_cnn.predict(X_test))**2)\nrmse = np.sqrt(mse)\nprint(mae)\nprint(mse)\nprint(rmse)","c1df6edb":"import re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nimport numpy as np\n\ntrain_df = pd.read_csv(train_file)\ncomments=[str(x) for x in train_df['less_toxic'].tolist()+train_df['more_toxic'].tolist()]\ndf_t = pd.DataFrame({'comments':comments})\n\n\nsnow = nltk.stem.SnowballStemmer('english')\n\ncorpus = []\nfor i in range(0, len(df_t)):\n    review = re.sub('[^a-zA-Z]', ' ', df_t['comments'][i])\n    review = review.lower()\n    review = review.split()\n    \n    review = [snow.stem(word) for word in review if not word in stopwords.words('english')]\n    review = ' '.join(review)\n    corpus.append(review)\n    \n# df_x = combined_comments\nvoc_size=5000\nonehot_repr=[one_hot(words,voc_size)for words in corpus] \ntype(onehot_repr)\n\nsent_length=400\nembedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\nprint(embedded_docs)\n\nsub_test =np.array(embedded_docs)","f1a11628":"sub_pred = model.predict(sub_test)","289e98f6":"sub = pd.read_csv(submission)\nsub['score'] = sub['score'].rank(method='first')\nsub.to_csv('submission.csv', index=False)","8e0b4f44":"![](https:\/\/i.imgur.com\/Va5BVFq.png)\n\n# About the competiotion:\nThis is Jigsaw's fourth Kaggle competition. The goal of this competition is to rank comments by the severity of toxicity. Each comment is given a rating according to its relative toxicity, which you must assign. The numerical value of comments that have a greater degree of toxicity should be greater than comments that have a lower degree of toxicity.\n\n<p align=\"center\">\n<img width = \"300\" src=\"https:\/\/i.imgur.com\/fRWxwmw.jpg\">\n<\/p>\n\nIt is ironic, however, that there are no training data for this competition. The majority of people use the training data from previous competitions. Nevertheless, previous competitions attempted to predict the probability of a comment's toxicity rather than its degree of severity. Don't get all flustered, when you see the dataset provided for this competition cause it's gonna have some profane, vulgar, or offensive text.\n\n"}}