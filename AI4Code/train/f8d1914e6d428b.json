{"cell_type":{"48e19e94":"code","c20731de":"code","1fcc3350":"code","6e98d475":"code","53819d82":"code","5b08fab7":"code","e20e8969":"code","e7f938d8":"code","5c104cba":"code","5abebfea":"code","76abc004":"code","ecefde7a":"code","a74140a5":"code","bb9ac48e":"code","3619a407":"code","92ccabbd":"code","1bf9549c":"code","c5c0d0f5":"code","2c3f7eb2":"code","2c34182b":"code","5a46900a":"code","c8be48ea":"code","aa4c5555":"code","ccc55139":"code","29ee6dd1":"code","912e4625":"code","05a82035":"code","019a46f3":"code","5c8ad558":"markdown","765c1c53":"markdown","8df0a4e6":"markdown","afb2b7e8":"markdown","854ad662":"markdown","1d555a79":"markdown","bba2cbd3":"markdown","d96ce5e5":"markdown","b84bf157":"markdown","dc8817e5":"markdown","568981d9":"markdown","dba1e9e3":"markdown","5c04c036":"markdown","2cf1bf20":"markdown","12587709":"markdown","1a4562e8":"markdown","afa028df":"markdown","a72ff7cb":"markdown","773bf788":"markdown","89e78ccb":"markdown","1348037b":"markdown"},"source":{"48e19e94":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c20731de":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import f1_score, confusion_matrix, classification_report, roc_auc_score,roc_curve,accuracy_score,precision_score,recall_score\nfrom sklearn.preprocessing import StandardScaler\nfrom datetime import datetime\nimport seaborn as sns\nimport numpy as np\nfrom scipy.spatial.distance import pdist\nimport math\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import plot_precision_recall_curve\nfrom sklearn import preprocessing\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport statsmodels.api as sm\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\nimport time\nimport matplotlib.patches as mpatches\nimport random\nfrom tensorflow import keras\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import permutation_test_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.feature_selection import SelectFpr\nfrom sklearn.feature_selection import SelectFdr\nfrom sklearn.feature_selection import SelectFwe\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.linear_model import Ridge\ndf=pd.read_csv('\/kaggle\/input\/company-bankruptcy-prediction\/data.csv')\n","1fcc3350":"df.head(10)","6e98d475":"df.describe()","53819d82":"df.hist(figsize = (35,30), bins = 50)\nplt.show()","5b08fab7":"# No N\/As in the data\nprint(\"N\/As: \"+str(df.isna().sum().sum()))\n# Are there negative values?\nprint(\"Negative values: \"+str(df[df<0].sum().sum()))","e20e8969":"# Separating dependent and independent variables\ny=df['Bankrupt?']\nX=df.iloc[:,1:]\nprint(\"Percentage of positive cases: \"+str(y.mean()))","e7f938d8":"X[' Total Asset Turnover'].hist()\nplt.show()\nprint(\"Skewness score: \"+str(X[' Total Asset Turnover'].skew()))","5c104cba":"# credit goes to user Marto93 for this handy function\ndef log_trans(data):\n    \n    for col in data:\n        skew = data[col].skew()\n        if skew > 0.5 or skew < -0.5:\n            data[col] = np.log1p(data[col])\n        else:\n            continue\n            \n    return data","5abebfea":"X=log_trans(X)","76abc004":"X[' Total Asset Turnover'].hist()\nplt.show()\nprint(\"Skewness score: \"+str(X[' Total Asset Turnover'].skew()))","ecefde7a":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=8)\n","a74140a5":"# Let's look at correlation between variables and remove highly correlated variables with r>0.97 (more thorough variable selection will be done ion the next step)\ncorrelations=X_train.corr()\ncorrelated_vars=pd.DataFrame(columns=['var1','var2','corr'])\nround=0\nfor c in correlations.columns:\n    round=round+1\n    for r in list(correlations.index)[round:]:\n# Showing variables with correlation higher than 97%        \n        if correlations.loc[r,c]>0.97:\n            correlated_vars=correlated_vars.append({'var1':c,'var2':r,'corr':correlations.loc[r,c]},ignore_index=True)\nprint(correlated_vars)    ","bb9ac48e":"# Let's drop some variables\nfor v in [' ROA(C) before interest and depreciation before interest',\n          ' ROA(A) before interest and % after tax',\n          ' Realized Sales Gross Margin',\n          ' Pre-tax net Interest Rate',\n          ' Current Liabilities\/Equity',\n          ' Current Liabilities\/Liability',\n          ' Net Value Growth Rate',\n          ' Continuous interest rate (after tax)',\n          ' Net Value Per Share (B)',\n          ' Net Value Per Share (C)',\n          ' Gross Profit to Sales',\n          ' Operating Profit Per Share (Yuan \u00a5)',\n          ' Per Share Net profit before tax (Yuan \u00a5)',\n          ' After-tax Net Profit Growth Rate',\n         ' After-tax net Interest Rate',\n         ' Borrowing dependency']:\n    X_train=X_train.drop(v,axis=1)\n    X_test=X_test.drop(v,axis=1)\nprint(X_train.shape)\nprint(X_test.shape)","3619a407":"vote=pd.DataFrame(columns=X_train.columns)\nvote['method']=['mutual_info','rfe','ctree','rforest','Fdr','permutation']\nvote.iloc[:,:-1]=0","92ccabbd":"# Feature selection is performed using ANOVA F measure via the f_classif() function\n\nprint('mutual_info')\nfs = SelectKBest(score_func=mutual_info_classif,k=20)\nfs.fit(X_train, y_train==1)\n\nvote.iloc[vote.method=='mutual_info',:-1]=(np.argsort(fs.scores_)+1)\n\nprint('Fdr')\nfs = SelectFdr(alpha=0.01)\nfs.fit_transform(X_train, y_train)\n\nvote.iloc[vote.method=='Fdr',:-1]=(np.argsort(fs.scores_)+1)\n\nprint('permutation')\nmodel = Ridge(alpha=1e-2)\nmodel.fit(X_train, (y_train==1))\nresults = permutation_importance(model, X_train, y_train)\nvote.iloc[vote.method=='permutation',:-1]=(np.argsort(results.importances_mean)+1)\nprint('rfe')\nmodel = LogisticRegression(max_iter=20000)\nrfe = RFE(model, n_features_to_select=1)\nrfe = rfe.fit(X_train, y_train)\nvote.iloc[vote.method=='rfe',:-1]=rfe.ranking_","1bf9549c":"model = RandomForestClassifier(max_depth=10, random_state=0)\n# fit the model\nmodel.fit(X_train, y_train)\n# get importance\nimportance = model.feature_importances_\nvote.iloc[vote.method=='rforest',:-1]=(np.argsort(importance)+1)","c5c0d0f5":"model = DecisionTreeClassifier(random_state=0)\n# fit the model\nmodel.fit(X_train, y_train)\n# get importance\nimportance = model.feature_importances_\nvote.iloc[vote.method=='ctree',:-1]=(np.argsort(importance)+1)","2c3f7eb2":"vote","2c34182b":"final_vote=1\/vote.iloc[:,:-1]\nvote_results=final_vote.sum(axis=0).nlargest(9)\nprint(vote_results)","5a46900a":"selection=vote_results.index\nprint(selection)","c8be48ea":"from sklearn.model_selection import cross_val_score\nimport sklearn.metrics\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom imblearn.pipeline import make_pipeline\n\nparams = {\"logisticregression__penalty\": ['l2','l1','elasticnet'],\n                  'logisticregression__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],}\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npipe = make_pipeline(SMOTE(sampling_strategy=0.5), StandardScaler(), LogisticRegression(solver='saga',max_iter=5000))\ngrid_search_logit = GridSearchCV(pipe, param_grid=params,cv=3,scoring='f1')\ngrid_search_logit.fit(X_train[selection],y_train)","aa4c5555":"grid_search_logit.best_score_ ","ccc55139":"params = {\"randomforestclassifier__max_depth\": [10,20,40,80],\n                  'randomforestclassifier__criterion':['entropy','gini'],\n                  'randomforestclassifier__max_features': ['auto', 'sqrt', 'log2'],\n                  'randomforestclassifier__n_estimators': [5,10,15],\n          'randomforestclassifier__class_weight' : ['balanced','balanced_subsample'],\n          'randomforestclassifier__bootstrap' : [True,False]\n         }\npipe = make_pipeline(SMOTE(sampling_strategy=0.5), StandardScaler(), RandomForestClassifier())\ngrid_search_rf = GridSearchCV(pipe, param_grid=params,cv=3,scoring='f1')\ngrid_search_rf.fit(X_train[selection],y_train)","29ee6dd1":"grid_search_rf.best_score_","912e4625":"from sklearn.ensemble import AdaBoostClassifier\n\nparams = {'adaboostclassifier__learning_rate' : [0.1,0.01,0.001],\n              'adaboostclassifier__n_estimators' : range(1,10)}\n\npipe = make_pipeline(SMOTE(sampling_strategy=0.5), StandardScaler(), AdaBoostClassifier(random_state=8))\ngrid_search_ada = GridSearchCV(pipe, param_grid=params,cv=3,scoring='f1')\ngrid_search_ada.fit(X_train[selection],y_train)","05a82035":"grid_search_ada.best_score_","019a46f3":"#model_logit.fit(X_train[selection],y_train)\ny_pred=grid_search_rf.predict(X_test[selection])     \n# showing output:\ncnf_matrix = confusion_matrix(y_test, y_pred)\nprint(cnf_matrix)\nprint(classification_report(y_test, y_pred))\nclass_names=[0,1] # name  of classes\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\n# create heatmap\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()\n    \nprint(\"Accuracy:\",accuracy_score(y_test, y_pred))\nprint(\"Precision:\",precision_score(y_test, y_pred))\nprint(\"Recall:\",recall_score(y_test, y_pred))\n\n# ROC curve\ny_pred_proba = grid_search_rf.predict_proba(X_test[selection])[::,1]\nfpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\nauc = roc_auc_score(y_test, y_pred_proba)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()\n    \n# precision recall curve\ndisp = plot_precision_recall_curve(grid_search_rf, X_test[selection], y_test)\ndisp.ax_.set_title('2-class Precision-Recall curve: '\n                   'AP={0:0.2f}')\nplt.show()\n    # Adding Moody's scoring methodology: \u201cRating Methodology: Moody\u2019s Public Firm Risk Model: A Hybrid Approach to Modeling\n#Short Term Default Risk,\u201d Moody\u2019s Investors Service, March 2000. The AC ratio is somewhat\n#related to the Kolmogorov-Smirnov test.\n# In (Vassalou and Xing, 2004)\n    # We start by sorting the results by order of probability\ny_pred_proba=pd.Series(y_pred_proba)\n#y_test=y_test.reset_index()\ndata = pd.DataFrame({'y_pred_proba': y_pred_proba,'y_test':y_test})\n#AR(data)\n    \n    # Printing the mean and median of probabilities versus real incidence\nprint('Mean probability: '+str(np.mean(y_pred_proba)))\nprint('Median probability: '+str(np.median(y_pred_proba)))\nprint('Real incidence: '+str(y_test.sum()\/len(y_test)))","5c8ad558":"As we can see below, there is severe class imbalance in the data, with postive cases representing only 3.23% of total cases so we'll use the Synthetic Minority Oversampling Technique (SMOTE)","765c1c53":"<h2>Logistic Regression<\/h2>","8df0a4e6":"As we can see in the histograms above, some datapoints are very skewed, we'll use a log-transform to try and remedy that. Let's check if we have any N\/As or negative values in our data (log transforms don't really like negative values)","afb2b7e8":"# **ADABoost**","854ad662":"We get a final F1 score of 0.43 on our test...","1d555a79":"<h1>Variable Selection - part 1: removing correlated variables<\/h1>","bba2cbd3":"Not perfect, but it looks less skewed, let's now create our Train and Test sets","d96ce5e5":"<h1>Defining the pipeline<\/h1>","b84bf157":"<h2>Random Forest<\/h2>","dc8817e5":"**FINAL VOTE, AND THE WINNER IS...**","568981d9":"<h1>Variable Selection - part 2: a voting ensemble<\/h1>","dba1e9e3":"<h1>Very quick EDA<\/h1>","5c04c036":"We will use a the Nauru variant of a Borda count to choose our top 9 features, out of 79 features.","2cf1bf20":"We will need to transform skewed values with a log transform and standardize the data before proceeding to the be next st steps","12587709":"For this notebook, we'll be using a simple workflow with variable transformation, variable selection with a voting ensemble method, followed by a grid search with cross-validation on 3 classification models (logistic regression, random forest and ADA boost). We then selec the best model based on the CV results for the F1 score and use that model on our test data.","1a4562e8":"<h1>Bankruptcy Prediction<\/h1>\n\n<img src= \"https:\/\/images.cnbctv18.com\/wp-content\/uploads\/2019\/06\/bankruptcy-768x512.jpg\">","afa028df":"# **LOG TRANSFORM AND STANDARDISATION**","a72ff7cb":"Loading the data and libraries","773bf788":"As we can see above, we got the highest cross-validation score for our Random Forest model, let's see how it performs on our test...","89e78ccb":"We will now look at correlated variables and remove variables that are highly (linearly) correlated","1348037b":"If we look at say, at Total assets turnover, we can see the data is skewed:"}}