{"cell_type":{"77e893fc":"code","aabfd097":"code","dee228d7":"code","e0d814f0":"code","a8b7d782":"code","bb7c1969":"code","7d0323f2":"code","92670fbc":"code","3185dfda":"code","05ac7d5e":"code","fa1785df":"code","94b19a1b":"code","e3341798":"code","5588c980":"code","61248b05":"code","8f749e71":"code","e5685eb2":"code","04172dda":"code","0f5e8458":"code","a912a22d":"code","f5bb9925":"code","7afab84c":"code","2b69de94":"code","cdc95f9b":"code","167448f9":"code","351cb37f":"code","ba336a6f":"code","b5fc76e2":"code","afa574c7":"code","d5387ffd":"code","c2468283":"code","a17711bc":"code","50888822":"code","6c622cd6":"code","81f4191c":"code","928319af":"code","c5f92995":"code","2027dfb9":"code","84dfe601":"code","0818d779":"code","cb3ebfe4":"code","0372d084":"code","0d50214c":"code","6702ea49":"code","a18ba37f":"code","a02e150c":"code","95f2cfbe":"code","2d217e50":"code","09de3d26":"code","6dd76ae2":"code","12c5e219":"code","d70bee25":"code","d816ad52":"code","3875c38b":"code","1f67d75f":"code","ced3da65":"code","ee7a799b":"code","e31b1ee8":"code","c11d71d3":"code","a5963f77":"code","1e51af1d":"code","bacccfb7":"code","ce155acb":"code","4292d2b9":"code","fc34aad4":"code","ed241b51":"code","27f8309d":"code","51d30af2":"code","aef386aa":"code","73a14e05":"code","acd90cff":"code","cd8d0487":"code","75e3b957":"code","2a97c3c4":"code","ef593280":"code","77baf051":"code","330f555d":"code","d935a594":"code","d6c56697":"code","ea264206":"code","e5c9ca1f":"code","a9468180":"code","6d382697":"code","7dd7e736":"code","c5a5d6a7":"code","8113e803":"code","63bd7a0c":"code","dfe4ffeb":"markdown","6ffbd0e5":"markdown","bb9c3852":"markdown","f56f79ae":"markdown","c9944894":"markdown","4136a6a4":"markdown","649f109d":"markdown"},"source":{"77e893fc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","aabfd097":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set()","dee228d7":"train_data=pd.read_excel(r'..\/input\/flight-fare-prediction\/Data_Train.xlsx')\ntrain_data.head()\npd.set_option('display.max_columns',None)","e0d814f0":"train_data.info()","a8b7d782":"train_data.columns","bb7c1969":"train_data['Duration'].value_counts()","7d0323f2":"train_data.dropna(inplace=True)","92670fbc":"train_data.isnull().sum()","3185dfda":"train_data.shape","05ac7d5e":"#import dtale","fa1785df":"#dtale.show(train_data)","94b19a1b":"train_data['journey_day']=pd.to_datetime(train_data.Date_of_Journey, format='%d\/%m\/%Y').dt.day","e3341798":"train_data['journey_month']=pd.to_datetime(train_data.Date_of_Journey, format='%d\/%m\/%Y').dt.month","5588c980":"train_data.head()","61248b05":"type('journey_day')","8f749e71":"train_data.drop(['Date_of_Journey'],axis=1,inplace=True)","e5685eb2":"train_data['Dep_hours']=pd.to_datetime(train_data['Dep_Time']).dt.hour\ntrain_data['Dep_min']=pd.to_datetime(train_data['Dep_Time']).dt.minute\n\n\ntrain_data.drop(['Dep_Time'],axis=1,inplace=True)","04172dda":"train_data.head()","0f5e8458":"train_data['Arrival_hour']=pd.to_datetime(train_data['Arrival_Time']).dt.hour\ntrain_data['Arrival_min']=pd.to_datetime(train_data['Arrival_Time']).dt.minute\n\n\ntrain_data.drop(['Arrival_Time'],axis=1,inplace=True)","a912a22d":"train_data.head()","f5bb9925":"# Time taken by plane to reach destination is called Duration\n# It is the differnce betwwen Departure Time and Arrival time\n\n\n# Assigning and converting Duration column into list\nduration = list(train_data[\"Duration\"])\n\nfor i in range(len(duration)):\n    if len(duration[i].split()) != 2:    # Check if duration contains only hour or mins\n        if \"h\" in duration[i]:\n            duration[i] = duration[i].strip() + \" 0m\"   # Adds 0 minute\n        else:\n            duration[i] = \"0h \" + duration[i]           # Adds 0 hour\n\nduration_hours = []\nduration_mins = []\nfor i in range(len(duration)):\n    duration_hours.append(int(duration[i].split(sep = \"h\")[0]))    # Extract hours from duration\n    duration_mins.append(int(duration[i].split(sep = \"m\")[0].split()[-1]))   # Extracts only minutes from duration","7afab84c":"train_data['Duration_hours']=duration_hours\ntrain_data['Duration_mins']=duration_mins","2b69de94":"train_data.head()","cdc95f9b":"train_data.drop(['Duration'],axis=1,inplace=True)","167448f9":"train_data.head()","351cb37f":"train_data['Airline'].value_counts()","ba336a6f":"# From graph we can see that Jet Airways Business have the highest Price.\n# Apart from the first Airline almost all are having similar median\n\n# Airline vs Price","b5fc76e2":"#dtale.show(train_data)","afa574c7":"# From graph we can see that Jet Airways Business have the highest Price.\n# Apart from the first Airline almost all are having similar median\n\n# airline vs price\nsns.catplot(y='Price',x='Airline',data=train_data.sort_values('Price',ascending=False),kind='boxen',height=6,aspect=3)","d5387ffd":"# as Airline is nominal categorical data we will perform onehotencoding\nAirline=train_data['Airline']\nAirline=pd.get_dummies(Airline,drop_first=True)\nAirline.head()","c2468283":"train_data['Source'].value_counts()","a17711bc":"sns.catplot(y='Price',x='Source',data=train_data.sort_values('Price',ascending=False),kind='boxen',height=6,aspect=3)","50888822":"Source=train_data['Source']\nSource=pd.get_dummies(Source,drop_first=True)\nSource.head()","6c622cd6":"train_data['Destination'].value_counts()","81f4191c":"## Destination is nominal data we will perform onehotencoding\nDestination=train_data['Destination']\nDestination=pd.get_dummies(Destination,drop_first=True)\nDestination.head()","928319af":"train_data['Route']","c5f92995":"train_data['Total_Stops'].value_counts()","2027dfb9":"## Additional info contains almost 80% of no info\n## Route and total stopes are related to each other\ntrain_data.drop(['Route','Additional_Info'],axis=1,inplace=True)","84dfe601":"train_data.head()","0818d779":"## as this case of ordinal categorical type we perform labelencoder\n# here value are assigned with corresponding keys\ntrain_data.replace({'non-stop':0,'1 stop':1,'2 stops':2,'3 stops':3,'4 stops':4},inplace=True)","cb3ebfe4":"train_data.head()","0372d084":"## concatenate dataframe ---> train data + airline + Source + Destination\ndata_train=pd.concat([train_data,Airline,Source,Destination],axis=1)","0d50214c":"data_train.head()","6702ea49":"data_train.drop(['Airline','Source','Destination'],axis=1,inplace=True)","a18ba37f":"data_train.head()","a02e150c":"data_train.columns","95f2cfbe":"test_data=pd.read_excel(r'..\/input\/test-data\/Test_set.xlsx')\ntest_data.head()","2d217e50":"# Preprocessing\n\nprint(\"Test data Info\")\nprint(\"-\"*75)\nprint(test_data.info())\n\nprint()\nprint()\n\nprint(\"Null values :\")\nprint(\"-\"*75)\ntest_data.dropna(inplace = True)\nprint(test_data.isnull().sum())\n\n# EDA\n\n# Date_of_Journey\ntest_data[\"Journey_day\"] = pd.to_datetime(test_data.Date_of_Journey, format=\"%d\/%m\/%Y\").dt.day\ntest_data[\"Journey_month\"] = pd.to_datetime(test_data[\"Date_of_Journey\"], format = \"%d\/%m\/%Y\").dt.month\ntest_data.drop([\"Date_of_Journey\"], axis = 1, inplace = True)\n\n# Dep_Time\ntest_data[\"Dep_hour\"] = pd.to_datetime(test_data[\"Dep_Time\"]).dt.hour\ntest_data[\"Dep_min\"] = pd.to_datetime(test_data[\"Dep_Time\"]).dt.minute\ntest_data.drop([\"Dep_Time\"], axis = 1, inplace = True)\n\n# Arrival_Time\ntest_data[\"Arrival_hour\"] = pd.to_datetime(test_data.Arrival_Time).dt.hour\ntest_data[\"Arrival_min\"] = pd.to_datetime(test_data.Arrival_Time).dt.minute\ntest_data.drop([\"Arrival_Time\"], axis = 1, inplace = True)\n\n# Duration\nduration = list(test_data[\"Duration\"])\n\nfor i in range(len(duration)):\n    if len(duration[i].split()) != 2:    # Check if duration contains only hour or mins\n        if \"h\" in duration[i]:\n            duration[i] = duration[i].strip() + \" 0m\"   # Adds 0 minute\n        else:\n            duration[i] = \"0h \" + duration[i]           # Adds 0 hour\n\nduration_hours = []\nduration_mins = []\nfor i in range(len(duration)):\n    duration_hours.append(int(duration[i].split(sep = \"h\")[0]))    # Extract hours from duration\n    duration_mins.append(int(duration[i].split(sep = \"m\")[0].split()[-1]))   # Extracts only minutes from duration\n\n# Adding Duration column to test set\ntest_data[\"Duration_hours\"] = duration_hours\ntest_data[\"Duration_mins\"] = duration_mins\ntest_data.drop([\"Duration\"], axis = 1, inplace = True)\n\n\n# Categorical data\n\nprint(\"Airline\")\nprint(\"-\"*75)\nprint(test_data[\"Airline\"].value_counts())\nAirline = pd.get_dummies(test_data[\"Airline\"], drop_first= True)\n\nprint()\n\nprint(\"Source\")\nprint(\"-\"*75)\nprint(test_data[\"Source\"].value_counts())\nSource = pd.get_dummies(test_data[\"Source\"], drop_first= True)\n\nprint()\n\nprint(\"Destination\")\nprint(\"-\"*75)\nprint(test_data[\"Destination\"].value_counts())\nDestination = pd.get_dummies(test_data[\"Destination\"], drop_first = True)\n\n# Additional_Info contains almost 80% no_info\n# Route and Total_Stops are related to each other\ntest_data.drop([\"Route\", \"Additional_Info\"], axis = 1, inplace = True)\n\n# Replacing Total_Stops\ntest_data.replace({\"non-stop\": 0, \"1 stop\": 1, \"2 stops\": 2, \"3 stops\": 3, \"4 stops\": 4}, inplace = True)\n\n# Concatenate dataframe --> test_data + Airline + Source + Destination\ndata_test = pd.concat([test_data, Airline, Source, Destination], axis = 1)\n\ndata_test.drop([\"Airline\", \"Source\", \"Destination\"], axis = 1, inplace = True)\n\nprint()\nprint()\n\nprint(\"Shape of test data : \", data_test.shape)\n","09de3d26":"data_test.head()","6dd76ae2":"data_train.shape","12c5e219":"data_train.columns","d70bee25":"X=data_train.loc[:,['Total_Stops','journey_day', 'journey_month', 'Dep_hours',\n       'Dep_min', 'Arrival_hour', 'Arrival_min', 'Duration_hours',\n       'Duration_mins', 'Air India', 'GoAir', 'IndiGo', 'Jet Airways',\n       'Jet Airways Business', 'Multiple carriers',\n       'Multiple carriers Premium economy', 'SpiceJet', 'Trujet', 'Vistara',\n       'Vistara Premium economy', 'Chennai', 'Delhi', 'Kolkata', 'Mumbai',\n       'Cochin', 'Delhi', 'Hyderabad', 'Kolkata', 'New Delhi']]","d816ad52":"X.head()","3875c38b":"y=data_train.iloc[:,1]\ny.head()","1f67d75f":"## Finds the corelation between independent and dependent variable\n\nplt.figure(figsize=(18,18))\nsns.heatmap(train_data.corr(),annot=True,cmap= 'inferno')","ced3da65":"from sklearn.ensemble import ExtraTreesRegressor","ee7a799b":"selection=ExtraTreesRegressor()\nselection.fit(X,y)","e31b1ee8":"print(selection.feature_importances_)","c11d71d3":"## plot graph of features importance for better visulization\nplt.figure(figsize=(12,8))\nfeat_importance=pd.Series(selection.feature_importances_,index=X.columns)\nfeat_importance.nlargest(20).plot(kind='barh')","a5963f77":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.2,random_state=42)","1e51af1d":"from sklearn.ensemble import RandomForestRegressor\nreg_rf=RandomForestRegressor()\nreg_rf.fit(X_train,y_train)","bacccfb7":"y_pred=reg_rf.predict(X_test)","ce155acb":"y_pred","4292d2b9":"y_test","fc34aad4":"reg_rf.score(X_train,y_train)","ed241b51":"reg_rf.score(X_test,y_test)","27f8309d":"sns.distplot(y_test-y_pred)\nplt.show()","51d30af2":"plt.scatter(y_test,y_pred,alpha=.6)\nplt.xlabel('y_test')\nplt.ylabel('y_pred')\nplt.show()","aef386aa":"from sklearn import metrics","73a14e05":"print('MAE:',metrics.mean_absolute_error(y_test,y_pred))\nprint('MSE:',metrics.mean_squared_error(y_test,y_pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","acd90cff":"metrics.r2_score(y_test,y_pred)","cd8d0487":"from sklearn.model_selection import RandomizedSearchCV","75e3b957":"#Randomized Search CV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15, 100]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 5, 10]","2a97c3c4":"# Create the random grid\n\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}","ef593280":"# Random search of parameters, using 5 fold cross validation, \n# search across 100 different combinations\nrf_random = RandomizedSearchCV(estimator = reg_rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2,\n                               random_state=42, n_jobs = 1)","77baf051":"rf_random.fit(X_train,y_train) ","330f555d":"rf_random.best_params_","d935a594":"prediction = rf_random.predict(X_test)","d6c56697":"prediction","ea264206":"y_test","e5c9ca1f":"#dtale.show(r)","a9468180":"plt.figure(figsize=(8,8))\nsns.distplot(y_test-prediction)","6d382697":"plt.figure(figsize=(8,8))\nplt.scatter(y_test,prediction,alpha=.6)\nplt.xlabel('y_test')\nplt.ylabel('prediction')\nplt.show()","7dd7e736":"import pickle\n# open a file, where you ant to store the data\nfile = open('flight_rf_1.pkl', 'wb')\n\n# dump information to that file\npickle.dump(rf_random, file)","c5a5d6a7":"model=open('flight_rf_1.pkl','rb')\nforest=pickle.load(model)","8113e803":"y_prediction = forest.predict(X_test)","63bd7a0c":"metrics.r2_score(y_test,y_prediction)","dfe4ffeb":"##### Fitting model using Random Forest\n1. Split dataset into train and test set in order to prediction w.r.t X_test\n2. If needed do scaling of data\n3. Scaling is not done in Random forest\n4. Import model\n5. Fit the data\n6. Predict w.r.t X_test\n7. In regression check RSME Score\n8. Plot graph","6ffbd0e5":"### Test Data","bb9c3852":"#### Handling Categorical Data","f56f79ae":"## EDA","c9944894":"One can find many ways to handle categorical data. Some of them categorical data are,\n\n1. **Nominal data** --> data are not in any order --> **OneHotEncoder** is used in this case\n2. **Ordinal data** --> data are in order --> **LabelEncoder** is used in this case","4136a6a4":"#### Hyperparameter Tuning\n1. Choose following method for hyperparameter tuning\n2. RandomizedSearchCV --> Fast\n3. GridSearchCV\n4. Assign hyperparameters in form of dictionery\n5. Fit the model\n6. Check best paramters and best score","649f109d":"### features selection\n\n1. heatmap\n2. feature_importance\n3. selectKBest"}}