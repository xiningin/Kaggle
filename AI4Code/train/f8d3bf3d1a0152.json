{"cell_type":{"b1f1c483":"code","71fc3088":"code","7abfbd3a":"code","5ab9fb22":"code","4ae23e46":"code","8c010eb1":"code","77d8206f":"code","5b5c63ed":"code","fe73446a":"code","f367be41":"code","4f86ff3a":"code","19cc6ac7":"code","725bd3c9":"code","133c212f":"code","be8b9356":"code","e487b402":"code","51625ba8":"code","69b2e389":"code","020ad2f2":"code","095c32a3":"code","9d89d2a0":"code","bb3003ae":"code","7acd8117":"code","1c1c1774":"code","a970dd3d":"code","6e1b0918":"code","261ceea4":"code","9b22e456":"code","a0286eca":"code","6615a4d6":"code","a5b0dead":"code","0f0929c9":"code","1dad087b":"code","73005ce5":"markdown","c2b177a8":"markdown","ada03ab9":"markdown","f7b3d622":"markdown","25a7d8e4":"markdown","fd8e263e":"markdown","011060f0":"markdown","5fae3370":"markdown","6e48b2ad":"markdown","5d236269":"markdown","b442b109":"markdown","79af47e7":"markdown","0d6b875e":"markdown"},"source":{"b1f1c483":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","71fc3088":"data = pd.read_csv(\"\/kaggle\/input\/sentiment140\/training.1600000.processed.noemoticon.csv\",encoding='latin-1')\ndata.head()","7abfbd3a":"DATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"TweetText\"]\ndata.columns = DATASET_COLUMNS","5ab9fb22":"data.describe(include='all')","4ae23e46":"data.dtypes","8c010eb1":"import copy\ndata_ = copy.deepcopy(data)\n\npositif_data = data_[data_.target==4].iloc[:80000,:]\nnegative_data = data_[data_.target==0].iloc[:80000,:]\n\nsub_data = pd.concat([positif_data,negative_data],axis=0)","77d8206f":"data.boxplot(column='target')","5b5c63ed":"data_target=data.groupby('target')","fe73446a":"data['target'].value_counts()","f367be41":"data.head()","4f86ff3a":"data_ = {'target': data['target'], 'date': data['date']}\ndf = pd.DataFrame(data_)\ndf.head()","19cc6ac7":"# lets ensure the 'date' column is in date format\ndf['date'] = pd.to_datetime(df['date'])","725bd3c9":"hour = [ df['date'][i].hour for i in range(len(df['date'])) ]\ndf['hour'] = hour\ndf.head()","133c212f":"hour_data = {'0': [0]*24, '2': [0]*24, '4': [0]*24}\nfor i in range(len(df['hour'])):\n    target = str(df['target'][i])\n    hour = int(df['hour'][i])\n    hour_data[target][hour] += 1","be8b9356":"hour_data = [hour_data['0'], hour_data['2'], hour_data['4']]\n# Transpose\nhour_data = list(map(list,zip(*hour_data)))","e487b402":"df1 = pd.DataFrame(hour_data,index = [i for i in range(24)],columns=['negative', 'neutral', 'positive'])","51625ba8":"df1.plot()","69b2e389":"positive_at_count = 0\nnegative_at_count = 0\nTweetTextList = list(sub_data['TweetText'])\ntargetList = list(sub_data['target'])\nfor i in range(len(sub_data['TweetText'])):\n    if TweetTextList[i].find('@') != -1:\n        if targetList[i] == 4:\n            positive_at_count += 1\n        else:\n            negative_at_count += 1\nat_counts = [positive_at_count, negative_at_count]","020ad2f2":"import matplotlib.pyplot as plt\nnames = ['positive', 'negative']\nvalues = [positive_at_count, negative_at_count]\nplt.bar(names, values)","095c32a3":"import copy\nnewdata = copy.deepcopy(sub_data)\nnewdata.drop(['ids','date','flag','user'],axis = 1,inplace = True)","9d89d2a0":"import wordcloud\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport re\nimport nltk\nimport string\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os","bb3003ae":"all_words = ' '.join([text for text in newdata['TweetText']])\nwordcloud = WordCloud(width=800,height=500,random_state=21,max_font_size=110).generate(all_words)\nplt.figure(figsize=(10,7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","7acd8117":"positive_words = ' '.join([text for text in data['TweetText'][data['target']==4]])\nwordcloud = WordCloud(width=800,height=500,random_state=21,max_font_size=110).generate(positive_words)\nplt.figure(figsize=(10,7))\nplt.imshow(wordcloud,interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","1c1c1774":"negative_words = ' '.join([text for text in data['TweetText'][data['target']==0]])\nwordcloud = WordCloud(width=800,height=500,random_state=21,max_font_size=110).generate(negative_words)\nplt.figure(figsize=(10,7))\nplt.imshow(wordcloud,interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","a970dd3d":"positif_data = data[data.target==4].iloc[:10000,:]\nprint(positif_data.shape)\nnegative_data = data[data.target==0].iloc[:10000,:]\nprint(negative_data.shape)\ndata = pd.concat([positif_data,negative_data],axis = 0)\nprint(data.shape)\ndata.head()","6e1b0918":"# Removing Twitter Handles (@user)\ndata['Clean_TweetText'] = data['TweetText'].str.replace(\"@\", \"\") \n# Removing links\ndata['Clean_TweetText'] = data['Clean_TweetText'].str.replace(r\"http\\S+\", \"\") \n# Removing Punctuations, Numbers, and Special Characters\ndata['Clean_TweetText'] = data['Clean_TweetText'].str.replace(\"[^a-zA-Z]\", \" \") \n# Remove stop words\nimport nltk\nstopwords=nltk.corpus.stopwords.words('english')\ndef remove_stopwords(text):\n    clean_text=' '.join([word for word in text.split() if word not in stopwords])\n    return clean_text\ndata['Clean_TweetText'] = data['Clean_TweetText'].apply(lambda text : remove_stopwords(text.lower()))\ndata.head()","261ceea4":"# Text Tokenization and Normalization\ndata['Clean_TweetText'] = data['Clean_TweetText'].apply(lambda x: nltk.word_tokenize(x))\ndata.head()","9b22e456":"# Now let\u2019s stitch these tokens back together\ndata['Clean_TweetText'] = data['Clean_TweetText'].apply(lambda x: ' '.join([w for w in x]))\n# Removing small words\ndata['Clean_TweetText'] = data['Clean_TweetText'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\ndata.head()","a0286eca":"from xgboost import XGBClassifier\nimport xgboost as xgb\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn import svm\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer","6615a4d6":"count_vectorizer = CountVectorizer(stop_words='english') \ncv = count_vectorizer.fit_transform(data['Clean_TweetText'])\ncv.shape","a5b0dead":"X_train,X_test,y_train,y_test = train_test_split(cv,data['target'] , test_size=.2,stratify=data['target'], random_state=42)","0f0929c9":"# XGBC\nxgbc = XGBClassifier(max_depth=6, n_estimators=1000, nthread= 3)\nxgbc.fit(X_train,y_train)\nprediction_xgb = xgbc.predict(X_test)\nprint(accuracy_score(prediction_xgb,y_test))","1dad087b":"# RandomForest\nrf = RandomForestClassifier(n_estimators=1000, random_state=42)\nrf.fit(X_train,y_train)\nprediction_rf = rf.predict(X_test)\nprint(accuracy_score(prediction_rf,y_test))","73005ce5":"# Data explore","c2b177a8":"What are the most common negative words in dataset?","ada03ab9":"What are the most common words in dataset?","f7b3d622":"## Words\n\nWords distribution.","25a7d8e4":"What are the most common positive words in dataset?","fd8e263e":"### Target","011060f0":"## Date\n\nAt what time do people like to tweet? Is there a clear link between the time of tweeting and the emotion of the content?","5fae3370":"# Models","6e48b2ad":"# Introduction\n\nWe are going to build an depression classifier on twitter. You may wonder why we are doing this, for these reasons:\n1. there are more and more depressions happening in our socialty\n2. twitter had provided it's api for developer to access it's data before 2006\n\nWhat did others do?\n1. better than 0.9 accuracy\n\nWhat will we do after catching up with the achievements of others?\n1. get more data on Twitter, and study the method\n2. make some interesting exploration. such as detect in tweet time as [this](https:\/\/medium.com\/datadriveninvestor\/a-machine-learning-approach-for-detection-of-depression-and-mental-illness-in-twitter-3f3a32a4df60) said and ating behaviour\n3. try more models, and study the differences and principles\n4. trace twitts of a user, use lstm model and Markov chain\n5. explore the ating behaviour","5d236269":"Get a subset of data to speed it up.","b442b109":"# Data preprocess","79af47e7":"# Dataset\n\nWe use the [sentiment140 dataset](https:\/\/www.kaggle.com\/kazanova\/sentiment140). It contains 1,600,000 tweets extracted using the twitter api. The tweets have been annotated (0 = negative, 4 = positive) and they can be used to detect sentiment.\n\nIt contains the following 6 fields:\n\n1. target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n2. ids: The id of the tweet ( 2087)\n3. date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n4. flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n5. user: the user that tweeted (robotickilldozr)\n6. text: the text of the tweet (Lyx is cool)","0d6b875e":"## @\n\nDoes @Behavior reflect the mood of that tweet?"}}