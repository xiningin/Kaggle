{"cell_type":{"436cff2d":"code","db16ebeb":"code","f057a5c0":"code","98cc3164":"code","5fcdbe6d":"code","e8f20d26":"code","0c716a2b":"code","f9bfb726":"code","36e98588":"code","9fc29255":"code","1e6b294c":"code","d1e3ce26":"code","40c2e5e9":"code","e089116c":"code","4688f6db":"code","6aced82d":"code","5a60a44f":"code","af0a0f3d":"code","8d6fa47d":"code","3137fa71":"code","4c2493d6":"code","105bc798":"code","22c52fc3":"code","c0f83c5b":"code","6efe89bb":"code","029a0ec2":"code","e09e9cc6":"code","1ec5e409":"code","7ff41b2d":"code","3889875f":"code","9919597e":"code","f10e42a5":"code","3e1306ef":"code","822c960e":"code","c9cdc49b":"code","84e77ab6":"code","2681cd59":"code","2cb49cb3":"code","cccc0cb2":"code","978664c8":"code","3a4b5130":"code","225ba709":"code","828f0f72":"code","3ecd1a06":"code","a7902f4b":"code","6298d995":"code","e9d556c6":"code","e9b26d54":"code","6a3859d5":"code","7971246e":"code","2bce7bbf":"code","79d033b2":"code","e0e1cf44":"code","7702e3c0":"code","0b448f42":"code","b24909a3":"code","610a7be6":"code","dab25559":"code","33e7744b":"code","5d1569cd":"code","4d2396c5":"code","52d6a01f":"code","62b64375":"code","54caee10":"code","a905cb3e":"code","2a0f5142":"code","745039b9":"code","63fc3468":"code","f98915d0":"code","8cb13e6a":"code","289b5a79":"code","2e3f86ee":"markdown","efcc6322":"markdown","751ff003":"markdown","65a9a5f6":"markdown","730128b7":"markdown","5bc2a0f0":"markdown","fe1a6cb9":"markdown"},"source":{"436cff2d":"\n# Objective :\n# To run 4 models (KNN, Logistic Regression, SVM and Decision Tree) \n# on the data, pick the best model, then tune the hyperparameter of the selected model\n# and present analysis and recommendations to the Client\n# Analysis and recoendations on on Powerpoint presentation\nimport pandas as pd\nimport numpy as np\nfrom sklearn import datasets\nimport seaborn as sns\nsns.set()\n# Sklearn related imports\nfrom sklearn import datasets\nfrom sklearn import metrics\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LinearRegression\n\nimport matplotlib.pyplot as plt\n","db16ebeb":"import os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n #       print(os.path.join(dirname, filename))","f057a5c0":"\n#\n# Load bank marketing data \n#\ndf = pd.read_csv('\/kaggle\/input\/bank-term-deposit\/Train.csv')\n\ndf","98cc3164":"df['term_deposit_subscribed'].value_counts()","5fcdbe6d":"3394\/(28253+3394)","e8f20d26":"sns.countplot(x='term_deposit_subscribed', data=df)\nplt.show()","0c716a2b":"df.info()","f9bfb726":"# Look for cells with missing data (i.e. Null or NA)\ndf.isna().any().any()\n# count the number of missing data for each feature\ndf_na = df.isna().sum()\ndf_na\n# this will show only features that have nonzero missing values\n# df_na[df_na!=0]","36e98588":"df= df.fillna(df.mean())","9fc29255":"df_na = df.isna().sum()\ndf_na","1e6b294c":"df= df.ffill()","d1e3ce26":"del df['days_since_prev_campaign_contact']","40c2e5e9":"df_na = df.isna().sum()\ndf_na","e089116c":"del df['id'] # drop three irrelevant columns","4688f6db":"del df['month']","6aced82d":"del df['day_of_month']","5a60a44f":"df\n","af0a0f3d":"# limit to categorical data using df.select_dtypes()\ndf_cat = df.select_dtypes(include=['object'])\ndf_cat.nunique()","8d6fa47d":"# limit to numerical data using df.select_dtypes()\ndf_num = df.select_dtypes(include=['number'])\ndf_num.nunique()","3137fa71":"df_cat.columns","4c2493d6":"df.describe() ","105bc798":"_ = sns.pairplot(df, corner=True)","22c52fc3":"# Visualize correlations drilled down by dependent variable\n_ = sns.pairplot(df, corner=True, hue='term_deposit_subscribed')","c0f83c5b":"categorical_var = [i for i in df.columns if df[i].dtypes !='object']\ncategorical_var","6efe89bb":"df.hist(bins=40, figsize=(20,20), layout=(10,3), color=\"#FA5858\") \nplt.show()","029a0ec2":"df.hist(column='balance', bins=50, figsize=(5,5), color=\"#FA5858\") \nplt.show()","e09e9cc6":"df.hist(column='num_contacts_in_campaign', bins=40, figsize=(5,5), color=\"#FA5858\") \nplt.show()","1ec5e409":"df.hist(column='num_contacts_prev_campaign', bins=80, figsize=(5,5), color=\"#FA5858\") \nplt.show()","7ff41b2d":"df.hist(column='last_contact_duration', bins=40, figsize=(5,5), color=\"#FA5858\") \nplt.show()","3889875f":"# Let's look at all the categorical variables and their impact on churn\n\n# Removing churn variable for analysis\ncategorical_var = [i for i in df.columns if df[i].dtypes =='object']\ncatVars_noChurn = categorical_var[:]\n\nfig ,ax = plt.subplots(4,2,figsize=(20,20))\nfor axi ,var in zip(ax.flat,catVars_noChurn):\n    sns.countplot(x=df.term_deposit_subscribed,hue=df[var],ax=axi)","9919597e":"# correlation matrix heatmap visualization\nsns.set(style=\"white\")\n\n# Generate a mask for the upper triangle\nmatrix = np.triu(df.corr())\n\n# Set up the matplotlib figure to control size of heatmap\nfig, ax = plt.subplots(figsize=(10,10))\n\n\n_ = sns.heatmap(df.corr(), annot=True, annot_kws={\"size\": 12}, square=True, \ncmap='coolwarm' , vmin=-1, vmax=1, fmt='.2f')\n\n","f10e42a5":"# Encode variables with more than 2 Classes\ndf = pd.get_dummies(df, columns= [i for i in df.columns if df[i].dtypes=='object'],drop_first=True)","3e1306ef":"df","822c960e":"plt.figure(figsize=(16,8))\n_ = sns.heatmap(df.corr(), annot=True, annot_kws={\"size\": 9})","c9cdc49b":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score,f1_score,make_scorer,mean_squared_error, mean_absolute_error,r2_score\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nimport imblearn\nfrom imblearn.over_sampling import SMOTE","84e77ab6":"X=df.drop(['term_deposit_subscribed'],axis=1)\ny=df['term_deposit_subscribed']","2681cd59":"# 2) Splitting our data into training and testing sets\n# Split the Data\nX_y_train_test = train_test_split(X, y, test_size = 0.2, shuffle=True, random_state = 0)\nX_train, X_test, y_train, y_test = X_y_train_test\n","2cb49cb3":"X","cccc0cb2":"X_train.shape","978664c8":"X_test.shape","3a4b5130":"#importing imbalanced Learning library\nimport imblearn\nfrom imblearn.over_sampling import SMOTE\n\n# transform the dataset using SMOTE\nsmote = SMOTE()\nX_resampled, y_resampled = smote.fit_resample(X_train.values, y_train.values)\nX_y_resampled_test = X_resampled, X_test,y_resampled, y_test","225ba709":"# Confirmed training data has been resampled\nX_resampled.shape","828f0f72":"y_resampled.shape","3ecd1a06":"X_resampled","a7902f4b":"X_test.shape","6298d995":"y_test.shape","e9d556c6":"pd.Series(y_resampled).value_counts()","e9b26d54":"# Create the classifiers\nrf = RandomForestClassifier() \nsvm_clf = SVC()\nlog_res = LogisticRegression(max_iter=2000) \nknn = KNeighborsClassifier(n_neighbors=3) \n","6a3859d5":"# Manual pipeline\n\ndef train_predict_F1score(model, X_y_train_test):\n    X_train, X_test, y_train, y_test = X_y_train_test\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    score = f1_score(y_test, y_pred, average='macro')\n    return score","7971246e":"# KNN Model\nknn = KNeighborsClassifier(n_neighbors=3)\ntrain_predict_F1score(knn, X_y_train_test)","2bce7bbf":"# Random Forest model \nrf = RandomForestClassifier() \ntrain_predict_F1score(rf, X_y_train_test)","79d033b2":"# Logistic Regression model \nlog_res = LogisticRegression(max_iter=2000) \ntrain_predict_F1score(log_res, X_y_train_test)","e0e1cf44":"# SVM Model\nsvm_clf = SVC()\ntrain_predict_F1score(svm_clf, X_y_train_test)","7702e3c0":"# Manual pipeline\n\ndef train_predict_F1score(model, X_y_resampled_test):\n    X_train, X_test, y_train, y_test = X_y_resampled_test\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    score = f1_score(y_test, y_pred, average='macro')\n    return score","0b448f42":"# KNN model resampled with SMOTE()\nknn = KNeighborsClassifier(n_neighbors=3)\ntrain_predict_F1score(knn, X_y_resampled_test)","b24909a3":"# Random Forest modelresampled with SMOTE()\nrf = RandomForestClassifier() \ntrain_predict_F1score(rf, X_y_resampled_test)","610a7be6":"# Logistic Regression model resampled with SMOTE()\nlog_res = LogisticRegression(max_iter=2000) \ntrain_predict_F1score(log_res, X_y_resampled_test)","dab25559":"# SMV model resampled with SMOTE()\nsvm_clf = SVC()\ntrain_predict_F1score(svm_clf, X_y_resampled_test)","33e7744b":"y_pred = knn.predict(X_test)\n\n# Classification Report\nprint(\"Classification report for KNN Model with resampled train data:\")\nprint()\nprint(classification_report(y_test, y_pred))\nprint()\n\n# Plot the confusion matrix using Seaborn library\nprint(\"Correlation Matrix for KNN Model with resampled train data:\")\nplt.figure(figsize=(5,5))\n_ = sns.heatmap(confusion_matrix(y_test, y_pred), \n                annot=True,fmt='', annot_kws={\"size\": 18},\n                cmap=plt.cm.Blues)\n_ = plt.ylabel('Actual', fontweight='bold')\n_ = plt.xlabel('Predicted', fontweight='bold')","5d1569cd":"y_pred = rf.predict(X_test)\n\n# Classification Report\nprint(\"Classification report for Random Forest Model with resampled train data:\")\nprint()\nprint(classification_report(y_test, y_pred))\nprint()\n\n# Plot the confusion matrix using Seaborn library\nprint(\"Correlation Matrix for Random Forest Model with resampled train data:\")\nplt.figure(figsize=(5,5))\n_ = sns.heatmap(confusion_matrix(y_test, y_pred), \n                annot=True,fmt='', annot_kws={\"size\": 18},\n                cmap=plt.cm.Blues)\n_ = plt.ylabel('Actual', fontweight='bold')\n_ = plt.xlabel('Predicted', fontweight='bold')","4d2396c5":"y_pred = log_res.predict(X_test)\n\n# Classification Report\nprint(\"Classification report for Logistic Regression Model with resampled train data:\")\nprint()\nprint(classification_report(y_test, y_pred))\nprint()\n\n# Plot the confusion matrix using Seaborn library\nprint(\"Correlation Matrix for Logistic Regression Model with resampled train data:\")\nplt.figure(figsize=(5,5))\n_ = sns.heatmap(confusion_matrix(y_test, y_pred), \n                annot=True,fmt='', annot_kws={\"size\": 18},\n                cmap=plt.cm.Blues)\n_ = plt.ylabel('Actual', fontweight='bold')\n_ = plt.xlabel('Predicted', fontweight='bold')","52d6a01f":"y_pred = svm_clf.predict(X_test)\n\n# Classification Report\nprint(\"Classification report for SVM Model with resampled train data:\")\nprint()\nprint(classification_report(y_test, y_pred))\nprint()\n\n\n# Plot the confusion matrix using Seaborn library\nprint(\"Correlation Matrix for SVM Model with resampled train data:\")\nplt.figure(figsize=(5,5))\n_ = sns.heatmap(confusion_matrix(y_test, y_pred), \n                annot=True,fmt='', annot_kws={\"size\": 18},\n                cmap=plt.cm.Blues)\n_ = plt.ylabel('Actual', fontweight='bold')\n_ = plt.xlabel('Predicted', fontweight='bold')","62b64375":"rf_classifier = RandomForestClassifier()\nprint(rf.get_params().keys())","54caee10":"# Random forest initially\nfrom sklearn.model_selection import GridSearchCV\n\nrf_classifier = RandomForestClassifier()\n\nparam_grid = { \n    'n_estimators': [800,900,1000,1100,1200],\n    'max_features':  ['auto'],            \n    'max_depth' : [22,24,26,28,30],\n    'criterion' :['gini', 'entropy']\n}\n # ['auto', 'sqrt', 'log2'],\ngs_clf = GridSearchCV(rf_classifier,\n                      param_grid, \n                      cv=5,\n                      scoring='f1',\n                      n_jobs=-1)\ngs_clf.fit(X_train, y_train)","a905cb3e":"gs_clf.best_params_","2a0f5142":"rf_classifier = RandomForestClassifier()\ntrain_predict_F1score(gs_clf.best_estimator_, X_y_resampled_test)","745039b9":"# Creating the feature importances dataframe\nfeature_importance = np.array(rf.feature_importances_)\nfeature_names = np.array(X.columns)\nsorted_importance = np.array(sorted(list(zip(feature_importance, feature_names)), reverse=True))\nsorted_importance.shape\nfeat_imp = pd.DataFrame({'feature_names':sorted_importance[:,1],'feature_importance':sorted_importance[:,0]})","63fc3468":"print(\"Sorted Feature Importance for Random Forest:\")\nfeat_imp","f98915d0":"# Creating the feature importances dataframe\n#feature_importance = np.array(rf.feature_importances_)\n#feature_names = np.array(X.columns)\n#sorted_importance = np.array(sorted(list(zip(feature_importance, feature_names)), reverse=True))\n#sorted_importance.shape\n#feat_imp = pd.DataFrame({'feature_names':sorted_importance[:,1],'feature_importance':sorted_importance[:,0]})","8cb13e6a":"# Creating the feature importances dataframe\nfeature_importance = np.array(rf.feature_importances_)\nfeature_names = np.array(X.columns)\n\nfeat_imp = pd.DataFrame({'feature_names':feature_names,'feature_importance':feature_importance})","289b5a79":"plt.figure(figsize=(10,8))\nsns.barplot(x=feat_imp['feature_importance'], y=feat_imp['feature_names'])","2e3f86ee":"# USING SMOTE \"RESAMPLED\" for training and testing","efcc6322":"# Data preparation for training and testing\n","751ff003":"# convert categorical data to numeric","65a9a5f6":"# Hyperparameters for Random Forest (RESAMPLED)","730128b7":"# Data loading and preprocessing","5bc2a0f0":"# EDA","fe1a6cb9":"# post mortem analysis\n-What went wrong? Any unexpected results?\n-Show features importance (What is the most important predictors? why\/expected?)\n-Steps needed to improve predictions (e.g. data collections, data preprocessing, feature engineering, chosen model, -chosen hyperparameters) ?\n\n"}}