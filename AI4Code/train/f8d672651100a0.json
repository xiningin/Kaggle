{"cell_type":{"4f4468e6":"code","bc387a9e":"code","1d615904":"code","9ce627da":"code","9a4e824b":"code","09ea9415":"code","4b4dbf91":"code","230af1f8":"code","85656003":"code","ffc64ffc":"code","69e612d6":"code","bce91560":"code","e75424d8":"code","dc12b5b6":"code","745cd8d7":"code","b4cbd5ee":"code","b17e45c1":"code","fb510f74":"code","6e823c74":"code","5cbcb8ed":"code","2269119e":"code","5ab079f5":"code","f81dd82b":"code","f39b0541":"code","cd779956":"code","5619b14c":"code","3b89930f":"code","a26dd9e1":"code","cdf04d71":"code","8bc57ce9":"code","656cf194":"code","5658adca":"code","5103e0ea":"code","1b4fdf8f":"code","2ea4e806":"code","79ffdc54":"code","816fb579":"code","c0b3eecd":"code","1e0c2f8c":"code","9d19dfe5":"code","a9087b8d":"code","d297e06c":"code","dc3af98c":"code","14ab4889":"code","b2644337":"code","ad12cb49":"code","de440c58":"code","5293f299":"code","fe3d654f":"code","1cb76e62":"code","6e4b8c5e":"code","7fd0e851":"code","2fe74a73":"code","c194e672":"code","ef8094e6":"code","18fe8b36":"code","1c127a38":"code","b1e6dc5d":"code","684ed446":"code","590a7f4a":"code","121e6375":"code","66d324dc":"code","baa22360":"code","d7c1d624":"code","275b7ce2":"code","bd60481a":"code","c76cd32e":"code","2792b1d8":"code","302747c5":"code","9a367e97":"code","1f91e7e0":"code","ff3c82aa":"code","ca8dba03":"code","18d66590":"code","4d563735":"code","4108f683":"code","c357e91c":"markdown","9f7e0608":"markdown","98c7e438":"markdown","77a2165e":"markdown","72976d7f":"markdown","9d6db1d1":"markdown","7279cabe":"markdown","ee96cdd0":"markdown","f86ad329":"markdown","da74067a":"markdown","797cd3d6":"markdown","47aae80a":"markdown","31d9b63d":"markdown","366104d1":"markdown","125e4451":"markdown","b23094c3":"markdown","de8d56b2":"markdown"},"source":{"4f4468e6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bc387a9e":"# Here we are going to import these libraries for data analysis \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom matplotlib import style\n\n# Now we need these libraries for appling Machine Learning\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, plot_confusion_matrix, precision_score, recall_score\n\nfrom sklearn.metrics import precision_recall_curve, plot_precision_recall_curve, plot_roc_curve\n\n%matplotlib inline","1d615904":"# here we are loading train data \ndf_train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\n# checking what kind of data we have\ndf_train.head()","9ce627da":"# here we are loading test data\ndf_test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n# checking what kind of data we have\ndf_test.head()","9a4e824b":"# here we are loading gender data\ndf_y_test = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\ndf_y_test.head()","09ea9415":"#now we join y_test data to df_test dataframe\ndf_test['Survived'] = df_y_test['Survived']","4b4dbf91":"# using seaborn we graph heatmap of train data \nsns.heatmap(df_train.corr(), annot=True,cmap='Reds')","230af1f8":"# using seaborn we graph heatmap of test data \nsns.heatmap(df_test.corr(), annot=True,cmap='Reds')","85656003":"# Checking train data \ndf_train.head()","ffc64ffc":"# here we get compute correlation of Survied coulumn of train data by sort  \npd.DataFrame([df_train.corr()['Survived'].sort_values()])","69e612d6":"# set font ,color,font size and weight\nfont = {'family': 'serif',\n        'color':  'white',\n        'weight': 'normal',\n        'size': 20,\n        }\n# set background dark\nplt.style.use('dark_background')\n# using the variable ax for single a Axes\nfig, ax = plt.subplots()\n\nX=df_train['Fare']\ny=df_train['Survived']\n\n#Create a figure Fare on x axis and Survied on y axis\nax.set_xlabel('Fare', fontdict=font)\nax.set_ylabel('Survived', fontdict=font)\n\n# plot graph\/diagram  \nplt.plot(X,y, 'ro', alpha=0.5)","bce91560":"#There are 177 rows with missing Age, 687 rows with missing Cabin and 2 rows with missing Embarked information.\ndf_train.isnull().sum()","e75424d8":"# percent of missing data in train data\n100*(df_train.isnull().sum()\/len(df_train))","dc12b5b6":"#Make a Function to calculate the percent of missing data in each columns (feature) and then sort it\ndef missing_percent(df):\n    nan_percent= 100*(df.isnull().sum()\/len(df))\n    nan_percent= nan_percent[nan_percent>0].sort_values()\n    return nan_percent","745cd8d7":"# display missing percent\nnan_percent= missing_percent(df_train)\nnan_percent","b4cbd5ee":"# now ploting barplot of missing percent\nnan_percent= missing_percent(df_train)\n\nplt.figure(figsize=(12,6))\nsns.barplot(x=nan_percent.index, y=nan_percent)\nplt.xticks(rotation=90)","b17e45c1":"# appling check\nnan_percent[nan_percent<1].index","fb510f74":"# extract nan values\ndf_train[df_train['Embarked'].isnull()]","6e823c74":"#These rows has the same ticket number. So they Embarked place is same too! But we don't have more information about these 2 rows. So we decide to drop these 2.\ndf_train= df_train.dropna(axis=0, subset=['Embarked'])\n","5cbcb8ed":"#now ploting age and cabin\nnan_percent= missing_percent(df_train)\n\nplt.figure(figsize=(12,6))\nsns.barplot(x=nan_percent.index, y=nan_percent)\nplt.xticks(rotation=90)","2269119e":"# appling median and then sum\ndf_train['Age'] = df_train['Age'].fillna(df_train['Age'].median())\ndf_train.isnull().sum()","5ab079f5":"#ploting cabin\nnan_percent= missing_percent(df_train)\n\nplt.figure(figsize=(12,6))\nsns.barplot(x=nan_percent.index, y=nan_percent)\nplt.xticks(rotation=90)","f81dd82b":"#droping cabin\ndf_train=df_train.drop(['Cabin'], axis=1)","f39b0541":"# no value now\nnan_percent= missing_percent(df_train)\nnan_percent","cd779956":"#checking test data\ndf_test.head()","5619b14c":"# here we get compute correlation of Survied coulumn of test data by sort \npd.DataFrame([df_test.corr()['Survived'].sort_values()])","3b89930f":"# set font ,color,font size and weight\nfont = {'family': 'serif',\n        'color':  'white',\n        'weight': 'normal',\n        'size': 20,\n        }\n\n# set background dark\nplt.style.use('dark_background')\n\n# using the variable ax for single a Axes\nfig, ax = plt.subplots()\n\nX=df_test['Fare']\ny=df_test['Survived']\n\n#Create a figure Fare on x axis and Survied on y axis\nax.set_xlabel('Fare', fontdict=font)\nax.set_ylabel('Survived', fontdict=font)\n\n  # plot graph\/diagram \nplt.plot(X,y, 'ro', alpha=0.5)","a26dd9e1":"#There are 86 rows with missing Age, 327 rows with missing Cabin \ndf_test.isnull().sum()","cdf04d71":"# percent of missing data in train data\n100*(df_test.isnull().sum()\/len(df_test))","8bc57ce9":"# display missing percent\nnan_percent= missing_percent(df_test)\nnan_percent","656cf194":"# now ploting barplot of missing percent\nplt.figure(figsize=(12,6))\nsns.barplot(x=nan_percent.index, y=nan_percent)\nplt.xticks(rotation=90)","5658adca":"# filtering\nnan_percent[nan_percent<1].index","5103e0ea":"#null value in Fare\ndf_test[df_test['Fare'].isnull()]","1b4fdf8f":"# Drop null value\ndf_test= df_test.dropna(axis=0, subset=['Fare'])","2ea4e806":"#plot age and cabin\nnan_percent= missing_percent(df_test)\n\nplt.figure(figsize=(12,6))\nsns.barplot(x=nan_percent.index, y=nan_percent)\nplt.xticks(rotation=90)","79ffdc54":"# appling median and then sum\ndf_test['Age'] = df_test['Age'].fillna(df_test['Age'].median())\ndf_test.isnull().sum()","816fb579":"#plot cabin\nnan_percent= missing_percent(df_test)\n\nplt.figure(figsize=(12,6))\nsns.barplot(x=nan_percent.index, y=nan_percent)\nplt.xticks(rotation=90)","c0b3eecd":"#drop cabin\ndf_test=df_test.drop(['Cabin'], axis=1)","1e0c2f8c":"# getting nan_percent\nnan_percent= missing_percent(df_test)\nnan_percent","9d19dfe5":"#droping values which is not need\ndf_train.drop(['Name','Ticket', 'PassengerId'], axis = 1, inplace=True)\ndf_test.drop(['Name','Ticket', 'PassengerId'], axis = 1, inplace=True)","a9087b8d":"#train data detail\ndf_train.info()","d297e06c":"#rows and coulumns\ndf_train.shape","dc3af98c":"#Survived should not be integer. It's a categorical variable and it's not calculatable.\ndf_train['Survived'] = df_train['Survived'].apply(str)","14ab4889":"# data detail\ndf_train.info()","b2644337":"#Pclass should not be integer. It's a categorical variable and it's not calculatable.\ndf_train['Pclass'] = df_train['Pclass'].apply(str)","ad12cb49":"#data detail\ndf_train.info()","de440c58":"#Seperate the columns with object type\ndf_train_num = df_train.select_dtypes(exclude='object')\ndf_train_obj = df_train.select_dtypes(include='object')","5293f299":"#drop value\ndf_train_obj.drop('Survived', axis = 1, inplace=True)","fe3d654f":"#transform to Dummy variable\ndf_train_obj = pd.get_dummies(df_train_obj, drop_first=True)","1cb76e62":"#Join the columns\nFinal_train_df = pd.concat([df_train_num, df_train_obj,df_train['Survived']], axis=1)\n\nFinal_train_df","6e4b8c5e":"#rows and colulumns\nFinal_train_df.shape","7fd0e851":"# data detail\ndf_test.info()","2fe74a73":"# rows and coulumns\ndf_test.shape","c194e672":"#Survived should not be integer. It's a categorical variable and it's not calculatable.\n\ndf_test['Survived'] = df_test['Survived'].apply(str)","ef8094e6":"# data info\ndf_test.info()","18fe8b36":"#Pclass should not be integer. It's a categorical variable and it's not calculatable.\n\ndf_test['Pclass'] = df_test['Pclass'].apply(str)","1c127a38":"# data information\ndf_test.info()","b1e6dc5d":"#Seperate the columns with object type\ndf_test_num = df_test.select_dtypes(exclude='object')\ndf_test_obj = df_test.select_dtypes(include='object')","684ed446":"# drop value\ndf_test_obj.drop('Survived', axis = 1, inplace=True)","590a7f4a":"#transform to dummy variable\ndf_test_obj = pd.get_dummies(df_test_obj, drop_first=True)","121e6375":"#Join the columns\nFinal_test_df = pd.concat([df_test_num, df_test_obj,df_test['Survived']], axis=1)","66d324dc":"#X : Features\n#y : Target variable\n\nX_train=Final_train_df.drop('Survived', axis = 1)\ny_train=Final_train_df['Survived']\n\nX_test=Final_test_df.drop('Survived', axis = 1)\ny_test = Final_test_df['Survived']","baa22360":"# rows and coulumns\nX_train.shape","d7c1d624":"# rows and coulumns\nX_test.shape","275b7ce2":"#we use StandardScaler from sklearn.preprocessing to make dataset values standard\nscaler= StandardScaler()\n\nscaler.fit(X_train)","bd60481a":" #our model is a Logistic Regression\nLogistic_model = LogisticRegression(max_iter=4000)\n\nLogistic_model.fit(X_train,y_train)","c76cd32e":"# predict value\ny_pred = Logistic_model.predict(X_test)","2792b1d8":"#Evaluate Model\nconfusion_matrix(y_test, y_pred)","302747c5":"#plot confusion matrix\nplot_confusion_matrix(Logistic_model, X_test, y_test)","9a367e97":"#accuracy\naccuracy_score(y_test, y_pred)","1f91e7e0":"#precision\nprecision_score(y_test, y_pred,average='macro')","ff3c82aa":"#recall score\nrecall_score(y_test, y_pred, average='macro')","ca8dba03":"#Ploting\nplot_precision_recall_curve(Logistic_model, X_test, y_test)","18d66590":"#Ploting\nplot_roc_curve(Logistic_model, X_test, y_test)","4d563735":"#fetching accuracy,precision, recall\npd.DataFrame({ '':['Accuracy','precision','recall'],'Value': [accuracy_score(y_test, y_pred),\n                precision_score(y_test, y_pred,average='macro'),recall_score(y_test, y_pred, average='macro')]})","4108f683":"#submitting the predicted data into Logistic_Reg_submission.csv\nsubmission = pd.DataFrame()\nsubmission['PassengerId'] = df_test.index\nsubmission['Survived'] = y_pred\nsubmission.to_csv('Logistic_Reg_submission.csv',index=False)","c357e91c":"**Scaling Data**","9f7e0608":"**Split Data to Train & Test**","98c7e438":"**Accuracy**","77a2165e":"**Submitting the predicted data**","72976d7f":"**Train**","9d6db1d1":"**Curves & AUC**","7279cabe":"**Data Prepration**","ee96cdd0":"**Loading Libraries required**","f86ad329":"**Test Set**","da74067a":"**Precision Score**","797cd3d6":"**Evaluate Model**","47aae80a":" **Test Set**","31d9b63d":"**Build the model**","366104d1":" **Now null values Removed**","125e4451":"**Predict**","b23094c3":"**Recall Score**","de8d56b2":"**Test Set**"}}