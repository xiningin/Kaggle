{"cell_type":{"6ad47638":"code","19aa553b":"code","4baec197":"code","8c8c23bb":"code","10daf5eb":"code","175c9122":"code","3d6ab4e7":"code","8e69c533":"code","d93e95f2":"code","9e1d6809":"code","281bf005":"markdown"},"source":{"6ad47638":"# Importing core libraries\nimport numpy as np\nimport pandas as pd\nfrom time import time\nimport pprint\nimport joblib\nfrom functools import partial\n\n# Suppressing warnings because of skopt verbosity\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Regressors\nimport lightgbm as lgb\n\n# Model selection\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\n# Metrics\nfrom sklearn.metrics import mean_squared_error\n\n# Data processing\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer","19aa553b":"# Loading data \nX = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\nX_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")","4baec197":"# Preparing data as a tabular matrix\ny = X.target\nX = X.set_index('id').drop('target', axis='columns')\nX_test = X_test.set_index('id')","8c8c23bb":"# Stratifying the target\ny_stratified = pd.cut(y, bins=10, labels=False)","10daf5eb":"# Processing categoricals with SVD encoding\ncategoricals = [item for item in X.columns if 'cat' in item]\nX_seq = X[categoricals].apply(lambda x: \" \".join(list([str(y) + str(i) for i, y in enumerate(x)])), axis=1)\nX_test_seq = X_test[categoricals].apply(lambda x: \" \".join(list([str(y) + str(i) for i, y in enumerate(x)])), axis=1)\n\nlatent_dims = 24\n\nsvd_feats = ['svd_'+str(l) for l in range(latent_dims)]\nvectorizer = TfidfVectorizer()\n\ndim_reductio = TruncatedSVD(n_components=24, random_state=0)\nX[svd_feats] =  dim_reductio.fit_transform(vectorizer.fit_transform(X_seq))\nX_test[svd_feats] = dim_reductio.transform(vectorizer.fit_transform(X_test_seq))","175c9122":"# Dealing with categorical data\ncategoricals = [item for item in X.columns if 'cat' in item]\nordinal_encoder = OrdinalEncoder()\nX[categoricals] = ordinal_encoder.fit_transform(X[categoricals])\nX_test[categoricals] = ordinal_encoder.transform(X_test[categoricals])","3d6ab4e7":"# Transferring the best parameters to our basic regressor\n\nbest_params = dict([('colsample_bytree', 0.45),\n                    ('learning_rate', 0.01),\n                    ('max_depth', 8),\n                    ('min_child_samples', 83),\n                    ('min_child_weight', 8.7),\n                    ('n_estimators', 2700),\n                    ('num_leaves', 512),\n                    ('objective', 'tweedie'),\n                    ('reg_alpha', 0.005),\n                    ('reg_lambda', 1e-09),\n                    ('subsample', 0.46),\n                    ('subsample_freq', 2)])\n\n\nreg = lgb.LGBMRegressor(boosting_type='gbdt',\n                        verbose=-1,\n                        random_state=0,\n                        **best_params)","8e69c533":"# Cross-validation prediction\nfolds = 10\nskf = StratifiedKFold(n_splits=folds,\n                      shuffle=True, \n                      random_state=0)\n\npredictions = np.zeros(len(X_test))\nfor k, (train_idx, val_idx) in enumerate(skf.split(X, y_stratified)):\n    reg.fit(X.iloc[train_idx, :], y[train_idx])\n    val_preds = reg.predict(X.iloc[val_idx, :])\n    val_rmse = mean_squared_error(y_true=y[val_idx], y_pred=val_preds, squared=False)\n    print(f\"Fold {k} RMSE: {val_rmse:0.5f}\")\n    predictions += reg.predict(X_test).ravel()\n    \npredictions \/= folds","d93e95f2":"# Preparing the submission\nsubmission = pd.DataFrame({'id':X_test.index, \n                           'target': predictions})\n\nsubmission.to_csv(\"submission.csv\", index = False)","9e1d6809":"submission","281bf005":"Until now I couldn't see much feature engineering around, hence I've decided to publish a few notebooks on smart feature engineering. \n\nI will start from SVD encoding. This approach is most useful for models based on feature combinations, such as linear models or neural networks. However, also GBMs can get an advantage because they will converge much quicker if you use SVD encoding. \n\nThe idea is to first encode your categorical variables with one hot encoding or with tf.idf (in order to enphasise the rarer classes) and then process all this with a SVD. By retaining only the most important components of the resulting SVD, you will have few dense features that summarize how the categorical features interact with each other. "}}