{"cell_type":{"eca97b71":"code","8afec7ff":"code","4cc037ac":"code","cde73e9c":"code","7882d052":"markdown"},"source":{"eca97b71":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pandas_profiling import ProfileReport\n\nfrom sklearn import preprocessing\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nimport os\n\nimport pickle\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","8afec7ff":"train_data=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\nprint(train_data.head())\ngender_submission=pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\nprint(gender_submission.head())\n\n\n#Uncomment below lined to generate full profile report of the dataframe\n# profile = ProfileReport(train_data, title=\"Pandas Profiling Report\")\n# profile.to_widgets()","4cc037ac":"\nprint(train_data.corr())\nprint(train_data.describe())\n\n\n#Preprocessing Step\n\n#select needed columns from the dataframe\ntrain_data=train_data[['Pclass','Sex','SibSp','Name','Parch','Embarked','Age','Fare','Survived']]\n\n#Impute Null values\ntrain_data['Age']=train_data['Age'].fillna(train_data['Age'].mean())\ntrain_data['Fare']=train_data['Fare'].fillna(train_data['Fare'].mean())\n\n#New column name Family created using parch and sibsp values (Feature engineering)\ntrain_data['Family']=0\n\nfor x in range(len(train_data)):\n    Parch=int(train_data['Parch'].iloc[x])\n    SibSp=int(train_data['SibSp'].iloc[x])\n    family_size=Parch+SibSp\n  \n    if family_size ==1:\n        train_data['Family'].iloc[x]=0\n    elif family_size < 5 and family_size > 1:\n        train_data['Family'].iloc[x]=1\n    elif family_size >=5 :\n        train_data['Family'].iloc[x]=2\n        \n#Feature engineering the column name and adding it to the dataframe as title\ndataset=pd.DataFrame()     \n\ndataset['Title'] = train_data.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\ndataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\ndataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\ndataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\ndataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\ndataset['Title'] = dataset['Title'].replace('Master', 'Mr')\n\ndataset['Title'] = dataset['Title'].replace('Miss', 0)\ndataset['Title'] = dataset['Title'].replace('Mr', 1)\ndataset['Title'] = dataset['Title'].replace('Mrs', 2)\ndataset['Title'] = dataset['Title'].replace('Rare', 3)\n    \n    \ntrain_data['Title']=dataset['Title'].tolist()\n\n#Binning age and fare columns\nbins = [0,10,20,30,40,50,60,70,80,90,100]\nlabels = [0,1,2,3,4,5,6,7,8,9]\ntrain_data['Age_Binned'] = pd.cut(train_data['Age'], bins=bins,labels=labels)\nbins = [0,32,64,128,256,512,1025]\nlabels = [0,1,2,3,4,5]\ntrain_data['Fare_Binned'] = pd.cut(train_data['Fare'], bins=bins,labels=labels)\n\n#encoding all the catergorical data\nenc_embarked = preprocessing.LabelEncoder()\ntrain_data['Embarked']=enc_embarked.fit_transform(train_data['Embarked'].tolist())\n\nenc_sex= preprocessing.LabelEncoder()\ntrain_data['Sex']=enc_sex.fit_transform(train_data['Sex'].tolist())\n\n#pickel saving models\nfilename = 'enc_embarked.sav'\npickle.dump(enc_embarked, open(filename, 'wb'))\n\nfilename = 'enc_sex.sav'\npickle.dump(enc_sex, open(filename, 'wb'))\n\n\ntrain_data=train_data[['Pclass','Sex','SibSp','Parch','Embarked','Age_Binned','Family','Title','Survived']]\nprint(train_data)\n\nnp_input_data=train_data.iloc[:,0:len(train_data.columns)-1].values\n\n\n#Scaling the data \nscaler =  MinMaxScaler()\nnp_input_data_scaled=scaler.fit_transform(np_input_data)\n\nfilename = 'scaling.sav'\npickle.dump(scaler, open(filename, 'wb'))\n\npoly = PolynomialFeatures(3)\nnp_input_data_scaled=poly.fit_transform(np_input_data_scaled)\n\nfilename = 'polynomial.sav'\npickle.dump(poly, open(filename, 'wb'))\n\npca = PCA(n_components=5)\nnp_input_data_scaled=pca.fit_transform(np_input_data_scaled)\n\nfilename = 'pca.sav'\npickle.dump(pca, open(filename, 'wb'))\n\nnp_output_data=train_data.iloc[:,len(train_data.columns)-1].values\n\n#Train split data\nX_train, X_test, y_train, y_test = train_test_split(np_input_data_scaled, np_output_data, test_size=0.1, random_state=0)\n\nprint(X_train)\nprint(y_train)\n\n\n\nclf = DecisionTreeClassifier()\n# Train Decision Tree Classifer\nclf = clf.fit(X_train,y_train)\n#Predict the response for test dataset\ny_pred = clf.predict(X_test)\nprint('DT')\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"Accuracy F1:\",metrics.f1_score(y_test, y_pred, average='macro'))\n\n\nfilename = 'dt.sav'\npickle.dump(clf, open(filename, 'wb'))\n\n#Create a Gaussian Classifier\nclf= RandomForestClassifier(criterion='gini',\n                                           n_estimators=1750,\n                                           max_depth=7,\n                                           min_samples_split=6,\n                                           min_samples_leaf=6,\n                                           max_features='auto',\n                                           oob_score=True,\n                                           random_state=42,\n                                           n_jobs=-1,\n                                           verbose=1)\n#Train the model using the training sets y_pred=clf.predict(X_test)\nclf.fit(X_train,y_train)\ny_pred=clf.predict(X_test)\nprint('Random Forest')\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"Accuracy F1:\",metrics.f1_score(y_test, y_pred, average='macro'))\n\nfilename = 'random_forest.sav'\npickle.dump(clf, open(filename, 'wb'))\n\nmodel = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1.0, gamma=1.5, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.02, max_delta_step=0, max_depth=6,\n              min_child_weight=1, monotone_constraints='()',\n              n_estimators=400, n_jobs=1, nthread=1, num_parallel_tree=1,\n              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n              subsample=1.0, tree_method='exact', validate_parameters=1,\n              verbosity=None)\n\n\nmodel.fit(X_train, y_train)\ny_pred=model.predict(X_test)\nprint('XGboost')\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"Accuracy F1:\",metrics.f1_score(y_test, y_pred, average='macro'))\nfilename = 'xgb.sav'\npickle.dump(model, open(filename, 'wb'))\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nclf = GradientBoostingClassifier( loss='deviance',n_estimators=100,learning_rate=0.1)\nclf.fit(X_train,y_train)\ny_pred=clf.predict(X_test)\nprint('GradientBoosting')\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"Accuracy F1:\",metrics.f1_score(y_test, y_pred, average='macro'))\n\n\nfilename = 'gradien_boost.sav'\npickle.dump(clf, open(filename, 'wb'))","cde73e9c":"\n#Below code is used to test the model generated above to obtain the required submission file.\n\ntest_data=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ngender_sub=pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\nprint(test_data.head())\n\n\ntest_data=pd.merge(test_data, gender_sub, on=['PassengerId'])\n\ntest_data=test_data[['Pclass','Sex','SibSp','Name','Parch','Embarked','Age','Fare','Survived']]\nprint(train_data)\n\ntest_data['Age']=test_data['Age'].fillna(test_data['Age'].mean())\ntest_data['Fare']=test_data['Fare'].fillna(test_data['Fare'].mean())\n# train_data=train_data.drop_duplicates()\n\ntest_data['Family']=0\n\n\nfor x in range(len(test_data)):\n    Parch=int(test_data['Parch'].iloc[x])\n    SibSp=int(test_data['SibSp'].iloc[x])\n    family_size=Parch+SibSp\n  \n    if family_size ==1:\n        test_data['Family'].iloc[x]=0\n    elif family_size < 5 and family_size > 1:\n        test_data['Family'].iloc[x]=1\n    elif family_size >=5 :\n        test_data['Family'].iloc[x]=2\n    \ndataset=pd.DataFrame()     \n\ndataset['Title'] = test_data.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\ndataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\ndataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\ndataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\ndataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\ndataset['Title'] = dataset['Title'].replace('Master', 'Mr')\n\ndataset['Title'] = dataset['Title'].replace('Miss', 0)\ndataset['Title'] = dataset['Title'].replace('Mr', 1)\ndataset['Title'] = dataset['Title'].replace('Mrs', 2)\ndataset['Title'] = dataset['Title'].replace('Rare', 3)\n    \n    \ntest_data['Title']=dataset['Title'].tolist()\n\n\n\n\nbins = [0,10,20,30,40,50,60,70,80,90,100]\nlabels = [0,1,2,3,4,5,6,7,8,9]\ntest_data['Age_Binned'] = pd.cut(test_data['Age'], bins=bins,labels=labels)\nbins = [0,32,64,128,256,512,1025]\nlabels = [0,1,2,3,4,5]\ntest_data['Fare_Binned'] = pd.cut(test_data['Fare'], bins=bins,labels=labels)\n\n\n\nenc_sex = pickle.load(open('.\/enc_sex.sav', 'rb'))\ntest_data['Sex'] = enc_sex.transform(test_data['Sex'].tolist())\n\nenc_embarked = pickle.load(open('.\/enc_embarked.sav', 'rb'))\ntest_data['Embarked'] = enc_embarked.transform(test_data['Embarked'].tolist())\n\ntest_data=test_data[['Pclass','Sex','SibSp','Parch','Embarked','Age_Binned','Family','Title','Survived']]\nprint(test_data)\n\nnp_input_data=test_data.iloc[:,0:len(train_data.columns)-1].values\n\n\n\nscaler = pickle.load(open('.\/scaling.sav', 'rb'))\nnp_input_data_scaled = scaler.transform(np_input_data)\n\n\n\npoly = pickle.load(open('.\/polynomial.sav', 'rb'))\nnp_input_data_scaled = poly.transform(np_input_data_scaled)\n\n\npca = pickle.load(open('.\/pca.sav', 'rb'))\nnp_input_data_scaled = pca.transform(np_input_data_scaled)\n\nnp_output_data=train_data.iloc[:,len(train_data.columns)-1].values\n\nloaded_model = pickle.load(open('.\/gradien_boost.sav', 'rb'))\nresult=loaded_model.predict(np_input_data_scaled)\nprint(len(result))\nprint(len(np_output_data))\nprint(result)\nprint(np_output_data)\n# print(\"Accuracy:\",metrics.accuracy_score(result, np_output_data))\n# print(\"Accuracy F1:\",metrics.f1_score(result, np_output_data, average='macro'))\n\n\ntest_data=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nprint(len(test_data['PassengerId'].tolist()))\nresult_df=pd.DataFrame()\nresult_df['PassengerId']=test_data['PassengerId'].tolist()\nresult_df['Survived']=result\nprint(result_df)\n\nresult_df.to_csv('submission.csv',index=False)","7882d052":"Encoding the relevant columns"}}