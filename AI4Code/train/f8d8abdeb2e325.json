{"cell_type":{"465b4dd0":"code","e7c74315":"code","48e5577b":"code","8a816ba0":"code","6520eeb4":"code","f920a703":"code","d50b6581":"code","71d64222":"code","a586f38f":"code","a5236dae":"code","e482d5c7":"code","83bfca69":"code","26ed6854":"code","629ca119":"code","bf865116":"code","648999a0":"code","1ce8d7e0":"code","f968c8ea":"code","8e4e6d9d":"code","886b272f":"code","1396283d":"code","871e71d0":"markdown","d83c3b2a":"markdown","a0c7004f":"markdown","df65165f":"markdown","020d3ade":"markdown","eeaf2e34":"markdown","37c3c8b8":"markdown","54c18dbe":"markdown","7de398cf":"markdown","67d827ed":"markdown","0625c6e0":"markdown","ce528353":"markdown","aed7f251":"markdown","e2b3051e":"markdown","a18bf6e7":"markdown","25b79ede":"markdown","21770d08":"markdown","9a23a152":"markdown","08adeb21":"markdown","3a4574de":"markdown"},"source":{"465b4dd0":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom datetime import datetime\nimport itertools\nimport json\nimport string\nimport re    #for regex\nimport nltk\nfrom nltk.corpus import stopwords\n\nfrom nltk import pos_tag\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import TweetTokenizer \nfrom wordcloud import WordCloud\n\n\nplt.style.use('seaborn-whitegrid')\n# Set Matplotlib defaults\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=14, titlepad=10)","e7c74315":"\ndata = pd.read_csv(\"..\/input\/youtube-new\/USvideos.csv\",parse_dates=[\"publish_time\"])\ndata.head(3)","48e5577b":"# first convert trending date to datetime format\ndata[\"trending_date\"] = data[\"trending_date\"].apply(lambda x : datetime.strptime(x,\"%y.%d.%m\"))\n# create three columns , using dt function to extract dayofweek, hour and month\ndata[\"publish_day_week\"] = data[\"publish_time\"].dt.dayofweek\ndata[\"publish_hour\"] = data[\"publish_time\"].dt.hour\ndata[\"publish_month\"] = data[\"publish_time\"].dt.month","8a816ba0":"# groupby trending date by views\ntime = data.groupby(\"trending_date\")[\"views\"].sum()\n\n\n# let,s plot using rolling (7 -> week)\nplt.figure(figsize=(8,4))\nsns.lineplot(data=time.rolling(7).mean(), linewidth=2.5,color=\"#EB7827\",label=\"Weekly\")\nsns.lineplot(data=time, linewidth=2.5,color=\"grey\",alpha=0.2,label=\"Dayly\")\n# using plt.grid and sns.despine you get a cleaner look in the graphics\nplt.grid(None)\nsns.despine()\nplt.title(\"Trending videos: Total views per week \")\nplt.ylabel(\"Total Views\")","6520eeb4":"# Let's check how many videos there are per year in the dataset; using the same dataframe as before (Time) and reset_index, to facilitate their visualization.\nyear = time.reset_index()\n\n#now with reset_index I have two columns, the number of views and trending_date; as before I use the dt.year function to extract the year and value_counts to count them.\nyear_count = year.trending_date.dt.year.value_counts()\nyear_count.plot(kind=\"bar\",color=[\"#F6794B\",\"#E3E3E3\"])\nplt.xticks(rotation=45)\nplt.title(\"Total videos per year\")\nsns.despine()\nplt.grid(None)","f920a703":"# creating a dict\nnumeros = [*range(7)]\nday = [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\ndict_day = {}\nfor key,item in zip(day,numeros):\n   dict_day[item] = key\n# groupby by day of week\nday_week = data.groupby(\"publish_day_week\").size().reset_index()\n# apply dict to publish day week column using map\nday_week[\"publish_day_week\"] = day_week[\"publish_day_week\"].map(dict_day)\n\n# let,s plot \u00a1\u00a1\nplt.figure(figsize=(8,4))\ncolors = [\"#FAEFBE\",\"#FAE0A7\",\"#FAD19E\",\"#FABC96\",\"#EFA492\",\"#18BADA\",\"#6B6D8C\"]\nsns.barplot(x=\"publish_day_week\",y=0,data=day_week,palette=colors,saturation=0.8)\nplt.title(\"Total published videos per day of week\")\nplt.xlabel(\"Day of week\")\nplt.ylabel(\"Total\")\nplt.xticks(rotation=45)\nsns.despine(left=True)","d50b6581":"numeros_a\u00f1o = [*range(1,13)]\nmonth = [ 'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov','Dec']\n\ndict_month = {}\nfor key,item in zip(month,numeros_a\u00f1o):\n   dict_month[item] = key\n\nmonth_df = data.groupby(\"publish_month\").size().reset_index()\n\nmonth_df[\"publish_month\"] = month_df[\"publish_month\"].map(dict_month)\n\nplt.figure(figsize=(8,4))\n\nsns.barplot(x=\"publish_month\",y=0,data=month_df,palette=\"Paired\",saturation=0.8)\nplt.xlabel(\"Month\")\nplt.ylabel(\"Total\")\nplt.title(\"Total videos per month\")\nplt.xticks(rotation=45)\nsns.despine(left=True)","71d64222":"hour = data.groupby(\"publish_hour\").size()\n\nplt.figure(figsize=(8,4))\nsns.barplot(x=hour.index.values,y=hour.values,palette=\"mako_r\")\nsns.despine()\nplt.title(\"Number of videos uploaded per hour\")","a586f38f":"# First we load the json file\n\nwith open(\"..\/input\/youtube-new\/US_category_id.json\") as f:\n    data_json = json.load(f)[\"items\"]","a5236dae":"# # how is a json file? well, let,say they are dictionaries within dictionaries..\ndata_json[0]","e482d5c7":"# create a dict with titles (we go first to snippet and them title)\ntitle_dict = {}\nfor cat in data_json:\n\n    title_dict[int(cat[\"id\"])] = cat[\"snippet\"][\"title\"]\n    ","83bfca69":"# now we use map to apply it to the dataframe so we can work better!\ndata[\"category_id\"] = data[\"category_id\"].map(title_dict)\n# let's see what percentage of videos per category are in our dataframe.\n","26ed6854":"# let's see what percentage of videos per category are in our dataframe. First, with value_counts and normalize we get the relative frequency\ncategorys = data[\"category_id\"].value_counts(normalize=True).reset_index()\n# rename columns names\ncategorys.rename(columns={\"index\":\"Category\",\"category_id\":\"Percentage\"},inplace=True)\n# get percentage\ncategorys[\"Percentage\"] = round(categorys[\"Percentage\"] *100,2)\n# we use style background \u00a1\ncategorys.style.background_gradient(cmap='mako_r')","629ca119":"# Well, neither one nor the other, I will do better the sum of both, that is, the percentage of like\/dislikes by views....\ntotal = data[\"likes\"] + data[\"dislikes\"]","bf865116":"# let's add up the comments.\ntotal_def = data[\"comment_count\"] + total\n# we created a new column...percentage iterations per view \u00a1\u00a1\u00a1\ndata[\"percentage_iterations_per_view\"] = round((total_def \/ data[\"views\"]) * 100,2)","648999a0":"def data_views(groupby=\"channel_title\",by=\"percentage_iterations_per_view\",ascending=False):\n    \"\"\"\n    return dataframe groupby channels, showing mean from columns:\n    \"views\",\"likes\",\"dislikes\",\"comment_count\",\"percentage_iterations_per_view\"\n    ,sort by and ascending included\n    \"\"\"\n    return  (data\n            .groupby(groupby)[\"views\",\"likes\",\"dislikes\",\"comment_count\",\"percentage_iterations_per_view\"]\n            .mean()\n            .sort_values(by=by,ascending=ascending)\n            )","1ce8d7e0":"# Let's take a look; these are the channels ordered by \"percentage_iterations_per_view\"\ndata_views().head(10)","f968c8ea":"# Which are the channels with more videos in the dataset ?\ntotal_chanel = data.groupby(\"channel_title\")[\"video_id\"].count().sort_values(ascending=False).head(10).reset_index()\ntotal_chanel.rename(columns={\"channel_title\":\"Channel_title\",\"video_id\":\"Total_videos\"},inplace=True)\ntotal_chanel.style.background_gradient(cmap='mako_r')","8e4e6d9d":"# is there any relation between the number of comments and the likes\/dislikes ? well, let's chart it with a scatter plot and with size=\"comment_coun\nplt.figsize=(10,6)\nsns.scatterplot(x=\"likes\",y=\"dislikes\",size=\"comment_count\", hue=\"comment_count\",data=data,alpha=0.7)\nplt.title(\"Relation between the number of comments and the likes\/dislikes\")\nplt.grid(False)\nplt.legend( bbox_to_anchor=(1.05, 1), loc='upper left',fontsize='xx-small')","886b272f":"most_comment_category= data.groupby(\"category_id\")[\"percentage_iterations_per_view\"].mean()                                                                 .sort_values(ascending=False)\n                                    \ncolors = [\"#EAEBE9\" for _ in range(len(most_comment_category))]\ncolors[0] = \"#FD0B3B\"\nplt.figure(figsize=(10,6))\nsns.barplot(x=most_comment_category.index.values,y=most_comment_category.values,palette=colors,saturation=0.8,errwidth=0.4)\nplt.title(\"Average percentage iterations per category\")\nplt.xlabel(\"Categorys\")\nplt.ylabel(\"Percentage iterations\")\nplt.xticks(rotation=45)\nsns.despine(left=True)","1396283d":"#Setting the stopwords\neng_stopwords = set(stopwords.words(\"english\"))\n\nwordcloud = WordCloud(\n                          background_color='white',\n                          stopwords=eng_stopwords,\n                          max_words=1000,\n                          max_font_size=120, \n                          random_state=42\n                         ).generate(str(data['title']))\n\nprint(wordcloud)\nfig = plt.figure(1)\nplt.imshow(wordcloud)\nplt.title(\"WordCloud Titles\")\nplt.axis('off')\nplt.show()","871e71d0":"### Channels, Youtubers...\nThere is a column in our dataframe called category_id but they are numbers; luckily there is a Json file, where we can find the video categories. \nLet's create, as we did before with the dates, a dictionary of video categories!","d83c3b2a":"On second thought, I will also take into account the comments; I will create a column that will measure the percentage of user interactions based on the number of views","a0c7004f":"Let's have a look at our dataset! First I will take care of the time format columns; from them I will extract three additional columns that will be helpful for the exploratory analysis (day, hour and month of publication)","df65165f":"If we can see that the greater the increase of both likes and dislikes, the number of comments increases.\n\nLet's check the average, in percentage, of iterations per category","020d3ade":"Forty percent of the videos belong to the categories Music and Enterteiment \u00a1\u00a1\n\nIn our dataframe there are columns that we can play with, for example likes, dislikes; we can create new columns like the percentage of likes by views, or the percentage of dislikes by views...","eeaf2e34":"There are many Youtubers who do not sleep at night! The truth is that the night is when it is quieter, there are fewer distractions and noise.","37c3c8b8":"We see that on weekends the Youtubers tend to rest and they tend to upload more videos on Thursday and Friday. Now I will do the same but with the months of the year","54c18dbe":"Most videos belong to 2018!\n\nLet's continue with time charts; let's see what day, month and time the videos are usually published!","7de398cf":"From April 2018 the sights will go off like a rocket! BTS has surely released a new song :) ","67d827ed":"We see that Music, Style or Comedy tend to have the greatest number of iteractions for videos; on the other hand, Politics and Sports do not tend to have as many, which is surprising since they are always controversial topics.","0625c6e0":"These are the channels with more interactions (likes, dislikes, comments); Daily caller , Desimpedidos or KickThePj for example are the ones that have more; it would be curious to see their theme ...\n\nhttps:\/\/www.youtube.com\/user\/dailycaller","ce528353":"I created a small function to group, average and sort the following columns (\"views\", \"likes\", \"dislikes\", \"comment_count\", \"percentage_iterations_per_view\")","aed7f251":"### WordCloud\n\nNow finally we will make a WordCloud; it is a very visual and fast way to do text analysis without going into too much detail; let's see what result we get with the titles.","e2b3051e":"First we are going to import everything we need to do the analysis","a18bf6e7":"Ahhhh the summer months people prefer to go to the beach or the mountains instead of uploading videos, and it makes a lot of sense, you have to enjoy the good weather!\n\nAnd finally, let's see what time the Youtubers usually upload their videos!","25b79ede":"### Analyzing the time !","21770d08":"First I group the views by day of the week; then I create a dictionary with the days of the week to convert the numbers into strings and make the display more enjoyable","9a23a152":"### Trending YouTube Video Statistics\nDaily statistics for trending YouTube videos\n\n### Description\nYouTube (the world-famous video sharing website) maintains a list of the top trending videos on the platform. According to Variety magazine, \u201cTo determine the year\u2019s top-trending videos, YouTube uses a combination of factors including measuring users interactions (number of views, shares, comments and likes). Note that they\u2019re not the most-viewed videos overall for the calendar year\u201d. Top performers on the YouTube trending list are music videos (such as the famously virile \u201cGangam Style\u201d), celebrity and\/or reality TV performances, and the random dude-with-a-camera viral videos that YouTube is well-known for.\n\nThis dataset is a daily record of the top trending YouTube videos in USA for several months.","08adeb21":"Well this has been a quick analysis of the videos that are trends in Youtube, in a certain period, in Usa. I hope you liked it, draw your conclusions and if you have any comments or ideas for improvement, please do not hesitate to tell me, it will be of great help to improve!\n\na hug to everyone!","3a4574de":"I will see the evolution of the number of views of the videos over time; for this I use the Rolling function and I group them by week to better appreciate the trend"}}