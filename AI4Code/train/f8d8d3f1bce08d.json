{"cell_type":{"bf773ae3":"code","a3c359e1":"code","aa4a9b29":"code","300e2ff9":"code","fe7da379":"code","06522ed4":"code","56fe8be2":"code","30fc261a":"code","f18d1014":"code","9d744fe9":"code","9d510b22":"code","81d83932":"code","c2b383be":"code","c8e72165":"code","b3a76e7e":"code","be86a8b9":"code","fb24942f":"markdown","e6bf3ee5":"markdown","ca039e0f":"markdown","a388bf3d":"markdown","6e4c375d":"markdown","0b54f236":"markdown","231d105d":"markdown","008bc458":"markdown","168c3970":"markdown","3a9b97e7":"markdown","7b23e32b":"markdown","2cfd0fb9":"markdown","e444297a":"markdown"},"source":{"bf773ae3":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as bi_rnn\nfrom tensorflow.contrib.rnn import BasicLSTMCell\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.utils import shuffle\nimport time\nfrom sklearn.metrics import f1_score\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","a3c359e1":"class ABLSTM(object):\n    def __init__(self, config):\n        self.max_len = config[\"max_len\"]\n        self.hidden_size = config[\"hidden_size\"]\n        self.vocab_size = config[\"vocab_size\"]\n        self.embedding_size = config[\"embedding_size\"]\n        self.n_class = config[\"n_class\"]\n        self.learning_rate = config[\"learning_rate\"]\n\n        # placeholder\n        self.x = tf.placeholder(tf.int32, [None, self.max_len])\n        self.label = tf.placeholder(tf.int32, [None])\n        self.keep_prob = tf.placeholder(tf.float32)\n\n    def build_graph(self):\n        print(\"building graph\")\n        # Word embedding\n        embeddings_var = tf.Variable(tf.random_uniform([self.vocab_size, self.embedding_size], -1.0, 1.0),\n                                     trainable=True)\n        batch_embedded = tf.nn.embedding_lookup(embeddings_var, self.x)\n\n        rnn_outputs, _ = bi_rnn(BasicLSTMCell(self.hidden_size),\n                                BasicLSTMCell(self.hidden_size),\n                                inputs=batch_embedded,dtype=tf.float32)\n\n        fw_outputs, bw_outputs = rnn_outputs\n\n        W = tf.Variable(tf.random_normal([self.hidden_size], stddev=0.1))\n        H = fw_outputs + bw_outputs  # (batch_size, seq_len, HIDDEN_SIZE)\n        M = tf.tanh(H)  # M = tanh(H)  (batch_size, seq_len, HIDDEN_SIZE)\n\n        self.alpha = tf.nn.softmax(tf.reshape(tf.matmul(tf.reshape(M, [-1, self.hidden_size]),\n                                                        tf.reshape(W, [-1, 1])),\n                                              (-1, self.max_len)))  # batch_size x seq_len\n        r = tf.matmul(tf.transpose(H, [0, 2, 1]),\n                      tf.reshape(self.alpha, [-1, self.max_len, 1]))\n        r = tf.squeeze(r)\n        h_star = tf.tanh(r)  # (batch , HIDDEN_SIZE\n\n        h_drop = tf.nn.dropout(h_star, self.keep_prob)\n\n        # Fully connected layer\uff08dense layer)\n        FC_W = tf.Variable(tf.truncated_normal([self.hidden_size, self.n_class], stddev=0.1))\n        FC_b = tf.Variable(tf.constant(0., shape=[self.n_class]))\n        y_hat = tf.nn.xw_plus_b(h_drop, FC_W, FC_b)\n\n        self.loss = tf.reduce_mean(\n            tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y_hat, labels=self.label))\n\n        # prediction\n        self.prediction = tf.argmax(tf.nn.sigmoid(y_hat), 1)\n\n        # optimization\n        loss_to_minimize = self.loss\n        tvars = tf.trainable_variables()\n        gradients = tf.gradients(loss_to_minimize, tvars, aggregation_method=tf.AggregationMethod.EXPERIMENTAL_TREE)\n        grads, global_norm = tf.clip_by_global_norm(gradients, 1.0)\n\n        self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n        self.train_op = self.optimizer.apply_gradients(zip(grads, tvars), global_step=self.global_step,\n                                                       name='train_step')\n        print(\"graph built successfully!\")","aa4a9b29":"\nnames = [\"qid\", \"question_text\", \"target\"]\n\ndef load_data(file_name, sample_ratio=1, names=names):\n    '''load data from .csv file'''\n    csv_file = pd.read_csv(file_name, names=names)\n    shuffle_csv = csv_file.sample(frac=sample_ratio)\n    return shuffle_csv[\"question_text\"], shuffle_csv[\"target\"]\n\ndef data_preprocessing(train, test, max_len):\n    \"\"\"transform to one-hot idx vector by VocabularyProcessor\"\"\"\n    \"\"\"VocabularyProcessor is deprecated, use v2 instead\"\"\"\n    vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(max_len)\n    x_transform_train = vocab_processor.fit_transform(train)\n    x_transform_test = vocab_processor.transform(test)\n    vocab = vocab_processor.vocabulary_\n    vocab_size = len(vocab)\n    x_train_list = list(x_transform_train)\n    x_test_list = list(x_transform_test)\n    x_train = np.array(x_train_list)\n    x_test = np.array(x_test_list)\n\n    return x_train, x_test, vocab, vocab_size\n\n\ndef data_preprocessing_v2(train, max_len, max_words=50000):\n    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_words)\n    tokenizer.fit_on_texts(train)\n    train_idx = tokenizer.texts_to_sequences(train)\n    train_padded = pad_sequences(train_idx, maxlen=max_len, padding='post', truncating='post')\n    # vocab size = len(word_docs) + 2  (<UNK>, <PAD>)\n    return train_padded, max_words + 2\n\n\ndef data_preprocessing_with_dict(train, test, max_len):\n    tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='<UNK>')\n    tokenizer.fit_on_texts(train)\n    train_idx = tokenizer.texts_to_sequences(train)\n    test_idx = tokenizer.texts_to_sequences(test)\n    train_padded = pad_sequences(train_idx, maxlen=max_len, padding='post', truncating='post')\n    test_padded = pad_sequences(test_idx, maxlen=max_len, padding='post', truncating='post')\n    # vocab size = len(word_docs) + 2  (<UNK>, <PAD>)\n    return train_padded, test_padded, tokenizer.word_docs, tokenizer.word_index, len(tokenizer.word_docs) + 2\n\n\ndef split_dataset(x_test, y_test, dev_ratio):\n    \"\"\"split test dataset to test and dev set with ratio \"\"\"\n    test_size = len(x_test)\n    print(test_size)\n    dev_size = (int)(test_size * dev_ratio)\n    print(dev_size)\n    x_dev = x_test[:dev_size]\n    x_test = x_test[dev_size:]\n    y_dev = y_test[:dev_size]\n    y_test = y_test[dev_size:]\n    return x_test, x_dev, y_test, y_dev, dev_size, test_size - dev_size\n\n\ndef fill_feed_dict(data_X, data_Y, batch_size):\n    \"\"\"Generator to yield batches\"\"\"\n    shuffled_X, shuffled_Y = shuffle(data_X, data_Y)\n    for idx in range(data_X.shape[0] \/\/ batch_size):\n        x_batch = shuffled_X[batch_size * idx: batch_size * (idx + 1)]\n        y_batch = shuffled_Y[batch_size * idx: batch_size * (idx + 1)]\n        yield x_batch, y_batch","300e2ff9":"def make_train_feed_dict(model, batch):\n    \"\"\"make train feed dict for training\"\"\"\n    feed_dict = {model.x: batch[0],\n                 model.label: batch[1],\n                 model.keep_prob: .5}\n    return feed_dict\n\n\ndef make_test_feed_dict(model, batch):\n    feed_dict = {model.x: batch[0],\n                 model.label: batch[1],\n                 model.keep_prob: 1.0}\n    return feed_dict\n\n\ndef run_train_step(model, sess, batch):\n    feed_dict = make_train_feed_dict(model, batch)\n    to_return = {\n        'train_op': model.train_op,\n        'loss': model.loss,\n        'global_step': model.global_step,\n    }\n    return sess.run(to_return, feed_dict)\n\n\ndef run_eval_step(model, sess, batch):\n    feed_dict = make_test_feed_dict(model, batch)\n    prediction = sess.run(model.prediction, feed_dict)\n    return prediction\n\n\ndef get_attn_weight(model, sess, batch):\n    feed_dict = make_train_feed_dict(model, batch)\n    return sess.run(model.alpha, feed_dict)","fe7da379":"train=pd.read_csv('..\/input\/train.csv')\nx_train, y_train = train['question_text'],train['target']","06522ed4":"x_train, vocab_size = data_preprocessing_v2(x_train, max_len=32)","56fe8be2":"x_train, x_dev, y_train, y_dev, dev_size, train_size = split_dataset(x_train, y_train, 0.1)\nprint(\"Validation Size: \", dev_size)","30fc261a":"config = {\n        \"max_len\": 32,\n        \"hidden_size\": 64,\n        \"vocab_size\": vocab_size,\n        \"embedding_size\": 128,\n        \"n_class\": 2,\n        \"learning_rate\": 1e-3,\n        \"batch_size\": 32,\n        \"train_epoch\": 2\n}","f18d1014":"tf.reset_default_graph()\nclassifier = ABLSTM(config)\nclassifier.build_graph()","9d744fe9":"sess = tf.Session()\nsess.run(tf.global_variables_initializer())\ndev_batch = (x_dev, y_dev)","9d510b22":"start = time.time()\npredictions=[]\nfor e in range(config[\"train_epoch\"]):\n   t0 = time.time()\n   print(\"Epoch %d start !\" % (e + 1))\n   for x_batch, y_batch in fill_feed_dict(x_train, y_train, config[\"batch_size\"]):\n    return_dict = run_train_step(classifier, sess, (x_batch, y_batch))\n    attn = get_attn_weight(classifier, sess, (x_batch, y_batch))\n   t1 = time.time()\n   print(\"Train Epoch time:  %.3f s\" % (t1 - t0))\n   dev_preds = run_eval_step(classifier, sess, dev_batch)\n   print(\"validation F1: %.3f \" % f1_score(y_dev.values,dev_preds))","81d83932":"test_df=pd.read_csv('..\/input\/test.csv')\n\ntarget=[0 for i in range(test_df.shape[0])]\ntest_df['target']=target","c2b383be":"x_text, vocab_size = data_preprocessing_v2(test_df['question_text'], max_len=32)","c8e72165":"test_batch=(x_text,test_df['target'].values)\ntest_preds = run_eval_step(classifier, sess, test_batch)","b3a76e7e":"sub=pd.read_csv('..\/input\/sample_submission.csv')\nsub.head()","be86a8b9":"sub.prediction=test_preds\nsub.to_csv('submission.csv',index=False)","fb24942f":"**Training**","e6bf3ee5":"**Submission**","ca039e0f":"#### Template for preprocessing block:","a388bf3d":"**Hyper Parameters**","6e4c375d":"### References:\n\nhttps:\/\/github.com\/TobiasLee\/Text-Classification\/blob\/master\/models\/attn_bi_lstm.py\n\nhttp:\/\/univagora.ro\/jour\/index.php\/ijccc\/article\/download\/3142\/pdf","0b54f236":"#### Read the train dataset:\n\n**I am facing resource exhausted issue so at start I am using just one percent of train data**","231d105d":"**Attention model for text classification:**\n\nText classification is one of the principal tasks of machine learning. It aims to design proper algorithms to enable computers to extract features and classify texts automatically.\n\nThis classification uses an attention mechanism to learn weighting for each word. Under the setting, key words will have a higher weight, and common words will have lower weight. Therefore, the representation of texts not only considers all words, but also pays more attention to key words. Then we feed the feature vector to a softmax classifier. \n\n![Attention](https:\/\/www.researchgate.net\/publication\/323130660\/figure\/fig1\/AS:593383479324672@1518485054048\/Attention-based-bidirectional-RNN-structure_W840.jpg)\n\n**I have added this refernce at the bottom**","008bc458":"#### Import Dependencies","168c3970":"**Tensorflow session**","3a9b97e7":"#### Model Helper Functions","7b23e32b":"**Preprocessing and Data splitting**","2cfd0fb9":"#### Template for Attention Block","e444297a":"**Overview**\n\nThe overview of notebook is to create a basic attention based LSTM for text classification. There are some advanced  strategies like adding embeddings to this process is yet to be done. It is still at it's raw form. This kernel follows PEP-8 convention.\n\n**Upvote this kernel if you feel it is useful, It also makes me motivated. Any comments on this kernel are welcome I will take that feedback constructively and go ahead **\n\n**Business Understanding:**\n\nAn existential problem for any major website today is how to handle toxic and divisive content. Quora wants to tackle this problem head-on to keep their platform a place where users can feel safe sharing their knowledge with the world.\n\n"}}