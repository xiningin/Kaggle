{"cell_type":{"9432b7d2":"code","c4b63fff":"code","45b795c3":"code","d3e995c6":"code","e0089275":"code","c29a4328":"code","2ef25a2d":"code","ca64bc08":"code","28c4dd63":"code","85c0ff91":"code","644436ea":"code","3c536925":"code","928eab5f":"code","6ea5679c":"code","69b2a6de":"code","b36e4daa":"code","80fddeef":"code","799c562c":"code","78c327b4":"code","9daf021e":"code","d9e8b630":"code","6b97b312":"code","2853422e":"code","20b2361e":"code","ff92857e":"code","20cb603c":"code","028b3aca":"code","d31928d1":"code","a4f841e1":"code","88bf1fda":"code","df974030":"code","438ffb0a":"code","64880a71":"code","54cb62fa":"code","f21a4baa":"code","5d035927":"code","3f5eaaac":"code","abd0de7a":"code","88e9fe77":"code","8d191bb6":"code","e952d46f":"code","0301d98a":"code","e181eb9f":"code","98965a45":"code","5df80823":"code","7399361d":"code","fbfcab6a":"code","d31ffcca":"code","507df048":"code","04875a78":"code","f4c134d9":"code","d7ee6515":"code","b299319e":"code","c8041b5e":"code","ac7341a3":"code","957fd831":"code","f597bde2":"code","0e81ffac":"code","d230618c":"code","aafc1696":"code","2d93f623":"code","a2b80a9f":"code","b8e0ff11":"code","afcf0403":"code","c9492b14":"code","ab4f56b9":"code","561b9ab2":"code","2b02a62a":"code","b2245d8a":"code","51aa9cc9":"code","360919cb":"code","7203ee1b":"code","46da2d24":"code","f7f4f78e":"code","61750085":"code","0908a777":"code","1fb5dffe":"code","303fc746":"code","8348a8ae":"code","8fdbf676":"code","ce9926f1":"code","133cc372":"code","8cbb6edd":"code","155e8824":"code","b556f5e4":"code","07eea583":"code","cd21dfc1":"code","c9d7f07e":"code","0468382d":"code","f0ea3d3c":"code","f62237ba":"code","d1ce1447":"code","2d6da040":"markdown","f1614c65":"markdown","4c477282":"markdown","b7668392":"markdown","7b8be97d":"markdown","78005518":"markdown","8f0b2bfa":"markdown","8f6b0999":"markdown","6c573c02":"markdown","aa942487":"markdown","a5744492":"markdown","f13f4562":"markdown","0fee3b99":"markdown","8a4d2213":"markdown","1c0ddbe8":"markdown","1ea17872":"markdown","106055d2":"markdown","fcabe497":"markdown","51f856c6":"markdown","49e5d4f6":"markdown","b700c772":"markdown","83facee8":"markdown","d4bf851e":"markdown","d3f61e5b":"markdown","386049bd":"markdown","4536e974":"markdown","5d8d3471":"markdown","9bd4e72e":"markdown","7f1f0e17":"markdown","7f424d31":"markdown","41a1f96a":"markdown","946823b6":"markdown","fe1cd1b6":"markdown","d6f700c4":"markdown","0c3ad0a0":"markdown","0d722bda":"markdown","37cb3d1a":"markdown","9bc19228":"markdown","a7ba02f5":"markdown","7e6c85af":"markdown","203e10ed":"markdown","56e98818":"markdown","5b656791":"markdown","9c591628":"markdown","40e7626b":"markdown","84d522e5":"markdown","ed393010":"markdown","7eead2c4":"markdown","1cf96460":"markdown","0890cd42":"markdown","d1b22415":"markdown","1e193474":"markdown","739e27a7":"markdown","88fb2459":"markdown","7827be4e":"markdown","620dc2b1":"markdown","423e6400":"markdown","d99fcc6b":"markdown","65d8fffb":"markdown","36fc67de":"markdown","44b0a053":"markdown","992b0863":"markdown","2d90c4d2":"markdown","70ceacc9":"markdown","d95117f5":"markdown","c7054080":"markdown","f7f9501a":"markdown","dbc96a96":"markdown"},"source":{"9432b7d2":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c4b63fff":"#general purpose packages\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#data processing\nimport re, string\nimport emoji\nimport nltk\n\nfrom sklearn import preprocessing\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.model_selection import train_test_split\n\n\n#Naive Bayes\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\n\n#transformers\nfrom transformers import BertTokenizerFast\nfrom transformers import TFBertModel\nfrom transformers import RobertaTokenizerFast\nfrom transformers import TFRobertaModel\n\n#keras\nimport tensorflow as tf\nfrom tensorflow import keras\n\n\n#metrics\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n#set seed for reproducibility\nseed=42\n\n#set style for plots\nsns.set_style(\"whitegrid\")\nsns.despine()\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlepad=10)\n","45b795c3":"def conf_matrix(y, y_pred, title):\n    fig, ax =plt.subplots(figsize=(5,5))\n    labels=['Negative', 'Neutral', 'Positive']\n    ax=sns.heatmap(confusion_matrix(y, y_pred), annot=True, cmap=\"Blues\", fmt='g', cbar=False, annot_kws={\"size\":25})\n    plt.title(title, fontsize=20)\n    ax.xaxis.set_ticklabels(labels, fontsize=17) \n    ax.yaxis.set_ticklabels(labels, fontsize=17)\n    ax.set_ylabel('Test', fontsize=20)\n    ax.set_xlabel('Predicted', fontsize=20)\n    plt.show()","d3e995c6":"df = pd.read_csv('..\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv',encoding='ISO-8859-1')\ndf_test = pd.read_csv('..\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv')","e0089275":"df.head()","c29a4328":"df.info()","2ef25a2d":"df['TweetAt'] = pd.to_datetime(df['TweetAt'])","ca64bc08":"df.drop_duplicates(subset='OriginalTweet',inplace=True)","28c4dd63":"df.info()","85c0ff91":"tweets_per_day = df['TweetAt'].dt.strftime('%m-%d').value_counts().sort_index().reset_index(name='counts')","644436ea":"plt.figure(figsize=(20,5))\nax = sns.barplot(x='index', y='counts', data=tweets_per_day,edgecolor = 'black',ci=False, palette='Blues_r')\nplt.title('Tweets count by date')\nplt.yticks([])\nax.bar_label(ax.containers[0])\nplt.ylabel('count')\nplt.xlabel('')\nplt.show()","3c536925":"tweets_per_country = df['Location'].value_counts().loc[lambda x : x > 100].reset_index(name='counts')","928eab5f":"plt.figure(figsize=(15,6))\nax = sns.barplot(x='index', y='counts', data=tweets_per_country,edgecolor = 'black',ci=False, palette='Spectral')\nplt.title('Tweets count by country')\nplt.xticks(rotation=70)\nplt.yticks([])\nax.bar_label(ax.containers[0])\nplt.ylabel('count')\nplt.xlabel('')\nplt.show()","6ea5679c":"df = df[['OriginalTweet','Sentiment']]","69b2a6de":"df_test = df_test[['OriginalTweet','Sentiment']]","b36e4daa":"##CUSTOM DEFINED FUNCTIONS TO CLEAN THE TWEETS\n\n#Clean emojis from text\ndef strip_emoji(text):\n    return re.sub(emoji.get_emoji_regexp(), r\"\", text) #remove emoji\n\n#Remove punctuations, links, mentions and \\r\\n new line characters\ndef strip_all_entities(text): \n    text = text.replace('\\r', '').replace('\\n', ' ').replace('\\n', ' ').lower() #remove \\n and \\r and lowercase\n    text = re.sub(r\"(?:\\@|https?\\:\/\/)\\S+\", \"\", text) #remove links and mentions\n    text = re.sub(r'[^\\x00-\\x7f]',r'', text) #remove non utf8\/ascii characters such as '\\x9a\\x91\\x97\\x9a\\x97'\n    banned_list= string.punctuation + '\u00c3'+'\u00b1'+'\u00e3'+'\u00bc'+'\u00e2'+'\u00bb'+'\u00a7'\n    table = str.maketrans('', '', banned_list)\n    text = text.translate(table)\n    return text\n\n#clean hashtags at the end of the sentence, and keep those in the middle of the sentence by removing just the # symbol\ndef clean_hashtags(tweet):\n    new_tweet = \" \".join(word.strip() for word in re.split('#(?!(?:hashtag)\\b)[\\w-]+(?=(?:\\s+#[\\w-]+)*\\s*$)', tweet)) #remove last hashtags\n    new_tweet2 = \" \".join(word.strip() for word in re.split('#|_', new_tweet)) #remove hashtags symbol from words in the middle of the sentence\n    return new_tweet2\n\n#Filter special characters such as & and $ present in some words\ndef filter_chars(a):\n    sent = []\n    for word in a.split(' '):\n        if ('$' in word) | ('&' in word):\n            sent.append('')\n        else:\n            sent.append(word)\n    return ' '.join(sent)\n\ndef remove_mult_spaces(text): # remove multiple spaces\n    return re.sub(\"\\s\\s+\" , \" \", text)","80fddeef":"texts_new = []\nfor t in df.OriginalTweet:\n    texts_new.append(remove_mult_spaces(filter_chars(clean_hashtags(strip_all_entities(strip_emoji(t))))))","799c562c":"texts_new_test = []\nfor t in df_test.OriginalTweet:\n    texts_new_test.append(remove_mult_spaces(filter_chars(clean_hashtags(strip_all_entities(strip_emoji(t))))))","78c327b4":"df['text_clean'] = texts_new\ndf_test['text_clean'] = texts_new_test","9daf021e":"df['text_clean'].head()","d9e8b630":"df_test['text_clean'].head()","6b97b312":"df['text_clean'][1:8].values","2853422e":"text_len = []\nfor text in df.text_clean:\n    tweet_len = len(text.split())\n    text_len.append(tweet_len)","20b2361e":"df['text_len'] = text_len","ff92857e":"text_len_test = []\nfor text in df_test.text_clean:\n    tweet_len = len(text.split())\n    text_len_test.append(tweet_len)","20cb603c":"df_test['text_len'] = text_len_test","028b3aca":"plt.figure(figsize=(7,5))\nax = sns.countplot(x='text_len', data=df[df['text_len']<10], palette='mako')\nplt.title('Training tweets with less than 10 words')\nplt.yticks([])\nax.bar_label(ax.containers[0])\nplt.ylabel('count')\nplt.xlabel('')\nplt.show()","d31928d1":"plt.figure(figsize=(7,5))\nax = sns.countplot(x='text_len', data=df_test[df_test['text_len']<10], palette='mako')\nplt.title('Test tweets with less than 10 words')\nplt.yticks([])\nax.bar_label(ax.containers[0])\nplt.ylabel('count')\nplt.xlabel('')\nplt.show()","a4f841e1":"print(f\" DF SHAPE: {df.shape}\")\nprint(f\" DF TEST SHAPE: {df_test.shape}\")","88bf1fda":"df = df[df['text_len'] > 4]","df974030":"df_test = df_test[df_test['text_len'] > 4]","438ffb0a":"print(f\" DF SHAPE: {df.shape}\")\nprint(f\" DF TEST SHAPE: {df_test.shape}\")","64880a71":"tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')","54cb62fa":"token_lens = []\n\nfor txt in df['text_clean'].values:\n    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n    token_lens.append(len(tokens))\n    \nmax_len=np.max(token_lens)","f21a4baa":"print(f\"MAX TOKENIZED SENTENCE LENGTH: {max_len}\")","5d035927":"token_lens = []\n\nfor i,txt in enumerate(df['text_clean'].values):\n    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n    token_lens.append(len(tokens))\n    if len(tokens)>80:\n        print(f\"INDEX: {i}, TEXT: {txt}\")  ","3f5eaaac":"df['token_lens'] = token_lens","abd0de7a":"df = df.sort_values(by='token_lens', ascending=False)\ndf.head(20)","88e9fe77":"df = df.iloc[12:]\ndf.head()","8d191bb6":"df = df.sample(frac=1).reset_index(drop=True)","e952d46f":"token_lens_test = []\n\nfor txt in df_test['text_clean'].values:\n    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n    token_lens_test.append(len(tokens))\n    \nmax_len=np.max(token_lens_test)","0301d98a":"print(f\"MAX TOKENIZED SENTENCE LENGTH: {max_len}\")","e181eb9f":"token_lens_test = []\n\nfor i,txt in enumerate(df_test['text_clean'].values):\n    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n    token_lens_test.append(len(tokens))\n    if len(tokens)>80:\n        print(f\"INDEX: {i}, TEXT: {txt}\")","98965a45":"df_test['token_lens'] = token_lens_test","5df80823":"df_test = df_test.sort_values(by='token_lens', ascending=False)\ndf_test.head(10) ","7399361d":"df_test = df_test.iloc[5:]\ndf_test.head(3)","fbfcab6a":"df_test = df_test.sample(frac=1).reset_index(drop=True)","d31ffcca":"df['Sentiment'].value_counts()","507df048":"df['Sentiment'] = df['Sentiment'].map({'Extremely Negative':0,'Negative':0,'Neutral':1,'Positive':2,'Extremely Positive':2})","04875a78":"df_test['Sentiment'] = df_test['Sentiment'].map({'Extremely Negative':0,'Negative':0,'Neutral':1,'Positive':2,'Extremely Positive':2})","f4c134d9":"df['Sentiment'].value_counts()","d7ee6515":"ros = RandomOverSampler()\ntrain_x, train_y = ros.fit_resample(np.array(df['text_clean']).reshape(-1, 1), np.array(df['Sentiment']).reshape(-1, 1));\ntrain_os = pd.DataFrame(list(zip([x[0] for x in train_x], train_y)), columns = ['text_clean', 'Sentiment']);","b299319e":"train_os['Sentiment'].value_counts()","c8041b5e":"X = train_os['text_clean'].values\ny = train_os['Sentiment'].values","ac7341a3":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, stratify=y, random_state=seed)","957fd831":"X_test = df_test['text_clean'].values\ny_test = df_test['Sentiment'].values","f597bde2":"y_train_le = y_train.copy()\ny_valid_le = y_valid.copy()\ny_test_le = y_test.copy()","0e81ffac":"ohe = preprocessing.OneHotEncoder()\ny_train = ohe.fit_transform(np.array(y_train).reshape(-1, 1)).toarray()\ny_valid = ohe.fit_transform(np.array(y_valid).reshape(-1, 1)).toarray()\ny_test = ohe.fit_transform(np.array(y_test).reshape(-1, 1)).toarray()","d230618c":"print(f\"TRAINING DATA: {X_train.shape[0]}\\nVALIDATION DATA: {X_valid.shape[0]}\\nTESTING DATA: {X_test.shape[0]}\" )","aafc1696":"clf = CountVectorizer()\nX_train_cv =  clf.fit_transform(X_train)\nX_test_cv = clf.transform(X_test)","2d93f623":"tf_transformer = TfidfTransformer(use_idf=True).fit(X_train_cv)\nX_train_tf = tf_transformer.transform(X_train_cv)\nX_test_tf = tf_transformer.transform(X_test_cv)","a2b80a9f":"nb_clf = MultinomialNB()","b8e0ff11":"nb_clf.fit(X_train_tf, y_train_le)","afcf0403":"nb_pred = nb_clf.predict(X_test_tf)","c9492b14":"print('\\tClassification Report for Naive Bayes:\\n\\n',classification_report(y_test_le,nb_pred, target_names=['Negative', 'Neutral', 'Positive']))","ab4f56b9":"MAX_LEN=128","561b9ab2":"def tokenize(data,max_len=MAX_LEN) :\n    input_ids = []\n    attention_masks = []\n    for i in range(len(data)):\n        encoded = tokenizer.encode_plus(\n            data[i],\n            add_special_tokens=True,\n            max_length=MAX_LEN,\n            padding='max_length',\n            return_attention_mask=True\n        )\n        input_ids.append(encoded['input_ids'])\n        attention_masks.append(encoded['attention_mask'])\n    return np.array(input_ids),np.array(attention_masks)","2b02a62a":"train_input_ids, train_attention_masks = tokenize(X_train, MAX_LEN)\nval_input_ids, val_attention_masks = tokenize(X_valid, MAX_LEN)\ntest_input_ids, test_attention_masks = tokenize(X_test, MAX_LEN)","b2245d8a":"bert_model = TFBertModel.from_pretrained('bert-base-uncased')","51aa9cc9":"def create_model(bert_model, max_len=MAX_LEN):\n    \n    ##params###\n    opt = tf.keras.optimizers.Adam(learning_rate=1e-5, decay=1e-7)\n    loss = tf.keras.losses.CategoricalCrossentropy()\n    accuracy = tf.keras.metrics.CategoricalAccuracy()\n\n\n    input_ids = tf.keras.Input(shape=(max_len,),dtype='int32')\n    \n    attention_masks = tf.keras.Input(shape=(max_len,),dtype='int32')\n    \n    embeddings = bert_model([input_ids,attention_masks])[1]\n    \n    output = tf.keras.layers.Dense(3, activation=\"softmax\")(embeddings)\n    \n    model = tf.keras.models.Model(inputs = [input_ids,attention_masks], outputs = output)\n    \n    model.compile(opt, loss=loss, metrics=accuracy)\n    \n    \n    return model","360919cb":"model = create_model(bert_model, MAX_LEN)\nmodel.summary()","7203ee1b":"history_bert = model.fit([train_input_ids,train_attention_masks], y_train, validation_data=([val_input_ids,val_attention_masks], y_valid), epochs=4, batch_size=32)","46da2d24":"result_bert = model.predict([test_input_ids,test_attention_masks])","f7f4f78e":"y_pred_bert =  np.zeros_like(result_bert)\ny_pred_bert[np.arange(len(y_pred_bert)), result_bert.argmax(1)] = 1","61750085":"conf_matrix(y_test.argmax(1), y_pred_bert.argmax(1),'BERT Sentiment Analysis\\nConfusion Matrix')","0908a777":"print('\\tClassification Report for BERT:\\n\\n',classification_report(y_test,y_pred_bert, target_names=['Negative', 'Neutral', 'Positive']))","1fb5dffe":"tokenizer_roberta = RobertaTokenizerFast.from_pretrained(\"roberta-base\")","303fc746":"token_lens = []\n\nfor txt in X_train:\n    tokens = tokenizer_roberta.encode(txt, max_length=512, truncation=True)\n    token_lens.append(len(tokens))\nmax_length=np.max(token_lens)\nmax_length","8348a8ae":"MAX_LEN=128","8fdbf676":"def tokenize_roberta(data,max_len=MAX_LEN) :\n    input_ids = []\n    attention_masks = []\n    for i in range(len(data)):\n        encoded = tokenizer_roberta.encode_plus(\n            data[i],\n            add_special_tokens=True,\n            max_length=max_len,\n            padding='max_length',\n            return_attention_mask=True\n        )\n        input_ids.append(encoded['input_ids'])\n        attention_masks.append(encoded['attention_mask'])\n    return np.array(input_ids),np.array(attention_masks)","ce9926f1":"train_input_ids, train_attention_masks = tokenize_roberta(X_train, MAX_LEN)\nval_input_ids, val_attention_masks = tokenize_roberta(X_valid, MAX_LEN)\ntest_input_ids, test_attention_masks = tokenize_roberta(X_test, MAX_LEN)","133cc372":"def create_model(bert_model, max_len=MAX_LEN):\n    \n    opt = tf.keras.optimizers.Adam(learning_rate=1e-5, decay=1e-7)\n    loss = tf.keras.losses.CategoricalCrossentropy()\n    accuracy = tf.keras.metrics.CategoricalAccuracy()\n\n    input_ids = tf.keras.Input(shape=(max_len,),dtype='int32')\n    attention_masks = tf.keras.Input(shape=(max_len,),dtype='int32')\n    output = bert_model([input_ids,attention_masks])\n    output = output[1]\n    output = tf.keras.layers.Dense(3, activation=tf.nn.softmax)(output)\n    model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n    model.compile(opt, loss=loss, metrics=accuracy)\n    return model","8cbb6edd":"roberta_model = TFRobertaModel.from_pretrained('roberta-base')","155e8824":"model = create_model(roberta_model, MAX_LEN)\nmodel.summary()","b556f5e4":"history_2 = model.fit([train_input_ids,train_attention_masks], y_train, validation_data=([val_input_ids,val_attention_masks], y_valid), epochs=4, batch_size=30)","07eea583":"result_roberta = model.predict([test_input_ids,test_attention_masks])","cd21dfc1":"y_pred_roberta =  np.zeros_like(result_roberta)\ny_pred_roberta[np.arange(len(y_pred_roberta)), result_roberta.argmax(1)] = 1","c9d7f07e":"conf_matrix(y_test.argmax(1),y_pred_roberta.argmax(1),'RoBERTa Sentiment Analysis\\nConfusion Matrix')","0468382d":"print('\\tClassification Report for RoBERTa:\\n\\n',classification_report(y_test,y_pred_roberta, target_names=['Negative', 'Neutral', 'Positive']))","f0ea3d3c":"print('Classification Report for BERT:\\n',classification_report(y_test,y_pred_bert, target_names=['Negative', 'Neutral', 'Positive']))","f62237ba":"print('Classification Report for RoBERTa:\\n',classification_report(y_test,y_pred_roberta, target_names=['Negative', 'Neutral', 'Positive']))","d1ce1447":"fig, ax = plt.subplots(1,2,figsize=(9,5.5))\n\nlabels = ['Negative', 'Neutral', 'Positive']\nplt.suptitle('Sentiment Analysis Comparison\\n Confusion Matrix', fontsize=20)\n\n\nsns.heatmap(confusion_matrix(y_test.argmax(1),y_pred_bert.argmax(1)), annot=True, cmap=\"Blues\", fmt='g', cbar=False, ax=ax[0], annot_kws={\"size\":25})\n\nax[0].set_title('BERT Classifier', fontsize=20)\nax[0].set_yticklabels(labels, fontsize=17);\nax[0].set_xticklabels(labels, fontsize=17);\nax[0].set_ylabel('Test', fontsize=20)\nax[0].set_xlabel('Predicted', fontsize=20)\n\nsns.heatmap(confusion_matrix(y_test.argmax(1),y_pred_roberta.argmax(1)), annot=True, cmap=\"Blues\", fmt='g', cbar=False, ax=ax[1], annot_kws={\"size\":25})\nax[1].set_title('RoBERTa Classifier', fontsize=20)\nax[1].set_yticklabels(labels, fontsize=17);\nax[1].set_xticklabels(labels, fontsize=17);\nax[1].set_ylabel('Test', fontsize=20)\nax[1].set_xlabel('Predicted', fontsize=20)\n\n\nplt.show()","2d6da040":"Now we can create a new column, for both train and test sets, to host the cleaned version of the tweets' text.","f1614c65":"## Training data deeper cleaning","4c477282":"# Sentiment Analysis Results:","b7668392":"First, we check the length of the longest tokenized sentence by roberta tokenizer:","7b8be97d":"Then we can define the tokenization function","78005518":"Good news, No duplicate tweets !","8f0b2bfa":"As we can see, there are lots of cleaned tweets with 0 words: this is due to the cleaning performed before. This means that some tweets contained only mentions, hashtags and links, which have been removed. We will drop these empty tweets and also those with less than 5 words.","8f6b0999":"The first thing we can do is to encode the categories with numbers. We will also create just 3 possible emotions: Positive, Neutral and Negative.","6c573c02":"We already performed a basic analyis of the tokenized sentences, now we just need to define a custom tokenizer function and call the encode_plus method of the BERT tokenizer.","aa942487":"This project is about the **analysis of tweets about coronavirus**, with the goal of performing a **Sentiment Analysis using BERT and roBERTa** algorithms to predict the emotion of a tweet (Positive, Negative or Neutral). In particular, both **BERT and ROBERTA will be fine tuned** using the given dataset in order to improve the model overall performance.<br>\nBefore feeding the data to the algorithms, **the tweets will be deeply cleaned to remove links, hashtags at the end of the sentences and punctuation** to allow the algorithms to better understand the text and improve the prediction performance.","a5744492":"We notice that in the dataset there are some days without tweets in the dataset. Among the days with tweets, most of them are made around the end of March: from 18th of Match to the 26th of March.","f13f4562":"Now the data cleaning is completed. I will perform more data cleaning if I have new ideas !! :)","0fee3b99":"We convert the date column 'TweetAt' to pandas datetime format to improve its usability in the further analysis.","8a4d2213":"# Tweets Deep Cleaning","1c0ddbe8":"Moreover, we will also create a column to host the lenght of the cleaned text, to check if by cleaning the text we removed too much text or almost entirely the tweet!","1ea17872":"We note that the three classes are imbalanced. We will proceed with oversampling the train test, to remove bias towards the majority classes.","106055d2":"## BERT Classification Report","fcabe497":"Now we can import the BERT model from the pretrained library from Hugging face.","51f856c6":"We can see that both the algorithms performed well on the classification task, with performance scores around 90%.","49e5d4f6":"## Train - Validation - Test split","b700c772":"# Loading the data","83facee8":"<img src=\"https:\/\/i.imgur.com\/3tASSmp.jpg\" width=\"700px\">","d4bf851e":"Then, we create a custom function to host the pre trained BERT model, and attach to it a 3 neurons output layer, necessary to perform the classification of the 3 different classes of the dataset (the 3 emotions).","d3f61e5b":"## One hot encoding","386049bd":"I performed a similar analysis on non-labeled tweets about Omicron Variant using Vader, NLTK, TextBLOB and Flair NLP algorithms at the following link:\n- https:\/\/www.kaggle.com\/ludovicocuoghi\/how-are-people-reacting-to-omicron-on-twitter","4536e974":"**Then we define custom functions to clean the text of the tweets.**","5d8d3471":"NOTE: UTF-8 encoding does not work on the dataset when loading it with pandas 'read_csv' function. This lead to the use of 'ISO-8859-1'\/latin-1 encoding. <br>\nIt will be found later that some special characters like apostrophes are turned into '\\x92', which will be taken care of during the data cleaning process.\n                                                                                                         ","9bd4e72e":"As seen for BERT, we first import the tokenizer used to train the original roberta transformer by Facebook.","7f1f0e17":"We will perform the data cleaning based on the tokenized sentences on the test set.","7f424d31":"Now we will look at the target column 'Sentiment'.","41a1f96a":"# Twitter Sentiment Analysis with BERT and roBERTa transformers","946823b6":"Let's check the long tokenized sentences (with more than 80 tokens ):","fe1cd1b6":"## Classification Matrix Comparison","d6f700c4":"A validation set will be extracted from the training set to monitor the validation accuracy, and so prevent overfitting.","0c3ad0a0":"# Sentiment column analysis","0d722bda":"# Results Summary","37cb3d1a":"## Class Balancing by RandomOverSampler","9bc19228":"Finally we can start fine tuning the BERT transformer !","a7ba02f5":"Then we create the TF-IDF (term-frequency times inverse document-frequency) versions of the tokenized tweets.","7e6c85af":"The 'location' column contains both countries and cities. It could be interesting to separate cities and countries, however this wont be investigated in this work.","203e10ed":"In the following, we will perform some data cleaning on the raw text of the tweets.<br>\nTo simplify the analaysis, we will just keep the columns 'Originaltweet' (raw tweets) and the target column 'Sentiment'.","56e98818":"## Duplicate tweets?","5b656791":"# RoBERTa results","9c591628":"The dataset looks more clean now. We will shuffle it and reset the index.","40e7626b":"**Thank you for reading my notebook!! Let me know if you have any question or if you want me to check out your works !! :)**","84d522e5":"# Tweets count by date","ed393010":"These sentences are not in english. They should be dropped.","7eead2c4":"# Tweets per country and city","1cf96460":"# RoBERTa Sentiment Analysis","0890cd42":"# BERT modeling","d1b22415":"Before implementing BERT, we will define a simple Naive Bayes baseline model to classify the tweets.","1e193474":"Let's perform a further cleaning checking the tokenizer version of the sentences.","739e27a7":"# RoBERTa modeling","88fb2459":"First we need to tokenize the tweets using CountVectorizer.","7827be4e":"In the next section we will perform the sentiment analysis using BERT.","620dc2b1":"<img src=\"https:\/\/i.imgur.com\/htll3Fu.png\" width=\"900px\">","423e6400":"# BERT results","d99fcc6b":"First, we import the BERT tokenizer.","65d8fffb":"Now we can define the Naive Bayes Classifier model","36fc67de":"After performing some tests, by using one hot encoding on the target variable we achieved higher accuracy. For this reason we will choose one hot enconding over label encoding. <br>\nEDIT: We will save a copy of the label encoded target columns since they could be useful for further analysis.","44b0a053":"# Baseline model: Naive Bayes Classifier","992b0863":"## RoBERTa Classification Report","2d90c4d2":"## Custom functions definition:","70ceacc9":"## Test data deeper cleaning","d95117f5":"Then, we apply the tokenizer function to the train, validation and test sets.","c7054080":"**The algorithm performance is not so bad. <br> The F1 score is around 70% for the more populated classes (Negative and Positive emotions), and lower for the Neutral class (F1=0.53).<br>\nIn particular, the overall accuracy is 70%.**","f7f9501a":"**The two algorithms performed quite well on the dataset, showing F1 and accuracy scores around 90%.**<br> Such high scores can only be achieved by performing a good cleaning of the original data, allowing the algorithms to learn the most from it.<br> **In particular, also a baseline Naive Bayes Classifier model has been trained to perform the sentiment classification, with a resulting accuracy and F1 around 70% (much lower than BERT)**.<br>\n**The training of BERT and roBERTa took around 11 minutes per epoch (for a total of 4 epochs) on GPU** per algorithm, since both **the transformers parameters (more than 100 million) have been fine tuned** to perform the best on the given dataset. It is possible to train only the last layer of the transformer without fine tuning the other parameters: however, this usually leads to inferior results compared to the fine tuning approach.","dbc96a96":"# BERT Sentiment Analysis"}}