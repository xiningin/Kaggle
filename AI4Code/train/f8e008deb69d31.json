{"cell_type":{"384abae7":"code","cdfc14d1":"code","5b9b4ed1":"code","ca88a1f6":"code","5b0401ec":"code","26fe1e88":"code","4f96aae7":"code","388433b5":"code","472914c9":"code","dc927b8e":"code","d64bf0e9":"code","6d36458f":"code","9167e5e9":"code","62765ea5":"code","443ef80d":"code","ed550676":"code","1e5dc6a9":"code","a4cc65fe":"code","8aad061e":"code","ba3f1f7d":"code","48929ab8":"code","58769301":"code","b79caa87":"code","57dc3a05":"code","9b588555":"code","05d32d95":"code","b024da2c":"code","ce172c82":"code","28a94bda":"code","efa3f85a":"code","95ed2fa4":"code","2d3d5752":"markdown","bc0b9217":"markdown","ae188d2e":"markdown","049e2806":"markdown","1a3ebdb1":"markdown","5264d6f6":"markdown","c671a814":"markdown","3428ba75":"markdown","27a302d0":"markdown","8f74da2a":"markdown","05b889be":"markdown","6018e2b7":"markdown","7ace396b":"markdown","c5c2e2e6":"markdown","4b38bacb":"markdown","ee25f324":"markdown","7ddff086":"markdown","d0ce8e57":"markdown","095b1870":"markdown","49dd5abf":"markdown","ec0628de":"markdown","c1684f17":"markdown","2dc89ec2":"markdown","ac0c9ee1":"markdown","32a1af63":"markdown","df01fa59":"markdown"},"source":{"384abae7":"import os #paths to file\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nimport warnings# warning filter\n\n\n#ploting libraries\nimport matplotlib.pyplot as plt \nimport matplotlib\nimport seaborn as sns\n\n#relevant ML libraries\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\n\n#ML models\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n#default theme\nsns.set(context='notebook', style='darkgrid', palette='deep', font='sans-serif', font_scale=1, color_codes=False, rc=None)\nmatplotlib.rcParams['figure.figsize'] =[10,8]\nmatplotlib.rcParams.update({'font.size': 12})\nmatplotlib.rcParams['font.family'] = 'sans-serif'\n\n#warning hadle\nwarnings.filterwarnings(\"ignore\")","cdfc14d1":"train=pd.read_csv('..\/input\/loan-prediction-analytics-vidhya\/train_ctrUa4K.csv')\ntest=pd.read_csv('..\/input\/loan-prediction-analytics-vidhya\/test_lAUu6dG.csv')","5b9b4ed1":"train.head(5)","ca88a1f6":"print(f\"training set (row, col): {train.shape}\\n\\ntesting set (row, col): {test.shape}\")","5b0401ec":"train.info()","26fe1e88":"train.describe(include='all')","4f96aae7":"num_columns = [f for f in train.columns if train.dtypes[f] != 'object']\nnum_columns","388433b5":"Categ_columns = [f for f in train.columns if train.dtypes[f] == 'object']\nCateg_columns.remove('Loan_ID')\nCateg_columns","472914c9":"train.dtypes.value_counts().plot.pie(explode=[0.1,0.1,0.1],autopct='%1.1f%%',)\nplt.title('data type %')","dc927b8e":"#the Id column is not needed, let's drop it for both test and train datasets\ntrain=train.drop('Loan_ID',axis=1)\ntest=test.drop('Loan_ID',axis=1)\n#checking the new shapes\nprint(f\"training set (row, col): {train.shape}\\n\\ntesting set (row, col): {test.shape}\")","d64bf0e9":"#missing values in decsending order\ntrain.isnull().sum().sort_values(ascending=False)","6d36458f":"# show the missing value in plot\nmissing = train.isnull().sum()\nmissing = missing[missing > 0]\nmissing.sort_values(inplace=True)\nmissing.plot.bar()","9167e5e9":"#filling the missing data\nprint(\"Before filling missing values \\n\")\n\nnull_cols = ['Credit_History', 'Self_Employed', 'LoanAmount','Dependents', 'Loan_Amount_Term', 'Gender', 'Married']\n","62765ea5":"for col in null_cols:\n    print(f\"{col}:\\n{train[col].value_counts()}\\n\",\"-\"*50)\n    train[col] = train[col].fillna(\n    train[col].dropna().mode().values[0] )   \n","443ef80d":"train.isnull().sum().sort_values(ascending=False)","ed550676":"print(\"After filling missing values\\n\\n\",\"#\"*50,\"\\n\")\nfor col in null_cols:\n    print(f\"\\n{col}:\\n{ train[col].value_counts()}\\n\",\"-\"*50)\n","1e5dc6a9":"train.hist(figsize=(12,15),edgecolor='black');","a4cc65fe":"#list of all the numeric columns\nnum = train.select_dtypes('number').columns.to_list()\n#list of all the categoric columns\ncat = train.select_dtypes('object').columns.to_list()\n\n#numeric df\ntrain_num =  train[num]\n#categoric df\ntrain_cat = train[cat]","8aad061e":"for i in train_num:\n    plt.hist(train_num[i])\n    plt.title(i)\n    plt.show()","ba3f1f7d":"for i in cat[:-1]: \n    plt.figure(figsize=(15,10))\n    plt.subplot(2,3,1)\n    sns.countplot(x=i ,hue='Loan_Status', data=train )\n    plt.xlabel(i, fontsize=14)","48929ab8":"#converting categorical values to numbers\n\nto_numeric = {'Male': 1, 'Female': 2,\n'Yes': 1, 'No': 2,\n'Graduate': 1, 'Not Graduate': 2,\n'Urban': 3, 'Semiurban': 2,'Rural': 1,\n'Y': 1, 'N': 0,\n'3+': 3}\n\n# adding the new numeric values from the to_numeric variable to both datasets\ntrain = train.applymap(lambda lable: to_numeric.get(lable) if lable in to_numeric else lable)\ntest = test.applymap(lambda lable: to_numeric.get(lable) if lable in to_numeric else lable)\n\n# convertind the Dependents column\nDependents_ = pd.to_numeric(train.Dependents)\nDependents__ = pd.to_numeric(test.Dependents)\n\n# dropping the previous Dependents column\ntrain.drop(['Dependents'], axis = 1, inplace = True)\ntest.drop(['Dependents'], axis = 1, inplace = True)\n\n# concatination of the new Dependents column with both datasets\ntrain = pd.concat([train, Dependents_], axis = 1)\ntest = pd.concat([test, Dependents__], axis = 1)\n\n# checking the our manipulated dataset for validation\nprint(f\"training set (row, col): {train.shape}\\n\\ntesting set (row, col): {test.shape}\\n\")\nprint(train.info(), \"\\n\\n\", test.info())","58769301":"train","b79caa87":"sns.heatmap(train.corr())","57dc3a05":"corr = train.corr()\ncorr.style.background_gradient(cmap='coolwarm').set_precision(2)","9b588555":"x = train.drop('Loan_Status', axis = 1)\ny = train['Loan_Status']\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 0)","05d32d95":"LR = LogisticRegression()\nLR.fit(x_train, y_train)\n\ny_predict = LR.predict(x_test)\n\n#  prediction Summary by species\nprint(classification_report(y_test, y_predict))\n\n# Accuracy score\nLR_SC = accuracy_score(y_predict,y_test)\nprint('accuracy is',accuracy_score(y_predict,y_test)*100)","b024da2c":"Logistic_Regression=pd.DataFrame({'y_test':y_test,'prediction':y_predict})\nLogistic_Regression.to_csv(\"Logistic Regression.csv\")  ","ce172c82":"DT = DecisionTreeClassifier()\nDT.fit(x_train, y_train)\n\ny_predict = DT.predict(x_test)\n\n#  prediction Summary by species\nprint(classification_report(y_test, y_predict))\n\n# Accuracy score\nDT_SC = accuracy_score(y_predict,y_test)\nprint(f\"{round(DT_SC*100,2)}% Accurate\")","28a94bda":"RF = RandomForestClassifier()\nRF.fit(x_train, y_train)\n\ny_predict = RF.predict(x_test)\n\n#  prediction Summary by species\nprint(classification_report(y_test, y_predict))\n\n# Accuracy score\nRF_SC = accuracy_score(y_predict,y_test)\nprint(f\"{round(RF_SC*100,2)}% Accurate\")","efa3f85a":"XGB = XGBClassifier()\nXGB.fit(x_train, y_train)\n\ny_predict = XGB.predict(x_test)\n\n#  prediction Summary by species\nprint(classification_report(y_test, y_predict))\n\n# Accuracy score\nXGB_SC = accuracy_score(y_predict,y_test)\nprint(f\"{round(XGB_SC*100,2)}% Accurate\")","95ed2fa4":"score = [DT_SC,RF_SC,XGB_SC,LR_SC]\nModels = pd.DataFrame({\n    'n_neighbors': [\"Decision Tree\",\"Random Forest\",\"XGBoost\", \"Logistic Regression\"],\n    'Score': score})\nModels.sort_values(by='Score', ascending=False)","2d3d5752":"# load data \ud83d\udcc2","bc0b9217":"split our data to categorical and numerical data,\n\nusing the .select_dtypes('dtype').columns.to_list() combination.","ae188d2e":"We can clearly see that Credit_History has the highest correlation with Loan_Status (a positive correlation of 0.54). Therefore our target value is highly dependant on this column.","049e2806":"### \tShow the categorical columns :","1a3ebdb1":"# Predict Loan Eligibility for Dream Housing Finance\n\n\nPredict Loan Eligibility for Dream Housing Finance company\nDream Housing Finance company deals in all kinds of home loans. They have presence across all urban, semi urban and rural areas. Customer first applies for home loan and after that company validates the customer eligibility for loan.\n\nCompany wants to automate the loan eligibility process (real time) based on customer detail provided while filling online application form. These details are Gender, Marital Status, Education, Number of Dependents, Income, Loan Amount, Credit History and others. To automate this process, they have provided a dataset to identify the customers segments that are eligible for loan amount so that they can specifically target these customers. \n\n\n\nData Dictionary\nTrain file: CSV containing the customers for whom loan eligibility is known as 'Loan_Status'\n\n![1.png](attachment:1.png)\n\n\nTest file: CSV containing the customer information for whom loan eligibility is to be predicted\n\n![2.png](attachment:2.png)\n\n\nSubmission file format\n\n![3.png](attachment:3.png)\n","5264d6f6":"### \tShow the numerical columns :","c671a814":"## First look at the data","3428ba75":"## B) decision tree\n![image.png](attachment:image.png)","27a302d0":"# finiding missing values","8f74da2a":"## Correlation table \n ","05b889be":"### Conclusion :\n1. Credit_History is a very important variable because of its high correlation with Loan_Status therefor showind high Dependancy for the latter.\n2. The Logistic Regression algorithm is the most accurate: approximately 83.76%.","6018e2b7":"### The Process of Modeling the Data:\n1. Importing the model\n\n2. Fitting the model\n\n3. Predicting Loan Status\n\n4. Classification report by Loan Status\n\n5. Overall accuracy","7ace396b":"# Data visalization \ud83d\udcca","c5c2e2e6":"### Each value will be replaced by the most frequent value (mode).","4b38bacb":"# import libraries \ud83d\udcd5\ud83d\udcd7\ud83d\udcd8","ee25f324":"# features transformation ","7ddff086":"## A) Logistic Regression\n![image.png](attachment:image.png)","d0ce8e57":"# Preprocessing and Data Analysis \ud83d\udcbb","095b1870":"### Models we will use:\n- Decision Tree\n- Random Forest\n- XGBoost\n- Logistic Regression","49dd5abf":"If you found this notebook interesting please upvote!\n\n![image.png](attachment:image.png)","ec0628de":"# Machine learning models","c1684f17":"### split data","2dc89ec2":"### check our data after replacing missing value ","ac0c9ee1":"# Correlation matrix\n","32a1af63":"##  XGBoost\n![image.png](attachment:image.png)","df01fa59":"## Random Forest\n![image.png](attachment:image.png)"}}