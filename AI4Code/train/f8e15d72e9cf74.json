{"cell_type":{"a710c13d":"code","6e566cf3":"code","5a69eff9":"code","a06f3aae":"markdown","67a3c3ed":"markdown","1244d7a7":"markdown"},"source":{"a710c13d":"##### INSTALLATION\n\nimport transformers\nfrom transformers import AutoModel, AutoTokenizer\ntransformers.__version__","6e566cf3":"##### INSTANTIATE MODEL\n\n# model name (if Internet is enabled)\n# model path (if Internet is disabled)\nmodel_path_or_name = '..\/input\/transformers\/roberta-base'\n\n# instantiate model & tokenizer\nmodel     = AutoModel.from_pretrained(model_path_or_name)\ntokenizer = AutoTokenizer.from_pretrained(model_path_or_name)","5a69eff9":"##### CHECK\n\nprint(model)","a06f3aae":"In NLP competitions, we frequently need to perform inference in a notebook that does not have internet access. This means that we need to instatiate transformer models offline.\n\nThis notebook demonstartes how to use the [pretrained transformers dataset](https:\/\/www.kaggle.com\/kozodoi\/transformers) to initialize some of the most popular `huggingface` transfomers models from the locally saved pre-trained weights, tokenizers and configuration files.\n\nFirst, we need to import the `transformers` library.","67a3c3ed":"The `transformers` package works with many NLP models. See [model hub](https:\/\/huggingface.co\/models) with the full list of models. \n\nThe [pretrained transformers dataset](https:\/\/www.kaggle.com\/kozodoi\/transformers) dataset currently includes the following models:\n- `albert-large-v2`\n- `bert-base-uncased`\n- `bert-large-uncased`\n- `distilroberta-base`\n- `distilbert-base-uncased`\n- `google\/electra-base-discriminator`\n- `facebook\/bart-base`\n- `facebook\/bart-large`\n- `funnel-transformer\/small`\n- `funnel-transformer\/large`\n- `roberta-base`\n- `roberta-large`\n- `t5-base`\n- `t5-large`\n- `xlnet-base-cased`\n- `xlnet-large-cased`\n\nYou can use the dataset as follows:\n- When Internet access is enabled (in the training notebooks), we can simply specify the model name in the model initialization function.\n- When Internet access is disabled (in the inference notebooks), we can instantiate a model from the pre-trained weights and confidgurations saved in the [pretrained transformers dataset](https:\/\/www.kaggle.com\/kozodoi\/transformers).\n\nLet's illustrate this by instantiating a Roberta model","1244d7a7":"This is it! The model is initialized. We can now load custom weights using:\n`model.load_state_dict(torch.load(weights_path))` \n\nHappy Kaggling and good luck in NLP competitions! :)"}}