{"cell_type":{"6bac2105":"code","8607c3b3":"code","663c00de":"code","6edfaef6":"code","fde1800c":"code","86219650":"code","2c1d5337":"code","6c079796":"code","d548ac4b":"code","92cad3f3":"code","d7d9bb3f":"code","59a098f3":"code","9d206f32":"code","7299befd":"code","eca69e85":"code","f32d78e8":"code","96a7437b":"code","a06d882d":"code","2a84b71a":"code","9e646c51":"code","74d66b55":"code","46afbd22":"markdown","59323c2b":"markdown","a9698171":"markdown","0a59376c":"markdown","3bacfa68":"markdown","3cb0cca8":"markdown","4242ddeb":"markdown","cd900489":"markdown","6f214afe":"markdown","43c1508e":"markdown","5c52763d":"markdown","760870e8":"markdown","e4933f57":"markdown","49cfc1d5":"markdown","f590da40":"markdown"},"source":{"6bac2105":"import pandas as pd \nimport numpy as np\nimport os\nimport pydicom\nimport glob\nfrom tqdm.notebook import tqdm\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport matplotlib.pyplot as plt \nfrom skimage import exposure\nimport cv2\nimport warnings\nfrom path import Path\nfrom fastai.vision.all import *\nfrom fastai.medical.imaging import *\nwarnings.filterwarnings(\"ignore\")","8607c3b3":"datapath = Path(\"..\/input\/siim-covid19-detection\")","663c00de":"datapath.ls()","6edfaef6":"train_study_df = pd.read_csv(datapath\/'train_study_level.csv')","fde1800c":"train_study_df.head()","86219650":"study_classes = [\"Negative for Pneumonia\" , \"Typical Appearance\",\n                \"Indeterminate Appearance\", \"Atypical Appearance\"]\nnp.unique(train_study_df[study_classes].values , axis = 0)","2c1d5337":"plt.figure(figsize = (10,5))\nplt.bar([1,2,3,4] , train_study_df[study_classes].values.sum(axis = 0))\nplt.xticks([1,2,3,4] , study_classes)\nplt.ylabel(\"Frequency\")\nplt.show()","6c079796":"train_image_df = pd.read_csv(datapath\/\"train_image_level.csv\")\ntrain_image_df.head()","d548ac4b":"train_image_df['split_label'] = train_image_df.label.apply(lambda x: [x.split()[offs:offs+6] for offs in range(0, len(x.split()), 6)])","92cad3f3":"classes_freq = []\nfor i in range(len(train_image_df)):\n    for j in train_image_df.iloc[i].split_label: classes_freq.append(j[0])\n\nplt.hist(classes_freq)\nplt.ylabel(\"Frequency\")","d7d9bb3f":"bbox_areas=[]\nfor i in range(len(train_image_df)):\n    for j in train_image_df.iloc[i].split_label:\n        bbox_areas.append((float(j[4])-float(j[2]))*(float(j[5])*float(j[3])))\n        \nplt.hist(bbox_areas)\nplt.ylabel(\"Frequency\")","59a098f3":"def dicom2array(path, voi_lut=True, fix_monochrome=True):\n    dicom = pydicom.read_file(path)\n    # VOI LUT (if available by DICOM device) is used to\n    # transform raw DICOM data to \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n    data = data - np.min(data)\n    data = data \/ np.max(data)\n    data = (data * 255).astype(np.uint8)\n    return data\n\n\ndef plot_img(img, size=(7, 7), is_rgb=True, title=\"\", cmap='gray'):\n    plt.figure(figsize=size)\n    plt.imshow(img, cmap=cmap)\n    plt.suptitle(title)\n    plt.show()\n\n    \ndef plot_imgs(imgs, cols=4, size=7, is_rgb=True, title=\"\", cmap='gray', img_size=(500,500)):\n    rows = len(imgs)\/\/cols + 1\n    fig = plt.figure(figsize=(cols*size, rows*size))\n    for i, img in enumerate(imgs):\n        if img_size is not None:\n            img = cv2.resize(img, img_size)\n        fig.add_subplot(rows, cols, i+1)\n        plt.imshow(img, cmap=cmap)\n    plt.suptitle(title)\n    plt.show()","9d206f32":"dicom_paths = get_dicom_files(datapath\/'train')\nimgs = [dicom2array(path) for path in dicom_paths[:4]]\nplot_imgs(imgs)","7299befd":"num_images_per_study = []\nfor i in (datapath\/'train').ls():\n    num_images_per_study.append(len(get_dicom_files(i)))\n    if len(get_dicom_files(i)) > 5:\n        print(f\"Study {i} had {len(get_dicom_files(i))} images\")","eca69e85":"plt.hist(num_images_per_study)","f32d78e8":"def image_path(row):\n    study_path = datapath\/\"train\"\/row.StudyInstanceUID\n    for i in get_dicom_files(study_path):\n        if row.id.split('_')[0] == i.stem: return i\n        \ntrain_image_df['image_path'] = train_image_df.apply(image_path , axis = 1)\n","96a7437b":"train_image_df.head()","a06d882d":"imgs = []\nimage_paths = train_image_df['image_path'].values\n\n# map label_id to specify color\nthickness = 10\nscale = 5\n\n\nfor i in range(8):\n    image_path = random.choice(image_paths)\n    print(image_path)\n    img = dicom2array(path=image_path)\n    img = cv2.resize(img, None, fx=1\/scale, fy=1\/scale)\n    img = np.stack([img, img, img], axis=-1)\n    for i in train_image_df.loc[train_image_df['image_path'] == image_path].split_label.values[0]:\n        if i[0] == 'opacity':\n            img = cv2.rectangle(img,\n                                (int(float(i[2])\/scale), int(float(i[3])\/scale)),\n                                (int(float(i[4])\/scale), int(float(i[5])\/scale)),\n                                [255,0,0], thickness)\n    \n    img = cv2.resize(img, (500,500))\n    imgs.append(img)\n\nplot_imgs(imgs, cmap=None)","2a84b71a":"submission_df = pd.read_csv(datapath\/\"sample_submission.csv\")\nsubmission_df.head()","9e646c51":"submission_df.iloc[2000:2010]","74d66b55":"submission_df.to_csv(\"submission.csv\" , index = False)","46afbd22":"# Lets look at the CSVs file\ncheck train_study_level.csv file","59323c2b":"As you can see, at the study-level, we are predicting the following classes:\n\n - Negative for Pneumonia\n - Typical Appearance\n - Indeterminate Appearance\n - Atypical Appearance\n - This here is a standard multi-label classification problem. In the training set, interestingly they are not multi-label, but it is mentioned that:\n\nStudies in the test set may contain more than one label.\n\nLet's look at the distribution:","a9698171":"Let's look at the unique labels:","0a59376c":"# Load the data","3bacfa68":"Let's actually look at how many images are available per study:","3cb0cca8":"We have our bounding box labels provided in the label column. The format is as follows:\n\n[class ID] [confidence score] [bounding box]\n\n - class ID - either opacity or none\n - confidence score - confidence from your neural network model. If none, the confidence is 1.\n - bounding box - typical xmin ymin xmax ymax format. If class ID is none, the bounding box is 1 0 0 1 1.\n \nThe bounding boxes are also provided in easily readable dictionary format in column boxes, and the study that each image is a part of is provided inStudyInstanceUID.\n\nLet's quick look at the distribution of opacity vs none:","4242ddeb":"We also have to provide the image-level bounding box. These will be of the format [class ID] [confidence score] [bounding box] as described earlier.\n\nOf course, in both cases, you can have multi-label scenarios.","cd900489":"We can see we have to provide the study-level class label. These will be of the format [class] 1 0 0 1 1","6f214afe":"Let's now look at train_image_level.csv:","43c1508e":"We can see that we have:\n\n - train_study_level.csv - the train study-level metadata, with one row for each study, including correct labels.\n - train_image_level.csv - the train image-level metadata, with one row for each image, including both correct labels and any bounding boxes in a dictionary format. Some images in both test and train have multiple bounding boxes.\n - sample_submission.csv - a sample submission file containing all image- and study-level IDs.\n - train folder - comprises 6,334 chest scans in DICOM format, stored in paths with the form study\/series\/image\n - test folder - The hidden test dataset is of roughly the same scale as the training dataset.","5c52763d":"# SIIM-FISABIO-RSNA COVID-19 Detection: A Simple and Easy EDA\nIn this competition, we are used DICOM images of chest X-ray radiographs, and to identify and localize COVID-19 abnormalities. This is important because typical diagnosis of COVID-19 requires molecular testing (polymerase chain reaction) requires several hours, while chest radiographs can be obtained in minutes, but it is hard to distinguish between COVID-19 pneumonia and other other viral and bacterial pneumonias. Therefore, in this competition, be hope to develop AI that that eventually help radiologists diagnose the millions of COVID-19 patients more confidently and quickly.\n","760870e8":"## Import the Depedencies","e4933f57":"## How to submit\nLet's now go over the sample_submission.csv file so we know how to submit our predictions.\n\nBefore we do so, it's worth reminding ourselves that this is a code-only competition, meaning that your submission file has to be generated in a script\/notebook. The sample_submission.csv file demonstrated what kind of file needs to be produced:\n\n","49cfc1d5":"## A look at the images\nOkay, let's now look at some example images:","f590da40":"Let's also look at the distribution of the bounding box areas:"}}