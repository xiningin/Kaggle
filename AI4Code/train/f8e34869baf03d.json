{"cell_type":{"a7fc6c3c":"code","ec00ad13":"code","0900b521":"code","a5937b05":"code","c727b1cc":"code","ae3524ea":"code","4221bc91":"code","be98a568":"code","dd5350c3":"code","80a72706":"code","9d862d74":"code","ff373cfa":"code","a65c9760":"code","f691164f":"code","b8f14880":"markdown","e2c84898":"markdown","5b638394":"markdown","670f381c":"markdown","c39d13e5":"markdown","cf4e2c01":"markdown","4a8e91b9":"markdown","9f0cfe59":"markdown","024d6b67":"markdown"},"source":{"a7fc6c3c":"## Data analytics ##\n####################\nimport pandas as pd\n\n## Machine learning ##\n######################\n\n## Get pipeline constructor\nfrom sklearn.pipeline import make_pipeline\n\n## Get preprocessing tools\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import make_column_transformer\n\n## Get model\nfrom sklearn.tree import DecisionTreeClassifier\n\n## Get cross validation tool\nfrom sklearn.model_selection import cross_val_score","ec00ad13":"mushroom_set = pd.read_csv('..\/input\/mushroom-classification\/mushrooms.csv')\nmushroom_set.head()","0900b521":"mushroom_set.shape","a5937b05":"## We are lucky there are no NaNs. Otherwise, I would drop NaN instances before\n## feeding the model with data.\nmushroom_set.isna().sum()","c727b1cc":"## I choose odor and spore-print-color as they are in top 2 IG (0.90, 0.48)\nfeatures_to_use = 'odor spore-print-color'.split()","ae3524ea":"## Create a feature and class frames\nX = mushroom_set[features_to_use]\ny = mushroom_set['class']\n\n## I will use cross_val_score I do not need to split data explicitly onto train and test\n## frames. Although, I use train_test_split function to extract out of sample data\n## from in sample data to use it later for a prediction test. \nX_train, X_out, y_train, y_out = train_test_split(X, \n                                                  y, \n                                                  test_size = 0.001, \n                                                  random_state = 42)","4221bc91":"## Instantiate One Hot Encoder\nohe = OneHotEncoder(handle_unknown='error')","be98a568":"## Set column transformer\ncolumn_trans = make_column_transformer(\n    (ohe,\n    features_to_use),\n    remainder='passthrough')","dd5350c3":"## Instantiate a decision tree with entropy as splitting criterion\nclassifier_en = DecisionTreeClassifier(criterion='entropy', \n                                       max_depth=4, \n                                       random_state=42)","80a72706":"pipe = make_pipeline(column_trans, classifier_en)","9d862d74":"## Evaluate\ncross_val_score(pipe, X_train, y_train, cv=6, scoring='accuracy').mean()","ff373cfa":"## Train\npipe.fit(X_train, y_train)","a65c9760":"## Check the order of classes in the pipeline\npipe.classes_","f691164f":"## Use out of sample data to predict target variable. I use .predict_proba() to see estimated \n## probability of predictions. The outpout is np array that I pass to DataFrame for more clear \n## data representation. With given features and the model accuracy I get 100% probability for\n## each prediction.\n##\n## It's worth to experiment with different features to see how probabilities change.\nprediction = pipe.predict_proba(X_out)\npd.DataFrame(prediction, columns=pipe.classes_)","b8f14880":"## Train & predict","e2c84898":"## Read data and explore","5b638394":"## Import section","670f381c":"# Tree-structure classifier model implementation\n## Case workbook\n<br><br>\n### Source: \n[F. Provost, T. Fawcett, \"Data Science for Business\"](https:\/\/data-science-for-biz.com\/)\n<br><br>\n### Dataset source: \n[Mushroom Data Set](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Mushroom)\n<br><br>\n### Problem outline: \nimplement a tree-structure classifier to predict a target variable (edible, poisonous). This is a supervised classification problem. [In the previous workbook](https:\/\/www.kaggle.com\/rafpast\/attribute-selection-with-information-gain-3-1-dsfb), we calculated [Information gain](https:\/\/en.wikipedia.org\/wiki\/Information_gain_in_decision_trees) for each feature which will serve here as a feature selection criterion. I will use features with the top 2 IG values in the model. IG table attached below.  \nAdditional problem to solve here is caused by the fact, that decision trees in sklearn do not handle nominal data. That is why I need to dummy encode it into numerical values using OneHotEncoder. [This lecture](https:\/\/www.youtube.com\/watch?v=irHhDMbw3xo) helped me a lot to understand how it works.\n<br><br>\nProblem type: classification\n\nDataset values: categorical\n\nTarget variable: edible (e), poisonous (p)\n\nSplitting criterion: [Informastion gain]\n(https:\/\/en.wikipedia.org\/wiki\/Information_gain_in_decision_trees)\n<br><br>\n### TODO, TOANSWER\n- How to visualize dummy encoded decision tree?\n- Is cross val score enough for pipeline validation (what are the pitfalls)?\n- What is the impact of handle_unknown on model cross-validation score?\n<br><br>\n<img src=\"https:\/\/raw.githubusercontent.com\/nefiu\/data_science_for_business_implementations\/1be90fcf25d74235a1636cac93abaf97e112077e\/3_intro_to_predictive_modeling\/static\/ig_table.png\"><\/img>","c39d13e5":"## Preprocessing","cf4e2c01":"## Building feature vector","4a8e91b9":"## Visualize\nwork in progress","9f0cfe59":"## Instantiate a model","024d6b67":"## Build a pipeline"}}