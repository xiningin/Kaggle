{"cell_type":{"16a7990f":"code","3a33237f":"code","3a9fb404":"code","c5e2ae2c":"code","e89d9dbc":"code","3e539c5d":"code","72513700":"code","c895d5b3":"code","d48de7dd":"code","af0e15a1":"code","fe7f24ec":"code","5fbdff6e":"code","04fded2b":"code","418271ab":"code","eee7bb40":"code","19214350":"code","c0a0ce47":"code","f5770192":"code","edfc8217":"code","a3ecbba6":"code","1ba3e573":"code","b256317c":"code","fc2ac493":"code","891893db":"code","15487a07":"code","6a60470f":"code","1f008d60":"code","7af91c58":"code","21f7c14f":"code","d2581ed6":"code","ff2ce9c0":"code","3e8f41f9":"markdown","2e42a294":"markdown"},"source":{"16a7990f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\nfrom sklearn.feature_selection import RFECV\nimport lightgbm as lgb\nfrom tqdm import tqdm_notebook\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\nimport multiprocessing\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","3a33237f":"# Load the dataset from the csv file using pandas\ntrain_transaction_dataset = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/train_transaction.csv')\ntrain_identity_dataset = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/train_identity.csv')\ntest_transaction_dataset = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/test_transaction.csv')\ntest_identity_dataset = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/test_identity.csv')\nsample_submission_dataset = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/sample_submission.csv')","3a9fb404":"# View columns\nprint(train_transaction_dataset.columns)\nprint(train_identity_dataset.columns)\nprint(test_transaction_dataset.columns)\nprint(test_identity_dataset.columns)","c5e2ae2c":"print(train_transaction_dataset.shape)\nprint(train_identity_dataset.shape)\nprint(test_transaction_dataset.shape)\nprint(test_identity_dataset.shape)","e89d9dbc":"# Merge transaction and identity dataset\ntrain_dataset = train_transaction_dataset.merge(train_identity_dataset, how='left', left_index=True, right_index=True)\ntest_dataset = test_transaction_dataset.merge(test_identity_dataset, how='left', left_index=True, right_index=True)\n\nprint(train_dataset.shape)\nprint(test_dataset.shape)","3e539c5d":"del train_transaction_dataset, test_transaction_dataset, train_identity_dataset, test_identity_dataset","72513700":"# source https:\/\/www.kaggle.com\/krishonaveen\/xtreme-boost-and-feature-engineering\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","c895d5b3":"train_dataset=reduce_mem_usage(train_dataset)\ntest_dataset=reduce_mem_usage(test_dataset)\nsample_submission_dataset=reduce_mem_usage(sample_submission_dataset)\nprint('training set shape:', train_dataset.shape)\nprint('test set shape:', test_dataset.shape)","d48de7dd":"import gc\n\ngc.collect()","af0e15a1":"# source https:\/\/www.kaggle.com\/nroman\/recursive-feature-elimination\n# we will eliminate some of the features:\n\n# 1. Features with only 1 unique value\none_value_cols = [col for col in train_dataset.columns if train_dataset[col].nunique() <= 1]\none_value_cols_test = [col for col in test_dataset.columns if test_dataset[col].nunique() <= 1]\n\n# 2. Features with more than 90% missing values\nmany_null_cols = [col for col in train_dataset.columns if train_dataset[col].isnull().sum() \/ train_dataset.shape[0] > 0.9]\nmany_null_cols_test = [col for col in test_dataset.columns if test_dataset[col].isnull().sum() \/ test_dataset.shape[0] > 0.9]\n\n# 3. Features with the top value appears more than 90% of the time\nbig_top_value_cols = [col for col in train_dataset.columns if train_dataset[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\nbig_top_value_cols_test = [col for col in test_dataset.columns if test_dataset[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\n\ncols_to_drop = list(set(many_null_cols + many_null_cols_test + big_top_value_cols + big_top_value_cols_test + one_value_cols + one_value_cols_test))\ncols_to_drop.remove('isFraud')\nprint('{} features are going to be dropped for being useless'.format(len(cols_to_drop)))\n\ntrain_dataset = train_dataset.drop(cols_to_drop, axis=1)\ntest_dataset = test_dataset.drop(cols_to_drop, axis=1)","fe7f24ec":"def id_split(dataframe):\n    dataframe['device_name'] = dataframe['DeviceInfo'].str.split('\/', expand=True)[0]\n    dataframe['device_version'] = dataframe['DeviceInfo'].str.split('\/', expand=True)[1]\n\n    dataframe['OS_id_30'] = dataframe['id_30'].str.split(' ', expand=True)[0]\n    dataframe['version_id_30'] = dataframe['id_30'].str.split(' ', expand=True)[1]\n\n    dataframe['browser_id_31'] = dataframe['id_31'].str.split(' ', expand=True)[0]\n    dataframe['version_id_31'] = dataframe['id_31'].str.split(' ', expand=True)[1]\n\n    dataframe['screen_width'] = dataframe['id_33'].str.split('x', expand=True)[0]\n    dataframe['screen_height'] = dataframe['id_33'].str.split('x', expand=True)[1]\n\n    dataframe.loc[dataframe['device_name'].str.contains('SM', na=False), 'device_name'] = 'Samsung'\n    dataframe.loc[dataframe['device_name'].str.contains('SAMSUNG', na=False), 'device_name'] = 'Samsung'\n    dataframe.loc[dataframe['device_name'].str.contains('GT-', na=False), 'device_name'] = 'Samsung'\n    dataframe.loc[dataframe['device_name'].str.contains('Moto G', na=False), 'device_name'] = 'Motorola'\n    dataframe.loc[dataframe['device_name'].str.contains('Moto', na=False), 'device_name'] = 'Motorola'\n    dataframe.loc[dataframe['device_name'].str.contains('moto', na=False), 'device_name'] = 'Motorola'\n    dataframe.loc[dataframe['device_name'].str.contains('LG-', na=False), 'device_name'] = 'LG'\n    dataframe.loc[dataframe['device_name'].str.contains('rv:', na=False), 'device_name'] = 'RV'\n    dataframe.loc[dataframe['device_name'].str.contains('HUAWEI', na=False), 'device_name'] = 'Huawei'\n    dataframe.loc[dataframe['device_name'].str.contains('ALE-', na=False), 'device_name'] = 'Huawei'\n    dataframe.loc[dataframe['device_name'].str.contains('-L', na=False), 'device_name'] = 'Huawei'\n    dataframe.loc[dataframe['device_name'].str.contains('Blade', na=False), 'device_name'] = 'ZTE'\n    dataframe.loc[dataframe['device_name'].str.contains('BLADE', na=False), 'device_name'] = 'ZTE'\n    dataframe.loc[dataframe['device_name'].str.contains('Linux', na=False), 'device_name'] = 'Linux'\n    dataframe.loc[dataframe['device_name'].str.contains('XT', na=False), 'device_name'] = 'Sony'\n    dataframe.loc[dataframe['device_name'].str.contains('HTC', na=False), 'device_name'] = 'HTC'\n    dataframe.loc[dataframe['device_name'].str.contains('ASUS', na=False), 'device_name'] = 'Asus'\n\n    dataframe.loc[dataframe.device_name.isin(dataframe.device_name.value_counts()[dataframe.device_name.value_counts() < 200].index), 'device_name'] = \"Others\"\n    dataframe['had_id'] = 1\n    gc.collect()\n    \n    return dataframe","5fbdff6e":"train_dataset = id_split(train_dataset)\ntest_dataset = id_split(test_dataset)","04fded2b":"train_dataset['P_email']=(train_dataset['P_emaildomain']=='xmail.com')\ntrain_dataset['R_email']=(train_dataset['R_emaildomain']=='xmail.com')\ntest_dataset['P_email']=(test_dataset['P_emaildomain']=='xmail.com')\ntest_dataset['R_email']=(test_dataset['R_emaildomain']=='xmail.com')","418271ab":"train_dataset['null'] = train_dataset.isna().sum(axis=1)\ntest_dataset['null'] = test_dataset.isna().sum(axis=1)","eee7bb40":"a = np.zeros(train_dataset.shape[0])\ntrain_dataset[\"lastest_browser\"] = a\na = np.zeros(test_dataset.shape[0])\ntest_dataset[\"lastest_browser\"] = a\ndef browser(df):\n    df.loc[df[\"id_31\"]==\"samsung browser 7.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"opera 53.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"mobile safari 10.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"google search application 49.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"firefox 60.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"edge 17.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 69.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 67.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 63.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 63.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0 for ios\",'lastest_browser']=1\n    return df\ntrain_dataset=browser(train_dataset)\ntest_dataset=browser(test_dataset)","19214350":"emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft', 'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo', 'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', 'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink', 'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other', 'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', 'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', 'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo', 'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other', 'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft', 'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', 'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', 'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', 'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', 'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', 'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', 'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other', 'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\nus_emails = ['gmail', 'net', 'edu']\nfor c in ['P_emaildomain', 'R_emaildomain']:\n    train_dataset[c + '_bin'] = train_dataset[c].map(emails)\n    test_dataset[c + '_bin'] = test_dataset[c].map(emails)\n    \n    train_dataset[c + '_suffix'] = train_dataset[c].map(lambda x: str(x).split('.')[-1])\n    test_dataset[c + '_suffix'] = test_dataset[c].map(lambda x: str(x).split('.')[-1])\n    \n    train_dataset[c + '_suffix'] = train_dataset[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n    test_dataset[c + '_suffix'] = test_dataset[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')","c0a0ce47":"%%time\nfrom sklearn.preprocessing import LabelEncoder\n\nfor col in test_dataset.columns:\n    if test_dataset[col].dtype == 'object':\n        le = LabelEncoder()\n        le.fit(list(train_dataset[col].astype(str).values) + list(test_dataset[col].astype(str).values))\n        train_dataset[col] = le.transform(list(train_dataset[col].astype(str).values))\n        test_dataset[col] = le.transform(list(test_dataset[col].astype(str).values))","f5770192":"%%time\ntrain_dataset = reduce_mem_usage(train_dataset)\ntest_dataset = reduce_mem_usage(test_dataset)","edfc8217":"# New feature - log of transaction amount. ()\ntrain_dataset['TransactionAmt_Log'] = np.log(train_dataset['TransactionAmt'])\ntest_dataset['TransactionAmt_Log'] = np.log(test_dataset['TransactionAmt'])\n\n# New feature - decimal part of the transaction amount.\ntrain_dataset['TransactionAmt_decimal'] = ((train_dataset['TransactionAmt'] - train_dataset['TransactionAmt'].astype(int)) * 1000).astype(int)\ntest_dataset['TransactionAmt_decimal'] = ((test_dataset['TransactionAmt'] - test_dataset['TransactionAmt'].astype(int)) * 1000).astype(int)\n\n# New feature - day of week in which a transaction happened.\ntrain_dataset['Transaction_day_of_week'] = np.floor((train_dataset['TransactionDT'] \/ (3600 * 24) - 1) % 7)\ntest_dataset['Transaction_day_of_week'] = np.floor((test_dataset['TransactionDT'] \/ (3600 * 24) - 1) % 7)\n\n# New feature - hour of the day in which a transaction happened.\ntrain_dataset['Transaction_hour'] = np.floor(train_dataset['TransactionDT'] \/ 3600) % 24\ntest_dataset['Transaction_hour'] = np.floor(test_dataset['TransactionDT'] \/ 3600) % 24","a3ecbba6":"important_cols = ['TransactionAmt', 'ProductCD',\n'card1',\n'card2',\n'card3',\n'card4',\n'card5',\n'card6',\n'addr1',\n'addr2',\n'dist1',\n'P_emaildomain',\n'R_emaildomain',\n'C1',\n'C2',\n'C4',\n'C5',\n'C6',\n'C7',\n'C8',\n'C9',\n'C10',\n'C11',\n'C12',\n'C13',\n'C14',\n'D1',\n'D2',\n'D3',\n'D4',\n'D5',\n'D6',\n'D8',\n'D9',\n'D10',\n'D11',\n'D12',\n'D13',\n'D14',\n'D15',\n'M2',\n'M3',\n'M4',\n'M5',\n'M6',\n'M8',\n'M9',\n'V4',\n'V5',\n'V12',\n'V13',\n'V19',\n'V20',\n'V30',\n'V34',\n'V35',\n'V36',\n'V37',\n'V38',\n'V44',\n'V45',\n'V47',\n'V53',\n'V54',\n'V56',\n'V57',\n'V58',\n'V61',\n'V62',\n'V70',\n'V74',\n'V75',\n'V76',\n'V78',\n'V82',\n'V83',\n'V87',\n'V91',\n'V94',\n'V96',\n'V97',\n'V99',\n'V126',\n'V127',\n'V128',\n'V130',\n'V131',\n'V139',\n'V143',\n'V149',\n'V152',\n'V160',\n'V165',\n'V170',\n'V187',\n'V189',\n'V201',\n'V203',\n'V204',\n'V207',\n'V208',\n'V209',\n'V210',\n'V212',\n'V217',\n'V221',\n'V222',\n'V234',\n'V257',\n'V258',\n'V261',\n'V264',\n'V265',\n'V266',\n'V267',\n'V268',\n'V271',\n'V274',\n'V275',\n'V277',\n'V278',\n'V279',\n'V280',\n'V282',\n'V283',\n'V285',\n'V287',\n'V289',\n'V291',\n'V292',\n'V294',\n'V306',\n'V307',\n'V308',\n'V310',\n'V312',\n'V313',\n'V314',\n'V315',\n'V317',\n'V323',\n'V324',\n'V332',\n'V333',\n'id_01',\n'id_02',\n'id_05',\n'id_06',\n'id_09',\n'id_13',\n'id_14',\n'id_17',\n'id_19',\n'id_20',\n'id_30',\n'id_31',\n'id_33',\n'id_38',\n'DeviceType',\n'DeviceInfo',\n'device_name', 'device_version', 'OS_id_30', 'version_id_30', 'browser_id_31', 'version_id_31', 'screen_width', 'screen_height',\n'P_email','R_email', 'lastest_browser', 'null', 'P_emaildomain_bin', 'P_emaildomain_suffix', 'R_emaildomain_bin', 'R_emaildomain_suffix',\n'TransactionAmt_Log', 'TransactionAmt_decimal', 'Transaction_day_of_week', 'Transaction_hour'\n]","1ba3e573":"print(len(important_cols))","b256317c":"train_dataset = train_dataset[important_cols + ['isFraud'] ]\ntest_dataset = test_dataset[important_cols]","fc2ac493":"train_dataset['card1_count_full'] = train_dataset['card1'].map(pd.concat([train_dataset['card1'], test_dataset['card1']], ignore_index=True).value_counts(dropna=False))\ntest_dataset['card1_count_full'] = test_dataset['card1'].map(pd.concat([train_dataset['card1'], test_dataset['card1']], ignore_index=True).value_counts(dropna=False))\n\ntrain_dataset['card2_count_full'] = train_dataset['card2'].map(pd.concat([train_dataset['card2'], test_dataset['card2']], ignore_index=True).value_counts(dropna=False))\ntest_dataset['card2_count_full'] = test_dataset['card2'].map(pd.concat([train_dataset['card2'], test_dataset['card2']], ignore_index=True).value_counts(dropna=False))\n\ntrain_dataset['card3_count_full'] = train_dataset['card3'].map(pd.concat([train_dataset['card3'], test_dataset['card3']], ignore_index=True).value_counts(dropna=False))\ntest_dataset['card3_count_full'] = test_dataset['card3'].map(pd.concat([train_dataset['card3'], test_dataset['card3']], ignore_index=True).value_counts(dropna=False))\n\ntrain_dataset['card4_count_full'] = train_dataset['card4'].map(pd.concat([train_dataset['card4'], test_dataset['card4']], ignore_index=True).value_counts(dropna=False))\ntest_dataset['card4_count_full'] = test_dataset['card4'].map(pd.concat([train_dataset['card4'], test_dataset['card4']], ignore_index=True).value_counts(dropna=False))\n\ntrain_dataset['card5_count_full'] = train_dataset['card5'].map(pd.concat([train_dataset['card5'], test_dataset['card5']], ignore_index=True).value_counts(dropna=False))\ntest_dataset['card5_count_full'] = test_dataset['card5'].map(pd.concat([train_dataset['card5'], test_dataset['card5']], ignore_index=True).value_counts(dropna=False))\n\ntrain_dataset['card6_count_full'] = train_dataset['card6'].map(pd.concat([train_dataset['card6'], test_dataset['card6']], ignore_index=True).value_counts(dropna=False))\ntest_dataset['card6_count_full'] = test_dataset['card6'].map(pd.concat([train_dataset['card6'], test_dataset['card6']], ignore_index=True).value_counts(dropna=False))\n\n\ntrain_dataset['addr1_count_full'] = train_dataset['addr1'].map(pd.concat([train_dataset['addr1'], test_dataset['addr1']], ignore_index=True).value_counts(dropna=False))\ntest_dataset['addr1_count_full'] = test_dataset['addr1'].map(pd.concat([train_dataset['addr1'], test_dataset['addr1']], ignore_index=True).value_counts(dropna=False))\n\ntrain_dataset['addr2_count_full'] = train_dataset['addr2'].map(pd.concat([train_dataset['addr2'], test_dataset['addr2']], ignore_index=True).value_counts(dropna=False))\ntest_dataset['addr2_count_full'] = test_dataset['addr2'].map(pd.concat([train_dataset['addr2'], test_dataset['addr2']], ignore_index=True).value_counts(dropna=False))","891893db":"train_dataset['TransactionAmt_to_mean_card1'] = train_dataset['TransactionAmt'] \/ train_dataset.groupby(['card1'])['TransactionAmt'].transform('mean')\ntrain_dataset['TransactionAmt_to_mean_card4'] = train_dataset['TransactionAmt'] \/ train_dataset.groupby(['card4'])['TransactionAmt'].transform('mean')\ntrain_dataset['TransactionAmt_to_std_card1'] = train_dataset['TransactionAmt'] \/ train_dataset.groupby(['card1'])['TransactionAmt'].transform('std')\ntrain_dataset['TransactionAmt_to_std_card4'] = train_dataset['TransactionAmt'] \/ train_dataset.groupby(['card4'])['TransactionAmt'].transform('std')\n\ntest_dataset['TransactionAmt_to_mean_card1'] = test_dataset['TransactionAmt'] \/ test_dataset.groupby(['card1'])['TransactionAmt'].transform('mean')\ntest_dataset['TransactionAmt_to_mean_card4'] = test_dataset['TransactionAmt'] \/ test_dataset.groupby(['card4'])['TransactionAmt'].transform('mean')\ntest_dataset['TransactionAmt_to_std_card1'] = test_dataset['TransactionAmt'] \/ test_dataset.groupby(['card1'])['TransactionAmt'].transform('std')\ntest_dataset['TransactionAmt_to_std_card4'] = test_dataset['TransactionAmt'] \/ test_dataset.groupby(['card4'])['TransactionAmt'].transform('std')\n\ntrain_dataset['id_02_to_mean_card1'] = train_dataset['id_02'] \/ train_dataset.groupby(['card1'])['id_02'].transform('mean')\ntrain_dataset['id_02_to_mean_card4'] = train_dataset['id_02'] \/ train_dataset.groupby(['card4'])['id_02'].transform('mean')\ntrain_dataset['id_02_to_std_card1'] = train_dataset['id_02'] \/ train_dataset.groupby(['card1'])['id_02'].transform('std')\ntrain_dataset['id_02_to_std_card4'] = train_dataset['id_02'] \/ train_dataset.groupby(['card4'])['id_02'].transform('std')\n\ntest_dataset['id_02_to_mean_card1'] = test_dataset['id_02'] \/ test_dataset.groupby(['card1'])['id_02'].transform('mean')\ntest_dataset['id_02_to_mean_card4'] = test_dataset['id_02'] \/ test_dataset.groupby(['card4'])['id_02'].transform('mean')\ntest_dataset['id_02_to_std_card1'] = test_dataset['id_02'] \/ test_dataset.groupby(['card1'])['id_02'].transform('std')\ntest_dataset['id_02_to_std_card4'] = test_dataset['id_02'] \/ test_dataset.groupby(['card4'])['id_02'].transform('std')\n\ntrain_dataset['D15_to_mean_card1'] = train_dataset['D15'] \/ train_dataset.groupby(['card1'])['D15'].transform('mean')\ntrain_dataset['D15_to_mean_card4'] = train_dataset['D15'] \/ train_dataset.groupby(['card4'])['D15'].transform('mean')\ntrain_dataset['D15_to_std_card1'] = train_dataset['D15'] \/ train_dataset.groupby(['card1'])['D15'].transform('std')\ntrain_dataset['D15_to_std_card4'] = train_dataset['D15'] \/ train_dataset.groupby(['card4'])['D15'].transform('std')\n\ntest_dataset['D15_to_mean_card1'] = test_dataset['D15'] \/ test_dataset.groupby(['card1'])['D15'].transform('mean')\ntest_dataset['D15_to_mean_card4'] = test_dataset['D15'] \/ test_dataset.groupby(['card4'])['D15'].transform('mean')\ntest_dataset['D15_to_std_card1'] = test_dataset['D15'] \/ test_dataset.groupby(['card1'])['D15'].transform('std')\ntest_dataset['D15_to_std_card4'] = test_dataset['D15'] \/ test_dataset.groupby(['card4'])['D15'].transform('std')\n\ntrain_dataset['D15_to_mean_addr1'] = train_dataset['D15'] \/ train_dataset.groupby(['addr1'])['D15'].transform('mean')\ntrain_dataset['D15_to_mean_card4'] = train_dataset['D15'] \/ train_dataset.groupby(['card4'])['D15'].transform('mean')\ntrain_dataset['D15_to_std_addr1'] = train_dataset['D15'] \/ train_dataset.groupby(['addr1'])['D15'].transform('std')\ntrain_dataset['D15_to_std_card4'] = train_dataset['D15'] \/ train_dataset.groupby(['card4'])['D15'].transform('std')\n\ntest_dataset['D15_to_mean_addr1'] = test_dataset['D15'] \/ test_dataset.groupby(['addr1'])['D15'].transform('mean')\ntest_dataset['D15_to_mean_card4'] = test_dataset['D15'] \/ test_dataset.groupby(['card4'])['D15'].transform('mean')\ntest_dataset['D15_to_std_addr1'] = test_dataset['D15'] \/ test_dataset.groupby(['addr1'])['D15'].transform('std')\ntest_dataset['D15_to_std_card4'] = test_dataset['D15'] \/ test_dataset.groupby(['card4'])['D15'].transform('std')","15487a07":"X_train = train_dataset.drop(columns='isFraud', axis=1)\ny_train = train_dataset['isFraud']\n\nX_test = test_dataset\n\ndel train_dataset, test_dataset\ngc.collect()","6a60470f":"#fill in mean for floats\nfor c in X_train.columns:\n    if X_train[c].dtype=='float16' or  X_train[c].dtype=='float32' or  X_train[c].dtype=='float64':\n        X_train[c].fillna(X_train[c].mean())\n        X_test[c].fillna(X_train[c].mean())\n\n#fill in -111 for categoricals\nX_train = X_train.fillna(-111)\nX_test = X_test.fillna(-111)","1f008d60":"X_train.head()","7af91c58":"os.environ['KMP_DUPLICATE_LIB_OK']='True'\nxgb_sub=sample_submission_dataset.copy()\nxgb_sub['isFraud'] = 0\n\n\n\nxgb = XGBClassifier(alpha=4, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n                    colsample_bynode=1, colsample_bytree=0.9, gamma=0.1,\n                    learning_rate=0.05, max_delta_step=0, max_depth=9,\n                    min_child_weight=1, missing=-111, n_estimators=500, n_jobs=1,\n                    nthread=None, objective='binary:logistic', random_state=0,\n                    reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n                    silent=None, subsample=0.9, tree_method='gpu_hist', verbosity=1,\n                    eval_metric=\"auc\")\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(X_train)):\n    X_train_, X_valid = X_train.iloc[train_index], X_train.iloc[valid_index]\n    y_train_, y_valid = y_train.iloc[train_index], y_train.iloc[valid_index]\n    xgb.fit(X_train_,y_train_)\n    del X_train_,y_train_\n    #pred=xgb.predict_proba(X_test)[:,1]\n    val=xgb.predict_proba(X_valid)[:,1]\n    del X_valid\n    print('ROC accuracy: {}'.format(roc_auc_score(y_valid, val)))\n    del val,y_valid\n    #xgb_sub['isFraud'] = xgb_sub['isFraud']+pred\/n_fold\n    #del pred\n    gc.collect()\n    \ndel xgb\n\npred=xgb.predict_proba(X_test)[:,1]\nxgb_sub['isFraud'] = pred","21f7c14f":"xgb_sub.to_csv('submission_cv.csv', index=False)","d2581ed6":"xgb = XGBClassifier(alpha=4, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n                    colsample_bynode=1, colsample_bytree=0.9, gamma=0.1,\n                    learning_rate=0.05, max_delta_step=0, max_depth=9,\n                    min_child_weight=1, missing=-111, n_estimators=500, n_jobs=1,\n                    nthread=None, objective='binary:logistic', random_state=0,\n                    reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n                    silent=None, subsample=0.9, tree_method='gpu_hist', verbosity=1,\n                    eval_metric=\"auc\")\n\nxgb.fit(X_train,y_train)\n\nxgb_sub['isFraud'] = xgb.predict_proba(X_test)[:,1]","ff2ce9c0":"xgb_sub.to_csv('submission_full.csv', index=False)","3e8f41f9":"## Feature Engineering","2e42a294":"## Loading dataset"}}