{"cell_type":{"43aacaae":"code","755323bd":"code","baa7821d":"code","0741d586":"code","dd063f12":"code","e0529bec":"code","7cb55729":"code","14694155":"code","80b3eaeb":"code","9e0afe1b":"code","63ee9905":"code","93f502a2":"code","c9488c1c":"code","f9d894b7":"code","82ac2e1f":"code","d704225b":"code","fdde34cf":"code","b4f4e033":"code","643fbea1":"code","58f569f4":"code","0ed46cff":"code","138a5a63":"code","90a2d5a6":"code","c46ae5b1":"code","2f0e8c92":"code","bf51a359":"code","107049f8":"code","17e8427c":"code","d1f8ce6e":"code","2601f805":"code","bc89daad":"code","68ad6124":"code","82c90128":"code","60816572":"markdown","9e0ae6fd":"markdown","9a615fe5":"markdown","6567c1e8":"markdown","12214cac":"markdown","ba2b5ef6":"markdown","fe8b5d8f":"markdown","1e9c79c0":"markdown","d0de94c5":"markdown","0eea673b":"markdown","7785f4c9":"markdown","103cf3b6":"markdown","8542d644":"markdown","ec929aa0":"markdown","46b02e26":"markdown","25877596":"markdown","4346aecb":"markdown","7e9d7c40":"markdown","1ae988c1":"markdown"},"source":{"43aacaae":"##Import libraries to head structured data\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","755323bd":"##reading and saving data\ndf = pd.read_csv(\"..\/input\/concrete-dataset\/Concrete_Data.csv\")\ndf","baa7821d":"df.head()","0741d586":"df.columns","dd063f12":"## renaming column names\ndf = df.rename(columns={'Cement (component 1)(kg in a m^3 mixture)':'Cement',\n       'Blast Furnace Slag (component 2)(kg in a m^3 mixture)':'BFS',\n       'Fly Ash (component 3)(kg in a m^3 mixture)':'Fly_Ash',\n       'Water  (component 4)(kg in a m^3 mixture)':'Water',\n       'Superplasticizer (component 5)(kg in a m^3 mixture)':'Superplasticizer',\n       'Coarse Aggregate  (component 6)(kg in a m^3 mixture)':'Coarser_agg',\n       'Fine Aggregate (component 7)(kg in a m^3 mixture)':'Fine_agg',\n       'Age (day)':'Days',\n       'Concrete compressive strength(MPa. megapascals)':'Comp_str'})","e0529bec":"##Checking the new columns names\ndf.columns","7cb55729":"##checking the data statistics\ndf.describe()","14694155":"## checking if there's any missing value .sum() will count how many lines will have missing data\ndf.isnull().sum()","80b3eaeb":"## Importing relevante libraries to plot graphs\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","9e0afe1b":"## Plotting scatterplot to check if there's a relation between variables\nplt.figure(figsize=(15,10))\nsns.regplot(x='Cement',y='Comp_str', data=df)\nplt.title(\"Cement x Comp_str\", size=20)\nplt.xlabel(\"Cement\", size=18)\nplt.ylabel(\"Comp_str\", size = 18)","63ee9905":"## Too many grouped data, plotting histogram\n\nplt.figure(figsize=(15,10))\nsns.histplot(data=df.BFS)\nplt.title(\"BFS Distribution\", size=20)\nplt.xlabel(\"BFS\", size=18)\nplt.ylabel(\"Count\", size = 18)","93f502a2":"## Too many grouped data, plotting histogram\n\nplt.figure(figsize=(15,10))\nsns.histplot(data=df.Fly_Ash)\nplt.title(\"Distribution of Fly_Ash\", size=20)\nplt.xlabel(\"Fly Ash\", size=18)\nplt.ylabel(\"Count\", size = 18)","c9488c1c":"plt.figure(figsize=(15,10))\nsns.regplot(x='Water',y='Comp_str', data=df)\nplt.title(\"Water x Comp_str\", size=20)\nplt.xlabel(\"Water\", size=18)\nplt.ylabel(\"Comp_str\", size = 18)","f9d894b7":"## Too many grouped data, plotting histogram\n\nplt.figure(figsize=(15,10))\nsns.histplot(data=df.Superplasticizer)\nplt.title(\"Distribution of Superplasticizer\", size=20)\nplt.xlabel(\"Superplasticizer\", size=18)\nplt.ylabel(\"Count\", size = 18)","82ac2e1f":"plt.figure(figsize=(15,10))\nsns.regplot(x='Coarser_agg',y='Comp_str', data=df)\nplt.title(\"Coarser_agg x Comp_str\", size=20)\nplt.xlabel(\"Coarser_agg\", size=18)\nplt.ylabel(\"Comp_str\", size = 18)","d704225b":"plt.figure(figsize=(15,10))\nsns.regplot(x='Fine_agg',y='Comp_str', data=df)\nplt.title(\"Fine_agg x Comp_str\", size=20)\nplt.xlabel(\"Fine_agg\", size=18)\nplt.ylabel(\"Comp_str\", size = 18)","fdde34cf":"## Too many grouped data, plotting histogram\n\nplt.figure(figsize=(15,10))\nsns.histplot(data=df.Fine_agg)\nplt.title(\"Fine_agg Distribution\", size=20)\nplt.xlabel(\"Fine_agg\", size=18)\nplt.ylabel(\"Count\", size = 18)","b4f4e033":"## Too many grouped data, plotting histogram\n\nplt.figure(figsize=(15,10))\nsns.histplot(data=df.Coarser_agg)\nplt.title(\"Coarser_agg Distribution\", size=20)\nplt.xlabel(\"Coarser_agg\", size=18)\nplt.ylabel(\"Count\", size = 18)","643fbea1":"## Too many grouped data, plotting histogram\n\nplt.figure(figsize=(15,10))\nsns.histplot(data=df.Days)\nplt.title(\"Days Distribution\", size=20)\nplt.xlabel(\"Days\", size=18)\nplt.ylabel(\"Count\", size = 18)","58f569f4":"## Filtering data and saving in a new variable\n\ndf_low = df.loc[(df.BFS<150) & (df.Fly_Ash<50) & (df.Superplasticizer<15) & (df.Days<50)]\ndf_low","0ed46cff":"## Dividing the other part of the dataset, by removing the index of the new created dataset.\n\ndf_high = df.drop(df_low.index)\ndf_high","138a5a63":"## Checking if the data is still grouped even after the filter\n\nfig, [(ax1, ax2), (ax3, ax4)] = plt.subplots(2,2,figsize=(15,15))\n\n\nsns.regplot(ax=ax1, x='BFS',y='Comp_str', data=df_high)\nax1.set(title=\"BFS x Comp_str\", xlabel = \"BFS\", ylabel=\"Comp_str\")\n\nsns.regplot(ax=ax2, x='Fly_Ash',y='Comp_str', data=df_high)\nax2.set(title=\"Fly_Ash x Comp_str\", xlabel = \"Fly_Ash\", ylabel=\"Comp_str\")\n\n\nsns.regplot(ax=ax3, x='Superplasticizer',y='Comp_str', data=df_high)\nax3.set(title=\"Superplasticizer x Comp_str\", xlabel = \"Superplasticizer\", ylabel=\"Comp_str\")\n\n\nsns.regplot(ax=ax4, x='Days',y='Comp_str', data=df_high)\nax4.set(title=\"Days x Comp_str\", xlabel = \"Days\", ylabel=\"Comp_str\")","90a2d5a6":"## Checking correlation between features\n\nmask = np.zeros_like(df.corr())\nmask[np.triu_indices_from(mask)] = True ## This will hide the upper triangle,\n                                        ##so we check the relation between the target and features only once\n\nplt.subplots(figsize=(15,10))\nsns.heatmap(df.corr(), annot=True, mask=mask) ## Annot=true shows the values within the rectangles","c46ae5b1":"## Importing libraries to process linear regression\n\nfrom sklearn.linear_model import LinearRegression","2f0e8c92":"## Defining features\n\nfeatures = ['Cement', 'BFS', 'Fly_Ash', 'Water', 'Superplasticizer', 'Coarser_agg',\n       'Fine_agg', 'Days']\ntargets = ['Comp_str']","bf51a359":"## Importing model and fitting the data\n\nmodel = LinearRegression(fit_intercept=True) ## We assume that if all features are 0, the comp_str is 0,\n                                             ##if there's no material, there are no concrete, thus, no compt_str\nresults = model.fit(df[features], df[targets])","107049f8":"## Checking the R2 of the fitted data\n\nprint(\"The R^2 for this dataset in this Linear Regression is: \", results.score(df[features],df[targets]))","17e8427c":"results.coef_[0][0]","d1f8ce6e":"from sklearn import tree\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n## Importing relevant libraries","2601f805":"X_train, X_test, y_train, y_test = train_test_split(df[features], df[targets]) ## Creating train and test\n\nmax_depth = [2,3,4,5,6,7,8,9,10,11] ## Creating for loop to check the tree in different depths\nscore_DTR_test = []\nscore_DTR_train = []\nscore_RFR_test = []\nscore_RFR_train = [] ## Creating lists to append the results and plot a graph\n\nfor depth in max_depth:\n    DTR = tree.DecisionTreeRegressor(max_depth=depth,\n                                 min_samples_split=5,\n                                 min_samples_leaf=2) ## Defining model with best parameters\n\n    model_DTR = DTR.fit(X_train,y_train.values.ravel()) ## Fitting the data\n    y_pred_DTR = model_DTR.predict(X_test) ## Prediticting using the test data\n\n    RFR = RandomForestRegressor(max_depth=depth,\n                            n_estimators=100, ## number of trees \n                            min_samples_split=5, ## min samples to split data\n                            min_samples_leaf=2, ## min samples in a leaf\n                            criterion='mse') \n\n    model_RFR = RFR.fit(X_train,y_train.values.ravel())\n    y_pred_RFR = model_RFR.predict(X_test) ## Prediticting using the test data\n    \n    print(\"-----------------------------------------------------------\\nCurrent depth of: \", depth)\n    print(\"The R^2 for the test data in this Decision Tree Regression is: \", model_DTR.score(X_test,y_test))\n    ## Getting score for the current depth of the tree with test data\n    score_DTR_test.append(model_DTR.score(X_test,y_test)) ## Saving the Decision tree test result into the list\n\n    print(\"The R^2 for the training data in this Decision Tree Regression is: \", model_DTR.score(X_train,y_train))\n    ## Getting score for the current depth of the tree with train data\n    score_DTR_train.append(model_DTR.score(X_train,y_train)) ## Saving the Decision tree train result into the list\n\n    RMSE = np.sqrt(mean_squared_error(y_test,y_pred_DTR))\n    print(\"The Root Mean Squared Error for this is: \",RMSE,\"\\n\" ) ##Getting the Root Mean Squared error (measuring the fit)\n    \n    print(\"The R^2 for the test data in this Random Forest Regression is: \", model_RFR.score(X_test,y_test))\n    ## Getting score for the current depth of the forest with test data\n    score_RFR_test.append(model_RFR.score(X_test,y_test)) ## Saving the Random Forest test result into the list\n    \n    print(\"The R^2 for the training data in this Random Forest Regression is: \", model_RFR.score(X_train,y_train))\n    ## Getting score for the current depth of the forest with train data\n    score_RFR_train.append(model_RFR.score(X_train,y_train)) ## Saving the Random Forest train result into the list\n    \n    RMSE = np.sqrt(mean_squared_error(y_test,y_pred_RFR))\n    print(\"The Root Mean Squared Error for this is: \",RMSE,\"\\n\") ##Getting the Root Mean Squared error (measuring the fit)\n\nfig, (ax1,ax2) =  plt.subplots(1,2,sharey=True, figsize=(12,10))\nsns.lineplot(ax=ax1, x=max_depth,y=score_DTR_test)\nsns.lineplot(ax=ax1, x=max_depth,y=score_DTR_train)\nax1.set(title=\"Decision Tree Results\",xlabel=\"depth\",ylabel=\"Accuracy\")\nax1.legend([\"Test Data\", \"Train Data\"])\n\nsns.lineplot(ax=ax2, x=max_depth,y=score_RFR_test)\nsns.lineplot(ax=ax2, x=max_depth,y=score_RFR_train)\nax2.set(title=\"Random Forest Results\",xlabel=\"depth\",ylabel=\"Accuracy\")\nax2.legend([\"Test Data\", \"Train Data\"])","bc89daad":"## conda install python-graphviz\n## Remove comments if you don't have it installed","68ad6124":"import graphviz \nfrom IPython.display import Image\n\n\ndot_data = tree.export_graphviz(model_DTR, out_file='tree.dot',\n                                feature_names=df[features].columns,  \n                                class_names=df[targets],\n                                filled=True, rounded=True,  \n                                special_characters=True) \n\ngraph = graphviz.Source(dot_data)\n\n!dot -Tpng -Gdpi=600 tree.dot -o tree.png ","82c90128":"print(model_RFR)","60816572":"If you want to see more in depth thoughts on this analysis, please visit my blog: https:\/\/datasciencerecruit.com\/","9e0ae6fd":"# Checking Data and Renaming Columns","9a615fe5":"# Importing Data","6567c1e8":"Initially. We'll use a Linear regression to try to fit the model, since this seems the reasonable approach for this case.","12214cac":"The effect of Coarser and Fina Aggregators seems to have a small negative impact when incresing their percentage. We'll use this feature.","ba2b5ef6":"It looks like that, with more days, our concrete becomes a bit stronger.","fe8b5d8f":"# Checking Missing Values","1e9c79c0":"Same thing as the BFS data, but more concentrated.","d0de94c5":"-----------","0eea673b":"Data here seems to have more variance. Most of our data have no BFS (Please refer to this link for more details on its use: https:\/\/theconstructor.org\/concrete\/blast-furnace-slag-cement\/23534\/)\n\nIt seems that this feature is more important in certain cases on the industry. We see that with values of 200, the Comp_str decreases a little, which is expected. For smaller values than 100 the comp_str is more disperse, so it may be related to the proportin of the other features. ","7785f4c9":"Since this variable have many values in 0, let's try to run a jointplot","103cf3b6":"# Linear Regression","8542d644":"Now let's check the columns before we start applying some visualiations","ec929aa0":"# Data Visualization","46b02e26":"It seems is a positive relationship, let's check more features","25877596":"Similar to BFS and Fly Ash. The use of superpasticizes aims to reduce the use of Water in concrete (This can be seen in their correlation which is very negative -0.66), which would decrease the Comp_str even more.","4346aecb":"# Exploring Decision Trees and Random Forest","7e9d7c40":"We want to measure the Compressive Strength of the Concrete, therefore, we'll drop this feature from the dataset.","1ae988c1":"No missing values. Proceeding to visuals"}}