{"cell_type":{"ab1de194":"code","17dd6127":"code","ea51bd97":"code","784f4f3d":"code","ace4680d":"code","3abd0c23":"code","6e24c78f":"code","fd858346":"code","d513999f":"code","be222b4d":"code","034ab779":"code","7c6c7c2b":"code","eced8fd0":"code","29dfda5a":"code","99b3a9af":"code","eec041fb":"code","292e1bd2":"code","acbd93f8":"code","50a7ed5b":"code","934417d3":"code","f877fa1b":"code","955546e5":"code","84ea1e17":"code","3f5ba248":"code","180080ca":"code","055adc5b":"code","f6f51fa3":"code","9d0eee15":"code","bc19b9de":"code","52482b84":"code","d61aa593":"code","aef4ff0d":"code","06ff1dc2":"code","ef948b4b":"code","3592d3f6":"code","f11d207a":"markdown","a4f6dea1":"markdown","8ff8a68b":"markdown","c2aabdfe":"markdown"},"source":{"ab1de194":"import tarfile\ndef extract(tar_file, path):\n    opened_tar = tarfile.open(tar_file)\n     \n    if tarfile.is_tarfile(tar_file):\n        opened_tar.extractall(path)\n    else:\n        print(\"The tar file you entered is not a tar file\")\n        \n# extract('..\/input\/stanford-car-dataset-full\/car_ims.tar', '\/kaggle\/working\/data\/')","17dd6127":"!pip install console-progressbar","ea51bd97":"import numpy as np\nimport scipy.io as sio \nimport os\nimport shutil\nimport cv2\nimport matplotlib.pyplot as plt\nimport random\nfrom console_progressbar import ProgressBar","784f4f3d":"os.listdir('..\/input\/')","ace4680d":"def get_make(class_name):\n    idx = class_name.find(' ')\n    return class_name[:idx]\n\nmeta = sio.loadmat('..\/input\/stanford-cars-dataset\/cars_annos.mat')\nmake_list = []\nconvert = {}\n\nnumber_to_make = {}\nfor i, clas in enumerate(meta['class_names'][0]):\n    \n    make = get_make(clas[0])\n    if make not in make_list:\n        make_list.append(make)\n    convert[i] = make_list.index(make)\n    number_to_make[convert[i]] = make","3abd0c23":"number_to_make[convert[192]]","6e24c78f":"def get_labels(path):\n    annos = sio.loadmat('..\/input\/devkit\/devkit\/cars_train_annos.mat')\n#     annos = sio.loadmat('..\/input\/stanford-cars-dataset\/cars_annos.mat')\n    _, total_size = annos[\"annotations\"].shape\n    labels = {}\n    for i in range(total_size):\n        path = annos['annotations'][:,i][0][5][0]\n        clas = annos['annotations'][:,i][0][4][0][0]\n        labels[path] = convert[clas-1]\n    return labels\nlabels_n= get_labels('cars_train_annos')","fd858346":"print(len(labels_n.keys()))","d513999f":"# for given filenames, labels and boxes function split\n# data into validation and training data, cropping bounding boxes\n# resizing and saving to a folder, if it's test data we don't have labels and split data\ndef save_data(fnames, labels, bboxes, data_type='train'):\n    if data_type == 'train':\n        src_folder = '..\/input\/stanford-cars-dataset\/cars_train\/cars_train\/'\n    else:\n        src_folder = '..\/input\/stanford-cars-dataset\/cars_test\/cars_test'\n    num_samples = len(fnames)\n    \n    if data_type == 'train':\n        perm = np.random.permutation(num_samples)\n\n        train_split = 0.8\n        num_train = int(round(num_samples * train_split))\n\n    pb = ProgressBar(total=100, prefix='Save train data', suffix='', decimals=3, length=50, fill='=')\n\n    for i in range(num_samples):\n        fname = fnames[i]\n        if data_type == 'train':\n            label = labels[i]\n        (x1, y1, x2, y2) = bboxes[i]\n\n        src_path = os.path.join(src_folder, fname)\n        src_image = cv2.imread(src_path)\n        height, width = src_image.shape[:2]\n        # margins of 16 pixels\n        margin = 16\n        x1 = max(0, x1 - margin)\n        y1 = max(0, y1 - margin)\n        x2 = min(x2 + margin, width)\n        y2 = min(y2 + margin, height)\n        # print(\"{} -> {}\".format(fname, label))\n        pb.print_progress_bar((i + 1) * 100 \/ num_samples)\n        dst_path = os.path.join('\/kaggle\/working\/data\/test', fname)\n        if data_type == 'train':\n            if i < num_train:\n                dst_folder = '\/kaggle\/working\/data\/train\/'\n            else:\n                dst_folder = '\/kaggle\/working\/data\/valid\/'\n\n            dst_path = os.path.join(dst_folder, label)\n            if not os.path.exists(dst_path):\n                os.makedirs(dst_path)\n            dst_path = os.path.join(dst_path, fname)\n\n        crop_image = src_image[y1:y2, x1:x2]\n        dst_img = cv2.resize(src=crop_image, dsize=(224, 224))\n        cv2.imwrite(dst_path, dst_img)        ","be222b4d":"# parsing labels: class indexes, bounding boxes, file names\ndef process_data(data_type = 'train'):\n    print(\"Processing train data...\")\n    if data_type == 'train':\n        cars_annos = sio.loadmat('..\/input\/devkit\/devkit\/cars_train_annos.mat')\n    else:\n        cars_annos = sio.loadmat('..\/input\/stanford-cars-dataset\/car_devkit\/devkit\/cars_test_annos.mat')\n    annotations = cars_annos['annotations']\n    annotations = np.transpose(annotations)\n\n    fnames = []\n    class_ids = []\n    bboxes = []\n    labels = []\n\n    for annotation in annotations:\n        bbox_x1 = annotation[0][0][0][0]\n        bbox_y1 = annotation[0][1][0][0]\n        bbox_x2 = annotation[0][2][0][0]\n        bbox_y2 = annotation[0][3][0][0]\n        \n        if data_type == 'train':\n            class_id = annotation[0][4][0][0]\n            fname = annotation[0][5][0]\n            labels.append('%02d' % labels_n[fname])\n            class_ids.append(labels_n[fname])\n            \n        else:\n            fname = annotation[0][4][0]\n        \n        bboxes.append((bbox_x1, bbox_y1, bbox_x2, bbox_y2))\n        fnames.append(fname)\n        \n    if data_type == 'train':\n        labels_count = np.unique(class_ids).shape[0]\n        print(np.unique(class_ids))\n        print('The number of different cars is %d' % labels_count)\n    else:\n        print('Test data processing')\n    save_data(fnames, labels, bboxes, data_type)","034ab779":"def make_folder(folder):\n    if not os.path.exists(folder):\n        os.makedirs(folder)","7c6c7c2b":"new_dirs = ['\/kaggle\/working\/data\/train', '\/kaggle\/working\/data\/valid', '\/kaggle\/working\/data\/test']\n\nfor new_dir in new_dirs:\n    make_folder(new_dir)","eced8fd0":"process_data('train')","29dfda5a":"process_data('test')","99b3a9af":"def del_dirs(dirs_list):\n    for dirr in dirs_list:\n        shutil.rmtree(dirr)\n# del_dirs(new_dirs)","eec041fb":"from __future__ import print_function, division\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport numpy as np\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport matplotlib.pyplot as plt\nimport time\nimport os\nimport copy\n\nplt.ion()   # interactive mode","292e1bd2":"bs = 64\n\n# Data augmentation and normalization for training\n# Just normalization for validation\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'valid': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\ndata_dir = 'data\/'\nimage_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'valid']}\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=bs,\n                                             shuffle=True, num_workers=4)\n              for x in ['train', 'valid']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'valid']}\nclass_names = image_datasets['train'].classes\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","acbd93f8":"def imshow(inp, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\n\n# Get a batch of training data\ninputs, classes = next(iter(dataloaders['train']))\n\n# Make a grid from batch\nout = torchvision.utils.make_grid(inputs)\nimshow(out, title=[class_names[x] for x in classes])","50a7ed5b":"def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'valid']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        # clip\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            epoch_loss = running_loss \/ dataset_sizes[phase]\n            epoch_acc = running_corrects.double() \/ dataset_sizes[phase]\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'valid' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model\n","934417d3":"def visualize_model(model, num_images=6):\n    was_training = model.training\n    model.eval()\n    images_so_far = 0\n    fig = plt.figure()\n\n    with torch.no_grad():\n        for i, (inputs, labels) in enumerate(dataloaders['valid']):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n\n            for j in range(inputs.size()[0]):\n                images_so_far += 1\n                ax = plt.subplot(num_images\/\/2, 2, images_so_far)\n                ax.axis('off')\n                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n                imshow(inputs.cpu().data[j])\n\n                if images_so_far == num_images:\n                    model.train(mode=was_training)\n                    return\n        model.train(mode=was_training)","f877fa1b":"# using focal loss function for better result\nimport torch.nn.functional as F\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1., gamma=2.):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n\n    def forward(self, inputs, targets, **kwargs):\n        CE_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)\n        pt = torch.exp(-CE_loss)\n        F_loss = self.alpha * ((1-pt)**self.gamma) * CE_loss\n        return F_loss.mean()","955546e5":"# using resnext pretrained model\nmodel_conv = torchvision.models.resnext101_32x8d(pretrained=True)\nfor param in model_conv.parameters():\n    param.requires_grad = False\n\n# Parameters of newly constructed modules have requires_grad=True by default\nnum_ftrs = model_conv.fc.in_features\n# fine tuning last layer of resnext pretrained model\nmodel_conv.fc = nn.Sequential(\n                                nn.Linear(num_ftrs, 512),\n                                nn.ReLU(),\n                                nn.BatchNorm1d(512),\n                                nn.Linear(512, 49)\n                                )\nmodel_conv = model_conv.to(device)\ncriterion = FocalLoss()\n\n# Observe that only parameters of final layer are being optimized as\n# opposed to before.\noptimizer_conv = optim.Adam(model_conv.fc.parameters())#, lr=0.0, lr_decay=0.96)","84ea1e17":"model_conv = train_model(model_conv, criterion, optimizer_conv,\n                         exp_lr_scheduler, num_epochs=100)","3f5ba248":"torch.save(model_conv, '\/kaggle\/working\/data\/modresn66')","180080ca":"model = torch.load('..\/input\/resnextmod\/modresn66')","055adc5b":"# for name, param in model.named_parameters():\n#     print(name, param.requires_grad)\n#     param.requires_grad = False\n# Parameters of newly constructed modules have requires_grad=True by default\n# num_ftrs = model.fc.in_features\n# model.fc = nn.Linear(2048, 49)\nmodel = model.to(device)\n\ncriterion = FocalLoss()\n\n# Observe that only parameters of final layer are being optimized as\n# opposed to before.\noptimizer_conv = optim.Adam(model.fc.parameters(), lr=0.00003)#, lr=0.0, lr_decay=0.96)\n\n# Decay LR by a factor of 0.1 every 7 epochs\nexp_lr_scheduler = lr_scheduler.ExponentialLR(optimizer=optimizer_conv, gamma=0.96)\n","f6f51fa3":"model = train_model(model, criterion, optimizer_conv,\n                         exp_lr_scheduler, num_epochs=50)","9d0eee15":"visualize_model(model)\n\nplt.ioff()\nplt.show()","bc19b9de":"model.eval()","52482b84":"from torch.autograd import Variable\ndef image_loader(img):\n    cv2.cvtColor(srcBGR, cv2.COLOR_BGR2RGB)x\n    \"\"\"load image, returns cuda tensor\"\"\"\n    image = data_transforms['valid'](img).float()\n    plt.imshow(img)\n    image = torch.Tensor(image)\n    image = image.unsqueeze(0)\n    return image.cuda()  #assumes that you're using GPU","d61aa593":"from tqdm import tqdm_notebook\nfrom PIL import Image, ImageFile\nnum_samples,all_preds = 8041,[]\nout = open('result.txt', 'a')\nfor i in range(num_samples):\n#     i=6\n    filename = os.path.join('.\/data\/test', '%05d.jpg' % (i + 1))\n    bgr_img = cv2.imread(filename)\n    rgb_img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2RGB)\n    im_pil = Image.fromarray(rgb_img)\n    \n    im_pill = image_loader(im_pil)\n    preds = model(im_pill)\n    \n    class_id = preds.max(1)[1]\n#     print(number_to_make[int(class_id)])\n    all_preds.append(class_id)\n    out.write('{}\\n'.format(str(class_id + 1)))\n#     break\n    \nout.close()\n","aef4ff0d":"labels = sio.loadmat('..\/input\/devkitt\/cars_test_annos_withlabels.mat')\na = []\nfor lab in labels['annotations']['class'][0]:\n#     print(lab)\n    a.append(convert[lab[0][0]-1])\nactual_preds = np.array(a,dtype=np.int);\nactual_preds = actual_preds.squeeze()\nall_preds = np.array(all_preds)","06ff1dc2":"print(len(all_preds), len(actual_preds))","ef948b4b":"print('accuracy = ',(all_preds==actual_preds).sum()\/len(actual_preds))","3592d3f6":"def predict_single_image(ID):","f11d207a":"# Pre-Processing","a4f6dea1":"# Creating data transformations, data loaders","8ff8a68b":"# Matching between class number and make name","c2aabdfe":"# Function for model training"}}