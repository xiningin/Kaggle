{"cell_type":{"b5bfafb4":"code","e6bbbd60":"code","a70597b9":"code","34fd0f6f":"code","ce86e6cf":"code","ad3cd4f7":"code","e177921b":"code","6f2eaf4d":"code","5eaba4a4":"code","453b1bc1":"code","13dfc6fb":"code","e068cde1":"code","be2de1e9":"code","bcc25ff6":"code","9a6d7d8e":"code","07da5f56":"code","7f23f2c4":"code","17a0253b":"code","5cd02446":"code","fbb589c1":"code","a5c8e9b6":"code","74b13997":"code","cf0523ce":"code","c36e7486":"code","0bc4bf67":"code","1aee09d1":"code","e336084e":"code","43a95f75":"code","40355784":"code","75682bbd":"code","6c894fb8":"code","88f9576a":"code","8a3bdb5a":"code","b4558737":"code","f075ec24":"code","eed4b306":"code","15d2d34a":"code","86ca2cbc":"markdown","20e30ebb":"markdown","6b26e379":"markdown","d231fba3":"markdown","16b5723b":"markdown","62d7bc53":"markdown","75cd0d89":"markdown","508041bc":"markdown","c1bce092":"markdown","67da8097":"markdown","61c00cf2":"markdown","696c0312":"markdown","96a5eea8":"markdown","ab75d483":"markdown","e22ffd98":"markdown","e3241c5e":"markdown","8ff5522f":"markdown","da2608c2":"markdown","879e3bb2":"markdown","a70d6353":"markdown","3505d944":"markdown","ebcee8ce":"markdown","9013a0b7":"markdown","4f6461f4":"markdown","02486a0e":"markdown","fa522054":"markdown","1002ae94":"markdown","f27e1306":"markdown","3cdebe02":"markdown","6026500e":"markdown","70c9fcca":"markdown","a42bb087":"markdown","a00a3e57":"markdown","889058a7":"markdown","9bd34dec":"markdown","7810b385":"markdown","3a156041":"markdown","30447885":"markdown","100b59f2":"markdown","bdbdf7ab":"markdown","2ae030b5":"markdown","660ddb9a":"markdown","b2ff25f8":"markdown","a919a060":"markdown"},"source":{"b5bfafb4":"import numpy as np\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nimport xgboost as xgb\n\nfrom scipy.stats import kurtosis, skew # to explore statistics on Sale Price\n\n# Importing plotting libraries\nfrom plotly.offline import init_notebook_mode, iplot, plot \nimport plotly.graph_objs as go \ninit_notebook_mode(connected=True)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","e6bbbd60":"datafile = \"..\/input\/ameshousing\/Ames_Housing_Data.tsv\"\ndf=pd.read_csv(datafile, sep='\\t')","a70597b9":"df.head()","34fd0f6f":"# Let's have a look at the features\ndf.info()","ce86e6cf":"# Quick peek at the data\ndef expandHead(x, nrow = 6, ncol = 4):\n    # https:\/\/stackoverflow.com\/a\/53873661\/1578274\n    pd.set_option('display.expand_frame_repr', False)\n    seq = np.arange(0, len(x.columns), ncol)\n    for i in seq:\n        print(x.loc[range(0,nrow), x.columns[range(i,min(i+ncol, len(x.columns)))]])\n    pd.set_option('display.expand_frame_repr', True)\n    \nexpandHead(df, 3, 8)","ad3cd4f7":"# Exclude nominal and ordinal as well (per data dict)\nexc_cols = ['Bedroom AbvGr', 'HalfBath', 'Kitchen AbvGr','Bsmt Full Bath', 'Bsmt Half Bath', 'MS SubClass']\nnumerical_cols = [c for c in df.columns if df[c].dtype != 'object' and c not in exc_cols]\n\n# expandHead(df.loc[:4, numerical_cols], 4, 8)","e177921b":"#Lets start by plotting a heatmap to determine if any variables are correlated\n# Correlation Heatmap\n# ref: https:\/\/towardsdatascience.com\/the-art-of-effective-visualization-of-multi-dimensional-data-6c7202990c57\nf, ax = plt.subplots(figsize=(25, 15))\ncorr = df[numerical_cols].corr()\nhm = sns.heatmap(round(corr,2), annot=False, ax=ax, cmap=\"coolwarm\",fmt='.2f',\n                 linewidths=.05) # Set annot=True for pearson labels.\nf.subplots_adjust(top=0.93)\nt= f.suptitle('Housing Attributes Correlation Heatmap', fontsize=18)","6f2eaf4d":"df['N_priceLbl'] = df.SalePrice.apply(lambda p: \n                                    'low' if p < 129500 else\n                                   'medium' if p < 213500 else\n                                   'high')","5eaba4a4":"corr_features = ['Overall Qual', 'Year Built', 'Total Bsmt SF', 'Enclosed Porch', 'SalePrice']\n\n# Density and scatter pair plots for highly correlated features\nsns.pairplot(df, vars=corr_features, hue='N_priceLbl', diag_kind = 'kde',\n             plot_kws = {'alpha': 0.6, 's': 80, 'edgecolor': 'k'},\n             height = 4)","453b1bc1":"missing_df = df.isnull().sum(axis=0).reset_index()\nmissing_df.columns = ['column_name', 'missing_count']\nmissing_df = missing_df.loc[missing_df['missing_count']>0]\nmissing_df = missing_df.sort_values(by='missing_count')\n\nind = np.arange(missing_df.shape[0])\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(12,15))\nrects = ax.barh(ind, missing_df.missing_count.values, color='blue')\nax.set_yticks(ind)\nax.set_yticklabels(missing_df.column_name.values, rotation='horizontal')\nax.set_xlabel(\"Count of missing values\")\nax.set_title(\"Number of missing values in each column\")\n\n# Add value labels\nfor i, v in enumerate(missing_df.missing_count.values):\n    ax.text(v + 10, i, str(v), color='b')\nplt.show()","13dfc6fb":"# GarageYrBlt has missing values compared to YearBuilt. So let's drop it\ndf = df.drop('Garage Yr Blt', axis=1)","e068cde1":"mask = df['Garage Area'] == 0\ndf.loc[mask, 'Garage Area'].count()","be2de1e9":"df.loc[df['Bsmt Half Bath'].isnull(), 'Bsmt Half Bath'] = 0.0\ndf.loc[df['Bsmt Full Bath'].isnull(), 'Bsmt Full Bath'] = 0.0\ndf.loc[df['Garage Cars'].isnull(), 'Garage Cars'] = 0.0\ndf.loc[df['BsmtFin SF 1'].isnull(), 'BsmtFin SF 1'] = 0.0\ndf.loc[df['BsmtFin SF 2'].isnull(), 'BsmtFin SF 2'] = 0.0\ndf.loc[df['Bsmt Unf SF'].isnull(), 'Bsmt Unf SF'] = 0.0\ndf.loc[df['Total Bsmt SF'].isnull(), 'Total Bsmt SF'] = 0.0\ndf.loc[df['Garage Area'].isnull(), 'Garage Area'] = 0.0\ndf.loc[df['Electrical'].isnull(), 'Electrical'] = 'SBrkr'","bcc25ff6":"missing_df = df.isnull().sum(axis=0).reset_index()\nmissing_df.columns = ['column_name', 'missing_count']\nmissing_df = missing_df.loc[missing_df['missing_count']>0]\nmissing_df = missing_df.sort_values(by='missing_count', ascending=False)\nmissing_df","9a6d7d8e":"missing_prop = (df.isnull().sum()\/df.shape[0]).reset_index()\nmissing_prop.columns = ['field', 'proportion']\nmissing_prop = missing_prop.sort_values(by='proportion', ascending=False)\nmissing_prop.head()","07da5f56":"df.loc[df.nunique().values == 1]\n# There are no constant variables","7f23f2c4":"plt.figure(figsize=(14,6))\n\nsns.countplot(x='Neighborhood', data=df, order = df['Neighborhood'].value_counts()[:10].index)\nplt.title(\"Top 10 Most Frequent Neighborhoods\", fontsize=20) # Adding Title and seting the size\nplt.xlabel(\"Neighborhood\", fontsize=16) # Adding x label and seting the size\nplt.ylabel(\"Sale Counts\", fontsize=16) # Adding y label and seting the size\nplt.xticks(rotation=45) # Adjust the xticks, rotating the labels\n\nplt.show()","17a0253b":"plt.figure(figsize=(16,6))\nsns.set_style(\"whitegrid\")\ng1 = sns.boxenplot(x='Neighborhood', y='SalePrice', \n                   data=df[df['SalePrice'] > 0])\ng1.set_title('Neighborhoods by SalePrice', fontsize=20)\ng1.set_xticklabels(g1.get_xticklabels(),rotation=45)\ng1.set_xlabel('Neighborhood', fontsize=18) # Xlabel\ng1.set_ylabel('SalePrice', fontsize=18) #Ylabel\n\nplt.show()","5cd02446":"# Setting the first trace\ntrace1 = go.Histogram(x=df[\"Yr Sold\"],\n                      name='Year Count')\n\n# Setting the second trace\ntrace2 = go.Histogram(x=df[\"Mo Sold\"],\n                name='Month Count')\n\ndata = [trace1, trace2]\n\n# Creating menu options\nupdatemenus = list([\n    dict(active=-1,\n         x=-0.15,\n         buttons=list([  \n             dict(\n                 label = 'Years Count',\n                 method = 'update',\n                 args = [{'visible': [True, False]}, # This trace visible flag\n                         {'title': 'Count of Year'}]),\n             dict(\n                 label = 'Months Count',\n                 method = 'update',\n                 args = [{'visible': [False, True]},\n                         {'title': 'Count of Months'}])\n         ]))\n])\n\nlayout = dict(title='Number of Sales by Year\/Month (Select from Dropdown)',\n              showlegend=False,\n              updatemenus=updatemenus,\n#              xaxis = dict(\n#                  type=\"category\"\n#                      ),\n              barmode=\"group\"\n             )\nfig = dict(data=data, layout=layout)\nprint(\"SELECT OPTION BELOW: \")\niplot(fig)\n","fbb589c1":"df.SalePrice.describe()","a5c8e9b6":"df.SalePrice.plot.hist()","74b13997":"print('Excess kurtosis of normal distribution (should be 0): {}'.format(\n    kurtosis(df[df['SalePrice'] > 0]['SalePrice'])))\nprint( 'Skewness of normal distribution (should be < abs 0.5): {}'.format(\n    skew((df[df['SalePrice'] > 0]['SalePrice']))))\n","cf0523ce":"def explore_outliers(df_num, num_sd = 3, verbose = False): \n    '''\n    Set a numerical value and it will calculate the upper, lower and total number of outliers.\n    It will print a lot of statistics of the numerical feature that you set on input.\n    Adapted from: https:\/\/www.kaggle.com\/kabure\/exploring-the-consumer-patterns-ml-pipeline\n    '''\n    \n    data_mean, data_std = np.mean(df_num), np.std(df_num)\n\n    # Outlier SD\n    cut = data_std * num_sd\n\n    # IQR thresholds\n    lower, upper = data_mean - cut, data_mean + cut\n\n    # creating an array of lower, higher and total outlier values \n    outliers_lower = [x for x in df_num if x < lower]\n    outliers_higher = [x for x in df_num if x > upper]\n    outliers_total = [x for x in df_num if x < lower or x > upper]\n\n    # array without outlier values\n    outliers_removed = [x for x in df_num if x > lower and x < upper]\n    \n    print('Identified lower outliers: %d' % len(outliers_lower)) # printing total number of values in lower cut of outliers\n    print('Identified upper outliers: %d' % len(outliers_higher)) # printing total number of values in higher cut of outliers\n    print('Total outliers: %d' % len(outliers_total)) # printing total number of values outliers of both sides\n    print('Non-outlier observations: %d' % len(outliers_removed)) # printing total number of non outlier values\n    print(\"Total percentage of Outliers: \", round((len(outliers_total) \/ len(outliers_removed) )*100, 4)) # Percentual of outliers in points\n    \n    if verbose:\n        print('\\nVerbose: Printing outliers')\n        if len(outliers_lower) > 0:\n            print(f'Lower outliers: {outliers_lower}')\n            \n        if len(outliers_higher) > 0:\n            print(f'Upper outliers: {outliers_higher}')\n    \n    return\n\nexplore_outliers(df.SalePrice, 5, True)","c36e7486":"# But first, let's remove the feature created using target variable and split train\/test\ndf.drop('N_priceLbl', axis=1, inplace=True)\n\n# Also, perform logarithmic transformation on target. Why? because we're interested in \n#..relative differences in prices and this normalizes the skew. \n# Read more here: https:\/\/stats.stackexchange.com\/a\/48465\/236332\n# and here: https:\/\/towardsdatascience.com\/why-take-the-log-of-a-continuous-target-variable-1ca0069ee935\nY = np.log(df.loc[:, 'SalePrice'] + 1)#.apply(lambda y: )\n# Y.fillna(-1)\ndf.drop('SalePrice', axis=1, inplace=True)\n\n# Also drop 'order' and 'PID' as they're just record identifiers\ndf.drop(['Order', 'PID'], axis=1, inplace=True)","0bc4bf67":"Y.plot.hist()","1aee09d1":"# Let's Label encode all categorical variables\nfor c in df.columns:\n    df[c]=df[c].fillna(-1) # Imp. for both encoder and regressor. They don't like NaNs.\n    if df[c].dtype == 'object':\n        le = preprocessing.LabelEncoder()\n        df[c] = le.fit_transform(df[c].astype('str')) # https:\/\/stackoverflow.com\/a\/46406995\/1578274","e336084e":"# Split into train\/test (80% training, 20% testing)\nX_train, X_test, Y_train, Y_test = train_test_split(df, Y, test_size=0.20)\nX_train.shape, X_test.shape, Y_train.shape, Y_test.shape","43a95f75":"# Linear Regression\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error as mse\nreg = linear_model.LinearRegression()\nreg.fit(X_train, Y_train)\nlr_pred = reg.predict(X_test)","40355784":"mse(Y_test, lr_pred)","75682bbd":"# Ridge regression using CV\nreg = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0], cv=3)\nreg.fit(X_train, Y_train)\nlr_pred = reg.predict(X_test)\nmse(Y_test, lr_pred)","6c894fb8":"# ref: https:\/\/www.kaggle.com\/nikunjm88\/creating-additional-features\/data\nxgb_params = {\n    'eta': 0.05,\n    'max_depth': 8,\n    'subsample': 0.7,\n    'colsample_bytree': 0.7,\n    'objective': 'reg:linear',\n    'silent': 1,\n    'seed' : 0\n}\n\ndtrain = xgb.DMatrix(X_train, Y_train, feature_names=X_train.columns.values)\nmodel = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=150)\n\n# plot the important features\nfig, ax = plt.subplots(figsize=(12,18))\nxgb.plot_importance(model, max_num_features=30, height=0.8, ax=ax)\nplt.show()","88f9576a":"from sklearn.feature_selection import SelectPercentile, f_classif\n\nX_indices = np.arange(X_train.shape[-1])\nselector = SelectPercentile(f_classif, percentile=10)\nselector.fit(X_train, Y_train)\nscores = -np.log10(selector.pvalues_)\nscores \/= scores.max()","8a3bdb5a":"top_k = 30\ntop_cols = df.columns[np.argsort(scores)][-top_k:]\ntop_scores = np.sort(scores)[-top_k:]\n\nind = np.arange(top_cols.shape[0])\nwidth = 0.2\nfig, ax = plt.subplots(figsize=(12,15))\nrects = ax.barh(ind, top_scores, color='darkorange',\n        edgecolor='black')\nax.set_yticks(ind)\nax.set_yticklabels(top_cols, rotation='horizontal')\nax.set_xlabel(r'Univariate score ($-Log(p_{value})$)')\nax.set_title(\"Feature importance using SelectPercentile\")\n\n# Add value labels\nfor i, v in enumerate(top_scores):\n    ax.text(v+0.01, i, str(round(v, 4)), color='k')\nplt.show()","b4558737":"# Split into train\/test\nX_train, X_test, Y_train, Y_test = train_test_split(df.loc[:,top_cols], Y, test_size=0.20)\nX_train.shape, X_test.shape, Y_train.shape, Y_test.shape","f075ec24":"from sklearn.neighbors import KNeighborsRegressor\nreg = KNeighborsRegressor(n_neighbors=5)\nreg.fit(X_train, Y_train)\nlr_pred = reg.predict(X_test)\nmse(Y_test, lr_pred)","eed4b306":"reg = linear_model.LinearRegression()\nreg.fit(X_train, Y_train)\nlr_pred = reg.predict(X_test)\nmse(Y_test, lr_pred)","15d2d34a":"reg = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0], cv=3)\nreg.fit(X_train, Y_train)\nlr_pred = reg.predict(X_test)\nmse(Y_test, lr_pred)","86ca2cbc":"#### Let's pick some correlated features and further visualize the patterns using pair-plot with density.","20e30ebb":"# Data Science on Housing Prices","6b26e379":"#### Some interesting results can be observed from the density and corr plots: \n- Most recently built houses fall in higher price bracket.\n- Most houses with enclosed porch fall in lower price.\n- Higher price bracket has larger variance than medium or lower.\n\nThe number of enclosed porches are negatively correlated with year built. It seems that potential housebuyers do not want an enclosed porch and house developers have been building less enclosed porches in recent years. It is also negatively correlated with SalePrice, which makes sense.\n\nThere is some slight negative correlation between OverallCond and SalePrice. There is also strong negative correlation between Yearbuilt and OverallCond. It seems to be that recently built houses tend to been in worse Overall Condition.","d231fba3":"Pretty much the same RMSE.\n\n### Let us find feature importances using XGBoost and auto-select best features using SelectPercentile","16b5723b":"Seems RidgeCV with all features is performing better with MSE of 0.0172 so far however, using just 30 features seem to capture most of the variance in data.","62d7bc53":"## Start by importing the required libraries","75cd0d89":"#### Let us now define a helper function <i>expandHead()<\/i> to take a quick peek at the dataset.","508041bc":"<b>Interesting insight!<\/b> StoneBr, NridgeHt, and NoRidge seem to have most of the expensive houses. <br \/>\nBrkSide, OldTown, IDOTRR, MeadowV have the cheapest houses.\n\nThis <b>BoxPlot<\/b> also helps highlight the spread of the prices, eg. North Ridge seems to have some serious outliers - may be a Mansion?","c1bce092":"## Load and understand the data\nWe begin by loading our data, which is stored in tab-separated value (TSV) format. For that, we use the CSV reader from Pandas with tab as separator, which creates a Pandas DataFrame containing the dataset.\n\n### Steps for data exploration\n1. Take a quick peek at the data and data-types.\n2. Separate numerical features to perform correlation analysis and basic statistics.\n3. Bucketed pair-plot analysis of highly related features to study their distribution and infer findings.\n4. Identifying missing values and imputation strategies.\n5. Studying frequency and sales prices by neighborhoods. \n6. Studying sales by year using Plotly's interactive graphs.\n7. Identifying skew in Sales Price using Moments analysis and exploring outliers using Standard Deviation. \n8. Normalizing dependent variable, labelizing categoricals, removing extraneous cols.\n9. Prepping for Machine Learning.\n\n### Machine Learning\n1. Train baseline linear regressor.\n2. Demonstrate K-Fold Cross Validation. \n3. Determing feature importances using ensemble techniques (eg. XGBoost).\n4. Feature selection using XGBoost.\n5. Reiterate training and compare performance metric (RMSE).\n","67da8097":"### Split dataset for training, and testing\nTesting set is also canonically known as the 'held-out' set which is used only to evaluate the trained model at the end. This <b>must never<\/b> be used during training.","61c00cf2":"Slightly different results than XGBoost. OverallQual made it to the top this time. <br \/>\nSurprisingly, Street and External Quality are in the top three features!\n\nBut overall, the top 30 features look pretty much the same. <br \/>\nLet's use these features to test our models again and observe the difference, if any.","696c0312":"Linear Regression comparison","96a5eea8":"#### Sale prices seems to be centered around the 150k mark.","ab75d483":"Quite a few variables are positively correlated with SalePrice, eg. OverallQual, YearBuilt, etc.","e22ffd98":"## Thank you for taking the time\nThis work is still in progress and refinements will continue. In the meanwhile..\n- Didn't understand something? Ask a question.\n- Something could've been better? Share your feedback.\n- Found the notebook useful? Please share your vote!\n\n#### Used the Ames Housing Data:\n- Dean De Cock Truman State University Journal of Statistics Education Volume 19, Number 3(2011), www.amstat.org\/publications\/jse\/v19n3\/decock.pdf","e3241c5e":"### Missing value analysis\n\nViewing the data indicates that there are columns which have missing values. The categorical variables with the largest number of missing values are: Alley, FirePlaceQu, PoolQC, Fence, and MiscFeature.\n\n- Alley: indicates the type of alley access\n- FirePlaceQu: FirePlace Quality\n- PoolQC: Pool Quality\n- Fence: Fence Quality\n- MiscFeature: Miscellaneous features not covered in other categories\n\nThe missing values indicate that majority of the houses do not have alley access, no pool, no fence and no elevator, 2nd garage, shed or tennis court that is covered by the MiscFeature.\n\nThe numeric variables do not have as many missing values but there are still some present. There are 490 values for the LotFrontage, 23 missing values for MasVnrArea and 159 missing values for GarageYrBlt.\n\n- LotFrontage: Linear feet of street connected to property\n- GarageYrBlt: Year garage was built\n- MasVnrArea: Masonry veener area in square feet","8ff5522f":"Seems worse than the baseline. Let's check RidgeCV again","da2608c2":"Sometimes it helps to check for 'constant' variables - we don't need them. ","879e3bb2":"Let's take a quick look at the missing values again","a70d6353":"### Which neighborhoods have most houses?","3505d944":"#### So we have 2930 sales records with 82 features (including target - SalePrice).","ebcee8ce":"Sale prices seems to be centered around the 150k mark.\n\n### Let us perform the **moment** analysis to understand the spread of data\nUpto abs 0.5 skewness is fairly symmetrical\n","9013a0b7":"Ignoring the SalePrice variable, it seems *Garage Yr Blt* and *Year Built* are strongly correlated. We'll keep one with lesser missing values.\n*Garage Cars* - *Garage Area* and *TotRms AbvGrd* - *Gr liv Area* are related as well, but let's keep them for now as they represent different values.\n\n### Let's plot missing values","4f6461f4":"Let's check the proportion of missing values. We will drop more than 95% missing features after checking their importance later.","02486a0e":"<b>Let's bucket SalePrice<\/b> into three qualitative categories, based on IQR, and generate a new feature for further analysis. Buckets High, Low, Medium help us identify the distribution of different categories per price ranges.","fa522054":"#### Let's compare some Regression performances.\nTrain a baseline regressor","1002ae94":"### How expensive are the houses?","f27e1306":"### ---","3cdebe02":"There seems to be some inconsistency between GarageArea and other Garage* features..\nthat have missing values where area is present.\nLet us explore this.\n","6026500e":"Hopefully, that normalized the skew. Let's do a quick sanity check\/viz.","70c9fcca":"# Machine Learning\n\n## Make predictions, and evaluate results\nOur final step will be to use our fitted model to make predictions on new data. We will use our held-out test set, but you could also use this model to make predictions on completely new data. For example, if we created some features data based on a different State, we could predict House Prices expected in that region!\n\nWe will also evaluate our predictions. Computing evaluation metrics is important for understanding the quality of predictions, as well as for comparing models and tuning parameters.","a42bb087":"Per the Data Dictionary, 'The data has 82 columns which include 23 nominal, 23 ordinal, 14 discrete, and 20 continuous variables (and 2 additional observation identifiers)'.\n\n#### Let's separate numerical features (includes bool).","a00a3e57":"Interestingly, LotFrontage makes it to the top of the list, which was not apparent from the corr matrix before. <br \/>\nOn the contrary, OverallQual lands at position 12.","889058a7":"### Label encode categorical variables\nMost ML algorithms don't fair well with categorical\/string data. We must labelize or binarize them before feeding to the models.","9bd34dec":"A high kurtosis is a strong indicator of outliers, so is the positive skew.\n\n### Let us explore and remove the outliers per the target variable.","7810b385":"# Improving our model\nWe are not done yet! This section describes how to take this notebook and improve the results even more. Try forking this kernel and extending it. See how much you can improve the predictions (or lower RMSE).\n\nThere are several ways we could further improve our model:\n\n- <b>Expert knowledge:<\/b> We may not be experts on the Housing industry, but we know a few things we can use: The property hasn't been remodelled if the Year Remodelled is same is Year Built. Regressors does not know that, but we could create a new boolean feature indicating whether or not the house was remodelled.\n- <b>Better tuning:<\/b> To make this notebook run quickly, we only tried a few hyperparameter settings. To get the most out of our data, we should test more settings. Start by increasing the number of trees in our XGBoost model by setting max_depth=100, or RidgeCV k-fold to 5 or more; it will take longer to train but can be more accurate.\n- <b>Feature engineering:<\/b> We used the basic set of features given to us, but we could potentially improve them, as indicated in the first point here. For example, we may guess that an area is more or less important depending on whether or not it is closer to the city\/downtown using spatial info.\n- <b>Exploring more complex models:<\/b> While respecting Occam's Razor, we should explore more sophisticated models, eg. Decision Tree Regressors, or even Neural Networks and compare their performances with baselines.\n\nGood luck!","3a156041":"#### Let's see if using nearby houses (data points) bring any positive improvement in the regression.","30447885":"This confirms our hunch that there is actually no inconsistency, as those missing garages have 0 area.\n\n### Imputing missing values\nLet us impute the 1-2 missing values features with corresponding default values.","100b59f2":"#### Let's see if SelectPercentile tells a similar story","bdbdf7ab":"## Analyzing a Housing dataset\nThis Python notebook demonstrates a step-by-step Data Science experiment using tools like Numpy and Pandas to preprocess and analyze the dataset, using statistical techniques, and Sklearn for Machine Learning to predict Housing Prices. \n\n<b>Data:<\/b> Data set contains information from the Ames Assessor's Office used in computing assessed values for individual residential properties sold in Ames, IA from 2006 to 2010. See attached <i>DataDictionary_AmesHousing.txt<\/i>  for more info. \n\n<b>Goal:<\/b> We want to learn and predict housing prices from information such as Neighborhood, Year Built, Interior and Exterior specifications, overall condition, etc. Having good prediction of housing prices enables both sellers and buyers to make informed decisions when choosing to sell or buy a house.\n\n<b>Approach:<\/b> We will start by loding and preprocessing the data using Pandas and Numpy. In-depth statistical analysis, data visualization, and feature engineering will be demonstrated that will help the users to approach similar Data Science problems and a structured way. We will then explore several Machine Learning methods to build regression models and compare prediction performance.\n\n","2ae030b5":"### Interactive graph\nLet's see number of Sales by month and year.","660ddb9a":"We should probably remove these outliers, more than 5 SD away. <br \/>\nWill leave it for later.","b2ff25f8":"Interesting. For some reason, the sales tend to be centered around mid year (June); Perhaps the best time to sell your house,\nand Dec\/Jan to buy!","a919a060":"### Let's cross SalePrice with Neighborhood to analyze price variance by region"}}