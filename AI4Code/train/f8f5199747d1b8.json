{"cell_type":{"91a19969":"code","7d7dc7cd":"code","e779e4f9":"code","9fbedb65":"code","0a6d3643":"code","cd5d0964":"code","c1930c92":"code","a1a9d124":"code","7958fb2a":"code","a918b707":"code","1635b053":"code","ead536de":"code","eee39fec":"code","c71ff9fe":"code","5787ab82":"code","a53d51bd":"code","824a4c08":"code","84b5b1e2":"code","71000f3e":"code","b4b99b8f":"code","b205f959":"code","938a9b3a":"code","ff1cc109":"code","00f4813f":"code","14145eef":"code","a4f3333a":"code","324cfda0":"code","b606a190":"code","170e89b5":"code","55923511":"code","1e3fd350":"code","2587586c":"code","b676b49d":"code","bfbbc0de":"code","97f03f5e":"code","e4f1dd70":"code","e8cf2cca":"code","5179fa1a":"code","5d5077ed":"code","461a3a1b":"code","e451a3a9":"code","24211c4b":"code","8a2ab634":"code","df71f311":"code","8d8788b1":"markdown","4673e516":"markdown","1b9a47ab":"markdown","01cb178c":"markdown","416d19f9":"markdown","552d4a75":"markdown","14871e2b":"markdown","55d68f20":"markdown","a6053cca":"markdown","be89e39c":"markdown","8b9c5926":"markdown","da1cabd2":"markdown","f8313e06":"markdown","8ac186ad":"markdown","8bdcb332":"markdown","8e22c0d4":"markdown","c6eacaeb":"markdown"},"source":{"91a19969":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nwarnings.filterwarnings('ignore')\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7d7dc7cd":"df = pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')","e779e4f9":"df.head()","9fbedb65":"df.shape","0a6d3643":"df.info()","cd5d0964":"df.describe().T","c1930c92":"# value cannot be 0 for these columns\ncols = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin','BMI']","a1a9d124":"# replacing 0 with null values\nfor x in cols:\n    df[x] = df[x].where(df[x]!=0,np.nan)","7958fb2a":"df.describe().T","a918b707":"df.isna().sum().sort_values(ascending=False)","1635b053":"# calculating age group wise mean for Insulin,SkinThickness,BloodPressure,BMI\ndef compute_agewise(col):\n    \n    # age 21 to 35, replace null values with mean \n    df[col].loc[df[(df[col].isna()==True)&((df.Age>20)&(df.Age<=35))].index] = \\\n                            round(df[(df.Age>20)&(df.Age<=35)][col].mean(),1)\n    \n    # age 36 to 50, replace null values with mean \n    df[col].loc[df[(df[col].isna()==True)&((df.Age>35)&(df.Age<=50))].index] = \\\n                            round(df[(df.Age>35)&(df.Age<=50)][col].mean(),1)\n    \n    # age 51 to 70, replace null values with mean \n    df[col].loc[df[(df[col].isna()==True)&((df.Age>50)&(df.Age<=70))].index] = \\\n                            round(df[(df.Age>50)&(df.Age<=70)][col].mean(),1)\n    \n    # age geater than 71, replace null values with mean \n    df[col].loc[df[(df[col].isna()==True)&(df.Age>70)].index] = \\\n                            round(df[df.Age>70][col].mean(),1)\ncompute_agewise('Insulin')\ncompute_agewise('SkinThickness')\ncompute_agewise('BloodPressure')\ncompute_agewise('BMI')","ead536de":"# Replacing with mean\ndf.Glucose.fillna(round(df.Glucose.mean(),1),inplace=True)","eee39fec":"df.isna().sum().sort_values(ascending=False)","c71ff9fe":"for x in df.columns:\n    sns.boxplot(y=df[x])\n    plt.show()","5787ab82":"X = df.drop('Outcome',axis=1)\nX","a53d51bd":"Y = df.Outcome\nY","824a4c08":"from sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\nscaler.fit(X)\nX=scaler.transform(X) \nprint(X)","84b5b1e2":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, \nrandom_state=10)","71000f3e":"from sklearn.linear_model import LogisticRegression\nlr=LogisticRegression()\nlr.fit(X_train, Y_train)\nY_pred=lr.predict(X_test)","b4b99b8f":"from sklearn.metrics import confusion_matrix,accuracy_score,classification_report,precision_score,recall_score,roc_curve,roc_auc_score\n\ncfm=confusion_matrix(Y_test,Y_pred)\n\nprint(cfm)\n\nprint(\"Classification Report\")\n\nprint(classification_report(Y_test,Y_pred))\n\nlracc=round(accuracy_score(Y_test,Y_pred),2)\nlrrecall = round(recall_score(Y_test,Y_pred),2)\nlrprec = round(precision_score(Y_test,Y_pred),2)\n\nprint('Accuracy:',lracc,'Recall:',lrrecall,'Precision:',lrprec)","b205f959":"# predict probabilities\nlrprob = lr.predict_proba(X_test)\n# roc curve\nlr_fpr, lr_tpr, lr_thresh = roc_curve(Y_test, lrprob[:,1], pos_label=1)\n\n# roc curve for tpr = fpr \nrandom_probs = [0 for i in range(len(Y_test))]\np_fpr, p_tpr, _ = roc_curve(Y_test, random_probs, pos_label=1)\n\n# auc scores\nlr_auc_score = round(roc_auc_score(Y_test, lrprob[:,1]),2)\nprint('AUC Score:',lr_auc_score)","938a9b3a":"#plot roc curve\nplt.plot(lr_fpr, lr_tpr, linestyle='--',color='orange', label='Logistic Regression')\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n# title\nplt.title('ROC curve')\n# x label\nplt.xlabel('False Positive Rate')\n# y label\nplt.ylabel('True Positive rate')\nplt.show()","ff1cc109":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(X_train, Y_train)\nY_pred=dt.predict(X_test)","00f4813f":"cfm=confusion_matrix(Y_test,Y_pred)\n\nprint(cfm)\n\nprint(\"Classification Report\")\n\nprint(classification_report(Y_test,Y_pred))\n\ndtacc=round(accuracy_score(Y_test,Y_pred),2)\ndtrecall = round(recall_score(Y_test,Y_pred),2)\ndtprec = round(precision_score(Y_test,Y_pred),2)\n\nprint('Accuracy:',dtacc,'Recall:',dtrecall,'Precision:',dtprec)","14145eef":"# predict probabilities\ndtprob = dt.predict_proba(X_test)\n# roc curve\ndt_fpr, dt_tpr, dt_thresh = roc_curve(Y_test, dtprob[:,1], pos_label=1)\n\n# roc curve for tpr = fpr \nrandom_probs = [0 for i in range(len(Y_test))]\np_fpr, p_tpr, _ = roc_curve(Y_test, random_probs, pos_label=1)\n\n# auc scores\ndt_auc_score = round(roc_auc_score(Y_test, dtprob[:,1]),2)\nprint('AUC Score:',dt_auc_score)\n\n\n#plot roc curve\nplt.plot(dt_fpr, dt_tpr, linestyle='--',color='orange', label='Decision Tree')\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n# title\nplt.title('ROC curve')\n# x label\nplt.xlabel('False Positive Rate')\n# y label\nplt.ylabel('True Positive rate')\nplt.show()","a4f3333a":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nrf.fit(X_train, Y_train)\nY_pred=rf.predict(X_test)","324cfda0":"cfm=confusion_matrix(Y_test,Y_pred)\n\nprint(cfm)\n\nprint(\"Classification Report\")\n\nprint(classification_report(Y_test,Y_pred))\n\nrfacc=round(accuracy_score(Y_test,Y_pred),2)\nrfrecall = round(recall_score(Y_test,Y_pred),2)\nrfprec = round(precision_score(Y_test,Y_pred),2)\n\nprint('Accuracy:',rfacc,'Recall:',rfrecall,'Precision:',rfprec)","b606a190":"# predict probabilities\nrfprob = rf.predict_proba(X_test)\n# roc curve\nrf_fpr, rf_tpr, rf_thresh = roc_curve(Y_test, rfprob[:,1], pos_label=1)\n\n# roc curve for tpr = fpr \nrandom_probs = [0 for i in range(len(Y_test))]\np_fpr, p_tpr, _ = roc_curve(Y_test, random_probs, pos_label=1)\n\n# auc scores\nrf_auc_score = round(roc_auc_score(Y_test, rfprob[:,1]),2)\nprint('AUC Score:',rf_auc_score)\n\n\n#plot roc curve\nplt.plot(rf_fpr, rf_tpr, linestyle='--',color='orange', label='Random Forest')\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n# title\nplt.title('ROC curve')\n# x label\nplt.xlabel('False Positive Rate')\n# y label\nplt.ylabel('True Positive rate')\nplt.show()","170e89b5":"from sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier()\ngb.fit(X_train,Y_train)\nY_pred = gb.predict(X_test)","55923511":"cfm=confusion_matrix(Y_test,Y_pred)\n\nprint(cfm)\n\nprint(\"Classification Report\")\n\nprint(classification_report(Y_test,Y_pred))\n\ngbacc=round(accuracy_score(Y_test,Y_pred),2)\ngbrecall = round(recall_score(Y_test,Y_pred),2)\ngbprec = round(precision_score(Y_test,Y_pred),2)\n\nprint('Accuracy:',gbacc,'Recall:',gbrecall,'Precision:',gbprec)","1e3fd350":"# predict probabilities\ngbprob = gb.predict_proba(X_test)\n# roc curve\ngb_fpr, gb_tpr, gb_thresh = roc_curve(Y_test, gbprob[:,1], pos_label=1)\n\n# roc curve for tpr = fpr \nrandom_probs = [0 for i in range(len(Y_test))]\np_fpr, p_tpr, _ = roc_curve(Y_test, random_probs, pos_label=1)\n\n# auc scores\ngb_auc_score = round(roc_auc_score(Y_test, gbprob[:,1]),2)\nprint('AUC Score:',gb_auc_score)\n\n\n#plot roc curve\nplt.plot(rf_fpr, rf_tpr, linestyle='--',color='orange', label='Gradient Boosting')\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n# title\nplt.title('ROC curve')\n# x label\nplt.xlabel('False Positive Rate')\n# y label\nplt.ylabel('True Positive rate')\nplt.show()","2587586c":"from xgboost import XGBClassifier\nxg = XGBClassifier()\nxg.fit(X_train,Y_train)\nY_pred = xg.predict(X_test)","b676b49d":"cfm=confusion_matrix(Y_test,Y_pred)\n\nprint(cfm)\n\nprint(\"Classification Report\")\n\nprint(classification_report(Y_test,Y_pred))\n\nxgacc=round(accuracy_score(Y_test,Y_pred),2)\nxgrecall = round(recall_score(Y_test,Y_pred),2)\nxgprec = round(precision_score(Y_test,Y_pred),2)\n\nprint('Accuracy:',xgacc,'Recall:',xgrecall,'Precision:',xgprec)","bfbbc0de":"# predict probabilities\nxgprob = xg.predict_proba(X_test)\n# roc curve\nxg_fpr, xg_tpr, xg_thresh = roc_curve(Y_test, xgprob[:,1], pos_label=1)\n\n# roc curve for tpr = fpr \nrandom_probs = [0 for i in range(len(Y_test))]\np_fpr, p_tpr, _ = roc_curve(Y_test, random_probs, pos_label=1)\n\n# auc scores\nxg_auc_score = round(roc_auc_score(Y_test, xgprob[:,1]),2)\nprint('AUC Score:',xg_auc_score)\n\n\n#plot roc curve\nplt.plot(rf_fpr, rf_tpr, linestyle='--',color='orange', label='XG Boost')\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n# title\nplt.title('ROC curve')\n# x label\nplt.xlabel('False Positive Rate')\n# y label\nplt.ylabel('True Positive rate')\nplt.show()","97f03f5e":"from sklearn.ensemble import VotingClassifier\n\n# create sub models\nestimators = []\n\nmodel1 = DecisionTreeClassifier()\nestimators.append(('dt',model1))\nmodel2 = GradientBoostingClassifier()\nestimators.append(('gb',model2))\nmodel3 = XGBClassifier()\nestimators.append(('xgb',model3))\n\n# create the ensemble model\nensemble = VotingClassifier(estimators,voting='soft')\nensemble.fit(X_train,Y_train)\nY_pred = ensemble.predict(X_test)","e4f1dd70":"cfm=confusion_matrix(Y_test,Y_pred)\n\nprint(cfm)\n\nprint(\"Classification Report\")\n\nprint(classification_report(Y_test,Y_pred))\n\nvcacc=round(accuracy_score(Y_test,Y_pred),2)\nvcrecall = round(recall_score(Y_test,Y_pred),2)\nvcprec = round(precision_score(Y_test,Y_pred),2)\n\nprint('Accuracy:',vcacc,'Recall:',vcrecall,'Precision:',vcprec)","e8cf2cca":"# predict probabilities\nvcprob = ensemble.predict_proba(X_test)\n# roc curve\nvc_fpr, vc_tpr, vc_thresh = roc_curve(Y_test, vcprob[:,1], pos_label=1)\n\n# roc curve for tpr = fpr \nrandom_probs = [0 for i in range(len(Y_test))]\np_fpr, p_tpr, _ = roc_curve(Y_test, random_probs, pos_label=1)\n\n# auc scores\nvc_auc_score = round(roc_auc_score(Y_test, vcprob[:,1]),2)\nprint('AUC Score:',vc_auc_score)\n\n\n#plot roc curve\nplt.plot(rf_fpr, rf_tpr, linestyle='--',color='orange', label='Voting Classifier')\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n# title\nplt.title('ROC curve')\n# x label\nplt.xlabel('False Positive Rate')\n# y label\nplt.ylabel('True Positive rate')\nplt.show()","5179fa1a":"result = pd.DataFrame({'Accuracy':[lracc,dtacc,rfacc,gbacc,xgacc,vcacc],\n                         'Recall':[lrrecall,dtrecall,rfrecall,gbrecall,xgrecall,vcrecall],\n                         'Precision':[lrprec,dtprec,rfprec,gbprec,xgprec,vcprec],\n                         'Auc':[lr_auc_score,dt_auc_score,rf_auc_score,gb_auc_score,xg_auc_score,vc_auc_score]},\n                      \n                        index=['Logistic Regression','Decision Tree','Random Forest','Gradient Boosting','XG Boost','Voting Classifier'])","5d5077ed":"result","461a3a1b":"sns.barplot(y = result.Accuracy.sort_values(ascending=False).index,\n           x = result.Accuracy.sort_values(ascending=False))","e451a3a9":"sns.barplot(y = result.Precision.sort_values(ascending=False).index,\n           x = result.Precision.sort_values(ascending=False))","24211c4b":"sns.barplot(y = result.Recall.sort_values(ascending=False).index,\n           x = result.Recall.sort_values(ascending=False))","8a2ab634":"sns.barplot(y = result.Auc.sort_values(ascending=False).index,\n           x = result.Auc.sort_values(ascending=False))","df71f311":"plt.plot(lr_fpr, lr_tpr, linestyle='--',color='blue', label='Logistic Regression')\nplt.plot(dt_fpr, dt_tpr, linestyle='--',color='brown', label='Decision Tree')\nplt.plot(rf_fpr, rf_tpr, linestyle='--',color='green', label='Random Forest')\nplt.plot(rf_fpr, rf_tpr, linestyle='-.',color='orange', label='Gradient Boosting')\nplt.plot(rf_fpr, rf_tpr, linestyle='--',color='purple', label='XG Boost')\nplt.plot(rf_fpr, rf_tpr, linestyle='--',color='red', label='Voting Classifier')\nplt.plot(p_fpr, p_tpr, linestyle='--', color='yellow')\n\n# title\nplt.title('ROC curve')\n# x label\nplt.xlabel('False Positive Rate')\n# y label\nplt.ylabel('True Positive rate')\nplt.legend(loc='best')\nplt.show()","8d8788b1":"# Random Forest","4673e516":"## Recall","1b9a47ab":"No outliers, all data points looks in clusters ","01cb178c":"# Scaling","416d19f9":"# Logistic Regression","552d4a75":"## Precision","14871e2b":"No outliers, all data points looks in clusters # Checking Outliers","55d68f20":"## Accuracy","a6053cca":"# CHecking missing values","be89e39c":"# XG Boost","8b9c5926":"# Decision Tree","da1cabd2":"# Gradient Boosting","f8313e06":"# Comparing Models","8ac186ad":"# Model Building","8bdcb332":"# Data Inspection","8e22c0d4":"# Voting Classifier","c6eacaeb":"## AUC Score"}}