{"cell_type":{"21ce0941":"code","b189d828":"code","d1d2dca5":"code","b0b99c41":"code","999bde0e":"code","31082d74":"code","544d8869":"code","fba1cb7f":"code","6442e909":"code","ecffe05c":"code","366177cf":"code","db74a6ab":"code","336e606a":"code","1bbcda6f":"code","598379d6":"code","4752d9bf":"code","0026e8cf":"code","a2eddeaf":"code","4319a03a":"code","a04e1703":"code","bff04166":"code","f750a2a5":"code","78f7e6e9":"code","957e258d":"code","6df427f6":"code","02d9eed1":"code","39c1c84a":"code","a487ede5":"code","d6a880d7":"code","9a120626":"code","24ecebff":"code","53012169":"code","2501f4cf":"code","41851d9c":"code","e0c8117a":"markdown","af5f6682":"markdown","b55b6929":"markdown","4ca80146":"markdown","05e86b3d":"markdown","9dae1214":"markdown","3dd4999e":"markdown"},"source":{"21ce0941":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.losses import MeanSquaredError\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","b189d828":"df=pd.read_csv('..\/input\/g-research-crypto-forecasting\/train.csv')","d1d2dca5":"df=df.drop(['timestamp'],axis=1)\ndf.replace([np.inf, -np.inf], np.nan, inplace=True)","b0b99c41":"df.isna().value_counts()","999bde0e":"na_idx=np.unique(list(df.Target[df.Target.isna()==True].index)+list(df.VWAP[df.VWAP.isna()==True].index))","31082d74":"df=df.drop(na_idx.tolist())","544d8869":"df.Target.isna().value_counts()","fba1cb7f":"df.Asset_ID.unique().shape[0]","6442e909":"for i in range(14):\n    plt.plot(range(50),df[df.Asset_ID==i][:50].Target,label=f'Asset:{i}')\n    \nplt.legend()\n\nplt.show()","ecffe05c":"dfs={}\n\nfor i in range(14):\n    dfs[i]=(df[df.Asset_ID==i].drop(['Target','Asset_ID'],axis=1),df[df.Asset_ID==i].Target)","366177cf":"lengths=[]\n\nfor k,v in dfs.items():\n    print(f'length of asset {k} : ',v[0].shape[0])\n    \n    lengths.append(v[0].shape[0])","db74a6ab":"max_len=np.max(lengths)","336e606a":"#this operation is very memory intensive \n\nx=[]\ny=[]\n\nfor k,v in dfs.items():\n    print(k)\n    x.append(np.array(v[0]))\n    y.append(np.array(v[1]))","1bbcda6f":"#this operation is very memory intensive \nx=pad_sequences(x,dtype='float32')","598379d6":"x.shape #14 subbatch , 1955978 sequence length , each have 8 dimension","4752d9bf":"y=pad_sequences(y,dtype='float32')","0026e8cf":"y.shape #14 subbatch, 1955978 sequence length","a2eddeaf":"x.shape","4319a03a":"x=np.transpose(x,[1,0,2])\ny=np.transpose(y,[1,0])","a04e1703":"x.shape,y.shape","bff04166":"mmin=x.min(axis=0)\nmmax=x.max(axis=0)","f750a2a5":"def norm(x):\n    return (x-mmin)\/(mmax-mmin+1)","78f7e6e9":"mmin.shape,mmax.shape","957e258d":"def build_ds(X,y,time_slice,batch_size):\n    \n    ds_X=tf.keras.preprocessing.timeseries_dataset_from_array(\n      data=X,\n      targets=tf.ones((X.shape[0])),\n      sequence_length=time_slice,\n      sequence_stride=time_slice,\n      shuffle=False,\n      batch_size=batch_size\n    ).map(lambda x,y : x)\n\n    ds_Y=tf.keras.preprocessing.timeseries_dataset_from_array(\n      data=y,\n      targets=tf.ones((y.shape[0])),\n      sequence_length=time_slice,\n      sequence_stride=time_slice,\n      shuffle=False,\n      batch_size=batch_size\n    ).map(lambda x,y : x)\n    ds=tf.data.Dataset.zip((ds_X,ds_Y))\n    ds=ds.map(lambda x,y:(norm(x),y)) #broadcast\n    ds=ds.map(lambda x,y:(tf.transpose(x,perm=[0,2,1,3]),tf.transpose(y,perm=[0,2,1])))\n    ds=ds.prefetch(tf.data.experimental.AUTOTUNE)\n    return ds","6df427f6":"time_slice=1024\nbatch_size=128\n\nepochs=3\nunits=512\nlr=0.0001","02d9eed1":"ds=build_ds(x,y,time_slice,batch_size)","39c1c84a":"for seq,label in ds:\n    print(seq.shape,label.shape)  #x:(batch,seq length, sub batch ,dim)\n    break","a487ede5":"#check operation bug\n\ntf.reduce_sum(tf.cast(tf.math.is_nan(seq),'float32'))","d6a880d7":"class Model(tf.keras.Model):\n    def __init__(self,units,batch_size=14,dim=7):\n        super().__init__()\n        self.batch_size=batch_size\n        self.units=units\n        self.mask=layers.Masking(batch_input_shape=(batch_size, time_slice,dim))\n        self.lstm=layers.LSTM(units=units,\n                            return_sequences=True,\n                            return_state=True,\n                            dropout=0.3)\n        self.d1=layers.Dense(units=int(units\/2),activation='relu')\n        self.d2=layers.Dense(units=1,activation=None)\n        \n    def call(self,x,hidden=None,training=False):\n        x=self.mask(x)\n        if hidden==None:\n            x, h, c=self.lstm(x,initial_state =self.initialize_hidden_state(),training=training)\n        else:\n            x, h, c=self.lstm(x,initial_state=hidden,training=training)\n        x=self.d1(x)\n        x=self.d2(x)\n        return x,h, c\n    \n    \n    def  initialize_hidden_state(self):\n        return [tf.zeros((self.batch_size, self.units)), tf.zeros((self.batch_size, self.units))]\n        ","9a120626":"@tf.function\ndef flow(x,y,model,loss_func,opt,init=None,):\n    with tf.GradientTape() as tape:\n        y_pred,h,c=model(x,hidden=init,training=True)\n        loss=loss_func(y,y_pred)\n    grad=tape.gradient(loss,model.trainable_weights)\n    opt.apply_gradients(zip(grad,model.trainable_weights))\n    \n    return loss,h,c","24ecebff":"def train():\n    model=Model(units)\n    opt=tf.keras.optimizers.Adam(learning_rate=lr)\n    mse=MeanSquaredError()\n    ckpt = tf.train.Checkpoint(opt=opt, model=model)\n    manager = tf.train.CheckpointManager(ckpt, '.\/ckpt', max_to_keep=1)\n    \n    print('start training')\n    for epoch in range(epochs):\n        init=None\n        for i,(x,y) in enumerate(ds): #batch\n            for j,(seq,label) in enumerate(zip(x,y)):\n                label=tf.expand_dims(label,axis=-1)\n                loss,h,c=flow(seq,label,model,mse,opt,init=init)\n                init=[h,c]\n                if j%50==0:\n                    print(f'epoch :{epoch} , batch:{i} sub: {j} , loss={loss}')\n        manager.save()\n    return model","53012169":"model=train()","2501f4cf":"model.summary()","41851d9c":"import gresearch_crypto\n\nh,c=0,0\nenv = gresearch_crypto.make_env()  \niter_test = env.iter_test()    \nmodel.reset_states() #h0-->h1-->h2--->h3\nfor i,(test_df, sample_prediction_df) in enumerate(iter_test):\n    \n    test_df=test_df.drop(['timestamp','row_id','Asset_ID'],axis=1)\n    k=tf.reshape(test_df,(test_df.shape[0],1,test_df.shape[1]))\n    mmax_=tf.cast(tf.reshape(mmax,(14,1,7)),'float64')\n    mmin_=tf.cast(tf.reshape(mmin,(14,1,7)),'float64')\n    k=(k-mmin_)\/(mmax_-mmin_+1)\n    \n    y_pred,h,c=model(k,hidden=None if i==0 else [h,c],training=False)\n    sample_prediction_df['Target']=tf.cast(y_pred,'float64')[:,0,0]\n    env.predict(sample_prediction_df)   ","e0c8117a":"## Modeling","af5f6682":"* Hyperparameter","b55b6929":"## Submit","4ca80146":"* Model\n\n   * Stateful : Boolean (default False). If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch\n   \n   * If lstm is stateful , we need to specify batch input shape in the input layer","05e86b3d":"* End to End idea \n\n   * Since RNN pass zero initial hidden input ,doing preorder padding to the same length\n   \n   * Use asset_id to representation which asset \n   \n   * Each asset series size : (1,seq slice length , dim)\n   \n       * So total size will be : (14,   seq slice length , dim)\n       \n   * When Using lstm or gru , pass last hidden output to next initial hidden input to retain time dependency\n   ","9dae1214":"*  Predict each data once , passing hidden state to next batch","3dd4999e":"* There are 14 different pattern time series"}}