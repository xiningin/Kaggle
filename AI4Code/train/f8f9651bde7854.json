{"cell_type":{"02fadf4a":"code","ceb41cc3":"code","2f84aa34":"code","5d771d93":"code","4e1ee6b9":"code","25c391be":"code","f3e58287":"code","b90f814a":"markdown","699b07a6":"markdown","8ef90850":"markdown","b1e9b7d5":"markdown"},"source":{"02fadf4a":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.model_selection import train_test_split \nfrom sklearn import metrics\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import SGDClassifier\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))        \n","ceb41cc3":"# This is where I am loading the data\ntrain_df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ngender_df = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\n","2f84aa34":"# exploratory data analysis\n# First, lets breakdown our training set, and look at the data.\nprint(train_df.head())","5d771d93":"for column in train_df:\n    check_nan = train_df[column].isnull().values.any()\n    print(column, check_nan)\nprint(train_df.isnull().sum())","4e1ee6b9":"train_df['Age'].fillna(train_df['Age'].mean(), inplace=True)\ntrain_df['Embarked'].fillna(train_df['Embarked'].mode()[0], inplace=True)\n# Now I'm going to plot some columns to survival to try and visualize relationships\nplot1 = plt.figure(1)\nsns.violinplot(x=\"Survived\", y=\"Age\", data=train_df, linewidth=0)\nplot2 = plt.figure(2)\nsns.countplot(x=\"Survived\", hue=\"Sex\", data=train_df)\nplt.xticks([0,1], ['Dead', 'Survived'])\nplot3 = plt.figure(3)\nsns.countplot(x=\"Pclass\", hue=\"Survived\", data=train_df)\nplot4 = plt.figure(4)\nsns.countplot(x=\"Parch\", hue=\"Survived\", data=train_df)\nplot5 = plt.figure(5)\nsns.countplot(x=\"SibSp\", hue=\"Survived\", data=train_df)\nplt.show()","25c391be":"# I need to convert \"Sex\" column to a numerical value\ndef convert_to_number(some_sex):\n    if(some_sex == 'male'):\n        return 1\n    else:\n        return 2\n    \ntrain_df['Sex_num'] = train_df['Sex'].apply(convert_to_number)\ntest_df['Sex_num'] = test_df['Sex'].apply(convert_to_number)\n\n# Get X and y to split data\ncolumns = ['Age', 'Sex_num', 'Pclass', 'Parch', 'SibSp']\nX = train_df[columns]\ny = train_df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1111)\n\n# build classifier, train, predict, then score\nclf = DecisionTreeClassifier(max_depth=7)\nclf = clf.fit(X_train,y_train)\ny_pred = clf.predict(X_test)\n\nlin_reg = LinearRegression()\nlin_reg = lin_reg.fit(X_train, y_train)\ny_pred2 = lin_reg.predict(X_test)\n\nknn = KNeighborsClassifier(n_neighbors=6)\nknn.fit(X_train, y_train)\ny_pred3 = knn.predict(X_test)\n\nsgdc = SGDClassifier(max_iter=1000, tol=0.01)\nsgdc.fit(X_train, y_train)\ny_pred4 = sgdc.predict(X_test)\n\nprint(\"Decision Tree Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"R squared of linear regression: \", lin_reg.score(X_train, y_train))\nprint(\"KNN Accuracy:\",metrics.accuracy_score(y_test, y_pred3))\nprint(\"SGDClassifier Accuracy: \", metrics.accuracy_score(y_test, y_pred4))","f3e58287":"# The test set has missing values in the age column, so we can do something similar like before.\ntest_df['Age'].fillna(test_df['Age'].mean(), inplace=True)\n# get test data\ntest_data_X = test_df[columns]\n# make new predictions\ntest_pred = clf.predict(test_data_X)\n# turn predictions into dataframe\ntest_pred_df = pd.DataFrame(data=test_pred, columns=['Survived'], index=test_data_X.index.copy())\ntest_pred_df['PassengerId'] = test_df[\"PassengerId\"]\nprint(test_pred_df)\ntest_pred_df.to_csv('submission.csv', index=False)","b90f814a":"So far, we have trained our classifier on the training data. We were able to get some accuracies over 80%. \n- First, let's look at the R^2 of linear regression. A value of 0.35 shows some correlation, but it seems closer to 0 than it does 1. \n- The KNN provides probably the second best accuracy after several runs. \n- The SGD Classifier provides the lower accuracies of them all. \nNow, we will test on the testing data. \n","699b07a6":"Looking over the data, we can see some NaN values in the Cabin column. Lets go over all of the columns and try to find NaN values. ","8ef90850":"Looking over these columns, it is clear that Age, Cabin, and Embarked all contain NaN values. My first instinct is to replace the age NaN values with the mean. Embarked only has 2 missing, so I will replace the NaN with the most occuring value. Cabin however seems to have many missing values. Let's start with Age and Embarked first. ","b1e9b7d5":"I chose these plots because the data from these columns seemed most relevant in terms of predicting survival. It appears that the majority of people were between 20 and 40. Survivors seemed to include more people that were younger and older than the 20-40 range. \n\nThe majority of survivors were women, and majority of passengers that did not survive were men. \n\nIt appears that having a pclass of 3 lowered your chances of survival, and having a pclass of 1 or 2 seemed to be pretty even chances in survival. \n\nIt's hard to see many obvious patterns with the number of parents\/children, and the number of siblings\/spouses. The only one that sticks out to me is if you had 1 parent\/child\/spouse\/sibling, your chances of survival seemed greater than having none.  \n\nI believe that using these variables alongside a decision tree could possibly give me some good results. "}}