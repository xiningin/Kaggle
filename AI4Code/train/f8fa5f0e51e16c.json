{"cell_type":{"61f069e5":"code","7e12e084":"code","f47538c0":"code","1803d640":"code","a970aff7":"code","97dc5459":"code","627d664b":"code","280c2ebb":"code","ea03228a":"code","6f17509e":"code","884b0a75":"code","f6dff77b":"code","2ae2fc4c":"code","c57ef88c":"markdown","5b4786a9":"markdown","6ebda9a4":"markdown","b449a76e":"markdown","b0bda602":"markdown"},"source":{"61f069e5":"#!pip install \/kaggle\/input\/pytorch-fairseq\/fairseq-0.9.0\/ > \/dev\/null\n#! pip install \/kaggle\/input\/pytorchtransformers\/transformers-2.5.1 > \/dev\/null","7e12e084":"## Basic Library\nimport os, time, sys, gc\nfrom tqdm.notebook import tqdm\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport json\n\n## Pytorch\nimport torch\nimport pytorch_transformers\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/xlnet-pretrained-models-pytorch'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f47538c0":"INPUTDIR = '\/kaggle\/input\/tweet-sentiment-extraction\/'","1803d640":"%%time\ntrain_df = pd.read_csv(f'{INPUTDIR}\/train.csv')\ntest_df = pd.read_csv(f'{INPUTDIR}\/test.csv')\nprint('train shape is {}, and test shape is {}'.format(train_df.shape, test_df.shape))","a970aff7":"train = np.array(train_df)\ntest = np.array(test_df)\n\n!mkdir -p data","97dc5459":"\"\"\"\nPrepare training data in QA-compatible format\n\"\"\"\n\n# Adpated from https:\/\/www.kaggle.com\/cheongwoongkang\/roberta-baseline-starter-simple-postprocessing\ndef find_all(input_str, search_str):\n    l1 = []\n    length = len(input_str)\n    index = 0\n    while index < length:\n        i = input_str.find(search_str, index)\n        if i == -1:\n            return l1\n        l1.append(i)\n        index = i + 1\n    return l1\n\ndef do_qa_train(train):\n\n    output = {}\n    output['version'] = 'v1.0'\n    output['data'] = []\n    \n    for line in tqdm(train):\n        context = line[1]\n        paragraphs = []\n        qas = []\n        question = line[-1]\n        qid = line[0]\n        answers = []\n        answer = line[2]\n        if type(answer) != str or type(context) != str or type(question) != str:\n            print(context, type(context))\n            print(answer, type(answer))\n            print(question, type(question))\n            continue\n        answer_starts = find_all(context, answer)\n        for answer_start in answer_starts:\n            answers.append({'answer_start': answer_start, 'text': answer.lower()})\n            break\n        qas.append({'question': question, 'id': qid, 'is_impossible': False, 'answers': answers})\n\n        paragraphs.append({'context': context.lower(), 'qas': qas})\n        output['data'].append({'title': 'None', 'paragraphs': paragraphs})\n        \n    return output\n\nqa_train = do_qa_train(train)\n\nwith open('data\/train.json', 'w') as outfile:\n    json.dump(qa_train, outfile)","627d664b":"\"\"\"\nPrepare testing data in QA-compatible format\n\"\"\"\n\noutput = {}\noutput['version'] = 'v1.0'\noutput['data'] = []\n\ndef do_qa_test(test):\n    paragraphs = []\n    for line in tqdm(test):\n        paragraphs = []\n        context = line[1]\n        qas = []\n        question = line[-1]\n        qid = line[0]\n        if type(context) != str or type(question) != str:\n            print(context, type(context))\n            print(answer, type(answer))\n            print(question, type(question))\n            continue\n        answers = []\n        answers.append({'answer_start': 1000000, 'text': '__None__'})\n        qas.append({'question': question, 'id': qid, 'is_impossible': False, 'answers': answers})\n\n        paragraphs.append({'context': context.lower(), 'qas': qas})\n        output['data'].append({'title': 'None', 'paragraphs': paragraphs})\n    return output\n\nqa_test = do_qa_test(test)\n\nwith open('data\/test.json', 'w') as outfile:\n    json.dump(qa_test, outfile)","280c2ebb":"!mkdir -p results_roberta_large","ea03228a":"import sentencepiece","6f17509e":"!python -m torch.distributed.launch --nproc_per_node=1 \/kaggle\/input\/bert-squad\/BERT-SQuAD-master\/training\/run_squad.py \\\n--model_type xlnet \\\n--model_name_or_path \/kaggle\/input\/xlnet-pretrained-models-pytorch\/xlnet-large-cased-pytorch_model.bin \\\n--tokenizer_name \/kaggle\/input\/xlnet-pretrained-models-pytorch\/xlnet-large-cased-spiece.model \\\n--config_name \/kaggle\/input\/xlnet-pretrained-models-pytorch\/xlnet-large-cased-config.json \\\n--do_train \\\n--do_eval \\\n--do_lower_case \\\n--train_file .\/data\/train.json \\\n--predict_file .\/data\/test.json \\\n--learning_rate 3e-5 \\\n--num_train_epochs 2 \\\n--max_seq_length 192 \\\n--doc_stride 64 \\\n--output_dir .\/results_roberta_large\/ \\\n--per_gpu_eval_batch_size=10   \\\n--per_gpu_train_batch_size=10   \\\n--save_steps=100000","884b0a75":"# Copy predictions to submission file.\npredictions = json.load(open('results_roberta_large\/predictions_.json', 'r'))\nsubmission = pd.read_csv(open('\/kaggle\/input\/tweet-sentiment-extraction\/sample_submission.csv', 'r'))\nfor i in range(len(submission)):\n    id_ = submission['textID'][i]\n    if test_df['sentiment'][i] == 'neutral': # neutral postprocessing\n        submission.loc[i, 'selected_text'] = test_df['text'][i]\n    else:\n        submission.loc[i, 'selected_text'] = predictions[id_]","f6dff77b":"submission.head()","2ae2fc4c":"# Save the submission file.\nsubmission.to_csv('submission.csv', index=False)","c57ef88c":"# Import Data","5b4786a9":"# Install and Import Libraries","6ebda9a4":"# Define Functions","b449a76e":"# Submission","b0bda602":"# Global Parameters"}}