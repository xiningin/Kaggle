{"cell_type":{"9ad0d8e7":"code","db08a3be":"code","f7a0f25b":"code","cf184377":"code","acf2d547":"code","95e34db8":"code","e728714a":"code","a05b43a1":"code","9cedc045":"code","51b02e03":"code","868da3d2":"code","295fa3b4":"code","25d220a6":"code","520e81f8":"code","abd7fcf4":"code","d826ad24":"code","91e0c83e":"code","039604df":"code","112682af":"code","8c09bd4e":"code","878cb16e":"code","8ccc193f":"code","5a1d095e":"code","82d626f3":"code","bfd94170":"code","4e4fe611":"markdown","99279056":"markdown","d08de91e":"markdown"},"source":{"9ad0d8e7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport lightgbm as lgb\nfrom tqdm import tqdm_notebook\nimport matplotlib.pyplot as plt\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","db08a3be":"train_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')","f7a0f25b":"# train_df.describe()","cf184377":"# test_df.describe()","acf2d547":"# train_df.shape, test_df.shape","95e34db8":"#list(train_df.columns)","e728714a":"def find_null(df):\n    column_list = list(df.columns)\n    for column in column_list:\n        print (train_df[column].isnull().sum())","a05b43a1":"find_null(train_df)","9cedc045":"train_df.head(10)","51b02e03":"# sns.countplot(train_df['target'])","868da3d2":"train_df_1 = train_df.loc[train_df.target ==1]\ntrain_df_0 = train_df.loc[train_df.target ==0]\nprint(\"Number of target value as 1 %d\" %len(train_df_1))\nprint(\"Number of target value as 0 {}\".format(len(train_df_0)))","295fa3b4":"# # let us try to find co-relation between features\n# plt.figure(figsize=(30,30))\n# corr = train_df.corr()\n# sns.heatmap(corr)","25d220a6":"# Lets work on creating a model, lets define X and Y\nX_train = train_df.drop(['ID_code','target'], axis=1)\nY_train = train_df['target']\nX_test = test_df.drop(['ID_code'],axis=1)\nprint(X_train.shape)\nprint(Y_train.shape)","520e81f8":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold, cross_val_predict\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import StandardScaler","abd7fcf4":"cv = list (StratifiedKFold(5,random_state=5756).split(X_train,Y_train))\nscaler = StandardScaler()\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.fit_transform(X_test)\n# print(len(Y_train))\n# lr = LogisticRegression()\n# y_pred = cross_val_predict(lr, X_train_scaled, Y_train, cv=cv, method = 'predict_proba',verbose=2 )[:,1]\n# #lr.fit(X_train_scaled, Y_train)\n# print(len(y_pred))\n# roc_auc_score(Y_train, y_pred)","d826ad24":"# lr.fit(X_train_scaled,Y_train)\n# #test_preds = lr.predict(X_test_scaled)\n# preds=lr.predict(X_test)","91e0c83e":"from keras import layers\nfrom keras import Sequential\nfrom keras.layers import Dense, Flatten, Activation, Dropout\nfrom keras.optimizers import Adam, sgd, RMSprop\nfrom imblearn.over_sampling import SMOTE\nfrom keras import regularizers\nfrom keras.callbacks import EarlyStopping","039604df":"# print(\"Before OverSampling, counts of label '1': {}\".format(sum(Y_train==1)))\n# print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(Y_train==0)))\n\n# sm = SMOTE(random_state=2)\n# X_train_res, Y_train_res = sm.fit_sample(X_train_scaled, Y_train.ravel())\n\n# print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\n# print('After OverSampling, the shape of train_y: {} \\n'.format(Y_train_res.shape))\n\n# print(\"After OverSampling, counts of label '1': {}\".format(sum(Y_train_res==1)))\n# print(\"After OverSampling, counts of label '0': {}\".format(sum(Y_train_res==0)))","112682af":"from sklearn.model_selection import train_test_split\nfrom sklearn.utils import class_weight\n\nX_train_split,  X_test_split, Y_train_split, Y_test_split = train_test_split(X_train_scaled,Y_train, test_size = 0.20, random_state=143234)\nprint(\"Length of training data {}\".format(len(X_train_split)))\nprint(\"Length of testing data {}\".format(len(X_test_split)))","8c09bd4e":"class_weights = class_weight.compute_class_weight('balanced',\n                                                 np.unique(Y_train),\n                                                 Y_train)","878cb16e":"model = Sequential()\nmodel.add(Dense(1024,input_dim=len(X_train.columns), activation='sigmoid', kernel_initializer = 'he_uniform'))\nmodel.add(Dense(1024, activation='sigmoid', kernel_initializer = 'he_uniform',kernel_regularizer=regularizers.l2(0.001)))\n#model.add(Dropout(0.25))\n# model.add(Dense(2048, activation='relu', kernel_initializer = 'he_uniform'))\n# model.add(Dropout(0.25))\nmodel.add(Dense(512, activation='sigmoid', kernel_initializer = 'he_uniform'))\n#model.add(Dropout(0.25))\n#model.add(Dense(256, activation='sigmoid', kernel_initializer = 'he_uniform'))\n#model.add(Dropout(0.2))\nmodel.add(Dense(256, activation='sigmoid', kernel_initializer = 'he_uniform' ))\n#model.add(Dropout(0.25))\n#model.add(Dense(1024, activation='relu', kernel_initializer = 'he_uniform'))\n#model.add(Dropout(0.05))\n#model.add(Dense(512, activation='relu', kernel_initializer = 'he_uniform'))\n#model.add(Dense(512, activation='relu', kernel_initializer = 'he_uniform',kernel_regularizer=regularizers.l1(0.001)))\n#model.add(Dropout(0.15))\nmodel.add(Dense(64, activation='relu', kernel_initializer = 'he_uniform'))\n#model.add(Dense(32, activation='relu', kernel_initializer = 'he_uniform',))\nmodel.add(Dense(1,activation='sigmoid', kernel_initializer='he_uniform'))\nmodel.compile(loss='binary_crossentropy',optimizer=Adam(lr=0.0001),metrics=['binary_accuracy'])\n#model.compile(loss='binary_crossentropy',optimizer=sgd(lr=0.0001),metrics=['binary_accuracy'])\n\n              \nmodel.summary\nmodel.fit(X_train_split, \n          Y_train_split, \n          batch_size=20, \n          epochs=40, \n          verbose=1, \n          class_weight=class_weights,\n          callbacks=[EarlyStopping(monitor='binary_accuracy')],\n          validation_data=(X_test_split, Y_test_split))\n\n              \n\n","8ccc193f":"from sklearn.metrics import accuracy_score\n\n\n\npreds=model.predict(X_test_split)\npreds = np.where(preds >= 0.5, 1, 0)\nTest_Accuracy=accuracy_score(Y_test_split, preds)\nprint(Test_Accuracy)","5a1d095e":"#from sklearn.metrics import roc_auc_score\n\ntrain_predict = model.predict_proba(X_train)\ntrain_roc = roc_auc_score(Y_train, train_predict)\nprint('Train AUC: {}'.format(train_roc))\n\nval_predict = model.predict_proba(X_test_split)\nval_roc = roc_auc_score(Y_test_split, val_predict )\nprint('Val AUC: {}'.format(val_roc))","82d626f3":"preds_final=model.predict(X_test)\npreds_final=preds_final[:,0]","bfd94170":"X_test_ID = test_df['ID_code']\nsubmission = pd.DataFrame({ 'ID_code': X_test_ID,'target': preds_final})\nsubmission.to_csv(\"submission_17.csv\", index=False)","4e4fe611":"* What seems to be quite surprising is that there is no co-relation between any off the variables","99279056":"# NN model\n","d08de91e":"Some observations\n- No missing values\n- There seems be no outliers\n- Lets plot some graphs so that we can understand the data better"}}