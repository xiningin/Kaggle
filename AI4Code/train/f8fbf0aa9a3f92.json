{"cell_type":{"ab093625":"code","9b3ceaba":"code","2361af8b":"code","6a888680":"code","6c39dfe3":"code","15d78aec":"code","4159862f":"code","0e548636":"code","119933b9":"code","deef7452":"code","5cbda500":"code","4235de47":"code","f837abeb":"code","8d8040f9":"code","a917bb90":"code","c91af853":"code","9f8e54ee":"code","d9df19a9":"code","adbefc55":"code","5eebe4b5":"code","885c7fce":"code","ce3eb52f":"code","f45ae77a":"code","e0db0502":"markdown","d3510d85":"markdown","48b32e6d":"markdown","095630ae":"markdown","c05def00":"markdown","2a8cae06":"markdown","e873c117":"markdown"},"source":{"ab093625":"%matplotlib inline\nimport pathlib\nimport matplotlib.pyplot as plt\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 8, 6\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\npd.set_option('display.max_columns', 500)\nfrom collections import defaultdict\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity='all'\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.metrics import mean_squared_error\n\npd.options.mode.chained_assignment = None\n\nfrom torch.nn import init\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nfrom torch.utils import data\nfrom torch.optim import lr_scheduler\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice\n\nfrom tqdm import tqdm, tqdm_notebook, tnrange\ntqdm.pandas(desc='Progress')","9b3ceaba":"def haversine_distance(df, start_lat, end_lat, start_lng, end_lng, prefix):\n    \"\"\"\n    calculates haversine distance between 2 sets of GPS coordinates in df\n    \"\"\"\n    R = 6371  #radius of earth in kilometers\n       \n    phi1 = np.radians(df[start_lat])\n    phi2 = np.radians(df[end_lat])\n    \n    delta_phi = np.radians(df[end_lat]-df[start_lat])\n    delta_lambda = np.radians(df[end_lng]-df[start_lng])\n    \n        \n    a = np.sin(delta_phi \/ 2.0) ** 2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda \/ 2.0) ** 2\n    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n    d = (R * c) #in kilometers\n    df[prefix+'distance_km'] = d\n\ndef add_datepart(df, col, prefix):\n    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n            'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n    attr = attr + ['Hour', 'Minute', 'Second']\n    for n in attr: df[prefix + n] = getattr(df[col].dt, n.lower())\n    df[prefix + 'Elapsed'] = df[col].astype(np.int64) \/\/ 10 ** 9\n    df.drop(col, axis=1, inplace=True)\n    \ndef reject_outliers(data, m = 2.):\n    d = np.abs(data - np.median(data))\n    mdev = np.median(d)\n    s = d\/(mdev if mdev else 1.)\n    return s<m\n\ndef parse_gps(df, prefix):\n    lat = prefix + '_latitude'\n    lon = prefix + '_longitude'\n    df[prefix + '_x'] = np.cos(df[lat]) * np.cos(df[lon])\n    df[prefix + '_y'] = np.cos(df[lat]) * np.sin(df[lon]) \n    df[prefix + '_z'] = np.sin(df[lat])\n    df.drop([lat, lon], axis=1, inplace=True)\n    \ndef prepare_dataset(df):\n    df['pickup_datetime'] = pd.to_datetime(df.pickup_datetime, infer_datetime_format=True)\n    add_datepart(df, 'pickup_datetime', 'pickup')\n    haversine_distance(df, 'pickup_latitude', 'dropoff_latitude', 'pickup_longitude', 'dropoff_longitude', '')\n    parse_gps(df, 'pickup')\n    parse_gps(df, 'dropoff')\n    df.dropna(inplace=True)\n    y = np.log(df.fare_amount)\n    df.drop(['key', 'fare_amount'], axis=1, inplace=True)\n    \n    return df, y\n\ndef split_features(df):\n    catf = ['pickupYear', 'pickupMonth', 'pickupWeek', 'pickupDay', 'pickupDayofweek', \n            'pickupDayofyear', 'pickupHour', 'pickupMinute', 'pickupSecond', 'pickupIs_month_end',\n            'pickupIs_month_start', 'pickupIs_quarter_end', 'pickupIs_quarter_start',\n            'pickupIs_year_end', 'pickupIs_year_start']\n\n    numf = [col for col in df.columns if col not in catf]\n    for c in catf: \n        df[c] = df[c].astype('category').cat.as_ordered()\n        df[c] = df[c].cat.codes+1\n    \n    return catf, numf\n\ndef numericalize(df):\n    df[name] = col.cat.codes+1\n\ndef split_dataset(df, y): return train_test_split(df, y, test_size=0.25, random_state=42)\n\ndef inv_y(y): return np.exp(y)\n\ndef get_numf_scaler(train): return preprocessing.StandardScaler().fit(train)\n\ndef scale_numf(df, num, scaler):\n    cols = numf\n    index = df.index\n    scaled = scaler.transform(df[numf])\n    scaled = pd.DataFrame(scaled, columns=cols, index=index)\n    return pd.concat([scaled, df.drop(numf, axis=1)], axis=1)\n\nclass RegressionColumnarDataset(data.Dataset):\n    def __init__(self, df, cats, y):\n        self.dfcats = df[cats]\n        self.dfconts = df.drop(cats, axis=1)\n        \n        self.cats = np.stack([c.values for n, c in self.dfcats.items()], axis=1).astype(np.int64)\n        self.conts = np.stack([c.values for n, c in self.dfconts.items()], axis=1).astype(np.float32)\n        self.y = y.values.astype(np.float32)\n        \n    def __len__(self): return len(self.y)\n\n    def __getitem__(self, idx):\n        return [self.cats[idx], self.conts[idx], self.y[idx]]\n    \ndef rmse(targ, y_pred):\n    return np.sqrt(mean_squared_error(inv_y(y_pred), inv_y(targ))) #.detach().numpy()\n\ndef emb_init(x):\n    x = x.weight.data\n    sc = 2\/(x.size(1)+1)\n    x.uniform_(-sc,sc)\n\nclass MixedInputModel(nn.Module):\n    def __init__(self, emb_szs, n_cont, emb_drop, out_sz, szs, drops, y_range, use_bn=True):\n        super().__init__()\n        for i,(c,s) in enumerate(emb_szs): assert c > 1, f\"cardinality must be >=2, got emb_szs[{i}]: ({c},{s})\"\n        self.embs = nn.ModuleList([nn.Embedding(c, s) for c,s in emb_szs])\n        for emb in self.embs: emb_init(emb)\n        n_emb = sum(e.embedding_dim for e in self.embs)\n        self.n_emb, self.n_cont=n_emb, n_cont\n        \n        szs = [n_emb+n_cont] + szs\n        self.lins = nn.ModuleList([nn.Linear(szs[i], szs[i+1]) for i in range(len(szs)-1)])\n        self.bns = nn.ModuleList([nn.BatchNorm1d(sz) for sz in szs[1:]])\n        for o in self.lins: nn.init.kaiming_normal_(o.weight.data)\n        self.outp = nn.Linear(szs[-1], out_sz)\n        nn.init.kaiming_normal_(self.outp.weight.data)\n\n        self.emb_drop = nn.Dropout(emb_drop)\n        self.drops = nn.ModuleList([nn.Dropout(drop) for drop in drops])\n        self.bn = nn.BatchNorm1d(n_cont)\n        self.use_bn,self.y_range = use_bn,y_range\n\n    def forward(self, x_cat, x_cont):\n        if self.n_emb != 0:\n            x = [e(x_cat[:,i]) for i,e in enumerate(self.embs)]\n            x = torch.cat(x, 1)\n            x = self.emb_drop(x)\n        if self.n_cont != 0:\n            x2 = self.bn(x_cont)\n            x = torch.cat([x, x2], 1) if self.n_emb != 0 else x2\n        for l,d,b in zip(self.lins, self.drops, self.bns):\n            x = F.relu(l(x))\n            if self.use_bn: x = b(x)\n            x = d(x)\n        x = self.outp(x)\n        if self.y_range:\n            x = torch.sigmoid(x)\n            x = x*(self.y_range[1] - self.y_range[0])\n            x = x+self.y_range[0]\n        return x.squeeze()\n\ndef fit(model, train_dl, val_dl, loss_fn, opt, scheduler, epochs=3):\n    num_batch = len(train_dl)\n    for epoch in tnrange(epochs):      \n        y_true_train = list()\n        y_pred_train = list()\n        total_loss_train = 0          \n        \n        t = tqdm_notebook(iter(train_dl), leave=False, total=num_batch)\n        for cat, cont, y in t:\n            cat = cat.cuda()\n            cont = cont.cuda()\n            y = y.cuda()\n            \n            t.set_description(f'Epoch {epoch}')\n            \n            opt.zero_grad()\n            pred = model(cat, cont)\n            loss = loss_fn(pred, y)\n            loss.backward()\n            lr[epoch].append(opt.param_groups[0]['lr'])\n            tloss[epoch].append(loss.item())\n            scheduler.step()\n            opt.step()\n            \n            t.set_postfix(loss=loss.item())\n            \n            y_true_train += list(y.cpu().data.numpy())\n            y_pred_train += list(pred.cpu().data.numpy())\n            total_loss_train += loss.item()\n            \n        train_acc = rmse(y_true_train, y_pred_train)\n        train_loss = total_loss_train\/len(train_dl)\n        \n        if val_dl:\n            y_true_val = list()\n            y_pred_val = list()\n            total_loss_val = 0\n            for cat, cont, y in tqdm_notebook(val_dl, leave=False):\n                cat = cat.cuda()\n                cont = cont.cuda()\n                y = y.cuda()\n                pred = model(cat, cont)\n                loss = loss_fn(pred, y)\n                \n                y_true_val += list(y.cpu().data.numpy())\n                y_pred_val += list(pred.cpu().data.numpy())\n                total_loss_val += loss.item()\n                vloss[epoch].append(loss.item())\n            valacc = rmse(y_true_val, y_pred_val)\n            valloss = total_loss_val\/len(valdl)\n            print(f'Epoch {epoch}: train_loss: {train_loss:.4f} train_rmse: {train_acc:.4f} | val_loss: {valloss:.4f} val_rmse: {valacc:.4f}')\n        else:\n            print(f'Epoch {epoch}: train_loss: {train_loss:.4f} train_rmse: {train_acc:.4f}')\n    \n    return lr, tloss, vloss","2361af8b":"PATH='..\/input\/'","6a888680":"names = ['key','fare_amount','pickup_datetime','pickup_longitude',\n         'pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count']\ndf = pd.read_csv(f'{PATH}train.csv', nrows=6000000)\n\nprint(df.shape)\nprint(df.head())","6c39dfe3":"print(df.passenger_count.describe())\nprint(df.passenger_count.quantile([.85, .99]))","15d78aec":"print(df.fare_amount.describe())\nprint(df.fare_amount.quantile([.85, .99]))","4159862f":"df = df.loc[(df.fare_amount > 0) & (df.passenger_count < 6) & (df.fare_amount < 53),:]","0e548636":"df, y = prepare_dataset(df)\n\nprint(df.shape)\nprint(df.head())","119933b9":"ax = y.hist(bins=20, figsize=(8,6))\n_ = ax.set_xlabel(\"Ride Value (EUR)\")\n_ = ax.set_ylabel(\"# Rides\")\n_ = ax.set_title('Ditribution of Ride Values (USD)')","deef7452":"catf, numf = split_features(df)\n\nlen(catf)\ncatf\n\nlen(numf)\nnumf","5cbda500":"df.head()","4235de47":"y_range = (0, y.max()*1.2)\ny_range\n\ny = y.clip(y_range[0], y_range[1])\nX_train, X_test, y_train, y_test = split_dataset(df, y)\n\nX_train.shape\nX_test.shape","f837abeb":"scaler = get_numf_scaler(X_train[numf])\n\nX_train_sc = scale_numf(X_train, numf, scaler)\nX_train_sc.std(axis=0)","8d8040f9":"X_test_sc = scale_numf(X_test, numf, scaler)\n\nX_train_sc.shape\nX_test_sc.shape\nX_test_sc.std(axis=0)","a917bb90":"trainds = RegressionColumnarDataset(X_train_sc, catf, y_train)\nvalds = RegressionColumnarDataset(X_test_sc, catf, y_test)","c91af853":"class RegressionColumnarDataset(data.Dataset):\n    def __init__(self, df, cats, y):\n        self.dfcats = df[cats]\n        self.dfconts = df.drop(cats, axis=1)\n        \n        self.cats = np.stack([c.values for n, c in self.dfcats.items()], axis=1).astype(np.int64)\n        self.conts = np.stack([c.values for n, c in self.dfconts.items()], axis=1).astype(np.float32)\n        self.y = y.values.astype(np.float32)\n        \n    def __len__(self): return len(self.y)\n\n    def __getitem__(self, idx):\n        return [self.cats[idx], self.conts[idx], self.y[idx]]","9f8e54ee":"params = {'batch_size': 128,\n          'shuffle': True,\n          'num_workers': 8}\n\ntraindl = data.DataLoader(trainds, **params)\nvaldl = data.DataLoader(valds, **params)","d9df19a9":"cat_sz = [(c, df[c].max()+1) for c in catf]\ncat_sz\n\nemb_szs = [(c, min(50, (c+1)\/\/2)) for _,c in cat_sz]\nemb_szs","adbefc55":"m = MixedInputModel(emb_szs=emb_szs, \n                    n_cont=len(df.columns)-len(catf), \n                    emb_drop=0.04, \n                    out_sz=1, \n                    szs=[1000,500,250], \n                    drops=[0.001,0.01,0.01], \n                    y_range=y_range).to(device)\n\nopt = optim.Adam(m.parameters(), 1e-2)\nlr_cosine = lr_scheduler.CosineAnnealingLR(opt, 1000)\n\nlr = defaultdict(list)\ntloss = defaultdict(list)\nvloss = defaultdict(list)","5eebe4b5":"m","885c7fce":"epoch_n = 12\n\nlr, tloss, vloss = fit(model=m, train_dl=traindl, val_dl=valdl, loss_fn=F.mse_loss, opt=opt, scheduler=lr_cosine, epochs=epoch_n)","ce3eb52f":"_ = plt.plot(lr[0])\n_ = plt.title('Learning Rate Cosine Annealing over Train Batches Iterations (Epoch 0)')","f45ae77a":"t = [np.mean(tloss[el]) for el in tloss]\nv = [np.mean(vloss[el]) for el in vloss]\np = pd.DataFrame({'Train Loss': t, 'Validation Loss': v, 'Epochs': range(1, epoch_n+1)})\n\n_ = p.plot(x='Epochs', y=['Train Loss', 'Validation Loss'], \n           title='Train and Validation Loss over Epochs')","e0db0502":"# Preparing the data","d3510d85":"Tha data is a half-million random sample from the 55M original training set from the [New York City Taxi Fare Prediction](https:\/\/www.kaggle.com\/c\/new-york-city-taxi-fare-prediction\/data) Kaggle's challenge","48b32e6d":"## Defining pytorch datasets and dataloaders","095630ae":"### Helper Functions","c05def00":"## Training the model","2a8cae06":"# PyTorch for Tabular Data: Predicting NYC Taxi Fares\n_FULL CREDIT TO FRANCESCO POCHETTI_ <br>\n_http:\/\/francescopochetti.com\/pytorch-for-tabular-data-predicting-nyc-taxi-fares\/_\n\n### Imports","e873c117":"## Defining model and related variables"}}