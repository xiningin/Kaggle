{"cell_type":{"6d4fbdcd":"code","021f1d35":"code","d67f5aac":"code","2925b148":"code","4ad84391":"code","cfe56cb2":"code","b5439f29":"code","68b185a4":"code","0b6e388f":"code","4deb3f2e":"code","b108c8c9":"code","666b6710":"code","9e1ffff1":"code","e15ea9d5":"code","778dd1b7":"code","b51e197f":"code","833cb656":"code","febd31e4":"code","a43e3386":"code","d72a3d32":"code","b047103f":"code","0315f919":"code","737582dc":"code","3a63927e":"code","e6a3247d":"code","79a9cf55":"code","651ce810":"code","4381c82b":"code","511d09c7":"code","4ae78edf":"code","0c167aa7":"code","90864521":"code","0dc5f240":"code","4c077cb6":"code","70d63125":"code","22f90274":"code","4b5f7ee1":"code","c1152ac2":"code","b264713e":"code","25a5c8ca":"code","e8e9f40d":"code","529f042a":"code","c7063ce7":"code","a8b5b553":"code","13448fd5":"code","e4f3da29":"code","fa9ffbe1":"code","9bb1d518":"code","0f7871c7":"code","95907024":"code","767a274b":"code","9c984f6b":"code","1fd905b9":"code","688afcd1":"code","419ee3eb":"code","15f4dd57":"code","36521367":"code","67e382fc":"code","9216d7c8":"code","85f2b36f":"code","c48bb8a5":"code","0f574469":"code","c730cd93":"code","cceb3ab3":"code","df0bfaa2":"code","779fd116":"code","c49b61ec":"code","11c9977f":"code","cc6a17fb":"code","cbfa6323":"code","dd8355be":"code","92abf4e6":"markdown","d3416eb1":"markdown","13120334":"markdown","368470aa":"markdown","e1de1e42":"markdown","802ede4e":"markdown","88820d0b":"markdown","946bf32e":"markdown","20d145f5":"markdown","7f854fdd":"markdown","93f82336":"markdown","cb7c272b":"markdown","53b2e24d":"markdown","81872d77":"markdown","2e7f8715":"markdown"},"source":{"6d4fbdcd":"import torch\nimport torchvision\nfrom torchvision.datasets import MNIST\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.pyplot import figure","021f1d35":"# Download training dataset\ndataset = MNIST(root='data\/', download=True, transform=transforms.ToTensor())","d67f5aac":"dataset[0]","2925b148":"print(type(dataset))\nprint(len(dataset))","4ad84391":"test_dataset = MNIST(root='data\/', train=False)\nlen(test_dataset)","cfe56cb2":"from torch.utils.data import random_split\n\ntrain_ds, val_ds = random_split(dataset, [50000, 10000])\nlen(train_ds), len(val_ds)","b5439f29":"from torch.utils.data import DataLoader\n\nbatch_size = 128\n\ntrain_loader = DataLoader(train_ds, batch_size, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size)","68b185a4":"import torch.nn as nn\n\ninput_size = 28*28\nnum_classes = 10\n\n# Logistic regression model\nmodel = nn.Linear(input_size, num_classes)","0b6e388f":"print(model.weight.shape)\nmodel.weight","4deb3f2e":"print(model.bias.shape)\nmodel.bias","b108c8c9":"for images, labels in train_loader:\n    print(labels)\n    print(images.shape)\n    outputs = model(images)\n    break","666b6710":"class MnistModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(input_size, num_classes)\n        \n    #this function is called when ever thr model is used..\n    def forward(self, xb):\n        xb = xb.reshape(-1, 784)\n        out = self.linear(xb)\n        return out\n    \nmodel = MnistModel()","9e1ffff1":"print(model.linear.weight.shape, model.linear.bias.shape)\nlist(model.parameters())","e15ea9d5":"for images, labels in train_loader:\n    outputs = model(images)\n    break\n\nprint('outputs.shape : ', outputs.shape)\nprint('Sample outputs :\\n', outputs[:2].data)","778dd1b7":"import torch.nn.functional as F","b51e197f":"# Apply softmax for each output row\nprobs = F.softmax(outputs, dim=1)\n\n# Look at sample probabilities\nprint(\"Sample probabilities:\\n\", probs[:2].data)\n\n# Add up the probabilities of an output row\nprint(\"Sum: \", torch.sum(probs[0]).item())","833cb656":"#max provides two results, maximum value and index with maximum value\nmax_probs, preds = torch.max(probs, dim=1)\nprint(preds)\nprint(max_probs)","febd31e4":"labels","a43e3386":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))","d72a3d32":"accuracy(outputs, labels)","b047103f":"loss_fn = F.cross_entropy","0315f919":"# Loss for current batch of data\nloss = loss_fn(outputs, labels)\nprint(loss)","737582dc":"#Dimension in pytorch : https:\/\/towardsdatascience.com\/understanding-dimensions-in-pytorch-6edf9972d3be\nclass MnistModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(input_size, num_classes)\n        \n    def forward(self, xb):\n        #reshaping\n        xb = xb.reshape(-1, 784)\n        out = self.linear(xb)\n        return out\n    \n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        #returns a dictionary containg validation loss and validation accurracy\n        return {'val_loss': loss, 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        #stacks the batch loss over columns and do sum over columns. Dim 0 is columns and Dim 1 is row\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n    \nmodel = MnistModel()\n","3a63927e":"def evaluate(model, val_loader):\n    #output contains a list of dictionaries with validation loass and validation accuracy\n    outputs = [model.validation_step(batch) for batch in val_loader]\n#     print(outputs)\n    return model.validation_epoch_end(outputs)","e6a3247d":"def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n    history = []\n    optimizer = opt_func(model.parameters(), lr)\n    for epoch in range(epochs):\n        # Training Phase \n        for batch in train_loader:\n            loss = model.training_step(batch)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        # Validation phase\n        result = evaluate(model, val_loader)\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","79a9cf55":"result0 = evaluate(model, val_loader)\nresult0","651ce810":"history1 = fit(5, 0.001, model, train_loader, val_loader)","4381c82b":"history2 = fit(5, 0.001, model, train_loader, val_loader)","511d09c7":"history3 = fit(5, 0.001, model, train_loader, val_loader)","4ae78edf":"history4 = fit(5, 0.001, model, train_loader, val_loader)","0c167aa7":"# Replace these values with your results\nfigure(num=None, figsize=(18, 10), dpi=80, facecolor='w', edgecolor='k')\nhistory = [result0] + history1 + history2 + history3 + history4\naccuracies = [result['val_acc'] for result in history]\nplt.plot(accuracies, '-x', label='Base Model')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.legend(loc=\"lower right\")\nplt.grid()\nplt.title('Accuracy vs. No. of epochs');","90864521":"# Replace these values with your results\nfigure(num=None, figsize=(18, 10), dpi=80, facecolor='w', edgecolor='k')\nhistory = [result0] + history1 + history2 + history3 + history4\nlosses = [result['val_loss'] for result in history]\nplt.plot(losses, '-x', label='Base Model')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend(loc=\"lower right\")\nplt.grid()\nplt.title('Loss vs. No. of epochs');","0dc5f240":"# Define test dataset\ntest_dataset = MNIST(root='data\/', \n                     train=False,\n                     transform=transforms.ToTensor())","4c077cb6":"img, label = test_dataset[0]\nplt.imshow(img[0], cmap='gray')\nprint('Shape:', img.shape)\nprint('Label:', label)","70d63125":"type(img)","22f90274":"img.unsqueeze(0).shape #we can see that one more dimension was added here.","4b5f7ee1":"def predict_image(img, model):\n    xb = img.unsqueeze(0)\n    yb = model(xb)\n    _, preds  = torch.max(yb, dim=1)\n    return preds[0].item()","c1152ac2":"img, label = test_dataset[0]\nplt.imshow(img[0], cmap='gray')\nprint('Label:', label, ', Predicted:', predict_image(img, model))","b264713e":"img, label = test_dataset[101]\nplt.imshow(img[0], cmap='gray')\nprint('Label:', label, ', Predicted:', predict_image(img, model))","25a5c8ca":"img, label = test_dataset[2340]\nplt.imshow(img[0], cmap='gray')\nprint('Label:', label, ', Predicted:', predict_image(img, model))","e8e9f40d":"img, label = test_dataset[3478]\nplt.imshow(img[0], cmap='gray')\nprint('Label:', label, ', Predicted:', predict_image(img, model))","529f042a":"img, label = test_dataset[9755]\nplt.imshow(img[0], cmap='gray')\nprint('Label:', label, ', Predicted:', predict_image(img, model))","c7063ce7":"img, label = test_dataset[4512]\nplt.imshow(img[0], cmap='gray')\nprint('Label:', label, ', Predicted:', predict_image(img, model))","a8b5b553":"img, label = test_dataset[1839]\nplt.imshow(img[0], cmap='gray')\nprint('Label:', label, ', Predicted:', predict_image(img, model))","13448fd5":"test_loader = DataLoader(test_dataset, batch_size=256)\nresult = evaluate(model, test_loader)\nresult","e4f3da29":"torch.save(model.state_dict(), 'mnist-logistic.pth')","fa9ffbe1":"from torch.utils.data import random_split\n\ntrain_ds, val_ds = random_split(dataset, [57000, 3000])\nlen(train_ds), len(val_ds)","9bb1d518":"batch_size = 128\n\ntrain_loader = DataLoader(train_ds, batch_size, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size)","0f7871c7":"model = MnistModel()","95907024":"result0vlow = evaluate(model, val_loader)\nresult0vlow","767a274b":"history1vlow = fit(5, 0.001, model, train_loader, val_loader)","9c984f6b":"history2vlow = fit(5, 0.001, model, train_loader, val_loader)","1fd905b9":"history3vlow = fit(5, 0.001, model, train_loader, val_loader)","688afcd1":"history4vlow = fit(5, 0.001, model, train_loader, val_loader)","419ee3eb":"# Replace these values with your results\nfigure(num=None, figsize=(18, 10), dpi=80, facecolor='w', edgecolor='k')\nhistoryvlow = [result0vlow] + history1vlow + history2vlow + history3vlow + history4vlow\naccuraciesvlow = [result['val_acc'] for result in historyvlow]\nplt.plot(accuracies, '-x', label='Base Model')\nplt.plot(accuraciesvlow, '-o', label='Small Validation set')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.legend(loc=\"lower right\")\nplt.grid()\nplt.title('Accuracy vs. No. of epochs');","15f4dd57":"# Replace these values with your results\nfigure(num=None, figsize=(18, 10), dpi=80, facecolor='w', edgecolor='k')\nhistoryvlow = [result0vlow] + history1vlow + history2vlow + history3vlow + history4vlow\nlossesvlow = [result['val_loss'] for result in historyvlow]\nplt.plot(losses, '-x', label='Base Model')\nplt.plot(lossesvlow, '-o', label='Small Validation set')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend(loc=\"lower right\")\nplt.grid()\nplt.title('Loss vs. No. of epochs');","36521367":"train_ds, val_ds = random_split(dataset, [36000, 24000])\nlen(train_ds), len(val_ds)","67e382fc":"batch_size = 128\n\ntrain_loader = DataLoader(train_ds, batch_size, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size)\n\nmodel = MnistModel()","9216d7c8":"result0vhigh = evaluate(model, val_loader)\nresult0vhigh","85f2b36f":"history1vhigh = fit(5, 0.001, model, train_loader, val_loader)","c48bb8a5":"history2vhigh = fit(5, 0.001, model, train_loader, val_loader)","0f574469":"history3vhigh = fit(5, 0.001, model, train_loader, val_loader)\n","c730cd93":"history4vhigh = fit(5, 0.001, model, train_loader, val_loader)","cceb3ab3":"# Replace these values with your results\nfigure(num=None, figsize=(18, 10), dpi=80, facecolor='w', edgecolor='k')\nhistoryvhigh = [result0vhigh] + history1vhigh + history2vhigh + history3vhigh + history4vhigh\naccuraciesvhigh = [result['val_acc'] for result in historyvhigh]\nplt.plot(accuracies, '-x', label='Base Model')\nplt.plot(accuraciesvlow, '-o', label='Small Validation set')\nplt.plot(accuraciesvhigh, '-^', label='Higl Validation set')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.legend(loc=\"lower right\")\nplt.grid()\nplt.title('Accuracy vs. No. of epochs');","df0bfaa2":"# Replace these values with your results\nfigure(num=None, figsize=(18, 10), dpi=80, facecolor='w', edgecolor='k')\nhistoryvhigh = [result0vhigh] + history1vhigh + history2vhigh + history3vhigh + history4vhigh\nlossesvhigh = [result['val_loss'] for result in historyvhigh]\nplt.plot(losses, '-x', label='Base Model')\nplt.plot(lossesvlow, '-o', label='Small Validation set')\nplt.plot(lossesvhigh, '-^', label='Higl Validation set')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend(loc=\"upper right\")\nplt.grid()\nplt.title('Loss vs. No. of epochs');","779fd116":"train_ds, val_ds = random_split(dataset, [50000, 10000])\nlen(train_ds), len(val_ds)\n\nbatch_size = 64\n\ntrain_loader = DataLoader(train_ds, batch_size, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size)\n\nmodel = MnistModel()\n\nresult0blow = evaluate(model, val_loader)\nresult0blow\n\nhistory1blow = fit(5, 0.001, model, train_loader, val_loader)\n\nhistory2blow = fit(5, 0.001, model, train_loader, val_loader)\n\nhistory3blow = fit(5, 0.001, model, train_loader, val_loader)\n\nhistory4blow = fit(5, 0.001, model, train_loader, val_loader)","c49b61ec":"# Replace these values with your results\nfigure(num=None, figsize=(18, 10), dpi=80, facecolor='w', edgecolor='k')\nhistoryblow = [result0blow] + history1blow + history2blow + history3blow + history4blow\naccuraciesblow = [result['val_acc'] for result in historyblow]\nplt.plot(accuracies, '-x', label='Base Model')\nplt.plot(accuraciesvlow, '-o', label='Small Validation set')\nplt.plot(accuraciesvhigh, '-^', label='Higl Validation set')\nplt.plot(accuraciesblow, '--', label='Low batch size')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.legend(loc=\"lower right\")\nplt.title('Accuracy vs. No. of epochs');\nplt.grid()","11c9977f":"# Replace these values with your results\nfigure(num=None, figsize=(18, 10), dpi=80, facecolor='w', edgecolor='k')\nhistoryblow = [result0blow] + history1blow + history2blow + history3blow + history4blow\nlossesblow = [result['val_loss'] for result in historyblow]\nplt.plot(losses, '-x', label='Base Model')\nplt.plot(lossesvlow, '-o', label='Small Validation set')\nplt.plot(lossesvhigh, '-^', label='Higl Validation set')\nplt.plot(lossesblow, '--', label='Low batch size')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend(loc=\"upper right\")\nplt.title('Loss vs. No. of epochs');\nplt.grid()","cc6a17fb":"train_ds, val_ds = random_split(dataset, [50000, 10000])\nlen(train_ds), len(val_ds)\n\nbatch_size = 512\n\ntrain_loader = DataLoader(train_ds, batch_size, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size)\n\nmodel = MnistModel()\n\nresult0bhigh = evaluate(model, val_loader)\nresult0bhigh\n\nhistory1bhigh = fit(5, 0.001, model, train_loader, val_loader)\n\nhistory2bhigh = fit(5, 0.001, model, train_loader, val_loader)\n\nhistory3bhigh = fit(5, 0.001, model, train_loader, val_loader)\n\nhistory4bhigh = fit(5, 0.001, model, train_loader, val_loader)","cbfa6323":"# Replace these values with your results\nfigure(num=None, figsize=(18, 10), dpi=80, facecolor='w', edgecolor='k')\nhistorybhigh = [result0bhigh] + history1bhigh + history2bhigh + history3bhigh + history4bhigh\naccuraciesbhigh = [result['val_acc'] for result in historybhigh]\nplt.plot(accuracies, '-x', label='Base Model')\nplt.plot(accuraciesvlow, '-o', label='Small Validation set')\nplt.plot(accuraciesvhigh, '-^', label='Higl Validation set')\nplt.plot(accuraciesblow, '--', label='Low batch size')\nplt.plot(accuraciesbhigh, '--', label='High batch size')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.legend(loc=\"lower right\")\nplt.title('Accuracy vs. No. of epochs');\nplt.grid()","dd8355be":"# Replace these values with your results\nfigure(num=None, figsize=(18, 10), dpi=80, facecolor='w', edgecolor='k')\nhistorybhigh = [result0bhigh] + history1bhigh + history2bhigh + history3bhigh + history4bhigh\nlossesbhigh = [result['val_loss'] for result in historybhigh]\nplt.plot(losses, '-x', label='Base Model')\nplt.plot(lossesvlow, '-o', label='Small Validation set')\nplt.plot(lossesvhigh, '-^', label='Higl Validation set')\nplt.plot(lossesblow, '--', label='Low batch size')\nplt.plot(lossesbhigh, '--', label='High batch size')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend(loc=\"lower right\")\nplt.grid()\nplt.title('Accuracy vs. No. of epochs');","92abf4e6":"## Split the dataset as train and validation (can also split as train, validation and test)","d3416eb1":"Total number of paramerters\/weights are 10 * 784 + 10 = 7850","13120334":"## High batch size (512)","368470aa":"## Experimenting with a small validation size (40%)","e1de1e42":"Different losses : https:\/\/ml-cheatsheet.readthedocs.io\/en\/latest\/loss_functions.html#:~:text=Cross%2Dentropy%20loss%2C%20or%20log,diverges%20from%20the%20actual%20label.","802ede4e":"## Downloading Data","88820d0b":"## Importing libraries","946bf32e":"There is a type size missmatch. We have given a [3584 x 28] to multiply with [784 x 10]. This is happening because we are passing a batch here. To solve this we need to reshape the input tensor.","20d145f5":"The unsqueeze method add a new dimension so the model see this as batch of data with batch size 1","7f854fdd":"## Low batch size 64","93f82336":"## Make a sample model","cb7c272b":"## Checking the result","53b2e24d":"## Experimenting with a small validation size (5%)","81872d77":"We have 10 outputs so 10 rows. We have 784 inputs so 784 columns","2e7f8715":"## Test loss and accuracy"}}