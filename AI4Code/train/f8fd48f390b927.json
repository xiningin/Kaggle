{"cell_type":{"45e7c76b":"code","4d451d8d":"code","481a65fd":"code","a73de644":"code","725b9c75":"code","a678d6b4":"code","46ffdee5":"code","e1d54951":"code","446c316c":"code","05282716":"code","2b10dbfa":"code","50c8f476":"code","d31de3e2":"code","26093007":"code","b54c3a6a":"code","16309764":"code","97f9b188":"code","5ede2218":"code","7279b4e6":"code","541f7fbc":"code","acaafec6":"code","95cc5d05":"code","fd92c2b8":"code","97d065a1":"code","eed8daa8":"code","0a51594b":"code","d1d43d8d":"code","93d275b1":"code","5240b1d7":"code","417fc88f":"code","ba73903c":"code","5dd96ce4":"code","23790572":"code","adda451d":"code","c4e94ee2":"code","e4c2f4ae":"code","54d8d58c":"code","a972bfdf":"code","4c321dca":"code","5cec3462":"code","8bf5a65a":"code","bc9798cb":"code","37e3b0ad":"code","05196072":"code","c3c64368":"code","b3256cb8":"code","66a89593":"code","310e2dc8":"code","84f705c5":"code","d8857911":"code","baca6f96":"code","c4f8fde9":"code","2f79220c":"code","f70c47e5":"code","affaf363":"code","e48cc9e5":"code","76a8532a":"code","3710a1a7":"code","6b869c74":"code","491971e3":"code","8baf49c7":"code","6f939db6":"code","7d8e6e77":"code","f2c932d4":"code","ef17fd9c":"code","6c8e12f6":"code","96aa2ce6":"code","e87640fa":"code","3914170e":"code","c744e84e":"code","069614d2":"code","75f05c31":"code","ef4c9710":"code","09f64b42":"code","ca165a5e":"code","087fdfec":"code","3bd8601d":"code","7ea4bb51":"code","6becc866":"code","28d9d0a5":"code","db548fae":"code","308af66c":"code","eeeea249":"code","9ead222d":"code","db0b77b6":"code","8f7d28dc":"code","4a640d3c":"code","12f3a58c":"code","bdd19f99":"code","ec468992":"code","62042edc":"code","d5ca15d7":"code","32a09753":"code","5ac32ac3":"code","966a8837":"code","7eef08b0":"code","7782d87d":"code","839f65c0":"code","61d58a17":"code","780a02f9":"code","498b2d12":"code","d71022e4":"code","7d5e302b":"code","6732d306":"code","52b2fa31":"code","e648479f":"code","068771fb":"code","7aec2f70":"code","b921b82b":"code","3f084ed3":"code","05e5d11e":"code","b2d9aa4f":"code","61b93893":"code","d6996391":"code","12d61212":"code","fc550bd9":"code","562055c7":"code","d3af8fa9":"code","aeedd398":"code","19c6a85a":"code","9e2e9155":"code","8c2e7ccc":"code","d5f052fa":"code","eff8f9a7":"code","f9b21421":"code","24da89ae":"code","55523fe1":"code","9e4905a1":"code","a1c10877":"code","5353897a":"code","1e9fdeb8":"code","8877896c":"code","105b27bc":"code","3822836b":"code","c92c4f8e":"code","41e8b91a":"code","638c6c8a":"code","c08409f0":"code","4a06c096":"code","91164c98":"code","0ea2856e":"code","9ee229cb":"code","fcd9f066":"code","a66f87c8":"code","0c1db6f6":"markdown","67122d03":"markdown","9569b4ea":"markdown","3fb9e972":"markdown","34bf9fe2":"markdown","9efc0688":"markdown","c66276de":"markdown","b9690c50":"markdown","93f9ae9b":"markdown","4a978695":"markdown","eae8a45a":"markdown","853fb581":"markdown","073d6992":"markdown","8fcb1712":"markdown","af579780":"markdown","07384636":"markdown","157179de":"markdown","59ce3ee7":"markdown","fc8f3cd4":"markdown","05863327":"markdown","5c895348":"markdown","270ac8f1":"markdown","6a707fd7":"markdown","39f722e4":"markdown","df5c4b66":"markdown","aaab58ef":"markdown","864e7249":"markdown","311bfc40":"markdown","d3a33846":"markdown","8770b4b7":"markdown","13adba05":"markdown","527234e6":"markdown","b8509ed2":"markdown","b9bc936f":"markdown","2cfe2145":"markdown","db870da5":"markdown","a78cef94":"markdown","77593801":"markdown","1f8ad663":"markdown","2ff0a4f2":"markdown","147bc2ca":"markdown","542906f4":"markdown","690bd590":"markdown","2a7d248f":"markdown","013a3cfe":"markdown","bc14f30b":"markdown","59ebd4ef":"markdown","f83878f0":"markdown","29b5e588":"markdown","49bc749a":"markdown","cfb55de6":"markdown","2e180420":"markdown","aea3b507":"markdown","e85f3c66":"markdown","2ba5f9eb":"markdown","0cd624c0":"markdown","7ce1e2d3":"markdown","66664631":"markdown","ee8744d4":"markdown","8d9ba0ba":"markdown","dc44f591":"markdown","fc3ebbd5":"markdown","928855d0":"markdown","f8949e68":"markdown","70a18315":"markdown"},"source":{"45e7c76b":"#Installation of required libraries\nimport numpy as np\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import scale \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import model_selection\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn import neighbors\nfrom sklearn.svm import SVR\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","4d451d8d":"#Reading the dataset\ndf = pd.read_csv(\"..\/input\/hitters\/Hitters.csv\")","481a65fd":"# The first 5 observation units of the data set were accessed.\ndf.head()","a73de644":"# The size of the data set was examined. It consists of 322 observation units and 20 variables.\ndf.shape","725b9c75":"#Feature information\ndf.info()","a678d6b4":"# Number of unique observations in variables\ndf.nunique()","46ffdee5":"# Because League is a categorical variable, we examined how many of the classes there are.\ndf[\"League\"].value_counts()","e1d54951":"df.League.value_counts().plot.barh();","446c316c":"# Since NewLeague is a categorical variable, we have examined how many of the classes there are.\ndf[\"NewLeague\"].value_counts()","05282716":"df.NewLeague.value_counts().plot.barh();","2b10dbfa":"# Since Division is a categorical variable, we have examined how many of the classes there are.\ndf[\"Division\"].value_counts()","50c8f476":"df.Division.value_counts().plot.barh();","d31de3e2":"#  The maximum value of the Salary variable has been reached. It is also our dependent variable.\ndf[\"Salary\"].max()","26093007":"# The minimum value of the Salary variable has been reached.\ndf[\"Salary\"].min()","b54c3a6a":"#Distribution of dependent variable\nimport seaborn as sns\nsns.distplot(df.Salary);","16309764":"# Descriptive statistics of the data set accessed.\ndf.describe().T","97f9b188":"# Access to the correlation of the data set was provided. What kind of relationship is examined between the variables. \n# If the correlation value is> 0, there is a positive correlation. While the value of one variable increases, the value of the other variable also increases.\n# Correlation = 0 means no correlation.\n# If the correlation is <0, there is a negative correlation. While one variable increases, the other variable decreases. \n# When the correlations are examined, there are 2 variables that act as a positive correlation to the Salary dependent variable.\n# These variables are CRBI and CRuns. As these increase, Salary variable increases.\ndf.corr()","5ede2218":"# Correlation matrix graph of the data set\nf, ax = plt.subplots(figsize= [20,15])\nsns.heatmap(df.corr(), annot=True, fmt=\".2f\", ax=ax, cmap = \"magma\" )\nax.set_title(\"Correlation Matrix\", fontsize=20)\nplt.show()","7279b4e6":"# According to the league variable, groupby is made and how much salary is taken in which league on average.\ndf.groupby(\"League\").agg({\"Salary\": \"mean\"})","541f7fbc":"# In 1987, the average salary of the new leagues was examined.\ndf.groupby(\"NewLeague\").agg({\"Salary\": \"mean\"})","acaafec6":"# According to the position of the player, how much salary is on average, this is examined.\ndf.groupby(\"Division\").agg({\"Salary\": \"mean\"})","95cc5d05":"# When the correlation of the data set was examined, the CRBI variable had a positive correlation.\n# As this variable increases, it should increase in Salary variable.\n# Based on this, the CRBI variable is the number of innings he has made during his career.\n# The mean of the Salary variable was examined by grouping it accordingly.\ndf.groupby(\"CRBI\").agg({\"Salary\": \"mean\"})","fd92c2b8":"# When the correlation of the data set was examined, the CRuns variable had a positive correlation.\n# As this variable increases, it should increase in Salary variable.\n# Based on this, the CRuns variable is the number of innings he has made during his career.\n# The mean of the Salary variable was examined by grouping it accordingly.\ndf.groupby(\"CRuns\").agg({\"Salary\": \"mean\"})","97d065a1":"# What is the average salary based on League and Years?\n# What is the average salary the player receives, grouped by league and total career years? \ndf.groupby([\"League\",\"Years\"]).agg({\"Salary\": \"mean\"})","eed8daa8":"# What is the average salary based on the league he played in 1987 and the duration of his career,grouped by NewLeague and Years variables?\n#  What is the average salary the player receives, grouped by new league and total career duration?\ndf.groupby([\"NewLeague\",\"Years\"]).agg({\"Salary\": \"mean\"})","0a51594b":"# What is the average salary the players receive based on the region they play in?\ndf.groupby([\"Division\",\"Years\"]).agg({\"Salary\": \"mean\"})","d1d43d8d":"# The observation units with the highest number of hits during his career were accessed by ranking the data set in descending order.  \ndf.sort_values(\"CHits\",ascending = False)","93d275b1":"# He was grouped by leagues and reached the maximum number of hits made during his career.\ndf.groupby(\"League\").agg({\"CHits\": \"max\"})","5240b1d7":"# Players are grouped according to the region they play and the maximum number of hits is reached during their career.\ndf.groupby(\"Division\").agg({\"CHits\": \"max\"})","417fc88f":"# The maximum hit values in 1986 were reached by grouping them according to the league variable.\ndf.groupby(\"League\").agg({\"Hits\": \"max\"})","ba73903c":"# The maximum number of innings with a baseball bat was reached in 1986, grouped by league variable.\ndf.groupby(\"League\").agg({\"AtBat\": \"max\"})","5dd96ce4":"# Average values of career duration were grouped according to the league variable.\ndf.groupby(\"League\").agg({\"Years\": \"mean\"})","23790572":"# The maximum values of career duration were reached by grouping by league variable.\ndf.groupby(\"League\").agg({\"Years\": \"max\"})","adda451d":"# The average values of how many errors are made in which league are grouped according to the league variable.\ndf.groupby(\"League\").agg({\"Errors\": \"mean\"})","c4e94ee2":"# The maximum values of how many errors are made in the league are reached by grouping them according to the league variable.\ndf.groupby(\"League\").agg({\"Errors\": \"max\"})","e4c2f4ae":"df.groupby(\"League\").agg({\"PutOuts\": \"max\"})","54d8d58c":"# The maximum number of assists in leagues is reached by grouping them according to the league variable.\ndf.groupby(\"League\").agg({\"Assists\": \"max\"})","a972bfdf":"df.groupby(\"Years\").agg({\"CAtBat\": \"max\"})","4c321dca":"df.groupby([\"League\", \"Years\"]).agg({\"CAtBat\": \"max\"})","5cec3462":"df.groupby(\"League\").agg({\"CAtBat\": \"mean\"})","8bf5a65a":"# The variables related to their careers were divided into career years and new values were created in the data set by obtaining average values.\ndf[\"OrtCAtBat\"] = df[\"CAtBat\"] \/ df[\"Years\"]\ndf[\"OrtCHits\"] = df[\"CHits\"] \/ df[\"Years\"]\ndf[\"OrtCHmRun\"] = df[\"CHmRun\"] \/ df[\"Years\"]\ndf[\"OrtCruns\"] = df[\"CRuns\"] \/ df[\"Years\"]\ndf[\"OrtCRBI\"] = df[\"CRBI\"] \/ df[\"Years\"]\ndf[\"OrtCWalks\"] = cwalks = df[\"CWalks\"] \/ df[\"Years\"]\ndf.head()","bc9798cb":"# Based on some trials and correlation results, we subtract variables that do not contribute to the model from our data set.\ndf = df.drop(['AtBat','Hits','HmRun','Runs','RBI','Walks','Assists','Errors',\"PutOuts\",'League','NewLeague', 'Division'], axis=1)","37e3b0ad":"# access to the first 5 observation units of the df was provided.\ndf.head()","05196072":"#How many missing values?\ndf.isnull().sum()","c3c64368":"# Have been visualized using the missingno library for the visualization of missing observations.\nimport missingno as msno\nmsno.bar(df);","b3256cb8":"df_eksik = df[df[\"Salary\"].isnull()].head()\ndf_eksik","66a89593":"#We fill in the missing observations with the KNN method.\nfrom sklearn.impute import KNNImputer\nimputer = KNNImputer(n_neighbors = 4)\ndf_filled = imputer.fit_transform(df)","310e2dc8":"df = pd.DataFrame(df_filled,columns = df.columns)\ndf.isnull().sum()","84f705c5":"# The process of visualizing the Salary variable with boxplot method was done. We find the outlier observations on the chart.\nimport seaborn as sns\nsns.boxplot(x = df[\"Salary\"]);","d8857911":"df[\"Salary\"].describe()","baca6f96":"# First, when the quarters of 1% and 99% were examined, there were no outlier.\n# Later when the quarters of 5% and 95% were examined, there were no outlier.\n# Finally, when 25% quarters and 75% quarters were examined, outlier observations were found.\n\nfor feature in df:\n\n    Q1 = df[feature].quantile(0.25)\n    Q3 = df[feature].quantile(0.75)\n    IQR = Q3-Q1\n    upper = Q3 + 1.5*IQR\n    lower = Q1 - 1.5*IQR\n\n    if df[(df[feature] > upper) | (df[feature] < lower)].any(axis=None):\n        print(feature,\"yes\")\n        print(df[(df[feature] > upper) | (df[feature] < lower)].shape[0])\n    else:\n        print(feature, \"no\")","c4f8fde9":"#We conduct a stand alone observation review for the salary variable\n#We suppress contradictory values\nQ1 = df.Salary.quantile(0.25)\nQ3 = df.Salary.quantile(0.75)\nIQR = Q3-Q1\nlower = Q1 - 1.5*IQR\nupper = Q3 + 1.5*IQR\ndf.loc[df[\"Salary\"] > upper,\"Salary\"] = upper","2f79220c":"import seaborn as sns\nsns.boxplot(x = df[\"Salary\"]);","f70c47e5":"#We determine outliers between all variables with the LOF method\nfrom sklearn.neighbors import LocalOutlierFactor\nlof =LocalOutlierFactor(n_neighbors= 10)\nlof.fit_predict(df)","affaf363":"df_scores = lof.negative_outlier_factor_\nnp.sort(df_scores)[0:30]","e48cc9e5":"#We choose the threshold value according to lof scores\nthreshold = np.sort(df_scores)[7]\nthreshold","76a8532a":"#We delete those that are higher than the threshold\noutlier = df_scores > threshold\ndf = df[outlier]","3710a1a7":"# The size of the data set was examined.\ndf.shape","6b869c74":"y = df[\"Salary\"]\nX = df.drop(\"Salary\",axis=1)","491971e3":"X.head()","8baf49c7":"#Feature Selection\n#Wrapper Method\n#Backward Elimination\n#https:\/\/towardsdatascience.com\/feature-selection-with-pandas-e3690ad8504b\ncols = X.columns\n\nimport statsmodels.api as sm\nfrom sklearn.feature_selection import RFE\n#Backward Elimination\ncols = list(X.columns)\npmax = 1\nwhile (len(cols)>0):\n    p= []\n    X_1 = X[cols]\n    X_1 = sm.add_constant(X_1)\n    model = sm.OLS(y,X_1).fit()\n    p = pd.Series(model.pvalues.values[1:],index = cols)      \n    pmax = max(p)\n    feature_with_p_max = p.idxmax()\n    if(pmax>0.05):\n        cols.remove(feature_with_p_max)\n    else:\n        break\nselected_features_BE = cols\nprint(selected_features_BE)","6f939db6":"X = df[selected_features_BE]\nX.head()","7d8e6e77":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.20, \n                                                    random_state=46)","f2c932d4":"knn_model = KNeighborsRegressor().fit(X_train, y_train)","ef17fd9c":"y_pred = knn_model.predict(X_test)\nknn_base = np.sqrt(mean_squared_error(y_test, y_pred))\nknn_base","6c8e12f6":"knn_params = {\"n_neighbors\": np.arange(2,30,1)}\n\nknn_model = KNeighborsRegressor()\n\nknn_cv_model = GridSearchCV(knn_model, knn_params, cv = 10).fit(X_train, y_train)","96aa2ce6":"knn_cv_model.best_params_","e87640fa":"knn_tuned = KNeighborsRegressor(**knn_cv_model.best_params_).fit(X_train, y_train)","3914170e":"y_pred = knn_tuned.predict(X_test)\nknn_final = np.sqrt(mean_squared_error(y_test, y_pred))\nknn_final","c744e84e":"svr_model = SVR().fit(X_train, y_train)","069614d2":"y_pred = svr_model.predict(X_test)\nsvr_base = np.sqrt(mean_squared_error(y_test, y_pred))\nsvr_base","75f05c31":"svr_model = SVR() \n\nsvr_params = {\"C\": [0.01,0.001, 0.2, 0.1,0.5,0.8,0.9,1, 10, 100, 500,1000]}\n\nsvr_cv_model = GridSearchCV(svr_model, svr_params, cv = 10, n_jobs = -1, verbose =  2).fit(X_train, y_train)","ef4c9710":"svr_cv_model.best_params_","09f64b42":"svr_tuned = SVR(**svr_cv_model.best_params_).fit(X_train, y_train)","ca165a5e":"y_pred = svr_tuned.predict(X_test)\nsvr_final = np.sqrt(mean_squared_error(y_test, y_pred))\nsvr_final","087fdfec":"cart_model = DecisionTreeRegressor()\ncart_model.fit(X_train, y_train)","3bd8601d":"y_pred = cart_model.predict(X_test)\ncart_base = np.sqrt(mean_squared_error(y_test, y_pred))\ncart_base","7ea4bb51":"cart_model = DecisionTreeRegressor()","6becc866":"cart_params = {\"max_depth\": [2,3,4,5,10,20,100, 1000],\n              \"min_samples_split\": [2,10,5,30,50,10]}","28d9d0a5":"cart_cv_model = GridSearchCV(cart_model, cart_params, cv = 10, n_jobs = -1, verbose =  2).fit(X_train, y_train)","db548fae":"cart_cv_model.best_params_","308af66c":"cart_tuned = DecisionTreeRegressor(**cart_cv_model.best_params_).fit(X_train, y_train)","eeeea249":"y_pred = cart_tuned.predict(X_test)\ncart_final = np.sqrt(mean_squared_error(y_test, y_pred))\ncart_final","9ead222d":"rf_model = RandomForestRegressor(random_state = 42).fit(X_train, y_train)","db0b77b6":"y_pred = rf_model.predict(X_test)\nrf_base = np.sqrt(mean_squared_error(y_test, y_pred))\nrf_base","8f7d28dc":"rf_params = {\"max_depth\": [5,10,None],\n            \"max_features\": [2,5,10],\n            \"n_estimators\": [100, 500, 900],\n            \"min_samples_split\": [2,10,30]}","4a640d3c":"rf_cv_model = GridSearchCV(rf_model, rf_params, cv = 10, n_jobs = -1, verbose = 2).fit(X_train , y_train)","12f3a58c":"rf_cv_model.best_params_","bdd19f99":"rf_tuned = RandomForestRegressor(**rf_cv_model.best_params_).fit(X_train, y_train)","ec468992":"y_pred = rf_tuned.predict(X_test)\nrf_final = np.sqrt(mean_squared_error(y_test, y_pred))\nrf_final","62042edc":"rf_tuned.feature_importances_","d5ca15d7":"Importance = pd.DataFrame({'Importance':rf_tuned.feature_importances_*100}, \n                          index = ['CRuns', 'CWalks', 'OrtCAtBat', 'OrtCHits', 'OrtCRBI', 'OrtCWalks'])\n\nImportance.sort_values(by = 'Importance', \n                       axis = 0, \n                       ascending = True).plot(kind = 'barh', \n                                              color = 'r', )\n\nplt.xlabel('Variable Importance')\nplt.gca().legend_ = None","32a09753":"gbm_model = GradientBoostingRegressor().fit(X_train, y_train)","5ac32ac3":"y_pred = gbm_model.predict(X_test)\ngbm_base = np.sqrt(mean_squared_error(y_test, y_pred))\ngbm_base","966a8837":"gbm_params = {\"learning_rate\": [0.001,0.1,0.01],\n             \"max_depth\": [3,5,8,10],\n             \"n_estimators\": [200,500,1000],\n             \"subsample\": [1,0.5,0.8],\n             \"loss\": [\"ls\",\"lad\",\"quantile\"]}","7eef08b0":"gbm_model = GradientBoostingRegressor().fit(X_train, y_train)","7782d87d":"gbm_cv_model = GridSearchCV(gbm_model, \n                            gbm_params, \n                            cv = 10, \n                            n_jobs=-1, \n                            verbose = 2).fit(X_train, y_train)","839f65c0":"gbm_cv_model.best_params_","61d58a17":"gbm_tuned = GradientBoostingRegressor(**gbm_cv_model.best_params_).fit(X_train, y_train)","780a02f9":"y_pred = gbm_tuned.predict(X_test)\ngbm_final = np.sqrt(mean_squared_error(y_test, y_pred))\ngbm_final","498b2d12":"Importance = pd.DataFrame({'Importance':gbm_tuned.feature_importances_*100}, \n                          index = ['CRuns', 'CWalks', 'OrtCAtBat', 'OrtCHits', 'OrtCRBI', 'OrtCWalks'])\n\nImportance.sort_values(by = 'Importance', \n                       axis = 0, \n                       ascending = True).plot(kind = 'barh', \n                                              color = 'r', )\n\nplt.xlabel('Variable Importance')\nplt.gca().legend_ = None","d71022e4":"import xgboost\nfrom xgboost import XGBRegressor","7d5e302b":"xgb = XGBRegressor().fit(X_train, y_train)","6732d306":"y_pred = xgb.predict(X_test)\nxgb_base = np.sqrt(mean_squared_error(y_test, y_pred))\nxgb_base","52b2fa31":"xgb_params = {\"learning_rate\": [0.1,0.01,1],\n             \"max_depth\": [2,5,8],\n             \"n_estimators\": [100,500,1000],\n             \"colsample_bytree\": [0.3,0.6,1]}","e648479f":"xgb_cv_model  = GridSearchCV(xgb,xgb_params, cv = 10, n_jobs = -1, verbose = 2).fit(X_train, y_train)","068771fb":"xgb_cv_model.best_params_","7aec2f70":"xgb_tuned = XGBRegressor(**xgb_cv_model.best_params_).fit(X_train, y_train)","b921b82b":"y_pred = xgb_tuned.predict(X_test)\nxgb_final = np.sqrt(mean_squared_error(y_test, y_pred))\nxgb_final","3f084ed3":"Importance = pd.DataFrame({'Importance':xgb_tuned.feature_importances_*100}, \n                          index = ['CRuns', 'CWalks', 'OrtCAtBat', 'OrtCHits', 'OrtCRBI', 'OrtCWalks'])\n\nImportance.sort_values(by = 'Importance', \n                       axis = 0, \n                       ascending = True).plot(kind = 'barh', \n                                              color = 'r', )\n\nplt.xlabel('Variable Importance')\nplt.gca().legend_ = None","05e5d11e":"from lightgbm import LGBMRegressor","b2d9aa4f":"lgb_model = LGBMRegressor().fit(X_train, y_train)","61b93893":"y_pred = lgb_model.predict(X_test)\nlbg_base = np.sqrt(mean_squared_error(y_test, y_pred))\nlbg_base","d6996391":"lgb_model = LGBMRegressor()","12d61212":"lgbm_params = {\"learning_rate\": [0.01, 0.1, 1],\n              \"n_estimators\": [200,1000,10000],\n              \"max_depth\": [2,5,10],\n              \"colsample_bytree\": [1,0.5,0.3]}","fc550bd9":"lgbm_cv_model = GridSearchCV(lgb_model, \n                             lgbm_params, \n                             cv = 10, \n                             n_jobs = -1, \n                             verbose =2).fit(X_train, y_train)","562055c7":"lgbm_cv_model.best_params_","d3af8fa9":"lgbm_tuned = LGBMRegressor(**lgbm_cv_model.best_params_).fit(X_train, y_train)","aeedd398":"y_pred = lgbm_tuned.predict(X_test)\nlbg_final = np.sqrt(mean_squared_error(y_test, y_pred))\nlbg_final","19c6a85a":"Importance = pd.DataFrame({'Importance':lgbm_tuned.feature_importances_*100}, \n                          index = ['CRuns', 'CWalks', 'OrtCAtBat', 'OrtCHits', 'OrtCRBI', 'OrtCWalks'])\n\nImportance.sort_values(by = 'Importance', \n                       axis = 0, \n                       ascending = True).plot(kind = 'barh', \n                                              color = 'r', )\n\nplt.xlabel('Variable Importance')\nplt.gca().legend_ = None","9e2e9155":"from catboost import CatBoostRegressor","8c2e7ccc":"catb_model = CatBoostRegressor(verbose = False).fit(X_train, y_train)","d5f052fa":"y_pred = catb_model.predict(X_test)\ncat_base = np.sqrt(mean_squared_error(y_test, y_pred))\ncat_base","eff8f9a7":"catb_params = {\"iterations\": [500,1000,10000],\n              \"learning_rate\": [0.01,0.1,1],\n              \"depth\": [2,6,10]}","f9b21421":"catb_model = CatBoostRegressor()","24da89ae":"catb_cv_model = GridSearchCV(catb_model, \n                           catb_params, \n                           cv = 5, \n                           n_jobs = -1, \n                           verbose = 2).fit(X_train, y_train)","55523fe1":"catb_cv_model.best_params_","9e4905a1":"catb_tuned = CatBoostRegressor(**catb_cv_model.best_params_, verbose = False).fit(X_train, y_train)","a1c10877":"y_pred = catb_tuned.predict(X_test)\ncat_final = np.sqrt(mean_squared_error(y_test, y_pred))\ncat_final","5353897a":"Importance = pd.DataFrame({'Importance':catb_tuned.feature_importances_*100}, \n                          index = ['CRuns', 'CWalks', 'OrtCAtBat', 'OrtCHits', 'OrtCRBI', 'OrtCWalks'])\n\nImportance.sort_values(by = 'Importance', \n                       axis = 0, \n                       ascending = True).plot(kind = 'barh', \n                                              color = 'r', )\n\nplt.xlabel('Variable Importance')\nplt.gca().legend_ = None","1e9fdeb8":"scaler = StandardScaler()","8877896c":"scaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)","105b27bc":"scaler.fit(X_test)\nX_test_scaled = scaler.transform(X_test)","3822836b":"mlp_model = MLPRegressor().fit(X_train_scaled, y_train)","c92c4f8e":"y_pred = mlp_model.predict(X_test_scaled)\nneural_base = np.sqrt(mean_squared_error(y_test, y_pred))\nneural_base","41e8b91a":"mlp_params = {\"alpha\": [0.1, 0.01, 0.02, 0.001, 0.0001], \n             \"hidden_layer_sizes\": [(10,20), (5,5), (100,100), (1000,100,10)]}","638c6c8a":"mlp_cv_model = GridSearchCV(mlp_model, mlp_params, cv = 10, verbose = 2, n_jobs = -1).fit(X_train_scaled, y_train)","c08409f0":"mlp_cv_model.best_params_","4a06c096":"mlp_tuned = MLPRegressor(**mlp_cv_model.best_params_).fit(X_train_scaled, y_train)","91164c98":"y_pred = mlp_tuned.predict(X_test)\nneural_final = np.sqrt(mean_squared_error(y_test, y_pred))\nneural_final","0ea2856e":" models = [\n    knn_tuned,\n    svr_tuned,\n    cart_tuned,\n    rf_tuned,\n    gbm_tuned,\n    xgb_tuned,\n    lgbm_tuned,\n    catb_tuned,\n    mlp_tuned]\n\n\nfor model in models:\n    isimler = model.__class__.__name__\n    y_pred = model.predict(X_test)\n    error = np.sqrt(mean_squared_error(y_test, y_pred))\n    print(\"-\"*28)\n    print(isimler + \":\" )\n    print(\"Error:\" + str(error))","9ee229cb":"sonuc = []\n\nsonuclar = pd.DataFrame(columns= [\"Models\",\"Error\"])\n\nfor model in models:\n    isimler = model.__class__.__name__\n    y_pred = model.predict(X_test)\n    hata = np.sqrt(mean_squared_error(y_test, y_pred))    \n    sonuc = pd.DataFrame([[isimler, hata]], columns= [\"Models\",\"Error\"])\n    sonuclar = sonuclar.append(sonuc)\n    \n    \nsns.barplot(x= 'Error', y = 'Models', data=sonuclar, color=\"r\")\nplt.xlabel('Error')\nplt.title('Error Rate of Models');  ","fcd9f066":"sonuclar_df = pd.DataFrame({\"Base Model Test Error\":[knn_base, svr_base, cart_base,rf_base, gbm_base, xgb_base, lbg_base, cat_base, neural_base],\n                            \"Test Tuned Error\":[knn_final, svr_final, cart_final, rf_final,gbm_final, xgb_final, lbg_final, cat_final, neural_final]})\nsonuclar_df.index= [\"KNN\", \"SVR\",\"CART\",\"Random Forests\",\"GBM\",\"XGBoost\", \"LightGBM\", \"CatBoost\", \"Neural Networks\"]","a66f87c8":"sonuclar_df","0c1db6f6":"## 3) CART","67122d03":"### 6.4) Final Model Installation","9569b4ea":"# 5) Reporting\n\nThe goal in this study was to establish nonlinear regression models for the hitters data set and examine and minimize error scores on 9 models. The works performed are as follows:\n\n#### **1).** Hitters Data Set read.\n#### **2).** With Exploratory Data Analysis;\n* the data set's structural data were checked.\n* The types of variables in the dataset were examined.\n* Size information of the dataset was accessed.\n* How many missing observations were obtained from which variable in the data set. Only the dependent variable \"Salary\" was observed to have 59 missing observations.\n* Descriptive statistics of the data set were examined.\n\n#### **3).** Data Preprocessing section; \n* **df for:**The NA values were filled with the KNN algorithm, the Outliers were determined by LOF and dropped. The X variables were normalized.\n\n#### **4).** During Model Building;\n\nKNN, SVR, CART, Random Forests, GBM, XGBoost, LightGBM, CatBoost, Neural Networks using machine learning models, **RMSE** values representing the difference between real values and predicted values were calculated. Later KNN, SVR, CART, Random Forests, GBM, XGBoost, LightGBM, CatBoost, Neural Networks hyperparameter optimizations were applied to further reduce the error value.\n\n#### **5).** Result;\n\nThe model created as a result of GBM Hyperparameter optimization became the model with the lowest RMSE value. (114)**","3fb9e972":"### 4.1) Model Installation ","34bf9fe2":"### 1.1) Model Installation","9efc0688":"# 1) Exploratory Data Analysis","c66276de":"### 3.4) Final Model Installation","b9690c50":"### 2) Missing Observation Analysis","93f9ae9b":"### 7.4) Final Model Installation","4a978695":"# Salary Predict with Nonlinear Regression Models in Hitters Dataset\n\nThis dataset was originally taken from the StatLib library which is maintained at Carnegie Mellon University. This is part of the data that was used in the 1988 ASA Graphics Section Poster Session. The salary data were originally from Sports Illustrated, April 20, 1987. The 1986 and career statistics were obtained from The 1987 Baseball Encyclopedia Update published by Collier Books, Macmillan Publishing Company, New York.\n\n![beyzbol.jpg](attachment:beyzbol.jpg)\n\n**Nonlinear Regression Models:**\n- **KNN**\n- **Support Vector Regression (SVR)**\n- **CART**\n- **Random Forests (RF)**\n- **Gradient Boosting Machines (GBM)**\n- **eXtreme Gradient Boosting (XGBoost)**\n- **LightGBM**\n- **CatBoost**\n- **Neural Networks**\n\n\n- A data frame with 322 observations of major league players on the following 20 variables.\n- AtBat Number of times at bat in 1986\n- Hits Number of hits in 1986\n- HmRun Number of home runs in 1986\n- Runs Number of runs in 1986\n- RBI Number of runs batted in in 1986\n- Walks Number of walks in 1986\n- Years Number of years in the major leagues\n- CAtBat Number of times at bat during his career\n- CHits Number of hits during his career\n- CHmRun Number of home runs during his career\n- CRuns Number of runs during his career\n- CRBI Number of runs batted in during his career\n- CWalks Number of walks during his career\n- League A factor with levels A and N indicating player\u2019s league at the end of 1986\n- Division A factor with levels E and W indicating player\u2019s division at the end of 1986\n- PutOuts Number of put outs in 1986\n- Assists Number of assists in 1986\n- Errors Number of errors in 1986\n- Salary 1987 annual salary on opening day in thousands of dollars\n- NewLeague A factor with levels A and N indicating player\u2019s league at the beginning of 1987\n\n**Number of Observation Units: 322**\n\n**Variable Number: 20**\n\n**In the context of the results obtained, the GBM algorithm gives the least error:**\n\n![vvv.PNG](attachment:vvv.PNG)\n\n![aaaa.PNG](attachment:aaaa.PNG)","eae8a45a":"## 1) KNN","853fb581":"### 9.4) Final Model Installation","073d6992":"### 2.1) Filling of Missing Observation Units","8fcb1712":"### 2.5) Final Model Test Error","af579780":"### 9.1) Model Installation","07384636":"### 3.1) Model Installation","157179de":"### 1.4) Final Model Installation","59ce3ee7":"# 4) Comparison Of Models","fc8f3cd4":"### 2.3) Model Tuning","05863327":"### 1.2) Test Error","5c895348":"### 5.3) Model Tuning","270ac8f1":"### 4.4) Final Model Installation","6a707fd7":"### 6.1) Model Installation","39f722e4":"### 2.4) Final Model Installation","df5c4b66":"### 6.5) Final Model Test Error","aaab58ef":"### 3.2) Test Error","864e7249":"### 3.1) Local Outlier Factor (LOF)","311bfc40":"### 3) Outlier Observation Analysis","d3a33846":"### 8.2) Test Error","8770b4b7":"### 5.1) Model Installation","13adba05":"# 3) Modeling with Nonlinear Regression Models\n\n- **KNN**\n- **Support Vector Regression (SVR)**\n- **CART**\n- **Random Forests (RF)**\n- **Gradient Boosting Machines (GBM)**\n- **eXtreme Gradient Boosting (XGBoost)**\n- **LightGBM**\n- **CatBoost**\n- **Neural Networks**\n\nEach model can be modeled with the whole data set, but this has a disadvantage. The model will produce bad results in data it has never seen before. By separating the data set as train and test, a model installation process will be performed and error values will be observed. If you want, you can also build models with all data.","527234e6":"## 9) Neural Networks","b8509ed2":"### 4.3) Model Tuning","b9bc936f":"### 4.5) Final Model Test Error","2cfe2145":"## 5) GBM","db870da5":"## 2)SVR","a78cef94":"# 2) Data Preprocessing","77593801":"### 5.2) Test Error","1f8ad663":"### 7.5) Final Model Test Error","2ff0a4f2":"### 1.3) Model Tuning","147bc2ca":"### 9.2) Test Error","542906f4":"### 1.5) Final Model Test Error","690bd590":"### 3.3) Model Tuning","2a7d248f":"### 5.4) Final Model Installation","013a3cfe":"## 4) Random Forests","bc14f30b":"### 3.5) Final Model Test Error","59ebd4ef":"### 9.3) Model Tuning","f83878f0":"### 7.3) Model Tuning","29b5e588":"## 8) CatBoost","49bc749a":"### 8.4) Final Model Installation","cfb55de6":"### 8.3) Model Tuning","2e180420":"### 2.2) Test Error","aea3b507":"### 1) Creating New Variables\n\nCreating new variables is important for models. But you need to create a logical new variable. This data set contains variables linked to the player's career and includes career duration. In this case **career-related variables \/ career duration** is done, a logical variable can be created by obtaining average values for the player. We'll be addressing that in the process below.","e85f3c66":"### 6.3) Model Tuning","2ba5f9eb":"### 8.5) Final Model Test Error","0cd624c0":"### 6.2) Test Error","7ce1e2d3":"### 8.1) Model Installation","66664631":"### 7.1) Model Installation","ee8744d4":"### 9.5) Final Model Test Error","8d9ba0ba":"## 7) LightGBM","dc44f591":"### 5.5) Final Model Test Error","fc3ebbd5":"### 2.1) Model Installation","928855d0":"### 4.2) Test Error","f8949e68":"## 6) XGBoost","70a18315":"### 7.2) Test Error"}}