{"cell_type":{"d4180fef":"code","ac2798e8":"code","3644e0ec":"code","c46d2e7c":"code","8d5a09f8":"code","32c4fcd2":"code","23988790":"code","ea3dc2ef":"code","78a922d4":"code","dcc573d5":"code","c110cc70":"code","240e3d63":"code","8d4c2851":"code","401900d2":"code","020dc104":"code","d4d392e0":"code","d9d5a4e2":"code","1f719d72":"code","596321fa":"code","8c00213e":"code","44673cca":"code","b0ff1d1d":"code","04959063":"code","af803311":"code","42fa305d":"code","afaffa5b":"code","8f8195b1":"code","8b7dabd8":"code","9f591111":"code","43a1ec74":"code","394db3c9":"code","8d8df4cb":"code","1658ea0f":"code","6caa02ec":"code","7db7ec8f":"code","313d61a2":"code","bc0136c0":"code","ece114aa":"code","e536ffe0":"code","bed26650":"code","a2578a4e":"code","eabeddf6":"code","cdf0835a":"code","83099f62":"code","f3ebc405":"code","24265dce":"code","ad6936bc":"code","79938946":"code","1bdbb96b":"code","39ec8f7c":"code","4a732f87":"code","3a2695a3":"code","b3b1c91c":"code","66295a81":"code","bd21e98c":"code","eadf952e":"code","4ba2abdc":"markdown","80c9d9f6":"markdown","8c3d223c":"markdown","7aced878":"markdown","cffb8cd3":"markdown","b2c0ad2a":"markdown","16a10866":"markdown","b6bcd2e4":"markdown","0c81680a":"markdown","01c3bc2c":"markdown","1e72e4a0":"markdown","d94c8fff":"markdown","4239fc88":"markdown","dccbdcde":"markdown","4b29c17c":"markdown","e47b87ff":"markdown","d25c729b":"markdown","0b7aa2d5":"markdown","eae5af80":"markdown","b78a9a4c":"markdown","8271a5a5":"markdown","48450d9c":"markdown","30d22b0a":"markdown","3c57de2a":"markdown","7b65f8aa":"markdown","e9b75b7d":"markdown","de9dd568":"markdown","119a3da5":"markdown","f41ca4bd":"markdown","c8410cb5":"markdown","ca96778d":"markdown","cf4e03d7":"markdown","0defe45a":"markdown","022b4278":"markdown","29aad7fe":"markdown","c6edb29b":"markdown","5a64223e":"markdown","edc47253":"markdown","90fa58a5":"markdown","bfd00b6b":"markdown","d214a1f4":"markdown","060c1c88":"markdown","6468aa1f":"markdown","1ad3a27d":"markdown","8d3b7651":"markdown","985b8c33":"markdown","9e648ae2":"markdown","5ceebd87":"markdown","cc01b7b6":"markdown","4b728d57":"markdown","4719178d":"markdown","1ce9c76c":"markdown","88f32cea":"markdown","02b3b243":"markdown","83a67798":"markdown","e083bdda":"markdown","c64549de":"markdown","ac00b849":"markdown","f78b4b0d":"markdown","d477ed1c":"markdown","c4e99cce":"markdown","e483c8d5":"markdown","e5b62831":"markdown","062b3664":"markdown","95d4072a":"markdown","7e38871d":"markdown","969ba0a6":"markdown","87ae8dea":"markdown","a3114fcc":"markdown","586166ea":"markdown","c0a38fa0":"markdown","7b0a0d09":"markdown"},"source":{"d4180fef":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\n%matplotlib inline\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import model_selection\n\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor   \nfrom scipy.stats import friedmanchisquare\nfrom scipy.stats import wilcoxon","ac2798e8":"pd.options.display.max_columns = None\nwarnings.filterwarnings('ignore')\ndataset = pd.read_csv('..\/input\/survey_results_public.csv')\ndataset.head()","3644e0ec":"# number of entries\nprint(\"Number of entries: \" + str(len(dataset.index)))","c46d2e7c":"pd.options.display.max_rows = None\ndataset.isnull().sum()","8d5a09f8":"all_data_na = (dataset.isnull().sum() \/ len(dataset)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)","32c4fcd2":"features = dataset[dataset.columns.difference(['Respondent','TimeAfterBootcamp', 'MilitaryUS', 'HackatonReasons', 'EngonomicDevices', 'AdBlockReasons', 'StackOverflowJobRecommend', 'SurveyTooLong', 'SurveyTooEasy', 'AdBlocker', 'AdBlockerDisable', 'AdBlockerReasons', 'AdsAgreeDisagree1', 'AdsAgreeDisagree2', 'AdsAgreeDisagree3', 'AdsActions', 'AdsPriorities1', 'AdsPriorities2', 'AdsPriorities3', 'AdsPriorities4', 'AdsPriorities5', 'AdsPriorities6', 'AdsPriorities7', 'HypotheticalTools1', 'HypotheticalTools2', 'HypotheticalTools3', 'HypotheticalTools4', 'HypotheticalTools5', 'CurrencySymbol', 'Salary', 'SalaryType'])]\nfeatures = features[features.columns.difference(['ErgonomicDevices', 'HackathonReasons', 'SurveyEasy', 'JobEmailPriorities1', 'JobEmailPriorities2', 'JobEmailPriorities3', 'JobEmailPriorities4', 'JobEmailPriorities5', 'JobEmailPriorities6', 'JobEmailPriorities7', 'JobContactPriorities1', 'JobContactPriorities2', 'JobContactPriorities3', 'JobContactPriorities4', 'JobContactPriorities5', 'AIDangerous', 'AIInteresting', 'AIResponsible', 'AIFuture', 'IDE', 'LanguageDesireNextYear', 'LanguageWorkedWith', 'PlatformDesireNextYear', 'PlatformWorkedWith', 'DatabaseDesireNextYear', 'DatabaseWorkedWith', 'FrameworkDesireNextYear', 'FrameworkWorkedWith', 'CommunicationTools', 'CheckInCode', 'VersionControl', 'UpdateCV',  'StackOverflowVisit', 'StackOverflowDevStory', 'StackOverflowHasAccount', 'StackOverflowParticipate', 'StackOverflowRecommend', 'StackOverflowJobs', 'StackOverflowJobsRecommend'])]\n#features = dataset[dataset.columns.difference(['UpdateCV',  'StackOverflowVisit', 'StackOverflowDevStory', 'StackOverflowHasAccount', 'StackOverflowParticipate', 'StackOverflowRecommend', 'StackOverflowJobs', 'StackOverflowJobsRecommend'])]","23988790":"features.nunique()","ea3dc2ef":"features = features[features.columns.difference(['Methodology', 'SelfTaughtTypes', 'DevType', 'EducationTypes'])]","78a922d4":"f, ax = plt.subplots(figsize=(30, 7))\nplt.xticks(rotation='90')\nsns.countplot(features['Country'])\nplt.title('Country distribution on the Stackoverflow 2018 survey', fontsize=12)","dcc573d5":"f, ax = plt.subplots(figsize=(30, 7))\nplt.xticks(rotation='45')\nsns.countplot(features['Currency'])\nplt.title('Currency  distribution on the Stackoverflow 2018 survey', fontsize=15)","c110cc70":"features = features[(features.Country == 'United States')]\nfeatures = features[features.columns.difference(['Currency', 'Country'])];","240e3d63":"features = features.dropna();\nprint(\"Number of entries: \" + str(len(features.index)))","8d4c2851":"features.columns","401900d2":"f, ax = plt.subplots(figsize=(14, 7))\nplt.xticks(rotation='45')\nsns.distplot(features['ConvertedSalary']);\nplt.xlabel('Annual salary in US dollars', fontsize=15)","020dc104":"features['ConvertedSalary'].describe()","d4d392e0":"features = features[features['ConvertedSalary'] < 300000]","d9d5a4e2":"features = features[features['ConvertedSalary'] > 15000]","1f719d72":"features['ConvertedSalary'].describe()","596321fa":"f, ax = plt.subplots(figsize=(14, 7))\nplt.xticks(rotation='45')\nsns.distplot(features['ConvertedSalary']);\nplt.xlabel('Annual salary in US dollars', fontsize=15)","8c00213e":"first_bracket = \"From 15k to 60k\"\nsecond_bracket =  \"From 60k to 110k\"\nthird_bracket = \"From 110k to 300k\"\n\nfeatures['SalaryRange'] = pd.cut(features['ConvertedSalary'], bins=[15000, 60000, 110000, 300000], labels=[first_bracket, second_bracket, third_bracket])\nfeatures = features[features.columns.difference(['ConvertedSalary'])];","44673cca":"f, ax = plt.subplots(figsize=(14, 7))\nplt.xticks(rotation='45')\nsns.countplot(features['SalaryRange'], order=[first_bracket, second_bracket, third_bracket])\nplt.title('Salary distribution', fontsize=15)","b0ff1d1d":"features = features.dropna();\nprint(\"Number of entries: \" + str(len(features.index)))","04959063":"features['SalaryRange'].describe()","af803311":"f, ax = plt.subplots(figsize=(14, 7))\nplt.xticks(rotation='45')\nsns.countplot(features['CareerSatisfaction'], order=['Extremely dissatisfied', 'Slightly dissatisfied', 'Moderately dissatisfied', 'Neither satisfied nor dissatisfied', 'Slightly satisfied', 'Moderately satisfied', 'Extremely satisfied'])\nplt.xlabel('Career satisfaction', fontsize=15)\nplt.title('How people feel about their career', fontsize=15)","42fa305d":"features['CareerSatisfaction'].describe()","afaffa5b":"labelencoder = LabelEncoder()\nfeatures['carrer_label'] = labelencoder.fit_transform(features['CareerSatisfaction'])\n\nf, ax = plt.subplots(figsize=(14, 7))\nsns.boxplot(x=\"SalaryRange\", y=\"carrer_label\", data=features)\nplt.xlabel('Salary bracket', fontsize=15)\nplt.ylabel('Career satisfaction', fontsize=15)\nplt.title('Career satisfaction across salary brackets', fontsize=15)\n\nfeatures = features[features.columns.difference(['carrer_label'])]","8f8195b1":"# Define a set of graphs, 3 by 5, usin the matplotlib library\nf, axes = plt.subplots(4, 3, figsize=(25, 25), sharex=False, sharey=False)\n#f.subplots_adjust(hspace=0.4)\nplt.subplots_adjust(left=0.2, wspace=0.3, top=0.95)\nplt.suptitle('Salary bracket versus importance given to job opportunities aspects', fontsize=16)\naxes[-1, -1].axis('off')\naxes[-1, -2].axis('off')\n\nsns.pointplot(x=\"SalaryRange\", y=\"AssessJob1\", data=features, ax=axes[0,0])\naxes[0,0].set(ylabel='Importance given to industry you will be working in')\n\nsns.pointplot(x=\"SalaryRange\", y=\"AssessJob2\", data=features, ax=axes[0,1])\naxes[0,1].set(ylabel='Importance given to financial performance of the company\/organization')\n\nsns.pointplot(x=\"SalaryRange\", y=\"AssessJob3\", data=features, ax=axes[0,2])\naxes[0,2].set(ylabel='Importance given to deparment or team you will be working on')\n\nsns.pointplot(x=\"SalaryRange\", y=\"AssessJob4\", data=features, ax=axes[1,0])\naxes[1,0].set(ylabel='Importance given to technologies you will be working with')\n\nsns.pointplot(x=\"SalaryRange\", y=\"AssessJob5\", data=features, ax=axes[1,1])\naxes[1,1].set(ylabel='Importance given to compensation and benefits offered')\n\nsns.pointplot(x=\"SalaryRange\", y=\"AssessJob6\", data=features, ax=axes[1,2])\naxes[1,2].set(ylabel='Importance given to the company culture')\n\nsns.pointplot(x=\"SalaryRange\", y=\"AssessJob7\", data=features, ax=axes[2,0])\naxes[2,0].set(ylabel='Importance given to the opportunity to work from home\/remotely')\n\nsns.pointplot(x=\"SalaryRange\", y=\"AssessJob8\", data=features, ax=axes[2,1])\naxes[2,1].set(ylabel='Importance given to opportunities for professioal development')\n\nsns.pointplot(x=\"SalaryRange\", y=\"AssessJob9\", data=features, ax=axes[2,2])\naxes[2,2].set(ylabel='Importance given to the diversity of the company or organization')\n\nsns.pointplot(x=\"SalaryRange\", y=\"AssessJob10\", data=features, ax=axes[3,0])\naxes[3,0].set(ylabel='Importance given to the impact of the product\/software you will be working with has')","8b7dabd8":"features = features[features.columns.difference(['AssessJob3', 'AssessJob5', 'AssessJob7'])]","9f591111":"# Define a set of graphs, 3 by 5, usin the matplotlib library\nf, axes = plt.subplots(4, 3, figsize=(25, 25), sharex=False, sharey=False)\n#f.subplots_adjust(hspace=0.4)\nplt.subplots_adjust(left=0.2, wspace=0.3, top=0.95)\nplt.suptitle('Salary brackets versus importance given to benefits', fontsize=16)\naxes[-1, -1].axis('off')\n\nsns.pointplot(x=\"SalaryRange\", y=\"AssessBenefits1\", data=features, ax=axes[0,0])\naxes[0,0].set(ylabel='Importance given to salary\/bonuses')\n\nsns.pointplot(x=\"SalaryRange\", y=\"AssessBenefits2\", data=features, ax=axes[0,1])\naxes[0,1].set(ylabel='Importance given to stock options\/shares')\n\nsns.pointplot(x=\"SalaryRange\", y=\"AssessBenefits3\", data=features, ax=axes[0,2])\naxes[0,2].set(ylabel='Importance given to healthcare')\n\nsns.pointplot(x=\"SalaryRange\", y=\"AssessBenefits4\", data=features, ax=axes[1,0])\naxes[1,0].set(ylabel='Importance given to parental leave')\n\nsns.pointplot(x=\"SalaryRange\", y=\"AssessBenefits5\", data=features, ax=axes[1,1])\naxes[1,1].set(ylabel='Importance given to fitness or wellness benefits')\n\nsns.pointplot(x=\"SalaryRange\", y=\"AssessBenefits6\", data=features, ax=axes[1,2])\naxes[1,2].set(ylabel='Importance given to retirement or pension savings matching')\n\nsns.pointplot(x=\"SalaryRange\", y=\"AssessBenefits7\", data=features, ax=axes[2,0])\naxes[2,0].set(ylabel='Importance given to company provided meals or snacks')\n\nsns.pointplot(x=\"SalaryRange\", y=\"AssessBenefits8\", data=features, ax=axes[2,1])\naxes[2,1].set(ylabel='Importance given to computer\/office equipment allowance')\n\nsns.pointplot(x=\"SalaryRange\", y=\"AssessBenefits9\", data=features, ax=axes[2,2])\naxes[2,2].set(ylabel='Importance given to childcare benefit')\n\nsns.pointplot(x=\"SalaryRange\", y=\"AssessBenefits10\", data=features, ax=axes[3,0])\naxes[3,0].set(ylabel='Importance given to transportation benefit')\n\nsns.pointplot(x=\"SalaryRange\", y=\"AssessBenefits11\", data=features, ax=axes[3,1])\naxes[3,1].set(ylabel='Importance given to conference or education budget')","43a1ec74":"features = features[features.columns.difference(['AssessBenefits3', 'AssessBenefits4', 'AssessBenefits6', 'AssessBenefits9', 'AssessBenefits10'])]","394db3c9":"output = features['SalaryRange']\nfeatures = features[features.columns.difference(['SalaryRange'])]","8d8df4cb":"categorical = []\nfor col, value in features.iteritems():\n    if value.dtype == 'object':\n        categorical.append(col)\n\n# Store the numerical columns in a list numerical\nnumerical = features.columns.difference(categorical)\n\nprint(categorical)","1658ea0f":"# get the categorical dataframe\nfeatures_categorical = features[categorical]","6caa02ec":"# one hot encode it\nfeatures_categorical = pd.get_dummies(features_categorical, drop_first=True)","7db7ec8f":"# get the numerical dataframe\nfeatures_numerical = features[numerical]","313d61a2":"# concatenate the features\nfeatures = pd.concat([features_numerical, features_categorical], axis=1)","bc0136c0":"labelencoder = LabelEncoder()\noutput = labelencoder.fit_transform(output)","ece114aa":"def calculate_vif_(X, thresh=100):\n    cols = X.columns\n    variables = np.arange(X.shape[1])\n    dropped=True\n    while dropped:\n        dropped=False\n        c = X[cols[variables]].values\n        vif = [variance_inflation_factor(c, ix) for ix in np.arange(c.shape[1])]\n    \n        maxloc = vif.index(max(vif))\n        if max(vif) > thresh:\n            print('dropping \\'' + X[cols[variables]].columns[maxloc] + '\\' at index: ' + str(maxloc))\n            variables = np.delete(variables, maxloc)\n            dropped=True\n\n    print('Remaining variables:')\n    print(X.columns[variables])\n    return X[cols[variables]]","e536ffe0":"features = features[features.columns.difference(['WakeTime_Between 6:01 - 7:00 AM','SexualOrientation_Straight or heterosexual','OperatingSystem_Windows'])]","bed26650":"features_train, features_test, salary_train, salary_test = train_test_split(features, output, test_size = 0.3, random_state = 0)","a2578a4e":"def plot_confusion_matrix(cm, title):\n    # building a graph to show the confusion matrix results\n    cm_plot = pd.DataFrame(cm, index = [i for i in {first_bracket, second_bracket, third_bracket}],\n                  columns = [i for i in {first_bracket, second_bracket, third_bracket}])\n    plt.figure(figsize = (6,5))\n    sns.heatmap(cm_plot, annot=True, vmin=5, vmax=90.5, cbar=False, fmt='g')","eabeddf6":"# Fit the classifier, get the prediction array and print the accuracy\ndef fit_and_pred_classifier(classifier, X_train, X_test, y_train, y_test):\n    # Fit the classifier to the training data\n    classifier.fit(X_train, y_train)\n\n    # Get the prediction array\n    y_pred = classifier.predict(X_test)\n    \n    # Get the accuracy %\n    print(\"Accuracy: \" + str(accuracy_score(y_test, y_pred) * 100) + \"%\") \n    \n    return y_pred","cdf0835a":"# Build and fit the model\nrf = RandomForestClassifier(n_estimators = 800, random_state = 0)\nrf_salary_pred = fit_and_pred_classifier(rf, features_train, features_test, salary_train, salary_test)","83099f62":"cm = confusion_matrix(salary_test, rf_salary_pred)\nplot_confusion_matrix(cm, 'Random Forest Confusion Matrix')","f3ebc405":"# Build and fit the model\nsvc = SVC(kernel = 'linear', probability=True, random_state = 0)\nsvc_salary_pred = fit_and_pred_classifier(svc, features_train, features_test, salary_train, salary_test)","24265dce":"cm = confusion_matrix(salary_test, svc_salary_pred)\nplot_confusion_matrix(cm, 'Linear SVC Classifier Confusion Matrix')","ad6936bc":"lda = LinearDiscriminantAnalysis()\nlda_salary_pred = fit_and_pred_classifier(lda, features_train, features_test, salary_train, salary_test)","79938946":"cm = confusion_matrix(salary_test, lda_salary_pred)\nplot_confusion_matrix(cm, 'OneVsRest Linear SVC Confusion Matrix')","1bdbb96b":"ensemble = VotingClassifier(estimators=[('rf', rf), ('svc', svc), ('lda', lda)], voting='soft', weights=[2,3,2], flatten_transform=True)\nensemble_salary_pred = fit_and_pred_classifier(ensemble, features_train, features_test, salary_train, salary_test)","39ec8f7c":"cm = confusion_matrix(salary_test, ensemble_salary_pred)\nplot_confusion_matrix(cm, 'Ensemble Confusion Matrix')","4a732f87":"# Friedman test\nstat, p = friedmanchisquare(svc_salary_pred, lda_salary_pred, ensemble_salary_pred)\nprint('p=%.10f' % (p))","3a2695a3":"# interpret\ndef h0_test(p):\n    alpha = 0.05\n    if p > alpha:\n        print('Same distributions (fail to reject H0)')\n    else:\n        print('Different distributions (reject H0)')\n\nh0_test(p)","b3b1c91c":"from scipy.stats import f_oneway\n# compare samples\nstat, p = f_oneway(svc_salary_pred, lda_salary_pred, ensemble_salary_pred)\nprint('p=%.3f' % (p))\n\n# interpret\nh0_test(p)","66295a81":"# compare samples\nstat, p = f_oneway(svc_salary_pred, lda_salary_pred, ensemble_salary_pred, rf_salary_pred)\nprint('p=%.3f' % (p))\n\n# interpret\nh0_test(p)","bd21e98c":"# evaluate each model in turn\nmodels = [['Linear SVC', svc], ['Random Forest', rf], ['Linear Discriminant Analysis', lda],['Ensemble', ensemble]]\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n    kfold = model_selection.KFold(n_splits=10, random_state=7)\n    cv_results = model_selection.cross_val_score(model, features_train, salary_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    print(\"%s accuracy: %0.4f (+\/- %0.4f)\" % (name, cv_results.mean(), cv_results.std() * 2))","eadf952e":"# boxplot algorithm comparison\nf, ax = plt.subplots(figsize=(15, 7))\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","4ba2abdc":"The next step is to import the csv using the pandas library, and take a look at quick peek at the data: ","80c9d9f6":"Let's take a look at the distribution:","8c3d223c":"We should also remove features that have way too many options, as we will end-up with too many features. Let's see what are the worst offenders:","7aced878":"With several models built, it's now time to evaluate our results. Looking at the precision % gives us a good idea of what model went better, however, we can't blindly trust that. For that, we can use the Friedmann test.","cffb8cd3":"Only the first three columns have no empty values: respondent, hobby and open source. Respondent is obviously needed and probably registered automatically by StackOverflow. As for the other values, it's pretty interesting to note that only two fields have no empty replies: hobby and open source. This probably says more about people than about the questions itself, that is, lazyness probably took over when people noticed the size of the survey. Let's see what the top offenders are:","b2c0ad2a":"# 5. Statistically testing our models","16a10866":"## 4.1 Random Forest","b6bcd2e4":"## 5.1 Friedmann Test","0c81680a":"Lets take a look at the career satisfaction distribution:","01c3bc2c":"With that in mind, our model can now only predict the average developer, which should be over 80% of the population.","1e72e4a0":"Lets take a look at how salary is distributed:","d94c8fff":"In the end, Linear SVC, Linear Discriminant Analysis and our Ensemble performed very similarly. LDA had the highest mean, and the second best variation (Q1 and Q3). Our ensemble performed very similarly, didn't had a single outlier performance (as it was the case for Linear SVC and LDA) and had the highest maximum. We can discard the Random Forest model and keep the other three, to perform one final test.\n\nIt's hard to pick a winner, no model in particular achieved a consistent performance above 70%, which means that our features are not enough in their current form. \n\nThat being said, the ensemble model was able to make use of the Linear SVC's good min\/max values while eliminating the bad outlier performance and having a higher mean, which means that we can (barely) pick it as our model of choice.","4239fc88":"This survey has many in-depth questions that explore how someone assess a potential job oportunity in the following categories:\n1. The opportunity itself: industry, department you will be working with, overall benefits and so on;\n2. Benefits: explores a little bit more in depth which benefits are the most interesting;\n3. Contact preference: how do you prefer being contacted (telephone, email, etc.);\n4. Information you would like to see on a job opportunity sent by email.\n\nOnly the first and second question groups were kept for this study, as the last too are too specific and are already covered to some extent  by other questions. ","dccbdcde":"# 4. Model building","4b29c17c":"We can see that salary is a left skewed normal distribution, with some outliers, as seen on the 1 and 2 million mark. These do not represent the vast majority of people that participated in this survey, so we should strip it out least it affect our models. Before doing that, let's take a look at the distribution:","e47b87ff":"### Checking for empty values","d25c729b":"Our models did not perform terribly well. Perhaps there's a limitation with what we can achieve with the given dataset, perhaps they were not properly modeled. In theory, an ensemble should perform better, as even though the fitted classifiers are not that different, the small difference in them should help boost up the overall accuracy:","0b7aa2d5":"At the end, we ended up retaining 8216 responses in full, which is more than enough to run a meaningful analysis. In the end, we kept the following attributes:","eae5af80":"We start off by importing all libraries we're gonna use (remove unused imports later):","b78a9a4c":"Lets check the new distribution and plot:","8271a5a5":"## 4.5 Ensemble","48450d9c":"## 4.2 Linear SVC","30d22b0a":"## 5.2 Cross validating our models","3c57de2a":"Our ensemble improved our situation a little bit with roughly 68.28% precision. It's far from ideal, but it seems to be about as far as we'll go with the current pool of data.","7b65f8aa":"With that in mind, we can now go more in depth and take a look at the benefits are rated:","e9b75b7d":"With p = 0.099, we can't reject H0. It seems like our models are not that different in their predictions, at least for a single train\/test split. However, the same ain't true if we include the random forest model on the pool:","de9dd568":"In this analysis it's important to see what country the respondent is from, as it greatly affect how much he earns. A software developer in Brazil, for example, earns about 4 times less than his counterpart in USA. Therefore, we lose a bit of meaning without doing any operations to address this. We have two possibilities:\n   1) Processing the currencies and converting the salary to the estimated value, if this person was a USA citzen.\n   2) Ignoring all non-dollar, euro or pound based countries (which usually pay better).\n   \nIn order to decide what aproach to take, we need to take a look at the country and currencies distribution:","119a3da5":"We can see that most of the respondents are either earning in dollars, followed by euros and pounds. Although all three currencies have similar values, it's better if we only keep developers from the USA in this study. The reason for this is that, in averagem a developer in the USA earns more than others: https:\/\/stackoverflow.blog\/2017\/09\/19\/much-developers-earn-find-stack-overflow-salary-calculator\/, therefore, it seems like we can't mix this data without getting too much noise.","f41ca4bd":"# Which (USA) developers earn the most?","c8410cb5":"Now we can separate the categorical variances from the numerical ones:","ca96778d":"The distribution indicates that people who use StackOverflow and participate on its events (such as this survey) are, on average, much better paid.","cf4e03d7":"Now we can see how career satisfaction (from 0 to 6) compare to salary:","0defe45a":"## 2.1 Countries and currencies","022b4278":"This kernel is organized in the following way:\n\n1. [Data pre-processing](#1.-Data-pre-processing)\n2. [Data analysis and feature selection](#2.-Data-analysis-and-feature-selection)\n3. [Feature engineering](#3.-Feature-engineering)\n4. [Model building](#4.-Model-building)\n5. [Statistically testing our models](#5.-Statistically-testing-our-models)","29aad7fe":"With some preliminary feature selection and engineering being done, we can test a few models and see how they perform without further changes. \n\nAt first, we will not do any cross-validation and just get a feel of what we can achieve with each model. The next chapter will wrap it up by cross-validating each model and comparing the results.\n\nThe following two methods will help us along the way:","c6edb29b":"No problem is too big or too small for a random forest model. In fact, I tend to always start my analysis with it as it gives me a quick overview of what I can expect about a problem:\n1. Are there a clear set of rules that govern the dependent variable being studied?\n2. If so, what are the most important ones?\n3. If not, go back to the drawing board.","5a64223e":"Remove collinear variables:","edc47253":"## 2.2 Salary","90fa58a5":"And lastly, we split it into train and test sets:","bfd00b6b":"And the salary is encoded:","d214a1f4":"# 3. Feature engineering","060c1c88":"# 2. Data analysis and feature selection","6468aa1f":"In the end, we can reject the then null hypothesis: the methods are different, which means that we must go ahead and do pair-wise comparison between the models to identify which is the best. Wilcoxon Signed-Rank test should help us with that:","1ad3a27d":"First, let's split the output from the features:","8d3b7651":"* Objectives: Separate the salary into tree groups, test different models and find out what answers are the most relevant.\n* Kaggle link: https:\/\/www.kaggle.com\/stackoverflow\/stack-overflow-2018-developer-survey\n\nStackoverflow is the biggest hub for developers in the world with about 50 million visitors a month. It is used on a daily by professionals and hobysts alike, mainly as a tool (a forum, if you will) for discussing programming problems. \n\nEver since 2015, Stackoverflow has hosted a developer survey to learn more about its user-base, with questions ranging from programming related questions, salary and ethics. The new installment of the survey reached about 100.000 people, almost 4 times as much as the first one held 3 years ago.\n\nThere's a wide range of information available, and no obligation to answer all questions. The main focus of this study is to find out if there's a direct correlation between how much someone earns and several programming and career related topics. In order to have a meaningful analysis, questions that have little relation with salary, or that have too many missing values, will be removed.\n\nSince people from all over the world respond to this survey and it's impossible to have a good global salary analysis in a single model (without build extremely complex ensembles) we will limit our analysis to USA residents.","985b8c33":"To save up on computing time, I'm not going to execute the method above. I'll simply drop the variables it indicated on the first time I ran it: \n\n> dropping 'WakeTime_Between 6:01 - 7:00 AM' at index: 190\n\n> dropping 'OperatingSystem_Windows' at index: 108\n\n> dropping 'SexualOrientation_Straight or heterosexual' at index: 156","9e648ae2":"To simplify things, we'll one hot encode all categorical variables and remove one for each to avoid the dummy variable trap:","5ceebd87":"\"Does money bring career satisfaction\" is the modern adaptation of \"does money bring happiness\", and the answer (at least to USA developers) seems to be no. That is understandable, as it can mean several things:\n 1. The developer is already burned out due to working too many years to get where he is;\n 2. Works for a company or in an industry which he does not care much about only to get a high pay-check;\n 3. There's little sense of going forward, since you probably already achieved most of your professional goals.","cc01b7b6":"## 2.3 Career satisfaction","4b728d57":"Lastly, we can try a simple linear support vector classification, because sometimes less is more:","4719178d":"Without further ado, here is our first model:","1ce9c76c":"With that done, let's take a quick look at what columns have empty values:","88f32cea":"We already know that LDA, Linear SVC and our Ensemble are fairly similar. To make sure our analysis is correct, we can't trust our analysis on top of a single train\/test split, we need to cross-validate our models in 10 folds and see how they perform under different circumstances:","02b3b243":"It's interesting to note that the general trend does not suggest that people got lazy midway through the survey, as some of the questions at the end had the similar number of empty replies as some at the beginning. The biggest offenders with low reply rate are:\n1. TimeAfterBootcamp;\n2. MilitaryUS;\n3. HackatonReasons;\n4. EngonomicDevices;\n5. AdBlockReasons;\n6. StackOverflowJobRecommend.\n\nHalf of these can be attributed to being dependent on previous questions, which have a high chance of being empty (either due to previous question being skipped or reponded negativelly): time after bootcamp, hackaton reasons and adblock reasons. The reamining three are probably due to not being very popular questions.\n\nDue to the ammount of empty replies, it's pretty clear we will have to remove some attributes (that is, some questions won't be used in our analysis):\n1. The top 6 offenders: too many empty values;  \n2. SurveyTooLong and SurveyTooEasy: unless there's an odd correlation between salary and people finding surveys easy, we can remove these questions;\n3. All advertizing related questions, which is a survey in itself;\n4. All currency\/salary related questions, except by ConvertedSalary. We have too many options to say the same thing, having a single source of information for income should be enough. However, before removing this information, it will be used a filter to keep only entries that have similar earnings (pound, euros and dollars).\n5. Hypothetical tools, which is a survey for what tools people would like to see on StackOverflow. Although it could be indirectly related to salary, as in, people who want certain tools are either happy or unhappy in their jobs, it's too specific for us to consider as part of a general study;\n6. Job contact and email priorities: it tells a similar story to job assessment questions;\n7. AI related questions, which should tell little about salary;\n8. Questions with too many possible answers, such as framework, languages, platforms and so on, both on the worked with and desired next year categories;\n9. Respondent number, which is only an identifier without any meaning for this study.\n\nSome of the other attributes may seen uncorrelated to salary at first sight (such as if you consider being a member of stackoverflow community), but they may end up showing a relation in the form of \"how much does community engagement helps you being successfull in your career, and thus earn more\".","83a67798":"## 5.2 ANOVA","e083bdda":"## 2.4 Job preferences","c64549de":"This is due to the fact that the RF model performed worst than the others enough for it to significantly different.","ac00b849":"Let's take a closer look at the distribution:","f78b4b0d":"There are some pretty bad features out there: Methodology, SelfTaughtTypes, DevType and EducationTypes. It's tempting to not use country due to that much variation, however, it will likely play a big role on salary, so it can't be ignored. Converted salary does have a lot of possibilities, but that's because it's a user-inputed number. This will be corrected by creating salary ranges. As for the rest, they should be removed:","d477ed1c":"The graphs above tell us the following story:\n1. Without exception, people who earn the least had the biggest variance on how they rate the aspects of a job offer. This could indicate several things; for instance, it could mean that our bracket system is not perfect, or that people who earn less don't have a clear goal.\n2. On a similar note, people who are in the middle salary bracket had the smallest variance on how they rate job opportunities.\n3. Everyone seems to rate the importance given to the department\/team they will be working with the same.\n4. On average, people who earn more prioritize: financial performance of the company, compensation and benefits offered, diversity of the company and the impact of the product\/software he works with has. \n5. On the other hand, people who are earn the least give more importance to opportunities for professional development, what industry they work with, company culture and technologies used.\n\nIn general, we can that people who earn more are not really worried anymore about career development; they want to earn well, have stability and work with products that have an impact on the world. On the other hand, people who ear less want to develop their careers, work with technologies and industries they like.\n\nA few of this topics had too little variance to matter, so we're going to remove it: compensation and benefits, department or team will be working with and home-office.","c4e99cce":"# Index","e483c8d5":"There's little feature engineering to be done on this dataset: almost all questions were either multiple choice or yes\/no, with no room for input error. The only special one is salary, which should be converted into salary ranges to better categorize people. We know that the lower 10% is at 49k, the top is at 102k and the medium is 69k dollars. That is, the majority should be somewhere in between 49k and 102k, which seems about right for developer salaries in most parts of USA, except by cities with high cost of living. \n\nHowever, we can see on our distribution above that most developers seems to be on the 50k to 150k range! With that in mind, we can't blindly follow the average USA salary statistics, it seems more appropriate to use the following salary brackets:\n 1. 15k to 60k\n 2. 60k to 120k\n 3. 120k to 300k\n","e5b62831":"According to the payscale website (https:\/\/www.payscale.com\/research\/US\/Job=Software_Developer\/Salary), 90% of USA developers salary range from 46,797 to 107,228 U$. However, to go through the bigger picture, we can use 15000 as a baseline, as that is the minimum wage in USA (https:\/\/poverty.ucdavis.edu\/faq\/what-are-annual-earnings-full-time-minimum-wage-worker). \n\nWe also need a cap, as the original dataset has salaries up to 2M, which are way out of the ordinary and should be treated separately on a different study. We'll cap it at 300k, as it seems to be what most successful developers can hope to achieve (https:\/\/www.networkworld.com\/article\/3167569\/careers\/13-tech-jobs-that-pay-200k-salaries.html) ","062b3664":"The Friedman test is the nonparametric version of the repeated measures analysis of variance test, or repeated measures ANOVA. The test can be thought of as a generalization of the Kruskal-Wallis H Test to more than two samples.\n\nThe default assumption, or null hypothesis, is that the multiple paired samples have the same distribution. A rejection of the null hypothesis indicates that one or more of the paired samples has a different distribution.\n\n* Fail to Reject H0: Paired sample distributions are equal.\n* Reject H0: Paired sample distributions are not equal.\n    \nOur h0 is: there's no significant difference between our models. In theory, the ensemble model should be supperior, as it incorporates the strong points of random forest, linear svc and linear discriminant analysis. However, since the models performed very similarly, and made very similar mistakes, it could be a close call. Sometimes the ensemble may actually perform worse if there are not enough differences to go by.","95d4072a":"Almost half the respondents are moderately satisfied with their careers. In second place we have people who are \"extremely satisfied\", closely followed by slightly satisfied. In general, we're seeing a big indication that people who took this survey are in general more satisfied than not with their careers. ","7e38871d":"Let's take a look at the distribution: ","969ba0a6":"## 4.3 Linear discriminant analysis","87ae8dea":"Here are the general trends for how people evaluate a job's benefits package:\n\n1. In a similar fashion to the job offer aspects analysis, people who earn the least have the biggest variance on how they rate the benefit package of a job offer. \n2. In general, people who earn more care more about having stock options\/shares participation and bonuses more than people who earn less.\n3. People who earn less give care more about education budget, fitness\/wellbeing offered and equipment allowance than people who earn more.\n4. Overall, salary\/bonuses are by far the most important aspect of a job's benefits package, and childcare the least.\n\nThe other benefits are too similar across all brackets and can be removed: health care,  parental leave, pension savings matching, company provided meals\/snacks,  childcare benefits and transportation.","a3114fcc":"First, lets take a look at how salary relates to job opportunity assessment. There's one graphic per question, with the y axis being how the respondent rates the job aspected from 1 to 11 (the lowest, the most important) and the x axis being the salary bracket. The lenght of the vertical lines indicate the variance and the dot indicates the average rating for that particular group.","586166ea":"### Importing the data","c0a38fa0":"We can now finally drop all empty rows:","7b0a0d09":"# 1. Data pre-processing"}}