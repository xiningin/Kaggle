{"cell_type":{"30ec658b":"code","79f04c86":"code","83038d07":"code","128ff1aa":"code","f11039a6":"code","d5262e21":"code","d3d0ceb9":"code","eca0041a":"code","914d9b8d":"code","f7cc4a98":"code","4c30f596":"code","881c6d09":"code","3b2af5bf":"code","1e84e878":"code","4e6e15d8":"code","714ffe00":"code","36ba732b":"code","935557c4":"code","898a78b6":"code","322c6ad3":"code","bc13713b":"code","f191ae46":"code","36e004f0":"code","28cdc180":"code","44128a2e":"code","fc28e46b":"code","64c95ca1":"code","2f31563a":"code","1b0d4db2":"code","22c3205f":"code","65f15134":"code","d84c1735":"code","9610dd29":"code","38f1c4d2":"code","3ff846f0":"code","daeede8d":"code","04dd4bf4":"code","2f2621f6":"code","9be33786":"code","ef7401fc":"code","440b9ef1":"markdown","1daef274":"markdown","7dfe350b":"markdown","5e27b5f8":"markdown","6430183c":"markdown","bcbe4f3b":"markdown","f82bf136":"markdown","984bb763":"markdown","22ddb0dd":"markdown","bd52f748":"markdown","2fa56cfd":"markdown","900d0064":"markdown"},"source":{"30ec658b":"import pandas as pd\nimport re\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statistics import mode, mean, median\nimport seaborn as sns","79f04c86":"read = pd.read_csv('ski_main')\nuseless_cols = ['link', 'Uniquenumber', 'Resort name', 'Ski map', 'E-mail', 'Website', \n                'Current piste quality', 'Youtube url', 'Skiresort.info url', \n                'Telephone', 'HP', 'SKIMAP', 'Total lifts open', 'Current upper snow depth', \n                'Current lower snow depth', 'Current fresh snow']\ndf_analysis = read.drop(useless_cols, axis = 1)\nprint(read.columns)","83038d07":"#split strings in season column to starting and ending season\nnoreport = list()\nseasonstart = list()\nseasonend = list()\nfor each in df_analysis['Season']:\n    eachsplit = each.split(' - ')\n    if len(eachsplit) == 2:\n        seasonstart.append(eachsplit[0])\n        seasonend.append(eachsplit[1])\n    elif len(eachsplit) > 2:\n        splitagain = eachsplit[1].split(' ')\n        seasonstart.append(eachsplit[0])\n        seasonend.append(splitagain[0])\n    else:\n        seasonstart.append(float('nan'))\n        seasonend.append(float('nan'))\ndf_analysis['seasonstart'] = seasonstart\ndf_analysis['seasonend'] = seasonend\ndf_analysis = df_analysis.drop('Season', axis = 1)","128ff1aa":"#drop columns that have only 1 value or if the entire column is null or if each row has a unique value\nfor each in df_analysis.columns:\n    if ((df_analysis[each].isnull()).sum == df_analysis.shape[0]) or (\n        len(df_analysis[each].unique()) == 1):\n        df_analysis = df_analysis.drop(each, axis = 1)\n        print(each)","f11039a6":"#check for columns that have nan values\ncol_nan = list(df_analysis.columns[(df_analysis.isnull()).sum() != 0])\nprint(col_nan)","d5262e21":"#substitude the nan values with mode since all the nan columns have strings\nfor each in col_nan:\n    not_nan = df_analysis.loc[df_analysis[each].notnull(), each]\n    col_mode = mode(not_nan)\n    df_analysis.loc[df_analysis[each].isnull(), each] = col_mode\nprint(((df_analysis.isnull()).sum() == 0).all()) #check if every column has zero nans","d3d0ceb9":"plt.scatter('Regions', 'Elevation', data = df_analysis)\nplt.xticks(rotation = 90)\nplt.xlabel('Regions')\nplt.ylabel('Elevation')\nplt.show()\n\nplt.scatter('Regions', 'Ski pass price adult', data = df_analysis)\nplt.xticks(rotation = 90)\nplt.xlabel('Regions')\nplt.ylabel('Ski Price Adult')\nplt.show()\n\nplt.scatter('Beginner slopes', 'Ski pass price adult', data = df_analysis)\nplt.scatter('Intermediate slopes', 'Ski pass price adult', data = df_analysis)\nplt.scatter('Difficult slopes', 'Ski pass price adult', data = df_analysis)\nplt.xlabel('Beginner slopes')\nplt.ylabel('Ski pass price adult')\n# plt.legend('Beginner slopes', 'Intermediate slopes', 'Difficult slopes')\nplt.show()\n\nplt.scatter('User rating', 'Ski pass price adult', data = df_analysis)\nplt.xlabel('rating')\nplt.ylabel('price')\nplt.show()\n\nplt.hist('seasonstart', data = df_analysis)\nplt.show()","eca0041a":"#from plot above, there seems to be a lot of 0a ratings, which means that the resort does not have rating\nif (df_analysis['User rating'] == '0a').sum()\/df_analysis.shape[0] > 0.6:\n    df_analysis = df_analysis.drop('User rating', axis = 1)\n    print('dropped user rating')","914d9b8d":"# print(df_analysis.dtypes)\ntest = df_analysis.copy()\ndf_model = test.loc[:, test.dtypes != object]\nfor each in test.columns:\n#     print(df_analysis[each].dtype)\n    if test[each].dtype == object:\n        test[each] = test[each].astype('category')\n#         print(test[each].cat.codes)\n        dummy = test[each].cat.codes\n        df_model[each] = dummy","f7cc4a98":"# from mlxtend.plotting import heatmap\n# import mlxtend\ncols = df_model.columns\ncm = np.corrcoef(df_model[cols].values.T)\n# hm = heatmap(cm, row_names = cols, column_names = cols)\n# plt.show\n\nsns.heatmap(cm)\nprint(df_model.columns[2:15])\nprint(df_model.columns[15:])","4c30f596":"df_model.head()","881c6d09":"adult_price_mean = (df_model['Ski pass price adult']).mean()\nkid_price_mean = (df_model['Ski pass price children']).mean()\ndf_model.loc[df_model['Ski pass price adult'] == 0, 'Ski pass price adult'] = adult_price_mean\ndf_model.loc[df_model['Ski pass price children'] == 0, 'Ski pass price children'] = kid_price_mean\n# adult_price_max = max(df_model['Ski pass price children'])\nprint(adult_price_mean, kid_price_mean)\nprint((df_model['Ski pass price adult']).std(), (df_model['Ski pass price children']).std())\n# print(ss.fit_transform(df_model))\nprint('mean and median of adult price: ', mean(df_model['Ski pass price adult']), median(df_model['Ski pass price adult']))","3b2af5bf":"from sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\nfrom sklearn.model_selection import cross_val_score\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures","1e84e878":"ycol = ['Ski pass price adult', 'Ski pass price children'] #predict two values\ny1col = ['Ski pass price adult']\nxcol = list(df_model.columns.values) #all other variables\nfor each in ycol:\n    xcol.remove(each) #remove the columns that are y\n    \n#categorical df\ny_cat = df_model['Ski pass price adult']\nx_cat = df_model[xcol]\nx_cat_train, x_cat_test, y_cat_train, y_cat_test = train_test_split(x_cat, y_cat, test_size = 0.2)\ndf_cat = df_model\n\n#continuous\nss = StandardScaler()\ndf_fit = ss.fit(df_model)\nymean = df_fit.mean_[2:4] #column 2, 3 are means for price adult and price children\nystd = df_fit.var_[2:4] ** (1\/2) #column 2, 3 are means for price adult and price children\ndf_model = pd.DataFrame(ss.fit_transform(df_model), columns = cols)\n# y = df_model['Ski pass price adult']\ny = df_model[ycol]\nx = df_model[xcol]\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\ny1 = df_model['Ski pass price adult']\nx1_train, x1_test, y1_train, y1_test = train_test_split(x, y1, test_size = 0.2, random_state = 0)","4e6e15d8":"#linear regression\nlr = linear_model.LinearRegression()\nscores = cross_val_score(estimator = lr, X = x, y = y, cv=5, scoring = 'neg_mean_squared_error')\nprint(scores)","714ffe00":"# #linear regression\n# pipe_lr = make_pipeline(StandardScaler(),\n#                        linear_model.LinearRegression())\n# scores = cross_val_score(estimator = pipe_lr, X = x, y = y, cv=5, scoring = 'neg_mean_squared_error')\n# print(scores)","36ba732b":"#linear regression with loop\nfold = 5\nbyrow = df_model.sample(frac = 1) #set the split by row\nfolds = np.array_split(byrow, 5) #split dataframe into 5 so that 20% is testing and 80% is training\nmses = list() #record mse of each fold\n\nfor eachfold in range(fold): #for each number of fold\n    fold_num = list(np.arange(0, fold)) #create list of indices to refer which dataframe in folds is testing\n    fold_num.pop(eachfold) #pop current fold number\n#     print(fold_num)\n    #form training and testing set\n    test = folds[eachfold] #let current fold number be testing\n    train_list = list() #list for list of subdataframes to form training\n    for eachtrain in fold_num: #for each training fold number \n        train_list.append(folds[eachtrain]) #append the training sub-dataframe\n    train = pd.concat(train_list) #combine all the training data\n    \n    #get x and y\n    y_train_lr = train[ycol[0]]\n    x_train_lr = train[xcol]\n    y_test_lr = test[ycol[0]]\n    x_test_lr = test[xcol]\n    \n    #modeling\n    lr = linear_model.LinearRegression()\n    lr.fit(x_train_lr, y_train_lr)\n    mse = mean_squared_error(y_test_lr, lr.predict(x_test_lr))\n    mses.append(mse)\n    \nprint('mean of MSEs of all the folds are: ', mean(mses))\nprint((mean(mses)) ** (1\/2) * ystd[0] + ymean[0])","935557c4":"lr.summary","898a78b6":"#poly regression\ndegrees = np.arange(1, 5)\nmean_error = list()\nfor each in degrees:\n    pipe_poly = make_pipeline(PolynomialFeatures(degree = each),\n                              linear_model.LinearRegression())\n    scores = cross_val_score(estimator = pipe_poly, X = df_model[xcol], y = df_model[ycol], cv=5, scoring = 'neg_mean_squared_error')\n    mean_error.append(mean(scores))\nplt.scatter(degrees, mean_error)\nplt.xlabel('Degrees')\nplt.ylabel('mean squared error')\nplt.show()","322c6ad3":"from sklearn.preprocessing import scale\nfrom sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV, MultiTaskLasso, MultiTaskLassoCV","bc13713b":"#set up grid for lambda, used in both ridge and lasso\ngrid = 10**np.linspace(10, -2, 100)","f191ae46":"#both adult and children prices\nridgecv = RidgeCV(alphas = grid, normalize = True, cv = 5, scoring = 'neg_mean_squared_error')\nridgecv.fit(x_train, y_train)\nprint('best alpha:', ridgecv.alpha_)\nridgebest = Ridge(alpha = ridgecv.alpha_, normalize = True)\nridgebest.fit(x_train, y_train)\nprint('testing mean squared error:', mean_squared_error(y_test, ridgebest.predict(x_test)))\nridge_coef = pd.DataFrame({'adult price':ridgebest.coef_[0], 'children price':ridgebest.coef_[1]}, index = xcol)\nprint(ridge_coef)","36e004f0":"#just adult price\nridgecv = RidgeCV(alphas = grid, normalize = True, cv = 5, scoring = 'neg_mean_squared_error')\nridgecv.fit(x_train, y1_train)\nprint('best alpha:', ridgecv.alpha_)\nridgebest = Ridge(alpha = ridgecv.alpha_, normalize = True)\nridgebest.fit(x_train, y1_train)\nprint('testing mean squared error:', mean_squared_error(y1_test, ridgebest.predict(x_test)))\nridge_coef = pd.DataFrame({'adult price':ridgebest.coef_}, index = xcol)\nprint(ridge_coef)","28cdc180":"lassocv = MultiTaskLassoCV(alphas = grid, normalize = True, cv = 5, max_iter = 5000)\nlassocv.fit(x_train, y_train)\nprint(lassocv.alpha_)\nlasso = MultiTaskLasso(alpha = lassocv.alpha_, normalize = True, max_iter = 5000)\nlasso.fit(x_train, y_train)\nprint(mean_squared_error(y_test, lasso.predict(x_test)))\ncoeffs = pd.DataFrame(lasso.coef_, columns = x.columns, index = ['adult price', 'children price']).transpose()\nprint('do target variable have the same nonzero parameters:', ((coeffs['adult price'] == 0) == (coeffs['children price'] == 0)).all())\nprint(coeffs[coeffs['adult price']>0])","44128a2e":"lassocv = LassoCV(alphas = grid, normalize = True, cv = 5)\nlassocv.fit(x1_train, y1_train)\nprint('best alpha: ', lassocv.alpha_)\nlasso = Lasso(alpha = lassocv.alpha_, normalize = True)\nlasso.fit(x1_train, y1_train)\nmse = mean_squared_error(y1_test, lasso.predict(x_test))\nmse_dollar = mse ** (1\/2) * ystd[0] + ymean[0]\nprint('MSE: ', mse, '\\nMSE in dollar amount: ', mse_dollar)\ncoeffs1 = pd.Series(lasso.coef_, index = x.columns)\n# coeffs = pd.DataFrame(lasso.coef_, columns = x.columns).transpose()\nprint(coeffs1[coeffs1!=0])\n# print(coeffs1)","fc28e46b":"lasso.summary","64c95ca1":"from sklearn.svm import SVR, SVC","2f31563a":"#split into train and test\nsvr_train, svr_test = train_test_split(df_model, test_size = 0.2, random_state = 1)\n# svm_train, svm_test = train_test_split(df_svm, test_size = 0.2, random_state = 1)\n# x_svm = svm_test[xcol]\n# y_svm = svm_test['Ski pass price adult']\n\n#split svr into 5 subdataframes for CV\nfold = 10\nsvr_dfs = svr_train.sample(frac = 1, random_state = 1)\nsvr_folds = np.array_split(svr_dfs, fold)\n\n# #split svm into 5 subdataframes for cross validation\n# byrow = svm_train.sample(frac = 1, random_state = 1) #set the split by row\n# folds = np.array_split(byrow, 5) #split dataframe into 5 so that 20% is testing and 80% is training","1b0d4db2":"#rbf\ngamma = [0.0001, 0.001, 0.01, 0.1, 0.5, 1, 2, 3, 4, 5] #range of gamma\n# c = [0.00001, 0.0001, 0.001, 0.01, 0.1] #range of c\nc = [1, 5, 10, 50, 100, 200, 300, 400, 500] #c for CV\nmses = list() #store mse for each gamma\n\nfor eachgamma in gamma:\n    mse_gamma = list()\n    for eachc in c:\n        mse_fold = list() #store mse for current gamma, c and each fold\n        for eachfold in range(fold): #for each number of fold\n            fold_num = list(np.arange(0, fold)) #create list of indices to refer which dataframe in folds is testing\n            fold_num.pop(eachfold) #pop current fold number\n\n            #form training and testing set\n            test = svr_folds[eachfold] #let current fold number be testing\n            train_list = list() #list for list of subdataframes to form training\n            for eachtrain in fold_num: #for each training fold number \n                train_list.append(svr_folds[eachtrain]) #append the training sub-dataframe\n            train = pd.concat(train_list) #combine all the training data\n\n            #get x and y\n            y_train_svr = train[ycol[0]]\n            x_train_svr = train[xcol]\n            y_test_svr = test[ycol[0]]\n            x_test_svr = test[xcol]\n            \n            #modeling\n            svr = SVR(gamma = eachgamma, C = eachc)\n            svr.fit(x_train_svr, y_train_svr)\n            mse = ((y_train_svr - svr.predict(x_train_svr)) ** 2).sum()\/len(y_train_svr)\n            mse_fold.append(mse) #store mse of current fold\n        mse_gamma.append(mean(mse_fold)) #store the mean mse for current gamma and c\n    mses.append(mse_gamma)\n\n#plotting \nmin_mse_list = list()\nfor eachmse in range(len(mses)):\n    plt.scatter(c, mses[eachmse], label = gamma[eachmse])\n    min_mse_list.append(min(mses[eachmse]))\nplt.xlabel('c')\nplt.ylabel('mse')\nplt.title('MSE vs C per gamma')\nplt.legend(title = 'gamma', loc = 'lower right')\nplt.show()\n\n#best gamma\nmin_mse = min(min_mse_list) #overall min value\nbest_gamma = gamma[min_mse_list.index(min_mse)] #find the gamma that has the min mse\nbest_gamma_mse = mses[gamma.index(best_gamma)] #get all the mse of best gamma\nbest_c = c[best_gamma_mse.index(min_mse)] #get best c in best gamma\nprint('best gamma is: ', best_gamma, '\\nbest c is: ', best_c)\n\n#investigate gamma = 2\n# c = np.arange(0.001, 0.1, 1e-3) #for new range of c, where mse declines\n# mses = list()\n# for eachc in c:\n#     svr = SVR(gamma = 2, C = eachc)\n#     mse = cross_val_score(svr, X = x_train, y = y1_train, cv=5, scoring = 'neg_mean_squared_error')\n#     if (mse<0).any():\n#         print(mse)\n#     mses.append(mean(mse))\n# minmse = min(mses)\n# minc = c[mses.index(minmse)]\n# plt.plot(c, mses)\n# plt.scatter(minc, minmse, color = 'red')\n# plt.xlabel('c')\n# plt.ylabel('mse')\n# plt.title('C vs MSE when gamma = 2')\n# plt.show()\n# print('c that provides the smallest mse is: ', minc, '\\nmin training mse is: ', minmse)\n# #best c = 0.032\n\n#get testing error for gamma = 2, c = minc = 0.032\nx_svr = svr_test[xcol]\ny_svr = svr_test['Ski pass price adult']\ntest_svr = SVR(gamma = best_gamma, C = best_c)\ntest_svr.fit(x_svr, y_svr)\ntest_mse = mean_squared_error(y_svr, test_svr.predict(x_svr))\nmse_dollar = test_mse ** (1\/2) * ystd[0] + ymean[0]\nprint('testing mse: ', test_mse, '\\ntesting mse in dollar', mse_dollar)","22c3205f":"from sklearn.metrics import accuracy_score","65f15134":"#try split by median\nmedian_price = median(df_cat['Ski pass price adult'])\ndf_above_mean = df_cat[df_cat['Ski pass price adult'] > median_price]\nprint(df_above_mean.shape[0]\/df_cat.shape[0] * 100) #will split dataframe by half","d84c1735":"#split by median of adult price\ndf_svm = df_cat.copy()\ndf_svm = df_svm.drop('Ski pass price children', axis = 1) \ndf_svm.loc[df_svm['Ski pass price adult'] <= median_price, 'Ski pass price adult'] = 0\ndf_svm.loc[df_svm['Ski pass price adult'] > median_price, 'Ski pass price adult'] = 1\ndf_svm['Ski pass price adult'] = df_svm['Ski pass price adult'].astype('category')\n\n#split into train and test\nsvm_train, svm_test = train_test_split(df_svm, test_size = 0.2, random_state = 1)\nx_svm = svm_test[xcol]\ny_svm = svm_test['Ski pass price adult']\n\n#split svm into 5 subdataframes for cross validation\nfold = 5\nbyrow = svm_train.sample(frac = 1, random_state = 1) #set the split by row\nfolds = np.array_split(byrow, 5) #split dataframe into 5 so that 20% is testing and 80% is training\n\n# #split into subdataframes\n# fold = 5\n# byrow = svm_train.sample(frac = 1, random_state = 1) #set the split by row\n# folds = np.array_split(byrow, 5) #split dataframe into 5 so that 20% is testing and 80% is training\naccuracies = list() #record mse of each fold\n\n#modeling procedure\nc = [0.001, 0.01, 0.1, 1, 2, 3, 5, 10, 20, 30, 40, 50, 100] #c for CV\nfor eachc in c:\n    accuracy_c = list()\n    for eachfold in range(fold): #for each number of fold\n        fold_num = list(np.arange(0, fold)) #create list of indices to refer which dataframe in folds is testing\n        fold_num.pop(eachfold) #pop current fold number\n\n        #form training and testing set\n        test = folds[eachfold] #let current fold number be testing\n        train_list = list() #list for list of subdataframes to form training\n        for eachtrain in fold_num: #for each training fold number \n            train_list.append(folds[eachtrain]) #append the training sub-dataframe\n        train = pd.concat(train_list) #combine all the training data\n\n        #get x and y\n        y_train_svm = train[ycol[0]]\n        x_train_svm = train[xcol]\n        y_test_svm = test[ycol[0]]\n        x_test_svm = test[xcol]\n\n        #modeling\n#     for eachc in c:\n        svm = SVC(C = eachc, kernel='linear')\n        svm.fit(x_train_svm, y_train_svm)\n        accuracy = accuracy_score(y_test_svm, svm.predict(x_test_svm))\n        accuracy_c.append(accuracy)\n    accuracies.append(mean(accuracy_c))\n\n#graphing accuracy VS c\nmaxaccuracy = max(accuracies) #find the highest accuracy\nmaxc = c[accuracies.index(maxaccuracy)] #find c that has highest accuracy\n\nfig = plt.figure() #figure\nax = fig.add_subplot(111) #add ax to same figure\nplt.scatter(c, accuracies)\nplt.xlabel('c')\nplt.ylabel('Mean Accuracy')\nplt.title('Mean Accuracy of Each C value')\nplt.scatter(maxc, maxaccuracy)\nannotate_text = 'max c: ' + str(\"%.2f\" % maxc) + '\\nmax accuracy' + str(\"%.2f\" % maxaccuracy)\nax.annotate(s = annotate_text, xy = (maxc, maxaccuracy))\nplt.show()\n\n#investigate maxc = 0.1\nx_svm = svm_test[xcol]\ny_svm = svm_test['Ski pass price adult']\nsvm_all = SVC(C = maxc, kernel = 'linear')\nsvm_all.fit(x_svm, y_svm)\naccuracy_all = accuracy_score(y_svm, svm_all.predict(x_svm))\nprint('accuracy is: ', accuracy_all)","9610dd29":"# def svmcv(fold, df, ycol, xcol, random = None):\n'''performing CV for svm\n    input:\n    fold = number of folds wanted in CV\n    df = the dataframe to use\n    ycol = name of the column in df as response variable, only 1\n    xcol = names of columns in df as predicting variables\n    random = random_state number for splitting\n    \n    output:\n    accuracy = mean accuracy of this CV\n'''\nkernel = ['linear', 'poly', 'rbf']\naccuracy_kernel = list()\nacc_calculated = list()\nfor eachkernel in kernel:\n    accuracy_c = list()\n    acc_cal_c = list()\n    for eachc in c:\n        byrow = svm_train.sample(frac = 1, random_state = 1)\n        folds = np.array_split(byrow, 5)\n        accuracy_fold = list()\n        acc_cal_fold = list()\n        for eachfold in range(fold): #for each number of fold\n            fold_num = list(np.arange(0, fold)) #create list of indices to refer which dataframe in folds is testing\n            fold_num.pop(eachfold) #pop current fold number\n\n            #form training and testing set\n            test = folds[eachfold] #let current fold number be testing\n            train_list = list() #list for list of subdataframes to form training\n            for eachtrain in fold_num: #for each training fold number \n                train_list.append(folds[eachtrain]) #append the training sub-dataframe\n            train = pd.concat(train_list) #combine all the training data\n\n            #get x and y\n            y_train_svm = train[ycol[0]]\n            x_train_svm = train[xcol]\n            y_test_svm = test[ycol[0]]\n            x_test_svm = test[xcol]\n            \n            #modeling\n            svm = SVC(C = eachc, kernel = eachkernel, gamma = 'auto')\n            svm.fit(x_train_svm, y_train_svm)\n            accuracy = accuracy_score(y_test_svm, svm.predict(x_test_svm))\n            accuracy_fold.append(accuracy)\n            correct_count = (y_test_svm == svm.predict(x_test_svm)).sum()\n            acc_cal_fold.append(correct_count)\n        accuracy_c.append(mean(accuracy_fold))\n        acc_cal_c.append(mean(acc_cal_fold))\n    accuracy_kernel.append(accuracy_c)\n    acc_calculated.append(acc_cal_c)\n\n#plotting accuracy VS c of each kernel\nplt.scatter(c, accuracy_kernel[0], label = kernel[0])\nplt.scatter(c, accuracy_kernel[1], label = kernel[1])\nplt.scatter(c, accuracy_kernel[2], label = kernel[2])\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.title('Accuracy VS C by Kernel')\nplt.legend()\nplt.show()\n\n#summary of graph\naccuracies = list() #store the best accuracy of each kernel\nfor eachkernel in range(len(kernel)):\n    accuracies.append(max(accuracy_kernel[eachkernel]))\n\nmax_accuracy = max(accuracies) #get the best accuracy among all kernels\nmax_kernel = kernel[accuracies.index(max_accuracy)] #get the kernel that has the best accuracy\nk_accuracy = accuracy_kernel[kernel.index(max_kernel)] #get the accuracies of best kernel\nmax_c = c[k_accuracy.index(max_accuracy)] #find the c corresponding to the best kernel\nprint('best kernel: ', max_kernel, '\\nmax training accuracy: ', max_accuracy, '\\nmax c: ', max_c)\n\n#investigate best c and best kernel \nsvm_best = SVC(C = max_c, kernel = max_kernel, gamma = 'auto')\nsvm_best.fit(x_svm, y_svm)\n# df_svm_coef = pd.DataFrame({'col':xcol, 'coef':svm_best.dual_coef_[0]})\n# print(len(svm_best.dual_coef_[0]),x_svm.shape)\naccuracy_best = accuracy_score(y_svm, svm_best.predict(x_svm))\nprint('testing accuracy: ', accuracy_best)","38f1c4d2":"print(acc_calculated[1], acc_calculated[2])","3ff846f0":"from sklearn.ensemble import RandomForestClassifier as RFC","daeede8d":"def tr_ts_fold(n_fold, folds, xcol, ycol):\n    dfs = list()\n    for eachfold in range(n_fold):\n        fold_num = list(np.arange(0, n_fold)) #create list of indices to refer which dataframe in folds is testing\n        fold_num.pop(eachfold) #pop current fold number\n\n        #form training and testing set\n        test = folds[eachfold] #let current fold number be testing\n        train_list = list() #list for list of subdataframes to form training\n        for eachtrain in fold_num: #for each training fold number \n            train_list.append(folds[eachtrain]) #append the training sub-dataframe\n        train = pd.concat(train_list) #combine all the training data\n\n        #get x and y\n        y_train = train[ycol]\n        x_train = train[xcol]\n        y_test = test[ycol]\n        x_test = test[xcol]\n        \n        #append dataframes for this fold\n        fold_df = [x_train, y_train, x_test, y_test]\n        dfs.append(fold_df)\n    return dfs","04dd4bf4":"# m = int(len(xcol)**(1\/2)) #number of features considered at each split\nn_bs = 0.5 #percentage of observations for bootstrapping\nn_trees = np.arange(10, 110, 10)\ndepth = [2, 5, 10, 20, 30, 40, 50, 100] #depth of trees\naccuracy = list()\ndf_fold = tr_ts_fold(5, folds, xcol, y1col[0])\nfor eachtree in n_trees:\n    tree_accuracy = list()\n    for eachdepth in depth:\n        depth_accuracy = list()\n        for eachfold in range(len(df_fold)): #for each number of fold\n            #get train and test data for current fold\n            current_fold = df_fold[eachfold]\n            x_train_rf = current_fold[0]\n            y_train_rf = current_fold[1]\n            x_test_rf = current_fold[2]\n            y_test_rf = current_fold[3]\n\n            #modeling\n            rfc = RFC(n_estimators = eachtree,\n                      max_depth = eachdepth, \n                      max_features = 'sqrt', \n                      bootstrap = True, \n    #                   max_samples = n_bs, \n                      random_state = 0)\n            rfc.fit(x_train_rf, y_train_rf)\n            depth_accuracy.append(rfc.score(x_test_rf, y_test_rf)) #record accuracy of current fold\n        tree_accuracy.append(mean(depth_accuracy)) #store mean fold accuracy as accuracy for current depth\n    accuracy.append(tree_accuracy)\n\n# #plot accuracy by depth\n# max_accuracy_list = list()\n# for each in range(len(accuracy)):\n#     max_accuracy_list.append(max(accuracy[each]))\n#     plt.scatter(depth, accuracy[each], label = n_trees[each])\n# plt.legend(title = 'number of trees')\n# plt.xlabel('depth of trees')\n# plt.ylabel('accuracy')\n# plt.title('Accuracy VS Depth of Trees by Number of Trees')\n# plt.show()\n\n# #finding the best depth and number of trees\n# max_accuracy = max(max_accuracy_list)","2f2621f6":"#plot accuracy by depth\nmax_accuracy_list = list()\nfor each in range(len(accuracy)):\n    max_accuracy_list.append(max(accuracy[each]))\n    plt.scatter(depth, accuracy[each], label = n_trees[each])\nplt.legend(title = 'number of trees')\nplt.xlabel('depth of trees')\nplt.ylabel('accuracy')\nplt.title('Accuracy VS Depth of Trees by Number of Trees')\nplt.show()\n\n#finding the best depth and number of trees\nmax_accuracy = max(max_accuracy_list)\nbest_tree = n_trees[max_accuracy_list.index(max_accuracy)]\nbest_tree_accuracy = accuracy[max_accuracy_list.index(max_accuracy)]\nbest_depth = depth[best_tree_accuracy.index(max_accuracy)]\nprint('highest accuracy: ', max_accuracy, '\\nbest depth: ', best_depth, '\\nbest number of trees: ', best_tree)","9be33786":"#find the best depth of best number of trees\ndepth = np.arange(5, 20)\nacc = list()\nfor eachdepth in depth: #for each new range of depth\n    depth_acc = list()\n    for eachfold in range(len(df_fold)): #for each number of fold\n        #get train and test data for current fold\n        current_fold = df_fold[eachfold]\n        x_train_rf = current_fold[0]\n        y_train_rf = current_fold[1]\n        x_test_rf = current_fold[2]\n        y_test_rf = current_fold[3]\n\n        #modeling\n        rfc = RFC(n_estimators = best_tree,\n                  max_depth = eachdepth, \n                  max_features = 'sqrt', \n                  bootstrap = True, \n    #             max_samples = n_bs, \n                  random_state = 0)\n        rfc.fit(x_train_rf, y_train_rf)\n        depth_acc.append(rfc.score(x_test_rf, y_test_rf)) #record accuracy of current fold\n    acc.append(mean(depth_acc))","ef7401fc":"#plotting\nplt.scatter(depth, acc)\nplt.xlabel('depth of trees')\nplt.ylabel('accuracy')\nplt.title('Accuracy VS Depth of Trees for 30 Trees in Forest')\nbest_acc = max(acc)\nbest_depth = depth[acc.index(best_acc)]\nprint('highest accuracy: ', best_acc, '\\nbest depth: ', best_depth)\n\n#fit data to model\nrfc_test = RFC(n_estimators = best_tree, \n               max_depth = best_depth, \n               max_features = 'sqrt', \n               bootstrap = True, \n               random_state = 0)\nrfc_test.fit(x_svm, y_svm)\nprint('testing accuracy: ', rfc_test.score(x_svm, y_svm))","440b9ef1":"# Machine Learning","1daef274":"Ridge","7dfe350b":"SVM - Support Vector Machine","5e27b5f8":"Linear Regression","6430183c":"Check why poly and rbf are not affected by C","bcbe4f3b":"Poly","f82bf136":"Decision Trees","984bb763":"try larger range for c, smaller values for gamma, more folds for CV, double check MSE calculation","22ddb0dd":"Lasso","bd52f748":"SVR - Support Vector Regression","2fa56cfd":"Regression: linear, poly, lasso, ridge, exhaustive selection, random forest, bagging, support vector machine","900d0064":"# Preprocessing"}}