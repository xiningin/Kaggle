{"cell_type":{"46a77aeb":"code","d24ce987":"code","6fdb3e19":"code","19c36015":"code","5ecb612a":"code","9c9dd77d":"code","9c60f042":"code","64800ca3":"code","f1053bd7":"code","0ac2e73d":"code","fde171f1":"code","cdba5199":"code","d6a5719d":"code","cb4b32cd":"code","f0906159":"code","95289943":"code","853aa169":"code","a8d393c1":"code","03cec303":"code","d3db3212":"code","4288069d":"code","db042365":"code","da523b70":"markdown"},"source":{"46a77aeb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d24ce987":"diabetes_data = pd.read_csv(os.path.join(dirname, filename))\nprint(\"Check the number of records:  \", diabetes_data.shape)\ndiabetes_data.head()","6fdb3e19":"diabetes_data.info(verbose=True)","19c36015":"diabetes_data.describe()","5ecb612a":"import plotly.graph_objs as go\nimport plotly.offline as py\ndef plotPie(df, nameOfFeature, labels):\n#     labels = [str(df[nameOfFeature].unique()[i]) for i in range(df[nameOfFeature].nunique())]\n    values = [df[nameOfFeature].value_counts()[i] for i in range(df[nameOfFeature].nunique())]\n    trace=go.Pie(labels=labels,values=values)\n    py.iplot([trace])","9c9dd77d":"df = diabetes_data.copy(deep = True)\ndf['Outcome'] = df['Outcome'].replace({1: 'Positive', 0: 'Negative'})\n# print(df[df['Outcome']=='Positive'].count())\nplotPie(df, 'Outcome', ['Negative','Positive'])\n","9c60f042":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.heatmap(diabetes_data.corr(),annot=True, cmap = 'YlGnBu')\nfig = plt.gcf()\nfig.set_size_inches(10, 8)\nplt.show()","64800ca3":"# diabetes_data_copy = diabetes_data.copy(deep = True)\n\nfor col in diabetes_data:\n    if(col != 'Pregnancies' and col!='SkinThickness' and col != 'Outcome' and col != 'DiabetesPedigreeFunction'):\n        diabetes_data[col] = diabetes_data[col].replace(0,np.NaN)\n## showing the count of Nans\nprint(diabetes_data.isnull().sum())","f1053bd7":"for col in diabetes_data:\n    diabetes_data[col].fillna(diabetes_data[col].mean(), inplace = True)\nprint(diabetes_data.isnull().sum())","0ac2e73d":"sns.heatmap(diabetes_data.corr(),annot=True, cmap = 'YlGnBu')\nfig = plt.gcf()\nfig.set_size_inches(10, 8)\nplt.show()","fde171f1":"plt.figure(figsize=(60,60))\nfor index, col in enumerate(diabetes_data,1):\n    if(col == 'Outcome'):\n        continue\n    sub_plot_index = (index - 1) *2\n    plt.subplot(3,3, index)\n    sns.violinplot(x='Outcome', y=f'{col}', data=diabetes_data, palette='muted', split=True)\n","cdba5199":"X = diabetes_data.drop(['Outcome'],axis = 1)\nY = diabetes_data.Outcome","d6a5719d":"#importing train_test_split\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.25,random_state=42, stratify=Y)","cb4b32cd":"X_feature_imp = diabetes_data[['Glucose','BMI','Age','Pregnancies']]\nY_feature_imp = diabetes_data.Outcome\nX_train_feature_imp,X_test_feature_imp,Y_train_feature_imp,Y_test_feature_imp = train_test_split(X_feature_imp,Y_feature_imp,test_size=0.25,random_state=42, stratify=Y_feature_imp)","f0906159":"print(X_train.shape, Y_train.shape)\nprint(X_test.shape, Y_test.shape)","95289943":"# def train(X_train, X_test, Y_train, Y_test, model):\nfrom sklearn.metrics import classification_report, confusion_matrix\ndef train(model):\n    model.fit(X_train, Y_train)\n    Y_pred = model.predict(X_test)\n    score = metrics.accuracy_score(Y_test, Y_pred)\n    print('Score: %0.4f'%score)\n#     print(classification_report(model.predict(X_test), Y_test))\n    cm = confusion_matrix(Y_test, Y_pred)\n    sns.set(color_codes =True)\n    sns.set(font_scale=1.5)\n    sns.heatmap(cm, annot=True, fmt='g')\n    plt.show()\n    return model","853aa169":"# def train(X_train, X_test, Y_train, Y_test, model):\ndef train_feature_imp(model):\n    model.fit(X_train_feature_imp, Y_train_feature_imp)\n    Y_pred_feature_imp = model.predict(X_test_feature_imp)\n    score = metrics.accuracy_score(Y_test_feature_imp, Y_pred_feature_imp)\n    print('Score: %0.4f'%score)\n#     print(classification_report(model.predict(X_test_feature_imp), Y_test_feature_imp))\n    cm = confusion_matrix(Y_test_feature_imp, Y_pred_feature_imp)\n    sns.set(color_codes =True)\n    sns.set(font_scale=1.5)\n    sns.heatmap(cm, annot=True, fmt='g')\n    plt.show()\n    return model","a8d393c1":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nmodel_LR = LogisticRegression(max_iter=2500)\ntrain(model_LR)\ntrain_feature_imp(model_LR)","03cec303":"accuracy_rate = []\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfor i in range(1, 30):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    score = cross_val_score(knn, X, Y, cv=20)\n    accuracy_rate.append(score.mean())\naccuracy_rate","d3db3212":"plt.figure(figsize=(10,10))\nplt.plot(range(1,30),accuracy_rate, color='blue', linestyle='dashed', marker='o',markerfacecolor='red', markersize=10)\nplt.title('accuracy_rate vs K value')\nplt.xlabel('K value')\nplt.ylabel('accuracy_rate')","4288069d":"\nmodel_knn = KNeighborsClassifier(n_neighbors=20)\ntrain(model_knn)\ntrain_feature_imp(model_knn)","db042365":"from sklearn.ensemble import RandomForestClassifier\nmodel_rdc = RandomForestClassifier()\ntrain(model_rdc)\ntrain_feature_imp(model_rdc)","da523b70":"Data Exploration"}}