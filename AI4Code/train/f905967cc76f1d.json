{"cell_type":{"0bf2c732":"code","eb87625d":"code","134a2c6a":"code","e4c17026":"code","d03d685b":"code","177f8f22":"code","bb5a6c3d":"code","fca7636d":"code","2da7ffc6":"code","3f25d04a":"code","ad368962":"code","8ee0f646":"code","8e874496":"code","fa0f5f81":"code","f975645a":"code","8de99882":"code","0b1536cc":"code","e84fd22d":"code","7860dedb":"code","30496978":"code","55b29c6e":"code","4d4d8fc8":"code","ce59da7a":"code","ddbdf7dd":"code","55b7edec":"code","4719abfc":"code","b78fdedc":"code","731da78e":"code","32f448ee":"code","cd338cf5":"code","da3dc8bd":"code","9e7dff8b":"code","5645c995":"code","33e57931":"code","529b0cca":"code","574a186c":"code","7c4df073":"code","958180cc":"code","11165c50":"code","5e789451":"code","3749a62b":"code","12d01b34":"code","0677a3cf":"code","28c13d5b":"code","a041a9fa":"code","b3c4aba2":"code","4d06b36e":"code","6e28b444":"code","f4963628":"code","32d77e2d":"code","023a4ff9":"code","9d6504e4":"code","3d404466":"code","cb98f259":"code","032e72b4":"code","daaafe2e":"code","cd138f8a":"code","eac10d3c":"code","6401054a":"code","145430c1":"code","488038a0":"code","30aea600":"code","df1f7c46":"code","13a7f7b0":"code","3b85a7d4":"code","be4fc9af":"code","9439248a":"code","9e6071e9":"code","8433e72b":"code","1221a3ec":"code","ccf69910":"code","d460419a":"code","bd3af67e":"code","d4d539ec":"code","e6e938de":"code","1944496a":"code","5a4954d2":"code","887eec6f":"code","54a41470":"code","fc474ce4":"code","d8971294":"code","e84b90ee":"markdown","4e1ba897":"markdown","21f41904":"markdown","497e3597":"markdown","8db12a9c":"markdown","7ff4b310":"markdown","5c16daf7":"markdown","002bb99e":"markdown","772568f8":"markdown","429368e6":"markdown","a6339dda":"markdown","eabb1837":"markdown","c3e8f732":"markdown","0b8b456d":"markdown","27072e04":"markdown","89807d43":"markdown","33898055":"markdown","13dbc740":"markdown","a81ab39d":"markdown","c899397d":"markdown","81476276":"markdown","d7c60935":"markdown","b7e7c1fc":"markdown","c9a5427e":"markdown","26c8e2ac":"markdown","3d7dad4f":"markdown","3d75da27":"markdown","f341f6f0":"markdown","2bf03283":"markdown","af23b327":"markdown","176b8e14":"markdown","071f07cc":"markdown","8af45d07":"markdown","7c1efc53":"markdown","f08286fb":"markdown","d5188da4":"markdown","fce94f9c":"markdown","b0f3c3f2":"markdown","6e452280":"markdown","974132de":"markdown","aa203720":"markdown","74643679":"markdown","f76bb353":"markdown","70afde7f":"markdown","d91df6c7":"markdown","19944753":"markdown","72d284c7":"markdown","b24d0f20":"markdown","13602441":"markdown","2a9cc458":"markdown","cc89b0d8":"markdown","b6dd6f38":"markdown","a4f03f65":"markdown","5be3b146":"markdown","9dccb232":"markdown","fdc98a50":"markdown","b53d3f5b":"markdown","6c3af270":"markdown","d6d6c4b7":"markdown","1e11993a":"markdown","0aa3f99a":"markdown","78c258e5":"markdown","43e1c3d5":"markdown","93548315":"markdown","a71834da":"markdown","22ec3f36":"markdown","b9d7ec3c":"markdown","ea2e38b6":"markdown","19b38876":"markdown","f519844a":"markdown","aae3c59d":"markdown","24ec1107":"markdown","d56745d8":"markdown","c85b5bf1":"markdown","73bdd038":"markdown","d99f3862":"markdown","dfbc2373":"markdown","c0917325":"markdown","74ccdef8":"markdown","d41b9250":"markdown","92bd4928":"markdown","f558ce37":"markdown","d0cf7201":"markdown","03f44cff":"markdown","5cb1645e":"markdown","f0dbfaf7":"markdown","71f8dd8b":"markdown","5e434493":"markdown","ea47fca0":"markdown","b7a314d0":"markdown","db45f4e0":"markdown","7a33b1f9":"markdown","977c2926":"markdown","dd8367ba":"markdown","b5050c9e":"markdown"},"source":{"0bf2c732":"#import some necessary librairies\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\n\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n\n\nfrom subprocess import check_output\n#print(check_output([\"ls\", \"housing\"]).decode(\"utf8\")) #check the files available in the directory","eb87625d":"#Now let's import and put the train and test datasets in  pandas dataframe\n\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n","134a2c6a":"##display the first five rows of the train dataset.\ntrain.T","e4c17026":"##display the first five rows of the test dataset.\ntest.head(5)\n","d03d685b":"#check the numbers of samples and features\nprint(\"The train data size before dropping Id feature is : {} \".format(train.shape))\nprint(\"The test data size before dropping Id feature is : {} \".format(test.shape))\n\n#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n\n#check again the data size after dropping the 'Id' variable\nprint(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(test.shape))","177f8f22":"sns.set_theme(rc = {'grid.linewidth': 0.5,\n                    'axes.linewidth': 0.75, 'axes.facecolor': '#fff3e9', 'axes.labelcolor': '#6b1000',\n                    'figure.facecolor': '#f7e7da',\n                    'xtick.labelcolor': '#6b1000', 'ytick.labelcolor': '#6b1000'})","bb5a6c3d":"train_missing = train.count().loc[train.count() < 1460].sort_values(ascending = False)\n\nwith plt.rc_context(rc = {'figure.dpi': 120, 'axes.labelsize': 8.5, \n                          'xtick.labelsize': 6, 'ytick.labelsize': 6}): \n\n    fig, ax = plt.subplots(1, 1, figsize = (6, 4))\n\n    sns.barplot(x = train_missing.values, y = train_missing.index, palette = 'viridis')\n\n    plt.xlabel('Non-Na values')\n\n    plt.show()","fca7636d":"test_missing = test.count().loc[test.count() < 1459].sort_values(ascending = False)\nwith plt.rc_context(rc = {'figure.dpi': 120, 'axes.labelsize': 8.5, \n                          'xtick.labelsize': 6, 'ytick.labelsize': 6}):\n\n    fig, ax = plt.subplots(1, 1, figsize = (7, 6))\n\n    sns.barplot(x = test_missing.values, y = test_missing.index, palette = 'viridis')\n\n    plt.xlabel('Non-Na values')\n\n    plt.show()","2da7ffc6":"\nfig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()\n","3f25d04a":"#Deleting outliers\n#train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n\n#Check the graphic again\nfig, ax = plt.subplots()\nax.scatter(train['GrLivArea'], train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","ad368962":"sns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","8ee0f646":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n#Check the new distribution \nsns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()\n","8e874496":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","fa0f5f81":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)","f975645a":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","8de99882":"#Correlation map to see how features are correlated with SalePrice\ncorrmat = train.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","0b1536cc":"all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")","e84fd22d":"all_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")","7860dedb":"all_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")","30496978":"all_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")","55b29c6e":"all_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")","4d4d8fc8":"#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","ce59da7a":"for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')","ddbdf7dd":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)","55b7edec":"for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)","4719abfc":"for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')","b78fdedc":"all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)","731da78e":"all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])","32f448ee":"all_data = all_data.drop(['Utilities'], axis=1)","cd338cf5":"all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")","da3dc8bd":"all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])","9e7dff8b":"all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])","5645c995":"all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])","33e57931":"all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])","529b0cca":"\nall_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")\n\n","574a186c":"#Check remaining missing values if any \nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()","7c4df073":"sns.set_theme(rc = {'grid.linewidth': 0.5,\n                    'axes.linewidth': 0.75, 'axes.facecolor': '#fff3e9', 'axes.labelcolor': '#6b1000',\n                    'figure.facecolor': '#f7e7da',\n                    'xtick.color': '#6b1000', 'ytick.color': '#6b1000'})","958180cc":"numeric_features = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', \n            'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', \n            '2ndFlrSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', \n            'OpenPorchSF', 'EnclosedPorch', 'ScreenPorch', 'MiscVal', \n            '3SsnPorch' , 'PoolArea' , 'LowQualFinSF']","11165c50":"# with plt.rc_context(rc = {'figure.dpi': 300, 'axes.labelsize': 8,\n#                           'xtick.labelsize': 6, 'ytick.labelsize': 6}):\n#     fig_0, ax_0 = plt.subplots(5, 5, figsize = (8, 7))\n\n#     for idx, (column, axes) in list(enumerate(zip(numeric_features, ax_0.flatten()))):\n#         sns.scatterplot(ax = axes, x = all_data.loc[:1459, [column]][column],\n#                         y = np.log(y_train),\n#                         hue =  np.log(y_train),\n#                         palette = 'viridis', alpha = 0.7, s = 8)\n#     ### Getting rid of a legend\n#         axes.legend([], [], frameon = False)\n#     ### Removing empty figures\n#     else:\n#         [axes.set_visible(False) for axes in ax_0.flatten()[idx + 1:]]\n\n# plt.tight_layout()\n# plt.show()","5e789451":"#MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)\n","3749a62b":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))\n\n\n","12d01b34":"from sklearn.feature_selection import mutual_info_regression\ndef make_mi_scores(X, y):\n    X = X.copy()\n    for colname in X.select_dtypes([\"object\", \"category\"]):\n        X[colname], _ = X[colname].factorize()\n    # All discrete features should now have integer dtypes\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores","0677a3cf":"mi_scores = make_mi_scores(all_data[:ntrain], y_train)\nmi_scores","28c13d5b":"# Adding features that track if an amenity is present\nall_data['hasPool'] = all_data['PoolArea'].apply(lambda x: \"1\" if x > 0 else \"0\")\nall_data['hasGarage'] = all_data['GarageArea'].apply(lambda x: \"1\" if x > 0 else \"0\")\nall_data['hasBsmt'] = all_data['TotalBsmtSF'].apply(lambda x: \"1\" if x > 0 else \"0\")\nall_data['hasFireplace'] = all_data['Fireplaces'].apply(lambda x: \"1\" if x > 0 else \"0\")\n# Adding remodel caused a drop in score\n#all_data['Remodel'] = all_data['YearRemodAdd'].apply(lambda x: \"1\" if x > 0 else \"0\")\n\n#all_data.drop([\"YearRemodAdd\", \"PoolArea\", \"GarageArea\", \"Fireplaces\"], axis=1, inplace=True)\n","a041a9fa":"# Adding total sqfootage feature\nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n\n#all_data.drop([\"TotalBsmtSF\", '1stFlrSF', '2ndFlrSF'], axis=1, inplace=True)\n\n","b3c4aba2":"\n# Calculate house age (doesn't account for remodels)\nall_data['Age'] = all_data['YrSold'] - all_data['YearBuilt']\nall_data['Age'].clip(lower=0, inplace=True)\nall_data['Age'].describe()\n\n# Dropping those features resulted in a drop in score\n# all_data.drop([\"YrSold\", 'YearBuilt'], axis=1, inplace=True)\n","4d06b36e":"all_data['TotalBath'] = all_data['FullBath'] + (all_data['HalfBath'] * 0.5) + all_data['BsmtFullBath'] + (all_data['BsmtHalfBath'] * 0.5)\n","6e28b444":"from sklearn.cluster import KMeans\ncluster_features = [\n    \"LotArea\",\n    \"TotalBsmtSF\",\n    \"1stFlrSF\",\n    \"2ndFlrSF\",\n    \"GrLivArea\",\n    \"GarageArea\"\n]\n#all_data.drop([\"TotalBsmtSF\", '1stFlrSF', '2ndFlrSF'], axis=1, inplace=True)\n\n\ndef cluster_labels(df, features, n_clusters=4):\n    X = df.copy()\n    X_scaled = X.loc[:, features]\n    X_scaled = (X_scaled - X_scaled.mean(axis=0)) \/ X_scaled.std(axis=0)\n    kmeans = KMeans(n_clusters=n_clusters, n_init=100, random_state=0)\n    X_new = pd.DataFrame()\n    X_new[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n    return X_new\n\n\ndef cluster_distance(df, features, n_clusters=20):\n    X = df.copy()\n    X_scaled = X.loc[:, features]\n    X_scaled = (X_scaled - X_scaled.mean(axis=0)) \/ X_scaled.std(axis=0)\n    kmeans = KMeans(n_clusters=20, n_init=50, random_state=0)\n    X_cd = kmeans.fit_transform(X_scaled)\n    # Label features and join to dataset\n    X_cd = pd.DataFrame(\n        X_cd, columns=[f\"Centroid_{i}\" for i in range(X_cd.shape[1])]\n    )\n#all_data[\"Cluster sq foot\"] = cluster_labels(all_data, cluster_features)\ntest = cluster_labels(all_data, cluster_features).loc[:1459]\ntest[\"sale price\"] = y_train\n\ny_train\n# Played around with clustering. The total sqft feature seems better than clustering\n#all_data['Cluster sq foot'] = all_data['Cluster sq foot'].astype(str)\n#all_data[\"Cluster sq foot\"].value_counts()\n\n#sns.catplot(x=\"Cluster\", y=\"sale price\", kind=\"boxen\", data=test)","f4963628":"from sklearn.decomposition import PCA\n\ndef apply_pca(X, standardize=True):\n    # Standardize\n    if standardize:\n        X = (X - X.mean(axis=0)) \/ X.std(axis=0)\n    # Create principal components\n    pca = PCA()\n    X_pca = pca.fit_transform(X)\n    # Convert to dataframe\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns=component_names)\n    # Create loadings\n    loadings = pd.DataFrame(\n        pca.components_.T,  # transpose the matrix of loadings\n        columns=component_names,  # so the columns are the principal components\n        index=X.columns,  # and the rows are the original features\n    )\n    return pca, X_pca, loadings\n\n\ndef plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n    )\n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n    )\n    # Set up figure\n    fig.set(figwidth=8, dpi=100)\n    return axs\n\ndef pca_components(df, features):\n    X = df.loc[:, features]\n    return apply_pca(X)\n    return X_pca\n\n\npca_features = [\n    \"GarageArea\",\n    \"YearRemodAdd\",\n    \"TotalBsmtSF\",\n    \"GrLivArea\",\n    \"1stFlrSF\",\n    \"2ndFlrSF\"\n]\npca, X_pca, loadings = pca_components(all_data, pca_features)\nloadings\nplot_variance(pca)","32d77e2d":"#all_data.drop([\"FullBath\", 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath'], axis=1, inplace=True)\n#all_data.drop([\"TotalBsmtSF\", '1stFlrSF', '2ndFlrSF', 'GarageArea', 'GrLivArea', 'LowQualFinSF', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'LotArea'], axis=1, inplace=True)\n#all_data.drop([\"YearRemodAdd\", \"PoolArea\", \"GarageArea\", \"Fireplaces\"], axis=1, inplace=True)\n","023a4ff9":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)\n","9d6504e4":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    all_data[feat] = boxcox1p(all_data[feat], lam)\n    \n#all_data[skewed_features] = np.log1p(all_data[skewed_features])","3d404466":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","cb98f259":"train = all_data[:ntrain]\ntest = all_data[ntrain:]\n","032e72b4":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nimport lightgbm as lgb\n\n","daaafe2e":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","cd138f8a":"house_train_split, house_test_split, sales_price_train_split, sales_price_test_split = train_test_split(train, y_train, test_size=.15, train_size=.85, random_state=42)\nmodel = RandomForestRegressor(n_jobs=-1, n_estimators=500, random_state=42)\nscore = rmsle_cv(model)\nprint(\"\\nRandomForest score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n# model.fit(house_train_split, sales_price_train_split)\n# model.score(house_test_split, sales_price_test_split)\n\n\n# out = model.predict(house_test_split)\n# (model.score(house_test_split, sales_price_test_split), mean_squared_error(out, sales_price_test_split))\n# (0.8849285993244069, 0.021927868421321398)\n# (0.8873154659262069, 0.02147302997774433) # drop other features\n# (0.8882775257200658, 0.0212897010146021) #age\n# (0.886568892398346, 0.02161529613588331) # combine bathrooms\n# (0.8893723179894191, 0.021081078709753925) #anemities\n","eac10d3c":"# sns.set(rc = {'figure.figsize':(15,8)})\n# sorted = model.feature_importances_.argsort()[::-1]\n# model.feature_importances_[sorted]\n# barplot = sns.barplot(y=model.feature_importances_[sorted][:10], x=house_train_split.columns[sorted][:10])","6401054a":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","145430c1":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","488038a0":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)","30aea600":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","df1f7c46":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\n\n","13a7f7b0":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","3b85a7d4":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","be4fc9af":"score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","9439248a":"score = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","9e6071e9":"score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","8433e72b":"\nscore = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","1221a3ec":"score = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","ccf69910":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","d460419a":"averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","bd3af67e":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","d4d539ec":"stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n                                                 meta_model = lasso)\n\nscore = rmsle_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","e6e938de":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","1944496a":"stacked_averaged_models.fit(train.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(train.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\nprint(rmsle(y_train, stacked_train_pred))","5a4954d2":"model_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.expm1(model_xgb.predict(test))\nprint(rmsle(y_train, xgb_train_pred))","887eec6f":"model_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.expm1(model_lgb.predict(test.values))\nprint(rmsle(y_train, lgb_train_pred))","54a41470":"'''RMSE on the entire Train data when averaging'''\n\nprint('RMSLE score on train data:')\nprint(rmsle(y_train,stacked_train_pred*0.70 +\n               xgb_train_pred*0.15 + lgb_train_pred*0.15 ))","fc474ce4":"ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15","d8971294":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = ensemble\nsub.to_csv('submission.csv',index=False)","e84b90ee":"**Skewed features**","4e1ba897":"This competition is very important to me as  it helped me to begin my journey on Kaggle few months ago. I've read  some great notebooks here. To name a few:\n\n1. [Comprehensive data exploration with Python][1] by **Pedro Marcelino**  : Great and very motivational data analysis\n\n2. [A study on Regression applied to the Ames dataset][2] by **Julien Cohen-Solal**  : Thorough features engeneering and deep dive into linear regression analysis  but really easy to follow for beginners.\n\n3. [Regularized Linear Models][3] by **Alexandru Papiu**  : Great Starter kernel on modelling and Cross-validation\n\nI can't recommend enough every beginner to go carefully through these kernels (and of course through many others great kernels) and get their first insights in data science and kaggle competitions.\n\nAfter that (and some basic pratices) you should be more confident to go through [this great script][7] by **Human Analog**  who did an impressive work on features engeneering. \n\nAs the dataset is particularly handy, I  decided few days ago to get back in this competition and apply things I learnt so far, especially stacking models. For that purpose, we build two stacking classes  ( the simplest approach and a less simple one). \n\nAs these classes are written for general purpose, you can easily adapt them and\/or extend them for your regression problems. \nThe overall approach is  hopefully concise and easy to follow.. \n\nThe features engeneering is rather parsimonious (at least compared to some others great scripts) . It is pretty much :\n\n- **Imputing missing values**  by proceeding sequentially through the data\n\n- **Transforming** some numerical variables that seem really categorical\n\n- **Label Encoding** some categorical variables that may contain information in their ordering set\n\n-  [**Box Cox Transformation**][4] of skewed features (instead of log-transformation) : This gave me a **slightly better result** both on leaderboard and cross-validation.\n\n- ** Getting dummy variables** for categorical features. \n\nThen we choose many base models (mostly sklearn based models + sklearn API of  DMLC's [XGBoost][5] and Microsoft's [LightGBM][6]), cross-validate them on the data before stacking\/ensembling them. The key here is to make the (linear) models robust to outliers. This improved the result both on LB and cross-validation. \n\n  [1]: https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python\n  [2]:https:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset\n  [3]: https:\/\/www.kaggle.com\/apapiu\/regularized-linear-models\n  [4]: http:\/\/onlinestatbook.com\/2\/transformations\/box-cox.html\n  [5]: https:\/\/github.com\/dmlc\/xgboost\n [6]: https:\/\/github.com\/Microsoft\/LightGBM\n [7]: https:\/\/www.kaggle.com\/humananalog\/xgboost-lasso\n\nTo my surprise, this does well on LB ( 0.11420 and top 4% the last time I tested it : **July 2, 2017** )\n\n","21f41904":"- **MSSubClass** : Na most likely means No building class. We can replace missing values with None\n","497e3597":"- **MSZoning (The general zoning classification)** :  'RL' is by far  the most common value.  So we can fill in missing values with 'RL'\n","8db12a9c":"###Simplest Stacking approach : Averaging base models","7ff4b310":"**Hope that at the end of this notebook, stacking will be clear for those, like myself, who found the concept not so easy to grasp**","5c16daf7":"## PCA","002bb99e":"##Base models","772568f8":"- **BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath** : missing values are likely zero for having no basement","429368e6":"- **KitchenQual**: Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent)  for the missing value in KitchenQual.\n","a6339dda":"On this gif, the base models are algorithms 0, 1, 2 and the meta-model is algorithm 3. The entire training dataset is \nA+B (target variable y known) that we can split into train part (A) and holdout part (B). And the test dataset is C. \n\nB1 (which is the prediction from the holdout part)  is the new feature used to train the meta-model 3 and C1 (which\nis the prediction  from the test dataset) is the meta-feature on which the final prediction is done. ","eabb1837":"###Note : \n Outliers removal is note always safe.  We decided to delete these two as they are very huge and  really  bad ( extremely large areas for very low  prices). \n\nThere are probably others outliers in the training data.   However, removing all them  may affect badly our models if ever there were also  outliers  in the test data. That's why , instead of removing them all, we will just manage to make some of our  models robust on them. You can refer to  the modelling part of this notebook for that. ","c3e8f732":"Let's see how these base models perform on the data by evaluating the  cross-validation rmsle error","0b8b456d":"**Box Cox Transformation of (highly) skewed features**","27072e04":"###Less simple Stacking : Adding a Meta-model","89807d43":"- **XGBoost** :","33898055":"- **Utilities** : For this categorical feature all records are \"AllPub\", except for one \"NoSeWa\"  and 2 NA . Since the house with 'NoSewa' is in the training set, **this feature won't help in predictive modelling**. We can then safely  remove it.\n","13dbc740":"**Transforming some numerical variables that are really categorical**","a81ab39d":"We use the **cross_val_score** function of Sklearn. However this function has not a shuffle attribut, we add then one line of code,  in order to shuffle the dataset  prior to cross-validation","c899397d":"- **Functional** : data description says NA means typical","81476276":"**Label Encoding some categorical variables that may contain information in their ordering set** ","d7c60935":"###More features engeneering","b7e7c1fc":"- **MiscFeature** : data description says NA means \"no misc feature\"\n","c9a5427e":"**Averaged base models score**","26c8e2ac":"**Import librairies**","3d7dad4f":"###Missing Data","3d75da27":"- **Exterior1st and Exterior2nd** : Again Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string\n","f341f6f0":"- **PoolQC** : data description says NA means \"No  Pool\". That make sense, given the huge ratio of missing value (+99%) and majority of houses have no Pool at all in general. ","2bf03283":"**StackedRegressor:**","af23b327":"**Averaged base models class**","176b8e14":"##Stacking  models","071f07cc":"###Final Training and Prediction","8af45d07":"- **SaleType** : Fill in again with most frequent which is \"WD\"","7c1efc53":"- **LightGBM** :","f08286fb":"We add **XGBoost and LightGBM** to the** StackedRegressor** defined previously. ","d5188da4":"Since area related features are very important to determine house prices, we add one more feature which is the total area of basement, first and second floor areas of each house","fce94f9c":"**Submission**","b0f3c3f2":"**Define a cross validation strategy**","6e452280":"- **Electrical** : It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.\n","974132de":"- **Gradient Boosting Regression** :\n\nWith **huber**  loss that makes it robust to outliers\n    ","aa203720":"-  **LASSO  Regression**  : \n\nThis model may be very sensitive to outliers. So we need to made it more robust on them. For that we use the sklearn's  **Robustscaler()**  method on pipeline ","74643679":"##Copied from https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\n\n* What did you like about the specific submission that you chose?<br>\n    I chose this dataset because it used some interesting techniques that produced a model involving a number of regressions. This notebook produced a \u201cstacked\u201d model which took several regression models and averaged them together. This notebook introduced concepts such as skewedness which is used to normalize data to be used as input into regression type models.\n* Anything you didn\u2019t like?<br>\n    I didn\u2019t like how this dataset didn\u2019t much visualization or feature engineering (in particular adding new features). However this gave me an opportunity to created features where I saw fit.\n* Did you learn something from it?<br>\n    I learned about averaging multiple models, normalization, skewedness, and the box-cox transformation.\n* What changes did you make to it and why?<br>\n    I made the following changes:\n        * Added features to track whether an anemity was present (pool, garage, basement, fireplace)\n        * Calculated the age of the house\n        * Total number of baths.\n        * Tried clustering based on sq ft area features.\n        * Tried removing features.\n\n* Were your results different than the original submission? How so? <br>\n    * The rmsle went down despite the feature engineering I did.\n","f76bb353":"## Clustering","70afde7f":"Getting the new train and test sets. ","d91df6c7":"## Visualize","19944753":"**Ensemble prediction:**","72d284c7":"In this approach, we add a meta-model on averaged base models and use the out-of-folds predictions of these base models to train our meta-model. \n\nThe procedure, for the training part, may be described as follows:\n\n\n1. Split the total training set into two disjoint sets (here **train** and .**holdout** )\n\n2. Train several base models on the first part (**train**)\n\n3. Test these base models on the second part (**holdout**)\n\n4. Use the predictions from 3)  (called  out-of-folds predictions) as the inputs, and the correct responses (target variable) as the outputs  to train a higher level learner called **meta-model**.\n\nThe first three steps are done iteratively . If we take for example a 5-fold stacking , we first split the training data into 5 folds. Then we will do 5 iterations. In each iteration,  we train every base model on 4 folds and predict on the remaining fold (holdout fold). \n\nSo, we will be sure, after 5 iterations , that the entire data is used to get out-of-folds predictions that we will then use as \nnew feature to train our meta-model in the step 4.\n\nFor the prediction part , We average the predictions of  all base models on the test data  and used them as **meta-features**  on which, the final prediction is done with the meta-model.\n","b24d0f20":"![kaz](http:\/\/5047-presscdn.pagely.netdna-cdn.com\/wp-content\/uploads\/2017\/06\/image5.gif)\n\nGif taken from [KazAnova's interview](http:\/\/blog.kaggle.com\/2017\/06\/15\/stacking-made-easy-an-introduction-to-stacknet-by-competitions-grandmaster-marios-michailidis-kazanova\/)","13602441":"##Outliers","2a9cc458":"We impute them  by proceeding sequentially  through features with missing values ","cc89b0d8":"- **BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2** : For all these categorical basement-related features, NaN means that there is no  basement.\n","b6dd6f38":"- **Alley** : data description says NA means \"no alley access\"","a4f03f65":"###Imputing missing values ","5be3b146":"**LightGBM:**","9dccb232":"- **LotFrontage** : Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood , we can **fill in missing values by the median LotFrontage of the neighborhood**.","fdc98a50":"[Documentation][1] for the Ames Housing Data indicates that there are outliers present in the training data\n[1]: http:\/\/ww2.amstat.org\/publications\/jse\/v19n3\/Decock\/DataDocumentation.txt","b53d3f5b":"**Data Correlation**\n","6c3af270":"Copied from https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\n\n#Stacked Regressions to predict House Prices\n\n\n##Serigne\n\n**July 2017**\n\n**If you use parts of this notebook in your scripts\/notebooks, giving  some kind of credit would be very much appreciated :)  You can for instance link back to this notebook. Thanks!**","d6d6c4b7":"![Faron](http:\/\/i.imgur.com\/QBuDOjs.jpg)\n\n(Image taken from [Faron](https:\/\/www.kaggle.com\/getting-started\/18153#post103381))","1e11993a":"Is there any remaining missing value ? ","0aa3f99a":"**XGBoost:**","78c258e5":"## Visualization\nPartially copied from https:\/\/www.kaggle.com\/suprematism\/house-prices-advanced-visualisation","43e1c3d5":"- **GarageType, GarageFinish, GarageQual and GarageCond** : Replacing missing data with None","93548315":"#Data Processing","a71834da":"Wow ! It seems even the simplest stacking approach really improve the score . This encourages \nus to go further and explore a less simple stacking approch. ","22ec3f36":"- **Kernel Ridge Regression** :","b9d7ec3c":"**Adding one more important feature**","ea2e38b6":"We begin with this simple approach of averaging base models.  We build a new **class**  to extend scikit-learn with our model and also to laverage encapsulation and code reuse ([inheritance][1]) \n\n\n  [1]: https:\/\/en.wikipedia.org\/wiki\/Inheritance_(object-oriented_programming)","19b38876":"To make the two approaches comparable (by using the same number of models) , we just average **Enet KRR and Gboost**, then we add **lasso as meta-model**.","f519844a":"let's first  concatenate the train and test data in the same dataframe","aae3c59d":"We just average four models here **ENet, GBoost,  KRR and lasso**.  Of course we could easily add more models in the mix. ","24ec1107":"We first define a rmsle evaluation function ","d56745d8":"**Getting dummy categorical features**","c85b5bf1":"##Features engineering","73bdd038":"- **MasVnrArea and MasVnrType** : NA most likely means no masonry veneer for these houses. We can fill 0 for the area and None for the type. \n","d99f3862":"**Stacking averaged Models Class**","dfbc2373":"We get again a better score by adding a meta learner","c0917325":"The skew seems now corrected and the data appears more normally distributed. ","74ccdef8":"The target variable is right skewed.  As (linear) models love normally distributed data , we need to transform this variable and make it more normally distributed.","d41b9250":"It remains no missing value.\n","92bd4928":"###Base models scores","f558ce37":"Let's explore these outliers\n","d0cf7201":"## Ensembling StackedRegressor, XGBoost and LightGBM","03f44cff":"- **Fence** : data description says NA means \"no fence\"","5cb1645e":"#Modelling","f0dbfaf7":"**If you found this notebook helpful or you just liked it , some upvotes would be very much appreciated -  That will keep me motivated to update it on a regular basis** :-)","71f8dd8b":"We can see at the bottom right two with extremely large GrLivArea that are of a low price. These values are huge oultliers.\nTherefore, we can safely delete them.","5e434493":" **Log-transformation of the target variable**","ea47fca0":"We use the scipy  function boxcox1p which computes the Box-Cox transformation of **\\\\(1 + x\\\\)**. \n\nNote that setting \\\\( \\lambda = 0 \\\\) is equivalent to log1p used above for the target variable.  \n\nSee [this page][1] for more details on Box Cox Transformation as well as [the scipy function's page][2]\n[1]: http:\/\/onlinestatbook.com\/2\/transformations\/box-cox.html\n[2]: https:\/\/docs.scipy.org\/doc\/scipy-0.19.0\/reference\/generated\/scipy.special.boxcox1p.html","b7a314d0":"**SalePrice** is the variable we need to predict. So let's do some analysis on this variable first.","db45f4e0":"- **GarageYrBlt, GarageArea and GarageCars** : Replacing missing data with 0 (Since No garage = no cars in such garage.)\n","7a33b1f9":"##Target Variable","977c2926":"**Stacking Averaged models Score**","dd8367ba":"- **Elastic Net Regression** :\n\nagain made robust to outliers","b5050c9e":"- **FireplaceQu** : data description says NA means \"no fireplace\""}}