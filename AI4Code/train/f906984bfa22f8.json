{"cell_type":{"cf655d0a":"code","2425096d":"code","f64f9728":"code","7bf798e4":"code","1278e1ac":"code","be763798":"code","65372831":"code","803c4ec0":"code","54a03a4d":"code","aa6e0c18":"code","bc9eaa63":"code","7947d6c7":"markdown","40aa2bb6":"markdown","589d9d30":"markdown","e452cdbf":"markdown","5fb76935":"markdown","88dd6106":"markdown","53c4be1b":"markdown","e227c0b5":"markdown","7e91fd05":"markdown","e135c863":"markdown"},"source":{"cf655d0a":"import os\nimport numpy as np\nimport pandas as pd\nimport struct\n\nimport scipy.io\nimport scipy.misc\nimport PIL\nimport cv2\nfrom skimage.transform import resize\n\nimport tensorflow as tf\nfrom keras import backend as K\nfrom keras.layers import Input, Lambda, Conv2D, BatchNormalization, LeakyReLU, ZeroPadding2D, UpSampling2D\nfrom keras.models import load_model, Model\nfrom keras.layers.merge import add, concatenate\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow\nfrom matplotlib.patches import Rectangle","2425096d":"class Read_Weights:\n\n    def __init__(self, file_name):\n    \n        with open(file_name, 'rb') as w_f:\n\n            major, = struct.unpack('i', w_f.read(4))\n            minor, = struct.unpack('i', w_f.read(4))\n            revision, = struct.unpack('i', w_f.read(4))\n\n            if (major*10 + minor) >= 2 and major < 1000 and minor < 1000:\n                w_f.read(8)\n            else:\n                w_f.read(4)\n\n            transpose = (major > 1000) or (minor > 1000)\n            binary = w_f.read()\n\n        self.offset = 0\n        self.all_weights = np.frombuffer(binary, dtype = 'float32')\n \n    def read_bytes(self, size):\n\n        self.offset = self.offset + size\n        \n        return self.all_weights[ self.offset-size : self.offset ]\n \n    def load_weights(self, model):\n\n        for i in range(106):\n            try:\n                conv_layer = model.get_layer('conv_' + str(i))\n                print(\"loading weights of convolution #\" + str(i))\n\n                if i not in [81, 93, 105]:\n\n                    norm_layer = model.get_layer('bnorm_' + str(i))\n                    size = np.prod(norm_layer.get_weights()[0].shape)\n                    \n                    beta  = self.read_bytes(size) # bias\n                    gamma = self.read_bytes(size) # scale\n                    mean  = self.read_bytes(size) # mean\n                    var   = self.read_bytes(size) # variance\n\n                    weights = norm_layer.set_weights([gamma, beta, mean, var])\n\n                if len(conv_layer.get_weights()) > 1:\n\n                    bias   = self.read_bytes(np.prod(conv_layer.get_weights()[1].shape))\n                    kernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n                    kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n                    kernel = kernel.transpose([2,3,1,0])\n                    conv_layer.set_weights([kernel, bias])\n\n                else:\n\n                    kernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n                    kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n                    kernel = kernel.transpose([2,3,1,0])\n                    conv_layer.set_weights([kernel])\n\n            except ValueError:\n                print(\"no convolution #\" + str(i))\n \n    def reset(self):\n        self.offset = 0","f64f9728":"def conv_block(inp, convs, skip=True):\n\n    x = inp\n    count = 0\n    \n    for conv in convs:\n\n        if count == (len(convs) - 2) and skip:\n            skip_connection = x\n\n        count += 1\n        if conv['stride'] > 1 :  x = ZeroPadding2D(((1,0),(1,0)))(x) # peculiar padding as darknet prefers left and top\n\n        x = Conv2D(conv['filter'],\n                   conv['kernel'],\n                   strides = conv['stride'],\n                   padding = 'valid' if conv['stride'] > 1 else 'same', # peculiar padding as darknet prefers left and top\n                   name = 'conv_' + str(conv['layer_idx']),\n                   use_bias = False if conv['bnorm'] else True)(x)\n\n        if conv['bnorm']: x = BatchNormalization(epsilon = 0.001, name = 'bnorm_' + str(conv['layer_idx']))(x)\n        if conv['leaky']: x = LeakyReLU(alpha = 0.1, name = 'leaky_' + str(conv['layer_idx']))(x)\n            \n    return add([skip_connection, x]) if skip else x\n\ndef make_yolov3_model():\n\n    input_image = Input(shape=(None, None, 3))\n\n    # Layers 0 to 4\n    x = conv_block(input_image, [{'filter': 32, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 0},\n                                {'filter': 64, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 1},\n                                {'filter': 32, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 2},\n                                {'filter': 64, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 3}])\n\n    # Layers 5 to 8\n    x = conv_block(x, [{'filter': 128, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 5},\n                        {'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 6},\n                        {'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 7}])\n\n    # Layers 9 to 11\n    x = conv_block(x, [{'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 9},\n                        {'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 10}])\n\n    # Layers 12 to 15\n    x = conv_block(x, [{'filter': 256, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 12},\n                        {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 13},\n                        {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 14}])\n\n    # Layers 16 to 36\n    for i in range(7):\n        x = conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 16+i*3},\n                            {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 17+i*3}])\n    skip_36 = x\n    \n    # Layers 37 to 40\n    x = conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 37},\n                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 38},\n                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 39}])\n\n    # Layers 41 to 61\n    for i in range(7):\n        x = conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 41+i*3},\n                            {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 42+i*3}])\n    skip_61 = x\n\n    # Layers 62 to 65\n    x = conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 62},\n                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 63},\n                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 64}])\n\n    # Layers 66 to 74\n    for i in range(3):\n        x = conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 66+i*3},\n                            {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 67+i*3}])\n\n    # Layers 75 to 79\n    x = conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 75},\n                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 76},\n                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 77},\n                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 78},\n                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 79}], skip=False)\n\n    # Layers 80 to 82\n    yolo_82 = conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 80},\n                             {'filter':  255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 81}], skip=False)\n\n    # Layers 83 to 86\n    x = conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 84}], skip=False)\n    x = UpSampling2D(2)(x)\n    x = concatenate([x, skip_61])\n\n    # Layers 87 to 91\n    x = conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 87},\n                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 88},\n                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 89},\n                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 90},\n                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 91}], skip=False)\n\n    # Layers 92 to 94\n    yolo_94 = conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 92},\n                                                                                     {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 93}], skip=False)\n\n    # Layers 95 to 98\n    x = conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True,   'layer_idx': 96}], skip=False)\n    x = UpSampling2D(2)(x)\n    x = concatenate([x, skip_36])\n\n    # Layers 99 to 106\n    yolo_106 = conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 99},\n                                {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 100},\n                                {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 101},\n                                {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 102},\n                                {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 103},\n                                {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 104},\n                                {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 105}], skip=False)\n\n    model = Model(input_image, [yolo_82, yolo_94, yolo_106])\n    return model","7bf798e4":"# define the yolo v3 model\nyolov3 = make_yolov3_model()\n\n# load the weights\nweight_reader = Read_Weights(\"..\/input\/data-for-yolo-v3-kernel\/yolov3.weights\")\n\n# set the weights\nweight_reader.load_weights(yolov3)\n\n# save the model to file\nyolov3.save('yolo_model.h5')","1278e1ac":"def load_image_pixels(filename, shape):\n\n  # load image to get its shape\n  image = load_img(filename)\n  width, height = image.size\n\n  # load image with required size\n  image = load_img(filename, target_size = shape)\n  image = img_to_array(image)\n\n  # grayscale image normalization\n  image = image.astype('float32')\n  image \/= 255.0\n\n  # add a dimension so that we have one sample\n  image = np.expand_dims(image, 0)\n  return image, width, height","be763798":"class BoundBox:\n\n    def __init__(self, xmin, ymin, xmax, ymax, objness = None, classes = None):\n        self.xmin = xmin\n        self.ymin = ymin\n        self.xmax = xmax\n        self.ymax = ymax\n        self.objness = objness\n        self.classes = classes\n        self.label = -1\n        self.score = -1\n\n    def get_label(self):\n        if self.label == -1:\n            self.label = np.argmax(self.classes)\n\n        return self.label\n\n    def get_score(self):\n        if self.score == -1:\n            self.score = self.classes[self.get_label()]\n        return self.get_score\n\ndef _sigmoid(x):\n    return 1. \/(1. + np.exp(-x))\n\ndef decode_netout(netout, anchors, obj_thresh, net_h, net_w):\n\n    grid_h, grid_w = netout.shape[:2]\n    nb_box = 3\n    netout = netout.reshape((grid_h, grid_w, nb_box, -1))\n    nb_class = netout.shape[-1] - 5\n    boxes = []\n    netout[..., :2]  = _sigmoid(netout[..., :2])\n    netout[..., 4:]  = _sigmoid(netout[..., 4:])\n    netout[..., 5:]  = netout[..., 4][..., np.newaxis] * netout[..., 5:]\n    netout[..., 5:] *= netout[..., 5:] > obj_thresh\n\n    for i in range(grid_h*grid_w):\n        row = i \/ grid_w\n        col = i % grid_w\n        for b in range(nb_box):\n\n            # 4th element is objectness score\n            objectness = netout[int(row)][int(col)][b][4]\n            if(objectness.all() <= obj_thresh): continue\n\n            # first 4 elements are x, y, w, and h\n            x, y, w, h = netout[int(row)][int(col)][b][:4]\n            x = (col + x) \/ grid_w # center position, unit: image width\n            y = (row + y) \/ grid_h # center position, unit: image height\n            w = anchors[2 * b + 0] * np.exp(w) \/ net_w # unit: image width\n            h = anchors[2 * b + 1] * np.exp(h) \/ net_h # unit: image height\n\n            # last elements are class probabilities\n            classes = netout[int(row)][col][b][5:]\n            box = BoundBox(x-w\/2, y-h\/2, x+w\/2, y+h\/2, objectness, classes)\n            boxes.append(box)\n\n    return boxes","65372831":"def correct_yolo_boxes(boxes, image_h, image_w, net_h, net_w):\n\n    new_w, new_h = net_w, net_h\n    for i in range(len(boxes)):\n\n        x_offset, x_scale = (net_w - new_w)\/2.\/net_w, float(new_w)\/net_w\n        y_offset, y_scale = (net_h - new_h)\/2.\/net_h, float(new_h)\/net_h\n\n        boxes[i].xmin = int((boxes[i].xmin - x_offset) \/ x_scale * image_w)\n        boxes[i].xmax = int((boxes[i].xmax - x_offset) \/ x_scale * image_w)\n        boxes[i].ymin = int((boxes[i].ymin - y_offset) \/ y_scale * image_h)\n        boxes[i].ymax = int((boxes[i].ymax - y_offset) \/ y_scale * image_h)","803c4ec0":"def interval_overlap(interval_a, interval_b):\n\n    x1, x2 = interval_a\n    x3, x4 = interval_b\n\n    if x3 < x1:\n        if x4 < x1:\n            return 0\n        else:\n            return min(x2,x4) - x1\n    else:\n        if x2 < x3:\n             return 0\n        else:\n            return min(x2,x4) - x3\n \ndef bbox_iou(box1, box2):\n\n    intersect_w = interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])\n    intersect_h = interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])\n    intersect = intersect_w * intersect_h\n    w1, h1 = box1.xmax-box1.xmin, box1.ymax-box1.ymin\n    w2, h2 = box2.xmax-box2.xmin, box2.ymax-box2.ymin\n    union = w1*h1 + w2*h2 - intersect\n    return float(intersect) \/ union\n \ndef nms(boxes, nms_thresh):\n\n    if len(boxes) > 0:\n        nb_class = len(boxes[0].classes)\n    else:\n        return\n\n    for c in range(nb_class):\n        sorted_indices = np.argsort([-box.classes[c] for box in boxes])\n\n        for i in range(len(sorted_indices)):\n            index_i = sorted_indices[i]\n\n            if boxes[index_i].classes[c] == 0: continue\n\n            for j in range(i+1, len(sorted_indices)):\n                index_j = sorted_indices[j]\n\n                if bbox_iou(boxes[index_i], boxes[index_j]) >= nms_thresh:\n                    boxes[index_j].classes[c] = 0\n\n# get all of the results above a threshold\ndef get_boxes(boxes, labels, thresh):\n\n    v_boxes, v_labels, v_scores = list(), list(), list()\n    # enumerate all boxes\n    for box in boxes:\n        # enumerate all possible labels\n        for i in range(len(labels)):\n            # check if the threshold for this label is high enough\n            if box.classes[i] > thresh:\n                v_boxes.append(box)\n                v_labels.append(labels[i])\n                v_scores.append(box.classes[i]*100)\n                # don't break, many labels may trigger for one box\n\n    return v_boxes, v_labels, v_scores\n\n# draw all results\ndef draw_boxes(filename, v_boxes, v_labels, v_scores):\n  \n    data = plt.imread(filename)\n    plt.imshow(data)\n    ax = plt.gca()\n\n    # plot each box\n    for i in range(len(v_boxes)):\n        box = v_boxes[i]\n\n        # get coordinates\n        y1, x1, y2, x2 = box.ymin, box.xmin, box.ymax, box.xmax\n        width, height = x2 - x1, y2 - y1\n        # create the shape\n        rect = plt.Rectangle((x1, y1), width, height, fill = False, color = 'red', linewidth = '2')\n\n        # draw the box\n        ax.add_patch(rect)\n        # draw text and score in top left corner\n        label = \"%s (%.3f)\" % (v_labels[i], v_scores[i])\n        plt.text(x1, y1, label, color = 'b')\n\n    # show the plot\n    plt.show()","54a03a4d":"# define the anchors\nanchors = [[116,90, 156,198, 373,326], [30,61, 62,45, 59,119], [10,13, 16,30, 33,23]]\n\n# define the probability threshold for detected objects\nclass_threshold = 0.6\n\n# define the labels\nlabels = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\",\n    \"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\",\n    \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\",\n    \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\",\n    \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\",\n    \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\",\n    \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\",\n    \"chair\", \"sofa\", \"pottedplant\", \"bed\", \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\",\n    \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\",\n    \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"]","aa6e0c18":"input_w, input_h = 416, 416\n\ndef predict_boxes(image_names):\n    for image_name in image_names:\n      image, image_w, image_h = load_image_pixels(image_name, (input_w, input_h))\n\n      # make prediction\n      yhat = yolov3.predict(image)\n      # summarize the shape of the list of arrays\n      print([a.shape for a in yhat])\n\n      boxes = list() \n      for i in range(len(yhat)):\n        # decode the output of the network\n        boxes += decode_netout(yhat[i][0], anchors[i], class_threshold, input_h, input_w)\n\n      # correct the sizes of the bounding boxes for the shape of the image\n      correct_yolo_boxes(boxes, image_h, image_w, input_h, input_w)\n\n      # suppress non-maximal boxes\n      nms(boxes, 0.5)\n\n      # get the details of the detected objects\n      v_boxes, v_labels, v_scores = get_boxes(boxes, labels, class_threshold)\n\n      # summarize what we found\n      for i in range(len(v_boxes)):\n        print(v_labels[i], v_scores[i])\n\n      # draw what we found\n      draw_boxes(image_name, v_boxes, v_labels, v_scores)","bc9eaa63":"image_names = [\"..\/input\/data-for-yolo-v3-kernel\/dog.jpg\", \"..\/input\/data-for-yolo-v3-kernel\/office.jpg\"]\npredict_boxes(image_names)","7947d6c7":"## Make predictions","40aa2bb6":"## Construction of the YOLO model\n- `conv_block` is a function to create convolutional layers for CNN\n- `make_yolov3_model` is a function to create layers of convoluational and stack together as a whole yolo model (Since YOLO v3 uses darknet for feature extraction we will be constructing the model accordingly)\n\nFor feature extraction Yolo uses Darknet-53 neural net pretrained on ImageNet. Same as ResNet, Darknet-53 has shortcut (residual) connections, which help information from earlier layers flow further. We omit the last 3 layers (Avgpool, Connected and Softmax) since we only need the features.\n\n","589d9d30":"## Defining the model\n\n","e452cdbf":"### `correct_yolo_boxes` is used to resize the bounding boxes to the original image shape.","5fb76935":" ## Bounding Boxes\n### `BoundBox` class is used to return object bounding box coordinates, object name and  the threshold score.\n\n### `decode_netout` function is used to convert the prediction output into rectangle coordinates","88dd6106":"## Non Max Supression (NMS) and Intersection over Union (IOU)\n### `interval_overlap` function is used to find the overlapping regions\n\n### `bbox_iou` function is the implementation of IOU\n\n### `nms` function is the implementation of NMS\n\n","53c4be1b":"## Extracting YOLO v3 weights\n`Read_Weights` class is used to extract the model weights from the \"yolov3.weights\" file into a suitbale format that can be used for keras. The class is also used to integrate the Darknet and YOLO architecures.\n\nYolo is an algorithm that uses convolutional neural networks for object detection. So what's great about object detection? In comparison to recognition algorithms, a detection algorithm does not only predict class labels, but detects locations of objects as well.\n","e227c0b5":"### `load_image_pixels` function is used to load an image\n \n","7e91fd05":"## Anchors and labels\n","e135c863":"## Necessary libraries"}}