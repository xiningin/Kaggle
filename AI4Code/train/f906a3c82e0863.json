{"cell_type":{"c6558931":"code","6d52de0b":"code","151f0107":"code","8f573d36":"code","824e79a3":"code","8c97a38b":"code","fa643a5c":"code","92b78051":"code","1cf8daf5":"code","db6ca3df":"code","245db71c":"code","dc29e3ed":"code","45d744d6":"code","39b9e168":"code","798acbe2":"code","23828925":"code","2d80cc42":"code","770797a8":"code","56bee0b6":"code","8b73bd6e":"code","f3e11562":"code","bcb53786":"code","ae01eef1":"markdown","6c7efdcf":"markdown","755afbc0":"markdown","391015e4":"markdown","8be8e18d":"markdown","b755e6d1":"markdown","dadffe0b":"markdown","22ce1924":"markdown"},"source":{"c6558931":"DATA_PATH = '..\/input\/shopee-product-matching\/'\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport cv2, matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\nimport gc\n\ntrain = pd.read_csv(DATA_PATH + 'train.csv')\ntrain['image'] = DATA_PATH + 'train_images\/' + train['image']\ntmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\ntrain['target'] = train.label_group.map(tmp)","6d52de0b":"train = train.sort_values(by='label_group')\ntrain.head()","151f0107":"import imagehash\nfrom PIL import Image","8f573d36":"ahash = imagehash.average_hash(Image.open(train['image'].iloc[0]))\nahash","824e79a3":"# conver to string\nstr(ahash)","8c97a38b":"print(str(imagehash.average_hash(Image.open(train['image'].iloc[0]))))\nprint(str(imagehash.phash(Image.open(train['image'].iloc[0]))))\nprint(str(imagehash.dhash(Image.open(train['image'].iloc[0]))))\nprint(str(imagehash.whash(Image.open(train['image'].iloc[0]))))","fa643a5c":"print(str(imagehash.average_hash(Image.open(train['image'].iloc[0]))))\nprint(str(imagehash.average_hash(Image.open(train['image'].iloc[1]))))\nprint(str(imagehash.average_hash(Image.open(train['image'].iloc[2]))))","92b78051":"img = cv2.imread(train['image'].iloc[0],0)\n\n# https:\/\/docs.opencv.org\/master\/d1\/db7\/tutorial_py_histogram_begins.html\narr = cv2.calcHist([img],[0],None,[256],[0,256])\narr = arr.reshape(-1)\narr.shape","1cf8daf5":"%pylab inline\nplt.plot(range(256), arr)","db6ca3df":"def cal_hist(path):\n    img = cv2.imread(path,0)\n    arr = cv2.calcHist([img],[0],None,[256],[0,256])\n    arr = arr.reshape(-1)\n    return arr \/ arr.sum()\n\nplt.plot(range(256), cal_hist(train['image'].iloc[0]))\nplt.plot(range(256), cal_hist(train['image'].iloc[2]))","245db71c":"img1 = cv2.imread(train['image'].iloc[0], 0)          # queryImage\nimg2 = cv2.imread(train['image'].iloc[2], 0)          # trainImage\n\n# Initiate SIFT detector\nsift = cv2.SIFT_create()\n\n# find the keypoints and descriptors with SIFT\nkp1, des1 = sift.detectAndCompute(img1,None)\nkp2, des2 = sift.detectAndCompute(img2,None)\n\n# BFMatcher with default params\nbf = cv2.BFMatcher()\nmatches = bf.knnMatch(des1,des2,k=2)\n\n# Apply ratio test\ngood = []\nfor m,n in matches:\n    if m.distance < 0.75*n.distance:\n        good.append([m])\n\n        # cv.drawMatchesKnn expects list of lists as matches.\nimg3 = cv2.drawMatchesKnn(img1,kp1,img2,kp2,good,None,\n                          flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n\nplt.figure(figsize=(10, 7))\nplt.imshow(img3),plt.show()\n","dc29e3ed":"img1 = cv2.imread(train['image'].iloc[0], 0)          # queryImage\nimg2 = cv2.imread(train['image'].iloc[1], 0)          # trainImage\n\n# Initiate SIFT detector\nsift = cv2.SIFT_create()\n\n# find the keypoints and descriptors with SIFT\nkp1, des1 = sift.detectAndCompute(img1,None)\nkp2, des2 = sift.detectAndCompute(img2,None)\n\n# BFMatcher with default params\nbf = cv2.BFMatcher()\nmatches = bf.knnMatch(des1,des2,k=2)\n\n# Apply ratio test\ngood = []\nfor m,n in matches:\n    if m.distance < 0.75*n.distance:\n        good.append([m])\n\n        # cv.drawMatchesKnn expects list of lists as matches.\nimg3 = cv2.drawMatchesKnn(img1,kp1,img2,kp2,good,None,\n                          flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n\nplt.figure(figsize=(10, 7))\nplt.imshow(img3),plt.show()\n","45d744d6":"# a function for sift distance\ndef sift_match(path1, path2):\n    img1 = cv2.imread(path1, 0)\n    img2 = cv2.imread(path2, 0)\n    \n    # Initiate SIFT detector\n    sift = cv2.SIFT_create(300)\n    \n    # find the keypoints and descriptors with SIFT\n    kp1, des1 = sift.detectAndCompute(img1,None)\n    kp2, des2 = sift.detectAndCompute(img2,None)\n    \n    # BFMatcher with default params\n    bf = cv2.BFMatcher()\n    matches = bf.knnMatch(des1,des2,k=2)\n    # Apply ratio test\n    good = []\n    for m,n in matches:\n        if m.distance < 0.8*n.distance:\n            good.append([m])\n\n    return len(good)","39b9e168":"# save group\nsift_match(train[train['label_group'] == 249114794]['image'].iloc[0],\n            train[train['label_group'] == 249114794]['image'].iloc[1])","798acbe2":"# different group\nsift_match(train[train['label_group'] == 249114794]['image'].iloc[0],\n            train[train['label_group'] == 258047]['image'].iloc[1])","23828925":"import torch\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data.dataset import Dataset\n\nclass ShopeeImageDataset(Dataset):\n    def __init__(self, img_path, transform):\n        self.img_path = img_path\n        self.transform = transform\n        \n    def __getitem__(self, index):\n        img = Image.open(self.img_path[index]).convert('RGB')\n        img = self.transform(img)\n        return img\n    \n    def __len__(self):\n        return len(self.img_path)","2d80cc42":"imagedataset = ShopeeImageDataset(\n    train['image'].values[:100],\n    transforms.Compose([\n        transforms.Resize((512, 512)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]))\n    \nimageloader = torch.utils.data.DataLoader(\n    imagedataset,\n    batch_size=10, shuffle=False, num_workers=2\n)\n\n# pretrain models\nclass ShopeeImageEmbeddingNet(nn.Module):\n    def __init__(self):\n        super(ShopeeImageEmbeddingNet, self).__init__()\n              \n        model = models.resnet18(True)\n        model.avgpool = nn.AdaptiveMaxPool2d(output_size=(1, 1))\n        model = nn.Sequential(*list(model.children())[:-1])\n        model.eval()\n        self.model = model\n        \n    def forward(self, img):        \n        out = self.model(img)\n        return out","770797a8":"DEVICE = 'cpu'\n\nimgmodel = ShopeeImageEmbeddingNet()\nimgmodel = imgmodel.to(DEVICE)\n\nimagefeat = []\nwith torch.no_grad():\n    for data in tqdm_notebook(imageloader):\n        data = data.to(DEVICE)\n        feat = imgmodel(data)\n        feat = feat.reshape(feat.shape[0], feat.shape[1])\n        feat = feat.data.cpu().numpy()\n        \n        imagefeat.append(feat)","56bee0b6":"from sklearn.preprocessing import normalize\n\n# l2 norm to kill all the sim in 0-1\nimagefeat = np.vstack(imagefeat)\nimagefeat = normalize(imagefeat)","8b73bd6e":"train.head(5)","f3e11562":"# in this case, the group 258047 (the first three) have high confidence.\nnp.dot(imagefeat[0], imagefeat.T)[:5]","bcb53786":"# in this case, the group 297977 (the last two) have high confidence.\nnp.dot(imagefeat[3], imagefeat.T)[:5]","ae01eef1":"# 2 Color Hist\n\nhttps:\/\/en.wikipedia.org\/wiki\/Color_histogram\n\ncolor histogram is a representation of the distribution of colors in an image. For digital images, a color histogram represents the number of pixels that have colors in each of a fixed list of color ranges, that span the image's color space, the set of all possible colors.\n\n\u989c\u8272\u76f4\u65b9\u56fe\u7edf\u8ba1\u7684\u662f\u56fe\u50cf\u4e2d\u989c\u8272\u53d6\u503c\u7684\u8ba1\u6570\u4fe1\u606f\uff0c\u5982\u679c\u56fe\u50cf\u989c\u8272\u76f8\u4f3c\u5219\u989c\u8272\u76f4\u65b9\u56fe\u76f8\u4f3c\u3002\n\n![image.png](attachment:image.png)","6c7efdcf":"In this notebook, some image feature is introduced. And i will give some insight and product method in shopee compte. If you find usefule, please give upvote, thx.\n\nimage feature:\n* image hash\n* image color hist\n* image keypint\n* **image cnn fearure**, most accurate \n\n\n\u5728\u672c\u4e2anotebook\u4e2d\uff0c\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u56fe\u7247\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u3002\u6211\u4e5f\u4f1a\u5728\u4ecb\u7ecd\u4f7f\u7528\u7684\u540c\u65f6\uff0c\u7ed3\u5408shopee\u6bd4\u8d5b\u7ed9\u51fa\u4e00\u4e9b\u5efa\u8bae\u3002\u5982\u679c\u4f60\u611f\u8c22\u5185\u5bb9\u5bf9\u4f60\u6709\u5e2e\u52a9\uff0c\u8bf7\u7ed9\u6211\u70b9\u8d5e\uff0c\u8c22\u8c22\u3002\n\n\nYou can check my other notebooks:\n\n- [Shopee Products Matching: Image Part [English+\u4e2d\u6587]](https:\/\/www.kaggle.com\/finlay\/shopee-products-matching-image-part-english)\n- [Shopee Products Matching: Text Part [English+\u4e2d\u6587]](https:\/\/www.kaggle.com\/finlay\/shopee-products-matching-text-part-english)\n- [Shopee Products Matching: BoF Part [English+\u4e2d\u6587]](https:\/\/www.kaggle.com\/finlay\/shopee-products-matching-bof-part-english)\n- [Shopee Products Matching: Augment Part [English\u4e2d\u6587]](https:\/\/www.kaggle.com\/finlay\/shopee-products-matching-augment-part-english)\n- [[Unsupervised] Image + Text Baseline in 20min](https:\/\/www.kaggle.com\/finlay\/unsupervised-image-text-baseline-in-20min)\n\n","755afbc0":"# 1 Image Hash\n\nhttps:\/\/github.com\/jenssegers\/imagehash\n\n\nfour different hash method:\n* average hashing (aHash)\n* perception hashing (pHash)\n* difference hashing (dHash)\n* wavelet hashing (wHash)\n\n","391015e4":"image hash can calculate the distance by char edit distance. in below example, the hashs of sample group are differenet in some char.\n\n\u76f8\u4f3c\u56fe\u7247\u7684\u54c8\u5e0c\u503c\u4e5f\u6709\u53ef\u80fd\u662f\u76f8\u4f3c\u7684\uff0c\u56e0\u6b64\u53ef\u4ee5\u8003\u8651\u7528\u7f16\u8f91\u8ddd\u79bb\u6765\u8861\u91cf\u3002","8be8e18d":"# 4 Deep Convnet(CNN)\n\nDeep learning can do many things, include image classification and detection. In shopee compte, we can use CNN to calcuate the image feature (the internal output of CNN models).\n\nAnd we can use pretrain models or custom models (train in Shopee dataset by your self).\n\n\u4f7f\u7528\u8bad\u7ec3\u597d\u7684CNN\u6765\u63d0\u53d6\u7279\u5f81\uff0c\u662f\u975e\u5e38\u597d\u7684\u3002\u5f53\u7136\u4f60\u4e5f\u53ef\u4ee5\u5728shopee\u6bd4\u8d5b\u4e2d\u81ea\u5df1\u8bad\u7ec3\u6570\u636e\u96c6\u5f97\u5230\u6a21\u578b\u3002\n\nhttps:\/\/cs231n.github.io\/convolutional-networks\/\n\n![image.png](attachment:image.png)","b755e6d1":"# 3 Image Keypoint\n\nKeypoint is a feature detection algorithm in computer vision to detect and describe local features in images. SIFT and ORB is common keypoint method, you can find they in OpenCV.\n\nKeypoint is a local image feature, so count of keypoint may not same. And keypoint is easy attract by text in image.\n\n\u56fe\u50cf\u5173\u952e\u70b9\u662f\u4e00\u79cd\u5c40\u90e8\u7279\u5f81\uff0c\u6240\u4ee5\u4e0d\u540c\u56fe\u50cf\u6709\u53ef\u80fd\u5173\u952e\u70b9\u4e0d\u540c\uff0c\u4e14\u5173\u952e\u70b9\u5bb9\u6613\u53d7\u56fe\u50cf\u4e2d\u6587\u5b57\u7684\u5f71\u54cd\u3002\n\nhttps:\/\/en.wikipedia.org\/wiki\/Scale-invariant_feature_transform\nhttps:\/\/docs.opencv.org\/master\/dc\/dc3\/tutorial_py_matcher.html\n\n![image.png](attachment:image.png)","dadffe0b":"color hist can calculate the distance by array distance. in below example, the hist of sample group are very similar.","22ce1924":"# 6 Conclusion & Advice\n\n1. **CNN feature is most accurate**, so i think you should spend more time in find beter CNN model.\n    * You can try train model with arcface model in shopee compte data, then you will get a better image embedding.\n    * You can train image matching or image classification.\n    * You can try different pooing layer(max\/mean\/gem), these layers may give different result.\n    * You can try different model architecture.\n    * https:\/\/www.kaggle.com\/ragnar123\/unsupervised-baseline-arcface\n    \n2. You can find similar image by cnn feature, and re-order they by color hist and keypoint distance.\n3. You can use external data to train model.\n    * https:\/\/www.kaggle.com\/lakritidis\/product-clustering-matching-classification\n    * https:\/\/www.kaggle.com\/promptcloud\/walmart-product-data-2019\n    * https:\/\/www.kaggle.com\/paramaggarwal\/fashion-product-images-dataset\n    * https:\/\/www.kaggle.com\/mylee2009\/shopee-round-2-product-detection-challenge    "}}