{"cell_type":{"96b6ff44":"code","cd4ee573":"code","a7bc3d4a":"code","4ea24943":"code","1af3087e":"code","d4415a1b":"code","e71fcfd8":"code","27908fc6":"code","af9f3582":"code","eb80569c":"code","f8c19ebd":"code","a0ea1e02":"code","b4d33af0":"code","9e62f19b":"code","8b12bf91":"code","6e7893e3":"code","5cd4db43":"markdown","05b36993":"markdown","cbe49e19":"markdown","d9c41a0d":"markdown","5b068d48":"markdown","6bd7f995":"markdown","e6144bad":"markdown"},"source":{"96b6ff44":"# Run this to ensure TensorFlow 2.x is used\ntry:\n  # %tensorflow_version only exists in Colab.\n  %tensorflow_version 2.x\nexcept Exception:\n  pass","cd4ee573":"import json\nimport tensorflow as tf\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","a7bc3d4a":"vocab_size = 10000\nembedding_dim = 16\nmax_length = 100\ntrunc_type='post'\npadding_type='post'\noov_tok = \"<OOV>\"\ntraining_size = 20000\n","4ea24943":"#Snippet is used to get the path \nimport os\nfor dirname, _, filenames in os.walk('..\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1af3087e":"\"\"\"import pandas as pd\npath = \"..\/input\/Sarcasm_Headlines_Dataset.json\"\ndatastore = pd.read_json(path, lines=True)\"\"\"\n\n\n\ndata = [json.loads(line) for line in open('..\/input\/sarcasm-headlines-dataset\/Sarcasm_Headlines_Dataset.json', 'r')]\n\nsentences = []\nlabels = []\n\nfor item in data:\n    sentences.append(item['headline'])\n    labels.append(item['is_sarcastic'])","d4415a1b":"training_sentences = sentences[0:training_size]\ntesting_sentences = sentences[training_size:]\ntraining_labels = labels[0:training_size]\ntesting_labels = labels[training_size:]","e71fcfd8":"tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(training_sentences)\n\nword_index = tokenizer.word_index\n\ntraining_sequences = tokenizer.texts_to_sequences(training_sentences)\ntraining_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\ntesting_sequences = tokenizer.texts_to_sequences(testing_sentences)\ntesting_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)","27908fc6":"# Need this block to get it to work with TensorFlow 2.x\nimport numpy as np\ntraining_padded = np.array(training_padded)\ntraining_labels = np.array(training_labels)\ntesting_padded = np.array(testing_padded)\ntesting_labels = np.array(testing_labels)","af9f3582":"from IPython.display import Image\nsource = \"..\/input\/imageda\/Capture.JPG\"\nImage(source)\n","eb80569c":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])","f8c19ebd":"model.summary()\n","a0ea1e02":"num_epochs = 30\nhistory = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=2)","b4d33af0":"import matplotlib.pyplot as plt\n\n\ndef plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string])\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()\n  \nplot_graphs(history, \"accuracy\")\nplot_graphs(history, \"loss\")","9e62f19b":"reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n\ndef decode_sentence(text):\n    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n\nprint(decode_sentence(training_padded[0]))\nprint(training_sentences[2])\nprint(labels[2])","8b12bf91":"e = model.layers[0]\nweights = e.get_weights()[0]\nprint(weights.shape) # shape: (vocab_size, embedding_dim)\n","6e7893e3":"sentence = [\"granny starting to fear spiders in the garden might be real\", \"game of thrones season finale showing this sunday night\"]\nsequences = tokenizer.texts_to_sequences(sentence)\npadded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\nprint(model.predict(padded))","5cd4db43":"**Visualization **\n\nHere we use the graphs to show the relationship between the accuracy and loss of both training and validation data.","05b36993":"**NLP Model with TensorFlow**","cbe49e19":"**Prediction**\n\nHere we specifiy the sentence in sarcastic and opposite way and the prediction gave show the result correctly","d9c41a0d":"Now, if we plot this on an x- and y-axis as shown in above image,\n\nwe can start to determine the good or bad sentiment\nas coordinates in the x and y.\nGood is 1, 0.\nMeh is minus 0.4, 0.7, et cetera.\nBy looking at the direction of the vector,\nwe can start to determine the meaning of the word.\nSo what if you extend that into multiple dimensions instead\nof just two?\nWhat if words that are labeled with sentiments,\nlike sarcastic and not sarcastic,\nare plotted in these multiple dimensions?\nAnd then, as we train, we try to learn\nwhat the direction in these multi-dimensional spaces\nshould look like.\nWords that only appear in the sarcastic sentences\nwill have a strong component in the sarcastic direction,\nand others will have one in the not-sarcastic direction.\nAs we load more and more sentences\ninto the network for training, these directions can change.\nAnd when we have a fully trained network\nand give it a set of words, it could look up\nthe vectors for these words, sum them up, and thus, give us\nan idea for the sentiment.\nThis concept is known as embedding.","5b068d48":"**Features & Label**\n\nFrom the download json data we take feature and label for training the NLP Classifier Model\n\nFeature -> headline\n\nLabel   -> is_sarcastic","6bd7f995":"**NLP Steps**\n\nBy understanding the data we will classify the given data as sarcastic or not!\n\nBut to do that the general steps are need to be followed\n\n**Tokenization**\n\nThis process is the first step where the given words will be converted to different id or word_index.\nExample -- I love ML can have a following word_index [0,1,2]\n\n**Padding Sequences**\n\nThe padding sequence is used to have a padding value to the given sentence to make every sequence of sentence in a same length,since the neural net needs a fixed size of input.","e6144bad":"**Model Creation**\n\nBut you might be wondering at this point,we've turned our sentences into numbers,\nwith the numbers being tokens representing words.\nBut how do we get meaning from that?\nHow do we determine if something is sarcastic just\nfrom the numbers?\nWell, here's where the context of embeddings come in.\n\nLet's consider the most basic of sentiments.\nSomething is good or something is bad.\nWe often see these as being opposites,\nso we can plot them as having opposite directions.\nSo then what happens with a word like \"meh\"?\nIt's not particularly good, and it's not particularly bad.\nProbably a little more bad than good.\nOr the phrase, \"not bad,\" which is usually\nmeant to plot something as having\na little bit of goodness, but not necessarily very good.\n\n\n"}}