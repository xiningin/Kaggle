{"cell_type":{"50a9b97c":"code","f8e4630c":"code","9aa69279":"code","4f842169":"code","d9122e4e":"code","e0818ec3":"code","15985e21":"code","d38f65eb":"code","b4f30496":"code","f275003a":"code","49d38521":"code","3ad6f66a":"code","e88796ff":"code","451abf66":"code","5b802f2c":"code","8d538e84":"code","c8440a35":"code","7a672f3f":"code","b12971f3":"code","28205d07":"code","9240d6a1":"code","daf294d9":"code","42ea29ec":"code","ce04a09a":"code","1c323426":"code","dc8b9b68":"code","30332738":"code","266ceadd":"code","3908c4a4":"code","e6d0b7b0":"code","d72de6ed":"code","47590b8d":"code","f862721f":"code","f969f46a":"code","ac97fbf5":"code","4378232b":"code","57257de8":"code","7793cddb":"markdown","9c8185ac":"markdown","4a676a11":"markdown","c8f7174c":"markdown","1e97a3c1":"markdown","322509b9":"markdown","70e6cc08":"markdown","2309d70f":"markdown","116b0636":"markdown","b4acbe05":"markdown","9ce4b6bd":"markdown","2c436d83":"markdown","12e8b260":"markdown","5b267484":"markdown","97530ea2":"markdown","834339d2":"markdown","544c48f8":"markdown","49ebd3ff":"markdown","d8bc2fb7":"markdown","4c5011ca":"markdown","1f4d6eef":"markdown","5775405e":"markdown","2adbd171":"markdown","f02b1ebc":"markdown","05132143":"markdown","f5c03dd5":"markdown"},"source":{"50a9b97c":"import sys #access to sysem parameters\nimport pandas as pd #collection of functions for data processing and alaysis \npd.set_option('display.max_columns',None)\nimport numpy as np #foundation package for scientific computing\nnp.set_printoptions(threshold=np.inf)\nimport sklearn #collection of machine learning algorithms\nimport matplotlib.pyplot as plt #collection of functions for visualization\nimport scipy as sp #collection of functions for scientific computing and advance mathematics\n\n#import IPython\n#from Iphython import display # pretty printing of dataframes in Jupyter notebook\n#misc libraries\nimport random\nimport time\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n###load data modelling libraries\n#Common model algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n#from xgboost import XGBClassifier\n#Common model helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n###Visualization\nimport matplotlib as mpl\n#import matplotlib.pylot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pandas.tools.plotting import scatter_matrix\n#Configure visualization defaults\n#%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize']=12,8","f8e4630c":"data_raw = pd.read_csv('..\/input\/train.csv')\n#891 passengers in the training set\ndata_val = pd.read_csv('..\/input\/test.csv')\n#418 passengers in the test set\n\n#print (train.info())\n#create a copy for the train data\n#data1 = data_raw.copy(deep = True)\ncombine = [data_raw, data_val]\n\ndata_raw.describe()","9aa69279":"# Check for the missing values\nprint('Train columns with null values:\\n', data_raw.isnull().sum())\nprint(\"-\"*10)\n\nprint('Test\/Validation columns with null values:\\n', data_val.isnull().sum())\nprint(\"-\"*10)","4f842169":"###Visualize embarkment, Pclass, fare\nplt.figure(figsize=[15,7])\n\nplt.subplot(121)\nsns.boxplot(x = 'Embarked', y = 'Fare', data = data_raw)\n#Passengers embarked at C tend to pay higher fare\nplt.subplot(122)\nsns.boxplot(x = 'Pclass', y = 'Fare', data = data_raw)\n#Passengers from higher class tend to pay higher fare","d9122e4e":"for dataset in combine:\n    dataset['Cabin_null'] =data_raw['Cabin'].isnull()*1\nage_survive = sns.barplot(x='Cabin_null', y='Survived', data=data_raw)\nage_survive = plt.xlabel('Cabin is Missing')\nage_survive = plt.ylabel('Survived')\nplt.figure(figsize=[7,7])\nplt.show()","e0818ec3":"###Complete the missing values\n# The missing Embarked data is filed in with the most frequent value\nfor dataset in combine:\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0],inplace = True) # The missing embarked data is filled in with mode\n    #dataset['Fare'].fillna(dataset['Fare'].median(),inplace = True) # The missing fare data is filled in with median \n    \n# The missing Fare data is filled in with the mean fare value for the pclass the passenger belongs to \nfor x in range(len(data_val[\"Fare\"])):\n    if pd.isnull(data_val[\"Fare\"][x]):\n        pclass = data_val[\"Pclass\"][x] #Pclass = 3\n        data_val[\"Fare\"][x] = round(data_raw[data_raw[\"Pclass\"] == pclass][\"Fare\"].mean(), 4)","15985e21":"###Feature engineering\n#Create the FareBin feature\n#Keep the first letter for Cabin data\nFare_Bins=[-1,7.91,14.454,31,10000]\nFare_Labels=['cheap','medium','medium high','high']\n\nfor dataset in combine:\n    dataset[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in dataset['Cabin'] ])\n    dataset['FareBin']=pd.cut(dataset['Fare'], Fare_Bins, labels=Fare_Labels)\n\ndata_raw.head()","d38f65eb":"###Data visualization\n\nplt.figure(figsize=[12,10])\n\nplt.subplot(221)\nsns.barplot(x = 'Pclass', y = 'Survived', data = data_raw)\n#People with higher socieconomic class had a higher rate of survival\n\nplt.subplot(222)\nsns.barplot(x = 'Embarked', y = 'Survived', data = data_raw)\n#People embarked at C are more likely to survive\n\nplt.subplot(223)\nsns.barplot(x='Cabin',y='Survived',data = data_raw)\n#People with a recorded Cabin number are more likely to survive\n\nplt.subplot(224)\nsns.barplot(x='FareBin',y='Survived',data = data_raw)\n#People who pay a higher fare are more likely to survive","b4f30496":"###Convert the features into numbers\n#cleanup_Embarked = {'S':0,'C':1,'Q':2}\ncleanup_FareBin ={'cheap':0,'medium':1,'medium high':2,'high':3}\nfor dataset in combine:\n    #dataset['Embarked']=dataset['Embarked'].map(cleanup_Embarked).astype(int)\n    dataset['FareBin']=dataset['FareBin'].map(cleanup_FareBin).astype(int)\n\ndata_raw.head()","f275003a":"# For ticket feature we only keep the prefix part\nfor dataset in combine:\n    Ticket = []\n    for i in list(dataset.Ticket):\n        if not i.isdigit() :\n            Ticket.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(' ')[0]) #Take prefix\n        else:\n            Ticket.append(\"X\")\n    dataset[\"Ticket\"] = Ticket\ndataset[\"Ticket\"].head()","49d38521":"age_bins=[0,5,12,18,50,65,120]\nage_labels=['baby','kid','teenager','adult','aging','elderly']\nfor dataset in combine:\n    dataset['AgeGroup']=pd.cut(dataset['Age'], age_bins, labels=age_labels)\nage_survive = sns.barplot(x='AgeGroup', y='Survived', data=data_raw)\nage_survive = plt.xlabel('Age')\nage_survive = plt.ylabel('Survived')\nplt.figure(figsize=[7,5])\nplt.show()","3ad6f66a":"for dataset in combine:\n    dataset['Age_null'] =data_raw['Age'].isnull()*1\nage_survive = sns.barplot(x='Age_null', y='Survived', data=data_raw)\nage_survive = plt.xlabel('Age is Missing')\nage_survive = plt.ylabel('Survived')\nplt.show()","e88796ff":"# Complete missing age Values. we noticed that in the name, there is also title information, this can be helpful to predict the age\nfor dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract('([A-Za-z]+)\\.', expand=False)\npd.crosstab(data_raw['Title'], data_raw['Sex'])","451abf66":"# Group different title (especially rare titles) into common groups which are more closely related to the age\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Capt', 'Col', 'Sir', 'Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Rev', 'Countess', 'Lady', 'Dona'], 'Rare')\n#     dataset['Title'] = dataset['Title'].replace(['Countess', 'Lady', 'Dona'], 'Rare_F')\n    dataset['Title'] = dataset['Title'].replace(['Mlle', 'Ms'], 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    dataset['FamilySize'] = dataset['SibSp']+dataset['Parch'] + 1\n    dataset['IsAlone'] = 1 #initialize to yes\/1 is alone\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # now update to no\/0 if family size is greater than 1\npd.crosstab(data_raw['Title'], data_raw['Sex'])","5b802f2c":"# According to our common sense, the title should be related to the age, e.g. title Miss should be younger than Mrs. To identify if the title is related to the title, visualize their relationship\nplt.figure(figsize=[15,7])\nplt.subplot(121)\nage_title = sns.boxplot(x=\"Title\", y=\"Age\",hue=\"Survived\", data=data_raw)\nage_title = plt.xlabel('Title')\nage_title = plt.ylabel('Age')\n# We assume the age is also related to the pclass so we plot the corresponding diagram to confirm\nplt.subplot(122)\nage_title = sns.boxplot(x=\"Pclass\", y=\"Age\",hue=\"Survived\", data=data_raw)\nage_title = plt.xlabel('PClass')\nage_title = plt.ylabel('Age')\nplt.show()","8d538e84":"# assign the median of each [title, pclass] group if the age value is missed\ndef impute_age(dataset):\n   for pclass in [1,2,3]:\n        for title in ['Master','Miss','Mr','Mrs','Rare']:\n            ds=dataset[dataset['Pclass']==pclass]\n            ds=ds[ds['Title']==title]\n            median=ds['Age'].median()\n            dataset.loc[\n                (dataset['Age'].isnull())&\n                (dataset['Pclass']==pclass)&\n                (dataset['Title']==title),\n                'Age'\n            ]=median\nimpute_age(data_raw)\nimpute_age(data_val)","c8440a35":"for dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp']+dataset['Parch'] + 1\n\n# to study the influence of the family size to the survival chance and the distributino of different family size, \n# we use the bubble plot to visualize it. The size of each bubble is count of each family size.\nfs_count = data_raw['FamilySize'].value_counts()\nfs_prob_survived = data_raw['FamilySize'][data_raw['Survived'] == 1].value_counts()\nfs_prob = fs_prob_survived\/fs_count\nfamilysize = sns.relplot(fs_prob.index, fs_prob.values, size = fs_count.values, sizes = (100,1000), data=data_raw)\nplt.xlabel('FamilySize')\nplt.ylabel('Prob of Survived')\nplt.show()","7a672f3f":"sns.set_style(\"white\")\nsex_sur = sns.barplot(x=\"Sex\", y=\"Survived\", data=data_raw)\nsex_sur = plt.xlabel('Sex')\nsex_sur = plt.ylabel('Survived')\nplt.show()","b12971f3":"#for dataset in combine:\n #   dataset[\"Age\"] = dataset[\"Age\"].fillna(-0.5)\n\nage_bins=[0,5,12,18,50,65,120]\nage_labels=['baby','kid','teenager','adult','aging','elderly']\nfor dataset in combine:\n    dataset['AgeGroup']=pd.cut(dataset['Age'], age_bins, labels=age_labels)\n\ncleanup_agegroup = {'baby':1,'kid':2,'teenager':3,'adult':4,'aging':5,'elderly':6}\ncleanup_sex = {'male':0,'female':1}\nfor dataset in combine:\n    dataset['AgeGroup']=dataset['AgeGroup'].map(cleanup_agegroup).astype(int)\n    dataset['Sex']=dataset['Sex'].map(cleanup_sex).astype(int)\ndata_raw.head()","28205d07":"# drop_column = ['Name','Age','PassengerId','Fare','Cabin']\n# data_raw.drop(drop_column, axis=1, inplace=True)\n# data_val.drop(drop_column, axis=1, inplace=True)\n# data_val.head()","9240d6a1":"# First we combine the training data and test dat in order to get the same number of dummy features when using one hot encoded\n# to deal with the training data and test data together, we firstly separate features and labels in training data into data_raw and Y_train \nY_train = data_raw[\"Survived\"]\ndrop_column = ['Survived']\ndata_raw.drop(drop_column, axis=1, inplace=True)\n\ndata_raw_len = len(data_raw)\ndataset_comb =  pd.concat(objs=[data_raw, data_val], axis=0).reset_index(drop=True)","daf294d9":"# in feature group1, we drop ['Name','AgeGroup','PassengerId','FareBin','Ticket','Cabin'] to use numerical age and fare for the model\n# in feature group2, instead we drop ['Name','Age','PassengerId','Fare','FamilySize','Cabin']\n\nFeature_Group1 = ['Pclass','Sex','Age','Age_null', 'FamilySize','SibSp', 'Parch','Fare','Cabin_null','Embarked', 'Title', 'IsAlone']\nFeature_Group2 = ['Pclass','Sex', 'AgeGroup','Age_null','SibSp', 'Parch', 'Ticket','FareBin','Cabin_null','Cabin','Embarked', 'Title', 'IsAlone']\n\ndataset1 = dataset_comb[Feature_Group1]\ndataset2 = dataset_comb[Feature_Group2]\ndataset1.head()","42ea29ec":"# As for title and embarked features, \n# each different types of values should be of the same weight, \n# we encode them by one hot encoder to transfer each possible value into a boolean type\nfrom sklearn.preprocessing import OneHotEncoder\none_hot_encoded_dataset1 = pd.get_dummies(dataset1)\none_hot_encoded_dataset2 = pd.get_dummies(dataset2)\none_hot_encoded_dataset1.head()","ce04a09a":"from sklearn.preprocessing import MinMaxScaler\n# scaler_age = MinMaxScaler()\n# scaler_fare = MinMaxScaler()\n\n# # print([dataset1['Age']].size)\n# scaled_age = scaler_age.fit_transform(dataset1[['Age']])\n# scaled_fare = scaler_fare.fit_transform(dataset1[['Fare']])\n# dataset1['Age'] = scaled_age\n# dataset1['Fare'] = scaled_fare\n\none_hot_encoded_dataset1[['Age', 'Fare']] = MinMaxScaler().fit_transform(dataset1[['Age', 'Fare']])\n#one_hot_encoded_dataset1[['Age']] = MinMaxScaler().fit_transform(dataset1[['Age']])\n#one_hot_encoded_dataset1.head()\n# dataset2['Fare'] = preprocessing.normalize(dataset1['Fare'])\n\n# preprocessing.normalize(dataset1['Age'])\n# dataset2['Fare'] = preprocessing.normalize(dataset1['Fare'])","1c323426":"#Y_train = one_hot_encoded_data_raw[\"Survived\"]\n#drop_column = ['Survived']\n#one_hot_encoded_data_raw.drop(drop_column, axis=1, inplace=True)\n\nX_train = one_hot_encoded_dataset1[:data_raw_len]\nX_test = one_hot_encoded_dataset1[data_raw_len:]\n\nX_train.head()","dc8b9b68":"from sklearn.model_selection import train_test_split\nx_train, x_dev, y_train, y_dev = train_test_split(X_train, Y_train, test_size = 0.25, random_state = 1)","30332738":"## Logistic classification\nfrom sklearn import linear_model\nfrom sklearn.metrics import accuracy_score, fbeta_score\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(penalty = 'l2', C = 0.2,random_state = 0)\nlogreg.fit(x_train, y_train)\ny_pred = logreg.predict(x_dev)\ny_train_pred = logreg.predict(x_train)\nacc_log = round(accuracy_score(y_pred, y_dev) * 100, 2)\nacc_log_train = round(accuracy_score(y_train_pred, y_train) * 100, 2)\nacc_log,acc_log_train","266ceadd":"## Linear supporter vector classifier\nfrom sklearn.svm import SVC, LinearSVC\nlinear_svc = LinearSVC()\nlinear_svc.fit(x_train, y_train)\ny_pred = linear_svc.predict(x_dev)\ny_train_pred = linear_svc.predict(x_train)\nacc_linear_svc = round(accuracy_score(y_pred,y_dev) * 100, 2)\nacc_linear_train = round(accuracy_score(y_train_pred, y_train) * 100, 2)\nacc_linear_svc, acc_linear_train","3908c4a4":"## Perceptron\nfrom sklearn.linear_model import Perceptron\nperceptron = Perceptron()\n\nperceptron.fit(x_train, y_train)\ny_pred = perceptron.predict(x_dev)\ny_train_pred = perceptron.predict(x_train)\nacc_perceptron = round(accuracy_score(y_pred,y_dev) * 100, 2)\nacc_perceptron_train = round(accuracy_score(y_train_pred, y_train) * 100, 2)\nacc_perceptron, acc_perceptron_train","e6d0b7b0":"## Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\ngaussian = GaussianNB()\ngaussian.fit(x_train, y_train)\ny_pred = gaussian.predict(x_dev)\ny_train_pred = gaussian.predict(x_train)\nacc_gaussian = round(accuracy_score(y_pred, y_dev) * 100, 2)\nacc_gaussian_train = round(accuracy_score(y_train_pred, y_train) * 100, 2)\nacc_gaussian, acc_gaussian_train","d72de6ed":"# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\ndecisiontree = DecisionTreeClassifier(max_depth=8, min_samples_leaf=10, min_samples_split=15)\ndecisiontree.fit(x_train, y_train)\ny_pred = decisiontree.predict(x_dev)\ny_train_pred = decisiontree.predict(x_train)\nacc_dt = round(accuracy_score(y_pred, y_dev) * 100, 2)\nacc_dt_train = round(accuracy_score(y_train_pred, y_train) * 100, 2)\nacc_dt, acc_dt_train","47590b8d":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrandomforest = RandomForestClassifier(n_estimators = 200, max_depth=4, max_features = 5 , min_samples_leaf=2, min_samples_split=5)\nrandomforest.fit(x_train, y_train)\ny_pred = randomforest.predict(x_dev) \ny_train_pred = randomforest.predict(x_train)\nacc_rf = round(accuracy_score(y_pred, y_dev) * 100, 2)\nacc_rf_train = round(accuracy_score(y_train_pred, y_train) * 100, 2)\nacc_rf, acc_rf_train","f862721f":"# Support Vector Machines\nfrom sklearn.svm import SVC\nsvc = SVC(kernel='rbf',gamma=0.001,C=10)\nsvc.fit(x_train, y_train)\ny_pred = svc.predict(x_dev)\ny_train_pred = svc.predict(x_train)\nacc_svc = round(accuracy_score(y_pred, y_dev) * 100, 2)\nacc_svc_train = round(accuracy_score(y_train_pred, y_train) * 100, 2)\n\n# x_dev_copy = x_dev\n# x_dev_copy['Label'] = y_dev.values\n# x_dev_copy['Pred'] = y_pred\n# x_dev_copy[x_dev_copy['Label']!=x_dev_copy['Pred']].head(100)\n# # for i in range(0,len(y_dev)):\n# # #     print(y_pred[i], y_dev.values[i])\n# # #     print(i)\n# #     if y_pred[i] != y_dev.values[i]:\n# # #         print(y_dev.index[i])\n# # #         print(x_dev.loc[680])\n# #         errors.append(x_dev.loc[y_dev.index[i]])\n# # errors\nacc_svc, acc_svc_train\n","f969f46a":"# K-nearest Neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\nknn.fit(x_train, y_train)\ny_pred = knn.predict(x_dev)\ny_train_pred = knn.predict(x_train)\nacc_knn = round(accuracy_score(y_pred, y_dev) * 100, 2)\nacc_knn_train = round(accuracy_score(y_train_pred, y_train) * 100, 2)\nacc_knn, acc_knn_train","ac97fbf5":"#Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n\n#kfold = StratifiedKFold(n_splits=10)\n\nGBC = GradientBoostingClassifier(max_depth=4,min_samples_leaf=100,max_features=0.2)\n#gbc_param_grid = {'loss':[\"deviance\"],\n #               'n_estimators':[100,200,300],\n  #              'learning_rate':[0.1,0.05,0.01],\n   #             'max_depth':[4,8],\n    #            'min_samples_leaf':[100,150],\n     #           'max_features':[0.3,0.1]\n      #          }\n#gsGBC = GridSearchCV(GBC,param_grid = gbc_param_grid, cv=kfold, scoring =\"accuracy\",n_jobs = 4,verbose =1)\nGBC.fit(x_train, y_train)\ny_pred = GBC.predict(x_dev)\ny_train_pred = GBC.predict(x_train)\nacc_gbc = round(accuracy_score(y_pred,y_dev)*100,2)\nacc_gbc_train = round(accuracy_score(y_train_pred, y_train) * 100, 2)\nacc_gbc,acc_gbc_train","4378232b":"from sklearn.ensemble import VotingClassifier\n\nvotingC = VotingClassifier(estimators=[('rf', randomforest), ('lr', logreg), ('sv',svc),('gb',GBC)], voting='hard')","57257de8":"votingC.fit(X_train, Y_train)\nY_pred = votingC.predict(X_test)\n\ndata_val1 = pd.read_csv('..\/input\/test.csv')\nsubmission = pd.DataFrame({\n        \"PassengerId\": data_val1[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('submission.csv', index=False)","7793cddb":"Here we can learn that if you have a family size of 2-4, you will in geneneral have a higher chance to survive. However, it is noted that as passengers with family size 4 or higher are limisted, there might be bias here. \n","9c8185ac":"**Titanic Survival Prediction**\n\n1. Import libraries\n2. Import data\n3. Data cleaning, feature engineeirng, data visualization\n4. Establishing the model\n5. Model Implementation\n6. Model ensembling\n7. Prediction and submit the data","4a676a11":"We standardardize the age and fare features in type 1 model","c8f7174c":"The FareBin feature is converted into numbers.","1e97a3c1":"**7. Prediction and submit results**","322509b9":"**1. Import libraries**","70e6cc08":"**Part 1: Logistic regression, linear supporter vector classifier,  Perceptron, Naive Bayes**","2309d70f":"**2. Import data**","116b0636":"**6. Model ensembling**\n\nWe chose a voting classifer to combine the predictions coming from the 4 classifers which have the highest scores.","b4acbe05":"The figures showing survival rate ploted as functions of Pclass, Embarked, Cabin, FareBin are shown as below.","9ce4b6bd":"A new feature FareBin is created.\n\nThe first letter of the Cabin is extracted and stored as the Cabin feature.","2c436d83":"According to the above analysis, the age value is related to both the title and the pclass, as we hypothesized initially. It is also interesting to discovered that in all pclass group, survived people are of younger median age than dead ones. So we assign the median of each [title, pclass] group if the age value is missed.","12e8b260":"To establish our model, we split the training data into training set and development set with the ratio 75:25","5b267484":"**Category 2: Sex, Age, SibSp, Parch**\n\nMissing values: 177 Age values","97530ea2":"The 2 missing Embarked values are filled with the most frequent value for the Embarked feature. \n\nSince Fare feature is very related to the pclass feature, the 1 missing Fare value is filled in with the mean Fare value for the pclass the passenger belongs to.","834339d2":"**3. Data cleaning, feature engineering, data visualization**\n\nIn this section we are going to do some exploratory data analysis with descriptive and graphical methods. The features of interest fall into the following categories:\n1. Pclass, Fare, Embarked, Cabin (Yu)\n2. Sex, Title, Age, SibSp, Parch (Shiyan)\n\n\n**Category 1: Pclass, Fare, Embarked, Cabin, Ticket**\n\nMissing values:\n2 Embarked values, 1 Fare value, 1014 Cabin values\n","544c48f8":"We noticed that several age features are missing, we assume that one important reason for the value missing is that they did not survived so there is no record. To prove our hypothesis, we establish a new feature to indicate if the age feature is missing and to study their relationship with the survival rate.","49ebd3ff":"**Feature: Age**\n\nFirstly, let us have a look at the relationship between the age and the survival rate. It is interesting to discovered that babies of age 0-5 have a higher chance to survive, while at the same time, elderly passengers who are order than 65 have the survival rate as low as only ~10%.","d8bc2fb7":"The prefix part of the ticket feature is extracted and stored as the ticket feature.","4c5011ca":"**5. Model implementation**\n\n**Part 1: Logistic regression, linear supporter vector classifier,  Perceptron, Naive Bayes,  (Yu)**\n\n**Part 2: Support vector classifier, K-nearest neighbors, Decision tree, Random forest (Shiyan)**\n\n**Gradient boost classifier**","1f4d6eef":"**Feature SibSp and Parch**\n\nBoth these features represent the number of relatives on the Titanic, so we create a new feature to represent the size of the family for each passengers.","5775405e":"**4. Establishing the model**\n\nTo establish our model, we drop features including name, passenger ID  as they can hardly provide new insights (we already took out the title features)\nWe establish two types of model dealing with the age and fare information: in type 1, we keep the numerical age and fare and drop the agegroup and farebin features. in type 2, we instead use the agegroup and farebin to represent the age and fare.\nas we use agegroup instead, sibsp and parch as we used family size instead, and Fare as we think it is determined by the pclass and embarked features. ","2adbd171":"We noticed that several cabin features are missing, we assume that one important reason for the value missing is that they did not survived so there is no record. To prove our hypothesis, we establish a new feature to indicate if the cabin feature is missing and to study their relationship with the survival rate.","f02b1ebc":"**Feature Sex**\n\nAs can be seen below, over 70% of female passengers are survived while only ~20% male are survived. Thank you gentlemen!","05132143":"**Part 2: Support vector classifier, K-nearest neighbors, Decision tree, Random forest**","f5c03dd5":"**Gradient boosting classifier**"}}