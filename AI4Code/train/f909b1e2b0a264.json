{"cell_type":{"ed650ad5":"code","1f3c1955":"code","cc943af1":"code","acd48d44":"code","b3a9a934":"code","922f3b2e":"code","15adb9c9":"code","edd53e3d":"code","da1fcce0":"code","ab33d21e":"code","50e1d8bf":"code","7f131485":"code","f5df0765":"code","bd590bbc":"code","d0e6ca9b":"code","e9459c81":"code","1f8af658":"code","5c0f842f":"code","0f6128bf":"code","f1182989":"code","b253b7cc":"markdown","83b920b2":"markdown","13896388":"markdown","60157352":"markdown","aa680b35":"markdown","87e54502":"markdown","5fe88a7f":"markdown","6b1f9146":"markdown","67b51719":"markdown"},"source":{"ed650ad5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1f3c1955":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n%matplotlib inline","cc943af1":"plt.style.use(\"seaborn\")","acd48d44":"mean_01=np.array([1,0.5])\ncov_01=np.array([[1,0.1],[0.1,1.2]])\nprint(\"parameters for x1\",mean_01,cov_01,sep=\"\\n\")\nmean_02=np.array([4,5])\ncov_02=np.array([[1.21,0.1],[0.1,1.3]])\nprint(\"parameters for x2\",mean_01,cov_01,sep=\"\\n\")","b3a9a934":"dist_01=np.random.multivariate_normal(mean_01,cov_01,500)\ndist_02=np.random.multivariate_normal(mean_02,cov_02,500)","922f3b2e":"plt.figure(0)\nplt.scatter(dist_01[:,0],dist_01[:,1],label=\"class0\")\nplt.scatter(dist_02[:,0],dist_02[:,1],color=\"r\",marker=\"^\",label=\"class1\")\nplt.xlim(-3,8)\nplt.ylim(-3,8)\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.legend()\nplt.show()","15adb9c9":"data=np.vstack((dist_01,dist_02))","edd53e3d":"np.random.shuffle(data)#as we created two different distributions and merged them.","da1fcce0":"print(data)","ab33d21e":"# As our output should be either 1 or 0 becoz we have two classes or we are using binary classification.\ndef sigmoid(x):\n    training_data=[]\n    mean=np.mean(x[:,-1])\n    for i in x[:,-1]:\n        if i > mean :\n            training_data.append(1)\n        elif i<mean:\n            training_data.append(0)\n    training_data= np.array(training_data)\n    return training_data","50e1d8bf":"Y=sigmoid(data)","7f131485":"print(Y.shape)\nprint(Y[0:10])","f5df0765":"X_train=data[:800,:]\nX_test=data[800:,:]\nY_train=Y[:800,]\nY_test=Y[800:,]\nprint(X_train.shape,X_test.shape)\nprint(Y_train.shape,Y_test.shape)\n#we used 20 % data testing purpose and rest for training","bd590bbc":"def hypothesis(x,w,b):\n    h=np.dot(x,w)+b\n    return sigmoid(h)\n\ndef sigmoid(z):\n    return 1.0\/(1.0+np.exp(-1.0*z))\n\ndef error(y_true,x,w,b):\n    m=x.shape[0]\n    err=0.0\n    for i in range(m):\n        hx = hypothesis(x[i],w,b)\n        err += y_true[i]*np.log2(hx) + (1-y_true[i])*np.log2(1-hx)\n    return -err\/m\n\ndef get_grad(y_true,x,w,b):\n    grad_w=np.zeros(w.shape)\n    grad_b=0.0\n    m=x.shape[0]\n    for i in range(m):\n        hx=hypothesis(x[i],w,b)\n        grad_w += (y_true[i]-hx)*x[i]\n        grad_b += (y_true[i]-hx)\n    grad_w \/=m\n    grad_b \/=m\n    return [grad_w,grad_b]\n\ndef grad_descent(x,y_true,w,b,learning_rate=0.1):\n    err=error(y_true,x,w,b)\n    [grad_w,grad_b] = get_grad(y_true,x,w,b)\n    \n    w = w +learning_rate* grad_w\n    b = b + learning_rate* grad_b\n    return err,w,b\n\ndef predict(x,w,b):\n    confidence = hypothesis(x,w,b)\n    if confidence <0.5:\n        return 0\n    else:\n        return 1\n\ndef get_acc(x_tst,y_tst,w,b):\n    y_pred=[]\n    for i in range(x_tst.shape[0]):\n        p=predict(x_tst[i],w,b)\n        y_pred.append(p)\n    y_perd=np.array(y_pred)\n    return float((y_pred==y_tst).sum())\/y_tst.shape[0]","d0e6ca9b":"w = 2*np.random.random((X_train.shape[1],))\nb = 5*np.random.random()\nloss=[]\nacc=[]","e9459c81":"#training \nfor i in range (500):\n    l,w,b = grad_descent(X_train,Y_train,w,b,learning_rate=0.7)\n    acc.append(get_acc(X_test,Y_test,w,b))\n    loss.append(l)","1f8af658":"print(loss[0:20])\nplt.plot(loss)\nplt.xlabel(\"Time\")\nplt.ylabel(\"Negative of log likelihood\")\nplt.show()","5c0f842f":"print(acc[0:20])\nplt.plot(acc)\nplt.show()","0f6128bf":"print(w)\nprint(b)\n#for Visualizing the separating boundaries we are going to use these values","f1182989":"plt.figure(0)\nplt.scatter(dist_01[:,0],dist_01[:,1],label=\"class0\")\nplt.scatter(dist_02[:,0],dist_02[:,1],color=\"r\",marker=\"^\",label=\"class1\")\nplt.xlim(-3,8)\nplt.ylim(-3,8)\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nx=np.linspace(-3,8,10)\ny= - ( w[0] * x + b ) \/ w[1]\nplt.plot(x,y,color=\"k\")\nplt.legend()\nplt.show()","b253b7cc":"- randomly select the value of w,b to start the function","83b920b2":"## Generating two Random Distributions from the above data","13896388":"## Implementing Logistic regression from scratch !","60157352":"## Initializing and runing the code :","aa680b35":"## Joining data","87e54502":"## Importing various libraries","5fe88a7f":"## Data preparation","6b1f9146":"## visualization of decision surface","67b51719":"## Visualisation of data"}}