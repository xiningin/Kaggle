{"cell_type":{"8de21d11":"code","0040dbec":"code","48d876af":"code","f1b17747":"code","cd8b6562":"code","73f3886f":"code","72bc1cde":"code","27c806ff":"code","6d4cf2d5":"code","90834abd":"code","34b9c3a2":"code","3f62a0f3":"code","9fbabb9e":"code","bb3d3baa":"code","4725ccc2":"code","5256c819":"code","503adff9":"code","6b8fbfe0":"code","852167ab":"code","925cd2e9":"code","7d1f6302":"code","be0a3820":"code","fd5f7023":"code","9240fafb":"code","c8d23fe0":"code","829d5947":"code","e390b80e":"code","d547d70b":"code","7b315302":"code","aa1ffb9d":"code","3e38fd75":"code","cdc8ff92":"code","e153680f":"markdown","2d6b3ed2":"markdown","cb1e3989":"markdown","4ea57dc5":"markdown"},"source":{"8de21d11":"!pip install pytorch-transformers","0040dbec":"import os\nfrom pytorch_transformers import BertTokenizer, BertConfig\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom pytorch_transformers import BertTokenizer, BertConfig\nfrom pytorch_transformers import AdamW, BertForSequenceClassification\nfrom tqdm import tqdm, trange\nimport pandas as pd\nimport io\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('stopwords')\nlemmatizer = WordNetLemmatizer()\nstop_words_en = set(stopwords.words('english'))\nstemmer_en = SnowballStemmer('english')\n\nimport warnings\nwarnings.filterwarnings('ignore')","48d876af":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif device == 'cpu':\n    print('cpu')\nelse:\n    n_gpu = torch.cuda.device_count()\n    print(torch.cuda.get_device_name(0))","f1b17747":"dir_data = '..\/input\/imdb-dataset-of-50k-movie-reviews'\n# dir_models = '..\/models'\nname_file = 'IMDB Dataset.csv'\n# os.makedirs(dir_data, exist_ok=True)\n# os.makedirs(dir_models, exist_ok=True)","cd8b6562":"df = pd.read_csv(os.path.join(dir_data, name_file))","73f3886f":"df.sample(5)","72bc1cde":"config = {\n    'TextPreprocessor': {\n        'del_orig_col': False,\n        'mode_stemming': True,\n        'mode_norm': True,\n        'mode_remove_stops': True,\n        'mode_drop_long_words': True,\n        'mode_drop_short_words': True,\n        'min_len_word': 3,\n        'max_len_word': 15,\n        'columns_names': 'review'        \n    },\n}\n\nclass TextPreprocessor(object):\n    def __init__(self, config):\n        \"\"\"Preparing text features.\"\"\"\n        self._del_orig_col = config.get('del_orig_col', True)\n        self._mode_stemming = config.get('mode_stemming', True)\n        self._mode_norm = config.get('mode_norm', True)\n        self._mode_remove_stops = config.get('mode_remove_stops', True)\n        self._mode_drop_long_words = config.get('mode_drop_long_words', True)\n        self._mode_drop_short_words = config.get('mode_drop_short_words', True)\n        self._min_len_word = config.get('min_len_word', 3)\n        self._max_len_word = config.get('max_len_word', 17)\n        self._max_size_vocab = config.get('max_size_vocab', 100000)\n        self._max_doc_freq = config.get('max_doc_freq', 0.8) \n        self._min_count = config.get('min_count', 5)\n        self._pad_word = config.get('pad_word', None)\n        self._columns_names = config.get('columns_names', None)\n\n    def _clean_text(self, input_text):\n        \"\"\"Delete special symbols.\"\"\"\n        input_text = input_text.str.lower()\n        input_text = input_text.str.replace(r'[^a-z ]+', ' ')\n        input_text = input_text.str.replace(r' +', ' ')\n        input_text = input_text.str.replace(r'^ ', '')\n        input_text = input_text.str.replace(r' $', '')\n        return input_text\n\n    def _text_normalization_en(self, input_text):\n        '''Normalization of english text'''\n        return ' '.join([lemmatizer.lemmatize(item) for item in input_text.split(' ')])\n\n    def _remove_stops_en(self, input_text):\n        '''Delete english stop-words'''\n        return ' '.join([w for w in input_text.split() if not w in stop_words_en])\n\n    def _stemming_en(self, input_text):\n        '''Stemming of english text'''\n        return ' '.join([stemmer_en.stem(item) for item in input_text.split(' ')])\n\n    def _drop_long_words(self, input_text):\n        \"\"\"Delete long words\"\"\"\n        return ' '.join([item for item in input_text.split(' ') if len(item) < self._max_len_word])\n\n    def _drop_short_words(self, input_text):\n        \"\"\"Delete short words\"\"\"\n        return ' '.join([item for item in input_text.split(' ') if len(item) > self._min_len_word])\n    \n    def transform(self, df):        \n        \n        df[self._columns_names] = df[self._columns_names].astype('str')\n        df['union_text'] = df[self._columns_names]\n            \n        if self._del_orig_col:\n            df = df.drop(self._columns_names, 1)\n    \n        df['union_text'] = self._clean_text(df['union_text'])\n        \n        if self._mode_norm:\n            df['union_text'] = df['union_text'].apply(self._text_normalization_en, 1)\n            \n        if self._mode_remove_stops:\n            df['union_text'] = df['union_text'].apply(self._remove_stops_en, 1)\n            \n        if self._mode_stemming:\n            df['union_text'] = df['union_text'].apply(self._stemming_en)\n            \n        if self._mode_drop_long_words:\n            df['union_text'] = df['union_text'].apply(self._drop_long_words, 1)\n            \n        if self._mode_drop_short_words:\n            df['union_text'] = df['union_text'].apply(self._drop_short_words, 1)\n            \n        df.loc[(df.union_text == ''), ('union_text')] = 'empt'\n\n        return df","27c806ff":"df = TextPreprocessor(config['TextPreprocessor']).transform(df)","6d4cf2d5":"df.sample(5)","90834abd":"df.sentiment.value_counts()","34b9c3a2":"sentences = [\"[CLS] \" + sentence[0:500] + \" [SEP]\" for sentence in df['union_text'].values]\nlabels = [[1] if i == 'positive' else [0] for i in df['sentiment'].values]","3f62a0f3":"print(sentences[1000], labels[1000])","9fbabb9e":"train_sentences, test_sentences, train_gt, test_gt = train_test_split(\n    sentences, \n    labels, \n    test_size=0.3, \n    random_state=123,\n)","bb3d3baa":"print(len(train_gt), len(test_gt))","4725ccc2":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\ntokenized_texts = [tokenizer.tokenize(sent) for sent in train_sentences]\nprint (tokenized_texts[10])","5256c819":"input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]","503adff9":"input_ids = pad_sequences(\n    input_ids,\n    maxlen=250,\n    dtype=\"long\",\n    truncating=\"post\",\n    padding=\"post\"\n)","6b8fbfe0":"attention_masks = [[float(i>0) for i in seq] for seq in input_ids]","852167ab":"train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(\n    input_ids, train_gt, \n    random_state=123,\n    test_size=0.1\n)\n\ntrain_masks, validation_masks, _, _ = train_test_split(\n    attention_masks,\n    input_ids,\n    random_state=123,\n    test_size=0.1\n)","925cd2e9":"train_inputs = torch.tensor(train_inputs)\ntrain_labels = torch.tensor(train_labels)\ntrain_masks = torch.tensor(train_masks)","7d1f6302":"validation_inputs = torch.tensor(validation_inputs)\nvalidation_labels = torch.tensor(validation_labels)\nvalidation_masks = torch.tensor(validation_masks)","be0a3820":"train_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_dataloader = DataLoader(\n    train_data,\n    sampler=RandomSampler(train_data),\n    batch_size=32\n)","fd5f7023":"validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_dataloader = DataLoader(\n    validation_data,\n    sampler=SequentialSampler(validation_data),\n    batch_size=32\n)","9240fafb":"model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\nmodel.cuda()","c8d23fe0":"param_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'gamma', 'beta']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.0}\n]\n\noptimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)","829d5947":"from IPython.display import clear_output\n\ntrain_loss_set = []\ntrain_loss = 0\n\n# Switch on training mode\nmodel.train()\n\nfor step, batch in enumerate(train_dataloader):\n    batch = tuple(t.to(device) for t in batch)\n    b_input_ids, b_input_mask, b_labels = batch\n    \n    optimizer.zero_grad()\n    \n    loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n\n    train_loss_set.append(loss[0].item())  \n\n    loss[0].backward()\n    \n    optimizer.step()\n    train_loss += loss[0].item()\n    \n    clear_output(True)\n    plt.plot(train_loss_set)\n    plt.title(\"Training loss\")\n    plt.xlabel(\"Batch\")\n    plt.ylabel(\"Loss\")\n    plt.show()\n    \nprint(\"Train Loss: {0:.5f}\".format(train_loss \/ len(train_dataloader)))","e390b80e":"# torch.save(model, os.path.join(dir_models, 'Bert_epoch_1'))","d547d70b":"# Validate\n# Switch on evaluation mode\nmodel.eval()\n\nvalid_preds, valid_labels = [], []\n\nfor batch in validation_dataloader:   \n    batch = tuple(t.to(device) for t in batch)\n    b_input_ids, b_input_mask, b_labels = batch\n    \n    with torch.no_grad():\n        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n\n    logits = logits[0].detach().cpu().numpy()\n    label_ids = b_labels.to('cpu').numpy()\n    \n    batch_preds = np.argmax(logits, axis=1)\n    batch_labels = np.concatenate(label_ids)     \n    valid_preds.extend(batch_preds)\n    valid_labels.extend(batch_labels)\n\nprint(\"Valid ACC: {0:.2f}%\".format(\n    accuracy_score(valid_labels, valid_preds) * 100\n))","7b315302":"tokenized_texts = [tokenizer.tokenize(sent) for sent in test_sentences]\ninput_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n\ninput_ids = pad_sequences(\n    input_ids,\n    maxlen=250,\n    dtype=\"long\",\n    truncating=\"post\",\n    padding=\"post\"\n)","aa1ffb9d":"attention_masks = [[float(i>0) for i in seq] for seq in input_ids]\n\nprediction_inputs = torch.tensor(input_ids)\nprediction_masks = torch.tensor(attention_masks)\nprediction_labels = torch.tensor(test_gt)\n\nprediction_data = TensorDataset(\n    prediction_inputs,\n    prediction_masks,\n    prediction_labels\n)\n\nprediction_dataloader = DataLoader(\n    prediction_data, \n    sampler=SequentialSampler(prediction_data),\n    batch_size=32\n)","3e38fd75":"model.eval()\ntest_preds, test_labels = [], []\n\nfor batch in prediction_dataloader:\n    batch = tuple(t.to(device) for t in batch)\n    b_input_ids, b_input_mask, b_labels = batch\n    \n    with torch.no_grad():\n        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n\n    logits = logits[0].detach().cpu().numpy()\n    label_ids = b_labels.to('cpu').numpy()\n\n    batch_preds = np.argmax(logits, axis=1)\n    batch_labels = np.concatenate(label_ids)  \n    test_preds.extend(batch_preds)\n    test_labels.extend(batch_labels)","cdc8ff92":"acc_score = accuracy_score(test_labels, test_preds)\nprint('Test ACC: {0:.2f}%'.format(\n    acc_score*100\n))","e153680f":"## Train the model","2d6b3ed2":"# Test the model","cb1e3989":"## Inputs","4ea57dc5":"# IMDB Reviews with Bert (1 epoch)"}}