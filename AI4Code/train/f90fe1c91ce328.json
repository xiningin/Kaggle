{"cell_type":{"8d56129c":"code","3daa92fe":"code","96e9e269":"code","68fc620e":"code","e19f5288":"code","5bf48878":"code","a6de051a":"code","63800caf":"code","948ba1f2":"code","0655295c":"code","8ea368b3":"code","a751b2f3":"code","f9d784b8":"code","12306d4f":"code","4ec4fd5d":"code","6390a6d3":"code","433514e8":"code","065d6073":"code","757d4a98":"code","f74975cf":"code","5c4f10b5":"code","86b4f265":"code","3edcb479":"code","61bf3bbe":"code","c629f701":"markdown"},"source":{"8d56129c":"import numpy as np\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split, GridSearchCV,cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import scale \nfrom sklearn import model_selection\nfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import BaggingRegressor\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","3daa92fe":"# Import and split ","96e9e269":"hit = pd.read_csv(\"..\/input\/hittlers\/Hitters.csv\")\ndf = hit.copy()\ndf = df.dropna()\ndms = pd.get_dummies(df[['League', 'Division', 'NewLeague']])\ny = df[\"Salary\"]\nX_ = df.drop(['Salary', 'League', 'Division', 'NewLeague'], axis=1).astype('float64')\nX = pd.concat([X_, dms[['League_N', 'Division_W', 'NewLeague_N']]], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.25, \n                                                    random_state=42)","68fc620e":"#Set and fit the model","e19f5288":"!pip install xgboost","5bf48878":"import xgboost as xgb","a6de051a":"#We can use special data structure for XGBoost to have high performanced result.\n\nDM_train=xgb.DMatrix(data=X_train, label=y_train)   #enter dependent variable for \"label\"\nDM_test=xgb.DMatrix(data=X_test, label=y_test)  ","63800caf":"from xgboost import XGBRegressor","948ba1f2":"xgb_model=XGBRegressor().fit(X_train,y_train) # i prefer to use classic data structure which i get accustomed to use","0655295c":"#Prediction","8ea368b3":"y_pred=xgb_model.predict(X_test)","a751b2f3":"np.sqrt(mean_squared_error(y_pred,y_test))","f9d784b8":"#Model Tuning","12306d4f":"xgb_model","4ec4fd5d":"# Important params\n#booster\n#colsample_bytree\n#learning_rate  :avoids overfitting. \n#max_depth      \n#n_estimators","6390a6d3":"xgb_grid = {\n     'colsample_bytree': [0.4, 0.5,0.6,0.9,1], \n     'n_estimators':[100, 200, 500, 1000],\n     'max_depth': [2,3,4,5,6],\n     'learning_rate': [0.1, 0.01, 0.5]\n}","433514e8":"xgb_cv=GridSearchCV(xgb_model, xgb_grid, cv=10,n_jobs=-1, verbose=2)\nxgb_cv.fit(X_train,y_train)","065d6073":"xgb_cv.best_params_","757d4a98":"# Final tuned model","f74975cf":"xgb_tuned= XGBRegressor(colsample_bytree=0.6,\n learning_rate=0.1,\n max_depth= 2,\n n_estimators=1000)","5c4f10b5":"xgb_tuned=xgb_tuned.fit(X_train,y_train)","86b4f265":"y_pred=xgb_tuned.predict(X_test)","3edcb479":"np.sqrt(mean_squared_error(y_test,y_pred))","61bf3bbe":"# We found 413 for KNN, \n#          367 for SVR,\n#          363 for Artifical Neural Network.\n#          376 for CART\n#          349 for Bagged Trees\n#          350 for Random Forest\n#          344 for GBM\n#And now,  355 for XG Boosting\n\n#In these models, the best one is GBM model for \"hitters\" data set, till now.","c629f701":"# XG Boost Steps\n#### 1)Import and split\n#### 2)Set and fit the model\n#### 3)Predict\n#### 4)Model Tuning\n#### 5)Find best params, set and fit the model again, find final RMSE."}}