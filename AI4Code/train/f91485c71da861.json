{"cell_type":{"e1b76956":"code","f3bc4a22":"code","1cad8826":"code","34a3ca36":"code","69a0c673":"code","60089531":"code","16bc1080":"code","60f5ddf5":"code","554a4c1a":"code","5646477a":"code","49788c02":"code","bbf8602e":"code","1617e5c7":"code","cec3fcd7":"code","d7b28351":"code","a41d96f3":"code","8e4d2c44":"code","c270f0bc":"code","23f2d660":"code","f04650d5":"code","bc413e64":"code","dc40f3e8":"code","7c633910":"code","8935f45d":"code","622fee2f":"code","e3937d75":"markdown","ad8b77dc":"markdown","877ce172":"markdown","527fad18":"markdown","42e197a4":"markdown","f7fccf2c":"markdown","a3ad3c1d":"markdown","c49b0ab2":"markdown"},"source":{"e1b76956":"# Global variables for testing changes to this notebook quickly\nRANDOM_SEED = 0\nNUM_FOLDS = 5\nETA = 0.016\nNUM_TREES = 10000\nEARLY_STOP = 150","f3bc4a22":"# Remove CPU only verson\n!pip uninstall -y lightgbm\n\n# Install boost development library\n!apt-get install -y libboost-all-dev\n\n# Clone LightGBM repository\n!git clone --recursive https:\/\/github.com\/Microsoft\/LightGBM","1cad8826":"%%bash\ncd LightGBM\nmkdir build\ncd build\ncmake -DUSE_GPU=1 -DOpenCL_LIBRARY=\/usr\/local\/cuda\/lib64\/libOpenCL.so -DOpenCL_INCLUDE_DIR=\/usr\/local\/cuda\/include\/ ..\nmake -j$(nproc)","34a3ca36":"# Reinstall LightGBM\n!cd LightGBM\/python-package\/;python3 setup.py install --precompile\n\n# Cleanup\n!mkdir -p \/etc\/OpenCL\/vendors && echo \"libnvidia-opencl.so.1\" > \/etc\/OpenCL\/vendors\/nvidia.icd\n!rm -r LightGBM","69a0c673":"# General Imports\nimport numpy as np\nimport pandas as pd\nimport time\nimport gc\n\n# Models\nimport xgboost\nimport lightgbm\nimport catboost\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n\n# Model evaluation\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\n# Plotting\nimport matplotlib\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n# Hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')","60089531":"%%time\n\n# Load data\ntrain = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv')\nsubmission = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv')\n\n# Save feature columns\nfeatures = [x for x in train.columns if x not in ['id', 'target', 'kfold']]","16bc1080":"# Downcast float\/int datatypes\ndef reduce_memory_usage(df, verbose=True):\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    for col, dtype in df.dtypes.iteritems():\n        if dtype.name.startswith('int'):\n            df[col] = pd.to_numeric(df[col], downcast ='integer')\n        elif dtype.name.startswith('float'):\n            df[col] = pd.to_numeric(df[col], downcast ='float')\n        \n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n    return df","60f5ddf5":"%%time\n\n# Downcast data\ntrain = reduce_memory_usage(train)\ntest = reduce_memory_usage(test)","554a4c1a":"# Create cross-validation scheme\ntrain['kfold'] = -1\nskf = StratifiedKFold(n_splits = NUM_FOLDS, shuffle = True, random_state = RANDOM_SEED)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train, train['target'])):\n    train['kfold'].iloc[valid_idx] = fold","5646477a":"# CatBoost parameters\ncatboost_params = {\n    'random_state': RANDOM_SEED,\n    'n_estimators': NUM_TREES,\n    #'learning_rate': ETA,\n    'eval_metric': 'AUC:hints=skip_train~false',\n    'task_type': 'GPU',\n}","49788c02":"def train_catboost(model_params = {}, fit_params = {}):\n    \n    # Store the predictions\n    oof_preds = np.zeros((train.shape[0],))\n    test_preds = np.zeros((test.shape[0],))\n    scores = np.zeros(NUM_FOLDS)\n    times = np.zeros(NUM_FOLDS)\n    print('')\n\n    # Stratified k-fold cross-validation\n    for fold in range(NUM_FOLDS):\n        \n        # Training and Validation Sets\n        X_train, y_train = train[train.kfold != fold][features], train[train.kfold != fold]['target']\n        X_valid, y_valid = train[train.kfold == fold][features], train[train.kfold == fold]['target']\n        X_test = test[features]\n        \n        start = time.time()\n        \n        # Define Model\n        model = CatBoostClassifier(**{**catboost_params, **model_params})\n        gc.collect()\n        \n        model.fit(\n            X_train, y_train,\n            verbose = False,\n            eval_set = [(X_valid, y_valid)],\n            early_stopping_rounds = EARLY_STOP,\n            use_best_model = True,\n            **fit_params\n        )\n        \n        # validation and test predictions\n        valid_preds = model.predict_proba(X_valid)[:, 1]\n        test_preds += model.predict_proba(X_test)[:, 1] \/ NUM_FOLDS\n        oof_preds[train.kfold == fold] = valid_preds\n        \n        # fold auc score\n        fold_auc = roc_auc_score(y_valid, valid_preds)\n        end = time.time()\n        print(f'Fold {fold} (AUC):', fold_auc)\n        scores[fold] = fold_auc\n        times[fold] = end-start\n    \n    print(\"\\nAverage AUC:\", scores.mean())\n    print(f'Training Time: {round(times.sum(), 2)}s')\n    \n    return model, test_preds, oof_preds","bbf8602e":"# Make CatBoost submission\ncatb_model, catb_preds, catb_oof = train_catboost()\nsubmission['target'] =  catb_preds\nsubmission.to_csv('catboost_submission.csv', index=False)","1617e5c7":"# Get model evaluation results\nresults = catb_model.get_evals_result()\nnum_iter = len(results['learn']['AUC'])\nx_axis = range(0, num_iter)\n\n# Plot training curve\nfig, ax = plt.subplots(figsize = (9,6))\nax.plot(x_axis, results['learn']['AUC'], label='Train')\nax.plot(x_axis, results['validation']['AUC'], label='Valid')\nplt.axvline(x=catb_model.get_best_iteration(), color='k', linestyle='--')\nax.legend()\nplt.ylabel('AUC')\nplt.xlabel('Iterations')\nplt.title('CatBoost AUC')\nplt.grid(True)\nplt.show()","cec3fcd7":"# LightGBM parameters\nlightgbm_params = {\n    'random_state': RANDOM_SEED,\n    'n_estimators': NUM_TREES,\n    'learning_rate': ETA,\n    'verbose': 0,\n    'device': 'gpu',\n    'gpu_platform_id': 0,\n    'gpu_device_id': 0,\n}","d7b28351":"def train_lightgbm(model_params = {}, fit_params = {}):\n    \n    # Store the holdout predictions\n    oof_preds = np.zeros((train.shape[0],))\n    test_preds = np.zeros((test.shape[0],))\n    scores = np.zeros(NUM_FOLDS)\n    times = np.zeros(NUM_FOLDS)\n    print('')\n    \n    # Stratified k-fold cross-validation\n    for fold in range(NUM_FOLDS):\n        \n        # Training and Validation Sets\n        X_train, y_train = train[train.kfold != fold][features], train[train.kfold != fold]['target']\n        X_valid, y_valid = train[train.kfold == fold][features], train[train.kfold == fold]['target']\n        X_test = test[features]\n        \n        # Define Model\n        model = LGBMClassifier(**{**lightgbm_params, **model_params})\n        gc.collect()\n        \n        start = time.time()\n        \n        model.fit(\n            X_train, y_train,\n            eval_set = [(X_train, y_train), (X_valid, y_valid)],\n            eval_names = ['Train', 'Valid'],\n            eval_metric = \"auc\",\n            callbacks = [lightgbm.early_stopping(EARLY_STOP, verbose = False)],\n            **fit_params\n        )\n        \n        # validation and test predictions\n        valid_preds = model.predict_proba(X_valid)[:, 1]\n        test_preds += model.predict_proba(X_test)[:, 1] \/ NUM_FOLDS\n        oof_preds[train.kfold == fold] = valid_preds\n        \n        # fold auc score\n        fold_auc = roc_auc_score(y_valid, valid_preds)\n        end = time.time()\n        print(f'Fold {fold} (AUC):', fold_auc)\n        scores[fold] = fold_auc\n        times[fold] = end-start\n    \n    print(\"\\nAverage AUC:\", scores.mean())\n    print(f'Training Time: {round(times.sum(), 2)}s')\n    \n    return model, test_preds, oof_preds","a41d96f3":"# Make LightGBM submission\nlgbm_model, lgbm_preds, lgbm_oof = train_lightgbm()\nsubmission['target'] =  lgbm_preds\nsubmission.to_csv('lightgbm_submission.csv', index=False)","8e4d2c44":"# Get model evaluation results\nresults = lgbm_model.evals_result_\nnum_iter = len(results['Train']['auc'])\nx_axis = range(0, num_iter)\n\n# Plot training curve\nfig, ax = plt.subplots(figsize = (9,6))\nax.plot(x_axis, results['Train']['auc'], label='Train')\nax.plot(x_axis, results['Valid']['auc'], label='Valid')\nplt.axvline(x=lgbm_model.best_iteration_, color='k', linestyle='--')\nax.legend()\nplt.ylabel('AUC')\nplt.xlabel('Iterations')\nplt.title('LightGBM AUC')\nplt.grid(True)\nplt.show()","c270f0bc":"# XGBoost parameters\nxgboost_params = {\n    'random_state': RANDOM_SEED,\n    'n_estimators': NUM_TREES,\n    'learning_rate': ETA,\n    'tree_method': 'gpu_hist',\n    'predictor': \"gpu_predictor\",\n}","23f2d660":"def train_xgboost(model_params = {}, fit_params = {}):\n    \n    # Store the  predictions\n    oof_preds = np.zeros((train.shape[0],))\n    test_preds = np.zeros((test.shape[0],))\n    scores = np.zeros(NUM_FOLDS)\n    times = np.zeros(NUM_FOLDS)\n    print('')\n    \n    # Stratified k-fold cross-validation\n    for fold in range(NUM_FOLDS):\n        \n        # Training and Validation Sets\n        X_train, y_train = train[train.kfold != fold][features], train[train.kfold != fold]['target']\n        X_valid, y_valid = train[train.kfold == fold][features], train[train.kfold == fold]['target']\n        X_test = test[features]\n        \n        # Define Model\n        model = XGBClassifier(**{**xgboost_params, **model_params})\n        gc.collect()\n        \n        start = time.time()\n        \n        model.fit(\n            X_train, y_train,\n            verbose = False,\n            eval_set = [(X_train, y_train), (X_valid, y_valid)],\n            eval_metric = \"auc\",\n            early_stopping_rounds = EARLY_STOP,\n            **fit_params\n        )\n        \n        # validation and test predictions\n        valid_preds = model.predict_proba(X_valid)[:, 1]\n        test_preds += model.predict_proba(X_test)[:, 1] \/ NUM_FOLDS\n        oof_preds[train.kfold == fold] = valid_preds\n        \n        # fold auc score\n        fold_auc = roc_auc_score(y_valid, valid_preds)\n        end = time.time()\n        print(f'Fold {fold} (AUC):', fold_auc)\n        scores[fold] = fold_auc\n        times[fold] = end-start\n    \n    print(\"\\nAverage AUC:\", scores.mean())\n    print(f'Training Time: {round(times.sum(), 2)}s')\n        \n    return model, test_preds, oof_preds","f04650d5":"# Make XGBoost submission\nxgb_model, xgb_preds, xgb_oof = train_xgboost()\nsubmission['target'] = xgb_preds\nsubmission.to_csv('xgboost_trees_submission.csv', index=False)","bc413e64":"# Get model evaluation results\nresults = xgb_model.evals_result()\nnum_iter = len(results['validation_0']['auc'])\nx_axis = range(0, num_iter)\n\n# Plot training curve\nfig, ax = plt.subplots(figsize = (9,6))\nax.plot(x_axis, results['validation_0']['auc'], label='Train')\nax.plot(x_axis, results['validation_1']['auc'], label='Valid')\nplt.axvline(x=xgb_model.best_iteration, color='k', linestyle='--')\nax.legend()\nplt.ylabel('AUC')\nplt.xlabel('Iterations')\nplt.title('XGBoost w\/ Trees')\nplt.grid(True)\nplt.show()","dc40f3e8":"# XGBoost parameters\nxgboost_params = {\n    'random_state': RANDOM_SEED,\n    'booster': \"gblinear\",\n}","7c633910":"def train_xgboost(model_params = {}, fit_params = {}):\n    \n    # Store the  predictions\n    oof_preds = np.zeros((train.shape[0],))\n    test_preds = np.zeros((test.shape[0],))\n    scores = np.zeros(NUM_FOLDS)\n    times = np.zeros(NUM_FOLDS)\n    print('')\n    \n    # Stratified k-fold cross-validation\n    for fold in range(NUM_FOLDS):\n        \n        # Training and Validation Sets\n        X_train, y_train = train[train.kfold != fold][features], train[train.kfold != fold]['target']\n        X_valid, y_valid = train[train.kfold == fold][features], train[train.kfold == fold]['target']\n        X_test = test[features]\n        \n        # Define Model\n        model = XGBClassifier(**{**xgboost_params, **model_params})\n        gc.collect()\n        \n        start = time.time()\n        \n        model.fit(\n            X_train, y_train,\n            verbose = False,\n            eval_set = [(X_train, y_train), (X_valid, y_valid)],\n            eval_metric = \"auc\",\n            early_stopping_rounds = EARLY_STOP,\n            **fit_params\n        )\n        \n        # validation and test predictions\n        valid_preds = model.predict_proba(X_valid)[:, 1]\n        test_preds += model.predict_proba(X_test)[:, 1] \/ NUM_FOLDS\n        oof_preds[train.kfold == fold] = valid_preds\n        \n        # fold auc score\n        fold_auc = roc_auc_score(y_valid, valid_preds)\n        end = time.time()\n        print(f'Fold {fold} (AUC):', fold_auc)\n        scores[fold] = fold_auc\n        times[fold] = end-start\n    \n    print(\"\\nAverage AUC:\", scores.mean())\n    print(f'Training Time: {round(times.sum(), 2)}s')\n        \n    return model, test_preds, oof_preds","8935f45d":"# Make XGBoost submission\nxgb_model, xgb_preds, xgb_oof = train_xgboost()\nsubmission['target'] = xgb_preds\nsubmission.to_csv('xgboost_linear_submission.csv', index=False)","622fee2f":"# Get model evaluation results\nresults = xgb_model.evals_result()\nnum_iter = len(results['validation_0']['auc'])\nx_axis = range(0, num_iter)\n\n# Plot training curve\nfig, ax = plt.subplots(figsize = (9,6))\nax.plot(x_axis, results['validation_0']['auc'], label='Train')\nax.plot(x_axis, results['validation_1']['auc'], label='Valid')\nplt.axvline(x=xgb_model.best_iteration, color='k', linestyle='--')\nax.legend()\nplt.ylabel('AUC')\nplt.xlabel('Iterations')\nplt.title('XGBoost w\/ Linear Boosting')\nplt.grid(True)\nplt.show()","e3937d75":"## Imports","ad8b77dc":"# Prepare Data","877ce172":"# CatBoost Baseline","527fad18":"## Install GPU-enabled LightGBM\n\nWe follow [this notebook](https:\/\/www.kaggle.com\/abhishek\/running-lightgbm-on-gpu\/notebook) for installing GPU-enabled LightGBM.","42e197a4":"# XGBoost Baseline (Boosted Trees)","f7fccf2c":"# Gradient Boosting Baselines\n\nIn this notebook we get baselines for the GPU enabled LightGBM, XGBoost and CatBoost models. We mostly leave our settings as default and do not preprocess the training data with the exception of the following:\n\n* Downcast our data to it's lowest subtype (e.g. `float64` to `float32`)\n* We set `n_estimators = 10000` and `learning_rate = 0.016` with early stopping.\n* Enable training on GPU\n\nWe change the learning rates so that all the models considered are consistent with what CatBoost chooses by default.\n\n**Note:** The performance of one model relative to another does not indicate that it will perform better than the other after we tweak more parameters or perform feature engineering.","a3ad3c1d":"# LightGBM Baseline","c49b0ab2":"# XGBoost (Linear Boosting)"}}