{"cell_type":{"309e9dda":"code","35300bd3":"code","23960b7d":"code","044094da":"code","0b85f9ba":"code","0280fac0":"code","0b83a198":"code","d831d281":"code","d773bf46":"code","3915c26c":"code","feddfbc3":"code","b6274636":"code","5da5114b":"code","1cbad938":"code","b1ba0812":"code","491a8f41":"code","22f6c063":"code","2bfd6e03":"code","d501218d":"markdown","ee72f2d7":"markdown","6be14514":"markdown","2130c0e7":"markdown","36835a74":"markdown","200629bc":"markdown","da786340":"markdown","ed9c41a7":"markdown","49e4892d":"markdown","a29c5b85":"markdown","9cd32ffd":"markdown","bb296063":"markdown","158e9537":"markdown","9d6b45b3":"markdown","dd46ac83":"markdown"},"source":{"309e9dda":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n#import numpy as np # linear algebra\n#import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","35300bd3":"import json, sys, random\nimport numpy as np\nimport pandas as pd\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, Activation\nfrom keras.layers import Dropout\nfrom keras import regularizers\nfrom keras import optimizers\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom keras.utils import to_categorical","23960b7d":"# download dataset from json object\nf = open(r'..\/input\/ships-in-satellite-imagery\/shipsnet.json')\ndataset = json.load(f)\nf.close()\n\ninput_data = np.array(dataset['data']).astype('uint8')\nlabels_data = np.array(dataset['labels']).astype('uint8')","044094da":"n_spectrum = 3 # color chanel (RGB)\nweight = 80\nheight = 80\nX = input_data.reshape([-1, n_spectrum, weight, height])\nX.shape","0b85f9ba":"Xt=X.transpose(0,2,3,1)\nXt.shape","0280fac0":"plt.figure( figsize = (15,15))\n\n# show each channel\nfor i in range(0,3):\n    plt.subplot(1, 3, (i+1))\n    plt.imshow(Xt[100,:,:,i])","0b83a198":"plt.figure( figsize = (20,20))\n\n# show each channel\nfor i in range(0,20):\n    plt.subplot(5, 4, (i+1))\n    plt.imshow(Xt[i,:,:,:])\n\n","d831d281":"Presence_Absence=pd.value_counts(pd.Series(labels_data))\nPresence_Absence","d773bf46":"# Create train set\ndata_train, data_test, labels_train, labels_test = train_test_split(\n    input_data,\n    labels_data,\n    test_size=.45, random_state=0, stratify=labels_data)\n\n# Create validation and test sets\ndata_validation, data_test, labels_validation, labels_test = train_test_split(\ndata_test, labels_test,test_size=.20, random_state=0)\n\ndata_train=data_train.reshape(-1, n_spectrum, weight, height)\ndata_test=data_test.reshape(-1, n_spectrum, weight, height)\ndata_validation=data_validation.reshape(-1, n_spectrum, weight, height)\n\nprint('Train:',data_train.shape, labels_train.shape)\nprint('Test:', data_test.shape, labels_test.shape)\nprint('Validation:', data_validation.shape, labels_validation.shape)","3915c26c":"data_t_train=data_train.transpose(0,2,3,1)\ndata_t_test=data_test.transpose(0,2,3,1)\ndata_t_validation=data_validation.transpose(0,2,3,1)","feddfbc3":"pca = PCA(n_components=100)\nvalues_train=data_train.reshape(-1,(80*80*3))\npca.fit(values_train)\n\n\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of components')\nplt.ylabel('Cumulative explained variance')\nplt.title('PCA')\n\n","b6274636":"NCOMPONENTS = 80\n\npca = PCA(n_components=NCOMPONENTS)\n\n\ndata_pca_train = pca.fit_transform(data_train.reshape(-1,(80*80*3)))\ndata_pca_val = pca.transform(data_validation.reshape(-1,(80*80*3)))\ndata_pca_test = pca.transform(data_test.reshape(-1,(80*80*3)))\npca_std = np.std(data_pca_train)\n","5da5114b":"labels_train=to_categorical(labels_train)\nlabels_test=to_categorical(labels_test)\nlabels_validation=to_categorical(labels_validation)","1cbad938":"print(data_pca_val.shape)\nprint(labels_validation.shape)","b1ba0812":"model_fcnn = Sequential([\n  Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.0001),input_shape=(80,)),\n  Dropout(0.1),\n  Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.0001)),\n  Dropout(0.1),\n  Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.0001)),\n  Dropout(0.1),\n  Dense(2, activation='softmax', kernel_regularizer=regularizers.l2(0.0001)),\n])\n\nmodel_fcnn.summary()\n\n\nmodel_fcnn.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.RMSprop(),\n              metrics=['accuracy'])\n\nmodel_fcnn.fit(\n  data_pca_train, # training data\n  labels_train, # training targets\n  validation_data=(data_pca_val, labels_validation),\n  epochs=100,\n  batch_size=200\n)","491a8f41":"acc=model_fcnn.history.history['accuracy']\nval_acc=model_fcnn.history.history['val_accuracy']\nloss=model_fcnn.history.history['loss']\nval_loss=model_fcnn.history.history['val_loss']\n\nepochs=range(1,len(acc)+1)\n\nplt.plot(epochs,acc,'b',label='Trainning accuracy')\nplt.plot(epochs,val_acc,'r',label='Validation accuracy')\nplt.title('Trainning & Validation Accuracy - 25 epochs')\nplt.legend()\nplt.figure()\nplt.plot(epochs,loss, 'b', label='Training loss')\nplt.plot(epochs,val_loss, 'r', label='Validation loss')\nplt.title('Trainning & Validation Loss - 50 epochs')\nplt.legend()\nplt.show()\n\n\ntest_loss, test_acc =model_fcnn.evaluate(\n  data_pca_test,\n  labels_test\n)\n\nprint('Test loss', round(test_loss,2))\nprint('Test accuracy', round(test_acc,2))","22f6c063":"prediction=model_fcnn.predict_classes(data_pca_test)","2bfd6e03":"fig=plt.figure(figsize=(20, 20))\n\nrandom.randint(0,len(data_test))\n\ndata_t_test=data_test.transpose(0,2,3,1)\n# show each channel\nfor i in range(0,100):\n    fig.add_subplot(10, 10, i+1)\n    fig.subplots_adjust(left=0.125, bottom=0.1, right=0.9, top=0.9, wspace=.2, hspace=.2)\n    plt.imshow(data_t_test[i,:,:,:])\n    plt.title('input:'+str(labels_test.argmax(axis = 1)[i])+'\\npred:'+str(prediction[i]), fontsize=18, y=.15,color='w')\n    plt.axis('off')\nplt.show()","d501218d":"Convert the labels vectors from integers to binary class matrices:","ee72f2d7":"FCNN model calibration:","6be14514":"The dataset contains 4000 images. \nEach image is represented as a vector of length 19200 elements, containing 3 layers (R,G & B) and 80x80 of weight and height:","2130c0e7":"Label prediction for the test dataset:","36835a74":"Dsiplaying the RGB image for the first 20 images:","200629bc":"Displaying the first 100 images from the test dataset overlaying the test labels as well as the predicted labels with the FCNN model:","da786340":"The dataset contains of 1000 images without ships (labelled with 0) and 3000 images with ships (labelled with 1): ","ed9c41a7":"Download and study the dataset","49e4892d":"Changing the RGB layers to the last dimension:","a29c5b85":"80 components would be enought to retain most of the dataset information:","9cd32ffd":"Splitting the dataset into Train, Validation and Test:","bb296063":"Load necessary packages\/libraries","158e9537":"Displaying each layer RGB for the 100th image:","9d6b45b3":"Choosing the Number of Components in a PCA for the dataset:","dd46ac83":"Permute the dimensions of each array:"}}