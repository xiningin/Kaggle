{"cell_type":{"3456ac4c":"code","1fce6de9":"code","d607c60c":"code","132d2ba2":"code","8d155a03":"code","f6f8fd54":"code","856d8ea2":"code","997440da":"code","dae16a38":"code","8386fe41":"code","0ced8be0":"code","cc3e1fdb":"code","480f3c1d":"code","74597fba":"code","454f117e":"code","4558b177":"code","736af522":"code","0eb68f63":"code","21f5abe6":"code","efab0d9e":"code","0b90bf4e":"code","a524697b":"code","566c82a7":"code","9852d27b":"code","0d7415b2":"code","7db12e94":"code","1a7620b9":"code","22eb078e":"code","64ad8c4b":"code","5827aa95":"code","2e614423":"code","b1292335":"code","41f1377f":"code","ded947b1":"code","73aca614":"code","1132460e":"code","bf6b07cf":"code","10624c8d":"code","c43ace1c":"code","f80054b7":"code","b4680820":"code","f0ef3ba3":"code","c3aaf556":"code","8517dffb":"code","28ac9573":"code","3dbaa945":"code","92f45106":"code","a8d3bd6b":"code","49500c28":"code","1b778197":"code","d90cf399":"code","68d84b7e":"code","49428373":"code","c595dee0":"code","d87ad5eb":"code","14feba00":"code","dd001b52":"code","2d26837d":"code","6b902c94":"code","e1796c7b":"code","b54a06a9":"code","32c52896":"code","daa30b10":"markdown","f8dbe221":"markdown","9bd1ba85":"markdown","b7e85ada":"markdown","87fa9085":"markdown","2216e35a":"markdown","2c5f4030":"markdown","b1033f05":"markdown","2b49b3cb":"markdown","33281225":"markdown","3253f4eb":"markdown","f15b17b7":"markdown","83980708":"markdown","e4a6bd63":"markdown"},"source":{"3456ac4c":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","1fce6de9":"pd.options.display.float_format = '{:,.2f}'.format\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)","d607c60c":"df = pd.read_csv(\"..\/input\/breast-cancer-wisconsin-data\/data.csv\", delimiter=',')\ndf.drop(['Unnamed: 32'], axis = 1, inplace = True)  #unnecessary column\ndf.head(10)   # Showing first 10 rows ","132d2ba2":"df.info()   # Give information about dataset","8d155a03":"df.describe().T","f6f8fd54":"df.corr()","856d8ea2":"ax = sns.countplot(df.diagnosis,label=\"Count\") \nB, M = df.diagnosis.value_counts()\nprint('Number of Benign: ',B)\nprint('Number of Malignant : ',M)","997440da":"mean_features = [\"radius_mean\", \"texture_mean\", \"perimeter_mean\", \"area_mean\", \"smoothness_mean\", \"compactness_mean\", \"concavity_mean\", \n                 \"symmetry_mean\", \"fractal_dimension_mean\"]\n\nfor column in mean_features:\n    plt.figure(figsize = (10,5))\n    feature = df[column]\n    sns.distplot(feature, hist=True, kde=True, \n                    bins=int(180\/5), color = 'blue',\n                     hist_kws={'edgecolor':'black'})","dae16a38":"plt.subplots(figsize=(10,8))\nsns.heatmap(df[mean_features].corr(),cmap='coolwarm',annot=True);","8386fe41":"se_features = [\"radius_se\", \"texture_se\", \"perimeter_se\", \"area_se\", \"smoothness_se\", \"compactness_se\", \"concavity_se\", \n                 \"symmetry_se\", \"fractal_dimension_se\"]\n\nfor column in se_features:\n    plt.figure(figsize = (10,5))\n    feature = df[column]\n    sns.distplot(feature, hist=True, kde=True, \n                    bins=int(180\/5), color = 'red',\n                     hist_kws={'edgecolor':'black'})","0ced8be0":"plt.subplots(figsize=(10,8))\nsns.heatmap(df[se_features].corr(),cmap='coolwarm',annot=True);","cc3e1fdb":"worst_features = [\"radius_worst\", \"texture_worst\", \"perimeter_worst\", \"area_worst\", \"smoothness_worst\", \"compactness_worst\", \"concavity_worst\", \n                 \"symmetry_worst\", \"fractal_dimension_worst\"]\n\nfor column in worst_features:\n    plt.figure(figsize = (10,5))\n    feature = df[column]\n    sns.distplot(feature, hist=True, kde=True, \n                    bins=int(180\/5), color = 'yellow',\n                     hist_kws={'edgecolor':'black'});\n\n","480f3c1d":"plt.subplots(figsize=(10,8))\nsns.heatmap(df[worst_features].corr(),cmap='coolwarm',annot=True);","74597fba":"for x_column in mean_features:\n    for y_cloumn in se_features:\n        sns.jointplot(x= x_column,y=y_cloumn,data=df,kind='scatter');","454f117e":"for x_column in mean_features:\n    for y_cloumn in worst_features:\n        sns.jointplot(x= x_column,y=y_cloumn,data=df,kind='kde');","4558b177":"for x_column in se_features:\n    for y_cloumn in worst_features:\n        sns.jointplot(x= x_column,y=y_cloumn,data=df,kind='hex');","736af522":"data = pd.concat([df.diagnosis,df[mean_features]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(20,10))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data);\n","0eb68f63":"data = pd.concat([df.diagnosis,df[se_features]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(20,10))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data);","21f5abe6":"data = pd.concat([df.diagnosis,df[worst_features]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(20,10))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data);","efab0d9e":"# We have target feature = 'diagnosis' in order to classification so we split this feature\n\ny = df.diagnosis \nx = df.drop('diagnosis', axis = 1)","0b90bf4e":"ax = sns.countplot(y,label=\"Count\") \nB, M = y.value_counts()\nprint('Number of Benign: ',B)\nprint('Number of Malignant : ',M)","a524697b":"x.describe()","566c82a7":"y = df.diagnosis.values\nx_df = df.drop([\"diagnosis\"],axis=1)","9852d27b":"# X Dataframe should be normalized to avoid dominance among numerical values because it has several features and \n#model success becomes more realistic if numbers are drawn between 0-1.\n\nx = (x_df - np.min(x_df))\/(np.max(x_df)-np.min(x_df)).values           # Formula of normalization","0d7415b2":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=42)\n","7db12e94":"print(\"x_train: \",x_train.shape)\nprint(\"x_test: \",x_test.shape)\nprint(\"y_train: \",y_train.shape)\nprint(\"y_test: \",y_test.shape)","1a7620b9":"# Transforming arrays to transpoze in order to avoid getting shape error\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T","22eb078e":"print(\"x_train: \",x_train.shape)\nprint(\"x_test: \",x_test.shape)\nprint(\"y_train: \",y_train.shape)\nprint(\"y_test: \",y_test.shape)","64ad8c4b":"name_list = ['Logistic Regression', 'Knn(n=8)', 'SVM', 'Native Bayes','Decision Tree', 'Random Foerest']\npred_score = []    # In order to compare all pred scores","5827aa95":"# We determite to parameter initialize values for sigmoid function \n# Giving dimension value to funciton and change weight and bias values each step\n\ndef change_weights_and_bias(dimension):\n    \n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w,b\n","2e614423":"# Creating sigmoid function \n\ndef sigmoid(z):\n    \n    y_head = 1\/(1+ np.exp(-z))    # Sigmoid function formula\n    \n    return y_head\n","b1292335":"# Making forward and backward iteration formula functions and changing values each step by step\n# We need x_train, y_train and z, loss, cost formula \n# Backward interation contains derivate weight and bias values step by step and return this values with gradients dictionary \n# Evaluating cost and gradients value after these interations\n\ndef forward_backward(w,b,x_train,y_train):\n    \n    z = np.dot(w.T,x_train) + b   #  Sigmoid function formula\n    y_head = sigmoid(z)           # Creating pred values to return sigmoid funciton\n    \n    loss = ((-1)* y_train * np.log(y_head)) + ((-1) * (1 - y_train) * np.log(1- y_head))    # Evaluating loss value within this formula \n    \n    #loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    \n    cost = (np.sum(loss))\/x_train.shape[1]                         # Evaluating cost value within this formula \n\n\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1] \n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]\n    \n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    \n    return cost,gradients\n","41f1377f":"# Each step, updating learning parametres automatically\n# Giving learning rate and number of interation value because they are hyperparametre\n# Saving cost values in cost_list after confuse cost values step by step\n\ndef update(w, b, x_train, y_train, learning_rate, number_iteration):\n    \n    cost_list = []\n    \n    for i in range(number_iteration):\n        \n        cost,gradients = forward_backward(w,b,x_train,y_train)   # Calling previous function\n        cost_list.append(cost)\n        \n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        \n        # Updating values \n        \n    parameters = {\"weight\": w,\"bias\": b}\n    \n    # Show all of them\n    \n    plt.plot(index,cost_list)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    \n    return parameters, gradients, cost_list\n","ded947b1":"# Creating predict values and need weight, bias and x_test value\n# x_test is a input for forward iteration\n# Using sigmoid functions : if z is bigger than 0.5, y_head = 1 or if z is smaller than 0.5, y_head = 0 \n\ndef predict(w,b,x_test):\n\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    y_prediction = np.zeros((1,x_test.shape[1])),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            y_prediction[0,i] = 0\n        else:\n            y_prediction[0,i] = 1\n\n    return y_prediction","73aca614":"# We use all funcitons for logistic regression \n# We need x and y splits, learning rate and number of iterations\n# Initializing value of dimension is x_train.shape[0] \n\ndef logistic_regression(x_train, y_train, x_test, y_test, learning_rate, number_iterations):\n    # initialize\n    dimension =  x_train.shape[0]  # that is 30\n    w,b = change_weights_and_bias(dimension)     # Calling change of this values funciton\n    \n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,number_iterations)  # Calling update function\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)  # Calling predict function\n    \n    # Print accuracy value \n    \n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))","1132460e":"# Then use logistic regression funciton for example learning rate is 1 and number of iterations are 250\n\n\n# logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, number_iterations = 250) ","bf6b07cf":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()","10624c8d":"lr.fit(x_train.T,y_train.T)\n\nprint(\"accuracy of logistic regression is:  {}\".format(lr.score(x_test.T,y_test.T)))\n\npred_score.append(lr.score(x_test.T,y_test.T))\n","c43ace1c":"df = pd.read_csv(\"..\/input\/breast-cancer-wisconsin-data\/data.csv\", delimiter=',')\ndf.drop([\"id\",\"Unnamed: 32\"],axis=1,inplace=True)\n\ndf.tail(10)   # Controlling last 10 rows","f80054b7":"df.info()","b4680820":"df.describe()","f0ef3ba3":"# Target Feature is diagnosis wthin M and B class\n\nM = df[df.diagnosis == \"M\"]\nB = df[df.diagnosis == \"B\"]\n\n","c3aaf556":"plt.scatter(M.perimeter_mean,M.texture_mean,color=\"blue\",label=\"M class diagnosis\")\nplt.scatter(B.radius_mean,B.texture_mean,color=\"red\",label=\"B class diagnosis\")\n\nplt.xlabel(\"radius_mean\")\nplt.ylabel(\"perimeter_mean\")\nplt.legend()\n\nplt.show()","8517dffb":"plt.scatter(M.radius_mean,M.area_mean,color=\"green\",label=\"M class diagnosis\")\nplt.scatter(B.radius_mean,B.area_mean,color=\"yellow\",label=\"B class diagnosis\")\n\nplt.xlabel(\"radius_mean\")\nplt.ylabel(\"area_mean\")\nplt.legend()\n\nplt.show()","28ac9573":"plt.scatter(M.smoothness_mean,M.compactness_mean,color=\"cyan\",label=\"M class diagnosis\")\nplt.scatter(B.smoothness_mean,B.compactness_mean,color=\"black\",label=\"B class diagnosis\")\n\nplt.xlabel(\"smoothness_mean\")\nplt.ylabel(\"compactness_mean\")\nplt.legend()\n\nplt.show()","3dbaa945":"sns.jointplot(x='smoothness_mean',y='compactness_mean',data=df,kind='scatter');","92f45106":"sns.jointplot(x='radius_mean',y='perimeter_mean',data=df,kind='scatter');","a8d3bd6b":"df.corr()","49500c28":"sns.heatmap(df.corr());\n","1b778197":"# We assign 1 value if diagnosis is M and we assing 0 value if diagnosis is B  \n\ndf.diagnosis = [1 if each == \"M\" else 0 for each in df.diagnosis]\n\ny = df.diagnosis.values\nx_df = df.drop([\"diagnosis\"],axis=1)","d90cf399":"# normalization \nx = (x_df - np.min(x_df))\/(np.max(x_df)-np.min(x_df))","68d84b7e":"# Train and test split\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state=1)\n","49428373":"# Using KNN Model\n\nfrom sklearn.neighbors import KNeighborsClassifier\nn_neighbors = 5  # as an example\n\nknn = KNeighborsClassifier(n_neighbors = n_neighbors) # n_neighbors = k\n\nknn.fit(x_train,y_train)\n\nprediction = knn.predict(x_test)\nprint(\" {} nn score: {} \".format(n_neighbors, knn.score(x_test,y_test)))\n","c595dee0":"# Using KNN Model\n\nn_neighbors = 8  # as an example\n\nknn = KNeighborsClassifier(n_neighbors = n_neighbors) # n_neighbors = k\n\nknn.fit(x_train,y_train)\n\nprediction = knn.predict(x_test)\nprint(\" {} nn score: {} \".format(n_neighbors, knn.score(x_test,y_test)))\n\npred_score.append(knn.score(x_test,y_test))\n","d87ad5eb":"# Finding optimal  k value\n\nscore_list = []\n\nfor each in range(1,15):\n    knn2 = KNeighborsClassifier(n_neighbors = each)\n    knn2.fit(x_train,y_train)\n    score_list.append(knn2.score(x_test,y_test))\n    \nplt.plot(range(1,15),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()","14feba00":"from sklearn.svm import SVC\n \nsvm = SVC(random_state = 1)\nsvm.fit(x_train,y_train)\n \nprint(\"accuracy of svm value is: \",svm.score(x_test,y_test))\n\npred_score.append(svm.score(x_test,y_test))\n","dd001b52":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\n \nprint(\"accuracy of naive bayes : \",nb.score(x_test,y_test))\n\npred_score.append(nb.score(x_test,y_test))","2d26837d":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\n\nprint(\"accuracy of decision tree: \", dt.score(x_test,y_test))\n\npred_score.append(dt.score(x_test,y_test))\n\n","6b902c94":"from sklearn.ensemble import RandomForestClassifier\n\nn_estimators = 100  # as an example number of trees\n\nrf = RandomForestClassifier(n_estimators = n_estimators,random_state = 1)\n\nrf.fit(x_train,y_train)\n\nprint(\"accuracy of random forest (100): \",rf.score(x_test,y_test))\n\npred_score.append(rf.score(x_test,y_test))","e1796c7b":"y_pred = rf.predict(x_test)\ny_true = y_test","b54a06a9":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\n\n# visualization with heatmap\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,fmt = \".0f\",ax = ax)\n\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()\n","32c52896":"accuracy = pd.DataFrame({'algorithmas' : name_list, 'accuracy_value': pred_score})\n\nplt.figure(figsize=(12,6))\n\nplt.plot(accuracy.algorithmas,accuracy.accuracy_value)\n\nplt.title(\"Comparison of Accuracy Values\")\n\nplt.show()","daa30b10":"## 6. Decision Tree","f8dbe221":"## 4. Support Vector Machines ","9bd1ba85":"##### As an example Random Forests predictions values","b7e85ada":"## Data Analysis \/ EDA","87fa9085":"##### We change a lo of things on datasets so again importing original version dataset and using KNN algorithm","2216e35a":"#####  As you seen, optimal k values to 6 from 12. \n","2c5f4030":"## 7. Random Forest","b1033f05":"## Import libraries","2b49b3cb":"## 5. Native Bayes Classification","33281225":"## 3. KNN Algorithm and Visualization","3253f4eb":"## 2. Second Way Logistic Regression with SKLearn","f15b17b7":"### Train and test split ","83980708":"## 1. First Way to Logistic Regresion Using by Deep Learning Formulas","e4a6bd63":"## Evaluating Confusion Matrix"}}