{"cell_type":{"91ff26f3":"code","fe5eb90f":"code","ac7a8372":"code","dc732398":"code","19493924":"code","4923651d":"code","06fd7a43":"code","c8013f98":"code","5c47adb2":"code","a71e8248":"code","b8dc3c01":"markdown","602590a8":"markdown","92371223":"markdown","7988a0a3":"markdown","5e3c0bcb":"markdown","0a12d3d9":"markdown","e6a460f5":"markdown","e7917079":"markdown","6c2d1261":"markdown","750e4261":"markdown","92e8be02":"markdown","fb18635c":"markdown","76b7a755":"markdown","e3de444c":"markdown","9f01e059":"markdown","c93c09a7":"markdown","3819fcb4":"markdown","17af0cee":"markdown","8758cb6c":"markdown","ac5ee8a6":"markdown"},"source":{"91ff26f3":"# Dependencies\n\n!pip install --no-deps '..\/input\/timm-package\/timm-0.1.26-py3-none-any.whl' > \/dev\/null\n!pip install --no-deps '..\/input\/pycocotools\/pycocotools-2.0-cp37-cp37m-linux_x86_64.whl' > \/dev\/null\n\n\n\n\n\nimport sys\nsys.path.insert(0, \"..\/input\/timm-efficientdet-pytorch\")\nsys.path.insert(0, \"..\/input\/omegaconf\")\nsys.path.insert(0, \"..\/input\/weightedboxesfusion\")\n\n\n\nimport ensemble_boxes\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom glob import glob\nfrom torch.utils.data import Dataset,DataLoader\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nimport cv2\nimport gc\nfrom matplotlib import pyplot as plt\nfrom effdet import get_efficientdet_config, EfficientDet, DetBenchEval\nfrom effdet.efficientdet import HeadNet\n\n\n\n\ndef get_valid_transforms():\n    return A.Compose([\n            A.Resize(height=512, width=512, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.0)\n\n\n\n\nDATA_ROOT_PATH = '..\/input\/global-wheat-detection\/test'\n\n\n\n\nclass DatasetRetriever(Dataset):\n\n    def __init__(self, image_ids, transforms=None):\n        super().__init__()\n        self.image_ids = image_ids\n        self.transforms = transforms\n\n    def __getitem__(self, index: int):\n        image_id = self.image_ids[index]\n        image = cv2.imread(f'{DATA_ROOT_PATH}\/{image_id}.jpg', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0\n        image_size, width, _ = image.shape\n        if self.transforms:\n            sample = {'image': image}\n            sample = self.transforms(**sample)\n            image = sample['image']\n        return image, image_id, image_size\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]\n    \n    \n    \n    \n    \ndataset = DatasetRetriever(\n    image_ids=np.array([path.split('\/')[-1][:-4] for path in glob(f'{DATA_ROOT_PATH}\/*.jpg')]),\n    transforms=get_valid_transforms()\n)\n\n\n\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\n\n\n\n\ndata_loader = DataLoader(\n    dataset,\n    batch_size=4,\n    shuffle=False,\n    num_workers=2,\n    drop_last=False,\n    collate_fn=collate_fn\n)\n\n\n\n\n'''\ndef load_net(checkpoint_path):\n    config = get_efficientdet_config('tf_efficientdet_d5')\n    net = EfficientDet(config, pretrained_backbone=False)\n\n    config.num_classes = 1\n    config.image_size=512\n    net.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n\n    checkpoint = torch.load(checkpoint_path)\n    net.load_state_dict(checkpoint['model_state_dict'])\n\n    del checkpoint\n    gc.collect()\n\n    net = DetBenchEval(net, config)\n    net.eval();\n    return net.cuda()\n\n\n\n\nnet = load_net('..\/input\/efficientdet5f0\/best-checkpoint-028epoch.bin')\n'''\n\n\n\n\nclass BaseWheatTTA:\n    \"\"\" author: @shonenkov \"\"\"\n    image_size = 512\n\n    def augment(self, image):\n        raise NotImplementedError\n    \n    def batch_augment(self, images):\n        raise NotImplementedError\n    \n    def deaugment_boxes(self, boxes):\n        raise NotImplementedError\n\nclass TTAContrastBrightness(BaseWheatTTA):\n    \"\"\" author: @ffares \"\"\"\n\n    def augment(self, image):\n        alpha = round(random.uniform(1, 1.3),2) # Contrast control\/\/ alpha value [1.0-3.0]\n        beta = round(random.uniform(0, 0.2),2)  # Brightness control\/\/ beta value [0-100]\n        return torch.clamp(torch.add(torch.mul(image,alpha),beta),0, 1, out=None)\n    \n    def batch_augment(self, images):\n        alpha = round(random.uniform(1, 1.3),2) # Contrast control\/\/ alpha value [1.0-3.0]\n        beta = round(random.uniform(0, 0.2),2)  # Brightness control\/\/ beta value [0-100]\n        return torch.clamp(torch.add(torch.mul(images,alpha),beta),0, 255, out=None)\n    \n    def deaugment_boxes(self, boxes):\n        return boxes\n        \nclass TTAHorizontalFlip(BaseWheatTTA):\n    \"\"\" author: @shonenkov \"\"\"\n\n    def augment(self, image):\n        return image.flip(1)\n    \n    def batch_augment(self, images):\n        return images.flip(2)\n    \n    def deaugment_boxes(self, boxes):\n        boxes[:, [1,3]] = self.image_size - boxes[:, [3,1]]\n        return boxes\n\nclass TTAVerticalFlip(BaseWheatTTA):\n    \"\"\" author: @shonenkov \"\"\"\n    \n    def augment(self, image):\n        return image.flip(2)\n    \n    def batch_augment(self, images):\n        return images.flip(3)\n    \n    def deaugment_boxes(self, boxes):\n        boxes[:, [0,2]] = self.image_size - boxes[:, [2,0]]\n        return boxes\n    \nclass TTARotate90(BaseWheatTTA):\n    \"\"\" author: @shonenkov \"\"\"\n    \n    def augment(self, image):\n        return torch.rot90(image, 1, (1, 2))\n\n    def batch_augment(self, images):\n        return torch.rot90(images, 1, (2, 3))\n    \n    def deaugment_boxes(self, boxes):\n        res_boxes = boxes.copy()\n        res_boxes[:, [0,2]] = self.image_size - boxes[:, [1,3]]\n        res_boxes[:, [1,3]] = boxes[:, [2,0]]\n        return res_boxes\n    \nclass TTARotate180(BaseWheatTTA):\n    \"\"\" author: @shonenkov \"\"\"\n    \n    def augment(self, image):\n        return torch.rot90(image, 2, (1, 2))\n\n    def batch_augment(self, images):\n        return torch.rot90(images, 2, (2, 3))\n    \n    def deaugment_boxes(self, boxes):\n        boxes[:, [0,1,2,3]] = self.image_size - boxes[:, [2,3,0,1]]\n        return boxes\n    \nclass TTARotate270(BaseWheatTTA):\n    \"\"\" author: @shonenkov \"\"\"\n    \n    def augment(self, image):\n        return torch.rot90(image, 3, (1, 2))\n\n    def batch_augment(self, images):\n        return torch.rot90(images, 3, (2, 3))\n    \n    def deaugment_boxes(self, boxes):\n        res_boxes = boxes.copy()\n        res_boxes[:, [0,2]] = boxes[:, [1,3]]\n        res_boxes[:, [1,3]] = self.image_size - boxes[:, [2,0]]\n        return res_boxes\n    \nclass TTACompose(BaseWheatTTA):\n    \"\"\" author: @shonenkov \"\"\"\n    def __init__(self, transforms):\n        self.transforms = transforms\n        \n    def augment(self, image):\n        for transform in self.transforms:\n            image = transform.augment(image)\n        return image\n    \n    def batch_augment(self, images):\n        for transform in self.transforms:\n            images = transform.batch_augment(images)\n        return images\n    \n    def prepare_boxes(self, boxes):\n        result_boxes = boxes.copy()\n        result_boxes[:,0] = np.min(boxes[:, [0,2]], axis=1)\n        result_boxes[:,2] = np.max(boxes[:, [0,2]], axis=1)\n        result_boxes[:,1] = np.min(boxes[:, [1,3]], axis=1)\n        result_boxes[:,3] = np.max(boxes[:, [1,3]], axis=1)\n        return result_boxes\n    \n    def deaugment_boxes(self, boxes):\n        for transform in self.transforms[::-1]:\n            boxes = transform.deaugment_boxes(boxes)\n        return self.prepare_boxes(boxes)\n    \n    \n    \nfrom itertools import product\n\ntta_transforms = []\nfor tta_combination in product([TTAHorizontalFlip(), None], \n                               [TTAVerticalFlip(), None],\n                               [TTARotate90(), None]):\n    tta_transforms.append(TTACompose([tta_transform for tta_transform in tta_combination if tta_transform]))\n    \n\ndef load_net5(checkpoint_path):\n    config = get_efficientdet_config('tf_efficientdet_d5')\n    net = EfficientDet(config, pretrained_backbone=False)\n\n    config.num_classes = 1\n    config.image_size=512\n    net.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n\n    checkpoint = torch.load(checkpoint_path)\n    net.load_state_dict(checkpoint['model_state_dict'])\n\n    del checkpoint\n    gc.collect()\n\n    net = DetBenchEval(net, config)\n    net.eval();\n    return net.cuda()\n\ndef load_net7(checkpoint_path):\n    config = get_efficientdet_config('tf_efficientdet_d7')\n    net = EfficientDet(config, pretrained_backbone=False)\n\n    config.num_classes = 1\n    config.image_size=512\n    net.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n\n    checkpoint = torch.load(checkpoint_path)\n    net.load_state_dict(checkpoint['model_state_dict'])\n\n    del checkpoint\n    gc.collect()\n\n    net = DetBenchEval(net, config)\n    net.eval();\n    return net.cuda()\n\n","fe5eb90f":"# Models\nmodels =[    \n    #With Cleaning\/ Old Version fold 0\n    load_net5('..\/input\/efficientdetearlierversionf0\/best-checkpoint-047epoch.bin'), \n    #With Cleaning\/ New Version fold 0\n    load_net5('..\/input\/efficientdet5f0\/best-checkpoint-028epoch.bin'), \n    #With CLEANING fold 1 fine tuned on half arvalis\n    #load_net5('..\/input\/efficientdet5f1finetunearv2\/best-checkpoint-001epoch.bin'), \n    #Without Cleaning fold 3\n    #load_net7('..\/input\/training-efficientdet-f3\/effdet5-cutmix-augmix\/best-checkpoint-033epoch.bin'),\n    #Without Cleaning fold 4\n    load_net7('..\/input\/training-efficientdet-f4\/effdet5-cutmix-augmix\/best-checkpoint-037epoch.bin'), \n]\n    \n\ndef make_predictions(images, score_threshold=0.1):\n    images = torch.stack(images).cuda().float()\n    predictions = []\n    with torch.no_grad():\n        det = net(images, torch.tensor([1]*images.shape[0]).float().cuda())\n        for i in range(images.shape[0]):\n            boxes = det[i].detach().cpu().numpy()[:,:4]    \n            scores = det[i].detach().cpu().numpy()[:,4]\n            indexes = np.where(scores > score_threshold)[0]\n            boxes = boxes[indexes]\n            boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n            boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n            predictions.append({\n                'boxes': boxes[indexes],\n                'scores': scores[indexes],\n            })\n    return [predictions]\n  \n    \ndef make_tta_predictions(images, score_threshold=0.25):\n    with torch.no_grad():\n        images = torch.stack(images).float().cuda()\n        predictions = []\n        for tta_transform in tta_transforms:\n            result = []\n            det = net(tta_transform.batch_augment(images.clone()), torch.tensor([1]*images.shape[0]).float().cuda())\n\n            for i in range(images.shape[0]):\n                boxes = det[i].detach().cpu().numpy()[:,:4]    \n                scores = det[i].detach().cpu().numpy()[:,4]\n                indexes = np.where(scores > score_threshold)[0]\n                boxes = boxes[indexes]\n                boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n                boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n                boxes = tta_transform.deaugment_boxes(boxes.copy())\n                result.append({\n                    'boxes': boxes,\n                    'scores': scores[indexes],\n                })\n            predictions.append(result)\n    return predictions\n\n\ndef make_tta_models_predictions(images, score_threshold=0.25):\n    images = torch.stack(images).float().cuda()\n    with torch.no_grad():\n        predictions = []\n        for tta_transform in tta_transforms:\n            for net in models:\n                result = []\n                det = net(tta_transform.batch_augment(images.clone()), torch.tensor([1]*images.shape[0]).float().cuda())\n                for i in range(images.shape[0]):\n                    boxes = det[i].detach().cpu().numpy()[:,:4]    \n                    scores = det[i].detach().cpu().numpy()[:,4]\n                    indexes = np.where(scores > score_threshold)[0]\n                    boxes = boxes[indexes]\n                    boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n                    boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n                    boxes = tta_transform.deaugment_boxes(boxes.copy())\n                    \n                    result.append({\n                        'boxes': boxes,\n                        'scores': scores[indexes],\n                    })\n                predictions.append(result)\n    return predictions\n\ndef make_models_predictions(images, score_threshold=0.1):\n    images = torch.stack(images).cuda().float()\n    predictions = []\n    for net in models:\n        with torch.no_grad():\n            det = net(images, torch.tensor([1]*images.shape[0]).float().cuda())\n            result = []\n            for i in range(images.shape[0]):\n                boxes = det[i].detach().cpu().numpy()[:,:4]    \n                scores = det[i].detach().cpu().numpy()[:,4]\n                indexes = np.where(scores > score_threshold)[0]\n                boxes = boxes[indexes]\n                boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n                boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n                result.append({\n                    'boxes': boxes[indexes],\n                    'scores': scores[indexes],\n                })\n            predictions.append(result)\n    return predictions\n\n\n# Inference \ndef format_prediction_string(boxes, scores):\n    pred_strings = []\n    for j in zip(scores, boxes):\n        pred_strings.append(\"{0:.4f} {1} {2} {3} {4}\".format(j[0], j[1][0], j[1][1], j[1][2], j[1][3]))\n    return \" \".join(pred_strings)","ac7a8372":"def calibrate(liste, conf_max=0.91):\n    '''\n    This method is essential to callibrate the bounding boxes confidence of an algorithm\n    This is important to make ensembling more efficient\n    Input: List of probablities with maximimum p_max\n    Output: List of probablities with maximum conf_max\n    '''\n    m=max(liste)\n\n    if m<conf_max: \n        alpha=conf_max-m\n\n        for i in range(len(liste)):\n            liste[i]+=alpha\n    \n    return liste\n\n\ndef normalize(liste):\n    '''\n    This method is essential to normalize the bounding boxes confidence of an algorithm\n    This is important to make ensembling more efficient\n    Input: List of initial probablities \n    Output: List of probablities normalized\n    '''\n    maximum = max(liste)\n    minimum = min(liste)\n    \n    for i in range(len(liste)):\n        liste[i]= (liste[i]-minimum) \/ (maximum-minimum) \n\n    return liste\n","dc732398":"\ndef resize_predicitions(predictions): \n    '''\n    Resize bboxes of efficient det as it predicts bboxes in the range of 512 \n    We need to double the predicitions\n    '''\n    for i in range(len(predictions)):\n        for j in range(len(predictions[i])):\n            predictions[i][j]['boxes']=predictions[i][j]['boxes']*2\n        \n    return predictions\n\n\ndef effdet_organize(predictions, image_index):\n    \n    boxes = [(prediction[image_index]['boxes']).tolist() for prediction in predictions]\n    scores = [prediction[image_index]['scores'].tolist() for prediction in predictions]\n    labels = [np.ones(prediction[image_index]['scores'].shape[0]).astype(int).tolist() for prediction in predictions]\n    \n    if 0:\n        for i in range(len(scores)): \n            scores[i]=normalize(scores[i])\n        \n    return boxes, scores, labels\n\n\ndef yolo_organize(boxes,scores):\n    \n    boxes_output=[]\n    for i in range(len(boxes[0])):\n        boxes_output.append(boxes[0][i].tolist())\n    \n    scores_output=[]\n    for i in range(len(scores[0])):\n        scores_output.append(scores[0][i].cpu().float().item())   \n        \n    # Calibrate if 1    \n    if 0:\n        scores_output = calibrate(scores_output)\n        \n    # Normalize if 1\n    if 0:\n        scores_output = normalize(scores_output)\n    \n    labels_output = [1]*len(scores_output)\n    \n    return [boxes_output], [scores_output], [labels_output]\n\ndef run_wbf_ensemble(boxes_effdet, boxes_yolo, scores_effdet, scores_yolo, labels_effdet, labels_yolo, iou_thr=0.6, skip_box_thr=0.5, weights=None):    \n        \n    boxes = boxes_effdet + boxes_yolo\n    scores = scores_effdet + scores_yolo\n    labels = labels_effdet + labels_yolo\n    \n    boxes, scores, labels = ensemble_boxes.ensemble_boxes_wbf.weighted_boxes_fusion(boxes, scores, labels, weights=weights, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n    return boxes, scores, labels\n\n\n\ndef run_wbf(boxes, scores, labels, iou_thr=0.5, skip_box_thr=0.3, weights=None):    \n    boxes, scores, labels = ensemble_boxes.ensemble_boxes_wbf.weighted_boxes_fusion(boxes, scores, labels, weights=weights, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n    return boxes, scores, labels  \n\n\ndef run_wbf_initial(boxes,scores, image_size=1024, iou_thr=0.41, skip_box_thr=0.4, weights=None):\n    labels0 = [np.ones(len(scores[idx])) for idx in range(len(scores))]\n    boxes, scores, labels = ensemble_boxes.ensemble_boxes_wbf.weighted_boxes_fusion(boxes, scores, labels0, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n    return boxes, scores, labels","19493924":"for images, image_ids, image_size in data_loader:\n    \n    #predictions = make_tta_predictions(images)\n    predictions = make_tta_models_predictions(images)\n    #predictions = make_predictions(images)\n    predictions=resize_predicitions(predictions)\n\n    for i, image in enumerate(images):\n        \n        image_id=image_ids[i]\n\n        \n        # boxes using efficientdet5\n        boxes_effdet, scores_effdet, labels_effdet = effdet_organize(predictions, image_index=i)\n        \n        boxes, scores, labels = run_wbf(boxes_effdet, scores_effdet, labels_effdet, iou_thr=0.6, skip_box_thr=0.5, weights=None)\n\n        boxes = (boxes\/2).round().astype(np.int32).clip(min=0, max=512)\n\n    \n    \n        fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n    \n    \n        sample = images[i].permute(1,2,0).cpu().numpy()\n    \n        font = cv2.FONT_HERSHEY_SIMPLEX \n    \n        # fontScale \n        fontScale = 1\n\n        # Blue color in RGB \n        color = (0, 0, 1) \n\n        # Line thickness of 2 px \n        thickness = 2\n\n        for box,score in zip(boxes,scores):\n            cv2.rectangle(sample, (box[0], box[1]), (box[2], box[3]), (0, 0, 1), 1)\n            cv2.putText(sample, '{:.2}'.format(score), (box[0]+np.random.randint(20),box[1]), font, fontScale, color, thickness, cv2.LINE_AA)\n    \n        ax.set_axis_off()\n        ax.imshow(sample);\n    \n        break","4923651d":"sys.path.insert(0, \"..\/input\/yolov5tta\/\")\nsys.path.insert(0, \"..\/input\/configyolov5\")\n#sys.path.insert(0, \"..\/input\/yolov5\/\")\n\nimport sys\nimport glob\nimport argparse\nfrom utils.datasets import *\nfrom utils.utils import *\n\n\n\ndef detect(save_img=False):\n    weights, imgsz = opt.weights,opt.img_size\n    source = '..\/input\/global-wheat-detection\/test\/'\n    \n    # Initialize\n    device = torch_utils.select_device(opt.device)\n    half = True\n    # Load model\n\n    model = torch.load(weights, map_location=device)['model'].to(device).eval()\n\n    dataset = LoadImages(source, img_size=1024)\n\n    t0 = time.time()\n    img = torch.zeros((1, 3, imgsz, imgsz), device=device)  # init img\n    all_path=[]\n    all_bboxex =[]\n    all_score =[]\n    for path, img, im0s, vid_cap in dataset:\n        print(im0s.shape)\n        img = torch.from_numpy(img).to(device)\n        img = img.half() if half else img.float()  # uint8 to fp16\/32\n        img \/= 255.0  # 0 - 255 to 0.0 - 1.0\n        if img.ndimension() == 3:\n            img = img.unsqueeze(0)\n\n        # Inference\n        t1 = torch_utils.time_synchronized()\n        bboxes_2 = []\n        score_2 = []\n        if True:\n            pred = model(img, augment=opt.augment)[0]\n            \n            #Skip this?\n            pred = non_max_suppression(pred, opt.conf_thres, opt.iou_thres,merge=True, classes=None, agnostic=False)\n            \n            t2 = torch_utils.time_synchronized()\n\n            bboxes = []\n            score = []\n            # Process detections\n            for i, det in enumerate(pred):  # detections per image\n                p, s, im0 = path, '', im0s\n                gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  #  normalization gain whwh\n                if det is not None and len(det):\n                    det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n                    for c in det[:, -1].unique():\n                        n = (det[:, -1] == c).sum()  # detections per class\n\n                    for *xyxy, conf, cls in det:\n                        if True:  # Write to file\n                            xywh = torch.tensor(xyxy).view(-1).numpy()  \n                            bboxes.append(xywh)\n                            score.append(conf)\n                            \n            bboxes_2.append(bboxes)\n            \n            #add calibration while append\n            #score_2.append(calibrate(score))\n            score_2.append(score)\n            \n        all_path.append(path)\n        all_score.append(score_2)\n        all_bboxex.append(bboxes_2)\n    return all_path,all_score,all_bboxex\n\n\n\nif __name__ == '__main__':\n    class opt:\n        weights = \"..\/input\/train-continuity-yolov5x\/weights\/best_yolov5_fold0.pt\"\n        img_size = 1024\n        conf_thres = 0.3\n        iou_thres = 0.94 #0.94\n        augment = True\n        device = '0'\n        classes=None\n        agnostic_nms = True\n        \n    opt.img_size = check_img_size(opt.img_size)\n    #print(opt)\n\n    with torch.no_grad():\n        res = detect()\n        \n\nall_path,all_score,all_bboxex = res","06fd7a43":"for images, image_ids, image_sizes in data_loader:\n    \n    #predictions = make_tta_predictions(images)\n    predictions = make_tta_models_predictions(images)\n    #predictions = make_predictions(images)\n    predictions=resize_predicitions(predictions)\n\n    for i, image in enumerate(images):\n        \n        image_id=image_ids[i]\n        \n\n        \n        # boxes using yolov5\n        for row in range(len(all_path)):\n            image_id_yolo = all_path[row].split(\"\/\")[-1].split(\".\")[0]\n            if image_id_yolo==image_id:\n                boxes_yolo = all_bboxex[row]\n                scores_yolo = all_score[row]\n                \n            \n                boxes_yolo, scores_yolo, labels_yolo = yolo_organize(boxes_yolo,scores_yolo)\n\n        \n        boxes, scores, labels =run_wbf(boxes_yolo, scores_yolo, labels_yolo, iou_thr=0.41, skip_box_thr=0.4, weights=None)\n\n        boxes = (boxes\/2).round().astype(np.int32).clip(min=0, max=512)\n\n    \n    \n        fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n    \n    \n        sample = images[i].permute(1,2,0).cpu().numpy()\n    \n        font = cv2.FONT_HERSHEY_SIMPLEX \n    \n        # fontScale \n        fontScale = 1\n\n        # Blue color in RGB \n        color = (0, 0, 1) \n\n        # Line thickness of 2 px \n        thickness = 2\n\n        for box,score in zip(boxes,scores):\n            cv2.rectangle(sample, (box[0], box[1]), (box[2], box[3]), (0, 0, 1), 1)\n            cv2.putText(sample, '{:.2}'.format(score), (box[0]+np.random.randint(20),box[1]), font, fontScale, color, thickness, cv2.LINE_AA)\n    \n        ax.set_axis_off()\n        ax.imshow(sample);\n    \n        break","c8013f98":"results = []\n\nfor images, image_ids, image_sizes in data_loader:\n    #predictions = make_tta_predictions(images)\n    predictions = make_tta_models_predictions(images)\n    #predictions = make_models_predictions(images)\n    #predictions = make_predictions(images)\n    predictions=resize_predicitions(predictions)\n    \n    for i, image in enumerate(images):\n        \n        image_id=image_ids[i]\n        image_size= image_sizes[i]\n        \n        # YOLOv5 predictions\n        for row in range(len(all_path)):\n            image_id_yolo = all_path[row].split(\"\/\")[-1].split(\".\")[0]\n            if image_id_yolo==image_id:\n                boxes_yolo_init = all_bboxex[row]\n                scores_yolo_init = all_score[row]\n                \n                boxes_yolo, scores_yolo, labels_yolo = yolo_organize(boxes_yolo_init,scores_yolo_init)\n\n                \n        \n        # EfficientDet predictions\n        boxes_effdet, scores_effdet, labels_effdet = effdet_organize(predictions, image_index=i)\n        \n        \n        # Fusion of both predictions\n        boxes, scores, labels = run_wbf_ensemble(boxes_effdet, boxes_yolo, scores_effdet, scores_yolo, labels_effdet, \n                                                 labels_yolo, iou_thr=0.41, skip_box_thr=0.4, weights=None)\n        \n        boxes = (boxes*(image_size\/1024)).round().astype(np.int32).clip(min=0, max=image_size-1)\n        \n        \n        \n        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n\n        result = {\n            'image_id': image_id,\n            'PredictionString': format_prediction_string(boxes, scores)\n        }\n        results.append(result)\n        ","5c47adb2":"test_df = pd.DataFrame(results, columns=['image_id', 'PredictionString'])\ntest_df.to_csv('submission.csv', index=False)\ntest_df.head(10)","a71e8248":"for images, image_ids, image_sizes in data_loader:\n    \n    #predictions = make_tta_predictions(images)\n    predictions = make_tta_models_predictions(images)\n    #predictions = make_predictions(images)\n    predictions=resize_predicitions(predictions)\n\n    for i, image in enumerate(images):\n        \n        image_id=image_ids[i]\n        \n        image_size= image_sizes[i]\n        \n        # boxes using yolov5\n        for row in range(len(all_path)):\n            image_id_yolo = all_path[row].split(\"\/\")[-1].split(\".\")[0]\n            if image_id_yolo==image_id:\n                boxes_yolo = all_bboxex[row]\n                scores_yolo = all_score[row]\n                \n            \n                boxes_yolo, scores_yolo, labels_yolo = yolo_organize(boxes_yolo,scores_yolo)\n\n        \n        # boxes using efficientdet5\n        boxes_effdet, scores_effdet, labels_effdet = effdet_organize(predictions, image_index=i)\n        \n        boxes, scores, labels = run_wbf_ensemble(boxes_effdet, boxes_yolo, scores_effdet, scores_yolo, labels_effdet, \n                                                 labels_yolo, iou_thr=0.41, skip_box_thr=0.4, weights=None)\n\n        boxes = (boxes\/2).round().astype(np.int32).clip(min=0, max=512)\n\n    \n    \n        fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n    \n    \n        sample = images[i].permute(1,2,0).cpu().numpy()\n    \n        font = cv2.FONT_HERSHEY_SIMPLEX \n    \n        # fontScale \n        fontScale = 1\n\n        # Blue color in RGB \n        color = (0, 0, 1) \n\n        # Line thickness of 2 px \n        thickness = 2\n\n        for box,score in zip(boxes,scores):\n            cv2.rectangle(sample, (box[0], box[1]), (box[2], box[3]), (0, 0, 1), 1)\n            cv2.putText(sample, '{:.2}'.format(score), (box[0]+np.random.randint(20),box[1]), font, fontScale, color, thickness, cv2.LINE_AA)\n    \n        ax.set_axis_off()\n        ax.imshow(sample);\n    \n        break","b8dc3c01":"## Most Important Hyperparameters \n\n* IOU Threshold \n\n* Confidence Threshold","602590a8":"## Inference","92371223":"# Refrences ","7988a0a3":"https:\/\/arxiv.org\/abs\/1911.09070\n\nhttps:\/\/github.com\/rwightman\/efficientdet-pytorch\n\nhttps:\/\/github.com\/rwightman\/pytorch-image-models\n\nhttps:\/\/arxiv.org\/pdf\/2004.10934\n\nhttps:\/\/github.com\/ultralytics\/yolov5\n\nhttps:\/\/arxiv.org\/abs\/1910.13302\n\nhttps:\/\/github.com\/ZFTurbo\/Weighted-Boxes-Fusion\n\nhttps:\/\/www.kaggle.com\/shonenkov\/inference-efficientdet\n\nhttps:\/\/www.kaggle.com\/wasupandceacar\/yolov5-single-model-with-more-tta-lb-0-745\n","5e3c0bcb":"![image.png](attachment:image.png)","0a12d3d9":"![image.png](attachment:image.png)","e6a460f5":"## YOLOv5 Comparaison","e7917079":"# Introduction","6c2d1261":"## Thank you for reading my kernel!","750e4261":"## WBF Ensemble EfficientDet & YOLOv5 ","92e8be02":"# EfficientDet5","fb18635c":"# Global Wheat Head Detection ","76b7a755":"# Why not ensemble both? ","e3de444c":"![image.png](attachment:image.png)","9f01e059":"## Ensemble different EfficientDet Models\n### Different architectures \n### Or, same architectures but trained on different folds & using different augmentations\n","c93c09a7":"## EfficientDet Architecture ","3819fcb4":"![image.png](attachment:image.png)","17af0cee":"# YOLOv5","8758cb6c":"Object detection is one of the most important topics of computer vision since it has many applications in several fields. One application of it is this amazing challenge.\n\nObject detection models can be improved thanks to ensemble techniques.\n\nHowever, the process of ensembling object detectors poses\nseveral challenges including the selection of models but most importantly the way of ensembling itself. Because different models have different output types with different confidence range which requires some 'work'. \n\nWe had the initiative to ensemble YOLOv5x model with EfficientDet5 and doing some hacks to make it optimal!","ac5ee8a6":"## EfficientDet ModelFlops vs COCO accuracy"}}