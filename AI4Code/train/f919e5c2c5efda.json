{"cell_type":{"3f8a6365":"code","acf1c2a1":"code","ba3f0762":"code","9f2aeb3a":"code","a668e138":"code","20b9c094":"code","1be243a7":"code","4b2a5e3e":"code","ac2c7502":"code","2bc5ea33":"code","b25b2bae":"code","7b180cc4":"code","8f337be9":"code","e1e61a36":"code","18c90acf":"code","c8145fb3":"markdown","6665eba3":"markdown","d9756c91":"markdown","7908c131":"markdown","bd82d09f":"markdown","c32ba45a":"markdown","f2f1b46f":"markdown","e0b4c549":"markdown","a12a181f":"markdown","7ec4052d":"markdown","18ce46aa":"markdown","5e529b16":"markdown"},"source":{"3f8a6365":"import os\nimport shutil\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Dense, Activation,Dropout, MaxPooling2D,BatchNormalization\nfrom tensorflow.keras.optimizers import Adam, Adamax\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model, load_model\nimport numpy as np\nimport time\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nimport logging\nlogging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\nprint ('modules loaded')","acf1c2a1":"img_path=r'..\/input\/chess-pieces-detection-images-dataset\/KnightImages\/00000001.jpg'\nimg=plt.imread(img_path)\nprint ('Image shape is: ', img.shape)\nplt.axis('off')\nplt.imshow(img)","ba3f0762":"sdir=r'..\/input\/chess-pieces-detection-images-dataset'\nclasslist=os.listdir(sdir)\nlabels=[]\nfilepaths=[]\nfor klass in classlist:\n    classpath=os.path.join(sdir, klass)\n    flist=os.listdir(classpath)\n    for f in flist:\n        fpath=os.path.join(classpath,f)\n        filepaths.append(fpath)\n        labels.append(klass)\nFseries=pd.Series(filepaths, name='filepaths')\nLseries = pd.Series(labels, name='labels')\ndf=pd.concat([Fseries, Lseries], axis=1)\nclass_count=len(list(df['labels'].unique()))\nprint('Number of classes in dataset is ', class_count)","9f2aeb3a":"train_df, dummy_df =train_test_split(df, train_size=.9, shuffle=True, random_state=123, stratify= df['labels'])\nvalid_df, test_df = train_test_split(dummy_df, train_size=.5, shuffle=True, random_state=123, stratify =dummy_df['labels'])\nprint('train_df length: ', len(train_df), '  test_df length: ',len(test_df), '  valid_df length: ', len(valid_df))\nprint (train_df['labels'].value_counts())","a668e138":"def balance(train_df,max_samples, min_samples, column, working_dir, image_size):\n    train_df=train_df.copy()        \n    # make directories to store augmented images\n    aug_dir=os.path.join(working_dir, 'aug')\n    if os.path.isdir(aug_dir): # check if directory exists, if it does remove it and create a fresh empty directory\n        shutil.rmtree(aug_dir)\n    os.mkdir(aug_dir)\n    for label in train_df['labels'].unique():    \n        dir_path=os.path.join(aug_dir,label) # make sub directories in aug directory, one for each unique label (class)   \n        os.mkdir(dir_path)\n    # create and store the augmented images  \n    total=0\n    gen=ImageDataGenerator(horizontal_flip=True,  rotation_range=20, width_shift_range=.2,\n                                  height_shift_range=.2, zoom_range=.2)\n    groups=train_df.groupby('labels') # create dataframes one dataframe for each class\n    for label in train_df['labels'].unique():  # for every class               \n        group=groups.get_group(label)  # a dataframe holding only rows with the specified label \n        sample_count=len(group)   # determine how many samples there are in this class  \n        if sample_count< max_samples: # if the class has less than target number of images\n            aug_img_count=0\n            delta=max_samples-sample_count  # number of augmented images to create\n            target_dir=os.path.join(aug_dir, label)  # define where to write the images    \n            aug_gen=gen.flow_from_dataframe( group,  x_col='filepaths', y_col=None, target_size=image_size,\n                                            class_mode=None, batch_size=1, shuffle=False, \n                                            save_to_dir=target_dir, save_prefix='aug-', color_mode='rgb',\n                                            save_format='jpg')\n            while aug_img_count<delta:\n                images=next(aug_gen)            \n                aug_img_count += len(images)\n            total +=aug_img_count\n    print('Total Augmented images created= ', total)\n    # create aug_df and merge with train_df to create composite training set ndf\n    if total>0:\n        aug_fpaths=[]\n        aug_labels=[]\n        classlist=os.listdir(aug_dir)\n        for klass in classlist:\n            classpath=os.path.join(aug_dir, klass)     \n            flist=os.listdir(classpath)    \n            for f in flist:        \n                fpath=os.path.join(classpath,f)         \n                aug_fpaths.append(fpath)\n                aug_labels.append(klass)\n        Fseries=pd.Series(aug_fpaths, name='filepaths')\n        Lseries=pd.Series(aug_labels, name='labels')\n        aug_df=pd.concat([Fseries, Lseries], axis=1)\n        train_df=pd.concat([train_df,aug_df], axis=0).reset_index(drop=True)\n   \n    print (list(train_df['labels'].value_counts()) )\n    return train_df ","20b9c094":"max_samples=200\nmin_samples=0\ncolumn='labels'\nimg_size = (224,224)\nworking_dir = r'.\/'\ntrain_df=balance(train_df, max_samples, min_samples,column, working_dir, img_size)","1be243a7":"\nbatch_size= 40\n# calculate test_batch_size and test_step so we go through test files exactly once\nlength=len(test_df)\ntest_batch_size=sorted([int(length\/n) for n in range(1,length+1) if length % n ==0 and length\/n<=80],reverse=True)[0]  \ntest_steps=int(length\/test_batch_size)\nprint ('test batch size= ', test_batch_size, '  test steps= ', test_steps)\ntrgen=ImageDataGenerator(horizontal_flip=True)\ntvgen=ImageDataGenerator()\ntrain_gen=trgen.flow_from_dataframe(train_df, x_col='filepaths', y_col='labels', target_size=img_size, class_mode='categorical',\n                                    color_mode='rgb', shuffle=True, batch_size=batch_size)\nvalid_gen=tvgen.flow_from_dataframe(valid_df, x_col='filepaths', y_col='labels', target_size=img_size, class_mode='categorical',\n                                    color_mode='rgb', shuffle=True, batch_size=batch_size)\ntest_gen=tvgen.flow_from_dataframe(test_df, x_col='filepaths', y_col='labels', target_size=img_size, class_mode='categorical',\n                                    color_mode='rgb', shuffle=False, batch_size=test_batch_size)\n","4b2a5e3e":"img_shape=(img_size[0], img_size[1], 3)\nmodel_name='EfficientNetB3'\nbase_model=tf.keras.applications.efficientnet.EfficientNetB3(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max') \nx=base_model.output\nx=keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\nx = Dense(256, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n                bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)\nx=Dropout(rate=.45, seed=123)(x)        \noutput=Dense(class_count, activation='softmax')(x)\nmodel=Model(inputs=base_model.input, outputs=output)\nmodel.compile(Adamax(learning_rate=.001), loss='categorical_crossentropy', metrics=['accuracy']) ","ac2c7502":"class SOMT(keras.callbacks.Callback):\n    def __init__(self, model,  train_thold, valid_thold):\n        super(SOMT, self).__init__()\n        self.model=model        \n        self.train_thold=train_thold\n        self.valid_thold=valid_thold\n        \n    def on_train_begin(self, logs=None):\n        print('Starting Training - training will halt if training accuracy achieves or exceeds ', self.train_thold)\n        print ('and validation accuracy meets or exceeds ', self.valid_thold) \n            \n    def on_train_batch_end(self, batch, logs=None):\n        acc=logs.get('accuracy')* 100  # get training accuracy \n        loss=logs.get('loss')\n        msg='{0:1s}processed batch {1:4s}  training accuracy= {2:8.3f}  loss: {3:8.5f}'.format(' ', str(batch),  acc, loss)\n        print(msg, '\\r', end='') # prints over on the same line to show running batch count       \n        \n    def on_epoch_end(self,epoch, logs=None):             \n        tacc=logs.get('accuracy')           \n        vacc=logs.get('val_accuracy')\n        print(f'for epoch {epoch+1} training accuracy = {tacc:6.4f} and validation accuracy = {vacc:6.3f}')\n        if tacc>= self.train_thold and vacc>= self.valid_thold:\n            print( f'\\ntraining accuracy and validation accuracy both reached their thresholds on epoch {epoch + 1}' )\n            self.model.stop_training = True # stop training","2bc5ea33":"rlronp=tf.keras.callbacks.ReduceLROnPlateau( monitor=\"val_loss\", factor=0.5,   patience=1,  verbose=1)\nestop=tf.keras.callbacks.EarlyStopping( monitor=\"val_loss\",   patience=4,  verbose=1,   restore_best_weights=True)\nepochs=40\ntrain_thold=.98\nvalid_thold=.92\ncallbacks=[rlronp, estop, SOMT( model,train_thold, valid_thold)]   \n","b25b2bae":"history=model.fit(x=train_gen,  epochs=epochs, verbose=0, callbacks=callbacks,  validation_data=valid_gen,\n               validation_steps=None,  shuffle=False,  initial_epoch=0)","7b180cc4":" acc= model.evaluate(test_gen, verbose= 1,  steps=test_steps)[1]  *100 \n print(f'Model accuracy on test set is {acc:6.2f}')","8f337be9":"working_dir=r'.\/'\nsave_path=os.path.join(working_dir, 'EfficientNetB3.h5')\nmodel.save(save_path, overwrite=True, include_optimizer=True, save_format='h5')","e1e61a36":"from sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\nsns.set_style('darkgrid')\nclasses=list(train_gen.class_indices.keys())\nclass_count=len(classes)\nlabels=test_gen.labels\nfiles=test_gen.filenames\nerror_file_list=[]\nindexes=[]\nerrors=0\npreds=model.predict(test_gen, steps=test_steps, verbose=1)\ntests=len(preds)\nfor i, p in enumerate (preds):\n    index=np.argmax(p) \n    indexes.append(index)\n    if index != labels[i]:\n        errors +=1\n        error_file_list.append(files[i])\nacc=( tests-errors)\/tests * 100\nprint(f'There were {errors}, errors in {tests} tests for an accuracy of {acc:6.2f} %' )\nif errors > 0:\n    print ('A list of files that were incorrectly predicted is shown below')\n    for i in range (len(error_file_list)):\n        print (error_file_list[i])\n\nclr = classification_report(labels, indexes, target_names=classes, digits= 4)\nprint(\"Classification Report:\\n----------------------\\n\", clr)\ncm = confusion_matrix(labels, indexes )        \nlength=len(classes)\nif length<8:\n    fig_width=8\n    fig_height=8\nelse:\n    fig_width= int(length * .5)\n    fig_height= int(length * .5)\nplt.figure(figsize=(fig_width, fig_height))\nsns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False)       \nplt.xticks(np.arange(length)+.5, classes, rotation= 90)\nplt.yticks(np.arange(length)+.5, classes, rotation=0)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n","18c90acf":"import cv2\nimg=plt.imread(img_path) # read in the image shown earlier\nprint ('Input image shape is ', img.shape)\n# resize the image so it is the same size as the images the model was trained on\nimg=cv2.resize(img, img_size) # in earlier code img_size=(150,150) was used for training the model\nprint ('the resized image has shape ', img.shape)\n### show the resized image\nplt.axis('off')\nplt.imshow(img)\n# Normally the next line of code rescales the images. However the EfficientNet model expects images in the range 0 to 255\n# img= img\/255\n# plt.imread returns a numpy array so it is not necessary to convert the image to a numpy array\n# since we have only one image we have to expand the dimensions of img so it is off the form (1,150,150,3)\n# where the first dimension 1 is the batch size used by model.predict\nimg=np.expand_dims(img, axis=0)\nprint ('image shape after expanding dimensions is ',img.shape)\n# now predict the image\npred=model.predict(img)\nprint ('the shape of prediction is ', pred.shape)\n# this dataset has 15 classes so model.predict will return a list of 15 probability values\n# we want to find the index of the column that has the highest probability\nindex=np.argmax(pred[0])\n# to get the actual Name of the class earlier Imade a list of the class names called classes\nklass=classes[index]\n# lets get the value of the highest probability\nprobability=pred[0][index]*100\n# print out the class, and the probability \nprint(f'the image is predicted as being {klass} with a probability of {probability:6.2f} %')","c8145fb3":"## create a dataframe of the form filepaths(path to the image file), labels (label of the file)","6665eba3":"## do predictions on the test set and generate classification report","d9756c91":"## input an image and get the shape","7908c131":"The SOMT callback is useful to end training based on the value of the training accuracy or the validation loss or both.\nSOMT stands for stop on metrics threshold. The form of use is\ncallbacks=[SOMT(model, train_thold, valid_thold)] where\n* model is the name of your complied model\n* train_thold is a float. It is the value of accuracy (in Percent) that must be achieved by the model in order to conditionally\n  stop training\n* valid_threshold is a float. It is the value of validation accuracy (in Percent) that must be achieved by the model in order to conditionally\n  stop training\n  \n  Note to stop training both the train_thold and valid_thold must BOTH be exceeded in the SAME epoch. If you want to stop training based\n  soley on the training accuracy set the valid_thold to 0.0. Similarly if you want to stop training on just validation accuracy set\n  train_thold= 0.0. Note if both thresholds are not achieved in the same epoch training will continue until the value of epochs set in\n  model.fit is reached. \n  For example lets take the case that you want to stop training when the training accuracy has reached or exceeded 95 % and the validation\n  accuracy has achieved at least 85% then the code would be\n  callbacks=[SOMT(my_model, 95, 85)]","bd82d09f":"## save the model","c32ba45a":"## train_df is not balanced and has a small number of image samples\n## use the balance function defined below to increase the samples in each class\n## to the value max_samples","f2f1b46f":"## train the model ","e0b4c549":"## Define an early stop callback and a reduce learning rate callback and instantiate the ASK callback","a12a181f":"## Use transfer learning with EfficientNetB3 model","7ec4052d":"# split df into a train_df, a test_df and a valid df","18ce46aa":"## How to do predictions on a single image file","5e529b16":"## evaluate model on the test set"}}