{"cell_type":{"a777c653":"code","63cffffd":"code","aeb10d17":"code","34d459b2":"code","6fb78753":"code","5e4e4781":"code","4b5fb2c3":"code","56e3c2e3":"code","8a2ca393":"code","011852a4":"code","68295574":"code","86a5b227":"markdown","9a9a0b58":"markdown","4a750a94":"markdown","a8849f27":"markdown","ddce3fad":"markdown"},"source":{"a777c653":"import tensorflow as tf\nfrom tensorflow.keras.layers import *\nimport pandas as pd\nimport numpy as np\nimport os\nimport re\n\nwith open('..\/input\/all-donald-trump-transcripts\/trump_3.6.txt', 'r', encoding='utf-8') as file:\n  text =  file.read()\ntext = text.lower()","63cffffd":"text = re.sub(r\"\\n\" , \"\", text)\ntext = re.sub(r\"\u00f1\" , \"\", text)\ntext = re.sub(r\"\u00f3\" , \"\", text)\ntext = re.sub(r\"\u014d\" , \"\", text)\ntext = re.sub(r\"\u2013\" , \"\", text)\ntext = re.sub(r\"_\" , \"\", text)\ntext = re.sub(r\"\u2026\" , \"\", text)\ntext = re.sub(r\"\\*\" , \"\", text)\ntext = re.sub(r\"\\+\" , \"\", text)\ntext = re.sub(r\"\\\/\" , \"\", text)\ntext = re.sub(r\"\\[\" , \"\", text)\ntext = re.sub(r\"\\]\" , \"\", text)\ntext = re.sub(r\"\\$\" , \"\", text)\ntext = re.sub(r\"\\&\" , \"\", text)\ntext = re.sub(r\"\\%\" , \"\", text)\ntext = re.sub(r\"-\" , \"\", text)\ntext = re.sub(r\"\u2018\" , \"''\", text)\ntext = re.sub(r\"\u2019\" , \"''\", text)\ntext = re.sub(r\"\u201c\" , \"''\", text)\ntext = re.sub(r\"\u201d\" , \"''\", text)\ntext = re.sub(r\"0\" , \"\", text)\ntext = re.sub(r\"1\" , \"\", text)\ntext = re.sub(r\"2\" , \"\", text)\ntext = re.sub(r\"3\" , \"\", text)\ntext = re.sub(r\"4\" , \"\", text)\ntext = re.sub(r\"5\" , \"\", text)\ntext = re.sub(r\"6\" , \"\", text)\ntext = re.sub(r\"7\" , \"\", text)\ntext = re.sub(r\"8\" , \"\", text)\ntext = re.sub(r\"9\" , \"\", text)\ntext = re.sub(' +', ' ', text)\ntext = re.sub(r\"'+'\" , \"'\", text)","aeb10d17":"print ('Lunghezza del testo: {} caratteri'.format(len(text)))\nvocab = sorted(set(text))\nprint ('{} caratteri unici'.format(len(vocab)))\nchar2idx = {u:i for i, u in enumerate(vocab)}\nidx2char = np.array(vocab)\ntext_as_int = np.array([char2idx[c] for c in text])\n\nseq_length = 75\nprint('Numero di sequenze: {}'.format(len(text) \/\/ (seq_length+1)))\n\nchar_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\nsequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n\nfor item in sequences.take(10):\n  print(repr(''.join(idx2char[item.numpy()])))","34d459b2":"def split_input_target(chunk):\n    input_text = chunk[:-1]\n    target_text = chunk[1:]\n    return input_text, target_text\n\ndataset = sequences.map(split_input_target)\n\nfor input_example, target_example in  dataset.take(1):\n    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))\n\nbatch_size = 64\ndataset = dataset.shuffle(len(dataset)).batch(batch_size, drop_remainder=True)\ndataset","6fb78753":"def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n    model = tf.keras.Sequential([\n        Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n        #LSTM(rnn_units, return_sequences=True, stateful=True),\n        GRU(rnn_units, return_sequences=True, stateful=True),\n        Dense(vocab_size, activation='softmax')\n    ])\n    return model","5e4e4781":"tf.random.set_seed(0)\n\nvocab_size = len(vocab)\nembedding_dim = 256\nrnn_units = 2048\n\nmodel = build_model(len(vocab), embedding_dim, rnn_units, batch_size)\n\nmodel.summary()\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(0.001), \n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n\nhistory = model.fit(dataset,\n                    epochs=7)\n\nmodel_w = model.get_weights()","4b5fb2c3":"def get_id(predictions, temperature):\n    if temperature <= 0:\n        return np.argmax(predictions)\n    predictions = np.log(predictions) \/ temperature\n    exp_predictions = np.exp(predictions)\n    predictions = exp_predictions \/ np.sum(exp_predictions)\n    predictions = np.random.multinomial(1, predictions, 1)\n    return np.argmax(predictions)\n\ndef generate_text(model, num_generate, start_string, temperature=1.0):\n    input_eval = tf.expand_dims([char2idx[s] for s in start_string], 0)\n    text_generated = []\n    model.reset_states()\n    for i in range(num_generate):\n        predictions = tf.squeeze(model(input_eval), 0)\n        predictions = np.asarray(predictions[-1]).astype('float64')\n        predicted_id = get_id(predictions, temperature)\n        input_eval = tf.expand_dims([predicted_id], 0)\n        text_generated.append(idx2char[predicted_id])\n    return start_string + ''.join(text_generated)\n\nmodel = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\nmodel.set_weights(model_w)\nmodel.build(tf.TensorShape([1, None]))","56e3c2e3":"print(generate_text(model, num_generate=150, start_string='america', temperature=0.4))","8a2ca393":"print(generate_text(model, num_generate=150, start_string='stupid', temperature=0.5))","011852a4":"print(generate_text(model, num_generate=150, start_string='great', temperature=0.4))","68295574":"print(generate_text(model, num_generate=300, start_string='biden', temperature=0.4))","86a5b227":"# Generate Text","9a9a0b58":"# Model and Training","4a750a94":"# Little Pre-Processing","a8849f27":"# library\n","ddce3fad":"This is a simple example based on generating contextualized text through recurring networks.\n\nExample based on: https:\/\/www.tensorflow.org\/tutorials\/text\/text_generation"}}