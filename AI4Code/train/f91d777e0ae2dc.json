{"cell_type":{"54ca5c8e":"code","36d7e14e":"code","01e4d060":"code","eed082be":"code","bd631af1":"code","0c67c364":"code","3d77d98c":"code","2027f7a8":"code","f12f06c6":"code","44e733ae":"code","ac4575de":"code","05c04219":"code","565ca844":"code","f5c0e4e5":"code","97183a16":"code","a72e33ae":"code","53f117bb":"code","b1f1413f":"code","637d63d6":"code","3c90f98b":"code","f29fc1e3":"code","6a31ba6a":"code","c8d6eccc":"code","6967b1f2":"code","9c5cc3a2":"code","8e418bd3":"code","ed6a2f33":"code","49529afb":"code","3fd7497b":"code","7647424c":"code","8a2710d4":"code","8a252fbf":"code","134e63a4":"code","f48df92d":"code","d3c82345":"code","f1d799bc":"code","e73794f0":"code","039e5c1e":"code","98c46e46":"code","6c11a285":"code","b62b387a":"code","ab5ed972":"code","22bdb65a":"code","499a7928":"code","bbf75434":"code","a05b2518":"code","8478e00c":"markdown","5c1a5405":"markdown","5342166f":"markdown","d8d27dda":"markdown","f20c9d44":"markdown","4223cd7c":"markdown","681247dc":"markdown","86fef2fb":"markdown","093c1f7c":"markdown","e281036c":"markdown","b8330f66":"markdown","344916ab":"markdown","9ac7fd37":"markdown"},"source":{"54ca5c8e":"## Import basic packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","36d7e14e":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","01e4d060":"## Read data\ntrain = pd.read_csv('\/kaggle\/input\/sentiment-analysis-on-movie-reviews\/train.tsv.zip',sep=\"\\t\") \ntest = pd.read_csv('\/kaggle\/input\/sentiment-analysis-on-movie-reviews\/test.tsv.zip',sep=\"\\t\") ","eed082be":"train.head()","bd631af1":"test.head()","0c67c364":"train.Sentiment.value_counts()","3d77d98c":"train.info()","2027f7a8":"## Show the number of class distributed\nplt.figure(figsize=(10,5))\nax=plt.axes()\nax.set_title('Number of sentiment class')\nsns.countplot(x=train.Sentiment,data=train)","f12f06c6":"import string\nstring.punctuation","44e733ae":"train['Phrase1']=train.Phrase.apply(lambda x: x.translate(str.maketrans('','',string.punctuation)).lower())\ntest['Phrase1']=test.Phrase.apply(lambda x: x.translate(str.maketrans('','',string.punctuation)).lower())","ac4575de":"train.head()","05c04219":"train_shuffle=train.sample(frac=1, random_state=1)","565ca844":"split=int(0.7*train.shape[0])\ntrain_data=train_shuffle[0:split]\nvalid_data=train_shuffle[split:]","f5c0e4e5":"words=set()\nword_to_vec_map={}\nwith open('\/kaggle\/input\/glove-50d\/glove.6B.50d.txt','r',encoding='UTF-8') as f:\n    for line in f:\n        value=line.strip().split()\n        word=value[0]\n        words.add(word)\n        word_to_vec_map[word]=np.array(value[1:],dtype=np.float64)","97183a16":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import regularizers\nfrom tensorflow import keras","a72e33ae":"## fit train data into vector\ntokenizer=Tokenizer()\ntokenizer.fit_on_texts(train.Phrase1)\ntrain_sequences=tokenizer.texts_to_sequences(train_data.Phrase1)\nvalid_sequences=tokenizer.texts_to_sequences(valid_data.Phrase1)\ntest_sequences=tokenizer.texts_to_sequences(test.Phrase1)","53f117bb":"## Pad sequence data with 0 for same length\nmaxlen=train.Phrase1.apply(lambda x: len(x)).max()\ntrain_padded=pad_sequences(train_sequences,maxlen=maxlen,padding='post',truncating='post')\nvalid_padded=pad_sequences(valid_sequences,maxlen=maxlen,padding='post',truncating='post')\ntest_padded=pad_sequences(test_sequences,maxlen=maxlen,padding='post',truncating='post')\nprint(train_padded.shape,valid_padded.shape,test_padded.shape)","b1f1413f":"word_index=tokenizer.word_index\nvocab_size=len(word_index)\nembedding_matrix=np.zeros((vocab_size+1,50))   ## for unknown word, add 1 in vocab_size","637d63d6":"## Use Glove wordembedding\nfor word, i in word_index.items():\n    embedding_vector=word_to_vec_map.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i]=embedding_vector","3c90f98b":"train_x=np.array(train_padded)\ntrain_y=np.array(train_data.Sentiment)\nvalid_x=np.array(valid_padded)\nvalid_y=np.array(valid_data.Sentiment)\ntest_x=np.array(test_padded)","f29fc1e3":"N_TRAIN=len(train_x)\nBATCH_SIZE=256\nSTEPS_PER_EPOCH=N_TRAIN\/BATCH_SIZE\n\n## decrease the learning rate when epoch increase.\nlr_decay=tf.keras.optimizers.schedules.InverseTimeDecay(\n    1e-2,\n    decay_steps=STEPS_PER_EPOCH*100,\n    decay_rate=1,\n    staircase=False\n)\nhistories={}","6a31ba6a":"## Simple NN model\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size+1,50,input_length=maxlen, weights=[embedding_matrix],trainable=False),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(256, activation='relu'),\n    tf.keras.layers.Dense(5, activation='softmax')\n])\n","c8d6eccc":"model.compile(loss='sparse_categorical_crossentropy',optimizer=keras.optimizers.Adam(lr_decay),metrics='accuracy')\nmodel.summary()","6967b1f2":"histories['ANN']=model.fit(train_x, train_y, epochs = 10,validation_data=(valid_x,valid_y), batch_size = 128)","9c5cc3a2":"colors=['blue','green']\ndef plot_metrics(history):\n  metrics =  ['loss', 'accuracy']\n  for n, metric in enumerate(metrics):\n    name = metric.replace(\"_\",\" \").capitalize()\n    plt.subplot(1,2,n+1)\n    plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')\n    plt.plot(history.epoch, history.history['val_'+metric],\n             color=colors[1], linestyle=\"--\", label='Val')\n    plt.xlabel('Epoch')\n    plt.ylabel(name)\n    plt.legend()","8e418bd3":"plt.figure(figsize=(20,7))\nplot_metrics(histories['ANN'])","ed6a2f33":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size+1,50,input_length=maxlen, weights=[embedding_matrix],trainable=False),\n    #tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Conv1D(128, 2, padding='same',activation='relu'),\n    #tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.MaxPooling1D(pool_size=2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(256, activation='relu'),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(5, activation='softmax')\n])\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer=keras.optimizers.Adam(lr_decay),metrics='accuracy')\nmodel.summary()","49529afb":"histories['CNN']=model.fit(train_x, train_y, epochs = 10,validation_data=(valid_x,valid_y), batch_size = 256)","3fd7497b":"plt.figure(figsize=(20,7))\nplot_metrics(histories['CNN'])","7647424c":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size+1,50,input_length=maxlen, weights=[embedding_matrix],trainable=False),\n    #tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.LSTM(256, return_sequences=True),\n    tf.keras.layers.LSTM(256),\n    #tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(125, activation='relu'),\n    #tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(5, activation='softmax')\n])\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer=keras.optimizers.Adam(lr_decay),metrics='accuracy')\nmodel.summary()\n","8a2710d4":"histories['LSTM_1']=model.fit(train_x, train_y, epochs = 10,validation_data=(valid_x,valid_y), batch_size = 128)","8a252fbf":"plt.figure(figsize=(20,7))\nplot_metrics(histories['LSTM_1'])","134e63a4":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size+1,50,input_length=maxlen, weights=[embedding_matrix],trainable=False),\n    #tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    #tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(256, activation='relu'),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(5, activation='softmax')\n])\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer=keras.optimizers.Adam(lr_decay),metrics='accuracy')\nmodel.summary()","f48df92d":"histories['LSTM_2']=model.fit(train_x, train_y, epochs = 10,validation_data=(valid_x,valid_y), batch_size = 256)","d3c82345":"plt.figure(figsize=(20,7))\nplot_metrics(histories['LSTM_2'])","f1d799bc":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size+1,50,input_length=maxlen, weights=[embedding_matrix],trainable=False),\n    #tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n    #tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(256, activation='relu'),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(5, activation='softmax')\n])\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer=keras.optimizers.Adam(lr_decay),metrics='accuracy')\nmodel.summary()","e73794f0":"histories['LSTM_3']=model.fit(train_x, train_y, epochs = 10,validation_data=(valid_x,valid_y), batch_size = 256)","039e5c1e":"plt.figure(figsize=(20,7))\nplot_metrics(histories['LSTM_3'])","98c46e46":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size+1,50,input_length=maxlen, weights=[embedding_matrix],trainable=False),\n    #tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n    #tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(256, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(5, activation='softmax')\n])\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer=keras.optimizers.Adam(lr_decay),metrics='accuracy')\nmodel.summary()","6c11a285":"histories['LSTM_4']=model.fit(train_x, train_y, epochs = 10,validation_data=(valid_x,valid_y), batch_size = 256)","b62b387a":"plt.figure(figsize=(20,7))\nplot_metrics(histories['LSTM_4'])","ab5ed972":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size+1,50,input_length=maxlen, weights=[embedding_matrix],trainable=False),\n    #tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(256, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(5, activation='softmax')\n])\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer=keras.optimizers.Adam(lr_decay),metrics='accuracy')\nmodel.summary()","22bdb65a":"histories['LSTM_5']=model.fit(train_x, train_y, epochs = 10,validation_data=(valid_x,valid_y), batch_size = 256)","499a7928":"plt.figure(figsize=(20,7))\nplot_metrics(histories['LSTM_5'])","bbf75434":"test_y=model.predict_classes(test_x)","a05b2518":"sub_file = pd.read_csv('\/kaggle\/input\/sentiment-analysis-on-movie-reviews\/sampleSubmission.csv',sep=',')\nsub_file.Sentiment=test_y\nsub_file.to_csv('Submission.csv',index=False)","8478e00c":"## Explore Data Analysis","5c1a5405":"### ANN","5342166f":"## Use Glove for Wordembedding","d8d27dda":"## Remove punctuation","f20c9d44":"# Movie sentiment analysis\n\n**Data:**\n\nThe data source is Movie Sentiment project from Kaggle, including train and test datasets; In trainning data, there is a Phrase column and a Sentiment column as the result score from 0 to 4 (negative to positve); In test data, there is a Phrase column for us to analyze the sentiment of each phrase.\n\n**Goal:**\n\n\nPerform different deep learning models (ANN, CNN, RNN-LSTM) in NLP analysis on trainning data and compare model performances, and predict Phrase sentiment on test data. \n\n**Highlight:** \n\n- **Show how to prepare data for deep learning in NLP analysis**\n- **Compare different deep learning models in NLP analysis**\n\n**Reference**\n\nKaggel notebook: https:\/\/www.kaggle.com\/chiranjeevbit\/movie-review-prediction\n\n\nTensorflow helpfile: https:\/\/www.tensorflow.org\/tutorials\/structured_data\/imbalanced_data","4223cd7c":"### Bidirectional LSTM","681247dc":"### RNN-LSTM, GRU","86fef2fb":"### Convolution NN","093c1f7c":"## Submission","e281036c":"## Split train data into train and validation datasets","b8330f66":"## Deep learning model comparision","344916ab":"Bidirectional LSTM tunning","9ac7fd37":"## Data preparation for Modelling"}}