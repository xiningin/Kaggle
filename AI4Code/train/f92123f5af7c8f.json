{"cell_type":{"1bcdffc2":"code","618e4b6b":"code","ad044c1a":"code","420fa4c3":"code","7829fb83":"code","297e3906":"code","911f6aa4":"code","18aa09af":"code","32852f7e":"code","2b9564bd":"code","3f1bb25e":"code","fb99a122":"code","f7f077da":"code","e7ee1b2f":"code","2de38d15":"code","4613a7cf":"code","9642ec23":"code","ef7bc35a":"code","b3aa2434":"code","7ad0d9b8":"code","c5fc7ac7":"code","653667d3":"code","8986a89c":"code","a57ef4d4":"code","42bfb87a":"code","ed38c266":"code","552522f3":"code","2bf613c7":"code","87eb8aa6":"code","d813398c":"code","55f19b41":"code","9e4aa9e9":"code","33fd03c5":"code","2d3b4ccb":"code","d67ba833":"code","8324d60e":"code","0dbdddb9":"code","8ee96c70":"code","a930deb1":"code","b530db4c":"code","7948fbe0":"code","45a3c658":"code","d9f92170":"code","3f85658b":"markdown","0442f2f4":"markdown","d8bf4a12":"markdown"},"source":{"1bcdffc2":"import pandas as pd\nfrom nltk.corpus import stopwords\nimport string","618e4b6b":"fake = pd.read_csv(\"\/kaggle\/input\/fake-and-real-news-dataset\/Fake.csv\", names=None,encoding='latin-1',low_memory=False)\nfake.tail(2)","ad044c1a":"true = pd.read_csv(\"\/kaggle\/input\/fake-and-real-news-dataset\/True.csv\", names=None,encoding='latin-1',low_memory=False)\ntrue.tail(2)","420fa4c3":"fake['label']=1\nfake.head(2)","7829fb83":"true['label']=0\ntrue.head(2)","297e3906":"df=pd.concat([fake,true]).reset_index(drop=True)\ndf.head(2)","911f6aa4":"df.shape","18aa09af":"df.label.value_counts()","32852f7e":"STOPWORDS = set(stopwords.words('english'))","2b9564bd":"def clean(text):\n    #1. Remove punctuation\n    translator1 = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n    text = text.translate(translator1)\n    \n    #2. Convert to lowercase characters\n    text = text.lower()\n    \n    #3. Remove stopwords\n    text = ' '.join([word for word in text.split() if word not in STOPWORDS])\n    \n    return text","3f1bb25e":"df['clean_title']=df['title'].apply(clean)","fb99a122":"df['clean_text']=df['text'].apply(clean)\ndf.head(2)","f7f077da":"df['clean_subject']=df['subject'].str.lower()\ndf.head(2)","e7ee1b2f":"df['combined']=df['clean_subject']+' '+df['clean_title']+' '+df['clean_text']\ndf.head(2)","2de38d15":"from nltk.stem.porter import PorterStemmer","4613a7cf":"stop = set(stopwords.words('english'))\npunctuation = list(string.punctuation)\nstop.update(punctuation)","9642ec23":"stemmer = PorterStemmer()\n\ndef stem_text(text):\n    final_text = []\n    for i in text.split():\n        if i.strip().lower() not in stop:\n            word = stemmer.stem(i.strip())\n            final_text.append(word)\n    return \" \".join(final_text)","ef7bc35a":"df['after_stemming'] = df.combined.apply(stem_text)\ndf.head(2)","b3aa2434":"import numpy as np","7ad0d9b8":"#visualize word distribution\ndf['doc_len'] = ''\ndf['doc_len'] = df['after_stemming'].apply(lambda words: len(words.split(\" \")))\nmax_seq_len = np.round(df['doc_len'].mean() + df['doc_len'].std()).astype(int)\nprint(max_seq_len)","c5fc7ac7":"from sklearn.model_selection import train_test_split","653667d3":"X_train, X_test, y_train, y_test = train_test_split(df['after_stemming'].to_list(), df['label'].values, test_size=0.33, random_state=42)\nprint(len(X_train))\nprint(len(X_test))\nprint(len(y_train))\nprint(len(y_test))","8986a89c":"from keras.preprocessing.text import Tokenizer","a57ef4d4":"MAX_NB_WORDS = 28000\n\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\ntokenizer.fit_on_texts(X_train)\n\nword_index = tokenizer.word_index\nprint(\"dictionary size: \", len(word_index))\n","42bfb87a":"from keras.preprocessing import sequence","ed38c266":"word_seq_train = tokenizer.texts_to_sequences(X_train)\nword_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)","552522f3":"word_seq_train.shape","2bf613c7":"gensim_news_desc = []\n\nchunk_data = X_train\n\nfor record in range(0,len(chunk_data)):\n    news_desc_list = []\n    for tok in chunk_data[record].split():\n        news_desc_list.append(str(tok))\n    gensim_news_desc.append(news_desc_list)\n\nlen(gensim_news_desc)","87eb8aa6":"from gensim.models import Word2Vec\n\ngensim_model = Word2Vec(gensim_news_desc, min_count=5, size = 200, sg=1)\n\n# summarize the loaded model\nprint(gensim_model)\n\n# summarize vocabulary\nwords = list(gensim_model.wv.vocab)\n","d813398c":"len(words)","55f19b41":"#training params\nbatch_size = 1024\nnum_epochs = 10\n#model parameters\nnum_filters = 128\nembed_dim = 200 \nweight_decay = 1e-4\nclass_weight = {0: 1,\n               1: 1}","9e4aa9e9":"print('preparing embedding matrix...')\n\ngensim_words_not_found = []\ngensim_nb_words = len(gensim_model.wv.vocab)\nprint(\"gensim_nb_words : \",gensim_nb_words)\n\ngensim_embedding_matrix = np.zeros((gensim_nb_words, embed_dim))\n\nfor word, i in word_index.items():\n    #print(word)\n    if i >= gensim_nb_words:\n        continue\n    if word in gensim_model.wv.vocab :\n        embedding_vector = gensim_model[word]\n        if (embedding_vector is not None) and len(embedding_vector) > 0:\n            gensim_embedding_matrix[i] = embedding_vector\n    else :\n        gensim_words_not_found.append(word)","33fd03c5":"gensim_embedding_matrix.shape","2d3b4ccb":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout, Flatten\nfrom keras.layers import Input, Embedding, Conv1D, MaxPooling1D\nfrom keras.models import Model\nfrom keras import regularizers\nfrom keras import optimizers\nfrom keras.callbacks import EarlyStopping\nfrom keras import backend as K","d67ba833":"model = Sequential()\n# gensim word2vec embedding\nmodel.add(Embedding(gensim_nb_words, embed_dim, weights=[gensim_embedding_matrix], input_length=max_seq_len))\nmodel.add(Conv1D(num_filters, 5, activation='relu', padding='same'))\nmodel.add(MaxPooling1D(2))\nmodel.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\nmodel.add(Dropout(0.6))\nmodel.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\nmodel.add(Dropout(0.6))\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid')) \n\nadam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\nmodel.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\nmodel.summary()","8324d60e":"#define callbacks\nearly_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\ncallbacks_list = [early_stopping]","0dbdddb9":"# gensim model training\nhist = model.fit(word_seq_train, y_train, batch_size=batch_size, epochs=num_epochs, callbacks=callbacks_list, validation_split=0.1, shuffle=True, verbose=2,class_weight=class_weight)","8ee96c70":"import matplotlib.pyplot as plt","a930deb1":"# list all data in history\n\nprint(hist.history.keys())\n\n# summarize history for accuracy\nplt.plot(hist.history['accuracy'])\nplt.plot(hist.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","b530db4c":"word_seq_test = tokenizer.texts_to_sequences(X_test)\nword_seq_test = sequence.pad_sequences(word_seq_test, maxlen=max_seq_len)","7948fbe0":"predictions = model.predict(word_seq_test)\npred_labels = predictions.round()","45a3c658":"unique, counts = np.unique(y_test, return_counts=True)\nunique, counts","d9f92170":"from sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, pred_labels, labels=[1,0])\ncm","3f85658b":"> ","0442f2f4":"# TRAIN MODEL ","d8bf4a12":"# PREPROCESSING AND CLEANING DATA"}}