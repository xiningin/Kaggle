{"cell_type":{"bab35805":"code","a6584859":"code","be3d37ea":"code","710d57ff":"code","b17a43a3":"code","3300ea4b":"code","0d292f6a":"code","68601655":"code","6aa4238a":"code","9176046c":"code","11b61bf3":"code","0f57a5c6":"code","8df5682b":"code","34b3c404":"code","345da0e4":"code","e53f8f17":"code","9facedde":"code","f527cc35":"code","d7a6bc82":"code","a045c59f":"code","be33264f":"code","c299fb23":"code","535e4219":"code","9757ba37":"code","16cc9052":"code","914ade89":"code","4dc8f94d":"markdown","0a023e08":"markdown"},"source":{"bab35805":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom keras.datasets import cifar10\n\nimport os\nimport tarfile\nimport sys\nimport pickle\nprint(os.listdir(\"..\/input\/cifar10\/\"))\n\nimport keras\nfrom keras import backend   as K\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Conv2D,Flatten,Dropout,MaxPooling2D,Activation, BatchNormalization\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n# Any results you write to the current directory are saved as output.","a6584859":"from tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())","be3d37ea":"def load_batch(fpath, label_key='labels'):\n    \"\"\"Internal utility for parsing CIFAR data.\n\n    # Arguments\n        fpath: path the file to parse.\n        label_key: key for label data in the retrieve\n            dictionary.\n\n    # Returns\n        A tuple `(data, labels)`.\n    \"\"\"\n    with open(fpath, 'rb') as f:\n        if sys.version_info < (3,):\n            d = pickle.load(f)\n        else:\n            d = pickle.load(f, encoding='bytes')\n            # decode utf8\n            d_decoded = {}\n            for k, v in d.items():\n                d_decoded[k.decode('utf8')] = v\n            d = d_decoded\n    data = d['data']\n    labels = d[label_key]\n\n    data = data.reshape(data.shape[0], 3, 32, 32)\n    return data, labels","710d57ff":"train_num = 50000\ntrain_x = np.zeros(shape=(train_num,3,32,32))\ntrain_y = np.zeros(shape=(train_num))\n\ntest_num = 10000\ntest_x = np.zeros(shape=(test_num,3,32,32))\ntest_y = np.zeros(shape=(test_num))","b17a43a3":"def load_data():\n    for i in range(1,6):\n        begin = (i-1)*10000\n        end = i*10000\n        train_x[begin:end,:,:,:],train_y[begin:end] = load_batch(\"..\/input\/cifar10\/data_batch_\"+str(i))\n    \n    test_x[:],test_y[:] = load_batch(\"..\/input\/cifar10\/test_batch\")","3300ea4b":"load_data()","0d292f6a":"test_y[1:10]","68601655":"train_x[1]","6aa4238a":"if K.image_data_format() == 'channels_last':\n    print(\"channels_last\")\n    test_x = test_x.transpose(0, 2, 3, 1)\n    train_x = train_x.transpose(0, 2, 3, 1)\nelse:\n    print(\"channels_first\")\n","9176046c":"train_x.shape","11b61bf3":"train_y.shape","0f57a5c6":"train_y[0:10]","8df5682b":"test_x.shape","34b3c404":"test_y.shape","345da0e4":"# zero-center \ntrain_x -= np.mean(train_x, axis = 0)\ntest_x -= np.mean(test_x, axis = 0)","e53f8f17":"# Normalization\ntrain_x \/= np.std(train_x, axis = 0)\ntest_x \/= np.std(test_x, axis = 0)","9facedde":"train_x[2][2][3]","f527cc35":"train_y = to_categorical(train_y,10)\ntest_y = to_categorical(test_y,10)","d7a6bc82":"train_y[1:5]","a045c59f":"train_y.shape","be33264f":"model = Sequential()\nmodel.add(Conv2D(32, (3, 3), padding='same',\n                 input_shape=train_x.shape[1:]))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(32, (3, 3)))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(64, (3, 3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(64, (3, 3)))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(512))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(10))\nmodel.add(BatchNormalization())\nmodel.add(Activation('softmax'))\n\n# initiate RMSprop optimizer\nopt = keras.optimizers.Adam(lr=0.0001)\n\n# Let's train the model using RMSprop\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=opt,\n              metrics=['accuracy'])","c299fb23":"batch_size = 64\nepochs = 40","535e4219":"history = model.fit(train_x,train_y,batch_size,epochs,validation_data=(test_x,test_y),shuffle=True)","9757ba37":"history.history","16cc9052":"fig = plt.plot(history.history[\"acc\"],label = \"train\", color='green')\nplt.plot(history.history[\"val_acc\"],label = \"test\", color='red')\nplt.legend(loc='upper left')\nplt.xlabel(\"epochs\")\nplt.ylabel(\"accuracy\")\nplt.title(\"accuracy by epochs\")\nplt.show()","914ade89":"fig = plt.plot(history.history[\"loss\"],label = \"train\", color='green')\nplt.plot(history.history[\"val_loss\"],label = \"test\", color='red')\nplt.legend(loc='upper left')\nplt.xlabel(\"epochs\")\nplt.ylabel(\"loss\")\nplt.title(\"loss by epochs\")\nplt.show()","4dc8f94d":"This kernel is done with sone editing in this kernel  [cifar10_cnn](https:\/\/www.kaggle.com\/lianglirong\/cifar10-cnn-by-keras)\"\n\nin version_3 we use adam as an optimizer and another small editing and use 40 epoch  with 64 batch size to avoiad overfitting","0a023e08":"### we should zero-center and normalize data first"}}