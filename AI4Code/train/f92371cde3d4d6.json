{"cell_type":{"9109d116":"code","14a781bc":"code","b9e114d3":"code","1a0eec67":"code","e5f690d0":"code","0fb95a4a":"code","b567460f":"markdown","d676eb15":"markdown","fa964f40":"markdown"},"source":{"9109d116":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","14a781bc":"from copy import deepcopy\nfrom itertools import product\nfrom collections import defaultdict\n\nfrom xgboost import XGBRegressor","b9e114d3":"def GridSearchCV_XGB_early_stoppping(param_grid, fit_params, scorer, cv, X, y):\n    \"\"\"This function performs grid search for the best set of parameters of XGBoost model with early stopping.\n\n    Args:\n        param_grid (dict): The parameter ranges for which the function searches.\n        fit_params (dict): The fitting parameters for XGBoost.\n        scorer (_PredictScorer): The sklearn's scorer instance.\n        cv (model_selection._split): The sklearn's split instance.\n        X (DataFrame): The input data matrix.\n        y (Series): The ground truth label.\n        \n    Returns:\n        dict: The best set of parameters found via grid search.\n    \"\"\"\n    if isinstance(X, pd.DataFrame):\n        X = X.to_numpy()\n    if isinstance(y, pd.Series):\n        y = y.to_numpy()\n        \n    param_names, param_values = zip(*list(param_grid.items()))\n\n    cv_best_iterations = defaultdict(list)\n    cv_results = defaultdict(list)\n\n    for train_index, test_index in cv.split(X, y):\n        X_in, X_out = X[train_index], X[test_index]\n        y_in, y_out = y[train_index], y[test_index]\n\n        fit_params_cv = deepcopy(fit_params)\n        fit_params_cv['eval_set'] = [(X_out, y_out)]\n\n        for value_combination in product(*param_values):\n            param_grid_cv = tuple(zip(param_names, value_combination))\n            xgboost = XGBRegressor(**dict(param_grid_cv))\n\n            xgboost.fit(X_in, y_in, **fit_params_cv)\n            best_iteration = xgboost.get_num_boosting_rounds() if 'early_stopping_rounds' not in fit_params_cv else xgboost.best_iteration\n            cv_best_iterations[param_grid_cv].append(best_iteration)\n\n            score = scorer(xgboost, X_out, y_out)\n            cv_results[param_grid_cv].append(score)\n        \n    best_params_xgb, score_list = max(cv_results.items(), key=lambda x: np.array(x[1]).mean())\n\n    # Note that our XGBoost model may stop early,\n    # so we calculate the mean of the actual number of estimators in each fold,\n    # in place of the originally planned n_estimators after finishing cross validation.\n    n_estimators = int(round(np.array(cv_best_iterations[best_params_xgb]).mean()))\n    \n    best_params_xgb = dict(best_params_xgb)\n    best_params_xgb['n_estimators'] = n_estimators\n\n    print (\"Best score: {:.3f}\".format(np.array(score_list).mean()))\n    print (\"Best Parameters: {}\".format(best_params_xgb))\n    \n    return best_params_xgb","1a0eec67":"from sklearn.model_selection import train_test_split\n\nseed = 0\n\n# Read the data\nX = pd.read_csv('..\/input\/house-prices-data\/train.csv', index_col='Id')\n\n# For simplicity, we drop all categorical features\nX = X.select_dtypes(exclude=['object'])\n\n# Remove rows with missing target, separate target from predictors\nX.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X.SalePrice\nX.drop(['SalePrice'], axis=1, inplace=True)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=seed)","e5f690d0":"from sklearn.metrics import mean_absolute_error, make_scorer\nfrom sklearn.model_selection import KFold\n\nparam_grid = {\n    'objective': ['reg:squarederror'],\n    'n_estimators': [200, 500, 1000, 1500, 2000],\n    'max_depth': [2],\n    'learning_rate': [0.1],\n    'random_state': [seed]\n}\n\nfit_params = {\n    'eval_metric': \"rmse\",\n    'early_stopping_rounds': 100,\n    'verbose': False\n}\n\nscorer = make_scorer(mean_absolute_error, greater_is_better=False)\nkf = KFold(n_splits=5, shuffle=True, random_state=seed)\n\nbest_params_xgb = GridSearchCV_XGB_early_stoppping(param_grid, fit_params, scorer, kf, X_train, y_train)\n\nbest_xgb = XGBRegressor(**best_params_xgb)\nbest_xgb.fit(X_train, y_train, eval_metric=fit_params['eval_metric'], verbose=False)\nbest_score = scorer(best_xgb, X_valid, y_valid)\nprint (\"The best score for XGBoost on validation set is {:.3f}\".format(best_score))\n\nnp.save('best_params_xgb.npy', best_params_xgb)","0fb95a4a":"# We can load the model back with previously found parameters:\nfinal_xgb = XGBRegressor(**np.load('best_params_xgb.npy', allow_pickle=True).item())","b567460f":"One nice feature about XGBoost is the possibility to do early-stopping. However, this functionality is not integrated well with scikit-learn's API. So I wrote a custom function which allows us to do grid search and early-stopping at the same time.\n\nBy the way, if you choose not to specify `early_stopping_rounds`, this custom function would reduce to an ordinary grid searcher.\n\nThe logic and implementation of this custom function might not be ideal and any suggestion is welcome.","d676eb15":"Let's try it on a real [dataset](https:\/\/www.kaggle.com\/c\/home-data-for-ml-course).","fa964f40":"This notebook ends here and please kindly give a vote if you find it helpful. You are also welcome to check my other notebooks."}}