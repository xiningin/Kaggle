{"cell_type":{"f693f3ca":"code","801069b3":"code","98ce1f1e":"code","c78f1d36":"code","d24f7159":"code","aa7382de":"code","c4cda539":"code","771a6b55":"code","3ea25918":"code","d4e21152":"code","0f54f017":"code","c8a5d291":"code","292307e1":"code","1ca5c661":"code","6a03de5b":"code","7d53f5c0":"code","7ed59a34":"code","163de1a7":"markdown","9add1f78":"markdown","996cb40e":"markdown","26a8c5db":"markdown"},"source":{"f693f3ca":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns # data visualisation\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n    \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","801069b3":"#Libraries that will be used for training and predicting\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor, FeaturesData, Pool\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split","98ce1f1e":"# Using standard naming convention will improve the understandability of the notebook and \"human\" learning. \n\ntrain_data = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/train.csv',index_col=0)\ntest_data  = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/test.csv', index_col=0)\nsample     = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')\nbase_features = ['f52','f77','f81','f13']\n\nX_full = train_data[base_features].copy()\ny_full = train_data.pop('loss')\n\ntrain_X, val_X, train_y, val_y = train_test_split(X_full, y_full,random_state = 0) #Default is 33.33% split, which is verified below","c78f1d36":"#Check the shape of the training, and validation data for understanding data\nprint(train_X.shape) # Data used for fitting the model\nprint(val_X.shape) # Evaluating the fitted model for scores\nprint(train_y.shape) #Data used for fitting the model \nprint(val_y.shape)  # Evaluating the fitted model for scores\nprint(\"Percentage of validation to training data is:\",(val_y.shape[0]\/train_y.shape[0])*100)","d24f7159":"#Considering the following parameters, and tuning only learning rate and estimator\n#1) n_estimators : 500\n#2) learning rate: 0.1\n#3) n_jobs : 4 (running on Kaggle, so unsure how many cores)\n#4) Early Stoping : 5","aa7382de":"def get_estimator(est, train_X, val_X, train_y, val_y):\n    base_model = XGBRegressor(n_estimators=est, learning_rate=0.1, n_jobs=4)\n    base_model.fit(train_X, train_y)\n    preds_val = base_model.predict(val_X)\n    rmse = mean_squared_error(val_y, preds_val,squared=False)\n    return(rmse)","c4cda539":"#Taking best estimator 50 and running best learning rate\ndef get_lr(lr, train_X, val_X, train_y, val_y):\n    base_model = XGBRegressor(n_estimators=50, learning_rate=lr, n_jobs=4)\n    base_model.fit(train_X, train_y)\n    preds_val = base_model.predict(val_X)\n    rmse = mean_squared_error(val_y, preds_val,squared=False)\n    return(rmse)","771a6b55":"max_est= [10,50,100,200,500]\n# Write loop to find the ideal tree size from candidate_max_leaf_nodes\nbest_est = []\nfor est in max_est:\n    best_est.append(get_estimator(est,train_X, val_X, train_y, val_y))","3ea25918":"max_lr= [0.01,0.05,0.1,0.5,0.7,1]\n# Write loop to find the ideal tree size from candidate_max_leaf_nodes\nbest_lr = []\nfor lr in max_lr:\n    best_lr.append(get_lr(lr,train_X, val_X, train_y, val_y))","d4e21152":"# Store the best value of max_estimators (it will be one of the max_est)\nfinal_est = max_est[best_est.index(min(best_est))]\nprint(\"The best estimator is :\",final_est)","0f54f017":"# Store the best value of max_estimators (it will be one of the max_est)\nfinal_lr = max_lr[best_lr.index(min(best_lr))]\nprint(\"The best Learning rate is:\",final_lr)","c8a5d291":"plt.title('Max estimator plotting')\nplt.plot(best_est)\nplt.xlabel('estimator')\nplt.ylabel('rmse')","292307e1":"plt.title('Max learningRate plotting')\nplt.plot(best_lr)\nplt.xlabel('learningRate')\nplt.ylabel('rmse')","1ca5c661":"# The estimator has been identified as 50, and LR as 0.1. Entering Cross validation\n# Multiply by -1 since sklearn calculates *negative* MAE\n\nmy_pipeline = Pipeline(steps=[('model',  XGBRegressor(n_estimators=50, learning_rate=0.1, n_jobs=4))])","6a03de5b":"scores = -1 * cross_val_score(my_pipeline, X_full,y=y_full,\n                              cv = 5,\n                              scoring='neg_root_mean_squared_error')\n\nprint(\"MAE scores:\\n\", scores)","7d53f5c0":"#Since the pipeline is already tested, we can use the full data also.\nmy_pipeline.fit(X_full, y_full)\ntest_pred = my_pipeline.predict(test_data[base_features])","7ed59a34":"my_submission = pd.DataFrame({'id': test_data.index, 'loss':test_pred})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)","163de1a7":"#The Random Forest model has further improved the score, even though it is marginal. The next model, I have been hearing is the best in the industry, and is used to win competitions. XGBoost or Extreme Gradient Booster. \n\nThis algorithm also has multiple variables that are unrelated and needs to be tune individually. \n\nIn this model we will only use the base features that were selected from the Decision Tree Regressor, and improve the model.","9add1f78":"## Objective: \nImproving the model to recieve better scoring, and achieve better understanding of CatBoost, XGBoost and Cross Validation\n\n## Strategy:\nTo use the XGBoost and Catboost regressors with Cross validation\n\n## Tactics:\n\n1) Re-Write a function for scoring the models, based on the new models\n\n2) Write hyper parameter tuning of each models and identify the parameters\n\n3) After Hyper parameter tuning start the Cross Validation\n\n4) Apply the CV model to the final X_test and submit\n\n5) We will consider the reduced dimensions from the beginning i.e base_features = ['f52','f77','f81','f13']","996cb40e":"## There are 4 important parameters to be considered for XGBoost\n\n1) n_estimators : Number of iterations the learning process will occur, usually (100 to 1000)\n\n2) learning rate: Rate at which the learning happens (0.01 to 0.001)\n\n3) n_jobs : Whether multicore is used (3 to 5)\n\n4) Early Stoping : If there is no improvement then stop early.(5 minimum)\n\nAll these parameters are independent. ","26a8c5db":"## XGBoost Regressor"}}