{"cell_type":{"8cc321da":"code","9e7f9a3f":"code","bebd3f1f":"code","c140d0d3":"code","26d95851":"code","4364330b":"code","a59bba1c":"code","bea5e388":"code","ace11e2a":"code","e0663e4a":"code","d33a94f3":"code","4fd957ed":"code","5e3e71bb":"code","6db86f90":"code","eed4d67c":"code","7b847566":"code","47ad27de":"code","e0a152e4":"code","255778e3":"code","ca350144":"code","9f3f8137":"code","8471e218":"markdown","05eea8a7":"markdown"},"source":{"8cc321da":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9e7f9a3f":"train_set = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest_set = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","bebd3f1f":"import re","c140d0d3":"def clean_url(x):\n    cleaned_x = re.sub(r'http\\S{0,}', r'', x)\n    \n    return cleaned_x","26d95851":"train_set['text'] = train_set['text'].apply(clean_url)\ntest_set['text'] = test_set['text'].apply(clean_url)","4364330b":"def lower_case(x):\n    return x.lower()","a59bba1c":"def clean_new_line(x):\n    return re.sub(r\"\\n\", \" \", x)","bea5e388":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","ace11e2a":"train_set['text'] = train_set['text'].apply(clean_new_line)\ntrain_set['text'] = train_set['text'].apply(remove_emoji)\ntrain_set['text'] = train_set['text'].apply(lower_case)","e0663e4a":"test_set['text'] = test_set['text'].apply(clean_new_line)\ntest_set['text'] = test_set['text'].apply(remove_emoji)\ntest_set['text'] = test_set['text'].apply(lower_case)","d33a94f3":"!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","4fd957ed":"import tokenization\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow import keras\nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback","5e3e71bb":"bert_layer = hub.KerasLayer('https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/1', trainable=True)","6db86f90":"K = 2\nskf = StratifiedKFold(n_splits=K, shuffle=True)","eed4d67c":"class ClassificationReport(Callback):\n    \n    def __init__(self, train_data=(), validation_data=()):\n        super(Callback, self).__init__()\n        \n        self.X_train, self.y_train = train_data\n        self.train_precision_scores = []\n        self.train_recall_scores = []\n        self.train_f1_scores = []\n        \n        self.X_val, self.y_val = validation_data\n        self.val_precision_scores = []\n        self.val_recall_scores = []\n        self.val_f1_scores = [] \n               \n    def on_epoch_end(self, epoch, logs={}):\n        train_predictions = np.round(self.model.predict(self.X_train, verbose=0))        \n        train_precision = precision_score(self.y_train, train_predictions, average='macro')\n        train_recall = recall_score(self.y_train, train_predictions, average='macro')\n        train_f1 = f1_score(self.y_train, train_predictions, average='macro')\n        self.train_precision_scores.append(train_precision)        \n        self.train_recall_scores.append(train_recall)\n        self.train_f1_scores.append(train_f1)\n        \n        val_predictions = np.round(self.model.predict(self.X_val, verbose=0))\n        val_precision = precision_score(self.y_val, val_predictions, average='macro')\n        val_recall = recall_score(self.y_val, val_predictions, average='macro')\n        val_f1 = f1_score(self.y_val, val_predictions, average='macro')\n        self.val_precision_scores.append(val_precision)        \n        self.val_recall_scores.append(val_recall)        \n        self.val_f1_scores.append(val_f1)\n        \n        print('\\nEpoch: {} - Training Precision: {:.6} - Training Recall: {:.6} - Training F1: {:.6}'.format(epoch + 1, train_precision, train_recall, train_f1))\n        print('Epoch: {} - Validation Precision: {:.6} - Validation Recall: {:.6} - Validation F1: {:.6}'.format(epoch + 1, val_precision, val_recall, val_f1))  ","7b847566":"class DisasterDetector:\n    \n    def __init__(self, bert_layer, optimizer, max_seq_length=128, lr=0.0001, epochs=15, batch_size=32):\n        \n        # BERT and Tokenization params\n        self.bert_layer = bert_layer\n        \n        self.max_seq_length = max_seq_length        \n        vocab_file = self.bert_layer.resolved_object.vocab_file.asset_path.numpy()\n        do_lower_case = self.bert_layer.resolved_object.do_lower_case.numpy()\n        self.tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n        self.optimizer = optimizer\n        \n        # Learning control params\n        self.lr = lr\n        self.epochs = epochs\n        self.batch_size = batch_size\n        \n        self.models = []\n        self.scores = {}\n        \n        \n    def preprocess(self, texts):\n                \n        all_tokens = []\n        all_masks = []\n        all_segments = []\n\n        for text in texts:\n            text = self.tokenizer.tokenize(text)\n            text = text[:self.max_seq_length - 2]\n            input_sequence = ['[CLS]'] + text + ['[SEP]']\n            pad_len = self.max_seq_length - len(input_sequence)\n\n            tokens = self.tokenizer.convert_tokens_to_ids(input_sequence)\n            tokens += [0] * pad_len\n            pad_masks = [1] * len(input_sequence) + [0] * pad_len\n            segment_ids = [0] * self.max_seq_length\n\n            all_tokens.append(tokens)\n            all_masks.append(pad_masks)\n            all_segments.append(segment_ids)\n\n        return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n    \n    \n    def build_model(self):\n        \n        input_word_ids = Input(shape=(self.max_seq_length,), dtype=tf.int32, name='input_word_ids')\n        input_mask = Input(shape=(self.max_seq_length,), dtype=tf.int32, name='input_mask')\n        segment_ids = Input(shape=(self.max_seq_length,), dtype=tf.int32, name='segment_ids')    \n        \n        pooled_output, sequence_output = self.bert_layer([input_word_ids, input_mask, segment_ids])   \n        clf_output = sequence_output[:, 0, :]\n        out = Dense(1, activation='sigmoid')(clf_output)\n        \n        model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n        model.compile(loss='binary_crossentropy', optimizer=self.optimizer, metrics=['accuracy'])\n        \n        return model\n    \n    \n    def train(self, X, y):\n        \n        for fold, (trn_idx, val_idx) in enumerate(skf.split(X, y)):\n            \n            print('\\nFold {}\\n'.format(fold))\n        \n            X_trn_encoded = self.preprocess(X.loc[trn_idx].str.lower())\n            y_trn = y.loc[trn_idx]\n            X_val_encoded = self.preprocess(X.loc[val_idx].str.lower())\n            y_val = y.loc[val_idx]\n        \n            # Callbacks\n            metrics = ClassificationReport(train_data=(X_trn_encoded, y_trn), validation_data=(X_val_encoded, y_val))\n            \n            # Model\n            model = self.build_model()        \n            model.fit(X_trn_encoded, y_trn, validation_data=(X_val_encoded, y_val), callbacks=[metrics], epochs=self.epochs, \n                      batch_size=self.batch_size)\n            \n            self.models.append(model)\n            self.scores[fold] = {\n                'train': {\n                    'precision': metrics.train_precision_scores,\n                    'recall': metrics.train_recall_scores,\n                    'f1': metrics.train_f1_scores                    \n                },\n                'validation': {\n                    'precision': metrics.val_precision_scores,\n                    'recall': metrics.val_recall_scores,\n                    'f1': metrics.val_f1_scores                    \n                }\n            }\n                    \n                \n    def plot_learning_curve(self):\n        \n        fig, axes = plt.subplots(nrows=K, ncols=2, figsize=(20, K * 6), dpi=100)\n    \n        for i in range(K):\n            \n            # Classification Report curve\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.models[i].history.history['val_accuracy'], ax=axes[i][0], label='val_accuracy')\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.scores[i]['validation']['precision'], ax=axes[i][0], label='val_precision')\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.scores[i]['validation']['recall'], ax=axes[i][0], label='val_recall')\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.scores[i]['validation']['f1'], ax=axes[i][0], label='val_f1')        \n\n            axes[i][0].legend() \n            axes[i][0].set_title('Fold {} Validation Classification Report'.format(i), fontsize=14)\n\n            # Loss curve\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.models[0].history.history['loss'], ax=axes[i][1], label='train_loss')\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.models[0].history.history['val_loss'], ax=axes[i][1], label='val_loss')\n\n            axes[i][1].legend() \n            axes[i][1].set_title('Fold {} Train \/ Validation Loss'.format(i), fontsize=14)\n\n            for j in range(2):\n                axes[i][j].set_xlabel('Epoch', size=12)\n                axes[i][j].tick_params(axis='x', labelsize=12)\n                axes[i][j].tick_params(axis='y', labelsize=12)\n\n        plt.show()\n        \n        \n    def predict(self, X):\n        \n        X_test_encoded = self.preprocess(X.str.lower())\n        y_pred = np.zeros((X_test_encoded[0].shape[0], 1))\n\n        for model in self.models:\n            y_pred += model.predict(X_test_encoded) \/ len(self.models)\n\n        return y_pred","47ad27de":"sgd = SGD(1e-3)\nclf = DisasterDetector(bert_layer,sgd, max_seq_length=128, lr=0.0001, epochs=10, batch_size=32)\n\nclf.train(train_set['text'], train_set['target'])","e0a152e4":"y_pred = clf.predict(test_set['text'])\ny_pred_thres = [1 if pred[0] >=0.5 else 0 for pred in y_pred]\ny_pred_df = pd.DataFrame(y_pred_thres, columns=['target'])","255778e3":"sample_subm = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\nid = sample_subm.id","ca350144":"subm = pd.concat([id, y_pred_df], axis=1)","9f3f8137":"subm.to_csv('sample_subm.csv', index=False, header = True)","8471e218":"# Predicting Test File","05eea8a7":"# Training Bert Model"}}