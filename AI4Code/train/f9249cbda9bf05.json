{"cell_type":{"99824e66":"code","55903e12":"code","11bd1c48":"code","ef75cd93":"code","79e49497":"code","0ad51cec":"code","ce1b0cec":"code","b7668b7d":"code","8bc94463":"code","06bb1339":"code","26d18a16":"code","44c82aff":"code","7e514be3":"code","63f631c1":"code","21d9250d":"code","232df129":"code","700695b1":"code","35a86fb5":"code","c10785a4":"code","2bd4b0a5":"code","c0edd055":"code","463abd84":"code","281fcfc9":"code","b06515e7":"code","f78041a0":"code","648cbdd4":"code","31ade4dc":"code","a9cc9176":"code","f3f81bba":"code","719fa146":"code","bc9b5c26":"code","acf7d057":"code","2b1b37fe":"code","d96e8a2f":"code","1c8a8530":"code","08dbfc61":"code","0b99ed7b":"code","20a76645":"code","10340c17":"code","c0ef188e":"code","b0a7b418":"code","f2f44827":"code","c0ce6e23":"code","c454ead1":"code","6c1543a1":"code","17f75f2e":"code","8adb3fde":"code","f7112404":"code","530a416f":"code","b61df445":"code","c5cb9672":"code","3149a7b6":"code","ffc07651":"code","51f7c57e":"code","f7c4745d":"code","c8b83afe":"code","6a7f42fb":"code","672c6434":"code","3c764eee":"code","5256fa31":"code","680d85b5":"code","e7f90cb5":"code","2a256a49":"code","85804e35":"code","efc7c6af":"code","0c89f5c9":"code","488262bc":"code","abd6736d":"code","e1ff2d09":"code","e8828256":"code","aeda01fb":"code","9bf3502b":"code","4024b794":"code","8929bcb4":"code","c300a96d":"code","2c16ef9d":"code","05e63eb2":"code","a5ca04c8":"code","a7c18fac":"code","36d9e5ac":"code","78f26af0":"markdown","6a6730f8":"markdown","9a6d52d5":"markdown","797d2143":"markdown","44cda0ed":"markdown","44c66c7d":"markdown","ed1b4afa":"markdown","ef76ac40":"markdown","c69dcd9e":"markdown","f4ca455d":"markdown","bacc4211":"markdown","919ff20c":"markdown","d31aa8e4":"markdown","5146022d":"markdown","ebcea413":"markdown","39b8ab63":"markdown","e2a83d8e":"markdown","edf78a08":"markdown","5f8511d4":"markdown","219daf7d":"markdown","5e219aa0":"markdown","a4f2dc8a":"markdown","14ff908c":"markdown","bad77970":"markdown","a41d85c5":"markdown","32a60816":"markdown","78d9c04d":"markdown","c4082116":"markdown","01ca34e8":"markdown","07c9ffe9":"markdown","c1d7fac1":"markdown","adbc8230":"markdown","f26f8185":"markdown","9f11ed76":"markdown","b7808188":"markdown","b13654ca":"markdown","ea9cebc3":"markdown","e94dcb43":"markdown","9a182d64":"markdown","bf13c0f8":"markdown","4b58a8bc":"markdown","43b1334e":"markdown","6765a161":"markdown","ad5a4df0":"markdown","89abbb6a":"markdown","7b21cef7":"markdown","ed0d597e":"markdown","01f5966a":"markdown","78051a42":"markdown","bfda5580":"markdown","33b64768":"markdown","e9631e6e":"markdown","4f31cd14":"markdown","d1c5f9b0":"markdown","549b7309":"markdown","56c73a7a":"markdown","4c4ba717":"markdown","92398785":"markdown","28f8defa":"markdown","2088e969":"markdown","20ecc89b":"markdown"},"source":{"99824e66":"import xgboost as xgb\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn')\nimport seaborn as sns\nfrom pprint import pprint","55903e12":"# reading in dataset and viewing it\ndf = pd.read_csv('..\/input\/basic_income_dataset_dalia.csv')\ndf.head()","11bd1c48":"# get the number of rows and columns\nprint(df.shape)\n\n# get datetype info\nprint()\nprint(df.info())","ef75cd93":"# get an overview of the numeric agecolumn (.T = transposing the dataframe's order)\ndf.describe().T","79e49497":"# get an overview of all 14 object columns\/features\ndf.describe(include='object').T","0ad51cec":"df.rename(columns = {'rural':'city_or_rural',\n                     'dem_education_level':'education',\n                     'dem_full_time_job':'full_time_job',\n                     'dem_has_children':'has_children',\n                     'question_bbi_2016wave4_basicincome_awareness':'awareness',\n                     'question_bbi_2016wave4_basicincome_vote':'vote',\n                     'question_bbi_2016wave4_basicincome_effect':'effect',\n                     'question_bbi_2016wave4_basicincome_argumentsfor':'arg_for',\n                     'question_bbi_2016wave4_basicincome_argumentsagainst':'arg_against'},\n          inplace=True)","ce1b0cec":"df.drop(['uuid', 'weight', 'age_group'], axis=1, inplace=True)","b7668b7d":"# new number of rows and columns\ndf.shape","8bc94463":"# checking how much total missing data we have\ndf.isna().sum()","06bb1339":"# in percentage: 7%\nround(df['education'].isna().sum() \/ len(df), 3)","26d18a16":"df.education.unique()","44c82aff":"df.education.value_counts()","7e514be3":"df['education'].fillna('no', inplace=True)","63f631c1":"df.isna().sum().sum()","21d9250d":"df.education.value_counts()","232df129":"# new number of rows and columns\ndf.shape","700695b1":"# check if there are any duplicates\ndf.duplicated().sum()","35a86fb5":"df.drop_duplicates(keep='first', inplace=True)","c10785a4":"# final number of rows and columns\ndf.shape","2bd4b0a5":"# recode voting\ndef vote_coding(row):\n    if row == 'I would vote for it' : return('for')\n    elif row == 'I would probably vote for it': return('for')\n    elif row == 'I would vote against it': return('against')\n    elif row == 'I would probably vote against it': return('against')\n    elif row == 'I would not vote': return('no_action')\n\n# apply function\ndf['vote'] = df['vote'].apply(vote_coding)","c0edd055":"# drop all records who are not \"for\" or \"against\"\ndf = df.query(\"vote != 'no_action'\")","463abd84":"df.vote.value_counts(normalize=True)","281fcfc9":"def awareness_coding(row):\n    if row == 'I understand it fully': return('fully')\n    elif row == 'I know something about it': return('something')\n    elif row == 'I have heard just a little about it': return('little')\n    elif row == 'I have never heard of it': return('nothing')\n\ndf['awareness'] = df['awareness'].apply(awareness_coding)","b06515e7":"def effect_coding(row):\n    if row == '\u2030\u00db_ stop working': return('stop_working')\n    elif row == '\u2030\u00db_ work less': return('work_less')\n    elif row == '\u2030\u00db_ do more volunteering work': return('volunteering_work')\n    elif row == '\u2030\u00db_ spend more time with my family': return('more_family_time')\n    elif row == '\u2030\u00db_ look for a different job': return('different_job')\n    elif row == '\u2030\u00db_ work as a freelancer': return('freelancer')\n    elif row == '\u2030\u00db_ gain additional skills': return('additional_skills')\n    elif row == 'A basic income would not affect my work choices': return('no_effect')\n    else: return('none_of_the_above')\n    \ndf['effect'] = df['effect'].apply(effect_coding).astype(str)","f78041a0":"df.age.describe(percentiles=[.2, .4, .6, .8])","648cbdd4":"def age_groups(row):\n    if row <= 26: return('14_26')\n    elif row <= 35: return('27_35')\n    elif row <= 42: return('36_42')\n    elif row <= 49: return('43_49')\n    else: return('above_50')\n    \ndf['age_group'] = df['age'].apply(age_groups)\ndf.drop(['age'], axis=1, inplace=True)","31ade4dc":"df['age_group'].value_counts(normalize=True).plot(kind='barh', figsize=(8,4));","a9cc9176":"arg_for = ['It reduces anxiety about financing basic needs',\n           'It creates more equality of opportunity',\n           'It encourages financial independence and self-responsibility',\n           'It increases solidarity, because it is funded by everyone',\n           'It reduces bureaucracy and administrative expenses',\n           'It increases appreciation for household work and volunteering',\n           'None of the above']\n\n# count all arguments\ncounter = [0,0,0,0,0,0,0]\n\nfor row in df.iterrows():\n    for i in range(0, len(arg_for)):\n        if arg_for[i] in row[1]['arg_for'].split('|'):\n            counter[i] = counter[i] + 1\n\n# create a new dictionary \ndict_keys = ['less anxiety', 'more equality', 'financial independance', \n             'more solidarity', 'less bureaucracy', 'appreciates volunteering', 'none']\n\narg_dict = {}\n\nfor i in range(0, len(arg_for)):\n    arg_dict[dict_keys[i]] = counter[i]\n\n# sub-df for counted arguments\nsub_df = pd.DataFrame(list(arg_dict.items()), columns=['Arguments PRO basic income', 'count'])\n\n# plot\nsub_df.sort_values(by=['count'], ascending=True).plot(kind='barh', x='Arguments PRO basic income', y='count',  \n                                                      figsize=(10,6), legend=False, color='darkgrey',\n                                                      title='Arguments PRO basic income')\nplt.xlabel('Count'); ","f3f81bba":"df['less_anxiety'] = df['arg_for'].str.contains('anxiety')\ndf['more_equality'] = df['arg_for'].str.contains('equality')","719fa146":"arg_against = ['It is impossible to finance', 'It might encourage people to stop working',\n               'Foreigners might come to my country and take advantage of the benefit',\n               'It is against the principle of linking merit and reward', \n               'Only the people who need it most should get something from the state',\n               'It increases dependence on the state', 'None of the above']\n\n# count all arguments\ncounter = [0,0,0,0,0,0,0]\n\nfor row in df.iterrows():\n    for i in range(0, len(arg_against)):\n        if arg_against[i] in row[1]['arg_against'].split('|'):\n            counter[i] = counter[i] + 1\n\n# create a new dictionary \ndict_keys = ['impossible to finance', 'people stop working', 'foreigners take advantage', \n             'against meritocracy', 'only for people in need', 'more dependence on state', 'none']\n\narg_dict = {}\n\nfor i in range(0, len(arg_against)):\n    arg_dict[dict_keys[i]] = counter[i]\n\n# sub-df for counted arguments\nsub_df = pd.DataFrame(list(arg_dict.items()), columns=['Arguments AGAINST basic income', 'count'])\n\n# plot\nsub_df.sort_values(by=['count'], ascending=True).plot(kind='barh', x='Arguments AGAINST basic income', y='count',  \n                                                      figsize=(10,6), legend=False, color='darkgrey',\n                                                      title='Arguments AGAINST basic income')\nplt.xlabel('Count'); ","bc9b5c26":"df['in_need'] = df['arg_against'].str.contains('need')\ndf['stop_working'] = df['arg_against'].str.contains('stop working')\ndf['too_costly'] = df['arg_against'].str.contains('impossible')","acf7d057":"df.drop(['arg_for', 'arg_against'], axis=1, inplace=True)","2b1b37fe":"df.head()","d96e8a2f":"df.shape","1c8a8530":"df['vote'].value_counts(normalize=True).plot(kind='barh', figsize=(8,4), \n                                             color=['maroon','midnightblue']);","08dbfc61":"from statsmodels.graphics.mosaicplot import mosaic\n\nmosaic(df, ['gender', 'vote'], gap=0.015, title='Vote vs. Gender - Mosaic Chart');","0b99ed7b":"mosaic(df, ['city_or_rural', 'vote'], gap=0.015, title='Vote vs. Area - Mosaic Chart');","20a76645":"mosaic(df, ['full_time_job', 'vote'], gap=0.015, \n       title='Vote vs. Having a Full Time Job or not - Mosaic Chart');","10340c17":"mosaic(df, ['has_children', 'vote'], gap=0.015, \n       title='Vote vs. Having children or not - Mosaic Chart');","c0ef188e":"# Votes depending on having a full-time-job\n\nsub_df = df.groupby('full_time_job')['vote'].value_counts(normalize=True).unstack()\nsub_df.plot(kind='bar', color=['midnightblue', 'maroon'], figsize=(7,4))\nplt.xlabel(\"Full Time Job\")\nplt.xticks(rotation=0)\nplt.ylabel(\"Percentage of Voters\\n\")\nplt.title('\\nVote depending on having a full-time-job\\n', fontsize=14, fontweight='bold')\nplt.legend(bbox_to_anchor=(1.2, 1.0), title='Vote');","b0a7b418":"# Votes in GERMANY and GREECE - depending on having a full-time-job\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,5))\n\n# create sub-df for Germany\nsub_df_1 = df[df['country_code']=='DE'].groupby('full_time_job')['vote'].value_counts(normalize=True).unstack()\nsub_df_1.plot(kind='bar', color = ['midnightblue', 'maroon'], ax=ax1, legend=False)\nax1.set_title('\\nVotes in GERMANY depending on having a full-time-job\\n', fontsize=14, fontweight='bold')\nax1.set_xlabel(\"Full Time Job\")\nax1.set_xticklabels(labels=['No', 'Yes'], rotation=0)\nax1.set_ylabel(\"Percentage of Voters\\n\")\n\n# create sub-df for Greece\nsub_df_2 = df[df['country_code']=='GR'].groupby('full_time_job')['vote'].value_counts(normalize=True).unstack()\nsub_df_2.plot(kind='bar', color = ['midnightblue', 'maroon'], ax=ax2, legend=False)\nax2.set_title('\\nVotes in GREECE depending on having a full-time-job\\n', fontsize=14, fontweight='bold')\nax2.set_xlabel(\"Full Time Job\")\nax2.set_xticklabels(labels=['No', 'Yes'], rotation=0)\nax2.set_ylabel(\"Percentage of Voters\\n\")\n\n# create one legend\nhandles, labels = ax2.get_legend_handles_labels()\nfig.legend(handles, labels, bbox_to_anchor=(0.84, 0.85))\nplt.show();","f2f44827":"# Votes depending on education level\n\nsub_df = df.groupby('education')['vote'].value_counts(normalize=True).unstack()\nsub_df.plot(kind='bar', color = ['midnightblue','maroon'], figsize=(12,5))\nplt.xlabel(\"Education Level\")\nplt.xticks(rotation=0)\nplt.ylabel(\"Percentage of Voters\\n\")\nplt.title('\\nVote depending on education level\\n', fontsize=14, fontweight='bold')\nplt.legend(bbox_to_anchor=(1.15, 1), title='Vote');","c0ce6e23":"# Votes in GERMANY and GREECE - depending on education level\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,5))\n\n# create sub-df for Germany\nsub_df_1 = df[df['country_code']=='DE'].groupby('education')['vote'].value_counts(normalize=True).unstack()\nsub_df_1.plot(kind='bar', color = ['midnightblue', 'maroon'], ax=ax1, legend=False)\nax1.set_title('\\nVotes in GERMANY depending on education level\\n', fontsize=14, fontweight='bold')\nax1.set_xlabel(\"Education Level\")\nax1.set_xticklabels(labels=['High', 'Low', 'Medium', 'No'], rotation=0)\nax1.set_ylabel(\"Percentage of Voters\\n\")\n\n# create df for Greece\nsub_df_2 = df[df['country_code']=='GR'].groupby('education')['vote'].value_counts(normalize=True).unstack()\nsub_df_2.plot(kind='bar', color = ['midnightblue', 'maroon'], ax=ax2, legend=False)\nax2.set_title('\\nVotes in GREECE depending on education level\\n', fontsize=14, fontweight='bold')\nax2.set_xlabel(\"Education Level\")\nax2.set_xticklabels(labels=['High', 'Low', 'Medium', 'No'], rotation=0)\n\n# create one legend\nhandles, labels = ax2.get_legend_handles_labels()\nfig.legend(handles, labels, bbox_to_anchor=(0.83, 0.85))\nplt.show();","c454ead1":"# Votes in 4 countries - depending on education level\n\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14,10))\n\n# create sub-df for Germany\nsub_df_1 = df[df['country_code']=='DE'].groupby('education')['vote'].value_counts(normalize=True).unstack()\nsub_df_1.plot(kind='bar', color = ['midnightblue', 'maroon'], ax=ax1, legend=False)\nax1.set_title('\\nVotes in GERMANY depending on education level\\n', fontsize=14, fontweight='bold')\nax1.set_xlabel(\"Education Level\")\nax1.set_xticklabels(labels=['High', 'Low', 'Medium', 'No'], rotation=0)\nax1.set_ylabel(\"Percentage of Voters\\n\")\n\n# create sub-df for France\nsub_df_2 = df[df['country_code']=='FR'].groupby('education')['vote'].value_counts(normalize=True).unstack()\nsub_df_2.plot(kind='bar', color = ['midnightblue', 'maroon'], ax=ax2, legend=False)\nax2.set_title('\\nVotes in France depending on education level\\n', fontsize=14, fontweight='bold')\nax2.set_xlabel(\"Education Level\")\nax2.set_xticklabels(labels=['High', 'Low', 'Medium', 'No'], rotation=0)\nax2.set_ylabel(\"Percentage of Voters\\n\")\n\n# create sub-df for Italy\nsub_df_3 = df[df['country_code']=='IT'].groupby('education')['vote'].value_counts(normalize=True).unstack()\nsub_df_3.plot(kind='bar', color = ['midnightblue', 'maroon'], ax=ax3, legend=False)\nax3.set_title('\\nVotes in Italy depending on education level\\n', fontsize=14, fontweight='bold')\nax3.set_xlabel(\"Education Level\")\nax3.set_xticklabels(labels=['High', 'Low', 'Medium', 'No'], rotation=0)\n\n# create sub-df for Slovakia\nsub_df_4 = df[df['country_code']=='SK'].groupby('education')['vote'].value_counts(normalize=True).unstack()\nsub_df_4.plot(kind='bar', color = ['midnightblue', 'maroon'], ax=ax4, legend=False)\nax4.set_title('\\nVotes in Slovakia depending on education level\\n', fontsize=14, fontweight='bold')\nax4.set_xlabel(\"Education Level\")\nax4.set_xticklabels(labels=['High', 'Low', 'Medium', 'No'], rotation=0)\n\n# create only one legend\nhandles, labels = ax2.get_legend_handles_labels()\nfig.legend(handles, labels, bbox_to_anchor=(1.0, 0.95))\nplt.tight_layout()\nplt.show();","6c1543a1":"# Votes depending on awareness\n\nsub_df = df.groupby('awareness')['vote'].value_counts(normalize=True).unstack()\nsub_df.plot(kind='bar', color=['midnightblue', 'maroon'], figsize=(7,4))\nplt.xlabel(\"Awareness\")\nplt.xticks(rotation=0)\nplt.ylabel(\"Percentage of Voters\\n\")\nplt.title('\\nVote depending on awareness\\n', fontsize=14, fontweight='bold')\nplt.legend(bbox_to_anchor=(1.2, 1.0), title='Vote');","17f75f2e":"# Votes depending on age\n\nsub_df = df.groupby('age_group')['vote'].value_counts(normalize=True).unstack()\nsub_df.plot(kind='bar', color=['midnightblue', 'maroon'], figsize=(9,5))\nplt.xlabel(\"Age Group\")\nplt.xticks(rotation=0)\nplt.ylabel(\"Percentage of Voters\\n\")\nplt.title('\\nVote depending on age\\n', fontsize=14, fontweight='bold')\nplt.legend(bbox_to_anchor=(1.2, 1.0), title='Vote');","8adb3fde":"# Votes depending on effect\n\nsub_df = df.groupby('effect')['vote'].value_counts(normalize=True).unstack()\nsub_df.plot(kind='bar', color=['midnightblue', 'maroon'], figsize=(14,5))\nplt.xlabel(\"\\nEffect of Basic Income\")\nplt.xticks(rotation=0)\nplt.ylabel(\"Percentage of Voters\\n\")\nplt.title('\\nVote depending on effect\\n', fontsize=14, fontweight='bold')\nplt.legend(bbox_to_anchor=(1.1, 1.0), title='Vote');","f7112404":"# plot votes in 4 countries - depending on education level\n\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16,12))\n\n# create sub-df for those who agree\/disagree with the argument:\n# \"It reduces anxiety about financing basic needs\"\nsub_df_1 = df.groupby('less_anxiety')['vote'].value_counts(normalize=True).unstack()\nsub_df_1.plot(kind='bar', color = ['midnightblue', 'maroon'], ax=ax1, legend=False)\nax1.set_title('\\nVotes depending on attitude towards reducing_anxiety\\n', fontsize=14, fontweight='bold')\nax1.set_xlabel('\"It reduces anxiety about financing basic needs\"')\nax1.set_xticklabels(labels=['False', 'True'], rotation=0)\nax1.set_ylabel(\"Percentage of Voters\\n\")\n\n# create sub-df for those who agree\/disagree with the argument:\n# \"It creates more equality of opportunity\"\nsub_df_2 = df.groupby('more_equality')['vote'].value_counts(normalize=True).unstack()\nsub_df_2.plot(kind='bar', color = ['midnightblue', 'maroon'], ax=ax2, legend=False)\nax2.set_title('\\nVotes depending on attitude towards more_equality\\n', fontsize=14, fontweight='bold')\nax2.set_xlabel('\"It creates more equality of opportunity\"')\nax2.set_xticklabels(labels=['False', 'True'], rotation=0)\n\n# create sub-df for those who agree\/disagree with the argument:\n# \"It might encourage people to stop working\"\nsub_df_3 = df.groupby('stop_working')['vote'].value_counts(normalize=True).unstack()\nsub_df_3.plot(kind='bar', color = ['midnightblue', 'maroon'], ax=ax3, legend=False)\nax3.set_title('\\nVotes depending on attitude towards people_stop_working\\n', fontsize=14, fontweight='bold')\nax3.set_xlabel('\"It might encourage people to stop working\"')\nax3.set_xticklabels(labels=['False', 'True'], rotation=0)\nax3.set_ylabel(\"Percentage of Voters\\n\")\n\n# create sub-df for those who agree\/disagree with the argument:\n# \"Only the people who need it most should get something from the state\"\nsub_df_4 = df.groupby('in_need')['vote'].value_counts(normalize=True).unstack()\nsub_df_4.plot(kind='bar', color = ['midnightblue', 'maroon'], ax=ax4, legend=False)\nax4.set_title('\\nVotes depending on attitude towards only_for_people_in_need\\n', fontsize=14, fontweight='bold')\nax4.set_xlabel('\"Only the people who need it most should get something from the state\"')\nax4.set_xticklabels(labels=['False', 'True'], rotation=0)\n\n# create only one legend\nhandles, labels = ax2.get_legend_handles_labels()\nfig.legend(handles, labels, bbox_to_anchor=(1.0, 0.95))\nplt.tight_layout()\nplt.show();","530a416f":"# define our features \nfeatures = df.drop([\"vote\"], axis=1)\n\n# define our target\ntarget = df[[\"vote\"]]\n\n# create dummy variables\nfeatures = pd.get_dummies(features)","b61df445":"print(features.shape)\nfeatures.tail(2)","c5cb9672":"print(target.shape)","3149a7b6":"# import train_test_split function\nfrom sklearn.model_selection import train_test_split\n\n# import LogisticRegression\nfrom sklearn.linear_model import LogisticRegression\n\n# import metrics\nfrom sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n\n# suppress all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","ffc07651":"# split our data\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2)","51f7c57e":"# instantiate the logistic regression\nlogreg = LogisticRegression()\n\n# train\nlogreg.fit(X_train, y_train)\n\n# predict\ntrain_preds = logreg.predict(X_train)\ntest_preds = logreg.predict(X_test)\n\n# evaluate\ntrain_accuracy_logreg = accuracy_score(y_train, train_preds)\ntest_accuracy_logreg = accuracy_score(y_test, test_preds)\nreport_logreg = classification_report(y_test, test_preds)\n\nprint(\"Logistic Regression\")\nprint(\"------------------------\")\nprint(f\"Training Accuracy: {(train_accuracy_logreg * 100):.4}%\")\nprint(f\"Test Accuracy:     {(test_accuracy_logreg * 100):.4}%\")\n\n# store accuracy in a new dataframe\nscore_logreg = ['Logistic Regression', train_accuracy_logreg, test_accuracy_logreg]\nmodels = pd.DataFrame([score_logreg])","f7c4745d":"# import random forest classifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV","c8b83afe":"# create a baseline\nforest = RandomForestClassifier()","6a7f42fb":"# create Grid              \nparam_grid = {'n_estimators': [80, 100, 120],\n              'criterion': ['gini', 'entropy'],\n              'max_features': [5, 7, 9],         \n              'max_depth': [5, 8, 10], \n              'min_samples_split': [2, 3, 4]}\n\n# instantiate the tuned random forest\nforest_grid_search = GridSearchCV(forest, param_grid, cv=3, n_jobs=-1)\n\n# train the tuned random forest\nforest_grid_search.fit(X_train, y_train)\n\n# print best estimator parameters found during the grid search\nprint(forest_grid_search.best_params_)","672c6434":"# instantiate the tuned random forest with the best found parameters\n# here I use the parameters originally got back from GridSearch in the first round\nforest = RandomForestClassifier(n_estimators=120, criterion='gini', max_features=9, \n                                max_depth=10, min_samples_split=4, random_state=4)\n\n# train the random forest\nforest.fit(X_train, y_train)\n\n# predict\ntrain_preds = forest.predict(X_train)\ntest_preds = forest.predict(X_test)\n\n# evaluate\ntrain_accuracy_forest = accuracy_score(y_train, train_preds)\ntest_accuracy_forest = accuracy_score(y_test, test_preds)\nreport_forest = classification_report(y_test, test_preds)\n\nprint(\"Random Forest\")\nprint(\"-------------------------\")\nprint(f\"Training Accuracy: {(train_accuracy_forest * 100):.4}%\")\nprint(f\"Test Accuracy:     {(test_accuracy_forest * 100):.4}%\")\n\n# append accuracy score to our dataframe\nscore_forest = ['Random Forest', train_accuracy_forest, test_accuracy_forest]\nmodels = models.append([score_forest])","3c764eee":"# create a baseline\nbooster = xgb.XGBClassifier()","5256fa31":"# create Grid\nparam_grid = {'n_estimators': [100],\n              'learning_rate': [0.05, 0.1], \n              'max_depth': [3, 5, 10],\n              'colsample_bytree': [0.7, 1],\n              'gamma': [0.0, 0.1, 0.2]}\n\n# instantiate the tuned random forest\nbooster_grid_search = GridSearchCV(booster, param_grid, scoring='accuracy', cv=3, n_jobs=-1)\n\n# train the tuned random forest\nbooster_grid_search.fit(X_train, y_train)\n\n# print best estimator parameters found during the grid search\nprint(booster_grid_search.best_params_)","680d85b5":"# instantiate tuned xgboost\nbooster = xgb.XGBClassifier(learning_rate=0.1, max_depth=5, n_estimators=100,\n                            colsample_bytree=0.7, gamma=0.1, random_state=4)\n\n# train\nbooster.fit(X_train, y_train)\n\n# predict\ntrain_preds = booster.predict(X_train)\ntest_preds = booster.predict(X_test)\n\n# evaluate\ntrain_accuracy_booster = accuracy_score(y_train, train_preds)\ntest_accuracy_booster = accuracy_score(y_test, test_preds)\nreport_booster = classification_report(y_test, test_preds)\n\nprint(\"XGBoost\")\nprint(\"-------------------------\")\nprint(f\"Training Accuracy: {(train_accuracy_booster * 100):.4}%\")\nprint(f\"Test Accuracy:     {(test_accuracy_booster * 100):.4}%\")\n\n# append accuracy score to our dataframe\nscore_booster = ['XGBoost', train_accuracy_booster, test_accuracy_booster]\nmodels = models.append([score_booster])","e7f90cb5":"from sklearn import svm","2a256a49":"# instantiate Support Vector Classification\nsvm = svm.SVC(kernel='rbf', random_state=4)\n\n# train\nsvm.fit(X_train, y_train)\n\n# predict\ntrain_preds = svm.predict(X_train)\ntest_preds = svm.predict(X_test)\n\n# evaluate\ntrain_accuracy_svm = accuracy_score(y_train, train_preds)\ntest_accuracy_svm = accuracy_score(y_test, test_preds)\nreport_svm = classification_report(y_test, test_preds)\n\nprint(\"Support Vector Machine\")\nprint(\"-------------------------\")\nprint(f\"Training Accuracy: {(train_accuracy_svm * 100):.4}%\")\nprint(f\"Test Accuracy:     {(test_accuracy_svm * 100):.4}%\")\n\n# append accuracy score to our dataframe\nscore_svm = ['Support Vector Machine', train_accuracy_svm, test_accuracy_svm]\nmodels = models.append([score_svm])","85804e35":"models","efc7c6af":"models.columns = ['Classifier', 'Training Accuracy', \"Testing Accuracy\"]\nmodels.set_index(['Classifier'], inplace=True)\n# sort by testing accuracy\nmodels.sort_values(['Testing Accuracy'], ascending=[False])","0c89f5c9":"print('Classification Report XGBoost: \\n', report_booster)\nprint('------------------------------------------------------')\nprint('Classification Report Logistic Regression: \\n', report_logreg)\nprint('------------------------------------------------------')\nprint('Classification Report SVM: \\n', report_svm)\nprint('------------------------------------------------------')\nprint('Classification Report Random Forest: \\n', report_forest)","488262bc":"from imblearn.over_sampling import SMOTE","abd6736d":"# view previous class distribution\nprint(target['vote'].value_counts()) \n\n# resample data ONLY using training data\nX_resampled, y_resampled = SMOTE().fit_sample(X_train, y_train) \n\n# view synthetic sample class distribution\nprint(pd.Series(y_resampled).value_counts()) ","e1ff2d09":"# then perform ususal train-test-split\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, random_state=0)","e8828256":"# instantiate the logistic regression\nlogreg2 = LogisticRegression()\n\n# train\nlogreg2.fit(X_train, y_train)\n\n# predict\ntrain_preds = logreg2.predict(X_train)\ntest_preds = logreg2.predict(X_test)\n\n# evaluate\ntrain_accuracy_logreg2 = accuracy_score(y_train, train_preds)\ntest_accuracy_logreg2 = accuracy_score(y_test, test_preds)\nreport_logreg2 = classification_report(y_test, test_preds)\n\nprint(\"Logistic Regression with balanced classes\")\nprint(\"------------------------\")\nprint(f\"Training Accuracy: {(train_accuracy_logreg2 * 100):.4}%\")\nprint(f\"Test Accuracy:     {(test_accuracy_logreg2 * 100):.4}%\")\n\n# store accuracy in a new dataframe\nscore_logreg2 = ['Logistic Regression balanced', train_accuracy_logreg2, test_accuracy_logreg2]\nmodels2 = pd.DataFrame([score_logreg2])","aeda01fb":"# instantiate the random forest with the best found parameters\nforest2 = RandomForestClassifier(n_estimators=120, criterion='gini', max_features=9, \n                                 max_depth=10, min_samples_split=4, random_state=4)\n\n# train the random forest\nforest2.fit(X_train, y_train)\n\n# predict\ntrain_preds = forest2.predict(X_train)\ntest_preds = forest2.predict(X_test)\n\n# evaluate\ntrain_accuracy_forest2 = accuracy_score(y_train, train_preds)\ntest_accuracy_forest2 = accuracy_score(y_test, test_preds)\nreport_forest2 = classification_report(y_test, test_preds)\n\nprint(\"Random Forest with balanced classes\")\nprint(\"-------------------------\")\nprint(f\"Training Accuracy: {(train_accuracy_forest2 * 100):.4}%\")\nprint(f\"Test Accuracy:     {(test_accuracy_forest2 * 100):.4}%\")\n\n# append accuracy score to our dataframe\nscore_forest2 = ['Random Forest balanced', train_accuracy_forest2, test_accuracy_forest2]\nmodels2 = models2.append([score_forest2])","9bf3502b":"# instantiate tuned xgboost\nbooster2 = xgb.XGBClassifier(learning_rate=0.1, max_depth=5, n_estimators=100,\n                            colsample_bytree=0.7, gamma=0.1, random_state=4)\n\n# train\nbooster2.fit(X_train, y_train)\n\n# predict\ntrain_preds = booster2.predict(X_train)\ntest_preds = booster2.predict(X_test)\n\n# evaluate\ntrain_accuracy_booster2 = accuracy_score(y_train, train_preds)\ntest_accuracy_booster2 = accuracy_score(y_test, test_preds)\nreport_booster2 = classification_report(y_test, test_preds)\n\nprint(\"XGBoost with balanced classes\")\nprint(\"-------------------------\")\nprint(f\"Training Accuracy: {(train_accuracy_booster2 * 100):.4}%\")\nprint(f\"Test Accuracy:     {(test_accuracy_booster2 * 100):.4}%\")\n\n# append accuracy score to our dataframe\nscore_booster2 = ['XGBoost balanced', train_accuracy_booster2, test_accuracy_booster2]\nmodels2 = models2.append([score_booster2])","4024b794":"from sklearn import svm","8929bcb4":"# instantiate Support Vector Classification\nsvm2 = svm.SVC(kernel='rbf')\n\n# train\nsvm2.fit(X_train, y_train)\n\n# predict\ntrain_preds = svm2.predict(X_train)\ntest_preds = svm2.predict(X_test)\n\n# evaluate\ntrain_accuracy_svm2 = accuracy_score(y_train, train_preds)\ntest_accuracy_svm2 = accuracy_score(y_test, test_preds)\nreport_svm2 = classification_report(y_test, test_preds)\n\nprint(\"Support Vector Machine with balanced classes\")\nprint(\"-------------------------\")\nprint(f\"Training Accuracy: {(train_accuracy_svm2 * 100):.4}%\")\nprint(f\"Test Accuracy:     {(test_accuracy_svm2 * 100):.4}%\")\n\n# append accuracy score to our dataframe\nscore_svm2 = ['SVM balanced', train_accuracy_svm2, test_accuracy_svm2]\nmodels2 = models2.append([score_svm2])","c300a96d":"models2","2c16ef9d":"# Accuracy for balanced data\nmodels2.columns = ['Classifier balanced', 'Training Accuracy', \"Testing Accuracy\"]\nmodels2.set_index(['Classifier balanced'], inplace=True)\nmodels2.sort_values(['Testing Accuracy'], ascending=[False])","05e63eb2":"# Accuracy for imbalanced data\nmodels.sort_values(['Testing Accuracy'], ascending=[False])","a5ca04c8":"print('Classification Report XGBoost: \\n', report_booster2)\nprint('------------------------------------------------------')\nprint('Classification Report Logistic Regression: \\n', report_logreg2)\nprint('------------------------------------------------------')\nprint('Classification Report SVM: \\n', report_svm2)\nprint('------------------------------------------------------')\nprint('Classification Report Random Forest: \\n', report_forest2)","a7c18fac":"# plot the important features - based on XGBoost\nfeat_importances = pd.Series(booster.feature_importances_, index=features.columns)\nfeat_importances.nlargest(10).sort_values().plot(kind='barh', color='darkgrey', figsize=(10,5))\nplt.xlabel('Relative Feature Importance with XGBoost');","36d9e5ac":"# plot the important features - based on Random Forest\nfeat_importances = pd.Series(forest.feature_importances_, index=features.columns)\nfeat_importances.nlargest(10).sort_values().plot(kind='barh', color='darkgrey', figsize=(10,5))\nplt.xlabel('Relative Feature Importance with Random Forest');","78f26af0":"Machine learning algorithms generally need all data - including categorical data - in numeric form. To satisfy these algorithms, categorical features are converted into separate binary features called dummy variables.\nTherefore, we have to find a way to represent these variables as numbers before handing them off to the model. One usual way is **one-hot encoding**, which creates a new column for each unique category in a categorical variable. Each observation receives a 1 in the column for its corresponding category and a 0 in all other new columns. To conduct one-hot encoding, we use the **pandas get_dummies function.**","6a6730f8":"* Next, let's build **new age groups** according to the 0.2 percentiles, and then drop the numeric \"age\" column:","9a6d52d5":"**Random Forest:**","797d2143":"* Someone who \"probably votes for\" basic income, will vote the same way as someone who \"votes for it\" - namely with \"yes\". The same holds for rejection. Our target is to first predict whether someone is for or against basic income. We're not particularly interested in someone who has no opinion and\/or won't vote, so let's **simplify our target** and recode the answers to drop all records that won't take any clear action:","44cda0ed":"# Table of contents\n<a id='Table of contents'><\/a>\n\n### <a href='#1. Obtaining and Viewing the Data'>1. Obtaining and Viewing the Data<\/a>\n\n### <a href='#2. Preprocessing the Data'>2. Preprocessing the Data<\/a>\n\n* <a href='#2.1. Renaming Columns'>2.1. Renaming Columns<\/a>\n* <a href='#2.2. Excluding Unrelated Data'>2.2. Excluding Unrelated Data<\/a>\n* <a href='#2.3. Dealing with Misleading Data'>2.3. Dealing with Misleading Data<\/a>\n* <a href='#2.4. Dealing with Missing Data'>2.4. Dealing with Missing Data<\/a>\n* <a href='#2.5. Dealing with Duplicate Data'>2.5. Dealing with Duplicate Data<\/a>\n* <a href='#2.6. Basic Feature Extraction and Engineering'>2.6. Basic Feature Extraction and Engineering<\/a>\n\n### <a href='#3. Data Visualization'>3. Data Visualization<\/a>\n* <a href='#3.1. Mosaic Plots'>3.1. Mosaic Plots<\/a>\n* <a href='#3.2. Bar Charts'>3.2. Bar Charts<\/a>\n\n### <a href='#4. Machine Learning'>4. Machine Learning<\/a>\n\n* <a href='#4.1. Recoding Categorical Features'>4.1. Recoding Categorical Features<\/a>\n* <a href='#4.2. Training a Logistic Regression'>4.2. Training a Logistic Regression<\/a>\n* <a href='#4.3. Training a Random Forest Classifier'>4.3. Training a Random Forest Classifier<\/a>\n* <a href='#4.4. Training an XGBoost Classifier'>4.4. Training an XGBoost Classifier<\/a>\n* <a href='#4.5. Training a Support Vector Machine'>4.5. Training a Support Vector Machine<\/a>\n* <a href='#4.6. Model Comparison'>4.6. Model Comparison<\/a>\n* <a href='#4.7. Balancing the Data'>4.7. Balancing the Data<\/a>\n* <a href='#4.8. Model Comparison II'>4.8. Model Comparison II<\/a>\n\n### <a href='#5. Conclusions'>5. Conclusions<\/a>\n* <a href='#5.1. Feature Importance'>5.1. Feature Importance<\/a>\n* <a href='#5.2. Recommendation'>5.2. Recommendation<\/a>","44c66c7d":"Going back to our intial question: **\"Can we predict how people are likely to vote?\"**, we can now conclude that the answer is **\"Yes\"**.\n\nWith an XGBoost and the SMOTE algorithm to account for imbalanced classes in our target variable, we were able to both improve our accuracy to nearly 84% - in addition to removing any bias towards the yes-voters. So as fas as classification algorithms go, let's use the **XGBoost**.","ed1b4afa":"##### Vote vs. age group","ef76ac40":"By looking at the number of records we have in each class, we see that roughly 70% of the votes are for a basic income, as opposed to 30% against.","c69dcd9e":"**Success! Balancing our data has removed the bias towards the more prevalent class.**","f4ca455d":"**SVM:**","bacc4211":"*Back to: <a href='#Table of contents'> Table of contents<\/a>*\n#### 4.2. Training a Logistic Regression\n<a id='4.2. Training a Logistic Regression'><\/a>","919ff20c":"* Lastly, let's extract the **2 or 3 most mentioned arguments PRO** and the **2 or 3 most mentioned arguments CONTRA** a basic income, and build new columns with boolean values:","d31aa8e4":"Let's start by obtaining and investigating the pandas DataFrame:","5146022d":"*Back to: <a href='#Table of contents'> Table of contents<\/a>*\n### 4. Machine Learning\n<a id='4. Machine Learning'><\/a>","ebcea413":"Gradient Boosting is one of the most powerful concepts in machine learning right now. The term Gradient Boosting refers to a class of algorithms, rather than any single one. The version with the highest performance right now is XGBoost, which is short for eXtreme Gradient Boosting. XGBoost is a great choice for classification tasks. It provides best-in-class performance compared to other classification algorithms (with the exception of Deep Learning).","39b8ab63":"We've now balanced our dataset. Did balancing our dataset improved the models' bias?","e2a83d8e":"Next, let's run a Random Forest Classifier with predefined specifications or \"hyperparameters\". Some of the important ones to tune for a Random Forest are:\n\n* n_estimators = number of trees\n* criterion = splitting criterion (for maximizing the information gain from each split)\n* max_features = max number of features considered for splitting a node\n* max_depth = max number of levels in each decision tree\n* min_samples_split = min samples needed to make a split","edf78a08":"### 1. Obtaining and Viewing the Data\n<a id='1. Obtaining and Viewing the Data'><\/a>","5f8511d4":"Indeed, we have some duplicates, so let's drop them:","219daf7d":"**Logistic Regression:**","5e219aa0":"# Predicting How Someone Will Vote on Basic Income\n\n### The dataset\nThis study on basic income across Europe was conducted by Dalia Research in April 2016 across 28 EU member states. The sample of n = 9.649 was drawn from all 28 states, taking into account current population distributions with regards to age (14-65 years), gender, and region\/country. The dataset is available on kaggle: https:\/\/www.kaggle.com\/daliaresearch\/basic-income-survey-european-dataset\/home\n\nThe dataset contains **9649 records and 15 columns**. These include demographics such as age, gender, education etc. It also includes opinions on the effects of a basic income on someone's work choices, their familiarity with the idea of a basic income, convincing arguments for and against a basic income - and of course, whether they ultimately approve or reject the idea.\n\nOur **goal** in this notebook is **to predict how people are likely to vote**. The target variable originally consisted of multiple classes, however, it was converted to a binary outcome. We thus have a typical classification task to solve. Several different classification models such as Logistic Regression, Random Forest, XGBoost, and Support Vector Machine (SVM) are built, optimized, evaluated, and compared. Additionally, balancing the data using the SMOTE algorithm is applied to remove a bias in prediction.\n\n### The OSEMiN-approach\n\nThe OSEMiN Process is an acronym that rhymes with \u201cawesome\u201d and stands for **Obtain, Scrub, Explore, Model, and iNterpret**. It can be used as a blueprint for working on data problems using machine learning tools. Preprocessing involves scrubbing (also called cleaning) and exploring the data. Building the model, evaluating, and optimizing it make up the process of machine learning.","a4f2dc8a":"##### Vote vs. expected effects","14ff908c":"##### Vote vs. Awareness","bad77970":"*Back to: <a href='#Table of contents'> Table of contents<\/a>*\n### 3. Data Visualization\n<a id='3. Data Visualization'><\/a>","a41d85c5":"#### 2.2. Excluding Unrelated Data\n<a id='2.2. Excluding Unrelated Data'><\/a>","32a60816":"*Back to: <a href='#Table of contents'> Table of contents<\/a>*\n#### 4.6. Model Comparison\n<a id='4.6. Model Comparison'><\/a>","78d9c04d":"*Back to: <a href='#Table of contents'> Table of contents<\/a>*\n#### 4.7. Balancing the Data\n<a id='4.7. Balancing the Data'><\/a>","c4082116":"#### 3.2. Bar Charts\n<a id='3.2. Bar Charts'><\/a>","01ca34e8":"*Back to: <a href='#Table of contents'> Table of contents<\/a>*\n#### 4.8. Model Comparison II\n<a id='4.8. Model Comparison II'><\/a>","07c9ffe9":"*Back to: <a href='#Table of contents'> Table of contents<\/a>*","c1d7fac1":"Some of the important hyperparameters to tune for an XGBoost are:\n\n* n_estimators = number of trees\n* learning_rate = rate at which our model learns patterns in data (After every round, it shrinks the feature weights to reach the best optimum:)\n* max_depth = max number of levels in each decision tree\n* colsample_bytree = similar to max_features (max number of features considered for splitting a node)\n* gamma = specifies the minimum loss reduction required to make a split","adbc8230":"##### Vote vs. arguments","f26f8185":"#### 4.3. Training a Random Forest Classifier\n<a id='4.3. Training a Random Forest Classifier'><\/a>","9f11ed76":"This last plot, produced by our tuned Random Forest, draws the most distinct picture. We see that for the Random Forest, the attitude towards the argument **\"It creates more equality of opportunity\"** is the most important feature to split people.","b7808188":"#### 4.4. Training an XGBoost Classifier\n<a id='4.4. Training an XGBoost Classifier'><\/a>","b13654ca":"All the data makes perfect sense; there is nothing to correct.","ea9cebc3":"*Back to: <a href='#Table of contents'> Table of contents<\/a>*\n### 5. Conclusions\n<a id='5. Conclusions'><\/a>","e94dcb43":"#### 2.3. Dealing with Misleading Data\n<a id='2.3. Dealing with Misleading Data'><\/a>","9a182d64":"Data visualization is an important step that lies between preprocessing and model buildung. It serves as a sanity check for the features and target, and may help explore the relationship between both. This will guide us in model building, and assist us in our understanding of the model and predictions. The target is what we are asked to predict: a \"yes\" to basic income, or a \"no\". \n\n**We should first examine the number of votes that fall into each category.**","bf13c0f8":"#### 2.6. Basic Feature Extraction and Engineering\n<a id='2.6. Basic Feature Extraction and Engineering'><\/a>","4b58a8bc":"#### 2.4. Dealing with Missing Data\n<a id='2.4. Dealing with Missing Data'><\/a>","43b1334e":"Now that we have run several models, let's check the testing accuracy we stored in a dataframe on the side as well as the classification reports:","6765a161":"##### Vote vs. Education Level","ad5a4df0":"When approaching a supervised learning problem like ours, we should always use multiple algorithms and compare the performances of the various models. Sometimes simplest is best, and so we will start by applying logistic regression. Logistic regression makes use of what's called the logistic function to calculate the odds that a given data point belongs to a given class. Once we have more models, we can compare them based on a few performance metrics.\n\nBefore we start, let's prepare our work and import all the libraries we need for classifying our data:","89abbb6a":"#### 5.1. Feature Importance \n<a id='5.1. Feature Importance'><\/a>","7b21cef7":"*Back to: <a href='#Table of contents'> Table of contents<\/a>*\n### 2. Preprocessing the Data\n<a id='2. Preprocessing the Data'><\/a>","ed0d597e":"All our models have done similarly well, boasting a **weighted average** F1 score between 72% to 76%. However, looking at our classification report, we can see that the *\"for\"* votes are fairly well classified, but *\"against\"* votes are disproportionately misclassified.\n\nWhy might this be the case? Well, just by looking at the number of data points we have for each class, we see that we have far more data points for the *\"for\"* votes than for *\"against\"* votes, potentially skewing our model's ability to distinguish between classes. This also tells us that most of our model's accuracy is only driven by its ability to classify the *\"for\"* votes, which is less than ideal.\n\nTo account for our imbalanced dataset, we can use an oversampling algorithm called SMOTE (Synthetic Minority Oversampling Technique). SMOTE uses the nearest neighbors of observations to create synthetic data. It's important to know is that we **only oversample the training data** - that way, none of the information in the validation data is used to create synthetic observations.","01f5966a":"#### 4.5. Training a Support Vector Machine\n<a id='4.5. Training a Support Vector Machine'><\/a>","78051a42":"#### 3.1. Mosaic Plots\n<a id='3.1. Mosaic Plots'><\/a>","bfda5580":"##### Vote vs. Full Time Job","33b64768":"663 missing values - that is of no small concern! These records likely represent people with no formal education who may have been averse to disclosing that information, or who thought that giving no answer would be the right answer. So I decide to fill the NaN's with **no** formal education:","e9631e6e":"The column labels are quite wordy. Let's change that:","4f31cd14":"Again, our target is to predict how people are likely to vote. Hence, features should be included only if they're suspected to be related to the target variable. As the goal of supervised classification is to predict the target, features that obviously have nothing to do with the target should be excluded.\n\nBoth variables, the **uuid** and the **weight** (given to obtain census-representative results), are irrelevant for our classification task here. As we want to construct our own age groups later, we will also drop the predefined **age group**:","d1c5f9b0":"Another fast and popular classification technique is: Support Vector Machines (also referred to as SVMs). The idea behind SVMs is that we perform classification by finding the seperation line, or \"hyperplane\", that best differentiates between two classes.","549b7309":"**XGBoost:**","56c73a7a":"As we come to an end, let's take a look at the 10 most important features for predicting someone's vote. For the sake of simplicity we use the **XGBoost** classifier and the **Random Forest** classifier:","4c4ba717":"#### 4.1. Recoding Categorical Features\n<a id='4.1. Recoding Categorical Features'><\/a>","92398785":"#### 2.1. Renaming Columns \n<a id='2.1. Renaming Columns'><\/a>","28f8defa":"#### 2.5. Dealing with Duplicate Data\n<a id='2.5. Dealing with Duplicate Data'><\/a>","2088e969":"#### 5.2. Recommendation \n<a id='5.2. Recommendation'><\/a>","20ecc89b":"* Another two columns, **\"awareness\" and \"effect\"**, contain whole sentences that need to be shortened to one word to then be ready to be processed later:"}}