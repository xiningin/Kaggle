{"cell_type":{"71b0ea09":"code","444753dd":"code","1dd68fb2":"code","c190fce9":"code","54e8bbc4":"code","bd632301":"code","35dc1655":"code","97a826b2":"code","b5ef0347":"code","d69728a4":"markdown","100fb992":"markdown","5bb48928":"markdown","4063d159":"markdown","3cea6b55":"markdown","235de373":"markdown","2b33fe9e":"markdown"},"source":{"71b0ea09":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nfrom numpy.random import randn,random,normal,rand\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib as ml\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","444753dd":"def view_images(matrices,rows,cols):\n    fig,axes = plt.subplots(figsize=(10,10),nrows=rows,ncols=cols,sharex=True,sharey=True)\n    for axis,image in zip(axes.flatten(),matrices):\n        axis.xaxis.set_visible(False)\n        axis.yaxis.set_visible(False)\n        im = axis.imshow(1-image.reshape((2,2)),cmap='Greys_r')\n    return fig,axes\n\n# Viewing sample data\n# Original pictures\nsamples = [np.array([1,0,0,1]),\n           np.array([0.9,0.1,0.2,0.8]),\n           np.array([0.9,0.2,0.1,0.8]),\n           np.array([0.8,0.1,0.2,0.9]),\n           np.array([0.8,0.2,0.1,0.9])]\nprint(\"PLOTTING ORIGINAL IMAGE SAMPLES - \")\n_ = view_images(matrices = samples,rows = 1,cols = 4)","1dd68fb2":"#Viewing sample data\n# Noisy data\nnoise = [np.random.randn(2,2) for i in range(12)]    # 10 images\nprint(\"PLOTTING NOISY IMAGE SAMPLES - \")\n_ = view_images(matrices = noise,rows = 3,cols = 4)","c190fce9":"# Defining the function for the sigmoid activation function\ndef sigmoid(x):\n    return np.exp(x)\/(1.0 + np.exp(x))\n\n# Building the Discriminator class\nclass Discriminator():\n    def __init__(self):\n        self.weights = np.array([np.random.normal() for i in range(4)])\n        self.bias = np.random.normal()\n    # Forward pass\n    def forward(self,X):\n        return sigmoid(np.dot(X,self.weights) + self.bias)\n    # Error calculation from image\n    def error_from_image(self,image):\n        pred = self.forward(image)\n        # We want positive prediction, so E = -ln(D(x)) = -ln(pred)\n        return -np.log(pred)\n    # Obtaining derivative values before back propagation\n    def deriv_from_images(self,image):\n        dx = self.forward(image)\n        d_wts = -(1-dx)*image\n        d_bias = -(1-dx)\n        return d_wts,d_bias\n    # Updating the weights for Discriminator trained on real images\n    def update_from_image(self,x):\n        dervs = self.deriv_from_images(x)\n        self.weights -= learning_rate*dervs[0]\n        self.bias -= learning_rate*dervs[1]\n    # Error calculation from noise\n    def error_from_noise(self,noise):\n        pred = self.forward(noise)\n        # We want negative prediction, so E = -ln(1-D(x)) = -ln(1-pred)\n        return -np.log(1-pred)\n    # Obtaining derivative values fro back propagation (Point no. 4)\n    def deriv_from_noise(self,noise):\n        dx = self.forward(noise)\n        dwts_n = (dx)*noise\n        dbias_n = dx\n        return dwts_n,dbias_n\n    # Updating the weights for Discriminator trained on noisy images\n    def update_from_noise(self,noise):\n        dervs = self.deriv_from_noise(noise)\n        self.weights -= learning_rate*dervs[0]\n        self.bias -= learning_rate*dervs[1]","54e8bbc4":"# Building the Generatoe class\nclass Generator():\n    def __init__(self):\n        self.weights = np.array([np.random.normal() for i in range(4)])\n        self.bias = np.array([np.random.normal() for i in range(4)])\n    # Forward pass\n    def forward(self,z):\n        return sigmoid(z*self.weights + self.bias)\n    # Calculating error in a Generator produced image\n    def error(self,z,d):\n        x = self.forward(z)\n        y = self.forward(x)\n        return -np.log(y)\n    # Get derivatives wrt weights\n    def deriv(self,z,d):\n        d_wts = d.weights\n        d_bs = d.bias\n        x = self.forward(z)\n        y = d.forward(x)\n        factor = -(1-y)*d_wts*x*(1-x)\n        dwts_g = factor*z\n        dbs_g = factor\n        return dwts_g,dbs_g\n    # Updating Discriminator weights\n    def update(self,z,d):\n        err_before = self.error(z,d)\n        ders = self.deriv(z,d)\n        self.weights -= learning_rate*ders[0]\n        self.bias -= learning_rate*ders[1]\n        err_after = self.error(z,d)","bd632301":"np.random.seed(45)        # Settiing seed value\n\n# Set hyperparameters\nlearning_rate = 0.01\nepochs = 25000\n\n# Setting up the GAN\n# 1. The Discriminator\nD = Discriminator()\n# 2. The Generator\nG = Generator()\n\nerr_D,err_G = [],[]   # To store error values and plot them later.\nfor e in range(epochs):\n    for s in samples:\n        # Doing step-by-step\n        # 1. Updating D with weights from real image\n        D.update_from_image(s)\n        # 2. Picking a random value of z to initiate the process of producing a Generator.\n        z = rand()\n        # 3. Calculate Discriminator error\n        ''' Error = -ln(D(x)) - ln(1-D(x)) \n                  = -ln(D(s)) - ln(1-D(z)) '''\n        err_D.append(sum(D.error_from_image(s) + D.error_from_noise(z)))\n        # 4. Calculate Generator error\n        ''' Error = -ln(D(G(z))) '''\n        err_G.append(G.error(z,D))\n        # 5. Build the fake image\n        noise = G.forward(z)\n        # 6. Backpropagate to update discriminator weights from fake face\n        D.update_from_noise(noise)\n        # 7. Backpropagate to update generator weights from fake face\n        G.update(z,D)","35dc1655":"# 1. Plotting\ngenerated_images = []\nfor i in range(4):\n    z = rand()\n    generated_image = G.forward(z)\n    generated_images.append(generated_image)\n_ = view_images(generated_images, 1, 4)\n\n# Printing the matrix values\nfor img in generated_images:\n    print(img.reshape(2,2),\"\\n\")","97a826b2":"# 2. Plotting the error vs epoch lapse for both the Discriminator and the Generator\nplt.figure(figsize=(10,5))\nplt.plot(err_G,color='red',label=\"Generator\")\nplt.plot(err_D,color='blue',label=\"Discriminator\")\nplt.xlabel(\"EPOCHS --->\")\nplt.title(\"ERROR VS EPOCHS FOR BOTH GENERATOR(RED) AND DISCRIMINATOR(BLUE)\")\nplt.show()","b5ef0347":"# 3. Printing the weights and bias related to each Network.\n# The Discriminator\nprint(\"THE DISCRIMINATOR : \\nThe final weights = {}\\nThe final bias = {}\".format(D.weights,D.bias))\nprint(\"\\nTHE GENERATOR : \\nThe final weights = {}\\nThe final biases = {}\".format(G.weights,G.bias))","d69728a4":"# 2. BUILDING THE NEURAL NETWORK\nUnderstanding the mathematics behind the neural networks in a GAN\n\n## The Discriminator\n\n![](https:\/\/github.com\/BALaka-18\/gans\/blob\/master\/discriminator.png?raw=true)\n\nCalculating the errors and derivatives for updating the weights during back propagation(Point number 2) using the output from the sigmoid activation function, D(x).\n\n![](https:\/\/github.com\/BALaka-18\/gans\/blob\/master\/discriminator_math.png?raw=true)\n\n### Why do we use the sigmoid and not any other mathematical function ? \n![](https:\/\/github.com\/BALaka-18\/images\/blob\/master\/5.png?raw=true)\n\nThis shows that, for outputs closer to 1, the sigmoid function yields higher values. For outputs closer to 0, it yields lower values.\n\nSo, a high output of the sigmoid function tells us that the picture is most probably closer to being an original image(basic test)\n\n________________________________________________________________________________________________________________________________________________________________________________\n\n### A little more about the error from images and from noise :\n\nLet us consider the Discriminator's outputs are binary lables : 0 for noise, 1 for real images.\n\nWhen the label is 1, or, the situation demands 1 as an ideal label(say when real images are passed into the Discriminator), then the error E = -ln(D(x))\n\n![](https:\/\/github.com\/BALaka-18\/images\/blob\/master\/1_2.png?raw=true)\n\n![](https:\/\/github.com\/BALaka-18\/images\/blob\/master\/3.png?raw=true)\n________________________________________________________________________________________________________________________________________________________________________________\nWhen the label is 0, or, the situation demands 0 as an ideal label(say when noisy images are passed into the Discriminator), then the error E = -ln(1-D(x))\n\n![](https:\/\/github.com\/BALaka-18\/images\/blob\/master\/2.png?raw=true)\n\n![](https:\/\/github.com\/BALaka-18\/images\/blob\/master\/4.png?raw=true)","100fb992":"# 4. GENERATING THE IMAGES\n1. Plotting the images\n2. Plotting the error vs epoch lapse for both the Discriminator and the Generator\n3. Printing out the final weights and bias values of the Discriminator and the Generator","5bb48928":"# GANs - Generative Adversarial Networks\nIt basically consists of two competing neural networks that are constantly engaged in a fight with each other. These two networks are : the Generator and the Discriminator.\nLet us take a common example :\n\nThe investigator is the Discriminator, the forger is the Generator.\n\n1. The forger is trying to fool the investigator by generating pictures that almost resemble the original pictures seen by the investigator. \n2. The investigator assigns the pictures labels : 0 for No, 1 for Yes : for each picture generated by the forger. \n3. The result of the investigator is used by the forger as information, to generate better pictures with each pass.\n4. This continues till the forger produces an image that fools the  investigator\n\nExpressing in terms of GANs :\n\n![](https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2017\/06\/11000153\/g1.jpg)\n\n1. The Discriminator is trained on original pictures(forward pass). The Generator is frozen(read: no training). \n2. The weights of the Discriminator weights are updated.\n\n![](https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2017\/06\/14204616\/s1.jpg)\n\n3. The Generator is trained(forward pass) and the Discriminator is frozen(only outputs, no training).\n\n![](https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2017\/06\/14204626\/s21.png)\n\n4. The Discriminator is then trained on the noisy data, the wieghts are updated.\n5. The Generator weights are updated(depends on the discriminator weights and output).\n6. This continues till the epochs are exhausted.\n\n# OBJECTIVE OF THIS NOTEBOOK\n### Here, we are building a pair of simple one-layered GANs which generate very basic 2x2 black and white images. The original image reperesents a backslash('\\')\n\n![](data:image\/png;base64,iVBORw0KGgoAAAANSUhEUgAAAj8AAACLCAYAAACOaPh9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+\/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAADiUlEQVR4nO3cQUpbURSA4feqghtwJnTiRHAQSBbg0l1AxA04sOKsCu7gdthJhUp9edb\/+8aPyyEcbn4SknmMMQEAVHxbewAAgEMSPwBAivgBAFLEDwCQIn4AgBTxAwCkHL\/n4Xmec7+L3263a49wUA8PD9Pz8\/O81Pl2qOH29vZ5jHG21PnFPdpsNmuPcFCPj4\/Ty8vLYnfR0dHRODk5Wer4T+nq6mrtEQ7urbtofs\/\/\/BQvnNr\/IO12u2m\/34ufD1TboWmapnmeb8cYuwXPz72or6+va49wUNfX19Pd3d1id9Hp6ek4Pz9f6vhP6f7+fu0RDu6tu8jXXgBAivgBAFLEDwCQIn4AgBTxAwCkiB8AIEX8AAAp4gcASBE\/AECK+AEAUsQPAJAifgCAFPEDAKSIHwAgRfwAACniBwBIET8AQIr4AQBSxA8AkCJ+AIAU8QMApIgfACBF\/AAAKeIHAEgRPwBAivgBAFLEDwCQIn4AgBTxAwCkiB8AIEX8AAAp4gcASBE\/AECK+AEAUsQPAJAifgCAFPEDAKSIHwAgRfwAACniBwBIET8AQIr4AQBSxA8AkCJ+AIAU8QMApIgfACBF\/AAAKeIHAEgRPwBAivgBAFLEDwCQIn4AgBTxAwCkiB8AIEX8AAAp4gcASBE\/AECK+AEAUo7f8\/B2u532+\/1Ss3xK8zyvPcKXstlsppubm7XHOKiLi4u1R\/hyindRbY+enp4WPf\/y8jJ3F3k\/+80nPwBAivgBAFLEDwCQIn4AgBTxAwCkiB8AIEX8AAAp4gcASBE\/AECK+AEAUsQPAJAifgCAFPEDAKSIHwAgRfwAACniBwBIET8AQIr4AQBSxA8AkCJ+AIAU8QMApIgfACBF\/AAAKeIHAEgRPwBAivgBAFLEDwCQIn4AgBTxAwCkiB8AIEX8AAAp4gcASBE\/AECK+AEAUsQPAJAifgCAFPEDAKSIHwAgRfwAACniBwBIET8AQIr4AQBSxA8AkCJ+AIAU8QMApIgfACBF\/AAAKeIHAEgRPwBAivgBAFLEDwCQIn4AgBTxAwCkiB8AIEX8AAAp4gcASBE\/AECK+AEAUsQPAJAifgCAlHmM8fcPz\/PPaZp+LDcOn8D3McbZUofboQx7xL+yQ3yEP+7Ru+IHAOB\/52svACBF\/AAAKeIHAEgRPwBAivgBAFLEDwCQIn4AgBTxAwCkiB8AIOUXpZRsGiREe9EAAAAASUVORK5CYII=%0A)\n\nAs it is evident, since the picture is a Backslash, so in the 2 x 2 image matrix, the cells [0][0] and [1][1] (along the main diagonal) will have the highest values (0.7 - 1)(i.e, they'll be the darkest) ; and the other two cells will have low values.\n\nSteps we'll follow :\n1. Create the original pictures.\n2. Generate noisy data.\n3. Build the GAN :\n    1. Build the Discriminator class.\n    2. Build the Genarator class.\n4. Training phase\n5. Generation phase\n    1. Display the generated images.\n    2. Display final weights of the generator and discriminator\n6. Plot errors of both discriminator and the generator for all epochs.","4063d159":"## The Generator\n\n![](https:\/\/github.com\/BALaka-18\/gans\/blob\/master\/generator.png?raw=true)\n\nCalculating the errors and derivatives for updating the weights during back propagation(Point number 2) using the output from the sigmoid activation function, D(x), and also the Generator, G(z)\n![](https:\/\/github.com\/BALaka-18\/gans\/blob\/master\/generator_math.png?raw=true)\n\n","3cea6b55":"# 3. TRAINING THE GAN","235de373":"# 1. PLOTTING FUNCTION\n","2b33fe9e":"#### Source of Inspiration for writing this notebook : Luis Serrano"}}