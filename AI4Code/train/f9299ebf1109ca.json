{"cell_type":{"96acf7b1":"code","093aafc2":"code","7fb8bbb0":"code","3f1470fb":"code","f2fb2fdc":"code","e6148452":"code","d0838619":"code","1fb0c9cc":"code","ed807f39":"code","d1f18143":"code","10e123c6":"code","a9b20a02":"code","884262d3":"code","08627d48":"code","9cce03dd":"code","d14a6079":"code","3d3ad5d0":"code","21bbdcee":"code","c5c1a455":"code","e97cb3aa":"markdown","f767687b":"markdown","98bb4f31":"markdown","062a02d7":"markdown","a2e1ca09":"markdown","3ccd821f":"markdown","7536c258":"markdown","3f55f82e":"markdown","e00e8eab":"markdown","28bc0804":"markdown","e7d144e3":"markdown","ea6cfacf":"markdown","6f415250":"markdown","44c3cf73":"markdown","63ea973e":"markdown","2243e5cf":"markdown","d9948cf4":"markdown","4a53681f":"markdown","d89725f4":"markdown","84804276":"markdown","b55b566c":"markdown","33a2b90d":"markdown","1dcf7da0":"markdown","91f19883":"markdown","c06f2788":"markdown","7e250b7e":"markdown","4b67cbf8":"markdown","dc8c68e9":"markdown","d98bf2df":"markdown","07e64a16":"markdown"},"source":{"96acf7b1":"import numpy as np\n\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\nx = np.random.rand(100, 1)\ny = 1 + 2 * x + .3 * np.random.randn(100, 1)\n\n# Shuffles the indices\nidx = np.arange(100)\n#np.random.shuffle(idx)\n\n# Uses first 80 random indices for train\ntrain_idx = idx[:80]\n# Uses the remaining indices for validation\nval_idx = idx[80:]\n\n# Generates train and validation sets\nx_train, y_train = x[train_idx], y[train_idx]\nx_val, y_val = x[val_idx], y[val_idx]\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2,figsize=(15,5))\nfig.suptitle('Synthetic data \u2014 Train and Validation sets')\n\nax1.scatter(x_train, y_train)\nax1.set_title('Training set')\n\nax2.scatter(x_val, y_val,color='red')\nax2.set_title('Validation set')\n\n","093aafc2":"# Initializes parameters \"a\" and \"b\" randomly\nfrom random import random\nnp.random.seed(42)\n#a = np.random.randn(1)\n#b = np.random.randn(1)\na=random()\nb=random()\n\na0=a\nb0=b\n\nprint(\"Initializes parameters a and b randomly\")\nprint(\"a=\",a0, \"b=\",b0)\nprint(\"------------------------------------------------------------\")\ndef implement_Grd(n_epochs,lr,a,b,x,y):\n    for epoch in range(n_epochs):\n        # Computes our model's predicted output\n        yhat = a + b * x_train\n\n        # How wrong is our model? That's the error! \n        error = (y_train - yhat)\n        # It is a regression, so it computes mean squared error (MSE)\n        loss = (error ** 2).mean()\n\n        # Computes gradients for both \"a\" and \"b\" parameters\n        a_grad = -2 * error.mean()\n        b_grad = -2 * (x_train * error).mean()\n\n        # Updates parameters using gradients and the learning rate\n        a = a - lr * a_grad\n        b = b - lr * b_grad  \n        \n    return  a,b\n         \nfig, axes = plt.subplots(2, 2,figsize=(15,11))  \n\nn=[100, 5000]\nr=[0.1, 0.01]\nfor i in range(2):\n    for j in range(2):\n        a,b=implement_Grd(n[i],r[j],a0,b0,x,y)\n        print(\"Final parameters a and b where n epochs=\"+ str(n[i]) + \" & lr=\"+ str(r[j]))\n        print(\"a=\",a, \"b=\",b)\n        print(\"------------------------------------------------------------\")\n        axes[i, j].plot(x, a*x + b, label=\"Final parameters\")\n        axes[i, j].plot(x, a0*x + b0, label=\"Initial parameters\")\n        axes[i, j].scatter(x, y)\n        title='n epochs='+ str(n[i]) + ' & lr='+ str(r[j])\n        axes[i, j].set_title(title)\n        axes[i, j].legend(bbox_to_anchor=(0.01, 0.99), loc='upper left', borderaxespad=0.)\nplt.show()","7fb8bbb0":"from sklearn.linear_model import LinearRegression\nlinr = LinearRegression()\nlinr.fit(x_train, y_train)\nprint(\"Final parameters a and b where n epochs=\"+ str(n[i]) + \" & lr=\"+ str(r[j]))\nprint(\"a=\",linr.intercept_, \"b=\",linr.coef_[0])\nprint(\"------------------------------------------------------------\")              \nfig, ax = plt.subplots(1,figsize=(8,5))\nax.scatter(x_train, y_train)\nax.set_title('Scikit-Learn\u2019s Linear Regression-Training set')\nax.plot(x, linr.intercept_*x + linr.coef_[0], label=\"Scikit-Learn\u2019s Linear Regression-Training set\")","3f1470fb":"pip install torchviz","f2fb2fdc":"import torch\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torchviz import make_dot\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Our data was in Numpy arrays, but we need to transform them into PyTorch's Tensors\n# and then we send them to the chosen device\nx_train_tensor = torch.from_numpy(x_train).float().to(device)\ny_train_tensor = torch.from_numpy(y_train).float().to(device)\n\n# Here we can see the difference - notice that .type() is more useful\n# since it also tells us WHERE the tensor is (device)\nprint(type(x_train), type(x_train_tensor), x_train_tensor.type())","e6148452":"# FIRST\n# Initializes parameters \"a\" and \"b\" randomly, ALMOST as we did in Numpy\n# since we want to apply gradient descent on these parameters, we need\n# to set REQUIRES_GRAD = TRUE\na = torch.randn(1, requires_grad=True, dtype=torch.float)\nb = torch.randn(1, requires_grad=True, dtype=torch.float)\nprint(a, b)\n\n# SECOND\n# But what if we want to run it on a GPU? We could just send them to device, right?\na = torch.randn(1, requires_grad=True, dtype=torch.float).to(device)\nb = torch.randn(1, requires_grad=True, dtype=torch.float).to(device)\nprint(a, b)\n# Sorry, but NO! The to(device) \"shadows\" the gradient...\n\n# THIRD\n# We can either create regular tensors and send them to the device (as we did with our data)\na = torch.randn(1, dtype=torch.float).to(device)\nb = torch.randn(1, dtype=torch.float).to(device)\n# and THEN set them as requiring gradients...\na.requires_grad_()\nb.requires_grad_()\nprint(a, b)","d0838619":"# We can specify the device at the moment of creation - RECOMMENDED!\ntorch.manual_seed(42)\na = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\nb = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\nprint(a, b)","1fb0c9cc":"lr = 1e-1\nn_epochs = 10\n\ntorch.manual_seed(42)\na = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\nb = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n\nfor epoch in range(n_epochs):\n    yhat = a + b * x_train_tensor\n    error = y_train_tensor - yhat\n    loss = (error ** 2).mean()\n\n    # No more manual computation of gradients! \n    # a_grad = -2 * error.mean()\n    # b_grad = -2 * (x_tensor * error).mean()\n    \n    # We just tell PyTorch to work its way BACKWARDS from the specified loss!\n    loss.backward()\n    # Let's check the computed gradients...\n\n    print(\"a.grad=\",a.grad, \"b.grad=\",b.grad)\n    # What about UPDATING the parameters? Not so fast...\n    \n    # FIRST ATTEMPT\n    # AttributeError: 'NoneType' object has no attribute 'zero_'\n    # a = a - lr * a.grad\n    # b = b - lr * b.grad\n    # print(a)\n\n    # SECOND ATTEMPT\n    # RuntimeError: a leaf Variable that requires grad has been used in an in-place operation.\n    # a -= lr * a.grad\n    # b -= lr * b.grad        \n    \n    # THIRD ATTEMPT\n    # We need to use NO_GRAD to keep the update out of the gradient computation\n    # Why is that? It boils down to the DYNAMIC GRAPH that PyTorch uses...\n    with torch.no_grad():\n        a -= lr * a.grad\n        b -= lr * b.grad\n    \n    # PyTorch is \"clingy\" to its computed gradients, we need to tell it to let it go...\n    a.grad.zero_()\n    b.grad.zero_()\n    \nprint(a, b)","ed807f39":"torch.manual_seed(42)\na = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\nb = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\nprint(\"Befor\")\nprint(a, b)\nprint(\"===================================\")\nlr = 1e-1\nn_epochs = 1000\n\n# Defines a SGD optimizer to update the parameters\noptimizer = optim.SGD([a, b], lr=lr)\n\nfor epoch in range(n_epochs):\n    yhat = a + b * x_train_tensor\n    error = y_train_tensor - yhat\n    loss = (error ** 2).mean()\n\n    loss.backward()    \n    \n    # No more manual update!\n    # with torch.no_grad():\n    #     a -= lr * a.grad\n    #     b -= lr * b.grad\n    optimizer.step()\n    \n    # No more telling PyTorch to let gradients go!\n    # a.grad.zero_()\n    # b.grad.zero_()\n    optimizer.zero_grad()\n    \nprint(\"After\")\nprint(a, b)\nprint(\"===================================\")","d1f18143":"torch.manual_seed(42)\na = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\nb = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\nprint(\"Befor\")\nprint(a, b)\nprint(\"===================================\")\n\nlr = 1e-1\nn_epochs = 1000\n\n# Defines a MSE loss function\nloss_fn = nn.MSELoss(reduction='mean')\n\noptimizer = optim.SGD([a, b], lr=lr)\n\nfor epoch in range(n_epochs):\n    yhat = a + b * x_train_tensor\n    \n    # No more manual loss!\n    # error = y_tensor - yhat\n    # loss = (error ** 2).mean()\n    loss = loss_fn(y_train_tensor, yhat)\n\n    loss.backward()    \n    optimizer.step()\n    optimizer.zero_grad()\n    \nprint(\"After\")\nprint(a, b)\nprint(\"===================================\")","10e123c6":"class ManualLinearRegression(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # To make \"a\" and \"b\" real parameters of the model, we need to wrap them with nn.Parameter\n        self.a = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n        self.b = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n        \n    def forward(self, x):\n        # Computes the outputs \/ predictions\n        return self.a + self.b * x","a9b20a02":"torch.manual_seed(42)\n\n# Now we can create a model and send it at once to the device\nmodel = ManualLinearRegression().to(device)\n# We can also inspect its parameters using its state_dict\nprint(model.state_dict())\n\nlr = 1e-1\nn_epochs = 1000\n\nloss_fn = nn.MSELoss(reduction='mean')\noptimizer = optim.SGD(model.parameters(), lr=lr)\n\nfor epoch in range(n_epochs):\n    # Set the model to training mode\n    model.train()\n\n    # No more manual prediction!\n    # yhat = a + b * x_tensor\n    yhat = model(x_train_tensor)\n    \n    loss = loss_fn(y_train_tensor, yhat)\n    loss.backward()    \n    optimizer.step()\n    optimizer.zero_grad()\n    \nprint(model.state_dict())","884262d3":"def make_train_step(model, loss_fn, optimizer):\n    # Builds function that performs a step in the train loop\n    def train_step(x, y):\n        # Sets model to TRAIN mode\n        model.train()\n        # Makes predictions\n        yhat = model(x)\n        # Computes loss\n        loss = loss_fn(y, yhat)\n        # Computes gradients\n        loss.backward()\n        # Updates parameters and zeroes gradients\n        optimizer.step()\n        optimizer.zero_grad()\n        # Returns the loss\n        return loss.item()\n    \n    # Returns the function that will be called inside the train loop\n    return train_step\n\n# Creates the train_step function for our model, loss function and optimizer\ntrain_step = make_train_step(model, loss_fn, optimizer)\nlosses = []\n\n# For each epoch...\nfor epoch in range(n_epochs):\n    # Performs one train step and returns the corresponding loss\n    loss = train_step(x_train_tensor, y_train_tensor)\n    losses.append(loss)\n    \n# Checks model's parameters\nprint(model.state_dict())","08627d48":"from torch.utils.data import Dataset, TensorDataset\n\nclass CustomDataset(Dataset):\n    def __init__(self, x_tensor, y_tensor):\n        self.x = x_tensor\n        self.y = y_tensor\n        \n    def __getitem__(self, index):\n        return (self.x[index], self.y[index])\n\n    def __len__(self):\n        return len(self.x)\n\n# Wait, is this a CPU tensor now? Why? Where is .to(device)?\nx_train_tensor = torch.from_numpy(x_train).float()\ny_train_tensor = torch.from_numpy(y_train).float()\n\ntrain_data = CustomDataset(x_train_tensor, y_train_tensor)\nprint(train_data[0])\n\ntrain_data = TensorDataset(x_train_tensor, y_train_tensor)\nprint(train_data[0])","9cce03dd":"from torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(dataset=train_data, batch_size=16, shuffle=True)","d14a6079":"next(iter(train_loader))","3d3ad5d0":"losses = []\ntrain_step = make_train_step(model, loss_fn, optimizer)\n\nfor epoch in range(n_epochs):\n    for x_batch, y_batch in train_loader:\n        # the dataset \"lives\" in the CPU, so do our mini-batches\n        # therefore, we need to send those mini-batches to the\n        # device where the model \"lives\"\n        x_batch = x_batch.to(device)\n        y_batch = y_batch.to(device)\n        \n        loss = train_step(x_batch, y_batch)\n        losses.append(loss)\n        \nprint(model.state_dict())","21bbdcee":"from torch.utils.data.dataset import random_split\n\nx_tensor = torch.from_numpy(x).float()\ny_tensor = torch.from_numpy(y).float()\n\ndataset = TensorDataset(x_tensor, y_tensor)\n\ntrain_dataset, val_dataset = random_split(dataset, [80, 20])\n\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=16)\nval_loader = DataLoader(dataset=val_dataset, batch_size=20)","c5c1a455":"losses = []\nval_losses = []\ntrain_step = make_train_step(model, loss_fn, optimizer)\n\nfor epoch in range(n_epochs):\n    for x_batch, y_batch in train_loader:\n        x_batch = x_batch.to(device)\n        y_batch = y_batch.to(device)\n\n        loss = train_step(x_batch, y_batch)\n        losses.append(loss)\n        \n    with torch.no_grad():\n        for x_val, y_val in val_loader:\n            x_val = x_val.to(device)\n            y_val = y_val.to(device)\n            \n            model.eval()\n\n            yhat = model(x_val)\n            val_loss = loss_fn(y_val, yhat)\n            val_losses.append(val_loss.item())\n\nprint(model.state_dict())","e97cb3aa":"# 4-PyTorch\nFirst, we need to cover a few basic concepts.\n## Tensor\nIn Deep Learning, we see tensors everywhere,In Numpy, you may have an array that has three dimensions, right? That is, technically speaking, a tensor.\n* A scalar (a single number) has zero dimensions.\n* A vector has one dimension. \n* A matrix has two dimensions. \n* A tensor has three or more dimensions.\n","f767687b":"## Data Generation\nLet\u2019s start generating some synthetic data: we start with a vector of 100 points for our feature x and create our labels using a = 1, b = 2 and some Gaussian noise.\nNext, let\u2019s split our synthetic data into train and validation sets, shuffling the array of indices and using the first 80 shuffled points for training.","98bb4f31":"In the __init__ method, we define our two parameters, a and b, using the Parameter() class, to tell PyTorch these tensors should be considered parameters of the model they are an attribute of.\n\nMoreover, we can get the current values for all parameters using our model\u2019s state_dict() method.\n\nWe can use all these handy methods to change our code, which should be looking like this:","062a02d7":"## Creating Parameters\n\nThe latter tensors require the computation of their gradients, so we can update their values. That\u2019s what the requires_grad=True argument is good for. It tells PyTorch we want it to compute gradients for us.\n\nYou may be tempted to create a simple tensor for a parameter and, later on, send it to your chosen device, as we did with our data.","a2e1ca09":"# 6-Optimizer\nSo far, we\u2019ve been manually updating the parameters using the computed gradients. That\u2019s probably fine for two parameters\u2026 but what if we had a whole lot of them?! We use one of PyTorch\u2019s optimizers, like SGD or Adam.\n\nAn optimizer takes the parameters we want to update, the learning rate we want to use (and possibly many other hyper-parameters as well!) and performs the updates through its step() method.\n\nBesides, we also don\u2019t need to zero the gradients one by one anymore. We just invoke the optimizer\u2019s zero_grad() method and that\u2019s it!","3ccd821f":"We can also go the other way around, turning tensors back into Numpy arrays, using numpy(). It should be easy as x_train_tensor.numpy(),Unfortunately, Numpy cannot handle GPU tensors\u2026 you need to make them CPU tensors first using cpu().","7536c258":"How does this change our training loop? Let\u2019s check it out!","3f55f82e":"\n## Loading Data, Devices and CUDA\n\n### How do we go from Numpy\u2019s arrays to PyTorch\u2019s tensors?, and we want to use my fancy GPU\u2026\nNo worries, that\u2019s what to() is good for,it sends your tensor to whatever device you specify, including your GPU (referred to as cuda or cuda:0).\n\n### What if I want my code to fallback to CPU if no GPU is available?\n\nYou can use cuda.is_available() to find out if you have a GPU at your disposal and set your device accordingly.AndYou can also easily cast it to a lower precision (32-bit float) using float().","e00e8eab":"# 5- Autograd\n\nAutograd is PyTorch\u2019s automatic differentiation package.we need to invoke the backward() method from the corresponding Python variable, like, loss.backward().\n\nIf you check the method\u2019s documentation, it clearly states that gradients are accumulated. So, every time we use the gradients to update the parameters, we need to zero the gradients afterwards. And that\u2019s what zero_() is good for.\n\nSo, let\u2019s ditch the manual computation of gradients and use both backward() and zero_() methods instead.","28bc0804":"In PyTorch, models have a train() method which does NOT perform a training step. Its only purpose is to set the model to training mode. ","e7d144e3":"Two things are different now: not only we have an inner loop to load each and every mini-batch from our DataLoader but, more importantly, we are now sending only one mini-batch to the device.\n\n\nSo far, we\u2019ve focused on the training data only. We built a dataset and a data loader for it. We could do the same for the validation data, using the split we performed at the beginning of this post\u2026 or we could use random_split instead.","ea6cfacf":"# 9-Training Step\n\nSo far, we\u2019ve defined an optimizer, a loss function and a model. Scroll up a bit and take a quick look at the code inside the loop. Would it change if we were using a different optimizer, or loss, or even model? If not, how can we make it more generic?\n\nWell, I guess we could say all these lines of code perform a training step, given those three elements (optimizer, loss and model),the features and the labels.\n\nSo, how about writing a function that takes those three elements and returns another function that performs a training step, taking a set of features and labels as arguments and returning the corresponding loss?\n\nThen we can use this general-purpose function to build a train_step() function to be called inside our training loop. Now our code should look like this\u2026 see how tiny the training loop is now?\n","6f415250":"Although the last approach worked fine, it is much better to assign tensors to a device at the moment of their creation.","44c3cf73":"# 13-References\n\n1. https:\/\/towardsdatascience.com\/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e\n\n1. Stevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications Company.","63ea973e":"# 10-Dataset\n\nIn PyTorch, a dataset is represented by a regular Python class that inherits from the Dataset class. You can think of it as a kind of a Python list of tuples, each tuple corresponding to one point (features, label).\n\nThe most fundamental methods it needs to implement are:\n\n* __init__(self) : it takes whatever arguments needed to build a list of tuples \u2014 it may be the name of a CSV file that will be loaded and processed; it may be two tensors, one for features, another one for labels; or anything else, depending on the task at hand.\n\n* __get_item__(self, index): it allows the dataset to be indexed, so it can work like a list (dataset[i]) \u2014 it must return a tuple (features, label) corresponding to the requested data point. We can either return the corresponding slices of our pre-loaded dataset or tensors or, as mentioned above, load them on demand.\n\n\n* __len__(self): it should simply return the size of the whole dataset so, whenever it is sampled, its indexing is limited to the actual size.\n\n\n\nLet\u2019s build a simple custom dataset that takes two tensors as arguments: one for the features, one for the labels. For any given index, our dataset class will return the corresponding slice of each of those tensors. It should look like this:\n","2243e5cf":"# Table of Contents\n1. A Simple Regression Problem\n1. Gradient Descent\n1. Linear Regression in Numpy\n1. PyTorch\n1. Autograd\n1. Optimizer\n1. Loss\n1. Model\n1. Training Step\n1. Dataset\n1. DataLoader\n1. Evaluation","d9948cf4":"# 11-DataLoader\nUntil now, we have used the whole training data at every training step. It has been batch gradient descent all along. This is fine for our ridiculously small dataset, sure, but if we want to go serious about all this, we must use mini-batch gradient descent. Thus, we need mini-batches. Thus, we need to slice our dataset accordingly. Do you want to do it manually?! Me neither!\n\nSo we use PyTorch\u2019s DataLoader class for this job. We tell it which dataset to use (the one we just built in the previous section), the desired mini-batch size and if we\u2019d like to shuffle it or not. That\u2019s it!\n\nOur loader will behave like an iterator, so we can loop over it and fetch a different mini-batch every time.","4a53681f":"# 12-Evaluation\n\nThis is the last part of our journey \u2014 we need to change the training loop to include the evaluation of our model, that is, computing the validation loss. The first step is to include another inner loop to handle the mini-batches that come from the validation loader , sending them to the same device as our model. Next, we make predictions using our model (line 23) and compute the corresponding loss (line 24).\n\nThat\u2019s pretty much it, but there are two small, yet important, things to consider:\n* torch.no_grad(): even though it won\u2019t make a difference in our simple model, it is a good practice to wrap the validation inner loop with this context manager to disable any gradient calculation that you may inadvertently trigger \u2014 gradients belong in training, not in validation steps;\n\n* eval(): the only thing it does is setting the model to evaluation mode (just like its train() counterpart did), so the model can adjust its behavior regarding some operations, like Dropout.\n\nNow, our training loop should look like this:","d89725f4":"To retrieve a sample mini-batch, one can simply run the command below \u2014 it will return a list containing two tensors, one for the features, another one for the labels.","84804276":"# Motivation\n\nPyTorch is easy to recommend because of its simplicity. Many researchers and practitioners\nfind it easy to learn, use, extend, and debug. It\u2019s Pythonic, and while like any\ncomplicated domain it has caveats and best practices, using the library generally feels\nfamiliar to developers who have used Python previously.\n\n\nPyTorch also has a compelling story for the transition from research and development\ninto production. While it was initially focused on research workflows, PyTorch\nhas been equipped with a high-performance C++ runtime that can be used to deploy\nmodels for inference without relying on Python, and can be used for designing and\ntraining models in C++. It has also grown bindings to other languages and an interface\nfor deploying to mobile devices. These features allow us to take advantage of\nPyTorch\u2019s flexibility and at the same time take our applications where a full Python\nruntime would be hard to get or would impose expensive overhead.\n","b55b566c":"# 1- A Simple Regression Problem\nFor this reason, in this tutorial, I will stick with a simple and familiar problem: a linear regression with a single feature x! It doesn\u2019t get much simpler than that","33a2b90d":"# 8-Model\nIn PyTorch, a model is represented by a regular Python class that inherits from the Module class\n\n* __init__(self): it defines the parts that make up the model \u2014in our case, two parameters, a and b.\n* forward(self, x): it performs the actual computation, that is, it outputs a prediction, given the input x.\n\nLet\u2019s build a proper (yet simple) model for our regression task. It should look like this:","1dcf7da0":"# 2-Gradient Descent\nI\u2019ll cover the four basic steps you\u2019d need to go through to compute it.\n## Step 1: Compute the Loss\nFor a regression problem, the loss is given by the Mean Square Error (MSE), that is, the average of all squared differences between labels (y) and predictions (a + bx).\n![image.png](attachment:image.png)\n## Step 2: Compute the Gradients\nA gradient is a partial derivative \u2014 why partial? Because one computes it with respect to (w.r.t.) a single parameter. We have two parameters, a and b, so we must compute two partial derivatives.\n\nA derivative tells you how much a given quantity changes when you slightly vary some other quantity. \n![zzz.jpg](attachment:zzz.jpg)\n## Step 3: Update the Parameters\nIn the final step, we use the gradients to update the parameters. Since we are trying to minimize our losses, we reverse the sign of the gradient for the update.\n\nThere is still another parameter to consider: the learning rate, denoted by the Greek letter eta (that looks like the letter n), which is the multiplicative factor that we need to apply to the gradient for the parameter update.\n\n![1_eWnUloBYcSNPRBzVcaIr1g.png](attachment:1_eWnUloBYcSNPRBzVcaIr1g.png)\n\n## Step 4: Rinse and Repeat!\nNow we use the updated parameters to go back to Step 1 and restart the process","91f19883":"![image.png](attachment:image.png)","c06f2788":"# 7-Loss\nWe now tackle the loss computation. As expected, PyTorch got us covered once again. There are many loss functions to choose from, depending on the task at hand. Since ours is a regression, we are using the Mean Square Error (MSE) loss.","7e250b7e":"# 3-Linear Regression in Numpy","4b67cbf8":"We can use Scikit-Learn\u2019s Linear Regression to fit the model and compare the coefficients.","dc8c68e9":"# Introduction\n\nPyTorch is the fastest-growing Deep Learning framework and it is also used by Fast.ai in its MOOC, Deep Learning for Coders, and its library.\nPyTorch is a library for Python programs that facilitates building deep learning projects.\nIt emphasizes flexibility and allows deep learning models to be expressed in idiomatic\nPython. \nThis approachability and ease of use found early adopters in the\nresearch community, and in the years since its first release, it has grown into one of\nthe most prominent deep learning tools across a broad range of applications.\n> Using PyTorch may even improve your health, according to Andrej Karpathy.","d98bf2df":"OK, fine, but then again, why are we building a dataset anyway? We\u2019re doing it because we want to use a DataLoader","07e64a16":"## Random Split\n\nPyTorch\u2019s random_split() method is an easy and familiar way of performing a training-validation split. Just keep in mind that, in our example, we need to apply it to the whole dataset (not the training dataset we built in two sections ago).\n\nThen, for each subset of data, we build a corresponding DataLoader, so our code looks like this:"}}