{"cell_type":{"38890c92":"code","c4803c08":"code","2cb08fd6":"code","05a8d30d":"code","656e274d":"code","1f18e325":"code","6b62e9fd":"code","9a5673bc":"code","2ceda3f0":"code","75408334":"code","709df0be":"code","602dcb49":"code","820c0f2a":"code","e189b468":"code","9d1e49a4":"code","2354d429":"code","4ea364de":"code","f762daf5":"code","9e0905bc":"code","6cd090af":"code","8ef35903":"code","025c31ab":"code","32580dc8":"code","9a7d9075":"code","801678db":"code","18c27ddc":"code","e01eb6a1":"code","78aacb4f":"code","ecbf3352":"code","93e50b44":"code","fdd95a5e":"code","393c3a2e":"code","21b0d858":"code","50d96428":"code","0c4a6c39":"code","bc9367f9":"code","93cc5c67":"code","5706e560":"code","3f4a80b2":"code","9853d7bb":"code","40433ebd":"code","fccf8c65":"code","c444e8d9":"code","a455c091":"code","42a88e01":"code","5bdfaae4":"code","378ade03":"code","0101d0c9":"code","bdf8ce2d":"code","7063480f":"code","be5a7441":"code","ed03edba":"markdown","87b957ad":"markdown","4cc80f7b":"markdown","de50df1d":"markdown","38028520":"markdown","60cad683":"markdown","05301679":"markdown","213cd3a4":"markdown","09865da2":"markdown","711ad251":"markdown","4be61cbd":"markdown","7faf989a":"markdown","62d2a1da":"markdown","172045cd":"markdown","4c622a28":"markdown","da92d3b9":"markdown","827b0ee1":"markdown","3dd22a03":"markdown","40b4e186":"markdown","f52ec3ce":"markdown","6ce7c1fa":"markdown","607f2978":"markdown","06152bde":"markdown","0c776e92":"markdown","a83a92c7":"markdown","8744623f":"markdown","f24a437f":"markdown","3d148c24":"markdown","621f7fd9":"markdown","b00ae6c2":"markdown","443a8e5f":"markdown","fa648f84":"markdown","c8c9689c":"markdown","dad936f5":"markdown","3a1b750e":"markdown","d1609e93":"markdown","c1611071":"markdown","161d1f9b":"markdown","9ebc9473":"markdown","c3530c5c":"markdown"},"source":{"38890c92":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\nimport os\nprint(os.listdir(\"..\/input\"))\n","c4803c08":"df= pd.read_csv('..\/input\/diabetes.csv')","2cb08fd6":"df","05a8d30d":"# shape\nprint(df.shape)","656e274d":"#columns*rows\ndf.size","1f18e325":"df.isnull().sum()","6b62e9fd":"print(df.info())","9a5673bc":"df.head(5)","2ceda3f0":"df.tail()","75408334":"df.sample(5)","709df0be":"df.describe()","602dcb49":"df.isnull().sum()","820c0f2a":"df.columns","e189b468":"# histograms\ndf.hist(figsize=(16,48))\nplt.figure()","9d1e49a4":"df.hist(figsize=(8,8))\nplt.show()","2354d429":"# Using seaborn pairplot to see the bivariate relation between each pair of features\nsns.pairplot(df)","4ea364de":"sns.heatmap(df.corr())","f762daf5":"sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis')","9e0905bc":"sns.distplot(df['Pregnancies'])","6cd090af":"plt.figure(figsize=(8,6))\nsns.countplot(x='Outcome',data=df)\nplt.title('Positive Outcome to Diabetes in Dataset')\nplt.ylabel('Number of People')\nplt.show()","8ef35903":"plt.figure(figsize=(10,6))\nsns.barplot(data=df,x='Outcome',y='Pregnancies')\nplt.title('Pregnancies Among Diabetes Outcomes.')\nplt.show()","025c31ab":"plt.figure(figsize=(10,6))\nsns.countplot(x='Pregnancies',data=df,hue='Outcome')\nplt.title('Diabetes Outcome to Pregnancies')\nplt.show()","32580dc8":"plt.figure(figsize=(13,6))\nsns.countplot(x='Age',data=df,hue='Outcome')\nplt.title('Diabetes Outcome to Age')\nplt.show()","9a7d9075":"plt.figure(figsize=(13,6))\nsns.countplot(x='SkinThickness',data=df,hue='Outcome')\nplt.title('Diabetes Outcome to SkinThickness')\nplt.show()","801678db":"X=df[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n       'BMI', 'DiabetesPedigreeFunction', 'Age']]\ny=df['Outcome']","18c27ddc":"from sklearn.model_selection import train_test_split","e01eb6a1":"X_train, X_test, y_train, y_test = train_test_split(df.drop('Outcome',axis=1), \n                                                    df['Outcome'], test_size=0.2, \n                                                    random_state=201)","78aacb4f":"from sklearn.linear_model import LogisticRegression","ecbf3352":"logmodel = LogisticRegression()\n#** Train\/fit lm on the training data.**\nlogmodel.fit(X_train,y_train)","93e50b44":"predictions = logmodel.predict(X_test)","fdd95a5e":"from sklearn.metrics import classification_report,confusion_matrix","393c3a2e":"print(confusion_matrix(y_test,predictions))","21b0d858":"print(classification_report(y_test,predictions))","50d96428":"#Standardize the Variables\nfrom sklearn.preprocessing import StandardScaler","0c4a6c39":"scaler = StandardScaler()","bc9367f9":"scaler.fit(df.drop('Outcome',axis=1))","93cc5c67":"scaled_features = scaler.transform(df.drop('Outcome',axis=1))","5706e560":"df_feat = pd.DataFrame(scaled_features,columns=df.columns[:-1])\ndf_feat.head()","3f4a80b2":"from sklearn.model_selection import train_test_split","9853d7bb":"X_train, X_test, y_train, y_test = train_test_split(scaled_features,df['Outcome'],\n                                                    test_size=0.10,random_state=200)","40433ebd":"#Using KNN\nfrom sklearn.neighbors import KNeighborsClassifier","fccf8c65":"knn = KNeighborsClassifier(n_neighbors=1)","c444e8d9":"knn.fit(X_train,y_train)","a455c091":"pred = knn.predict(X_test)","42a88e01":"#Predictions and Evaluations\nfrom sklearn.metrics import classification_report,confusion_matrix","5bdfaae4":"print(confusion_matrix(y_test,pred))","378ade03":"print(classification_report(y_test,pred))","0101d0c9":"#Choosing a K Value\nerror_rate = []\n\n# Will take some time\nfor i in range(1,40):\n    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))","bdf8ce2d":"plt.figure(figsize=(10,6))\nplt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","7063480f":"# FIRST A QUICK COMPARISON TO OUR ORIGINAL K=1\nknn = KNeighborsClassifier(n_neighbors=1)\n\nknn.fit(X_train,y_train)\npred = knn.predict(X_test)\n\nprint('WITH K=1')\nprint('\\n')\nprint(confusion_matrix(y_test,pred))\nprint('\\n')\nprint(classification_report(y_test,pred))","be5a7441":"# NOW WITH K=19\nknn = KNeighborsClassifier(n_neighbors=16)\n\nknn.fit(X_train,y_train)\npred = knn.predict(X_test)\n\n#print('WITH K=19')\nprint('\\n')\nprint(confusion_matrix(y_test,pred))\nprint('\\n')\nprint(classification_report(y_test,pred))","ed03edba":"**Create an instance of a LogisticRegression() model named lm.**","87b957ad":"**To pop up 5 random rows from the data set, we can use sample(5) function**","4cc80f7b":"Show the counts of observations in each categorical bin using bars.","de50df1d":"**To check out how many null info are on the dataset, we can use isnull().sum().**","38028520":"Flexibly plot a univariate distribution of observations.","60cad683":"**Predictions and Evaluations**\n\n\nLet's evaluate our KNN model!","05301679":"**Data Collection**","213cd3a4":"**To check the first 5 rows of the data set, we can use head(5).**","09865da2":"Now that we have fit our model, let's evaluate its performance by predicting off the test values!\n\n** Use lm.predict() to predict off the X_test set of the data.**","711ad251":"**To give a statistical summary about the dataset, we can use describe()****","4be61cbd":"**Missing Data**","7faf989a":"**For getting some information about the dataset you can use info() command**","62d2a1da":"**Train Test Split**","172045cd":"**Distplot**","4c622a28":"**In this kernel, I have tried to cover all the parts related to the process of Machine Learning algorithm logistic regression and Support vector machine with a variety of Python packages . I hope to get your feedback to improve it.**","da92d3b9":"**Standardize the Variables**","827b0ee1":"**Conclusion**","3dd22a03":"Let's start by reading in the diabetes.csv file into a pandas dataframe.","40b4e186":"Because the KNN classifier predicts the class of a given test observation by identifying the observations that are nearest to it, the scale of the variables matters. Any variables that are on a large scale will have a much larger effect on the distance between the observations, and hence on the KNN classifier, than variables that are on a small scale.","f52ec3ce":"**Choosing a K Value**","6ce7c1fa":"**Predicting Test Data**","607f2978":"**Plot rectangular data as a color-encoded matrix.**","06152bde":"**We can also create a histogram of each input variable to get an idea of the distribution.**","0c776e92":"**Heatmap**","a83a92c7":"We can use seaborn to create a simple heatmap to see where we are missing data!","8744623f":"**Pairplot**","f24a437f":"**Explorer Dataset**","3d148c24":"**Using KNN**\n\nRemember that we are trying to come up with a model to predict whether someone will TARGET CLASS or not. We'll start with k=1.","621f7fd9":"**How many NA elements in every column**","b00ae6c2":"Now its time to train our model on our training data!\n\n--Import LinearRegression from sklearn.linear_model--","443a8e5f":"**Visualization**","fa648f84":"**Using Logistic Regression**","c8c9689c":"**To check out last 5 row of the data set, we use tail() function**","dad936f5":"Here we can see that that after arouns K>16 the error rate just tends to hover around 0.0-0.16 Let's retrain the model with that and check the classification report!","3a1b750e":"**Histogram**","d1609e93":"**Now above data are Standardize**","c1611071":"**Countplot**","161d1f9b":"**Train Test Split**","9ebc9473":"Let's go ahead and use the elbow method to pick a good K Value:","c3530c5c":"**To print dataset columns, we can use columns atribute**"}}