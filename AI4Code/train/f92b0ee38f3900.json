{"cell_type":{"815ce99e":"code","35e9a3e0":"code","30b61d2a":"code","ac24352c":"code","6a58f693":"code","13a26415":"code","f050a321":"code","dcf51c76":"code","d02c18d3":"code","9701e724":"code","09dbcbf0":"code","e586c494":"code","f82036ea":"code","6d7a866d":"code","76eac15c":"code","edfad0f7":"code","b6837f3d":"code","5f97b2fa":"code","c1a43b3e":"markdown","58b26c2e":"markdown","7ab1a9be":"markdown","676903c4":"markdown","8061b733":"markdown","545296c3":"markdown","0864fac6":"markdown","9be9dfea":"markdown","2f5746b7":"markdown","34c23f83":"markdown","9667909f":"markdown","a5973799":"markdown","09720209":"markdown","7989aab3":"markdown","3cdc3427":"markdown","9782c672":"markdown","908ebd2b":"markdown","c95f9c27":"markdown","8fcb732a":"markdown","4da23c27":"markdown","76a9bc44":"markdown"},"source":{"815ce99e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nfrom tqdm import tnrange, tqdm_notebook, notebook, tqdm\nimport time\n\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import GridSearchCV, cross_validate\nfrom sklearn.metrics import *\nfrom sklearn import preprocessing","35e9a3e0":"# Load the training data\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\", index_col=0)\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\", index_col=0)\nsubmission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\", index_col=0)","30b61d2a":"print(train.shape)\nprint(test.shape)\nprint(submission.shape)","ac24352c":"encoder = preprocessing.LabelEncoder()\ntrain_label = train.copy()\ntest_label = test.copy()\n\nfor i in range(10):\n    train_label.iloc[:,i] = encoder.fit_transform(train.iloc[:,i])\n    test_label.iloc[:,i] = encoder.transform(test.iloc[:,i])","6a58f693":"X_label=train_label.drop('target',axis=1)\ny=train['target']","13a26415":"learn_rate=[0.05,0.3,0.5,0.9]\nmax_depth=[5,10,15,20]\n\nhyper={'learning_rate':learn_rate,'max_depth':max_depth}\ngd=GridSearchCV(estimator=LGBMRegressor(n_estimators=50, n_jobs=-1),param_grid=hyper,verbose=True, n_jobs=-1, cv=3, scoring='neg_root_mean_squared_error')\ngd.fit(X_label,y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","f050a321":"!pip install bayesian-optimization","dcf51c76":"from bayes_opt import BayesianOptimization","d02c18d3":"# create purpose function\ndef lgbm_cv(learning_rate, n_estimators, max_depth, num_leaves, subsample, min_child_weight, colsample_bytree, max_bin, reg_alpha, reg_lambda):\n    model = LGBMRegressor(learning_rate = learning_rate,\n                                n_estimators = int(n_estimators),\n                                num_leaves = int(round(num_leaves)),\n                                max_depth = int(round(max_depth)),\n                                n_jobs = -1,\n                                random_state = 0,\n                                subsample = max(subsample, 0),\n                                min_child_weight = int(round(min_child_weight)),\n                                colsample_bytree = colsample_bytree,\n                                max_bin = int(round(max_bin)),\n                                reg_alpha = max(reg_alpha, 0),\n                                reg_lambda = max(reg_lambda, 0)\n                               )\n    scores = cross_validate(model, X_label, y, cv=5, n_jobs = -1, scoring='neg_root_mean_squared_error')\n    return np.mean(scores['test_score'])","9701e724":"# Interval to be explored for input values\npbounds = {'learning_rate': (0.005, 0.5),\n           'n_estimators': (30, 80),\n           'max_depth': (15, 50),\n           'num_leaves': (0, 100),\n           'subsample': (0, 0.99),\n           'min_child_weight' : (0, 100),\n           'colsample_bytree': (0, 0.99),\n           'max_bin': (0, 1000),\n           'reg_alpha': (0, 10),\n           'reg_lambda' : (0, 10)\n          }","09dbcbf0":"lgbmBO = BayesianOptimization(f = lgbm_cv, pbounds = pbounds, verbose = 2, random_state = 0)","e586c494":"lgbmBO.maximize(init_points=5, n_iter = 20, acq='ei', xi=0.01)","f82036ea":"lgbmBO.max","6d7a866d":"fit_lgbm = LGBMRegressor(learning_rate=lgbmBO.max['params']['learning_rate'],\n                               n_estimators = int(round(lgbmBO.max['params']['n_estimators'])),\n                               num_leaves = int(round(lgbmBO.max['params']['num_leaves'])),\n                               max_depth = int(round(lgbmBO.max['params']['max_depth'])),\n                               max_bin = int(round(lgbmBO.max['params']['max_bin'])),\n                               min_child_weight = int(round(lgbmBO.max['params']['min_child_weight'])),\n                               colsample_bytree=lgbmBO.max['params']['colsample_bytree'],\n                               subsample = lgbmBO.max['params']['subsample'],\n                               reg_alpha = lgbmBO.max['params']['reg_alpha'],\n                               reg_lambda = lgbmBO.max['params']['reg_lambda']\n                               )","76eac15c":"fit_lgbm","edfad0f7":"model = fit_lgbm.fit(X_label,y)","b6837f3d":"pred_y = model.predict(test_label)","5f97b2fa":"submission['target']=pred_y\nsubmission.to_csv('submission_BO.csv')","c1a43b3e":"General properties of Bayesian optimization methods:\n1. It is a sequential approach. The calculations are not parallelized.\n2. Do not use the derivative of the objective function. [Appropriate for cases where the derivative is unknownly discrete.\nDepending on the problem, the derivative may not be defined.\nOf course, even if a derivative is defined, it can also be applied when it is complicated to calculate. In this case, automatic gradient may be used.]\n3. Use machine learning methods to predict where there is a better year. (surrogate model) [using artificial intelligence]\n4. Various models are possible. In addition, results can change sensitively to model selection.\n5. It is known as an appropriate method for situations where too many objective function calculations cannot be done.\n  - That is, if the calculation is too high.\nIn other words, it is an optimal method when the cost of creating a surrogate model with machine learning is very low.\n(However, too much data can make a problem.\nProblems that are proportional to three wins in the number of data arise. This is the complexity associated with the inverse matrix calculation.\n  - How much time is spent calculating the objective function? The time it takes to achieve expectations should be directly compared.\n6. A general optimization algorithm that can be used when the objective function has noise.\n7. Machine learning methods are continuously available when additional data is reinforced.\n\n\nhttps:\/\/github.com\/fmfn\/BayesianOptimization\n\n\n- It should be noted that the use is different from that of a typical local minimization algorithm.\n- For example, traditional local minimization algorithms are still better for very simple function local minimization. [Nelder-Mead]\n- If the derivative of the objective function is known analytically, traditional computational methods should be used. [BFGS]\n- The machine learning method is to use when things are more twisted.\n\n","58b26c2e":"  <br\/>\n\nWe will proceed with modeling by applying the Bayesian optimization method during Hyperparameter Optimization, which is a field of AutoML.\n\nExcept for the model ensemble, we will calculate the optimal result value with a single model. The model to use is LGBM.\n\nFor bayesian-optimization to be used in a Python environment, you must install the package. You can install it with the pip command.\n\n- bayesian-optimization 1.2.0 [link](https:\/\/pypi.org\/project\/bayesian-optimization\/)","7ab1a9be":"`cross_validate`is a function that calculates the score.\nIt is common to use the `cross_val_score` method when using a single evaluation index or the `make_scorer` method when using multiple indicators. We will use `cross_val_score` because there is a competition evaluation index.\n\nThe parameter cv value was specified as 3.","676903c4":"\nThe following sets the input value, i.e., the navigation interval for the parameter.","8061b733":"\n\n<img src =\"https:\/\/blog.kakaocdn.net\/dn\/c5AeYX\/btqNqC6y46P\/yCsuigpZvzKbDUK6dHZg51\/img.png\" width=\"500\" height=\"600\"><img>\n\n\n\nHere, we will use this technique to find the optimal hyper-parameters","545296c3":"The 'bayes_opt' module is downloaded to your environment and the installation is complete.\n\n<br\/>\n\nI will import the required package.","0864fac6":"`init_points` is the number of first-time searches. The calculation proceeds by sampling the input value by `init_points` within the interval set in pbound. `n_iter` is the number of operations. Therefore, you will perform 25 times in total.\n\nI will set acq to EI. xi is an argument that controls the intensity of the expansion-exposition, which typically increases the expansion by setting it to 0.01.\n\nThe results of the operation are as follows:","9be9dfea":"The following must create a purpose function: The objective function is a performance function of LGBMRegressor that has a combination of parameters in the model as an input value. Since the performance evaluation is RMSE, it returns the score value.","2f5746b7":"The `target` value corresponds to the RMSE score as the return value of the objective function. I was able to get a value of about 0.721.\n\nAlthough there is no significant improvement in performance, estimating a small number of parameter combinations or proceeding with model ensembles can result in better scores.\n\n<br\/>\n\nThe parameter values calculated are as follows:","34c23f83":"# Bayesian Optimization\n\nBayesian optimization is a sequential design strategy for global optimization of black-box functions that does not assume any functional forms. It is usually employed to optimize expensive-to-evaluate functions.\n","9667909f":"## Pre-Modeling Operations\n","a5973799":"The navigation range of the parameters was set by referring to the model-specific descriptive materials. I will omit the detailed description of the parameter.\n\n\nThe following creates an object:\n","09720209":"Arguments for functions are parameters of the model. Enter the parameter that you want to explore the optimal value.","7989aab3":"The model operation is complete.","3cdc3427":"## Modeling (Bayesian Optimization)","9782c672":"I composed the combination of 10 parameters in total.\n\n\n`learning_rate` and `max_depth`, `n_estimators` are set to a narrow range that does not deviate significantly from the parameters found earlier. And I set the remaining parameters to a wide range. `int(round())` sets for parameters with int values, and `max` and `min` functions for parameters with fixed maximum and minimum values.","908ebd2b":"---\nHope this helps. In the previous process, creating a slightly more advanced model through GridSearchCV, estimating combinations with fewer parameters, and ensembling models with good scores will likely lead to greater performance improvements.","c95f9c27":"- First, find heavy parameters such as `n_estimators` and `learn_rate` using GridSearchCV first.\n\n- For time problems, set the parameter to a low value and n_estimators are fixed to a value of 50.\n","8fcb732a":"\n- reference \n\nhttps:\/\/www.dacon.io\/competitions\/official\/235647\/codeshare\/1720\n\nhttp:\/\/egloos.zum.com\/incredible\/v\/7479039\n\nhttps:\/\/en.wikipedia.org\/wiki\/Bayesian_optimization","4da23c27":"The first factor is the objective function f, and pbounds refer to the navigation interval of the input value. Random seed is set to zero.\n\n<br\/>\n\nLet's do Bayesian Optimization.","76a9bc44":"## Model Fitting"}}