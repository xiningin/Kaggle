{"cell_type":{"a26dde52":"code","c2e9e8b7":"code","4b0f87ed":"code","196bf516":"code","289b6d33":"code","e1c31a10":"code","26630cc7":"code","73359b65":"code","9d098e3d":"code","423195e1":"code","840106bb":"code","9b149d5b":"code","0ec79bab":"code","9e1312a4":"code","79440b62":"code","59429ea0":"code","71ec0ea7":"code","54c744bd":"code","011e88cf":"code","dc925f16":"code","a60b3fb7":"code","7785c4a2":"code","b647f4ba":"code","5f8170e4":"code","7f7c19ef":"code","966a6f5d":"code","b18d478b":"code","3f4a6d43":"code","8bbf1efa":"code","3ba86c07":"code","2c705920":"code","eb69b052":"code","d069a18e":"code","5f53351b":"code","74fef49e":"code","fc7431b9":"code","c52a64da":"code","4156da21":"code","134ea765":"code","cfe56a07":"code","d948c7dc":"code","9e3fef4c":"code","c12925c1":"code","67df248d":"code","77138670":"code","6aff19b6":"code","fe93ec06":"code","e2123ddb":"markdown","5d989b4f":"markdown","1f11f1c4":"markdown","d98a0693":"markdown","e35731af":"markdown","c2c4e407":"markdown","67879b03":"markdown","e87af7e3":"markdown","e3bc2a5b":"markdown","9082d962":"markdown"},"source":{"a26dde52":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c2e9e8b7":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom  sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\n#from sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.model_selection import train_test_split","4b0f87ed":"df = pd.read_csv(\"..\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv\")\ndf.head()","196bf516":"df.describe()","289b6d33":"df.isnull().sum()","e1c31a10":"# Gre Score\nplt.figure(figsize=(8,5))\nsns.distplot(df['GRE Score'])\nplt.title('GRE Score distribution')\nplt.show()\n\nplt.figure(figsize=(8,5))\nsns.boxplot(df['GRE Score'])\nplt.title('GRE Score distribution')\nplt.show()","26630cc7":"q = df['GRE Score'].quantile(0.01)\ndf = df[df['GRE Score']>q]\ndf.describe()","73359b65":"# Gre Score\nplt.figure(figsize=(8,5))\nsns.distplot(df['GRE Score'])\nplt.title('GRE Score distribution')\nplt.show()","9d098e3d":"# TOEFL score\nplt.figure(figsize=(8,5))\nsns.distplot(df['TOEFL Score'])\nplt.title('TOEFL Score distribution')\nplt.show()","423195e1":"q = df['TOEFL Score'].quantile(0.01)\ndf = df[df['TOEFL Score']>q]\ndf.describe()","840106bb":"# TOEFL score\nplt.figure(figsize=(8,5))\nsns.distplot(df['TOEFL Score'])\nplt.title('TOEFL Score distribution')\nplt.show()","9b149d5b":"# CGPA score\nplt.figure(figsize=(8,5))\nsns.distplot(df['CGPA'])\nplt.title('CGPA Score distribution')\nplt.show()","0ec79bab":"q = df['CGPA'].quantile(0.01)\ndf = df[df['CGPA']>q]\ndf.describe()","9e1312a4":"# CGPA score\nplt.figure(figsize=(8,5))\nsns.distplot(df['CGPA'])\nplt.title('CGPA Score distribution')\nplt.show()","79440b62":"# Chance of admit, LOR, SOP\nplt.figure(figsize=(8,5))\nsns.distplot(df['Chance of Admit '])\nplt.title('Chance of Admit distribution')\nplt.show()\n\nplt.figure(figsize=(8,5))\nsns.distplot(df['LOR '])\nplt.title('LOR distribution')\nplt.show()\n\nplt.figure(figsize=(8,5))\nsns.distplot(df['SOP'])\nplt.title('SOP distribution')\nplt.show()","59429ea0":"q = df['Chance of Admit '].quantile(0.01)\ndf = df[df['Chance of Admit ']>q]\n\n\nq = df['LOR '].quantile(0.01)\ndf = df[df['LOR ']>q]\n\nq = df['SOP'].quantile(0.01)\ndf = df[df['SOP']>q]\n\ndf.describe()","71ec0ea7":"plt.figure(figsize=(8,5))\nsns.distplot(df['Chance of Admit '])\nplt.title('Chance of Admit distribution')\nplt.show()\n\nplt.figure(figsize=(8,5))\nsns.distplot(df['LOR '])\nplt.title('LOR distribution')\nplt.show()\n\nplt.figure(figsize=(8,5))\nsns.distplot(df['SOP'])\nplt.title('SOP distribution')\nplt.show()","54c744bd":"print(df['GRE Score'].max())\nprint(df['TOEFL Score'].max())\nprint(df['CGPA'].max())\nprint(df['LOR '].max())\nprint(df['SOP'].max())\nprint(df['University Rating'].max())","011e88cf":"print(df['Chance of Admit '].max()\n     )","dc925f16":"df.drop('Serial No.',axis=1, inplace=True)\ndf.head()","a60b3fb7":"df.dtypes","7785c4a2":"df.keys()","b647f4ba":"x = df.loc[:, df.columns != 'Chance of Admit ']\ny = df['Chance of Admit ']","5f8170e4":"print(x.shape)\nprint(y.shape)","7f7c19ef":"scaler = StandardScaler()\nx_trans = scaler.fit_transform(x)","966a6f5d":"from sklearn.linear_model import LinearRegression","b18d478b":"x_train, x_test, y_train, y_test = train_test_split(x_trans, y, test_size=0.15, random_state=3) ","3f4a6d43":"print(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)","8bbf1efa":"lr = LinearRegression()\nlr.fit(x_train,y_train)\ny_pred = lr.predict(x_test)","3ba86c07":"lr.score(x_train, y_train)","2c705920":"lr.score(x_test, y_test)","eb69b052":"y_pred","d069a18e":"y_test","5f53351b":"lr.coef_","74fef49e":"df_weights = pd.DataFrame(x.columns.values, columns=['Col_name'])\ndf_weights['Weight'] = lr.coef_\ndf_weights.head()","fc7431b9":"df_pr = pd.DataFrame(y_pred, columns=['Predictions'])\nprint(df_pr.shape)\ndf_pr.head()","c52a64da":"y_test.keys()","4156da21":"df_pr['Original'] = y_test","134ea765":"df_pr.head()","cfe56a07":"y_test= y_test.reset_index(drop=True)","d948c7dc":"df_pr['Original'] = y_test\ndf_pr.head()","9e3fef4c":"df_pr['difference'] = df_pr['Original'] - df_pr['Predictions']\ndf_pr.head()","c12925c1":"df_pr['Predictions'] = round(df_pr['Predictions'],2)\ndf_pr['difference'] = round(df_pr['difference'],2)","67df248d":"df_pr.head()","77138670":"df_pr.shape","6aff19b6":"lr.predict([[320,110,4,4.5,4.0,8.70,0]])","fe93ec06":"df.head()","e2123ddb":"Building up the dataframe for Predictions done vs Original labels","5d989b4f":"Importing Libraries","1f11f1c4":"There are no null values so we will just start with visualizations and outliers.\nWe will Plot for each column and remove outliers or just to normalize the dataset","d98a0693":"# The following notebook shows basic manipulation and usage of linear regression. This is the predictive analysis of admission in US. ","e35731af":"Weights associated with the columns","c2c4e407":"Creating a dataframe associated with features and weights","67879b03":"Start with describing the data","e87af7e3":"predicting individually","e3bc2a5b":"Using Standard Scaler to normalize the inputs. We have remove the outliers so we use Standard Scaler.","9082d962":"We start with splitting of targets and inputs "}}