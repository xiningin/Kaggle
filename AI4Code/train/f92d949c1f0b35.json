{"cell_type":{"2c6520c7":"code","642ade4f":"code","cb49eabe":"code","0adb62d4":"code","6ca2fee8":"code","35a68cc5":"code","ca12d600":"code","ec515482":"code","cc1a7db2":"code","4294d809":"code","bd831cb8":"code","a1d969be":"code","1097c82c":"code","87f106fa":"code","8e797d24":"code","5154b298":"code","8d62a216":"code","1a87a9be":"code","b78e4f1a":"code","8f78dc69":"code","9800b705":"code","29b4b212":"code","5b218a0b":"code","cc112817":"markdown","affc1ef4":"markdown","58379333":"markdown","7b656460":"markdown","d17b1a3f":"markdown","279222b9":"markdown","487570d8":"markdown","13967f63":"markdown","62676c15":"markdown","aef5d15c":"markdown","4c3058f3":"markdown","3a5babaf":"markdown","4f091f04":"markdown","773e868c":"markdown","7ed9a90e":"markdown","de240f38":"markdown","83b6bf2e":"markdown","16a7ec11":"markdown","2f934147":"markdown","46b9115f":"markdown","b805ef45":"markdown","6dbf965e":"markdown","78c7a277":"markdown","88307285":"markdown","3e34bf8b":"markdown","77a32058":"markdown","6d8cbbcd":"markdown","6d1bcbdb":"markdown","854f2e8a":"markdown","43513343":"markdown","643e1185":"markdown","984432f8":"markdown","9a4d09f7":"markdown"},"source":{"2c6520c7":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import TomekLinks\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.combine import SMOTETomek\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier","642ade4f":"train = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv')\ntrain.head(7)","cb49eabe":"train.info()","0adb62d4":"Tr_total = train.isnull().sum().sort_values(ascending = False)\nTr_percent = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending = False)\n\nTr_missing_data = pd.concat([Tr_total,Tr_percent], axis = 1, keys = ['Total', 'Percent'])\nTr_missing_data.head(9)","6ca2fee8":"for feature in Tr_missing_data.index:\n    train[feature] = train[feature].fillna(train[feature].mode()[0])","35a68cc5":"numeric_variables = train.select_dtypes(include= [np.number])\nnumeric_variables.columns\nnumeric_variables = numeric_variables.drop(columns = 'target' , axis = 1)","ca12d600":"#City Development\n\nplt.subplots(sharex = True , figsize= (10,5))\nplt.suptitle('City Development', size=16, y=(0.94))\nsns.distplot(numeric_variables['city_development_index'], hist= True)\nplt.show()","ec515482":"#Training Hours\n\nplt.subplots(sharex = True , figsize= (10,5))\nplt.suptitle('Training Hours', size=16, y=(0.94))\nsns.distplot(numeric_variables['training_hours'], hist= True)\nplt.show()","cc1a7db2":"train['training_hours'] = pd.cut(train['training_hours'] ,bins = 3 , labels = ['long' , 'medium_long', 'short'])\n\ntrain['city_development_index'] = pd.cut(train['city_development_index'] ,bins = 3 , labels = ['nothing' , 'moreorless', 'a_lot'])","4294d809":"categoricals = train.select_dtypes(exclude = [np.number])\ncategoricals.columns","bd831cb8":"def number_categories(categoricals): \n    c_count = 0\n    height = 6\n    width = 2\n    fig, axes = plt.subplots(height, width, sharex=True, figsize=(20,23))\n    plt.suptitle('Number of categorical variables', size=16, y=(0.94))\n    \n    for row in range(height):\n        for col in range(width):\n            c_idx = categoricals.columns[c_count]\n            c_data = categoricals[c_idx].value_counts()\n            sns.barplot(x = c_data.values, y = c_data.index, palette='deep', ax=axes[row, col])\n            axes[row,col].set_title(c_idx)\n            c_count= c_count + 1\n\nnumber_categories(categoricals)","a1d969be":"sns.countplot(train['target'])","1097c82c":"#Fix experience values\n\ntrain['experience'].replace(to_replace = '<1',\n                      value = 'a_few', inplace = True )\ntrain['experience'].replace(to_replace = '1',\n                      value = 'a_few', inplace = True )\ntrain['experience'].replace(to_replace = '2',\n                      value = 'a_few', inplace = True )\ntrain['experience'].replace(to_replace = '3',\n                      value = 'a_few', inplace = True )\ntrain['experience'].replace(to_replace = '4',\n                      value = 'a_few', inplace = True )\ntrain['experience'].replace(to_replace = '5',\n                      value = 'a_few', inplace = True )    \ntrain['experience'].replace(to_replace = '6',\n                      value = 'medium', inplace = True )      \ntrain['experience'].replace(to_replace = '7',\n                      value = 'medium', inplace = True )      \ntrain['experience'].replace(to_replace = '8',\n                      value = 'medium', inplace = True )\ntrain['experience'].replace(to_replace = '9',\n                      value = 'medium', inplace = True )\ntrain['experience'].replace(to_replace = '10',\n                      value = 'medium', inplace = True )\ntrain['experience'].replace(to_replace = '11',\n                      value = 'high', inplace = True )\ntrain['experience'].replace(to_replace = '12',\n                      value = 'high', inplace = True )\ntrain['experience'].replace(to_replace = '13',\n                      value = 'high', inplace = True )\ntrain['experience'].replace(to_replace = '14',\n                      value = 'high', inplace = True )\ntrain['experience'].replace(to_replace = '15',\n                      value = 'high', inplace = True )\ntrain['experience'].replace(to_replace = '16',\n                      value = 'high', inplace = True )\ntrain['experience'].replace(to_replace = '17',\n                      value = 'high', inplace = True )\ntrain['experience'].replace(to_replace = '18',\n                      value = 'high', inplace = True )\ntrain['experience'].replace(to_replace = '19',\n                      value = 'high', inplace = True )\ntrain['experience'].replace(to_replace = '20',\n                      value = 'high', inplace = True )\ntrain['experience'].replace(to_replace = '>20',\n                      value = 'high', inplace = True )\n\n\n\n#Fix company size\n\ntrain['company_size'].replace(to_replace = '<10',\n                      value = 'small_size', inplace = True )\ntrain['company_size'].replace(to_replace = '10\/49',\n                      value = 'small_size', inplace = True )\ntrain['company_size'].replace(to_replace = '50-99',\n                      value = 'small_size', inplace = True )\ntrain['company_size'].replace(to_replace = '100-500',\n                      value = 'medium_size', inplace = True )\ntrain['company_size'].replace(to_replace = '500-999',\n                      value = 'medium_size', inplace = True )\ntrain['company_size'].replace(to_replace = '1000-4999',\n                      value = 'big_size', inplace = True )\ntrain['company_size'].replace(to_replace = '5000-9999',\n                      value = 'big_size', inplace = True )\ntrain['company_size'].replace(to_replace = '10000+',\n                      value = 'big_size', inplace = True )\n\n\n#Fix last new job\n\ntrain['last_new_job'].replace(to_replace = 'never',\n                      value = 'fresher', inplace = True )\ntrain['last_new_job'].replace(to_replace = '1',\n                      value = 'fresher', inplace = True )\ntrain['last_new_job'].replace(to_replace = '2',\n                      value = 'medium_time', inplace = True )\ntrain['last_new_job'].replace(to_replace = '3',\n                      value = 'medium_time', inplace = True )\ntrain['last_new_job'].replace(to_replace = '4',\n                      value = 'medium_time', inplace = True )\ntrain['last_new_job'].replace(to_replace = '>4',\n                      value = 'long_time', inplace = True )","87f106fa":"categoricals = train.select_dtypes(exclude = [np.number])\ncategoricals.columns\n\nnumber_categories(categoricals) #plot","8e797d24":"train = train.drop(columns = ['city' , 'enrollee_id'] , axis = 1)\n\n\n#Fix resampling errors in columns [repetitive names]\n\ntrain['major_discipline'].replace(to_replace = 'Other', value = 'Other_1', inplace = True )\ntrain['company_type'].replace(to_replace = 'Other', value = 'Other_2', inplace = True )","5154b298":"gd_city_development_index = pd.get_dummies(train[['city_development_index']], drop_first=True, prefix=[None])\ngd_gender = pd.get_dummies(train[['gender']] , drop_first=True, prefix=[None])\ngd_relevent_experience = pd.get_dummies(train[['relevent_experience']], drop_first=True, prefix=[None])\ngd_enrolled_university = pd.get_dummies(train[['enrolled_university']], drop_first=True, prefix=[None])\ngd_education_level = pd.get_dummies(train[['education_level']], drop_first=True ,prefix=[None])\ngd_major_discipline = pd.get_dummies(train[['major_discipline']], drop_first=True, prefix=[None])\ngd_experience = pd.get_dummies(train[['experience']],  drop_first=True, prefix=[None])\ngd_company_size = pd.get_dummies(train[['company_size']], drop_first=True, prefix=[None])\ngd_company_type = pd.get_dummies(train[['company_type']], drop_first=True, prefix=[None])\ngd_last_new_job = pd.get_dummies(train[['last_new_job']], drop_first=True, prefix=[None])\ngd_training_hours = pd.get_dummies(train[['training_hours']], drop_first=True, prefix=[None])","8d62a216":"#Drop original columns\n\nto_drop = ['city_development_index', 'gender', 'relevent_experience',\n       'enrolled_university', 'education_level', 'major_discipline',\n       'experience', 'company_size', 'company_type', 'last_new_job',\n       'training_hours']\n\ntrain = train.drop(columns = to_drop , axis = 1)\n\n\ndata = pd.concat([train, gd_city_development_index, gd_gender, gd_relevent_experience,\n                  gd_enrolled_university, gd_education_level, gd_major_discipline, \n                  gd_experience, gd_company_size, gd_company_type,\n                  gd_last_new_job, gd_training_hours ], axis = 1)\n\n\ndata.head(7)","1a87a9be":"#Separe the data\n\nX = data.drop(columns = 'target' , axis = 1)                           \nY = data['target']","b78e4f1a":"#Define resampling strategies\n\nRUS = RandomUnderSampler()\nROS = RandomOverSampler()\nTL = TomekLinks(sampling_strategy='majority')\nSMT = SMOTE(sampling_strategy='minority')\nSMTL = SMOTETomek(sampling_strategy='auto')\n\n#Apply them\n\nX_rus, Y_rus = RUS.fit_sample(X,Y)\nX_ros, Y_ros = ROS.fit_sample(X,Y)\nX_tl, Y_tl = TL.fit_sample(X,Y)\nX_smt, Y_smt = SMT.fit_sample(X, Y)\nX_smtl, Y_smtl = SMTL.fit_sample(X, Y)\n\n\nX_train_rus, X_test_rus, Y_train_rus, Y_test_rus = train_test_split(X_rus, Y_rus, test_size = 0.2 ,random_state = 2020)\nX_train_ros, X_test_ros, Y_train_ros, Y_test_ros = train_test_split(X_ros, Y_ros, test_size = 0.2 ,random_state = 2020)\nX_train_tl, X_test_tl, Y_train_tl, Y_test_tl = train_test_split(X_tl, Y_tl,test_size = 0.2 ,random_state = 2020)\nX_train_smt, X_test_smt, Y_train_smt, Y_test_smt = train_test_split(X_smt, Y_smt, test_size = 0.2 ,random_state = 2020)\nX_train_smtl, X_test_smtl, Y_train_smtl, Y_test_smtl = train_test_split(X_smtl, Y_smtl, test_size = 0.2 ,random_state = 2020)","8f78dc69":"DT = DecisionTreeClassifier(criterion = 'entropy')","9800b705":"methods = ['RUS' , 'ROS' , 'TomekLinks' , 'SMOTE' , 'SMOTE + Tomek']\n\n\nscores = []\n\n#Fit with RUS\n\nDT.fit(X_train_rus,Y_train_rus)\n\n\n\nscore_1 = DT.score(X_train_rus, Y_train_rus)\nscores.append(score_1)\n\n\n#Fit with ROS\n\nDT.fit(X_train_ros,Y_train_ros)\n\nscore_2 = DT.score(X_train_ros, Y_train_ros)\nscores.append(score_2)\n\n\n#Fit with TomekLinks\n\nDT.fit(X_train_tl,Y_train_tl)\n\nscore_3 = DT.score(X_train_tl, Y_train_tl)\nscores.append(score_3)\n\n\n#Fit with SMOTE\n\nDT.fit(X_train_smt,Y_train_smt)\n\nscore_4 = DT.score(X_train_smt, Y_train_smt)\nscores.append(score_4)\n\n\n\n#Fit with SMOTE + Tomek\n\nDT.fit(X_train_smtl,Y_train_smtl)\n\nscore_5 = DT.score(X_train_smtl, Y_train_smtl)\nscores.append(score_5)","29b4b212":"#concate the results\n\nresults = pd.DataFrame(methods, columns=['ReSampling'])\nresults['accuracy'] = scores\n\n#print the results\n\nprint('\\nModelling results:')\nprint(results.sort_values(by = 'accuracy' , ascending = False))","5b218a0b":"#Plotting them\n\nsns.set_style(\"dark\")\nresults.plot.bar(x = 'ReSampling' , y= 'accuracy' , rot=0 , legend = False)","cc112817":"It goes without saying that variables as _city_ and _enrollee id_ are not relevant","affc1ef4":"### What is an imbalanced dataset?","58379333":"Using the library _imlearn_ we can apply techniques which will balance our dataset. Some of them are:\n\n**RUS:** Randomly take examples from the majority class and delete them from the training.\n\n**ROS:** Randomly take examples from the minority class and add them to the training\n\n**Tomek Link:** Looks for Tomek Links ( pairs of opposing instances that are very close together) and removes the majority instance.\n\n**SMOTE:** It aims to enrich the minority class boundaries by creating artificial examples in the minority class.\n\n**SMOTE & Tomek Link:** Over-sampling using SMOTE and cleaning using Tomek Links.","7b656460":"We can check some information about the data","d17b1a3f":"Plotting again the variables","279222b9":" The model chosen for classification is the decission tree \ud83c\udf32","487570d8":"### Visualization of numeric variables","13967f63":"Most of the machine learning algorithms used for classification were designed around the assumption of an equal number of examples for each class. This results in models that have poor predictive performance, specifically for the minority class. This is a problem because typically, the minority class is more important and therefore the problem is more sensitive to classification errors for the minority class than the majority class.","62676c15":"To begin with, let's import all libraries needed","aef5d15c":"Load the data","4c3058f3":"One strategy can be to fill all the _NA_ with the mode of each column.","3a5babaf":"The dataset contains categorical variables and Python modelling libraries as _sklearn_ cannot model them. Thus, getting dummies is necessary:","4f091f04":"### \ud83c\udfa8 Visualization of categorical variables","773e868c":"An imbalanced dataset is a special case for classification problem where the class distribution is not uniform among the classes. Typically, they are composed by two classes: The majority (negative) class and the minority (positive) class. When we speak about the distribution is not uniform it means that it is biased or skewed.","7ed9a90e":"## Data context ","de240f38":"![image.png](attachment:image.png)","83b6bf2e":"#  How to deal with an imbalanced datatset \ud83e\udd7a","16a7ec11":"The purpose of this notebook is to show what is an imbalanced dataset and which are some of the techniques available in Python to deal with. I am going to compare the effectiveness of one another applying to the \" _HR Analytics: Job Change of Data Scientists_ \" dataset. You can access to the data [here](https:\/\/www.kaggle.com\/arashnic\/hr-analytics-job-change-of-data-scientists)\n\nI hope you to enjoy this notebook. Don't forget to write any suggestion in the comment section. Not only will you help me out increasing the level of this notebook but you also will help me to improve my knowledge. \ud83e\udd1c\ud83e\udd1b","2f934147":"It's the turn of the categorical variables","46b9115f":"In this plot the dataset imbalanced can be seen. This can be confused for both model and metrics. \ud83d\ude15\n\nThe following preprocessing step is not always correct. I am going to concatenate some categories between them in order to have approximately the same number of categories per variable.","b805ef45":"Due to the fact that the majority of the variables are categorical, one option is to discretize these numeric variables in order to transform them into categorical.","6dbf965e":"### Why should we fix it?","78c7a277":"As regards with the variables, most of them are categorical and have missing values, thus, let's deal with them.","88307285":"Thank you for reading till here! \u270c\ufe0f\n\nThe final result has shown that in this problem the **Tomek Link** provides the best accuracy at least for the decission tree tested. I hope this notebook has helped you to realize that when a imbalanced dataset needs to be balanced we shouldn't take the first method that goes through our mind and we should to check if other method can be more accurate. ","3e34bf8b":"A company wants to hire new data scientist for the team. Candidates have previously passed some courses conducted by the company. The company wants to ensure that each candidate wants to work with them since it will boost the recruitment process. Based on some variables, How can they ensure hiring process is going to be excellent? \ud83e\udd14","77a32058":"## Conclusion","6d8cbbcd":"Now, let's work with numeric variables","6d1bcbdb":"## Introduction","854f2e8a":"![image.png](attachment:image.png)","43513343":"Let's fit it in each training data:","643e1185":"### How can we deal with it?","984432f8":"## \ud83d\udd75\ufe0f Exploratory Data Analysis (EDA)","9a4d09f7":"## \ud83e\udd16 Modelling"}}