{"cell_type":{"b700fd46":"code","164521c2":"code","5cb3dfe0":"code","7bb8786b":"code","373120df":"code","6ac23b34":"code","9bc5136c":"code","14ca74f4":"code","de7349d9":"code","3748634b":"code","eb39d764":"code","ae3a294d":"code","c25ce22b":"code","440a8b84":"code","8fb03a39":"code","6b1535a1":"code","5f95d178":"code","98b2d788":"code","60fe85ea":"code","270d36d7":"code","aeb37e03":"code","5790bbf7":"code","917d74dd":"code","50f42763":"code","dc22f740":"code","51338591":"code","cbdbe15c":"code","e624078f":"code","86ce7cce":"code","d7f2b619":"code","afd5d3c4":"code","4d506191":"code","8afcfd92":"code","cfaa1843":"code","0e183125":"code","e55481a6":"code","874c3dc5":"code","b1036ec0":"code","680cdd62":"code","afe6ce00":"code","c71c99cd":"code","f374ce5a":"code","0873cb06":"code","22991fd4":"code","78669bb7":"code","1b528fb1":"code","c051b475":"code","75187297":"code","c5904f2c":"code","3465e9a6":"code","e760fdd1":"code","e61758af":"code","3274114e":"code","3c7b57ec":"code","0f29ecf4":"code","d9c2fbaa":"code","d5dc8981":"code","045b1809":"code","2c59468c":"code","8518d760":"code","b9cb6f4f":"code","6e0791b1":"code","56350334":"code","a26e4e5b":"code","7494f3b5":"code","e644ba68":"code","78641a84":"code","498f2cce":"code","2b1f5793":"code","5ef9fc9a":"code","05beefdd":"code","85b50558":"code","0f6e8d84":"code","52529ea4":"code","b872b791":"code","8f45624e":"code","7ec98664":"code","385e233a":"code","255f32d8":"markdown","7061f2d8":"markdown","c8a348f9":"markdown","30e39fa4":"markdown","0a730941":"markdown","ce6d2b6d":"markdown","daf19581":"markdown","a26f8451":"markdown","8a6872fe":"markdown","05befd9c":"markdown","9ec50868":"markdown","2cd73e61":"markdown","83fe7e25":"markdown","6c17f007":"markdown","ebf22154":"markdown","d743f93d":"markdown","d0dc9940":"markdown","ec848a59":"markdown","e0466843":"markdown","907a74b0":"markdown","80242944":"markdown","77cd8b3f":"markdown","b477a611":"markdown","f9114dba":"markdown"},"source":{"b700fd46":"# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n# Import model libraries\nfrom sklearn.linear_model import LinearRegression\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n\n# Suppress the warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","164521c2":"# Import the price data\ndf = pd.read_csv(\"..\/input\/gold-prices\/monthly_csv.csv\")\ndf.head()","5cb3dfe0":"df.shape","7bb8786b":"print(f\"Date range of gold prices available from - {df.loc[:,'Date'][0]} to {df.loc[:,'Date'][len(df)-1]}\")","373120df":"date = pd.date_range(start='1\/1\/1950', end='8\/1\/2020', freq='M')\ndate","6ac23b34":"df['month'] = date\ndf.drop('Date',axis=1,inplace=True)\ndf = df.set_index('month')\ndf.head()","9bc5136c":"df.plot(figsize=(20,8))\nplt.title(\"Gold price (Monthly) since 1950\")\nplt.xlabel(\"Months\")\nplt.ylabel(\"Price\")\nplt.grid();","14ca74f4":"round(df.describe(),3)","de7349d9":"_, ax = plt.subplots(figsize=(25,8))\nsns.boxplot(x = df.index.year,y = df.values[:,0],ax=ax)\nplt.title(\"Gold price (Monthly) since 1950\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Price\")\nplt.xticks(rotation=90)\nplt.grid();","3748634b":"_, ax = plt.subplots(figsize=(22,8))\nsns.boxplot(x = df.index.month_name(),y = df.values[:,0],ax=ax)\nplt.title(\"Gold price (Monthly) since 1950\")\nplt.xlabel(\"Months\")\nplt.ylabel(\"Price\")\nplt.grid();","eb39d764":"from statsmodels.graphics.tsaplots import month_plot\n\nfig, ax = plt.subplots(figsize=(22,8))\n\nmonth_plot(df,ylabel='Gold price',ax=ax)\nplt.title(\"Gold price (Monthly) since 1950\")\nplt.xlabel(\"Months\")\nplt.ylabel(\"Price\")\nplt.grid();","ae3a294d":"# Average gold price per year trend since 1950\ndf_yearly_sum = df.resample('A').mean()\ndf_yearly_sum.plot();\nplt.title(\"Average Gold price (Yearly) since 1950\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Price\")\nplt.grid()","c25ce22b":"# Average gold price per quarter trend since 1950\ndf_quarterly_sum = df.resample('Q').mean()\ndf_quarterly_sum.plot();\nplt.title(\"Average Gold price (Quarterly) since 1950\")\nplt.xlabel(\"Quarter\")\nplt.ylabel(\"Price\")\nplt.grid()","440a8b84":"# Average gold price per decade trend since 1950\ndf_decade_sum = df.resample('10Y').mean()\ndf_decade_sum.plot();\nplt.title(\"Average Gold price (Decade) since 1950\")\nplt.xlabel(\"Decade\")\nplt.ylabel(\"Price\")\nplt.grid()","8fb03a39":"# Coefficient of variation in price\ndf_1 = df.groupby(df.index.year).mean().rename(columns={'Price':'Mean'})\ndf_1 = df_1.merge(df.groupby(df.index.year).std().rename(columns={'Price':'Std'}),left_index=True,right_index=True)\ndf_1['CoV_pct'] = ((df_1['Std']\/df_1['Mean'])*100).round(2)\ndf_1.head()","6b1535a1":"# Average gold price per year trend since 1950\nfig, ax = plt.subplots(figsize=(15,10))\ndf_1['CoV_pct'].plot();\nplt.title(\"Average Gold price (Yearly) since 1950\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Coefficient of Variation in %\")\nplt.grid()","5f95d178":"train    =   df[df.index.year <= 2015] \ntest     =   df[df.index.year > 2015]","98b2d788":"print(train.shape)\nprint(test.shape)","60fe85ea":"train['Price'].plot(figsize=(13,5), fontsize=14)\ntest['Price'].plot(figsize=(13,5), fontsize=14)\nplt.grid()\nplt.legend(['Training Data','Test Data'])\nplt.show()","270d36d7":"train_time = [i+1 for i in range(len(train))]\ntest_time = [i+len(train)+1 for i in range(len(test))]\nlen(train_time), len(test_time)","aeb37e03":"LR_train = train.copy()\nLR_test = test.copy()","5790bbf7":"LR_train['time'] = train_time\nLR_test['time'] = test_time","917d74dd":"lr = LinearRegression()\nlr.fit(LR_train[['time']],LR_train['Price'].values)","50f42763":"test_predictions_model1         = lr.predict(LR_test[['time']])\nLR_test['forecast'] = test_predictions_model1\n\nplt.figure(figsize=(13,6))\nplt.plot( train['Price'], label='Train')\nplt.plot(test['Price'], label='Test')\nplt.plot(LR_test['forecast'], label='Regression On Time_Test Data')\nplt.legend(loc='best')\nplt.grid();","dc22f740":"def mape(actual,pred):\n    return round((np.mean(abs(actual-pred)\/actual))*100,2)","51338591":"# Get MAPE of the model\n\nmape_model1_test = mape(test['Price'].values,test_predictions_model1)\nprint(\"For RegressionOnTime forecast on the Test Data,  MAPE is %3.3f\" %(mape_model1_test),\"%\")","cbdbe15c":"results = pd.DataFrame({'Test MAPE (%)': [mape_model1_test]},index=['RegressionOnTime'])\nresults","e624078f":"Naive_train = train.copy()\nNaive_test = test.copy()","86ce7cce":"Naive_test['naive'] = np.asarray(train['Price'])[len(np.asarray(train['Price']))-1]\nNaive_test['naive'].head()","d7f2b619":"plt.figure(figsize=(12,8))\nplt.plot(Naive_train['Price'], label='Train')\nplt.plot(test['Price'], label='Test')\nplt.plot(Naive_test['naive'], label='Naive Forecast on Test Data')\nplt.legend(loc='best')\nplt.title(\"Naive Forecast\")\nplt.grid();","afd5d3c4":"# Get MAPE of the model\n\nmape_model2_test = mape(test['Price'].values,Naive_test['naive'].values)\nprint(\"For Naive forecast on the Test Data,  MAPE is %3.3f\" %(mape_model2_test),\"%\")","4d506191":"resultsDf_2 = pd.DataFrame({'Test MAPE (%)': [mape_model2_test]},index=['NaiveModel'])\n\nresults = pd.concat([results, resultsDf_2])\nresults","8afcfd92":"SimpleAvg_train = train.copy()\nSimpleAvg_test = test.copy()\nSimpleAvg_test['mean_forecast'] = train['Price'].mean()\nSimpleAvg_test.head()","cfaa1843":"plt.figure(figsize=(12,8))\nplt.plot(SimpleAvg_train['Price'], label='Train')\nplt.plot(SimpleAvg_test['Price'], label='Test')\nplt.plot(SimpleAvg_test['mean_forecast'], label='Simple Average on Test Data')\nplt.legend(loc='best')\nplt.title(\"Simple Average Forecast\")\nplt.grid();","0e183125":"## Test Data - MAPE\n\nmape_model3_test = mape(test['Price'].values,SimpleAvg_test['mean_forecast'].values)\nprint(\"For Simple Average forecast on the Test Data,  MAPE is %3.3f\" %(mape_model3_test),\"%\")","e55481a6":"resultsDf_3 = pd.DataFrame({'Test MAPE (%)': [mape_model3_test]},index=['SimpleAverageModel'])\n\nresults = pd.concat([results, resultsDf_3])\nresults","874c3dc5":"Mvg_Avg = df.copy()\nMvg_Avg['Trailing_2'] = Mvg_Avg['Price'].rolling(2).mean()\nMvg_Avg['Trailing_3'] = Mvg_Avg['Price'].rolling(3).mean()\nMvg_Avg['Trailing_5'] = Mvg_Avg['Price'].rolling(5).mean()\nMvg_Avg['Trailing_7'] = Mvg_Avg['Price'].rolling(7).mean()\nMvg_Avg.head()","b1036ec0":"## Plotting on the whole data\n\nplt.figure(figsize=(16,8))\nplt.plot(Mvg_Avg['Price'], label='Train')\nplt.plot(Mvg_Avg['Trailing_2'],label='2 Point Moving Average')\nplt.plot(Mvg_Avg['Trailing_3'],label='3 Point Moving Average')\nplt.plot(Mvg_Avg['Trailing_5'],label = '5 Point Moving Average')\nplt.plot(Mvg_Avg['Trailing_7'],label = '7 Point Moving Average')\n\nplt.legend(loc = 'best')\nplt.grid();","680cdd62":"#Creating train and test set \ntrailing_Mvg_Avg_train=Mvg_Avg[Mvg_Avg.index.year <= 2015] \ntrailing_Mvg_Avg_test=Mvg_Avg[Mvg_Avg.index.year > 2015]","afe6ce00":"## Plotting on both the Training and Test data\n\nplt.figure(figsize=(16,8))\nplt.plot(trailing_Mvg_Avg_train['Price'], label='Train')\nplt.plot(trailing_Mvg_Avg_test['Price'], label='Test')\n\nplt.plot(trailing_Mvg_Avg_train['Trailing_2'],label='2 Point Trailing Moving Average on Training Set')\nplt.plot(trailing_Mvg_Avg_train['Trailing_3'],label='3 Point Trailing Moving Average on Training Set')\nplt.plot(trailing_Mvg_Avg_train['Trailing_5'],label = '5 Point Trailing Moving Average on Training Set')\nplt.plot(trailing_Mvg_Avg_train['Trailing_7'],label = '7 Point Trailing Moving Average on Training Set')\n\nplt.plot(trailing_Mvg_Avg_test['Trailing_2'], label='2 Point Trailing Moving Average on Test Set')\nplt.plot(trailing_Mvg_Avg_test['Trailing_3'], label='3 Point Trailing Moving Average on Test Set')\nplt.plot(trailing_Mvg_Avg_test['Trailing_5'],label = '5 Point Trailing Moving Average on Test Set')\nplt.plot(trailing_Mvg_Avg_test['Trailing_7'],label = '7 Point Trailing Moving Average on Test Set')\nplt.legend(loc = 'best')\nplt.grid();","c71c99cd":"## Test Data - MAPE --> 2 point Trailing MA\n\nmape_model4_test_2 = mape(test['Price'].values,trailing_Mvg_Avg_test['Trailing_2'].values)\nprint(\"For 2 point Moving Average Model forecast on the Training Data,  MAPE is %3.3f\" %(mape_model4_test_2),\"%\")\n\n## Test Data - MAPE  --> 3 point Trailing MA\n\nmape_model4_test_3 = mape(test['Price'].values,trailing_Mvg_Avg_test['Trailing_3'].values)\nprint(\"For 3 point Moving Average Model forecast on the Training Data,  MAPE is %3.3f\" %(mape_model4_test_3),\"%\")\n\n## Test Data - MAPE --> 5 point Trailing MA\n\nmape_model4_test_5 = mape(test['Price'].values,trailing_Mvg_Avg_test['Trailing_5'].values)\nprint(\"For 5 point Moving Average Model forecast on the Training Data,  MAPE is %3.3f\" %(mape_model4_test_5),\"%\")\n\n## Test Data - MAPE  --> 7 point Trailing MA\n\nmape_model4_test_7 = mape(test['Price'].values,trailing_Mvg_Avg_test['Trailing_7'].values)\nprint(\"For 7 point Moving Average Model forecast on the Training Data,  MAPE is %3.3f \" %(mape_model4_test_7),\"%\")","f374ce5a":"resultsDf_4 = pd.DataFrame({'Test MAPE (%)': [mape_model4_test_2,mape_model4_test_3\n                                          ,mape_model4_test_5,mape_model4_test_7]}\n                           ,index=['2pointTrailingMovingAverage','3pointTrailingMovingAverage'\n                                   ,'5pointTrailingMovingAverage','7pointTrailingMovingAverage'])\n\nresults = pd.concat([results, resultsDf_4])\nresults","0873cb06":"SES_train = train.copy()\nSES_test = test.copy()","22991fd4":"model_SES = SimpleExpSmoothing(SES_train['Price'])\nmodel_SES_autofit = model_SES.fit(optimized=True)","78669bb7":"model_SES_autofit.params","1b528fb1":"SES_test['predict'] = model_SES_autofit.forecast(steps=len(test))\nSES_test.head()","c051b475":"## Plotting on both the Training and Test data\n\nplt.figure(figsize=(16,8))\nplt.plot(SES_train['Price'], label='Train')\nplt.plot(SES_test['Price'], label='Test')\n\nplt.plot(SES_test['predict'], label='Alpha =0.995 SES predictions on Test Set')\n\nplt.legend(loc='best')\nplt.grid()\nplt.title('Alpha =0.995 Predictions');","75187297":"## Test Data\n\nmape_model5_test_1 = mape(SES_test['Price'].values,SES_test['predict'].values)\nprint(\"For Alpha =0.995 SES Model forecast on the Test Data, MAPE is %3.3f\" %(mape_model5_test_1),\"%\")","c5904f2c":"resultsDf_5 = pd.DataFrame({'Test MAPE (%)': [mape_model5_test_1]},index=['Alpha=0.995,SimpleExponentialSmoothing'])\n\nresults = pd.concat([results, resultsDf_5])\nresults","3465e9a6":"resultsDf_6 = pd.DataFrame({'Alpha Values':[],'Train MAPE':[],'Test MAPE': []})\n\nfor i in np.arange(0.3,1,0.1):\n    model_SES_alpha_i = model_SES.fit(smoothing_level=i,optimized=False,use_brute=True)\n    SES_train['predict',i] = model_SES_alpha_i.fittedvalues\n    SES_test['predict',i] = model_SES_alpha_i.forecast(steps=55)\n    \n    mape_model5_train_i = mape(SES_train['Price'].values,SES_train['predict',i].values)\n    \n    mape_model5_test_i = mape(SES_test['Price'].values,SES_test['predict',i].values)\n    \n    resultsDf_6 = resultsDf_6.append({'Alpha Values':i,'Train MAPE':mape_model5_train_i \n                                      ,'Test MAPE':mape_model5_test_i}, ignore_index=True)","e760fdd1":"resultsDf_6.sort_values(by=['Test MAPE'],ascending=True)","e61758af":"## Plotting on both the Training and Test data\n\nplt.figure(figsize=(18,9))\nplt.plot(SES_train['Price'], label='Train')\nplt.plot(SES_test['Price'], label='Test')\n\nplt.plot(SES_test['predict'], label='Alpha =1 SES predictions on Test Set')\n\nplt.plot(SES_test['predict', 0.3], label='Alpha =0.3 SES predictions on Test Set')\n\n\n\nplt.legend(loc='best')\nplt.grid();","3274114e":"resultsDf_6_1 = pd.DataFrame({'Test MAPE (%)': [resultsDf_6.sort_values(by=['Test MAPE'],ascending=True).values[0][2]]}\n                           ,index=['Alpha=0.3,SimpleExponentialSmoothing'])\n\nresults = pd.concat([results, resultsDf_6_1])\nresults","3c7b57ec":"DES_train = train.copy()\nDES_test = test.copy()","0f29ecf4":"model_DES = Holt(DES_train['Price'])","d9c2fbaa":"resultsDf_7 = pd.DataFrame({'Alpha Values':[],'Beta Values':[],'Train MAPE':[],'Test MAPE': []})\n\nfor i in np.arange(0.3,1.1,0.1):\n    for j in np.arange(0.3,1.1,0.1):\n        model_DES_alpha_i_j = model_DES.fit(smoothing_level=i,smoothing_trend=j,optimized=False,use_brute=True)\n        DES_train['predict',i,j] = model_DES_alpha_i_j.fittedvalues\n        DES_test['predict',i,j] = model_DES_alpha_i_j.forecast(steps=55)\n        \n        mape_model6_train = mape(DES_train['Price'].values,DES_train['predict',i,j].values)\n        \n        mape_model6_test = mape(DES_test['Price'].values,DES_test['predict',i,j].values)\n        \n        resultsDf_7 = resultsDf_7.append({'Alpha Values':i,'Beta Values':j,'Train MAPE':mape_model6_train\n                                          ,'Test MAPE':mape_model6_test}, ignore_index=True)","d5dc8981":"resultsDf_7.sort_values(by=['Test MAPE']).head()","045b1809":"## Plotting on both the Training and Test data\n\nplt.figure(figsize=(18,9))\nplt.plot(DES_train['Price'], label='Train')\nplt.plot(DES_test['Price'], label='Test')\n\nplt.plot(DES_test['predict', 0.3, 0.3], label='Alpha=0.3,Beta=0.3,DoubleExponentialSmoothing predictions on Test Set')\n\n\nplt.legend(loc='best')\nplt.grid();","2c59468c":"resultsDf_7_1 = pd.DataFrame({'Test MAPE (%)': [resultsDf_7.sort_values(by=['Test MAPE']).values[0][3]]}\n                           ,index=['Alpha=0.3,Beta=0.3,DoubleExponentialSmoothing'])\n\nresults = pd.concat([results, resultsDf_7_1])\nresults","8518d760":"TES_train = train.copy()\nTES_test = test.copy()","b9cb6f4f":"model_TES = ExponentialSmoothing(TES_train['Price'],trend='additive',seasonal='additive',freq='M')\nmodel_TES_autofit = model_TES.fit()","6e0791b1":"model_TES_autofit.params","56350334":"## Prediction on the test data\n\nTES_test['auto_predict'] = model_TES_autofit.forecast(steps=len(test))\nTES_test.head()","a26e4e5b":"## Plotting on both the Training and Test using autofit\n\nplt.figure(figsize=(18,9))\nplt.plot(TES_train['Price'], label='Train')\nplt.plot(TES_test['Price'], label='Test')\n\nplt.plot(TES_test['auto_predict'], label='Alpha=0.99,Beta=0.05,Gamma=0.001,TripleExponentialSmoothing predictions on Test Set')\n\n\nplt.legend(loc='best')\nplt.grid();","7494f3b5":"## Test Data\n\nmape_model6_test_1 = mape(TES_test['Price'].values,TES_test['auto_predict'].values)\nprint(\"For A=0.99,B=0.05,G=0.001, Triple ES Model forecast on the Test Data,  MAPE is %3.3f\" %(mape_model6_test_1),\"%\")","e644ba68":"resultsDf_8_1 = pd.DataFrame({'Test MAPE (%)': [mape_model6_test_1]}\n                           ,index=['Alpha=0.99,Beta=0.05,Gamma=0.001,TripleExponentialSmoothing'])\n\nresults = pd.concat([results, resultsDf_8_1])\nresults","78641a84":"## First we will define an empty dataframe to store our values from the loop\n\nresultsDf_8_2 = pd.DataFrame({'Alpha Values':[],'Beta Values':[],'Gamma Values':[],'Train MAPE':[],'Test MAPE': []})\nresultsDf_8_2","498f2cce":"for i in np.arange(0.3,1.1,0.1):\n    for j in np.arange(0.3,1.1,0.1):\n        for k in np.arange(0.3,1.1,0.1):\n            model_TES_alpha_i_j_k = model_TES.fit(smoothing_level=i,smoothing_trend=j,smoothing_seasonal=k,optimized=False,use_brute=True)\n            TES_train['predict',i,j,k] = model_TES_alpha_i_j_k.fittedvalues\n            TES_test['predict',i,j,k] = model_TES_alpha_i_j_k.forecast(steps=55)\n        \n            mape_model8_train = mape(TES_train['Price'].values,TES_train['predict',i,j,k].values)\n            \n            mape_model8_test = mape(TES_test['Price'].values,TES_test['predict',i,j,k].values)\n            \n            resultsDf_8_2 = resultsDf_8_2.append({'Alpha Values':i,'Beta Values':j,'Gamma Values':k,\n                                                  'Train MAPE':mape_model8_train,'Test MAPE':mape_model8_test}\n                                                 , ignore_index=True)","2b1f5793":"resultsDf_8_2.sort_values(by=['Test MAPE']).head()","5ef9fc9a":"model_TES_alpha_best = model_TES.fit(smoothing_level=0.4,\n                                      smoothing_trend=0.3,\n                                      smoothing_seasonal=0.6,\n                                      optimized=False,\n                                      use_brute=True)\nTES_train['predict',0.4,0.3,0.6] = model_TES_alpha_best.fittedvalues\nTES_test['predict',0.4,0.3,0.6] = model_TES_alpha_best.forecast(steps=55)","05beefdd":"## Plotting on both the Training and Test data using brute force alpha, beta and gamma determination\n\nplt.figure(figsize=(18,9))\nplt.plot(TES_train['Price'], label='Train')\nplt.plot(TES_test['Price'], label='Test')\n\n#The value of alpha and beta is taken like that by python\nplt.plot(TES_test['predict', 0.4, 0.3, 0.6], label='Alpha=0.4,Beta=0.3,Gamma=0.6,TES predictions on Test Set')\n\n\nplt.legend(loc='best')\nplt.grid();","85b50558":"## Test Data\n\nmape_model7_test = mape(TES_test['Price'].values,TES_test['predict',0.4,0.3,0.6].values)\nprint(\"For A=0.4,B=0.3,G=0.6, Triple ES Model forecast on the Test Data,  MAPE is %3.3f\" %(mape_model7_test),\"%\")","0f6e8d84":"resultsDf_9_1 = pd.DataFrame({'Test MAPE (%)': [mape_model7_test]}\n                           ,index=['Alpha=0.4,Beta=0.3,Gamma=0.6,TripleExponentialSmoothing'])\n\nresults = pd.concat([results, resultsDf_9_1])\nresults","52529ea4":"final_model =  ExponentialSmoothing(df,\n                                  trend='additive',\n                                  seasonal='additive').fit(smoothing_level=0.4,\n                                                           smoothing_trend=0.3,\n                                                           smoothing_seasonal=0.6)","b872b791":"MAPE_final_model = mape(df['Price'].values,final_model.fittedvalues)\n\nprint('MAPE:',MAPE_final_model)","8f45624e":"# Getting the predictions for the same number of times stamps that are present in the test data\nprediction = final_model.forecast(steps=len(test))","7ec98664":"# Compute 95% confidence interval for predicted values\npred_df = pd.DataFrame({'lower_CI':prediction - 1.96*np.std(final_model.resid,ddof=1),\n                        'prediction':prediction,\n                        'upper_CI': prediction + 1.96*np.std(final_model.resid,ddof=1)})\npred_df.head()","385e233a":"# plot the forecast along with the confidence band\n\naxis = df.plot(label='Actual', figsize=(15,8))\npred_df['prediction'].plot(ax=axis, label='Forecast', alpha=0.5)\naxis.fill_between(pred_df.index, pred_df['lower_CI'], pred_df['upper_CI'], color='k', alpha=.15)\naxis.set_xlabel('Year-Months')\naxis.set_ylabel('Price')\nplt.legend(loc='best')\nplt.grid()\nplt.show()","255f32d8":"### Model 4 - Moving Average <a class=\"anchor\" id=\"section3.5\"><\/a>","7061f2d8":"### Model 7 - Triple Exponential Smoothing <a class=\"anchor\" id=\"section3.8\"><\/a>","c8a348f9":"### Import libraries <a class=\"anchor\" id=\"section_1.1\"><\/a>","30e39fa4":"[![green-arrow-over-gold-bars.jpg](https:\/\/i.postimg.cc\/XYgHHFT4\/green-arrow-over-gold-bars.jpg)](https:\/\/postimg.cc\/Yvhzj4rs)","0a730941":"### Model 2 - Naive prediction <a class=\"anchor\" id=\"section3.3\"><\/a>","ce6d2b6d":"## Gold Price - Analysis & Forecasting","daf19581":"1. The coefficient of variation (CV) is a statistical measure of the relative dispersion of data points in a data series around the mean.\n2. In finance, the coefficient of variation allows investors to determine how much volatility, or risk, is assumed in comparison to the amount of return expected from investments.\n3. The lower the ratio of the standard deviation to mean return, the better risk-return trade-off.\n\nLet us look at the CV values for each year in Gold price","a26f8451":"### Model 1 - Linear Regression <a class=\"anchor\" id=\"section3.2\"><\/a>","8a6872fe":"### Exploratory Data Analysis <a class=\"anchor\" id=\"section2\"><\/a>","05befd9c":"### Time Series - Forecasting models <a class=\"anchor\" id=\"section3\"><\/a>","9ec50868":"### Model 3 - Simple Average <a class=\"anchor\" id=\"section3.4\"><\/a>","2cd73e61":"### END OF NOTEBOOK","83fe7e25":"### Final Model <a class=\"anchor\" id=\"section4\"><\/a>","6c17f007":"**Inference**\n\n1. The CV value reached its highest in year 1978 near to 25%, which could have made the asset as highly risky\n2. But in 2020, the CV value is closer to 5%, which makes the asset viable for good investment","ebf22154":"### Model 5 - Simple Exponential Smoothing <a class=\"anchor\" id=\"section3.6\"><\/a>","d743f93d":"### Model 6 - Double Exponential smoothing <a class=\"anchor\" id=\"section3.7\"><\/a>","d0dc9940":"### Import data <a class=\"anchor\" id=\"section_1.2\"><\/a>","ec848a59":"### Train - Test split to build Time series forecasting models <a class=\"anchor\" id=\"section3.1\"><\/a>","e0466843":"### Import libraries & data <a class=\"anchor\" id=\"section1\"><\/a>","907a74b0":"### Table of Contents\n\n* [Import libraries & data](#section1)\n    * [Import libraries](#section_1.1)\n    * [Import data](#section_1.2)\n* [Exploratory data analysis](#section2)\n    * [Understanding the data](#section2.1)\n    * [Visual Analysis](#section2.2)\n* [Time series - Forecasting Model](#section3)\n    * [Train - test split](#section3.1)\n    * [Model 1 - Simple Linear Regression](#section3.2)\n    * [Model 2 - Naive Prediction](#section3.3)\n    * [Model 3 - Simple Average](#section3.4)\n    * [Model 4 - Moving Average](#section3.5)\n    * [Model 5 - Simple Exponential smoothing](#section3.6)\n    * [Model 6 - Double Exponential smoothing](#section3.7)\n    * [Model 7 - Triple Exponential smoothing](#section3.8)\n* [Final Model prediction](#section4)","80242944":"### Analysis in Coefficient of variation","77cd8b3f":"### Understanding the data <a class=\"anchor\" id=\"section2.1\"><\/a>","b477a611":"**Inference**\n\n1. The Average gold price in last 70 years is **$416.56**\n\n2. **Only 25% of the time**, the gold price is above **$447.07**\n\n3. Highest Gold price ever touched is **$1840.81**","f9114dba":"### Visual Analysis <a class=\"anchor\" id=\"section2.2\"><\/a>"}}