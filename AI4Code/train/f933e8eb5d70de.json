{"cell_type":{"6dbc4ecf":"code","e8cc9c7c":"code","fcee016f":"code","f502fc90":"code","7cd364b2":"code","76ee166a":"code","612ba1de":"code","c68bbc8f":"code","30170f0e":"code","003c6979":"code","35514163":"code","cf44b4fb":"code","da792643":"code","75d47864":"code","b0e1095e":"code","71d498fc":"code","a7da2f2c":"code","a5ea29ac":"code","ff7f1a1e":"code","86b6eef7":"code","abe82255":"code","5ef5c105":"code","4b10f7d9":"code","79b0e389":"code","14a468ad":"code","385db083":"code","41b5575f":"code","5eb09e0e":"code","e23e1f94":"code","fbf042ed":"code","61084b81":"code","9f94a9cf":"code","cff6dbb2":"code","077c7f9c":"code","8d5b2df2":"code","e547b1c4":"code","d79c1df2":"code","f65a708d":"code","fbe683d2":"code","0dbe5d1c":"code","79cf8e13":"code","524ec752":"code","096ff0cb":"code","4467a214":"code","093793df":"code","7493ccf8":"code","c825ae2d":"code","93571be6":"code","751e2f3e":"code","e7654375":"code","0298c907":"code","0eece8ab":"markdown","24db3155":"markdown","76447375":"markdown","48e18d36":"markdown","1a355861":"markdown","a1701de9":"markdown","8ceadfe4":"markdown","3addfb42":"markdown","ee09a97c":"markdown","ecf2dc5d":"markdown","472fad46":"markdown","b8a96eb6":"markdown","638acbc8":"markdown","b89c8af5":"markdown","ec35e4ba":"markdown","7bd03c6d":"markdown"},"source":{"6dbc4ecf":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","e8cc9c7c":"df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')","fcee016f":"df.head(5)","f502fc90":"df.info()","7cd364b2":"sns.heatmap(df.isnull())","76ee166a":"df.drop(['location','keyword'],axis=1,inplace=True)","612ba1de":"df","c68bbc8f":"real = df[df['target']==1]","30170f0e":"real","003c6979":"unreal = df[df['target']==0]","35514163":"unreal","cf44b4fb":"print('real disaster message percentage:',(len(real)\/len(df))*100)","da792643":"print('fake disaster message percentage:',(len(unreal)\/len(df))*100)","75d47864":"sns.countplot(df['target'])","b0e1095e":"import string\n","71d498fc":"string.punctuation","a7da2f2c":"from nltk.corpus import stopwords","a5ea29ac":"stopwords.words('english');","ff7f1a1e":"def message_cleaning(message):\n    test_punc_removed = [char   for char in message if char not in string.punctuation]\n    test_punc_removed_joined = ''.join(test_punc_removed)\n    test_punc_removed_joined_clean = [word   for word in test_punc_removed_joined.split(' ') if word not in stopwords.words('english')]\n    return test_punc_removed_joined_clean","86b6eef7":"from sklearn.feature_extraction.text import CountVectorizer","abe82255":"vectorizer = CountVectorizer(analyzer=message_cleaning)","5ef5c105":"disaster_tweet_vectorizer = vectorizer.fit_transform(df['text'])","4b10f7d9":"print(vectorizer.get_feature_names());","79b0e389":"print(disaster_tweet_vectorizer.toarray())","14a468ad":"disaster_tweet_vectorizer.shape","385db083":"label = df['target']","41b5575f":"label.shape","5eb09e0e":"X = disaster_tweet_vectorizer","e23e1f94":"X = X.toarray()","fbf042ed":"X","61084b81":"y = label","9f94a9cf":"from sklearn.model_selection import train_test_split","cff6dbb2":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)","077c7f9c":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n","8d5b2df2":"LR = LogisticRegression()\nDTC = DecisionTreeClassifier()\nRFC = RandomForestClassifier()\nNB = GaussianNB()","e547b1c4":"RFC.fit(X_train,y_train)\nDTC.fit(X_train,y_train)\nNB.fit(X_train,y_train)\nLR.fit(X_train,y_train)","d79c1df2":"predict1 = RFC.predict(X_test)\npredict2 = DTC.predict(X_test)\npredict3 = NB.predict(X_test)\npredict4 = LR.predict(X_test)","f65a708d":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix","fbe683d2":"#print(classification_report(y_test,prediction))","0dbe5d1c":"print(accuracy_score(y_test,predict1))\nprint('\\n')\nprint(accuracy_score(y_test,predict2))\nprint('\\n')\nprint(accuracy_score(y_test,predict3))\nprint('\\n')\nprint(accuracy_score(y_test,predict4))","79cf8e13":"test_df = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","524ec752":"test_df.head()","096ff0cb":"test_df.drop(['keyword','location'],axis=1,inplace= True)","4467a214":"test_df.head()","093793df":"test_vectorizer = vectorizer.transform(test_df['text'])","7493ccf8":"test_vectorizer.shape","c825ae2d":"final_predictions = LR.predict(test_vectorizer)","93571be6":"final_predictions","751e2f3e":"submission_df = pd.DataFrame()","e7654375":"submission_df['id'] = test_df['id']\nsubmission_df['target'] = final_predictions","0298c907":"submission_df['target'].value_counts()","0eece8ab":"Highest Accuracy Using Logisitic Regression.","24db3155":"Making Prediction","76447375":"Data Visualisation","48e18d36":"# Bag Of Words","1a355861":"We have quite balanced data!","a1701de9":"The columns - keyword and location will be dropped as they are of no use to us.","8ceadfe4":"We are going to vectorize the text along with increasing the readablity of the text by removing the punctuations and countwords!","3addfb42":"![image.png](attachment:image.png)","ee09a97c":"The accuracy is quite good!","ecf2dc5d":"So there are 7613 disaster tweets and 26918 unique words.","472fad46":"This is the simplest method in NLP. It includes a basic concept that is creating the text into a vector form which includes the frequency of each word in the sentence - hence creating a vector which will be an independent feature for our model and the classification of text(1 or 0 as in this dataset) will be the dependent feature. Model can be trained using any algorithm (Decision Tree Classifier, Naive Bais). However this method is less efficient as it ignores the semantic part of the text. ","b8a96eb6":"**Checking the Performance**","638acbc8":"Lets predict for testing dataset","b89c8af5":"This matrix shows the count of unique words (as shown in previous cell output) in each sentance.","ec35e4ba":"Lets Predict!","7bd03c6d":"**Training the Model**"}}