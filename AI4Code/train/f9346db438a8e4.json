{"cell_type":{"b86306cb":"code","83da0d15":"code","2be5087b":"code","76d56853":"code","61a120aa":"code","e8f7b1cd":"code","05196ee9":"code","9bd90cf1":"code","759200e4":"code","5010959e":"code","b4e46a32":"code","71b931c5":"code","f37a8efc":"code","0e117226":"code","15fa48df":"code","33c456d6":"code","fda53182":"code","de6ed6d1":"code","8bf909b6":"code","206033fc":"code","048de470":"code","63540759":"code","b3bdce96":"code","cf273696":"code","09b63932":"code","7e4a3ff1":"code","c1525046":"code","b22e7938":"code","ea5b6c66":"code","c2cc5c19":"code","a6fc40e2":"code","0189f050":"code","e730bf1f":"code","1fff7a72":"code","cb4789cf":"code","1ece0210":"code","117ee3f1":"code","0aca4480":"code","950b39f6":"code","9e45a950":"code","149ed5e0":"code","978b0419":"code","6628fcce":"code","b648af56":"code","66997f1a":"code","78ee14c5":"code","525e86c3":"code","a4745e36":"code","782bb8ac":"code","a83fde99":"code","714a53bc":"code","58b01141":"code","8ef28ab2":"code","c5bf842e":"code","ff4da8e9":"code","5b0fdc25":"code","e0710198":"code","d9c0641d":"code","9b490172":"code","62512d7f":"code","646bb5d4":"code","47e41042":"code","67a89ee0":"code","d2b6630a":"code","4ce97b5d":"code","518b89c7":"code","8b32e274":"code","39a7ee5b":"code","a140199f":"code","e7f4bc07":"code","0a5038c3":"code","6f43b529":"code","6c9f8aa9":"code","107f93ae":"code","fbd97d7c":"code","2d52e79a":"code","6047d365":"code","e217627c":"code","3e86f9d3":"code","5592a64e":"code","015426a1":"code","984b9221":"markdown","b18d60d2":"markdown","3ad3960d":"markdown","43e1ee30":"markdown","abfb48af":"markdown","73dbdb71":"markdown","a87b8f27":"markdown","bf9b6177":"markdown","4441f307":"markdown","bb4f7f88":"markdown","752cb08f":"markdown","302f28ca":"markdown","95698c1c":"markdown","2c74c325":"markdown","ad672960":"markdown","7e9d60b9":"markdown","7e9db93c":"markdown","a0f50f93":"markdown","d1a1e3c7":"markdown","2f58e8a0":"markdown","c0efa5f1":"markdown","63a47d6e":"markdown","1db1722e":"markdown","7de384da":"markdown","af122e9f":"markdown","f71f78ed":"markdown","a4735693":"markdown","6a60bbf2":"markdown","c7cb9cb8":"markdown","6312a565":"markdown","3450d6a3":"markdown","baaa8027":"markdown","fcc0b288":"markdown","34d0c7e5":"markdown","2dc2b17a":"markdown","3e9beb02":"markdown","7b5488d3":"markdown","d684c887":"markdown","6de70e85":"markdown","f1d709c1":"markdown","a2b963c9":"markdown","1b3d9b9d":"markdown","98298c0d":"markdown","c75474fa":"markdown","d1cb49b6":"markdown","c54fbe44":"markdown","e0baae75":"markdown","b9985db4":"markdown","10180c29":"markdown","ff5d743b":"markdown"},"source":{"b86306cb":"import torch\n\n# If there's a GPU available...\nif torch.cuda.is_available():    \n\n    # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","83da0d15":"!nvidia-smi","2be5087b":"!pip install -qq sentencepiece","76d56853":"!pip install -qq transformers","61a120aa":"!pip install -qq nlpaug","e8f7b1cd":"!pip install -qq kaggle\n!pip install -qq --upgrade --force-reinstall --no-deps kaggle","05196ee9":"!mkdir \/root\/.kaggle","9bd90cf1":"!mv kaggle.json \/root\/.kaggle\/","759200e4":"!chmod 600 \/root\/.kaggle\/kaggle.json","5010959e":"!kaggle competitions download -c text-classification-int20h","b4e46a32":"!unzip -qq text-classification-int20h.zip","71b931c5":"from typing import List, Mapping, Tuple","f37a8efc":"import logging\nimport os\nfrom pathlib import Path\nfrom tqdm import tqdm\nfrom shutil import copyfile\nimport multiprocessing","0e117226":"import pandas as pd\nimport numpy as np\nimport re\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom transformers import AutoConfig, AutoModel, AdamW, get_linear_schedule_with_warmup\nfrom transformers import AutoTokenizer\nimport nlpaug.augmenter.word as naw\nimport nltk\nnltk.download('stopwords')","15fa48df":"import seaborn as sns\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc","33c456d6":"from sklearn.model_selection import train_test_split","fda53182":"%matplotlib inline\n%config InlineBackend.figure_format='retina'\n\nsns.set(style='whitegrid', palette='muted', font_scale=1.2)\n\nHAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n\nsns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n\nrcParams['figure.figsize'] = 12, 8\n\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)","de6ed6d1":"TEXT_COLUMN = 'review'\nLABEL_COLUMN = 'sentiment'\n\ntrain_df = pd.read_csv('train.csv')\ntest_df = pd.read_csv('test.csv')\n\nprint('Shape of train_df =', train_df.shape)\nprint('Shape of test_df =', test_df.shape)","8bf909b6":"train_df.head()","206033fc":"ax = sns.countplot(train_df[LABEL_COLUMN])\nplt.xlabel('type of comment');","048de470":"# some constants for text cleaning\nCONTRACTION_MAPPING = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\nsymbols_to_isolate = '.,?!-;*\"\u2026:\u2014()%#$&_\/@\uff3c\u30fb\u03c9+=\u201d\u201c[]^\u2013>\\\\\u00b0<~\u2022\u2260\u2122\u02c8\u028a\u0252\u221e\u00a7{}\u00b7\u03c4\u03b1\u2764\u263a\u0261|\u00a2\u2192\u0336`\u2765\u2501\u2523\u252b\u2517\uff2f\u25ba\u2605\u00a9\u2015\u026a\u2714\u00ae\\x96\\x92\u25cf\u00a3\u2665\u27a4\u00b4\u00b9\u2615\u2248\u00f7\u2661\u25d0\u2551\u25ac\u2032\u0254\u02d0\u20ac\u06e9\u06de\u2020\u03bc\u2712\u27a5\u2550\u2606\u02cc\u25c4\u00bd\u02bb\u03c0\u03b4\u03b7\u03bb\u03c3\u03b5\u03c1\u03bd\u0283\u272c\uff33\uff35\uff30\uff25\uff32\uff29\uff34\u263b\u00b1\u264d\u00b5\u00ba\u00be\u2713\u25fe\u061f\uff0e\u2b05\u2105\u00bb\u0412\u0430\u0432\u2763\u22c5\u00bf\u00ac\u266b\uff23\uff2d\u03b2\u2588\u2593\u2592\u2591\u21d2\u2b50\u203a\u00a1\u2082\u2083\u2767\u25b0\u2594\u25de\u2580\u2582\u2583\u2584\u2585\u2586\u2587\u2199\u03b3\u0304\u2033\u2639\u27a1\u00ab\u03c6\u2153\u201e\u270b\uff1a\u00a5\u0332\u0305\u0301\u2219\u201b\u25c7\u270f\u25b7\u2753\u2757\u00b6\u02da\u02d9\uff09\u0441\u0438\u02bf\u2728\u3002\u0251\\x80\u25d5\uff01\uff05\u00af\u2212\ufb02\ufb01\u2081\u00b2\u028c\u00bc\u2074\u2044\u2084\u2320\u266d\u2718\u256a\u25b6\u262d\u272d\u266a\u2614\u2620\u2642\u2603\u260e\u2708\u270c\u2730\u2746\u2619\u25cb\u2023\u2693\u5e74\u220e\u2112\u25aa\u2599\u260f\u215b\uff43\uff41\uff53\u01c0\u212e\u00b8\uff57\u201a\u223c\u2016\u2133\u2744\u2190\u263c\u22c6\u0292\u2282\u3001\u2154\u00a8\u0361\u0e4f\u26be\u26bd\u03a6\u00d7\u03b8\uffe6\uff1f\uff08\u2103\u23e9\u262e\u26a0\u6708\u270a\u274c\u2b55\u25b8\u25a0\u21cc\u2610\u2611\u26a1\u2604\u01eb\u256d\u2229\u256e\uff0c\u4f8b\uff1e\u0295\u0250\u0323\u0394\u2080\u271e\u2508\u2571\u2572\u258f\u2595\u2503\u2570\u258a\u258b\u256f\u2533\u250a\u2265\u2612\u2191\u261d\u0279\u2705\u261b\u2669\u261e\uff21\uff2a\uff22\u25d4\u25e1\u2193\u2640\u2b06\u0331\u210f\\x91\u2800\u02e4\u255a\u21ba\u21e4\u220f\u273e\u25e6\u266c\u00b3\u306e\uff5c\uff0f\u2235\u2234\u221a\u03a9\u00a4\u261c\u25b2\u21b3\u25ab\u203f\u2b07\u2727\uff4f\uff56\uff4d\uff0d\uff12\uff10\uff18\uff07\u2030\u2264\u2215\u02c6\u269c\u2601'\nsymbols_to_delete = '\\n\ud83c\udf55\\r\ud83d\udc35\ud83d\ude11\\xa0\\ue014\\t\\uf818\\uf04a\\xad\ud83d\ude22\ud83d\udc36\ufe0f\\uf0e0\ud83d\ude1c\ud83d\ude0e\ud83d\udc4a\\u200b\\u200e\ud83d\ude01\u0639\u062f\u0648\u064a\u0647\u0635\u0642\u0623\u0646\u0627\u062e\u0644\u0649\u0628\u0645\u063a\u0631\ud83d\ude0d\ud83d\udc96\ud83d\udcb5\u0415\ud83d\udc4e\ud83d\ude00\ud83d\ude02\\u202a\\u202c\ud83d\udd25\ud83d\ude04\ud83c\udffb\ud83d\udca5\u1d0d\u028f\u0280\u1d07\u0274\u1d05\u1d0f\u1d00\u1d0b\u029c\u1d1c\u029f\u1d1b\u1d04\u1d18\u0299\u0493\u1d0a\u1d21\u0262\ud83d\ude0b\ud83d\udc4f\u05e9\u05dc\u05d5\u05dd\u05d1\u05d9\ud83d\ude31\u203c\\x81\u30a8\u30f3\u30b8\u6545\u969c\\u2009\ud83d\ude8c\u1d35\u035e\ud83c\udf1f\ud83d\ude0a\ud83d\ude33\ud83d\ude27\ud83d\ude40\ud83d\ude10\ud83d\ude15\\u200f\ud83d\udc4d\ud83d\ude2e\ud83d\ude03\ud83d\ude18\u05d0\u05e2\u05db\u05d7\ud83d\udca9\ud83d\udcaf\u26fd\ud83d\ude84\ud83c\udffc\u0b9c\ud83d\ude16\u1d20\ud83d\udeb2\u2010\ud83d\ude1f\ud83d\ude08\ud83d\udcaa\ud83d\ude4f\ud83c\udfaf\ud83c\udf39\ud83d\ude07\ud83d\udc94\ud83d\ude21\\x7f\ud83d\udc4c\u1f10\u1f76\u03ae\u03b9\u1f72\u03ba\u1f00\u03af\u1fc3\u1f34\u03be\ud83d\ude44\uff28\ud83d\ude20\\ufeff\\u2028\ud83d\ude09\ud83d\ude24\u26fa\ud83d\ude42\\u3000\u062a\u062d\u0643\u0633\u0629\ud83d\udc6e\ud83d\udc99\u0641\u0632\u0637\ud83d\ude0f\ud83c\udf7e\ud83c\udf89\ud83d\ude1e\\u2008\ud83c\udffe\ud83d\ude05\ud83d\ude2d\ud83d\udc7b\ud83d\ude25\ud83d\ude14\ud83d\ude13\ud83c\udffd\ud83c\udf86\ud83c\udf7b\ud83c\udf7d\ud83c\udfb6\ud83c\udf3a\ud83e\udd14\ud83d\ude2a\\x08\u2011\ud83d\udc30\ud83d\udc07\ud83d\udc31\ud83d\ude46\ud83d\ude28\ud83d\ude43\ud83d\udc95\ud835\ude0a\ud835\ude26\ud835\ude33\ud835\ude22\ud835\ude35\ud835\ude30\ud835\ude24\ud835\ude3a\ud835\ude34\ud835\ude2a\ud835\ude27\ud835\ude2e\ud835\ude23\ud83d\udc97\ud83d\udc9a\u5730\u7344\u8c37\u0443\u043b\u043a\u043d\u041f\u043e\u0410\u041d\ud83d\udc3e\ud83d\udc15\ud83d\ude06\u05d4\ud83d\udd17\ud83d\udebd\u6b4c\u821e\u4f0e\ud83d\ude48\ud83d\ude34\ud83c\udfff\ud83e\udd17\ud83c\uddfa\ud83c\uddf8\u043c\u03c5\u0442\u0455\u2935\ud83c\udfc6\ud83c\udf83\ud83d\ude29\\u200a\ud83c\udf20\ud83d\udc1f\ud83d\udcab\ud83d\udcb0\ud83d\udc8e\u044d\u043f\u0440\u0434\\x95\ud83d\udd90\ud83d\ude45\u26f2\ud83c\udf70\ud83e\udd10\ud83d\udc46\ud83d\ude4c\\u2002\ud83d\udc9b\ud83d\ude41\ud83d\udc40\ud83d\ude4a\ud83d\ude49\\u2004\u02e2\u1d52\u02b3\u02b8\u1d3c\u1d37\u1d3a\u02b7\u1d57\u02b0\u1d49\u1d58\\x13\ud83d\udeac\ud83e\udd13\\ue602\ud83d\ude35\u03ac\u03bf\u03cc\u03c2\u03ad\u1f78\u05ea\u05de\u05d3\u05e3\u05e0\u05e8\u05da\u05e6\u05d8\ud83d\ude12\u035d\ud83c\udd95\ud83d\udc45\ud83d\udc65\ud83d\udc44\ud83d\udd04\ud83d\udd24\ud83d\udc49\ud83d\udc64\ud83d\udc76\ud83d\udc72\ud83d\udd1b\ud83c\udf93\\uf0b7\\uf04c\\x9f\\x10\u6210\u90fd\ud83d\ude23\u23fa\ud83d\ude0c\ud83e\udd11\ud83c\udf0f\ud83d\ude2f\u0435\u0445\ud83d\ude32\u1f38\u1fb6\u1f41\ud83d\udc9e\ud83d\ude93\ud83d\udd14\ud83d\udcda\ud83c\udfc0\ud83d\udc50\\u202d\ud83d\udca4\ud83c\udf47\\ue613\u5c0f\u571f\u8c46\ud83c\udfe1\u2754\u2049\\u202f\ud83d\udc60\u300b\u0915\u0930\u094d\u092e\u093e\ud83c\uddf9\ud83c\uddfc\ud83c\udf38\u8521\u82f1\u6587\ud83c\udf1e\ud83c\udfb2\u30ec\u30af\u30b5\u30b9\ud83d\ude1b\u5916\u56fd\u4eba\u5173\u7cfb\u0421\u0431\ud83d\udc8b\ud83d\udc80\ud83c\udf84\ud83d\udc9c\ud83e\udd22\u0650\u064e\u044c\u044b\u0433\u044f\u4e0d\u662f\\x9c\\x9d\ud83d\uddd1\\u2005\ud83d\udc83\ud83d\udce3\ud83d\udc7f\u0f3c\u3064\u0f3d\ud83d\ude30\u1e37\u0417\u0437\u25b1\u0446\ufffc\ud83e\udd23\u5356\u6e29\u54e5\u534e\u8bae\u4f1a\u4e0b\u964d\u4f60\u5931\u53bb\u6240\u6709\u7684\u94b1\u52a0\u62ff\u5927\u574f\u7a0e\u9a97\u5b50\ud83d\udc1d\u30c4\ud83c\udf85\\x85\ud83c\udf7a\u0622\u0625\u0634\u0621\ud83c\udfb5\ud83c\udf0e\u035f\u1f14\u6cb9\u522b\u514b\ud83e\udd21\ud83e\udd25\ud83d\ude2c\ud83e\udd27\u0439\\u2003\ud83d\ude80\ud83e\udd34\u02b2\u0448\u0447\u0418\u041e\u0420\u0424\u0414\u042f\u041c\u044e\u0436\ud83d\ude1d\ud83d\udd91\u1f50\u1f7b\u03cd\u7279\u6b8a\u4f5c\u6226\u7fa4\u0449\ud83d\udca8\u5706\u660e\u56ed\u05e7\u2110\ud83c\udfc8\ud83d\ude3a\ud83c\udf0d\u23cf\u1ec7\ud83c\udf54\ud83d\udc2e\ud83c\udf41\ud83c\udf46\ud83c\udf51\ud83c\udf2e\ud83c\udf2f\ud83e\udd26\\u200d\ud835\udcd2\ud835\udcf2\ud835\udcff\ud835\udcf5\uc548\uc601\ud558\uc138\uc694\u0416\u0459\u041a\u045b\ud83c\udf40\ud83d\ude2b\ud83e\udd24\u1fe6\u6211\u51fa\u751f\u5728\u4e86\u53ef\u4ee5\u8bf4\u666e\u901a\u8bdd\u6c49\u8bed\u597d\u6781\ud83c\udfbc\ud83d\udd7a\ud83c\udf78\ud83e\udd42\ud83d\uddfd\ud83c\udf87\ud83c\udf8a\ud83c\udd98\ud83e\udd20\ud83d\udc69\ud83d\udd92\ud83d\udeaa\u5929\u4e00\u5bb6\u26b2\\u2006\u26ad\u2686\u2b2d\u2b2f\u23d6\u65b0\u2700\u254c\ud83c\uddeb\ud83c\uddf7\ud83c\udde9\ud83c\uddea\ud83c\uddee\ud83c\uddec\ud83c\udde7\ud83d\ude37\ud83c\udde8\ud83c\udde6\u0425\u0428\ud83c\udf10\\x1f\u6740\u9e21\u7ed9\u7334\u770b\u0281\ud835\uddea\ud835\uddf5\ud835\uddf2\ud835\uddfb\ud835\ude06\ud835\uddfc\ud835\ude02\ud835\uddff\ud835\uddee\ud835\uddf9\ud835\uddf6\ud835\ude07\ud835\uddef\ud835\ude01\ud835\uddf0\ud835\ude00\ud835\ude05\ud835\uddfd\ud835\ude04\ud835\uddf1\ud83d\udcfa\u03d6\\u2000\u04af\u057d\u1d26\u13a5\u04bb\u037a\\u2007\u0570\\u2001\u0269\uff59\uff45\u0d66\uff4c\u01bd\uff48\ud835\udc13\ud835\udc21\ud835\udc1e\ud835\udc2b\ud835\udc2e\ud835\udc1d\ud835\udc1a\ud835\udc03\ud835\udc1c\ud835\udc29\ud835\udc2d\ud835\udc22\ud835\udc28\ud835\udc27\u0184\u1d28\u05df\u146f\u0ed0\u03a4\u13e7\u0be6\u0406\u1d11\u0701\ud835\udc2c\ud835\udc30\ud835\udc32\ud835\udc1b\ud835\udc26\ud835\udc2f\ud835\udc11\ud835\udc19\ud835\udc23\ud835\udc07\ud835\udc02\ud835\udc18\ud835\udfce\u051c\u0422\u15de\u0c66\u3014\u13ab\ud835\udc33\ud835\udc14\ud835\udc31\ud835\udfd4\ud835\udfd3\ud835\udc05\ud83d\udc0b\ufb03\ud83d\udc98\ud83d\udc93\u0451\ud835\ude25\ud835\ude2f\ud835\ude36\ud83d\udc90\ud83c\udf0b\ud83c\udf04\ud83c\udf05\ud835\ude6c\ud835\ude56\ud835\ude68\ud835\ude64\ud835\ude63\ud835\ude61\ud835\ude6e\ud835\ude58\ud835\ude60\ud835\ude5a\ud835\ude59\ud835\ude5c\ud835\ude67\ud835\ude65\ud835\ude69\ud835\ude6a\ud835\ude57\ud835\ude5e\ud835\ude5d\ud835\ude5b\ud83d\udc7a\ud83d\udc37\u210b\ud835\udc00\ud835\udc25\ud835\udc2a\ud83d\udeb6\ud835\ude62\u1f39\ud83e\udd18\u0366\ud83d\udcb8\u062c\ud328\ud2f0\uff37\ud835\ude47\u1d7b\ud83d\udc42\ud83d\udc43\u025c\ud83c\udfab\\uf0a7\u0411\u0423\u0456\ud83d\udea2\ud83d\ude82\u0a97\u0ac1\u0a9c\u0ab0\u0abe\u0aa4\u0ac0\u1fc6\ud83c\udfc3\ud835\udcec\ud835\udcfb\ud835\udcf4\ud835\udcee\ud835\udcfd\ud835\udcfc\u2618\ufd3e\u032f\ufd3f\u20bd\\ue807\ud835\udc7b\ud835\udc86\ud835\udc8d\ud835\udc95\ud835\udc89\ud835\udc93\ud835\udc96\ud835\udc82\ud835\udc8f\ud835\udc85\ud835\udc94\ud835\udc8e\ud835\udc97\ud835\udc8a\ud83d\udc7d\ud83d\ude19\\u200c\u041b\u2012\ud83c\udfbe\ud83d\udc79\u238c\ud83c\udfd2\u26f8\u516c\u5bd3\u517b\u5ba0\u7269\u5417\ud83c\udfc4\ud83d\udc00\ud83d\ude91\ud83e\udd37\u64cd\u7f8e\ud835\udc91\ud835\udc9a\ud835\udc90\ud835\udc74\ud83e\udd19\ud83d\udc12\u6b22\u8fce\u6765\u5230\u963f\u62c9\u65af\u05e1\u05e4\ud835\ude6b\ud83d\udc08\ud835\udc8c\ud835\ude4a\ud835\ude6d\ud835\ude46\ud835\ude4b\ud835\ude4d\ud835\ude3c\ud835\ude45\ufdfb\ud83e\udd84\u5de8\u6536\u8d62\u5f97\u767d\u9b3c\u6124\u6012\u8981\u4e70\u989d\u1ebd\ud83d\ude97\ud83d\udc33\ud835\udfcf\ud835\udc1f\ud835\udfd6\ud835\udfd1\ud835\udfd5\ud835\udc84\ud835\udfd7\ud835\udc20\ud835\ude44\ud835\ude43\ud83d\udc47\u951f\u65a4\u62f7\ud835\udde2\ud835\udff3\ud835\udff1\ud835\udfec\u2981\u30de\u30eb\u30cf\u30cb\u30c1\u30ed\u682a\u5f0f\u793e\u26f7\ud55c\uad6d\uc5b4\u3138\u3153\ub2c8\u035c\u0296\ud835\ude3f\ud835\ude54\u20b5\ud835\udca9\u212f\ud835\udcbe\ud835\udcc1\ud835\udcb6\ud835\udcc9\ud835\udcc7\ud835\udcca\ud835\udcc3\ud835\udcc8\ud835\udcc5\u2134\ud835\udcbb\ud835\udcbd\ud835\udcc0\ud835\udccc\ud835\udcb8\ud835\udcce\ud835\ude4f\u03b6\ud835\ude5f\ud835\ude03\ud835\uddfa\ud835\udfee\ud835\udfed\ud835\udfef\ud835\udff2\ud83d\udc4b\ud83e\udd8a\u591a\u4f26\ud83d\udc3d\ud83c\udfbb\ud83c\udfb9\u26d3\ud83c\udff9\ud83c\udf77\ud83e\udd86\u4e3a\u548c\u4e2d\u53cb\u8c0a\u795d\u8d3a\u4e0e\u5176\u60f3\u8c61\u5bf9\u6cd5\u5982\u76f4\u63a5\u95ee\u7528\u81ea\u5df1\u731c\u672c\u4f20\u6559\u58eb\u6ca1\u79ef\u552f\u8ba4\u8bc6\u57fa\u7763\u5f92\u66fe\u7ecf\u8ba9\u76f8\u4fe1\u8036\u7a23\u590d\u6d3b\u6b7b\u602a\u4ed6\u4f46\u5f53\u4eec\u804a\u4e9b\u653f\u6cbb\u9898\u65f6\u5019\u6218\u80dc\u56e0\u5723\u628a\u5168\u5802\u7ed3\u5a5a\u5b69\u6050\u60e7\u4e14\u6817\u8c13\u8fd9\u6837\u8fd8\u267e\ud83c\udfb8\ud83e\udd15\ud83e\udd12\u26d1\ud83c\udf81\u6279\u5224\u68c0\u8ba8\ud83c\udfdd\ud83e\udd81\ud83d\ude4b\ud83d\ude36\uc950\uc2a4\ud0f1\ud2b8\ub93c\ub3c4\uc11d\uc720\uac00\uaca9\uc778\uc0c1\uc774\uacbd\uc81c\ud669\uc744\ub835\uac8c\ub9cc\ub4e4\uc9c0\uc54a\ub85d\uc798\uad00\ub9ac\ud574\uc57c\ud569\ub2e4\uce90\ub098\uc5d0\uc11c\ub300\ub9c8\ucd08\uc640\ud654\uc57d\uae08\uc758\ud488\ub7f0\uc131\ubd84\uac08\ub54c\ub294\ubc18\ub4dc\uc2dc\ud5c8\ub41c\uc0ac\uc6a9\ud83d\udd2b\ud83d\udc41\u51f8\u1f70\ud83d\udcb2\ud83d\uddef\ud835\ude48\u1f0c\ud835\udc87\ud835\udc88\ud835\udc98\ud835\udc83\ud835\udc6c\ud835\udc76\ud835\udd7e\ud835\udd99\ud835\udd97\ud835\udd86\ud835\udd8e\ud835\udd8c\ud835\udd8d\ud835\udd95\ud835\udd8a\ud835\udd94\ud835\udd91\ud835\udd89\ud835\udd93\ud835\udd90\ud835\udd9c\ud835\udd9e\ud835\udd9a\ud835\udd87\ud835\udd7f\ud835\udd98\ud835\udd84\ud835\udd9b\ud835\udd92\ud835\udd8b\ud835\udd82\ud835\udd74\ud835\udd9f\ud835\udd88\ud835\udd78\ud83d\udc51\ud83d\udebf\ud83d\udca1\u77e5\u5f7c\u767e\\uf005\ud835\ude40\ud835\udc9b\ud835\udc72\ud835\udc73\ud835\udc7e\ud835\udc8b\ud835\udfd2\ud83d\ude26\ud835\ude52\ud835\ude3e\ud835\ude3d\ud83c\udfd0\ud835\ude29\ud835\ude28\u1f7c\u1e51\ud835\udc71\ud835\udc79\ud835\udc6b\ud835\udc75\ud835\udc6a\ud83c\uddf0\ud83c\uddf5\ud83d\udc7e\u14c7\u14a7\u152d\u1403\u1427\u1426\u1473\u1428\u14c3\u14c2\u1472\u1438\u146d\u144e\u14c0\u1423\ud83d\udc04\ud83c\udf88\ud83d\udd28\ud83d\udc0e\ud83e\udd1e\ud83d\udc38\ud83d\udc9f\ud83c\udfb0\ud83c\udf1d\ud83d\udef3\u70b9\u51fb\u67e5\u7248\ud83c\udf6d\ud835\udc65\ud835\udc66\ud835\udc67\uff2e\uff27\ud83d\udc63\\uf020\u3063\ud83c\udfc9\u0444\ud83d\udcad\ud83c\udfa5\u039e\ud83d\udc34\ud83d\udc68\ud83e\udd33\ud83e\udd8d\\x0b\ud83c\udf69\ud835\udc6f\ud835\udc92\ud83d\ude17\ud835\udfd0\ud83c\udfc2\ud83d\udc73\ud83c\udf57\ud83d\udd49\ud83d\udc32\u0686\u06cc\ud835\udc6e\ud835\uddd5\ud835\uddf4\ud83c\udf52\ua725\u2ca3\u2c8f\ud83d\udc11\u23f0\u9244\u30ea\u4e8b\u4ef6\u0457\ud83d\udc8a\u300c\u300d\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600\u71fb\u88fd\u30b7\u865a\u507d\u5c41\u7406\u5c48\u0413\ud835\udc69\ud835\udc70\ud835\udc80\ud835\udc7a\ud83c\udf24\ud835\uddf3\ud835\udddc\ud835\uddd9\ud835\udde6\ud835\udde7\ud83c\udf4a\u1f7a\u1f08\u1f21\u03c7\u1fd6\u039b\u290f\ud83c\uddf3\ud835\udc99\u03c8\u0541\u0574\u0565\u057c\u0561\u0575\u056b\u0576\u0580\u0582\u0564\u0571\u51ac\u81f3\u1f40\ud835\udc81\ud83d\udd39\ud83e\udd1a\ud83c\udf4e\ud835\udc77\ud83d\udc02\ud83d\udc85\ud835\ude2c\ud835\ude31\ud835\ude38\ud835\ude37\ud835\ude10\ud835\ude2d\ud835\ude13\ud835\ude16\ud835\ude39\ud835\ude32\ud835\ude2b\u06a9\u0392\u03ce\ud83d\udca2\u039c\u039f\u039d\u0391\u0395\ud83c\uddf1\u2672\ud835\udf48\u21b4\ud83d\udc92\u2298\u023b\ud83d\udeb4\ud83d\udd95\ud83d\udda4\ud83e\udd58\ud83d\udccd\ud83d\udc48\u2795\ud83d\udeab\ud83c\udfa8\ud83c\udf11\ud83d\udc3b\ud835\udc0e\ud835\udc0d\ud835\udc0a\ud835\udc6d\ud83e\udd16\ud83c\udf8e\ud83d\ude3c\ud83d\udd77\uff47\uff52\uff4e\uff54\uff49\uff44\uff55\uff46\uff42\uff4b\ud835\udff0\ud83c\uddf4\ud83c\udded\ud83c\uddfb\ud83c\uddf2\ud835\uddde\ud835\udded\ud835\uddd8\ud835\udde4\ud83d\udc7c\ud83d\udcc9\ud83c\udf5f\ud83c\udf66\ud83c\udf08\ud83d\udd2d\u300a\ud83d\udc0a\ud83d\udc0d\\uf10a\u10da\u06a1\ud83d\udc26\\U0001f92f\\U0001f92a\ud83d\udc21\ud83d\udcb3\u1f31\ud83d\ude47\ud835\uddf8\ud835\udddf\ud835\udde0\ud835\uddf7\ud83e\udd5c\u3055\u3088\u3046\u306a\u3089\ud83d\udd3c'\n\nisolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\nremove_dict = {ord(c):f'' for c in symbols_to_delete}\n\nREPLACE_BY_SPACE_RE = re.compile('[\/{}\\[\\]\\|@,;.]')\nBAD_SYMBOLS_RE = re.compile('[^0-9a-zA-z #+_&]')\nHTML_TAGS = re.compile('<[^>]*>')\nSTOPWORDS = set(nltk.corpus.stopwords.words('english'))\n\ndef text_cleaning(text: str) -> str:\n    \"\"\"\n    Args:\n        text (str): a string\n    Returns:\n        modified initial string\n    \"\"\"\n    for k, v in CONTRACTION_MAPPING.items():\n        text = text.replace(' %s ' % k, ' %s ' % v)\n    #text = text.lower() # lowercase text\n    text = text.translate(remove_dict)\n    text = text.translate(isolate_dict)\n    text = HTML_TAGS.sub(' ', text)\n    # text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n    # text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n    # text = re.sub(r'\\W+', '', text)\n    # text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n    return text","63540759":"# Pandas multiprocessing\ndef _apply_df(args):\n    df, func, kwargs = args\n    return df.apply(func, **kwargs)\n\ndef apply_by_multiprocessing(df, func, **kwargs):\n    workers = kwargs.pop('workers')\n    pool = multiprocessing.Pool(processes=workers)\n    result = pool.map(_apply_df, [(d, func, kwargs)\n            for d in np.array_split(df, workers)])\n    pool.close()\n    return pd.concat(list(result))","b3bdce96":"df_train, df_val = train_test_split(train_df, test_size=0.15, random_state=RANDOM_SEED)\ndf_test = test_df.copy()","cf273696":"df_train.shape, df_val.shape, df_test.shape","09b63932":"def augment_text(df, model_path='distilbert-base-uncased', samples=1000, pr_list=[0.2], device='cuda'):\n    if not isinstance(pr_list, list):\n        pr_list = [pr_list]\n    \n    new_text=[]\n    labels = []\n    for pr in pr_list:\n        aug = naw.ContextualWordEmbsAug(model_path=model_path, aug_p=pr, device=device)\n        for i in np.random.randint(0,len(df), samples):\n            text = df.iloc[i][TEXT_COLUMN]\n            labels.append(df.iloc[i][LABEL_COLUMN])\n            augmented_text = aug.augment(text)\n            new_text.append(augmented_text)\n    new = pd.DataFrame({TEXT_COLUMN: new_text, LABEL_COLUMN: labels})\n    \n    ## dataframe\n    df = df.append(new).reset_index(drop=True)\n    return df\n   \ndf_train = augment_text(df_train, samples=int(df_train.shape[0] * 0.08), pr_list=[0.1, 0.2, 0.3])","7e4a3ff1":"df_train[TEXT_COLUMN] = apply_by_multiprocessing(df_train[TEXT_COLUMN], text_cleaning, workers=4)\ndf_val[TEXT_COLUMN] = apply_by_multiprocessing(df_val[TEXT_COLUMN], text_cleaning, workers=4)\ndf_test[TEXT_COLUMN] = apply_by_multiprocessing(df_test[TEXT_COLUMN], text_cleaning, workers=4)","c1525046":"class TextClassificationDataset(Dataset):\n    \"\"\"\n    Wrapper around Torch Dataset to perform text classification\n    \"\"\"\n\n    def __init__(\n        self,\n        texts: List[str],\n        labels: List[str] = None,\n        label_dict: Mapping[str, int] = None,\n        max_seq_length: int = 512,\n        model_name: str = \"bert-base-uncased\",\n    ):\n        \"\"\"\n        Args:\n            text_dir (str): a list with texts to classify or to train the\n                classifier on\n            labels (List[str]): a list with classification labels (optional)\n            label_dict (dict): a dictionary mapping class names to class ids,\n                to be passed to the validation data (optional)\n            max_seq_length (int): maximal sequence length in tokens,\n                texts will be stripped to this length\n            model_name (str): transformer model name, needed to perform\n                appropriate tokenization\n        \"\"\"\n\n        self.texts = texts\n        self.labels = labels\n        self.label_dict = label_dict\n        self.max_seq_length = max_seq_length\n\n        if self.label_dict is None and labels is not None:\n            # {'class1': 0, 'class2': 1, 'class3': 2, ...}\n            # using this instead of `sklearn.preprocessing.LabelEncoder`\n            # no easily handle unknown target values\n            self.label_dict = dict(\n                zip(sorted(set(labels)), range(len(set(labels))))\n            )\n\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n        # suppresses tokenizer warnings\n        logging.getLogger(\"transformers.tokenization_utils\").setLevel(\n            logging.FATAL\n        )\n\n    def __len__(self) -> int:\n        \"\"\"\n        Returns:\n            int: length of the dataset\n        \"\"\"\n        return len(self.texts)\n\n    def __getitem__(self, index: int) -> Mapping[str, torch.Tensor]:\n        \"\"\"Gets element of the dataset\n        Args:\n            index (int): index of the element in the dataset\n        Returns:\n            Single element by index\n        \"\"\"\n\n        # encoding the text\n        x = self.texts[index]\n\n        # a dictionary with `input_ids` and `attention_mask` as keys\n        output_dict = self.tokenizer.encode_plus(\n            x,\n            add_special_tokens=True,\n            padding=\"max_length\",\n            max_length=self.max_seq_length,\n            return_tensors=\"pt\",\n            truncation=True,\n            return_attention_mask=True,\n            return_token_type_ids=False,\n        )\n\n        output_dict[\"input_ids\"] = output_dict[\"input_ids\"].flatten()\n        output_dict[\"attention_mask\"] = output_dict[\"attention_mask\"].flatten()\n\n        # encoding target\n        if self.labels is not None:\n            y = self.labels[index]\n            y_encoded = (\n                torch.Tensor([self.label_dict.get(y, -1)]).long().squeeze(0)\n            )\n            output_dict[\"targets\"] = y_encoded\n\n        # output_dict['text'] = x\n\n        return output_dict\n","b22e7938":"MODEL_NAME = 'textattack\/xlnet-base-cased-imdb' #'roberta-base'","ea5b6c66":"print('Loading tokenizer...')\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)","c2cc5c19":"tokenizer","a6fc40e2":"# Record the length of each sequence (after truncating to 512).\nlengths = []\n\nprint('Tokenizing comments...')\n\n# For every sentence...\nfor sen in df_train[TEXT_COLUMN]:\n    \n    # `encode` will:\n    #   (1) Tokenize the sentence.\n    #   (2) Prepend the `[CLS]` token to the start.\n    #   (3) Append the `[SEP]` token to the end.\n    #   (4) Map tokens to their IDs.\n    encoded_sent = tokenizer.encode(\n                        sen,                      # Sentence to encode.\n                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                        #max_length = 512,          # Truncate all sentences.                        \n                        #return_tensors = 'pt',     # Return pytorch tensors.\n                   )\n\n    # Record the truncated length.\n    lengths.append(len(encoded_sent))\n\nprint('DONE.')","0189f050":"print('   Min length: {:,} tokens'.format(min(lengths)))\nprint('   Max length: {:,} tokens'.format(max(lengths)))\nprint('Median length: {:,} tokens'.format(np.median(lengths)))","e730bf1f":"# Truncate any comment lengths greater than 512.\nlengths = [min(l, 512) for l in lengths]\n\n# Plot the distribution of comment lengths.\nsns.distplot(lengths, kde=False, rug=False)\n\n# Alternatively, you might try using a log scale on the x-axis, but this is \n# tricky. See here for one approach:\n# https:\/\/stackoverflow.com\/questions\/47850202\/plotting-a-histogram-on-a-log-scale-with-matplotlib?rq=1\n#plt.xscale('log')\n\nplt.title('Text Lengths')\nplt.xlabel('Text Length')\nplt.ylabel('# of Text')\n","1fff7a72":"# Count the number of sentences that had to be truncated to 512 tokens.\nnum_truncated = lengths.count(512)\n\n# Compare this to the total number of training sentences.\nnum_sentences = len(lengths)\nprcnt = float(num_truncated) \/ float(num_sentences)\n\nprint('{:,} of {:,} sentences ({:.1%}) in the training set are longer than 512 tokens.'.format(num_truncated, num_sentences, prcnt))","cb4789cf":"num_pos = 0\nnum_neg = 0\n\n# Iterate through the comment lengths...\nfor i, l in enumerate(lengths):\n    \n    # If the sentence was truncated...\n    if l == 512:\n\n        if train_df[LABEL_COLUMN].values[i] == 1:\n            num_pos += 1\n        else:\n            num_neg += 1\n\n# Report the total.\nprint('{:,} ({:.1%}) of the truncated examples have class 1.'.format(num_pos, num_pos \/ (num_neg + num_pos)))","1ece0210":"MAX_SEQ_LEN = 512\nBATCH_SIZE = 8","117ee3f1":"full_train_dataset = TextClassificationDataset(\n    texts=train_df[TEXT_COLUMN].values.tolist(),\n    labels=train_df[LABEL_COLUMN].values,\n    max_seq_length=MAX_SEQ_LEN,\n    model_name=MODEL_NAME,\n)\n\ntrain_dataset = TextClassificationDataset(\n    texts=df_train[TEXT_COLUMN].values.tolist(),\n    labels=df_train[LABEL_COLUMN].values,\n    max_seq_length=MAX_SEQ_LEN,\n    model_name=MODEL_NAME,\n)\n\nvalid_dataset = TextClassificationDataset(\n    texts=df_val[TEXT_COLUMN].values.tolist(),\n    labels=df_val[LABEL_COLUMN].values,\n    max_seq_length=MAX_SEQ_LEN,\n    model_name=MODEL_NAME,\n)\n\ntest_dataset = TextClassificationDataset(\n    texts=df_test[TEXT_COLUMN].values.tolist(),\n    max_seq_length=MAX_SEQ_LEN,\n    model_name=MODEL_NAME,\n)","0aca4480":"full_train_data_loader = DataLoader(\n    dataset=full_train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n)\n\ntrain_data_loader = DataLoader(\n    dataset=train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n)\n\nval_data_loader = DataLoader(\n    dataset=valid_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n)\n\ntest_data_loader = DataLoader(\n    dataset=test_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n)","950b39f6":"mini_batch_data = next(iter(train_data_loader))\nmini_batch_data.keys()","9e45a950":"print(mini_batch_data['input_ids'].shape)\nprint(mini_batch_data['attention_mask'].shape)\nprint(mini_batch_data['targets'].shape)","149ed5e0":"class BertForSequenceClassification(nn.Module):\n    \"\"\"\n    Simplified version of the same class by HuggingFace.\n    See transformers\/modeling_distilbert.py in the transformers repository.\n    \"\"\"\n\n    def __init__(\n        self,\n        pretrained_model_name: str,\n        num_classes: int = None,\n        dropout: float = 0.3,\n    ):\n        \"\"\"\n        Args:\n            pretrained_model_name (str): HuggingFace model name.\n                See transformers\/modeling_auto.py\n            num_classes (int): the number of class labels\n                in the classification task\n            dropout (float): dropout rate for hidden layers\n        \"\"\"\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(\n            pretrained_model_name, num_labels=num_classes\n        )\n        \n        self.model = AutoModel.from_pretrained(pretrained_model_name, config=config)\n\n        self.dropout1 = nn.Dropout(dropout)\n        self.l1 = nn.Linear(config.hidden_size*6, config.hidden_size)\n        self.bn1 = torch.nn.LayerNorm(config.hidden_size)\n        self.dropout2 = nn.Dropout(dropout)\n        self.l2 = torch.nn.Linear(config.hidden_size, num_classes)\n        \n    def forward(self, input_ids, attention_mask=None, head_mask=None, text=None):\n        \"\"\"Compute class probabilities for the input sequence.\n\n        Args:\n            features (torch.Tensor): ids of each token,\n                size ([bs, seq_length]\n            attention_mask (torch.Tensor): binary tensor, used to select\n                tokens which are used to compute attention scores\n                in the self-attention heads, size [bs, seq_length]\n            head_mask (torch.Tensor): 1.0 in head_mask indicates that\n                we keep the head, size: [num_heads]\n                or [num_hidden_layers x num_heads]\n        Returns:\n            PyTorch Tensor with predicted class scores\n        \"\"\"\n        assert attention_mask is not None, \"attention mask is none\"\n\n        # taking BERTModel output\n        # see https:\/\/huggingface.co\/transformers\/model_doc\/bert.html#transformers.BertModel\n        bert_output = self.model(\n            input_ids=input_ids, attention_mask=attention_mask, head_mask=head_mask,\n            output_hidden_states=True, return_dict=True,\n        )\n\n        # detoxify_output = self.detoxify_model(\n        #     input_ids=input_ids, attention_mask=attention_mask, head_mask=head_mask,\n        #     output_hidden_states=True, return_dict=True\n        # )\n        # we only need the hidden state here and don't need\n        # transformer output, so index 0\n        seq_output1 = bert_output['hidden_states'][-1]  # (bs, seq_len, dim)\n        seq_output2 = bert_output['hidden_states'][-4]\n        seq_output3 = bert_output['hidden_states'][-7]\n        # seq_output4 = bert_output['hidden_states'][-10]\n        # seq_output2 = detoxify_output['hidden_states'][-1] # (bs, seq_len, dim)\n        \n        # mean pooling, i.e. getting average representation of all tokens\n        pooled_output1 = seq_output1.mean(axis=1)  # (bs, dim)\n        pooled_output2 = seq_output2.mean(axis=1)  # (bs, dim)\n        pooled_output3 = seq_output3.mean(axis=1)  # (bs, dim)\n        pooled_output4, _ = seq_output1.max(dim=1)  # (bs, dim)\n        pooled_output5, _ = seq_output2.max(dim=1)  # (bs, dim)\n        pooled_output6, _ = seq_output3.max(dim=1)  # (bs, dim)\n        # pooled_output1 = bert_output['pooler_output']\n        # pooled_output2 = seq_output2.mean(axis=1)  # (bs, dim)\n\n        # and concat it\n        pooled_output = torch.cat([pooled_output1, pooled_output2, pooled_output3, pooled_output4, pooled_output5, pooled_output6], dim=1)  # (bs, 6*dim)\n        pooled_output = self.dropout1(pooled_output)  # (bs, dim)\n        pooled_output = self.l1(pooled_output)  # (bs, dim)\n        pooled_output = self.bn1(pooled_output)  # (bs, dim)\n        pooled_output = nn.Tanh()(pooled_output)\n        pooled_output = self.dropout2(pooled_output)  # (bs, dim)\n        pooled_output = self.l2(pooled_output)  # (bs, num_classes)\n\n        # probs = self.sigmoid(scores)\n\n        return pooled_output\n\n    def freeze(self):\n        for param in self.model.parameters():\n            param.requires_grad = False\n\n    def unfreeze(self):\n        for param in self.model.parameters():\n            param.requires_grad = True","978b0419":"# Number of training epochs (authors recommend between 2 and 4)\nN_EPOCH_PRETRAIN = 5\nN_EPOCH_TRAIN = 8","6628fcce":"model = BertForSequenceClassification(\n    pretrained_model_name=MODEL_NAME,\n    num_classes=train_df[LABEL_COLUMN].unique().shape[0],\n    dropout=0.4\n).to(device)","b648af56":"no_decay = ['bias', 'LayerNorm.weight', 'gamma', 'beta']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n]","66997f1a":"optimizer_train = AdamW(optimizer_grouped_parameters, lr=1e-5)","78ee14c5":"model.freeze()\noptimizer_pretrain = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\nmodel.unfreeze()","525e86c3":"list(filter(lambda p: p.requires_grad, model.parameters())).__len__()","a4745e36":"steps_per_epoch = len(df_train) \/\/ BATCH_SIZE\nwarmup_step = 0\ntotal_steps_pretrain = steps_per_epoch * (N_EPOCH_PRETRAIN + 1)\ntotal_steps_train = steps_per_epoch * (N_EPOCH_TRAIN + 1)\n\nscheduler_pretrain = get_linear_schedule_with_warmup(optimizer_pretrain,\n                                                     warmup_step,\n                                                     total_steps_pretrain)\n\nscheduler_train = get_linear_schedule_with_warmup(optimizer_train,\n                                                  warmup_step,\n                                                  total_steps_train)\n\nloss_fn = nn.CrossEntropyLoss().to(device)","782bb8ac":"# prediction = model(input_ids=mini_batch_data['input_ids'].to(device),\n#                    attention_mask=mini_batch_data['attention_mask'].to(device)\n# )","a83fde99":"# print(prediction.size())\n# prediction[:3]","714a53bc":"def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n    model = model.train()\n    \n    losses = []\n    correct_predictions = 0\n\n    for d in data_loader:\n        optimizer.zero_grad()\n        input_ids = d[\"input_ids\"].to(device)\n        attention_mask = d[\"attention_mask\"].to(device)\n        targets = d[\"targets\"].to(device)\n        \n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask)\n        \n        _, preds = torch.max(outputs, dim=1)\n        loss = loss_fn(outputs, targets)\n        \n        correct_predictions += torch.sum(preds == targets)\n        losses.append(loss.item())\n        \n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n        \n    return correct_predictions.double() \/ n_examples, np.mean(losses)","58b01141":"def eval_model(model, data_loader, loss_fn, device, n_examples):\n    model = model.eval()\n    \n    losses = []\n    correct_predictions = 0\n\n    with torch.no_grad():\n        for d in data_loader:\n            input_ids = d[\"input_ids\"].to(device)\n            attention_mask = d[\"attention_mask\"].to(device)\n            targets = d[\"targets\"].to(device)\n            \n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask)\n            \n            _, preds = torch.max(outputs, dim=1)\n            loss = loss_fn(outputs, targets)\n            \n            correct_predictions += torch.sum(preds == targets)\n            losses.append(loss.item())\n\n    return correct_predictions.double() \/ n_examples, np.mean(losses)","8ef28ab2":"from collections import defaultdict\nfrom time import time","c5bf842e":"from google.colab import drive\ndrive.mount('\/content\/gdrive', force_remount=True)","ff4da8e9":"SUFIX = MODEL_NAME.split('\/')[-1] + '_' + str(MAX_SEQ_LEN)\nSUFIX","5b0fdc25":"LAST_MODEL_STATE_NAME = 'last_model_state_' + SUFIX + '.bin'\nBEST_MODEL_STATE_NAME = 'best_model_state_' + SUFIX + '.bin'\nSUBMISSION_NAME = 'submission_' + SUFIX + '.csv'","e0710198":"BEST_MODEL_STATE_NAME, LAST_MODEL_STATE_NAME","d9c0641d":"history = defaultdict(list)\nbest_accuracy = 0\n\n# Pretrain linear layers, do not train bert\nmodel.freeze()\n\nfor epoch in range(N_EPOCH_PRETRAIN):\n    print(f'Epoch {epoch + 1}\/{N_EPOCH_PRETRAIN}')\n    print('-' * 10)\n    t1 = time()\n    train_acc, train_loss = train_epoch(\n        model,\n        train_data_loader,    \n        loss_fn, \n        optimizer_pretrain, \n        device, \n        scheduler_pretrain, \n        len(df_train)\n    )\n\n    print(f'Train loss {train_loss} accuracy {train_acc}')\n\n    val_acc, val_loss = eval_model(\n        model,\n        val_data_loader,\n        loss_fn, \n        device, \n        len(df_val)\n    )\n\n    print(f'Val   loss {val_loss} accuracy {val_acc}')\n    print()\n\n    history['train_acc'].append(train_acc)\n    history['train_loss'].append(train_loss)\n    history['val_acc'].append(val_acc)\n    history['val_loss'].append(val_loss)\n\n    model.unfreeze()\n    if val_acc > best_accuracy:\n        torch.save(model.state_dict(), BEST_MODEL_STATE_NAME)\n        best_accuracy = val_acc\n\n    torch.save(model.state_dict(), LAST_MODEL_STATE_NAME)\n    model.freeze()\n    copyfile(LAST_MODEL_STATE_NAME, '\/content\/gdrive\/MyDrive\/' + LAST_MODEL_STATE_NAME)\n\n    print(f'time = {round((time() - t1) \/ 60, 2)}m')\n\ncopyfile(BEST_MODEL_STATE_NAME, '\/content\/gdrive\/MyDrive\/' + BEST_MODEL_STATE_NAME)\nmodel.unfreeze()","9b490172":"plt.plot(history['train_acc'], label='train accuracy')\nplt.plot(history['val_acc'], label='validation accuracy')\n\nplt.title('Training history')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\nplt.ylim([0, 1]);","62512d7f":"model.load_state_dict(torch.load(BEST_MODEL_STATE_NAME))\nmodel = model.to(device)\nbest_accuracy","646bb5d4":"history = defaultdict(list)\n\nfor epoch in range(N_EPOCH_TRAIN):\n    print(f'Epoch {epoch + 1}\/{N_EPOCH_TRAIN}')\n    print('-' * 10)\n    t1 = time()\n    train_acc, train_loss = train_epoch(\n        model,\n        train_data_loader,    \n        loss_fn, \n        optimizer_train, \n        device, \n        scheduler_train, \n        len(df_train)\n    )\n\n    print(f'Train loss {train_loss} accuracy {train_acc}')\n\n    val_acc, val_loss = eval_model(\n        model,\n        val_data_loader,\n        loss_fn, \n        device, \n        len(df_val)\n    )\n\n    print(f'Val   loss {val_loss} accuracy {val_acc}')\n    print()\n\n    history['train_acc'].append(train_acc)\n    history['train_loss'].append(train_loss)\n    history['val_acc'].append(val_acc)\n    history['val_loss'].append(val_loss)\n\n    if val_acc > best_accuracy:\n        torch.save(model.state_dict(), BEST_MODEL_STATE_NAME)\n        best_accuracy = val_acc\n    \n    torch.save(model.state_dict(), LAST_MODEL_STATE_NAME)\n    copyfile(LAST_MODEL_STATE_NAME, '\/content\/gdrive\/MyDrive\/' + LAST_MODEL_STATE_NAME)\n\n    print(f'time = {round((time() - t1) \/ 60, 2)}m')\ncopyfile(BEST_MODEL_STATE_NAME, '\/content\/gdrive\/MyDrive\/' + BEST_MODEL_STATE_NAME)","47e41042":"plt.plot(history['train_acc'], label='train accuracy')\nplt.plot(history['val_acc'], label='validation accuracy')\n\nplt.title('Training history')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\nplt.ylim([0, 1]);","67a89ee0":"model.load_state_dict(torch.load('\/content\/gdrive\/MyDrive\/' + BEST_MODEL_STATE_NAME))\nmodel = model.to(device)","d2b6630a":"def get_predictions(model, data_loader,):\n    model = model.eval()\n\n    predictions = []\n    prediction_probs = []\n    real_values = []\n\n    with torch.no_grad():\n        for d in data_loader:\n\n            input_ids = d[\"input_ids\"].to(device)\n            attention_mask = d[\"attention_mask\"].to(device)\n            targets = d[\"targets\"].to(device)\n\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask\n            )\n            _, preds = torch.max(outputs, dim=1)\n\n            probs = F.softmax(outputs, dim=1)\n\n            predictions.extend(preds)\n            prediction_probs.extend(probs)\n            real_values.extend(targets)\n\n    predictions = torch.stack(predictions).cpu()\n    prediction_probs = torch.stack(prediction_probs).cpu()\n    real_values = torch.stack(real_values).cpu()\n    return predictions, prediction_probs, real_values","4ce97b5d":"from sklearn.metrics import confusion_matrix, classification_report","518b89c7":"y_pred_val, y_pred_probs_val, y_val = get_predictions(\n    model,\n    val_data_loader,\n)","8b32e274":"print(classification_report(y_val, y_pred_val))","39a7ee5b":"def show_confusion_matrix(confusion_matrix):\n    hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n    hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n    hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n    plt.ylabel('True sentiment')\n    plt.xlabel('Predicted sentiment');\n\ncm = confusion_matrix(y_val, y_pred_val)\ndf_cm = pd.DataFrame(cm, index=[0,1], columns=[0,1])\nshow_confusion_matrix(df_cm)","a140199f":"def get_test_predictions(model, data_loader,):\n    model = model.eval()\n\n    predictions = []\n    prediction_probs = []\n\n    with torch.no_grad():\n        for d in data_loader:\n\n            input_ids = d[\"input_ids\"].to(device)\n            attention_mask = d[\"attention_mask\"].to(device)\n\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask\n            )\n            _, preds = torch.max(outputs, dim=1)\n\n            probs = F.softmax(outputs, dim=1)\n\n            predictions.extend(preds)\n            prediction_probs.extend(probs)\n\n    predictions = torch.stack(predictions).cpu()\n    prediction_probs = torch.stack(prediction_probs).cpu()\n    return predictions, prediction_probs","e7f4bc07":"y_pred, y_pred_probs = get_test_predictions(\n    model,\n    test_data_loader,\n)","0a5038c3":"pd.DataFrame({'id': range(len(y_pred)), 'sentiment': y_pred}).to_csv(SUBMISSION_NAME, index=False)","6f43b529":"# !cp submission.csv \/content\/gdrive\/MyDrive\/","6c9f8aa9":"copyfile(SUBMISSION_NAME, '\/content\/gdrive\/MyDrive\/' + SUBMISSION_NAME)","107f93ae":"from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve","fbd97d7c":"def Find_Optimal_Cutoff(target, predicted):\n    \"\"\" Find the optimal probability cutoff point for a classification model related to event rate\n    Parameters\n    ----------\n    target : Matrix with dependent or target data, where rows are observations\n\n    predicted : Matrix with predicted data, where rows are observations\n\n    Returns\n    -------     \n    list type, with optimal cutoff value\n        \n    \"\"\"\n    fpr, tpr, threshold = roc_curve(target, predicted)\n    i = np.arange(len(tpr)) \n    roc = pd.DataFrame({'tf' : pd.Series(tpr-(1-fpr), index=i), 'threshold' : pd.Series(threshold, index=i)})\n    roc_t = roc.iloc[(roc.tf-0).abs().argsort()[:1]]\n\n    return list(roc_t['threshold']) ","2d52e79a":"threshold = Find_Optimal_Cutoff(y_val, y_pred_probs_val[:,1])\nthreshold","6047d365":"pd.DataFrame({'id': range(len(y_pred)), 'sentiment': (y_pred_probs[:, 1] > threshold[0]).int()}).to_csv('roc-auc-' + SUBMISSION_NAME, index=False)","e217627c":"y_test.shape","3e86f9d3":"precision, recall, thresholds = precision_recall_curve(y_val, y_pred_probs_val[:,1])","5592a64e":"f1_scores = 2*recall*precision\/(recall+precision)\nprint('Best threshold: ', thresholds[np.argmax(f1_scores)])\nprint('Best F1-Score: ', np.max(f1_scores))","015426a1":"pd.DataFrame({'id': range(len(y_pred)), 'sentiment': (y_pred_probs[:, 1] > thresholds[np.argmax(f1_scores)]).int()}).to_csv('f1-' + SUBMISSION_NAME, index=False)","984b9221":"### 4.2. Optimizer & Learning Rate Scheduler","b18d60d2":"How many truncated comments have class 1 ?","3ad3960d":"### 2.2. Parse & Inspect\n-------------------\n","43e1ee30":"Now that we have our model loaded we need to grab the training hyperparameters from within the stored model.","abfb48af":"### 4.1. BertForSequenceClassification","73dbdb71":"Now, we are going to clean our texts (remove bad symbols, spaces and stopwords)","a87b8f27":"### Train our model","bf9b6177":"### 3.2. Comment Length Distribution","4441f307":"# Part I - Setup & Dataset Prep\n-------------------------------","bb4f7f88":"### 2.1. Download\n--------------------------------------\nDownload the text.","752cb08f":"Let's write another one that helps us evaluate the model on a given data loader:","302f28ca":"Now, let's add simple augmentation to our train dataset","95698c1c":"Note that increasing the batch size reduces the training time significantly, but gives you lower accuracy.\n\n","2c74c325":"Training the model should look familiar, except for two things. The scheduler gets called every time a batch is fed to the model. We're avoiding exploding gradients by clipping the gradients of the model using [clip_gradnorm](https:\/\/pytorch.org\/docs\/stable\/nn.html#clip-grad-norm).","ad672960":"Let's have a look at the classification report","7e9d60b9":"Let's take a look at the first few rows of the table just to see what's in there.","7e9db93c":"### 1.2. Installing the Hugging Face Library\n","a0f50f93":"## 1. Setup\n","d1a1e3c7":"First of all, you have to upload your kaggle credential. Just check [this link](https:\/\/www.kaggle.com\/docs\/api) if you don't know what is it.","2f58e8a0":"### 3.1. Examples of 512 Tokens","c0efa5f1":"### 4.3. Training Loop","63a47d6e":"Using those two, we can write our training loop. We'll also store the training history:","1db1722e":"# Prediction","7de384da":"The structure of the current notebook is similar to awesome [Chris McCormick tutorial](http:\/\/mccormickml.com\/2019\/07\/22\/BERT-fine-tuning\/)","af122e9f":"Next, we should divide our dataset on train, validation and test","f71f78ed":"### 1.1. Using Colab GPU for Training\n","a4735693":"Let's define class of our Dataset","6a60bbf2":"Let's take a look at the distribution of our labels","c7cb9cb8":"### Pretrain our model","6312a565":"Let's grab some quick statistics--what are the min, max and median comment lenghts?","3450d6a3":"Let's look at an example comment which gets split into more than 512 tokens, so that we can get a sense for how much text fits within this limit.\n\nFirst we'll need to load the DistilBERT tokenizer.","baaa8027":"## 4. Train Our Classification Model","fcc0b288":"## 3. BERT Input Length Limitation","34d0c7e5":"We'll continue with the confusion matrix:","2dc2b17a":"To decide on a truncation strategy for this dataset, let's first look at the distribution of comment lenghts.\n\nTo do this, our first step is to tokenize all of the comments in the training set.","3e9beb02":"**Tokenize All Comments**\n\nThe `tokenizer.encode` function combines multiple steps for us:\n1. Split the sentence into tokens.\n2. Add the special `[CLS]` and `[SEP]` tokens.\n3. Map the tokens to their IDs.\n\nIn order to explore the distribution of comment lengths, we will not perform any truncation here. Unfortunately, this results in the tokenizer spitting out a warning for every comment that's longer than 512 tokens. We'll just have to ignore those for now!","7b5488d3":"Clearly most comments are \"short\", and there's a long tail of longer comments. \n\nJust how many of the examples run into the 512-token limit?","d684c887":"## 2. Retrieve & Inspect Dataset\n\n","6de70e85":"# WelcomePackOnly Team INT20H\n","f1d709c1":"# Part II - BERT Fine-Tuning","a2b963c9":"Now, let's create our DataLoaders:","1b3d9b9d":"Let's import some modules for visualization, building our dataset and building model for classification","98298c0d":"Let's try to improve our result using other thresholds:","c75474fa":"## Evaluation","d1cb49b6":"We'll define a helper function to get the predictions from our model:","c54fbe44":"Now that our input data is properly formatted, it's time to fine tune the BERT model. ","e0baae75":"Let's have a look at an example batch from our training data loader:","b9985db4":"To further analyze it, let's plot the distribution. To keep the scale of the x-axis reasonable, *we'll clip the lengths to 512.*\n","10180c29":"Let's continue with writing a helper function for training our model for one epoch:","ff5d743b":"\nNext, let's install the [transformers](https:\/\/github.com\/huggingface\/transformers) package from Hugging Face which will give us a pytorch interface for working with BERT. "}}