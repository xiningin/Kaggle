{"cell_type":{"ad0c9c18":"code","a04098fc":"code","c6fb78eb":"code","38982aab":"code","26688518":"code","0502b1d0":"code","c39cc911":"code","d3db0441":"code","fe52089b":"code","6d0a8086":"code","dded8e84":"code","ac40ee11":"code","53cf110a":"code","9ad0be8d":"code","e2c99bf0":"code","7b349699":"code","70512271":"code","ecf640af":"code","d28bb6e1":"code","d6d7ce63":"code","515992f5":"code","3fa30d6e":"code","9c455d40":"code","0561d7e4":"code","33d9260b":"code","f3685d5f":"code","26abe8e6":"code","326c1a4c":"code","37f87344":"code","d943a7e7":"code","844f5097":"code","87f0ad3a":"code","97027094":"code","8c74718c":"code","7886efbd":"code","73df705d":"code","5af35adb":"code","d2bbd40c":"code","70300911":"code","01f53e39":"code","a1cdab89":"code","8ab7a2c7":"code","ee6c4111":"code","6509b7bb":"code","dcd84440":"code","083195ec":"code","3253772c":"code","69d0324a":"code","45823a1e":"code","0420e3f3":"code","fe80fe19":"code","af3c8f73":"code","d708ee04":"code","419dbcf9":"code","fb28db38":"code","127130f0":"code","e970c3fa":"code","66342c1b":"code","5f00ece5":"code","b2b0683a":"code","4f96c560":"code","083998cc":"code","f5f5f5b9":"code","763371be":"code","e8034643":"code","eaa29020":"code","b9861643":"markdown","a2903bc1":"markdown","a999c327":"markdown","a3946684":"markdown","27f672ad":"markdown","b5e00400":"markdown","5547e5e5":"markdown","ac88c176":"markdown","2baa5eb6":"markdown","b10a8b80":"markdown","3bc8ba9d":"markdown","6e3c36a1":"markdown","5fd66285":"markdown","a63cd899":"markdown","20da3542":"markdown","a297246b":"markdown","a67e176f":"markdown","2165c42c":"markdown","5d78db7f":"markdown","a7b1bbbd":"markdown","1b7d5856":"markdown","1411872a":"markdown","711c8aa8":"markdown","066071b0":"markdown","096779d0":"markdown","dcaab3fa":"markdown","9803992d":"markdown","d0b12cd7":"markdown","57aefa19":"markdown"},"source":{"ad0c9c18":"import numpy as np\nimport pandas as pd \nimport os\nimport random\nfrom sklearn.utils import shuffle\n\n#for text cleaning\nimport string\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom collections import Counter\n\n# clustering\nfrom gensim.models import word2vec, KeyedVectors\nfrom sklearn.cluster import KMeans\nfrom sklearn.neighbors import KDTree\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport spacy\n\nimport re;\nimport logging;\nimport sqlite3;\nimport time;\nimport sys;\nimport multiprocessing;\nfrom wordcloud import WordCloud, ImageColorGenerator, STOPWORDS\nimport matplotlib.pyplot as plt;\nfrom itertools import cycle;\n\nfrom tqdm import tqdm\ntqdm.pandas()\n","a04098fc":"SEED = 2021","c6fb78eb":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n#Importing the dataset \ndata = pd.read_csv(\"..\/input\/covidvaccine-tweets\/covidvaccine.csv\")","38982aab":"data.head()","26688518":"random.seed(SEED)\ndata.iloc[random.randint(0, len(data))]","0502b1d0":"data.shape #183494 entries","c39cc911":"def clean_text(txt):\n    '''\n    cleans the input text in the following steps:\n    1 - replace contractions\n    2 - removing punctuation\n    3 - spliting into words\n    4 - removing stopwords\n    5 - removing leftover punctuations\n    6 - lower-case everything\n    '''\n    contraction_dict = {   \n        \"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \n        \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\",\n        \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \n        \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \n        \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \n        \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\n        \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \n        \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \n        \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n        \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \n        \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \n        \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n        \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n        \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \n        \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \n        \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \n        \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n        \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \n        \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \n        \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \n        \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \n        \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n        \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \n        \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n        \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n        \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n        \"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n        \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n    def _get_contractions(contraction_dict):\n        contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n        return contraction_dict, contraction_re\n\n    def replace_contractions(text):\n        contractions, contractions_re = _get_contractions(contraction_dict)\n        def replace(match):\n            return contractions[match.group(0)]\n        return contractions_re.sub(replace, text)\n\n    # replace contractions\n    txt = replace_contractions(txt)\n    \n    #remove punctuations\n    txt  = \"\".join([char for char in txt if char not in string.punctuation])\n    txt = re.sub('[0-9]+', '', txt)\n    \n    # split into words\n    words = word_tokenize(txt)\n    \n    # remove stopwords\n    stop_words = set(stopwords.words('english'))\n    words = [w for w in words if not w in stop_words]\n    \n    # removing leftover punctuations\n    words = [word for word in words if word.isalpha()]\n    \n    # lower-case everything\n    words = [w.lower() for w in words]\n    \n    # stem the words\n    porter = PorterStemmer()\n    words = [porter.stem(w) for w in words]\n    \n    \n    cleaned_text = ' '.join(words)\n    return cleaned_text","d3db0441":"data = data[['text', 'hashtags']].fillna('')\ndata.head()\n\ndata['raw_tweet'] = data['text'] + ' ' + data['hashtags']\n\ndata.head()","fe52089b":"data['tweet'] = data['raw_tweet'].progress_apply(lambda txt: clean_text(txt))\ndata.head()","6d0a8086":"data.sort_values(by=['tweet']).head()\n# still a bunch of empty lists in the dataset. We will have to remove them before clustering.","dded8e84":"#remove the empty strings\ndf = data[data['tweet'].astype(bool)]\ndf.sort_values(by=['tweet']).head()","ac40ee11":"# I only want to keep hashtags and tweet\ndf = df[['hashtags', 'tweet']]\ndf.sort_values(by=['tweet']).head()","53cf110a":"df['tweet_split'] = df.tweet.str.split()\ndf.head()","9ad0be8d":"# find out the longest tweet\nmax_len = df.tweet.str.len().max()\nprint(max_len)","e2c99bf0":"from sklearn.feature_extraction.text import HashingVectorizer\n# list of text documents\ntext = [\"The quick brown fox jumped over the lazy dog.\"]\n# create the transform\nvectorizer = HashingVectorizer(n_features=20)\n# encode document\nvector = vectorizer.transform(text)\n# summarize encoded vector\nprint(vector.shape)\nprint(vector.toarray())","7b349699":"vectorizer = HashingVectorizer (n_features = max_len)\ndf['vector'] = df['tweet'].progress_apply(lambda t: vectorizer.fit_transform([t]).toarray())\ndf.head()","70512271":"X = np.concatenate(df['vector'].values)","ecf640af":"kmeans = KMeans(n_clusters = 4)\ndf['cluster'] = kmeans.fit_predict(X)","d28bb6e1":"pca = PCA(n_components=2)\npca_result = pca.fit_transform(X)\ndf['x'] = pca_result[:, 0]\ndf['y'] = pca_result[:, 1]\ndf.head()","d6d7ce63":"cluster_colors = pd.np.array(['#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#bcf60c', '#fabebe', '#008080', '#e6beff', '#9a6324', '#fffac8', '#800000', '#aaffc3', '#808000', '#ffd8b1', '#000075', '#808080', '#ffffff', '#000000'])\ndf['color'] = cluster_colors[df.cluster.values]\ndf['text'] = df.tweet.str[:50]","515992f5":"import bokeh.io\nfrom bokeh.io import push_notebook, show, output_notebook\nfrom bokeh.plotting import figure\nfrom bokeh.models import ColumnDataSource, LabelSet\n\n# from bokeh.charts import Donut, HeatMap, Histogram, Line, Scatter, show, output_notebook, output_file\nbokeh.io.output_notebook()","3fa30d6e":"#visualize the data using bokeh\n#output_file(\"top_artists.html\", title=\"top artists\")\n# TOOLS = \"pan,wheel_zoom,box_zoom,reset,hover,previewsave\"\n\nsource = ColumnDataSource.from_df(df[['x', 'y', 'color', 'text']])\nTOOLTIPS = [(\"text\", \"@text\")]\nTOOLS = \"pan,wheel_zoom,box_zoom,reset,hover,save\"\n\nplot = figure(plot_width=800, plot_height=450, tooltips=TOOLTIPS, tools=TOOLS)\n\n#draw circles\nplot.circle(y='y', x='x', source=source, size=15, fill_color='color')\nshow(plot)","9c455d40":"df.head()","0561d7e4":"df.query('cluster == 4').tweet","33d9260b":"cluster0 = df.query('cluster == 0').tweet\ncluster1 = df.query('cluster == 1').tweet\ncluster2 = df.query('cluster == 2').tweet\ncluster3 = df.query('cluster == 3').tweet\n\n\ntext0 = ' '.join(tweet for tweet in cluster0)\ntext1 = ' '.join(tweet for tweet in cluster1)\ntext2 = ' '.join(tweet for tweet in cluster2)\ntext3 = ' '.join(tweet for tweet in cluster3)\n\nprint(f'There are {len(text0)} words in the combination of all cells in column \"tweet\" labeled as cluster 0.')\nprint(f'There are {len(text1)} words in the combination of all cells in column \"tweet\" labeled as cluster 1.')\nprint(f'There are {len(text2)} words in the combination of all cells in column \"tweet\" labeled as cluster 2.')\nprint(f'There are {len(text3)} words in the combination of all cells in column \"tweet\" labeled as cluster 3.')","f3685d5f":"# create stopwords\nstopwords = set(STOPWORDS)\nstopwords.update(['amp', 'covid', 'covidvaccin', 'vaccin', 'today', 'peopl', 'coronaviru', 'say', 'day', 'one']) \n# I want to see what's left after filtered out the common keywords.","26abe8e6":"wordcloud0 = WordCloud(stopwords=stopwords, background_color='white').generate(text0)\nwordcloud1 = WordCloud(stopwords=stopwords, background_color='white').generate(text1)\nwordcloud2 = WordCloud(stopwords=stopwords, background_color='white').generate(text2)\nwordcloud3 = WordCloud(stopwords=stopwords, background_color='white').generate(text3)\n\nwordcloud = [wordcloud0, wordcloud1, wordcloud2, wordcloud3]","326c1a4c":"# cluster 0\nplt.figure(figsize=(10,5))\nplt.tight_layout(pad=0)\nplt.imshow(wordcloud0, interpolation='bilinear')\nplt.title('Cluster 0 Key words', size=16)\nplt.show()","37f87344":"# cluster 1\nplt.figure(figsize=(10,5))\nplt.tight_layout(pad=0)\nplt.imshow(wordcloud1, interpolation='bilinear')\nplt.title('Cluster 1 Key words', size=16)\nplt.show()","d943a7e7":"# cluster 2\nplt.figure(figsize=(10,5))\nplt.tight_layout(pad=0)\nplt.imshow(wordcloud2, interpolation='bilinear')\nplt.title('Cluster 2 Key words', size=16)\nplt.show()","844f5097":"# cluster 3\nplt.figure(figsize=(10,5))\nplt.tight_layout(pad=0)\nplt.imshow(wordcloud3, interpolation='bilinear')\nplt.title('Cluster 3 Key words', size=16)\nplt.show()","87f0ad3a":"from textblob import TextBlob\n\ndef get_subjectivity(text):\n    return TextBlob(text).sentiment.subjectivity\n\ndef get_polarity(text):\n    return TextBlob(text).sentiment.polarity\n\ndef get_sentiment(score):\n    if score > 0:\n        return 'Positive'\n    elif score == 0:\n        return 'Neutral'\n    else:\n        return 'Negative'","97027094":"df['subjectivity'] = df['text'].apply(get_subjectivity)\ndf['polarity'] = df['text'].apply(get_polarity)\ndf['sentiment'] = df['polarity'].apply(get_sentiment)\ndf.head(20)","8c74718c":"# View sentiment counts\n\ndf.sentiment.value_counts()","7886efbd":"fig = plt.figure(figsize=(12,6))\nplt.scatter(df['polarity'], df['subjectivity'], s=4)\n\nplt.ylabel('Subjectivity')\nplt.xlabel('Polarity')","73df705d":"textblob_output = df.loc[:,['hashtags', 'tweet','subjectivity', 'polarity', 'sentiment']].sort_values('sentiment')\ntextblob_output.head()","5af35adb":"textblob_output.to_csv('textblog.csv',index=False)","d2bbd40c":"#df=pd.read_csv(\"..\/input\/covidvaccine-tweets\/covidvaccine.csv\")\ndf = pd.read_csv(\"..\/input\/covidvaccine-tweets\/covidvaccine.csv\")\ndf.head()","70300911":"df.info()","01f53e39":"df.is_retweet.value_counts()","a1cdab89":"#We create a pandas dataframe as follows:\ndata = pd.DataFrame(data=df.text)\ndata = data.rename(columns={'text' : 'Tweets'})\ndata.head()","8ab7a2c7":"# We display the first 10 elements of the dataframe:\npd.set_option('max_colwidth',170)\ndisplay(data.head(10))","ee6c4111":"docs=df.text.head(1000).values\ntype(docs)","6509b7bb":"docs_clean = []\nfor doc in docs:\n    doc_2 = re.sub(r':.*$', \":\", doc)\n    docs_clean.append(doc_2)\n\ndocs_clean[:20]","dcd84440":"docs2=docs_clean","083195ec":"# remove punctuations\npunctuationChars = '!@#$%^&*(){}{}|;:\",.\/<>?' # you might choose different charcters to drop\nfor i in punctuationChars:\n    docs2 = np.char.replace(docs2, i, ' ')\n# remove apostrophe's (single quotes)\ndocs2 = np.char.replace(docs2,\"'\",' ')\n# remove line feeds\ndocs2 = np.char.replace(docs2,\"\\n\",' ')\n# remove 'http:'\ndocs2 = np.char.replace(docs2,\"https:\",' ')\ndocs2 = np.char.replace(docs2,\"https\",' ')\n\n# make lower case\nfor i,s in enumerate(docs2):\n    docs2[i] = s.lower()\n    \n# Show the cleaned data\n# Show the beginning of each document\n\n#for i in range(len(docs2)):\n#        print(f'\\ndoc{i}: {docs2[i]}') \ndocs2[:10]","3253772c":"def spacy_tokenizer(document):\n    tokens = nlp(document)\n    tokens = [token for token in tokens if (\n        token.is_stop == False and \\\n        token.is_punct == False and \\\n        token.lemma_.strip()!= '')]\n    tokens = [token.lemma_ for token in tokens]\n    return tokens","69d0324a":"# test data to see what spacy tokenizer can do.\nexample_corpus = [\n    \"Monsters are bad. They likes to eat geese. I saw one goose flying away\", \\\n    \"I saw a monster yesterday. The meaning is so obvious!\", \\\n    \"Why are we talking about bad monsters? They are meanness.\"]","45823a1e":"nlp = spacy.load(\"en_core_web_sm\")\n\ntfidf_vector = TfidfVectorizer(input = 'content', tokenizer = spacy_tokenizer)\n# test\ncorpus=example_corpus\n# fit: learns vocabulary and idf\n# transform: transforms documents into document-term matrix\nresult_test = tfidf_vector.fit_transform(corpus)\nresult_test","0420e3f3":"dense = result_test.todense()\ndenselist = dense.tolist()\ndf_test = pd.DataFrame(\n    denselist,columns=tfidf_vector.get_feature_names())\ndf_test","fe80fe19":"tfidf_vector = TfidfVectorizer(input = 'content', tokenizer = spacy_tokenizer)\ncorpus = docs2\n\n# fit: learns vocabulary and idf\n# transform: transforms documents into document-term matrix\nresult = tfidf_vector.fit_transform(corpus)\nresult","af3c8f73":"# We can check which terms are actually considered from the sentences with the get_feature_names method:\ntfidf_vector.get_feature_names()[1:20]","d708ee04":"dense = result.todense()\ndenselist = dense.tolist()\ndf = pd.DataFrame(\n    denselist,columns=tfidf_vector.get_feature_names())\ndf","419dbcf9":"df[[\"australia\", \"manufacture\", \"covid-19\"]]","fb28db38":"from sklearn.metrics.pairwise import linear_kernel\ncos_df = pd.DataFrame(columns=df.index)\nfor i in range(999):\n    curr_cos_sim = linear_kernel(result[i:i+1], result).flatten()\n    cos_df[i] = curr_cos_sim\n    \ncos_df","127130f0":"kmeans_models = {}\nfor i in range(2,13+1):\n    current_kmean = KMeans(n_clusters=i).fit(result)\n    kmeans_models[i] = current_kmean","e970c3fa":"cluster_df = pd.DataFrame()\ncluster_df['Review Texts'] = docs\nfor i in range(2, 13+1):\n    col_name = str(i) +'means_label'\n    cluster_df[col_name] = kmeans_models[i].labels_\ncluster_df","66342c1b":"Sum_of_squared_distances = []\nK = range(1,18)\nfor k in K:\n    km = KMeans(n_clusters=k)\n    km = km.fit(result)\n    Sum_of_squared_distances.append(km.inertia_)","5f00ece5":"plt.figure(figsize=(16,8))\nplt.plot(K, Sum_of_squared_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow Method For Optimal k')\nplt.show()","b2b0683a":"cluster10 = cluster_df.iloc[:,[0,9]]\ncluster10_0 = cluster10.loc[cluster10[\"10means_label\"] == 0]\ncluster10_0.head(50)","4f96c560":"cluster10_1 = cluster10.loc[cluster10[\"10means_label\"] == 1]\ncluster10_1.head(50)","083998cc":"cluster10_2 = cluster10.loc[cluster10[\"10means_label\"] == 2]\ncluster10_2","f5f5f5b9":"cluster10_3 = cluster10.loc[cluster10[\"10means_label\"] == 3]\ncluster10_3.head(50)","763371be":"cluster10_4 = cluster10.loc[cluster10[\"10means_label\"] == 4]\ncluster10_4","e8034643":"cluster10_5 = cluster10.loc[cluster10[\"10means_label\"] == 5]\ncluster10_5","eaa29020":"cluster10 = cluster10.sort_values(by='10means_label')\ncluster10.to_csv('cluster10.csv',index=False)","b9861643":"### Cluster 3 contains more rumors and negative reactions","a2903bc1":"# Import Libraries and Load Data into Memory","a999c327":"### Upload Output 2: 10 clusters Tweets","a3946684":"# Clustering using HushingVectorizer in sklearn\n\nThis part is reference by Vadim Nareyko's Kaggle Notebook: Google word2vec, KMeans, PCA.\nHere is the link to his notebook. https:\/\/www.kaggle.com\/nareyko\/google-word2vec-kmeans-pca\nThank you.","27f672ad":"### Data Cleaning","b5e00400":"**Steps:**\n- Import Libraries and Load Data into Memory\n- K-Means Clustering Using HashingVectorizer\n    - Clustering using Scikit-Learn's HashingVectorizer and KMeans modules\n    - PCA dimension reduction to 2D in order to visualize the data points\n    - Making WordClouds to show the clusters\n        - Thanks to Vadim Nareyko's Kaggle Notebook: Google word2vec, KMeans, PCA https:\/\/www.kaggle.com\/nareyko\/google-word2vec-kmeans-pca Got inspired by his post\n- TextBlob Clustering\n    - Output 1: Clustering and TextBlob output\n- Finding the best K for K-means Using SpaCy, TF-IDF, and Elbow Method\n    - Output 2: Clustering 10 Out","5547e5e5":"# Using TextBlob for Sentiment Analysis","ac88c176":"### Data Preprocessing","2baa5eb6":"## Visualizae Polarity and Subjectivity of Tweets","b10a8b80":"### Successfully extraxt intended meaning of the words. 14 tokens for the example corpus.","3bc8ba9d":"### Cluster_2 focus on topics related to Russia Vaccine","6e3c36a1":"### Upload Output 1: TextBlob Sentiments","5fd66285":"### Elbow Method to determine the best K","a63cd899":"It\u2019s a sparse matrix with 1000 reviews and 3191 terms, out of those 3191000 possible numbers there are 9169 non-zero TF-IDF values. We can check which terms are actually considered from the sentences with the get_feature_names method:","20da3542":"The sparse matrix format is an efficient way to store this information, but you might want to convert it to a more readable, dense matrix format using the todense method. To create a pandas DataFrame from the results, you can use the following code:","a297246b":"### Apply Spacy tokenizer, TF-IDF, K-means for the first 100 tweets.","a67e176f":"See a random data entry:","2165c42c":"### Define SpaCy Tokenizer","5d78db7f":"### Create the clustering table","a7b1bbbd":"Take a look at the Data Frame:","1b7d5856":"### Import DataFrame into Memory","1411872a":"### Choose K=10 to experiment","711c8aa8":"### Let's see the weights for words contained in the first tweet:","066071b0":"### Read the Data","096779d0":"### Check cosine similarity","dcaab3fa":"### WordClouds","9803992d":"clean_df = clean_df.str.split()\nclean_df.head()","d0b12cd7":"# TF-IDF Vectorizer and Finding the Best K with Elbow Method","57aefa19":"### PCA for visualization"}}