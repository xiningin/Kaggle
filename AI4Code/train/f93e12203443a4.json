{"cell_type":{"8bd7f164":"code","b88bf1c0":"code","b13304b9":"code","d581b09c":"code","ab99afaa":"code","21a32ddb":"code","d1ef5e13":"code","52f131bf":"code","bb0a7ba1":"code","45b5c4af":"code","ff5aaef4":"code","38a1ac46":"code","3a31c2f4":"code","2b702427":"code","9a7bb444":"code","a838a08c":"code","33629853":"code","00e69016":"code","ff12a98d":"code","411a7d4a":"code","f6651584":"code","f6208106":"code","5b54e8c5":"markdown","2b9175e3":"markdown","42d66ba3":"markdown","36029111":"markdown","21f87f33":"markdown","ec9ce43f":"markdown","96bda348":"markdown","85ad8999":"markdown","7035fc6e":"markdown","e7b221a9":"markdown","0c0a7e1f":"markdown","91506500":"markdown","e7b41e01":"markdown","e14422ee":"markdown","b4269eed":"markdown","c7b50218":"markdown","a3d94d73":"markdown","60bf7b77":"markdown","c961095b":"markdown"},"source":{"8bd7f164":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import precision_recall_fscore_support","b88bf1c0":"train=pd.read_csv('..\/input\/preprocessed-disaster-dataset\/train.csv') \ntest=pd.read_csv('..\/input\/preprocessed-disaster-dataset\/test.csv')\nfeatures=train.columns[0:-1]\n\nScore=[]","b13304b9":"from sklearn.model_selection import KFold\nincremental_data_X=[]\n\n#split seprately the class 0 and 1 and combined then+\nkf_0 = KFold(n_splits=3,random_state=42, shuffle=True) \nkf_1 = KFold(n_splits=3,random_state=42, shuffle=True)\nirrelevant=train[train['target']==0].reset_index(drop=True)\nrelevant=train[train['target']==1].reset_index(drop=True)\n\nfor (_, val_index1),(_,val_index2) in zip(kf_0.split(irrelevant),kf_1.split(relevant)):\n    incremental_data_X.append(pd.concat([irrelevant.loc[val_index1],relevant.loc[val_index2]],axis=0)) ","d581b09c":"\ndef get_tree_detail(clf,index_features):\n    #get the tree details\n    n_nodes = clf.tree_.node_count\n    children_left = clf.tree_.children_left #left child node\n    children_right = clf.tree_.children_right #right child node\n    feature = clf.tree_.feature #feature index\n    threshold = clf.tree_.threshold #threshold\n    node_value=clf.tree_.value #node value\n    node_depth = np.zeros(shape=n_nodes, dtype=np.int64)\n    is_leaves = np.zeros(shape=n_nodes, dtype=bool)\n    stack = [(0, 0)]\n    Dict={}\n    while len(stack) > 0:\n        node_id, depth = stack.pop()\n        node_depth[node_id] = depth\n        # node\n        is_split_node = children_left[node_id] != children_right[node_id]\n        # If a split node, append left and right children and depth to `stack`\n        # so we can loop through them\n        if is_split_node:\n            stack.append((children_left[node_id], depth + 1))\n            stack.append((children_right[node_id], depth + 1))\n        else:\n            is_leaves[node_id] = True\n    j=0\n    for i in range(n_nodes):\n        if is_leaves[i]:\n            Dict[i]={'name':None,'depth':node_depth[i],'left':None,'right':None,'thresh':None,'value':node_value[i],'leaves_node':True}\n        else: \n            Dict[i]={'name':features[index_features[i]],'depth':node_depth[i],'left':children_left[i],'right':children_right[i],'thresh':float(threshold[i]),'value':node_value[i],'leaves_node':False}\n            j+=1\n    return Dict\n\ndef get(Dict,i):\n    temp=Dict[i]\n    if(temp['left'] is not None):\n        Id=temp['left']\n        Dict[i]['left']=get(Dict,Id)\n            \n    if(temp['right'] is not None):\n        Id=temp['right']\n        Dict[i]['right']=get(Dict,Id)\n        \n    return Dict[i]\n\n      ","ab99afaa":"from copy import  deepcopy\n\ngenerated_data=pd.DataFrame()\nn=100\ndef generator(Dict,feature_detail,Index):\n    #print()\n    if(Dict['name'] is None):#leave node\n        if('target' not in generated_data.columns):\n            generated_data['target']=np.NaN\n        generated_data['target'].loc[Index]=np.argmax(Dict['value'])#get dominat class\n    else:\n        detail=feature_detail[Dict['name']]\n        if(detail['dtype'] is 'category'):\n            cat=True\n            #generate data in the given category\n            category=detail['values']\n            #n should be size of data frame  see for sample weight in order to replicate real data\n        else:\n            cat=False\n            #continous type\n            Min=detail['min'] #store the min and max for non categorical\n            Max=detail['max']\n\n        if(Index is None):\n            if(cat):\n                values=np.random.choice(category,n) #get random sample form the uniform distrubution\n            else:\n                values=np.random.uniform(Min,Max,n) # get the random value \n            generated_data[Dict['name']]=values\n            Index=generated_data.index\n\n        else:\n            if(cat):\n\n                values=np.random.choice(category,len(Index))\n            else:\n                #generate data between min and max\n                #Futher: Use some of the distrubution\n                values=np.random.uniform(Min,Max,len(Index))\n\n\n            try:\n                generated_data[Dict['name']].loc[Index]=values\n\n            except:\n                generated_data[Dict['name']]=np.NaN\n                generated_data[Dict['name']].loc[Index]=values\n        \n        #go to left\n        temp=generated_data.loc[Index].copy()\n\n        left_index=temp.loc[temp[Dict['name']]<=Dict['thresh']].index\n        left_feature_detail=deepcopy(feature_detail)\n        if(cat):\n            category=np.array(category)\n            left_feature_detail[Dict['name']]['values']=category[category<=Dict['thresh']]\n        else:\n            left_feature_detail[Dict['name']]['max']=Dict['thresh']\n\n        generator(Dict['left'],left_feature_detail,left_index)\n\n        #go to right\n        right_index=temp.loc[temp[Dict['name']]>Dict['thresh']].index\n        right_feature_detail=deepcopy(feature_detail)\n\n        if(cat):\n            category=np.array(category)\n            right_feature_detail[Dict['name']]['values']=category[category>Dict['thresh']]\n        else:\n            right_feature_detail[Dict['name']]['min']=Dict['thresh']+0.02\n\n\n        generator(Dict['right'],right_feature_detail,right_index)\n\n\n#finally see for duplicates or look what to do\n#suppose parent node is intialized and passed to function","21a32ddb":"#get feature detail.\n#along some statistical details.\n\nrows=train.shape[0]\nfeature_detail={}\nfor col in features:\n    unq_elem=np.unique(train[col])\n    tmp=len(unq_elem)\n    if(tmp<rows\/\/2 and 'int' in str(train[col].dtype)):\n        #categorical\n        feature_detail[col]={'dtype':'category','values':unq_elem}\n    else:\n        #continous\n        feature_detail[col]={'dtype':train[col].dtype,'max':train[col].max(),'min':train[col].min(),'mean':train[col].mean()}","d1ef5e13":"def get_misclassify_data(pred,actual,X):\n    index=np.where(actual!=pred)[0]\n    data=X.iloc[index].copy()\n    data['target']=actual[index]\n    return data","52f131bf":"clf = DecisionTreeClassifier(max_depth=6, random_state=0)\nX=incremental_data_X[0].copy().reset_index(drop=True)\ny=X['target']\nX.drop(columns=['target'],inplace=True)\n\nclf.fit(X,y)\ndata_missclassified=get_misclassify_data(clf.predict(X),y,X)\n#fimnd\nb_AUC=round( roc_auc_score(y, clf.predict(X)),3)\nprint(\"AUC of Batch 1 : \",b_AUC)\ntrain_AUC=round( roc_auc_score(train['target'].values, clf.predict(train.iloc[:,0:-1])),3)\nprint(\"AUC of train_data : \",train_AUC)\ntest_AUC=round( roc_auc_score(test['target'], clf.predict(test.iloc[:,0:-1])),3)\nprint(\"AUC of test_data : \",test_AUC)\n\nScore.append(['batch-1',b_AUC,train_AUC,test_AUC])\n\n\n#clf.feature_importances_ ##note this","bb0a7ba1":"precision_recall_fscore_support(train['target'].values, clf.predict(train.iloc[:,0:-1]),average='micro')","45b5c4af":"from sklearn import tree\ntext=tree.plot_tree(clf)\nindex_features=[]\nfor i in range(len(text)):\n    temp=text[i].get_text().split('<=')[0][2:-2]\n    if(temp.isdigit()):\n        index_features.append(int(temp))\n    else:\n        index_features.append(None)","ff5aaef4":"#get the tree detail and store in Dictionary\nDict=get_tree_detail(clf,index_features)\n_==get(Dict,0)\nDict=Dict[0]","38a1ac46":"from sklearn.metrics import confusion_matrix\nn=100 # get 100 sample form the classifier\ngenerated_data=pd.DataFrame()\ngenerator(Dict,feature_detail,Index=None)\n\ncolum_drop=generated_data.columns[np.where(n==generated_data.isna().sum().values)[0]]\ngenerated_data.drop(columns=colum_drop,inplace=True)\n\nfor col in features:\n    if(col not in generated_data.columns ):\n        if(feature_detail[col]['dtype'] is 'category'):\n            category=feature_detail[col]['values']\n            generated_data[col]=np.random.choice(category,n)\n        else:\n            Min,Max=feature_detail[col]['min'],feature_detail[col]['max']\n            generated_data[col]=np.random.uniform(Min,Max,n)\n\ntarget=generated_data['target']\ngenerated_data=generated_data[features]\ngenerated_data.fillna(value=generated_data.mean(),inplace=True)\nprint('AUC',clf.score(generated_data,target)) \ngenerated_data=generated_data.fillna(value=generated_data.mean())\n\nprint('Confusion Matrix')\nconfusion_matrix(target, clf.predict(generated_data))","3a31c2f4":"generated_data['target']=target\ngenerated_data","2b702427":"clf = DecisionTreeClassifier(max_depth=6, random_state=0)\nX=incremental_data_X[1].copy()\ny=X['target'].values\nX.drop(columns=['target'],inplace=True)\n\ny_gen=generated_data['target'].values\ny_mis_clf=data_missclassified.iloc[:,-1]\ngenerated_data.drop(columns=['target'],inplace=True)\ny=np.hstack([y,y_gen,y_mis_clf])\nupdated_data=pd.concat([X,generated_data,data_missclassified.iloc[:,0:-1]],axis=0).reset_index(drop=True)\n\nclf.fit(updated_data,y)\ndata_missclassified=get_misclassify_data(clf.predict(X),y,X)\n\nb_AUC=round( roc_auc_score(y, clf.predict(updated_data)),3)\nprint(\"AUC of Batch 2 : \",b_AUC)\ntrain_AUC=round( roc_auc_score(train['target'].values, clf.predict(train.iloc[:,0:-1])),3)\nprint(\"AUC of train_data : \",train_AUC)\ntest_AUC=round( roc_auc_score(test['target'], clf.predict(test.iloc[:,0:-1])),3)\nprint(\"AUC of test_data : \",test_AUC)\n\nScore.append(['batch-2',b_AUC,train_AUC,test_AUC])","9a7bb444":"precision_recall_fscore_support(train['target'].values, clf.predict(train.iloc[:,0:-1]),average='micro')","a838a08c":"from sklearn import tree\ntext=tree.plot_tree(clf)\nindex_features=[]\nfor i in range(len(text)):\n    temp=text[i].get_text().split('<=')[0][2:-2]\n    if(temp.isdigit()):\n        index_features.append(int(temp))\n    else:\n        index_features.append(None)","33629853":"Dict=get_tree_detail(clf,index_features)\n_==get(Dict,0)\nDict=Dict[0]","00e69016":"from sklearn.metrics import confusion_matrix\nn=100\ngenerated_data=pd.DataFrame()\ngenerator(Dict,feature_detail,Index=None)\n\ncolum_drop=generated_data.columns[np.where(n==generated_data.isna().sum().values)[0]]\ngenerated_data.drop(columns=colum_drop,inplace=True)\n\nfor col in features:\n    if(col not in generated_data.columns ):\n        if(feature_detail[col]['dtype'] is 'category'):\n            category=feature_detail[col]['values']\n            generated_data[col]=np.random.choice(category,n)\n        else:\n            Min,Max=feature_detail[col]['min'],feature_detail[col]['max']\n            generated_data[col]=np.random.uniform(Min,Max,n)\n\ntarget=generated_data['target']\ngenerated_data=generated_data[features]\ngenerated_data.fillna(value=generated_data.mean(),inplace=True)\nprint('AUC',clf.score(generated_data,target)) \ngenerated_data=generated_data.fillna(value=generated_data.mean())\n\nconfusion_matrix(target, clf.predict(generated_data))","ff12a98d":"generated_data['target']=target\n\nclf = DecisionTreeClassifier(max_depth=6, random_state=0)\nX=incremental_data_X[2].copy()\ny=X['target'].values\nX.drop(columns=['target'],inplace=True)\n\ny_gen=generated_data['target'].values\ny_mis_clf=data_missclassified.iloc[:,-1]\ngenerated_data.drop(columns=['target'],inplace=True)\ny=np.hstack([y,y_gen,y_mis_clf])\nupdated_data=pd.concat([X,generated_data,data_missclassified.iloc[:,0:-1]],axis=0).reset_index(drop=True)\nclf.fit(updated_data,y)\nclf.score(updated_data,y)\n\n\n\nb_AUC=round( roc_auc_score(y, clf.predict(updated_data)),3)\nprint(\"AUC of Batch 3 : \",b_AUC)\ntrain_AUC=round( roc_auc_score(train['target'].values, clf.predict(train.iloc[:,0:-1])),3)\nprint(\"AUC of train_data : \",train_AUC)\ntest_AUC=round( roc_auc_score(test['target'], clf.predict(test.iloc[:,0:-1])),3)\nprint(\"AUC of test_data : \",test_AUC)\n\nScore.append(['batch-3',b_AUC,train_AUC,test_AUC])","411a7d4a":"precision_recall_fscore_support(train['target'].values, clf.predict(train.iloc[:,0:-1]),average='micro')","f6651584":"y_train=train['target'].values\nX_train=train.drop(columns=['target']).copy()\n#test\ny_test=test['target'].values\nX_test=test.drop(columns=['target']).copy()\n\nclf = DecisionTreeClassifier(max_depth=6,random_state=0)\nclf.fit(X_train,y_train)\nprint(\"AUC of Train:\",roc_auc_score(y_train,clf.predict(X_train)))\nprint(\"AUC of Test:\",roc_auc_score(y_test,clf.predict(X_test)))","f6208106":"Score=pd.DataFrame(Score,columns=['batch_no','Batch_AUC','train_AUC','test_AUC'])\nScore","5b54e8c5":"**Most of research Paper focused only classification** <br>\n**But this method can be applicable for both regression and classification Problems**","2b9175e3":"# Phase I","42d66ba3":"* The training AUC is improving afer every batch which mean classifier is updating it's knowledge on train-data\n* The training AUC in Batch_3 is 0.819 and when the entire data is trained AUC of training data  is 0.82,\n* For the test_data AUC in Batch_3 is 0.806 annd for entire_data AUC is 0.804\n  AUC are nearly same. ","36029111":"# Generated Data,\n* set n=100 to get 100 data points","21f87f33":"# Import dataset and divide in 3 group\n* all the data is preprocesed and Normalized, focused only to show the Incremental Learning","ec9ce43f":"# Phase II\n\n* After Data Generation \n* Get the New data\n* combine both data and update the model","96bda348":"<!-- ### Work to deal with NaN values -->","85ad8999":"![regression-tree_g8zxq5.webp](attachment:regression-tree_g8zxq5.webp)","7035fc6e":"## Create 3 groups for simulation as if they are generated after period of time\n","e7b221a9":"# AUC for whole dataset","0c0a7e1f":"### Sample Diagram for Explaination","91506500":"\n\n## Decision Tree Data Generating\n* Get the Skeleton of Decision Tree\n* Store all the nodes,leaves and their Thresholds.\n* Get all possible combination of data using node and leaves, such that generated data depicts the strucutre of Decision Tree\n* Use AUC(Area Under Cover), to verify the quality of data.\n<br>\n\n<div>\n<img src=\"https:\/\/miro.medium.com\/max\/1174\/1*7wRAwQjvOGXNUx753F_-Vg.png\" height=\"200\" width=\"500\">\n<\/div>","e7b41e01":"## Data generation","e14422ee":"<!-- <div>\n<img src=\"https:\/\/ibb.co\/jD0p2sD\" height=\"200\" width=\"500\">\n<\/div> -->\n![generated data.PNG](attachment:fde710e3-f9e1-444b-a983-7bb8b3e2d6d4.PNG)\n##### If Again new data comes , Repeat the Second part of above image for updating model parameter","b4269eed":"**AUC-1 means generated data depicts the decision tree and able to preserve the skeleton of decision tree.**","c7b50218":"# Incremental Learning\n\nIncremental learning is a machine learning paradigm where the learning process takes place whenever new data is added with the time and to trained for whole data that means old data + new data which will cause too much time and resource. Instead by some adjustments in model we can only train for new data and  the overall Performance is nearly same. <br>\n\nTo Demonstrate the Algorithm, the task Used **Irrelevant classification of social media post**\n","a3d94d73":"* randomly generate data points on every region or box (above image)\n* store the value of generated data and corresponding class value","60bf7b77":"## get all the tree detail\n* left and right child\n* threshold\n* depth\n* leave node value","c961095b":"## PHASE III"}}