{"cell_type":{"9167bf0c":"code","4d685fe5":"code","e281dd99":"code","d528e5bd":"code","40328719":"code","64f42ded":"code","979d37a5":"code","bedace72":"code","20a89eac":"code","6f4bf789":"code","c3b1d663":"code","8cfd24a1":"code","c378a07f":"code","13fdab6f":"code","7970e5f8":"code","cb7ca05c":"code","f41badb6":"code","3688bc24":"code","63ca90ec":"code","859aa388":"code","7a7d9a53":"code","1579717b":"code","7b5a317c":"code","be01b0ba":"code","786bd6fe":"code","0f2fbad2":"code","c21aa990":"code","a728f79d":"code","a9637db8":"code","92702ec0":"code","4a3661fc":"code","32f0f8ff":"code","44e4b6bc":"code","0be9ae8b":"code","91254315":"code","45924703":"markdown","1104371d":"markdown","eee41b0e":"markdown","681b3cdd":"markdown","5f8bcd1f":"markdown","d73d629d":"markdown","df2dfeb0":"markdown","5a6637ae":"markdown","6ecd77b3":"markdown","d04429e8":"markdown","46288ced":"markdown","2dd4bec8":"markdown","2ae35501":"markdown","a09d3e2a":"markdown","1fb06133":"markdown","cb1a9a2f":"markdown","eff9eb15":"markdown"},"source":{"9167bf0c":"import pandas as pd\nimport re \nfrom matplotlib import pyplot\nimport seaborn as sns\nimport numpy as np\nimport os\nprint(os.listdir(\"..\/input\"))\n","4d685fe5":"notclean = pd.read_csv('..\/input\/bitcoin-tweets-14m\/cleanprep.csv', delimiter=',', error_bad_lines=False,engine = 'python',header = None)\nnotclean.columns =['dt', 'name','text','polarity','sensitivity']\nnotclean =notclean.drop(['name','text'], axis=1)\nnotclean['dt'] = pd.to_datetime(notclean['dt'])\nnotclean['DateTime'] = notclean['dt'].dt.floor('h')\nvdf = notclean.groupby(pd.Grouper(key='dt',freq='H')).size().reset_index(name='tweet_vol')\nvdf.index = pd.to_datetime(vdf.index)\nvdf=vdf.set_index('dt')\nnotclean.index = pd.to_datetime(notclean.index)\nvdf['tweet_vol'] =vdf['tweet_vol'].astype(float)\ndf = notclean.groupby('DateTime').agg(lambda x: x.mean())\ndf = df.drop(df.index[0])\nbtcDF = pd.read_csv('..\/input\/btc-price\/btcSave2.csv', error_bad_lines=False,engine = 'python')\nbtcDF['Timestamp'] = pd.to_datetime(btcDF['Timestamp'])\nbtcDF = btcDF.set_index(pd.DatetimeIndex(btcDF['Timestamp']))\nbtcDF = btcDF.drop(['Timestamp'], axis=1)\nFinal_df = pd.merge(df,btcDF, how='inner',left_index=True, right_index=True)\nFinal_df=Final_df.drop(['Weighted Price'],axis=1 )\nprint(list(Final_df))\nFinal_df.columns = ['Polarity', 'Sensitivity', 'Open','High','Low', 'Close_Price', 'Volume_BTC', 'Volume_Dollar']\nFinal_df = Final_df[['Polarity', 'Sensitivity', 'Open','High','Low', 'Volume_BTC', 'Volume_Dollar', 'Close_Price']]\nFinal_df.head()","e281dd99":"df = Final_df\ndf['Polarity'].describe()","d528e5bd":"df['Sensitivity'].describe()","40328719":"df['Close_Price'].describe()","64f42ded":"def detect(signal,treshold=2.0):\n    detected = []\n    for i in range(len(signal)):\n        if np.abs(signal[i]) > treshold:\n            detected.append(i)\n    return detected","979d37a5":"signal = np.copy(df['Close_Price'].values)\nstd_signal = (signal - np.mean(signal)) \/ np.std(signal)\ns = pd.Series(std_signal)\ns.describe(percentiles=[0.25,0.5,0.75,0.95])","bedace72":"outliers = detect(std_signal, 1.3)","20a89eac":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","6f4bf789":"plt.figure(figsize=(15,7))\nplt.plot(np.arange(len(signal)), signal)\nplt.plot(np.arange(len(signal)), signal, 'X', label='outliers',markevery=outliers, c='r')\nplt.xticks(np.arange(len(signal))[::15], df.index[::15], rotation='vertical')\nplt.show()","c3b1d663":"from sklearn.preprocessing import MinMaxScaler\n\nminmax = MinMaxScaler().fit(df[['Polarity','Sensitivity','Close_Price']])\nscaled = minmax.transform(df[['Polarity','Sensitivity','Close_Price']])","8cfd24a1":"plt.figure(figsize=(15,7))\nplt.plot(np.arange(len(signal)), scaled[:,0], label = 'Scaled polarity')\nplt.plot(np.arange(len(signal)), scaled[:,1], label = 'Scaled sensitivity')\nplt.plot(np.arange(len(signal)), scaled[:,2], label = 'Scaled closed price')\nplt.plot(np.arange(len(signal)), scaled[:,0], 'X', label='outliers polarity based on close', markevery=outliers, c='r')\nplt.plot(np.arange(len(signal)), scaled[:,1], 'o', label='outliers polarity based on close', markevery=outliers, c='r')\nplt.xticks(np.arange(len(signal))[::15], df.index[::15], rotation='vertical')\nplt.legend()\nplt.show()","c378a07f":"colormap = plt.cm.RdBu\nplt.figure(figsize=(15,7))\nplt.title('pearson correlation', y=1.05, size=16)\n\nmask = np.zeros_like(df.corr())\nmask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(df.corr(), mask=mask, linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)\nplt.show()","13fdab6f":"def df_shift(df,lag=0, start=1, skip=1, rejected_columns = []):\n    df = df.copy()\n    if not lag:\n        return df\n    cols ={}\n    for i in range(start,lag+1,skip):\n        for x in list(df.columns):\n            if x not in rejected_columns:\n                if not x in cols:\n                    cols[x] = ['{}_{}'.format(x, i)]\n                else:\n                    cols[x].append('{}_{}'.format(x, i))\n    for k,v in cols.items():\n        columns = v\n        dfn = pd.DataFrame(data=None, columns=columns, index=df.index)    \n        i = 1\n        for c in columns:\n            dfn[c] = df[k].shift(periods=i)\n            i+=1\n        df = pd.concat([df, dfn], axis=1, join_axes=[df.index])\n    return df","7970e5f8":"df_new = df_shift(df, lag=42, start=7, skip=7)\ndf_new.shape","cb7ca05c":"colormap = plt.cm.RdBu\nplt.figure(figsize=(30,20))\nax=plt.subplot(111)\nplt.title('42 hours correlation', y=1.05, size=16)\nselected_column = [col for col in list(df_new) if any([k in col for k in ['Polarity','Sensitivity','Close']])]\n\nsns.heatmap(df_new[selected_column].corr(), ax=ax, linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)\nplt.show()","f41badb6":"def moving_average(signal, period):\n    buffer = [np.nan] * period\n    for i in range(period,len(signal)):\n        buffer.append(signal[i-period:i].mean())\n    return buffer","3688bc24":"signal = np.copy(df['Close_Price'].values)\nma_7 = moving_average(signal, 7)\nma_14 = moving_average(signal, 14)\nma_30 = moving_average(signal, 30)","63ca90ec":"plt.figure(figsize=(15,7))\nplt.plot(np.arange(len(signal)), signal, label='real signal')\nplt.plot(np.arange(len(signal)), ma_7, label='ma 7')\nplt.plot(np.arange(len(signal)), ma_14, label='ma 14')\nplt.plot(np.arange(len(signal)), ma_30, label='ma 30')\nplt.legend()\nplt.show()","859aa388":"num_layers = 1\nlearning_rate = 0.005\nsize_layer = 128\ntimestamp = 5\nepoch = 500\ndropout_rate = 0.6","7a7d9a53":"dates = pd.to_datetime(df.index).tolist()","1579717b":"class Model:\n    def __init__(self, learning_rate, num_layers, \n                 size, size_layer, forget_bias = 0.8):\n        \n        def lstm_cell(size_layer):\n            return tf.nn.rnn_cell.LSTMCell(size_layer, state_is_tuple = False)\n        rnn_cells = tf.nn.rnn_cell.MultiRNNCell([lstm_cell(size_layer) for _ in range(num_layers)], \n                                                state_is_tuple = False)\n        self.X = tf.placeholder(tf.float32, (None, None, size))\n        self.Y = tf.placeholder(tf.float32, (None, size))\n        drop = tf.contrib.rnn.DropoutWrapper(rnn_cells, output_keep_prob = forget_bias)\n        self.hidden_layer = tf.placeholder(tf.float32, \n                                           (None, num_layers * 2 * size_layer))\n        self.outputs, self.last_state = tf.nn.dynamic_rnn(drop, self.X, \n                                                          initial_state = self.hidden_layer, \n                                                          dtype = tf.float32)\n        self.logits = tf.layers.dense(self.outputs[-1],size,\n                       kernel_initializer=tf.glorot_uniform_initializer())\n        self.cost = tf.reduce_mean(tf.square(self.Y - self.logits))\n        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.cost)","7b5a317c":"minmax = MinMaxScaler().fit(df[['Polarity','Sensitivity','Close_Price']].astype('float32'))\ndf_scaled = minmax.transform(df[['Polarity','Sensitivity','Close_Price']].astype('float32'))\ndf_scaled = pd.DataFrame(df_scaled)\ndf_scaled.head()","be01b0ba":"tf.reset_default_graph()\nmodelnn = Model(learning_rate, num_layers, df_scaled.shape[1], size_layer, dropout_rate)\nsess = tf.InteractiveSession()\nsess.run(tf.global_variables_initializer())","786bd6fe":"for i in range(epoch):\n    init_value = np.zeros((1, num_layers * 2 * size_layer))\n    total_loss = 0\n    for k in range(0, (df_scaled.shape[0] \/\/ timestamp) * timestamp, timestamp):\n        batch_x = np.expand_dims(df_scaled.iloc[k: k + timestamp].values, axis = 0)\n        batch_y = df_scaled.iloc[k + 1: k + timestamp + 1].values\n        last_state, _, loss = sess.run([modelnn.last_state, \n                                        modelnn.optimizer, \n                                        modelnn.cost], feed_dict={modelnn.X: batch_x, \n                                                                  modelnn.Y: batch_y, \n                                                                  modelnn.hidden_layer: init_value})\n        init_value = last_state\n        total_loss += loss\n    total_loss \/= (df.shape[0] \/\/ timestamp)\n    if (i + 1) % 100 == 0:\n        print('epoch:', i + 1, 'avg loss:', total_loss)","0f2fbad2":"def predict_future(future_count, df, dates, indices={}):\n    date_ori = dates[:]\n    cp_df = df.copy()\n    output_predict = np.zeros((cp_df.shape[0] + future_count, cp_df.shape[1]))\n    output_predict[0, :] = cp_df.iloc[0]\n    upper_b = (cp_df.shape[0] \/\/ timestamp) * timestamp\n    init_value = np.zeros((1, num_layers * 2 * size_layer))\n    for k in range(0, (df.shape[0] \/\/ timestamp) * timestamp, timestamp):\n        out_logits, last_state = sess.run(\n            [modelnn.logits, modelnn.last_state],\n            feed_dict = {\n                modelnn.X: np.expand_dims(\n                    cp_df.iloc[k : k + timestamp], axis = 0\n                ),\n                modelnn.hidden_layer: init_value,\n            },\n        )\n        init_value = last_state\n        output_predict[k + 1 : k + timestamp + 1] = out_logits\n    out_logits, last_state = sess.run(\n        [modelnn.logits, modelnn.last_state],\n        feed_dict = {\n            modelnn.X: np.expand_dims(cp_df.iloc[upper_b:], axis = 0),\n            modelnn.hidden_layer: init_value,\n        },\n    )\n    init_value = last_state\n    output_predict[upper_b + 1 : cp_df.shape[0] + 1] = out_logits\n    cp_df.loc[cp_df.shape[0]] = out_logits[-1]\n    date_ori.append(date_ori[-1] + timedelta(hours = 1))\n    if indices:\n        for key, item in indices.items():\n            cp_df.iloc[-1,key] = item\n    for i in range(future_count - 1):\n        out_logits, last_state = sess.run(\n            [modelnn.logits, modelnn.last_state],\n            feed_dict = {\n                modelnn.X: np.expand_dims(cp_df.iloc[-timestamp:], axis = 0),\n                modelnn.hidden_layer: init_value,\n            },\n        )\n        init_value = last_state\n        output_predict[cp_df.shape[0], :] = out_logits[-1, :]\n        cp_df.loc[cp_df.shape[0]] = out_logits[-1, :]\n        date_ori.append(date_ori[-1] + timedelta(hours = 1))\n        if indices:\n            for key, item in indices.items():\n                cp_df.iloc[-1,key] = item\n    return {'date_ori': date_ori, 'df': cp_df.values}    ","c21aa990":"def anchor(signal, weight):\n    buffer = []\n    last = signal[0]\n    for i in signal:\n        smoothed_val = last * weight + (1 - weight) * i\n        buffer.append(smoothed_val)\n        last = smoothed_val\n    return buffer","a728f79d":"predict_30 = predict_future(30, df_scaled, dates)\npredict_30['df'] = minmax.inverse_transform(predict_30['df'])","a9637db8":"plt.figure(figsize=(15,7))\nplt.plot(np.arange(len(predict_30['date_ori'])), anchor(predict_30['df'][:,-1],0.5), label='predict signal')\nplt.plot(np.arange(len(signal)), signal, label='real signal')\nplt.legend()\nplt.show()","92702ec0":"scaled_polarity = (minmax.data_max_[0] * 2 - minmax.data_min_[0]) \/ (minmax.data_max_[0] - minmax.data_min_[0])\nscaled_polarity","4a3661fc":"plt.figure(figsize=(15,7))\n\nfor retry in range(3):\n    plt.subplot(3, 1, retry + 1)\n    predict_30 = predict_future(30, df_scaled, dates, indices = {0:scaled_polarity})\n    predict_30['df'] = minmax.inverse_transform(predict_30['df'])\n    plt.plot(np.arange(len(predict_30['date_ori'])), anchor(predict_30['df'][:,-1],0.5), label='predict signal')\n    plt.plot(np.arange(len(signal)), signal, label='real signal')\n    plt.legend()\nplt.show()","32f0f8ff":"scaled_polarity = (minmax.data_max_[0] * 4 - minmax.data_min_[0]) \/ (minmax.data_max_[0] - minmax.data_min_[0])\nscaled_polarity","44e4b6bc":"plt.figure(figsize=(15,7))\n\nfor retry in range(3):\n    plt.subplot(3, 1, retry + 1)\n    predict_30 = predict_future(30, df_scaled, dates, indices = {0:scaled_polarity})\n    predict_30['df'] = minmax.inverse_transform(predict_30['df'])\n    plt.plot(np.arange(len(predict_30['date_ori'])), anchor(predict_30['df'][:,-1],0.5), label='predict signal')\n    plt.plot(np.arange(len(signal)), signal, label='real signal')\n    plt.legend()\nplt.show()","0be9ae8b":"scaled_polarity = (minmax.data_min_[0] \/ 4 - minmax.data_min_[0]) \/ (minmax.data_max_[0] - minmax.data_min_[0])\nscaled_polarity","91254315":"plt.figure(figsize=(15,7))\n\nfor retry in range(3):\n    plt.subplot(3, 1, retry + 1)\n    predict_30 = predict_future(30, df_scaled, dates, indices = {0:scaled_polarity})\n    predict_30['df'] = minmax.inverse_transform(predict_30['df'])\n    plt.plot(np.arange(len(predict_30['date_ori'])), anchor(predict_30['df'][:,-1],0.5), label='predict signal')\n    plt.plot(np.arange(len(signal)), signal, label='real signal')\n    plt.legend()\nplt.show()","45924703":"This is cross-correlation, I am actually just interested with Close_Price and Polarity_X","1104371d":"Define some smoothing, using previous value as an anchor","eee41b0e":"# Data Pre-processing","681b3cdd":"From 0.95, I will take 1.3","5f8bcd1f":"# Forecast bitcoin based on Polarity using LSTM","d73d629d":"## Detecting outliers \/ sudden spikes in our close prices","df2dfeb0":"## Pearson correlation","5a6637ae":"## What happen if polarity is double from the max? Polarity is first index","6ecd77b3":"## What happen if polarity is quadriple from the max? Polarity is first index","d04429e8":"## Simple metrics study","46288ced":" i think i had too much playing with daily trending data","2dd4bec8":"## Now deep learning LSTM","2ae35501":"## What happen if polarity is quadriple from the min? polarity is first index","a09d3e2a":"Doesnt show much from trending, how about covariance correlation?","1fb06133":"## How about we check trends from moving average? i chose 7, 14, 30 hours","cb1a9a2f":"I will copy previous author boilerplate to get necessary dataframe.","eff9eb15":"I retried for 3 times just to study how fitted our model is, if every retry has big trend changes, so we need to retrain again."}}