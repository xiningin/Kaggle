{"cell_type":{"3c9f1430":"code","f1a6d228":"code","d66b9499":"code","ff834504":"code","eb0aac2a":"code","4c7991dd":"code","6ded88d4":"code","1e934e84":"code","f6bba70e":"code","a52d376c":"code","653bdfef":"code","6b8c7265":"code","f25cafe9":"code","9ac400fe":"code","33fa0ec4":"code","896baab4":"code","b31fe430":"code","7dd5fcaf":"code","90f0ba87":"code","ef66fd19":"code","cd7e9f11":"code","71f95f18":"code","63e7644d":"code","55aeab50":"code","fb744835":"code","72ed1ab2":"code","7a202db0":"code","2b021001":"code","5cd0befe":"markdown","eeee9b6c":"markdown","a4b6f6d3":"markdown","1cfb61b6":"markdown","5f49f23d":"markdown","2560097b":"markdown","7d567b37":"markdown","8864322f":"markdown","fb329645":"markdown","8c1807c8":"markdown","3c0844d7":"markdown","953da62e":"markdown","07eead5d":"markdown","5d74930f":"markdown","f7a72ba5":"markdown","02908950":"markdown","ab457b70":"markdown","ea2ae1c5":"markdown","b6802b25":"markdown","e302e7e8":"markdown","eca72871":"markdown","cd099f3e":"markdown","d89d01be":"markdown","cfab3a59":"markdown","ea35c1a7":"markdown","9391a9ac":"markdown","ecbf5a83":"markdown"},"source":{"3c9f1430":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f1a6d228":"filename = \"\/kaggle\/input\/personal-cars-classifieds\/all_anonymized_2015_11_2017_03.csv\"\n\ndtypes = {\n    \"maker\": str, # brand name\n    \"model\": str,\n    \"mileage\": float, # km\n    \"manufacture_year\": float,\n    \"engine_displacement\": float,\n    \"engine_power\": float,\n    \"body_type\": str, # almost never present\n    \"color_slug\": str, # also almost never present\n    \"stk_year\": str,\n    \"transmission\": str, # automatic or manual\n    \"door_count\": str,\n    \"seat_count\": str,\n    \"fuel_type\": str, # gasoline or diesel\n    \"date_created\": str, # when the ad was scraped\n    \"date_last_seen\": str, # when the ad was last seen\n    \"price_eur\": float} # list price converted to EUR\n\ndf = pd.read_csv(filename, dtype=dtypes)\nprint(f\"Raw data has {df.shape[0]} rows, and {df.shape[1]} columns\")\ndf.head()","d66b9499":"# Tons of missing values\nprint(df.isna().sum())","ff834504":"# Here we can see strage data, like the Min() and the Max() of some features\ndf.describe().apply(lambda s: s.apply(lambda x: format(x, 'g')))","eb0aac2a":"df.info()","4c7991dd":"df = df[df[\"price_eur\"] != 1295.34]\ndf = df[df[\"price_eur\"] > 150.00]\n\ndf_ = df.select_dtypes(exclude=['int', 'float'])\ndf_ = df_[['maker', 'seat_count', 'door_count', 'stk_year', 'transmission', 'fuel_type']]\nfor col in df_.columns:\n    print(\"Col: \", col)\n    print(df_[col].unique(), end='\\n\\n') # to print categories name only\n    # print(df_[col].value_counts()) # to print count of every category","6ded88d4":"df_new = df.reset_index(drop=True).copy()\n\n# Manufacture Year\nmin_year = 1970 # minimum threshold of the year. Lower values will be replaced with the average of manufacture_year\naverage_years = df_new.loc[df_new[\"manufacture_year\"] >= min_year].\\\ngroupby([\"maker\", \"model\"])[\"manufacture_year\"].\\\nmean().round().rename(\"avg_manufacture_year\").reset_index() # support column with the average year grouped by maker and model\n\ndf_filtered = df_new.merge(average_years, how=\"left\", on=[\"maker\", \"model\"]) # merge of the support column with the dataframe\ndf_filtered.loc[df_new[\"manufacture_year\"] < min_year, \"manufacture_year\"] = np.nan # force values below the minimum threshold to nan\ndf_filtered[\"manufacture_year\"] = df_filtered[\"manufacture_year\"].fillna(df_filtered[\"avg_manufacture_year\"]) # fill nan values with the average year\ndf_filtered = df_filtered[df_filtered['manufacture_year'].notna()].reset_index(drop=True) # delete of those rows that have not manufacture_year valorized yet\n\n# Engine Power\nmin_engine_power = 50\naverage = df_filtered.loc[df_filtered[\"engine_power\"] >= min_engine_power].\\\ngroupby([\"maker\", \"model\", \"manufacture_year\"])[\"engine_power\"].mean().round().rename(\"avg_engine_power\").reset_index()\n\ndf_filtered2 = df_filtered.merge(average, how=\"left\", on=[\"maker\", \"model\", \"manufacture_year\"])\ndf_filtered2.loc[df_filtered[\"engine_power\"] < min_engine_power, \"engine_power\"] = np.nan\ndf_filtered2[\"engine_power\"] = df_filtered2[\"engine_power\"].fillna(df_filtered2[\"avg_engine_power\"])\ndf_filtered2 = df_filtered2[df_filtered2['engine_power'].notna()].reset_index(drop=True)\n\n# Engine Displacement\nmin_engine_displacement = 600\naverage = df_filtered2.loc[df_filtered2[\"engine_displacement\"] >= min_engine_displacement].\\\ngroupby([\"maker\", \"model\"])[\"engine_displacement\"].mean().round().rename(\"avg_engine_displacement\").reset_index()\n\ndf_filtered3 = df_filtered2.merge(average, how=\"left\", on=[\"maker\", \"model\"])\ndf_filtered3.loc[df_filtered2[\"engine_displacement\"] < min_engine_displacement, \"engine_displacement\"] = np.nan\ndf_filtered3[\"engine_displacement\"] = df_filtered3[\"engine_displacement\"].fillna(df_filtered3[\"avg_engine_displacement\"])\ndf_filtered3 = df_filtered3[df_filtered3['engine_displacement'].notna()].reset_index(drop=True)\n\n# Mileage\naverage = df_filtered3.groupby([\"manufacture_year\"])[\"mileage\"].mean().\\\nround().rename(\"avg_mileage\").reset_index() # support column with the average mileage with same manufacture_year\n\ndf_filtered4 = df_filtered3.merge(average, how=\"left\", on=[\"manufacture_year\"]) # merge of the support column with the dataframe\ndf_filtered4[\"mileage\"] = df_filtered4[\"mileage\"].fillna(df_filtered4[\"avg_mileage\"]).reset_index(drop=True) # fill nan values with the average mileage\n\n# Fuel Type\naverage = df_filtered4.loc[df_filtered4[\"fuel_type\"].notna()].\\\ngroupby([\"maker\",\"model\"])[\"fuel_type\"].agg(lambda x:x.value_counts().index[0]).\\\nrename(\"mode_fuel_type\").reset_index() # support column with the most frequent value of fuel_type grouped by maker and model\n\ndf_filtered5 = df_filtered4.merge(average, how=\"left\", on=[\"maker\",\"model\"]) # merge of the support column with the dataframe\ndf_filtered5[\"fuel_type\"] = df_filtered5[\"fuel_type\"].fillna(df_filtered5[\"mode_fuel_type\"]) # fill nan values with the mode of fuel_type\ndf_filtered5 = df_filtered5[df_filtered5['fuel_type'].notna()].reset_index(drop=True) # delete of those rows that have not fuel_type valorized yet\n\ndf_filtered5 = pd.get_dummies(df_filtered5,columns=[\"fuel_type\"]) # get dummies variables of this categorical feature \n\ndf_cleaned = df_filtered5.copy()","1e934e84":"# AD duration\ndf_cleaned['date_created'] = pd.to_datetime(df_cleaned['date_created']).dt.normalize()\ndf_cleaned['date_last_seen'] = pd.to_datetime(df_cleaned['date_last_seen']).dt.normalize()\ndf_cleaned['ad_duration'] = (df_cleaned['date_last_seen'] - df_cleaned['date_created']).dt.days # new feature: how much last the ad","f6bba70e":"features_to_fillna = [\"seat_count\"]\nfor feature in features_to_fillna:\n    df_cleaned[feature] = pd.to_numeric(df_cleaned[feature], errors=\"coerce\") # transform to numeric\n    replace_with = df_cleaned[feature].median()                       # deduction of the median\n    df_cleaned[feature].fillna(replace_with,inplace=True)             # replace null\n    \ndf_cleaned[\"seat_str\"] = np.select(\n    condlist=[\n        (df_cleaned[\"seat_count\"] >= 0) & (df_cleaned[\"seat_count\"] < 4),\n        (df_cleaned[\"seat_count\"] >= 4) & (df_cleaned[\"seat_count\"] < 6),\n        (df_cleaned[\"seat_count\"] >= 6)],\n    choicelist=[\n        \"small\",\n        \"medium\",\n        \"large\",\n        ],\n    default=\"unknown\")\ndf_cleaned = pd.get_dummies(df_cleaned,columns=[\"seat_str\"])\ndf_cleaned = df_cleaned.drop([\"seat_count\"], axis=1)","a52d376c":"df_cleaned = df_cleaned[df_cleaned['transmission'].notna()]\ndf_cleaned = pd.get_dummies(df_cleaned,columns=[\"transmission\"])\n#size_mapping = {\"man\":0,\"auto\":1}\n#df_cleaned[\"transmission\"] = df_cleaned[\"transmission\"].map(size_mapping)","653bdfef":"# drop features\nfeatures_to_drop = [\"model\", \"door_count\", \"body_type\", \"color_slug\", \"stk_year\", \"avg_manufacture_year\", \"date_created\", \"date_last_seen\", \"avg_engine_displacement\",\"avg_engine_power\",\"avg_mileage\", \"mode_fuel_type\"]\ndf_cleaned = df_cleaned.drop(features_to_drop, axis=\"columns\")","6b8c7265":"# Missing values\nprint(df_cleaned.isna().sum())","f25cafe9":"corr = df_cleaned.corr()\nplt.subplots(figsize=(15,10))\nsns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, )","9ac400fe":"mse_list = []\nr2_score_list = []\n\ndef remove_outliers(dataframe):\n    '''\n    return a dataframe without rows that are outliers in any column\n    '''\n    return dataframe\\\n    .loc[:, lambda df: df.std() > 0.04]\\\n    .loc[lambda df: (np.abs(stats.zscore(df)) < 3).all(axis=1)]\n\ndef plot_regression(Y_test, Y_pred):\n    '''\n    method that plot a linear regression line on a scatter plot\n    '''\n    x = Y_test\n    y = Y_pred\n\n    plt.xlabel(\"True label\")\n    plt.ylabel(\"Predicted label\")\n    plt.plot(x, y, 'o')\n\n    m, b = np.polyfit(x, y, 1)\n\n    plt.plot(x, m*x + b)","33fa0ec4":"df_no_maker = df_cleaned.copy()\n\n# drop of the maker feature \ndf_no_maker = df_no_maker.drop(\"maker\", axis=\"columns\")\n\ndf_no_maker = remove_outliers(df_no_maker)\n\nX = df_no_maker.drop(\"price_eur\", axis=1).values\nY = np.log1p(df_no_maker[\"price_eur\"].values)\n\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.1, random_state=0)\n\nll = LinearRegression()\nll.fit(X_train, Y_train)\nY_pred = ll.predict(X_test)\n\nmse_list.append(mean_squared_error(Y_test, Y_pred))\nr2_score_list.append(r2_score(Y_test, Y_pred))\n\nprint(\"MSE: \"+str(mean_squared_error(Y_test, Y_pred)))\nprint(\"R2 score: \"+str(r2_score(Y_test, Y_pred)))","896baab4":"plot_regression(Y_test, Y_pred)","b31fe430":"df_replace_mode = df_cleaned.copy()\n\nreplace_with = df_replace_mode[\"maker\"].mode()             # deduction of the mode\ndf_replace_mode[\"maker\"].fillna(replace_with,inplace=True) # replace null with the mode\n\ndf_replace_mode = pd.get_dummies(df_replace_mode,columns=[\"maker\"])\n\ndf_replace_mode = remove_outliers(df_replace_mode)\n\nX = df_replace_mode.drop(\"price_eur\", axis=1).values\nY = np.log1p(df_replace_mode[\"price_eur\"].values)\n\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.1, random_state=0)\n\nll = LinearRegression()\nll.fit(X_train, Y_train)\nY_pred = ll.predict(X_test)\n\nmse_list.append(mean_squared_error(Y_test, Y_pred))\nr2_score_list.append(r2_score(Y_test, Y_pred))\n\nprint(\"MSE: \"+str(mean_squared_error(Y_test, Y_pred)))\nprint(\"R2 score: \"+str(r2_score(Y_test, Y_pred)))","7dd5fcaf":"plot_regression(Y_test, Y_pred)","90f0ba87":"df_del_rows = df_cleaned.copy()\n\n# deleteing row with null maker\ndf_del_rows = df_del_rows[df_del_rows['maker'].notna()]\ndf_del_rows = pd.get_dummies(df_del_rows,columns=[\"maker\"])\n\ndf_del_rows = remove_outliers(df_del_rows)\n\nX = df_del_rows.drop(\"price_eur\", axis=1).values\nY = np.log1p(df_del_rows[\"price_eur\"].values)\n\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.1, random_state=0)\n\nll = LinearRegression()\nll.fit(X_train, Y_train)\nY_pred = ll.predict(X_test)\n\nmse_list.append(mean_squared_error(Y_test, Y_pred))\nr2_score_list.append(r2_score(Y_test, Y_pred))\n\nprint(\"MSE: \"+str(mean_squared_error(Y_test, Y_pred)))\nprint(\"R2 score: \"+str(r2_score(Y_test, Y_pred)))","ef66fd19":"plot_regression(Y_test, Y_pred)","cd7e9f11":"df_with_maker = df_cleaned[df_cleaned['maker'].notna()]\nprint(\"N. rows with maker not null:\", df_with_maker.shape[0])\n\ndf_no_maker = df_cleaned[df_cleaned['maker'].isna()]\nprint(\"N. rows with maker null:\", df_no_maker.shape[0])","71f95f18":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import accuracy_score\n\nX = df_with_maker.drop(\"maker\", axis=1).values\nY = df_with_maker[\"maker\"].values\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=0)\n\nforest = RandomForestClassifier(n_estimators=45, max_depth=25, random_state=False, \n                                max_features=0.6, min_samples_leaf=3, n_jobs=-1)\n\nforest.fit(X_train, Y_train)\n\ny_pred_train = forest.predict(X_train)\ny_pred = forest.predict(X_test)\n\ny_pred_proba = forest.predict_proba(X_test)\n\naccuracy_train = accuracy_score(Y_train, y_pred_train)\naccuracy_test = accuracy_score(Y_test, y_pred)\n\nprint(\"ACCURACY: TRAIN=%.4f TEST=%.4f\" % (accuracy_train,accuracy_test))\nprint(\"LOG LOSS: \"+str(log_loss(Y_test, y_pred_proba)))\n\nimportances = forest.feature_importances_\nindices = list(np.argsort(importances))[::-1]\n\nplt.title(\"Feature importances\")\nplt.barh(range(len(indices)), importances[indices], color=\"g\", align=\"center\")\nplt.yticks(range(len(indices)), df_with_maker.iloc[:, 1:].columns[indices])\n# plt.ylim([-1, len(indices)])\nplt.gca().invert_yaxis()","63e7644d":"df_no_maker = df_no_maker.drop('maker', axis=1)\nprediction = forest.predict(df_no_maker)\n\ndf_no_maker.insert(0, 'maker', prediction)","55aeab50":"frames = [df_with_maker, df_no_maker]\ndf_final = pd.concat(frames)\nprint(df_final.shape[0])","fb744835":"df_final.head()","72ed1ab2":"df_final = pd.get_dummies(df_final,columns=[\"maker\"])\n\ndf_final = remove_outliers(df_final)\n\nX = df_final.drop(\"price_eur\", axis=1).values\nY = np.log1p(df_final[\"price_eur\"].values)\n\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.1, random_state=0)\n\nll = LinearRegression()\nll.fit(X_train, Y_train)\nY_pred = ll.predict(X_test)\n\nmse_list.append(mean_squared_error(Y_test, Y_pred))\nr2_score_list.append(r2_score(Y_test, Y_pred))\n\nprint(\"MSE: \"+str(mean_squared_error(Y_test, Y_pred)))\nprint(\"R2 score: \"+str(r2_score(Y_test, Y_pred)))","7a202db0":"plot_regression(Y_test, Y_pred)","2b021001":"options_list = [\n    \"Delete the entire column Maker\", \n    \"Replace null values with the mode\", \n    \"Delete rows with null values\", \n    \"Predict the missing values\"\n]\n\ndf_metrics = pd.DataFrame({\n    \"\": options_list,\n    \"MSE\": mse_list,\n    \"R2_score\": r2_score_list,\n})\ndf_metrics.head()","5cd0befe":"## Some Feature Engineering\nLet's start to fill the numerous missing data of our dataset. The technique that I have decided to implement here is to calculate the average\/mode value of the data grouped by some features (maker, model, manufacture_year), and then fill the missing values with the calculated data","eeee9b6c":"## Delete rows with null values\nAnother option is to delete rows that have null values. Absolutely not recommended if our dataset is very small, but easily feasible if there are missing only a few values or if we have a very large dataset\n","a4b6f6d3":"As already mentioned, the **seat_count** feature presents very unexpected data. We then replace the missing values with the median and then create a new categorical feature","1cfb61b6":"## Predict the missing values with the RandomForestClassifier\nThe most interesting approach is for sure the predicting of missing values with a classification algorithm. This will give us the opportunity not to waste a good chunk of the dataset, and thus a large amount of information. If our predictions are accurate enough with this technique we should have the best metric scores.\n\nTherefore:\n- we have to split the dataset between rows with the column *maker* valorized, and rows with null values.\n- the first dataframe will become the one on which we will create the classification model with the *maker* as target feature.\n- use the model thus created to predict the missing values from the dataframe with null values\n- merge the two dataframe into one\n- training the linear regression model","5f49f23d":"Here's we can see the best metrics values thanks to the rows saved through the aforementioned dataset merge","2560097b":"## Replace missing values with the most frequent data\nA certainly more effective method is to assign the missing values with the most frequent data, the mode. But be careful that this could lead to an unbalanced dataset, in case the missing values are a considerable number.","7d567b37":"Now let's look at the values of our data. Some are really unusual. A car with an engine power of 32k or 1 doesn't make much sense... or like a car with 1 million mileage, a price of a few cents, etc ...","8864322f":"Here we transform the dates features from string to datetime and then create a new feature called **ad_duration**, calculated as the distance in days between the two dates. This new feature will allow us to understand how tempting the AD is.","fb329645":"## Conclusions\nOur considerations are driven by metrics. Deleting the feature from the dataset should be our last resort. Replacing null data with the most frequent ones or deleting the rows can be a convenient solution in case we have few missing data or a very large dataset. On the contrary, in case we have a lot of missing values or a small dataset, a prediction\/clustering could save us valuable information that would otherwise be lost.","8c1807c8":"Here's our final dataframe","3c0844d7":"## Delete the entire column *Maker*\nOur first basic try will be to create a model without the column **maker**. This \"no-deal\" practice is needed when the large amount of missing data threatens to invalidate the entire feature.\n\nWith this scenario we will probably have the worst scores on metrics. We will use them to compare the other methods.","953da62e":"*Drop* support columns and unusable features","07eead5d":"Now that we have the model, let's fill the 2nd dataframe with the values of the prediction...","5d74930f":"As expected, we got better scoring metrics than before","f7a72ba5":"Without too much surprise the model tells us that **engine_displacement** and **engine_power** are the key features to determine the **maker**","02908950":"## Show Raw Data","ab457b70":"Let's divide the dataset in two","ea2ae1c5":"# Handle missing values in Categorical Features\nIn this notebook will be shown how to deal with categorical features with missing values.\nIt will be used the [Classified Ads for Cars](https:\/\/www.kaggle.com\/mirosval\/personal-cars-classifieds) dataset to predict the price of ADs through a simple model of Linear Regression. \n\nIn order to show the various strategies and relevants pros \/ cons, we will focus on a particular categorical feature of this dataset, the **maker**, the name of the brand of cars (Toyota, Kia, Ford, Bmw, ...). \n\nWe will cover the following techniques:\n* **Replace** missing values with the *most frequent values*.\n* **Delete** rows with null values.\n* **Predict** values using a Classifier Algorithm (supervised or unsupervised)\n\n\n## Steps of this notebook\n* **Show Raw Data**: let's see how our dataset looks like.\n* **Some Feature Engineering**: data cleaning and extraction of new features.\n* **Dealing with missing values in Categorical Features**: we will deal missing values by comparing different techniques.\n* **Conclusions**","b6802b25":"Let's start by creating the classification model and take a look at its metrics","e302e7e8":"Now that we've cleaned all the features, with the exception of the **maker**, let's see how they look like","eca72871":"It seems this dataset is chock full of missing values... here we can see how many and in which features","cd099f3e":"...and then, merge the two dataframe togheter","d89d01be":"Now that our final dataset is ready we can re-create the linear regression model and check if we have improved its metrics","cfab3a59":"This is our first try. Can we do better?","ea35c1a7":"With the **transmission** feature a few data is missing, so we can *Delete* rows with null values. With the remaining rows we can create two new binary columns starting from the initial categorical feature.","9391a9ac":"As previously said, missing values are not the only problem. This dataset presents various \"unexpected\" data. Let's take a quick look at, for example, **seat_count**, **door_count** or **stk_year**. Below here are shown. Due to certain values or their lack, certain features are unlikely to be useful for our purposes.","ecbf5a83":"## Dealing with missing values in Categorical Features\nAlmost there! Now we just have to handle the **maker** feature, and we will do it with four different ways. Then, for each of them, we will create a simple model of Linear Regression for the prediction of the price.\n\n* **1st Model**: Delete the entire column **maker**.\n* **2nd Model**: Replace missing values with the *most frequent values*.\n* **3rd Model**: Delete rows with null values.\n* **4th Model**: Predict the missing values with the RandomForestClassifier."}}