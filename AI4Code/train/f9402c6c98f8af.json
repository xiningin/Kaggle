{"cell_type":{"910fd7f5":"code","bafc9f11":"code","5b6d9ebf":"code","c355d8cf":"code","cabe9925":"code","87232bb7":"code","6425d2f0":"code","8e12aab0":"code","33bffbd9":"code","e297854f":"code","09c6281d":"code","fc3de7fd":"code","a2aea25b":"code","52f9d7fa":"code","ca323038":"code","0f290aba":"code","47f7bce4":"code","7c6f6f55":"code","62aa9fe1":"code","ad54eb05":"code","3be3e566":"code","e1a48318":"code","a439d9b5":"code","60b7c036":"code","38d0e3fb":"markdown"},"source":{"910fd7f5":"# This data set contains data about purchases made during a Black Friday along with demographic data of the customers.\n\n# Our goal in this exploratory analysis is to find correlations between the purchases made and demographic data to \n# deliver our findings to the marketing department. These findings should assist in a better targeting those customers \n# who are already more loyal to the store AS WELL AS as finding any missed (if any) opportunities.  \n\n# Analyzing data: \n# 1) What were the top products sold on BF? \n# 2) Who is our most loyal customer on BF? \n# 3) Which cities are driving most sales? \n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib as mlp\nimport matplotlib.pyplot as plt\nimport os\n","bafc9f11":"datafile = \"..\/input\/BlackFriday.csv\"\ndf=pd.read_csv(datafile)\ndf.info();","5b6d9ebf":"df.shape","c355d8cf":"df.head()","cabe9925":"df.info()","87232bb7":"# Filling missing values\n\ndf = df.fillna(0)","6425d2f0":"df.describe()","8e12aab0":"# Before starting to dive into different parameters let's look at the distribution of spendings during the Black Friday\n\nplt.hist(df['Purchase'],bins=20, alpha=0.6, color='g');\n\n# The average purchase size was around $9,333 while the median purchase was at $8,062","33bffbd9":"# 1) Let's look at the top 10 products that were most popular during the Black Friday\n\ndf['Product_ID'].value_counts().sort_values(ascending=False).head(10)\n\n# If we could compare this data to the regular day sales during the rest of the year we could find out \n# which products are the most in demand on the BF specifically","e297854f":"# 2) let's look at the data by gender \n# How many women vs men were shopping on Black Froday \n\nplt.pie(df.groupby(['Gender']).size(), radius=1, labels=['F', 'M'], wedgeprops={'alpha':0.6},colors=['r','b'] );\nplt.axis('equal');\n\n# Who could have thought that men are such a bargain hunters?  ","09c6281d":"# How the amount of $ spent differed between men and women \n\ngrouped = df.groupby('Gender').agg({'Purchase':'mean'})\nprint (grouped)\ngrouped.plot(kind='bar', legend=None, alpha = 0.6, color = ['r', 'b']);\n\n# Not only men are more in numbers but they also spend on average $700 more during the BF","fc3de7fd":"# And now let's see if single people spend more in our stores \n\ngrouped = df.groupby('Marital_Status').agg({'Purchase':'mean'})\nprint (grouped)\ngrouped.plot(kind='bar', legend=None, alpha = 0.7);\n\n# It looks like on average there is no difference in spendings","a2aea25b":"# How about the age groups? Is it a younger or older group that visits our stores more often?  \n\ngrouped = df.groupby('Age').agg({'Purchase':'count'})\ngrouped.plot(kind='bar', legend=None, alpha = 0.6, color = 'y');\n\n# It looks like our stores are popular among the mid aged people in their early 30s","52f9d7fa":"# Let's now see men's and women's behavior in various age groups  \n\nage_order = ['0-17','18-25','26-35','36-45','46-50','51-55','55+']\n\nplt.subplots(figsize=(10,5))\nplt.subplot(1, 2, 1)\nsns.countplot('Age',order=age_order,hue='Gender',data=df, alpha = 0.7)\nplt.xlabel('Age')\nplt.ylabel('')\nplt.xticks(rotation=90)\nplt.title('Number of customers')\nplt.legend(['Female','Male'])\n\nplt.subplot(1,2,2)\ndf_by_Age = df.groupby(['Age','Gender']).agg({'Purchase':np.sum}).reset_index()\nsns.barplot('Age','Purchase',hue='Gender',data=df_by_Age, alpha = 0.7)\nplt.xlabel('Age')\nplt.ylabel('')\nplt.xticks(rotation=90)\nplt.title('Total purchase')\nplt.legend().set_visible(False)\n\n# These graphs show that our primary clientelle were men in their early 30s and they tend to spend most. ","ca323038":"# And now let's dive deeper to help our marketing folks to paint the portrait of our most loyal customers\n\ndf['combined_G_M'] = df.apply(lambda x:'%s_%s' % (x['Gender'],x['Marital_Status']),axis=1)\nsns.countplot(df['Age'],hue=df['combined_G_M'], order=age_order, alpha = 0.7)\n\n# Here we can conclude that single men in 26-35 years old are our primary clientelle. On the second place would be married men\n# in the same age group. And the third place goes to single men 18-25 years old. ","0f290aba":"# Does the time lived in a city play a role in shopping during the BF? \n\ngrouped = df.groupby('Stay_In_Current_City_Years').agg({'Purchase':'sum'})\ngrouped.plot(kind='bar', legend=None, alpha = 0.7);\n\n# We can clearly see that the those who lived at least one year but no more that 2 years in the city tend to visit our store during the BF.","47f7bce4":"# How the amount of $ spent differed between various occupations \n\ngrouped = df.groupby('Occupation').agg({'Purchase':'sum'})\ngrouped.plot(kind='bar', legend=None, alpha = 0.7);\n\n# Our marketing team will be happy to know that people in occupations 4, 0, and 7 are the biggest spenders. \n# This could considerably help them narrow down and target those potential customers based on their occupation ","7c6f6f55":"# Let's now see our top customers based on age, marital status, and their occupation\n\ndf['combined_G_M_O'] = df.apply(lambda x:'%s_%s_%s' % (x['Gender'],x['Marital_Status'],x['Occupation']),axis=1)\nprint (df['combined_G_M_O'].value_counts().sort_values(ascending=False).head(15))\n\n# We see the already clearly established trend that single men in their early 30s who are in occupations 4, 0, and 7 \n# are our target audience ","62aa9fe1":"# 3) Let's look at the sales generated by the stores in each city \n\ngrouped = df.groupby('City_Category').agg({'Purchase':'sum'})\ngrouped.plot(kind='bar', legend=None, alpha = 0.7);\n\n\n","ad54eb05":"#Let's look at the number of visitors per store \n\ngrouped = df.groupby('City_Category').agg({'User_ID':'count'})\ngrouped.plot(kind='bar', legend=None, alpha = 0.7);\n\n# The distribution is tha same as with the sales generated, thus we can conlcude that the city B drives more sales \n# through more customers rather than through bigger size of purchases ","3be3e566":"# CONCLUSION\n# 1) We have identified top 10 most popular products on BF\n# 2) Our most loyal customer on BF are single men in their early 30s who are in occupations 4, 0, and 7 \n# 3) The city B did the best during this season in attracting the most customers and thus selling more  ","e1a48318":"# Let's look at correlation matrix\n\ndf.corr()","a439d9b5":"from sklearn.feature_selection import SelectPercentile\nfrom sklearn.feature_selection import f_regression\n\ndata=df.drop(['Product_ID','User_ID','combined_G_M_O','combined_G_M'],axis=1)\n\ndata['Gender']=data['Gender'].map( {'M': 1, 'F': 0} ).astype(int)\ndata['City_Category']=data['City_Category'].map( {'A': 0, 'B': 1, 'C':2} ).astype(int)\ndata['Age']=data['Age'].map( {'0-17': 0, '18-25': 1, '26-35': 2,'36-45':3,'46-50':4,'51-55':5,'55+':6} ).astype(int)\ndata['Stay_In_Current_City_Years']=data['Stay_In_Current_City_Years'].map( {'0': 0, '1': 1, '2': 2,'3':3,'4+':4}).astype(int)\ndf['Product_Category_1']=df['Product_Category_1'].astype(int)\ndf['Product_Category_2']=df['Product_Category_2'].astype(int)\ndf['Product_Category_3']=df['Product_Category_3'].astype(int)\n\nX=data.drop(['Purchase'],axis=1).values\ny=data['Purchase'].values\n\n# Chosing the features based on the percentile scores \nSelector_f = SelectPercentile(f_regression, percentile=25)\nSelector_f.fit(X,y)\nname_score=list(zip(data.drop(['Purchase'],axis=1).columns.tolist(),Selector_f.scores_))\nname_score_df=pd.DataFrame(data=name_score,columns=['Feat_names','F_scores'])\nname_score_df.sort_values('F_scores',ascending=False)\n","60b7c036":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\n\ndata=df.copy()\ndata=data[['City_Category','Gender','Purchase', 'Product_Category_1','Product_Category_3']]\n\ndata=pd.get_dummies(data=data,columns=['City_Category','Gender', 'Product_Category_1', 'Product_Category_3'])\n\ndata.drop(['City_Category_A','Gender_F','Product_Category_1_1', 'Product_Category_3_0'],axis=1,inplace=True)\n\nX=data.drop(['Purchase'],axis=1).values\ny=data['Purchase'].values\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=0)\n\nsc_X=StandardScaler()\nX_train=sc_X.fit_transform(X_train)\nX_test=sc_X.transform(X_test) \n\nregressor=LinearRegression()\nregressor.fit(X_train,y_train)\n\ny_pred=regressor.predict(X_test)\nprint(\"Prediction\\n\",y_pred)\nprint(\"Actual\\n\",y_test)\nprint(\"R_squared Score:\",regressor.score(X_test,y_test))\nmae = mean_absolute_error(y_test,y_pred)\nprint(\"MAE:\",mae)\nprint(\"RMSE:\",mean_squared_error(y_test,y_pred)**0.5)","38d0e3fb":"# Black Friday Purchasing Trends"}}