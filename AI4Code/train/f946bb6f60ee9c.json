{"cell_type":{"b2938b42":"code","fa6672a3":"code","f1183739":"code","a2c6bf4a":"code","4957a706":"code","9c164fdd":"code","1d0fda8b":"code","a71f3531":"code","17783c1e":"code","463490e6":"markdown"},"source":{"b2938b42":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import *\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom sklearn.model_selection import train_test_split\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","fa6672a3":"df = pd.read_csv('..\/input\/onion-or-not\/OnionOrNot.csv')","f1183739":"df.head()","a2c6bf4a":"# Here we tokenize each character (rather than each word).\ntokenize = Tokenizer(char_level=True)\ntokenize.fit_on_texts(df.text)","4957a706":"X = pad_sequences(tokenize.texts_to_sequences(df.text), maxlen=250, padding=\"post\")\nY = df.label","9c164fdd":"\nmodel = Sequential([\n                   Embedding(len(tokenize.word_index) + 1, 64),\n                   Conv1D(64, 5, activation=\"relu\"),\n                   Conv1D(64, 5, activation=\"relu\"),\n                   GlobalMaxPooling1D(),\n                   Dense(64, activation=\"relu\"),\n                   Dropout(.25),\n                   Dense(16, activation=\"relu\"),\n                   Dropout(.25),\n                   Dense(2, activation=\"softmax\"),\n])","1d0fda8b":"model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"])","a71f3531":"# split data into training and test sets\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=.1, random_state=5)","17783c1e":"history = model.fit(x_train, y_train, validation_data=([x_test, y_test]), epochs=5, verbose=1)","463490e6":"After reviewing a few of the other onion-or-not kernels, I figured I'd try a character based approach. Instead of cleaning the text and tokenizing each word, I tried tokenizing each character, then fed the integer sequences to a CNN to see what results I got. I consistently achieved over 80% accuracy, which is not bad for such a simple approach."}}