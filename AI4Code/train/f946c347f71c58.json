{"cell_type":{"bb4539d4":"code","3a38f164":"code","9a8d4c54":"code","dde33250":"code","11846ab0":"code","ffd5143d":"code","b40c13b8":"code","49152092":"code","72a81f57":"code","7a111196":"code","60243735":"code","246d6fb7":"code","891d519c":"code","592446b2":"code","b46720dc":"code","0f99cde6":"code","a300b172":"code","72019ea7":"code","bba0107d":"code","88e74661":"code","f8afc401":"code","ca35bb56":"code","a4915555":"code","8fb9f8af":"code","efe5a60c":"code","5e60e256":"code","7fa698bc":"code","50aae9ab":"code","f1dec921":"code","35450b7e":"code","3fe0e145":"code","3a2a6076":"code","4560cca6":"code","ecf1cb4d":"code","7b4a40e3":"code","8ee36fa6":"code","a946b6a4":"code","4d828c48":"code","06dc5325":"code","9c2e9c4d":"code","eec0cc21":"code","0e60ed3b":"code","107b43df":"code","1393b73e":"code","1b6b7d2c":"code","9baddfd6":"code","54a975f7":"code","bf7c1764":"code","0832a302":"code","bec96d21":"code","d253dccc":"code","62af114c":"code","23ef3a01":"code","2092e271":"code","3a72c9a6":"code","af81045f":"code","65eb8958":"code","5e5fce6d":"code","48499511":"markdown","256bbc53":"markdown","d954868d":"markdown","a68ff111":"markdown","feda40c0":"markdown","cc923580":"markdown","8863f3d7":"markdown","67fdd106":"markdown","feacbb09":"markdown","58543c48":"markdown","908cdd5e":"markdown","3bbf2347":"markdown","e1f84352":"markdown","9e217639":"markdown","18c98fe1":"markdown","363fc3e3":"markdown","5f621ab0":"markdown"},"source":{"bb4539d4":"import pandas as pd\nimport numpy as np\nimport re\nimport nltk\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom wordcloud import WordCloud\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport tensorflow as tf\n\nfrom sklearn.metrics import classification_report,confusion_matrix","3a38f164":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","9a8d4c54":"train.head()","dde33250":"train.info()","11846ab0":"train[['id', 'keyword', 'location', 'text']].isnull().sum()","ffd5143d":"train['text'].describe()","b40c13b8":"import cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)","49152092":"train[\"target\"].value_counts().iplot(kind='bar',text=['Fake', 'Real'], title='Comparando os tweets se s\u00e3o sobre desastres (1) ou n\u00e3o (0)',color=['blue'])","72a81f57":"plt.figure(figsize= (10,7))\nsns.countplot(y= train['keyword'], order=train['keyword'].value_counts().iloc[:10].index)\nplt.title('As 10 palavra-chaves mais usadas no dataset de treino')\nplt.show()","7a111196":"plt.figure(figsize= (10,7))\nsns.countplot(y= train['location'], order=train['location'].value_counts().iloc[:10].index)\nplt.title('Os 10 lugares mais frequentes onde foram publicados os tweets')\nplt.show()","60243735":"train = train.drop(['keyword', 'location', 'id'], axis=1)\n\n#test\ntest = test.drop(['keyword', 'location', 'id'], axis=1)","246d6fb7":"def RemoveStopWords(text):\n    stopwords = set(nltk.corpus.stopwords.words('english'))\n    words = [i for i in text.split() if not i in stopwords]\n    return (\" \".join(words))","891d519c":"train['text'] = [RemoveStopWords(i) for i in train['text']]\n\n# test\ntest['text'] = [RemoveStopWords(i) for i in test['text']]\n\ntrain['text'][:10]","592446b2":"def Remove_carac(text):\n    text = text.str.lower() \n    text = text.str.replace(r\"\\#\",\"\") \n    text = text.str.replace(r\"http\\S+\",\"\")  \n    text = text.str.replace(r\"@\",\"\")\n    text = text.str.replace(r\"[^a-zA-Z#]\", \" \")\n    text = text.str.replace(\"\\s{2,}\", \"\")\n    return text","b46720dc":"train['clean_text'] = Remove_carac(train['text'])\n\n#test\ntest['clean_text'] = Remove_carac(test['text'])\n\ntrain['clean_text'][:10]","0f99cde6":"lem = WordNetLemmatizer()\n\ndef Lemmatization(text):\n    words = []\n    for w in text.split():\n        words.append(lem.lemmatize(w))\n    return (\" \".join(words))","a300b172":"train['clean_text'] = [Lemmatization(f) for f in train['clean_text']]\n\n#test\ntest['clean_text'] = [Lemmatization(f) for f in test['clean_text']]\n\ntrain['clean_text'][:10]","72019ea7":"def Stemming(text):\n    stemmer = SnowballStemmer(language='english')\n    words = []\n    for w in text.split():\n        words.append(stemmer.stem(w))\n    return (\" \".join(words))","bba0107d":"train['clean_text'] = [Stemming(t) for t in train['clean_text']]\n\n#test\ntest['clean_text'] = [Stemming(t) for t in test['clean_text']]\n\ntrain['clean_text'][:10]","88e74661":"# excluindo a coluna text e reorganizando as colunas\ntrain = train.drop(['text'], axis=1)\ntrain = train.reindex(columns=['clean_text', 'target'])\n\n#test\ntest = test.drop(['text'], axis=1)\ntest = test.reindex(columns=['clean_text', 'target'])\n\ntrain.head()","f8afc401":"# unindo treino e teste\ndf = pd.concat([train,test], ignore_index=True)","ca35bb56":"# excluindo palavras com menos de 3 letras para melhor entendimento do gr\u00e1fico\ndf['clean_text'] = df['clean_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\ndf.head()","a4915555":"# usando o WordCloud para visualizar as palavras que aparecem com frequencia\nwords = ' '.join([text for text in df['clean_text']])\n\nwordcloud = WordCloud(width=800, height = 500, max_font_size=110).generate(words)\n\n# plotando\nplt.figure(figsize=(10,7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","8fb9f8af":"from collections import Counter\n\n# quantas palavras unicas tem no texto\ndef counter_word (text):\n    count = Counter()\n    for i in text.values:\n        for word in i.split():\n            count[word] += 1\n    return count","efe5a60c":"text_values = train[\"clean_text\"]\n\ncounter = counter_word(text_values)","5e60e256":"print(f\"The len of unique words is: {len(counter)}\")\nlist(counter.items())[:10]","7fa698bc":"# o n\u00famero m\u00e1ximo de palavras usadas\n\nvocab_size = len(counter)\nembedding_dim = 32\nmax_length = 20\ntrunc_type='post'\npadding_type='post'\noov_tok = \"<XXX>\"\ntraining_size = 3807\nseq_len = 12","50aae9ab":"X = train['clean_text']\ny = train['target']","f1dec921":"from sklearn.model_selection import train_test_split\nnp.random.seed(42)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.5, random_state=42)","35450b7e":"from sklearn.model_selection import RepeatedKFold\nkf = RepeatedKFold(n_splits= 2, n_repeats=10, random_state=42)\n\nfor train_lines, valid_lines in kf.split(X):\n    X_train, X_valid = X.iloc[train_lines], X.iloc[valid_lines]\n    y_train, y_valid = y.iloc[train_lines], y.iloc[valid_lines]","3fe0e145":"print('The Shape of training ',X_train.shape)\nprint('The Shape of testing',X_valid.shape)","3a2a6076":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","4560cca6":"oov_tok = \"<XXX>\"\n\ntokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(X_train)","ecf1cb4d":"word_index = tokenizer.word_index","7b4a40e3":"training_sequences = tokenizer.texts_to_sequences(X_train)\ntraining_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)","8ee36fa6":"print(train.clean_text[1])\nprint(training_sequences[1])","a946b6a4":"reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])","4d828c48":"# os primeiros 10 elementos\nprint(\"Os primeiros 10 elementos do \u00edndice de palavras: \")\nfor x in list(reverse_word_index)[0:15]:\n    print (\" {},  {} \".format(x,  reverse_word_index[x]))\n","06dc5325":"def decode(text):\n    return ' '.join([reverse_word_index.get(i, '?') for i in text])","9c2e9c4d":"decode(training_sequences[1]) # this can be usefull for check predictions","eec0cc21":"training_padded[1628]","0e60ed3b":"valid_sequences = tokenizer.texts_to_sequences(X_valid)\nvalid_padded = pad_sequences(valid_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)","107b43df":"model = Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n    tf.keras.layers.Dense(units = 14, activation = 'relu'),\n    tf.keras.layers.Dense(units = 1, activation = 'sigmoid')\n])\n\nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","1393b73e":"model.summary()","1b6b7d2c":"from tensorflow.keras.utils import plot_model\nplot_model(model, to_file='model_plot4a.png', show_shapes=True, show_layer_names=True)","9baddfd6":"import time\nstart_time = time.time()\n\nnum_epochs = 10\nhistory = model.fit(training_padded, y_train, epochs=num_epochs, validation_data=(valid_padded, y_valid))\n\nfinal_time = (time.time()- start_time)\/60\nprint(f'The time in minutos: {final_time}')","54a975f7":"model_loss = pd.DataFrame(model.history.history)\nmodel_loss.head()","bf7c1764":"# verificando no gr\u00e1fico o desempenho do modelo\nmodel_loss[['accuracy','val_accuracy']].plot(ylim=[0,1]);","0832a302":"#predictions = model.predict_classes(valid_padded)   # predict_ clases because is classification problem with the split test\npredictions = np.argmax(model.predict(valid_padded), axis=-1)","bec96d21":"predictions","d253dccc":"# matriz de confus\u00e3o\ndef plot_cm(y_true, y_pred, title, figsize=(5,4)):\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm \/ cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Atual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)","62af114c":"# plotando o gr\u00e1fico\nplot_cm(y_valid, predictions, 'Matriz de confus\u00e3o dos tweets', figsize=(7,7))","23ef3a01":"testing_sequences1 = tokenizer.texts_to_sequences(test.clean_text)\ntesting_padded1 = pad_sequences(testing_sequences1, maxlen=max_length, padding=padding_type, truncating=trunc_type)","2092e271":"predictions = model.predict(testing_padded1)","3a72c9a6":"submission =  pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","af81045f":"submission.head()","65eb8958":"submission['target'] = (predictions > 0.5).astype(int)\nsubmission","5e5fce6d":"submission.to_csv('submission.csv', index=False, header=True)","48499511":"## Lemmatization","256bbc53":"## Dando uma olhada nas palavras mais frequentes com WordCloud","d954868d":"## Excluindo as colunas que n\u00e3o vamos trabalhar","a68ff111":"## An\u00e1lise explorat\u00f3ria dos dados","feda40c0":"## Removendo os caracteres especiais","cc923580":"## Train test split","8863f3d7":"## Tokenization","67fdd106":"## Validando o modelo","feacbb09":"# Submiss\u00e3o","58543c48":"# Conjunto de dados do test","908cdd5e":"## Treinando o modelo","3bbf2347":"# Visualizando alguns gr\u00e1ficos","e1f84352":"## Matriz de confus\u00e3o","9e217639":"## Removendo as Stopwords","18c98fe1":"## Stemming","363fc3e3":"# Construindo a rede neural","5f621ab0":"# Pr\u00e9-processamento dos dados"}}