{"cell_type":{"fbe0e740":"code","d82d95ae":"code","d4812711":"code","1c8ad155":"code","76314dd0":"code","3aa30f0b":"code","bed3051b":"code","2c3ad63f":"code","ec535e68":"code","7543df1c":"code","fb5be782":"code","4ec37a26":"code","b4947226":"code","e64b3969":"code","9d9e259c":"code","1b94d91f":"code","14842c99":"code","92266067":"code","b27f5c6b":"code","ec692bf6":"code","3ecfe0ea":"code","049e1739":"code","b9114f7f":"code","7f986a3c":"code","3fbc1611":"code","2f8b2ecc":"code","5d28361e":"code","6ab49bb2":"code","2e3cdccd":"markdown","76a80c42":"markdown","8741c9c5":"markdown","0480dbc0":"markdown","0e0c80dc":"markdown","17b10059":"markdown","d72b5b94":"markdown","026427ac":"markdown","5b5516e9":"markdown","cd0779b2":"markdown","c2176459":"markdown","eabea85e":"markdown","c1eaa853":"markdown","a095398f":"markdown","e39d5655":"markdown","57104030":"markdown","9a0a33ed":"markdown"},"source":{"fbe0e740":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","d82d95ae":"import pandas as pd\nimport matplotlib.pyplot as plt, matplotlib.image as mpimg\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm\n#%matplotlib inline\nprint('Import complete.')","d4812711":"# load the data\nlabeled_images = pd.read_csv('..\/input\/train.csv')\nimages = labeled_images.iloc[0:5000,1:] #return a DataFrame consisted of rows indexed in 0:5000, and columns indexed from index 1.\nlabels = labeled_images.iloc[0:5000,:1] #return a DataFrame consisted of rows indexed in 0:5000, and columns indexed from beginning to index 1\ntrain_images, test_images,train_labels, test_labels = train_test_split(images, labels, test_size=0.2, random_state=0)","1c8ad155":"# Data check\nprint(labeled_images.describe())\nprint(images.describe())\nprint(labels.describe())","76314dd0":"i=1\nimg=train_images.iloc[i].as_matrix()\nimg=img.reshape((28,28))\nplt.imshow(img,cmap='gray')\nplt.title(train_labels.iloc[i,0])","3aa30f0b":"number = np.unique(train_labels) # np.unique is for returning the unique element of given array\nprint (number)\n\n\nindex = [0,0,0,0,0,0,0,0,0,0]    # declare temporary index for searching the result in train_labels, \n                                 # this index will eventualy be replace with the number location in every class from 0-9\n\n# In order to find said Index, we use loops in every row in train_labels \n\nfor i in range (len(train_labels)):\n    label = train_labels.iloc[i].label # label variable contain number class, in order to find the index of i\n    index[label] = i                   # (iloc[i] serach the row of i, and .label search the coloumn of label)\n    \nfor i in index:     # Now we print the image for every column of number\n    plt.figure()\n    img=train_images.iloc[i].values\n    img=img.reshape((28,28))\n    plt.imshow(img,cmap='gray')\n    plt.title(train_labels.iloc[i,0])\n","bed3051b":"#train_images.iloc[i].describe()\n#print(type(train_images.iloc[i]))\nplt.hist(train_images.iloc[i])","2c3ad63f":"labelcount = train_labels[\"label\"]\nprint(labelcount.value_counts())","ec535e68":"# create histogram for each class (data merged per class)\n# Todo\n#print(train_labels.iloc[:5])\n#data1 = train_images.iloc[1]\n#data2 = train_images.iloc[3]\n#data1 = np.array(data1)\n#data2 = np.array(data2)\n#data3 = np.append(data1,data2)\n#print(len(data3))\n#plt.hist(data3)\n\nlabel = [[],[],[],[],[],[],[],[],[],[]]\nfor j in range(10):\n    for i in range(len(train_images)):\n        if (train_labels.iloc[i].label == j):\n            data = train_images.iloc[i]\n            data = np.array(data)\n            label[j] = np.append(label[j],data)\n            \n    plt.figure(j)\n    plt.hist(label[j])\n    plt.title(j)","7543df1c":"clf = svm.SVC() # Define model\nclf.fit(train_images, train_labels.values.ravel()) # Fit: Capture patterns from provided data.\nclf.score(test_images,test_labels) # Determine how accurate the model's","fb5be782":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\n\ntree = DecisionTreeRegressor(random_state=0)\ntree.fit(train_images, train_labels)\ntest_predict = tree.predict(test_images)\nprint(mean_absolute_error(test_labels, test_predict))","4ec37a26":"print(train_labels.values.ravel())\nprint(np.unique(test_labels)) # to see class number\n\nfor y in index:\n    plt.figure(y)\n    img=train_images.iloc[y].values\n    img=img.reshape((28,28))\n    plt.imshow(img,cmap='gray')\n    plt.title(train_labels.iloc[y,0])","b4947226":"test_images[test_images>0]=1\ntrain_images[train_images>0]=1\n\nimg=train_images.iloc[i].values.reshape((28,28))\nplt.imshow(img,cmap='binary')\nplt.title(train_labels.iloc[i])","e64b3969":"plt.hist(train_images.iloc[i])","9d9e259c":"clf = svm.SVC()\nclf.fit(train_images, train_labels.values.ravel())\nclf.score(test_images,test_labels)","1b94d91f":"# Test again to data test\ntest_data=pd.read_csv('..\/input\/test.csv')\ntest_data[test_data>0]=1\nresults=clf.predict(test_data[0:5000])","14842c99":"# separate code section to view the results\nprint(results)\nprint(len(results))","92266067":"df = pd.DataFrame(results)\ndf.index.name='ImageId'\ndf.index+=1\ndf.columns=['Label']\ndf.to_csv('results.csv', header=True)","b27f5c6b":"#check if the file created successfully\nprint(os.listdir(\".\"))","ec692bf6":"# from https:\/\/www.kaggle.com\/rtatman\/download-a-csv-file-from-a-kernel\n\n# import the modules we'll need\nfrom IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\ncreate_download_link(df)","3ecfe0ea":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\n\n# Set the parameters by cross-validation\nparameters = {'gamma': [0.01, 0.001, 0.0001],'C': [1, 10, 100,1000]}\n\n# Create a classifier object with the classifier and parameter candidates\nclf = GridSearchCV(estimator=svm.SVC(), param_grid = parameters)\nclf.fit(train_images,train_labels.values.ravel())","049e1739":"print('Best C:',clf.best_estimator_.C) \nprint('Best Gamma:',clf.best_estimator_.gamma)","b9114f7f":"#final svm\nbest_c = 10\nbest_gamma = 0.01\nclf_final = svm.SVC(C=best_c,gamma=best_gamma)\nclf_final.fit(train_images, train_labels.values.ravel())\nfinalsvm = clf_final.score(test_images,test_labels)\nprint(clf_final.score(test_images,test_labels))","7f986a3c":"from sklearn.tree import DecisionTreeClassifier\ndef get_mae_train_classifie(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = DecisionTreeClassifier(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(train_X, train_y)\n    preds_train = model.predict(train_X)\n    mae = mean_absolute_error(train_y, preds_train)\n    return(mae)\n\ndef get_mae_test_classifie(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = DecisionTreeClassifier(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)\n\n\nmaee_train=[]\nmaee_test= []\nleaf_nodes=[5,25,50,70,100,300,500,1000,3000,5000,7000]\nfor max_leaf_nodes in leaf_nodes:\n    my_maetrain = get_mae_train_classifie(max_leaf_nodes, train_images, test_images, train_labels, test_labels)\n    my_maetest = get_mae_test_classifie(max_leaf_nodes, train_images, test_images, train_labels, test_labels)\n    maee_train.append(my_maetrain)\n    maee_test.append(my_maetest)\n\nplt.figure()\nplt.plot(leaf_nodes,maee_test,color=\"red\",label='Validation')\nplt.plot(leaf_nodes,maee_train,color=\"blue\",label='Training')\nplt.xlabel(\"Tree Depth\")\nplt.ylabel(\"MAE\")\nplt.title(\"Decision Tree Classifier\")\nplt.legend()\nplt.show()\n\nprint (maee_train)\nprint (maee_test)","3fbc1611":"#final model dtclassifier\n\nbest_tree_size=1000\ntreeclassifie = DecisionTreeClassifier(max_leaf_nodes=best_tree_size, random_state=0)\ntreeclassifie.fit(train_images,train_labels)\nscoredtc = treeclassifie.score(test_images,test_labels)\nprint (\"DTC = \",scoredtc)\nprint (\"SVM = \",finalsvm)","2f8b2ecc":"# Decision Tree Regressor\ndef get_mae_train_regress(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes)\n    model.fit(train_X, train_y)\n    preds_train = model.predict(train_X)\n    mae = mean_absolute_error(train_y, preds_train)\n    return(mae)\n\ndef get_mae_test_regress(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)\n\nmaee_train=[]\nmaee_test= []\nleaf_nodes=[5,25,50,70,100,300,500,1000,3000,5000,7000]\nfor max_leaf_nodes in leaf_nodes:\n    my_maetrain = get_mae_train_regress(max_leaf_nodes, train_images, test_images, train_labels, test_labels)\n    my_maetest = get_mae_test_regress(max_leaf_nodes, train_images, test_images, train_labels, test_labels)\n    maee_train.append(my_maetrain)\n    maee_test.append(my_maetest)\n\nplt.figure()\nplt.plot(leaf_nodes,maee_test,color=\"red\",label='Validation')\nplt.plot(leaf_nodes,maee_train,color=\"blue\",label='Training')\nplt.xlabel(\"Tree Depth\")\nplt.ylabel(\"MAE\")\nplt.title(\"Decision Tree Regression\")\nplt.legend()\nplt.show()\n\nprint (maee_train)\nprint (maee_test)","5d28361e":"#final model dtregressor\nbest_tree_size=1000\ntreeregres = DecisionTreeRegressor(max_leaf_nodes=best_tree_size, random_state=0)\ntreeregres.fit(train_images,train_labels)\nscoredtr = treeregres.score(test_images,test_labels)\nprint (\"DTR = \",scoredtr)\nprint (\"DTC = \",scoredtc)\nprint (\"SVM = \",finalsvm)","6ab49bb2":"# Decision Tree Classifier\n# labeled_images.iloc[0:5000,1:], yang terseleksi kedalam variabel images baris ke-0 sampai 4999 dan kolom ke-1 sampai kolom terakhir\nimages = labeled_images.iloc[0:5000,1:]\n# labeled_images.iloc[0:5000,:1], yang terseleksi kedalam variabel labels baris ke-0 sampai 4999 dan kolom ke-0\nlabels = labeled_images.iloc[0:5000,:1]\ntrain_images, test_images,train_labels, test_labels = train_test_split(images, labels, test_size=0.2, random_state=0)\nbest_tree_size=1000\ntree = DecisionTreeClassifier(max_leaf_nodes=best_tree_size,random_state=0)\ntree.fit(train_images, train_labels)\ntest_predict = tree.predict(test_images)\n#print(mean_absolute_error(test_labels, test_predict))\nscoredtr_ver2 = tree.score(test_images,test_labels)\nprint (\"DTC = \",scoredtr_ver2)","2e3cdccd":"**Labelling the test data**\n\nNow for those making competition submissions, we can load and predict the unlabeled data from test.csv. Again, for time we're just using the first 5000 images. We then output this data to a results.csv for competition submission.","76a80c42":"# **Question 3**\n\nCan you check in what class does this histogram represent?. How many class are there in total for this digit data?. How about the histogram for other classes\n\n**Q3 Answer **\n\nTL;DR  basically it represent the amount of number\/coloured pixel( represented in value of 0-255) in the array. with the total class of 10 number(0-9)\n\nThe histogram contain information about the classes of each pixel values. The \"X\" direction represent the frequency, and \"Y\" represent for  each class from one of the sample number 6 class as the 2nd data in train data. With that information and the information that the pixel value is grayscale betwwen black (0) and white (255), we can determine tahat there are not only a lot of black blank spaces but also small portion of white\/gray-ish pixels  ( with 0< value < 255) which is important for learning the stroke of handwritten digit numbers in order to determine the result of our data.","8741c9c5":"Now plot the histogram within img\n\nwe used plt.hist() for generate the histogram for the amount (frequency) of number occurence on the index of i \n\n\n**STEP 4: Examining the Pixel Values**\n\nNote that these images aren't actually black and white (0,1). They are gray-scale (0-255).\nA histogram of this image's pixel values shows the range.","0480dbc0":"**Retraining our model**\n\nWe follow the same procedure as before, but now our training and test sets are black and white instead of gray-scale. \nOur score still isn't great, but it's a huge improvement","0e0c80dc":"# **Question 1**\n\nNotice in the above we used _images.iloc?, can you confirm on the documentation? what is the role?\n\n**Q1 Answer**\n\nAccording to the [Documentation](http:\/\/https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.iloc.html) . \nBasically .iloc is an index based location selector, it's used to select parts of dataframe. .iloc will raise IndexError if a requested indexer is out-of-bounds, except slice indexers which allow out-of-bounds indexing (this conforms with python\/numpy slice semantics). .iloc[] is primarily integer position based (from 0 to length-1 of the axis), but may also be used with a boolean array.\n\nWhere in this case;\n1. > images = labeled_images.iloc[0:5000,1:]\n \n image will be a DataFrame consisted of rows indexed in 0:5000, and columns indexed from index 1.\n  \n2. > labels = labeled_images.iloc[0:5000,:1] \n \nAnd label a DataFrame consisted of rows indexed in 0:5000, and columns indexed from beginning to index 1\n","17b10059":"**STEP 1 : Lybrary Import**","d72b5b94":"# **Question 2**\n\nNow plot an image for each image class\n\n**Q2 Answer**\n\nto plot an image for each of class in every digit number (0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 and 9).","026427ac":"# **Data Download**\n\nWe have the file, can listed it but how we are take it from sever. Thus we also need to code the download link.","5b5516e9":"**STEP 2: Loading the data**\n\ninputting some known test data.\n\n**For the sake of time, we're only using 5000 images. You should increase or decrease this number to see how it affects model training.**","cd0779b2":"**Improving Performance**\n\nDid you noticed, that the performance is so miniscule in range of ~0.1. Before doing any improvement, we need to analyze what are causes of the problem?. But allow me to reveal one such factor. It was due to pixel length in [0, 255]. Let's see if we capped it into [0,1] how the performance are going to improved.","c2176459":"**STEP 5: Training our model**\n\nFirst, we use the sklearn.svm module to create a vector classifier.\nNext, we pass our training images and labels to the classifier's fit method, which trains our model.\nFinally, the test images and labels are passed to the score method to see how well we trained our model. Fit will return a float between 0-1 indicating our accuracy on the test data set\nTry playing with the parameters of svm.SVC to see how the results change.","eabea85e":"# **Question 4**\n\nIn above, did you see score() function?, open SVM.score() documentation at SKLearn, what does it's role?. Does it the same as MAE discussed in class previously?. Ascertain it through running the MAE. Now does score() and mae() produce the same results?.\n\n**Q4 Answer**\n\n1. .score() function will determine how accurate the model's given from the test images, with the result of 0.887, (score with value closer to 1 is better)\n2. .mean_absolute_error() function will determine the average error from the test labels and prediction from test images, with the result of 0,782. (MAE with value closer to 0 is better)","c1eaa853":"# **Question 5**\n\nBased on this finding, Can you explain why if the value is capped into [0,1] it improve significantly?. Perharps you need to do several self designed test to see why.\n\n**Q5 Answer**\nDetermine the capped value to [0,1], will change the performance because it reduce the probability from 255 into 2. As we seen before we process the data, it contain variety of shade and colour (white,gray,black). And thus the capped value of [0,1] will resulting the picture only have black or white colour.","a095398f":"Nama : Fachry Muhammad  |\nNRM   : 1313617019  |\nProdi   : Ilmu Komputer  | Lecture3 Digit Recoqnition Experiment\n\n\n\n# **DISCLAIMER**\nThe Dataset is not mine, and mostly copied from this [tutorial](http:\/\/https:\/\/www.kaggle.com\/archaeocharlie\/a-beginner-s-approach-to-classification) for my reference. I intended to do modification later to the tutorial, so please permit me for using it. The purpose of this note book is to learn and ML experiment of Digit Recognition on Kaggle.com. \n\n\n**Welcome to my first Kernel on Kaggle.com**\n\nThis kernel is an experiment to learn how to run ML experiment of Digit Recognition on Kaggle.com. ","e39d5655":"**STEP 3: Viewing an Image**\nSince the image is currently one-dimension, we load it into a numpy array and reshape it so that it is two-dimensional (28x28 pixels)\nThen, we plot the image and label with matplotlib\n**You can change the value of variable i to check out other images and labels.**","57104030":"**STEP 6: How did our model do?**\n\nYou should have gotten around 0.10, or 10% accuracy. This is terrible. 10% accuracy is what get if you randomly guess a number. There are many ways to improve this, including not using a vector classifier, but here's a simple one to start. Let's just simplify our images by making them true black and white.\nTo make this easy, any pixel with a value simply becomes 1 and everything else remains 0.\nWe'll plot the same image again to see how it looks now that it's black and white. Look at the histogram now.","9a0a33ed":"# **Question 6**\n\nAlhamdulillah, we have completed our experiment. Here's things to do for your next task:\n\n1. What are the overfitting factor of SVM algorithm?. Previously on decision tree regression, the factor was max_leaf nodes. Do similar expriment using SVM!\n2. Apply Decision Tree Classifier on this dataset, seek the best overfitting factor, then compare it with results of SVM.\n3. Apply Decision Tree Regressor on this dataset, seek the best overfitting factor, then compare it with results of SVM & Decision Tree Classifier. Provides the results in table\/chart. I suspect they are basically the same thing.\n4. Apply Decision Tree Classifier on the same dataset, use the best overfitting factor & value. But do not use unnormalized dataset, before the value normalized to [0,1]\n\n**Q6 answers**\n\nOverfitting factor of SVM algorithm is the kernel, gamma and C parameter.\nWhen modifying certain data parameter will resulted in different MAE value.\n\nIn order to get the optimal value of gamma and C parameter, we can use [grid search](http:\/\/). Grid search will allow us to seatch all the value needed for to determine the parameter needed for the estimator\n"}}