{"cell_type":{"a857cbce":"code","cf5f10cd":"code","6f98cfc9":"code","50c75fcf":"code","77eebc85":"code","6b5e5197":"code","c50e3f85":"code","f1c719c6":"code","9f65d337":"code","fce3430f":"code","4e230600":"code","a55a4e5a":"code","b1f5ae38":"code","39152566":"markdown","6ab60a9d":"markdown","c704e1af":"markdown","7aac779c":"markdown","3ae6df71":"markdown","52239d73":"markdown","abed67ec":"markdown","010e2304":"markdown","517268a0":"markdown","c54050c4":"markdown","e372d645":"markdown","b1ad6254":"markdown","9b290ff7":"markdown","9cf7f0a3":"markdown","d0457f0d":"markdown","f9c62d03":"markdown","9d2bfcac":"markdown","fa610ded":"markdown","eaeb0ee7":"markdown"},"source":{"a857cbce":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom lightfm import LightFM # LightFM Recommendation tool\nfrom sklearn.model_selection import train_test_split #to split the train and test data\nimport matplotlib.pyplot as plt\n\nfrom lightfm import evaluation\n\nfrom scipy.spatial.distance import correlation, cosine\nfrom sklearn.metrics import pairwise_distances\n\nfrom scipy import sparse #to convert dataframe to sparse matrix\nfrom lightfm import cross_validation\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","cf5f10cd":"movie_df=pd.read_csv(\"..\/input\/movies.csv\")\nratings_df=pd.read_csv(\"..\/input\/ratings.csv\")\nlinks_df=pd.read_csv(\"..\/input\/links.csv\")\ntag_df=pd.read_csv(\"..\/input\/tags.csv\")","6f98cfc9":"ratings_df=ratings_df.loc[ratings_df['rating']== 5]\nratings_df=ratings_df.drop(['timestamp'],axis=1)\nratings_df.loc[ratings_df.rating == 5.0 , 'rating'] = 1\nratings_df[['rating']] = ratings_df[['rating']].astype(int)\nratings_df","50c75fcf":"R_df=ratings_df.pivot(index='userId',columns='movieId',values='rating').fillna(0).astype(int)\ninteractions=sparse.csr_matrix(R_df.values)\ndata = 1-pairwise_distances(interactions, metric=\"cosine\")\nR_df.shape","77eebc85":"temp=[]\n\nfor row in R_df.iterrows():\n    index, data = row\n    temp.append(data.tolist())","6b5e5197":"train_df,test_df=cross_validation.random_train_test_split(interactions, test_percentage=0.2, random_state=None)","c50e3f85":"model_loss=[]\nprecision=[]\nrecall=[]\nauc=[]","f1c719c6":"# Instantiate and train the model using warp\nmodel = LightFM(loss='warp')\nmodel.fit(train_df, epochs=30, num_threads=2)\n\n# Testing the model using precision_at_k for the warp model\ntest_precision = evaluation.precision_at_k(model, test_df, k=5).mean()\nprecision.append(test_precision)\nprint(test_precision)\n\n# Testing the model using recall_at_K for the warp model\ntest_recall = evaluation.recall_at_k(model, test_df,k=5).mean()\nrecall.append(test_recall)\nprint(test_recall)\n\n# Testing the model using auc_score for the warp model\ntest_auc = evaluation.auc_score(model,test_df).mean()\nauc.append(test_auc)\nprint(test_auc)\n\n# Preparing data for visulaization\nmodel_loss.append(\"warp\")","9f65d337":"# Instantiate and train the model using bpr\nmodel = LightFM(loss='bpr')\nmodel.fit(train_df, epochs=30, num_threads=2)\n\n# Testing the model using precision_at_k for the bpr model\ntest_precision = evaluation.precision_at_k(model, test_df, k=5).mean()\nprecision.append(test_precision)\nprint(test_precision)\n\n# Testing the model using recall_at_K for the bpr model\ntest_recall = evaluation.recall_at_k(model, test_df,k=5).mean()\nrecall.append(test_recall)\nprint(test_recall)\n\n# Testing the model using auc_score for the bpr model\ntest_auc = evaluation.auc_score(model,test_df).mean()\nauc.append(test_auc)\nprint(test_auc)\n\n# Preparing data for visulaization\nmodel_loss.append(\"bpr\")\n\n","fce3430f":"# Instantiate and train the model using logistic\nmodel = LightFM(loss='logistic')\nmodel.fit(train_df, epochs=30, num_threads=2)\n\n# Testing the model using precision_at_k for the logistic model\ntest_precision = evaluation.precision_at_k(model, test_df, k=5).mean()\nprecision.append(test_precision)\nprint(test_precision)\n\n# Testing the model using recall_at_K for the logistic model\ntest_recall = evaluation.recall_at_k(model, test_df,k=5).mean()\nrecall.append(test_recall)\nprint(test_recall)\n\n# Testing the model using auc_score for the logistic model\ntest_auc = evaluation.auc_score(model,test_df).mean()\nauc.append(test_auc)\nprint(test_auc)\n\n# Preparing data for visulaization\nmodel_loss.append(\"logistic\")\n\n","4e230600":"# set width of bar\nbarWidth = 0.30\n \n \n# Set position of bar on X axis\nr1 = np.arange(len(precision))\nr2 = [x + barWidth for x in r1]\nr3 = [x + barWidth for x in r2]\n \n# Make the plot\nplt.bar(r1, precision, color='#7f6d5f', width=barWidth, edgecolor='white', label='precision')\nplt.bar(r2, recall, color='#557f2d', width=barWidth, edgecolor='white', label='recall')\nplt.bar(r3, auc, color='#2d7f5e', width=barWidth, edgecolor='white', label='auc')\n \n# Add xticks on the middle of the group bars\nplt.xlabel('Loss', fontweight='bold')\nplt.xticks([r + barWidth for r in range(len(precision))], model_loss)\n\n# Set figure width to 12 and height to 9\nfig_size[0] = 20\nfig_size[1] = 11\nplt.rcParams[\"figure.figsize\"] = fig_size\n \n# Create legend & Show graphic\nplt.legend()\nplt.show()\n\n","a55a4e5a":"from __future__ import print_function\nimport random, time\nfrom annoy import AnnoyIndex\n\ntry:\n    xrange\nexcept NameError:\n    # Python 3 compat\n    xrange = range\n\nn, f = 644, 3127\nprecision=[]\nmetrics=[\"angular\",\"euclidean\",\"manhattan\",\"hamming\"]\nfor m in metrics:\n    t = AnnoyIndex(f,metric=m)\n    for i in xrange(n):\n        t.add_item(i, temp[i])\n\n    t.build(2 * f)\n    t.save('test.tree')\n\n    limits = [10]\n    k = 10\n    prec_sum = {}\n    prec_n = 1000\n    time_sum = {}\n\n    for i in xrange(prec_n):\n        j = random.randrange(0, n)\n\n        closest = set(t.get_nns_by_item(j, k, n))\n        for limit in limits:\n            t0 = time.time()\n            toplist = t.get_nns_by_item(j, k, limit)\n            T = time.time() - t0\n\n            found = len(closest.intersection(toplist))\n            hitrate = 1.0 * found \/ k\n            prec_sum[limit] = prec_sum.get(limit, 0.0) + hitrate\n            time_sum[limit] = time_sum.get(limit, 0.0) + T\n\n    for limit in limits:\n        print('Using:',m)\n        prec=100.0 * prec_sum[limit] \/ (i + 1)\n        precision.append(prec)\n        print('limit: %-9d precision: %6.2f%% avg time: %.6fs'\n              % (limit, prec , time_sum[limit] \/ (i + 1)))\n","b1f5ae38":"#Visulaiztion\ny_pos = np.arange(len(metrics))\n \n# Create bars\nplt.bar(y_pos, precision)\n \n# Create names on the x-axis\nplt.xticks(y_pos, metrics, color='black')\nplt.yticks(color='black')\n \n# Show graphic\nplt.show()","39152566":"**USING LIGHTFM**\n\nLightFM is a Python implementation of a number of popular recommendation algorithms for both implicit and explicit feedback, including efficient implementation of BPR and WARP ranking losses. It's easy to use, fast (via multithreaded model estimation), and produces high quality results.\n\nIt also makes it possible to incorporate both item and user metadata into the traditional matrix factorization algorithms. It represents each user and item as the sum of the latent representations of their features, thus allowing recommendations to generalise to new items (via item features) and to new users (via user features).","6ab60a9d":"** Methods to build Recommendation System :**\n\n* **Content-Based Recommendation System:** Filter the keytopics from the document where user interested and interacted with and train the model with those keywords to provide the relevant document to the user. Eg: User watched \"The Olypus has Fallen\" & \"White House Down\" the interacted data will be the gener of the movie like action , rescue , president etc movie artists,director etc  so the recommendation system will be trained according to that it and it will suggests \"London Has Fallen\" ,\"Paris\" etc.\n![CB][2]\n[2]: https:\/\/image.slidesharecdn.com\/trustrecsys-130710205719-phpapp02\/95\/trust-and-recommender-systems-7-638.jpg?cb=1373490026\n\n* **Collabrative Filtering:** Identifying the similarity with user and items interactions and find the best similar user\/item for the target user , this similarity data act as the interaction dataset for the recommendation systems and some clustering , similarity algorithms it will be trained. Eg:  Find the chan rating by caluclating the simalrity of the chan neighbouring user.\n![CF][1]\n[1]: http:\/\/4.bp.blogspot.com\/-D6ei8i-APx8\/Vn4e8nr9QeI\/AAAAAAAAAZc\/2vOjvjBKnRQ\/s1600\/item_item_collaborative_filtering1.jpg\n\n* **Hybrid Recommendation System:** A system that combines content-based filtering and collaborative filtering could take advantage from both the representation of the content as well as the similarities among users. If we make the recommendation system more personalized to the user then the chances of purchasing the item will be high.\nIn this example the user and the item keyword interactions are collected ( Content Based ) and finding the simalirty of the item keytopics (Collabrative) . After training the model we found that Fago likes the movie indicated by \"+\" or dislike the movie \"-\" .\n![image.png](attachment:image.png)\n","c704e1af":"**NOTE:** There are other famous representation it depends on how we are going to use in the data in what model .Other representation are ** Linear Kernels,Deep Nets,Word2Vec( Basically used for document recommendation) , Autoencoders etc **","7aac779c":"**Creating Utility Matrix from the Dataset**","3ae6df71":"* **Item Features:**: It descibe the features of the item and its key values. Eg Car as the item and its features are model,company,engine,speed etc .","52239d73":" Before calculating the loss you should understand some terminology for evaluation metrics \n* **TRUE POSITIVE:** The items or products which are correctly recommended to the user by the system\n* **FALSE POSITIVE:** The items or products which are left by the system to recommend\n* **TRUE NEGATIVE:** The items or products which are not correctly recommended by the system\n* ** FALSE NEGATIVE:** The items or products which are wrongly recommended by the system \n![image.png](attachment:image.png)\n![5](https:\/\/cdn-images-1.medium.com\/max\/800\/1*CPnO_bcdbE8FXTejQiV2dg.png)   \n\nTo understand the tradeoff between precision and recall let us describe with a simple example: First we have to convert all the predicted values to a binary representation ( 1 he likes the recommendation or 0  not likes the recommendation) For that we have to choose a threshold value incase of movie rating we can choose 3.5 as threshold value\nif a movie rating > 3.5 ( Liked ) or movie rating  < 3.5 (disliked) this will convert all the interactions value to binary representations. Now set the k value as we said above to rank and k act as the top k values which can be used for learning. Eg : item_1 , item_5,item_10 which are top@3 relevant items to the user this is called as relevant items. Now our recommendation predicts the top@3 items for user this is called as recommended items.\n\n* **Precision:** Precision is the proportion of recommendations that are relevant in the recommended top@k. Formula to calculate the precision is: **No of relevant item recommeded \/ No of recommended items **. In terms of example : recommended items are item_10,item_1,item_6 there are 2 relevant items . The precision is 2\/3 = 66.67% .The accurate precision is 100%.\n\n* **Recall:** Recall is the proportion of good recommendations that appear in top recommendations . Formula to calculate the recall is : **No of relevant items recommended in top@k \/ Total No of relevant items** we know the top@3 relevant item for user and that is meant to be total no of relevant items.In term of example : item_10,item_1,item_6 are the recommended items and only two relevant items are from the total no of relevant items therefore 2\/3 : 66.67% .The accurate recall is 100%.\n\n\n           ","abed67ec":"**Importing the DataSet**","010e2304":"**SPLIT THE TRAIN AND TEST DATA**","517268a0":"**Framework for Recommendation Systems:** The working of recommendation systems is splitted into various layers as per in the below diagram . we will discuss all the layers in detail.\n![image.png](attachment:image.png)","c54050c4":"**Recommendation Systems:** \nIt is an automated system which can recommend relevant items to the user based on his previous interactions with the items.","e372d645":"* **Prediction:** By using the user and item profiles we compute the utility matrix . This matrix contains the the similarity value of the user with each item (the higher the similarity value the user most prefer that ) and its done by using content-based or collabrative filtering or hybrid methods . Many algorithms are invloved in calculating the similarity value and also  ranking the items based on the values in the recommendation methods.Let us discuss some majorly used algorithms for calulating simalrity values .\n  * **Cosine Similarity:** The cosine similarity function uses the difference in the direction that two articles go, i.e. the difference in angle between two article directions.Less the distance or angle the more similar they are.\n![3](https:\/\/lh4.googleusercontent.com\/SodVc3Xo77b8LhEjqXymSaA-bI-kQdPeY8uG-J0wSSp5q-pxVAf_rPMUX9Y)\n\n  * **Dot Product:** Calculating the similarity by taking dot product of the user and item vector and fitting linear to the model.\n  \n  * **Eucledian Distance:**  It is the distance between the target user and their similar user by means of their item properties and user properties . The distance is calculated by using the formula mentioned below. This used like a nearest neighbour concept to map the neighbours of the target.\n![4](https:\/\/chrisjmccormick.files.wordpress.com\/2013\/08\/2d_euclidean_distance_illustration.png)  \n\n\n  ","b1ad6254":"**Using Annoy**","9b290ff7":"* **Item Representation:** It represent the item feature values as sparse matrix because matrix format is easty to compute and also embedding with the other representation.\n![image.png](attachment:image.png)\n","9cf7f0a3":"*  **User Repersentation:** This is also one type of interactions called the user interactions . To represent it we use sparse matrix(the matrix which has very little filled value and the non-interacted place are left unfilled.\n![image.png](attachment:image.png)","d0457f0d":"* **User Features:** Describe the qualities and properties of the user invloved which he demand in that product . Eg User A may like movie with genre love , crush , romance these are the properties of the user and many other are there .","f9c62d03":"* ** Learning:** After mapping all the similarity values for the utility matrix we can directly evaluate the loss or rank the similar item for the user  like top@k ( Show the top k item for the user )  using ranking algorithm (Ranking can used to improve accuracy than directly using the similarity value ) . There are four major evaluation metrics to calculate the performance of the model in terms of prediction values ( Similarity Value ) ","9d2bfcac":"* **Interactions:** Denotes the item which the user interact with.To get the interaction data of the user we use two major methodoligies they are explicit feedback and implicit feedback .**Explicit Feedback** means getting the feedback of the user for the feedback by getting the user feedback by means of survey , rating column and reviews ( Some user can vounteerly involved for this process but the majority wont) This method is effective when large nuber of user interact with it.**Implicit Feedback ** is getting the user interactions by montioring the user behaviour in their domain or with item ex: number of clicks , threshold time , searching relevant content of that item ( Mostly this will be used as a primary dataset to train the recommendation systems ).\n\n\n","fa610ded":"**MODEL LOSS**\n* **WARP ( Weighted Approxiamte Rankwise Pair ) :**  ","eaeb0ee7":"Now we keep the threshold value for the user by their ratings and also drop the timestamp column "}}