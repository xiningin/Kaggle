{"cell_type":{"27d23ccf":"code","0b3e58da":"code","8b4acabf":"code","b1e7ca00":"code","0ea05d66":"code","3c167cfe":"code","f203206e":"code","67bcf559":"code","092bff3b":"code","df9637b9":"code","66718c44":"code","7d40e2d6":"code","e0652b4a":"code","5163ae4e":"code","e3ac4cf5":"code","a8abdc32":"code","69e5a17e":"code","080f2473":"code","82bae921":"code","f17aae01":"code","95e6a942":"code","04e4d059":"code","869d0d0b":"code","419fb8ac":"code","d6c05a42":"code","3d994a0c":"code","db403fc5":"code","b5f28138":"code","a5c5e20a":"code","af53aafa":"code","c59ecbeb":"markdown","1b00e30d":"markdown","3023777d":"markdown","479ecafa":"markdown","706c54a5":"markdown","f79ce2d3":"markdown","7d55c488":"markdown","6a589c81":"markdown","6ee9c774":"markdown","8291aa17":"markdown"},"source":{"27d23ccf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0b3e58da":"import pandas as pd, numpy as np, os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('max_columns',None)\n\nimport random\nimport math\nfrom scipy import stats\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import preprocessing, model_selection, metrics","8b4acabf":"dir = '..\/input\/tabular-playground-series-nov-2021\/'\nz = '.csv'\n\ntrain = pd.read_csv(dir+'train'+z)\ntest = pd.read_csv(dir+'test'+z)\n\nsample_submission = pd.read_csv(dir+'sample_submission'+z)","b1e7ca00":"train.head()","0ea05d66":"train.isnull().sum()","3c167cfe":"train.describe()","f203206e":"train.skew()","67bcf559":"X_main = train.drop(columns='target')\ny_main = train['target']","092bff3b":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.preprocessing import MinMaxScaler\nX_norm = MinMaxScaler().fit_transform(X_main)\nchi_selector = SelectKBest(chi2, k=50)\nchi_selector.fit(X_norm, y_main)\nchi_support = chi_selector.get_support()\nchi_feature = X_main.loc[:,chi_support].columns.tolist()\nprint(str(len(chi_feature)), 'selected features')\nchi_feature","df9637b9":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndata_rescaled = scaler.fit_transform(X_main)\npca = PCA(n_components = 0.90)\npca.fit(data_rescaled)\nreduced = pca.transform(data_rescaled)","66718c44":"reduced.shape","7d40e2d6":"plt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')","e0652b4a":"pca = PCA().fit(data_rescaled)\n\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (12,6)\n\nfig, ax = plt.subplots()\nxi = np.arange(1, 102, step=1)\ny = np.cumsum(pca.explained_variance_ratio_)\n\nplt.ylim(0.0,1.1)\nplt.plot(xi, y, marker='o', linestyle='--', color='b')\n\nplt.xlabel('Number of Components')\nplt.xticks(np.arange(0, 100, step=5)) #change from 0-based array index to 1-based human-readable label\nplt.ylabel('Cumulative variance (%)')\nplt.title('The number of components needed to explain variance')\n\nplt.axhline(y=0.90, color='r', linestyle='-')\nplt.text(0.5, 0.85, '90% cut-off threshold', color = 'red', fontsize=16)\n\nax.grid(axis='x')\nplt.show()","5163ae4e":"reduced[0]","e3ac4cf5":"from numpy import loadtxt\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score","a8abdc32":"from sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split \nx_, x_val_, y_, y_val_ = train_test_split(reduced, y_main, train_size = 0.85, random_state = 0)","69e5a17e":"\nalgorithm = LogisticRegression(penalty='l2', dual=False, tol=1e-4,\nC=1.0, fit_intercept=True,\nintercept_scaling=1, class_weight=None,\nrandom_state=None, solver='lbfgs',\nmax_iter=100, multi_class='auto',\nverbose=0, warm_start=False, n_jobs=None,\nl1_ratio=None)\nLogistic_Regression_Model = algorithm.fit(x_, y_)\n\ny_pred = Logistic_Regression_Model.predict(x_val_)\n\nprint('Logistic Regression AUC:', roc_auc_score(y_val_, y_pred))\n        ","080f2473":"x = train.iloc[:, :-1]\ny = train['target']\nx_test = test.iloc[:,:]\ny_test = sample_submission.iloc[:, :]","82bae921":"x_train = pd.get_dummies(x)\ny_train = pd.get_dummies(y)\nx_test = pd.get_dummies(x_test)\ny_test = pd.get_dummies(y_test)","f17aae01":"missing_val = x.isnull().sum()\nprint(missing_val[missing_val > 0])\nx, x_val, y, y_val = train_test_split(x, y, train_size = 0.85, random_state = 0)","95e6a942":"import keras \nfrom tensorflow.keras import layers \nfrom tensorflow.keras.callbacks import EarlyStopping \n\nmodel = keras.Sequential([ \n    layers.BatchNormalization(),\n    layers.Dense(2048, activation='sigmoid', input_shape=[101]),\n    layers.Dropout(rate = 0.3), \n    layers.Dense(2500, activation='sigmoid'),\n    layers.Dropout(rate = 0.3),\n    layers.BatchNormalization(),    \n    layers.Dense(2500, activation='sigmoid'), \n    layers.Dropout(rate = 0.3), \n    layers.BatchNormalization(),\n    layers.Dense(2048, activation='sigmoid'), \n    layers.Dropout(rate = 0.3), \n    layers.Dense(1, activation = 'sigmoid'), \n]) \n\nearly_stopping = EarlyStopping(min_delta = 0.001, \npatience = 10, restore_best_weights = True)","04e4d059":"model.compile( \n    optimizer='adam', \n    loss='binary_crossentropy', \n    metrics=['AUC'],\n    )","869d0d0b":"prediction = model.fit( \n    x, y, \n    validation_data=(x_val, y_val), \n    batch_size=256, \n    epochs=50, \n    callbacks = [early_stopping], \n    verbose = 2, \n)","419fb8ac":"y_pred_test = Logistic_Regression_Model.predict(reduced_test)\n","d6c05a42":"print(sample_submission['target'],y_pred_test)","3d994a0c":"y_final = model.predict(pd.get_dummies(test))\n","db403fc5":"y_final.shape","b5f28138":"\nsample_submission['target'] = y_final","a5c5e20a":"sample_submission.head()","af53aafa":"sample_submission.to_csv('submission.csv', index=False)","c59ecbeb":"**XGB algorithm**","1b00e30d":"**Logistic Algorithm**","3023777d":"Here We have splited out dataset into dependant and independant features","479ecafa":"**Skewness is present in data**","706c54a5":"**ANN**","f79ce2d3":"**Random Forest Algorithm**","7d55c488":"Here We are finding the chi square test for 50 features which gives us maximum variance.\n\nChi Square Test to narrow down to 50 features","6a589c81":"**Here I have Imported Packages and ignored Warnings**","6ee9c774":"**Here we have imported the PCA for dimentionality reduction**\n\n1.In PCA we are reducing independant features such that 90% of data is described by output of PCA or eigen vectors.\n\n2.In PCA we observed that 45 features contains 90% of data so we derrived PCA for 45 features as shown in graph","8291aa17":"**Here We observed that dataset contains 600000 rows and 102 columns**\n\nOut of these columns non of the column have null value"}}