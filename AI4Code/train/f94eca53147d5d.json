{"cell_type":{"5a724a30":"code","2e4f926b":"code","4a88ed22":"code","d1ae78a5":"code","d277c875":"code","b0b37060":"code","6aa6ca65":"code","7fbf5078":"code","0f465116":"code","33fd385b":"code","11c2a2ee":"markdown","3ada1374":"markdown","f43d3c09":"markdown"},"source":{"5a724a30":"import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use(\"fivethirtyeight\")\nsns.set_style('whitegrid')\n%matplotlib inline\n\n# Load the data\ndf = pd.read_csv('\/kaggle\/input\/csgo-round-winner-classification\/csgo_round_snapshots.csv')\n\n# Split X and y\ny = df.round_winner\nX = df.drop(['round_winner'], axis=1)\n\nprint(f\"Total number of samples: {len(X)}\")\n\nX.head()","2e4f926b":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import power_transform\n\ndef encode_targets(y):\n    encoder = LabelEncoder()\n    encoder.fit(y)\n    y_encoded = encoder.transform(y)\n    return y_encoded\n\ndef encode_inputs(X, object_cols):\n    ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n    X_encoded = pd.DataFrame(ohe.fit_transform(X[object_cols]))\n    X_encoded.columns = ohe.get_feature_names(object_cols)\n    X_encoded.index = X.index\n    return X_encoded\n\n# Use OH encoder to encode predictors\nobject_cols = ['map', 'bomb_planted']\nX_encoded = encode_inputs(X, object_cols)\nnumerical_X = X.drop(object_cols, axis=1)\nX = pd.concat([numerical_X, X_encoded], axis=1)\n\n# Use label encoder to encode targets\ny = encode_targets(y)","4a88ed22":"from sklearn.model_selection import train_test_split\nimport lightgbm as lgb\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)\nd_train = lgb.Dataset(X_train, label=y_train)\nd_test = lgb.Dataset(X_test, label=y_test)","d1ae78a5":"params = {\n    \"max_bin\": 512,\n    \"learning_rate\": 0.05,\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"binary\",\n    \"metric\": \"binary_logloss\",\n    \"num_leaves\": 10,\n    \"verbose\": -1,\n    \"min_data\": 100,\n    \"boost_from_average\": True\n}\n\nmodel = lgb.train(params, d_train, 100000, valid_sets=[d_test], early_stopping_rounds=50, verbose_eval=1000)","d277c875":"import shap\n\n# print the JS visualization code to the notebook\nshap.initjs()","b0b37060":"explainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X)","6aa6ca65":"shap.force_plot(explainer.expected_value[1], shap_values[1][0,:], X.iloc[0,:])","7fbf5078":"shap.force_plot(explainer.expected_value[1], shap_values[1][:1000,:], X.iloc[:1000,:])","0f465116":"shap.summary_plot(shap_values, X)","33fd385b":"for name in X_train.columns:\n    shap.dependence_plot(name, shap_values[1], X, display_features=X)","11c2a2ee":"# Train the model","3ada1374":"Task: Find the determining factors for winning rounds in CS:GO\nExecution time: 1.5 hours\n\nIn this notebook we use a LightGBM model to predict winners with (70% accuracy) and then try to determine what factors were most important. Jump to [Explain predictions](#Explain-predictions).\n\nIdeas\/References:\n* [Data Transformation](https:\/\/www.kaggle.com\/christianlillelund\/predict-winners-in-cs-go-with-keras-80)\n* [Feature Exploration](https:\/\/slundberg.github.io\/shap\/notebooks\/tree_explainer\/Census%20income%20classification%20with%20LightGBM.html)","f43d3c09":"# Explain predictions"}}