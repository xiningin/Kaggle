{"cell_type":{"c041b1df":"code","48eb5dd8":"code","61b7624c":"code","33bad388":"code","e7987ad6":"code","06292494":"code","cc8e67e4":"code","3f3ede08":"code","da0516b1":"code","f5abdfc9":"code","371a2600":"code","77c15ab1":"code","e1dc42d0":"code","a1541264":"code","773eef04":"code","7e735428":"code","104d4287":"code","b9d89f40":"code","4a8292cb":"markdown","c51b61bd":"markdown","27c32e6e":"markdown","b1745bec":"markdown","4e8e9a42":"markdown","0ee7194a":"markdown","bce712ea":"markdown","a4a371a4":"markdown","d8f34f78":"markdown","1a6b4969":"markdown","e13a1052":"markdown","4940628a":"markdown","35a30628":"markdown","0a335dce":"markdown","d90c5940":"markdown","7e1f3e18":"markdown","4260a0dd":"markdown","05f23d83":"markdown","fa12b697":"markdown","b11da287":"markdown","2b55fd0e":"markdown","5c293a3a":"markdown","08f319f5":"markdown","19adfe43":"markdown","3e934c38":"markdown","177f1e87":"markdown","b630c442":"markdown","1cfff651":"markdown","415f56d5":"markdown","eaeb5bce":"markdown","23078c23":"markdown","d35dc882":"markdown","e44469f4":"markdown","2b6465f8":"markdown"},"source":{"c041b1df":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt","48eb5dd8":"import lightgbm as lgbm\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold, GridSearchCV\nfrom sklearn import preprocessing\nfrom sklearn.metrics import r2_score\n\n\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ndata = pd.concat([train, test], sort=False)\ndata = data.reset_index(drop=True)\ndata.head()","61b7624c":"nans=pd.isnull(data).sum()\n\ndata['MSZoning']  = data['MSZoning'].fillna(data['MSZoning'].mode()[0])\ndata['Utilities'] = data['Utilities'].fillna(data['Utilities'].mode()[0])\ndata['Exterior1st'] = data['Exterior1st'].fillna(data['Exterior1st'].mode()[0])\ndata['Exterior2nd'] = data['Exterior2nd'].fillna(data['Exterior2nd'].mode()[0])\n\ndata[\"BsmtFinSF1\"]  = data[\"BsmtFinSF1\"].fillna(0)\ndata[\"BsmtFinSF2\"]  = data[\"BsmtFinSF2\"].fillna(0)\ndata[\"BsmtUnfSF\"]   = data[\"BsmtUnfSF\"].fillna(0)\ndata[\"TotalBsmtSF\"] = data[\"TotalBsmtSF\"].fillna(0)\ndata[\"BsmtFullBath\"] = data[\"BsmtFullBath\"].fillna(0)\ndata[\"BsmtHalfBath\"] = data[\"BsmtHalfBath\"].fillna(0)\ndata[\"BsmtQual\"] = data[\"BsmtQual\"].fillna(\"None\")\ndata[\"BsmtCond\"] = data[\"BsmtCond\"].fillna(\"None\")\ndata[\"BsmtExposure\"] = data[\"BsmtExposure\"].fillna(\"None\")\ndata[\"BsmtFinType1\"] = data[\"BsmtFinType1\"].fillna(\"None\")\ndata[\"BsmtFinType2\"] = data[\"BsmtFinType2\"].fillna(\"None\")\n\ndata['KitchenQual']  = data['KitchenQual'].fillna(data['KitchenQual'].mode()[0])\ndata[\"Functional\"]   = data[\"Functional\"].fillna(\"Typ\")\ndata[\"FireplaceQu\"]  = data[\"FireplaceQu\"].fillna(\"None\")\n\ndata[\"GarageType\"]   = data[\"GarageType\"].fillna(\"None\")\ndata[\"GarageYrBlt\"]  = data[\"GarageYrBlt\"].fillna(0)\ndata[\"GarageFinish\"] = data[\"GarageFinish\"].fillna(\"None\")\ndata[\"GarageCars\"] = data[\"GarageCars\"].fillna(0)\ndata[\"GarageArea\"] = data[\"GarageArea\"].fillna(0)\ndata[\"GarageQual\"] = data[\"GarageQual\"].fillna(\"None\")\ndata[\"GarageCond\"] = data[\"GarageCond\"].fillna(\"None\")\n\ndata[\"PoolQC\"] = data[\"PoolQC\"].fillna(\"None\")\ndata[\"Fence\"]  = data[\"Fence\"].fillna(\"None\")\ndata[\"MiscFeature\"] = data[\"MiscFeature\"].fillna(\"None\")\ndata['SaleType']    = data['SaleType'].fillna(data['SaleType'].mode()[0])\ndata['LotFrontage'].interpolate(method='linear',inplace=True)\ndata[\"Electrical\"]  = data.groupby(\"YearBuilt\")['Electrical'].transform(lambda x: x.fillna(x.mode()[0]))\ndata[\"Alley\"] = data[\"Alley\"].fillna(\"None\")\n\ndata[\"MasVnrType\"] = data[\"MasVnrType\"].fillna(\"None\")\ndata[\"MasVnrArea\"] = data[\"MasVnrArea\"].fillna(0)\nnans=pd.isnull(data).sum()\nnans[nans>0]","33bad388":"_list = []\nfor col in data.columns:\n    if type(data[col][0]) == type('str'): \n        _list.append(col)\n\nle = preprocessing.LabelEncoder()\nfor li in _list:\n    le.fit(list(set(data[li])))\n    data[li] = le.transform(data[li])\n\ntrain, test = data[:len(train)], data[len(train):]\n\nX = train.drop(columns=['SalePrice', 'Id']) \ny = train['SalePrice']\n\ntest = test.drop(columns=['SalePrice', 'Id'])","e7987ad6":"kfold = KFold(n_splits=5, random_state = 2020, shuffle = True)\n\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\nmodel_lgb.fit(X, y)\nr2_score(model_lgb.predict(X), y)\n","06292494":"import xgboost as xgb\n#Data is used the same as LGB\nX = train.drop(columns=['SalePrice', 'Id']) \ny = train['SalePrice']\nX.head()","cc8e67e4":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\nmodel_xgb.fit(X, y)\nr2_score(model_xgb.predict(X), y)\n","3f3ede08":"from catboost import CatBoostRegressor\n#Data is used the same as LGB\nX = train.drop(columns=['SalePrice', 'Id']) \ny = train['SalePrice']\nX.head()","da0516b1":"cb_model = CatBoostRegressor(iterations=500,\n                             learning_rate=0.05,\n                             depth=10,\n                             random_seed = 42,\n                             bagging_temperature = 0.2,\n                             od_type='Iter',\n                             metric_period = 50,\n                             od_wait=20)\ncb_model.fit(X, y)\nr2_score(cb_model.predict(X), y)\n","f5abdfc9":"from sklearn.linear_model import SGDRegressor\n#Data is used the same as LGB\nX = train.drop(columns=['SalePrice', 'Id']) \ny = train['SalePrice']\nX.head()","371a2600":"SGD = SGDRegressor(max_iter = 100)\nSGD.fit(X, y)\nr2_score(SGD.predict(X), y)\n","77c15ab1":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\n#Data is used the same as LGB\nX = train.drop(columns=['SalePrice', 'Id']) \ny = train['SalePrice']\nX.head()","e1dc42d0":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\nlasso.fit(X, y)\nr2_score(lasso.predict(X), y)\n","a1541264":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nKRR.fit(X, y)\nr2_score(KRR.predict(X), y)","773eef04":"from sklearn.linear_model  import BayesianRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\n#Data is used the same as LGB\nX = train.drop(columns=['SalePrice', 'Id']) \ny = train['SalePrice']\nX.head()","7e735428":"BR = BayesianRidge()\nBR.fit(X, y)\nr2_score(BR.predict(X), y)","104d4287":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\n#Data is used the same as LGB\nX = train.drop(columns=['SalePrice', 'Id']) \ny = train['SalePrice']\nX.head()","b9d89f40":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\nENet.fit(X, y)\nr2_score(ENet.predict(X), y)","4a8292cb":"# Elastic Net Regression \n","c51b61bd":"> **Preprocessing**","27c32e6e":"**Library and Data**","b1745bec":"**Elastic net is a hybrid of ridge regression and lasso regularization.It combines feature elimination from Lasso and feature coefficient reduction from the Ridge model to improve your model's predictions.**\n","4e8e9a42":"**Model and Accuracy**","0ee7194a":"# Light GBM","bce712ea":"# BayesianRidge","a4a371a4":"* **LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:**\n\n1. Faster training speed and higher efficiency.\n2. Lower memory usage.\n3. Better accuracy.\n4. Support of parallel and GPU learning.\n5. Capable of handling large-scale data.\n\n![build-an-efficient-machine-learning-model-with-lightgbm-29-638.jpg](attachment:build-an-efficient-machine-learning-model-with-lightgbm-29-638.jpg)\n","d8f34f78":"**Library and Data**","1a6b4969":"**Library and Data**","e13a1052":"**Catboost is a type of gradient boosting algorithms which can  automatically deal with categorical variables without showing the type conversion error, which helps you to focus on tuning your model better rather than sorting out trivial errors.Make sure you handle missing data well before you proceed with the implementation.\n**\n![boosting-approach-to-solving-machine-learning-problems-15-638.jpg](attachment:boosting-approach-to-solving-machine-learning-problems-15-638.jpg)","4940628a":"**Model and Accuracy**","35a30628":"**KRR combine Ridge regression and classification with the kernel trick.It is similar to Support vector Regression but relatively very fast.This is suitable for smaller dataset (less than 100 samples)**","0a335dce":"** Bayesian regression, is a regression model defined in probabilistic terms, with explicit priors on the parameters. The choice of priors can have the regularizing effect.Bayesian approach is a general way of defining and estimating statistical models that can be applied to different models.**","d90c5940":"**Stochastic means random , so in Stochastic Gradient Descent dataset sample is choosedn random instead of the whole dataset.hough, using the whole dataset is really useful for getting to the minima in a less noisy or less random manner, but the problem arises when our datasets get really huge and for that SGD come in action**\n![an-overview-of-gradient-descent-optimization-algorithms-13-638.jpg](attachment:an-overview-of-gradient-descent-optimization-algorithms-13-638.jpg)\n\n![image.png](attachment:image.png)","7e1f3e18":"# Lasso\n**In statistics and machine learning, lasso (least absolute shrinkage and selection operator; also Lasso or LASSO) is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the statistical model it produces. Though originally defined for least squares, lasso regularization is easily extended to a wide variety of statistical models including generalized linear models, generalized estimating equations, proportional hazards models, and M-estimators, in a straightforward fashion**\n","4260a0dd":"**Library and Data**","05f23d83":"**Model and Accuracy**","fa12b697":"**Model and Accuracy**","b11da287":"**Model and Accuracy**","2b55fd0e":"**XGBoost is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework. In prediction problems involving unstructured data (images, text, etc.) artificial neural networks tend to outperform all other algorithms or frameworks.It is a perfect combination of software and hardware optimization techniques to yield superior results using less computing resources in the shortest amount of time.**\n![1_1kjLMDQMufaQoS-nNJfg1Q.png](attachment:1_1kjLMDQMufaQoS-nNJfg1Q.png)","5c293a3a":"# Kernel Ridge Regression","08f319f5":"first import the main Library","19adfe43":"> > # **XGBoost**","3e934c38":"**Library and Data**","177f1e87":"# This is the second notebook of the Machine Learning series faster\nEnjoy learning","b630c442":"**Model and Accuracy**","1cfff651":"# Stochastic Gradient Descent","415f56d5":"In this notebook we will talk about four very important algorithms as well as some other important algorithms \n\nLight GBM\n\nXGBoost\n                 \n Catboost\n                 \n Stochastic Gradient Descent\n              \n  Lasso","eaeb5bce":"**Library and Data**","23078c23":"![images%20%282%29.jpg](attachment:images%20%282%29.jpg)","d35dc882":"**Library and Data**","e44469f4":"**Model and Accuracy**","2b6465f8":"# Catboost"}}