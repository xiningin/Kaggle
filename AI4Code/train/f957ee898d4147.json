{"cell_type":{"bee817ad":"code","fc6a3fc5":"code","16d465fe":"code","656d3814":"code","e0ba09d9":"code","262736fd":"code","1920b35a":"code","235f9776":"code","20077e2a":"code","aa4dbb74":"code","cd8613b6":"code","bf21ad7a":"code","54b07f03":"code","cc77de89":"code","8b8f9b8f":"code","4d9e5da4":"code","c1ebbca0":"code","a79c7a45":"code","a9bb8389":"code","02e73ecb":"code","dac0d42d":"code","1c517d62":"code","3e918cd1":"code","43b8ce49":"code","67158937":"code","ebe1a546":"code","3937bc37":"code","1ca8bd65":"code","c92288f3":"code","6203716c":"code","03c9b68f":"code","4b0bb55a":"code","513773a1":"code","beb107c2":"code","1427d651":"code","5f0e2e11":"code","319eb79e":"code","ef088432":"code","473111f2":"code","25a9fbec":"code","2929c81e":"code","312d1059":"code","88ed3327":"code","5c423c3f":"code","7d714c26":"code","4c371227":"code","7833e7f1":"code","0d81d942":"code","aa1d1403":"code","e7854e03":"code","be25e81b":"code","9a8d6dab":"code","5164a3c3":"code","0531ed8a":"code","f5813732":"code","e29c8a62":"code","83f45896":"code","96030f01":"code","8d35c444":"code","d154ddfc":"code","f81f07cc":"code","7233e2f7":"code","e93d3ac2":"markdown","cf921e3b":"markdown","a05f7309":"markdown","4ecc07bb":"markdown","6d2348cc":"markdown","ced31aea":"markdown","04f002ec":"markdown","a5b86f9b":"markdown","590d3484":"markdown","fb31a1a2":"markdown","77c7e234":"markdown","750171ca":"markdown","09225067":"markdown","663de3e2":"markdown","41b671e7":"markdown","c8068fcf":"markdown","be824fdb":"markdown","821bed61":"markdown","a47f6b8a":"markdown","d683fec2":"markdown","9ba5f623":"markdown","792bdbcc":"markdown","2708d3ca":"markdown","3c9642b1":"markdown","44d04dc3":"markdown","45c51d62":"markdown","798f957a":"markdown","9bee0a8d":"markdown","31bfd04d":"markdown","7a66aab2":"markdown","fe3047ba":"markdown","563382e6":"markdown"},"source":{"bee817ad":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport nltk\nimport re\nimport string\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","fc6a3fc5":"real_data = pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/True.csv')\nfake_data = pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/Fake.csv')","16d465fe":"real_data.head()","656d3814":"fake_data.head()","e0ba09d9":"real_data.info()","262736fd":"fake_data.info()","1920b35a":"real_data['target'] = 1\nfake_data['target'] = 0 ","235f9776":"fake_data.tail()","20077e2a":"combine_data = pd.concat([real_data, fake_data], ignore_index=True, sort=False)\ncombine_data.tail()","aa4dbb74":"plt.figure(figsize=(7, 7))\nsns.set(style=\"darkgrid\")\n\ncolor = sns.color_palette(\"Set2\")\nax = sns.countplot(x=\"target\", data=combine_data, palette=color)\n\nax.set(xticklabels=['fake', 'real'])\n\nplt.title(\"Data distribution of fake and real data\")","cd8613b6":"plt.figure(figsize=(15, 10))\nsns.set(style=\"darkgrid\")\n\ncolor = sns.color_palette(\"Set2\")\nax = sns.countplot(x=\"subject\",  hue='target', data=combine_data, palette=color)\n\n# ax.set(xticklabels=['fake', 'real'])\n\nplt.title(\"Data distribution of fake and real data\")","bf21ad7a":"combine_data.isnull().sum()","54b07f03":"import re","cc77de89":"def clean_train_data(x):\n    text = x\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text) # remove square brackets\n    text = re.sub(r'[^\\w\\s]','',text) # remove punctuation\n    text = re.sub('\\w*\\d\\w*', '', text) # remove words containing numbers\n    text = re.sub(r'http\\S+', '', text)\n    text = re.sub('\\n', '', text)\n    return text","8b8f9b8f":"clean_combine_data = combine_data.copy()\nclean_combine_data['text'] = combine_data.text.apply(lambda x : clean_train_data(x))\nclean_combine_data.head()","4d9e5da4":"clean_combine_data.tail()","c1ebbca0":"# clean_combine_data[clean_combine_data['target'] == 0]['text'][21417]","a79c7a45":"# fake_data['text'][0]","a9bb8389":"eng_stopwords = nltk.corpus.stopwords.words(\"english\")","02e73ecb":"def remove_eng_stopwords(text):\n    token_text = nltk.word_tokenize(text)\n    remove_stop = [word for word in token_text if word not in eng_stopwords]\n    join_text = ' '.join(remove_stop)\n    return join_text","dac0d42d":"stopword_combine_data = clean_combine_data.copy()\nstopword_combine_data['text'] = clean_combine_data.text.apply(lambda x : remove_eng_stopwords(x))\nstopword_combine_data.head()","1c517d62":"from itertools import chain\nfrom collections import Counter","3e918cd1":"list_words = stopword_combine_data['text'].str.split()\nlist_words_merge = list(chain(*list_words))\n\nd = Counter(list_words_merge)\ndf = pd.DataFrame(data=d, index=['count'])\ntop_common_words = df.T.sort_values(by=['count'], ascending=False).reset_index().head(50)\ntop_common_words.head()","43b8ce49":"plt.figure(figsize=(15,7))\nsns.set(style=\"darkgrid\")\nsns.barplot(x=\"index\", y='count', data=top_common_words)\nplt.xticks(rotation=90)","67158937":"from nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer","ebe1a546":"lemm = WordNetLemmatizer()","3937bc37":"def word_lemmatizer(text):\n    token_text = nltk.word_tokenize(text)\n    remove_stop = [lemm.lemmatize(w) for w in token_text]\n    join_text = ' '.join(remove_stop)\n    return join_text\n","1ca8bd65":"lemmatize_data = stopword_combine_data.copy()\nlemmatize_data['text'] = stopword_combine_data.text.apply(lambda x : word_lemmatizer(x))\nlemmatize_data.head()","c92288f3":"string = ' '.join(lemmatize_data['text'])","6203716c":"str_val = string.split(\" \")","03c9b68f":"data_unigram=(pd.Series(nltk.ngrams(str_val, 1)).value_counts())[:30]","4b0bb55a":"data_unigram_df=pd.DataFrame(data_unigram)\ndata_unigram_df = data_unigram_df.reset_index()\ndata_unigram_df = data_unigram_df.rename(columns={\"index\": \"key\", 0: \"value\"})\ndata_unigram_df.head()","513773a1":"plt.figure(figsize = (16,9))\nsns.barplot(x='value',y='key', data=data_unigram_df)","beb107c2":"data_bigram=(pd.Series(nltk.ngrams(str_val, 2)).value_counts())[:30]","1427d651":"data_bigram_df=pd.DataFrame(data_bigram)\ndata_bigram_df = data_bigram_df.reset_index()\ndata_bigram_df = data_bigram_df.rename(columns={\"index\": \"key\", 0: \"value\"})\ndata_bigram_df.head()","5f0e2e11":"plt.figure(figsize = (16,9))\nsns.barplot(x='value',y='key', data=data_bigram_df)","319eb79e":"data_trigram=(pd.Series(nltk.ngrams(str_val, 3)).value_counts())[:30]","ef088432":"data_trigram_df=pd.DataFrame(data_trigram)\ndata_trigram_df = data_trigram_df.reset_index()\ndata_trigram_df = data_trigram_df.rename(columns={\"index\": \"key\", 0: \"value\"})\ndata_trigram_df.head()","473111f2":"plt.figure(figsize = (16,9))\nsns.barplot(x='value',y='key', data=data_trigram_df)","25a9fbec":"model_data = stopword_combine_data.copy()","2929c81e":"model_data['combine_text'] = model_data['subject'] + \" \" + model_data['title'] + \" \" + model_data['text']\ndel model_data['title']\ndel model_data['subject']\ndel model_data['date']\ndel model_data['text']\nmodel_data.head()","312d1059":"from sklearn.model_selection import train_test_split","88ed3327":"X_train, X_test, y_train, y_test = train_test_split(model_data['combine_text'], model_data['target'], random_state=0)","5c423c3f":"from sklearn.feature_extraction.text import CountVectorizer","7d714c26":"vec_train = CountVectorizer().fit(X_train)\nX_vec_train = vec_train.transform(X_train)","4c371227":"X_vec_test = vec_train.transform(X_test)","7833e7f1":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score","0d81d942":"model = LogisticRegression()\nmodel.fit(X_vec_train, y_train)","aa1d1403":"predicted_value = model.predict(X_vec_test)","e7854e03":"accuracy_value = roc_auc_score(y_test, predicted_value)\nprint(accuracy_value)","be25e81b":"model_2_data = lemmatize_data.copy()\nmodel_2_data['combine_text'] = model_2_data['title'] + \" \" + model_2_data['text']\ndel model_2_data['title']\ndel model_2_data['subject']\ndel model_2_data['date']\ndel model_2_data['text']\nmodel_2_data.head()","9a8d6dab":"X_train, X_test, y_train, y_test = train_test_split(model_2_data['combine_text'], model_2_data['target'], test_size=0.33, random_state=0)","5164a3c3":"vec_train = CountVectorizer().fit(X_train)\nX_vec_train = vec_train.transform(X_train)\nX_vec_test = vec_train.transform(X_test)","0531ed8a":"model = LogisticRegression()\nmodel.fit(X_vec_train, y_train)\npredicted_value = model.predict(X_vec_test)\naccuracy_value = roc_auc_score(y_test, predicted_value)","f5813732":"print(accuracy_value)","e29c8a62":"ex_combine_data = combine_data.copy()\nex_combine_data = ex_combine_data.replace([\"politicsNews\"], 'politics')\nex_combine_data.head()","83f45896":"plt.figure(figsize=(15, 10))\nsns.set(style=\"darkgrid\")\n\ncolor = sns.color_palette(\"Set2\")\nax = sns.countplot(x=\"subject\",  hue='target', data=ex_combine_data, palette=color)\n\n# ax.set(xticklabels=['fake', 'real'])\n\nplt.title(\"Data distribution of fake and real data\")","96030f01":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(17,8))\n\nword = ex_combine_data[ex_combine_data['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='blue')\nax1.set_title('Real text')\n\nword = ex_combine_data[ex_combine_data['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='red')\nax2.set_title('Fake text')\n\nfig.suptitle('Average word length in each text')","8d35c444":"all_words_after = real_data['text'].str.split()\nmerged = list(chain(*all_words_after))\nd = Counter(merged)\ndf = pd.DataFrame(data=d, index=['count'])\ntop_count_words = df.T.sort_values(by=['count'], ascending=False).reset_index().head(50)\ntop_count_words.head()","d154ddfc":"from collections import Counter\nresults = Counter()\nreal_data['text'].str.lower().str.split().apply(results.update)\nreal_unq_count = len(results)\nprint(real_unq_count)","f81f07cc":"results = Counter()\nfake_data['text'].str.lower().str.split().apply(results.update)\nfake_unq_count = len(results)\nprint(fake_unq_count)","7233e2f7":"plt.figure(figsize=(8,8))\nplt.bar([1, 2], [real_unq_count, fake_unq_count], color=['#72b6a1', '#e99675'])\nplt.xticks([1,2], ('real', 'fake'))\nplt.show()","e93d3ac2":"## Bigram Analysis","cf921e3b":"### so our new predicted result is 99.66, not much difference from previous one. though im not doing much work in modeling. but i can assure you this dataset always given u above 90% accuracy.","a05f7309":"# Modeling","4ecc07bb":"## So What is yours finding, is this title real or fake?","6d2348cc":"#### Subjects are not well distributed The real data contains only two subjects and the fake data contains the remaining subjects. Only Politics are common.\n\n#### So the question is, does this description follow their titles? Otherwise we can say that any news we get about the US or the Middle East is completely fake. Does it make sense?","ced31aea":"# Deep drive in this Dataset","04f002ec":"---\n\n<center><img style=\"width: 700px;\" src=\"https:\/\/images.deepai.org\/glossary-terms\/867de904ba9b46869af29cead3194b6c\/8ARA1.png\"><\/center>\n\n---\n<i>Source: Image from Google<\/i>","a5b86f9b":"#### Average text length are not same for both. And the difference are really remarkable. Yes length can not be same but this difference is huge. Maybe it can be hamper some training model.","590d3484":"## Data Cleaning","fb31a1a2":"## Unigram Analysis","77c7e234":"#### Earlier we saw that the average text length of fake information is not very long, but in unique words it appears higher than the real data. That's mean, the ratio of same words is too much. What Do you think is this okay for prediction?","750171ca":"## Fact-2: Text Length","09225067":"# Bag of Words","663de3e2":"## Trigram Analysis","41b671e7":"<h2 style=\"font-size: 30px;color: #ae2e28;\">Part - 1<\/h2>","c8068fcf":"<h4>Oh please don't be surprised about this title. This title represents only this notebook. Actually this notebook is divided into two parts. <\/h4>\n<h4>In the first part we predict the use of this dataset where we get almost 100% accuracy and in the second part we try to examine this dataset. Whether it is biased or not.<\/h4>\n<h4>So when we get almost 100% accuracy you can say yes this title is true. However, after observing this dataset, your observation may or may not change. If you think I can't see anything wrong with this dataset, you can say this title is ok. Or if you see some biased material in this dataset, you can change your mind. So this is a question to you, what do you think?<\/h4>","be824fdb":"### ohh you see this result it is 99.86 means this title is almost real. but actually is it? u might be found somethings wrong in model execution. ok lets do some in modeling to do more reliable. ","821bed61":"### why this is. is it really easy to find out which news are fake and which are real. i don't know. but i want to show u something about this dataset. ","a47f6b8a":"#### Wow they incredibly use Donald Trump too many times. What u think is this ok?","d683fec2":"## Stopword Removal","9ba5f623":"## Basic EDA","792bdbcc":"## Fact-1: Subject Distribution","2708d3ca":"## Fact-3: Unique Words","3c9642b1":"## Vectorizing","44d04dc3":"---\n\n<center><img style=\"width: 700px;\" src=\"https:\/\/3.bp.blogspot.com\/-4pxORQAgAFI\/XMNZhEssXtI\/AAAAAAAAGmA\/SuQGsp-GyT4jKlUZieg_A5lnTza_GujfwCLcBGAs\/s1600\/bag_of_words.png\"><\/center>\n\n---\n<i>Source: Image from Google<\/i>","45c51d62":"# Modeling -2","798f957a":"# N-Gram Analysis","9bee0a8d":"<h2 style=\"font-size: 30px;color: #ae2e28;\">Part - 2<\/h2>","31bfd04d":"---\n\n<h1 style=\"text-align: center;font-size: 30px; color: #013b86;\">Fake or Real. Two sides of the same coin?<\/h1>\n\n---\n\n<center><img style=\"width: 700px;\" src=\"https:\/\/images.ctfassets.net\/yqezig6gzu6c\/5ur280lovm0DKoHmlEGe1P\/9779c9555ebed3dbd7e3445d0a666843\/https___cdn2.hubspot.net_hubfs_656775_Fact_20Fake_201200_20x_20627px_2x-100_20copy.jpg?w=900&q=100\"><\/center>\n\n---\n<i>Source: Image from Google<\/i>","7a66aab2":"## Find out common words","fe3047ba":"### I'm much inspired from this notebook. You can check also. Im getting some idea from this notebook to knowing you about this dataset.\n[https:\/\/www.kaggle.com\/josutk\/only-one-word-99-2](https:\/\/www.kaggle.com\/josutk\/only-one-word-99-2)","563382e6":"## Lemmatization"}}